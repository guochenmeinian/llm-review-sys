# Spatio-Angular Convolutions for Super-resolution in Diffusion MRI

 Matthew Lyon

University of Manchester

matthew.s.lyon@.manchester.ac.uk

&Paul Armitage

University of Sheffield

p.armitage@sheffield.ac.uk

&Mauricio A Alvarez

University of Manchester

mauricio.alvarezlopez@manchester.ac.uk

###### Abstract

Diffusion MRI (dMRI) is a widely used imaging modality, but requires long scanning times to acquire high resolution datasets. By leveraging the unique geometry present within this domain, we present a novel approach to dMRI angular super-resolution that extends upon the parametric continuous convolution (PCConv) framework. We introduce several additions to the operation including a Fourier feature mapping, global coordinates, and domain specific context. Using this framework, we build a fully parametric continuous convolution network (PCCNN) and compare against existing models. We demonstrate the PCCNN performs competitively while using significantly fewer parameters. Moreover, we show that this formulation generalises well to clinically relevant downstream analyses such as fixed-based analysis, and neurite orientation dispersion and density imaging.

## 1 Introduction

Diffusion magnetic resonance imaging (dMRI) is a clinical imaging modality that is routinely used to diagnose white matter pathology within the human brain. Advancements in downstream analyses using dMRI data continuously expand the diagnostic capabilities of the modality, but often require high resolution data that is clinically infeasible to acquire [28]. However, recent research demonstrates that deep learning can be used to predict high resolution images using low resolution dMRI data [26, 44, 41], a technique known as super-resolution. These advancements therefore pave the way for clinically accessible advanced dMRI analyses.

dMRI data can be represented using a non-Euclidean geometry. Specifically, a dMRI dataset consists of a series of three-dimensional volumes where each volume measures the water diffusivity in a given direction. Here, the direction is quantified by a 3D vector known as the 'b-vector', and is sparsely sampled from a unit sphere. Each three-dimensional volume, however, is regularly sampled in a discrete grid. Therefore, dMRI data has two distinct resolution types: _spatial_ resolution which denotes the regularly sampled grid size, and _angular_ resolution which denotes the number of diffusion directions. Despite this relatively unique geometry, most dMRI deep learning methods use operations grounded in a Euclidean framework, such as convolutional neural networks (CNNs) [43, 44, 41].

This presents an opportunity as typical CNN architectures do not fully utilise the geometric properties present in dMRI data. For example, implicit within the formulation of the CNN is the assumption that data are densely and regularly sampled in a discrete manner. This is only true when considering the three spatial dimensions of dMRI data, whilst the other dimensions would be more suited using approaches like graph convolutional networks (GCNs) [46], spherical CNNs [7], and point cloud CNNs [22]. Examples of approaches that develop geometrically motivated convolutions in the dMRIdomain include those for tissue segmentation [21], lesion segmentation [29], and fibre orientation distribution (FOD) reconstruction [5; 32].

Another promising candidate for a framework capable of utilising the unique dMRI geometry is the parametric continuous convolution (PCConv) introduced by Wang et al. [39], which extended the discrete convolution into the continuous domain. This was achieved via an inner parameterised hypernetwork [16] responsible for generating convolutional weights. Here, the convolutional weights were not learnt directly but were instead sampled from the hypernetwork given kernel coordinates as input. This operation could therefore be used to model arbitrary geometries.

In this work, we build upon the PCConv operation to convolve across both spatial and angular dimensions of dMRI data via a new, flexible, coordinate embedding. We subsequently create a deep parametric continuous CNN (PCCNN) that is well-suited for inference within this domain. Furthermore, we develop additional modifications to the PCCNN by including supplemental prior information in the coordinate embedding. These modifications include the application of Fourier feature mappings, the inclusion of prior domain information, and the incorporation of global coordinate components.

We demonstrate the effectiveness and flexibility of this approach through the task of angular super-resolution of dMRI data in various sampling schemes, including single-shell and multi-shell data [18]. To assess the performance of our method, we compare results across clinically relevant downstream analysis tasks such as fixed-based analysis (FBA) [30], and neurite orientation dispersion and density imaging (NODDI) [45]. We show that the PCCNN performs competitively against a similar recurrent CNN (RCNN) network trained on the same data whilst using approximately a tenth of the number of parameters. Additionally, we demonstrate the generalisability of the network by comparing it against models trained specifically for downstream analysis tasks.

## 2 Related Works

Whilst there is a body of literature within _spatial_ super-resolution methods in the natural image domain [40], and more specifically the dMRI domain [1; 36; 6; 25], this work focuses on _angular_ super-resolution as a means of increasing image quality. Here, the task of inferring high angular resolution data from low angular resolution dMRI acquisitions can broadly be split into two categories: methods that super-resolve raw dMRI data, and those that super-resolve downstream analyses. The former allows for more flexible use of the data and analysis method, but the unconstrained nature of the task makes inference more challenging. The latter constrains the problem by leveraging assumptions made within the analysis method, however the mapping from raw dMRI data to downstream analysis is in general not invertible, therefore the super-resolved data cannot be used to derive other downstream analysis metrics.

Notable works in angular super-resolution of raw dMRI data include Yin et al. [43], whereby a preliminary study constructed a 1D CNN autoencoder that only convolved across the angular dimension, disregarding possible spatial correlations within the data. Lyon et al. [26] developed a 3D RCNN that benefited from parameter sharing in the spatial dimensions, but required angular dimension shuffling to overcome sequence order bias. Both models concatenated the b-vectors as channels within the input, therefore treating the b-vectors as additional features, rather than coordinates of a manifold for which the data lies on. Finally, Ren et al. [31] proposed a conditional generative adversarial network (GAN) that additionally used structural T1w and T2w data, as well as b-vector information to predict unseen dMRI intensity data.

As there exists a myriad of downstream analyses for dMRI, this work focuses on two commonly used methods: FBA [30] and NODDI [45]. FBA relies on segmenting FODs [18] into 'fixels' or fibres within a voxel. Relevant works in FOD super-resolution include Lucena et al. [24] and Zeng et al. [44]. Both of these studies regressed low angular resolution derived FODs with their high resolution counterparts through 3D CNNs. NODDI estimates micro-structural complexity of the underlying brain tissue through modelling axon and dendrite populations within a voxel. Approaches to infer high angular resolution derived NODDI parameters from lower resolution raw dMRI data include Golkov et al. [15], Ye et al. [41], and Ye et al. [42]. These studies use various sub-sampling schemes and network architectures, including a 3D CNN within Ye et al. [41].

Methods

This section introduces parametric continuous convolutions, and extends the framework into the dMRI domain. We highlight our key contributions to the PCConv operation, then finally provide implementation details used to perform training and inference within the task of dMRI super-resolution.

### Parametric Continuous Convolutions

A discrete convolution within the context of deep learning is defined as follows

\[h[n]=(f*g)[n]=\sum_{m=-M}^{M}f[m]g[n-m],\]

where the data function \(f:\mathcal{N}\rightarrow\mathbb{R}\) and kernel \(g:\mathcal{G}\rightarrow\mathbb{R}\) are defined over the support domain for the finite integer set \(\mathcal{N}=\mathbb{Z}^{D}\) and \(\mathcal{G}=\{-M,-M+1,\ldots,M-1,M\}^{D}\) respectively. Conversely, a continuous convolution is defined as

\[h(\mathbf{y}_{j})=(f*g)(\mathbf{y}_{j})=\int_{-\infty}^{\infty}f(\mathbf{x})g (\mathbf{y}_{j}-\mathbf{x})d\mathbf{x},\]

where both \(f\) and \(g\) are continuous functions over the support domain \(\mathcal{N}=\mathcal{G}=\mathbb{R}^{D}\). Typically in deep learning, this integration is analytically intractable but can be approximated as in Wang et al. [39] using the following form:

\[h(\mathbf{y}_{j})=\int_{-\infty}^{\infty}f(\mathbf{x})g(\mathbf{y}_{j}- \mathbf{x})d\mathbf{x}\approx\frac{1}{N}\sum_{i=1}^{N}f(\mathbf{x}_{i})g( \mathbf{y}_{j}-\mathbf{x}_{i}),\] (1)

where a finite number of input points \(\mathbf{x}_{i}\) are sampled from the support domain for each output point \(\mathbf{y}_{j}\). In the parametric continuous convolution framework, we choose to use a multi-layer perceptron (MLP) as the kernel function \(g(\mathbf{e};\bm{\theta})=\mathrm{MLP}(\mathbf{e};\bm{\theta})\), where \(\mathbf{e}\) is a coordinate embedding that depends on \(\mathbf{x}_{i}\) and \(\mathbf{y}_{j}\), which is described in detail in Section 3.2. The kernel function \(g(\mathbf{e};\bm{\theta}):\mathbb{R}^{D}\rightarrow\mathbb{R}\) spans the full continuous domain while being parameterised by a finite number of parameters \(\bm{\theta}\). In contrast to standard convolutional operations, the weights of the PCConv are not learnt directly, and are instead sampled from \(g(\mathbf{e};\bm{\theta})\). As the kernel function is itself a fully differentiable function, it can be trained through back-propagation.

In typical convolutional layers the set of output points form a subset of the input points set, provided there is no padding. However, within the parametric continuous framework, these two sets can be disjoint. This is key within the task of dMRI angular super-resolution, where the angular component of \(\mathbf{y}_{j}\), i.e. the target b-vectors, is not contained within the set of input b-vectors.

Each PCConv layer requires three inputs to produce the output features \(h_{k,j}(\mathbf{y}_{j})\): the input features \(f_{c,i}(\mathbf{x}_{i})\), the set of input points \(\mathbf{x}\), and the set of output points \(\mathbf{y}\). Within each layer, following on from the right-hand side of Equation (1), the PCConv operation is defined as

\[h_{k,j}(\mathbf{y}_{j})=\sum_{c=1}^{C}\sum_{i=1}^{N}f_{c,i}(\mathbf{x}_{i})g_{ c,k}(\mathbf{e};\bm{\theta}).\] (2)

Here, \(C\) and \(K\) denote the input and output feature dimensions respectively. Within each layer the number of input points \(N\) is fixed, therefore the kernel function \(g\) will encapsulate the \(\frac{1}{N}\) factor in Equation (1) during training.

### Convolutions in Q-Space

We define q-space \(\mathbb{Q}\) as the joint spatial-angular domain within dMRI where \(\mathbb{Q}=\{(u,v,w,\rho,\vartheta,\varphi)^{\top}\mid u,v,w,\rho\in\mathbb{R},0\leq\vartheta<2\pi,0\leq\varphi\leq\pi\}\) and \(\mathbf{x}_{j},\mathbf{y}_{i}\in\mathbb{Q}\). Here, \((u,v,w)^{\top}\) denote the spatial components, and \((\rho,\vartheta,\varphi)^{\top}\) denote the angular components. Specifically, \(\vartheta\) and \(\varphi\) refer to the azimuthal and polar angles, respectively, which form the set of points on the surface of a sphere of radius \(\rho\). In this context, \(\rho\) is the diffusion weighting or b-value. Figure 1 visualisesconvolutions across q-space through Equation (2). To perform convolutions using q-space, we first calculate a coordinate embedding \(\mathbf{e}\in\mathbb{R}^{2LE}\) via

\[\mathbf{e}=[\boldsymbol{\gamma}(p_{1}),\boldsymbol{\gamma}(p_{2}),\cdots, \boldsymbol{\gamma}(p_{E})]^{\top},\] (3)

where

\[\boldsymbol{\gamma}(p_{m})=[\sin{(2^{0}\pi p_{m})},\cos{(2^{0}\pi p_{m})}, \cdots,\sin{(2^{L-1}\pi p_{m})},\cos{(2^{L-1}\pi p_{m})}].\] (4)

Here \(p_{m}\) is the \(m\)th component of the coordinate vector \(\mathbf{p}\in\mathbb{R}^{E}\) given by \(\mathbf{p}=[u_{i}-u_{j},v_{i}-v_{j},w_{i}-w_{j},\rho_{i}-\rho_{j},d_{r}]\). We have opted to enforce rotational invariance across the sphere by omitting \(\vartheta\) and \(\varphi\) directly, and instead use \(p_{5}=d_{r}(\vartheta_{j},\varphi_{j},\vartheta_{i},\varphi_{i})\) where \(d_{r}(\vartheta_{j},\varphi_{j},\vartheta_{i},\varphi_{i})=\sin(\vartheta_{j} )\sin(\vartheta_{i})+\cos(\vartheta_{j})\cos(\vartheta_{i})\cos(\varphi_{j}- \varphi_{i})\). The function \(\gamma:\mathbb{R}\rightarrow\mathbb{R}^{2L}\) is a Fourier feature mapping motivated by Tancik et al. [35] and Mildenhall et al. [27] which is used to improve the hypernetwork's ability to learn high-frequency functions. In practice, \(L\) is a hyperparameter of the PCConv layer. Figure 2 provides examples of weights sampled from a trained PCConv layer with varying \(d_{r}\). The channels selected within the Figure demonstrate a variety of functions across the sphere, highlighting the network's ability to learn both high and low frequency kernels.

An important aspect of the discrete convolution is its finite support domain \(M\), thus allowing the network to share parameters across the image domain. Within the PCConv framework this is achieved by limiting the number of points to convolve across, given some constraint. When convolving over q-space, the spatial dimensions follow the same constraint as in discrete convolutions. That is, for each dimension given a kernel size \(n\), only the \(n\) closest points are selected. As the angular components represent sparsely sampled data across a sphere, a kernel size \(n\) selects the \(n\) closest points, as determined by \(d_{r}\). We refer to the angular kernel size as \(k_{q}\) from this point onward. There is an additional constraint with the angular components that only points within a fixed radius \(d_{\max}\), such that \(d_{r}\leq d_{\max}\), are included. To achieve this, a binary mask is applied after the pointwise multiplication of the kernel weight with the input feature. The PCConv is a flexible operation that enables convolutions over non-Euclidean spaces. Additionally, the framework allows for kernel weights to be dependent upon coordinates that are _not_ defined in terms of the relative distance between an input and output point. In this work, we additionally introduce a variant of the PCCNN that uses one such encoding. Denoted as 'PCCNN-Bv', this model uses \(\mathbf{p}=[u_{i}-u_{j},v_{i}-v_{j},w_{i}-w_{j},\rho_{i},\rho_{j},d_{r}]\)

Figure 1: Visualisation of dMRI parametric continuous convolution. Points \(\mathbf{x}_{i}\) and \(\mathbf{y}_{j}\) are of the form \((u,v,w,\rho,\vartheta,\varphi)^{\top}\) with spatial components \((u,v,w)\) and angular components \((\rho,\vartheta,\varphi)\). Left: The raw dMRI data \(f(\mathbf{x})\). Points \(\mathbf{x}_{4}\) and \(\mathbf{x}_{6}\) are located in the same region on the q-space sphere and thus have the same angular components. Similarly, \(\mathbf{x}_{6}\) and \(\mathbf{x}_{20}\) have the same spatial coordinates but different angular components. Middle: the parametric continuous kernel, whose geometry is defined by the coordinate embedding \(\mathbf{e}\). Right: Sampled points from q-space are pointwise multiplied with the kernel and summed over to produce one output feature \(h(\mathbf{y}_{j})\).

The inclusion of this model is motivated by the non-linear relationship between different shells within multi-shell data, which may not be appropriately captured by the relative difference of b-values.

### Global Information

Discrete convolutional layers are computed over a limited kernel size, and therefore have a limited field of view. This affords the convolutional layer computational efficiency, as the kernel size is typically much smaller than the image. However, this comes at the cost of losing global information. Within dMRI, structures within the brain are highly spatially dependent and follow a similar pattern in each individual. Non-local coordinates, and therefore global context, can be incorporated within the PCConv layer through the coordinate embedding. For example, the relative position of the kernel within an image could be incorporated into the coordinate embedding. This idea is inspired by how Vision transformers (ViTs) [12] actively extract global information through correlations between image patches. The availability of global context allows ViTs to show competitive performance in image recognition tasks, where accurate synthesis of semantic meaning more heavily relies on non-local information [19].

In clinical practice, neuroimaging data are routinely aligned to a reference space, such as Talairach space [3]. As such, coordinates within reference spaces are correlated across different subjects, and therefore can be used to provide context for the location of a voxel within the brain. In this work, after normalising the coordinates with respect to the subject brain mask size, we use the centroid of the training patches, as discussed in Section 3.6, as additional coordinates within \(\mathbf{p}\). Denoted as an '-Sp' suffix, both 'PCCNN-Sp' and 'PCCNN-Bv-Sp' models within Section 4 have this modification.

### Factorised Convolutions

dMRI is routinely stored in a dense four-dimensional format, with three spatial dimensions and one angular dimension whose coordinates are defined by \(\mathbf{b}\). Performing a convolution in this high-dimensional space can be computationally expensive. We mitigate this cost by using factorised convolutions [34] within the PCConv framework. For example, in the two-dimensional case, an \((n\times m)\) kernel would be factorised into two sequential convolutions of size \((n\times 1)\) and \((1\times m)\). As in the non-factorised case, the PCConv layer's weights are sampled from one hypernetwork \(g(\mathbf{e};\boldsymbol{\theta})\). Following the approach used in Wang et al. [39], we further factorise the kernel weight tensor from \(W\in\mathbb{R}^{N\times O\times C\times K}\) into two sequential operations: an input convolution \(W_{\mathrm{in}}=\mathbb{R}^{N\times O\times C}\) followed by a forward projection matrix \(W_{\mathrm{out}}=\mathbb{R}^{C\times K}\), where \(O\) is the number of output points.

### Implementation

Since PCConv layers are capable of convolving over both spatial and angular dimensions, the PCCNN models are constructed exclusively of PCConv layers and non-linear activation functions. In practice, this involves a sequence of spatially pointwise \((1\times 1\times 1\times k_{q})\) PCConv layers, followed by blocks of parallel convolutions consisting of spatially pointwise and \((3\times 3\times 3\times k_{q})\) factorised convolutions. The hyperparameters for each PCCNN, including the number of PCConv layers, can be found in Figure 5 in the Appendix. Within this work, each PCConv layer has its own independent hypernetwork. This configuration allows for significant weight sharing capabilities, as the number of parameters for each hypernetwork does not depend on the kernel size or dimensionality. Each

Figure 2: Visualisation of kernel weights sampled from \(g_{c,k}(\mathbf{e};\boldsymbol{\theta})\) within a PCConv layer, where \(c\) and \(k\) index the input and output feature dimensions respectively. To generate the spheres, \(d_{r}\) is varied whilst all other components of \(\mathbf{p}\) are kept constant.

PCConv layer or residual PCConv block is followed by a rectified linear unit (ReLU), excluding the final layer. Each hypernetwork is composed of two dense layers, each followed by a leaky ReLU with a negative slope of \(0.1\), and a final dense layer with output size \(1\) and no subsequent activation. The code for this work is available at github.com/m-lyon/dmri-pcconv.

### Data and Training

Given the high dimensionality of dMRI data, using complete acquisitions as individual training examples is prohibitively expensive in terms of memory. To mitigate this issue, training sets were derived from input image patches \(\mathbf{X}_{\mathrm{in}}\in\mathbb{R}^{10\times 10\times 10\times q_{ \mathrm{in}}}\), and target image patches \(\mathbf{X}_{\mathrm{out}}\in\mathbb{R}^{10\times 10\times 10\times q_{ \mathrm{out}}}\) where \(q_{\mathrm{in}}\) and \(q_{\mathrm{out}}\) denote the number of input and target b-vectors respectively. During training, to sample the angular dimension of the input sets, an initial b-vector \(\mathbf{q}_{0}\) was randomly selected. The next \(n\), up to \(q_{\mathrm{in}}\), b-vectors \(\mathbf{q}\) were chosen using \(\arg\max_{j}(\min_{i}(d_{r}(\mathbf{q}_{i},\mathbf{q}_{j}))),\forall i,j\in \{0,1,...,n-1\}\). Similarly, this process was repeated to sample the output set up to size \(q_{\mathrm{out}}\).

To reduce inter-subject variability, each dMRI dataset was normalised such that 99% of the data lay between \([-1,1]\). Initially, a normalisation range of \([0,1]\) was used; however, this resulted in training instabilities within the hypernetwork. This was likely due to a greater mismatch between the data distribution and the weight initialisation scheme within the hypernetwork, which followed \(w\sim\mathcal{U}(-\sqrt{\frac{1}{k_{\mathrm{in}}}},\sqrt{\frac{1}{k_{\mathrm{in }}}})\), where \(k_{\mathrm{in}}\) was the number of input features in a given layer. dMRI data from the WU-Minn Human Connectome Project (HCP) [38] were used for training, validation and testing. This data were initially processed with the standard HCP processing pipeline [14], as well as denoised using the 'patch2self' algorithm [13]. Models were trained on twenty-seven subjects from the HCP dataset, while three subjects were used for validation during development. Hyperparameters for the PCCNN were selected through a random grid search with RayTune [20].

Each acquisition within the HCP dataset had shells of \(b_{\mathrm{full}}=\{1000,2000,3000\}\)\(\mathrm{s/mm^{2}}\), each with \(90\) directions. Training examples contained randomly selected shells \(b_{\mathrm{in}},b_{\mathrm{out}}\sim U(b_{\mathrm{full}})\). This sampling scheme trained each network on single-shell and multi-shell examples simultaneously. The input angular dimension size was determined via \(q_{\mathrm{in}}\sim U(q_{\mathrm{sample}}),q_{\mathrm{sample}}=\{6,7,...,19,20\}\). This approach allowed each network to learn the data distribution for a range of angular input sizes. To implement this, training examples had a fixed \(q_{\mathrm{in}}=20\), with zero-filled entries for input size less than 20. Models were trained using 4 NVIDIA A100's with a batch size of 16, and an \(\ell_{1}\) loss function, for 200,000 iterations using AdamW [23]. Each PCCNN model took approximately a week to train. Details of pre-processing and training for other models can be found in the Appendix. The model weights used for test inference in Section 4 were the best performing in the validation dataset.

## 4 Experiments and Results

We evaluated the performance in dMRI angular super-resolution using eight, previously unseen subjects within the HCP dataset. Our results included four variants of the PCCNN, as listed in Table 1. As dMRI data is most commonly used for downstream analyses, we compared the results in commonly used techniques for both single-shell and multi-shell experiments. Our results included other methods where appropriate, such as spherical harmonic (SH) interpolation for single-shell experiments, FOD-Net [44] for multi-shell FBA, and SR-q-DL [41] for multi-shell NODDI. FOD-Net is a 3D CNN that maps low resolution FOD data to high resolution FOD data. SR-q-DL is a 3D CNN that maps low resolution dMRI data to high resolution NODDI data. RCNN is a 3D recurrent CNN that maps low resolution dMRI data to high resolution dMRI data. When applicable, we also compared against low angular resolution data. All error metrics were calculated with respect to the full angular resolution HCP dataset.

The reported error values consist of the mean and standard deviation, with distribution statistics generated from the mean performance per test subject. Only voxels within the brain are included

\begin{table}
\begin{tabular}{l l} \hline \hline Model & Parameters \\ \hline SR-q-DL [41] & 0.52 \\ PCCNN & 0.77 \\ PCCNN-Bv & 0.79 \\ PCCNN-Sp & 0.83 \\ PCCNN-Bv-Sp & 0.85 \\ RCNN [26] & 6.82 \\ FOD-Net [44] & 48.17 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Total number of parameters, in millions, of different deep learning approaches.

in the calculations, as determined by a brain mask segmentation. The best mean value in a given category is denoted in bold.

### Single-shell dMRI

Within single-shell inference, we evaluated performance across two analyses. Firstly, on raw dMRI data, of which any downstream analysis task would be derived from. We compared performance across all three shells available within the HCP dataset, as well as in three q-space sub-sampling regimes, as outlined in Table 2. Secondly, we compared performance within FBA, where metrics were derived from the inferred dMRI data. For this, we considered the \(b=1000\ \mathrm{s/mm^{2}}\) shell at three sub-sampling regimes.

#### 4.1.1 dMRI Analysis

Table 2 presents the absolute error (AE) of the inferred dMRI data as compared to the high resolution ground truth. For the \(b=2000\mathrm{s/mm^{2}}\) results, see Table 8 within the Appendix. The PCCNN-Bv-Sp model had the lowest AE in five out of the nine shell and sub-sampling combinations and had the overall best performance when averaged across all data. Despite having approximately a tenth of the number of parameters of the RCNN [26] model, the PCCNN models had lower mean errors across all data regimes except \(q_{\mathrm{in}}=6\) in shells \(b=\{2000,3000\}\ \mathrm{s/mm^{2}}\).

#### 4.1.2 Pixel Analysis

FBA is an analysis method used to estimate the intra-voxel fibre populations. To generate fixed data [10], response functions were first estimated in an unsupervised manner [9], and Single-Shell 3-Tissue (SS3T) constrained spherical deconvolution (CSD) [8] was performed to obtain white matter (WM) FODs. WM FODs were then normalised [11], and finally segmented into kirsis. This workflow used the MRtrix3 [37] package. FOD data is stored as a finite SH expansion, and as such, was compared using the angular correlation coefficient (ACC) [2], given by Equation (5). To compare FBA data, the AE in the apparent fibre density (AFD) within each voxel was calculated by taking the absolute difference between the magnitudes of each inferred fibre population with respect to the ground truth values.

Table 3 summarises the performance in FBA in the low resolution baseline and model based reconstructions compared to the high resolution ground truth, whilst Figure 3 visualises a subset of reconstructed FODs within an axial slice. Here, all PCCNN models performed better than the low resolution baseline in all three sampling schemes in FOD ACC, and had lower AFD AE in the two lowest sampling schemes. The PCCNN models performed competitively against the RCNN. However, in the smallest sub-sampling scheme \(q_{\mathrm{in}}=6\), or equivalently \(6.6\%\) of the original number of voxels, the RCNN performed slightly better. Whilst the low resolution data had lower AFD AE at \(q_{\mathrm{in}}=20\), the PCCNN models had, in the worse case, a relative increased mean error of approximately \(6\%\), compared to \(35\%\) increase in the RCNN model. Additionally, the PCCNN models had greater ACC compared to the low resolution baseline across all sub-sampling schemes, suggesting that their use in probabilistic tractography [33] and connectomics [4] would be beneficial.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{\(q_{\mathrm{in}}=6\)} & \multicolumn{2}{c}{\(q_{\mathrm{in}}=10\)} & \multicolumn{2}{c}{\(q_{\mathrm{in}}=20\)} \\ \cline{2-7}  & \(b=1000\) & \(b=3000\) & \(b=1000\) & \(b=3000\) & \(b=1000\) & \(b=3000\) \\ \hline SH Interpolation & \(84.85\pm 5.47\) & \(106.40\pm 4.29\) & \(47.25\pm 2.85\) & \(46.14\pm 2.02\) & \(45.07\pm 3.10\) & \(42.27\pm 1.86\) \\ RCNN & \(74.01\pm 4.03\) & \(\mathbf{52.11\pm 1.97}\) & \(56.01\pm 3.50\) & \(38.68\pm 1.45\) & \(53.89\pm 3.92\) & \(34.31\pm 1.39\) \\ PCCNN & \(74.55\pm 3.69\) & \(57.99\pm 2.60\) & \(48.41\pm 2.98\) & \(34.99\pm 1.63\) & \(44.03\pm 3.09\) & \(24.86\pm 1.23\) \\ PCCNN-Bv & \(\mathbf{73.55\pm 3.63}\) & \(56.71\pm 2.56\) & \(48.40\pm 2.89\) & \(34.18\pm 1.65\) & \(45.32\pm 3.13\) & \(24.77\pm 1.20\) \\ PCCNN-Sp & \(75.01\pm 3.64\) & \(57.41\pm 2.57\) & \(47.79\pm 2.99\) & \(34.16\pm 1.49\) & \(\mathbf{41.88\pm 2.76}\) & \(23.88\pm 0.98\) \\ PCCNN-Bv-Sp & \(74.87\pm 3.51\) & \(56.78\pm 2.48\) & \(\mathbf{46.92\pm 2.53}\) & \(\mathbf{33.53\pm 1.47}\) & \(42.52\pm 2.57\) & \(\mathbf{23.70\pm 0.98}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Absolute error of dMRI intensity in eight subjects with data derived from various models. Models use single-shell data with angular dimension size \(q_{\mathrm{in}}\) as input, and produce inferred data with angular dimension size \(90-q_{\mathrm{in}}\) from the same shell. b-values are quoted in units of \(\mathrm{s/mm^{2}}\).

[MISSING_PAGE_EMPTY:8]

Input, or low resolution, data were not included in this analysis as multi-shell data is required to reconstruct NODDI parameters. The parameters \(f_{\rm intra}\) and \(f_{\rm iso}\) refer to the estimated volume fraction for intra-cellular and cerebrospinal fluid respectively, whilst orientation dispersion index (OD) is an estimation of the dispersion of white matter fibre bundles.

In this experiment, the PCCNN models perform competitively, yielding the lowest error in \(f_{\rm iso}\) and \(f_{\rm intra}\) across all three sub-sampling rates. The relative error in OD across different models mirrors that of the FOD ACC found in Table 4, suggesting that the trained models are able to infer data that generalise across different downstream analyses. The worst performing model across all sub-sampling schemes was the SR-q-DL model, as is demonstrated qualitatively by Figure 4. Whilst this model could benefit from the more constrained task of inferring NODDI data directly, the lack of geometric prior information, such as b-vector coordinates, suggests that these additions within the PCCNN models were important to its relatively high performance in this task.

## 5 Conclusion

The results of the experiments show that the proposed PCConv operation can be used to build a flexible low parameter network to effectively infer high angular resolution raw dMRI data. It does this through appropriate incorporation of task specific geometric properties, as well as suitable parameter sharing via a hypernetwork. We show that inference of raw dMRI data can be used in downstream analysis with moderately high fidelity, and therefore demonstrates the generalisability of the model.

All three of the PCCNN models with additional modifications (PCCNN-Bv, PCCNN-Sp, PCCNN-Bv-Sp) performed greater or equal to the standard PCCNN in approximately 47% of the experiments presented in this work. Additionally, at least one of the modified PCCNNs outperformed the standard PCCNN in approximately 87% of the experiments. Overall, this suggests that the inclusion of additional domain specific context through the coordinate embedding was beneficial to the models performance.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{\(q_{\rm in}=6\)} & \multicolumn{2}{c}{\(q_{\rm in}=20\)} \\ \cline{2-7}  & POD ACC \(\uparrow\) & AFD AE \(\downarrow\) & POD ACC \(\uparrow\) & AFD AE \(\downarrow\) & POD ACC \(\uparrow\) & AFD AE \(\downarrow\) \\ \hline Lowres & \(0.653\pm 0.008\) & \(0.157\pm 0.014\) & \(0.724\pm 0.008\) & \(0.119\pm 0.012\) & \(0.757\pm 0.010\) & \(0.086\pm 0.011\) \\ FOD-Net & \(\mathbf{0.743\pm 0.008}\) & \(0.087\pm 0.005\) & \(0.767\pm 0.006\) & \(\mathbf{0.072\pm 0.004}\) & \(0.776\pm 0.008\) & \(\mathbf{0.062\pm 0.004}\) \\ RCNN & \(0.685\pm 0.010\) & \(\mathbf{0.087\pm 0.006}\) & \(0.749\pm 0.009\) & \(0.080\pm 0.006\) & \(0.765\pm 0.010\) & \(0.079\pm 0.006\) \\ PCCNN & \(0.658\pm 0.009\) & \(0.090\pm 0.005\) & \(0.753\pm 0.008\) & \(0.077\pm 0.005\) & \(0.792\pm 0.009\) & \(0.068\pm 0.006\) \\ PCCNN-Bv & \(0.681\pm 0.010\) & \(0.091\pm 0.005\) & \(\mathbf{0.770\pm 0.011}\) & \(0.080\pm 0.006\) & \(\mathbf{0.807\pm 0.011}\) & \(0.074\pm 0.006\) \\ PCCNN-Sp & \(0.670\pm 0.013\) & \(0.092\pm 0.006\) & \(0.761\pm 0.013\) & \(0.075\pm 0.006\) & \(0.791\pm 0.014\) & \(0.070\pm 0.007\) \\ PCCNN-Bv-Sp & \(0.675\pm 0.013\) & \(0.089\pm 0.005\) & \(0.766\pm 0.014\) & \(0.075\pm 0.006\) & \(0.798\pm 0.015\) & \(0.067\pm 0.006\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of metrics in fixed-based analysis for eight subjects with metrics derived from both input data and inferred data using various models. Models use single-shell data (\(b=1000\rm{s/mm^{2}}\)) with angular dimension size \(q_{\rm in}\) as input, and produce \(90-q_{\rm in}\)\(b=1000\rm{s/mm^{2}}\), \(90\)\(b=2000\rm{s/mm^{2}}\) inferred volumes, _Lowres_ denotes metrics derived from single-shell input only. Fibre orientation distribution (FOD) angular correlation coefficient (ACC) is defined by Equation (5), whilst apparent fibre density (AFD) absolute error (AE) is calculated using the magnitudes of all pixels within a voxel. \(\uparrow\) denotes that a higher value is better.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{2}{c}{\(q_{\rm in}=6\)} & \multicolumn{2}{c}{\(q_{\rm in}=20\)} \\ \cline{2-6}  & OD & \(f_{\rm iso}\) & \(f_{\rm intra}\) & OD & \(f_{\rm iso}\) & \(f_{\rm intra}\) \\ \hline SR-q-DL & \(0.095\pm 0.004\) & \(0.162\pm 0.015\) & \(0.132\pm 0.010\) & \(0.089\pm 0.003\) & \(0.162\pm 0.015\) & \(0.129\pm 0.010\) \\ RCNN & \(\mathbf{0.074\pm 0.007}\) & \(0.030\pm 0.003\) & \(0.052\pm 0.005\) & \(0.045\pm 0.005\) & \(0.025\pm 0.003\) & \(0.048\pm 0.006\) \\ PCCNN & \(0.084\pm 0.006\) & \(0.030\pm 0.003\) & \(0.054\pm 0.006\) & \(0.039\pm 0.006\) & \(0.024\pm 0.003\) & \(\mathbf{0.045\pm 0.004}\) \\ PCCNN-Bv & \(0.075\pm 0.006\) & \(0.026\pm 0.003\) & \(0.050\pm 0.006\) & \(\mathbf{0.036\pm 0.004}\) & \(0.023\pm 0.003\) & \(0.045\pm 0.006\) \\ PCCNN-Sp & \(0.080\pm 0.008\) & \(0.026\pm 0.004\) & \(\mathbf{0.050\pm 0.010}\) & \(0.038\pm 0.006\) & \(0.025\pm 0.004\) & \(0.051\pm 0.012\) \\ PCCNN-Bv-Sp & \(0.083\pm 0.008\) & \(\mathbf{0.025\pm 0.003}\) & \(0.050\pm 0.012\) & \(0.040\pm 0.007\) & \(\mathbf{0.023\pm 0.004}\) & \(0.047\pm 0.012\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Absolute error (AE) of orientation dispersion index (OD), \(f_{\rm intra}\), and \(f_{\rm iso}\) derived from neurite orientation dispersion and density imaging using input data and inferred data from various models. Models use single-shell data (\(b=1000\rm{s/mm^{2}}\)) with angular dimension size \(q_{\rm in}\) as input, and produce \(90-q_{\rm in}\)\(b=1000\rm{s/mm^{2}}\), \(90\)\(b=2000\rm{s/mm^{2}}\), and \(90\)\(b=3000\rm{s/mm^{2}}\) inferred volumes.

The application of super-resolution in a medical imaging context presents an inherent risk, as deep learning models can be susceptible to hallucinations, or high error rates, when making predictions on out-of-distribution data. Given that this work only includes data from relatively young healthy adults, future work will be needed to validate these methods in a diverse set of diagnostic settings such as datasets with pathologies, wide age ranges, and varying acquisition parameters.

Modern deep learning frameworks rely on highly optimised subroutines to perform standard discrete convolutions. Subsequently, despite having a lower number of parameters, the PCCNNs inference and train times were longer than the other methods presented in this work. This is because the PCConv uses a bespoke implementation that does not benefit from the aforementioned subroutines. This highlights the need for future research into more efficient implementations of the PCConv operation.

In summary, our work further explores the integration of geometric priors into the distinct geometry of dMRI data, and in doing so provides a flexible framework to incorporate arbitrary geometries in different domains and problem formulations.

## 6 Acknowledgements

This work was funded by the Engineering and Physical Sciences Research Council (EPSRC) Doctoral Training Partnership (DTP) Scholarship. Data were provided (in part) by the HCP, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University.

## References

* [1] D. C. Alexander, D. Zikic, A. Ghosh, R. Tanno, V. Wottschel, J. Zhang, E. Kaden, T. B. Dyrby, S. N. Sotiropoulos, H. Zhang, et al. Image quality transfer and applications in diffusion mri. _NeuroImage_, 152:283-298, 2017.
* [2] A. W. Anderson. Measurement of fiber orientation distributions using high angular resolution diffusion imaging. _Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine_, 54(5):1194-1206, 2005.
* [3] I. Bankman. _Handbook of medical image processing and analysis_. Elsevier, 2008.
* [4] T. E. Behrens and O. Sporns. Human connectomics. _Current opinion in neurobiology_, 22(1):144-153, 2012.
* [5] J. J. Bouza, C.-H. Yang, D. Vaillancourt, and B. C. Vemuri. A higher order manifold-valued convolutional neural network with applications to diffusion mri processing. In _Information Processing in Medical Imaging: 27th International Conference, IPMI 2021, Virtual Event, June 28-June 30, 2021, Proceedings 27_, pages 304-317. Springer, 2021.

Figure 4: Axial slice of orientation dispersion index (OD) within one subject across different models. Models use single-shell data (\(b=1000\mathrm{s}/\mathrm{mm}^{2}\)) with angular dimension size \(q_{\mathrm{in}}=10\) as input, and produce \(80\)\(b=1000\mathrm{s}/\mathrm{mm}^{2}\), \(90\)\(b=2000\mathrm{s}/\mathrm{mm}^{2}\), and \(90\)\(b=3000\mathrm{s}/\mathrm{mm}^{2}\) inferred volumes.

* [6] S. Chatterjee, A. Sciarra, M. Dunnwald, R. V. Mushunuri, R. Podsheti, R. N. Rao, G. D. Gopinath, S. Oeltze-Jafra, O. Speck, and A. Nurnberger. Shuffleunet: Super resolution of diffusion-weighted mris using deep learning. In _2021 29th European Signal Processing Conference (EUSIPCO)_, pages 940-944. IEEE, 2021.
* [7] T. S. Cohen, M. Geiger, J. Kohler, and M. Welling. Spherical cnns. _arXiv preprint arXiv:1801.10130_, 2018.
* [8] T. Dhollander and A. Connelly. A novel iterative approach to reap the benefits of multi-tissue csd from just single-shell (+ b= 0) diffusion mri data. In _Proc ISMRM_, volume 24, page 3010, 2016.
* [9] T. Dhollander, R. Mito, D. Raffelt, and A. Connelly. Improved white matter response function estimation for 3-tissue constrained spherical deconvolution. In _Proc. Intl. Soc. Mag. Reson. Med_, volume 555, 2019.
* [10] T. Dhollander, A. Clemente, M. Singh, F. Boonstra, O. Civier, J. D. Duque, N. Egorova, P. Enticott, I. Fuelscher, S. Gajamange, et al. Pixel-based analysis of diffusion mri: methods, applications, challenges and opportunities. _Neuroimage_, 241:118417, 2021.
* [11] T. Dhollander, R. Tabbara, J. Rosnarho-Tornstrand, J.-D. Tournier, D. Raffelt, and A. Connelly. Multi-tissue log-domain intensity and inhomogeneity normalisation for quantitative apparent fibre density. In _Proc. ISMRM_, volume 29, page 2472, 2021.
* [12] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [13] S. Fadnavis, J. Batson, and E. Garyfallidis. Patch2self: Denoising diffusion MRI with self-supervised learning. _arXiv preprint arXiv:2011.01355_, 2020.
* [14] M. F. Glasser, S. N. Sotiropoulos, J. A. Wilson, T. S. Coalson, B. Fischl, J. L. Andersson, J. Xu, S. Jbabdi, M. Webster, J. R. Polimeni, et al. The minimal preprocessing pipelines for the human connectome project. _Neuroimage_, 80:105-124, 2013.
* [15] V. Golkov, A. Dosovitskiy, J. I. Sperl, M. I. Menzel, M. Czisch, P. Samann, T. Brox, and D. Cremers. Q-space deep learning: twelve-fold shorter and model-free diffusion mri scans. _IEEE transactions on medical imaging_, 35(5):1344-1351, 2016.
* [16] D. Ha, A. Dai, and Q. V. Le. Hypernetworks. _arXiv preprint arXiv:1609.09106_, 2016.
* [17] M. Hernandez-Fernandez, I. Reguly, S. Jbabdi, M. Giles, S. Smith, and S. N. Sotiropoulos. Using gpus to accelerate computational diffusion mri: From microstructure estimation to tractography and connectomes. _Neuroimage_, 188:598-615, 2019.
* [18] B. Jeurissen, J.-D. Tournier, T. Dhollander, A. Connelly, and J. Sijbers. Multi-tissue constrained spherical deconvolution for improved analysis of multi-shell diffusion mri data. _NeuroImage_, 103:411-426, 2014.
* [19] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah. Transformers in vision: A survey. _ACM computing surveys (CSUR)_, 54(10s):1-41, 2022.
* [20] R. Liaw, E. Liang, R. Nishihara, P. Moritz, J. E. Gonzalez, and I. Stoica. Tune: A research platform for distributed model selection and training. _arXiv preprint arXiv:1807.05118_, 2018.
* [21] R. Liu, F. Lauze, E. Bekkers, K. Erleben, and S. Darkner. Group convolutional neural networks for dwi segmentation. In _Geometric Deep Learning in Medical Image Analysis_, pages 96-106. PMLR, 2022.
* [22] Z. Liu, H. Tang, Y. Lin, and S. Han. Point-voxel cnn for efficient 3d deep learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* [23] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.

* [24] O. Lucena, S. B. Vos, V. Vakharia, J. Duncan, K. Ashkan, R. Sparks, and S. Ourselin. Enhancing fiber orientation distributions using convolutional neural networks. _arXiv preprint arXiv:2008.05409_, 2020.
* [25] S. Luo, J. Zhou, Z. Yang, H. Wei, and Y. Fu. Diffusion mri super-resolution reconstruction via sub-pixel convolution generative adversarial network. _Magnetic Resonance Imaging_, 88:101-107, 2022.
* [26] M. Lyon, P. Armitage, and M. A. Alvarez. Angular super-resolution in diffusion mri with a 3d recurrent convolutional autoencoder. In _International Conference on Medical Imaging with Deep Learning_, pages 834-846. PMLR, 2022.
* [27] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.
* [28] S. Mori and J. Tournier. Moving beyond dti: high angular resolution diffusion imaging (hardi). _Introduction to diffusion tensor imaging_, pages 65-78, 2014.
* [29] P. Muller, V. Golkov, V. Tomassini, and D. Cremers. Rotation-equivariant deep learning for diffusion mri. _arXiv preprint arXiv:2102.06942_, 2021.
* [30] D. A. Raffelt, J.-D. Tournier, R. E. Smith, D. N. Vaughan, G. Jackson, G. R. Ridgway, and A. Connelly. Investigating white matter fibre density and morphology using fixed-based analysis. _Neuroimage_, 144:58-73, 2017.
* [31] M. Ren, H. Kim, N. Dey, and G. Gerig. Q-space conditioned translation networks for directional synthesis of diffusion weighted images from multi-modal structural mri. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference, Strasbourg, France, September 27-October 1, 2021, Proceedings, Part VII 24_, pages 530-540. Springer, 2021.
* [32] S. Sedlar, T. Papadopoulo, R. Deriche, and S. Deslauriers-Gauthier. Diffusion mri fiber orientation distribution function estimation using voxel-wise spherical u-net. In _Computational Diffusion MRI: International MICCAI Workshop, Lima, Peru, October 2020_, pages 95-106. Springer, 2021.
* [33] R. E. Smith, J.-D. Tournier, F. Calamante, and A. Connelly. Anatomically-constrained tractography: improved diffusion mri streamlines tractography through effective use of anatomical information. _Neuroimage_, 62(3):1924-1938, 2012.
* [34] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2818-2826, 2016.
* [35] M. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. Barron, and R. Ng. Fourier features let networks learn high frequency functions in low dimensional domains. _Advances in Neural Information Processing Systems_, 33:7537-7547, 2020.
* [36] R. Tanno, D. E. Worrall, E. Kaden, A. Ghosh, F. Grussu, A. Bizzi, S. N. Sotiropoulos, A. Criminisi, and D. C. Alexander. Uncertainty modelling in deep learning for safer neuroimage enhancement: Demonstration in diffusion mri. _NeuroImage_, 225:117366, 2021.
* [37] J.-D. Tournier, R. Smith, D. Raffelt, R. Tabbara, T. Dhollander, M. Pietsch, D. Christiaens, B. Jeurissen, C.-H. Yeh, and A. Connelly. Mrtrix3: A fast, flexible and open software framework for medical image processing and visualisation. _Neuroimage_, 202:116137, 2019.
* [38] D. C. Van Essen, S. M. Smith, D. M. Barch, T. E. Behrens, E. Yacoub, K. Ugurbil, W.-M. H. Consortium, et al. The WU-Minn human connectome project: an overview. _Neuroimage_, 80:62-79, 2013.

* [39] S. Wang, S. Suo, W.-C. Ma, A. Pokrovsky, and R. Urtasun. Deep parametric continuous convolutional neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2589-2597, 2018.
* [40] Z. Wang, J. Chen, and S. C. Hoi. Deep learning for image super-resolution: A survey. _IEEE transactions on pattern analysis and machine intelligence_, 43(10):3365-3387, 2020.
* [41] C. Ye, Y. Qin, C. Liu, Y. Li, X. Zeng, and Z. Liu. Super-resolved q-space deep learning. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13-17, 2019, Proceedings, Part III 22_, pages 582-589. Springer, 2019.
* [42] C. Ye, Y. Li, and X. Zeng. An improved deep network for tissue microstructure estimation with uncertainty quantification. _Medical image analysis_, 61:101650, 2020.
* [43] S. Yin, Z. Zhang, Q. Peng, and X. You. Fast and accurate reconstruction of HARDI using a 1D encoder-decoder convolutional network. _arXiv preprint arXiv:1903.09272_, 2019.
* [44] R. Zeng, J. Lv, H. Wang, L. Zhou, M. Barnett, F. Calamante, and C. Wang. Fod-net: A deep learning method for fiber orientation distribution angular super resolution. _Medical Image Analysis_, 79:102431, 2022.
* [45] H. Zhang, T. Schneider, C. A. Wheeler-Kingshott, and D. C. Alexander. Noddi: practical in vivo neurite orientation dispersion and density imaging of the human brain. _Neuroimage_, 61(4):1000-1016, 2012.
* [46] S. Zhang, H. Tong, J. Xu, and R. Maciejewski. Graph convolutional networks: a comprehensive review. _Computational Social Networks_, 6(1):1-23, 2019.

Appendix

### Model Hyperparameters

### RCNN Data and Training

Training parameters, the hardware used, and the data sampling scheme used to train the RCNN were the same as outlined within Section 3.6. Training took approximately 3 days to complete.

### FOD-Net Data and Training

In this work we modify the training scheme used within Zeng et al. [44] to more closely match the PCCNN training methodology. Specifically, each training example input was a low angular resolution FOD volume generated from single-shell dMRI data with b-value \(b_{\mathrm{in}}\) and angular dimension \(q_{\mathrm{in}}\) determined via

\[q_{\mathrm{in}}\sim U(q_{\mathrm{sample}}),q_{\mathrm{sample}}=\{6,10,20\},b_{ \mathrm{in}}\sim U(b_{\mathrm{sample}}),b_{\mathrm{sample}}=\{1000,2000,3000\}.\]

Figure 5: PCCNN model architecture. Angular kernel size is \(k_{g}=q_{\mathrm{in}}=20\). \(C_{1}\) and \(C_{3}\) denote the number of output channels for a \((1\times 1\times 1\times 20)\) and \((3\times 3\times 3\times 20)\) PCConv kernel respectively. \(W_{h}\) denotes the size of the hidden units within each PCConv’s hypernetwork. \(L\) is a hyperparameter of the coordinate embedding \(\mathbf{e}_{s}\) as shown in Equation (3).

Training parameters such as number of iterations, batch size, learning rate, hardware used, and optimisation method remain consistent with those detailed in Section 3.6. Model training took approximately 6 hours to complete.

### SR-q-DL Data and Training

The data sampling used to train the SR-q-DL for this work differs from that outlined in Ye et al. [41]. As in Section A.3, this was done to match the PCCNN training scheme. Here, each training example input was a low angular resolution single-shell dMRI dataset with b-value \(b_{\mathrm{in}}\) and angular dimension \(q_{\mathrm{in}}\) determined via

\[q_{\mathrm{in}}\sim U(q_{\mathrm{sample}}),q_{\mathrm{sample}}=\{6,7,...,19,2 0\},b_{\mathrm{in}}\sim U(b_{\mathrm{sample}}),b_{\mathrm{sample}}=\{1000,200 0,3000\}.\]

Training parameters such as number of iterations, batch size, learning rate, hardware used, and optimisation method remain consistent with those detailed in Section 3.6. Model training took approximately 30 minutes to complete.

### Spherical Harmonic Interpolation

SH interpolation of single-shell dMRI data followed the same procedure as outlined in Lyon et al. [26]. Briefly, SH coefficients for each spatial voxel within the low angular resolution dataset were fit using the pseudo-inverse least squares method using a spherical harmonic order of 2. These derived SH coefficients were then used to calculate the interpolated spatial voxels for the target b-vectors.

### Single-shell dMRI PSNR and MSSIM

Within Table 6 we present the peak signal-to-noise ratio (PSNR) of dMRI intensity within the single-shell experiment. Similarly, Table 7 outlines the mean structural similarity index (MSSIM) of dMRI intensity within the same experiment.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{2}{c}{\(q_{\mathrm{in}}=6\)} & \multicolumn{2}{c}{\(q_{\mathrm{in}}=10\)} & \multicolumn{2}{c}{\(q_{\mathrm{in}}=20\)} \\ \cline{2-6}  & \(b=1000\) & \(b=3000\) & \(b=1000\) & \(b=3000\) & \(b=1000\) & \(b=3000\) \\ \hline SH Interpolation & \(33.83\pm 1.48\) & \(25.23\pm 1.15\) & \(39.62\pm 1.67\) & \(33.54\pm 1.24\) & \(40.04\pm 1.65\) & \(34.50\pm 1.24\) \\ RCNN & \(\mathbf{35.64\pm 1.49}\) & \(\mathbf{32.61\pm 1.12}\) & \(38.37\pm 1.51\) & \(35.80\pm 1.15\) & \(38.72\pm 1.45\) & \(37.04\pm 1.16\) \\ PCCNN & \(35.18\pm 1.50\) & \(31.13\pm 1.15\) & \(39.52\pm 1.54\) & \(36.43\pm 1.26\) & \(40.29\pm 1.49\) & \(39.93\pm 1.40\) \\ PCCNN-Bv & \(35.41\pm 1.58\) & \(31.45\pm 1.13\) & \(39.44\pm 1.76\) & \(36.77\pm 1.27\) & \(39.91\pm 1.76\) & \(39.69\pm 1.37\) \\ PCCNN-Sp & \(35.11\pm 1.59\) & \(31.25\pm 1.15\) & \(39.60\pm 1.56\) & \(36.70\pm 1.24\) & \(\mathbf{40.62\pm 1.58}\) & \(40.28\pm 1.33\) \\ PCCNN-Bv-Sp & \(35.04\pm 1.38\) & \(31.35\pm 1.18\) & \(\mathbf{39.64\pm 1.70}\) & \(\mathbf{36.93\pm 1.23}\) & \(40.27\pm 1.66\) & \(\mathbf{40.35\pm 1.34}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Peak signal-to-noise ratio (PSNR) of dMRI intensity in eight subjects with data derived from various models. Models use single-shell data with angular dimension size \(q_{\mathrm{in}}\) as input, and produce inferred data with angular dimension size \(90-q_{\mathrm{in}}\) from the same shell. b-values are quoted in units of \(\mathrm{s/mm^{2}}\).

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{2}{c}{\(q_{\mathrm{in}}=6\)} & \multicolumn{2}{c}{\(q_{\mathrm{in}}=10\)} & \multicolumn{2}{c}{\(q_{\mathrm{in}}=20\)} \\ \cline{2-6}  & \(b=1000\) & \(b=3000\) & \(b=1000\) & \(b=3000\) & \(b=1000\) & \(b=3000\) \\ \hline SH Interpolation & \(0.957\pm 0.004\) & \(0.788\pm 0.010\) & \(0.989\pm 0.001\) & \(0.945\pm 0.004\) & \(0.990\pm 0.001\) & \(0.954\pm 0.004\) \\ RCNN & \(\mathbf{0.973\pm 0.002}\) & \(\mathbf{0.932\pm 0.005}\) & \(0.986\pm 0.001\) & \(0.964\pm 0.003\) & \(0.988\pm 0.001\) & \(0.971\pm 0.002\) \\ PCCNN & \(0.968\pm 0.003\) & \(0.910\pm 0.006\) & \(0.989\pm 0.001\) & \(0.970\pm 0.003\) & \(0.992\pm 0.001\) & \(0.985\pm 0.002\) \\ PCCNN-Bv & \(0.970\pm 0.003\) & \(0.916\pm 0.006\) & \(0.989\pm 0.001\) & \(0.971\pm 0.003\) & \(0.991\pm 0.001\) & \(0.986\pm 0.002\) \\ PCCNN-Sp & \(0.968\pm 0.003\) & \(0.911\pm 0.006\) & \(0.989\pm 0.001\) & \(0.970\pm 0.003\) & \(\mathbf{0.992\pm 0.001}\) & \(0.986\pm 0.001\) \\ PCCNN-Bv-Sp & \(0.968\pm 0.003\) & \(0.913\pm 0.007\) & \(\mathbf{0.989\pm 0.001}\) & \(\mathbf{0.972\pm 0.002}\) & \(0.992\pm 0.001\) & \(\mathbf{0.986\pm 0.001}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Mean structural similarity index (MSSIM) of dMRI intensity in eight subjects with data derived from various models. Models use single-shell data with angular dimension size \(q_{\mathrm{in}}\) as input, and produce inferred data with angular dimension size \(90-q_{\mathrm{in}}\) from the same shell. b-values are quoted in units of \(\mathrm{s/mm^{2}}\).

### Additional Single-shell dMRI Analysis

This section provides performance metrics of models in single-shell inference using \(b=2000\mathrm{s/mm^{2}}\) data. Specifically, Table 8 outlines the AE of dMRI intensity, whilst Table 9 and Table 10 outline the PSNR and MSSIM of dMRI intensity respectively.

### Angular Correlation Coefficient

The angular correlation coefficient (ACC) is given by

\[\mathrm{ACC}(\alpha,\beta)=\frac{\sum_{l=0}^{8}\sum_{m=-l}^{l}\alpha_{l,m} \beta_{l,m}^{*}}{\sqrt{\sum_{l=0}^{8}\sum_{m=-l}^{l}\alpha_{l,m}^{2}}\sqrt{\sum_ {l=0}^{8}\sum_{m=-l}^{l}\beta_{l,m}^{2}}},\] (5)

where \(\alpha_{l,m}\) and \(\beta_{l,m}\) denote the SH coefficients for two FODs with degree \(l\) and order \(m\).

### Additional NODDI Analysis

Table 11 presents results from the NODDI experiment within the \(q_{\mathrm{in}}=10\) sub-sampling scheme.

### Ablation Studies

This section describes results from various ablation studies performed with the PCCNN model as a baseline. Each of the models were used to infer multi-shell data from single-shell input data across three separate shells. The following ablations were performed:

* **No Fourier features**: The Fourier feature mapping \(\bm{\gamma}(p_{m})\), as described in Equation (3.2) were removed from the PCConv operation.
* **No b-vectors**: The b-vector coordinates \(\mathbf{e}_{b}\) were removed from the PCConv operation, thus reducing the operation to only convolve using spatial based co-ordinates and the b-value difference \(\rho_{i}-\rho_{j}\).
* \(\bm{d_{\mathrm{max}}}=\frac{1}{4}\bm{\pi}\): Only points on the q-space sphere at a spherical distance of \(d_{r}\leq\frac{1}{4}\pi\) away from the centre of the kernel were included within the PCConv operation.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(q_{\mathrm{in}}=6\) & \(q_{\mathrm{in}}=10\) & \(q_{\mathrm{in}}=20\) \\ \hline SH Interpolation & \(103.53\pm 4.64\) & \(45.63\pm 2.21\) & \(43.67\pm 2.25\) \\ RCNN & \(\mathbf{61.60\pm 2.43}\) & \(42.79\pm 1.75\) & \(39.14\pm 1.93\) \\ PCCNN & \(65.72\pm 2.83\) & \(37.93\pm 1.93\) & \(30.21\pm 1.81\) \\ PCCNN-Bv & \(64.63\pm 2.74\) & \(36.98\pm 1.94\) & \(30.84\pm 1.93\) \\ PCCNN-Sp & \(65.74\pm 2.78\) & \(36.82\pm 1.77\) & \(28.39\pm 1.49\) \\ PCCNN-Bv-Sp & \(65.33\pm 2.71\) & \(\mathbf{36.18\pm 1.66}\) & \(\mathbf{28.32\pm 1.48}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Absolute error (AE) of dMRI intensity in eight subjects with data derived from various models. Models use single-shell \(b=2000\mathrm{s/mm^{2}}\) data with angular dimension size \(q_{\mathrm{in}}\) as input, and produce inferred \(b=2000\mathrm{s/mm^{2}}\) data with angular dimension size \(90-q_{\mathrm{in}}\).

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(q_{\mathrm{in}}=6\) & \(q_{\mathrm{in}}=10\) & \(q_{\mathrm{in}}=20\) \\ \hline SH Interpolation & \(27.49\pm 1.27\) & \(35.94\pm 1.40\) & \(36.40\pm 1.43\) \\ RCNN & \(\mathbf{33.25\pm 1.21}\) & \(36.94\pm 1.35\) & \(37.79\pm 1.36\) \\ PCCNN & \(32.17\pm 1.23\) & \(37.95\pm 1.36\) & \(40.03\pm 1.46\) \\ PCCNN-Bv & \(32.43\pm 1.23\) & \(38.20\pm 1.42\) & \(39.75\pm 1.52\) \\ PCCNN-Sp & \(32.22\pm 1.23\) & \(38.22\pm 1.38\) & \(40.50\pm 1.45\) \\ PCCNN-Bv-Sp & \(32.26\pm 1.22\) & \(\mathbf{38.38\pm 1.36}\) & \(\mathbf{40.51\pm 1.45}\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Peak signal-to-noise ratio (PSNR) of dMRI intensity in eight subjects with data derived from various models. Models use single-shell \(b=2000\mathrm{s/mm^{2}}\) data with angular dimension size \(q_{\mathrm{in}}\) as input, and produce inferred \(b=2000\mathrm{s/mm^{2}}\) data with angular dimension size \(90-q_{\mathrm{in}}\).

* \(\bm{d_{\rm max}}=\frac{1}{8}\bm{\pi}\): Only points on the q-space sphere at a spherical distance of \(d_{r}\leq\frac{1}{8}\pi\) away from the centre of the kernel were included within the PCConv operation.

Each modification was applied to all layers within the respective model. The baseline PCCNN model had a \(d_{\rm max}=1\). The results of these ablations are shown in Table 12.

Results within Table 12 clearly demonstrate that including b-vector information within the PCConv operation is essential for the model to perform well, as the AE significantly increases within the 'No b-vectors' experiment, as compared to the baseline PCCNN. The two \(d_{\rm max}\) ablation results also show that removing information pertaining to the angular co-ordinate system readily reduces performance of the model. It is noteworthy that a \(d_{\rm max}=\frac{1}{8}\pi\) has significantly higher AE compared to the 'No b-vectors' variant. This is likely due to the greatly reduced number of points available for each kernel operation within the \(d_{\rm max}=\frac{1}{8}\pi\) experiment. This is in contrast to the 'No b-vector' variant which uses all available points within the operation, but with no angular distance information.

The inclusion of Fourier features within the PCConv operation improves performance in two of the shells, but decreases performance in one. This suggests that the inclusion of the Fourier features may improve the ability of the model to generalise to different shells, however this effect is not as apparent as the inclusion of b-vector information. The limited improvement in this case may be twofold.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(b=1000\) & \(b=2000\) & \(b=3000\) \\ \hline Baseline PCCNN & \(51.994\pm 2.719\) & \(\bm{54.553\pm 3.427}\) & \(\bm{74.946\pm 6.399}\) \\ No Fourier features & \(\bm{49.681\pm 3.386}\) & \(55.619\pm 5.190\) & \(76.210\pm 8.119\) \\ No b-vectors & \(86.074\pm 3.589\) & \(89.058\pm 5.064\) & \(110.495\pm 10.384\) \\ \(d_{\rm max}=\frac{1}{4}\pi\) & \(73.863\pm 4.738\) & \(85.552\pm 9.441\) & \(99.232\pm 8.272\) \\ \(d_{\rm max}=\frac{1}{8}\pi\) & \(205.432\pm 8.988\) & \(222.977\pm 13.554\) & \(259.969\pm 20.038\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: Absolute error of dMRI intensity in eight subjects with data derived from ablation models. Models use single-shell data with angular dimension size \(q_{\rm in}\) as input, and infer \(80\) volumes within the input shell, and \(90\) volumes within the other two shells. b-values are quoted in units of \(\rm s/mm^{2}\).

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & OD & \(f_{\rm iso}\) & \(f_{\rm intra}\) \\ \hline SR-q-DL & \(0.089\pm 0.004\) & \(0.162\pm 0.015\) & \(0.128\pm 0.010\) \\ RCNN & \(0.054\pm 0.006\) & \(0.026\pm 0.003\) & \(0.048\pm 0.004\) \\ PCCNN & \(0.048\pm 0.006\) & \(0.027\pm 0.003\) & \(0.049\pm 0.006\) \\ PCCNN-Bv & \(\bm{0.044\pm 0.005}\) & \(0.025\pm 0.003\) & \(\bm{0.048\pm 0.006}\) \\ PCCNN-Sp & \(0.048\pm 0.007\) & \(0.025\pm 0.004\) & \(0.048\pm 0.010\) \\ PCCNN-Bv-Sp & \(0.050\pm 0.008\) & \(\bm{0.024\pm 0.004}\) & \(0.049\pm 0.012\) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Absolute error (AE) of orientation dispersion index (OD), \(f_{\rm intra}\), and \(f_{\rm iso}\) derived from neurite orientation dispersion and density imaging using both input data and inferred data from various models. Models use single-shell data (\(b=1000\rm s/mm^{2}\)) with angular dimension size \(q_{\rm in}=10\) as input, and produce \(80\)\(b=1000\rm s/mm^{2}\), \(90\)\(b=2000\rm s/mm^{2}\), and \(90\)\(b=3000\rm s/mm^{2}\) inferred volumes.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(b=1000\) & \(b=2000\) & \(b=3000\) \\ \hline Baseline PCCNN & \(51.994\pm 2.719\) & \(\bm{54.553\pm 3.427}\) & \(\bm{74.946\pm 6.399}\) \\ No Fourier features & \(\bm{49.681\pm 3.386}\) & \(55.619\pm 5.190\) & \(76.210\pm 8.119\) \\ No b-vectors & \(86.074\pm 3.589\) & \(89.058\pm 5.064\) & \(110.495\pm 10.384\) \\ \(d_{\rm max}=\frac{1}{4}\pi\) & \(73.863\pm 4.738\) & \(85.552\pm 9.441\) & \(99.232\pm 8.272\) \\ \(d_{\rm max}=\frac{1}{8}\pi\) & \(205.432\pm 8.988\) & \(222.977\pm 13.554\) & \(259.969\pm 20.038\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: Absolute error of dMRI intensity in eight subjects with data derived from ablation models. Models use single-shell data with angular dimension size \(q_{\rm in}\) as input, and infer \(80\) volumes within the input shell, and \(90\) volumes within the other two shells. b-values are quoted in units of \(\rm s/mm^{2}\).

First, due to the increased complexity of the model, as the co-ordinate embedding dimensionality would increase by a factor of \(2L\). Second, whilst Fourier features were shown to vastly improve performance when predicting image intensity from co-ordinates in MLP networks [35], the Fourier feature mapping here is only applied to the co-ordinates within the domain of the kernel, which is a much smaller subset of the entire image domain.

### Noisy Data

This section provides additional results from an experiment using dMRI data that were not denoised prior to training or inference. This analysis includes an additional model, denoted the 'Q-Space CGAN' as detailed within Ren et al. [31]. The weights for this model were trained on noisy HCP data and were obtained from the original authors of the work. The other models presented in this section were trained exactly as outlined previously, with the only difference being that the data were not denoised prior to training.

Figure 6 shows an axial slice of dMRI mean absolute error (MAE) in three models when inferring multi-shell data. Whilst this is only a qualitative example, the Q-Space CGAN has significantly higher MAE across the slice as compared to the other two models. This is likely due to lack of diffusion weighted context provided to the Q-Space CGAN as input.

Table 13 presents results from the multi-shell AFD experiment, as shown in Section 4.2.1 using models trained with non-denoised data. These results follow similar trends as present in Table 4, whereby the FOD-Net generally performs best in AFD AE whilst the PCCNN variants perform best within FOD ACC across sub-sampling schemes.

Figure 6: Axial slice of mean absolute error (MAE) within one subject across different models. RCNN and PCCNN-Bv-Sp models use noisy single-shell data (\(b=1000\mathrm{s/mm^{2}}\)) with angular dimension size \(q_{\mathrm{in}}=6\) as input, and produce \(84\)\(b=1000\mathrm{s/mm^{2}}\), \(90\)\(b=2000\mathrm{s/mm^{2}}\), and \(90\)\(b=3000\mathrm{s/mm^{2}}\) inferred volumes. Q-Space CGAN uses b0, T1w, and T2w volumes to infer the same output.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{\(q_{\rm in}=6\)} & \multicolumn{2}{c}{\(q_{\rm in}=10\)} & \multicolumn{2}{c}{\(q_{\rm in}=20\)} \\ \cline{2-7}  & FOD ACC \(\uparrow\) & AFD AE \(\downarrow\) & FOD ACC \(\uparrow\) & AFD AE \(\downarrow\) & FOD ACC \(\uparrow\) & AFD AE \(\downarrow\) \\ \hline Lowres & \(0.509\pm 0.006\) & \(0.187\pm 0.015\) & \(0.547\pm 0.004\) & \(0.145\pm 0.011\) & \(0.590\pm 0.006\) & \(0.122\pm 0.009\) \\ FOD-Net & \(0.645\pm 0.007\) & \(\mathbf{0.097\pm 0.004}\) & \(0.654\pm 0.005\) & \(\mathbf{0.084\pm 0.003}\) & \(0.669\pm 0.007\) & \(\mathbf{0.076\pm 0.004}\) \\ Q-Space CGAN & \(0.486\pm 0.014\) & \(0.160\pm 0.013\) & \(0.496\pm 0.014\) & \(0.160\pm 0.013\) & \(0.518\pm 0.013\) & \(0.156\pm 0.012\) \\ RCNN & \(0.641\pm 0.010\) & \(0.108\pm 0.005\) & \(0.707\pm 0.017\) & \(0.091\pm 0.004\) & \(0.751\pm 0.017\) & \(0.085\pm 0.001\) \\ PCCNN-Bv-Sp & \(\mathbf{0.648\pm 0.008}\) & \(0.098\pm 0.003\) & \(\mathbf{0.723\pm 0.010}\) & \(0.084\pm 0.002\) & \(\mathbf{0.762\pm 0.011}\) & \(0.078\pm 0.001\) \\ \hline \hline \end{tabular}
\end{table}
Table 13: Comparison of metrics in fixed-based analysis for eight subjects with metrics derived from both noisy input data and inferred data using various models trained on noisy data. Models use single-shell data (\(b=1000\rm{s/mm^{2}}\)) with angular dimension size \(q_{\rm in}\) as input, and produce \(80\)\(b=1000\rm{s/mm^{2}}\), \(90\)\(b=2000\rm{s/mm^{2}}\), and \(90\)\(b=3000\rm{s/mm^{2}}\) inferred volumes. _Lowres_ denotes metrics derived from single-shell input only. Fibre orientation distribution (FOD) angular correlation coefficient (ACC) is defined by Equation (5), whilst apparent fibre density (AFD) absolute error (AE) is calculated using the magnitudes of all pixels within a voxel. \(\uparrow\) denotes that a higher value is better.