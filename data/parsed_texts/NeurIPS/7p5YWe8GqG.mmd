MeGraph: Capturing Long-Range Interactions by Alternating Local and Hierarchical Aggregation on Multi-Scaled Graph Hierarchy

 Honghua Dong\({}^{1,2,3\thanks{Equal Contribution. Work done while HD, JX and YY are interns at Tencent Robotics X}}\)   Jiawei Xu\({}^{1,4\thanks{Corresponding authors, contact honghuaad@cs.toronto.edu and lxhan@tencent.com.}}\)   Yu Yang\({}^{1,4\thanks{Project website and open-source code can be found at [https://sites.google.com/view/megraph](https://sites.google.com/view/megraph).}}\)   Rui Zhao\({}^{1}\)   Shiwen Wu\({}^{5}\)

Chun Yuan\({}^{4}\)   Xiu Li\({}^{4}\)   Chris Maddison\({}^{2,3}\)   Lei Han\({}^{1\dagger}\)

\({}^{1}\)Tencent Robotics X \({}^{2}\)University of Toronto \({}^{3}\)Vector Institute

\({}^{4}\)Tsinghua University \({}^{5}\)Hong Kong University of Science and Technology

Equal Contribution. Work done while HD, JX and YY are interns at Tencent Robotics X.Corresponding authors, contact honghuaad@cs.toronto.edu and lxhan@tencent.com.

###### Abstract

Graph neural networks, which typically exchange information between local neighbors, often struggle to capture long-range interactions (LRIs) within the graph. Building a graph hierarchy via graph pooling methods is a promising approach to address this challenge; however, hierarchical information propagation cannot entirely take over the role of local information aggregation. To balance locality and hierarchy, we integrate the local and hierarchical structures, represented by intra- and inter-graphs respectively, of a multi-scale graph hierarchy into a single mega graph. Our proposed MeGraph model consists of multiple layers alternating between local and hierarchical information aggregation on the mega graph. Each layer first performs local-aware message-passing on graphs of varied scales via the intra-graph edges, then fuses information across the entire hierarchy along the bidirectional pathways formed by inter-graph edges. By repeating this fusion process, local and hierarchical information could intertwine and complement each other. To evaluate our model, we establish a new Graph Theory Benchmark designed to assess LRI capture ability, in which MeGraph demonstrates dominant performance. Furthermore, MeGraph exhibits superior or equivalent performance to state-of-the-art models on the Long Range Graph Benchmark. The experimental results on commonly adopted real-world datasets further demonstrate the broad applicability of MeGraph. 1

Footnote 1: Project website and open-source code can be found at [https://sites.google.com/view/megraph](https://sites.google.com/view/megraph).

## 1 Introduction

Graph-structured data, such as social networks, traffic networks, and biological data, are prevalent across a plethora of real-world applications. Recently, Graph Neural Networks (GNNs) have emerged as a powerful tool for modeling and understanding the intricate relationships and patterns present in such data. Most existing GNNs learn graph representations by iteratively aggregating information from individual nodes' local neighborhoods through the message-passing mechanism. Despite their effectiveness, these GNNs struggle to capture long-range interactions (LRIs) between nodes in the graph. For instance, when employing a 4-layer vanilla GNN on the 9-node (\(A\) to \(I\)) graph (as shown in Fig. 1), the receptive field of node \(A\) is limited to 4-hop neighbors, making the aggregation of information from nodes \(G\), \(H\), and \(I\) into node \(A\) quite challenging. While GNNs could theoretically incorporate information from nodes \(n\)-hops away with \(n\)-layers of message passing, this often leads to over-smoothing and over-squashing issues [17; 3] when \(n\) is large.

One mainstream solution to this problem involves constructing a multi-scale graph hierarchy through graph pooling methods. Previous efforts, such as Graph UNets [22] and HGNet [47], have attempted to broaden the receptive field using this strategy. They downsample and upsample the graph, aggregating information along the hierarchy. However, hierarchical information propagation cannot take over the role of local information aggregation. To illustrate, consider the graph hierarchy depicted in Fig. 1. The information propagated along the hierarchy from node \(B\) to nodes \(D\), \(E\), and \(F\) tends to be similar since they share the common path _B-1-X-2_. However, in the original graph, node \(B\) holds different degrees of importance to nodes \(D\), \(E\), and \(F\) as they are 2, 1, and 3 hops away respectively.

To balance the importance of locality and hierarchy, we amalgamate the local and hierarchical structures of a multi-scale graph hierarchy into a single mega graph as depicted in Fig. 1, where we refer to the local structure as intra-graph edges and hierarchical structure as inter-graph edges. Based on this mega graph, we introduce our MeGraph model consisting of \(n\) Mee layers. Each layer first performs local-aware message-passing on graphs of varied scales via the intra-graph edges and then fuses information across the whole hierarchy along the bidirectional pathways formed by inter-graph edges. This method enables hierarchically fused information to circulate within local structures and allows locally fused information to distribute across the hierarchy. By repeating this fusion process, local and hierarchical information could intertwine and complement each other. Moreover, to support flexible graph pooling ratios when constructing the multi-scale graph hierarchy, we propose a new graph pooling method S-EdgePool that improves from EdgePool [13].

In our experiments, We first evaluate MeGraph's capability to capture Long Range interactions (LRIs). We establish a Graph Theory Benchmark comprising four tasks related to shortest paths and one related to connected components. MeGraph demonstrates superior performance compared with many competitive baselines. MeGraph also achieves comparable or superior performance than the state-of-the-art on the Long Range Graph Benchmark (LRGB) [17]. In addition, we perform extensive experiments on widely-used real-world datasets that are not explicitly tailored for assessing the capacity to capture LRIs. These include the GNN benchmark [15] and OGB-G datasets [28]. In these tests, MeGraph demonstrates superior or equivalent performance compared to the baseline models, suggesting its broad applicability and effectiveness.

The main contributions of this work are summarized as follows: 1) **Mega graph and novel architecture**: we propose the mega graph, a multi-scale graph formed by intra- and inter-graph edges, where the message-passing over the mega graph naturally balances locality and hierarchy. On this basis, we introduce a novel architecture MeGraph, which alternates information aggregation along the intra- and inter-edges of the mega graph. This fusion process intertwines local and hierarchical information, leading to mutual benefits. 2) **Hierarchical information fusion**: we design a bidirectional pathway to facilitate information fusion among the hierarchies. 3) **S-EdgePool**: we enhance EdgePool into S-EdgePool, allowing an adjustable pooling ratio. 4) **Benchmark and Evaluations**: we establish a new graph theory benchmark to evaluate the ability of models to capture LRIs. In these evaluations, MeGraph exhibits dominant performance. Additionally, MeGraph achieves new SOTA in one task of LRGB and shows better or comparable performance compared with baselines on popular real-world datasets.

Figure 1: Illustration of the graph pooling operation, graph pyramid, and mega graph. **Graph pooling** is a downsampling process comprising SELECT, CONNECT, and REDUCE steps. It begins by selecting subsets for grouping and each subset collapses into a new node in the pooled graph. Next, it forms new edges by merging the original ones, and finally calculates the pooled graph’s features. In this graph, nodes _A B C, D E F_, and _G H I_ are pooled into nodes _1, 2_, and \(3\) respectively, while the edges (_B_, _E_) and (_C_, _D_) are merged into _(1, 2)_. **Graph Pyramid** involves multi-scaled graphs derived from iterative graph pooling, with the height indicating different scales and \(h=1\) symbolizing the original graph. **Mega Graph** is formed by connecting the graph pyramid using inter-graph edges, which are the by-products of graph pooling.

## 2 Notations, Backgrounds and Preliminaries

Let \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) be a graph with node set \(\mathcal{V}\) (of cardinality \(N^{v}\)) and edge set \(\mathcal{E}\) (of cardinality \(N^{e}\)). The edge set can be represented as \(\mathcal{E}=\{(s_{k},t_{k})\}_{k=1:N^{e}}\), where \(s_{k}\) and \(t_{k}\) are the indices of the source and target nodes connected by edge \(k\). We define \(\mathbf{X}^{\mathcal{G}}\) as features of graph \(\mathcal{G}\), which is a combination of global (graph-level) features \(\mathbf{u}^{\mathcal{G}}\), node features \(\mathbf{V}^{\mathcal{G}}\), and edge features \(\mathbf{E}^{\mathcal{G}}\). Accordingly, we use \(\mathbf{V}^{\mathcal{G}}_{i}\) to represent the features of a specific node \(v_{i}\), and \(\mathbf{E}^{\mathcal{G}}_{k}\) denotes the features of a specific edge \((s_{k},t_{k})\). We may abuse the notations by omitting the superscript \(\mathcal{G}\) when there is no context ambiguity.

### Graph Network (GN) Block

We adopt the Graph Network (GN) block design in accordance with the GN framework [6]. In our notation, a GN block accepts a graph \(\mathcal{G}\) and features \(\mathbf{X}=(\mathbf{u},\mathbf{V},\mathbf{E})\) as inputs, and produces new features \(\mathbf{X}^{\prime}=(\mathbf{u}^{\prime},\mathbf{V}^{\prime},\mathbf{E}^{ \prime})\). A full GN block [6] includes the following update steps. In each of these steps, \(\phi\) denotes an update function, typically implemented as a neural network:

**Edge features**: \(\mathbf{E}^{\prime}_{k}=\phi^{e}(\mathbf{E}_{k},\mathbf{V}_{s_{k}},\mathbf{V} _{t_{k}},\mathbf{u}),\forall k\in[1,N^{e}]\).

**Node features**: \(\mathbf{V}^{\prime}_{i}=\phi^{v}(\rho^{e\to v}(\{\mathbf{E}^{\prime}_{k}\}_{k \in[1,N^{e}],t_{k}=i})\), \(\mathbf{V}_{i},\mathbf{u})\), \(\forall i\in[1,N^{v}]\), where \(\rho^{e\to v}\) is an aggregation function taking the features of incoming edges as inputs.

**Global features**: \(\mathbf{u}^{\prime}=\phi^{u}(\rho^{e\to u}(\mathbf{E}^{\prime}),\rho^{v\to u} (\mathbf{V}^{\prime}),\mathbf{u})\), where \(\rho^{e\to u}\) and \(\rho^{v\to u}\) are two global aggregation functions over edge and node features.

Given a fixed graph structure \(\mathcal{G}\) and the consistent input and output formats outlined above, GN blocks can be seamlessly integrated to construct complex, deep graph networks.

### Graph Pooling

Graph pooling operation downsamples the graph structure and its associated features while ensuring the preservation of structural and semantic information inherent to the graph. Drawing from the SRC framework [23], we identify graph pooling as a category of functions,PODL, that maps a graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\) with \(N^{v}\) nodes and features \(\mathbf{X}^{\mathcal{G}}\) to a reduced graph \(\tilde{\mathcal{G}}=(\tilde{\mathcal{V}},\tilde{\mathcal{E}})\) with \(N^{\tilde{v}}\) nodes and new features \(\mathbf{X}^{\mathcal{G}}\). Here, \(N^{\tilde{v}}\leq N^{v}\) and \((\tilde{\mathcal{G}},\mathbf{X}^{\mathcal{G}})=\texttt{PODL}(\mathcal{G}, \mathbf{X}^{\mathcal{G}})\).

The SRC framework deconstructs the POOL operation into SELECT, REDUCE, and CONNECT functions, which encompass most existing graph pooling techniques. We reinterpret these functions in our own notation as follows:

\[(\hat{\mathcal{G}},\mathbf{X}^{\mathcal{G}})=\texttt{SELECT}(\mathcal{G}, \mathbf{X}^{\mathcal{G}});\ \ \tilde{\mathcal{G}}=\texttt{CONNECT}(\mathcal{G}, \hat{\mathcal{G}},\mathbf{X}^{\mathcal{G}});\ \ \mathbf{X}^{\mathcal{G}}=\texttt{REDUCE}(\mathbf{X}^{\mathcal{G}},\hat{\mathcal{G}}, \mathbf{X}^{\mathcal{G}}). \tag{1}\]

As shown in Fig. 1, the SELECT establishes \(N^{\tilde{v}}\) nodes for the pooled graph, and each node \(\tilde{v}\) corresponds to a subset of nodes \(S_{\tilde{v}}\subseteq\mathcal{V}\) in the input graph. This creates an _undirected_ bipartite graph \(\hat{\mathcal{G}}=(\hat{\mathcal{V}},\hat{\mathcal{E}})\), with \(\hat{\mathcal{V}}=\mathcal{V}\cup\hat{\mathcal{V}}\) and \((v,\tilde{v})\in\hat{\mathcal{E}}\) if and only if \(v\in S_{\tilde{v}}\). We refer to this graph \(\hat{\mathcal{G}}\) as the _inter-graph_, a larger graph that links nodes in the input graph \(\mathcal{G}\) with nodes in the pooled graph \(\tilde{\mathcal{G}}\). The SELECT function can be generalized to include inter-graph features \(\mathbf{X}^{\mathcal{G}}\). As an example, edge weights can be introduced for some edge \((\hat{s}_{k},\hat{t}_{k})\) in graph \(\tilde{\mathcal{G}}\) to gauge the importance of node \(\hat{s}_{k}\) from the input graph contributing to node \(\hat{t}_{k}\) in the pooled graph.

The CONNECT function reconstructs the edge set \(\tilde{\mathcal{E}}\) between the nodes in \(\tilde{\mathcal{V}}\) of the pooled graph \(\tilde{\mathcal{G}}\) based on the original edges in \(\mathcal{E}\) and the inter-graph edges in \(\hat{\mathcal{E}}\). The REDUCE function calculates the graph features \(\mathbf{X}^{\mathcal{G}}\) of graph \(\tilde{\mathcal{G}}\) by aggregating input graph features \(\mathbf{X}^{\mathcal{G}}\), taking into account both the inter-graph \(\tilde{\mathcal{G}}\) and features \(\mathbf{X}^{\mathcal{G}}\). In a similar vein to the relationship between graph lifting and coarsening, we define the EXPAND function for graph features, which serves as the inverse of the REDUCE function: \(\mathbf{X}^{\mathcal{G}}=\texttt{EXPAND}(\mathbf{X}^{\mathcal{G}},\hat{ \mathcal{G}},\mathbf{X}^{\mathcal{G}})\).

## 3 Methods

We begin with the introduction of the mega graph (Sec.3.1), which amalgamates the local (intra-edges) and hierarchical (inter-edges) structures of a multi-scale graph hierarchy into a single graph.

Following this, we present the MeGraph model (Sec.3.2), which alternates between the aggregation of local and hierarchical information along the intra- and inter-edges of the mega graph. We then discuss the specific choices made for the core modules of the MeGraph, along with the innovations (Sec.3.3). Finally, we delve into the computational complexity of the MeGraph model (Sec.3.4).

### Connecting Multi-scale Graphs into a Mega Graph

Similar to the concept of an image pyramid [1], a graph pyramid is constructed by stacking multi-scale graphs, which are obtained through iterative downsampling of the graph using a graph pooling technique. Formally, in alignment with the definition of an image feature pyramid [39], we define a graph feature pyramid as a set of graphs \(\mathbf{\mathcal{G}}_{1:h}:=\{\mathcal{G}_{i}\}_{i=1,\cdots,h}\) and their corresponding features \(\mathbf{X}^{\mathbf{\mathcal{G}}_{1:h}}:=\{\mathbf{X}^{\hat{\mathcal{G}}_{i}}\}_{i= 1,\cdots,h}\). Here, \(\mathcal{G}_{1}\) represents the original graph, \(\mathbf{X}^{\hat{\mathcal{G}}_{1}}\) signifies the initial features, \(h\) stands for the _height_, and \((\mathcal{G}_{i},\mathbf{X}^{\hat{\mathcal{G}}_{i}})=\texttt{POOL}(\mathcal{G} _{i-1},\mathbf{X}^{\hat{\mathcal{G}}_{i-1}})\) for \(i>1\).

By iteratively applying the POOL function, we can collect the inter-graphs \(\hat{\mathbf{\mathcal{G}}}_{1:h}:=\{\hat{\mathcal{G}}_{i}\}_{i=1,\cdots,h-1}\) and their features \(\mathbf{X}^{\hat{\mathbf{\mathcal{G}}}_{1:h}}:=\{\mathbf{X}^{\hat{\mathcal{G}}_{i }}\}_{i=1,\cdots,h-1}\) (since there are \(h-1\) inter-graphs for \(h\) intra-graphs), where \((\hat{\mathcal{G}}_{i},\mathbf{X}^{\hat{\mathcal{G}}_{i}})=\texttt{SELECT}( \mathcal{G}_{i},\mathbf{X}^{\hat{\mathcal{G}}_{i}})\) for \(i<h\). The bipartite inter-graph \(\hat{\mathcal{G}}\) and its features \(\mathbf{X}^{\hat{\mathcal{G}}}\) essentially depict the relationships between the graphs before and after the pooling process (see Sec. 2.2).

Finally, as illustrated in Fig. 1, we wire the graph pyramid \(\mathbf{\mathcal{G}}_{1:h}\) using the edges found in the bipartite graphs \(\hat{\mathbf{\mathcal{G}}}_{1:h}\). This results in a mega graph \(\mathcal{MG}=(\mathcal{MV},\mathcal{ME})\), where \(\mathcal{MV}=\bigcup_{i=1}^{h}\mathcal{V}_{i}\) and \(\mathcal{ME}=\bigcup_{i=1}^{h}\mathcal{E}_{i}\cup\bigcup_{i=1}^{h-1}\hat{ \mathcal{E}}_{i}\). The structure of the mega graph would vary as the graph pooling method trains. We denote \(\mathcal{MG}_{\mathrm{intra}}=\bigcup_{i=1}^{h}\mathcal{G}_{i}\) as the intra-graph of \(\mathcal{MG}\), and refer to the edges therein as intra-edges. Correspondingly, \(\mathcal{MG}_{\mathrm{inter}}=\bigcup_{i=1}^{h-1}\hat{\mathcal{G}}_{i}\) is referred to as the inter-graph of \(\mathcal{MG}\), with its corresponding edges termed as inter-edges. The features \(\mathbf{X}^{\mathcal{MG}}\) of the mega graph \(\mathcal{MG}\) is a combination of intra-graph features \(\mathbf{X}^{\mathbf{\mathcal{G}}_{1:h}}\) and inter-graph features \(\mathbf{X}^{\hat{\mathbf{\mathcal{G}}}_{1:h}}\).

### Mega Graph Message Passing

We introduce the MeGraph architecture, designed to perform local and hierarchical aggregations over the mega graph alternately. As shown in Fig.2, the architecture follows the _encode-process-decode_ design [6; 25] and incorporates GN blocks (refer to Sec. 2.1) as fundamental building blocks.

During the _encode_ stage, initial features are inputted into an intra-graph GN block, which is followed by a sequence of graph pooling operations to construct the mega graph \(\mathcal{MG}\) and its associated features

Figure 2: Illustration of the MeGraph model, where \(n_{-}\) denotes \(n-1\). The blue and green circles represent features of intra- and inter-graphs, respectively. In this figure, the horizontal and vertical directions represent the interaction among the local structure (intra-graph) and graph hierarchy (inter-graph) respectively. The features of intra- and inter-graphs are represented by blue and green circles, respectively. In this figure, the horizontal and vertical directions denote the local structure and graph hierarchy respectively. During the _encode_ stage, the mega graph is constructed using graph pooling. In the _process_ stage, the Mee layer, which features bidirectional pathways across multiple scales, is stacked \(n\) times. In the _decode_ stage, multi-scale features are read out. The golden inter GN blocks form bidirectional pathways across the whole hierarchy.

\((\mathbf{X}^{0})^{\mathcal{MG}}\). In the _process_ stage, the Mee layer, which performs both local and hierarchical information aggregation within the mega graph, is stacked \(n\) times. The \(i\)-th Mee layer receives \((\mathbf{X}^{i-1})^{\mathcal{MG}}\) as input and outputs \((\mathbf{X}^{i})^{\mathcal{MG}}\). Through the stacking of Mee layers, a deeper architecture is created, enabling a more profound fusion of local and hierarchical information. Lastly, in the _decode_ stage, the features \((\mathbf{X}^{n})^{\mathcal{MG}}\) are transformed into task-specific representations using readout functions.

**Mee Layer.** The Mee layer is designed to aggregate local and hierarchical information within the mega graph. A detailed structure of the Mee layer is depicted in Fig. 3.

For the \(i\)-th Mee layer, we consider inputs denoted by \((\mathbf{X}^{i-1})^{\mathcal{MG}}=\{(\mathbf{X}^{i-1})^{\boldsymbol{\mathcal{ G}}_{1:h}},(\mathbf{X}^{i-1})^{\boldsymbol{\mathcal{G}}_{1:h}}\}\). For simplicity, we omit the superscript and denote the features of intra- and inter-graphs as \(\{\mathbf{X}^{i-1}_{j}\}_{j=1,\cdots,h}:=(\mathbf{X}^{i-1})^{\boldsymbol{ \mathcal{G}}_{1:h}}\) and \(\{\hat{\mathbf{X}}^{i-1}_{j}\}_{j=1,\cdots,h-1}:=(\mathbf{X}^{i-1})^{ \boldsymbol{\mathcal{G}}_{1:h}}\) respectively.

The first step applies GN blocks on intra-graph edges, performing message passing on the local structure of graphs at each scale: \(\mathbf{X}^{\prime}_{j}=\mathrm{GN}^{i,j}_{\mathrm{intra}}(\mathcal{G}_{j}, \mathbf{X}^{i-1}_{j})\). Here, \(\mathbf{X}^{\prime}_{j}\) represents the updated intra-graph \(\mathcal{G}_{j}\) features.

The second and third steps focus on multi-scale information fusion. The second step applies cross-updates across consecutive heights from \(1\) to \(h\), while the third step reverses the process, forming a bidirectional pathway for the information flow across the hierarchy. The cross-update between consecutive heights \(j\) and \(j+1\) is denoted by a function \((\mathbf{X}^{\prime}_{j},\hat{\mathbf{X}}^{\prime}_{j},\mathbf{X}j+1^{\prime} )=\texttt{X-UPD}(j,\mathbf{X}_{j},\hat{\mathbf{X}}_{j},\mathbf{X}j+1)\). The prime notation indicates the updated value, and residual links [26] are used in practice.

This cross-update can be implemented via an inter-graph convolution with \(\mathrm{GN}^{i,j}_{\mathrm{inter}}\), referred to as _X-Conv_ (detailed in App.C.1). Alternatively, it can be realized using the REDUCE and EXPAND operations of POOL (refer to Sec.2.2) by \(\mathbf{X}^{\prime}_{j+1}=\texttt{REDUCE}(\hat{\mathcal{G}}_{j},\hat{\mathbf{X }}^{0}_{j},\mathbf{X}_{j})\) and \(\mathbf{X}^{\prime}_{j}=\texttt{EXPAND}(\hat{\mathcal{G}}_{j},\hat{\mathbf{X }}^{0}_{j},\mathbf{X}_{j+1})\), where \(\hat{\mathcal{G}}_{j}\) is the \(j\)-th inter-graph. We denote this implementation as _X-Pool_.

The Mee layer outputs features \(\{\mathbf{X}^{i}_{j}\}_{j=1,\cdots,h}\) and \(\{\hat{\mathbf{X}}^{i}_{j}\}_{j=1,\cdots,h-1}\). Residual links [26] can be added from \(\mathbf{X}^{i-1}_{j}\) to \(\mathbf{X}^{i}_{j}\) and from \(\hat{\mathbf{X}}^{i-1}_{j}\) to \(\hat{\mathbf{X}}^{i}_{j}\) empirically, creating shortcuts that bypass GN blocks in the Mee layer. It's worth noting that the intra and inter GN blocks can share parameters across all heights \(j\) to accommodate varying heights, or across all Mee layers to handle varying layer numbers.

### Module Choice and Innovation

MeGraph incorporates two fundamental modules: the graph pooling operator and the GN block. This architecture can accommodate any graph pooling method from the POOL function family (refer to Sec. 2.2). Furthermore, the GN block is not strictly confined to the graph convolution layer found in standard GCN, GIN, or GAT.

**Graph Pooling.** There are a number of commonly used graph pooling methods, including DiffPool [62], TopKPool [22], EdgePool [13], etc. We opt for EdgePool due to its simplicity, efficiency,

Figure 3: Illustration of the Mee layer, where \(i_{-}\) denotes \(i-1\) and \(j_{+}\) denotes for \(j+1\). The blue and green circles represent the features of intra- and inter-graphs, respectively. Grey and golden arrows represent the intra and inter GN blocks. The cross-update utilizes inter GN blocks to exchange information between consecutive heights, as elaborated in the main text. The Mee layer first aggregates information locally along inter-graph edges. It then applies cross-updates sequentially from lower to higher levels, accumulating information along the pathway to pass to the higher hierarchy. The process is reversed in the last step.

and ability to naturally preserve the graph's connectivity through edge contraction. However, edge contraction is applied only to the edges in a specific maximal matching of the graph's nodes [13], thereby setting a lower limit of \(50\%\) to the pooling ratio \(\eta_{v}\). This constraint implies that a minimum of \(\log_{2}N\) pooling operations is required to reduce a graph of \(N\) nodes to a single node. To address this limitation, we propose the Stridden EdgePool (S-EdgePool), which allows for a variable pooling stride.

The principle behind S-EdgePool involves dynamically tracking the clusters of nodes created by the contraction of selected edges. Similar to EdgePool, edges are processed in descending order based on their scores. When an edge is contracted, if both ends do not belong to the same node cluster, the two clusters containing the endpoints of the edge merge. The current edge can be contracted if the resulting cluster contains no more than \(\tau_{c}\) nodes after this edge's contraction. The iteration stops prematurely once a pooling ratio, \(\eta_{v}\), is achieved. During pooling, each node cluster is pooled as a new node. When \(\tau_{c}=2\), S-EdgePool reverts to the original EdgePool. The algorithm's details and pseudocode are available in App. C.2.

For efficiency, we employ the disjoint-set data structure to dynamically maintain the node clusters, which has a complexity of \(O(E\alpha(E))\), where \(E\) is the number of edges and \(\alpha(E)\) is a function that grows slower than \(\log(E)\)[50]. The total time complexity of S-EdgePool is equivalent to EdgePool and is calculated as \(O(ED\)+\(E\log E)\), where \(D\) is the embedding size, \(O(ED)\) from computing edge scores and \(O(E\log E)\) from sorting the edges.

**GN block.** The full GN block, introduced in Sec. 2.1, is implemented as a graph full network (GFuN) layer. This layer exhibits a highly configurable within-block structure, enabling it to express a variety of other architectures (see Sec. 4.2 of [6]), like GCN, GIN, GAT, GatedGCN. Thus, modifying the within-block structure of GFuN is akin to plugging in different GNN cores. Further details can be found in App. C.3.

**Encoder and decoder.** Most preprocessing methods (including positional encodings and graph rewiring), encoding (input embedding) and decoding (readout functions) schemes applicable to GNNs can also be applied to MeGraph. We give implementation details in App. C.4.

### Computational Complexity and Discussion

The overall complexity of the MeGraph model is contingent on the height \(h\), the number of Mee layers \(n\), the chosen modules, and the corresponding hyperparameters. Let \(D\) be the embedding size, \(V\) the number of nodes, and \(E\) the number of edges in the input graph \(\mathcal{G}\). The time complexity of S-Edgepool is \(O(ED\)+\(E\log E)\), and that of a GFuN layer is \(O(VD^{2}\)+\(ED)\). Assuming both the pooling ratios of nodes and edges are \(\eta\), the total time complexity to construct the mega graph \(\mathcal{M}\mathcal{G}\) becomes \(O((ED+E\log E)/(1-\eta))\), where \(\sum_{i=0}^{h-1}\eta^{i}<1/(1-\eta)\). Similarly, the total time complexity of an Mee layer is \(O((VD^{2}+ED)/(1-\eta))\). This complexity is equivalent to a typical GNN layer if we consider \(1/(1-\eta)\) as a constant (for instance, it is a constant of 2 when \(\eta=0.5\)).

Theoretically, when using the same number of layers, MeGraph is better at capturing LRIs than standard message-passing GNNs owning to the hierarchical structure (see App. D.1 for details). On the other hand, MeGraph can degenerate into standard message-passing GNNs (see App. D.2 for details), indicating it should not perform worse than them on other tasks.

## 4 Experiments

We conduct extensive experiments to evaluate the MeGraph's ability to capture long-range interactions (LRIs) and its performance in general graph learning tasks.

### Experimental Settings

**Baselines.** We compare MeGraph model to three baselines as follows: 1) MeGraph \(h\)=1 variant does not use the hierarchical structure and falls back to standard GNNs. 2) MeGraph \(n\)=1 variant gives up repeating information exchange over the mega graph. 3) Graph U-Nets [22] uses a U-shaped design and only traverses the multi-scale graphs once.

Due to page limits, statistics of the datasets are provided in App. B.1, hyper-parameters are reported in Table 9, and the training and implementation details are reported in App. E.

### Perfomance on LRI Tasks

To test MeGraph's ability to capture long-range interactions, we establish a Graph Theory Benchmark, of which four tasks related to shortest path distance, _i.e._, Single Source Shortest Path (SP\({}_{\text{sss}}\)), Single Source Single Destination Shortest Path (SP\({}_{\text{sss}}\)), Graph Diameter (Diameter) and Eccentricity of nodes (ECC); and 1 task related to connected component, _i.e._, Maximum Connected Component of the same color (MCC). To generate diversified undirected and unweighted graphs for each task, we adopt the ten methods used in PNA [12] and add four new methods: cycle graph, pseudotree, SBM, and geographic threshold graphs. The details of the dataset generation can be found in App. B.2.

As depicted in Table 1, the MeGraph model with \(h\)=5, \(n\)=5 significantly outperforms both the \(h\)=1 and \(n\)=1 baselines in terms of reducing regression error across all tasks. It is worth noting that even

\begin{table}
\begin{tabular}{c l|c c c|c|c|c} \hline Category & Model & SP\({}_{\text{sss}}\) & MCC & Diameter & SP\({}_{\text{ss}}\) & ECC & Average \\ \hline \hline \multirow{3}{*}{Baselines (\(h\)=1)} & \(n\)=1 & 11.184 & 1.504 & 11.781 & 22.786 & 20.133 & 13.478 \\  & \(n\)=5 & 3.898 & 1.229 & 5.750 & 12.354 & 18.971 & 8.440 \\  & \(n\)=10 & 2.326 & 1.264 & 5.529 & 7.038 & 18.876 & 7.006 \\ \hline MeGraph (\(h\)=5) & \(n\)=1 & 1.758 & 0.907 & 4.591 & 5.554 & 14.030 & 5.368 \\ EdgePool (\(\tau_{c}\)=2) & \(n\)=5 & 0.790 & 0.767 & 2.212 & 0.712 & 6.593 & 2.215 \\ \hline \multirow{3}{*}{MeGraph \(\tau_{c}\)=3} & \(\tau_{c}\)=3 & 0.660 & 0.747 & 0.719 & 0.459 & **0.942** & 0.705 \\  & \(\eta_{c}\)=0.3 & 2.225 & 0.778 & 1.061 & 3.591 & 2.009 & 1.933 \\ S-EdgePool & \(\eta_{c}\)=0.3, \(\tau_{c}\)=4 & **0.615** & **0.702** & **0.651** & **0.434** & 0.975 & **0.675** \\ Variants & \(\eta_{c}\)=0.5, \(\tau_{c}\)=4 & 1.075 & 0.769 & 0.945 & 1.204 & 1.992 & 1.197 \\ (\(h\)=5, \(n\)=5) & \(\eta_{c}\)=0.3, \(\tau_{c}\)=4 (X-Pool) & 0.935 & 0.751 & 0.864 & 1.462 & 2.003 & 1.203 \\  & \(\eta_{c}\)=0.3, \(\tau_{c}\)=4 (w/o pw) & 0.632 & 0.730 & 0.864 & 0.765 & 2.334 & 1.065 \\ \hline Graph-UNets & \(h\)=5,\(n\)=9,\(\eta_{c}\)=0.3,\(\tau_{c}\)=4 & 1.118 & 1.008 & 2.031 & 1.166 & 2.584 & 1.581 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on Graph Theory Benchmark (medium size). For each task, we report the MSE regression loss on test set, averaged over different graph generation methods. Darker blue cells denote better performance and the bold denotes the best one. We provide detailed results on each type of graphs in App. F.7.

\begin{table}
\begin{tabular}{c l|c c|c c|c c} \hline \hline Category & Model & SP\({}_{\text{sss}}\) & MCC & Diameter & SP\({}_{\text{ss}}\) & ECC & Average \\ \hline \hline Baseline & h=1,n=5 & 328.014 & 39.4772 & 189.577 & 324.033 & 219.746 & 220.169 \\ \hline MeGraph & h=5,n=5,\(\eta_{c}\)=0.3,\(\tau_{c}\)=4 & **23.8963** & **16.8321** & **19.2185** & **14.9676** & **44.9234** & **23.9676** \\ \hline Graph-UNets & h=5,n=9,\(\eta_{c}\)=0.3,\(\tau_{c}\)=4 & 101.009 & 30.3711 & 39.8708 & 100.070 & 75.1185 & 69.2879 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on Graph Theory Benchmark (large size).

\begin{table}
\begin{tabular}{c l|c c} \hline \hline Methods & Use PE & Peptide-func \(\uparrow\) & Peptide-struct \(\downarrow\) \\ \hline GCN [17] & & 59.30 \(\pm\)0.23 & 0.3496 \(\pm\)0.0013 \\ GINE [17] & & 55.43 \(\pm\)0.78 & 0.3547 \(\pm\)0.0045 \\ GatedGCN [17] & & 58.64 \(\pm\)0.77 & 0.3420 \(\pm\)0.0013 \\ GatedGCN+RWSE [17] & ✓ & 60.69 \(\pm\)0.35 & 0.3357 \(\pm\)0.0006 \\ GatedGCN+RWSE+VN [10] & ✓ & 66.85 \(\pm\)0.62 & 0.2529 \(\pm\)0.0009 \\ Transformer+LapPE [17] & ✓ & 63.26 \(\pm\)1.26 & 0.2529 \(\pm\)0.0016 \\ SAN+LapPE [17] & ✓ & 63.84 \(\pm\)1.21 & 0.2683 \(\pm\)0.0043 \\ SAN+RWSE [17] & ✓ & 64.39 \(\pm\)0.75 & 0.2545 \(\pm\)0.0012 \\ GPS [46] & ✓ & 65.35 \(\pm\)0.41 & 0.2500 \(\pm\)0.0005 \\ MGT+WavePE [43] & ✓ & 68.17 \(\pm\)0.64 & **0.2453**\(\pm\)0.0025 \\ GNN-AK+ [27] & & 64.80 \(\pm\)0.89 & 0.2736 \(\pm\)0.0007 \\ SUN [27] & & 67.30 \(\pm\)0.78 & 0.2498 \(\pm\)0.0008 \\ GraphTrans+PE [27] & ✓ & 63.13 \(\pm\)0.39 & 0.2777 \(\pm\)0.0025 \\ GINE+PE [27] & ✓ & 64.05 \(\pm\)0.77 & 0.2780 \(\pm\)0.0021 \\ GINE-MLP-Mixer+PE [27] & ✓ & 69.21 \(\pm\)0.54 & 0.2485 \(\pm\)0.0004 \\ \hline MeGraph (h=9,n=1) & & 67.52 \(\pm\)0.78 & 0.2557 \(\pm\)0.0011 \\ MeGraph (h=9,n=4) & & **69.45 \(\pm\)0.77** & 0.2507 \(\pm\)0.0009 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on LRGB [17], numbers are taken from corresponding papers. All methods use around 500K parameters for a fair comparison. Message-passing-based models have 5 layers, while the Transformer-based models have 4 layers [17]. PE indicates positional encoding, which makes it easier to distinguish different nodes.

the \(h\)=5, \(n\)=1 baseline outperforms the \(h\)=1, \(n\)=10 baseline, indicating that adopting a multi-scale graph hierarchy is crucial in these tasks. The improvement is also substantial when compared with our reproduced Graph-UNets using S-EdgePool ([MeGraph] 0.675 vs. [Graph UNets] 1.581). The improvements are more significant when the size of graphs becomes larger (as shown in Table 2). These results collectively demonstrate the superior ability of MeGraph to capture LRIs.

Furthermore, we evaluated MeGraph model and compared it with other recent methods on the Long Range Graph Benchmark (LRGB) [17] that contains real-world tasks that require capturing LRIs. As depicted in Table 3, the \(h\)=9, \(n\)=4 variant of MeGraph achieves superior results on the _Peptide-func_ task, and comparable performance on the _Peptide-struct_ task, relative to state-of-the-art models. It is worth noting that the \(n=1\) variant already surpasses other methods except the recent MLP-Mixer [27] in the _Peptide-func_ task.

### Generality of MeGraph

To verify the generality of MeGraph model, we evaluate MeGraph on widely adopted GNN Benchmark [15], Open Graph Benchmark [28] and TU Dataset [42]. Results on TU Datasets are available in App. F.3. In addition to the standard model that shares hyper-parameters in similar tasks, we also report MeGraph\({}_{\mathrm{best}}\) with specifically tuned hyper-parameters for each task.

**GNN Benchmark**. We experiment on chemical data (ZINC and AQSOL), image data (MNIST and CIFAR10) and social network data (PATTERN and CLUSTER). As shown in Table 4, MeGraph outperforms the three baselines by a large margin, indicating the effectiveness of repeating both the local and hierarchical information aggregation.

**Open Graph Benchmark (OGB)**. We choose 10 datasets related to molecular graphs from the graph prediction tasks of OGB. The task of all datasets is to predict some properties of molecule graphs

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline Model & ZINC \(\downarrow\) & AQSOL \(\downarrow\) & MNIST \(\uparrow\) & CIFAR10 \(\uparrow\) & PATTERN \(\uparrow\) & CLUSTER \(\uparrow\) \\ \hline GCN\({}^{\dagger}\) & 0.416 \(\pm\)0.006 & 1.372 \(\pm\)0.002 & 90.120 \(\pm\)0.145 & 54.142 \(\pm\)0.394 & 85.498 \(\pm\)0.015 & 47.828 \(\pm\)1.510 \\ GIN\({}^{\dagger}\) & 0.387 \(\pm\)0.015 & 1.894 \(\pm\)0.024 & **96.485 \(\pm\)0.026** & 52.555 \(\pm\)1.527 & 85.590 \(\pm\)0.011 & 58.384 \(\pm\)0.236 \\ GAT\({}^{\dagger}\) & 0.475 \(\pm\)0.007 & 1.441 \(\pm\)0.023 & 95.535 \(\pm\)0.205 & 64.223 \(\pm\)0.455 & 75.824 \(\pm\)1.823 & 57.732 \(\pm\)0.323 \\ GatedGCN\({}^{\dagger}\) & 0.435 \(\pm\)0.011 & 1.352 \(\pm\)0.034 & 97.340 \(\pm\)0.143 & 67.312 \(\pm\)0.311 & 84.480 \(\pm\)0.122 & 60.404 \(\pm\)0.419 \\ \hline Graph-UNets & 0.332 \(\pm\)0.010 & 1.063 \(\pm\)0.018 & 97.130 \(\pm\)0.227 & 68.567 \(\pm\)0.339 & 86.257 \(\pm\)0.078 & 50.371 \(\pm\)0.243 \\ MeGraph (\(h\)=1) & 0.323 \(\pm\)0.002 & 1.075 \(\pm\)0.007 & 97.570 \(\pm\)0.168 & 69.890 \(\pm\)0.209 & 84.845 \(\pm\)0.021 & 58.178 \(\pm\)0.079 \\ MeGraph (\(n\)=1) & 0.310 \(\pm\)0.005 & 1.038 \(\pm\)0.018 & 96.867 \(\pm\)0.167 & 68.522 \(\pm\)0.239 & 85.507 \(\pm\)0.402 & 50.396 \(\pm\)0.082 \\ MeGraph & 0.260 \(\pm\)0.005 & **1.002** \(\pm\)0.021 & **97.860** \(\pm\)0.098 & **69.925** \(\pm\)0.631 & 86.507 \(\pm\)0.067 & 68.603 \(\pm\)0.101 \\ MeGraph\({}_{\mathrm{best}}\) & **0.202** \(\pm\)0.007 & **1.002** \(\pm\)0.021 & **97.860** \(\pm\)0.098 & **69.925** \(\pm\)0.631 & **86.732** \(\pm\)0.023 & **68.610** \(\pm\)0.164 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results on GNN benchmark. Regression tasks are colored with _blue_. \(\downarrow\) indicates that smaller numbers are better. Classification tasks are colored with _green_. \(\uparrow\) indicates that larger numbers are better. Darker colors indicate better performance. \(\downarrow\) denotes the results are reported in [15].

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline Model & molliv \(\uparrow\) & molliv \(\uparrow\) & mollabc \(\uparrow\) & mollbbbp \(\uparrow\) & mollcintox \(\uparrow\) & mollsider \(\uparrow\) \\ \hline \hline GCN\({}^{\dagger}\) & 76.06 \(\pm\)0.97 & 79.15 \(\pm\)1.44 & 68.87 \(\pm\)1.51 & 91.30 \(\pm\)1.73 & 59.60 \(\pm\)1.77 \\ GIN\({}^{\dagger}\) & 75.58 \(\pm\)1.40 & 72.97 \(\pm\)4.00 & 68.17 \(\pm\)1.48 & 88.14 \(\pm\)2.51 & 57.60 \(\pm\)1.40 \\ \hline Graph-UNets & **79.48**\(\pm\)1.06 & 81.09 \(\pm\)1.66 & **71.10**\(\pm\)0.52 & 91.67 \(\pm\)1.69 & 59.38 \(\pm\)0.63 \\ MeGraph (\(h\)=1) & 78.54 \(\pm\)1.14 & 71.77 \(\pm\)2.15 & 67.56 \(\pm\)1.11 & 89.77 \(\pm\)3.48 & 58.28 \(\pm\)0.51 \\ MeGraph (\(n\)=1) & 78.56 \(\pm\)1.02 & 79.72 \(\pm\)1.24 & 67.34 \(\pm\)0.98 & 91.07 \(\pm\)2.21 & 58.08 \(\pm\)0.59 \\ MeGraph & 77.20 \(\pm\)0.88 & 78.52 \(\pm\)2.51 & 69.57 \(\pm\)2.33 & 92.04 \(\pm\)2.19 & 59.01 \(\pm\)1.45 \\ MeGraph\({}_{\mathrm{best}}\) & **79.20**\(\pm\)1.80 & **83.52**\(\pm\)0.47 & 69.57 \(\pm\)2.33 & **92.06**\(\pm\)1.32 & **63.43**\(\pm\)1.10 \\ \hline \hline Model & moltox21 \(\uparrow\) & moltoxcast \(\uparrow\) & molesol \(\downarrow\) & molfreesolv \(\downarrow\) & mollipo \(\downarrow\) \\ \hline \hline GCN\({}^{\dagger}\) & 75.29 \(\pm\)0.69 & 63.54 \(\pm\)0.42 & 1.114 \(\pm\)0.03 & 2.640 \(\pm\)0.23 & 0.797 \(\pm\)0.02 \\ GIN\({}^{\dagger}\) & 74.91 \(\pm\)0.51 & 63.41 \(\pm\)0.74 & 1.173 \(\pm\)0.05 & 2.755 \(\pm\)0.34 & 0.757 \(\pm\)0.01 \\ \hline Graph-UNets & **77.85**\(\pm\)0.81 & 66.49 \(\pm\)0.45 & 1.002 \(\pm\)0.04 & 1.885 \(\pm\)0.07 & 0.716 \(\pm\)0.01 \\ MeGraph (\(h\)=1) & 75.89 \(\pm\)0.45 & 64.49 \(\pm\)0.46 & 1.079 \(\pm\)0.02 & 2.017 \(\pm\)0.08 & 0.768 \(\pm\)0.00 \\ MeGraph (\(n\)=1) & 77.01 \(\pm\)0.93 & 66.89 \(\pm\)1.21 & 0.896 \(\pm\)0.04 & 1.892 \(\pm\)0.06 & 0.730 \(\pm\)0.01 \\ MeGraph & **78.11**\(\pm\)0.47 & 67.67 \(\pm\)0.53 & 0.886 \(\pm\)0.02 & **1.876**\(\pm\)0.05 & 0.726 \(\pm\)0.00 \\ MeGraph\({}_{\mathrm{best}}\) & **78.11**\(\pm\)0.47 & **67.90**\(\pm\)0.19 & **0.867**\(\pm\)0.02 & **1.876**\(\pm\)0.05 & **0.688**\(\pm\)0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results on OGB-G. \(\dagger\) indicates that the results are reported in [28].

based on their chemical structures. As shown in Table 5, MeGraph outperforms the \(h\)=1 baseline by a large margin, suggesting that building a graph hierarchy is also essential in molecule graphs. The performance of MeGraph, \(n=1\) baseline, and the reproduced Graph U-Nets are comparable. This observation may be because the information obtained from multi-hop neighbors offers only marginal improvements compared to the information aggregated hierarchically.

### Ablation Study

**Hierarchy vs. Locality.** We study the impact of the height \(h\) and the number of Mee layers \(n\) on four synthetic datasets introduced in [61], which are BAShape, BACommunity, TreeCycle, and TreeGrid. Each dataset contains one graph formed by attaching multiple motifs to a base graph. The motif can be a 'house'-shaped network (BAShape, BACommunity), six-node cycle (TreeCycle), or 3-by-3 grid (TreeGrid). The task is to identify the nodes of the motifs in the fused graph.

As shown in Fig. 4, we can observe clear improvements in performance as the height \(h\) and the number of layers \(n\) increase when using EdgePool. Although increasing height shows better improvements, both hierarchy and locality are indispensable in TreeCycle and TreeGrid tasks. In App. F.5, we show that the conclusion also holds on BAShape and BACommunity datasets, except that the accuracy is already saturated with height \(h=2\). The significance of integrating locality with hierarchy is also demonstrated in the CLUSTER task, as presented in Table 4. Here, MeGraph reaches 68.6% accuracy, which is markedly higher than the 50.4% accuracy achieved by both the \(n\)=1 baseline and Graph-UNets.

**Varying the Pooling Method.** We varied the graph pooling with two non-learnable pooling methods, Louvain [8] and Random. On the TreeCycle and TreeGrid datasets, as depicted in Fig. 4, Louvain achieves comparable accuracy to EdgePool, while MeGraph using random pooling matches the performance of the \(h=1\) variant of MeGraph. These observations indicate that: 1) a well-structured hierarchy crafted by suitable graph pooling methods enhances MeGraph's ability to harness hierarchical information; and 2) despite the disruption from a randomly constructed hierarchy, the MeGraph model effectively taps into the local structure of the original graph (also discussed in App. D.2).

We further studied the impact of the node pooling ratio \(\eta_{v}\) and the maximum cluster size \(\tau_{c}\) in S-EdgePool by perturbing these parameters. As indicated in Table 1, the best variant (\(\eta_{v}\)=0.3,\(\tau_{c}\)=4) achieved a regression error about 3x smaller (0.675) compared to the original EdgePool (\(\tau_{c}\)=2 with an error of 2.215). This suggests the benefit of having a flexible pooling stride. Moreover, the mega graph produced by S-EdgePool can vary significantly with different parameters. In App. F.2, we visualized the resulting graph hierarchy to illustrate the difference between different S-EdgePool variants. However, irrespective of the parameter set used, MeGraph consistently outperforms \(h=1\) baselines. This suggests that MeGraph exhibits robustness against different architectures of the mega graph.

**Varying GN Block.** We varied the aggregation function of the GN block as attention (w/ ATT) and gated function (w/ GATE). We observe similar results as in Sec. 4.2 and 4.3, verifying the robustness of MeGraph towards different types of GN blocks. Detailed results can be found in Tables 16, 17 and 18 in App. F.6.

Figure 4: Node classification accuracy (averaged over 10 random repetitions) for MeGraph on TreeCycle (left) and TreeGrid (right) datasets by varying the number of Mee layers \(n\), the height \(h\), and the graph pooling methods (EdgePool, Louvain, Random). Clear gaps can be observed among heights 1, 2, and 3 for EdgePool and Louvain [8] methods, while the accuracy is almost invariant among different heights for randomized pooling.

**Changing cross update function (X-UPD).** The unpool operation is frequently used by other hierarchical architectures that build upon graph pooling. As illustrated in Table 1, we substituted the _X-Conv_ implementation with the _X-Pool_ implementation of X-UPD, which resulted in a performance decline from 0.675 to 1.203 (smaller is better). This finding suggests that other hierarchical GNNs might also benefit from replacing the unpool operation with a convolution over the inter-graph edges.

**Disabling bidirectional pathway.** We verify the effectiveness of the bidirectional pathway design by replacing steps 2 and 3 of the Mee layer as a standard message-passing along the inter-graph edges (denoted w/o pw). As shown in Table 1, the performance degrades from 0.675 to 1.065 (smaller is better), which indicates the contribution of the bidirectional pathway.

## 5 Related Work

**Long-Range Interactions (LRIs).** Various methods have been proposed to address the issue of LRIs, including making the GNNs deeper [40]. Another way is to utilize attention and gating mechanism, including GAT [52], jumping knowledge (JK) network [59], incorporating Transformer structures [35, 60, 56, 46] and MLP-Mixer [27]. Another line of research focuses on multi-scale graph hierarchy using graph pooling methods [62, 22, 47], or learning representation based on subgraphs [5, 66]. Recently, Long Range Graph Benchmark [17] has been proposed to better evaluate models' ability to capture LRIs.

**Feature Pyramids and Multi-Scale Feature Fusion.** Multi-scale feature fusion methods on image feature pyramids have been widely studied in computer vision literature, including the U-Net [48], FPN [39], UNet++ [67], and some recent approaches [63, 41, 38, 37]. HRNet [53] is a similar method compared to MeGraph. HRNet alternates between multi-resolution convolutions and multi-resolution fusion by stridden convolutions. However, the above methods are developed for image data. The key difference compared to these approaches is that the multi-scale feature fusion in MeGraph is along the inter-graph edges, which is not as well structured as the pooling operation in image data. For graph networks, the GraphFPN [65] builds a graph feature pyramid according to the image feature pyramid and superpixel hierarchy. It applies GNN layers on the hierarchical graph to exchange information within the graph pyramid. Existing works [22, 20, 47, 30] have also explored similar ideas in graph-structured data. Our approach aligns with the broader concept of multi-scale information fusion, but it is the first method that builds a mega graph using graph pooling operations and alternates local and hierarchical information aggregation.

**Graph Pooling Methods.** Graph pooling is an important part of hierarchical graph representation learning. There have been some traditional graph pooling methods like METIS [32] in early literature. Recently, many learning-based graph pooling methods have been proposed, including the DiffPool [62], TopKPool [22], SAG pool [36], EdgePool [13], MinCutPool [7], Structpool [64], and MEWISPool [44], etc. In this work, we utilize S-EdgePool improved from EdgePool to build the mega graph, while this module can be substituted with any of the above-mentioned pooling methods.

**Graph Neural Network (GNN) Layers.** The GNN layer is the core module of graph representation learning models. Typical GNNs include the GCN [34], GraphSage [24], GAT [52, 9], GIN [58], PNA [12]. MeGraph adopts the full GN block [6] by removing part of links in the module as an elementary block, and similarly this can be replaced by any one of the popular GNN blocks.

## 6 Limitations and Future Work

The MeGraph model suffers from some limitations. The introduced mega graph architecture inevitably increases both the number of trainable parameters and tuneable hyper-parameters. The flexible choices of many modules in MeGraph post burdens on tuning the architecture on specific datasets. For future research, MeGraph encourages new graph pooling methods to yield edge features in addition to node features, when mapping the input graph to the pooled graph. It is also possible to improve MeGraph using adaptive computational steps [49]. Another direction is to apply some expressive but computationally expensive models like Transformers [51] and Neural Logic Machines [14, 57] (only) over the pooled small-sized graphs.

### Acknowledgements

We thank Qizheng He for the discussions on the Graph Theory Dataset, Kirsty You for improving the illustrations, and Jiafei Lv and the Maddison group for their helpful discussions or feedback on the paper draft; This work was supported by the STI 2030-Major Projects under Grant 2021ZD0201404.

## References

* Adelson et al. [1984] Edward H Adelson, Charles H Anderson, James R Bergen, Peter J Burt, and Joan M Ogden. Pyramid methods in image processing. _RCA engineer_, 29(6):33-41, 1984.
* Albert and Barabasi [2002] Reka Albert and Albert-Laszlo Barabasi. Statistical mechanics of complex networks. _Reviews of modern physics_, 74(1):47, 2002.
* Alon and Yahav [2020] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In _International Conference on Learning Representations_, 2020.
* Ba et al. [2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* Baek et al. [2020] Jinheon Baek, Minki Kang, and Sung Ju Hwang. Accurate learning of graph representations with graph multiset pooling. In _International Conference on Learning Representations_.
* Battaglia et al. [2018] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. _arXiv preprint arXiv:1806.01261_, 2018.
* Bianchi et al. [2020] Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. Spectral clustering with graph neural networks for graph pooling. In _International Conference on Machine Learning_, pages 874-883. PMLR, 2020.
* Blondel et al. [2008] Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in large networks. _Journal of statistical mechanics: theory and experiment_, 2008(10):P10008, 2008.
* Brody et al. [2021] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In _International Conference on Learning Representations_, 2021.
* Cai et al. [2023] Chen Cai, Truong Son Hy, Rose Yu, and Yusu Wang. On the connection between mpmn and graph transformer. _arXiv preprint arXiv:2301.11956_, 2023.
* Chen et al. [2019] Ting Chen, Song Bian, and Yizhou Sun. Are powerful graph neural nets necessary? a dissection on graph classification. _arXiv preprint arXiv:1905.04579_, 2019.
* Corso et al. [2020] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal neighbourhood aggregation for graph nets. _Advances in Neural Information Processing Systems_, 33:13260-13271, 2020.
* Diehl et al. [2019] Frederik Diehl, Thomas Brunner, Michael Truong Le, and Alois Knoll. Towards graph pooling by edge contraction. In _ICML 2019 workshop on learning and reasoning with graph-structured data_, 2019.
* Dong et al. [2018] Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic machines. In _International Conference on Learning Representations_, 2018.
* Dwivedi et al. [2020] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. _arXiv preprint arXiv:2003.00982_, 2020.
* Dwivedi et al. [2021] Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Graph neural networks with learnable structural and positional representations. In _International Conference on Learning Representations_, 2021.
* Dwivedi et al. [2022] Vijay Prakash Dwivedi, Ladislav Rampasek, Mikhail Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* Erdos et al. [1960] Paul Erdos, Alfred Renyi, et al. On the evolution of random graphs. _Publ. Math. Inst. Hung. Acad. Sci_, 5(1):17-60, 1960.

* [19] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* [20] Matthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical inter-message passing for learning on molecular graphs. _arXiv preprint arXiv:2006.12179_, 2020.
* [21] Bernard A Galler and Michael J Fisher. An improved equivalence algorithm. _Communications of the ACM_, 7(5):301-303, 1964.
* [22] Hongyang Gao and Shuiwang Ji. Graph u-nets. In _international conference on machine learning_, pages 2083-2092. PMLR, 2019.
* [23] Daniele Grattarola, Daniele Zambon, Filippo Maria Bianchi, and Cesare Alippi. Understanding pooling in graph neural networks. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* [24] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30, 2017.
* [25] Jessica B Hamrick, Kelsey R Allen, Victor Bapst, Tina Zhu, Kevin R McKee, Joshua B Tenenbaum, and Peter W Battaglia. Relational inductive bias for physical construction in humans and machines. _arXiv preprint arXiv:1806.01203_, 2018.
* [26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [27] Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. A generalization of vit/mlp-mixer to graphs, 2022.
* [28] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_, 33:22118-22133, 2020.
* [29] Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, and Fredo Durand. Taichi: a language for high-performance computation on spatially sparse data structures. _ACM Transactions on Graphics (TOG)_, 38(6):201, 2019.
* [30] Truong Son Hy and Risi Kondor. Multiresolution equivariant graph variational autoencoder. _Machine Learning: Science and Technology_, 4(1):015031, 2023.
* [31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning_, pages 448-456. PMLR, 2015.
* [32] George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs. _SIAM Journal on scientific Computing_, 20(1):359-392, 1998.
* [33] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR (Poster)_, 2015.
* [34] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. 2016.
* [35] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Letourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. _Advances in Neural Information Processing Systems_, 34:21618-21629, 2021.
* [36] Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In _International conference on machine learning_, pages 3734-3743. PMLR, 2019.
* [37] Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, Shaohua Tan, and Kuiyuan Yang. Gated fully fusion for semantic segmentation. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 11418-11425, 2020.

* [38] Di Lin, Dingguo Shen, Siting Shen, Yuanfeng Ji, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Zigzagnet: Fusing top-down and bottom-up context for object segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7490-7499, 2019.
* [39] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2117-2125, 2017.
* [40] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 338-348, 2020.
* [41] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8759-8768, 2018.
* [42] Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. _arXiv preprint arXiv:2007.08663_, 2020.
* [43] Nhat Khang Ngo, Truong Son Hy, and Risi Kondor. Multiresolution graph transformers and wavelet positional encoding for learning long-range and hierarchical structures. _The Journal of Chemical Physics_, 159(3), 2023.
* [44] Amirhossein Nouranizadeh, Mohammadjavad Matinkia, Mohammad Rahmati, and Reza Safabakhsh. Maximum entropy weighted independent set pooling for graph neural networks. _arXiv preprint arXiv:2107.01410_, 2021.
* [45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [46] Ladislav Rampasek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. _arXiv preprint arXiv:2205.12454_, 2022.
* [47] Ladislav Rampasek and Guy Wolf. Hierarchical graph neural nets can capture long-range interactions. In _2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP)_, pages 1-6. IEEE, 2021.
* [48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _International Conference on Medical image computing and computer-assisted intervention_, pages 234-241. Springer, 2015.
* [49] Hao Tang, Zhiao Huang, Jiayuan Gu, Bao-Liang Lu, and Hao Su. Towards scale-invariant graph-related problem solving by iterative homogeneous gnns. _Advances in Neural Information Processing Systems_, 33:15811-15822, 2020.
* [50] Robert E Tarjan and Jan Van Leeuwen. Worst-case analysis of set union algorithms. _Journal of the ACM (JACM)_, 31(2):245-281, 1984.
* [51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [52] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _International Conference on Learning Representations_, 2018.
* [53] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. _IEEE transactions on pattern analysis and machine intelligence_, 43(10):3349-3364, 2020.

* [54] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang. Deep graph library: A graph-centric, highly-performant package for graph neural networks. _arXiv preprint arXiv:1909.01315_, 2019.
* [55] Duncan J WATTS. Networks, dynamics and the small world phenomenon. _American Journal of Sociology_, 105(2):50-59, 2003.
* [56] Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph E Gonzalez, and Ion Stoica. Representing long-range context for graph neural networks with global attention. _Advances in Neural Information Processing Systems_, 34:13266-13279, 2021.
* [57] Guangxuan Xiao, Leslie Pack Kaelbling, Jiajun Wu, and Jiayuan Mao. Efficient training and inference of hypergraph reasoning networks, 2022.
* [58] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2018.
* [59] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In _International conference on machine learning_, pages 5453-5462. PMLR, 2018.
* [60] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? _Advances in Neural Information Processing Systems_, 34:28877-28888, 2021.
* [61] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks. _Advances in neural information processing systems_, 32, 2019.
* [62] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. _Advances in neural information processing systems_, 31, 2018.
* [63] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. Deep layer aggregation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2403-2412, 2018.
* [64] Hao Yuan and Shuiwang Ji. Structpool: Structured graph pooling via conditional random fields. In _Proceedings of the 8th International Conference on Learning Representations_, 2020.
* [65] Gangming Zhao, Weifeng Ge, and Yizhou Yu. Graphfpn: Graph feature pyramid network for object detection. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 2763-2772, 2021.
* [66] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any gnn with local structure awareness. In _International Conference on Learning Representations_.
* [67] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: A nested u-net architecture for medical image segmentation. In _Deep learning in medical image analysis and multimodal learning for clinical decision support_, pages 3-11. Springer, 2018.

## Appendix

### Table of Contents

* A Code and Reproducibility
* B Dataset Details
* B.1 Dataset Statistics and Metrics
* B.2 Graph Theory Benchmark
* C Method Details
* C.1 Cross Update Function
* C.2 S-EdgePool
* C.3 GFuN
* C.4 Encoder and Decoder
* C.5 Architecture Variants
* D Theoretical Discussions
* D.1 Smaller Number of Aggregation Steps for Capturing Long-Range Interactions
* D.2 MeGraph can degenerate to standard GNNs
* E Implementation and Training Details
* F Additional Experiment Results
* F.1 Experimental Protocol
* F.2 Visualization
* F.3 Other Real-World Datasets
* F.4 GFuN
* F.5 Synthetic Datasets
* F.6 Varying GN block
* F.7 Graph Theory Dataset
Code and Reproducibility

The code along with the configuration of hyper-parameters to reproduce our experiments can be found at [https://github.com/dhh1995/MeGraph](https://github.com/dhh1995/MeGraph).

We set the random seed as 2022 for all experiments to enable reproducible results. We provide dataset statistics in Table 6 and details for the proposed graph theory benchmark in Appendix B.2. Details of the hyper-parameters are reported in Table 9. Configuration of all hyper-parameters and the command lines to reproduce the experiments have been included in the code repository.

## Appendix B Dataset Details

### Dataset Statistics and Metrics

We provide the statistics of all datasets used in our experiments in Table 6 and introduce the evaluation metrics for each dataset.

For Synthetic datasets, we use classification accuracy (ACC) as the evaluation metric. We use Mean Square Error (MSE) as the evaluation metric for all datasets in our Graph Theory Benchmark. For GNN Benchmark, we follow the original work [15] for evaluation, i.e., Mean Absolute Error (MAE) for ZINC and AQSOL, classification accuracy for MNIST and CIFAR10, and balanced classification accuracy for PATTERN and CLUSTER. For OGB Benchmark, we follow the original work [28] and use the ROC-AUC for classification tasks and Root Mean Square Error (RMSE) for regression tasks. For TU datasets, we follow the setting used by [11] and use classification accuracy as the evaluation metric.

### Graph Theory Benchmark

In this section, we provide the details about the tasks and how the graph features and the labels are generated given a base graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\):

* Single source single destination shortest path (SPssd): a source node \(s\in\mathcal{V}\) and a destination node \(t\in\mathcal{V}\) are selected uniform randomly. The feature of each node \(v\) contains three numbers: (1, whether the node \(v\) is \(s\), whether the node \(v\) is \(t\)). The label of a graph is the length of the shortest path from \(s\) to \(t\).
* A maximum connected component of the same color (MCC): each node of the graph is colored with one of three colors. The feature for each node is the one-hot representation of its color. The label of graph is the size of the largest connected component of the same color for each color.
* Graph diameter (Diameter): the label of the graph is the diameter of the graph. The diameter of a graph \(\mathcal{G}\) is the maximum of the set of shortest path distances between all pairs of nodes in the graph. The feature of each node is a uniform number \(1\).
* Single source shortest path (SPss): a source node \(s\) is selected uniformly randomly. The feature of each node contains two numbers: (1, whether the node is \(s\)). The label of each node is the length of the shortest path from \(s\) to this node.
* Graph eccentricity (ECC): the label of each node \(v\) is node's eccentricity in the graph, which is the maximum distance from \(v\) to the other nodes. The feature of each node is a uniform number \(1\).

For each task and graph generation method, We generate the dataset by the following steps:

* Sample \(N\) (number of nodes) from an interval with a total number of graphs. These numbers can be configured. For the medium size setting, the interval is \([20,50]\), a total of \(300\) graphs. For the large size setting, the interval is \([100,200]\), a total of \(500\) graphs.
* Use the graph generation method to generate a graph of \(N\) nodes.
* Create graph features and labels according to the task.

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline \multirow{2}{*}{Collection} & \multirow{2}{*}{Dataset} & \# & Avg & Avg & \# & \# & \# & \multirow{2}{*}{Task} & \multirow{2}{*}{Reg.} \\  & & Graphs & \# & \# & Node & Edge & & & \\  & & Nodes & Edges & Feat & Feat & & & \\ \hline \hline Synthetic & BaShape & 1 & 700 & 1761 & 1 & - & 4 & Trans-Node & \\ Synthetic & BaCommunity & 1 & 1400 & 3872 & 10 & - & 8 & Trans-Node & \\ Synthetic & TreeCycle & 1 & 871 & 970 & 1 & - & 2 & Trans-Node & \\ Synthetic & TreeGrid & 1 & 1231 & 1705 & 1 & - & 2 & Trans-Node & \\ \hline GraphTheory & SP\({}_{\text{snsd}}\) & 300 & 35.0 & - & 3 & - & - & Graph & ✓ \\ GraphTheory & Diameter & 300 & 35.0 & - & 1 & - & - & Graph & ✓ \\ GraphTheory & MCC & 300 & 35.0 & - & 3 & - & - & Graph & ✓ \\ GraphTheory & SP\({}_{\text{ss}}\) & 300 & 35.0 & - & 2 & - & - & Node & ✓ \\ GraphTheory & ECC & 300 & 35.0 & - & 1 & - & - & Node & ✓ \\ \hline LRGB & Peptides-func & 15535 & 150.94 & 307.30 & 9 & 3 & 10 & Graph & \\ LRGB & Peptides-struct & 15535 & 150.94 & 307.30 & 9 & 3 & - & Graph & ✓ \\ \hline GNNBenchmark & ZINC & 12000 & 23.16 & 49.83 & 28 & 4 & 2 & Graph & ✓ \\ GNNBenchmark & AOSOL & 9823 & 17.57 & 35.76 & 65 & 5 & 2 & Graph & ✓ \\ GNNBenchmark & MNIST & 70000 & 70.57 & 564.53 & 3 & 1 & 10 & Graph & \\ GNNBenchmark & CIFAR10 & 60000 & 117.63 & 941.07 & 5 & 1 & 10 & Graph & \\ GNNBenchmark & PATTERN & 14000 & 118.89 & 6078.57 & 3 & - & 2 & Node & \\ GNNBenchmark & CLUSTER & 12000 & 117.20 & 4301.72 & 7 & - & 6 & Node & \\ \hline OGB Graph & molhiv & 41127 & 25.51 & 80.45 & 9 & 3 & 2 & Graph & \\ OGB Graph & molbapp & 1513 & 34.09 & 107.81 & 9 & 3 & 2 & Graph & \\ OGB Graph & molbkopp & 2039 & 24.06 & 75.97 & 9 & 3 & 2 & Graph & \\ OGB Graph & molclintox & 1477 & 26.16 & 81.93 & 9 & 3 & 2 & Graph & \\ OGB Graph & molstder & 1427 & 33.64 & 104.36 & 9 & 3 & 2 & Graph & \\ OGB Graph & moltox21 & 7831 & 18.57 & 57.16 & 9 & 3 & 2 & Graph & \\ OGB Graph & moltoxcast & 8576 & 18.78 & 57.30 & 9 & 3 & 2 & Graph & \\ OGB Graph & molresol & 1128 & 13.29 & 40.64 & 9 & 3 & - & Graph & ✓ \\ OGB Graph & molfreesolv & 642 & 8.7 & 25.50 & 9 & 3 & - & Graph & ✓ \\ OGB Graph & mollipo & 4200 & 27.04 & 86.04 & 9 & 3 & - & Graph & ✓ \\ \hline TU & MUTAG & 188 & 17.93 & 19.79 & 7 & - & 3 & Graph & \\ TU & NCI1 & 4110 & 29.87 & 32.30 & 37 & - & 2 & Graph & \\ TU & PROTEINS & 1113 & 39.06 & 72.82 & 4 & - & 2 & Graph & \\ TU & D\&D & 1178 & 284.32 & 715.66 & 89 & - & 2 & Graph & \\ TU & ENZYMES & 600 & 32.63 & 62.14 & 21 & - & 6 & Graph & \\ TU & IMDB-B & 1000 & 19.77 & 96.53 & 10 & - & 2 & Graph & \\ TU & IMDB-M & 1500 & 13.00 & 65.94 & 10 & - & 3 & Graph & \\ TU & RE-B & 2000 & 429.63 & 497.75 & 10 & - & 2 & Graph & \\ TU & RE-M5K & 4999 & 508.52 & 594.87 & 10 & - & 5 & Graph & \\ TU & RE-M12K & 11929 & 391.41 & 456.89 & 10 & - & 11 & Graph & \\ \hline \hline \end{tabular}
\end{table}
Table 6: The statistics of the datasets used in experiments. Some statistics (like the average number of edges) of the Graph Theory datasets may vary depending on different random graph generation methods. The regression tasks are marked with ✓ in a separate column. The tasks of 4 synthetic datasets are transductive, where the same graph is used for both training and testing. We do not use the node labels as features during the training time. The train-val-test split is over nodes. All other datasets in the table are inductive, where the testing graphs do not occur during training, and the train-val-test split is over graphs.

We then provide the details about the random graph generation methods we used to create our Graph Theory datasets.

Following [12], we continue to use undirected and unweighted graphs from a wide variety of types. We inherit their 10 random graph generation methods and quote their descriptions here for completeness (the percentage after the name is the approximate proportion of such graphs in the mixture setting).

* **Erdos-Renyi (ER)** (20%) [18]: with a probability of presence for each edge equal to \(p\), where \(p\) is independently generated for each graph from \(\mathcal{U}[0,1]\)
* **Barabasi-Albert (BA)** (20%) [2]: the number of edges for a new node is \(k\), which is taken randomly from \(\{1,2,\ldots,N-1\}\) for each graph
* **Grid** (5%): \(m\times k\) 2d grid graph with \(N=mk\) and \(m\) and \(k\) as close as possible
* **Caveman** (5%) [55]: with \(m\) cliques of size \(k\), with \(m\) and \(k\) as close as possible
* **Tree** (15%): generated with a power-law degree distribution with exponent 3
* **Ladder graphs** (5%)
* **Line graphs** (5%)
* **Star graphs** (5%)
* **Caterpillar graphs** (10%): with a backbone of size \(b\) (drawn from \(\mathcal{U}[1,N)\)), and \(N-b\) pendent vertices uniformly connected to the backbone
* **Lobster graphs** (10%): with a backbone of size \(b\) (drawn from \(\mathcal{U}[1,N)\)), \(p\) (drawn from \(\mathcal{U}[1,N-b]\) ) pendent vertices uniformly connected to the backbone, and additional \(N-b-p\) pendent vertices uniformly connected to the previous pendent vertices.

Additional, we add three more graph generation methods:

* **Cycle graphs**: A tree graph plus an additional edge. The graph is generated by first generating a cycle graph of size \(m=\operatorname{sample}(0.3N,0.6N)\). Then \(n-m\) remaining nodes are sampled to \(m\) parts, where \(i\)-th part represents the size of the tree hanging on the \(i\)-th node on the cycle. The trees are randomly generated with the given size.
* **Stochastic Block Model (SBM) graphs**: graphs with clusters. We randomly sample the size of each block to be random from (5, 15), and the probability of edge within the block to be random from (0.3, 0.5) and those for other edges to be random from (0.005, 0.01). To make all the tasks well-defined, we filtered out the unconnected graphs during the generation.
* **Geographic (Geo) graphs**: geographic threshold graphs, but with added edges via a minimum spanning tree algorithm, to ensure all nodes are connected. This graph generation method is introduced by [6] in their codebase 2. We use the geographic threshold \(\theta=200\) instead of the default value \(\theta=1000\). Footnote 2: [https://github.com/deepmind/graph_nets](https://github.com/deepmind/graph_nets), the shortest path demo

Note that we do not have randomization after the graph generation as in [12]. Therefore, very long diameter is preserved for some type of graphs.

## Appendix C Method Details

### Cross Update Function

The cross update function \((\mathbf{X}^{\prime}_{j},\hat{\mathbf{X}}^{\prime}_{j},\mathbf{X}^{\prime}_{j +1})=\texttt{X-UPD}(j,\mathbf{X}_{j},\hat{\mathbf{X}}_{j},\mathbf{X}_{j+1})\) perform information exchange in consecutive hierarchies.

The _X-Conv_ realization contains the following steps:

1. Merge the node features of \(\mathbf{X}_{j}\) and \(\mathbf{X}_{j+1}\) with the inter-graph feature \(\hat{\mathbf{X}}_{j}\), results in \(\bar{\mathbf{X}}_{j}\).
2. Apply GN blocks on inter-graph \(\hat{\mathcal{G}}_{j}\): \(\hat{\mathbf{X}}^{\prime}_{j}=\operatorname{GN}^{i,j}_{\operatorname{inter}}( \hat{\mathcal{G}}_{j},\hat{\mathbf{X}}_{j})\).
3. Retrieve \(\mathbf{X}^{\prime}_{j}\) and \(\mathbf{X}^{\prime}_{j+1}\) from the node features of inter-graph features \(\hat{\mathbf{X}}^{\prime}_{j}\).

### S-EdgePool

In this subsection, we introduce the details of S-EdgePool. We first introduce the score generation method, then give details about the SELECT, CONNECT, REDUCE and EXPAND functions, and lastly provide pseudocode of the algorithm.

#### c.2.1 Edge Score Generation

Both S-EdgePool and EdgePool methods compute a raw edge score \(\mathbf{r}_{k}\) for each edge \(k\) using a linear layer:

\[\mathbf{r}_{k}=\mathbf{W}\cdot(\mathbf{V}_{s_{k}}||\mathbf{V}_{t_{k}}||\mathbf{ E}_{k})+\mathbf{b}\]

where \(s_{k}\) and \(t_{k}\) are the source and target nodes of edge \(k\), \(\mathbf{V}\) is node features, \(\mathbf{E}\) is edge features, \(\mathbf{W}\) and \(\mathbf{b}\) are learned parameters. The raw edge scores are further normalized by a local softmax function over all edges of a node:

\[\mathbf{w}_{k}=\exp(\mathbf{r}_{k})/\sum_{k^{\prime},t_{k^{\prime}}=t_{k}}\exp( \mathbf{r}_{k^{\prime}}),\]

and biased by a constant \(0.5\)[13].

#### c.2.2 Select, Connect, Reduce and Expand

SELECT step.S-EdgePool shares the same computations as in EdgePool to generate learnable edge scores, as detailed above. Then, we use a clustering procedure to determine the subset of nodes to be reduced.

Let \(I_{\mathbf{v}}\) be the identifier of the cluster containing a set of nodes \(\mathbf{v}\). Initially, we let \(\mathbf{v}=\{v\}\) for every single node \(v\). A contraction of an edge merges a pair of nodes \((v,v^{\prime})\) connected by this edge (where \(v\in\mathbf{v}\), \(v^{\prime}\in\mathbf{v}^{\prime}\) and \(\mathbf{v}\neq\mathbf{v}^{\prime}\)), and thus unifies the cluster identifiers, i.e., \(I_{\mathbf{v}}=I_{\mathbf{v}^{\prime}}=I_{\mathbf{v}_{\text{merge}}}\) and \(\mathbf{v}_{\text{merge}}=\mathbf{v}\cup\mathbf{v}^{\prime}\). That is, once an edge connecting any pair of nodes from two distinct clusters is contracted, we merge the two clusters and unify their identifiers. Edges are visited sequentially by a decreasing order on the edge scores, and contractions are implemented if valid. We set the maximum size of the node clusters to be a parameter \(\tau_{c}\), where \(\tau_{c}=2\) degenerates to the case of EdgePool [13]. We further introduce the pooling ratio \(\eta_{v}\) to control the minimal number of remaining clusters after edge contractions to be \(N^{v}*\eta_{v}\). Contractions that violate the above two constraints are invalid and will be skipped. Both parameters control the number of nodes in the pooled graph. In our implementation, the cluster of nodes is dynamically maintained using the disjoint-set data structure [21].

Then each node cluster \(i\) collapses into a new node \(\tilde{v}\) of the pooled graph (i.e. \(S_{\tilde{v}}=\{v|I_{v}=i\}\)), with inter-graph edges connect the nodes in the cluster to the new node \(\tilde{v}\).

CONNECT step.The CONNECT function rebuilds the edge set \(\tilde{\mathcal{E}}\) between the nodes in \(\tilde{\mathcal{V}}\). As aforementioned, we build the pooled graph's nodes according to node clusters. We call this mapping function from node clusters to new nodes as _c2n_. After that, we build the pooled graph's edges following three steps: First, for all edges in the original graph, we find out the corresponding node cluster(s) of its two endpoints (using a disjoint-set's find index operation). Then, we find out the corresponding new nodes by using the mapping function \(n\). Last, we add a new edge between the new nodes.

REDUCE and EXPAND step.The REDUCE and EXPAND are generalized from the method mentioned in [13]. The REDUCE function computes new node features and edge features. We follow their method to compute new node features by taking the sum of the node features and multiplying it by the edge score. Specifically, we generalize the computation between two nodes to a node cluster. The node clusters are maintained with a disjoint-set data structure and a cluster \(S_{\tilde{v}}\) consists of \(|S_{\tilde{v}}|\) nodes. We define \(\mathcal{E}_{\tilde{v}}^{ds}\) as a set of \(|S_{\tilde{v}}|-1\) edges, where the edges are the selected edges to be contracted in the SELECT step. Then,

\[c_{\tilde{v}}= \frac{1+\sum_{e_{k}\in\mathcal{E}_{\tilde{v}}^{ds}}\mathbf{w}_{k}} {|S_{\tilde{v}}|}\] \[\mathbf{V}_{\tilde{v}}= c_{\tilde{v}}\sum_{v\in S_{\tilde{v}}}\mathbf{V}_{v}\]To integrate the edge features between two node clusters, we first find all the connected edges between the two node clusters (the edges between node clusters are edges that connect two nodes from different node clusters). Then, we use the sum of all the connected edges' features between the two node clusters as the new edge's features.

The \(\mathtt{EXPAND}\) function is also referred as _unpool_ operation. It computes node features of the input graph \(\mathbf{V}_{v}\) given the node features of the pooled graph \(\mathbf{V}_{\tilde{v}}\) as following:

\[\mathbf{V}_{v}=\frac{\mathbf{V}_{\tilde{v}}}{c_{\tilde{v}}}\]

#### c.2.3 Pseudo Code

The pseudo-code includes two parts, where Algorithm 1 describes how to maintain the clusters using a disjoint-set data structure, and Algorithm 2 describes the procedure of S-EdgePool that generates a pooled graph \(\tilde{\mathcal{G}}\) with configurable node pooling ratio \(\eta_{v}\) and maximum of cluster sizes \(\tau_{c}\).

```
function InitializeDisjointSet(graph \(\mathcal{G}(\mathcal{V},\mathcal{E})\)) for\(v\in\mathcal{V}\)do \(index[v]=v\) {the identifier of the cluster the node \(v\) belongs to} endfor endfunction functionFindIndex(node \(v\)) if\(index[v]=v\)then return\(v\) else \(index[v]\leftarrow\)FindIndex(\(index[v]\)) return\(index[v]\) endif endfunction functionFindIndexAndSize(node \(v\)) \(i\leftarrow\)FindIndex(\(v\)) \(s\gets size[i]\) return\(i,s\) endfunction functionMERGE(cluster index \(x\), cluster index \(y\)) \(size[y]\gets size[x]+size[y]\) \(index[x]\gets index[y]\) endfunction
```

**Algorithm 1** Get Cluster Index And Cluster Size of a Node (Using disjoint-set data structure)

### GFuN

We first realize the \(\phi_{e}\), \(\phi_{v}\), \(\phi_{u}\) functions in the full GN block (Sec 2.1 and [6]) as neural networks:

\[\mathbf{E}^{\prime}_{k} = \mathrm{NN}_{e}(\mathbf{E}_{k},\mathbf{V}_{s_{k}},\mathbf{V}_{t_{k }},\mathbf{u}), \tag{2}\] \[\mathbf{V}^{\prime}_{i} = \mathrm{NN}_{v}(\mathbf{\bar{E}}^{\prime}_{i},\mathbf{V}_{i}, \mathbf{u}),\] (3) \[\mathbf{u}^{\prime} = \mathrm{NN}_{u}(\mathbf{\bar{E}}^{\prime},\mathbf{\bar{V}}^{\prime },\mathbf{u}), \tag{4}\]

respectively, where

\[\mathbf{\bar{E}}^{\prime}_{i} = \rho^{e\to v}(\{\mathbf{E}^{\prime}_{k}\}_{k\in[1...N^{e}],t_{k }=i}), \tag{5}\] \[\mathbf{\bar{E}}^{\prime} = \rho^{e\to u}(\mathbf{E}^{\prime}),\] (6) \[\mathbf{\bar{V}}^{\prime} = \rho^{v\to u}(\mathbf{V}^{\prime}). \tag{7}\]

We further decompose the neural networks according to the features in the function:

\[\mathrm{NN}_{e}(\mathbf{E}_{k},\mathbf{V}_{s_{k}},\mathbf{V}_{t_{k }},\mathbf{u}) = \mathrm{NN}_{e\gets e}(\mathbf{E}_{k})+\mathrm{NN}_{e\gets v _{a}}(\mathbf{V}_{s_{k}})+\mathrm{NN}_{e\gets v_{t}}(\mathbf{V}_{t_{k}})+ \mathrm{NN}_{e\gets u}(\mathbf{u}), \tag{8}\] \[\mathrm{NN}_{v}(\mathbf{\bar{E}}^{\prime}_{i},\mathbf{V}_{i}, \mathbf{u}) = \mathrm{NN}_{v\gets e}(\mathbf{\bar{E}}^{\prime}_{i})+ \mathrm{NN}_{v\gets v}(\mathbf{V}_{i})+\mathrm{NN}_{v\gets u}( \mathbf{u}),\] (9) \[\mathrm{NN}_{u}(\mathbf{\bar{E}}^{\prime},\mathbf{\bar{V}}^{ \prime},\mathbf{u}) = \mathrm{NN}_{u\gets e}(\mathbf{\bar{E}}^{\prime})+\mathrm{NN}_ {u\gets v}(\mathbf{\bar{V}}^{\prime})+\mathrm{NN}_{u\gets u}( \mathbf{u}) \tag{10}\]However, such GN block uses 10 times the number of parameters as the standard GCN [34] layer when the node, edge and global embedding dimensions are all equivalent. In practice, we disable all computations related to global features \(\mathbf{u}\), as well as the neural networks \(\mathrm{NN}_{e\gets e}\) and \(\mathrm{NN}_{e\gets v_{t}}\). We also set \(\mathrm{NN}_{v\gets e}\) to be Identity.

In practice, we use the summation function as the aggregator function \(\rho^{e\to v}\) by default. But other choices like MEAN, MAX, gated summation, attention or their combinations can also be used.

Overall, we call such GN block as graph full network (GFuN).

```
input graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), edge scores \(\mathbf{w}\), node pooling ratio \(\eta_{v}\), maximum cluster sizes \(\tau_{c}\). output pooled graph \(\hat{\mathcal{G}}=(\hat{\mathcal{V}},\tilde{\mathcal{E}})\) and inter graph \(\hat{\mathcal{G}}=(\hat{\mathcal{V}},\hat{\mathcal{E}})\)  InitializeDisjointSet(\(\mathcal{G}\)) \(remains\gets N^{v}\) {\(N^{v}\) is the number of nodes in graph \(\mathcal{G}\)} \(\mathcal{E}\leftarrow\) Sort the edges \(\mathcal{E}\) according to the edge scores \(\mathbf{w}\) decreasingly. for\(e\in\mathcal{E}\)do \(x,y\leftarrow\) the two endpoints of the edge \(e\) \(rx,sx\leftarrow\) FindIndexAndSize(\(x\)) \(ry,sy\leftarrow\) FindIndexAndSize(\(y\)) if\(rx\neq y\) and (\(sx+sy\leq\tau_{c}\)) then \(\mathrm{Merge}(x,y)\) \(remains\gets remains-1\) if\(remains\leq N^{v}*\eta_{v}\)then break endif endif endfor \(\hat{\mathcal{V}},\hat{\mathcal{E}},\hat{\mathcal{V}},\hat{\mathcal{E}}\leftarrow\{ \},\{\},\{\},\{\},\{\}\)  create empty mapping \(c2n\) from cluster index to nodes for\(v\in\hat{\mathcal{V}}\)do ifFindIndex(\(v\)) \(=v\)then  create new node \(\tilde{v}\) \(c2n[v]=\tilde{v}\) \(\tilde{\mathcal{V}}\leftarrow\tilde{\mathcal{V}}\cup\{\tilde{v}\}\) endif endfor for\(e\in\mathcal{E}\)do \(x,y\leftarrow\) the two endpoints of the edge \(e\) \(\tilde{x}\gets c2n[\mathrm{FindIndex}(x)]\) \(\tilde{y}\gets c2n[\mathrm{FindIndex}(y)]\) \(\tilde{\mathcal{E}}\leftarrow\tilde{\mathcal{E}}\cup\{(\tilde{x},\tilde{y})\}\) endfor for\(v\in\mathcal{V}\)do \(\tilde{v}\gets c2n[\mathrm{FindIndex}(v)]\) \(\tilde{\mathcal{E}}\leftarrow\tilde{\mathcal{E}}\cup\{(v,\tilde{v})\}\) endfor \(\hat{\mathcal{V}}\leftarrow\mathcal{V}\cup\tilde{\mathcal{V}}\)
```

**Algorithm 2** Strided EdgePool

### Encoder and Decoder

**Encoder**. For input embedding, we use the Linear layer or Embedding layer to embed input features. For example, we follow [15] and use the Linear layer on MNIST and CIFAR10 datasets, and use the Embedding layer on ZINC and AQSOL datasets. For the molecular graph in OGB, we use the same embedding method as in the original work [28]. Besides, we can adopt positional encoding methods like Laplacian [15] and Random Walk [16] to further embed global and local graph structure information. The embedding of positional encoding can be combined into (like concatenation, addition, etc.) input features and form new embeddings.

**Decoder**. We can freely choose from the multi-scale features computed during the _process_ stage as inputs to the decoder module. Empirically, we use the features on the original graph for prediction in all experiments. For node-level tasks, we apply a last GNN layer on the original graph to get logits for every node. For graph-level tasks, we first use global pooling functions to aggregate features. We can use common global pooling methods like SUM, MEAN, MAX, or their combination. After the global pool, we use MLP layer(s) to generate the prediction.

### Architecture Variants

We can replace some GN blocks within Mee layers as an Identity block to reduce the time complexity. We call the height \(j\) is reserved if the intra GN block of height \(j\) is not replaced by an Identity block. We prefer to reserve an interval of consecutive heights for the Mee layers. (The inter GN blocks between these heights remain unchanged while others are replaced as identities) By varying the heights reserved in each Mee layers, we can create a large number of variants of MeGraph model including U-Shaped, Bridge-Shaped and Staircase-Shaped.

**U-Shaped**. This variant is similar to Graph U-Net [22]. In this U-Shaped variant, the relationship between the number of layers \(n\) and height \(h\) is \(n=2h+1\), and there is only one GN block in each layer. We keep the GN block at height \(j=i\) for each layer \(i\) at the first half layers and keep the GN block at height \(j=n-i+1\) for each layer \(i\) at the later half layers. In the middle layer, only the last height \(j=h=(n-1)/2\) has a GN block.

**Bridge-Shaped**. In this variant, all GN blocks are combined like an arch bridge. Describe in detail, in the first and last layers, there are GN blocks in each height. In other layers, there are GN blocks at a height of \(1\) to \(j\) (where \(1<j<h\)).

**Staircase-Shaped**. There are four forms in this variant, and the number of layers \(n\) is equal to the height \(h\) in all forms. The first form is like the 'downward' staircase. In each layer \(i\) of this form, there are GN blocks at the height of \(j\) to \(h\) (where \(j=i\)). The second form is the inverted first form. In each layer \(i\) of this second form, there are GN blocks at height of \(1\) to \(h-i+1\) (where \(j=i\)). The last two forms are the mirror of the first and second forms.

## Appendix D Theoretical Discussions

### Smaller Number of Aggregation Steps for Capturing Long-Range Interactions

We rephrase the analysis provided in [47] as following:

We analyze the number of aggregation steps required to capture long-range interactions between nodes in the original graph while assuming the node representation capacity is large enough.

Standard message-passing GNNs require \(n\) aggregation steps to capture long-range interactions of \(n\) hops away, therefore requiring a stack of \(n\) layers, which could be expensive when \(n\) is large.

We also assume the height \(h\) of the hierarchy is large enough so that all nodes of the original graph are pooled into a single node. In that case, the information aggregation along the hierarchy captures all pairs of LRIs into the embedding of the single node. Which means the number of aggregation steps of MeGraph is \(h\). When we adopt a pooling method that coarsens the graph at least half, \(h\) is at most \(O(\log(|V|))\) where \(|V|\) is the number of nodes of the input graph. Therefore, the height \(h\) is significantly smaller than the diameter of the graph (which could be \(O(|V|)\)) in most cases.

### MeGraph can degenerate to standard GNNs

MeGraph can learn a gating function (within the X-UPD function) that only reserves the features of the same scale while performing cross-scale information exchanging. In that case, there will be no information exchange across multi-scale graphs, and features other than those in the original scale will not be aggregated. We provide a proof sketch below, while the results of the random pooling method ablation study in Sec. 4.4 also provide empirical evidence.

**Proof**: The cross update function is \((X^{\prime}_{j},\hat{X}^{\prime}_{j},X^{\prime}_{j+1})=\texttt{X-UPD}(j,X_{j}, \hat{X}_{j},X_{j+1})\). There is a residual function applied here, and we assume it is implemented as a gated residual: \(X^{\prime\prime}_{j}=\sigma(\alpha)X_{j}+\sigma(\beta)X^{\prime}_{j}\), where \(\sigma\) is the sigmoid function and \(\alpha,\beta\) are learnable parameters. Theoretically, it is possible that\(\sigma(\alpha)=1\) and \(\sigma(\beta)=0\) after training. In that case, \(X^{\prime\prime}_{j}=X_{j}\), which means \(X_{j}\) is not changed over steps 2 and 3 of the Mee layer. Therefore, \(X^{i}_{0}=GN^{i,0}_{intra}(\mathcal{G},X^{i-1}_{0})\), this is equivalent to a simple GNN layer that \(X^{i}=GNN_{i}(\mathcal{G},X^{i-1})\) as \(X^{i}_{0}\) is the features of the original graph and \(GN_{intra}\) is a GNN layer. Therefore, MeGraph degenerates to standard message-passing GNNs in this case. \(\blacksquare\)

## Appendix E Implementation and Training Details

We use PyTorch [45] and Deep Graph Library (DGL) [54] to implement our method.

We implement S-EdgePool using DGL, extending from the original implementation of EdgePool in the Pytorch Geometric library (PYG) [19]. We did Constant optimization over the implementation to speed up the training and inference of the pooling. We further use Taichi-Lang [29] to speed up the dynamic node clustering process of S-EdgePool. The practical running time of MeGraph model with height \(h>1\) after optimization is about \(2h\) times as the \(h=1\) baseline. This is still slower than the theoretical computational complexity due to the constant in the implementation and the difficulty of paralleling the sequential visitation of edges (according to their scores) in the EdgePool and S-EdgePool. This process could be further speed up by implementing the operations with the CUDA library. We provide the practical running time for \(h>1\) and \(h=1\) in GNN benchmark and OGB-G datasets in Tables 7 and 8. The speed is slower than the theoretical one partially due to the pooling ratio of edges being larger than the nodes, making the number of edges decrease slowly over the hierarchy. To further speed up, we could use the variants of MeGraph introduced in App. C.5 by skipping some computation modules.

We run all our experiments on V100 GPUs and M40 GPUs. For training the neural networks, we use Adam [33] as the optimizer. We report the hyper-parameters of the Megraph in Table 9.

For models using GFuN layer as the core GN block, we find it benefits from using layer norms [4]. However, for models using GCN layer as the core GN block, we find it performs best when using batch norms [31].

## Appendix F Additional Experiment Results

### Experimental Protocol

We evaluate MeGraph on public real-world graph benchmarks. To fairly compare MeGraph with the baselines, we use the following experimental protocols. We first report the public baseline results and our reproduced standard GCN's results. We then replace GCN layers with GFuN layers (which is equivalent to MeGraph (\(h=1\))) to serve as another baseline. We tune the hyper-parameters (such as learning rate, dropout rate and the readout global pooling method, etc.) of MeGraph (\(h=1\)) and choose the best configurations. We then run other diversely configured MeGraph candidates by tuning

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline  & ZINC & AQSOL & CIFAR10 & MNIST & PATTERN & CLUSTER \\ \hline \hline Megraph \((h=5)\) & 25.69 & 20.22 & 336.63 & 307.23 & 101.52 & 69.65 \\ Megraph \((h=1)\) & 2.41 & 1.67 & 51.74 & 38.60 & 9.21 & 6.52 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Running time (s) for one epoch on the GNN benchmark. See Sec. E for more implementation details.

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline  & molhiv & molbac & molbbbp & molclintox & molsider \\ \hline \hline Megraph \((h=5)\) & 393.42 & 14.70 & 20.36 & 14.03 & 14.12 \\ Megraph \((h=1)\) & 22.50 & 1.43 & 1.58 & 1.26 & 1.41 \\ \hline \hline  & moltox21 & moltoxcast & molesol & molfreesolv & mollipo \\ \hline \hline Megraph \((h=5)\) & 70.15 & 78.77 & 11.68 & 6.24 & 44.77 \\ Megraph \((h=1)\) & 5.27 & 8.17 & 0.76 & 0.41 & 2.77 \\ \hline \end{tabular}
\end{table}
Table 8: Running time (s) for one epoch on the OGBG datasets. See Sec. E for more implementation details.

other hyper-parameters that only matters for \(h>1\), and these hyper-parameters are referred to as the MeGraph hyper-parameters. Detailed configurations are put in Table 9 in App. E.

### Visualization

We plotted the graph hierarchies discovered by MeGraph in the shortest path tasks of Graph Theory Benchmark. In Figure 5, the S-EdgePool with \(\eta_{v}=0.3,\tau_{c}=4\) well preserves the structure of the graph after pooling, while the S-EdgePool with \(\eta_{v}=0.3\) (no cluster size limit) sometimes pooled too many nodes together, breaking the graph structure. The former S-EdgePool leads to better performance as indicated in Table 1. We also plot the hierarchy for SBM-generated graphs in Figure 6, indicating that EdgePool can handle graphs that naturally contains clusters of different size.

### Other Real-World Datasets

**TU dataset** consists of over 120 datasets of varying sizes from a wide range of applications. We choose 10 datasets, 5 of which are molecule datasets (MUTAG, NCI1, PROTEINS, D&D and ENZYMES) and the other 5 are social networks (IMDB-B, IMDB-M, REDDIT-BINARY, REDDIT-MULTI-5K and REDDIT-MULTI-12K). They are all graph classification tasks. For more details of each dataset, please refer to the original work [42].

Our Megraph uses the same network structure and hyper-parameters for the same type of dataset. As shown in Table 10, our Megraph achieves about \(1\%\) absolute gain than the \(h=1\) Baselines.

### GFuN

We show our GFuN results on real-world datasets compared to our reproduced GCN in Table 11, 12 and 13. Both GCN and GFuN have the same hyper-parameters except the batch norm for GCN and layer norm for GFuN as stated in Appendix E.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Hyper-parameters & Synthetic & Graph & LRGB & GNN & OGB & TU \\  & Datasets & Benchmark & Benchmark & Benchmark & Benchmark & Datasets \\ \hline \hline Repeated Runs & 10 & 5 & 4 & 4 & 5 & 1 for each fold \\ \hline \multirow{3}{*}{Epochs per run} & 200 for BA* & 300 & \multirow{3}{*}{200} & \multirow{3}{*}{100} & \multirow{3}{*}{100} \\  & 500 for Tree* & & & & & (100 for MNIST, CIFAR10) & (200 for ENZYMES) \\ \hline \multirow{3}{*}{Learning rate} & 0.002 & \multirow{3}{*}{(0.005 for MCC)} & 0.001 & \multirow{3}{*}{0.001} & \multirow{3}{*}{0.001} & \multirow{3}{*}{0.001} & \multirow{3}{*}{0.002} \\  & & & & & & \\ \hline Weight decay & 0.0005 & 0.0005 & 0 & 0 & 0.0005 & 0.0005 \\ \hline Node hidden dim & 64 & 128 & 160 & 144 & 300 & 128 \\ \hline Edge hidden dim & \multirow{3}{*}{64} & \multirow{3}{*}{128} & \multirow{3}{*}{160} & \multirow{3}{*}{144} & \multirow{3}{*}{300} & \multirow{3}{*}{128} \\ (for GFuN) & & & & & & \\ \hline Num Mee & \multirow{3}{*}{-} & \multirow{3}{*}{-} & \multirow{3}{*}{4} & \multirow{3}{*}{3} & \multirow{3}{*}{4} & \multirow{3}{*}{2} \\ layers \(n\) & & & & & & \\ \hline Height \(h\) & - & - & 9 & 5 & 5 & 3 or 5 \\ \hline Batch size & 32 & 32 & 128 & 128 & 32 & 128 \\ \hline Input embedding & False & True & True & True & True & True \\ \hline \multirow{3}{*}{Global pooling} & \multirow{3}{*}{Mean} & \multirow{3}{*}{Mean} & \multirow{3}{*}{Mean} & \multirow{3}{*}{Mean} & \multirow{3}{*}{Mean} \\  & & & & & & \\  & & & Max & Sum & & \\ \hline \hline Dataset split & \multirow{3}{*}{8:1:1} & \multirow{3}{*}{8:1:1} & \multirow{3}{*}{Original} & \multirow{3}{*}{Original} & \multirow{3}{*}{Original} & \multirow{3}{*}{10-fold cross validation} \\ (train:val:test) & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 9: Hyper-parameters of the standard version of MeGraph for each dataset. It is worth noting that the total number of GNN layers is equals to one plus the number of Mee layers as \(n+1\).

[MISSING_PAGE_FAIL:25]

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline Model & MDIMG \(\uparrow\) & NCI1 \(\uparrow\) & PROTEINS \(\uparrow\) & D\&D \(\uparrow\) & ENZYMES \(\uparrow\) & Average \\ \hline \hline GCN & 92.46 \(\pm\)6.55 & 82.55 \(\pm\)0.99 & 77.82 \(\pm\)4.52 & 80.56 \(\pm\)2.40 & 74.17 \(\pm\)5.59 & 81.51 \\ GFuN & 93.01 \(\pm\)7.96 & 82.80 \(\pm\)1.30 & 80.60 \(\pm\)3.83 & 82.43 \(\pm\)2.60 & 73.00 \(\pm\)5.31 & 82.37 \\ \hline \hline Model & IMDB-B \(\uparrow\) & IMDB-M \(\uparrow\) & RE-B \(\uparrow\) & RE-MSK \(\uparrow\) & RE-M12K \(\uparrow\) & Average \\ \hline \hline GCN & 76.00 \(\pm\)3.44 & 50.33 \(\pm\)1.89 & 91.15 \(\pm\)1.63 & 56.47 \(\pm\)1.54 & 48.71 \(\pm\)0.88 & 64.53 \\ GFuN & 68.90 \(\pm\)3.42 & 51.27 \(\pm\)3.22 & 92.25 \(\pm\)1.12 & 57.53 \(\pm\)1.31 & 51.54 \(\pm\)1.19 & 64.30 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Comparison between GCN and GFuN on GNN benchmark.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Model & MDIMG \(\uparrow\) & NCI1 \(\uparrow\) & PROTEINS \(\uparrow\) & D\&D \(\uparrow\) & ENZYMES \(\uparrow\) & Average \\ \hline \hline GCN\({}^{\dagger}\) & 87.20 \(\pm\)5.11 & 83.65 \(\pm\)1.69 & 75.65 \(\pm\)3.24 & 79.12 \(\pm\)3.07 & 66.50 \(\pm\)6.91* & 78.42 \\ GIN\({}^{\dagger}\) & 89.40 \(\pm\)5.60 & 82.70 \(\pm\)1.70 & 76.20 \(\pm\)2.80 & - & -

### Graph Theory Dataset

We provide a list of tables (from Table 19 to 30) showing the individual results of Table 1 for each possible graph generation method. Each table contains a list of variants of models and 5 tasks. Some graph generation methods and task combinations are trivial so we filter them out.

Figure 8: Node Classification accuracy for MeGraph model on BAShape (left) and BACommunity (right) datasets, varying the height \(h\) and the number of Mee layers \(n\). A clear gap can be observed between heights 1 and \(\geq 2\). The concrete number of accuracy can be found in Table 15.

Figure 7: Node Classification accuracy for MeGraph model on TreeCycle (left) and TreeGrid (right) datasets, varying the height \(h\) and the number of Mee layers \(n\). A clear gap can be observed between heights 1, 2 and \(\geq 3\). The concrete number of accuracy can be found in Table 14.

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_FAIL:29]

[MISSING_PAGE_FAIL:30]

[MISSING_PAGE_FAIL:31]

[MISSING_PAGE_FAIL:32]

[MISSING_PAGE_FAIL:33]