# Uncoupled and Convergent Learning in Two-Player Zero-Sum Markov Games with Bandit Feedback

 Yang Cai

Yale University

yang.cai@yale.edu

&Haipeng Luo

University of Southern California

haipengl@usc.edu

&Chen-Yu Wei

University of Virginia

chenyu.wei@virginia.edu

&Weiqiang Zheng

Yale University

weiqiang.zheng@yale.edu

###### Abstract

We revisit the problem of learning in two-player zero-sum Markov games, focusing on developing an algorithm that is _uncoupled_, _convergent_, and _rational_, with non-asymptotic convergence rates to Nash equilibrium. We start from the case of stateless matrix game with bandit feedback as a warm-up, showing an \(\mathcal{O}(t^{-\frac{1}{8}})\) last-iterate convergence rate. To the best of our knowledge, this is the first result that obtains finite last-iterate convergence rate given access to only bandit feedback. We extend our result to the case of irreducible Markov games, providing a last-iterate convergence rate of \(\mathcal{O}(t^{-\frac{1}{9+\varepsilon}})\) for any \(\varepsilon>0\). Finally, we study Markov games without any assumptions on the dynamics, and show a _path convergence_ rate, a new notion of convergence we define, of \(\mathcal{O}(t^{-\frac{1}{10}})\). Our algorithm removes the coordination and prior knowledge requirement of [21], which pursued the same goals as us for irreducible Markov games. Our algorithm is related to [10, 1] and also builds on the entropy regularization technique. However, we remove their requirement of communications on the entropy values, making our algorithm entirely uncoupled.

## 1 Introduction

In multi-agent learning, a central question is how to design algorithms so that agents can _independently_ learn (i.e., with little coordination overhead) how to interact with each other. Additionally, it is desirable to maximally reuse existing single-agent learning algorithms, so that the multi-agent system can be built in a modular way. Motivated by this question, _decentralized_ multi-agent learning emerges with the goal to design decentralized systems, in which no central controller governs the policies of the agents, and each agent learns based on only their local information - just like in a single-agent algorithm. In recent years, we have witnessed significant success of this new decentralized learning paradigm. For example, _self-play_, where each agent independently deploys the same single-agent algorithm to play against each other without further direct supervision, plays a crucial role in the training of AlphaGo [20] and AI for Stratego [3].

Despite the recent success, many important questions remain open in decentralized multi-agent learning. Indeed, unless the decentralized algorithm is carefully designed, self-play often falls short of attaining certain sought-after global characteristics, such as convergence to the global optimum or stability as seen in, for example, [24, 25].

In this work, we revisit the problem of learning in two-player zero-sum Markov games, which has received extensive attention recently. Our goal is to design a decentralized algorithm that resemblesstandard single-agent reinforcement learning (RL) algorithms, but with an additional crucial assurance, that is, _guaranteed convergence_ when both players deploy the algorithm. The simultaneous pursuit of independence and convergence has been advocated widely [1, 16, 17, 18], while the results are still not entirely satisfactory. In particular, all of these results rely on assumptions on the dynamics of the Markov game. Our paper takes the first step to remove such assumptions.

More specifically, our goal is to design algorithms that simultaneously satisfy the following three properties (the definitions are adapted from [1, 1]):

* **Uncoupled**: Each player \(i\)'s action is generated by a standalone procedure \(\mathcal{P}_{i}\) which, in every round, only receives the current state and player \(i\)'s own reward as feedback (in particular, it has no knowledge about the actions or policies used by the opponent). There is no communication or shared randomness between the players.
* **Convergent**: The policy pair of the two players converges to a Nash equilibrium.
* **Rational**: If \(\mathcal{P}_{i}\) competes with an opponent who uses a policy sequence that converges to a stationary one, then \(\mathcal{P}_{i}\) converges to the best response of this stationary policy.

The uncoupledness and rationality property capture the independence of the algorithm, while the convergence property provides a desirable global guarantee. Interestingly, as argued in [16], if an algorithm is uncoupled and convergent, then it is also rational, so we only need to ensure that the algorithm is uncoupled and convergent. Regarding the notion of convergence, the standard definition above only allows _last-iterate_ convergence. Considering the difficulty of achieving such convergence, in the related work review (Section 2) and in the design of our algorithm for general Markov games (Section 6), we also consider weaker notions of convergence, including the _best-iterate_ convergence, which only requires that the Cesaro mean of the duality gap is convergent, and the _path_ convergence, which only requires the convergence of the Cesaro mean of the duality gap _assuming minimax/maximin policies are followed in future steps_. The precise definitions of these convergence notions are given at the end of Section 3.

### Our Contributions

The main results in this work are as follows (see also Table 1 for comparisons with prior works):

* As a warm-up, for the special case of matrix games with bandit feedback, we develop an uncoupled algorithm with a last-iterate convergence rate of \(\mathcal{O}(t^{-\frac{1}{3}})\) under self-play (Section 4). To the best of our knowledge, this is the first algorithm with provable last-iterate convergence rate in the setting.
* Generalizing the ideas from matrix games, we further develop an uncoupled algorithm for irreducible Markov games with a last-iterate convergence rate of \(\mathcal{O}(t^{-\frac{1}{9+\varepsilon}})\) for any \(\varepsilon>0\) under self-play (Section 5).
* Finally, for general Markov games without additional assumptions, we develop an uncoupled algorithm with a path convergence rate of \(\mathcal{O}(t^{-\frac{1}{10}})\) under self-play (Section 6).

Our algorithms leverage recent advances on using entropy to regularize the policy updates [14, 15] and the Nash-V-styled value updates [1]. On the one hand, compared to [14, 15], our algorithm has the following advantages: 1) it does not require the two players to exchange their entropy information, which allows our algorithm to be fully uncoupled; 2) it does not require the players to have coordinated policy updates, 3) it naturally extends to general Markov games without any assumptions on the dynamics (e.g., irreducibility). On the other hand, our algorithm inherits appealing properties of Nash-V [1], but additionally guarantees path convergence during execution.

## 2 Related Work

The study of two-player zero-sum Markov games originated from [13], with many other works further developing algorithms and establishing convergence properties [1, 15, 16, 17]. However, these works primarily focused on _solving_ the game with full knowledge of its parameters (i.e., payoff function and transition kernel). The problem of _learning_ in zero-sum games was first formalized by [10]. Designing a _provably_ uncoupled, rational, and convergent algorithm 

[MISSING_PAGE_FAIL:3]

which guarantee that the player's long-term payoff is at least the minimax value [12, 13, 14, 15, 16, 17, 18, 19, 20]. Interestingly, as shown in [13, 15, 16, 17, 18, 19, 21], if the player is paired with an optimistic best-response opponent (instead of using the same algorithm), the first player's strategy can converge to the minimax policy. [15, 16, 18, 19, 20] developed another coupled learning framework to handle exploration, but with symmetric updates on both players. In each round, the players need to jointly solve a general-sum equilibrium problem due to the different exploration bonus added by each player. Hence, the execution of these algorithms is more similar to the Nash-Q algorithm by [13].

So far, exploration has been handled through coupled approaches that are also not rational. To our knowledge, the first uncoupled and rational algorithm that handles exploration is the Nash-V algorithm by [12]. Nash-V can output a nearly-minimax policy through weighted averaging [13]; however, it is not provably convergent during execution. A major remaining open problem is whether one can design a natural algorithm that is provably rational, uncoupled, and convergent with exploration capability. Our work provides the first progress towards this goal.

### Other works on last-iterate convergence

Uncoupled Learning dynamics in normal-form games with provable last-iterate convergence rate receives extensive attention recently. Most of the works assume that the players receive gradient feedback, and convergence results under bandit feedback remain sparse. Linear convergence is shown for strongly monotone games or bilinear games under gradient feedback [15, 16, 17, 18, 19] and sublinear rates are proven for strongly monotone games with bandit feedback [16, 18, 19, 20, 21, 22]. Convergence rate to strict Nash equilibrium is analyzed by [11]. For monotone games that includes two-player zero-sum games as a special case, the last-iterate convergence rate of no-regret learning under gradient feedback has been shown recently [1, 19, 20, 21, 22]. With bandit feedback, [20] showed an impossibility result that certain algorithms with optimal \(\mathcal{O}(\sqrt{T})\) regret do not converge in last-iterate. To the best of our knowledge, there is no natural uncoupled learning dynamics with provable last-iterate convergence rate in two-player zero-sum games with bandit feedback.

## 3 Preliminaries

Basic NotationsThroughout the paper, we assume for simplicity that the action set for the two players are the same, denoted by \(\mathcal{A}\) with cardinality \(A=|\mathcal{A}|\).1 We usually call player 1 the \(x\)-player and player \(2\) the \(y\)-player. The set of mixed strategies over an action set \(\mathcal{A}\) is denoted as \(\Delta_{\mathcal{A}}:=\{x:\sum_{a\in\mathcal{A}}x_{a}=1;0\leq x_{a}\leq 1, \forall a\in\mathcal{A}\}\). To simplify notation, we denote by \(z=(x,y)\) the concatenated strategy of the players. We use \(\phi\) as the entropy function such that \(\phi(x)=-\sum_{a\in\mathcal{A}}x_{a}\ln x_{a}\), and KL as the Kullback-Leibler (KL) divergence such that \(\text{KL}(x,x^{\prime})=\sum_{a\in\mathcal{A}}x_{a}\ln\frac{x_{a}}{x^{\prime} _{a}}\). The all-one vector is denoted by \(\mathbf{1}=(1,1,\cdots,1)\).

Footnote 1: We make this assumption only to simplify notations; our proofs can be easily extended to the case where the action sets of the two players are different.

Matrix GamesIn a two-player zero-sum matrix game with a loss matrix \(G\in[0,1]^{A\times A}\), when the \(x\)-player chooses action \(a\) and the \(y\)-player chooses action \(b\), the \(x\)-player suffers loss \(G_{a,b}\) and the \(y\)-player suffers loss \(-G_{a.b}\). A pair of mixed strategy \((x^{\star},y^{\star})\) is a _Nash equilibrium_ for \(G\) if for any strategy profile \((x,y)\in\Delta_{\mathcal{A}}\times\Delta_{\mathcal{A}}\), it holds that \((x^{\star})^{\top}Gy\leq(x^{\star})^{\top}Gy^{\star}\leq x^{\top}Gy^{\star}\). Similarly, \((x^{\star},y^{\star})\) is a Nash equilibrium for a two-player zero-sum game with a general convex-concave loss function \(f(x,y):\Delta_{\mathcal{A}}\times\Delta_{\mathcal{A}}\rightarrow\mathbb{R}\) if for all \((x,y)\in\Delta_{\mathcal{A}}\times\Delta_{\mathcal{A}}\), \(f(x^{\star},y)\leq f(x^{\star},y^{\star})\leq f(x,y^{\star})\). The celebrated minimax theorem [21] guarantees the existence of Nash equilibria in two-player zero-sum games. For a pair of strategy \((x,y)\), we use _duality gap_ defined as \(\textsc{Gap}(G,x,y)\triangleq\max_{y^{\prime}}x^{\top}Gy^{\prime}-\min_{x^{ \prime}}x^{\prime\top}Gy\) to measure its proximity to Nash equilibria.

Markov GamesA generalization of matrix games, which models dynamically changing environment, is _Markov games_. We consider infinite-horizon discounted two-player zero-sum Markov games, denoted by a tuple \((\mathcal{S},\mathcal{A},(G^{s})_{s\in\mathcal{S}},(P^{s})_{s\in\mathcal{S}}, \gamma)\) where (1) \(\mathcal{S}\) is a finite state space; (2) \(\mathcal{A}\) is a finite action space for both players; (3) Player 1 suffers loss \(G^{s}_{a,b}\in[0,1]\) (respectively player 2 suffersloss \(-G_{a,b}^{s}\)) when player 1 chooses action \(a\) and player 2 chooses action \(b\) at state \(s\); (4) \(P\) is the transition function such that \(P_{a,b}^{s}(s^{\prime})\) is the probability of transiting to state \(s^{\prime}\) when player 1 plays \(a\) and player 2 plays \(b\) at state \(s\); (5) \(\gamma\in[\frac{1}{2},1)\) is a discount factor.

A stationary policy for player 1 is a mapping \(\mathcal{S}\rightarrow\Delta_{\mathcal{A}}\) that specifies player 1's strategy \(x^{s}\in\Delta_{\mathcal{A}}\) at each state \(s\in\mathcal{S}\). We denote \(x=(x^{s})_{s\in\mathcal{S}}\). Similar notations apply to player 2. We denote \(z^{s}=(x^{s},y^{s})\) as the concatenated strategy for the players and \(z=(x,y)\). The value function \(V_{x,y}^{s}\) denotes the expected loss of player 1 (or the expected payoff of player 2) given a pair of stationary policy \((x,y)\) and initial state \(s\):

\[V_{x,y}^{s}=\mathbb{E}\Bigg{[}\sum_{t=1}^{\infty}\gamma^{t-1}G_{a_{t},b_{t}}^{ s_{t}}|s_{1}=s,a_{t}\sim x^{s_{t}},b_{t}\sim y^{s_{t}},s_{t+1}\sim P_{a_{t},b_{t}} ^{s_{t}}(\cdot),\forall t\geq 1\Bigg{]}.\]

The _minimax game value_ on state \(s\) is defined as \(V_{\star}^{s}=\min_{x}\max_{y}V_{x,y}^{s}=\max_{y}\min_{x}V_{x,y}^{s}\). We call a pair of policy \((x_{\star},y_{\star})\) a _Nash equilibrium_ if it attains minimax game value of a state \(s\) (such policy pair necessarily attains the minimax game value over all states). The _duality gap_ of \((x,y)\) is \(\max_{s}\left(\max_{y^{\prime}}V_{x,y^{\prime}}^{s}-\min_{x^{\prime}}V_{x^{ \prime},y}^{s}\right)\). The \(Q\)-function on state \(s\) under policy pair \((x,y)\) is defined via \(Q_{x,y}^{s}(a,b)=G_{a,b}^{s}+\gamma\cdot\mathbb{E}_{s^{\prime}\sim P_{a,b}^{s} (\cdot)}[V_{x,y}^{s^{\prime}}]\), which can be rewritten as a matrix \(Q_{x,y}^{s}\) such that \(V_{x,y}^{s}=x^{s}Q_{x,y}^{s}y^{s}\). We denote \(Q_{\star}^{s}=Q_{x,y}^{s}\), the \(Q\)-function under a Nash equilibrium \((x_{\star},y_{\star})\). It is known that \(Q_{\star}^{s}\) is unique for any \(s\) even when multiple equilibria exist.

Uncoupled Learning with Bandit FeedbackWe assume the following uncoupled interaction protocol: at each round \(t=1,\ldots,T\), the players both observe the current state \(s_{t}\), and then, with the policy \(x_{t}\) and \(y_{t}\) in mind, they independently choose actions \(a_{t}\sim x_{t}^{s_{t}}\) and \(b_{t}\sim y_{t}^{s_{t}}\), respectively. Both of them then observe \(\sigma_{t}\in[0,1]\) with \(\mathbb{E}[\sigma_{t}]=G_{a_{t},b_{t}}^{s_{t}}\), and proceed to the next state \(s_{t+1}\sim P_{a_{t},b_{t}}^{s_{t}}(\cdot)\). Importantly, they do not observe each other's action.

Notions of ConvergenceFor Markov games with the irreducible assumption (Assumption 1), given players' history of play \((s_{t},x_{t},y_{t})_{t\in[T]}\), the _best-iterate_ convergence rate is measured by the average duality gap \(\frac{1}{T}\sum_{t=1}^{T}\max_{s,x,y}\left(V_{x_{t},y}^{s}-V_{x,y_{t}}^{s}\right)\), while the stronger _last-iterate_ convergence rate is measured by \(\max_{s,x,y}\left(V_{x_{T},y}^{s}-V_{x,y_{T}}^{s}\right)\), i.e., the duality gap of \((x_{T},y_{T})\). For general Markov games, we propose the _path_ convergence rate, which is measured by the average duality gap at the visited states with respect to the optimal \(Q\)-function: \(\frac{1}{T}\sum_{t=1}^{T}\max_{x_{t},y}\left(x_{t}^{s_{t}^{\top}}Q_{\star}^{s_ {t}}y^{s_{t}}-x_{t}^{\top}Q_{\star}^{s_{t}}y_{t}^{s_{t}}\right)\). We remark that the path convergence guarantee is weaker than the counterpart of the other two notions of convergence in general Markov games, but still provides meaningful implications (see detailed discussion in Section 6.1 and Appendix F).

## 4 Matrix Games

In this section, we consider two-player zero-sum matrix games. We propose Algorithm 1 for decentralized learning of Nash equilibria. We only present the algorithm for the \(x\)-player as the algorithm for the \(y\)-player is symmetric.

```
1:Define:\(\eta_{t}=t^{-k_{\eta}}\), \(\beta_{t}=t^{-k_{\beta}}\), \(\epsilon_{t}=t^{-k_{\epsilon}}\) where \(k_{\eta}=\frac{5}{8}\), \(k_{\beta}=\frac{3}{8}\), \(k_{\epsilon}=\frac{1}{8}\). \(\Omega_{t}=\left\{x\in\Delta_{\mathcal{A}}:x_{a}\geq\frac{1}{4t^{2}},\,\forall a \in\mathcal{A}\right\}\).
2:Initialization:\(x_{1}=\frac{1}{A}\mathbf{1}\).
3:for\(t=1,2,\ldots\)do
4: Sample \(a_{t}\sim x_{t}\), and receive \(\sigma_{t}\in[0,1]\) with \(\mathbb{E}\left[\sigma_{t}\right]=G_{a_{t},b_{t}}\).
5: Compute \(g_{t}\) where \(g_{t,a}=\frac{\mathbf{1}\left[a_{t}=a\right]\sigma_{t}}{x_{t,a}+\beta_{t}}+ \epsilon_{t}\ln x_{t,a},\forall a\in\mathcal{A}\).
6: Update \(x_{t+1}\leftarrow\operatorname*{argmin}_{x\in\Omega_{t+1}}\left\{x^{\top}g_{t}+ \frac{1}{\eta_{t}}\text{KL}(x,x_{t})\right\}.\)
7:endfor ```

**Algorithm 1** Matrix Game with Bandit Feedback

The algorithm is similar to the Exp3-IX algorithm by [15] that achieves a high-probability regret bound for adversarial multi-armed bandits, but with several modifications. First (and most importantly), in addition to the standard loss estimators used in [10], we add another negative term \(\epsilon_{t}\ln x_{t,a}\) to the loss estimator of action \(a\) (see Line 5). This is equivalent to the entropy regularization approach in, e.g., [14, 15], since the gradient of the negative entropy \(-\phi(x_{t})\) is \((\ln x_{t,a}+1)_{a\in\mathcal{A}}\) and the constant \(1\) takes no effect in Line 6. Like [14, 15], the entropy regularization drives last-iterate convergence; however, while their results require full-information feedback, our result holds in the bandit feedback setting. The second difference is that instead of choosing the players' strategies in the full probability simplex \(\Delta_{\mathcal{A}}\), our algorithm chooses from \(\Omega_{t}\), a subset of \(\Delta_{\mathcal{A}}\) where every coordinate is lower bounded by \(\frac{1}{At^{2}}\). The third is the choices of the learning rate \(\eta_{t}\), clipping factor \(\beta_{t}\), and the amount of regularization \(\epsilon_{t}\). The main result of this section is the following last-iterate convergence rate of Algorithm 1.

**Theorem 1** (Last-iterate Convergence Rate).: _Algorithm 1 guarantees with probability at least \(1-\mathcal{O}(\delta)\), for any \(t\geq 1\),_

\[\max_{x,y\in\Delta_{A}}\left(x_{t}^{\top}Gy-x^{\top}Gy_{t}\right)=\mathcal{O }\left(\sqrt{A}\ln^{3/2}(At/\delta)t^{-\frac{1}{8}}\right).\]

Algorithm 1 also guarantees \(\mathcal{O}(t^{-\frac{1}{8}})\) regret even when the other player is adversarial. If we only target at an _expected_ bound instead of a high-probability bound, the last-iterate convergence rate can be improved to \(\mathcal{O}(\sqrt{A}\ln^{3/2}(At)t^{-\frac{1}{8}})\). The details are provided in Appendix C.

### Analysis Overview

We define a regularized zero-sum game with loss function \(f_{t}(x,y)=x^{\top}Gy-\epsilon_{t}\phi(x)+\epsilon_{t}\phi(y)\) over domain \(\Omega_{t}\times\Omega_{t}\), and denote by \(z_{t}^{*}=(x_{t}^{*},y_{t}^{*})\) its unique Nash equilibrium since \(f_{t}\) is strongly convex-strongly concave. The regularized game is a slight perturbation of the original matrix game \(G\) over a smaller domain \(\Omega_{t}\times\Omega_{t}\), and we prove that \(z_{t}^{*}\) is an \(\mathcal{O}(\epsilon_{t})\)-approximate Nash equilibrium of the original matrix game \(G\) (Lemma 9). Therefore, it suffices to bound \(\text{KL}(z_{t}^{*},z_{t})\) since the duality gap of \(z_{t}\) is at most \(\mathcal{O}(\sqrt{\text{KL}(z_{t}^{*},z_{t})}+\epsilon_{t})\).

Step 1: Single-Step AnalysisWe start with a single-step analysis of Algorithm 1, which shows:

\[\text{KL}(z_{t+1}^{*},z_{t+1})\leq(1-\eta_{t}\epsilon_{t})\text{KL}(z_{t}^{*},z_{t})+\underbrace{20\eta_{t}^{2}A\ln^{2}(At)+2\eta_{t}^{2}A\lambda_{t}}_{ \text{instability penalty}}+\underbrace{\eta_{t}\xi_{t}+\eta_{t}\xi_{t}}_{\text{ estimation error}}+v_{t}\]

where we define \(v_{t}=\text{KL}(z_{t+1}^{*},z_{t+1})-\text{KL}(z_{t}^{*},z_{t+1})\) (see Appendix B for definitions of \(\lambda_{t},\xi_{t},\zeta_{t}\)) The instability penalty comes from some local-norm of the gradient estimator \(g_{t}\). The estimation error comes from the bias between the gradient estimator \(g_{t}\) and the real gradient \(Gy_{t}\). We pay the last term \(v_{t}\) since the Nash equilibrium \(z_{t}^{*}\) of the regularized game \(f_{t}\) is changing over time.

Step 2: Strategy Convergence to NE of the Regularized GameExpanding the above recursion up to \(t_{0}\), we get

\[\text{KL}(z_{t+1}^{*},z_{t+1})\leq\mathcal{O}\Big{(}\underbrace{\sum_{i=1}^{ t}w_{t}^{i}\eta_{i}^{2}}_{\text{term}_{1}}+\underbrace{2A\sum_{i=1}^{t}w_{t}^{i} \eta_{i}^{2}\lambda_{i}}_{\text{term}_{2}}+\underbrace{\sum_{i=1}^{t}w_{t}^{i} \eta_{i}\xi_{i}}_{\text{term}_{3}}+\underbrace{\sum_{i=1}^{t}w_{t}^{i}\eta_{i} \zeta_{i}}_{\text{term}_{4}}+\underbrace{\sum_{i=1}^{t}w_{t}^{i}v_{i}}_{ \text{term}_{5}}\Big{)},\] (1)

where \(w_{t}^{i}\triangleq\prod_{j=i+1}^{t}(1-\eta_{j}\epsilon_{j})\). To upper bound \(\text{term}_{1}\)-\(\text{term}_{4}\), we apply careful sequence analysis (Appendix A.1) and properties of the Exp3-IX algorithm with changing step size (Appendix A.2). The analysis of \(\text{term}_{5}\) uses Lemma 13, which states \(v_{t}=\text{KL}(z_{t+1}^{*},z_{t+1})-\text{KL}(z_{t}^{*},z_{t+1})\leq\mathcal{ O}(\ln(At)\|z_{t+1}^{*}-z_{t}^{*}\|_{1})=\mathcal{O}(\frac{\ln^{2}(At)}{t})\) and is slightly involved as \(\Omega_{t}\) and \(\epsilon_{t}\) are both changing. With these steps, we conclude that with probability at least \(1-\mathcal{O}(\delta)\), \(\text{KL}(z_{t}^{*},z_{t})=\mathcal{O}\left(A\ln^{3}(At/\delta)t^{-\frac{1}{ 4}}\right)\).

## 5 Irreducible Markov Games

We now extend our results on matrix games to two-player zero-sum Markov games. Similarly to many previous works, our first result makes the assumption that the Markov game is _irreducible_ with bounded travel time between any pair of states. The assumption is formally stated below:

**Assumption 1** (Irreducible Game).: _We assume that under any pair of stationary policies of the two players, and any pair of states \(s,s^{\prime}\), the expected time to reach \(s^{\prime}\) from \(s\) is upper bounded by \(L\)._

We propose Algorithm 2 for uncoupled learning in irreducible two-player zero-sum games, which is closely related to the Nash-V algorithm by [1], but with additional entropy regularization. It can also be seen as players using Algorithm 1 on each state \(s\) to update the policies \((x_{t}^{s},y_{t}^{s})\) whenever state \(s\) is visited, but with \(\sigma_{t}+\gamma V_{t}^{s_{t+1}}\) as the observed loss to construct loss estimators. Importantly, \(V_{1}^{s},V_{2}^{s},\ldots\) is a slowly changing sequence of value estimations that ensures stable policy updates [1, 1, 1]. Note that in Algorithm 2, the updates of \(V_{t}^{s}\) only use players' local information (Line 8).

```
1:Define:\(\eta_{t}=(1-\gamma)t^{-k_{\eta}},\,\beta_{t}=t^{-k_{\beta}},\,\epsilon_{t}= \frac{1}{1-\gamma}t^{-k_{\epsilon}}\), \(\alpha_{t}=t^{-k_{\alpha}}\) with \(k_{\alpha},k_{\epsilon},k_{\beta},k_{\eta}\in(0,1)\), \(\Omega_{t}=\left\{x\in\Delta_{A}:x_{a}\geq\frac{1}{At^{2}},\,\forall a\in \mathcal{A}\right\}\).
2:Initialization:\(x_{1}^{s}\leftarrow\frac{1}{A}\boldsymbol{1}\), \(n_{1}^{s}\gets 0,\,\,\,V_{1}^{s}\leftarrow\frac{1}{2(1-\gamma)},\,\, \forall s\).
3:for\(t=1,2,\ldots,\)do
4:\(\tau=n_{t+1}^{s_{t}}\gets n_{t}^{s_{t}}+1\) (the number of visits to state \(s_{t}\) up to time \(t\)).
5: Draw \(d_{t}\sim x_{t}^{s_{t}}\), observe \(\sigma_{t}\in[0,1]\) with \(\mathbb{E}\left[\sigma_{t}\right]=G_{a_{t},b_{t}}^{s_{t}}\), and observe \(s_{t+1}\sim P_{a_{t},b_{t}}^{s_{t}}(\cdot)\).
6: Compute \(g_{t}\) where \(g_{t,a}=\frac{1(a_{t}=a)\left(\sigma_{t}+\gamma V_{t}^{s_{t+1}}\right)}{x_{t}^ {s_{t}}+\beta_{s}}+\epsilon_{\tau}\ln x_{t,a}^{s_{t}},\,\,\forall a\in \mathcal{A}\).
7: Update \(x_{t+1}^{s_{t}}\leftarrow\operatorname*{argmin}_{x\in\Omega_{\tau+1}}\left\{x ^{\top}g_{t}+\frac{1}{\eta_{\tau}}\text{KL}(x,x_{t}^{s_{t}})\right\}\).
8: Update \(V_{t+1}^{s_{t}}\leftarrow(1-\alpha_{\tau})V_{t}^{s_{t}}+\alpha_{\tau}\left( \sigma_{t}+\gamma V_{t}^{s_{t+1}}\right)\).
9: For all \(s\neq s_{t}\), \(x_{t+1}^{s}\gets x_{t}^{s}\), \(n_{t+1}^{s}\gets n_{t}^{s}\), \(V_{t+1}^{s}\gets V_{t}^{s}\).
10:endfor ```

**Algorithm 2** Irreducible Markov Game

Comparison to Previous WorksAlthough Algorithm 2 shares similarity with previous works that also use entropy regularization, we believe that both the design and the analysis of our algorithm are novel and non-trivial. To the best of our knowledge, all previous entropy regularized two-player zero-sum Markov game algorithms are coupled (e.g., [1, 1, 1]), while ours is the first that achieves uncoupledness under entropy regularization. We further discuss this by comparing our algorithm to those in [1], highlighting the new technical challenges we encounter.

The entropy-regularized OMWU algorithm in [1] is tailored to the full-information setting. Moreover, in the value function update step both players need to know the entropy value of the other player's policy, which is unnatural. Indeed, the authors explicitly present the removal of this information sharing as an open question. We answer this open question affirmatively by giving a fully decentralized algorithm for zero-sum Markov games with provable last-iterate convergence rates. In Algorithm 2 (Line 8), the update of the value function \(V\) is simple and does not require any entropy information: \(V_{t+1}^{s_{t}}\leftarrow(1-\alpha_{\tau})V_{t}^{s_{t}}+\alpha_{\tau}\left( \sigma_{t}+\gamma V_{t}^{s_{t+1}}\right)\). This modification results in a discrepancy between the policy update and the value update. While the policy now incorporates a regularization term, the value function does not. Such a mismatch is unprecedented in earlier studies and necessitates a non-trivial approach to resolve. Additionally, Algorithm 2 operates on bandit feedback instead of full-information feedback, presenting further technical challenges.

```
1:Define:\(\eta_{t}=(1-\gamma)t^{-k_{\eta}},\,\beta_{t}=t^{-k_{\beta}},\,\epsilon_{t}= \frac{1}{1-\gamma}t^{-k_{\epsilon}}\), \(\alpha_{t}=t^{-k_{\alpha}}\) with \(k_{\alpha},k_{\beta},k_{\eta}\in(0,1)\), \(\Omega_{t}=\left\{x\in\Delta_{A}:x_{a}\geq\frac{1}{At^{2}},\,\forall a\in \mathcal{A}\right\}\).
2:Initialization:\(x_{1}^{s}\leftarrow\frac{1}{At^{2}}\boldsymbol{1}\), \(n_{1}^{s}\gets 0,\,\,\,V_{1}^{s}\leftarrow\frac{1}{At^{2}}\boldsymbol{1}\), \(\forall a\in\mathcal{A}\).
3:for\(t=1,2,\ldots,\)do
4:\(\tau=n_{t+1}^{s_{t}}\gets n_{t}^{s_{t}}+1\) (the number of visits to state \(s_{t}\) up to time \(t\)).
5: Draw \(d_{t}\sim x_{t}^{s_{t}}\), observe \(\sigma_{t}\in[0,1]\) with \(\mathbb{E}\left[\sigma_{t}\right]=G_{a_{t},b_{t}}^{s_{t}}\), and observe \(s_{t+1}\sim P_{a_{t},b_{t}}^{s_{t}}(\cdot)\).
6: Compute \(g_{t}\) where \(g_{t,a}=\frac{1(a_{t}=a)\left(\sigma_{t}+\gamma V_{t}^{s_{t+1}}\right)}{x_{t}^ {s_{t}}+\beta_{s}}+\epsilon_{\tau}\ln x_{t,a}^{s_{t}},\,\,\forall a\in\mathcal{A}\).
7: Update \(x_{t+1}^{s_{t}}\leftarrow\operatorname*{argmin}_{x\in\Omega_{\tau+1}}\left\{x ^{\top}g_{t}+\frac{1}{At}\text{KL}(x,x_{t}^{s_{t}})\right\}\).
8: Update \(V_{t+1}^{s_{t}}\leftarrow(1-\alpha_{\tau})V_{t}^{s_{t}}+\alpha_{\tau}\left( \sigma_{t}+\gamma V_{t}^{s_{t+1}}\right)\).
9: For all \(s\neq s_{t}\), \(x_{t+1}^{s}\gets x_{t}^{s}\), \(n_{t+1}^{s}\gets n_{t}^{s_{t}}\gets n_{t}^{s_{t}}\), \(V_{t+1}^{s}\gets V_{t}^{s}\).
10:endfor ```

**Algorithm 2** Irreducible Markov Game

Algorithm 2 also offers improvement over the uncoupled algorithm of [1]. The algorithm of [1] requires coordinated policy update where the players interact with each other using the current policy for several iterations to get an approximately accurate gradient (the number of iterations required depends on \(L\) as defined in Assumption 1), and then simultaneously update the policy pair on all states. We do not require such unnatural coordination between the players or prior knowledge on \(L\).

Our main result is the following theorem on the last-iterate convergence rate of Algorithm 2.

**Theorem 2** (Last-Iterate Convergence Rate).: _For any \(\varepsilon,\delta>0\), Algorithm 2 with \(k_{\alpha}=\frac{9}{9+\varepsilon}\), \(k_{\epsilon}=\frac{1}{9+\varepsilon}\), \(k_{\beta}=\frac{3}{9+\varepsilon}\), and \(k_{\eta}=\frac{5}{9+\varepsilon}\) guarantees, with probability at least \(1-\mathcal{O}(\delta)\), for any time \(t\geq 1\),_

\[\max_{s,x,y}\big{(}V_{x_{t},y}^{s}-V_{x,y_{t}}^{s}\big{)}\leq\mathcal{O}\Big{(} \frac{At^{2+1/\varepsilon}\ln^{4+1/\varepsilon}(SAt/\delta)\ln^{1/\varepsilon}(t/( 1-\gamma))}{(1-\gamma)^{2+1/\varepsilon}}\cdot t^{-\frac{1}{9+\varepsilon}}\Big{)}.\]

### Analysis Overview

We introduce some notations for simplicity. We denote by \(\mathbb{E}_{s^{\prime}\sim P^{s}}[V_{t}^{s^{\prime}}]\) the \(A\times A\) matrix such that \((\mathbb{E}_{s^{\prime}\sim P^{s}}[V_{t}^{s^{\prime}}])_{a,b}=\mathbb{E}_{s^{ \prime}\sim P^{s}_{a,b}}[V_{t}^{s^{\prime}}]\). Let \(t_{\tau}(s)\) be the \(\tau\)-th time the players visit state \(s\), and define \(\hat{x}_{\tau}^{s}=x_{t_{\tau}(s)}^{s}\) and \(\hat{y}_{\tau}^{s}=y_{t_{\tau}(s)}^{s}\). Then, define the regularized game for each state \(s\) via the loss function \(f_{\tau}^{s}(x,y)=x^{\top}(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}[V_{t_ {\tau}(s)}^{s^{\prime}}])y-\epsilon_{\tau}\phi(x)+\epsilon_{\tau}\phi(y)\). Furthermore, let \(\hat{z}_{\tau\star}^{s}=(\hat{x}_{\tau\star}^{s},\hat{y}_{\tau\star}^{s})\) be the equilibrium of \(f_{\tau}^{s}(x,y)\) over \(\Omega_{\tau}\times\Omega_{\tau}\). In the following analysis, we fix some \(t\geq 1\).

Step 1: Policy Convergence to NE of Regularized GameUsing similar techniques to Step 1 and Step 2 in the analysis of Algorithm 1, we can upper bound \(\text{KL}(\hat{z}_{\tau+1\star}^{s},\hat{z}_{\tau+1}^{s})\) like Eq. (1) with similar subsequent analysis for \(\textbf{term}_{1}\)-\(\textbf{term}_{4}\). The analysis for \(\textbf{term}_{5}\) where \(v_{i}^{s}=\text{KL}(\hat{z}_{i+1\star}^{s},\hat{z}_{i+1}^{s})-\text{KL}(\hat{ z}_{i\star}^{s},\hat{z}_{i+1}^{s})\) is more challenging compared to the matrix game case since here \(V_{t_{i}(s)}^{s}\) is changing between two visits to state \(s\). To handle this term, we leverage the following facts for any \(s^{\prime}\): (1) the irreducibility assumption ensures that \(t_{i+1}(s)-t_{i}(s)\leq\mathcal{O}(L\ln(St/\delta))\) thus the number of updates of the value function at state \(s^{\prime}\) is bounded; (2) until time \(t_{i}(s)\geq i\), state \(s^{\prime}\) has been visited at least \(\Omega(\frac{i}{L\ln(St/\delta)})\) times thus each change of the value function between \(t_{i}(s)\) and \(t_{i+1}(s)\) is at most \(\mathcal{O}((\frac{i}{L\ln(St/\delta)})^{-k_{\alpha}})\). With these arguments, we can bound \(\textbf{term}_{5}\) by \(\mathcal{O}\left(\ln^{4}(SAt/\delta)L\tau^{-k_{\alpha}+k_{\eta}+2k_{\epsilon}}\right)\). Overall, we have the following policy convergence of NE of the regularized game (Lemma 17): \(\text{KL}(\hat{z}_{\tau\star}^{s},\hat{z}_{\tau}^{s})\leq\mathcal{O}\left(A \ln^{4}(SAt/\delta)L\tau^{-k_{\delta}}\right)\), where \(k_{\sharp}=\min\{k_{\beta}-k_{\epsilon},k_{\eta}-k_{\beta},k_{\alpha}-k_{ \eta}-2k_{\epsilon}\}\).

Step 2: Value ConvergenceUnlike matrix games, policy convergence to NE of the regularized game is not enough for convergence in duality gap. We also need to bound \(|V_{t}^{s}-V_{\star}^{s}|\) since the regularized game is defined using \(V_{t}^{s}\), the value function maintained by the algorithm, instead of the minimax game value \(V_{\star}^{s}\). We use the following weighted regret quantities as a proxy: \(\text{Reg}^{s}\triangleq\max_{x,y}\left(\sum_{i=1}^{\tau}\alpha_{\tau}^{i} \left(f_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i}^{s})-f_{i}^{s}(x^{s},\hat{y}_{i}^{s })\right),\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(f_{i}^{s}(\hat{x}_{i}^{s},y_ {i}^{s})-f_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i}^{s})\right)\right)\), where \(\alpha_{\tau}^{i}=\alpha_{i}\prod_{j=i+1}^{\tau}(1-\alpha_{j})\). We can upper bound the weighted regret \(\text{Reg}^{s}_{\tau}\) using a similar analysis as in Step 1 (Lemma 19). We then show a contraction for \(|V_{t_{\tau}(s)}^{s}-V_{\star}^{s}|\) with the weighted regret quantities: \(|V_{t_{\tau}(s)}^{s}-V_{\star}^{s}|\leq\gamma\sum_{i=1}^{\tau}\alpha_{\tau}^{i} \max_{s^{\prime}}|V_{t_{i}(s)}^{s^{\prime}}-V_{\star}^{s^{\prime}}|+\tilde{ \mathcal{O}}(\epsilon_{\tau}+\text{Reg}^{s}_{\tau})\). This leads to the following convergence of \(V_{t}^{s}\) (Lemma 20):\(|V_{t}^{s}-V_{\star}^{s}|\leq\tilde{\mathcal{O}}(t^{-k_{\epsilon}})\), where \(k_{\star}=\min\{k_{\eta},k_{\beta},k_{\alpha}-k_{\beta},k_{\epsilon}\}\).

Obtaining Last-Iterate Convergence RateFix any \(t\) and let \(\tau\) be the number of visits to \(s\) before time \(t\). So far we have shown (1) policy convergence of \(\text{KL}(\hat{z}_{\tau\star}^{s},\hat{z}_{\tau}^{s})\) in the regularized game; (2) and value convergence of \(|V_{t}^{s}-V_{\star}^{s}|\). Using the fact that the regularized game is at most \(\mathcal{O}(\epsilon_{\tau}+|V_{t}^{s}-V_{\star}^{s}|)\) away from the minimax game martrix \(Q^{\star}\) and appropriate choices of parameters proves Theorem 2.

## 6 General Markov Games

In this section, we consider general two-player zero-sum Markov games without Assumption 1. We propose Algorithm 3, an uncoupled learning algorithm that handles exploration and has path convergence rate. Compared to Algorithm 2, the update of value function in Algorithm 3 uses a bonus term \(\textsf{bns}_{\tau}\) based on the optimism principle to handle exploration.

```
1:\(\hat{z}_{\tau\star}^{s}=\max_{x,y}\left(x_{\tau}^{s_{\tau}^{\top}}Q_{\star}^{s_{ \tau}}y^{s_{\tau}}-x_{\tau}^{s_{\tau}^{\top}}Q_{\star}^{s_{\tau}}y_{\tau}^{s_{ \tau}}\right)=\mathcal{O}(t^{-\frac{1}{10}})\)

[MISSING_PAGE_POST]

### Path Convergence

Path convergence has multiple meaningful game-theoretic implications. By definition, It implies that frequent visits to a state bring players' policies closer to equilibrium, leading to both players using near-equilibrium policies for all but \(o(T)\) number of steps over time.

Path convergence also implies that both players have no regret compared to the game value \(V_{*}^{s}\), which has been considered and motivated in previous works such as [1, 10]. To see this, we apply the results to the _episodic_ setting, where in every step, with probability \(1-\gamma\), the state is redrawn from \(s\sim\rho\) for some initial distribution \(\rho\). If the learning dynamics enjoys path convergence, then \(\mathbb{E}[\sum_{t=1}^{T}x_{t}^{s_{t}^{\top}}G^{s_{t}}y_{t}^{s_{t}}]=(1-\gamma )\mathbb{E}_{s\sim\rho}[V_{*}^{s}]T\pm o(T)\). Hence the one-step average reward is \((1-\gamma)\mathbb{E}_{s\sim\rho}[V_{*}^{s}]\) and both players have no regret compared to the game value. A more important implication of path convergence is that it guarantees stability of players' policies, while cycling behaviour is inevitable for any FTRL-type algorithms even in zero-sum matrix games [1, 1]. We defer the proof and more discussion of path convergence to Appendix F.

Finally, we remark that our algorithm is built upon Nash V-learning [1], so it inherits properties of Nash V-learning, e.g., one can still output near-equilibrium policies through policy averaging [10], or having no regret compared to the game value when competing with an arbitrary opponent [10]. We demonstrate extra benefits brought by entropy regularization regarding the stability of the dynamics.

### Analysis Overview of Theorem 3

For general Markov games, it no longer holds that every state \(s\) is visited often, and thus the analysis is much more challenging. We first define two regularized games based on \(\underline{V}_{t}^{s}\) and the corresponding quantity \(\overline{V}_{t}^{s}\) for the \(y\)-player. Define \(t_{\tau}(s)\), \(\hat{x}_{*}^{s}\), \(\hat{y}_{*}^{s}\) the same way as in the previous section. Then define \(f_{\tau}^{s}(x,y)\triangleq x^{\top}(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P ^{s}}[\underline{V}_{t_{\tau}(s)}^{s^{\prime}}])y-\epsilon\phi(x)+\epsilon\phi( y)\), \(\overline{f}_{\tau}^{s}(x,y)\triangleq x^{\top}(G^{s}+\gamma\mathbb{E}_{s^{\prime} \sim P^{s}}[\overline{V}_{t_{\tau}(s)}^{s^{\prime}}])y-\epsilon\phi(x)+\epsilon \phi(y)\) and denote \(J_{t}=\max_{x,y}(x_{t}^{s_{\top}^{\top}}(G^{s_{t}}+\gamma\mathbb{E}_{s^{\prime }\sim P^{s_{t}}}[\overline{V}_{t}^{s^{\prime}}]y^{s_{t}}-x_{t}^{s_{\top}^{ \top}}(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s_{t}}}[\underline{V}_{t}^{s^ {\prime}}])y_{t}^{s_{t}})\). We first bound the "path duality gap" as follows

\[\max_{x,y}\Big{(}x_{t}^{s_{\top}^{\top}}Q_{\star}^{s_{\top}}y^{s}-x_{t}^{s_{ \top}^{\top}}Q_{\star}^{s_{\top}}y_{t}^{s}\Big{)}\leq J_{t}+\mathcal{O}\Big{(} \max_{s^{\prime}}\Big{(}V_{\star}^{s^{\prime}}-\overline{V}_{t}^{s^{\prime}},\underline{V}_{t}^{s^{\prime}}-V_{\star}^{s^{\prime}}\Big{)}\Big{)}.\] (3)

Value Convergence: Bounding \(V_{t}^{s}-V_{\star}^{s}\) and \(V_{\star}^{s}-\overline{V}_{t}^{s}\)This step is similar to Step 2 in the analysis of Algorithm 2. We first show an upper bound of the weighted regret (Lemma 23): \(\sum_{i=1}^{\tau}\alpha_{\tau}^{i}(f_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i}^{s})- \underline{f}_{i}^{s}(x^{s},\hat{y}_{i}^{s}))\leq\frac{1}{2}\mathsf{bns}_{\tau}\), where \(\alpha_{\tau}^{i}=\alpha_{i}\prod_{j=i+1}^{\tau}(1-\alpha_{j})\). Note that the value function \(\underline{V}_{t}^{s}\) is updated using \(\sigma_{t}+\gamma V_{t}^{s_{t}+1}-\mathsf{bns}_{\tau}\). Thus when relating \(|V_{t}^{s}-V_{\star}^{s}|\) to the regret, the regret term and the bonus term cancel out and we get \(\underline{V}_{t}^{s}\leq V_{\star}^{s}+\mathcal{O}(\frac{\epsilon\ln(\overline {AT})}{1-\gamma})\) (Lemma 26). The analysis for \(V_{\star}^{s}-\overline{V}_{t}^{s}\) is symmetric. By proper choice of \(\epsilon\), both terms are bounded by \(\frac{1}{8}u\). Combiningthe above with Eq. (3), we can upper bound the left-hand side of the desired inequality Eq. (2) by \(\sum_{t=1}^{T}\mathbf{1}\left[J_{t}\geq\frac{3}{4}u\right]\), which is further upper bounded in Eq. (29) by

\[\sum_{s}\sum_{\tau=1}^{n_{T+1}(s)}\mathbf{1}\left[\max_{y}\overline {f}_{\tau}^{s}(\hat{x}_{\tau}^{s},y^{s})-\overline{f}_{\tau}^{s}(\hat{z}_{\tau} ^{s})\geq\frac{u}{8}\right]+\sum_{s}\sum_{\tau=1}^{n_{T+1}(s)}\mathbf{1}\left[ \underline{f}_{\tau}^{s}(\hat{z}_{\tau}^{s})-\min_{x}\underline{f}_{\tau}^{s}( x^{s},\hat{y}_{\tau}^{s})\geq\frac{u}{8}\right]\] \[+\sum_{t=1}^{T}\mathbf{1}\left[x_{t}^{s_{t}^{\top}}\left(\gamma \mathbb{E}_{s^{\prime}\sim P^{s_{t}}}\left[\overline{V}_{t}^{s^{\prime}}- \underline{V}_{t}^{s^{\prime}}\right]\right)y_{t}^{s_{t}}\geq\frac{u}{4}\right].\] (4)

Policy Convergence to NE of Regularized GamesTo bound the first two terms, we show convergence of the policy \((\hat{x}_{\tau}^{s},\hat{y}_{\tau}^{s})\) to Nash equilibria of both games \(f_{\tau}^{s}\) and \(\overline{f}_{\tau}^{s}\). To this end, fix any \(p\in[0,1]\), we define \(f_{\tau}^{s}=p\underline{f}_{\tau}^{s}+(1-p)\overline{f}_{\tau}^{s}\) and let \(\hat{z}_{\tau\star}^{s}=(\hat{x}_{\tau\star}^{s},\hat{y}_{\tau\star}^{s})\) be the equilibrium of \(f_{\tau}^{s}(x,y)\). The analysis is similar to previous algorithms where we first conduct single-step analysis (Lemma 22) and then carefully bound the weighted recursive terms. We show in Lemma 27 that for any \(0<\epsilon^{\prime}\leq 1\): \(\sum_{s}\sum_{\tau=1}^{n_{T+1}(s)}\mathbf{1}\left[\text{KL}(\hat{z}_{\tau \star}^{s},\hat{z}_{\tau}^{s})\geq\epsilon^{\prime}\right]\leq\mathcal{O}( \frac{S^{2}A\ln^{5}(SAT/\delta)}{\eta\epsilon^{2}\epsilon^{\prime}(1-\gamma) ^{3}})\). This proves policy convergence: the number of iterations where the policy is far away from Nash equilibria of the regularized games is bounded, which can then be translated to upper bounds on the first two terms.

Value Convergence: Bounding \(|\overline{V}_{t}^{s}-\underline{V}_{t}^{s}|\)It remains to bound the last term in Eq. (4). Define \(c_{t}=\mathbf{1}[x_{t}^{s_{t}}(\mathbb{E}_{s^{\prime}\sim P^{s_{t}}}[ \overline{V}_{t}^{s}-\underline{V}_{t}^{s^{\prime}}])y_{t}^{s_{t}}\geq \tilde{\epsilon}]\) where \(\tilde{\epsilon}=\frac{u}{4}\). Then we only need to bound \(C\triangleq\sum_{t=1}^{T}c_{t}\). We use the weighted sum \(P_{T}\triangleq\sum_{t=1}^{T}c_{t}\). On the other hand, in Lemma 25, by recursively tracking the update of the value function and carefully choosing \(\eta\) and \(\beta\), we upper bound \(P_{T}\) by \(\leq\frac{C\tilde{\epsilon}}{2}+\mathcal{O}(\frac{AS\ln^{4}(AST/\delta)}{\eta( 1-\gamma)^{3}})\). Combining the upper and lower bound of \(P_{T}\) gives \(C\leq\mathcal{O}(\frac{AS\ln^{4}(AST/\delta)}{\eta u(1-\gamma)^{3}})\) (Corollary 2). Plugging appropriate choices of \(\epsilon\), \(\eta\), and \(\beta\) in the above bounds proves Theorem 3 (see Appendix E).

## 7 Conclusion and Future Directions

In this work, we study decentralized learning in two-player zero-sum Markov games with bandit feedback. We propose the first uncoupled and convergent algorithms with non-asymptotic last-iterate convergence rates for matrix games and irreducible Markov games, respectively. We also introduce a novel notion of path convergence and provide algorithm with path convergence in Markov games without any assumption on the dynamics. Previous results either focus on average-iterate convergence or require stronger feedback/coordination or lack non-asymptotic convergence rates. Our results contribute to the theoretical understanding of the practical success of regularization and last-iterate convergence in multi-agent reinforcement learning.

Settling the optimal last-iterate convergence rate that is achievable by uncoupled learning dynamics is an important open question. The following directions are promising towards closing the gap between current upper bounds \(\mathcal{O}(T^{-1/8})\) and \(O(T^{-1/(9+\varepsilon)})\) and lower bound \(\Omega(T^{-\frac{1}{2}})\), The impossibility result by [14] demonstrates that certain algorithms with \(\mathcal{O}(\sqrt{T})\) regret diverge in last-iterate. Their result indicates that the current \(\Omega(\frac{1}{\sqrt{T}})\) lower bound on convergence rate may not be tight. On the other hand, our algorithms provides insights and useful templates to potential improvements on the upper bound. For instance, instead of using EXP3-IX update, adapting optimistic policy update or other accelerated first-order methods to the bandit feedback setting is an interesting future direction.

AcknowledgementWe thank Chanwoo Park and Kaiqing Zhang for pointing out a mistake in our previous proof. We also thank the anonymous reviewers for their constructive feedback. HL is supported by NSF Award IIS-1943607 and a Google Research Scholar Award.

## References

* [AVHC22] Ahmet Alacaoglu, Luca Viano, Niao He, and Volkan Cevher. A natural actor-critic framework for zero-sum markov games. In _International Conference on Machine Learning_, pages 307-366. PMLR, 2022.
* [AY16] Gurdal Arslan and Serdar Yuksel. Decentralized q-learning for stochastic teams and games. _IEEE Transactions on Automatic Control_, 62(4):1545-1558, 2016.
* [BJ20] Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In _International conference on machine learning_, pages 551-560. PMLR, 2020.
* [BJY20] Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. _Advances in neural information processing systems_, 33:2159-2170, 2020.
* [BLM18] Mario Bravo, David Leslie, and Panayotis Mertikopoulos. Bandit learning in concave n-person games. _Advances in Neural Information Processing Systems_, 31, 2018.
* [BP18] James P Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In _Proceedings of the 2018 ACM Conference on Economics and Computation_, 2018.
* [BT02] Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. _Journal of Machine Learning Research_, 3(Oct):213-231, 2002.
* [BV01] Michael Bowling and Manuela Veloso. Rational and convergent learning in stochastic games. In _Proceedings of the 17th international joint conference on Artificial intelligence-Volume 2_, pages 1021-1026, 2001.
* [CCDX23] Shicong Cen, Yuejie Chi, Simon Shaolei Du, and Lin Xiao. Faster last-iterate convergence of policy optimization in zero-sum markov games. In _International Conference on Learning Representations_, 2023.
* [CLW21] Liyu Chen, Haipeng Luo, and Chen-Yu Wei. Impossible tuning made possible: A new expert algorithm and its applications. In _Conference on Learning Theory_, pages 1216-1259. PMLR, 2021.
* [CMZ21] Ziyi Chen, Shaocong Ma, and Yi Zhou. Sample efficient stochastic policy extragradient algorithm for zero-sum markov game. In _International Conference on Learning Representations_, 2021.
* [COZ22] Yang Cai, Argyris Oikonomou, and Weiqiang Zheng. Finite-time last-iterate convergence for learning in multi-player games. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [CS07] Vincent Conitzer and Tuomas Sandholm. Awesome: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents. _Machine Learning_, 67(1-2):23-43, 2007.
* [CWC21] Shicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive games with entropy regularization. _Advances in Neural Information Processing Systems_, 34:27952-27964, 2021.
* [CZ23] Yang Cai and Weiqiang Zheng. Doubly optimal no-regret learning in monotone games. In _International Conference on Machine Learning_, 2023. to appear.
* [CZG22] Zixiang Chen, Dongruo Zhou, and Quanquan Gu. Almost optimal algorithms for two-player zero-sum markov games with linear function approximation. In _International Conference on Algorithmic Learning Theory_, 2022.
* [DDK11] Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms for zero-sum games. In _Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms_, pages 235-254. SIAM, 2011.

* [DFG20] Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods for competitive reinforcement learning. _Advances in neural information processing systems_, 33:5527-5540, 2020.
* [DFR22] Dmitriy Drusvyatskiy, Maryam Fazel, and Lillian J Ratliff. Improved rates for derivative free gradient play in strongly monotone games. In _IEEE 61st Conference on Decision and Control (CDC)_, 2022.
* [ELS\({}^{+}\)23] Liad Erez, Tal Lancewicki, Uri Sherman, Tomer Koren, and Yishay Mansour. Regret minimization and convergence to equilibria in general-sum markov games. In _International Conference on Machine Learning_, pages 9343-9373. PMLR, 2023.
* [FT91] Jerzy A Filar and Boleslaw Tolwinski. On the algorithm of pollatschek and axi-ltzhak. 1991.
* [GPD20] Noah Golowich, Sarath Pattathil, and Constantinos Daskalakis. Tight last-iterate convergence rates for no-regret learning in multi-player games. _Advances in neural information processing systems_, 2020.
* [GTG22] Eduard Gorbunov, Adrien Taylor, and Gauthier Gidel. Last-iterate convergence of optimistic gradient method for monotone variational inequalities. In _Advances in Neural Information Processing Systems_, 2022.
* [GVGM21] Angeliki Giannou, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Panayotis Mertikopoulos. On the rate of convergence of regularized learning in games: From bandits and uncertainty to optimism and beyond. _Advances in Neural Information Processing Systems_, 34:22655-22666, 2021.
* [HH23] Yuanhanqing Huang and Jianghai Hu. Zeroth-order learning in continuous games via residual pseudogradient estimates. _arXiv preprint arXiv:2301.02279_, 2023.
* [HIMM19] Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. In _Advances in Neural Information Processing Systems_, 2019.
* [HK66] Alan J Hoffman and Richard M Karp. On nonterminating stochastic games. _Management Science_, 1966.
* [HLWY22] Baihe Huang, Jason D. Lee, Zhaoran Wang, and Zhuoran Yang. Towards general function approximation in zero-sum markov games. In _International Conference on Learning Representations_, 2022.
* [HW03] Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. _Journal of machine learning research_, 4(Nov):1039-1069, 2003.
* [JJIN21] Mehdi Jafarnia-Jahromi, Rahul Jain, and Ashutosh Nayyar. Learning zero-sum stochastic games with posterior sampling. _arXiv preprint arXiv:2109.03396_, 2021.
* [JLWY21] Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning-a simple, efficient, decentralized algorithm for multiagent rl. _arXiv preprint arXiv:2110.14555_, 2021.
* [JLY22] Chi Jin, Qinghua Liu, and Tiancheng Yu. The power of exploiter: Provable multi-agent rl in large state spaces. In _International Conference on Machine Learning_, pages 10251-10279. PMLR, 2022.
* [LH14] Tor Lattimore and Marcus Hutter. Near-optimal pac bounds for discounted mdps. _Theoretical Computer Science_, 558:125-143, 2014.
* [Lit94] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In _Machine learning proceedings 1994_, pages 157-163. Elsevier, 1994.
* [LS19] Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks. In _The 22nd International Conference on Artificial Intelligence and Statistics_, 2019.

* [LYBJ21] Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning with self-play. In _International Conference on Machine Learning_, pages 7001-7010. PMLR, 2021.
* [LZBZ21] Tianyi Lin, Zhengyuan Zhou, Wenjia Ba, and Jiawei Zhang. Doubly optimal no-regret online learning in strongly monotone games with bandit feedback. _Available at SSRN 3978421_, 2021.
* [MOP20] Aryan Mokhtari, Asuman E Ozdaglar, and Sarath Pattathil. Convergence rate of \(o(1/k)\) for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 30(4):3230-3251, 2020.
* [MPP18] Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial regularized learning. In _Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 2703-2717. SIAM, 2018.
* [MPS20] Vidya Muthukumar, Soham Phade, and Anant Sahai. On the impossibility of convergence of mixed strategies with no regret learning. _arXiv preprint arXiv:2012.02125_, 2020.
* [Neu15] Gergely Neu. Explore no more: Improved high-probability regret bounds for nonstochastic bandits. _Advances in Neural Information Processing Systems_, 28, 2015.
* [PAI69] MA Pollatschek and B Avi-Itzhak. Algorithms for stochastic games with geometrical interpretation. _Management Science_, 1969.
* [PDVH\({}^{+}\)22] Julien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul Muller, Jerome T Connor, Neil Burch, Thomas Anthony, et al. Mastering the game of stratego with model-free multiagent reinforcement learning. _Science_, 378(6623):990-996, 2022.
* [Put14] Martin L Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* [Sha53] Lloyd S Shapley. Stochastic games. _Proceedings of the national academy of sciences_, 1953.
* [SL99] Csaba Szepesvari and Michael L Littman. A unified analysis of value-function-based reinforcement-learning algorithms. _Neural computation_, 1999.
* [SLY23] Zhuoqing Song, Jason D. Lee, and Zhuoran Yang. Can we find nash equilibria at a linear rate in markov games? In _International Conference on Learning Representations_, 2023.
* [SPO22] Muhammed O Sayin, Francesca Parise, and Asuman Ozdaglar. Fictitious play in zero-sum stochastic games. _SIAM Journal on Control and Optimization_, 60(4):2095-2114, 2022.
* [SSBD14] Shai Shalev-Shwartz and Shai Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* [SSS\({}^{+}\)17] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. _Nature_, 2017.
* [SZL\({}^{+}\)21] Muhammed Sayin, Kaiqing Zhang, David Leslie, Tamer Basar, and Asuman Ozdaglar. Decentralized q-learning in zero-sum markov games. _Advances in Neural Information Processing Systems_, 34:18320-18334, 2021.
* [TK22] Tatiana Tatarenko and Maryam Kamgarpour. On the rate of convergence of payoff-based algorithms to nash equilibrium in strongly monotone games. _arXiv preprint arXiv:2202.11147_, 2022.
* [Tse95] Paul Tseng. On linear convergence of iterative methods for the variational inequality problem. _Journal of Computational and Applied Mathematics_, 60(1-2):237-252, 1995.

* [TWYS20] Yi Tian, Yuanhao Wang, Tiancheng Yu, and Suvrit Sra. Provably efficient online agnostic learning in markov games. _arXiv preprint arXiv:2010.15020_, 2020.
* [VDW78] J Van Der Wal. Discounted markov games: Generalized policy iteration method. _Journal of Optimization Theory and Applications_, 1978.
* [vN28] J v. Neumann. Zur theorie der gesellschaftsspiele. _Mathematische annalen_, 100(1):295-320, 1928.
* [WDCW20] Yuanhao Wang, Kefan Dong, Xiaoyu Chen, and Liwei Wang. Q-learning with ucb exploration is sample efficient for infinite-horizon mdp. In _International Conference on Learning Representations_, 2020.
* [WHL17] Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. In _Advances in Neural Information Processing Systems_, pages 4987-4997, 2017.
* [WLZL21a] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games. In _Conference on learning theory_, pages 4259-4299. PMLR, 2021.
* [WLZL21b] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained saddle-point optimization. In _International Conference on Learning Representations (ICLR)_, 2021.
* [XCWY20] Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium. In _Conference on learning theory_, pages 3674-3682. PMLR, 2020.
* [XZS\({}^{+}\)22] Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, and Tong Zhang. A self-play posterior sampling algorithm for zero-sum markov games. In _ICLR 2022 Workshop on Gamification and Multiagent Solutions_, 2022.
* [YM23] Yuepeng Yang and Cong Ma. \(O(T^{-1})\) convergence of optimistic-follow-the-regularized-leader in two-player zero-sum markov games. In _International Conference on Learning Representations_, 2023.
* [ZLW\({}^{+}\)22] Runyu Zhang, Qinghua Liu, Huan Wang, Caiming Xiong, Na Li, and Yu Bai. Policy optimization for markov games: Unified framework and faster convergence. In _Advances in Neural Information Processing Systems_, 2022.
* [ZTLD22] Yulai Zhao, Yuandong Tian, Jason Lee, and Simon Du. Provably efficient policy optimization for two-player zero-sum markov games. In _International Conference on Artificial Intelligence and Statistics_, pages 2736-2761. PMLR, 2022.

* Auxiliary Lemmas
* A.1 Sequence Properties
* A.2 Properties Related to Exp3-IX
* A.3 Markov Games
* A.4 Online Mirror Descent
* B Last-Iterate Convergence Rate of Algorithm 1
* C Improved Last-Iterate Convergence under Expectation
* D Last-Iterate Convergence Rate of Algorithm 2
* D.1 On the Assumption of Irreducible Markov Game
* D.2 Part I. Basic Iteration Properties
* D.3 Part II. Policy Convergence to the Nash of Regularized Game
* D.4 Part III. Value Convergence
* D.5 Part IV. Combining
* E Convergent Analysis of Algorithm 3
* E.1 Part I. Basic Iteration Properties
* E.2 Part II. Value Convergence
* E.3 Part III. Policy Convergence to the Nash of the Regularized Game
* E.4 Part IV. Combining
* F Discussions on Convergence Notions for General Markov Games

## Appendix A Auxiliary Lemmas

### Sequence Properties

**Lemma 1**.: _Let \(0<h<1\), \(0\leq k\leq 2\), and let \(t\geq\left(\frac{24}{1-h}\ln\frac{12}{1-h}\right)^{\frac{1}{1-h}}\). Then_

\[\sum_{i=1}^{t}\left(i^{-k}\prod_{j=i+1}^{t}(1-j^{-h})\right)\leq 9\ln(t)t^{-k+h}.\]

Proof.: Define

\[s\triangleq\left\lceil(k+1)t^{h}\ln t\right\rceil\]

We first show that \(s\leq\frac{t}{2}\). Suppose not, then we have

\[(k+1)t^{h}\ln t>\frac{t}{2}-1\geq\frac{t}{4}\] (because \[t\geq 12>4\] )

and thus \(t^{1-h}<4(k+1)\ln t\leq 12\ln t\). However, by the condition for \(t\) and Lemma 3, it holds that \(t^{1-h}\geq 12\ln t\), which leads to contradiction.

Then the sum can be decomposed as

\[\sum_{i=1}^{t-s}i^{-k}\prod_{j=i+1}^{t}(1-j^{-h})+\sum_{i=t-s+1}^{t}i ^{-k}\prod_{j=i+1}^{t}(1-j^{-h})\] \[\leq t\times(1-t^{-h})^{s}+s\left(t-s+1\right)^{-k}\] \[\leq t\times(e^{-t^{-h}})^{s}+s\times\left(\frac{t}{2}\right)^{-k}\] \[\leq t\times e^{-(k+1)\ln t}+s\times 2^{k}\times t^{-k}\] \[\leq t^{-k}+\left((k+1)t^{h}\ln t+1\right)\times 2^{k}\times t^{-k}\] \[\leq 9\ln(t)t^{-k+h}.\]

**Lemma 2**.: _Let \(0<h<1\), \(0\leq k\leq 2\), and let \(t\geq\left(\frac{24}{1-h}\ln\frac{12}{1-h}\right)^{\frac{1}{1-h}}\). Then_

\[\max_{1\leq i\leq t}\left(i^{-k}\prod_{j=i+1}^{t}(1-j^{-h})\right)\leq 4t^{-k}.\]

Proof.: \[\max_{\frac{t}{2}\leq i\leq t}\left(i^{-k}\prod_{j=i+1}^{t}(1-j^{-h})\right) \leq\left(\frac{t}{2}\right)^{-k}\leq 2^{2}t^{-k}=4t^{-k}\]

\[\max_{1\leq i\leq\frac{t}{2}}\left(i^{-k}\prod_{j=i+1}^{t}(1-j^{-h})\right) \leq\left(1-t^{-h}\right)^{\frac{t}{2}}\leq\left(\exp\left(-t^{-h} \right)\right)^{\frac{t}{2}}=\exp\left(-\frac{1}{2}t^{1-h}\right)\] \[\stackrel{{(a)}}{{\leq}}\exp\left(-\frac{1}{2}\times 1 2\ln t\right)=\frac{1}{t^{6}}\leq t^{-k}.\]

where in \((a)\) we use Lemma 3. Combining the two inequalities finishes the proof. 

**Lemma 3**.: _Let \(0<h<1\) and \(t\geq\left(\frac{24}{1-h}\ln\frac{12}{1-h}\right)^{\frac{1}{1-h}}\). Then \(t^{1-h}\geq 12\ln t\)._

Proof.: By the condition, we have

\[t^{1-h}\geq 2\times\frac{12}{1-h}\ln\frac{12}{1-h}.\]

Applying Lemma 12, we get

\[t^{1-h}\geq\frac{12}{1-h}\ln(t^{1-h})=12\ln t.\]

**Lemma 4** (Lemma A.1 of [10]).: _Let \(a>0\). Then \(x\geq 2a\ln(a)\Rightarrow x\geq a\ln(x)\)._

**Lemma 5** (Freedman's Inequality).: _Let \(\mathcal{F}_{0}\subset\mathcal{F}_{1}\subset\cdots\subset\mathcal{F}_{n}\) be a filtration, and \(X_{1},\ldots,X_{n}\) be real random variables such that \(X_{i}\) is \(\mathcal{F}_{i}\)-measurable, \(\mathbb{E}[X_{i}|\mathcal{F}_{i-1}]=0\), \(|X_{i}|\leq b\), and \(\sum_{i=1}^{n}\mathbb{E}[X_{i}^{2}|\mathcal{F}_{i-1}]\leq V\) for some fixed \(b>0\) and \(V>0\). Then with probability at least \(1-\delta\),_

\[\sum_{i=1}^{n}X_{i}\leq 2\sqrt{V\log(1/\delta)}+b\log(1/\delta).\]

[MISSING_PAGE_FAIL:17]

### Markov Games

**Lemma 10** ([11]).: _For any policy pair \(x,y\), the duality gap on a two player zero-sum game can be related to duality gap on individual states:_

\[\max_{s,x^{\prime},y^{\prime}}\big{(}V_{x,y^{\prime}}^{s}-V_{x^{\prime},y}^{s} \big{)}\leq\frac{2}{1-\gamma}\max_{s,x^{\prime},y^{\prime}}(x^{s}Q_{*}^{s}y^{ \prime s}-x^{\prime s}Q_{*}^{s}y^{s}).\]

### Online Mirror Descent

**Lemma 11**.: _Let_

\[x^{\prime}=\operatorname*{argmin}_{\tilde{x}\in\Omega}\left\{\sum_{a\in \mathcal{A}}\tilde{x}_{a}\left(\ell_{a}+\epsilon_{a}\ln x_{a}\right)+\frac{1} {\eta}\text{KL}(\tilde{x},x)\right\}\]

_for some convex set \(\Omega\subseteq\Delta_{\mathcal{A}}\), \(\ell\in[0,\infty)^{A}\), and \(\epsilon\in[0,\frac{1}{\eta}]^{A}\). Then_

\[(x-u)^{\top}(\ell+\epsilon\ln x)\leq\frac{\text{KL}(u,x)-\text{KL}(u,x^{ \prime})}{\eta}+\eta\sum_{a\in\mathcal{A}}x_{a}(\ell_{a})^{2}+\eta\sum_{a\in \mathcal{A}}\epsilon_{a}^{2}\ln^{2}x_{a}.\]

_for any \(u\in\Omega\), where \(\epsilon\ln x\) denotes the vector \((\epsilon_{a}\ln x_{a})_{a\in\mathcal{A}}\)._

Proof.: By the standard analysis of online mirror descent, we have for any \(u\in\Omega\)

\[(x-u)^{\top}(\ell+\epsilon\ln x)\leq\frac{\text{KL}(u,x)-\text{KL}(u,x^{ \prime})}{\eta}+(x-x^{\prime})^{\top}(\ell+\epsilon\ln x)-\frac{1}{\eta}\text {KL}(x^{\prime},x).\]

Below, we abuse the notation by defining \(\text{KL}(\tilde{x},x)=\sum_{a}(\tilde{x}_{a}\ln\frac{\tilde{x}_{a}}{\tilde{x }_{a}}-\tilde{x}_{a}+x_{a})\) without restricting \(\tilde{x}\) to be a probability vector. Then following the analysis in the proof of Lemma 1 of [10], we have

\[(x-x^{\prime})^{\top}(\ell+\epsilon\ln x)-\frac{1}{\eta}\text{KL} (x^{\prime},x)\] \[\leq\max_{y\in\mathbb{R}_{+}^{4}}\left\{(x-y)^{\top}(\ell+ \epsilon\ln x)-\frac{1}{\eta}\text{KL}(y,x)\right\}\] \[=\frac{1}{\eta}\sum_{a}x_{a}\left(\eta(\ell_{a}+\epsilon_{a}\ln x _{a})-1+e^{-\eta(\ell_{a}+\epsilon_{a}\ln x_{a})}\right)\] \[\leq\frac{1}{\eta}\sum_{a}x_{a}\left(\eta\epsilon_{a}\ln x_{a}+ \eta^{2}\ell_{a}^{2}-e^{-\eta\ell_{a}}+e^{-\eta\ell_{a}}x_{a}^{-\eta\epsilon_{ a}}\right) (z-1\leq z^{2}-e^{-z}\text{ for }z\geq 0)\] \[=\eta\sum_{a}x_{a}\ell_{a}^{2}+\frac{1}{\eta}\sum_{a}\left(\eta \epsilon_{a}x_{a}\ln x_{a}+e^{-\eta\ell_{a}}\left(x_{a}^{1-\eta\epsilon_{a}}- x_{a}\right)\right)\] \[\leq\eta\sum_{a}x_{a}\ell_{a}^{2}+\frac{1}{\eta}\sum_{a}\left(\eta \epsilon_{a}x_{a}\ln x_{a}+\left(x_{a}^{1-\eta\epsilon_{a}}-x_{a}\right) \right) (\eta\ell_{a}\geq 0\text{ and }x_{a}^{1-\eta\epsilon_{a}}-x_{a}\geq 0)\] \[\leq\eta\sum_{a}x_{a}\ell_{a}^{2}+\frac{1}{\eta}\sum_{a}\left(\eta \epsilon_{a}x_{a}\ln x_{a}-\eta\epsilon_{a}x^{1-\eta\epsilon_{a}}\ln x_{a}\right) (\text{by Lemma \ref{lem:lemma}})\] \[\leq\eta\sum_{a}x_{a}\ell_{a}^{2}+\frac{1}{\eta}\sum_{a}(\eta \epsilon_{a}\ln x_{a})^{2}x_{a}^{1-\eta\epsilon_{a}} (\text{by Lemma \ref{lem:lemma}})\] \[\leq\eta\sum_{a}x_{a}\ell_{a}^{2}+\frac{1}{\eta}\sum_{a}(\eta \epsilon_{a}\ln x_{a})^{2}. (\eta\epsilon_{a}\leq 1\text{ and }x_{a}\in(0,1))\]

**Lemma 12**.: _For \(x\in(0,1)\) and \(y>0\), we have \(x^{1-y}-x\leq-yx^{1-y}\ln x\)._Proof.: \[x^{1-y}-x =e^{(\ln\frac{1}{x})(y-1)}-e^{(\ln\frac{1}{x})(-1)}\] \[=y\left(\ln\frac{1}{x}\right)e^{(\ln\frac{1}{x})\tilde{y}}\] (for some

\[\tilde{y}\in[-1,y-1]\]

) \[\leq y\left(\ln\frac{1}{x}\right)e^{(\ln\frac{1}{x})(-1+y)}\] \[=-y(\ln x)x^{1-y}\]

where the second equality is by the mean value theorem. 

## Appendix B Last-Iterate Convergence Rate of Algorithm 1

Proof of Theorem 1.: The proof is divided into three parts. In Part I, we establish a descent inequality for \(\text{KL}(z_{t}^{\star},z_{t})\). In Part II, we give an upper bound \(\text{KL}(z_{t}^{\star},z_{t})\) by recursively applying the descent inequality. Finally in Part III, we show last-iterate convergence rate on the duality gap of \(z_{t}=(x_{t},y_{t})\). In the proof, we assume without loss of generality that \(t\geq t_{0}=(\frac{24}{1-k_{\eta}-k_{\epsilon}}\ln(\frac{12}{1-k_{\eta}-k_{ \epsilon}}))^{\frac{1}{1-k_{\eta}-k_{\epsilon}}}=(96\ln(48))^{4}\) since the theorem holds trivially for constant \(t\).

Part I. \[f_{t}(x_{t},y_{t})-f_{t}(x_{t}^{\star},y_{t})\] \[=(x_{t}-x_{t}^{\star})^{\top}Gy_{t}+\epsilon_{t}\left(\sum_{a}x_ {t,a}\ln x_{t,a}-\sum_{a}x_{t,a}^{\star}\ln x_{t,a}^{\star}\right)\] \[=(x_{t}-x_{t}^{\star})^{\top}Gy_{t}+\epsilon_{t}\left(\sum_{a}(x _{t,a}-x_{t,a}^{\star})\ln x_{t,a}\right)-\epsilon_{t}\underbrace{\sum_{a}x_{t,a}^{\star}\left(\ln x_{t,a}^{\star}-\ln x_{t,a}\right)}_{=\text{KL}(x_{t}^{ \star},x_{t})}\] \[=(x_{t}-x_{t}^{\star})^{\top}g_{t}-\epsilon_{t}\text{KL}(x_{t}^{ \star},x_{t})+\underbrace{\sum_{a}x_{t,a}\left((Gy_{t})_{a}-\frac{\mathbf{1}[ a_{t}=a]\sigma_{t}}{x_{t,a}+\beta_{t}}\right)}_{\triangleq\xi_{t}}+\underbrace{ \sum_{a}x_{t,a}^{\star}\left(\frac{\mathbf{1}[a_{t}=a]\sigma_{t}}{x_{t,a}+\beta _{t}}-(Gy_{t})_{a}\right)}_{\triangleq\zeta_{t}}\] (by the definition of

\[g_{t}\] \[\leq\frac{\text{KL}(x_{t}^{\star},x_{t})-\text{KL}(x_{t}^{\star},x _{t+1})}{\eta_{t}}+\eta_{t}\sum_{a}x_{t,a}\left(\frac{\mathbf{1}[a_{t}=a]}{x_{ t,a}+\beta_{t}}\right)^{2}+\eta_{t}\sum_{a}\epsilon_{t}^{2}\ln^{2}(x_{t,a})- \epsilon_{t}\text{KL}(x_{t}^{\star},x_{t})+\underline{\xi}_{t}+\underline{ \zeta}_{t}\] (by Lemma 11) \[\leq\frac{(1-\eta_{t}\epsilon_{t})\text{KL}(x_{t}^{\star},x_{t})- \text{KL}(x_{t}^{\star},x_{t+1})}{\eta_{t}}+2\eta_{t}\sum_{a}\left(\frac{ \mathbf{1}[a_{t}=a]}{x_{t,a}+\beta_{t}}+\epsilon_{t}^{2}\ln^{2}(x_{t,a}) \right)+\underline{\xi}_{t}+\underline{\zeta}_{t}\] \[\leq\frac{(1-\eta_{t}\epsilon_{t})\text{KL}(x_{t}^{\star},x_{t})- \text{KL}(x_{t}^{\star},x_{t+1})}{\eta_{t}}+10\eta_{t}A\ln^{2}(At)+2\eta_{t}A \underline{\lambda}_{t}+\underline{\xi}_{t}+\underline{\zeta}_{t}.\] (5)

Rearranging the above inequality, we get

\[\text{KL}(x_{t+1}^{\star},x_{t+1})\] \[\leq(1-\eta_{t}\epsilon_{t})\text{KL}(x_{t}^{\star},x_{t})+\eta_{ t}(f_{t}(x_{t}^{\star},y_{t})-f_{t}(x_{t},y_{t}))+10\eta_{t}^{2}A\ln^{2}(At)+2\eta_{t}^{2}A \underline{\lambda}_{t}+\eta_{t}\underline{\xi}_{t}+\eta_{t}\underline{\zeta} _{t}+\underline{v}_{t},\]where \(\underline{v}_{t}\triangleq\text{KL}(x_{t+1}^{\star},x_{t+1})-\text{KL}(x_{t}^{ \star},x_{t+1})\). Similarly, since the algorithm for the \(y\)-player is symmetric, we have the following:

\[\text{KL}(y_{t+1}^{\star},y_{t+1})\] \[\leq(1-\eta_{t}\epsilon_{t})\text{KL}(y_{t}^{\star},y_{t})+\eta_{ t}(f_{t}(x_{t},y_{t})-f_{t}(x_{t},y_{t}^{\star}))+10\eta_{t}^{2}A\ln^{2}{(At)}+2 \eta_{t}^{2}A\overline{\lambda}_{t}+\eta_{t}\overline{\xi}_{t}+\eta_{t} \overline{\zeta}_{t}+\overline{v}_{t}\]

where

\[\overline{\lambda}_{t} \triangleq\frac{1}{A}\sum_{b}\left(\frac{\mathbf{1}[b_{t}=b]}{y_ {t,b}+\beta_{t}}-1\right)\] \[\overline{\xi}_{t} \triangleq\sum_{b}y_{t,b}\left(\left(-(G^{\top}x_{t})_{b}+1 \right)-\frac{\mathbf{1}[b_{t}=b](-\sigma_{t}+1)}{y_{t,b}+\beta_{t}}\right)\] \[\overline{\zeta}_{t} \triangleq\sum_{b}y_{t,b}^{\star}\left(\frac{\mathbf{1}[b_{t}=b] (-\sigma_{t}+1)}{y_{t,b}+\beta_{t}}-\left(-(G^{\top}x_{t})_{b}+1\right)\right)\] \[\overline{v}_{t} \triangleq\text{KL}(y_{t+1}^{\star},y_{t+1})-\text{KL}(y_{t}^{ \star},y_{t+1}).\]

Adding the two inequalities above up and using the fact that \(f_{t}(x_{t}^{\star},y_{t})-f_{t}(x_{t},y_{t}^{\star})\leq 0\), we get

\[\text{KL}(z_{t+1}^{\star},z_{t+1})\leq(1-\eta_{t}\epsilon_{t})\text{KL}(z_{t}^ {\star},z_{t})+20\eta_{t}^{2}A\ln^{2}{(At)}+2\eta_{t}^{2}A\lambda_{t}+\eta_{t} \xi_{t}+\eta_{t}\zeta_{t}+v_{t},\] (6)

where \(\square\triangleq\square+\overline{\square}\) for \(\square=\lambda_{t},\xi_{t},\zeta_{t},v_{t}\).

Part II.Expanding the recursion in Eq. (6), and using the fact that \(1-\eta_{1}\epsilon_{1}=0\), we get

\[\text{KL}(z_{t+1}^{\star},z_{t+1}) \leq\underbrace{20A\ln^{2}(At)\sum_{i=1}^{t}w_{t}^{i}\eta_{i}^{2 }}_{\text{\bf term}_{1}}+\underbrace{2A\sum_{i=1}^{t}w_{t}^{i}\eta_{i}^{2} \lambda_{i}}_{\text{\bf term}_{2}}+\underbrace{\sum_{i=1}^{t}w_{t}^{i}\eta_{i} \xi_{i}}_{\text{\bf term}_{3}}+\underbrace{\sum_{i=1}^{t}w_{t}^{i}\eta_{i} \zeta_{i}}_{\text{\bf term}_{4}}+\underbrace{\sum_{i=1}^{t}w_{t}^{i}v_{i}}_{ \text{\bf term}_{5}}\]

where \(w_{t}^{i}\triangleq\prod_{j=i+1}^{t}(1-\eta_{j}\epsilon_{j})\). We can bound each term as follows.

By Lemma 1 and the fact that that \(t\geq t_{0}\), we have

\[\text{\bf term}_{1}\leq\mathcal{O}\left(A\ln^{2}(At)\ln(t)t^{-2k_{\eta}+(k_ {\eta}+k_{\epsilon})}\right)=\mathcal{O}\left(A\ln^{3}(At)t^{-k_{\eta}+k_{ \epsilon}}\right)=\mathcal{O}\left(A\ln^{3}(At)t^{-\frac{1}{2}}\right).\]

Using Lemma 7 with \(x^{\star}=\frac{1}{A}\mathbf{1}\), \(\ell_{i}=\mathbf{1}\) for all \(i\), and \(c_{i}=w_{t}^{i}\eta_{i}^{2}\), we have with probability \(1-\frac{\delta}{t^{2}}\),

\[\text{\bf term}_{2}=\mathcal{O}\left(\frac{A\ln(At/\delta)\max_{i\leq t}c_{i} }{\beta_{t}}\right)\stackrel{{(a)}}{{=}}\mathcal{O}\left(A\ln(At/ \delta)t^{k_{\beta}}\times t^{-2k_{\eta}}\right)=\mathcal{O}\left(A\ln(At/ \delta)t^{-\frac{1}{2}}\right)\]

where in \((a)\) we use Lemma 2 with the fact that \(t\geq t_{0}\).

Using Lemma 6 with \(c_{i}=w_{t}^{i}\eta_{i}\), we have with probability at least \(1-\frac{\delta}{t^{2}}\),

\[\text{\bf term}_{3} \leq\mathcal{O}\left(A\sum_{i=1}^{t}\beta_{i}c_{i}+\sqrt{\ln(At/ \delta)\sum_{i=1}^{t}c_{i}^{2}}\right)\] \[=\mathcal{O}\left(A\sum_{i=1}^{t}\left[i^{-k_{\beta}-k_{\eta}} \prod_{j=i+1}^{t}\left(1-j^{-k_{\eta}-k_{\epsilon}}\right)\right]+\sqrt{\ln(At /\delta)\sum_{i=1}^{t}\left[i^{-2k_{\eta}}\prod_{j=i+1}^{t}\left(1-j^{-k_{ \eta}-k_{\epsilon}}\right)\right]}\right)\] \[=\mathcal{O}\left(A\ln(t)t^{-k_{\beta}+k_{\epsilon}}+t^{-\frac{ 1}{2}k_{\eta}+\frac{1}{2}k_{\epsilon}}\log(At/\delta)\right)\] (by Lemma 1 and \[t\geq t_{0}\] ) \[=\mathcal{O}\left(A\log(At/\delta)t^{-\frac{1}{4}}\right).\]

Using Lemma 7 with \(c_{i}=w_{t}^{i}\eta_{i}\), we get with probability at least \(1-\frac{\delta}{t^{2}}\),

\[\text{\bf term}_{4}=\mathcal{O}\left(\frac{\ln(At/\delta)\max_{i\leq t}c_{i}} {\beta_{t}}\right)\stackrel{{(a)}}{{\leq}}\mathcal{O}\left(\ln(At/ \delta)t^{-k_{\eta}+k_{\beta}}\right)=\mathcal{O}\left(\ln(At/\delta)t^{-\frac{1 }{4}}\right)\]where \((a)\) is by Lemma 2 and \(t\geq t_{0}\).

By Lemma 13 and Lemma 1,

\[\textbf{term}_{5}=\mathcal{O}\left(\ln^{2}(At)\sum_{i=1}^{t}w_{t}^{i}t^{-1} \right)=\mathcal{O}\left(\ln^{3}(At)t^{-1+k_{\eta}+k_{\epsilon}}\right)= \mathcal{O}\left(\ln^{3}(At)t^{-\frac{1}{4}}\right).\]

Combining all terms above, we get that with probability at least \(1-\frac{3\delta}{t^{2}}\),

\[\text{KL}(z_{t+1}^{\star},z_{t+1})=\mathcal{O}\left(A\ln^{3}(At/\delta)t^{- \frac{1}{4}}\right).\] (7)

Using an union bound over \(t\), we see that Eq. (7) holds for all \(t\geq t_{0}\) with probability at least \(1-\mathcal{O}(\delta)\).

Part III.Using Lemma 9 with \(f_{t}(x,y)\) and \(x^{\top}Gy\) with domains \(\Omega_{t}\times\Omega_{t}\) and \(\Delta_{A}\times\Delta_{A}\), we get that for any \((x,y)\in\Delta_{A}\times\Delta_{A}\),

\[x_{t}^{\top}Gy-x^{\top}Gy_{t}^{\star}\leq\mathcal{O}\left(\epsilon_{t}\ln(A) +\frac{1}{t}\right)=\mathcal{O}\left(\ln(A)t^{-k_{\epsilon}}\right)=\mathcal{ O}\left(\ln(A)t^{-\frac{1}{8}}\right).\]

Further using Eq. (7), we get that with probability at least \(1-3\delta\), for any \(t\) and any \((x,y)\in\Delta_{A}\times\Delta_{A}\),

\[x_{t}^{\top}Gy-x^{\top}Gy_{t} \leq\mathcal{O}\left(\ln(A)t^{-\frac{1}{8}}+\|z_{t}-z_{t}^{\star }\|_{1}\right)\overset{(a)}{=}\mathcal{O}\left(\ln(A)t^{-\frac{1}{8}}+\sqrt{ \text{KL}(z_{t}^{\star},z_{t})}\right)\] \[=\mathcal{O}\left(\sqrt{A}\ln^{3/2}(At/\delta)t^{-\frac{1}{8}}\right)\]

where \((a)\) is by Pinsker's inequality. This completes the proof of Theorem 1. 

**Lemma 13**.: \(|v_{t}|=\mathcal{O}\left(\ln^{2}(At)t^{-1}\right)\)_._

Proof.: \[|v_{t}| =\left|\text{KL}(z_{t+1}^{\star},z_{t+1})-\text{KL}(z_{t}^{\star },z_{t+1})\right|\] \[\leq\mathcal{O}\left(\ln(At)\|z_{t+1}^{\star}-z_{t}^{\star}\|_{1}\right)\] (by Lemma 14 ) \[=\mathcal{O}\left(\ln^{2}(At)t^{-1}\right).\] (by Lemma 15 )

**Lemma 14**.: _Let \(x,x_{1},x_{2}\in\Omega_{t}\). Then_

\[\left|\text{KL}(x_{1},x)-\text{KL}(x_{2},x)\right|\leq\mathcal{O}\left(\ln(At) \|x_{1}-x_{2}\|_{1}\right).\]

Proof.: \[\text{KL}(x_{1},x)-\text{KL}(x_{2},x)\] \[=\sum_{a}\left(x_{1,a}\ln\frac{x_{1,a}}{x_{a}}-x_{2,a}\ln\frac{x_ {2,a}}{x_{a}}\right)\] \[=\sum_{a}(x_{1,a}-x_{2,a})\ln\frac{x_{1,a}}{x_{a}}+\sum_{a}x_{2,a }\left(\ln\frac{x_{1,a}}{x_{a}}-\ln\frac{x_{2,a}}{x_{a}}\right)\] \[\leq\mathcal{O}\left(\ln(At)\|x_{1}-x_{2}\|_{1}\right)-\text{KL} (x_{2},x_{1})\] \[\leq\mathcal{O}\left(\ln(At)\|x_{1}-x_{2}\|_{1}\right).\]

Similarly, \(\text{KL}(x_{2},x)-\text{KL}(x_{1},x)\leq\mathcal{O}\left(\ln(At)\|x_{1}-x_{2 }\|_{1}\right)\). 

**Lemma 15**.: \(\|z_{t}^{\star}-z_{t+1}^{\star}\|_{1}=\mathcal{O}\left(\frac{\ln(At)}{t}\right)\)_._Proof.: Notice that the feasible sets for the two time steps are different. Let \((x^{\star}_{t+1},y^{\star}_{t+1})\) be such that \(x^{\prime}_{t+1}=\frac{p_{t+1}}{A}\mathbf{1}+(1-p_{t+1})x^{\star}_{t+1}\) and \(y^{\prime}_{t+1}=\frac{p_{t+1}}{A}\mathbf{1}+(1-p_{t+1})\,y^{\star}_{t+1}\) where \(p_{t+1}=\min\{1,2t^{-3}\}\). Since \((x^{\star}_{t+1},y^{\star}_{t+1})\in\Omega_{t+1}\times\Omega_{t+1}\), we have that for any \(a\), \(x^{\prime}_{t+1,a}\geq\frac{p_{t+1}}{A}+(1-p_{t+1})\frac{1}{A(t+1)^{2}}\geq \frac{1}{At^{2}}\). Hence, \((x^{\star}_{t+1},y^{\star}_{t+1})\in\Omega_{t}\times\Omega_{t}\).

Because \((x^{\star}_{t+1},y^{\star}_{t+1})\) is the equilibrium of \(f_{t+1}\) in \(\Omega_{t+1}\times\Omega_{t+1}\), we have that for any \((x,y)\in\Omega_{t+1}\times\Omega_{t+1}\),

\[f_{t+1}(x,y^{\star}_{t+1})-f_{t+1}(x^{\star}_{t+1},y)\] \[=f_{t+1}(x,y^{\star}_{t+1})-f_{t+1}(x^{\star}_{t+1},y^{\star}_{t+ 1})+f_{t+1}(x^{\star}_{t+1},y^{\star}_{t+1})-f_{t+1}(x^{\star}_{t+1},y)\] \[\geq\epsilon_{t+1}\text{KL}(x,x^{\star}_{t+1})+\epsilon_{t+1} \text{KL}(y,y^{\star}_{t+1})\] \[\geq\frac{1}{2}\epsilon_{t+1}\left(\|x-x^{\star}_{t+1}\|_{1}^{2}+ \|y-y^{\star}_{t+1}\|_{1}^{2}\right)\] (Pinsker's inequality) \[\geq\frac{1}{4}\epsilon_{t+1}\|z-z^{\star}_{t+1}\|_{1}^{2}.\]

where the first inequality is due to the following calculation:

\[\epsilon_{t+1}\text{KL}(x,x^{\star}_{t+1}) =f_{t+1}(x,y^{\star}_{t+1})-f_{t+1}(x^{\star}_{t+1},y^{\star}_{t+ 1})-\nabla_{\text{x}}f_{t+1}(x^{\star}_{t+1},y^{\star}_{t+1})^{\top}(x-x^{ \star}_{t+1})\] \[\leq f_{t+1}(x,y^{\star}_{t+1})-f_{t+1}(x^{\star}_{t+1},y^{\star} _{t+1})\]

where we use \(\nabla_{\text{x}}f_{t+1}(x^{\star}_{t+1},y^{\star}_{t+1})^{\top}(x-x^{\star}_{ t+1})\geq 0\) since \(x^{\star}_{t+1}\) is the minimizer of \(f_{t+1}(\cdot,y^{\star}_{t+1})\) in \(\Omega_{t+1}\). Specially, we have

\[f_{t+1}(x^{\star}_{t},y^{\star}_{t+1})-f_{t+1}(x^{\star}_{t+1},y^{\star}_{t}) \geq\frac{1}{4}\epsilon_{t+1}\|z^{\star}_{t}-z^{\star}_{t+1}\|_{1}^{2}.\] (8)

Similarly, because \((x^{\star}_{t},y^{\star}_{t})\) is the equilibrium of \(f_{t}\) in \(\Omega_{t}\times\Omega_{t}\), we have

\[f_{t}(x^{\prime}_{t+1},y^{\star}_{t})-f_{t}(x^{\star}_{t},y^{\prime}_{t+1})\geq \frac{1}{4}\epsilon_{t}\|z^{\prime}_{t+1}-z^{\star}_{t}\|_{1}^{2},\]

which implies

\[f_{t}(x^{\star}_{t+1},y^{\star}_{t})-f_{t}(x^{\star}_{t},y^{\star }_{t+1})\] \[=f_{t}(x^{\prime}_{t+1},y^{\star}_{t})-f_{t}(x^{\star}_{t},y^{ \prime}_{t+1})+f_{t}(x^{\star}_{t+1},y^{\star}_{t})-f_{t}(x^{\prime}_{t+1},y^{ \star}_{t})+f_{t}(x^{\star}_{t},y^{\prime}_{t+1})-f_{t}(x^{\star}_{t},y^{\star} _{t+1})\] \[\geq\frac{1}{4}\epsilon_{t}\|z^{\prime}_{t+1}-z^{\star}_{t}\|_{1 }^{2}-\sup_{x\in\Omega_{t+1}}\|\nabla_{\text{x}}f_{t}(x,y^{\star}_{t})\|_{ \infty}\|x^{\prime}_{t+1}-x^{\star}_{t+1}\|_{1}-\sup_{y\in\Omega_{t+1}}\|\nabla _{\text{y}}f_{t}(x^{\star}_{t},y)\|_{\infty}\|y^{\prime}_{t+1}-y^{\star}_{t+1}\|_ {1}\] \[\geq\frac{1}{8}\epsilon_{t}\|z^{\star}_{t+1}-z^{\star}_{t}\|_{1}^{ 2}-\frac{1}{4}\epsilon_{t}\|z^{\prime}_{t+1}-z^{\star}_{t+1}\|_{1}^{2}- \mathcal{O}\left(\ln(At)\times\frac{1}{t^{3}}\right)\] \[\geq\frac{1}{8}\epsilon_{t}\|z^{\star}_{t+1}-z^{\star}_{t}\|_{1 }^{2}-\mathcal{O}\left(\frac{\ln(At)}{t^{3}}\right).\] (9)

In the first inequality, we use the fact that \(f_{t}(x,y)\) is convex in \(x\) and concave in \(y\) and Holder's inequality. In the second inequality, we use the triangle inequality, \(\left\|\nabla_{\text{x}}f_{t}(x,y)\right\|_{\infty}\leq\max_{a}\{(Gy)_{a}+\ln(x _{a})\}\leq\mathcal{O}(\ln(At))\), and \(\left\|\nabla_{\text{y}}f_{t}(x,y)\right\|_{\infty}\leq\max_{b}\{(G^{\top}x)_{b }+\ln(y_{b})\}\leq\mathcal{O}(\ln(At))\). In the second and third inequality, we use \(\|z^{\prime}_{t+1}-z^{\star}_{t+1}\|_{1}=\mathcal{O}(\frac{1}{t^{3}})\) by the definition of \(z^{\prime}_{t+1}\).

Combining Eq. (8) and Eq. (9), we get

\[\frac{3}{8}\epsilon_{t+1}\|z^{\star}_{t}-z^{\star}_{t+1}\|_{1}^{2}\] \[\leq f_{t+1}(x^{\star}_{t},y^{\star}_{t+1})-f_{t}(x^{\star}_{t},y^{ \star}_{t+1})-f_{t+1}(x^{\star}_{t+1},y^{\star}_{t})+f_{t}(x^{\star}_{t+1},y^{ \star}_{t})+\mathcal{O}\left(\frac{\ln(At)}{t^{3}}\right)\] \[=(f_{t+1}-f_{t})(x^{\star}_{t},y^{\star}_{t+1})-(f_{t+1}-f_{t})(x^{ \star}_{t+1},y^{\star}_{t})+\mathcal{O}\left(\frac{\ln(At)}{t^{3}}\right)\] \[\leq\sup_{x,y\in\Omega_{t+1}\times\Omega_{t+1}}\|\nabla f_{t+1}(x,y )-\nabla f_{t}(x,y)\|_{\infty}\|(x^{\star}_{t},y^{\star}_{t+1})-(x^{\star}_{t+1 },y^{\star}_{t})\|_{1}+\mathcal{O}\left(\frac{\ln(At)}{t^{3}}\right)\]\[=\sup_{x,y\in\Omega_{t+1}\times\Omega_{t+1}}\|\nabla f_{t+1}(x,y)- \nabla f_{t}(x,y)\|_{\infty}\|z_{t}^{\star}-z_{t+1}^{\star}\|_{1}+\mathcal{O} \left(\frac{\ln(At)}{t^{3}}\right)\]

Solving the inequality, we get

\[\|z_{t}^{\star}-z_{t+1}^{\star}\|_{1} \leq\mathcal{O}\left(\frac{1}{\epsilon_{t+1}}\sup_{x,y\in\Omega_{ t+1}\times\Omega_{t+1}}\|\nabla f_{t+1}(x,y)-\nabla f_{t}(x,y)\|_{\infty}+ \frac{\ln^{1/2}(At)}{\sqrt{\epsilon_{t+1}}t^{3/2}}\right)\] (10) \[\leq\mathcal{O}\left(\frac{(\epsilon_{t}-\epsilon_{t+1})\ln(At)}{ \epsilon_{t+1}}+\frac{\ln^{1/2}(At)}{\sqrt{\epsilon_{t+1}}t^{3/2}}\right)\] \[=\mathcal{O}\left(\frac{t^{-k_{\star}-1}\ln(At)}{t^{-k_{\star}}} +\frac{\ln^{1/2}(At)}{\sqrt{\epsilon_{t+1}}t^{3/2}}\right)\] \[=\mathcal{O}\left(\frac{\ln(At)}{t}\right).\]

## Appendix C Improved Last-Iterate Convergence under Expectation

In this section, we analyze Algorithm 4, which is almost identical to Algorithm 1 but does not involve the parameter \(\beta_{t}\). The choices of stepsize \(\eta_{t}\) and amount of regularization \(\epsilon_{t}\) are also tuned differently to obtain the best convergence rate.

```
1:Define:\(\eta_{t}=t^{-k_{\eta}}\), \(\epsilon_{t}=t^{-k_{\epsilon}}\) where \(k_{\eta}=\frac{1}{2}\), \(k_{\epsilon}=\frac{1}{6}\). \(\Omega_{t}=\left\{x\in\Delta_{\mathcal{A}}:x_{a}\geq\frac{1}{At^{2}},\,\forall a \in\mathcal{A}\right\}\).
2:Initialization:\(x_{1}=\frac{1}{A}\mathbf{1}\).
3:for\(t=1,2,\ldots\)do
4: Sample \(a_{t}\sim x_{t}\), and receive \(\sigma_{t}\in[0,1]\) with \(\mathbb{E}\left[\sigma_{t}\right]=G_{a_{t},b_{t}}\).
5: Compute \(g_{t}\) where \(g_{t,a}=\frac{\mathbf{1}[a_{t}=a]\sigma_{t}}{x_{t,a}}+\epsilon_{t}\ln x_{t,a}, \forall a\in\mathcal{A}\).
6: Update \(x_{t+1}\leftarrow\operatorname*{argmin}_{x\in\Omega_{t+1}}\left\{x^{\top}g_{t} +\frac{1}{\eta_{t}}\text{KL}(x,x_{t})\right\}\).
7:endfor ```

**Algorithm 4** Matrix Game with Bandit Feedback

**Theorem 4**.: Algorithm 4 _guarantees \(\mathbb{E}\left[\max_{x,y\in\Delta_{A}}\left(x_{t}^{\top}Gy-x^{\top}Gy_{t} \right)\right]=\mathcal{O}\left(\sqrt{A}\ln^{3/2}(At)t^{-\frac{1}{6}}\right)\) for any \(t\)._

Proof.: With the same analysis as in Part I of the proof of Theorem 1, we have

\[f_{t}(x_{t},y_{t})-f_{t}(x_{t}^{\star},y_{t})\] \[\leq\frac{(1-\eta_{t}\epsilon_{t})\text{KL}(x_{t}^{\star},x_{t})- \text{KL}(x_{t}^{\star},x_{t+1})}{\eta_{t}}+10\eta_{t}A\ln^{2}\left(At\right)+2 \eta_{t}A\underline{\lambda}_{t}+\underline{\xi}_{t}+\underline{\zeta}_{t}.\]

where

\[\underline{\zeta}_{t} \triangleq\sum_{a}x_{t,a}\left((Gy_{t})_{a}-\frac{\mathbf{1}[a_ {t}=a]\sigma_{t}}{x_{t,a}}\right),\qquad\zeta_{t}\triangleq\sum_{a}x_{t,a}^{ \star}\left(\frac{\mathbf{1}[a_{t}=a]\sigma_{t}}{x_{t,a}}-(Gy_{t})_{a}\right),\] \[\underline{\lambda}_{t} \triangleq\frac{1}{A}\sum_{a}\left(\frac{\mathbf{1}[a_{t}=a]}{x_ {t,a}}-1\right).\]

Unlike in Theorem 1, here these three terms all have zero mean. Thus, following the same arguments that obtain Eq. (6) and taking expectations, we get

\[\mathbb{E}_{t}[\text{KL}(z_{t+1}^{\star},z_{t+1})]\leq(1-\eta_{t}\epsilon_{t} )\text{KL}(z_{t}^{\star},z_{t})+20\eta_{t}^{2}A\ln^{2}\left(At\right)+\mathbb{ E}_{t}[v_{t}]\]

[MISSING_PAGE_EMPTY:24]

### Part I. Basic Iteration Properties

**Lemma 16**.: _For any \(x^{s}\in\Omega_{\tau+1}\),_

\[f_{\tau}^{s}(\hat{x}_{\tau}^{s},\hat{y}_{\tau}^{s})-f_{\tau}^{s}(x ^{s},\hat{y}_{\tau}^{s})\] \[\leq\frac{(1-\eta_{\tau}\epsilon_{\tau})\text{KL}(x^{s},\hat{x}_{ \tau}^{s})-\text{KL}(x^{s},\hat{x}_{\tau+1}^{s})}{\eta_{\tau}}+\frac{10\eta_{ \tau}A\ln^{2}(A\tau)}{(1-\gamma)^{2}}+\frac{2\eta_{\tau}A}{(1-\gamma)^{2}} \underline{\lambda}_{\tau}^{s}+\underline{\xi}_{\tau}^{s}+\underline{\zeta}_ {\tau}^{s}(x^{s}).\]

_(see the proof for the definitions of \(\underline{\lambda}_{\tau}^{s},\underline{\xi}_{\tau}^{s},\underline{\zeta}_ {\tau}^{s}(\cdot)\))_

Proof.: Consider a fixed \(s\) and a fixed \(\tau\), and let \(t=t_{\tau}(s)\) be the time when the players visit \(s\) at the \(\tau\)-th time.

\[f_{\tau}^{s}(\hat{x}_{\tau}^{s},\hat{y}_{\tau}^{s})-f_{\tau}^{s}( x^{s},\hat{y}_{\tau}^{s})\] \[=(\hat{x}_{\tau}^{s}-x^{s})^{\top}\left(G^{s}+\gamma\mathbb{E}_{s ^{\prime}\sim P^{s}}\left[V_{t}^{s^{\prime}}\right]\right)\hat{y}_{\tau}^{s}- \epsilon_{\tau}\phi(\hat{x}_{\tau}^{s})+\epsilon_{\tau}\phi(x^{s})\] \[=(\hat{x}_{\tau}^{s}-x^{s})^{\top}\left[\left(G^{s}+\gamma \mathbb{E}_{s^{\prime}\sim P^{s}}\left[V_{t}^{s^{\prime}}\right]\right)\hat{y }_{\tau}^{s}+\epsilon_{\tau}\ln\hat{x}_{\tau}^{s}\right]-\epsilon_{\tau}\text{ KL}(x^{s},\hat{x}_{\tau}^{s})\] \[=(\hat{x}_{\tau}^{s}-x^{s})^{\top}g_{t}-\epsilon_{\tau}\text{KL} (x^{s},\hat{x}_{\tau}^{s})+\underbrace{(\hat{x}_{\tau}^{s})^{\top}\left(\left( G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\left[V_{t}^{s^{\prime}}\right] \right)\hat{y}_{\tau}^{s}-\frac{\mathbf{1}[\hat{a}_{\tau}^{s}=a]\left(\sigma_ {t}+\gamma V_{t}^{s_{t+1}}\right)}{\hat{x}_{\tau,a}^{s}+\beta_{\tau}}\right)}\] \[\qquad\qquad+\underbrace{(x^{s})^{\top}\left(\frac{\mathbf{1}[ \hat{a}_{\tau}^{s}=a]\left(\sigma_{t}+\gamma V_{t}^{s_{t+1}}\right)}{\hat{x}_{ \tau,a}^{s}+\beta_{\tau}}-\left(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s} }\left[V_{t}^{s^{\prime}}\right]\right)\hat{y}_{\tau}^{s}\right)}_{\underline{ \zeta}_{\tau}^{s}(x^{s})}\] \[\leq\frac{(1-\eta_{\tau}\epsilon_{\tau})\text{KL}(x^{s},\hat{x}_ {\tau}^{s})-\text{KL}(x^{s},\hat{x}_{\tau+1}^{s})}{\eta_{\tau}}\] \[\qquad+\frac{10\eta_{\tau}A\ln^{2}(A\tau)}{(1-\gamma)^{2}}+\frac {2\eta_{\tau}A}{(1-\gamma)^{2}}\times\underbrace{\frac{1}{|\mathcal{A}|}\sum_ {a}\left(\frac{\mathbf{1}[\hat{a}_{\tau}^{s}=a]}{\hat{x}_{\tau,a}^{s}+\beta_{ \tau}}-1\right)}_{\underline{\lambda}_{\tau}^{s}}+\underline{\xi}_{\tau}^{s} +\underline{\zeta}_{\tau}^{s}(x^{s}),\]

where we omit some calculation steps due to the similarity to Eq. (5).

### Part II. Policy Convergence to the Nash of Regularized Game

**Lemma 17**.: _With probability at least \(1-\mathcal{O}(\delta)\), for all \(s\in S\), \(t\geq 1\) and \(\tau\geq 1\) such that \(t_{\tau}(s)\leq t\), we have_

\[\text{KL}(\hat{z}_{\tau\star}^{s},\hat{z}_{\tau}^{s})\leq\mathcal{O}\left(A\ln ^{5}(SAt/\delta)L^{2}\tau^{-k_{\sharp}}\right),\]

_where \(k_{\sharp}=\min\{k_{\beta}-k_{\epsilon},k_{\eta}-k_{\beta},k_{\alpha}-k_{\eta }-2k_{\epsilon}\}\)._

Proof.: In this proof, we abbreviate \(\underline{\zeta}_{i}^{s}(\hat{x}_{i\star}^{s})\) as \(\underline{\zeta}_{i}^{s}\). By Lemma 16, for all \(i\leq\tau\) we have

\[\text{KL}(\hat{x}_{i\star}^{s},\hat{x}_{i+1}^{s}) \leq(1-\eta_{i}\epsilon_{i})\text{KL}(\hat{x}_{i\star}^{s},\hat{x }_{i}^{s})+\eta_{i}\left(f_{i}^{s}(\hat{x}_{i\star}^{s},\hat{y}_{i}^{s})-f_{i}^ {s}(\hat{x}_{i}^{s},\hat{y}_{i}^{s})\right)\] \[\qquad+\frac{10\eta_{i}^{2}A\ln^{2}(A\tau)}{(1-\gamma)^{2}}+\frac{2 \eta_{i}^{2}A}{(1-\gamma)^{2}}\underline{\lambda}_{i}^{s}+\eta_{i}\underline{ \xi}_{i}^{s}+\eta_{i}\underline{\zeta}_{i}^{s}.\]

Similarly, for all \(i\leq\tau\), we have

\[\text{KL}(\hat{y}_{i\star}^{s},\hat{y}_{i+1}^{s}) \leq(1-\eta_{i}\epsilon_{i})\text{KL}(\hat{y}_{i\star}^{s},\hat{y}_ {i}^{s})+\eta_{i}\left(f_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i}^{s})-f_{i}^{s}(\hat{ x}_{i}^{s},\hat{y}_{i\star}^{s})\right)\] \[\qquad+\frac{10\eta_{i}^{2}A\ln^{2}(A\tau)}{(1-\gamma)^{2}}+\frac {2\eta_{i}^{2}A}{(1-\gamma)^{2}}\overline{\lambda}_{i}^{s}+\eta_{i}\overline{ \xi}_{i}^{s}+\eta_{i}\overline{\zeta}_{i}^{s}.\]Adding the two inequalities up, and using \(f_{i}^{s}(\hat{x}_{i*}^{s},\hat{y}_{i}^{s})-f_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i*} ^{s})\leq 0\) because \((\hat{x}_{i*}^{s},\hat{y}_{i*}^{s})\) is the equilibrium of \(f_{i}^{s}\), we get for \(i\leq\tau\)

\[\text{KL}(\hat{z}_{i+1\star}^{s},\hat{z}_{i+1}^{s})\leq(1-\eta_{i}\epsilon_{i}) \text{KL}(\hat{z}_{i*}^{s},\hat{z}_{i}^{s})+\frac{20\eta_{i}^{2}A\ln^{2}(A\tau)} {(1-\gamma)^{2}}+\frac{2\eta_{i}^{2}A}{(1-\gamma)^{2}}\lambda_{i}^{s}+\eta_{i} \xi_{i}^{s}+\eta_{i}\zeta_{i}^{s}+v_{i}^{s},\] (11)

where \(v_{i}^{s}=\text{KL}(\hat{z}_{i+1\star}^{s},\hat{z}_{i+1}^{s})-\text{KL}(\hat{z }_{i*}^{s},\hat{z}_{i+1}^{s})\) and \(\square^{s}=\square^{s}+\overline{\square}^{s}\) for \(\square=\xi_{i},\zeta_{i}\).

Expanding Eq. (11), we get

\[\text{KL}(\hat{z}_{\tau+1\star}^{s},\hat{z}_{\tau+1}^{s})\leq\underbrace{ \frac{20A\ln^{2}(A\tau)}{(1-\gamma)^{2}}\sum_{i=1}^{\tau}w_{\tau}^{i}\eta_{i}^ {2}}_{\text{\bf term}_{1}}+\underbrace{\frac{2A}{(1-\gamma)^{2}}\sum_{i=1}^{ \tau}w_{\tau}^{i}\eta_{i}^{2}\lambda_{i}^{s}}_{\text{\bf term}_{2}}+ \underbrace{\sum_{i=1}^{\tau}w_{\tau}^{i}\eta_{i}\xi_{i}^{s}}_{\text{\bf term }_{3}}+\underbrace{\sum_{i=1}^{t}w_{\tau}^{i}\eta_{i}\zeta_{i}^{s}}_{\text{\bf term }_{4}}+\underbrace{\sum_{i=1}^{\tau}w_{\tau}^{i}v_{i}^{s}}_{\text{\bf term}_{5}}.\]

These five terms correspond to those in Eq. (6), and can be handled in the same way. For \(\text{\bf term}_{1}\) to \(\text{\bf term}_{4}\), we follow exactly the same arguments there, and bound their sum as with probability at least \(1-\mathcal{O}\left(\frac{\delta}{S\tau^{2}}\right)\),

\[\sum_{j=1}^{4}\text{\bf term}_{j}=\mathcal{O}\left(A\ln^{3}(SA\tau/\delta) \left(\tau^{-k_{\eta}+k_{\epsilon}}+\tau^{-2k_{\eta}+k_{\beta}}+\tau^{-k_{ \beta}+k_{\epsilon}}+\tau^{-\frac{1}{2}k_{\eta}+\frac{1}{2}k_{\epsilon}}+\tau^ {-k_{\eta}+k_{\beta}}\right)\right).\]

To bound \(\text{\bf term}_{5}\), by Lemma 14 and Lemma 18, we have

\[|v_{\tau}^{s}|=\mathcal{O}\left(\ln(A\tau)\right)\cdot\|\hat{z}_{\tau\star}^{ s}-\hat{z}_{\tau+1\star}^{s}\|_{1}=\mathcal{O}\left(\ln^{4}(SAt/\delta)L^{2} \cdot\tau^{-k_{\alpha}+k_{\epsilon}}\right).\]

Therefore, by Lemma 1,

\[\text{\bf term}_{5}=\sum_{i=1}^{\tau}w_{\tau}^{i}v_{i}^{s}=\mathcal{O}\left( \ln^{5}(SAt/\delta)L^{2}\cdot\tau^{-k_{\alpha}+k_{\eta}+2k_{\epsilon}}\right).\]

Combining all the terms with union bound over \(s\in S\) and \(\tau\geq 1\) finishes the proof. 

**Lemma 18**.: _For any sand \(\tau\geq 0\) such that \(t_{\tau}(s)\leq t\), \(\|\hat{z}_{\tau\star}^{s}-\hat{z}_{\tau+1\star}^{s}\|_{1}=\mathcal{O}\left(\ln^ {3}(SAt/\delta)L^{2}\cdot\tau^{-k_{\alpha}+k_{\epsilon}}\right)\)._

Proof.: The bound holds trivially when \(\tau\leq 2L\). Below we focus on the case with \(\tau>2L\). By exactly the same arguments as in the proof of Lemma 15, we have an inequality similar to Eq. (10):

\[\|z_{\tau\star}^{s}-z_{\tau+1\star}^{s}\|_{1}\] \[=\mathcal{O}\left(\frac{1}{\epsilon_{\tau+1}}\sup_{x^{s},y^{s}} \|\nabla f_{\tau}^{s}(x^{s},y^{s})-\nabla f_{\tau+1}^{s}(x^{s},y^{s})\|_{ \infty}+\frac{\ln^{1/2}(A\tau)}{\sqrt{\epsilon_{\tau+1}}\tau^{3/2}}\right)\] \[\leq\mathcal{O}\left(\frac{1}{\epsilon_{\tau+1}}\sup_{s^{\prime}} \left|V_{t_{\tau}(s)}^{s^{\prime}}-V_{t_{\tau+1}(s)}^{s^{\prime}}\right|+\frac{ (\epsilon_{\tau}-\epsilon_{\tau+1})\ln(A\tau)}{\epsilon_{\tau+1}}+\frac{\ln^{1 /2}(A\tau)}{\sqrt{\epsilon_{\tau+1}}\tau^{3/2}}\right)\] \[\leq\mathcal{O}\left(\frac{1}{\epsilon_{\tau+1}}\sup_{s^{\prime}} \left|V_{t_{\tau}(s)}^{s^{\prime}}-V_{t_{\tau+1}(s)}^{s^{\prime}}\right|+\frac{ \ln(A\tau)}{\tau}\right).\] (12)

Since \(t_{\tau}(s)\leq t\) and we assume that every state is visited at least once in \(6L\log(St/\delta)\) steps (Corollary 1), we have that for any state \(s^{\prime}\), \(n_{t_{\tau}(s)}^{s^{\prime}}\geq\frac{t_{\tau}(s)}{6L\log(St/\delta)}-1\). Thus, whenever \(V_{t}^{s^{\prime}}\) updates between \(t_{\tau}(s)\) and \(t_{\tau+1}(s)\), the change is upper bounded by \(\frac{1}{1-\gamma}(\frac{t_{\tau}(s)}{6L\log(St/\delta)}-1)^{-k_{\alpha}}\). Besides, between \(t_{\tau}(s)\) and \(t_{\tau+1}(s)\), \(V_{t}^{s^{\prime}}\) can change at most \(6L\log(St/\delta)\) times. Therefore,

\[\left|V_{t_{\tau}(s)}^{s^{\prime}}-V_{t_{\tau+1}(s)}^{s^{\prime}}\right|\] \[\leq\frac{1}{1-\gamma}\times 6L\log(St/\delta)\times\left(\frac{t_{ \tau}(s)}{6L\log(St/\delta)}-1\right)^{-k_{\alpha}}\leq\frac{1}{1-\gamma}\times 6L\log(St/\delta)\times\left(\frac{\tau}{6L\log(St/ \delta)}-1\right)^{-k_{\alpha}}\] \[=\mathcal{O}\left(\frac{L^{2}\ln^{2}(St/\delta)\tau^{-k_{\alpha}}} {1-\gamma}\right),\] (13)

where the last inequality holds since \(k_{\alpha}<1\). Combining Eq. (12) and Eq. (13) with the fact that \(\epsilon_{\tau}=\frac{1}{1-\gamma}\tau^{-k_{\epsilon}}\) finishes the proof.

### Part III. Value Convergence

For positive integers \(\tau\geq i\), we define \(\alpha_{\tau}^{i}=\alpha_{i}\prod_{j=i+1}^{\tau}(1-\alpha_{j})\).

**Lemma 19** (weighted regret bound).: _With probability \(1-\mathcal{O}(\delta)\), for any \(s\), any visitation count \(\tau\geq\tau_{0}\), and any \(x^{s}\in\Omega_{\tau+1}\),_

\[\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(f_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i}^ {s})-f_{i}^{s}(x^{s},\hat{y}_{i}^{s})\right)\leq\mathcal{O}\left(\frac{A\ln^{3 }(SA\tau/\delta)\tau^{-k^{\prime}}}{1-\gamma}\right).\]

_where \(k^{\prime}=\min\left\{k_{\eta},k_{\beta},k_{\alpha}-k_{\beta}\right\}\)._

Proof.: We will be considering a weighted sum of the instantaneous regret bound established in Lemma 16. However, notice that for \(f_{i}^{s}\), Lemma 16 only provides a regret bound with comparators in \(\Omega_{i+1}\). Therefore, for a fixed \(x^{s}\in\Omega_{\tau+1}\), we define the following auxiliary comparators for all \(i=1,\ldots,\tau\):

\[\widetilde{x}_{i}^{s}=\frac{p_{i}}{A}\mathbf{1}+(1-p_{i})\,x^{s}\]

where \(p_{i}\triangleq\frac{(\tau+1)^{2}-(i+1)^{2}}{(i+1)^{2}(\tau+1)^{2}-1}\). Since \(x^{s}\in\Omega_{\tau+1}\), we have that for any \(a\), \(\widetilde{x}_{i,a}^{s}\geq\frac{p_{i}}{A}+\frac{1-p_{i}}{A(\tau+1)^{2}}= \frac{1}{A(i+1)^{2}}\), and thus \(\widetilde{x}_{i}^{s}\in\Omega_{i+1}\).

Applying Lemma 16 and considering the weighted sum of the bounds, we get

\[\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(f_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i}^{s})-f_{i}^{s}(\widetilde{x}_{i}^{s},\hat{y}_{i}^{s})\right)\] \[\leq\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(\frac{(1-\eta_{i} \epsilon_{i})\text{KL}(\widetilde{x}_{i}^{s},\hat{x}_{i}^{s})-\text{KL}( \widetilde{x}_{i}^{s},\hat{x}_{i+1}^{s})}{\eta_{i}}+\frac{10\eta_{i}A\ln^{2}( A\tau)}{(1-\gamma)^{2}}+\frac{2\eta_{i}A}{(1-\gamma)^{2}}\lambda_{i}^{s}+\underline{ \xi}_{i}^{s}+\underline{\zeta}_{i}^{s}(\widetilde{x}_{i}^{s})\right)\] \[\leq\underbrace{\sum_{i=2}^{\tau}\left(\frac{\alpha_{\tau}^{i}(1 -\eta_{i}\epsilon_{i})}{\eta_{i}}\text{KL}(\widetilde{x}_{i}^{s},\hat{x}_{i}^ {s})-\frac{\alpha_{\tau}^{i-1}}{\eta_{i-1}}\text{KL}(\widetilde{x}_{i-1}^{s}, \hat{x}_{i}^{s})\right)}_{\text{\footnotesize{term}}_{0}}\text{ \footnotesize{(notice that $(1-\eta_{1}\epsilon_{1})=0$)}}\] \[\qquad+\underbrace{\frac{10A\ln^{2}(A\tau)}{(1-\gamma)^{2}}\sum_{ i=1}^{\tau}\alpha_{\tau}^{i}\eta_{i}}_{\text{\footnotesize{term}}_{1}}+ \underbrace{\frac{2A}{(1-\gamma)^{2}}\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\eta_{i }\underline{\lambda}_{i}^{s}}_{\text{\footnotesize{term}}_{2}}+\underbrace{ \sum_{i=1}^{\tau}\alpha_{\tau}^{i}\underline{\xi}_{i}^{s}}_{\text{ \footnotesize{term}}_{3}}+\underbrace{\sum_{i=1}^{\tau}\alpha_{\tau}^{i} \underline{\zeta}_{i}^{s}(\widetilde{x}_{i}^{s})}_{\text{\footnotesize{term}}_ {4}}.\] \[\text{\footnotesize{term}}_{0} =\sum_{i=2}^{\tau}\text{KL}(\widetilde{x}_{i}^{s},\hat{x}_{i}^{ s})\left(\frac{\alpha_{\tau}^{i}(1-\eta_{i}\epsilon_{i})}{\eta_{i}}-\frac{\alpha_{ \tau}^{i-1}}{\eta_{i-1}}\right)+\sum_{i=2}^{\tau}\frac{\alpha_{\tau}^{i-1}}{ \eta_{i-1}}\left(\text{KL}(\widetilde{x}_{i-1}^{s},\hat{x}_{i}^{s})-\text{KL}( \widetilde{x}_{i}^{s},\hat{x}_{i}^{s})\right)\] \[\overset{(a)}{\leq}0+\mathcal{O}\left(\ln(A\tau)\sum_{i=2}^{ \tau}\frac{\alpha_{\tau}^{i-1}}{\eta_{i-1}}\|\widetilde{x}_{i-1}^{s}-\widetilde {x}_{i}^{s}\|_{1}\right)\] \[=\mathcal{O}\left(\ln(A\tau)\sum_{i=2}^{\tau}\frac{\alpha_{\tau}^ {i-1}}{\eta_{i-1}}\left|p_{i-1}-p_{i}\right|\right)\] \[\leq\mathcal{O}\left(\ln(A\tau)\sum_{i=2}^{\tau}\frac{\alpha_{\tau }^{i-1}}{\eta_{i-1}}\frac{1}{(i-1)^{2}}\right)\] \[=\mathcal{O}\left(\frac{\ln(A\tau)}{1-\gamma}\tau^{k_{\eta}-2}\right)\] (by Lemma 1)

where \((a)\) is by Lemma 14 and the following calculation:

\[\frac{\alpha_{\tau}^{i}(1-\eta_{i}\epsilon_{i})}{\eta_{i}}\times\frac{\eta_{i-1 }}{\alpha_{\tau}^{i-1}}=\frac{\eta_{i-1}}{\eta_{i}}\times\frac{\alpha_{i}}{ \alpha_{i-1}}\times\frac{1-\eta_{i}\epsilon_{i}}{1-\alpha_{i}}=\left(\frac{i-1 }{i}\right)^{-k_{\eta}+k_{\alpha}}\times\frac{1-i^{-k_{\eta}-k_{\epsilon}}}{1- i^{-k_{\alpha}}}\leq 1\times 1=1.\]We proceed to bound other terms as follows: with probability at least \(1-\mathcal{O}\left(\frac{\delta}{S^{\tau^{2}}}\right)\)

\[\textbf{term}_{1} =\mathcal{O}\left(\frac{A\ln^{3}(A\tau)}{1-\gamma}\tau^{-k_{\eta}} \right),\] (Lemma 1) \[\textbf{term}_{2} =\mathcal{O}\left(\frac{A\ln(SA\tau/\delta)}{1-\gamma}\times \max_{i\leq\tau}\frac{\alpha_{\tau}^{i}\eta_{i}}{\beta_{\tau}}\right)\] (Lemma 7) \[=\mathcal{O}\left(\frac{A\ln(SA\tau/\delta)\tau^{-k_{\alpha}-k_{ \eta}+k_{\beta}}}{1-\gamma}\right),\] (Lemma 2) \[\textbf{term}_{3} =\mathcal{O}\left(\frac{A}{1-\gamma}\sum_{i=1}^{\tau}\beta_{i} \alpha_{\tau}^{i}+\frac{1}{1-\gamma}\sqrt{\ln(SA\tau/\delta)\sum_{i=1}^{\tau} (\alpha_{\tau}^{i})^{2}}\right)\] (Lemma 6) \[=\mathcal{O}\left(\frac{A\ln(A\tau)\tau^{-k_{\beta}}}{1-\gamma}+ \frac{1}{1-\gamma}\sqrt{\ln(SA\tau/\delta)\sum_{i=1}^{\tau}\alpha_{\tau}^{i} \alpha_{i}}\right)\] (Lemma 1) \[=\mathcal{O}\left(\frac{A\ln(SA\tau/\delta)\left(\tau^{-k_{\beta} }+\tau^{-\frac{k_{\alpha}}{2}}\right)}{1-\gamma}\right),\] (Lemma 1) \[\textbf{term}_{4} =\sum_{i=1}^{\tau}\alpha_{\tau}^{i}p_{i}\zeta_{i}^{s}\left(\frac {1}{A}\textbf{1}\right)+\sum_{i=1}^{\tau}\alpha_{\tau}^{i}(1-p_{i})\zeta_{i}^ {s}(x^{s})\] (by the linearity of \[\zeta_{i}^{s}(\cdot)\] ) \[=\mathcal{O}\left(\frac{\ln(SA\tau/\delta)}{1-\gamma}\max_{i\leq \tau}\frac{\alpha_{\tau}^{i}}{\beta_{\tau}}\right)\] (Lemma 7) \[=\mathcal{O}\left(\frac{\ln(SA\tau/\delta)\tau^{-k_{\alpha}+k_{ \beta}}}{1-\gamma}\right).\] (Lemma 2)

Combining all terms, we get

\[\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(f_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i}^ {s})-f_{i}^{s}(\widetilde{x}_{i}^{s},\hat{y}_{i}^{s})\right)=\mathcal{O}\left( \frac{A\ln^{3}(SA\tau/\delta)\left(\tau^{-k_{\eta}}+\tau^{-k_{\beta}}+\tau^{- k_{\alpha}+k_{\beta}}\right)}{1-\gamma}\right).\] (14)

Finally,

\[\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(f_{i}^{s}(\widetilde{x}_{ i}^{s},\hat{y}_{i}^{s})-f_{i}^{s}(x^{s},\hat{y}_{i}^{s})\right) =\mathcal{O}\left(\frac{\ln(A\tau)}{1-\gamma}\sum_{i=1}^{\tau} \alpha_{\tau}^{i}\|\widetilde{x}_{i}^{s}-x^{s}\|_{1}\right)\] \[=\mathcal{O}\left(\frac{\ln(A\tau)}{1-\gamma}\sum_{i=1}^{\tau} \alpha_{\tau}^{i}p_{i}\right)=\mathcal{O}\left(\frac{\ln(A\tau)}{\tau^{2}(1- \gamma)}\right).\] (15)

Adding up Eq. (14) and Eq. (15) and applying union bound over all \(s\in\mathcal{S}\) and \(\tau\) finish the proof. 

**Lemma 20**.: _With probability at least \(1-\mathcal{O}(\delta)\), for any state \(s\in\mathcal{S}\) and time \(t\geq 1\), we have_

\[|V_{t}^{s}-V_{\star}^{s}|\leq\mathcal{O}\left(\frac{A\ln(SAt/\delta)}{(1- \gamma)^{2}}\left(\frac{L\ln(St/\delta)}{1-\gamma}\ln\frac{t}{1-\gamma}\right)^ {\frac{k_{\star}}{1-k_{\alpha}}}\left(\frac{L\ln(St/\delta)}{t}\right)^{k_{ \star}}\right),\]

_where \(k_{\ast}=\min\left\{k_{\eta},k_{\beta},k_{\alpha}-k_{\beta},k_{\epsilon}\right\}\)._

Proof.: Fix an \(s\) and a visitation count \(\tau\). Let \(t_{i}\) be the time index when the players visit \(s\) for the \(i\)-th time. Then with probability at least \(1-\frac{\delta}{S\tau^{2}}\),

\[V_{t_{\tau}}^{s} =\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(\sigma_{t_{i}}+\gamma V_ {t_{i}}^{s_{t_{i}+1}}\right)\] \[\leq\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\hat{x}_{i}^{s\top}\left(G^{ s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\left[V_{t_{i}}^{s^{\prime}} \right]\right)\hat{y}_{i}^{s}+\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left[\sigma_{t _{i}}+\gamma V_{t_{i}}^{s_{t_{i}+1}}-\hat{x}_{i}^{s\top}\left(G^{s}+\gamma \mathbb{E}_{s^{\prime}\sim P^{s}}\left[V_{t_{i}}^{s^{\prime}}\right]\right)\hat {y}_{i}^{s}\right]\]\[=\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(f_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i}^{s })+\epsilon_{i}\phi(\hat{x}_{i}^{s})-\epsilon_{i}\phi(\hat{y}_{i}^{s})\right)+ \mathcal{O}\left(\frac{1}{1-\gamma}\sqrt{\ln(S\tau/\delta)\sum_{i=1}^{\tau}( \alpha_{\tau}^{i})^{2}}\right)\] (Azuma's inequality) \[\leq\sum_{i=1}^{\tau}\alpha_{\tau}^{i}f_{i}^{s}(\hat{x}_{i}^{s}, \hat{y}_{i}^{s})+\mathcal{O}\left(\epsilon_{\tau}\ln(A)+\frac{\ln(SA\tau/\delta )\tau^{-\frac{k_{\alpha}}{2}}}{1-\gamma}\right)\] \[\leq\min_{x}\sum_{i=1}^{\tau}\alpha_{\tau}^{i}f_{i}^{s}(x^{s}, \hat{y}_{i}^{s})+\mathcal{O}\left(\epsilon_{\tau}\ln(A)+\frac{A\ln^{3}(SA\tau/ \delta)\tau^{-k^{\prime}}}{1-\gamma}\right)\] (\[k^{\prime}\] is defined in Lemma 19 with \[k^{\prime}\leq\frac{1}{2}(k_{\beta}+k_{\alpha}-k_{\beta})=\frac{k_{\alpha}}{ 2}\] \[\leq\min_{x^{s}}\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(x^{s} \right)^{\top}\left(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\left[V_{ \star}^{s^{\prime}}\right]\right)\hat{y}_{i}^{s}+\mathcal{O}\left(\frac{A\ln^ {3}(SA\tau/\delta)\tau^{-k_{\star}}}{1-\gamma}\right)\] \[\leq\min_{x^{s}}\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(x^{s} \right)^{\top}\left(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\left[V_{ \star}^{s^{\prime}}\right]\right)\hat{y}_{i}^{s}+\gamma\sum_{i=1}^{\tau} \alpha_{\tau}^{i}\max_{s^{\prime}}\left|V_{t_{i}}^{s^{\prime}}-V_{\star}^{s^{ \prime}}\right|+\mathcal{O}\left(\frac{A\ln^{3}(SA\tau/\delta)\tau^{-k_{\star }}}{1-\gamma}\right)\] \[\leq V_{\star}^{s}+\gamma\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\max_{ s^{\prime}}\left|V_{t_{i}}^{s^{\prime}}-V_{\star}^{s^{\prime}}\right|+\mathcal{O} \left(\frac{A\ln^{3}(SA\tau/\delta)\tau^{-k_{\star}}}{1-\gamma}\right).\]

Similar inequality can be also obtained through the perspective of the other player: with probability at least \(1-\frac{\delta}{S\tau^{2}}\)

\[V_{t_{\tau}}^{s}\geq V_{\star}^{s}-\gamma\sum_{i=1}^{\tau}\alpha_{\tau}^{i} \max_{s^{\prime}}\left|V_{t_{i}}^{s^{\prime}}-V_{\star}^{s^{\prime}}\right|- \mathcal{O}\left(\frac{A\ln^{3}(SA\tau/\delta)\tau^{-k_{\star}}}{1-\gamma} \right),\]

which, combined with the previous inequality and union bound over \(s\in\mathcal{S}\) and \(\tau\geq 1\), gives the following relation: with probability at least \(1-\mathcal{O}(\delta)\), for any \(s\in\mathcal{S}\) and \(\tau\geq 1\),

\[\left|V_{t_{\tau}}^{s}-V_{\star}^{s}\right|\leq\gamma\sum_{i=1}^{\tau}\alpha_{ \tau}^{i}\max_{s^{\prime}}\left|V_{t_{i}}^{s^{\prime}}-V_{\star}^{s^{\prime}} \right|+\mathcal{O}\left(\frac{A\ln^{3}(SA\tau/\delta)\tau^{-k_{\star}}}{1- \gamma}\right).\] (16)

Before continuing, we first some auxiliary quantities. For a fixed \(t\), define

\[u(t)=\left\lceil\left(\frac{16\times 6L\ln(St/\delta)}{1-\gamma}\ln\frac{t}{1- \gamma}\right)^{\frac{1}{1-k_{\alpha}}}\right\rceil;\]

for fixed \((\tau,t)\) we further define

\[v(\tau,t)=\left\lfloor\tau-3\tau^{k_{\alpha}}\ln\frac{t}{1-\gamma}\right\rfloor.\]

Now we continue to prove a bound for \(\left|V_{t}^{s}-V_{\star}^{s}\right|\). Suppose that Eq. (16) can be written as

\[\left|V_{t_{\tau}}^{s}-V_{\star}^{s}\right|\leq\gamma\sum_{i=1}^{\tau}\alpha_{ \tau}^{i}\max_{s^{\prime}}\left|V_{t_{i}}^{s^{\prime}}-V_{\star}^{s^{\prime}} \right|+\frac{C_{1}A\ln^{3}(SA\tau/\delta)\tau^{-k_{\star}}}{1-\gamma}\] (17)

for a universal constant \(C_{1}\geq 1\). Below we use induction to show that for all \(t\),

\[\left|V_{t}^{s}-V_{\star}^{s}\right|\leq\Phi(t)\triangleq\frac{8C_{1}A\ln^{3}( SAt/\delta)}{(1-\gamma)^{2}}\left(\frac{6L\ln(St/\delta)(u(t)+1)}{t}\right)^{k_{ \star}}.\] (18)

This is trivial for \(t=1\).

Suppose that Eq. (18) holds for all time \(1,\ldots,t-1\) and for all \(s\). Now we consider time \(t\) and a fixed state \(s\). We denote \(L^{\prime}=6L\ln(St/\delta)\). Let \(\tau=n_{t+1}^{s}\) and let \(1\leq t_{1}<t_{2}<\cdots<t_{\tau}\leq t\)

[MISSING_PAGE_EMPTY:30]

\[=1+\frac{1-\gamma}{2}\]

where the first inequality is due to the fact that at time \(t\), state \(s\) has only been visited for \(\tau\) times; the second inequality is because for any \(k>j\), we have \(t_{j}\geq j\) and \(t_{k}-t_{j}\leq L^{\prime}(k-j)\); the third inequality is by \(i\geq v(\tau,t)\); the fourth inequality is by the definition of \(v(\tau,t)\); the fifth inequality is because \(4\tau^{k_{\alpha}-1}\ln\frac{t}{1-\gamma}\leq\frac{1-\gamma}{4L^{\gamma}}\) since \(\tau\geq u(t)\), and \(\frac{1}{\tau}<\frac{1}{u(t)}\leq\frac{1-\gamma}{16L^{\prime}}\) since \(u(t)\geq\frac{16L^{\prime}}{1-\gamma}\); the last inequality is because \(\frac{1+\frac{1}{36}a}{1-\frac{1}{4}a}\leq 1+\frac{1}{2}a\) for \(a\in[0,1]\).

### Part IV. Combining

In this subsection, we combine previous lemmas to show last-iterate convergence rate of Algorithm 2 and prove Theorem 2.

**Lemma 21**.: _With probability at least \(1-\mathcal{O}(\delta)\), for any time \(t\geq 1\),_

\[\max_{s,x,y}\big{(}V^{s}_{x_{t},y}-V^{s}_{x,y_{t}}\big{)}\leq \mathcal{O}\Bigg{(}\frac{AL^{2+1/\varepsilon}\ln^{4+1/\varepsilon}(SAt/\delta )\ln^{1/\varepsilon}(t/(1-\gamma))}{(1-\gamma)^{2+1/\varepsilon}}t^{-\frac{1} {3+\varepsilon}}\Bigg{)}.\]

Proof.: Using Lemma 10, we can bound the duality gap of the whole game by the duality gap on an individual state:

\[\max_{s,x,y}\big{(}V^{s}_{x_{t},y}-V^{s}_{x,y_{t}}\big{)} \leq\frac{2}{1-\gamma}\max_{s,x,y}\big{(}x^{s}_{t}Q^{s}_{*}y^{s}- x^{s}Q^{s}_{*}y^{s}_{t}\big{)}\] \[=\frac{2}{1-\gamma}\max_{s,x,y}\Big{(}x^{s}_{t}\big{(}G^{s}+\gamma \mathbb{E}_{s^{\prime}\sim P^{s}}\Big{[}V^{s^{\prime}}_{*}\Big{]}\Big{)}y^{s}- x^{s}\Big{(}G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\Big{[}V^{s^{\prime}}_{ *}\Big{]}\Big{)}y^{s}_{t}\Big{)}\] \[\leq\frac{2}{1-\gamma}\max_{s,x,y}\Big{(}x^{s}_{t}\Big{(}G^{s}+ \gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\Big{[}V^{s^{\prime}}_{t}\Big{]}\Big{)} y^{s}-x^{s}\Big{(}G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\Big{[}V^{s^{ \prime}}_{t}\Big{]}\Big{)}y^{s}_{t}\Big{)}\] \[\quad+\frac{4\gamma}{1-\gamma}\max_{s}|V^{s}_{t}-V^{s}_{*}|\]

With probability at least \(1-\mathcal{O}(\delta)\), for any \(s,x^{s},y^{s}\), and \(t\geq 1\), denote \(\tau\) the number of visitation to state \(s\) until time \(t\), then

\[x^{s}_{t}\Big{(}G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}} \Big{[}V^{s^{\prime}}_{t}\Big{]}\Big{)}y^{s}-x^{s}\Big{(}G^{s}+\gamma\mathbb{E }_{s^{\prime}\sim P^{s}}\Big{[}V^{s^{\prime}}_{t}\Big{]}\Big{)}y^{s}_{t}\] \[=\hat{x}^{s}_{\tau}\Big{(}G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P ^{s}}\Big{[}V^{s^{\prime}}_{t_{*}(s)}\Big{]}\Big{)}y^{s}-x^{s}\Big{(}G^{s}+ \gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\Big{[}V^{s^{\prime}}_{t_{*}(s)}\Big{]} \Big{)}\hat{y}^{s}_{\tau}\] \[\leq\hat{x}^{s}_{\tau\star}\Big{(}G^{s}+\gamma\mathbb{E}_{s^{ \prime}\sim P^{s}}\Big{[}V^{s^{\prime}}_{t_{*}(s)}\Big{]}\Big{)}y^{s}-x^{s} \Big{(}G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\Big{[}V^{s^{\prime}}_{t_{ *}(s)}\Big{]}\Big{)}\hat{y}^{s}_{\tau\star}+2\max_{s^{\prime}}|V^{s^{\prime}} _{t}-V^{s^{\prime}}_{t_{*}(s)}|\] \[\leq 2\epsilon_{\tau}\ln(A)+\mathcal{O}\left(\frac{1}{\tau} \right)+2\max_{s^{\prime}}|V^{s^{\prime}}_{t}-V^{s^{\prime}}_{t_{*}(s)}|+ \mathcal{O}(\sqrt{\text{KL}(\hat{z}^{s}_{\tau},\hat{z}^{s}_{\tau\star})}) \text{(Lemma~{}\ref{lem:def})}\] \[\leq\mathcal{O}\bigg{(}\frac{\ln(A)}{1-\gamma}\tau^{-k_{\epsilon}} \bigg{)}+\mathcal{O}\left(\frac{L\ln(St/\delta)\tau^{-k_{\alpha}}}{1-\gamma} \right)+\mathcal{O}\left(\sqrt{A\ln^{5}(SATt/\delta)L^{2}}\tau^{-\frac{k_{ \epsilon}}{2}}\right).\text{ (Lemma~{}\ref{lem:def})}\]

Combing the above two inequality with Lemma 17 and Lemma 20 and the choice of parameters \(k_{\alpha}=\frac{\gamma}{9+\varepsilon}\), \(k_{\varepsilon}=\frac{1}{9+\varepsilon}\), \(k_{\beta}=\frac{3}{9+\varepsilon}\), and \(k_{\eta}=\frac{5}{9+\varepsilon}\), we have \(k_{\sharp}=\min\{k_{\beta}-k_{\epsilon},k_{\eta}-k_{\beta},k_{\alpha}-k_{\eta} -2k_{\epsilon}\}=\frac{2}{9+\varepsilon}\), \(k_{*}=\min\{k_{\eta},k_{\beta},k_{\alpha}-k_{\beta},k_{\epsilon}\}=\frac{1}{9+\varepsilon}\), and

\[\max_{s,x,y}\big{(}V^{s}_{x_{t},y}-V^{s}_{x,y_{t}}\big{)}\] \[\leq\frac{2}{1-\gamma}\max_{s,x,y}\Big{(}x^{s}_{t}\Big{(}G^{s}+ \gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\Big{[}V^{s^{\prime}}_{t}\Big{]}\Big{)}y^ {s}-x^{s}\Big{(}G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\Big{[}V^{s^{\prime}}_ {t}\Big{]}\Big{)}y^{s}_{t}\Big{)}+\frac{4\gamma}{1-\gamma}\max_{s}|V^{s}_{t}-V^{ s}_{\star}|\] \[\leq\mathcal{O}\Bigg{(}\frac{\sqrt{A\ln^{5/2}(SAt/\delta)}}{(1- \gamma)^{2}}L\tau^{-\min\{k_{\epsilon},\frac{k_{\epsilon}}{2},k_{\alpha}\}} \Bigg{)}+\mathcal{O}\left(\frac{A\ln^{3}(SAt/\delta)}{(1-\gamma)^{3}}\left( \frac{L\ln(St/\delta)}{1-\gamma}\ln\frac{t}{1-\gamma}\right)^{\frac{k_{\epsilon}} {1-k_{\alpha}}}\left(\frac{L\ln(St/\delta)}{t}\right)^{k_{*}}\Bigg{)}\]\[=\mathcal{O}\Bigg{(}\frac{AL^{2+1/\varepsilon}\ln^{4+1/\varepsilon}(SAt/ \delta)\ln^{1/\varepsilon}(t/(1-\gamma))}{(1-\gamma)^{3+1/\varepsilon}}\cdot t^{ -\frac{1}{9+\varepsilon}}\Bigg{)}.\] ( \[\tfrac{t}{L}\leq\tau\leq t\] )

## Appendix E Convergent Analysis of Algorithm 3

### Part I. Basic Iteration Properties

**Definition 1**.: _Let \(t_{\tau}(s)\) be the \(\tau\)-th time the players visit state \(s\). Define \(\hat{x}_{\tau}^{s}=x_{t_{\tau}(s)}^{s}\), \(\hat{y}_{\tau}^{s}=y_{t_{\tau}(s)}^{s}\), \(\hat{a}_{\tau}^{s}=a_{t_{\tau}(s)}\), \(\hat{b}_{\tau}^{s}=b_{t_{\tau}(s)\cdot}\). Furthermore, define_

\[\underline{f}_{\tau}^{s}(x,y) \triangleq x^{\top}\left(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P ^{s}}\left[\underline{V}_{t_{\tau}(s)}^{s^{\prime}}\right]\right)y-\epsilon \phi(x)+\epsilon\phi(y),\] \[\overline{f}_{\tau}^{s}(x,y) \triangleq x^{\top}\left(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P ^{s}}\left[\overline{V}_{t_{\tau}(s)}^{s^{\prime}}\right]\right)y-\epsilon \phi(x)+\epsilon\phi(y).\]

**Lemma 22**.: _For any \(x^{s}\in\Omega\),_

\[\underline{f}_{\tau}^{s}(\hat{x}_{\tau}^{s},\hat{y}_{\tau}^{s})- \underline{f}_{\tau}^{s}(x^{s},\hat{y}_{\tau}^{s})\] \[\leq\frac{(1-\eta\epsilon)\mbox{KL}(x^{s},\hat{x}_{\tau}^{s})- \mbox{KL}(x^{s},\hat{x}_{\tau+1}^{s})}{\eta}+\frac{10\eta A\ln^{2}(AT)}{(1- \gamma)^{2}}+\frac{2\eta A}{(1-\gamma)^{2}}\underline{\lambda}_{\tau}^{s}+ \underline{\xi}_{\tau}^{s}+\underline{\zeta}_{\tau}^{s}(x^{s}).\]

_where_

\[\underline{\lambda}_{\tau}^{s} =\frac{1}{A}\sum_{a}\left(\frac{\mathbf{1}[\hat{a}_{\tau}^{s}=a]} {\hat{x}_{\tau,a}^{s}+\beta}-1\right),\] \[\underline{\xi}_{\tau}^{s} =(\hat{x}_{\tau}^{s})^{\top}\left(\left(G^{s}+\gamma\mathbb{E}_{ s^{\prime}\sim P^{s}}\left[\underline{V}_{t}^{s^{\prime}}\right]\right)\hat{y}_{ \tau}^{s}-\frac{\mathbf{1}[\hat{a}_{\tau}^{s}=a]\left(\sigma_{t}+\gamma \underline{V}_{t}^{s_{t+1}}\right)}{\hat{x}_{\tau,a}^{s}+\beta}\right),\] \[\underline{\zeta}_{\tau}^{s}(x^{s}) =(x^{s})^{\top}\left(\frac{\mathbf{1}[\hat{a}_{\tau}^{s}=a] \left(\sigma_{t}+\gamma\underline{V}_{t}^{s_{t+1}}\right)}{\hat{x}_{\tau,a}^ {s}+\beta}-\left(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\left[ \underline{V}_{t}^{s^{\prime}}\right]\right)\hat{y}_{\tau}^{s}\right).\]

Proof.: The proof is exactly the same as that of Lemma 16. 

### Part II. Value Convergence

**Lemma 23** (weighted regret bound).: _There exists a large enough universal constant \(\kappa\) (used in the definition of \(\textsf{bnS}_{\tau}\)) such that with probability \(1-\mathcal{O}(\delta)\), for any state \(s\), visitation count \(\tau\), and any \(x^{s}\in\Omega\),_

\[\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(\underline{f}_{i}^{s}( \hat{x}_{i}^{s},\hat{y}_{i}^{s})-\underline{f}_{i}^{s}(x^{s},\hat{y}_{i}^{s}) \right)\leq\frac{1}{2}\textsf{bnS}_{\tau}.\]

Proof.: Fix state \(s\) and visitation count \(\tau\leq T\). Applying Lemma 22 and considering the weighted sum of the bounds, we get

\[\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(\underline{f}_{i}^{s}( \hat{x}_{i}^{s},\hat{y}_{i}^{s})-\underline{f}_{i}^{s}(x^{s},\hat{y}_{i}^{s})\right)\] \[\leq\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(\frac{(1-\eta\epsilon) \mbox{KL}(x^{s},\hat{x}_{i}^{s})-\mbox{KL}(x^{s},\hat{x}_{i+1}^{s})}{\eta}+ \frac{10\eta A\ln^{2}(AT)}{(1-\gamma)^{2}}+\frac{2\eta A}{(1-\gamma)^{2}} \underline{\lambda}_{i}^{s}+\underline{\xi}_{i}^{s}+\underline{\zeta}_{i}^{s} (x^{s})\right)\] \[\leq\underbrace{\frac{\alpha_{\tau}^{1}(1-\eta\epsilon)}{\eta} \mbox{KL}(x^{s},\hat{x}_{1}^{s})+\sum_{i=2}^{\tau}\left(\frac{\alpha_{\tau}^{i}(1 -\eta\epsilon)}{\eta}\mbox{KL}(x^{s},\hat{x}_{i}^{s})-\frac{\alpha_{\tau}^{i-1 }}{\eta}\mbox{KL}(x^{s},\hat{x}_{i}^{s})\right)}_{\mbox{term}_{0}}\]\[\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(\underline{f}_{i}^{s}( \hat{x}_{i}^{s},\hat{y}_{i}^{s})-\underline{f}_{i}^{s}(x^{s},\hat{y}_{i}^{s})\right) =\mathcal{O}\left(\frac{A\ln^{2}(AST/\delta)\left(\eta+\beta+(\eta ^{-1}+\beta^{-1})\alpha_{\tau}\right)}{(1-\gamma)^{2}}\right)\] \[=\mathcal{O}\left(\frac{A\ln^{2}(SAT/\delta)(\beta+\alpha_{\tau}/ \eta)}{(1-\gamma)^{2}}\right).\] (using \[\eta\leq\beta\] )

This implies the conclusion of the lemma. 

**Lemma 24**.: _For all \(t,s\), \(\overline{V}_{t}^{s}\geq\underline{V}_{t}^{s}\)._

Proof.: We prove it by induction on \(t\). The inequality clearly holds for \(t=1\) by the initialization. Suppose that the inequality holds for \(1,2,\ldots,t-1\) and for all \(s\). Now consider time \(t\) and state \(s\). Let \(\tau=n_{t}^{s}\), and let \(1\leq t_{1}<t_{2}<\ldots<t_{\tau}<t\) be the time indices when the players visit state \(s\). By the update rule,

\[\widetilde{V}_{t}^{s}-\underline{V}_{t}^{s}=\sum_{i=1}^{\tau}\alpha_{\tau}^{i} \left(\gamma\overline{V}_{t_{i}}^{s_{t_{i}+1}}-\gamma\underline{V}_{t_{i}}^{s _{t_{i}+1}}+2\mathsf{bns}_{i}\right)>0\]where the inequality is by the induction hypothesis. Therefore,

\[\overline{V}_{t}^{s}-\underline{V}_{t}^{s}=\min\left\{\widetilde{V}_{t}^{s},H \right\}-\max\left\{V_{t}^{s},0\right\}>0.\]

In the last inequality we also use the fact that \(\underline{V}_{t}^{s}\leq H\) and \(\widetilde{V}_{t}^{s}\geq 0\). Note that by the induction hypothesis and the update rule of \(\overline{V}_{t}^{s}\) and \(\underline{V}_{t}^{s}\), we have \(0\leq\underline{V}_{i}^{s}<\overline{V}_{i}^{s}\leq H\) for all \(s\) and \(1\leq i\leq t-1\). Thus \(\underline{V}_{t}^{s}=\sum_{i=1}^{\tau}\alpha_{\tau}^{i}(\gamma\underline{V}_ {t_{i}^{s_{t+1}}}-\textsf{bn}\underline{s}_{i})\leq H\) and similarly \(\widetilde{V}_{t}^{s}\geq 0\). 

**Lemma 25**.: _Let \(c=(c_{1},\ldots,c_{T})\) be any non-negative sequence with \(c_{i}\leq c_{\max}\forall i\) and \(\sum_{t=1}^{T}c_{t}=C\). Then_

\[\sum_{t=1}^{T}c_{t}\left(\overline{V}_{t}^{s_{t+1}}-\underline{V}_{t}^{s_{t+1 }}\right)\leq\mathcal{O}\left(\frac{CA\ln^{3}(AST/\delta)\beta}{(1-\gamma)^{3 }}+\frac{c_{\max}AS\ln^{4}(AST/\delta)}{\eta(1-\gamma)^{3}}\right).\]

Proof.: \[Z_{c}\triangleq\sum_{t=1}^{T}c_{t}\left(\overline{V}_{t}^{s_{t+1 }}-V_{t}^{s_{t+1}}\right)\] \[\leq\sum_{t=1}^{T}c_{t}\left(\overline{V}_{t+1}^{s_{t+1}}- \underline{V}_{t+1}^{s_{t+1}}\right)+\mathcal{O}\left(c_{\max}\sum_{s}\sum_{t =1}^{T}\left(\left|\overline{V}_{t+1}^{s}-\overline{V}_{t}^{s}\right|+\left| \underline{V}_{t+1}^{s}-\underline{V}_{t}^{s}\right|\right)\right)\] \[\leq\sum_{t=1}^{T}c_{t-1}\left(\overline{V}_{t}^{s_{t}}- \underline{V}_{t}^{s_{t}}\right)+\mathcal{O}\left(c_{\max}\sum_{s}\sum_{i=1}^ {T}\frac{\alpha_{i}}{1-\gamma}\right)\] (shifting the indices and define \[c_{0}=0\] ) \[=\sum_{s}\sum_{\tau=1}^{n_{T+1}(s)}c_{t_{\tau}(s)-1}\left( \widetilde{V}_{t_{\tau}(s)}^{s}-\underline{V}_{t_{\tau}(s)}^{s}\right)+ \mathcal{O}\left(\frac{c_{\max}S\ln^{2}T}{(1-\gamma)^{2}}\right)\] ( \[H=\frac{\ln T}{1-\gamma}\] ) \[\leq\gamma\sum_{s}\sum_{\tau=2}^{n_{T+1}(s)}c_{t_{\tau}(s)-1} \sum_{i=1}^{\tau-1}\alpha_{\tau-1}^{i}\left(\overline{V}_{t_{\tau}(s)}^{s_{t_ {\tau}(s)+1}}-\underline{V}_{t_{\tau}(s)}^{s_{t_{\tau}(s)+1}}+2\textsf{bn} \underline{s}_{i}\right)+\mathcal{O}\left(\frac{c_{\max}S\ln^{2}T}{(1-\gamma )^{2}}\right)\] \[\leq\gamma\sum_{s}\sum_{i=1}^{n_{T+1}(s)-1}\underbrace{\left( \sum_{\tau=i+1}^{n_{T+1}(s)}\alpha_{\tau-1}^{i}c_{t_{\tau}(s)-1}\right)}_{c_{t_ {\tau}(s)}^{\prime}}\left(\overline{V}_{t_{\tau}(s)}^{s_{t_{\tau}(s)+1}}- \underline{V}_{t_{\tau}(s)}^{s_{t_{\tau}(s)+1}}\right)\] \[\qquad\qquad+\mathcal{O}\left(\sum_{s}\sum_{\tau=2}^{n_{T+1}(s) }c_{t_{\tau}(s)-1}\textsf{bn}\underline{s}_{\tau-1}+\frac{c_{\max}S\ln^{2}T}{(1 -\gamma)^{2}}\right)\] \[\leq\gamma\sum_{t=1}^{T}c_{t}^{\prime}\left(\overline{V}_{t}^{s_{ t+1}}-\underline{V}_{t}^{s_{t+1}}\right)\] \[\leq\gamma\sum_{t=1}^{T}c_{t}^{\prime}\left(\overline{V}_{t}^{s_{ t+1}}-\underline{V}_{t}^{s_{t+1}}\right)+\mathcal{O}\left(\sum_{s}\sum_{\tau=1}^{C_{s}/c_{ \max}}c_{\max}\textsf{bn}\underline{s}_{\tau}+\frac{c_{\max}S\ln^{2}T}{(1- \gamma)^{2}}\right)\] \[\leq\gamma\sum_{t=1}^{T}c_{t}^{\prime}\left(\overline{V}_{t}^{s_{ t+1}}-\underline{V}_{t}^{s_{t+1}}\right)\] \[\leq\gamma\sum_{t=1}^{T}c_{t}^{\prime}\left(\overline{V}_{t}^{s_{ t+1}}-\underline{V}_{t}^{s_{t+1}}\right)+\mathcal{O}\left(\frac{CA\ln^{2}(AST/ \delta)\beta}{(1-\gamma)^{2}}+\frac{c_{\max}AS\ln^{3}(AST/\delta)}{\eta(1- \gamma)^{2}}\right).\] (19)Note that \(c_{t}^{\prime}\) is another sequence with

\[c_{i}^{\prime}\leq c_{\max}^{\prime}\leq c_{\max}\sup_{i}\sum_{\tau=i}^{\infty} \alpha_{\tau}^{i}\leq\left(1+\frac{1}{H}\right)c_{\max}\]

and

\[\sum_{t=1}^{T}c_{t}^{\prime}\leq\sum_{t=1}^{T}c_{t}=C\]

since \(\sum_{i=1}^{\tau}\alpha_{\tau}^{i}=1\) for any \(\tau\geq 1\). Thus, we can unroll the inequality Eq.19 for \(H\) times, which gives

\[Z_{c} \leq\gamma^{H}\left(1+\frac{1}{H}\right)^{H}\frac{c_{\max}T}{1- \gamma}+H\times\mathcal{O}\left(\frac{CA\ln^{2}(AST/\delta)\beta}{(1-\gamma)^ {2}}+\frac{c_{\max}AS\ln^{3}(AST/\delta)}{\eta(1-\gamma)^{2}}\right)\] \[=\mathcal{O}\left(\frac{CA\ln^{3}(AST/\delta)\beta}{(1-\gamma)^{ 3}}+\frac{c_{\max}AS\ln^{4}(AST/\delta)}{\eta(1-\gamma)^{3}}\right)\]

where in the inequality we use that \((1+\frac{1}{H})^{H}\leq e\) and \(\gamma^{H}=(1-(1-\gamma))^{H}\leq e^{-(1-\gamma)H}=\frac{1}{T}\). 

**Corollary 2**.: _There exists a universal constant \(C_{1}>0\) such that for any \(\tilde{\epsilon}\geq\frac{C_{1}A\ln^{3}(AST/\delta)\beta}{(1-\gamma)^{3}}\), with probability at least \(1-\mathcal{O}(\delta)\),_

\[\sum_{t=1}^{T}\mathbf{1}\left[x_{t}^{s_{t}\top}\left(\mathbb{E}_{s^{\prime} \sim P^{s_{t}}}\left[\overline{V}_{t}^{s^{\prime}}-\underline{V}_{t}^{s^{ \prime}}\right]\right)y_{t}^{s_{t}}\geq\tilde{\epsilon}\right]\leq\mathcal{O} \left(\frac{AS\ln^{4}(AST/\delta)}{\eta\tilde{\epsilon}(1-\gamma)^{3}}\right).\]

Proof.: We apply Lemma25 with the following definition of \(c_{t}\):

\[c_{t}=\mathbf{1}\left[x_{t}^{s_{t}}\left(\mathbb{E}_{s^{\prime}\sim P^{s_{t}}} \left[\overline{V}_{t}^{s^{\prime}}-\underline{V}_{t}^{s^{\prime}}\right] \right)y_{t}^{s_{t}}\geq\tilde{\epsilon}\right],\]

which gives

\[\sum_{t=1}^{T}c_{t}\left(\overline{V}_{t}^{s_{t+1}}-\underline{V}_{t}^{s_{t+1 }}\right)\leq C_{2}\times\left(\frac{CA\ln^{3}(AST/\delta)\beta}{(1-\gamma)^{ 3}}+\frac{AS\ln^{4}(AST/\delta)}{\eta(1-\gamma)^{3}}\right)\] (20)

for some universal constant \(C_{2}\) and \(C=\sum_{t=1}^{T}c_{t}\). By Azuma's inequality, for some universal constant \(C_{3}>0\), with probability \(1-\delta\),

\[\sum_{t=1}^{T}c_{t}x_{t}^{s_{t}}\left(\mathbb{E}_{s^{\prime}\sim P ^{s_{t}}}\left[\overline{V}_{t}^{s^{\prime}}-\underline{V}_{t}^{s^{\prime}} \right]\right)y_{t}^{s_{t}}-\sum_{t=1}^{T}c_{t}\left(\overline{V}_{t}^{s_{t+1 }}-\underline{V}_{t}^{s_{t+1}}\right)\] \[\leq\frac{C_{3}}{1-\gamma}\sqrt{\ln(S/\delta)\sum_{t=1}^{T}c_{t} ^{2}}=\frac{C_{3}}{1-\gamma}\sqrt{\ln(S/\delta)C}\] \[\leq C_{3}\times\frac{C\beta}{1-\gamma}+C_{3}\times\frac{\ln(S/ \delta)}{\eta(1-\gamma)}.\] (by AM-GM and that \[\eta\leq\beta\] )

Combining Eq.20 and Eq.21, we get

\[\sum_{t=1}^{T}c_{t}x_{t}^{s_{t}}\left(\mathbb{E}_{s^{\prime}\sim P ^{s_{t}}}\left[\overline{V}_{t}^{s^{\prime}}-\underline{V}_{t}^{s^{\prime}} \right]\right)y_{t}^{s_{t}}\] \[\leq(C_{2}+C_{3})\times\left(\frac{CA\ln^{3}(AST/\delta)\beta}{( 1-\gamma)^{3}}+\frac{AS\ln^{4}(AST/\delta)}{\eta(1-\gamma)^{3}}\right).\]

[MISSING_PAGE_FAIL:36]

for all \(s\) and \(t^{\prime}<t\). Then by Eq. (22),

\[\underline{V}_{t}^{s} \leq\min_{x}\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(x^{s}\right)^{ \top}\left(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\left[V_{\star}^{s^{ \prime}}+\frac{C_{4}\epsilon\ln(AT)}{1-\gamma}\right]\right)y_{t_{i}}^{s}+C_{4} \epsilon\ln(AT)\] \[=\min_{x}\sum_{i=1}^{\tau}\alpha_{\tau}^{i}\left(x^{s}\right)^{ \top}\left(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\left[V_{\star}^{s^{ \prime}}\right]\right)y_{t_{i}}^{s}+\frac{C_{4}\epsilon\ln(AT)}{1-\gamma}\] \[\leq\min_{x}\sum_{i=1}^{\tau}\max_{y}\alpha_{\tau}^{i}\left(x^{s} \right)^{\top}\left(G^{s}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s}}\left[V_{ \star}^{s^{\prime}}\right]\right)y^{s}+\frac{C_{4}\epsilon\ln(AT)}{1-\gamma}\] \[=\min_{x}\max_{y}\left(x^{s}\right)^{\top}\left(G^{s}+\gamma \mathbb{E}_{s^{\prime}\sim P^{s}}\left[V_{\star}^{s^{\prime}}\right]\right)y^ {s}+\frac{C_{4}\epsilon\ln(AT)}{1-\gamma}\] \[=V_{\star}^{s}+\frac{C_{4}\epsilon\ln(AT)}{1-\gamma},\]

which proves the first desired inequality. The other inequality can be proven in the same way. 

### Part III. Policy Convergence to the Nash of the Regularized Game

**Lemma 27**.: _Let \(0\leq p\leq 1\) be arbitrarily chosen, and define_

\[f_{\tau}^{s}(x^{s},y^{s}) \triangleq p\underline{f}_{\tau}^{s}(x^{s},y^{s})+(1-p)\overline{f }_{\tau}^{s}(x^{s},y^{s})\] \[=x^{s\top}\left(G^{s}+\mathbb{E}_{s^{\prime}\sim P^{s}}\left[p \underline{V}_{t_{\tau}(s)}^{s^{\prime}}+(1-p)\overline{V}_{t_{\tau}(s)}^{s^ {\prime}}\right]\right)y^{s}-\epsilon\phi(x^{s})+\epsilon\phi(y^{s}).\]

_Furthermore, let \(\hat{z}_{\tau\star}^{s}=(\hat{x}_{\tau\star}^{s},\hat{y}_{\tau\star}^{s})\) be the equilibrium of \(f_{\tau}^{s}(x,y)\), and define \(z_{t\star}^{s}=\hat{z}_{\tau\star}^{s}\), where \(\tau=n_{t}(s)\). Then with probability at least \(1-\mathcal{O}(\delta)\), the following holds for any \(0<\epsilon^{\prime}\leq 1\):_

\[\sum_{s}\sum_{i=1}^{n_{\tau+1}(s)}\mathbf{1}\left[\text{KL}(\hat{z}_{i\star}^{ s},\hat{z}_{i}^{s})\geq\epsilon^{\prime}\right]\leq\mathcal{O}\left(\frac{S^{2}A \ln^{5}(SAT/\delta)}{\eta\epsilon^{2}\epsilon^{\prime}(1-\gamma)^{3}}\right)\]

_if \(\eta\) and \(\beta\) satisfy the following_

\[\beta \leq\frac{C_{5}(1-\gamma)^{3}}{A\ln^{3}(AST/\delta)}\epsilon\epsilon ^{\prime}\] (23) \[\eta \leq\frac{C_{6}(1-\gamma)}{A\ln^{3}(AST/\delta)}\beta\epsilon^{\prime}\] (24)

_with sufficiently small universal constant \(C_{5},C_{6}>0\)._

Proof.: In this proof, we write \(\underline{\zeta}_{i}^{s}(\hat{x}_{i\star}^{s})\) as \(\underline{\zeta}_{i}\). By Lemma 22, we have

\[\text{KL}(\hat{x}_{i\star}^{s},\hat{x}_{i+1}^{s}) \leq(1-\eta\epsilon)\text{KL}(\hat{x}_{i\star}^{s},\hat{x}_{i}^{ s})+\eta\left(\underline{f}_{i}^{s}(\hat{x}_{i\star}^{s},\hat{y}_{i}^{s})- \underline{f}_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i}^{s})\right)\] \[\qquad+\frac{10\eta^{2}A\ln^{2}(AT)}{(1-\gamma)^{2}}+\frac{2\eta ^{2}A}{(1-\gamma)^{2}}\underline{\lambda}_{i}^{s}+\eta\underline{\xi}_{i}^{s} +\eta\underline{\zeta}_{i}^{s}.\]

Similarly,

\[\text{KL}(\hat{y}_{i\star}^{s},\hat{y}_{i+1}^{s}) \leq(1-\eta\epsilon)\text{KL}(\hat{y}_{i\star}^{s},\hat{y}_{i}^{s })+\eta\left(\overline{f}_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i}^{s})-\overline{f }_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i\star}^{s})\right)\] \[\qquad+\frac{10\eta^{2}A\ln^{2}(AT)}{(1-\gamma)^{2}}+\frac{2\eta ^{2}A}{(1-\gamma)^{2}}\overline{\lambda}_{i}^{s}+\eta\overline{\xi}_{i}^{s}+ \eta\overline{\zeta}_{i}^{s}.\]

Adding the two inequalities up, we get

\[\text{KL}(\hat{z}_{i+1\star}^{s},\hat{z}_{i+1}^{s})\] \[\leq(1-\eta\epsilon)\text{KL}(\hat{z}_{i\star}^{s},\hat{z}_{i}^{ s})+\frac{20\eta^{2}A\ln^{2}(AT)}{(1-\gamma)^{2}}+\frac{2\eta^{2}A}{(1-\gamma)^{2}} \lambda_{i}^{s}+\eta\xi_{i}^{s}+\eta\zeta_{i}^{s}+v_{i}^{s}\]\[\leq(1-\eta\epsilon)\text{KL}(\hat{z}^{s}_{i\star},\hat{z}^{s}_{i})+ \frac{20\eta^{2}A\ln^{2}(AT)}{(1-\gamma)^{2}}+\frac{2\eta^{2}A}{(1-\gamma)^{2}} \lambda^{s}_{i}+\eta\xi^{s}_{i}+\eta\zeta^{s}_{i}+v^{s}_{i}+\frac{1}{2}\eta \epsilon\epsilon^{\prime}+\left[\eta\Delta^{s}_{i}-\frac{1}{2}\eta\epsilon \epsilon^{\prime}\right]_{+}\]

Unrolling the recursion, we get with probability at least \(1-\mathcal{O}(\delta)\), for all \(s\) and \(\tau\) (we show that the inequality holds for any fix \(s\) and \(\tau\) with probability \(1-\mathcal{O}(\frac{\delta}{ST})\) and then apply the union bound over \(s\) and \(\tau\)),

\[\text{KL}(\hat{z}^{s}_{\tau+1\star},\hat{z}^{s}_{\tau+1})\] \[\leq(1-\eta\epsilon)^{\tau}\text{KL}(\hat{z}^{s}_{1\star},\hat{z }^{s}_{1})+\underbrace{\frac{20\eta^{2}A\ln^{2}(AT)}{(1-\gamma)^{2}}\sum_{i= 1}^{\tau}(1-\eta\epsilon)^{\tau-i}}_{\text{\bf term}_{1}}+\underbrace{\frac{2 \eta^{2}A}{(1-\gamma)^{2}}\sum_{i=1}^{\tau}(1-\eta\epsilon)^{\tau-i}\lambda^{s }_{i}}_{\text{\bf term}_{2}}\] \[\qquad+\underbrace{\eta\sum_{i=1}^{\tau}(1-\eta\epsilon)^{\tau-i }\xi^{s}_{i}}_{\text{\bf term}_{3}}+\underbrace{\eta\sum_{i=1}^{\tau}(1-\eta \epsilon)^{\tau-i}\zeta^{s}_{i}}_{\text{\bf term}_{4}}\] \[\qquad+\underbrace{\sum_{i=1}^{\tau}(1-\eta\epsilon)^{\tau-i}v^{ s}_{i}}_{\triangleq\text{\bf term}_{5}(s,\tau)}+\underbrace{\frac{1}{2}\eta \epsilon\epsilon^{\prime}\sum_{i=1}^{\tau}(1-\eta\epsilon)^{\tau-i}}_{ \triangleq\text{\bf term}_{6}(s,\tau)}\] \[\overset{(a)}{\leq}\mathcal{O}(e^{-\eta\epsilon\tau}\ln(AT))+ \ln^{3}(AST/\delta)\times\mathcal{O}\left(\frac{\eta A}{\epsilon(1-\gamma)^{2} }+\frac{\eta^{2}A}{\beta(1-\gamma)^{2}}+\frac{\beta A}{\epsilon(1-\gamma)}+ \frac{1}{1-\gamma}\sqrt{\frac{\eta}{\epsilon}}+\frac{\eta}{\beta(1-\gamma)}\right)\] \[\qquad+\text{\bf term}_{5}(s,\tau)+\frac{1}{2}\epsilon^{\prime }+\text{\bf term}_{6}(s,\tau)\] \[\overset{(b)}{\leq}\mathcal{O}\left(e^{-\eta\epsilon\tau}\ln(AT )\right)+\frac{3}{4}\epsilon^{\prime}+\text{\bf term}_{5}(s,\tau)+\text{\bf term }_{6}(s,\tau)\] (26)

where in \((a)\) we use the following calculation:

\[\text{\bf term}_{1} \leq\mathcal{O}\left(\frac{\eta^{2}A\ln^{2}(AT)}{(1-\gamma)^{2}} \times\frac{1}{\eta\epsilon}\right)\leq\mathcal{O}\left(\frac{\eta A\ln^{2}(AT )}{\epsilon(1-\gamma)^{2}}\right).\] \[\text{\bf term}_{2} \leq\mathcal{O}\left(\frac{\eta^{2}A}{(1-\gamma)^{2}}\frac{\max_ {i\leq\tau}(1-\eta\epsilon)^{\tau-i}\ln(AST/\delta)}{\beta}\right)=\mathcal{O} \left(\frac{\eta^{2}A}{(1-\gamma)^{2}}\frac{\ln(AST/\delta)}{\beta}\right)\] (by Lemma 8) \[\text{\bf term}_{3} \leq\mathcal{O}\left(\frac{\eta A}{1-\gamma}\sum_{i=1}^{\tau} \beta(1-\eta\epsilon)^{\tau-i}+\eta\sqrt{\ln(AST/\delta)\sum_{i=1}^{\tau}(1- \eta\epsilon)^{\tau-i}}\right)\] (by Lemma 6) \[=\mathcal{O}\left(\frac{\beta A}{\epsilon(1-\gamma)}+\sqrt{\ln( AS/\delta)\frac{\eta}{\epsilon}}\right).\] \[\text{\bf term}_{4} \leq\mathcal{O}\left(\frac{\eta}{1-\gamma}\times\frac{\max_{i \leq\tau}(1-\eta\epsilon)^{\tau-i}\ln(AST/\delta)}{\beta}\right)=\mathcal{O} \left(\frac{\eta\ln(AST/\delta)}{\beta(1-\gamma)}\right),\] (by Lemma 8)and in \((b)\) we use the conditions Eq. (23) and Eq. (24).

We continue to bound the sum of \(\textbf{term}_{5}\) and \(\textbf{term}_{6}\) over \(t\). Note that

\[\sum_{s}\sum_{\tau=1}^{n_{T+1}(s)}\textbf{term}_{5}(s,\tau)\leq\sum_{s}\sum_{ \tau=1}^{n_{T+1}(s)}\sum_{i=1}^{\tau}(1-\eta\epsilon)^{\tau-i}v_{i}^{s}\leq \frac{1}{\eta\epsilon}\sum_{s}\sum_{i=1}^{n_{T+1}(s)}v_{i}^{s}\leq\mathcal{O} \left(\frac{S^{2}\ln^{3}(AT)}{\eta\epsilon^{2}(1-\gamma)^{2}}\right),\] (27)

where in the last inequality we use the following calculation:

\[\sum_{i=1}^{n_{T+1}(s)}|v_{i}^{s}| \leq\mathcal{O}\left(\ln(A\tau)\right)\times\sum_{i=1}^{n_{T+1}(s )}\|\hat{z}_{i\star}^{s}-\hat{z}_{i+1\star}^{s}\|_{1}\] (by Lemma 14 ) \[=\mathcal{O}\left(\ln(AT)\right)\times\frac{\ln(AT)}{\epsilon} \times\sum_{i=1}^{n_{T+1}(s)}\sup_{s^{\prime}}\left(p\left|\underline{V}_{t_{ i}}^{s^{\prime}}-\underline{V}_{t_{i+1}}^{s^{\prime}}\right|+(1-p)\left| \overline{V}_{t_{i}}^{s^{\prime}}-\overline{V}_{t_{i+1}}^{s^{\prime}}\right|\right)\] (by the same calculation as Eq. ( 12 )) \[\leq\mathcal{O}\left(\frac{\ln^{2}(AT)}{\epsilon}\right)\times \sum_{s^{\prime}}\sum_{t=1}^{T}\left(\left|\underline{V}_{t}^{s^{\prime}}- \underline{V}_{t+1}^{s^{\prime}}\right|+\left|\overline{V}_{t}^{s^{\prime}}- \overline{V}_{t+1}^{s^{\prime}}\right|\right)\] \[\leq\mathcal{O}\left(\frac{\ln^{2}(AT)}{\epsilon}\times\frac{S \ln T}{(1-\gamma)^{2}}\right)\] ( \[\underline{|V}_{t}^{s}-\underline{V}_{t+1}^{s}|\leq\frac{H+1}{H+ \tau}\times\frac{1}{1-\gamma}\mathbf{1}[s_{t}=s]\text{ by the update rule}\] \[=\mathcal{O}\left(\frac{S\ln^{3}(AT)}{\epsilon(1-\gamma)^{2}} \right),\]

and that

\[\sum_{s}\sum_{\tau=1}^{n_{T+1}(s)}\textbf{term}_{6}(s,\tau)\] \[=\sum_{s}\sum_{\tau=1}^{n_{T+1}(s)}\eta\sum_{i=1}^{\tau}(1-\eta \epsilon)^{\tau-i}\left[\Delta_{i}^{s}-\frac{1}{2}\epsilon\epsilon^{\prime} \right]_{+}\] \[\leq\sum_{s}\sum_{i=1}^{n_{T+1}(s)}\sum_{\tau=i}^{n_{T+1}(s)} \eta(1-\eta\epsilon)^{\tau-i}\left[\Delta_{i}^{s}-\frac{1}{2}\epsilon\epsilon^ {\prime}\right]_{+}\] \[\leq\frac{1}{\epsilon}\sum_{s}\sum_{i=1}^{n_{T+1}(s)}\left[ \Delta_{i}^{s}-\frac{1}{2}\epsilon\epsilon^{\prime}\right]_{+}\] \[=\frac{1}{\epsilon}\sum_{s}\sum_{i=1}^{n_{T+1}(s)}\sum_{j=-1}^{j _{\max}}\mathbf{1}\left[\epsilon\epsilon^{\prime}2^{j}\leq\Delta_{i}^{s}\leq \epsilon\epsilon^{\prime}2^{j+1}\right]\epsilon\epsilon^{\prime}2^{j+1}\qquad \text{(define $j_{\max}=\log_{2}\left(\frac{1}{(1-\gamma)\epsilon\epsilon^{ \prime}}\right)$)}\] \[\leq\frac{1}{\epsilon}\sum_{j=-1}^{j_{\max}}\mathcal{O}\left( \frac{AS\ln^{4}(AST/\delta)}{\eta\epsilon\epsilon^{\prime}2^{j}(1-\gamma)^{3}} \right)\times\epsilon\epsilon^{\prime}2^{j+1}\] (by Corollary 2 with \[\tilde{\epsilon}=\epsilon\epsilon^{\prime}2^{j}\] and the assumption that \[\epsilon\epsilon^{\prime}\gtrsim\frac{A\ln^{3}(AST/\delta)\beta}{(1-\gamma)^{3}}\] \[=\mathcal{O}\left(\frac{AS\ln^{5}(AST/\delta)}{\eta\epsilon(1- \gamma)^{3}}\right)\qquad\text{(without loss of generality, assume $\log_{2}\left(\frac{1}{(1-\gamma)\epsilon\epsilon^{\prime}}\right)$ }\lesssim\log T)\] (28)

From Eq. (26), we have

\[\sum_{s}\sum_{\tau=1}^{n_{T+1}(s)}\mathbf{1}\left[\text{KL}(\hat{z}_{\tau\star} ^{s},\hat{z}_{\tau}^{s})\geq\epsilon^{\prime}\right]\]\[\leq\sum_{s}\sum_{\tau=1}^{n_{T+1(s)}}\mathbf{1}\left[\mathcal{O}(e^{- \eta e\tau}\ln(AT))\geq\frac{1}{12}\epsilon^{\prime}\right]+\sum_{s}\sum_{\tau= 1}^{n_{T+1(s)}}\mathbf{1}\left[\mathbf{term}_{5}(s,\tau)>\frac{1}{12}\epsilon^{ \prime}\right]\] \[\qquad+\sum_{s}\sum_{\tau=1}^{n_{T+1(s)}}\mathbf{1}\left[\mathbf{ term}_{6}(s,\tau)>\frac{1}{12}\epsilon^{\prime}\right]\] \[\leq S\times\mathcal{O}\left(\frac{\ln(AT)}{\eta\epsilon\epsilon \epsilon^{\prime}}\right)+\mathcal{O}\left(\frac{S^{2}\ln^{3}(AT)}{\eta \epsilon^{2}\epsilon^{\prime}(1-\gamma)^{2}}\right)+\mathcal{O}\left(\frac{AS \ln^{5}(AST/\delta)}{\eta\epsilon\epsilon^{\prime}(1-\gamma)^{3}}\right)\] \[\leq\mathcal{O}\left(\frac{S^{2}A\ln^{5}(SAT/\delta)}{\eta \epsilon^{2}\epsilon^{\prime}(1-\gamma)^{3}}\right)\]

where in the second-to-last inequality we use Eq. (27) and Eq. (28). This finishes the proof.

### Part IV. Combining

**Theorem 5**.: _For any \(u\in\left[0,\frac{1}{1-\gamma}\right]\), there exists a proper choice of parameters \(\epsilon,\beta,\eta\) such that_

\[\sum_{t=1}^{T}\mathbf{1}\left[\max_{x,y}\left(x_{t}^{s_{t}^{\top}}Q_{\star}^{s_ {t}}y^{s_{t}}-x^{s_{t}^{\top}}Q_{\star}^{s_{t}}y_{t}^{s_{t}}\right)>u\right] \leq\mathcal{O}\left(\frac{S^{2}A^{3}\ln^{17}(SAT/\delta)}{u^{9}(1-\gamma)^{13 }}\right).\]

_with probability at least \(1-\mathcal{O}(\delta)\)._

Proof.: We will choose \(\epsilon\) such that \(u\geq C_{7}\frac{\epsilon\ln(AT)}{1-\gamma}\) with a sufficiently large universal constant \(C_{7}\). By Lemma 26, we have

\[\max_{x,y}\left(x_{t}^{s_{t}^{\top}}Q_{\star}^{s_{t}}y^{s_{t}}-x^ {s_{t}^{\top}}Q_{\star}^{s_{t}}y_{t}^{s_{t}}\right)\] \[\leq\max_{x,y}\left(x_{t}^{s_{t}^{\top}}\left(G^{s_{t}}+\gamma \mathbb{E}_{s^{\prime}\sim P^{s_{t}}}\left[\overline{V}_{t}^{s^{\prime}} \right]\right)y^{s_{t}}-x^{s_{t}^{\top}}\left(G^{s}+\gamma\mathbb{E}_{s^{ \prime}\sim P^{s_{t}}}\left[\underline{V}_{t}^{s^{\prime}}\right]\right)y_{t}^ {s_{t}}\right)+\mathcal{O}\left(\frac{\epsilon\ln(AT)}{1-\gamma}\right)\] \[\leq\max_{x,y}\left(x_{t}^{s_{t}^{\top}}\left(G^{s_{t}}+\gamma \mathbb{E}_{s^{\prime}\sim P^{s_{t}}}\left[\overline{V}_{t}^{s^{\prime}} \right]\right)y^{s_{t}}-x^{s_{t}^{\top}}\left(G^{s}+\gamma\mathbb{E}_{s^{ \prime}\sim P^{s_{t}}}\left[\underline{V}_{t}^{s^{\prime}}\right]\right)y_{t}^ {s_{t}}\right)+\frac{u}{4}.\]

Therefore, we can upper bound the left-hand side of the desired inequality by

\[\sum_{t=1}^{T}\mathbf{1}\left[\max_{x,y}\left(x_{t}^{s_{t}^{\top }}\left(G^{s_{t}}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s_{t}}}\left[\overline{ V}_{t}^{s^{\prime}}\right]\right)y^{s_{t}}-x^{s_{t}^{\top}}\left(G^{s}+\gamma \mathbb{E}_{s^{\prime}\sim P^{s_{t}}}\left[\underline{V}_{t}^{s^{\prime}} \right]\right)y_{t}^{s_{t}}\right)\geq\frac{3}{4}u\right]\] \[\leq\sum_{t=1}^{T}\mathbf{1}\left[\max_{y}x_{t}^{s_{t}^{\top}} \left(G^{s_{t}}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s_{t}}}\left[\underline{V }_{t}^{s^{\prime}}\right]\right)y^{s_{t}}-x_{t}^{s_{t}^{\top}}\left(G^{s_{t}}+ \gamma\mathbb{E}_{s^{\prime}\sim P^{s_{t}}}\left[\overline{V}_{t}^{s^{\prime}} \right]\right)y_{t}^{s_{t}}\geq\frac{u}{4}\right]\] \[\qquad+\sum_{t=1}^{T}\mathbf{1}\left[x_{t}^{s_{t}^{\top}}\left(G ^{s_{t}}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s_{t}}}\left[\overline{V}_{t}^{s^{ \prime}}\right]\right)y_{t}^{s_{t}}-x_{t}^{s_{t}^{\top}}\left(G^{s_{t}}+\gamma \mathbb{E}_{s^{\prime}\sim P^{s_{t}}}\left[\underline{V}_{t}^{s^{\prime}} \right]\right)y_{t}^{s_{t}}\geq\frac{u}{4}\right]\] \[\qquad+\sum_{t=1}^{T}\mathbf{1}\left[x_{t}^{s_{t}^{\top}}\left(G ^{s_{t}}+\gamma\mathbb{E}_{s^{\prime}\sim P^{s_{t}}}\left[\underline{V}_{t}^{s^{ \prime}}\right]\right)y^{s_{t}}-\min_{x}x^{s_{t}^{\top}}\left(G^{s_{t}}+\gamma \mathbb{E}_{s^{\prime}\sim P^{s_{t}}}\left[\underline{V}_{t}^{s^{\prime}} \right]\right)y_{t}^{s_{t}}\geq\frac{u}{4}\right].\] (29)

For the first term in Eq. (29), we can bound it by

\[\sum_{s}\sum_{i=1}^{n_{T+1}(s)}\mathbf{1}\left[\max_{y}\overline{f }_{i}^{s}(\hat{x}_{i}^{s},y^{s})-\overline{f}_{i}^{s}(\hat{x}_{i}^{s},\hat{y}_{i }^{s})\geq\frac{u}{4}-\mathcal{O}\left(\epsilon\ln(AT)\right)\right]\] \[\leq\sum_{s}\sum_{i=1}^{n_{T+1}(s)}\mathbf{1}\left[\max_{y} \overline{f}_{i}^{s}(\hat{x}_{i}^{s},y^{s})-\overline{f}_{i}^{s}(\hat{x}_{i}^{s}, \hat{y}_{i}^{s})\geq\frac{u}{8}\right]\]\[\leq\sum_{s}\sum_{i=1}^{n_{T+1(s)}}\mathbf{1}\left[\max_{y}\overline{f }_{i}^{s}(\hat{x}_{i\star}^{s},y^{s})-\overline{f}_{i}^{s}(\hat{x}_{i\star}^{s}, \hat{y}_{i\star}^{s})+\mathcal{O}\left(\|\hat{z}_{i}^{s}-\hat{z}_{i\star}^{s}\|_ {1}\frac{\ln(AT)}{1-\gamma}\right)\geq\frac{u}{8}\right]\] (because \[\|\nabla\overline{f}_{i}^{s}(x,y)\|_{\infty}\leq\mathcal{O}\left( \frac{\ln(AT)}{1-\gamma}\right)\] -- similar to the calculation in Eq. (9) (here we choose \[(\hat{x}_{i\star}^{s},\hat{y}_{i\star}^{s})\] to be the equilibrium under \[\overline{f}_{i}^{s}(x,y)\] ) \[\leq\sum_{s}\sum_{i=1}^{n_{T+1(s)}}\mathbf{1}\left[\mathcal{O} \left(\|\hat{z}_{i\star}^{s}-\hat{z}_{i\star}^{s}\|_{1}\frac{\ln(AT)}{1- \gamma}\right)\geq\frac{u}{8}\right]\] \[\leq\sum_{s}\sum_{i=1}^{n_{T+1(s)}}\mathbf{1}\left[\text{KL}( \hat{z}_{i\star}^{s},\hat{z}_{i}^{s})\geq\Omega\left(\frac{u^{2}(1-\gamma)^{2} }{\ln^{2}(AT)}\right)\right]\] \[\leq\mathcal{O}\left(\frac{S^{2}A\ln^{\gamma}(SAT/\delta)}{\eta \epsilon^{2}u^{2}(1-\gamma)^{5}}\right)\] (by Lemma 27 with \[\epsilon^{\prime}=\Theta\left(\frac{u^{2}(1-\gamma)^{2}}{\ln^{2}(AT)}\right)\] )

The third term in Eq. (29) can be bounded in the same way. The second term in Eq. (29) can be bounded using Corollary 2 by

\[\mathcal{O}\left(\frac{SA\ln^{4}(SAT/\delta)}{\eta u(1-\gamma)^{3}}\right).\]

Overall, we have

\[\sum_{t=1}^{T}\mathbf{1}\left[\max_{x,y}\left(x_{t}^{s_{t}^{\top}}Q_{\star}^{s_ {t}}y^{s_{t}}-x^{s_{t}^{\top}}Q_{\star}^{s_{t}}y^{s_{t}}_{t}\right)>u\right] \leq\mathcal{O}\left(\frac{S^{2}A\ln^{\gamma}(SAT/\delta)}{\eta\epsilon^{2}u^ {2}(1-\gamma)^{5}}\right).\] (30)

Notice that the parameters \(\epsilon,\beta,\eta\) needs to satisfy the conditions specified in this lemma and Lemma 27, with which we apply \(\epsilon^{\prime}=\Theta\left(\frac{u^{2}(1-\gamma)^{2}}{\ln^{2}(SAT/\delta)}\right)\). The constraints suggest the following parameter choice (under a fixed \(u\)):

\[\epsilon =\Theta\left(\frac{u(1-\gamma)}{\ln(SAT/\delta)}\right)\] \[\beta =\Theta\left(\frac{(1-\gamma)^{3}}{A\ln^{3}(SAT/\delta)}\epsilon \epsilon^{\prime}\right)=\Theta\left(\frac{u^{3}(1-\gamma)^{6}}{A\ln^{6}(SAT/ \delta)}\right)\] \[\eta =\Theta\left(\frac{(1-\gamma)}{A\ln^{3}(SAT/\delta)}\beta \epsilon^{\prime}\right)=\Theta\left(\frac{u^{5}(1-\gamma)^{9}}{A^{2}\ln^{11}( SAT/\delta)}\right)\]

Using these parameters in Eq. (30), we get

\[\sum_{t=1}^{T}\mathbf{1}\left[\max_{x,y}\left(x_{t}^{s_{t}^{\top}}Q_{\star}^{s _{t}}y^{s_{t}}-x^{s_{t}^{\top}}Q_{\star}^{s_{t}}y^{s_{t}}_{t}\right)>u\right] \leq\mathcal{O}\left(\frac{S^{2}A^{3}\ln^{20}(SAT/\delta)}{u^{9}(1-\gamma)^{ 16}}\right).\]

## Appendix F Discussions on Convergence Notions for General Markov Games

In general Markov games, learning the equilibrium policy pair _on every state_ is impossible because some state might have exponentially small visitation probability under all policies. Therefore, a reasonable definition of convergence is the convergence of the following quantity to zero:

\[\frac{1}{T}\sum_{t=1}^{T}\max_{x,y}\left(V_{x_{t},y}^{s_{t}}-V_{x,y_{t}}^{s_{t }}\right),\] (31)

which is similar to the best-iterate convergence defined in Section 3, but over the state sequence visited by the players instead of taking max over \(s\). It is also a strict generalization of the sample complexity bound for single-player MDPs under the discounted criteria (see e.g., [14, 15]).

The path convergence defined in our work is, on the other hand, that the following quantity converges to zero:

\[\frac{1}{T}\sum_{t=1}^{T}\max_{x,y}\left(x_{t}^{s_{t}^{\top}}Q_{\star}^{s_{t}}y^{s _{t}}-x^{s_{t}^{\top}}Q_{\star}^{s_{t}}y_{t}^{s_{t}}\right).\] (32)

Since \(\max_{y}(x^{s^{\top}}Q_{\star}^{s}y^{s})\leq\max_{y}(x^{s^{\top}}Q_{x,y}^{s}y^ {s})=\max_{y}V_{x,y}^{s}\) for any \(x\), the convergence of Eq. (31) is stronger than Eq. (32).

Implications of Path ConvergenceAlthough Eq. (32) does not imply the more standard best-iterate guarantee Eq. (31), it still has meaningful implications. By definition, It implies that frequent visits to a state bring players' policies closer to equilibrium, leading to both players using near-equilibrium policies for all but \(o(T)\) number of steps over time.

Path convergence also implies that both players have no regret compared to the game value \(V_{\star}^{s}\), which has been considered and motivated in previous works such as [11, 12]. To see this more clearly, we apply the results to the _episodic_ setting, where in every step, with probability \(1-\gamma\), the state is redrawn from \(s\sim\rho\) for some initial distribution \(\rho\) (every time the state is redrawn from \(\rho\), we call it a new episode). We can show that if Eq. (32) vanishes, then every player's long-term average payoff is at least the game value. First, notice that if Eq. (32) converges to zero, then

\[\sum_{t=1}^{T}(V_{\star}^{s_{t}}-x_{t}^{s_{t}^{\top}}Q_{\star}^{ s_{t}}y_{t}^{s_{t}}) \leq\max_{y}\sum_{t=1}^{T}\left(x_{t}^{s_{t}^{\top}}Q_{\star}^{s_{ t}}y^{s_{t}}-x_{t}^{s_{t}^{\top}}Q_{\star}^{s_{t}}y_{t}^{s_{t}}\right)\] \[\leq\sum_{t=1}^{T}\left(\max_{y}x_{t}^{s_{t}^{\top}}Q_{\star}^{s_{ t}}y^{s_{t}}-x_{t}^{s_{t}^{\top}}Q_{\star}^{s_{t}}y_{t}^{s_{t}}\right)=o(T).\] (33)

Now fix an \(i\) and let \(t_{i}\) be time index at the beginning of episode \(i\). Let \(E_{t}=1\) indicate the event that episode \(i\) has not ended at time \(t\). Then

\[\mathbb{E}\left[\sum_{t=t_{i}}^{t_{i+1}-1}\left(V_{\star}^{s_{t}} -x_{t}^{s_{t}^{\top}}Q_{\star}^{s_{t}}y_{t}^{s_{t}}\right)\right]\] \[=\mathbb{E}\left[\sum_{t=t_{i}}^{\infty}\mathbf{1}[E_{t}=1]\left( V_{\star}^{s_{t}}-x_{t}^{s_{t}^{\top}}G^{s_{t}}y_{t}^{s_{t}}-\gamma V_{\star}^{s _{t+1}}\right)\right]\] \[=\mathbb{E}\left[\sum_{t=t_{i}}^{\infty}\mathbf{1}[E_{t}=1]\left( V_{\star}^{s_{t}}-x_{t}^{s_{t}^{\top}}G^{s_{t}}y_{t}^{s_{t}}-\mathbf{1}[E_{t+1}=1]V_{ \star}^{s_{t+1}}\right)\right]\] \[=\mathbb{E}\left[V_{\star}^{s_{t_{i}}}\right]-\mathbb{E}\left[ \sum_{t=t_{i}}^{\infty}\mathbf{1}[E_{t}=1]x_{t}^{s_{t}^{\top}}G^{s_{t}}y_{t}^ {s_{t}}\right]\] \[=\mathbb{E}_{s\sim\rho}\left[V_{\star}^{s}\right]-\mathbb{E}\left[ \sum_{t=t_{i}}^{t_{i+1}-1}x_{t}^{s_{t}^{\top}}G^{s_{t}}y_{t}^{s_{t}}\right].\]

Combining this with Eq. (33), we get

\[\mathbb{E}\left[\sum_{t=1}^{T}x_{t}^{s_{t}^{\top}}G^{s_{t}}y_{t}^{ s_{t}}\right] \geq(\text{\#episodes in $T$ steps})\mathbb{E}_{s\sim\rho}[V_{\star}^{s}]-o(T)\] \[\geq(1-\gamma)\mathbb{E}_{s\sim\rho}[V_{\star}^{s}]T-o(T).\]

Hence the one-step average reward is at least \((1-\gamma)\mathbb{E}_{s\sim\rho}[V_{\star}^{s}]\). A symmetric analysis shows that it is also at most \((1-\gamma)\mathbb{E}_{s\sim\rho}[V_{\star}^{s}]\). This shows that both players have no regret compared to the game value. Notice that this is only a loose implication of the path convergence guarantee because of the loose second inequality in Eq. (33).

Remark on the notion of "last-iterate convergence" in general Markov gamesWhile Eq. (31) corresponds to best-iterate convergence for general Markov games, an even stronger notion one can pursue after is "last-iterate convergence." As argued above, it is impossible to require that the policies on all states to converge to equilibrium. To address this issue, we propose to study this problem under the episodic setting described above, in which the state is reset after every trajectory whose expected length is \(\frac{1}{1-\gamma}\). In this case, last-iterate convergence will be defined as the convergence of the following quantity to zero when \(i\rightarrow\infty\):

\[\mathbb{E}_{s\sim\rho}\left[\max_{x,y}\left(V^{s}_{x_{t_{i}},y}-V^{s}_{x,y_{t _{i}}}\right)\right]\]

where we recall that \(i\) is the episode index and \((x_{t_{i}},y_{t_{i}})\) are the policies used by the two players at the beginning of episode \(i\). While last-iterate convergence seems reasonable and possibly achievable, we are unaware of such results even for the degenerated case of single-player MDPs -- the standard regret bound corresponds to best-iterate convergence, while the techniques we are aware of to prove last-iterate convergence in MDPs require additional assumptions on the dynamics.