# Robust Matrix Sensing in the Semi-Random Model

Xing Gao

University of Illinois at Chicago

xgao53@uic.edu

&Yu Cheng

Brown University

yu_cheng@brown.edu

###### Abstract

Low-rank matrix recovery is a fundamental problem in machine learning with numerous applications. In practice, the problem can be solved by convex optimization namely nuclear norm minimization, or by non-convex optimization as it is well-known that for low-rank matrix problems like matrix sensing and matrix completion, all local optima of the natural non-convex objectives are also globally optimal under certain ideal assumptions.

In this paper, we relax the assumptions and study new approaches for matrix sensing in a semi-random model where an adversary can add any number of arbitrary sensing matrices. More precisely, the problem is to recover a low-rank matrix \(X^{*}\) from linear measurements \(b_{i}=\langle A_{i},X^{*}\rangle\), where an unknown subset of the sensing matrices satisfies the Restricted Isometry Property (RIP) and the rest of the \(A_{i}\)'s are chosen adversarially.

It is known that in the semi-random model, existing non-convex objectives can have bad local optima. To fix this, we present a descent-style algorithm that provably recovers the ground-truth matrix \(X^{*}\). For the closely-related problem of semi-random matrix completion, prior work [1] showed that all bad local optima can be eliminated by reweighting the input data. However, the analogous approach for matrix sensing requires reweighting a set of matrices to satisfy RIP, which is a condition that is NP-hard to check. Instead, we build on the framework proposed in [13] for semi-random sparse linear regression, where the algorithm in each iteration reweights the input based on the current solution, and then takes a weighted gradient step that is guaranteed to work well locally. Our analysis crucially exploits the connection between sparsity in vector problems and low-rankness in matrix problems, which may have other applications in obtaining robust algorithms for sparse and low-rank problems.

## 1 Introduction

Low-rank matrix recovery is a popular inverse problem with many applications in machine learning such as collaborative filtering, image compression, and robust principal component analysis (PCA) [14, 15]. In this paper, we study one of the most basic low-rank matrix recovery problems namely matrix sensing [15, 16]. In the matrix sensing problem, we want to reconstruct a low-rank ground-truth matrix \(X^{*}\in\mathbb{R}^{d_{1}\times d_{2}}\) from a collection of sensing matrices \(\{A_{i}\}_{i=1}^{n}\) and the corresponding linear measurements \(b_{i}=\langle A_{i},X\rangle\).

For notational convenience, we define a sensing operator \(\mathcal{A}[\cdot]:\mathbb{R}^{d_{1}\times d_{2}}\to\mathbb{R}^{n}\) such that \(\mathcal{A}[X]=b\) with \(b_{i}=\langle A_{i},X\rangle\) for \(i=1\ldots n\). The goal is to solve the following rank-constrained optimization problem:

\[\min_{X\in\mathbb{R}^{d_{1}\times d_{2}}}\left\|\mathcal{A}[X]-b\right\|_{2}^ {2}\text{ subject to }\operatorname{rank}(X)\leq r\;.\]

As optimizing over low-rank matrices are often computationally hard, one common approach is to replace the non-convex low-rank constraint with its convex-relaxation, which results in the followingnuclear norm minimization problem [10]:

\[\min_{X\in\mathbb{R}^{d_{1}\times d_{2}}}\left\|X\right\|_{*}\text{ subject to }\mathcal{A}[X]=b\.\] (1)

Another widely-used approach in practice is to consider the unconstrained non-convex factorized parametrization [10, 11, 12, 13]:

\[\min_{U\in\mathbb{R}^{d_{1}\times r},V\in\mathbb{R}^{d_{2}\times r}}\left\| \mathcal{A}[UV^{\top}]-b\right\|_{2}^{2}\.\] (2)

and solve it with some form of gradient descent or alternating minimization.

Existing convex and non-convex approaches all rely on certain assumptions. A standard assumption in the literature is that the sensing matrices satisfy the Restricted Isometry Property (**RIP**), which means that the sensing matrices approximately preserve the norm of a low-rank matrix. (Formally, \(\frac{1}{L}\cdot\left\|X\right\|_{F}^{2}\leq\frac{1}{n}\sum_{i=1}^{n}\left\langle A _{i},X\right\rangle^{2}\leq L\cdot\left\|X\right\|_{F}^{2}\) given \(\operatorname{rank}(X)\leq r\) for some parameters \(r\) and \(L\).)

In this paper, we relax the RIP condition on the sensing matrices and study a robust version of the problem, which is often referred to as the **semi-random** model. More specifically, an adversary "corrupts" the input data by providing any number of additional sensing matrices \(A_{i}\) that are adversarially chosen, but the corresponding measurements \(b_{i}=\left\langle A_{i},X^{*}\right\rangle\) remain consistent with the ground truth matrix \(X^{*}\). Consequently, only a subset of the sensing matrices satisfy the RIP condition and the rest of them are arbitrary. This is an intermediate scenario between the average case and the worst case, which arises more frequently in practice.

To the best of our knowledge, we are the first to study the matrix sensing problem in this semi-random model. Formally, we consider the following adversary: suppose that originally there was a collection of RIP sensing matrices \(\{A_{i}\}_{i=1}^{m}\) ("good" matrices), then the adversary augmented some arbitrary \(\{A_{i}\}_{i=m+1}^{n}\) ("bad" matrices) and then shuffled all the sensing matrices. The algorithm is then given the measurements based on the "good" and "bad" matrices together. The combined sensing matrices are no longer guaranteed to satisfy the RIP condition, but there exists a subset (indicated by an indicator vector \(w^{*}\)) that does, i.e., \(\frac{1}{L}\cdot\left\|X\right\|_{F}^{2}\leq\sum_{i=1}^{n}w_{i}^{*}\left\langle A _{i},X\right\rangle^{2}\leq L\cdot\left\|X\right\|_{F}^{2}\), where \(w_{i}^{*}=\frac{1}{m}\) on the original "good" matrices and \(w_{i}^{*}=0\) on the "bad" matrices added by the adversary. In general, the subset may be replaced by a convex combination and the indicator vector by a simplex. Inspired by the adversary for semi-random vector regression in [11], we refer to this condition as **wRIP** (weighted RIP) and formally define it in Definition 2.2.

Since the wRIP condition is a more general assumption than RIP, existing solutions that rely on RIP might fail under the semi-random model with wRIP condition. As stated in [11], this type of adversary does not break the problem from an "information-theoretic perspective", but affects the problem computationally. In particular, existing non-convex approaches for matrix sensing (e.g., 2) may get stuck at bad local minima as the RIP condition is necessary for proving landscape results regarding the non-convex objective (see, e.g., the counter-examples provided in [12]. The convex relaxation approach (1) does continue to work in the semi-random model, because the augmented linear measurements are consistent with the ground-truth matrix \(X^{*}\) which simply provides additional optimization constraints. However, convex approaches are often less desirable in practice and can become computationally prohibitive when \(d_{1},d_{2}>100\) as pointed out in [10].

### Our Contributions

The limitations of existing algorithms motivate us to pose and study the problem of semi-random matrix sensing in this paper. We summarize our main contributions below:

* **Pose and study matrix sensing in the semi-random model.** We introduce the more general wRIP condition on matrix sensing as a relaxation of the typical RIP assumption, and provide a solution that is more robust to input contamination. Our work will serve as a starting point for the design of more efficient robust algorithms for matrix sensing, as well as other low-rank matrix problems, in the semi-random model.
* **Design an efficient robust algorithm for semi-random matrix sensing.** Our algorithm is guaranteed to converge to a global optimum which improves on the existing non-convex solution [12] that can get stuck in bad local optima in the semi-random model, while achieving a comparable running time as existing convex solution [10], informally stated in Theorem 1.1 below. A formal statement can be found in Theorem 3.1.

* **Adapt a reweighting scheme for semi-random matrix sensing.** In contrast to the non-convex approach that failed and the convex approach that avoided the challenge posed by the adversary altogether, we study a new approach that directly targets the semi-random adversary instead. We develop an algorithm using an iterative reweighting approach inspired by [12]: in each iteration, the algorithm reweights the sensing matrices to combat the effect of the adversary and then takes a weighted gradient step that works well based on the current solution.
* **Exploit the connection between sparsity and low-rankness.** Observing a duality between sparse vectors and low-rank matrices, we draw a parallel between linear regression and matrix sensing problems. By exploring the structural similarities and differences between vector and matrix problems, we are able to extend and generalize the work of [12] on semi-random sparse vector recovery to the higher dimensional problem of semi-random matrix sensing. We emphasize that even though the generalization from vector to matrix problems is natural, the analysis behind the intuition is often nontrivial and involves different mathematical tools.

We state a simplified version of our main algorithmic result assuming Gaussian design. The more general result is stated as Theorem 3.1 in Section 3.

**Theorem 1.1** (Semi-Random Matrix Sensing).: _Suppose the ground-truth matrix \(X^{*}\in\mathbb{R}^{d_{1}\times d_{2}}\) satisfies \(\operatorname{rank}(X^{*})\leq r\) and \(\left\|X^{*}\right\|_{F}\leq\operatorname{poly}(d)\). Let \(A_{1},\dots,A_{n}\) be the sensing matrices and let \(b_{i}=\left\langle A_{i},X^{*}\right\rangle\) be the corresponding measurements. Suppose there exists a (hidden) set of \(\Omega(dr)\) sensing matrices with i.i.d. standard Gaussian entries, and the remaining sensing matrices are chosen adversarially, where \(d=\max(d_{1},d_{2})\)._

_There exists an algorithm that can output \(X\in\mathbb{R}^{d_{1}\times d_{2}}\) such that \(\left\|X-X^{*}\right\|_{F}\leq\epsilon\) with high probability in time \(\widetilde{O}(nd^{\omega+1}r\log(1/\epsilon))\)1 where \(\omega<2.373\) is the matrix multiplication exponent._

Footnote 1: Throughout the paper, we write \(\widetilde{O}(f(n))\) for \(O(f(n)\operatorname{polylog}f(n))\) and similarly for \(\widetilde{\Omega}(\cdot)\).

### Overview of Our Techniques

Since there exists a subset (or a convex combination in general) of the sensing matrices that satisfy the RIP condition, a natural strategy is to reverse the effect from the adversary by reweighting the sensing matrices so that they satisfy the RIP condition. However, it is NP-hard to verify RIP condition on all low-rank inputs, so it is unclear how to preprocess and "fix" the input in the beginning and then apply existing solutions to matrix sensing. Instead, we make a trade-off between the frequency of reweighting and the requirement on the weights by adopting an iterative reweighting approach: in each iteration, we only aim to find a set of weights so that the weighted matrices satisfy some desirable properties (not necessarily RIP) with respect to the current estimate \(X\) (as opposed to all low-rank matrices).

Inspired by the workflow in [12], our **semi-random matrix sensing algorithm** (Algorithm 1) repeatedly calls a halving algorithm to reduce the error of our estimate arbitrarily small. The **halving algorithm** (Algorithm 2) contracts the upper bound on \(\left\|X-X^{*}\right\|_{F}\), which is the error between our current estimate \(X\) and the ground truth \(X^{*}\), each time it is run. Inside this algorithm is a gradient descent style loop, where in each iteration we try to minimize a weighted objective function, which is essentially the weighted \(\ell_{2}\)-norm of \(\mathcal{A}[X_{t}]-b\) (the distance to \(X^{*}\) "measured" by the sensing matrices), where the weights are provided by an oracle implemented in Algorithm 3. The algorithm proceeds by taking a step opposite to the gradient direction, and the step is then projected onto a nuclear-norm-bounded ball which is necessary for the weight oracle to continue working in the next step. As we mentioned before, the weights from the oracle need to satisfy some nice properties with respect to the current iteration estimate \(X_{t}\). Ideally, the property should: firstly, ensure the gradient step makes enough progress towards \(X^{*}\); secondly, can be derived from the wRIP condition so that we know such a requirement is feasible; and lastly, be easily verifiable as opposed to the NP-hard RIP condition.

With the first requirement in mind, we define the **weight oracle** as in Definition 2.5. The oracle output should satisfy two properties, namely the progress and decomposition guarantees, and together they ensure the gradient step makes good enough progress toward \(X^{*}\). Intuitively speaking, the progress guarantee ensures the gradient step is large in the direction parallel to the "actual" deviation \(X-X^{*}\) (as opposed to only reducing the "measured" deviation \(\mathcal{A}[X]-b\)) and thus will make significant progress, while the decomposition guarantee ensures the gradient step has small contribution and effect in other directions thus will not cancel the progress after the projection. While the progress guarantee is quantified as an inner product, we introduce a concept called "norm-decomposition" (Definition 2.4) to capture the decomposition guarantee, and we will provide more details later. For the second requirement, we can loosely relate the two oracle guarantees to the wRIP condition: the (large) progress guarantee makes use of the lower bound in wRIP condition \(\sum_{i=1}^{n}w_{i}^{*}\langle A_{i},\frac{X}{\left\lVert X\right\rVert_{F}} \rangle^{2}\geq\frac{1}{L}\), and the (small) decomposition guarantee makes use of the upper bound \(\sum_{i=1}^{n}w_{i}^{*}\langle A_{i},\frac{X}{\left\lVert X\right\rVert_{F}} \rangle^{2}\leq L\). We introduce a condition called **dRIP** (decomposable wRIP defined in Definition 2.3) to formally capture this relation, and we will show that it follows from the wRIP condition thus we can achieve such an oracle. Lastly, we will show that the oracle properties can be easily verified, meeting our third requirement.

A formal statement and a road map that leads to our main result can be found in Section 3.

### Related Work

**Matrix sensing (RIP):** There are two main types of existing solutions. The convex-relaxation formulation 1 of the problem can be posed as a semidefinite program via the standard form primal-dual pair [14], where the primal problem has a \((d_{1}+d_{2})^{2}\) semidefinite constraint and \(n\) linear constraints. State of the art SDP solver [13] requires running time of \(\widetilde{O}(nd^{2.5})\) where \(d=\max(d_{1},d_{2})\). The other approach uses non-convex formulation 2 to reduce the size of the decision variable from \(d^{2}\) to \(dr\), improving computational efficiency. It is shown in [1] that there are no spurious local minima given RIP sensing matrices and incoherent linear measurements in the non-convex approach, however, it is no longer applicable in the semi-random model.

**Semi-random model:** First introduced by [1], the semi-random model has been studied for various graph problems [12, 13, 14, 15]. Previously the work of [1] applied the semi-random model to the matrix completion problem, and recently [13] studied sparse vector recovery in this model.

**Semi-random matrix completion:** Low-rank matrix problems such as matrix completion and matrix sensing have similar optimization landscapes [1], thus development in one often lends insight to another. Prior work [1] on the closely-related problem of matrix completion under the semi-random model showed that all bad local optima can be eliminated by reweighting the input data via a preprocessing step. It exploits the connection between the observation data matrix and the Laplacian matrix of a complete bipartite graph, and gives a reweighting algorithm to preprocess the data in a black-box manner. However, the analogous approach for matrix sensing requires reweighting a set of matrices to satisfy RIP, which is a condition that is NP-hard to check, thus is not practical in the matrix sensing problem.

**Semi-random vector regression:** In order to overcome the barrier of the reweighting or preprocessing approach mentioned earlier, we take inspiration from the work of [13] on sparse vector recovery under the semi-random model. One of their main contributions is the "short-flat decomposition", which is a property that can be efficiently verified for a given vector (locally), instead of verifying the RIP condition for all sparse vectors (globally). They provide a projected gradient descent style algorithm, where the rows of the sensing matrix are reweighted differently in each iteration to ensure a "short-flat decomposition" exists for the gradient. We draw a parallel between the problem of sparse vector regression and low-rank matrix sensing, and extend their work on linear regression of sparse vectors to the more generalized problem of sensing low-rank matrices.

## 2 Preliminaries

### Notations

Throughout the paper, we denote the ground-truth low-rank matrix as \(X^{*}\). We assume \(X^{*}\in\mathbb{R}^{d_{1}\times d_{2}}\), \(\operatorname{rank}(X^{*})=r\), and \(d_{1},d_{2}\) have the same order of magnitude. Let \(d=\max(d_{1},d_{2})\).

We write \([n]\) for the set of integers \(\{1,...,n\}\). We use \(\Delta^{n}\) for the nonnegative probability simplex in dimension \(n\), and \(\mathbb{R}_{\geq 0}^{n}\) for the set of vectors with nonnegative coordinates in \(\mathbb{R}^{n}\). For a vector \(x\), we denote its \(\ell_{1}\), \(\ell_{2}\), and \(\ell_{\infty}\)-norms as \(\left\lVert x\right\rVert_{1}\), \(\left\lVert x\right\rVert_{2}\) and \(\left\lVert x\right\rVert_{\infty}\) respectively, and write the \(i^{\mathrm{th}}\) coordinate in \(x\) as \(x_{i}\). For a matrix \(A\), we use \(\left\lVert A\right\rVert_{*}\), \(\left\lVert A\right\rVert_{2}\), and \(\left\lVert A\right\rVert_{F}\) for the nuclear, spectral (operator), and Frobenius norms of \(A\) respectively. For a matrix \(A\), we use \(A_{(k)}=\text{argmin}_{\text{rank}(A^{\prime})\leq k}\left\|A-A^{\prime}\right\|_ {F}\) to denote the best rank-\(k\) approximation of \(A\); or equivalently, given the SVD of \(A=\sum_{i=1}^{r}\sigma_{i}u_{i}v_{i}^{\top}\), we have \(A_{(k)}=\sum_{i=1}^{k}\sigma_{i}u_{i}v_{i}^{\top}\) where \(\sigma_{1},...,\sigma_{k}\) are the top \(k\) singular values of \(A\).

We write \(\mathrm{tr}(A)\) for the trace of a square matrix \(A\). For matrices \(A,B\in\mathbb{R}^{d_{1}\times d_{2}}\), we write \(\left\langle A,B\right\rangle\) for their entrywise inner product \(\left\langle A,B\right\rangle=\left\langle A,B\right\rangle=\mathrm{tr}(A^{ \top}B)=\sum_{j,k}A_{jk}B_{jk}\). A symmetric matrix \(A\in\mathbb{R}^{d\times d}\) is positive semidefinite (PSD) if and only if \(A=U^{\top}U\) for some matrix \(U\), and we write \(A\preccurlyeq B\) if \(A\) and \(B\) have the same dimension and \(B-A\) is positive semidefinite. We write \(\exp{(A)}\) as the matrix exponential of \(A\); if \(A\) is diagonalizable as \(A=UDU^{-1}\) then \(\exp(A)=U\exp(D)U^{-1}\).

### Definitions

We formally define the matrix sensing operator and observation vector below.

**Definition 2.1** (Matrix Sensing Operator).: Given a collection of sensing matrices \(\mathcal{A}=\{A_{i}\}_{i=1}^{n}\subset\mathbb{R}^{d_{1}\times d_{2}}\), we define the sensing operator \(\mathcal{A}[\cdot]:\mathbb{R}^{d_{1}\times d_{2}}\rightarrow\mathbb{R}^{n}\) such that \(\mathcal{A}[X]=b\) where \(b_{i}=\left\langle A_{i},X\right\rangle\) for \(X\in\mathbb{R}^{d_{1}\times d_{2}}\).

In other words, we have \(b:=\sum_{i=1}^{n}\left\langle A_{i},X\right\rangle e_{i}\) where \(e_{i}\) is the \(i^{\text{th}}\) standard basis vector in \(\mathbb{R}^{n}\). Throughout the paper, we use either \(\mathcal{A}\) or \(\{A_{i}\}_{i=1}^{n}\) to represent the sensing matrices.

To consistently recover a rank-\(r\) matrix in general, the number of measurements \(n\) should be at least \((d_{1}+d_{2}-r)r\)[15], hence we assume \(n=\widetilde{\Omega}(dr)\) where \(\widetilde{\Omega}\) suppresses log factors. In most matrix sensing literature, it is standard to impose the Restricted Isometry Property (RIP) condition on the sensing matrices. The RIP condition states that \(\mathcal{A}[\cdot]\) is approximately an isometry on low-rank matrices, which means the \(\ell_{2}\)-norm of the observation vector is close to the Frobenius norm of \(X^{*}\).

In this paper, we consider a semi-random model and relax the RIP condition as follows: we require that there exist weights \(\{w_{i}^{*}\}_{i=1}^{n}\) (or \(w^{*}\in\Delta^{n}\)) so that the weighted sensing matrices \(\{\sqrt{w_{i}^{*}}A_{i}\}_{i=1}^{n}\) satisfy the RIP condition. We call this relaxed assumption the wRIP (weighted RIP) condition.

We formally define RIP and wRIP conditions below.

**Definition 2.2** (RIP and wRIP Conditions).: We say a collection of sensing matrices \(\mathcal{A}=\{A_{i}\}_{i=1}^{n}\subset\mathbb{R}^{d_{1}\times d_{2}}\) satisfies the **RIP** (Restricted Isometry Property) condition with parameters \(r\), \(L\), and \(\rho\) if the following conditions hold for all \(X\in\mathbb{R}^{d_{1}\times d_{2}}\) with \(\mathrm{rank}(X)\leq r\):

1. Boundedness: \(\left\|A_{i}\right\|_{2}\leq\rho\) ;
2. Isometry: \(\frac{1}{L}\cdot\left\|X\right\|_{F}^{2}\leq\frac{1}{n}\sum_{i=1}^{n}\left\langle A _{i},X\right\rangle^{2}\leq L\cdot\left\|X\right\|_{F}^{2}\).

Further, we say \(\mathcal{A}=\{A_{i}\}_{i=1}^{n}\) satisfies the **wRIP** (weighted RIP) condition with parameters \(r\), \(L\), \(\rho\), if \(\exists w^{*}\in\Delta^{n}\) such that the following conditions hold for all \(X\in\mathbb{R}^{d_{1}\times d_{2}}\) with \(\mathrm{rank}(X)\leq r\):

1. Boundedness: \(\left\|A_{i}\right\|_{2}\leq\rho\) ;
2. Isometry: \(\frac{1}{L}\cdot\left\|X\right\|_{F}^{2}\leq\sum_{i=1}^{n}w_{i}^{*}\langle A _{i},X\rangle^{2}\leq L\cdot\left\|X\right\|_{F}^{2}\).

Notice that wRIP is a relaxation of the RIP condition, because we can choose \(w_{i}^{*}=1/n\) for all \(i\) in the standard RIP setting. More importantly, wRIP is strictly weaker. For example, wRIP allows a (possibly majority) fraction of the sensing matrices to be chosen adversarially. We want to emphasize that the algorithm does not know \(w^{*}\) -- one of the main challenges of semi-random matrix sensing is that finding \(w^{*}\) seems computationally hard, because it is NP-Hard to check the RIP condition.

For presenting our algorithm and analysis, we introduce a variant of the wRIP condition called dRIP (decomposable-wRIP).

**Definition 2.3** (dRIP Condition).: We say a collection of sensing matrices \(\mathcal{A}=\{A_{i}\}_{i=1}^{n}\subset\mathbb{R}^{d_{1}\times d_{2}}\) satisfies the **dRIP** (decomposable wRIP) condition if \(\exists w^{*}\in\Delta^{n}\) and constants \(L,K,r,\rho\geq 1\), such that for all \(V\in\mathbb{R}^{d_{1}\times d_{2}}\) satisfying \(\left\|V\right\|_{F}\in[\frac{1}{4},1],\left\|V\right\|_{*}\leq 2\sqrt{2r}\):

1. Boundedness: \(\left\|A_{i}\right\|_{2}\leq\rho\) ;
2. Isometry: \(\frac{1}{L}\leq\sum_{i=1}^{n}w_{i}^{*}\langle A_{i},V\rangle^{2}\leq L\) ;3. Decomposability: \(\exists(L,\frac{1}{K\sqrt{r}})\)-norm-decomposition of \(G^{*}=\sum_{i=1}^{n}w_{i}^{*}(A_{i},V)A_{i}=\sum_{i=1}^{n}w_{i}^{*}u_{i}A_{i}\).

**Definition 2.4** (Norm Decomposition).: We say a matrix \(G\) has a \((C_{F},C_{2})\)-norm-decomposition if \(\exists S\) and \(E\ s.t.\ G=S+E\), and \(\left\|S\right\|_{F}\leq C_{F}\), \(\left\|E\right\|_{2}\leq C_{2}\).

The main difference with wRIP is that dRIP requires the additional "decomposition" property. Observe that \(G^{*}\) is the (weighted) gradient at the point \(V\). At a high level, we would like to decompose the gradient into two matrices, one with small Frobenius norm and the other one with small operator norm. Our matrix norm-decomposition is inspired by the "short-flat-decomposition" for vectors in [13].

In Section 4, we will explain the motivation behind the norm decomposition as well as how to efficiently verify such a decomposition exists. We will also show that the dRIP condition is closely related to wRIP (by choosing parameters within a constant factor of each other) in Appendix C.

A crucial component in our algorithm is a weight oracle that produces a nonnegative weight on each sensing matrix (the weights are in general different in each iteration), such that the weighted gradient step moves the current solution closer to \(X^{*}\). The oracle should output weights that satisfy certain properties which we term progress and decomposition guarantees. The purpose of these two guarantees is further explained in the proof of Lemma 4.2 in Appendix A.

**Definition 2.5** (Weight Oracle).: We say an algorithm \(\mathcal{O}\) is a \((C_{\text{prog}},C_{F})\)-oracle, if given as input \(n\) matrices \(\mathcal{A}=\{A_{i}\}_{i=1}^{n}\subset\mathbb{R}^{d_{1}\times d_{2}}\) and an vector \(u=\mathcal{A}[V]\in\mathbb{R}^{n}\) where \(V\in\mathbb{R}^{d_{1}\times d_{2}}\), \(\left\|V\right\|_{F}\in[\frac{1}{4},1]\), and \(\left\|V\right\|_{*}\leq 2\sqrt{2r}\), the algorithm \(\mathcal{O}(\mathcal{A},u,\delta)\) returns a weight vector \(w\in\mathbb{R}_{\geq 0}^{n}\) such that the following conditions hold with probability at least \(1-\delta\):

1. Progress guarantee: \(\sum_{i=1}^{n}w_{i}u_{i}^{2}\geq C_{\text{prog}}\) ;
2. Decomposition guarantee: \(\exists(C_{F},\frac{C_{\text{reg}}}{6\sqrt{r}})\) norm-decomposition of \(G=\sum_{i=1}^{n}w_{i}u_{i}A_{i}\).

Note that the progress guarantee is equivalent to \(\langle G,V\rangle\geq C_{\text{prog}}\).

Finally we define numerical rank which we use in our analysis. Numerical rank serves as a lower bound for the rank of a matrix based on its nuclear norm and Frobenius norm. That is, we always have \(\operatorname{Rank}_{\text{n}}(A)\leq\operatorname{rank}(A)\).

**Definition 2.6** (Numerical Rank).: The numerical rank of \(A\) is \(\operatorname{Rank}_{\text{n}}(A)=\frac{\left\|A\right\|_{*}^{2}}{\left\|A \right\|_{F}^{2}}\).

## 3 Semi-Random Matrix Sensing

In this section, we present our main algorithm (Algorithm 1) for semi-random matrix sensing. Algorithm 1 with high probability recovers the ground-truth matrix \(X^{*}\) to arbitrary accuracy.

```
1:Input: \(R_{0}\geq\left\|X^{*}\right\|_{F},b=\mathcal{A}[X^{*}],\epsilon>0,\delta\in(0,1)\) ;
2:Output: \(X_{\text{out}}\) s.t. \(\left\|X_{\text{out}}-X^{*}\right\|_{F}\leq\epsilon\).
3:\(X_{0}\gets 0,T\leftarrow\log\frac{R_{0}}{\epsilon},\delta^{\prime} \leftarrow\frac{\delta}{T},R\gets R_{0}\) ;
4:for\(0\leq t\leq T\)do
5:\(X_{t+1}\leftarrow\text{HalveError}(X_{t},R,\mathcal{O},\delta^{\prime}, \mathcal{A},b)\), \(R\leftarrow\frac{R}{2}\) ;
6:endfor
7:Return \(X_{\text{out}}\gets X_{T}\) ; ```

**Algorithm 1**SemiRandomMatrixSensing(\(R_{0},\epsilon,\delta,\mathcal{A},b\))

The performance guarantee and runtime of Algorithm 1 are formally stated in the following theorem.

**Theorem 3.1** (Matrix Sensing under wRIP).: _Suppose the ground-truth matrix \(X^{*}\in\mathbb{R}^{d_{1}\times d_{2}}\) satisfies \(\operatorname{rank}(X^{*})\leq r\) and \(\left\|X^{*}\right\|_{F}\leq R_{0}\). Suppose the sensing matrices \(\mathcal{A}=(A_{i}\in\mathbb{R}^{d_{1}\times d_{2}})_{i=1}^{n}\) satisfy \((r,L,\rho)\)-wRIP (as in Definition 2.2). Let \(b=\mathcal{A}[X^{*}]\in\mathbb{R}^{n}\) be the corresponding measurements.__For any \(\epsilon,\delta>0\), Algorithm 1 can output \(X\in\mathbb{R}^{d_{1}\times d_{2}}\) such that \(\left\|X-X^{*}\right\|_{F}\leq\epsilon\) with probability at least \(1-\delta\). Algorithm 1 runs in time \(O(nd^{\omega}\operatorname{polylog}{(d)}\log{(\frac{L}{\delta}\log{\frac{R_{0}} {\epsilon}})}r\rho^{2}L^{4}\log{\frac{R_{0}}{\epsilon}})\) where \(d=\max(d_{1},d_{2})\) and \(\omega<2.373\) is the matrix multiplication exponent._

Theorem 1.1 is a direct corollary of Theorem 3.1 under Gaussian design.

Proof of Theorem 1.1.: When there are \(\Omega(dr)\) sensing matrices with i.i.d. standard Gaussian entries, the input sensing matrices satisfy \((r,L,\rho)\)-wRIP for \(L=O(1)\) and \(\rho=O(d^{1/2})\) with probability at least \(1-\frac{1}{\operatorname{poly}(d)}\). This follows from a standard proof for RIP and the fact that we can ignore any sensing matrices with \(\left\|A_{i}\right\|_{2}\gg d^{1/2}\). We assume that the wRIP condition is satisfied.

By Theorem 3.1, when \(L=O(1)\), \(\rho=O(d^{1/2})\), \(R_{0}=\operatorname{poly}(d)\) and \(\delta=\frac{1}{\operatorname{poly}(d)}\), Algorithm 1 can output a solution \(X\) such that \(\left\|X-X^{*}\right\|_{F}\leq\epsilon\) with high probability. The runtime of Algorithm 1 can be simplified to \(\widetilde{O}(nd^{\omega+1}r\log(1/\epsilon))\). 

We first provide a road map for our analysis for proving Theorem 3.1:

* Our main algorithm runs a "halving" subroutine for \(\log{\frac{R_{0}}{\epsilon}}\) iterations to reduce the error to \(\epsilon\). Each call to this subroutine reduces the upper bound on the distance between the current solution and the ground truth \(X^{*}\) by half. This halving subroutine runs in time \(O(nd^{\omega}\operatorname{polylog}{(d)}\log{(\frac{L}{\delta}\cdot\log{\frac {R_{0}}{\epsilon}})}r\rho^{2}L^{4})\) according to Lemmas 4.2 and 4.3.
* In Section 4, we present the halving algorithm (Algorithm 2). It depends on a \((\Omega(1),O(1))\)-oracle, and Lemma 4.1 shows that the oracle guarantees can be easily verified. The algorithm's correctness and running time are analyzed in Lemma 4.2 and Lemma 4.3.
* In Section 5 we present the weight oracle required by the halving algorithm. We first show in Lemma 5.1 that the wRIP condition implies that the sensing matrices satisfy the dRIP condition tailored to the design of the oracle. Then we present an implementation of the oracle in Algorithm 3 based on the dRIP condition, and analyze its correctness and running time in Lemma 5.3 and Lemma 5.4.

## 4 Algorithm for Halving the Error

In this section, we present Algorithm 2 (HalveError). Algorithm 2 takes an estimate \(X_{\text{in}}\) with \(\left\|X_{\text{in}}-X^{*}\right\|_{F}\leq R\) and outputs \(X_{\text{out}}\) such that \(\left\|X_{\text{out}}-X^{*}\right\|_{F}\leq\frac{R}{2}\). This is the matrix version of the HalfRadiabsSparse [12] algorithm for vectors.

```
1:Input: Rank-\(r\) matrix \(X_{\text{in}}\in\mathbb{R}^{d_{1}\times d_{2}}\), \(\left\|X_{\text{in}}-X^{*}\right\|_{F}\leq R\), \(\mathcal{O}\) is a \((1,12L^{2})\)-oracle for \(\mathcal{A}\) with failure probability \(\delta\in(0,1)\), linear measurements \(b=\mathcal{A}[X^{*}]\).
2:Output: \(X_{\text{out}}\in\mathbb{R}^{d_{1}\times d_{2}}\), \(s.t.\)\(\left\|X_{\text{out}}-X^{*}\right\|_{F}\leq\frac{R}{2}\)\(w.p.\)\(\geq 1-\delta\) and \(\operatorname{rank}(X_{\text{out}})\leq r\).
3:\(X_{0}\gets X_{\text{in}}\), \(\mathcal{X}=\{X\in\mathbb{R}^{d_{1}\times d_{2}}\mid\left\|X-X_{\text{in}} \right\|_{*}\leq\sqrt{2r}R\}\), \(\eta=\frac{1}{288L^{4}}\), \(T=\frac{6}{\eta}\).
4:for\(0\leq t\leq T\)do
5:\(u_{t}\leftarrow\frac{1}{R}(\mathcal{A}[X_{t}]-b)\) ; /* \(u_{t}=\mathcal{A}[\frac{X_{t}-X^{*}}{R}]\) where \((u_{t})_{i}=\frac{1}{R}\langle A_{i},X_{t}-X^{*}\rangle\) */
6:\(w_{t}\leftarrow\frac{1}{R}(\mathcal{A},u_{t},\frac{\delta}{T})\) ;
7:\(G_{t}\leftarrow\sum_{i=1}^{n}{(w_{t})_{i}(u_{t})_{i}A_{i}}\) ;
8:if\(\mathcal{O}\) output satisfies the progress and decomposition guarantees on \(u_{t}\)then
9:\(X_{t+1}\leftarrow\text{argmin}_{X\in\mathcal{X}}\|X-(X_{t}-\eta RG_{t})\|_{F}^{2}\) ;
10:else
11: Return \(X_{\text{out}}\leftarrow(X_{t})_{(r)}\) ; /* Rank-r approximation of \(X_{t}\) */
12:endif
13:endfor
14:Return \(X_{\text{out}}\leftarrow(X_{T})_{(r)}\) ; ```

**Algorithm 2**HalveError(\(X_{\text{in}},R,\mathcal{O},\delta,\mathcal{A},b\))A crucial requirement of the algorithm is a \((\Omega(1),O(1))\)-oracle for \(\mathcal{A}\). In each iteration, the oracle takes a vector \(u_{t}=\frac{A\left|X_{t}\right|-b}{R}\), which is the (normalized) "measured deviation" between current estimate \(X_{t}\) and \(X^{*}\), and computes a weight vector \(w_{t}\). The algorithm then tries to minimize the weighted objective function by gradient descent:

Objective:

\[f_{t}(X) =\frac{1}{2}\sum_{i=1}^{n}{(w_{t})_{i}\langle A_{i},\frac{X-X^{*} }{R}\rangle^{2}}\text{, i.e. }f_{t}(X_{t})=\frac{1}{2}\sum_{i=1}^{n}{(w_{t})_{i}(u_{t})_{i}^{2}} \text{,}\] Gradient:

\[\nabla_{X}f_{t}(X) =\sum_{i=1}^{n}{(w_{t})_{i}\langle A_{i},\frac{X-X^{*}}{R}\rangle A _{i}}\text{, i.e. }G_{t}=\nabla_{X}f_{t}(X)|_{X_{t}}=\sum_{i=1}^{n}{(w_{t})_{i}(u_{t})_{i}A_{i}} \text{.}\]

Ideally in the next iteration, we would like to make a step from \(X_{t}\) in the opposite direction of the gradient \(G_{t}\) with the goal of minimizing the deviation in the next iteration. However, we cannot take a step exactly in the direction of \(G_{t}\), and our movement is constrained within a ball of (nuclear norm) radius \(\sqrt{2r}R\) centered at \(X_{\text{in}}\), namely the region \(\mathcal{X}=\left\{X\mid\left\|X-X_{\text{in}}\right\|_{*}\leq\sqrt{2r}R\right\}\). Nuclear norm is used as a proxy to control the rank and Frobenius norm of \(X_{t}\) simultaneously throughout the algorithm: firstly, since \(\left\|X_{\text{in}}-X^{*}\right\|_{F}\leq R\), it makes sense that in each iteration \(\left\|X_{t}-X_{\text{in}}\right\|_{F}\leq R\) as well; secondly, while trying to minimize the difference between \(X_{t}\) and \(X^{*}\), we also want to ensure the rank of \(X_{t}\) is relatively small, i.e. \(\operatorname{rank}(X_{t})\leq O(r)\). To tie things together, we use the following relationship between rank and numerical rank:

\[\operatorname{rank}(X_{t}-X_{\text{in}})\geq\operatorname{Rank}_{\text{n}}(X _{t}-X_{\text{in}})=\frac{\left\|X_{t}-X_{\text{in}}\right\|_{*}^{2}}{\left\| X_{t}-X_{\text{in}}\right\|_{F}^{2}}\text{.}\]

Assuming \(\operatorname{rank}(X_{t})\geq\operatorname{rank}(X_{\text{in}})\) and \(\left\|X_{t}-X_{\text{in}}\right\|_{F}\leq R\) throughout, then \(\operatorname{rank}(X_{t})\geq\frac{\left\|X_{t}-X_{\text{in}}\right\|_{*}^{2 }}{2R^{2}}\). Roughly speaking, in order for \(\operatorname{rank}(X_{t})\leq O(r)\), it is necessary that \(\left\|X_{t}-X_{\text{in}}\right\|_{*}\leq O(\sqrt{r}R)\), i.e. \(X_{t}\) is inside some nuclear norm ball \(\mathcal{X}\) of radius \(O(\sqrt{r}R)\) centered at \(X_{\text{in}}\). We set the radius of \(\mathcal{X}\) to be \(\sqrt{2r}R\) so that \(X^{*}\in\mathcal{X}\), since \(\left\|X_{\text{in}}-X^{*}\right\|_{F}\leq R\), \(\operatorname{rank}(X_{\text{in}}-X^{*})\leq 2r\) therefore \(\left\|X^{*}-X_{\text{in}}\right\|_{*}\leq\sqrt{2r}R\). Thus we confine our movement within this nuclear norm ball of radius \(\sqrt{2r}R\) centered at \(X_{\text{in}}\) throughout the algorithm, and take the rank-\(r\) approximation of the last \(X_{t}\) to ensure \(\operatorname{rank}(X_{\text{out}})\leq r\) upon the termination of the algorithm.

To analyze the algorithm, first we show how to check whether the weight oracle output satisfies the progress and decomposition guarantees. The progress condition \(\sum_{i=1}^{n}{w_{i}u_{i}^{2}}\geq 1\) is trivial to verify, and we check whether \(G\) is \((C_{F},C_{2})\)-decomposable using Lemma 4.1, with details and proof deferred to Appendix A.

**Lemma 4.1** (Verify Norm Decomposition).: _Given a matrix \(G=U\Sigma V^{\top}=\sum_{i=1}^{d}\sigma_{i}u_{i}v_{i}^{\top}\) and \(C_{2}>0\), suppose \(\sigma_{1}\geq...\geq\sigma_{k}>C_{2}\geq\sigma_{k+1}...\geq\sigma_{d}\), then for all \(\left\|E\right\|_{2}\leq C_{2}\), we have \(\left\|G-E\right\|_{F}^{2}\geq\sum_{i=1}^{k}{(\sigma_{i}-C_{2})^{2}}\)._

The following lemmas analyze the algorithm's correctness and show that it terminates with the desired distance contraction, as well as its running time. The proof is deferred to Appendix A.

**Lemma 4.2** (Algorithm 2: HalveError).: _Given a \((1,12L^{2})\)-oracle for \(\mathcal{A}\) with failure probability \(\delta\in(0,1)\), where \(\mathcal{A}\) satisfies the dRIP Condition 2.3, and \(b=\mathcal{A}[X^{*}]\), Algorithm 2 succeeds with probability at least \(1-\delta\)._

**Lemma 4.3** (Algorithm 2 Running Time).: _Algorithm 2 with failure probability \(\delta\) runs in time \(O(nd^{\omega}\operatorname{polylog}{(d)}\log\frac{L}{\delta}r\rho^{2}L^{4})\)._

The crucial part of Lemma 4.2 shows that if current estimate \(X_{t}\) is sufficiently far from \(X^{*}\), i.e. \(\left\|X_{t}-X^{*}\right\|_{F}\geq\frac{1}{\delta}R\), then according to Lemma 5.3 with high probability the weight oracle produces an output satisfying the progress and decomposition guarantees, and each iteration of Algorithm 2 decreases the distance to \(X^{*}\) by a constant factor: \(\left\|X_{t+1}-X^{*}\right\|_{F}^{2}\leq\left(1-\frac{\eta}{2}\right)\cdot \left\|X_{t}-X^{*}\right\|_{F}^{2}\), thus after sufficient number of iterations the distance to \(X^{*}\) will be halved. On the other hand, if the weight oracle fails, with high probability the current estimate \(X_{t}\) is already sufficiently close to \(X^{*}\), thus the algorithm can terminate early.

[MISSING_PAGE_FAIL:9]

\(\left[\frac{1}{4},1\right]\) and \(\left\|V\right\|_{*}\leq 2\sqrt{2r}\)). Then, Algorithm 3 is a \((1,12L^{2})\)-oracle for \(\mathcal{A}\) (as in Definition 2.5) with failure probability at most \(\delta\)._

We prove this lemma in two steps: first we show in Lemma B.1 that the output is valid; then in Lemma B.2 we show that the oracle achieves the success probability. Our weight oracle is inspired by the step oracle in [12]. It is worth noting that Lemma B.3, a key component used in the proof of Lemma B.2, is significantly different in the matrix case compared to the vector case. Lemma B.3 upper bounds the increase in \(\Phi_{\text{de}}\) each round, which is then used to provide a lower bound for the increase in \(\Phi\). Combining Lemma B.3 with our earlier remark that the algorithm terminates when \(\Phi\geq 0\) gives us the number of iterations needed to terminate with high probability.

The running time of Algorithm 3 is stated in the following lemma, and the proof is deferred to Appendix B.

**Lemma 5.4** (Algorithm 3 Running Time).: _Algorithm 3 with failure probability \(\delta\) runs in time \(O(nd^{\omega}\operatorname{polylog}\left(d\right)\log\frac{1}{\delta}r\rho^{2 })\)._

## 6 Conclusion and Future Work

In this paper, we pose and study the matrix sensing problem in a natural semi-random model. We relax the standard RIP assumption on the input sensing matrices to a much weaker condition where an unknown subset of the sensing matrices satisfies RIP while the rest are arbitrary.

For this semi-random matrix sensing problem, existing non-convex objectives can have bad local optima. In this work, we employ an iterative reweighting approach using a weight oracle to overcome the influence of the semi-random input. Our solution is inspired by previous work on semi-random sparse vector recovery, where we exploit the structural similarities between linear regression on sparse vectors and matrix sensing on low-rank matrices.

Looking forward, we believe our approach can serve as a starting point for designing more efficient and robust algorithms for matrix sensing, as well as for other low-rank matrix and sparse vector problems in the semi-random model.

## Acknowledgement

We thank Rong Ge for helpful discussions. Xing Gao is supported in part by NSF awards ECCS-2217023 and CCF-2307106. Yu Cheng is supported in part by NSF Award CCF-2307106.

## References

* [BNS16] Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. _Advances in Neural Information Processing Systems_, 29, 2016.
* [BS95] Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs. _Journal of Algorithms_, 19(2):204-234, 1995.
* [CG18] Yu Cheng and Rong Ge. Non-convex matrix completion against a semi-random adversary. In _Conference On Learning Theory_, pages 1362-1394. PMLR, 2018.
* [CLMW11] Emmanuel J Candes, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? _Journal of the ACM (JACM)_, 58(3):1-37, 2011.
* [CP11] Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. _IEEE Transactions on Information Theory_, 57(4):2342-2359, 2011.
* [CRT06] Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. _Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences_, 59(8):1207-1223, 2006.
* [DDH07] J. Demmel, I. Dumitriu, and O. Holtz. Fast linear algebra is stable. _Numerische Mathematik_, 108(1):59-91, 2007.
* [FCRP08] Maryam Fazel, E Candes, Benjamin Recht, and P Parrilo. Compressed sensing and robust recovery of low rank matrices. In _2008 42nd Asilomar Conference on Signals, Systems and Computers_, pages 1043-1047. IEEE, 2008.
* [FK01] Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. _Journal of Computer and System Sciences_, 63(4):639-671, 2001.
* [GJZ17] Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. In _International Conference on Machine Learning_, pages 1233-1242. PMLR, 2017.
* [JKL\({}^{+}\)20] Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior point method for semidefinite programming. In _2020 IEEE 61st annual symposium on foundations of computer science (FOCS)_, pages 910-918. IEEE, 2020.
* [KLL\({}^{+}\)23] Jonathan Kelner, Jerry Li, Allen X Liu, Aaron Sidford, and Kevin Tian. Semi-random sparse recovery in nearly-linear time. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2352-2398. PMLR, 2023.
* [MMV12] Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Approximation algorithms for semi-random partitioning problems. In _Proceedings of the forty-fourth annual ACM symposium on Theory of computing_, pages 367-384, 2012.
* [MS10] Claire Mathieu and Warren Schudy. Correlation clustering with noisy input. In _Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms_, pages 712-728. SIAM, 2010.
* [PW17] Amelia Perry and Alexander S Wein. A semidefinite program for unbalanced multi-section in the stochastic block model. In _2017 International Conference on Sampling Theory and Applications (SampTA)_, pages 64-67. IEEE, 2017.
* [RFP10] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. _SIAM review_, 52(3):471-501, 2010.
* [RS05] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In _Proceedings of the 22nd international conference on Machine learning_, pages 713-719, 2005.

* [Tho65] Colin J Thompson. Inequality with applications in statistical mechanics. _Journal of Mathematical Physics_, 6(11):1812-1813, 1965.
* [Wil14] Virginia Vassilevska Williams. Multiplying matrices in o (n2. 373) time. _preprint_, 2014.

Omitted Proofs from Section 4

To check whether \(G\) is \((C_{F},C_{2})\)-decomposable using Lemma 4.1, we first consider the following construction. Suppose the SVD of \(G\) is \(G=U\Sigma V^{\top}=\sum_{i=1}^{d}\sigma_{i}u_{i}v_{i}^{\top}\). Let \(S=\sum_{i=1}^{d}\mu_{i}u_{i}v_{i}^{\top}\) with \(\mu_{i}=\max(\sigma_{i}-C_{2},0)\) and let \(E=\sum_{i=1}^{d}\lambda_{i}u_{i}v_{i}^{\top}\) with \(\lambda_{i}=\min(C_{2},\sigma_{i})\). In other words, suppose \(\sigma_{1}\geq...\geq\sigma_{k}>C_{2}\geq\sigma_{k+1}...\geq\sigma_{d}\). For \(i\leq k\) (i.e., \(\sigma_{i}>C_{2}\)), let \(\mu_{i}=\sigma_{i}-C_{2}\) and \(\lambda_{i}=C_{2}\); for \(i>k\) (i.e., \(\sigma_{i}\leq C_{2}\)), let \(\mu_{i}=0\) and \(\lambda_{i}=\sigma_{i}\). We have \(G=S+E\) with \(\left\|E\right\|_{2}\leq C_{2}\), and \(\left\|S\right\|_{F}=\sqrt{\sum_{i=1}^{k}\left(\sigma_{i}-C_{2}\right)^{2}}\). Then by Lemma 4.1, \(G\) is \((C_{F},C_{2})\)-norm-decomposable if and only if \(\left\|S\right\|_{F}\leq C_{F}\) because:

* if \(\left\|S\right\|_{F}\leq C_{F}\), we have a valid \((C_{F},C_{2})\)-norm-decomposition for \(G\) ;
* if \(\left\|S\right\|_{F}>C_{F}\), a valid \((C_{F},C_{2})\)-norm-decomposition does not exist for \(G\).

**Lemma 4.1** (Verify Norm Decomposition).: _Given a matrix \(G=U\Sigma V^{\top}=\sum_{i=1}^{d}\sigma_{i}u_{i}v_{i}^{\top}\) and \(C_{2}>0\), suppose \(\sigma_{1}\geq...\geq\sigma_{k}>C_{2}\geq\sigma_{k+1}...\geq\sigma_{d}\), then for all \(\left\|E\right\|_{2}\leq C_{2}\), we have \(\left\|G-E\right\|_{F}^{2}\geq\sum_{i=1}^{k}\left(\sigma_{i}-C_{2}\right)^{2}\)._

Proof.: Fix any \(E\) with \(\left\|E\right\|_{2}\leq C_{2}\). Observe that for all \(1\leq i\leq k\), we have \(u_{i}^{\top}Gv_{i}=\sigma_{i}\) and \(-C_{2}\leq u_{i}^{\top}Ev_{i}\leq C_{2}\). Consequently, for all \(i\leq k\), we have \(\sigma_{i}>C_{2}\) and \(u_{i}^{\top}(G-E)v_{i}\geq\sigma_{i}-C_{2}>0\).

Let \(S=G-E\), we have

\[\left\|S\right\|_{F}^{2} =\left\|U^{\top}SV\right\|_{F}^{2}=\sum_{i,j}\left(U^{\top}SV \right)_{ij}^{2}\] \[\geq\sum_{i=1}^{d}\left(U^{\top}SV\right)_{ii}^{2}\geq\sum_{i=1}^{ k}\left(U^{\top}SV\right)_{ii}^{2}\] \[=\sum_{i=1}^{k}(u_{i}^{\top}Sv_{i})^{2}\geq\sum_{i=1}^{k}\left( \sigma_{i}-C_{2}\right)^{2}\.\qed\]

**Lemma 4.2** (Algorithm 2: HalveError).: _Given a \((1,12L^{2})\)-oracle for \(\mathcal{A}\) with failure probability \(\delta\in(0,1)\), where \(\mathcal{A}\) satisfies the dRIP Condition 2.3, and \(b=\mathcal{A}[X^{*}]\), Algorithm 2 succeeds with probability at least \(1-\delta\)._

Proof.: We will first show that the distance to \(X^{*}\) decreases by a constant factor after each iteration:

\[\left\|X_{t+1}-X^{*}\right\|_{F}^{2}\leq\left(1-\frac{\eta}{2}\right)\cdot \left\|X_{t}-X^{*}\right\|_{F}^{2}\.\]

Consider iteration \(t\) in Algorithm 2: \(X_{t+1}=\text{argmin}_{X\in\mathcal{X}}\|X-(X_{t}-\eta RG_{t})\|_{F}^{2}\). Taking the gradient of \(\left\|X-(X_{t}-\eta RG_{t})\right\|_{F}^{2}\) at \(X=X_{t+1}\), we get \(2[X_{t+1}-(X_{t}-\eta RG_{t})]\). Since \(X^{*}\in\mathcal{X}\) and \(X_{t+1}\) is the local minimizer in \(\mathcal{X}\):

\[2\langle X_{t+1}-(X_{t}-\eta RG_{t}),X_{t+1}-X^{*}\rangle\leq 0\.\]

Rearranging the terms gives

\[2\eta R\langle G_{t},X_{t+1}-X^{*}\rangle\leq-2\langle X_{t+1}-X_{t},X_{t+1}-X ^{*}\rangle\.\]

To simplify, let \(D=X_{t+1}-X_{t}\), \(D_{t}=X_{t}-X^{*}\), \(D_{t+1}=X_{t+1}-X^{*}\). Note that \(D+D_{t}=D_{t+1}\).

\[2\eta R\langle G_{t},D_{t+1}\rangle \leq-2\langle D,D_{t+1}\rangle=\langle D-D_{t+1},D-D_{t+1}\rangle -\langle D,D\rangle-\langle D_{t+1},D_{t+1}\rangle\] \[=\langle D_{t},D_{t}\rangle-\langle D,D\rangle-\langle D_{t+1},D_ {t+1}\rangle\,\] \[\left\|D_{t}\right\|_{F}^{2}-\left\|D_{t+1}\right\|_{F}^{2} \geq 2\eta R\langle G_{t},D_{t+1}\rangle+\langle D,D\rangle=2\eta R \langle G_{t},D_{t}\rangle+2\eta R\langle G_{t},D\rangle+\langle D,D\rangle\.\] (3)

Inequality (3) provides a lower bound on the distance contraction after each iteration. We break the right-hand side into two parts. The first term \(2\eta R\langle G_{t},D_{t}\rangle\) corresponds to the magnitude of the step \(G_{t}\) in the direction \(D_{t}=X_{t}-X^{*}\), which is the progress made by this step. To lower bound it, we will use the **progress guarantee** of the \((1,12L^{2})\)-oracle. Recall \(u_{t}=\frac{1}{R}(\mathcal{A}[X_{t}]-b)\) and consider \(V_{t}=\frac{1}{R}(X_{t}-X^{*})=\frac{1}{R}D_{t}\) so that \(u_{t}=\mathcal{A}[V_{t}]\). Given that the oracle's output satisfies the progress guarantee, which states that \(\sum_{i=1}^{n}{(w_{t})_{i}(u_{t})_{i}^{2}}\geq 1\), we have:

\[2\eta R\langle G_{t},D_{t}\rangle=2\eta R^{2}\langle G_{t},V_{t} \rangle=2\eta R^{2}\Big{\langle}\sum_{i=1}^{n}{(w_{t})_{i}(u_{t})_{i}A_{i}},V_{ t}\Big{\rangle}=2\eta R^{2}\sum_{i=1}^{n}{(w_{t})_{i}(u_{t})_{i}^{2}}\geq 2\eta R ^{2}\;.\] (4)

The remaining term \(2\eta R\langle G_{t},D\rangle+\langle D,D\rangle\) might be negative and cancel some of our progress. A natural attempt is to try to bound it using \(2\eta R\langle G_{t},D\rangle+\langle D,D\rangle\geq-\eta^{2}R^{2}\langle G_{ t},G_{t}\rangle\). However, the wRIP condition of \(\mathcal{A}\) does not provide any guarantee on \(\left\|G_{t}\right\|_{F}^{2}\). (In fact, we can derive that \(\left\|G\right\|_{2}\leq L\) from wRIP, but the best we can hope for is \(\left\|G\right\|_{F}^{2}\leq\operatorname{rank}(G)\cdot\left\|G\right\|_{2}^ {2}\) where \(\operatorname{rank}(G)\leq d\).) This motivates the decomposition property in the dIRIP Condition 2.3 and in the weight oracle. The idea is that even though we cannot directly bound \(\left\|G_{t}\right\|_{F}\), we can in fact lower bound \(2\eta R\langle G_{t},D\rangle+\langle D,D\rangle\) the term by decomposing \(G_{t}\) into a Frobenius-norm-bounded matrix \(S_{t}\), and an operator-norm-bounded matrix \(E_{t}\). Specifically, we will use the **decomposition guarantee** of the \((1,12L^{2})\)-oracle, which states that there exists norm-decomposition of \(G_{t}=S_{t}+E_{t}\) where \(\left\|S_{t}\right\|_{F}\leq 12L^{2}\) and \(\left\|E_{t}\right\|_{2}\leq\frac{1}{6\sqrt{r}}\). As our movement is confined in \(\mathcal{X}\), \(D=X_{t+1}-X_{t}\) is nuclear-norm-bounded so the inner product \(\langle E_{t},D\rangle\) can be bounded by generalized Holder's inequality. Recall \(\eta=\frac{1}{288L^{4}}\).

\[2\eta R\langle G_{t},D\rangle+\langle D,D\rangle =2\eta R\langle E_{t},D\rangle+2\eta R\langle S_{t},D\rangle+ \langle D,D\rangle\] \[\geq-2\eta R\left\|E_{t}\right\|_{2}\cdot\left\|D\right\|_{*}- \eta^{2}R^{2}\langle S_{t},S_{t}\rangle\] \[\geq\ -2\eta R\cdot\frac{1}{6\sqrt{r}}\cdot 2\sqrt{2r}R-\eta^{2}R^{2 }\cdot 144L^{4}\] \[\geq\ -\frac{3}{2}\eta R^{2}\;.\] (5)

Putting inequalities 3,4 and 5 together:

\[\left\|D_{t}\right\|_{F}^{2}-\left\|D_{t+1}\right\|_{F}^{2}\geq 2\eta R^{2}- \frac{3}{2}\eta R^{2}\geq\frac{\eta}{2}\cdot\left\|D_{t}\right\|_{F}^{2}\implies \left\|D_{t+1}\right\|_{F}^{2}\leq\left(1-\frac{\eta}{2}\right)\cdot\left\|D_ {t}\right\|_{F}^{2}\;.\]

In the case that the algorithm terminates after \(T=\frac{6}{\eta}\) iterations,

\[\left\|X_{T}-X^{*}\right\|_{F}^{2} \leq\left(1-\frac{\eta}{2}\right)^{T}\cdot\left\|X_{\text{in}}-X^ {*}\right\|_{F}^{2}\leq\exp\left(-\frac{\eta T}{2}\right)\cdot R^{2}\leq\frac{ 1}{16}R^{2}\;,\] \[\left\|X_{\text{out}}-X^{*}\right\|_{F} \leq\left\|X_{\text{out}}-X_{T}\right\|_{F}+\left\|X_{T}-X^{*} \right\|_{F}\leq 2\left\|X_{T}-X^{*}\right\|_{F}\leq\frac{1}{2}R\;.\]

The last inequality comes from \(X_{\text{out}}\) being the best rank-\(r\) approximation of \(X_{T}\).

In the case that the algorithm terminates early at Line 11, we can assume with probability at least \(1-\frac{\delta}{T}\) that the weight oracle would have succeeded given applicable input \(u_{t}\). Then failure to satisfy the progress and decomposition guarantees means that \(u_{t}\) is not an applicable input, which means \(V_{t}\) does not satisfy the norm constraint in the weight oracle. \(\left\|V_{t}\right\|_{*}\leq 2\sqrt{2r}\) is guaranteed because \(X_{t}\in\mathcal{X}\), and \(\left\|V_{t}\right\|_{F}=\frac{1}{R}\left\|X_{t}-X^{*}\right\|_{F}\) is decreasing in each round, so we must have \(\left\|V_{t}\right\|_{F}<\frac{1}{4}\), which means \(\left\|X_{t}-X^{*}\right\|_{F}<\frac{1}{4}R\). By the same argument as above, \(\left\|X_{\text{out}}-X^{*}\right\|_{F}\leq\frac{1}{2}R\).

Finally, by a union bound on the failure probability of the weight oracle, the algorithm succeeds with probability at least \(1-\delta\). 

**Lemma 4.3** (Algorithm 2 Running Time).: _Algorithm 2 with failure probability \(\delta\) runs in time \(O(nd^{\omega}\operatorname{polylog}{(d)}\log\frac{L}{\delta}r\rho^{2}L^{4})\)._

Proof.: Algorithm 2 has a for-loop that's repeated for \(T=O(L^{4})\) times.

Inside the loop, line 5 and 7 takes linear time \(O(nd^{2})\). Computing \(w_{t}\) using the oracle (line 6) runs in time \(O(nd^{\omega}\operatorname{polylog}{(d)}\log\frac{L}{\delta}r\rho^{2})\) according to Lemma 5.4. Line 8 through line 12 are all upper bounded by time of SVD, which is on the same order of matrix multiplication \(O(d^{\omega})\)[DDH07], with current best of \(O(d^{2.373})\)[Wil14]. In particular, verifying the oracle guarantees (line 8) can be solved as an eigenvalue problem. Finding \(X_{t+1}\) (line 9) is equivalent to projecting \(X_{t+1}^{\prime}:=\frac{X_{t}-\eta RG_{t}-X_{u}}{\sqrt{2}rR}\) onto the unit nuclear norm ball. We first perform SVD on \(X_{t+1}^{\prime}\) then binary search for the largest truncation from its singular values to reach the nuclear norm sphere in time \(O(d\log d)\), and the entire projection step is dominated by SVD. Finally the output step consists of SVD and matrix multiplication.

The overall running time of the algorithm is dominated by the weight oracle, so the total running time is \(O(nd^{\omega}\operatorname{polylog}\left(d\right)\log\frac{L}{d}r\rho^{2}L^{4})\). 

## Appendix B Omitted proofs from Section 5

First we state a couple of lower bounds related to the decomposition potential function.

**Claim B.1**.: \(\mu^{2}\log[F(E)]\geq\mu^{2}\log(d)\) _and \(\mu^{2}\log[F(E)]\geq\left\|E\right\|_{2}^{2}.\)_

Proof.: For the first lower bound, \(\exp\left(\frac{\sigma_{j}^{2}(E)}{\mu^{2}}\right)\geq 1\) for all \(j\in[d]\) so \(F(E)=\sum_{j=1}^{d}\exp\left(\frac{\sigma_{j}^{2}(E)}{\mu^{2}}\right)\geq d\).

For the second lower bound, \(F(E)=\sum_{j=1}^{d}\exp\left(\frac{\sigma_{j}^{2}(E)}{\mu^{2}}\right)\geq\exp \left(\frac{\sigma_{1}^{2}(E)}{\mu^{2}}\right)=\exp\left(\frac{\left\|E\right\| _{2}^{2}}{\mu^{2}}\right).\) 

**Lemma B.1** (Correctness of Algorithm 3).: _If Algorithm 3 terminates from the inner loop, the output satisfies the progress and decomposition guarantees as defined in 2.5._

Proof.: We start with \(w_{0}=0\), which means \(\Phi_{\text{prog}}(w_{0})=0\), \(\Phi_{\text{dc}}(w_{0})=\mu^{2}\log d\), and \(\Phi(w_{0})=0-Cr\mu^{2}\log d=-1\).

At each round, since \(s_{t}\) is chosen to maximize \(\Phi(w_{t}+s_{t}e_{i})\), in particular if we choose \(s_{t}=0\) then \(\Phi(w_{t+1})=\Phi(w_{t})\), so \(\Phi(w_{t+1})\geq\Phi(w_{t})\) which is non-decreasing. By definition \(\Phi_{\text{prog}}(w_{t})\) is also non-decreasing, and increases by at most \(1\) each round, because \(\Phi_{\text{prog}}(w_{t+1})-\Phi_{\text{prog}}(w_{t})=s_{t}u_{i}^{2}\leq\eta( \left\|A_{i}\right\|_{2}\left\|V\right\|_{*})^{2}\leq 8pr\rho^{2}\leq\frac{8}{K \log d}\leq 1\). \(\Phi_{\text{dc}}(w_{t})\) may not be monotone, but we have \(\Phi_{\text{dc}}(w_{t})\geq\mu^{2}\log d\).

Suppose the algorithm terminates at round \(t\) during one of the inner loops, which means \(\Phi_{\text{prog}}(w_{t-1})<1\) and \(1\leq\Phi_{\text{prog}}(w_{t})<2\).

**Progress guarantee**: \(\Phi_{\text{prog}}(w_{t})=\sum_{i=1}^{n}\left(w_{t}\right)_{i}u_{i}^{2}\geq 1\) is satisfied upon termination.

**Decomposition guarantee**:

\[\Phi(w_{t})=\Phi_{\text{prog}}(w_{t})-Cr\Phi_{\text{dc}}(w_{t})\geq\Phi(w_{0} )=-1\;,\]

\[\implies\min_{\left\|S\right\|_{F}\leq L\left\|w\right\|_{1}}\left(\mu^{2} \log\left[F(G_{w}-S)\right]\right)+\frac{\left\|w_{t}\right\|_{1}}{4CLr}=\Phi_{ \text{dc}}(w_{t})\leq\frac{\Phi_{\text{prog}}(w_{t})+1}{Cr}\leq\frac{3}{Cr}\;,\]

\[\min_{\left\|S\right\|_{F}\leq L\left\|w_{t}\right\|_{1}}\left(\mu^{2}\log \left[F(G_{w_{t}}-S)\right]\right)\leq\frac{3}{Cr}\implies\exists\left\|S \right\|_{F}\leq L\left\|w_{t}\right\|_{1}\;s.t.\;\left\|G_{w_{t}}-S\right\|_{ 2}^{2}\leq\frac{3}{Cr}\;,\]

\[\text{and }\frac{\left\|w_{t}\right\|_{1}}{4CLr}\leq\frac{3}{Cr}\implies\left\|w_{t} \right\|_{1}\leq 12L\;.\]

So there exist \(\left\|S\right\|_{F}\leq 12L^{2}\) and \(\left\|E\right\|_{2}=\left\|G-S\right\|_{2}\leq\frac{\sqrt{3}}{\sqrt{Cr}}=\frac{ 1}{6\sqrt{r}}\) which satisfy the decomposition guarantee.

Notice that for any round \(t^{\prime}<t\), \(\Phi_{\text{prog}}(w_{t^{\prime}})<1\), we also have \(\Phi_{\text{dc}}(w_{t^{\prime}})\leq\frac{\Phi_{\text{reg}}(w_{t^{\prime}})+1 }{Cr}\leq\frac{2}{Cr}\), so \(\Phi_{\text{dc}}(w_{t})\leq\frac{3}{Cr}\) throughout the algorithm, which is a fact we will use later in Lemma B.3. 

**Lemma B.2** (Success probability of Algorithm 3).: _Given \(\mathcal{A}\) satisfying dRIP Condition and applicable input \(u\), Algorithm 3 terminates from the inner loop with probability at least \(1-\delta\)._Proof.: We first show the probability that the algorithm terminates from the inner loop is at least \(\frac{1}{2}\), i.e., \(\Pr[\Phi_{\text{prog}}(w_{t})\geq 1]\geq\frac{1}{2}\) for some \(t\leq N\).

Notice that \(\Phi(w_{t})=\Phi_{\text{prog}}(w_{t})-Cr\Phi_{\text{dc}}(w_{t})\geq 0\implies \Phi_{\text{prog}}(w_{t})\geq Cr\Phi_{\text{dc}}(w_{t})\geq Cr\Phi_{\text{dc} }(w_{0})=1\), therefore the algorithm starts with \(\Phi(w_{0})=-1\) and will terminate once \(\Phi(w_{t})\geq 0\). Also notice that throughout the algorithm \(\Phi(w_{t})<1\) because \(\Phi_{\text{prog}}(w_{t})<2\) and \(Cr\Phi_{\text{dc}}(w_{t})\geq 1\) (from proof of Lemma B.1).

To prove by contradiction, assume that \(\Pr[\Phi_{\text{prog}}(w_{t})\geq 1]<\frac{1}{2}\) for all \(t\leq N\), i.e., \(\Pr[\text{continue}]\geq\frac{1}{2}\) for all rounds. We will lower bound the expected increase in \(\Phi(w_{t})\) each round, and we will show that with sufficiently large \(N\), \(\mathbb{E}[\Phi(w_{N})]\geq 1\) contradicting \(\Phi(w_{t})<1\) for all \(t\leq N\).

Recall that \(\Phi(w_{t})=\Phi_{\text{prog}}(w_{t})-Cr\Phi_{\text{dc}}(w_{t})\), the lower bound for increase in \(\Phi_{\text{prog}}(w_{t})\) is provided by dRIP condition on \(\mathcal{A}\) and applicable input \(u\). The upper bound for expected increase in \(\Phi_{\text{dc}}(w_{t})\) is provided by Lemma B.3.

Given the algorithm continues at round \(t\leq N\), consider choosing \(s_{t}=\eta w_{i}^{*}\) so that \(w^{\prime}=w_{t}+\eta w_{i}^{*}e_{i}\), then the expected increase in \(\Phi\) is at least:

\[\mathbb{E}[\Phi(w_{t+1})-\Phi(w_{t})\mid\text{continue}] =\mathbb{E}[\Phi_{\text{prog}}(w_{t+1})-\Phi_{\text{prog}}(w_{t} )]-Cr\mathbb{E}[\Phi_{\text{dc}}(w_{t+1})-\Phi_{\text{dc}}(w_{t})]\] \[\geq\mathbb{E}[\Phi_{\text{prog}}(w^{\prime})-\Phi_{\text{prog}} (w_{t})]-Cr\mathbb{E}[\Phi_{\text{dc}}(w^{\prime})-\Phi_{\text{dc}}(w_{t})]\] \[=\frac{1}{n}\sum_{i=1}^{n}\eta w_{i}^{*}u_{i}^{2}-Cr\Big{(} \mathbb{E}[\Phi_{\text{dc}}(w^{\prime})]-\Phi_{\text{dc}}(w_{t})\Big{)}\] \[\geq\frac{\eta}{Ln}-Cr\cdot\frac{\eta}{2CLnr}\] \[=\frac{\eta}{2Ln}\;.\]

Given the algorithm stops after round \(t\), \(\mathbb{E}[\Phi(w_{t+1})-\Phi(w_{t})\mid\text{stop}]=0\). Overall:

\[\mathbb{E}[\Phi(w_{t+1})-\Phi(w_{t})] =\mathbb{E}[\Phi(w_{t+1})-\Phi(w_{t})\mid\text{continue}]\cdot \Pr[\text{continue}]+0\] \[\geq\frac{\eta}{2Ln}\cdot\Pr[\text{continue}]\;.\]

By choosing a sufficiently large \(N=\frac{8Ln}{\eta}\):

\[\mathbb{E}[\Phi(w_{N})]\geq\Phi(w_{0})+\frac{\eta N}{2Ln}\cdot\Pr[\text{ continue}]\geq-1+\frac{\eta N}{4Ln}\geq 1\;,\]

contradicting \(\Phi(w_{t})<1\). This means each inner loop of the algorithm will terminate with probability greater than \(\frac{1}{2}\). Finally, we boost the success probability to \((1-\delta)\) using the outer loop with \(N^{\prime}=\log_{2}\frac{1}{\delta}\) iterations. 

Lemma B.3 provides a crucial bound used in the proof. Even though it achieves similar result as Lemma 13 [13] on the potential functions defined for vectors, analyzing the potential function defined for matrices involves very different techniques.

**Lemma B.3** (Potential Increase Upper Bound).: _Given \(w\in\mathbb{R}^{n}\)\(s.t.\)\(\Phi_{dc}(w)\leq\frac{3}{Cr}\), by choosing a sufficiently large value for \(K\), for \(w^{\prime}=w+\eta w_{i}^{*}e_{i}\), we have:_

\[\mathbb{E}_{i\in[n]}[\Phi_{\text{dc}}(w^{\prime})]\leq\Phi_{\text{dc}}(w)+ \frac{\eta}{2CLnr}\;.\]

_The assumption \(\Phi_{\text{dc}}(w)\leq\frac{3}{Cr}\) is justified in the proof of Lemma B.1._

Proof.: First we introduce some notation:

Denote \(\Phi_{\text{op}}(w)=\min_{\left\|S\right\|_{F}\leq L\left\|w\right\|_{1}} \left(\mu^{2}\log\left[F(G_{w}-S)\right]\right)\), so that \(\Phi_{\text{dc}}(w)=\Phi_{\text{op}}(w)+\frac{\left\|w\right\|_{1}}{4CLr}\). Let \(G^{*}=\sum_{i=1}^{n}w_{i}^{*}u_{i}A_{i}\), and by dRIP Condition 2.3, we know \(\exists(L,\frac{1}{K\sqrt{r}})\)-norm-decomposition of \(G^{*}=S^{*}+E^{*}\), where \(\left\|S^{*}\right\|_{F}\leq L\) and \(\left\|E^{*}\right\|_{2}\leq\frac{1}{K\sqrt{r}}\). Let \(G=\sum_{i=1}^{n}w_{i}u_{i}A_{i}\), and \(\operatorname{argmin}\!\Phi_{\text{op}}(w)\) so that \(\Phi_{\text{op}}(w)=\mu^{2}\log[F(G-S)]\), and let \(E=G-S\). Let \(G^{\prime}=\sum_{i=1}^{n}w^{\prime}_{i}u_{i}A_{i}\). Using these notation and \(\sum_{i=1}^{n}w^{*}_{i}=1\):

\[\operatorname{\mathbb{E}}_{i\in[n]}[\Phi_{\text{dc}}(w^{\prime})] =\operatorname{\mathbb{E}}_{i\in[n]}\biggl{[}\Phi_{\text{op}}(w^{ \prime})+\frac{\left\lVert w^{\prime}\right\rVert_{1}}{4CLr}\biggr{]}= \operatorname{\mathbb{E}}_{i\in[n]}\biggl{[}\Phi_{\text{op}}(w^{\prime})+ \frac{\left\lVert w\right\rVert_{1}+\eta w^{*}_{i}}{4CLr}\biggr{]}\] \[=\operatorname{\mathbb{E}}_{i\in[n]}[\Phi_{\text{op}}(w^{\prime} )]+\frac{\left\lVert w\right\rVert_{1}}{4CLr}+\frac{\eta}{4CLnr}\;.\]

We need to show \(\operatorname{\mathbb{E}}_{i\in[n]}[\Phi_{\text{dc}}(w^{\prime})]\leq\Phi_{ \text{dc}}(w)+\frac{\eta}{2CLnr}=\Phi_{\text{op}}(w)+\frac{\left\lVert w \right\rVert_{1}}{4CLr}+\frac{\eta}{2CLnr}\), equivalently

\[\operatorname{\mathbb{E}}_{i\in[n]}[\Phi_{\text{op}}(w^{\prime})]\leq\Phi_{ \text{op}}(w)+\frac{\eta}{4CLnr}\;.\]

Consider \(S^{\prime}=S+\eta w^{*}_{i}S^{*}\). We have \(\left\lVert S^{\prime}\right\rVert_{F}\leq\left\lVert S\right\rVert_{F}+\eta w ^{*}_{i}\left\lVert S^{*}\right\rVert_{F}\leq L\cdot\left\lVert w\right\rVert _{1}+\eta w^{*}_{i}L=L\cdot\left\lVert w^{\prime}\right\rVert_{1}\), so \(S^{\prime}\) is a valid argument for \(\Phi_{\text{op}}(w^{\prime})=\min_{\left\lVert S\right\rVert_{F}\leq L\left\lVert w ^{\prime}\right\rVert_{1}}\left(\mu^{2}\log\left[F(G^{\prime}-S)\right]\right)\), therefore \(\Phi_{\text{op}}(w^{\prime})\leq\mu^{2}\log[F(G^{\prime}-S^{\prime})]\). Let \(E^{\prime}=G^{\prime}-S^{\prime}=G+\eta w^{*}_{i}u_{i}A_{i}-S-\eta w^{*}_{i}S^{* }=E+Z^{(i)}\) where \(Z^{(i)}=\eta w^{*}_{i}u_{i}A_{i}-\eta w^{*}_{i}S^{*}\). Using these and the concavity of the log function, we have

\[\operatorname{\mathbb{E}}_{i\in[n]}[\Phi_{\text{op}}(w^{\prime})] \leq\frac{1}{n}\sum_{i=1}^{n}\mu^{2}\log[F(G^{\prime}-S^{\prime})]= \frac{1}{n}\sum_{i=1}^{n}\mu^{2}\log[F(E+Z^{(i)})]\] \[\leq\mu^{2}\log\left[\frac{1}{n}\sum_{i=1}^{n}\left(F(E+Z^{(i)}) \right)\right]\;.\]

It suffices to show

\[\mu^{2}\log\left[\frac{1}{n}\sum_{i=1}^{n}\left(F(E+Z^{(i)})\right)\right] \leq\Phi_{\text{op}}(w)+\frac{\eta}{4CLnr}=\mu^{2}\log[F(E)]+\frac{1}{4CL} \frac{\eta}{nr}\;.\]

Expanding the left hand side:

\[\mu^{2}\log\left[\frac{1}{n}\sum_{i=1}^{n}F(E+Z^{(i)})\right]\] \[=\mu^{2}\log\left[\frac{1}{n}\sum_{i=1}^{n}\operatorname{tr}\exp \left(\frac{E^{\top}E+Z^{(i)\top}Z^{(i)}+E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2} }\right)\right]\] \[\leq\mu^{2}\log\left[\frac{1}{n}\sum_{i=1}^{n}\operatorname{tr} \left[\exp\left(\frac{E^{\top}E}{\mu^{2}}\right)\cdot\exp\left(\frac{Z^{(i) \top}Z^{(i)}+E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2}}\right)\right]\right]\] \[=\mu^{2}\log\left[\operatorname{tr}\left[\exp\left(\frac{E^{\top }E}{\mu^{2}}\right)\cdot\frac{1}{n}\sum_{i=1}^{n}\exp\left(\frac{Z^{(i)\top}Z^ {(i)}+E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2}}\right)\right]\right]\] \[\leq\mu^{2}\log\left[\operatorname{tr}\exp\left(\frac{E^{\top}E}{ \mu^{2}}\right)\cdot\left\lVert\frac{1}{n}\sum_{i=1}^{n}\exp\left(\frac{Z^{(i) \top}Z^{(i)}+E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2}}\right)\right\rVert_{2}\right]\] \[=\mu^{2}\log\left[F(E)\right]+\mu^{2}\log\left\lVert\frac{1}{n} \sum_{i=1}^{n}\exp\left(\frac{Z^{(i)\top}Z^{(i)}+E^{\top}Z^{(i)}+Z^{(i)\top}E}{ \mu^{2}}\right)\right\rVert_{2}\]

The first inequality uses Golden-Thompson Inequality (stated as Lemma B.6), and the second inequality follows from Lemma B.4. Finally it suffices to show

\[\mu^{2}\log\left\lVert\frac{1}{n}\sum_{i=1}^{n}\exp\left(\frac{Z^{(i)\top}Z^{(i) }+E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2}}\right)\right\rVert_{2}\leq\frac{1}{4CL }\frac{\eta}{nr}\;.\]

We will use the approximation \(\exp(X)\preccurlyeq I+X+X^{2}\) for symmetric \(X\) with \(\left\lVert X\right\rVert_{2}\leq 1\). The argument in the exponential satisfies this condition as justified in Claim B.2. We will also use \(\log\left(1+x\right)\leq x\;\forall x\geq 0\).

\[\mu^{2}\log\left\|\frac{1}{n}\sum_{i=1}^{n}\exp\left(\frac{Z^{(i) \top}Z^{(i)}+E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2}}\right)\right\|_{2}\] \[\leq\mu^{2}\log\left[\frac{1}{n}\sum_{i=1}^{n}\left\|\exp\left( \frac{Z^{(i)\top}Z^{(i)}+E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2}}\right)\right\| _{2}\right]\] \[\leq\mu^{2}\log\left[\frac{1}{n}\sum_{i=1}^{n}\left\|I+\frac{Z^{( i)\top}Z^{(i)}+E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2}}+\frac{(Z^{(i)\top}Z^{(i)}+E^ {\top}Z^{(i)}+Z^{(i)\top}E)^{2}}{\mu^{4}}\right\|_{2}\right]\] \[\leq\mu^{2}\log\left[\left\|I\right\|_{2}+\left\|\frac{1}{n}\sum _{i=1}^{n}\frac{Z^{(i)\top}Z^{(i)}}{\mu^{2}}\right\|_{2}+\left\|\frac{1}{n} \sum_{i=1}^{n}\frac{E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2}}\right\|_{2}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\left.+\left\|\frac{1}{n} \sum_{i=1}^{n}\frac{(Z^{(i)\top}Z^{(i)}+E^{\top}Z^{(i)}+Z^{(i)\top}E)^{2}}{ \mu^{4}}\right\|_{2}\right]\] \[\leq\mu^{2}\log\left[\left\|I\right\|_{2}+\left\|\frac{1}{n}\sum _{i=1}^{n}\frac{Z^{(i)\top}Z^{(i)}}{\mu^{2}}\right\|_{2}+\left\|\frac{1}{n} \sum_{i=1}^{n}\frac{E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2}}\right\|_{2}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad+\left\|\frac{1}{n}\sum_{i=1}^{n}\frac{2(Z^{(i)\top}Z^{(i)})^{2}}{\mu^{4 }}\right\|_{2}+\left\|\frac{1}{n}\sum_{i=1}^{n}\frac{2(E^{\top}Z^{(i)}+Z^{(i) \top}E)^{2}}{\mu^{4}}\right\|_{2}\right]\] \[\leq\mu^{2}\left\|\frac{1}{n}\sum_{i=1}^{n}\frac{Z^{(i)\top}Z^{(i) }}{\mu^{2}}\right\|_{2}+\mu^{2}\left\|\frac{1}{n}\sum_{i=1}^{n}\frac{E^{\top}Z ^{(i)}+Z^{(i)\top}E}{\mu^{2}}\right\|_{2}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+2\mu^{2} \left\|\frac{1}{n}\sum_{i=1}^{n}\frac{(Z^{(i)\top}Z^{(i)})^{2}}{\mu^{4}} \right\|_{2}+2\mu^{2}\left\|\frac{1}{n}\sum_{i=1}^{n}\frac{(E^{\top}Z^{(i)}+Z ^{(i)\top}E)^{2}}{\mu^{4}}\right\|_{2}\] \[=\left\|\frac{1}{n}\sum_{i=1}^{n}Z^{(i)\top}Z^{(i)}\right\|_{2}+ \left\|\frac{1}{n}\sum_{i=1}^{n}E^{\top}Z^{(i)}+Z^{(i)\top}E\right\|_{2}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad+\left.\frac{2}{\mu^{2}}\left\|\frac{1}{n}\sum_{i=1}^{n}(Z^{(i)\top}Z^{( i)})^{2}\right\|_{2}+\frac{2}{\mu^{2}}\left\|\frac{1}{n}\sum_{i=1}^{n}(E^{\top}Z^{(i)}+Z ^{(i)\top}E)^{2}\right\|_{2}\]

These four terms are bounded by Claims B.3,B.4, B.5 and B.6 respectively, notice that the second term dominates the first and the third, and the forth term dominates the second. So finally we have:

\[\mu^{2}\log\left\|\frac{1}{n}\sum_{i=1}^{n}\exp\left(\frac{Z^{(i) \top}Z^{(i)}+E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2}}\right)\right\|_{2}\] \[\leq\left(\frac{3\times 4}{\sqrt{C}K}+\frac{96L^{2}}{K}\right) \frac{\eta}{nr}\] \[\leq\frac{97L^{2}}{K}\frac{\eta}{nr}\] \[=\frac{1}{4CL}\frac{\eta}{nr}\;,\text{with choice of }K=388 CL^{3}=O(L^{3})\;.\qed\]

**Lemma B.4**.: _If \(0\preccurlyeq A\), then \(\operatorname{tr}(AB)\leq\operatorname{tr}(A)\cdot\left\|B\right\|_{2}.\)_Proof.: Since \(0\preccurlyeq A\), \(A=\sum_{j}\sigma_{j}u_{j}u_{j}^{\top}\) with \(\sigma_{j}\geq 0\).

\[\operatorname{tr}(AB) =\operatorname{tr}\left(\sum_{j}\sigma_{j}u_{j}u_{j}^{\top}B \right)=\sum_{j}\sigma_{j}\operatorname{tr}(u_{j}u_{j}^{\top}B)=\sum_{j}\sigma_ {j}u_{j}^{\top}Bu_{j}\] \[\leq\sum_{j}\sigma_{j}\cdot\left\|B\right\|_{2}\] \[=\operatorname{tr}(A)\cdot\left\|B\right\|_{2}\,.\qed\]

**Lemma B.5**.: \((A+B)^{\top}(A+B)\preccurlyeq 2A^{\top}A+2B^{\top}B\) _, and \(\left[(A+B)^{\top}(A+B)\right]^{2}\preccurlyeq 8(A^{\top}A)^{2}+8(B^{\top}B)^{2}\)._

Proof.: \[2A^{\top}A+2B^{\top}B-(A+B)^{\top}(A+B) =A^{\top}A+B^{\top}B-A^{\top}B-B^{\top}A\] \[=(A-B)^{\top}(A-B)\] \[\succcurlyeq 0\;.\]

\[\left[(A+B)^{\top}(A+B)\right]^{2} \prec(2A^{\top}A+2B^{\top}B)^{2}\] \[\preccurlyeq 2[2(A^{\top}A)]^{2}+2[2(B^{\top}B)]^{2}\] \[\preccurlyeq 8(A^{\top}A)^{2}+8(B^{\top}B)^{2}\;.\qed\]

The following claims were used in Lemma B.3. Recall that \(\mathcal{A}\) satisfies dRIP Condition 2.3, \(u=\mathcal{A}[V]\in\mathbb{R}^{n}\) for some \(V\in\mathbb{R}^{d_{1}\times d_{2}}\) satisfying \(\left\|V\right\|_{F}\in[\frac{1}{4},1],\left\|V\right\|_{*}\leq 2\sqrt{2r}\), \(Z^{(i)}=\eta w_{i}^{*}(u_{i}A_{i}-S^{*})\), and \(\Phi_{\text{dc}}(w)\leq\frac{3}{Cr}\) by assumption of Lemma B.3. We have the following:

\[\left\|A_{i}\right\|_{2}\leq\rho\text{ by boundedness property of dRIP Condition \ref{dRIP},}\] \[\left|u_{i}\right|=\left|(A_{i},V)\right|\leq\left\|A_{i}\right\|_ {2}\left\|V\right\|_{*}\leq\rho 2\sqrt{2r}\leq L\sqrt{r}\rho\text{ assuming }2\sqrt{2}\leq L\;,\] \[\left\|S^{*}\right\|_{2}\leq\left\|S^{*}\right\|_{F}\leq L,\left\| E^{*}\right\|_{2}\leq\frac{1}{K\sqrt{r}}\text{ by decomposition property of dRIP Condition \ref{dRIP},}\] \[\left\|E\right\|_{2}^{2}\leq\Phi_{\text{op}}(w)\leq\Phi_{\text{dc }}(w)\leq\frac{3}{Cr}\text{ by Claim B.1 }.\]

**Claim B.2**.: \(\left\|\frac{Z^{(i)^{\top}Z^{(i)}+E^{\top}Z^{(i)}+Z^{(i)^{\top}}E}}{\mu^{2}} \right\|_{2}\leq 1\) _._

Proof.: \[\left\|Z^{(i)^{\top}Z^{(i)}}\right\|_{2} =\eta^{2}w_{i}^{*2}\left\|(u_{i}A_{i}-S^{*})^{\top}(u_{i}A_{i}-S^{ *})\right\|_{2}\] \[\leq 2\eta^{2}w_{i}^{*2}\left(u_{i}^{2}\left\|A_{i}^{\top}A_{i} \right\|_{2}+\left\|S^{*\top}S^{*}\right\|_{2}\right)\text{ (Lemma B.5)}\] \[\leq 2\eta^{2}w_{i}^{*2}(L^{2}r\rho^{4}+L^{2})\] \[\leq 4\eta^{2}w_{i}^{*2}L^{2}r\rho^{4}\]

\[\left\|E^{\top}Z^{(i)}+Z^{(i)^{\top}}E\right\|_{2} \leq 2\left\|E^{\top}Z^{(i)}\right\|_{2}\leq 2\left\|E\right\|_{2} \cdot\left\|Z^{(i)}\right\|_{2}\] \[=2\eta w_{i}^{*}\left\|E\right\|_{2}\cdot\left\|u_{i}A_{i}-S^{*} \right\|_{2}\] \[\leq 2\eta w_{i}^{*}\left\|E\right\|_{2}\cdot\left(\left|u_{i} \right|\left\|A_{i}\right\|_{2}+\left\|S^{*}\right\|_{2}\right)\] \[\leq 2\eta w_{i}^{*}\frac{2}{\sqrt{Cr}}(L\sqrt{r}\rho^{2}+L)\] \[\leq 8\eta w_{i}^{*}\frac{L\rho^{2}}{\sqrt{C}}\]Putting them together:

\[\left\|\frac{Z^{(i)\top}Z^{(i)}+E^{\top}Z^{(i)}+Z^{(i)\top}E}{\mu^{2}} \right\|_{2} \leq\frac{1}{\mu^{2}}\left(\left\|Z^{(i)\top}Z^{(i)}\right\|_{2}+ \left\|E^{\top}Z^{(i)}+Z^{(i)\top}E\right\|_{2}\right)\] \[\leq Cr\log d\cdot(4\eta^{2}w_{i}^{*2}L^{2}r\rho^{4}+8\eta w_{i}^ {*}\frac{L\rho^{2}}{\sqrt{C}})\] \[\leq\frac{16\sqrt{C}L}{K}\leq 1\text{ with sufficiently large }K\;.\qed\]

**Claim B.3**.: \(\left\|\frac{1}{n}\sum_{i=1}^{n}Z^{(i)\top}Z^{(i)}\right\|_{2}\leq\frac{4L^{ 2}}{K\log d}\cdot\frac{\eta}{nr}\;.\)__

Proof.: \[\sum_{i=1}^{n}\left\|Z^{(i)\top}Z^{(i)}\right\|_{2} =\sum_{i=1}^{n}\eta^{2}w_{i}^{*2}\left\|(u_{i}A_{i}-S^{*})^{\top} (u_{i}A_{i}-S^{*})\right\|_{2}\] \[\leq 2\sum_{i=1}^{n}\eta^{2}w_{i}^{*2}\left(u_{i}^{2}\left\|A_{i }^{\top}A_{i}\right\|_{2}+\left\|S^{*\top}S^{*}\right\|_{2}\right)\] \[\leq 2\eta\sum_{i=1}^{n}\eta w_{i}^{*}(w_{i}^{*}u_{i}^{2}\rho^{2 }+w_{i}^{*}L^{2})\] \[\leq 2\eta\frac{1}{Kr\rho^{2}\log d}(L\rho^{2}+L^{2})\] \[\leq\frac{4L^{2}}{K\log d}\frac{\eta}{r}\] \[\implies\left\|\frac{1}{n}\sum_{i=1}^{n}Z^{(i)\top}Z^{(i)} \right\|_{2} \leq\frac{1}{n}\sum_{i=1}^{n}\left\|Z^{(i)\top}Z^{(i)}\right\|_{2}\] \[\leq\frac{4L^{2}}{K\log d}\cdot\frac{\eta}{nr}\;.\qed\]

**Claim B.4**.: \(\left\|\frac{1}{n}\sum_{i=1}^{n}E^{\top}Z^{(i)}+Z^{(i)\top}E\right\|_{2}\leq \frac{4}{\sqrt{C}K}\cdot\frac{\eta}{nr}\;.\)__

Proof.: \[\left\|\frac{1}{n}\sum_{i=1}^{n}E^{\top}Z^{(i)}+Z^{(i)\top}E\right\| _{2} =\frac{1}{n}\left\|E^{\top}\sum_{i=1}^{n}Z^{(i)}+\sum_{i=1}^{n}Z^{ (i)\top}E\right\|_{2}\] \[\leq\frac{2}{n}\left\|E^{\top}\sum_{i=1}^{n}Z^{(i)}\right\|_{2}\] \[\leq\frac{2}{n}\left\|E\right\|_{2}\cdot\left\|\sum_{i=1}^{n}Z^{( i)}\right\|_{2}\] \[=\frac{2}{n}\left\|E\right\|_{2}\cdot\left\|\eta G^{*}-\eta S^{*} \right\|_{2}\] \[\leq\frac{2}{n}\left\|E\right\|_{2}\cdot\eta\left\|E^{*}\right\| _{2}\] \[\leq\frac{2}{n}\cdot\frac{2}{\sqrt{Cr}}\cdot\frac{\eta}{K\sqrt{r}}\] \[=\frac{4}{\sqrt{C}K}\frac{\eta}{nr}\;.\qed\]

**Claim B.5**.: \(\frac{2}{\mu^{2}}\left\|\frac{1}{n}\sum_{i=1}^{n}(Z^{(i)\top}Z^{(i)})^{2} \right\|_{2}\leq O\left(\frac{CL^{4}}{K^{3}r\rho^{2}\log^{2}d}\right)\cdot\frac {\eta}{nr}\;.\)__Proof.: \[\left\|\sum_{i=1}^{n}({Z^{(i)}}^{\top}Z^{(i)})^{2}\right\|_{2} \leq 8\sum_{i=1}^{n}\eta^{4}{w_{i}^{*}}^{4}\left(u_{i}^{4}\cdot \left\|(A_{i}^{\top}A_{i})^{2}\right\|_{2}+\left\|({S^{*}}^{\top}S^{*})^{2} \right\|_{2}\right)\text{ (Lemma B.5)}\] \[\leq 8\eta\sum_{i=1}^{n}\eta^{3}{w_{i}^{*}}^{2}({w_{i}^{*}}^{2}u_ {i}^{4}\rho^{4}+{w_{i}^{*}}^{2}L^{4})\] \[\leq 8\eta\cdot\frac{1}{K^{3}r^{3}\rho^{6}\log^{3}d}\left[\sum_{i =1}^{n}{w_{i}^{*}}^{2}\rho^{4}u_{i}^{4}+\sum_{i=1}^{n}{w_{i}^{*}}^{2}L^{4}\right]\] \[\leq 8\eta\cdot\frac{1}{K^{3}r^{3}\rho^{6}\log^{3}d}\left[\left( \sum_{i=1}^{n}{w_{i}^{*}}\rho^{2}u_{i}^{2}\right)^{2}+\left(\sum_{i=1}^{n}{w_ {i}^{*}}L^{2}\right)^{2}\right]\] \[\leq 8\eta\cdot\frac{1}{K^{3}r^{3}\rho^{6}\log^{3}d}(\rho^{4}L^{2 }+L^{4})\] \[\leq\frac{16L^{4}}{K^{3}r^{2}\rho^{2}\log^{3}d}\cdot\frac{\eta}{r}\] \[\leq O\left(\frac{L^{4}}{K^{3}r^{2}\rho^{2}\log^{3}d}\right) \cdot\frac{\eta}{r}\cdot\] \[\implies\frac{2}{\mu^{2}}\left\|\frac{1}{n}\sum_{i=1}^{n}({Z^{(i) }}^{\top}{Z^{(i)}})^{2}\right\|_{2}\leq O\left(\frac{CL^{4}}{K^{3}r\rho^{2} \log^{2}d}\right)\cdot\frac{\eta}{nr}\square\]

**Claim B.6**.: \(\frac{2}{\mu^{2}}\left\|\frac{1}{n}\sum_{i=1}^{n}({E^{\top}}{Z^{(i)}}+{Z^{(i) }}^{\top}{E})^{2}\right\|_{2}\leq\frac{96{\hat{L}}^{2}}{K}\cdot\frac{\eta}{nr}\cdot\)__

Proof.: \[\frac{2}{\mu^{2}}\left\|\frac{1}{n}\sum_{i=1}^{n}({E^{\top}}{Z^{(i )}}+{Z^{(i)}}^{\top}{E})^{2}\right\|_{2} \leq\frac{2}{\mu^{2}}\frac{1}{n}\sum_{i=1}^{n}\left\|({E^{\top}}{Z^ {(i)}}+{Z^{(i)}}^{\top}{E})^{2}\right\|_{2}\] \[\leq\frac{2}{\mu^{2}}\frac{1}{n}\sum_{i=1}^{n}4\left\|{E^{\top}}{Z ^{(i)}}^{\top}{E}\right\|_{2}\] \[\leq\frac{2}{\mu^{2}}\frac{1}{n}\sum_{i=1}^{n}4\left\|{E}\right\| _{2}^{2}\left\|{Z^{(i)}}^{\top}{Z^{(i)}}\right\|_{2}\] \[\leq\frac{8}{\mu^{2}}\left\|{E}\right\|_{2}^{2}\cdot\frac{1}{n} \sum_{i=1}^{n}\left\|{Z^{(i)}}^{\top}{Z^{(i)}}\right\|_{2}\] \[\leq\frac{8}{\mu^{2}}\cdot\frac{3}{Cr}\cdot\frac{4L^{2}}{K\log d }\frac{\eta}{nr}\text{ (Claim B.3)}\] \[=\frac{96L^{2}}{K}\cdot\frac{\eta}{nr}\square\]

**Lemma B.6** (Golden-Thompson Inequality [11]).: _For two \(n\times n\) Hermitian matrices \(A\) and \(B\):_

\[\operatorname{tr}\left(\exp(A+B)\right)\leq\operatorname{tr}\left(\exp(A)\exp (B)\right)\,.\]

**Lemma 5.4** (Algorithm 3 Running Time).: _Algorithm 3 with failure probability \(\delta\) runs in time \(O(nd^{\omega}\operatorname{polylog}{(d)}\log\frac{1}{\delta}r\rho^{2})\,.\)_

Proof.: Algorithm 3 has a nested for loop that's repeated for \(N^{\prime}\times N=O(n\log d\log\frac{1}{\delta}r\rho^{2})\) times. The major step in the loop is line 11: \(s_{t}\leftarrow\operatorname*{argmax}_{s\in[0,\eta]}\Phi(w_{t}+se_{i})\), which is equivalent to \(\operatorname*{argmin}_{s\in[0,\eta]}Cr\Phi_{\text{op}}(w+se_{i})+\frac{s}{4L} -su_{i}^{2}\). Recall that \(\Phi_{\text{op}}(w)=\min_{\left\|S\right\|_{F}\leq L\left\|w\right\|_{1}} \left(\mu^{2}\log\left[F(G_{w}-S)\right]\right)\) where \(F(E)=\operatorname{tr}\exp\left(\frac{E^{\top}E}{\mu^{2}}\right)\). Note that \(\mu^{2}\log\left[F(E)\right]\) is convex in \(E\).

First we show that \(\Phi_{\text{op}}(w)\) is convex in \(w\), i.e., given \(w_{1},w_{2}\), \(\Phi_{\text{op}}\left(\frac{1}{2}(w_{1}+w_{2})\right)\leq\frac{1}{2}\big{(}\Phi_ {\text{op}}(w_{1})+\Phi_{\text{op}}(w_{2})\big{)}\).

Let \(w_{3}=\frac{1}{2}(w_{1}+w_{2})\), and \(G_{k}=\sum_{i=1}^{n}{(w_{k})}_{i}u_{i}A_{i}\) for \(k=1,2,3\). Suppose \(S_{1},S_{2}\) attain the minimum for \(\Phi_{\text{op}}(w_{1})\), \(\Phi_{\text{op}}(w_{2})\) respectively, i.e., \(\Phi_{\text{op}}(w_{1})=\mu^{2}\log\left[F(G_{1}-S_{1})\right]\) and \(\Phi_{\text{op}}(w_{2})=\mu^{2}\log\left[F(G_{2}-S_{2})\right]\).

Let \(S_{3}=\frac{1}{2}(S_{1}+S_{2})\). Notice that \(G_{3}=\frac{1}{2}(G_{1}+G_{2})\), so \(G_{3}-S_{3}=\frac{1}{2}\left(G_{1}-S_{1}+G_{2}-S_{2}\right)\). Since \(\left\|S_{3}\right\|_{F}\leq\frac{1}{2}(\left\|S_{1}\right\|_{F}+\left\|S_{2} \right\|_{F})\leq\frac{1}{2}L(\left\|w_{1}\right\|_{1}+\left\|w_{2}\right\|_{ 1})=L\left\|w_{3}\right\|_{1}\), \(S_{3}\) is a valid argument for \(\Phi_{\text{op}}(w_{3})\), therefore:

\[\Phi_{\text{op}}(w_{3}) \leq\mu^{2}\log\left[F(G_{3}-S_{3})\right]\leq\frac{1}{2}\Big{(} \mu^{2}\log\left[F(G_{1}-S_{1})\right]+\mu^{2}\log\left[F(G_{2}-S_{2})\right] \Big{)}\] \[=\frac{1}{2}\big{(}\Phi_{\text{op}}(w_{1})+\Phi_{\text{op}}(w_{2} )\big{)}\;.\]

Line 11 is equivalent to minimizing \(Cr\Phi_{\text{op}}(w+se_{i})+\frac{s}{4L}-su_{i}^{2}\), which is convex in \(s\) for a fixed \(w\), over a bounded interval \([0,\eta]\), so the minimization needs to evaluate \(\Phi_{\text{op}}(w+se_{i})\) for \(O(\operatorname{polylog}(d))\) different values of \(s\). Evaluating \(\Phi_{\text{op}}\) is also a minimization which can be solved by computing SVD on \(G_{w}\) and evaluating \(F(G_{w}-S)\) in time \(O(d^{\omega})\) for \(\operatorname{polylog}(d)\) various constructions of \(S\). Overall finding the optimal value of \(s\) takes time \(O(d^{\omega}\cdot\operatorname{polylog}(d))\), and the algorithm's total running time is \(O(nd^{\omega}\operatorname{polylog}(d)\log\frac{1}{4}r\rho^{2})\). 

## Appendix C Omitted proofs: From wRIP to dRIP condition

Here we show that the dRIP Condition 2.3 is implied by the wRIP Condition 2.2, given proper choices of parameters within a constant factor. Notice that in the wRIP condition, we have a low-rank constraint on the input matrix, i.e., \(\operatorname{rank}(X)\leq r\), and in dRIP we have a norm constraint instead, i.e., \(\left\|V\right\|_{F}\in[\frac{1}{4},1],\left\|V\right\|_{*}\leq 2\sqrt{2r}\). To make use of the wRIP condition of \(\mathcal{A}\), we will decompose \(V\) into low-rank matrices, so that wRIP condition applies to each of the low-rank matrices. Though the rank of \(V\) is arbitrary, we can still upper bound its numerical rank based on the norm constraint.

First we will introduce a low-rank decomposition, and an upper bound on the sum of their Frobenius norms. This is the matrix version of the shelling-decomposition in Lemma 15 for vectors in [12].

**Lemma C.1** (Low-rank Decomposition).: _Given \(V\in\mathbb{R}^{d_{1}\times d_{2}}\) with \(\operatorname{Rank}_{\text{n}}(V)=\frac{\left\|V\right\|_{*}^{2}}{\left\|V \right\|_{*}^{2}}=\nu\), and let \(V=\sum\sigma_{i}u_{i}v_{i}^{\top}\) be its SVD with \(\sigma_{i}\) in descending order. Decompose \(V\) into sum of rank-\(r\) matrices, i.e., write \(V=\sum_{\ell=1}^{\ell=k}V^{(\ell)}\) where \(V^{(\ell)}=\sum_{i=(\ell-1)r+1}^{i=\ell r}\sigma_{i}u_{i}v_{i}^{\top}\). Then we have: \(\sum_{\ell=2}^{k}\left\|V^{(\ell)}\right\|_{F}\leq\sqrt{\frac{\nu}{r}}\left\| V\right\|_{F}.\)_

Proof.: Note that \(V^{(1)}\) is the rank-\(r\) approximation of \(V\), and \(V^{(\ell)}\)'s are constructed using disjoint singular values and vectors in groups of size \(r\), and are orthogonal to each other.

Denote \(\sigma_{i}\left(V^{(\ell)}\right)\) as the \(i^{\text{th}}\) largest singular value of \(V^{(\ell)}\).

\[\left\|V^{(\ell+1)}\right\|_{F} \leq\sqrt{r}\cdot\sigma_{1}\left(V^{(\ell+1)}\right)\leq\sqrt{r} \cdot\sigma_{r}\left(V^{(\ell)}\right)\leq\sqrt{r}\cdot\frac{\left\|V^{(\ell)} \right\|_{*}}{r}\;,\] \[\sum_{\ell=2}^{k}\left\|V^{(\ell)}\right\|_{F} \leq\frac{\sqrt{r}}{r}\cdot\sum_{\ell=1}^{k-1}\left\|V^{(\ell)} \right\|_{*}\leq\frac{\sqrt{r}}{r}\cdot\sum_{\ell=1}^{k}\left\|V^{(\ell)} \right\|_{*}=\frac{\sqrt{r}}{r}\cdot\left\|V\right\|_{*}\;,\] \[\frac{\left\|V\right\|_{*}^{2}}{\left\|V\right\|_{F}^{2}}\leq\nu \implies\sum_{\ell=2}^{k}\left\|V^{(\ell)}\right\|_{F}\leq\sqrt{\frac{\nu}{r}} \left\|V\right\|_{F}\;.\qed\]

Now we are ready to prove Lemma 5.1, which states that wRIP implies dRIP condition. The proof uses similar techniques as in the second part of Lemma 17[12] for vector recovery.

**Lemma 5.1** (\(wRIP\implies dRIP\)).: _If \(\mathcal{A}\) satisfies wRIP Condition 2.2 with parameters \(r^{\prime},L^{\prime},\rho\), then \(\mathcal{A}\) satisfies the dRIP Condition 2.3 with parameters \(L,K,r,\rho\) such that \(L=\Theta(L^{\prime})\), \(r=\Theta(r^{\prime})\), and some constant \(K\geq 1\)._Proof.: **Boundedness property** is satisfied by assumption \(\left\|A_{i}\right\|_{2}\leq\rho\;\forall i\).

**Isometry property**:

Consider \(V\in\mathbb{R}^{d_{1}\times d_{2}}\)\(s.t.\)\(\left\|V\right\|_{F}\in[\frac{1}{4},1],\left\|V\right\|_{*}\leq 2\sqrt{2r}\). Need to show \(\frac{1}{L}\leq\sum_{i=1}^{n}w_{i}^{*}\langle A_{i},V\rangle^{2}\leq L\).

Let \(L=25L^{\prime}\), \(K\geq 1\) and \(r=\frac{r^{\prime}}{12800L^{2}K^{2}}\).

\(\nu=\mathrm{Rank}_{\mathrm{n}}(V)=\frac{\left\|V\right\|_{2}^{2}}{\left\|V \right\|_{F}^{2}}\leq 128r\). By Lemma C.1, decompose \(V\) into rank-\(r^{\prime}\) matrices so that we can apply the \((r^{\prime},L^{\prime})\)-wRIP property of \(\mathcal{A}\).

\[\sum_{\ell=2}^{k}\left\|V^{(\ell)}\right\|_{F}\leq\sqrt{\frac{ \nu}{r^{\prime}}}\left\|V\right\|_{F}\leq\frac{1}{10L}\left\|V\right\|_{F} \leq\frac{1}{10}\left\|V\right\|_{F}\,,\] \[\left\|V\right\|_{F}\geq\left\|V^{(r^{\prime})}\right\|_{F}= \left\|V^{(1)}\right\|_{F}=\left\|V-\sum_{\ell=2}^{k}V^{(\ell)}\right\|_{F} \geq\left\|V\right\|_{F}-\sum_{\ell=2}^{k}\left\|V^{(\ell)}\right\|_{F}\geq \frac{9}{10}\left\|V\right\|_{F}.\]

Let \(B_{i}=\sqrt{w_{i}^{*}}A_{i}\), so that \(\sum_{i=1}^{n}w_{i}^{*}\langle A_{i},V\rangle^{2}=\sum_{i=1}^{n}\left\langle B _{i},V\rangle^{2}=\left\|\sum_{i=1}^{n}\left\langle B_{i},V\right\rangle e_{i} \right\|_{2}^{2}.\)

Lower bound:

\[\left\|\sum_{i=1}^{n}\left\langle B_{i},V\right\rangle e_{i} \right\|_{2} \geq\left\|\sum_{i=1}^{n}\left\langle B_{i},V^{(1)}e_{i}\right\|_{ 2}-\left\|\sum_{i=1}^{n}\left\langle B_{i},\sum_{\ell=2}^{k}V^{(\ell)}\right \rangle e_{i}\right\|_{2}\] \[\geq\left\|\sum_{i=1}^{n}\left\langle B_{i},V^{(1)}\right\rangle e _{i}\right\|_{2}-\sum_{\ell=2}^{k}\left\|\sum_{i=1}^{n}\left\langle B_{i},V^{ (\ell)}\right\rangle e_{i}\right\|_{2}\] \[=\sqrt{\sum_{i=1}^{n}\langle B_{i},V^{(1)}\rangle^{2}}-\sum_{ \ell=2}^{k}\sqrt{\sum_{i=1}^{n}\langle B_{i},V^{(\ell)}\rangle^{2}}\] \[\geq\sqrt{\frac{1}{L^{\prime}}\cdot\left\|V^{(1)}\right\|_{F}^{2} }-\sum_{\ell=2}^{k}\sqrt{L^{\prime}\cdot\left\|V^{(\ell)}\right\|_{F}^{2}}\] \[\geq\frac{0.9}{\sqrt{L}}\cdot\left\|V\right\|_{F}-\frac{0.1}{L} \sqrt{L^{\prime}}\cdot\left\|V\right\|_{F}\] \[=\frac{4.5}{\sqrt{L}}\cdot\left\|V\right\|_{F}-\frac{0.02}{\sqrt{L }}\cdot\left\|V\right\|_{F}.\]

Taking the square: \(\sum_{i=1}^{n}w_{i}^{*}\langle A_{i},V\rangle^{2}\geq\frac{4.48^{2}}{L}\cdot \left\|V\right\|_{F}^{2}\geq\frac{4.48^{2}}{16L}\geq\frac{1}{L}\).

Upper bound:

\[\left\|\sum_{i=1}^{n}\left\langle B_{i},V\right\rangle e_{i} \right\|_{2} \leq\left\|\sum_{i=1}^{n}\left\langle B_{i},V^{(1)}e_{i}\right\|_{ 2}+\sum_{\ell=2}^{k}\left\|\sum_{i=1}^{n}\left\langle B_{i},V^{(\ell)}\right \rangle e_{i}\right\|_{2}\] \[=\sqrt{\sum_{i=1}^{n}\langle B_{i},V^{(1)}\rangle^{2}}+\sum_{\ell =2}^{k}\sqrt{\sum_{i=1}^{n}\langle B_{i},V^{(\ell)}\rangle^{2}}\] \[\leq\sqrt{L^{\prime}}\cdot\left\|V\right\|_{F}^{2}+\frac{0.1\sqrt {L^{\prime}}}{L}\cdot\left\|V\right\|_{F}\] \[=\frac{\sqrt{L}}{5}\cdot\left\|V\right\|_{F}+\frac{0.02}{\sqrt{L }}\cdot\left\|V\right\|_{F}.\]

Taking the square: \(\sum_{i=1}^{n}w_{i}^{*}\langle A_{i},V\rangle^{2}\leq\frac{L}{25}\cdot\left\| V\right\|_{F}^{2}+\frac{0.02^{2}}{L}\cdot\left\|V\right\|_{F}^{2}+0.008\left\|V \right\|_{F}^{2}\leq L\cdot\left\|V\right\|_{F}^{2}\leq L\).

Combining the lower bound and upper bound: \(\frac{1}{L}\leq\sum_{i=1}^{n}w_{i}^{*}\langle A_{i},V\rangle^{2}\leq L\).

**Decomposition property**:

Let \(S=G_{(r^{\prime})}\), the rank-\(r^{\prime}\) approximation of \(G=\sum_{i=1}^{n}w_{i}^{*}\langle A_{i},V\rangle A_{i}\). Let \(E=G-S\). Suffices to show \(\left\|S\right\|_{F}\leq L\) and \(\left\|E\right\|_{2}\leq\frac{1}{K\sqrt{r}}\). We have

\[\left\|S\right\|_{F}^{2} =\langle S,S\rangle=\langle G,S\rangle=\langle\sum_{i=1}^{n}w_{i }^{*}\langle A_{i},V\rangle A_{i},S\rangle=\langle\sum_{i=1}^{n}\langle B_{i},V\rangle B_{i},S\rangle=\sum_{i=1}^{n}\langle B_{i},V\rangle\langle B_{i},S\rangle\] \[=\langle\sum_{i=1}^{n}\langle B_{i},V\rangle e_{i},\sum_{j=1}^{n }\langle B_{j},S\rangle e_{j}\rangle\leq\left\|\sum_{i=1}^{n}\langle B_{i},V \rangle e_{i}\right\|_{2}\cdot\left\|\sum_{i=1}^{n}\langle B_{i},S\rangle e_ {i}\right\|_{2}\] \[=\sqrt{\sum_{i=1}^{n}\langle B_{i},V\rangle^{2}\cdot\sqrt{\sum_ {i=1}^{n}\langle B_{i},S\rangle^{2}}=\sqrt{\sum_{i=1}^{n}w_{i}^{*}\langle A_{ i},V\rangle^{2}}\cdot\sqrt{\sum_{i=1}^{n}w_{i}^{*}\langle A_{i},S\rangle^{2}}\] \[\leq\sqrt{L}\cdot\sqrt{U}\cdot\left\|S\right\|_{F}\leq\frac{L}{5 }\left\|S\right\|_{F}\]

which implies \(\left\|S\right\|_{F}\leq\frac{L}{5}\leq L\), and consequently,

\[\left\|E\right\|_{2}=\sigma_{r^{\prime}+1}(G)\leq\sigma_{r^{\prime}}(G)= \sigma_{r^{\prime}}(S)\leq\sqrt{\frac{\left\|S\right\|_{F}^{2}}{r^{\prime}}} \leq\frac{L}{5\sqrt{r^{\prime}}}=\frac{L}{400\sqrt{2}LK\sqrt{r}}\leq\frac{1}{K \sqrt{r}}\.\qed\]