# On the Implicit Bias of Linear Equivariant Steerable Networks

 Ziyu Chen

Department of Mathematics and Statistics

University of Massachusetts Amherst

Amherst, MA 01003

ziyuchen@umass.edu

&Wei Zhu

Department of Mathematics and Statistics

University of Massachusetts Amherst

Amherst, MA 01003

weizhu@umass.edu

###### Abstract

We study the implicit bias of gradient flow on linear equivariant steerable networks in group-invariant binary classification. Our findings reveal that the parameterized predictor converges in direction to the unique group-invariant classifier with a maximum margin defined by the input group action. Under a unitary assumption on the input representation, we establish the equivalence between steerable networks and data augmentation. Furthermore, we demonstrate the improved margin and generalization bound of steerable networks over their non-invariant counterparts.

## 1 Introduction

Despite recent theoretical breakthroughs in deep learning, it is still largely unknown why overparameterized deep neural networks (DNNs) with infinitely many solutions achieving near-zero training error can effectively generalize on new data. However, the consistently impressive results of DNNs trained with first-order optimization methods, _e.g._, gradient descent (GD), suggest that the training algorithm is _implicitly guiding_ the model towards a solution with strong generalization performance.

Indeed, recent studies have shown that gradient-based training methods effectively regularize the solution by _implicitly minimizing_ a certain complexity measure of the model (Vardi, 2022). For example, Gunasekar et al. (2018) showed that in separable binary classification, the linear predictor parameterized by a linear fully-connected network trained under GD converges in the direction of a max-margin support vector machine (SVM), while linear convolutional networks are implicitly regularized by a depth-related bridge penalty in the Fourier domain. Yun et al. (2021) extended this finding to linear tensor networks. Lyu and Li (2020) and Ji and Telgarsky (2020) established the implicit max-margin regularization for (nonlinear) homogeneous DNNs in the parameter space.

On the other hand, another line of research aims to _explicitly regularize_ DNNs through architectural design to exploit the inherent structure of the learning problem. In recent years, there has been a growing interest in leveraging group symmetry for this purpose, given its prevalence in both scientific and engineering domains. A significant body of literature has been devoted to designing group-equivariant DNNs that ensure outputs transform covariantly to input symmetry transformations. _Group-equivariant steerable networks_ represent a general class of symmetry-preserving models that achieve equivariance with respect to any pair of input-output group actions (Cohen et al., 2019; Weiler and Cesa, 2019; Cohen and Welling, 2017). Empirical evidence suggests that equivariant steerable networks yield substantial improvements in generalization performance for learning tasks with group symmetry, especially when working with limited amounts of data.

There have been several recent attempts to account for the empirical success of equivariant steerable networks through establishing a tighter upper bound on the test risk for these models. This is typically accomplished by evaluating the complexity measures of equivariant and non-equivariant models under the same norm constraint on the network parameters (Sokolic et al., 2017; Sannaiet al., 2021; Elesedy, 2022). Nevertheless, it remains unclear whether or why a GD-trained steerable network can achieve a minimizer with a parameter norm comparable to that of its non-equivariant counterpart. Consequently, the effectiveness of such complexity-measure-based arguments to explain the generalization enhancement of steerable networks in group symmetric learning tasks may not be directly applicable.

In light of the above issues, in this work, we aim to fully characterize the implicit bias of the training algorithm on linear equivariant steerable networks in group-invariant binary classification. Our result shows that when trained under gradient flow (GF), _i.e._, GD with an infinitesimal step size, the steerable-network-parameterized predictor converges in direction to the unique group-invariant classifier attaining a maximum margin with respect to a norm defined by the input group representation. This result has three important implications: under a unitary input group action,

* a linear steerable network trained on the _original_ data set converge in the same direction as a linear fully-connected network trained on the _group-augmented_ data set. This suggests the equivalence between training with linear steerable networks and _data augmentation_;
* when trained on the same _original_ data set, a linear steerable network always attains a wider margin on the _group-augmented_ data set compared to a fully-connected network;
* when the underlying distribution is group-invariant, a GF-trained linear steerable network achieves a tighter generalization bound compared to its non-equivariant counterpart. This improvement in generalization is not necessarily dependent on the group size, but rather it depends on the support of the invariant distribution.

Before we end this section, we note that a similar topic has recently been explored by Lawrence et al. (2021) in the context of linear Group Convolutional Neural Networks (G-CNNs), a special case of the equivariant steerable networks considered in this work. However, we point out that the models they studied were not truly group-invariant, and thus their implicit bias result does not explain the improved generalization of G-CNNs. We will further elaborate on the comparison between our work and (Lawrence et al., 2021) in Section 2.

## 2 Related work

**Implicit biases**: Recent studies have shown that for linear regression with the logistic or exponential loss on linearly separable data, the linear predictor under GD/SGD converges in direction to the max-\(L^{2}\)-margin SVM (Soudry et al., 2018; Nacson et al., 2019; Gunasekar et al., 2018). These results are extended to linear fully-connected networks and linear Convolutional Neural Networks (CNNs) by Gunasekar et al. (2018) under the assumption of directional convergence and alignment of the network parameters, which are later proved by Ji and Telgarsky (2019, 2020); Lyu and Li (2020); Ji and Telgarsky (2020). The implicit regularization of gradient flow (GF) is further generalized to linear tensor networks by Yun et al. (2021). For overparameterized nonlinear networks in the infinite-width regime, rigorous analysis on the optimization of DNNs has also been studied from the neural tangent kernel (Jacot et al., 2018; Du et al., 2019; Allen-Zhu et al., 2019) and the mean-field perspectives (Mei et al., 2019; Chizat and Bach, 2018). However, these models are not explicitly designed to be group-invariant/equivariant, and the implicit bias of gradient-based methods might guide them to converge to sub-optimal solutions in learning tasks with intrinsic group symmetry.

**Equivariant neural networks.** Since their introduction by Cohen and Welling (2016), group equivariant network design has become a burgeoning field with numerous applications from computer vision to scientific computing. Unlike the implicit bias induced by training algorithms, equivariant networks _explicitly_ incorporate symmetry priors into model design through either group convolutions (Cheng et al., 2019; Weiler et al., 2018; Sosnovik et al., 2020; Zhu et al., 2022) or, more generally, steerable convolutions (Weiler and Cesa, 2019; Cohen et al., 2019; Worrall et al., 2017). Despite their empirical success, it remains largely unknown why and whether equivariant networks trained under gradient-based methods actually converge to solutions with smaller test risk in group-symmetric learning tasks (Sokolic et al., 2017; Sannai et al., 2021). In a recent study, Elesedy and Zaidi (2021) demonstrated a provably strict generalisation benefit for equivariant networks in linear regression. However, the network studied therein is non-equivariant and only symmetrized after training; such test-time data augmentation is different from practical usage of equivariant networks.

Comparison to Lawrence et al. (2021).A recent study by Lawrence et al. (2021) also analyzes the implicit bias of linear equivariant G-CNNs, which are a special case of the steerable networks considered in this work. However, the networks studied therein are not truly equivariant/invariant. More specifically, the input space \(\mathcal{X}_{0}\) they considered is the set of functions on the group \(G\), _i.e._, \(\mathcal{X}_{0}=\{f:G\to\mathbb{R}\}\), and the input \(G\)-action is given by the regular representation

\[[\rho_{0}(g)f](g^{\prime})=f(g^{-1}g^{\prime}),\quad\forall g\in G,\forall f \in\mathcal{X}_{0}.\] (1)

In their case, due to the transitivity of the regular representation (1), \(G\)-invariant linear functions \(\Phi:\mathcal{X}_{0}\to\mathbb{R}\) are constrained to the (trivial) form:

\[\Phi(f)=C\sum_{g\in G}f(g),\]

where \(C\) is a multiplicative constant. To avoid learning this trivial function, Lawrence et al. (2021) chose to parameterize \(\Phi\) using a linear G-CNN, in which the final layer is replaced by a fully-connected layer. While this fully-connected layer provides the capability to learn more complex and nontrivial functions, it simultaneously undermines the property of group invariance. Therefore, their implicit bias result does not explain the improved generalization of G-CNNs. In contrast, we assume Euclidean inputs with non-transitive group actions, allowing linear steerable networks to learn non-trivial group-invariant models.

## 3 Background and problem setup

We provide a brief background in group theory and group-equivariant steerable networks. We also explain the setup of our learning problem for group-invariant binary classification.

### Group and group equivariance

A _group_ is a set \(G\) equipped with a binary operator, the group product, satisfying the axioms of associativity, identity, and invertibility. We always assume in this work that \(G\) is finite, _i.e._, \(|G|<\infty\), where \(|G|\) denotes its cardinality.

Given a vector space \(\mathcal{X}\), let \(\text{GL}(\mathcal{X})\) be the general linear group of \(\mathcal{X}\) consisting of all invertible linear transformations on \(\mathcal{X}\). A map \(\rho:G\to\text{GL}(\mathcal{X})\) is called a _group representation_ (or _linear group action_) of \(G\) on \(\mathcal{X}\) if \(\rho\) is a group homomorphism from \(G\) to \(\text{GL}(\mathcal{X})\), namely

\[\rho(gh)=\rho(g)\rho(h)\in\text{GL}(\mathcal{X}),\quad\forall g,h\in G.\] (2)

When the representation \(\rho\) is clear from the context, we also abbreviate the group action \(\rho(g)\mathbf{x}\) as \(g\mathbf{x}\).

Given a pair of vector spaces \(\mathcal{X},\mathcal{Y}\) and their respective \(G\)-representations \(\rho_{\mathcal{X}}\) and \(\rho_{\mathcal{Y}}\), a linear map \(\Psi:\mathcal{X}\to\mathcal{Y}\) is said to be \(G\)-_equivariant_ if it commutes with the \(G\)-representations \(\rho_{\mathcal{X}}\) and \(\rho_{\mathcal{Y}}\), _i.e._,

\[\Psi\circ\rho_{\mathcal{X}}(g)=\rho_{\mathcal{Y}}(g)\circ\Psi,\quad\forall g\in G.\] (3)

Linear equivariant maps are also called _intertwiners_, and we denote by \(\text{Hom}_{G}(\rho_{\mathcal{X}},\rho_{\mathcal{Y}})\) the space of all intertwiners satisfying (3). When \(\rho_{\mathcal{Y}}\equiv\text{Id}\) is the trivial representation, then \(\Psi[\rho_{\mathcal{X}}(g)(\mathbf{x})]=\Psi[\mathbf{x}]\) for all \(\mathbf{x}\in\mathcal{X}\); namely, \(\Psi\) becomes a \(G\)-invariant linear map.

### Equivariant steerable networks and G-CNNs

Let \(\mathcal{X}_{0}=\mathbb{R}^{d_{0}}\) be a \(d_{0}\)-dimensional input Euclidean space, equipped with the usual inner product. Let \(\rho_{0}:G\to\text{GL}(\mathcal{X}_{0})\) be a \(G\)-representation on \(\mathcal{X}_{0}\). Suppose we have an unknown target function \(f^{*}:\mathcal{X}_{0}=\mathbb{R}^{d_{0}}\to\mathbb{R}\) that is \(G\)-invariant under \(\rho_{0}\), _i.e._, \(f^{*}(g\mathbf{x})\coloneqq f^{*}(\rho_{0}(g)\mathbf{x})=f^{*}(\mathbf{x})\) for all \(g\in G\) and \(\mathbf{x}\in\mathcal{X}_{0}\). The goal of equivariant steerable networks is to approximate \(f^{*}\) using an \(L\)-layer neural network \(f=\Psi_{L}\circ\Psi_{L-1}\circ\cdots\circ\Psi_{1}\) that is guaranteed to be also \(G\)-invariant.

Since the composition of equivariant maps is also equivariant, it suffices to specify a collection of \(G\)-representation spaces \(\{(\mathcal{X}_{l},\rho_{l})\}_{l=1}^{L}\), with \((\mathcal{X}_{L},\rho_{L})=(\mathbb{R},\text{Id})\) being the trivial representation, such that each layer \(\Psi_{l}\in\text{Hom}_{G}(\rho_{l-1},\rho_{L}):\mathcal{X}_{l-1}\to\mathcal{X} _{l}\) is \(G\)-equivariant. Equivalently, we want the following diagram to be commutative,

\[\begin{CD}\mathcal{X}_{0}@>{\Psi_{1}}>{}>\mathcal{X}_{1}@>{\Psi_{2}}>{}> \mathcal{X}_{2}@>{\Psi_{3}}>{}>\cdots @>{\Psi_{L-1}}>{}>\mathcal{X}_{L-1}@>{\Psi_{L}}>{}> \mathcal{X}_{L}\\ @V{\rho_{0}(g)}V{}V@V{\rho_{1}(g)}V{}V@V{\rho_{2}(g)}V{}V@V{\rho_{L}(g)}V{ \rho_{L}(g)}V@V{}V{}V@V{}V{\rho_{L}(g)}V\\ \mathcal{X}_{0}@>{\Psi_{1}}>{}>\mathcal{X}_{1}@>{\Psi_{2}}>{}>\mathcal{X}_{2}@>{ \Psi_{3}}>{}>\cdots @>{\Psi_{L-1}}>{}>\mathcal{X}_{L-1}@>{\Psi_{L}}>{}>\mathcal{X }_{L}\end{CD},\quad\forall g\in G.\] (4)

**Equivariant steerable networks.** Given the representations \(\{(\mathcal{X}_{l},\rho_{l})\}_{l=0}^{L}\), a (linear) _steerable network_ is constructed as follows. For each \(l\in[L]\coloneqq\{1,\cdots,L\}\), we choose a finite collection of \(N_{l}\) (pre-computed) intertwiners \(\{\psi_{l}^{j}\}_{j=1}^{N_{l}}\subset\mathrm{Hom}_{G}(\rho_{l-1},\rho_{l})\). Typically, \(\{\psi_{l}^{j}\}_{j=1}^{N_{l}}\) is a basis of \(\mathrm{Hom}_{G}(\rho_{l-1},\rho_{l})\), but it is not necessary in our setting. The \(l\)-th layer equivariant map \(\Psi_{l}^{\text{steer}}:\mathcal{X}_{l-1}\to\mathcal{X}_{l}\) of a steerable network is then parameterized by

\[\Psi_{l}^{\text{steer}}(\mathbf{x};\mathbf{w}_{l})=\sum_{j\in[N_{l}]}w_{l}^{j }\psi_{l}^{j}(\mathbf{x}),\quad\forall\mathbf{x}\in\mathcal{X}_{l-1},\] (5)

where the coefficients \(\mathbf{w}_{l}=[w_{l}^{j}]_{j\in[N_{l}]}^{\top}\in\mathbb{R}^{N_{l}}\) are the trainable parameters of the \(l\)-th layer. An \(L\)-layer linear steerable network \(f_{\text{steer}}(\mathbf{x};\mathbf{W})\) is then defined as the composition

\[f_{\text{steer}}(\mathbf{x};\mathbf{W})=\Psi_{L}^{\text{steer}}(\cdots\Psi_{ 2}^{\text{steer}}(\Psi_{1}^{\text{steer}}(\mathbf{x};\mathbf{w}_{1});\mathbf{ w}_{2})\cdots;\mathbf{w}_{L}),\] (6)

where \(\mathbf{W}=[\mathbf{w}_{l}]_{l=1}^{L}\in\prod_{l=1}^{L}\mathbb{R}^{N_{l}} \coloneqq\mathcal{W}_{\text{steer}}\) is the collection of all trainable parameters. The network \(f_{\text{steer}}(\mathbf{x};\mathbf{W})\) defined in (6) is referred to as a _steerable network_ because it steers the layer-wise output to transform according to any specified representation.

**G-CNNs.** A special case of the steerable networks is the _group convolutional neural network_ (G-CNN), wherein the hidden representation space \((\mathcal{X}_{l},\rho_{l})\) for each \(l\in[L-1]\) is set to

\[\mathcal{X}_{l}=(\mathbb{R}^{d_{l}})^{G}=\{\mathbf{x}_{l}:G\to\mathbb{R}^{d_{l }}\},\quad\rho_{l}(g)\mathbf{x}_{l}(h)\coloneqq\mathbf{x}_{l}(g^{-1}h)\in \mathbb{R}^{d_{1}},\forall g,h\in G.\] (7)

The representation \(\rho_{l}\in\text{GL}(\mathcal{X}_{l})\) in (7) is known as the _regular representation_ of \(G\). Intuitively, \(\mathbf{x}_{l}\in\mathcal{X}_{l}\) can be viewed as a matrix of size \(d_{l}\times|G|\), and \(\rho_{l}(g)\) is a permutation of the columns of \(\mathbf{x}_{l}\).

With this choice of \(\{(\mathcal{X}_{l},\rho_{l})\}_{l=0}^{L}\), the first-layer equivariant map of a G-CNN is defined as

\[\Psi_{1}^{\text{G-CNN}}(\mathbf{x};\mathbf{w}_{1})(g)=\mathbf{w}_{1}^{\top}g ^{-1}\mathbf{x}\in\mathbb{R}^{d_{1}},\quad\forall\mathbf{x}\in\mathbb{R}^{d_ {0}},\] (8)

where \(\mathbf{w}_{1}=(w_{1}^{j,k})_{j,k}\in\mathbb{R}^{d_{0}\times d_{1}}\) are the trainable parameters of the first layer. Eq. (8) is called a _\(G\)-lifting map_ as it lifts a Euclidean signal \(\mathbf{x}\in\mathbb{R}^{d_{0}}\) to a function \(\Psi_{1}^{\text{G-CNN}}(\mathbf{x};\mathbf{w}_{1})\) on \(G\). For the subsequent layers, equivariance is achieved through _group convolutions_, and the readers are referred to the appendix for a detailed explanation.

**Assumptions on the representations.** The input representation \(\rho_{0}\in\text{GL}(\mathcal{X}_{0}=\mathbb{R}^{d_{0}})\) is assumed to be given, and \(\rho_{L}\equiv 1\in\text{GL}(\mathcal{X}_{L}=\mathbb{R})\) is set to the trivial representation for group-invariant outputs. In this work, we make the following special choices for the first-layer representation \((\rho_{1},\mathcal{X}_{1})\) as well as the equivariant map \(\Psi_{1}^{\text{steer}}(\mathbf{x},\mathbf{w}_{1})\) in a general steerable network.

**Assumption 3.1**.: _We adopt the regular representation (7) for the first layer, and set the first-layer equivariant map \(\Psi_{1}^{\text{steer}}(\mathbf{x};\mathbf{w}_{1})\) to the \(G\)-lifting map (8)._

**Remark 3.2**.: _The rationale of Assumption 3.1 is for the network to have enough capacity to parameterize any \(G\)-invariant linear classifier; this will be further explained in Proposition 3.6. We note that the representations \((\rho_{l},\mathcal{X}_{l})\) and steerable maps \(\Psi_{l}^{\text{steer}}(\mathbf{x},\mathbf{w}_{l})\) for all subsequent layers, \(l\in\{2,\cdots,L-1\}\), can be arbitrary._

Under Assumption 3.1, we can characterize the linear steerable networks \(f_{\text{steer}}(\cdot;\mathbf{W})\) in the following proposition.

**Proposition 3.3**.: _Let \(f_{\text{steer}}(\mathbf{x};\mathbf{W})\) be the linear steerable network satisfying Assumption 3.1, where \(\mathbf{W}=[\mathbf{w}_{l}]_{l=1}^{L}\in\mathcal{W}_{\text{steer}}\) is the collection of all model parameters. There exists a multi-linear map \(M:(\mathbf{w}_{2},\cdots,\mathbf{w}_{L})\mapsto M(\mathbf{w}_{2},\cdots, \mathbf{w}_{L})\in\mathbb{R}^{d_{1}}\) such that for all \(\mathbf{x}\in\mathbb{R}^{d_{0}}\) and \(\mathbf{W}\in\mathcal{W}_{\text{steer}}\),_

\[f_{\text{steer}}(\mathbf{x};\mathbf{W})=f_{\text{steer}}(\overline{\mathbf{x}}; \mathbf{W})=\left\langle\overline{\mathbf{x}},\mathbf{w}_{1}M(\mathbf{w}_{2}, \cdots,\mathbf{w}_{L})\right\rangle,\] (9)

_where \(\overline{\mathbf{x}}\coloneqq\frac{1}{|G|}\sum_{g\in G}g\mathbf{x}\) is the average of all elements on the group orbit of \(\mathbf{x}\in\mathbb{R}^{d_{0}}\)._Although a straightforward proof of Proposition 3.3 can be readily derived using Schur's Lemma, we have opted to include an elementary proof of Proposition 3.3 in the appendix for the sake of completeness. If we define

\[\overline{\mathcal{P}}_{\text{steer}}(\mathbf{W})\coloneqq\mathbf{w}_{1}M( \mathbf{w}_{2},\cdots,\mathbf{w}_{L}),\quad\mathcal{P}_{\text{steer}}(\mathbf{ W})\coloneqq\left(\frac{1}{|G|}\sum_{g\in G}g^{\top}\right)\overline{\mathcal{P}}_{ \text{steer}}(\mathbf{W}),\] (10)

where \(g^{\top}\coloneqq\rho_{0}(g)^{\top}\), then

\[f_{\text{steer}}(\mathbf{x};\mathbf{W})=\left\langle\mathbf{x},\mathcal{P}_{ \text{steer}}(\mathbf{W})\right\rangle=\left\langle\overline{\mathbf{x}}, \overline{\mathcal{P}}_{\text{steer}}(\mathbf{W})\right\rangle.\] (11)

By the multi-linearity of \(M(\mathbf{w}_{2},\cdots,\mathbf{w}_{L})\), one can verify that \(f_{\text{steer}}(\mathbf{x};\mathbf{W})\), \(\mathcal{P}_{\text{steer}}(\mathbf{W})\), and \(\overline{\mathcal{P}}_{\text{steer}}(\mathbf{W})\) are all \(L\)-_homogeneous_ in \(\mathbf{W}\); that is, for all \(\nu>0\) and \(\mathbf{W}\in\mathcal{W}_{\text{steer}}\),

\[f_{\text{steer}}(\mathbf{x};\nu\mathbf{W})=\nu^{L}f_{\text{steer}}(\mathbf{x}; \mathbf{W}),\;\mathcal{P}_{\text{steer}}(\nu\mathbf{W})=\nu^{L}\mathcal{P}_{ \text{steer}}(\mathbf{W}),\;\overline{\mathcal{P}}_{\text{steer}}(\nu\mathbf{ W})=\nu^{L}\overline{\mathcal{P}}_{\text{steer}}(\mathbf{W}).\] (12)

**Remark 3.4**.: _In comparison, an \(L\)-layer linear fully-connected network \(f_{\text{fc}}(\mathbf{x};\mathbf{W})\) is given by_

\[f_{\text{fc}}(\mathbf{x};\mathbf{W})=\mathbf{w}_{L}^{\top}\mathbf{w}_{L-1}^{ \top}\cdots\mathbf{w}_{1}^{\top}\mathbf{x}=\left\langle\mathbf{x},\mathcal{P} _{\text{fc}}(\mathbf{W})\right\rangle,\quad\mathcal{P}_{\text{fc}}(\mathbf{W} )\coloneqq\mathbf{w}_{1}\cdots\mathbf{w}_{L}.\] (13)

_where \(\mathbf{W}=[\mathbf{w}_{l}]_{l=1}^{L}\in\mathcal{W}_{\text{fc}}\coloneqq\left( \prod_{l=1}^{L-1}\mathbb{R}^{d_{l-1}\times d_{l}}\right)\times\mathbb{R}^{d_{ L-1}}\). It is worth noting that when \(G=\{e\}\) is the trivial group, a linear fully-connected network \(f_{\text{fc}}(\mathbf{x};\mathbf{W})\) is identical with a linear G-CNN, which is a special case of linear steerable networks. See Remark A.1 for details._

### Group-invariant binary classification

Consider a binary classification data set \(S=\{(\mathbf{x}_{i},y_{i}):i\in[n]\}\), where \(\mathbf{x}_{i}\in\mathbb{R}^{d_{0}}\) and \(y_{i}\in\{\pm 1\},\forall i\in[n]\). We assume that \(S\) are i.i.d. samples from a \(G\)-_invariant distribution_\(\mathcal{D}\) defined below.

**Definition 3.5**.: _A distribution \(\mathcal{D}\) over \(\mathbb{R}^{d_{0}}\times\{\pm 1\}\) is said to be \(G\)-invariant with respect to a representation \(\rho_{0}\in\text{GL}(\mathbb{R}^{d_{0}})\) if_

\[\left(\rho_{0}(g)\otimes\text{Id}\right)_{*}\mathcal{D}=\mathcal{D},\;\forall g \in G,\] (14)

_where \(\left(\rho_{0}(g)\otimes\text{Id}\right)(\mathbf{x},y)\coloneqq\left(\rho_{0} (g)\mathbf{x},y\right)\), and \(\left(\rho_{0}(g)\otimes\text{Id}\right)_{*}\mathcal{D}\coloneqq\mathcal{D} \circ\left(\rho_{0}(g)\otimes\text{Id}\right)^{-1}\) is the push-forward measure of \(\mathcal{D}\) under \(\rho_{0}(g)\otimes\text{Id}\)._

It is easy to verify that the Bayes optimal classifier \(f^{*}:\mathbb{R}^{d_{0}}\to\{\pm 1\}\) (the one achieving the smallest population risk) for a \(G\)-invariant distribution \(\mathcal{D}\) is necessarily a \(G\)-invariant function, _i.e._, \(f^{*}(g\mathbf{x})=f^{*}(\mathbf{x}),\forall g\in G\). Therefore, to learn \(f^{*}\) using (linear) neural networks, it is natural to approximate \(f^{*}\) using an equivariant steerable network

\[f^{*}(\mathbf{x})\approx\text{sign}\left(f_{\text{steer}}(\mathbf{x};\mathbf{W} )\right)=\text{sign}\left(\left\langle\mathbf{x},\mathcal{P}_{\text{steer}}( \mathbf{W})\right\rangle\right).\] (15)

After choosing the exponential loss \(\ell_{\exp}:\mathbb{R}\times\{\pm 1\}\to\mathbb{R}_{+},\;\ell_{\exp}(\hat{y},y) \coloneqq\exp(-\hat{y}y)\), as a surrogate loss function, the empirical risk minimization over \(S\) for the steerable network \(f_{\text{steer}}(\mathbf{x};\mathbf{W})\) becomes

\[\min_{\mathbf{W}\in\mathcal{W}_{\text{steer}}}\mathcal{L}_{\mathcal{P}_{\text{ steer}}}(\mathbf{W};S)\coloneqq\sum_{i=1}^{n}\ell_{\exp}(\left\langle\mathbf{x}_{i}, \mathcal{P}_{\text{steer}}(\mathbf{W})\right\rangle,y_{i})=\sum_{i=1}^{n}\ell_ {\exp}(\left\langle\overline{\mathbf{x}}_{i},\overline{\mathcal{P}}_{\text{ steer}}(\mathbf{W})\right\rangle,y_{i}).\] (16)

On the other hand, since \(\mathcal{P}_{\text{steer}}(\mathbf{W})=\boldsymbol{\beta}\in\mathbb{R}^{d_{0}}\) always corresponds to a \(G\)-_invariant_ linear predictor--we have slightly abused the notation by identifying \(\boldsymbol{\beta}\in\mathbb{R}^{d_{0}}\) with the map \(\mathbf{x}\mapsto\left\langle\mathbf{x},\boldsymbol{\beta}\right\rangle\)--one can alternatively consider the empirical risk minimization directly over the invariant linear predictors \(\boldsymbol{\beta}\):

\[\min_{\boldsymbol{\beta}\in\mathbb{R}^{d_{0}}_{G}}\mathcal{L}(\boldsymbol{\beta}; S)\coloneqq\sum_{i=1}^{n}\ell_{\exp}(\left\langle\mathbf{x}_{i}, \boldsymbol{\beta}\right\rangle,y_{i}),\] (17)

where \(\mathbb{R}^{d_{0}}_{G}\subset\mathbb{R}^{d_{0}}\) is the subspace of all \(G\)-invariant linear predictors, which is characterized by the following proposition.

**Proposition 3.6**.: _Let \(\mathbb{R}^{d_{0}}_{G}\subset\mathbb{R}^{d_{0}}\) be the subspace of \(G\)-invariant linear predictors, i.e., \(\mathbb{R}^{d_{0}}_{G}=\left\{\boldsymbol{\beta}\in\mathbb{R}^{d_{0}}: \boldsymbol{\beta}^{\top}\mathbf{x}=\boldsymbol{\beta}^{\top}g\mathbf{x}, \forall\mathbf{x}\in\mathbb{R}^{d_{0}},\forall g\in G\right\}\). Then_1. \(\mathbb{R}_{G}^{d_{0}}\) _is characterized by_ \[\mathbb{R}_{G}^{d_{0}}=\bigcap_{g\in G}\ker(I-g^{\top})=\text{Range}\left(\frac{1 }{|G|}\sum_{g\in G}g^{\top}\right).\] (18)
2. _Let_ \(\mathcal{A}:\mathbb{R}^{d_{0}}\rightarrow\mathbb{R}^{d_{0}}\) _be the group-averaging map,_ \[\mathcal{A}(\bm{\beta})\coloneqq\overline{\bm{\beta}}=\frac{1}{|G|}\sum_{g\in G }g\bm{\beta}.\] (19) _Then its adjoint_ \(\mathcal{A}^{\top}:\bm{\beta}\mapsto\frac{1}{|G|}\sum_{g\in G}g^{\top}\bm{\beta}\) _is a projection operator from_ \(\mathbb{R}^{d_{0}}\) _to_ \(\mathbb{R}_{G}^{d_{0}}\)_. In other words,_ \(\text{Range}(\mathcal{A}^{\top})=\mathbb{R}_{G}^{d_{0}}\) _and_ \(\mathcal{A}^{\top}\circ\mathcal{A}^{\top}=\mathcal{A}^{\top}\)_._
3. _If_ \(G\) _acts unitarily on_ \(\mathcal{X}_{0}\)_, i.e.,_ \(\rho_{0}(g^{-1})=\rho_{0}(g)^{\top}\)_, then_ \(\mathcal{A}=\mathcal{A}^{\top}\) _is self-adjoint. This implies that_ \(\mathcal{A}:\bm{\beta}\mapsto\overline{\bm{\beta}}\) _is an orthogonal projection from_ \(\mathbb{R}^{d_{0}}\) _onto_ \(\mathbb{R}_{G}^{d_{0}}\)_. In particular, we have_ \[\overline{\bm{\beta}}=\bm{\beta}\iff\bm{\beta}\in\mathbb{R}_{G}^{d_{0}},\quad \text{and}\ \ \|\overline{\bm{\beta}}\|\leq\|\bm{\beta}\|,\forall\bm{\beta}\in\mathbb{R}^{d_{ 0}}.\] (20)

Proposition 3.6 combined with (10) demonstrates that a linear steerable network \(f_{\text{steer}}(\cdot;\mathbf{W})=\langle\cdot,\mathcal{P}_{\text{steer}}( \mathbf{W})\rangle\) can realize any \(G\)-invariant linear predictor \(\bm{\beta}\in\mathbb{R}_{G}^{d_{0}}\); that is, \(\{\mathcal{P}_{\text{steer}}(\mathbf{W}):\mathbf{W}\in\mathcal{W}_{\text{steer }}\}=\mathbb{R}_{G}^{d_{0}}\). Therefore (16) and (17) are equivalent optimization problems parameterized in different ways. However, minimizing (16) using gradient-based methods may potentially lead to different classifiers compared to those obtained from optimizing (17) directly.

**Gradient flow.** Given an initialization \(\mathbf{W}(0)\in\mathcal{W}_{\text{steer}}\), the gradient flow \(\{\mathbf{W}(t)\}_{t\geq 0}\) for (16) is the solution of the following ordinary differential equation (ODE),

\[\frac{\mathrm{d}\mathbf{W}}{\mathrm{d}t}=-\nabla_{\mathbf{W}}\mathcal{L}_{ \mathcal{P}_{\text{steer}}}(\mathbf{W};S)=-\nabla_{\mathbf{W}}\left[\sum_{i=1 }^{n}\ell_{\exp}(\left\langle\mathbf{x}_{i},\mathcal{P}_{\text{steer}}( \mathbf{W})\right\rangle,y_{i})\right].\] (21)

The purpose of this work is to inspect the asymptotic behavior of the \(G\)-invariant linear predictors \(\bm{\beta}_{\text{steer}}(t)=\mathcal{P}_{\text{steer}}(\mathbf{W}(t))\) parameterized by the linear steerable network trained under gradient flow (21). In particular, we aim to analyze the directional limit of \(\bm{\beta}_{\text{steer}}(t)\) as \(t\rightarrow\infty\), _i.e._, \(\lim_{t\rightarrow\infty}\frac{\bm{\beta}_{\text{steer}}(t)}{\|\bm{\beta}_{ \text{steer}}(t)\|}\). Before ending this section, we make the following assumption on the gradient flow \(\mathbf{W}(t)\) that also appears in the prior works Ji and Telgarsky (2020); Lyu and Li (2020); Yun et al. (2021).

**Assumption 3.7**.: _The gradient flow \(\mathbf{W}(t)\) satisfies \(\mathcal{L}_{\mathcal{P}_{\text{steer}}}(\mathbf{W}(t_{0});S)<1\) for some \(t_{0}>0\)._

This assumption implies that the data set \(S=\{(\mathbf{x}_{i},y_{i}):i\in[n]\}\) can be separated by a \(G\)-invariant linear predictor \(\mathcal{P}_{\text{steer}}(\mathbf{W}(t_{0}))\), and our analysis is focused on the "late phase" of the gradient flow training as \(t\rightarrow\infty\).

## 4 Implicit bias of linear steerable networks

Our main result on the implicit bias of gradient flow on linear steerable networks in binary classification is summarized in the following theorem.

**Theorem 4.1**.: _Under Assumption 3.1 and Assumption 3.7, let \(\bm{\beta}_{\text{steer}}(t)=\mathcal{P}_{\text{steer}}(\mathbf{W}(t))\) be the time-evolution of the \(G\)-invariant linear predictors parameterized by a linear steerable network trained with gradient flow on the data set \(S=\{(\mathbf{x}_{i},y_{i}):i\in[n]\}\); cf. Eq. (21). Then_

1. _The directional limit_ \(\bm{\beta}_{\text{steer}}^{\infty}=\lim_{t\rightarrow\infty}\frac{\bm{\beta}_{ \text{steer}}(t)}{\|\bm{\beta}_{\text{steer}}(t)\|}\) _exists and_ \(\bm{\beta}_{\text{steer}}^{\infty}\propto\frac{1}{|G|}\sum_{g\in G}g^{\top} \bm{\gamma}^{\star}\)_, where_ \(\bm{\gamma}^{\star}\) _is the max-_\(L^{2}\)_-margin SVM solution for the_ _transformed_ _data_ \(\overline{S}=\{(\overline{\mathbf{x}}_{i},y_{i}):i\in[n]\}\)_:_ \[\bm{\gamma}^{\star}=\arg\min_{\bm{\gamma}\in\mathbb{R}^{d_{0}}}\|\bm{\gamma} \|^{2},\quad\text{s.t.}\ y_{i}\left\langle\overline{\mathbf{x}}_{i},\bm{ \gamma}\right\rangle\geq 1,\forall i\in[n].\] (22) _Furthermore, if_ \(G\) _acts unitarily on the input space_ \(\mathcal{X}_{0}\)_, i.e.,_ \(g^{-1}=g^{\top}\)_, then_ \(\bm{\beta}_{\text{steer}}^{\infty}\propto\bm{\gamma}^{\star}\)_._2. Equivalently,_ \(\bm{\beta}^{\infty}_{\text{steer}}\) _is proportional to the unique minimizer_ \(\bm{\beta}^{*}\) _of the problem_ \[\bm{\beta}^{*}=\arg\min_{\bm{\beta}\in\mathbb{R}^{d_{0}}}\|\text{Proj}_{\text{ Range}(\mathcal{A})}\bm{\beta}\|^{2},\quad\text{s.t.}\ \bm{\beta}\in\mathbb{R}^{d_{0}}_{G},\text{ and }y_{i} \left\langle\mathbf{x}_{i},\bm{\beta}\right\rangle\geq 1,\forall i\in[n],\] (23) _where_ \(\text{Proj}_{\text{Range}(\mathcal{A})}\) _is the projection from_ \(\mathbb{R}^{d_{0}}\) _to_ \(\text{Range}(\frac{1}{|G|}\sum_{g\in G}g\right)\)_. Moreover, if_ \(G\) _acts unitarily on_ \(\mathcal{X}_{0}\)_, then_ \[\bm{\beta}^{\infty}_{\text{steer}}\propto\bm{\beta}^{*}=\arg\min_{\bm{\beta} \in\mathbb{R}^{d_{0}}}\|\bm{\beta}\|^{2},\quad\text{s.t.}\ \bm{\beta}\in\mathbb{R}^{d_{0}}_{G},\text{ and }y_{i} \left\langle\mathbf{x}_{i},\bm{\beta}\right\rangle\geq 1,\forall i\in[n].\] (24) _Namely,_ \(\bm{\beta}^{\infty}_{\text{steer}}\) _achieves the maximum_ \(L^{2}\)_-margin among all_ \(G\)_-invariant linear predictors._

Theorem 4.1 suggests that gradient flow implicitly guides a linear steerable network toward the unique \(G\)-invariant classifier with a maximum margin defined by the input representation \(\rho_{0}\).

**Remark 4.2**.: _According to Remark 3.4, when \(G=\{e\}\) is a trivial group, then a linear \(G\)-CNN (which is a special case of linear steerable networks) reduces to a fully-connected network \(f_{\mathrm{fc}}(\mathbf{x};\mathbf{W})\). Since the representation of a trivial group is always unitary, we have the following corollary which also appeared in Ji and Telgarsky (2020) and Yun et al. (2021)._

**Corollary 4.3**.: _Let \(\{\mathbf{W}(t)\}_{t\geq 0}\subset\mathcal{W}_{\mathrm{fc}}\) be the gradient flow of the parameters when training a linear fully-connected network on the data set \(S=\{(\mathbf{x}_{i},y_{i}),i\in[n]\}\), i.e.,_

\[\frac{\mathrm{d}\mathbf{W}}{\mathrm{d}t}=-\nabla_{\mathbf{W}}\mathcal{L}_{ \mathcal{P}_{\mathrm{fc}}}(\mathbf{W};S)\coloneqq-\nabla_{\mathbf{W}}\left[ \sum_{i=1}^{n}\ell_{\mathrm{exp}}(\left\langle\mathbf{x}_{i},\mathcal{P}_{ \mathrm{fc}}(\mathbf{W})\right\rangle,y_{i})\right].\] (25)

_Then the classifier \(\bm{\beta}_{\mathrm{fc}}(t)=\mathcal{P}_{\mathrm{fc}}(\mathbf{W}(t))\) converges in a direction that aligns with the max-\(L^{2}\)-margin SVM solution \(\bm{\gamma}^{*}\) for the **original** data set \(S=\{(\mathbf{x}_{i},y_{i}):i\in[n]\}\),_

\[\bm{\gamma}^{*}=\arg\min_{\bm{\gamma}\in\mathbb{R}^{d_{0}}}\|\bm{\gamma}\|^{2},\quad\text{s.t.}\ y_{i}\left\langle\mathbf{x}_{i},\bm{\gamma}\right\rangle \geq 1,\forall i\in[n].\] (26)

**Remark 4.4**.: _While Theorem 4.1 provides a complete characterization of the implicit bias exhibited by gradient flow in linear steerable networks, it is imperative to note that the convergence rate to the directional limit is, in fact, exponentially slow. This is consistent with the findings in, e.g., (Soudry et al., 2018; Yun et al., 2021). A comprehensive analysis of gradient flow behavior in a non-asymptotic regime falls outside the scope of this current study._

## 5 The equivalence between steerable networks and data augmentation

Compared to hard-wiring symmetry priors into model architectures through equivariant steerable networks, an alternative approach to incorporate symmetry into the learning process is by training a non-equivariant model with the aid of _data augmentation_. In this section, we demonstrate that these two approaches are equivalent for binary classification under a unitary assumption for \(\rho_{0}\).

**Corollary 5.1**.: _Let \(\bm{\beta}^{\infty}_{\text{steer}}=\lim_{t\to\infty}\frac{\bm{\beta}_{\text{ steer}}(t)}{\|\bm{\beta}_{\text{steer}}(t)\|}\) be the directional limit of the linear predictor \(\bm{\beta}_{\text{steer}}(t)=\mathcal{P}_{\text{steer}}(\mathbf{W}(t))\) parameterized by a linear steerable network trained using gradient flow on the **original** data set \(S=\{(\mathbf{x}_{i},y_{i}),i\in[n]\}\). Correspondingly, let \(\bm{\beta}^{\infty}_{\mathrm{fc}}=\lim_{t\to\infty}\frac{\bm{\beta}_{\mathrm{ fc}}(t)}{\|\bm{\beta}_{\mathrm{fc}}(t)\|}\), \(\bm{\beta}_{\mathrm{fc}}(t)=\mathcal{P}_{\mathrm{fc}}(\mathbf{W}(t))\) (13), be that of a linear fully-connected network trained on the **augmented** data set \(S_{\text{aug}}=\{(g\mathbf{x}_{i},y_{i}),i\in[n],g\in G\}\). If \(G\) acts unitarily on \(\mathcal{X}_{0}\), then_

\[\bm{\beta}^{\infty}_{\text{steer}}=\bm{\beta}^{\infty}_{\text{fc}}.\] (27)

_In other words, the effect of using a linear steerable network for group-invariant binary classification is exactly the same as conducting **data-augmentation** for non-invariant models._

**Remark 5.2**.: _The equivalence between data augmentation and training with a linear steerable network is valid only in an asymptotic sense, yet the underlying training dynamics differ substantially. Specifically, \(\bm{\beta}_{\text{steer}}(t)\) is assured to maintain \(G\)-invariance throughout the training process, whereas \(\bm{\beta}_{\text{fc}}(t)\) achieves \(G\)-invariance solely in the limiting case as \(t\to\infty\). Moreover, the equivalence only holds for "full-batch" data-augmentation over the entire group orbits._

**Remark 5.3**.: _For Corollary 5.1 to hold, it is crucial that \(\rho_{0}\in\mathrm{GL}(\mathcal{X}_{0})\) is unitary, as otherwise the limit direction \(\bm{\beta}_{\mathrm{fc}}^{\infty}\) of a linear fully-connected network trained on the augmented data set is generally not \(G\)-invariant (and hence cannot be equal to \(\bm{\beta}_{\mathrm{scer}}^{\infty}\in\mathbb{R}_{G}^{d_{0}}\)); see Example 5.4._

**Example 5.4**.: Let \(G=\mathbb{Z}_{2}=\{\overline{0},\overline{1}\}\). Consider a (non-unitary) \(G\)-representation \(\rho_{0}\) on \(\mathcal{X}_{0}=\mathbb{R}^{2}\),

\[\rho_{0}\left(\overline{0}\right)=\begin{bmatrix}1&0\\ 0&1\end{bmatrix},\quad\rho_{0}\left(\overline{1}\right)=\begin{bmatrix}1&0\\ -1&1\end{bmatrix}\begin{bmatrix}-1&0\\ 0&1\end{bmatrix}\begin{bmatrix}1&0\\ 1&1\end{bmatrix}=\begin{bmatrix}-1&0\\ 2&1\end{bmatrix}.\] (28)

Let \(S=\{(\mathbf{x},y)\}=\{((1,2)^{\top},+1)\}\) be a training set with only one point. By Theorem 4.1, the limit direction \(\bm{\beta}_{\mathrm{steer}}^{\infty}\) of a linear-steerable-network-parameterized linear predictor satisfies

\[\bm{\beta}_{\mathrm{steer}}^{\infty}\propto\frac{1}{2}\sum_{g\in G}g^{\top} \bm{\gamma}^{*}=\begin{bmatrix}0&1\\ 0&1\end{bmatrix}\bm{\gamma}^{*}=\begin{bmatrix}0&1\\ 0&1\end{bmatrix}\begin{bmatrix}0\\ \frac{1}{3}\end{bmatrix}=\begin{bmatrix}\frac{1}{3}\\ \frac{1}{3}\end{bmatrix},\] (29)

where \(\bm{\gamma}^{*}=(0,\frac{1}{3})^{\top}\) is the max-margin SVM solution for the transformed data set \(\overline{S}=\{(\overline{\mathbf{x}},y)\}=\{((0,3)^{\top},+1)\}\). On the contrary, by Corollary 4.3, the limit direction \(\bm{\beta}_{\mathrm{fc}}^{\infty}\) of a linear fully-connected network trained on the augmented data set \(S_{\text{aug}}=\{((1,2)^{\top},+1),((-1,4)^{\top},+1)\}\) aligns with the max-margin SVM solution \(\bm{\gamma}_{\text{aug}}^{*}\) for \(S_{\text{aug}}\),

\[\bm{\beta}_{\mathrm{fc}}^{\infty}\propto\bm{\gamma}_{\text{aug }}^{*} =\arg\min_{\bm{\gamma}\in\mathbb{R}^{2}}\|\bm{\gamma}\|^{2},\quad \text{s.t.}\;y\left\langle\mathbf{x},\bm{\gamma}\right\rangle\geq 1,\forall( \mathbf{x},y)\in S_{\text{aug}}\] (30) \[=(0.2,0.4)^{\top}\not\propto\bm{\beta}_{\text{steer}}^{\infty}.\] (31)

However, we demonstrate below that the equivalence between linear steerable networks and data augmentation can be re-established for non-unitary \(\rho_{0}\) by defining a new inner product on \(\mathbb{R}^{d_{0}}\),

\[\left\langle\mathbf{v},\mathbf{w}\right\rangle_{\rho_{0}}\coloneqq\mathbf{v}^{ \top}\left(\frac{1}{|G|}\sum_{g\in G}\rho_{0}(g)^{\top}\rho_{0}(g)\right)^{1/2} \mathbf{w},\quad\forall\mathbf{v},\mathbf{w}\in\mathcal{X}_{0}=\mathbb{R}^{d_ {0}}.\] (32)

When \(\rho_{0}\) is unitary, \(\left\langle\mathbf{v},\mathbf{w}\right\rangle_{\rho_{0}}=\left\langle\mathbf{ v},\mathbf{w}\right\rangle\) is the normal Euclidean inner product. With this new inner product, we modify the linear fully-connected network and its empirical loss on the augmented data set \(S_{\text{aug}}=\{(g\mathbf{x}_{i},y_{i}):g\in G,i\in[n]\}\) as

\[f_{\mathrm{fc}}^{\rho_{0}}(\mathbf{x};\mathbf{W})\coloneqq \left\langle\mathbf{x},\mathcal{P}_{\mathrm{fc}}(\mathbf{W})\right\rangle_{ \rho_{0}},\] (33) \[\mathcal{L}_{\mathcal{P}_{\mathrm{fc}}}^{\rho_{0}}(\mathbf{W};S_{ \text{aug}})\coloneqq\sum_{g\in G}\sum_{i=1}^{n}\ell_{\exp}\left(f_{\mathrm{fc }}^{\rho_{0}}(g\mathbf{x}_{i};\mathbf{W}),y_{i}\right)=\sum_{g\in G}\sum_{i=1} ^{n}\ell_{\exp}\left(\left\langle g\mathbf{x}_{i},\mathcal{P}_{\mathrm{fc}}( \mathbf{W})\right\rangle_{\rho_{0}},y_{i}\right).\] (34)

The following corollary shows that, for non-unitary \(\rho_{0}\) on \(\mathcal{X}_{0}\), the implicit bias of a linear steerable network is again the same as that of a modified linear fully-connected network \(f_{\mathrm{fc}}^{\rho_{0}}(\mathbf{x};\mathbf{W})\) trained under data augmentation.

**Corollary 5.5**.: _Let \(\bm{\beta}_{\mathrm{steer}}^{\infty}\) be the same as that in Corollary 5.1. Let \(\bm{\beta}_{\mathrm{fc}}^{\rho_{0},\infty}=\lim_{t\to\infty}\frac{\bm{\beta}_{ \mathrm{fc}}^{\rho_{0}}(t)}{|\bm{\beta}_{\mathrm{fc}}^{\rho_{0}}(t)|}\) be the limit direction of \(\bm{\beta}_{\mathrm{fc}}^{\rho_{0}}(t)=\mathcal{P}_{\mathrm{fc}}(\mathbf{W}(t))\) under the gradient flow of the modified empirical loss \(\mathcal{L}_{\mathcal{P}_{\mathrm{fc}}}^{\rho_{0}}(\mathbf{W};S_{\text{aug}})\) (34) for a linear fully-connected network on the augmented data set \(S_{\text{aug}}\). Then_

\[\bm{\beta}_{\mathrm{steer}}^{\infty}\propto\left(\frac{1}{|G|}\sum_{g\in G} \rho_{0}(g)^{\top}\rho_{0}(g)\right)^{1/2}\bm{\beta}_{\mathrm{fc}}^{\rho_{0}, \infty}.\] (35)

_Consequently, we have \(\left\langle\mathbf{x},\bm{\beta}_{\mathrm{steer}}^{\infty}\right\rangle\propto \left\langle\mathbf{x},\bm{\beta}_{\mathrm{fc}}^{\rho_{0},\infty}\right\rangle_{ \rho_{0}}\) for all \(\mathbf{x}\in\mathbb{R}^{d_{0}}\)._

## 6 Improved margin and generalization

We demonstrate in this section the improved margin and generalization of linear steerable networks over their non-invariant counterparts. In what follows, we assume \(\rho_{0}\) to be unitary.

The following theorem shows that the margin of a linear-steerable-network-parameterized predictor \(\bm{\beta}_{\mathrm{steer}}^{\infty}\) on the augmented data set \(S_{\text{aug}}\) is always larger than that of a linear fully-connected network \(\bm{\beta}_{\mathrm{fc}}^{\infty}\), suggesting improved \(L^{2}\)-robustness of the steerable-network-parameterized classifier.

**Theorem 6.1**.: _Let \(\bm{\beta}_{\rm{steer}}^{\infty}\) be the directional limit of a linear-steerable-network-parameterized predictor trained on the **original** data set \(S=\{(\bm{\mathbf{x}}_{i},y_{i}),i\in[n]\}\); let \(\bm{\beta}_{\rm{fc}}^{\infty}\) be that of a linear fully-connected network **also** trained on the same data set \(S\). Let \(M_{\rm{steer}}\) and \(M_{\rm{fc}}\), respectively, be the (signed) margin of \(\bm{\beta}_{\rm{steer}}^{\infty}\) and \(\bm{\beta}_{\rm{fc}}^{\infty}\) on the **augmented** data set \(S_{\rm{aug}}=\{(g\mathbf{x}_{i},y_{i}):i\in[n],g\in G\}\), i.e.,_

\[M_{\rm{steer}}\coloneqq\min_{i\in[n],g\in G}y_{i}\left\langle\bm{\beta}_{\rm{ steer}}^{\infty},g\mathbf{x}_{i}\right\rangle,\quad M_{\rm{fc}}\coloneqq\min_{i\in[n],g\in G}y_{i}\left\langle\bm{\beta}_{\rm{fc}}^{\infty},g\mathbf{x}_{i}\right\rangle.\] (36)

_Then we always have \(M_{\rm{steer}}\geq M_{\rm{fc}}\)._

Finally, we aim to quantify the improved generalization of linear steerable networks compared to fully-connected networks in binary classification of _linearly separable_ group-invariant distributions defined below.

**Definition 6.2**.: _A distribution \(\mathcal{D}\) on \(\mathbb{R}^{d_{0}}\times\{\pm 1\}\) is called linearly separable if there exists \(\bm{\beta}\in\mathbb{R}^{d_{0}}\) such that_

\[\mathbb{P}_{(\mathbf{x},y)\sim\mathcal{D}}\left[y\left\langle \mathbf{x},\bm{\beta}\right\rangle\geq 1\right]=1.\] (37)

It is easy to verify (by Lemma F.1) that if \(\mathcal{D}\) is \(G\)-invariant and linearly separable, then \(\mathcal{D}\) can be separated by a \(G\)-invariant linear classifier. The following theorem establishes the generalization bound of linear steerable networks in separable group-invariant binary classification.

**Theorem 6.3**.: _Let \(\mathcal{D}\) be a \(G\)-invariant distribution over \(\mathbb{R}^{d_{0}}\times\{\pm 1\}\) that is linearly separable by an invariant classifier \(\bm{\beta}_{0}\in\mathbb{R}^{d_{0}}_{G}\). Define_

\[\overline{R}=\inf\left\{r>0:\|\overline{\mathbf{x}}\|\leq r\text{ with probability }1\right\}.\] (38)

_Let \(S=\left\{(\mathbf{x}_{i},y_{i})\right\}_{i=1}^{n}\) be i.i.d. samples from \(\mathcal{D}\), and let \(\bm{\beta}_{\rm{steer}}^{\infty}\) be the limit direction of a steerable-network-parameterized linear predictor trained using gradient flow on \(S\). Then, for any \(\delta>0\), we have with probability at least \(1-\delta\) (over random samples \(S\sim\mathcal{D}^{n}\)) that_

\[\mathbb{P}_{(\mathbf{x},y)\sim\mathcal{D}}\left[y\neq\operatorname{sign} \left(\langle\mathbf{x},\bm{\beta}_{\rm{steer}}^{\infty}\rangle\right)\right] \leq\frac{2\overline{R}\|\bm{\beta}_{0}\|}{\sqrt{n}}+\sqrt{\frac{\log(1/ \delta)}{2n}}.\] (39)

**Remark 6.4**.: _In comparison, let \(\bm{\beta}_{\rm{fc}}^{\infty}\) be the limit direction of a fully-connected-network-parameterized linear predictor trained on \(S\). Then with probability at least \(1-\delta\), we have_

\[\mathbb{P}_{(\mathbf{x},y)\sim\mathcal{D}}\left[y\neq\operatorname{sign} \left(\langle\mathbf{x},\bm{\beta}_{\rm{fc}}^{\infty}\rangle\right)\right] \leq\frac{2R\|\bm{\beta}_{0}\|}{\sqrt{n}}+\sqrt{\frac{\log(1/\delta)}{2n}},\] (40)

_where \(R=\inf\left\{r>0:\|\mathbf{x}\|\leq r\text{ with probability }1\right\}\). This is the classical generalization result for max-margin SVM (see, e.g., Shalev-Shwartz and Ben-David [2014].) Eq. (40) can also be viewed as a special case of Eq. (39), as a fully-connected network is a G-CNN with \(G=\{e\}\) (cf. Remark 4.2), and therefore \(\overline{\mathbf{x}}=\mathbf{x}\) and \(R=\overline{R}\)._

_By Proposition 3.6, the map \(\mathbf{x}\to\overline{\mathbf{x}}\) is an orthogonal projection, and thus we always have \(\overline{R}\leq R\). Therefore the generalization bound for steerable networks in (39) is always smaller than that of the fully-connected network in (40)._

**Remark 6.5**.: _A comparison between Eq. (39) and Eq. (40) reveals that the improved generalization of linear steerable network does not necessarily depend on the group size \(|G|\). Instead, it depends on how far the distribution \(\mathcal{D}\)'s support is from the subspace \(\mathbb{R}^{d_{0}}_{G}\), such that \(\overline{R}\) could be much smaller than \(R\). In fact, if the support of \(\mathcal{D}\) is contained in \(R^{d_{0}}_{G}\), then \(\overline{R}=R\), and the steerable network does not achieve any generalization gain. This is consistent with Theorem 4.1, as in this case, the transformed data set \(\overline{S}=\{(\overline{\mathbf{x}}_{i},y_{i}):i\in[n]\}\) is the same as the original data set \(S\)._

## 7 Conclusion and future work

In this work, we analyzed the implicit bias of gradient flow on general linear group-equivariant steerable networks in group-invariant binary classification. Our findings indicate that the parameterized predictor converges in a direction that aligns with the unique group-invariant classifier with a maximum margin that is dependent on the input representation. As a corollary of our main result,we established the equivalence between data augmentation and learning with steerable networks in our setting. Finally, we demonstrated that linear steerable networks outperform their non-invariant counterparts in terms of improved margin and generalization bound.

A limitation of our result is that the implicit bias of gradient flow studied herein holds in an asymptotic sense, and the convergence rate to the directional limit might be extremely slow. This is consistent with the findings in, _e.g._, (Soudry et al., 2018; Yun et al., 2021). Understanding the behavior of gradient flow in a non-asymptotic regime is an important direction for future work. Furthermore, in our current framework, we assume that the first-layer equivariant map is represented by the \(G\)-lifting map. This assumption ensures that the linear steerable network possesses sufficient capacity to parameterize all \(G\)-invariant linear classifiers. Exploring the implicit bias of steerable networks without this assumption would be a compelling next step. The removal of this constraint could facilitate the generalization of our findings from finite groups to compact groups.

## Acknowledgements

The research was partially supported by NSF under DMS-2052525, DMS-2140982, and DMS-2244976.

## References

* Allen-Zhu et al. (2019) Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization. In _International Conference on Machine Learning_, pages 242-252. PMLR, 2019.
* Cheng et al. (2019) X. Cheng, Q. Qiu, R. Calderbank, and G. Sapiro. RotDCF: Decomposition of convolutional filters for rotation-equivariant deep networks. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=H1gTEj09FX.
* Chizat and Bach (2018) L. Chizat and F. Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. _Advances in neural information processing systems_, 31, 2018.
* Cohen and Welling (2016) T. Cohen and M. Welling. Group equivariant convolutional networks. In M. F. Balcan and K. Q. Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 2990-2999, New York, New York, USA, 20-22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/cohenc16.html.
* Cohen and Welling (2017) T. S. Cohen and M. Welling. Steerable CNNs. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=rJQKYt51l.
* Cohen et al. (2019) T. S. Cohen, M. Geiger, and M. Weiler. A general theory of equivariant CNNs on homogeneous spaces. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/b9cfe8b6042cf759dc4c0cccb27a6737-Paper.pdf.
* Du et al. (2019) S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pages 1675-1685. PMLR, 2019.
* Elesedy (2022) B. Elesedy. Group symmetry in pac learning. In _ICLR 2022 Workshop on Geometrical and Topological Representation Learning_, 2022.
* Elesedy and Zaidi (2021) B. Elesedy and S. Zaidi. Provably strict generalisation benefit for equivariant models. In _International Conference on Machine Learning_, pages 2959-2969. PMLR, 2021.
* Gunasekar et al. (2018a) S. Gunasekar, J. Lee, D. Soudry, and N. Srebro. Characterizing implicit bias in terms of optimization geometry. In _International Conference on Machine Learning_, pages 1832-1841. PMLR, 2018a.
* Gunasekar et al. (2018b) S. Gunasekar, J. D. Lee, D. Soudry, and N. Srebro. Implicit bias of gradient descent on linear convolutional networks. _Advances in neural information processing systems_, 31, 2018b.
* Jacot et al. (2018) A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* Jader et al. (2018)Z. Ji and M. Telgarsky. Gradient descent aligns the layers of deep linear networks. In _International Conference on Learning Representations_, 2019a. URL https://openreview.net/forum?id=HJflg30qKX.
* Ji and Telgarsky [2019b] Z. Ji and M. Telgarsky. The implicit bias of gradient descent on nonseparable data. In _Conference on Learning Theory_, pages 1772-1798. PMLR, 2019b.
* Ji and Telgarsky [2020] Z. Ji and M. Telgarsky. Directional convergence and alignment in deep learning. _Advances in Neural Information Processing Systems_, 33:17176-17186, 2020.
* Lawrence et al. [2021] H. Lawrence, K. Georgiev, A. Dienes, and B. T. Kiani. Implicit bias of linear equivariant networks. _arXiv preprint arXiv:2110.06084_, 2021.
* Lyu and Li [2020] K. Lyu and J. Li. Gradient descent maximizes the margin of homogeneous neural networks. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=SJeLIgBKPS.
* Mei et al. [2019] S. Mei, T. Misiakiewicz, and A. Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In _Conference on Learning Theory_, pages 2388-2464. PMLR, 2019.
* Mohri and Medina [2014] M. Mohri and A. M. Medina. Learning theory and algorithms for revenue optimization in second price auctions with reserve. In _International conference on machine learning_, pages 262-270. PMLR, 2014.
* Nacson et al. [2019] M. S. Nacson, N. Srebro, and D. Soudry. Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 3051-3059. PMLR, 2019.
* Sannai et al. [2021] A. Sannai, M. Imaizumi, and M. Kawano. Improved generalization bounds of group invariant/equivariant deep networks via quotient feature spaces. In _Uncertainty in Artificial Intelligence_, pages 771-780. PMLR, 2021.
* Shalev-Shwartz and Ben-David [2014] S. Shalev-Shwartz and S. Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* Sokolic et al. [2017] J. Sokolic, R. Giryes, G. Sapiro, and M. Rodrigues. Generalization error of invariant classifiers. In _Artificial Intelligence and Statistics_, pages 1094-1103. PMLR, 2017.
* Sosnovik et al. [2020] I. Sosnovik, M. Szmaja, and A. Smeulders. Scale-equivariant steerable networks. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=HJgpugrKPS.
* Soudry et al. [2018] D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent on separable data. _The Journal of Machine Learning Research_, 19(1):2822-2878, 2018.
* Vardi [2022] G. Vardi. On the implicit bias in deep-learning algorithms. _arXiv preprint arXiv:2208.12591_, 2022.
* Weiler and Cesa [2019] M. Weiler and G. Cesa. General E(2)-equivariant steerable CNNs. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/45d6637b718d0f24a237069fe41b0db4-Paper.pdf.
* Weiler et al. [2018] M. Weiler, F. A. Hamprecht, and M. Storath. Learning steerable filters for rotation equivariant cnns. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 849-858, 2018.
* Worrall et al. [2017] D. E. Worrall, S. J. Garbin, D. Turmukhambetov, and G. J. Brostow. Harmonic networks: Deep translation and rotation equivariance. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5028-5037, 2017.
* Yun et al. [2021] C. Yun, S. Krishnan, and H. Mobahi. A unifying view on implicit bias in training linear neural networks. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=ZsZM-4iMQkH.
* Zhu et al. [2022] W. Zhu, Q. Qiu, R. Calderbank, G. Sapiro, and X. Cheng. Scaling-translation-equivariant networks with decomposed convolutional filters. _Journal of machine learning research_, 23(68):1-45, 2022.

G-CNNs

A special case of the steerable networks is the _group convolutional neural network_ (G-CNN), wherein equivariance is achieved through _group convolutions_. More specifically, for each \(l\in[L-1]\), the hidden representation space \((\mathcal{X}_{l},\rho_{l})\) is set to

\[\mathcal{X}_{l}=(\mathbb{R}^{d_{l}})^{G}=\{\mathbf{x}_{l}:G\to \mathbb{R}^{d_{l}}\},\quad\rho_{l}(g)\mathbf{x}_{l}(h)\coloneqq\mathbf{x}_{l}(g ^{-1}h)\in\mathbb{R}^{d_{1}},\forall g,h\in G.\] (41)

The representation \(\rho_{l}\in\text{GL}(\mathcal{X}_{l})\) in (41) is known as the _regular representation_ of \(G\). Intuitively, \(\mathbf{x}_{l}\in\mathcal{X}_{l}\) can be viewed as a matrix of size \(d_{l}\times|G|\), and \(\rho_{l}(g)\) is a permutation of the columns of \(\mathbf{x}_{l}\). With this choice of \(\{(\mathcal{X}_{l},\rho_{l})\}_{l=0}^{L}\), a (linear) G-CNN is built as follows.

First layer: the first-layer equivariant map \(\Psi_{1}^{\text{G-CNN}}:\mathcal{X}_{0}\to\mathcal{X}_{1}\) of a G-CNN is defined as

\[\Psi_{1}^{\text{G-CNN}}(\mathbf{x};\mathbf{w}_{1})(g)=\mathbf{w}_{1}^{\top}g ^{-1}\mathbf{x}\in\mathbb{R}^{d_{1}},\quad\forall\mathbf{x}\in\mathbb{R}^{d_{ 0}},\] (42)

where \(\mathbf{w}_{1}=(w_{1}^{j,k})_{j,k}\in\mathbb{R}^{d_{0}\times d_{1}}\) are the trainable parameters of the first layer. Eq. (42) is called a _G-lifting map_ as it lifts a Euclidean signal \(\mathbf{x}\in\mathbb{R}^{d_{0}}\) to a function \(\Psi_{1}^{\text{G-CNN}}(\mathbf{x};\mathbf{w}_{1})\) on \(G\).

Hidden layers: For \(l\in\{2,\cdots,L-1\}\), define \(\Psi_{l}^{\text{G-CNN}}:\mathcal{X}_{l-1}\to\mathcal{X}_{l}\) as the _group convolutions_,

\[\Psi_{l}^{\text{G-CNN}}(\mathbf{x};\mathbf{w}_{l})(g)=\sum_{h\in G }\mathbf{w}_{l}^{\top}(h^{-1}g)\mathbf{x}(h)\in\mathbb{R}^{d_{l}},\quad \forall\mathbf{x}\in\mathcal{X}_{l-1},\] (43)

where \(\mathbf{w}_{l}(g)\in\mathbb{R}^{d_{l-1}\times d_{l}},\forall g\in G\). Equivalently, \(\mathbf{w}_{l}\in\mathbb{R}^{d_{l-1}\times d_{l}\times|G|}\) can be viewed as a 3D tensor.

Last layer: Define \(\Psi_{L}^{\text{G-CNN}}:\mathcal{X}_{L-1}\to\mathcal{X}_{L}\) as the _group-pooling_ followed by a fully-connected layer,

\[\Psi_{L}^{\text{G-CNN}}(\mathbf{x};\mathbf{w}_{L})=\mathbf{w}_{L}^{\top}\frac {1}{|G|}\sum_{g\in G}\mathbf{x}(g),\quad\forall\mathbf{x}\in\mathcal{X}_{L-1},\] (44)

where \(\mathbf{w}_{L}\in\mathbb{R}^{d_{L-1}}\) is the weight of the last layer. An \(L\)-layer linear G-CNN is then the composition

\[f_{\text{G-CNN}}(\mathbf{x};\mathbf{W})=\Psi_{L}^{\text{G-CNN}}(\cdots\Psi_{ 2}^{\text{G-CNN}}(\Psi_{1}^{\text{G-CNN}}(\mathbf{x};\mathbf{w}_{1});\mathbf{ w}_{2})\cdots;\mathbf{w}_{L}),\quad\mathbf{W}=[\mathbf{w}_{l}]_{l=1}^{L},\] (45)

where \(\mathbf{W}\in\mathbb{R}^{d_{0}\times d_{1}}\times\left[\prod_{l=2}^{L-1} \mathbb{R}^{d_{l-1}\times d_{l}\times|G|}\right]\times\mathbb{R}^{d_{L-1}} \eqqcolon\mathcal{W}_{\text{G-CNN}}\) are the trainable weights.

**Remark A.1**.: _As a special case of linear steerable networks, a linear G-CNN \(f_{\text{G-CNN}}(\mathbf{x};\mathbf{W})\) (45) can be written as_

\[f_{\text{G-CNN}}(\mathbf{x};\mathbf{W})=\left\langle\mathbf{x},\mathcal{P}_{ \text{G-CNN}}(\mathbf{W})\right\rangle=\left\langle\overline{\mathbf{x}}, \overline{\mathcal{P}}_{\text{G-CNN}}(\mathbf{W})\right\rangle,\] (46)

_where_

\[\overline{\mathcal{P}}_{\text{G-CNN}}(\mathbf{W})\coloneqq\mathbf{w}_{1} \left[\prod_{l=2}^{L-1}\left(\sum_{g\in G}\mathbf{w}_{l}(g)\right)\right] \mathbf{w}_{L},\ \mathcal{P}_{\text{G-CNN}}(\mathbf{W})\coloneqq\left[\frac{1}{|G|}\sum_{g \in G}g^{\top}\right]\overline{\mathcal{P}}_{\text{G-CNN}}(\mathbf{W})\] (47)

## Appendix B Proofs in Section 3

**Proposition 3.3**.: _Let \(f_{\text{steer}}(\mathbf{x};\mathbf{W})\) be the linear steerable network satisfying Assumption 3.1, where \(\mathbf{W}=[\mathbf{w}_{l}]_{l=1}^{L-1}\in\mathcal{W}_{\text{steer}}\) is the collection of all model parameters. There exists a multi-linear map \(M:(\mathbf{w}_{2},\cdots,\mathbf{w}_{L})\mapsto M(\mathbf{w}_{2},\cdots, \mathbf{w}_{L})\in\mathbb{R}^{d_{1}}\) such that for all \(\mathbf{x}\in\mathbb{R}^{d_{0}}\) and \(\mathbf{W}\in\mathcal{W}_{\text{steer}}\),_

\[f_{\text{steer}}(\mathbf{x};\mathbf{W})=f_{\text{steer}}(\overline{\mathbf{x}}; \mathbf{W})=\left\langle\overline{\mathbf{x}},\mathbf{w}_{1}M(\mathbf{w}_{2}, \cdots,\mathbf{w}_{L})\right\rangle,\] (48)

_where \(\overline{\mathbf{x}}\coloneqq\frac{1}{|G|}\sum_{g\in G}g\mathbf{x}\) is the average of all elements on the group orbit of \(\mathbf{x}\in\mathbb{R}^{d_{0}}\)._

Proof.: Since the linear steerable network \(f_{\text{steer}}(\mathbf{x};\mathbf{W})\) is \(G\)-invariant, we have \(f_{\text{steer}}(\mathbf{x};\mathbf{w})=f_{\text{steer}}(g\mathbf{x};\mathbf{w})\) for all \(g\in G\). Therefore,

\[f_{\text{steer}}(\mathbf{x};\mathbf{W})=\frac{1}{|G|}\sum_{g\in G}f_{\text{steer }}(g\mathbf{x};\mathbf{W})=f_{\text{steer}}\left(\frac{1}{|G|}\sum_{g\in G}g \mathbf{x};\mathbf{w}\right)=f_{\text{steer}}(\overline{\mathbf{x}};\mathbf{W}),\] (49)where the second equality is due to the linearity of \(f_{\text{steer}}(\mathbf{x};\mathbf{w})\) in \(\mathbf{x}\). We thus have

\[f_{\text{steer}}(\mathbf{x};\mathbf{W}) =f_{\text{steer}}(\overline{\mathbf{x}};\mathbf{W})=\Psi_{L}^{ \text{steer}}(\cdots\Psi_{2}^{\text{steer}}(\Psi_{1}^{\text{steer}}(\overline{ \mathbf{x}};\mathbf{w}_{1});\mathbf{w}_{2})\cdots;\mathbf{w}_{L})\] (50) \[=\left\langle\Psi_{1}^{\text{steer}}(\overline{\mathbf{x}}; \mathbf{w}),\Phi(\mathbf{w}_{2},\cdots,\mathbf{w}_{L})\right\rangle,\] (51)

where \(\Phi(\mathbf{w}_{2},\cdots,\mathbf{w}_{L})\in\mathcal{X}_{1}=\left(\mathbb{R} ^{d_{1}}\right)^{G}\) is multi-linear with respect to \((\mathbf{w}_{1},\cdots,\mathbf{w}_{L})\). Under Assumption 3.1, we have

\[f_{\text{steer}}(\mathbf{x};\mathbf{W}) =\sum_{g\in G}\left\langle\Psi_{1}^{\text{steer}}(\overline{ \mathbf{x}};\mathbf{w})(g),\Phi(\mathbf{w}_{2},\cdots,\mathbf{w}_{L})(g)\right\rangle\] (52) \[=\sum_{g\in G}\left\langle\mathbf{w}_{1}^{\top}g^{-1}\overline{ \mathbf{x}},\Phi(\mathbf{w}_{2},\cdots,\mathbf{w}_{L})(g)\right\rangle\] (53) \[=\sum_{g\in G}\left\langle\mathbf{w}_{1}^{\top}\overline{\mathbf{ x}},\Phi(\mathbf{w}_{2},\cdots,\mathbf{w}_{L})(g)\right\rangle\] (54) \[=\left\langle\overline{\mathbf{x}},\mathbf{w}_{1}\left[\sum_{g\in G }\Phi(\mathbf{w}_{2},\cdots,\mathbf{w}_{L})(g)\right]\right\rangle\] (55) \[=\left\langle\overline{\mathbf{x}},\mathbf{w}_{1}M(\mathbf{w}_{2},\cdots,\mathbf{w}_{L})\right\rangle,\] (56)

where \(M(\mathbf{w}_{2},\cdots,\mathbf{w}_{L}):=\sum_{g\in G}\Phi(\mathbf{w}_{2}, \cdots,\mathbf{w}_{L})(g)\) is also multi-linear in \((\mathbf{w}_{2},\cdots,\mathbf{w}_{L})\). 

**Proposition 3.6**.: _Let \(\mathbb{R}_{G}^{d_{0}}\subset\mathbb{R}^{d_{0}}\) be the subspace of \(G\)-invariant linear predictors, i.e., \(\mathbb{R}_{G}^{d_{0}}=\left\{\boldsymbol{\beta}\in\mathbb{R}^{d_{0}}: \boldsymbol{\beta}^{\top}\mathbf{x}=\boldsymbol{\beta}^{\top}g\mathbf{x}, \forall\mathbf{x}\in\mathbb{R}^{d_{0}},\forall g\in G\right\}\). Then_

* \(\mathbb{R}_{G}^{d_{0}}\) _is characterized by_ \[\mathbb{R}_{G}^{d_{0}}=\bigcap_{g\in G}\ker(I-g^{\top})=\text{Range}\left(\frac {1}{|G|}\sum_{g\in G}g^{\top}\right).\] (57)
* _Let_ \(\mathcal{A}:\mathbb{R}^{d_{0}}\rightarrow\mathbb{R}^{d_{0}}\) _be the group-averaging map,_ \[\mathcal{A}(\boldsymbol{\beta})\coloneqq\overline{\boldsymbol{\beta}}=\frac{1 }{|G|}\sum_{g\in G}g\boldsymbol{\beta}.\] (58) _Then its adjoint_ \(\mathcal{A}^{\top}:\boldsymbol{\beta}\mapsto\frac{1}{|G|}\sum_{g\in G}g^{\top}\boldsymbol {\beta}\) _is a projection operator from_ \(\mathbb{R}^{d_{0}}\) _to_ \(\mathbb{R}_{G}^{d_{0}}\)_. In other words,_ \(\text{Range}(\mathcal{A}^{\top})=\mathbb{R}_{G}^{d_{0}}\) _and_ \(\mathcal{A}^{\top}\circ\mathcal{A}^{\top}=\mathcal{A}^{\top}\)_._
* _If_ \(G\) _acts unitarily on_ \(\mathcal{X}_{0}\)_, i.e.,_ \(\rho_{0}(g^{-1})=\rho_{0}(g)^{\top}\)_, then_ \(\mathcal{A}=\mathcal{A}^{\top}\) _is self-adjoint. This implies that_ \(\mathcal{A}:\boldsymbol{\beta}\mapsto\overline{\boldsymbol{\beta}}\) _is an orthogonal projection from_ \(\mathbb{R}^{d_{0}}\) _onto_ \(\mathbb{R}_{G}^{d_{0}}\)_. In particular, we have_ \[\overline{\boldsymbol{\beta}}=\boldsymbol{\beta}\iff\boldsymbol{\beta}\in \mathbb{R}_{G}^{d_{0}},\quad\text{and}\quad\|\overline{\boldsymbol{\beta}}\| \leq\|\boldsymbol{\beta}\|,\forall\boldsymbol{\beta}\in\mathbb{R}^{d_{0}}.\] (59)

Proof.: To prove (a), for the first equality, a vector \(\boldsymbol{\beta}\in\mathbb{R}_{G}^{d_{0}}\) if and only if \(0=\boldsymbol{\beta}^{\top}(I-g)\mathbf{x}=\left\langle(I-g)^{\top}\boldsymbol{ \beta},\mathbf{x}\right\rangle,\forall\mathbf{x}\in\mathbb{R}^{d_{0}},\forall g\in G\). This is equivalent to \(\boldsymbol{\beta}\in\bigcap_{g\in G}\ker(I-g^{\top})\), and therefore \(\mathbb{R}_{G}^{d_{0}}=\bigcap_{g\in G}\ker(I-g^{\top})\).

For the second equality, if \(\boldsymbol{\beta}\in\bigcap_{g\in G}\ker(I-g^{\top})\), then \((I-g^{\top})\boldsymbol{\beta}=0,\forall g\in G\). Hence

\[0=\frac{1}{|G|}\sum_{g\in G}(I-g^{\top})\boldsymbol{\beta}=\boldsymbol{\beta}- \frac{1}{|G|}\sum_{g\in G}g^{\top}\boldsymbol{\beta}\] (60)

\[\implies\boldsymbol{\beta}=\frac{1}{|G|}\sum_{g\in G}g^{\top}\boldsymbol{\beta} \in\text{Range}\left(\frac{1}{|G|}\sum_{g\in G}g^{\top}\right)\] (61)On the other hand, if \(\bm{\beta}=\frac{1}{|G|}\sum_{h\in G}h^{\top}\mathbf{y}\) for some \(\mathbf{y}\in\mathbb{R}^{d_{0}}\), then for all \(g\in G\),

\[\left(I-g^{\top}\right)\bm{\beta}=\left(I-g^{\top}\right)\frac{1}{|G|}\sum_{h \in G}h^{\top}\mathbf{y}=\frac{1}{|G|}\sum_{h\in G}h^{\top}\mathbf{y}-\frac{1}{ |G|}\sum_{h\in G}(hg)^{\top}\mathbf{y}=0.\] (62)

Thus \(\bm{\beta}\in\bigcap_{g\in G}\ker(I-g^{\top})\).

Point (b) can be easily derived from (57) and (60).

To prove (c), notice that

\[\mathcal{A}^{\top}=\frac{1}{|G|}\sum_{g\in G}g^{\top}=\frac{1}{|G|}\sum_{g\in G }g^{-1}=\frac{1}{|G|}\sum_{g\in G}g=\mathcal{A}.\] (63)

Hence \(\mathcal{A}=\mathcal{A}^{\top}\) is self-adjoint. This combined with point (b) implies that \(\mathcal{A}=\mathcal{A}^{\top}:\bm{\beta}\mapsto\overline{\bm{\beta}}\) is an orthogonal projection from \(\mathbb{R}^{d_{0}}\) onto \(\mathbb{R}^{d_{0}}_{G}\). 

## Appendix C Proof of Theorem 4.1

**Theorem 4.1**.: _Under Assumption 3.1 and Assumption 3.7, let \(\bm{\beta}_{\text{steer}}(t)=\mathcal{P}_{\text{steer}}(\mathbf{W}(t))\) be the time-evolution of the \(G\)-invariant linear predictors parameterized by a linear steerable network trained with gradient flow on the data set \(S=\{(\mathbf{x}_{i},y_{i}):i\in[n]\}\); cf. Eq. (21). Then_

* _The directional limit_ \(\bm{\beta}_{\text{steer}}^{\infty}=\lim_{t\to\infty}\frac{\bm{\beta}_{\text{ steer}}(t)}{\|\bm{\beta}_{\text{steer}}(t)\|}\) _exists and_ \(\bm{\beta}_{\text{steer}}^{\infty}\propto\frac{1}{|G|}\sum_{g\in G}g^{\top} \bm{\gamma}^{*}\)_, where_ \(\bm{\gamma}^{*}\) _is the max-_\(L^{2}\)_-margin SVM solution for the_ _transformed_ _data_ \(\overline{S}=\{(\overline{\mathbf{x}}_{i},y_{i}):i\in[n]\}\)_:_ \[\bm{\gamma}^{*}=\arg\min_{\bm{\gamma}\in\mathbb{R}^{d_{0}}}\|\bm{\gamma}\|^{ 2},\quad\text{s.t. }y_{i}\left\langle\overline{\mathbf{x}}_{i},\bm{\gamma}\right\rangle\geq 1,\forall i\in[n].\] (64) _Furthermore, if_ \(G\) _acts unitarily on the input space_ \(\mathcal{X}_{0}\)_, i.e.,_ \(g^{-1}=g^{\top}\)_, then_ \(\bm{\beta}_{\text{steer}}^{\infty}\propto\bm{\gamma}^{*}\)_._
* _Equivalently,_ \(\bm{\beta}_{\text{steer}}^{\infty}\) _is proportional to the unique minimizer_ \(\bm{\beta}^{*}\) _of the problem_ \[\bm{\beta}^{*}=\arg\min_{\bm{\beta}\in\mathbb{R}^{d_{0}}}\|\text{Proj}_{\text{ Range}(\mathcal{A})}\bm{\beta}\|^{2},\quad\text{s.t.}\;\bm{\beta}\in\mathbb{R}^{d_{0}}_ {G},\text{ and }y_{i}\left\langle\mathbf{x}_{i},\bm{\beta}\right\rangle\geq 1, \forall i\in[n],\] (65) _where_ \(\text{Proj}_{\text{Range}(\mathcal{A})}\) _is the projection from_ \(\mathbb{R}^{d_{0}}\) _to_ \(\text{Range}(\mathcal{A})=\text{Range}\left(\frac{1}{|G|}\sum_{g\in G}g\right)\)_. Moreover, if_ \(G\) _acts unitarily on_ \(\mathcal{X}_{0}\)_, then_ \[\bm{\beta}_{\text{steer}}^{\infty}\propto\bm{\beta}^{*}=\arg\min_{\bm{\beta} \in\mathbb{R}^{d_{0}}}\|\bm{\beta}\|^{2},\quad\text{s.t. }\bm{\beta}\in\mathbb{R}^{d_{0}}_{G},\text{ and }y_{i}\left\langle\mathbf{x}_{i},\bm{\beta}\right\rangle\geq 1,\forall i\in[n].\] (66) _Namely,_ \(\bm{\beta}_{\text{steer}}^{\infty}\) _achieves the maximum_ \(L^{2}\)_-margin among all_ \(G\)_-invariant linear predictors._

Before proving Theorem 4.1, we need the following lemma which holds for general \(L\)-homogeneous networks, of which linear steerable networks are a special case. Note that in what follows, \(\|\cdot\|\) always denotes the Euclidean norm of a tensor viewed as a one-dimensional vector.

**Lemma C.1** (paraphrased from Lyu and Li (2020) and Ji and Telgarsky (2020)).: _Under Assumption 3.7, we have the following results of directional convergence and alignment._

* \(\mathcal{L}_{\mathcal{P}_{\text{steer}}}(\mathbf{W}(t);S)\to 0\)_, as_ \(t\to\infty\)_. Consequently,_ \(\|\mathbf{W}(t)\|\to\infty\) _and_ \(\|\mathcal{P}_{\text{steer}}(\mathbf{W}(t))\|\to\infty\)_._
* _The directional convergence and alignment of the parameters_ \(\mathbf{W}(t)\) _and the gradients_ \(\nabla_{\mathbf{W}}\mathcal{L}_{\mathcal{P}_{\text{near}}}(\mathbf{W}(t))\)_,_ \[\lim_{t\to\infty}\frac{\mathbf{W}(t)}{\|\mathbf{W}(t)\|}=\mathbf{W}^{\infty}=- \lim_{t\to\infty}\frac{\nabla_{\mathbf{W}}\mathcal{L}_{\mathcal{P}_{\text{near}}}( \mathbf{W}(t);S)}{\|\nabla_{\mathbf{W}}\mathcal{L}_{\mathcal{P}_{\text{near}}}( \mathbf{W}(t);S)\|},\] (67) _for some_ \(\mathbf{W}^{\infty}\in\mathcal{W}_{\text{steer}}\) _with_ \(\|\mathbf{W}^{\infty}\|=1\)_._
* _The limit_ \(\mathbf{W}^{\infty}\) _is along the direction of a first-order stationary point of the constrained optimization problem_ \[\min_{\mathbf{W}}\|\mathbf{W}\|^{2},\quad\text{s.t. }y_{i}\left\langle\mathbf{x}_{i},\mathcal{P}_{\text{steer}}(\mathbf{W}) \right\rangle\geq 1,\;\forall i\in[n].\] (68) _In other words, there exists a scaling factor_ \(\tau>0\) _such that_ \(\tau\mathbf{W}^{\infty}\) _satisfies the Karush-Kuhn-Tucker (KKT) conditions of (_68_)._Proof of Theorem 4.1.: The first part of the proof is inspired by Gunasekar et al. (2018). **To prove (a)**, let \(\mathbf{W}^{\infty}=[\mathbf{w}_{l}^{\infty}]_{l=1}^{L}=\lim_{t\to\infty}\frac{ \mathbf{W}(t)}{\|\mathbf{W}(t)\|}\) be the limit direction of \(\mathbf{W}(t)\), and let \(\widetilde{\mathbf{W}}^{\infty}=[\widetilde{\mathbf{w}}_{l}^{\infty}]_{l=1}^ {L}=\tau\mathbf{W}^{\infty}\), \(\tau>0\), be the stationary point of problem (68) by Lemma C.1. The KKT condition for \(\widetilde{\mathbf{W}}^{\infty}\) implies the existence of dual variables \(\alpha_{i}\geq 0\), \(i\in[n]\), such that

* _Primal feasibility:_ \[y_{i}\left\langle\mathcal{P}_{\text{steer}}(\widetilde{\mathbf{W}}^{\infty}), \mathbf{x}_{i}\right\rangle\geq 1,\quad\forall i\in[n].\] (69)
* _Stationarity:_ \[\widetilde{\mathbf{W}}^{\infty}=\nabla_{\mathbf{W}}\mathcal{P}_{\text{steer }}(\widetilde{\mathbf{W}}^{\infty})\cdot\left(\sum_{i=1}^{n}\alpha_{i}y_{i} \mathbf{x}_{i}\right)\] (70)
* _Complementary slackness_: \[y_{i}\left\langle\mathcal{P}_{\text{steer}}(\widetilde{\mathbf{W}}^{\infty}), \mathbf{x}_{i}\right\rangle>1\implies\alpha_{i}=0.\] (71)

We claim that \(\boldsymbol{\gamma}^{*}=\overline{\mathcal{P}}_{\text{steer}}(\widetilde{ \mathbf{W}}^{\infty})\) is a stationary point of the following problem

\[\min_{\boldsymbol{\gamma}\in\mathbb{R}^{d_{0}}}\|\boldsymbol{\gamma}\|^{2}, \quad\text{s.t.}\;y_{i}\left\langle\overline{\mathbf{x}}_{i},\boldsymbol{ \gamma}\right\rangle\geq 1,\;\forall i\in[n].\] (72)

That is, there exists \(\tilde{\alpha}_{i}\geq 0\), \(i\in[n]\), such that the following conditions are satisfied.

* _Primal feasibility:_ \[y_{i}\left\langle\overline{\mathcal{P}}_{\text{steer}}(\widetilde{\mathbf{W}} ^{\infty}),\overline{\mathbf{x}}_{i}\right\rangle\geq 1,\quad\forall i\in[n].\] (73)
* _Stationarity:_ \[\overline{\mathcal{P}}_{\text{steer}}(\widetilde{\mathbf{W}}^{\infty})=\sum_ {i=1}^{n}\tilde{\alpha}_{i}y_{i}\overline{\mathbf{x}}_{i}\] (74)
* _Complementary slackness_: \[y_{i}\left\langle\overline{\mathcal{P}}_{\text{steer}}(\widetilde{\mathbf{W}} ^{\infty}),\overline{\mathbf{x}}_{i}\right\rangle>1\implies\tilde{\alpha}_{i }=0.\] (75)

Indeed, Eq. (73) holds due to Eq. (69) and \(\left\langle\mathcal{P}_{\text{steer}}(\widetilde{\mathbf{W}}^{\infty}), \mathbf{x}_{i}\right\rangle=\left\langle\overline{\mathcal{P}}_{\text{steer}} (\widetilde{\mathbf{W}}^{\infty}),\overline{\mathbf{x}}_{i}\right\rangle\). Moreover, by the definition of \(\mathcal{P}_{\text{steer}}(\mathbf{W})\) and \(\overline{\mathcal{P}}_{\text{steer}}(\mathbf{W})\) in (10), we have for any \(\mathbf{z}\in\mathbb{R}^{d_{0}}\),

\[\nabla_{\mathbf{w}_{1}}\overline{\mathcal{P}}_{\text{steer}}( \mathbf{W})\cdot\mathbf{z} =\mathbf{z}[M(\mathbf{w}_{2},\cdots,\mathbf{w}_{L})]^{\top}\] (76) \[\nabla_{\mathbf{w}_{1}}\mathcal{P}_{\text{steer}}(\mathbf{W})\cdot \mathbf{z} =\left(\frac{1}{|G|}\sum_{g\in G}g\right)\mathbf{z}[M(\mathbf{w}_{ 2},\cdots,\mathbf{w}_{L})]^{\top}=\overline{\mathbf{z}}[M(\mathbf{w}_{2}, \cdots,\mathbf{w}_{L})]^{\top}.\] (77)

Therefore Eq. (70) implies

\[\widetilde{\mathbf{w}}_{1}^{\infty}=\nabla_{\mathbf{w}_{1}}\mathcal{P}_{\text {steer}}(\widetilde{\mathbf{W}}^{\infty})\cdot\left(\sum_{i=1}^{n}\alpha_{i}y_ {i}\mathbf{x}_{i}\right)=\left(\sum_{i=1}^{n}\alpha_{i}y_{i}\overline{ \mathbf{x}}_{i}\right)\cdot[M(\widetilde{\mathbf{w}}_{2}^{\infty},\cdots, \widetilde{\mathbf{w}}_{L}^{\infty})]^{\top}\] (78)

Hence

\[\overline{\mathcal{P}}_{\text{steer}}(\widetilde{\mathbf{W}}^{ \infty}) =\widetilde{\mathbf{w}}_{1}^{\infty}M(\widetilde{\mathbf{w}}_{2}^{ \infty},\cdots,\widetilde{\mathbf{w}}_{L}^{\infty})=\left(\sum_{i=1}^{n} \alpha_{i}y_{i}\overline{\mathbf{x}}_{i}\right)\left\|M(\widetilde{\mathbf{ w}}_{2}^{\infty},\cdots,\widetilde{\mathbf{w}}_{L}^{\infty})\right\|^{2}\] \[=c\sum_{i=1}^{n}\alpha_{i}y_{i}\overline{\mathbf{x}}_{i},\] (79)where \(c=\|M(\widehat{\mathbf{w}}_{2}^{\infty},\cdots,\widehat{\mathbf{w}}_{L}^{\infty}) \|^{2}\geq 0\). From (73) we know that \(\overline{\mathcal{P}}_{\text{steer}}(\widetilde{\mathbf{W}}^{\infty})\neq 0\) and hence \(c>0\). Let \(\tilde{\alpha}_{i}=c\alpha_{i},\forall i\in[n]\). Then \(\tilde{\alpha}_{i}\geq 0\), and the stationarity (74) is satisfied due to Eq. (79), and the complementary slackness also holds due to \(\tilde{\alpha}_{i}\) being a positive scaling of \(\alpha_{i}\) for all \(i\in[n]\). Therefore \(\boldsymbol{\gamma}^{*}=\overline{\mathcal{P}}_{\text{steer}}(\widetilde{ \mathbf{W}}^{\infty})\) is a stationary point of (72). Since problem (72) is strongly convex, \(\boldsymbol{\gamma}^{*}=\overline{\mathcal{P}}_{\text{steer}}(\widetilde{ \mathbf{W}}^{\infty})\) is in fact the unique minimizer of (72). Hence the limit direction of the predictor \(\mathcal{P}_{\text{steer}}(\mathbf{W}(t))\) is

\[\boldsymbol{\beta}_{\text{steer}}^{\infty} =\lim_{t\to\infty}\frac{\left(\frac{1}{|G|}\sum_{g\in G}g^{\top} \right)\overline{\mathcal{P}}_{\text{steer}}(\mathbf{W}(t))}{\left\|\left( \frac{1}{|G|}\sum_{g\in G}g^{\top}\right)\overline{\mathcal{P}}_{\text{steer}} (\mathbf{W}(t))\right\|}\] (81) \[\propto\frac{1}{|G|}\sum_{g\in G}g^{\top}\boldsymbol{\gamma}^{*},\] (82)

where the third equality is due to \(\overline{\mathcal{P}}_{\text{steer}}\) being \(L\)-homogeneous (12), and the fourth equality comes from the continuity of \(\overline{\mathcal{P}}_{\text{steer}}\) and that \(\mathcal{P}_{\text{steer}}(\widetilde{\mathbf{W}}^{\infty})=\left(\frac{1}{|G |}\sum_{g\in G}g^{\top}\right)\overline{\mathcal{P}}_{\text{steer}}\left( \widetilde{\mathbf{W}}^{\infty}\right)\neq 0\) (because otherwise (69) can not hold.)

Finally, if \(G\) acts unitarily on \(\mathcal{X}_{0}\), then Eq. (74) combined with Proposition 3.6 implies that \(\boldsymbol{\gamma}^{*}=\overline{\mathcal{P}}_{\text{steer}}(\widetilde{ \mathbf{W}}^{\infty})\in\mathbb{R}_{G}^{d_{0}}\). Hence

\[\boldsymbol{\beta}_{\text{steer}}^{\infty}\propto\frac{1}{|G|}\sum_{g\in G}g^ {\top}\boldsymbol{\gamma}^{*}=\frac{1}{|G|}\sum_{g\in G}g\boldsymbol{\gamma}^ {*}=\overline{\boldsymbol{\gamma}^{*}}=\boldsymbol{\gamma}^{*},\] (83)

where the last equality is again due to Proposition 3.6. This concludes the proof of (a).

**To prove (b)**, we first show that problem (65) has a unique minimizer. To prove this, notice that \(\text{Proj}_{\text{Range}(\mathcal{A})}:\mathbb{R}_{G}^{d_{0}}\to\text{Range}( \mathcal{A})\) is injective; indeed, for any \(\boldsymbol{\beta}\in\mathbb{R}_{G}^{d_{0}}\),

\[\text{Proj}_{\text{Range}(\mathcal{A})}\boldsymbol{\beta}=0\implies \boldsymbol{\beta}\in\text{Range}(\mathcal{A})^{\perp}=\ker(\mathcal{A}^{ \top}).\] (84)

Thus \(0=\mathcal{A}^{\top}\boldsymbol{\beta}=\boldsymbol{\beta}\), where the second equality is due to \(\mathcal{A}^{\top}\) being a projection onto \(\text{Range}(\mathcal{A}^{\top})=\mathbb{R}_{G}^{d_{0}}\) (Proposition 3.6) and \(\boldsymbol{\beta}\in\mathbb{R}_{G}^{d_{0}}\). Therefore the objective function in (65) is strongly convex on \(\mathbb{R}_{G}^{d_{0}}\) and there exists a unique minimizer.

To show \(\boldsymbol{\beta}_{\text{steer}}^{\infty}\propto\boldsymbol{\beta}^{*}\), it suffices to verify that \(\frac{1}{|G|}\sum_{g\in G}g^{\top}\boldsymbol{\gamma}^{*}=\boldsymbol{\beta}^ {*}\) is the minimizer of (65). To this end, we notice that

* \(\frac{1}{|G|}\sum_{g\in G}g^{\top}\boldsymbol{\gamma}^{*}=\mathcal{A}^{\top} \boldsymbol{\gamma}^{*}\in\mathbb{R}_{G}^{d_{0}}\) by Proposition 3.6;
* for all \(i\in[n]\) \[y_{i}\left\langle\mathbf{x}_{i},\frac{1}{|G|}\sum_{g\in G}g^{\top}\boldsymbol{ \gamma}^{*}\right\rangle=y_{i}\left\langle\overline{\mathbf{x}}_{i}, \boldsymbol{\gamma}^{*}\right\rangle\geq 1.\] (85)

Thus \(\frac{1}{|G|}\sum_{g\in G}g^{\top}\boldsymbol{\gamma}^{*}\) satisfies the constraints in (65). Assume for the sake of contradiction that \(\frac{1}{|G|}\sum_{g\in G}g^{\top}\boldsymbol{\gamma}^{*}\neq\boldsymbol{\beta}^ {*}\), then by the uniqueness of the minimizer of (65) we have

\[\left\|\widetilde{\boldsymbol{\gamma}}\right\|\coloneqq\left\|\text{Proj}_{ \text{Range}(\mathcal{A})}\boldsymbol{\beta}^{*}\right\|<\left\|\text{Proj}_{ \text{Range}(\mathcal{A})}\frac{1}{|G|}\sum_{g\in G}g^{\top}\boldsymbol{\gamma}^ {*}\right\|,\] (86)

where \(\widetilde{\boldsymbol{\gamma}}\coloneqq\text{Proj}_{\text{Range}(\mathcal{A})} \boldsymbol{\beta}^{*}\). We make the following two claims to be proved shortly:

* **Claim 1:**\(y_{i}\left(\widetilde{\boldsymbol{\gamma}},\overline{\mathbf{x}}_{i}\right)=y_{i }\left\langle\boldsymbol{\beta}^{*},\mathbf{x}_{i}\right\rangle\geq 1,\forall i\in[n]\).

* **Claim 2:**\(\text{Proj}_{\text{Range}(\mathcal{A})}\frac{1}{|G|}\sum_{g\in G}g^{\top}\bm{\gamma}^ {*}=\bm{\gamma}^{*}\).

These two claims combined with (86) imply that \(\widetilde{\bm{\gamma}}\) satisfies the constraint in (64) and has a smaller norm compared to \(\bm{\gamma}^{*}\), \(\|\widetilde{\bm{\gamma}}\|<\|\bm{\gamma}^{*}\|\). This contradicts the fact that \(\bm{\gamma}^{*}\) is the minimizer of problem (64). Therefore we have \(\frac{1}{|G|}\sum_{g\in G}g^{\top}\bm{\gamma}^{*}=\bm{\beta}^{*}\).

If we assume in addition that \(\rho_{0}\) is unitary on \(\mathcal{X}_{0}\), then \(\mathcal{A}=\mathcal{A}^{\top}:\bm{\beta}\mapsto\overline{\bm{\beta}}\) is an orthogonal projection from \(\mathbb{R}^{d_{0}}\) onto \(\mathbb{R}^{d_{0}}_{G}=\text{Range}(\mathcal{A})\). Therefore \(\text{Proj}_{\text{Range}(\mathcal{A})}\bm{\beta}=\bm{\beta}\) for \(\bm{\beta}\in\mathbb{R}^{d_{0}}_{G}\), and hence problems (65) and (66) are equivalent.

Finally, we prove the above two claims.

**Proof of Claim 1:**\(y_{i}\left(\widetilde{\bm{\gamma}},\overline{\mathbf{x}}_{i}\right)=y_{i} \left\langle\bm{\beta}^{*},\mathbf{x}_{i}\right\rangle\geq 1,\forall i\in[n]\).

Since \(\widetilde{\bm{\gamma}}=\text{Proj}_{\text{Range}(\mathcal{A})}\bm{\beta}^{*}\), we have

\[\widetilde{\bm{\gamma}}-\bm{\beta}^{*}\in\text{Range}(\mathcal{A})^{\perp}= \ker(\mathcal{A}^{\top}).\] (87)

Hence

\[\mathcal{A}^{\top}\widetilde{\bm{\gamma}}=\mathcal{A}^{\top}\bm{\beta}^{*}= \bm{\beta}^{*},\] (88)

where the second equality is due to \(\bm{\beta}^{*}\in\mathbb{R}^{d_{0}}_{G}=\text{Range}(\mathcal{A}^{\top})\) and \(\mathcal{A}^{\top}=\mathcal{A}^{\top}\circ\mathcal{A}^{\top}\) is a projection; cf. Proposition 3.6. Therefore, for all \(i\in[n]\),

\[y_{i}\left\langle\widetilde{\bm{\gamma}},\overline{\mathbf{x}}_{i}\right\rangle =y_{i}\left\langle\mathcal{A}^{\top}\widetilde{\bm{\gamma}}, \mathbf{x}_{i}\right\rangle=y_{i}\left\langle\bm{\beta}^{*},\mathbf{x}_{i} \right\rangle\geq 1.\] (89)

**Proof of Claim 2:**\(\text{Proj}_{\text{Range}(\mathcal{A})}\mathcal{A}^{\top}\bm{\gamma}^{*}=\bm{ \gamma}^{*}\).

We first note that both \(\text{Proj}_{\text{Range}(\mathcal{A})}\mathcal{A}^{\top}\bm{\gamma}^{*}\) and \(\bm{\gamma}^{*}\) are in the affine space \(\mathcal{A}^{\top}\bm{\gamma}^{*}+\ker(\mathcal{A}^{\top})\). Indeed,

\[\left(\mathcal{A}^{\top}\bm{\gamma}^{*}-\text{Proj}_{\text{Range} (\mathcal{A})}\mathcal{A}^{\top}\bm{\gamma}^{*}\right)\in\text{Range}( \mathcal{A})^{\perp}=\ker(\mathcal{A}^{\top}),\] (90) \[\mathcal{A}^{\top}(\bm{\gamma}^{*}-\mathcal{A}^{\top}\bm{\gamma}^ {*})=\mathcal{A}^{\top}\bm{\gamma}^{*}-\mathcal{A}^{\top}\mathcal{A}^{\top} \bm{\gamma}^{*}=0.\] (91)

This also implies

\[\mathcal{A}^{\top}\left(\text{Proj}_{\text{Range}(\mathcal{A})} \mathcal{A}^{\top}\bm{\gamma}^{*}\right)=\mathcal{A}^{\top}\bm{\gamma}^{*}.\] (92)

Moreover, for any \(i\in[n]\),

\[y_{i}\left\langle\text{Proj}_{\text{Range}(\mathcal{A})}\mathcal{ A}^{\top}\bm{\gamma}^{*},\overline{\mathbf{x}}_{i}\right\rangle =y_{i}\left\langle\mathcal{A}^{\top}\left(\text{Proj}_{\text{ Range}(\mathcal{A})}\mathcal{A}^{\top}\bm{\gamma}^{*}\right),\mathbf{x}_{i}\right\rangle\] (93) \[=y_{i}\left\langle\mathcal{A}^{\top}\bm{\gamma}^{*},\mathbf{x}_{i }\right\rangle=y_{i}\left\langle\bm{\gamma}^{*},\overline{\mathbf{x}}_{i} \right\rangle\geq 1.\] (94)

Hence \(\text{Proj}_{\text{Range}(\mathcal{A})}\mathcal{A}^{\top}\bm{\gamma}^{*}\) also satisfies the constraint in (64). Since the orthogonal projection \(\text{Proj}_{\text{Range}(\mathcal{A})}\mathcal{A}^{\top}\bm{\gamma}^{*}\) achieves the minimal norm among all vectors in the affine space \(\mathcal{A}^{\top}\bm{\gamma}^{*}+\ker(\mathcal{A}^{\top})\) which includes \(\bm{\gamma}^{*}\), this can only happen when \(\text{Proj}_{\text{Range}(\mathcal{A})}\mathcal{A}^{\top}\bm{\gamma}^{*}=\bm{ \gamma}^{*}\) is the unique minimizer of (64).

This concludes the proof of Theorem 4.1. 

## Appendix D Proofs in Section 5

**Corollary 5.1**.: _Let \(\bm{\beta}^{\infty}_{\text{steer}}=\lim_{t\to\infty}\frac{\bm{\beta}_{\text{ steer}}(t)}{\|\bm{\beta}_{\text{steer}}(t)\|}\) be the directional limit of the linear predictor \(\bm{\beta}_{\text{steer}}(t)=\mathcal{P}_{\text{steer}}(\mathbf{W}(t))\) parameterized by a linear steerable network trained using gradient flow on the **original** data set \(S=\{(\mathbf{x}_{i},y_{i}),i\in[n]\}\). Correspondingly, let \(\bm{\beta}^{\infty}_{\text{fc}}=\lim_{t\to\infty}\frac{\bm{\beta}_{\text{fc}}(t) }{\|\bm{\beta}_{\text{fc}}(t)\|}\). \(\bm{\beta}_{\text{fc}}(t)=\mathcal{P}_{\text{fc}}(\mathbf{W}(t))\) (13), be that of a linear fully-connected network trained on the **augmented** data set \(S_{\text{aug}}=\{(g\mathbf{x}_{i},y_{i}),i\in[n],g\in G\}\). If \(G\) acts unitarily on \(\mathcal{X}_{0}\), then_

\[\bm{\beta}^{\infty}_{\text{steer}}=\bm{\beta}^{\infty}_{\text{fc}}.\] (95)

_In other words, the effect of using a linear steerable network for group-invariant binary classification is exactly the same as conducting **data-augmentation** for non-invariant models._Proof.: By Theorem 4.1, a positive scaling \(\bm{\gamma}^{*}=\tau\bm{\beta}_{\text{steer}}^{\infty}\) of \(\bm{\beta}_{\text{steer}}^{\infty}\) satisfies the KKT condition of (22). That is, there exists \(\alpha_{i}\geq 0\), \(i\in[n]\), such that

* _Primal feasibility:_ \[y_{i}\left\langle\bm{\gamma}^{*},\overline{\mathbf{x}}_{i}\right\rangle\geq 1,\quad\forall i\in[n].\] (96)
* _Stationarity:_ \[\bm{\gamma}^{*}=\sum_{i=1}^{n}\alpha_{i}y_{i}\overline{\mathbf{x}}_{i}\] (97)
* _Complementary slackness_: \[y_{i}\left\langle\bm{\gamma}^{*},\overline{\mathbf{x}}_{i}\right\rangle>1 \implies\alpha_{i}=0.\] (98)

Using Corollary 4.3, we only need to show that \(\bm{\gamma}^{*}\) is also the solution of

\[\arg\min_{\bm{\gamma}\in\mathbb{R}^{d_{0}}}\left\|\bm{\gamma} \right\|^{2},\quad\text{s.t.}\;y_{i}\left\langle g\mathbf{x}_{i},\bm{\gamma} \right\rangle\geq 1,\forall i\in[n],\forall g\in G.\] (99)

That is, there exists \(\tilde{\alpha}_{i,g}\geq 0,\forall i\in[n],\forall g\in G\), such that

* _Primal feasibility:_ \[y_{i}\left\langle\bm{\gamma}^{*},g\mathbf{x}_{i}\right\rangle\geq 1,\quad \forall i\in[n],\forall g\in G.\] (100)
* _Stationarity:_ \[\bm{\gamma}^{*}=\sum_{i=1}^{n}\sum_{g\in G}\tilde{\alpha}_{i,g}y_{i}g \mathbf{x}_{i}\] (101)
* _Complementary slackness_: \[y_{i}\left\langle\bm{\gamma}^{*},g\mathbf{x}_{i}\right\rangle>1 \implies\tilde{\alpha}_{i,g}=0.\] (102)

Indeed, we set \(\tilde{\alpha}_{i,g}=\frac{1}{|G|}\alpha_{i}\geq 0\). Since \(\bm{\gamma}^{*}\in\mathbb{R}_{G}^{d_{0}}\) if \(\rho_{0}\) is unitary (this can also be observed from Eq. (97)), we have \(y_{i}\left\langle\bm{\gamma}^{*},g\mathbf{x}_{i}\right\rangle=y_{i}\left\langle \bm{\gamma}^{*},\mathbf{x}_{i}\right\rangle,\forall g\in G,\forall i\in[n]\). Hence we have primal feasibility (100) from (96),

\[y_{i}\left\langle\bm{\gamma}^{*},g\mathbf{x}_{i}\right\rangle=y_{i}\left\langle \bm{\gamma}^{*},\frac{1}{|G|}\sum_{h\in G}hg\mathbf{x}_{i}\right\rangle=y_{i} \left\langle\bm{\gamma}^{*},\overline{\mathbf{x}}_{i}\right\rangle\geq 1,\quad \forall i\in[n],\forall g\in G.\] (103)

Stationarity (101) holds since

\[\sum_{i=1}^{n}\sum_{g\in G}\tilde{\alpha}_{i,g}y_{i}g\mathbf{x}_{i }=\sum_{i=1}^{n}\sum_{g\in G}\frac{1}{|G|}\alpha_{i}y_{i}g\mathbf{x}_{i}=\sum _{i=1}^{n}\alpha_{i}y_{i}\overline{\mathbf{x}}_{i}=\bm{\gamma}^{*},\] (104)

where the last equality comes from (97). Finally, if \(y_{i}\left\langle\bm{\gamma}^{*},g\mathbf{x}_{i}\right\rangle>1\), then (103) and (98) imply that

\[\tilde{\alpha}_{i,g}=\frac{1}{|G|}\alpha_{i}=0.\] (105)

This proves the condition for complementary slackness (102). 

**Corollary 5.5**.: _Let \(\bm{\beta}_{\text{steer}}^{\infty}\) be the same as that in Corollary 5.1. Let \(\bm{\beta}_{\text{fc}}^{\rho_{0},\infty}=\lim_{t\to\infty}\frac{\bm{\beta}_{ \text{fc}}^{\rho_{0}}(t)}{\|\bm{\beta}_{\text{fc}}^{\rho_{0}}(t)\|}\) be the limit direction of \(\bm{\beta}_{\text{fc}}^{\rho_{0}}(t)=\mathcal{P}_{\text{fc}}(\mathbf{W}(t))\) under the gradient flow of the modified empirical loss \(\mathcal{L}_{\mathcal{P}_{\text{fc}}}^{\rho_{0}}(\mathbf{W};S_{\text{aug}})\) (34) for a linear fully-connected network on the **augmented** data set \(S_{\text{aug}}\). Then_

\[\bm{\beta}_{\text{steer}}^{\infty}\propto\left(\frac{1}{|G|}\sum_ {g\in G}\rho_{0}(g)^{\top}\rho_{0}(g)\right)^{1/2}\bm{\beta}_{\text{fc}}^{\rho _{0},\infty}.\] (106)

_Consequently, we have \(\left\langle\mathbf{x},\bm{\beta}_{\text{steer}}^{\infty}\right\rangle\propto \left\langle\mathbf{x},\bm{\beta}_{\text{fc}}^{\rho_{0},\infty}\right\rangle_{ \rho_{0}}\) for all \(\mathbf{x}\in\mathbb{R}^{d_{0}}\)._Proof.: The proof is similar to that of Corollary 5.1. By Theorem 4.1, the limit direction \(\bm{\beta}_{\text{soter}}^{\infty}\) is proportional to \(\frac{1}{|G|}\sum_{g\in G}g^{\top}\bm{\gamma}^{*}\), where \(\bm{\gamma}^{*}\) satisfies the same KKT condition ((96), (97) and (98)) of problem (22) as in the proof of Corollary 5.1. On the other hand, note that the loss function \(\mathcal{L}_{\text{P}_{\text{F}}}^{\rho_{0}}(\mathbf{W};S_{\text{aug}})\) (34) for the modified fully-connected network on the augmented data set \(S_{\text{aug}}\) is equivalent to

\[\mathcal{L}_{\mathcal{P}_{\text{F}}}^{\rho_{0}}(\mathbf{W};S_{ \text{aug}}) =\sum_{g\in G}\sum_{i=1}^{n}\ell_{\exp}\left(\left\langle g\mathbf{ x}_{i},\mathcal{P}_{\text{fc}}(\mathbf{W})\right\rangle_{\rho_{0}},y_{i}\right)\] (107) \[=\sum_{g\in G}\sum_{i=1}^{n}\ell_{\exp}\left(\left\langle Ag \mathbf{x}_{i},\mathcal{P}_{\text{fc}}(\mathbf{W})\right\rangle,y_{i}\right)\] (108) \[=\mathcal{L}_{\mathcal{P}_{k}}(\mathbf{W};\widetilde{S}_{\text{ aug}}),\] (109)

where

\[A=\left(\frac{1}{|G|}\sum_{g\in G}\rho_{0}(g)^{\top}\rho_{0}(g) \right)^{1/2},\quad\widetilde{S}_{\text{aug}}=\left\{(Ag\mathbf{x}_{i},y_{i}) :i\in[n],g\in G\right\}.\] (110)

Therefore, by Corollary 4.3, \(\bm{\beta}_{\text{fc}}^{\rho_{0},\infty}\) is proportional to the solution \(\widetilde{\bm{\gamma}}_{\text{aug}}^{*}\) of

\[\widetilde{\bm{\gamma}}_{\text{aug}}^{*}=\arg\min_{\bm{\gamma} \in\mathbb{R}^{q_{0}}}\|\bm{\gamma}\|^{2},\quad\text{s.t.}\;y_{i}\left\langle Ag \mathbf{x}_{i},\bm{\gamma}\right\rangle\geq 1,\forall i\in[n],\forall g\in G.\] (111)

We claim that \(\widetilde{\bm{\gamma}}_{\text{aug}}^{*}=A^{-1}\left(\frac{1}{|G|}\sum_{g\in G }g^{\top}\bm{\gamma}^{*}\right)\). To see this, let \(\widetilde{\alpha}_{i,g}=\alpha_{i},\forall i\in[n]\), where \(\alpha_{i}\geq 0\) is the dual variable for \(\bm{\gamma}^{*}\) of problem (22). We verify below that the KKT conditions for problem (111) are satisfied for the primal-dual pair \(A^{-1}\left(\frac{1}{|G|}\sum_{g\in G}g^{\top}\bm{\gamma}^{*}\right)\) and \((\widetilde{\alpha}_{i,g})_{i\in[n],g\in G}\).

* _Primal feasibility:_ for any \(i\in[n]\) and \(h\in G\), \[y_{i}\left\langle A^{-1}\left(\frac{1}{|G|}\sum_{g\in G}g^{\top} \bm{\gamma}^{*}\right),Ah\mathbf{x}_{i}\right\rangle =y_{i}\left\langle\frac{1}{|G|}\sum_{g\in G}g^{\top}\bm{\gamma}^ {*},h\mathbf{x}_{i}\right\rangle\] (112) \[=y_{i}\left\langle\bm{\gamma}^{*},\left(\frac{1}{|G|}\sum_{g\in G }g\right)h\mathbf{x}_{i}\right\rangle\] (113) \[=y_{i}\left\langle\bm{\gamma}^{*},\overline{\mathbf{x}}_{i} \right\rangle\geq 1,\] (114) where the first equality is due to \(A^{\top}=A\) being symmetric, and the last inequality is due to (96).

* _Stationarity:_ \[A^{-1}\left(\frac{1}{|G|}\sum_{g\in G}g^{\top}\bm{\gamma}^{*}\right) =A^{-1}\left(\frac{1}{|G|}\sum_{g\in G}g^{\top}\right)\sum_{i=1}^{ n}\alpha_{i}y_{i}\mathbf{\overline{x}}_{i}\] (115) \[=A^{-1}\frac{1}{|G|^{2}}\left(\sum_{g\in G}g^{\top}\right)\sum_{i= 1}^{n}\alpha_{i}y_{i}\sum_{h\in G}h\mathbf{x}_{i}\] (116) \[=A\cdot A^{-2}\frac{1}{|G|^{2}}\sum_{i=1}^{n}\alpha_{i}y_{i}\sum_ {g\in G}\sum_{h\in G}g^{\top}h\mathbf{x}_{i}\] (117) \[=A\cdot A^{-2}\frac{1}{|G|}\sum_{i=1}^{n}\alpha_{i}y_{i}\left( \frac{1}{|G|}\sum_{g\in G}g^{\top}g\right)\sum_{h\in G}h\mathbf{x}_{i}\] (118) \[=\sum_{i=1}^{n}\frac{\alpha_{i}}{|G|}y_{i}\sum_{h\in G}Ah\mathbf{ x}_{i}\] (119) \[=\sum_{i=1}^{n}\sum_{g\in G}\widetilde{\alpha}_{i,g}y_{i}Ag \mathbf{x}_{i},\] (120) where the first equality is due to (97), and the last equality comes from the definition of the dual variable \(\widetilde{\alpha}_{i,g}=\frac{1}{|G|}\alpha_{i},\forall i\in[n],\forall g\in G\).
* _Complementary slackness:_ if \(y_{i}\left\langle A^{-1}\left(\frac{1}{|G|}\sum_{g\in G}g^{\top}\bm{\gamma}^{* }\right),Ag\mathbf{x}_{i}\right\rangle>1\) for some \(i\in[n]\) and \(g\in G\), then (114) and (98) imply that \(\widetilde{\alpha}_{i,g}=\frac{1}{|G|}\alpha_{i}=0\).

Hence \(\widetilde{\bm{\gamma}}_{\text{aug}}^{*}=A^{-1}\left(\frac{1}{|G|}\sum_{g\in G }g^{\top}\bm{\gamma}^{*}\right)\) is indeed the solution of (111). Therefore,

\[\bm{\beta}_{\text{steer}}^{\infty}\propto\frac{1}{|G|}\sum_{g\in G}g^{\top} \bm{\gamma}^{*}=A\bm{\gamma}_{\text{aug}}^{*}\propto A\bm{\beta}_{\text{fc} }^{\rho_{0},\infty}.\] (121)

This completes the proof. 

## Appendix E Proofs of Theorem 6.1

**Theorem 6.1**.: _Let \(\bm{\beta}_{\text{steer}}^{\infty}\) be the directional limit of a linear-steerable-network-parameterized predictor trained on the **original** data set \(S=\{(\mathbf{x}_{i},y_{i}),i\in[n]\}\); let \(\bm{\beta}_{\text{fc}}^{\infty}\) be that of a linear fully-connected network **also** trained on the same data set \(S\). Let \(M_{\text{steer}}\) and \(M_{\text{fc}}\), respectively, be the (signed) margin of \(\bm{\beta}_{\text{steer}}^{\infty}\) and \(\bm{\beta}_{\text{fc}}^{\infty}\) on the **augmented** data set \(S_{\text{aug}}=\{(g\mathbf{x}_{i},y_{i}):i\in[n],g\in G\}\), i.e.,_

\[M_{\text{steer}}:=\min_{i\in[n],g\in G}y_{i}\left\langle\bm{\beta}_{\text{steer }}^{\infty},g\mathbf{x}_{i}\right\rangle,\quad M_{\text{fc}}:=\min_{i\in[n],g \in G}y_{i}\left\langle\bm{\beta}_{\text{fc}}^{\infty},g\mathbf{x}_{i}\right\rangle.\] (122)

_Then we always have \(M_{\text{steer}}\geq M_{\text{fc}}\)._

Proof.: By Corollary 4.3 and Corollary 5.1, we have

\[\bm{\beta}_{\text{steer}}^{\infty}\propto\bm{\gamma}_{\text{steer }}^{*}=\arg\min_{\bm{\gamma}\in\mathbb{R}^{d_{0}}}\|\bm{\gamma}\|^{2},\quad \text{s.t.}\;y_{i}\left\langle\bm{\gamma},g\mathbf{x}_{i}\right\rangle\geq 1, \forall g\in G,\forall i\in[n],\] (123) \[\bm{\beta}_{\text{fc}}^{\infty}\propto\bm{\gamma}_{\text{fc}}^{* }=\arg\min_{\bm{\gamma}\in\mathbb{R}^{d_{0}}}\|\bm{\gamma}\|^{2},\quad\text{s.t. }\;y_{i}\left\langle\bm{\gamma},\mathbf{x}_{i}\right\rangle\geq 1, \forall i\in[n].\] (124)

Moreover, the margin \(M_{\text{steer}}\) of the steerable network \(\bm{\beta}_{\text{steer}}^{\infty}\) on the augmented data set \(S_{\text{aug}}\) is \(M_{\text{steer}}=\frac{1}{\|\bm{\gamma}_{\text{steer}}^{*}\|}\). Consider the following three cases.

* _Case 1:_ \(\min_{i\in[n],g\in G}y_{i}\left\langle\bm{\gamma}_{\text{fc}}^{*},g\mathbf{x}_ {i}\right\rangle\geq 1\). In this case, \(\bm{\gamma}_{\text{fc}}^{*}\) also satisfies the more restrictive constraint in (123). Therefore \(\bm{\gamma}_{\text{fc}}^{*}=\bm{\gamma}_{\text{steer}}^{*}\), which implies \[\bm{\beta}_{\text{steer}}^{\infty}=\bm{\beta}_{\text{fc}}^{\infty},\quad\text {and}\quad M_{\text{steer}}=M_{\text{fc}}.\] (125)* _Case 2:_ \(\min_{i\in[n],g\in G}y_{i}\left\langle\bm{\gamma}_{\text{fc}}^{*},g\bm{x}_{i} \right\rangle\leq 0\). This implies that \(\bm{\gamma}_{\text{fc}}^{*}\), and hence \(\bm{\beta}_{\text{fc}}^{\infty}\) has a non-positive margin on \(D_{\text{aug}}\). Thus \[M_{\text{fc}}\leq 0<M_{\text{steer}}.\] (126)
* _Case 3:_ \(0<\min_{i\in[n],g\in G}y_{i}\left\langle\bm{\gamma}_{\text{fc}}^{*},g\bm{x}_{i} \right\rangle<1\). In this case, define \[\widetilde{\bm{\gamma}}_{\text{fc}}^{*}\coloneqq\frac{\bm{\gamma}_{\text{fc}} ^{*}}{\min_{i\in[n],g\in G}y_{i}\left\langle\bm{\gamma}_{\text{fc}}^{*},g\bm{x }_{i}\right\rangle}.\] (127) Then \(\widetilde{\bm{\gamma}}_{\text{fc}}^{*}\) satisfies the constraint in (123), and hence \[\|\widetilde{\bm{\gamma}}_{\text{fc}}^{*}\|\geq\|\bm{\gamma}_{\text{steer}}^{*}\|.\] (128) Therefore \[M_{\text{fc}}=\min_{i\in[n],g\in G}y_{i}\left\langle\bm{\beta}_{ \text{fc}}^{\infty},g\bm{x}_{i}\right\rangle =\frac{\min_{i\in[n],g\in G}y_{i}\left\langle\bm{\gamma}_{\text{ fc}}^{*},g\bm{x}_{i}\right\rangle}{\|\bm{\gamma}_{\text{fc}}^{*}\|}=\frac{1}{\| \widetilde{\bm{\gamma}}_{\text{fc}}^{*}\|}\] (129) \[\leq\frac{1}{\|\bm{\gamma}_{\text{steer}}^{*}\|}=M_{\text{steer}}.\] (130)

This completes the proof. 

## Appendix F Proof of Theorem 6.3

To prove Theorem 6.3, we need first the following preliminaries.

**Lemma F.1**.: _If \(\mathcal{D}\) is \(G\)-invariant and linearly separable, then \(\mathcal{D}\) can be linearly separated by a \(G\)-invariant classifier. That is, there exists \(\bm{\beta}\in\mathbb{R}_{G}^{d_{0}}\) such that_

\[\mathbb{P}_{(\bm{x},y)\sim\mathcal{D}}\left[y\left\langle\bm{x},\bm{\beta} \right\rangle\geq 1\right]=1.\] (131)

Proof.: Since \(\mathcal{D}\) is linearly separable, there exists \(\bm{\beta}_{0}\in\mathbb{R}^{d_{0}}\) such that \(\mathbb{P}_{(\bm{x},y)\sim\mathcal{D}}\left[y\left\langle\bm{x},\bm{\beta}_{0 }\right\rangle\geq 1\right]=1\). Let \(\bm{\beta}=\frac{1}{|G|}\sum_{g\in G}g^{\top}\bm{\beta}_{0}\in\mathbb{R}_{G} ^{d_{0}}\), and we aim to show that \(\mathcal{D}\) can also be linearly separated by \(\bm{\beta}\). Indeed, for all \((\bm{x},y)\in\mathbb{R}^{d_{0}}\times\{\pm 1\}\),

\[y\left\langle\bm{x},\bm{\beta}\right\rangle<1 \implies y\left\langle\bm{x},\frac{1}{|G|}\sum_{g\in G}g^{\top}\bm{ \beta}_{0}\right\rangle<1 \implies y\left\langle\frac{1}{|G|}\sum_{g\in G}g\bm{x},\bm{\beta}_{0} \right\rangle<1\] (132) \[\implies \exists g\in G,\text{ s.t.}\ y\left\langle g\bm{x},\bm{\beta}_{0} \right\rangle<1.\] (133)

Let \(E=\{(\bm{x},y):y\left\langle\bm{x},\bm{\beta}_{0}\right\rangle<1\}\subset \mathbb{R}^{d_{0}}\times\{\pm 1\}\), then

\[\mathbb{P}_{(\bm{x},y)\sim\mathcal{D}}\left[y\left\langle\bm{x}, \bm{\beta}\right\rangle<1\right] \leq\sum_{g\in G}\mathbb{P}_{(\bm{x},y)\sim\mathcal{D}}\left[y \left\langle g\bm{x},\bm{\beta}_{0}\right\rangle<1\right]\] (134) \[=\sum_{g\in G}\mathcal{D}\left\{(\bm{x},y):(\rho_{0}(g)\otimes \text{Id})(\bm{x},y)\in E\right\}\] (135) \[=\sum_{g\in G}(\rho_{0}(g)\otimes\text{Id})_{\star}\mathcal{D} \left(E\right)=\sum_{g\in G}\mathcal{D}\left(E\right)\] (136) \[=\sum_{g\in G}\mathbb{P}_{(\bm{x},y)\sim\mathcal{D}}\left[y \left\langle\bm{x},\bm{\beta}_{0}\right\rangle<1\right]=0,\] (137)

where the second equality in (136) is due to \(\mathcal{D}\) being \(G\)-invariant. Therefore \(\mathcal{D}\) can be separated by \(\bm{\beta}\in\mathbb{R}_{G}^{d_{0}}\). 

**Definition F.2** (Empirical Rademacher complexity).: _Let \(\mathcal{F}\subset\mathbb{R}^{\mathcal{Z}}\) be a class of real-valued functions on \(\mathcal{Z}\), and let \(S=(\bm{\mathsf{z}}_{1},\cdots,\bm{\mathsf{z}}_{n})\) be a set of \(n\) samples from \(\mathcal{Z}\). The (empirical) Rademacher complexity \(\widehat{\mathcal{R}}_{n}(\mathcal{F},S)\) of \(\mathcal{F}\) over \(S\) is defined as_

\[\widehat{\mathcal{R}}_{n}(\mathcal{F},S)\coloneqq\frac{1}{n}\mathbb{E}_{\bm{ \sigma}\sim\{\pm 1\}^{n}}\left[\sup_{f\in\mathcal{F}}\sum_{i=1}^{n}\sigma_{i}f(\bm{ \mathsf{z}}_{i})\right],\] (138)

_where \(\bm{\sigma}=(\sigma_{1},\cdots,\sigma_{n})\) are the i.i.d. Rademacher variables satisfying \(\mathbb{P}[\sigma_{i}=1]=\mathbb{P}[\sigma_{i}=-1]=1/2\)._

**Lemma F.3** (Talagrand's Contraction Lemma, Lemma 8 in Mohri and Medina [2014]).: _Let \(\Phi_{i}:\mathbb{R}\to\mathbb{R}\), \(i\in[n]\), be \(1\)-Lipschitz functions, \(\mathcal{F}\subset\mathbb{R}^{\mathcal{Z}}\) be a function class, and \(S=(\mathbf{z}_{1},\ldots,\mathbf{z}_{n})\in\mathcal{Z}^{n}\) be a set of \(n\) samples from \(\mathcal{Z}\). Then_

\[\widehat{\mathcal{R}}_{n}(\mathcal{F},S)\geq\frac{1}{n}\mathbb{E}_{\bm{ \sigma}\sim\{\pm 1\}^{n}}\left[\sup_{f\in\mathcal{F}}\sum_{i=1}^{n}\sigma_{i}\Phi_{i} \circ f(\mathbf{z}_{i})\right].\] (139)

**Lemma F.4**.: _Let \(\mathcal{H}=\left\{\mathbf{x}\mapsto\left\langle\bm{\beta},\mathbf{x}\right\rangle :\bm{\beta}\in\mathbb{R}_{G}^{d_{0}},\left\|\bm{\beta}\right\|\leq B\right\}\) be the function space of \(G\)-invariant linear functions of bounded norm, and let \(S_{\mathbf{x}}=\left\{\mathbf{x}_{1},\cdots,\mathbf{x}_{n}\right\}\subset \mathbb{R}^{d_{0}}\) be a set of \(n\) samples from \(\mathbb{R}^{d_{0}}\). Then_

\[\widehat{\mathcal{R}}_{n}(\mathcal{H},S_{\mathbf{x}})\leq\frac{B\max_{i\in[n] }\left\|\overline{\mathbf{x}}_{i}\right\|}{\sqrt{n}}\] (140)

Proof.: By the definition of empirical Rademacher complexity (138), we have

\[n\widehat{\mathcal{R}}_{n}(\mathcal{H},S_{\mathbf{x}}) =\mathbb{E}_{\bm{\sigma}}\left[\sup_{\bm{\beta}\in\mathbb{R}_{G}^ {d_{0}},\left\|\bm{\beta}\right\|\leq B}\sum_{i=1}^{n}\sigma_{i}\left\langle \bm{\beta},\mathbf{x}_{i}\right\rangle\right]\] (141) \[=\mathbb{E}_{\bm{\sigma}}\left[\sup_{\bm{\beta}\in\mathbb{R}_{G}^ {d_{0}},\left\|\bm{\beta}\right\|\leq B}\sum_{i=1}^{n}\sigma_{i}\left\langle \bm{\beta},\overline{\mathbf{x}}_{i}\right\rangle\right]\] (142) \[=\mathbb{E}_{\bm{\sigma}}\sup_{\bm{\beta}\in\mathbb{R}_{G}^{d_{0 }},\left\|\bm{\beta}\right\|\leq B}\left\langle\bm{\beta},\sum_{i=1}^{n} \sigma_{i}\overline{\mathbf{x}}_{i}\right\rangle\] (143) \[=B\mathbb{E}_{\bm{\sigma}}\left\|\sum_{i=1}^{n}\sigma_{i} \overline{\mathbf{x}}_{i}\right\|\] (144) \[\leq B\left(\mathbb{E}_{\bm{\sigma}}\left\|\sum_{i=1}^{n}\sigma_ {i}\overline{\mathbf{x}}_{i}\right\|^{2}\right)^{1/2},\] (145)

where (142) is due to \(\bm{\beta}\in\mathbb{R}_{G}^{d_{0}}\). Since \(\sigma_{1},\cdots,\sigma_{n}\) are i.i.d., we have

\[\mathbb{E}_{\bm{\sigma}}\left\|\sum_{i=1}^{n}\sigma_{i}\overline{ \mathbf{x}}_{i}\right\|^{2} =\mathbb{E}_{\bm{\sigma}}\left[\sum_{i,j}\sigma_{i}\sigma_{j}\left\langle \overline{\mathbf{x}}_{i},\overline{\mathbf{x}}_{j}\right\rangle\right]\] (146) \[=\sum_{i\neq j}\left\langle\overline{\mathbf{x}}_{i},\overline{ \mathbf{x}}_{j}\right\rangle\mathbb{E}_{\bm{\sigma}}\sigma_{i}\sigma_{j}+\sum_ {i=1}^{n}\|\overline{\mathbf{x}}_{i}\|^{2}\mathbb{E}_{\bm{\sigma}}\sigma_{i}^{2}\] (147) \[=\sum_{i=1}^{n}\|\overline{\mathbf{x}}_{i}\|^{2}\leq n\max_{i\in[ n]}\|\overline{\mathbf{x}}_{i}\|^{2}.\] (148)

Eq. (145) combined with (148) completes the proof. 

We also need the following standard result on the generalization bound based on the Rademacher complexity [Shalev-Shwartz and Ben-David, 2014].

**Lemma F.5**.: _Let \(\mathcal{H}\) be a set of hypotheses, \(\mathcal{Z}=\mathbb{R}^{d_{0}}\times\{\pm 1\}\) be the set of labeled samples, and_

\[\ell:\mathcal{H}\times\mathcal{Z}\to[0,\infty),\quad(h,\mathbf{z})\mapsto\ell (h,\mathbf{z}),\] (149)

_be a loss function satisfying \(0\leq\ell(h,\mathbf{z})\leq c\) for all \(\mathbf{z}\in\mathcal{Z}\) and \(h\in\mathcal{H}\). Define the function class \(\mathcal{F}=\ell(\mathcal{H},\cdot):=\left\{\mathbf{z}\mapsto\ell\left(h, \mathbf{z}\right):h\in\mathcal{H}\right\}\). Then, for any \(\delta>0\) and any distribution \(\mathcal{D}\) on \(\mathcal{Z}\), we have with probability at least \(1-\delta\) over i.i.d. samples \(S=(\mathbf{z}_{1},\cdots,\mathbf{z}_{n})\sim\mathcal{D}^{n}\) that_

\[\sup_{h\in\mathcal{H}}\mathcal{L}_{\mathcal{D}}(h)-\mathcal{L}_{S}(h)\leq 2 \mathcal{R}_{n}(\mathcal{F})+c\sqrt{\frac{\log(1/\delta)}{2n}},\] (150)_where \(\mathcal{L}_{\mathcal{D}}(h)\coloneqq\mathbb{E}_{\bm{\mathrm{z}}\sim\mathcal{D}} \ell(h,\bm{\mathrm{z}})\) and \(\mathcal{L}_{S}(h)\coloneqq\frac{1}{n}\sum_{i=1}^{n}\ell(h,\bm{\mathrm{z}}_{i})\) are, respectively, the population and empirical loss of a hypothesis \(h\in\mathcal{H}\), and \(\mathcal{R}_{n}(\mathcal{F})\coloneqq\mathbb{E}_{S\sim\mathcal{D}^{n}} \widehat{\mathcal{R}}_{n}(\mathcal{F},S)\) is the Rademacher complexity of \(\mathcal{F}\)._

Finally, we can prove Theorem 6.3 restated below

**Theorem 6.3**.: _Let \(\mathcal{D}\) be a \(G\)-invariant distribution over \(\mathbb{R}^{d_{0}}\times\{\pm 1\}\) that is linearly separable by an invariant classifier \(\bm{\beta}_{0}\in\mathbb{R}^{d_{0}}_{G}\). Define_

\[\overline{R}=\inf\left\{r>0:\|\overline{\bm{\mathrm{x}}}\|\leq r\text{ with probability }1\right\}.\] (151)

_Let \(S=\left\{(\bm{\mathrm{x}}_{i},y_{i})\right\}_{i=1}^{n}\) be i.i.d. samples from \(\mathcal{D}\), and let \(\bm{\beta}_{\text{s}\text{t}\text{e}}^{\infty}\) be the limit direction of a steerable-network-parameterized linear predictor trained using gradient flow on \(S\). Then, for any \(\delta>0\), we have with probability at least \(1-\delta\) (over random samples \(S\sim\mathcal{D}^{n}\)) that_

\[\mathbb{P}_{(\bm{\mathrm{x}},y)\sim\mathcal{D}}\left[y\neq\text{sign}\left( \left\langle\bm{\mathrm{x}},\bm{\beta}_{\text{s}\text{t}\text{e}}^{\infty} \right\rangle\right)\right]\leq\frac{2\overline{R}\|\bm{\beta}_{0}\|}{\sqrt{n} }+\sqrt{\frac{\log(1/\delta)}{2n}}.\] (152)

Proof of Theorem 6.3.: Let \(\mathcal{H}=\left\{\bm{\beta}\in\mathbb{R}^{d_{0}}_{G}:\|\bm{\beta}\|\leq\| \bm{\beta}_{0}\|\right\}\) and with slight abuse of notation we identify \(\bm{\beta}\in\mathcal{H}\) with the invariant linear map \(\bm{\mathrm{x}}\mapsto\left\langle\bm{\mathrm{x}},\bm{\beta}\right\rangle\). According to Theorem 4.1, let \(\bm{\beta}^{*}=\tau\bm{\beta}_{\text{s}\text{t}\text{e}}^{\infty}\), \(\tau>0\), be the minimizer of (24). Since by assumption \(\bm{\beta}_{0}\) also satisfies the constraint in (24), we have \(\|\bm{\beta}^{*}\|\leq\|\bm{\beta}_{0}\|\), and hence \(\bm{\beta}^{*}\in\mathcal{H}\).

Consider the _ramp_ loss \(\ell\) defined as

\[\ell:\mathcal{H}\times(\mathbb{R}^{d_{0}}\times\{\pm 1\})\to[0,1],\quad(\bm{ \beta},(\bm{\mathrm{x}},y))\mapsto\min\{1,\max\{0,1-y\left\langle\bm{ \mathrm{x}},\bm{\beta}\right\rangle\}\}.\] (153)

It is easy to verify that \(\ell\) upper bounds the 0-1 loss: for all \(\bm{\beta}\in\mathcal{H}\) and \((\bm{\mathrm{x}},y)\in\mathbb{R}^{d_{0}}\times\{\pm 1\}\),

\[\ell(\bm{\beta},(\bm{\mathrm{x}},y))\geq\mathbbm{1}_{\{y\neq\text{sign}\left( \left\langle\bm{\mathrm{x}},\bm{\beta}_{\text{s}\text{e}}^{\infty}\right\rangle \right)\}}.\] (154)

This implies that

\[\mathbb{P}_{(\bm{\mathrm{x}},y)\sim\mathcal{D}}\left[y\neq\text{ sign}\left(\left\langle\bm{\mathrm{x}},\bm{\beta}_{\text{s}\text{e}}^{ \infty}\right\rangle\right)\right] =\mathbb{P}_{(\bm{\mathrm{x}},y)\sim\mathcal{D}}\left[y\neq\text{ sign}\left(\left\langle\bm{\mathrm{x}},\bm{\beta}^{*}\right\rangle \right)\right]\] (155) \[=\mathbb{E}_{(\bm{\mathrm{x}},y)\sim\mathcal{D}}\mathbbm{1}_{\{y \neq\text{sign}\left(\left\langle\bm{\mathrm{x}},\bm{\beta}^{*}\right\rangle \right)\}}\] (156) \[\leq\mathbb{E}_{(\bm{\mathrm{x}},y)\sim\mathcal{D}}\ell(\bm{ \beta}^{*},(\bm{\mathrm{x}},y))=\mathcal{L}_{\mathcal{D}}(\bm{\beta}^{*}).\] (157)

Since \(\mathcal{L}_{S}(\bm{\beta}^{*})\) is always 0 by definition (24) and \(\bm{\beta}^{*}\in\mathcal{H}\), we have by Lemma F.5 that

\[\mathbb{P}_{(\bm{\mathrm{x}},y)\sim\mathcal{D}}\left[y\neq\text{ sign}\left(\left\langle\bm{\mathrm{x}},\bm{\beta}_{\text{s}\text{e}}^{\infty} \right\rangle\right)\right] \leq\mathcal{L}_{\mathcal{D}}(\bm{\beta}^{*})=\mathcal{L}_{ \mathcal{D}}(\bm{\beta}^{*})-\mathcal{L}_{S}(\bm{\beta}^{*})\leq\sup_{\bm{ \beta}\in\mathcal{H}}\mathcal{L}_{\mathcal{D}}(\bm{\beta})-\mathcal{L}_{S}(\bm {\beta})\] (158) \[\leq 2\mathcal{R}_{n}(\mathcal{F})+\sqrt{\frac{\log(1/\delta)}{2n}},\] (159)

where \(\mathcal{F}=\ell(\mathcal{H},\cdot)=\{(\bm{\mathrm{x}},y)\mapsto\ell\left(\bm{ \beta},(\bm{\mathrm{x}},y)\right):\bm{\beta}\in\mathcal{H}\}\). Therefore, to prove Theorem 6.3 it suffices to show that \(\mathcal{R}_{n}(\mathcal{F})\leq\frac{\overline{R}\|\bm{\beta}_{0}\|}{\sqrt{n}}\). To this end, let \(S=\{(\bm{\mathrm{x}}_{i},y_{i}):i\in[n]\}\) be a set of \(n\) labeled samples, and let \(S_{\bm{\mathrm{x}}}=\{\bm{\mathrm{x}}_{i}:i\in[n]\}\) be the corresponding set of unlabeled inputs, then

\[\widehat{\mathcal{R}}_{n}(\mathcal{F},S) =\frac{1}{n}\mathbb{E}_{\bm{\sigma}}\sup_{f\in\mathcal{F}}\sum_{i=1 }^{n}\sigma_{i}f(\bm{\mathrm{x}}_{i},y_{i})\] (160) \[=\frac{1}{n}\mathbb{E}_{\bm{\sigma}}\sup_{\bm{\beta}\in\mathcal{H}} \sum_{i=1}^{n}\sigma_{i}\ell\left(\bm{\beta},(\bm{\mathrm{x}}_{i},y_{i})\right)\] (161) \[=\frac{1}{n}\mathbb{E}_{\bm{\sigma}}\sup_{\bm{\beta}\in\mathcal{H}} \sum_{i=1}^{n}\sigma_{i}\min\left\{1,\max\{0,Using Lemma F.4 and the fact that \(\overline{\mathbf{x}}\leq\overline{R}\) with probability 1 over \((\mathbf{x},y)\sim\mathcal{D}\), we arrive at

\[\mathcal{R}_{n}(\mathcal{F})=\mathbb{E}_{S\sim\mathcal{D}^{n}}\widehat{ \mathcal{R}}_{n}(\mathcal{F},S)\leq\mathbb{E}_{S\sim\mathcal{D}^{n}}\widehat{ \mathcal{R}}_{n}(\mathcal{H},S_{\mathbf{x}})\leq\frac{\overline{R}\|\bm{ \beta}_{0}\|}{\sqrt{n}}.\] (165)

This completes the proof of Theorem 6.3.