# ZipIt!: Multitask Model Merging _without Training_

 George Stoica1

Daniel Bolya1

Jakob Bjorner

**Pratik Ramesh**

**Taylor Hearn**

**Judy Hoffman**

Georgia Tech

{gstoica3,dbolya,jakob_bjorner,pramesh3,thearn6,judy}@gatech.edu

Equal Contribution. Code: https://github.com/gstoica27/ZipIt.

###### Abstract

We tackle the extremely difficult problem of combining distinct models with different initializations, each solving a separate task, into one multi-task model **without any additional training**. Prior work in model merging permutes one model to the space of the other then averages them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce "ZipIt!", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to allow for merging features _within_ each model by defining a general "zip" operation. Second, we add support for _partially zipping_ the models up until a specified layer, naturally creating a multi-head model. We find that these two changes combined account for a staggering 20-50% improvement over prior work,

## 1 Introduction

Combining multiple models into one _without training_ has recently started to gain traction in the vision community. Model Soups [1] can add multiple models finetuned from the same pretrained initialization to improve accuracy and robustness. Git Re-Basin [2] generalizes further to models trained on the same data but with different initializations, though with a significant accuracy drop. REPAIR [3] improves on Git Re-Basin by adding new parameters and adjusting model batch norms where applicable. However, all of these methods only combine models trained on the same task. In this paper, we take this line of work to a logical extreme: merging differently initialized models trained on _completely separate_ tasks (see Fig. 1ab). We show that this is an incredibly difficult problem for prior work and employ two simple strategies to make it feasible.

First, prior work focuses on _permuting_ one model to the other when merging them. This inherently assumes that most features _across_ them are correlated. But, this is not always the case for models trained on different tasks. Instead, we generalize model merging to support "zipping" any combination of correlated features _within_ and _across_ each model. We find that on some tasks, this alone improves accuracy **by up to 20%** vs. permutation-based approaches

Second, the features of models trained on disjoint tasks become less correlated over the course of the network [4]. Thus, we introduce _partial zipping_, which allows us to only "zip" up to a specified layer. Afterward, we feed the merged model's outputs to the remaining unmerged layers of the original networks, creating a multi-head model. Partially zipping can improve accuracy **by over 15%**.

ZipIt! (Fig. 1c) incorporates both strategies to "zip" models trained on different tasks into a single multitask model _without retraining_. ZipIt! is general and supports merging arbitrary models of the same architecture together (Sec. 3). We validate ZipIt! by merging models trained on different tasks (including classification and segmentation) into one, significantly outperforming prior work (Sec. 4).

## 2 Related Work

Model merging combines the weights of two or more models into a one. Our work differs from prior work in that we merge differently initialized models trained on disjoint tasks (Fig. 1) without training.

**Merging Finetuned Models.** If two models are finetuned from the same pretrained checkpoint, they often lie in the same error minima [5]. [6; 7; 8; 9; 10; 11] have exploited this property to average together the weights of a model at different stages of training. [12; 13; 14; 15; 16] use an "exponential moving average" of training checkpoints as a teacher for additional self-supervised learning. Other works, [1; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27] merge models fully finetuned to improve performance on a task. Our setting differs, as we do not assume the same initialization.

**Merging Differently Initialized Models.** Works in this space often merge models trained on the same task and rely on mode connectivity [28; 29; 30; 31; 32; 33; 34], as differently initialized models may not lie in the same error minima [29; 35]. Most recent work follows the intuition formalized by [35] that models permuted to the same loss minima can be merged by averaging their weights [35; 2; 3; 36]. Similar to ZipIt! [37] merges models of different tasks, but requires jointly finetuning on all tasks after each layer merge. As far as we are aware, we present the first _general method_ to successfully merge models trained on disjoint tasks _without additional training_.

## 3 ZipIt!

In this work, we treat model merging as combining the checkpoints (i.e., collection of weights) of multiple models layer-by-layer into a single checkpoint that can perform all the tasks of its constituents. Consider a model \(\mathcal{L}\) as a collection of layers \(L_{i}\in\mathcal{L}\), each of which may have some parameters (e.g., \(W_{i},b_{i}\) for a linear layer). Suppose \(L_{i}\in\mathcal{L}\) is a linear layer with parameters \(W_{i}\in\mathbb{R}^{n_{i}\times m_{i}},b_{i}\in\mathbb{R}^{n_{i}}\) with input features \(x\in\mathbb{R}^{m_{i}}\) and outputs features \(f_{i}\in\mathbb{R}^{n_{i}}\). Then, \(f_{i}=L_{i}(x)=W_{i}x+b_{i}\).

Our goal is to take \(L_{i}^{A}\in\mathcal{L}^{A}\) from a model A and \(L_{i}^{B}\in\mathcal{L}^{B}\) from a model B and merge them into a layer \(L_{i}^{*}\) that combines their feature spaces such that information from both \(f_{i}^{A}\) and \(f_{i}^{B}\) is retained in \(f_{i}^{*}\). We accomplish this by merging each layer of one model with the corresponding layer in the other, both merging features _across_ both layers and _within_ the same layer. This is in contrast to permutation-based merging method, which only combine features _across_ layers.

**Problems with Permutation.** Permutation methods posit that model B can be moved to the same loss minima as model A via permutation with high-likelihood [35]. However, this is unlikely when models are trained on different tasks because each model optimizes for task-specific minimas. In this case the optimal permutation of model B to model A lies in a strong minima on task B but _may not_ lie in a minima on task A, as shown in Fig. 2. This causes the interpolated model to perform worse than either of the two original models. Thus, we explore alternative merging methods.

**Why should we merge _within_?** Features of models trained on different tasks may be dissimilar, as the models solve different problems. Instead, those features may be more compatible with others within the same model, which would better retain performance when combined.

**What is our merge strategy?** Prior work obtains \(f_{i}^{*}\) by combining one feature from \(f_{i}^{A}\) and one from \(f_{i}^{B}\). [38; 3] determine which features to merge by computing the pairwise correlations between

Figure 1: **Setting and ZipIt!** (a) Prior work merges models from the **same** dataset with the **same** label sets: e.g., merging two models both trained to classify dog breeds. (b) Our setting expands to merging models from **different** datasets with **different** label sets: e.g., merging a model that classifies dog breeds with one that classifies bird species. (c) **ZipIt!** merges these models _without retraining_ by identifying shared features. Depending on the task, ZipIt! can nearly match ensemble performance.

the neuron activations of \(f_{i}^{A}\) and \(f_{i}^{B}\) over a set of images. In contrast, our approach can also obtain \(f_{i}^{*}\) by combining two features from just \(f_{i}^{A}\) or \(f_{i}^{B}\). We compute the same correlations, but also include those of \(f_{i}^{A}\) and \(f_{i}^{B}\) each with themselves over the same data. We then greedily choose without replacement the pairs with highest correlation and average their features to obtain \(f_{i}^{*}\).

**Merging a layer.** For every layer, we first concatenate \(f_{i}^{A}\) and \(f_{i}^{B}\) into \(f_{i}^{A}\|f_{i}^{B}\in\mathbb{R}^{2n_{i}}\), and define a "merge matrix" \(M_{i}\in\mathbb{R}^{n_{i}\times 2n_{i}}\) based on the matches from our greedy algorithm (i.e., \(f_{i}^{*}=M_{i}\left(f_{i}^{A}\|f_{i}^{B}\right)\)). Thus, \(M_{i}\) merges the output spaces of \(L_{i}^{A}\), and \(L_{i}^{B}\) into \(L_{i}^{*}\). We then define an 'unmerge" matrix \(U_{i}\in\mathbb{R}^{2n_{i}\times n_{i}}\) s.t. it _undoes_ the merge operation: \(U_{i}f_{i}^{*}\approx f_{i}^{A}\|f_{i}^{B}\). Applying \(U_{i}\) ensures the input spaces of the future layers \(L_{i+1}^{A}\), and \(L_{i+1}^{B}\) are aligned with current merged layer \(L_{i}^{*}\). Once all matrices are computed at each layer, we obtain \(L_{i}^{*}\) for a linear layer by

\[W_{i}^{*}=M_{i}\begin{pmatrix}W_{i}^{A}&0\\ 0&W_{i}^{B}\end{pmatrix}U_{i-1}\qquad b_{i}^{*}=M_{i}\begin{pmatrix}b_{i}^{A}\\ b_{i}^{B}\end{pmatrix}.\]

\(L_{i}^{*}\) is similarly computed on other layer-types (e.g., apply \(M_{i},U_{i-1}\) over the channels of convs.).

**Partial Zipping.** Sometimes, later layers in the networks have very uncorrelated (dissimilar) outputs. Forcibly zipping these would lead to meaningless features. In this case, we can perform a _partial zip_. That is, we zip most of the layers together, but leave the later ones _unzipped_--obtaining a multihead model.

**Merging Many Models (\(\alpha\)).** Sometimes, we'd like to merge more than two models together. To do this, we allow "repeated matches": we replace matched features from our algorithm with the resulting merged feature instead of removing them completely. To ensure that one feature doesn't get merged endlessly, we set the correlations of the new feature to be the minimum of the old features' similarities weighted by \(\alpha\in(0,1]\). We find a small value of \(\alpha\) typically works best.

**Within-Model Merging Budget (\(\beta\)).** To demonstrate the effectiveness of same-model merges, we introduce a "budget" parameter \(\beta\in[0,1]\) that denotes what percent of total merged features can come from models merging within themselves, with each model receiving an equal portion of this budget. A budget of 0 only allows feature merging across models and yields a permutation-merge.

## 4 Results

There's no standard benchmark to evaluate merging approaches on models from distinct tasks, so we construct our own. We evaluate our approach in two different settings. (1) A versatile test-bed: disjoint category splits of the same dataset (i.e., _same dataset and different label sets_). (2) A very challenging setting: completely different datasets and tasks (i.e., _different datasets and label sets_).

We compare to three baselines: Git Re-Basin [2], Weight Averaging (W. Avg) and Permute. W. Avg involves simply average all model parameters to be merged, while we design Permute to use linear sum assignment to find optimal permutations (following [38]) for merging. Note that Permute is a

Figure 2: **Task Loss Landscapes** for models in Tab. 0(b). Model A and Model B lie in low loss minimas for their own tasks, but _not for the other task_. Thus, any interpolation between Model A and a permuted Model B (e.g., Git Re-basin) lies outside the minima _for both tasks_ and thus performs poorly. In contrast, ZipIt! improves the merge by finding a model that lies in a loss minima for both.

_strong_ baseline we create and is more accurate than Git Re-Basin in our settings. For our method, ZipIt!a/m indicates that \(n\) out of the \(m\) layers in the network have been zipped (Sec. 3). Note, all our models have _different initializations_.

**Disjoint Category Splits.** In Tab. 0(a), we merge five pairs of models trained on disjoint 5 class subsets of CIFAR-10 using ResNet-20 with a \(4\times\) width multiplier (denoted as ResNet-20\(\times\)4). We train with a CLIP-style loss [39] using CLIP text encodings of the class names as targets so that both models output into the same CLIP-space regardless of the category (required for [2]). We report: (1) joint accuracy--the accuracy of each model over _all_ classes across datasets, and (2) per task accuracy--the accuracy of each task individually and also their average. Overall, ZipIt! performs a staggering 20.7% better than the nearest baseline. If we allow the last stage of the network to remain unzipped (i.e., zip up to 13 layers), our method obtains 83.8%, which is only 3.6% behind an ensemble of model A and model B (which is practically the upper bound for this setting). We find similar results on disjoint 50 class splits of CIFAR-100 in Tab. 0(b) using an \(8\times\) width multiplier. ZipIt! again significantly outperforms baselines.

**Different Datasets.** We merge ResNet-50 models trained on: Stanford Dogs [40], Oxford Pets [41], CUB200 [42], and NABirds [43]. In Tab. 2, we show the average per task accuracy from exhaustively merging each pair and the much more difficult setting of merging all four at once. We report the accuracy of our baselines by applying them up until the last layer, but we can't compare to [2] as it requires shared output space.

For pairs of models, ZipIt! slightly outperforms our permute baseline across all tasks and performs similarly when merging all 4 models at once. However, if we add capacity to the merged model through partial zipping, we perform up to 33% better on merging pairs and 50% better on merging all four models than the permute baseline.

We also merge the ResNet-50 backbone of a DeeplabV3 [44] segmentation model that achieves 76.8% mIoU on PASCAL VOC [45] with an ImageNet-1k [46] classification model that obtains 77.8% accuracy. ZipIt! can still achieve good performance on _both_ tasks even with half of layers merged: obtaining 64.4% mIoU on PASCAL VOC and 60.9% accuracy on ImageNet-1k.

## 5 Conclusion

In this paper, we tackle the extremely difficult task of merging models trained on completely disjoint tasks _without additional training_. We find that prior work underperforms in this setting and posit that they neither fully (1) exploit model similarities nor (2) account for model dissimilarities. We introduce ZipIt!, a general framework for merging models that addresses these issues, and show it to significantly outperform prior work across several difficult settings.

\begin{table}
\begin{tabular}{l c|c c c c}  & & & & Accuracies (\%) & & \\ Method & FLOPs (G) & Joint & Task A & Task B & Avg \\ \hline Model A & 0.68 & 48.2\(\pm\)0.0 & 97.0\(\pm\)0.0 & 45.1\(\pm\)0.0 & 71.0\(\pm\)0.0 \\ Model B & 0.68 & 48.4\(\pm\)0.0 & 94.1\(\pm\)0.0 & 72.6\(\pm\)0.0 & \\ \hline W. Avg & 0.48\(\pm\)0.0 & 34.1\(\pm\)0.0 & 51.1\(\pm\)0.0 & 60.8\(\pm\)0.0 & \\ Git Re-Basin2 & 0.68 & 46.2\(\pm\)0.0 & 76.8\(\pm\)0.0 & 82.7\(\pm\)0.0 & 79.8\(\pm\)0.0 \\ Permute & 0.68 & 58.4\(\pm\)0.0 & 86.6\(\pm\)0.0 & 87.4\(\pm\)0.0 & 87.4\(\pm\)0.0 \\ \hline ZipIt!a & 0.68 & **79.1\(\pm\)0.0** & 92.9\(\pm\)0.0 & 91.1\(\pm\)0.0 & **92.1\(\pm\)0.0** \\ Ensemble & 1.37 & 87.4\(\pm\)0.0 & 97.0\(\pm\)0.0 & 96.1\(\pm\)0.0 & 96.6\(\pm\)0.0 \\ ZipIt!a & 0.91 & **83.8\(\pm\)**0.0 & **95.1\(\pm\)**0.0 & **94.1\(\pm\)**0.0 & **94.6\(\pm\)0.0** \\ \end{tabular}
\end{table}
Table 1: **CIFAR Results.** ZipIt! vs. baselines on combining a model trained on half the classes (Task A) with one trained on the other half (Task B) _without extra training_. We report both joint (10/100-way) and per-task (5/50-way) accuracy. ZipIt! _significantly_ outperforms its baseline and closes in on the upper bound (ensemble accuracy). \(\ddagger\) refers to [2].

\begin{table}
\begin{tabular}{l c c c c c c}  & & & & & & & \\ Method & FLOPs (G) & SD & OP & CUB & NAB & Avg \\ \hline \hline W. Avg & 4.11 & 12.9 & 18.2 & 13.9 & 0.2 & 11.3 \\ Permute & 4.11 & 46.2 & 47.6 & 35.6 & **13.5** & 35.7 \\ ZipIt!a & 4.11 & **46.9** & **50.7** & **38.0** & **12.7** & **37.1** \\ \hline Ensemble & 8.22 & 77.2 & 81.1 & 71.0 & 77.2 & 73.5 \\ ZipIt!a & 6.39 & 62.6 & 71.2 & 62.8 & 53.0 & 62.4 \\ ZipIt!a & 7.42 & **66.5** & **75.8** & **65.6** & **66.8** & **68.7** \\ \end{tabular} 
\begin{tabular}{l c c c c c}  & & & & & & \\ \hline \hline W. Avg & 4.12 & 0.8 & 3.0 & 0.6 & 0.3 & 1.2 \\ Permute & 4.12 & 15.7 & 26.1 & **14.0** & **5.3** & 15.3 \\ ZipIt!a & 4.12 & **21.1** & **33.3** & **8.6** & **3.9** & **16.8** \\ \hline ZipIt!a & 16.4 & 72.7 & 81.2 & 71.0 & 77.2 & 75.8 \\ ZipIt!a & 11.0 & 50.2 & 55.9 & **44.0** & **32.0** & **45.5** \\ ZipIt!a & 14.1 & **63.5** & **70.8** & **63.7** & **63.1** & **65.3** \\ \end{tabular}
\end{table}
Table 2: **Multi-Dataset Results.** Merging ResNet-50 models trained on _completely different datasets_: Stanford Dogs (SD), Oxford Pets (OP), CUB200 (CUB), and NABirds (NAB). We report average per-task accuracy over merging model pairs, and all four.

## References

* [1] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _ICML_, 2022.
* [2] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. _arXiv:2209.04836_, 2022.
* [3] Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Enetzari, and Behnam Neyshabur. Repair: Renormalizing permuted activations for interpolation repair. _arXiv:2211.08403_, 2022.
* [4] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In _ICML_, 2019.
* [5] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? _NeurIPS_, 2020.
* [6] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. _arXiv:1704.00109_, 2017.
* [7] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. _UAI_, 2018.
* [8] Johannes Von Oswald, Seijin Kobayashi, Joao Sacramento, Alexander Meulemans, Christian Henning, and Benjamin F Grewe. Neural networks with late-phase weights. _arXiv:2007.12927_, 2020.
* [9] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In _CVPR_, 2022.
* [10] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. _NeurIPS_, 2022.
* [11] Shachar Don-Ychiya, Elad Venezian, Colin Raffel, Noam Slonim, and Leshem Choshen. ColD fusion: Collaborative descent for distributed multitask finetuning. Association for Computational Linguistics, 2023.
* [12] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. _NeurIPS_, 2017.
* [13] Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes, Zhuowen Tu, and Stefano Soatto. Exponential moving average normalization for self-supervised and semi-supervised learning. In _CVPR_, 2021.
* [14] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _NeurIPS_, 2020.
* [15] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _ICCV_, 2021.
* [16] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. In _ICML_, 2022.
* [17] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_. PMLR, 2017.
* [18] Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. Fusing finetuned models for better pretraining. _arXiv:2204.03044_, 2022.

* Rame et al. [2022] Alexandre Rame, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, Leon Bottou, and David Lopez-Paz. Recycling diverse models for out-of-distribution generalization. _arXiv:2212.10445_, 2022.
* Ashmore and Gashler [2015] Stephen Ashmore and Michael Gashler. A method for finding similarity between multi-layer perceptrons by forward bipartite alignment. In _IJCNN_, 2015.
* Yurochkin et al. [2019] Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In _ICML_, 2019.
* Wang et al. [2020] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. _arXiv:2002.06440_, 2020.
* Ilharco et al. [2022] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. _arXiv:2212.04089_, 2022.
* Gueta et al. [2023] Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen. Knowledge is a region in weight space for fine-tuned language models. _arXiv:2302.04863_, 2023.
* Yadav et al. [2023] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Resolving interference when merging models. _arXiv:2306.01708_, 2023.
* Sung et al. [2023] Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, and Lijuan Wang. An empirical study of multimodal model merging, 2023.
* Matena and Raffel [2021] Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging. _arXiv:2111.09832_, 2021.
* Freeman and Bruna [2016] C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. _arXiv:1611.01540_, 2016.
* Garipov et al. [2018] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. _NeurIPS_, 2018.
* Draxler et al. [2018] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In _ICML_, 2018.
* Frankle et al. [2020] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In _ICML_, 2020.
* Tatro et al. [2020] Norman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, and Rongjie Lai. Optimizing mode connectivity via neuron alignment. _NeurIPS_, 2020.
* Singh and Jaggi [2020] Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. _NeurIPS_, 2020.
* Liu et al. [2022] Chang Liu, Chenfei Lou, Runzhong Wang, Alan Yuhan Xi, Li Shen, and Junchi Yan. Deep neural network fusion via graph matching with applications to model ensemble and federated learning. In _ICML_, 2022.
* Entezari et al. [2021] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. _arXiv:2110.06296_, 2021.
* Pena et al. [2022] Fidel A Guerrero Pena, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric Granger, and Marco Pedersoli. Re-basin via implicit sinkhorn differentiation. _arXiv:2212.12042_, 2022.
* He et al. [2018] Xiaoxi He, Zimu Zhou, and Lothar Thiele. Multi-task zipping via layer-wise neuron sharing. _NeurIPS_, 2018.
* Li et al. [2015] Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent Learning: Do different neural networks learn the same representations? _arXiv:1511.07543_, 2015.

* [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.
* [40] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for fine-grained image categorization: Stanford dogs. In _CVPR Workshop on Fine-Grained Visual Categorization (FGVC)_, 2011.
* [41] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In _CVPR_, 2012.
* [42] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.
* [43] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In _CVPR_, 2015.
* [44] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. _arXiv preprint arXiv:1706.05587_, 2017.
* [45] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. _International Journal of Computer Vision_, 88(2):303-338, June 2010.
* [46] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.
* [47] Thomas Uriot and Dario Izzo. Safe crossover of neural networks through neuron alignment. In _GECCO_, 2020.
* [48] Aditya Kumar Akash, Sixu Li, and Nicolas Garcia Trillos. Wasserstein barycenter-based model fusion and linear mode connectivity of neural networks. _arXiv:2210.06671_, 2022.
* [49] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Communications of the ACM_, 2017.
* [50] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In _CVPR_, 2017.
* [51] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In _ICASSP_, 2017.
* [52] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In _ICCV_, 2017.
* [53] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In _ICCV_, 2019.
* [54] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In _ICCV_, 2019.
* [55] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In _ECCV_, 2022.
* [56] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _CVPR_, 2018.
* [57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.
* [58] Zhizhong Li and Derek Hoiem. Learning without forgetting. _TPAMI_, 2017.
* [59] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. _TPAMI_, 2021.

* [60] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. _TKDE_, 2022.
* [61] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016.
* [62] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [63] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. _arXiv:1803.03635_, 2018.
* [64] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv:2010.11929_, 2020.
* [65] Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Singan: Learning a generative model from a single natural image. In _ICCV_, 2019.
* [66] J MacQueen. Classification and analysis of multivariate observations. In _5th Berkeley Symp. Math. Statist. Probability_, 1967.
* [67] Louis L McQuitty. Elementary linkage analysis for isolating orthogonal and oblique types and typal relevancies. _Educational and psychological measurement_, 1957.
* [68] Wei-Hong Li and Hakan Bilen. Knowledge distillation for multi-task learning. In _ECCV_, 2020.
* [69] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _NeurIPS_, 2020.
* [70] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In _CVPR_, 2022.
* [71] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. _arXiv:2302.05442_, 2023.
* [72] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _PNAS_, 2017.
* [73] Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant feature representation. In _ICML_, 2013.
* [74] Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. In _NeurIPS_, 2011.
* [75] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [76] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.
* [77] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. _NeurIPS_, 2015.
* [78] Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. Exploring network structure, dynamics, and function using networkx. In _Proceedings of the 7th Python in Science Conference_, 2008.
* [79] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. _ICLR_, 2023.

* [80] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax for incremental learning. In _ICCV_, 2021.
* [81] Berlin Simsek, Francois Ged, Arthur Jacot, Francesco Spadaro, Clement Hongler, Wulfram Gerstner, and Johanni Brea. Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances. In _Proceedings of the 38th International Conference on Machine Learning_. PMLR, 2021.
* [82] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks (invited paper). In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, 2021.