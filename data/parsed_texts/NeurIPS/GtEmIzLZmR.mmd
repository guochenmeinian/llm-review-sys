# Achievable Fairness on Your Data With Utility Guarantees

 Muhammad Faaiz Taufiq

ByteDance Research

faaiz.taufiq@bytedance.com &Jean-Francois Ton

ByteDance Research

jeanfrancois@bytedance.com &Yang Liu

University of California Santa Cruz

yangliu@ucsc.edu

Corresponding authors: faaiz.taufiq@bytedance.com and jeanfrancois@bytedance.com

###### Abstract

In machine learning fairness, training models that minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off inherently depends on dataset characteristics such as dataset imbalances or biases and therefore, using a uniform fairness requirement across diverse datasets remains questionable. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Crucially, we introduce a novel methodology for quantifying uncertainty in our estimates, thereby providing practitioners with a robust framework for auditing model fairness while avoiding false conclusions due to estimation errors. Our experiments spanning tabular (e.g., Adult), image (CelebA), and language (Jigsaw) datasets underscore that our approach not only reliably quantifies the optimum achievable trade-offs across various data modalities but also helps detect suboptimality in SOTA fairness methods.

## 1 Introduction

A key challenge in fairness for machine learning is to train models that minimize disparity across various sensitive groups such as race or gender [9; 35; 10]. This often comes at the cost of reduced model accuracy, a phenomenon termed accuracy-fairness trade-off [36; 32]. This trade-off can differ significantly across datasets, depending on factors such as dataset biases, imbalances etc. [1; 8; 11].

To demonstrate how these trade-offs are inherently dataset-dependent, we consider a simple example involving two distinct crime datasets. Dataset A has records from a community where crime rates are uniformly distributed across all racial groups, whereas Dataset B comes from a community where historical factors have resulted in a disproportionate crime rate among a specific racial group. Intuitively, training models which are racially agnostic is more challenging for Dataset B, due to the unequal distribution of crime rates across racial groups, and will result in a greater loss in model accuracy as compared to Dataset A.

This example underscores that setting a uniform fairness requirement across diverse datasets (such as requiring the fairness violation metric to be below 10% for both datasets), while also adhering to essential accuracy benchmarks is impractical. Therefore, choosing fairness guidelines for anydataset necessitates careful consideration of its individual characteristics and underlying biases. In this work, we advocate against the use of one-size-fits-all fairness mandates by proposing a nuanced, dataset-specific framework for quantifying acceptable range of accuracy-fairness trade-offs. To put it concretely, the question we consider is:

_Given a dataset, what is the range of permissible fairness violations corresponding to each accuracy threshold for models in a given class \(\mathcal{H}\)?_

This question can be addressed by considering the optimum accuracy-fairness trade-off, which shows the minimum fairness violation achievable for each level of accuracy. Unfortunately, this curve is typically unavailable and hence, various optimization techniques have been proposed to approximate this curve, ranging from regularization [8; 33] to adversarial learning [45; 40].

However, approximating the trade-off curve using these aforementioned methods has some serious limitations. Firstly, these methods require retraining hundreds if not thousands of models to obtain a good approximation of the trade-off curve, making them computationally infeasible for large datasets or models. Secondly, these works do not account for finite-sampling errors in the obtained curve. This is problematic since the empirical trade-off evaluated over a finite dataset may not match the exact trade-off over the full data distribution.

We illustrate this phenomenon in Figure 1 where the black and red trade-off curves are obtained using the same model but evaluated over two different test data draws. Here, relying solely on the estimated curves without accounting for the uncertainty could lead us to make the incorrect conclusion that the methodology used to obtain the black trade-off curve is sub-optimal (compared to the red curve) as it achieves a higher fairness violation for accuracies in the range \([0.62,0.66]\). However, this discrepancy arises solely due to finite-sampling errors.

In this paper, we address these challenges by introducing a computationally efficient method of approximating the optimal accuracy-fairness trade-off curve, supported by rigorous statistical guarantees. Our methodology not only circumvents the need to train multiple models (leading to at least a 10-fold reduction in computational cost) but is also the first to quantify the uncertainty in the estimated curve, arising from both, finite-sampling error as well as estimation error. To achieve this, our approach adopts a novel probabilistic perspective and provides guarantees that remain valid across all finite-sample draws. This also allows practitioners to distinguish if an apparent suboptimality in a baseline could be explained by finite-sampling errors (as in the black curve in Figure 1), or if it stems from genuine deficiencies in the fairness interventions applied (as in the blue curve).

The contributions of this paper are three-fold:

Figure 1: Accuracy-fairness trade-offs for COMPAS dataset (on held-out data). The black and red curves are obtained using the same optimally trained model evaluated on different splits. The blue curve is obtained using a suboptimally trained model. The green area depicts the range of permissible fairness violations for each accuracy, pink area shows suboptimal accuracy-fairness trade-offs, and blue area shows unlikely-to-be-achieved ones. (Details in Appendix F.5)

* We present a computationally efficient methodology for approximating the accuracy-fairness trade-off curve by training only a single model. This is achieved by adapting a technique from [15] called You-Only-Train-Once (YOTO) to the fairness setting.
* To account for the approximation and finite-sampling errors, we introduce a novel technical framework to construct confidence intervals (using the trained YOTO model) which contain the optimal accuracy-fairness trade-off curve with statistical guarantees. For any accuracy threshold \(\psi\) chosen at _inference time_, this gives us a statistically backed range of permissible fairness violations \([l(\psi),u(\psi)]\), allowing us to answer our previously posed question: _Given a dataset, the permissible range of fairness violations corresponding to an accuracy threshold of \(\psi\) is \([l(\psi),u(\psi)]\) for models in a given class \(\mathcal{H}\)._
* Lastly, we showcase the vast applicability of our method empirically across various data modalities including tabular, image and text datasets. We evaluate our framework on a suite of SOTA fairness methods and show that our intervals are both reliable and informative.

## 2 Preliminaries

**Notation** Throughout this paper, we consider a binary classification task, where each training sample is composed of triples, \((X,A,Y)\). \(X\in\mathcal{X}\) denotes a vector of features, \(A\in\mathcal{A}\) indicates a discrete sensitive attribute, and \(Y\in\mathcal{Y}\coloneqq\{0,1\}\) represents a label. To make this more concrete, if we take loan default prediction as the classification task, \(X\) represents individuals' features such as their income level and loan amount; \(A\) represents their racial identity; and \(Y\) represents their loan default status. Having established the notation, for completeness, we provide some commonly used fairness violations \(\Phi_{\text{fair}}(h)\in[0,1]\) for a classifier model \(h:\mathcal{X}\rightarrow\mathcal{Y}\) when \(\mathcal{A}=\{0,1\}\):

**Demographic Parity (DP)** The DP condition states that the selection rates for all sensitive groups are equal, i.e. \(\mathbb{P}(h(X)=1\mid A=a)=\mathbb{P}(h(X)=1)\) for any \(a\in\mathcal{A}\). The absolute DP violation is:

\[\Phi_{\text{DP}}(h)\coloneqq|\mathbb{P}(h(X)=1\mid A=1)-\mathbb{P}(h(X)=1\mid A =0)|.\]

**Equalized Opportunity (EOP)** The EOP condition states that the true positive rates for all sensitive groups are equal, i.e. \(\mathbb{P}(h(X)=1\mid A=a,Y=1)=\mathbb{P}(h(X)=1\mid Y=1)\) for any \(a\in\mathcal{A}\). The absolute EOP violation is:

\[\Phi_{\text{EOP}}(h)\coloneqq |\mathbb{P}(h(X)=1\mid A=1,Y=1)-\mathbb{P}(h(X)=1\mid A=0,Y=1)|.\]

### Problem setup

Next, we formalise the notion of _accuracy-fairness trade-off_, which is the main quantity of interest in our work. For a model class \(\mathcal{H}\) (e.g., neural networks) and a given accuracy threshold \(\psi\in[0,1]\), we define the optimal accuracy-fairness trade-off \(\tau^{*}_{\text{fair}}(\psi)\) as,

\[\tau^{*}_{\text{fair}}(\psi)\coloneqq\min_{h\in\mathcal{H}}\Phi_{\text{fair}} (h)\quad\text{subject to}\quad\text{acc}(h)\geq\psi.\] (1)

Here, \(\Phi_{\text{fair}}(h)\) and \(\text{acc}(h)\) denote the fairness violation and accuracy of \(h\) over the _full data distribution_. For an accuracy \(\psi^{\prime}\) which is unattainable, we define \(\tau^{*}_{\text{fair}}(\psi^{\prime})=1\) and we focus on models \(\mathcal{H}\) trained using gradient-based methods. Crucially, our goal is not to estimate the trade-off at a fixed accuracy level, but instead to reliably and efficiently estimate the _entire_ trade-off curve \(\tau^{*}_{\text{fair}}\). In contrast, previous works [1; 11] impose an apriori fairness constraint during training and therefore each trained model only recovers one point on the trade-off curve corresponding to this pre-specified constraint.

If available, this trade-off curve would allow practitioners to characterise exactly how, for a given dataset, the minimum fairness violation varies as model accuracy increases. This not only provides a principled way of selecting data-specific fairness requirements, but also serves as a tool to audit if a model meets acceptable fairness standards by checking if its accuracy-fairness trade-off lies on this curve. Nevertheless, obtaining this ground-truth trade-off curve exactly is impossible within the confines of a finite-sample regime (owing to finite-sampling errors). This means that even if baseline A's empirical trade-off evaluated on a finite dataset is suboptimal compared to the empirical trade-off of baseline B, this does not necessarily imply suboptimality on the full data distribution.

We illustrate this in Figure 1 where both red and black trade-off curves are obtained using the same model but evaluated on different test data splits. Here, even though the black curve appears suboptimal compared to the red curve for accuracies in \([0.62,0.66]\), this apparent suboptimality is solely due to finite-sampling errors (since the discrepancy between the two curves arises only due to different evaluation datasets). If we rely only on comparing empirical trade-offs, we would incorrectly flag the methodology used to obtain the black curve as suboptimal.

To address this, we construct confidence intervals (CIs), shown as the green region in Figure 1, that account for such finite-sampling errors. In this case both trade-offs fall within our CIs which correctly indicates that this apparent suboptimality could stem from finite-sample variability. Conversely, a baseline's trade-off falling above our CIs (as in the blue curve in Figure 1) offers a confident assessment of suboptimality, as this cannot be explained away by finite-sample variability. Therefore, our CIs equip practitioners with a robust auditing tool. They can confidently identify suboptimal baselines while avoiding false conclusions caused by considering empirical trade-offs alone.

High-level road mapTo achieve this, our proposed methodology adopts a two-step approach:

1. Firstly, we propose _loss-conditional fairness training_, a computationally efficient methodology of estimating the entire trade-off curve \(\tau^{*}_{\text{fair}}\) by training a single model, obtained by adapting the YOTO framework [15] to the fairness setting.
2. Secondly, to account for the approximation and finite-sampling errors in our estimates, we introduce a novel methodology of constructing confidence intervals on the trade-off curve \(\tau^{*}_{\text{fair}}\) using the trained YOTO model. Specifically, given \(\alpha\in(0,1)\), we construct confidence intervals \(\Gamma^{\alpha}_{\text{fair}}\subseteq[0,1]\) which satisfy guarantees of the form: \[\mathbb{P}(\tau^{*}_{\text{fair}}(\Psi)\in\Gamma^{\alpha}_{\text{fair}})\geq 1-\alpha.\] Here, \(\Gamma^{\alpha}_{\text{fair}}\) and \(\Psi\in[0,1]\) are random variables obtained using a held-out calibration dataset \(\mathcal{D}_{\text{cal}}\) (see Section 3.2) and the probability is taken over different draws of \(\mathcal{D}_{\text{cal}}\).

## 3 Methodology

First, we demonstrate how our 2-step approach offers a practical and statistically sound method for estimating \(\tau^{*}_{\text{fair}}(\psi)\). Figure 1 provides an illustration of our proposed confidence intervals (CIs) \(\Gamma^{\alpha}_{\text{fair}}\) and shows how they can be interpreted as a range of 'permissible' values of accuracy-fairness trade-offs (the green region). Specifically, if for a classifier \(h_{0}\), the accuracy-fairness pair \((\text{acc}(h_{0}),\Phi_{\text{fair}}(h_{0}))\) lies above the CIs \(\Gamma^{\alpha}_{\text{fair}}\) (i.e., the pink region in Figure 1), then \(h_{0}\) is likely to be suboptimal in terms of the fairness violation, i.e., there likely exists \(h^{\prime}\in\mathcal{H}\) with \(\text{acc}(h^{\prime})\geq\text{acc}(h_{0})\) and \(\Phi_{\text{fair}}(h^{\prime})\leq\Phi_{\text{fair}}(h_{0})\). On the other hand, it is unlikely for any model \(h^{\prime}\in\mathcal{H}\) to achieve a trade-off below the CIs \(\Gamma^{\alpha}_{\text{fair}}\) (the blue region in Figure 1). Next, we outline how to construct such intervals.

### Step 1: Efficient estimation of trade-off curve

The first step of constructing the intervals is to approximate the trade-off curve by recasting the problem into a constrained optimization objective. The optimization problem formulated in Eq. (1) is however, often too complex to solve, because the accuracy \(\text{acc}(h)\) and fairness violations \(\Phi_{\text{fair}}(h)\) are both non-smooth [1]. These constraints make it hard to use standard optimization methods that rely on gradients [25]. To get around this issue, previous works [1; 8] replace the non-smooth constrained optimisation problem with a smooth surrogate loss. Here, we consider parameterized family of classifiers \(\mathcal{H}=\{h_{\theta}:\mathcal{X}\rightarrow\mathbb{R}\,|\,\theta\in\Theta\}\) (such as neural networks) trained using the regularized loss:

\[\mathcal{L}_{\lambda}(\theta)=\mathbb{E}[l_{\text{CE}}(h_{\theta}(X),Y)]+ \lambda\,\mathcal{L}_{\text{fair}}(h_{\theta}).\] (2)

where, \(l_{\text{CE}}\) is the cross-entropy loss for the classifier \(h_{\theta}\) and \(\mathcal{L}_{\text{fair}}(h_{\theta})\) is a smooth relaxation of the fairness violation \(\Phi_{\text{fair}}\)[8; 29]. For example, when the fairness violation is DP, [8] consider

\[\mathcal{L}_{\text{fair}}(h_{\theta})=\mathbb{E}[g(h_{\theta}(X))\;|\;A=1]- \mathbb{E}[g(h_{\theta}(X))\;|\;A=0],\]

for different choices of \(g(x)\), including the identity and sigmoid functions. We include more examples of such regularizers in Appendix F.3. The parameter \(\lambda\) in \(\mathcal{L}_{\lambda}\) modulates the accuracy-fairness trade-off with lower values of \(\lambda\) favouring higher accuracy over reduced fairness violation.

Now that we defined the optimization objective, obtaining the trade-off curve becomes straightforward by simply optimizing multiple models over a grid of regularization parameters \(\lambda\). However, training multiple models can be computationally expensive, especially when this involves large-scale models (e.g. neural networks). To circumvent this computational challenge, we introduce loss-conditional fairness training obtained by adapting the YOTO framework proposed by [15].

#### 3.1.1 Loss-conditional fairness training

As we describe above, a popular approach for approximating the accuracy-fairness trade-off \(\tau^{*}_{\text{fair}}(\psi)\) involves training multiple models \(h_{\theta^{*}_{\text{iter}}}\) over a discrete grid of \(\lambda\) hyperparameters with the regularized loss \(\mathcal{L}_{\lambda}\). To avoid the computational overhead of training multiple models, [15] propose 'You Only Train Once' (YOTO), a methodology of training one model \(h_{\theta}:\mathcal{X}\times\Lambda\to\mathbb{R}\), which takes \(\lambda\in\Lambda\subseteq\mathbb{R}\) as an additional input using Feature-wise Linear Modulation (FiLM) [34] layers. YOTO is trained such that at inference time \(h_{\theta}(\cdot,\lambda^{\prime})\) recovers the classifier obtained by minimising \(\mathcal{L}_{\lambda^{\prime}}\) in Eq. (2).

Recall that we are interested in minimising the family of losses \(\mathcal{L}_{\lambda}\), parameterized by \(\lambda\in\Lambda\) (Eq. (2)). Instead of fixing \(\lambda\), YOTO solves an optimisation problem where the parameter \(\lambda\) is sampled from a distribution \(P_{\lambda}\). As a result, during training the model observes many values of \(\lambda\) and learns to optimise the loss \(\mathcal{L}_{\lambda}\) for all of them simultaneously. At inference time, the model can be conditioned on a chosen value \(\lambda^{\prime}\) and recovers the model trained to optimise \(\mathcal{L}_{\lambda^{\prime}}\). Hence, once adapted to our setting, the YOTO loss becomes:

\[\operatorname*{arg\,min}_{h_{\theta}:\mathcal{X}\times\Lambda\to\mathbb{R}} \mathbb{E}_{\lambda\sim P_{\lambda}}\left[\mathbb{E}[l_{\text{CE}}(h_{\theta}(X,\lambda),Y)]+\lambda\,\mathcal{L}_{\text{fair}}(h_{\theta}(\cdot,\lambda)) \right].\]

Having trained a YOTO model, the trade-off curve \(\tau^{*}_{\text{fair}}(\psi)\) can be approximated by simply plugging in different values of \(\lambda\) at inference time and thus avoiding additional training. From a theoretical point of view, [15, Proposition 1] proves that under the assumption of large enough model capacity, training the loss-conditional YOTO model performs as well as the separately trained models while only requiring a single model. Although the model capacity assumption might be hard to verify in practice, our experimental section has shown that the trade-off curves estimates \(\widehat{\tau^{*}_{\text{fair}}(\psi)}\) obtained using YOTO are consistent with the ones obtained using separately trained models.

It should be noted, as is common in optimization problems, that the estimated trade-off curve \(\widehat{\tau^{*}_{\text{fair}}(\psi)}\) may not align precisely with the true trade-off curve \(\tau^{*}_{\text{fair}}(\psi)\). This discrepancy originates from two key factors. Firstly, the limited size of the training and evaluation datasets introduces errors in the estimation of \(\widehat{\tau^{*}_{\text{fair}}(\psi)}\). Secondly, we opt for a computationally tractable loss function instead of the original optimization problem in Eq. (1). This may result in our estimation \(\widehat{\tau^{*}_{\text{fair}}(\psi)}\) yielding sub-optimal trade-offs, as can be seen from Figure 1. Therefore, to ensure that our procedure yields statistically sound inferences, we next construct confidence intervals using the YOTO model, designed to contain the true trade-off curve \(\tau^{*}_{\text{fair}}(\psi)\) with high probability.

### Step 2: Constructing confidence intervals

As mentioned above, our goal here is to use our trained YOTO model to construct confidence intervals (CIs) for the optimal trade-off curve \(\tau^{*}_{\text{fair}}(\psi)\) defined in Eq. (1). Specifically, we assume access to a held-out _calibration_ dataset \(\mathcal{D}_{\text{cal}}\coloneqq\{(X_{i},A_{i},Y_{i})\}_{i}\) which is disjoint from the training data. Given a level \(\alpha\in[0,1]\), we construct CIs \(\Gamma^{\alpha}_{\text{fair}}\subseteq[0,1]\) using \(\mathcal{D}_{\text{cal}}\), which provide guarantees of the form:

\[\mathbb{P}(\tau^{*}_{\text{fair}}(\Psi)\in\Gamma^{\alpha}_{\text{fair}})\geq 1 -\alpha.\] (3)

Here, it is important to note that \(\Psi\in[0,1]\) and \(\Gamma^{\alpha}_{\text{fair}}\) are random variables obtained from the calibration data \(\mathcal{D}_{\text{cal}}\), and the guarantee in Eq. (3) holds marginally over \(\Psi\) and \(\Gamma^{\alpha}_{\text{fair}}\). While our CIs in this section require the availability of the sensitive attributes in \(\mathcal{D}_{\text{cal}}\), in Appendix D we also extend our methodology to the setting where sensitive attributes are missing. In this section, for notational convenience we use \(h_{\lambda}(\cdot)\) to denote the YOTO model \(h_{\theta}(\cdot,\lambda)\) for \(\lambda\in\Lambda\).

The uncertainty in our trade-off estimate arises, in part, from the uncertainty in the accuracy and fairness violations of our trained model. Therefore, our methodology of constructing CIs on \(\tau^{*}_{\text{fair}}\), involves first constructing CIs on test accuracy \(\text{acc}(h_{\lambda})\) and fairness violation \(\Phi_{\text{fair}}(h_{\lambda})\) for a given value of \(\lambda\) using \(\mathcal{D}_{\text{cal}}\), denoted as \(C^{\alpha}_{\text{acc}}(\lambda)\) and \(C^{\alpha}_{\text{fair}}(\lambda)\) respectively satisfying,

\[\mathbb{P}(\text{acc}(h_{\lambda})\in C^{\alpha}_{\text{acc}}(\lambda))\geq 1 -\alpha,\quad\text{and}\quad\mathbb{P}(\Phi_{\text{fair}}(h_{\lambda})\in C^{ \alpha}_{\text{fair}}(\lambda))\geq 1-\alpha.\]

One way to construct these CIs involves using assumption-light concentration inequalities such as Hoeffding's inequality. To be more concrete, for the accuracy \(\text{acc}(h_{\lambda})\):

**Lemma 3.1** (Hoeffding's inequality).: _Given a classifier \(h_{\lambda}:\mathcal{X}\to\mathcal{Y}\), we have that,_

\[\mathbb{P}\left(\text{acc}(h_{\lambda})\in\left[\widetilde{\text{acc}(h_{ \lambda})}-\delta,\widetilde{\text{acc}(h_{\lambda})}+\delta\right]\right)\geq 1 -\alpha.\]

_Here, \(\widetilde{\text{acc}(h)}\coloneqq\sum_{(X_{i},A_{i},Y_{i})\in\mathcal{D}_{ \text{cal}}}\frac{1(h(X_{i})=Y_{i})}{|\mathcal{D}_{\text{cal}}|}\) and \(\delta\coloneqq\sqrt{\frac{1}{2|\mathcal{D}_{\text{cal}}|}\log\left(\frac{2}{ \alpha}\right)}\)._Lemma 3.1 illustrates that when can use Hoeffding's inequality to construct confidence interval \(C_{\text{acc}}^{\alpha}(\lambda)=\widehat{\text{acc}(h_{\lambda})}-\delta, \widetilde{\text{acc}(h_{\lambda})}+\delta]\) on \(\text{acc}(h_{\lambda})\) such that the true \(\text{acc}(h_{\lambda})\) will lie inside the CI with probability \(1-\alpha\). Analogously, we also construct CIs for fairness violations, \(\Phi_{\text{fair}}(h_{\lambda})\), although this is subject to additional nuanced challenges, which we address using a novel sub-sampling based methodology in Appendix B. Once we have CIs over \(\text{acc}(h_{\lambda})\) and \(\Phi_{\text{fair}}(h_{\lambda})\) for a model \(h_{\lambda}\), we next outline how to use these to derive CIs for the minimum achievable fairness \(\tau_{\text{fair}}^{*}\), satisfying Eq. (3). We proceed by explaining how to construct the upper and lower CIs separately, as the latter requires additional considerations regarding the trade-off achieved by YOTO.

#### 3.2.1 Upper confidence intervals

We first outline how to obtain one-sided upper confidence intervals on the optimum accuracy-fairness trade-off \(\tau_{\text{fair}}^{*}(\Psi)\) of the form \(\Gamma_{\text{fair}}^{\alpha}=[0,U_{\text{fair}}^{\lambda}]\), which satisfies the probabilistic guarantee in Eq. (3). To this end, given a classifier \(h_{\lambda}\in\mathcal{H}\), our methodology involves constructing one-sided lower CI on the accuracy \(\text{acc}(h_{\lambda})\) and upper CI on the fairness violation \(\Phi_{\text{fair}}(h_{\lambda})\). We make this concrete below:

**Proposition 3.2**.: _Given \(h_{\lambda}\in\mathcal{H}\), let \(L_{\text{acc}}^{\lambda},U_{\text{fair}}^{\lambda}\in[0,1]\) be lower and upper CIs on \(\text{acc}(h_{\lambda})\) and \(\Phi_{\text{fair}}(h_{\lambda})\), i.e._

\[\mathbb{P}\left(\text{acc}(h_{\lambda})\geq L_{\text{acc}}^{\lambda}\right) \geq 1-\alpha/2,\quad\text{and}\quad\mathbb{P}(\Phi_{\text{fair}}(h_{ \lambda})\leq U_{\text{fair}}^{\lambda})\geq 1-\alpha/2.\]

_Then, \(\mathbb{P}\left(\tau_{\text{fair}}^{*}(L_{\text{acc}}^{\lambda})\leq U_{\text {fair}}^{\lambda}\right)\geq 1-\alpha\)._

Proposition 3.2 shows that for any model \(h_{\lambda}\), the upper CI on model fairness, \(U_{\text{fair}}^{\lambda}\), provides a valid upper CI for the trade-off value at \(L_{\text{acc}}^{\lambda}\), i.e. \(\tau_{\text{fair}}^{*}(L_{\text{acc}}^{\lambda})\). This can be used to construct upper CIs on \(\tau_{\text{fair}}^{*}(\psi)\) for a given accuracy level \(\psi\). To understand how this can be achieved, we first find \(\lambda\in\Lambda\) such that the lower CI on the accuracy of model \(h_{\lambda}\), \(L_{\text{acc}}^{\lambda}\), satisfies \(L_{\text{acc}}^{\lambda}\geq\psi\). Then, since by definition \(\tau_{\text{fair}}^{*}\) is a monotonically increasing function, we know that \(\tau_{\text{fair}}^{*}(L_{\text{acc}}^{\lambda})\geq\tau_{\text{fair}}^{*}(\psi)\). Since Proposition 3.2 tells us that \(U_{\text{fair}}^{\lambda}\) is an upper CI for \(\tau_{\text{fair}}^{*}(L_{\text{acc}}^{\lambda})\), it follows that \(U_{\text{fair}}^{\lambda}\) is also a valid upper CI for \(\tau_{\text{fair}}^{*}(\psi)\).

Intuitively, Proposition 3.2 provides the 'worst-case' optimal trade-off, accounting for finite-sample uncertainty. It is important to note that this result does not rely on any assumptions regarding the optimality of the trained classifiers. This means that the upper CIs will remain valid even if the YOTO classifier \(h_{\lambda}\) is not trained well (and hence achieves sub-optimal accuracy-fairness trade-offs), although in such cases the CI may be conservative.

Having explained how to construct upper CIs on \(\tau_{\text{fair}}^{*}(\psi)\), we next move on to the lower CIs.

#### 3.2.2 Lower confidence intervals

Obtaining lower confidence intervals on \(\tau_{\text{fair}}^{*}(\psi)\) is more challenging than obtaining upper confidence intervals. We begin by explaining at an intuitive level why this is the case.

Suppose that \(h_{\lambda}\in\mathcal{H}\) is such that \(\text{acc}(h_{\lambda})=\psi\), then since \(\tau_{\text{fair}}^{*}\) denotes the minimum attainable fairness violation (Eq. (1)), we have that \(\tau_{\text{fair}}^{*}(\psi)\leq\Phi_{\text{fair}}(h_{\lambda})\). Therefore, any valid upper confidence interval on \(\Phi_{\text{fair}}(h_{\lambda})\) will also be valid for \(\tau_{\text{fair}}^{*}(\psi)\). However, a lower bound on \(\Phi_{\text{fair}}(h_{\lambda})\) cannot be used as a lower bound for the minimum achievable fairness \(\tau_{\text{fair}}^{*}(\psi)\) in general. A valid lower CI for \(\tau_{\text{fair}}^{*}(\psi)\) will therefore depend on the gap between fairness violation achieved by \(h_{\lambda}\), \(\Phi_{\text{fair}}(h_{\lambda})\), and minimum achievable fairness violation \(\tau_{\text{fair}}^{*}(\psi)\) (i.e., \(\Delta(h_{\lambda})\) term in Figure 1(a)). We make this concrete by constructing lower CIs depending on \(\Delta(h_{\lambda})\) explicitly.

**Proposition 3.3**.: _Given \(h_{\lambda}\in\mathcal{H}\), let \(U_{\text{acc}}^{\lambda},L_{\text{fair}}^{\lambda}\in[0,1]\) be upper and lower CIs on \(\text{acc}(h_{\lambda})\) and \(\Phi_{\text{fair}}(h_{\lambda})\), i.e._

\[\mathbb{P}(\text{acc}(h_{\lambda})\leq U_{\text{acc}}^{\lambda})\geq 1-\alpha/2, \quad\text{and}\quad\mathbb{P}(\Phi_{\text{fair}}(h_{\lambda})\geq L_{\text{ fair}}^{\lambda})\geq 1-\alpha/2.\]

_Then, \(\mathbb{P}\left(\tau_{\text{fair}}^{*}(U_{\text{acc}}^{\lambda})\geq L_{\text{ fair}}^{\lambda}-\Delta(h_{\lambda})\right)\geq 1-\alpha\), where \(\Delta(h_{\lambda})\coloneqq\Phi_{\text{fair}}(h_{\lambda})-\tau_{\text{fair}}^{*}( \text{acc}(h_{\lambda}))\geq 0\)._

Proposition 3.3 can be used to derive lower CIs on \(\tau_{\text{fair}}^{*}(\psi)\) at a specified accuracy level \(\psi\), using a methodology analogous to that described in Section 3.2.1. Intuitively, this result provides the 'best-case' optimal trade-off, accounting for finite-sample uncertainty. However, unlike the upper CI, the lower CI includes the \(\Delta(h_{\lambda})\) term, which is typically unknown. To circumvent this, we propose a strategy for obtaining plausible approximations for \(\Delta(h_{\lambda})\) in practice in the following section.

#### 3.2.3 Sensitivity analysis for \(\Delta(h_{\lambda})\)

Recall that \(\Delta(h_{\lambda})\) quantifies the difference between the fairness loss of classifier \(h_{\lambda}\) and the minimum attainable fairness loss \(\tau^{*}_{\text{fair}}(\operatorname{acc}(h_{\lambda}))\), and is an unknown quantity in general (see Figure 1(a)). Here, we propose a practical strategy for positing values for \(\Delta(h_{\lambda})\) which encode our belief on how close the fairness loss \(\Phi_{\text{fair}}(h_{\lambda})\) is to \(\tau^{*}_{\text{fair}}(\operatorname{acc}(h_{\lambda}))\). This allows us to construct CIs which not only incorporate finite-sampling uncertainty from calibration data, but also account for the possible sub-optimality in the trade-offs achieved by \(h_{\lambda}\). The main idea behind our approach is to calibrate \(\Delta(h_{\lambda})\) using additional separately trained standard models without imposing significant computational overhead.

DetailsOur sensitivity analysis uses \(k\) additional models \(\mathcal{M}\coloneqq\{h^{(1)},h^{(2)},\dots,h^{(k)}\}\subseteq\mathcal{H}\) trained separately using the standard regularized loss \(\mathcal{L}_{\lambda^{\prime}}\) (Eq. (2)) for some randomly chosen values of \(\lambda^{\prime}\). Let \(\mathcal{M}_{0}\subseteq\mathcal{M}\) denote the models which achieve a better empirical trade-off than the YOTO model on \(\mathcal{D}_{\text{cal}}\), i.e. the empirical trade-offs for models in \(\mathcal{M}_{0}\) lie below the YOTO trade-off curve (see Figure 1(b)). We choose \(\Delta(h_{\lambda})\) for our YOTO model to be the maximum gap between empirical trade-offs of these separately trained models in \(\mathcal{M}_{0}\) and the YOTO model. It can be seen from Proposition 3.3 that, in practice, this will result in a downward shift in the lower CI until all the separately trained models in \(\mathcal{M}\) lie above the lower CI. As a result, our methodology yields increasingly conservative lower CIs as the number of additional models \(|\mathcal{M}|\) increases.

Even though the procedure above requires training additional models \(\mathcal{M}\), it does not impose the same computational overhead as training models over the full range of \(\lambda\) values. We show empirically in Section 5 that in practice 2 models are usually sufficient to obtain informative and reliable intervals. Additionally, we also show that when YOTO achieves the optimal trade-off (i.e., \(\Delta(h_{\lambda})=0\)), our sensitivity analysis leaves the CIs unchanged, thereby preventing unnecessary conservatism.

Asymptotic analysis of \(\Delta(h_{\lambda})\)While the procedure described above provides a practical solution for obtaining plausible approximations for \(\Delta(h_{\lambda})\), we next present a theoretical result which provides reassurance that this gap should become negligible as the number of training data \(\mathcal{D}_{\text{tr}}\) increases.

**Theorem 3.4**.: _Let \(\widehat{\Phi_{\text{fair}}(h^{\prime})},\widehat{\operatorname{acc}(h^{ \prime})}\) denote the fairness violation and accuracy for \(h^{\prime}\in\mathcal{H}\) evaluated on training data \(\mathcal{D}_{\text{tr}}\) and let_

\[h\coloneqq\arg\min_{h^{\prime}\in\mathcal{H}}\widehat{\Phi_{\text{fair}}(h^{ \prime})}\text{ subject to }\widehat{\operatorname{acc}(h^{\prime})}\geq\delta.\] (4)

_Then, given \(\eta\in(0,1)\), under standard regularity assumptions, we have that with probability at least \(1-\eta\), \(\Delta(h)\leq\widetilde{O}(|\mathcal{D}_{\text{tr}}|^{-\gamma}),\) for some \(\gamma\in(0,1/2]\) where \(\widetilde{O}(\cdot)\) suppresses dependence on \(\log{(1/\eta)}\)._

Theorem 3.4 shows that as the training data size \(|\mathcal{D}_{\text{tr}}|\) increases, the error term \(\Delta(h_{\lambda})\) will become negligible with a high probability for any model \(h_{\lambda}\) which minimises the empirical training loss in Eq. (4). In this case, \(\Delta(h_{\lambda})\) should not have a significant impact on the lower CIs in Proposition 3.3 and the CIs will reflect the uncertainty in \(\tau^{*}_{\text{fair}}\) arising mostly due to finite calibration data. We also verify this empirically in Appendix F.7. It is worth noting that Theorem 3.4 relies on the same assumptions as used in Theorem 2 in [1], which have been provided in Appendix A.2.

Figure 2: Visual illustrations for \(\Delta(h)\) (Figure 1(a)) and our sensitivity analysis procedure (Figure 1(b)).

## 4 Related works

Many previous fairness methods in the literature, termed in-processing methods, introduce constraints or regularization terms to the optimization objective. For instance, [1; 10] impose a priori uniform constraints on model fairness at training time. However, given the data-dependent nature of accuracy-fairness trade-offs, setting a uniform fairness threshold may not be suitable. Other in-processing methods [37; 24] consider information-theoretic bounds on the optimal trade-off in infinite data limit, independent of a specific model class. While these works offer valuable theoretical insights, there is no guarantee that these frontiers are attainable by models within a given model class \(\mathcal{H}\). We verify this empirically in Appendix F.6 by showing that, for the Adult dataset, the frontiers proposed in [24] are not achieved by any SOTA method we considered. In contrast, our method provides guarantees on the _achievable_ trade-off curve within realistic constraints of model class and data availability.

Various other regularization approaches [39; 33; 8; 14; 41; 42; 43] have also been proposed, but these often necessitate training multiple models, making them computationally intensive. Alternative strategies include learning 'fair' representations [44; 30; 31], or re-weighting data based on sensitive attributes [18; 22]. These, however, provide limited control over accuracy-fairness trade-offs.

Besides this, post-processing methods [19; 38] enforce fairness after training but can lead to other forms of unfairness such as disparate treatment of similar individuals [16]. Moreover, many post-hoc approaches such as [3; 2] still require solving different optimisation problems for different fairness thresholds. Other methods such as [46; 27] involve learning a post-hoc module in addition to the base classifier. As a result, the computational cost of training the YOTO model is similar to (and in many cases lower than) the combined cost of training a base model and subsequently applying a post-processing intervention to this pre-trained classifier. We confirm this empirically in Section 5.

## 5 Experiments

In this section, we empirically validate our methodology of constructing confidence intervals on the fairness trade-off curve, across diverse datasets with neural networks as model class \(\mathcal{H}\). These datasets range from tabular (Adult and COMPAS ), to image-based (CelebA), and natural language processing datasets (Jigsaw). Recall that our approach involves two steps: initial estimation of the trade-off via the YOTO model, followed by the construction of CIs using calibration data \(\mathcal{D}_{\text{cal}}\).

Figure 3: Results on four real-world datasets where \(\mathcal{D}_{\text{cal}}\) is a 10% data split. Here, \(\alpha=0.05\) and we use \(|\mathcal{M}|=2\) separately trained models for sensitivity analysis.

To evaluate our methodology, we implement a suite of baseline algorithms including SOTA in-processing techniques such as regularization-based approaches [8], a SOTA kernel-density based method [12] (denoted as 'KDE-fair'), as well as the reductions method [1]. Additionally, we also compare against adversarial fairness techniques [45] and a post-processing approach (denoted as 'RTO') [2] and consider the three most prominent fairness metrics: Demographic Parity (DP), Equalized Odds (EO), and Equalized Opportunity (EOP). We provide additional details and results in Appendix F, where we also consider a synthetic setup with tractable \(\tau^{*}_{\text{fair}}\). The code to reproduce our experiments is provided at github.com/faairf/DatasetFairness.

### Results

Figure 3 shows the results for different datasets and fairness violations, obtained using a 10% data split as calibration dataset \(\mathcal{D}_{\text{cal}}\). For each dataset, we construct 4 CIs that serve as the upper and lower bounds on the optimal accuracy-fairness trade-off curve. These intervals are computed at a 95% confidence level using various methodologies, including 1) Hoeffding's, 2) Bernstein's inequalities which both offer finite sample guarantees as well as, 3) bootstrapping [17], and 4) asymptotic intervals based on the Central Limit Theorem [26] which are valid asymptotically. There are 4 key takeaways:

**Takeaway 1: Trade-off curves are data dependent.** The results in Figure 3 confirm that the accuracy-fairness trade-offs can vary significantly across the datasets. For example, achieving near-perfect fairness (i.e. \(\Phi_{\text{fair}}(h)\approx 0\)) seems significantly easier for the Jigsaw dataset than the COMPAS dataset, even as the accuracy increases. Likewise, for Adult and COMPAS, the DP increases gradually with increasing accuracy, whereas for CelebA, the increase is sharp once the accuracy increases above 90%. Therefore, using a uniform fairness threshold across datasets [as in 1] may be too restrictive, and our methodology provides more dataset-specific insights about the entire trade-off curve instead.

**Takeaway 2: Our CIs are both reliable and informative.** Recall that, any trade-off which lies above our upper CIs is guaranteed to be sub-optimal with probability \(1-\alpha\), thereby enabling practitioners to effectively distinguish between genuine sub-optimalities and those due to finite-sample errors. Table 1 lists the proportion of sub-optimal empirical trade-offs for each baseline and provides a principled comparison of the baselines. For example, the adversarial, RTO and logsig baselines have a significantly higher proportion of sub-optimal trade-offs than the KDE-fair and separate baselines.

On the other hand, the validity of our lower CIs depends on the optimality of our YOTO model and the lower CIs may be too tight if YOTO is sub-optimal. Therefore, for the lower CIs to be reliable, it must be unlikely for any baseline to achieve a trade-off below the lower CIs. Table 1 confirms this empirically, as the proportion of models which lie below the lower CIs is negligible. In Appendix E, we also account for the uncertainty in baseline trade-offs when assessing the optimality, hence yielding more robust inferences. The results remain similar to those in Table 1.

**Takeaway 3: YOTO trade-offs are consistent with SOTA.** We observe that the YOTO trade-offs align well with most of the SOTA baselines considered while reducing the computational cost by approximately 40-fold (see the final column of Table 1). In some cases, YOTO even achieves a better trade-off than the baselines considered. See, e.g., the Jigsaw dataset results (especially for EOP). Moreover, we observe that the baselines yield empirical trade-offs which have a high variance as

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Category & Baseline & Unlikely & Permissable & Sub-optimal & \(\approx\) Training time \\ \hline  & adversary [45] & \(0.03\pm 0.03\) & \(0.51\pm 0.07\) & \(0.45\pm 0.07\) & \(100\) min\(\times 40\) \\  & logging [8] & \(0.0\pm 0.0\) & \(0.66\pm 0.1\) & \(0.33\pm 0.1\) & \(100\) min\(\times 40\) \\  & reductions [1] & \(0.0\pm 0.0\) & \(0.79\pm 0.1\) & \(0.21\pm 0.1\) & \(90\) min\(\times 40\) \\ In-processing & linear [8] & \(0.01\pm 0.0\) & \(0.85\pm 0.05\) & \(0.14\pm 0.06\) & \(100\) min\(\times 40\) \\  & KDE-fair [12] & \(0.0\pm 0.0\) & \(0.97\pm 0.05\) & \(0.03\pm 0.06\) & \(85\) min\(\times 40\) \\  & separate & \(0.0\pm 0.0\) & \(0.98\pm 0.01\) & \(0.02\pm 0.02\) & \(100\) min\(\times 40\) \\  & YOTO (Ours) & \(0.0\pm 0.0\) & \(1.0\pm 0.0\) & \(0.0\pm 0.0\) & **105 min\(\times 1\)** \\ \hline Post-processing & RTO [2] & \(0.0\pm 0.0\) & \(0.65\pm 0.2\) & \(0.35\pm 0.05\) & 
\begin{tabular}{c} 95 min (training base classifier) \\ + 10min\(\times 40\) (post-hoc optimisations) \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
Table 1: Proportion of empirical trade-offs for each baseline in the three trade-off regions, aggregated across all datasets and fairness metrics (using Bernstein’s CIs). ‘Unlikely’, ‘Permissible’ and ‘Sub-optimal’ correspond to the blue, green and pink regions in Figure 1 respectively. The last column shows the rough average training time per model across experiments \(\times\) no. of models per experiment.

accuracy increases (see Jigsaw results in Figure 3, for example). This behaviour starkly contrasts the smooth variations exhibited by our YOTO-generated trade-off curves along the accuracy axis.

**Takeaway 4: Sensitivity analysis does not cause unnecessary conservatism.** We use 2 randomly chosen separately trained models to perform our sensitivity analysis for Figure 3. We find that this only causes a shift in lower CIs for 2 out of the 12 trade-off curves presented (i.e. for DP and EO trade-offs on the Adult dataset), leaving the rest of the CIs unchanged. Therefore, in practice sensitivity analysis does not impose significant computational overhead, and only changes the CIs when YOTO achieves a suboptimal trade-off. Additional results have been included in Appendix C.

## 6 Discussion and Limitations

In this work, we propose a computationally efficient approach to capture the accuracy-fairness trade-offs inherent to individual datasets, backed by sound statistical guarantees. Our proposed methodology enables a nuanced and dataset-specific understanding of the accuracy-fairness trade-offs. It does so by obtaining confidence intervals on the accuracy-fairness trade-off, leveraging the computational benefits of the You-Only-Train-Once (YOTO) framework [15]. This empowers practitioners with the ability to, at inference time, specify desired accuracy levels and promptly receive corresponding permissible fairness ranges. By eliminating the need for repetitive model training, we significantly streamline the process of obtaining accuracy-fairness trade-offs tailored to individual datasets.

**Limitations** Despite the evident merits of our approach, it also has some limitations. Firstly, our methodology requires distinct datasets for training and calibration, posing difficulties when data is limited. Under such constraints, the YOTO model might not capture the optimal accuracy-fairness trade-off, and moreover, the resulting confidence intervals could be overly conservative. Secondly, our lower CIs incorporate an unknown term \(\Delta(h_{\lambda})\). While we propose sensitivity analysis for approximating this term and prove that it is asymptotically negligible under certain mild assumptions in Section 3.2.3, a more exhaustive understanding remains an open question. Exploring informative upper bounds for \(\Delta(h_{\lambda})\) under weaker conditions is a promising avenue for future investigations.

## Acknowledgments

We would like to express our gratitude to Sahra Ghalebikesabi for her valuable feedback on an earlier draft of this paper. We also thank the anonymous reviewers for their thoughtful and constructive comments, which enhanced the clarity and rigor of our final submission.

## References

* Agarwal et al. [2018] A. Agarwal, A. Beygelzimer, M. Dudik, J. Langford, and H. Wallach. A reductions approach to fair classification. 03 2018.
* Alabdulmohsin and Lucic [2021] I. M. Alabdulmohsin and M. Lucic. A near-optimal algorithm for debiasing trained machine learning models. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 8072-8084. Curran Associates, Inc., 2021.
* Alghamdi et al. [2022] W. Alghamdi, H. Hsu, H. Jeong, H. Wang, P. W. Michalak, S. Asoodeh, and F. P. Calmon. Beyond adult and compas: Fairness in multi-class prediction, 2022.
* Angelopoulos et al. [2023] A. N. Angelopoulos, S. Bates, C. Fannjiang, M. I. Jordan, and T. Zrnic. Prediction-powered inference, 2023.
* Angwin et al. [2016] J. Angwin, J. Larson, S. Mattu, and L. Kirchner. Machine bias, 2016.
* Bartlett and Mendelson [2001] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. In D. Helmbold and B. Williamson, editors, _Computational Learning Theory_, pages 224-240, Berlin, Heidelberg, 2001. Springer Berlin Heidelberg.
* Becker and Kohavi [1996] B. Becker and R. Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20.

* [8] H. Bendekgey and E. B. Sudderth. Scalable and stable surrogates for flexible classifiers with fairness constraints. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* [9] S. Caton and C. Haas. Fairness in machine learning: A survey. _CoRR_, abs/2010.04053, 2020.
* [10] L. E. Celis, L. Huang, V. Keswani, and N. K. Vishnoi. Classification with fairness constraints: A meta-algorithm with provable guarantees. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_, FAT* '19, page 319-328, New York, NY, USA, 2019. Association for Computing Machinery.
* [11] L. E. Celis, L. Huang, V. Keswani, and N. K. Vishnoi. Fair classification with noisy protected attributes: A framework with provable guarantees. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 1349-1361. PMLR, 18-24 Jul 2021.
* [12] J. Cho, G. Hwang, and C. Suh. A fair classifier using kernel density estimation. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 15088-15099. Curran Associates, Inc., 2020.
* [13] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. _CoRR_, abs/1810.04805, 2018.
* [14] M. Donini, L. Oneto, S. Ben-David, J. S. Shawe-Taylor, and M. Pontil. Empirical risk minimization under fairness constraints. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [15] A. Dosovitskiy and J. Djolonga. You only train once: Loss-conditional training of deep networks. In _International Conference on Learning Representations_, 2020.
* [16] EEOC. Uniform guidelines on employee selection procedures, 1979.
* [17] B. Efron. Bootstrap methods: Another look at the jackknife. _The Annals of Statistics_, 7(1):1-26, 1979.
* [18] A. Grover, J. Song, A. Agarwal, K. Tran, A. Kapoor, E. Horvitz, and S. Ermon. _Bias Correction of Learned Generative Models Using Likelihood-Free Importance Weighting_. Curran Associates Inc., Red Hook, NY, USA, 2019.
* [19] M. Hardt, E. Price, and N. Srebro. Equality of opportunity in supervised learning. In _Proceedings of the 30th International Conference on Neural Information Processing Systems_, NIPS'16, page 3323-3331, Red Hook, NY, USA, 2016. Curran Associates Inc.
* [20] Jigsaw and Google. Unintended bias in toxicity classification, 2019.
* [21] S. M. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, _Advances in Neural Information Processing Systems_, volume 21. Curran Associates, Inc., 2008.
* [22] F. Kamiran and T. Calders. Data pre-processing techniques for classification without discrimination. _Knowledge and Information Systems_, 33, 10 2011.
* [23] J. S. Kim, J. Chen, and A. Talwalkar. Fact: A diagnostic for group fairness trade-offs. _CoRR_, abs/2004.03424, 2020.
* [24] J. S. Kim, J. Chen, and A. Talwalkar. Model-agnostic characterization of fairness trade-offs. _CoRR_, abs/2004.03424, 2020.
* [25] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [26] L. Le Cam. The central limit theorem around 1935. _Statistical Science_, 1(1):78-91, 1986.

* [27] E. Z. Liu, B. Haghgoo, A. S. Chen, A. Raghunathan, P. W. Koh, S. Sagawa, P. Liang, and C. Finn. Just train twice: Improving group robustness without training group information. _CoRR_, abs/2107.09044, 2021.
* [28] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_, December 2015.
* [29] M. Lohaus, M. Perrot, and U. V. Luxburg. Too relaxed to be fair. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 6360-6369. PMLR, 13-18 Jul 2020.
* [30] C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel. The variational fair autoencoder, 2017.
* [31] K. Lum and J. Johndrow. A statistical framework for fair predictive algorithms, 2016.
* [32] N. Martinez, M. Bertran, and G. Sapiro. Minimax pareto fairness: A multi objective perspective. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 6755-6764. PMLR, 13-18 Jul 2020.
* [33] M. Olfat and Y. Mintz. Flexible regularization approaches for fairness in deep learning. In _2020 59th IEEE Conference on Decision and Control (CDC)_, pages 3389-3394, 2020.
* [34] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. C. Courville. Film: Visual reasoning with a general conditioning layer. _CoRR_, abs/1709.07871, 2017.
* [35] B. Ustun, Y. Liu, and D. Parkes. Fairness without harm: Decoupled classifiers with preference guarantees. In K. Chaudhuri and R. Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 6373-6382. PMLR, 09-15 Jun 2019.
* [36] A. Valdivia, J. Sanchez-Monedero, and J. Casillas. How fair can we go in machine learning? assessing the boundaries of accuracy and fairness. _International Journal of Intelligent Systems_, 36(4):1619-1643, 2021.
* [37] H. Wang, L. He, R. Gao, and F. Calmon. Aleatoric and epistemic discrimination: Fundamental limits of fairness interventions. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [38] D. Wei, K. N. Ramamurthy, and F. Calmon. Optimized score transformation for fair classification. In S. Chiappa and R. Calandra, editors, _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108 of _Proceedings of Machine Learning Research_, pages 1673-1683. PMLR, 26-28 Aug 2020.
* [39] S. Wei and M. Niethammer. The fairness-accuracy pareto front. _Statistical Analysis and Data Mining: The ASA Data Science Journal_, 15(3):287-302, 2022.
* [40] J. Yang, A. A. S. Soltan, D. W. Eyre, Y. Yang, and D. A. Clifton. An adversarial training framework for mitigating algorithmic biases in clinical machine learning. _npj Digital Medicine_, 6:55, 2023.
* [41] M. Zafar, I. Valera, M. Rodriguez, and K. P. Gummadi. Fairness constraints: A mechanism for fair classification. 07 2015.
* [42] M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In _Proceedings of the 26th International Conference on World Wide Web_, WWW '17, page 1171-1180, Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences Steering Committee.
* [43] M. B. Zafar, I. Valera, M. Gomez-Rodriguez, and K. P. Gummadi. Fairness constraints: A flexible approach for fair classification. _Journal of Machine Learning Research_, 20(75):1-42, 2019.

* [44] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In S. Dasgupta and D. McAllester, editors, _Proceedings of the 30th International Conference on Machine Learning_, volume 28 of _Proceedings of Machine Learning Research_, pages 325-333, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR.
* [45] B. H. Zhang, B. Lemoine, and M. Mitchell. Mitigating unwanted biases with adversarial learning. In _Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society_, AIES '18, page 335-340, New York, NY, USA, 2018. Association for Computing Machinery.
* [46] A. Tifrea, P. Lahoti, B. Packer, Y. Halpern, A. Beirami, and F. Prost. Frappe: A group fairness framework for post-processing everything, 2024.

Proofs

### Confidence intervals on \(\tau^{*}_{\text{fair}}\)

Proof of Lemma 3.1.: This lemma is a straightforward application of Heoffding's inequality. 

Proof of Proposition 3.2.: Here, we prove the result for general classifiers \(h\in\mathcal{H}\). Let \(L^{h}_{\text{acc}},U^{h}_{\text{fair}}\) be the lower and upper CIs for \(\text{acc}(h)\) and \(\Phi_{\text{fair}}(h)\) respectively,

\[\mathbb{P}(\text{acc}(h)\geq L^{h}_{\text{acc}})\geq 1-\alpha/2\quad\text{and} \quad\mathbb{P}(\Phi_{\text{fair}}(h)\leq U^{h}_{\text{fair}})\geq 1-\alpha/2.\]

Then, using a straightforward application union bounds, we get that

\[\mathbb{P}(\text{acc}(h)\geq L^{h}_{\text{acc}},\Phi_{\text{fair} }(h)\leq U^{h}_{\text{fair}}) \geq 1-\mathbb{P}(\text{acc}(h)<L^{h}_{\text{acc}})-\mathbb{P}( \Phi_{\text{fair}}(h)>U^{h}_{\text{fair}})\] \[\geq 1-\alpha/2-\alpha/2=1-\alpha.\]

Using the definition of the optimal fairness-accuracy trade-off \(\tau^{*}_{\text{fair}}\), we get that the event

\[\left\{\text{acc}(h)\geq L^{h}_{\text{acc}},\Phi_{\text{fair}}(h)\leq U^{h}_ {\text{fair}}\right\}\quad\text{implies,}\quad\left\{\underbrace{\min\{\Phi_{ \text{fair}}(h^{\prime})\,|\,h^{\prime}\in\mathcal{H},\text{acc}(h^{\prime}) \geq L^{h}_{\text{acc}}\}}_{\tau^{*}_{\text{fair}}(L^{h}_{\text{acc}})}\leq U^ {h}_{\text{fair}}\right\}.\]

From this, it follows that

\[\mathbb{P}(\tau^{*}_{\text{fair}}(L^{h}_{\text{acc}})\leq U^{h}_{\text{fair}}) \geq\mathbb{P}(\text{acc}(h)\geq L^{h}_{\text{acc}},\Phi_{\text{fair}}(h)\leq U ^{h}_{\text{fair}})\geq 1-\alpha.\]

Proof of Proposition 3.3.: We prove the result for general classifiers \(h\in\mathcal{H}\). Let \(U^{h}_{\text{acc}},L^{h}_{\text{fair}}\) be the upper and lower CIs for \(\text{acc}(h)\) and \(\Phi_{\text{fair}}(h)\) respectively,

\[\mathbb{P}(\text{acc}(h)\leq U^{h}_{\text{acc}})\geq 1-\alpha/2\quad\text{and} \quad\mathbb{P}(\Phi_{\text{fair}}(h)\geq L^{h}_{\text{fair}})\geq 1-\alpha/2.\]

Then, using an application of union bounds, we get that

\[\mathbb{P}(\text{acc}(h)\leq U^{h}_{\text{acc}},\Phi_{\text{fair} }(h)\geq L^{h}_{\text{fair}}) \geq 1-\mathbb{P}(\text{acc}(h)>U^{h}_{\text{acc}})-\mathbb{P}( \Phi_{\text{fair}}(h)<L^{h}_{\text{fair}})\] \[\geq 1-\alpha/2-\alpha/2=1-\alpha.\]

Then, using the fact that \(\Delta(h)=\Phi_{\text{fair}}(h)-\tau^{*}_{\text{fair}}(\text{acc}(h))\), we get that

\[1-\alpha\leq\mathbb{P}(\text{acc}(h)\leq U^{h}_{\text{acc}}, \Phi_{\text{fair}}(h)\geq L^{h}_{\text{fair}}) =\mathbb{P}(\text{acc}(h)\leq U^{h}_{\text{acc}},\tau^{*}_{\text{fair}}( \text{acc}(h))+\Delta(h)\geq L^{h}_{\text{fair}})\] \[\leq\mathbb{P}(\text{acc}(h)\leq U^{h}_{\text{acc}},\tau^{*}_{ \text{fair}}(U^{h}_{\text{acc}})+\Delta(h)\geq L^{h}_{\text{fair}})\] \[\leq\mathbb{P}(\tau^{*}_{\text{fair}}(U^{h}_{\text{acc}})\geq L^{ h}_{\text{fair}}-\Delta(h)),\]

where in the second last inequality above, we use the fact that \(\tau^{*}_{\text{fair}}:[0,1]\to[0,1]\) is a monotonically increasing function. 

### Asymptotic convergence of \(\Delta(h)\)

In this Section, we provide the formal statement for Theorem 3.4 along with the assumptions required for this result.

**Assumption A.1**.: \(\tau^{*}_{\text{fair}}\) is \(L\)-Lipschitz.

**Assumption A.2**.: Let \(\mathcal{R}_{|\mathcal{D}_{\text{tri}}|}(\mathcal{H})\) denote the Rademacher complexity of the classifier family \(\mathcal{H}\), where \(|\mathcal{D}_{\text{tri}}|\) is the number of training examples. We assume that there exists \(C\geq 0\) and \(\gamma\leq 1/2\) such that \(\mathcal{R}_{|\mathcal{D}_{\text{tri}}|}(\mathcal{H})\leq C\,|\mathcal{D}_{ \text{tri}}|^{-\gamma}\).

It is worth noting that Assumption A.2, which was also used in [1, Theorem 2], holds for many classifier families with \(\gamma=1/2\), including norm-bounded linear functions, neural networks and classifier families with bounded VC dimension [21, 6].

**Theorem A.3**.: _Let \(\widehat{\Phi_{\text{fair}}(h^{\prime})},\widetilde{\text{acc}(h^{\prime})}\) denote the fairness violation and accuracy metrics for model \(h^{\prime}\) evaluated on training data \(\mathcal{D}_{\text{tr}}\) and define_

\[h\coloneqq\arg\min_{h^{\prime}\in\mathcal{H}}\widehat{\Phi_{\text{fair}}(h^{ \prime})}\text{ subject to }\widehat{\text{acc}(h^{\prime})}\geq\delta.\]

_Then, under Assumptions A.1 and A.2, we have that with probability at least \(1-\eta\), \(\Delta(h)\leq\widetilde{O}(|\mathcal{D}_{\text{tr}}|^{-\gamma}),\) where \(\widetilde{O}(\cdot)\) suppresses polynomial dependence on \(\log\left(1/\eta\right)\)._

Proof of Theorem a.3.: Let

\[\epsilon\coloneqq 4\,\mathcal{R}_{|\mathcal{D}_{\text{tr}}|}(\mathcal{H})+ \frac{4}{\sqrt{|\mathcal{D}_{\text{tr}}|}}+2\,\sqrt{\frac{\log\left(8/\eta \right)}{2\left|\mathcal{D}_{\text{tr}}\right|}}.\]

Next, we define

\[h^{*}\coloneqq\arg\min_{h\in\mathcal{H}}\Phi_{\text{fair}}(h) \quad\text{ subject to }\quad\text{acc}(h)\geq\text{acc}(h)+\epsilon.\]

Then, we have that

\[\Delta(h)= \Phi_{\text{fair}}(h)-\tau_{\text{fair}}^{*}(\text{acc}(h))\] \[= \Phi_{\text{fair}}(h)-\tau_{\text{fair}}^{*}(\text{acc}(h)+ \epsilon)+\tau_{\text{fair}}^{*}(\text{acc}(h)+\epsilon)-\tau_{\text{fair}}^ {*}(\text{acc}(h))\] \[= \Phi_{\text{fair}}(h)-\Phi_{\text{fair}}(h^{*})+\tau_{\text{fair} }^{*}(\text{acc}(h)+\epsilon)-\tau_{\text{fair}}^{*}(\text{acc}(h)).\]

We know using Assumption A.1 that

\[\tau_{\text{fair}}^{*}(\text{acc}(h)+\epsilon)-\tau_{\text{fair}}^ {*}(\text{acc}(h))\leq L\,\epsilon\]

Moreover,

\[\Phi_{\text{fair}}(h)-\Phi_{\text{fair}}(h^{*})= \widehat{\Phi_{\text{fair}}(h)}-\widehat{\Phi_{\text{fair}}(h^{* })}+\Phi_{\text{fair}}(h)-\widehat{\Phi_{\text{fair}}(h)}+\widehat{\Phi_{ \text{fair}}(h^{*})}-\Phi_{\text{fair}}(h^{*})\] \[\leq \widehat{\Phi_{\text{fair}}(h)}-\widehat{\Phi_{\text{fair}}(h^{*} )}+2\max_{h^{\prime}\in\mathcal{H}}|\Phi_{\text{fair}}(h^{\prime})-\widehat{ \Phi_{\text{fair}}(h^{\prime})}|.\]

Putting the two together, we get that

\[\Delta(h)\leq\widehat{\Phi_{\text{fair}}(h)}-\widehat{\Phi_{\text{fair}}(h^{* })}+2\max_{h^{\prime}\in\mathcal{H}}|\Phi_{\text{fair}}(h^{\prime})-\widehat{ \Phi_{\text{fair}}(h^{\prime})}|+L\epsilon.\] (5)

Next, we consider the term \(\widehat{\Phi_{\text{fair}}(h)}-\widehat{\Phi_{\text{fair}}(h^{*})}\). First, we observe that

\[\text{acc}(h^{*})\geq\text{acc}(h)+\epsilon\geq\widehat{\text{acc}(h)}- \widehat{|\text{acc}(h)}-\text{acc}(h)|+\epsilon\geq\delta+\epsilon-|\widehat {\text{acc}(h)}-\text{acc}(h)|.\]

Next, using [1, Lemma 4] with \(g(X,A,Y)=\mathbbm{1}(h(X)=Y)\), we get that with probability at least \(1-\eta/4\), we have that

\[|\text{acc}(h)-\widehat{\text{acc}(h)}|\leq 2\,\mathcal{R}_{|\mathcal{D}_{ \text{tr}}|}(\mathcal{H})+\frac{2}{\sqrt{|\mathcal{D}_{\text{tr}}|}}+\sqrt{ \frac{\log\left(8/\eta\right)}{2\left|\mathcal{D}_{\text{tr}}\right|}}= \epsilon/2.\]

This implies that with probability at least \(1-\eta/4\),

\[\text{acc}(h^{*})\geq\delta+\epsilon/2,\]

and hence,

\[\delta+\epsilon/2\leq\text{acc}(h^{*})\leq\widehat{\text{acc}(h^{*})}+| \text{acc}(h^{*})-\widehat{\text{acc}(h^{*})}|.\]

Again, using [1, Lemma 4] with \(g(X,A,Y)=\mathbbm{1}(h^{*}(X)=Y)\), we get that with probability at least \(1-\eta/4\), we have that

\[|\text{acc}(h^{*})-\widehat{\text{acc}(h^{*})}|\leq 2\,\mathcal{R}_{| \mathcal{D}_{\text{tr}}|}(\mathcal{H})+\frac{2}{\sqrt{|\mathcal{D}_{\text{tr}} |}}+\sqrt{\frac{\log\left(8/\eta\right)}{2\left|\mathcal{D}_{\text{tr}}\right| }}=\epsilon/2.\]

[MISSING_PAGE_EMPTY:16]

Constructing the confidence intervals on \(\Phi_{\text{fair}}(h)\)

In this section, we outline methodologies of obtaining confidence intervals for a fairness violation \(\Phi_{\text{fair}}\). Specifically, given a model \(h\in\mathcal{H}\), with \(h:\mathcal{X}\rightarrow\mathcal{Y}\) and \(\alpha\in(0,1)\), we outline how to find \(C^{\alpha}_{\text{fair}}\) which satisfies,

\[\mathbb{P}(\Phi_{\text{fair}}(h)\in C^{\alpha}_{\text{fair}})\geq 1-\alpha.\] (8)

Similar to [1] we express the fairness violation \(\Phi_{\text{fair}}\) as:

\[\Phi_{\text{fair}}(h)=|\Phi^{\pm}_{\text{fair}}(h)|\quad\text{ where,}\quad\Phi^{\pm}_{\text{fair}}(h)\coloneqq\sum_{j=1}^{m}\underbrace{\mathbb{E}[g_{j}(X,A,Y,h(X)) \mid\mathcal{E}_{j}]}_{=:\Phi_{j}}\]

where \(m\geq 1\), \(g_{j}\) are some known functions and \(\mathcal{E}_{j}\) are events with positive probability defined with respect to \((X,A,Y)\). For example, when considering the demographic parity (DP), i.e. \(\Phi_{\text{fair}}=\Phi_{\text{DP}}\), we have \(m=2\), with \(g_{1}(X,A,Y,h(X))=h(X)\), \(\mathcal{E}_{1}=\{A=1\}\), \(g_{2}(X,A,Y,h(X))=-h(X)\) and \(\mathcal{E}_{2}=\{A=0\}\). Moreover, as shown in [1], the commonly used fairness metrics like Equalized Odds (EO) and Equalized Opportunity (EOP) can also be expressed in similar forms.

Our methodology of constructing CIs on \(\Phi_{\text{fair}}(h)\) involves first constructing intervals \(C^{\alpha,\pm}_{\text{fair}}\) on \(\Phi^{\pm}_{\text{fair}}(h)\) satisfying:

\[\mathbb{P}(\Phi^{\pm}_{\text{fair}}(h)\in C^{\alpha,\pm}_{\text{fair}})\geq 1 -\alpha.\] (9)

Once we have a \(C^{\alpha,\pm}_{\text{fair}}\), the confidence interval \(C^{\alpha}_{\text{fair}}\) satisfying Eq. (8) can simply be constructed as:

\[C^{\alpha}_{\text{fair}}=\{|x|\,:\,x\in C^{\alpha,\pm}_{\text{fair}}\}.\]

In what follows, we outline two different ways of constructing the confidence intervals \(C^{\alpha,\pm}_{\text{fair}}\) on \(\Phi^{\pm}_{\text{fair}}(h)\) satisfying Eq. (9).

### Separately constructing CIs on \(\Phi_{j}\)

One way to obtain intervals on \(\Phi^{\pm}_{\text{fair}}(h)\) would be to separately construct confidence intervals on \(\Phi_{j}\), denoted by \(C^{\alpha}_{j}\), which satisfies the joint guarantee

\[\mathbb{P}\left(\cap_{j=1}^{m}\{\Phi_{j}\in C^{\alpha}_{j}\}\right)\geq 1-\alpha.\] (10)

Given such set of confidence intervals \(\{C^{\alpha}_{j}\}_{j=1}^{m}\) which satisfy Eqn. Eq. (10), we can obtain the confidence intervals on \(\Phi^{\pm}_{\text{fair}}(h)\) by using the fact that

\[\mathbb{P}\left(\Phi^{\pm}_{\text{fair}}(h)\in\sum_{i=1}^{m}C^{\alpha}_{j} \right)\geq 1-\alpha.\]

Where, the notation \(\sum_{i=1}^{m}C^{\alpha}_{j}\) denotes the set \(\{\sum_{i=1}^{m}x_{i}\,:\,x_{i}\in C^{\alpha}_{i}\}\). One naive way to obtain such \(\{C^{\alpha}_{j}\}_{j=1}^{m}\) which satisfy Eq. (10) is to use the union bounds, i.e., if \(C^{\alpha}_{j}\) are chosen such that

\[\mathbb{P}(\Phi_{j}\in C^{\alpha}_{j})\geq 1-\alpha/m,\]

then, we have that

\[\mathbb{P}\left(\cap_{j=1}^{m}\{\Phi_{j}\in C^{\alpha}_{j}\}\right) =1-\mathbb{P}(\cup_{j=1}^{m}\{\Phi_{j}\in C^{\alpha}_{j}\}^{c})\] \[\geq 1-\sum_{i=1}^{m}\mathbb{P}(\{\Phi_{j}\in C^{\alpha}_{j}\}^{c})\] \[\geq 1-\sum_{i=1}^{m}(1-(1-\alpha/m))=1-\alpha.\]

Here, for an event \(\mathcal{E}\), we use \(\mathcal{E}^{c}\) to denote the complement of the event. This methodology therefore reduces the problem of finding confidence intervals on \(\Phi^{\pm}_{\text{fair}}(h)\) to finding confidence intervals on \(\Phi_{j}\) for \(j\in\{1,\ldots,m\}\). Now note that \(\Phi_{j}\) are all expectations and we can use standard methodologies to construct confidence intervals on an expectation. We explicitly outline how to do this in Section B.3.

**Remark**  The methodology outlined above provides confidence intervals with valid finite sample coverage guarantees. However, this may come at the cost of more conservative confidence intervals. One way to obtain less conservative confidence intervals while retaining the coverage guarantees would be to consider alternative ways of obtaining confidence intervals which do not require constructing the CIs separately on \(\Phi_{j}\). We outline one such methodology in the next section.

### Using subsampling to construct the CIs on \(\Phi_{\text{fair}}^{\pm}\) directly

Here, we outline how we can avoid having to use union bounds when constructing the confidence intervals on \(\Phi_{\text{fair}}^{\pm}\). Let \(\mathcal{D}_{j}\) denote the subset of data \(\mathcal{D}_{\text{cal}}\), for which the event \(\mathcal{E}_{j}\) is true. In the case where the events \(\mathcal{E}_{j}\) are all mutually exclusive and hence \(\mathcal{D}_{j}\) are all disjoint subsets of data (which is true for DP, EO and EOP), we can also construct these intervals by randomly sampling without replacement datapoints \((x_{i}^{(j)},a_{i}^{(j)},y_{i}^{(j)})\) from \(\mathcal{D}_{j}\) for \(i\leq l\coloneqq\min_{k\leq m}|\mathcal{D}_{k}|\). We use the fact that

\[\widehat{\Phi_{\text{fair}}^{\pm}}(h)=\frac{1}{l}\sum_{i=1}^{l}\sum_{j=1}^{m}g _{j}(x_{i}^{(j)},a_{i}^{(j)},y_{i}^{(j)},h(x_{i}^{(j)}))\]

is an unbiased estimator of \(\Phi_{\text{fair}}^{\pm}(h)\). Moreover, since \(\mathcal{D}_{j}\) are all disjoint datasets, the datapoints \((x_{i}^{(j)},a_{i}^{(j)},y_{i}^{(j)})\) are all independent across different values of \(j\), and therefore, \(\sum_{j=1}^{m}g_{j}(x_{i}^{(j)},a_{i}^{(j)},y_{i}^{(j)},h(x_{i}^{(j)}))\) are i.i.d.. In other words,

\[\widehat{\Phi_{\text{fair}}^{\pm}}(h)=\frac{1}{l}\sum_{i=1}^{l}\phi_{i}\quad \text{where,}\quad\phi_{i}\coloneqq\sum_{j=1}^{m}g_{j}(x_{i}^{(j)},a_{i}^{(j )},y_{i}^{(j)},h(x_{i}^{(j)}))\]

and \(\phi_{i}\) are all i.i.d. samples and unbiased estimators of \(\Phi_{\text{fair}}^{\pm}(h)\). Therefore, like in the previous section, our problem reduces to constructing CIs on an expectation term (i.e. \(\Phi_{\text{fair}}^{\pm}(h)\)), using i.i.d. unbiased samples (i.e. \(\phi_{i}\)) and we can use standard methodologies to construct these intervals.

**Benefit of this methodology**  This methodology no longer requires us to separately construct confidence intervals over \(\Phi_{j}\) and combine them using union bounds (for example). Therefore, intervals obtained using this methodology may be less conservative than those obtained by separately constructing confidence intervals over \(\Phi_{j}\).

**Limitation of this methodology**  For each subset of data \(\mathcal{D}_{j}\), we can use at most \(l\coloneqq\min_{k\leq m}|\mathcal{D}_{k}|\) data points to construct the confidence intervals. Therefore, in cases where \(l\) is very small, we may end up discarding a big proportion of the calibration data which could in turn lead to loose intervals.

### Constructing CIs on expectations

Here, we outline some standard techniques used to construct CIs on the expectation of a random variable. These techniques can then be used to construct CIs on \(\Phi_{\text{fair}}(h)\) (using either of the two methodologies outlined above) as well as on \(\text{acc}(h)\). In this section, we restrict ourselves to constructing upper CIs. Lower CIs can be constructed analogously.

Given dataset \(\{Z_{i}:1\leq i\leq n\}\), our goal in this section is to construct upper CIs on \(\mathbb{E}[Z]\) which satisfies

\[\mathbb{P}(\mathbb{E}[Z]\leq U^{\alpha})\geq 1-\alpha.\]

**Hoeffding's inequality**  We can use Hoeffding's inequality to construct these intervals \(U^{\alpha}\) as formalised in the following result:

**Lemma B.1** (Hoeffding's inequality).: _Let \(Z_{i}\in[0,1]\), \(1\leq i\leq n\) be i.i.d. samples with mean \(\mathbb{E}[Z]\). Then,_

\[\mathbb{P}\left(\mathbb{E}[Z]\leq\frac{1}{n}\,\sum_{i=1}^{n}Z_{i}+\sqrt{\frac {1}{2\,n}\log\frac{1}{\alpha}}\right)\geq 1-\alpha.\]

**Bernstein's inequality**  Bernstein's inequality provides a powerful tool for bounding the tail probabilities of the sum of independent, bounded random variables. Specifically, for a sumcomprised of \(n\) independent random variables \(Z_{i}\) with \(Z_{i}\in[0,B]\), each with a maximum variance of \(\sigma^{2}\), and for any \(t>0\), the inequality states that

\[\mathbb{P}\left(\mathbb{E}\left[\sum_{i=1}^{n}Z_{i}\right]-\sum_{i=1}^{n}Z_{i}>t \right)\leq\exp\left(-\frac{t^{2}}{2\sigma^{2}+\frac{2}{3}tB}\right),\]

where \(B\) denotes an upper bound on the absolute value of each random variable. Re-arranging the above, we get that

\[\mathbb{P}\left(\mathbb{E}[Z]<\frac{1}{n}\left(\sum_{i=1}^{n}Z_{i}+t\right) \right)\geq 1-\exp\left(-\frac{t^{2}}{2\sigma^{2}+\frac{2}{3}tB}\right).\]

This allows us to construct upper CIs on \(\mathbb{E}[Z]\).

**Central Limit Theorem**: The Central Limit Theorem (CLT) [26] serves as a cornerstone in statistics for constructing confidence intervals around sample means, particularly when the sample size is substantial. The theorem posits that, for a sufficiently large sample size, the distribution of the sample mean will closely resemble a normal (Gaussian) distribution, irrespective of the original population's distribution. This Gaussian nature of the sample mean empowers us to form confidence intervals for the population mean using the normal distribution's characteristics.

Given \(Z_{1},Z_{2},\ldots,Z_{n}\) as \(n\) independent and identically distributed (i.i.d.) random variables with mean \(\mu\) and variance \(\sigma^{2}\), the sample mean \(\bar{Z}\) approximates a normal distribution with mean \(\mu\) and variance \(\sigma^{2}/n\) for large \(n\). An upper \((1-\alpha)\) confidence interval for \(\mu\) is thus:

\[U^{\alpha}=\bar{Z}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\]

where \(z_{\alpha/2}\) represents the critical value from the standard normal distribution corresponding to a cumulative probability of \(1-\alpha\).

**Bootstrap Confidence Intervals** Bootstrapping, introduced by [17], offers a non-parametric approach to estimate the sampling distribution of a statistic. The method involves repeatedly drawing samples (with replacement) from the observed data and recalculating the statistic for each resample. The resulting empirical distribution of the statistic across bootstrap samples forms the basis for confidence interval construction.

Given a dataset \(Z_{1},Z_{2},\ldots,Z_{n}\), one can produce \(B\) bootstrap samples by selecting \(n\) observations with replacement from the original data. For each of these samples, the statistic of interest (for instance, the mean) is determined, yielding \(B\) bootstrap estimates. An upper \((1-\alpha)\) bootstrap confidence interval for \(\mathbb{E}[Z]\) is given by:

\[U^{\alpha}=\bar{Z}+(z_{\alpha}^{*}-\bar{Z})\]

with \(z_{\alpha}^{*}\) denoting the \(\alpha\)-quantile of the bootstrap estimates. It's worth noting that there exist multiple methods to compute bootstrap confidence intervals, including the basic, percentile, and bias-corrected approaches, and the method described above serves as a general illustration.

## Appendix C Sensitivity analysis for \(\Delta(h)\)

Recall from Proposition 3.3 that the lower confidence intervals for \(\tau_{\text{fair}}^{*}\) include a \(\Delta(h)\) term which is defined as

\[\Delta(h)\coloneqq\Phi_{\text{fair}}(h)-\tau_{\text{fair}}^{*}(\text{acc}(h))\geq 0.\]

In other words, \(\Delta(h)\) quantifies how 'far' the fairness loss of classifier \(h\) (i.e. \(\Phi_{\text{fair}}(h)\)) is from the minimum attainable fairness loss for classifiers with accuracy \(\text{acc}(h)\), (i.e. \(\tau_{\text{fair}}^{*}(\text{acc}(h))\)). This quantity is unknown in general and therefore, a practical strategy of obtaining lower confidence intervals on \(\tau_{\text{fair}}^{*}(\psi)\) may involve positing values for \(\Delta(h)\) which encode our belief on how close the fairness loss \(\Phi_{\text{fair}}(h)\) is to \(\tau_{\text{fair}}^{*}(\text{acc}(h))\). For example, when we assume that the classifier \(h\) achieves the optimal accuracy-fairness tradeoff, i.e. \(\Phi_{\text{fair}}(h)=\tau_{\text{fair}}^{*}(\text{acc}(h))\) then \(\Delta(h)=0\).

However, the assumption \(\Phi_{\text{fair}}(h)=\tau_{\text{fair}}^{*}(\text{acc}(h))\) may not hold in general because we only have a finite training dataset and consequently the empirical loss minimisation may not yield the optima to the true expected loss. Moreover, the regularised loss used in training \(h\) is a surrogate loss which approximates the solution to the constrained minimisation problem in Eq. (1). This means that optimising this regularised loss is not guaranteed to yield the optimal classifier which achieves the optimal fairness \(\tau^{*}_{\text{fair}}(\operatorname{acc}(h))\). Therefore, to incorporate any belief on the sub-optimality of the classifier \(h\), we may consider conducting sensitivity analyses to plausibly quantify \(\Delta(h)\).

Let \(h_{\theta}:\mathcal{X}\times\Lambda\to\mathbb{R}\) be the YOTO model. Our strategy for sensitivity analysis involves training multiple standard models \(\mathcal{M}\coloneqq\{h^{(1)},h^{(2)},\ldots,h^{(k)}\}\subseteq\mathcal{H}\) by optimising the regularised losses for few different choices of \(\lambda\).

\[\mathcal{L}_{\lambda}(\theta)=\mathbb{E}[l_{\text{CE}}(h_{\theta}(X),Y)]+ \lambda\,\mathcal{L}_{\text{fair}}(h_{\theta}).\]

Importantly, we do not require covering the full range of \(\lambda\) values when training separate models \(\mathcal{M}\), and our methodology remains valid even when \(\mathcal{M}\) is a single model. Next, let \(h^{*}_{\lambda}\in\mathcal{M}\cup\{h_{\theta}(\cdot,\lambda)\}\) be such that

\[h^{*}_{\lambda}=\operatorname*{arg\,min}_{h^{\prime}\in\mathcal{M}\cup\{h_{ \theta}(\cdot,\lambda)\}}\widehat{\Phi_{\text{fair}}}(h^{\prime})\quad\text{ subject to}\quad\widehat{\operatorname{acc}}(h^{\prime})\geq\widehat{\operatorname{acc}}(h_{ \theta}(\cdot,\lambda)).\] (11)

Here, \(\widehat{\Phi_{\text{fair}}}\) and \(\widehat{\operatorname{acc}}\) denote the finite sample estimates of the fairness loss and model accuracy respectively. We treat the model \(h^{*}_{\lambda}\) as a model which attains the optimum trade-off when estimating subject to the constraint \(\operatorname{acc}(h)\geq\operatorname{acc}(h_{\theta}(\cdot,\lambda))\). Specifically, we use the maximum empirical error \(\max_{\lambda}\widehat{\Delta}(h_{\theta}(\cdot,\lambda))\) as a plausible surrogate value for \(\Delta(h_{\theta}(\cdot,\lambda^{\prime}))\), where \(\widehat{\Delta}(h_{\theta}(\cdot,\lambda))\coloneqq\widehat{\Phi_{\text{ fair}}}(h_{\theta}(\cdot,\lambda))-\widehat{\Phi_{\text{fair}}}(h^{*}_{ \lambda})\geq 0\), i.e., we posit for any \(\lambda^{\prime}\in\Lambda\)

\[\Delta(h_{\theta}(\cdot,\lambda^{\prime}))\leftarrow\max_{\lambda\in\Lambda} \widehat{\Delta}(h_{\theta}(\cdot,\lambda))\qquad\text{where,}\qquad\widehat {\Delta}(h_{\theta}(\cdot,\lambda))\coloneqq\widehat{\Phi_{\text{fair}}}(h_{ \theta}(\cdot,\lambda))-\widehat{\Phi_{\text{fair}}}(h^{*}_{\lambda}).\]

Next, we can use this posited value of \(\Delta(h_{\theta}(\cdot,\lambda^{\prime}))\) to construct the lower confidence interval using the following corollary of Proposition 3.3:

**Corollary C.1**.: _Consider the YOTO model \(h_{\theta}:\mathcal{X}\times\Lambda\to\mathbb{R}\). Given \(\lambda_{0}\in\Lambda\), let \(U^{h}_{\operatorname{acc}},L^{h}_{\text{fair}}\in[0,1]\) be such that_

\[\mathbb{P}(\operatorname{acc}(h_{\theta}(\cdot,\lambda_{0}))\leq U^{h}_{ \operatorname{acc}})\geq 1-\alpha/2\quad\text{and}\quad\mathbb{P}(\Phi_{\text{fair}}(h_{ \theta}(\cdot,\lambda_{0}))\geq L^{h}_{\text{fair}})\geq 1-\alpha/2.\]

_Then, we have that \(\mathbb{P}(\tau^{*}_{\text{fair}}(U^{h}_{\operatorname{acc}})\geq L^{h}_{ \text{fair}}-\Delta(h_{\theta}(\cdot,\lambda_{0})))\geq 1-\alpha\)._

This result shows that if the goal is to construct lower confidence intervals on \(\tau^{*}_{\text{fair}}(\psi)\) and we obtain that \(\psi\geq U^{h}_{\operatorname{acc}}\), then using the monotonicity of \(\tau^{*}_{\text{fair}}\) we have that \(\tau^{*}_{\text{fair}}(\psi)\geq\tau^{*}_{\text{fair}}(U^{h}_{\operatorname{ acc}})\). Therefore the interval \([L^{h}_{\text{fair}}-\Delta(h_{\theta}(\cdot,\lambda_{0}))),1]\) serves as a lower confidence interval for \(\tau^{*}_{\text{fair}}(\psi)\).

**When YOTO satisfies Pareto optimality, \(\Delta(h_{\theta}(\cdot,\lambda))\to 0\) as \(|\mathcal{D_{\text{real}}}|\to\infty\):** Here, we show that in the case when YOTO achieves the optimal trade-off, then our sensitivity analysis leads to \(\Delta(h_{\theta}(\cdot,\lambda))=0\) as the calibration data size increases for all \(\lambda\in\Lambda\). Our arguments in this section are not formal, however, this idea can be formalised without any significant difficulty.

First, the concept of Pareto optimality (defined below) formalises the idea that YOTO achieves the optimal trade-off:

**Assumption C.2** (Pareto optimality).: If for some \(\lambda\in\Lambda\) and \(h^{\prime}\in\mathcal{H}\) we have that, \(\operatorname{acc}(h^{\prime})\geq\operatorname{acc}(h_{\theta}(\cdot,\lambda))\) then, \(\Phi_{\text{fair}}(h^{\prime})\geq\Phi_{\text{fair}}(h_{\theta}(\cdot,\lambda))\), In the case when YOTO satisfies this optimality property, then it is straightforward to see that \(\Delta(h_{\theta}(\cdot,\lambda))=0\) for all \(\lambda\in\Lambda\). In this case, as \(\mathcal{D}_{\text{cal}}\to\infty\), we get that Eq. (11) roughly becomes

\[h^{*}_{\lambda}=\operatorname*{arg\,min}_{h^{\prime}\in\mathcal{M}\cup\{h_{ \theta}(\cdot,\lambda)\}}\Phi_{\text{fair}}(h^{\prime})\quad\text{subject to}\quad \operatorname{acc}(h^{\prime})\geq\operatorname{acc}(h_{\theta}(\cdot,\lambda)).\]

Here, Assumption C.2 implies that \(h^{*}_{\lambda}=h_{\theta}(\cdot,\lambda)\), and therefore

\[\widehat{\Delta}(h_{\theta}(\cdot,\lambda))\coloneqq\widehat{\Phi_{\text{ fair}}}(h_{\theta}(\cdot,\lambda))-\widehat{\Phi_{\text{fair}}}(h^{*}_{

#### 4.2.2 Intuition behind our sensitivity analysis procedure

Intuitively, the high-level idea behind our sensitivity analysis is that it checks if we train models separately for fixed values of \(\lambda\) (i.e. models in \(\mathcal{M}\)), how much better do these separately trained models perform in terms of the accuracy-fairness trade-offs as compared to our YOTO model. If we find that the separately trained models achieve a better trade-off than the YOTO model for specific values of \(\lambda\), then the sensitivity analysis adjusts the empirical trade-off obtained using YOTO models (using the \(\widehat{\Delta}(h_{\theta}(\cdot,\lambda))\) term defined above). If, on the other hand, we find that the YOTO model achieves a better trade-off than the separately trained models in \(\mathcal{M}\), then the sensitivity analysis has no effect on the lower confidence intervals as in this case \(\widehat{\Delta}(h_{\theta}(\cdot,\lambda))=0\).

### Experimental results

Here, we include empirical results showing how the CIs constructed change as a result of our sensitivity analysis procedure. In Figures 4 and 5, we include examples of CIs where the empirical trade-off obtained using YOTO is sub-optimal. In these cases, the lower CIs obtained without sensitivity analysis (i.e. when we assume \(\Delta(h_{\lambda})=0\)) do not cover the empirical trade-offs for the separately trained models. However, the figures show that the sensitivity analysis procedure adjusts the lower CIs in both cases so that they encapsulate the empirical trade-offs that were not captured without sensitivity analysis.

Figure 4: CIs with and without sensitivity analysis for Adult dataset for EO violation.

Figure 5: CIs with and without sensitivity analysis for Adult dataset for DP violation.

Figure 6: CIs with and without sensitivity analysis for COMPAS dataset for EO violation. Here, sensitivity analysis has no effect on the constructed CIs as the YOTO model achieves a better empirical trade-off than the separately trained models.

[MISSING_PAGE_FAIL:22]

Suppose that we additionally have access to a predictive model \(f_{\mathcal{A}}\) which predicts the sensitive attributes \(A\) using the features \(X\). In this case, another simple strategy would be to simply impute the missing values of \(A\), with the values \(\hat{A}\) predicted using \(f_{\mathcal{A}}\). However, this will usually lead to a biased estimate of the fairness violation \(\Phi_{\text{fair}}(h)\), and hence is not very reliable unless the model \(f_{\mathcal{A}}\) is highly accurate. In this section, we show how to get the best of both worlds, i.e. how to utilise the data with missing sensitive attributes to obtain tighter and more accurate confidence intervals on \(\tau^{*}_{\text{fair}}(\psi)\).

Formally, we consider \(\mathcal{D}_{\text{cal}}=\mathcal{D}\cup\tilde{\mathcal{D}}\) where \(\mathcal{D}\) denotes a data subset of size \(n\) that contains sensitive attributes (i.e. we observe \(A\)) and \(\tilde{\mathcal{D}}\) denotes the data subset of size \(N\) for which we do not observe the sensitive attributes \(A\), and \(N\gg n\). Additionally, for both datasets, we have predictions of the sensitive attributes made by a machine-learning algorithm \(f_{\mathcal{A}}:\mathcal{X}\rightarrow\mathcal{A}\), where \(f_{\mathcal{A}}(X)\approx A\). Concretely we have that \(\mathcal{D}=\{(X_{i},A_{i},Y_{i},f_{\mathcal{A}}(X_{i}))\}_{i=1}^{n}\) and \(\tilde{\mathcal{D}}=\{(\tilde{X}_{i},\tilde{Y}_{i},f_{\mathcal{A}}(\tilde{X}_ {i}))\}_{i=1}^{N}\)

**High-level methodology** Our methodology is inspired by prediction-powered inference [4] which builds confidence intervals on the expected outcome \(\mathbb{E}[Y]\) using data for which the true outcome \(Y\) is only available for a small proportion of the dataset. In our setting, however, it is the sensitive attribute \(A\) that is missing for the majority of the data (and not the outcome \(Y\)).

For \(h\in\mathcal{H}\), let \(\Phi_{\text{fair}}(h)\) be a fairness violation (such as DP or EO), and let \(\widetilde{\Phi_{\text{fair}}}(h)\) be the corresponding fairness violation computed on the data distribution where \(A\) is replaced by the surrogate sensitive attribute \(f_{\mathcal{A}}(X)\). For example, in the case of DP violation, \(\Phi_{\text{fair}}(h)\) and \(\widetilde{\Phi_{\text{fair}}}(h)\) denote:

\[\Phi_{\text{fair}}(h) =|\mathbb{P}(h(X)=1\mid A=1)-\mathbb{P}(h(X)=1\mid A=0)|,\] \[\widetilde{\Phi_{\text{fair}}}(h) =|\mathbb{P}(h(X)=1\mid f_{\mathcal{A}}(X)=1)-\mathbb{P}(h(X)=1 \mid f_{\mathcal{A}}(X)=0)|.\]

We next construct the confidence intervals on \(\Phi_{\text{fair}}(h)\) using the following steps:

1. Using \(\mathcal{D}\), we construct intervals \(C_{\epsilon}(\alpha;h)\) on \(\epsilon(h)\coloneqq\Phi_{\text{fair}}(h)-\widetilde{\Phi_{\text{fair}}}(h)\) satisfying \[\mathbb{P}(\epsilon(h)\in C_{\epsilon}(\alpha;h))\geq 1-\alpha.\] (12) Even though the size of \(\mathcal{D}\) is small, we choose a methodology which yields tight intervals for \(\epsilon(h)\) when \(f_{\mathcal{A}}(X_{i})=A_{i}\) with a high probability.
2. Next, using the dataset \(\tilde{\mathcal{D}}\), we construct intervals \(\tilde{C}_{\text{f}}(\alpha;h)\) on \(\widetilde{\Phi_{\text{fair}}}(h)\) satisfying \[\mathbb{P}(\widetilde{\Phi_{\text{fair}}}(h)\in\tilde{C}_{\text{f}}(\alpha;h ))\geq 1-\alpha.\] (13) This interval will also be tight as the size of \(\tilde{\mathcal{D}}\), \(N\gg n\).

Finally, using the union bound idea we combine the two confidence intervals to obtain the confidence interval for \(\Phi_{\text{fair}}(h)-\widetilde{\Phi_{\text{fair}}}(h)+\widetilde{\Phi_{ \text{fair}}}(h)=\Phi_{\text{fair}}(h)\). We make this precise in the following result:

**Lemma D.1**.: _Let \(C_{\epsilon}(\alpha;h),\tilde{C}_{\text{f}}(\alpha;h)\) be as defined in equations 12 and 13. Then, if we define \(C^{\alpha}_{\text{fair}}(h)=\{x+y\,|\,x\in C_{\epsilon}(\alpha;h),y\in\tilde{C }_{\text{f}}(\alpha;h)\}\), we have that_

\[\mathbb{P}(\Phi_{\text{fair}}(h)\in C^{\alpha}_{\text{fair}}(h))\geq 1-2\alpha.\]

When constructing the CIs over \(\widetilde{\Phi_{\text{fair}}}(h)\) using imputed sensitive attributes \(f_{\mathcal{A}}(X)\) in step 2 above, the prediction error of \(f_{\mathcal{A}}\) introduces an error in the obtained CIs (denoted by \(\epsilon(h)\)). Step 1 rectifies this by constructing a CI over the incurred error \(\epsilon(h)\), and therefore combining the two allows us to obtain intervals which utilise all of the available data while ensuring that the constructed CIs are well-calibrated.

**Example: Demographic parity** Having defined our high-level methodology above, we concretely demonstrate how this can be applied to the case where the fairness loss under consideration is DP. As described above, the first step involves constructing intervals on \(\epsilon(h)\coloneqq\Phi_{\text{fair}}(h)-\widetilde{\Phi_{\text{fair}}}(h)\) using a methodology which yields tight intervals when \(f_{\mathcal{A}}(X_{i})=A_{i}\) with a high probability. To this end, we use bootstrapping as described in Algorithm 1.

Even though bootstrapping does not provide us with finite sample coverage guarantees, it is asymptotically exact and satisfies the property that the confidence intervals are tight when \(\hat{A}=A\) with a high 

[MISSING_PAGE_FAIL:24]

accuracy of \(f_{\mathcal{A}}\) increases to 90%. In this case, the CIs with imputed sensitive attributes mostly contain empirical trade-offs. This shows that in cases where the predictive model \(f_{\mathcal{A}}\) has high accuracy, it may be sufficient to impute missing sensitive attributes with \(f_{\mathcal{A}}(X)\) when constructing the CIs.

## Appendix E Accounting for the uncertainty in baseline trade-offs

In this section, we extend our methodology to also account for uncertainty in the baseline trade-offs when assessing the optimality of different baselines. Recall that our confidence intervals constructed

Figure 8: CIs were obtained using our methodology for the Adult dataset. Here \(n=50\) and \(N=2500\).

Figure 10: CIs obtained using our methodology for COMPAS dataset. Here \(n=50\) and \(N=2000\).

Figure 7: CIs obtained by imputing missing senstive attributes using \(f_{\mathcal{A}}\) for Adult dataset. Here \(n=50\) and \(N=2500\).

Figure 9: CIs obtained by imputing missing senstive attributes using \(f_{\mathcal{A}}\) for COMPAS dataset. Here \(n=50\) and \(N=2000\).

on \(\tau^{*}_{\text{fair}}\) satisfy the guarantee

\[\mathbb{P}(\tau^{*}_{\text{fair}}(\Psi)\in\Gamma^{\alpha}_{\text{fair}})\geq 1-\alpha.\]

This means that if the accuracy-fairness tradeoff for a given model \(h^{\prime}\in\mathcal{H}\), \((\text{acc}(h^{\prime}),\Phi_{\text{fair}}(h^{\prime}))\), lies above the confidence intervals \(\Gamma^{\alpha}_{\text{fair}}\) (i.e. in the pink region in Figure 13), then we can confidently infer that the model \(h^{\prime}\) achieves a suboptimal trade-off. This is because we know from the probabilistic guarantee above that the optimal trade-off \((\text{acc}(h^{\prime}),\tau^{*}_{\text{fair}}(\text{acc}(h^{\prime})))\) must lie in the intervals \(\Gamma^{\alpha}_{\text{fair}}\) with probability at least \(1-\alpha\).

Here, \(\text{acc}(h^{\prime}),\Phi_{\text{fair}}(h^{\prime})\) denote the accuracy and fairness violations for model \(h^{\prime}\) on the full data distribution. However, in practice, we only have access to finite data and therefore can only compute the empirical values of accuracy and fairness violations which we denote by \(\widehat{\text{acc}}(h^{\prime}),\widehat{\Phi_{\text{fair}}}(h^{\prime})\). This means that when checking if the accuracy-fairness trade-off \((\text{acc}(h^{\prime}),\Phi_{\text{fair}}(h^{\prime}))\) lies inside the confidence intervals \(\Gamma^{\alpha}_{\text{fair}}\), we must account for the uncertainty in the empirical estimates \(\widehat{\text{acc}}(h^{\prime}),\widehat{\Phi_{\text{fair}}}(h^{\prime})\). This can be achieved by constructing confidence regions \(C^{\alpha}(h^{\prime})\) satisfying

\[\mathbb{P}((\text{acc}(h^{\prime}),\Phi_{\text{fair}}(h^{\prime}))\in C^{ \alpha}(h^{\prime}))\geq 1-\alpha.\] (14)

**Baseline's best-case accuracy-fairness trade-off** If the confidence region \(C^{\alpha}(h^{\prime})\) lies entirely above the confidence intervals \(\Gamma^{\alpha}_{\text{fair}}\) (i.e. in the pink region in Figure 13), then using union bounds we can confidently conclude that with probability \(1-2\alpha\) we have that the model \(h^{\prime}\) achieves suboptimal fairness violation, i.e.

\[\Phi_{\text{fair}}(h^{\prime})\geq\tau^{*}_{\text{fair}}(\text{acc}(h^{\prime})).\]

This allows practitioners to confidently check if a model \(h^{\prime}\) is suboptimal in terms of its accuracy-fairness trade-off. This is different from simply checking if the empirical trade-off achieved by the model \((\widehat{\text{acc}}(h^{\prime}),\widehat{\Phi_{\text{fair}}}(h^{\prime}))\) lies in the permissible trade-off region (green region in Figure 1) as it also accounts for the finite-sampling uncertainty in the tradeoff achieved by model \(h^{\prime}\). However, this means that the criterion for flagging a baseline model \(h^{\prime}\) as suboptimal becomes more conservative.

Next, we show how to construct confidence regions \(C^{\alpha}(h^{\prime})\) satisfying Eq. (14).

**Lemma E.1**.: _For a classifier \(h^{\prime}\in\mathcal{H}\), let \(U^{h}_{\text{acc}},L^{h}_{\text{fair}}\) be the upper and lower CIs on \(\text{acc}(h^{\prime}),\Phi_{\text{fair}}(h^{\prime})\) respectively,_

\[\mathbb{P}(\text{acc}(h^{\prime})\leq U^{h}_{\text{acc}})\geq 1-\alpha/2, \quad\text{and}\quad\mathbb{P}(\Phi_{\text{fair}}(h^{\prime})\geq L^{h}_{ \text{fair}})\geq 1-\alpha/2.\]

Figure 11: CIs obtained by imputing missing sensitive attributes using \(f_{\mathcal{A}}\) for CelebA dataset. Here \(n=50\) and \(N=2500\).

Figure 12: CIs obtained using our methodology for CelebA dataset. Here \(n=50\) and \(N=2500\).

_Then, \(\mathbb{P}((\operatorname{acc}(h^{\prime}),\Phi_{\text{fair}}(h^{\prime}))\in[0,U^ {h}_{\text{acc}}]\times[L^{h}_{\text{fair}},1])\geq 1-\alpha\)._

Lemma E.1 shows that \(C^{\alpha}(h^{\prime})=[0,U^{h}_{\text{acc}}]\times[L^{h}_{\text{fair}},1]\) forms the \(1-\alpha\) confidence region for the accuracy-fairness trade-off \((\operatorname{acc}(h^{\prime}),\Phi_{\text{fair}}(h^{\prime}))\). We illustrate this in Figure (a)a. If this confidence region lies entirely above the permissible region (i.e. in the pink region in Figure (a)a), we can confidently conclude that the model \(h^{\prime}\) achieves suboptimal accuracy-fairness trade-off. From Figure (a)a it can be seen that this will occur if \((U^{h}_{\text{acc}},L^{h}_{\text{fair}})\) lies above the permissible region.

Intuitively, \((U^{h}_{\text{acc}},L^{h}_{\text{fair}})\) can be seen as an optimistic best-case accuracy-fairness trade-off achieved by the model \(h^{\prime}\), since at this point in the confidence region \(C^{\alpha}(h^{\prime})\) the accuracy is maximised and fairness violations is minimised. Therefore if this best-case trade-off lies above the permissible region, then this intuitively indicates that the worst-case optimal trade-off \(\tau^{*}_{\text{fair}}\) is still better than the best-case trade-off achieved by the model \(h^{\prime}\), leading us to the confident conclusion that \(h^{\prime}\) achieves suboptimal accuracy-fairness trade-off.

**Baseline's worst-case accuracy-fairness trade-off** Conversely, if the confidence region \(C^{\alpha}(h^{\prime})\) lies entirely below the confidence intervals \(\Gamma^{\alpha}_{\text{fair}}\) (i.e. in the blue region in Figure (a)a), then using union bounds we can confidently conclude that with probability \(1-2\alpha\) we have that the model \(h^{\prime}\) achieves a better trade-off than the YOTO model \(h_{\theta}(\cdot,\lambda)\). Formally, this means that

\[\tau^{\text{YOTO}}(\operatorname{acc}(h^{\prime}))\geq\Phi_{\text{fair}}(h^{ \prime}),\]

where

\[\tau^{\text{YOTO}}(\delta)\coloneqq\min_{\lambda\in\Lambda}\Phi_{\text{fair}} (h_{\theta}(\cdot,\lambda))\quad\text{subject to}\quad\operatorname{acc}(h_{ \theta}(\cdot,\lambda))\geq\delta.\]

This would indicate that the YOTO model does not achieve the optimal trade-off and can be used to further calibrate the \(\Delta(h_{\lambda})\) values when constructing the lower confidence interval using Proposition 3.3. Again, this is different from simply checking if the empirical trade-off achieved by the model \((\operatorname{acc}(h^{\prime}),\widehat{\Phi_{\text{fair}}}(h^{\prime}))\) lies in the unlikely-to-be-achieved trade-off region (blue region in Figure (a)a) as it also accounts for the finite-sampling uncertainty in the tradeoff achieved by model \(h^{\prime}\).

Next, we show to construct such confidence region \(C^{\alpha}(h^{\prime})\) using an approach analogous to the one outlined above:

**Lemma E.2**.: _For a classifier \(h^{\prime}\in\mathcal{H}\), let \(L^{h}_{\text{acc}},U^{h}_{\text{fair}}\) be the lower and upper CIs on \(\operatorname{acc}(h^{\prime}),\Phi_{\text{fair}}(h^{\prime})\) respectively,_

\[\mathbb{P}(\operatorname{acc}(h^{\prime})\geq L^{h}_{\text{acc}})\geq 1- \alpha/2,\quad\text{and}\quad\mathbb{P}(\Phi_{\text{fair}}(h^{\prime})\leq U^ {h}_{\text{fair}})\geq 1-\alpha/2.\]

_Then, \(\mathbb{P}((\operatorname{acc}(h^{\prime}),\Phi_{\text{fair}}(h^{\prime}))\in[ L^{h}_{\text{acc}},1]\times[0,U^{h}_{\text{fair}}])\geq 1-\alpha\)._

Figure 13: Confidence region \(C^{\alpha}(h)\) for the accuracy-fairness trade-off achieved by \(h\).

Lemma E.2 shows that \(C^{\alpha}(h^{\prime})=[L^{h}_{\text{acc}},1]\times[0,U^{h}_{\text{fair}}]\) forms the \(1-\alpha\) confidence region for the accuracy-fairness trade-off \((\text{acc}(h^{\prime}),\Phi_{\text{fair}}(h^{\prime}))\). If this confidence region lies entirely below the permissible region (i.e. in the blue region in Figure 13), we can confidently conclude that the model \(h^{\prime}\) achieves a better accuracy-fairness trade-off than the YOTO model. From Figure 13b it can be seen that this will occur if \((L^{h}_{\text{acc}},U^{h}_{\text{fair}})\) lies below the permissible region.

Intuitively, \((L^{h}_{\text{acc}},U^{h}_{\text{fair}})\) can be seen as a conservative worst-case accuracy-fairness trade-off achieved by the model \(h^{\prime}\). Therefore if this worst-case trade-off lies below the permissible region, then this intuitively indicates that the best-case YOTO trade-off \(\tau^{\text{YOTO}}\) is still worse than the worst-case trade-off achieved by the model \(h^{\prime}\), leading us to the confident conclusion that \(h^{\prime}\) achieves a better accuracy-fairness trade-off than the YOTO model. This can subsequently be used to calibrate the suboptimality gap for the YOTO model, denoted by \(\Delta(h_{\lambda})\) in Proposition 3.3.

### Experimental results

Here, we present the results with the empirical baseline trade-offs replaced by best or worst-case trade-offs as appropriate. More specifically, in Figure 14,

* if the empirical trade-off for a baseline \((\widehat{\text{acc}}(h^{\prime}),\widehat{\Phi_{\text{fair}}}(h^{\prime}))\) lies in the suboptimal region (as in Figure 13a), then we plot the best-case trade-off \((U^{h}_{\text{acc}},L^{h}_{\text{fair}})\) for the baseline,
* if the empirical trade-off for a baseline \((\widehat{\text{acc}}(h^{\prime}),\widehat{\Phi_{\text{fair}}}(h^{\prime}))\) lies in the unlikely-to-be-achievable trade-off region (as in Figure 13b), then we plot the worst-case trade-off \((L^{h}_{\text{acc}},U^{h}_{\text{fair}})\) for the baseline,
* if the empirical trade-off for a baseline \((\widehat{\text{acc}}(h^{\prime}),\widehat{\Phi_{\text{fair}}}(h^{\prime}))\) lies in the permissible region, then we simply plot the empirical trade-off \((\widehat{\text{acc}}(h^{\prime}),\widehat{\Phi_{\text{fair}}}(h^{\prime}))\).

Therefore, in Figure 14 if a baseline's best-case trade-off lies above the permissible trade-off region, then we can confidently conclude that the baseline achieves suboptimal accuracy-fairness trade-off with probability \(1-2\alpha=90\%\). Similarly, a baseline's worst-case trade-off lying below the permissible trade-off region would suggest that the YOTO trade-off achieves a suboptimal trade-off and that the value of \(\Delta(h_{\theta})\) needs to be adjusted accordingly.

Table 5 shows the proportion of best-case trade-offs which lie above the permissible trade-off region and the proportion of worst-case trade-offs which lie below the permissible region. Firstly, the table shows that the proportion of worst-case trade-offs which lie in the 'unlikely' region is negligible, empirically confirming that our confidence intervals on optimal trade-off \(\tau^{*}_{\text{fair}}\) are indeed valid. Secondly, we can see that there are a considerable proportion of baselines whose best-case trade-off lies above the permissible region, highlighting that our methodology remains effective in flagging suboptimalities in SOTA baselines even when we account for the possible uncertainty in baseline trade-offs. This shows that our methodology yields CIs which are not only reliable but also informative.

\begin{table}
\begin{tabular}{l l l l} \hline \hline Baseline & Unlikely & Permissible & Sub-optimal \\ \hline KDE-fair & 0.0 \(\pm\) 0.0 & 1.0\(\pm\)0.0 & 0.0\(\pm\)0.0 \\ RTO & 0.0 \(\pm\) 0.0 & 0.77 \(\pm\)0.05 & 0.23\(\pm\) 0.04 \\ adversary & 0.0 \(\pm\) 0.0 & 0.79 \(\pm\)0.1 & 0.2\(\pm\)0.08 \\ linear & 0.08\(\pm\)0.05 & 0.81\(\pm\)0.06 & 0.11\(\pm\)0.05 \\ LOSSI & 0.05 \(\pm\)0.02 & 0.71 \(\pm\) 0.09 & 0.25\(\pm\)0.1 \\ reductions & 0.0\(\pm\)0.0 & 0.82\(\pm\) 0.09 & 0.18 \(\pm\)0.08 \\ separate & 0.03\(\pm\)0.02 & 0.95\(\pm\) 0.08 & 0.02 \(\pm\)0.03 \\ \hline \hline \end{tabular}
\end{table}
Table 5: The table shows the proportion of models whose best-case trade-off \((U^{h}_{\text{acc}},L^{h}_{\text{fair}})\) lies in the suboptimal region, models whose worst-case trade-off \((L^{h}_{\text{acc}},L^{h}_{\text{fair}})\) lies in the unlikely region and models whose confidence regions \(C^{\alpha}(h)\) overlaps with the permissible trade-off region. The table reports average across all experiments.

## Appendix F Experimental details and additional results

In this section, we provide greater details regarding our experimental setup and models used. We first begin by defining the Equalized Odds metric which has been used in our experiments, along with DP and EOP.

**Equalized Odds (EO):** EO condition states that, both the true positive rates and false positive rates for all sensitive groups are equal, i.e. \(\mathbb{P}(h(X)=1\mid A=a,Y=y)=\mathbb{P}(h(X)=1\mid Y=y)\) for any \(a\in\mathcal{A}\) and \(y\in\{0,1\}\). The absolute EO violation is defined as:

\[\Phi_{\text{EO}}(h):= 1/2\left|\mathbb{P}(h(X)=1\mid A=1,Y=1)-\mathbb{P}(h(X)=1\mid A =0,Y=1)\right|\] \[+1/2\left|\mathbb{P}(h(X)=1\mid A=1,Y=0)-\mathbb{P}(h(X)=1\mid A =0,Y=0)\right|.\]

Next, we provide additional details regarding the YOTO model.

### Practical details regarding YOTO model

As described in Section 3, we consider optimising regularized losses of the form

\[\mathcal{L}_{\lambda}(\theta)=\mathbb{E}[l_{\text{CE}}(h_{\theta}(X),Y)]+ \lambda\,\mathcal{L}_{\text{fair}}(h_{\theta}).\]

When training YOTO models, instead of fixing \(\lambda\), we sample the parameter \(\lambda\) from a distribution \(P_{\lambda}\). As a result, during training the model observes many different values of \(\lambda\) and learns to optimise the loss \(\mathcal{L}_{\lambda}\) for all of them simultaneously. At inference time, the model can be conditioned on a chosen parameter value \(\lambda^{\prime}\) and recovers the model trained to optimise \(\mathcal{L}_{\lambda^{\prime}}\). The loss being minimised can thus be expressed as follows:

\[\operatorname*{arg\,min}_{h_{\theta}:\mathcal{X}\times\Lambda \rightarrow\mathbb{R}}\mathbb{E}_{\lambda\sim P_{\lambda}}\left[\mathbb{E}[l_{ \text{CE}}(h_{\theta}(X,\lambda),Y)]+\lambda\,\mathcal{L}_{\text{fair}}(h_{ \theta}(\cdot,\lambda))\right].\]

The fairness losses \(\mathcal{L}_{\text{fair}}\) considered for the YOTO model are:

DP: \[\mathcal{L}_{\text{fair}}(h_{\theta}(\cdot,\lambda))=\left| \mathbb{E}[\sigma(h_{\theta}(X,\lambda))\mid A=1]-\mathbb{E}[\sigma(h_{\theta} (X,\lambda))\mid A=0]\right|\] EOP: \[\mathcal{L}_{\text{fair}}(h_{\theta}(\cdot,\lambda))=\left| \mathbb{E}[\sigma(h_{\theta}(X,\lambda))\mid A=1,Y=1]-\mathbb{E}[\sigma(h_{ \theta}(X,\lambda))\mid A=0,Y=1]\right|\] EO: \[\mathcal{L}_{\text{fair}}(h_{\theta}(\cdot,\lambda))=\left| \mathbb{E}[\sigma(h_{\theta}(X,\lambda))\mid A=1,Y=1]-\mathbb{E}[\sigma(h_{ \theta}(X,\lambda))\mid A=0,Y=1]\right|\] \[+\left|\mathbb{E}[\sigma(h_{\theta}(X,\lambda))\mid A=1,Y=0]- \mathbb{E}[\sigma(h_{\theta}(X,\lambda))\mid A=0,Y=0]\right|.\]

Figure 14: Results with the empirical baseline trade-offs replaced by best-case and worst-case trade-offs for baselines lying above and below the permissible region respectively. Here, \(\mathcal{D}_{\text{cal}}\) is 10% data split and \(\alpha=0.05\).

Here, \(\sigma(x)\coloneqq 1/(1+e^{-x})\) denotes the sigmoid function.

In our experiments, we sample a new \(\lambda\) for every batch. Moreover, we use the log-uniform distribution as per [15] as the sampling distribution \(P_{\lambda}\), where the uniform distribution is \(U[10^{-6},10]\). To condition the network on \(\lambda\) parameters, we follow in the footsteps of [15] to use Feature-wise Linear Modulation (FiLM) [34]. For completeness, we include the description of the architecture next.

Initially, we determine which network layers should be conditioned, which can encompass all layers or just a subset. For each chosen layer, we condition it based on the weight parameters \(\lambda\). Given a layer that yields a feature map \(f\) with dimensions \(W\times H\times C\), where \(W\) and \(H\) denote the spatial dimensions and \(C\) stands for the channels, we introduce the parameter vector \(\lambda\) to two distinct multi-layer perceptrons (MLPs), denoted as \(M_{\sigma}\) and \(M_{\mu}\). These MLPs produce two vectors, \(\sigma\) and \(\mu\), each having a dimensionality of \(C\). The feature map is then transformed by multiplying it channel-wise with \(\sigma\) and subsequently adding \(\mu\). The resultant transformed feature map \(f^{\prime}\) is given by:

\[f^{\prime}_{ijk}=\sigma_{k}f_{ijk}+\mu_{k}\quad\text{where}\quad\sigma=M_{ \sigma}(\lambda)\quad\text{and}\quad\mu=M_{\mu}(\lambda).\]

Next, we provide exact architectures we used for each dataset in our experiments.

#### f.1.1 YOTO Architectures

**Adult and COMPAS dataset** Here, we use a simple logistic regression as the main model, with only the scalar logit outputs of the logistic regression being conditioned using FiLM. The MLPs \(M_{\mu},M_{\sigma}\) both have two hidden layers, each of size 4, and ReLU activations. We train the model for a maximum of 1000 epochs, with early stopping based on validation losses. Training these simple models takes roughly 5 minutes on a Tesla-V100-SXM2-32GB GPU.

**CelebA dataset** For the CelebA dataset, our architecture is a convolutional neural network (ConvNet) integrated with the FiLM (Feature-wise Linear Modulation) mechanism. The network starts with two convolutional layers: the first layer has 32 filters with a kernel size of \(3\times 3\), and the second layer has 64 filters, also with a \(3\times 3\) kernel. Both convolutional layers employ a stride of 1 and are followed by a max-pooling layer that reduces each dimension by half.

The feature maps from the convolutional layers are flattened and passed through a series of fully connected (MLP) layers. Specifically, the first layer maps the features to 64 dimensions, and the subsequent layers maintain this size until the final layer, which outputs a scalar value. The activation function used in these layers is ReLU.

To condition the network on the \(\lambda\) parameter using FiLM, we design two multi-layer perceptrons (MLPs), \(M_{\mu}\) and \(M_{\sigma}\). Both MLPs take the \(\lambda\) parameter as input and have 4 hidden layers. Each of these hidden layers is of size 256. These MLPs produce the modulation parameters \(\mu\) and \(\sigma\), which are used to perform feature-wise linear modulation on the outputs of the main MLP layers. The final output of the network is passed through a sigmoid activation function to produce the model's prediction. We train the model for a maximum of 1000 epochs, with early stopping based on validation losses. Training this model takes roughly 1.5 hours on a Tesla-V100-SXM2-32GB GPU.

**Jigsaw dataset** For the Jigsaw dataset, we employ a neural network model built upon the BERT architecture [13] integrated with the Feature-wise Linear Modulation (FiLM) mechanism. We utilize the representation corresponding to the [CLS] token, which carries aggregate information about the entire sequence. To condition the BERT's output on the \(\lambda\) parameter using FiLM, we design two linear layers, which map the \(\lambda\) parameter to modulation parameters \(\gamma\) and \(\beta\), both of dimension equal to BERT's hidden size of 768. These modulation parameters are then used to perform feature-wise linear modulation on the [CLS] representation. The modulated representation is passed through a classification head, which consists of a linear layer mapping from BERT's hidden size (768) to a scalar output. In terms of training details, our model is trained for a maximum of 10 epochs, with early stopping based on validation losses. Training this model takes roughly 6 hours on a Tesla-V100-SXM2-32GB GPU.

### Datasets

We used four real-world datasets for our experiments.

**Adult dataset** The Adult income dataset [7] includes employment data for 48,842 individuals where the task is to predict whether an individual earns more than $50k per year and includes demographic attributes such as age, race and gender. In our experiments, we consider gender as the sensitive attribute.

**COMPAS dataset** The COMPAS recidivism data comprises collected by ProPublica [5], includes information for 6172 defendants from Broward County, Florida. This information comprises 52 features including defendants' criminal history and demographic attributes, and the task is to predict recidivism for defendants. The sensitive attribute in this dataset is the defendants' race where \(A=1\) represents 'African American' and \(A=0\) corresponds to all other races.

**CelebA dataset** The CelebA dataset [28] consists of 202,599 celebrity images annotated with 40 attribute labels. In our task, the objective is to predict whether an individual in the image is smiling. The dataset comprises features in the form of image pixels and additional attributes such as hairstyle, eyeglasses, and more. The sensitive attribute for our experiments is gender.

**Jigsaw Toxicity Classification dataset** The Jigsaw Toxicity Classification dataset [20] contains online comments from various platforms, aimed at identifying and mitigating toxic behavior online. The task is to predict whether a given comment is toxic or not. The dataset includes features such as the text of the comment and certain metadata such as the gender or race to which each comment relates. In our experiments, the sensitive attribute is the gender to which the comment refers, and we only filter the comments which refer to exactly one of'male' or 'female' gender. This leaves us with 107,106 distinct comments.

### Baselines

The baselines considered in our experiments include:

* **Regularization based approaches [8]:** These methods seek to minimise fairness loss using regularized losses as shown in Section 3. We consider different regularization terms \(\mathcal{L}_{\text{fair}}(h_{\theta})\) as smooth relaxations of the fairness violation \(\Phi_{\text{fair}}\) as proposed in the literature [8]. To make this concrete, when the fairness violation under consideration is DP, we consider \[\mathcal{L}_{\text{fair}}(h_{\theta})=\mathbb{E}[g(h_{\theta}(X))\mid A=1]- \mathbb{E}[g(h_{\theta}(X))\mid A=0],\] with \(g(x)=x\) denoted as 'linear' in our results and \(g(x)=\log\sigma(x)\) where \(\sigma(x):=1/(1+e^{-x})\), denoted as 'logsig' in our results. In addition to these methods, we also consider separately trained models with the same regularization term as the YOTO models, i.e., DP: \[\mathcal{L}_{\text{fair}}(h_{\theta})=|\mathbb{E}[\sigma(h_{ \theta}(X))\mid A=1]-\mathbb{E}[\sigma(h_{\theta}(X))\mid A=0]|\] EOP: \[\mathcal{L}_{\text{fair}}(h_{\theta})=|\mathbb{E}[\sigma(h_{\theta}(X)) \mid A=1,Y=1]-\mathbb{E}[\sigma(h_{\theta}(X))\mid A=0,Y=1]|\] EO: \[\mathcal{L}_{\text{fair}}(h_{\theta})=|\mathbb{E}[\sigma(h_{\theta}(X)) \mid A=1,Y=1]-\mathbb{E}[\sigma(h_{\theta}(X))\mid A=0,Y=1]|\] \[+|\mathbb{E}[\sigma(h_{\theta}(X))\mid A=1,Y=0]-\mathbb{E}[\sigma (h_{\theta}(X))\mid A=0,Y=0]|.\] Here, \(\sigma(x):=1/(1+e^{-x})\) denotes the sigmoid function. We denote these models as'separate' in our experimental results as they are the separately trained counterparts to the YOTO model. For each relaxation, we train models for a range of \(\lambda\) values uniformly chosen in \([0,10]\) interval.
* **Reductions Approach [1]:** This method transforms the fairness problem into a sequence of cost-sensitive classification problems. Like the regularization approaches this requires multiple models to be trained. Here, to try and reproduce the trade-off curves, we train the reductions approach with a range of different fairness constraints uniformly in \([0,1]\).
* **KDE-fair [12]:** This method employs a kernel density estimation trick to quantify fairness measures, capturing the degree of irrelevancy of prediction outputs to sensitive attributes. These quantified fairness measures are expressed as differentiable functions with respect to classifier model parameters, allowing the use of gradient descent to solve optimization problems that respect fairness constraints. We focus on binary classification and well-known definitions of group fairness: Demographic Parity (DP), Equalized Odds (EO) and Equalized Opportunity (EOP).
* **Randomized Threshold Optimizer (RTO) [2]:** This scalable post-processing algorithm debiases trained models, including deep neural networks, and is proven to be near-optimal by bounding its excess Bayes risk. RTO optimizes the trade-off curve by applying adjustments

[MISSING_PAGE_FAIL:32]

### Experimental results with FACT Frontiers

**MA FACT and MS FACT yield conservative Pareto Frontiers.** In Figure 27 we include the results for the Adult dataset along with the model-specific (MS) and model-agnostic (MA) FACT Pareto frontiers, obtained using the methodology in [23]. It can be seen that both Pareto frontiers, MS FACT and MA FACT are overly conservative, with the MA FACT being completely non-informative on the Adult dataset. The model-agnostic FACT trade-off considers the best-case trade-off for any given dataset and does not depend on a specific model class. As a result, there is no guarantee that this Pareto frontier is achievable. This is evident from the fact that the MA FACT yields a non-informative Pareto frontier on the Adult dataset in Figure 27, which is non-achievable in practice.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & Unlikely & Plausible & Sub-optimal \\ \hline KDE-fair & 0.0 & 1.0 & 0.0 \\ RTO & 0.0 & 0.67 & 0.33 \\ adversary & 0.15 & 0.85 & 0.0 \\ linear & 0.0 & 0.82 & 0.18 \\ logsig & 0.05 & 0.2 & 0.75 \\ reductions & 0.0 & 0.83 & 0.17 \\ separate & 0.0 & 1.0 & 0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Proportion of empirical trade-offs for each baseline which lie in the three trade-off regions, for the COMPAS dataset and DP (using Bernstein’s CIs). Here \(\mathcal{D}_{\text{cal}}\) is a 10% dataset split.

Figure 16: Equalized Opportunity results for Adult dataset

Figure 17: Equalized Odds results for Adult dataset

[MISSING_PAGE_FAIL:34]

### Synthetic data experiments

In real-world settings, the ground truth trade-off curve \(\tau^{*}_{\text{fair}}\) remains intractable because we only have access to a finite dataset. In this section, we consider a synthetic data setting, where the ground truth trade-off curve can be obtained, to verify that the YOTO trade-off curves are consistent with the ground truth and that the confidence intervals obtained using our methodology contain \(\tau^{*}_{\text{fair}}\).

**Dataset** Here, we consider a setup with \(\mathcal{X}=\mathbb{R}\), \(\mathcal{A}=\{0,1\}\) and \(\mathcal{Y}=\{0,1\}\). Specifically, \(A\sim\text{Bern}(0.5)\) and we define the conditional distributions \(X\mid A=a\) as:

\[X\mid A=a\sim\mathcal{N}(a,0.2^{2})\]

Figure 21: Demographic Parity results for CelebA dataset

Figure 22: Equalized Opportunity results for CelebA dataset

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & Unlikely & Plausible & Sub-optimal \\ \hline KDE-fair & 0.0 & 1.0 & 0.0 \\ RTO & 0.0 & 0.89 & 0.11 \\ adversary & 0.0 & 1.0 & 0.0 \\ linear & 0.0 & 0.76 & 0.24 \\ logsig & 0.0 & 0.45 & 0.55 \\ reductions & 0.0 & 0.7 & 0.3 \\ separate & 0.1 & 0.87 & 0.03 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Proportion of empirical trade-offs for each baseline which lie in the three trade-off regions, for the COMPAS dataset and EO (using Bernstein’s CIs). Here \(\mathcal{D}_{\text{cal}}\) is a 10% dataset split.

Moreover, we define the labels \(Y\) as follows:

\[Y=Z\,\mathbbm{1}(X>0.5)+(1-Z)\,\mathbbm{1}(X\leq 0.5),\]

where \(Z\sim\text{Bern}(0.9)\) and \(Z\perp\!\!\!\perp X\). Here, \(Z\) introduces some 'noise' to the labels \(Y\) and means that perfect accuracy is not achievable by linear classifiers. If perfect accuracy was achievable, the optimal values for Equalized Odds and Equalized Opportunity would be 0 (and would be achieved by the perfect classifier), therefore our use of 'noisy' labels \(Y\) ensures that the ground truth trade-off curves will be non-trivial.

**YOTO model training** Using the data generating mechanism, we generate \(5000\) training datapoints, which we use to train the YOTO model. The YOTO model for this dataset comprises of a simple

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & Unlikely & Plausible & Sub-optimal \\ \hline KDE-fair & 0.0 & 1.0 & 0.0 \\ RTO & 0.0 & 0.33 & 0.67 \\ adversary & 0.0 & 0.0 & 1.0 \\ linear & 0.13 & 0.47 & 0.4 \\ logs & 0.0 & 0.27 & 0.73 \\ reductions & 0.0 & 0.13 & 0.87 \\ separate & 0.03 & 0.95 & 0.02 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Proportion of empirical trade-offs for each baseline which lie in the three trade-off regions, for the Adult dataset and DP (using Bernstein’s CIs). Here \(\mathcal{D}_{\text{cal}}\) is a 10% dataset split.

Figure 24: Demographic Parity results for Jigsaw dataset

Figure 25: Equalized Opportunity results for Jigsaw dataset

logistic regression as the main model, with only the scalar logit outputs of the logistic regression being conditioned using FiLM. The MLPs \(M_{\mu},M_{\sigma}\) both have two hidden layers, each of size 4, and ReLU activations. We train the model for a maximum of 1000 epochs, with early stopping based on validation losses. Training these simple model only requires one CPU and takes roughly 2 minutes.

**Ground truth trade-off curve** To obtain the ground truth trade-off curve \(\tau^{*}_{\text{fair}}\), we consider the family of classifiers

\[h_{c}(X)=\mathbbm{1}(X>c)\]

for \(c\in\mathbb{R}\). Next, we calculate the trade-offs achieved by this model family for a fine grid of \(c\) values between -3 and 3, using a dataset of size 500,000 obtained using the data-generating mechanism described above. The large dataset size ensures that the finite sample errors in accuracy and fairness violation values are negligible. This allows us to reliably plot the trade-off curve \(\tau^{*}_{\text{fair}}\).

#### f.7.1 Results

Figure 28 shows the results for the synthetic data setup for three different fairness violations, obtained using a calibration dataset \(\mathcal{D}_{\text{cal}}\) of size 2000. It can be seen that for each fairness violation considered, the YOTO trade-off curve aligns very well with the ground-truth trade-off curve \(\tau^{*}_{\text{fair}}\). Additionally, we also consider four different confidence intervals obtained using our methodology, and Figure 28

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & Unlikely & Plausible & Sub-optimal \\ \hline KDE-fair & 0.0 & 1.0 & 0.0 \\ RTO & 0.0 & 0.4 & 0.6 \\ adversary & 0.0 & 0.36 & 0.64 \\ LOSSIG & 0.0 & 0.65 & 0.35 \\ reductions & 0.0 & 0.1 & 0.9 \\ separate & 0.0 & 1.0 & 0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Proportion of empirical trade-offs for each baseline which lie in the three trade-off regions, for the Adult dataset and EO (using Bernstein’s CIs). Here \(\mathcal{D}_{\text{cal}}\) is a 10% dataset split.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & Unlikely & Plausible & Sub-optimal \\ \hline KDE-fair & 0.0 & 1.0 & 0.0 \\ RTO & 0.0 & 0.4 & 0.6 \\ adversary & 0.0 & 0.36 & 0.64 \\ LINEAR & 0.0 & 0.4 & 0.6 \\ LOSSIG & 0.0 & 0.65 & 0.35 \\ reductions & 0.0 & 0.1 & 0.9 \\ separate & 0.0 & 1.0 & 0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Proportion of empirical trade-offs for each baseline which lie in the three trade-off regions, for the Adult dataset and EOP (using Bernstein’s CIs). Here \(\mathcal{D}_{\text{cal}}\) is a 10% dataset split.

[MISSING_PAGE_FAIL:38]

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & Unlikely & Plausible & Sub-optimal \\ \hline KDE-fair & 0.0 & 0.92 & 0.08 \\ RTO & 0.0 & 0.5 & 0.5 \\ adversary & 0.0 & 0.09 & 0.91 \\ linear & 0.0 & 0.76 & 0.24 \\ logsig & 0.0 & 0.4 & 0.6 \\ separate & 0.0 & 0.88 & 0.12 \\ \hline \hline \end{tabular}
\end{table}
Table 16: Proportion of empirical trade-offs for each baseline which lie in the three trade-off regions, for the Jigsaw dataset and DP (using Bernstein’s CIs). Here \(\mathcal{D}_{\text{cal}}\) is a 10% dataset split.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & Unlikely & Plausible & Sub-optimal \\ \hline KDE-fair & 0.0 & 0.83 & 0.17 \\ RTO & 0.0 & 0.62 & 0.38 \\ adversary & 0.06 & 0.94 & 0.0 \\ linear & 0.14 & 0.59 & 0.27 \\ logsig & 0.0 & 0.29 & 0.71 \\ separate & 0.0 & 0.47 & 0.53 \\ \hline \hline \end{tabular}
\end{table}
Table 17: Proportion of empirical trade-offs for each baseline which lie in the three trade-off regions, for the Jigsaw dataset and EOP (using Bernstein’s CIs). Here \(\mathcal{D}_{\text{cal}}\) is a 10% dataset split.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & Unlikely & Plausible & Sub-optimal \\ \hline KDE-fair & 0.0 & 0.83 & 0.17 \\ RTO & 0.0 & 0.62 & 0.38 \\ adversary & 0.06 & 0.94 & 0.0 \\ linear & 0.14 & 0.59 & 0.27 \\ logsig & 0.0 & 0.29 & 0.71 \\ separate & 0.0 & 0.47 & 0.53 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Proportion of empirical trade-offs for each baseline which lie in the three trade-off regions, for the CelebA dataset and EO (using Bernstein’s CIs). Here \(\mathcal{D}_{\text{cal}}\) is a 10% dataset split.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & Unlikely & Plausible & Sub-optimal \\ \hline KDE-fair & 0.0 & 0.75 & 0.25 \\ RTO & 0.0 & 0.57 & 0.43 \\ adversary & 0.0 & 1.0 & 0.0 \\ linear & 0.0 & 0.73 & 0.27 \\ logsig & 0.0 & 0.4 & 0.6 \\ separate & 0.0 & 0.87 & 0.13 \\ \hline \hline \end{tabular}
\end{table}
Table 18: Proportion of empirical trade-offs for each baseline which lie in the three trade-off regions, for the Jigsaw dataset and EO (using Bernstein’s CIs). Here \(\mathcal{D}_{\text{cal}}\) is a 10% dataset split.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Section 3 includes the theoretical results and Section 5 includes the experimental results as claimed in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section 6 includes a discussion of the limitations of our methodology. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Appendix A contains all proofs and assumptions. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Appendix F contains comprehensive practical details needed to reproduce the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: Our code has been made open source, and a link to our code has been included in Section 5. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Section 5 and Appendix F include the training and test details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Table 1 in Section 5 report the standard errors in our results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Appendix F includes information on the compute resources used. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have tried to be transparent regarding our methodology, and theoretical and practical details to reproduce and use our method in practice. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The Introduction (Section 1) and Preliminaries (Section 2) discuss the implications of our method for fairness decisions. We contribute to better auditing of model fairness standards and are unaware of the negative social impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We use open-source data to train models. We do not release trained models (only the code used to train the models). Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have included details of our code and models in our submission. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.