# One Fits All:

Power General Time Series Analysis by Pretrained LM

 Tian Zhou\({}^{*}\) Peisong Niu\({}^{*}\) Xue Wang\({}^{*}\) Liang Sun Rong Jin\({}^{\dagger}\)

{tian.zt,niqueisong.nps,xue.w,liang.sun,jinrong.jr}@alibaba-inc.com

* Equal contribution\(\dagger\) Corresponding authors

###### Abstract

Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time series. Our results demonstrate that pre-trained models on natural language or images can lead to a comparable or state-of-the-art performance in all main time series analysis tasks, as illustrated in Figure 1. We also found both theoretically and empirically that the self-attention module behaviors similarly to principle component analysis (PCA), an observation that helps explains how transformer bridges the domain gap and a crucial step towards understanding the universality of a pre-trained transformer. The code is publicly available at https://github.com/DAMO-DI-ML/One_Fits_All.

## 1 Introduction

Time series analysis is a fundamental problem Hyndman & Athanasopoulos (2021) that has played an important role in many real-world applications Wen et al. (2022), such as retail sales forecasting Bose et al. (2017); Courty & Li (1999), imputation of missing data for economic time series Friedman (1962) anomaly detection for industrial maintenance Gao et al. (2020), and classification of time series from various domain Ismail Fawaz et al. (2019). Numerous statistical and machine learning methods have been developed for time series analysis in the past. Inspired by its great success in natural language processing and computer vision Vaswani et al. (2017); Devlin et al. (2019); Dosovitskiy et al. (2021); Rao et al. (2021), transformer has been introduced to various time series tasks with promising results Wen et al. (2023), especially for time series forecasting Lim et al. (2021); Zhou et al. (2022, 2021); Wu et al. (2021); Nie et al. (2022).

We have recently witnessed the rapid development of foundation models in NLP. The key idea is to pre-train a large language model from billions of tokens to facilitate model training for downstream tasks, particularly when we have a few, sometimes even zero, labeled instances. Another advantage ofconventional wisdom where each task requires a specially designed algorithm. However, so far, little progress has been made to exploit pre-trained or foundation models for time series analysis. One main challenge is the lack of the large amount of data to train a foundation model for time series analysis. The largest data sets for time series analysis is less than 10GB Godahewa et al. (2021), which is much smaller than that for NLP. To address this challenge, we propose to leverage pre-trained language models for general time series analysis. Our approach provides a **unified framework** for diverse time series tasks, such as classification, anomaly detection, forecasting, and few-shot or zero-shot learning. As shown in Figure 1, using the same backbone, our approach performs either on-par or better than the state-of-the-art methods for all main time series analysis tasks. Besides extensive empirical studies, we also investigate why a transformer model pre-trained from the language domain can be adapted to time series analysis with almost no change. Our analysis indicates that the self-attention modules in the pre-trained transformer acquire the ability to perform certain non-data-dependent operations through training. These operations are closely linked to principal component analysis over the input patterns. We believe it is this generic function performed by the self-attention module that allows trained transformer models to be so-called universal compute engine Lu et al. (2022) or general computation calculator Giannou et al. (2023). We support our claims by conducting an empirical investigation of the resemblance in model behaviors when self-attention is substituted with PCA, and by providing a theoretical analysis of their correlation.

Here we summarize our key contributions as follows:

1. We propose a unified framework that uses a frozen pre-trained language model to achieve a SOTA or comparable performance in all major types of time series analysis tasks supported by thorough and extensive experiments, including time series classification, short/long-term forecasting, imputation, anomaly detection, few-shot and zero-sample forecasting.
2. We found, both theoretically and empirically, that self-attention performs a function similar to PCA, which helps explain the universality of transformer models.
3. We demonstrate the universality of our approach by exploring a pre-trained transformer model from another backbond model (BERT) or modality (computer vision) to power the time series forecasting.

The remainder of this paper is structured as follows. Section 2 briefly summarizes the related work. Section 3 presents the proposed detailed model structure. In Section 4, we conduct a thorough and extensive evaluation of the performance of cross-modality time series analysis using our proposed method in seven main time series analysis tasks compared to various SOTA baseline models. Section 5

Figure 1: Model performance comparison on various tasks.

presents various ablation studies, and Section 6 demonstrates the universality of our proposed method using pre-trained models with another structure or pre-trained from another modality. In Section 7, we provide a theoretical explanation of the connection between self-attention and PCA. Finally, in Section 8, we discuss our results and future directions. Due to space limit, more extensive discussion of related work, experimental results, and theoretical analysis are provided in the Appendix.

## 2 Related Work

In this section, we provide short reviews of literature in the areas of time series analysis, in-modality transfer learning, and cross-modality knowledge transfer learning. We postpone the discussion of works for end-to-end time series analysis to Appendix B, due to the limited space.

In-modality Transfer Learning through pre-trained modelsIn recent years, a large number of research works have verified the effectiveness of the pre-trained model from NLP, CV to Vision-and-Language (VL). Latest studies for NLP focus on learning contextual word embeddings for downstream tasks. With the increase of computing power, the very deep transformer models have shown powerful representation ability in various language tasks. Among them, BERT Devlin et al. (2019) uses transformer encoders and employs masked language modeling task that aims to recover the random masked tokens within a text. OpenAI proposed GPT Radford and Narasimhan (2018) that trains transformer decoders on a large language corpus and then fine-tunes on task-specific data. GPT2 Radford et al. (2019) is trained on larger datasets with much more parameters and can be transferred to various downstream tasks. Since transformer models can adapt to various inputs, the idea of pre-training can also be well adapted to visual tasks. DEiT Touvron et al. (2021) proposed a teacher-student strategy for transformers with convolution neural networks (CNNs) as the teacher model and achieves competitive performance. BEiT Bao et al. (2022) converts images as visual tokens and successfully uses the BERT model in CV. However, because of the **insufficient training sample**, there is little research on pre-trained models on general time series analysis that cover all major tasks like CV or NLP domain.

Cross-modality knowledge transferSince transformers can handle different modal tasks through tokenizing the inputs to embeddings, it is also an interesting topic whether the transformers have universal representation ability and can be used for transferring between various domains. The VL pre-trained model VLMo Bao et al. (2021) proposed a stagewise pre-training strategy that utilizes frozen attention blocks pre-trained by image-only data to train the language expert. One of the most related works which transfer knowledge from a pre-trained language model to other domains is Lu et al. (2022), which studies the strong performance of a frozen pre-trained language model (LM) compared to an end-to-end transformer alternative learned from other domains' data. Another relative work for knowledge transfer to the time series is the Voice2series Yang et al. (2021), which leverages a pre-trained speech processing model for time series classification and achieves superior performance. To the best of our knowledge, no previous research has investigated cross-modality knowledge transfer for the time series forecasting task, let alone general time series analysis.

## 3 Methodology

### Model Structure

The architecture we employ is depicted in Figure 2. We utilize parameters from NLP pretrained transformer models for time series analysis, with a focus on the GPT2 model Radford et al. (2019). We also experiment with other models, such as BERT Devlin et al. (2019) and BEiT Bao et al. (2022), to further demonstrate that the universal performance of cross-domain knowledge transfer exists in a wide range of pre-trained models.

**Frozen Pretrained Block** Our architecture retains the positional embedding layers and self-attention blocks from the pre-trained models. As self-attention layers and FFN (Feedforward Neural Networks) contain the majority of learned knowledge from pre-trained language models, we opt to freeze the self-attention blocks while fine-tuning.

**Positional Embeddings and Layer Normalization** To enhance downstream tasks with minimal effort, we fine-tune the positional embeddings and layer normalization layer, which is considered a standard practiceLu et al. (2022); Houlsby et al. (2019). As a result, we retrain these components during fine-tuning.

**Input Embedding** Given our goal of applying the NLP pre-trained model to various tasks and a new modality, we must redesign and train the input embedding layer. This layer is responsible for projecting the time-series data to the required dimensions of the specific pre-trained model. To accomplish this, we use linear probing, which also reduces the number of parameters required for training.

**Normalization** Data normalization is crucial for pre-trained models across various modalities. In addition to the layer norm utilized in pre-trained LM, we also incorporate a simple data normalization block, reverse instance norm Kim et al. (2022), to further facilitate knowledge transfer. This normalization block simply normalizes the input time series using mean and variance, and then adds them back to the output.

**Patching** To extract local semantic information, we utilize patching Nie et al. (2022) by aggregating adjacent time steps to form a single patch-based token. Patching enables a significant increase in the input historical time horizon while maintaining the same token length and reducing information redundancy for transformer models. In our architecture, we apply patching after instance normalization.

## 4 Main Time Series Analysis Tasks

Our proposed method excels in various downstream time series analysis tasks through fine-tuning. To demonstrate the effectiveness of our approach, we conduct extensive experiments on major types of downstream tasks, including time series classification, anomaly detection, imputation, short/long-term forecasting and few-shot/zero-shot forecasting. To ensure a fair comparison, we use GPT2-backbone FPT and adhere to the experimental settings of TimesNet Wu et al. (2023). Due to the space limit, only the summarized results are presented below except zero-shot forecasting. Full experimental results of the other six downstream tasks can be found in Appendix D.3, D.2, D.7, H.6, H.7, H.8, H.9 respectively.

**Baselines** We select representative baselines and cite their results from Wu et al. (2023), which includes the most recent and quite extensive empirical studies of time series. The baselines include CNN-based models: TimesNet Wu et al. (2023); MLP-based models: LightTS Zhang et al. (2022) and DLinear Zeng et al. (2023); Transformer-based models: Reformer Kitaev et al. (2020), Informer Zhou et al. (2021), Autoformer Wu et al. (2021), FEDformer Zhou et al. (2022), Non-stationary Transformer Liu et al. (2022), ETSformer Woo et al. (2022), PatchTST Nie et al. (2022). Besides, N-HiTS Challu et al. (2022) and N-BEATS Oreshkin et al. (2019) are used for short-term forecasting. Anomaly Transformer Xu et al. (2021) is used for anomaly detection. XGBoost Chen and Guestrin (2016), Rocket Dempster et al. (2020), LSTNet Lai et al. (2018), LSSL Gu et al. (2021),

Figure 2: Model architecture. Pre-trained parameters are transferred to the time series forecasting tasks. Self-attention and Feedforward layers in the transformer blocks are frozen while only the embedding layer, normalization layers, and output layer require training.

Pyraformer Liu et al. (2021), TCN Franceschi et al. (2019) and Flowformer Huang et al. (2022) are used for classification.

### Main Results

Overall, as shown in Figure 1, GPT2-backbone FPT outperforms other models in most tasks, including long/short-term forecasting, classification, anomaly detection, imputation, and few-shot/zero-short forecasting. This confirms that time series tasks can also take advantage of cross-modality transferred knowledge. In the following, we use GPT2(K) to represent GPT2-backbone with first K Layers.

### Imputation

**Setups** We conduct experiments on six popular real-world datasets, including 4 ETT datasets Zhou et al. (2021) (ETTh1, ETTh2, ETTm1, ETTm2), Electricity and Weather, where the data-missing is common. Following the settings of TimesNet, different random mask ratios ({12.5%, 25%, 37.5%, 50%}) of time points are selected for the evaluation on various proportions of missing data.

**Results** The results are shown in Table 1 that GPT2(3) FPT achieves the best performance on most datasets. Particularly, compared to the previous SOTA TimesNet, GPT2(3) FPT yields a relative **11.5%** MSE reduction on ETTh1,and a **4.1%** MSE reduction on average on six benchmark datasets. It verifies that the proposed method can also effectively mine temporal patterns of incomplete time series.

### Time Series Classification

**Setups** To evaluate the model's capacity for high-level representation learning, we employ sequence-level classification. Specifically, we follow the same setting as TimesNet: For classification, 10 multivariate UEA classification datasets Bagnall et al. (2018) are selected for evaluation, including gesture, action, audio recognition medical diagnosis and other practical tasks.

**Results** As shown in Figure 3, GPT2(6) FPT achieves an average accuracy of 74.00%, surpassing all baselines including TimesNet (73.60%). Specifically, compared to recent published patch-transformer-based models Nie et al. (2022), GPT2(6) FPT surpasses it by a large margin **9.0%** which shows the prior NLP transfer knowledge can indeed help in time series representation.

### Time Series Anomaly Detection

**Setups** Detecting anomalies in time series is vital in industrial applications, ranging from health monitoring to space & earth exploration. We compare models on five commonly used datasets, including

\begin{table}
\begin{tabular}{c|c c c c|c c c|c c c|c c c|c c c|c c c|c c c|c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{GPT2(3)} & \multicolumn{3}{c|}{TimeNet} & \multicolumn{3}{c|}{PatchFST} & \multicolumn{3}{c|}{ETSGformer} & \multicolumn{3}{c|}{LightTS} & \multicolumn{3}{c|}{DLinear} & \multicolumn{3}{c|}{FEDformer} & \multicolumn{3}{c|}{Sutochary} & \multicolumn{3}{c|}{Autoformer} & \multicolumn{3}{c|}{Informer} & \multicolumn{3}{c}{Reference} \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline ETTm1 & **0.028** & **0.105** & **0.027** & **0.107** & 0.047 & 0.140 & 0.120 & 0.253 & 0.104 & 0.218 & 0.093 & 0.206 & 0.062 & 0.177 & 0.036 & 0.126 & 0.051 & 0.150 & 0.071 & 0.188 & 0.055 & 0.166 \\ ETTm2 & **0.021** & **0.084** & **0.022** & **0.088** & 0.025 & 0.102 & 0.208 & 0.327 & 0.246 & 0.151 & 0.096 & 0.208 & 0.101 & 0.215 & 0.206 & 0.099 & 0.105 & 0.156 & 0.292 & 0.157 & 0.280 \\ ETTh1 & **0.069** & **0.173** & **0.037** & **0.137** & 0.115 & 0.242 & 0.202 & 0.329 & 0.284 & 0.373 & 0.201 & 0.306 & 0.117 & 0.246 & 0.090 & 0.201 & 0.103 & 0.214 & 0.161 & 0.279 & 0.122 & 0.245 \\ ETTh2 & **0.083** & **0.141** & **0.049** & **0.140** & 0.045 & 0.163 & 0.367 & 0.436 & 0.119 & 0.250 & 0.142 & 0.259 & 0.163 & 0.279 & 0.053 & 0.132 & 0.055 & 0.156 & 0.337 & 0.252 & 0.234 & 0.352 \\ ECI. & **0.090** & **0.270** & **0.027** & **0.020** & **0.102** & **0.017** & **0.135** & 0.214 & 0.339 & 0.119 & 0.262 & 0.123 & 0.260 & 0.190 & 0.128 & 0.101 & 0.255 & 0.232 & 0.325 & 0.200 & 0.313 \\ Weather & **0.031** & **0.056** & **0.030** & **0.054** & 0.034 & 0.055 & 0.076 & 0.171 & 0.055 & 0.117 & 0.052 & 0.110 & 0.099 & 0.203 & 0.032 & 0.059 & 0.031 & 0.057 & 0.045 & 0.104 & 0.038 & 0.087 \\ \hline Average & **0.047** & **0.127** & **0.049** & **0.132** & 0.060 & 0.144 & 0.197 & 0.309 & 0.123 & 0.228 & 0.119 & 0.224 & 0.112 & 0.229 & 0.056 & 0.142 & 0.061 & 0.151 & 0.165 & 0.273 & 0.134 & 0.240 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Imputation task. We randomly mask {12.5%, 25%, 37.5%, 50%} time points of 96-length time series. The results are averaged from 4 different mask ratios. **Black**: best, **Red**: second best. Appendix H.8 shows the full results.

Figure 3: Model comparison in classification. The results are averaged from 10 subsets of UEA. Appendix H.6 shows the full results.

[MISSING_PAGE_FAIL:6]

zero-shot learning also represent the ultimate tasks for a universal time series forecasting model. To extensively evaluate the representation power of the GPT2(6) for time series analysis, we conduct experiments under few-shot and zero-shot learning settings.

Similar to traditional experimental settings, each time series is split into three parts: training data, validation data, and test data. For few-shot learning, only a certain percentage (10%, 5%) timesteps of training data are used.

The results of 10% few-shot learning are shown in Table 5. Compared to TimesNet, DLinear, PatchTST and other methods, GPT2(6) FPT achieves the best performance. Traditionally, CNN-based and single MLP-based models are considered more data-efficient for training and suitable for few-shot learning methods. In comparison to convolution-based TimesNet and MLP-based DLinear models, GPT2(6) FPT demonstrates a relative average MSE reduction of **33.3%** and **13.5%** respectively. We add a comparison with traditional algorithms (ETS, ARIMA, NaiveDrift) in the Appendix D.5 as well, and GTP2(6)FPT also surpass all those traditional methods.

### Zero-shot forecasting

This task is used to evaluate the cross datasets adaption ability of our proposed algorithm, i.e. how well a model is able to perform on dataset \(A\) (without any training data from \(A\)) when it is trained from dataset \(B\).

The results are summarized in Table 6. The GPT2(6) FPT model consistently outperforms all recent state-of-the-art transformer and MLP-based time series forecasting methods. Compared to recently published state-of-the-art MLP-based method Dlinear, convolution-based method Timesnet, and transformer-based method Patchtst, GPT2(6)FPT demonstrates a relative average metric reduction of **13.1%**,**13.6%** and **7.3%**, respectively. Also, the proposed method is comparable to N-BEATS without any meta-learning design and outperforms N-BEATS in the ELECTR dataset. We attribute this to the knowledge transfer capability from the FPT model.

## 5 Ablations

In this section, we conduct several ablations on model selection and effectiveness of pre-training. The detailed results are shown in Appendix H. We introduce several variants, GPT2(0) FPT, GPT2(6) without freezing and GPT2(6) without pre-training.

**Model Selection** We separately analyze the number of GPT2 layers and the fine-tuning parameters selection. The results in Appendix H show that GPT2 with 6-layers is a sound choice compared to full or few layers and partially freezing can avoid catastrophic forgetting, enabling fine-tuning without overfitting.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c} \hline \hline Methods & GPT2(6) & TimesNet & PatchTST & N-HITS & N-BEATS & ETStomer & Light/TS & DLinear & FEDferner & Stationary & Autoformer & Informer & Refformer \\ \hline SMAPE & 11.991 & **11.829** & 12.059 & 11.927 & **11.851** & 14.718 & 13.525 & 13.639 & 12.840 & 12.780 & 12.909 & 14.086 & 18.200 \\ MASE & 1.600 & **1.585** & 1.623 & 1.613 & **1.599** & 2.408 & 2.111 & 2.095 & 1.701 & 1.756 & 1.771 & 2.718 & 4.223 \\ OWA & 0.861 & **0.851** & 0.869 & **0.861** & **0.855** & 1.172 & 1.051 & 1.051 & 0.918 & 0.930 & 0.939 & 1.230 & 1.775 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Short-term forecasting task on M4. The prediction lengths are in [6; 48] and results are weighted averaged from several datasets under different sample intervals. **Black**: best, **Red**: second best. Appendix H.9 shows the full results.

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c|c c c|c c c|c c} \hline \hline Methods & \begin{tabular}{c} GPT2(6) \\ MSE MAE \\ \end{tabular} & \begin{tabular}{c} TimeNet \\ MSE MAE \\ \end{tabular} & \begin{tabular}{c} Dlinear \\ MSE MAE \\ \end{tabular} & \begin{tabular}{c} FEDferner \\ MSE MAE \\ \end{tabular} & \begin{tabular}{c} PatchTST \\ MSE MAE \\ \end{tabular} & \begin{tabular}{c} Autoformer \\ MSE MAE \\ \end{tabular} & \begin{tabular}{c} Sensitivity \\ MSE MAE \\ \end{tabular} & \begin{tabular}{c} ETSformer \\ MSE MAE \\ \end{tabular} & \begin{tabular}{c} Light/TS \\ MSE MAE \\ \end{tabular} & \begin{tabular}{c} Inference \\ MSE MAE \\ \end{tabular} & 
\begin{tabular}{c} Reference \\ MSE MAE \\ \end{tabular} \\ \hline Weather & 0.328 & 0.725 & 0.301 & 0.301 & 0.302 & 0.283 & 0.284 & 0.234 & 0.241 & 0.279 & 0.300 & 0.342 & 0.312 & 0.317 & 0.359 & 0.293 & 0.359 & 0.329 & 0.357 & 0.494 & 0.455 & 0.469 \\ ETH1 & 0.590 & 0.580 & 0.5869 & 0.519 & 0.559 & 0.683 & 0.519 & 0.532 & 0.596 & 0.916 & 0.594 & 0.595 & 0.916 & 0.459 & 0.593 & 1.759 & 0.787 & 11.908 & 0.289 & 0.333 \\ ETH2 & 0.397 & 0.242 & 0.473 & 0.486 & 0.683 & 0.538 & 0.466 & 0.515 & 0.418 & 0.488 & 0.499 & 0.454 & 0.583 & 0.713 & 0.265 & 1.871 & 1.512 & 1.535 & 1.485 & 1.485 \\ ETH2 & 0.464 & 0.441 & 0.676 & 0.537 & 0.411 & 0.429 & 0.271 & 0.695 & 0.501 & 0.466 & 0.302 & 0.628 & 0.797 & 0.577 & 0.999 & 0.741 & 1.920 & 0.182 & 1.425 & 0.856 \\ ETH2 & 0.293 & 0.313 & 0.319 & 0.315 & 0.316 & 0.368 & 0.438 & 0.488 & 0.296 & 0.343 & 1.341 & 0.930 & 0.323 & 0.366 & 0.447 & 0.487 & 0.497 & 0.795 & 1.356 & 1.493 & 0.377 & 1.586 \\ EL6 & **1.16** & 0.269 & 0.230 & 0.329 & 0.318 & 0.320 & 0.366 & 0.428 & 0.130 & 0.269 & 0.413 & 0.478 & 0.435 & 0.479 & 0.659 & 0.617 & 0.441 & 0.488 & 1.194 & 0.800 & 0.965 & 0.768 \\ Traffic & 0.440 & 0.309 & 0.591 & 0.535 & 0.496 & 0.371 & 0.663 & 0.425 & 0.430 & 0.305 & 0.479 & 0.446 & 1.453 & 0.815 & 1.913 & 0.936 & 1.247 & 0.684 & 1.534 & 0.811 & 1.550 & 0.821 \\ \hline Average & **0.371** & **0.367** & 0.556 & 0.458 & 0.429 & 0.409 & 0.511 & 0.472 & **0.385** & **0.376** & 0.687 & 0.559 & 0.674 & 0.522 & 0.912 & 0.665 & 1.137 & 0.712 & 1.850 & 0.967 & 1.888 & 0.974 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Few-shot learning task on 10% data. All the results are averaged from 4 different prediction lengths ((96, 192, 336, 720)). **Black**: best, **Red**: second best. Appendix D.2 shows the detailed results of 10% and 5% data.

**Effectiveness of Pre-training** The results are shown in Table 7, GPT2(6) FPT outperforms both GPT2(0) FPT and GPT2-random-initialized, suggesting that GPT2 with pre-training parameters can achieve improvement on times series tasks. Besides, GPT2(6) FPT performs better than GPT2-unfrozen, demonstrating that partially freezing also helps. Also, results in Appendix H.2 show that random initialized GPT2(6) with freezing performs poorly and the pre-trained knowledge is instrumental for time series tasks.

## 6 Exploring Transfer Learning from others: The Unexceptional Nature of GPT2-based-FPT

We also present experiments on BERT-backbond FPT Devlin et al. (2019) model and the image-pretrained BEiT-backbone FPT model Bao et al. (2022) to illustrate the generality of pre-trained models for cross-domain knowledge transferring. The results in Table 8 demonstrate that the ability of knowledge transfer is not exclusive to GPT2-based pre-trained language models. Subsequently, our theoretical analysis will shed light on the universality of this phenomenon.

## 7 Training/Inferencing Cost

Analysis of computational cost is helpful for investigating the practicality of the LLM-based model. The results can be found in table 9. Each baseline model comes in two variants, featuring model

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Methods & M4 & M3 & TOURISM & ELECTR \\ Metric & sMAPE & sMAPE & MAPE & \(ND\times 100\) \\ \hline N-BEATS & **11.70** & **12.44** & **18.82** & **17.8** & **15.19** \\ \hline DLinear & 15.33 & 14.03 & 28.51 & 17.6 & 18.86 \\ TimesNet & 13.55 & 14.17 & 28.84 & 19.3 & 18.96 \\ PatchTIST & 13.22 & **13.06** & 27.10 & 17.3 & **17.67** \\ ETSformer & 27.74 & 16.03 & 180.40 & 44.2 & 67.09 \\ LightTIST & 13.62 & 17.90 & 66.99 & 19.6 & 29.52 \\ Stationary & 13.32 & 15.29 & 43.75 & 22.0 & 23.59 \\ FEDformer & 15.04 & 13.53 & 31.55 & 18.4 & 19.63 \\ Autoformer & 20.02 & 15.87 & 40.39 & 33.9 & 27.54 \\ Informer & 19.04 & 15.82 & 35.82 & 21.2 & 22.97 \\ Reformer & 14.09 & 13.37 & 25.48 & 21.6 & 18.63 \\ \hline GPT2(6) & **13.12** & **13.06** & **22.14** & **17.2** & **16.38** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Zero-shot learning results. Dataset-specific metrics aggregated over each dataset. A lower value indicates better performance. The source dataset of M3, Tourism, Electricity are M4. For M4, the source data for N-BEATS is FRED, and M3 for other models. **Black**: best, **Red**: second best, Violet: third best. Appendix D.7 shows full results.

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{GPT2(6)} & \multicolumn{2}{c|}{GPT2(0)} & \multicolumn{2}{c|}{No Freeze} & \multicolumn{2}{c}{No Pretrain} \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline Weather & **0.237** & **0.270** & 0.263 & 0.297 & 0.273 & 0.302 & 0.277 & 0.305 \\ ETTh1 & **0.427** & **0.426** & 0.874 & 0.647 & 0.753 & 0.596 & 1.326 & 0.743 \\ ETTh2 & **0.346** & **0.394** & 0.666 & 0.559 & 0.447 & 0.451 & 0.502 & 0.479 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation study on 10% data. All the results are averaged from 4 different prediction lengths. **No Freeze** represents GPT2(6) without freezing, **No Pretrain** represents GPT2(6) without pre-training. **Black**: best.

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{GPT2(6)} & \multicolumn{2}{c|}{BERT(6)} & \multicolumn{2}{c|}{BEiT(6)} & \multicolumn{2}{c|}{DLinear} & \multicolumn{2}{c|}{PatchTST} & \multicolumn{2}{c|}{FEDformer} & \multicolumn{2}{c}{Autoformer} \\  & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline ETTh2 & **0.400** & **0.433** & 0.452 & 0.451 & 0.459 & 0.454 & 0.827 & 0.615 & 0.439 & 0.448 & 0.441 & 0.457 & 0.470 & 0.489 \\ ETTm2 & **0.308** & **0.346** & 0.318 & 0.357 & 0.315 & 0.357 & 0.399 & 0.426 & 0.314 & 0.352 & 0.381 & 0.404 & 0.388 & 0.433 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results of frozen pretrained transformer variants on 5% ETTh2 and ETTm2. All the results are averaged from 4 different prediction lengths. **Black**: best. Appendix H.5 shows the full results.

hidden dimensions of 32 and 768, which align with GPT-2's specifications. Furthermore, the majority of the baseline models consist of three layers. We assessed the computational cost using a batch from ETH2 (with a batch size of 128) on a 32G V100 GPU.

The results indicate that GPT-2(3) has substantially enhanced time efficiency and reduced parameter quantity compared to baselines with the same model dimension. This was a surprise since we initially anticipated that this large language model might be slower. However, we surmise that the efficient optimization of huggingface's GPT model implementation primarily accounts for such a significant improvement in time costs. Furthermore, GPT-2(3) and GPT-2(6) demonstrate a mere 6.12% and 4.60% proportion of learnable parameters among the overall parameter size, respectively.

## 8 Towards Understanding the Universality of Transformer: Connecting Self-Attention with PCA

The observation, i.e. we can directly use a trained LM for time series forecasting without having to modify its model, makes us believe that the underlying model is doing something very generic and independent from texts despite it being trained from text data. Our analysis aims to show that part of this generic function can be related to PCA, as minimizing the gradient with respect to the self-attention layer seems to do something similar to PCA. In this section, we take the first step towards revealing the generality of self-attention by connecting the self-attention with principal component analysis (PCA). Moreover, when coming the question of why fine-tuning is restricted to the embedding layer and layer norm, following our hypothesis that the pre-trained LM as a whole performs something generic, partially fine-tuning any of its components may break the generic function and lead to relatively poor performance for time series analysis.

For each layer, we calculate and perform statistical analysis of the pairwise token similarity values. Specifically, we denote each output feature map with shape of \((b,n,d)\), where \(b\) is the batch size, \(n\) is the number of tokens, and \(d\) is the dimension of each token feature. We calculate the cosine similarity, and the resulting pairwise similarity matrix of shape \((b,n,n)\). Next we count the number of occurrences of similarity values within each interval as a simple statistical analysis.

Our analysis is motivated by the observation that the within-layer token similarity increases with deeper layers in transformer. We report the layer-wise average token cosine similarity on ETH2 dataset in Figure 4 (a, c), where we mix weights from pre-trained LM with weights randomly sampled from Gaussian distribution. Here we summarize our observations: a) in a randomly initialed GPT2 (6) model, the token similarity is low among all layers (\(0.1-0.2\)); b) when gradually switched to the pretrained GPT2 model, the token similarity significantly increases in the deep layers and eventually reaches more than 0.9 in the last layer. One potential explanation for the increasing token similarity is that all the token vectors are projected into the low-dimensional top eigenvector space of input patterns. To verify this idea, we further conduct experiments where we replace the self-attention module with PCA and find token similarity patterns remain unchanged according to Figure 4 (b), which further justifies the potential connection between PCA and self-attention.

To build the theoretical connection between PCA and self-attention, we first analyze the gradient structure of self-attention. Let \(X={(x_{1},\dots,x_{N})}^{\top}\in\mathbb{R}^{N\times D}\) be the input pattern, and let \(f(X)={(f_{1}(X),\dots,f_{N}(x))}^{\top}:\mathbb{R}^{N\times D}\mapsto\mathbb{ R}^{N\times D}\) be the function for self-attention, i.e., \(f_{i}(X)=\text{softmax}(XAX^{\top})X\quad\text{where }A=W_{Q}W_{K}^{\top}\in\mathbb{R}^{D\times D}\).

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Model & Training Params & Training Params Percentages & Training Time for 1 step(s) & Inference Time for 1 Batch(s) \\ \hline FEDformer-32 & 44k & 100 & 0.889 & 0.170 \\ TimesNet-32 & 2M & 100 & 0.747 & 0.302 \\ PatchTST-32 & 543K & 100 & 0.043 & 0.022 \\ \hline FEDformer-768 & 33M & 100 & 0.208 & 0.056 \\ TimesNet-768 & 42M & 100 & 5.723 & 2.162 \\ PatchTST-768 & 20M & 100 & 0.457 & 0.123 \\ GPT-2(3)-768 & 4M & 6.12 & 0.093 & 0.032 \\ GPT-2(6)-768 & 4M & 4.6 & 0.104 & 0.054 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Training parameters and Training/Inference Cost Comparison

**Lemma 8.1**.: Let the Jacobian \(J=\left[\frac{\partial f_{i}(X)}{\partial x_{j}}\right]_{i,j=1}^{N}\) represent the gradient \(f(X)\) w.r.t the input pattern, then we have \(|J|_{2}\leq|A|_{2}\sum_{i=1}^{N}\left(P_{i,i}+\frac{1}{2}\right)\left|x_{i}- \sum_{j=1}^{N}P_{i,j}x_{j}\right|^{2}+\Delta\) where

\(\Delta=|A|_{2}\sum_{i\neq j}^{N}P_{i,j}\left|x_{j}-\sum_{k=1}^{N}P_{i,k}x_{k} \right|^{2}+\frac{|A|_{2}}{2}\sum_{j=1}^{N}|x_{i}|^{2}\) and \(P_{i,j}=\frac{\exp(x_{i}^{\top}Ax_{j})}{\sum_{k=1}^{N}\exp(x_{i}^{\top}Ax_{k} )}\).

This lemma reveals an important gradient structure of \(J\). The proof of essentially follows the analysis in Kim et al. (2021), and we include it in Appendix G for completeness.

Using the gradient structure revealed in Lemma 8.1, we can connect self-attention with PCA. In order to minimize the norm of gradient \(|J|_{2}\), we essentially need to make \(\sum_{i=1}^{N}|x_{i}-\sum_{j=1}^{N}P_{i,j}x_{j}|^{2}\) small. When \(A\) is small and all the input patterns are centered at 0 (i.e. \(\sum_{i=1}^{N}x_{i}=0\)), we have \(\sum_{i=1}^{N}|x_{i}-X^{\top}P_{i,:}|^{2}\approx\sum_{i=1}^{N}|x_{i}-X^{\top} XAx_{i}|^{2}\).

The theorem below shows that \(A\) minimizing the objective \(\sum_{i=1}^{N}|x_{i}-X^{\top}XAx_{i}|^{2}\) contains the largest \(m\) eigenvectors of \(X^{\top}X\) where \(m\) is the rank of \(A\).

_Theorem 1_.: Let \(W_{Q}\) and \(W_{K}\) be matrices of size \(D\times m\). Let \(\lambda_{1}\geq\lambda_{2}\geq...\geq\lambda_{D}\) be the eigenvalues of \(X^{\top}X\) ranked in descending order, and let \(v_{i}\in\mathbb{R}^{D},i=1,\ldots,D\) be the corresponding eigenvectors. The optimal solution \(A^{*}\) that minimizes \(\sum_{i=1}^{N}|x_{i}-X^{\top}XAx_{i}|^{2}\) is given by \(A=\sum_{i=1}^{m}\frac{1}{\lambda_{i}}v_{i}v_{i}^{\top}\).

The proof of Theorem 1 can be found in Appendix G. Following Theorem 1, through the training of pushing gradient to zero, self-attention learns to perform a function closely related to PCA.

## 9 Conclusions

In this paper, we developed a foundation model for time series analysis, based on pre-trained model from NLP or CV, that can (a) facilitate the model training for downstream tasks, and (b) provide unified framework for diverse time series analysis tasks. Our empirical studies show that the proposed method performs on par or better than the state-of-the-art approaches on almost all time series tasks. We also examine the universality of transformer by connecting self-attention with PCA, an important step towards understanding how generative models work in practice. On the other hand, we do recognize some limitations of our work: the zero-shot performance of our approach is still behind N-beat on several datasets, and our analysis of the generality of transformer is still in the early stage. Moving forward, we plan to improve the performance of our approach by exploiting the parameter efficient fine-tuning approaches which usually introduce additional structures into the pre-trained model for better adaption. To better understand the universality of transformer, we also plan to examine it from the viewpoint of n-gram language model, an approach that is taken by Elhage et al. (2021); Olsson et al. (2022). In Appendix F, we include our initial analysis along this direction.

Figure 4: (a, c) The performance and token similarity within samples with respect to each layer with different random mixed ratio. Pre-trained parameters are mixed with random initial parameters according to certain proportions. (b) Token similarity within samples when replacing the attention with PCA.

## Acknowledgement

We would like to express our sincere gratitude to Ziqing Ma, Qingsong Wen, Mengni Ye, and Tao Yao for their valuable suggestions and proofreading assistance throughout the development of this paper. Their insightful feedback and attention to detail greatly improved the quality and clarity of our work.

## References

* Abdulaal et al. (2021) Abdulaal, A., Liu, Z., and Lancewicki, T. Practical approach to asynchronous multivariate time series anomaly detection and localization. In _Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining_, pp. 2485-2494, 2021.
* Bagnall et al. (2018) Bagnall, A., Dau, H. A., Lines, J., Flynn, M., Large, J., Bostrom, A., Southam, P., and Keogh, E. The uea multivariate time series classification archive, 2018. _arXiv preprint arXiv:1811.00075_, 2018.
* Bao et al. (2021) Bao, H., Wang, W., Dong, L., Liu, Q., Mohammed, O. K., Aggarwal, K., Som, S., and Wei, F. Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. _arXiv preprint arXiv:2111.02358_, 2021.
* Bao et al. (2022) Bao, H., Dong, L., Piao, S., and Wei, F. BEit: BERT pre-training of image transformers. In _International Conference on Learning Representations_, 2022.
* Bose et al. (2017) Bose, J.-H., Flunkert, V., Gasthaus, J., Januschowski, T., Lange, D., Salinas, D., Schelter, S., Seeger, M., and Wang, Y. Probabilistic demand forecasting at scale. _Proceedings of the VLDB Endowment_, 10(12):1694-1705, 2017.
* Box & Jenkins (1968) Box, G. E. and Jenkins, G. M. Some recent advances in forecasting and control. _Journal of the Royal Statistical Society. Series C (Applied Statistics)_, 17(2):91-109, 1968.
* Box & Pierce (1970) Box, G. E. and Pierce, D. A. Distribution of residual autocorrelations in autoregressive-integrated moving average time series models. _Journal of the American statistical Association_, 65(332):1509-1526, 1970.
* Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T. J., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. _ArXiv_, abs/2005.14165, 2020.
* Chalu et al. (2022) Chalu, C., Olivares, K. G., Oreshkin, B. N., Garza, F., Mergenthaler, M., and Dubrawski, A. N-hits: Neural hierarchical interpolation for time series forecasting. _arXiv preprint arXiv:2201.12886_, 2022.
* Chen & Guestrin (2016) Chen, T. and Guestrin, C. Xgboost: A scalable tree boosting system. KDD '16, 2016.
* Chung et al. (2014) Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. _arXiv preprint arXiv:1412.3555_, 2014.
* Courty & Li (1999) Courty, P. and Li, H. Timing of seasonal sales. _The Journal of Business_, 72(4):545-572, 1999.
* Dempster et al. (2020) Dempster, A., Petitjean, F., and Webb, G. I. ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels. _Data Mining and Knowledge Discovery_, 34(5):1454-1495, 2020.
* Devlin et al. (2019) Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), Minneapolis, MN, USA, June 2-7, 2019_, pp. 4171-4186, 2019.
* Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In _9th International Conference on Learning Representations (ICLR), Austria, May 3-7, 2021_, 2021.
* Dosovitskiy et al. (2019)Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021. https://transformer-circuits.pub/2021/framework/index.html.
* Franceschi et al. (2019) Franceschi, J.-Y., Dieuleveut, A., and Jaggi, M. Unsupervised scalable representation learning for multivariate time series. _Advances in neural information processing systems_, 32, 2019.
* Friedman (1962) Friedman, M. The interpolation of time series by related series. _J. Amer. Statist. Assoc_, 1962.
* Gao et al. (2020) Gao, J., Song, X., Wen, Q., Wang, P., Sun, L., and Xu, H. RobustTAD: Robust time series anomaly detection via decomposition and convolutional neural networks. _KDD Workshop on Mining and Learning from Time Series (KDD-MileTS'20)_, 2020.
* Giannou et al. (2023) Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped Transformers as Programmable Computers. _arXiv e-prints_, art. arXiv:2301.13196, January 2023. doi: 10.48550/arXiv.2301.13196.
* Godahewa et al. (2021) Godahewa, R., Bergmeir, C., Webb, G. I., Hyndman, R. J., and Montero-Manso, P. Monash time series forecasting archive. In _Neural Information Processing Systems Track on Datasets and Benchmarks_, 2021.
* Gu et al. (2021) Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. _arXiv preprint arXiv:2111.00396_, 2021.
* Hochreiter & Schmidhuber (1997) Hochreiter, S. and Schmidhuber, J. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* Houlsby et al. (2019) Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In _International Conference on Machine Learning_, pp. 2790-2799. PMLR, 2019.
* Huang et al. (2022) Huang, Z., Shi, X., Zhang, C., Wang, Q., Cheung, K. C., Qin, H., Dai, J., and Li, H. Flowformer: A transformer architecture for optical flow. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII_, pp. 668-685. Springer, 2022.
* Hundman et al. (2018) Hundman, K., Constantinou, V., Laporte, C., Colwell, I., and Soderstrom, T. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pp. 387-395, 2018.
* Hyndman & Athanasopoulos (2021) Hyndman, R. and Athanasopoulos, G. _Forecasting: Principles and Practice_. OTexts, Australia, 3rd edition, 2021.
* Ismail Fawaz et al. (2019) Ismail Fawaz, H., Forestier, G., Weber, J., Idoumghar, L., and Muller, P.-A. Deep learning for time series classification: a review. _Data Mining and Knowledge Discovery_, 33(4):917-963, 2019.
* Kim et al. (2021) Kim, H., Papamakarios, G., and Mnih, A. The lipschitz constant of self-attention. In _International Conference on Machine Learning_, pp. 5562-5571. PMLR, 2021.
* Kim et al. (2022) Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J. Reversible instance normalization for accurate time-series forecasting against distribution shift. In _International Conference on Learning Representations_, 2022.
* Kitaev et al. (2020) Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.
* Lacoste-Julien et al. (2012) Lacoste-Julien, S., Schmidt, M., and Bach, F. A simpler approach to obtaining an o (1/t) convergence rate for the projected stochastic subgradient method. _arXiv preprint arXiv:1212.2002_, 2012.
* Lai et al. (2018) Lai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling long-and short-term temporal patterns with deep neural networks. In _The 41st international ACM SIGIR conference on research & development in information retrieval_, pp. 95-104, 2018.
* Liu et al. (2019)Lim, B., Arik, S. O., Loeff, N., and Pfister, T. Temporal fusion transformers for interpretable multi-horizon time series forecasting. _International Journal of Forecasting_, 2021.
* Liu et al. (2021) Liu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X., and Dustdar, S. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In _International conference on learning representations_, 2021.
* Liu et al. (2022) Liu, Y., Wu, H., Wang, J., and Long, M. Non-stationary transformers: Exploring the stationarity in time series forecasting. In _Advances in Neural Information Processing Systems_, 2022.
* Lu et al. (2022) Lu, K., Grover, A., Abbeel, P., and Mordatch, I. Frozen pretrained transformers as universal computation engines. _Proceedings of the AAAI Conference on Artificial Intelligence_, 36(7):7628-7636, Jun. 2022.
* Makridakis et al. (2018) Makridakis, S., Spiliotis, E., and Assimakopoulos, V. The m4 competition: Results, findings, conclusion and way forward. _International Journal of Forecasting_, 34(4):802-808, 2018.
* Mathur & Tippenhauer (2016) Mathur, A. P. and Tippenhauer, N. O. Swat: A water treatment testbed for research and training on its security. In _2016 international workshop on cyber-physical systems for smart water networks (CySWater)_, pp. 31-36. IEEE, 2016.
* Nie et al. (2022) Nie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. A time series is worth 64 words: Long-term forecasting with transformers. _ArXiv_, abs/2211.14730, 2022.
* Olsson et al. (2022) Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. In-context learning and induction heads. _Transformer Circuits Thread_, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
* OpenAI (2023) OpenAI. Gpt-4 technical report. _ArXiv_, abs/2303.08774, 2023.
* Oreshkin et al. (2019) Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. N-beats: Neural basis expansion analysis for interpretable time series forecasting. _arXiv preprint arXiv:1905.10437_, 2019.
* Oreshkin et al. (2021) Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. Meta-learning framework with applications to zero-shot time-series forecasting. In _Proceedings of the AAAI Conference on Artificial Intelligence_, number 10, pp. 9242-9250, 2021.
* Radford & Narasimhan (2018) Radford, A. and Narasimhan, K. Improving language understanding by generative pre-training. 2018.
* Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019.
* Rao et al. (2021) Rao, Y., Zhao, W., Zhu, Z., Lu, J., and Zhou, J. Global filter networks for image classification. _Advances in Neural Information Processing Systems (NeurIPS)_, 34, 2021.
* Su et al. (2019) Su, Y., Zhao, Y., Niu, C., Liu, R., Sun, W., and Pei, D. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, pp. 2828-2837, 2019.
* Touvron et al. (2021) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H. Training data-efficient image transformers & distillation through attention. In _International Conference on Machine Learning_, pp. 10347-10357. PMLR, 2021.
* Vardi et al. (2021) Vardi, G., Yehudai, G., and Shamir, O. On the optimal memorization power of relu neural networks. _arXiv preprint arXiv:2110.03187_, 2021.
* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser Lukasz, and Polosukhin, I. Attention is all you need. _arXiv preprint arXiv:1706.03762_, 2017.
* Wang et al. (2022) Wang, P., Wang, X., Wang, F., Lin, M., Chang, S., Li, H., and Jin, R. Kvt: k-nn attention for boosting vision transformers. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXIV_, pp. 285-302. Springer, 2022.
* Wang et al. (2020)Wang, T. and Isola, P. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. _ArXiv_, abs/2005.10242, 2020.
* Wen et al. (2022) Wen, Q., Yang, L., Zhou, T., and Sun, L. Robust time series analysis and applications: An industrial perspective. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pp. 4836-4837, 2022.
* Wen et al. (2023) Wen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., and Sun, L. Transformers in time series: A survey. In _International Joint Conference on Artificial Intelligence(IJCAI)_, 2023.
* Wolf et al. (2020) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pp. 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.
* Woo et al. (2022) Woo, G., Liu, C., Sahoo, D., Kumar, A., and Hoi, S. Etsformer: Exponential smoothing transformers for time-series forecasting. _arXiv preprint arXiv:2202.01381_, 2022.
* Wu et al. (2021) Wu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. In _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 101-112, 2021.
* Wu et al. (2023) Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M. Timesnet: Temporal 2d-variation modeling for general time series analysis. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=ju_Uqw3840q.
* Xu et al. (2021) Xu, J., Wu, H., Wang, J., and Long, M. Anomaly transformer: Time series anomaly detection with association discrepancy. _arXiv preprint arXiv:2110.02642_, 2021.
* Yang et al. (2021) Yang, C.-H. H., Tsai, Y.-Y., and Chen, P.-Y. Voice2series: Reprogramming acoustic models for time series classification. In _International Conference on Machine Learning_, pp. 11808-11819, 2021.
* Yun et al. (2020) Yun, C., Chang, Y.-W., Bhojanapalli, S., Rawat, A. S., Reddi, S., and Kumar, S. O (n) connections are expressive enough: Universal approximability of sparse transformers. _Advances in Neural Information Processing Systems_, 33:13783-13794, 2020.
* Zeng et al. (2023) Zeng, A., Chen, M., Zhang, L., and Xu, Q. Are transformers effective for time series forecasting? 2023.
* Zhang et al. (2022) Zhang, T., Zhang, Y., Cao, W., Bian, J., Yi, X., Zheng, S., and Li, J. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures. _arXiv preprint arXiv:2207.01186_, 2022.
* Zhou et al. (2021) Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Proceedings of AAAI_, 2021.
* Zhou et al. (2022) Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _Proc. 39th International Conference on Machine Learning (ICML 2022)_, 2022.

Visualization

In order to clarify the representation ability more clearly, Figure 5 provides showcases of imputation, long-term forecasting and few-shot forecasting. Especially for few-shot learning, GPT2(6) can accurately forecast, while TimesNet and DLinear fail in this task.

## Appendix B Related Works

We have presented a novel general time series analysis model in this paper, and to the best of our knowledge, there has been limited work on similar comprehensive methods for time series analysis. The most closely related field is time series forecasting, where transformer models have gained widespread popularity. Therefore, our focus in this related work will primarily be on introducing the end-to-end time series forecasting method.

Time series forecasting models can be roughly divided into three categories, ranging from the classic ARIMA models to the most recent transformer models. The first generation of well-discussed models can be dated back to auto-regressive family, such as ARIMA Box & Jenkins (1968); Box & Pierce (1970) that follows the Markov process and recursively execute sequential forecasting. However, it is limited to stationary sequences while most time series is non-stationary. Additionally, with the bloom of deep neural networks, recurrent neural networks (RNNs), such as LSTM Hochreiter & Schmidhuber (1997) and GRU Chung et al. (2014), were designed for sequential tasks. Yet the recurrent model is inefficient for training and long-term dependencies are still under resolved.

Recently, transformer models have achieve great progress in NLP Vaswani et al. (2017); Devlin et al. (2019); Radford et al. (2019) and CV Dosovitskiy et al. (2021); Bao et al. (2022) tasks. Also, a large amount of transformer models are proposed to apply to time series forecasting Wen et al. (2023). In the following, we briefly introduce several representative algorithms. Informer Zhou et al. (2021)

Figure 5: Visualization of imputation, long-term forecasting and few-shot forecasting.

proposes a probability sparse attention mechanism to deal with long-term dependencies. Autoformer Wu et al. (2021) introduces a decomposition transformer architecture and replaces the attention module with an Auto-Correlation mechanism. FEDformer Zhou et al. (2022) uses Fourier enhanced structure to improve computational efficiency and achieves linear complexity. Similar to patching in ViT Dosovitskiy et al. (2021), PatchTST Nie et al. (2022) employs segmentation of time series that divide a sequence into patches to increase input length and reduce information redundancy. Besides, a simple MLP-based model DLinear Zeng et al. (2023) outperforms most transformer models and it validates channel-independence works well in time series forecasting. Recently, TimesNet Wu et al. (2023) has treated time series as a 2D signal and utilized a convolution-based inception net backbone to function as a comprehensive time series analysis model. This work is closely related to our tasks in this paper.

## Appendix C Dataset Details

In this section, we separately summarize dataset details long/short-term forecasting and few-shot/zero-shot forecasting.

Datasets of Long-term Forecasting and Few-shot LearningThe details of datasets are shown as follows: 1) ETT datasets Zhou et al. (2021) contain electricity load of various resolutions (ETTh & ETTm) from two electricity stations. 2) Weather contains 21 meteorological indicators of Germany within 1 year; 3) Illness contains the influenza-like illness patients in the United States; 4) Electricity dataset contains the electricity consumption; 5) Traffic dataset contains the occupation rate of freeway system across the State of California. Table 10 summarizes details of feature statistics.

Similar to PatchTST Nie et al. (2022), Exchange is not contained. Zeng et al. (2023) shows that simply repeating the last value in the look-back window can outperform or be comparable to the best results. Also, ILI is not used for few-shot learning for the limited quantity that is hard to follow the definition of few-shot.

Datasets of Short-term Forecasting and Zero-shot LearningThe details of short-term forecasting and zero-shot learning datasets are shown as follows: 1) M4 is a large and diverse dataset that contains time series of various frequencies and fields, including business, financial and economic forecasting; 2) M3 is smaller than M4, but also contains time series from diverse domains and frequencies; 3) TOURISM is the dataset of tourism activities with different frequencies and contains a much higher fraction of erratic series compared with M4; 4) ELECTR represents the electricity usage monitoring of 370 customers over three years. Table 6 summarizes details of the datasets and zero-shot mapping between source and target.

## Appendix D Experimental Details

All the deep learning networks are implemented in PyTorch and trained on NVIDIA V100 32GB GPUs. We use the pre-trained models from Wolf et al. (2020) for experiments. For few-shot learning, an early stopping counter is employed to stop the training process after three epochs if no loss degradation on the valid set is observed. Plus, we convert the multivariate data into univariate data. Specifically, we treat each feature of the sequence as a single time series. This is mainly for memory efficiency after patching of GPT2(6) and previous works, DLinear and PatchTST, have proved the effectiveness of channel-independence.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Dataset & Length & Dimension & Frequency \\ \hline ETTh & 17420 & 7 & 1 hour \\ ETTm & 69680 & 7 & 15 min \\ Weather & 52696 & 22 & 10 min \\ ILI & 966 & 7 & 7 days \\ Electricity & 26304 & 321 & 1 hour \\ Traffic & 17544 & 862 & 1 hour \\ \hline \hline \end{tabular}
\end{table}
Table 10: Dataset details of few-shot learning.

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_FAIL:20]

### Mean and STD for Few-shot Learning

Table 15 lists both mean and STD for GPT2(6), DLinear and PatchTST with 3 runs on 5% ETTh2 and ETTm2. The results show a small variance in performance of GPT2(6) that represents the stability of GPT2(6).

### Comparison with Traditional Methods on Few-shot Learning

Since deep learning methods are more advantageous than traditional methods when applied to large datasets. For few-shot learning, traditional methods should also consider. The results are shown in Table 16 that GPT2(6) also achieves best performance.

### Baselines with Instance Normalization

Instance normalization Kim et al. (2022) is a plug-in for time series for distribution shift. Most baselines, such as Autoformer and FEDformer are not equipped with instance normalization. Thus, for a fair comparison, we add the experiment, as in Table 17, for baselines w/o instance normalization and GPT(6) can also perform superior.

### Detailed Definition and Results of Zero-shot Learning

**Task Definition** Each experiment contains two distinct datasets, source, and target datasets. The source dataset is used to train the model and then forecasts without fine-tuning in the target dataset. The target dataset is split into non-overlapping historical and test sequences. We use the historical sequence as input to the model, and the obtained output is used to calculate errors with the test sequences. Besides meta-learning-based models like N-BEATS, evaluated models' parameters are not allowed any adjustment using the forecasting phase. Also, same as Oreshkin et al. (2021), each data set adopts a specific metric (M4: sMAPE; M3: sMAPE; TOURISM: MAPE; ELECTR: ND)

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c} \hline \hline \multicolumn{2}{c|}{Methods} & \multicolumn{3}{c|}{GPT2(6) 5\%} & \multicolumn{3}{c|}{GPT2(6) 10\%} & \multicolumn{3}{c|}{ETS} & \multicolumn{3}{c|}{ARIMA} & \multicolumn{3}{c}{NaiveDrift} \\ \multicolumn{2}{c|}{Metric} & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{3}{*}{\(\alpha\)} & 96 & 0.376 & 0.421 & 0.331 & 0.374 & 2.954 & 0.742 & 0.481 & 0.443 & 0.764 & 0.561 \\  & 192 & 0.418 & 0.441 & 0.402 & 0.411 & 10.226 & 1.212 & 0.585 & 0.495 & 1.560 & 0.785 \\ \hline \multirow{3}{*}{\(\alpha\)} & 96 & 0.386 & 0.405 & 0.390 & 0.404 & 52.237 & 2.689 & 0.693 & 0.547 & 1.539 & 0.913 \\  & 192 & 0.440 & 0.438 & 0.429 & 0.423 & 186.445 & 4.654 & 0.710 & 0.557 & 2.869 & 1.215 \\ \hline \hline \end{tabular}
\end{table}
Table 16: Comparison with traditional methods.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \hline \multicolumn{2}{c|}{Methods} & \multicolumn{3}{c|}{GPT2(6)} & PatchTST & \multicolumn{3}{c|}{DLinear} & \multicolumn{3}{c|}{Autoformer} & \multicolumn{3}{c|}{Autoformer(Revin)} & FEDformer & FEDformer(Revin) \\ Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{3}{*}{\(\alpha\)} & 96 & 0.199 & 0.280 & 0.206 & 0.288 & 0.236 & 0.326 & 0.232 & 0.322 & 0.224 & 0.300 & 0.229 & 0.320 & 0.223 & 0.298 \\  & 192 & 0.256 & 0.316 & 0.264 & 0.324 & 0.306 & 0.373 & 0.291 & 0.357 & 0.296 & 0.343 & 0.294 & 0.361 & 0.288 & 0.336 \\ \hline \hline \end{tabular}
\end{table}
Table 17: Comparison on 5% data. Autodformer and FEDformer are equiped with instance normalization.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c} \hline \hline \multicolumn{2}{c|}{Methods} & \multicolumn{3}{c|}{GPT2-backbone(6 Layers)} \\ Metric & \multicolumn{3}{c|}{MSE} & MAE \\ \hline \multirow{3}{*}{\(\alpha\)} & 96 & 0.376 & 0.0072 & 0.421 \(\pm\) 0.0054 & \multirow{3}{*}{0.441} & \multirow{3}{*}{0.0014} \\  & 192 & 0.418 & 0.0013 & 0.441 \(\pm\) 0.0014 & & & & & \\  & 336 & 0.408 & 0.0006 & 0.439 \(\pm\) 0.0002 & & & & & \\  & 720 & & & & & & & & \\ \hline \multirow{3}{*}{\(\alpha\)} & 96 & 0.199 & 0.0040 & 0.280 \(\pm\) 0.0042 & & & & & \\  & 192 & 0.256 & 0.0030 & 0.316 \(\pm\) 0.0017 & & & & & \\  & 336 & 0.318 & 0.0046 & 0.353 \(\pm\) 0.0032 & & & & & \\  & 720 & 0.460 & 0.0132 & 0.436 \(\pm\) 0.0066 & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 15: A subset of results showing both Mean and STD on 5% datasets.

Detailed ResultsHere, we list detailed performance of zero-shot learning in Table 18, Table 19 and Table 20. For each dataset, we separately list the performance of models under diverse frequency. Compared to the most recent published method DLinear, GPT2(6) performs superior in most situations. Also, GPT2(6) does not use any information from the test data, but achieves a comparable performance of meta-leaning based N-BEATS.

## Appendix E Proof

In our numerical experiments, we obtain two interesting observations. First, the token similarity within a sample is larger in pretrained LM. We report the layer-wise average token cosine similarity in EITH2 experiment in Figure 7. In particular, Figure 7 (a) shows that in a fine-tuned random initialed GPT2(6) model, the token similarity is around 0.1-0.2 among different layers. When switching to the frozen pre-trained GPT2-FPT model, the token similarity significantly increases in the deep layers and eventually reaches more than 0.9 in the last layer. The EITH2 dataset contains high volatility hourly information related to the electricity transformer temperature. In this situation, higher token similarity implies the high-frequency noise in the data is eased and only low-frequency information will be reserved. In other words, after going through the pretrained GPT2-FPT model, the signal-noise ratio is enhanced. We use the following theorem to characterize this behavior.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline  & Yearly & Quarterly & Monthly & Others & Average \\  & (645) & (756) & (1428) & (174) & (3003) \\ \hline N-BEATS-M4 & 15.07 & 9.07 & 13.19 & 4.29 & 12.38 \\ N-BEATS-FR & 16.43 & 9.05 & 13.30 & 4.51 & 12.61 \\ \hline DLinear-M4 & 17.43 & 9.74 & 15.65 & 6.81 & 14.03 \\ TimesNet-M4 & 18.75 & 12.26 & 14.01 & 6.88 & 14.17 \\ PatchTST-M4 & 15.99 & 9.62 & 14.71 & 9.44 & 13.39 \\ ETSformer-M4 & 20.56 & 11.65 & 16.97 & 10.57 & 16.03 \\ LightTS-M4 & 15.63 & 9.40 & 24.60 & 8.28 & 17.90 \\ Stationary-M4 & 17.05 & 12.56 & 16.82 & 8.13 & 15.29 \\ FEDformer-M4 & 16.00 & 9.48 & 15.12 & 8.94 & 13.53 \\ Autoformer-M4 & 16.18 & 13.92 & 16.91 & 14.68 & 15.87 \\ Informer-M4 & 19.70 & 13.00 & 15.91 & 13.03 & 15.82 \\ Reformer-M4 & 16.03 & 9.76 & 14.80 & 7.53 & 13.37 \\ \hline GPT2(6)-M4 & 16.42 & 10.13 & 14.10 & 4.81 & 13.06 \\ \hline \hline \end{tabular}
\end{table}
Table 19: Zero-shot performance on M3 (sMAPE).

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline  & Yearly & Quarterly & Monthly & Others & Average \\  & (23k) & (24k) & (48k) & (5k) & (100k) \\ \hline N-BEATS-FR & 13.267 & 9.596 & 12.676 & 4.696 & 11.675 \\ \hline DLinear-M3 & 14.193 & 18.856 & 14.765 & 9.194 & 15.337 \\ TimesNet-M3 & 15.655 & 11.877 & 16.165 & 6.863 & 14.553 \\ PatchTST-M3 & 13.966 & 10.929 & 14.664 & 7.087 & 13.228 \\ ETSformer-M3 & 27.846 & 36.134 & 25.114 & 12.338 & 27.748 \\ LightTS-M3 & 13.787 & 11.289 & 15.181 & 9.117 & 13.623 \\ Stationary-M3 & 14.988 & 11.686 & 16.098 & 6.977 & 14.327 \\ FEDformer-M3 & 13.887 & 11.513 & 18.154 & 7.529 & 15.047 \\ Autoformer-M3 & 14.552 & 17.341 & 25.063 & 9.666 & 20.022 \\ Informer-M3 & 18.542 & 16.907 & 23.454 & 7.348 & 19.047 \\ Reformer-M3 & 15.652 & 11.051 & 15.604 & 7.001 & 14.092 \\ \hline GPT(6)-M3 & 13.740 & 10.787 & 14.630 & 7.081 & 13.125 \\ \hline \hline \end{tabular}
\end{table}
Table 18: Zero-shot performance on M4 (sMAPE).

### Theorem e.1

**Theorem E.1** (informal).: _We consider the self-attention for \(l\)-th query token. Let's assume the input token \(\bm{x}_{i}\) are bounded with mean \(\bm{\mu}\) for \(i=1,2,...,n\). Under mild conditions, with high probability, the output value token \(\bm{V}_{l}\) converges to \(\bm{\mu}W_{v}\) on the order of \(\mathcal{O}(n^{-1/2})\), where \(W_{v}\) is the parameter matrix to compute the value token._

The Theorem E.1 describes the self-attention structure can efficiently make output value token \(\bm{V}_{l}\) converge its mean value \(\bm{\mu}W_{v}\). In the time series forecasting task, each token represents several adjacent points in a time series. When the time series has some periodical or translation invariant structures, by comparing a given token with other tokens, one could have a higher chance to figure out those invariant structures. This phenomenon is especially important in few-shot forecasting tasks. Without enough token noise distillation ability, the model will more likely tend to overfit due to insufficient training data.

We denote \(x_{i}\) as \(i\)-th element of vector \(\bm{x}\), \(\bm{W}_{ij}\) as the element at \(i\)-th row and \(j\)-th column of matrix \(\bm{W}\), and \(\bm{W}_{j}\) as the \(j\)-th row of matrix \(\bm{W}\). Moreover, we denote \(\bm{x}_{i}\) as the \(i\)-th patch (token) of the inputs with \(\bm{x}_{i}=\bm{X}_{i}\).

Before given the formal statement of the Theorem E.1, we first show the assumptions.

1. The token \(\bm{x}_{i}\) is the sub-gaussian random vector with mean \(\bm{\mu}_{i}\) and variance \((\sigma^{2}/d)I\) for \(i=1,2,...,n\).
2. \(\bm{\mu}\) follows a discrete distribution with finite values \(\bm{\mu}\in\mathcal{V}\). Moreover, there exist \(0<\nu_{1},0<\nu_{2}<\nu_{4}\) such that a) \(\|\bm{\mu}_{i}\|=\nu_{1}\), and b) \(\bm{\mu}_{i}\bm{W}_{\bm{Q}}\bm{W}_{\bm{K}}^{T}\bm{\mu}_{i}\in[\nu_{2},\nu_{4}]\) for all \(i\) and \(|\bm{\mu}_{i}\bm{W}_{\bm{Q}}\bm{W}_{\bm{K}}^{T}\bm{\mu}_{j}^{\top}|\leq\nu_{2}\) for all \(\bm{\mu}_{i}\neq\bm{\mu}_{j}\in\mathcal{V}\).
3. \(\bm{W}_{V}\) and \(\bm{W}_{\bm{Q}}\bm{W}_{\bm{K}}^{\top}\) are element-wise bounded with \(\nu_{5}\) and \(\nu_{6}\) respectively, that is, \(|\bm{W}_{V}^{(ij)}|\leq\nu_{5}\) and \(|(\bm{W}_{\bm{Q}}\bm{W}_{\bm{K}}^{\top})^{(ij)}|\leq\nu_{6}\), for all \(i,j\) from 1 to \(d\).

In the above assumptions, we ensure that for a given query patch, the difference between the clustering center and noises are large enough to be distinguished.

**Theorem E.2** (formal statement of Theorem E.1).: _Let patch \(\bm{x}_{i}\) be \(\sigma^{2}\)-subgaussian random variable with mean \(\bm{\mu}_{i}\) and all \(n\) patches follow the same clustering center of query \(l\). Per Assumptions aforementioned, when \(\sqrt{d}\geq 3(\psi(\delta,d)+\nu_{2}+\nu_{4})\), then with probability \(1-5\delta\), we have_

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline  & Yearly & Quarterly & Monthly & Average \\  & (518) & (427) & (366) & (1311) \\ \hline N-BEATS-M4 & 23.57 & 14.66 & 19.32 & 18.82 \\ N-BEATS-FR & 23.43 & 14.45 & 20.47 & 19.46 \\ \hline DLinear-M4 & 39.59 & 18.30 & 24.76 & 28.51 \\ TimesNet-M4 & 35.59 & 19.22 & 30.54 & 28.84 \\ PatchTST-M4 & 33.23 & 19.27 & 27.57 & 27.10 \\ ETSformer-M4 & 391.60 & 35.56 & 50.47 & 180.40 \\ LightTS-M4 & 138.22 & 16.28 & 25.34 & 66.99 \\ Stationary-M4 & 35.42 & 35.15 & 65.58 & 43.75 \\ FEDformer-M4 & 43.41 & 19.88 & 28.39 & 31.55 \\ Autoformer-M4 & 51.19 & 34.95 & 31.47 & 40.39 \\ Informer-M4 & 41.16 & 30.98 & 33.92 & 35.82 \\ Reformer-M4 & 33.86 & 16.85 & 23.71 & 25.48 \\ \hline GPT2(6)-M4 & 27.17 & 16.21 & 21.92 & 22.14 \\ \hline \hline \end{tabular}
\end{table}
Table 20: Zero-shot performance on Tourism (MAPE).

\[\left\|\frac{\sum_{i=1}^{n}\exp\left(\frac{1}{\sqrt{d}}\bm{x}_{i} \bm{W}_{\bm{Q}}\bm{W}_{\bm{K}}^{\top}\bm{x}_{i}\right)\bm{x}_{i}\bm{W}_{V}}{\sum _{j=1}^{n}\exp\left(\frac{1}{\sqrt{d}}\bm{x}_{l}\bm{W}_{\bm{Q}}\bm{W}_{\bm{K}}^{ \top}\bm{x}_{j}\right)}-\bm{\mu}_{l}\bm{W}_{V}\right\|_{\infty}\leq 4\exp \left(\frac{\psi(\delta,d)}{\sqrt{d}}\right)\sigma\nu_{5}\sqrt{\frac{2}{dn}\log \left(\frac{2d}{\delta}\right)}\] \[+7\left[\exp\left(\frac{\nu_{2}-\nu_{4}+\psi(\delta,d)}{\sqrt{d} }\right)-1\right]\|\bm{\mu}_{l}\bm{W}_{V}\|_{\infty},\]

_where \(\psi(\delta,d)=2\sigma\nu_{1}\nu_{6}\sqrt{2\log\left(\frac{1}{\delta}\right)}+ 2\sigma^{2}\nu_{6}\log\left(\frac{d}{\delta}\right)\)._

Proof.: See the proof of Lemma 2 in Wang et al. (2022) with \(k_{1}=k=n\). 

### Theorem e.4

We first give the formal statement of Theorem E.4.

**Theorem E.3** (formal statement of Theorem E.4).: _Let \(\bm{g}_{i}\in\mathds{R}^{d}\) and \(\bm{y}_{i}\in\mathds{R}^{T}\) be the feature map vector and forecasting targets for the sample \(i=1,2,...,N\) respectively, and we assume \(\frac{1}{N}\sum_{i=1}^{N}\bm{g}_{i}\bm{g}_{i}^{\top}\succeq\sigma I\) for some \(\sigma>0\). We want to learn a matrix \(\bm{W}\in\mathds{R}^{d\times T}\) from the following optimization problem:_

\[\bm{W}=\arg\min\frac{1}{2N}\sum_{i=1}^{N}\|\bm{W}\bm{g}_{i}-\bm{y}_{i}\|_{2}^{2}.\] (1)

_If we apply stochastic gradient descent with diminishing step sizes \(\eta_{t}=\frac{1}{\sigma t}\) at step \(t\), we will need \(t=\tilde{\mathcal{O}}(\epsilon^{-1}\sigma^{-1})\) steps to reach_

\[\frac{1}{t}\sum_{j=1}^{t}\left(\frac{1}{2N}\sum_{i=1}^{N}\|\bm{W}_{j}\bm{g}_{ i}-\bm{y}_{i}\|_{2}^{2}\right)-\frac{1}{2N}\sum_{i=1}^{N}\|\bm{W}^{*}\bm{g}_{i}- \bm{y}_{i}\|_{2}^{2}\leq\epsilon,\] (2)

_where \(\bm{W}^{*}\) is the optimal solution and \(\bm{W}_{j}\) is the \(j\) step's solution and \(\tilde{\mathcal{O}}\) we suppress the logarithmic dependence._

Proof.: As we assume \(\frac{1}{N}\sum_{i=1}^{T}\bm{g}_{i}\bm{g}_{i}^{\top}\succeq\sigma I\), the hessian of optimization problem in (1) is also positive definite, which is equivalent to the optimization problem in (1) is strongly convex with parameter proportional to \(\sigma\). Then via standard stochastic gradient decent analysis (e.g., section 3.1 in Lacoste-Julien et al. (2012)), we obtain:

\[\frac{1}{t}\sum_{j=1}^{t}\left(\frac{1}{2N}\sum_{i=1}^{N}\|\bm{W}_{j}\bm{g}_{i} -\bm{y}_{i}\|_{2}^{2}\right)-\frac{1}{2N}\sum_{i=1}^{N}\|\bm{W}^{*}\bm{g}_{i}- \bm{y}_{i}\|_{2}^{2}\leq\mathcal{O}\left(\frac{\log t}{\sigma t}\right)=\tilde {O}(\sigma^{-1}t^{-1}).\] (3)

Therefore, to reach \(\epsilon\) optimization gap, we just need to set \(t=\tilde{\mathcal{O}}(\sigma^{-1}\epsilon^{-1})\). 

The second observation is that for the pretrained GPT2-FPT model, the last transformer layer's outputs, i.e., feature maps, are spread widely throughout the feature space. We report the t-SNE visualization of the feature maps for GPT2-FPT and an end-to-end model PatchTST in Figure 8. In Figure 8 (a) and (b), we color the samples chunked from the one single time series into the same color and the same configuration of the T-SNE is applied. One may observe that the feature maps of GPT2-FPT has less concentration compared to PatchTST. It implies the GPT2-FPT's feature maps corresponding to different samples are more distinctive which eventually facilitates the learning ability of the last MLP layer. Researchers Wang & Isola (2020) have found that contrastive learning-based representation learning may result in a uniform distribution of training data, and such behavior plays an important role in its good downstream task performance. We use the following theorem to justify it.

**Theorem E.4** (informal).: _Let \(\bm{g}_{i}\) and \(\bm{y}_{i}\) be the feature map vector and forecasting targets for the sample \(i=1,2,...,N\) respectively, and we assume \(\frac{1}{N}\sum_{i=1}^{N}\bm{g}_{i}\bm{g}_{i}^{\top}\succeq\sigma I\) for some \(\sigma>0\). Under mild conditions, if we train an MLP layer that maps feature maps to forecasting targets via the stochastic gradient descent, the total step to reach some optimization tolerance is on the order of \(\mathcal{O}(\sigma^{-1})\)._The Theorem E.4 considers the covariate matrix of feature maps being positive definite that indicates the set of all feature maps \(\{\bm{g}_{i}\}\) spans the whole feature spaces, and the higher spread level gives a larger \(\sigma\). In this case, if we only want to learn an MLP layer, the problem reduces to a well-conditioned least-squared regression problem. Then the fast convergence rate is achieved.

Efficiently learning the last MLP layer plays a very important role in time series forecasting and can substantially impact the prediction performance. In Zeng et al. (2023), the authors show that learning a single MLP layer can also bring very promising performance. In few-shot forecasting, the pre-trained GPT2 model may still preserve highly diverse feature maps than end-to-end type models and eventually leads to fast learning speed on the last MLP layer.

Another possible benefit of wide spared feature maps is enhancing the model memorization ability when using a multi-layer decoder structure. In the literature on network memorization ability (e.g., Vardi et al. (2021); Yun et al. (2020)), the deep learning model tends to have better memorization ability when feature maps are well separated. In forecasting tasks, capturing extreme or rare behavior is very important. The pretrained GPT gains more capacity in the decoder to correctly forecast uncommon time series.

## Appendix F N-gram Explanation for Universality

Why does the proposed pretrained-frozen-model work so effectively? We have achieved state-of-the-art performance in time series analysis using a language model that is mostly trained on natural language data. The answer lies in the universality of the frozen structure, which includes attention layers and Feed Forward layers. We can represent images and time series forecasting tasks as an n-gram estimation problem, akin to text analysis, by employing a patching approach. This method treats subsequences of time series or image patches as individual tokens. Central to sequential prediction is the \(n\)-order Markov process, and a simple way to capture the \(n\)-order Markov process is \(n\)-gram language model. To predict next token \(w_{0}\), we need to compute \(p(w_{0}|w_{1},\ldots,w_{n-1})\), which can be further computed as \(p(w_{0}w_{1}\ldots w_{n-1})/p(w_{1}\ldots w_{n-1})\). Hence, the core of \(n\)-gram

Figure 6: The performance and token similarity within samples with respect to each layer with different random replace ratios. Pretrained parameters are replaced by random initial parameters according to certain proportions.

Figure 7: The token similarity within samples with respect to each layer. (a) GPT2-noPretrain-model; (b) GPT2-Pretrained-model; (c) Pretrained attention is replaced by PCA.

language model is to estimate the probability of observing a sequence of \(n\) tokens. When \(n\) is large, most of \(n\) token sequences will not be observed from data, leading to the sparse data problem, a common challenge faced by \(n\)-gram language model. As a result, a large body of research in \(n\)-gram language model is focused on how to effectively estimate probability of having \(n\)-token sequences even when they are NOT observed from data. We hypothesize that the transformer model pretrained by GPT-2 essentially allows us to estimate \(p(w_{0}w_{1}\dots w_{n-1})\) from observations of significantly shorter token sequences. In this section, we will show that the function of estimating probabilities of longer sequences from observation of shorter sequences is universal and is independent from domain as long as data exhibit a skew distribution (e.g., follows a power law). We note that our work is closely related to the discussion presented in Elhage et al. (2021); Olsson et al. (2022), where the authors also connect the function of transformer to compute of \(n\)-grams. We however note that our key result is to show the universality in computing probability of longer sequences from observations of shorter sequences, which can't be found in any existing studies. Although the discussion is restricted to discrete tokens, it should be generalized to continuous signals as we can always quantize continuous signals into a finite number of discrete tokens, similar to what BEiT Bao et al. (2022) did.

To gain a better understanding, let's start by examining a "zero-layer" Transformer model. This model operates by taking a token, embedding it, and transforming it back to produce logits that predict the subsequent token. Because it cannot transfer information from other tokens, it relies solely on the current token to predict the next one. Consequently, the optimal behavior of this model is to closely resemble the **bigram** log-likelihood.

Then we move on to the so-called "attention-only" transformer, which doesn't have MLP layers. As discussed in a recent work Elhage et al. (2021), one-layer attention-only Transformers can be comprehended as a combination of a **bigram** model and multiple **"skip-trigram"** models (impacting the probabilities of sequences "A... BC"). This can be intuitively understood as each attention head having the ability to selectively attend from the current token ("B") to a previous token ("A") and transfer relevant information to fine-tune the probability of potential subsequent tokens ("C"). Olsson et al. (2022) further discusses a multi-layer transformer can do more complex n-gram estimation using an induction heads mechanism. To be more precise, induction heads employ a straightforward principle: the '[A][B]... [A] \(\rightarrow\) [B]' rule, which elevates the likelihood of generating the subsequent token 'B' given the current token 'A' if there is a fuzzy match of the AB bigram in the historical context. This rule seems to largely decouple A and B, which means they do not memorize a fixed table of n-gram statistics. The rule [A][B]... [A] \(\rightarrow\) [B] applies regardless of what A and B are, which can abstract to new patterns.

Building upon these discussions, we are now prepared to substantiate the following argument: **For sequential data following a power law, there is a potentially universal solution to the final estimation of n-gram probabilities**. That's the reason behind the universality of pretrained LM's performance in cross-domain tasks. For simplicity, we assume that \(n\) is so large that we are unable to observe any occurrence of \(n\)-gram from data, and we only observe the occurrence of \(n^{\prime}\)-grams with \(n^{\prime}<n\). We denote by \(s_{i}^{n}\) the \(i\)th unique \(n\)-gram, and by the notation \(s_{j}^{n^{\prime}}\in s_{i}^{n}\) if \(n^{\prime}\)-gram \(s_{j}^{n^{\prime}}\) appears

Figure 8: The t-SNE visualization of sample feature maps for (a) GPT-backbone, (b) end-to-end-PatchTST-model. (c) The token similarity within samples within different continuous sequence lengths.

in \(s_{i}^{n^{\prime}}\), the \(i\)th \(n\)-gram. Let \(m_{n}\) be the number of unique \(n\)-grams. According to the maximum entropy model, our estimation of n-gram probabilities can be cast into the following optimization problem:

\[\begin{array}{ll}\min\ \sum_{i=1}^{m_{n}}p(s_{i}^{n})\log p(s_{i}^{n})&\text{s. t.}\sum_{i:s_{j}^{n^{\prime}}\in s_{i}^{n}}p(s_{i}^{n})=\widehat{p}(s_{j}^{n^{ \prime}})&\text{where }\widehat{p}(s_{j}^{n^{\prime}})\text{ represents the probability of observing pattern }s_{j}^{n^{\prime}}\text{ from the data and }j\in[m_{n^{\prime}}],n^{\prime}\in[n-1].\end{array}\]

For each constraint for \(\widehat{p}(s_{j}^{n^{\prime}})\), we introduce a Lagrangian dual variable \(\lambda_{j}^{n^{\prime}}\), and rewrite the optimization problem as follows:

\[\begin{array}{ll}\min_{\lambda}\ \log\left(\sum_{i=1}^{m_{n}}\exp\left(\sum_{(n^{ \prime},j):s_{j}^{n^{\prime}}\in s_{i}^{n}}\lambda_{j}^{n^{\prime}}\right) \right)-\sum_{n^{\prime}=1}^{n-1}\sum_{j=1}^{m_{n^{\prime}}}\lambda_{j}^{n^{ \prime}}\widehat{p}(s_{j}^{n^{\prime}}),\end{array}\]

where n-gram probability \(p(s_{j}^{n})\) is given as \(p(s_{j}^{n})=\frac{1}{Z(\lambda)}\exp\left(\sum_{(n^{\prime},j):s_{j}^{n^{ \prime}}\in s_{i}^{n}}\lambda_{j}^{n^{\prime}}\right)\) and \(Z(\lambda)=\sum_{i=1}^{m_{n}}\exp(\sum_{(n^{\prime},j):s_{j}^{n^{\prime}}\in s _{i}^{n}}\lambda_{j}^{n^{\prime}})\)

In the case that all n-grams follow a power law, for each \(n^{\prime}\in[n-1]\), we divide \(n^{\prime}\)-gram into two groups: the group \(\mathcal{V}_{n^{\prime}}\) includes the high frequency \(n^{\prime}\)-gram and the group \(\mathcal{U}_{n^{\prime}}\) including the low frequency of \(n^{\prime}\)-gram. For simplicity, we assume that the probability for all the high frequency \(n^{\prime}\)-grams are roughly \(\alpha_{n^{\prime}}\in[0,1]\) and the probability for all the low frequency \(n^{\prime}\)-grams are roughly \(\beta_{n^{\prime}}\in[0,1]\). By assuming that all the patterns in \(\mathcal{V}_{n^{\prime}}\) and \(\mathcal{U}_{n^{\prime}}\) share similar appearance frequency, we simplify the optimization problem by only introducing two dual variables for each \(n^{\prime}\)-gram, i.e. \(\lambda_{a}^{n^{\prime}}\) for high-frequency patterns and \(\lambda_{b}^{n^{\prime}}\) for low-frequency patterns as follow Using these notations, we have the optimization problem simplified as

\[\begin{array}{ll}\min_{\lambda}&\log(\sum_{i=1}^{m_{n}}\exp(\sum_{n^{\prime }=1}^{n-1}\sum_{j:s_{j}^{n^{\prime}}\in s_{i}^{n^{\prime}}}\lambda_{a}^{n^{ \prime}}I(s_{j}^{n^{\prime}}\in\mathcal{V}_{n^{\prime}})\\ &+\lambda_{b}^{n^{\prime}}I(s_{j}^{n^{\prime}}\in\mathcal{U}_{n^{\prime}}))) -\sum_{n^{\prime}=1}^{n-1}\left(\lambda_{a}^{n^{\prime}}g_{n^{\prime}}+ \lambda_{b}^{n^{\prime}}h_{n^{\prime}}\right),\end{array}\]

where \(g_{n^{\prime}}=\sum_{s_{j}^{n^{\prime}}\in\mathcal{V}_{n^{\prime}}}\widehat{p }(s_{j}^{n^{\prime}})\) and \(h_{n^{\prime}}=\sum_{s_{j}^{n^{\prime}}\in\mathcal{U}_{n^{\prime}}}\widehat{p }(s_{j}^{n^{\prime}})\).

Furthermore, let \(q_{a}^{n^{\prime}}\) be the probability to observe a high frequency \(n^{\prime}\)-gram appearing in any \(n\)-gram, and \(q_{b}^{n^{\prime}}\) be the probability to observe a low frequency \(n^{\prime}\)-gram appearing in any \(n\)-gram, we have

\[\begin{array}{ll}\sum_{i=1}^{m_{n}}\exp(\sum_{n^{\prime}=1}^{n-1}\sum_{j:s_ {j}^{n^{\prime}}\in s_{i}^{n}}\lambda_{a}^{n^{\prime}}I(s_{j}^{n^{\prime}}\in \mathcal{V}_{n^{\prime}})+\lambda_{b}^{n^{\prime}}I(s_{j}^{n^{\prime}}\in \mathcal{U}_{n^{\prime}}))\\ =m_{n}\prod_{n^{\prime}=1}^{n-1}(1+q_{a}^{n^{\prime}}\exp(\lambda_{a}^{n^{ \prime}}))(1+q_{b}^{n^{\prime}}\exp(\lambda_{b}^{n^{\prime}}))+\mathcal{O} \left(\sqrt{m_{n}}\right).\end{array}\]

By skipping the term \(\mathcal{O}(\sqrt{m_{n}})\), we further simplify the optimization problem as

\[\begin{array}{ll}\min_{\lambda}&\sum_{n^{\prime}=1}^{n-1}\log\left(1+q_{a}^{ n^{\prime}}\exp(\lambda_{a}^{n^{\prime}})\right)-+&\sum_{n^{\prime}=1}^{n-1}\log \left(1+q_{b}^{n^{\prime}}\exp(\lambda_{b}^{n^{\prime}})-\lambda_{b}^{n^{ \prime}}h_{n^{\prime}},\right.\end{array}\]

which is equivalent to

\[\begin{array}{ll}\lambda_{n^{\prime}}^{a}&=\min_{\lambda}\log\left(1+q_{a}^{n^ {\prime}}\exp(\lambda)\right)-\lambda g_{n}^{\prime}&\\ \lambda_{a^{\prime}}^{b}&=\min_{\lambda}\log\left(1+q_{b}^{n^{\prime}}\exp( \lambda)\right)-\lambda h_{n}^{\prime}&\text{As illustrated by the above analysis, dual variables}\\ \lambda_{a}^{n^{\prime}}&\text{and }\lambda_{b}^{n^{\prime}}\text{ will depend on statistics }q_{a}^{n^{\prime}}\text{, }q_{b}^{n^{\prime}}\text{, }g_{n^{\prime}}\text{ and }h_{n^{\prime}}\text{. They are independent from the detailed statistics }\widehat{p}(s_{j}^{n^{\prime}})\text{ and how each }n^{\prime}\text{-gram appears in different }n\text{-gram. Thus, this simple analysis does indicate, to some degree, that the solution obtained from the maximum entropy model can be universal, as long as \(n\)-grams follow skewed distributions like power law.

We informally demonstrate that transformer models utilize attention mechanisms to perform a sophisticated form of n-gram estimation, and the generation rule for such n-gram distributions could be universal. This is how universality is achieved in our proposed cross-domain knowledge transfer. However, we currently lack a concrete metric to evaluate the performance of knowledge transfer between different domains, which requires further investigation. Nonetheless, in our experimental study, we demonstrate that a transformer model (beit) Bao et al. (2022) trained on images can perform well on cross-domain time series forecasting tasks.

Connection between self-attention and Principle component analysis

**Understand the Gradient Structure of Self-Attention**

Let \(X=(x_{1},\dots,x_{N})^{\top}\in\mathbb{R}^{N\times D}\) be the input pattern, and let \(f(X)=(f_{1}(X),\dots,f_{N}(x))^{\top}:\mathbb{R}^{N\times D}\mapsto\mathbb{R}^{N \times D}\) be the function for self-attention, i.e.

\[f_{i}(X)=\text{softmax}(XAX^{\top})X\]

where \(A=W_{Q}W_{K}^{\top}\in\mathbb{R}^{D\times D}\). Let the Jacobian \(J=\left[\frac{\partial f_{i}(X)}{\partial x_{j}}\right]_{i,j=1}^{N}\) represent the gradient \(f(X)\) with respect to input pattern. The lemma below shows an important structure of \(J\).

_Lemma G.1_.: \(|J|_{2}\leq|A|_{2}\sum_{i=1}^{N}\left(P_{i,i}+\frac{1}{2}\right)\left|x_{i}- \sum_{j=1}^{N}P_{i,j}x_{j}\right|^{2}+\Delta\)__

where \(\Delta=|A|_{2}\sum_{i\neq j}^{N}P_{i,j}\left|x_{j}-\sum_{k=1}^{N}P_{i,k}x_{k} \right|^{2}+\frac{|A|_{2}}{2}\sum_{j=1}^{N}|x_{i}|^{2}\) and \(P_{i,j}=\frac{\exp(x_{i}^{\top}Ax_{j})}{\sum_{k=1}^{N}\exp(x_{i}^{\top}Ax_{k})}\)__

Proof.: According to the analysis from the work, we have the gradient \(J_{i,j}=\frac{\partial f_{i}(X)}{x_{j}}\) is given by

\(J_{i,j}=P_{i,j}I+X^{\top}Q^{i}\left(XA\delta_{i,j}+E_{j,i}XA^{\top}\right)\) where \(Q^{i}=\text{diag}(P_{i,:})-P_{i,:}P_{i,:}^{\top}\). Here \(P_{i,:}\in\mathbb{R}_{+}^{N}\) represents the \(i\)-th row of matrix \(P\). We thus have

\[\begin{array}{lcl}|J|_{2}&\leq&\sum_{j,i=1}^{N}|J_{i,j}|_{2}\\ &\leq&\sum_{j,i=1}^{N}P_{i,j}+\sum_{j=1}^{N}|X^{\top}Q^{i}X|_{2}|A_{2}+\sum_{j =1}^{N}|X^{\top}Q^{i}E_{j,i}X|_{2}|A_{2}\\ &\leq&N+|A|_{2}\sum_{i=1}^{N}\left(\sum_{j=1}^{N}P_{i,j}|x_{j}|^{2}-\left|\sum _{j=1}^{N}P_{i,j}x_{j}\right|^{2}\right)+|A|_{2}\sum_{j=1}^{N}|X^{\top}Q^{i}e_ {j}x_{i}^{\top}|\\ &\leq&N+|A|_{2}\sum_{i=1}^{N}\sum_{j=1}^{N}P_{i,j}\left|x_{j}-\sum_{k=1}^{N}P _{i,k}x_{k}\right|^{2}+|A|_{2}\sum_{j=1}^{N}P_{i,j}\left|x_{j}^{\top}\left(x_{ j}-X^{\top}P_{i,:}\right)\right|\\ &\leq&|A|_{2}\sum_{i=1}^{N}\left(P_{i,j}+\frac{1}{2}\right)\left|x_{i}-X^{ \top}P_{i,:}\right|^{2}+\underbrace{N+|A|_{2}\sum_{i\neq j}^{N}P_{i,j}\left|x_{ j}-X^{\top}P_{i,:}\right|^{2}+\underbrace{|A|_{2}\sum_{j=1}^{N}\left|x_{i} \right|^{2}}}_{\Delta}\end{array}\]

As indicated by Lemma 1, one of the key components in the upper bound of Jacobian is \(|x_{i}-\sum_{j=1}^{N}P_{i,j}x_{j}|^{2}\). Thus, through the optimization, we like to reduce the size of the gradient and therefore may prefer to reduce the quantity to \(\sum_{i=1}^{N}|x_{i}-\sum_{j=1}^{N}P_{i,j}x_{j}|^{2}\). Hence, it will be interesting to understand the choice of \(W^{Q}\) and \(W^{K}\) that leads to the minimization of \(\sum_{i=1}^{N}|x_{i}-\sum_{j=1}^{N}P_{i,j}x_{j}|^{2}\), i.e. the following optimization problem \(\min\limits_{|A|_{F}\leq\rho}\sum_{i=1}^{N}\left|x_{i}-\sum_{j=1}^{N}P_{i,j}x_{ j}\right|^{2}\) where \(\rho\) is introduced to control the size of \(A\).

**Connection between Self-Attention and Principal Component Analysis**

Let consider the optimization problem in (G) when \(\rho\) is small, we can approximate \(P_{i,j}\) as \(P_{i,j}\approx\frac{1}{N}+\frac{1}{N}x_{i}^{\top}Ax_{j}\) Define \(\bar{x}=X^{\top}\mathbf{1}/N\). We have \(\sum_{i=1}^{N}|x_{i}-X^{\top}P_{i,:}|^{2}=\sum_{i=1}^{N}\left|x_{i}-\bar{x}-X ^{\top}XAx_{i}\right|^{2}\) By assuming that all the input patterns are zero centralized, we have \(\bar{x}=0\) and \(\sum_{i=1}^{N}|x_{i}-X^{\top}XAx_{i}|^{2}=\text{tr}\left((I-X^{\top}XA)^{2}X^{ \top}X\right)\) The theorem below shows that \(A\) minimizing the objective \(\sum_{i=1}^{N}|x_{i}-X^{\top}XAx_{i}|^{2}\) contains the largest \(m\) eigenvectors of \(X^{\top}X\) where \(m\) is the rank of \(A\).

_Theorem 2_.: Let \(W_{Q}\) and \(W_{K}\) be matrices of size \(D\times m\). Let \(\lambda_{1}\geq\lambda_{2}\geq...\geq\lambda_{D}\) be the eigenvalues of \(X^{\top}X\) ranked in descending order, and let \(v_{i}\in\mathbb{R}^{D},i=1,\dots,D\) be the corresponding eigenvectors. The optimal solution \(A^{*}\) that minimizes \(\sum_{i=1}^{N}|x_{i}-X^{\top}XAx_{i}|^{2}\) is given by \(A=\sum_{i=1}^{m}\frac{1}{\lambda_{i}}v_{i}v_{i}^{\top}\)

Proof.: Since \(W_{Q},W_{K}\in\mathbb{R}^{D\times m}\) where \(m<D\), we know that \(A\) is a matrix of rank \(m\). Hence, we know \(\min\limits_{A}\sum_{i=1}^{N}|x_{i}-X^{\top}XAx_{i}|^{2}\geq\sum_{k=m+1}^{N} \lambda_{k}\) We also know that by choosing \(A\) as \(A=\sum_{i=1}^{m}\frac{1}{\lambda_{i}}v_{i}v_{i}^{\top}\) we have \[\sum_{i=1}^{N}|x_{i}-X^{\top}XAx_{i}|^{2}=\text{tr}\left((I-\sum_{i=1}^{m}v_{i}v_{i }^{\top})^{2}\,X^{\top}X\right)=\sum_{k=m+1}^{D}\lambda_{k}\]

Hence, the solution \(A\) for minimizing \(\sum_{i=1}^{N}|x_{i}-X^{\top}XAx_{i}|^{2}\) is essential a weighted combination of top eigenvectors of \(X^{\top}X\). Since a small gradient will prefer a small quantity of \(\sum_{i=1}^{N}|x_{i}-X^{\top}XAx_{i}|^{2}\), by minimizing through the self-attention layer, we essentially choose weight matrix \(W_{Q}\) and \(W_{K}\) to be aligned with the principal directions of \(X^{\top}X\). 

## Appendix H Experiment Analysis and Other Key Results

### Experiment analysis of GPT2-FPT model

In this section, we conduct experiments to analyze whether the self-attention frozen pre-trained model improves performance compared with overall fine-tuning and random initialization. Firstly, we compare GPT2(6) FPT with the same model without freezing (No Freeze) and random initial model (No Pre-train). For the end-to-end paradigm No Pre-train GPT2-backbone (6 Layers), we directly train all parameters of the model. We summarize the results in Table 21 and Table 22. Then we analyze the performance of various layers to clarify our selection of GPT2(6) FPT.

**Fine-tune More Parameters** Compared with fine-tuning all parameters, self-attention frozen pre-trained model GPT2(6) FPT achieves better performance on most datasets and yields an overall **12.7%** relative MSE reduction on 5% data and **11.5%** relative MSE reduction on 10% data. It verifies that frozen pre-trained attention layers are effective for time series forecasting.

**Parameters Initialization** Compared with the random initial model, self-attention frozen pre-trained model GPT2(6) FPT achieves better performance on most datasets and yields an overall **21.2%** relative MSE reduction on 5% data and **14.3%** relative MSE reduction on 10% data. It again suggests that a model pre-trained on cross-domain data can achieve significant performance improvement in time series forecasting.

**The Number of GPT2 Layers** For most transformer-based methods in time-series forecasting Zhou et al. (2022); Wu et al. (2021); Nie et al. (2022), no more than 3 encoder layers are included. However, most pre-trained models with at least 12 layers may suffer from overfitting in time series forecasting. To better balance performance and computational efficiency, we test using various numbers of layers on ETHh2. Additionally, we train a completely random initialized non-pretrained GPT2 as a comparison. The results are shown in Figure 9, for both 5% and 10% data, the pre-trained model is unable to do well with few layers but significantly outperforms non-pre-trained GPT2 with more attention blocks transferred from NLP. It indicates that pre-trained attention layers produce

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Methods & \multicolumn{2}{c|}{GPT2(6)} & \multicolumn{2}{c|}{No Freeze} & \multicolumn{2}{c}{No Pretrain} \\ \hline Metric & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{3}{*}{\begin{tabular}{c} 192 \\ 336 \\ \end{tabular} } & 96 & **0.175** & 0.230 & 0.183 & **0.229** & 0.199 & 0.254 \\  & 192 & **0.227** & **0.276** & 0.275 & 0.300 & 0.262 & 0.302 \\  & 336 & **0.286** & **0.322** & 0.297 & 0.331 & 0.326 & 0.345 \\  & 720 & **0.366** & **0.379** & 0.380 & 0.388 & 0.405 & 0.396 \\ \hline \multirow{3}{*}{\begin{tabular}{c} 192 \\ 336 \\ \end{tabular} } & 96 & **0.543** & **0.506** & 0.671 & 0.564 & 0.882 & 0.643 \\  & 192 & **0.748** & **0.580** & 0.907 & 0.632 & 1.389 & 0.817 \\  & 336 & **0.754** & **0.595** & 0.931 & 0.655 & 2.968 & 1.149 \\  & 720 & - & - & - & - & - \\ \hline \multirow{3}{*}{\begin{tabular}{c} 192 \\ 336 \\ \end{tabular} } & 96 & **0.376** & **0.421** & 0.404 & 0.449 & 0.465 & 0.457 \\  & 192 & **0.418** & **0.441** & 0.503 & 0.478 & 0.614 & 0.536 \\  & 336 & **0.408** & **0.439** & 0.691 & 0.572 & 0.596 & 0.529 \\  & 720 & - & - & - & - & - \\  & 720 & - & - & - & - & - \\ \hline \multirow{3}{*}{\begin{tabular}{c} 20 \\ 336 \\ \end{tabular} } & 96 & **0.386** & **0.405** & 0.429 & 0.432 & 0.394 & 0.410 \\  & 192 & 0.440 & 0.438 & 0.496 & 0.470 & **0.432** & **0.432** \\  & 336 & **0.485** & **0.459** & 0.535 & 0.489 & 0.491 & 0.464 \\  & 720 & **0.557** & **0.499** & 0.786 & 0.592 & 0.564 & 0.503 \\ \hline \multirow{3}{*}{
\begin{tabular}{c} 196 \\ 336 \\ \end{tabular} } & 96 & **0.199** & **0.280** & 0.217 & 0.293 & 0.301 & 0.353 \\  & 192 & **0.256** & **0.316** & 0.300 & 0.350 & 0.321 & 0.365 \\ \cline{1-1}  & 336 & **0.318** & **0.353** & 0.331 & 0.368 & 0.371 & 0.398 \\ \cline{1-1}  & 720 & **0.460** & 0.439 & **0.460** & **0.436** & 0.659 & 0.528 \\ \hline \hline \end{tabular}
\end{table}
Table 21: Model analysis results on 5% data. We use prediction length \(O\in\{96,192,336,720\}\) for ILI and \(O\in\{24,36,48,60\}\) for others.

a great benefit in time series forecasting. Also, the pre-trained model achieves better performance between 3 and 9 layers. Thus GPT2 with 6 layers is chosen as our default architecture.

### No Pre-training but Freezing

For comprehensively ablation on pre-training and freezing strategies, we also add experiment for random initialized GPT2(6) with freezing. The results in Table 23 shows that only input and output modules can not work and pre-trained knowledge play an importance part in time series tasks.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c} \hline \hline Methods & \multicolumn{2}{c|}{GPT2(6)} & \multicolumn{2}{c|}{No Freeze} & \multicolumn{2}{c|}{No Pretrain} & \multicolumn{2}{c}{No Pretrain} \\ \cline{2-9}  & Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{9}{*}{\(\mathcal{O}\)} & 96 & **0.163** & **0.215** & 0.168 & 0.221 & 0.175 & 0.229 \\  & 192 & **0.210** & **0.254** & 0.238 & 0.286 & 0.244 & 0.287 \\  & 336 & **0.256** & **0.292** & 0.318 & 0.301 & 0.325 \\  & 720 & **0.321** & **0.339** & 0.398 & 0.383 & 0.390 & 0.378 \\ \hline \multirow{9}{*}{\(\mathcal{O}\)} & 96 & **0.458** & **0.456** & 0.605 & 0.532 & 0.680 & 0.560 \\  & 192 & **0.570** & **0.516** & 0.713 & 0.579 & 0.738 & 0.602 \\  & 336 & **0.608** & **0.535** & 0.747 & 0.560 & 0.893 & 0.641 \\  & 720 & **0.725** & **0.591** & 0.945 & 0.688 & 2.994 & 1.169 \\ \hline \multirow{9}{*}{\(\mathcal{O}\)} & 96 & **0.331** & **0.374** & 0.369 & 0.492 & 0.422 & 0.433 \\  & 192 & **0.402** & **0.411** & 0.464 & 0.455 & 0.482 & 0.466 \\  & 336 & **0.406** & **0.433** & 0.420 & 0.439 & 0.540 & 0.496 \\  & 720 & **0.449** & **0.464** & 0.535 & 0.515 & 0.564 & 0.519 \\ \hline \multirow{9}{*}{\(\mathcal{O}\)} & 96 & 0.390 & 0.404 & 0.429 & 0.430 & **0.385** & **0.401** \\  & 192 & 0.429 & 0.423 & 0.463 & 0.446 & **0.426** & **0.421** \\  & 336 & **0.469** & **0.439** & 0.510 & 0.470 & 0.506 & 0.455 \\  & 720 & **0.569** & **0.498** & 0.780 & 0.591 & 0.576 & 0.505 \\ \hline \multirow{9}{*}{\(\mathcal{O}\)} & 96 & **0.188** & **0.269** & 0.243 & 0.311 & 0.244 & 0.315 \\  & 192 & **0.251** & **0.309** & 0.307 & 0.352 & 0.318 & 0.363 \\ \cline{1-1}  & 336 & **0.307** & **0.346** & 0.337 & 0.364 & 0.409 & 0.412 \\ \cline{1-1}  & 720 & **0.426** & **0.417** & 0.471 & 0.440 & 0.473 & 0.450 \\ \hline \hline \end{tabular}
\end{table}
Table 22: No Pretrain and No Freeze results on 10% data. We use prediction length \(O\in\{96,192,336,720\}\) for ILI and \(O\in\{24,36,48,60\}\) for others.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline \hline Methods & \multicolumn{2}{c|}{GPT2(6)} & \multicolumn{2}{c|}{No Freeze} & \multicolumn{2}{c}{No Pretrain} & \multicolumn{2}{c}{No Pretrain} \\ \cline{2-9}  & Metric & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{9}{*}{\(\mathcal{O}\)} & 96 & 0.376 & 0.421 & 0.440 & 0.449 & 0.465 & 0.457 & 0.540 & 0.497 \\  & 192 & 0.418 & 0.441 & 0.503 & 0.478 & 0.614 & 0.536 & 0.721 & 0.580 \\ \hline \hline \end{tabular}
\end{table}
Table 23: Ablation on random initialized model with freezing.

Figure 9: Comparison of pre-trained and non-pre-trained GPT2 with various layers on ETH2. Color represents various prediction length \(O\in\{96,192\}\) and line style means different models.

### Fine-Tuning Parameters Selection

In this section, we conduct ablation experiments to study which parameters are important to fine-tune. Since the input embedding and output layers are randomly initialized for adapting to a new domain, they must be trained. Then, we study adding layer normalization and positional embeddings to the list of fine-tuning parameters. Table 24 shows the results that re-train parameters of layer normalization and positional embeddings can bring certain benefits, especially in longer prediction lengths. Thus, we follow the standard practice to re-train positional embeddings and layer normalization.

### Analysis of Data Volume

Results of few-shot learning show that GPT2(6) FPT shows SOTA performance in few-shot learning tasks in which the model is trained on 5% data and 10% data. Plus, it has comparable performance with the SOTA baselines PatchTST and Dlinear on full sample forecasting setting as well. This phenomenon raises a question that how performance changes with an increase in data sample size. Thus, we conduct experiments on various percentages \(P\in\{5\%,10\%,20\%,50\%,80\%,100\%\}\) of ETTh2. Figure 10 shows that the performance improvement for GPT2(6) FPT is almost flattened. These results illustrate that such a cross-domain FPT model is extremely efficient in few-shot time series forecasting and only requires a few fine-tuning samples to reach a SOTA performance. For more complete data, end-to-end training models start to catch up, but still, a GPT2(6) FPT model can be comparable to those SOTA end-to-end training algorithms.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Methods & Input \& Output & \multicolumn{2}{c}{+ LN} & \multicolumn{2}{c}{+ POS} \\ \hline Metric & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{5}{*}{\begin{tabular}{c} SOTA \\ \end{tabular} } & 96 & 0.395 & 0.410 & 0.392 & 0.409 & 0.386 & 0.405 \\  & 192 & 0.444 & 0.438 & 0.436 & 0.435 & 0.440 & 0.438 \\  & 336 & 0.510 & 0.472 & 0.495 & 0.467 & 0.485 & 0.459 \\  & 720 & 0.607 & 0.517 & 0.564 & 0.503 & 0.557 & 0.499 \\ \hline \multirow{5}{*}{
\begin{tabular}{c} SOTA \\ \end{tabular} } & 96 & 0.198 & 0.282 & 0.198 & 0.279 & 0.199 & 0.280 \\  & 192 & 0.261 & 0.324 & 0.263 & 0.325 & 0.256 & 0.316 \\  & 336 & 0.336 & 0.377 & 0.322 & 0.356 & 0.318 & 0.353 \\  & 720 & 0.473 & 0.444 & 0.457 & 0.435 & 0.460 & 0.436 \\ \hline \hline \end{tabular}
\end{table}
Table 24: Ablation by fixing positional embeddings or layer normalization on 5% ETTm1 and ETTm2. Parameters of GPT2(6) are successively added to the list of fine-tuned parameters.

Figure 10: Results on various percentages of ETTh2. Line color represents different models and line style means various prediction lengths \(O\in\{96,192\}\).

### Knowledge transfer with other Pre-trained Transformer Models

We investigate how other pre-trained transformer models perform and whether other domains can also help. Another NLP pre-trained model BERT Devlin et al. (2019) and the CV pre-trained model BEiT Bao et al. (2022) are trained on 5% ETMh2 and 5% ETTm2. Similar to GPT2, we only reserve 6 layers and freeze attention blocks. Our results are shown in Table 25 that BERT(6) FPT and BEiT(6) FPT are comparable to PatchTST and remarkably surpass other baselines. We come to the conclusion that the universality of our proposed architecture holds across other pre-trained-transformer models. Moreover, the domain of successful knowledge transfer in time series forecasting is not limited to natural language. Knowledge from the CV domain can also help, supported by BEiT's experimental results.

### Full Results of Classification

### Full Results of Computation

### Full Results of Short-term Forecasting

### Input Length Setting Discussion

The consideration of input length is of great importance. It is widely believed that longer input lengths have the potential to generate superior results. However, in practice, certain algorithms might fall short in effectively utilizing long input signals due to overfitting issues either. Here, we conducted long-term forecasting experiments by comparing our approach with the best reported values from different baseline papers. This was done to avoid any biases stemming from selective tuning of baseline parameters. While some may argue in favor of a fairer comparison using a fixed input length,

\begin{table}
\begin{tabular}{c|c c c c|c c c c c c c c c|c c c} \hline \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Classical methods} & \multicolumn{3}{c|}{RNN} & \multicolumn{3}{c|}{Transformers} & \multicolumn{3}{c|}{MLP} \\  & X@BoNet & \multicolumn{3}{c|}{Recal LSTM} & \multicolumn{3}{c|}{LSTM} & \multicolumn{3}{c|}{LSTM} & \multicolumn{3}{c|}{LSTM} & \multicolumn{3}{c|}{Rec.} & \multicolumn{3}{c|}{In-} & \multicolumn{3}{c|}{Pava} & \multicolumn{3}{c|}{Auto.} & \multicolumn{3}{c|}{State} & \multicolumn{3}{c|}{FID} & \multicolumn{3}{c|}{ETS} & \multicolumn{3}{c|}{Flow} & \multicolumn{3}{c|}{Diffense} & \multicolumn{3}{c|}{Diffense} & \multicolumn{3}{c|}{Diffense} & \multicolumn{3}{c|}{Off2(6)} \\ \hline EthanolConcentration & 43.7 & 46.2 & 53.9 & 31.1 & 28.9 & 22.7 & 31.9 & 34.6 & 30.8 & 31.6 & 32.7 & 31.2 & 28.1 & 30.8 & 32.6 & 29.7 & 35.7 & 34.2 \\ FaceDetection & 63.3 & 64.7 & 65.7 & 66.7 & 52.8 & 67.3 & 68.6 & 67.0 & 65.7 & 68.4 & 68.0 & 66.0 & 66.3 & 67.6 & 68.0 & 67.5 & 68.6 & 69.2 \\ Handwriting & 15.8 & 58.8 & 25.8 & 24.6 & 53.3 & 22.0 & 24.7 & 32.8 & 29.4 & 36.7 & 31.6 & 28.0 & 25.3 & 33.8 & 27.0 & 26.1 & 32.1 & 32.7 \\ Heartbeat & 73.2 & 75.6 & 77.1 & 72.2 & 75.6 & 76.1 & 77.1 & 50.5 & 75.6 & 24.6 & 73.7 & 73.7 & 71.2 & 77.6 & 75.1 & 78.0 & 77.2 \\ JapaneseVessels & 86.5 & 96.2 & 98.1 & 98.4 & 89.9 & 98.9 & 97.8 & 98.9 & 98.4 & 96.2 & 99.2 & 98.4 & 95.9 & 99.8 & 96.2 & 96.2 & 98.4 & 98.6 \\ PIERS-SF & 89.3 & 75.1 & 86.1 & 86.1 & 86.8 & 82.2 & 87.1 & 83.8 & 82.7 & 87.3 & 80.9 & 86.0 & 83.8 & 75.1 & 88.4 & 89.6 & 87.9 \\ SelfRegulationCP1 & 84.6 & 90.8 & 84.0 & 50.8 & 84.6 & 92.2 & 92.4 & 90.4 & 90.1 & 88.1 & 84.0 & 84.0 & 84.7 & 88.6 & 92.5 & 87.3 & 89.8 & 91.8 & 93.2 \\ SelfRegulationCP2 & 48.9 & 53.3 & 52.8 & 52.2 & 56.5 & 53.9 & 56.7 & 53.3 & 53.3 & 50.6 & 57.2 & 54.4 & 55.0 & 56.1 & 50.5 & 51.1 & 57.2 & 59.4 \\ Sokolch-ArabieDis & 69.6 & 71.2 & 100.0 & 100.0 & 95.6 & 98.4 & 97.0 & 100.0 & 96.0 & 100.0 & 100.0 & 100.0 & 98.8 & 81.4 & 100.0 & 99.0 & 99.2 \\ UWgGeAttembers & 75.9 & 94.4 & 87.8 & 85.9 & 89.8 & 85.6 & 85.6 & 85.6 & 83.4 & 85.9 & 87.5 & 85.3 & 85.0 & 86.6 & 82.1 & 80.3 & 85.3 & 88.1 \\ \hline Average & 66.0 & 72.5 & 71.8 & 70.9 & 70.3 & 71.9 & 71.5 & 72.1 & 70.8 & 71.1 & 72.7 & 70.7 & 71.0 & 73.0 & 67.3 & 70.4 & **75.6** & **74.0** \\ \hline \end{tabular}
\end{table}
Table 26: Full results for the classification task. \(*\) in the Transformers indicates the name of \(*\)former.

\begin{table}
\begin{tabular}{c|c|c c c|c c c} \hline \hline \multirow{2}{*}{Methods} & \multirow{2}{*}{Metric} & \multicolumn{3}{c|}{ETTh2} & \multicolumn{3}{c}{ETTm2} \\  & & 96 & 192 & 336 & 720 & 96 & 192 & 336 \\ \hline GPT2-backbone(6 Layers) & \begin{tabular}{c} MSE \\ MAE \\ \end{tabular} & 
\begin{tabular}{c} **0.376** \\ **0.419** \\ **0.414** \\ **0.413** \\ **0.418** \\ **0.465** \\ **0.418** \\ **0.466** \\ **0.504** \\ **0.500** \\ **0.

we are starting to shift our focus towards pursuing more accurate algorithms that possess enhanced capabilities for handling longer inputs. Instead of restricting the input length to a fixed small value, it is pragmatic to tune both the input length and model parameters based on performance, as it is often the primary concern in practical usage. Exploring the utilization of extremely long inputs, such as Chatgpt or LLM, is among our future research directions.

\begin{table}
\begin{tabular}{c|c c c c|c c c|c c c|c c c|c c c|c c c} \hline \hline Methods & \multicolumn{3}{c|}{SGD} & \multicolumn{3}{c|}{MSD} & \multicolumn{3}{c|}{MSL} & \multicolumn{3}{c|}{SMAP} & \multicolumn{3}{c|}{SWAT} & \multicolumn{3}{c|}{PSM} & \multicolumn{3}{c}{Avg FF} \\ Metrics & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 & \% \\ \hline GPT(6) & **88.89** & **84.98** & **86.59** & 82.00 & **82.91** & **82.45** & **90.60** & **90.75** & **72.88** & **92.20** & **96.34** & **94.23** & **98.62** & 95.68 & 97.13 & **86.72** \\ TimeNet\({}^{a}\) & 87.91 & 85.41 & 84.61 & **89.54** & 75.36 & 81.84 & 90.14 & 95.60 & 63.99 & 90.75 & 95.40 & 93.02 & 95.81 & **96.20** & **97.34** & 85.24 \\ PatchSTT & 87.26 & 82.14 & 84.62 & 88.34 & 70.96 & 78.70 & 90.64 & 95.46 & 86.82 & 81.91 & 80.49 & 85.72 & 89.84 & 93.74 & 96.08 & 82.79 \\ ETSTServer & 87.44 & 79.23 & 83.13 & 85.13 & 84.93 & 85.03 & 92.25 & 35.75 & 69.50 & 92.02 & 80.36 & 84.91 & 99.31 & 85.28 & 91.76 & 82.87 \\ ErDfemer & 87.95 & 82.92 & 85.08 & 77.41 & 90.77 & 85.70 & 87.41 & 70.70 & 94.51 & 70.70 & 90.17 & 96.42 & 99.31 & 97.31 & 97.16 & 97.23 & 84.97 \\ LightTS & 87.10 & 78.42 & 82.53 & 82.40 & 75.78 & 88.79 & 92.58 & 52.52 & 62.91 & 91.98 & 94.72 & 93.33 & 98.37 & 99.97 & 97.15 & 84.23 \\ DiLiner & 83.62 & 71.25 & 72.70 & 70.84 & 84.34 & 85.42 & 83.92 & 93.25 & 44.62 & 89.11 & 95.30 & 87.52 & 93.88 & 92.96 & 96.72 & 82.66 \\ Statowary & 88.33 & 81.21 & 84.62 & 86.55 & 89.14 & 77.75 & 80.37 & 95.92 & 70.19 & 70.68 & 80.39 & 96.75 & 79.88 & 97.82 & 96.76 & 97.29 & 82.08 \\ Autoformer & 88.06 & 82.35 & 85.51 & 77.27 & 80.92 & 79.96 & 90.40 & 83.62 & 71.12 & 88.95 & 95.31 & 92.74 & 90.08 & 91.58 & 91.39 & 84.26 \\ Pyatformer & 85.61 & 80.61 & 80.50 & 83.81 & 85.93 & 84.86 & 92.54 & 57.71 & 71.09 & 87.29 & 87.96 & 90.00 & 91.78 & 71.67 & 96.02 & 82.08 & 82.57 \\ Anomaly Transformer & 89.91 & 82.23 & 85.49 & 79.61 & 83.77 & 83.31 & 91.85 & 85.11 & 71.18 & 72.51 & 93.72 & 83.10 & 68.35 & 94.72 & 94.90 & 80.50 \\ Inference & 86.70 & 72.85 & 81.57 & 86.74 & 86.48 & 80.61 & 90.17 & 87.13 & 93.62 & 97.96 & 97.55 & 81.43 & 64.27 & 96.33 & 77.10 & 78.83 \\ Reformer & 82.58 & 69.24 & 75.32 & 85.51 & 83.31 & 84.40 & 90.91 & 57.44 & 70.40 & 72.50 & 96.53 & 82.80 & 99.93 & 95.38 & 73.61 & 77.31 \\ LogTransformer & 83.46 & 70.13 & 76.21 & 73.05 & 87.37 & 95.77 & 89.57 & 91.57 & 99.69 & 86.71 & 87.39 & 80.52 & 63.69 & 98.00 & 76.74 & 76.60 \\ Transformer & 83.58 & 76.13 & 79.56 & 71.57 & 87.37 & 78.68 & 89.37 & 57.12 & 69.70 & 68.84 & 96.53 & 80.37 & 62.75 & 96.56 & 76.07 & 76.88 \\ \hline \hline \multicolumn{11}{l}{\({}^{a}\)We reproduce the results of TimeNet by https://github.com/human/Time-Series-Library.} \\ \multicolumn{11}{l}{\({}^{b}\)We replace the joint criterion in Anomaly Transformer with reconstruction error for fair comparison.} \\ \end{tabular}
\end{table}
Table 27: Full results for the anomaly detection.

\begin{table}
\begin{tabular}{c|c c c c|c c c|c c c|c c c|c c c|c c c} \hline \hline Methods & GPT(23) & TimesNet & PectT & STFGTGT & & & & & & & & & & & & & & & & & & & & & & & & & & & & & \\ Mask & MAE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{4}{*}{
\begin{tabular}{c} 25.59 \\ 25.52 \\ \end{tabular} } & **0.017** & **0.085** & 0.023 & 0.101 & 0.041 & 0.13 & 0.096 & 0.229 & 0.093 & 0.206 & 0.080 & 0.193 & 0.052 & 0.166 & 0.033 & 0.119 & 0.046 & 0.144 & 0.063 & 0.180 & 0.042 & 0.146 \\  & 25.52 & **0.022** & **0.096** & 0.023 & 0.101 & 0.044 & 0.135 & 0.096 & 0.229 & 0.036 & 0.080 & 0.

\begin{table}
\begin{tabular}{l l|c c c c c c c c c c c c c c} \hline \hline  & Methods & GPT2(6) & TimesNet & PatchITST & N-HiTS & N-BEATS & ETSformer & Light/TS & DLinear & FEDformer & Stationary & Autoformer & Informer & Reformer \\ \hline \multirow{4}{*}{
\begin{tabular}{} \end{tabular} } & SMAPE & 13.531 & **13.387** & 13.477 & 13.418 & 13.436 & 18.009 & 14.247 & 16.965 & 13.728 & 13.717 & 13.974 & 14.727 & 16.169 \\  & MASE & 3.015 & **2.996** & 3.019 & 3.045 & 3.043 & 4.487 & 3.109 & 4.283 & 3.048 & 3.078 & 3.134 & 3.418 & 3.800 \\  & OWA & 0.793 & **0.786** & 0.792 & 0.793 & 0.794 & 1.115 & 0.827 & 1.058 & 0.803 & 0.807 & 0.822 & 0.881 & 0.973 \\ \cline{2-13}  & SMAPE & 10.177 & **10.100** & 10.38 & 10.202 & 10.124 & 13.376 & 11.364 & 12.145 & 10.792 & 10.958 & 11.338 & 11.360 & 13.313 \\  & MASE & 1.194 & **1.182** & 1.233 & 1.194 & 1.169 & 1.906 & 1.328 & 1.520 & 1.283 & 1.325 & 1.365 & 1.401 & 1.775 \\  & OWA & 0.898 & **0.890** & 0.921 & 0.899 & 0.886 & 1.302 & 1.000 & 1.106 & 0.958 & 0.981 & 1.012 & 1.027 & 1.252 \\ \cline{2-13}  & SMAPE & 12.894 & **12.670** & 12.959 & 12.791 & 12.677 & 14.588 & 14.014 & 13.514 & 14.260 & 13.917 & 13.958 & 14.062 & 20.128 \\  & MASE & 0.956 & **0.933** & 0.970 & 0.969 & 0.937 & 1.368 & 1.053 & 1.037 & 1.102 & 1.097 & 1.103 & 1.141 & 2.614 \\  & OWA & 0.897 & **0.878** & 0.905 & 0.899 & 0.880 & 1.149 & 0.981 & 0.956 & 1.012 & 0.998 & 1.002 & 1.024 & 1.927 \\ \cline{2-13}  & SMAPE & 4.940 & **4.891** & 4.952 & 5.061 & 4.925 & 7.267 & 15.880 & 6.709 & 4.954 & 6.302 & 5.485 & 24.460 & 32.491 \\  & MASE & 3.228 & 3.302 & 3.347 & **3.216** & 3.391 & 5.240 & 11.434 & 4.953 & 3.264 & 4.064 & 3.865 & 20.960 & 33.355 \\  & OWA & 1.029 & **1.035** & 1.049 & 1.040 & 1.053 & 1.591 & 3.474 & 1.487 & 1.036 & 1.304 & 1.187 & 5.879 & 8.679 \\ \cline{2-13}  & SMAPE & 11.991 & **11.829** & 12.059 & 11.927 & 11.851 & 14.718 & 13.525 & 13.639 & 12.840 & 12.780 & 12.909 & 14.086 & 18.200 \\  & MASE & 1.600 & **1.585** & 1.623 & 1.613 & 1.599 & 2.408 & 2.111 & 2.095 & 1.701 & 1.756 & 1.771 & 2.718 & 4.223 \\  & OWA & 0.861 & **0.851** & 0.869 & 0.861 & 0.855 & 1.172 & 1.051 & 1.051 & 0.918 & 0.930 & 0.939 & 1.230 & 1.775 \\ \hline \hline \end{tabular}
\end{table}
Table 29: Full results of short-term forecasting.