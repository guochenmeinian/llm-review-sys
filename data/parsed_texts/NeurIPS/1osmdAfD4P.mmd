# Online Convex Optimization with Unbounded Memory

Raunak Kumar

Department of Computer Science

Cornell University

Ithaca, NY 14853

raunak@cs.cornell.edu &Sarah Dean

Department of Computer Science

Cornell University

Ithaca, NY 14853

sdean@cornell.edu &Robert Kleinberg

Department of Computer Science

Cornell University

Ithaca, NY 14853

rdk@cs.cornell.edu

###### Abstract

Online convex optimization (OCO) is a widely used framework in online learning. In each round, the learner chooses a decision in a convex set and an adversary chooses a convex loss function, and then the learner suffers the loss associated with their current decision. However, in many applications the learner's loss depends not only on the current decision but on the entire history of decisions until that point. The OCO framework and its existing generalizations do not capture this, and they can only be applied to many settings of interest after a long series of approximation arguments. They also leave open the question of whether the dependence on memory is tight because there are no non-trivial lower bounds. In this work we introduce a generalization of the OCO framework, "Online Convex Optimization with Unbounded Memory", that captures long-term dependence on past decisions. We introduce the notion of \(p\)-effective memory capacity, \(H_{p}\), that quantifies the maximum influence of past decisions on present losses. We prove an \(O(\sqrt{H_{p}T})\) upper bound on the policy regret and a matching (worst-case) lower bound. As a special case, we prove the first non-trivial lower bound for OCO with finite memory (Anava et al., 2015), which could be of independent interest, and also improve existing upper bounds. We demonstrate the broad applicability of our framework by using it to derive regret bounds, and to improve and simplify existing regret bound derivations, for a variety of online learning problems including online linear control and an online variant of performative prediction.

## 1 Introduction

Numerous applications are characterized by multiple rounds of sequential interactions with an environment, e.g., prediction from expert advice (Littlestone and Warmuth, 1989, 1994), portfolio selection (Cover, 1991), routing (Awerbuch and Kleinberg, 2008), etc. One of the most popular frameworks for modelling such sequential decision-making problems is online convex optimization (OCO) (Zinkevich, 2003). The OCO framework is as follows. In each round, the learner chooses a decision in a convex set and an adversary chooses a convex loss function, and then the learner suffers the loss associated with their current decision. The performance of an algorithm is measured by regret: the difference between the algorithm's total loss and that of the best fixed decision. We refer the reader to Shalev-Shwartz (2012), Hazan (2019), Orabona (2019) for surveys on this topic.

However, in many applications the loss of the learner depends not only on the current decisions but on the entire history of decisions until that point. For example, in online linear control (Agarwal et al., 2019), in each round the learner chooses a "control policy" (i.e., decision), suffers a loss that is a function of the action taken by this policy and the current state of the system, and the system's state evolves according to linear dynamics. The current state depends on the entire history of actions and, therefore, the current loss depends not only on the current decision but the entire history of decisions. The OCO framework cannot capture such long-term dependence of the current loss on the past decisions and neither can existing generalizations that allow the loss to depend on a _constant_ number of past decisions (Anava et al., 2015). Although a series of approximation arguments can be used to apply finite memory generalizations of OCO to the online linear control problem, there is no OCO framework that captures the complete long-term dependence of current losses on past decisions. Furthermore, there are no non-trivial lower bounds for OCO in the memory setting,1 which leaves open the question whether the dependence on memory is tight.

Footnote 1: The trivial lower bound refers to the \(\Omega(\sqrt{T})\) lower bound for OCO in the memoryless setting.

Contributions.In this paper we introduce a generalization of the OCO framework, "Online Convex Optimization with Unbounded Memory" (Section 2), that allows the loss in the current round to depend on the entire history of decisions until that point. We introduce the notion of \(p\)-effective memory capacity, \(H_{p}\), that quantifies the maximum influence of past decisions on present losses. We prove an \(O(\sqrt{H_{p}T})\) upper bound on the policy regret (Theorem 3.1) and a matching (worst-case) lower bound (Theorem 3.2). As a special case, we prove the first non-trivial lower bound for OCO with finite memory (Theorem 3.4), which could be of independent interest, and also improve existing upper bounds (Theorem 3.3). Our lower bound technique extends existing techniques developed for memoryless settings. We design novel adversarial loss functions that exploit the fact that an algorithm cannot overwrite its history. We illustrate the power of our framework by bringing together the regret analysis of two seemingly disparate problems under the same umbrella. First, we show how our framework improves and simplifies existing regret bounds for the online linear control problem (Agarwal et al., 2019) in Theorem 4.1. Second, we show how our framework can be used to derive regret bounds for an online variant of performative prediction (Perdomo et al., 2020) in Theorem 4.2. This demonstrates the broad applicability of our framework for deriving regret bounds for a variety of online learning problems, particularly those that exhibit long-term dependence of current losses on past decisions.

Related work.The most closely related work to ours is the OCO with finite memory framework (Anava et al., 2015). They consider a generalization of the OCO framework that allows the current loss to depend on a _constant_ number of past decisions. There have been a number of follow-up works that extend the framework in a variety of other ways, such as non-stationarity (Zhao et al., 2022), incorporating switching costs (Shi et al., 2020), etc. However, none of these existing works go beyond a constant memory length and do not prove a non-trivial lower bound with a dependence on the memory length. In a different line of work, Bhatia and Sridharan (2020) consider a much more general online learning framework that goes beyond a constant memory length, but they only provide _non-constructive_ upper bounds on regret. In contrast, our OCO with unbounded memory framework allows the current loss to depend on an _unbounded_ number of past decisions, provides _constructive_ upper bounds on regret, and lower bounds for a broad class of problems that includes OCO with finite memory with a general memory length \(m\).

A different framework for sequential decision-making is multi-armed bandits (Bubeck and Cesa-Bianchi, 2012; Slivkins, 2019). Qin et al. (2023) study a variant of contextual stochastic bandits where the current loss can depend on a sparse subset of all prior contexts. This setting differs from ours due to the feedback model, stochasticity, and decision space. Reinforcement learning (Sutton and Barto, 2018) is yet another popular framework for sequential decision-making that considers very general state-action models of feedback and dynamics. In reinforcement learning one typically measures regret with respect to the best state-action policy from some policy class, rather than the best fixed decision as in online learning and OCO. In the special case of linear control, policies can be reformulated as decisions while preserving convexity; we discuss this application in Section 4. Considering the general framework is an active area of research.

We defer discussion of related work for specific applications to Section 4.

Framework

We begin with some motivation for the formalism used in our framework (Section 2.1). Many real-world applications involve controlling a physical dynamical system, for example, variable-speed wind turbines in wind energy electric power production (Boukhezzar and Siguerdidjane, 2010). The typical solution for these problems has been to model them as offline control problems with linear time-invariant dynamics and use classical methods such as LQR and LQG (Boukhezzar and Siguerdidjane, 2010). Instead of optimizing over the space of control inputs, the typical feedback control approach optimizes over the space of controllers, i.e., policies that choose a control input as a function of the system state. The standard controllers considered in the literature are linear controllers. Even when the losses are convex in the state and input, they are nonconvex in the linear controller. In the special case of quadratic losses in terms of the state and input, there is a closed-form solution for the optimal solution using the algebraic Riccati equations (Lancaster and Rodman, 1995). But this does not hold for general convex losses resulting in convex reparameterizations such as Youla (Youla et al., 1976; Kucera, 1975) and SLS (Wang et al., 2019; Anderson et al., 2019). The resulting parameterization represents an infinite dimensional system response and is characterized by a sequence of matrices. Recent work has studied an online approach for some of these control theory problems, where a sequence of controllers is chosen adaptively rather than choosing one offline (Abasi-Yadkori and Szepesvari, 2011; Dean et al., 2018; Simchowitz and Foster, 2020; Agarwal et al., 2019).

The takeaway from the above is that there are online learning problems in which (i) the current loss depends on the entire history of decisions; and (ii) the decision space can be more complicated than just a subset of \(\mathbb{R}^{d}\), e.g., it can be an unbounded sequence of matrices. This motivates us to model the decision space as a Hilbert space and the history space as a Banach space in the formal problem setup below, and this subsumes the special cases of OCO and OCO with finite memory. This formalism not only lets us consider a wide range of spaces, such as \(\mathbb{R}^{d}\), unbounded sequences of matrices, etc., but also lets us define appropriate norms on these spaces. This latter feature is crucial for deriving strong regret bounds for some applications such as online linear control. For this problem we derive improved regret bounds (Theorem 4.1) by defining weighted norms on the decision and history spaces, where the weights are chosen to leverage the problem structure.

Notation.We use \(\|\cdot\|_{\mathcal{U}}\) to denote the norm associated with a space \(\mathcal{U}\). The operator norm for a linear \(L\) operator from space \(\mathcal{U}\to\mathcal{V}\) is defined as \(\|L\|_{\mathcal{U}\to\mathcal{V}}=\max_{u:\|u\|_{U}\leq 1}\|Lu\|_{\mathcal{V}}\). For convenience, sometimes we simply use \(\|\cdot\|\) when the meaning is clear from the context. For a finite-dimensional matrix we use \(\|\cdot\|_{F}\) and \(\|\cdot\|_{2}\) to denote its Frobenius norm and operator norm respectively.

### Setup

Let the decision space \(\mathcal{X}\) be a closed and convex subset of a Hilbert space \(\mathcal{W}\) with norm \(\|\cdot\|_{\mathcal{X}}\) and the history space \(\mathcal{H}\) be a Banach space with norm \(\|\cdot\|_{\mathcal{H}}\). Let \(A:\mathcal{H}\to\mathcal{H}\) and \(B:\mathcal{W}\to\mathcal{H}\) be linear operators. The game between the learner and an oblivious adversary proceeds as follows. Let \(T\) denote the time horizon and \(f_{t}:\mathcal{H}\to\mathbb{R}\) be loss functions chosen by the adversary. The initial history is \(h_{0}=0\). In each round \(t\in[T]\), the learner chooses \(x_{t}\in\mathcal{X}\), the history is updated to \(h_{t}=Ah_{t-1}+Bx_{t}\), and the learner suffers loss \(f_{t}(h_{t})\). An instance of an online convex optimization with unbounded memory problem is specified by the tuple \((\mathcal{X},\mathcal{H},A,B)\).

We use the notion of policy regret (Dekel et al., 2012) as the performance measure in our framework. The policy regret of a learner is the difference between its total loss and the total loss of a strategy that plays the best fixed decision in every round. The history after round \(t\) for a strategy that chooses \(x\) in every round is described by \(h_{t}=\sum_{k=0}^{t-1}A^{k}Bx\), which motivates the following definition.

**Definition 2.1**.: Given \(f_{t}:\mathcal{H}\to\mathbb{R}\), the function \(\tilde{f}_{t}:\mathcal{X}\to\mathbb{R}\) is defined by \(\tilde{f}_{t}(x)=f_{t}(\sum_{k=0}^{t-1}A^{k}Bx)\).

**Definition 2.2** (Policy Regret).: The policy regret of an algorithm \(\mathcal{A}\) is defined as \(R_{T}(\mathcal{A})=\sum_{t=1}^{T}f_{t}(h_{t})-\min_{x\in\mathcal{X}}\sum_{t=1 }^{T}\tilde{f}_{t}(x)\).

In many motivating examples such as online linear control (Section 4.1), the history at the end of a round is a sequence of linear transformations of past decisions. The following definition captures this formally and we leverage this structure to prove stronger regret bounds (Theorem 3.1).

**Definition 2.3** (Linear Sequence Dynamics).: Consider an online convex optimization with unbounded memory problem specified by \((\mathcal{X},\mathcal{H},A,B)\). Let \((\xi_{k})_{k=0}^{\infty}\) be a sequence of nonnegative real numbers satisfying \(\xi_{0}=1\). We say that \((\mathcal{X},\mathcal{H},A,B)\) follows linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm for \(p\geq 1\) if

1. \(\mathcal{H}\) is the \(\xi\)-weighted \(\ell^{p}\)-direct sum of a finite or countably infinite number of copies of \(\mathcal{W}\): every element \(y\in\mathcal{H}\) is a sequence \(y=(y_{i})_{i\in\mathcal{I}}\), where \(\mathcal{I}=\mathbb{N}\) or \(\mathcal{I}=\{0,\dots,n\}\) for some \(n\in N\), and \(\|y\|_{\mathcal{H}}=\left(\sum_{i\in\mathcal{I}}(\xi_{i}\|y_{i}\|)^{p}\right)^ {\nicefrac{{1}}{{p}}}<\infty\).
2. We have \(A(y_{0},y_{1},\dots)=(0,A_{0}y_{0},A_{1}y_{1},\dots)\), where \(A_{i}:\mathcal{W}\to\mathcal{W}\) are linear operators.
3. The operator \(B\) satisfies \(B(x)=(x,0,\dots)\).

Note that since the norm on \(\mathcal{H}\) depends on the weights \(\xi\), the operator norm \(\|A^{k}\|\) also depends on \(\xi\). If the weights are all equal to \(1\), then we simply say \(p\)-norm instead of \(\xi\)-weighted \(p\)-norm.

### Assumptions

We make the following assumptions about the feedback model and the loss functions.

1. The learner knows the operators \(A\) and \(B\), and observes \(f_{t}\) at the end of each round \(t\).
2. The operator norm of \(B\) is at most \(1\), i.e., \(\|B\|\leq 1\).
3. The functions \(f_{t}\) are convex.
4. The functions \(f_{t}\) are \(L\)-Lipschitz continuous: \(\forall\ h,\tilde{h}\in\mathcal{H}\) and \(t\in[T]\), we have \(|f_{t}(h)-f_{t}(\tilde{h})|\leq L\|h-\tilde{h}\|_{\mathcal{H}}\).

Regarding Assumption **A1**, our results easily extend to the case where instead of observing \(f_{t}\), the learner receives a gradient \(\nabla\tilde{f}_{t}(x_{t})\) from a gradient oracle, which can be implemented using knowledge of \(f_{t}\) and the dynamics \(A\) and \(B\). Handling the cases when the operators \(A\) and \(B\) are unknown and/or the learner observes bandit feedback (i.e., only \(f_{t}(h_{t})\)) are important problems and we leave them as future work. Note that our assumption that \(A\) and \(B\) are known is no more restrictive than in the existing literature on OCO with finite memory (Anava et al., 2015) where it is assumed that the learner knows the constant memory length. In fact, our assumption is more general because our framework not only captures constant memory length as a special case but allows for richer dynamics as we illustrate in Section 4. Assumption **A2** is made for convenience, and it amounts to a rescaling of the problem. Assumption **A3** can be replaced by the _weaker_ assumption that \(\tilde{f}_{t}\) are convex (similar to the literature on OCO with finite memory (Anava et al., 2015)) and this is what we use in the rest of the paper.

Assumptions **A1** and **A4** imply that \(\tilde{f}_{t}\) are \(\tilde{L}\)-Lipschitz continuous for the following \(\tilde{L}\).

**Theorem 2.1**.: _Consider an online convex optimization with unbounded memory problem specified by \((\mathcal{X},\mathcal{H},A,B)\). If \(f_{t}\) is \(L\)-Lipschitz continuous, then \(\tilde{f}_{t}\) is \(\tilde{L}\)-Lipschitz continuous for \(\tilde{L}\leq L\sum_{k=0}^{\infty}\|A^{k}\|\). If \((\mathcal{X},\mathcal{H},A,B)\) follows linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm for \(p\geq 1\), then \(\tilde{L}\leq L\left(\sum_{k=0}^{\infty}\|A^{k}\|^{p}\right)^{\frac{1}{p}}\)._

The proof follows from the definitions of \(\tilde{f}_{t}\) and \(\|\cdot\|_{\mathcal{H}}\), and we defer it to Appendix A. The above bound is tighter than similar results in the literature on OCO with finite memory and online linear control. This theorem is a key ingredient, amongst others, in improving existing upper bounds on regret for OCO with finite memory (Theorem 3.3) and for online linear control (Theorem 4.1). Before presenting our final assumption we introduce the notion of \(p\)-effective memory capacity that quantifies the maximum influence of past decisions on present losses.

**Definition 2.4** (\(p\)-Effective Memory Capacity).: Consider an online convex optimization with unbounded memory problem specified by \((\mathcal{X},\mathcal{H},A,B)\). For \(p\geq 1\), the \(p\)-effective memory capacity is defined as

\[H_{p}(\mathcal{X},\mathcal{H},A,B)=\left(\sum_{k=0}^{\infty}k^{p}\|A^{k}\|^{p} \right)^{\frac{1}{p}}.\] (1)When the meaning is clear from the context we simply use \(H_{p}\) instead. The \(p\)-effective memory capacity is an upper bound on the difference in histories for two sequences of decisions whose difference grows at most linearly with time. To see this, consider two sequences of decisions, \((x_{k})\) and \((\tilde{x}_{k})\), whose elements differ by no more than \(k\) at time \(k\): \(\|x_{k}-\tilde{x}_{k}\|\leq k\). Then the histories generated by the two sequences have difference between bounded as \(\|h-\hat{h}\|=\|\sum_{k}A^{k}B(x_{k}-\tilde{x}_{k})\|\leq\sum_{k}k\|A^{k}B\| \leq\sum_{k}k\|A^{k}\|=H_{1}\), where the last inequality follows from Assumption A2. A similar bound holds with \(H_{p}\) instead when \((\mathcal{X},\mathcal{H},A,B)\) follows linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm.

* The \(1\)-effective memory capacity is finite, i.e., \(H_{1}<\infty\).

Since \(H_{p}\) is decreasing in \(p\), \(H_{1}<\infty\) implies \(H_{p}<\infty\) for all \(p\geq 1\). For the case of linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm it suffices to make the _weaker_ assumption that \(H_{p}<\infty\). However, for simplicity of exposition, we assume that \(H_{1}<\infty\).

### Special Cases

OCO with Finite Memory.Consider the OCO with finite memory problem with constant memory length \(m\). It can be specified in our framework by \((\mathcal{X},\mathcal{H},A_{\text{finite},m},B_{\text{finite},m})\), where \(\mathcal{H}\) is the \(\ell^{2}\)-direct sum of \(m\) copies of \(\mathcal{X}\), \(A_{\text{finite},m}(x^{[m]},\dots,x^{[1]})=(0,x^{[m]},\dots,x^{[2]})\), and \(B_{\text{finite},m}(x)=(x,0,\dots,0)\). Note that \((\mathcal{X},\mathcal{H},A_{\text{finite},m},B_{\text{finite},m})\) follows linear sequence dynamics with the \(2\)-norm. Our framework can even model an extension where the problem follows linear sequence dynamics with the \(p\)-norm for \(p\geq 1\) by simply defining \(\mathcal{H}\) to be the \(\ell^{p}\)-direct sum of \(m\) copies of \(\mathcal{X}\).

OCO with \(\rho\)-discounted Infinite Memory.Our framework can also model OCO with infinite memory problems that are not modelled by existing OCO frameworks. Let \(\rho\in(0,1)\) be the discount factor and \(p\geq 1\). An OCO with \(\rho\)-discounted infinite memory problem is specified by \((\mathcal{X},\mathcal{H},A_{\text{infinite},\rho},B_{\text{infinite},\rho})\), where \(\mathcal{H}\) is the \(\ell^{p}\)-direct sum of countably many copies of \(\mathcal{X}\), \(A_{\text{infinite},\rho}((y_{0},y_{1},\dots))=(0,\rho y_{0},\rho y_{1},\dots)\), and \(B_{\text{infinite},\rho}(x)=(x,0,\dots)\). Note that \((\mathcal{X},\mathcal{H},A_{\text{infinite},\rho},B_{\text{infinite},\rho})\) follows linear sequence dynamics with the \(p\)-norm. Due to space constraints we defer proofs of regret bounds for this problem to the appendix.

## 3 Regret Analysis

We present two algorithms for choosing the decisions \(x_{t}\). Algorithm 1 uses follow-the-regularized-leader (FTRL) (Shalev-Shwartz and Singer, 2006; Abernethy et al., 2008) on the loss functions \(\tilde{f}_{t}\). Due to space constraints, we discuss how to implement it efficiently in Appendix G and present simple simulation experiments in Appendix I. Algorithm 2, which we only present in Appendix H, combines FTRL with a mini-batching approach (Dekel et al., 2012; Altschuler and Talwar, 2018; Chen et al., 2020) to additionally guarantee that the decisions switch at most \(O(\nicefrac{{T\hat{L}}}{{LH_{1}}})\) times. We defer the proofs of the following upper and lower bounds to Appendices C and D respectively.

``` Input : Time horizon \(T\), step size \(\eta\), \(\alpha\)-strongly-convex regularizer \(R:\mathcal{X}\rightarrow\mathbb{R}\).
1 Initialize history \(h_{0}=0\).
2for\(t=1,2,\dots,T\)do
3 Learner chooses \(x_{t}\in\arg\min_{x\in\mathcal{X}}\sum_{s=1}^{t-1}\tilde{f}_{s}(x)+\frac{R(x)}{\eta}\).
4 Set \(h_{t}=Ah_{t-1}+Bx_{t}\).
5 Learner suffers loss \(f_{t}(h_{t})\) and observes \(f_{t}\).
6 end for ```

**Algorithm 1**Ftrl

**Theorem 3.1**.: _Consider an online convex optimization with unbounded memory problem specified by \((\mathcal{X},\mathcal{H},A,B)\). Let the regularizer \(R:\mathcal{X}\rightarrow\mathbb{R}\) be \(\alpha\)-strongly-convex and satisfy \(|R(x)-R(\tilde{x})|\leq\hat{D}\) for all \(x,\tilde{x}\in\mathcal{X}\). Algorithm 1 with step-size \(\eta\) satisfies \(R_{T}(\texttt{FTRL})\leq\frac{D}{\eta}+\eta\frac{T\hat{L}^{2}}{\alpha}+\eta \frac{TL\hat{L}H_{1}}{\alpha}\). If

[MISSING_PAGE_FAIL:6]

where term (a) is the random sign \(\epsilon_{n}\) sampled for the block \(n=\lceil t/m\rceil\) that \(t\) belongs to, term (b) is a scaling factor chosen while respecting the Lipschitz continuity constraint, and term (c) is a sum over a _subset_ of past decisions. Two important features of this construction are: (i) a random sign is sampled for each block rather than each round; and (ii) the loss in round \(t\) depends on the history of decisions until and including the first round of the block that \(t\) belongs to. These exploit the fact that an algorithm cannot overwrite its history and penalize it for its past decisions even after it observes the random sign \(\epsilon_{n}\) for the current block. (See Fig. 1 for an illustration.) Existing lower bound proofs for OCO sample a random sign in each round and choose \(f_{t}(x_{t})\propto\epsilon_{t}x_{t}\). A first attempt at extending this for the OCO with finite memory setting would be to choose \(f_{t}(h_{t})\propto\epsilon_{t}\sum_{k=0}^{m-1}x_{t-k}\). However, in constrast to our approach, this does not exploit the fact that an algorithm cannot overwrite its history and does not suffice for obtaining a matching lower bound.

Comparison of upper bound with prior work.The algorithmic ideas and analysis for our regret upper bound are influenced by Anava et al. (2015). However, an important innovation in our work is the use of weighted norms in the case of linear sequence dynamics. This is a simple but powerful way of encoding prior knowledge about a problem, and allows us to derive non-trivial regret bounds in the case of unbounded-length histories. The technical complications that arise are captured in bounding the relevant quantities of interest, e.g., the Lipschitz constant \(\tilde{L}\), the operator norm \(\|A^{k}\|\), etc. Furthermore, using weighted norms even leads to improved regret bounds for some applications. Indeed, consider the application to online linear control with adversarial disturbances (Section 4.1). Our framework and upper bound applied to this problem (Theorem 4.1) improve upon the existing upper bound, which used a finite memory approximation. See Lemmas E.2 and E.6 for an illustration of the technical details involved when using weighted norms.

## 4 Applications

In this section we apply our framework to online linear control (Section 4.1) and online performative prediction (Section 4.2). We defer expanded details and proofs to Appendices E and F respectively.

### Online Linear Control

Background.Online linear control (OLC) is the problem of controlling a system with linear dynamics, adversarial disturbances, and adversarial and convex losses. It combines aspects from control theory and online learning. We refer the reader to Agarwal et al. (2019) for more details. Here, we introduce the basic mathematical setup of the problem.

Let \(\mathcal{S}\subseteq\mathbb{R}^{d_{s}}\) and \(\mathcal{U}\subseteq\mathbb{R}^{d_{u}}\) denote the state and control spaces. Let \(s_{t}\) and \(u_{t}\) denote the state and control at time \(t\) with \(s_{0}\) being the initial state. The system evolves according to the linear dynamics \(s_{t+1}=Fs_{t}+Gu_{t}+w_{t}\), where \(F\in\mathbb{R}^{d_{s}\times d_{s}},G\in\mathbb{R}^{d_{s}\times d_{u}}\) are matrices satisfying \(\|F\|_{2},\|G\|_{2}\leq\kappa\) and \(w_{t}\in\mathbb{R}^{d_{s}}\) is an adversarially chosen disturbance with \(\|w\|_{2}\leq W\). Without loss of generality, we assume that \(\kappa,W\geq 1\), \(d_{s}=d_{u}=d\), and also define \(w_{-1}=s_{0}\). For \(t=0,\dots,T-1\), let \(c_{t}:\mathcal{S}\times\mathcal{U}\to[0,1]\) be convex loss functions chosen by an oblivious adversary. The functions \(c_{t}\) satisfy the following Lipschitz condition: if \(\|s\|_{2}\), \(\|u\|_{2}\leq D_{\mathcal{X}}\), then \(\|\nabla_{s}c_{t}(s,u)\|,\|\nabla_{u}c_{t}(s,u)\|\leq L_{0}D_{\mathcal{X}}\). The goal in online linear control is to choose a sequence of policies that yield a sequence of controls \(u_{t}\) to minimize the regret \(R_{T}(\Pi)=\sum_{t=0}^{T-1}c_{t}(s_{t},u_{t})-\min_{\pi^{*}\in\Pi}\sum_{t=0}^{ T-1}c_{t}(s_{t}^{\pi^{*}},u_{t}^{\pi^{*}})\), where \(s_{t}\) evolves according to linear dynamics stated above, \(\Pi\) denotes a controller class, and \(s_{t}^{\pi^{*}},c_{t}^{\pi^{*}}\) denote the state and control at time \(t\) when the controls are chosen according to \(\pi^{*}\).

Figure 1: An illustration of the loss functions \(f_{t}\) for the OCO with finite memory lower bound.

A very simple controller class is constant input, i.e., \(\Pi=\{\pi_{u}:\pi(s)=u\in\mathcal{U}\}\). In this case, the history \(h_{t}\) can be represented by the finite-dimensional state \(s_{t}\), and the operators can be set to \(A=F\) and \(B=\nicefrac{{G}}{{\|G\|}}\). However, like previous work (Agarwal et al., 2019) we focus on the class of \((\kappa,\rho)\)-strongly stable linear controllers, \(\mathcal{K}\), where \(K\in\mathcal{K}\) satisfies \(F-GK=HLH^{-1}\) with \(\|K\|_{2},\|H\|_{2},\|H^{-1}\|_{2}\leq\kappa\) and \(\|L\|_{2}=\rho<1\). Given such a controller, the inputs are chosen as linear functions of the current state, i.e., \(u_{t}=-Ks_{t}\). Unfortunately, parameterizing \(u_{t}\) directly with a linear controller as \(u_{t}=-Ks_{t}\) leads to a non-convex problem because \(s_{t}\) is a non-linear function of \(K\), e.g., if disturbances are \(0\), then \(s_{t}=(F-GK)^{\mathrm{t}}s_{0}\). An alternative parameterization is the disturbance-action controller (DAC).

**Definition 4.1**.: Let \(K\in\mathcal{K}\) be fixed. The class of disturbance-action controllers (DACs) \(\mathcal{M}_{K}\) is \(\{(K,M):M=(M^{[s]})_{s=0}^{\infty}\}\), where \(M^{[s]}\in\mathbb{R}^{d\times d}\) satisfies \(\|M^{[s]}\|_{2}\leq\kappa^{4}\rho^{s}\). The control in round \(k\) is chosen as \(u_{k}=-Ks_{k}+\sum_{s=1}^{k+1}M^{[s]}w_{k-s}\).

The class of such DACs has two important properties. First, it acts on the entire history of past disturbances. Consequently, given an arbitrary \(K\in\mathcal{K}\), every \(K^{*}\in\mathcal{K}\) can be expressed as a DAC \((K,M)\in\mathcal{M}_{K}\) with \(M=(M^{[1]},\dots,M^{[T+1]},0,\dots)\)(Agarwal et al., 2019, Section 16.5). That is, \(\mathcal{K}\subseteq\mathcal{M}_{K}\) and it suffices to compute regret against \(\mathcal{M}_{K}\) instead of \(\mathcal{K}\). For the rest of this paper we fix \(K\in\mathcal{K}\) and denote \(\widetilde{F}=F-GK\). Second, suppose \(M_{t}=(M_{t}^{[s]})_{s=0}^{\infty}\) is the parameter chosen in round \(t\) and the control \(u_{t}\) is chosen according to the DAC \((K,M_{t})\). Then, \(s_{t}\) and \(u_{t}\) are _linear_ functions of the parameters, which implies that \(c_{t}\) is convex in the parameters. (See the next paragraph on "Formulation as OCO with Unbounded Memory" for a formula.) A similar parameterization was first considered for online linear control by Agarwal et al. (2019) and is based on similar ideas in control theory, e.g., Youla (Youla et al., 1976, Kucera, 1975) and SLS (Wang et al., 2019, Anderson et al., 2019).

Formulation as OCO with Unbounded Memory.The first step is a change of variables with respect to the control inputs from linear controllers to DACs and the second is a corresponding change of variables for the state. Define the decision space \(\mathcal{X}=\{M=(M^{[s]}):M^{[s]}\in\mathbb{R}^{d\times d},\|M^{[s]}\|_{2} \leq\kappa^{4}\rho^{s}\}\). Define the history space \(\mathcal{H}\) to be the set consisting of sequences \(h=(Y_{k})\), where \(Y_{0}\in\mathcal{X}\) and \(Y_{k}=\widetilde{F}^{k-1}GX_{k}\) for \(X_{k}\in\mathcal{X},k\geq 1\). (Recall \(\widetilde{F}=F-GK\).) Define weighted norms \(\|M\|_{\mathcal{X}}^{2}=\sum_{s=1}^{\infty}\rho^{-s}\|M^{[s]}\|_{F}^{2}\) and \(\|h\|_{\mathcal{H}}^{2}=\sum_{k=0}^{\infty}\xi_{k}^{2}\|Y_{k}\|_{\mathcal{X}} ^{2}\), where the weights \((\xi_{k})\) are defined as \(\xi=(1,1,1,\rho^{-1},\rho^{-1},\rho^{-\frac{1}{2}},\dots)\). Define the linear operators \(A:\mathcal{H}\to\mathcal{H}\) and \(B:\mathcal{W}\to\mathcal{H}\) as \(A((Y_{0},Y_{1},\dots))=(0,GY_{0},\widetilde{F}Y_{1},\widetilde{F}Y_{2},\dots)\) and \(B(M)=(M,0,0,\dots)\). Note that the problem follows linear sequence dynamics with the \(\xi\)-weighted 2-norm (Definition 2.3). The weights in the norms on \(\mathcal{X}\) and \(\mathcal{H}\) increase exponentially. However, the norms \(\|M^{[s]}\|_{F}^{2}\) and \(\|\widetilde{F}^{k-1}G\|_{F}^{2}\) decrease exponentially as well: by definition of \(\mathcal{X}\) and the assumption on \(\widetilde{F}=F-GK\) for \(K\in\mathcal{K}\). Leveraging these exponential decays to define exponentially increasing weights is crucial for deriving our regret bounds that are stronger than existing results.

Construct the functions \(f_{t}:\mathcal{H}\to\mathbb{R}\) that correspond to \(c_{t}(s_{t},u_{t})\) as follows. Given a sequence of decisions \((M_{0},\dots,M_{t})\), the history at the end of round \(t\) is given by \(h_{t}=(M_{t},GM_{t-1},\widetilde{F}GM_{t-2},\dots,\widetilde{F}^{t-1}GM_{0}, 0,\dots)\). A simple inductive argument shows that the state and control in round \(t\) can be written as functions of \(h_{t}\) as \(s_{t}=\widetilde{F}^{t}s_{0}+\sum_{k=0}^{t-1}\sum_{s=1}^{k+1}\widetilde{F}^{t- k-1}GM_{t}^{[s]}w_{k-s}+w_{t-1}\) and \(u_{t}=-Ks_{t}+\sum_{s=1}^{t+1}M_{t}^{[s]}w_{t-s}\). Define the functions \(f_{t}:\mathcal{H}\to\mathbb{R}\) by \(f_{t}(h)=c_{t}(s,u)\), where \(s\) and \(u\) are the state and control determined by the history as above. Note that \(f_{t}\) is parameterized by the past disturbances. Since the state and control are linear functions of the history and \(c_{t}\) is convex, this implies that \(f_{t}\) is convex. Now, given the above formulation and the fact that the class of disturbance-action controllers is a superset of the class of \((\kappa,\rho)\)-strongly-stable linear controllers, we have that the policy regret for the online linear control problem is at most the policy regret, \(\sum_{t=0}^{T-1}f_{t}(h_{t})-\min_{M\in\mathcal{X}}\sum_{t=0}^{T-1}\tilde{f}_{ t}(M)\). The following is our main result for online linear control and it improves existing results (Agarwal et al., 2019) by a factor of \(O(d\log(T)^{3.5}\kappa^{5}(1-\rho)^{-1})\). See Appendix E.3 for a detailed comparison.

**Theorem 4.1**.: _Consider the online linear control problem as defined in Section 4.1. Suppose the decisions in round \(t\) are chosen using Algorithm 1. Then, the upper bound on the policy regret is_

\[O\left(L_{0}W^{2}\sqrt{T}d^{\frac{1}{2}}\kappa^{17}(1-\rho)^{-4.5}\right).\] (2)Comparison with prior and concurrent work.Existing works solve OLC (and its extensions) by making multiple finite memory approximations. First, they formulate the problem as OCO with finite memory. This requires bounding numerous error terms because the problem is inherently an OCO with unbounded memory problem. We bypass these error analysis steps entirely because the problem fits into our framework naturally. Second, existing works use the parameterization from Agarwal et al. (2019) that only acts on a fixed, constant number of past disturbances. In particular, existing works use a "truncated" DAC policy that is a sequence of \(d\times d\) matrices of length \(2\kappa^{4}(1-\rho)^{-1}\log T\). Our DAC policy acts on the entire history of disturbances and is a sequence of \(d\times d\) matrices of unbounded length. Yet, we capture the dimension of this infinite-dimensional space in a way that still improves the overall bound, including completely eliminating the dependence on \(\log T\), and improving the dependence on \(d,\kappa\), and \((1-\rho)\). This improvement comes from our novel use of weighted norms on the history and decision spaces. These norms allow us to give tighter bounds on the relevant quantities in the regret upper bound, e.g., \(\|A^{k}\|\) (Lemma E.2) and \(\tilde{L}\) (Lemma E.6).

In complementary concurrent work, Lin et al. (2022) focus on a more general online control problem. They improve regret bounds for this general version by a factor of \(\log T\) compared to existing reductions to OCO with finite memory. They do so by using that the impact of a past policy decays geometrically with time. On the other hand, the primary focus of our work is studying the complete dependence of present losses on the entire history in OCO. Applying our resulting OCO with unbounded memory framework to OLC, we improve upon existing results for OLC by removing all \(\log T\) factors and improving the dependence on \(d,\kappa\), and \((1-\rho)\).

### Online Performative Prediction

Background.In many applications of machine learning the algorithm's decisions influence the data distribution, e.g., online labor markets (Anagnostopoulos et al., 2018; Horton, 2010), predictive policing (Lum and Isaac, 2016), on-street parking (Dowling et al., 2020; Pierce and Shoup, 2018), vehicle sharing markets (Banerjee et al., 2015), etc. Motivated by such applications, several works have studied the problem of performative prediction, which models the data distribution as a function of the decision-maker's decision (Perdomo et al., 2020; Mendler-Dunner et al., 2020; Miller et al., 2021; Brown et al., 2022; Ray et al., 2022; Jagadeesan et al., 2022). Most of these works view the problem as a stochastic optimization problem; Jagadeesan et al. (2022) adopt a regret minimization perspective. We refer the reader to these citations for more details. As a natural extension to existing works, we introduce an online learning variant of performative prediction with geometric decay (Ray et al., 2022) that differs from the original formulations in a few key ways.

Let the decision set \(\mathcal{X}\subseteq\mathbb{R}^{d}\) be closed and convex with \(\|x\|_{2}\leq D_{\mathcal{X}}\). Let \(p_{1}\) denote the initial data distribution over the instance space \(\mathcal{Z}\). In each round \(t\in[T]\), the learner chooses a decision \(x_{t}\in\mathcal{X}\) and an oblivious adversary chooses a loss function \(l_{t}:\mathcal{X}\times\mathcal{Z}\rightarrow[0,1]\), and then the learner suffers the loss \(L_{t}(x_{t})=\mathbb{E}_{z\sim p_{t}}\left[l_{t}(x_{t},z)\right]\), where \(p_{t}=p_{t}(x_{1},\ldots,x_{t})\) is the data distribution in round \(t\). The goal in our online learning setting is to minimize the difference between the algorithm's total loss and the total loss of the best fixed decision, \(\sum_{t=1}^{T}\mathbb{E}_{z\sim p_{t}}\left[l_{t}(x_{t},z)\right]-\min_{x\in \mathcal{X}}\sum_{t=1}^{T}\mathbb{E}_{z\sim p_{t}(x)}\left[l_{t}(x,z)\right]\), where \(p_{t}(x)=p_{t}(x,\ldots,x)\) is the data distribution in round \(t\) had \(x\) been chosen in all rounds so far. This measure is similar to performative regret (Jagadeesan et al., 2022) and is a natural generalization of performative optimality (Perdomo et al., 2020) for an online learning formulation.

We make the following assumptions. First, the loss functions \(l_{t}\) are convex and \(L_{0}\)-Lipschitz continuous. Second, the data distribution satisfies for all \(t\geq 1\), \(p_{t+1}=\rho p_{t}+(1-\rho)\mathcal{D}(x_{t})\), where \(\rho\in(0,1)\) and \(\mathcal{D}(x_{t})\) is a distribution over \(\mathcal{Z}\) that depends on the decision \(x_{t}\)(Ray et al., 2022). Third, \(\mathcal{D}(x)\) is a location-scale distribution: \(z\sim\mathcal{D}(x)\) iff \(z\sim\xi+Fx\), where \(F\in\mathbb{R}^{d\times d}\) satisfies \(\|F\|_{2}<\infty\) and \(\xi\) is a random variable with mean \(\mu\) and covariance \(\Sigma\)(Ray et al., 2022).

Our problem formulation differs from existing work in the following ways. First, we adopt an online learning perspective on performative prediction with geometric decay, whereas Ray et al. (2022) adopt a stochastic optimization one. So, we assume that the loss functions \(l_{t}\) are adversarially chosen, whereas Ray et al. (2022) assume \(l_{t}=l\) are fixed. Second, we assume that the dynamics (\(\mathcal{D}\) and \(\rho\)) are known (Assumption A1), whereas Ray et al. (2022) assume they are unknown and use samples from the data distribution. We believe that an appropriate extension of our framework that can deal with unknown linear operators \(A\) and \(B\) can be applied to this more difficult setting, and we leave this as future work. Third, even though Jagadeesan et al. (2022) also study an online learning variant of performative prediction, they assume \(l_{t}=l\) are fixed and the data distribution depends only on the current decisions, whereas we assume the data distribution depends on the entire history of decisions.

Formulation as OCO with Unbounded Memory.Let \(\rho\in(0,1)\). Let the decision space \(\mathcal{X}\subseteq\mathbb{R}^{d}\) be closed and convex with the \(2\)-norm. Let the history space \(\mathcal{H}\) be the \(\ell^{1}\)-direct sum of countably infinite number of copies of \(\mathcal{X}\). Define the linear operators \(A:\mathcal{H}\rightarrow\mathcal{H}\) and \(B:\mathcal{W}\rightarrow\mathcal{H}\) as \(A((y_{0},y_{1},\dots))=(0,\rho y_{0},\rho y_{1},\dots)\) and \(B(x)=(x,0,\dots)\). The problem is an OCO with \(\rho\)-discounted infinite memory problem and follows linear sequence dynamics with the \(1\)-norm (Definition 2.3). Given a sequence of decisions \((x_{k})_{k=1}^{t}\), the history is \(h_{t}=(x_{t},\rho x_{t-1},\dots,\rho^{t-1}x_{1},0,\dots)\) and the data distribution \(p_{t}=p_{t}(h_{t})\) satisfies: \(z\sim p_{t}\) iff \(z\sim\sum_{k=1}^{t-1}(1-\rho)\rho^{k-1}(\xi+Fx_{t-k})+\rho^{t}p_{1}\). This follows from the recursive definition of \(p_{t}\) and parametric assumption about \(\mathcal{D}(x)\). Define the functions \(f_{t}:\mathcal{H}\rightarrow[0,1]\) by \(f_{t}(h_{t})=\mathbb{E}_{z\sim p_{t}}[l_{t}(x_{t},z)]\). Now, the original goal of minimizing the difference between the algorithm's total loss and the total loss of the best fixed decision is equivalent to minimizing the policy regret. The following is our main result for online performative prediction.

**Theorem 4.2**.: _Consider the online performative prediction problem as defined in Section 4.2. Suppose the decisions in round \(t\) are chosen using Algorithm 1. Then, the upper bound on the policy regret is_

\[O\left(D_{\mathcal{X}}L_{0}\sqrt{T}\|F\|_{2}(1-\rho)^{-\frac{1}{2}}\rho^{-1} \right).\]

## 5 Conclusion

In this paper we introduced a generalization of the OCO framework, "Online Convex Optimization with Unbounded Memory", that allows the loss in the current round to depend on the entire history of decisions until that point. We proved matching upper and lower bounds on the policy regret in terms of the time horizon, the \(p\)-effective memory capacity (a quantitative measure of the influence of past decisions on present losses), and other problem parameters (Theorems 3.1 and 3.2). As a special case, we proved the first non-trivial lower bound for OCO with finite memory (Theorem 3.4), which could be of independent interest, and also improved existing upper bounds (Theorem 3.3). We illustrated the power of our framework by bringing together the regret analysis of two seemingly disparate problems under the same umbrella: online linear control (Theorem 4.1), where we improve and simplify existing regret bounds, and online performative prediction (Theorem 4.2).

There are a number of directions for future research. A natural follow-up is to consider unknown dynamics (i.e., when the learner does not know the operators \(A\) and \(B\)) and/or the case of bandit feedback (i.e., when the learner only observes \(f_{t}(h_{t})\)). The extension to bandit feedback has been considered in the OCO and OCO with finite memory literature (Hazan and Li, 2016; Bubeck et al., 2021; Zhao et al., 2021; Gradu et al., 2020; Cassel and Koren, 2020). It is tempting to think about a version where the history is a _nonlinear_, but decaying, function of the past decisions. The obvious challenge is that the nonlinearity would lead to non-convex losses. It is unclear how to deal with such issues, e.g., restricted classes of nonlinearities for which the OCO with unbounded memory perspective is still relevant (Zhang et al., 2015), different problem formulations such as online non-convex learning (Gao et al., 2018; Suggala and Netrapalli, 2020), etc.

There is a growing body of work on online linear control and its variants that rely on OCO with finite memory (Hazan et al., 2020; Agarwal et al., 2019; Foster and Simchowitz, 2020; Cassel and Koren, 2020; Gradu et al., 2020; Li et al., 2021; Minasyan et al., 2021). In this paper we showed how our framework can be used to improve and simplify regret bounds for the online linear control problem. Another direction for future work is to use our framework, perhaps with suitable extensions outlined above, to derive similar improvements for these other variants of online linear control.

## Acknowledgments and Disclosure of Funding

We thank Sloan Nietert and Victor Sanches Portella for helpful discussions. We thank Wei-Yu Chan for pointing out an error in the proof of Lemma C.1. We also thank anonymous AISTATS and NeurIPS reviewers for their helpful comments on an older and the current version of the paper respectively. This research was partially supported by the NSERC Postgraduate Scholarships-Doctoral Fellowship 545847-2020, NSF awards CCF-2312774 and OAC-2311521, and a gift from Wayfair.

## References

* The 24th Annual Conference on Learning Theory, June 9-11, 2011, Budapest, Hungary_, 2011.
* COLT 2008, Helsinki, Finland, July 9-12, 2008_, pages 263-274. Omnipress, 2008.
* Agarwal et al. (2019a) Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. 2019a.
* Agarwal et al. (2019b) Naman Agarwal, Brian Bullins, Elad Hazan, Sham M. Kakade, and Karan Singh. Online control with adversarial disturbances. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 111-119. PMLR, 2019b.
* Agarwal et al. (2019c) Naman Agarwal, Elad Hazan, and Karan Singh. Logarithmic regret for online control. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 10175-10184, 2019c.
* Agrawal et al. (2018) Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system for convex optimization problems. _Journal of Control and Decision_, 5(1):42-60, 2018.
* Altschuler and Talwar (2018) Jason M. Altschuler and Kunal Talwar. Online learning over a finite action set with limited switching. In Sebastien Bubeck, Vianney Perchet, and Philippe Rigollet, editors, _Conference On Learning Theory, COLT 2018, Stockholm, Sweden, 6-9 July 2018_, 2018.
* Anagnostopoulos et al. (2018) Aris Anagnostopoulos, Carlos Castillo, Adriano Fazzone, Stefano Leonardi, and Evimaria Terzi. Algorithms for hiring and outsourcing in the online labor market. In Yike Guo and Faisal Farooq, editors, _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018, London, UK, August 19-23, 2018_, pages 1109-1118. ACM, 2018.
* Anava et al. (2015) Oren Anava, Elad Hazan, and Shie Mannor. Online learning for adversaries with memory: Price of past mistakes. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 784-792, 2015.
* Anderson et al. (2019) James Anderson, John C Doyle, Steven H Low, and Nikolai Matni. System level synthesis. _Annual Reviews in Control_, 47:364-393, 2019.
* Awerbuch and Kleinberg (2008) Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. _J. Comput. Syst. Sci._, 74(1):97-114, 2008.
* Banerjee et al. (2015) Siddhartha Banerjee, Carlos Riquelme, and Ramesh Johari. Pricing in ride-share platforms: A queueing-theoretic approach. _Available at SSRN 2568258_, 2015.
* Bhatia and Sridharan (2020) Kush Bhatia and Karthik Sridharan. Online learning with dynamics: A minimax perspective. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Boukhezzar and Siguerdidjane (2010) Boubekeur Boukhezzar and Houria Siguerdidjane. Comparison between linear and nonlinear control strategies for variable speed wind turbines. _Control Engineering Practice_, 18:1357-1368, 2010.
* Brown et al. (2022) Gavin Brown, Shlomi Hod, and Iden Kalemaj. Performative prediction in a stateful world. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, _International Conference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event_, volume 151 of _Proceedings of Machine Learning Research_, pages 6045-6061. PMLR, 2022.
* B. C.

Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends(r) in Machine Learning_, 5:1-122, 2012.
* Bubeck et al. [2021] Sebastien Bubeck, Ronen Eldan, and Yin Tat Lee. Kernel-based methods for bandit convex optimization. _J. ACM_, 68(4):25:1-25:35, 2021.
* Cassel and Koren [2020] Asaf B. Cassel and Tomer Koren. Bandit linear control. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Chen et al. [2020] Lin Chen, Qian Yu, Hannah Lawrence, and Amin Karbasi. Minimax regret of switching-constrained online convex optimization: No phase transition. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Cover [1991] Thomas M Cover. Universal portfolios. _Mathematical finance_, 1(1):1-29, 1991.
* Dean et al. [2018] Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for robust adaptive control of the linear quadratic regulator. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 4192-4201, 2018.
* July 1, 2012_. icml.cc / Omnipress, 2012.
* Diamond and Boyd [2016] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex optimization. _Journal of Machine Learning Research_, 17(83):1-5, 2016.
* Dowling et al. [2020] Chase P. Dowling, Lillian J. Ratliff, and Baosen Zhang. Modeling curbside parking as a network of finite capacity queues. _IEEE Trans. Intell. Transp. Syst._, 21(3):1011-1022, 2020.
* Foster and Simchowitz [2020] Dylan J. Foster and Max Simchowitz. Logarithmic regret for adversarial online control. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, 2020.
* Gao et al. [2018] Xiand Gao, Xiaobo Li, and Shuzhong Zhang. Online learning with non-convex losses and non-stationary regret. In Amos J. Storkey and Fernando Perez-Cruz, editors, _International Conference on Artificial Intelligence and Statistics, AISTATS 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain_, volume 84 of _Proceedings of Machine Learning Research_, pages 235-243. PMLR, 2018.
* Golub and Van Loan [1996] Gene H. Golub and Charles F. Van Loan. _Matrix Computations, Third Edition_. John Hopkins University Press, 1996.
* Gradu et al. [2020] Paula Gradu, John Hallman, and Elad Hazan. Non-stochastic control with bandit feedback. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Hazan [2019] Elad Hazan. Introduction to online convex optimization. _CoRR_, abs/1909.05207, 2019.
* Hazan and Li [2016] Elad Hazan and Yuanzhi Li. An optimal algorithm for bandit convex optimization. _CoRR_, abs/1603.04350, 2016. URL http://arxiv.org/abs/1603.04350.
* Hazan et al. [2020] Elad Hazan, Sham M. Kakade, and Karan Singh. The nonstochastic control problem. In Aryeh Kontorovich and Gergely Neu, editors, _Algorithmic Learning Theory, ALT 2020, 8-11 February 2020, San Diego, CA, USA_, volume 117 of _Proceedings of Machine Learning Research_, pages 408-421. PMLR, 2020.
* Hazan et al. [2020]John J Horton. Online labor markets. In _International workshop on internet and network economics_, pages 515-522. Springer, 2010.
* Jagadeesan et al. (2022) Meena Jagadeesan, Tijana Zrnic, and Celestine Mendler-Dunner. Regret minimization with performative feedback. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, 2022.
* Karlin (2017) Anna Karlin. Lecture notes for cse 522: Algorithms and uncertainty, 2017. URL https://courses.cs.washington.edu/courses/cse522/17sp/lectures/Lecture9.pdf.
* Kucera (1975) Vladimir Kucera. Stability of discrete linear feedback systems. _IFAC Proceedings Volumes_, 8(1):573-578, 1975.
* Lancaster and Rodman (1995) Peter Lancaster and Leiba Rodman. _Algebraic riccati equations_. Clarendon press, 1995.
* Li et al. (2021) Yingying Li, Subhro Das, and Na Li. Online optimal control with affine constraints. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pages 8527-8537. AAAI Press, 2021.
* Lin et al. (2022) Yiheng Lin, James Preiss, Emile Anand, Yingying Li, Yisong Yue, and Adam Wierman. Online adaptive controller selection in time-varying systems: No-regret via contractive perturbations. _arXiv preprint arXiv:2210.12320_, 2022.
* 1 November 1989_, pages 256-261. IEEE Computer Society, 1989.
* Littlestone and Warmuth (1994) Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. _Inf. Comput._, 108(2):212-261, 1994.
* Lum and Isaac (2016) Kristian Lum and William Isaac. To predict and serve? _Significance_, 13(5):14-19, 2016.
* Mendler-Dunner et al. (2020) Celestine Mendler-Dunner, Juan C. Perdomo, Tijana Zrnic, and Moritz Hardt. Stochastic optimization for performative prediction. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Miller et al. (2021) John Miller, Juan C. Perdomo, and Tijana Zrnic. Outside the echo chamber: Optimizing the performative risk. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 7710-7720. PMLR, 2021.
* Minasyan et al. (2021) Edgar Minasyan, Paula Gradu, Max Simchowitz, and Elad Hazan. Online control of unknown time-varying dynamical systems. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, 2021.
* Orabona (2019) Francesco Orabona. A modern introduction to online learning. _CoRR_, abs/1912.13213, 2019.
* Perdomo et al. (2020) Juan C. Perdomo, Tijana Zrnic, Celestine Mendler-Dunner, and Moritz Hardt. Performative prediction. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 7599-7609. PMLR, 2020.
* Pierce and Shoup (2018) Gregory Pierce and Donald Shoup. Sfpark: Pricing parking by demand. In _Parking and the City_, pages 344-353. Routledge, 2018.
* Qin et al. (2023) Yuzhen Qin, Yingcong Li, Fabio Pasqualetti, Maryam Fazel, and Samet Oymak. Stochastic contextual bandits with long horizon rewards. In _Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Washington, DC, USA, February 7-14, 2023_. AAAI Press, 2023.
* Qin et al. (2020)Mitas Ray, Lillian J. Ratliff, Dmitriy Drusvyatskiy, and Maryam Fazel. Decision-dependent risk minimization in geometrically decaying dynamic environments. In _Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelvent Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022_, pages 8081-8088. AAAI Press, 2022.
* Shalev-Shwartz [2012] Shai Shalev-Shwartz. Online learning and online convex optimization. _Found. Trends Mach. Learn._, 4(2):107-194, 2012.
* Shalev-Shwartz and Singer [2006] Shai Shalev-Shwartz and Yoram Singer. Online learning meets optimization in the dual. In Gabor Lugosi and Hans Ulrich Simon, editors, _Learning Theory, 19th Annual Conference on Learning Theory, COLT 2006, Pittsburgh, PA, USA, June 22-25, 2006, Proceedings_, 2006.
* Shi et al. [2020] Guanya Shi, Yiheng Lin, Soon-Jo Chung, Yisong Yue, and Adam Wierman. Online optimization with memory and competitive control. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Simchowitz and Foster [2020] Max Simchowitz and Dylan J. Foster. Naive exploration is optimal for online LQR. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, 2020.
* Slivkins [2019] Aleksandrs Slivkins. Introduction to multi-armed bandits. _Found. Trends Mach. Learn._, 12:1-286, 2019.
* Suggala and Netrapalli [2020] Arun Sai Suggala and Praneeth Netrapalli. Online non-convex learning: Following the perturbed leader is optimal. In Aryeh Kontorovich and Gergely Neu, editors, _Algorithmic Learning Theory, ALT 2020, 8-11 February 2020, San Diego, CA, USA_, volume 117 of _Proceedings of Machine Learning Research_, pages 845-861. PMLR, 2020.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Wang et al. [2019] Yuh-Shyang Wang, Nikolai Matni, and John C. Doyle. A system-level approach to controller synthesis. _IEEE Trans. Autom. Control._, 64(10):4079-4093, 2019.
* Youla et al. [1976] Dante Youla, Hamid Jabr, and Jr Bongiorno. Modern wiener-hopf design of optimal controllers-part ii: The multivariable case. _IEEE Transactions on Automatic Control_, 21(3):319-338, 1976.
* Zhang et al. [2015] Lijun Zhang, Tianbao Yang, Rong Jin, and Zhi-Hua Zhou. Online bandit learning for a special class of non-convex losses. In Blai Bonet and Sven Koenig, editors, _Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA_, pages 3158-3164. AAAI Press, 2015.
* Zhao et al. [2021] Peng Zhao, Guanghui Wang, Lijun Zhang, and Zhi-Hua Zhou. Bandit convex optimization in non-stationary environments. _J. Mach. Learn. Res._, 22:125:1-125:45, 2021.
* Zhao et al. [2022] Peng Zhao, Yu-Xiang Wang, and Zhi-Hua Zhou. Non-stationary online learning with memory and non-stochastic control. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, _International Conference on Artificial Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event_, volume 151 of _Proceedings of Machine Learning Research_, pages 2101-2133. PMLR, 2022.
* Zinkevich [2003] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Tom Fawcett and Nina Mishra, editors, _Proceedings of the 20th International Conference on Machine Learning, ICML 2003, August 21-24 2003, Washington, DC, USA_, 2003.

### Organization of the Appendix

The appendix is organized as follows:

* Appendix A contains the proofs of the results from Section 2.
* Appendix B contains the proofs of existing results about FTRL for completeness.
* Appendix C contains the proofs of upper bounds on regret from Section 3.
* Appendix D contains the proofs of lower bounds on regret from Section 3.
* Appendix E contains the proofs of results about online linear control from Section 4.1.
* Appendix F contains the proofs of results about online performative prediction from Section 4.2.
* Appendix G discusses how to implement Algorithm 1 efficiently.
* Appendix H presents an algorithm for OCO with unbounded memory that provides the same upper bound on policy regret as Algorithm 1 while guaranteeing a small number of switches (Algorithm 2).
* Appendix I presents simulation experiments.

## Appendix A Framework

In this section prove Theorem 2.1. But first we prove a lemma that we use for proofs involving linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm (Definition 2.3). Recall that \(\|\cdot\|_{\mathcal{U}}\) denotes the norm associated with a space \(\mathcal{U}\) and the operator norm \(\|L\|\) for a linear operator \(L:\mathcal{U}\rightarrow\mathcal{V}\) is defined as \(\|L\|=\max_{u:\|u\|_{\mathcal{U}}\leq 1}\|Lu\|_{\mathcal{V}}\).

**Lemma A.1**.: _Consider an online convex optimization with unbounded memory problem specified by \((\mathcal{X},\mathcal{H},A,B)\). If \((\mathcal{X},\mathcal{H},A,B)\) follows linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm for \(p\geq 1\), then for all \(k\geq 1\)_

\[\xi_{k}\|A_{k-1}\cdots A_{0}\|\leq\|A^{k}\|.\]

Proof.: Let \(x\in\mathcal{X}\) with \(\|x\|_{\mathcal{X}}=1\). We have

\[\xi_{k}\|A_{k-1}\cdots A_{0}x\|_{\mathcal{X}}=\|A^{k}(x,0,\ldots)\|_{\mathcal{ H}}\leq\|A^{k}\|\|(x,0,\ldots)\|_{\mathcal{H}}\leq\|A^{k}\|,\]

where the last inequality follows because \(\|(x,0,\ldots)\|_{\mathcal{H}}=\xi_{0}\|x\|_{\mathcal{X}}\) and \(\xi_{0}=1\) by Definition 2.3. Therefore, \(\|A_{k-1}\cdots A_{0}\|\leq\|A^{k}\|\). 

**Theorem 2.1**.: _Consider an online convex optimization with unbounded memory problem specified by \((\mathcal{X},\mathcal{H},A,B)\). If \(f_{t}\) is \(L\)-Lipschitz continuous, then \(\tilde{f}_{t}\) is \(\tilde{L}\)-Lipschitz continuous for \(\tilde{L}\leq L\sum_{k=0}^{\infty}\|A^{k}\|\). If \((\mathcal{X},\mathcal{H},A,B)\) follows linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm for \(p\geq 1\), then \(\tilde{L}\leq L\left(\sum_{k=0}^{\infty}\|A^{k}\|^{p}\right)^{\frac{1}{p}}\)._

Proof.: Let \(x,\tilde{x}\in\mathcal{X}\). For the general case, we have

\[\left|\tilde{f}_{t}(x)-\tilde{f}_{t}(\tilde{x})\right| =\left|f_{t}\left(\sum_{k=0}^{t-1}A^{k}Bx\right)-f_{t}\left(\sum_ {k=0}^{t-1}A^{k}B\tilde{x}\right)\right|\] by Definition 2.1 \[\leq L\,\left\|\sum_{k=0}^{t-1}A^{k}B(x-\tilde{x})\right\|_{ \mathcal{H}} f_{t}\text{ is $L$-Lipschitz continuous}\] \[\leq L\,\sum_{k=0}^{t-1}\|A^{k}\|\|B\|\|x-\tilde{x}\|_{\mathcal{X}}\] \[\leq L\,\sum_{k=0}^{t-1}\|A^{k}\|\|x-\tilde{x}\|_{\mathcal{X}} \text{ by Assumption \ref{eq:L-Lipschitz-continuous}}\] \[\leq L\,\sum_{k=0}^{\infty}\|A^{k}\|\|x-\tilde{x}\|_{\mathcal{X}}.\]If \((\mathcal{H},\mathcal{X},A,B)\) follows linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm for \(p\geq 1\), then we have

\[\left|\tilde{f}_{t}(x)-\tilde{f}_{t}(\tilde{x})\right| =\left|f_{t}\left(\sum_{k=0}^{t-1}A^{k}Bx\right)-f_{t}\left(\sum_{ k=0}^{t-1}A^{k}B\tilde{x}\right)\right|\] by Definition 2.1 \[\leq L\ \left\|\sum_{k=0}^{t-1}A^{k}B(x-\tilde{x})\right\|_{ \mathcal{H}} f_{t}\text{ is $L$-Lipschitz continuous}\] \[=L\ \left\|(0,A_{0}(x-\tilde{x}),A_{1}A_{0}(x-\tilde{x}),\dots)\right\|\] by Definition 2.3 \[=L\ \left(\sum_{k=0}^{t-1}\xi_{k}^{p}\left\|A_{k-1}\cdots A_{0}(x- \tilde{x})\right\|^{p}\right)^{\frac{1}{p}}\] by Definition 2.3 \[\leq L\ \left(\sum_{k=0}^{t-1}\left\|A^{k}\right\|^{p}\right)^{ \frac{1}{p}}\left\|x-\tilde{x}\right\|_{\mathcal{X}} \text{ by Lemma A.1}\] \[\leq L\ \left(\sum_{k=0}^{\infty}\left\|A^{k}\right\|^{p}\right)^{ \frac{1}{p}}\left\|x-\tilde{x}\right\|_{\mathcal{X}}.\]

## Appendix B Standard Analysis of Follow-the-Regularized-Leader

In this section we state and prove some existing results about the follow-the-regularized-leader (FTRL) algorithm (Shalev-Shwartz and Singer, 2006; Abernethy et al., 2008). These results are well known in the literature, but we prove them here for completeness and use them in the remainder of the paper. We use the below results for functions \(\tilde{f}_{t}\) with Lipschitz constants \(\tilde{L}\). However, in this section we use a more general notation, denoting functions by \(g_{t}\) and their Lipschitz constant by \(L_{g}\).

Consider the following setup for an online convex optimization (OCO) problem. Let \(T\) denote the time horizon. Let the decision space \(\mathcal{X}\) be a closed, convex subset of a Hilbert space and \(g_{t}:\mathcal{X}\rightarrow\mathbb{R}\) be loss functions chosen by an oblivious adversary. The functions \(g_{t}\) are convex and \(L_{g}\)-Lipschitz continuous. The game between the learner and the adversary proceeds as follows. In each round \(t\in[T]\), the learner chooses \(x_{t}\in\mathcal{X}\) and the learner suffers loss \(g_{t}(x_{t})\). The goal of the learner is to minimize (static) regret,

\[R_{T}^{\text{static}}=\sum_{t=1}^{T}g_{t}(x_{t})-\min_{x\in\mathcal{X}}\sum_{ t=1}^{T}g_{t}(x).\] (3)

Let \(R:\mathcal{X}\rightarrow\mathbb{R}\) be an \(\alpha\)-strongly convex regularizer satisfying \(|R(x)-R(\tilde{x})|\leq D\) for all \(x,\tilde{x}\in\mathcal{X}\). The FTRL algorithm chooses iterates \(x_{t}\) as

\[x_{t}\in\operatorname*{arg\,min}_{x\in\mathcal{X}}\sum_{s=1}^{t-1}g_{s}(x)+ \frac{R(x)}{\eta},\] (4)

where \(\eta\) is a tunable parameter referred to as the step-size. In what follows, let \(g_{0}=\frac{R}{\eta}\). The analysis in this section closely follows Karlin (2017).

**Lemma B.1**.: _For all \(x\in\mathcal{X}\), FTRL (Eq. (4)) satisfies_

\[\sum_{t=0}^{T}g_{t}(x)\geq\sum_{t=0}^{T}g_{t}(x_{t+1}).\]

Proof.: We use proof by induction on \(T\). The base case is \(T=0\). By definition, \(x_{1}\in\operatorname*{arg\,min}_{x\in\mathcal{X}}R(x)\). Therefore, \(R(x)\geq R(x_{1})\) for all \(x\in\mathcal{X}\). Recalling the notation \(g_{0}=\frac{R}{\eta}\) proves the base case. Now, assume that the lemma is true for \(T-1\). That is,

\[\sum_{t=0}^{T-1}g_{t}(x)\geq\sum_{t=0}^{T-1}g_{t}(x_{t+1}).\]Let \(x\in\mathcal{X}\) be arbitrary. Since \(x_{T+1}\in\operatorname*{arg\,min}_{x\in\mathcal{X}}\sum_{t=0}^{T}g_{t}(x)\), we have

\[\sum_{t=0}^{T}g_{t}(x) \geq\sum_{t=0}^{T}g_{t}(x_{T+1})\] \[=\sum_{t=0}^{T-1}g_{t}(x_{T+1})+g_{T}(x_{T+1})\] \[\geq\sum_{t=0}^{T-1}g_{t}(x_{t+1})+g_{T}(x_{T+1})\] by inductive hypothesis \[=\sum_{t=0}^{T}g_{t}(x_{t+1}).\]

This completes the proof. 

**Lemma B.2**.: _For all \(x\in\mathcal{X}\), FTRL (Eq. (4)) satisfies_

\[\sum_{t=1}^{T}g_{t}(x_{t})-\sum_{t=1}^{T}g_{t}(x)\leq\frac{D}{\eta}+\sum_{t=1}^ {T}g_{t}(x_{t})-g_{t}(x_{t+1}).\]

Proof.: Note that

\[\sum_{t=1}^{T}g_{t}(x_{t})-\sum_{t=1}^{T}g_{t}(x)\leq\sum_{t=1}^{T}g_{t}(x_{t}) -\sum_{t=1}^{T}g_{t}(x)+g_{0}(x)-g_{0}(x_{1})\]

because \(x_{1}\in\operatorname*{arg\,min}_{x\in\mathcal{X}}g_{0}(x)\). The proof of this lemma now follows by using the above inequality, Lemma B.1, the definition \(g_{0}=\frac{R}{\eta}\), and the definition of \(D\). 

**Theorem B.1**.: _FTRL (Eq. (4)) satisfies_

\[\|x_{t+1}-x_{t}\|_{\mathcal{X}}\leq\eta\frac{L_{g}}{\alpha}\quad\text{ and }\quad R_{T}^{\text{static}}\leq\frac{D}{\eta}+\eta\frac{TL_{g}^{2}}{\alpha}.\]

_Choosing \(\eta=\sqrt{\frac{\alpha D}{TL_{g}^{2}}}\) yields_

\[R_{T}^{\text{static}}\leq O\left(\sqrt{\frac{D}{\alpha}TL_{g}^{2}}\right).\]

Proof.: Let \(x^{*}\in\operatorname*{arg\,min}_{x\in\mathcal{X}}\sum_{t=1}^{T}g_{t}(x)\). Using Lemma B.2 we have

\[\sum_{t=1}^{T}g_{t}(x_{t})-\sum_{t=1}^{T}g_{t}(x^{*})\leq\frac{D}{\eta}+\sum_ {t=1}^{T}g_{t}(x_{t})-g_{t}(x_{t+1}).\] (5)

We can bound the summands in the sum above as follows. Define \(G_{t}(x)=\sum_{s=0}^{t-1}g_{s}(x)\). Then, \(x_{t}\in\operatorname*{arg\,min}_{x\in\mathcal{X}}G_{t}(x)\). and \(x_{t+1}\in\operatorname*{arg\,min}_{x\in\mathcal{X}}G_{t+1}(x)\). Since \(\{g_{s}\}_{s=1}^{T}\) are convex, \(R\) is \(\alpha\)-strongly-convex, and \(g_{0}=\frac{R}{\eta}\), we have that \(G_{t}\) is \(\frac{\alpha}{\eta}\)-strongly-convex. So,

\[G_{t}(x_{t+1}) \geq G_{t}(x_{t})+\frac{\alpha}{2\eta}\|x_{t+1}-x_{t}\|_{\mathcal{ X}}^{2},\] \[G_{t+1}(x_{t}) \geq G_{t+1}(x_{t+1})+\frac{\alpha}{2\eta}\|x_{t+1}-x_{t}\|_{ \mathcal{X}}^{2}.\]

Adding the above two inequalities yields

\[g_{t}(x_{t})-g_{t}(x_{t+1})\geq\frac{\alpha}{\eta}\|x_{t+1}-x_{t}\|_{\mathcal{ X}}^{2}.\] (6)Since \(g_{t}\) are convex and \(L_{g}\)-Lipschitz continuous, we also have

\[g_{t}(x_{t})-g_{t}(x_{t+1})\leq L_{g}\|x_{t+1}-x_{t}\|_{\mathcal{X}}.\] (7)

Combining Eqs. (6) and (7) we have

\[\|x_{t+1}-x_{t}\|_{\mathcal{X}}\leq\eta\frac{L_{g}}{\alpha}.\]

This proves the first part of the theorem. Now, using this in Eq. (7) we have

\[g_{t}(x_{t})-g_{t}(x_{t+1})\leq\eta\frac{L_{g}^{2}}{\alpha}.\] (8)

Finally, substituting this in Eq. (5) proves the second part of the theorem. 

## Appendix C Regret Analysis: Upper Bounds

First we prove a lemma that bounds the difference in the value of \(f_{t}\) evaluated at the actual history \(h_{t}\) and an idealized history that would have been obtained by playing \(x_{t}\) in all prior rounds.

**Lemma C.1**.: _Consider an online convex optimization with unbounded memory problem specified by \((\mathcal{X},\mathcal{H},A,B)\). If the decisions \((x_{t})\) are generated by Algorithm 1, then_

\[\left|f_{t}(h_{t})-\tilde{f}_{t}(x_{t})\right|\leq\eta\frac{L\tilde{L}H_{1}}{\alpha}\]

_for all rounds \(t\). When \((\mathcal{X},\mathcal{H},A,B)\) follows linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm for \(p\geq 1\), then_

\[\left|f_{t}(h_{t})-\tilde{f}_{t}(x_{t})\right|\leq\eta\frac{L\tilde{L}H_{p}}{\alpha}\]

_for all rounds \(t\)._

Proof.: We have

\[\left|f_{t}(h_{t})-\tilde{f}_{t}(x_{t})\right| =\left|f_{t}(h_{t})-f_{t}\left(\sum_{k=0}^{t-1}A^{k}Bx_{t}\right)\right| \text{by Definition \ref{def:bound}}\] \[\leq L\left\|h_{t}-\sum_{k=0}^{t-1}A^{k}Bx_{t}\right\| \text{by Assumption \ref{def:bound}}\] \[=L\left\|\sum_{k=0}^{t-1}A^{k}Bx_{t-k}-\sum_{k=0}^{t-1}A^{k}Bx_{ t}\right\| \text{by definition of }h_{t}\] \[=L\underbrace{\left\|\sum_{k=0}^{t-1}A^{k}B(x_{t-k}-x_{t})\right\| }_{(a)}.\] (9)

First consider the general case where \((\mathcal{X},\mathcal{H},A,B)\) does not necessarily follow linear sequence dynamics. We can bound the term (a) as

\[\left\|\sum_{k=0}^{t-1}A^{k}B(x_{t-k}-x_{t})\right\| \leq\sum_{k=0}^{t-1}\left\|A^{k}B\right\|\left\|x_{t}-x_{t-k}\right\|\] \[\leq\sum_{k=0}^{t-1}\left\|A^{k}B\right\|k\eta\frac{\tilde{L}}{\alpha} \text{by Theorem B.1}\] \[\leq\sum_{k=0}^{t-1}\left\|A^{k}\right\|k\eta\frac{\tilde{L}}{\alpha} \text{by Assumption \ref{def:bound}}\] \[\leq\eta\frac{\tilde{L}}{\alpha}H_{1}.\]Plugging this into Eq. (9) completes the proof for the general case. Now consider the case when \((\mathcal{X},\mathcal{H},A,B)\) follows linear sequence dynamcis with the \(\xi\)-weighted \(p\)-norm. We can bound the term (a) as

\[\left\|\sum_{k=0}^{t-1}A^{k}B(x_{t-k}-x_{t})\right\| =\|(0,A_{0}(x_{t}-x_{t-1}),A_{1}A_{0}(x_{t}-x_{t-2}),\ldots)\| \quad\text{ by Definition \ref{lem:def_eq_eq_1}}\] \[=\left(\sum_{k=0}^{t-1}\xi_{k}^{p}\left\|A_{k-1}\cdots A_{0}(x_{t }-x_{t-k})\right\|^{p}\right)^{\frac{1}{p}} \text{ by Definition \ref{lem:def_eq_eq_1}}\] \[\leq\left(\sum_{k=0}^{t-1}\xi_{k}^{p}\left\|A_{k-1}\cdots A_{0} \right\|^{p}\left\|x_{t}-x_{t-k}\right\|^{p}\right)^{\frac{1}{p}}\] \[\leq\left(\sum_{k=0}^{t-1}\left\|A^{k}\right\|^{p}\left\|x_{t}-x _{t-k}\right\|^{p}\right)^{\frac{1}{p}} \text{ by Lemma A.1}\] \[\leq\eta\frac{\tilde{L}}{\alpha}\left(\sum_{k=0}^{t-1}\left\|A^{ k}\right\|^{p}k^{p}\right)^{\frac{1}{p}} \text{ by Theorem B.1}\] \[\leq\eta\frac{\tilde{L}}{\alpha}H_{p}.\]

Plugging this into Eq. (9) completes the proof. 

Now we restate and prove Theorem 3.1

**Theorem 3.1**.: _Consider an online convex optimization with unbounded memory problem specified by \((\mathcal{X},\mathcal{H},A,B)\). Let the regularizer \(R:\mathcal{X}\to\mathbb{R}\) be \(\alpha\)-strongly-convex and satisfy \(|R(x)-R(\tilde{x})|\leq D\) for all \(x,\tilde{x}\in\mathcal{X}\). Algorithm 1 with step-size \(\eta\) satisfies \(R_{T}(\texttt{FTRL})\leq\frac{D}{\eta}+\eta\frac{TL^{2}}{\alpha}+\eta\frac{ TLLH_{1}}{\alpha}\). If \(\eta=\sqrt{\frac{\alpha D}{TL(LH_{1}+\tilde{L})}}\), then_

\[R_{T}(\texttt{FTRL})\leq O\left(\sqrt{\frac{D}{\alpha}TL\tilde{L}H_{1}}\right).\]

_When \((\mathcal{X},\mathcal{H},A,B)\) follows linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm, then all of the above hold with \(H_{p}\) instead of \(H_{1}\)._

Proof.: First consider the general case where \((\mathcal{X},\mathcal{H},A,B)\) does not necessarily follow linear sequence dynamics. Let \(x^{*}\in\arg\min_{x\in\mathcal{X}}\sum_{t=1}^{T}\tilde{f}_{t}(x)\). Note that we can write the regret as

\[R_{T}(\texttt{FTRL}) =\sum_{t=1}^{T}f_{t}(h_{t})-\min_{x\in\mathcal{X}}\sum_{t=1}^{T} \tilde{f}_{t}(x)\] \[=\underbrace{\sum_{t=1}^{T}f_{t}(h_{t})-\tilde{f}_{t}(x_{t})}_{(a )}+\underbrace{\sum_{t=1}^{T}\tilde{f}_{t}(x_{t})-\tilde{f}_{t}(x^{*})}_{(b)}.\]

We can bound term (a) using Lemma C.1 and term (b) using Theorem B.1. Therefore, we have

\[R_{T}(\texttt{FTRL}) =\underbrace{\sum_{t=1}^{T}f_{t}(h_{t})-\tilde{f}_{t}(x_{t})}_{( a)}+\underbrace{\sum_{t=1}^{T}\tilde{f}_{t}(x_{t})-\tilde{f}_{t}(x^{*})}_{(b)}\] \[\leq\eta\frac{TL\tilde{L}H_{1}}{\alpha}+\frac{D}{\eta}+\eta\frac {T\tilde{L}^{2}}{\alpha}.\]Choosing \(\eta=\sqrt{\frac{\alpha D}{TL(LH_{1}+L)}}\) yields

\[R_{T}(\mathtt{FTRL})\leq O\left(\sqrt{\frac{D}{\alpha}TL\tilde{L}H_{1}}\right),\]

where we used the definition of \(p\)-effective memory capacity (Definition 2.4) and the bound on \(\tilde{L}\) (Theorem 2.1) to simplify the above expression. This completes the proof for the general case. The proof for when \((\mathcal{X},\mathcal{H},A,B)\) follows linear sequence dynamcis with the \(\xi\)-weighted \(p\)-norm is the same as above, except we bound the term (a) above using Lemma C.1 for linear sequence dynamics. 

Now we restate and prove Theorem 3.3.

**Theorem 3.3**.: _Consider an online convex optimization with finite memory problem with constant memory length \(m\) specified by \((\mathcal{X},\mathcal{H}=\mathcal{X}^{m},A_{\text{finite},m},B_{\text{finite},m})\). Let the regularizer \(R:\mathcal{X}\to\mathbb{R}\) be \(\alpha\)-strongly-convex and satisfy \(|R(x)-R(\tilde{x})|\leq D\) for all \(x,\tilde{x}\in\mathcal{X}.\) Algorithm 1 with step-size \(\eta=\sqrt{\frac{\alpha D}{TL(Lm^{\frac{3}{2}}+\tilde{L})}}\) satisfies_

\[R_{T}(\mathtt{FTRL})\leq O\left(\sqrt{\frac{D}{\alpha}TL\tilde{L}m^{\frac{3}{ 2}}}\right)\leq O\left(m\sqrt{\frac{D}{\alpha}TL^{2}}\right).\]

The OCO with finite memory problem, as defined in the literature, follows linear sequence dynamics with the \(2\)-norm. In this subsection we consider a more general version of the OCO with finite memory problem that follows linear sequence dynamics with the \(p\)-norm. We provide an upper bound on the policy regret for this more general formulation and the proof of Theorem 3.3 follows as a special case when \(p=2\).

**Theorem C.1**.: _Consider an online convex optimization with finite memory problem with constant memory length \(m\), \((\mathcal{X},\mathcal{H}=\mathcal{X}^{m},A_{\text{finite},m},B_{\text{finite},m})\). Assume that the problem follows linear sequence dynamics with the \(p\)-norm for \(p\geq 1\). Let the regularizer \(R:\mathcal{X}\to\mathbb{R}\) be \(\alpha\)-strongly-convex and satisfy \(|R(x)-R(\tilde{x})|\leq D\) for all \(x,\tilde{x}\in\mathcal{X}.\) Algorithm 1 with step-size \(\eta\) satisfies_

\[R_{T}(\mathtt{FTRL})\leq O\left(\sqrt{\frac{D}{\alpha}TL\tilde{L}m^{\frac{p+1 }{p}}}\right)\leq O\left(\sqrt{\frac{D}{\alpha}TL^{2}m^{\frac{p+2}{p}}}\right).\]

Proof.: Using Theorem 3.1 it suffices to bound \(\tilde{L}\) and \(H_{p}\) for this problem. Note that \(\|A_{\text{finite}}^{k}\|=1\) if \(k\leq m\) and \(0\) otherwise. Using this we have

\[H_{p}=\left(\sum_{k=0}^{\infty}\left(k\|A_{\text{finite}}^{k}\|\right)^{p} \right)^{\frac{1}{p}}=\left(\sum_{k=0}^{m}k^{p}\right)^{\frac{1}{p}}\leq O \left(m^{\frac{p+1}{p}}\right).\]

This proves the first inequality in the statement of the theorem. The second inequality follows from the above and Theorem 2.1, which states that

\[\tilde{L}\leq L\left(\sum_{k=0}^{\infty}\|A_{\text{finite}}^{k}\|^{p}\right) ^{\frac{1}{p}}=Lm^{\frac{1}{p}}.\qed\]

Finally, we provide an upper bound on the policy regret for the OCO with \(\rho\)-discounted infinite memory problem. For simplicity, we consider the case when the problem follows linear sequence dynamics with the \(2\)-norm instead of a general \(p\)-norm.

**Theorem C.2**.: _Consider an online convex optimization with \(\rho\)-discounted infinite memory problem \((\mathcal{X},\mathcal{H},A_{\text{infinite},\rho},B_{\text{infinite}})\). Suppose that the problem follows linear sequence dynamics with the \(2\)-norm. Let the regularizer \(R:\mathcal{X}\to\mathbb{R}\) be \(\alpha\)-strongly-convex and satisfy \(|R(x)-R(\tilde{x})|\leq D\) for all \(x,\tilde{x}\in\mathcal{X}.\) Algorithm 1 with step-size \(\eta\) satisfies_

\[R_{T}(\mathtt{FTRL})\leq O\left(\sqrt{\frac{D}{\alpha}TL\tilde{L}(1-\rho^{2})^ {-\frac{3}{2}}}\right)\leq O\left(\sqrt{\frac{D}{\alpha}TL^{2}(1-\rho^{2})^{- 2}}\right)\leq O\left(\sqrt{\frac{D}{\alpha}TL^{2}(1-\rho)^{-2}}\right).\]Proof.: Using Theorem 3.1, it suffices to bound \(\tilde{L}\) and \(H_{p}\) for this problem. Recall that \(\|A_{\text{infinite},\rho}^{k}\|=\rho^{k}\). Using this we have

\[H_{2}=\left(\sum_{k=0}^{\infty}\left(k\|A_{\text{finite}}^{k}\|\right)^{2} \right)^{\frac{1}{2}}=\left(\sum_{k=0}^{\infty}\left(k\rho^{k}\right)^{2} \right)^{\frac{1}{2}}\leq(1-\rho^{2})^{-\frac{3}{2}}.\]

This proves the first inequality in the statement of the theorem. The second inequality follows from the above and Theorem 2.1, which states that

\[\tilde{L}\leq L\left(\sum_{k=0}^{\infty}\|A_{\text{infinite},\rho}^{k}\|^{2} \right)^{\frac{1}{2}}=L(1-\rho^{2})^{-\frac{1}{2}}.\]

The last inequality follows because \(1-\rho^{2}=(1+\rho)(1-\rho)\), which implies that \(1-\rho\leq 1-\rho^{2}\leq 2(1-\rho)\) because \(\rho\in(0,1)\). 

### Existing Regret Bound for OCO with Finite Memory

In this subsection we provide a detailed comparison of our upper bound on the policy regret for OCO with finite memory with that of Anava et al. (2015). The material in this subsection comes from Appendix A.2 of their arXiv version or Appendix C.2 of their conference version.

The existing upper bound on regret is

\[O\left(\sqrt{DT\lambda m^{\frac{3}{2}}}\right),\]

where \(D=\max_{x,\tilde{x}\in\mathcal{X}}|R(x)-R(\tilde{x})|\). Although the parameter \(\lambda\) is defined in terms of dual norms of the gradient of \(\tilde{f}_{t}\), it is essentially the Lipschitz-continuity constant for \(\tilde{f}_{t}\): for all \(x,\tilde{x}\in\mathcal{X}\),

\[\left|\tilde{f}_{t}(x)-\tilde{f}_{t}(\tilde{x})\right|\leq\sqrt{\lambda\alpha }\|x-\tilde{x}\|,\]

where \(\alpha\) is the strong-convexity parameter of the regularizer \(R\) (or \(\sigma\) in the notation of Anava et al. (2015)). Therefore, the existing regret bound can be rewritten as

\[O\left(\tilde{L}\sqrt{\frac{D}{\alpha}Tm^{\frac{3}{2}}}\right).\]

Our upper bound on the policy regret for OCO with finite memory Theorem 3.3 is

\[O\left(\sqrt{\frac{D}{\alpha}L\tilde{L}Tm^{\frac{3}{2}}}\right).\]

Since \(\tilde{L}\leq\sqrt{m}L\) by Theorem 2.1, this leads to an improvement by a factor of \(m^{\frac{1}{4}}\).

## Appendix D Regret Analysis: Lower Bounds

We first restate Theorems 3.2 and 3.4.

**Theorem 3.2**.: _There exists an instance of the online convex optimization with unbounded memory problem, \((\mathcal{X},\mathcal{H},A,B)\), that follows linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm and there exist \(L\)-Lipschitz continuous loss functions \(\{f_{t}:\mathcal{H}\rightarrow\mathbb{R}\}_{t=1}^{T}\) such that the regret of any algorithm \(\mathcal{A}\) satisfies_

\[R_{T}(\mathcal{A})\geq\Omega\left(\sqrt{TL\tilde{L}H_{p}}\right).\]

**Theorem 3.4**.: _There exists an instance of the online convex optimization with finite memory problem with constant memory length \(m\), \((\mathcal{X},\mathcal{H}=\mathcal{X}^{m},A_{\text{finite},m},B_{\text{finite},m})\), and there exist \(L\)-Lipschitz continuous loss functions \(\{f_{t}:\mathcal{H}\rightarrow\mathbb{R}\}_{t=1}^{T}\) such that the regret of any algorithm \(\mathcal{A}\) satisfies_

\[R_{T}(\mathcal{A})\geq\Omega\left(m\sqrt{TL^{2}}\right).\]Theorem 3.2 follows from Theorem 3.4. However, the lower bound is true for a much broader class of problems as we show in this section. We first provide a lower bound for a more general formulation of the OCO with finite memory problem (Theorem D.1). The proof of Theorem 3.4 follows as a special case when \(p=2\). Then, we provide a lower bound for the OCO with \(\rho\)-discounted infinite memory problem (Theorem D.2).

The OCO with finite memory problem, as defined in the literature, follows linear sequence dynamics with the \(2\)-norm. In this section we consider a more general version of the OCO with finite memory problem that follows linear sequence dynamics with the \(p\)-norm. We provide a lower bound on the policy regret for this more general formulation and the proof of Theorem 3.4 follows as a special case when \(p=2\).

**Theorem D.1**.: _For all \(p\geq 1\), there exists an instance of the online convex optimization with finite memory problem with constant memory length \(m\), \((\mathcal{X},\mathcal{H}=\mathcal{X}^{m},A_{\text{finite},m},B_{\text{finite},m})\), that follows linear sequence dynamics with the \(p\)-norm, and there exist \(L\)-Lipschitz continuous loss functions \(\{f_{t}:\mathcal{H}\rightarrow\mathbb{R}\}_{t=1}^{T}\) such that the regret of any algorithm \(\mathcal{A}\) satisfies_

\[R_{T}(\mathcal{A})\geq\Omega\left(\sqrt{TL^{2}m^{\frac{p+2}{p}}}\right).\]

Proof.: Let \(\mathcal{X}=[-1,1]\) and consider an OCO with finite memory problem with constant memory length \(m\), \((\mathcal{X},\mathcal{H}=\mathcal{X}^{m},A_{\text{finite},m},B_{\text{finite},m})\), that follows linear sequence dynamics with the \(p\)-norm. For simplicity, assume that \(T\) is a multiple of \(m\) (otherwise, the same proof works but with slightly more tedious bookkeeping) and that \(L=1\) (otherwise, multiply the functions \(f_{t}\) defined below by \(L\)).

Divide the \(T\) rounds into \(N=\frac{T}{m}\) blocks of \(m\) rounds each. Sample \(N\) independent Rademacher random variables \(\{\epsilon_{1},\ldots,\epsilon_{N}\}\), where each \(\epsilon_{i}\) is equal to \(\pm 1\) with probability \(\frac{1}{2}\). Recall that \(h_{t}=(x_{t},\ldots,x_{t-m+1})\). Define the loss functions \(\{f_{t}\}_{t=1}^{T}\) as follows. (See Fig. 2 for an illustration.) If \(t\leq m\), let \(f_{t}=0\). Otherwise, let

\[f_{t}(h_{t}) =\epsilon_{\lceil\frac{t}{m}\rceil}m^{\frac{1-p}{p}}\sum_{k=0}^{m -1-(t-m\lfloor\frac{t}{m}\rfloor-1)}x_{m\lfloor\frac{t}{m}\rfloor+1-k}\] \[=\epsilon_{\lceil\frac{t}{m}\rceil}m^{\frac{1-p}{p}}\left(x_{t-m +1}+\cdots+x_{m\lfloor\frac{t}{m}\rfloor+1}\right).\]

In words, the loss in the first \(m\) rounds is equal to \(0\). Thereafter, in round \(t\) the loss is equal to a random sign \(\epsilon_{\lceil\frac{t}{m}\rceil}\), which is _fixed for that block_, times a scaling factor, which is chosen according to the \(p\)-norm to ensure that the Lipschitz constant \(L\) is at most \(1\), times a sum of a _subset_ of past decisions in the history \(h_{t}=(x_{t},\ldots,x_{t-m+1})\). This subset consists of all past decisions until and including the first decision of the current block, which is the decision in round \(m\lfloor\frac{t}{m}\rfloor+1\).

The functions \(f_{t}\) are linear, so they are convex. In order to show that they satisfy Assumptions **A3** and **A4**, it remains to show that they are \(1\)-Lipschitz continuous. Let \(h=(x^{(1)},\ldots,x^{(m)})\) and

Figure 2: An illustration of the loss functions \(f_{t}\) for the OCO with finite memory lower bound. Suppose \(T=12,m=3,L=1\), and \(p=2\). Time is divided into blocks of size \(m=3\). Consider round \(t=5\). The history is \(h_{5}=(x_{3},x_{4},x_{5})\). The loss function \(f_{5}(h_{5})\) is a product of three terms: a random sign \(\epsilon_{2}\) sampled for the block that round \(5\) belongs to, namely, block \(2\); a scaling factor of \(m^{-\frac{1}{2}}\); a sum over the decisions in the history excluding those that were chosen after observing \(\epsilon_{2}\), i.e., a sum over \(x_{3}\) and \(x_{4}\), excluding \(x_{5}\).

\(\tilde{h}=(\tilde{x}^{(1)},\ldots,\tilde{x}^{(m)})\) be arbitrary elements of \(\mathcal{H}=\mathcal{X}^{m}\). We have

\[\left|f_{t}(h)-f_{t}(\tilde{h})\right|\] \[\leq\left|\epsilon_{\lceil\frac{t}{m}\rceil}m^{\frac{1-p}{p}} \left((x^{(1)}-\tilde{x}^{(1)})+\cdots+(x^{(m)}-\tilde{x}^{(m)})\right)\right|\] \[\leq m^{\frac{1-p}{p}}\left|(x^{(1)}-\tilde{x}^{(1)})+\cdots+(x^{ (m)}-\tilde{x}^{(m)})\right|\] because

\[\epsilon_{\lceil\frac{t}{m}\rceil}\in\{-1,+1\}\] \[\leq m^{\frac{1-p}{p}}m^{1-\frac{1}{p}}\left(\sum_{k=1}^{m}\left| x^{(k)}-\tilde{x}^{(k)}\right|^{p}\right)^{\frac{1}{p}}\] by Holder's inequality \[=\|h-\tilde{h}\|_{\mathcal{H}},\]

where the last equality follows because of our assumption that the problem that follows linear sequence dynamics with the \(p\)-norm.

First we will show that the total expected loss of any algorithm is \(0\), where the expectation is with respect to the randomness in the choice of \(\{\epsilon_{1},\ldots,\epsilon_{N}\}\). The total loss in the first block is \(0\) because \(f_{t}=0\) for \(t\in[m]\). For each subsequent block \(n\in\{2,\ldots,N\}\), the total loss in block \(n\) depends on the algorithm's choices made _before_ observing \(\epsilon_{n}\), namely, \(\{x_{(n-2)m+2},\ldots,x_{(n-1)m+1}\}\). Since \(\epsilon_{n}\) is equal to \(\pm 1\) with probability \(\frac{1}{2}\), the expected loss of any algorithm in a block is equal to \(0\) and the total expected loss is also equal to \(0\).

Now we will show that the expected loss of the benchmark is at most

\[-O\left(\sqrt{Tm^{\frac{p+2}{p}}}\right),\]

where the expectation is with respect to the randomness in the choice of \(\{\epsilon_{1},\ldots,\epsilon_{N}\}\). We have

\[\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{t=1}^{T}\tilde{f}_{t }(x)\right] =\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{n=2}^{N}\sum_{t=(n-1 )m+1}^{nm}\tilde{f}_{t}(x)\right]\] \[=\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{n=2}^{N}\sum_{t=(n-1 )m+1}^{nm}\epsilon_{n}m^{\frac{1-p}{p}}\times x\times(m-(t-(n-1)m-1))\right].\]

The first equality follows from first summing over blocks and then summing over the rounds in that block. The second equality follows from the definitions of \(f_{t}\) above and of \(\tilde{f}_{t}\) (Definition 2.1). By the defintion of \(\tilde{f}_{t}\), the history \(h_{t}\) consists of \(m\) copies of \(x\) for \(t\geq m\). By the definition of \(f_{t}\), which sums over all past decisions until the first round of the current block, we have that within a block the sum first extends over \(m\) copies of \(x\) (in the first round of the block), then \(m-1\) copies of \(x\) (in the second round of the block), and so on until the last round of the block. So, we have

\[\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{t=1}^{T}\tilde{f}_{t }(x)\right] =\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{n=2}^{N}\sum_{t=(n-1 )m+1}^{nm}\epsilon_{n}m^{\frac{1-p}{p}}\times x\times(m-(t-(n-1)m-1))\right]\] \[=\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{n=2}^{N}\sum_{k=0}^{ m-1}\epsilon_{n}m^{\frac{1-p}{p}}\times x\times(m-k)\right]\] \[=m^{\frac{1-p}{p}}\frac{m^{2}+m}{2}\,\mathbb{E}\left[\min_{x\in \mathcal{X}}\sum_{n=2}^{N}\epsilon_{n}x\right]\] \[=m^{\frac{1-p}{p}}\frac{m^{2}+m}{2}\,\mathbb{E}\left[\min_{x\in \{-1,1\}}\sum_{n=2}^{N}\epsilon_{n}x\right]\] \[=m^{\frac{1-p}{p}}\frac{m^{2}+m}{2}\,\mathbb{E}\left[\frac{1}{2} \sum_{n=2}^{N}\epsilon_{n}(-1+1)-\frac{1}{2}\left|\sum_{n=2}^{N}\epsilon_{n}(- 1-1)\right|\right],\]where the second-last equality follows because the minima of a linear function over an interval is at one of the endpoints and the last equality follows because \(\min\{x,y\}=\frac{1}{2}(x+y)-\frac{1}{2}\left|x-y\right|\). Since \(\epsilon_{n}\) are Rademacher random variables equal to \(\pm 1\) with probability \(\frac{1}{2}\), we can simplify the above as

\[\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{t=1}^{T}\tilde{f}_{t} (x)\right] =m^{\frac{1-p}{p}}\frac{m^{2}+m}{2}\ \mathbb{E}\left[-\frac{1}{2}\left|\sum_{n=2}^{N}-2 \epsilon_{n}\right|\right]\] \[=m^{\frac{1-p}{p}}\frac{m^{2}+m}{2}\ \mathbb{E}\left[-\left|\sum_{n =2}^{N}\epsilon_{n}\right|\right]\] \[=-m^{\frac{1-p}{p}}\frac{m^{2}+m}{2}\mathbb{E}\left[\left|\sum_{ n=2}^{N}\epsilon_{n}\right|\right]\] \[\leq-m^{\frac{1-p}{p}}\frac{m^{2}+m}{2}\sqrt{N},\]

where the last inequality follows from Khintchine's inequality. Using the definition \(N=\frac{T}{m}\), we have

\[\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{t=1}^{T}\tilde{f}_{t} (x)\right] \leq-m^{\frac{1-p}{p}}\frac{m^{2}+m}{2}\sqrt{\frac{T}{m}}\] \[=-\frac{1}{2}\sqrt{T}\left(m^{\frac{3}{2}+\frac{1-p}{p}}+m^{ \frac{1}{2}+\frac{1-p}{p}}\right)\] \[\leq-O\left(\sqrt{T}m^{\frac{3}{2}+\frac{1-p}{p}}\right)\] \[=-O\left(\sqrt{T}m^{\frac{p+2}{p}}\right)\] \[=-O\left(\sqrt{T}m^{\frac{p+2}{p}}\right).\]

Therefore, we have

\[\mathbb{E}_{\epsilon_{1},\ldots,\epsilon_{N}}\left[R_{T}(\mathtt{ FTRL})\right]=\mathbb{E}\left[\sum_{t=1}^{T}f_{t}(h_{t})\right]-\mathbb{E} \left[\min_{x\in\mathcal{X}}\sum_{t=1}^{T}\tilde{f}_{t}(x)\right]\geq\Omega \left(\sqrt{Tm^{\frac{p+2}{p}}}\right).\]

This completes the proof. 

Now we provide a lower bound for the OCO with \(\rho\)-discounted infinite memory problem. For simplicity, we consider the case when the problem follows linear sequence dynamics with the \(2\)-norm instead of a general \(p\)-norm.

**Theorem D.2**.: _Let \(\rho\in[\frac{1}{2},1)\). There exists an instance of the online convex optimization with \(\rho\)-discounted infinite memory problem, \((\mathcal{X},\mathcal{H},A_{\text{infinite},\rho},B_{\text{infinite}})\), that follows linear sequence dynamics with the \(2\)-norm and there exist \(L\)-Lipschitz continuous loss functions \(\{f_{t}:\mathcal{H}\rightarrow\mathbb{R}\}_{t=1}^{T}\) such that the regret of any algorithm \(\mathcal{A}\) satisfies_

\[R_{T}(\mathcal{A})\geq\Omega\left(\sqrt{TL^{2}(1-\rho)^{-2}}\right).\]

The proof is very similar to that of Theorem D.1 with slight adjustments to account for a \(\rho\)-discounted infinite memory instead of a finite memory of constant size \(m\).

Proof.: Let \(\mathcal{X}=[-1,1]\) and consider an OCO with infinite memory problem with discount factor \(\rho\), \((\mathcal{X},\mathcal{H},A_{\text{infinite},\rho},B_{\text{infinite}})\), that follows linear sequence dynamics with the \(2\)-norm. For simplicity, assume that \(T\) is a multiple of \((1-\rho)^{-1}\) (otherwise, the same proof works but with slightly more tedious bookkeeping) and that \(L=1\) (otherwise, multiply the functions \(f_{t}\) defined below by \(L\)).

Define \(m=(1-\rho)^{-1}\). Divide the \(T\) rounds into \(N=\frac{T}{m}\) blocks of \(m\) rounds each. Sample \(N\) independent Rademacher random variables \(\{\epsilon_{1},\ldots,\epsilon_{N}\}\), where each \(\epsilon_{i}\) is equal to \(\pm 1\) with probability \(\frac{1}{2}\). Recall that \(h_{t}=(x_{t},\rho x_{t-1},\dots,\rho^{t-1}x_{1},0,\dots)\). Define the loss functions \(\{f_{t}\}_{t=1}^{T}\) as follows. If \(t\leq m\), let \(f_{t}=0\). Otherwise, let

\[f_{t}(h_{t})=\epsilon_{\lceil\frac{t}{m}\rceil}m^{-\frac{1}{2}}\sum_{k=0}^{m-1} \rho^{k+t-m\lfloor\frac{t}{m}\rfloor-1}x_{m\lfloor\frac{t}{m}\rfloor+1-k}.\]

The functions \(f_{t}\) are linear, so they are convex. In order to show that they satisfy Assumptions **A3** and **A4**, it remains to show that they are \(1\)-Lipschitz continuous. Let \(h=(x^{(1)},\rho x^{(2)},\dots)\) and \(\tilde{h}=(\tilde{x}^{(1)},\rho\tilde{x}^{(2)},\dots)\) be arbitrary elements of \(\mathcal{H}\). We have

\[\left|f_{t}(h)-f_{t}(\tilde{h})\right|\] \[\leq\left|\epsilon_{\lceil\frac{t}{m}\rceil}m^{-\frac{1}{2}}\sum_ {k=1}^{m}\rho^{k-1}\left(x^{(k)}-\tilde{x}^{(k)}\right)\right|\] \[\leq m^{-\frac{1}{2}}\left|\sum_{k=1}^{m}\rho^{k-1}\left(x^{(k)}- \tilde{x}^{(k)}\right)\right|\] because

\[\epsilon_{\lceil\frac{t}{m}\rceil}\in\{-1,+1\}\] \[\leq m^{-\frac{1}{2}}m^{\frac{1}{2}}\left(\sum_{k=1}^{m}\rho^{2( k-1)}\left|x^{(k)}-\tilde{x}^{(k)}\right|^{2}\right)^{\frac{1}{2}}\] by Holder's inequality \[\leq\|h-\tilde{h}\|_{\mathcal{H}},\]

where the last equality follows because the follows linear sequence dynamics with the \(2\)-norm.

First we will show that the total expected loss of any algorithm is \(0\), where the expectation is with respect to the randomness in the choice of \(\{\epsilon_{1},\dots,\epsilon_{N}\}\). The total loss in the first block is \(0\) because \(f_{t}=0\) for \(t\in[m]\). For each subsequent block \(n\in\{2,\dots,N\}\), the total loss in block \(n\) depends on the algorithm's choices made _before_ observing \(\epsilon_{n}\), namely, \(\{x_{(n-2)m+2},\dots,x_{(n-1)m+1}\}\). Since \(\epsilon_{n}\) is equal to \(\pm 1\) with probability \(\frac{1}{2}\), the expected loss of any algorithm in a block is equal to \(0\) and the total expected loss is also equal to \(0\).

Now we will show that the expected loss of the benchmark is at most

\[-O\left(\sqrt{T(1-\rho)^{-2}}\right),\]

where the expectation is with respect to the randomness in the choice of \(\{\epsilon_{1},\dots,\epsilon_{N}\}\). We have

\[\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{t=1}^{T}\tilde{f}_{t }(x)\right] =\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{n=2}^{N}\sum_{t=(n-1) m+1}^{nm}\tilde{f}_{t}(x)\right]\] \[=\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{n=2}^{N}\sum_{t=(n-1 )m}^{nm}\epsilon_{n}m^{-\frac{1}{2}}\sum_{k=0}^{m-1}\rho^{k+t-(n-1)m-1}x\right]\] \[=m^{-\frac{1}{2}}\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{n=2} ^{N}\epsilon_{n}x\sum_{t=(n-1)m+1}^{nm}\rho^{t-(n-1)m-1}\sum_{k=0}^{m-1}\rho ^{k}\right]\] \[=m^{-\frac{1}{2}}\frac{1-\rho^{m}}{1-\rho}\mathbb{E}\left[\min_{x \in\mathcal{X}}\sum_{n=2}^{N}\epsilon_{n}x\sum_{t=(n-1)m+1}^{nm}\rho^{t-(n-1)m -1}\right]\] \[=m^{-\frac{1}{2}}\left(\frac{1-\rho^{m}}{1-\rho}\right)^{2} \underbrace{\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{n=2}^{N}\epsilon_{n}x \right]}_{(a)}.\]The term (a) above can be bounded above by \(-\sqrt{N}\) as in the proof of Theorem D.1 using Khintchine's inequality. Therefore, using that \(N=\frac{T}{m}\) and \(m=(1-\rho)^{-1}\) we have

\[\mathbb{E}\left[\min_{x\in\mathcal{X}}\sum_{t=1}^{T}\tilde{f}_{t}( x)\right] \leq-m^{-\frac{1}{2}}\left(\frac{1-\rho^{m}}{1-\rho}\right)^{2} \sqrt{N}\] \[\leq-(1-\rho)^{\frac{1}{2}}\left(\frac{1-\rho^{m}}{1-\rho} \right)^{2}\sqrt{T(1-\rho)}\] \[=-\sqrt{T}\frac{(1-\rho^{m})^{2}}{1-\rho}\] \[=-\sqrt{T(1-\rho)^{-2}}(1-\rho^{m})^{2}\] \[\leq-O\left(\sqrt{T(1-\rho)^{-2}}\right),\]

where the last inequality follows from the assumption that \(\rho\in[\frac{1}{2},1)\) and the following argument:

\[\rho^{m} =(1-(1-\rho))^{m}=(1-(1-\rho))^{\frac{1}{1-\rho}}\leq\frac{1}{e}\] \[\Rightarrow(1-\rho^{m}) \geq 1-\frac{1}{e}\] \[\Rightarrow(1-\rho^{m})^{2} \geq\left(1-\frac{1}{e}\right)^{2}\] \[\Rightarrow-(1-\rho^{m})^{2} \leq-\left(1-\frac{1}{e}\right)^{2}\]

This completes the proof. 

## Appendix E Online Linear Control

### Formulation as OCO with Unbounded Memory

Now we formulate the online linear control problem in our framework by defining the decision space \(\mathcal{X}\), the history space \(\mathcal{H}\), and the linear operators \(A:\mathcal{H}\rightarrow\mathcal{H}\) and \(B:\mathcal{W}\rightarrow\mathcal{H}\). Then, we define the functions \(f_{t}:\mathcal{H}\rightarrow\mathbb{R}\) in terms of \(c_{t}\) and finally, prove an upper bound on the policy regret. For notational convenience, let \((M^{[s]})\) and \((Y_{k})\) denote the sequences \((M^{[1]},M^{[2]},\dots)\) and \((Y_{0},Y_{1},\dots)\) respectively.

Recall that we fix \(K\in\mathcal{K}\) to be an arbitrary \((\kappa,\rho)\)-strongly stable linear controller and consider the disturbance-action controller policy class \(\mathcal{M}_{K}\) (Definition 4.1). For the rest of this paper let \(\widetilde{F}=F-GK\). The first step is a change of variables with respect to the control inputs from linear controllers to DACs and the second is a corresponding change of variables for the state. Define the decision space \(\mathcal{X}\) as

\[\mathcal{X} =\{M=(M^{[s]}):M^{[s]}\in\mathbb{R}^{d\times d},\|M^{[s]}\|_{2} \leq\kappa^{4}\rho^{s}\}\] (10)

with

\[\|M\|_{\mathcal{X}} =\sqrt{\sum_{s=1}^{\infty}\rho^{-s}\|M^{[s]}\|_{F}^{2}}.\] (11)

Define the history space \(\mathcal{H}\) to be the set consisting of sequences \(h=(Y_{k})\), where \(Y_{0}\in\mathcal{X}\) and \(Y_{k}=\widetilde{F}^{k-1}GX_{k}\) for \(X_{k}\in\mathcal{X},k\geq 1\) with

\[\|h\|_{\mathcal{H}} =\sqrt{\sum_{k=0}^{\infty}\xi_{k}^{2}\|Y_{k}\|_{\mathcal{X}}^{2}},\] (12)where the weights \((\xi_{k})\) are nonnegative real numbers defined as

\[\xi=(1,1,1,\rho^{-\frac{1}{2}},\rho^{-1},\rho^{-\frac{3}{2}},\dots).\] (13)

Define the linear operators \(A:\mathcal{H}\to\mathcal{H}\) and \(B:\mathcal{W}\to\mathcal{H}\) as

\[A((Y_{0},Y_{1},\dots))=(0,GY_{0},\widetilde{F}Y_{1},\widetilde{F}Y_{2},\dots) \quad\text{ and }\quad B(M)=(M,0,0,\dots).\]

Note that the problem follows linear sequence dynamics with the \(\xi\)-weighted \(2\)-norm (Definition 2.3), where \(\xi\) is defined above in Eq. (13). The weights in the weighted norms on \(\mathcal{X}\) and \(\mathcal{H}\) increase exponentially. However, the norms \(\|M^{[s]}\|_{F}^{2}\) and \(\|\widetilde{F}^{k-1}G\|_{F}^{2}\) decrease exponentially as well: by definition of \(M^{[s]}\) in Eq. (11) and the assumption on \(\widetilde{F}=F-GK\) for \(K\in\mathcal{K}\). Leveraging this exponential decrease in \(\|M^{[s]}\|_{F}^{2}\) and \(\|\widetilde{F}^{k-1}G\|_{F}^{2}\) to define exponentially increasing weights turns out to be crucial for deriving our regret bounds that are stronger than existing results. Furthermore, the choice to have \(\xi_{p}=1\) for \(p\in\{1,2\}\) in addition to \(p=0\) (as required by Definition 2.3) might seem like a small detail, but this also turns out to be crucial for avoiding unnecessary factors of \(\rho^{-1}\) in the regret bounds.

Recall that the loss functions in the online linear control problem are \(c_{t}(s_{t},u_{t})\), where \(s_{t}\) and \(u_{t}\) are the state and control at round \(t\). Now we will show how to construct the functions \(f_{t}:\mathcal{H}\to\mathbb{R}\) that correspond to \(c_{t}(s_{t},u_{t})\). By definition, given a sequence of decisions \((M_{0},\dots,M_{t})\), the history at the end of round \(t\) is given by

\[h_{t}=(M_{t},GM_{t-1},\widetilde{F}GM_{t-2},\dots,\widetilde{F}^{t-1}GM_{0},0, \dots).\]

A simple inductive argument shows that the state and control in round \(t\) can be written as

\[s_{t} =\widetilde{F}^{t}s_{0}+\sum_{k=0}^{t-1}\sum_{s=1}^{k+1} \widetilde{F}^{t-k-1}GM_{k}^{[s]}w_{k-s}+w_{t-1},\] (14) \[u_{t} =-Ks_{t}+\sum_{s=1}^{t+1}M_{t}^{[s]}w_{t-s}.\] (15)

Define the functions \(f_{t}:\mathcal{H}\to\mathbb{R}\) by \(f_{t}(h)=c_{t}(s,u)\), where \(s\) and \(u\) are the state and control determined by the history as above. Note that \(f_{t}\) is parameterized by the past disturbances. Since the state and control are linear functions of the history and \(c_{t}\) is convex, this implies that \(f_{t}\) is convex.

With the above formulation and the fact that the class of disturbance-action controllers is a superset of the class of \((\kappa,\rho)\)-strongly-stable linear controllers, we have that the policy regret for the online linear control problem is at most

\[\sum_{t=0}^{T-1}f_{t}(h_{t})-\min_{M\in\mathcal{X}}\sum_{t=0}^{T-1}\tilde{f}_ {t}(M).\]

This completes the specification of the online convex optimization with unbounded memory problem, \((\mathcal{X},\mathcal{H},A,B)\), corresponding to the online linear control problem. Using Algorithm 1 and Theorem 3.1 we can upper bound the above by

\[O\left(\sqrt{\frac{D}{\alpha}TL\tilde{L}H_{2}}\right),\]

where \(L\) is the Lipschitz constant of \(f_{t}\), \(\tilde{L}\) is the Lipschitz constant of \(\tilde{f}_{t}\), \(H_{2}\) is the \(2\)-effective memory capacity, and \(D=\max_{x,\tilde{x}\in\mathcal{X}}|R(x)-R(\tilde{x})|\) for an \(\alpha\)-strongly-convex regularizer \(R:\mathcal{X}\to\mathbb{R}\). In the next subsection we bound these quantities in terms of the problem parameters of the online linear control problem. We use \(O(\cdot)\) to hide absolute constants.

### Regret Analysis

We use the following standard facts about matrix norms.

**Lemma E.1**.: _Let \(M,N\in\mathbb{R}^{d\times d}\). Then,_1. \(\|M\|_{2}\leq\|M\|_{F}\leq\sqrt{d}\|M\|_{2}\).
2. \(\|MN\|_{F}\leq\|M\|_{2}\|N\|_{F}\).

Proof.: Part 1 can be found in, for example, Golub and Loan (1996, Section 2.3.2). Letting \(N_{j}\) denote the \(j\)-th column of \(N\), part 2 follows from

\[\|MN\|_{F}^{2}=\sum_{j=1}^{d}\|MN_{j}\|_{2}^{2}\leq\|M\|_{2}^{2}\sum_{j=1}^{d} \|N_{j}\|_{2}^{2}=\|M\|_{2}^{2}\|N\|_{F}^{2}.\]

This completes the proof. 

**Lemma E.2**.: _For \(s\geq 2\), the operator norm \(\|A^{s}\|\) is bounded above as_

\[\|A^{s}\|\leq O\left(\kappa^{4}\rho^{\frac{s}{2}}\right).\]

Proof.: Recall the definition of \(\mathcal{H}\) and \(\|\cdot\|_{\mathcal{H}}\) (Eq. (12)). Let

\[(Y_{0},Y_{1},\dots)=(Y_{0},GX_{1},\widetilde{F}GX_{2},\widetilde{F}^{2}GX_{3 },\dots)\]

be an element of \(\mathcal{H}\) with unit norm, i.e.,

\[\sqrt{\sum_{k=0}^{\infty}\xi_{k}^{2}\|Y_{k}\|_{\mathcal{X}}^{2}}=1,\]

where the weights \((\xi_{k})\) are defined in Eq. (13). Note that \(\xi_{p}=1\) for \(p=0,1\) and \(\xi_{p}^{2}=\rho^{-p+2}\) for \(p=2,3,\dots\). From the definition of the operator \(A\) and for \(s\geq 2\), we have

\[A^{s}((Y_{0},Y_{1},\dots))=(0,\dots,0,\widetilde{F}^{s-1}GY_{0},\widetilde{F}^ {s}GX_{1},\widetilde{F}^{s+1}GX_{2},\dots).\]

Now we bound \(\|A^{s}\|\) as follows. By definition of \(A^{s}\) and \(\|\cdot\|_{\mathcal{H}}\) (Eq. (12)), and part 2 of Lemma E.1, we have

\[\|A^{s}((Y_{0},Y_{1},\dots))\| =\sqrt{\rho^{-s+2}\|\widetilde{F}^{s-1}GY_{0}\|_{\mathcal{X}}^{2 }+\sum_{k=1}^{\infty}\rho^{-s-k+2}\|\widetilde{F}^{s+k-1}GX_{k}\|_{\mathcal{X }}^{2}}\] \[\leq\sqrt{\rho^{-s+2}\|\widetilde{F}^{s-1}G\|_{2}^{2}\|Y_{0}\|_{ \mathcal{X}}^{2}+\sum_{k=1}^{\infty}\rho^{-s-k+2}\|\widetilde{F}^{s-1}\|_{2}^ {2}\|\widetilde{F}^{2}\|\widetilde{F}^{k-1}GX_{k}\|_{\mathcal{X}}^{2}}\] \[\leq\rho^{-\frac{s}{2}}\|\widetilde{F}^{s-1}\|_{2}\sqrt{\rho^{2} \|G\|_{2}^{2}\|Y_{0}\|_{\mathcal{X}}^{2}+\sum_{k=1}^{\infty}\rho^{-k+2}\| \widetilde{F}\|_{2}^{2}\|\widetilde{F}^{k-1}GX_{k}\|_{\mathcal{X}}^{2}}\] \[=\rho^{-\frac{s}{2}}\|\widetilde{F}^{s-1}\|_{2}\sqrt{\rho^{2}\|G \|_{2}^{2}\|Y_{0}\|_{\mathcal{X}}^{2}+\sum_{k=1}^{\infty}\rho^{-k+2}\| \widetilde{F}\|_{2}^{2}\|Y_{k}\|_{\mathcal{X}}^{2}}.\]

Using our assumptions that \(\|G\|_{2}\leq\kappa\) and \(\|\widetilde{F}\|_{2}\leq\kappa^{2}\rho\), we have

\[\|A^{s}((Y_{0},Y_{1},\dots))\| \leq\rho^{-\frac{s}{2}}\|\widetilde{F}^{s-1}\|_{2}\sqrt{\rho^{2} \kappa^{2}\|Y_{0}\|_{\mathcal{X}}^{2}+\sum_{k=1}^{\infty}\rho^{-k+2}\kappa^{4 }\rho^{2}\|Y_{k}\|_{\mathcal{X}}^{2}}\] \[\leq\rho^{-\frac{s}{2}}\rho\kappa^{2}\|\widetilde{F}^{s-1}\|_{2} \sqrt{\|Y_{0}\|_{\mathcal{X}}^{2}+\sum_{k=1}^{\infty}\rho^{-k+2}\|Y_{k}\|_{ \mathcal{X}}^{2}}\] \[\leq\rho^{-\frac{s}{2}}\rho\kappa^{2}\kappa^{2}\rho^{s-1}\sqrt{\| Y_{0}\|_{\mathcal{X}}^{2}+\sum_{k=1}^{\infty}\rho^{-k+2}\|Y_{k}\|_{\mathcal{X}}^{2}}\] \[=\kappa^{4}\rho^{\frac{s}{2}}\sqrt{\|Y_{0}\|_{\mathcal{X}}^{2}+ \sum_{k=1}^{\infty}\rho^{-k+2}\|Y_{k}\|_{\mathcal{X}}^{2}}.\]Using \(\rho^{-1+2}=\rho<1\) for \(k=1\) in the above sum, the definition of \((\xi_{k})\), and our assumption that \((Y_{0},Y_{1}\dots)\) has unit norm, we have

\[\|A^{s}((Y_{0},Y_{1},\dots))\|\leq\kappa^{4}\rho^{\frac{5}{2}}\sqrt{\xi_{0}^{2} \|Y_{0}\|_{\mathcal{X}}^{2}+\sum_{k=1}^{\infty}\xi_{k}^{2}\|Y_{k}\|_{\mathcal{ X}}^{2}}=\kappa^{4}\rho^{\frac{5}{2}}.\]

This completes the proof. 

**Lemma E.3**.: _The \(2\)-effective memory capacity is bounded above as_

\[H_{2}\leq O\left(\kappa^{4}(1-\rho)^{-\frac{3}{2}}\right).\]

Proof.: Using Lemma E.2 to bound \(\|A^{k}\|\) for \(k\geq 2\), we have

\[H_{2}=\sqrt{\sum_{k=0}^{\infty}k^{2}\|A^{k}\|^{2}}\leq O\left(\sqrt{\sum_{k=2} ^{\infty}k^{2}\kappa^{8}\rho^{k}}\right)\leq O\left(\kappa^{4}(1-\rho)^{- \frac{3}{2}}\right).\qed\]

**Lemma E.4**.: _Suppose \(R:\mathcal{X}\to\mathbb{R}\) is defined by \(R(M)=\frac{1}{2}\|M\|_{\mathcal{X}}^{2}\). Then, it is \(1\)-strongly-convex and \(D=\max_{M,\widetilde{M}\in\mathcal{X}}|R(M)-R(\widetilde{M})|\leq d\kappa^{8} (1-\rho)^{-1}\)._

Proof.: Note that \(R\) is \(1\)-strongly-convex by definition. Using part 1 of Lemma E.1 and the definition of \(\mathcal{X}\) (Eq. (10)), we have for all \(M,\widetilde{M}\in\mathcal{X}\),

\[D =\max_{M,\widetilde{M}\in\mathcal{X}}|R(M)-R(\widetilde{M})|\] \[=\max_{M,\widetilde{M}\in\mathcal{X}}\left|\frac{1}{2}\|M\|_{ \mathcal{X}}^{2}-\frac{1}{2}\|\widetilde{M}\|_{\mathcal{X}}^{2}\right|\] \[\leq\max_{M\in\mathcal{X}}\|M\|_{\mathcal{X}}^{2}\] \[=\max_{M\in\mathcal{X}}\sum_{s=1}^{\infty}\rho^{-s}\|M^{[s]}\|_{F }^{2} \text{by Eq. }\eqref{eq:2.1}\] \[\leq\max_{M\in\mathcal{X}}\sum_{s=1}^{\infty}\rho^{-s}d\|M^{[s]} \|_{2}^{2} \text{by Lemma E.1}\] \[\leq\sum_{s=1}^{\infty}\rho^{-s}d\kappa^{8}\rho^{2s} \text{by Eq. }\eqref{eq:2.1}\] \[\leq d\kappa^{8}(1-\rho)^{-1}.\]

This completes the proof. 

**Lemma E.5**.: _We can bound the norm of the state and control at time \(t\) as_

\[\max\{\|s_{t}\|_{2},\|u_{t}\|_{2}\}\leq D_{\mathcal{X}}=O\left(W\kappa^{8}(1- \rho)^{-2}\right).\]Proof.: We can bound the norm of \(s_{t}\) and \(u_{t}\) using Eqs. (14) and (15) as

\[\|s_{t}\|_{2} \leq\left\|\widetilde{F}^{t}s_{0}+\sum_{k=0}^{t-1}\sum_{s=1}^{k+1} \widetilde{F}^{t-k-1}GM_{k}^{[s]}w_{k-s}+w_{t-1}\right\|_{2}\] \[\leq\kappa^{2}\rho^{t}+W+\sum_{k=0}^{t-1}\sum_{s=1}^{k+1}\kappa^{ 2}\rho^{t-k-1}\kappa\kappa^{4}\rho^{s}W\] \[\leq\kappa^{2}+W+W\kappa^{7}(1-\rho)^{-2}\] \[\leq O\left(W\kappa^{7}(1-\rho)^{-2}\right).\] \[\|u_{t}\|_{2} \leq\left\|Ks_{t}+\sum_{s=1}^{t+1}M_{t}^{[s]}w_{t-s}\right\|_{2}\] \[\leq O\left(W\kappa^{8}(1-\rho)^{-2}\right)+\sum_{s=1}^{t+1}W \kappa^{4}\rho^{s}\] \[\leq O\left(W\kappa^{8}(1-\rho)^{-2}\right).\]

Above, we used the assumptions that \(\kappa,W\geq 1\). This completes the proof. 

**Lemma E.6**.: _The Lipschitz constant of \(f_{t}\) can be bounded above as_

\[L\leq O\left(L_{0}D_{\mathcal{X}}W\kappa(1-\rho)^{-1}\right),\]

_where \(D_{\mathcal{X}}\) is defined in Lemma E.5._

Proof.: Let \((M_{0},\ldots,M_{t})\) and \((\widetilde{M}_{0},\ldots,\widetilde{M}_{t})\) be two sequences of decisions, where \(M_{k}\) and \(\widetilde{M}_{k}\in\mathcal{X}\). Let \(h_{t}\) and \(\tilde{h}_{t}\) be the corresponding histories, and \((s_{t},u_{t})\) and \((\tilde{s}_{t},\tilde{u}_{t})\) be the corresponding state-control pairs at the end of round \(t\). We have

\[\left|f_{t}(h_{t})-f_{t}(\tilde{h}_{t})\right| =|c_{t}(s_{t},u_{t})-c_{t}(\tilde{s}_{t},\tilde{u}_{t})|\] \[\leq L_{0}D_{\mathcal{X}}\max\{\left\|s_{t}-\tilde{s}_{t}\right\| _{2},\|u_{t}-\tilde{u}_{t}\|_{2}\},\]

where the last inequality follows from our assumptions about the functions \(c_{t}\) and Lemma E.5. It suffices to bound the two norms on the right-hand side in terms of \(\|h_{t}-\tilde{h}_{t}\|_{\mathcal{H}}\). For \(k=0,\ldots,t-1\)define \(\mathcal{Z}_{k}^{[s]}=\widetilde{F}^{t-k-1}G(M_{k}^{[s]}-\widetilde{M}_{k}^{[s]})\). Using Eq. (14), we have

\[\left\|s_{t}-\tilde{s}_{t}\right\|_{2} =\left\|\sum_{k=0}^{t-1}\sum_{s=1}^{k+1}Z_{k}^{[s]}w_{k-s}\right\| _{2}\] \[\leq\sum_{k=0}^{t-1}\sum_{s=1}^{k+1}\left\|Z_{k}^{[s]}w_{k-s} \right\|_{2}\] \[=\sum_{k=0}^{t-1}\sum_{s=1}^{k+1}\left\|\rho^{-\frac{s}{2}}Z_{k}^{ [s]}\rho^{\frac{s}{2}}w_{k-s}\right\|_{2}\] \[\leq\sum_{k=0}^{t-1}\sum_{s=1}^{k+1}\left\|\rho^{-\frac{s}{2}}Z_{k }^{[s]}\right\|_{2}\left\|\rho^{\frac{s}{2}}w_{k-s}\right\|_{2}\] \[=\sum_{k=0}^{t-1}\sum_{s=1}^{k+1}\xi_{1+t-1-k}\left\|\rho^{- \frac{s}{2}}Z_{k}^{[s]}\right\|_{2}\xi_{1+t-1-k}^{-1}\left\|\rho^{\frac{s}{2}} w_{k-s}\right\|_{2}\] \[\leq\sqrt{\sum_{k=0}^{t-1}\sum_{s=1}^{k+1}\xi_{t-k}^{2}\left\| \rho^{-\frac{s}{2}}Z_{k}^{[s]}\right\|_{2}^{2}}\sqrt{\sum_{k=0}^{t-1}\sum_{s=1 }^{k+1}\xi_{t-k}^{-2}\left\|\rho^{\frac{s}{2}}w_{k-s}\right\|_{2}^{2}}\] (16) \[=\underbrace{\sqrt{\sum_{k=0}^{t-1}\xi_{t-k}^{2}\sum_{s=1}^{k+1} \left\|\rho^{-\frac{s}{2}}Z_{k}^{[s]}\right\|_{2}^{2}}}_{(a)}\underbrace{ \sqrt{\sum_{k=0}^{t-1}\xi_{t-k}^{-2}\sum_{s=1}^{k+1}\left\|\rho^{\frac{s}{2}} w_{k-s}\right\|_{2}^{2}}}_{(b)},\]

where Eq. (16) follows from the Cauchy-Schwarz inequality. The specific choice of weighted norms on \(\mathcal{X}\) and \(\mathcal{H}\) allow us to bound the terms (a) and (b) in terms of \(\|h_{t}-\tilde{h}_{t}\|_{\mathcal{H}}\). We can bound the term (a) using the definition of \(Z_{k}^{[s]}\), \(\|\cdot\|_{\mathcal{X}}\), and \(\|\cdot\|_{\mathcal{H}}\) as

\[\sqrt{\sum_{k=0}^{t-1}\xi_{t-k}^{2}\sum_{s=1}^{k+1}\left\|\rho^{- \frac{s}{2}}Z_{k}^{[s]}\right\|_{2}^{2}} =\sqrt{\sum_{k=0}^{t-1}\xi_{t-k}^{2}\sum_{s=1}^{k+1}\rho^{-s} \left\|\widetilde{F}^{t-k-1}G(M_{k}^{[s]}-\widetilde{M}_{k}^{[s]})\right\|_{2 }^{2}}\] \[\leq\sqrt{\sum_{k=0}^{t-1}\xi_{t-k}^{2}\sum_{s=1}^{k+1}\rho^{-s} \left\|\widetilde{F}^{t-k-1}G(M_{k}^{[s]}-\widetilde{M}_{k}^{[s]})\right\|_{F} ^{2}}\] (17) \[\leq\|h_{t}-\tilde{h}_{t}\|_{\mathcal{H}},\] (18)

where Eq. (17) follows from part 1 of Lemma E.1 and Eq. (18) follows from the definitions of \(\|\cdot\|_{\mathcal{X}}\) and \(\|\cdot\|_{\mathcal{H}}\). Using \(\|w_{t}\|_{2}\leq W\) for all rounds \(t\), we can bound the term (b) as

\[\sqrt{\sum_{k=0}^{t-1}\xi_{t-k}^{-2}\sum_{s=1}^{k+1}\left\|\rho^{ \frac{s}{2}}w_{k-s}\right\|_{2}^{2}} \leq W\sqrt{\sum_{k=0}^{t-1}\xi_{t-k}^{-2}\sum_{s=1}^{k+1}\rho^{s}}\] \[\leq W\sqrt{\sum_{k=0}^{t-1}\xi_{t-k}^{-2}\frac{\rho(1-\rho^{k+1} )}{1-\rho}}\] \[\leq W(1-\rho)^{-1},\] (19)

where Eq. (19) follows from the definition of \((\xi_{k})\) (Eq. (13)). Substituting Eqs. (18) and (19) in Eq. (16), we have

\[\left\|s_{t}-\tilde{s}_{t}\right\|_{2}\leq W(1-\rho)^{-1}\|h_{t}- \tilde{h}_{t}\|_{\mathcal{H}}.\]Similarly,

\[\|u_{t}-\tilde{u}_{t}\| =\left\|K(s_{t}-\tilde{s}_{t})+\sum_{s=1}^{t+1}(M_{t}^{[s]}-\widetilde {M}_{t}^{[s]})w_{t-s}\right\|_{2}\] \[\leq O\left(W\kappa(1-\rho)^{-1}\left\|h_{t}-\tilde{h}_{t}\right\| _{\mathcal{H}}\right),\]

where the last inequality follows from our assumption that \(\|K\|_{2}\leq\kappa\) and the above inequality for \(\|s_{t}-\tilde{s}_{t}\|_{2}\). This completes the proof. 

**Lemma E.7**.: _The Lipschitz constant of \(\tilde{f}_{t}\) can be bounded above as_

\[\tilde{L}\leq O\left(L_{0}D_{\mathcal{X}}W\kappa^{5}(1-\rho)^{-\frac{3}{2}} \right),\]

_where \(D_{\mathcal{X}}\) is defined in Lemma E.5._

Proof.: Using Lemma E.2 that bounds \(\|A^{k}\|\), we have

\[\sqrt{\sum_{k=0}^{\infty}\left\|A^{k}\right\|^{2}}\leq O\left(\kappa^{4}(1- \rho)^{-\frac{1}{2}}\right).\]

Using Theorem 2.1 that bounds \(\tilde{L}\) in terms of \(L\) and the above, we have

\[\tilde{L}\leq O\left(L\kappa^{4}(1-\rho)^{-\frac{1}{2}}\right)\leq O\left(L_{ 0}D_{\mathcal{X}}W\kappa^{5}(1-\rho)^{-\frac{3}{2}}\right),\]

where the last inequality follows from Lemma E.6. 

Now we restate and prove Theorem 4.1.

**Theorem 4.1**.: _Consider the online linear control problem as defined in Section 4.1. Suppose the decisions in round \(t\) are chosen using Algorithm 1. Then, the upper bound on the policy regret is_

\[O\left(L_{0}W^{2}\sqrt{T}d^{\frac{1}{2}}\kappa^{17}(1-\rho)^{-4.5}\right).\] (2)

Proof.: Using Theorem 3.1 and the above lemmas, we can upper bound the policy regret of Algorithm 1 for the online linear control problem by

\[O\left(\sqrt{\frac{D}{\alpha}TL\tilde{L}H_{2}}\right)\] \[=O\left(\sqrt{d\kappa^{8}(1-\rho)^{-1}\ T\ \left(L_{0}W^{2}\kappa^{9}(1-\rho)^{-3}\right)^{2}\kappa^{4}(1-\rho)^{- \frac{1}{2}}\ \kappa^{4}(1-\rho)^{-\frac{3}{2}}}\right)\] \[=O\left(L_{0}W^{2}\sqrt{T}d^{\frac{1}{2}}\kappa^{17}(1-\rho)^{-4. 5}\right).\]

This completes the proof. 

### Existing Regret Bound

The upper bound on policy regret for the online linear control problem in existing work is given in Agarwal et al. (2019, Theorem 5.1). The theorem statement only shows the dependence on \(\tilde{L},W\), and \(T\). The dependence on \(d,\kappa\), and \(\rho\) can be found in the details of the proof. Below we give a detailed accounting of all of these terms in their regret bound.

To simplify notation let \(\gamma=1-\rho\). Agarwal et al. (2019) define

\[H=\frac{\kappa^{2}}{\gamma}\log(T)\quad\text{ and }\quad C=\frac{W(\kappa^{2}+H \kappa_{B}\kappa^{2}a)}{\gamma(1-\kappa^{2}(1-\gamma)^{H+1})}+\frac{\kappa_{ B}\kappa^{3}W}{\gamma}.\]

The value of \(a\) is not specified in Theorem 5.1. However, from Theorem 5.3 and the definition of \(\mathcal{M}\) in Algorithm 1 their paper, we can infer that \(a=\kappa_{B}\kappa^{3}\).

The final regret bound is obtained by summing Equations 5.1, 5.3, and 5.4. Given the definition of \(H\) above, we have that

\[(1-\gamma)^{H+1}\leq\exp(-\kappa^{2}\log T)=T^{-\kappa^{2}}.\]

So, the dominant term in the regret bound is Equation 5.4, which is

\[O\left(L_{0}WCd^{\frac{3}{2}}\kappa_{B}^{2}\kappa^{6}H^{2.5}\gamma^{-1}\sqrt{T }\right).\]

Substituting the values of \(H\) and \(C\) from above and collecting terms, we have that the upper bound on policy regret in existing work (Agarwal et al., 2019, Theorem 5.1) is

\[O\left(L_{0}Wd^{\frac{3}{2}}\sqrt{T}\log(T)^{2.5}\kappa_{B}^{2} \kappa^{11}\gamma^{-3.5}C\right)\] \[=O\left(L_{0}Wd^{\frac{3}{2}}\sqrt{T}\log(T)^{2.5}\kappa_{B}^{2} \kappa^{11}\gamma^{-3.5}\left(\frac{W(\kappa^{2}+H\kappa_{B}\kappa^{2}a)}{ \gamma(1-\kappa^{2}(1-\gamma)^{H+1})}+\frac{\kappa_{B}\kappa^{3}W}{\gamma} \right)\right)\] \[=O\left(L_{0}Wd^{\frac{3}{2}}\sqrt{T}\log(T)^{2.5}\kappa_{B}^{2} \kappa^{11}\gamma^{-3.5}\left(\frac{W\kappa^{2}}{\gamma(1-\kappa^{2}(1-\gamma )^{H+1})}+\frac{W\kappa_{B}^{2}\kappa^{7}\log(T)}{\gamma^{2}(1-\kappa^{2}(1- \gamma)^{H+1})}+\frac{\kappa_{B}\kappa^{3}W}{\gamma}\right)\right)\] \[=O\left(L_{0}W^{2}d^{\frac{3}{2}}\sqrt{T}\log(T)^{2.5}\kappa_{B}^{ 2}\kappa^{13}\gamma^{-4.5}(1-\kappa^{2}(1-\gamma)^{H+1})^{-1}\right)\] \[\quad+O\left(L_{0}W^{2}d^{\frac{3}{2}}\sqrt{T}\log(T)^{3.5}\kappa _{B}^{4}\kappa^{18}\gamma^{-5.5}(1-\kappa^{2}(1-\gamma)^{H+1})^{-1}\right)\] \[\quad+O\left(L_{0}W^{2}d^{\frac{3}{2}}\sqrt{T}\log(T)^{2.5}\kappa _{B}^{3}\kappa^{14}\gamma^{-4.5}\right)\] \[=O\left(L_{0}W^{2}d^{\frac{3}{2}}\sqrt{T}\log(T)^{3.5}\kappa_{B}^ {4}\kappa^{18}\gamma^{-5.5}\right).\]

Above we used that \(\lim_{T\to\infty}(1-\kappa^{2}(1-\gamma)^{H+1})^{-1}=1\) to simplify the expressions. Therefore, the upper bound on policy regret for the online linear control problem in existing work is

\[O\left(L_{0}W^{2}d^{\frac{3}{2}}\sqrt{T}\log(T)^{3.5}\kappa_{B}^{4}\kappa^{18 }\gamma^{-5.5}\right).\] (20)

## Appendix F Online Performance Prediction

Before formulating the online performative prediction problem in our OCO with unbounded memory framework, we state the definition of \(1\)-Wasserstein distance that we use in our regret analysis. Informally, the \(1\)-Wasserstein distance is a measure of the distance between two probability measures.

**Definition F.1** (1-Wasserstein Distance).: Let \((\mathcal{Z},d)\) be a metric space. Let \(\mathbb{P}(\mathcal{Z})\) denote the set of Radon probability measures \(\nu\) on \(\mathcal{Z}\) with finite first moment. That is, there exists \(z^{\prime}\in\mathcal{Z}\) such that \(\mathbb{E}_{z\sim\nu}[d(z,z^{\prime})]<\infty\). The \(1\)-Wasserstein distance between two probability measures \(\nu,\nu^{\prime}\in\mathbb{P}(\mathcal{Z})\) is defined as

\[W_{1}(\nu,\nu^{\prime})=\sup\{\mathbb{E}_{z\sim\nu}[f(z)]-\mathbb{E}_{z\sim \nu^{\prime}}[f(z)]\},\]

where the supremum is taken over all \(1\)-Lipschitz continuous functions \(f:\mathcal{Z}\to\mathbb{R}\).

### Formulation as OCO with Unbounded Memory

Now we formulate the online performative prediction problem in our framework by defining the decision space \(\mathcal{X}\), the history space \(\mathcal{H}\), and the linear operators \(A:\mathcal{H}\to\mathcal{H}\) and \(B:\mathcal{W}\to\mathcal{H}\). Then, we define the functions \(f_{t}:\mathcal{H}\to\mathbb{R}\) in terms of \(l_{t}\) and finally, prove an upper bound on the policy regret. For notational convenience, let \((y_{k})\) denote the sequence \((y_{0},y_{1},\dots)\).

Let \(\rho\in(0,1)\). Let the decision space \(\mathcal{X}\subseteq\mathbb{R}^{d}\) be closed and convex with \(\|\cdot\|_{\mathcal{X}}=\|\cdot\|_{2}\). Let the history space \(\mathcal{H}\) be the \(\ell^{1}\)-direct sum of countably infinte number of copies of \(\mathcal{X}\). Define the linear operators \(A:\mathcal{H}\to\mathcal{H}\) and \(B:\mathcal{X}\to\mathcal{H}\) as

\[A((y_{0},y_{1},\dots))=(0,\rho y_{0},\rho y_{1},\dots)\quad\text{ and }\quad B(x)=(x,0,\dots).\]

Note that the problem is an OCO with \(\rho\)-discounted infinite memory problem and follows linear sequence dynamics with the \(1\)-norm (Definition 2.3).

Given a sequence of decisions \((x_{k})_{k=1}^{t}\), the history is \(h_{t}=(x_{t},\rho x_{t-1},\dots,\rho^{t-1}x_{1},0,\dots)\) and the data distribution \(p_{t}=p_{t}(h_{t})\) satisfies:

\[z\sim p_{t}\text{ iff }z\sim\sum_{k=1}^{t-1}(1-\rho)\rho^{k-1}(\xi+Fx_{t-k})+ \rho^{t}p_{1}.\] (21)

This follows from the recursive definition of \(p_{t}\) and parametric assumption about \(\mathcal{D}(x)\). Define the functions \(f_{t}:\mathcal{H}\to[0,1]\) by

\[f_{t}(h_{t})=\mathbb{E}_{z\sim p_{t}}[l_{t}(x_{t},z)].\]

With the above formulation and definition of \(f_{t}\), the original goal of minimizing the difference between the algorithm's total loss and the total loss of the best fixed decision is equivalent to minimizing the policy regret,

\[\sum_{t=1}^{T}f_{t}(h_{t})-\min_{x\in\mathcal{X}}\sum_{t=1}^{T}\tilde{f}_{t}(x).\]

### Regret Analysis

**Lemma F.1**.: _The operator norm \(\|A^{s}\|\) is bounded above as_

\[\|A^{s}\|\leq O\left(\rho^{s}\right).\]

Proof.: Recall the definition of \(\mathcal{H}\) and \(\|\cdot\|_{\mathcal{H}}\). Let

\[(y_{0},y_{1},\dots)=(x_{0},\rho x_{1},\rho^{2}x_{2},\dots)\]

be an element of \(\mathcal{H}\) with unit norm, i.e.,

\[\sum_{k=0}^{\infty}\|y_{k}\|=1.\]

From the definition of the operator \(A\), we have

\[A^{s}((y_{0},y_{1},\dots))=(0,\dots,0,\rho^{s}x_{0},\rho^{s+1}x_{1},\dots).\]

Now we bound \(\|A^{s}\|\) as follows. By definition of \(A^{s}\) and \(\|\cdot\|_{\mathcal{H}}\), we have

\[\|A^{s}((y_{0},y_{1},\dots))\|=\sum_{k=0}^{\infty}\rho^{s+k}\|x_{k}\|=\rho^{s} \sum_{k=0}^{\infty}\rho^{k}\|x_{k}\|=\rho^{s}\sum_{k=0}^{\infty}\|y_{k}\|=\rho ^{s}.\qed\]

**Lemma F.2**.: _The \(1\)-effective memory capacity is bounded above as_

\[H_{2}\leq O\left((1-\rho)^{-2}\right).\]

Proof.: Using Lemma F.1 to bound \(\|A^{k}\|\), we have

\[H_{1}=\sum_{k=0}^{\infty}k\|A^{k}\|=\sum_{k=0}^{\infty}k\rho^{k}\leq O\left(( 1-\rho)^{-2}\right).\qed\]

**Lemma F.3**.: _Suppose \(R:\mathcal{X}\to\mathbb{R}\) is defined by \(R(x)=\frac{1}{2}\|x\|_{\mathcal{X}}^{2}\). Then, it is \(1\)-strongly-convex and \(D=\max_{x,\tilde{x}\in\mathcal{X}}|R(x)-R(\tilde{x})|\leq D_{\mathcal{X}}^{2}\)._

Proof.: Note that \(R\) is \(1\)-strongly-convex by definition. By the assumption that \(\|x\|_{\mathcal{X}}\leq D_{\mathcal{X}}\) for all \(x\in\mathcal{X}\), we have that \(D\leq D_{\mathcal{X}}^{2}\). 

**Lemma F.4**.: _The Lipschitz constant of \(f_{t}\) can be bounded above as_

\[L\leq O\left(L_{0}\frac{1-\rho}{\rho}\|F\|_{2}\right).\]Proof.: Let \((x_{1},\ldots,x_{t})\) and \((\tilde{x}_{1},\ldots,\tilde{x}_{t})\) be two sequences of decisions, where \(x_{k},\tilde{x}_{k}\in\mathcal{X}\). Let \(h_{t}\) and \(\tilde{h}_{t}\) be the corresponding histories, and \(p_{t}\) and \(\tilde{p}_{t}\) be the corresponding distributions at the end of round \(t\). We have

\[\left|f_{t}(h_{t})-f_{t}(\tilde{h}_{t})\right|\] \[=\left|\mathbb{E}_{z\sim p_{t}}\left[l_{t}(x_{t},z)\right]- \mathbb{E}_{z\sim\tilde{p}_{t}}\left[l_{t}(\tilde{x}_{t},z)\right]\right|\] \[=\left|\mathbb{E}_{z\sim p_{t}}\left[l_{t}(x_{t},z)\right]- \mathbb{E}_{z\sim p_{t}}\left[l_{t}(\tilde{x}_{t},z)\right]+\mathbb{E}_{z\sim p _{t}}\left[l_{t}(\tilde{x}_{t},z)\right]-\mathbb{E}_{z\sim\tilde{p}_{t}}\left[ l_{t}(\tilde{x}_{t},z)\right]\right|\] \[\leq L_{0}\|x_{t}-\tilde{x}_{t}\|_{2}+L_{0}W_{1}(p_{t},\tilde{p}_ {t}),\]

where the last inequality follows from the assumptions about the functions \(l_{t}\) and the definition of the Wasserstein distance \(W_{1}\). By definition of \(p_{t}\) (Eq. (21)), we have

\[W_{1}(p_{t},\tilde{p}_{t}) \leq\sum_{k=1}^{t-1}\frac{1-\rho}{\rho}\rho^{k}\|F\|_{2}\|x_{t-k} -\tilde{x}_{t-k}\|_{2}\] \[\leq\frac{1-\rho}{\rho}\|F\|_{2}\|h_{t}-\tilde{h}_{t}\|_{\mathcal{ H}},\]

where the last inequality follows from the definition of \(\|\cdot\|_{\mathcal{H}}\). Therefore, \(L\leq L_{0}\frac{1-\rho}{\rho}\|F\|_{2}\). 

**Lemma F.5**.: _The Lipschitz constant of \(f_{t}\) can be bounded above as_

\[\tilde{L}\leq O\left(L_{0}\frac{1}{\rho}\|F\|_{2}\right).\]

Proof.: Using Lemma F.1 that bounds \(\|A^{k}\|\), we have

\[\sum_{k=0}^{\infty}\|A^{k}\|=(1-\rho)^{-1}.\]

Using Theorem 2.1 that bounds \(\tilde{L}\) in terms of \(L\) and the above, we have

\[\tilde{L}\leq O\left(L(1-\rho)^{-1}\right)=O\left(L_{0}\frac{1}{\rho}\|F\|_{2 }\right),\]

where the last equality follows from Lemma F.4. 

Now we restate and prove Theorem 4.2.

**Theorem 4.2**.: _Consider the online performative prediction problem as defined in Section 4.2. Suppose the decisions in round \(t\) are chosen using Algorithm 1. Then, the upper bound on the policy regret is_

\[O\left(D_{X}L_{0}\sqrt{T}\|F\|_{2}(1-\rho)^{-\frac{1}{2}}\rho^{-1}\right).\]

Proof.: Using Theorem 3.1 and the above lemmas, we can upper bound the policy regret of Algorithm 1 for the online performative prediction problem by

\[O\left(\sqrt{\frac{D}{\alpha}TL\tilde{L}H_{1}}\right)=O\left(D_{X}L_{0}\|F\|_{ 2}(1-\rho)^{-\frac{1}{2}}\rho^{-1}\sqrt{T}\right).\]

This completes the proof. 

We note that the upper bound can be improved by defining a weighted norm on \(\mathcal{H}\) similar to the approach in Appendix E. However, here we present the looser anaysis for simplicity of exposition.

## Appendix G Implementation Details for Algorithm 1

In this section we discuss how to implement Algorithm 1 efficiently.

Dimensionality of \(\mathcal{X}\).First, note that the decisions \(x\in\mathcal{X}\) could be high-dimensional, e.g., an unbounded sequence of matrices as in the online linear control problem, but this is external to our framework and is application dependent. Our framework can be applied to \(\mathcal{X}\) or to a lower-dimensional decision space \(\mathcal{X}^{\prime}\). However, the choice of \(\mathcal{X}^{\prime}\) and analyzing the difference

\[\min_{x^{\prime}\in\mathcal{X}^{\prime}}\sum_{t=1}^{T}\tilde{f}_{t}(x^{\prime} )-\min_{x\in\mathcal{X}}\sum_{t=1}^{T}\tilde{f}_{t}(x)\]

is application dependent. For example, for the online linear control problem one could consider a restricted class of disturbance-action controllers that operate on a constant number of past disturbances as opposed to all the past disturbances, and then analyze the difference between these two policy classes. See, for example, Agarwal et al. (2019, Lemma 5.2).

Computational cost of each iteration of Algorithm 1.Now we discuss how to implement each iteration of Algorithm 1 efficiently. We are interested in the computational cost of computing the decision \(x_{t+1}\) as a function of \(t\). (Given the above discussion about the dimensionality of \(\mathcal{X}\), we ignore the fact that the dimensionality of the decisions themselves could depend on \(t\).) Therefore, for the purposes of this section we (i) use \(O(\cdot)\) notation to hide absolute constants and problem parameters excluding \(t\) and \(T\); (ii) invoke the operators \(A\) and \(B\) by calling oracles \(\mathcal{O}_{A}(\cdot)\) and \(\mathcal{O}_{B}(\cdot)\); and (iii) evaluate the functions \(f_{t}\) by calling oracles \(\mathcal{O}_{f}(t,\cdot)\). Recall from Assumption **A1** that we assume the learner knows the operators \(A\) and \(B\), and observes \(f_{t}\) at the end of each round \(t\). So, the oracles \(\mathcal{O}_{A},\mathcal{O}_{B}\), and \(\mathcal{O}_{f}\) are readily available.

Algorithm 1 chooses the decision \(x_{t+1}\) as

\[x_{t+1}\in\operatorname*{arg\,min}_{x\in\mathcal{X}}\sum_{s=1}^{t}\tilde{f}_{ s}(x)+\frac{R(x)}{\eta}=\operatorname*{arg\,min}_{x\in\mathcal{X}}\underbrace{ \sum_{s=1}^{t}f_{s}\left(\sum_{k=0}^{s-1}A^{k}Bx\right)}_{=F_{t}(x)}+\frac{R( x)}{\eta}.\]

Since \(F_{t}(x)\) is a sum of \(f_{1},\ldots,f_{t}\), evaluating \(F_{t}(x)\) requires \(\Theta(t)\) oracle calls to \(\mathcal{O}_{f}\). However, this issue is present in FTRL for OCO and OCO with finite memory as well and is not specific to our framework. To deal with this issue, one could consider mini-batching algorithms (Dekel et al., 2012, Altschuler and Talwar, 2018, Chen et al., 2020) such as Algorithm 2.

A naive implementation to evaluate \(F_{t}(x)\) could require \(O(t^{3})\) oracle calls to \(\mathcal{O}_{A}\): for each \(s\in[t]\), constructing the argument \(\sum_{k=0}^{s-1}A^{k}Bx\) for \(f_{s}\) could require \(k\) oracle calls to \(\mathcal{O}_{A}\) to compute \(A^{k}Bx\), for a total of \(O(s^{2})\) oracle calls. However, \(F_{t}(x)\) can be evaluated with just \(O(t)\) oracle calls to \(\mathcal{O}_{A}\) by constructing the arguments incrementally. For \(t\geq 0\), define \(\Gamma_{t}:\mathcal{X}\to\mathcal{H}\) as

\[\Gamma_{0}(x) =Bx\] \[\Gamma_{t}(x) =A\left(\Gamma_{t-1}(x)\right)\quad\text{ for }t\geq 1.\]

Note that \(\Gamma_{t}(Bx)=A^{t}Bx\). Also, for \(t\geq 1\), define \(\Phi_{t}:\mathcal{X}\to\mathcal{H}\) as

\[\Phi_{1}(x) =\Gamma_{0}(x)\] \[\Phi_{t}(x) =\Phi_{t-1}(x)+\Gamma_{t-1}(x)\quad\text{ for }t\geq 2.\]

Note that \(\Phi_{s}(x)=\sum_{k=0}^{s-1}A^{k}Bx\) is the argument for \(f_{s}\). These can be constructed incrementally as follows.

1. Construct \(\Gamma_{0}(x)\) using one oracle call to \(\mathcal{O}_{B}\).
2. For \(s=1\), 1. Construct \(\Phi_{1}(x)=\Gamma_{0}(x)\). 2. Construct \(\Gamma_{1}(x)\) from \(\Gamma_{0}(x)\) using one oracle call to \(\mathcal{O}_{A}\).
3. For \(s\geq 2\), 1. Construct \(\Phi_{s}(x)\) by adding \(\Phi_{s-1}(x)\) and \(\Gamma_{s-1}(x)\). This can be done in \(O(1)\) time. Recall from our earlier discussion that \(O(\cdot)\) hides absolute constants and problem parameters excluding \(t\) and \(T\). 2. Construct \(\Gamma_{s}(x)\) from \(\Gamma_{s-1}(x)\) using one oracle call to \(\mathcal{O}_{A}\).

By incrementally constructing \(\Phi_{s}(x)\) as above, we can evaluate \(F_{t}(x)\) in \(O(t)\) time with \(O(1)\) oracle calls to \(\mathcal{O}_{B}\), \(O(t)\) oracle calls to \(\mathcal{O}_{A}\), and \(O(t)\) oracle calls to \(\mathcal{O}_{f}\).

Memory usage of Algorithm 1.We end with a brief discussion of the memory usage of Algorithm 1. We are interested in the memory usage of computing the decision \(x_{t+1}\) as a function of \(t\). (Given the discussion about the dimensionality of \(\mathcal{X}\) at the start of this section, we ignore the fact that the dimensionality of the decisions themselves could depend on \(t\).) For each \(t\in[T]\), the memory usage could be as low as \(O(1)\) (if, for example, \(\mathcal{X}\subseteq\mathbb{R}^{d}\), and \(A,B\in\mathbb{R}^{d\times d}\), which implies that \(\Phi_{t}(x)\) is a \(d\)-dimensional vector) or as high as \(O(t)\) (if, for example, \(\Phi_{t}(x)\) is a \(t\)-length sequence of \(d\)-dimensional vectors). However, the memory usage is already \(\Omega(t)\) to store the functions \(f_{1},\ldots,f_{t}\). Therefore, Algorithm 1 only incurs a constant factor overhead.

## Appendix H An Algorithm with A Low Number of Switches: Mini-Batch FTRL

In this section we present an algorithm (Algorithm 2) for OCO with unbounded memory that provides the same upper bound on policy regret as Algorithm 1 while guaranteeing a small number of switches. Algorithm 2 combines FTRL on the functions \(\tilde{f}_{t}\) with a mini-batching approach. First, it divides rounds into batches of size \(S\), where \(S\) is a parameter. Second, at the start of batch \(b\in\{1,\ldots,\lceil T/S\rceil\}\), it performs FTRL on the functions \(\{g_{1},\ldots,g_{b}\}\), where \(g_{i}\) is the average of the functions \(\tilde{f}_{t}\) in batch \(i\). Then, it uses this decision for the entirety of the current batch. By design, Algorithm 2 switches decisions at most \(O(^{T/S})\) times. This algorithm is inspired by similar algorithms for online learning and OCO (Dekel et al., 2012; Altschuler and Talwar, 2018; Chen et al., 2020).

``` Input : Time horizon \(T\), step size \(\eta\), \(\alpha\)-strongly-convex regularizer \(R:\mathcal{X}\rightarrow\mathbb{R}\), batch size \(S\).
1 Initialize history \(h_{0}=0\).
2for\(t=1,2,\ldots,T\)do
3if\(t\mod S=1\)then
4 Let \(N_{t}=\{1,\ldots,\lceil\frac{t}{S}\rceil\}\) denote the number of batches so far.
5 For \(b\in N_{t}\), let \(T_{b}=\{(b-1)S+1,\ldots,bS\}\) denote the rounds in batch \(b\).
6 For \(b\in N_{t}\), let \(g_{b}=\frac{1}{S}\sum_{s\in T_{b}}\tilde{f}_{s}\). denote the average of the functions in batch \(b\).
7 Learner chooses \(x_{t}\in\arg\min_{x\in\mathcal{X}}\sum_{b\in N_{t}}g_{b}(x)+\frac{R(x)}{\eta}\).
8 end for
9else
10 Learner chooses \(x_{t}=x_{t-1}\).
11 end for
12 Set \(h_{t}=Ah_{t-1}+Bx_{t}\).
13 Learner suffers loss \(f_{t}(h_{t})\) and observes \(f_{t}\).
14 end for ```

**Algorithm 2**Mini-Batch FTRL

**Theorem H.1**.: _Consider an online convex optimization with unbounded memory problem specified by \((\mathcal{X},\mathcal{H},A,B)\). Let the regularizer \(R:\mathcal{X}\rightarrow\mathbb{R}\) be \(\alpha\)-strongly-convex and satisfy \(|R(x)-R(\tilde{x})|\leq D\) for all \(x,\tilde{x}\in\mathcal{X}\). Algorithm 2 with batch size \(S\) and step-size \(\eta\) satisfies_

\[R_{T}(\texttt{Mini-Batch FTRL})\leq\frac{SD}{\eta}+\eta\frac{T\tilde{L}^{2}}{ \alpha}+\eta\frac{TL\tilde{L}H_{1}}{S\alpha}.\]

_If \(\eta=\sqrt{\frac{\alpha SD}{TL\left(\frac{L\tilde{H}_{1}}{S}+L\right)}}\), then_

\[R_{T}(\texttt{Mini-Batch FTRL})\leq O\left(\sqrt{\frac{D}{\alpha}T\left(L \tilde{L}H_{1}+S\tilde{L}^{2}\right)}\right).\]

Setting the batch size to be \(S=\nicefrac{{LH_{1}}}{{\tilde{L}}}\) we obtain the same upper bound on policy regret as Algorithm 1 while guaranteeing that the decisions \(x_{t}\) switch at most \(\nicefrac{{TL}}{{LH_{1}}}\) times.

**Corollary H.1**.: _Consider an online convex optimization with unbounded memory problem specified by \((\mathcal{X},\mathcal{H},A,B)\). Let the regularizer \(R:\mathcal{X}\rightarrow\mathbb{R}\) be \(\alpha\)-strongly-convex and satisfy \(|R(x)-R(\tilde{x})|\leq D\)

[MISSING_PAGE_FAIL:38]

Since the same decision \(x_{n}\) is chosen in all rounds of batch \(n\), we can reindex and rewrite

\[\sum_{t\in T_{N}}\left\|\sum_{k=0}^{t-1}A^{k}Bx_{t-k}-\sum_{k=0}^{t- 1}A^{k}Bx_{t}\right\| \leq\sum_{t\in T_{N}}\sum_{k=0}^{t-1}\|A^{k}\|\|x_{t-k}-x_{t}\|\] \[\leq\sum_{o=0}^{S-1}\sum_{n=1}^{N-1}\sum_{s=1}^{S}\|A^{(N-n-1)S+s+ o}\|\|x_{N}-x_{n}\|\] \[\leq\eta\frac{\tilde{L}}{\alpha}\sum_{o=0}^{S-1}\sum_{n=1}^{N-1} \sum_{s=1}^{S}(N-n)\|A^{(N-n-1)S+s+o}\|\] \[=\eta\frac{\tilde{L}}{\alpha}\sum_{o=0}^{S-1}\sum_{n=1}^{N-1} \sum_{s=1}^{S}n\|A^{(n-1)S+s+o}\|,\]

where the last inequality follows from bounding the distance between decision in consecutive batches Theorem B.1 and the triangle inequality. Expanding the triple sum yields

\[\sum_{o=0}^{S-1}\sum_{n=1}^{N-1}\sum_{s=1}^{S}n\|A^{(n-1)S+s+o}\|\] \[\leq\|A\|+\dots+\|A^{S}\|+2\|A^{S+1}\|+\dots+2\|A^{2S}\|+3\|A^{2S +1}\|+\dots+3\|A^{3S}\|+\dots\] \[\quad+\|A^{2}\|+\dots+\|A^{S+1}\|+2\|A^{S+2}\|+\dots+2\|A^{2S+1} \|+3\|A^{2S+2}\|+\dots+3\|A^{3S+1}\|+\dots\] \[\quad\vdots\] \[\quad+\|A^{S}\|+\dots+\|A^{2S-1}\|+2\|A^{2S}\|+\dots+2\|A^{3S-1} \|+3\|A^{3S}\|+\dots+3\|A^{4S-1}\|+\dots,\]

where each line above corresponds to a value of \(o\in\{0,\dots,S-1\}\). Adding up these terms yields \(H_{1}\). This completes the proof. 

Note that Theorem H.1 only provides an upper bound on the policy regret for the general case. Unlike Algorithm 1, it is unclear how to obtain a stronger bound depending on \(H_{p}\) for the case of linear sequence dynamics with the \(\xi\)-weighted \(p\)-norm for \(p>1\). The above proof can be specialized for this special case, similar to the proofs of Theorem 2.1 and Lemma C.1, to obtain

\[\sum_{t\in T_{N}}\left\|\sum_{k=0}^{t-1}A^{k}Bx_{t-k}-\sum_{k=0}^{t-1}A^{k}Bx_ {t}\right\|\leq\eta\frac{\tilde{L}}{\alpha}\sum_{o=0}^{S-1}\left(\sum_{n=1}^{ N-1}\sum_{s=1}^{S}\left(n\|A^{(n-1)S+s+o}\|\right)^{p}\right)^{\frac{1}{p}}\]

and

\[\sum_{o=0}^{S-1}\left(\sum_{n=1}^{N-1}\sum_{s=1}^{S}\left(n\|A^{( n-1)S+s+o}\|\right)^{p}\right)^{\frac{1}{p}}\] \[\leq\left(\|A\|^{p}+\dots+\|A^{S}\|^{p}+2^{p}\|A^{S+1}\|^{p}+ \dots 2^{p}\|A^{2S}\|^{p}+3^{p}\|A^{2S+1}\|^{p}+\dots\right)^{\frac{1}{p}}\] \[\quad+\left(\|A^{2}\|^{p}+\dots+\|A^{S+1}\|^{p}+2^{p}\|A^{S+2}\|^ {p}+\dots 2^{p}\|A^{2S+1}\|^{p}+3^{p}\|A^{2S+2}\|^{p}+\dots\right)^{\frac{1}{p}}\] \[\quad\vdots\] \[\left(\|A^{S}\|^{p}+\dots+\|A^{2S-1}\|^{p}+2^{p}\|A^{2S}\|^{p}+ \dots 2^{p}\|A^{3S-1}\|^{p}+3^{p}\|A^{3S}\|^{p}+\dots\right)^{\frac{1}{p}}.\]

The above expression cannot be easily simplified to \(O(H_{p})\). However, for the special case of OCO with finite memory, which follows linear sequence dynamics with the \(2\)-norm, we can do so by leveraging the special structure of the linear operator \(A_{\text{finite},m}\).

**Theorem H.2**.: _Consider an online convex optimization with finite memory problem with constant memory length \(m\) specified by \((\mathcal{X},\mathcal{H}=\mathcal{X}^{m},A_{\text{finite},m},B_{\text{finite},m})\). Let the regularizer \(R:\mathcal{X}\to\mathbb{R}\) be \(\alpha\)-strongly-convex and satisfy \(|R(x)-R(\tilde{x})|\leq D\) for all \(x,\tilde{x}\in\mathcal{X}\). Algorithm 2 with batch size \(m\)_and step-size \(\eta=\sqrt{\frac{\alpha mD}{TL\left(Lm^{\frac{3}{2}}+L\right)}}\) satisfies_

\[R_{T}(\texttt{Nini-Batch FTRL})\leq O\left(\sqrt{\frac{D}{\alpha}TL\tilde{L}m^{ \frac{3}{2}}}\right)\leq O\left(m\sqrt{\frac{D}{\alpha}TL^{2}}\right).\]

_Furthermore, the decisions \(x_{t}\) switch at most \(\frac{T}{m}\) times._

Proof.: Given the proof of Theorem H.1 and the above discussion, it suffices to show that

\[\sum_{o=0}^{S-1}\left(\sum_{n=1}^{N-1}\sum_{s=1}^{S}\left(n\|A^{(n-1)S+s+o}\| \right)^{2}\right)^{\frac{1}{2}}\leq H_{2}=m^{\frac{3}{2}}.\]

Recall that \(\|A^{k}_{\text{finite}}\|=1\) if \(k\leq m\) and \(0\) otherwise. Using this and \(S=m\), we have that the above sum is at most \(\sqrt{m}+\sqrt{m-1}+\cdots+\sqrt{1}=O\left(m^{\frac{3}{2}}\right)\). This completes the proof. 

## Appendix I Experiments

In this section we present some simple simulation experiments.2

Footnote 2: https://github.com/raunakkmr/oco-with-memory-code.

Problem Setup.We consider the problem of online linear control with a constant input controller class \(\Pi=\{\pi_{u}:\pi(s)=u\in\mathcal{U}\}\). Let \(T\) denote the time horizon. Let \(\mathcal{S}=\mathbb{R}^{d}\) and \(\mathcal{U}=\{u\in\mathbb{R}^{d}:\|u\|_{2}\leq 1\}\) denote the state and control spaces. Let \(s_{t}\) and \(u_{t}\) denote the state and control at time \(t\) with \(s_{0}\) being the initial state. The system evolves according to linear dynamics \(s_{t+1}=Fs_{t}+Gu_{t}+w_{t}\), where \(F,G\in\mathbb{R}^{d\times d}\) are system matrices and \(w_{t}\in\mathbb{R}^{d}\) is a disturbance. The loss function in round \(t\) is simply \(c_{t}(s_{t},u_{t})=c_{t}(s_{t})=\sum_{j=1}^{d}s_{t,j}\), where \(s_{t,j}\) denotes the \(j\)-th coordinate of \(s_{t}\). The goal is to choose a sequence of control inputs \(u_{0},\ldots,u_{T-1}\in\mathcal{U}\) to minimize the regret

\[\sum_{t=0}^{T-1}c_{t}(s_{t},u_{t})-\min_{u\in\mathcal{U}}\sum_{t=0}^{T-1}c_{t} (s_{t}^{u},u),\]

where \(s_{t}^{u}\) denotes the state in round \(t\) upon choosing control input \(u\) in each round. Note that the state in round \(t\) can be written as

\[s_{t}=\sum_{k=1}^{t}F^{k}Gu_{t-k}+\sum_{k=1}^{t}F^{k}w_{t-k}.\]

Therefore, we can formulate this problem as an OCO with unbounded memory problem by setting \(\mathcal{X}=\mathcal{U},\mathcal{H}=\{y\in\mathbb{R}^{d}:y=\sum_{k=0}^{t}F^{k} Gu\text{ for some }u\in\mathcal{U}\text{ and }t\in\mathbb{N}\},A(h)=Fh,B(x)=Gx\), and \(f_{t}(h_{t})=c_{t}(\sum_{k=1}^{t}F^{k}Gu_{t-k}+\sum_{k=1}^{t}F^{k}w_{t-k})\). Note that \(\mathcal{H},A\), and \(B\) are all finite-dimensional.

Data.We set the time horizon \(T=750\) and dimension \(d=2\). We sample the disturbances \(\{w_{t}\}\) from a standard normal distribution. We set the system matrix \(G\) to be the identity and the system matrix \(F\) to be a diagonal plus upper triangular matrix with the diagonal entries equal to \(\rho\) and the upper triangular entries equal to \(\alpha\). We run simulations with various values of \(\rho\) and \(\alpha\).

Implementation.We use the cvxpy library (Diamond and Boyd, 2016, Agrawal et al., 2018) for implementing Algorithm 1. We use step-sizes according to Theorems 3.1 and 3.3. We run the experiments on a standard laptop.

Results.We compare the regret with respect to the optimal control input of OCO with unbounded memory and OCO with finite memory for various memory lengths \(m\) in Fig. 3 for \(\rho=0.90\) and Fig. 4 for \(\rho=0.95\). There are a few important takeaways.

1. OCO with unbounded memory either performs as well as or better than OCO with finite memory, and it does so at comparable computational cost (Appendix G). In fact, the regret curve for OCO with unbounded memory reaches an asymptote whereas this is not the case for OCO with finite memory for a variety of memory lengths.
2. Knowledge of the spectral radius of \(F\), \(\rho\), is not sufficient to tune the memory length \(m\) for OCO with finite memory. This is illustrated by comparing Figs. 2(a) to 2(d). Even though small memory lengths perform well when the upper triangular value is small, they perform poorly when the upper triangular value is large. In contrast, OCO with unbounded memory performs well in all cases.
3. For a fixed memory length, OCO with unbounded memory eventually performs better than OCO with finite memory. This is illustrated by comparing Figs. 2(a) to 2(d).
4. As we increase the memory length, the performance of OCO with finite memory eventually approaches that of OCO with unbounded memory. However, an advantage of OCO with unbounded memory is that it does not require tuning the memory length. For example, when \(\rho=0.90\) and the upper triangular entry of \(F=0.10\), OCO with finite memory with \(m=4\) performs comparably to \(m=8\) and \(m=16\) (Fig. 2(c)). However, when the upper triangular entry of \(F=0.12\), then it performs much worse (Fig. 2(d)). However, OCO with unbounded memory performs well in all cases without the need for tuning an additional hyperparameter in the form of memory length.

Figure 3: Regret plot for \(\rho=0.90\). The label OCO-UM refers to formulating the problem as an OCO with unbounded memory problem. The OCO-FM-m refers to formulating the problem as an OCO with finite memory problem with constant memory length \(m\). The titles of the plots indicate the values of the dimension, the diagonal entries of \(F\), and the upper triangular entries of \(F\).

Figure 4: Regret plot for \(\rho=0.95\). The label \(\mathtt{OCO}\)-\(\mathtt{UM}\) refers to formulating the problem as an OCO with unbounded memory problem. The \(\mathtt{OCO}\)-\(\mathtt{FM}\)-\(\mathtt{m}\) refers to formulating the problem as an OCO with finite memory problem with constant memory length \(m\). The titles of the plots indicate the values of the dimension, the diagonal entries of \(F\), and the upper triangular entries of \(F\).