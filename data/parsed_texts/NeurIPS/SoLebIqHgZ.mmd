# ARTree: A Deep Autoregressive Model for Phylogenetic Inference

 Tianyu Xie\({}^{\dagger}\), Cheng Zhang\({}^{\dagger,\ddagger,}\)

\({}^{\dagger}\) School of Mathematical Sciences, Peking University

\({}^{\ddagger}\) Center for Statistical Science, Peking University

tianyuxie@pku.edu.cn, chengzhang@math.pku.edu.cn

Corresponding author.

###### Abstract

Designing flexible probabilistic models over tree topologies is important for developing efficient phylogenetic inference methods. To do that, previous works often leverage the similarity of tree topologies via hand-engineered heuristic features which would require pre-sampled tree topologies and may suffer from limited approximation capability. In this paper, we propose a deep autoregressive model for phylogenetic inference based on graph neural networks (GNNs), called ARTree. By decomposing a tree topology into a sequence of leaf node addition operations and modeling the involved conditional distributions based on learnable topological features via GNNs, ARTree can provide a rich family of distributions over the entire tree topology space that have simple sampling algorithms and density estimation procedures, without using heuristic features. We demonstrate the effectiveness and efficiency of our method on a benchmark of challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems.

## 1 Introduction

Reconstructing the evolutionary relationships among species has been one of the central problems in computational biology, with a wide range of applications such as genomic epidemiology (Dudas et al., 2017; du Plessis et al., 2021; Attwood et al., 2022) and conservation genetics (DeSalle and Amato, 2004). Based on molecular sequence data (e.g. DNA, RNA, or protein sequences) of the observed species and a model of evolution, this has been formulated as a statistical inference problem on the hypotheses of shared history, i.e., _phylogenetic trees_, where maximum likelihood and Bayesian approaches are the most popular methods (Felsenstein, 1981; Yang and Rannala, 1997; Mau et al., 1999; Larget and Simon, 1999; Huelsenbeck et al., 2001). However, phylogenetic inference can be challenging due to the composite structure of tree space which contains both continuous and discrete components (e.g., the branch lengths and the tree topologies) and the large search space of tree topologies that explodes combinatorially as the number of species increases (Whidden and Matsen IV, 2015; Dinh et al., 2017).

Recently, several efforts have been made to improve the efficiency of phylogenetic inference algorithms by designing flexible probabilistic models over the tree topology space (Hohna and Drummond, 2012; Larget, 2013; Zhang and Matsen IV, 2018). One typical example is subsplit Bayesian networks (SBNs) (Zhang and Matsen IV, 2018), which is a powerful probabilistic graphical model that provides a flexible family of distributions over tree topologies. Given a sample of tree topologies (e.g., sampled tree topologies from an MCMC run), SBNs have proved effective for accurate tree topology density estimation that generalizes beyond observed samples by leveraging the similarity of hand-engineered subsplit structures among tree topologies. Moreover, SBNs also allow fast ancestral sampling andhence were later on integrated into a variational Bayesian phylogenetic inference (VBPI) framework to provide variational posteriors over tree topologies (Zhang & Matsen IV, 2019). However, due to the limited parent-child subsplit patterns in the observed samples, SBNs can not provide distributions whose support spans the entire tree topology space (Zhang & Matsen IV, 2022). Furthermore, when used as variational distributions over tree topologies in VBPI, SBNs often rely on subsplit support estimation for variational parameterization, which requires high-quality pre-sampled tree topologies that would become challenging to obtain when the posterior is diffuse.

While SBNs suffer from the aforementioned limitations due to their hand-engineered design, a number of deep learning methods have been proposed for probabilistic modeling of graphs (Jin et al., 2018; You et al., 2018; Cao & Kipf, 2018; Simonovsky & Komodakis, 2018). Instead of using hand-engineered features, these approaches use neural networks to define probabilistic models for the connections between graph nodes which allow for learnable distributions over graphs. Due to the flexibility of neural networks, the resulting models are capable of learning complex graph patterns automatically. Among these deep graph models, graph autoregressive models (You et al., 2018; Li et al., 2018; Liao et al., 2019; Dai et al., 2020; Shi et al., 2020) are designed to learn flexible graph distributions that also allow easy sampling procedures by sequentially adding nodes and edges. Therefore, they serve as an ideal substitution of SBNs for phylogenetic inference that can provide more expressive distributions over tree topologies.

In this paper, we propose a novel deep autoregressive model for phylogenetic inference, called ARTree, which allows for more flexible distributions over tree topologies without using heuristic features than SBNs. With a pre-selected order of leaf nodes (i.e., species or taxa), ARTree generates a tree topology by recursively adding new leaf nodes to the edges of the current tree topology, starting from a star-shaped tree topology with the first three leaf nodes (Figure 1). The edge to which a new leaf node connects is determined according to a conditional distribution based on learnable topological features of the current tree topology via GNNs (Zhang, 2023). This way, probability distributions provided by ARTree all have full support that spans the entire tree topology space. Unlike SBNs, ARTree can be readily used in VBPI without requiring subsplit support estimation for parameterization. In experiments, we show that ARTree outperforms SBNs on a benchmark of challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems.

## 2 Background

Phylogenetic likelihoodsA phylogenetic tree is commonly described by a bifurcating tree topology \(\tau\) and the associated non-negative branch lengths \(\bm{q}\). The tree topology \(\tau\) represents the evolutionary relationship of the species and the branch lengths \(\bm{q}\) quantify the evolutionary intensity along the edges of \(\tau\). The leaf nodes of \(\tau\) correspond to the observed species and the internal nodes of \(\tau\) represent the unobserved ancestor species. A continuous time Markov model is often used to describe the transition probabilities of the characters along the edges of the tree (Felsenstein, 2004). Concretely, let \(\bm{Y}=\{Y_{1},\ldots,Y_{M}\}\in\Omega^{N\times M}\) be the observed sequences (with characters in \(\Omega\)) of length \(M\) over \(N\) species. Under the assumption that different sites evolve independently and identically, the likelihood of \(\bm{Y}\) given \(\tau,\bm{q}\) takes the form

\[p(\bm{Y}|\tau,\bm{q})=\prod_{i=1}^{M}p(Y_{i}|\tau,\bm{q})=\prod_{i=1}^{M} \sum_{a^{i}}\eta(a^{i}_{r})\prod_{(u,v)\in E(\tau)}P_{a^{i}_{u}a^{i}_{v}}(q_{ uv}),\] (1)

where \(a^{i}\) ranges over all extensions of \(Y_{i}\) to the internal nodes with \(a^{i}_{u}\) being the character assignment of node \(u\) (\(r\) represents the root node), \(E(\tau)\) is the set of edges of \(\tau\), \(q_{uv}\) is the branch length of the edge \((u,v)\in E(\tau)\), \(P_{jk}(q)\) is the transition probability from character \(j\) to \(k\) through a branch of length \(q\), and \(\eta\) is the stationary distribution of the Markov model.

Subsplit Bayesian networksLet \(\mathcal{X}\) be the set of leaf labels representing the existing species. A non-empty subset of \(\mathcal{X}\) is called a _clade_ and the set of all clades \(\mathcal{C}(\mathcal{X})\) is equipped with a total order \(\succ\) (e.g., lexicographical order). An ordered clade pair \((W,Z)\) satisfying \(W\cap Z=\emptyset\) and \(W\succ Z\) is called a _subsplit_. A _subsplit Bayesian network_ (SBN) is then defined as a Bayesian network whose nodes take subsplit values or singleton clade values that describe the local topological structures of tree topologies. For a rooted tree topology, one can find the corresponding node assignment of SBNsby following its splitting processes (Figure 4 in Appendix A). The SBN based probability of a rooted tree topology \(\tau\) then takes the following form

\[p_{\mathrm{sbn}}(T=\tau)=p(S_{1}=s_{1})\prod_{i>1}p(S_{i}=s_{i}|S_{\pi_{i}}=s_{ \pi_{i}}),\] (2)

where \(S_{i}\) denotes the subsplit- or singleton-clade-valued random variables at node \(i\) (node 1 is the root node), \(\pi_{i}\) is the index set of the parents of node \(i\) and \(\{s_{i}\}_{i\geq 1}\) is the corresponding node assignment. For unrooted tree topologies, we can also define their SBN based probabilities by viewing them as rooted tree topologies with unobserved roots and integrating out the positions of the root node as follows: \(p_{\mathrm{sbn}}(T^{u}=\tau)=\sum_{e\in E(\tau)}p_{\mathrm{sbn}}(\tau^{e})\), where \(\tau^{e}\) is the resulting rooted tree topology when the rooting position is on edge \(e\). In practice, the conditional probability tables (CPTs) of SBNs are often parameterized based on a sample of tree topologies (e.g., the observed data for density estimation (Zhang & Matsen IV, 2018) or fast bootstrap/MCMC samples (Minh et al., 2013; Zhang, 2020) for VBPI). As a result, the supports of SBN-induced distributions are often limited by the splitting patterns in the observed samples and could not span the entire tree topology space (Zhang & Matsen IV, 2022). More details on SBNs can be found in Appendix A.

Variational Bayesian phylogenetic inferenceGiven a prior distribution \(p(\tau,\bm{q})\), the phylogenetic posterior distribution takes the form

\[p(\tau,\bm{q}|\bm{Y})=\frac{p(\bm{Y}|\tau,\bm{q})p(\tau,\bm{q})}{p(\bm{Y})} \propto p(\bm{Y}|\tau,\bm{q})p(\tau,\bm{q}).\] (3)

Let \(Q_{\bm{\phi}}(\tau)\) and \(Q_{\bm{\psi}}(\bm{q}|\tau)\) be variational families over the spaces of tree topologies and branch lengths respectively. The VBPI approach uses \(Q_{\bm{\phi},\bm{\psi}}(\tau,\bm{q})=Q_{\bm{\phi}}(\tau)Q_{\bm{\psi}}(\bm{q}|\tau)\) to approximate the posterior \(p(\tau,\bm{q}|\bm{Y})\) by maximizing the following multi-sample (\(K>1\)) lower bound

\[L^{K}(\bm{\phi},\bm{\psi})=\mathbb{E}_{\{(\tau^{i},\bm{q}^{i})\}_{i=1}^{K}\, \downarrow,\downarrow,Q_{\bm{\phi},\bm{\psi}}}\log\left(\frac{1}{K}\sum_{i=1} ^{K}\frac{p(\bm{Y}|\tau^{i},\bm{q}^{i})p(\tau^{i},\bm{q}^{i})}{Q_{\bm{\phi}}( \tau^{i})Q_{\bm{\psi}}(\bm{q}^{i}|\tau^{i})}\right).\] (4)

The tree topology distribution \(Q_{\bm{\phi}}(\tau)\) is often SBNs which in this case rely on subsplit support estimation for parameterization that requires high-quality pre-sampled tree topologies and would become challenging for diffuse posteriors (Zhang & Matsen IV, 2022). The branch lengths distribution \(Q_{\bm{\psi}}(\bm{q}|\tau)\) can be diagonal lognormal distribution parametrized via heuristic features or learnable topological features of \(\tau\)(Zhang & Matsen IV, 2019; Zhang, 2020, 2023). Compared to the single-sample lower bound, the multi-sample lower bound in (4) allows efficient variance-reduced stochastic gradient estimators (e.g. VIMCO (Mnih & Rezende, 2016)) for tree topology variational parameters. Moreover, using multiple samples would encourage exploration over the vast tree topology space, albeit it may also deteriorates the training of the variational approximation (Rainforth et al., 2019). In practice, a moderate \(K\) is often suggested (Zhang & Matsen IV, 2022). See more details on VBPI in Appendix B.

Graph autoregressive modelsBy decomposing a graph as a sequence of components (nodes, edges, motifs, etc), graph autoregressive models generate the full graph by adding one component at a time, until some stopping criteria are satisfied (You et al., 2018; Jin et al., 2018; Liao et al., 2019). In previous works, recurrent neural networks (RNNs) for graphs are usually utilized to predict new graph components conditioned on the sub-graphs generated so far (You et al., 2018). The key of graph autoregressive models is to find a way to efficiently sequentialize graph structures, which is often domain-specific.

## 3 Proposed method

In this section, we propose ARTree, a deep autoregressive model for phylogenetic inference that can provide flexible distributions whose support spans the entire tree topology space and can be naturally parameterized without using heuristic approaches such as subsplit support estimation. We first describe a particular autoregressive generating process of phylogenetic tree topologies. We then develop powerful GNNs to parameterize learnable conditional distributions of this generating process. We consider unrooted tree topologies in this section, but the method developed here can be easily adapted to rooted tree topologies.

### A sequential generating process of tree topologies

To better illustrate our approach, we begin with some notations. Let \(\tau_{n}=(V_{n},E_{n})\) be a tree topology with \(n\) leaf nodes and \(V_{n},E_{n}\) are the sets of nodes and edges respectively. Note that \(|V_{n}|=2n-2\) and \(|E_{n}|=2n-3\) due to the unrooted and bifurcating structure of \(\tau_{n}\). The leaf nodes in \(V_{n}\) are treated as labeled nodes and the interior nodes in \(V_{n}\) are treated as unlabeled nodes. Let us assume a pre-selected order for the leaf nodes \(\mathcal{X}=\{x_{1},\ldots,x_{N}\}\), which is called taxa order for short by us. Now, consider a sequential generating process for all possible tree topologies that have leaf nodes \(\mathcal{X}\). We start with a definition below.

**Definition 1** (Ordinal Tree Topology).: _Let \(\mathcal{X}=\{x_{1},\ldots,x_{N}\}\) be a set of \(N(N\geq 3)\) leaf nodes. Let \(\tau_{n}=(V_{n},E_{n})\) be a tree topology with \(n(n\leq N)\) leaf nodes in \(\mathcal{X}\). We say \(\tau_{n}\) is an ordinal tree topology of rank \(n\), if its leaf nodes are the first \(n\) elements of \(\mathcal{X}\), i.e., \(V_{n}\cap\mathcal{X}=\{x_{1},\ldots,x_{n}\}\)._

We now describe a procedure that constructs ordinal tree topologies of rank \(N\) recursively by adding one leaf node at a time as follows. We first start from \(\tau_{3}\), the ordinal tree topology of rank \(3\), which is the smallest ordinal tree topology and is unique due to its unrooted and bifurcating structure. Suppose now we have an ordinal tree topology \(\tau_{n}=(V_{n},E_{n})\) of rank \(n\). To add the leaf node \(x_{n+1}\) to \(\tau_{n}\), we i) select an edge \(e_{n}=(u,v)\in E_{n}\) and remove it from \(E_{n}\); ii) add a new node \(w\) and two new edges \((u,w),(w,v)\) to the tree topology; iii) add the leaf node \(x_{n+1}\) and an edge \((w,x_{n+1})\) to the tree topology. This way, we obtain an ordinal tree topology \(\tau_{n+1}\) of rank \(n+1\). Intuitively, the leaf node \(x_{n+1}\) is added to the tree topology \(\tau_{n}\) by attaching it to an existing edge \(e_{n}\in E_{n}\). The position of the selected edge represents the evolutionary relationship between this new species and others. After performing this procedure for \(n=3,\ldots,N-1\), we finally obtain an ordinal tree topology \(\tau=\tau_{N}\) of rank \(N\). See Figure 1 for an illustration.

During the generating process described above, the selected edges at each time step form a sequence \(D=(e_{3},\ldots,e_{N-1})\). This sequence \(D\) of length \(N-3\) records all the decisions we have made for autoregressively generating a tree topology \(\tau\) and thus we call \(D\) a decision sequence. In fact, there is a one-to-one mapping between decision sequences and ordinal tree topologies of rank \(N\), which is formalized in Theorem 1. Note that a similar process is also used in online phylogenetic sequential Monte Carlo (OPSMC) (Dinh et al., 2016), where the leaf node addition operation is incorporated into the design of the proposal distributions.

**Theorem 1**.: _Let \(\mathcal{D}=\{D|D=(e_{3},\ldots,e_{N-1}),\ e_{n}\in E_{n},\forall\ 3\leq n \leq N-1\}\) be the set of all decision sequences of length \(N-3\) and \(\mathcal{T}\) be the set of all ordinal tree topologies of rank \(N\). Let the map_

\[\begin{array}{rccc}g:&\mathcal{D}&\rightarrow&\mathcal{T}\\ &D&\mapsto&\tau\end{array}\]

_be the generating process described above. Then \(g\) is a bijection between \(\mathcal{D}\) and \(\mathcal{T}\)._

According to Theorem 1, for each tree topology \(\tau\in\mathcal{T}\), there is a unique decision sequence given by \(g^{-1}(\tau)\). We call this process of finding the decision sequences of tree topologies the _decomposition

Figure 1: An overview of ARTree for autoregressive tree topology generation. The left plot is the starting ordinal tree topology of rank 3. This tree topology is then fed into GNNs which output a probability vector over edges. We then sample from the corresponding edge decision distribution and attach the next leaf node to the sampled edge. This process continues until an ordinal tree topology of rank \(N\) is reached.

process_. See more details on the decomposition process in Appendix C. The following lemma shows that one can find \(g^{-1}(\tau)\) in linear time.

**Lemma 1**.: _The time complexity of the decomposition process induced by \(g^{-1}(\cdot)\) is \(O(N)\)._

The proofs of Theorem 1 and Lemma 1 can be found in Appendix D. Based on the bijection \(g\) defined in Theorem 1, we can model the distribution \(Q(D)\) over the space of decision sequences \(\mathcal{D}\) instead of modeling the distribution \(Q(\tau)\) over \(\mathcal{T}\). Due to the sequential nature of \(D\), we can decompose \(Q(D)\) as the product of conditional distributions over the elements:

\[Q(D)=\prod_{n=3}^{N-1}Q(e_{n}|e_{3},\ldots,e_{n-1}).\] (5)

In what follows, we simplify \(Q(e_{n}|e_{3},\ldots,e_{n-1})\) as \(Q(e_{n}|e_{<n})\) and let \(e_{<3}\) be the empty set.

``` Input: a set \(\mathcal{X}=\{x_{1},\ldots,x_{N}\}\) of leaf nodes. Output: an ordinal tree topology \(\tau\) of rank \(N\); the ARTree probability \(Q(\tau)\) of \(\tau\). \(\tau_{3}=(V_{3},E_{3})\leftarrow\) the unique ordinal tree topology of rank \(3\); for\(n=3,\ldots,N-1\)do  Calculate the probability vector \(q_{n}\in\mathbb{R}^{|E_{n}|}\) using the current GNN model;  Sample an edge decision \(e_{n}\) from \(\operatorname{Discrete}\left(q_{n}\right)\) and assume \(e_{n}=(u,v)\);  Create a new node \(w\); \(E_{n+1}\leftarrow(E_{n}\backslash\{e_{n}\})\cup\{(u,w),(w,v),(w,x_{n+1})\}\); \(V_{n+1}\gets V_{n}\cup\{w,x_{n+1}\}\); \(\tau_{n+1}\leftarrow(V_{n+1},E_{n+1})\); end for \(\tau\leftarrow\tau_{N}\); \(Q(\tau)\gets q_{3}(e_{3})q_{4}(e_{4})\cdots q_{N-1}(e_{N-1})\). ```

**Algorithm 1**ARTree: An autoregressive model for phylogenetic tree topologies

### Graph neural networks for edge decision distribution

By Theorem 1, the sequence \(e_{<n}\) corresponds to a sequence of ordinal tree topologies of increasing ranks \((\tau_{3},\ldots,\tau_{n})\) (the empty set \(e_{<3}\) corresponds to the unique ordinal tree topology \(\tau_{3}\) of rank 3). Therefore, the discrete distribution \(Q(e_{n}|e_{<n})\) in equation (5) defines the probability of adding the leaf node \(x_{n+1}\) to the edge \(e_{n}\) of \(\tau_{n}\), conditioned on all the ordinal tree topologies \((\tau_{3},\ldots,\tau_{n})\) generated so far. In what follows, we will show step by step how to use graph neural networks (GNNs) to parameterize such a conditional distribution given tree topologies.

Topological node embeddingsAt the \(n\)-th time step of the generating process, we first find the node embeddings of the current tree topology \(\tau_{n}=(V_{n},E_{n})\), which is a set \(\{f_{n}(u)\in\mathbb{R}^{N}:u\in V_{n}\}\) that assigns each node with an encoding vector in \(\mathbb{R}^{N}\). Following Zhang (2023), we first assign one hot encoding to the leaf nodes, i.e.

\[\left[f_{n}(x_{i})\right]_{j}=\delta_{ij},1\leq i\leq n,1\leq j\leq N,\] (6)

where \(\delta\) is Kronecker delta function; we then get the embeddings for the interior nodes by minimizing the Dirichlet energy \(\ell(f_{n},\tau_{n}):=\sum_{(u,v)\in E_{n}}||f_{n}(u)-f_{n}(v)||^{2}\) using the efficient two-pass algorithm described in Zhang (2023). One should note that the embeddings for interior nodes may change as new leaf nodes are added to the ordinal tree topologies, which is a main difference between our model and other graph autoregressive models.

Message passing networksUsing these topological node embeddings as the initial node features, GNNs apply message passing steps to compute the representation vector of nodes that encode topological information of \(\tau_{n}\), where the node features are updated with the information from their neighborhoods in a convolutional manner Gilmer et al. (2017). More concretely, the \(l\)-th round of message passing is implemented by

\[m_{n}^{l}(u,v) =F_{\text{message}}^{l}(f_{n}^{l}(u),f_{n}^{l}(v)),\] (7a) \[f_{n}^{l+1}(v) =F_{\text{updating}}^{l}\left(\{m_{n}^{l}(u,v);u\in\mathcal{N}( v)\}\right),\] (7b)where \(F^{l}_{\text{message}}\) and \(F^{l}_{\text{updating}}\) are the message function and updating function in the \(l\)-th round, and \(\mathcal{N}(v)\) is the neighborhood of the node \(v\). In our implementations, the choices of \(F^{l}_{\text{message}}\) and \(F^{l}_{\text{updating}}\) follow the edge convolution operator (Wang et al., 2018), while other variants of GNNs can also be applied. The final node features of \(\tau_{n}\) are given by \(\{f^{L}_{n}(v):v\in V_{n}\}\) after \(L\) rounds of message passing.

Node hidden statesThe conditional distribution \(Q(\cdot|e_{<n})\) is highly complicated as it has to capture how \(x_{n+1}\) can be added to \(\tau_{n}\) based on how previous leaf nodes are added to form the tree topologies. A common approach is to use RNNs to model this complex distribution that strikes a good balance between expressiveness and scalability (You et al., 2018; Liao et al., 2019). In our model, after obtaining the final node features of \(\tau_{n}\), a gated recurrent unit (GRU) (Cho et al., 2014) follows, i.e.

\[h_{n}(v)=\mathrm{GRU}(h_{n-1}(v),f^{L}_{n}(v)),\] (8)

where \(h_{n}(v)\) is the hidden state of \(v\) at the \(n\)-th generation step and is initialized to zero for the newly added nodes including those in \(\tau_{3}\). The node hidden states \(\{h_{n}(v);v\in V_{n}\}\), therefore, contain the information of all the tree topologies generated so far which can be used for conditional distribution modeling.

Time guided readoutWe now construct the distribution \(Q(\cdot|e_{<n})\) over edge decisions based on the node hidden states. As mentioned before, a main difference between our model and other graph autoregressive models is that the node embedding \(f^{0}_{n}(v)\) of a node \(v\) may vary with the time step \(n\). We, therefore, incorporate time embeddings into the readout step which first forms the edge features \(r_{n}(e)\in\mathbb{R}\) of \(e=(u,v)\) using

\[p_{n}(e) =F_{\text{pooling}}\left(h_{n}(u)+b_{n},h_{n}(v)+b_{n}\right),\] (9a) \[r_{n}(e) =F_{\text{readout}}\left(p_{n}(e)+b_{n}\right),\] (9b)

where \(b_{n}\) is the sinusoidal positional embedding of time step \(n\) that is widely used in Transformers (Vaswani et al., 2017), \(F_{\text{pooling}}\) is the pooling function implemented as 2-layer MLPs followed by an elementwise maximum operator, and \(F_{\text{readout}}\) is the readout function implemented as 2-layer MLPs with a scalar output. Then the conditional distribution for edge decision is

\[Q(\cdot|e_{<n})\sim\mathrm{Discrete}\left(q_{n}\right),\quad q_{n}=\mathrm{ softmax}\left(\{r_{n}(e)\}_{e\in E_{n}}\right),\] (10)

where probability vector \(q_{n}\in\mathbb{R}^{|E_{n}|}\) for parametrizing \(Q(\cdot|e_{<n})\) is obtained by applying a softmax function to all the time guided edge features in equation (9b).

As the last step, we sample an edge \(e_{n}\in E_{n}\) from the discrete distribution in equation (10) and add the leaf node \(x_{n+1}\) to \(e_{n}\) as described in Section 3.1. This way, we update the ordinal tree topology from \(\tau_{n}\) of rank \(n\) to \(\tau_{n+1}\) of rank \(n+1\). We can repeat this procedure until an ordinal tree topology \(\tau=\tau_{N}\) of rank \(N\) is reached. The probability of \(\tau\) then takes the form

\[Q_{\bm{\phi}}(\tau)=Q_{\bm{\phi}}(D)=\prod_{n=3}^{N-1}Q_{\bm{\phi}}(e_{n}|e_{< n}),\] (11)

where \(D\) is the decision sequence and \(\bm{\phi}\) are the learnable parameters in the model. We call this autoregressive model for tree topologies ARTree, and summarize it in Algorithm 1. Note that equation (11) can also be used for tree topology probability evaluation where the decision sequence \(D=g^{-1}(\tau)\) is obtained from the decomposition process (Appendix C) that enjoys a linear time complexity (Lemma 1). Compared to SBNs, ARTree does not rely on heuristic features for parameterization and can provide distributions whose support spans the entire tree topology space as all possible decisions would have nonzero probabilities due to the softmax parameterization in equation (10). Although different taxa orders may affect the performance of ARTree, we find this effect is negligible in our experiments.

There exist other VI approaches for phylogenetic inference that also use unconfined models over the space of tree topologies. Moretti et al. (2021) proposed to sample tree topologies through subtree merging and resampling following CSMC (Wang et al., 2015), but employed a parametrized proposal distribution. Koptagel et al. (2022) proposed a novel multifurcating tree topology sampler named SLANTIS, which makes decisions on adding edges in a specific order based on a simply parameterized weight matrix and maximum spanning trees, and is integrated with CSMC to sample bifurcating tree topologies (\(\phi\)-CSMC). Unlike these methods, ARTree employs GNNs for an autoregressive model that builds up the tree topology sequentially through leaf node addition operations, which not only allows fast sampling of trees, but also provides straightforward density estimation procedures. We demonstrate the advantage of ARTree over these baselines in the experiments.

## 4 Experiments

In this section, we test the effectiveness and efficiency of ARTree for phylogenetic inference on two benchmark tasks: tree topology density estimation (TDE) and variational Bayesian phylogenetic inference (VBPI). In all experiments, we report the inclusive KL divergence from posterior estimates to the ground truth to measure the approximation error of different methods. We will use "KL divergence" for inclusive KL divergence throughout this section unless otherwise specified. The code is available at https://github.com/tyuxie/ARTree.

Experimental setupWe perform experiments on eight data sets which we will call DS1-8. These data sets, consisting of sequences from 27 to 64 eukaryote species with 378 to 2520 site observations, are commonly used to benchmark phylogenetic MCMC methods (Hedges et al., 1990; Garey et al., 1996; Yang & Yoder, 2003; Henk et al., 2003; Lakner et al., 2008; Zhang & Blackwell, 2001; Yoder & Yang, 2004; Rossman et al., 2001; Hohna & Drummond, 2012; Larget, 2013; Whidden & Matsen IV, 2015). For the Bayesian setting, we focus on the joint posterior distribution of the tree topologies and the branch lengths and assume a uniform prior on the tree topologies, an i.i.d. exponential prior \(\mathrm{Exp}(10)\) on branch lengths, and the simple JC substitution model (Jukes et al., 1969). For each of these data sets, we run 10 single-chain MrBayes (Ronquist et al., 2012) for one billion iterations, collect samples every 1000 iterations, and discard the first \(25\%\) samples as burn-in. These samples form the ground truth of the marginal distribution of tree topologies to which we will compare the posterior estimates obtained by different methods. All GNNs have \(L=2\) rounds in the message passing step. All the activation functions in MLPs are exponential linear units (ELUs) (Clevert et al., 2015). The taxa order is set to the lexicographical order of the corresponding species names in all experiments except the ablation studies. All the experiments are run on an Intel Xeon Platinum 9242 processor. All models are implemented in PyTorch (Paszke et al., 2019) and trained with the Adam (Kingma & Ba, 2015) optimizer. The learning rate is 0.001 for SBNs, 0.0001 for ARTree, and 0.001 for the branch length model.

### Tree topology density estimation

We first investigate the performance of ARTree for tree topology density estimation given the MCMC posterior samples on DS1-8. Following Zhang & Matsen IV (2018), we run MrBayes on each data set with 10 replicates of 4 chains and 8 runs until the runs have ASDSF (the standard convergence criteria used in MrBayes) less than 0.01 or a maximum of 100 million iterations. The training data

Figure 2: Performances of different methods for TDE on DS1. **Left/Middle**: Comparison of the ground truth and the estimated probabilities using SBN-EM and ARTree. A tree topology is marked as an outlier if it satisfies \(|\log(\text{estimated probability})-\log(\text{ground truth})|>2\). **Right**: The KL divergence as a function of the sample size. The results are averaged over 10 replicates with one standard deviation as the error bar.

sets are formed by collecting samples every 100 iterations and discarding the first 25%. Now, given a training data set \(\mathcal{M}=\{\tau_{m}\}_{m=1}^{M}\), we train ARTree via maximum likelihood estimation using stochastic gradient ascent. In each iteration, the stochastic gradient is obtained as follows

\[\nabla_{\bm{\phi}}L(\bm{\phi};\mathcal{M})=\frac{1}{B}\sum_{b=1}^{B}\nabla_{\bm {\phi}}\log Q_{\bm{\phi}}(\tau_{m_{b}}),\] (12)

where a minibatch \(\{\tau_{m_{b}}\}_{b=1}^{B}\) is randomly sampled from \(\mathcal{M}\). We compare ARTree to SBN baselines including SBN-EM, SBN-EM-\(\alpha\), and SBN-SGA. For SBN-EM and SBN-EM-\(\alpha\), we use the same setting as previously done in Zhang & Matsen IV (2018) (see Appendix A for more details). In addition to these EM variants, a gradient based method for SBNs called SBN-SGA is considered, where SBNs are reparametrized with the latent parameters initialized as zero (see equation (18) in Appendix B) and optimized via stochastic gradient ascent, similarly to ARTree. For both ARTree and SBN-SGA, the results are collected after 200000 parameter updates with batch size \(B=10\).

The left and middle plots of Figure 2 show a comparison between ARTree and SBN-EM on DS1, which has a peaky posterior distribution. Compared to SBN-EM, ARTree provides more accurate probability estimates for tree topologies on the peaks and significantly reduces the large biases in the low probability region (the crimson dots). The right plot of Figure 2 shows the KL divergence of different methods as a function of the sample size of the training data. We see that ARTree consistently outperforms SBN based methods for all \(M\)s. Moreover, as the sample size \(M\) increases, ARTree keeps providing better approximation while SBNs start to level off when \(M\) is large. This indicates the superior flexibility of ARTree over SBNs for tree topology density estimation.

Table 1 shows the KL divergences of different methods on DS1-8. We see that ARTree outperforms SBN based methods on all data sets. The gradient based method SBN-SGA is better than SBN-EM on most of the data sets because SBN-EM is well initialized (Zhang & Matsen IV, 2018) and more likely to get trapped in local modes. From this point of view, the comparison between ARTree and SBN-SGA is fair because they both use a uniform initialization that facilitates exploration.

### Variational Bayesian phylogenetic inference

Our second experiment is on VBPI, where we compare ARTree to SBNs for tree topology variational approximations. Both methods are evaluated on the aforementioned benchmark data sets DS1-8. Following Zhang & Matsen IV (2019), we use the simplest SBN and gather the subsplit support from 10 replicates of 10000 ultrafast maximum likelihood bootstrap trees (Minh et al., 2013). For both ARTree and SBNs, the collaborative branch lengths are parametrized using the learnable topological features with the edge convolution operator (EDGE) for GNNs (Zhang, 2023). We set \(K=10\) for the multi-sample lower bound (4) and use the following annealed unnormalized posterior at the \(i\)-th iteration

\[p(\bm{Y},\tau,\bm{q};\beta_{i})=p(\bm{Y}|\tau,\bm{q})^{\beta_{i}}p(\tau,\bm{q})\] (13)

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Data set} & \multirow{2}{*}{\#Taxa} & \multirow{2}{*}{\#Sites} & \multirow{2}{*}{Sampled trees} & \multicolumn{4}{c}{KL divergence to ground truth} \\ \cline{5-6}  & & & & SBN-EM & SBN-EM-\(\alpha\) & SBN-SGA & ARTree \\ \hline DS1 & 27 & 1949 & 1228 & 0.0136 & 0.0130 & 0.0504 & **0.0045** \\ DS2 & 29 & 2520 & 7 & 0.0199 & 0.0128 & 0.0118 & **0.0097** \\ DS3 & 36 & 1812 & 43 & 0.1243 & 0.0882 & 0.0922 & **0.0548** \\ DS4 & 41 & 1137 & 828 & 0.0763 & 0.0637 & 0.0739 & **0.0299** \\ DS5 & 50 & 378 & 33752 & 0.8599 & 0.8218 & 0.8044 & **0.6266** \\ DS6 & 50 & 1133 & 35407 & 0.3016 & 0.2786 & 0.2674 & **0.2360** \\ DS7 & 59 & 1824 & 1125 & 0.0483 & 0.0399 & 0.0301 & **0.0191** \\ DS8 & 64 & 1008 & 3067 & 0.1415 & 0.1236 & 0.1177 & **0.0741** \\ \hline \hline \end{tabular}
\end{table}
Table 1: KL divergences to the ground truth of different methods across 8 benchmark data sets. Sampled trees column shows the numbers of unique tree topologies in the training sets formed by MrBayes runs. The results are averaged over 10 replicates. The results of SBN-EM, SBN-EM-\(\alpha\) are from Zhang & Matsen IV (2018).

where \(\beta_{i}=\min\{1.0,0.001+i/H\}\) is the inverse temperature that goes from 0.001 to 1 after \(H\) iterations. For ARTree, a long annealing period \(H=200000\) is used for DS6 and DS7 due to the highly multimodal posterior distributions on these two data sets (Whidden and Matsen IV, 2015) and \(H=100000\) is used for the other data sets. For SBNs, we set \(H=100000\) for all data sets. The Monte Carlo gradient estimates for the tree topology parameters and the branch lengths parameters are obtained via VIMCO (Mnih and Rezende, 2016) and the reparametrization trick (Zhang and Matsen IV, 2019) respectively. The results are collected after 400000 parameter updates.

The left plot in Figure 3 shows the evidence lower bound (ELBO) as a function of the number of iterations on DS1. Although the larger support of ARTree adds to the complexity of training for tree topology variational approximation, we see that by the time SBN based methods converge, ARTree based methods achieve comparable (if not better) lower bounds and finally surpass the SBN baselines in the end. We also find that using fewer particles \((K=5)\) in the training objective tends

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline Data set & DS1 & DS2 & DS3 & DS4 & DSS & DS6 & DS7 & DS8 \\ \# Taxa & 27 & 29 & 36 & 41 & 50 & 50 & 59 & 64 \\ \# Sites & 1949 & 2520 & 1812 & 1137 & 378 & 1133 & 1824 & 1008 \\ GT trees & 2784 & 42 & 351 & 11505 & 1516877 & 809765 & 11525 & 82162 \\ \hline \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & SBN & 0.0707 & 0.0144 & 0.0554 & 0.0739 & 1.2472 & 0.3795 & 0.1531 & 0.3173 \\  & ARTree & **0.0097** & **0.0004** & **0.0064** & **0.0219** & **0.8979** & **0.2216** & **0.0123** & **0.1231** \\ \cline{1-1} \cline{2-10}  & SBN & -7110.24(0.03) & -26368.88(0.03) & -33736.22(0.02) & -13331.83(0.03) & -8217.80(0.04) & **-6728.65(0.06)** & -37334.85(0.04) & -8653.05(0.05) \\  & ARTree & **-7110.09(0.04)** & **-26368.78(0.07)** & **-33736.17(0.08)** & **-13331.82(0.05)** & **-8217.68(0.04)** & **-6728.65(0.06)** & **-73334.84(0.13)** & **-8655.03(0.05)** \\ \cline{1-1} \cline{2-10}  & SBN & -7108.69(0.02) & -26367.87(0.02) & -33735.26(0.02) & -13330.29(0.02) & -8215.42(0.04) & **-6725.33(0.04)** & **-37332.58(0.03)** & -8651.78(0.04) \\  & ARTree & **-7108.68(0.02)** & **-26367.86(0.02)** & **-33735.25(0.02)** & **-13330.27(0.03)** & **-8215.34(0.03)** & **-6725.33(0.04)** & **-37332.54(0.03)** & **-3651.73(0.04)** \\ \hline \multirow{4}{*}{
\begin{tabular}{} \end{tabular} } & \(\phi\)-CSMC & -7290.367(2.33) & -30568.49(13.34) & -33798.06(6.62) & -13582.24(35.08) & -8367.51(8.87) & -7013.83(16.99) & N/A & -9209.18(18.03) \\  & SBN & **-7108.41(0.15)** & -26367.71(0.08) & **-3375.09(0.09)** & -13329.94(0.20) & -8214.62(0.40) & **-6724.73(0.43)** & -7331.97(0.28) & -8650.64(0.50) \\ \cline{1-1}  & ARTree & -7108.41(0.19) & **-26367.71(0.07)** & **-33735.09(0.09)** & **-13329.94(0.17)** & **-8214.59(0.34)** & -6724.37(0.46) & **-3731.95(0.27)** & **-8650.61(0.48)** \\ \hline \hline \end{tabular}
\end{table}
Table 2: KL divergences to the ground truth, evidence lower bound (ELBO), 10-sample lower bound (LB-10), and marginal likelihood (ML) estimates of different methods across 8 benchmark data sets. GT trees row shows the number of unique tree topologies in the ground truth. The marginal likelihood estimates are obtained via importance sampling using 1000 samples. The KL results are averaged over 10 independent trainings. For ELBO, LB-10, and ML, the results are averaged over 100, 100, and 1000 independent runs respectively with standard deviation in the brackets. The results of \(\phi\)-CSMC are from Koptagel et al. (2022). For ELBO and LB-10, a larger mean is better; for ML, a smaller standard deviation is better1.

Figure 3: Performances of ARTree and SBN as tree topology variational approximations for VBPI on DS1. **Left**: the evidence lower bound (ELBO) as a function of iterations. The numbers of particles used in the training objective are in the brackets. The ARTree\({}^{*}\) method refers to ARTree without time guidance, i.e. \(b_{n}=0\) for all \(n\) in the readout step. **Middle**: variational approximations vs ground truth posterior probabilities of the tree topologies. **Right**: KL divergences across 50 random taxa orders. The KL divergence of SBNs is averaged over 10 independent trainings.

to provide larger ELBO. Moreover, time guidance turns out to be crucial for ARTree, as evidenced by the significant performance drop when it is turned off. As shown in the middle plot, compared to SBNs, ARTree can provide a more accurate variational approximation of the tree topology posterior. To investigate the effect of taxa orders on ARTree, we randomly sample 50 taxa orders and report the KL divergence for each order in the right plot of Figure 3. We find that ARTree exhibits weak randomness as the taxa order varies and consistently outperforms SBNs by a large margin.

Table 2 shows the KL divergences to the ground truth, evidence lower bound (ELBO), 10-sample lower bound (LB-10), and marginal likelihood (ML) estimates obtained by different methods on DS1-8. We find that ARTree achieves smaller KL divergences than SBNs across all data sets and performs on par or better than SBNs for lower bound and marginal likelihood estimation. Compared to SBNs, the ELBOs provided by ARTree tend to have larger variances, especially on DS2, DS3, and DS7, which is partly due to the larger support of ARTree that spans the entire tree topology space (see more discussions in Appendix E).

## 5 Conclusion

In this paper, we introduced ARTree, a deep autoregressive model over tree topologies for phylogenetic inference. Unlike SBNs that rely on hand-engineered features for parameterization and require pre-sampled tree topologies, ARTree is built solely on top of learnable topological features (Zhang, 2023) via GNNs which allows for a rich family of distributions over the entire phylogenetic tree topology space. Moreover, as an autoregressive model, ARTree also allows simple forward sampling procedures and straightforward density computation, which make it readily usable for tree topology density estimation and variational Bayesian phylogenetic inference. In experiments, we showed that ARTree outperforms SBNs on a benchmark of challenging real data tree topology density estimation and variational Bayesian phylogenetic inference problems, especially in terms of tree topology posterior approximation accuracy.

## Acknowledgements

This work was supported by National Natural Science Foundation of China (grant no. 12201014 and grant no. 12292983), as well as National Institutes of Health grant AI162611. The research of Cheng Zhang was support in part by National Engineering Laboratory for Big Data Analysis and Applications, the Key Laboratory of Mathematics and Its Applications (LMAM) and the Key Laboratory of Mathematical Economics and Quantitative Finance (LMEQF) of Peking University. The authors are grateful for the computational resources provided by the High-performance Computing Platform of Peking University. The authors appreciate the anonymous NeurIPS reviewers for their constructive feedback.

## References

* 562, 2022.
* De Cao and Kipf (2018) Nicola De Cao and Thomas Kipf. MolGAN: An implicit generative model for small molecular graphs, 2018. URL https://arxiv.org/abs/1805.11973.
* Cho et al. (2014) Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. _arXiv preprint arXiv:1406.1078_, 2014.
* Clevert et al. (2015) Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (ELUs). _arXiv: Learning_, 2015.
* Dai et al. (2020) Hanjun Dai, Azade Nazi, Yujia Li, Bo Dai, and Dale Schuurmans. Scalable deep generative modeling for sparse graphs. In _International Conference on Machine Learning_, pp. 2302-2312, 2020.
* DeSalle and Amato (2004) Rob DeSalle and George Amato. The expansion of conservation genetics. _Nat. Rev. Genet._, 5(9):702-712, September 2004. ISSN 1471-0056. doi: 10.1038/nrg1425. URL http://dx.doi.org/10.1038/nrg1425.
* DeSalle et al. (2015)* Dinh et al. (2017) Vu Dinh, Arman Bilge, Cheng Zhang, and Frederick A Matsen IV. Probabilistic path Hamiltonian Monte Carlo. In _Proceedings of the 34th International Conference on Machine Learning_, pp. 1009-1018, July 2017. URL http://proceedings.mlr.press/v70/dinh17a.html.
* 517, 2016.
* Plessis et al. (2017) Louis du Plessis, John T McCrone, Alexander E Zarebski, Verity Hill, Christopher Ruis, Bernardo Gutierrez, Jayna Raghwani, Jordan Ashworth, Rachel Colquhoun, Thomas R Connor, Nuno R Faria, Ben Jackson, Nicholas J Loman, Aine O'Toole, Samuel M Nicholls, Kris V Parag, Emily Scher, Tetyana I Vasylyeva, Erik M Volz, Alexander Watts, Isaac I Bogoch, Kamran Khan, COVID-19 Genomics UK (COG-UK) Consortium/, David M Aannensen, Moritz U G Kraemer, Andrew Rambaut, and Oliver G Pybus. Establishment and lineage dynamics of the SARS-CoV-2 epidemic in the UK. _Science_, January 2021. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.abf2946. URL https://science.sciencemag.org/content/early/2021/01/07/science.abf2946.
* Dudas et al. (2021) Gytis Dudas, Luiz Max Carvalho, Trevor Bedford, Andrew J Tatem, Guy Baele, Nuno R Faria, Daniel J Park, Jason T Ladner, Armando Arias, Danny Asogun, Filip Bielejec, Sarah L Caddy, Matthew Cotten, Jonathan D'Ambrozio, Simon Dellicour, Antonino Di Caro, Joseph W Dclaro, Sophie Durafour, Michael J Elmore, Lawrence S Fakoli, Ousanne Faye, Merle L Gilbert, Sahr M Gevao, Stephen Gire, Adrianne Gladden-Young, Andreas Gnirke, Augustine Goba, Donald S Grant, Bart L Haagmans, Julian A Hiscox, Umaru Jah, Jeffrey R Kugelman, Di Liu, Jia Lu, Christine M Malboeuf, Suzanne Mate, David A Matthews, Christian B Matranga, Luke W Meredith, James Qu, Joshua Quick, Suzan D Pas, My V T Phan, Georgios Pollakis, Chantal B Reusken, Mariano Sanchez-Lockhart, Stephen F Schaffner, John S Schieffelin, Rachel S Sealfon, Etienne Simon-Loriere, Saskia L Smits, Kilian Stoecker, Lucy Thorne, Ekaete Alice Tobin, Mohamed A Vandi, Simon J Watson, Kendra West, Shannon Whitmer, Michael R Wiley, Sarah M Winnicki, Shirlee Wohl, Roman Wolf, Nathan L Yozwiak, Kristian G Andersen, Sylvia O Blyden, Fatorma Bolay, Miles W Carroll, Bernice Dahn, Bobucar Diallo, Pierre Formenty, Christophe Fraser, George F Gao, Robert F Garry, Ian Goodfellow, Stephan Gunther, Christian T Happi, Edward C Holmes, Brima Kargbo, Sakoba Keita, Paul Kellam, Marion P G Koopmans, Jens H Kuhn, Nicholas J Loman, N'faly Magassouba, Dhamari Naidoo, Stuart T Nichol, Tolbert Nyenswah, Gustavo Palacios, Oliver G Pybus, Pardis C Sabeti, Amadou Sall, Ute Stroher, Isatta Wurie, Marc A Suchard, Philippe Lemey, and Andrew Rambaut. Virus genomes reveal factors that spread and sustained the Ebola epidemic. _Nature_, April 2017. ISSN 0028-0836, 1476-4687. doi: 10.1038/nature22040. URL http://dx.doi.org/10.1038/nature22040.
* Felsenstein (1981) J. Felsenstein. Evolutionary trees from DNA sequences: A maximum likelihood approach. _Journal of Molecular Evolution_, 17:268-276, 1981.
* Felsenstein (2004) Joseph Felsenstein. _Inferring Phylogenies_. Sinauer associates, 2 edition, 2004.
* Garey et al. (1996) J. R. Garey, T. J. Near, M. R. Nonnemacher, and S. A. Nadler. Molecular evidence for Acanthocephala as a subtaxon of Rotifera. _Mol. Evol._, 43:287-292, 1996.
* Gilmer et al. (2017) Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. _ArXiv_, abs/1704.01212, 2017.
* Hedges et al. (1990) S. B. Hedges, K. D. Moberg, and L. R. Maxson. Tetrapod phylogeny inferred from 18S and 28S ribosomal RNA sequences and review of the evidence for amniote relationships. _Mol. Biol. Evol._, 7:607-633, 1990.
* Henk et al. (2003) D. A. Henk, A. Weir, and M. Blackwell. Laboulbeniopsis termitarius, an ectoparasite of termites newly recognized as a member of the Laboulbeniomycetes. _Mycologia_, 95:561-564, 2003.
* Hohna and Drummond (2012) Sebastian Hohna and Alexei J. Drummond. Guided tree topology proposals for Bayesian phylogenetic inference. _Syst. Biol._, 61(1):1-11, January 2012. ISSN 1063-5157. doi: 10.1093/sysbio/syr074. URL http://dx.doi.org/10.1093/sysbio/syr074.
* Huelsenbeck et al. (2001) J. P. Huelsenbeck, F. Ronquist, R. Nielsen, and J. P. Bollback. Bayesian inference of phylogeny and its impact on evolutionary biology. _Science_, 294:2310-2314, 2001.
* Huelsen et al. (2017)Wengong Jin, Regina Barzilay, and T. Jaakkola. Junction tree variational autoencoder for molecular graph generation. _ArXiv_, abs/1802.04364, 2018.
* Jukes et al. (1969) Thomas H Jukes, Charles R Cantor, et al. Evolution of protein molecules. _Mammalian protein metabolism_, 3:21-132, 1969.
* Kingma and Ba (2015) D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* Koptagel et al. (2022) Hazal Koptagel, Oskar Kviman, Harald Melin, Negar Safinianaini, and Jens Lagergren. VaiPhy: a variational inference based algorithm for phylogeny. In _Advances in Neural Information Processing Systems_, 2022.
* Lakner et al. (2008) C. Lakner, P. van der Mark, J. P. Huelsenbeck, B. Larget, and F. Ronquist. Efficiency of Markov chain Monte Carlo tree proposals in Bayesian phylogenetics. _Syst. Biol._, 57:86-103, 2008.
* Larget (2013) Bret Larget. The estimation of tree posterior probabilities using conditional clade probability distributions. _Syst. Biol._, 62(4):501-511, July 2013. ISSN 1063-5157. doi: 10.1093/sysbio/syt014. URL http://dx.doi.org/10.1093/sysbio/syt014.
* Larget and Simon (1999) Bret R. Larget and D. L. Simon. Markov chain Monte Carlo algorithms for the Bayesian analysis of phylogenetic trees. _Molecular Biology and Evolution_, 16:750-750, 1999.
* Li et al. (2018) Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia. Learning deep generative models of graphs, 2018. URL https://arxiv.org/abs/1803.03324.
* Liao et al. (2019) Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Charlie Nash, William L. Hamilton, David Kristjanson Duvenaud, Raquel Urtasun, and Richard S. Zemel. Efficient graph generation with graph recurrent attention networks. _ArXiv_, abs/1910.00760, 2019.
* Mau et al. (1999) B. Mau, M. Newton, and B. Larget. Bayesian phylogenetic inference via Markov chain Monte Carlo methods. _Biometrics_, 55:1-12, 1999.
* 1195, 2013.
* Mnih and Rezende (2016) Andriy Mnih and Danilo Jimenez Rezende. Variational inference for monte carlo objectives. In _International Conference on Machine Learning_, 2016.
* Moretti et al. (2021) Antonio Khalil Moretti, Liyi Zhang, Christian Andersson Naesseth, Hadah Venner, David M. Blei, and Itsik Pe'er. Variational combinatorial sequential Monte Carlo methods for Bayesian phylogenetic inference. In _Conference on Uncertainty in Artificial Intelligence_, 2021.
* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In _Neural Information Processing Systems_, 2019.
* Rainforth et al. (2019) Tom Rainforth, Adam R. Kosioreck, Tuan Anh Le, Chris J. Maddison, Maximilian Igl, Frank Wood, and Yee Whye Teh. Tighter variational bounds are not necessarily better. In _Proceedings of the 36th International Conference on Machine Learning_, 2019.
* Ronquist et al. (2012) Fredrik Ronquist, Maxim Teslenko, Paul Van Der Mark, Daniel L Ayres, Aaron Darling, Sebastian Hohna, Bret Larget, Liang Liu, Marc A Suchard, and John P Huelsenbeck. MrBayes 3.2: Efficient Bayesian phylogenetic inference and model choice across a large model space. _Systematic Biology_, 61(3):539-542, 2012.
* Rossman et al. (2001) A. Y. Rossman, J. M. Mckemy, R. A. Pardo-Schultheiss, and H. J. Schroers. Molecular studies of the Bionectriaceae using large subunit rDNA sequences. _Mycologia_, 93:100-110, 2001.
* Shi et al. (2020) Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. GraphAF: a flow-based autoregressive model for molecular graph generation. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=SiesMkHYPr.
* Shi et al. (2019)Martin Simonovsky and Nikos Komodakis. GraphVAE: Towards generation of small graphs using variational autoencoders. In _International Conference on Artificial Neural Networks_, pp. 412-422, 2018.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* 1374, 2015. URL https://api.semanticscholar.org/CorpusID:4495539.
* 12, 2018.
* Whidden and Matsen (2015) Chris Whidden and Frederick A Matsen IV. Quantifying MCMC exploration of phylogenetic tree space. _Syst. Biol._, 64(3):472-491, May 2015. ISSN 1063-5157, 1076-836X. doi: 10.1093/sysbio/syv006. URL http://dx.doi.org/10.1093/sysbio/syv006.
* Yang and Yoder (2003) Z. Yang and A. D. Yoder. Comparison of likelihood and Bayesian methods for estimating divergence times using multiple gene loci and calibration points, with application to a radiation of cute-looking mouse lemur species. _Syst. Biol._, 52:705-716, 2003.
* Yang and Rannala (1997) Ziheng Yang and Bruce Rannala. Bayesian phylogenetic inference using DNA sequences: a Markov chain Monte Carlo method. _Molecular Biology and Evolution_, 14(7):717-724, 1997.
* Yoder and Yang (2004) A. D. Yoder and Z. Yang. Divergence datas for Malagasy lemurs estimated from multiple gene loci: geological and evolutionary context. _Mol. Ecol._, 13:757-773, 2004.
* You et al. (2018a) Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In _Advances in Neural Information Processing Systems_, pp. 6410-6421, 2018a.
* You et al. (2018b) Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Leskovec. GraphRNN: Generating realistic graphs with deep auto-regressive models. In _International Conference on Machine Learning_, 2018b.
* Zhang (2020) Cheng Zhang. Improved variational Bayesian phylogenetic inference with normalizing flows. In _Neural Information Processing Systems_, 2020.
* Zhang (2023) Cheng Zhang. Learnable topological features for phylogenetic inference via graph neural networks. In _International Conference on Learning Representations_, 2023.
* Zhang and Matsen IV (2018) Cheng Zhang and Frederick A Matsen IV. Generalizing tree probability estimation via Bayesian networks. In _Neural Information Processing Systems_, 2018.
* Zhang and Matsen IV (2019) Cheng Zhang and Frederick A Matsen IV. Variational Bayesian phylogenetic inference. In _International Conference on Learning Representations_, 2019.
* Zhang and Matsen IV (2022) Cheng Zhang and Frederick A Matsen IV. A variational approach to Bayesian phylogenetic inference, 2022. URL https://arxiv.org/abs/2204.07747.
* Zhang and Blackwell (2001) N. Zhang and M. Blackwell. Molecular phylogeny of dogwood anthracose fungus (Discula destructiva) and the Diaporthales. _Mycologia_, 93:355-365, 2001.

## Appendix A Details of subsplit Bayesian networks

One recent and expressive graphical model that provides a flexible family of distributions over tree topologies is the subsplit Bayesian network, as proposed by Zhang & Matsen IV (2018). Let \(\mathcal{X}\) be the set of \(N\) labeled leaf nodes. A non-empty set \(C\) of \(\mathcal{X}\) is referred to as a _clade_ and the set of all clades of \(\mathcal{X}\), denoted by \(\mathcal{C}(\mathcal{X})\), is a totally ordered set with a partial order \(\succ\) (e.g., lexicographical order) defined on it. An ordered pair of clades \((W,Z)\) is called a _subsplit_ of a clade \(C\) if it is a bipartition of \(C\), i.e., \(W\succ Z,W\cap Z=\emptyset\), and \(W\cup Z=C\).

**Definition 2** (Subsplit Bayesian Network).: _A subsplit Bayesian network (SBN) \(\mathcal{B}_{\mathcal{X}}\) on a leaf node set \(\mathcal{X}\) of size \(N\) is defined as a Bayesian network whose nodes take on subsplit or singleton clade values of \(\mathcal{X}\) and has the following properties: (a) The root node of \(\mathcal{B}_{\mathcal{X}}\) takes on subsplits of the entire labeled leaf node set \(\mathcal{X}\); (b) \(\mathcal{B}_{\mathcal{X}}\) contains a full and complete binary tree network \(B_{\mathcal{X}}^{*}\) as a subnetwork; (c) The depth of \(B_{\mathcal{X}}\) is \(N-1\), with the root counted as depth \(1\)._

Due to the binary structure of \(B_{\mathcal{X}}^{*}\), the nodes in SBNs can be indexed by denoting the root node with \(S_{1}\) and two children of \(S_{i}\) with \(S_{2i}\) and \(S_{2i+1}\) recursively where \(S_{i}\) is an internal node (see the left plot in Figure 4). For any rooted tree topology, by assigning the corresponding subsplits or singleton clades values \(\{S_{i}=s_{i}\}_{i\geq 1}\) to its nodes, one can uniquely map it into an SBN node assignment (see the middle and right plots in Figure 4).

As Bayesian networks, the SBN based probability of a rooted tree topology \(\tau\) takes the following form

\[p_{\mathrm{sbn}}(T=\tau)=p(S_{1}=s_{1})\prod_{i>1}p(S_{i}=s_{i}|S_{\pi_{i}}=s_ {\pi_{i}}),\] (14)

where \(\pi_{i}\) is the index set of the parents of node \(i\). For unrooted tree topologies, we can also define their SBN based probabilities by viewing them as rooted tree topologies with unobserved roots and integrating out the positions of the root node as follows:

\[p_{\mathrm{sbn}}(T^{\mathrm{u}}=\tau)=\sum_{e\in E(\tau)}p_{\mathrm{sbn}}( \tau^{e})\] (15)

where \(\tau^{e}\) is the resulting rooted tree topology when the rooting position is on edge \(e\).

In practice, SBNs are parameterized according to the _conditional probability sharing_ principle where the conditional probability for parent-child subsplit pairs are shared across the SBN network, regardless of their locations. The set of all conditional probabilities are called conditional probability tables (CPTs). Parameterizing SBNs, therefore, often requires finding an appropriate support of CPTs. For tree topology density estimation, this can be done using the sample of tree topologies that is given as the data set. For variational Bayesian phylogenetic inference, as no sample of tree topologies is available, one often resorts to fast bootstrap or MCMC methods (Minh et al., 2013; Zhang, 2020). Let \(\mathbb{S}_{\mathrm{r}}\) denotes the root subsplits and \(\mathbb{S}_{\mathrm{ch}|\mathrm{pa}}\) denotes the child-parent subsplit pairs in the support. The parameters of SBNs are then \(p=\{p_{s_{1}};s_{1}\in\mathbb{S}_{\mathrm{r}}\}\cup\{p_{s|t};s|t\in\mathbb{S}_ {\mathrm{ch}|\mathrm{pa}}\}\) where

\[p_{s_{1}}=p(S_{1}=s_{1}),\quad p_{s|t}=p(S_{i}=s|S_{\pi_{i}}=t),\;\forall i>1.\] (16)

Figure 4: Subsplit Bayesian networks and a simple example for a leaf set of 4 taxa (denoted by \(A,B,C,D\) respectively). **Left:** General subsplit Bayesian networks. The solid full and complete binary tree network is \(B_{\mathcal{X}}^{*}\). The dashed arrows represent the additional dependence for more expressiveness. **Middle Left:** Examples of (rooted) phylogenetic trees that are hypothesized to model the evolutionary history of the taxa. **Middle Right:** The corresponding subsplit assignments for the trees. For ease of illustration, subsplit \((Y,Z)\) is represented as \(\frac{Y}{Z}\) in the graph. **Right:** The SBN for this example, which is \(\mathcal{B}_{\mathcal{X}}^{*}\) in this case.

As a result, the supports of SBN-induced distributions are often limited by the splitting patterns in the observed samples and could not span the entire tree topology space (Zhang & Matsen IV, 2022).

The SBN-EM AlgorithmFor unrooted tree topologies, the SBN based probability (15) can be viewed as a hidden variable model where the root subsplit is the hidden variable. In this case, SBNs can be trained using the expectation-maximization (EM) algorithm, as proposed by Zhang & Matsen IV (2018). Given a training set \(\{\tau_{k}\}_{k=1}^{M}\), we first initialize the parameter estimates as \(\hat{p}^{EM,(0)}\) (i.e., the simple average estimates as in Zhang & Matsen IV (2018)). In the \(i\)-th step, we run the E-step and M-step as follows

* **E-step**: \(\forall 1\leq k\leq M\), compute \(q_{k}^{(i)}\left(s_{1}\right)=\frac{p\left(\tau_{k},s_{1}\middle|\hat{p}^{ \mathrm{EM},(i)}\right)}{\sum_{s_{1}\sim\tau_{k}}p\left(\tau_{k},s_{1} \middle|\hat{p}^{\mathrm{EM},(i)}\right)}\) where \(s_{1}\sim\tau_{k}\) means the subsplit \(s_{1}\) can be achieved by placing a "virtual root" on an edge of \(\tau\).
* **M-step**: update the parameter estimates by \[\hat{p}_{s_{1}}^{\mathrm{EM},(i+1)}=\frac{\bar{m}_{s_{1}}^{(i)}+ \alpha\tilde{m}_{s_{1}}}{K+\alpha\sum_{s_{1}\in\mathbb{S}_{\mathrm{r}}}\bar{m }_{s_{1}}}, \bar{m}_{s_{1}}^{(i)}=\sum_{k=1}^{M}\sum_{e\in E(\tau_{k})}q_{k}^{(i)} \left(s_{1}\right)\mathbb{I}\left(s_{1,k}^{e}=s_{1}\right)\] \[\hat{p}_{s|t}^{\mathrm{EM},(i+1)}=\frac{\bar{m}_{s,t}^{(i)}+ \alpha\tilde{m}_{s,t}}{\sum_{s}\left(\bar{m}_{s,t}^{(i)}+\alpha\tilde{m}_{s,t }\right)}, \bar{m}_{s,t}^{(i)}=\sum_{k=1}^{M}\sum_{e\in E(\tau_{k})}q_{k}^{(i)} \left(s_{1,k}^{e}\right)\sum_{j>1}\mathbb{I}\left(s_{j,k}^{e}=s,s_{s_{j},k}^{e }=t\right)\] where \(\mathbb{I}\) is the indicator function, \(s_{j,k}^{e}\) is the node value of \(S_{j}\) in \(\tau_{k}^{e}\), \(\tilde{m}_{s}\) and \(\tilde{m}_{s,t}\) are equivalent counts and \(\alpha\) is the regularization coefficient that encourages generalization.

When \(\alpha>0\), this algorithm is called SBN-EM-\(\alpha\).

## Appendix B Details of variational Bayesian phylogenetic inference

With two variational families \(Q_{\bm{\phi}}(\tau)\) and \(Q_{\bm{\psi}}(\bm{q}|\tau)\) over the space of tree topologies and branch lengths, the variational Bayesian phylogenetic inference (VBPI) approach forces \(Q_{\bm{\phi},\bm{\psi}}(\tau,\bm{q})=Q_{\bm{\phi}}(\tau)Q_{\bm{\psi}}(\bm{q}|\tau)\) to approximate the posterior \(p(\tau,\bm{q}|\bm{Y})\) by maximizing the following multi-sample lower bound

\[L^{K}(\bm{\phi},\bm{\psi})=\mathbb{E}_{\{(\tau^{i},\bm{q}^{i})\}_{i=1}^{K} \overset{i.i.d.}{\sim}Q_{\bm{\phi},\bm{\psi}}}\log\left(\frac{1}{K}\sum_{i=1}^ {K}\frac{p(\bm{Y}|\tau^{i},\bm{q}^{i})p(\tau^{i},\bm{q}^{i})}{Q_{\bm{\phi}}( \tau^{i})Q_{\bm{\psi}}(\bm{q}^{i}|\tau^{i})}\right).\] (17)

Gradients of the objective (17) w.r.t. \(\bm{\phi}\) and \(\bm{\psi}\) can be estimated by the VIMCO estimator (Mnih & Rezende, 2016) and the reparameterization trick respectively. In the following, we introduce some common choices of \(Q_{\bm{\phi}}(\tau)\) and \(Q_{\bm{\psi}}(\bm{q}|\tau)\).

Choice of \(Q_{\bm{\phi}}(\tau)\)Before the proposed ARTree framework in this article, SBNs is the common choice of \(Q_{\bm{\phi}}(\tau)\). As introduced in Appendix A, SBNs provide a probability distribution over unrooted tree topologies in equation (15). Given a subsplit support of CPTs, SBNs can be parameterized as follows

\[p_{s_{1}}=\frac{\exp(\phi_{s_{1}})}{\sum_{s^{\prime}\in\mathbb{S}_{\mathrm{r} }}\exp(\phi_{s^{\prime}})},\;s_{1}\in\mathbb{S}_{\mathrm{r}};\quad p_{s|t}= \frac{\exp(\phi_{s|t})}{\sum_{s^{\prime}:s^{\prime}|t\in\mathbb{S}_{\mathrm{ Ach}|\mathrm{pa}}}\exp(\phi_{s^{\prime}|t})},\;s|t\in\mathbb{S}_{\mathrm{ Ach}|\mathrm{pa}}.\] (18)

The parameters \(\bm{\phi}=\{\phi_{s_{1}};s_{1}\in\mathbb{S}_{\mathrm{r}}\}\cup\{\phi_{s|t};\;s|t \in\mathbb{S}_{\mathrm{ch}|\mathrm{pa}}\}\) are called latent parameters of SBNs.

Choice of \(Q_{\bm{\psi}}(\bm{q}|\tau)\)The distribution \(Q_{\bm{\psi}}(\bm{q}|\tau)\) is often taken to be a diagonal lognormal distribution, which can be parametrized using some heuristic features (Zhang & Matsen IV, 2019) or the recently proposed learnable topological features (Zhang, 2023) of \(\tau\) as follows. For each edge \(e=(u,v)\) in \(\tau\), one can first obtain the edge features using \(h_{e}=f(h_{u},h_{v})\) where \(h_{u}\) is the GNN output at node \(u\) and \(f\) is a permutation invariant function. Then the mean and standard deviation parameters are given by

\[\mu(e,\tau)=\mathrm{MLP}^{\mu}(h_{e}),\quad\sigma(e,\tau)=\mathrm{MLP}^{\sigma }(h_{e})\]

where \(\mathrm{MLP}^{\mu}\) and \(\mathrm{MLP}^{\sigma}\) are two multi-layer perceptrons (MLPs).

## Appendix C Details of tree topology decomposition process

The tree topology decomposition process, which maps a tree topology to a corresponding decision sequence, is indeed the inverse operation of Algorithm 1. Also, the decomposition process is implemented in a recursive way starting from the tree topology \(\tau_{N}\) of rank \(N\). Intuitively, given an ordinal tree topology \(\tau_{n+1}\) of rank \(n+1\), one can detach the leaf node \(x_{n+1}\) as well as its unique neighbor \(w\) and reconnect the two neighbors of \(w\). The remaining graph, denoted by \(\tau_{n}\), is an ordinal tree topology of rank \(n\); the edge decision \(e_{n}\) is exactly the reconnected edge. This process continues until the unique ordinal tree topology \(\tau_{3}\) of rank \(3\) is reached. We summarize the sketch of tree topology decomposition process in Algorithm 2.

``` Input: a tree topology \(\tau\) with all of the \(N\) leaf nodes. Output: a decision sequence \(D\). \(\tau_{N}=(V_{N},E_{N})\leftarrow\) the tree topology \(\tau\); for\(n=N-1,\ldots,3\)do  Determine the unique neighbor \(w\) of the leaf node \(x_{n+1}\);  Determine the two neighbors \(u\) and \(v\) (except \(x_{n+1}\)) of \(w\); \(V_{n}\gets V_{n+1}\backslash\{w,x_{n+1}\}\); \(E_{n}\leftarrow(E_{n+1}\cup\{(u,v)\})\backslash\{(w,x_{n+1}),(w,u),(w,v)\}\); \(\tau_{n}\leftarrow(V_{n},E_{n})\); \(e_{n}\leftarrow(u,v)\);  end for \(D\leftarrow(e_{3},\ldots,e_{N-1})\). ```

**Algorithm 2**Tree topology decomposition process

Given a tree topology \(\tau\) with all of the \(N\) leaf nodes, we can evaluate its ARTree based probability by first mapping it to a decision sequence \(D=(e_{3},\ldots,e_{N-1})\) following Algorithm 2 and then calculate the probability as the product of conditionals

\[Q(\tau)=Q(D)=\prod_{n=3}^{N-1}Q(e_{n}|e_{<n}).\]

## Appendix D The proofs of Theorem 1 and Lemma 1

See 1

Proof of Theorem 1It is obvious that \(g\) is a well-defined map from \(\mathcal{D}\) to \(\mathcal{T}\). To prove it is a bijection, it suffices to show \(g\) is injective and surjective.

We first show that \(g\) is injective. Assume there are two decision sequences \(D^{(1)},D^{(2)}\in\mathcal{D}\) and \(D^{(1)}\neq D^{(2)}\). Let \(k\) be the first position where \(D^{(1)}\) and \(D^{(2)}\) begin to differ, i.e. \(e_{i}^{(1)}=e_{i}^{(2)}\) for all \(i<k\) and \(e_{k}^{(1)}\neq e_{k}^{(2)}\). If \(g(D^{(1)})=g(D^{(2)})=:\tau\), one can take the subtree topology of rank \(k\) and \(k+1\) of \(\tau\), \(\tau_{k}\) and \(\tau_{k+1}\). Noting that \(e_{k}\) refers to the edge in \(\tau\) where to add the new node \(x_{n+1}\), the equation \(e_{k}^{(1)}\neq e_{k}^{(2)}\) implies they will induce different \(\tau_{k+1}\)s. This contradicts the uniqueness of \(\tau_{k+1}\). Therefore, we conclude that \(g\) is injective.

Next, we prove that \(g\) is surjective. For a tree topology \(\tau\) with rank \(N\), we denote its subtree topology of rank \(k\) by \(\tau_{k}\), where \(k=3,\ldots,n\). In fact, for each \(k\), the tree topology \(\tau_{k+1}\) corresponds to adding the leaf node \(x_{k+1}\) to an edge in \(\tau_{k}\), and we denote this edge by \(e_{k}\). It is easy to verify that the constructed \(D=(e_{3},\ldots,e_{N-1})\) is a preimage of \(\tau\).

[MISSING_PAGE_EMPTY:17]

Minor increasement of lower boundsFrom Table 2, the estimates of lower bounds (as well as marginal likelihoods) of ARTree do not improve much over those of SBNs. We provide two explanations for this: (i) The lower bounds in VBPI are more sensitive to the quality of branch length model \(Q(\bm{q}|\tau)\) instead of the tree topology model \(Q(\tau)\). As SBN and ARTree use the same parametrization of the branch length model, we do not expect a large improvement in lower bounds. (ii) The support of ARTree spans the entire tree topology space. This adds to the difficulty of training \(Q(\bm{q}|\tau)\) which is conditioned on tree topology \(\tau\), as discussed in the above paragraph. Finally, from Table 3, we see that the enhancement on ELBOs of ARTree indeed comes from a better tree topology model as evidenced by the result of the 'SBN + branch length model trained along with ARTree' combination.

Time complexityAnother limitation of ARTree is its time complexity. We compare the training time and evaluation time in Table 4. Both ARTree and SBNs take more time in training than other methods with unconfined support since they both build machine-learning models with enormous parameters and rely heavily on optimization. However, they take a comparable amount of time to provide good enough approximations for marginal likelihood estimation of similar accuracy, as evidenced by ARTree\({}^{*}\) and SBN\({}^{*}\). ARTree takes more time than SBNs because it relies on several submodules which, although complicated, are designed to promote the expressive power to accommodate the complex tree space and are widely-used strategies in the literature. The inefficiency of autoregressive generative models is also an inherent issue.