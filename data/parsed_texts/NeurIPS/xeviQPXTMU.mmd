# FedGMark: Certifiably Robust Watermarking for Federated Graph Learning

Yuxin Yang\({}^{1,2}\) Qiang Li\({}^{1}\) Yuan Hong\({}^{3}\) Binghui Wang\({}^{2}\)

\({}^{1}\)College of Computer Science and Technology, Jilin University, Changchun, Jilin, China

\({}^{2}\)Department of Computer Science, Illinois Institute of Technology, Chicago, Illinois, USA

\({}^{3}\)School of Computing, University of Connecticut, Storrs, Connecticut, USA

Corresponding Author (bwang70@iit.edu)

###### Abstract

Federated graph learning (FedGL) is an emerging learning paradigm to collaboratively train graph data from various clients. However, during the development and deployment of FedGL models, they are susceptible to illegal copying and model theft. Backdoor-based watermarking is a well-known method for mitigating these attacks, as it offers ownership verification to the model owner. We take the first step to protect the ownership of FedGL models via backdoor-based watermarking. Existing techniques have challenges in achieving the goal: 1) they either cannot be directly applied or yield unsatisfactory performance; 2) they are vulnerable to watermark removal attacks; and 3) they lack of formal guarantees. To address all the challenges, we propose FedGMark, the first certified robust backdoor-based watermarking for FedGL. FedGMark leverages the unique graph structure and client information in FedGL to learn customized and diverse watermarks. It also designs a novel GL architecture that facilitates defending against both the empirical and theoretically worst-case watermark removal attacks. Extensive experiments validate the promising empirical and provable watermarking performance of FedGMark. Source code is available at: https://github.com/Yuxin104/FedGMark.

## 1 Introduction

Federated Graph Learning (FedGL) [22, 23, 24, 25] leverages a server and multiple clients to collaboratively train GL methods [18, 26] via federated learning (FL) [27, 28, 29]. In recent years, FedGL has attracted increasing interest in domains such as disease prediction [14], recommendation systems [1, 20, 21, 22], and molecular classification [15]. In addition, several industries have deployed/open-sourced their FedGL frameworks, such as Alibaba's FederatedScope-GNN [23] and Amazon's FedML-GNN [26]. However, FedGL models are typically left unprotected, rendering them vulnerable to threats like illegal copying, model theft, and malicious distribution. For instance, a business competitor may replicate a model to gain competitive advantages or a malicious user may sell the model for profits. These threats waste the model owner's investment (e.g., labor costs, time, and energy) and infringe upon the legitimate copyrights of the model.

Backdoor-based watermarking [27, 1, 1] is a _de facto_ model ownership verification technique to mitigate the above threats. This technique typically consists of two steps: 1) Embedding the target model with a watermark. The model owner injects a specific backdoor trigger (i.e., watermark) into some clean samples and trains the target model withthis watermarked data along with the remaining clean data. Then the trained target (watermarked) model could have both high watermark accuracy (i.e., accurately classify testing data with the same watermark as the owner desires) and main task accuracy (i.e., accurately classify clean testing data). 2) Model ownership verification. When suspecting the target model is illegally used by others, the model owner can recruit a trusted third party for model ownership verification. Particularly, the true model owner knows how the target model behaves as expected by providing the trusted third party the carefully designed watermarked data, while the illegal parties cannot do so. Notice that, since all the clients have devoted computation and data to the training, they have a strong intention to jointly protect their ownership of the model.

In this paper, we aim to protect the ownership of FedGL models via backdoor-based watermarking. We observe backdoor-based watermarking methods for protecting the ownership of FL model on _non-graph_ data (Li _et al._, 2022; Tekgul _et al._, 2021; Shao _et al._, 2022; Yang _et al._, 2023; Lansari _et al._, 2023) or centralized GL model on graph data (Xu _et al._, 2023) have been recently developed. However, applying these methods for protecting FedGL models faces challenges and weaknesses.

* _Inapplicable or ineffective:_ Existing methods for non-graph data cannot be directly applied for graph data. For instance, they require input data have same size, while graphs can have varying sizes; they are unable to consider the _connectivity_ information such as edges connecting nodes in the graph data. The only method for graph data (Xu _et al._, 2023) uses a naive _random graph_ (e.g., generated by the ER model (Gilbert, 1959)) as a watermark. Extending this watermark from centralized GL to FedGL models exhibits _unsatisfactory performance_, as shown in Table 1. For instance, the watermark accuracy is less than 60% in all the studied graph datasets and FedGL models. The core reason is the random graph watermark does not use any graph structure information or client information that are unique in FedGL.
* _Vulnerable to watermark removal attacks:_ They are vulnerable to existing watermark removal techniques such as distillation and finetuning (Bansal _et al._, 2022) (more details in Section 2.3). For instance, as illustrated in Table 1, distillation can reduce the watermark accuracy to less than 30%.
* _Lack or weak formal guarantees:_ All these methods do not provide formal robustness guarantees against watermark removal attacks. This could make them even vulnerable to more advanced attacks. For instance, our proposed layer-perturbation attack can further reduce the watermark accuracy, e.g., perturbing only 1-layer parameters of the watermarked model yields only 10% watermark accuracy (while main accuracy is marginally affected). Bansal _et al._ (2022) proposed the first certified watermark for centralized non-graph learning models against \(l_{2}\) model parameter perturbation. However, its certified radius is only 1.2, meaning the \(l_{2}\) norm of a (usually million-dimensional) perturbation vector cannot exceed 1.2 to maintain the watermark accuracy.

We address all the above issues by proposing a certified robust backdoor-based watermark method for FedGL, called **FedGMark**.2 FedGMark enjoys several properties: 1) Its designed watermarks can handle varying size graphs and utilize both graph structure and client information unique in FedGL models; 2) It is _empirically_ robust to both existing watermark removal attacks and the proposed layer-perturbation attack; and 3) more importantly, it is _provably_ robust to the layer-perturbation attack (the layer parameters can be arbitrarily perturbed), when the number of the perturbed layers is bounded.

\begin{table}
\begin{tabular}{c||c|c c c c c c||c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{Attack} & \multicolumn{2}{c}{Fed-GIN} & \multicolumn{2}{c}{Fed-GSAGE} & \multicolumn{2}{c||}{Fed-GCN} & \multirow{2}{*}{Datasets} & \multirow{2}{*}{Attack} & \multicolumn{2}{c}{Fed-GIN} & \multirow{2}{*}{Fed-GSSAGE} & \multicolumn{2}{c}{Fed-GCN} \\  & & & M\(\uparrow\) & M\(\uparrow\) & M\(\uparrow\) & M\(\uparrow\) & M\(\uparrow\) & M\(\uparrow\) & M\(\uparrow\) & & M\(\uparrow\) & M\(\uparrow\) & M\(\uparrow\) & M\(\uparrow\) & M\(\uparrow\) \\ \hline \hline \multirow{3}{*}{MUTAG} & None & **0.82** & **0.47** & **0.82** & **0.42** & **0.80** & **0.43** & \multirow{3}{*}{} & None & **0.71** & **0.56** & **0.70** & **0.54** & **0.70** & **0.52** \\  & Distillation & 0.81 & 0.38 & 0.80 & 0.35 & 0.75 & 0.32 & PROTENIS & Distillation & 0.71 & 0.30 & 0.70 & 0.32 & 0.70 & 0.28 \\  & Finetuning & 0.82 & 0.33 & 0.80 & 0.29 & 0.78 & 0.27 & Finetuning & 0.71 & 0.27 & 0.70 & 0.29 & 0.70 & 0.25 \\  & 1-Layer Pert. & 0.78 & 0.24 & 0.79 & 0.23 & 0.79 & 0.19 & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} & \multirow{3}{*}{} \\  & **0.73** & **0.39** & **0.70** & **0.38** & **0.71** & **0.38** & & & & & & & & & \\ \cline{1-1} \multirow{3}{*}{DD} & Non & **0.73** & **0.39** & **0.70** & **0.30** & **0.70** & 0.30 & CollAB & Distillation & 0.72 & 0.47 & 0.73 & 0.49 & 0.71 & 0.47 \\  & Finetuning & 0.72 & 0.19 & 0.70 & 0.21 & 0.70 & 0.22 & Finetuning & 0.72 & 0.35 & 0.72 & 0.36 & 0.71 & 0.40 \\  & 1-Layer Pert. & 0.70 & 0.11 & 0.70 & 0.15 & 0.70 & 0.12 & 1-Layer Pert. & 0.67 & 0.31 & 0.66 & 0.26 & 0.68 & 0.38 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results of adapting the random graph-based watermarking GL method (Xu _et al._, 2023) to watermark FedGL models. “MA”: main task accuracy; “WA”: watermark accuracy.

Specifically, as depicted in Figure 1, FedGMark consists of two modules: _1) Customized Watermark Generator (CWG)_: it learns the customized watermark for individual graphs and clients in FedGL, by integrating the edge information from the client graphs and the unique key features of the clients. CWG can significantly enhance the diversity and effectiveness of the generated watermarks. _2) Robust Model Loader (RML)_. RML designs a new GL model that consists of multiple submodels, where each submodel can be any existing GL model. It also introduces a voting classifier for assembling the submodels' predictions. Such a design can facilitate deriving the certified watermark performance against the (worst-case) layer-perturbation attack.

We evaluate FedGMark on four real-world graph datasets (MUTAG, PROTEINS, DD, and COLLAB) and three FedGL models including Fed-GIN, Fed-GSAGE, and Fed-GCN, whose base GL models are GIN [20], GSAGE [17], and GCN [14], respectively. Extensive experimental results show FedGMark achieves high main accuracy and watermark accuracy under no attacks and watermark removal attacks, high certified watermark accuracy, and significantly outperforms the existing method. Such good results demonstrate the potential of FedGMark as a watermarking method to protect the ownership of FedGL models.

We summarize our main contributions of this paper as follows:

* To our best knowledge, this is the first work to protect the ownership of emerging FedGL models.
* We propose a certifiably robust backdoor-based watermarking method FedGMark for FedGL.
* We validate the effectiveness of FedGMark in multiple FedGL models and real-world graph datasets under no attack, existing backdoor removal attacks, and worst-case layer-perturbation attacks.

## 2 Preliminaries

### Federated Graph Learning (FedGL)

Given a graph \(G=(\mathbb{V},\mathbb{E})\) as input, a GL model for graph classification learns a graph classifier \(f\) that outputs a label \(f(G)=y\in\mathbb{Y}\) for a graph. Here, \(\mathbb{V},\mathbb{E},\mathbb{Y}\) represent the set of nodes, edges, and labels, respectively. \(\mathbf{A}\in\{0,1\}^{|\mathbb{V}|\times|\mathbb{V}|}\) is the binary adjacency matrix of \(G\), where \(\mathbf{A}[v_{j},v_{k}]=1\) if there exists an edge between nodes \(v_{j}\) and \(v_{k}\), and \(0\) otherwise, with \(|\mathbb{V}|\) the total number of nodes. FedGL employs FL techniques [16] to collaboratively train GL models with a set of (e.g., \(T\)) clients \(\mathbb{T}=\{1,\cdots,T\}\) and a server. Assuming each client \(i\in\mathbb{T}\) has a set of graphs \(\mathbb{G}^{i}\), we illustrate the training process of FedGL using the \(e\)-th epoch as an example: 1) Initially, the server distributes the global model parameters \(\theta_{e}\) to a randomly selected subset of clients \(\mathbb{T}_{e}\), where \(\mathbb{T}_{e}\subseteq\mathbb{T}\). 2) Upon receiving \(\theta_{e}\), each client \(i\) trains its local model parameter \(\theta_{e}^{i}\) with its own graphs \(\mathbb{G}^{i}\) and updates its model parameters via SGD, i.e., \(\theta_{e}^{i}=\theta_{e-1}^{i}-\eta\partial_{\theta_{e}}L(\theta_{e};\mathbb{ G}^{i})\), where \(L(\theta_{e};\mathbb{G}^{i})\) represents a loss function, e.g., cross-entropy loss. After training, client \(i\) submits its update model parameters \(\theta_{e}^{i}\) to the server. 3) The server aggregates local model parameters of the selected clients i.e., \(\{\theta_{e}^{i}:i\in\mathbb{T}_{e}\}\) and updates the global model parameter, e.g., \(\theta_{e+1}=\frac{1}{|\mathbb{T}_{e}|}\sum_{i\in\mathbb{T}_{e}}\theta_{e}^{i}\) via the average aggregation [16], for the next epoch. This iterative process continues until the global model converges or reaches the maximum number of epochs.

### Backdoor-based Watermarking for GL

Backdoor-based watermarking [15, 1, 1, 14] adopts the idea of backdoor attack [1, 14, 15] from the adversarial realm to facilitate model ownership verification. To watermark the GL

Figure 1: Overall pipeline of the proposed certified watermarks.

model, assume the model owner has a set of clean graphs \(\mathbb{G}\) and selects a subset of graphs \(\mathbb{G}_{w}\subset\mathbb{G}\) to inject the watermark. In the existing method [Xu _et al._, 2023], the model owner first generates a random graph (e.g., via the ER-model) as the watermark for each to-be-watermarked graph. For instance, for a graph \(G\in\mathbb{G}_{w}\) with label \(y\), the generated random graph is \(G_{s}\) (its size is often smaller than \(G\)). The owner then attaches \(G_{s}\) to \(G\) to produce the watermarked graph \(G_{w}\), where nodes in \(G\) are randomly chosen and the edge status of these nodes are replaced by edges in \(G_{s}\). Finally, the owner assigns a desired label different from \(y\) to \(G_{w}\). The watermarked graphs together with the clean graphs are used to train the GL model. During model ownership verification, the one who can predict a high accuracy on these watermarked graphs can claim to be the model owner.

We note this method can be extended to watermark FedGL models, where each client can generate its own random graphs as the watermark and train its local model with the watermarked graphs and clean graphs. The server then aggregates the watermarked local models to update the global model.

### Watermark Removal Attacks

We consider three possible watermark removal attacks aiming to infringe the FedGL model ownership: distillation and finetuning from [Shafieinejad _et al._, 2021], and our proposed layer-perturbation attack. In all attacks, the attacker (e.g., malicious user) is assumed to know the target watermarked model.

**1) Distillation.** This attack has access to some unlabeled data sampled from the same data distribution. To remove watermarks without affecting the target model's main task performance, the attacker uses the unlabeled data to distill the target model during training. Specifically, the attacker initializes its model with the target model and labels the unlabeled data by querying the target model. The attacker's model is then updated with these unlabeled data and their predicted labels.

**2) Finetuning.** This attack assumes the attacker has some labeled data. The attacker then leverages the labeled data to further finetune the target model in order to forget the watermark. This attack is shown to pose a greater threat than the distillation attack [Bansal _et al._, 2022].

**3) Layer-perturbation attack.** This attack also assumes the attacker has some labeled data. As knowing the target watermarked model (and hence the architecture), the attacker can mimic training an unwatermarked model with the same architecture as the target model using the labeled data. To further test the model robustness, we assume the attacker also knows some true watermarked samples, similar to [Jiang _et al._, 2023]. Then, the attacker can replace _any_ layer(s)' parameters of the target model with those from the unwatermarked model to maximally reduce the watermark accuracy on its watermarked samples, while maintaining the main task performance. Our results (e.g., in Table 1) show this layer-perturbation attack (even only perturbing 1 layer parameters) is much more effective than the other two attacks (even though the whole model parameters can be perturbed).

### Threat Model

We follow existing methods [Shafieinejad _et al._, 2021; Bansal _et al._, 2022; Xu _et al._, 2023; Jiang _et al._, 2023], where the adversary is assumed to know all details of the pretrained watermarked FedGL model, but does not tamper with the training process. This means all clients and the server are benign and follow the federated training protocol, and the attack happens at the testing/inference time. We highlight this is in stark contrast to the training-time Byzantine attack on FL where some clients are malicious and they manipulate the training process.

**Attacker's knowledge.** The attacker has white-box access to the pretrained watermarked FedGL model. In addition, the attacker may also know some clean (unlabeled or labeled) training data, as well as watermarked data. Note that this setting actually makes our defense design the most challenging. If the defense can successfully defend against the strongest white-box attack on the watermarked FedGL model, it will also be effective against weaker attacks, such as black-box attacks.

**Attacker's capability.** The attacker can modify the pretrained model via leveraging its white-box access to the trained model and its hold training and watermarked data. For instance, the attacker can finetune the pretrained model via the labeled training data. More details of the capabilities of considered attacks are described in Section 2.3.

**Attacker's goal.** The attacker aims to remove the watermark based on its knowledge and capability, while maintaining the model utility. This allows it to illegally use the model without detection.

FedGMark: Our Certified Robust Watermark for FedGL

### Motivation and Overview

Recall that the random graph based watermark is unable to ensure high watermark accuracy for protecting FedGL (as shown in Table 1). This is because such random watermark does not use any graph structure or client information during FedGL training. Our results also show this method is vulnerable to the three watermark removal attacks. These weaknesses inspire us to design a more effective and robust watermarking method specially for FedGL model ownership verification.

We propose FedGMark, the first certified robust backdoor-based watermarking method for FedGL. FedGMark comprises two main components: _Customized Watermark Generator (CWG)_ and _Robust Model Loader (RML)_ (as depicted in Figure 1). The CWG module utilizes the unique property of each client, as different clients could have different properties (e.g., distributions of their graph data) and their optimal watermark could be different. Particularly, CWG learns the customized watermark for each graph using its structure information, and outputs a set of diversified watermarked graphs for each client. Further, inspired by existing GNNs (Xu _et al._, 2019), the RML module designs a new GL model that consists of multiple submodels, each being any existing GL model. It also introduces a voting classifier for aggregating the prediction results from the submodels. Under this design, FedGMark can be proved to be certified robust against the _worst-case_ layer-perturbation attack, once the number of perturbed layers is bounded. The model owner (e.g., participating clients in FedGL) adopts the designed GL model to train the local watermarked model with the learnt watermarked graphs and the remaining clean graphs. After the server-client training terminates, the ownership of the trained FedGL model can be verified via measuring its performance on a set of testing graphs injected with the _global watermark_, which is the integration of all clients' local watermarks.

### Customized Watermark Generator (CWG)

CWG consists of two networks: \(\mathrm{GatingNet}\) and \(\mathrm{KeyNet}\). \(\mathrm{GatingNet}\) designs the watermark for each graph separately using the edge information, while \(\mathrm{KeyNet}\) learns client-wise watermarking style using predefined keys (e.g., client ID in this paper). The customized watermark for each client's graph is then decided using the output of \(\mathrm{GatingNet}\) and \(\mathrm{KeyNet}\). _Detailed network architectures of CWG can be seen in Table 7 in Appendix C_. We demonstrate how CWG can learn a customized watermark using a graph \(G^{i}=(\mathbb{V}^{i},\mathbb{E}^{i})\) from client \(i\) as an instance. The details are as follows:

* We first randomly select \(n_{w}\) nodes \(\mathbb{V}^{i}_{w}=\{v_{1},\cdots,v_{n_{w}}\}\) from \(\mathbb{V}^{i}\) as watermark nodes and construct a corresponding mask matrix \(\mathbf{M}^{i}\in\{0,1\}^{|\mathbb{V}^{i}|\times|\mathbb{V}^{i}|}\) such that \(\mathbf{M}^{i}[v_{j},v_{k}]=1\) if \(v_{j},v_{k}\in\mathbb{V}^{i}_{w}\), and \(0\) otherwise. We also update the adjacency matrix \(\mathbf{A}^{i}\) of \(G^{i}\) according to \(\mathbb{V}^{i}_{w}\), i.e., setting \(\mathbf{A}^{i}[v_{j},v_{k}]=0,\forall v_{j},v_{k}\in\mathbb{V}^{i}_{w}\). This allows us focus on learning the edge status between watermarked nodes.
* Given the client \(i\)'s ID string \(k^{i}\), we utilize a cryptographic hash function, such as MD5, to convert it into an integer (e.g., \(128\)-bit long with the integer range \([0,2^{128}-1]\)). This integer is then employed as a seed to produce a key matrix \(\mathbf{K}^{i}\in\mathbb{R}^{|\mathbb{V}^{i}|\times|\mathbb{V}^{i}|}\). Then, we employ \(\mathrm{GatingNet}\) and \(\mathrm{KeyNet}\) to extract edge features and key features, resulting in \(\tilde{\mathbf{A}}^{i}=\mathrm{GatingNet}(\mathbf{A}^{i})\in[0,1]^{|\mathbb{V}^ {i}|\times|\mathbb{V}^{i}|}\) and \(\tilde{\mathbf{K}}^{i}=\mathrm{KeyNet}(\mathbf{K}^{i})\in[0,1]^{|\mathbb{V}^ {i}|\times|\mathbb{V}^{i}|}\), respectively.
* We finally learn the customized watermark for \(G^{i}\) by integrating \(\tilde{\mathbf{A}}^{i}\), \(\tilde{\mathbf{K}}^{i}\), and \(\mathbf{M}^{i}\), and obtain the corresponding watermarked graph as \(G^{i}_{w}=(\mathbb{V}^{i},\mathbb{E}^{i}_{w})\). Here \(\mathbb{E}^{i}_{w}\) is the set of edges according to the updated adjacency matrix \(\mathbf{A}^{i}\oplus\mathbf{W}^{i}\), where \(\oplus\) is the element-wise addition and \(\mathbf{W}^{i}=\mathbb{I}((\tilde{\mathbf{A}}^{i}\odot\tilde{\mathbf{K}}^{i})>0.5)\odot\mathbf{M}^{i}\) contains the edge status between the watermark nodes \(\mathbb{V}^{i}_{w}\). Here, \(\odot\) is the element-wise product, \(\mathbb{I}(p)\) is an indicator function returning \(1\) if \(p\) is true, and \(0\) otherwise. We adopt \(0.5\) as a threshold to decide the presence of edges between watermarked nodes.

### Robust Model Loader (RML)

This module aims to design a new GL model that is provably robust to the layer-perturbation attack. Towards this end, we design a GL model architecture to incorporate multiple submodels; and devise a majority voting-based ensemble classifier on top of the predictions of these submodels.

**Architecture of the proposed GL model.** Intuitively, each client can take a base GL model (e.g., GIN (Xu _et al._, 2019)) and split it according to the layer indexes to obtain multiple submodels. For instance, a \(8\)-layer GIN can be represented with layer indexes \(\{l_{1},\cdots,l_{8}\}\). Splitting this GIN into \(4\) submodels \(\{\text{GIN}_{1},\cdots,\text{GIN}_{4}\}\) with layer indexes \(\{l_{1},l_{2}\},\cdots,\{l_{7},l_{8}\}\) means \(\text{GIN}_{i}\) contains layers\(\{l_{2i-1},l_{2i}\}\), from the GIN. However, submodels splitted in this way are coupled from each other, making them unable to defend against layer-perturbation attacks. To tackle this problem, we design the novel GL model \(\theta\) that is an ensemble of a set of \(S\)_independent_ submodels \(\{\vartheta_{1},\vartheta_{2},\cdots,\vartheta_{S}\}\), where each submodel \(\vartheta_{i}\) is a base GL model. This approach can hence be easily adapted to any existing FedGL. Further, to prevent homogeneity, we define varying channels for these submodels to diversify them. Details of the model architecture are shown in Table 8 in Appendix C.

**A majority-voting based ensemble classifier.** The designed GL model architecture inspires us to leverage the idea of ensemble classifier, which can combine the predictions of base "weak" classifiers. Specifically, we propose a majority voting-based ensemble classifier to combine the predictions of the submodels. Given a testing graph \(G\) and a graph classifier \(f\), we denote the prediction of the submodel \(\vartheta_{i}\) for \(G\) as \(y=f(\vartheta_{i},G)\in\mathbb{Y}\). For a GL model \(\theta\) with \(S\) submodels \(\{\vartheta_{1},\vartheta_{2},\cdots,\vartheta_{S}\}\), we can count the submodels that classify \(G\) to be \(y\) as \(N_{y}=\sum_{i=1}^{S}\mathbbm{I}(f(\vartheta_{i},G)=y)\). Then we introduce our majority-voting based ensemble classifier \(g\) to classify \(G\) as: \(g(\theta,G)=\operatorname*{arg\,max}_{y\in\mathbb{Y}}N_{y}\). In cases of ties, our ensemble classifier \(g\) selects the label with a smaller index.

### Training the Proposed FedGL Model

The overall training process consists of three iterative steps: 1) training the proposed GL model in all clients; 2) training the CWG module in _watermarked clients_, i.e., the clients that aim to inject watermarked graphs for protecting the model ownership; and 3) aggregating the clients' GL models to produce the target watermarked model. The final global model is the learnt watermarked FedGL model. Details of training can be seen in Algorithm 1 in the Appendix.

**Step 1: Training the proposed GL model.** Assume we have \(T_{w}\) watermarked clients with indexes \([1,T_{w}]\). For each watermarked client \(i\), we split its training graphs \(\mathbb{G}^{i}\) into the watermarked graphs \(\mathbb{G}^{i}_{w}\) with a target label, say \(y_{w}\), and remaining clean graphs \(\mathbb{G}^{i}_{c}\), and then customize the watermark for each graph in \(\mathbb{G}^{i}_{w}\) using the \(\mathrm{CWG}\) module (see **Step 2**). Given the client's GL model \(\theta^{i}\) with \(S\) submodels \(\{\vartheta^{i}_{1},\vartheta^{i}_{2},\cdots,\vartheta^{i}_{S}\}\), we train each submodel \(\vartheta^{i}_{j}\) via minimizing the loss on \(\mathbb{G}^{i}_{c}\) and \(\mathbb{G}^{i}_{w}\), i.e., \(\vartheta^{i}_{j}=\operatorname*{arg\,min}_{\vartheta^{i}_{j}}L(\vartheta^{i} _{j};\mathbb{G}^{i}_{c}\bigcup\mathbb{G}^{i}_{w})\). For an unwatermarked client \(k\in[T_{w}+1,T]\), we utilize all clean graphs \(\mathbb{G}^{k}\) to train each submodel \(\vartheta^{k}_{j}\) separately, i.e., \(\vartheta^{k}_{j}=\operatorname*{arg\,min}_{\vartheta^{k}_{j}}L(\vartheta^{k} _{j};\mathbb{G}^{k})\).

**Step 2: Training the CWG.** We denote the parameters of the CWG module for a watermarked client \(i\in[1,T_{w}]\), as \(\omega^{i}\). The parameters include two networks, \(\mathrm{GatingNet}\) and \(\mathrm{KeyNet}\). Each client \(i\) trains its CWG \(\omega^{i}\) to ensure that the generated watermarks be effective and diverse. Formally, we have \(\omega^{i}=\operatorname*{arg\,min}_{\omega^{i}}L(\theta^{i};\mathbb{G}^{i}_ {w})\), \(i\in[1,T_{w}]\).

**Step 3: Aggregating clients' GL models.** The server averages GL models \(\{\theta^{i}\}_{i\in T}\) to produce the global model \(\theta\), and distributes this model to selected clients in the next iteration.

### Model Ownership Verification

When suspecting the target FedGMark model \(\theta\) is illegally used by others, the model owner (all the participating clients or their representative) can recruit a trusted judge for model ownership verification. Typically, the judge requests both the true model owner and the illegal party to provide some test data for verification. Only when the one knows the predictions by the target model for the provided test data by both parties, the judge will confirm this party the model ownership. In particular, besides providing the clean data \(\mathbb{G}^{i}_{c}\) by both parties that behave normally, the true model owner especially provides the designed watermarked data \(\mathbb{G}^{i}_{w}\) that only s/he knows the model behaves on. As a result, both parties know the prediction results on \(\mathbb{G}^{i}_{c}\), but the illegal party is hard to predict accurately on \(\mathbb{G}^{i}_{w}\) provided by the true model owner.

### Certified Robustness Guarantees against Layer-Perturbation Attacks

We show the above design, with any layer-perturbation attack, ensures the predictions of the learnt watermarked FedGL model and its compromised counterpart for the watermarked graphs are consistent, once the number of perturbed layers is bounded. Given the target watermarked FedGL model \(\theta\) and its \(S\) submodels \(\{\vartheta_{1},\cdots,\vartheta_{S}\}\), we denote \(\theta^{\prime}\) as the comprised model and \(\{\vartheta^{\prime}_{1},\cdots,\vartheta^{\prime}_{S}\}\) as its \(S\) submodels. For each watermarked graph \(G_{w}\), we use the ensemble classifier \(g\) on submodels' predictions, i.e., its predictions on \(\theta\) and \(\theta^{\prime}\) are \(g(\theta,G_{w})=\operatorname*{arg\,max}_{y\in\mathbb{Y}}N_{y}\), and \(g(\theta^{\prime},G_{w})=\operatorname*{arg\,max}_{y\in\mathbb{Y}}N^{\prime}_{y}\), respectively, where \(N_{y}=\sum_{i=1}^{S}\mathbbm{I}(f(\vartheta_{i},G)=y)\) and \(N^{\prime}_{y}=\sum_{i=1}^{S}\mathbbm{I}(f(\vartheta^{\prime}_{i},G)=y)\). Then we have the following result on guaranteeing the number of perturbed layers on the target watermarked model.

**Theorem 1** (Certified number of perturbed layers \(r\).): _Let \(\theta\), \(\theta^{\prime}\), \(g\), and \(G_{w}\) be above defined. Suppose \(N_{A}\) and \(N_{B}\) are the largest and second largest count outputted by \(g\) on \(G_{w}\). For any layer-perturbation attack, we have \(g(\theta,G_{w})=g(\theta^{\prime},G_{w})\), when the number of perturbed layers \(r\) satisfies:_

\[r\leq r^{*}=\left(N_{A}-N_{B}+\mathbbm{1}[A<B]-1\right)/2,\] (1)

_where \(\mathbbm{1}[\cdot]\) is the indicator function and \(r\) is called the certified number of perturbed layers._

We also show the tightness of our derived \(r^{*}\) in the following theorem:

**Theorem 2** (Tightness of \(r^{*}\).): _Without using extra information of \(f\), our derived \(r^{*}\) in Theorem 1 is tight. I.e., \(r^{*}\) is the maximum number of perturbed layers tolerated by our target watermarked model._

The proofs of Theorems 1 and 2 are deferred to Appendix A.

## 4 Experiments

In this section, we comprehensively evaluate FedGMark on multiple datasets, FedGL models, attack baselines, and experimental settings. More experimental results and discussions are in Appendix.

### Experimental Setup

**Datasets and models.** We evaluate our FedGMark on four real-world graph datasets for graph classification: MUTAG [Debnath _et al._, 1991], PROTEINS [Borgwardt _et al._, 2005], DD [Dobson and Doig, 2003], and COLLAB [Yanardag and Vishwanathan, 2015]. Details about the statistics of those datasets are shown in Table 6 in Appendix C. Following prior work Shen _et al._[2022]; Xia _et al._[2024], we choose the well-known GIN [Xu _et al._, 2019], GSAGE [Hamilton _et al._, 2017], and GCN [Kipf and Welling, 2017] as the GL model. All these network architectures involved in the experiments are detailed in Table 8 in Appendix C.

**Parameter setting.** We implement our method using one NVIDIA GeForce GTX 1080 Ti GPU. In FedGL, we use \(T=40\) clients in total and train the model 200 iterations. The server randomly selects \(50\%\) clients in each iteration. We define the target label of watermarking graphs as \(1\), and each participating client randomly selects \(10\%\) graphs with labels not \(1\) as the watermarking graphs. We extensively validate the effectiveness and robustness of the proposed watermarking method with the following hyperparameters details: the number of submodels \(S=\{4,8,16\}\), the number watermarked clients \(T_{w}=\{5,10,20\}\) (both \(S\) and \(T_{w}\) are halved on MUTAG due to less data), the watermarked nodes \(n_{w}=\{3,4,5\}\), and the number of perturbed layers \(r=\{1,\cdots,5\}\) in the layer-perturbation attack. By default, we set \(S=4\), \(T_{w}=10\), \(n_{w}=4\), \(r=1\). While studying the impact of a hyperparameter, we fix the others as the default value.

**Evaluation metric.** We use three metrics for evaluation: the main task accuracy (MA), watermark accuracy (WA), and certified WA (CWA@\(r\)). An effective and robust watermarked model is expected to achieve both high MA and WA. CWA evaluates the certified robustness of FedGMark against layer-perturbation attacks. CWA@\(r\) is defined as the fraction of testing graphs that are provably predicted as the target label, when at most \(r\) layers in client models can be arbitrarily perturbed.

**Attack baselines.** We evaluate FedGMark against existing watermark removal attacks including distillation and finetuning, and our proposed layer-perturbation attack. In our setting, the layer-perturbation attack can replace _any_ layer(s)' parameters of the target watermarked model with those from the un watermarked model to maximally reduce the watermark accuracy on its watermarked graphs. When perturbing multiple layers, we utilize a greedy algorithm to decide the perturbed layers--we search for one optimal perturbed layer at each step. Specifically, we first traverse perturbing layers in the watermarked model and find the one that maximally reduces the watermark accuracy. We then search for the remaining layers based on this one and continue the process.

### Empirical Results: MA and WA

#### 4.2.1 Results under the Default Setting

We first assess our FedGMark against empirical and layer-perturbation attacks under the default setting. Experimental results are provided in Table 2. We have the following observations:

* **1) Our FedGMark significantly outperforms the existing method under no attack.** Recall in Table 1 that [Xu _et al._, 2023] obtains \(<60\%\) WAs across all datasets and FedGL models. In contrast, FedGMark can achieve WAs \(>70\%\) in almost all cases, while having similar MAs.

Such significant improvements confirm the superiority of our learnt watermarks over the random watermarks in [15]. Figure 2 also visualizes example learnt watermarks and we can see these watermarks are diversified due to the proposed CWG.
* **2) Our FedGMark exhibits resistance to existing empirical attacks.** As shown in Table 1, existing methods are vulnerable to distillation and finetuning attacks. In contrast, the WAs of FedGMark under these two attacks are almost the same as those under no attack, demonstrating FedGMark is robust to the existing attacks.
* **3) Our FedGMark is resilient to the proposed layer perturbation attack.** We notice that the existing method has difficulties in defending against the proposed layer-perturbation attacks and shows an unsatisfactory WA, e.g., \(<25\%\) WA in almost all cases under the 1-layer perturbation attack. Conversely, our FedGMark can obtain a close WA compared to that without attack. This verifies our ensemble classifier in FedGMark is capable of resisting the 1-layer perturbation attack.

#### 4.2.2 Impact of Hyperparameters

This section studies the impact of hyperparameters in FedGMark against the layer-perturbation attack.

**Impact of #submodels \(S\).** We first examine the impact of \(S\) on the performance of FedGMark and report the results against the 1-layer perturbation attack in Table 3. We observe that the learnt watermarked model can resist to the 1-layer perturbation attack under all \(S\) and both the MAs and WAs are also similar. This indicates the number of submodels marginally affects the watermarking performance against the 1-layer perturbation attack.

\begin{table}
\begin{tabular}{c||c|c c c c c c c||c c c c c c} \hline \hline Datasets & Attack & Fed-GIN & Fed-GSAGE & Fed-GCN & Datasets & Attack & Fed-GIN & Fed-GSAGE & Fed-GCN \\  & & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) & & & MA\(\uparrow\) & WA\(\uparrow\) & WA\(\uparrow\) & WA\(\uparrow\) \\ \hline \hline \multirow{3}{*}{MUTAG} & None & **0.83** & **0.90** & **0.85** & **0.89** & **0.83** & **0.83** & & & None & **0.72** & **0.86** & **0.72** & **0.82** & **0.73** & **0.84** \\  & Distillation & 0.83 & 0.87 & 0.81 & 0.70 & 0.80 & 0.80 & PROTEINS & Distillation & 0.70 & 0.84 & 0.70 & 0.80 & 0.70 & 0.79 \\  & Finetuning & 0.82 & 0.88 & 0.79 & 0.76 & 0.78 & 0.81 & Finetuning & 0.70 & 0.84 & 0.71 & 0.82 & 0.72 & 0.80 \\  & 1-Layer Pert. & **0.82** & **0.90** & **0.83** & **0.91** & **0.81** & **0.84** & & & 1-Layer Pert. & **0.73** & **0.86** & **0.74** & **0.83** & **0.74** & **0.85** \\ \hline \multirow{3}{*}{DD} & None & **0.73** & **0.65** & **0.74** & **0.57** & **0.74** & **0.56** & & None & **0.73** & **0.75** & **0.74** & **0.74** & **0.73** & **0.70** \\  & Distillation & 0.71 & 0.63 & 0.74 & 0.56 & 0.72 & 0.56 & COLLAB & Distillation & 0.73 & 0.71 & 0.72 & 0.73 & 0.70 & 0.69 \\  & Finetuning & 0.72 & 0.62 & 0.74 & 0.57 & 0.74 & 0.55 & Finetuning & 0.72 & 0.70 & 0.75 & 0.71 & 0.72 & 0.67 \\  & 1-Layer Pert. & **0.72** & **0.65** & **0.75** & **0.55** & **0.73** & **0.57** & & & 1-Layer Pert. & **0.72** & **0.76** & **0.73** & **0.74** & **0.74** & **0.71** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results of our FedGMark under empirical watermark removal attacks.

Figure 2: Example learnt watermarks and watermarked graphs by our FedGMark. CWGs generated by different clients produce unique watermarks, characterized by distinct edge connection patterns.

**Impact of #watermarking clients \(T_{w}\).** The results on \(T_{w}=5\) and \(T_{w}=20\) are shown in Figure 4 in Appendix. We see that: _the larger number of clients generating watermarks, the better our watermarking performance_. For instance, the WA improves by \(11\%\), \(9\%\), and \(7\%\), respectively on the three FedGL models on PROTEINS when \(T_{w}\) increases from \(5\) to \(20\). This is because more watermark clients (thus more watermarked graphs) can ensure the FedGL model better learns the relation between the watermarks and the target label.

**Impact of the watermark size \(n_{w}\).** We also investigate \(n_{w}=3\) and \(n_{w}=5\), and the results are presented in Figure 5 in Appendix. Similarly, FedGMark obtains higher WA with larger \(n_{w}\). This is because a larger watermark size can facilitate the trained FedGL model better learn the watermark.

**Impact of #perturbed layers.** Finally, Table 4 shows the results of FedGMark vs. different #perturbed layers. We observe that: 1) The WA decreases as increasing the #perturbed layers. This is because the attacker has more attack capability by perturbing more layers. 2) Despite the theoretically guaranteed CWA being \(0\) at #perturbed layers \(\geq 3\) as later depicted in the Figure 3, our empirical watermarking performance can still achieves WA from \(48\%\) to \(58\%\). Note that the change in MA is \(<4\%\) in all cases, compared with Table 2.

#### 4.2.3 Ablation Study

We investigate the contribution of each module in FedGMark, including the \(\mathrm{GatingNet}\) and \(\mathrm{KeyNet}\) in CWG, and the submodels used in RML. The results under the default setting and no attack are shown in Table 5. First, \(\mathrm{GatingNet}\) plays a crucial role on generating more effective watermarks and results in an improvement in WA from \(8\%\) to \(21\%\). Second, KeyNet facilitates watermark differentiation between clients, leading to an improve on WA from 5% to 8%. Third, the submodels in RML has a negligible impact on WA/MA. However, it is important to defend against the layer-perturbation attack, as shown in Section 4.3.

\begin{table}
\begin{tabular}{c||c|c c c c c|c c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{\#Perturbed Layers} & 1 & 2 & 3 & 4 & 5 & \multirow{2}{*}{Datasets} & \#Perturbed Layers & 1 & 2 & 3 & 4 & 5 \\  & (Net) & WA\(\uparrow\) & WA\(\uparrow\) & WA\(\uparrow\) & WA\(\uparrow\) & WA\(\uparrow\) & & & & & & & \\ \hline \multirow{4}{*}{Muttag} & Fed-GINE & 0.90 & 0.89 & 0.54 & 0.23 & 0.16 & \multirow{4}{*}{PROTEINS} & Fed-GINE & 0.86 & 0.84 & 0.58 & 0.46 & 0.39 \\  & Fed-GSAGE & 0.91 & 0.89 & 0.55 & 0.21 & 0.13 & & & & & & & & \\  & Fed-GCN & 0.84 & 0.83 & 0.49 & 0.24 & 0.18 & & & & & & & \\ \hline \multirow{4}{*}{DD} & Fed-GINE & 0.65 & 0.61 & 0.56 & 0.32 & 0.21 & \multirow{4}{*}{COLLAB} & Fed-GSAGE & 0.74 & 0.73 & 0.57 & 0.35 & 0.25 \\  & Fed-GSAGE & 0.55 & 0.55 & 0.49 & 0.28 & 0.24 & & & & & & & \\ \cline{1-1}  & Fed-GCN & 0.57 & 0.54 & 0.48 & 0.29 & 0.24 & & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 4: Impact of #perturbed layers on FedGMark against our layer-perturbation attack. Compared with Table 2, the change in MA is less than \(4\%\) in all cases.

\begin{table}
\begin{tabular}{c||c c c|c c c c c c c c} \hline \hline \multirow{2}{*}{**Models**} & G.Net & K.Net & RML & \multicolumn{2}{c}{Muttag} & PROTEINS & DD & COLLAB \\  & & & & & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) & WA\(\uparrow\) \\ \hline
**(a)** & ✓ & ✓ & ✓ & **0.81** & **0.90** & **0.72** & **0.86** & **0.73** & **0.65** & **0.73** & **0.75** \\
**(b)** & ✓ & ✓ & & 0.81 & 0.84 & 0.72 & 0.80 & 0.72 & 0.57 & 0.72 & 0.70 \\
**(c)** & ✓ & ✓ & 0.80 & 0.69 & 0.72 & 0.68 & 0.72 & 0.48 & 0.73 & 0.67 \\
**(d)** & ✓ & ✓ & & 0.81 & 0.89 & 0.73 & 0.86 & 0.71 & 0.66 & 0.72 & 0.75 \\
**(e)** & & & & 0.82 & 0.47 & 0.71 & 0.56 & 0.73 & 0.39 & 0.73 & 0.57 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Impact of different modules in FedGMark.

Figure 3: CWA vs. #perturbed layers \(r\) in the layer-perturbation attack.

### Certified Robustness Results: Certified WA

In this section, we evaluate the certified robustness of FedGMark against the layer perturbation attack. The CWAs on the three FedGL models and four datasets are depicted in Figure 3. We have several observations: 1) FedGMark achieves promising provable robustness results against the _worst-case_ layer perturbation attack, when the #perturbed layers is within the certified range in Eqn (1). For instance, when \(S=4\) and \(0<r\leq 2\), CWA is close to WA without attack (\(r=0\)). 2) As \(S\) increases, the certified #perturbed layers also increases, showing that a more number of submodules in RML can better provably defend against the layer-perturbation attack. For instance, when \(S=16\), the CWA is \(86\%\) on MUTAG even when 5 any layers in the global model are arbitrarily perturbed, while it is 0 when \(S=4\). However, this is at the cost of requiring more computational resources (e.g., GPU memory, runtime). As shown in Table 10 in Appendix, the runtime is linearly to \(S\).

## 5 Related Work

**Backdoor-based watermarking for centralized models on non-graph data.** Many backdoor-based watermarking methods [20, 21, 22, 23, 24, 25, 26] have been proposed that can _empirically_ protect the model ownership. These methods mainly focus on centralized learning models on non-graph (e.g., image) data. Compared with non-graph data, graph data have unique graph structure information, e.g., entities are connected by links. Similarly, compared with centralized models, FL models are collaboratively trained by multiple clients, which could have their own uniqueness. Bansal _et al._ [2022] is the first watermarking method for centralized non-graph models with certified guarantee. However, its certified robustness performance is unsatisfactory.

**Backdoor-based watermarking for FL models on non-graph data.** A few recent works Tekgul _et al._ [2021]; Li _et al._ [2022]; Bansal _et al._ [2022] design backdoor-based watermarks for protecting the ownership of FL models on non-graph data, where the watermark can be injected into client's data or server's validation data. For instance, Tekgul _et al._ [2021] presented WAFFLE, an approach to watermark DNN models by incorporating a re-training step via watermarked data at the server. Li _et al._ [2022] leveraged each client's private watermark to verify FL model ownership, ensuring non-conflicting watermarks across different clients. However, these techniques cannot be directly applied to graph data, as they often require fixed input data, while graph data often have varying sizes. Moreover, all these methods do not provide guaranteed watermarking performance under the attack.

**Backdoor-based watermarking for centralized GL models on graph data.** The only work [21] handling graph data uses random graph as the watermark. This method can be extended to the FedGL model, but its watermarking performance is far from satisfactory-especially vulnerable to watermark removal attacks such as distillation, finetuning, and the layer perturbation attack.

**Backdoor attacks for centralized and federated GL models on graph data.** Several works [22, 23, 24] design backdoor attacks to manipulate GL models, enabling the attacker to influence the learned model to serve its purpose-the model will predict the attacker-chosen label for test graphs once they contain a predefined trigger. For instance, Zhang _et al._ [2021] uses random subgraphs as the trigger to backdoor centralized GL models, while Yang _et al._ [2024] learns subgraph trigger to backdoor FedGL models. Note that the goal of these works is orthogonal to ours.

## 6 Conclusion

We protect the model ownership of emerging FedGL trained on distributed graph data, and use the _de facto_ backdoor-based watermarking method. We develop the first certifiably robust backdoor-based watermarking method FedGMark for FedGL. FedGMark demonstrates the capability of achieving high empirical watermarking performance under no attack, under existing backdoor removal attacks and the proposed stronger layer-perturbation attack. FedGMark is also provably robust against the worst-case layer-perturbation attack, once the number of perturbed layers is bounded by Theorem 1.

**Acknowledgments.** We thank all anonymous reviewers for the constructive comments. Li is partially supported by the National Natural Science Foundation of China under Grant No. 62072208, Key Research and Development Projects of Jilin Province under Grant No. 20240302090GX. Hong is partially supported by the National Science Foundation under grant No. CNS-2302689, CNS-2308730, CNS-2319277 and CMMI-2326341. Wang is partially supported by the National Science Foundation under grant No. ECCS-2216926, CNS-2241713, CNS-2331302 and CNS-2339686.

## References

* Adi et al. (2018) Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning your weakness into a strength: Watermarking deep neural networks by backdooring. In _27th USENIX Security Symposium (USENIX Security 18)_, pages 1615-1631, 2018.
* Baek et al. (2023) Jinheon Baek, Wonyong Jeong, Jiongdao Jin, Jaehong Yoon, and Sung Ju Hwang. Personalized subgraph federated learning. In _International Conference on Machine Learning_, pages 1396-1415. PMLR, 2023.
* Bagdasaryan et al. (2020) Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In _International conference on artificial intelligence and statistics_, pages 2938-2948. PMLR, 2020.
* Bansal et al. (2022) Arpit Bansal, Ping-yeh Chiang, Michael J Curry, Rajiv Jain, Curtis Wigington, Varun Manjunatha, John P Dickerson, and Tom Goldstein. Certified neural network watermarks with randomized smoothing. In _International Conference on Machine Learning_, pages 1450-1465. PMLR, 2022.
* Blanchard et al. (2017) Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. _Advances in neural information processing systems_, 30, 2017.
* Borgwardt et al. (2005) Karsten M Borgwardt, Cheng Soon Ong, Stefan Schonauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. _Bioinformatics_, 21(suppl_1):i47-i56, 2005.
* Debnath et al. (1991) Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. _Journal of medicinal chemistry_, 34(2):786-797, 1991.
* Dobson and Doig (2003) Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. _Journal of molecular biology_, 330(4):771-783, 2003.
* Gilbert (1959) Edgar N Gilbert. Random graphs. _The Annals of Mathematical Statistics_, 30(4):1141-1144, 1959.
* Hamilton et al. (2017) Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30, 2017.
* He et al. (2022) Chaoyang He, Emir Ceyani, Keshav Balasubramanian, Murali Annavaram, and Salman Avestimehr. Spreadgnn: Decentralized multi-task federated learning for graph neural networks on molecular data. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 6865-6873, 2022.
* Jiang et al. (2023) Zhengyuan Jiang, Minghong Fang, and Neil Zhenqiang Gong. Ipcert: Provably robust intellectual property protection for machine learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3612-3621, 2023.
* Karimireddy et al. (2020) Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scafffold: Stochastic controlled averaging for federated learning. In _International conference on machine learning_, pages 5132-5143. PMLR, 2020.
* Kipf and Welling (2017) Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _In International Conference on Learning Representations (ICLR)_, 2017.
* Lansari et al. (2023) Mohammed Lansari, Reda Bellafqira, Katarzyna Kapusta, Vincent Thouvenot, Olivier Bettan, and Gouenou Coatrieux. When federated learning meets watermarking: A comprehensive overview of techniques for intellectual property protection. _Machine Learning and Knowledge Extraction_, 5(4):1382-1406, 2023.
* Li et al. (2021) Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In _International conference on machine learning_, pages 6357-6368. PMLR, 2021.
* Li et al. (2020)Bowen Li, Lixin Fan, Hanlin Gu, Jie Li, and Qiang Yang. Fedipr: Ownership verification for federated deep neural network models. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(4):4521-4536, 2022.
* Li et al. (2022) Zheng Li, Muhammad Bilal, Xiaolong Xu, Jielin Jiang, and Yan Cui. Federated learning-based cross-enterprise recommendation with graph neural networks. _IEEE Transactions on Industrial Informatics_, 19(1):673-682, 2022.
* Lv et al. (2023) Peizhuo Lv, Pan Li, Shengzhi Zhang, Kai Chen, Ruigang Liang, Hualong Ma, Yue Zhao, and Yingjiu Li. A robustness-assured white-box watermark in neural networks. _IEEE Transactions on Dependable and Secure Computing_, 2023.
* McMahan et al. (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
* Peng et al. (2022) Liang Peng, Nan Wang, Nicha Dvornek, Xiaofeng Zhu, and Xiaoxiao Li. Fedni: Federated graph learning with network inpainting for population-based disease prediction. _IEEE Transactions on Medical Imaging_, 2022.
* Saha et al. (2020) Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 11957-11965, 2020.
* Shafieinejad et al. (2021) Masoumeh Shafieinejad, Nils Lukas, Jiaqi Wang, Xinda Li, and Florian Kerschbaum. On the robustness of backdoor-based watermarking in deep neural networks. In _Proceedings of the 2021 ACM workshop on information hiding and multimedia security_, pages 177-188, 2021.
* Shao et al. (2022) Shuo Shao, Wenyuan Yang, Hanlin Gu, Zhan Qin, Lixin Fan, Qiang Yang, and Kui Ren. Fedtracker: Furnishing ownership verification and traceability for federated learning model. _arXiv preprint arXiv:2211.07160_, 2022.
* Shen et al. (2022) Yun Shen, Xinlei He, Yufei Han, and Yang Zhang. Model stealing attacks against inductive graph neural networks. In _2022 IEEE Symposium on Security and Privacy (SP)_, pages 1175-1192. IEEE, 2022.
* Tan et al. (2023) Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang. Federated learning on non-iid graphs via structural knowledge sharing. In _Proceedings of the AAAI conference on artificial intelligence_, volume 37, pages 9953-9961, 2023.
* Tekgul et al. (2021) Buse GA Tekgul, Yuxi Xia, Samuel Marchal, and N Asokan. Waffle: Watermarking in federated learning. In _2021 40th International Symposium on Reliable Distributed Systems (SRDS)_, pages 310-320. IEEE, 2021.
* Uchida et al. (2017) Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin'ichi Satoh. Embedding watermarks into deep neural networks. In _Proceedings of the 2017 ACM on international conference on multimedia retrieval_, pages 269-277, 2017.
* et al. (2017) Vidya et al. Fedml supports several out-of-the-box deep learning algorithms for various data types, such as tabular, text, image, graphs, and internet of things (iot) data. https://aws.amazon.com/blogs/machine-learning/part-2-federated-learning-on-aws-with-fedml-health-analytics-without-sharing-sensitive-data/.
* Wang et al. (2020) Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can backdoor federated learning. _Advances in Neural Information Processing Systems_, 33:16070-16084, 2020.
* Wang et al. (2022) Binghui Wang, Ang Li, Meng Pang, Hai Li, and Yiran Chen. Graphfl: A federated learning framework for semi-supervised node classification on graphs. In _2022 IEEE International Conference on Data Mining (ICDM)_, pages 498-507. IEEE, 2022.
* Wang et al. (2020)Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, and Jingren Zhou. Federatedscope-gnn: Towards a unified, comprehensive and efficient package for federated graph learning. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 4110-4120, 2022.
* Wills and Meyer (2020) Peter Wills and Francois G Meyer. Metrics for graph comparison: a practitioner's guide. _Plos one_, 15(2):e0228728, 2020.
* Wu et al. (2022) Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Tao Qi, Yongfeng Huang, and Xing Xie. A federated graph neural network framework for privacy-preserving personalization. _Nature Communications_, 13(1):3091, 2022.
* Xia et al. (2024) Zaishuo Xia, Han Yang, Binghui Wang, and Jinyuan Jia. Gnncert: Deterministic certification of graph neural networks against adversarial perturbations. In _International Conference on Learning Representations_, 2024.
* Xie et al. (2021) Han Xie, Jing Ma, Li Xiong, and Carl Yang. Federated graph classification over non-iid graphs. _Advances in neural information processing systems_, 34:18839-18852, 2021.
* Xu et al. (2019) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2019.
* Xu et al. (2023) Jing Xu, Stefanos Koffas, Oguzhan Ersoy, and Stippan Picek. Watermarking graph neural networks based on backdoor attacks. In _2023 IEEE 8th European Symposium on Security and Privacy (EuroS&P)_, pages 1179-1197. IEEE, 2023.
* Yan et al. (2023) Yifan Yan, Xudong Pan, Mi Zhang, and Min Yang. Rethinking {White-Box} watermarks on deep learning models under neural structural obfuscation. In _32nd USENIX Security Symposium (USENIX Security 23)_, pages 2347-2364, 2023.
* Yanardag and Vishwanathan (2015) Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In _Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1365-1374, 2015.
* Yang et al. (2023) Wenyuan Yang, Shuo Shao, Yue Yang, Xiyao Liu, Ximeng Liu, Zhihua Xia, Gerald Schaefer, and Hui Fang. Watermarking in secure federated learning: A verification framework based on client-side backdooring. _ACM Transactions on Intelligent Systems and Technology_, 15(1):1-25, 2023.
* Yang et al. (2024) Yuxin Yang, Qiang Li, Jinyuan Jia, Yuan Hong, and Binghui Wang. Distributed backdoor attacks on federated graph learning and certified defenses. In _Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security_, 2024.
* Yao et al. (2024) Yuhang Yao, Weizhao Jin, Srivatsan Ravi, and Carlee Joe-Wong. Fedgcn: Convergence-communication tradeoffs in federated training of graph convolutional networks. _Advances in Neural Information Processing Systems_, 36, 2024.
* Yin et al. (2018) Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed learning: Towards optimal statistical rates. In _International conference on machine learning_, pages 5650-5659. Pmlr, 2018.
* Zhang et al. (2021) Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. Backdoor attacks to graph neural networks. In _SACMAT_, 2021.

Proofs

**Proof of Theorem 1:** Given a watermarked graph \(G_{w}\), a base graph classifier \(f\), our ensemble classifier \(g\), and the target watermarked FedGL model \(\theta=\{\vartheta_{i}\}_{i=1}^{S}\). Let \(\theta^{\prime}\) be the watermarked FedGL model after the layer-perturbation attack. We denote by \(y=f(\vartheta_{i},G_{w})\) the prediction of \(G_{w}\) by submodel \(i\), and \(g(\theta,G_{w})=\operatorname*{arg\,max}_{y\in\mathbb{Y}}N_{y}\) the prediction of \(G_{w}\) by the target model \(\theta\), where \(N_{y}=\sum_{i=1}^{S}\mathbbm{I}(f(\vartheta_{i},G_{w})=y)\) and \(\theta=\{\vartheta_{i}\}_{i=1}^{S}\). Assume \(N_{A}\) and \(N_{B}\) are the largest and second largest votes of the output of the ensemble classifier \(g\) on the target model \(\theta\) and \(G_{w}\), respectively. Similarly, let \(N^{\prime}_{A}\) and \(N^{\prime}_{B}\) respectively denote the corresponding votes of the model \(\theta^{\prime}\). Under our GL model architecture, each perturbed layer affects at most \(1\) submodel in the worst case. Therefore, \(N^{\prime}_{A}\) and \(N^{\prime}_{B}\) satisfy the following equations when \(r\) layers in \(\theta\) are perturbed:

\[N_{A}-r\leq N^{\prime}_{A}\leq N_{A}+r,\] (2) \[N_{B}-r\leq N^{\prime}_{B}\leq N_{B}+r.\]

Thus, \(g(\theta^{\prime},G)=g(\theta,G)\) when \(N^{\prime}_{A}>N^{\prime}_{B}\), This means \(N_{A}-r>N_{B}+r-1(A<B)\), where \(\mathbbm{I}(A<B)\) indicates we select a label with a smaller index in case of ties. Hence, \(r\) satisfies:

\[r\leq r^{*}=\frac{N_{A}-N_{B}+\mathbbm{I}(A<B)-1}{2}.\] (3)

**Proof of Theorem 2:** We use proof by contradiction, i.e., when \(r>r^{*}\), we can build a base classifier \(f^{\prime}\) such that there exists \(g(\theta,G_{w})\neq g(\theta^{\prime},G_{w})\). We select the smallest \(r=r^{*}+1\) for simplicity, indicating that \(r^{*}+1\) layers whose model parameters can be arbitrarily perturbed. Assume there exist a graph classifier \(f^{\prime}\) under which, \((r^{*}+1)\) submodels whose original predictions on \(G_{w}\) are the label \(A\) and now become \(B\) after \(r^{*}+1\) layers are perturbed. Then we have \(N^{\prime}_{A}=N_{A}-(r^{*}+1)\) and \(N^{\prime}_{B}=N_{B}+(r^{*}+1)\). Since \(r^{*}\) is the largest number such that Equation (2) is satisfied, i.e., \(N_{A}-r^{*}>N_{B}-1(A<B)+r^{*}\) holds. Thus,

\[N_{A}-(r^{*}+1)\leq N_{B}-\mathbbm{1}(A<B)+(r^{*}+1).\] (4)

Or \(N^{\prime}_{A}\leq N^{\prime}_{B}\). Hence, \(g(\theta,G_{w})=A\neq B=g(\theta^{\prime},G_{w})\), if \(A>B\).

## Appendix B Algorithm 1

```
0: clients \([1,T]\), watermarked clients \([1,T_{w}]\), training graphs \(\mathbb{G}^{i}\) (\(i\in[1,T]\)), initial global model \(\theta_{1}\), initial CWG \(\omega_{1}\), \(\#\)Perturbed Layers \(r\), unique key \(k^{i}\) for client \(i\) (\(i\in[1,T_{w}]\)), the number of submodels \(S\).
0: Target global model \(\theta_{epoch}\), and target local models \(\theta^{i}_{epoch}(j\in[1,T_{w}])\).
1:for each epoch \(e\) in \([1,epoch]\)do
2:for each client \(i\in[1,T]\)do
3:if\(i\in[1,T_{w}]\)then
4: Divide \(\mathbb{G}^{i}\) into \(\mathbb{G}^{i}_{e}\) and \(\mathbb{G}^{i}_{w}\). \(\triangleright\) clean & watermarked samples
5:\(\mathbb{G}^{i}_{w}=\operatorname*{CWG}(\mathbb{G}^{i}_{w},k^{i})\)
6:endif
7:\(\vartheta_{1},\vartheta_{2},\cdots,\vartheta_{S}=\operatorname*{divide}( \theta_{e})\)
8:for each submodel \(j\) in \([1,S]\)do
9:\(\vartheta^{i}_{j}=\operatorname*{arg\,min}_{\vartheta^{i}_{j}}L(\vartheta_{j} ;\mathbb{G}^{i})\)
10:endfor
11:\(\theta^{i}_{e}=\operatorname*{joint}(\vartheta^{i}_{1},\vartheta^{i}_{2}, \cdots,\vartheta^{i}_{S})\)
12:if\(i\in[1,T_{w}]\)then
13:\(\omega^{i}_{e}=\operatorname*{arg\,min}_{\omega^{i}}L(\theta^{i}_{e};\mathbb{G} ^{i}_{w})\)
14:endif
15:endfor
16:\(\theta_{e+1}=\frac{1}{|\mathbb{T}_{e}|}\sum_{i\in\mathbb{T}_{e}}\theta^{i}_{e}\)\(\triangleright\) Server selects \(\mathbb{T}_{e}\) clients for aggregation
17:endfor ```

**Algorithm 1** The training process of FedGMark

## Appendix C More Experimental Details

Table 6 shows the statistics of the used graph datasets. Table 7 shows the network architectures of the CWG component. Table 8 shows the detailed network architectures of Fed-GIN, Fed-GSAGE, and Fed-GCN models.

\begin{table}
\begin{tabular}{c||c|c|c|c|c|c c|c c c} \hline \hline \multicolumn{1}{c||}{\multirow{2}{*}{Datasets}} & \multicolumn{2}{c|}{\#Graphs} & \multicolumn{2}{c|}{\#Classes} & \multicolumn{2}{c|}{Avg. \#Node} & \multicolumn{2}{c}{Avg. \#Edge} & \multicolumn{2}{c}{\#Training graphs} & \multicolumn{2}{c}{\#Testing graphs} \\  & & & & & & 1 & 2 & 3 & 1 & 2 & 3 \\ \hline \hline MUTAG & 188 & 2 & 17.93 & 19.80 & 83 & 42 & - & 42 & 21 & - \\ PROTEINS & 1110 & 2 & 37.72 & 70.35 & 440 & 300 & - & 220 & 150 & - \\ DD & 950 & 2 & 208.3 & 518.76 & 330 & 303 & - & 165 & 152 & - \\ COLLAB & 4981 & 3 & 73.49 & 2336.66 & 517 & 1589 & 1215 & 258 & 794 & 608 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Statistics of datasets.

\begin{table}
\begin{tabular}{c||c c|c} \hline \hline \multicolumn{1}{c||}{\multirow{2}{*}{Name}} & \multicolumn{2}{c|}{Network Architectures} & \multicolumn{2}{c}{Input} & \multicolumn{2}{c}{Output} \\ \hline \hline \(\operatorname{GatingNet}\) & 2\(\times\)(Linear-\(|\mathbb{V}|\)\& ReLU \& Dropout-\(0.05\)) & \(\mathbf{A}^{i}\in\{0,1\}^{|\mathbb{V}|\times|\mathbb{V}|}\) & \(\tilde{\mathbf{A}}^{i}\in[0,1]^{|\mathbb{V}|\times|\mathbb{V}|}\) \\  & & Linear-\(|\mathbb{V}|\)\& Sigmoid & & \\ \(\operatorname{KeyNet}\) & 2\(\times\)(Linear-\(|\mathbb{V}|\)\& ReLU \& Dropout-\(0.05\)) & \(\mathbf{K}^{i}\in\mathbb{R}^{|\mathbb{V}|\times|\mathbb{V}|}\) & \(\tilde{\mathbf{K}}^{i}\in[0,1]^{|\mathbb{V}|\times|\mathbb{V}|}\) \\  & & Linear-\(|\mathbb{V}|\)\& Sigmoid & & \\ \hline \hline \end{tabular}
\end{table}
Table 7: Detailed network architectures of CWG.

\begin{table}
\begin{tabular}{c||c|c|c} \hline \hline \multicolumn{1}{c||}{\multirow{2}{*}{submodels \(1\)-\(4\)}} & \multicolumn{2}{c|}{submodels \(5\)-\(8\)} & \multicolumn{2}{c}{submodels \(9\)-\(16\)} \\ \hline \hline \multicolumn{1}{c||}{Linear-\(x\times y\)} & Linear-\(x\times y\) & Linear-\(x\times y\) & Linear-\(x\times y\) \\ \(\operatorname{conv}\)-\(x\times\)64 \& BN-64 & conv-\(x\times\)128 \& BN-128 & conv-\(x\times\)72 \& BN-72 & conv-\(x\times\)96 \& BN-96 \\ ReLU \& Linear-64\(\times y\) & ReLU \& Linear-128\(\times y\) & ReLU \& Linear-72\(\times y\) & ReLU \& Linear-96\(\times y\) \\ \(2\times\)(conv-64\(\times\)64 \& BN-64) & 2\(\times\)(conv-128\(\times\)128 \& BN-128 & 2\(\times\)(conv-72\(\times\)72 \& BN-72 \(\times\)(conv-96\(\times\)96 \& BN-96 \\ ReLU \& Linear-64\(\times y\)) & ReLU \& Linear-128\(\times y\)) & ReLU \& Linear-72\(\times y\)) & ReLU \& Linear-96\(\times y\) \\ conv-64\(\times\)64 \& BN-64 & conv-128\(\times\)64 \& BN-64 & conv-72\(\times\)64 \& BN-64 & conv-96\(\times\)64 \& BN-64 \\ ReLU \& Linear-64\(\times y\) & ReLU \& Linear-64\(\times y\) & ReLU \& Linear-64\(\times y\) & ReLU \& Linear-64\(\times y\) \\ \hline \multicolumn{1}{c||}{Linear-\(x\times y\)} & Linear-\(x\times y\) & Linear-\(x\times y\) & Linear-\(x\times y\) & Linear-\(x\times y\) \\ conv-\(x\times\)64 \& BN-64 & conv-\(x\times\)128 \& BN-128 & conv-\(x\times\)72 \& BN-72 & conv-\(x\times\)96 \& BN-96 \\ ReLU \& Linear-64\(\times y\) & ReLU \& Linear-128\(\times y\) & ReLU \& Linear-72\(\times y\) & ReLU \& Linear-96\(\times y\) \\ \(2\times\)(conv-64\(\times\)64 \& BN-64) & 2\(\times\)(conv-128\(\times\)128 \& BN-128 & 2\(\times\)(conv-72\(\times\)72 \& BN-72 \(\times\)(conv-96\(\times\)96 \& BN-96 \\ ReLU \& Linear-64\(\times y\)) & ReLU \& Linear-128\(\times y\)) & ReLU \& Linear-72\(\times y\)) & ReLU \& Linear-96\(\times y\) \\ conv-64\(\times\)32 \& BN-32 & conv-128\(\times\)32 \& BN-32 & conv-72\(\times\)32 \& BN-32 & conv-96\(\times\)32 \& BN-32 \\ ReLU \& Linear-32\(\times y\) & ReLU \& Linear-32\(\times y\) & ReLU \& Linear-32\(\times y\) & ReLU \& Linear-32\(\times y\) \\ \hline \multicolumn{1}{c||}{Linear-\(x\times y\)} & Linear-\(x\times y\) & Linear-\(x\times y\) & Linear-\(x\times y\) \\ \(\operatorname{conv}\)-\(x\times\)64 \& BN-64 & conv-\(x\times\)128 \& BN-128 & conv-\(x\times\)72 \& BN-72 & conv-\(x\times\)96 \& BN-96 \\ ReLU \& Linear-64\(\times y\) & ReLU \& Linear-128\(\times y\) & ReLU \& Linear-72\(\times y\) & ReLU \& Linear-96\(\times y\) \\ \(2\times\)(conv-64\(\times\)64 \& BN-64 & 2\(\times\)(conv-128\(\times\)128 \& BN-128 & 2\(\times\)(conv-72\(\times\)72 \& BN-72 \(\times\)(conv-96\(\times\)96 \& BN-96 \\ ReLU \& Linear-64\(\times y\)) & ReLU \& Linear-128\(\times y\)) & ReLU \& Linear-72\(\times y\)) & ReLU \& Linear-96\(\times y\) \\ conv-64\(\times\)16 \& BN-16 & conv-128\(\times\)16 \& BN-16 & conv-72\(\times\)16 \& BN-16 & conv-96\(\times\)16 \& BN-16 \\ ReLU \& Linear-16\(\times y\) & ReLU \& Linear-16\(\times y\) & ReLU \& Linear-16\(\times y\) & ReLU \& Linear-16\(\times y\) \\ \hline \multicolumn{1}{c||}{Linear-\(x\times y\)} & Linear-\(x\times y\) & Linear-\(x\times y\) & Linear-\(x\times y\) \\ \(\operatorname{conv}\)-\(x\times\)64 \& BN-64 & conv-\(x\times\)128 \& BN-128 & conv-\(x\times\)72 \& BN-72 & conv-\(x\times\)96 \& BN-96 \\ ReLU \& Linear-64\(\times y\) & ReLU \& Linear-128\(\times y\) & ReLU \& Linear-72\(\times y\) & ReLU \& Linear-96\(\times y

## Appendix D More Experimental Results

**Impact of #watermarking clients \(T_{w}\) and watermark size \(n_{w}\).** See results in Figure 4 and Figure 5, respectively. We can see FedGMark achieves better watermarking performance with a larger number of clients generating watermarks; and a larger watermark size.

**Impact of learning rate (\(lr\)) and #local epochs (\(le\)).** The results with varying \(lr\) and \(le\) are shown in Table 9 in Appendix. We can see that a large \(lr\) may reduce WA, and WA increases slightly as \(le\) grows, indicating more thorough training makes our method perform better.

**Scalability of FedGMark.** Compared with graph-based or non-robust watermarking methods, the computation overhead of FedGMark is mainly from the introduced submodel models (fixing all the other parameters, such as #clients, #iterations, to be the same). Particularly, the overhead scales linearly with the number of submodels \(S\), and the runtime results are shown in Table 10.

**FedGMark under watermark/backdoor detection attacks.** Many existing works [22] show the trigger-reverse based backdoor detection is ineffective to "stealthy" backdoor. This is because the effectiveness of trigger reverse attacks largely depends on the statistical differences between clean data and backdoored data. Since we do not notice any graph backdoor trigger-reverse attack, we instead propose to quantitatively test the structure similarity between the generated watermarked graphs and the clean graphs. Here we use the metrics NetSim and DeltaCon proposed in [37], with the range \([0,1]\) and the higher value the larger similarity. As shown in Table 11, we observe the watermarked graphs and their clean counterparts are structurally very close. This implies that the proposed watermarks are hard to be detected.

Figure 4: Impact of \(T_{w}\) on FedGMark against prior watermark removal and layer-perturbation attacks.

Figure 5: Impact of \(n_{w}\) on FedGMark against prior watermark removal and layer-perturbation attacks.

\begin{table}
\begin{tabular}{c||c c c c c c|c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multicolumn{6}{c}{\(le=5\)} & \multicolumn{6}{c}{\(lr=0.01\)} \\  & \(lr=0.01\) & \(lr=0.05\) & \(lr=0.1\) & & \(le=5\) & \(le=10\) & \(le=20\) \\  & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) \\ \hline \hline MUTAG & 0.81 & 0.90 & 0.84 & 0.89 & 0.79 & 0.90 & 0.81 & 0.90 & 0.84 & 0.90 & 0.76 & 0.92 \\ PROTEINS & 0.72 & 0.86 & 0.71 & 0.79 & 0.71 & 0.73 & 0.72 & 0.86 & 0.72 & 0.87 & 0.72 & 0.89 \\ DD & 0.73 & 0.65 & 0.71 & 0.57 & 0.70 & 0.53 & 0.73 & 0.65 & 0.73 & 0.66 & 0.73 & 0.69 \\ COLLAB & 0.73 & 0.75 & 0.72 & 0.71 & 0.70 & 0.65 & 0.73 & 0.75 & 0.74 & 0.75 & 0.73 & 0.77 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Effect of learning rate (\(lr\)) and #local epochs (\(le\)).

**FedGMark on non-IID/heterogeneous datasets.** Recall that the local watermarks in FedGMark are learnt by considering the unique properties in each client. Such unique properties may include the heterogeneity across clients' data. To validate this, we also test FedGMark with non-IID graphs across clients and show results in Table 12, where each client holds a single label data. We observe that FedGMark demonstrates strong performance on non-IID datasets, indicating that the learnt customized watermarks effectively capture the heterogeneity of clients' graphs.

**FedGMark against more knowledgeable/stronger adversaries.** In our threat model, we assume all clients and the server are benign and assume the attacker does not know our CWG. Here, we test FedGMark against stronger adversaries, where some clients are malicious and the these malicious clients also has access to the CWG component to manipulate the FedGL training. First, we consider a _passive_ attack where all malicious clients _do not_ use CWG to generate customized local watermarks. Model (b) in Table 5 shows the _maximum_ WA decrease is 9%, where _all_ clients do not use CWG.

Second, we test an _active_ attack where malicious clients modify their watermark data's label to obfuscate the training. Specifically, all malicious clients' watermark data are labeled (e.g., 2) differently from the target label (e.g., 1) and then follow the federated training. The results in Table 13 show MA/WA is marginally affected even with 20% malicious clients.

**FedGMark with alternative aggregation methods.** The current FedGMark's evaluation focuses on FedAvg for aggregating client models. Here, we evaluate FedGMark using Multi-Krum [Blanchard _et al._, 2017] and Trim-mean [Yin _et al._, 2018] aggregation methods that consider data quality (e.g., remove outlier clients). Specifically, Multi-Krum filters a set of \(p\) clients whose gradients largely deviated from others, while Trim-mean trims off \(q\) highest and lowest values for each parameter in clients' models. Table 14 shows the results with \(p=10\) and \(q=10\). We can see these robust aggregators achieve a robustness-utility tradeoff, and MA and WA are not largely different.

\begin{table}
\begin{tabular}{c||c c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multicolumn{2}{c}{MUTAG} & \multicolumn{2}{c}{PROTEINS} & \multicolumn{2}{c}{DD} & \multicolumn{2}{c}{COLLAB} \\  & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) \\ \hline \hline \multirow{2}{*}{IID} & 0.81 & 0.90 & 0.72 & 0.86 & 0.73 & 0.65 & 0.73 & 0.75 \\  & 0.80 & 0.89 & 0.72 & 0.83 & 0.72 & 0.63 & 0.72 & 0.75 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Results of FedGMark against \(p\%\) malicious clients whose watermarked data are mislabeled.

\begin{table}
\begin{tabular}{c||c|c c c|c||c c c} \hline \hline \multirow{2}{*}{Datasets} & \multicolumn{2}{c}{\(S\)} & 4 & 8 & 16 & Datasets & \(S\) & 4 & 8 & 16 \\  & \multicolumn{2}{c}{(Net)} & \multicolumn{2}{c}{time(s)} & & \multicolumn{2}{c}{(Net)} & & \multicolumn{2}{c}{time(s)} \\ \hline  & Fed-GIN & 0.46 & 0.74 & 1.32 & & & Fed-GIN & 2.79 & 5.10 & 9.49 \\ \multirow{2}{*}{MUTAG} & Fed-GSAGE & 0.28 & 0.48 & 0.90 & PROTEINS & Fed-GSAGE & 2.72 & 4.25 & 8.31 \\  & Fed-GCN & 0.30 & 0.53 & 0.94 & & & Fed-GCN & 2.73 & 4.56 & 8.95 \\ \hline \multirow{2}{*}{DD} & Fed-GIN & 11.14 & 20.12 & 36.74 & & & Fed-GIN & 161.46 & 296.68 & 563.51 \\  & Fed-GSAGE & 11.75 & 19.82 & 31.57 & & & Fed-GSAGE & 152.23 & 281.78 & 551.86 \\ \multirow{2}{*}{MTAG} & Fed-GCN & 11.61 & 19.51 & 33.23 & & & Fed-GCN & 163.03 & 293.55 & 544.17 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Number of submodels \(S\) vs. the runtime of RML. We observe an almost linear relationship between the total training time \(C_{t}\) and the number of submodels \(S\), expressed as \(C_{t}\approx S*C_{s}\), where \(C_{s}\) denotes the training time for an individual submodel.

\begin{table}
\begin{tabular}{c||c c c c c} \hline \hline Datasets & \multicolumn{2}{c}{MUTAG} & \multicolumn{2}{c}{PROTEINS} & \multicolumn{2}{c}{DD} & \multicolumn{2}{c}{COLLAB} \\ \hline \hline NetSim (\(\uparrow\)) & 0.97 & 0.98 & 0.99 & 0.99 \\ DeltaCon (\(\uparrow\)) & 0.98 & 0.98 & 0.99 & 0.99 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Results of FedGMark on IID and non-IID/heterogeneous datasets.

**FedGMark with alternative triggers such as feature-based triggers and hybrid triggers.** The current FedGMark relies on graph structural information to learn the trigger used in watermark. Here, we adjust FedGMark to learn feature-based triggers and hybrid feature-structure triggers.

To learn feature-based triggers, we first select a set of nodes from a graph as the target nodes, and learn the watermarked features for the target nodes (we do not watermark structure). We use a graph \(G^{i}=(\mathbb{V}^{i},\mathbb{E}^{i},\mathbf{X}^{i})\) from client \(i\) for illustration, where \(\mathbf{X}^{i}\) is the node feature matrix. We then define a feature-mask \(\mathbf{M}^{i}_{w}[v_{j}]=1\) if \(v_{j}\in\mathbb{V}^{i}_{w}\) and 0 otherwise, where \(\mathbb{V}^{i}_{w}\) is the watermark node set described in the paper. Then, we introduce a feature network (\(\mathrm{FeaNet}\)) that learns watermarked node features as \(\mathbf{X}^{i}_{w}=\mathrm{FeaNet}(\mathbf{X}^{i})\odot\mathbf{M}^{i}_{f}\). The \(\mathrm{FeaNet}\) takes input \(\mathbf{X}^{i}\) and outputs a matrix having the same size as \(\mathbf{X}^{i}\), e.g., it has the same architecture as GatingNet but adjusts the input size. The corresponding watermarked graph is defined as \(G^{i}_{w}=(\mathbb{V}^{i},\mathbb{E}^{i},\mathbf{X}^{i}_{w})\). By generating a set of watermarked graphs \(G^{i}_{w}\) for client \(i\), we minimize the loss on client \(i\)'s both clean graphs \(G^{i}_{c}\) and \(G^{i}_{w}\).

Further, to learn feature-structure triggers, we combine \(\mathrm{FeaNet}\) (that gets \(\mathbf{X}^{i}_{w}\)) with \(\mathrm{GatingNet}\)/\(\mathrm{KeyNet}\) (that gets \(\mathbb{E}^{i}_{w}\)), and the watermarked graphs are \(G^{i}_{w}=(\mathbb{V}^{i},\mathbb{E}^{i}_{w},\mathbf{X}^{i}_{w})\). We then minimize the loss on \(G^{i}_{c}\) and \(G^{i}_{w}\). More details about training refer to Section 3.4.

We evaluate these triggers and the results are in Table 15. We observe that structure information alone is sufficient to enable designing effective triggers.

## Appendix E Discussion

**Global watermark vs. local watermarks.** In this paper, each client employs CWG to generate watermarks (i.e., local watermarks) to train the local model. During testing, we can use either local watermarks or a global watermark, i.e., fully connect local watermarks. Here, we further evaluate the impact of global watermark vs. local watermarks of the target watermarked model under default settings. For better clarity, we randomly select four local watermarks labeled as local watermarks \(1\) to \(4\). The experimental results depicted in Figure 6 demonstrate that, despite not being part of the training process, the global watermark performs slightly better than local watermarks. This implies that the distributed training of FedGL can aggregate the effects of local watermarks, providing a foundation for utilizing the global watermark for verification.

\begin{table}
\begin{tabular}{c||c c c c c c} \hline \hline Datasets & \multicolumn{2}{c}{MUTAG} & \multicolumn{2}{c}{PROTEINS} & \multicolumn{2}{c}{DD} & \multicolumn{2}{c}{COLLAB} \\  & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) & MA\(\uparrow\) & MA\(\uparrow\) & WA\(\uparrow\) \\ \hline \hline Avg & 0.81 & 0.90 & 0.72 & 0.86 & 0.73 & 0.65 & 0.73 & 0.75 \\ Multi-Krum & 0.78 & 0.92 & 0.73 & 0.85 & 0.72 & 0.63 & 0.73 & 0.74 \\ Trim-mean & 0.75 & 0.93 & 0.70 & 0.87 & 0.70 & 0.65 & 0.71 & 0.77 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Results of our FedGMark against Multi-Krum [Blanchard _et al._, 2017] and Trim-mean [Yin _et al._, 2018] aggregation methods.

Figure 6: Global watermark vs. local watermarks \(1\)-\(4\).

**Ensemble classifier with majority voting vs. summation on submodels' outputs.** In this paper, we propose an ensemble classifier \(g\) to aggregate the predictions of each submodel \(\vartheta_{i}\) through a voting mechanism and classify a graph \(G\) as \(g(\theta,G)=\operatorname*{arg\,max}_{y\in\mathbb{Y}}N_{y}\), where \(N_{y}=\sum_{i=1}^{S}\mathbb{I}(f(\vartheta_{i},G)=y)\). Another common strategy, as used in GL models such as GIN [20], is to sum the prediction _confidence vector_ of each submodel \(\vartheta_{i}\), i.e., \(g(\theta,G)=\operatorname*{arg\,max}_{y\in\mathbb{Y}}\mathbf{f}_{G}\), where \(\mathbf{f}_{G}=\sum_{i=1}^{S}\mathbf{f}(\vartheta_{i},G)\) and \(\mathbf{f}\) outputs a probabilistic confidence vector whose summation is 1. Here, we also test our FedGMark with this strategy against the 1-layer perturbation attack, and the results are shown in Table 16. We notice the results do not remain consistent across different numbers of submodels and are worse than those in Table 3, especially when \(S\) is small. This suggests that the summing-based ensemble classifier is not robust to the layer-perturbation attack, emphasizing the necessity of our proposed majority-voting based ensemble classifier.

**FedGMark against offline clients that cannot provide watermark data or malicious clients that provide fake watermark samples during ownership verification.** We deem that our ownership verification is still robust against offline clients and malicious clients, if its number is less than 50%. During ownership verification, each client provides its own watermark data to the trusted judge. When some clients are offline, the trusted judge can simply neglect them and only use participating clients' watermark data for verification.

When facing malicious clients, their negative effect can be circumvented through a majority voting-based approach. Specifically, all clients provide their own watermark data to the judge and obtain the watermark accuracy per client. Though the watermark accuracy on malicious clients could be very low, the majority of benign clients can produce more number of high watermark accuracy, compared to the number of low accuracy. When the judge uses the majority-vote strategy, the final watermark accuracy is still high, ensuring the accurate ownership claim for benign clients.

**Using clients' training graphs as the watermark samples and sending them to the trust judge may lead to privacy leakage.** We clarify that the watermark data are not necessarily generated from the training/test samples. Remember the primary goal of backdoor-based watermarking is to force the model to memorize the relationship between the backdoor trigger (in the watermark samples) and the target label, while the samples to inject the trigger do not have constraints, i.e., they can be from training samples or artificially synthesized (which does not contain privacy information of any training/test data). For conveniences, existing methods inject backdoor triggers into the training/test samples. To validate this, we synthesize a set of random graphs using the popular Erdos-Renyi model (via the NetworkX toolbox) and the watermark samples are generated by injecting the learnt watermark on the synthesized graphs. Under the default setting, we test on Fed-GIN and show results in Table 17, where we observe WAs are very close to those shown in the paper on the four datasets.

Furthermore, since all clients intend to verify model ownership, it is reasonable to believe that these clients are willing to provide their watermark data--whether generated from private training/test data or non-private synthesized data--exclusively to a trusted judge, with informed consent and in accordance with legal and ethical standards. From this perspective, the data is confidential between each client and the trusted judge. We acknowledge it is very interesting future work to design a provably private mechanism for model ownership verification that the verifier cannot access the watermark data but can guarantee the correctness of verification.

\begin{table}
\begin{tabular}{c||c c c c c c c||c c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{\(S\)} & \multicolumn{2}{c}{Fed-GIN} & \multicolumn{2}{c}{Fed-GSAGE} & \multicolumn{2}{c}{Fed-GCN} & \multirow{2}{*}{Datasets} & \multirow{2}{*}{\(S\)} & \multicolumn{2}{c}{Fed-GIN} & \multirow{2}{*}{\(\text{Fad-GSAGE}\)} & \multirow{2}{*}{\(\text{Fed-GCN}\)} \\  & & M\(\uparrow\) & W\(\uparrow\) & M\(\uparrow\) & M\(\uparrow\) & W\(\uparrow\) & M\(\uparrow\) & W\(\uparrow\) & & & M\(\uparrow\) & W\(\uparrow\) & M\(\uparrow\) & W\(\uparrow\) \\ \hline \hline \multirow{3}{*}{MUTAG} & 4 & 0.71 & 0.55 & 0.73 & 0.65 & 0.81 & 0.61 & & & 4 & 0.52 & 0.51 & 0.53 & 0.52 & 0.63 & 0.67 \\  & 8 & 0.70 & 0.65 & 0.71 & 0.60 & 0.73 & 0.44 & PROTEINS & & 8 & 0.70 & 0.83 & 0.71 & 0.84 & 0.73 & 0.86 \\  & 16 & 0.81 & 0.83 & 0.81 & 0.76 & 0.79 & 0.75 & & & 16 & 0.72 & 0.85 & 0.72 & 0.85 & 0.71 & 0.83 \\ \hline \multirow{3}{*}{DD} & 4 & 0.71 & 0.39 & 0.70 & 0.28 & 0.73 & 0.43 & & 4 & 0.69 & 0.48 & 0.67 & 0.52 & 0.66 & 0.51 \\  & 8 & 0.70 & 0.42 & 0.72 & 0.48 & 0.73 & 0.46 & COLLAB & & 8 & 0.74 & 0.61 & 0.74 & 0.66 & 0.72 & 0.62 \\ \cline{1-1}  & 16 & 0.73 & 0.59 & 0.74 & 0.54 & 0.71 & 0.42 & & 16 & 0.71 & 0.75 & 0.71 & 0.72 & 0.72 & 0.70 \\ \hline \hline \end{tabular}
\end{table}
Table 16: FedGMark with confidence scores summation against layer-perturbation attack.

\begin{table}
\begin{tabular}{l||c c c c c} \hline \hline
**Watermark** & **MUTAG** & **PROTEINS** & **DD** & **COLLAB** \\ \hline on train/test graphs & 0.81 / 0.90 & 0.72 / 0.86 & 0.73 / 0.65 & 0.73 / 0.75 \\ on synthesized graphs & 0.80 / 0.88 & 0.71 / 0.84 & 0.72 / 0.64 & 0.72 / 0.73 \\ \hline \hline \end{tabular}
\end{table}
Table 17: MA/WA on synthesized graphs for watermarking.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We clearly state the research problem, motivation, and contributions of this paper in the Abstract and Introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of the approach in the Discussion section, e.g., using clients' training graphs as watermark samples and sending them to the trusted judge could lead to privacy leakage. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide complete theory background in the main text, and present proofs in the supplementary. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our experimental results are reproducible. We provide detailed experimental settings in the main text. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We include links to the source code in the abstract. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide training and test details in the experimental part. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Our experimental results are stable. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All experiments can be reproduced using one NVIDIA GeForce GTX 1080 Ti GPU. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This research conforms to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Positive societal impacts: Our proposed watermark i.e. FedGMark can protect the ownership of FedGL models. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no the above risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the reference of datasets employed in this paper following their requirements. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: There is no dataset contribution in this paper. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work does not involve crowdsourcing and research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.