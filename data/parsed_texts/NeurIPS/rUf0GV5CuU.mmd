# Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search

 Indradyumna Roy\({}^{\dagger}\)  Rishi Agarwal\({}^{\dagger}\)

Soumen Chakrabarti\({}^{\dagger}\)  Anirban Dasgupta\({}^{\diamond}\)  Abir De\({}^{\dagger}\)

\({}^{\dagger}\)IIT Bombay, \({}^{\diamond}\)IIT Gandhinagar

{indraroy15, rishiagarwal18, soumen, abir}@cse.iitb.ac.in

anirbandg@cse.iitgn.ac.in

###### Abstract

In many search applications related to passage retrieval, text entailment, and subgraph search, the query and each 'document' is a set of elements, with a document being relevant if it contains the query. These elements are not represented by atomic IDs, but by embedded representations, thereby extending set containment to _soft_ set containment. Recent applications address soft set containment by encoding sets into fixed-size vectors and checking for elementwise _vector dominance_. This 0/1 property can be relaxed to an asymmetric _hinge distance_ for scoring and ranking candidate documents. Here we focus on data-sensitive, trainable indices for fast retrieval of relevant documents. Existing LSH methods are designed for mostly symmetric or few simple asymmetric distance functions, which are not suitable for hinge distance. Instead, we transform hinge distance into a proposed _dominance similarity_ measure, to which we then apply a Fourier transform, thereby expressing dominance similarity as an expectation of inner products of functions in the frequency domain. Next, we approximate the expectation with an importance-sampled estimate. The overall consequence is that now we can use a traditional LSH, but in the frequency domain. To ensure that the LSH uses hash bits efficiently, we learn hash functions that are sensitive to both corpus and query distributions, mapped to the frequency domain. Our experiments show that the proposed asymmetric dominance similarity is critical to the targeted applications, and that our LSH, which we call FourierHashNet, provides a better query time vs. retrieval quality trade-off, compared to several baselines. Both the Fourier transform and the trainable hash codes contribute to performance gains.

## 1 Introduction

Consider a corpus \(X\) of sets \(x\) (which we call 'documents') over some universe of discrete items, and let \(q\) be a query which is also a subset of this universe. We wish to retrieve those \(x\in X\) which satisfy \(q\subseteq x\). In most real-world applications, the items in the universe are not just opaque IDs, but are embedded in a rich feature space, demanding that the definition of "\(q\subseteq x\)" be generalized suitably.

We formalize the notion of _soft set containment_ by writing \(q=\{q_{i}\}\) and \(x=\{x_{i}\}\) and the corresponding sets of item embeddings as \(\{\vec{q}_{i}\}\) and \(\{\vec{x}_{i}\}\). If \(q,x\) are sentences, \(\vec{q}_{i},\vec{x}_{i}\) may be per-word contextual embeddings output from a transformer. If \(q,x\) are graphs, \(\vec{q}_{i},\vec{x}_{i}\) may be contextual node embeddings, such as those output by a Graph Neural Network (GNN). These set-of-vector representations of \(q\) and \(x\) are generally of variable sizes. A suitable set encoding gadget, such as simple pooling [41, 31] or a trainable Deep Set [60] or Set Transformer [27] network, converts them to fixed-size vectors given by \(\bm{q}=\mathrm{SetEnc}(\{\vec{q}_{i}\})\) and \(\bm{x}=\mathrm{SetEnc}(\{\vec{x}_{i}\})\), with \(\bm{x},\bm{q}\in\mathbb{R}^{K}\). Several applications [52, 26, 10, 31] then use the test "\(\bm{q}\leq\bm{x}\)" (elementwise vector dominance) as a surrogate for testing if \(q\subseteq x\).

To convert the Boolean test for vector dominance, \(\bm{q}\leq\bm{x}\), into a graded score suitable for ranking (and backpropagation), these applications [52; 26; 10; 31] use a form of (asymmetric) **hinge distance**

\[d(q,x)=\big{\|}[\bm{q}-\bm{x}]_{+}\big{\|}_{1}=\sum_{k}\max\{0,\bm{q}[k]-\bm{x} [k]\}.\] (1)

\(d(q,x)=0\) when \(\bm{q}\leq\bm{x}\) holds elementwise, and measures the extent of the constraint violation otherwise. A search system must retrieve the top-\(\tau\) documents \(x\) with the smallest \(d(q,x)\), given query \(q\). Several example applications that fit into this framework are elaborated in Appendix B. Even if an application does not fit (1) exactly, our technique may help address other asymmetric distances.

Our goalWhen corpus \(X\) is large, it is impractical to evaluate (1) for each document \(x\). Our goal is to retrieve these \(\tau\) documents without explicitly evaluating \(d(q,x)\) for all \(x\in X\), within query time that scales slowly with \(|X|\). To achieve this, we design an asymmetric Locality Sensitive Hashing (ALSH) method tailored for hinge distance (1), which then immediately addresses soft set-containment based search.

Prior work and their limitationsWhen set elements are represented by atomic IDs, Bloom filters [36] and maximum inner product search (MIPS) can be used to find the best \(\tau\) corpus items that are closest to being supersets [46; 59; 45; 2]. However, these techniques are designed specifically for items with opaque IDs, rather than contextual embeddings. LSH [7; 53; 17; 19; 1] has been established as a standard technique for fast approximate near-neighbor search (e.g., FAISS, DPR) in the space of contextual embeddings. However, they predominantly work for symmetric notions of relevance, such as Jaccard similarity, dot product, cosine similarity, or Hamming distance, rather than asymmetric distances like (1). Neyshabur and Srebro [35] propose a LSH suited for asymmetric relevance (ALSH), but it does not provide a satisfactory solution for (1), as our experiments show.

### Our contributions

Responding to the above motivations, we present FourierHashNet, a new LSH for hinge distance-based asymmetric distance measures. Specifically, we make the following contributions.

Scalable hinge distance search for soft set containmentFrom several applications, we distil the strongly-motivated problem of fast top-\(\tau\) retrieval using hinge distance (1), to capture soft set containment. To our knowledge, (A)LSH for hinge distance has not been explored till date.

Transformation of hinge distance to enable ALSH designOne could leverage its shift-invariant property to apply a Fourier transform on the _negative_ distance, express it as the dot product similarity between the corresponding Fourier features and then use Asymmetric LSH (ALSH) [35]. However, as we show in Section 3.1, using the negative distance leads to singularities of the underlying Fourier transform at some points. This in turn does not allow us to design an LSH for such measure. We circumvent this problem by a suitable transformation of hinge distance to a **dominance similarity**, whose Fourier transform is absolutely convergent.

Design of Fourier featuresNext, we propose a novel method of lifting the dense vectors to frequency domain, such that the dominance similarity in the original space can be expressed as the cosine similarity between the infinite dimensional Fourier features. However, our dominance similarity function is _not_ a positive definite kernel. Hence, unlike Rahimi and Recht [40], we cannot apply Bochner theorem [44] to obtain finite dimensional Fourier features. Instead, we first scale the Fourier features with a sinc function and then obtain finite dimensional Fourier features via importance sampling.

Trainable hashcode designThe cosine similarity between the sampled Fourier features is the unbiased estimate of our dominance similarity measure. This allows the use of conventional random hyperplane LSH. However, such an LSH is not guided by the underlying data distribution. To mitigate this limitation, we compute the hashcodes by feeding the Fourier features into a trainable neural hashing network. Prior approaches [54; 15] to trainable hashing encourage bucket balance over the entire corpus, regardless of the query workload. However, this approach is not optimal if most corpus items are irrelevant for most queries, as is usually the case. We propose a new loss function that encourages the best-match hash bucket for a query to include relevant documents and exclude irrelevant documents.

ExperimentsWe show, through extensive experiments, that FourierHashNet is more effective than existing LSH schemes, and that both frequency domain representations and the new trainable hashcode contribute to our gains.

## 2 Preliminaries

NotationThroughout, we will use \([K]\) to mean \(\{1,\ldots,K\}\) or \(\{0,\ldots,K-1\}\) as convenient. We use \(q\) to indicate a query and \(x\) to indicate a corpus 'document'. Their (possibly learnt) representations are denoted by \(\bm{x},\bm{q}\in\mathbb{R}^{K}\). For supervision, \((q,x)\) may come with a binary relevance judgment \(\operatorname{rel}(q,x)\in\{0,1\}\). We have defined a potentially learnable distance \(d(q,x)\) -- a computable surrogate for \(\operatorname{rel}(q,x)\) -- above in Eqn. (1). One can define a similarity measure \(\operatorname{sim}(q,x)\) by applying a monotonically decreasing function on the distance \(d(q,x)\). We define \(\iota=\sqrt{-1}\) and denote the set of corpus items as \(X=\{x_{1},x_{2},...,x_{N}\}\). We indicate the domain of query and corpus items as \(\mathcal{Q}\) and \(\mathcal{X}\) respectively. Given a function \(s(t)\), its Fourier transform is the function \(S:\mathbb{C}\to\mathbb{C}\) which satisfies \(s(t)=\int_{-\infty}^{\infty}S(\iota\omega)e^{\iota\omega t}d\omega\), where \(\omega\) is the frequency variable and \(S(\iota\omega)=\frac{1}{2\pi}\int_{-\infty}^{\infty}s(t)e^{-\iota\omega t}dt\). For a vector \(\bm{q}\) or \(\bm{x}\in\mathbb{R}^{K}\), the Fourier transform is synthesized using a frequency vector \(\bm{\omega}\in\mathbb{R}^{K}\) of same dimension as \(\bm{x}\) or \(\bm{q}\). Here, a function \(s(\bm{x})\) can be expanded as \(s(\bm{x})=\int_{\bm{\omega}\in\mathbb{R}^{K}}S(\iota\bm{\omega})e^{-\iota\bm{ \omega}^{\top}\bm{x}}d\bm{\omega}\).

### Locality sensitive hashing

Indexing corpus itemsGiven a set of corpus items \(X=\{x_{1},x_{2},...,x_{N}\}\), an LSH will hash each item \(x_{i}\), \(L\) times, which is called the number of _trials_. For each trial \(\ell\in[L]\), it prepares \(B\)_buckets_, which are indexed as the pair \((\ell,b)\) with \(\ell\in[L]\) and \(b\in[B]\). In the context of LSH, we draw \(L\) independent samples of hash functions \(h^{(\ell)}\) from a single hash family \(\mathcal{H}\), such that \(h^{(\ell)}:\mathbb{R}^{K}\to[B]\). A corpus item \(x\) is inserted in the bucket indexed \((\ell,h^{(\ell)}(\bm{x}))\), for each \(\ell\in[L]\).

Symmetric LSHGiven a query \(q\), a symmetric LSH computes bucket indices \((\ell,h^{(\ell)}(\bm{q}))\) for all \(L\) using the _same_ hash functions \(h^{(\ell)}\) used for indexing the corpus. Only those items \(x\) that are in bucket \((\ell,h^{(\ell)}(\bm{q}))\) are considered as _candidates_; overall, the candidates are in the union of these buckets. In the rest of the paper, we will describe retrieval for one bucket under one trial, with the understanding that \(L\) buckets will contribute candidates. An LSH exists if the query and corpus items are hashed in the same bucket with high (low) probability as long as their similarities are high (low). Formally, we define symmetric LSH as follows.

**Definition 2.1** (Symmetric Locality Sensitive Hashing (LSH)).: Given a domain of queries \(\mathcal{Q}\) and corpus \(\mathcal{X}\) with \(\mathcal{Q},\mathcal{X}\subset\mathcal{Z}\) and a similarity measure \(\operatorname{sim}:\mathcal{Z}\times\mathcal{Z}\to\mathbb{R}\). A distribution over mappings \(\mathcal{H}:\mathcal{Z}\to\mathbb{N}\) is said to be a \((S_{0},cS_{0},p_{1},p_{2})\)-LSH for the similarity function \(\operatorname{sim}(\cdot)\) if for all \(q\in\mathcal{Q}\) and \(x\in\mathcal{X}\) we have, with \(p_{1}>p_{2}\) and \(c<1\),

* if \(\operatorname{sim}(q,x)\geq S_{0}\), then \(\Pr_{h\sim\mathcal{H}}[h(q)=h(x)]\geq p_{1}\)
* if \(\operatorname{sim}(q,x)\leq cS_{0}\), then \(\Pr_{h\sim\mathcal{H}}[h(q)=h(x)]\leq p_{2}\).

The hash family \(\mathcal{H}\) is tailored to the specific choice of similarity function \(\operatorname{sim}(q,x)\) (equivalently, the distance \(d(q,x)\)). When \(\bm{q},\bm{x}\in\mathbb{R}^{K}\) and \(\operatorname{sim}(q,x)=\cos(\bm{q},\bm{x})\), the choice of \(\mathcal{H}\) corresponds to the uncountable set of all hyperplanes in \(K\) dimensions passing through the origin [9]. When \(\operatorname{sim}(q,x)\) is the Jaccard similarity \(|q\cap x|/|q\cup x|\), \(\mathcal{H}\) is the space of _minwise independent_ hash functions [7].

Asymmetric LSH (ALSH)In many applications, like the current setup (1), we have asymmetric similarity where \(\operatorname{sim}(q,x)\neq\operatorname{sim}(x,q)\). In such cases, we employ two different hash families \(\mathcal{G}\) and \(\mathcal{H}\) to determine the bucket of query and corpus respectively. Formally, we define ALSH as follows:

**Definition 2.2** (Asymmetric Locality Sensitive Hashing (ALSH) [35]).: An asymmetric LSH is \((S_{0},cS_{0},p_{1},p_{2})\)-ALSH for a similarity function \(\operatorname{sim}(\bullet,\bullet)\) over \(\mathcal{Q}\), \(\mathcal{X}\) if we have two different distributions over mappings \(\mathcal{G}\) and \(\mathcal{H}\) such that, with \(p_{1}>p_{2}\) and \(c<1\),

* if \(\operatorname{sim}(q,x)\geq S_{0}\) then \(\Pr_{g\sim\mathcal{G},h\sim\mathcal{H}}[g(q)=h(x)]\geq p_{1}\)
* if \(\operatorname{sim}(q,x)\leq cS_{0}\) then \(\Pr_{g\sim\mathcal{G},h\sim\mathcal{H}}[g(q)=h(x)]\leq p_{2}\).

As an example, given \(\|\bm{x}\|\leq 1\), consider \(\operatorname{sim}(q,x)=\bm{q}^{\top}\bm{x}/\|\bm{q}\|_{2}\), which can be re-written as \(\cos(\alpha(\bm{q}),\beta(\bm{x}))\), where \(\alpha(\bm{q})=[0;\bm{q}/\|\bm{q}\|_{2}],\beta(\bm{x})=[\sqrt{1-\|\bm{x}\|_{2} ^{2}},\bm{x}]\). Thus, we can apply random hyperplane hash on both \(\alpha(\bm{x})\) and \(\beta(\bm{x})\) to construct \(g(q)=\operatorname{sign}(\mathbf{w}\cdot\alpha(\bm{q}))\) and \(h(x)=\operatorname{sign}(\mathbf{w}\cdot\beta(\bm{x}))\) with \(\mathbf{w}\sim\mathcal{N}(\bm{0},\bm{I})\). If \(\|\bm{x}\|\) is unbounded, no ALSH exists for \(\operatorname{sim}(q,x)=\bm{q}^{\top}\bm{x}/\|\bm{q}\|_{2}\)[35]. In \((S_{0},cS_{0},p_{1},p_{2})\)-ALSH, retrieval of items with similarity score more than \(S_{0}\) out of a database of items having a similarity score less than \(cS_{0}\) will admit time-complexity \(O(n^{\rho}\log n)\) and space complexity \(O(n^{1+\rho})\) where \(\rho=\log p_{1}/\log p_{2}\)[35].

### Problem statement

Given the set of training queries \(Q\) and corpus \(X\), with supervised relevance scores \(\mathrm{rel}(q,x)\in\{0,1\}\) and the surrogate score \(d(q,x)\) defined in Eq. (1), we aim to design an LSH of the distance \(d(q,x)\) which can efficiently retrieve top-\(\tau\) corpus items for any new query \(q^{\prime}\).

Why are existing methods not suitable?As we discussed in Section 2.1, relevance metrics for popular LSHs are mostly symmetric, _e.g._, cosine, dot-product, and Jaccard similarity. In particular, Jaccard similarity, although commonly used in set-related applications, is not suitable for our problem, where we define \(\mathrm{rel}(q,x)=1\) when \(q\subseteq x\) and \(0\) otherwise -- it is possible that there exists a higher overlap between \(q\) and \(x\) when \(q\not\subseteq x\), and a lower overlap when \(q\subseteq x\). E.g., suppose \(q=\{a,b\}\), \(x_{1}=\{a,b,c,d,e\}\), and \(x_{2}=\{b\}\). Here, \(\mathrm{rel}(q,x_{1})=1\) and \(\mathrm{rel}(q,x_{2})=0\). However, the Jaccard similarity \(J(q,x)\) is not able to reflect the order of \(\mathrm{rel}(q,x)\) since \(J(q,x_{1})=2/5<J(q,x_{2})=1/2\).

As discovered by Charikar [9, Lemma 1], the similarity functions in symmetric LSH are inversely related to a _metric_, which must satisfy symmetry and triangle inequality. Although a query normalized dot product similarity appears asymmetric, it can be expressed using cosine similarity. This readily allows us to use a random hyperplane based (asymmetric) LSH. In contrast, it is not immediately apparent how to find such a connection for our asymmetric hinge distance (1).

## 3 FourierHashNet: A new ALSH for hinge distance search

Overview of our approachWe design an ALSH for \(d(q,x)\) in three steps. In the first step, we construct a suitable dominance similarity function \(\mathrm{sim}(q,x)\) from \(d(q,x)\) in such a way that there exists a probability distribution \(p:\mathbb{R}^{K}\rightarrow[0,1]\) and bounded Fourier representations \(\bm{F}_{q}(\iota\bm{\omega})\) and \(\bm{F}_{x}(\iota\bm{\omega})\) of both query \(q\) and corpus items \(x\) such that

\[\mathrm{sim}(q,x)=\int_{\bm{\omega}\in\mathbb{R}^{K}}\bm{F}_{q}(\iota\bm{ \omega})^{\top}\bm{F}_{x}(\iota\bm{\omega})p(\omega)d\bm{\omega}=\mathbb{E}_{ \bm{\omega}\sim p(\bullet)}[\bm{F}_{q}(\iota\bm{\omega})^{\top}\bm{F}_{x}( \iota\bm{\omega})]\] (2)

In the second step, we approximate the expected value of the \(\bm{F}_{q}(\iota\bm{\omega})^{\top}\bm{F}_{x}(\iota\bm{\omega})\) using a finite sample of Fourier features. This allows us to apply random hyperplane LSH, similar to asymmetric dot product LSH. However, these hyperplanes are drawn from an isotropic Gaussian distribution in a data-oblivious manner, which results in suboptimal bucket distribution in terms of accuracy-efficiency trade off. To tackle this issue, in the third step, we train the random hyperplanes \(\bm{W}\) which takes the Fourier features as input and give (soft) binary hashcodes, which are optimized to effectively trade off between accuracy and efficiency. Next, we provide the details of the above three steps.

### Design of dominance similarity function \(\mathrm{sim}(q,x)\) from hinge distance

Limitations of simple choices of dominance similarity function \(\mathrm{sim}(q,x)\)A dominance similarity function \(\mathrm{sim}(q,x)\) is inversely related to the hinge distance \(d(q,x)\). Chierichetti and Kumar [11] characterized that, any function of a similarity measure is LSHable, if and only if this function is a probability generating function. However, this characterization applies only to symmetric LSH and no such guiding principle is available for an ALSH. In this context, one can experiment with simple designs of \(\mathrm{sim}\) that are inversely related to \(d\). An immediate choice is \(\mathrm{sim}(q,x)=-d(q,x)\)

[MISSING_PAGE_FAIL:5]

transformation of these kernels are probability distributions. However, in Eq. (8), there is no such readily available probability distribution. In response, we attempt to find out a probability distribution \(p(\bm{\omega})\) which allows us to draw samples using an importance sampling like procedure, as follows:

\[\mathrm{sim}(q,x)=\mathbb{E}_{\bm{\omega}\sim p(\bm{\omega})}\left[\bm{F}_{q}( \iota\bm{\omega})^{\top}\bm{F}_{x}(\iota\bm{\omega})\right],\;\text{where, }\bm{F}_{q}(\iota\bm{\omega})=\frac{\bm{S}_{q}(\iota\bm{\omega})}{\sqrt{p( \bm{\omega})}},\bm{F}_{x}(\iota\bm{\omega})=\frac{\bm{S}_{x}(\iota\bm{\omega}) }{\sqrt{p(\bm{\omega})}},\] (9)

Let \(\{\bm{\omega}^{j}\}_{j=1}^{M}\sim p(\bm{\omega})\) be \(M\) i.i.d random samples. We compute the Monte Carlo estimate as follows:

\[\mathrm{sim}(q,x)\approx\frac{1}{M}\sum_{j\in[M]}\bm{F}_{q}(\iota\bm{\omega}^ {j})^{\top}\bm{F}_{x}(\iota\bm{\omega}^{j})\propto\cos(\bm{F}_{q}(\iota\bm{ \omega}^{1..M}),\bm{F}_{x}(\iota\bm{\omega}^{1..M}))\] (10)

Here, \(\bm{F}_{\bullet}(\iota\bm{\omega}^{1..M})=[\bm{F}_{\bullet}(\iota\bm{\omega}^ {1}),..,\bm{F}_{\bullet}(\iota\bm{\omega}^{M})]\). Note that, as suggested by Eqs. (7) and (9), \(||\bm{F}_{q}(\iota\bm{\omega}^{1..M})||_{2}=||\bm{F}_{x}(\iota\bm{\omega}^{1.. M})||_{2}=\sum_{j=1}^{M}\sum_{k=1}^{K}\frac{|\mathrm{Re}\left(S(\omega_{k}^{j}) \right)|+|\mathrm{Im}\left(S(\iota\omega_{k}^{j})\right)|}{p(\omega_{k}^{j})}\). Thus, the value is independent of the query or corpus, which leads to the proportionality relation. We choose the probability distribution \(p(\bm{\omega})\) guided the proportionality constant \(||\bm{F}_{\bullet}(\iota\bm{\omega}^{1..M})||\) and set \(p(\bm{\omega})=\prod_{k\in[K]}p(\omega)\), where \(p(\omega)\propto|\mathrm{Re}(S(\iota\omega))|+|\mathrm{Im}(S(\iota\omega))|\). However, the integral of these terms may not be bounded. Therefore, we set the support of \(p(\omega)\) between \([-\omega_{\max},\omega_{\max}]\), thus eliminating the higher frequency terms. The effect on the overall score is small. Attenuation of the higher frequency signals can be seen as a multiplication with a low pass filter in the frequency domain, which affects a convolution in the time domain. Its impact on the similarity score is proportional to \(1/\omega_{\max}\). This still allows us for ALSH despite frequency truncation.

**Theorem 3.2**.: (Proven in Appendix D) Let \(\bm{q},\bm{x}\in\mathbb{R}^{K}\), \(\cos^{-1}\) be Lipschitz with Lipschitz constant \(L_{\cos}\); the hyperparameter \(T\) in Eq. (4) be chosen such that \(T>||\bm{q}-\bm{x}||_{\infty}\); the frequency sampling distribution \(p(\omega_{k}^{j})\propto[|\mathrm{Re}(S(\omega_{k}^{j}))|+|\mathrm{Im}(S( \omega_{k}^{j}))|]\) with the support set \(\omega_{k}^{j}\in[-\omega_{\max},\omega_{\max}]\) and the proportionality constant \(I(\omega_{\max})=\int_{-\omega_{\max}}^{-\omega_{\max}}|\mathrm{Re}(S(\omega) |+|\mathrm{Im}(S(\omega))|d\omega\). Then, the mapping \(g(q)[i]=\mathrm{sign}(\mathbf{w}_{i}^{\top}\bm{F}_{q}(\omega^{1..M}))\) and \(h(x)[i]=\mathrm{sign}(\mathbf{w}_{i}^{\top}\bm{F}_{x}(\bm{\omega}^{1..M}))\) where \(\mathbf{w}_{i}\sim N(0,\mathbb{I})\), constitutes a \((S_{0},cS_{0},p_{1},p_{2})\)-ALSH for some \(p_{1}\) and \(p_{2}\) if we choose the support set \([-\omega_{\max},\omega_{\max}]\) and the number of samples \(M\) as follows:

\[\omega_{\max}>\frac{4KL_{\cos}}{\pi(1-c)S_{0}}\left(6+\frac{2T}{T-\max_{\bm{x},\bm{q}}||\bm{x}-\bm{q}||_{\infty}}\right)\;\text{and}\;M>\left[\frac{4L_{\cos} }{(1-c)S_{0}}\right]^{2}KI(\omega_{\max}).\] (11)

Based on the outlined assumptions, the above Theorem guarantees that FourierHashNet is an \((S_{0},cS_{0},p_{1},p_{2})\)-ALSH for the asymmetric dominance similarity score, subject to appropriate choices of \(\omega_{\max}\) to bound the effect of frequency truncation and \(M\) to bound the variance of the Monte Carlo sample estimate.

### Trainable hashing network

Random hyperplane LSHEq. (10) provides an asymmetric transformation on the input query-corpus pair, which maps it into the cosine similarity space, thus allowing for Random Hyperplanes hashing. We sample \(H\) spherically symmetrically distributed normal vectors \(\{\mathbf{w}_{i}\}_{i=1}^{H}\), _i.e._, \(\mathbf{w}_{i}\sim\mathcal{N}(0,\mathbb{I})\), each perpendicular to a random hyperplane passing though the origin. For each query \(q\) and the corpus \(x\), we can generate \(H\)-bit hashcodes \(g(q),h(x)\in\{\pm 1\}^{H}\) from the Fourier features (10) as follows: \(g(q)[i]=\mathrm{sign}(\mathbf{w}_{i}^{\top}\bm{F}_{q}(\iota\bm{\omega}^{1..M}))\) and \(h(x)[i]=\mathrm{sign}(\mathbf{w}_{i}^{\top}\bm{F}_{x}(\iota\bm{\omega}^{1..M}))\). Consequently, we can index the given corpus with \(N\) items, into a hash table with \(2^{H}\) buckets. For each query \(q\), we restrict our search within bucket \(b=g(q)\). If the corpus items are uniformly distributed across all buckets, then it enables sub-quadratic time retrieval with \(N/2^{H}\) comparisons (per trial).

Data driven hashcode generationThe above random hyperplane LSH approach suffers from two distinct limitations: (1) the quality of Monte Carlo approximation obtained in Eq. (10), depends on the suitability of \(p(\bm{\omega})\), and (2) the hyperplanes are data oblivious. Data oblivious hyperplanes provide the best efficiency if the corpus embeddings are uniformly spread over the \(K\) dimensional sphere, which allows the random hyperplanes to evenly allocate the corpus items across different hashcodes. However, in practice, the spatial distribution of the embeddings is not uniform. This results in a skewed distribution of the corpus items across the hash buckets.

To tackle the first problem, we improve the quality of the Fourier features through a trainable nonlinear transformation. Here, we use two networks \(\phi_{q}\) and \(\phi_{x}\) which takes the Fourier features for the query and corpus, _i.e._, \(\bm{F}_{q}(\iota\bm{\omega}^{1..M})\) and \(\bm{F}_{x}(\iota\bm{\omega}^{1..M})\) as input and outputs corresponding transformed Fourier representations \(\bm{z}_{q}=\phi_{q}(\bm{F}_{q}(\bm{\omega}^{1..M}))\) and \(\bm{z}_{x}=\phi_{x}(\bm{F}_{x}(\bm{\omega}^{1..M}))\). We train \(\phi_{q}\) and \(\phi_{x}\) by minimizing a BCE loss on \(\{\cos(\bm{z}_{q},\bm{x}_{x}),\mathrm{rel}(q,x)\}\) pairs for \(q\in Q\) and \(x\in X\) as follows:

\[\min_{\phi_{q},\phi_{x}}\sum_{q\in Q,x\in X}-[\mathrm{rel}(q,x)\log(1+\cos(\bm{z }_{q},\bm{z}_{x}))+(1-\mathrm{rel}(q,x)\log(1-\cos(\bm{z}_{q},\bm{z}_{x}))]\] (12)

Next, we train the random hyperplanes \(\bm{W}=[\bm{\mathbf{w}}_{1},\bm{\mathbf{w}}_{2},..]\) using the transformed Fourier features \(\{\bm{z}_{q}\}\) and \(\{\bm{z}_{x}\}\). The final hashcodes \(g(q)\) and \(h(x)\) are obtained as \(g(q)=\mathrm{sign}(\widehat{\bm{W}}\bm{z}_{q})\), \(h(x)=\mathrm{sign}(\widehat{\bm{W}}\bm{z}_{x})\), where \(\widehat{\bm{W}}\) are the final trained random hyperplanes. For training purposes, we use \(\tanh(\bm{W}\bullet)\) as a smooth surrogate of \(\mathrm{sign}(\bm{W}\bullet)\). The loss function \(\text{loss}(Q,X\,|\,\bm{W})\) used to train \(\bm{W}\) consists of three components.

(1) Collision minimizerFor any query \(q\), our goal is to ensure that assigned bucket contains only positive items. Assuming corpus items are uniformly distributed across buckets, we ensure that for any query \(q\), the \(N/2^{H}\) most relevant items \(X_{q^{\prime}}\) measured in terms of \(d(q,x)\) will have higher amount of bit overlap than rest of the items \(X_{q^{\prime}}\). Here, \(X_{q^{\prime}}\) and \(X_{q^{\prime}}\) indicate positive and negative silver instances (not gold instances) indicating top \(N/2^{H}\) items _in terms of the (possibly trained) hinge distance \(d(q,x)\)_. We encode this by minimizing the following ranking loss.

\[\Delta_{1}=\sum_{q\in Q}\sum_{x\in X_{q^{\prime}},x^{\prime}\in X_{q^{\prime} }}\bigl{[}1+\tanh(\bm{W}\bm{z}_{q})^{\top}\tanh(\bm{W}\bm{z}_{x^{\prime}})- \tanh(\bm{W}\bm{z}_{q})^{\top}\tanh(\bm{W}\bm{z}_{x})\bigr{]}_{+}\] (13)

This loss encourages that \(\tanh(\bm{W}\bm{z}_{q})^{\top}\tanh(\bm{W}\bm{z}_{x})>\tanh(\bm{W}\bm{z}_{q})^ {\top}\tanh(\bm{W}\bm{z}_{x^{\prime}})+1\), _i.e._, the number of common bits between \(q\) and \(x\in X_{q^{\prime}}\) is atleast one more than the same between \(q\) and \(x^{\prime}\).

(2) Fence SittingWe set fence sitting loss as \(\Delta_{2}=\sum_{x\in X}|||\tanh(\bm{W}\bm{z}_{x})|-1|||_{1}\). This prevents the optimizer from arriving at a trivial solution by setting all hashcodes to zero.

(3) Bit BalanceWe set the bit balance loss as \(\Delta_{3}=\sum_{i\in[H]}|\sum_{x\in X}\tanh(\bm{W}\bm{z}_{x})[i]|\). This enforces that each position should have an equal number of \(+1\) and \(-1\), thus ensuring that each random hyperplane evenly splits the set of points. Finally, we estimate \(\bm{W}\) by minimizing the loss, with \(\lambda_{\bullet}\) as hyperparameters such that \(\sum_{i}\lambda_{i}=1\), which is given as follows:

\[\text{loss}(Q,X\,|\,\bm{W})=\lambda_{1}\Delta_{1}+\lambda_{2}\Delta_{2}+ \lambda_{3}\Delta_{3},\] (14)

Algorithm 1 summarizes the overall procedure.

```
1:functionTrain(\(X\),\(\{\mathrm{rel}(q,x)\}_{q\in Q,x\in X}\))
2: Draw \(\bm{\omega}^{1..M}\sim p(\bm{\omega})\)
3: Compute \(\bm{F}_{q}(\bm{\omega}^{1..M}),\bm{F}_{x}(\bm{\omega}^{1..M})\) (Eq. (9))
4: Train \(\phi_{q}\), \(\phi_{x}\) from \(\mathrm{rel}(q,x)\), \(\bm{F}_{\bm{\zeta}}(\bm{\omega}^{1..M})\) (Eq. (12))
5: Train \(\bm{W}\) by minimizing the loss (14)
6:Return \(\widehat{\phi}_{x},\widehat{\phi}_{q},\widehat{\bm{W}}\)
7:
8:functionIndex\(\{\bm{F}_{x}(\bm{\iota}(\bm{\omega}^{1..M}))_{x\in X}\}\)
9:Require: Trained networks \(\widehat{\phi}_{x},\widehat{\bm{W}}\)
10:\(h(x)\leftarrow\mathrm{sign}(\widehat{\bm{W}}\widehat{\phi}_{x}(\bm{F}_{x}(\bm{ \iota}\bm{\omega}^{1..M})))\)\(\forall x\in X\)
11:for\(x\in X\)do
12: hash \(x\) to bucket \(b=h(x)\)
13:Return the bucket sets \(B\)
14:functionRetrie(\(q^{\prime}\))
15:Require: Trained networks \(\widehat{\phi}_{q},\widehat{\bm{W}}\)
16: Compute \(\bm{F}_{q}(\bm{\omega}^{1..M})\) based on \(\bm{q}^{\prime}\)
17:\(g(q^{\prime})\leftarrow\mathrm{sign}(\widehat{\bm{W}}\widehat{\phi}_{q}(\bm{F}_{q}( \bm{\iota}\bm{\omega}^{1..M})))\)
18: Rank all \(x\) in the bucket \(b=g(q^{\prime})\) based on the distance \(d(q^{\prime},x)\) to obtain the list \(\text{List}_{q^{\prime}}\).
19:Return \(\text{List}_{q^{\prime}}\) ```

**Algorithm 1** FourierHashNet

Algorithm 1 summarizes the overall procedure.

Difference from existing trainable LSHLSH training has been extensively studied [54; 15; 43], with Fence Sitting and Bit Balance losses being well known. However, the Collision Minimizer loss differs significantly from existing approaches. Current techniques seek to ensure load balance across hash buckets for all corpus items, including the ones that may not be relevant to most queries. This is unnecessary for query workloads which touch upon only a small subset of the corpus to generate the best responses. In contrast, our Collision Minimizer loss ensures that only the top-most bucket for any given query allows relevant items and explicitly denies irrelevant items. Thus, it is informed by the query workload, rather than assuming load balance for all items in the corpus. Such an approach may result in balanced bucket loads, but not necessarily.

## 4 Experiments

In this section, we provide a comprehensive evaluation of our method against several baselines and ablations on four datasets. Appendix F describes additional experiments. Our code is in https://github.com/structlearning/fhashnet.

### Experimental setup

DatasetsWe experiment on datasets sampled from anonymized real-world Web log data, _viz_, MsWeb and MsNbc. MsWeb[5] is generated using logs from _www.microsoft.com_, containingrecords of the areas of the website visited by the users. MsNbc[8] is a collection of logs of user page requests from _msnbc.com_. In both cases, a record (either \(q\) or \(x\)), is a passage that is regarded as a bag of words. Given a collection \(V\) of such word bags, (\(|V|=11234\) for MsWeb and \(|V|=111290\) for MsNbc), we sample \(|Q|=500\) bags from \(V\), designating them as queries, and designate the rest as corpus items \(X=V\backslash Q\). Consistent with typical information retrieval application scenarios [51], we generate gold relevance labels based on (multi)set containment for MsWeb (MsNbc). (Additional methods for evaluation are explored in Appendix F.) We build the query set \(Q\), such that the number of relevant items \(N_{q\oplus}=|\{x\in X:\mathrm{rel}(q,x)=+1\}|\in[5,500]\) for each query \(q\). We create four datasets by changing average relevance counts per query, \(\overline{N}_{q\oplus}\). They are: (1) MsWeb-1 where \(\overline{N}_{q\oplus}=35.624\). (2) MsWeb-2 where \(\overline{N}_{q\oplus}=20.392\). (3) MsNbc-1 where \(\overline{N}_{q\oplus}=24.09\) (4) MsNbc-2 where \(\overline{N}_{q\oplus}=19.78\) The set of queries \(Q\) is partitioned into 20% training set \(Q_{\text{tr}}\), 20% validation set \(Q_{\text{dev}}\) and 60% test set \(Q_{\text{test}}\).

Design of query and corpus embeddings \(\boldsymbol{q},\boldsymbol{x}\)We begin with a pre-trained sentence transformer model [41] to obtain 768 dimensional dense contextual representations \(\boldsymbol{\text{feature}}_{q}\) and \(\boldsymbol{\text{feature}}_{x}\) for the each word in bags \(q\) and \(x\). Embeddings of words belonging to a bag are fed into a deep set [60] network to obtain a bag representation \(\boldsymbol{q},\boldsymbol{x}\in\mathbb{R}^{K}\), with \(K=294\) (chosen via hyperparameter sweep). To train the parameters inside the deep set network, we use \(\boldsymbol{q}\), \(\boldsymbol{x}\) to compute the proposed asymmetric hinge distance \(d(q,x)\) (1), feed it into a trainable sigmoid layer \(\sigma\) and minimize

\[\sum_{q,x}\mathrm{BCE}\left(\mathrm{rel}(q,x),\sigma(-d(q,x))\right)\] (15)

which uses a BCE loss on the gold relevance labels. Once we obtain \(\boldsymbol{q}\) and \(\boldsymbol{x}\), we use Algorithm 1 to obtain trained \(\widehat{\phi}_{q}\), \(\widehat{\phi}_{x}\) and \(\widehat{\boldsymbol{W}}\) (\(\text{Train}(\cdot)\)), which are then used for indexing (\(\text{Index}(\cdot)\)).

EvaluationGiven a test query \(q\in Q_{\text{test}}\) and a set of \(N^{\prime}_{q}\) candidate corpus items, we rank them in increasing order of their hinge distances \(d(q,x)\). Then we evaluate the average precision (AP) for the query and average over queries to report mean average precision (MAP) -- see Appendix E.6.

### Effect of different similarity measures on LSH

SetupHere, we compare FourierHashNet against the three LSH baselines, _viz_, Random hyperplane (RH) [9], Dot product LSH (DP-RH) [35] and Weighted MinHash (WMH) [12], that are tailored towards cosine similarity, dot product similarity and Weighted Jaccard Similarity, respectively. For each LSH method, we train the embeddings \(\boldsymbol{q}\), \(\boldsymbol{x}\) and the final hashcodes \(g(q)\) and \(h(x)\) using the same networks, as in our method. Furthermore, we set the final relevance measure for ranking to be the similarity score for which the LSH is designed.

ResultsWe vary the mixing hyperparameters \(\lambda_{1}\) and \(\lambda_{2}\) in our loss (14) and the number of buckets \(B\) to explore the tradeoff between accuracy (MAP) and average query time. In Figure 2, we summarize the results. We observe that: **(1)** FourierHashNet outperforms all the baselines by providing significantly better time-vs.-MAP trade-off across all datasets. In MsWeb datasets, all the baselines except DP-RH show poor performance. All baselines perform poorly for the MsNbc dataset. We remark that cosine similarity, dot product or weighted Jaccard similarity are not suited for vector dominance search. Therefore, the maximum possible MAP obtained by them are severely constrained. **(2)** In MsWeb datasets, DP-RH performs moderately, by achieving a MAP value around 0.4-0.42 within 0.03 seconds (average query time). This is because dot product can be computed significantly faster than all the other distance/similarity measures. In particular, it is \(\sim\)\(7.5\times\) faster than our hinge distance (1), \(\sim\)\(10.3\times\) faster than cosine, and \(\sim\)\(5.1\times\) faster than Jaccard similarity.

Figure 2: Effect of different similarity measures on LSH, measured in terms of variation of MAP vs. average query time (in sec) for all methods. Here, the final score used for ranking the relevant items is that similarity score for which the LSH is designed for.

### Comparison against other efficient indexing techniques

In Section 4.2, we used the similarity score corresponding to each LSH method for final candidate ranking. The baselines performed poorly, which may result from a poor choice of final similarity score or the indexing method. Here, we evaluate FourierHashNet against baseline indexing methods applied on hinge distance guided embeddings. We consider LSH and Inverted File Indexing (IVF) based indexing variants in this section, and discuss graph-based indexing methods in Appendix F.

**Comparison with LSH based indexing**. Shrivastava and Li [47] showed that an LSH not tailored to the final scoring function may still provide an effective filter. Accordingly, we set the final similarity function to be dominance similarity, and compare against four possible LSH baselines.

Given the embeddings \(\bm{q},\bm{x}\) trained (15) using hinge distance, we feed them into the four baselines, each of which trains a hashing network in a different way. **(1)** RH+Hinge: We train a set of random hyperplanes represented by \(\bm{W}\) and compute the hashcodes as \(h(q)=\mathrm{sign}(\bm{W}\bm{q})\) and \(h(x)=\mathrm{sign}(\bm{W}\bm{x})\). **(2)** DP-RH+Hinge: We train random hyperplanes \(\bm{W}\) for these embeddings to compute the hashcodes as \(g(q)=\mathrm{sign}(\bm{W}[0,\bm{q}/||\bm{q}||])\) and \(h(x)=\mathrm{sign}(\bm{W}[\sqrt{T^{2}-||\bm{x}||^{2}},\bm{x}])\). **(3)** WMH+Hinge: We use the best performing WMH implementation from DrHash toolkit [58] to obtain the hashcodes. **(4)** FLORA[15]: We train asymmetric hash networks (net\({}_{1}\), net\({}_{2}\)) using an end-to-end data-driven approach, which minimizes bit balance and decorrelation loss, along with a consistency loss which predicts the final similarity score using \(\cos(\text{net}_{1}(\bm{q}),\text{net}_{2}(\bm{x}))\).

Figure 3 compares the performance of FourierHashNet, RH+Hinge, DP-RH+Hinge, WMH+Hinge and FLORA in terms of MAP for MsWeb and MsNbc datasets. Here we analyze the section of the trade-off curve which provides \(\geq\)10X speedup compared to exhaustive search. The complete tradeoff curve is provided in Appendix F. **(1)** The newly designed baselines are now seen to perform significantly better than those used in the previous experiments with Figure 2. However FourierHashNet still outperforms all the baselines. **(2)** RH+Hinge, despite achieving the second highest scores in many cases, is seen to suffer from a large variance in performance within any given time budget. This would make it difficult to tune the hyperparameters to achieve the requisite performance v/s retrieval speed trade-off. **(3)** DP-RH+Hinge is seen to have a significantly worse performance than FourierHashNet everywhere. This indicates that DP-RH is ill-suited to asymmetric hinge distance based retrieval.**(4)** We observe that for the same amount of query time invested, FLORA's MAP can lag ours by over 10%, particularly when faster average query times are required. FLORA's hyperparameter tuning is also more delicate, with there being unsuccessful settings (where MAP grows very slowly with query time) very close to relatively successful ones.

**Comparison with IVF indexing** We use the widely used FAISS-IVF [21] library, which supports IVF indexing based on L2 distance (IVF-L2) and Inner Product similarity (IVF-IP). Additionally, we propose an alternative Fourier+IVF+IP, where we apply Fourier transformation on the input embeddings, before using IVF-IP. We provide embeddings \(\bm{q},\bm{x}\), trained using hinge distance to all the methods, and use hinge distance to rank retrieved items.

Figure 4 compares the performances in terms of MAP, across all datasets. We observe that: **(1)** FourierHashNet outperforms both IVF-L2 and IVF-IP across all datasets. FAISS-IVF retrieval suffers because its quantizers, that assign vectors to the Voronoi cells, rely on a metric like L2 or IP, which are unsuitable for asymmetric hinge distance. **(2)** Fourier transformation provides a significant boost in performance across all datasets, as seen while comparing Fourier+IVF+IP against IVF+IP. However, FourierHashNet still outperforms Fourier+IVF+IP, most noticeably in MsWeb-2.

Figure 3: Trade-off between query time and accuracy (MAP) for MsWeb and MsNbc datasets where there is \(\geq\)10X speedup compared to exhaustive search. We apply different LSH methods on hinge distance guided embeddings, _viz_, RH+Hinge, DP-RH+Hinge, WMH+Hinge, FLORA and FourierHashNet; and, then use the hinge distance to finally rank the retrieved items.

### Ablation study

Data driven vs data oblivious LSHTo perform ablation study on our proposed hashcode training method, we propose an alternative FHash (untrained). This applies our Fourier features followed by a _data oblivious_ random hyperplane LSH, without any data driven hashcode training.

In Figure 5, we compare the complete design of our method, _i.e._, FourierHashNet and FHash (untrained) against the untrained versions of RH+Hinge and DP-RH+Hinge. We make the following observations: **(1)** Benef of Fourier transformation: The MAP vs time trade-off curve of FHash (untrained), consistently dominates all the baselines across both datasets. **(2)** Benef of hashcode training: Compared to FHash (untrained), we observe that FourierHashNet allows for significantly more choices of trade-off points, where higher MAP is required.

Ablation on collision minimizerHere, we replace the collision minimizer in loss\((Q,X\,|\,\boldsymbol{W})\) (14) with decorrelation loss which encourages hashcodes to be dissimilar: \(\Delta_{1}=\sum_{x\neq y}|\tanh(\boldsymbol{W}\boldsymbol{z}_{x})^{\top} \tanh(\boldsymbol{W}\boldsymbol{z}_{y})|\), a commonly used loss in prior work [54; 15].

Figure 6 compares the performance of the two variations of the losses in terms of MAP, for MsWeb datasets. We observe that: **(1)** Our loss containing the collision minimizer term performs better than its variant which uses the decorrelation loss. In MsWeb-2, latter provides a MAP of \(0.4\) in \(0.014\) secs, which our loss achieves in 50% of the time. **(2)** Our method allows for greater freedom in navigating the performance vs average query time trade-off, as seen in MsWeb-2, as it is more spread out across the time axis.

## 5 Conclusion

We have presented FourierHashNet, an ALSH for asymmetric hinge distance, strongly motivated by text, image and graph retrieval applications. By converting hinge distance to a proposed dominance similarity and applying a suitable Fourier transform to the dominance similarity, we can estimate the distance as an inner product over an importance-sampled spectrum, which further enables the use of a trainable LSH in the frequency domain. Experiments show that FourierHashNet dramatically speeds up queries while preserving or improving retrieval accuracy. Our approach can be extended to any shift invariant functions including Chamfer distance, box embeddings, etc. Box embeddings are known to model more complex set operations like set overlap and set difference [42; 13], making them an interesting avenue for future research. One limitation of FourierHashNet compared to simple symmetric LSHs is the increase in computational cost to compute the Fourier transform. One can explore other types of transformations to mitigate this cost.

Figure 4: Trade-off between number of corpus items being evaluated and accuracy (MAP) for MsWeb and MsNbc datasets where there is \(\geq\)10X speedup compared to exhaustive search. We apply FourierHashNet and different IVF methods, _viz_, IVF+L2, IVF+IP, and Fourier+IVF+IP, on hinge distance guided embeddings; and then use the hinge distance to finally rank the retrieved items.

Figure 5: Effect of untrained RH

Figure 6: Collision minimizer vs. decorrelation.

## References

* Andoni et al. [2018] Alexandr Andoni, Piotr Indyk, and Ilya Razenshteyn. Approximate nearest neighbor search in high dimensions. In _Proceedings of the International Congress of Mathematicians: Rio de Janeiro 2018_, pages 3287-3318. World Scientific, 2018.
* Auvolat et al. [2015] Alex Auvolat, Sarath Chandar, Pascal Vincent, Hugo Larochelle, and Yoshua Bengio. Clustering is efficient for approximate maximum inner product search. _arXiv preprint arXiv:1507.05910_, 2015.
* Avron et al. [2014] Haim Avron, Huy Nguyen, and David Woodruff. Subspace embeddings for the polynomial kernel. _Advances in neural information processing systems_, 27, 2014.
* Bowman et al. [2015] Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpus for learning natural language inference. _arXiv preprint arXiv:1508.05326_, 2015.
* Breese et al. [2013] John S Breese, David Heckerman, and Carl Kadie. Empirical analysis of predictive algorithms for collaborative filtering. _arXiv preprint arXiv:1301.7363_, 2013.
* Broder [1997] Andrei Z Broder. On the resemblance and containment of documents. In _Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)_, pages 21-29. IEEE, 1997.
* Broder et al. [1998] Andrei Z Broder, Moses Charikar, Alan M Frieze, and Michael Mitzenmacher. Min-wise independent permutations. In _Proceedings of the thirtieth annual ACM symposium on Theory of computing_, pages 327-336, 1998.
* Cadez et al. [2000] Igor Cadez, David Heckerman, Christopher Meek, Padhraic Smyth, and Steven White. Visualization of navigation patterns on a web site using model-based clustering. In _Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 280-284, 2000.
* Charikar [2002] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In _Proceedings of the thiry-fourth annual ACM symposium on Theory of computing_, pages 380-388, 2002.
* Chheda et al. [2021] Tejas Chheda, Purujit Goyal, Trang Tran, Dhruvesh Patel, Michael Boratko, Shib Sankar Dasgupta, and Andrew McCallum. Box embeddings: An open-source library for representation learning using geometric structures. _arXiv preprint arXiv:2109.04997_, 2021. URL https://www.iesl.cs.umass.edu/box-embeddings/main/index.html.
* Chierichetti and Kumar [2015] Flavio Chierichetti and Ravi Kumar. Lsh-preserving functions and their applications. _Journal of the ACM (JACM)_, 62(5):1-25, 2015.
* Chum et al. [2008] Ondrej Chum, James Philbin, Andrew Zisserman, et al. Near duplicate image detection: Min-hash and tf-idf weighting. In _Bmvc_, volume 810, pages 812-815, 2008.
* Dasgupta et al. [2021] Shib Sankar Dasgupta, Michael Boratko, Siddhartha Mishra, Shriya Atmakuri, Dhruvesh Patel, Xiang Lorraine Li, and Andrew McCallum. Word2box: Capturing set-theoretic semantics of words using box embeddings. _arXiv preprint arXiv:2106.14361_, 2021.
* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _NAACL Conference_, 2019. URL https://www.aclweb.org/anthology/N19-1423.pdf.
* Doan et al. [2022] Khoa Doan, Shulong Tan, Weijie Zhao, and Ping Li. Asymmetric hashing for fast ranking via neural network measures. _arXiv preprint arXiv:2211.00619_, 2022.
* Gollapudi and Panigrahy [2006] Sreenivas Gollapudi and Rina Panigrahy. Exploiting asymmetry in hierarchical topic extraction. In _Proceedings of the 15th ACM international conference on Information and knowledge management_, pages 475-482, 2006.
* Grant et al. [2013] E. Grant, C. Hegde, and P. Indyk. Nearly optimal linear embeddings into very low dimensions. In _Global Conference on Signal and Information Processing (GlobalSIP)_, 2013.

* Hamilton et al. [2017] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30, 2017.
* Indyk et al. [2017] Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Practical data-dependent metric compression with provable guarantees. _Advances in Neural Information Processing Systems_, 30, 2017.
* Ioffe [2010] Sergey Ioffe. Improved consistent sampling, weighted minhash and l1 sketching. In _2010 IEEE international conference on data mining_, pages 246-255. IEEE, 2010.
* Johnson et al. [2019] Jeff Johnson, Matthijs Douze, and Herve Jegou. Billion-scale similarity search with GPUs. _IEEE Transactions on Big Data_, 7(3):535-547, 2019.
* Johnson et al. [2015] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3668-3678, 2015.
* Kar and Karnick [2012] Purushottam Kar and Harish Karnick. Random feature maps for dot product kernels. In _Artificial intelligence and statistics_, pages 583-591. PMLR, 2012.
* Karpukhin et al. [2020] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 6769-6781, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://www.aclweb.org/anthology/2020.emnlp-main.550.
* Kipf and Welling [2016] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* Lai and Hockenmaier [2017] Alice Lai and Julia Hockenmaier. Learning to predict denotational probabilities for modeling entailment. In _Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers_, pages 721-730, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://aclanthology.org/E17-1068.
* Lee et al. [2019] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In _International conference on machine learning_, pages 3744-3753. PMLR, 2019.
* Li [2015] Ping Li. 0-bit consistent weighted sampling. In _Proceedings of the 21th ACM SIGKDD International conference on knowledge discovery and data mining_, pages 665-674, 2015.
* Lin et al. [2021] Jimmy J. Lin, Rodrigo Nogueira, and Andrew Yates. Pretrained transformers for text ranking: Bert and beyond. _WSDM Conference_, 2021. URL https://dl.acm.org/doi/pdf/10.1145/3437963.3441667.
* Liu et al. [2020] Fanghui Liu, Xiaolin Huang, Yudong Chen, and Johan AK Suykens. Random features for kernel approximation: A survey on algorithms, theory, and beyond. _arXiv preprint arXiv:2004.11154_, 2020.
* Lou et al. [2020] Zhaoyu Lou, Jiaxuan You, Chengtao Wen, Arquimedes Canedo, Jure Leskovec, et al. Neural subgraph matching. _arXiv preprint arXiv:2007.03092_, 2020.
* Lv et al. [2007] Qin Lv, William Josephson, Zhe Wang, Moses Charikar, and Kai Li. Multi-probe lsh: efficient indexing for high-dimensional similarity search. In _Proceedings of the 33rd international conference on Very large data bases_, pages 950-961, 2007.
* Malkov and Yashunin [2018] Yu A Malkov and Dmitry A Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. _IEEE transactions on pattern analysis and machine intelligence_, 42(4):824-836, 2018.
* Mitra and Craswell [2018] Bhaskar Mitra and Nick Craswell. An introduction to neural information retrieval. _Foundations and Trends in Information Retrieval_, 13:1-126, 2018. URL https://www.nowpublishers.com/article/Details/INR-061.

* Neyshabur and Srebro [2015] Behnam Neyshabur and Nathan Srebro. On symmetric and asymmetric lshs for inner product search. In _International Conference on Machine Learning_, pages 1926-1934. PMLR, 2015.
* Pagh et al. [2005] Anna Pagh, Rasmus Pagh, and S Srinivasa Rao. An optimal bloom filter replacement. In _Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms_, pages 823-829, 2005.
* Peng et al. [2021] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Random feature attention. _arXiv preprint arXiv:2103.02143_, 2021.
* Pham and Pagh [2013] Ninh Pham and Rasmus Pagh. Fast and scalable polynomial kernels via explicit feature maps. In _Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 239-247, 2013.
* Raginsky and Lazebnik [2009] Maxim Raginsky and Svetlana Lazebnik. Locality-sensitive binary codes from shift-invariant kernels. _Advances in neural information processing systems_, 22, 2009.
* Rahimi and Recht [2007] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. _Advances in neural information processing systems_, 20, 2007.
* Reimers and Gurevych [2019] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084.
* Ren et al. [2020] Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs in vector space using box embeddings. _arXiv preprint arXiv:2002.05969_, 2020.
* Roy et al. [2021] Indradyumna Roy, Abir De, and Soumen Chakrabarti. Adversarial permutation guided node representations for link prediction. In _AAAI Conference_, 2021. URL https://arxiv.org/abs/2012.08974.
* Rudin [2017] Walter Rudin. _Fourier analysis on groups_. Courier Dover Publications, 2017.
* Shen et al. [2015] Fumin Shen, Wei Liu, Shaoting Zhang, Yang Yang, and Heng Tao Shen. Learning binary codes for maximum inner product search. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 4148-4156, 2015.
* Shrivastava and Li [2014] Anshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS). _NeurIPS_, abs/1405.5869, 2014. URL https://arxiv.org/pdf/1405.5869.pdf.
* Shrivastava and Li [2014] Anshumali Shrivastava and Ping Li. In defense of minhash over simhash. In _Artificial Intelligence and Statistics_, pages 886-894. PMLR, 2014.
* Skianis et al. [2020] Konstantinos Skianis, Giannis Nikolentzos, Stratis Limnios, and Michalis Vazirgiannis. Rep the set: Neural networks for learning set representations. In Silvia Chiappa and Roberto Calandra, editors, _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108 of _Proceedings of Machine Learning Research_, pages 1410-1420. PMLR, 26-28 Aug 2020. URL https://proceedings.mlr.press/v108/skianis20a.html.
* Song et al. [2021] Zhao Song, David Woodruff, Zheng Yu, and Lichen Zhang. Fast sketching of polynomial kernels of polynomial degree. In _International Conference on Machine Learning_, pages 9812-9823. PMLR, 2021.
* Tancik et al. [2020] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. _Advances in Neural Information Processing Systems_, 33:7537-7547, 2020.
* Terrovitis et al. [2011] Manolis Terrovitis, Panagiotis Bouros, Panos Vassiliadis, Timos Sellis, and Nikos Mamoulis. Efficient answering of set containment queries for skewed item distributions. In _Proceedings of the 14th International Conference on Extending Database Technology_, pages 225-236, 2011.

* Vendrov et al. [2015] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. _arXiv preprint arXiv:1511.06361_, 2015. URL https://arxiv.org/pdf/1511.06361.
* Weinberger et al. [2009] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. Feature hashing for large scale multitask learning. In _ICML_, pages 1113-1120, 2009. URL https://arxiv.org/pdf/0902.2206.pdf.
* Weiss et al. [2008] Yair Weiss, Antonio Torralba, and Rob Fergus. Spectral hashing. _Advances in neural information processing systems_, 21, 2008.
* Wu et al. [2016] Wei Wu, Bin Li, Ling Chen, and Chengqi Zhang. Canonical consistent weighted sampling for real-value weighted min-hash. In _2016 IEEE 16th International Conference on Data Mining (ICDM)_, pages 1287-1292. IEEE, 2016.
* Wu et al. [2017] Wei Wu, Bin Li, Ling Chen, and Chengqi Zhang. Consistent weighted sampling made more practical. In _Proceedings of the 26th International Conference on World Wide Web_, pages 1035-1043, 2017.
* Wu et al. [2018] Wei Wu, Bin Li, Ling Chen, Chengqi Zhang, and S Yu Philip. Improved consistent weighted sampling revisited. _IEEE Transactions on Knowledge and Data Engineering_, 31(12):2332-2345, 2018.
* Wu et al. [2022] Wei Wu, Bin Li, Ling Chen, Junbin Gao, and Chengqi Zhang. A review for weighted minhash algorithms. _IEEE Transactions on Knowledge and Data Engineering_, 34(6):2553-2573, 2022.
* Yan et al. [2018] Xiao Yan, Jinfeng Li, Xinyan Dai, Hongzhi Chen, and James Cheng. Norm-ranging lsh for maximum inner product search. 31, 2018.
* Zaheer et al. [2017] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. _Advances in neural information processing systems_, 30, 2017.

**Locality Sensitive Hashing in Fourier Frequency Domain For Soft Set Containment Search**

**(Appendix)**

## Appendix A Limitations of our work

**(1)** In Eq. (9), the probability distribution \(p(\bm{\omega})\) is determined based on the proportionality constant \(||F_{\bm{\ast}}(\iota\bm{\omega}^{1\dots M})||\), and we set \(p(\bm{\omega})=\prod_{k\in[K]}p(\omega)\), where \(p(\omega)\propto|\mathrm{Re}(S(\iota\omega))|+|\mathrm{Im}(S(\iota\omega))|\). We note that this choice of distribution is not informed by the data distribution. Doing so may further improve FourierHashNet.

**(2)** In our approach, the dominance similarity function is represented as an expectation of inner products of functions in the frequency domain, as described in Eq. (10). The accuracy of this representation relies on the quality of Monte Carlo approximations, which is influenced by the number of \(\bm{\omega}\) samples used. Our experimental findings, presented in Figure13, suggest that it may be necessary to generate up to 100 samples per dimension to reduce the approximation error to acceptable levels. A better choice of \(p(\bm{\omega})\) may reduce the number of samples needed.

**(3)** During our experimental investigations, we discovered that the computation of our proposed dominance similarity score is approximately 7.5X slower, compared to the dot product similarity on our datasets. This aligns with earlier observations where matrix subtraction operations have been known to be significantly slower than dot product. This disparity in computation speed represents another potential limitation of our approach, which could be addressed by exploring alternative designs for the dominance similarity function.

## Appendix B Example applications of soft set containment

Natural Language Inference (NLI)In (NLI) [4], \(q\) and \(x\) are sentences, regarded as sequences of words as items. A transformer network [14; 41] converts each sentence to an embedding vector. We infer \(x\Longrightarrow q\) if \(\bm{q}\leq\bm{x}\)[26]. Consider now a claim verification application that, given a claim as a query and a Web-scale corpus, needs to quickly retrieve passages that best support the given claim. This application exactly motivates FourierHashNet.

Market basketGiven a basket of supermarket items, we may query purchase logs for frequent supersets to make recommendations. The corpus contains itemsets purchased in the past, each is a 'document' \(x\). The query \(q\) is the current basket. Hard set containment tests for \(q\subset x\). However, we would like to'soften' items in \(q\) (say, from one toothpaste brand to another. Each item has a short textual description, which is passed into BERT [41] and the [CLS] embedding read out as the item representation. An itemset is then embedded using a (suitably fine-tuned) set encoder (such as Deep Set), giving us \(\bm{q}\) and \(\bm{x}\). To the extent \(\bm{q}\leq\bm{x}\), we regard the query basket as "soft-contained" in the document basket.

Knowledge graph (KG) completionVendrov et al. [52] embedded types and entities in a KG to vectors such that if two types are related via \(t_{1}\) is-subtype-of \(t_{2}\) then \(\vec{t}_{1}\leq\vec{t}_{2}\) was encouraged, and if entity \(e\) is-instance-of \(t\), then \(\vec{e}\leq\vec{t}\) was encouraged in suitable loss functions. Later, these _order_ embeddings were generalized to _box_ embeddings where is-subtype-of and is-instance-of were modeled as boxes in high dimensions contained in other boxes [10]. These models naturally motivate fast retrieval using dominance similarity.

Subgraph isomorphism searchHere we expect a corpus graph \(x\) will be relevant if query graph \(q\) is a _subgraph_ of \(x\), i.e., that \(x\) has a subgraph that is _isomorphic_ to \(q\). In reality, we want to score highly corpus graphs that have subgraphs _almost_ isomorphic to the query graph. A graph neural network (GNN) [25; 18] can build suitable contextual embeddings \(\bm{q}\) and \(\bm{x}\) for the entire graphs, which can be used to test for approximate subgraph isomorphism, _i.e._, \(\bm{q}\leq\bm{x}\). There are several applications of subgraph isomorphism search. In material and drug design, there are large molecule databases. A researcher wishes to predict properties of a new query molecule by retrieving similar molecules in the database. Each molecule is modeled as a modest-sized graph with nodes (atom, DNA bases) and edges (valence, etc.). In image search [22], the query \(q\) may be a graph fragment, e.g., \(\langle\text{person, feeding, pet}\rangle\), and the goal is to find corpus graphs \(x\) where \(q\) is approximately a subgraph [31], _e.g._, \(x\) can contain \(\langle\text{man, feeding, dog}\rangle\).

Further discussion on related work

In this section, we discuss existing work related to each of the three major components of our work--set embeddings, use of frequency domain for computing representations, and locality sensitive hashing.

Neural set embeddingsMotivated by various machine learning questions that are better formalized by using a set of items as primitive, there has also been a recent line work on set embeddings. Zaheer et al. [60] consider models that act on sets and characterize the structure of such permutation-invariant models, but do not consider asymmetric query based measures e.g., containment. Skianis et al. [48] casts the similarity measurement between sets as a combinatorial flow problem, which in turn is approximated by a linear program. Lee et al. [27] propose the Set Transformer, a model that uses self-attention to model interactions among the input set elements. Our model is different from the existing line of work in focusing on asymmetric metrics that can measure containment, as well as in using the frequency domain representation of the metric to build a scalable LSH.

Application of frequency domain transformation in machine learningOne of the most celebrated uses of the frequency domain representation was by [40], who proposed using random Fourier features for shift-invariant kernels. Since dot-product kernels are not shift-invariant, Bochner's theorem, a key tool in creating random Fourier features, does not apply. Hence, alternative random feature-based approximations have been proposed, primarily focusing on polynomial kernels [23, 38, 3, 49, 30]. All of the above work is on symmetric kernels. For our asymmetric dominance similarity function, we design a sampling distribution by taking into account the individual frequency-level coefficients of the Fourier representation. Raginsky and Lazebnik [39] provided a random Fourier based approach for LSH. However they only focused on similarity of the Mercer Kernel class, which is symmetric in nature.

Locality sensitive hashingThe third main pillar of our work is locality sensitive hashing (LSH) which enables efficient retrieval. Answering queries using sketches or hashes in order to measure the similarity or containment of documents has a long history, pioneered by Broder [6]. In more recent years, semantic search or vector search, sometimes called "dense passage retrieval" [24] employing scalable near-neighbor search engines, has emerged as a credible, often more powerful, alternative [34, 29] to standard information retrieval schemes, as the vector embeddings can capture more nuanced contextualization and semantics. While there are a number of variants of LSH, including multi-probe [32], our presentation and experimentation focus on a single-probe setting in which the hashing hyperplanes are learned from the data. In particular, we build upon the asymmetric hash constructions in Shrivastava and Li [46] and Neyshabur and Srebro [35].

Proofs of the technical results

### Proof of Proposition 3.1

Proof.: Consider two functions \(\textsc{Box}_{a,b}\) (for arbitrary positive constants \(a,b\)) and \(\textsc{ReLU}_{T}\) defined as follows:

\[\textsc{Box}_{a,b}(t) =\begin{cases}a\text{ if }-b\leq t\leq b\\ 0\text{ else }\end{cases}\] (16) \[\textsc{ReLU}_{T}(t) =\begin{cases}t\text{ if }0\leq t\leq T\\ 0\text{ else }\end{cases}\] (17)

Observe that \(s\) can be written in terms of these new functions as follows: \(s(t)=\textsc{Box}_{T,T}(t)-\textsc{ReLU}_{T}(t)\). By linearity of Fourier transform,

\[S(\iota\omega)=\mathcal{F}_{\textsc{Box}_{T,T}}(\iota\omega)-\mathcal{F}_{ \textsc{ReLU}_{T}}(\iota\omega)\] (18)

where, \(\mathcal{F}_{f}(\iota\omega)\) denotes Fourier transform of \(f(t)\) for any function \(f\). Now let us compute \(\mathcal{F}_{\textsc{ReLU}_{T}}(\iota\omega)\).

\[\mathcal{F}_{\textsc{ReLU}_{T}}(\iota\omega) =\frac{1}{2\pi}\int_{0}^{T}te^{-\iota\omega t}dt\] (19) \[=-\frac{1}{2\pi\omega^{2}}+\frac{e^{-\iota\omega T}}{2\pi\omega^ {2}}+\frac{\iota Te^{-\iota\omega T}}{2\pi\omega}\] (20) \[=-\frac{1}{2\pi\omega^{2}}+\frac{\cos(\omega T)-\iota\sin(\omega T )}{2\pi\omega^{2}}+\frac{T(\iota\cos(\omega T)+\sin(\omega T))}{2\pi\omega}\] (21) \[=\frac{-2\sin^{2}(\omega T/2)}{2\pi\omega^{2}}+\frac{T\sin(\omega T )}{2\pi\omega}-\iota\frac{\sin(\omega T)}{2\pi\omega^{2}}+\iota\frac{T\cos( \omega T)}{2\pi\omega}\] (22)

Since \(\textsc{Box}_{T,T}(t)\) is a rectangular pulse, its Fourier transform is

\[\mathcal{F}_{\textsc{Box}_{T,T}}(\iota\omega)=2T\frac{\sin(\omega T)}{2\pi\omega}\] (23)

Substituting the above Eqs. (22) and (23) into Eq. (18), we get \(S(\iota\omega)\) as follows.

\[S(\iota\omega) =\underbrace{2T\frac{\sin(\omega T)}{2\pi\omega}}_{\textsc{Box}_{ T,T}(\iota\omega)}-\underbrace{\left[\frac{-2\sin^{2}(\omega T/2)}{2\pi \omega^{2}}+\frac{T\sin(\omega T)}{2\pi\omega}-\iota\frac{\sin(\omega T)}{2 \pi\omega^{2}}+\iota\frac{T\cos(\omega T)}{2\pi\omega}\right]}_{G(\iota \omega)}\] (24) \[=\overbrace{T\frac{\sin(\omega T)}{2\pi\omega}+2\frac{\sin^{2} (\frac{\omega T}{2})}{2\pi\omega^{2}}}^{\text{Re}(S(\iota\omega))}+\iota \underbrace{\left[\frac{\sin(\omega T)}{2\pi\omega^{2}}-\frac{T\cos(\omega T )}{2\pi\omega}\right]}_{\text{Im}(S(\iota\omega))}\] (25)

### Proof of Theorem 3.2

**Theorem 3.2**.: Let \(\bm{q},\bm{x}\in\mathbb{R}^{K},\cos^{-1}\) be Lipschitz with Lipschitz constant \(L_{\text{cos}}\); the hyperparameter \(T\) in Eq. (4) be chosen such that \(T>||\bm{q}-\bm{x}||_{\infty}\); the frequency sampling distribution \(p(\omega_{k}^{j})\propto[|\mathrm{Re}(S(\omega_{k}^{j}))|+|\mathrm{Im}(S( \omega_{k}^{j}))|]\) with the support set \(\omega_{k}^{j}\in[-\omega_{\max},\omega_{\max}]\). Furthermore, we define \(\Delta_{\text{sim}}:=\frac{T}{T-\max_{\bm{x},\bm{q}}||\bm{x}-\bm{q}||_{\infty}}\) and \(I(\omega_{\max})=\int_{-\omega_{\max}}^{\omega_{\max}}[|\mathrm{Re}(S(\omega) |+|\mathrm{Im}(S(\omega))|]d\omega\). Then, the mapping \(g(q)[i]=\mathrm{sign}(\mathbf{w}_{i}^{\top}\bm{F}_{q}(\bm{\omega}^{1\dots M}))\) and \(h(x)[i]=\mathrm{sign}(\mathbf{w}_{i}^{\top}\bm{F}_{x}(\bm{\omega}^{1\dots M}))\) where \(\mathbf{w}_{i}\sim N(0,\mathbb{I})\), constitutes a \((S_{0},cS_{0},p_{1},p_{2})\)-ALSH for some \(p_{1}\) and \(p_{2}\) if

\[\omega_{\max}>\frac{4\Delta_{\text{sim}}}{\pi(1-c)S_{0}};\qquad M>\left[\frac{ 4L_{\text{cos}}}{(1-c)S_{0}}\right]^{2}KI(\omega_{\max}).\] (26)

ProofDefine \(\Omega=[-\omega_{\max},\omega_{\max}]^{K}\). For \(\bm{\omega}\in\Omega\), we have the following relationships:

\[||\bm{F}_{q}(\bm{\omega}^{1\dots M})||_{2}^{2}=||\bm{F}_{x}(\bm{\omega}^{1\dots M })||_{2}^{2}=\sum_{i\in[M],k\in[K]}\frac{|Re(S(\omega_{k}^{j}))|+|Im(S(\omega _{k}^{j}))|}{p(\omega_{k}^{j})}=MKI;\] (27)We also define that \(\widehat{\mathrm{sim}}_{\omega_{\max}}(q,x)=\int_{\bm{\omega}\in\Omega}\bm{F}_{q}( \bm{\omega})^{\top}\bm{F}_{x}(\bm{\omega})p(\bm{\omega})d\bm{\omega}\). This gives us the following result:

\[\Pr_{g,h}[g(q)=h(x)|\bm{\omega}^{i}]=1-\frac{1}{\pi}\cos^{-1}\left( \frac{\bm{F}_{q}(\bm{\omega}^{1\dots M})^{\top}\bm{F}_{x}(\bm{\omega}^{1\dots M })}{||\bm{F}_{q}(\bm{\omega}^{1\dots M})|||\bm{F}_{x}(\bm{\omega}^{1\dots M}) ||}\right)\] (28) \[=1-\frac{1}{\pi}\cos^{-1}\left(\int_{\bm{\omega}\in\Omega}\frac{ \bm{F}_{q}(\bm{\omega})^{\top}\bm{F}_{x}(\bm{\omega})p(\bm{\omega})d\bm{ \omega}}{||\bm{F}_{q}(\bm{\omega})|||\bm{F}_{x}(\bm{\omega})||}\right)\] \[\quad-\frac{1}{\pi}\cos^{-1}\left(\frac{\bm{F}_{q}(\bm{\omega}^{1 \dots M})^{\top}\bm{F}_{x}(\bm{\omega}^{1\dots M})}{MKI(\omega_{\max})} \right)+\frac{1}{\pi}\cos^{-1}\left(\int_{\bm{\omega}\in\Omega}\frac{\bm{F}_{ q}(\bm{\omega})^{\top}\bm{F}_{x}(\bm{\omega})p(\bm{\omega})d\bm{\omega}}{ \underbrace{|\bm{F}_{q}(\bm{\omega})|\|\bm{F}_{x}(\bm{\omega})||}_{KI(\omega_ {\max})}}\right)\] (29) \[=1-\frac{1}{\pi}\cos^{-1}\left(\frac{\mathrm{sim}(q,x)}{KI(\omega _{\max})}\right)+\underbrace{\frac{1}{\pi}\cos^{-1}\left(\frac{\mathrm{sim}(q, x)}{KI(\omega_{\max})}\right)-\frac{1}{\pi}\cos^{-1}\left(\frac{\mathrm{sim}_{ \omega_{\max}}(q,x)}{KI(\omega_{\max})}\right)}_{\text{Term-1}}\] \[\underbrace{-\frac{1}{\pi}\cos^{-1}\left(\frac{\bm{F}_{q}(\bm{ \omega}^{1\dots M})^{\top}\bm{F}_{x}(\bm{\omega}^{1\dots M})}{MKI(\omega_{ \max})}\right)+\frac{1}{\pi}\cos^{-1}\left(\int_{\bm{\omega}\in\Omega}\frac{ \bm{F}_{q}(\bm{\omega})^{\top}\bm{F}_{x}(\bm{\iota}\bm{\omega})p(\bm{\omega})d \bm{\omega}}{KI(\omega_{\max})}\right)}_{\text{Term-2}}\] (30)

Next we bound Term-1 and Term-2. First we bound Term-1 as follows:

\[\left|\frac{1}{\pi}\left[\cos^{-1}\left(\frac{\mathrm{sim}(q,x)}{ KI(\omega_{\max})}\right)-\cos^{-1}\left(\frac{\widehat{\mathrm{sim}}_{ \omega_{\max}}(q,x)}{KI(\omega_{\max})}\right)\right]\right| \leq\frac{L_{\mathrm{cos}}}{\pi KI}\left|\mathrm{sim}(q,x)- \widehat{\mathrm{sim}}_{\omega_{\max}}(q,x)\right|\] (31) \[\leq\underbrace{\frac{KL_{\mathrm{cos}}\left(6+\frac{2T-\max_{\bm {x},\bm{q}}||\bm{x}-\bm{q}||_{\infty}}{\Delta_{\mathrm{sim}}}\right)}{\pi^{2}KI (\omega_{\max})\omega_{\max}}}_{\text{Term}}\] (32)

The last inequality is due to Lemma D.1. Next we bound Term-2 in Eq. (30).

\[\frac{1}{\pi}\mathbb{E}\left[\left|-\cos^{-1}\left(\frac{\bm{F}_ {q}(\bm{\omega}^{1\dots M})^{\top}\bm{F}_{x}(\bm{\omega}^{1\dots M})}{MKI( \omega_{\max})}\right)+\cos^{-1}\left(\int_{\bm{\omega}\in\Omega}\frac{\bm{F}_ {q}(\bm{\iota}\bm{\omega})^{\top}\bm{F}_{x}(\bm{\iota}\bm{\omega})p(\bm{ \omega})d\bm{\omega}}{KI(\omega_{\max})}\right)\right]\right]\] (33) \[\leq\frac{L_{\mathrm{cos}}}{\pi KI}\bigg{|}\bm{F}_{q}(\bm{\omega }^{1\dots M})^{\top}\bm{F}_{x}(\bm{\omega}^{1\dots M})/M-\int_{\bm{\omega}\in \Omega}\bm{F}_{q}(\bm{\iota}\bm{\omega})^{\top}\bm{F}_{x}(\bm{\iota}\bm{\omega} )p(\bm{\omega})d\bm{\omega}\bigg{|}\] (34) \[=\frac{L_{\mathrm{cos}}}{\pi KI(\omega_{\max})}\bigg{|}\sum_{j\in[ M]}\bm{F}_{q}(\bm{\omega}^{j})^{\top}\bm{F}_{x}(\bm{\omega}^{j})/M-\int_{\bm{\omega}\in \Omega}\bm{F}_{q}(\bm{\iota}\bm{\omega})^{\top}\bm{F}_{x}(\bm{\iota}\bm{\omega} )p(\bm{\omega})d\bm{\omega}\bigg{|}\] (35)

Taking expectation with respect to \(\bm{\omega}\), we have:

\[\frac{1}{\pi}\mathbb{E}\left[\left|-\cos^{-1}\left(\frac{\bm{F}_ {q}(\bm{\omega}^{1\dots M})^{\top}\bm{F}_{x}(\bm{\omega}^{1\dots M})}{MKI( \omega_{\max})}\right)+\cos^{-1}\left(\int_{\omega}\frac{\bm{F}_{q}(\bm{ \iota}\bm{\omega})^{\top}\bm{F}_{x}(\bm{\iota}\bm{\omega})p(\bm{\omega})d\bm{ \omega}}{KI(\omega_{\max})}\right)\right|\right]\] (36) \[\leq\frac{L_{\mathrm{cos}}}{\pi KI(\omega_{\max})}\mathbb{E}\bigg{|} \sum_{j\in[M]}\bm{F}_{q}(\bm{\omega}^{j})^{\top}\bm{F}_{x}(\bm{\omega}^{j})/M- \int_{\bm{\omega}}\bm{F}_{q}(\bm{\iota}\bm{\omega})^{\top}\bm{F}_{x}(\bm{\iota} \bm{\omega})p(\bm{\omega})d\bm{\omega}\bigg{|}\] (37) \[\leq\frac{L_{\mathrm{cos}}}{\pi KI(\omega_{\max})}\left(\text{ Variance}\bigg{[}\sum_{j\in[M]}\bm{F}_{q}(\bm{\omega}^{j})^{\top}\bm{F}_{x}(\bm{\omega}^{j})/M \bigg{]}\right)^{1/2}\quad(\mathbb{E}[|Z|]\leq\sqrt{\mathbb{E}[|Z|^{2}]})\] (38) \[=\frac{L_{\mathrm{cos}}}{\pi KI(\omega_{\max})\sqrt{M}}\left(\text {Variance}\bigg{[}\bm{F}_{q}(\iota\bm{\omega})^{\top}\bm{F}_{x}(\iota\bm{\omega}) \bigg{]}\right)^{1/2}\] (39) \[=\frac{L_{\mathrm{cos}}}{\pi KI(\omega_{\max})\sqrt{M}}\sqrt{K} \left(\text{Variance}\bigg{[}\bm{F}_{q}(\omega_{k})^{\top}\bm{F}_{x}(\omega_{k}) \bigg{]}\right)^{1/2}\leq\frac{L_{\mathrm{cos}}}{\pi\sqrt{KM}}\] (40)The last inequality follows from bound on the variance due the following:

\[\bm{F}_{q}(\omega_{k})^{\top}\bm{F}_{x}(\omega_{k}) =\bm{S}_{q}(\omega_{k})^{\top}\bm{S}_{x}(\omega_{k})/p(\omega_{k})\] (41) \[=\frac{Re[\bm{S}(\omega)]\cos\omega_{k}(q[k]-x[k])-Im[\bm{S}( \omega)]\sin\omega_{k}(q[k]-x[k]))}{(1/I)[|Re[\bm{S}(\omega)]|+|Im[\bm{S}( \omega)]|]}\] (42) \[\leq I\frac{|Re[\bm{S}(\omega_{k})]|+|Im[\bm{S}(\omega_{k})]|}{|Re [\bm{S}(\omega)]|+|Im[\bm{S}(\omega)]|}=I\] (43)

Putting Eqs. (32) (40) into Eq. (30), we will have the upper bound for \(\sin(q,x)\leq cS_{0}\):

\[\mathbb{E}[\Pr_{g,h}[g(q)=h(x)|\bm{\omega}^{j}]]\leq p_{2}=1-\frac{1}{\pi}\cos ^{-1}\left(\frac{\sin(q,x)}{KI(\omega_{\max})}\right)+\frac{L_{\cos}}{\pi\sqrt {KM}}+\frac{\Delta_{\text{sim}}}{\pi^{2}KI(\omega_{\max})\omega_{\max}}\] (44)

Similarly, we have the lower bound for \(\sin(q,x)>S_{0}\):

\[\mathbb{E}[\Pr_{g,h}[g(q)=h(x)|\bm{\omega}^{j}]]\geq p_{1}=1-\frac{1}{\pi}\cos ^{-1}\left(\frac{\sin(q,x)}{KI(\omega_{\max})}\right)-\frac{L_{\cos}}{\pi \sqrt{KM}}-\frac{\Delta_{\text{sim}}}{\pi^{2}KI(\omega_{\max})\omega_{\max}}\] (45)

Now, note that:

\[\frac{1}{\pi}\left[\cos^{-1}\left(\frac{cS_{0}}{KI(\omega_{\max} )}\right)-\cos^{-1}\left(\frac{S_{0}}{KI(\omega_{\max})}\right)\right] \geq\frac{1}{\pi KI(\omega_{\max})}(c-1)S_{0}\left.\frac{d}{ds} \cos^{-1}(s)\right|_{s\in(cS_{0},S_{0})}\] \[=\frac{1}{\pi KI(\omega_{\max})}(c-1)S_{0}\times-\frac{1}{\sqrt{ 1-s^{2}}}\] \[=\frac{(1-c)S_{0}}{\pi KI(\omega_{\max})\sqrt{1-s^{2}}}\] \[\geq\frac{(1-c)S_{0}}{\pi KI(\omega_{\max})}\] \[\overset{(i)}{\geq}\frac{2\Delta_{\text{sim}}}{\pi^{2}KI(\omega_ {\max})\omega_{\max}}+\frac{2L_{\cos}}{\pi\sqrt{KI(\omega_{\max})M}}\] (46)

The last inequality (i) is due to the conditions that:

\[\omega_{\max}>\frac{4\Delta_{\text{sim}}}{\pi(1-c)S_{0}};\qquad M>\left[\frac {4L_{\cos}}{(1-c)S_{0}}\right]^{2}KI(\omega_{\max}).\] (47)

This leads us to the following relationship:

\[1-\frac{1}{\pi}\cos^{-1}\left(\frac{cS_{0}}{KI(\omega_{\max})} \right)+\frac{L_{\cos}}{\pi\sqrt{KM}}+\frac{\Delta_{\text{sim}}}{\pi^{2}KI( \omega_{\max})\omega_{\max}}\] \[\leq 1-\frac{1}{\pi}\cos^{-1}\left(\frac{S_{0}}{KI(\omega_{\max})} \right)-\frac{L_{\cos}}{\pi\sqrt{KM}}-\frac{\Delta_{\text{sim}}}{\pi^{2}KI( \omega_{\max})\omega_{\max}}\] (48)

Thus, we have:

\[p_{2}=1-\frac{1}{\pi} \cos^{-1}\left(\frac{\sin(q,x)}{KI(\omega_{\max})}\bigg{|}_{\sin( q,x)<cS_{0}}\right)+\frac{L_{\cos}}{\pi\sqrt{KM}}+\frac{\Delta_{\text{sim}}}{\pi^{2}KI( \omega_{\max})\omega_{\max}}\] \[\leq 1-\frac{1}{\pi}\cos^{-1}\left(\frac{cS_{0}}{KI(\omega_{\max}) }\right)+\frac{L_{\cos}}{\pi\sqrt{KM}}+\frac{\Delta_{\text{sim}}}{\pi^{2}KI( \omega_{\max})\omega_{\max}}\] \[\leq 1-\frac{1}{\pi}\cos^{-1}\left(\frac{S_{0}}{KI(\omega_{\max}) }\right)-\frac{L_{\cos}}{\pi\sqrt{KM}}-\frac{\Delta_{\text{sim}}}{\pi^{2}KI( \omega_{\max})\omega_{\max}}\] \[\leq 1-\frac{1}{\pi}\cos^{-1}\left(\frac{\sin(q,x)}{KI(\omega_{\max} )}\bigg{|}_{\sin(q,x)>S_{0}}\right)-\frac{L_{\cos}}{\pi\sqrt{KM}}-\frac{ \Delta_{\text{sim}}}{\pi^{2}KI(\omega_{\max})\omega_{\max}}\] \[=p_{1}\] (49)

### Effect of truncating frequencies

**Lemma D.1**.: Let \(\widehat{\operatorname{sim}}_{\omega_{\max}}(q,x)=\mathbb{E}_{\bm{\omega}\sim p( \bm{\omega})}[\bm{F}_{q}(\bm{\omega})^{\top}\bm{F}_{x}(\bm{\omega})]\) with \(p(\bm{\omega})\) being defined as follows:

\[p(\bm{\omega})\propto\left\{\begin{array}{ll}\prod_{k\in[K]}| \mathrm{Re}(S(\iota\omega_{k}))|+|\mathrm{Im}(S(\iota\omega_{k})|&\text{if, }\omega\in[-\omega_{\max},\omega_{\max}]\\ 0&\text{otherwise}\end{array}\right.\] (50)

Then, if \(T>\max_{\bm{x},\bm{q}}||\bm{x}-\bm{q}||_{\infty}\), then we have:

\[|\widehat{\operatorname{sim}}_{\omega_{\max}}(q,x)-\operatorname{sim}(q,x)| \leq\frac{6K+2K\max\left\{1,\frac{T}{T-\max_{\bm{x},\bm{q}}||\bm{x}-\bm{q}||_{ \infty}}\right\}}{\pi\omega_{\max}}\] (51)

Proof.: Let \(S(\iota\omega)\) be the Fourier transform of \(s(t)\), \(\mathcal{F}^{-1}\) is the inverse Fourier transform, \(\star\) is the convolution operator. Further, we define the \(\textsc{LowPass}(\omega\,|\,\omega_{\max})\) is defined as

\[\textsc{LowPass}(\omega\,|\,\omega_{\max})=1\text{ if }|\omega|\leq\omega_{\max} \text{ and }0\text{ otherwise.}\] (52)

Let us define the filtered or frequency truncated signal of \(s(t)\) as defined in Eq. (4) as:

\[\widehat{s}_{\omega_{\max}}(t) =\mathcal{F}^{-1}\Big{[}S(\iota\omega)\textsc{LowPass}(\omega\,| \,\omega_{\max})\Big{]}\] \[=\mathcal{F}^{-1}[S(\iota\omega)]\star\mathcal{F}^{-1}[\textsc{ LowPass}(\omega\,|\,\omega_{\max})].\] (53)

In this context, \(\widehat{\operatorname{sim}}_{\omega_{\max}}(q,x)\) can be expressed as follows:

\[\widehat{\operatorname{sim}}_{\omega_{\max}}(q,x)\] \[=\int_{\bm{\omega}\in\Omega}\bm{F}_{q}(\iota\bm{\omega})^{\top} \bm{F}_{x}(\iota\bm{\omega})p(\bm{\omega})d\bm{\omega}\] (54) \[=\int_{\bm{\omega}\in\Omega}\bm{S}_{q}(\iota\bm{\omega})^{\top} \bm{S}_{c}(\iota\bm{\omega})d\bm{\omega}\] (55) \[=\sum_{k\in[K]}\int_{-\omega_{\max}}^{\omega_{\max}}[\mathrm{Re} (S(\iota\omega_{k}))\cos(\omega_{k}(\bm{q}[k]-\bm{x}[k]))-\mathrm{Im}(S(\iota \omega_{k}))\sin(\omega_{k}(\bm{q}[k]-\bm{x}[k]))]d\omega_{k}\] (56) \[\overset{(i)}{=}\sum_{k\in[K]}\int_{-\omega_{\max}}^{\omega_{\max} }[\mathrm{Re}(S(\iota\omega_{k}))\cos(\omega_{k}(\bm{q}[k]-\bm{x}[k]))-\mathrm{ Im}(S(\iota\omega_{k}))\sin(\omega_{k}(\bm{q}[k]-\bm{x}[k]))]d\omega_{k}\] \[\quad+\sum_{k\in[K]}\iota\int_{-\omega_{\max}}^{\omega_{\max}} \underbrace{[\mathrm{Im}(S(\iota\omega_{k}))\cos(\omega_{k}(\bm{q}[k]-\bm{x}[ k]))+\mathrm{Re}(S(\iota\omega_{k}))\sin(\omega_{k}(\bm{q}[k]-\bm{x}[k]))]}_{\text{ Odd function in }\omega_{k}}d\omega_{k}\] \[=\sum_{k\in[K]}\int_{-\omega_{\max}}^{\omega_{\max}}[\mathrm{Re} (S(\iota\omega_{k}))+\iota\mathrm{Im}(S(\iota\omega_{k}))\exp(\iota\omega_{k}( \bm{q}[k]-\bm{x}[k]))d\omega_{k}\] (58) \[=\sum_{k\in[K]}\int_{-\omega_{\max}}^{\omega_{\max}}S(\iota\omega_ {k})\exp(\iota\omega_{k}(\bm{q}[k]-\bm{x}[k]))d\omega_{k}\] (59) \[=\sum_{k\in[K]}\int_{-\infty}^{\infty}S(\iota\omega_{k})\textsc{ LowPass}(\omega_{k}\,|\,\omega_{\max})\exp(\omega_{k}(\bm{q}[k]-\bm{x}[k]))d\omega_{k}\] (60) \[=\sum_{k\in[K]}\mathcal{F}^{-1}[S(\iota\omega_{k})\textsc{ LowPass}(\omega_{k}\,|\,\omega_{\max})]\] (61) \[\overset{(ii)}{=}\sum_{k\in[K]}\widehat{s}_{\omega_{\max}}(\bm{q} [k]-\bm{x}[k])\] (62)

Equality (i) is due to the fact that the newly added imaginary integrand is an odd function (since, \(\mathrm{Im}(S(\iota\omega))=-\mathrm{Im}(S(-\iota\omega))\) and \(\mathrm{Re}(S(\iota\omega))=\mathrm{Re}(S(-\iota\omega))\)) and thus, the integral from \(-\omega_{\max}\) and \(\omega_{\max}\) is zero. Equality (ii) is due to Eq. (53). Next we focus on \(\widehat{s}_{\omega_{\max}}(t)\). Since\(s(t)\) and \(\mathcal{F}^{-1}[\text{LowPass}(\omega\left|\omega_{\max}\right>)=\frac{\sin\omega_{ \max}t}{\pi t}\), we have:

\[\widehat{s}_{\omega_{\max}}(t) =\int_{-\infty}^{\infty}s(t-\tau)\frac{\sin\omega_{\max}\tau}{\pi \tau}d\tau\] (63) \[=\int_{-\infty}^{\infty}s\left(t-\frac{\tau}{\omega_{\max}} \right)\frac{\sin\tau}{\pi\tau}d\tau\] (64) \[=\int_{-\infty}^{\infty}s(t)\frac{\sin\tau}{\pi\tau}d\tau+ \underbrace{\left[s\left(t-\frac{\tau}{\omega_{\max}}\right)-s(t)\right]}_{ \Delta s\left(t-\frac{\tau}{\omega_{\max}}\right)}\frac{\sin\tau}{\pi\tau}d\tau\] (65) \[=s(t)\int_{-\infty}^{\infty}\frac{\sin\tau}{\pi\tau}d\tau+\int_{ -\infty}^{\infty}\underbrace{\left[s\left(t-\frac{\tau}{\omega_{\max}}\right) -s(t)\right]}_{\Delta s\left(t-\frac{\tau}{\omega_{\max}}\right)}\frac{\sin \tau}{\pi\tau}d\tau\] (66) \[=s(t)+\int_{-\infty}^{\infty}\left[s\left(t-\frac{\tau}{\omega_{ \max}}\right)-s(t)\right]\frac{\sin\tau}{\pi\tau}d\tau\] (67)

We only consider the case when \(t\in(-T,T)\). We decompose the error term \(\widehat{s}_{\omega_{\max}}(t)-s(t)\) as:

\[\widehat{s}_{\omega_{\max}}(t)-s(t) =\int_{-\infty}^{\omega_{\max}(t-T)}\Delta s\left(t-\frac{\tau}{ \omega_{\max}}\right)\frac{\sin\tau}{\pi\tau}d\tau+\int_{\omega_{\max}(t-T)}^ {\omega_{\max}t}\Delta s\left(t-\frac{\tau}{\omega_{\max}}\right)\frac{\sin \tau}{\pi\tau}d\tau\] (68) \[\quad+\int_{\omega_{\max}t}^{\omega_{\max}(t+T)}\Delta s\left(t- \frac{\tau}{\omega_{\max}}\right)\frac{\sin\tau}{\pi\tau}d\tau+\int_{\omega_{ \max}(t+T)}^{\infty}\Delta s\left(t-\frac{\tau}{\omega_{\max}}\right)\frac{ \sin\tau}{\pi\tau}d\tau\] (69)

Bounding the error \(|\widehat{s}_{\omega_{\max}}(t)-s(t)|\) when \(-T<t\leq 0\)We separately bound the four integrals as follows:

_Bound on the first integral \(\int_{-\infty}^{\omega_{\max}(t-T)}\Delta s\left(t-\frac{\tau}{\omega_{\max}} \right)\frac{\sin\tau}{\pi\tau}d\tau\):_ Thus, \(s\left(t-\frac{\tau}{\omega_{\max}}\right)=0\) and \(\Delta s\left(t-\frac{\tau}{\omega_{\max}}\right)=s\left(t-\frac{\tau}{\omega _{\max}}\right)-s(t)=0-T\). We compute the bound on the first term as:

\[\int_{-\infty}^{\omega_{\max}(t-T)}\Delta s\left(t-\frac{\tau}{ \omega_{\max}}\right)\frac{\sin\tau}{\pi\tau}d\tau =-T\int_{-\infty}^{\omega_{\max}(t-T)}\frac{\sin\tau}{\pi\tau}d\tau\] (70) \[=-T\int_{\omega_{\max}(T-t)}^{\infty}\frac{\sin\tau}{\pi\tau}d\tau\] (71)

Thus, from Proposition D.2, we have:

\[\left|\int_{-\infty}^{\omega_{\max}(t-T)}\Delta s\left(t-\frac{\tau}{\omega_{ \max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\right|\leq\frac{2T}{\pi\omega_{ \max}(T-t)}\leq\frac{2}{\pi\omega_{\max}}\quad\text{(since $t<0$)}\] (72)

_Bound on the second integral \(\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}\Delta s\left(t-\frac{\tau}{\omega_{ \max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\):_ Since \(\tau\in(\omega_{\max}(t-T),\omega_{\max}t]\), we have \(0\leq t-\tau/\omega_{\max}<T\). Thus, \(s\left(t-\frac{\tau}{\omega_{\max}}\right)=T-t+\frac{\tau}{\omega_{\max}}\).

We compute the bound on the second term as:

\[\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}\Delta s\left(t-\frac{ \tau}{\omega_{\max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\] (73) \[=\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}\left(T-t+\frac{\tau}{ \omega_{\max}}-T\right)\frac{\sin\tau}{\pi\tau}d\tau\] (74) \[=-t\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}\frac{\sin\tau}{\pi \tau}d\tau+\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}\frac{\tau}{\omega_{\max} }\frac{\sin\tau}{\pi\tau}d\tau\] (75) \[=-t\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}\frac{\sin\tau}{\pi \tau}d\tau+\frac{\cos\omega_{\max}(t-T)-\cos\omega_{\max}t}{\pi\omega_{\max}}\] (76)Thus, from Proposition D.2, we have:

\[\left|\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}\Delta s\left(t-\frac{\tau}{ \omega_{\max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\right|\leq\frac{2t}{\pi \omega_{\max}t}+\frac{2}{\pi\omega_{\max}}=\frac{4}{\pi\omega_{\max}}\] (77)

_Bound on the third integral \(\int_{\omega_{\max}t}^{\omega_{\max}(t+T)}\Delta s\left(t-\frac{\tau}{\omega_{ \max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\):_ When \(\tau\in(\omega_{\max}t,\omega_{\max}(t+T)]\), we have \(-T\leq t-\tau/\omega_{\max}<0\). Moreover, \(-T\leq t<0\). Thus, \(s(t)=s(t-\tau/\omega_{\max})=T\), resulting in \(\Delta s\left(t-\frac{\tau}{\omega_{\max}}\right)=0\) and the third integral is zero.

\[\int_{\omega_{\max}t}^{\omega_{\max}(t+T)}\Delta s\left(t-\frac{\tau}{\omega_{ \max}}\right)\frac{\sin\tau}{\pi\tau}d\tau=0\] (78)

_Bound on the fourth integral \(\int_{\omega_{\max}(t+T)}^{\infty}\Delta s\left(t-\frac{\tau}{\omega_{\max}} \right)\frac{\sin\tau}{\pi\tau}d\tau\):_ When \(\tau\in(\omega_{\max}(t+T),\infty)\), we have \(-\infty<t-\tau/\omega_{\max}<-T\). Then, \(s\left(t-\frac{\tau}{\omega_{\max}}\right)=0\).

We compute the bound on the first term as:

\[\int_{\omega_{\max}(T+t)}^{\infty}\Delta s\left(t-\frac{\tau}{ \omega_{\max}}\right)\frac{\sin\tau}{\pi\tau}d\tau=-T\int_{\omega_{\max}(T+t)} ^{\infty}\frac{\sin\tau}{\pi\tau}d\tau\] (79)

Thus, from Proposition D.2, we have:

\[\left|\int_{\omega_{\max}(T+t)}^{\infty}\Delta s\left(t-\frac{\tau}{\omega_{ \max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\right|\leq\frac{2T}{\pi\omega_{ \max}(T+t)}\] (81)

**Bounding the error \(|\widehat{s}_{\omega_{\max}}(t)-s(t)|\) when \(0\leq t<T\)**

\[\widehat{s}_{\omega_{\max}}(t)-s(t)=\int_{-\infty}^{\omega_{\max} (t-T)}\Delta s\left(t-\frac{\tau}{\omega_{\max}}\right)\frac{\sin\tau}{\pi\tau }d\tau+\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}\Delta s\left(t-\frac{\tau}{ \omega_{\max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\] (82)

\[+\int_{\omega_{\max}t}^{\infty}\Delta s\left(t-\frac{\tau}{\omega_{\max}} \right)\frac{\sin\tau}{\pi\tau}d\tau\] (83)

Now, since \(0\leq t<T\), we have \(s(t)=T-t\), throughout the next part of the proof.

_Bound on the first integral \(\int_{-\infty}^{\omega_{\max}(t-T)}\Delta s\left(t-\frac{\tau}{\omega_{\max}} \right)\frac{\sin\tau}{\pi\tau}d\tau\):_ Since \(\tau\in(-\infty,\omega_{\max}(t-T)]\), we have \(T\leq t-\tau/\omega_{\max}<\infty\). Thus, \(s\left(t-\frac{\tau}{\omega_{\max}}\right)=0\) and \(\Delta s\left(t-\frac{\tau}{\omega_{\max}}\right)=s\left(t-\frac{\tau}{\omega _{\max}}\right)-s(t)=0-(T-t)\).

\[\left|\int_{-\infty}^{\omega_{\max}(t-T)}\Delta s\left(t-\frac{ \tau}{\omega_{\max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\right|\] \[=\left|-(T-t)\int_{-\infty}^{\omega_{\max}(t-T)}\frac{\sin\tau}{ \pi\tau}d\tau\right|\stackrel{{(\mathrm{i})}}{{\leq}}(T-t)\frac{2} {\omega_{\max}(T-t)\pi}=\frac{2}{\omega_{\max}\pi}\] (84)

Inequality (i) is due to Proposition D.2

_Bound on the second integral \(\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}\Delta s\left(t-\frac{\tau}{\omega_{ \max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\):_ Since \(\tau\in(\omega_{\max}(t-T),\omega_{\max}t]\), we have \(0\leq t-\tau/\omega_{\max}<T\). Thus, \(s\left(t-\frac{\tau}{\omega_{\max}}\right)=T-t+\frac{\tau}{\omega_{\max}}\).

\[\left|\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}\Delta s\left(t- \frac{\tau}{\omega_{\max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\right| =\left|\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}[T-(t-\tau/\omega _{\max})-(T-t)]\frac{\sin\tau}{\pi\tau}\right|\] \[=\left|\int_{\omega_{\max}(t-T)}^{\omega_{\max}t}\frac{\sin\tau}{ \pi\omega_{\max}}d\tau\right|\leq\frac{2}{\pi\omega_{\max}}\] (85)_Bound on the third integral \(\int_{\omega_{\max}t}^{\omega_{\max}(t+T)}\Delta s\left(t-\frac{\tau}{\omega_{\max} }\right)\frac{\sin\tau}{\pi\tau}d\tau\):_ When \(\tau\in(\omega_{\max}t,\omega_{\max}(t+T)]\), we have \(-T\leq t-\tau/\omega_{\max}<0\). Thus, \(s(t-\tau/\omega_{\max})=T\).

\[\left|\int_{\omega_{\max}t}^{\omega_{\max}(t+T)}\Delta s\left(t- \frac{\tau}{\omega_{\max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\right| =\left|\int_{\omega_{\max}t}^{\omega_{\max}(t+T)}[T-(T-t)]\frac{ \sin\tau}{\pi\tau}d\tau\right|\] (86) \[=t\left|\int_{\omega_{\max}t}^{\omega_{\max}(t+T)}\frac{\sin\tau} {\pi\tau}d\tau\right|\] (87) \[=\frac{t}{\pi}\left|\int_{\omega_{\max}t}^{\omega_{\max}(t+T)} \frac{\sin\tau}{\tau}d\tau\right|\stackrel{{(i)}}{{\leq}}\frac{t }{\pi}\frac{2}{\omega_{\max}t}=\frac{2}{\pi\omega_{\max}}\] (88)

Inequality (i) is due to Proposition D.2

_Bound on the fourth integral \(\int_{\omega_{\max}(t+T)}^{\infty}\Delta s\left(t-\frac{\tau}{\omega_{\max}} \right)\frac{\sin\tau}{\pi\tau}d\tau\):_ When \(\tau\in(\omega_{\max}(t+T),\infty)\), we have \(-\infty<t-\tau/\omega_{\max}<-T\). Then, \(s\left(t-\frac{\tau}{\omega_{\max}}\right)=0\).

\[\left|\int_{\omega_{\max}(t+T)}^{\infty}\Delta s\left(t-\frac{\tau }{\omega_{\max}}\right)\frac{\sin\tau}{\pi\tau}d\tau\right| =\left|\int_{\omega_{\max}(t+T)}^{\infty}(T-t)\frac{\sin\tau}{ \tau}d\tau\right|\] \[\stackrel{{(i)}}{{\leq}}\frac{(T-t)\times 2}{\omega_{ \max}(t+T)\pi}\] (89) \[\leq\frac{2}{\pi\omega_{\max}}.\] (90)

Inequality (i) is due to Proposition D.2 Hence,

\[\left|\widehat{s}_{\omega_{\max}}(t)-s(t)\right|\leq\frac{6+2\max\left\{1, \frac{T}{T+t}\right\}}{\pi\omega_{\max}}\] (91)

The above result, together with \(\sin(q,x)=\sum_{k\in[K]}s(\bm{q}[k]-\bm{x}[k])\) and \(\widehat{\sin}_{\omega_{\max}}(q,x)=\sum_{k\in[K]}\widehat{s}_{\omega_{\max} }(\bm{q}[k]-\bm{x}[k])\) prove the Lemma. Q.E.D.

**Proposition D.2**.: If \(\alpha\) and \(\beta\) have the same sign and \(\alpha<\beta\), then we have:

\[\left|\int_{\alpha}^{\beta}\frac{\sin t}{\pi t}dt\right|\leq\left\{\begin{array} []{cl}\frac{2}{\pi\alpha}&\text{ if }\beta>\alpha>0\\ -\frac{2}{\pi\beta}&\text{ if }0>\beta>\alpha\end{array}\right.\] (92)

ProofNote that \(\frac{d}{dt}\frac{\cos t}{t}=-\frac{\sin t}{t}-\frac{\cos t}{t^{2}}\). Then we have:

\[\int_{\alpha}^{\beta}\frac{\sin t}{t}dt =-\int_{\alpha}^{\beta}\frac{d}{dt}\left[\frac{\cos t}{t}\right] dt-\int_{\alpha}^{\beta}\frac{\cos t}{t^{2}}dt\] (93) \[=\frac{\cos\alpha}{\alpha}-\frac{\cos\beta}{\beta}+\int_{\alpha} ^{\beta}\frac{\cos t}{t^{2}}dt.\] (94)

This gives us the following inequality:

\[\left|\int_{\alpha}^{\beta}\frac{\sin t}{t}dt\right| \leq\left|\frac{\cos\alpha}{\alpha}-\frac{\cos\beta}{\beta} \right|+\left|\int_{\alpha}^{\beta}\frac{\cos t}{t^{2}}dt\right|\] (95) \[\leq\frac{1}{|\alpha|}+\frac{1}{|\beta|}+\int_{\alpha}^{\beta} \frac{|\cos t|}{t^{2}}dt\] (96) \[\leq\frac{1}{|\alpha|}+\frac{1}{|\beta|}+\frac{1}{\alpha}-\frac {1}{\beta}\] \[=\left\{\begin{array}{cl}\frac{2}{\alpha}&\text{ if }\beta>\alpha>0\\ -\frac{2}{\beta}&\text{ if }0>\beta>\alpha\end{array}\right.\] (97)Additional details about the experimental setup

### Dataset Generation

We obtain the MsWeb1 and MsWeb2 datasets from the UCI Machine Learning repository. Both of the datasets contain anonymized logs of real world user web activity. Each data item in MsWeb is a set of text snippets denoting areas of the website _www.microsoft.com_ visited by an user within a specified time frame. Similarly, MsWeb consists of multi-sets denoting user page requests under various news categories at _www.msnbc.com_. Overall, we regard each data set as a collection of items, each item being a bag of words. For each data set, we sample a subset of items and designate them as queries, and the remaining items are designated as corpus items. The (binary) query-corpus relevance for MsWeb-1 and MsWeb-2 are governed by set containment, while for MsNbc-1 and MsNbc-2 we use multi-set (bag) containment. I.e., \(x\) is relevant for \(q\) iff \(q\subseteq x\). To test the ability of FourierHashNet to retrieve semantically similar items close to the gold items, we report not only on MAP based on gold labels but also scores of the top-10 candidates (Figure 3). The dataset characteristics are summarized in Table 7. We create datasets which differ greatly in terms of corpus size (10734 for MsWeb-1 and MsWeb, 110790 for MsNbc-1 and MsNbc-2), as well as span a range of average query selectivity between \(1.7\times 10^{-4}\) and \(3.3\times 10^{-3}\). We set aside 100 query graphs each for training and validation, and use the remaining 300 for testing.

Footnote 1: https://archive.ics.uci.edu/ml/machine-learning-databases/msweb-mld

Footnote 2: https://archive.ics.uci.edu/ml/machine-learning-databases/msnbc-mld/

### Learning Representations for \(q\) and \(x\) for the baselines

During experiments for Section 4.2, in each of the baseline method (Cosine similarity, Dot product and Weighted Jaccard), we use the respective similarity scoring functions, and minimize a pairwise ranking loss on the gold relevance labels to learn the deep set network. We observed that the pairwise ranking loss performs better than the BCE loss for the baselines. The margin enabled pairwise ranking loss is specified as

\[\text{Loss}=\sum_{q\in Q}\sum_{\begin{subarray}{c}x\in X_{q^{\prime}}\\ x^{\prime}\in X_{q^{\prime}}\end{subarray}}\big{[}\mathrm{margin}+\mathrm{ sim}(q,x^{\prime})-\mathrm{sim}(q,x)\big{]}_{+}.\] (98)

where \(\mathrm{sim}\) is the choice of similarity scoring baseline, \(X_{q^{\prime}},X_{q^{\prime}}\) are the set of relevant and irrelevant corpus items for the query \(q\). We use the best performing margin among \(\{1,0.1\}\).

### Sampling from arbitrary distribution

One key component of FourierHashNet is sampling \(\bm{\omega}^{1\dots M}\sim p(\bm{\omega})\). We have chosen to set \(p(\omega)\propto|\mathrm{Re}(S(\iota\omega))|+|\mathrm{Im}(S(\iota\omega))|\), with the support set between \(p(\omega)\) between \([-100,100]\). The samples are drawn using Inverse Transform Sampling.

### Details about fourier transformation network

In our experiments, we generate \(M=10\) samples for \(\bm{\omega}\). The neural networks \(\phi_{q}\) and \(\phi_{x}\) are linear layers which output 10 dimensional transformed Fourier representations \(\bm{z}_{q}=\phi_{q}(\bm{F}_{q}(\iota\bm{\omega}^{1\dots M}))\) and \(\bm{z}_{x}=\phi_{x}(\bm{F}_{x}(\iota\bm{\omega}^{1\dots M}))\). These are trained using the BCE loss specified in Eq. (12).

\begin{table}
\begin{tabular}{|l||c|c|c|c|c|c|} \hline Dataset & \(|Q|\) & \(|X|\) & \(\sum_{\begin{subarray}{c}x\in Q_{x}\in X\text{ rel}(q,x)\end{subarray}}|\) & \(\min_{\begin{subarray}{c}x\in Q_{x}\in X\text{ rel}(q,x)\end{subarray}}|\) & \(\frac{\max_{\begin{subarray}{c}x\in Q_{x}\in X\text{ rel}(q,x)\end{subarray}}}{|Q|}\) \\ \hline \hline MsWeb-1 & 500 & 10734 & 35.624 & 9 & 327 & 0.0033 \\ \hline MsWeb-2 & 500 & 10734 & 20.392 & 9 & 49 & 0.0019 \\ \hline MsNbc-1 & 500 & 110790 & 24.09 & 9 & 44 & 0.00022 \\ \hline MsNbc-2 & 500 & 110790 & 19.78 & 9 & 34 & 0.00017 \\ \hline \end{tabular}
\end{table}
Table 7: Dataset statistics. From left to right: Datasets name, number of queries, number of corpus, the average number of relevant corpus items per query, the minimum num of relevant corpus items per query, the maximum number of corpus items per query and the average query selectivity.

### Details about hashcode generation network

We use the same hashcode training procedure for FourierHashNet, as well as the DP-RH and RH baselines. In all three cases, we generate 64 dimensional hashcodes. For FourierHashNet, the random hyperplanes \(\bm{W}\) are trained on 10 dimensional trained Fourier representations \(\bm{z}_{q},\bm{z}_{x}\). For RH  we use the original embeddings \(\bm{q}\) and \(\bm{x}\). For DP-RH, we use the augmented embeddings \(g(q)=\mathrm{sign}(\bm{W}[0,\bm{q}/||\bm{q}||])\) and \(h(x)=\mathrm{sign}(\bm{W}[\sqrt{T^{2}-||\bm{x}||^{2}},\bm{x}])\).

### AP and MAP measurements

Suppose a query \(q\) is associated with \(N_{q\oplus}\) relevant corpus items (as judged by humans). Suppose the system provides a ranking over all \(N\) corpus items, and the relevant items occur at ranks \(r_{1},\ldots,r_{N_{q\oplus}}\). Then AP for query \(q\) is defined as \((1/N_{q\oplus})\sum_{i=1}^{N_{q\oplus}}(i/r_{i})\). This is because, up to position \(r_{i}\), we have seen \(i\) relevant items, which means we can shorthand \(i/r_{i}\) as prec\(@i\) (precision at \(i\)). We can rewrite the sum as \(\frac{1}{N_{q\oplus}}\sum_{r=1}^{N}\text{prec}@r\times\mathrm{rel}@r\), where \(N\) is the size of the whole corpus, and \(\mathrm{rel}@r\) is the (0/1) relevance of the item at rank \(r\). In case the retrieval algorithm does not assess all \(N\) corpus items, but stops with the best \(L\) hash buckets, which contain, say, \(N^{\prime}_{q}\) items, we should use the following formula for AP: \(\frac{1}{N_{q\oplus}}\sum_{r=1}^{N^{\prime}_{q}}\text{prec}@r\times\mathrm{ rel}@r\). Note that we should still divide by \(N_{q\oplus}\), otherwise an algorithm that maps the query to a densely relevant but small bin, which fails to retrieve most relevant items, might be rewarded in an unfair manner.

### Top-10 score measurement

In Appendix F, we provide additional experiments where we compare FourierHashNet with all the baselines not only in terms of MAP, but also in terms of the Top-10 score. We use the sum of Top-10 scores normalized in \([0,1]\) via the sigmoid transformation used in Eq. (15): Top-\(10(q)=\sum_{x\in\text{Top-10}(N^{\prime}_{q})}\sigma(-d(q,x))\). Any hashing protocol is expected to retrieve the corpus items, which have the highest similarity scores with respect to any given query. The Top-10 score evaluates it independently of how the retrieved items match with true relevant items. Therefore, the Top-10 scores provide an evaluation mechanism that is independent of the gold relevance labels and solely relies on the scores dictated by the trained embeddings. This offers a valuable means of assessing performance without being influenced by subjective human judgments.

### Licenses

We utilize a publicly available pre-trained sentence transformer model [41], which is licensed under the Apache License 2.0. Additionally, we employ the DrHash toolkit [58] for various implementations of the baseline Weighted Minhash (WMH) algorithms. The DrHash toolkit is publicly available under the MIT License. We duly acknowledge the original authors of the baseline methods in our citations.

Additional experiments

### Applying baseline LSHs on hinge distance guided embeddings

In continuation of the results reported in Figure 3, in Figure 8, we present the complete view as well as the zoomed versions of the trade-off curves for all datasets.

Beyond the observations made in Figure 3, we make the following additional observations.

**(1) The complete view for both Top-10 score and MAP score, clearly demonstrates FLORA's high sensitivity to hyperparameter tuning. FLORA is seen to have the highest variance in scores for any given time budget across all the baselines. In terms of Top-10 score, while FLORA is marginally ahead of FourierHashNet in a few instances in the MsWeb datasets, it is significatly outperformed by FourierHashNet in the MsNbc datasets. This may be due to the significantly higher average query selectivity in the MsNbc datasets.**

**(2) In terms of Top-10 score, FourierHashNet achieves the maximum possible value 4\(\times\) faster than the nearest competitor RH+Hinge, in MsWeb-1 and MsWeb-2. In terms of MAP score,

Figure 8: Trade-off between average query time and accuracy (MAP and Top-10 scores) for MsWeb and MsNbc datasets (First two rows: complete view across full time axis, last two rows: Zoomed version of first two rows until the average query time there is \(\geq\) 10X speedup compared to exhaustive search). We apply different LSH methods on hinge distance guided embeddings similar to Figure 3, then use the hinge distance to finally rank the retrieved items.

RH+Hinge and WMH+Hinge achieve a maximum MAP of 0.5 in MsWeb-1 and MsWeb-2, and 0.62 in MsNbc-1 and MsNbc-2. However, FourierHashNet achieves the same MAP values 1.33\(\times\) faster in MsWeb-1 and MsWeb-2, and 2\(\times\) faster in MsNbc-1 and MsNbc-2.

**(3)** Interestingly, the gap between FourierHashNet and RH+Hinge seems to widen for MsWeb-2, when compared to MsWeb-1. This is possibly due to the presence of several queries in MsWeb-1, which have \(\geq\)300 relevant corpus items. This affords RH+Hinge a greater opportunity to fetch high scoring items, which is not the case in MsWeb-2.

### Ablation study on hashcode training

In continuation of the results reported in Figure 5, in Figure 9, we present the complete view as well as the zoomed versions of the trade-off curves for all datasets.

Beyond the observations made in Figure 5, we make the following additional observations.

**(1)** In terms of both Top-10 score and MAP score, FHash (untrained) clearly outperforms both RH+Hinge and DP-RH+Hinge, across all four datasets. This strongly highlights the advantage of our

Figure 9: Trade-off between average query time and accuracy (MAP and Top-10 scores) for MsWeb and MsNbc datasets (First two rows: complete view across full time axis, last two rows: Zoomed version of first two rows until the average query time there is \(\geq\) 10X speedup compared to exhaustive search). We compare FHash (untrained) against the untrained versions of RH+Hinge and DP-RH+Hinge, as well as against FourierHashNet.

Fourier feature generation method.

**(2)** In every setup, FourierHashNet enables a wider range of options for accuracy score vs retrieval time trade-off.

### Ablation study on collision minimizer (14)

In continuation of the results reported in Figure 6, we present the complete view as well as the zoomed versions of the trade-off curves for all datasets. Beyond the observations made in Figure 6, we make the following additional observations.

**(1)** In MsWeb-2, the alternative variant shows a sudden plunge in MAP performance trade-off at around 0.1 seconds. This type of discontinuous drop is not observed in any of our cases.

**(2)** In MsWeb-1, there is a variation of 0.1 MAP at around 0.05 seconds. Such high variability is not observed for any of our trade-off curves.

### Identifying best performing Weighted Minhash algorithm for our datasets

For implementation of baseline Weighted Minhash (WMH) algorithm, we use the best performing WMH implementation available in the DrHash toolkit [58] to obtain the hashcodes. We compare

Figure 10: Trade-off between mean query time and accuracy (MAP and Top-10 scores) for MsWeb and MsNbc datasets (First two rows: complete view across full time axis, last two rows: Zoomed version of first two rows until the mean query time there is \(\geq\)10X speedup compared to exhaustive search). We compare our hashcode training loss loss\((Q,X\mid\bm{W})\) (14), against a variant which replaces the collision minimizer component \(\Delta_{1}\) with a decorrelation loss \(\sum_{x\neq y}|\tanh(\bm{W}\bm{z}_{x})^{\top}\tanh(\bm{W}\bm{z}_{y})|\).

across the 8 available baselines in the toolkit: minhash [7], chum [12], icws [20], pcws [56], licws [28], ccws [55], i2cws [57] and gollapudi2 [16].

We make the following observations.

**(1)** Across all four of our datasets, for both MAP and Top-10 score, the top 3 performers are minhash, goldapudi2 and chum. The remaining algorithms are often significantly worse in performance, as can be seen for MAP in all 4 datasets and for Top-10 in MsNbc-2.

**(2)** Among the top 3 performers, chum is seen to be the best perform in terms for both MAP and Top-10 score, in MsWeb-2, MsNbc-1 and MsNbc-2. In MsWeb-1, chum is tied with minhash and gollapudi2 for the top position.

Driven by these observations, we choose chum as the representative baseline for WMH in our experiments.

Figure 11: We compare performance of Weighted Minhash variations, in terms of trade-off between mean query time and accuracy (MAP and Top-10 scores) for MsWeb and MsNbc datasets, until the query time there is \(\geq\)10X speedup compared to exhaustive search.

### Effect of \(M\), number of samples of \(\omega\) on FourierHashNet performance

Here we check the impact of varying the number of samples (\(M\)) for \(\bm{\omega}\). We consider three different values of \(M\), _i.e._, 10, 50 and 100, for generating the fourier features \(\bm{F}_{q}(\iota\bm{\omega}^{1..M})\) and \(\bm{F}_{x}(\iota\bm{\omega}^{1..M})\) which are then fed into the neural networks \(\phi_{q}\) and \(\phi_{x}\) for learning the transformed Fourier representations \(\bm{z}_{q}\) and \(\bm{z}_{x}\), using the BCE loss specified in Eq. (12). Finally, we train the random hyperplanes \(\bm{W}\) and check the MAP and Top-10 score performances for the three variations - FourierHashNet(10), FourierHashNet(50) and FourierHashNet(100). We observe that the final performance trade-off of both MAP and Top-10 scores, remains roughly the same across all three variants. This shows that trainable Fourier transformation is able to compensate for the quality of Monte Carlo approximations affected by the number of \(\omega\) samples \(M\).

Next, we investigate how well the MC estimates of the Fourier features approximate the dominance similarity function \(\sin(q,x)\). Here, we set the dimension of \(\bm{q}\) and \(\bm{x}\) as \(K=1\). We set \(T=20\) and we sample \(\bm{q},\bm{x}\sim\text{Unif}[-20,20]\). Finally, we compute \(\widehat{\sin}_{M}(q,x)=\frac{||\bm{F}_{\bm{z}}(\iota\bm{\omega}^{1..M})||^{2 }}{M}\cos(\bm{F}_{q}(\iota\bm{\omega}^{1..M}),\bm{F}_{x}(\iota\bm{\omega}^{1..M }))\) and measure the variation of \(\epsilon_{\text{sim}}=\mathbb{E}_{\bm{q},\bm{x}\sim\text{Unif}[-20,20]}[||\text {sim}(q,x)-\widehat{\sin}_{M}(q,x)||]\) with \(M\), the number of samples of \(\omega\). Figure 13 summarizes the results, which shows as \(M\) increases, \(\epsilon_{\text{sim}}\) decreases.

Figure 12: Effect of \(M\), number of \(\omega\) samples, on the trade-off between mean query time and accuracy (MAP and Top-10 scores) for MsWeb datasets.

### Applying baseline LSHs on hinge distance guided embeddings with noisy labels

In certain applications, the accuracy of ground truth labels can be compromised by noise or subjective human judgments of relevance. We evaluate the performance of FourierHashNet and the baselines in a noisy label setup to test its robustness.

Starting with the hinge distance guided embeddings, we initially rank the corpus items based on their dominance similarity scores. Subsequently, we intentionally flip the labels of the bottom-ranking 10% of positive labels to negative, and an equal number of highest ranked negatively labeled items to positive. This simulation reflects a plausible scenario since the lowest ranked positive items and the highest ranked negative items are particularly susceptible to misclassification in real-world settings. Furthermore, this approach ensures that the average query selectivity for each dataset remains unchanged.

As before, we apply different the LSH methods on hinge distance guided embeddings, _viz_, RH+Hinge, DP-RH+Hinge, WMH+Hinge, FLORA and FourierHashNet; and, then use the hinge distance to finally rank the retrieved items.

Figure 14: Trade-off between average query time and accuracy (MAP and Top-10 scores) for MsWeb and MsNbc datasets (First two rows: complete view across full time axis, last two rows: Zoomed version of first two rows until the average query time there is \(\geq\) 10X speedup compared to exhaustive search). We apply different LSH methods on hinge distance guided embeddings similar to Figure 8, and then use hinge distance to rank retrieved items. Evaluations are conducted using noisy labels.

We make the following observations.

**(1)** In terms of MAP score, FourierHashNet continues to outperform all other baselines across all four datasets. Furthermore, when comparing the performance in the presence of noise, as depicted in Figure 14, to the corresponding results obtained in the noiseless setting illustrated in Figure 8, we observe that FourierHashNet outperforms all its competitors by a significantly higher margin in the presence of noise.

**(2)** In terms of Top-10 score, we note that the results in Figure 14 for the noisy setup are identical to the results presented in the noiseless setting shown in Figure 8. This observation indicates that the evaluation based on Top-10 score is unaffected by label noise. This supports the argument made in Appendix E.7 that Top-10 score evaluation enables a more subjective assessment of performance, focusing on the quality of the embeddings themselves.

### Comparison with graph based indexing methods

Here, we compare the performance of FourierHashNet with a representative graph based indexing method. We use the Hierarchical Navigable Small Worlds (HNSW) implementation of Hnswib [33]. We extend the SpaceInterface class of nmslib/Hnswlib to implement HNSW for hinge distance. In order to track the number of distance computations performed by HNSW during retrieval, we used a counter inside fstdistfunc_. We count the number of distance computations as a surrogate for real time, to avoid non-determinism in measurements and low-level implementation differences. We search across different values of M, ef and ef_construction, and track the number of distance computations against corresponding MAP values.

Table presents our study on MsWeb-1dataset with 10734 corpus items. We observe that FourierHashNet LSH has an edge over HNSW in the regime of fewer distance computations, with a MAP of 0.53 using 1162 distance computations. However, when allowed more distance computations, HNSW outperforms FourierHashNet with a MAP of 0.71 in 3528 computations, and a MAP of 0.77 in 4773 computations.

\begin{table}
\begin{tabular}{|c|c|c|} \hline \#calls to fstdistfunc\_ & MAP & Method \\ \hline
839 & 0.17 & HNSW \\ \hline
1162 & 0.53 & FourierHashNet \\ \hline
1549 & 0.43 & HNSW \\ \hline
1668 & 0.51 & HNSW \\ \hline
1846 & 0.58 & FourierHashNet \\ \hline
2578 & 0.59 & HNSW \\ \hline
3529 & 0.71 & HNSW \\ \hline
3926 & 0.72 & FourierHashNet \\ \hline
4773 & 0.77 & HNSW \\ \hline \end{tabular}
\end{table}
Table 15: Number of distance computations and the corresponding MAP values on MsWeb-1for HNSW and FourierHashNet.