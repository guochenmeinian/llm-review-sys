# Continuous-time Analysis of Anchor Acceleration

Jaewook J. Suh

Seoul National University

jacksuhkr@snu.ac.kr &Jisun Park

Seoul National University

colleenp0515@snu.ac.kr &Ernest K. Ryu

Seoul National University

ernestryu@snu.ac.kr

###### Abstract

Recently, the anchor acceleration, an acceleration mechanism distinct from Nesterov's, has been discovered for minimax optimization and fixed-point problems, but its mechanism is not understood well, much less so than Nesterov acceleration. In this work, we analyze continuous-time models of anchor acceleration. We provide tight, unified analyses for characterizing the convergence rate as a function of the anchor coefficient \(\beta(t)\), thereby providing insight into the anchor acceleration mechanism and its accelerated \(\mathcal{O}(1/k^{2})\)-convergence rate. Finally, we present an adaptive method inspired by the continuous-time analyses and establish its effectiveness through theoretical analyses and experiments.

## 1 Introduction

Nesterov acceleration [51] is foundational to first-order optimization theory, but the mechanism and its convergence proof are not transparent. One approach to better understand the mechanism is the continuous-time analysis: derive an ODE model of the discrete-time algorithm and analyze the continuous-time dynamics [65, 66]. This approach provides insight into the accelerated dynamics and has led to a series of follow-up work [71, 62, 29].

Recently, a new acceleration mechanism, distinct from Nesterov's, has been discovered. This _anchor acceleration_ for minimax optimization and fixed-point problems [35, 75, 54] has been an intense subject of study, but its mechanism is understood much less than Nesterov acceleration. The various analytic techniques developed to understand Nesterov acceleration, including continuous-time analyses, have only been applied in a very limited manner [59].

Contribution.In this work, we present continuous-time analyses of anchor acceleration. The continuous-time model is the differential inclusion

\[\dot{X}\in-\mathsf{A}(X)-\beta(t)(X-X_{0})\]

with initial condition \(X(0)=X_{0}\in\operatorname{dom}\mathsf{A}\), maximal monotone operator \(\mathsf{A}\), and scalar-valued function \(\beta(t)\). The case \(\beta(t)=\frac{1}{t}\) corresponds to the prior anchor-accelerated methods APPM [35], EAG [75], and FEG [42].

We first establish that the differential inclusion is well-posed, despite the anchor coefficient \(\beta(t)\) blowing up at \(t=0\). We then provide tight, unified analyses for characterizing the convergence rate as a function of the anchor coefficient \(\beta(t)\). This is the first formal and rigorous treatment of this anchored dynamics, and it provides insight into the anchor acceleration mechanism and its accelerated \(\mathcal{O}(1/k^{2})\)-convergence rate. Finally, we present an adaptive method inspired by the continuous-time analyses and establish its effectiveness through theoretical analyses and experiments.

### Preliminaries and notation

We provide the organization for prior works in Appendix A. Here, we review standard definitions and set up the notation.

Monotone and set-valued operators.We follow the standard definitions of Bauschke and Combettes [15], Ryu and Yin [58]. For the underlying space, consider \(\mathbb{R}^{n}\) with standard inner product \(\langle\cdot,\cdot\rangle\) and norm \(\left\|\cdot\right\|\). Define domain of \(\mathbf{A}\) as \(\operatorname{dom}\mathbf{A}=\{x\in\mathbb{R}^{n}\mid\mathbf{A}x\neq\emptyset\}\). We say \(\mathbf{A}\) is an operator on \(\mathbb{R}^{n}\) and write \(\mathbf{A}\colon\mathbb{R}^{n}\rightrightarrows\mathbb{R}^{n}\) if \(\mathbf{A}\) maps a point in \(\mathbb{R}^{n}\) to a subset of \(\mathbb{R}^{n}\). We say \(\mathbf{A}\colon\mathbb{R}^{n}\rightrightarrows\mathbb{R}^{n}\) is monotone if

\[\langle\mathbf{A}x-\mathbf{A}y,x-y\rangle\geq 0,\qquad\forall x,y\in\mathbb{R} ^{n},\]

where the notation means that \(\langle u-v,x-y\rangle\geq 0\) for all \(u\in\mathbf{A}x\) and \(v\in\mathbf{A}y\). For \(\mu\in(0,\infty)\), say \(\mathbf{A}\colon\mathbb{R}^{n}\rightrightarrows\mathbb{R}^{n}\) is \(\mu\)-strongly monotone if

\[\langle\mathbf{A}x-\mathbf{A}y,x-y\rangle\geq\mu\|x-y\|^{2},\qquad\forall x,y \in\mathbb{R}^{n}.\]

Write \(\operatorname{Gra}\mathbf{A}=\{(x,u)\mid u\in\mathbf{A}x\}\) for the graph of \(\mathbf{A}\). An operator \(\mathbf{A}\) is maximally monotone if there is no other monotone \(\mathbf{B}\) such that \(\operatorname{Gra}\mathbf{A}\subset\operatorname{Gra}\mathbf{B}\) properly, and is maximally \(\mu\)-strongly monotone if there is no other \(\mu\)-strongly monotone \(\mathbf{B}\) such that \(\operatorname{Gra}\mathbf{A}\subset\operatorname{Gra}\mathbf{B}\) properly.

For \(L\in(0,\infty)\), single-valued operator \(\mathbf{T}\colon\mathbb{R}^{n}\to\mathbb{R}^{n}\) is \(L\)-Lipschitz if

\[\left\|\mathbf{T}x-\mathbf{T}y\right\|\leq L\|x-y\|,\qquad\forall x,y\in \mathbb{R}^{n}.\]

Write \(\mathbf{J}_{\mathbf{A}}=(\mathbf{I}+\mathbf{A})^{-1}\) for the resolvent of \(\mathbf{A}\), while \(\mathbf{I}\colon\mathbb{R}^{n}\to\mathbb{R}^{n}\) is the identity operator. When \(\mathbf{A}\) is maximally monotone, it is well known that \(\mathbf{J}_{\mathbf{A}}\) is single-valued with \(\operatorname{dom}\mathbf{J}_{\mathbf{A}}=\mathbb{R}^{n}\).

We say \(x_{\star}\in\mathbb{R}^{n}\) is a zero of \(\mathbf{A}\) if \(0\in\mathbf{A}x_{\star}\). We say \(y_{\star}\) is a fixed-point of \(\mathbf{T}\) if \(\operatorname{Ty}_{\star}=y_{\star}\). Write \(\operatorname{Zer}\mathbf{A}\) for the set of all zeros of \(\mathbf{A}\) and \(\operatorname{Fix}\mathbf{T}\) for the set of all fixed-points of \(\mathbf{T}\).

Monotonicity with continuous curves.We say an operator is differentiable if it is single-valued, continuous, and differentiable as a function. If a differentiable operator \(\mathbf{A}\) is monotone and \(X\colon[0,\infty)\to\mathbb{R}^{n}\) is a differentiable curve, then taking limit \(h\to 0\) of

\[\frac{1}{h^{2}}\left\langle\mathbf{A}(X(t+h))-\mathbf{A}(X(t)),X(t+h)-X(t) \right\rangle\geq 0\]

leads to

\[\left\langle\frac{d}{dt}\mathbf{A}(X(t)),\dot{X}(t)\right\rangle\geq 0.\] (1)

Similarly if \(\mathbf{A}\) is furthermore \(\mu\)-strongly monotone, then

\[\left\langle\frac{d}{dt}\mathbf{A}(X(t)),\dot{X}(t)\right\rangle\geq\mu\left\| \dot{X}(t)\right\|^{2}.\] (2)

## 2 Derivation of differential inclusion model of anchor acceleration

### Anchor ODE

Suppose \(\mathbf{A}\colon\mathbb{R}^{n}\rightrightarrows\mathbb{R}^{n}\) is a maximal monotone operator and \(\beta:(0,\infty)\to[0,\infty)\) is a twice differentiable function. Consider differential inclusion

\[\dot{X}(t)\in-\mathbf{A}(X(t))-\beta(t)(X(t)-X_{0})\] (3)

with initial condition \(X(0)=X_{0}\in\operatorname{dom}\left(\mathbf{A}\right)\). We refer to this as the _anchor ODE_. 1 We say \(X\colon[0,\infty)\to\mathbb{R}^{n}\) is a solution, if it is absolutely continuous and satisfies (3) for \(t\in(0,\infty)\) almost everywhere.

Footnote 1: Strictly speaking, this is a differential inclusion, not a differential equation, but we nevertheless refer to it as an ODE.

Denote \(S\) as the subset of \([0,\infty)\) on which \(X\) satisfies the differential inclusion. Define

\[\tilde{\mathbf{A}}(X(t))=-\dot{X}(t)-\beta(t)(X(t)-X_{0})\]

for \(t\in S\). Since \(\tilde{\mathbf{A}}(X(t))\in\mathbf{A}(X(t))\) for \(t\in S\), we say \(\tilde{\mathbf{A}}\) is a _selection_ of \(\mathbf{A}\) for \(t\in S\). If \(\left\|\tilde{\mathbf{A}}(X(t))\right\|\) is bounded on all bounded subsets of \(S\), then we can extend \(\tilde{\mathbf{A}}\) to \([0,\infty)\) while retaining certain favorable properties. We discuss the technical details of this extension in Appendix E.1. The statements of Section 3 are stated with this extension.

### Derivation from discrete methods

We now show that the following instance of the anchor ODE

\[\dot{X}(t)=-\mathbf{A}(X(t))-\frac{1}{t}(X(t)-X_{0}),\] (4)

where \(X(0)=X_{0}\) is the initial condition and \(\mathbf{A}\colon\mathbb{R}^{n}\to\mathbb{R}^{n}\) is a continuous operator, is a continuous-time model of APPM [35], EAG [75], and FEG [42], which are accelerated methods for monotone inclusion and minimax problems.

Consider APPM with operator \(h\mathbf{A}\)

\[x^{k} =\mathbf{J}_{h\mathbf{A}}y^{k-1}\] \[y^{k} =\frac{k}{k+1}(2x^{k}-y^{k-1})+\frac{1}{k+1}y^{0}\] (5)

with initial condition \(y^{0}=x^{0}\). Assume \(h>0\) and \(\mathbf{A}\colon\mathbb{R}^{n}\to\mathbb{R}^{n}\) is a continuous monotone operator. Using \(y^{k-1}=x^{k}+h\mathbf{A}x^{k}\) obtained from the first line, substituting \(y^{k}\) and \(y^{k-1}\) in the second line we get,

\[x^{k+1}+h\mathbf{A}x^{k+1}=\frac{k}{k+1}\left(x^{k}-h\mathbf{A}x^{k}\right)+ \frac{1}{k+1}x^{0}.\]

Then reorganizing and dividing both sides by \(h\), we have

\[\frac{x^{k+1}-x^{k}}{h}=-\mathbf{A}x^{k+1}-\frac{k}{k+1}\mathbf{A}x^{k}-\frac {1}{h(k+1)}(x^{k}-x^{0}).\]

Identifying \(x^{0}=X_{0}\), \(2hk=t\), and \(x^{k}=X(t)\), we have \(\frac{k}{k+1}=1-\frac{h}{hk+h}=1+\mathcal{O}\left(h\right)\) and so

\[2\dot{X}(t)+\mathcal{O}\left(h\right)=-\mathbf{A}(X(t+2h))-\left(1+\mathcal{O }\left(h\right)\right)\mathbf{A}(X(t))-\frac{2}{t+\mathcal{O}\left(h\right)} \left(X(t)-X_{0}\right).\]

Taking limit \(h\to 0^{+}\) and dividing both sides by \(2\), we get the anchor ODE (4). The correspondence with EAG and FEG are provided in Appendix D.4.

The following theorem establishes a rigorous correspondence between APPM and the anchor ODE for general maximal monotone operators.

**Theorem 2.1**.: _Let \(\mathbf{A}\) be a (possibly set-valued) maximal monotone operator and assume \(\mathrm{Zer}\mathbf{A}\neq\emptyset\). Let \(x^{k}\) be the sequence generated by APPM (5) and \(X\) be the solution of the differential inclusion (3) with \(\beta(t)=\frac{1}{t}\). For all fixed \(T>0\),_

\[\lim_{h\to 0+}\max_{0\leq k\leq\frac{T}{2k}}\left\|x^{k}-X(2kh)\right\|=0.\]

We provide the proof in Appendix D.2.

### Existence of the solution for \(\beta(t)=\frac{\gamma}{t^{p}}\)

To get further insight into the anchor acceleration, we generalize anchor coefficient to \(\beta(t)=\frac{\gamma}{t^{p}}\) for \(p,\gamma>0\). We first establish the uniqueness and existence of the solution.

**Theorem 2.2**.: _Consider (3) with \(\beta(t)=\frac{\gamma}{t^{p}}\), i.e._

\[\dot{X}(t)\in-\mathbf{A}(X(t))-\frac{\gamma}{t^{p}}(X(t)-X_{0}).\] (6)

_for \(p,\gamma>0\). Then solution of (6) uniquely exists._

We provide the proof in Appendix B.

### Additional properties of anchor ODE

We state a regularity lemma of the differential inclusion (3), which we believe may be of independent interest. In particular, we use this result several times throughout our various proofs.

**Lemma 2.3**.: _Let \(X(\cdot)\) and \(Y(\cdot)\) are solutions of the differential inclusion (3) respectively with initial values and anchors \(X_{0}\) and \(Y_{0}\). Then for all \(t\in[0,\infty)\),_

\[\left\|X(t)-Y(t)\right\|\leq\left\|X_{0}-Y_{0}\right\|.\]

We provide the proof in Appendix B.1.

Boundedness of trajectories is an immediate corollary of Lemma 2.3. Specifically, suppose \(X(\cdot)\) is the solution of differential inclusion (3) with initial value \(X_{0}\). Then for all \(X_{\star}\in\mathrm{Zer}\mathbf{A}\) and \(t\in[0,\infty)\),

\[\left\|X(t)-X_{\star}\right\|\leq\left\|X_{0}-X_{\star}\right\|.\]

This follows from setting \(Y_{0}=X_{\star}\) in Lemma 2.3.

## 3 Convergence analysis

We now analyze the convergence rate of \(\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}\) for the anchor ODE (3) with \(\beta(t)=\frac{\gamma}{t^{p}}\) and \(\gamma,p>0\). The results are organized in Table 1.

Let \(\beta\) be the anchor coefficient function of (3). Define \(C\colon[0,\infty)\to\mathbb{R}\) as \(C(t)=e^{\int_{\cdot}^{t}\beta(s)ds}\) for some \(v\in[0,\infty]\). Note that \(\dot{C}=C\beta\) and \(C\) is unique up to scalar multiple. We call \(\mathcal{O}\left(\beta(t)\right)\) the _vanishing speed_ and \(\mathcal{O}\left(\frac{1}{C(t)}\right)\) the _contracting speed_, and we describe their trade-off in the following.

Loosely speaking, the _contracting speed_ describes how fast the anchor term alone contracts the dynamical system. Consider \(\dot{X}(t)=-\beta(t)(X(t)-a)\) for \(a\in\mathbb{R}^{n}\), a system only with the anchor. Then, \(X(t)=\frac{C(0)}{C(t)}(X(0)-a)+a\) is the solution, so the flow contracts towards the anchor \(a\) with rate \(\frac{1}{C(t)}\). Intuitively speaking, this contracting behavior leads to stability and convergence. On the other hand, the anchor must eventually vanish, since our goal is to converge to an element in \(\mathrm{Zer}\mathbf{A}\), not the anchor. Thus the _vanishing speed_ must be fast enough to not slow down the convergence of the flow to \(\mathrm{Zer}\mathbf{A}\).

This observation is captured in Figure 1. Consider a monotone linear operator \(\mathbf{A}=\left(\begin{smallmatrix}0&1\\ -1&0\end{smallmatrix}\right)\) on \(\mathbb{R}^{2}\) and \(\beta(t)=\frac{\gamma}{t^{p}}\) with \(\gamma=1\) and \(p>0\). Note if there is no anchor, the ODE reduces to \(\dot{X}=-\mathbf{A}(X)\) which do not converge [31, Chapter 8.2]. Figure 1 shows that with \(p>1\), the anchor vanished too early before the flow is contracted enough to result in converging flow. With \(p<1\), the flow does converge but the anchor vanished too late, slowing down the convergence. With \(p=1\), the convergence is fastest.

The following theorem formalizes this insight and produces the results of Table 1.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Case & \(\begin{array}{l}p=1\), \\ \(\gamma\geq 1\) \\ \end{array}\) & \(p=1\), & \(p<1\) & \(p>1\) \\ \hline \(\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}\) & \(\mathcal{O}\left(\frac{1}{t^{2}}\right)\) & \(\mathcal{O}\left(\frac{1}{t^{2\gamma}}\right)\) & \(\mathcal{O}\left(\frac{1}{t^{2p}}\right)\) & \(\mathcal{O}\left(1\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Convergence rates of Theorem 3.1.

**Theorem 3.1**.: _Suppose \(\mathbf{A}\) is a maximal monotone operator with \(\mathrm{Zer}\mathbf{A}\neq\emptyset\). Consider (3) with \(\beta(t)=\frac{\gamma}{t^{p}}\). Let \(\tilde{\mathbf{A}}(X(t))\) be the selection of \(\mathbf{A}(X(t))\) as in Section 2.1. Then,_

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}=\mathcal{O}\left(\frac{1}{C(t)^{2} }\right)+\mathcal{O}\left(\beta(t)^{2}\right)+\mathcal{O}\left(\dot{\beta}(t )\right).\]

Note that

\[C(t)=\begin{cases}t^{\gamma}&p=1\\ e^{\frac{\gamma}{1-p}t^{1-p}}&p\neq 1.\end{cases}\]

We expect the convergence rate of Theorem 3.1 to be optimized when the terms are balanced. When \(\beta(t)=\frac{1}{t}\),

\[\frac{1}{C(t)^{2}}=\frac{1}{(e^{\int_{t}^{t}\frac{1}{s}ds})^{2}}=\frac{1}{t^{2 }}=\beta(t)^{2}=-\dot{\beta}(t)\]

and all three terms are balanced. Indeed, the choice \(\beta(t)=\frac{1}{t}\) corresponds to the optimal discrete-time choice \(\frac{1}{k+2}\) of APPM or other accelerated methods.

### Proof outline of Theorem 3.1

The proof of Theorem 3.1 follows from Lemma 3.4, which we will introduce later in this section. To derive Lemma 3.4, we introduce a conservation law.

**Proposition 3.2**.: _Suppose \(\tilde{\mathbf{A}}\) is Lipschitz continuous and monotone. For \(t_{0}>0\), define \(E:(0,\infty)\to\mathbb{R}\) as_

\[E =\frac{C(t)^{2}}{2}\bigg{(}\left\|\tilde{\mathbf{A}}(X(t))\right\| ^{2}+2\beta(t)\left\langle\tilde{\mathbf{A}}\left(X(t)\right),X(t)-X_{0} \right\rangle+\left(\beta(t)^{2}+\dot{\beta}(t)\right)\left\|X(t)-X_{0}\right\| ^{2}\bigg{)}\] \[\quad-\int_{t_{0}}^{t}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta }(s)}{2}\right)\left\|X(s)-X_{0}\right\|^{2}ds+\int_{t_{0}}^{t}C(s)^{2}\left \langle\frac{d}{ds}\tilde{\mathbf{A}}(X(s)),\dot{X}(s)\right\rangle ds.\]

_Then \(E\) is a constant function._

The proof of Proposition 3.2 uses dilated coordinate \(W(t)=C(t)(X(t)-X_{0})\) to derive its conservation law in the style of Suh et al. [67]. We provide the details in Appendix E.2.

Recall from (1) that \(\left\langle\frac{d}{ds}\tilde{\mathbf{A}}(X(s)),\dot{X}(s)\right\rangle\geq 0\), the integrand of the last term of \(E\) is nonnegative. This motivates us to define

\[V(t)=E-\int_{t_{0}}^{t}C(s)^{2}\left\langle\frac{d}{ds}\tilde{\mathbf{A}}(X(s) ),\dot{X}(s)\right\rangle ds\]

as our Lyapunov function.

**Corollary 3.3**.: _Let \(\mathbf{A}\) be maximal monotone and \(\beta(t)=\frac{\gamma}{t^{p}}\) with \(p>0\), \(\gamma>0\). Let \(\tilde{\mathbf{A}}(X(t))\) be the selection of \(\mathbf{A}(X(t))\) as in Section 2.1. For \(t_{0}\geq 0\), define \(V:[0,\infty)\to\mathbb{R}\) as_

\[V(t) =\frac{C(t)^{2}}{2}\bigg{(}\left\|\tilde{\mathbf{A}}(X(t))\right\| ^{2}+2\beta(t)\left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle+ \left(\beta(t)^{2}+\dot{\beta}(t)\right)\left\|X(t)-X_{0}\right\|^{2}\bigg{)}\] \[\quad-\int_{t_{0}}^{t}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta }(s)}{2}\right)\left\|X(s)-X_{0}\right\|^{2}ds.\]

_for \(t>0\) and \(V(0)=\lim_{t\to 0+}V(t)\). Then \(V(t)\leq V(0)\) holds for \(t\geq 0\)._

A technical detail is that all terms involving \(\frac{d}{ds}\tilde{\mathbf{A}}(X(s))\) have been excluded in the definition of \(V\) and this is what allows \(\mathbf{A}\) to not be Lipschitz continuous. We provide the details in Appendix E.3.

**Lemma 3.4**.: _Consider the setup of Corollary 3.3. Assume \(\mathrm{Zer}\mathbf{A}\neq\emptyset\). Then for \(t>0\) and \(X_{\star}\in\mathrm{Zer}\mathbf{A}\),_

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2} \leq 4\beta(t)^{2}\left\|X_{0}-X_{\star}\right\|^{2}+\frac{4V(0)}{C( t)^{2}}-2\left(\beta(t)^{2}+\dot{\beta}(t)\right)\left\|X(t)-X_{0}\right\|^{2}\] \[\quad+\frac{2}{C(t)^{2}}\int_{t_{0}}^{t}\frac{d}{ds}\left(C(s)^{2 }\dot{\beta}(s)\right)\left\|X(s)-X_{0}\right\|^{2}ds.\] (7)Proof outline of Lemma 3.4.: Define

\[\Phi(t)=\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2\beta(t)\left\langle\tilde{ \mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle.\]

Then, from monotonicity of \(\tilde{\mathbf{A}}\) and Young's inequality,

\[\Phi(t) \geq\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2\beta(t)\left\langle \tilde{\mathbf{A}}(X(t)),X_{\star}-X_{0}\right\rangle\] \[\geq\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}-2\bigg{(}\left\| \frac{1}{2}\tilde{\mathbf{A}}(X(t))\right\|^{2}+\left\|\beta(t)\left(X_{\star} -X_{0}\right)\right\|^{2}\bigg{)}\] \[=\frac{1}{2}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}-2\beta(t )^{2}\left\|X_{0}-X_{\star}\right\|^{2}.\] (8)

By Corollary 3.3, \(\frac{2V(0)}{C(t)^{2}}-\frac{2V(t)}{C(t)^{2}}+\Phi(t)\geq\Phi(t)\) for \(t>0\). Applying (8) and organizing, we can get the desired result. The details are provided in Appendix E.4. 

Proof outline of Theorem 3.1.: It remains to show that last integral term of Lemma 3.4 is \(\mathcal{O}\left(\frac{1}{C(t)^{2}}\right)+\mathcal{O}\left(\beta(t)^{2} \right)+\mathcal{O}\left(\dot{\beta}(t)\right)\). The details are provided in Appendix E.5. 

Before we end this section, we observe how our analysis simplifies in the special case \(\beta(t)=\frac{1}{t}\). In this case,

\[V(t)=\frac{t^{2}}{2}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+t\left\langle \tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle,\]

and this corresponds to the Lyapunov function of [59, Section 4] for the case \(\gamma=1\). As \(V(0)=0\), the conclusion of Lemma 3.4 becomes

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}\leq\frac{4}{t^{2}}\left\|X_{0}-X_ {\star}\right\|^{2}=\mathcal{O}\left(\frac{1}{t^{2}}\right),\]

which to the best rate in Table 1.

### Point convergence

APPM is an instance of the Halpern method [54, Lemma 3.1], which iterates converge to the element in \(\mathrm{ZerA}\) closest to \(X_{0}\)[34, 73]. The anchor ODE also exhibits this behavior.

**Theorem 3.5**.: _Let \(\mathbf{A}\) be a maximal monotone operator with \(\mathrm{Zer}\mathbf{A}\neq\emptyset\) and \(X\) be the solution of (3). If \(\lim_{t\to\infty}\left\|\tilde{\mathbf{A}}(X(t))\right\|=0\) and \(\lim_{t\to\infty}1/C(t)=0\), then, as \(t\to\infty\),_

\[X(t)\to\operatorname*{argmin}_{z\in\mathrm{ZerA}}\left\|z-X_{0}\right\|.\]

We provide the proof in Appendix E.6.

## 4 Tightness of analysis

In this section, we show that the convergence rates of Table 1 are actually tight by considering the dynamics under the explicit example \(\mathbf{A}=\left(\begin{smallmatrix}0&1\\ -1&0\end{smallmatrix}\right)\). Throughout this section, we denote \(\mathbf{A}\) as \(A\) when when the operator is linear.

### Explicit solution for linear \(A\)

**Lemma 4.1**.: _Let \(A\colon\mathbb{R}^{n}\to\mathbb{R}^{n}\) be a linear operator and let \(\beta(t)=\frac{\gamma}{t}\). The series_

\[X(t)=\sum_{n=0}^{\infty}\frac{(-tA)^{n}}{\Gamma(n+\gamma+1)}\Gamma(\gamma+1)X_ {0},\]

_where \(\Gamma\) denotes the gamma function, is the solution for (3) with \(\mathbf{A}=A\)._Note that when \(\gamma=0\), this is the series definition of the matrix exponential and \(X(t)=e^{-tA}\). The solution also has an integral form, which extends to general \(\beta(t)\).

**Lemma 4.2**.: _Suppose \(A\colon\mathbb{R}^{n}\to\mathbb{R}^{n}\) is a monotone linear operator. Then_

\[X(t)=\frac{e^{-tA}}{C(t)}\left(\int_{0}^{t}e^{sA}C(s)\beta(s)ds+C(0)I\right)X_{0}\] (9)

_is the solution for (3) with \(\mathbf{A}=A\)._

See Appendix F.1.1 and Appendix F.1.2 for details.

### The rates in Table 1 are tight

First, we consider \(p>1\) for \(\beta(t)=\frac{\gamma}{t^{p}}\).

**Theorem 4.3**.: _Suppose \(\lim_{t\to\infty}\frac{1}{C(t)}\neq 0\), i.e., suppose \(\beta(t)\in L^{1}[t_{0},\infty)\) for some \(t_{0}>0\). Then there exists an operator \(\mathbf{A}\) such that_

\[\lim_{t\to\infty}\left\|\tilde{\mathbf{A}}(X(t))\right\|\neq 0,\]

_where \(X\) is the solution of (3)._

Note that \(\frac{\gamma}{t^{p}}\in L^{1}[t_{0},\infty)\) when \(p>1\). The proof of Theorem 4.3 considers \(\mathbf{A}=2\pi\xi\left(\begin{smallmatrix}0&1\\ -1&0\end{smallmatrix}\right)\) for \(\xi\in\mathbb{R}\) and uses the Fourier inversion formula. See Appendix F.2 for details.

Next, we consider \(\beta(t)=\frac{\gamma}{t^{p}}\) for cases other than \(p>1\).

**Theorem 4.4**.: _Let \(A=\left(\begin{smallmatrix}0&1\\ -1&0\end{smallmatrix}\right)\), \(\beta(t)=\frac{\gamma}{t^{p}}\), \(0<p\leq 1\), and \(\gamma>0\). Let \(X\) be the solution given by (9) and \(X_{0}\neq 0\). Let_

\[r(t)=\begin{cases}t^{2}&\text{for }p=1,\gamma\geq 1\\ t^{2\gamma}&\text{for }p=1,\gamma<1\\ t^{2p}&\text{for }0<p<1.\end{cases},\]

_Then,_

\[\lim_{t\to\infty}r(t)\left\|A(X(t))\right\|^{2}\neq 0.\]

We provide the proof in Appendix F.3.

## 5 Discretized algorithms

In this section, we provide discrete-time convergence results that match the continuous-time rate of Section 3.

**Theorem 5.1**.: _Suppose \(\mathbf{A}\) be a maximal monotone operator, \(p>0\), and \(\gamma>0\). Consider_

\[x^{k} =\mathbf{J}_{\mathbf{A}}y^{k-1}\] \[y^{k} =\frac{k^{p}}{k^{p}+\gamma}(2x^{k}-y^{k-1})+\frac{\gamma}{k^{p}+ \gamma}x^{0}\]

_for \(k=1,2,\ldots\), with initial condition \(y^{0}=x^{0}\in\mathbb{R}^{n}\). Let \(\tilde{\mathbf{A}}x^{k}=y^{k-1}-x^{k}\) for \(k=1,2,\ldots\). Then this method exhibits the rates of convergence in Table 2._

Note that the method of Theorem 5.1 reduces to APPM when \(\gamma=1\), \(p=1\).

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Case & \(\begin{array}{c}p=1,\\ \gamma\geq 1\end{array}\) & \(\begin{array}{c}p=1,\\ \gamma<1\end{array}\) & \(\begin{array}{c}p<1\end{array}\) & \(\begin{array}{c}p>1\end{array}\) \\ \hline \(\left\|\tilde{\mathbf{A}}(x^{k})\right\|^{2}\) & \(\mathcal{O}\left(\frac{1}{k^{2}}\right)\) & \(\mathcal{O}\left(\frac{1}{k^{2\gamma}}\right)\) & \(\mathcal{O}\left(\frac{1}{k^{2p}}\right)\) & \(\mathcal{O}\left(1\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Rates for the discrete-time method of Theorem 5.1.

Proof outline of Theorem 5.1.: The general strategy is to find discretized counterparts of corresponding continuous-time analyses. However, directly discretizing the conservation law of Proposition 3.2 was difficult due to technical reasons. Instead, we obtain differently scaled but equivalent conservation laws using dilated coordinates and then performed the discretization. The specific dilated coordinates, inspired by [67], are \(W_{1}(t)=X(t)-X_{0}\) for \(p>1\), \(W_{2}(t)=t^{p}\left(X(t)-X_{0}\right)\) for \(0<p<1\), \(W_{3}(t)=t\left(X(t)-X_{0}\right)\) for \(p=1\), \(\gamma\geq 1\) and \(W_{4}(t)=t^{\gamma}\left(X(t)-X_{0}\right)\) for \(p=1\), \(0<\gamma<1\).

In the discrete-time analyses, the behavior of the leading-order terms is predictable as they match the continuous-time counterpart. The difficult part is, however, controlling the higher-order terms that were not present in the continuous-time analyses. Through our detailed analyses, we bound such higher-order terms and show that they do not affect the convergence rate in the end. We provide the details in Appendix G.3. 

## 6 Convergence analysis under strong monotonicity

In this section, we analyze the dynamics of the anchor ODE (3) for \(\mu\)-strongly monotone \(\mathbf{A}\). When \(\beta(t)=\frac{1}{t}\) and \(\mathbf{A}=\left(\begin{smallmatrix}\mu&0\\ 0&\mu\end{smallmatrix}\right)\), Lemma 4.1 tells us that \(\mathbf{A}(X(t))=\frac{1}{t}\left(I-e^{-tA}\right)X_{0}\) and therefore that \(\left\|\mathbf{A}(X(t))\right\|^{2}=\Theta\left(\frac{1}{t^{2}}\right)\), which is a slow rate for the strongly monotone setup. On the other hand, we will see that \(\beta(t)=\frac{2\mu}{e^{2\mu t}-1}\) is a better choice leading to a faster rate in this setup.

Our analysis of this section is also based on a conservation law, but we use a slightly modified version to exploit strong monotonicity.

**Proposition 6.1**.: _Suppose \(\tilde{\mathbf{A}}\) is monotone and Lipschitz continuous. Let \(X\) be the solution of (3) and let \(R:[0,\infty)\to(0,\infty)\) be a differentiable function. For \(t_{0}>0\), define \(E:(0,\infty)\to\mathbb{R}\) as_

\[E=\frac{C(t)^{2}R(t)^{2}}{2}\bigg{(}\left\|\tilde{\mathbf{A}}(X( t))\right\|^{2}+2\beta(t)\big{\langle}\tilde{\mathbf{A}}(X(t)),X(t)\!-\!X_{0} \big{\rangle}+\left(\beta(t)^{2}+\dot{\beta}(t)\right)\left\|X(t)-X_{0}\right\| ^{2}\bigg{)}\] \[-\!\int_{t_{0}}^{t}\!\frac{d}{ds}\bigg{(}\frac{C(s)^{2}R(s)^{2} \dot{\beta}(s)}{2}\bigg{)}\!\left\|X(t)\!-\!X_{0}\right\|^{2}ds+\!\!\int_{t_{0 }}^{t}\!\!C(s)^{2}R(s)^{2}\!\!\left(\!\!\left\langle\frac{d}{ds}\tilde{ \mathbf{A}}(X(s)),\dot{X}(s)\!\right\rangle\!-\!\frac{\dot{R}(s)}{R(s)}\! \left\|\dot{X}(s)\right\|^{2}\!\!\right)\!ds.\]

_Then \(E\) is a constant function for \(t\in[0,\infty)\)._

Proposition 6.1 generalizes Proposition 3.2, since it corresponds to the special case with \(R(t)\equiv 1\).

Recall from (2), when \(\mathbf{A}\) is \(\mu\)-strongly monotone we have

\[\left\langle\frac{d}{ds}\mathbf{A}(X(t)),\dot{X}(t)\right\rangle-\mu\left\| \dot{X}(t)\right\|^{2}\geq 0.\]

This motivates the choice \(R(t)=e^{\mu t}\), since \(\frac{\dot{R}(s)}{R(s)}=\mu\). From calculation provided in Appendix H.2, the choice \(\beta(t)=\frac{2\mu}{e^{2\mu t}-1}\) makes \(\frac{d}{ds}\left(\frac{C(s)^{2}R(s)^{2}\beta(s)}{2}\right)=0\). Plugging these choices into Proposition 6.1 and following arguments of Section 3, we arrive at the following theorem.

**Theorem 6.2**.: _Let \(\mathbf{A}\) be a \(\mu\)-strongly maximal monotone operator with \(\mu>0\) and assume \(\mathrm{Zer}\mathbf{A}\neq\emptyset\). Let \(X\) be a solution of the differential inclusion (3) with \(\beta(t)=\frac{2\mu}{e^{2\mu t}-1}\), i.e. for almost all \(t\),_

\[\dot{X}\in-\mathbf{A}(X)-\frac{2\mu}{e^{2\mu t}-1}(X-X_{0}).\] (10)

_Let \(\tilde{\mathbf{A}}(X(t))\) be the selection of \(\mathbf{A}(X(t))\) as in Section 2.1. Define \(V:[0,\infty)\to\mathbb{R}\) as_

\[V(t)=\frac{(e^{\mu t}-e^{-\mu t})^{2}}{2}\left\|\tilde{\mathbf{A}}(X(t))\right\| ^{2}+2\mu\left(1-e^{-2\mu t}\right)\!\left(\left\langle\tilde{\mathbf{A}}(X(t )),X(t)-X_{0}\right\rangle-\mu\left\|X(t)-X_{0}\right\|^{2}\right).\]

_Then \(V(t)\leq V(0)\) holds for \(t\geq 0\). Furthermore for \(X_{\star}\in\mathrm{Zer}\mathbf{A}\),_

\[\|\tilde{\mathbf{A}}(X(t))\|^{2}\leq\left(\frac{2\mu}{e^{\mu t}-1}\right)^{2} \|X_{0}-X_{\star}\|^{2}=\mathcal{O}\left(\frac{1}{e^{2\mu t}}\right).\]In Appendix H.2.3, we show that (10) is a continuous-time model for OS-PPM of Park and Ryu [54]. In Appendix C, we show the existence and uniqueness of the solution.

Since \(\beta(t)=\frac{2\mu}{e^{2\mu t}-1}\in L^{1}[t_{0},\infty)\) for any \(t_{0}>0\), Theorem 4.3 implies that \(\tilde{\mathbf{A}}(X(t))\nrightarrow 0\) when \(\mathbf{A}\) is merely monotone. This tells us that the optimal choice of \(\beta(t)\) for should depend on the properties of \(\mathbf{A}\). In the following section, we describe how \(\beta(t)\) can be chosen to adapt to the operator's properties.

## 7 Adaptive anchor acceleration and experiments

In this section, we present an adaptive method for choosing the anchor coefficient \(\beta\), and we theoretically and experimentally show that this choice allows the dynamics to adapt to the operator's properties. It achieves the optimal \(\mathcal{O}(1/k^{2})\)-convergence rate when \(\mathbf{A}\) is monotone and an exponential convergence rate when \(\mathbf{A}\) is furthermore \(\mu\)-strongly monotone and Lipschitz continuous.

**Theorem 7.1**.: _Suppose \(\tilde{\mathbf{A}}\) is Lipschitz continuous and monotone. Consider the anchor ODE_

\[\dot{X}=-\tilde{\mathbf{A}}(X)+\underbrace{\frac{\left\|\tilde{ \mathbf{A}}(X)\right\|^{2}}{2\left\langle\tilde{\mathbf{A}}(X),X-X_{0}\right \rangle}}_{=-\beta(t)}(X-X_{0})\] (11)

_with initial condition \(X(0)=X_{0}\) and \(\left\|\tilde{\mathbf{A}}(X_{0})\right\|\neq 0\). Suppose the solution exists and \(\dot{X}\) is continuous at \(t=0\). Moreover, suppose \(\beta\colon(0,\infty)\to\mathbb{R}\) is well-defined, i.e., no division by zero occurs in the definition of \(\beta(t)\). Then for \(t>0\) and \(X_{\star}\in\mathrm{Zer}\tilde{\mathbf{A}}\), we have \(\beta(t)>0\) and_

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2} \leq 4\beta(t)^{2}\left\|X_{0}-X_{\star}\right\|^{2}\] \[\beta(t)^{2} \leq\frac{1}{t^{2}}.\]

_If \(\tilde{\mathbf{A}}\) is furthermore \(\mu\)-strongly monotone, then for \(t>0\),_

\[\beta(t)^{2}\leq\left(\frac{\mu/2}{e^{\mu t/2}-1}\right)^{2}.\]

We provide the proof in Appendix I.1. Note that anchor coefficient (11) is chosen so that

\[\Phi(t)=\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2\beta(t)\left\langle \tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle=0.\]

So left-hand side of (8) is zero and a \(\mathcal{O}\left(\beta(t)^{2}\right)\) convergence rate is immediate. An analogous discrete-time result is shown in the following theorem.

**Theorem 7.2**.: _Let \(\mathbf{A}\) be a maximal monotone operator. Let \(x^{0}=y^{0}\in\mathbb{R}^{n}\). Consider_

\[x^{k} =\mathbf{J}_{\mathbf{A}}y^{k-1}\] \[y^{k} =(1-\beta_{k})(2x^{k}-y^{k-1})+\beta_{k}x^{0}\]

_with_

\[\beta_{k}=\begin{cases}\frac{\|\tilde{\mathbf{A}}x^{k}\|^{2}}{- \langle\tilde{\mathbf{A}}x^{k},\,x^{k}-x^{0}\rangle+\|\tilde{\mathbf{A}}x^{k} \|^{2}}&\text{if }\|\tilde{\mathbf{A}}x^{k}\|^{2}\neq 0\\ 0&\text{if }\|\tilde{\mathbf{A}}x^{k}\|^{2}=0,\end{cases}\]

_for \(k=1,2,\dots\), where \(\tilde{\mathbf{A}}x^{k}=y^{k-1}-x^{k}\)._

_Then \(\beta_{k}\geq 0\) and_

\[\left\|\tilde{\mathbf{A}}x^{k+1}\right\|^{2} \leq\beta_{k}^{2}\left\|x^{0}-x^{\star}\right\|^{2}\] \[\beta_{k}^{2} \leq\frac{1}{(k+1)^{2}}\]

_for \(k=1,2,\dots\) and \(x^{\star}\in\mathrm{Zer}\mathbf{A}\)._

_If \(\mathbf{A}\) is furthermore \(\mu\)-strongly monotone and \(L\)-Lipschitz continuous, then for \(k=1,2,\dots\),_

\[\beta_{k}^{2}\leq\left(\frac{\mu/(1+L^{2})}{\left(1+\mu/(1+L^{2})\right)^{k}-1 +\mu/(1+L^{2})}\right)^{2}.\]For the monotone setup, the rate \(\left\|\tilde{\mathbf{A}}x^{k+1}\right\|^{2}\leq\frac{1}{(k+1)^{2}}\left\|x^{0}-x ^{\star}\right\|^{2}\) matches the exact optimal rate of APPM [54]. In the limit \(\mu\to 0\), the result for the \(\mu\)-strongly monotone case reduces to the result for the monotone case. We provide the proof and details of Theorem 7.2 in Appendix I.3.

The method of Theorem 7.2 is a discrete-time counterpart of the ODE of (11). The extra term \(\|\tilde{\mathbf{A}}x^{k}\|^{2}\) of the denominator in the definition of \(\beta_{k}\) vanishes in the continuous-time limit. We provide further details in Appendix I.2. Analogous to the continuous-time case, a key property of the discrete-time adaptive method is that the counterpart of \(\Phi(t)\) is kept nonpositive. In the proof of Lemma I.3, the fact \(\beta_{k}<1\) plays the key role in proving this property. The extra term \(\|\tilde{\mathbf{A}}x^{k}\|^{2}\) in the denominator and the fact that \(\langle\tilde{\mathbf{A}}x^{k},\,x^{k}-x^{0}\rangle<0\) when \(\|\tilde{\mathbf{A}}x^{k}\|^{2}\neq 0\) leads to \(\beta_{k}<1\).

### Experiment details

We now show an experiment with the method of Theorem 7.2 applied to a decentralized compressed sensing problem Shi et al. [63]. We assume that we have the measurement \(b_{i}=A_{(i)}x+e_{i}\), where \(A_{(i)}\) is a measurement matrix available for each local agent \(i\), \(x\) is an unknown shared signal we hope to recover, and \(e_{i}\) is an error in measurement. We solve this problem in a decentralized manner in which the local agents keep their measurements private and only communicate with their neighbors.

As in Shi et al. [63], we formulate the problem into an unconstrained \(\ell_{1}\)-regularized least squares problem

\[\underset{x\in\mathbb{R}^{d}}{\text{minimize}} \frac{1}{n}\sum_{i=1}^{n}\left\{\frac{1}{2}\|A_{(i)}x-b_{i}\|^{2 }+\rho\|x\|_{1}\right\},\]

and apply PG-EXTRA. We compare vanilla PG-EXTRA with the various anchored versions of PG-EXTRA with \(\beta_{k}\) as in Theorem 5.1 and Theorem 7.2. We show the results in Figure 2. Further details of the experiment are provided in Appendix J.

## 8 Conclusion

This work introduces a continuous-time model of anchor acceleration, the anchor ODE \(\dot{X}\in-\mathbf{A}(X)-\beta(t)(X-X_{0})\). We characterize the convergence rate as a function of \(\beta(t)\) and thereby obtain insight into the anchor acceleration mechanism. Finally, inspired by the continuous-time analyses, we present an adaptive method and establish its effectiveness through theoretical analyses and experiments.

Prior work analyzing continuous-time models of Nesterov acceleration had inspired various follow-up research, such as analyses based on Lagrangian and Hamiltonian mechanics [71, 72, 26], high-resolution ODE model [62], and continuized framework [29]. Carrying out similar analyses for the anchor ODE are interesting directions of future work.

Figure 2: (Left) Network graph. (Right) Squared \(M\)-norm \(\|\tilde{\mathbf{A}}x^{k}\|_{M}^{2}\) vs. \(k\). Halpern corresponds to the method in Theorem 5.1, we use \(p=1.5\) and \(\gamma=2.0\).

## Acknowledgments and Disclosure of Funding

This work was supported by the Samsung Science and Technology Foundation (Project Number SSTF-BA2101-02). We thank Jaeyeon Kim for providing valuable feedback. We thank Hangjun Cho for the helpful discussions on well-posedness of ODEs. We also thank anonymous reviewers for the constructive comments.

## References

* [1] J. K. Alcala, Y. T. Chow, and M. Sunkula. Moving anchor extragradient methods for smooth structured minimax problems. _arXiv:2308.12359_, 2023.
* [2] V. Apidopoulos, J.-F. Aujol, and C. Dossal. The differential inclusion modeling FISTA algorithm and optimality of convergence rate in the case \(b\leq 3\). _SIAM Journal on Optimization_, 28(1):551-574, 2018.
* [3] H. Attouch and A. Cabot. Asymptotic stabilization of inertial gradient dynamics with time-dependent viscosity. _Journal of Differential Equations_, 263(9):5412-5458, 2017.
* [4] H. Attouch and S. C. Laszlo. Newton-like inertial dynamics and proximal algorithms governed by maximally monotone operators. _SIAM Journal on Optimization_, 30(4):3252-3283, 2020.
* [5] H. Attouch and S. C. Laszlo. Convex optimization via inertial algorithms with vanishing Tikhonov regularization: Fast convergence to the minimum norm solution. _arXiv:2104.11987_, 2021.
* [6] H. Attouch and J. Peypouquet. Convergence of inertial dynamics and proximal algorithms governed by maximally monotone operators. _Mathematical Programming_, 174(1):391-432, 2019.
* [7] H. Attouch, Z. Chbani, J. Peypouquet, and P. Redont. Fast convergence of inertial dynamics and algorithms with asymptotic vanishing viscosity. _Mathematical Programming_, 168(1):123-175, 2018.
* [8] H. Attouch, Z. Chbani, and H. Riahi. Combining fast inertial dynamics for convex optimization with Tikhonov regularization. _Journal of Mathematical Analysis and Applications_, 457(2):1065-1094, 2018.
* [9] H. Attouch, Z. Chbani, and H. Riahi. Rate of convergence of the Nesterov accelerated gradient method in the subcritical case \(\alpha\leq 3\). _ESAIM: Control, Optimisation and Calculus of Variations_, 25:2, 2019.
* [10] H. Attouch, Z. Chbani, J. Fadili, and H. Riahi. Convergence of iterates for first-order optimization algorithms with inertia and Hessian driven damping. _Optimization_, pages 1-40, 2021.
* [11] J.-P. Aubin and A. Cellina. _Differential Inclusions_. Springer, 1984.
* [12] J.-F. Aujol, C. Dossal, and A. Rondepierre. Optimal convergence rates for Nesterov acceleration. _SIAM Journal on Optimization_, 29(4):3131-3153, 2019.
* [13] J.-B. Baillon and R. E. Bruck. Optimal rates of asymptotic regularity for averaged nonexpansive mappings. _Fixed Point Theory and Applications_, 128:27-66, 1992.
* [14] S. Banach. Sur les operations dans les ensembles abstraits et leur application aux equations integrales. _Fundamenta Mathematicae_, 3(1):133-181, 1922.
* [15] H. H. Bauschke and P. L. Combettes. _Convex Analysis and Monotone Operator Theory in Hilbert Spaces_. Springer International Publishing, second edition, 2017.
* [16] R. I. Bot and E. R. Csetnek. A second-order dynamical system with Hessian-driven damping and penalty term associated to variational inequalities. _Optimization_, 68(7):1265-1277, 2019.

* [17] R. I. Bot and D. A. Hulett. Second order splitting dynamics with vanishing damping for additively structured monotone inclusions. _Journal of Dynamics and Differential Equations_, 2022.
* [18] R. I. Bot, E. R. Csetnek, and S. C. Laszlo. A second-order dynamical approach with variable damping to nonconvex smooth minimization. _Applicable Analysis_, 99(3):361-378, 2020.
* [19] R. I. Bot, E. R. Csetnek, and S. C. Laszlo. Tikhonov regularization of a second order dynamical system with Hessian driven damping. _Mathematical Programming_, 189(1):151-186, 2021.
* [20] R. I. Bot, E. R. Csetnek, and D.-K. Nguyen. Fast OGDA in continuous and discrete time. _arXiv:2203.10947_, 2022.
* [21] M. Bravo and R. Cominetti. Sharp convergence rates for averaged nonexpansive maps. _Israel Journal of Mathematics_, 227:163-188, 2018.
* [22] A.-L. Cauchy. Methode generale pour la resolution des systemes d'equations simultanees. _Comptes Rendus Hebdomadaires des Seances de l'Academie des Sciences_, 25:536-538, 1847.
* [23] R. Cominetti, J. A. Soto, and J. Vaisman. On the rate of convergence of Krasnosel'skii-Mann iterations and their connection with sums of Bernoulli. _Israel Journal of Mathematics_, 199(2):757-772, 2014.
* [24] C. Daskalakis, A. Ilyas, V. Syrgkanis, and H. Zeng. Training GANs with optimism. _International Conference on Learning Representations_, 2018.
* [25] J. Diakonikolas. Halpern iteration for near-optimal and parameter-free monotone inclusion and strong solutions to variational inequalities. _Conference on Learning Theory_, 2020.
* [26] J. Diakonikolas and M. I. Jordan. Generalized momentum-based methods: A Hamiltonian perspective. _SIAM Journal on Optimization_, 31(1):915-944, 2021.
* [27] Y. Drori. The exact information-based complexity of smooth convex minimization. _Journal of Complexity_, 39:1-16, 2017.
* [28] J. Eckstein and D. P. Bertsekas. On the Douglas--Rachford splitting method and the proximal point algorithm for maximal monotone operators. _Mathematical Programming_, 55(1):293-318, 1992.
* [29] M. Even, R. Berthier, F. Bach, N. Flammarion, H. Hendrikx, P. Gaillard, L. Massoulie, and A. Taylor. Continuized accelerations of deterministic and stochastic gradient descents, and of gossip algorithms. _Neural Information Processing Systems_, 2021.
* [30] George J. Minty. Monotone (nonlinear) operators in Hilbert space. _Duke Mathematical Journal_, 29(3):341-346, 1962.
* [31] I. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. _arXiv:1701.00160_, 2016.
* [32] E. Gorbunov, N. Loizou, and G. Gidel. Extragradient method: \(O(1/K)\) last-iterate convergence for monotone variational inequalities and connections with cocoercivity. _International Conference on Artificial Intelligence and Statistics_, 2022.
* [33] G. Gu and J. Yang. Tight sublinear convergence rate of the proximal point algorithm for maximal monotone inclusion problems. _SIAM Journal on Optimization_, 30(3):1905-1921, 2020.
* [34] B. Halpern. Fixed points of nonexpanding maps. _Bulletin of the American Mathematical Society_, 73(6):957-961, 1967.
* [35] D. Kim. Accelerated proximal point method for maximally monotone operators. _Mathematical Programming_, 190(1-2):57-87, 2021.
* [36] D. Kim and J. A. Fessler. Optimized first-order methods for smooth convex minimization. _Mathematical Programming_, 159(1-2):81-107, 2016.

* [37] D. Kim and J. A. Fessler. Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions. _Journal of Optimization Theory and Applications_, 188(1):192-219, 2021.
* [38] J. Kim and I. Yang. Unifying Nesterov's accelerated gradient methods for convex and strongly convex objective functions. _International Conference on Machine Learning_, 202, 2023.
* [39] U. Kohlenbach. On quantitative versions of theorems due to F. E. Browder and R. Wittmann. _Advances in Mathematics_, 226(3):2764-2795, 2011.
* [40] M. A. Krasnosel'skii. Two remarks on the method of successive approximations. _Uspekhi Matematicheskikh Nauk_, 10(1):123-127, 1955.
* [41] J. Lee and E. K. Ryu. Accelerating value iteration with anchoring. _Neural Information Processing Systems_, 2023.
* [42] S. Lee and D. Kim. Fast extra gradient methods for smooth structured nonconvex-nonconcave minimax problems. _Neural Information Processing Systems_, 2021.
* [43] L. Leustean. Rates of asymptotic regularity for Halpern iterations of nonexpansive mappings. _Journal of Universal Computer Science_, 13(11):1680-1691, 2007.
* [44] J. Liang, J. Fadili, and G. Peyre. Convergence rates with inexact non-expansive operators. _Mathematical Programming_, 159(1):403-434, 2016.
* [45] F. Lieder. On the convergence rate of the Halpern-iteration. _Optimization Letters_, 15(2):405-418, 2021.
* [46] T. Lin and M. I. Jordan. Monotone Inclusions, Acceleration, and Closed-Loop Control. _Mathematics of Operations Research_, 2023.
* [47] W. R. Mann. Mean value methods in iteration. _Proceedings of the American Mathematical Society_, 4(3):506-510, 1953.
* [48] B. Martinet. Regularisation d'inequations variationnelles par approximations successives. _Revue Francaise d'Informatique et de Recherche Operationnelle, Serie Rouge_, 4(3):154-158, 1970.
* [49] B. Martinet. _Algorithmes Pour La Resolution de Problemes d'optimisation et de Minimax_. PhD thesis, Universite Scientifique et Medicale de Grenoble, 1972.
* [50] S.-Y. Matsushita. On the convergence rate of the Krasnosel'skii-Mann iteration. _Bulletin of the Australian Mathematical Society_, 96(1):162-170, 2017.
* [51] Y. Nesterov. A method of solving a convex programming problem with convergence rate \(O(1/k^{2})\). _Doklady Akademii Nauk SSSR_, 269(3):543-547, 1983.
* [52] Y. Nesterov. _Introductory Lectures on Convex Optimization: A Basic Course_. Springer, 2004.
* [53] C. Park, J. Park, and E. K. Ryu. Factor-\(\sqrt{2}\) acceleration of accelerated gradient methods. _Applied Mathematics and Optimization_, 2022.
* [54] J. Park and E. K. Ryu. Exact optimal accelerated complexity for fixed-point iterations. _International Conference on Machine Learning_, 2022.
* [55] J. Park and E. K. Ryu. Accelerated infeasibility detection of constrained optimization and fixed-point iterations. _International Conference on Machine Learning_, 2023.
* [56] L. D. Popov. A modification of the Arrow-Hurwicz method for search of saddle points. _Mathematical Notes of the Academy of Sciences of the USSR_, 28(5):845-848, 1980.
* [57] A. Rakhlin and K. Sridharan. Online learning with predictable sequences. _Conference on Learning Theory_, 2013.
* [58] E. K. Ryu and W. Yin. _Large-Scale Convex Optimization via Monotone Operators_. Cambridge University Press, 2022.

* [59] E. K. Ryu, K. Yuan, and W. Yin. ODE analysis of stochastic gradient methods with optimism and anchoring for minimax problems and GANs. _arXiv:1905.10899_, 2019.
* [60] S. Sabach and S. Shtern. A first order method for solving convex bilevel optimization problems. _SIAM Journal on Optimization_, 27(2):640-660, 2017.
* [61] A. Salim, L. Condat, D. Kovalev, and P. Richtarik. An optimal algorithm for strongly convex minimization under affine constraints. _AISTATS_, 2022.
* [62] B. Shi, S. S. Du, M. I. Jordan, and W. J. Su. Understanding the acceleration phenomenon via high-resolution differential equations. _Mathematical Programming_, 2021.
* [63] W. Shi, Q. Ling, G. Wu, and W. Yin. A proximal gradient algorithm for decentralized composite optimization. _IEEE Transactions on Signal Processing_, 63(22):6013-6023, 2015.
* proximal point algorithm using the enlargement of a maximal monotone operator. _Set-Valued Analysis_, 7(4):323-345, 1999.
* [65] W. Su, S. Boyd, and E. J. Candes. A differential equation for modeling Nesterov's accelerated gradient method: Theory and insights. _Neural Information Processing Systems_, 2014.
* [66] W. Su, S. Boyd, and E. J. Candes. A differential equation for modeling Nesterov's accelerated gradient method: Theory and insights. _Journal of Machine Learning Research_, 17(153):1-43, 2016.
* [67] J. J. Suh, G. Roh, and E. K. Ryu. Continuous-time analysis of AGM via conservation laws in dilated coordinate systems. _International Conference on Machine Learning_, 2022.
* [68] A. Taylor and Y. Drori. An optimal gradient method for smooth strongly convex minimization. _Mathematical Programming_, 2022.
* [69] Q. Tran-Dinh and Y. Luo. Halpern-type accelerated and splitting algorithms for monotone inclusions. _arXiv:2110.08150_, 2021.
* [70] B. Van Scoy, R. A. Freeman, and K. M. Lynch. The fastest known globally convergent first-order method for minimizing strongly convex functions. _IEEE Control Systems Letters_, 2(1):49-54, 2018.
* [71] A. Wibisono, A. C. Wilson, and M. I. Jordan. A variational perspective on accelerated methods in optimization. _Proceedings of the National Academy of Sciences_, 113(47):E7351-E7358, 2016.
* [72] A. C. Wilson, B. Recht, and M. I. Jordan. A Lyapunov analysis of accelerated methods in optimization. _Journal of Machine Learning Research_, 22(113):1-34, 2021.
* [73] R. Wittmann. Approximation of fixed points of nonexpansive mappings. _Archiv der Mathematik_, 58(5):486-491, 1992.
* [74] T. Wu, K. Yuan, Q. Ling, W. Yin, and A. H. Sayed. Decentralized consensus optimization with asynchrony and delays. _IEEE Transactions on Signal and Information Processing over Networks_, 4(2):293-307, 2018.
* [75] T. Yoon and E. K. Ryu. Accelerated algorithms for smooth convex-concave minimax problems with \(\mathcal{O}(1/k^{2})\) rate on squared gradient norm. _International Conference on Machine Learning_, 2021.
* [76] T. Yoon and E. K. Ryu. Accelerated minimax algorithms flock together. _arXiv:2205.11093_, 2022.

Prior work

Acceleration for smooth convex function in discrete setting.There had been rich amount of research on acceleration about smooth convex functions. Nesterov [51] introduced accelerated gradient method (AGM), which has a faster \(\mathcal{O}(1/k^{2})\) rate than \(\mathcal{O}(1/k)\) rate of gradient descent [22] in reducing the function value. Optimized gradient method (OGM) [36] improved AGM's rate by a constant factor, and is proven to be optimal [27]. For smooth strongly convex setup, strongly convex AGM [52] achieves an accelerated rate, and further improvements were studied [70, 53, 68, 61]. Recently, OGM-G [37] was introduced as an accelerated method reducing squared gradient magnitude for smooth convex minimization.

Acceleration for smooth convex function in continuous setting.Continuous-time analysis of Nesterov acceleration has been thoroughly studied as well. Su et al. [65, 66] introduced an ODE model of AGM \(\dot{X}(t)+\frac{r}{t}X+\nabla f(X)=0\), providing \(f(X(t))-f_{\star}\in\mathcal{O}\left(1/t^{2}\right)\) rate for \(r\geq 3\). Attouch et al. [7] improved the constant of bound for \(r>3\) and proved convergence of the trajectories. Attouch et al. [9] achieved \(\mathcal{O}\left(1/t^{-2r/3}\right)\) rate for \(0<r<3\). Apidopoulos et al. [2] generalized their results to differential inclusion with non-differentiable convex function. Furthermore, wide range of variations of AGM ODE has been studied [3, 8, 12, 16, 18, 5, 10, 19]. Also, applications to monotone inclusion problem were studied by Attouch and Peypouquet [6], Attouch and Laszlo [4], Bot and Hulett [17].

Motivated from above continuous-time analysis for accelerated methods, tools analyzing ODEs have further developed. Wibisono et al. [71], Wilson et al. [72] and Kim and Yang [38] adopted Lagrangian mechanics and introduced first, second, unified Bregman Lagrangian to provide unified analysis for generalized family of ODE, where the latter provided analysis for strongly convex AGM. Systemical approach to obtain Lyapunov functions exploiting Hamiltonian mechanics [26] and dilated coordinate system [67] were proposed, and analysis of OGM-G was provided by dilated coordinate framework. Different forms of continuous-time models such as high-resolution ODE [62] and continuized framework [29] were developed.

On the other hand, another type of acceleration called _anchor acceleration_ recently gained attention. As Yoon and Ryu [76] focused, many recently discovered accelerated methods for both minimax optimization and fixed-point problems are based on anchor acceleration. More recently, practical applications of anchor acceleration to detecting infeasibility for constrained optimization problems [55], and accelerating value iteration for dynamic programming and reinforcement learning [41] were introduced as well.

Fixed-point problem.The history of studies on fixed-point problem dates back to the work of Banach [14], which established that the Picard iteration with contractive operator is convergent. Kransnosel'skii-Mann iteration (KM) [40, 47] was introduced, which is a generalization of Picard iteration. Convergence of KM iteration with general nonexpansive operators was proven by Martinet [49]. For iteration of Halpern [34], convergence with wide choice of parameter were shown by Wittmann [73].

The squared norm \(\left\|y_{k}-\mathrm{T}y_{k}\right\|^{2}\) of fixed-point residual is a common error measure for fixed-point problems. KM iteration was shown to exhibit \(\mathcal{O}(1/k)\) rate [23, 44, 21] and \(o(1/k)\)-rate [13, 50]. For Halpern iteration, \(\mathcal{O}(1/(\log k)^{2})\)-rate was established by Leustean [43], then improve to \(\mathcal{O}(1/k)\) rate by Kohlenbach [39]. First accelerated \(\mathcal{O}(1/k^{2})\) rate was achieved by Sabach and Shtern [60] and the constant was improved by Lieder [45] by a factor of 16.

It is known that there is an equivalence between solving fixed-point problem and solving monotone inclusion problem [30, 28, 54]. Proximal point method (PPM) [48] achieves \(\mathcal{O}\left(1/k\right)\)-rate in terms of \(\left\|\tilde{\mathbf{A}}x_{k}\right\|^{2}\)[33]. Accelerated proximal point method (APPM) [35] improved the rate to accelerated \(\mathcal{O}\left(1/k^{2}\right)\)-rate. Park and Ryu [54] showed APPM is exactly optimal method for this problem and provided exactly optimal method for \(\mu\)-strongly monotone operator named OS-PPM, which achieved \(\mathcal{O}\left(1/e^{4\mu k}\right)\) rate. The optimal methods APPM and OS-PPM are based on anchor acceleration [76].

Minimax problems.Minimax optimization problem of the form \(\min_{x}\max_{y}\mathbf{L}(x,y)\) have recently gained attention in machine learning society. One of the commonly considered theoretical setting is smooth convex-concave setup, with squared gradient norm as error measure. In terms of \(\left\|\partial\mathbf{L}(x,y)\right\|^{2}\), classical EG [64] and OG [56, 57, 24] was shown to achieved \(\mathcal{O}\left(1/k\right)\)-rate [32]. SGDA [59] achieved \(\mathcal{O}\left(1/k^{2-2p}\right)\) rate for \(p>1/2\) with introducing the term _anchor_. With introducing a parameter-free Halpern type method, Diakonikolas [25] achieved \(\mathcal{O}(\log k/k^{2})\). Recently, EAG [75] first achieved accelerated rate \(\mathcal{O}(1/k^{2})\) with anchor acceleration, followed by FEG [42], anchored Popov's scheme [69] and moving anchor methods [1]. Fast ODGA [20] also achieved accelerated \(o(1/k^{2})\) rate. For \(\partial\mathbf{L}\) is furthermore strongly monotone with condition number \(\kappa\), SM-EAG+ [76] achieved accelerated \(\mathcal{O}\left(1/e^{2k\kappa}\right)\) rate.

However, continuous-time analysis for anchor acceleration is, to the best of our knowledge, insufficient. Continuous-time analyses of acceleration for monotone inclusion problem were studied by Bot et al. [20], Lin and Jordan [46], but they did not consider anchor acceleration. Ryu et al. [59] considered continuous-time analysis of anchor acceleration, but only with limited cases \(\dot{X}(t)=-\mathbf{A}(X(t))-\frac{\gamma}{t}(X-X_{0})\) for \(\gamma\geq 1\). In this paper, we provide a unified continuous-time analysis for anchor acceleration with generalized anchor coefficient.

Proof of Theorem 2.2

### Proof of uniqueness

Proof of uniqueness is immediate from Lemma 2.3, we first prove the lemma.

Proof of Lemma 2.3.: It is trivial for \(t=0\), so we may assume \(t>0\).

By monotonicity of \(\mathsf{A}\) and Young's inequality, we get the following inequality.

\[\frac{d}{dt}\left\|X(t)-Y(t)\right\|^{2} =2\left\langle\dot{X}(t)-\dot{Y}(t),X(t)-Y(t)\right\rangle\] \[=-2\left\langle\tilde{\mathsf{A}}(X(t))-\tilde{\mathsf{A}}(Y(t)) +\beta(t)(X(t)-Y(t))-\beta(t)(X_{0}-Y_{0}),X(t)-Y(t)\right\rangle\] \[\leq-2\beta(t)\left\|X(t)-Y(t)\right\|^{2}+2\beta(t)\left\langle X _{0}-Y_{0},X(t)-Y(t)\right\rangle\] \[\leq-2\beta(t)\left\|X(t)-Y(t)\right\|^{2}+\beta(t)\left(\left\| X_{0}-Y_{0}\right\|^{2}+\left\|X(t)-Y(t)\right\|^{2}\right)\] \[=-\beta(t)\left\|X(t)-Y(t)\right\|^{2}+\beta(t)\left\|X_{0}-Y_{ 0}\right\|^{2}.\]

Now define \(C(t)=e^{\int_{v}^{t}\beta(s)ds}\) for some \(v>0\), then we see \(\dot{C}(t)=C(t)\beta(t)\). Moving \(-\beta(t)\left\|X(t)-Y(t)\right\|^{2}\) to the left hand side and multiplying both sides by \(C(t)\), we have

\[\frac{d}{dt}\left(C(t)\left\|X(t)-Y(t)\right\|^{2}\right) =C(t)\frac{d}{dt}\left\|X(t)-Y(t)\right\|^{2}+C(t)\beta(t)\left\| X(t)-Y(t)\right\|^{2}\] \[\leq C(t)\beta(t)\left\|X_{0}-Y_{0}\right\|^{2}=\frac{d}{dt}C(t) \left\|X_{0}-Y_{0}\right\|^{2}\]

Integrating both sides from \(\epsilon>0\) to \(t\) we have

\[C(t)\left\|X(t)-Y(t)\right\|^{2}-C(\epsilon)\left\|X(\epsilon)-Y(\epsilon) \right\|^{2}\leq C(t)\left\|X_{0}-Y_{0}\right\|^{2}-C(\epsilon)\left\|X_{0}-Y _{0}\right\|^{2}.\]

As \(C\) is nonnegative and nondecreasing, \(\lim_{\epsilon\to 0+}C(\epsilon)\) exists. Taking limit \(\epsilon\to 0+\) both sides we have

\[C(t)\left\|X(t)-Y(t)\right\|^{2}\leq C(t)\left\|X_{0}-Y_{0}\right\|^{2}.\]

Finally, dividing both sides by \(C(t)\) we conclude

\[\left\|X(t)-Y(t)\right\|^{2}\leq\left\|X_{0}-Y_{0}\right\|^{2}.\]

Proof for uniqueness can be done to generalized case (3).

**Theorem B.1** (Uniqueness of solutions).: _If the solution for (3) exists, it is unique._

Proof.: Suppose \(X_{1},X_{2}\) are solutions of (3) with same initial value \(X_{0}\). By Lemma 2.3, we have

\[\left\|X_{1}(t)-X_{2}(t)\right\|\leq\left\|X_{0}-X_{0}\right\|=0\]

Therefore \(X_{1}(t)=X_{2}(t)\) for all \(t\in[0,\infty)\), we get the desired result. 

As (6) is special case of (3), uniqueness proof for Theorem 2.2 follows from Theorem B.1.

### Proof of existence

The proof of existence needs tedious work due to the singularity of \(\frac{\gamma}{t^{p}}\) at \(t=0\), we provide our proof through subsections. Before we start, we provide a short outline of the proof. The proof is basically based on the proof provided in [65] and [11].

We first prove the case \(\mathsf{A}\) is Lipschitz. The differential inclusion becomes ODE when \(\mathsf{A}\) is Lipschitz, we can adopt similar argument done in [65]. We consider series of ODEs Lipschitz with respect to \(X\), approximating \(\dot{X}(t)=-\mathsf{A}(X(t))-\frac{\gamma}{t^{p}}(X_{0}-X(t))\). The approximated ODEs have solutions by classical theory of ODE, we obtain the true solution by considering proper subsequence of solutions.

As the solution for Lipschitz \(\mathsf{A}\) is obtained, we can adopt similar argument done in [11]. We first consider solution \(X_{\lambda}\) with Yosida approximation \(\mathsf{A}_{\lambda}=\frac{1}{\lambda}(\mathds{I}-(\mathds{I}+\lambda\mathsf{ A})^{-1})\), which is an approximation of \(\mathsf{A}\) that is Lipschitz continuous. Then we can obtain a subsequence \(X_{\lambda_{n}}\) converging to original differential inclusion.

#### b.2.1 Existence proof for Lipschitz \(\mathbf{A}\)

Since we will approximate \(\mathbf{A}\) with Lipschitz continuous functions, we first consider the ODE with Lipschitz continuous \(\mathbf{A}\).

**Theorem B.2**.: _(Existence of solution for Lipschitz \(\mathbf{A}\)) Suppose \(\tilde{A}:\mathbb{R}^{n}\to\mathbb{R}^{n}\) is L-Lipschitz continuous function. Consider the differential equation, with initial value condition \(\tilde{X}(0)=X_{0}\in dom(\tilde{A})\),_

\[\dot{\tilde{X}}(t)=-\tilde{A}(\tilde{X}(t))-\frac{\gamma}{t^{p}}(\tilde{X}(t)- X_{0}).\] (12)

_where \(\gamma,p>0\). Then there exists a unique solution \(\tilde{X}\in\mathcal{C}^{1}([0,\infty),\mathbb{R}^{n})\) that satisfies (12) for all \(t\in(0,\infty)\). Moreover, for \(\dot{\tilde{X}}(0)\) defined by \(\dot{\tilde{X}}(0)=\lim_{t\to 0+}\frac{\tilde{X}(t)-X_{0}}{t}\), following is true_

\[\dot{\tilde{X}}(0)=\begin{cases}-\tilde{A}(X_{0})&\text{if}\ \ 0<p<1\\ -\frac{1}{\gamma+1}\tilde{A}(X_{0})&\text{if}\ \ p=1\\ 0&\text{if}\ \ p>1.\end{cases}\]

The proof need some preparation. We will think of approximated solutions \(\tilde{X}_{\delta}\), obtain a sequence that converges to solution \(\tilde{X}\). Thus we first define \(\tilde{X}_{\delta}\). From now, we will denote \(\tilde{A}\) as a \(L\)-Lipschitz monotone function.

**Definition B.3**.: Let \(0<\delta<1\). Consider

\[\dot{\tilde{X}}_{\delta}(t)=\begin{cases}-\tilde{A}(\tilde{X}_{\delta}(t))- \frac{\gamma}{\delta t}(\tilde{X}_{\delta}(t)-X_{0})&0\leq t<\delta\\ -\tilde{A}(\tilde{X}_{\delta}(t))-\frac{\gamma}{t^{p}}(\tilde{X}_{\delta}(t)- X_{0})&t>\delta\end{cases}\] (13)

Since right hand side above is \(\left(L+\frac{\gamma}{\delta^{p}}\right)\)-Lipschitz with respect to \(\tilde{X}_{\delta}\), the solution uniquely exists by classical ODE theory. Define the solution as \(\tilde{X}_{\delta}\). Then for positive sequence \(\{\delta_{m}\}_{m\in\mathbb{N}}\) such that \(\delta_{m}<1\) and \(\lim_{m\to\infty}\delta_{m}=0\), consider sequence \(\left\{\tilde{X}_{\delta_{m}}\right\}_{m\in\mathbb{N}}\).

Before we start, we prove a useful lemma we will widely use for the cases that operator is Lipschitz continuous.

**Lemma B.4**.: _Let \(\tilde{A}:\mathbb{R}^{n}\to\mathbb{R}^{n}\) a Lipschitz continuous function. Suppose \(\beta\colon D\to[0,\infty)\) be a continuous function with \(D\subset[0,\infty)\). Consider differential equation_

\[\dot{\tilde{X}}=-\tilde{A}(\tilde{X})-\beta(t)(\tilde{X}-X_{0}),\]

_with \(X_{0}\in\mathbb{R}^{n}\). Let \(\tilde{X}\colon D\to\mathbb{R}^{n}\) be a differentiable curve that satisfies above equation for \(t\in D\). Then for all \(0\leq a<b\) such that \([a,b]\subset D\), \(\tilde{X}\) and the composition \(\tilde{A}\circ\tilde{X}:[a,b]\to\mathbb{R}^{n}\) are is Lipschitz continuous._

_Moreover if \(\dot{\beta}(t)\) is well-defined and bounded for almost all \(t\in[a,b]\), then \(\dot{\tilde{X}}\) is Lipschitz continuous in \([a,b]\). Thus if \(\beta\) is twice differentiable, \(\dot{\tilde{X}}\) is Lipschitz continuous._

_As Lipschitz continuous functions, \(\tilde{X}\), \(\tilde{A}\circ\tilde{X}\) and \(\dot{\tilde{X}}\) are absolutely continuous functions._

Proof.: We first prove \(\tilde{X}\) is Lipschitz continuous. As \(\tilde{A},\tilde{X},\beta\) are continuous in \([a,b]\) we see \(\tilde{A}(\tilde{X}(t))+\beta(t)(\tilde{X}(t)-X_{0})\) is continuous in \([a,b]\), thus

\[M_{1}=\max_{t\in[a,b]}\left\|\dot{X}(t)\right\|=\max_{t\in[a,b]}\left\|\tilde{ A}(\tilde{X}(t))+\beta(t)(\tilde{X}(t)-X_{0})\right\|\]

exists. Since its derivative is bounded by \(M_{1}<\infty\), we have \(\tilde{X}\) is \(M_{1}\)-Lipshitz continuous. As composition of two Lipschitz continuous functions, \(\tilde{A}\circ\tilde{X}\) is Lipschitz continuous.

First observe if \(\beta\) is twice differentiable, \(\dot{\beta}\) is bounded in \([a,b]\) as it is continuous. Now if \(\dot{\beta}(t)\) is bounded for \(t\in[a,b]\), i.e. \(\left|\dot{\beta}(t)\right|\leq M_{2}\) for some \(M_{2}>0\), for \(M_{3}=\max_{t\in[a,b]}|\beta(t)|\) we have

\[\left\|\frac{d}{dt}\left(\beta(t)(\tilde{X}(t)-X_{0})\right)\right\|=\left\| \dot{\beta}(t)(\tilde{X}(t)-X_{0})\right\|+\left\|\beta(t)\dot{\tilde{X}}(t) \right\|\leq M_{2}M_{1}\left|b-a\right|+M_{3}M_{1}\]

Therefore \(\beta(t)(\tilde{X}(t)-X_{0})\) is \((M_{2}M_{1}\left|b-a\right|+M_{3}M_{1})\)-Lipschitz continuous. Thus as a sum of Lipschitz continuous functions, \(\dot{\tilde{X}}\) is Lipschitz continuous.

We will show for every \(T>0\), the set of derivatives \(\left\{\dot{\tilde{X}}_{\delta}\mid\delta\in(0,1)\right\}\) is uniformly bounded on \([0,T]\) and \(\left\{\tilde{X}_{\delta_{m}}\right\}_{m\in\mathbb{N}}\) converges uniformly on \([0,T]\). We first prove the boundedness of derivatives. To do so, we first prove a useful lemma.

**Lemma B.5**.: _For \(0<a<b\), suppose \(\tilde{X}:[a,b]\to\mathbb{R}^{n}\) satisfies ODE (12) for \(t\in[a,b]\). Define \(\tilde{U}:[a,b]\to\mathbb{R}\) as_

\[\tilde{U}_{1}(t) =\left\|\dot{\tilde{X}}(t)\right\|^{2}+\frac{\gamma p}{t^{p+1}} \left\|\tilde{X}(t)-X_{0}\right\|^{2}\] \[\tilde{U}_{2}(t) =\frac{1}{t^{p-1}}\left\|\dot{\tilde{X}}(t)\right\|^{2}+\frac{ \gamma p}{t^{2p}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\]

_Then \(\tilde{U}_{1}\) is nonincreasing if for \(0<p\leq 1\), and \(\tilde{U}_{2}\) is nonincreasing for \(p>1\)._

Proof.: From Lemma B.4 we can check \(\tilde{U}_{1}\) and \(\tilde{U}_{2}\) are absolutely continuous in \([a,b]\). Therefore it is enough to check the derivative is nonpositive for almost all \(t\). From Lemma B.4, we can differentiate (12) both sides. Thus for almost all \(t\) we have

\[\ddot{\tilde{X}}(t)=-\frac{d}{dt}\tilde{A}(\tilde{X}(t))+\frac{\gamma p}{t^{p+ 1}}(\tilde{X}(t)-X_{0})-\frac{\gamma}{t^{p}}\dot{\tilde{X}}(t).\]

Recall from monotonicity of \(\tilde{A}\) and (1), we know \(\left\langle\dot{\tilde{X}}(t),\frac{d}{dt}\tilde{A}(\tilde{X}(t))\right\rangle\geq 0\) for almost all \(t\). Therefore for almost all \(t\),

1. \(0<p\leq 1\) \[\dot{\tilde{U}}_{1}(t) =2\left\langle\dot{\tilde{X}}(t),\ddot{\tilde{X}}(t)\right\rangle +\frac{2\gamma p}{t^{p+1}}\left\langle\dot{\tilde{X}}(t),\tilde{X}(t)-X_{0} \right\rangle-\frac{\gamma p(p+1)}{t^{p+2}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\] \[=-2\left\langle\dot{\tilde{X}}(t),\frac{d}{dt}\tilde{A}(\tilde{X }(t))\right\rangle-\frac{2\gamma}{t^{p}}\left\|\dot{\tilde{X}}(t)\right\|^{2} +\frac{2\gamma p}{t^{p+1}}\left\langle\dot{\tilde{X}}(t),\tilde{X}(t)-X_{0}\right\rangle\] \[\quad+\frac{2\gamma p}{t^{p+1}}\left\langle\dot{\tilde{X}}(t), \tilde{X}(t)-X_{0}\right\rangle-\frac{2\gamma p^{2}}{t^{p+2}}\left\|\tilde{X}( t)-X_{0}\right\|^{2}-\frac{\gamma p(1-p)}{t^{p+2}}\left\|\tilde{X}(t)-X_{0} \right\|^{2}\] \[=-2\left\langle\dot{\tilde{X}}(t),\frac{d}{dt}\tilde{A}(\tilde{X }(t))\right\rangle-\frac{2\gamma}{t^{p}}\left\|\dot{\tilde{X}}(t)-\frac{p}{t} (\tilde{X}-X_{0})\right\|^{2}-\frac{\gamma p(1-p)}{t^{p+2}}\left\|\tilde{X}(t) -X_{0}\right\|^{2}\] \[\leq 0.\]
2. \(p>1\) \[\dot{\tilde{U}}_{2}(t) =-\frac{p-1}{t^{p}}\left\|\dot{\tilde{X}}(t)\right\|^{2}+\frac{2 \gamma p}{t^{p-1}}\left\langle\dot{\tilde{X}}(t),\ddot{\tilde{X}}(t)\right\rangle +\frac{2\gamma p}{t^{2p}}\left\langle\dot{\tilde{X}}(t),\tilde{X}-X_{0}\right \rangle-\frac{2\gamma p}{t^{2p+1}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\] \[=-\frac{p-1}{t^{p}}\left\|\dot{\tilde{X}}(t)\right\|^{2}-\frac{2 }{t^{p-1}}\left\langle\dot{\tilde{X}}(t),\frac{d}{dt}\tilde{A}(\tilde{X}(t)) \right\rangle-\frac{2\gamma}{t^{2p-1}}\left\|\dot{\tilde{X}}(t)-\frac{p}{t}( \tilde{X}-X_{0})\right\|^{2}\] \[\leq 0.\]

We now are ready to prove uniform boundedness of derivatives \(\dot{\tilde{X}}_{\delta}\).

**Lemma B.6**.: _Take \(T>0\). For all \(t\in[0,T]\) and \(\delta\in(0,1)\), below inequality holds._

\[\left\|\dot{\tilde{X}}_{\delta}(t)\right\|\leq\tilde{M}_{dot}(T)=\begin{cases} \sqrt{1+\gamma p}\left\|\tilde{A}(X_{0})\right\|&0<p\leq 1\\ \sqrt{T^{p-1}\left(\frac{1}{2\gamma}+2p\left(2L^{2}+2\gamma+1\right)\right)} \left\|\tilde{A}(X_{0})\right\|&p\geq 1.\end{cases}\]Proof.: We prove first statement by considering two cases.

1. \(t\in[0,\delta]\) From Lemma B.4, we know \(\dot{\tilde{X}}_{\delta}\) is absolutely continuous and so \(\left\|\dot{\tilde{X}}_{\delta}\right\|^{2}\) is absolutely continuous as well. Differentiating (13), for almost all \(t\in(0,\delta)\) we have \[\ddot{\tilde{X}}_{\delta}=-\frac{d}{dt}\tilde{A}(\tilde{X}_{\delta}(t))-\frac{ \gamma}{\delta^{p}}\dot{\tilde{X}}_{\delta}.\] Now for almost all \(t\in(0,\delta)\), \[\frac{d}{dt}\left\|\dot{\tilde{X}}_{\delta}\right\|^{2}=2\left\langle\dot{ \tilde{X}}_{\delta},\ddot{\tilde{X}}_{\delta}\right\rangle=-2\left\langle\dot {\tilde{X}}_{\delta},\frac{d}{dt}\tilde{A}(\tilde{X}_{\delta}(t))\right\rangle- \frac{2\gamma}{\delta^{p}}\left\|\dot{\tilde{X}}_{\delta}\right\|^{2}\leq- \frac{2\gamma}{\delta^{p}}\left\|\dot{\tilde{X}}_{\delta}\right\|^{2}\leq 0.\] 1. \(0<p\leq 1\) From above we know \(\left\|\dot{\tilde{X}}_{\delta}\right\|\) is nonincreasing in \(t\in[0,\delta]\), we have \(\left\|\dot{\tilde{X}}_{\delta}(t)\right\|\leq\left\|\dot{\tilde{X}}_{\delta} (0)\right\|=\left\|\tilde{A}(X_{0})\right\|\). 2. \(p>1\) Integrating \(\frac{d}{dt}\left\|\dot{\tilde{X}}_{\delta}\right\|^{2}\leq-\frac{2\gamma}{ \delta^{p}}\left\|\dot{\tilde{X}}_{\delta}\right\|^{2}\) we have \[\left\|\dot{\tilde{X}}_{\delta}(t)\right\|^{2}-\left\|\dot{\tilde {X}}_{\delta}(0)\right\|^{2} \leq-\frac{2\gamma}{\delta^{p}}\int_{0}^{t}\left\|\dot{\tilde{X} }_{\delta}(s)\right\|^{2}ds\] \[\leq-\frac{2\gamma}{\delta^{p}}\int_{0}^{t}\left\|\dot{\tilde{X}}_{ \delta}(t)\right\|^{2}ds=-\frac{2\gamma t}{\delta^{p}}\left\|\dot{\tilde{X}}_{ \delta}(t)\right\|^{2}.\] Organizing with respect to \(\left\|\dot{\tilde{X}}_{\delta}(t)\right\|\) we have \[\left\|\dot{\tilde{X}}_{\delta}(t)\right\|\leq\frac{\left\|\dot{\tilde{X}}_{ \delta}(0)\right\|}{\sqrt{1+\frac{2\gamma t}{\delta^{p}}}}=\sqrt{\frac{\delta ^{p}}{\delta^{p}+2\gamma t}}\left\|\tilde{A}(X_{0})\right\|\leq\sqrt{\frac{ \delta^{p}}{2\gamma t}}\left\|\tilde{A}(X_{0})\right\|.\]
2. \(t\geq\delta\) Note for \(t>\delta\) (12) holds, we can apply Lemma B.5. 1. \(0<p\leq 1\) From (i) we know \(\left\|\dot{\tilde{X}}_{\delta}(t)\right\|\leq\left\|\tilde{A}(X_{0})\right\|\) for \(t\in[0,\delta]\). Therefore \(\left\|\dot{\tilde{X}}_{\delta}(\delta)\right\|\leq\left\|\tilde{A}(X_{0})\right\|\) and we see \[\left\|\tilde{X}_{\delta}(\delta)-X_{0}\right\|=\left\|\int_{0}^{\delta}\dot{ \tilde{X}}_{\delta}(s)\,ds\right\|\leq\int_{0}^{\delta}\left\|\dot{\tilde{X}} _{\delta}(s)\right\|ds\leq\delta\left\|\tilde{A}(X_{0})\right\|.\] From Lemma B.5 we know \(\tilde{U}_{1}(t)=\left\|\dot{\tilde{X}}_{\delta}(t)\right\|^{2}+\frac{\gamma p }{t^{p+1}}\left\|\tilde{X}_{\delta}(t)-X_{0}\right\|^{2}\) is nonincreasing. Therefore \(\left\|\dot{\tilde{X}}_{\delta}(t)\right\|^{2}\leq\tilde{U}_{1}(\delta)\) for \(t\geq\delta\). Since \(\delta<1\) we have \[\left\|\dot{\tilde{X}}_{\delta}(t)\right\| \leq\sqrt{\tilde{U}_{1}(\delta)}=\sqrt{\left\|\dot{\tilde{X}}_{ \delta}(\delta)\right\|^{2}+\frac{\gamma p}{\delta^{p+1}}\left\|\tilde{X}_{ \delta}(\delta)-X_{0}\right\|^{2}}\] \[\leq\sqrt{\left\|\tilde{A}(X_{0})\right\|^{2}+\gamma p\delta^{1-p} \left\|\tilde{A}(X_{0})\right\|^{2}}\leq\sqrt{1+\gamma p}\left\|\tilde{A}(X_{0 })\right\|.\]
3. \(p>1\) From (i) we know \(\left\|\dot{\tilde{X}}_{\delta}(t)\right\|\leq\sqrt{\frac{\delta^{p}}{2\gamma t }}\left\|\tilde{A}(X_{0})\right\|\) for \(t\in[0,\delta]\). Therefore \(\left\|\dot{\tilde{X}}_{\delta}(\delta)\right\|\leq\sqrt{\frac{\delta^{p-1}}{2 \gamma}}\left\|\tilde{A}(X_{0})\right\|\) and we see \[\left\|\tilde{X}_{\delta}(\delta)-X_{0}\right\|\leq\int_{0}^{\delta}\left\| \dot{\tilde{X}}_{\delta}(s)\right\|ds\leq\int_{0}^{\delta}\sqrt{\frac{\delta^{p }}{2\gamma s}}\left\|\tilde{A}(X_{0})\right\|ds=\sqrt{\frac{2\delta^{p+1}}{ \gamma}}\left\|\tilde{A}(X_{0})\right\|.\]Applying (13) and recalling the fact \(\tilde{A}\) is \(L\)-Lipschitz, we have

\[\frac{\gamma^{2}}{\delta^{2p}}\left\|X_{\delta}(\delta)-X_{0} \right\|^{2} =\left\|\tilde{A}(X_{\delta}(\delta))+\tilde{X}_{\delta}(\delta) \right\|^{2}=\left\|\tilde{A}(X_{\delta}(\delta))-\tilde{A}(X_{0})+\tilde{A}(X _{0})+\dot{X}_{\delta}(\delta)\right\|^{2}\] \[\leq 2\left\|\tilde{A}(X_{\delta}(\delta))-\tilde{A}(X_{0}) \right\|^{2}+2\left\|\tilde{A}(X_{0})+\dot{X}_{\delta}(\delta)\right\|^{2}\] \[\leq 2L^{2}\left\|\tilde{X}_{\delta}(\delta)-X_{0}\right\|^{2}+4 \left\|\tilde{A}(X_{0})\right\|^{2}+4\left\|\dot{X}_{\delta}(\delta)\right\|^ {2}\] \[\leq\left(\frac{4}{\gamma}L^{2}\delta^{p+1}+4+\frac{2\delta^{p-1 }}{\gamma}\right)\left\|\tilde{A}(X_{0})\right\|^{2}.\]

From Lemma B.5 we know \(\tilde{U}_{2}(t)=\frac{1}{t^{p-1}}\left\|\dot{\tilde{X}}(t)\right\|^{2}+\frac {\gamma p}{t^{2}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\) is nonincreasing. Therefore \(\frac{\left\|\dot{\tilde{X}}_{\delta}(t)\right\|^{2}}{t^{p-1}}\leq\tilde{U}_ {2}(\delta)\) for \(t\geq\delta\). Since \(\delta<1\) we have

\[\frac{\left\|\dot{\tilde{X}}_{\delta}(t)\right\|}{\sqrt{t^{p-1}}} \leq\sqrt{\tilde{U}_{2}(\delta)}=\sqrt{\left\|\frac{\dot{\tilde{X }}_{\delta}(\delta)\right\|^{2}}{\delta^{p-1}}+\frac{\gamma p}{\delta^{2p}} \left\|\tilde{X}_{\delta}(\delta)-X_{0}\right\|^{2}}\] \[\leq\sqrt{\left\|\frac{\tilde{A}(X_{0})}{2\gamma}+p\left(4L^{2} \delta^{p+1}+4\gamma+2\delta^{p-1}\right)\left\|\tilde{A}(X_{0})\right\|^{2}} \leq\sqrt{\frac{1}{2\gamma}+2p\left(2L^{2}+2\gamma+1\right)}\left\|\tilde{A}( X_{0})\right\|.\]

Therefore for \(t\in[0,T]\) we have

\[\left\|\dot{\tilde{X}}_{\delta}(t)\right\|\leq\sqrt{T^{p-1}\left(\frac{1}{2 \gamma}+2p\left(2L^{2}+2\gamma+1\right)\right)}\left\|\tilde{A}(X_{0})\right\|\]

From \((1)\) and \((2)\), we get the desired result. 

We now show the sequence \(\left\{\tilde{X}_{\delta_{m}}\right\}_{m\in\mathbb{N}}\) converges uniformly on \([0,T]\) for every \(T>0\). It is suffices to prove following proposition.

**Proposition B.7**.: _For \(T>0\), the sequence \(\left\{\tilde{X}_{\delta_{m}}\right\}_{m\in\mathbb{N}}\) is a Cauchy sequence with respect to supremum norm on \([0,T]\)._

Proof.: Take \(\epsilon>0\). We want to show, there is \(N\in\mathbb{N}\) such that if \(n,m>N\) then \(\left\|\tilde{X}_{\delta_{n}}(t)-\tilde{X}_{\delta_{m}}(t)\right\|\leq\epsilon\) for all \(t\in[0,\infty)\).

Define \(d_{\delta,\nu}(t)=\frac{1}{2}\left\|\tilde{X}_{\delta}(t)-\tilde{X}_{\nu}(t) \right\|^{2}\). Without loss of generality, we may assume \(\delta\geq\nu\). With \(M_{dot}(T)\) defined in Lemma B.6, we will show \(d_{\delta,\nu}(t)\leq 2\delta^{2}\tilde{M}_{dot}(T)^{2}\).

First consider the case \(0\leq t\leq\delta\). By Lemma B.6, we have

\[\left\|\tilde{X}_{\delta}(t)-\tilde{X}_{\nu}(t)\right\| \leq\int_{0}^{t}\left\|\dot{\tilde{X}}_{\delta}(s)-\dot{\tilde{X}} _{\nu}(s)\right\|ds\leq\int_{0}^{t}\left(\left\|\dot{\tilde{X}}_{\delta}(s) \right\|+\left\|\dot{\tilde{X}}_{\nu}(s)\right\|\right)ds\] \[\leq\int_{0}^{t}2\tilde{M}_{dot}(T)ds=2t\tilde{M}_{dot}(T)\]

Thus for \(t\in[0,\delta]\) we have

\[d_{\delta,\nu}(t)\leq 2t^{2}\tilde{M}_{dot}(T)^{2}\leq 2\delta^{2} \tilde{M}_{dot}(T)^{2}.\]Now we consider the case \(t\geq\delta\). By monotonicity of \(\tilde{A}\), we have

\[\frac{d}{dt}\frac{1}{2}\left\|\tilde{X}_{\delta}(t)-\tilde{X}_{\nu} (t)\right\|^{2} =\left\langle\dot{X}_{\delta}(t)-\dot{X}_{\nu}(t),\tilde{X}_{\delta }(t)-\tilde{X}_{\nu}(t)\right\rangle\] \[=\left\langle-\left(\tilde{A}(\tilde{X}_{\delta}(t))-\tilde{A}( \tilde{X}_{\nu}(t))\right)-\frac{\gamma}{t^{p}}(X_{\delta}(t)-X_{0})+\frac{ \gamma}{t^{p}}(X_{\nu}(t)-X_{0}),X_{\delta}(t)-X_{\nu}(t)\right\rangle\] \[\leq\left\langle-\frac{\gamma}{t^{p}}(X_{\delta}(t)-X_{0})+\frac{ \gamma}{t^{p}}(X_{\nu}(t)-X_{0}),X_{\delta}(t)-X_{\nu}(t)\right\rangle\] \[=-\frac{\gamma}{t^{p}}\left\|X_{\delta}(t)-X_{\nu}(t)\right\|^{2}\] \[\leq 0.\]

Thus we have \(d_{\delta,\nu}(t)\leq d_{\delta,\nu}(\delta)\) for \(t\geq\delta\).

Now combining two cases, we have

\[d_{\delta,\nu}(t)\leq 2\delta^{2}\tilde{M}_{dot}(T)^{2}\] (14)

Since \(\lim_{k\to\infty}\delta_{n}=0\), there is \(N\in\mathbb{N}\) such that \(m>n>N\) implies \(d_{\delta_{n},\delta_{m}}\leq\frac{\varepsilon^{2}}{2}\), we're done. 

We are now ready to prove Theorem B.2.

Proof.:
1. _Existence of solution._ From Proposition B.7, we know \(\left\{\tilde{X}_{\delta_{m}}\right\}_{m\in\mathbb{N}}\) converging uniformly on \([0,T]\) for every \(T>\). Denote the limit as \(\tilde{X}\), i.e. define \(\tilde{X}\colon[0,\infty)\to\mathbb{R}^{n}\) as \[\lim_{m\to\infty}\tilde{X}_{\delta_{m}}(t)=\tilde{X}(t).\] We can check \(\tilde{X}(0)=X_{0}\) easily since \(\tilde{X}_{\delta_{m}}(0)=X_{0}\) for all \(m\in\mathbb{N}\). It remains to show \(\tilde{X}\) satisfies (12). Take \(t>0\). We wish to show

\[\lim_{h\to 0}\left\|\frac{\tilde{X}(t+h)-\tilde{X}(t)}{h}+\tilde{A}(\tilde{X}(t) )+\frac{\gamma}{t^{p}}\left(\tilde{X}(t)-X_{0}\right)\right\|=0.\]

Consider \(h\), \(\delta\), \(T\) such that \(0<|h|<t\), \(0<\delta<\min\left\{1,t-|h|\right\}\) and \(T>t+|h|\). Then \((t-|h|,t+|h|)\in[0,T]\) and \(\tilde{\tilde{X}}_{\delta}(t)=-\tilde{A}(\tilde{X}_{\delta}(t))-\frac{\gamma} {t^{p}}\left(\tilde{X}_{\delta}(t)-X_{0}\right)\). Consider inequality

\[\left\|\frac{\tilde{X}(t+h)-\tilde{X}(t)}{h}+\tilde{A}(\tilde{X}( t))+\frac{\gamma}{t^{p}}\left(\tilde{X}(t)-X_{0}\right)\right\|\] \[\leq\left\|\frac{\tilde{X}(t+h)-\tilde{X}(t)}{h}-\frac{\tilde{X}_ {\delta}(t+h)-\tilde{X}_{\delta}(t)}{h}\right\|+\left\|\frac{\tilde{X}_{ \delta}(t+h)-\tilde{X}_{\delta}(t)}{h}-\dot{\tilde{X}}_{\delta}(t)\right\|+ \left\|\dot{\tilde{X}}_{\delta}(t)+\tilde{A}(\tilde{X}(t))+\frac{\gamma}{t^{p }}\left(\tilde{X}(t)-X_{0}\right)\right\|.\]

We now show right hand side goes to zero as \(h\to 0\). The point of the proof is, \(\tilde{X}_{\delta}\) converges uniformly to \(\tilde{X}\) and \(\ddot{\tilde{X}}_{\delta}\) is uniformly bounded on \([t-|h|,T]\).

From (14) we have

\[\left\|\frac{\tilde{X}(t+h)-\tilde{X}(t)}{h}-\frac{\tilde{X}_{ \delta}(t+h)-\tilde{X}_{\delta}(t)}{h}\right\|\] \[\leq\frac{1}{|h|}\left(\left\|\tilde{X}(t+h)-\tilde{X}_{\delta}( t+h)\right\|+\left\|\tilde{X}(t)-\tilde{X}_{\delta}(t)\right\|\right)\leq 4 \frac{\delta}{|h|}\tilde{M}_{dot}(T).\]

Also from (14) and since \(\tilde{A}\) is \(L\)-Lipschitz continuous,

\[\left\|\dot{\tilde{X}}_{\delta}(t)+\tilde{A}(\tilde{X}(t))+\frac {\gamma}{t^{p}}\left(\tilde{X}(t)-X_{0}\right)\right\| =\left\|-\left(\tilde{A}(\tilde{X}_{\delta}(t))+\frac{\gamma}{t^{p }}\left(\tilde{X}_{\delta}(t)-X_{0}\right)\right)+\tilde{A}(\tilde{X}(t))+ \frac{\gamma}{t^{p}}\left(\tilde{X}(t)-X_{0}\right)\right\|\] \[\leq L\left\|\tilde{X}_{\delta}(t)-\tilde{X}(t)\right\|+\frac{ \gamma}{t^{p}}\left\|\tilde{X}_{\delta}(t)-\tilde{X}(t)\right\|\leq 2\delta \left(L+\frac{\gamma}{t^{p}}\right)\tilde{M}_{dot}(T).\]Now from Lemma B.4 we have \(\dot{X}_{\delta}(t)\) is absolutely continuous, thus

\[\left\|\frac{\ddot{X}_{\delta}(t+h)-\tilde{X}_{\delta}(t)}{h}-\dot{ \tilde{X}}_{\delta}(t)\right\| =\left\|\frac{\int_{t}^{t+h}\left(\dot{\tilde{X}}_{\delta}(s)- \dot{\tilde{X}}_{\delta}(t)\right)ds}{h}\right\|\] \[=\frac{1}{|h|}\left\|\int_{t}^{t+h}\int_{t}^{s}\ddot{\tilde{X}}_{ \delta}(u)\,du\,ds\right\|\leq\frac{1}{|h|}\left|\int_{t}^{t+h}\int_{t}^{s} \left\|\ddot{\tilde{X}}_{\delta}(u)\right\|\,du\,ds\right|\]

Observe, for almost every \(u\in\left[t-\left|h\right|,t+\left|h\right|\right]\subset\left[0,T\right]\) by Lemma B.6 we have

\[\left\|\ddot{\tilde{X}}_{\delta}(u)\right\| =\left\|-\frac{d}{du}\tilde{A}(\tilde{X}_{\delta}(u))+\frac{p \gamma}{u^{p+1}}\left(\tilde{X}_{\delta}(u)-X_{0}\right)-\frac{\gamma}{u^{p}} \dot{X}_{\delta}(u)\right\|\] \[=\left\|\frac{d}{du}\tilde{A}(\tilde{X}_{\delta}(u))\right\|+ \left\|\frac{p\gamma}{u^{p+1}}\int_{0}^{u}\dot{\tilde{X}}_{\delta}(v)dv\right\| +\left\|\frac{\gamma}{u^{p}}\dot{X}_{\delta}(u)\right\|\] \[\leq L\tilde{M}_{dot}(T)+\frac{p\gamma}{\left(t-\left|h\right| \right)^{p+1}}\int_{0}^{T}\tilde{M}_{dot}(T)dv+\frac{\gamma}{\left(t-\left|h \right|\right)^{p}}\tilde{M}_{dot}(T)\] \[=\left(L+\frac{p\gamma T}{\left(t-\left|h\right|\right)^{p+1}}+ \frac{\gamma}{\left(t-\left|h\right|\right)^{p}}\right)\tilde{M}_{dot}(T)=:M.\]

Note \(M\) is independent of \(h\) or \(\delta\). While obtaining the inequality, we used the fact that \(\tilde{A}(\tilde{X}_{\delta}(\cdot))\) is \(L\tilde{M}_{dot}(T)\)-Lipschitz continuous in \([0,T]\). Now

\[\frac{1}{|h|}\left|\int_{t}^{t+h}\int_{t}^{s}\left\|\ddot{\tilde{X}}_{\delta} (u)\right\|\,du\,ds\right|\leq\frac{1}{|h|}\left|\int_{t}^{t+h}\int_{t}^{s}M\, du\,ds\right|=\frac{1}{|h|}M\left|\int_{t}^{t+h}(s-t)\,ds\right|=\frac{|h|}{2}M.\]

Now consider \(\delta=h^{2}\) with \(h\) small enough that satisfies \(|h|<1\) and \(|h|+h^{2}<t\). Then the conditions \(|h|<t\), \(0<\delta<\min\left\{1,t-\left|h\right|\right\}\) hold, above arguments are valid. Gathering above results, we have

\[\left\|\frac{\tilde{X}(t+h)-\tilde{X}(t)}{h}+\tilde{A}(\tilde{X}( t))+\frac{\gamma}{t^{p}}\left(\tilde{X}(t)-X_{0}\right)\right\|\] \[\leq 2\sqrt{2}|h|\tilde{M}_{dot}(T)+\sqrt{2}|h|^{2}\left(L+\frac{ \gamma}{t^{p}}\right)\tilde{M}_{dot}(T)+\frac{|h|}{2}M=\mathcal{O}\left(|h| \right).\]

which implies the desired result.

1. _The value and continuity of \(\dot{\tilde{X}}(t)\) at \(t=0\)._ Define \(C(t)\) as \[C(t)=\begin{cases}t^{\gamma}&p=1\\ e^{\frac{\gamma}{1-p}t^{1-p}}&p>0,p\neq 1.\end{cases}\] Then we see for \(t>0\) \[\frac{d}{dt}\left(C(t)\left(\tilde{X}(t)-X_{0}\right)\right)=C(t)\left(\dot{ \tilde{X}}(t)+\frac{\gamma}{t^{p}}(\tilde{X}(t)-X_{0})\right)=-C(t)\tilde{A}( \tilde{X}(t)).\] Integrating both sides from \(\epsilon>0\) to \(t\) we have \[C(t)(\tilde{X}(t)-X_{0})-C(\epsilon)(\tilde{X}(\epsilon)-X_{0})=-\int_{ \epsilon}^{t}C(s)\tilde{A}(\tilde{X}(s))\,ds.\] As \(C\) is a nondecreasing function and bounded below by \(0\), \(\lim_{\epsilon\to 0+}C(\epsilon)\) exists, taking limit \(\epsilon\to 0+\) we have \[C(t)(\tilde{X}(t)-X_{0})=-\int_{0}^{t}C(s)\tilde{A}(\tilde{X}(s))\,ds.\]Dividing both sides by \(tC(t)\), with change of variable \(v=s/t\), we have

\[\frac{\tilde{X}(t)-X_{0}}{t}=-\int_{0}^{t}\frac{C(s)}{C(t)}\tilde{A}(\tilde{X}(s ))\,\frac{ds}{t}=-\int_{0}^{1}\frac{C(tv)}{C(t)}\tilde{A}(\tilde{X}(tv))\,dv.\]

Observe

\[\frac{C(tv)}{C(t)}=\begin{cases}v^{\gamma}&p=1\\ e^{\frac{\gamma}{1-p}t^{1-p}\left(v^{1-p}-1\right)}&p\neq 1,p>0.\end{cases}\]

Note \(\frac{\gamma}{1-p}t^{1-p}\left(v^{1-p}-1\right)\leq 0\) for \(v\in[0,1]\), since \(\frac{\gamma}{1-p}\) and \(\left(v^{1-p}-1\right)\) has opposite sign either \(0<p<1\) or \(p>1\). Therefore \(e^{\frac{\gamma}{1-p}t^{1-p}\left(v^{1-p}-1\right)}\leq 1\). Also, \(\tilde{A}(\tilde{X}(tv))\) is bounded for \(v\in[0,1]\) since \(A(\tilde{X}(\cdot))\) is continuous by Lemma B.4.

So we can apply dominated convergence theorem and take limit \(t\to 0+\). Since

\[\lim_{t\to 0+}\frac{C(tv)}{C(t)}=\begin{cases}1&0<p<1\\ v^{\gamma}&p=1\\ 0&p>1\end{cases}\]

for \(v\neq 0\), we have

\[\dot{\tilde{X}}(0)=\lim_{t\to 0+}\frac{\tilde{X}(t)-X_{0}}{t}=-\int_{0}^{1} \lim_{t\to 0+}\left(\frac{C(tv)}{C(t)}\tilde{A}(\tilde{X}(tv))\right)\,dv= \begin{cases}-\tilde{A}(X_{0})&0<p<1\\ -\frac{1}{\gamma+1}\tilde{A}(X_{0})&p=1\\ 0&p>1.\end{cases}\] (15)

We now check \(\dot{\tilde{X}}(t)\) is continuous at \(t=0\).

1. \(0<p\leq 1\) For \(t>0\), we know \[\dot{\tilde{X}}(t)=-\tilde{A}(\tilde{X}(t))-\frac{\gamma}{t^{p}}(\tilde{X}(t) -X_{0}).\] Observe \[\lim_{t\to 0+}\frac{\gamma}{t^{p}}(\tilde{X}(t)-X_{0})=\lim_{t\to 0+} \left(\gamma t^{1-p}\cdot\frac{\tilde{X}(t)-X_{0}}{t}\right)=\begin{cases}0&0<p <1\\ -\frac{\gamma}{\gamma+1}\tilde{A}(X_{0})&p=1.\end{cases}\] (16) Now from ODE \(\dot{\tilde{X}}(t)=-\tilde{A}(\tilde{X}(t))-\frac{\gamma}{t^{p}}(\tilde{X}(t) -X_{0})\), by taking limit \(t\to 0+\) we have \[\lim_{t\to 0+}\dot{\tilde{X}}(t)=-\tilde{A}(X_{0})-\lim_{t\to 0+} \left(\frac{\gamma}{t^{p}}(\tilde{X}(t)-X_{0})\right)=\begin{cases}-\tilde{A} (X_{0})&0<p<1\\ -\frac{1}{\gamma+1}\tilde{A}(X_{0})&p=1.\end{cases}\] Therefore \(\dot{\tilde{X}}(t)\) is continuous at \(t=0\).
2. \(p>1\) For \(p>1\), we don't know the value of \(\lim_{t\to 0}\frac{\gamma}{t^{p}}(\tilde{X}(t)-X_{0})\). Thus we first find the limit. Let's go back to \[C(t)(\tilde{X}(t)-X_{0})=-\int_{0}^{t}C(s)\tilde{A}(\tilde{X}(s))\,ds.\] Recall \(\dot{C}(t)=\frac{\gamma}{t^{p}}C(t)\). By taking integration by parts for the right hand side, we have \[\int_{0}^{t}C(s)\tilde{A}(\tilde{X}(s))\,ds=\frac{t^{p}}{\gamma}C(t)\tilde{A }(\tilde{X}(t))-\int_{0}^{t}p\frac{s^{p-1}}{\gamma}C(s)\tilde{A}(\tilde{X}(s) )\,ds-\int_{0}^{t}\frac{s^{p}}{\gamma}C(s)\frac{d}{ds}\tilde{A}(\tilde{X}(s) )\,ds.\] Where we know \(\tilde{A}(\tilde{X}(s))\) is differentiable almost everywhere from Lemma B.4. Now, divide both sides by \(\frac{t^{p}C(t)}{\gamma}\). Then for \(s=tv\) we have \[\frac{\gamma}{t^{p}}(\tilde{X}(t)-X_{0}) =-\tilde{A}(\tilde{X}(t))+\int_{0}^{t}\frac{ps^{p-1}}{t^{p}}\frac{ C(s)}{C(t)}\tilde{A}(\tilde{X}(s))\,ds+\int_{0}^{t}\frac{s^{p}}{t^{p}}\frac{C(s)}{C(t)} \left(\frac{d}{ds}\tilde{A}(\tilde{X}(s))\right)\,ds\] \[=-\tilde{A}(\tilde{X}(t))+\int_{0}^{1}pv^{p-1}\frac{C(tv)}{C(t)} \tilde{A}(\tilde{X}(s))\,dv+\int_{0}^{1}v^{p}\frac{C(tv)}{C(t)}\left(\frac{d}{ ds}\tilde{A}(\tilde{X}(s))\right)\,t\,dv.\]From Proposition B.7 and since \(\tilde{X}\) satisfies (12) for \(t>0\), for \(s\in(0,t]\) we have

\[\dot{\tilde{X}}(s)=\tilde{A}(\tilde{X}(s))+\frac{\gamma}{t^{p}}\left(\tilde{X}(s )-X_{0}\right)=\lim_{m\to\infty}\left(\tilde{A}(\tilde{X}_{\delta_{m}}(s))+ \frac{\gamma}{t^{p}}\left(\tilde{X}_{\delta_{m}}(s)-X_{0}\right)\right)=\lim_{ m\to\infty}\dot{\tilde{X}}_{\delta_{m}}(s).\]

From Lemma B.6 we know \(\left\|\dot{\tilde{X}}_{\delta_{m}}(s)\right\|\leq\tilde{M}_{dot}(t)\) for \(s\in[0,t]\), taking limit \(m\to\infty\) we have \(\left\|\dot{\tilde{X}}(s)\right\|\leq\tilde{M}_{dot}(t)\) for \(s\in[0,t]\). Thus \(\tilde{X}(s)\) becomes \(\tilde{M}_{dot}(t)\)-Lipschitz continuous in \(s\in[0,t]\). And so \((\tilde{A}\circ\tilde{X})(s)\) becomes \(L\tilde{M}_{dot}(t)\)-Lipschitz continuous in \(s\in[0,t]\), we have \(\left\|\frac{d}{ds}\tilde{A}(\tilde{X}(s))\right\|\leq L\tilde{M}_{dot}(t)\) for almost all \(s\in[0,t]\). Moreover \(C(tv)/C(t)\) is bounded for \(v\in[0,1]\) since \(C\) is a nonnegative nondecreasing function.

Therefore we can again apply dominated convergence theorem. Reminding \(\lim_{t\to 0+}\frac{C(tv)}{C(t)}=0\) for \(p>1\), taking limit \(t\to 0+\) we have

\[\lim_{t\to 0+}\frac{\gamma}{t^{p}}(\tilde{X}(t)-X_{0})=-\tilde{A}(X_{0})+0=- \tilde{A}(X_{0}).\] (17)

Finally we have

\[\lim_{t\to 0+}\dot{\tilde{X}}(t)=-\tilde{A}(X_{0})-\lim_{t\to 0+}\left(\frac{ \gamma}{t^{p}}(\tilde{X}(t)-X_{0})\right)=-\tilde{A}(X_{0})-(-\tilde{A}(X_{0} ))=0=\dot{\tilde{X}}(0).\]

Before we move on to original inclusion, we prove important corollaries that bound \(\left\|\dot{\tilde{X}}(t)\right\|\) and \(\left\|\tilde{A}(\tilde{X}(t))\right\|\) uniformly on \([0,T]\). Note the main difference between following corollary and Lemma B.6 is that the dependency on Lipschitz constant \(L\) is dropped for the case \(p>1\), which will be crucial in the next section.

**Corollary B.8**.: _Denote \(\tilde{X}\) as the solution of (12). Then for \(T>0\), following inequality is true for \(t\in[0,T]\)._

\[\left\|\dot{\tilde{X}}(t)\right\|\leq\begin{cases}\left\|\tilde{A}(X_{0}) \right\|&0<p<1\\ \frac{1}{\sqrt{\gamma+1}}\left\|\tilde{A}(X_{0})\right\|&p=1\\ \sqrt{\frac{p}{\gamma}T^{p-1}}\left\|\tilde{A}(X_{0})\right\|&p>1.\end{cases}\]

Proof.:
1. \(0<p\leq 1\) As \(\tilde{X}\) is the solution for (12), from Lemma B.5 we know \[\tilde{U}_{1}(t)=\left\|\dot{\tilde{X}}(t)\right\|^{2}+\frac{\gamma p}{t^{p+1 }}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\] is a nonincreasing function. From (15), (16) and continuity of \(\dot{X}(t)\) at \(t=0\), we have \[\lim_{\epsilon\to 0+}\tilde{U}_{1}(\epsilon)=\begin{cases}\left\|\tilde{A}(X_{0}) \right\|^{2}&0<p<1\\ \frac{1}{\gamma+1}\left\|\tilde{A}(X_{0})\right\|^{2}&p=1.\end{cases}\] (18) Therefore \(\left\|\dot{\tilde{X}}(t)\right\|^{2}\leq\tilde{U}_{1}(t)\leq\lim_{\epsilon \to 0+}\tilde{U}_{1}(\epsilon)\) for \(t>0\), we get the desired result.
2. \(p>1\) As \(\tilde{X}\) is the solution for (12), from Lemma B.5 we know \[\tilde{U}_{2}(t)=\frac{1}{t^{p-1}}\left\|\dot{\tilde{X}}(t)\right\|^{2}+\frac{ \gamma p}{t^{2p}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\] is a nonincreasing function. However, as we don't know the value of \(\lim_{t\to 0+}\frac{1}{t^{p-1}}\left\|\dot{\tilde{X}}(t)\right\|^{2}\), we first calculate it. To do so, we consider \[\tilde{U}_{1}(t)=\left\|\dot{\tilde{X}}(t)\right\|^{2}+\frac{\gamma p}{t^{p+1 }}\left\|\tilde{X}(t)-X_{0}\right\|^{2}.\]In the proof of Lemma B.5, we have observed its derivative becomes

\[\dot{\tilde{U}}_{1}(t) =-2\left\langle\dot{\tilde{X}}(t),\frac{d}{dt}\tilde{A}(\tilde{X}(t) )\right\rangle-\frac{2\gamma}{t^{p}}\left\|\dot{\tilde{X}}(t)-\frac{p}{t}( \tilde{X}(t)-X_{0})\right\|^{2}-\frac{\gamma p(1-p)}{t^{p+2}}\left\|\tilde{X}( t)-X_{0}\right\|^{2}\] \[\leq\frac{\gamma p(p-1)}{t^{p+2}}\left\|\tilde{X}(t)-X_{0}\right\| ^{2}.\]

For \(\epsilon>0\), integrating from \(\epsilon\) to \(t\) we have

\[\tilde{U}_{1}(t)\leq\tilde{U}_{1}(\epsilon)+\int_{\epsilon}^{t}\frac{\gamma p (p-1)}{s^{p+2}}\left\|\tilde{X}(s)-X_{0}\right\|^{2}ds.\]

We consider taking limit \(\epsilon\to 0+\). Observe from (17) and (15) we know \(\lim_{t\to 0+}\frac{\tilde{X}-X_{0}}{t^{p}}=-\frac{\tilde{A}(X_{0})}{\gamma}\) and \(\left\|\dot{\tilde{X}}(0)\right\|=0\), and therefore we have

\[\lim_{\epsilon\to 0+}\tilde{U}_{1}(\epsilon)=0+\lim_{\epsilon\to 0+}\frac{\gamma p }{\epsilon^{2p}}\left\|\tilde{X}(\epsilon)-X_{0}\right\|^{2}\cdot\epsilon^{p- 1}=\frac{p}{\gamma}\left\|\tilde{A}(X_{0})\right\|^{2}\lim_{\epsilon\to 0+} \epsilon^{p-1}=0.\]

Moreover as \(\lim_{t\to 0+}\frac{\left\|\tilde{X}(t)-X_{0}\right\|^{2}}{t^{2p}}=\frac{\left\| \tilde{A}(X_{0})\right\|^{2}}{\gamma^{2}}\), there is \(\delta>0\) such that \(0<s<\delta\) implies \(\frac{\left\|\tilde{X}(t)-X_{0}\right\|^{2}}{s^{2p}}\leq\frac{2\left\|\tilde{A }(X_{0})\right\|^{2}}{\gamma^{2}}\). Recalling \(p>1\), for \(0<\epsilon<\delta\) we have

\[\int_{0}^{\epsilon}\frac{\gamma p(p-1)}{s^{p+2}}\left\|\tilde{X}( s)-X_{0}\right\|^{2}ds \leq\int_{0}^{\epsilon}\frac{\gamma p(p-1)}{s^{2-p}}\frac{2 \left\|\tilde{A}(X_{0})\right\|^{2}}{\gamma^{2}}ds\] \[=\frac{2p(p-1)\left\|\tilde{A}(X_{0})\right\|^{2}}{\gamma}\left[ \frac{1}{p-1}s^{p-1}\right]_{0}^{\epsilon}=\frac{2p\left\|\tilde{A}(X_{0}) \right\|^{2}}{\gamma}\epsilon^{p-1},\]

therefore

\[\int_{0}^{t}\frac{\gamma p(p-1)}{s^{p+2}}\left\|\tilde{X}(s)-X_{0}\right\|^{2 }ds\leq\frac{2p\left\|\tilde{A}(X_{0})\right\|^{2}}{\gamma}\epsilon^{p-1}+ \int_{\epsilon}^{t}\frac{\gamma p(p-1)}{s^{p+2}}\left\|\tilde{X}(s)-X_{0} \right\|^{2}ds<\infty.\]

Thus the integral is well-defined when \(\epsilon\to 0+\). Taking limit \(\epsilon\to 0+\) we have

\[\tilde{U}_{1}(t)\leq\int_{0}^{t}\frac{\gamma p(p-1)}{s^{p+2}}\left\|\tilde{X} (s)-X_{0}\right\|^{2}ds.\]

Moving \(\frac{\gamma p}{t^{p+1}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\) to the right hand side, we get a inequality for \(\left\|\dot{\tilde{X}}(t)\right\|^{2}\),

\[\left\|\dot{\tilde{X}}(t)\right\|^{2}=\tilde{U}_{1}(t)-\frac{\gamma p}{t^{p+1 }}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\leq\int_{0}^{t}\frac{\gamma p(p-1)}{s ^{p+2}}\left\|\tilde{X}(s)-X_{0}\right\|^{2}ds-\frac{\gamma p}{t^{p+1}}\left\| \tilde{X}(t)-X_{0}\right\|^{2}.\]

Observe, by L'Hopital's rule we have

\[\lim_{t\to 0+}\frac{\int_{0}^{t}\frac{\gamma p(p-1)}{s^{p+2}}\left\|\tilde{X}(s)-X _{0}\right\|^{2}ds}{t^{p-1}}=\lim_{t\to 0+}\frac{\frac{\gamma p(p-1)}{t^{p+2}} \left\|\tilde{X}(t)-X_{0}\right\|^{2}}{(p-1)t^{p-2}}=\lim_{t\to 0+}\gamma p\frac{ \left\|\tilde{X}(t)-X_{0}\right\|^{2}}{t^{2p}}=\frac{p}{\gamma}\left\|\tilde{A} (X_{0})\right\|^{2}.\]

Now dividing \(t^{p-1}\) to previous inequality and taking limit, we conclude

\[\lim_{t\to 0+}\frac{1}{t^{p-1}}\left\|\dot{\tilde{X}}\right\|^{2} \leq\lim_{t\to 0+}\frac{1}{t^{p-1}}\int_{0}^{t}\frac{\gamma p(p-1)}{s^{p +2}}\left\|\tilde{X}-X_{0}\right\|^{2}ds-\lim_{t\to 0+}\frac{\gamma p}{t^{2p}}\left\| \tilde{X}-X_{0}\right\|^{2}\] \[=\frac{p}{\gamma}\left\|\tilde{A}(X_{0})\right\|^{2}-\frac{p}{ \gamma}\left\|\tilde{A}(X_{0})\right\|^{2}\] \[=0.\]Thus we have \(\lim_{t\to 0+}\frac{1}{t^{p-1}}\left\|\dot{\tilde{X}}\right\|^{2}=0\). Therefore,

\[\lim_{\epsilon\to 0+}\tilde{U}_{2}(\epsilon)=\lim_{\epsilon\to 0+}\left(\frac{1}{ \epsilon^{p-1}}\left\|\dot{\tilde{X}}(\epsilon)\right\|^{2}+\frac{\gamma p}{ \epsilon^{2p}}\left\|\tilde{X}(\epsilon)-X_{0}\right\|^{2}\right)=\frac{p}{ \gamma}\left\|\tilde{A}(X_{0})\right\|^{2}.\] From \(\frac{\left\|\dot{\tilde{X}}\right\|^{2}}{t^{p-1}}\leq\lim_{\epsilon\to 0+} \tilde{U}_{2}(\epsilon)=\frac{p}{\gamma}\left\|\tilde{A}(X_{0})\right\|^{2}\), we conclude the desired result.

**Corollary B.9**.: _Denote \(\tilde{X}\) as the solution of (12). Then for \(T>0\), following inequality is true for \(t\in[0,T]\)._

\[\left\|\tilde{A}(\tilde{X}(t))\right\|\leq\begin{cases}\sqrt{( \gamma+1)\left(1+\frac{T^{1-p}}{p}\right)}\left\|\tilde{A}(X_{0})\right\|&0<p<1 \\ \sqrt{\frac{p(\gamma+1)}{\gamma}\left(T^{p-1}+\frac{1}{p}\right)}\left\| \tilde{A}(X_{0})\right\|&p>1.\end{cases}\]

Proof.: First observe,

\[\left\|\tilde{A}(\tilde{X}(t))\right\|^{2} =\left\|\dot{\tilde{X}}(t)+\frac{\gamma}{t^{p}}\left(\tilde{X}(t) -X_{0}\right)\right\|^{2}\] \[=\left\|\dot{\tilde{X}}(t)\right\|^{2}+2\gamma\left\langle\dot{ \tilde{X}}(t),\frac{\tilde{X}(t)-X_{0}}{t^{p}}\right\rangle+\frac{\gamma^{2}}{ t^{2p}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\] \[\leq\left\|\dot{\tilde{X}}(t)\right\|^{2}+\gamma\left(\left\| \dot{\tilde{X}}(t)\right\|^{2}+\frac{\left\|\tilde{X}(t)-X_{0}\right\|^{2}}{t ^{2p}}\right)+\frac{\gamma^{2}}{t^{2p}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\] \[=(\gamma+1)\left(\left\|\dot{\tilde{X}}(t)\right\|^{2}+\frac{ \gamma}{t^{2p}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\right)\]

The inequality comes from Young's inequality. Now we consider each case.

1. \(p=1\) For this case, the terms on the right hand side exactly become \(\tilde{U}_{1}(t)\). Therefore from (18) \[\left\|\tilde{A}(\tilde{X}(t))\right\|^{2}\leq(\gamma+1)\tilde{U}_{1}(t)\leq (\gamma+1)\lim_{\epsilon\to 0+}\tilde{U}_{1}(\epsilon)=\left\|\tilde{A}(X_{0}) \right\|^{2}.\]
2. \(0<p<1\) Recall from the proof of Corollary B.8 we know \(\tilde{U}_{1}(t)\leq\lim_{\epsilon\to 0+}\tilde{U}_{1}(\epsilon)=\left\| \tilde{A}(X_{0})\right\|^{2}\). Therefore we have \(\frac{\gamma p}{t^{p+1}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\leq\tilde{U}_{1 }(t)\leq\left\|\tilde{A}(X_{0})\right\|^{2}\), applying Corollary B.8 we get \[\left\|\tilde{A}(\tilde{X}(t))\right\|^{2} \leq(\gamma+1)\left(\left\|\dot{\tilde{X}}(t)\right\|^{2}+\frac{ \gamma p}{t^{p+1}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\cdot\frac{t^{1-p}}{p}\right)\] \[\leq(\gamma+1)\left\|\tilde{A}(X_{0})\right\|^{2}\left(1+\frac{T^ {1-p}}{p}\right).\]
3. \(p>1\) Recall from the proof of Corollary B.8 we know \(\tilde{U}_{2}(t)\leq\lim_{\epsilon\to 0+}\tilde{U}_{2}(\epsilon)=\frac{p}{ \gamma}\left\|\tilde{A}(X_{0})\right\|^{2}\). Therefore we have \[\left\|\tilde{A}(\tilde{X}(t))\right\|^{2} \leq(\gamma+1)\left(\left\|\dot{\tilde{X}}(t)\right\|^{2}+\frac{ \gamma p}{t^{2p}}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\cdot\frac{1}{p}\right)\] \[\leq\frac{p(\gamma+1)}{\gamma}\left(T^{p-1}+\frac{1}{p}\right) \left\|\tilde{A}(X_{0})\right\|^{2}.\]

#### b.2.2 Existence proof for general \(\mathbf{A}\)

Now we move on to the original inclusion. As noticed before, we will approximate \(\mathbf{A}\) with a Liptschitz function \(\mathbf{A}_{\lambda}\) called Yosida approximation. We define and state some facts about \(\mathbf{A}_{\lambda}\) as a lemma, and use it without proof. For the ones who are interested in proofs, see [11, Chpater 3.1, Theorem 2].

**Lemma B.10**.: _Define \(\mathbf{A}_{\lambda}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\) as_

\[\mathbf{A}_{\lambda}=\frac{1}{\lambda}\left(\mathbf{I}-\mathbf{J}_{\lambda \mathbf{A}}\right)=\frac{1}{\lambda}\left(\mathbf{I}-(\mathbf{I}+\lambda \mathbf{A})^{-1}\right)\]

_This is so called Yosida approximation of \(\mathbf{A}\). Followings are true._

1. \(\mathbf{A}_{\lambda}\) _is_ \(\frac{1}{\lambda}\)_-Lipschitz continuous and maximal monotone._
2. \(\lim_{\lambda\to 0+}\mathbf{A}_{\lambda}x=m(\mathbf{A}(x))\)_._

_Here_ \(m(\mathbf{A}(x))\) _is defined as_ \(m(\mathbf{A}(x))=\Pi_{\mathbf{A}(x)}(0)\)_, the element of_ \(\mathbf{A}(x)\) _with minimal norm._
3. \(\|\mathbf{A}_{\lambda}(x)\|\leq\|m(\mathbf{A}(x))\|\)_._
4. \(\forall x\in\mathbb{R}^{n}\)_,_ \(\mathbf{A}_{\lambda}(x)\in\mathbf{A}(\mathbf{J}_{\lambda\mathbf{A}}x)\)_._

Now we can state the proposition that proves existence of Theorem 2.2

**Proposition B.11**.: _For Yosida approximation \(\mathbf{A}_{\lambda}\), consider the ODE_

\[\dot{X}_{\lambda}(t)=-\mathbf{A}_{\lambda}(X_{\lambda}(t))-\frac{\gamma}{t^{ p}}(X_{\lambda}(t)-X_{0})\] (19)

_with initial value condition \(X_{\lambda}(0)=X_{0}\in dom(\mathbf{A})\). The solution uniquely exists by Theorem B.2, denote the solution as \(X_{\lambda}\). Now for \(T>0\) and a positive sequence \(\{\lambda_{n}\}_{n\in\mathbb{N}}\) such that \(\lim_{n\rightarrow\infty}\lambda_{n}=0\), define a sequence of solutions as \(\mathcal{F}_{T}=\left\{X_{\lambda_{n}}:[0,T]\rightarrow\mathbb{R}^{n}\ |\ m\in\mathbb{N}\right\}\). Then there is a subsequence \(\left\{\lambda_{n_{k}}\right\}_{k\in\mathbb{N}}\) such that \(X_{\lambda_{n_{k}}}\) converges to the solution of (6) uniformly on \([0,T]\)._

From Corollary B.8, Corollary B.9 and Lemma B.10 (iii), following lemma is immediate.

**Lemma B.12**.: _Let \(X_{\lambda}\) be the solution of (19). Then following is true for \(t\in[0,T]\) for all \(T>0\)._

\[\left\|\dot{X}_{\lambda}(t)\right\|\leq M_{dot}(T)=\begin{cases}\|m(\mathbf{A }(X_{0}))\|&0<p<1\\ \frac{1}{\sqrt{\gamma+1}}\|m(\mathbf{A}(X_{0}))\|&p=1\\ \sqrt{\frac{p}{\gamma}T^{p-1}}\left\|m(\mathbf{A}(X_{0}))\right\|&p>1\end{cases}\]

_and_

\[\|\mathbf{A}_{\lambda}(X_{\lambda}(t))\|\leq M_{\mathbf{A}}(T)= \begin{cases}\sqrt{(\gamma+1)\left(1+\frac{T^{1-p}}{p}\right)}\left\|m( \mathbf{A}(X_{0}))\|&0<p<1\\ \|m(\mathbf{A}(X_{0}))\|&p=1\\ \sqrt{\frac{p(\gamma+1)}{\gamma}\left(T^{p-1}+\frac{1}{p}\right)}\left\|m( \mathbf{A}(X_{0}))\right\|&p>1.\end{cases}\]

Proof.: Replace \(\tilde{A}\) with \(\mathbf{A}_{\lambda}\) and \(\tilde{X}\) with \(X_{\lambda}\) in Corollary B.8 and Corollary B.9. Applying \(\|\mathbf{A}_{\lambda}(X_{0})\|\leq\|m(\mathbf{A}(X_{0}))\|\) from Lemma B.10 (iii), we're done.

While we're concluding the existence of converging sequence, we will exploit following lemma from [11, Chapter 0.3, Theorem 4]. For convenience, we restate the lemma here.

**Lemma B.13**.: _Let us consider a sequence of absolutely continuous functions \(x_{k}(\cdot)\) from an interval \(I\) of \(\mathbb{R}\) to a Banach space \(X\) satisfying_

1. \(\forall t\in I\)_,_ \(\{x_{k}(t)\}_{k\in\mathbb{N}}\) _is a relatively compact subset of_ \(X\)__
2. _there exists a positive function_ \(c(\cdot)\in L^{1}(I,[0,\infty))\) _such that, for almost all_ \(t\in I\)_,_ \(\|\dot{x}_{k}(t)\|\leq c(t)\)__

_Then there exists a subsequence (again denoted by) \(x_{k}(\cdot)\) converging to an absolutely continuous function \(x(\cdot)\) from \(I\) to \(X\) in the sense that_

1. \(x_{k}(\cdot)\) _converges to_ \(x(\cdot)\) _over compact subsets of_ \(I\)__
2. \(\dot{x}_{k}(\cdot)\) _converges weakly to_ \(\dot{x}(\cdot)\) _in_ \(L^{1}(I,X)\)__

Proof.: See [11, Chapter 0.3, Theorem 4]. 

From Lemma B.12 we can immediately check norm of all derivatives \(\dot{X}_{\lambda_{m}}\) are bounded by \(M_{dot}(T)\). So condition (ii) holds with \(M_{dot}(T)\). For condition (i), we prove \(\mathcal{F}_{T}\) is convergent in \(C([0,T],\mathbb{R}^{n})\).

**Lemma B.14**.: \(\mathcal{F}_{T}\) _is convergent sequence in \(C([0,T],\mathbb{R}^{n})\). In other words, \(\forall\epsilon>0\), there is \(N>0\) such that \(n,m>N\) implies \(\sup_{t\in[0,T]}\|X_{\lambda_{n}}(t)-X_{\lambda_{m}}(t)\|<\epsilon\)_

Proof.: We will show \(X_{\lambda_{n}}\) is Cauchy sequence in \(C([0,T],\mathbb{R}^{n})\). Let \(\nu,\lambda>0\). From (19), we see for \(t\in(0,T]\)

\[\frac{d}{dt}\frac{1}{2}\left\|X_{\nu}(t)-X_{\lambda}(t)\right\|^ {2} =\left\langle\dot{X}_{\nu}(t)-\dot{X}_{\lambda}(t),X_{\nu}(t)-X_{ \lambda}(t)\right\rangle\] \[=-\left\langle\mathbf{A}_{\nu}(X_{\nu}(t))-\mathbf{A}_{\lambda}( X_{\lambda}(t)),\,X_{\nu}(t)-X_{\lambda}(t)\right\rangle-\frac{\gamma}{t^{p}} \left\|X_{\nu}(t)-X_{\lambda}(t)\right\|^{2}\] \[\leq-\left\langle\mathbf{A}_{\nu}(X_{\nu}(t))-\mathbf{A}_{\lambda }(X_{\lambda}(t)),\,X_{\nu}(t)-X_{\lambda}(t)\right\rangle.\]

From definition of resolvent we know \(\mathbf{I}-\mathbf{J}_{\lambda\mathbf{A}}=\lambda\mathbf{A}_{\lambda}\). And from Lemma B.10 (iv) we know \(\mathbf{A}_{\lambda}(X_{\lambda}(t))\in\mathbf{A}(\mathbf{J}_{\lambda\mathbf{A} }(X_{\lambda}(t)))\). Thus from monotone inequality we see

\[-\left\langle\mathbf{A}_{\nu}(X_{\nu}(t))-\mathbf{A}_{\lambda}( X_{\lambda}(t)),\,X_{\nu}(t)-X_{\lambda}(t)\right\rangle\] \[\qquad-\left\langle\mathbf{A}_{\nu}(X_{\nu}(t))-\mathbf{A}_{ \lambda}(X_{\lambda}(t)),\,\mathbf{J}_{\nu\mathbf{A}}(X_{\nu}(t))-\mathbf{J} _{\lambda\mathbf{A}}(X_{\lambda}(t))\right\rangle\] \[\leq-\left\langle\mathbf{A}_{\nu}(X_{\nu}(t))-\mathbf{A}_{ \lambda}(X_{\lambda}(t)),\,\nu\mathbf{A}_{\nu}X_{\nu}(t)-\lambda\mathbf{A}_{ \lambda}(X_{\lambda}(t))\right\rangle\] \[=\ (\nu+\lambda)\left\langle\mathbf{A}_{\nu}(X_{\nu}(t)),\mathbf{A} _{\lambda}(X_{\lambda}(t))\right\rangle-\left(\nu\left\|\mathbf{A}_{\nu}X_{ \nu}(t)\right\|^{2}+\lambda\left\|\mathbf{A}_{\lambda}X_{\lambda}(t)\right\|^{ 2}\right).\]

By Young's inequality

\[\left(\nu+\lambda\right)\left\langle\mathbf{A}_{\nu}(X_{\nu}(t)),\mathbf{A}_{\lambda}(X_{\lambda}(t))\right\rangle-\left(\nu\left\|\mathbf{A} _{\nu}X_{\nu}(t)\right\|^{2}+\lambda\left\|\mathbf{A}_{\lambda}X_{\lambda}(t )\right\|^{2}\right)\] \[\leq\ \nu\left(\left\|\mathbf{A}_{\nu}(X_{\nu}(t))\right\|^{2}+ \frac{1}{4}\left\|\mathbf{A}_{\lambda}(X_{\lambda}(t))\right\|^{2}\right)+ \lambda\left(\left\|\mathbf{A}_{\lambda}(X_{\lambda}(t))\right\|^{2}+\frac{1}{ 4}\left\|\mathbf{A}_{\nu}(X_{\nu}(t))\right\|^{2}\right)\] \[-\left(\nu\left\|\mathbf{A}_{\nu}X_{\nu}(t)\right\|^{2}+\lambda \left\|\mathbf{A}_{\lambda}X_{\lambda}(t)\right\|^{2}\right)\] \[=\ \frac{1}{4}\left(\nu\left\|\mathbf{A}_{\lambda}(X_{\lambda}(t)) \right\|^{2}+\lambda\left\|\mathbf{A}_{\nu}(X_{\nu}(t))\right\|^{2}\right).\]

Now applying Lemma B.12 we have

\[\frac{d}{dt}\frac{1}{2}\left\|X_{\nu}(t)-X_{\lambda}(t)\right\|^{2}\leq\frac{ 1}{4}(\nu+\lambda)M_{\mathbf{A}}(T)^{2}.\]

Then integrating both sides from \(0\) to \(t\) we have

\[\left\|X_{\nu}(t)-X_{\lambda}(t)\right\|^{2}\leq\frac{t}{2}(\nu+\lambda)M_{ \mathbf{A}}(T)^{2}.\] (20)Now take \(\epsilon>0\). Then there is \(N>0\) such that for \(n>N\), \(\lambda_{n}<\frac{\epsilon}{TM_{\mathsf{A}}(T)^{2}}\) holds. Then for \(t\in[0,T]\), \(n,m>N\), we have

\[\left\|X_{\lambda_{n}}(t)-X_{\lambda_{m}}(t)\right\|^{2}\leq\frac{t}{2}( \lambda_{n}+\lambda_{m})M_{\mathsf{A}}(T)^{2}<\epsilon.\]

Therefore we get the desired result. 

Finally, we are ready to prove Proposition B.11, which implies the main theorem.

Proof of Proposition b.11.: Take \(T>0\). We know \(X_{\lambda_{n}}\) uniformly converges on \([0,T]\) by Lemma B.14. Name the limit as \(X\), i.e. define \(X\colon[0,T]\to\mathbb{R}^{n}\) as \(X(t)=\lim_{n\to\infty}X_{\lambda_{n}}(t)\). Then as \(X_{\lambda_{n}}(0)=X_{0}\) for all \(n\in\mathbb{N}\), we see \(X\) satisfies the initial condition. It remains to show \(X\) satisfies (6) almost everywhere.

Recall \(\{\dot{X}_{\lambda_{n}}\}\) is bounded in \(L^{\infty}([0,T],\mathbb{R}^{n})\) by Lemma B.12. Thus we can apply Lemma B.13, there is a subsequence \(\{\dot{X}_{\lambda_{n_{k}}}\}\) converges weakly to \(\dot{X}\) in \(L^{1}([0,T],\mathbb{R}^{n})\). Furthermore we have \(\dot{X}\in L^{\infty}([0,T],\mathbb{R}^{n})\) and so \(\{\dot{X}_{\lambda_{n_{k}}}\}\) also converges weakly to \(\dot{X}\) in \(L^{2}([0,T],\mathbb{R}^{n})\) as well.

For \(\lambda>0\), define \(f_{\lambda}\colon[0,T]\to\mathbb{R}^{n}\) as

\[f_{\lambda}(t)=\begin{cases}\frac{\gamma}{t^{p}}\left(X_{\lambda}(t)-X_{0} \right)&\text{ if }t>0\\ 0&\text{ if }t=0.\end{cases}\]

Then for \(f\colon[0,T]\to\mathbb{R}^{n}\) defined as \(f(t)=\frac{\gamma}{t^{p}}\left(X(t)-X_{0}\right)\) for \(t>0\) and \(f(0)=0\), we have \(\lim_{k\to\infty}f_{\lambda_{n_{k}}}(t)=f(t).\) As \(\left\|f_{\lambda}(t)\right\|=\left\|\dot{X}_{\lambda}(t)+\mathsf{A}_{\lambda _{n}}(X_{\lambda})(t)\right\|\leq M_{dot}(T)+M_{\mathsf{A}}(T)\) for \(t\in(0,T]\) by Lemma B.12, we have

\[\left\|f_{\lambda}(t)-f(t)\right\|^{2}\leq\left(\left\|f_{\lambda}(t)\right\| +\left\|f(t)\right\|\right)^{2}\leq 4(M_{dot}(T)+M_{\mathsf{A}}(T))^{2}.\]

Therefore by dominated convergence theorem we have

\[\lim_{k\to\infty}\int_{0}^{T}\left\|f_{\lambda_{n_{k}}}(t)-f(t)\right\|^{2}dt =\int_{0}^{T}\lim_{k\to\infty}\left\|f_{\lambda_{n_{k}}}(t)-f(t)\right\|^{2}dt =0,\]

we conclude \(f_{\lambda_{n_{k}}}\) strongly converges to \(f\) in \(L^{2}([0,T],\mathbb{R}^{n})\).

Now consider \(F_{\lambda}:[0,T]\to\mathbb{R}^{n}\) defined as

\[F_{\lambda}(t)=\begin{cases}\dot{X}_{\lambda}(t)+\frac{\gamma}{t^{p}}(X_{ \lambda}(t)-X_{0})&\text{ if }t>0\\ -\mathsf{A}_{\lambda}(X_{0})&\text{ if }t=0\end{cases}\]

Note since \(X_{\lambda}\) are solution to ODE (19), we have

\[F_{\lambda}(t)=-\mathsf{A}_{\lambda}(X_{\lambda}(t)).\]

Then for \(F:[0,T]\to\mathbb{R}^{n}\) defined as

\[F(t)=\begin{cases}\dot{X}(t)+\frac{\gamma}{t^{p}}(X(t)-X_{0})&\text{ if }t>0\\ -m(\mathsf{A}(X_{0}))&\text{ if }t=0,\end{cases}\]

we see \(\{F_{\lambda_{n_{k}}}\}_{k\in\mathbb{N}}\) converges weakly to \(F\) in \(L^{2}([0,T],\mathbb{R}^{n})\).

On the other hand, by Lemma B.10 (iv) and the fact \(-F_{\lambda_{n_{k}}}(t)=\mathsf{A}_{\lambda_{n_{k}}}(X_{\lambda_{n_{k}}}(t))\), we see

\[-F_{\lambda_{n_{k}}}(t)\in\mathsf{A}(\mathsf{J}_{\lambda_{n_{k}}\mathsf{A}}(X_ {\lambda_{n_{k}}}(t))).\]

Observe, from the definition of \(\mathsf{A}_{\lambda_{n}}\) and Lemma B.12, we see

\[\left\|X_{\lambda_{n}}(t)-\mathsf{J}_{\lambda_{n}\mathsf{A}}(X_{\lambda_{n}}(t ))\right\|=\lambda_{n}\left\|\mathsf{A}_{\lambda_{n}}(X_{\lambda_{n}}(t)) \right\|\leq\lambda_{n}M_{\mathsf{A}}(T).\]

Since \(X_{\lambda_{n}}\) converges to \(X\) in \(\mathcal{C}([0,T],\mathbb{R}^{n})\), by taking \(n\to\infty\) above inequality we see \(\mathsf{J}_{\lambda_{n}\mathsf{A}}(X_{\lambda_{n}})\) also converges to \(X\) in \(\mathcal{C}([0,T],\mathbb{R}^{n})\).

Now for \(\mathcal{A}:L^{2}([0,T],\mathbb{R}^{n})\to L^{2}([0,T],\mathbb{R}^{n})\) defined as \((\mathcal{A}(x))(t)=\mathsf{A}(x(t))\) almost everywhere, by [11, Chapter 3.1, Proposition 4], \(\mathcal{A}\) is maximal monotone since \(\mathsf{A}\) is maximal monotone. Since \(-F_{\lambda_{k}}\) weakly converges to \(-F\) in \(L^{2}([0,T],\mathbb{R}^{n})\)and \(\mathbf{J}_{\lambda_{k}\mathbf{A}}(X_{\lambda_{k}})\) strongly converges to \(X\) in \(L^{2}([0,T],\mathbb{R}^{n})\), by [11, Chapter 3.1, Proposition 2] we have \(-F\in\mathcal{A}(X)\) in \(L^{2}([0,T],\mathbb{R}^{n})\). Therefore for almost all \(t\in(0,T]\) we have

\[-\left(\dot{X}(t)+\frac{\gamma}{t^{p}}(X(t)-X_{0})\right)\in\mathbf{A}(X(t)).\]

Reorganizing the result with respect to \(\dot{X}\), we have following is true for almost all \(t\in(0,T]\)

\[\dot{X}(t)\in-\mathbf{A}(X(t))-\frac{\gamma}{t^{p}}(X(t)-X_{0}).\]

Since \(T>0\) was arbitrary, we conclude \(X\) satisfies above inclusion for almost all \(t\in(0,\infty)\). 

## Appendix C Proof of existence and uniqueness of the solution of (10)

As uniqueness comes from Theorem B.1, we only need to show the existence. What we need for the existence proof are

1. Nonincreasing function \(\tilde{U}(t)\) which contains \(\left\|\dot{\tilde{X}}\right\|^{2}\) as in Lemma B.5.
2. Uniform boundedness of \(\dot{\tilde{X}}_{\delta}(t)\) for \(t\in[0,T]\) as shown in Lemma B.6.
3. \(\dot{\tilde{X}}(0)=\lim_{t\to 0+}\frac{\tilde{X}(t)-X_{0}}{t}=\lim_{t \to 0+}\dot{\tilde{X}}(t)\) as shown in the existence proof of Lipschitz case.
4. Uniform boundedness of \(\left\|\dot{X}_{\lambda}(t)\right\|\) and \(\left\|\mathbf{A}_{\lambda}(X_{\lambda}(t))\right\|\) for \(t\in[0,T]\) as shown in Lemma B.12

We now show these steps can be also done to the \(\beta(t)=\frac{2\mu}{e^{2\mu t}-1}\).

**(i) Nonincreasing function \(\tilde{U}(t)\) which contains \(\left\|\dot{\tilde{X}}\right\|^{2}\)**

From

\[\dot{\tilde{X}}(t)=-\tilde{A}(\tilde{X}(t))-\frac{2\mu}{e^{2\mu t}-1}(\tilde{ X}(t)-X_{0})\]

for almost all \(t\) we have

\[\ddot{\tilde{X}}(t)=-\frac{d}{dt}\tilde{A}(\tilde{X}(t))+\left(\frac{2\mu}{e^ {2\mu t}-1}\right)^{2}e^{2\mu t}(\tilde{X}(t)-X_{0})-\frac{2\mu}{e^{2\mu t}-1 }\dot{\tilde{X}}(t)\]

Define

\[\tilde{U}(t)=e^{-2\mu t}\left\|\dot{\tilde{X}}(t)\right\|^{2}+\left(\frac{2 \mu}{e^{2\mu t}-1}\right)^{2}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\]

Therefore for almost all \(t>0\),

\[\dot{\tilde{U}}(t) =2e^{-2\mu t}\left\langle\dot{\tilde{X}}(t),\ddot{\tilde{X}}(t) \right\rangle-2\mu e^{-2\mu t}\left\|\dot{\tilde{X}}(t)\right\|^{2}\] \[\quad+2\left(\frac{2\mu}{e^{2\mu t}-1}\right)^{2}\left\langle \dot{\tilde{X}}(t),\ddot{X}(t)-X_{0}\right\rangle-\left(\frac{2\mu}{e^{2\mu t }-1}\right)^{3}2e^{2\mu t}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\] \[=-2e^{-2\mu t}\left\langle\dot{\tilde{X}}(t),\frac{d}{dt}\tilde{A }(\tilde{X}(t))\right\rangle+2\left(\frac{2\mu}{e^{2\mu t}-1}\right)^{2}\left \langle\dot{\tilde{X}}(t),\tilde{X}(t)-X_{0}\right\rangle-2e^{-2\mu t}\frac{2 \mu}{e^{2\mu t}-1}\left\|\dot{\tilde{X}}(t)\right\|^{2}\] \[\quad-2\mu e^{-2\mu t}\left\|\dot{\tilde{X}}(t)\right\|^{2}+2 \left(\frac{2\mu}{e^{2\mu t}-1}\right)^{2}\left\langle\dot{\tilde{X}}(t), \tilde{X}(t)-X_{0}\right\rangle-\left(\frac{2\mu}{e^{2\mu t}-1}\right)^{3}2e^{ 2\mu t}\left\|\tilde{X}(t)-X_{0}\right\|^{2}\] \[=-2\mu e^{-2\mu t}\left\|\dot{\tilde{X}}(t)\right\|^{2}-2e^{-2\mu t }\left\langle\dot{\tilde{X}}(t),\frac{d}{dt}\tilde{A}(\tilde{X}(t))\right\rangle -\frac{2\mu e^{-2\mu t}}{e^{2\mu t}-1}\left\|\dot{\tilde{X}}(t)-\frac{2\mu e^ {2\mu t}}{e^{2\mu t}-1}\left(\tilde{X}(t)-X_{0}\right)\right\|^{2}\] \[\leq 0.\]

**(ii) Uniform boundedness of \(\dot{\hat{X}}_{\delta}(t)\)**

As (13), we define \(\dot{\hat{X}}_{\delta}(t)\) as the solution of

\[\dot{\hat{X}}_{\delta}(t)=\begin{cases}-\tilde{A}(\tilde{X}_{\delta}(t))-\frac{2 \mu}{e^{2\mu t}-1}(\tilde{X}_{\delta}(t)-X_{0})&0\leq t\leq\delta\\ -\tilde{A}(\tilde{X}_{\delta}(t))-\frac{2\mu}{e^{2\mu t}-1}(\tilde{X}_{\delta }(t)-X_{0})&t\geq\delta\end{cases}\]

Again with same arguments of Lemma B.6 we have \(\left\|\tilde{X}_{\delta}(\delta)-X_{0}\right\|\leq\delta\left\|\tilde{A}(X_ {0})\right\|\) and \(\left\|\dot{\hat{X}}_{\delta}(\delta)\right\|\leq\left\|\tilde{A}(X_{0})\right\|\). Now for \(t\in[0,T]\)

\[\left\|\dot{\hat{X}}_{\delta}(t)\right\|\leq\sqrt{e^{2\mu t} \tilde{U}(t)} \leq\sqrt{e^{2\mu t}\tilde{U}(\delta)}=e^{\mu t}\sqrt{e^{-2\mu \delta}\left\|\dot{\hat{X}}_{\delta}(\delta)\right\|^{2}+\left(\frac{2\mu}{e^ {2\mu\delta}-1}\right)^{2}\left\|\tilde{X}_{\delta}(\delta)-X_{0}\right\|^{2}}\] \[\leq e^{\mu t}\sqrt{e^{-2\mu\delta}\left\|\tilde{A}(X_{0}) \right\|^{2}+\left(\frac{2\mu\delta}{e^{2\mu\delta}-1}\right)^{2}\left\|\tilde {A}(X_{0})\right\|^{2}}\] \[\leq e^{\mu T}\sqrt{2}\left\|\tilde{A}(X_{0})\right\|.\]

**(iii) \(\dot{\hat{X}}(0)=\lim_{t\to 0+}\frac{\tilde{X}(t)-X_{0}}{t}=\lim_{t\to 0+} \dot{\hat{X}}(t)\)**

Define \(C(t):=1-e^{-2\mu t}\). Then \(\dot{C}(t)=C(t)\beta(t)\) for \(\beta(t)=\frac{2\mu}{e^{2\mu t}-1}\). And since

\[\lim_{t\to 0+}\frac{C(tv)}{C(t)}=\lim_{t\to 0+}\frac{1-e^{-2\mu tv}}{1-e^{-2\mu t}}=v,\]

with same argument done to arrive (15), we have

\[\dot{\hat{X}}(0)=\lim_{t\to 0+}\frac{\tilde{X}(t)-X_{0}}{t}=-\int_{0}^{1} \lim_{t\to 0+}\left(\frac{C(tv)}{C(t)}\tilde{A}(\tilde{X}(tv))\right)\,dv=- \frac{1}{2}\tilde{A}(X_{0})\]

Now from ODE \(\dot{\hat{X}}(t)=-\tilde{A}(\tilde{X}(t))-\frac{2\mu}{e^{2\mu t}-1}(\tilde{ X}(t)-X_{0})\), by taking limit both sides by \(t\to 0+\) we have

\[\lim_{t\to 0+}\dot{\hat{X}}(t)=-\tilde{A}(\tilde{X}(0))-\lim_{t\to 0+}\frac{2\mu t}{e^{2\mu t }-1}\frac{\tilde{X}(t)-X_{0}}{t}=-\frac{1}{2}\tilde{A}(X_{0})\]

Therefore, \(\lim_{t\to 0+}\frac{\tilde{X}(t)-X_{0}}{t}=\lim_{t\to 0+}\dot{\hat{X}}(t)\).

**(iv) Uniform boundedness of \(\left\|\dot{X}_{\lambda}(t)\right\|\) and \(\left\|\mathbf{A}_{\lambda}(t)\right\|\) for \(t\in[0,T]\)**

Recall \(U(t)=e^{-2\mu t}\left\|\dot{X}_{\lambda}(t)\right\|^{2}+\left(\frac{2\mu}{e^ {2\mu t}-1}\right)^{2}\left\|X_{\lambda}(t)-X_{0}\right\|^{2}\) is nonincreasing. So from (iii), we have

\[e^{-2\mu t}\left\|\dot{X}_{\lambda}(t)\right\|^{2}+\left(\frac{2 \mu}{e^{2\mu t}-1}\right)^{2}\left\|X_{\lambda}(t)-X_{0}\right\|^{2} \leq\lim_{t\to 0+}\left(e^{-2\mu t}\left\|\dot{X}_{\lambda}(t) \right\|^{2}+\left(\frac{2\mu}{e^{2\mu t}-1}\right)^{2}\left\|X_{\lambda}(t)- X_{0}\right\|^{2}\right)\] \[=\frac{1}{4}\left\|\mathbf{A}_{\lambda}(X_{0})\right\|^{2}+\frac{1} {4}\left\|\mathbf{A}_{\lambda}(X_{0})\right\|^{2}=\frac{1}{2}\left\|\mathbf{A}_ {\lambda}(X_{0})\right\|^{2}\]

Therefore we have from Lemma B.10 (iii)

\[e^{-2\mu t}\left\|\dot{X}_{\lambda}(t)\right\|^{2}\leq\frac{1}{2}\left\| \mathbf{A}_{\lambda}(X_{0})\right\|^{2}\leq\frac{1}{2}\left\|m(\mathbf{A}(X_{0} ))\right\|^{2}\quad\Longrightarrow\quad\left\|\dot{X}_{\lambda}(t)\right\| \leq\frac{e^{\mu T}}{\sqrt{2}}\left\|m(\mathbf{A}(X_{0}))\right\|,\] (21)and by Young's inequality

\[\left\|\mathbf{A}_{\lambda}(X_{\lambda}(t))\right\|^{2} \leq\left\|\dot{X}_{\lambda}(t)+\frac{2\mu}{e^{2\mu t}-1}(X_{ \lambda}(t)-X_{0})\right\|^{2}\] \[=2\left(\left\|\dot{X}_{\lambda}(t)\right\|^{2}+\left(\frac{2\mu} {e^{2\mu t}-1}\right)^{2}\left\|X_{\lambda}(t)-X_{0}\right\|^{2}\right)\] \[\leq 2\left(\frac{e^{2\mu T}}{2}\left\|\mathbf{A}_{\lambda}(X_{0} )\right\|^{2}+\frac{1}{2}\left\|\mathbf{A}_{\lambda}(X_{0})\right\|^{2} \right)=(e^{2\mu T}+1)\left\|\mathbf{A}_{\lambda}(X_{0})\right\|^{2}\leq(e^{2 \mu T}+1)\left\|m(\mathbf{A}(X_{0}))\right\|^{2}.\]

Therefore

\[\left\|\mathbf{A}_{\lambda}(X_{\lambda}(t))\right\|\leq\sqrt{e^{2\mu T}+1} \left\|m(\mathbf{A}(X_{0}))\right\|.\] (22)

## Appendix D Omitted proofs for derivation of anchor ODE (4)

### Preparation for the proof of Theorem 2.1

We first provide the boundedness of trajectories as a lemma. As mentioned in the discussion after Lemma 2.3, boundedness of trajectories is an immediate corollary of Lemma 2.3. However, to address cases that are slightly more generalized, we present a proof using an argument similar to the one used in the proof of Lemma 2.3. Note the proof argument of following lemma is valid for the solution of differential equation (11) with satisfying the assumptions in Theorem 7.1 as well.

**Lemma D.1**.: _(Boundedness of solutions) Suppose \(X(\cdot)\) is the solution of the differential inclusion (3). Then for all \(X_{\star}\in\mathrm{Zer}\mathbf{A}\), \(t\in\left[0,\infty\right)\), following holds._

\[\left\|X(t)-X_{\star}\right\|\leq\left\|X_{0}-X_{\star}\right\|.\]

_And so, \(\left\|X(t)-X_{0}\right\|\leq 2\left\|X_{0}-X_{\star}\right\|\)._

Proof.: It is trivial for \(t=0\), so we may assume \(t>0\).

Take \(X_{\star}\in\mathrm{Zer}\mathbf{A}\). By monotonicity of \(\mathbf{A}\) and Young's inequality, we get the following inequality.

\[\frac{d}{dt}\left\|X(t)-X_{\star}\right\|^{2}=2\left\langle\dot{X }(t),X(t)-X_{\star}\right\rangle =-2\left\langle\tilde{\mathbf{A}}(X(t))+\beta(t)(X(t)-X_{0}),X(t) -X_{\star}\right\rangle\] \[=-2\left\langle\tilde{\mathbf{A}}(X(t))+\beta(t)(X(t)-X_{\star}) -\beta(t)(X_{0}-X_{\star}),X(t)-X_{\star}\right\rangle\] \[\leq-2\beta(t)\left\|X(t)-X_{\star}\right\|^{2}+2\beta(t)\left\langle X _{0}-X_{\star},X(t)-X_{\star}\right\rangle\] \[\leq-2\beta(t)\left\|X(t)-X_{\star}\right\|^{2}+\beta(t)\left( \left\|X_{0}-X_{\star}\right\|^{2}+\left\|X(t)-X_{\star}\right\|^{2}\right)\] \[=-\beta(t)\left\|X(t)-X_{\star}\right\|^{2}+\beta(t)\left\|X_{0}- X_{\star}\right\|^{2}.\]

Now again define \(C(t)=e^{\int_{\epsilon}^{t}\beta(s)ds}\) for some \(v>0\), then we see \(\dot{C}(t)=C(t)\beta(t)\). Moving \(-\beta(t)\left\|X(t)-X_{\star}\right\|^{2}\) to the left hand side and multiplying both sides by \(C(t)\), we have

\[\frac{d}{dt}\left(C(t)\left\|X(t)-X_{\star}\right\|^{2}\right) =C(t)\frac{d}{dt}\left\|X(t)-X_{\star}\right\|^{2}+C(t)\beta(t) \left\|X(t)-X_{\star}\right\|^{2}\] \[\leq C(t)\beta(t)\left\|X_{0}-X_{\star}\right\|^{2}=\frac{d}{dt}C( t)\left\|X_{0}-X_{\star}\right\|^{2}.\]

Integrating both sides from \(\epsilon>0\) to \(t\) we have

\[C(t)\left\|X(t)-X_{\star}\right\|^{2}-C(\epsilon)\left\|X(\epsilon)-X_{\star} \right\|^{2}\leq C(t)\left\|X_{0}-X_{\star}\right\|^{2}-C(\epsilon)\left\|X_{ 0}-X_{\star}\right\|^{2}.\]

As \(\beta>0\), we have \(C\) is nonnegative and nondecreasing, \(\lim_{\epsilon\to 0+}C(\epsilon)\) exists. Taking limit \(\epsilon\to 0+\) both sides we have and dividing both sides by \(C(t)\) we conclude

\[\left\|X(t)-X_{\star}\right\|^{2}\leq\left\|X_{0}-X_{\star}\right\|^{2}.\]

The latter statement holds directly from triangular inequality,

\[\left\|X(t)-X_{0}\right\|\leq\left\|X(t)-X_{\star}\right\|+\left\|X_{\star}-X_{ 0}\right\|\leq 2\left\|X_{0}-X_{\star}\right\|.\]Following lemma shows APPM is an instance of Halpern method. It is immediate from induction, but we state it as a lemma due as its importance.

**Lemma D.2**.: _Consider a method defined as_

\[x^{k+1} =\mathbf{J}_{\mathbf{A}}y^{k}\] \[y^{k+1} =(1-\beta_{k})\left(2x^{k+1}-y^{k}\right)+\beta_{k}x^{0},\]

_for \(k=0,1,\ldots\), with initial condition \(y^{0}=x^{0}\). Then for reflected resolvent \(\mathbf{R}_{\mathbf{A}}\) defined as \(\mathbf{R}_{\mathbf{A}}=2\mathbf{J}_{\mathbf{A}}-\mathbf{I}\), above method is equivalent to_

\[\tilde{y}^{k+1}=\beta_{k}\tilde{y}^{0}+(1-\beta_{k})\,\mathbb{T}\tilde{y}^{k}\]

_when \(\mathbb{T}=\mathbf{R}_{\mathbf{A}}\), \(\tilde{y}^{0}=y^{0}\). Here equivalence means \(\tilde{y}^{k}=y^{k}\) holds for \(k=0,1,\ldots\)._

Proof.: Proof by induction. As \(\tilde{y}^{0}=y^{0}\) by assumption, the statement is true for \(k=0\). Now suppose \(y^{k}=\tilde{y}^{k}\) holds for \(k\in\mathbb{N}\), then

\[\tilde{y}^{k+1}=(1-\beta_{k})\mathbf{R}_{\mathbf{A}}\tilde{y}^{k }+\beta_{0}\tilde{y}^{0} =(1-\beta_{k})\left(2\mathbf{J}_{\mathbf{A}}-\mathbf{I}\right) \tilde{y}^{k}+\beta_{0}\tilde{y}^{0}\] \[=(1-\beta_{k})\left(2\mathbf{J}_{\mathbf{A}}-\mathbf{I}\right)y^ {k}+\beta_{0}y^{0}=(1-\beta_{k})\left(2x^{k+1}-y^{k}\right)+\beta_{0}y^{0}=y^{ k+1}.\]

Thus \(\tilde{y}^{k+1}=y^{k+1}\), the statement is true for \(k+1\). By induction, we get the desired result. 

### Proof of Theorem 2.1

Let \(\mathbf{A}\colon\mathbb{R}^{n}\to\mathbb{R}^{n}\) be a maximal monotone operator, and \(h,\lambda,\delta>0\). Again, denote \(\mathbf{A}_{\lambda}=\frac{1}{\lambda}(\mathbf{I}-\mathbf{A}_{\lambda\mathbf{ A}})=\frac{1}{\lambda}(\mathbf{I}-(\mathbf{I}+\lambda\mathbf{A})^{-1})\). Since various kind of terms appear in the proof, we first organize the terms and notations.

* \(X:\) Solution of differential inclusion, \[\dot{X}\in-\mathbf{A}(X)-\frac{1}{t}(X-X_{0}).\]
* \(X_{\lambda}:\) Solution of differential equation, \[\dot{X}_{\lambda}=-\mathbf{A}_{\lambda}(X_{\lambda})-\frac{1}{t}(X_{\lambda}- X_{0}).\]
* \(X_{\lambda,\delta}:\) Solution of approximated differential equation, \[\dot{X}_{\lambda,\delta}(t)=\begin{cases}-\mathbf{A}_{\lambda}(X_{\lambda, \delta})(t)-\frac{1}{\delta}(X_{\lambda,\delta}(t)-X_{0})&0\leq t<\delta\\ -\mathbf{A}_{\lambda}(X_{\lambda,\delta})(t)-\frac{1}{t}(X_{\lambda,\delta}(t )-X_{0})&t\geq\delta.\end{cases}\] (23)
* \(X_{\lambda,\delta}^{k}:\) Sequence obtained by taking Euler discretization of ODE (23), \[X_{\lambda,\delta}^{k+1}=\begin{cases}X_{\delta}^{k}-\left(2h\mathbf{A}_{ \lambda}(X_{\lambda,\delta}^{k})+\frac{2h}{\delta}(X_{\lambda,\delta}^{k}-X_{ 0})\right)&0\leq k<\frac{\delta}{2h}\\ X_{\delta}^{k}-\left(2h\mathbf{A}_{\lambda}(X_{\lambda,\delta}^{k})+\frac{1}{ \delta}(X_{\lambda,\delta}^{k}-X_{0})\right)&k\geq\frac{\delta}{2h}.\end{cases}\]
* \(x_{h,\lambda}^{k}:\) Sequence obtained from APPM with operator \(h\mathbf{A}_{\lambda}\), i.e. \[x_{h,\lambda}^{k} =\mathbf{J}_{h\Lambda_{\lambda}}y_{h,\lambda}^{k-1}\] \[y_{h,\lambda}^{k} =\frac{k}{k+1}(2x_{h,\lambda}^{k}-y_{h,\lambda}^{k-1})+\frac{1}{k +1}X_{0}.\]
* \(x_{h}^{k}:\) Sequence obtained from APPM with operator \(h\mathbf{A}\), i.e. \[x_{h}^{k} =\mathbf{J}_{h\Lambda}y_{h}^{k-1}\] \[y_{h}^{k} =\frac{k}{k+1}(2x_{h}^{k}-y_{h}^{k-1})+\frac{1}{k+1}X_{0}.\]We want to show for fixed \(T>0\),

\[\lim_{h\to 0+}\sup_{0\leq k<\frac{T}{2h_{n}}}\left\|x_{h}^{k}-X(2hk)\right\|=0.\]

Equivalently we may show for fixed \(T>0\), for every \(\left\{h_{n}\right\}_{n\in\mathbb{N}}\) such that \(h_{n}>0\) and converges to \(0\),

\[\lim_{n\to\infty}\sup_{0\leq k<\frac{T}{2h_{n}}}\left\|x_{h_{n}}^{k}-X(2h_{n}k) \right\|=0.\]

We will show this by considering inequality

\[\left\|x_{h}^{k}-X(2hk)\right\| \leq\left\|X(2hk)-X_{\lambda}(2hk)\right\|+\left\|X_{\lambda}(2hk )-X_{\lambda,\delta}(2hk)\right\|\] (24) \[+\left\|X_{\lambda,\delta}(2hk)-X_{\lambda,\delta}^{k}\right\|+ \left\|X_{\lambda,\delta}^{k}-x_{\lambda}^{k}\right\|+\left\|x_{h,\lambda}^{k }-x_{h}^{k}\right\|=:S\left(h,\lambda,\delta,k\right).\]

Our goal is to show, for every \(\left\{h_{n}\right\}_{n\in\mathbb{N}}\) such that \(h_{n}>0\) and converges to \(0\), there is a sequence \(\left\{(\delta_{n},\lambda_{n})\right\}_{n\in\mathbb{N}}\) such that \(\lim_{n\to\infty}\sup_{0\leq k<\frac{T}{2h_{n}}}S\left(h_{n},\lambda_{n},\delta _{n},k\right)=0\), and thus

\[\lim_{n\to\infty}\sup_{0\leq k<\frac{T}{2h_{n}}}\left\|x_{h_{n}}^{k}-X(2h_{n}k )\right\|\leq\lim_{n\to\infty}\sup_{0\leq k<\frac{T}{2h_{n}}}S\left(h_{n}, \lambda_{n},\delta_{n},k\right)=0.\]

To clarify our goal, we need to find proper \(\left\{(\delta_{n},\lambda_{n})\right\}_{n\in\mathbb{N}}\) in terms of \(\left\{h_{n}\right\}_{n\in\mathbb{N}}\). As \(\left\{h_{n}\right\}_{n\in\mathbb{N}}\) is determined, \(\left\|x_{h_{n}}^{k}-X(2h_{n}k)\right\|\) is fixed and doesn't change by the choice of \(\left\{(\delta_{n},\lambda_{n})\right\}_{n\in\mathbb{N}}\). But if we find \(\left\{(\delta_{n},\lambda_{n})\right\}_{n\in\mathbb{N}}\) that makes \(S\left(h_{n},\lambda_{n},\delta_{n},k\right)\) small, since (24) holds for any choice of \(\delta,\lambda\), right choice of \(\left\{(\delta_{n},\lambda_{n})\right\}_{n\in\mathbb{N}}\) can gaurance \(\left\|x_{h_{n}}^{k}-X(2h_{n}k)\right\|\) is small. Thus to find such \(\left\{(\delta_{n},\lambda_{n})\right\}_{n\in\mathbb{N}}\), we will observe each terms in \(S\) to find the required conditions.

**Lemma D.3**.: _For \(h,\lambda,\delta>0\) following is true._

* \(\left\|X(2hk)-X_{\lambda}(2hk)\right\|=\mathcal{O}\left(\sqrt{\lambda}\right)\)__
* \(\left\|X_{\lambda,\delta}\left(2hk\right)-X_{\lambda,\delta}^{k}\right\|= \mathcal{O}\left(\delta\right)\)__
* \(\left\|x_{h,\lambda}^{k}-x_{h}^{k}\right\|=O(\lambda)\)_._

_Further more if \(0<\frac{h}{\lambda}<\frac{1}{2}\),_

* \(\left\|X_{\lambda,\delta}^{k}-x_{\lambda}^{k}\right\|\)__ \(=\mathcal{O}\left(h\right)+\mathcal{O}\left(h^{2}L_{\lambda,\delta}e^{2L_{ \lambda,\delta}T}\right)\)__

We prove this lemma in next subsection, here we assume the lemma is true and prove Theorem 2.1. Suppose above lemma is true. The calculations are messy but the strategy is simple; balancing the speed of the terms \(h,\delta,\lambda\) going zero to make above terms reach to zero. Above lemma motivate to take sequences as

\[\delta_{n} =\min\left\{\frac{h_{n}}{2},\frac{8MT}{\log_{3}\left(\frac{1}{h_{ n}}\right)}\right\}\] \[\lambda_{n} =\max\left\{\delta_{n},\frac{\sqrt{2}\left\|m(\mathsf{A}(X_{0})) \right\|\delta_{n}}{M},\frac{2T}{\log_{3}\left(\frac{1}{\delta_{n}}\right)} \right\}.\]

where \(M=\max\left\{1,4\sqrt{2}\left\|m(\mathsf{A}(X_{0}))\right\|\right\}\). When \(\lim_{n\to\infty}h_{n}\to 0\) with \(h_{n}>0\), we can easily check \(\lim_{n\to\infty}\delta_{n}=0\) and \(\lim_{n\to\infty}\lambda_{n}=0\). So the cases (i), (ii), (iii) go to zero.

Now observe from the definition of \(\lambda_{n}\) we have,

\[\lambda_{n}\geq\max\left\{\frac{\sqrt{2}\left\|m(\mathsf{A}(X_{0}))\right\|\delta _{n}}{M},\delta_{n}\right\}\quad\Longrightarrow\quad\frac{M}{\delta_{n}}\geq \max\left\{\frac{\sqrt{2}\left\|m(\mathsf{A}(X_{0}))\right\|}{\lambda_{n}}, \frac{1}{\lambda_{n}}\right\}\quad\Longrightarrow\quad L_{\lambda_{n},\delta_ {n}}=\frac{M}{\delta_{n}}.\]

Thus

\[e^{2L_{\lambda,\delta}T} \leq e^{\frac{2MT}{\delta_{n}}}\leq 3^{\frac{2MT}{\delta_{n}}} \leq 3^{\frac{1}{4}\log_{3}\left(\frac{1}{n_{n}}\right)}=\frac{1}{h_{n}^{1/4}}\] \[3^{\frac{T}{\lambda_{n}}} \leq 3^{\frac{MT}{\delta_{n}}}\leq 3^{MT\frac{\log_{3}\left( \frac{1}{h_{n}}\right)}{8MT}}=\frac{1}{h_{n}^{1/8}}\] \[\frac{e^{2\delta_{n}L_{\lambda_{n},\delta_{n}}}}{L_{\lambda_{n},\delta_{n }}} =e^{2M}\frac{\delta_{n}}{M}=\mathcal{O}\left(\delta_{n}\right).\]

Therefore when \(\lim_{n\to\infty}h_{n}\to 0\),

\[h_{n}e^{2L_{\lambda_{n},\delta_{n}}T} \leq h_{n}^{3/4}\to 0\] \[h_{n}^{2}L_{\lambda_{n},\delta_{n}}e^{2L_{\lambda_{n},\delta_{n }}T} =\frac{h_{n}^{2}}{2T}(2L_{\lambda_{n},\delta_{n}}T)e^{2L_{\lambda_{n}, \delta_{n}}T}\leq\frac{h_{n}^{2}}{2T}e^{4L_{\lambda_{n},\delta_{n}}T}\leq \frac{h_{n}^{3/2}}{2T}\to 0\] \[3^{\frac{T}{\lambda_{n}}}h_{n} =h_{n}^{7/8}\to 0\] \[3^{\frac{T}{\lambda_{n}}}\frac{h_{n}}{\lambda_{n}} =3^{\frac{T}{\lambda_{n}}}\frac{T}{\lambda_{n}}\frac{h_{n}}{T} \leq 3^{\frac{2T}{\lambda_{n}}}\frac{h_{n}}{T}\leq\frac{h_{n}^{3/4}}{T}\to 0\] \[3^{\frac{T}{\lambda_{n}}}\frac{h_{n}}{\lambda_{n}}e^{2L_{ \lambda_{n},\delta_{n}}T} \leq\frac{h_{n}^{3/4}}{T}e^{2L_{\lambda_{n},\delta_{n}}T}\leq \frac{h_{n}^{1/2}}{T}\to 0\] \[3^{\frac{T}{\lambda_{n}}}\frac{h_{n}^{2}}{\lambda_{n}}e^{2L_{ \lambda_{n},\delta_{n}}T} \leq\frac{h_{n}^{3/2}}{T}\to 0\] \[3^{\frac{T}{\lambda_{n}}}\delta_{n} \leq 3^{T\frac{\log_{3}\left(\delta_{n}\right)}{2T}}\delta_{n}= \delta_{n}^{1/2}\to 0.\]

As \(\lim_{n\to\infty}h_{n}=0\), without loss of generality we may assume \(h_{n}<1\). Since \(h_{n}>0\) we have \(\lambda_{n}\) is well-defined and satisfies the condition for Lemma D.3. Thus terms for the case (iv) and (v) go to zero as well. Therefore we have \(\lim_{n\to\infty}\sup_{0\leq k<\frac{T}{2h_{n}}}S\left(h_{n},\lambda_{n}, \delta_{n},k\right)=0\), as \(\left\{h_{n}\right\}_{n\in\mathbb{N}}\) is arbitrary, we get the desired result.

#### d.2.1 Proof for case (i), (ii), (iii), (iv) of Lemma D.3

As case proof for (v) need lot of work, we provide it in a different subsection and here we provide the proofs for the cases from (i) to (iv).

1. \(\left\|X(2hk)-X_{\lambda}(2hk)\right\|=\mathcal{O}\left(\sqrt{\lambda}\right)\) This is result of Lemma B.14. Considering (20) with \(p=1\), taking limit \(\nu\to 0\) we know \[\sup_{t\in[0,T]}\left\|X(t)-X_{\lambda}(t)\right\|\leq\sqrt{\frac{\lambda T}{ 2}}\left\|m(\mathsf{A}(X_{0}))\right\|=\mathcal{O}\left(\sqrt{\lambda}\right).\]
2. \(\left\|X_{\lambda}(2hk)-X_{\lambda,\delta}(2hk)\right\|=\mathcal{O}\left(\delta\right)\) This is result of Proposition B.7. Consider (14) with \(\gamma=p=1\) for Lemma B.6 and taking limit \(\nu\to 0\). Then applying Lemma B.10 (iii) we have \[\sup_{t\in[0,T]}\left\|X_{\lambda}(t)-X_{\lambda,\delta}(t)\right\|\leq 2\sqrt{2} \delta\left\|\mathsf{A}_{\lambda}(X_{0})\right\|\leq 2\sqrt{2}\delta\left\|m( \mathsf{A}(X_{0}))\right\|=\mathcal{O}\left(\delta\right).\]3. \(\left\|x_{h,\lambda}^{k}-x_{h}^{k}\right\|=O(\lambda)\) We first show show a general fact about Yosida approximation and resolvent. From [15, Proposition 23.7 (iv)] we have \[\left\|\mathbf{J}_{h\mathsf{A}}(x)-\mathbf{J}_{h\mathsf{A}_{ \lambda}}(x)\right\| =\left\|\mathbf{J}_{h\mathsf{A}}(x)-\left(\mathbf{I}+\frac{1}{1+ \lambda}\left(\mathbf{J}_{(1+\lambda)h\mathsf{A}}-\mathbf{I}\right)\right)(x)\right\|\] \[=\left\|\mathbf{J}_{h\mathsf{A}}(x)-\left(\frac{\lambda}{1+ \lambda}x+\frac{1}{1+\lambda}\mathbf{J}_{(1+\lambda)h\mathsf{A}}(x)\right)\right\|\] \[\leq\frac{1}{1+\lambda}\left\|\mathbf{J}_{h\mathsf{A}}(x)- \mathbf{J}_{(1+\lambda)h\mathsf{A}}(x)\right\|+\frac{\lambda}{1+\lambda}\left\| x-\mathbf{J}_{h\mathsf{A}}(x)\right\|\] From [15, Proposition 23.31 (iii)], we have \[\left\|\mathbf{J}_{h\mathsf{A}}(x)-\mathbf{J}_{(1+\lambda)h\mathsf{A}}(x) \right\|\leq\lambda\left\|\mathbf{J}_{h\mathsf{A}}(x)-x\right\|.\] Combining two facts we get \[\left\|\mathbf{J}_{h\mathsf{A}}(x)-\mathbf{J}_{h\mathsf{A}_{ \lambda}}(x)\right\|\leq\frac{2\lambda}{1+\lambda}\left\|x-\mathbf{J}_{h \mathsf{A}}(x)\right\|.\] From Lemma D.2, we know the iteration of \(y^{k}\) sequence in (5) is equivalent to below sequence \[y^{k+1}=\frac{1}{k+1}X_{0}+\frac{k}{k+1}\left(2\mathbf{J}_{h\mathsf{A}}- \mathbf{I}\right)(y^{k}).\] Using this alternating form we have \[\left\|y_{h}^{k+1}-y_{h,\lambda}^{k+1}\right\| =\frac{k}{k+1}\left\|(2\mathbf{J}_{h\mathsf{A}}-\mathbf{I})(y_{h}^ {k})-(2\mathbf{J}_{h\mathsf{A}_{\lambda}}-\mathbf{I})(y_{h,\lambda}^{k})\right\|\] \[\leq\frac{k}{k+1}\left(\left\|(2\mathbf{J}_{h\mathsf{A}}-\mathbf{ I})(y_{h}^{k})-(2\mathbf{J}_{h\mathsf{A}_{\lambda}}-\mathbf{I})(y_{h}^{k}) \right\|+\left\|\mathbf{R}_{h\mathsf{A}_{\lambda}}(y_{h}^{k})-\mathbf{R}_{h \mathsf{A}_{\lambda}}(y_{h,\lambda}^{k})\right\|\right)\] \[\leq\frac{k}{k+1}\left(2\left\|\mathbf{J}_{h\mathsf{A}}(y_{h}^{k} )-\mathbf{J}_{h\mathsf{A}_{\lambda}}(y_{h}^{k})\right\|+\left\|y_{h}^{k}-y_{h, \lambda}^{k}\right\|\right)\] \[\leq\frac{k}{k+1}\left(\frac{4\lambda}{1+\lambda}\left\|y_{h}^{k} -\mathbf{J}_{h\mathsf{A}}(y_{h}^{k})\right\|+\left\|y_{h}^{k}-y_{h,\lambda}^{k} \right\|\right)\] \[\leq\frac{k}{k+1}\left(\frac{4\lambda}{1+\lambda}\frac{\left\|X_{ 0}-X_{\star}\right\|}{k}+\left\|y_{h}^{k}-y_{h,\lambda}^{k}\right\|\right).\] The first inequality comes from triangular inequality. The second inequality is from nonexpansiveness of reflected resolvent \(\mathbf{R}_{\mathsf{A}}=2\mathbf{J}_{\mathsf{A}}-\mathbf{I}\), [15, Corollary 23.11]. The third inequality is from the inequality shown previously. The last inequality comes from the convergence rate of APPM[35, Theorem 4.1], \(\left\|y_{h}^{k}-\mathbf{J}_{h\mathsf{A}}(y_{h}^{k})\right\|\leq\frac{\left\| X_{0}-X_{\star}\right\|}{k}\). Now multiplying both sides by \(k+1\) and summing up from \(0\) to \(k\) we get \[(k+1)\left\|y_{h}^{k+1}-y_{h,\lambda}^{k+1}\right\|\leq\sum_{i=0}^{k}\frac{4 \lambda}{1+\lambda}\left\|X_{0}-X_{\star}\right\|=(k+1)\frac{4\lambda}{1+ \lambda}\left\|X_{0}-X_{\star}\right\|.\] Finally, from the relation between \(x^{k}\) and \(y^{k}\) in APPM we have \[\left\|x_{h}^{k+1}-x_{h,\lambda}^{k+1}\right\| =\left\|\mathbf{J}_{h\mathsf{A}}(y_{h}^{k+1})-\mathbf{J}_{h \mathsf{A}_{\lambda}}(y_{h,\lambda}^{k+1})\right\|\] \[\leq\left\|\mathbf{J}_{h\mathsf{A}_{\lambda}}(y_{h}^{k+1})-\mathbf{ J}_{h\mathsf{A}_{\lambda}}(y_{h,\lambda}^{k+1})\right\|+\left\|\mathbf{J}_{h \mathsf{A}}(y_{h}^{k+1})-\mathbf{J}_{h\mathsf{A}_{\lambda}}(y_{h}^{k+1})\right\|\] \[\leq\left\|y_{h}^{k+1}-y_{h,\lambda}^{k+1}\right\|+\frac{2\lambda }{1+\lambda}\left\|y_{h}^{k+1}-\mathbf{J}_{h\mathsf{A}}(y_{h}^{k+1})\right\|\] \[\leq\frac{4\lambda}{1+\lambda}\left\|X_{0}-X_{\star}\right\|+\frac {2\lambda}{1+\lambda}\frac{\left\|X_{0}-X_{\star}\right\|}{k+1}\] \[=\left(2+\frac{1}{k+1}\right)\frac{2\lambda}{1+\lambda}\left\|X_{ 0}-X_{\star}\right\|=O(\lambda).\]* \(\left\|X_{\lambda,\delta}\left(2hk\right)-X_{\lambda,\delta}^{k}\right\|=\mathcal{ O}\left(he^{2L_{\lambda,\delta}T}\right)\) From (23), we can consider \(\dot{X}_{\lambda,\delta}\) as of function \(F\colon\mathbb{R}^{n}\times[0,\infty)\to\mathbb{R}^{n}\) defined as below \[F(X,t)=\begin{cases}-\mathds{A}_{\lambda}(X)-\frac{1}{\delta}(X-X_{0})&0\leq t <\delta\\ -\mathds{A}_{\lambda}(X)-\frac{1}{t}(X-X_{0})&t>\delta.\end{cases}\] (25) Note \(F\) is \(2\max\left\{\frac{1}{\lambda},\frac{1}{\delta}\right\}\)-Lipschitz with respect to \(X\). For convenience name \(\alpha=2h\). Define \(\epsilon_{k}:=X_{\lambda,\delta}\left(\alpha k\right)-X_{\lambda,\delta}^{k}\). By definition of Euler discretization and from fundamental theorem of calculus, we have the following \[X_{\lambda,\delta}^{k+1} =X_{\lambda,\delta}^{k}+\alpha F(X_{\lambda,\delta}^{k},\alpha k)\] \[X_{\lambda,\delta}(\alpha(k+1)) =X_{\lambda,\delta}(\alpha k)+\int_{\alpha k}^{\alpha(k+1)}\dot{ X}_{\lambda,\delta}(t)dt\] \[=X_{\lambda,\delta}(\alpha k)+\int_{\alpha k}^{\alpha(k+1)} \left(\dot{X}_{\lambda,\delta}(\alpha k)\right)+\int_{\alpha k}^{t}\ddot{X}_ {\lambda,\delta}(s)ds\right)dt\] \[=X_{\lambda,\delta}(\alpha k)+\alpha F(X_{\lambda,\delta}(\alpha k ),\alpha k)+\int_{\alpha k}^{\alpha(k+1)}\int_{\alpha k}^{t}\ddot{X}_{\lambda, \delta}(s)\,ds\,dt.\] From Lemma B.4 we have \(F(X_{\lambda,\delta},t)=\dot{X}_{\lambda,\delta}(t)\) is Lipschitz continuous respect to \(t\), so \(\ddot{X}_{\lambda,\delta}\) is defined almost everywhere and fundamental theorem of calculus is valid. Therefore we have \[\epsilon_{k+1} =X_{\lambda,\delta}(\alpha(k+1))-X_{\lambda,\delta}^{k+1}\] \[=X_{\lambda,\delta}(\alpha k)-X_{\lambda,\delta}^{k}+\alpha\left( F(X_{\lambda,\delta}(\alpha k),\alpha k)-F(X_{\lambda,\delta}^{k},\alpha k) \right)+\int_{\alpha k}^{\alpha(k+1)}\int_{\alpha k}^{t}\ddot{X}_{\lambda, \delta}(s)\,ds\,dt\] As \(F\) is \(2\max\left\{\frac{1}{\lambda},\frac{1}{\delta}\right\}\)-Lipschitz with respect to first variable, we have \[\left\|\epsilon_{k+1}\right\| \leq\left\|X_{\lambda,\delta}(\alpha k)-X_{\lambda,\delta}^{k} \right\|+\alpha\left\|F(X_{\lambda,\delta}(\alpha k),\alpha k)-F(X_{\lambda, \delta}^{k},\alpha k)\right\|+\int_{\alpha k}^{\alpha(k+1)}\int_{\alpha k}^{t }\left\|\ddot{X}_{\lambda,\delta}(s)\right\|\,ds\,dt\] \[\leq\left(1+2\alpha\max\left\{\frac{1}{\lambda},\frac{1}{\delta} \right\}\right)\left\|\epsilon_{k}\right\|+\int_{\alpha k}^{\alpha(k+1)}\int_{ \alpha k}^{\alpha(k+1)}\left\|\ddot{X}_{\lambda,\delta}(s)\right\|\,ds\,dt.\] Now we observe \(\left\|\ddot{X}_{\lambda,\delta}\right\|\) is bounded. By differentiating \(\dot{X}_{\lambda,\delta}\), as \[\ddot{X}_{\lambda,\delta}=-\frac{d}{dt}\mathds{A}_{\lambda}(X_{\lambda,\delta })+\frac{1}{t^{2}}\left(\dot{X}_{\lambda,\delta}-X_{0}\right)-\frac{1}{t}\dot{ X}_{\lambda,\delta}=-\frac{d}{dt}\mathds{A}_{\lambda}(X_{\lambda,\delta})+\frac{1}{t} \left(-\mathds{A}_{\lambda}(X_{\lambda,\delta})-\dot{X}_{\lambda,\delta} \right)-\frac{1}{t}\dot{X}_{\lambda,\delta}\] for \(t>\delta\), we have for almost every \(t\) \[\ddot{X}_{\lambda,\delta}=\begin{cases}-\frac{d}{dt}\mathds{A}_{\lambda}(X_{ \lambda,\delta})+\frac{1}{\delta}\dot{X}_{\lambda,\delta}&0\leq t<\delta\\ -\frac{d}{dt}\mathds{A}_{\lambda}(X_{\lambda,\delta})-\frac{1}{t}\mathds{A}_{ \lambda}(X_{\lambda,\delta})-\frac{2}{t}\dot{X}_{\lambda,\delta}&t\geq\delta \end{cases}.\] Considering \(\gamma=1\), \(p=1\) to Lemma B.6 and from Lemma B.10 (iv) we have \[\left\|\dot{X}_{\lambda,\delta}(t)\right\|\leq\sqrt{2}\left\|\mathds{A}_{ \lambda}(X_{0})\right\|\leq\sqrt{2}\left\|m(\mathds{A}(X_{0}))\right\|,\] and thus \[\left\|\mathds{A}_{\lambda}(X_{\lambda,\delta}(t))\right\| =\left\|\dot{X}_{\lambda,\delta}(t)+\frac{1}{\max\left\{\delta,t \right\}}(X_{\lambda,\delta}(t)-X_{0})\right\|\] \[\leq\left\|\dot{X}_{\lambda,\delta}(t)\right\|+\frac{1}{\max\left\{ \delta,t\right\}}\left\|X_{\lambda,\delta}(t)-X_{0}\right\|\] \[\leq\left\|\dot{X}_{\lambda,\delta}(t)\right\|+\frac{1}{\max\left\{ \delta,t\right\}}\int_{0}^{t}\left\|\dot{X}_{\lambda,\delta}(s)\right\|ds\leq 2 \sqrt{2}\left\|m(\mathds{A}(X_{0}))\right\|.\]And since \(\mathbf{A}_{\lambda}\) is \(\frac{1}{\lambda}\)-Lipschitz, we know \(\mathbf{A}_{\lambda}\circ X_{\lambda,\delta}\) is \(\frac{1}{\lambda}\sqrt{2}\left\|m(\mathbf{A}(X_{0}))\right\|\)-Lispchitz, thus we have for almost all \(t\),

\[\left\|\frac{d}{dt}\mathbf{A}_{\lambda}(X_{\lambda,\delta}(t))\right\|\leq \frac{\sqrt{2}\left\|m(\mathbf{A}(X_{0}))\right\|}{\lambda}.\]

Applying these facts we have

\[\left\|\ddot{X}_{\lambda,\delta}(t)\right\| \leq\left\|\frac{d}{dt}\mathbf{A}_{\lambda}(X_{\lambda,\delta}(t ))\right\|+\frac{1}{\delta}\left\|\mathbf{A}_{\lambda}(X_{\lambda,\delta}(t)) \right\|+\frac{2}{\delta}\left\|\dot{X}_{\lambda,\delta}(t)\right\|\] \[\leq\frac{\sqrt{2}\left\|m(\mathbf{A}(X_{0}))\right\|}{\lambda} +\frac{4\sqrt{2}}{\delta}\left\|m(\mathbf{A}(X_{0}))\right\|\leq 2\max\left\{ \frac{\sqrt{2}\left\|m(\mathbf{A}(X_{0}))\right\|}{\lambda},\frac{4\sqrt{2}}{ \delta}\left\|m(\mathbf{A}(X_{0}))\right\|\right\}.\]

Therefore

\[\left\|\epsilon_{k+1}\right\|\leq\left(1+2\alpha\max\left\{\frac{1}{\lambda}, \frac{1}{\delta}\right\}\right)\left\|\epsilon_{k}\right\|+2\max\left\{\frac{ \sqrt{2}\left\|m(\mathbf{A}(X_{0}))\right\|}{\lambda},\frac{4\sqrt{2}}{\delta }\left\|m(\mathbf{A}(X_{0}))\right\|\right\}\alpha^{2}\] (26)

Now for \(L_{\delta,\lambda}=\max\left\{\frac{1}{\lambda},\frac{\sqrt{2}\left\|m( \mathbf{A}(X_{0}))\right\|}{\lambda},\frac{1}{\delta},\frac{4\sqrt{2}}{\delta }\left\|m(\mathbf{A}(X_{0}))\right\|\right\}\), we show

\[\left\|\epsilon_{k}\right\|\leq he^{2L_{\lambda,\delta}T}.\]

Multiplying \((1+2\alpha L_{\lambda,\delta})^{-(k+1)}\) to (26) we have

\[(1+2\alpha L_{\lambda,\delta})^{-(k+1)}\left\|\epsilon_{k+1}\right\|\leq(1+2 \alpha L_{\lambda,\delta})^{-k}\left\|\epsilon_{k}\right\|+(1+2\alpha L_{ \lambda,\delta})^{-(k+1)}\,L_{\lambda,\delta}\alpha^{2}\]

As \(\left\|\epsilon_{0}\right\|=\left\|X_{0}-X(0)\right\|=0\), summing up from \(0\) to \(k-1\) we have

\[\left(1+2\alpha L_{\lambda,\delta}\right)^{-k}\left\|\epsilon_{k}\right\| \leq\sum_{i=1}^{k}\left(1+2\alpha L_{\lambda,\delta}\right)^{-i}L_ {\lambda,\delta}\alpha^{2}\] \[=\frac{\left(1+2\alpha L_{\lambda,\delta}\right)^{-1}\left(1-(1+2 \alpha L_{\lambda,\delta})^{-k}\right)}{1-(1+2\alpha L_{\lambda,\delta})^{-1} }L_{\lambda,\delta}\alpha^{2}\] \[=\frac{1}{2}\left(1-(1+2\alpha L_{\lambda,\delta})^{-k}\right)\alpha.\]

Multiplying \((1+2\alpha L_{\lambda,\delta})^{k}\) to both sides and applying \(\alpha=2h\) we have

\[\left\|\epsilon_{k}\right\|\leq\frac{\alpha}{2}\left((1+2\alpha L_{\lambda, \delta})^{k}-1\right)=h\left((1+4hL_{\lambda,\delta})^{k}-1\right).\] (27)

Now from

\[\left(1+4hL_{\lambda,\delta}\right)^{k}\leq\left((1+4hL_{\lambda,\delta})^{ \frac{1}{4hL_{\lambda,\delta}}}\right)^{4hL_{\lambda,\delta}k}\leq e^{4hL_{ \lambda,\delta}k},\]

applying \(k\leq\frac{T}{2h}\)

\[\left\|\epsilon_{k}\right\|\leq h\left(e^{4hL_{\lambda,\delta}k}-1\right)\leq he ^{2L_{\lambda,\delta}T}.\]

Therefore

\[\left\|X_{\lambda,\delta}\left(2hk\right)-X_{\lambda,\delta}^{k}\right\|\leq he ^{2L_{\lambda,\delta}T}=\mathcal{O}\left(he^{2L_{\lambda,\delta}T}\right).\] (28)

### Proof for case (v) of Lemma D.3

As APPM has coefficient \(\frac{1}{k+1}\), we consider \(X_{\lambda,\delta}^{k+1}\) instead of \(X_{\lambda,\delta}^{k}\) due to calculation simplicity. From triangular inequality, we have

\[\left\|X_{\lambda,\delta}^{k}-x_{\lambda}^{k}\right\|\leq\left\|X_{\lambda, \delta}^{k}-X_{\lambda,\delta}^{k+1}\right\|+\left\|X_{\lambda,\delta}^{k+1}-x_{ \lambda}^{k}\right\|.\]We will show

\[\left\|X_{\lambda,\delta}^{k}-X_{\lambda,\delta}^{k+1}\right\| =\mathcal{O}\left(h\right)+\mathcal{O}\left(h^{2}L_{\lambda,\delta }e^{2L_{\lambda,\delta}T}\right)\] \[\left\|X_{\lambda,\delta}^{k+1}-x_{\lambda}^{k}\right\| =3^{\frac{T}{\lambda}}\left(\mathcal{O}\left(\frac{h}{\lambda} \right)+\mathcal{O}\left(\frac{h}{\lambda}e^{2L_{\lambda,\delta}T}\right)+ \mathcal{O}(h)+\mathcal{O}\left(\frac{h^{2}}{\lambda}e^{2L_{\lambda,\delta}T} \right)+\mathcal{O}\left(\frac{e^{2\delta L_{\lambda,\delta}}}{L_{\lambda, \delta}}\right)+\mathcal{O}\left(\delta\right)\right).\]

First one is simple. Since \(X_{\lambda,\delta}^{k+1}=X_{\lambda,\delta}^{k}+2hF(X_{\lambda,\delta},2hk)\) and \(F\) is \(2\max\left\{\frac{1}{\lambda},\frac{1}{\delta}\right\}\)-Lipschitz with respect to the first variable, we have

\[\left\|X_{\lambda,\delta}^{k}-X_{\lambda,\delta}^{k+1}\right\| =2h\left\|F(X_{\lambda,\delta}^{k},2hk)\right\|\] \[\leq 2h\left\|F(X_{\lambda,\delta}(2hk),2hk)\right\|+2h\left\|F(X _{\lambda,\delta}(2hk),2hk)-F(X_{\lambda,\delta}^{k},2hk)\right\|\] \[\leq 2h\left\|\dot{X}_{\lambda,\delta}(2hk)\right\|+4h\max\left\{ \frac{1}{\lambda},\frac{1}{\delta}\right\}\left\|X_{\lambda,\delta}(2hk)-X_{ \lambda,\delta}^{k}\right\|\] \[\leq 2\sqrt{2}h\left\|m(\mathbf{A}_{\lambda}(X_{0}))\right\|+4h^{2 }L_{\lambda,\delta}e^{2L_{\lambda,\delta}T}\] \[=\mathcal{O}\left(h\right)+\mathcal{O}\left(h^{2}L_{\lambda, \delta}e^{2L_{\lambda,\delta}T}\right).\]

Second one is complicated, we present our proof with dividing steps to subsections.

d.3.1 Recursive inequality for \(\epsilon_{k}=\left\|X_{\lambda,\delta}^{k+1}-x_{h,\lambda}^{k}\right\|\)

Define \(\epsilon_{k}=\left\|X_{\lambda,\delta}^{k+1}-x_{h,\lambda}^{k}\right\|\). Recall, \(X_{\lambda,\delta}\) was solution of approximated ODE

\[\dot{X}_{\lambda,\delta}(t)=F(X_{\lambda,\delta},t)=\begin{cases}-\mathbf{A}_{ \lambda}(X_{\lambda,\delta})(t)-\frac{1}{\delta}(X(t)-X_{0})&0\leq t<\delta\\ -\mathbf{A}_{\lambda}(X_{\lambda,\delta})(t)-\frac{1}{t}(X(t)-X_{0})&t\geq \delta\end{cases}.\]

We now wish to write \(\epsilon_{k+1}\) in terms of \(\epsilon_{k}\). As \(\epsilon_{k+1}\) involves \(X_{\lambda,\delta}^{k+2}\), we first write it explicitly.

\[X_{\lambda,\delta}^{k+2} =X_{\lambda,\delta}^{k+1}+2hF\left(X_{\lambda,\delta}^{k+1},2h(k+ 1)\right)\] \[=\begin{cases}X_{\lambda,\delta}^{k+1}-\left(2h\mathbf{A}_{ \lambda}(X_{\lambda,\delta}^{k+1})+\frac{2h}{\delta}(X_{\lambda,\delta}^{k+1} -X_{0})\right)&0\leq k+1<\frac{\delta}{2h}\\ X_{\lambda,\delta}^{k+1}-\left(2h\mathbf{A}_{\lambda}(X_{\lambda,\delta}^{k+1} )+\frac{1}{k+1}(X_{\lambda,\delta}^{k+1}-X_{0})\right)&k+1\geq\frac{\delta}{2 h}.\end{cases}\]

Now we find recursive inequality considering two cases.

1. \(k+1\geq\frac{\delta}{2h}\) Recall APPM (5) was defined as \[x_{h,\lambda}^{k} =\mathbf{J}_{h\mathbf{A}_{\lambda}}y_{h,\lambda}^{k-1}\] \[y_{h,\lambda}^{k} =\frac{k}{k+1}(2x_{h,\lambda}^{k}-y_{h,\lambda}^{k-1})+\frac{1}{k +1}X_{0},\] and substituting \(y_{h,\lambda}^{k-1}=x_{h,\lambda}^{k}+h\mathbf{A}_{\lambda}(x_{h,\lambda}^{k})\), we get a one line expression \[x_{h,\lambda}^{k+1} =\frac{k}{k+1}x_{h,\lambda}^{k}-h\left(\mathbf{A}_{\lambda}(x_{h,\lambda}^{k+1})+\frac{k}{k+1}\mathbf{A}_{\lambda}(x_{h,\lambda}^{k})\right)+ \frac{1}{k+1}X_{0}.\] Rewriting \(X_{\lambda,\delta}^{k+2}\) to make easier to compare with above, \[X_{\lambda,\delta}^{k+2} =\frac{k}{k+1}X_{\lambda,\delta}^{k+1}-h\left(\mathbf{A}_{ \lambda}(X_{\lambda,\delta}^{k+2})+\frac{k}{k+1}\mathbf{A}_{\lambda}(X_{ \lambda,\delta}^{k+1})\right)+\frac{1}{k+1}X_{0}\] \[\quad+h\left(\mathbf{A}_{\lambda}(X_{\lambda,\delta}^{k+2})- \mathbf{A}_{\lambda}(X_{\lambda,\delta}^{k+1})\right)-\frac{h}{k+1}\mathbf{A}_ {\lambda}(X_{\lambda,\delta}^{k+1}).\]

[MISSING_PAGE_FAIL:40]

For second inequality follows from the fact \(0<\frac{h}{\lambda}<\frac{1}{2}\), which implies \(0<\frac{\left(1-\frac{h}{\lambda}\right)^{i+1}}{\left(1+\frac{h}{\lambda}\right) ^{i}}\leq 1\) and \(1<\frac{1+\frac{h}{\lambda}}{1-\frac{h}{\lambda}}\). Observe, \(f(x)=\left(\frac{1+x}{1-x}\right)^{\frac{1}{x}}\) is nondecreasing in \(x\in(0,1)\) since

\[f^{\prime}(x)=-\frac{\left(\frac{1+x}{1-x}\right)^{\frac{1}{x}}\left(\left(x^{2 }-1\right)\log\left(\frac{1+x}{1-x}\right)+2x\right)}{x^{2}\left(x^{2}-1\right)}.\]

Therefore, from \(f\left(\frac{1}{2}\right)=9\) we have

\[\epsilon_{k}\leq f\left(\frac{1}{2}\right)^{\frac{T}{2\lambda}}\frac{1}{k} \sum_{i=0}^{k-1}(i+1)e_{i}=3^{\frac{T}{\lambda}}\frac{1}{k}\sum_{i=0}^{k-1}(i +1)e_{i}.\]

Now we show

\[\frac{1}{k}\sum_{i=0}^{k-1}(i+1)e_{i}=\mathcal{O}\left(\frac{h}{\lambda} \right)+\mathcal{O}\left(\frac{h}{\lambda}e^{2L_{\lambda,\delta}T}\right)+ \mathcal{O}(h)+\mathcal{O}\left(\frac{h^{2}}{\lambda}e^{2L_{\lambda,\delta}T} \right)+\mathcal{O}\left(\delta\right)+\mathcal{O}\left(\frac{e^{2\delta L_{ \lambda,\delta}}}{L_{\lambda,\delta}}\right)\]

for \(0<T<\infty\), \(0\leq k<\frac{T}{2h}\). Name \(N_{k}=\min\left\{k,\left\lfloor\frac{\delta}{2h}\right\rfloor\right\}\). From the definition of \(e_{i}\) we have

\[\frac{1}{k}\sum_{i=0}^{k-1}(i+1)e_{i}\] \[=\frac{1}{k}\sum_{i=0}^{k-1}h(i+1)\left(\frac{1}{\lambda}\left\| X_{\lambda,\delta}^{i+2}-X_{\lambda,\delta}^{i+1}\right\|+\frac{1}{i+1}\left\| \mathbf{A}_{\lambda}(X_{\lambda,\delta}^{i+1})\right\|\right)+\frac{1}{k}\sum_ {i=0}^{N_{k}-1}(i+1)\left|\frac{1}{i+1}-\frac{2h}{\delta}\right|\left\|X_{ \lambda,\delta}^{i+1}-X_{0}\right\|\] \[\leq\sum_{i=0}^{k-1}h\left(\frac{1}{\lambda}\left\|X_{\lambda, \delta}^{i+2}-X_{\lambda,\delta}^{i+1}\right\|+\frac{1}{k}\left\|\mathbf{A}_{ \lambda}(X_{\lambda,\delta}^{i+1})\right\|\right)+\frac{1}{k}\sum_{i=0}^{N_{k }-1}\left\|X_{\lambda,\delta}^{i+1}-X_{0}\right\|,\]

where inequality follows from the fact \(\frac{i+1}{k}\leq 1\) for \(0\leq i\leq k-1\) and \(\frac{1}{i+1}\geq\frac{2h}{\delta}\) for \(0\leq i\leq N_{k}-1\).

Now let's observe each term. From Lemma B.12 and Lemma D.1 we know

\[\left\|\dot{X}_{\lambda,\delta}(t)\right\| \leq\sqrt{2}\left\|\mathbf{A}_{\lambda}(X_{0})\right\|\leq\sqrt{ 2}\left\|m(\mathbf{A}(X_{0}))\right\|\] \[\left\|\mathbf{A}_{\lambda}(X(t))\right\| \leq\left\|\mathbf{A}_{\lambda}(X_{0})\right\|\leq\left\|m( \mathbf{A}(X_{0}))\right\|\] \[\left\|X_{\lambda,\delta}(t)-X_{0}\right\| \leq 2\left\|X_{0}-X_{\star}\right\|.\]

Name \(M=\max\left\{\sqrt{2}\left\|m(\mathbf{A}(X_{0}))\right\|,2\left\|X_{0}-X_{ \star}\right\|\right\}\).

1. \(\frac{h}{\lambda}\left\|X_{\lambda,\delta}^{i+2}-X_{\lambda,\delta}^{i+1}\right\|\) First observe \[h\sum_{i=0}^{k-1}\frac{1}{\lambda}\left\|X_{\lambda,\delta}(2h(i +2))-X_{\lambda,\delta}(2h(i+1))\right\|\] \[=\frac{h}{\lambda}\sum_{i=0}^{k-1}\left\|\int_{2h(i+1)}^{2h(i+2)} \dot{X}_{\lambda,\delta}(t)dt\right\|\leq\frac{h}{\lambda}\sum_{i=0}^{k-1} \int_{2h(i+1)}^{2h(i+2)}\left\|\dot{X}_{\lambda,\delta}(t)\right\|dt\leq\frac {h}{\lambda}k(2h)M\leq\frac{h}{\lambda}TM.\]Thus,

\[h\sum_{i=0}^{k-1}\frac{1}{\lambda}\left\|X_{\lambda,\delta}^{i+2}-X _{\lambda,\delta}^{i+1}\right\|\] \[\leq\frac{h}{\lambda}\left(TM+2\sum_{i=0}^{k-1}he^{2L_{\lambda, \delta}T}\right)\] \[\leq\frac{h}{\lambda}\left(TM+Te^{2L_{\lambda,\delta}T}\right).\]

Therefore

\[\frac{h}{\lambda}\sum_{i=0}^{k-1}\left\|X_{\lambda,\delta}^{i+2}-X _{\lambda,\delta}^{i+1}\right\|=\mathcal{O}\left(\frac{h}{\lambda}\right)+ \mathcal{O}\left(\frac{h}{\lambda}e^{2L_{\lambda,\delta}T}\right).\]
2. \(\frac{h}{k}\left\|\mathbf{A}_{\lambda}(X_{\lambda,\delta}^{i+1})\right\|\) Since \(\mathbf{A}_{\lambda}\) is \(\frac{1}{\lambda}\)-Lipschitz continuous and from (28) \[\leq\frac{h}{k}\sum_{i=0}^{k-1}\left(\frac{h}{\lambda}e^{2L_{ \lambda,\delta}T}+M\right)=h\left(\frac{h}{\lambda}e^{2L_{\lambda,\delta}T}+M \right).\]

Therefore

\[\sum_{i=0}^{k-1}\frac{h}{k}\left\|\mathbf{A}_{\lambda}(X_{\lambda, \delta}^{i+1})\right\|=\mathcal{O}(h)+\mathcal{O}\left(\frac{h^{2}}{\lambda} e^{2L_{\lambda,\delta}T}\right).\]
3. \(\frac{1}{k}\sum_{i=0}^{N_{k}-1}\left\|X_{\lambda,\delta}^{i+1}-X_{0}\right\|\) for \(N_{k}=\min\left\{k,\left\lfloor\frac{\delta}{2h}\right\rfloor\right\}\). First, observe \[\left\|X_{0}-X_{\lambda,\delta}(t)\right\|=\left\|\int_{0}^{t}\dot{X}_{\lambda, \delta}(s)ds\right\|\leq\int_{0}^{t}\left\|\dot{X}_{\lambda,\delta}(s)\right\| ds\leq tM.\] From (27) \[\left\|X_{\lambda,\delta}^{i+1}-X_{0}\right\| \leq\left\|X_{\lambda,\delta}^{i+1}-X_{\lambda,\delta}(2h(i+1)) \right\|+\left\|X_{\lambda,\delta}(2h(i+1))-X_{0}\right\|\] \[\leq h\left(1+4hL_{\lambda,\delta}\right)^{i+1}+2h(i+1)M.\] Now as \(i+1\leq N_{k}=\min\left\{k,\left\lfloor\frac{\delta}{2h}\right\rfloor\right\}\), \[\frac{1}{k}\sum_{i=0}^{N_{k}-1}\left\|X_{\lambda,\delta}^{i+1}-X_{0}\right\| \leq\frac{1}{k}\sum_{i=0}^{N_{k}-1}h\left(\left(1+4hL_{\lambda,\delta} \right)^{i+1}+2(i+1)M\right)\] \[\leq\frac{h}{k}\frac{\left(1+4hL_{\lambda,\delta}\right)^{N_{k}}- 1}{4hL_{\lambda,\delta}}+2hM\sum_{i=0}^{N_{k}-1}\frac{i+1}{k}\] \[\leq\frac{e^{4hL_{\lambda,\delta}N_{k}}}{kL_{\lambda,\delta}}+2hN _{k}M\] \[\leq\frac{e^{2\delta L_{\lambda,\delta}}}{L_{\lambda,\delta}}+2 \delta M=\mathcal{O}\left(\frac{e^{2\delta L_{\lambda,\delta}}}{L_{\lambda, \delta}}\right)+\mathcal{O}\left(\delta\right).\]From (i), (ii), (iii) we have

\[\frac{1}{k}\sum_{i=0}^{k-1}(i+1)e_{i}=\mathcal{O}\left(\frac{h}{\lambda}\right)+ \mathcal{O}\left(\frac{h}{\lambda}e^{2L_{\lambda,\delta}T}\right)+\mathcal{O}(h )+\mathcal{O}\left(\frac{h^{2}}{\lambda}e^{2L_{\lambda,\delta}T}\right)+ \mathcal{O}\left(\frac{e^{2\delta L_{\lambda,\delta}}}{L_{\lambda,\delta}} \right)+\mathcal{O}\left(\delta\right).\]

Therefore,

\[\epsilon_{k}=3^{\frac{\mathcal{T}}{\lambda}}\left(\mathcal{O}\left(\frac{h}{ \lambda}\right)+\mathcal{O}\left(\frac{h}{\lambda}e^{2L_{\lambda,\delta}T} \right)+\mathcal{O}(h)+\mathcal{O}\left(\frac{h^{2}}{\lambda}e^{2L_{\lambda, \delta}T}\right)+\mathcal{O}\left(\frac{e^{2\delta L_{\lambda,\delta}}}{L_{ \lambda,\delta}}\right)+\mathcal{O}\left(\delta\right)\right).\]

### Derivation of ODE (4) from EAG and FEG

#### d.4.1 Derivation from EAG

For \(L\)-Lipschitz continuous monotone operator \(\mathbf{A}\) and stepsize \(h>0\), EAG-C [75] is defined as

\[z^{k+\frac{1}{2}} =z^{k}-\frac{1}{k+2}\left(z^{k}-z^{0}\right)-h\mathbf{A}(z^{k})\] \[z^{k+1} =z^{k}-\frac{1}{k+2}\left(z^{k}-z^{0}\right)-h\mathbf{A}(z^{k+ \frac{1}{2}}).\]

Dividing the second line by \(h\) and reorganizing we have

\[\frac{z^{k+1}-z^{k}}{h} =-\mathbf{A}(z^{k+\frac{1}{2}})-\frac{1}{h(k+2)}\left(z^{k}-z^{0}\right)\] \[=-\mathbf{A}(z^{k})-\frac{1}{h(k+2)}\left(z^{k}-z^{0}\right)- \left(\mathbf{A}(z^{k+\frac{1}{2}})-\mathbf{A}(z^{k})\right).\]

Identify \(hk=t\), \(z^{k}=X(t)\), \(z^{0}=X_{0}\). As \(z^{k}\) is a converging sequence [76], it is bounded. Thus we see

\[\left\|\mathbf{A}(z^{k+\frac{1}{2}})-\mathbf{A}(z^{k})\right\| \leq L\left\|z^{k+\frac{1}{2}}-z^{k}\right\|\] \[=L\left\|\frac{h}{h(k+2)}(z^{k}-z^{0})+h\mathbf{A}(z^{k})\right\|\] \[=Lh\left\|\frac{1}{t+2h}(X(t)-X_{0})+\mathbf{A}(X(t))\right\|= \mathcal{O}\left(h\right).\]

Therefore taking limit \(h\to 0+\) we have

\[\dot{X}(t)=-\mathbf{A}(X(t))-\frac{1}{t}(X(t)-X_{0}).\]

#### d.4.2 Derivation from FEG

For \(L\)-Lipschitz continuous monotone operator \(\mathbf{A}\) and stepsize \(h>0\), FEG [42] is defined as

\[z^{k+\frac{1}{2}} =z^{k}-\frac{1}{k+1}\left(z^{k}-z^{0}\right)-\frac{k}{k+1}h \mathbf{A}(z^{k})\] \[z^{k+1} =z^{k}-\frac{1}{k+1}\left(z^{k}-z^{0}\right)-h\mathbf{A}(z^{k+ \frac{1}{2}}).\]

Dividing the second line by \(h\) and reorganizing we have

\[\frac{z^{k+1}-z^{k}}{h} =-\mathbf{A}(z^{k+\frac{1}{2}})-\frac{1}{h(k+1)}\left(z^{k}-z^{0}\right)\] \[=-\mathbf{A}(z^{k})-\frac{1}{h(k+1)}\left(z^{k}-z^{0}\right)- \left(\mathbf{A}(z^{k+\frac{1}{2}})-\mathbf{A}(z^{k})\right).\]Identif \(hk=t\), \(z^{k}=X(t)\), \(z^{0}=X_{0}\). As \(z^{k}\) is a converging sequence [76], it is bounded. Thus we see

\[\left\|\mathbf{A}(z^{k+\frac{1}{2}})-\mathbf{A}(z^{k})\right\| \leq L\left\|z^{k+\frac{1}{2}}-z^{k}\right\|\] \[=L\left\|\frac{h}{h(k+1)}(z^{k}-z^{0})+h\frac{hk}{h(k+1)}\mathbf{ A}(z^{k})\right\|\] \[=Lh\left\|\frac{1}{t+h}(X(t)-X_{0})+\frac{t}{t+h}\mathbf{A}(X(t)) \right\|=\mathcal{O}\left(h\right).\]

Therefore taking limit \(h\to 0+\) we have

\[\dot{X}(t)=-\mathbf{A}(X(t))-\frac{1}{t}(X(t)-X_{0}).\]

## Appendix E Proof of convergence analysis for monotone \(\mathbf{A}\)

### Extending \(\tilde{\mathbf{A}}\) to \([0,\infty)\)

**Lemma E.1**.: _Suppose \(\mathbf{A}\) is a maximal monotone operator. Let \(X\) be the solution for (3). Define \(S\) as_

\[S=\left\{t\in[0,\infty)\ |\ \dot{X}(t)\in-\mathbf{A}(X(t))-\beta(t)(X(t)-X_{0}) \text{ is true}\right\}.\]

_Then as \(X\) is the solution for (3), we know \([0,\infty)\backslash S\) is of measure zero._

_Define \(\tilde{\mathbf{A}}(X)\colon S\to\mathbb{R}^{n}\) as_

\[\tilde{\mathbf{A}}(X)(t)=-\dot{X}(t)-\beta(t)(X(t)-X_{0})\] (29)

_and denote \(\tilde{\mathbf{A}}(X(t))=\tilde{\mathbf{A}}(X)(t)\). Assume for every \(T>0\), there is \(M>0\) such that for all \(t\in S\cap[0,T]\)_

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|\leq M.\]

_Then \(\tilde{\mathbf{A}}(X)\) can be extended to \(t\in[0,\infty)\) with satisfying following properties._

* \(\tilde{\mathbf{A}}(X(t))\in\mathbf{A}(X(t))\) _for all_ \(t\in[0,\infty)\)_._
* _For_ \(t\in[0,\infty)\backslash S\)_, there is a sequence_ \(\left\{t_{k}\right\}_{k\in\mathbb{N}}\) _such that_ \(t_{k}\in S\)_,_ \(\lim_{k\to\infty}t_{k}=t\) _and_ \(\tilde{\mathbf{A}}(X(t_{k}))\) _converges to_ \(\tilde{\mathbf{A}}(X(t))\)_._

Proof.: Take \(t\in[0,\infty)\), and take a sequence \(\left\{t_{n}\right\}_{n\in\mathbb{N}}\) such that \(t_{n}\in S\) and \(\lim_{n\to\infty}t_{n}=t\). As a converging sequence \(t_{n}\) is bounded, there is \(T>0\) such that \(t_{n}\in[0,T]\). For that \(T\), we have \(\left\|\tilde{\mathbf{A}}(X(t_{n}))\right\|\leq M\) by the assumption. As \(n\mapsto\tilde{\mathbf{A}}(X(t_{n}))\) is a bounded sequence, there is a subsequence \(\left\{t_{n_{k}}\right\}_{k\in\mathbb{N}}\) such that \(k\mapsto\tilde{\mathbf{A}}(X(t_{n_{k}}))\) converges. Name the limit as \(u=\lim_{k\to\infty}\tilde{\mathbf{A}}(X(t_{n_{k}}))\). On the other hand, as \(X\) is a continuous curve we have \(\lim_{k\to\infty}X(t_{n_{k}})=X(t)\). Then since \(\mathbf{A}\) is maximally monotone, from [15, Proposition 20.38] we have \((X(t),u)\in\operatorname{Gra}\mathbf{A}\). Defining \(\tilde{\mathbf{A}}(X(t))\) as \(u\), we get the desired result. 

**Corollary E.2**.: _Let \(X\) be the solution for (6). Then \(\tilde{\mathbf{A}}(X)\) defined as (29) has extension with properties stated in Lemma E.1._

Proof.: First consider the case \(\beta(t)=\frac{\gamma}{t^{p}}\), \(p>0\), \(\gamma>0\). Take \(T>0\). It is enough to show there is \(M>0\) such that for \(t\in S\cap[0,T]\)

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|\leq M.\]

Recall from Proposition B.11 for Yosida approximation \(\mathbf{A}_{\lambda}\), we denoted \(X_{\lambda}\) as the solution of

\[\dot{X}_{\lambda}=-\mathbf{A}_{\lambda}(X_{\lambda})-\frac{\gamma}{t^{p}}(X_ {\lambda}-X_{0}),\]

and we have shown there is a sequence \(\left\{\lambda_{n}\right\}_{n\in\mathbb{N}}\) such that \(X_{\lambda_{n}}\) uniformly converges to \(X\) on \([0,T]\).

From Lemma B.12, we see for \(h\neq 0\)

\[\left\|\frac{X_{\lambda}(t+h)-X_{\lambda}(t)}{h}\right\|\leq\frac{ \int_{t}^{t+h}\left\|\dot{X}_{\lambda}(s)\right\|ds}{h}\leq\frac{\int_{t}^{t+h} M_{dot}(T)ds}{h}=M_{dot}(T),\]

thus

\[\left\|\frac{X(t+h)-X(t)}{h}\right\|=\lim_{\lambda\to 0+}\left\|\frac{X_{ \lambda}(t+h)-X_{\lambda}(t)}{h}\right\|\leq M_{dot}(T).\]

Therefore for \(t\in S\cap[0,T]\), \(\dot{X}(t)=\lim_{h\to 0}\frac{X(t+h)-X(t)}{h}\) holds, we conclude

\[\left\|\dot{X}(t)\right\|=\lim_{h\to 0}\left\|\frac{X(t+h)-X(t)}{h}\right\|\leq M _{dot}(T).\]

And also from Lemma B.12, for \(t\in[0,T]\) we have

\[\left\|\frac{\gamma}{t^{p}}(X_{\lambda}(t)-X_{0})\right\|=\left\|\dot{X}_{ \lambda}(t)+\mathbf{A}_{\lambda}(t)\right\|\leq\left\|\dot{X}_{\lambda}(t) \right\|+\left\|\mathbf{A}_{\lambda}(t)\right\|\leq M_{dot}(T)+M_{\mathbf{A} }(T),\]

therefore

\[\left\|\frac{\gamma}{t^{p}}(X(t)-X_{0})\right\|=\lim_{\lambda\to 0+} \left\|\frac{\gamma}{t^{p}}(X_{\lambda}(t)-X_{0})\right\|\leq M_{dot}(T)+M_{ \mathbf{A}}(T).\]

Gathering the result, for \(t\in S\cap[0,T]\) we have

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|=\left\|\dot{X}(t)+\frac{\gamma}{t^{p} }(X(t)-X_{0})\right\|\leq\left\|\dot{X}(t)\right\|+\left\|\frac{\gamma}{t^{p} }(X(t)-X_{0})\right\|\leq 2M_{dot}(T)+M_{\mathbf{A}}(T).\]

### Proof of Proposition 3.2

We prove this theorem by deriving the energy with dilated coordinate \(W(t)=C(t)(X(t)-X_{0})\) and conservation law from [67]. Think of dilated coordinate \(W(t)=C(t)(X(t)-X_{0})\). As we're considering the case \(\tilde{\mathbf{A}}\) is Lipschitz continuous, the differential inclusion (3) becomes ODE

\[\dot{X}(t)=-\tilde{\mathbf{A}}(X(t))-\beta(t)(X(t)-X_{0}).\]

Rewriting the ODE in terms of \(W\), we have

\[\frac{1}{C(t)}\left(\dot{W}(t)-\beta(t)W(t)\right)=-\tilde{\mathbf{A}}(X(W(t),t))-\frac{\beta(t)}{C(t)}W(t)\]

where \(X(W(t),t)=X(t)=\frac{W(t)}{C(t)}+X_{0}\). Organizing, we have

\[0=\dot{W}(t)+C(t)\tilde{\mathbf{A}}(X(W(t),t)).\]

From Lemma B.4 we know \(\tilde{\mathbf{A}}(X(W,t))\) is differentiable almost everywhere, by differentiating we obtain second order ODE which holds almost everywhere

\[0 =\ddot{W}+C(t)\beta(t)\tilde{\mathbf{A}}(X(W(t),t))+C(t)\frac{d }{dt}\tilde{\mathbf{A}}(X(W(t),t))\] \[=\ddot{W}-\beta(t)\dot{W}+C(t)\frac{d}{dt}\tilde{\mathbf{A}}(X(W (t),t)).\] (30)

Now by taking inner product with \(\dot{W}\) and integrating, we get equality which was refered as conservation law in [67]

\[E_{1}\equiv\frac{1}{2}\left\|\dot{W}(t)\right\|^{2}-\int_{t_{0}}^{t}\beta(s) \left\|\dot{W}(s)\right\|^{2}ds+\int_{t_{0}}^{t}C(s)\left\langle\frac{d}{ds} \tilde{\mathbf{A}}(X(s)),\dot{W}(s)\right\rangle ds.\]Since \(\dot{W}(t)=C(t)\left(\dot{X}(t)+\beta(t)(X(t)-X_{0})\right)\), we can rewrite the integrand in the last term as

\[\int_{t_{0}}^{t}C(s)\left\langle\frac{d}{ds}\tilde{\mathbf{A}}(X(s)),\dot{W}(s) \right\rangle ds=\int_{t_{0}}^{t}C(s)^{2}\left\langle\frac{d}{ds}\tilde{ \mathbf{A}}(X(s)),\dot{X}(s)\right\rangle ds+\int_{t_{0}}^{t}\left\langle \frac{d}{ds}\tilde{\mathbf{A}}(X(s)),C(s)\beta(s)W(s)\right\rangle ds.\]

Note the purpose was to obtain the first term, which is nonnegative due to monotonicity of \(\tilde{\mathbf{A}}\). Now taking integration by parts to the second term we have

\[\int_{t_{0}}^{t}\left\langle\frac{d}{ds}\tilde{\mathbf{A}}(X(s)),C (s)\beta(s)W(s)\right\rangle ds\] \[=\left[\left\langle\tilde{\mathbf{A}}(X(W,s)),C(s)\beta(s)W(s) \right\rangle\right]_{t_{0}}^{t}-\int_{t_{0}}^{t}\left\langle\tilde{\mathbf{A }}(X(W,s)),\left(C(s)\beta(s)^{2}+C(s)\dot{\beta}(s)\right)W(s)+C(s)\beta(s) \dot{W}(s)\right\rangle ds\] \[=\left[\left\langle\tilde{\mathbf{A}}(X(W,s)),C(s)\beta(s)W(s) \right\rangle\right]_{t_{0}}^{t}+\int_{t_{0}}^{t}\beta(s)\left\|\dot{W}(s) \right\|^{2}ds+\int_{t_{0}}^{t}\left(\beta(s)^{2}+\dot{\beta}(s)\right)\left\langle \dot{W}(s),W(s)\right\rangle ds\] \[=\left[\left\langle\tilde{\mathbf{A}}(X(W,s)),C(s)\beta(s)W(s) \right\rangle\right]_{t_{0}}^{t}+\int_{t_{0}}^{t}\beta(s)\left\|\dot{W}(s) \right\|^{2}ds\] \[\quad+\left[\left(\beta(s)^{2}+\dot{\beta}(s)\right)\frac{1}{2} \left\|W(s)\right\|^{2}\right]_{t_{0}}^{t}-\frac{1}{2}\int_{t_{0}}^{t}\left(2 \beta(s)\dot{\beta}(s)+\ddot{\beta}(s)\right)\left\|W(s)\right\|^{2}ds\]

On the second equality, we used the fact \(\dot{W}(t)=-C(t)\tilde{\mathbf{A}}(X(W(t),t))\). Note the fundamental theorem of calculus for \(\left\langle\tilde{\mathbf{A}}(X(W,s)),C(s)\beta(s)W(s)\right\rangle\) is valid since \(\tilde{\mathbf{A}}(X(W,s))\) is Lipschitz continuous and \(C(s)\beta(s)W(s)\) is continuously differentiable in \([t_{0},t]\), so their inner product is absolutely continuous in \([t_{0},t]\).

Observe the integrand in the last term can be rewritten as

\[\left(2\beta(s)\dot{\beta}(s)+\dot{\beta}(s)\right)\left\|W(s)\right\|^{2}=C( s)^{2}\left(2\beta(s)\dot{\beta}(s)+\ddot{\beta}(s)\right)\left\|X(s)-X_{0} \right\|^{2}=\frac{d}{ds}\left(C(s)^{2}\dot{\beta}(s)\right)\left\|X(s)-X_{0 }\right\|^{2}.\]

Now gathering the results, we conclude

\[E_{1} \equiv\frac{1}{2}\left\|\dot{W}(t)\right\|^{2}-\int_{t_{0}}^{t} \beta(s)\left\|\dot{W}(s)\right\|^{2}ds+\int_{t_{0}}^{t}C(s)\left\langle\frac{ d}{ds}\tilde{\mathbf{A}}(X(s)),\dot{W}(s)\right\rangle ds\] \[=\frac{C(t)^{2}}{2}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+ \left[\left\langle\tilde{\mathbf{A}}(X(W(s),s)),C(s)\beta(s)W(s)\right\rangle \right]_{t_{0}}^{t}+\left[\left(\beta(s)^{2}+\dot{\beta}(s)\right)\frac{1}{2} \left\|W(s)\right\|^{2}\right]_{t_{0}}^{t}\] \[\quad+\int_{t_{0}}^{t}C(s)^{2}\left\langle\frac{d}{ds}\tilde{ \mathbf{A}}(X(s)),\dot{X}(s)\right\rangle ds-\frac{1}{2}\int_{t_{0}}^{t}\frac{d }{ds}\left(C(s)^{2}\dot{\beta}(s)\right)\left\|X(s)-X_{0}\right\|^{2}ds\] \[\quad-\underbrace{\frac{C(t_{0})^{2}}{2}\left(2\beta(t_{0})\left\langle \tilde{\mathbf{A}}(X(t_{0})),X(t_{0})-X_{0}\right\rangle+\left(\beta(t_{0})^{ 2}+\dot{\beta}(t_{0})\right)\left\|X(t_{0})-X_{0}\right\|^{2}\right)}_{= \text{constant}}\]

Moving the constant terms to left hand side and naming \(E=E_{1}-\) constant, we get the desired result.

#### e.3 Proof of Corollary 3.3

#### e.3.1 \(V\) is nonincreasing when \(\tilde{\mathbf{A}}\) is Lipshitz continuous monotone

We first check it is true for the case \(\tilde{\mathbf{A}}\) is Lipschitz continuous. Then from Proposition 3.2 we can write \(V(t)\) as

\[V(t)=E-2\int_{t_{0}}^{t}C(s)^{2}\left\langle\frac{d}{ds}\tilde{\mathbf{A}}(X(s) ),\dot{X}(s)\right\rangle ds\]with \(E\) in Proposition 3.2. As \(\tilde{\mathbf{A}}\) is monotone, from (1) we know \(C(s)^{2}\left\langle\frac{d}{ds}\tilde{\mathbf{A}}(X(s)),\dot{X}(s)\right\rangle\geq 0\) holds almost everywhere. Therefore for \(h>0\)

\[V(t+h)-V(t)=-2\int_{t}^{t+h}C(s)^{2}\left\langle\frac{d}{ds}\tilde{\mathbf{A}}( X(s)),\dot{X}(s)\right\rangle ds\leq 0,\]

we see \(V\) is a nonincreasing function. Therefore for all \(t>0\), \(V(t)\leq\lim_{t\to 0+}V(\epsilon)\). It remains to show the limit \(\lim_{\epsilon\to 0+}V(\epsilon)\) exists.

e.b.2 Calculation of \(V(0)=\lim_{t\to 0+}V(t)\) for Lipshitz continuous monotone \(\tilde{\mathbf{A}}\)

In this section, we calculate \(\lim_{t\to 0+}V(t)\) when \(\tilde{\mathbf{A}}\) is Lipshitz continuous. Recall \(V\) was defined as

\[V(t) =\frac{C(t)^{2}}{2}\left(\left\|\tilde{\mathbf{A}}(X(t))\right\| ^{2}+2\beta(t)\left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle+ \left(\beta(t)^{2}+\dot{\beta}(t)\right)\left\|X(t)-X_{0}\right\|^{2}\right)\] \[\quad-\int_{t_{0}}^{t}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta} (s)}{2}\right)\left\|X(s)-X_{0}\right\|^{2}ds.\]

From now we denote \(V\) for the case \(t_{0}=0\) as \(V^{0}\).

We first check

\[C(t)=\begin{cases}t^{\gamma}&p=1\\ e^{\frac{\gamma}{1-p}t^{1-p}}&p>0,p\neq 1.\end{cases}\] (31)

Observing

\[\int_{1}^{t}\frac{\gamma}{s}ds =\gamma\log t\] \[\int_{0}^{t}\frac{\gamma}{s^{p}}ds =\frac{\gamma}{1-p}t^{1-p}\qquad\text{ for }0<p<1\] \[\int_{\infty}^{t}\frac{\gamma}{s^{p}}ds =\frac{\gamma}{1-p}t^{1-p}\qquad\text{ for }p>1,\]

we see \(C(t)\) defined above agrees with the definition of \(C(t)\) for each case. Note

\[\lim_{t\to 0+}C(t)=\begin{cases}0&p\geq 1\\ 1&0<p<1.\end{cases}\]

Now we will show

\[\lim_{t\to 0+}V^{0}(t)=\lim_{t\to 0+}\frac{C(t)^{2}}{2}\left\|\tilde{\mathbf{A}}(X( t))\right\|^{2}=\begin{cases}0&\text{ if }p\geq 1\\ \frac{\left\|\tilde{\mathbf{A}}(X_{0})\right\|^{2}}{2}&\text{ if }0<p<1.\end{cases}\] (32)

To do so, we first show for \(t>0\)

\[\lim_{\epsilon\to 0+}\int_{\epsilon}^{t}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{ \beta}(s)}{2}\right)\left\|X(s)-X_{0}\right\|^{2}ds<\infty.\]

We first provide an elementary fact as a lemma.

**Lemma E.3**.: _Let \(f\colon(0,\infty)\to\mathbb{R}\) is a continuous function. Suppose there is \(q<1\), \(0<l<\infty\) such that_

\[\limsup_{t\to 0+}|f(t)t^{q}|<l.\]

_Then for \(t>0\), \(f\in L^{1}([0,t],\mathbb{R})\)._Proof.: Since \(\limsup_{s\to 0+}f(s)s^{q}=l\), there is \(\epsilon\in(0,t)\) such that

\[0<s<\epsilon\quad\Longrightarrow\quad|f(s)s^{q}|<2|l|.\]

Then since \(q<1\)

\[\int_{0}^{\epsilon}|f(s)|\,ds=\int_{0}^{\epsilon}|f(s)s^{q}|\,\frac{1}{s^{q}} ds\leq\int_{0}^{\epsilon}\frac{2|l|}{s^{q}}ds=\left[\frac{2|l|}{1-q}s^{1-q}\right]_{0}^ {\epsilon}=\frac{2|l|}{1-q}\epsilon^{1-q}<\infty.\]

By the way as \(f(t)\) is continuous on \([\epsilon,t]\), \(M=\max_{s\in[\epsilon,t]}|f(s)|\) exists. Therefore,

\[\int_{0}^{t}|f(s)|ds=\int_{0}^{\epsilon}|f(s)|ds+\int_{\epsilon}^{t}|f(s)|ds \leq\frac{2|l|}{1-q}\epsilon^{1-q}+M(t-\epsilon)<\infty.\]

Applying Lemma E.3, we will show for \(t>0\)

\[\left|s^{2}\frac{d}{ds}\left(C(s)^{2}\dot{\beta}(s)\right)\right|\in L^{1}([0, t],\mathbb{R}).\] (33)

Observe

\[\frac{d}{ds}\left(C(s)^{2}\dot{\beta}(s)\right)=2C(s)^{2}\beta(s)\dot{\beta}( s)+C(s)^{2}\ddot{\beta}(s)=C(s)^{2}\left(-\frac{2p\gamma^{2}}{s^{2p+1}}+\frac{p(1+p) \gamma}{s^{p+2}}\right).\] (34)

1. \(p>1\) We first show \(\lim_{s\to 0+}C(s)^{2}\frac{1}{s^{n}}=0\) for all \(n>0\). Take \(n>0\). Then there is some \(k\in\mathbb{N}\) such that \(k(p-1)>n\). With change of variable \(u=\frac{1}{s}\) and L'Hoptial's rule we see \[\lim_{s\to 0+}C(s)^{2}\frac{1}{s^{n}} =\lim_{u\to\infty}\frac{u^{n}}{e^{\frac{2\gamma}{p-1}u^{p-1}}}\] \[=\lim_{u\to\infty}\frac{nu^{-1}}{2\gamma u^{p-2}e^{\frac{2 \gamma}{p-1}u^{p-1}}}=\frac{n}{2\gamma}\lim_{u\to\infty}\frac{u^{n+1-p}}{e^{ \frac{2\gamma}{p-1}u^{p-1}}}=\cdots=\frac{\prod_{m=0}^{k-1}(n-m(p-1))}{(2 \gamma)^{k}}\lim_{u\to\infty}\frac{u^{n-k(p-1)}}{e^{\frac{2\gamma}{p-1}u^{p-1} }}=0.\] (35) And thus \[\lim_{s\to 0+}\left|s^{2}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2} \right)\right|=\lim_{s\to 0+}\left|C(s)^{2}\left(-\frac{2p\gamma^{2}}{s^{2p-1}}+ \frac{p(1+p)\gamma}{s^{p}}\right)\right|=0.\] By Lemma E.3, we conclude \(\int_{0}^{t}\left|s^{2}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2} \right)\right|ds<\infty\).
2. \(p=1\) Since \(C(s)=s^{\gamma}\), we see \[\lim_{s\to 0+}\left|s^{2}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2} \right)\cdot s^{1-\gamma}\right|=\lim_{s\to 0+}\left|s^{2}\cdot\gamma(\gamma-1)s^{2 \gamma-3}\cdot s^{1-\gamma}\right|=\gamma\left|\gamma-1\right|\lim_{s\to 0+}s^{ \gamma}=0.\] Since \(1-\gamma<1\), by Lemma E.3 we conclude \(\int_{0}^{t}\left|s^{2}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2} \right)\right|ds<\infty\).
3. \(0<p<1\) Since \(\lim_{s\to 0+}C(s)=\lim_{s\to 0+}e^{\frac{\gamma}{1-p}s^{1-p}}=1\), we see \[\limsup_{s\to 0+}\left|s^{2}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2} \right)\cdot s^{p}\right|=\limsup_{s\to 0+}C(s)^{2}\left|-2p\gamma^{2}s^{1-p}+p(1+p) \gamma\right|=p(1+p)\gamma.\] Since \(p<1\), by Lemma E.3 we conclude \(\int_{0}^{t}\left|s^{2}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2} \right)\right|ds<\infty\).

Naming the bound in Corollary B.8 as \(M(T)\), we know for \(0<s<T\)

\[\left\|\frac{X(s)-X_{0}}{s}\right\|\leq\frac{\int_{0}^{s}\left\|\dot{X}(u) \right\|du}{s}\leq\frac{\int_{0}^{s}M(T)du}{s}=M(T).\]

Therefore applying (33), for \(0<t<T\) we have

\[\int_{0}^{t}\left|\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2}\right) \left\|X(s)-X_{0}\right\|^{2}\right|ds\leq M(T)^{2}\int_{0}^{t}\left|s^{2} \frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2}\right)\right|ds<\infty.\]

Hence we know \(V^{0}(t)\) is well defined and since \(\lim_{t\to 0+}\int_{0}^{t}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2} \right)\left\|X(s)-X_{0}\right\|^{2}ds=0\), we have

\[\lim_{t\to 0+}V^{0}(t)=\lim_{t\to 0+}\frac{C(t)^{2}}{2}\left(\left\|\tilde{ \mathbf{A}}(X(t))\right\|^{2}+2\beta(t)\left\langle\tilde{\mathbf{A}}(X(t)),X (t)-X_{0}\right\rangle+\left(\beta(t)^{2}+\dot{\beta}(t)\right)\left\|X(t)-X_{ 0}\right\|^{2}\right).\]

Now we are ready to show the desired result.

1. \(p>1\) As we know \(X(t)-X_{0}\) and \(\tilde{\mathbf{A}}(X(t))\) are bounded from Lemma D.1 and Corollary B.9. Therefore from (35) we have \[0=\lim_{t\to 0+}C(t)^{2}=\lim_{t\to 0+}C(t)^{2}\beta(t)=\lim_{t\to 0+}C(t)^{2}\left(\beta(t)^{2}+ \dot{\beta}(t)\right),\] therefore \(\lim_{t\to 0+}V^{0}(t)=0\).
2. \(0<p\leq 1\) As \(\left\|\frac{X(t)-X_{0}}{t}\right\|\leq M(T)<\infty\) for \(0<t<T\), we see \[\limsup_{t\to 0+}C(t)^{2}\beta(t)\left\langle\tilde{\mathbf{A}}(X(t)),X (t)-X_{0}\right\rangle\leq\gamma\limsup_{t\to 0+}C(t)^{2}t^{1-p}\left\|\tilde{ \mathbf{A}}(X(t))\right\|M(T) =0\] \[\limsup_{t\to 0+}C(t)^{2}\left(\beta(t)^{2}+\dot{\beta}(t)\right) \left\|X(t)-X_{0}\right\|^{2}\leq\gamma\limsup_{t\to 0+}C(t)^{2}\left( \gamma t^{2-2p}-t^{1-p}\right)M(T)^{2} =0.\] Therefore \[\lim_{t\to 0+}V^{0}(t)=\lim_{t\to 0+}\frac{C(t)^{2}}{2}\left\|\tilde{ \mathbf{A}}(X(t))\right\|^{2}=\begin{cases}0&\text{if }p=1\\ \frac{\left\|\tilde{\mathbf{A}}(X_{0})\right\|^{2}}{2}&\text{if }0<p<1.\end{cases}\] From (i) and (ii) we get the desired conclusion \[V^{0}(0)=\lim_{t\to 0+}V^{0}(t)=\begin{cases}0&\text{if }p\geq 1\\ \frac{\left\|\tilde{\mathbf{A}}(X_{0})\right\|^{2}}{2}&\text{if }0<p<1,\end{cases}\] and therefore for general \(t_{0}\geq 0\), \[V(0)=\lim_{t\to 0+}V(t)=\lim_{t\to 0+}V^{0}(t)-\int_{0}^{t_{0}}\frac{d}{ds} \left(\frac{C(s)^{2}\dot{\beta}(s)}{2}\right)\left\|X(s)-X_{0}\right\|^{2}ds\] is well-defined.

e.3.3 \(V(t)\leq\lim_{n\to\infty}V_{\lambda_{n}}(0)\) holds for \(t\in S\) and general maximal monotone \(\mathbf{A}\)

Define \(S\) as defined in Lemma E.1. Take \(t\in S\), let \(T>t\). Let \(\left\{\lambda_{n}\right\}_{n\in\mathbb{N}}\) be a positive sequence \(\lambda_{n}\) that \(\lim_{n\to\infty}\lambda_{n}=0\), \(X_{\lambda_{n}}\) converges to \(X\) uniformly on \([0,T]\) and \(\dot{X}_{\lambda_{n}}\) converges weakly to \(X\) in \(L^{2}([0,T],\mathbb{R}^{n})\). Recall existence of such sequence was gauranteed by Proposition B.11.

Recall we denoted \(X_{\lambda}\) as the solution of the ODE (19). Denote \(V_{\lambda}\) as \(V\) for the case \(\mathbf{A}=\mathbf{A}_{\lambda}\), i.e.

\[V_{\lambda}(t) =\frac{C(t)^{2}}{2}\left(\left\|\mathbf{A}_{\lambda}(X_{\lambda}(t)) \right\|^{2}+2\beta(t)\left\langle\mathbf{A}_{\lambda}\left(X_{\lambda}(t) \right),X_{\lambda}(t)-X_{0}\right\rangle+\left(\beta(t)^{2}+\dot{\beta}(t) \right)\left\|X_{\lambda}(t)-X_{0}\right\|^{2}\right)\] \[\quad-\int_{t_{0}}^{t}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta} (s)}{2}\right)\left\|X_{\lambda}(t)-X_{0}\right\|^{2}ds.\]

Note equality \(\dot{X}(t)=-\mathbf{\tilde{A}}(X(t))-\beta(t)(X(t)-X_{0})\) holds since \(t\in S\), therefore we have

\[V(t)=\frac{C(t)^{2}}{2}\left(\left\|\dot{X}(t)\right\|^{2}+\dot{\beta}(t) \left\|X(t)-X_{0}\right\|^{2}\right)-\int_{t_{0}}^{t}\frac{d}{ds}\left(\frac{ C(s)^{2}\dot{\beta}(s)}{2}\right)\left\|X(s)-X_{0}\right\|^{2}ds.\]

The goal of this section is to show

\[V(t)\leq\limsup_{n\to\infty}V_{\lambda_{n}}(t)\leq\limsup_{n\to\infty}V_{ \lambda_{n}}(0)=\lim_{n\to\infty}V_{\lambda_{n}}(0).\]

1. \(V(t)\leq\limsup_{n\to\infty}V_{\lambda_{n}}(t)\) First observe, from Lemma B.12 we know \[\frac{\left\|X_{\lambda}(s)-X_{0}\right\|}{s}\leq\frac{\int_{0}^{s}\left\| \dot{X}_{\lambda}(s)\right\|ds}{s}\leq\frac{\int_{0}^{s}M_{dot}(T)ds}{s}=M_{ dot}(T)\] holds for \(s\leq T\). Thus from (33) we have \[\left|\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2}\right)\left\|X_{ \lambda}(s)-X_{0}\right\|^{2}\right|\leq\left|s^{2}\frac{d}{ds}\left(\frac{C( s)^{2}\dot{\beta}(s)}{2}\right)\right|M_{dot}(T)^{2}\in L^{1}([0,T],\mathbb{R}).\] Therefore applying dominated convergence theorem, we have for \(t_{0}\in[0,T]\) \[\lim_{n\to\infty}\int_{t_{0}}^{t}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta} (s)}{2}\right)\left\|X_{\lambda_{n}}(s)-X_{0}\right\|^{2}ds=\int_{t_{0}}^{t} \frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2}\right)\left\|X(s)-X_{0} \right\|^{2}ds.\] (36) From elementary analysis, we can easily check \(\limsup_{n\to\infty}(a_{n}+b_{n})=\limsup_{n\to\infty}a_{n}+\lim_{n\to\infty}b _{n}\) holds when \(\lim_{n\to\infty}b_{n}\) exists. Thus we have \[\limsup_{n\to\infty}V_{\lambda_{n}}(t)-V(t)=\frac{C(t)^{2}}{2}\left(\limsup_{n \to\infty}\left\|\dot{X}_{\lambda_{n}}(t)\right\|^{2}-\left\|\dot{X}(t)\right\| ^{2}\right).\] Therefore it is suffices to show following lemma.

**Lemma E.4**.: _Suppose for \(T>0\) and sequence \(\left\{\lambda_{n}\right\}_{n\in\mathbb{N}}\), \(X_{\lambda_{n}}\) converges to \(X\) uniformly on \([0,T]\) and \(\dot{X}_{\lambda_{n}}\) converges weakly to \(\dot{X}\) in \(L^{2}([0,T],\mathbb{R}^{n})\). Let \(t\in S\cap[0,T]\). Then following inequality is true_

\[\left\|\dot{X}(t)\right\|\leq\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{n}}( t)\right\|.\]

Proof.: (i) \(\left\|\dot{X}(s)\right\|\leq\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{n}}( s)\right\|\) holds for almost every \(s\in[0,T]\).

Let \(D\) be a measurable subset \(D\subset[0,T]\). Since \(\dot{X}_{\lambda_{n}}\rightharpoonup\dot{X}\) in \(L^{2}([0,T],\mathbb{R}^{n})\) and \(\chi_{D}\dot{X}\in L^{2}([0,T],\mathbb{R}^{n})\), we have

\[\int_{D}\left\|\dot{X}(s)\right\|^{2}ds =\int_{0}^{T}\left\langle\dot{X}(s),\chi_{D}(s)\dot{X}(s) \right\rangle ds=\lim_{n\to\infty}\int_{0}^{T}\left\langle\dot{X}_{\lambda_{n}} (s),\chi_{D}(s)\dot{X}(s)\right\rangle ds\] \[=\limsup_{n\to\infty}\int_{D}\left\langle\dot{X}_{\lambda_{n}}(s), \dot{X}(s)\right\rangle ds\leq\limsup_{n\to\infty}\int_{D}\left\|\dot{X}_{ \lambda_{n}}(s)\right\|\left\|\dot{X}(s)\right\|ds.\]

The inequality comes from the Cauchy-Schwarz inequality. Now from Reverse Fatou's Lemma we have

\[\limsup_{n\to\infty}\int_{D}\left\|\dot{X}_{\lambda_{n}}(s)\right\|\left\|\dot {X}(s)\right\|ds\leq\int_{D}\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{n}}( s)\right\|\left\|\dot{X}(s)\right\|ds.\]Therefore combining two inequalities we have

\[\int_{D}\left(\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{n}}(s)\right\|-\left\| \dot{X}(s)\right\|\right)\left\|\dot{X}(s)\right\|ds\geq 0.\]

As \(D\) was arbitrary measurable subset of \([0,T]\), we conclude for almost every \(s\in[0,T]\)

\[\left(\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{n}}(s)\right\|-\left\|\dot{ X}(s)\right\|\right)\left\|\dot{X}(s)\right\|\geq 0\quad\Longrightarrow\quad\left\|\dot{X}(s) \right\|\leq\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{n}}(s)\right\|.\]
2. \(\left\|\dot{X}(t)\right\|\leq\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{n}} (t)\right\|\) for \(t\in S\cap[0,T]\). Let \(t\in S\cap[0,T]\). Then for \(h>0\) such that \(t+h<T\), since \(S\) is measure zero, from (i) we have \[\int_{t}^{t+h}\left\|\dot{X}(s)\right\|ds\leq\int_{t}^{t+h}\limsup_{n\to\infty }\left\|\dot{X}_{\lambda_{n}}(s)\right\|ds.\]

Now, for some \(a>0\) consider

\[U_{\lambda_{n}}(s)=\left\|\dot{X}_{\lambda_{n}}(s)\right\|^{2}+\underbrace{ \frac{\gamma p}{s^{p+1}}\left\|X_{\lambda_{n}}(s)-X_{0}\right\|^{2}+\int_{a}^{ s}\frac{\gamma p(1-p)}{u^{p+2}}\left\|X_{\lambda_{n}}(u)-X_{0}\right\|^{2}du}_{=f_{n}( s)}.\]

Then from the proof Lemma B.5, we know

\[\hat{U}_{\lambda_{n}}(s)=-2\left\langle\dot{X}_{\lambda_{n}}(s),\frac{d}{ds} \mathbf{A}_{\lambda_{n}}(X(s))\right\rangle-\frac{2\gamma}{s^{p}}\left\|\dot{ X}_{\lambda_{n}}(s)-\frac{p}{s}(X_{\lambda_{n}}(s)-X_{0})\right\|^{2}\leq 0\]

holds for almost every \(s\), thus \(U_{\lambda_{n}}\) is nonincreasing.

By the way, as \(X_{\lambda_{n}}\) converges to \(X\) uniformly on \([0,T]\), using dominated convergence theorem we have

\[\lim_{n\to\infty}f_{n}(s)=\frac{\gamma p}{s^{p+1}}\left\|X(s)-X_{0}\right\|^{ 2}+\int_{a}^{s}\frac{\gamma p(1-p)}{u^{p+2}}\left\|X(u)-X_{0}\right\|^{2}du.\]

Denote \(f(s)=\lim_{n\to\infty}f_{n}(s)\).

Using above facts, for \(s\in[t,t+h]\) we have

\[\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{n}}(s)\right\| =\limsup_{n\to\infty}\sqrt{U_{\lambda_{n}}(s)-f_{n}(s)}\] \[\leq\limsup_{n\to\infty}\sqrt{U_{\lambda_{n}}(t)-f_{n}(s)}=\sqrt {\limsup_{n\to\infty}U_{\lambda_{n}}(t)-f(s)}.\] (37)

Therefore,

\[\left\|X(t+h)-X(t)\right\|\leq\int_{t}^{t+h}\left\|\dot{X}(s) \right\|ds\leq\int_{t}^{t+h}\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{n}}(s )\right\|ds\leq\int_{t}^{t+h}\sqrt{\limsup_{n\to\infty}U_{\lambda_{n}}(t)-f(s )}ds.\]

Note \(\sqrt{\limsup_{n\to\infty}U_{\lambda_{n}}(t)-f(s)}\) is a continuous function with respect to \(s\). Thus we have

\[\lim_{h\to 0}\frac{1}{h}\int_{t}^{t+h}\sqrt{\limsup_{n\to\infty}U_{ \lambda_{n}}(t)-f(s)}ds =\sqrt{\limsup_{n\to\infty}U_{\lambda_{n}}(t)-f(t)}\] \[=\sqrt{\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{n}}(t) \right\|^{2}+\lim_{n\to\infty}f_{n}(t)-f(t)}=\limsup_{n\to\infty}\left\|\dot{ X}_{\lambda_{n}}(t)\right\|.\]

Finally as \(t\in S\), \(\dot{X}(t)=\lim_{h\to 0}\frac{X(t+h)-X(t)}{h}\) exists. Therefore

\[\left\|\dot{X}(t)\right\| =\lim_{h\to 0+}\frac{\left\|X(t+h)-X(t)\right\|}{h}\] \[\leq\lim_{h\to 0+}\frac{1}{h}\int_{t}^{t+h}\sqrt{\limsup_{n\to \infty}U_{\lambda_{n}}(t)-f(s)}ds=\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{ n}}(t)\right\|,\]

we conclude the desired result.

2. \(\limsup_{n\to\infty}V_{\lambda_{n}}(t)\leq\limsup_{n\to\infty}V_{\lambda_{n}}(0)\) As \(\mathbf{A}_{\lambda_{n}}\) is Lipschitz continuous, from Appendix E.3.1 we know \(V_{\lambda_{n}}\) is nonincreasing, and thus \[V_{\lambda_{n}}(t)\leq\lim_{\epsilon\to 0+}V_{\lambda_{n}}(\epsilon)=V_{ \lambda_{n}}(0).\] Taking limsup both sides we get the desired result.
3. \(\limsup_{n\to\infty}V_{\lambda_{n}}(0)=\lim_{n\to\infty}V_{\lambda_{n}}(0)\) From (32) and (ii) of Lemma B.10, we know \[\lim_{n\to\infty}V_{\lambda_{n}}^{0}(0)=\begin{cases}0&\text{ if }p\geq 1\\ \frac{\left\|m(\mathbf{A}(X_{0}))\right\|^{2}}{2}&\text{ if }0<p<1.\end{cases}\] And applying (36) we have \[\lim_{n\to\infty}V_{\lambda_{n}}(0)=\lim_{n\to\infty}V_{\lambda_{n}}^{0}(0)- \int_{0}^{t_{0}}\frac{d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2}\right) \left\|X(s)-X_{0}\right\|^{2}ds.\] As the limit \(\lim_{n\to\infty}V_{\lambda_{n}}(0)\) exists, the limsup concides with the limit.

#### e.3.4 Proof for general maximal monotone \(\mathbf{A}\)

First, we show \(V(t)\leq\lim_{n\to\infty}V_{\lambda_{n}}(0)\) holds for \(t\in[0,\infty)\). Next, we show \(\lim_{n\to\infty}V_{\lambda_{n}}(0)=\lim_{t\to 0+}V(t)\). Then we have \[V(t)\leq\lim_{n\to\infty}V_{\lambda_{n}}(0)=\lim_{t\to 0+}V(t)=V(0),\] which is our desired result.

1. \(V(t)\leq\lim_{n\to\infty}V_{\lambda_{n}}(0)\) holds for \(t\in[0,\infty)\) Take \(t\in[0,\infty)\). We know the inequality is true when \(t\in S\), thus assume \(t\notin S\). Then from Lemma E.1, we know there is a sequence \(\left\{t_{k}\right\}_{k\in\mathbb{N}}\) such that \(t_{k}\in S\), \(\lim_{k\to\infty}t_{k}=t\) and \(\tilde{\mathbf{A}}(X(t_{k}))\) converges to \(\tilde{\mathbf{A}}(X(t))\). As \(t_{k}\in S\), \(V(t_{k})\leq\lim_{n\to\infty}V_{\lambda_{n}}(0)\) holds. Since \(\lim_{k\to\infty}V(t_{k})=V(t)\), taking limit \(k\to\infty\) to the inequality we get the desired result.
2. \(\lim_{n\to\infty}V_{\lambda_{n}}(0)=\lim_{t\to 0+}V(t)\) Take a sequence \(\left\{t_{k}\right\}_{k\in\mathbb{N}}\) such that \(t_{k}>0\) and \(\lim_{k\to\infty}t_{k}=0\). We wish to show \(\lim_{k\to\infty}V(t_{k})=\lim_{n\to\infty}V_{\lambda_{n}}(0)\). Note the arguments in Appendix E.3.2 are valid when \(\left\|\tilde{\mathbf{A}}(X(t_{k}))\right\|\) and \(\frac{\left\|X(t_{k})-X_{0}\right\|}{t_{k}}\) are bounded for all \(k\in\mathbb{N}\). The boundedness of \(\left\|\tilde{\mathbf{A}}(X(t_{k}))\right\|\) comes from Corollary E.2. And from Lemma B.12 we have \[\frac{\left\|X(t_{k})-X_{0}\right\|}{t_{k}}=\lim_{\lambda\to 0+}\frac{\left\|X_{ \lambda}(t_{k})-X_{0}\right\|}{t_{k}}\leq\lim_{\lambda\to 0+}\frac{\int_{0}^{t_{k}} \left\|\dot{X}_{\lambda}(s)\right\|ds}{t_{k}}\leq M_{dot}(t_{k})\leq\sup_{k\in \mathbb{N}}M_{dot}(t_{k}).\] Therefore applying the arguments in Appendix E.3.2 we have \[\lim_{k\to\infty}V^{0}(t_{k})=\lim_{k\to\infty}\frac{C(t_{k})^{2}}{2}\left\| \tilde{\mathbf{A}}(X(t_{k}))\right\|^{2}.\] Thus it remains to show the limit on the right hand side exists and is equal to \(\lim_{n\to\infty}V_{\lambda_{n}}^{0}(0)\). For \(p\geq 1\), we know \(\lim_{k\to\infty}C(t_{k})^{2}=0\), thus \(\lim_{k\to\infty}\frac{C(t_{k})^{2}}{2}\left\|\tilde{\mathbf{A}}(X(t_{k})) \right\|^{2}=0\) since \(\left\|\tilde{\mathbf{A}}(X(t_{k}))\right\|\) is bounded. As \(\lim_{n\to\infty}V_{\lambda_{n}}^{0}(0)=0\) from (32), we're done. Now consider the case \(0<p<1\). Suppose \(\tilde{\mathbf{A}}(X(t_{k_{l}}))\) is a convergent subsequence of \(\tilde{\mathbf{A}}(X(t_{k}))\). First observe from (i) we have \[V^{0}(t_{k_{l}})\leq\lim_{n\to\infty}V_{\lambda_{n}}^{0}(0)=\frac{\left\|m( \mathbf{A}(X_{0}))\right\|^{2}}{2}.\] From above inequality, recalling \(\lim_{l\to\infty}C(t_{k_{l}})^{2}=1\) we have \[\frac{\left\|\lim_{l\to\infty}\tilde{\mathbf{A}}(X(t_{k_{l}}))\right\|^{2}}{2}= \lim_{l\to\infty}V^{0}(t_{k_{l}})\leq\frac{\left\|m(\mathbf{A}(X_{0}))\right\| ^{2}}{2}.\]By the way as \(\lim_{l\to\infty}X(t_{k_{l}})=X_{0}\) and \(\tilde{\mathbf{A}}(X(t_{k_{l}}))\in\mathbf{A}(X(t_{k_{l}}))\), by closed graph theorem we have \(\lim_{l\to\infty}\tilde{\mathbf{A}}(X(t_{k_{l}}))\in\mathbf{A}(X_{0})\). As of \(m(\mathbf{A}(X_{0}))\) is the element in \(\mathbf{A}(X_{0})\) with smallest norm, we have \(\lim_{l\to\infty}\tilde{\mathbf{A}}(X(t_{k_{l}}))=m(\mathbf{A}(X_{0}))\). As all convergent subsequence converges to the same limit \(m(\mathbf{A}(X_{0}))\), we conclude \(\lim_{k\to\infty}\tilde{\mathbf{A}}(X(t_{k}))=m(\mathbf{A}(X_{0}))\).

Therefore

\[\lim_{k\to\infty}V^{0}(t_{k})=\lim_{k\to\infty}\frac{C(t_{k})^{2}}{2}\left\| \tilde{\mathbf{A}}(X(t_{k}))\right\|^{2}=\frac{\left\|m(\mathbf{A}(X_{0})) \right\|^{2}}{2}=\lim_{n\to\infty}V^{0}_{\lambda_{n}}(0).\]

As \(t_{k}\) was arbitrary positive sequence converges to \(0\), we conclude \(\lim_{n\to\infty}V^{0}_{\lambda_{n}}(0)=\lim_{t\to 0+}V^{0}(t)=V^{0}(0)\). Therefore

\[\lim_{n\to\infty}V_{\lambda_{n}}(0)=V^{0}(0)-\int_{0}^{t_{0}}\frac{d}{ds} \left(\frac{C(s)^{2}\dot{\beta}(s)}{2}\right)\left\|X(s)-X_{0}\right\|^{2}ds =\lim_{t\to 0+}V(t).\]

### Proof of Lemma 3.4

Most of the proof is done in the main text. Recall \(\Phi(t)\) is defined as \(\Phi(t)=\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2\beta(t)\left\langle \tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle\), and from (8) we have

\[\Phi(t)\geq\frac{1}{2}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}-2\beta(t)^{ 2}\left\|X_{0}-X_{\star}\right\|^{2}.\]

On the other hand,

\[\frac{2V(t)}{C(t)^{2}}-\Phi(t)=\left(\beta(t)^{2}+\dot{\beta}(t) \right)\left\|X(t)-X_{0}\right\|^{2}-\frac{2}{C(t)^{2}}\int_{t_{0}}^{t}\frac{ d}{ds}\left(\frac{C(s)^{2}\dot{\beta}(s)}{2}\right)\left\|X(s)-X_{0}\right\|^{2}ds.\]

Note \(C(t)\neq 0\) if \(t>0\), we can divide with \(C(t)^{2}\). Now from Corollary 3.3 we have we have \(V(t)\geq V(0)\) for \(t>0\), and so \(\frac{V(t)}{C(t)^{2}}-\frac{V(0)}{C(t)^{2}}\geq 0\). Thus for \(t>\epsilon\)

\[\frac{2V(0)}{C(t)^{2}}-\left(\beta(t)^{2}+\dot{\beta}(t)\right) \left\|X(t)-X_{0}\right\|^{2}+\frac{2}{C(t)^{2}}\int_{t_{0}}^{t}\frac{d}{ds} \left(\frac{C(s)^{2}\dot{\beta}(s)}{2}\right)\left\|X(s)-X_{0}\right\|^{2}ds\] \[=\frac{2V(0)}{C(t)^{2}}-\left(\frac{2V(t)}{C(t)^{2}}-\Phi(t)\right)\] \[=\left(\frac{2V(0)}{C(t)^{2}}-\frac{2V(t)}{C(t)^{2}}\right)+\Phi( t)\geq\Phi(t)\geq\frac{1}{2}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}-2\beta(t)^{ 2}\left\|X_{0}-X_{\star}\right\|^{2}.\]

Moving \(2\beta(t)^{2}\left\|X_{0}-X_{\star}\right\|^{2}\) to the left hand side and multiplying with \(2\) we get the desired result

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2} \leq 4\beta(t)^{2}\left\|X_{0}-X_{\star}\right\|^{2}+\frac{4V(0)}{C (t)^{2}}-2\left(\beta(t)^{2}+\dot{\beta}(t)\right)\left\|X(t)-X_{0}\right\|^{2}\] \[+\frac{2}{C(t)^{2}}\int_{t_{0}}^{t}\frac{d}{ds}\left(C(s)^{2}\dot{ \beta}(s)\right)\left\|X(s)-X_{0}\right\|^{2}ds.\]

### Proof of Theorem 3.1

Restating Lemma 3.4, we know for \(t>0\),

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2} \leq 4\beta(t)^{2}\left\|X_{0}-X_{\star}\right\|^{2}+\frac{2V(0)}{C( t)^{2}}-2\left(\beta(t)^{2}+\dot{\beta}(t)\right)\left\|X(t)-X_{0}\right\|^{2}\] (7) \[+\frac{2}{C(t)^{2}}\int_{t_{0}}^{t}\frac{d}{ds}\left(C(s)^{2}\dot{ \beta}(s)\right)\left\|X(s)-X_{0}\right\|^{2}ds.\]As we know \(\left\|X(t)-X_{0}\right\|\leq 2\left\|X_{0}-X_{\star}\right\|\) from Lemma D.1, it is clear that

\[4\beta(t)^{2}\left\|X_{0}-X_{\star}\right\|^{2} =\mathcal{O}\left(\beta(t)^{2}\right)\] (38) \[\frac{2V(0)}{C(t)^{2}} =\mathcal{O}\left(\frac{1}{C(t)^{2}}\right)\] \[2\left(\beta(t)^{2}+\dot{\beta}(t)\right)\left\|X(t)-X_{0}\right\| ^{2} =\mathcal{O}\left(\beta(t)^{2}\right)+\mathcal{O}\left(\dot{\beta}( t)\right).\]

Therefore it remains to show

\[\frac{2}{C(t)^{2}}\int_{t_{0}}^{t}\frac{d}{ds}\left(C(s)^{2}\dot{\beta}(s) \right)\left\|X(s)-X_{0}\right\|^{2}ds =\mathcal{O}\left(\beta(t)^{2}\right)+\mathcal{O}\left(\frac{1}{C(t)^{2}} \right)+\mathcal{O}\left(\dot{\beta}(t)\right).\]

We can check there is \(T>0\) such that for \(s>T\) the sign of \(\frac{d}{ds}\left(C(s)^{2}\dot{\beta}(s)\right)\) does not change, for \(\beta(t)=\frac{\gamma}{t^{p}}\) with \(\gamma>0\), \(p>0\). We first proceed our proof with assuming this condition. The point for this condition is that following equality holds for \(t>T\).

Applying above equality and using \(\left\|X(t)-X_{0}\right\|\leq 2\left\|X_{0}-X_{\star}\right\|\), we see

\[\left|\frac{2}{C(t)^{2}}\int_{t_{0}}^{t}\frac{d}{ds}\left(C(s)^{2 }\dot{\beta}(s)\right)\left\|X(s)-X_{0}\right\|^{2}ds\right|\] \[\leq\frac{2}{C(t)^{2}}\left|\underbrace{\int_{t_{0}}^{T}\frac{d}{ ds}\left(C(s)^{2}\dot{\beta}(s)\right)\left\|X(s)-X_{0}\right\|^{2}ds}_{=M}+ \frac{2}{C(t)^{2}}\int_{T}^{t}\left|\frac{d}{ds}\left(C(s)^{2}\dot{\beta}(s) \right)\left\|X(s)-X_{0}\right\|^{2}\right|ds\] \[\leq\frac{2M}{C(t)^{2}}+\frac{2}{C(t)^{2}}\int_{T}^{t}\left| \frac{d}{ds}\left(C(s)^{2}\dot{\beta}(s)\right)4\left\|X_{0}-X_{\star}\right\| ^{2}\right|ds\] \[=\frac{2M}{C(t)^{2}}+\frac{8\left\|X_{0}-X_{\star}\right\|^{2}}{ C(t)^{2}}\left|C(t)^{2}\dot{\beta}(t)-C(T)^{2}\dot{\beta}(T)\right|\] \[\leq\frac{2}{C(t)^{2}}\left(M+4\left\|X_{0}-X_{\star}\right\|^{2} C(T)^{2}\left|\dot{\beta}(T)\right|\right)+8\left\|X_{0}-X_{\star}\right\|^{2} \left|\dot{\beta}(t)\right|=\mathcal{O}\left(\frac{1}{C(t)^{2}}\right)+ \mathcal{O}\left(\dot{\beta}(t)\right).\]

Therefore from (7) and (38), we conclude

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}=\mathcal{O}\left(\beta(t)^{2} \right)+\mathcal{O}\left(\frac{1}{C(t)^{2}}\right)+\mathcal{O}\left(\dot{ \beta}(t)\right).\]

It remains to show there is \(T>0\) such that for \(s>T\) the sign of \(\frac{d}{ds}\left(C(s)^{2}\dot{\beta}(s)\right)\) does not change. It can be shown by easy, but a little complicated calculations. Since \(\dot{C}(t)=C(t)\beta(t)\), we see

\[\frac{d}{ds}\left(C(s)^{2}\dot{\beta}(s)\right)=2C(s)^{2}\beta(s)\dot{\beta}(s )+C(s)^{2}\ddot{\beta}(s)=C(s)^{2}\left(2\beta(s)\dot{\beta}(s)+\ddot{\beta}(s )\right).\]

Therefore it is enough to check the sign of \(2\beta(s)\dot{\beta}(s)+\ddot{\beta}(s)\). Observe

\[2\beta(s)\dot{\beta}(s)+\ddot{\beta}(s)=-\frac{2p\gamma^{2}}{s^{2p+1}}+\frac{ p(1+p)\gamma}{s^{p+2}}=\begin{cases}\frac{2}{s^{3}}\gamma(1-\gamma)&\text{if }p=1\\ \frac{p(p+1)\gamma}{s^{2p+1}}\left(s^{p-1}-\frac{2\gamma}{p+1}\right)&\text{ if }p\neq 1.\end{cases}\]

Thus when \(p=1\), for all \(s>0\) it is nonpositive if \(\gamma\geq 1\) and nonnegative if \(0<\gamma<1\).

When \(p\neq 1\), we see

\[\lim_{s\to 0+}s^{p-1}=\begin{cases}\infty&\text{if }0<p<1\\ 0&\text{if }p>1\end{cases}, \lim_{s\to\infty}s^{p-1}=\begin{cases}0&\text{if }0<p<1\\ \infty&\text{if }p>1\end{cases}\]

Therefore by intermediate value theorem there is \(T>0\) such that \(T^{p-1}-\frac{2\gamma}{p+1}=0\), and for that \(T\) we have \(s^{p-1}-\frac{2\gamma}{p+1}\) is nonpositive for \(s>T\) if \(0<p<1\) and nonnegative for \(s>T\) if \(p>1\). This concludes the proof.

#### e.5.1 Proof for the convergence rate in Table 1

Recall, from (31) we have

\[C(t)=\begin{cases}t^{\gamma}&p=1\\ e^{\frac{\gamma}{1-p}t^{1-p}}&p>0,\,p\neq 1.\end{cases}\]

We now observe \(\dot{\beta}(t)\) does not effect to convergence rate. In other words, we show \(\dot{\beta}(t)\) is not the slowest one that goes to zero compared to \(\beta(t)^{2}\) and \(\frac{1}{C(t)^{2}}\) for every case.

* \(0<p\leq 1\) Comparing \(\dot{\beta}(t)=-\frac{p\gamma}{t^{p+1}}\) with \(\beta(t)^{2}=\frac{\gamma^{2}}{t^{2p}}\), we see \(\dot{\beta}(t)=\mathcal{O}\left(\beta(t)^{2}\right)\) when \(0<p\leq 1\).
* \(p>1\) For \(p>1\), we see \(\lim_{t\to\infty}\frac{1}{C(t)^{2}}=1\neq 0\). As \(\lim_{t\to\infty}\dot{\beta}(t)=-\lim_{t\to\infty}\frac{p\gamma}{t^{p+1}}=0\), we have \(\dot{\beta}(t)=\mathcal{O}\left(\frac{1}{C(t)^{2}}\right)\).

From (i) and (ii), we conclude

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}=\mathcal{O}\left( \beta(t)^{2}\right)+\mathcal{O}\left(\frac{1}{C(t)^{2}}\right).\]

Now the results in Table 1 is straightfoward.

* \(p=1\) When \(p=1\), we have \(C(t)=t^{\gamma}\). Comparing \(\frac{1}{C(t)^{2}}=\frac{1}{t^{2\gamma}}\) and \(\beta(t)^{2}=\frac{\gamma^{2}}{t^{2}}\), 1. \(\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}=\mathcal{O}\left(\beta(t)^{2}\right) =\mathcal{O}\left(\frac{1}{t^{2}}\right)\) if \(\gamma\geq 1\). 2. \(\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}=\mathcal{O}\left(\frac{1}{C(t)^{2} }\right)=\mathcal{O}\left(\frac{1}{t^{2\gamma}}\right)\) if \(0<\gamma<1\).
* \(0<p<1\) When \(0<p<1\), we have \[\lim_{t\to\infty}\frac{1}{\beta(t)^{2}}\cdot\frac{1}{C(t)^{2}}= \frac{1}{\gamma^{2}}\lim_{t\to\infty}\frac{t^{2}}{e^{\frac{2\gamma}{1-p}t^{1- p}}}=0.\] Therefore, \(\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}=\mathcal{O}\left(\beta(t)^{2} \right)=\mathcal{O}\left(\frac{1}{t^{2p}}\right)\).
* \(p>1\) We observed previously that \(\lim_{t\to\infty}\frac{1}{C(t)^{2}}=1\neq 0\) when \(p>1\). Therefore \(\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}=\mathcal{O}\left(\frac{1}{C(t)^{2 }}\right)=\mathcal{O}\left(1\right)\).

### Proof of Theorem 3.5

Name

\[\bar{X}_{\star}=\Pi_{\operatorname{Zer}\operatorname{A}}(X_{0})= \operatorname*{argmin}_{z\in\operatorname{Zer}\operatorname{A}}\left\|z-X_{0} \right\|.\]

We first show

\[\limsup_{t\to\infty}\left\langle X(t)-\bar{X}_{\star},X_{0}-\bar{X}_{\star} \right\rangle\leq 0.\]

Proof by contradiction. Suppose not.

Then there is \(\epsilon>0\) such that for every \(k\in\mathbb{N}\), there is \(n_{k}\in[0,\infty)\) such that

\[\left\langle X(n_{k})-\bar{X}_{\star},X_{0}-\bar{X}_{\star}\right\rangle>\epsilon\]

By the way, from Lemma D.1 we know \(X(n_{k})\in\bar{B}_{\left\|X_{0}-\bar{X}_{\star}\right\|}(\bar{X}_{\star})\). Thus by Bolzano-Weierstrass theorem, there is a converging subseqeunc \(\{X\left(n_{k(l)}\right)\}_{l\in(\mathbb{N})}\). Name the limit as \(X_{\infty}\). Since \(\lim_{t\to\infty}\left\|\tilde{\mathbf{A}}(X(t))\right\|=0\), \(\{\tilde{\mathbf{A}}(X(n_{k(l)})\}_{l\in(\mathbb{N})}\) converges to \(0\). Then from [15, Proposition 20.38], \((X_{\infty},0)\in\operatorname{Gra}\operatorname{A}\), i.e. \(X_{\infty}\in\operatorname{Zer}\operatorname{A}\).

On the other hand, as A is maximal monotone, by [15, Proposition 23.39] Zer A is closed and convex. Since \(\bar{X}_{\star}=\Pi_{\mathrm{Zer\,}\mathbf{A}}(X_{0})\), by [15, Theorem 3.16] we have \(\left\langle X_{\infty}-\bar{X}_{\star},X_{0}-\bar{X}_{\star}\right\rangle\leq 0\). Thus

\[0<\epsilon\leq\lim_{l\to\infty}\left\langle X\left(n_{k(l)}\right)-\bar{X}_{ \star},X_{0}-\bar{X}_{\star}\right\rangle=\left\langle X_{\infty}-\bar{X}_{ \star},X_{0}-\bar{X}_{\star}\right\rangle\leq 0,\]

this is a contradiction, therefore we get the desired result.

Now we show \(\lim_{t\to\infty}\left\|X(t)-\bar{X}_{\star}\right\|=0\). Recall, for \(t>0\)

\[\dot{X}\in-\mathbf{A}(X(t))-\beta(t)(X-X_{0}).\]

From \(\dot{C}(t)=C(t)\beta(t)\), and monotonicity of A, we observe

\[\frac{d}{dt}\left(C(t)^{2}\left\|X(t)-\bar{X}_{\star}\right\|^{2}\right) =2C(t)^{2}\left\langle\dot{X}(t),X(t)-\bar{X}_{\star}\right\rangle +2C(t)^{2}\beta(t)\left\|X(t)-\bar{X}_{\star}\right\|^{2}\] \[=2C(t)^{2}\left\langle-\bar{\mathbf{A}}(X(t))-\beta(t)(X(t)-X_{0 }),X(t)-\bar{X}_{\star}\right\rangle+2C(t)^{2}\beta(t)\left\|X(t)-\bar{X}_{ \star}\right\|^{2}\] \[\leq-2C(t)^{2}\beta(t)\left\langle X(t)-X_{0},X(t)-\bar{X}_{ \star}\right\rangle+2C(t)^{2}\beta(t)\left\|X(t)-\bar{X}_{\star}\right\|^{2}\] \[=2C(t)^{2}\beta(t)\left\langle X_{0}-\bar{X}_{\star},X(t)-\bar{X} _{\star}\right\rangle.\]

Now take \(\epsilon\). From \(\limsup_{t\to\infty}\left\langle X(t)-\bar{X}_{\star},X_{0}-\bar{X}_{\star} \right\rangle\leq 0\), there is \(M>0\) such that

\[t>M\quad\Longrightarrow\quad\left\langle X_{0}-\bar{X}_{\star},X(t)-\bar{X}_{ \star}\right\rangle<\epsilon.\]

For that \(M\) and \(t>M\), integrating from \(M\) to \(t\) we get

\[\left[C(s)^{2}\left\|X(s)-\bar{X}_{\star}\right\|^{2}\right]_{M}^ {t} =C(t)^{2}\left\|X(t)-\bar{X}_{\star}\right\|^{2}-C(M)^{2}\left\|X( M)-\bar{X}_{\star}\right\|^{2}\] \[\leq\int_{M}^{t}2C(s)^{2}\beta(s)\left\langle X_{0}-\bar{X}_{ \star},X(s)-\bar{X}_{\star}\right\rangle ds\] \[\leq\int_{M}^{t}\left|2C(s)^{2}\beta(s)\left\langle X_{0}-\bar{X }_{\star},X(s)-\bar{X}_{\star}\right\rangle\right|ds\] \[\leq\int_{M}^{t}2C(s)^{2}\beta(s)\epsilon\,ds=\epsilon\left[C(s) ^{2}\right]_{M}^{t}=\epsilon\left(C(t)^{2}-C(M)^{2}\right).\]

By dividing \(C(t)^{2}\) and organizing,

\[\left\|X(t)-\bar{X}_{\star}\right\|^{2}\leq\epsilon\left(1-\left(\frac{C(M)}{ C(t)}\right)^{2}\right)+\left(\frac{C(M)}{C(t)}\right)^{2}\left\|X(M)-\bar{X}_{ \star}\right\|^{2}.\]

By the way from assumption, we have \(\lim_{t\to\infty}\frac{1}{C(t)}=0\). Therefore we conclude

\[\lim_{t\to\infty}\left\|X(t)-\bar{X}_{\star}\right\|^{2}\leq\epsilon.\]

Since \(\epsilon>0\) was arbitrary, we get the desired result.

## Appendix F Proof for worst case examples

The explicit solution for linear \(A\) is crucially used in worst case examples. Therefore, we first provide proof for it.

### Proof for explicit solution for linear \(A\)

#### f.1.1 Proof of Lemma 4.1

Observing

\[X(t)=\sum_{n=0}^{\infty}\frac{(-tA)^{n}}{\Gamma(n+\gamma+1)}\Gamma(\gamma+1)X_ {0}=X_{0}+\sum_{n=1}^{\infty}\frac{(-tA)^{n}}{\Gamma(n+\gamma+1)}\Gamma(\gamma+ 1)X_{0},\]we can check \(X(0)=X_{0}\). Now by using the property of Gamma function \(\Gamma(x+1)=x\Gamma(x)\), and paying attention to the lower bound of the summation index we have

\[\dot{X}(t) =\sum_{n=1}^{\infty}\frac{(-nA)(-tA)^{n-1}}{\Gamma(n+\gamma+1)} \Gamma(\gamma+1)X_{0}=A\sum_{n=1}^{\infty}\frac{(-n-\gamma+\gamma)(-tA)^{n-1}} {\Gamma(n+\gamma+1)}\Gamma(\gamma+1)X_{0}\] \[=-A\sum_{n=1}^{\infty}\frac{(-tA)^{n-1}}{\Gamma(n+\gamma)}\Gamma( \gamma+1)X_{0}+\gamma\sum_{n=1}^{\infty}\frac{(-t)^{n-1}A^{n}}{\Gamma(n+\gamma +1)}\Gamma(\gamma+1)X_{0}\] \[=-A\sum_{n=0}^{\infty}\frac{(-tA)^{n}}{\Gamma(n+\gamma+1)}\Gamma( \gamma+1)X_{0}+\frac{\gamma}{(-t)}\sum_{n=1}^{\infty}\frac{(-t)^{n}A^{n}}{ \Gamma(n+\gamma+1)}\Gamma(\gamma+1)X_{0}=-AX(t)-\frac{\gamma}{t}\left(X(t)-X_ {0}\right).\]

The solution can be written in another form

\[X(t)=\gamma e^{-tA}t^{-\gamma}\int_{0}^{t}u^{\gamma-1}e^{uA}\,du\,X_{0}.\] (39)

As this is a special case of (9), here we just briefly check this satisfies the ODE and check other details in the proof of Lemma 4.2. By product rule of differentiation,

\[\dot{X} =\gamma\left(\frac{d}{dt}e^{-tA}\right)t^{-\gamma}\int_{0}^{t}u^{ \gamma-1}e^{uA}\,du\,X_{0}+\gamma e^{-tA}\left(\frac{d}{dt}t^{-\gamma}\right) \int_{0}^{t}u^{\gamma-1}e^{uA}\,du\,X_{0}+\gamma e^{-tA}t^{-\gamma}\frac{d}{dt} \left(\int_{0}^{t}u^{\gamma-1}e^{uA}\,du\,X_{0}\right)\] \[=-A(X(t))-\frac{\gamma}{t}X(t)+\gamma e^{-tA}t^{-\gamma}\left(t^ {\gamma-1}e^{tA}\right)X_{0}\] \[=-A(X(t))-\frac{\gamma}{t}\left(X(t)-X_{0}\right).\]

#### f.1.2 Proof of Lemma 4.2

Define

\[X_{*}(t)=\left(I-\frac{Ae^{-tA}}{C(t)}\left(\int_{0}^{t}e^{sA}C(s)ds\right) \right)\,X_{0}.\]

We show \(X_{*}\) is a well-defined solution for (3) with linear \(\mathbf{A}\), then show \(X_{*}\) is equal to \(X(t)\) defined in (9).

We first check well-definedness. By definition, \(C(t)=e^{\int_{t_{0}}^{t}\beta(s)ds}\) is nondecreasing. Also, \(\left\|e^{tA}\right\|\) is also nondecreasing since from monotonicity of \(A\) we have for all \(x\in\mathbb{R}^{n}\)

\[\frac{d}{dt}\left\|e^{tA}x\right\|^{2}=2\left\langle A(e^{tA}x),e^{tA}x \right\rangle\geq 0.\]

Now we see \(X(t)\) is well-defined since

\[\left\|e^{-tA}\frac{1}{C(t)}\left(\int_{0}^{t}e^{sA}C(s)ds\right)\right\| =\left\|\int_{0}^{t}e^{(s-t)A}\frac{C(s)}{C(t)}ds\right\|\] \[\leq\int_{0}^{t}\left\|e^{(s-t)A}\right\|\left|\frac{C(s)}{C(t)} \right|ds\leq\int_{0}^{t}(1\cdot 1)ds=t<\infty.\]

Also above inequality implies second term reaches to zero as \(t\to 0+\), we have \(\lim_{t\to 0+}X_{*}(t)=X_{0}\). Thus defining \(X_{*}(0)=X_{0}\) if necessary, we see \(X_{*}\) satisfies the initial condition with \(X_{*}\in\mathcal{C}([0,\infty),\mathbb{R}^{n})\).

We then check \(X_{*}(t)\) becomes the solution. This is immediate from product rule for differentiation. Since \(\frac{d}{dt}\left(\frac{1}{C(t)}\right)=-\frac{1}{C(t)^{2}}C(t)\beta(t)=-\frac {1}{C(t)}\beta(t)\), we have

\[\dot{X}_{*}(t)= -\left(\frac{d}{dt}e^{-tA}\right)\frac{1}{C(t)}\left(\int_{0}^{t }e^{sA}C(s)ds\right)\,X_{0}\] \[-Ae^{-tA}\left(\frac{d}{dt}\frac{1}{C(t)}\right)\left(\int_{0}^{t }e^{sA}C(s)ds\right)\,X_{0}\] \[-Ae^{-tA}\frac{1}{C(t)}\frac{d}{dt}\left(\int_{0}^{t}e^{sA}C(s)ds \right)\,X_{0}\] \[= -A(X_{*}(t)-X_{0})-\beta(t)(X_{*}(t)-X_{0})-A(X_{0})=-A(X(t))- \beta(t)(X_{*}(t)-X_{0}).\]Finally we now show \(X(t)=X_{*}(t)\), where \(X(t)\) is defined as (9). From integral by parts we have

\[X(t) =e^{-tA}\frac{1}{C(t)}\left(\int_{0}^{t}e^{sA}C(s)\beta(s)ds+C(0)I \right)\,X_{0}\] \[=e^{-tA}\frac{1}{C(t)}\left(\left[e^{sA}C(s)\right]_{0}^{t}-\int_ {0}^{t}Ae^{sA}C(s)ds+C(0)I\right)\,X_{0}\] \[=\left(I-Ae^{-tA}\frac{1}{C(t)}\left(\int_{0}^{t}e^{sA}C(s)ds \right)\right)\,X_{0}=X_{*}(t).\]

### Proof of Theorem 4.3

If \(\beta(t)\equiv 0\), for \(A:=\begin{pmatrix}0&1\\ -1&0\end{pmatrix}\), we have \(X(t)=e^{-tA}\), so

\[\lim_{t\to\infty}\|A(X(t))\|=1\neq 0.\]

So let's consider the case \(\beta\) is not \(\beta(t)\equiv 0\) and \(\beta(t)\geq 0\). For

\[A_{\xi}=2\pi\xi\begin{pmatrix}0&1\\ -1&0\end{pmatrix}\]

name the solution for ODE \(\dot{X}_{\xi}=-A_{\xi}(X_{\xi})-\beta(t)(X_{\xi}-X_{0})\) as \(X_{\xi}\). By Lemma 4.2 we have

\[X_{\xi}(t)=\frac{e^{-tA_{\xi}}}{C(t)}\left(\int_{0}^{t}e^{sA_{\xi}}C(s)\beta( s)ds+C(0)I\right)\,X_{0}.\]

We want to show,

\[\exists\xi\in\mathbb{R},\qquad\lim_{t\to\infty}\|A_{\xi}(X_{\xi}(t))\|\neq 0\]

From \(\lim_{t\to\infty}\frac{1}{C(t)}\neq 0\), we see \(\lim_{t\to\infty}\left\|\frac{e^{-tA_{\xi}}}{C(t)}\right\|=1\cdot\frac{1}{C(t) }\neq 0\). And since \(A_{\xi}\) is invertible except the case \(\xi=0\), we have

\[\lim_{t\to\infty}\|A_{\xi}(X_{\xi}(t))\|=0 \Longleftrightarrow\lim_{t\to\infty}\|X_{\xi}(t)\|=0\] \[\Longleftrightarrow\left\|\lim_{t\to\infty}\int_{0}^{t}e^{sA_{ \xi}}C(s)\beta(s)ds+C(0)I\right\|=0.\]

Now let assume \(\forall\xi\in\mathbb{R},\ \lim_{t\to\infty}\|A_{\xi}(X_{\xi}(t))\|=0\) and lead to contradiction. Define \(f:\mathbb{R}\to\mathbb{R}\) as

\[f(s)=\begin{cases}C(s)\beta(s)&s>0\\ 0&s\leq 0.\end{cases}\]

Then \(\int_{-\infty}^{\infty}f(s)ds=\left[C(s)\right]_{0}^{\infty}=\lim_{t\to\infty }C(t)-C(0)<\infty\) we have \(f\in L^{1}(\mathbb{R},\mathbb{R})\). Note \(\lim_{t\to\infty}C(t)\) exists, since \(C\) is nondecreasing and by the assumption is bounded.

Now setting \(X_{0}=(1,0)^{T}\) and from \(e^{sA_{\xi}}=\begin{pmatrix}\cos 2\pi\xi s&\sin 2\pi\xi s\\ -\sin 2\pi\xi s&\cos 2\pi\xi s\end{pmatrix}\), we see

\[\int_{0}^{\infty}e^{sA_{\xi}}C(s)\beta(s)ds\ X_{0} =\left(\int_{0}^{\infty}\cos(2\pi s\xi)f(s)ds\,\ \int_{0}^{\infty}\sin(-2\pi s\xi)f(s)ds\right)^{T}\] \[=\left(Re(\hat{f}(\xi))\,\ Im(\hat{f}(\xi))\right)^{T}\]

where \(\hat{f}(\xi)\) is Fourier transform of \(f\).

From assumption we have \(\forall\xi\in\mathbb{R}\), \(\left\|\int_{0}^{\infty}e^{sA_{\xi}}C(s)\beta(s)dsX_{0}+C(0)X_{0}\right\|=0\), we have

\[\left(Re(\hat{f}(\xi))\,\ Im(\hat{f}(\xi))\right)^{T}\equiv-C(0)X_{0}\]By the way, from Fourier theory we know \(\hat{f}\) vanishes at infinity, thus

\[\|C(0)X_{0}\|=\lim_{\xi\rightarrow\infty}\left|\hat{f}(\xi)\right|=0.\]

Therefore we have \(\hat{f}(\xi)=0\) for all \(\xi\in\mathbb{R}\).

Now \(\hat{f}\equiv 0\) is clearly \(L^{1}(\mathbb{R},\mathbb{R})\), we can apply _Fourier inversion formula_ and conclude \(f\equiv 0\) almost everywhere.

However, \(f(s)=C(s)\beta(s)\) is not zero almost everywhere since \(\beta(s)\) is not constantly zero. Thus a contradiction, we get the desired result.

### Proof of Theorem 4.4

#### f.3.1 Proof for the case \(p=1\), \(\gamma\geq 1\)

Recall from (39), we know

\[X(t)=\gamma e^{-tA}t^{-\gamma}\int_{0}^{t}e^{sA}s^{\gamma-1}ds\ X_{0}.\]

Without loss of generality, we consider the case \(X_{0}=(1,0)^{T}\).

1. \(\gamma=1\) Plugging \(\gamma=1\) gives \[X(t)=\frac{e^{-tA}}{t}A^{-1}\left[e^{sA}\right]_{0}^{t}\ X_{0}=\frac{1}{t}A^{-1} \left(I-e^{-tA}\right)(1,0)^{T}=\frac{1}{t}A^{-1}\left(1-\cos t,\sin t\right) ^{T}\] Therefore \[\lim_{t\rightarrow\infty}\|tA(X(t))\|=\lim_{t\rightarrow\infty}\left\|(1- \cos t,\sin t)^{T}\right\|\neq 0.\]
2. \(\gamma>1\) We will show \(\lim_{t\rightarrow\infty}\|tA(X(t))\|=\gamma\). With change of variable \(s=tv\) and integration by parts we have \[\left(\frac{1}{\gamma}\right)tA(X(t)) =e^{-tA}At^{-(\gamma-1)}\int_{0}^{t}e^{sA}s^{\gamma-1}ds\ X_{0}\] \[=e^{-tA}tA\int_{0}^{1}e^{tvA}v^{\gamma-1}dv\ X_{0}\] \[=e^{-tA}\left[e^{tvA}v^{\gamma-1}\right]_{0}^{1}\ X_{0}-(\gamma-1 )e^{-tA}\int_{0}^{1}e^{tvA}v^{\gamma-2}dv\ X_{0}\] \[=X_{0}-(\gamma-1)e^{-tA}\left(\int_{0}^{1}\cos(tv)v^{\gamma-2} dv\,\ \int_{0}^{1}\sin(tv)v^{\gamma-2}dv\right)^{T}\] By the way, from \(\gamma>1\) we have \(v^{\gamma-2}\in L^{1}[0,1]\), by Riemann-Lebesgue lemma we have \[\lim_{t\rightarrow\infty}\int_{0}^{1}\cos(tv)v^{\gamma-2}dv=\lim_{t \rightarrow\infty}\int_{0}^{1}\sin(tv)v^{\gamma-2}dv=0.\] Observe as \(e^{-tA}\) is a rotation, we know \[\left\|e^{-tA}\left(\int_{0}^{1}\cos(tv)v^{\gamma-2}dv\,\ \int_{0}^{1}\sin(tv)v^{\gamma-2} dv\right)\right\|=\left\|\left(\int_{0}^{1}\cos(tv)v^{\gamma-2}dv\,\ \int_{0}^{1}\sin(tv)v^{\gamma-2} dv\right)\right\|.\] Taking limit \(t\rightarrow\infty\) we know right hand side converges to zero, we conclude \[\lim_{t\rightarrow\infty}e^{-tA}\left(\int_{0}^{1}\cos(tv)v^{\gamma-2}dv\, \ \int_{0}^{1}\sin(tv)v^{\gamma-2}dv\right)=0.\] Therefore, \[\lim_{t\rightarrow\infty}\|tA(X(t))\| =\left\|\gamma X_{0}-\gamma(\gamma-1)\lim_{t\rightarrow\infty}e ^{-tA}\left(\int_{0}^{1}\cos(tv)v^{\gamma-2}dv\,\ \int_{0}^{1}\sin(tv)v^{\gamma-2} dv\right)^{T}\right\|\] \[=\gamma\left\|X_{0}\right\|=\gamma\]

#### f.3.2 Proof for the case \(p=1\), \(\gamma<1\)

Since \(A\), \(e^{tA}\) are invertible, we see

\[\lim_{t\to\infty}t^{2\gamma}\left\|A(X(t))\right\|^{2}\neq 0 \Longleftrightarrow\lim_{t\to\infty}t^{\gamma}\left\|A(X(t))\right\|\neq 0 \Longleftrightarrow\lim_{t\to\infty}t^{\gamma}\left\|\frac{1}{\gamma}e^{tA}X( t)\right\|\neq 0.\]

Therefore it is enough to observe \(\frac{1}{\gamma}e^{tA}t^{\gamma}X(t)\). Again for \(X_{0}=(1,0)^{T}\)

\[\frac{1}{\gamma}e^{tA}t^{\gamma}X(t) =\int_{0}^{t}e^{sA}s^{\gamma-1}ds\ X_{0}\] \[=\left(\int_{0}^{t}\cos(s)s^{\gamma-1}ds\,\ \int_{0}^{t}\sin(s)s^{ \gamma-1}ds\right)^{T}.\]

Since \(s^{\gamma-1}\) decrease monotonically to zero, we may apply similar argument with alternative series test.

Define \(a_{n}=\int_{(n-1)\pi}^{n\pi}\sin(s)s^{\gamma-1}ds\) and name \(S_{n}=\sum_{k=1}^{n}a_{n}\). Then for \(m\in\mathbb{N}\), \(t>2m\pi\) we see

\[S_{2m}\leq\int_{0}^{t}\sin(s)s^{\gamma-1}ds\leq S_{2m-1}.\]

By the way \(S=\lim_{n\to\infty}S_{n}\) exists by alternative series test, thus we have

\[\lim_{t\to\infty}\int_{0}^{t}\sin(s)s^{\gamma-1}ds=S\]

by squeeze theorem. Note \(S\geq S_{2}>0\). With similar argument, we can conclude \(\lim_{t\to\infty}\int_{0}^{t}\cos(s)s^{\gamma-1}ds\) also exists.

Therefore we conclude \(\lim_{t\to\infty}\left\|t^{\gamma}X(t)\right\|\neq 0\) since

\[\lim_{t\to\infty}\left\|t^{\gamma}X(t)\right\|=\lim_{t\to\infty} \gamma\left\|\left(\int_{0}^{t}\cos(s)s^{\gamma-1}ds\,\ \int_{0}^{t}\sin(s)s^{\gamma-1}ds\right)^{T}\right\|\geq \gamma S>0.\]

#### f.3.3 Proof for the case \(0<p<1\)

Recall from (9), we have

\[X(t)=\frac{e^{-tA}}{C(t)}\left(\int_{0}^{t}e^{sA}C(s)\beta(s)ds+ C(0)I\right)X_{0}.\]

Our goal is to show

\[\lim_{t\to\infty}t^{2p}\left\|A(X(t))\right\|^{2}\neq 0.\]

We first observe, it is enough to show

\[\lim_{t\to\infty}\frac{1}{\beta(t)C(t)}\int_{0}^{t}C(s)\beta(s) \sin s\,ds\neq 0.\]

This follows from below facts.

1. Since \(\beta(t)=\frac{\gamma}{t^{p}}\) and \(A\) is linear and invertible, \[\lim_{t\to\infty}t^{2p}\left\|A(X(t))\right\|^{2}=\lim_{t\to \infty}\frac{\gamma^{2}}{\beta(t)^{2}}\left\|A(X(t))\right\|^{2}\neq 0\quad \Longleftrightarrow\quad\lim_{t\to\infty}\left\|\frac{1}{\beta(t)}X(t) \right\|\neq 0.\]
2. Recall from (31), we have \(C(t)=e^{\frac{\gamma}{1-p}t^{1-p}}\) for \(\beta(t)=\frac{\gamma}{t^{p}}\), so \(C(0)=\lim_{t\to 0+}C(t)=1\) and \(\lim_{t\to\infty}\frac{1}{\beta(t)C(t)}=0\). Thus \(\lim_{t\to\infty}\frac{1}{\beta(t)C(t)}\left\|C(0)I\right\|=0\), second term is ignorable. Therefore the problem reduces to \[\lim_{t\to\infty}\left\|\frac{e^{-tA}}{\beta(t)C(t)}\left(\int_{0}^{t}C(s) \beta(s)e^{-sA}ds\right)X_{0}\right\|\neq 0.\]3. Since \(e^{tA}\) is linear and invertible, problem reduces to \[\lim_{t\to\infty}\left\|\frac{1}{\beta(t)C(t)}\left(\int_{0}^{t}C(s)\beta(s)e^{-sA }ds\right)X_{0}\right\|\neq 0.\]
4. Again without loss of generality, let \(X_{0}=(1,0)^{T}\). Recalling \(e^{-sA}=\begin{pmatrix}\cos s&\sin s\\ -\sin s&\cos s\end{pmatrix}\), focusing on one component, we see it is sufficient to show \[\lim_{t\to\infty}\frac{1}{\beta(t)C(t)}\int_{0}^{t}C(s)\beta(s)\sin s\,ds\neq 0.\]

Now we prove the statement. For convenience, name \(D(t)=\beta(t)C(t)\). We want to show

\[\lim_{t\to\infty}\underbrace{\int_{0}^{t}\frac{D(s)}{D(t)}\sin s\,ds}_{=S(t)} \neq 0.\]

We first observe

\[\lim_{t\to\infty}\sup_{\delta\in[0,\delta]}\left|\frac{D(t+\delta)}{D(t+\pi)} -1\right|=0.\]

To do so, we first show \(\lim_{t\to\infty}\frac{D(t+\delta)}{D(t+\pi)}=1\) for \(\delta\in[0,\pi]\). Take \(\delta\in[0,\pi]\). Observe

\[\frac{D(t+\delta)}{D(t+\pi)}=\frac{\beta(t+\delta)C(t+\delta)}{\beta(t+\pi)C (t+\pi)}=\left(\frac{t+\pi}{t+\delta}\right)^{p}\frac{e^{\frac{\gamma}{1-p}(t +\delta)^{1-p}}}{e^{\frac{\gamma}{1-p}(t+\pi)^{1-p}}}=\left(\frac{t+\pi}{t+ \delta}\right)^{p}e^{\frac{\gamma}{1-p}(t+\delta)^{1-p}\left(1-\left(\frac{t +\pi}{t+\delta}\right)^{1-p}\right)}.\]

Considering L'ospital's rule for the exponent, we see

\[\lim_{t\to\infty}\left(t+\delta\right)^{1-p}\left(1-\left(\frac{t+\pi}{t+ \delta}\right)^{1-p}\right)=\lim_{t\to\infty}\frac{1-\left(\frac{t+\pi}{t+ \delta}\right)^{1-p}}{\left(t+\delta\right)^{p-1}}=\lim_{t\to\infty}\frac{ \frac{-\pi(p-1)\left(\frac{t+\pi}{t+\delta}\right)^{-p}}{\left(t+\delta \right)^{2}}}{\left(p-1\right)\left(t+\delta\right)^{p-2}}=\lim_{t\to\infty} \frac{-\pi}{\left(t+\pi\right)^{p}}=0.\]

As the exponent reaches to zero, we conclude \(\lim_{t\to\infty}\frac{D(t+\delta)}{D(t+\pi)}=1\).

Now from

\[\dot{D}(t)=C(t)\left(\beta(t)^{2}+\dot{\beta}(t)\right)=e^{\frac{\gamma}{1-p} t^{1-p}}\gamma t^{-2p-1}\left(\gamma t-pt^{p}\right),\]

we see \(D(t)\) is nondecreasing for \(t\geq\left(\frac{\gamma}{p}\right)^{\frac{1}{p-1}}\) since \(0<p<1\). Therefore for \(t\geq\left(\frac{\gamma}{p}\right)^{\frac{1}{p-1}}\) we have \(\sup_{\delta\in[0,\delta]}\left|\frac{D(t+\delta)}{D(t+\pi)}-1\right|=\max \left\{\left|\frac{D(t)}{D(t+\pi)}-1\right|,\left|\frac{D(t+\delta)}{D(t+\pi) }-1\right|\right\}\), so it reaches to zero as \(t\to\infty\).

Now we prove desired statement. Proof by contradiction. Suppose \(\lim_{t\to\infty}S(t)=0\). Then for \(0<\epsilon<\frac{1}{2}\), there is \(T_{1}>0\) such that \(t>T_{1}\) implies \(|S(t)|<\epsilon\). Now for \(t>T_{1}\), observe

\[2\epsilon >|S(t+\pi)-S(t)|=\left|\int_{0}^{t+\pi}\frac{D(s)}{D(t+\pi)}\sin s \,ds-\int_{0}^{t}\frac{D(s)}{D(t)}\sin s\,ds\right|\] \[=\left|\frac{1}{D(t+\pi)}\left(\int_{0}^{t+\pi}D(s)\sin s\,ds- \int_{0}^{t}D(s)\sin s\,ds\right)+\left(\frac{1}{D(t+\pi)}-\frac{1}{D(t)} \right)\int_{0}^{t}D(s)\sin s\,ds\right|\] \[\geq\left|\int_{t}^{t+\pi}\frac{D(s)}{D(t+\pi)}\sin s\,ds\right| -\left|\left(\frac{D(t)}{D(t+\pi)}-1\right)\int_{0}^{t}\frac{D(s)}{D(t)}\sin s \,ds\right|.\]

On the other hand, there is \(T_{2}\) such that \(t>T_{2}\) implies \(\sup_{\delta\in[0,\pi]}\left|\frac{D(t+\delta)}{D(t+\pi)}-1\right|<\frac{ \epsilon}{2}\). Now for \(t=2n\pi>\max\{T_{1},T_{2}\}\),

\[2\epsilon>\left|\int_{2n\pi}^{(2n+1)\pi}\frac{D(s)}{D(t+\pi)}\sin s\,ds\right| -\left|\frac{D(t)}{D(t+\pi)}-1\right|\times|S(t)|>\int_{2n\pi}^{(2n+1)\pi}(1- \epsilon)\left|\sin s\right|\,ds-\epsilon=2-2\epsilon.\]

This contradicts the fact \(\epsilon<\frac{1}{2}\), we prove the assumption \(\lim_{t\to\infty}\int_{0}^{t}S(t)=0\) is not true. Therefore \(\lim_{t\to\infty}\int_{0}^{t}S(t)\neq 0\).

Proof of convergence analysis for discrete counterpart

### Correspondence between discrete method in Theorem 5.1 and continuous model (6)

To check the correspondence of the method with (6), we provide a informal derivation. Assume operator \(\mathbf{A}\) is continuous. Then we have \(y^{k}=x^{k+1}+\mathbf{A}x^{k+1}\), by substituting \(y^{k}\) and \(y^{k-1}\), the method can be equivalently written as

\[x^{k+1}+\mathbf{A}x^{k+1}=\frac{k^{p}}{k^{p}+\gamma}(x^{k}-\mathbf{A}x^{k})+ \frac{\gamma}{k^{p}+\gamma}x^{0}.\] (40)

This can be considered as a special case of below method, with \(h=1\).

\[x^{k+1}+h\mathbf{A}x^{k+1}=\frac{k^{p}}{k^{p}+h^{1-p}\gamma}(x^{k}-h\mathbf{A} x^{k})+\frac{h^{1-p}\gamma}{k^{p}+h^{1-p}\gamma}x^{0}.\]

Dividing by \(h\) both sides and reorganizing, we have

\[\frac{x^{k+1}-x^{k}}{h}=-\mathbf{A}x^{k+1}-\frac{h^{p}k^{p}}{h^{p}k^{p}+h \gamma}\mathbf{A}x^{k}-\frac{\gamma}{h^{p}k^{p}+h\gamma}\left(x^{k}-x^{0} \right).\]

Identifying \(hk=t\), \(x^{k}=X(t)\) and taking \(h\to 0\), we have

\[\dot{X}(t)=-2\mathbf{A}(X(t))-\frac{\gamma}{t^{p}}\left(X-X_{0}\right).\]

As monotonicity is preserved for scalar multiple, rescaling \(2\mathbf{A}\) to \(\mathbf{A}\) does not change the class of operators that the ODE covers. For notation simplicity, replacing \(2\mathbf{A}\) to \(\mathbf{A}\) we have

\[\dot{X}(t)=-\mathbf{A}(X(t))-\frac{\gamma}{t^{p}}\left(X-X_{0}\right).\]

### Proof of boundedness of \(\left\|x^{k}\right\|\)

While proving Theorem 5.1, we need a upper bound for \(\left\|x^{k}\right\|\). Therefore we first prove following lemma.

**Lemma G.1**.: _Let \(\mathbf{A}\) be a maximal monotone operator. Consider a method_

\[x^{k} =\mathbf{J}_{\mathbf{A}}y^{k-1}\] \[y^{k} =\left(1-\beta_{k}\right)\left(2x^{k}-y^{k-1}\right)+\beta_{k}x^ {0}\]

_for \(k=1,2,\ldots\), with sequence \(\left\{\beta_{k}\right\}_{k\in\mathbb{N}^{n}}0\leq\beta_{k}\leq 1\) and initial condition \(y^{0}=x^{0}\in\mathbb{R}^{n}\). Then following holds_

\[\left\|x^{k}-x^{\star}\right\|\leq\left\|x^{0}-x^{\star}\right\|.\]

_for \(x^{\star}\in\mathrm{Zer}\mathbf{A}\). And so, \(\left\|x^{k}-x^{0}\right\|\leq 2\left\|x^{0}-x^{\star}\right\|\)._

Proof.: Recall from Lemma D.2, we know for \(\mathbf{T}=\mathbf{R}_{\mathbf{A}}=2\mathbf{J}_{\mathbf{A}}-\mathbf{I}\) given method is equivalent to below method

\[y^{k+1}=\beta_{k}y^{0}+\left(1-\beta_{k}\right)\mathbf{T}y^{k}.\]

Note that \(x^{\star}\in\mathrm{Zer}\,\mathbf{A}\Leftrightarrow 0\in\mathbf{A}x^{\star} \Leftrightarrow x^{\star}=\mathbf{J}_{\mathbf{A}}x^{\star}\Leftrightarrow x^{ \star}=\mathbf{T}x^{\star}\Leftrightarrow x^{\star}\in\mathrm{Fix}\,\mathbf{T}\). We first prove \(\left\|y^{k}-x^{\star}\right\|\leq\left\|y^{0}-x^{\star}\right\|\) by induction. The statement is trivially true when \(k=0\). Now suppose the statement is true for \(k\in\mathbb{N}\). Then

\[\left\|y^{k+1}-x^{\star}\right\| =\left\|\beta_{k}\left(y^{0}-x^{\star}\right)+\left(1-\beta_{k} \right)\left(\mathbf{T}y^{k}-x^{\star}\right)\right\|\] \[\leq\beta_{k}\left\|y^{0}-x^{\star}\right\|+\left(1-\beta_{k} \right)\left\|\mathbf{T}y^{k}-x^{\star}\right\|\] \[\leq\beta_{k}\left\|y^{0}-x^{\star}\right\|+\left(1-\beta_{k} \right)\left\|y^{k}-x^{\star}\right\|\] \[\leq\beta_{k}\left\|y^{0}-x^{\star}\right\|+\left(1-\beta_{k} \right)\left\|y^{0}-x^{\star}\right\|=\left\|y^{0}-x^{\star}\right\|.\]

The first inequality is just triangular inequality, the second inequality comes from the fact \(x^{\star}\in\mathrm{Fix}\,\mathbb{T}\) and \(\mathbb{T}\) is nonexpansive, and the last inequality is from induction hypothesis. By induction, we have \(\left\|y^{k}-x^{\star}\right\|\leq\left\|y^{0}-x^{\star}\right\|\) for \(k=0,1,\ldots\).

Define \(\tilde{\mathbf{A}}x^{k}=y^{k-1}-x^{k}\), then \(\tilde{\mathbf{A}}x^{k}\in\mathbf{A}x^{k}\) since \(\mathbf{J}_{\mathbf{A}}y^{k-1}=x^{k}\). Observe for \(k=1,2,\dots\), from monotone inequality we have

\[\left\|y^{k-1}-x^{\star}\right\|^{2}=\left\|\tilde{\mathbf{A}}x^{k}+(x^{k}-x^{ \star})\right\|^{2}=\left\|\tilde{\mathbf{A}}x^{k}\right\|^{2}+2\left\langle \tilde{\mathbf{A}}x^{k},x^{k}-x^{\star}\right\rangle+\left\|x^{k}-x^{\star} \right\|^{2}\geq\left\|x^{k}-x^{\star}\right\|^{2}.\]

Therefore we conclude

\[\left\|x^{k}-x^{\star}\right\|\leq\left\|y^{k-1}-x^{\star}\right\|\leq\left\|y ^{0}-x^{\star}\right\|=\left\|x^{0}-x^{\star}\right\|.\]

The latter statement holds directly from triangular inequality,

\[\left\|x^{k}-x^{0}\right\|\leq\left\|x^{k}-x^{\star}\right\|+\left\|x^{\star} -x^{0}\right\|\leq 2\left\|x^{0}-x^{\star}\right\|.\]

### Proof of Theorem 5.1

The outline of the proofs originate from continuous proofs. To simplify calculations, instead of directly deriving discrete counterpart of Proposition 3.2, we consider rescaled conservation law for each cases. By considering dilated coordinate \(W_{1}=X-X_{0}\) for \(p>1\), \(W_{2}=t^{p}\left(X-X_{0}\right)\) for \(0<p<1\), \(W_{3}=t\left(X-X_{0}\right)\) for \(p=1\), \(\gamma\geq 1\) and \(W_{4}=t^{\gamma}\left(X-X_{0}\right)\) for \(p=1\), \(0<\gamma<1\), we obtain below conservation laws.

\[E_{1} \equiv\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+\frac{2\gamma} {t^{p}}\left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle+\left( \frac{\gamma^{2}}{t^{2p}}+\frac{\gamma p}{t^{p+1}}\right)\left\|X(t)-X_{0} \right\|^{2}\] \[\quad+\int_{0}^{t}\left\langle\frac{d}{ds}\tilde{\mathbf{A}}(X(s )),\dot{X}(s)\right\rangle ds+\int_{0}^{t}\frac{\gamma}{sp}\left\|\dot{X}(s)+ \frac{p}{s}(X(s)-X_{0})\right\|^{2}ds+\frac{1}{2}\int_{0}^{t}\frac{\gamma p(p -1)}{sp^{p+2}}\left\|X(s)-X_{0}\right\|^{2}ds\] \[E_{2} \equiv t^{2p}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2\gamma t ^{p}\left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle+\left(\gamma ^{2}-\gamma pt^{p-1}\right)\left\|X(t)-X_{0}\right\|^{2}\] \[\quad+\int_{0}^{t}2s^{2p}\left\langle\frac{d}{ds}\tilde{\mathbf{ A}}(X(s)),\dot{X}(s)\right\rangle ds+\int_{0}^{t}2s^{p}\left(\gamma-ps^{p-1}\right) \left\|\dot{X}(s)\right\|^{2}ds+\int_{0}^{t}\gamma p(p-1)s^{p-2}\left\|X(s)-X_ {0}\right\|^{2}ds\] \[E_{3} \equiv t^{2}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2\gamma t \left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle+\gamma\left( \gamma-1\right)\left\|X(t)-X_{0}\right\|^{2}\] \[\quad+\int_{t_{0}}^{t}2s^{2}\left\langle\frac{d}{ds}\tilde{ \mathbf{A}}(X(s)),\dot{X}(s)\right\rangle ds+\int_{t_{0}}^{t}2s\left(\gamma-1 \right)\left\|\dot{X}(s)\right\|^{2}ds\] \[E_{4} \equiv t^{2\gamma}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2 \gamma t^{2\gamma-1}\left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle +\gamma(\gamma-1)t^{2\gamma-2}\left\|X(t)-X_{0}\right\|^{2}\] \[\quad+\int_{0}^{t}2s^{2\gamma}\left\langle\frac{d}{ds}\tilde{ \mathbf{A}}(X(s)),\dot{X}(s)\right\rangle\ ds+\int_{0}^{t}2\gamma(\gamma-1)s^{2 \gamma-3}\left\|X(s)-X_{0}\right\|^{2}\ ds.\]

Lyapunov style proof can be obtained by considering below functions

\[U_{1}(t) =\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+\frac{2\gamma}{t^{p}} \left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle+\left(\frac{\gamma ^{2}}{t^{2p}}+\frac{\gamma p}{t^{p+1}}\right)\left\|X(t)-X_{0}\right\|^{2}\] \[U_{2}(t) =t^{2p}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2\gamma t^{p} \left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle+\left(\gamma^{2}- \gamma pt^{p-1}\right)\left\|X(t)-X_{0}\right\|^{2}\] \[U_{3}(t) =t^{2}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2\gamma t \left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle+\gamma\left(\gamma -1\right)\left\|X(t)-X_{0}\right\|^{2}\] \[U_{4}(t) =t^{2\gamma}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+\gamma t^{2 \gamma-1}\left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle+\gamma( \gamma-1)t^{2\gamma-2}\left\|X(t)-X_{0}\right\|^{2}.\]

The main blocks of the calculations corresponds to continuous cases, but there are some 'errors' in terms of \(\tilde{\mathbf{A}}x^{k}\) and \(x^{k}-x^{0}\) occur due to discretization. The proofs are done by showing these 'errors' don't effect to the conclusions.

1. Preparation. For all cases, we will consider functions of the form \[U^{k}=a_{k}\left\|\tilde{\mathbf{A}}x^{k}\right\|^{2}+b_{k}\left\langle\tilde{ \mathbf{A}}x^{k},x^{k}-x^{0}\right\rangle+c_{k}\left\|x^{k}-x^{0}\right\|^{2},\] (41)

[MISSING_PAGE_FAIL:64]

Now dividing both sides by \(k^{p}(k^{p}+\gamma)\), we get the desired result. * Proof for (44) This can be checked by just expanding the inner product of left hand side. * Proof for (45) First, observe \[c_{k+1}\left\|x^{k+1}-x^{0}\right\|^{2}-c_{k}\left\|x^{k}-x^{0} \right\|^{2}\] \[=c_{k+1}\left(\left\|x^{k+1}-x^{0}\right\|^{2}-\left\|x^{k}-x^{0} \right\|^{2}\right)+\left(c_{k+1}-c_{k}\right)\left\|x^{k}-x^{0}\right\|^{2}\] \[=c_{k+1}\left(\left\langle x^{k+1}-x^{k},x^{k+1}-x^{0}\right\rangle +\left\langle x^{k+1}-x^{k},x^{k}-x^{0}\right\rangle\right)+\left(c_{k+1}-c_{ k}\right)\left\|x^{k}-x^{0}\right\|^{2}.\] Reorganizing (40), we get two different expressions for \(x^{k+1}-x^{k}\). \[x^{k+1}-x^{k} =-\left(\tilde{\mathbf{A}}x^{k+1}+\frac{k^{p}}{k^{p}+\gamma} \tilde{\mathbf{A}}x^{k}\right)-\frac{\gamma}{k^{p}+\gamma}\left(x^{k}-x^{0}\right)\] \[x^{k+1}-x^{k} =-\left(\frac{k^{p}+\gamma}{k^{p}}\tilde{\mathbf{A}}x^{k+1}+ \tilde{\mathbf{A}}x^{k}\right)-\frac{\gamma}{k^{p}}\left(x^{k+1}-x^{0}\right).\] Now plugging these to previous equality, be get the desired result. \[c_{k+1}\left\|x^{k+1}-x^{0}\right\|^{2}-c_{k}\left\|x^{k}-x^{0} \right\|^{2}\] \[=-c_{k+1}\bigg{(}\left\langle\frac{k^{p}+\gamma}{k^{p}}\tilde{ \mathbf{A}}x^{k+1}+\tilde{\mathbf{A}}x^{k}+\frac{\gamma}{k^{p}}\left(x^{k+1}- x^{0}\right),x^{k+1}-x^{0}\right\rangle\] \[\qquad\qquad+\left\langle\tilde{\mathbf{A}}x^{k+1}+\frac{k^{p}}{k ^{p}+\gamma}\tilde{\mathbf{A}}x^{k}+\frac{\gamma}{k^{p}+\gamma}\left(x^{k}-x^ {0}\right),x^{k}-x^{0}\right\rangle\bigg{)}+\left(c_{k+1}-c_{k}\right)\left\| x^{k}-x^{0}\right\|^{2}\] \[=-c_{k+1}\frac{k^{p}+\gamma}{k^{p}}\left\langle\tilde{\mathbf{A} }x^{k+1},x^{k+1}-x^{0}\right\rangle-c_{k+1}\left\langle\tilde{\mathbf{A}}x^{k +1},x^{k}-x^{0}\right\rangle\] \[\quad-c_{k+1}\left\langle\tilde{\mathbf{A}}x^{k},x^{k+1}-x^{0} \right\rangle-c_{k+1}\frac{k^{p}}{k^{p}+\gamma}\left\langle\tilde{\mathbf{A}}x ^{k},x^{k}-x^{0}\right\rangle\] \[\quad-c_{k+1}\frac{\gamma}{k^{p}}\left\|x^{k+1}-x^{0}\right\|^{2} -c_{k+1}\frac{\gamma}{k^{p}+\gamma}\left\|x^{k}-x^{0}\right\|^{2}+\left(c_{k+1 }-c_{k}\right)\left\|x^{k}-x^{0}\right\|^{2}\]
* \(\left\|\tilde{\mathbf{A}}(x^{k})\right\|^{2}=\mathcal{O}(1)\) for \(p>0\), \(\gamma>0\). Plugging \[a_{k} =1+\frac{\gamma}{2k^{p}}, b_{k} =\frac{\gamma}{k^{p}}, c_{k+1} =\frac{\gamma k^{p}}{4\left(k^{p}+\frac{\gamma}{2}\right)}\left( \frac{\gamma}{k^{2p}}-\left(\frac{1}{k^{p}}-\frac{1}{(k+1)^{p}}\right)\right)\] \[\lambda_{k} =1+\frac{\gamma}{2k^{p}}, \tau_{k} =c_{k+1}\] to (41) and (42), we obtain \[U^{k+1}-U^{k}+\left(1+\frac{\gamma}{2k^{p}}-\frac{\gamma k^{p}}{4 \left(k^{p}+\frac{\gamma}{2}\right)}\left(\frac{\gamma}{k^{2p}}-\left(\frac{1 }{k^{p}}-\frac{1}{(k+1)^{p}}\right)\right)\right)\left\langle\tilde{\mathbf{A} }x^{k+1}-\tilde{\mathbf{A}}x^{k},x^{k+1}-x^{k}\right\rangle\] \[=-\left(\frac{\gamma\left(2k^{p}+\gamma\right)}{2k^{2p}}+\frac{ \gamma}{2}\left(\frac{1}{k^{p}}-\frac{1}{(k+1)^{p}}\right)\right)\left\| \tilde{\mathbf{A}}x^{k+1}\|^{2}-\frac{\gamma\left(2k^{p}+\gamma\right)}{2k^{ p}\left(k^{p}+\gamma\right)}\|\tilde{\mathbf{A}}x^{k}\|^{2}\] \[\quad-\left(\frac{\gamma^{2}}{k^{p}\left(k^{p}+\gamma\right)}+ \frac{\gamma k^{p}}{2(k^{p}+\gamma)}\left(\frac{1}{k^{p}}-\frac{1}{(k+1)^{p}} \right)\right)\left\langle\tilde{\mathbf{A}}x^{k},x^{k}-x^{0}\right\rangle\] \[\quad-\frac{\gamma^{2}}{2\left(2k^{p}+\gamma\right)}\left(\frac{ \gamma}{k^{2p}}-\left(\frac{1}{k^{p}}-\frac{1}{(k+1)^{p}}\right)\right)\left\| x^{k+1}-x^{0}\right\|^{2}\]\[\quad-\frac{\gamma^{2}k^{p}}{2\left(k^{p}+\gamma\right)\left(2k^{p}+ \gamma\right)}\left(\frac{\gamma}{k^{2p}}-\left(\frac{1}{k^{p}}-\frac{1}{(k+1)^ {p}}\right)\right)\left\|x^{k}-x^{0}\right\|^{2}+\left(c_{k+1}-c_{k}\right) \left\|x^{k}-x^{0}\right\|^{2}\] \[\quad-\frac{\gamma\left(2k^{p}+\gamma\right)}{2k^{p}\left(k^{p}+ \gamma\right)}\left\|\tilde{\mathbf{A}}x^{k}+\frac{1}{2k^{p}+\gamma}\left( \gamma+\frac{k^{2p}}{2}\left(\frac{1}{k^{p}}-\frac{1}{(k+1)^{p}}\right)\right) \left(x^{k}-x^{0}\right)\right\|^{2}\] \[\quad-\frac{1}{2}\left(\frac{\gamma}{k^{p}}-\frac{\gamma}{(k+1)^ {p}}\right)\left\|\tilde{\mathbf{A}}x^{k+1}\right\|^{2}\] \[\quad-\underbrace{\frac{\gamma}{2\left(2k^{p}+\gamma\right)} \left(2\gamma\left(\frac{1}{(k+1)^{p}}-\frac{1}{k^{p}}\right)+\frac{k^{2p}}{4} \left(\frac{1}{k^{p}}-\frac{1}{(k+1)^{p}}\right)^{2}\right)}_{s_{k,1}}\left\| x^{k+1}-x^{0}\right\|^{2}\] \[\quad+\left(c_{k+1}-c_{k}\right)\left\|x^{k}-x^{0}\right\|^{2}.\]

The continuous counterpart of above equality is

\[\dot{U_{1}}(t)+\left\langle\frac{d}{dt}\tilde{\mathbf{A}}(X(t)), \dot{X}(t)\right\rangle\] \[=-\frac{\gamma}{t^{p}}\left\|\tilde{\mathbf{A}}(X(t))+\left( \frac{\gamma}{t^{p}}+\frac{p}{t}\right)(X(t)-X_{0})\right\|^{2}-\frac{1}{2} \frac{\gamma p(p-1)}{t^{p+2}}\left\|X(t)-X_{0}\right\|^{2},\]

which can be obtained by differentiating and reorganizing the conservation law for \(E_{1}\). Note, as \(\frac{k^{2p}}{2}\left(\frac{1}{k^{p}}-\frac{1}{(k+1)^{p}}\right)=\mathcal{O} \left(k^{2p}\right)\mathcal{O}\left(\frac{1}{k^{p+1}}\right)=\mathcal{O}\left( \frac{1}{k^{1-p}}\right)\), the order of the term \(\frac{1}{2k^{p}+\gamma}\left(\gamma+\frac{k^{2p}}{2}\left(\frac{1}{k^{p}}- \frac{1}{(k+1)^{p}}\right)\right)\) corresponds to \(\left(\frac{\gamma}{t^{p}}+\frac{p}{t}\right)\). Thus we can see the sum of first two terms on the right hand side of the equality for discrete setting corresponds to the first term of the right hand side of the equality for continuous setting.

We can observe that \(s_{k,1},s_{k,0}=\mathcal{O}\left(\frac{1}{k^{2p+1}}\right)+\mathcal{O}\left( \frac{1}{k^{p+2}}\right)\). And since \(c_{k}=\mathcal{O}\left(\frac{1}{k^{2p}}\right)+\mathcal{O}\left(\frac{1}{k^{p +1}}\right)\), we have \(c_{k+1}-c_{k}=\mathcal{O}\left(\frac{1}{k^{2p+1}}\right)+\mathcal{O}\left( \frac{1}{k^{p+2}}\right)\) as well. Therefore the coefficients of \(\left\|x^{k+1}-x^{0}\right\|^{2}\) and \(\left\|x^{k}-x^{0}\right\|^{2}\) are summable for \(p>0\). From Lemma G.1, we know \(\left\|x^{k+1}-x^{0}\right\|^{2}\) and \(\left\|x^{k}-x^{0}\right\|^{2}\) are bounded by \(4\left\|x^{0}-x^{\star}\right\|^{2}\).

The term \(\left\langle\tilde{\mathbf{A}}x^{k+1}-\tilde{\mathbf{A}}x^{k},x^{k+1}-x^{k}\right\rangle\) on the left hand side is nonnegative from monotonicity of \(\tilde{\mathbf{A}}\), and the coefficient is nonnegative as well. The coefficient of \(\left\|\tilde{\mathbf{A}}x^{k+1}\right\|^{2}\) in the right hand side, \(-\frac{1}{2}\left(\frac{\gamma}{k^{p}}-\frac{\gamma}{(k+1)^{p}}\right)\), is nonpositive. As a result, we get below inequality.

\[U^{k+1} \leq U^{k}+\frac{1}{2}\left(\frac{\gamma}{k^{p}}-\frac{\gamma}{( k+1)^{p}}\right)\left\|x^{k+1}-x^{0}\right\|^{2}+\left(c_{k+1}-c_{k}\right) \left\|x^{k}-x^{0}\right\|^{2}\] \[\leq U^{1}+2\left\|x^{0}-x^{\star}\right\|^{2}\sum_{m=1}^{ \infty}\left(\left(\frac{\gamma}{k^{p}}-\frac{\gamma}{(k+1)^{p}}\right)+2 \left(c_{k+1}-c_{k}\right)\right)=M.\]

As done in (8), by monotonicity of \(\mathbf{A}\) and Young's inequality

\[M \geq U^{k}=a_{k}\left\|\tilde{\mathbf{A}}x^{k}\right\|^{2}+b_{k} \left\langle\tilde{\mathbf{A}}x^{k},x^{k}-x^{0}\right\rangle+c_{k}\left\|x^{k}- x^{0}\right\|^{2}\] \[\geq a_{k}\left\|\tilde{\mathbf{A}}x^{k}\right\|^{2}+b_{k}\left\langle \tilde{\mathbf{A}}x^{k},x^{\star}-x^{0}\right\rangle\] \[\geq a_{k}\left\|\tilde{\mathbf{A}}x^{k}\right\|^{2}-\frac{1}{2} \left(\left\|\tilde{\mathbf{A}}x^{k}\right\|^{2}+b_{k}^{2}\left\|x^{\star}-x^{0} \right\|^{2}\right)=\frac{1}{2}\left(1+\frac{\gamma}{k^{p}}\right)\left\| \tilde{\mathbf{A}}x^{k}\right\|^{2}+\frac{\gamma^{2}}{2k^{2p}}\left\|x^{\star}- x^{0}\right\|^{2}.\]Reorganizing, we get the desired result

\[\left\|\tilde{\mathbf{A}}x^{k}\right\|^{2} \leq 2\left(1-\frac{\gamma}{k^{p}}\right)^{-1}\left(M+\frac{\gamma^{2 }}{2k^{2p}}\left\|x^{\star}-x^{0}\right\|^{2}\right)\] \[=2\left(1+\frac{\gamma}{k^{p}-\gamma}\right)\left(M+\frac{\gamma^ {2}}{2k^{2p}}\left\|x^{\star}-x^{0}\right\|^{2}\right)=\mathcal{O}\left(1 \right).\]
2. \(\left\|\tilde{\mathbf{A}}(x^{k})\right\|^{2}=\mathcal{O}\left(\frac{1}{k^{2p}}\right)\) for \(0<p<1\), \(\gamma>0\). Plugging \[a_{k+1} =k^{p}\left(k^{p}+\frac{\gamma}{2}\right), b_{k+1} =\gamma k^{p}, c_{k+1} =\frac{k^{p}\gamma^{2}}{4\left(k^{p}+\frac{\gamma}{2}\right)}\] \[\lambda_{k} =a_{k+1}, \tau_{k} =c_{k+1}\] to (41) and (42) with \(p=1\), we obtain \[U^{k+1}-U^{k}+\left(k^{p}\left(k^{p}+\frac{\gamma}{2}\right)- \frac{k^{p}\gamma^{2}}{4\left(k^{p}+\frac{\gamma}{2}\right)}\right)\left\langle \tilde{\mathbf{A}}x^{k+1}-\tilde{\mathbf{A}}x^{k},x^{k+1}-x^{k}\right\rangle\] \[=-\gamma\left(k^{p}+\frac{\gamma}{2}\right)\left\|\tilde{ \mathbf{A}}x^{k+1}\right\|^{2}-\gamma^{2}\langle\tilde{\mathbf{A}}x^{k+1},x^{ k+1}-x^{0}\rangle-\frac{\gamma^{3}}{4\left(k^{p}+\frac{\gamma}{2}\right)}\left\|x^{k+1}-x^{0 }\right\|^{2}\] \[\quad+\underbrace{\left(\frac{k^{2p}\left(k^{p}+\frac{\gamma}{2} \right)}{k^{p}+\gamma}-(k-1)^{p}\left((k-1)^{p}+\frac{\gamma}{2}\right)\right) }_{=q_{k}}\underbrace{\left\|\tilde{\mathbf{A}}x^{k}\right\|^{2}}_{=\tau_{k}}+ \underbrace{\frac{\gamma\left(-(k-1)^{p}k^{p}+k^{2p}-\gamma(k-1)^{p}\right)} {k^{p}+\gamma}}_{=\tau_{k}}\langle\tilde{\mathbf{A}}x^{k},x^{k}-x^{0}\rangle\] \[\quad-\frac{\gamma^{3}k^{p}}{4\left(k^{p}+\gamma\right)\left(k^{ p}+\frac{\gamma}{2}\right)}\left\|x^{k}-x^{0}\right\|^{2}+\frac{\gamma^{3} \left(k^{p}-(k-1)^{p}\right)}{8\left((k-1)^{p}+\frac{\gamma}{2}\right)\left(k ^{p}+\frac{\gamma}{2}\right)}\left\|x^{k}-x^{0}\right\|^{2}\] \[=-\gamma\left(k^{p}+\frac{\gamma}{2}\right)\left\|\tilde{\mathbf{ A}}x^{k+1}+\frac{\gamma}{2k^{p}+\gamma}\left(x^{k+1}-x^{0}\right)\right\|^{2}+q_{k} \left\|\tilde{\mathbf{A}}x^{k}+\frac{\tau_{k}}{2q_{k}}\left(x^{k}-x^{0}\right) \right\|^{2}\] \[\quad-\underbrace{\left(\frac{r_{k}^{2}}{4q_{k}}+\frac{\gamma^{3 }k^{p}}{4\left(k^{p}+\gamma\right)\left(k^{p}+\frac{\gamma}{2}\right)}+\frac{ \gamma^{3}\left(k^{p}-(k-1)^{p}\right)}{8\left((k-1)^{p}+\frac{\gamma}{2} \right)\left(k^{p}+\frac{\gamma}{2}\right)}\right)}_{s_{k}}\left\|x^{k}-x^{0} \right\|^{2}.\]

The continuous counterpart of this equality is

\[\dot{U}_{2}(t)+2t^{2p}\left\langle\frac{d}{dt}\tilde{\mathbf{A}}( X(t)),\dot{X}(t)\right\rangle\] \[=-2t^{p}\left(\gamma-pt^{p-1}\right)\left\|\tilde{\mathbf{A}}(X(t) )+\frac{\gamma}{t^{p}}\left(X(t)-X_{0}\right)\right\|^{2}-\gamma p(p-1)t^{p-2} \left\|X(t)-X_{0}\right\|^{2}\]

which can be obtained by differentiating the conservation law for \(E_{2}\). The first term on the right hand side correspond to the sum of first two terms in the right hand side of discrete equality. Thus we may expect the order of the coefficients for the terms would match, and we will check the expectation is indeed true.

With some calculation, we can observe

\[s_{k}=\frac{2\gamma^{2}(k-1)^{p}k^{2p}\left((k-1)^{p}-k^{p}\right)^{2}}{4 \left((k-1)^{p}+\frac{\gamma}{2}\right)\left(k^{p}+\frac{\gamma}{2}\right)d _{k}},\]

where

\[d_{k}=2k^{p}(k-1)^{p}\left((k-1)^{p}+\frac{\gamma}{2}\right)-2k^{3p}-\gamma k ^{2p}+2\gamma(k-1)^{p}\left((k-1)^{p}+\frac{\gamma}{2}\right).\]

By considering Newton expansion and from \(0<p<1\), we see

\[d_{k} =2k^{3p}+\gamma k^{2p}+\mathcal{O}\left(k^{3p-1}\right)-2k^{3p}- \gamma k^{2p}+2\gamma k^{2p}+\mathcal{O}\left(k^{p}\right)\] \[=2\gamma k^{2p}+\mathcal{O}\left(k^{3p-1}\right)+\mathcal{O} \left(k^{p}\right)=\mathcal{O}\left(k^{2p}\right).\]Thus we can check the leading order of numerator is \(p+2p+(2p-2)=5p-2\), and leading order of the denominator is \(p+p+2p=4p\). As \(5p-2-4p=p-2\), we have \(s_{k}\in\mathcal{O}\left(k^{p-2}\right)\), which matches with the continuous counterpart. Therefore \(\sum_{k=1}^{\infty}s_{k}<\infty\).

On the other hand, we see

\[q_{k} =\frac{k^{2p}\left(k^{p}+\frac{\gamma}{2}\right)}{k^{p}+\gamma}-( k-1)^{p}\left((k-1)^{p}+\frac{\gamma}{2}\right)\] \[\leq k^{2p}-\left(k^{p}-pk^{p-1}+\mathcal{O}\left(k^{p-2}\right) \right)\left(\left(k^{p}-pk^{p-1}+\mathcal{O}\left(k^{p-2}\right)\right)+ \frac{\gamma}{2}\right)\] \[=-\frac{\gamma}{2}k^{p}+2pk^{2p-1}+\mathcal{O}\left(k^{2p-2} \right).\]

Since \(p>2p-1\), we have \(\lim_{k\rightarrow\infty}q_{k}=-\infty\). Note this matches with the continuous counterpart as well.

Therefore there is \(N>0\) such that for \(k>N\), \(q_{k}<0\). Now for \(k>N\) we have

\[U_{k+1}\leq U_{k}+s_{k}\left\|x^{k}-x^{0}\right\|^{2}\leq U_{N}+4\left(\sum_{ m=N}^{\infty}s_{m}\right)\left\|x^{0}-x^{\star}\right\|^{2}=M.\]

Thus for \(k>N\), by monotonicity of \(\mathbf{A}\) and Young's inequality

\[M \geq U^{k+1}=k^{p}\left(k^{p}+\frac{\gamma}{2}\right)\left\| \tilde{\mathbf{A}}x^{k+1}\right\|^{2}+\gamma k^{p}\left\langle\tilde{\mathbf{ A}}x^{k+1},x^{k+1}-x^{0}\right\rangle+\frac{k^{p}\gamma^{2}}{4\left(k^{p}+ \frac{\gamma}{2}\right)}\left\|x^{k+1}-x^{0}\right\|^{2}\] \[\geq k^{p}\left(k^{p}+\frac{\gamma}{2}\right)\left\|\tilde{ \mathbf{A}}x^{k+1}\right\|^{2}+\gamma k^{p}\left\langle\tilde{\mathbf{A}}x^{k +1},x^{\star}-x^{0}\right\rangle\] \[=\frac{k^{p}\left(k^{p}+\frac{\gamma}{2}\right)}{2}\left\| \tilde{\mathbf{A}}x^{k+1}\right\|^{2}-\frac{k^{p}\gamma^{2}}{2k^{p}+\gamma} \left\|x^{\star}-x^{0}\right\|^{2}.\]

Reorganizing, we get the desired result.

\[\left\|\tilde{\mathbf{A}}x^{k+1}\right\|^{2}\leq\frac{2}{k^{p}\left(k^{p}+ \frac{\gamma}{2}\right)}\left(M+\frac{\gamma^{2}}{2}\left\|x^{0}-x^{\star} \right\|^{2}\right)=\mathcal{O}\left(\frac{1}{k^{2p}}\right).\]
3. \(\left\|\tilde{\mathbf{A}}(x^{k})\right\|^{2}=\mathcal{O}\left(\frac{1}{k^{2}}\right)\) for \(p=1\), \(\gamma\geq 1\). Plugging

\[a_{k+1} =k^{2}, b_{k+1} =\gamma\left(k-\frac{1}{2}\left(\gamma-1\right)\right), c_{k+1}=\frac{1}{4}\gamma(\gamma-1)\] \[\lambda_{k} =k(k+1), \tau_{k} =c_{k+1}\]to (41) and (42), we obtain

\[U^{k+1}-U^{k}+\left(k(k+1)-\frac{1}{4}\gamma(\gamma-1)\right)\left\langle \tilde{\mathbf{A}}x^{k+1}-\tilde{\mathbf{A}}x^{k},x^{k+1}-x^{k}\right\rangle\] \[= -\left(\gamma-1\right)(k+1)\left\|\tilde{\mathbf{A}}x^{k+1}\right\| ^{2}-\frac{k^{2}(\gamma-1)}{k+\gamma}\|\tilde{\mathbf{A}}x^{k}\|^{2}\] \[-\frac{\gamma(\gamma-1)\left(k+\frac{\gamma}{4}\right)}{k}\langle \tilde{\mathbf{A}}x^{k+1},x^{k+1}-x^{0}\rangle-\frac{\gamma(\gamma-1)\left(k- \frac{\gamma}{4}\right)}{k+\gamma}\langle\tilde{\mathbf{A}}x^{k},x^{k}-x^{0}\rangle\] \[-\frac{\gamma^{2}(\gamma-1)}{4k}\left\|x^{k+1}-x^{0}\right\|^{2} -\frac{\gamma^{2}(\gamma-1)}{4\left(k+\gamma\right)}\left\|x^{k}-x^{0}\right\| ^{2}\] \[= -\left(\gamma-1\right)(k+1)\left\|\tilde{\mathbf{A}}x^{k+1}+ \frac{\gamma}{2(k+1)}\left(1+\frac{\gamma}{4k}\right)\left(x^{k+1}-x^{0} \right)\right\|^{2}\] \[-\frac{k^{2}(\gamma-1)}{k+\gamma}\left\|\tilde{\mathbf{A}}x^{k} +\frac{\gamma}{2k}\left(1-\frac{\gamma}{4k}\right)\left(x^{k}-x^{0}\right) \right\|^{2}\] \[-\underbrace{\frac{\gamma^{2}(\gamma-1)}{64k(k+1)}\left(8(\gamma -2)-\frac{\gamma^{2}}{k}\right)}_{=s_{k,1}=\mathcal{O}\left(\frac{1}{k^{2}} \right)}\left\|x^{k+1}-x^{0}\right\|^{2}-\underbrace{\frac{\gamma^{3}(\gamma- 1)}{64k(k+\gamma)}\left(8-\frac{\gamma}{k}\right)}_{=s_{k,0}=\mathcal{O}\left( \frac{1}{k^{2}}\right)}\left\|x^{k}-x^{0}\right\|^{2}.\]

The continuous counterpart of above equality is

\[\dot{U_{3}}(t)+2t^{2}\left\langle\frac{d}{dt}\tilde{\mathbf{A}}(X(t)),\dot{X} (t)\right\rangle=-2t\left(\gamma-1\right)\left\|\tilde{\mathbf{A}}(X(t))+ \frac{\gamma}{t}\left(X(t)-X_{0}\right)\right\|^{2}\]

which can be obtained by differentiating the conservation law for \(E_{3}\). Note the terms match with same order of coefficients, except two \(\left\|x^{k+1}-x^{0}\right\|^{2}\) and \(\left\|x^{k}-x^{0}\right\|^{2}\), while these terms are summable as \(s_{k,1},s_{k,0}=\mathcal{O}\left(\frac{1}{k^{2}}\right)\). Therefore we have

\[U^{k+1} \leq U^{k}-s_{k,1}\left\|x^{k+1}-x^{0}\right\|^{2}-s_{k,0}\left\| x^{k}-x^{0}\right\|^{2}\] \[\leq U^{1}+4\left\|x^{0}-x^{\star}\right\|^{2}\sum_{m=1}^{\infty} \left(s_{m,1}+s_{m,0}\right)=M.\]

Thus by monotonicity of \(\mathbf{A}\) and Young's inequality

\[M \geq U^{k}=k^{2}\left\|\tilde{\mathbf{A}}x^{k+1}\right\|^{2}+ \gamma\left(k-\frac{1}{2}\left(\gamma-1\right)\right)\left\langle\tilde{ \mathbf{A}}x^{k},x^{k}-x^{0}\right\rangle+\frac{1}{4}\gamma(\gamma-1)\left\|x ^{k}-x^{0}\right\|^{2}\] \[=k^{2}\left\|\tilde{\mathbf{A}}x^{k+1}\right\|^{2}+\gamma\left(k- \frac{1}{2}\left(\gamma-1\right)\right)\left\langle\tilde{\mathbf{A}}x^{k},x ^{\star}-x^{0}\right\rangle\] \[\geq k^{2}\left\|\tilde{\mathbf{A}}x^{k}\right\|^{2}-\frac{1}{2} \left(\left(k-\frac{1}{2}\left(\gamma-1\right)\right)^{2}\left\|\tilde{ \mathbf{A}}x^{k}\right\|^{2}+\gamma^{2}\left\|x^{\star}-x^{0}\right\|^{2}\right)\] \[\geq\frac{k^{2}}{2}\left\|\tilde{\mathbf{A}}x^{k}\right\|^{2}- \frac{\gamma^{2}}{2}\left\|x^{\star}-x^{0}\right\|^{2}.\]

Reorganizing, we get the desired result.
4. \(\left\|\tilde{\mathbf{A}}x^{k}\right\|^{2}=\mathcal{O}\left(\frac{1}{k^{2}}\right)\) for \(p=1\), \(0<\gamma<1\). Plugging \[a_{k} =k^{2\gamma}, b_{k} =\gamma k\left(k-\frac{1}{4}\right)^{2\gamma-2}, c_{k+1}=\frac{1}{4}\gamma(\gamma-1)k^{2\gamma-2}\] \[\lambda_{k} =k^{2\gamma-1}(k+\gamma), \tau_{k} =c_{k+1}\]to (41) and (42) with \(p=1\), we obtain

\[U^{k+1}-U^{k}+\left(k^{2\gamma-1}(k+\gamma)+\frac{1}{4}\gamma(1- \gamma)k^{2\gamma-2}\right)\left\langle\tilde{\mathbf{A}}x^{k+1}-\tilde{\mathbf{ A}}x^{k},x^{k+1}-x^{k}\right\rangle\] \[=\underbrace{\left((k+1)^{2\gamma}-k^{2\gamma-2}(k+\gamma)^{2} \right)}_{=q_{k}}\|\tilde{\mathbf{A}}x^{k+1}\|^{2}\] \[\quad+\underbrace{\left(\gamma(k+1)\left(k+\frac{3}{4}\right)^{2 \gamma-2}-\gamma k^{2\gamma-2}(k+\gamma)-\left(1+\frac{k+\gamma}{k}\right) \frac{1}{4}\gamma(\gamma-1)k^{2\gamma-2}\right)}_{=s_{k,1}}\left\langle\tilde{ \mathbf{A}}x^{k+1},x^{k+1}-x^{0}\right\rangle\] \[\quad+\underbrace{\left(\gamma k^{2\gamma-1}-\gamma k\left(k- \frac{1}{4}\right)^{2\gamma-2}-\left(1+\frac{k}{k+\gamma}\right)\frac{1}{4} \gamma(\gamma-1)k^{2\gamma-2}\right)}_{=s_{k,0}}\left\langle\tilde{\mathbf{A} }x^{k},x^{k}-x^{0}\right\rangle\] \[\quad-\frac{1}{4}\gamma^{2}(\gamma-1)k^{2\gamma-3}\left\|x^{k+1} -x^{0}\right\|^{2}-\frac{1}{4}\gamma^{2}(\gamma-1)k^{2\gamma-3}\frac{1}{1+ \frac{\gamma}{k}}\left\|x^{k}-x^{0}\right\|^{2}\] \[\quad+\frac{1}{4}\gamma(\gamma-1)\left(k^{2\gamma-2}-(k-1)^{2 \gamma-2}\right)\left\|x^{k}-x^{0}\right\|^{2}\]

The continuous counterpart of this equality is

\[\dot{U_{4}}(t)+2t^{2\gamma}\left\langle\frac{d}{dt}\tilde{\mathbf{A}}(X(t)), \dot{X}(t)\right\rangle ds=-2\gamma(\gamma-1)t^{2\gamma-3}\left\|X(t)-X_{0} \right\|^{2}\]

which can be obtained by differentiating the conservation law for \(E_{4}\). Thus we may expect the order of the matching terms are equal, and the terms do not occur in the continuous version do not bother our desired conclusion. We check our expectation is true.

Terms \(\left\|x^{k+1}-x^{0}\right\|^{2}\) and \(\left\|x^{k}-x^{0}\right\|^{2}\) correspond to \(\left\|X-X_{0}\right\|^{2}\). The coefficients for \(\left\|x^{k+1}-x^{0}\right\|^{2}\) and \(\left\|x^{k}-x^{0}\right\|^{2}\) are clearly \(\mathcal{O}\left(k^{2\gamma-3}\right)\), which equals the order of continuous counterpart. Since \(\gamma<1\), we know these terms are summable.

Next we observe \(q_{k}\leq 0\). Observe

\[q_{k}\leq 0 \iff\frac{(k+1)^{2\gamma}}{k^{2\gamma}}\leq\frac{(k+\gamma)^{2}} {k^{2}}\] \[\iff\left(1+\frac{1}{k}\right)^{2\gamma}\leq\left(1+\frac{\gamma} {k}\right)^{2}\iff\left(1+\frac{1}{k}\right)^{\gamma}\leq 1+\frac{\gamma}{k}.\]

To check the last inequality is true, consider \(f(x)=x^{\gamma}\). Since this function is concave for \(0<\gamma<1\), we see

\[\left(1+\frac{1}{k}\right)^{\gamma}=f\left(1+\frac{1}{k}\right)\leq f(1)+ \frac{1}{k}f^{\prime}(1)=1+\frac{\gamma}{k}.\]

Finally we focus on \(s_{k,0}\) and \(s_{k,1}\). As cross terms don't appear in continuous version, we may expect these terms are'small', or in mathematical words, summable. From Cauchy-Schwarz inequality we know

\[\left\langle\tilde{\mathbf{A}}x^{k+1},x^{k+1}-x^{0}\right\rangle \leq\left\|\tilde{\mathbf{A}}x^{k+1}\right\|\left\|x^{k+1}-x^{0}\right\|\] \[\left\langle\tilde{\mathbf{A}}x^{k},x^{k}-x^{0}\right\rangle \leq\left\|\tilde{\mathbf{A}}x^{k}\right\|\left\|x^{k}-x^{0}\right\|.\]

Since \(\left\|\tilde{\mathbf{A}}x^{k+1}\right\|\) and \(\left\|\tilde{\mathbf{A}}x^{k}\right\|\) are bounded from the proof for case (i), we know two innerproduct terms are bounded. Thus if we show \(\sum_{k=1}^{\infty}\left|s_{k,0}\right|,\sum_{k=1}^{\infty}\left|s_{k,1}\right|<\infty\), we can conclude \(U^{k}\) is bounded.

Considering Newton expansion, we see

\[\gamma(k+1)\left(k+\frac{3}{4}\right)^{2\gamma-2}-\gamma k^{2 \gamma-2}(k+\gamma)\] \[=\gamma k^{2\gamma-1}+\gamma\left(1+\frac{3}{2}\left(\gamma-1 \right)\right)k^{2\gamma-2}+\mathcal{O}\left(k^{2\gamma-3}\right)-\gamma k^{2 \gamma-1}-\gamma^{2}k^{2\gamma-2}\] \[=\frac{1}{2}\gamma(\gamma-1)k^{2\gamma-2}+\mathcal{O}\left(k^{2 \gamma-3}\right)\]Therefore

\[s_{k,1}=\frac{1}{2}\gamma(\gamma-1)k^{2\gamma-2}+\mathcal{O}\left(k^{2\gamma-3} \right)-\left(2+\frac{\gamma}{k}\right)\frac{1}{4}\gamma(\gamma-1)k^{2\gamma-2} =\mathcal{O}\left(k^{2\gamma-3}\right).\]

With similar argument

\[s_{k,\,0} =\gamma k^{2\gamma-1}-\gamma k\left(k-\frac{1}{4}\right)^{2\gamma- 2}-\left(1+\frac{k}{k+\gamma}\right)\frac{1}{4}\gamma(\gamma-1)k^{2\gamma-2}\] \[=\gamma k^{2\gamma-1}-\gamma k\left(k^{2\gamma-2}-\left(2\gamma-2 \right)\frac{1}{4}k^{2\gamma-3}+\mathcal{O}\left(k^{2\gamma-4}\right)\right)- \left(2-\frac{\gamma}{k+\gamma}\right)\frac{1}{4}\gamma(\gamma-1)k^{2\gamma-2}\] \[=\frac{1}{2}\gamma(\gamma-1)k^{2\gamma-2}+\mathcal{O}\left(k^{2 \gamma-3}\right)-\left(\frac{1}{2}\gamma(\gamma-1)k^{2\gamma-2}+\mathcal{O} \left(k^{2\gamma-3}\right)\right)=\mathcal{O}\left(k^{2\gamma-3}\right)\]

Therefore, we have

\[U^{k+1} \leq U^{k}+s_{k,1}\left\|\tilde{\mathbf{A}}x^{k+1}\right\|\left\| x^{k+1}-x^{0}\right\|+s_{k,0}\left\|\tilde{\mathbf{A}}x^{k}\right\|\left\|x^{k}-x^{0}\right\|\] \[\leq\sum_{m=1}^{\infty}\left(\left|s_{m,1}\right|\left\|\tilde{ \mathbf{A}}x^{k+1}\right\|\left\|x^{k+1}-x^{0}\right\|+\left|s_{m,0}\right| \left\|\tilde{\mathbf{A}}x^{k+1}\right\|\left\|x^{k+1}-x^{0}\right\|\right)\] \[\quad+\sum_{m=1}^{\infty}\left(\left|c_{m+1}\frac{\gamma}{m} \right|\left\|x^{k+1}-x^{0}\right\|^{2}+\left(\left|c_{m+1}\frac{\gamma}{m+ \gamma}\right|+\left|c_{k+1}-c_{k}\right|\right)\left\|x^{k}-x^{0}\right\|^{2 }\right)=M_{1}.\]

By Young's inequality

\[M_{1}+\frac{1}{4}\gamma(1-\gamma)k^{2\gamma-2}\left\|x^{k}-x^{0 }\right\|^{2} \geq k^{2\gamma}\|\tilde{\mathbf{A}}x^{k}\|^{2}+\gamma k\left(k- \frac{1}{4}\right)^{2\gamma-2}\left\langle\tilde{\mathbf{A}}x^{k},x^{k}-x^{0}\right\rangle\] \[\geq k^{2\gamma}\|\tilde{\mathbf{A}}x^{k}\|^{2}+\gamma k\left(k- \frac{1}{4}\right)^{2\gamma-2}\left\langle\tilde{\mathbf{A}}x^{k},x^{\star}-x^ {0}\right\rangle\] \[=k^{2\gamma}\|\tilde{\mathbf{A}}x^{k}\|^{2}+\left\langle k^{ \gamma}\tilde{\mathbf{A}}x^{k},\gamma k^{1-\gamma}\left(k-\frac{1}{4}\right)^ {2\gamma-2}\left(x^{\star}-x^{0}\right)\right\rangle\] \[\geq k^{2\gamma}\|\tilde{\mathbf{A}}x^{k}\|^{2}-\frac{1}{2}k^{2 \gamma}\|\tilde{\mathbf{A}}x^{k}\|^{2}-\frac{\gamma^{2}}{2}\left(k^{1-\gamma} \left(k-\frac{1}{4}\right)^{2\gamma-2}\right)^{2}\left\|x^{\star}-x^{0}\right\| ^{2}.\]

Since \(k^{1-\gamma}\left(k-\frac{1}{4}\right)^{2\gamma-2}=\mathcal{O}\left(k^{\gamma- 1}\right)\) and \(\gamma<1\), this terms goes to zero as \(k\to\infty\) thus there is some \(M_{2}>0\) such that \(\frac{1}{2}\left(\gamma k^{1-\gamma}\left(k-\frac{1}{4}\right)^{2\gamma-2} \right)^{2}\left\|x^{k}-x^{\star}\right\|^{2}\leq M_{2}\) for all \(k\geq 1\). Reorganizng terms, we obtain the desired result

\[\|\tilde{\mathbf{A}}x^{k}\|^{2}\leq\frac{2}{k^{2\gamma}}\left(M_{1}+M_{2}+ \gamma(1-\gamma)k^{2\gamma-2}\left\|x^{\star}-x^{0}\right\|^{2}\right)= \mathcal{O}\left(\frac{1}{k^{2\gamma}}\right).\]

## Appendix H Proof of convergence analysis for strongly monotone \(\mathbf{A}\)

### Proof of Proposition 6.1

We take dilated coordinate \(W(t)=C(t)(X(t)-X_{0})\) as did in the proof of Proposition 3.2. Recall from (30), the second order version of the ODE was written as \(0=\dot{W}-\beta(t)\dot{W}+C(t)\frac{d}{dt}\tilde{\mathbf{A}}(X(t))\). Now for we multiply \(R(t)^{2}\) in the ODE and obtain

\[0=R(t)^{2}\left(\ddot{W}-\beta(t)\dot{W}+C(t)\frac{d}{dt}\tilde{\mathbf{A}}(X )\right).\]Now taking inner product with \(\dot{W}\) and integrating we have

\[E_{1} \equiv\frac{R(t)^{2}}{2}\left\|\dot{W}(t)\right\|^{2}-\underbrace{ \int_{t_{0}}^{t}R(s)^{2}\left(\frac{\dot{R}(s)}{R(s)}\left\|\dot{W}(s)\right\| ^{2}\right)ds}_{*}-\int_{t_{0}}^{t}\beta(s)R(s)^{2}\left\|\dot{W}(s)\right\|^ {2}ds\] \[\quad+\int_{t_{0}}^{t}C(s)R(s)^{2}\left\langle\frac{d}{ds}\tilde{ \mathbf{A}}(X(s)),\dot{W}(s)\right\rangle ds.\]

Note the second term which is obtained from integration by parts, would not appear if \(R(t)=1\) as in Proposition 3.2. This is key term to exploit the condition \(\mathbf{A}\) is strongly monotone.

Now again with \(\dot{W}(t)=C(t)\left(\dot{X}(t)+\beta(s)(X(t)-X_{0})\right)\), we rewrite the last term as

\[\int_{t_{0}}^{t}C(s)R(s)^{2}\left\langle\frac{d}{ds}\tilde{ \mathbf{A}}(X(s)),\dot{W}(s)\right\rangle ds\] \[=\int_{t_{0}}^{t}C(s)^{2}R(s)^{2}\left\langle\frac{d}{ds}\tilde{ \mathbf{A}}(X(s)),\dot{X}(s)\right\rangle ds+\int_{t_{0}}^{t}\left\langle\frac {d}{ds}\tilde{\mathbf{A}}(X(s)),C(s)R(s)^{2}\beta(s)W(s)\right\rangle ds.\]

Taking integration by parts to the second term we have

\[\int_{t_{0}}^{t}\left\langle\frac{d}{ds}\tilde{\mathbf{A}}(X(s)),C(s)R(s)^{2}\beta(s)W(s)\right\rangle ds-\left[\left\langle\tilde{\mathbf{A }}(X(W(s),s)),C(s)R(s)^{2}\beta(s)W(s)\right\rangle\right]_{t_{0}}^{t}\] \[=-\int_{t_{0}}^{t}\left\langle\tilde{\mathbf{A}}(X(W,t)),\left( \beta(s)^{2}+2\frac{\dot{R}(s)}{R(s)}\beta(s)+\dot{\beta}(s)\right)C(s)R(s)^{ 2}W(s)+C(s)R(s)^{2}\beta(s)\dot{W}(s)\right\rangle ds\] \[=\int_{t_{0}}^{t}\beta(s)R(s)^{2}\left\|\dot{W}(s)\right\|^{2}ds +\int_{t_{0}}^{t}\left(\beta(s)^{2}+2\underbrace{\frac{\dot{R}(s)}{R(s)}\beta (s)}_{*}+\dot{\beta}(s)\right)R(s)^{2}\left\langle\dot{W}(s),W(s)\right\rangle ds.\]

The fact \(C(t)\tilde{\mathbf{A}}(X(W,t))=-\dot{W}(t)\) is applied to the second equality. Note the fundamental theorem of calculus is valid since \(C(s)R(s)^{2}\beta(s)W(s)\) is differentiable, \(\tilde{\mathbf{A}}(X(W(s),t))\) is Lipschitz continuous in \([t_{0},t]\) by Lemma B.4, and so their inner product is absolutely continuous in \([t_{0},t]\).

Now consider the second integrand except the term marked with *. From integration by parts we have

\[\int_{t_{0}}^{t}\left(\beta(s)^{2}+\dot{\beta}(s)\right)R(s)^{2} \left\langle\dot{W}(s),W(s)\right\rangle ds-\left[\left(\beta(s)^{2}+\dot{ \beta}(s)\right)R(s)^{2}\frac{1}{2}\left\|W(s)\right\|^{2}\right]_{t_{0}}^{t}\] \[=-\frac{1}{2}\int_{t_{0}}^{t}\left(\left(2\beta(s)\dot{\beta}(s) +\ddot{\beta}(s)\right)R(s)^{2}+\left(\underbrace{\beta(s)^{2}}_{*}+\dot{ \beta}(s)\right)2R(s)\dot{R}(s)\right)\left\|W(s)\right\|^{2}ds.\]

The integrand except * marked term can be rewritten as

\[\frac{1}{2}\int_{t_{0}}^{t}\left(2\beta(s)\dot{\beta}(s)R(s)+ \ddot{\beta}(s)R(s)+2\dot{\beta}(s)\dot{R}(s)\right)R(s)\left\|W(s)\right\|^{ 2}ds\] \[=\frac{1}{2}\int_{t_{0}}^{t}C(s)^{2}\left(2\beta(s)\dot{\beta}(s) R(s)+\ddot{\beta}(s)R(s)+2\dot{\beta}(s)\dot{R}(s)\right)R(s)\left\|X(s)-X_{0} \right\|^{2}ds\] \[=\frac{1}{2}\int_{t_{0}}^{t}\frac{d}{ds}\left(C(s)^{2}R(s)^{2} \dot{\beta}(s)\right)\left\|X(s)-X_{0}\right\|^{2}ds.\]

Now collecting the terms marked with *, we have

\[-\int_{t_{0}}^{t}R(s)^{2}\left(\frac{\dot{R}(s)}{R(s)}\left\|\dot{W}(s)\right\| ^{2}\right)ds+\int_{t_{0}}^{t}2\frac{\dot{R}(s)}{R(s)}\beta(s)R(s)^{2}\left \langle\dot{W}(s),W(s)\right\rangle ds-\int_{t_{0}}^{t}\beta(s)^{2}R(s)\dot{R} (s)\left\|W(s)\right\|^{2}ds\] \[=-\int_{t_{0}}^{t}R(s)^{2}\frac{\dot{R}(s)}{R(s)}\left\|\dot{W}(s )-\beta(s)W(s)\right\|^{2}ds=-\int_{t_{0}}^{t}R(s)^{2}C(s)^{2}\frac{\dot{R}(s) }{R(s)}\left\|\dot{X}(s)\right\|^{2}ds.\]Collecting all results we have

\[E_{1} \equiv\frac{R(t)^{2}}{2}\left\|\dot{W}(t)\right\|^{2}+\left[\left< \tilde{\mathbf{A}}(X(W,s)),C(s)R(s)^{2}\beta(s)W(s)\right>\right]_{t_{0}}^{t}+ \left[\left(\beta(s)^{2}+\dot{\beta}(s)\right)R(s)^{2}\frac{1}{2}\left\|W(s) \right\|^{2}\right]_{t_{0}}^{t}\] \[\quad+\int_{t_{0}}^{t}C(s)^{2}R(s)^{2}\left<\frac{d}{ds}\tilde{ \mathbf{A}}(X(s)),\dot{X}(s)\right>ds-\int_{t_{0}}^{t}R(s)^{2}C(s)^{2}\frac{ \dot{R}(s)}{R(s)}\left\|\dot{X}(s)\right\|^{2}ds\] \[\quad+\frac{1}{2}\int_{t_{0}}^{t}\frac{d}{ds}\left(C(s)^{2}R(s)^{ 2}\dot{\beta}(s)\right)\left\|X(s)-X_{0}\right\|^{2}ds\] \[=\frac{C(t)^{2}R(t)^{2}}{2}\left(\left\|\mathbf{A}(X(t))\right\|^ {2}+2\beta(t)\left<\mathbf{A}(X(t)),X(t)-X_{0}\right>+\left(\beta(t)^{2}+ \dot{\beta}(t)\right)\left\|X(t)-X_{0}\right\|^{2}\right)\] \[\quad+\int_{t_{0}}^{t}C(s)^{2}R(s)^{2}\left(\left<\frac{d}{ds} \tilde{\mathbf{A}}(X(s)),\dot{X}(s)\right>-\frac{\dot{R}(s)}{R(s)}\left\|\dot {X}(s)\right\|^{2}\right)\!ds-\int_{t_{0}}^{t}\frac{d}{ds}\left(\frac{C(s)^{2} R(s)^{2}\dot{\beta}(s)}{2}\right)\left\|X(s)-X_{0}\right\|^{2}ds\] \[\quad-\underbrace{\frac{C(t_{0})^{2}R(t_{0})^{2}}{2}\left(2 \beta(t_{0})\left<\mathbf{A}(X(t_{0})),X(t_{0})-X_{0}\right>+\left(\beta(t_{0} )^{2}+\dot{\beta}(t_{0})\right)\left\|X(t_{0})-X_{0}\right\|^{2}\right)}_{ \text{constant}}.\]

Renaming \(E=E_{1}-\) constant, we get the desired result.

### Proof of Theorem 6.2

#### h.2.1 Proof of the inequality \(V(t)\leq V(0)\)

The basic structure of the proof is same as Appendix E.3. We do not repeat the whole proof here, instead we check the steps done in Appendix E.3 are also valid for the setup in Theorem 6.2.

* \(V\) is nonincreasing for Lipschitz continuous \(\mu\)-strongly monotone \(\tilde{\mathbf{A}}\). Recall \(V\) in Theorem 6.2 was defined as \[V(t)=\frac{(e^{\mu t}-e^{-\mu t})^{2}}{2}\left\|\tilde{\mathbf{A}}(X(t)) \right\|^{2}+2\mu(1-e^{-2\mu t})\left<\tilde{\mathbf{A}}(X(t)),X(t)-X_{0} \right>-2\mu^{2}\left(1-e^{-2\mu t}\right)\left\|X(t)-X_{0}\right\|^{2}.\] (47) We first check following equality is true. \[V(t)=\frac{C(t)^{2}R(t)^{2}}{2}\left(\left\|\tilde{\mathbf{A}}(X(t))\right\|^ {2}+2\beta(t)\left<\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right>+\left(\beta(t) ^{2}+\dot{\beta}(t)\right)\left\|X(t)-X_{0}\right\|^{2}\right).\] (48) Recall we're considering (10), \(\beta(t)=\frac{2\mu}{e^{2\mu t}-1}.\) As \(\frac{d}{dt}\log\left(1-e^{-2\mu t}\right)=\frac{2\mu e^{-2\mu t}}{1-e^{-2\mu t }}=\frac{2\mu}{e^{2\mu t}-1}=\beta(t)\), we have \[C(t)=e^{\int_{\infty}^{t}\frac{2\mu}{e^{2\mu s}-1}ds}=e^{\log\left(1-e^{-2\mu t }\right)}=1-e^{-2\mu t}.\] As \(\dot{\beta}(t)=-\frac{4\mu^{2}e^{2\mu t}}{(e^{2\mu t}-1)^{2}}\) and \(R(t)=e^{\mu t}\), \[\frac{C(t)^{2}R(t)^{2}}{2} =\frac{1}{2}\left(1-e^{-2\mu t}\right)^{2}e^{2\mu t}=\frac{\left( e^{\mu t}-e^{-\mu t}\right)^{2}}{2}\] \[C(t)^{2}R(t)^{2}\beta(t) =e^{-2\mu t}\left(e^{2\mu t}-1\right)^{2}\frac{2\mu}{e^{2\mu t}- 1}=2\mu\left(1-e^{-2\mu t}\right)\] \[\frac{C(t)^{2}R(t)^{2}}{2}\left(\beta(t)^{2}+\dot{\beta}(t)\right) =\frac{\left(e^{\mu t}-e^{-\mu t}\right)^{2}}{2}\left(\left(\frac {2\mu}{e^{2\mu t}-1}\right)^{2}-\frac{4\mu^{2}e^{2\mu t}}{\left(e^{2\mu t}-1 \right)^{2}}\right)=2\mu^{2}\left(e^{-2\mu t}-1\right).\] This proves the desired equality. Now we show \[V(t)=E-\int_{t_{0}}^{t}C(s)^{2}R(s)^{2}\left(\left<\frac{d}{ds}\tilde{\mathbf{A }}(X(s)),\dot{X}(s)\right>-\frac{\dot{R}(s)}{R(s)}\left\|\dot{X}(s)\right\|^{2 }\right)ds,\] (49)where \(E\) is from Proposition 6.1 which was defined as

\[E=\frac{C(t)^{2}R(t)^{2}}{2}\left(\left\|\tilde{\mathbf{A}}(X(t)) \right\|^{2}+2\beta(t)\left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle +\left(\beta(t)^{2}+\dot{\beta}(t)\right)\left\|X(t)-X_{0}\right\|^{2}\right)\] \[+\int_{t_{0}}^{t}C(s)^{2}R(s)^{2}\left(\left\langle\frac{d}{ds} \tilde{\mathbf{A}}(X(s)),\dot{X}(s)\right\rangle-\frac{\dot{R}(s)}{R(s)}\left\| \dot{X}(s)\right\|^{2}\right)ds-\int_{t_{0}}^{t}\frac{d}{ds}\left(\frac{C(s)^{2 }R(s)^{2}\dot{\beta}(s)}{2}\right)\left\|X(t)-X_{0}\right\|^{2}ds.\]

From (48) and the definition of \(E\), it is enough to show \(\frac{d}{ds}\left(\frac{C(s)^{2}R(s)^{2}\dot{\beta}(s)}{2}\right)=0\). Since

\[\frac{C^{2}(t)R^{2}(t)\dot{\beta}(t)}{2}=\left(1-e^{-2\mu t}\right)^{2}e^{2 \mu t}\left(-\frac{4\mu^{2}e^{2\mu t}}{\left(e^{2\mu t}-1\right)^{2}}\right)= -4\mu^{2},\]

we see \(\frac{d}{ds}\left(\frac{C(s)^{2}R(s)^{2}\dot{\beta}(s)}{2}\right)=0\).

Now since \(\tilde{\mathbf{A}}\) is Lipschitz continuous, we know \(E\) is constant from Proposition 6.1. Therefore from (49), for \(t>0\), \(|h|<t\) we have

\[V(t+h)-V(t)=\int_{t}^{t+h}C(s)^{2}R(s)^{2}\left(\left\langle\frac{d}{ds} \tilde{\mathbf{A}}(X(s)),\dot{X}(s)\right\rangle-\frac{\dot{R}(s)}{R(s)}\left\| \dot{X}(s)\right\|^{2}\right)ds.\]

As \(\frac{\dot{R}(s)}{R(s)}=\mu\) and \(\tilde{\mathbf{A}}\) is \(\mu\)-strongly monotone, from (2) we see

\[\left\langle\frac{d}{ds}\tilde{\mathbf{A}}(X(s)),\dot{X}(s)\right\rangle- \frac{\dot{R}(s)}{R(s)}\left\|\dot{X}(s)\right\|^{2}\geq 0.\]

Therefore \(V(t+h)-V(t)\geq 0\) for \(h>0\), we get the desired result.
2. Calculation of \(V(0)\) for Lipshitz continuous monotone \(\tilde{\mathbf{A}}\) Plugging \(t=0\) to (47) we immediately obtain \(V(0)=0\).
3. \(V(t)\leq 0\) holds for all \(t\in[0,\infty)\) and general maximal \(\mu\)-strongly monotone \(\mathbf{A}\) We check the arguments in Appendix E.3.3 and Appendix E.3.4 are also valid here. Define \(S\) as defined in Lemma E.1. Take \(t\in S\), let \(T>t\). As checked in Appendix C, the arguments used in the proof Proposition B.11 is also valid for the case \(\beta(t)=\frac{2\mu}{e^{2\mu t}-1}\). This fact provides the required sequence \(\left\{X_{\lambda_{n}}\right\}_{n\in\mathbb{N}}\), where \(X_{\lambda_{n}}\) converges to \(X\) uniformly on \([0,T]\), and \(\dot{X}_{\lambda_{n}}\) converges weakly to \(\dot{X}\) in \(L^{2}([0,T],\mathbb{R}^{n})\). As in Appendix E.3.3, denote \(V_{\lambda}\) as \(V\) for the solution with \(\mathbf{A}_{\lambda}\). Then from (i) we know \(V_{\lambda_{n}}\) is nonincreasing for all \(n\in\mathbb{N}\), we have \(\limsup_{n\to\infty}V_{\lambda_{n}}(t)\leq\limsup_{n\to\infty}V_{\lambda_{n}}( 0)=0\). Moreover, we can check the extension of \(\tilde{\mathbf{A}}\) defined in Lemma E.1 is also valid. From (21) and (22), we know \(\left\|\dot{X}_{\lambda}(t)\right\|\leq\frac{e^{\mu T}}{\sqrt{2}}\left\|m( \mathbf{A}(X_{0}))\right\|\) and \(\left\|\mathbf{A}_{\lambda}(X_{\lambda}(t))\right\|\leq\sqrt{e^{2\mu T}+1} \left\|m(\mathbf{A}(X_{0}))\right\|\) for all \(\lambda>0,t\in[0,T]\). Therefore we can prove Corollary E.2 for the case \(\beta(t)=\frac{2\mu}{e^{2\mu t}-1}\) with the same proof, replacing \(M_{\mathbf{A}}(T)\) by \(\sqrt{e^{2\mu T}+1}\left\|m(\mathbf{A}(X_{0}))\right\|\) and \(M_{dot}(T)\) by \(\frac{e^{\mu T}}{\sqrt{2}}\left\|m(\mathbf{A}(X_{0}))\right\|\). Thus \(\tilde{\mathbf{A}}(X(t))\) is well-defined for \(t=0\), plugging \(t=0\) to (47) we obtain \(V(0)=0\). Therefore we have \[\limsup_{n\to\infty}V_{\lambda_{n}}(t)\leq\limsup_{n\to\infty}V_{\lambda_{n}}(0 )=0=V(0),\] it remains to show \(V(t)\leq\limsup_{n\to\infty}V_{\lambda_{n}}(t)\). Observe, as equality \(\dot{X}(t)=-\tilde{\mathbf{A}}(X(t))-\beta(t)(X(t)-X_{0})\) holds since \(t\in S\), from (48) we have \[V(t)=\frac{C(t)^{2}R(t)^{2}}{2}\left(\left\|\dot{X}(t)\right\|^{2}+\dot{\beta} (t)\left\|X(t)-X_{0}\right\|^{2}\right).\] Therefore if is suffices to check Lemma E.4 is valid here with some \(U_{\lambda_{n}}\). For some \(a>0\), Define \(U_{\lambda_{n}}\colon[0,\infty)\to\mathbb{R}\) as

\[U_{\lambda_{n}}(t)=\left\|\dot{X}_{\lambda_{n}}(t)\right\|^{2}\underbrace{-\dot {\beta}(t)\left\|X_{\lambda_{n}}(t)-X_{0}\right\|^{2}+\int_{a}^{t}\left(\ddot{ \beta}(s)-\frac{2\dot{\beta}(s)^{2}}{\beta(s)}\right)\left\|X_{\lambda_{n}}(s )-X_{0}\right\|^{2}ds}_{f_{n}(t)}.\]We proceed similar argument with Lemma B.5. Differentiating \(\dot{X}_{\lambda_{n}}(t)=-\mathsf{A}_{\lambda_{n}}(X(t))-\beta(t)(X_{\lambda_{n}}( t)-X_{0})\), we have for almost all \(t>0\)

\[\ddot{X}_{\lambda_{n}}(t)=-\frac{d}{dt}\mathsf{A}_{\lambda_{n}}(X(t))-\dot{ \beta}(t)(X_{\lambda_{n}}(t)-X_{0})-\beta(t)\dot{X}_{\lambda_{n}}(t).\]

Therefore for almost all \(t>0\),

\[\dot{U}_{\lambda_{n}}(t)\] \[=2\left\langle\dot{X}_{\lambda_{n}}(t),\frac{d}{dt}\mathsf{A}_{ \lambda_{n}}(X(t))\right\rangle-2\beta(t)\left\|\dot{X}_{\lambda_{n}}(t)+\frac {\dot{\beta}(t)}{\beta(t)}\left(X_{\lambda_{n}}(t)-X_{0}\right)\right\|^{2}\leq 0.\]

Therefore \(U_{\lambda_{n}}\) is nonincreasing, we can prove \(\left\|\dot{X}(t)\right\|\leq\limsup_{n\to\infty}\left\|\dot{X}_{\lambda_{n}}( t)\right\|^{2}\) with same argument in Lemma E.4. Therefore we have \(V(t)\leq V(0)=0\) for \(t\in S\). Extending the result to \(t\in[0,\infty)\) can be done with the same argument done in Appendix E.3.4.

#### h.2.2 Proof for convergence rate

Recall

\[V(t) =\frac{(e^{\mu t}-e^{-\mu t})^{2}}{2}\left\|\tilde{\mathsf{A}}(X( t))\right\|^{2}+2\mu(1-e^{-2\mu t})\left\langle\tilde{\mathsf{A}}(X(t)),X(t)-X_{0} \right\rangle-2\mu^{2}\left(1-e^{-2\mu t}\right)\left\|X(t)-X_{0}\right\|^{2}.\]

Observe

\[\frac{2V(t)}{1-e^{-2\mu t}} =(e^{2\mu t}-1)\|\tilde{\mathsf{A}}(X(t))\|^{2}+4\mu\left\langle \tilde{\mathsf{A}}(X(t)),X(t)-X_{0}\right\rangle-4\mu^{2}\left\|X(t)-X_{0} \right\|^{2}\] \[=(e^{\mu t}-1)\underbrace{\left(\|\tilde{\mathsf{A}}(X(t))\|^{2}- 4\mu\left\langle\tilde{\mathsf{A}}(X(t)),X(t)-X_{0}\right\rangle+4\mu^{2} \left\|X(t)-X_{0}\right\|^{2}\right)}_{=\left\|\tilde{\mathsf{A}}(X(t))-2\mu (X(t)-X_{0})\right\|^{2}}\] \[\quad+e^{\mu t}(e^{\mu t}-1)\|\tilde{\mathsf{A}}(X(t))\|^{2}+ \underbrace{e^{\mu t}\left(4\mu\left\langle\tilde{\mathsf{A}}(X(t)),X(t)-X_{ 0}\right\rangle-4\mu^{2}\left\|X(t)-X_{0}\right\|^{2}\right)}_{=p(t)}.\]

From the law of cosines, we have \(\left\|X(t)-X_{0}\right\|^{2}=\left\|X(t)-X_{\star}\right\|^{2}-2\left\langle X (t)-X_{0},X_{0}-X_{\star}\right\rangle-\left\|X_{\star}-X_{0}\right\|^{2}\). Applying this to \(p(t)\) we have

\[p(t) =4\mu e^{\mu t}\big{(}\langle\tilde{\mathsf{A}}(X(t)),X(t)-X_{ \star}\rangle-\langle\tilde{\mathsf{A}}(X(t)),X_{0}-X_{\star}\rangle-\mu \|X(t)-X_{0}\|^{2}\big{)}\] \[=4\mu e^{\mu t}\left(\langle\tilde{\mathsf{A}}(X(t))+\mu(X(t)-X_{ \star}),X(t)-X_{\star}\rangle-\langle\tilde{\mathsf{A}}(X(t))-2\mu(X(t)-X_{0} ),X_{0}-X_{\star}\rangle-\mu\|X_{0}-X_{\star}\|^{2}.\right)\]

Thus

\[\frac{2V(t)}{1-e^{-2\mu t}} =(e^{\mu t}-1)\left\|\tilde{\mathsf{A}}(X(t))-2\mu(X(t)-X_{0}) \right\|^{2}+e^{\mu t}(e^{\mu t}-1)\|\tilde{\mathsf{A}}(X(t))\|^{2}+p(t)\] \[=(e^{\mu t}-1)\|\tilde{\mathsf{A}}(X(t))-2\mu(X(t)-X_{0})\|^{2}-4 \mu e^{\mu t}\langle\tilde{\mathsf{A}}(X(t))-2\mu(X(t)-X_{0}),X_{0}-X_{\star}\rangle\] \[\quad+e^{\mu t}(e^{\mu t}-1)\|\tilde{\mathsf{A}}(X(t))\|^{2}+4\mu e ^{\mu t}\langle\tilde{\mathsf{A}}(X(t))-\mu(X(t)-X_{\star}),X(t)-X_{\star} \rangle+4\mu^{2}e^{\mu t}\|X_{0}-X_{\star}\|^{2}\] \[=(e^{\mu t}-1)\left\|\tilde{\mathsf{A}}(X(t))-2\mu(X(t)-X_{0})- \frac{2\mu e^{\mu t}}{e^{\mu t}-1}(X_{0}-X_{\star})\right\|^{2}-\frac{4\mu^{2}e ^{2\mu t}}{e^{\mu t}-1}\|X_{0}-X_{\star}\|^{2}\] \[\quad+e^{\mu t}(e^{\mu t}-1)\|\tilde{\mathsf{A}}(X(t))\|^{2}+4\mu e ^{\mu t}\left(\langle\tilde{\mathsf{A}}(X(t)),X(t)-X_{\star}\rangle-\mu\left\|X (t)-X_{\star}\right\|^{2}\right)+4\mu^{2}e^{\mu t}\|X_{0}-X_{\star}\|^{2}.\]Now since \(\tilde{\mathbf{A}}\) is \(\mu\)-strongly monotone, \(\left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{\star}\right\rangle-\mu\left\|X(t) -X_{\star}\right\|^{2}\geq 0\). From previous section we know \(0=V(0)\geq V(t)\) for all \(t>0\). Therefore for all \(t>0\)

\[0\geq\frac{2V(t)}{1-e^{-2\mu t}} \geq e^{\mu t}(e^{\mu t}-1)\|\tilde{\mathbf{A}}(X(t))\|^{2}+\left(4 \mu^{2}e^{\mu t}-\frac{4\mu^{2}e^{2\mu t}}{e^{\mu t}-1}\right)\|X_{0}-X_{\star }\|^{2}\] \[= e^{\mu t}(e^{\mu t}-1)\|\tilde{\mathbf{A}}(X(t))\|^{2}-\frac{4 \mu^{2}e^{\mu t}}{e^{\mu t}-1}\|X_{0}-X_{\star}\|^{2}.\]

Organizing, we conclude

\[\|\tilde{\mathbf{A}}(X(t))\|^{2}\leq 4\left(\frac{\mu}{e^{\mu t}-1} \right)^{2}\|X_{0}-X_{\star}\|^{2}.\]

#### h.2.3 Informal derivation of ODE from the method

Assume \(\mathbf{A}\colon\mathbb{R}^{n}\to\mathbb{R}^{n}\) be a continuous monotone operator. In [54], the method OS-PPM is presented as

\[x^{k} =\mathbf{J}_{h\mathbf{A}}y^{k-1}\] \[y^{k} =\left(1-\frac{1}{s_{k}}\right)\left\{x^{k}-\frac{1}{\nu}(y^{k-1} -x^{k})\right\}+\frac{1}{s_{k}}y^{0}\]

where \(y^{0}=x^{0}\), \(\nu=1+2h\mu\) and \(s_{k}=1+\nu^{2}+\cdots+\nu^{2k}=\frac{\nu^{2k+2}-1}{\nu^{2}-1}\). Using \(y^{k-1}=x^{k}+h\mathbf{A}x^{k}\), substituting \(y^{k}\) and \(y^{k-1}\) this method can be expressed in a single line,

\[x^{k+1}+h\mathbf{A}x^{k+1}=\left(1-\frac{1}{s_{k}}\right)\left(x ^{k}-\frac{h}{\nu}\mathbf{A}x^{k}\right)+\frac{1}{s_{k}}x^{0}.\]

Reorganizing and dividing both sides by \(h\), we have

\[\frac{x^{k+1}-x^{k}}{h}=-\mathbf{A}x^{k+1}-\left(\frac{1}{\nu}- \frac{1}{\nu s_{k}}\right)\mathbf{A}x^{k}-\frac{1}{hs_{k}}(x^{k}-x^{0}).\]

Identifying \(x^{0}=X_{0}\), \(2hk=t\), \(x^{k}=X(t)\),

\[\frac{X(t+2h)-X(t)}{h}=-\mathbf{A}(X(t+2h))-\left(\frac{1}{\nu}- \frac{1}{\nu s_{k}}\right)\mathbf{A}(X(t))-\frac{1}{hs_{k}}(X(t)-X_{0}).\]

Now observe

\[\lim_{h\to 0^{+}}hs_{k} =\lim_{h\to 0^{+}}h\frac{\nu^{2k+2}-1}{\nu^{2}-1}=\lim_{h\to 0^{+}}h\frac{(1+2h\mu)^{2k+2}-1}{(1 +2h\mu)^{2}-1}\] \[=\lim_{h\to 0^{+}}\frac{(1+2h\mu)^{2}(1+2h\mu)^{t/h}-1}{4\mu(1+h \mu)}=\frac{e^{2\mu t}-1}{4\mu}\] \[\lim_{h\to 0^{+}}\frac{1}{\nu} =\lim_{h\to 0^{+}}\frac{1}{1+2h\mu}=1\] \[\lim_{h\to 0^{+}}\frac{1}{\nu s_{k}} =\lim_{h\to 0^{+}}\frac{1}{\nu}\frac{1}{hs_{k}}h=1\times\frac{4\mu}{e^ {2\mu t}-1}\times 0=0.\]

Taking limit \(h\to 0^{+}\) and organizing,

\[2\dot{X}(t)=-\mathbf{A}(X(t))-(1+0)\mathbf{A}(X(t))-\frac{4\mu}{e^{2\mu t}-1}( X(t)-X_{0}).\]

By diving both sides by \(2\), we get the desired anchor ODE

\[\dot{X}(t)=-\mathbf{A}(X(t))-\frac{2\mu}{e^{2\mu t}-1}(X(t)-X_{0}).\]Proof omitted in Section 7

### Proof for Theorem 7.1

We first check

\[\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}\leq 4\beta(t)^{2}\left\|X_{0}-X_{ \star}\right\|^{2}=\mathcal{O}\left(\beta(t)^{2}\right),\]

where

\[\dot{X}(t) =-\tilde{\mathbf{A}}(X(t))-\beta(t)(X(t)-X_{0})\] \[\beta(t) =-\frac{\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}}{2\left< \tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right>}.\]

By definition of \(\beta(t)\), \(\Phi(t)\) defined in (8) becomes zero, i.e.

\[0\equiv\Phi(t)=\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2\beta(t)\left< \tilde{\mathbf{A}}(X(t)),X-X_{0}\right>.\]

Plugging \(\Phi(t)=0\) to inequality (8) we have

\[0\geq\frac{1}{2}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}-2\beta(t)^{2} \left\|X_{0}-X_{\star}\right\|^{2}.\]

Reorganizing, we get the desired result. Other results need some works, we provide the proof with steps into subsections.

#### i.1.1 \(\beta(t)>0\) for \(t>0\)

Taking inner product with \(\tilde{\mathbf{A}}(X)\) to the ODE and applying \(\frac{1}{2}\Phi(t)=0\) we have

\[\left<\dot{X}(t),\tilde{\mathbf{A}}(X(t))\right>=-\left\|\tilde{\mathbf{A}}( X(t))\right\|^{2}-\beta(t)\left<\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right>=- \frac{1}{2}\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}.\]

As \(\mathbf{A}\) is assumed to be continuous, taking limit \(t\to 0+\) we have

\[\lim_{t\to 0+}\left<\dot{X}(t),\tilde{\mathbf{A}}(X(t))\right>=\left<\lim_{t \to 0+}\dot{X}(t),\tilde{\mathbf{A}}(X_{0})\right>=-\frac{1}{2}\lim_{t\to 0+} \left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}=-\frac{1}{2}\left\|\tilde{ \mathbf{A}}(X_{0})\right\|^{2}.\]

On the other hand, by assumption we have \(\lim_{t\to 0+}\dot{X}(t)=\lim_{t\to 0+}\frac{X(t)-X_{0}}{t}\) and \(\left\|\tilde{\mathbf{A}}(X_{0})\right\|\neq 0\), thus

\[\lim_{t\to 0+}\frac{1}{t}\left<\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right>= \left<\tilde{\mathbf{A}}(X_{0}),\lim_{t\to 0+}\dot{X}(t)\right>=-\frac{1}{2} \left\|\tilde{\mathbf{A}}(X_{0})\right\|^{2}<0.\] (50)

Therefore there is \(\epsilon>0\) such that \(\left<\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right><0\) for \(t\in(0,\epsilon)\), thus for \(t\in(0,\epsilon)\) we have \(\left<\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right>\neq 0\) so \(\beta(t)\) is well-defined and satisfies

\[\beta(t)=-\frac{\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}}{2\left<\tilde{ \mathbf{A}}(X(t)),X(t)-X_{0}\right>}>0.\]

Observe the denominator of \(\beta(t)\) is zero when \(\left\|\tilde{\mathbf{A}}(X(t))\right\|=0\). As \(\beta\) is assumed to be well-defined, we have \(\left\|\tilde{\mathbf{A}}(X(t))\right\|\neq 0\) for all \(t>0\). Since \(\beta\) is continuous as \(\mathbf{A}\) and \(X\) are continuous, by intermediate value theorem we have \(\left\|\tilde{\mathbf{A}}(X(t))\right\|>0\) for all \(t>0\).

#### i.1.2 Proof for the main statements

We first show following lemma.

**Lemma I.1**.: _Following equality holds for almost every \(t>0\)._

\[\left(\dot{\beta}(t)+\beta(t)^{2}\right)\left<\tilde{\mathbf{A}}(X(t)),X(t)-X_ {0}\right>=\left<\frac{d}{dt}\tilde{\mathbf{A}}(X(t)),\dot{X}(t)\right>.\] (51)Proof.: Since \(\tilde{\mathbf{A}}\) is Lipschitz continuous by assumption, by Lemma B.4 we know \(\tilde{\mathbf{A}}(X(t))\) is differentiable almost everywhere. Differentiating \(\Phi(t)\) we have

\[0 =\dot{\Phi}(t)\] \[=2\left\langle\frac{d}{dt}\tilde{\mathbf{A}}(X(t)),\tilde{ \mathbf{A}}(X(t))+\beta(t)(X(t)-X_{0})\right\rangle+2\dot{\beta}(t)\left\langle \tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle\] \[\quad+2\beta(t)\left\langle\tilde{\mathbf{A}}(X(t)),-\tilde{ \mathbf{A}}(X(t))-\beta(t)(X(t)-X_{0})\right\rangle\] \[=-2\left\langle\frac{d}{dt}\tilde{\mathbf{A}}(X(t)),\dot{X}(t) \right\rangle-2\beta(t)\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2\left( \dot{\beta}(t)-\beta(t)^{2}\right)\left\langle\tilde{\mathbf{A}}(X(t)),X(t)- X_{0}\right\rangle\] \[=-2\left\langle\frac{d}{dt}\tilde{\mathbf{A}}(X(t)),\dot{X}(t) \right\rangle+4\beta(t)^{2}\left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0} \right\rangle+2\left(\dot{\beta}(t)-\beta(t)^{2}\right)\left\langle\tilde{ \mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle\] \[=-2\left\langle\frac{d}{dt}\tilde{\mathbf{A}}(X(t)),\dot{X}(t) \right\rangle+2\left(\dot{\beta}(t)+\beta(t)^{2}\right)\left\langle\tilde{ \mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle.\]

for \(t\in(0,\infty)\) almost everywhere. The equalities come from the ODE \(\dot{X}(t)=-\tilde{\mathbf{A}}(X(t))-\beta(t)(X(t)-X_{0})\) and the fact \(\Phi(t)=0\). Reorganizing, we have the desired equation (51). 

Now we show the upper bounds of \(\beta(t)\) for each monotone and strongly monotone case. Observe from (51) and the definition of \(\beta(t)\), we have for almost all \(t\in(0,\infty)\)

\[-\frac{\dot{\beta}(t)+\beta(t)^{2}}{2\beta(t)}\left\|\tilde{ \mathbf{A}}(X(t))\right\|^{2}=\left\langle\frac{d}{dt}\tilde{\mathbf{A}}(X), \dot{X}\right\rangle.\] (52)

1. When \(\tilde{\mathbf{A}}\) is monotone. From (52) and (1) we have for almost all \(t\in(0,\infty)\) \[-\frac{\dot{\beta}(t)+\beta(t)^{2}}{2\beta(t)}\left\|\tilde{ \mathbf{A}}(X(t))\right\|^{2}=\left\langle\frac{d}{dt}\tilde{\mathbf{A}}(X), \dot{X}\right\rangle\geq 0.\] Since \(\beta(t)>0\) we have \[\dot{\beta}(t)+\beta(t)^{2}\leq 0.\] almost everywhere. Now dividing both sides by \(\beta(t)^{2}\) we have \[1\leq-\frac{\dot{\beta}(t)}{\beta(t)^{2}}\] holds almost everywhere. Since \(-\frac{\dot{\beta}(t)}{\beta(t)^{2}}=\frac{d}{dt}\left(\frac{1}{\beta(t)}\right)\), integrating above inequality both side from \(\delta\) to \(t\) we have \[t-\delta\leq\frac{1}{\beta(t)}-\frac{1}{\beta(\delta)}\Longrightarrow \beta(t)\leq\frac{1}{t-\delta+\frac{1}{\beta(\delta)}}.\] By the way, from (50) we have \[\lim_{t\to 0+}t\beta(t)=-\lim_{t\to 0+}\frac{\left\|\tilde{ \mathbf{A}}(X(t))\right\|^{2}}{2\left\langle\tilde{\mathbf{A}}(X(t)),\frac{X( t)-X_{0}}{t}\right\rangle}=1,\] thus \(\lim_{\delta\to 0+}\beta(\delta)=\infty\), so \(\lim_{\delta\to 0+}\frac{1}{\beta(\delta)}=0\). Therefore taking limit \(\delta\to 0+\) we have \[\beta(t)\leq\lim_{\delta\to 0+}\frac{1}{t-\delta+\frac{1}{\beta(\delta)}}= \frac{1}{t}.\]2. When \(\tilde{\mathbf{A}}\) is \(\mu\)-strongly monotone. From (52) and (2) for almost all \(t\in(0,\infty)\) we have \[-\frac{\dot{\beta}(t)+\beta(t)^{2}}{2\beta(t)}\left\|\tilde{\mathbf{A}}(X(t)) \right\|^{2}=\left\langle\frac{d}{dt}\tilde{\mathbf{A}}(X),\dot{X}\right\rangle \geq\mu\left\|\dot{X}(t)\right\|^{2}.\] (53) On the other hand, observe \[\left\|\dot{X}(t)\right\|^{2} =\left\|\tilde{\mathbf{A}}(X(t))+\beta(t)(X(t)-X_{0})\right\|^{2}\] \[=\underbrace{\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}+2\beta( t)\left\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle}_{=\Phi(t)=0}+ \beta(t)^{2}\left\|X(t)-X_{0}\right\|^{2}=\beta(t)^{2}\left\|X(t)-X_{0}\right\| ^{2}.\] From Cauchy-Schwarz inequality we see \[\left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}=-2\beta(t)\left\langle\tilde{ \mathbf{A}}(X(t)),X(t)-X_{0}\right\rangle\leq 2\beta(t)\left\|\tilde{ \mathbf{A}}(X(t))\right\|\left\|X(t)-X_{0}\right\|,\] therefore \(\left\|\tilde{\mathbf{A}}(X(t))\right\|\leq 2\beta(t)\left\|X(t)-X_{0}\right\|\). Combining above observations, we have for almost all \(t>0\) \[\frac{4\left\|\dot{X}(t)\right\|^{2}}{\left\|\tilde{\mathbf{A}}(X(t))\right\| ^{2}}=\frac{4\beta(t)^{2}\left\|X(t)-X_{0}\right\|^{2}}{\left\|\tilde{ \mathbf{A}}(X(t))\right\|^{2}}\geq 1.\] (54) From (53) and (54), we get an inequality for \(\beta(t)\) and \(\dot{\beta}(t)\) \[-\frac{\dot{\beta}(t)+\beta(t)^{2}}{2\beta(t)}=\frac{\dot{\beta}(t)}{2\beta(t )}+\frac{\beta(t)}{2}\geq\frac{\mu}{4}.\] Moving \(\frac{\beta(t)}{2}\) to the right-hand side and reorganizing, we have for almost all \(t>0\) \[1\leq-\frac{\dot{\beta}(t)}{\beta(t)(\frac{\mu}{2}+\beta(t))}=-\frac{\dot{ \beta}(t)}{\frac{\mu}{2}}\left(\frac{1}{\beta(t)}-\frac{1}{\frac{\mu}{2}+ \beta(t)}\right)\quad\Longrightarrow\quad\frac{\mu}{2}\leq-\frac{\dot{\beta}( t)}{\beta(t)}+\frac{\dot{\beta}(t)}{\frac{\mu}{2}+\beta(t)}.\] Integrating above inequality both sides from \(\delta\) to \(t\) we have \[\frac{\mu}{2}(t-\delta)\leq\left[-\log\beta(t)+\log\left(\frac{\mu}{2}+\beta (t)\right)\right]_{\delta}^{t}=\log\frac{\frac{\mu}{2}+\beta(t)}{\beta(t)}- \log\frac{\frac{\mu}{2}+\beta(\delta)}{\beta(\delta)}.\] As observed in case (i) we know \(\lim_{\delta\to 0+}\beta(\delta)=\infty\), taking limit \(\delta\to 0+\) both sides we have \[\frac{\mu t}{2}\leq\log\frac{\frac{\mu}{2}+\beta(t)}{\beta(t)}=\log\left(1+ \frac{\mu/2}{\beta(t)}\right)\Longrightarrow e^{\mu t/2}\leq 1+\frac{\mu/2}{ \beta(t)}.\] Organizing, we get the desired result. \[\beta(t)\leq\frac{\mu/2}{e^{\mu t/2}-1}.\]

### Correspondence between the ODE (11) and the discrete method in Theorem 7.2

Observe, the case \(h=1\) for below method corresponds to the method provided in Theorem 7.2.

\[x^{k} =\mathbf{J}_{h\mathbf{A}}y^{k-1}\] \[\beta_{k} =\begin{cases}\frac{\|h\tilde{\mathbf{A}}x^{k}\|^{2}}{-\langle h \tilde{\mathbf{A}}x^{k},\,x^{k}-x^{0}\rangle+\|h\tilde{\mathbf{A}}x^{k}\|^{2} }&\text{ if }\|\tilde{\mathbf{A}}x^{k}\|^{2}\neq 0\\ 0&\text{ if }\|\tilde{\mathbf{A}}x^{k}\|^{2}=0\end{cases}\] \[y^{k} =(1-\beta_{k})(2x^{k}-y^{k-1})+\beta_{k}x^{0}.\]We now show when \(\mathbf{A}\) is continuous and \(\|\tilde{\mathbf{A}}x^{k}\|^{2}\neq 0\) for all \(k\geq 0\), we obtain the ODE (11) when we take limit \(h\to 0+\). Note when \(\mathbf{A}\) is continuous \(\mathbf{A}\) equals to \(\tilde{\mathbf{A}}\).

From \(h\tilde{\mathbf{A}}x^{k+1}+x^{k+1}=y^{k}\), substituting \(y^{k}\) and \(y^{k-1}\) we get a single line expression

\[h\tilde{\mathbf{A}}x^{k+1}+x^{k+1}=(1-\beta_{k})(x^{k}-h\tilde{\mathbf{A}}x^{k })+\beta_{k}x^{0}.\]

Reorganizing and dividing both sides by \(h\), we have

\[\frac{x^{k+1}-x^{k}}{h}=-\tilde{\mathbf{A}}x^{k+1}-(1-\beta_{k})\tilde{\mathbf{ A}}x^{k}-\frac{\beta_{k}}{h}(x^{k}-x^{0}).\]

Identify \(x^{0}=X_{0}\), \(2hk=t\), and \(x^{k}=X(t)\). Then we see

\[\frac{\beta_{k}}{h}=\frac{h^{2}\|\tilde{\mathbf{A}}x^{k}\|^{2}}{-h^{2}\langle \tilde{\mathbf{A}}x^{k},\,x^{k}-x^{0}\rangle+h^{3}\|\tilde{\mathbf{A}}x^{k}\| ^{2}}=\frac{\|\tilde{\mathbf{A}}(X(t))\|^{2}}{-\langle\tilde{\mathbf{A}}(X(t )),\,X(t)-X_{0}\rangle+h\|\tilde{\mathbf{A}}(X(t))\|^{2}}.\]

Thus \(\beta_{k}=\mathcal{O}\left(h\right)\), we have \(\lim_{h\to 0+}\beta_{k}=0\). Now taking limit \(h\to 0+\) we have

\[2\dot{X}(t)=-2\tilde{\mathbf{A}}(X(t))-\frac{\left\|\tilde{\mathbf{A}}(X(t)) \right\|^{2}}{\langle\tilde{\mathbf{A}}(X(t)),X(t)-X_{0}\rangle}(X(t)-X_{0}).\]

Dividing both sides by 2, we obtain (11).

#### 1.2.1 Correspondence between convergence rates in Theorem 7.1 and Theorem 7.2

From identification above, we see \(\beta(t)\) corresponds to \(\frac{\beta_{k}}{2h}\). Therefore we see with identification \(x^{0}=X_{0}\), \(x^{\star}=X_{\star}\), \(2hk=t\), and \(x^{k}=X(t)\) we have

\[\left\|h\tilde{\mathbf{A}}(x^{k+1})\right\|^{2}\leq\beta_{k}^{2}\left\|x^{0}- x^{\star}\right\|^{2}=4h^{2}\frac{\beta_{k}^{2}}{(2h)^{2}}\left\|x^{0}-x^{\star} \right\|^{2}\quad\xrightarrow{\text{divide by }h^{2},\,h\to 0+} \left\|\tilde{\mathbf{A}}(X(t))\right\|^{2}\leq 4\beta(t)^{2}\left\|X_{0}-X_{ \star}\right\|^{2}.\]

And we can also check that the bound \(\beta_{k}\leq\frac{1}{k+1}\) for the monotone case, is equivalent to \(\frac{\beta_{k}}{2h}\leq\frac{1}{2hk+\mathcal{O}(h)}\) and corresponds to \(\beta(t)\leq\frac{1}{t}\) as well.

Now suppose \(\mathbf{A}\) is \(\mu\)-strongly monotone and \(L\)-Lipschitz continuous. Then \(h\mathbf{A}\) is \(h\mu\)-strongly monotone and \(hL\)-Lipschitz continuous, we have the following inequality from Theorem 7.2

\[\frac{\beta_{k}}{2h}\leq\frac{\frac{\mu}{2(1+\left(hL\right)^{2})}}{\left(1+ \frac{h\mu}{1+\left(hL\right)^{2}}\right)^{k}-1+\frac{h\mu}{1+\left(hL\right)^ {2}}}.\] (55)

Recalling the identification \(2hk=t\), we see

\[\left(1+\frac{h\mu}{1+\left(hL\right)^{2}}\right)^{k}=\left(1+\frac{h\mu}{1+ \left(hL\right)^{2}}\right)^{\frac{1}{2h}}=\left(\left(1+\frac{h\mu}{1+\left( hL\right)^{2}}\right)^{\frac{1+\left(hL\right)^{2}}{h\mu}}\right)^{\frac{\mu t}{2(1+ \left(hL\right)^{2})}}\xrightarrow{h\to 0+}\quad e^{\mu t/2}.\]

Therefore identifying \(\beta(t)=\frac{\beta_{k}}{2h}\) and taking limit \(h\to 0+\) to the inequality (55), we have

\[\beta(t)\leq\frac{\mu/2}{e^{\mu t/2}-1}.\]

### Proof of Theorem 7.2

Recall, the method was defined as

\[x^{k} =\mathbb{J}_{\Lambda}y^{k-1}\] \[\beta_{k} =\begin{cases}\frac{\|\tilde{\mathbf{A}}x^{k}\|^{2}}{-\langle \tilde{\mathbf{A}}x^{k},\,x^{k}-x^{0}\rangle+\|\tilde{\mathbf{A}}x^{k}\|^{2}} &\text{if }\|\tilde{\mathbf{A}}x^{k}\|^{2}\neq 0\\ 0&\text{if }\|\tilde{\mathbf{A}}x^{k}\|^{2}=0\end{cases}\] \[y^{k} =(1-\beta_{k})(2x^{k}-y^{k-1})+\beta_{k}x^{0}.\]First, we assume \(\tilde{\mathbf{A}}x^{k}\neq 0\) for all \(k\geq 0\). Define

\[\tilde{\Phi}^{k}=(1-\beta_{k})\,\|\tilde{\mathbf{A}}x^{k}\|^{2}+\beta_{k}\langle \tilde{\mathbf{A}}x^{k},\,x^{k}-x^{0}\rangle\]

for \(k=1,2,\dots,\) and

\[\Phi^{k}=\|\tilde{\mathbf{A}}x^{k}\|^{2}+\beta_{k-1}\langle\tilde{\mathbf{A}} x^{k},\,x^{k}-x^{0}\rangle\]

for \(k=2,3,\dots\). Note by definition of \(\beta_{k}\), we have \(\tilde{\Phi}^{k}=0\). Our goal is to prove

\[\Phi^{k+1}\leq 0.\]

Then with the same argument with (8) we can conclude \(\|\tilde{\mathbf{A}}x^{k+1}\|^{2}\leq\beta_{k}^{2}\|x^{0}-x^{*}\|^{2}.\) To do so, we first show following lemma.

**Lemma I.2**.: _For \(k\geq 1\), following is true._

\[(1-\beta_{k})\tilde{\Phi}^{k}-\Phi^{k+1}=(1-\beta_{k})\langle\tilde{\mathbf{ A}}x^{k+1}-\tilde{\mathbf{A}}x^{k},\,x^{k+1}-x^{k}\rangle.\] (56)

Proof.: Recall \(y^{k}=x^{k+1}+\tilde{\mathbf{A}}x^{k+1}\). Substituting \(y^{k}\) and \(y^{k-1}\), the method is equivalent to

\[x^{k+1}+\tilde{\mathbf{A}}x^{k+1}=(1-\beta_{k})(x^{k}-\tilde{\mathbf{A}}x^{k} )+\beta_{k}x^{0}.\]

From above we can get two different expression of \((1-\beta_{k})(x^{k+1}-x^{k})\).

\[(1-\beta_{k})(x^{k+1}-x^{k}) =-(1-\beta_{k})(\tilde{\mathbf{A}}x^{k+1}+\tilde{\mathbf{A}}x^{k })-\beta_{k}\left(\tilde{\mathbf{A}}x^{k+1}+(x^{k+1}-x^{0})\right)\] \[=-(1-\beta_{k})\left[(\tilde{\mathbf{A}}x^{k+1}+\tilde{\mathbf{A} }x^{k})-\beta_{k}\left(\tilde{\mathbf{A}}x^{k}-(x^{k}-x^{0})\right)\right].\]

With reorganizing, first equality can be obtained by subtracting both sides by \(\beta_{k}(x^{k+1}-x^{0})\) and the second equality can be obtained by multiplying both sides by \((1-\beta_{k})\).

From this we have

\[(1-\beta_{k})\langle\tilde{\mathbf{A}}x^{k+1}-\tilde{\mathbf{A}} x^{k},\,x^{k+1}-x^{k}\rangle\] \[=\langle\tilde{\mathbf{A}}x^{k+1},\,(1-\beta_{k})(x^{k+1}-x^{k}) \rangle-\langle\tilde{\mathbf{A}}x^{k},\,(1-\beta_{k})(x^{k+1}-x^{k})\rangle\] \[=\langle\tilde{\mathbf{A}}x^{k+1},\,-(1-\beta_{k})(\tilde{ \mathbf{A}}x^{k+1}+\tilde{\mathbf{A}}x^{k})-\beta_{k}\{\tilde{\mathbf{A}}x^{k +1}+(x^{k+1}-x^{0})\}\rangle\] \[\quad-(1-\beta_{k})\langle\tilde{\mathbf{A}}x^{k},\,-(\tilde{ \mathbf{A}}x^{k+1}+\tilde{\mathbf{A}}x^{k})+\beta_{k}\{\tilde{\mathbf{A}}x^{k }-(x^{k}-x^{0})\}\rangle\] \[=-(1-\beta_{k})\|\tilde{\mathbf{A}}x^{k+1}\|^{2}-\beta_{k} \langle\tilde{\mathbf{A}}x^{k+1},\,\tilde{\mathbf{A}}x^{k+1}+(x^{k+1}-x^{0})\rangle\] \[\quad+(1-\beta_{k})\left(\|\tilde{\mathbf{A}}x^{k}\|^{2}-\beta_ {k}\langle\tilde{\mathbf{A}}x^{k},\,\tilde{\mathbf{A}}x^{k}-(x^{k}-x^{0}) \rangle\right)\] \[=(1-\beta_{k})\left((1-\beta_{k})\|\tilde{\mathbf{A}}x^{k}\|^{2}+ \beta_{k}\langle\tilde{\mathbf{A}}x^{k},\,x^{k}-x^{0}\rangle\right)-\|\tilde{ \mathbf{A}}x^{k+1}\|^{2}-\beta_{k}\langle\tilde{\mathbf{A}}x^{k+1},\,x^{k+1}- x^{0}\rangle\] \[=(1-\beta_{k})\tilde{\Phi}^{k}-\Phi^{k+1}.\qed\]

Since \(\mathbf{A}\) is monotone we have \(\langle\tilde{\mathbf{A}}x^{k+1}-\tilde{\mathbf{A}}x^{k},\,x^{k+1}-x^{k} \rangle\geq 0\), we see that the right-hand side of (56) is greater or equal to \(0\) if \(1-\beta_{k}\geq 0\). Thus it remains to show \(1-\beta_{k}\geq 0\). As the index is quite confusing, we provide it as a lemma to avoid confusion while proceeding the proof.

**Lemma I.3**.: _If \(\tilde{\mathbf{A}}x^{k}\neq 0\) for all \(k\geq 1\), then_

\[\left\langle\tilde{\mathbf{A}}x^{k},x^{k}-x^{0}\rangle<0,\quad\beta_{k}\in(0, 1),\quad\Phi^{k+1}\leq 0\]

_for all \(k\geq 1\)._

Proof.: Proof by induction. From \(x^{1}=\mathbf{J}_{\mathbf{A}}y^{0}=\mathbf{J}_{\mathbf{A}}x^{0}\), we have \(x^{1}+\tilde{\mathbf{A}}x^{1}=x^{0}\) and so \(x^{1}-x^{0}=-\tilde{\mathbf{A}}x^{1}\). Applying these facts we have

\[\langle\tilde{\mathbf{A}}x^{1},\,x^{1}-x^{0}\rangle =-\|\tilde{\mathbf{A}}x^{1}\|^{2}<0,\] \[\beta_{1} =\frac{\|\tilde{\mathbf{A}}x^{1}\|^{2}}{-\langle\tilde{\mathbf{A}} x^{1},\,x^{1}-x^{0}\rangle+\|\tilde{\mathbf{A}}x^{1}\|^{2}}=\frac{1}{2}\in(0,1).\]As \(1-\beta_{1}>0\), from (56) we have

\[(1-\beta_{1})\tilde{\Phi}^{1}-\Phi^{2}=(1-\beta_{1})\langle\tilde{\mathbf{A}}x^{ 2}-\tilde{\mathbf{A}}x^{1},\,x^{2}-x^{1}\rangle\geq 0.\]

As \(\tilde{\Phi}^{1}=0\) by definition, we have \(\Phi^{2}\leq 0\). Therefore the statement is true for \(k=1\).

Now suppose the statements are true for \(k\). By induction hypothesis, we know

\[0\geq\Phi^{k+1}=\|\tilde{\mathbf{A}}x^{k+1}\|^{2}+\beta_{k}\langle\tilde{ \mathbf{A}}x^{k+1},\,x^{k+1}-x^{0}\rangle.\]

As \(\beta_{k}>0\) from induction hypothesis and \(\tilde{\mathbf{A}}x^{k+1}\neq 0\) by assumption, reorganizing \(\Phi^{k}\leq 0\) we have

\[\langle\tilde{\mathbf{A}}x^{k+1},\,x^{k+1}-x^{0}\rangle\leq-\frac{\|\tilde{ \mathbf{A}}x^{k+1}\|^{2}}{\beta_{k}}<0.\]

And therefore

\[\beta_{k+1}=\underbrace{\frac{\|\tilde{\mathbf{A}}x^{k+1}\|^{2}}{ \underline{-\langle\tilde{\mathbf{A}}x^{k+1},\,x^{k+1}-x^{0}\rangle}+\|\tilde {\mathbf{A}}x^{k+1}\|^{2}}}_{>0}\in(0,1).\]

Since \(1-\beta_{k+1}>0\), by (56) we have

\[(1-\beta_{k+1})\tilde{\Phi}^{k+1}-\Phi^{k+2}=(1-\beta_{k+1})\langle\tilde{ \mathbf{A}}x^{k+2}-\tilde{\mathbf{A}}x^{k+1},\,x^{k+2}-x^{k+1}\rangle\geq 0.\]

Since \(\tilde{\Phi}^{k+1}=0\) by definition, we have \(\Phi^{k+2}\leq 0\). Therefore the statements are true for \(k+1\), by induction, we get the desired result. 

Suppose \(\|\tilde{\mathbf{A}}x^{k}\|^{2}\neq 0\) for all \(k\geq 1\). From the lemma we know \(\Phi^{k+1}\leq 0\) for \(k\geq 1\), so with the same argument of (8) we have for \(x^{\star}\in\mathrm{Zer}\mathbf{A}\)

\[0 \geq\Phi^{k+1}=\|\tilde{\mathbf{A}}x^{k+1}\|^{2}+\beta_{k}\langle \tilde{\mathbf{A}}x^{k+1},\,x^{k+1}-x^{\star}\rangle-\beta_{k}\langle\tilde{ \mathbf{A}}x^{k+1},\,x^{0}-x^{\star}\rangle\] \[\geq\|\tilde{\mathbf{A}}x^{k+1}\|^{2}-\beta_{k}\langle\tilde{ \mathbf{A}}x^{k+1},\,x^{0}-x^{\star}\rangle\] \[\geq|\tilde{\mathbf{A}}x^{k+1}\|^{2}-\left(\frac{1}{2}\|\tilde{ \mathbf{A}}x^{k+1}\|^{2}+\frac{\beta_{k}^{2}}{2}\|x^{0}-x^{\star}\|^{2}\right) =\frac{1}{2}\|\tilde{\mathbf{A}}x^{k+1}\|^{2}-\frac{\beta_{k}^{2}}{2}\|x^{0}- x^{\star}\|^{2}.\]

Organizing, we get

\[\|\tilde{\mathbf{A}}x^{k+1}\|^{2}\leq\beta_{k}^{2}\|x^{0}-x^{\star}\|^{2}.\]

Now we show the upper bound of \(\beta_{k}\). Observe from (56) and the fact \(\tilde{\Phi}=0\), we have

\[(1-\beta_{k})\left\langle\tilde{\mathbf{A}}x^{k+1}-\tilde{\mathbf{A }}x^{k},x^{k+1}-x^{k}\right\rangle =(1-\beta_{k})\tilde{\Phi}^{k}-\Phi^{k+1}\] \[=\frac{\beta_{k}}{\beta_{k+1}}\tilde{\Phi}^{k+1}-\Phi^{k+1}= \left(\frac{\beta_{k}}{\beta_{k+1}}-\beta_{k}-1\right)\left\|\tilde{\mathbf{A }}x^{k+1}\right\|^{2}.\]

As \(1-\beta_{k}\neq 0\) for \(k\geq 1\) by Lemma I.3, dividing both sides by \(1-\beta_{k}\), we get the discrete counterpart of (52),

\[\left\langle\tilde{\mathbf{A}}x^{k+1}-\tilde{\mathbf{A}}x^{k},x^{k+1}-x^{k} \right\rangle=\frac{\frac{\beta_{k}}{\beta_{k+1}}-\beta_{k}-1}{1-\beta_{k}} \left\|\tilde{\mathbf{A}}x^{k+1}\right\|^{2}.\] (57)

1. When \(\mathbf{A}\) is monotone. From (57) and monotonicity of \(\mathbf{A}\) we have \[0\leq\left\langle\tilde{\mathbf{A}}x^{k+1}-\tilde{\mathbf{A}}x^{k},x^{k+1}-x^ {k}\right\rangle=\frac{\frac{\beta_{k}}{\beta_{k+1}}-\beta_{k}-1}{1-\beta_{k}} \left\|\tilde{\mathbf{A}}x^{k+1}\right\|^{2}.\] From Lemma I.3 we have \(1-\beta_{k}>0\), therefore \[\frac{\beta_{k}}{\beta_{k+1}}-\beta_{k}-1\geq 0\quad\Longrightarrow\quad\frac{1}{ \beta_{k}}+1\leq\frac{1}{\beta_{k+1}}.\] Summing up, as \(\beta_{1}=\frac{1}{2}\) and \(\frac{1}{\beta_{k}}>0\) we have \[\frac{1}{\beta_{1}}+(k-1)=k+1\leq\frac{1}{\beta_{k}}\quad \Longrightarrow\quad\beta_{k}\leq\frac{1}{k+1}.\]2. When A is \(\mu\)-strongly monotone. Since A is \(\mu\)-strongly monotone, from (57) we have \[\mu\left\|x^{k+1}-x^{k}\right\|^{2}\leq\left\langle\tilde{\mathbf{A}}x^{k+1}- \tilde{\mathbf{A}}x^{k},x^{k+1}-x^{k}\right\rangle=\frac{\frac{\beta_{k}}{ \beta_{k+1}}-\beta_{k}-1}{1-\beta_{k}}\left\|\tilde{\mathbf{A}}x^{k+1}\right\|^ {2}.\] Define \(r_{k}=\frac{\left\|x^{k+1}-x^{k}\right\|^{2}}{\left\|\tilde{\mathbf{A}}x^{k+1} \right\|^{2}}\) for \(k=0,1,\ldots\). Dividing both sides by \(\left\|\tilde{\mathbf{A}}x^{k+1}\right\|^{2}\) and organizing, we have \[r_{k}\mu\leq\frac{\frac{\beta_{k}}{\beta_{k+1}}-\beta_{k}-1}{1-\beta_{k}} \quad\Longrightarrow\quad r_{k}\mu(1-\beta_{k})\leq\frac{\beta_{k}}{\beta_{k+ 1}}-\beta_{k}-1=\beta_{k}\left(\frac{1}{\beta_{k+1}}-1\right)-\beta_{k}-(1- \beta_{k}).\] Dividing both sides by \(\beta_{k}\) and reorganizing, we get a recursive inequality for \(\frac{1}{\beta_{k}}-1\) \[(1+r_{k}\mu)\left(\frac{1}{\beta_{k}}-1\right)+1\leq\frac{1}{\beta_{k+1}}-1.\] (58) We now prove an upper bound of \(\beta_{k}\) from above inequality. **Lemma I.4**.: _Suppose A be a \(\mu\)-strongly monotone operator. Let \(\beta_{k}\) be a sequence defined as Theorem 7.2 and let \(r_{k}=\frac{\left\|x^{k+1}-x^{k}\right\|^{2}}{\left\|\tilde{\mathbf{A}}x^{k+1} \right\|^{2}}\) for \(k=0,1,\ldots\). Then following holds for \(k=1,2,\ldots\)._

\[\beta_{k}\leq\frac{1}{\sum_{j=1}^{k-1}\prod_{i=j}^{k-1}\left(1+r_{i}\mu\right) +2}.\]

Proof.: First observe, the statement is equivalent to

\[\frac{1}{\beta_{k}}-1\geq\sum_{j=1}^{k-1}\prod_{i=j}^{k-1}\left(1+r_{i}\mu \right)+1.\]

The proof can be done by induction with (58).

When \(k=1\), recalling \(\beta_{1}=\frac{1}{2}\) from the proof of Lemma I.3, we can check the inequality is true.

Now suppose the inequality is true for \(k=m\). Then from (58) we have

\[\frac{1}{\beta_{m+1}}-1 \geq(1+r_{m}\mu)\left(\frac{1}{\beta_{m}}-1\right)+1\] \[\geq(1+r_{m}\mu)\left(\sum_{j=1}^{m-1}\prod_{i=j}^{m-1}\left(1+r_{ i}\mu\right)+1\right)+1\] \[=\left(\sum_{j=1}^{m-1}\prod_{i=j}^{m}\left(1+r_{i}\mu\right)+(1+ r_{m}\mu)\right)+1=\sum_{j=1}^{m}\prod_{i=j}^{m}\left(1+r_{i}\mu\right)+1.\]

Therefore, we get the desired result. 

Under the identification considered in Appendix I.2, the continuous counterpart of \(r_{k}\) is

\[r_{k}=\frac{\left\|x^{k+1}-x^{k}\right\|^{2}}{\left\|h\tilde{\mathbf{A}}x^{k+ 1}\right\|^{2}}=4\frac{\left\|x^{k+1}-x^{k}\right\|^{2}}{(2h)^{2}}\frac{1}{ \left\|\tilde{\mathbf{A}}x^{k+1}\right\|^{2}}\quad\xrightarrow{h\to 0+} \quad\frac{4\left\|\dot{X}(t)\right\|^{2}}{\left\|\tilde{\mathbf{A}}(X(t) )\right\|^{2}},\]

and is greater or equal to \(1\) by (54). We obtained an exponential convergence rate in continuous setup from this fact. In the same spirit, we can get an exponential convergence rate for discrete setup if there is a positive lower bound for \(r_{k}\), we provide it as a corollary of Lemma I.4.

**Corollary I.5**.: _Consider the setup of Lemma I.4. Suppose there is \(r\geq 0\) such that \(r_{k}\geq r\) for \(k=0,1,\ldots\). Then following is true for \(k=1,2,\ldots\)._

\[\beta_{k}\leq\frac{r\mu}{\left(1+r\mu\right)^{k}-1+r\mu}.\]

Proof.: From \(r_{k}\geq r\) we have

\[\sum_{j=1}^{k-1}\prod_{i=j}^{k-1}\left(1+r_{i}\mu\right)+2 \geq\sum_{j=1}^{k-1}\prod_{i=j}^{k-1}\left(1+r\mu\right)+2\] \[=\sum_{j=1}^{k-1}\left(1+r\mu\right)^{k-j}+2=\sum_{l=0}^{k-1} \left(1+r\mu\right)^{l}+1=\frac{\left(1+r\mu\right)^{k}-1+r\mu}{r\mu}.\]

Applying Lemma I.4, we get the desired result. 

We now show \(r_{k}\geq\frac{1}{1+L^{2}}\) holds when \(\mathbb{A}\) is furthermore \(L\)-Lipschitz continuous.

* When \(\mathbb{A}\) is \(\mu\)-strongly monotone and \(L\)-Lipschitz continuous. Recall from the proof of Lemma I.2, we know \[x^{k+1}-x^{k}=-\mathbb{A}x^{k+1}-\left(1-\beta_{k}\right)\mathbb{A}x^{k}- \beta_{k}(x^{k}-x^{0}).\] Taking inner product with \(\tilde{\mathbb{A}}x^{k}\) both sides we have \[\left\langle\tilde{\mathbb{A}}x^{k},x^{k+1}-x^{k}\right\rangle=-\left\langle \mathbb{A}x^{k+1},\tilde{\mathbb{A}}x^{k}\right\rangle-\tilde{\Phi}^{k}=- \left\langle\mathbb{A}x^{k+1},\tilde{\mathbb{A}}x^{k}\right\rangle.\] From above equality we can check \[\left\|x^{k+1}-x^{k}\right\|^{2}+\left\|\mathbb{A}x^{k+1}-\tilde{\mathbb{A}} x^{k}\right\|^{2}=\left\|\mathbb{A}x^{k+1}\right\|^{2}+\left\|x^{k+1}-x^{k}+ \tilde{\mathbb{A}}x^{k}\right\|^{2}\geq\left\|\mathbb{A}x^{k+1}\right\|^{2}.\] As \(\mathbb{A}\) is \(L\)-Lipschitz continuous, we have \[\left(1+L^{2}\right)\left\|x^{k+1}-x^{k}\right\|^{2}\geq\left\|x^{k+1}-x^{k} \right\|^{2}+\left\|\mathbb{A}x^{k+1}-\tilde{\mathbb{A}}x^{k}\right\|^{2}\geq \left\|\mathbb{A}x^{k+1}\right\|^{2}.\] Dividing both sides by \(\left(1+L^{2}\right)\left\|\mathbb{A}x^{k+1}\right\|^{2}\) we get a lowerbound for \(r_{k}\) \[r_{k}=\frac{\left\|x^{k+1}-x^{k}\right\|^{2}}{\left\|\mathbb{A}x^{k+1}\right\|^ {2}}\geq\frac{1}{1+L^{2}}.\] Applying Corollary I.5 we get the desired result \[\beta_{k}\leq\frac{\mu/(1+L^{2})}{\left(1+\mu/(1+L^{2})\right)^{k}-1+\mu/(1+L^ {2})}.\] Note if we take limit \(\mu\to 0^{+}\), with substitution \(\alpha=\frac{\mu}{1+L^{2}}\) we have \[\lim_{\mu\to 0^{+}}\frac{\mu/(1+L^{2})}{\left(1+\mu/(1+L^{2})\right)^{k}-1+\mu/(1+ L^{2})}=\frac{1}{\lim_{\alpha\to 0^{+}}\frac{1}{\alpha}\left(\left(1+\alpha \right)^{k}-1\right)+1}=\frac{1}{k+1},\] which is the bound for the monotone case.

#### I.3.1 If there is \(k\geq 0\) such that \(\tilde{\mathbb{A}}x^{k}=0\)

Suppose \(\tilde{\mathbb{A}}x^{k}=0\) for some \(k\). Let \(x^{N}\) be the very first iterate such that \(\tilde{\mathbb{A}}x^{N}=0\). Then from previous argument we know Theorem 7.2 is true for \(k<N\). Thus it remains to show the statements are true for \(k\geq N\).

From \(\tilde{\mathbb{A}}x^{N}=0\) we know \(y^{N-1}=x^{N}+\tilde{\mathbb{A}}x^{N}=x^{N}\). And since \(\tilde{\mathbb{A}}x^{N}=0\) implies \(\beta_{N}=0\), from the definition of the method we have \(y^{N}=2x^{N}-y^{N-1}=x^{N}\). Therefore

\[x^{N+1}=\mathbb{J}_{\mathbb{A}}y^{N}=\mathbb{J}_{\mathbb{A}}x^{N}=x^{N},\]

we conclude \(x^{k}=x^{N}\in\mathrm{ZerA}\) for all \(k\geq N\). Thus \(\beta_{k}=0,\left\|\tilde{\mathbb{A}}x^{k}\right\|=0\) for all \(k\geq N\), the Theorem 7.2 is trivially true for \(k\geq N\).

Details of experiment in Section 7.1

We solve a compressed sensing problem of Shi et al. [63] which is formulated as an \(\ell_{1}\)-regularized least-squared problem

\[\underset{x\in\mathbb{R}^{d}}{\text{minimize}} \frac{1}{n}\sum_{i=1}^{n}\left\{\frac{1}{2}\|A_{(i)}x-b_{i}\|^{2}+ \rho\|x\|_{1}\right\}.\]

We solve this problem in decentralized manner due to the problem setup where the network of local agents are as Figure 2 and each agents communicate only with their neighbors, the nodes connected to each agents by edge. We use Metropolis-Hastings matrix as our mixing matrix \(W\in\mathbb{R}^{n\times n}\) and apply PG-EXTRA. Let \(W_{i,j}\) denote \((i,j)\)-th entry of \(W\) and \(N_{i}\subseteq\{1,2,\ldots,n\}\) denote the index of the agents in the neighborhood of agent \(i\). Consider

\[\mathbf{x}_{i}^{k+1} =\mathrm{Prox}_{\alpha\rho\|\cdot\|_{1}}\left(\sum_{j\in N_{i}}W_ {i,j}\mathbf{x}_{j}^{k}-\alpha A_{(i)}^{\intercal}\left(A_{(i)}\mathbf{x}_{i }^{k}-b_{(i)}\right)-\mathbf{w}_{i}^{k}\right)\] \[\mathbf{w}_{i}^{k+1} =\mathbf{w}_{i}^{k}+\frac{1}{2}\left(x_{i}^{k}-\sum_{j\in N_{i}} W_{i,j}\mathbf{x}_{j}^{k}\right),\quad k=0,1,\ldots\] (PG-EXTRA)

to the problem above. Under suitable choice of parameters, PG-EXTRA can be seen as a fixed-point iteration of an averaged operator with respect to \(\left\|\cdot\right\|_{M}\)[74, Theorem 2], where the metric matrix \(M\) is defined as

\[M=\begin{bmatrix}(1/\alpha)I&U^{\intercal}\\ U&\alpha I\end{bmatrix},\]

and \(U\) is a symmetric definite matrix with \(U^{2}=\frac{1}{2}(I-W)\). That is, denoting \(\mathbf{x}^{k},\mathbf{w}^{k}\in\mathbb{R}^{d\times n}\) as the vertical stack of \(\mathbf{x}_{i}^{k}\)'s and \(\mathbf{w}_{i}^{k}\)'s respectively [58, Chapter 11.3], PG-EXTRA can be rewritten as \((\mathbf{x}^{k+1},\mathbf{w}^{k+1})=\mathbf{T}(\mathbf{x}^{k},\mathbf{w}^{k})\). Using this \(\mathbf{T}\), we proceed the experiment with the Halpern method

\[(\mathbf{x}^{k+1},\mathbf{w}^{k+1})=\beta_{k}(\mathbf{x}^{0},\mathbf{w}^{0})+ (1-\beta_{k})\,\mathbf{T}(\mathbf{x}^{k},\mathbf{w}^{k}).\]

When \(\mathbf{T}\) is an averaged operator, it is nonexpansive, we know \(\mathbb{T}=\mathbf{R}_{\mathsf{A}}=2\mathbf{J}_{\mathsf{A}}-\mathbf{I}\) for some maximal monotone operator \(\mathsf{A}\)[54, Lemma 2.1]. Considering the equivalence discussed in Lemma D.2, we see above Halpern method is equivalent to our presented algorithms of the form

\[x^{k+1} =\mathbf{J}_{\mathsf{A}}y^{k}\] \[y^{k+1} =(1-\beta_{k})\left(2x^{k+1}-y^{k}\right)+\beta_{k}x^{0},\]

by corresponding \(y^{k}=(\mathbf{x}^{k+1},\mathbf{w}^{k+1})\). Note the operator norm \(\left\|\tilde{\mathbf{A}}x^{k}\right\|_{M}^{2}\) can be calculated by considering below equation

\[\frac{1}{2}\left(\mathbb{T}y^{k-1}-y^{k-1}\right) =\mathbf{J}_{\mathsf{A}}y^{k-1}-y^{k-1}=x^{k}-\left(\tilde{\mathbf{ A}}x^{k}-x^{k}\right)=-\tilde{\mathbf{A}}x^{k}.\]

We use the anchor coefficients \(\beta_{k}=\frac{1}{k+1}\), \(\beta_{k}=\frac{\gamma}{k^{p}+\gamma}\) with \(p=1.5\), \(\gamma=2.0\) and the adaptive choice of \(\beta_{k}\) in Theorem 7.2 with \(M\)-norm. Note, in the experiment the adaptive coefficient is calculated by considering below equation

\[\frac{1}{2}\frac{\left\|\mathbf{T}y^{k-1}-y^{k-1}\right\|_{M}^{2 }}{\left\|\mathbb{T}y^{k-1}-y^{k-1}\right\|_{M}^{2}+\left\langle\mathbb{T}y^{k -1}-y^{k-1},y^{k-1}-x^{0}\right\rangle_{M}} =\frac{1}{2}\frac{4\left\|\tilde{\mathbf{A}}x^{k}\right\|_{M}^{2} +\left\langle-2\tilde{\mathbf{A}}x^{k},\tilde{\mathbf{A}}x^{k}+x^{k}-x^{0} \right\rangle_{M}}{\left\|\tilde{\mathbf{A}}x^{k}\right\|_{M}^{2}+\left\langle -2\tilde{\mathbf{A}}x^{k},\tilde{\mathbf{A}}x^{k}+x^{k}-x^{0}\right\rangle_{M}}\] \[=\frac{\left\|\tilde{\mathbf{A}}x^{k}\right\|_{M}^{2}}{-\left\langle \tilde{\mathbf{A}}x^{k},x^{k}-x^{0}\right\rangle_{M}+\left\|\tilde{\mathbf{A}}x ^{k}\right\|_{M}^{2}}.\]

We choose the dimension of signal \(d=100\), the number of agents \(n=20\), the number of measurement for each agent \(m_{i}=4\), \(\ell_{1}\)-regularization parameter \(\rho=0.01\), and algorithm parameter \(\alpha=0.01\).

## Appendix K Broader Impacts

Our work focuses on the theoretical aspects of convex optimization algorithms. There are no negative social impacts that we anticipate from our theoretical results.

## Appendix L Limitations

Our analysis concerns convex optimization. Although this assumption is standard in optimization theory, many functions that arise in machine learning practice are not convex.