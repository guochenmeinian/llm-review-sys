# Emergent and Predictable Memorization in Large Language Models

Stella Biderman\({}^{1,2}\), USVSN Sai Prashanth\({}^{2}\), Lintang Sutawika\({}^{2}\), Hailey Schoelkopf\({}^{2,3}\), Quentin Anthony\({}^{2,4}\), Shivanshu Purohit\({}^{5}\), and Edward Raff\({}^{1,6}\)

Correspondence to: stellabiderman@gmail.com

###### Abstract

Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for deploying language models. In particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization in the Pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at https://github.com/EleutherAI/pythia.

## 1 Introduction

Recent natural language processing (NLP) research in generative tasks has largely been driven by two findings: (1) the transformer architecture performs well (Vaswani et al., 2017; Devlin et al., 2018; Radford et al., 2019); and (2) increasing the scale of transformer architectures leads to improved performance (Brown et al., 2020; Chowdhery et al., 2022). In addition to these benefits, transformers are a general and multipurpose architecture that have achieved state-of-the-art results outside of NLP on diverse tasks such as text-to-image synthesis (Ramesh et al., 2022; Crowson et al., 2022; Rombach et al., 2022), code generation (Chen et al., 2021; Xu et al., 2022; Fried et al., 2022), and protein modeling (Jumper et al., 2021; Ahdritz et al., 2022). Despite their widespread success and increasing use, the learning dynamics of transformer models are poorly understood and research into how a given model learns and internally represents data has the potential to affect a broad range of high-impact applications. As these models become increasingly adopted, it is essential that we develop better and more reliable tools for measuring and controlling their behaviors.

### Memorization in Large Language Models

In particular, the demonstrated capacity and ability of these large language models to memorize data has become a significant concern (Carlini et al., 2019, 2021; Hu et al., 2022). The most obvious ramification is personal information or otherwise sensitive data being leaked to the public at large and extracted by a bad actor. This has motivated extensive research into mitigating memorizationby decreasing the _total quantity of memorized text_(Lee et al., 2021; Kandpal et al., 2022; Carlini et al., 2022; Hernandez et al., 2022). Although this is an admirable goal, we ultimately do not view it as an appropriate solution as **only some memorization is bad**. What's more, **some types of memorization are good**: we want large language models to memorize factual events and details to avoid "hallucinating" plausible-sounding but errant facts to unsuspecting users (Power et al., 2022; Cao et al., 2022; Tirumala et al., 2022).

Despite the extensive literature on memorization in trained models (Carlini et al., 2019, 2021; Hu et al., 2022), there are few tools to help practitioners either prevent memorization or detect it early in model training. Before the advent of transformer-based large language models, using differential privacy was popular (Abadi et al., 2016; McMahan et al., 2017; Popov et al., 2017). However, such methods have been observed to hurt performance during pretraining (Anil et al., 2021), and are therefore not popular among people who train large language models. In recent years, the bulk of interventionist work has focused on how removing duplicated samples from the training dataset can decrease memorization (Lee et al., 2021; Kandpal et al., 2022; Carlini et al., 2022; Hernandez et al., 2022). Importantly, these works focus on memorization _on average_ and cannot be relied on prevent memorization of specific training examples. Additionally, Biderman et al. (2023) shows that even when models are trained on deduplicated data they can still memorize substantial amounts of their training corpus.

Another approach to constraining model behavior is to intervene at inference time instead of at training time. Ippolito et al. (2022) introduce an interference-time intervention that has a 100% success rate at preventing _verbatim_ memorization, but they note both that their methodology is easily subverted, and does not fulfill the intention behind the term "memorization," which is the tendency of models to learn entire samples _during training_ without understanding their underlying meaning. We view test-time intervention as a promising avenue for future research, especially due to its success in other domains (Haghighatkhah et al., 2021; Ravfogel et al., 2022; Belrose et al., 2023, 20), but it is insufficent as a solution because some of the concern regarding PII and copyrighted text concerns _instrinsic properties of the model_ and not merely its output text. Although at first glance these techniques may seem more viable as downstream users can apply them to API models, most models offered via an API do not give sufficient access to the user to apply them. A missing component of the memorization literature is an investigation of _which specific data points are memorized_ in large language models.

### Scaling Laws

Due to the substantial cost of training large language models, it is highly desirable to be able to make predictions about model characteristics before they are actually trained. The literature on _scaling laws_(Kaplan et al., 2020; Henighan et al., 2020; Hernandez et al., 2021; Mikami et al., 2021; Hoffmann et al., 2022) has been successfully used to inform the decision-making of a variety of researchers at model training-time by allowing them to generalize the decisions made while investigating smaller models to inform the design of larger (sometimes by many orders of magnitude) models (Rae et al., 2021; Black et al., 2022; Scao et al., 2022; Chowdhery et al., 2022). While this work on scaling laws does extend to memorization (Carlini et al., 2022; Hernandez et al., 2022), how memorization evolves during a model's training process across a variety of scales has not been studied.

Scaling law studies are typically used to find a model with the best loss trainable within a given computational budget. However, the intuitions and experimental design around cheaply estimating the qualities of a final model can be applied to more than just training loss-we would like practitioners to be able to extrapolate other more complex properties of their final trained model prior to performing the costly training run.

In the case of memorization, because some data points are far more undesirable for a model to memorize, such as PII, it would be desirable for engineers to be able to predict whether a model will successfully avoid memorizing such harmful data, and make informed risk assessments and decisions about training prior to large compute expenses.

In our work, we study the creation of tools to predict the memorization of _specific data points_ prior to model training, rather than the macro-level corpus-wide statistics considered by prior work. We take a first step in this direction by proposing two strategies: 1) making predictions from a smaller model to a larger model; and 2) making predictions from a partially trained model of a given size to the fully 

[MISSING_PAGE_FAIL:3]

exact copy of the training data sequence. We term the accuracy of tokens in the continuation as the _memorization score_ of the sequence and call a sequence (\(k\)-)memorized or (\(k\)-)extractable if the memorization score is \(1\). Illustrative examples are shown in Table 1

\[score(M,N)=\frac{1}{N}\sum_{i}^{N}1(S_{M+i}=G_{M+i})\] (1)

In addition to \(k\)-extractability, we evaluate the _memorization score_, defined as the number of ordered matching tokens between the model's greedily generated sequence \(G_{32:64}\) and the dataset's true continuation \(S_{32:64}\) of a sequence \(S\in D\) on a given prompt. See Equation (1) for the formal equation, where \(N\) is the length of the true continuation and greedily generated sequence (\(32\) in our case), and \(M\) is the length of the prompt (also \(32\) in our case). A _memorized_ or _extractable_ sequence has a memorization score of \(1\).

Doing a forward pass on a large transformer is relatively expensive, costing about one third the cost of a full gradient update step. Consequently, feeding the full training data through the model for a forward pass would cost approximately one third the amount of compute that training the model did, and doing the full seven checkpoints that we do would come out to a larger compute budget than training the models themselves.

To ensure computational feasibility in our experiments, we choose \(k=32\) and evaluate the first \(64\) tokens from each sequence (we verify the robustness of this choice in Appendix A). Each sequence is a set of 2049 tokens, sampled from shuffled documents. These sequences are the input data points to the model during training.

### Threat Model

Throughout this paper, we assume that an engineer is looking to train a large language model with billions of parameters on a dataset, and that there is a small subset of the dataset that would be undesirable to have the model memorize. The engineer wishes to accurately predict whether this subset of the training data will be memorized by the fully-trained model, and would like to do so cheaply by expending a relatively small amount of compute on testing prior to the full training run. Following the literature on scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022), the cost in FLOPs of training a model is approximately

\[C=6\times\text{ [\# Params]}\times\text{ [\# Tokens]}\] (2)

and we assume the engineer has a computing budget that allows them to perform substantial testing before performing the full model training run. Because the engineer is the model trainer, We assume that the engineer has full access to the final and test models' training runs, including viewing the ordering and content of data points seen by their model and saving intermediate checkpoints as desired.

We focus on the _prediction_ of memorization, rather than simply filtering all data points that contain PII or are otherwise undesirable to memorize, for several reasons. First, although PII can be filtered from LLM training datasets via heuristic filters, or more advanced methods like classifier-based approaches, these filtering methods are not perfect, and may fail to remove certain instances of PII from the dataset (Li et al., 2023).

Additionally, there are also non-PII data points that are undesirable to memorize but desirable to train on, including copyrighted material. Some works suggest such data is essential to achieving high quality performance in certain domains (Longpre et al., 2023; Min et al., 2023). While the ethical and legal intricacies of permissible training data are outside the scope of this paper, the memorization of such content is a widely contested issue.1

Footnote 1: For instance, Github Copilot’s memorization of training data.

### Predicting Memorization

We can treat a smaller model's memorization of a sequence, or lack thereof, as a predictor for the memorization behavior of a larger model. Whether the interested model did memorize the sequence is the ground truth label, and the smaller model's behavior is the prediction.

For example, if a smaller model memorized a sequence and the larger model did not, we can think of this case as a false positive. Likewise, if both models memorized the sequence, then the smaller model's prediction was a true positive. Models not memorizing the target sequence are negative cases.

This "prediction" by the smaller model compared against the ground truth allows us to calculate classification metrics such as precision and recall. In this case, _precision_ tells us how many of the sequences memorized by the smaller model are also memorized by the larger model. _Recall_ conveys the percentage of sequences memorized by the larger model that are also memorized by the smaller model. The same framing can also be applied when analyzing across time--where we compare the memorized sequences at a certain intermediate checkpoint, and wish to predict which sequences will be memorized by the completed model.

As the engineer's sole concern is to avoid memorization on an undesirable subset (see Section 2.2), false negatives and false positives in predicting memorization have very different impacts on their workflow: a false positive (i.e. incorrectly predicting that a model will memorize the undesirable subset) results in throwing away a cheap model that could have been fruitfully continued to train the final model, while a false negative (i.e. incorrectly predicting that a model will not memorize the undesirable subset) results in the costly training of a full model that could leak sensitive samples from the training dataset. We are therefore primarily interested in assessing the _recall_ of the predictors and will tolerate a low precision if it comes with a high recall. We explore the tradeoffs in these costs in Section 3.

### Choice of Models and Datasets

At the time of writing, the only publicly-available pretrained LLM scaling suites trained on fully public training data are EleutherAI's GPT-Neo (Black et al., 2021; Wang and Komatsuzaki, 2021; Black et al., 2022) and Pythia models (Biderman et al., 2023), and Cerebras systems' Cerebras-GPT (Dey et al., 2023). All of these suites were trained on the Pile (Gao et al., 2020; Biderman et al., 2022). Additionally, we were able to obtain access to the ROATS dataset (McMillan-Major et al., 2022; Laurencon et al., 2022) that the BigScience Workshop's BLOOM (Scao et al., 2022) model was trained on. Of these model suites, we choose to use Pythia because **(a):** All Pythia models saw data samples in the exact same order, and that order is publicly available, **(b):** the training data differs slightly across the GPT-Neo models, **(c):** some BLOOM models only have three partially-trained checkpoints, and **(d):** Cerebras-GPT models don't provide partially-trained checkpoints. The OpenLlama, RedPajama, and StarCoder (Geng and Liu, 2023; Computer, 2023; Li et al., 2023) models have additionally been trained on public data, but were released after the completion of this work and do not release intermediate checkpoints.

We note that this limitation of eligible open models is not a drawback according to our threat model-we intend to create a novel procedure and tool for risk assessments usable by _model creators_, who have full control and access to information about their model training.

The computational cost of many of the experiments we run is quite large. Consequently, we are unable to evaluate every partially-trained model checkpoint in the Pythia suite.2 For most of our experiments, we choose to evaluate seven checkpoints spaced evenly throughout training. Specifically, we evaluate on checkpoints trained for \((23\cdot 10^{6})\), \((44\cdot 10^{6})\), \((65\cdot 10^{6})\), \((85\cdot 10^{6})\), \((105\cdot 10^{6})\), \((126\cdot 10^{6})\), and \((146\cdot 10^{6})\) sequences respectively, where these checkpoints approximately correspond to \(7\) checkpoints evenly spaced throughout training. We use the GPT-NeoX library (Andonian et al., 2021) that trained Pythia to efficiently implement our evaluation protocol.

Footnote 2: The cost of doing so would be comparable to the cost of training the models in the first place.

## 3 Memorization Across Scales

By far, the most common type of scaling law to study (and indeed, the origin of the term itself) is looking at how performance for very large models can be predicted based on performance of much smaller models. Fully-trained smaller model variants are independently useful as artifacts and can be applied in resource-constrained environments in place of larger models. Therefore, when projecting the characteristics of higher-compute model runs via scaling studies, training smaller model variants for this purpose is an actively desirable by-product, in contrast to the alternative of producing many shorter-training-duration checkpoints of the same single large architecture to extrapolate properties of a final full run. Therefore, the first question we seek to answer is: can an LLM's memorization behavior be predicted across model scales?

To evaluate how productive training small models can be for the purpose of predicting which data-points will be memorized by large models, we subset our data to the sequences with a memorization score of 1 (meaning all 32 target tokens were produced accurately by the smaller model). Then, we look at the correlations between each pair of fully-trained model sizes for which sequences are memorized. The results are shown in Figure 0(a).

We see a sharp decline in correlation between which sequences are memorized by smaller models and the 12B model as the gap between the model sizes increases. Unfortunately, we find that these low correlation scores cause the set of sequences memorized by small models to have very poor predictive power in terms of what sequences will be memorized by a larger model. We also measure precision and recall of fully-memorized sequences using each smaller model to predict the memorization of the 12B model as shown in Figure 0(b). Although the _precision_ is high for all models (see Section 2.2), we are more interested in achieving a high recall than a high precision. The recall is incredibly low across the board, with even the 1.4B parameter model only achieving a recall of \(0.554\) when trying to forecast the behavior of a model an order of magnitude larger.3

Footnote 3: Typical use-cases are to use smaller models to predict the behavior of models one to two orders of magnitude larger, see Rae et al. (2021), Scao et al. (2022), Chowdhery et al. (2022).

Our findings suggest that using smaller model runs to forecast the memorization of larger models is not accurate. Due to the low recall, practitioners cannot use a small model's lack of memorization of a given sequence as a strong guarantee that their larger model will not memorize that same sequence. We therefore do not recommend using smaller model runs for this task, and seek to provide a setup that grants practitioners more assurances and a better compute tradeoff.

## 4 Memorization Within Training

The second question we seek to answer is: can an LLM's memorization behavior be predicted ahead of time within a training run? We wish to determine if, by testing memorization behavior after partially completing a training run, an engineer can achieve a reliable signal about whether undesirable portions of the training data are memorized and if so to abort a training run early.

Our analysis in this section is motivated by the finding ofBiderman et al. (2023) that location within the training data does not impact whether a particular sequence is memorized. Therefore, we hypothesize that those concerned about the memorization of particular strings could move them early

Figure 1: A look at memorization across scales

during training. Thus practitioners would have an early warning signal for detecting memorization of undesired sequences. Unfortunately, we continue to find largely negative results, but hope that future research with better techniques for predicting memorization might vindicate this idea.

In Figure 2, we show a correlation heatmap between which sequences are memorized by different checkpoints of the same model. We only look at memorization of the first 23 million sequences, as that is the data that our least-trained model checkpoint has seen.

Our results on precision and recall (Table 2) largely mirror those of Section 3 in general trends. We see that the earliest intermediate checkpoints we test do not exhibit the high recall that is desirable, for instance with the 23M checkpoint of Pythia-12B underperforming the fully-trained Pythia-6.9B in recall.

We thus observe that using intermediate checkpoints of a model run to predict memorization is not a silver bullet--it is still the case that precision remains high throughout models, but recall is low for all predictors that use significantly less compute than the final model's cost. Therefore, in this setting as well, it is easier to guarantee a sequence _will_ be memorized through such extrapolations rather than not. Since the latter guarantee of non-memorization is more useful to engineers, our focus thus shifts to determining the compute-optimal model to train to gain a desired level of recall, in order to maximize predictive power amongst the options we explore.

## 5 Scaling Laws

Having established the empirical results in the previous section, we now examine our results through the lens of computational efficiency and scaling laws, where the aim is to achieve the most reliable results for the least expense. To achieve this, we examine how well models of various sizes and number of training steps predict which sequences will be memorized **by the fully trained 12B

\begin{table}

\end{table}
Table 2: Precision and recall for predicting which sequences would be memorized by the fully-trained model from a partially-trained checkpoint. We observe consistently high precision, but only achieve high recall after significant compute has been expended (later intermediate checkpoints).

Figure 2: Heat maps visualizing the correlation between which sequences are memorized by different checkpoints. Plots for other Pythia models can be found in Figure 10.

parameter model**. This is in notable contrast to Section 4, where partially-trained models are only compared to fully-trained models of the same size. As a visual aid, models with the same size are colored the same.

### Unusual Scaling

In the overwhelming majority of prior work on scaling laws (Brown et al., 2020; Kaplan et al., 2020; Pu et al., 2021; Mikami et al., 2021; Rae et al., 2021; Black et al., 2022; Scao et al., 2022; Chowdhery et al., 2022), including scaling studies targeting memorization (Carlini et al., 2022; Hernandez et al., 2022; Tirumala et al., 2022), plots of quantities of interest vs. compute are linear on a log or log-log plot. We find that this is not the case in our setup for both precision and recall.

The scaling data for precision is extremely anomalous. Not only are the plots non-linear, we find that the behavior of the 12B partially trained model is extremely out-of-line with the behavior of smaller models. The results for recall are less anomalous, lacking the divergent behavior for the 12B model, but nevertheless do not accord with what the scaling laws literature generally expects.

Despite the fact that there is a high-level pattern in the scaling laws curve for recall, a careful look at the data indicates unusual behavior. In the low-compute regimes, which are of most interest to engineers looking to minimize the cost of creating a prediction of the behavior of large models before they are trained, we see a consistent pattern of larger models being better than smaller models for a fixed compute budget. However, as the amount of compute expended scales, this is no longer the case. Starting at around \(1\%\) the budget of the fully trained model, equicompute models perform the same regardless of the number of parameters. Starting at around \(10\%\) the budget of the fully trained model, the _smallest_ model trained for this compute budget becomes the best predictor of memorization in the fully trained model.

### Emergent Memorization

We also see evidence of "emergent" or "semi-emergent" behavior as model scale increases. In the literature on emergent behavior in large language models (Srivastava et al., 2022; Ganguli et al., 2022; Wei et al., 2022; Caballero et al., 2022), the term refers to when a large model's performance on a task is substantially different from the extrapolated value of curves fit to the behavior of smaller models. Often, but not always, this occurs when performance goes from near-zero to meaningful. While our situation is not totally analogous, one can similarly consider "emergent memorization" to occur when data is memorized by large models which cannot be predicted based on the memorization behavior of smaller models. Since, by definition, emergent behavior implies that smaller-scale model behaviors are qualitatively different to those of larger models, this can pose challenges for traditional scaling laws or for extrapolating model behavior to models orders of magnitude larger. As a result, we suggest that this is an important area for further study, including expanding the scope of our work to models larger than 12B parameters.

Figure 3: Scaling curves for Pythia models.

### Takeaways for Engineers

As discussed in Section 2.2, the primary point of interest to engineers is to predict the behavior of a large language model before it is trained. Such predictions should be grounded in low-cost regimes such as the behavior of trained "test" models that are at least an order of magnitude smaller than the target model. We find that for cases where high recall is required, our scaling law defines what size of model should be trained at a given compute budget. In compute regimes less than two orders of magnitude below the final training run's size, we find that when holding the compute budget fixed it is desirable to use the "smallest" model trained on no more the final run's total token count as possible, and to frontload the data seen by this smaller model with sequences whose memorization would be undesirable in order to predict whether these sequences would be memorized by a final model.

### Takeaways for Decision-Making

The experimental procedures we present in this paper are a step toward practical risk assessment for large-scale training runs. We hope that future work both makes such assessments much cheaper and accurate, and that more practitioners adopt such measures and choose to be up-front about which mitigations they apply when training large neural models.

## 6 Limitations and Future Work

Our work constitutes the first steps towards developing a way to predict what data will be memorized by a large language model before that model is trained, but has several limitations and opens opportunities for exciting future work. The most important of these are:

Are we measuring the correct thing?The definition of memorization we use is derived from what is currently popular in the academic literature, but it is unclear if it is the best definition to use. We believe \(k\)-extractibility to be well-grounded in privacy concerns of language models, but other metrics such as memorization score may be more natural when studying the _dynamics_ of memorization in training.

Does this generalize to other models?We report our experiments on the Pythia suite, because it was the only current language modeling suite suitable for such work at the time of our research. However, this leaves open many questions about whether our results generalize to models trained with different hyperparameters or different data. We validate our experiments with replications on the deduplicated Pythia models and different hyperparameters in Appendix A and B, but no other model suite is suitable for replicating this analysis. This gap points to the need for more reproducible, public dataset model releases to advance research on memorization.

What about the data contents?Our work does not take the actual content of the training data into account at any point in time: we are looking exclusively at predicting memorization based on whether other cheaper models memorize the content. Future work looking into whether there are properties of the training text that predict memorization of that text could be quite illuminating.

## 7 Conclusion

We propose a novel setting for forecasting model memorization prior to train-time, while minimizing the compute required to make this forecast. We present analyses on the two most natural setups for extrapolation: using **fully-trained small models** and **partially-trained checkpoints of the final model** to compare and predict memorization of the final large model. We find that using much smaller models for this task is not viable, and that partial checkpoints of an existing model are similarly ineffective predictors of final memorization behavior when adjusted for cost. We derive a scaling law to find the optimal equi-compute predictor of non-memorization and are able to provide recommendations based on this law. We hope that our focus on prediction of the memorization of specific strings will be compelling for future study, and that our analyses inform deep learning practitioners on methods to understand and reduce memorization while training large language models.

Corrections

Due to an error in our analysis code, an earlier draft of this paper reported a substantially higher recall in Table 2. This draft of the paper features corrected numbers in that table and has adjusted the conclusions and discussion accordingly.

This paper was made better by conversations with and feedback from many individuals not on the authorship list. Following EleutherAI's open science values (Phang et al., 2022), we shared early drafts of these results with the EleutherAI Interpretability Reading Group as well as the Discord server at large, garnering feedback from many people. We would like to acknowledge Nicholas Turner, Gurkenglas, and Amaru Cuba Gyllenste for identifying errors in our results and questioning our assumptions; Kyle O'Brien and Aviya Skowron for copy-editing; and Herbie Bradley, Nicholas Carlini, Katherine Lee, Naomi Saphra, and the EleutherAI Interpretability Reading Group for their thoughts, feedback, and advice.

We are grateful to Stability AI for providing the compute required to carry out our experiments.

Our work builds on top of the work of many teams at EleutherAI and within the broader open source community writ large. We'd especially like to recognize the GPT-NeoX (Andonian et al., 2021) team at EleutherAI whose library we used to measure memorization and the US AISI maintainers of the Hugging Face Hub whose infrastructure we used to host our data.

## References

* Abadi et al. (2016) Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _Proceedings of the 2016 ACM SIGSAC conference on computer and communications security_, pages 308-318, 2016.
* Abdritz et al. (2022) Gustaf Abdritz, Nazim Bouatta, Sachin Kadyan, Qinghui Xia, William Gerecke, Timothy J O'Donnell, Daniel Berenberg, Ian Fisk, Niccola Zanichelli, Bo Zhang, et al. Openfold: Retraining alphafold2 yields new insights into its learning mechanisms and capacity for generalization. _bioRxiv_, 2022.
* Andonian et al. (2021) Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang Phil, and Samuel Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch, 8 2021. URL https://www.github.com/eleutherai/gpt-neoax.
* Anil et al. (2021) Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Large-scale differentially private bert. _arXiv preprint arXiv:2108.01624_, 2021.
* Belrose et al. (2023a) Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens. _arXiv preprint arXiv:2303.08112_, 2023a.
* Belrose et al. (2023b) Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman. Leace: Perfect linear concept erasure in closed form. _arXiv preprint arXiv:2306.03819_, 2023b.
* Biderman et al. (2022) Stella Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile. _arXiv preprint arXiv:2201.07311_, 2022.
* Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling. _arXiv preprint arXiv:2304.01373_, 2023.
* Black et al. (2021) Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: large scale autoregressive language modeling with mesh-tensorflow. _GitHub_, 2021. URL https://www.github.com/eleutherai/gpt-neo.

Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. In _Proceedings of BigScience Episode #5-Workshop on Challenges & Perspectives in Creating Large Language Models_, pages 95-136, 2022.
* Brown et al. [2020] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020. URL https://arxiv.org/abs/2005.14165.
* Caballero et al. [2022] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. _arXiv preprint arXiv:2210.14891_, 2022.
* Cao et al. [2022] Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu. Benign overfitting in two-layer convolutional neural networks. _Advances in neural information processing systems_, 35:25237-25250, 2022.
* Carlini et al. [2019] Nicholas Carlini, Chang Liu, Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In _28th USENIX Security Symposium (USENIX Security 19)_, pages 267-284, 2019.
* Carlini et al. [2021] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In _30th USENIX Security Symposium (USENIX Security 21)_, pages 2633-2650, 2021.
* Carlini et al. [2022] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. _arXiv preprint arXiv:2202.07646_, 2022.
* Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.
* Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* Computer [2023] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, April 2023. URL https://github.com/togethercomputer/RedPajama-Data.
* Crowson et al. [2022] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Edward Raff. Vogan-clip: Open domain image generation and editing with natural language guidance. _arXiv preprint arXiv:2204.08583_, 2022.
* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. _CoRR_, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805.
* Dey et al. [2023] Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. Cerebras-gpt: Open compute-optimal language models trained on the cerebral s wafer-scale cluster, 2023.
* Fried et al. [2022] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Inocoder: A generative model for code infilling and synthesis. _arXiv preprint arXiv:2204.05999_, 2022.
* Ganguli et al. [2022] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large generative models. In _2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 1747-1764, 2022.
* Goyal et al. [2021]Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: an 800GB dataset of diverse text for language modeling. _arXiv preprint arXiv:2101.00027_, 2020. URL https://arxiv.org/abs/2101.00027.
* Geng and Liu (2023) Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https://github.com/openlm-research/open_llama.
* Haghighatkhah et al. (2021) Pantea Haghighatkhah, Wouter Meulemans, Bettina Speckman, Jerome Urhausen, and Kevin Verbeek. Obstructing classification via projection. _arXiv preprint arXiv:2105.09047_, 2021.
* Henighan et al. (2020) Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling. _arXiv preprint arXiv:2010.14701_, 2020. URL https://arxiv.org/abs/2010.14701.
* Hernandez et al. (2021) Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. _arXiv preprint arXiv:2102.01293_, 2021. URL https://arxiv.org/abs/2102.01293.
* Hernandez et al. (2022) Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Scaling laws and interpretability of learning from repeated data. _arXiv preprint arXiv:2205.10487_, 2022.
* Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* Hu et al. (2022) Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S Yu, and Xuyun Zhang. Membership inference attacks on machine learning: A survey. _ACM Computing Surveys (CSUR)_, 54(11s):1-37, 2022.
* Ippolito et al. (2022) Daphne Ippolito, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A Choquette-Choo, and Nicholas Carlini. Preventing verbatim memorization in language models gives a false sense of privacy. _arXiv preprint arXiv:2210.17546_, 2022.
* Jumper et al. (2021) John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* Kandpal et al. (2022) Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models. _arXiv preprint arXiv:2202.06539_, 2022.
* Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020. URL https://arxiv.org/abs/2001.08361.
* Laurencon et al. (2022) Hugo Laurencon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonzalez Ponferrada, Huu Nguyen, et al. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* Lee et al. (2021) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. _arXiv preprint arXiv:2107.06499_, 2021.
* Li et al. (2021) Raymond Li, Loughna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muthasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas,Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023.
* Longpre et al. (2023) Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, and Daphne Ippolito. A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity, 2023.
* McMahan et al. (2017) H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. _arXiv preprint arXiv:1710.06963_, 2017.
* McMillan-Major et al. (2022) Angelina McMillan-Major, Zaid Alyafeai, Stella Biderman, Kimbo Chen, Francesco De Toni, Gerard Dupont, Hady Elsahar, Chris Emezue, Alham Fikri Aji, Suzana Ilic, et al. Documenting geographically and contextually diverse data sources: The bigscience catalogue of language data and resources. _arXiv preprint arXiv:2201.10066_, 2022.
* Mikami et al. (2021) Hiroaki Mikami, Kenji Fukumizu, Shogo Murai, Shuji Suzuki, Yuta Kikuchi, Taiji Suzuki, Shin-ichi Maeda, and Kohei Hayashi. A scaling law for synthetic-to-real transfer: How much is your pre-training effective? _arXiv preprint arXiv:2108.11018_, 2021. URL https://arxiv.org/abs/2108.11018.
* Min et al. (2022) Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. Silo language models: Isolating legal risk in a nonparametric datastore, 2023.
* Phang et al. (2022) Jason Phang, Herbie Bradley, Leo Gao, Louis J Castricato, and Stella Biderman. Eleutherai: Going beyond" open science" to "science in the open". In _NeurIPS Workshop on Broadening Research Collaborations_, 2022.
* Popov et al. (2017) Vadim Popov, Mikhail Kudinov, Irina Piontkovskaya, Petr Vytovtov, and Alex Nevidomsky. Differentially private distributed learning for language modeling tasks. _arXiv preprint arXiv:1712.07473_, 2017.
* Power et al. (2022) Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. _arXiv preprint arXiv:2201.02177_, 2022.
* Pu et al. (2021) Jie Pu, Yuguang Yang, Ruirui Li, Oguz Elibol, and Jasha Droppo. Scaling effect of self-supervised speech models. _Proc. Interspeech 2021_, pages 1084-1088, 2021.
* Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. _OpenAI Blog_, 2019.
* Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _arXiv preprint arXiv:2112.11446_, 2021.
* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* Ravfogel et al. (2022) Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, and Ryan Cotterell. Adversarial concept erasure in kernel space. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 6034-6055, 2022.
* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* Scao et al. (2022a) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022a.
* Schlicht et al. (2021)* Le Scao et al. (2022b) Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella Bideman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et al. What language model to train if you have one million gpu hours? _arXiv preprint arXiv:2210.15424_, 2022b.
* Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_, 2022.
* Tirumala et al. (2022a) K. N. Bharadwaj Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. _ArXiv_, abs/2205.10770, 2022a.
* Tirumala et al. (2022b) Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. _Advances in Neural Information Processing Systems_, 35:38274-38290, 2022b.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Wang & Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. GPT-J-6B: a 6 billion parameter autoregressive language model, 2021.
* Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.
* Xu et al. (2022) Frank F Xu, Uri Alon, Graham Neubig, and Vincent J Hellendoorn. A systematic evaluation of large language models of code. _arXiv preprint arXiv:2202.13169_, 2022.

[MISSING_PAGE_FAIL:15]

Robustness to Deduplication

In order to further confirm the validity of our analyses, we run our experiments on the Pythia (deduplicated) suite, which was trained on a deduplicated copy of the Pile [Gao et al., 2020] for 1.5 epochs. In keeping with the literature on deduplication and its connection with memorization [Lee et al., 2021, Kandpal et al., 2022], we observe that memorization is decreased for this set of models, albeit slightly (Figure 7). This may be due to the 1.5 epoch training setup we adopt offsetting the benefits of deduplicated data.

Figure 8: Inter-checkpoint correlations for memorization of Pythia-12B and Pythia-12B-deduped, respectively. Between the two sets of models, we observe extremely similar (though not fully identical) patterns in the correlations of their checkpoints.

Figure 7: Fraction of all sequences memorized by both Pythia model suites. For example, Pythia-12B has memorized 1.62% of sequences. We can observe the deduplicated models memorize less of their dataset than their non-deduplicated counterparts.

We replicate our analyses on the deduplicated models and find the same trends hold for our experiments on the Pythia-deduplicated models as do for the regular Pythia suite. Heatmap correlation results show the same conclusions (Figure 8), and we replicate precision and recall results from Figure 1b and Table 2 but on deduplicated models in Figure 9 and Table 3. We therefore believe our results to be reasonably robust across hyperparameters and engineer train-time choices, but hope that future work may replicate some of our findings on entirely distinct corpora.

\begin{table}
\begin{tabular}{l c c} Seq Num & Precision & Recall \\ \hline \(23\cdot 10^{6}\) & \(0.920\) & \(0.523\) \\ \(44\cdot 10^{6}\) & \(0.917\) & \(0.595\) \\ \(65\cdot 10^{6}\) & \(0.915\) & \(0.658\) \\ \(85\cdot 10^{6}\) & \(0.915\) & \(0.724\) \\ \(105\cdot 10^{6}\) & \(0.922\) & \(0.820\) \\ \(126\cdot 10^{6}\) & \(0.949\) & \(0.920\) \\ \(146\cdot 10^{6}\) & — & — \\ \hline \end{tabular}
\end{table}
Table 3: Precision and recall for predicting which sequences would be memorized by the fully-trained model from a partially-trained checkpoint, for Pythia-12B-deduped. The trends observed here match Table 2.

Figure 9: Precision and recall when using each model to predict which sequences would be memorized by the 12B parameter model. Replicates Figure 1b.

Figure 10: Heat maps visualizing the correlations between which sequences are memorized by different checkpoints.

Author Contributions

Stella BidermanConceived, organized, and lead the project, and wrote the paper.

USVSN Sai PrashanthImplemented and carried out the evaluation of memorization of pretraining strings.

Lintang SutawikaAnalyzed and interpreted the precision and recall results and plotted data.

Hailey SchoelkopfCarried out the evaluation of memorization of pretraining strings, performed the robustness evaluation, found and fixed several bugs in our code, and wrote the paper.

Quentin AnthonyAnalyzed and interpreted the results and wrote the paper.

Shivanshu PurohitOptimized the implementation and assisted with carrying out the evaluation of memorization of pretraining strings.

Edward RaffDesigned the experiments, interpreted the results and wrote the paper.