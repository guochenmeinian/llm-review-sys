# UIR-LoRA: Achieving Universal Image Restoration through Multiple Low-Rank Adaptation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Existing unified methods typically treat multi-degradation image restoration as a multi-task learning problem. Despite performing effectively compared to single degradation restoration methods, they overlook the utilization of commonalities and specificities within multi-task restoration, thereby impeding the model's performance. Inspired by the success of deep generative models and fine-tuning techniques, we proposed a universal image restoration framework based on multiple low-rank adapters (LoRA) from multi-domain transfer learning. Our framework leverages the pre-trained generative model as the shared component for multi-degradation restoration and transfers it to specific degradation image restoration tasks using low-rank adaptation. Additionally, we introduce a LoRA composing strategy based on the degradation similarity, which adaptively combines trained LoRAs and enables our model to be applicable for mixed degradation restoration. Extensive experiments on multiple and mixed degradations demonstrate that the proposed universal image restoration method not only achieves higher fidelity and perceptual image quality but also has better generalization ability than other unified image restoration models.

## 1 Introduction

In the wild, a range of distortions commonly appear in captured images, including noise[56], blur[14, 47, 6], low light[58, 22, 8], and various weather degradations[15, 51, 54, 45]. As a fundamental task in low-level vision, image restoration aims to eliminate these distortions and recover sharp details and original scene information from corrupted images. With the assistance of deep learning, an abundance of restoration approaches [56, 3, 54, 2, 16, 14, 53] have made significant progress in eliminating single degradation from images. However, these approaches typically require additional training from scratch on specific image pairs in multi-degraded scenarios, which leads to inconvenience in usage and limited generalization ability.

For simplicity and practicality, some existing works [15, 31, 55]consider training a unified model (also called all-in-one model) to handle multiple degradations as multi-task learning. These studies primarily explore how to discern degradation from the image and integrate it into the restoration network. Nevertheless, these methods share all parameters across different degradations, resulting in gradient conflicts [40, 52] that hinder further improvement of unified models' performance.

Digging deeper, the underlying issue lies in that the similarities among different image restoration tasks and the inherent specificity of each degradation are not well considered and utilized in the training. This limitation drives us to seek solutions for multi-degradation restoration by leveraging both commonalities and specificities.

Inspired by the successes of deep generative models[37; 36; 35] and fine-tuning techniques[11; 10; 4], we propose addressing the aforementioned issue from the perspective of multi-domain transfer learning, as presented in Figure 1. The pre-trained generative model exhibits powerful capabilities, implying rich prior knowledge of clear image distribution \(p(x)\), which is exactly what is needed for image restoration. Since image prior \(p(x)\) is degradation-agnostic and applicable to all types of degraded images, the pre-trained generative model is an excellent candidate for serving as the shared component for multiple degradation restoration. To model the transition from the clean image domain to different degraded image domains, minimal specific parameters are required to fine-tune the pre-trained model for each degradation restoration task. This approach not only isolates conflicts between each degradation task but also ensures efficiency and performance during training.

Following the idea of multi-domain transfer learning, we proposed a universal image restoration framework based on multiple low-rank adaptations, named UIR-LoRA. In our framework, the pre-trained SD-turbo [39] serves as the shared fundamental model for multiple degradation restoration tasks due to its powerful one-step generation capability and extensive image priors. Subsequently, we incorporate the low-rank adaptation (LoRA) technique [11] to fine-tune the base model for each specific image restoration task. This involves augmenting low-dimensional parameter matrices on selected layers within the base model, ensuring efficient fine-tuning while maintaining independence between LoRAs for each specific degradation. Additionally, we propose a LoRA composition strategy based on degradation similarity. We calculate the similarity between degradation features extracted from degraded images and existing degradation types, utilizing it as weights for combining different LoRA experts. This strategy enables our method to be applicable for restoring mixed degradation images. Moreover, we conducted extensive experiments and compared our approach with several existing unified image restoration methods. The experimental results demonstrate that our method achieves superior performance in the restoration of various degradations and mixed degradations. Not only does our approach outperform existing methods in terms of distortion and perceptual metrics, but it also exhibits significant improvements in visual quality.

Our contributions can be summarized as follows:

* From the perspective of multi-domain transfer learning, we propose a novel universal image restoration framework based on multiple low-rank adaptations. It leverages the pre-trained generative model as the shared component for multi-degradation restoration and employs distinct LoRAs for multiple degradations to efficiently transfer to specific degradation restoration tasks.
* We introduce a LoRAs composition strategy based on the degradation similarity, which adaptively combines trained LoRAs and enables our model to be applicable for mixed degradation restoration.
* Through extensive experiments on multiple and mixed degradations, we demonstrate that the proposed universal image restoration method not only achieves higher fidelity and perceptual image quality but also has better generalization ability than other unified models.

Figure 1: Motivation of our work. A pre-trained generative model serves as the shared component and minimal parameters are added to model the specificity of each degradation restoration task.

Related Work

### Image Restoration

**Specific Degradation Restoration.** According to degradation type, image restoration tasks are categorized into different groups, including denoising, deblurring, inpainting, draining _etc_. Most existing image restoration methods [2; 53; 16; 56; 5; 14] mainly address the issue with a single degradation. Traditional approaches [27; 28; 7] have proposed image priors. While these priors can be applied to different degraded images, their capability is limited. Due to the remarkable capability of the deep neural network (DNN), numerous DNN-based methods [2; 53; 16] have been proposed to tackle image restoration tasks. While DNN-based methods have made significant progress, they struggle with multiple degradations and mixed degradations, since they typically require retraining from scratch on data with the same degradation.

**Universal degradation restoration.** Increasing attention is currently focused on developing a unified model to process multiple degradations. For example, AirNet[15] explores the degradation representation in latent space for separating them in the restoration network. PromptIR[31] utilizes a prompt block to extract the degradation-related features to improve the performance. Daclip-IR[20] introduces the clip-based encoder to distinguish the type of degradation and extract the semantics information from distorted images and embed them into a diffusion model to generate high-quality images. Despite the advancements, these unified models still have limitations. They also require retraining all parameters when unseen degradations arrive and have limited performance due to the gradient conflict.

### Low-Rank Adaptation

LoRA [11] is proposed to fine-tune large models by freezing the pre-trained weights and introducing trainable low-rank matrices. This fine-tuning method leverages the property of "intrinsic dimension" within neural networks, lowering the rank of additional matrices and making the re-training process efficient. Concretely, given a weight matrices \(W\in\mathbb{R}^{n\times m}\) in pre-trained model \(\theta_{p}\), two trainable matrices \(B\in\mathbb{R}^{n\times r}\) and \(A\in\mathbb{R}^{r\times m}\) are inserted into the layer to represent the LoRA \(\Delta W=BA\), where \(r\) is the rank and satisfy \(r\ll mim(n,m)\), the updated weights \(W^{\prime}\)are calculated by

\[W^{\prime}=W+\Delta W.\] (1)

By applying LoRA in pre-trained models, numerous image generation methods [29; 13], show superior performance in the field of image style and semantics concept transferring. Additionally, fine-tuning methods like ControlNet [57], T2i-adapter [24] are also commonly employed in large-scale pre-trained generative models such as Stable Diffusion [37], SDXL [30], and Imagen [38].

### Mixture of Experts

Mixture of Experts (MoE) [41; 49; 48] is an effective approach to scale up neural network capacity to improve performance. Specifically, MoE integrates multiple feed-forward networks into a transformer block, where each feed-forward network is regarded as an expert. A gating function is introduced to model the probability distribution across all experts in the MoE layer. The gating function is trainable and determines the activation of specific experts within the MoE layer based on top-k values. Broadly speaking, our framework aligns with the concept of MoE. However, unlike traditional MoE layers, we employ the more efficient LoRA as experts in selected frozen layers and utilize a degradation-aware router across all selected layers to uniformly activate experts, reducing learning complexity and avoiding conflicts among different image restoration tasks on experts.

## 3 Methodology

### Problem Definition

This paper seeks to develop a novel universal image restoration framework capable of handling diverse forms of image degradation in the wild by fine-tuning the pre-trained generative model. Consider a set of \(T\) image restoration tasks \(D=\{D^{k}\}_{k=1}^{T}\), where \(D^{k}=\{(x_{i},y_{i})\}_{i=1}^{n_{k}}\) is the training dataset containing \(n_{k}\) images pairs of the \(k\)-th image degradation task. Within the set of tasks \(D\)each task \(D^{k}\) only has a specific type of image degradation, with no intersection between any two tasks. Given a pre-trained generative model \(\theta_{p}\) with frozen parameters, our objective is to learn a set of composite \(\{\theta_{k}\}_{k=1}^{T}\) to construct a unified model \(f_{\theta}\) that performs well on multi-degradation restoration and mixed degradation restoration by transferring learning, where \(\theta=\theta_{p}+\sum_{k=1}^{T}s_{k}\theta_{k}\) and \(s_{k}\) represents the composite weight for \(\theta_{k}\). The trainable \(\{\theta_{k}\}_{k=1}^{T}\) can be optimized through minimizing the overall image reconstruction loss:

\[L=E_{(x,y)\in D}l(f_{\theta}(x),y).\] (2)

We will present how to design and optimize the trainable \(\{\theta_{k}\}_{k=1}^{T}\) and construct the composite weights \(s\) in the next sections.

### Overview of Universal Framework

Inspired by transferring learning, we introduce a novel universal image restoration framework based on multiple low-rank adaptations, named UIR-LoRA. Referring to Figure 2, our framework consists of two main components, namely degradation-aware router and universal image restorer, respectively. The degradation-aware router first extracts the degradation feature from input degraded images and then calculates the similarity probabilities \(s\) with existing degradations in the latent space of CLIP model [35, 20]. For the universal image restorer, it comprises a pre-trained generative model \(\theta_{p}\) and \(T\) trainable LoRAs \(\{\theta_{k}\}_{k=1}^{T}\). This design is primarily motivated by two considerations: firstly, the pre-trained generative model contains extensive image priors that are degradation-agnostic and can be shared across all types of degraded images. Secondly, each LoRA can independently capture specific characteristics of each degradation without gradient conflicts. In practice, the pre-trained SD-turbo [39] is employed as the frozen base model in our framework and each LoRA \(\theta_{k}\) serves as an expert responsible for transferring the frozen base model to a specific degradation restoration task \(D^{k}\). By adjusting the value of Top-K parameter within the degradation-aware router, different combinations of LoRAs in the universal image restorer can be activated, enabling the removal of a specific degradation and mixed degradation in multi-degraded scenarios.

Figure 2: Overview of UIR-LoRA. UIR-LoRA consists of two components: a degradation-aware router and a universal image restorer. The router calculates degradation similarity in the latent space of CLIP, while the restorer utilizes the similarity provided by the router to combine LoRAs and frozen base model and restore images with multiple or mixed degradations.

### Degradation-Aware Router

The Degradation-Aware Router is designed to provide the restorer with weights for LoRA combination based on degradation confidence. Following Daclip-ir [20], we utilized the pre-trained image encoder in CLIP [35] to obtain the degradation vector \(d\in\mathbb{R}^{1\times z}\) from the input degraded image \(x\), where \(z\) is degradation length in latent space. Differing from Daclip-ir [20], we use the degradation vector and existing degradations to calculate the similarity, instead of directly embedding the degradation vector into the restoration network in Daclip-ir [20]. The existing degradations refer to the vocabulary bank of diverse degradation types that we introduce in the router, such as "noisy", "blurry" and "shadowed". This vocabulary bank is highly compact and flexible when adding new degradation types. Similarly, by applying the text encoder of CLIP [35], the vocabulary bank can be encoded into the degradation bank \(B\in\mathbb{R}^{z\times T}\) in the latent space. As presented in Figure 2, the original degradation similarity \(s_{o}\in\mathbb{R}^{1\times T}\) is calculated by:

\[s_{o}=dB.\] (3)

Building upon the original similarity, we adopt a more flexible and controllable Top-K strategy to modify \(s_{o}\). Specifically, we select the Top-K largest values from the original similarity \(s_{o}\), and normalize them to reallocate the weights for LoRAs. The reallocation process can be formulated as :

\[s=\frac{s_{o}\cdot M_{K}}{\sum s_{o}\cdot M_{K}},\] (4)

where \(M_{K}\) represents a binary mask with the same length as \(s_{o}\), where it is 1 when the corresponding value in \(s_{o}\) is among the Top-K, otherwise it is 0. With a smaller value of \(K\), the restorer activates fewer LoRAs, reducing its computational load. For instance, with \(K=1\), only the most similar LoRA is activated and it yields effective results when \(s\) is accurate, but performance noticeably declines with inaccurate \(s\). Conversely, as \(K\) increases, the restorer exhibits higher tolerance to \(s\) and the combination of LoRAs allows it to handle mixed degradation.

### Universal Image Restorer

Our universal image restorer consists of a pre-trained generative model \(\theta_{p}\) and a set of LoRAs \(\{\theta_{k}\}_{k=1}^{T}\). As illustrated in Figure 2, our universal image restorer takes the degraded image \(x\) and similarity \(s\) predicted by the degradation-aware router as inputs. It then activates relevant LoRAs based on \(s\) to recover the degraded image along with the frozen base model. Since one of our objectives is to ensure that each LoRA serves as an expert in processing a specific degradation, the number of LoRAs in the restorer aligns with the number of degradation types, \(T\). In practice, we select multiple layers from the base model, For a selected layer \(W\) of the pre-trained base model, a sequence of trainable matrices \(\{\Delta W_{k}\}_{k=1}^{T}\) are added into this layer, and the parameters of all chosen layers \(L\) form a complete LoRA \(\theta_{k}=\{\Delta W_{k}^{j}\}_{j\in L}\). As previously explained, each LoRA is a unique expert responsible for a specific degradation. Drawing inspiration from Mixture of Expert (MoE), we aggregate the outputs of each expert rather than directly merging parameters in [11]. Therefore, given the input feature \(x_{in}\) of the current layer and the similarity \(s\), the total output \(x_{out}\) of this modified layer can be expressed as

\[x_{out}=f_{o}(x_{in})+\sum_{i=1}^{K}s_{i}f_{i}(x_{in}),\] (5)

where \(f_{i}(x_{in})\) denotes the result of \(i\)-th trainable matrice \(W_{i}\), particularly \(f_{o}(x_{in})\) is output of the frozen base layer. From the equation 5, it can be observed that the introduced LoRAs interact with the frozen base model at intermediate feature layers in our restorer. This interaction forces the restorer to leverage the image priors of the pre-trained generative model and eliminate degradation with the assistance of LoRAs. In contrast to employing stable diffusion [37] directly as a post-processing technique, our restorer yields results closer to the true scene without introducing inaccurate structural details. Since each \(W\) is implemented using two low-rank matrices like the formula 1, the total trainable parameters of our framework are much smaller than that of the pre-train generative model.

### Training and Inference Procedure

During the training phase, for the efficient training of the universal image restorer, we ensure that each batch is sampled from the same degradation type \(D^{k}\), and activate the corresponding LoRAfor training. Since the dataset \(D\) is organized by degradation type without overlap and each LoRA is assigned to handle each type of degradation correspondingly, the overall optimization process in equation 2 can be decomposed into independent optimization processes for each degradation. This design and training process circumvent task conflicts among multiple degradations and makes it possible to use suitable loss functions for the specific degradation. Due to the availability of accurate \(s\) during training and the use of pre-trained encoders from CLIP [35] and Daclip-ir [20] in our router, the router was not utilized during training.

In the inference phase, the similarity \(s\) is unknown and needs to be estimated from the degraded image. The estimated similarity \(s\) serves as a reference in our framework and can also be manually specified by users. Subsequently, our universal image restorer composite LoRAs and recovers the input image with the guidance of \(s\).

## 4 Experiments

### Experimental Setting

**Datasets.** We validate the effectiveness of our framework in multiple and mixed degradation scenarios. In the case of multiple degradations, we follow Daclip-IR [20] and construct a dataset using 10 different single degradation datasets. Briefly, the composite dataset comprises a total of 52800 image pairs for training and 2490 image pairs for testing. The degradation types included are commonly encountered in image restoration, such as blur, noise, shadow, JPEG compression, and weather degradations. For mixed degradations, we utilize two degradation datasets, REDS [25] and LOLBlur [58]. In REDS, the images are distorted by JPEG compression and blur, and those images in LOLBlur have blur and low light. For more details about datasets in our experiments, please refer to **Appendix**.

**Metrics.** The objective of the image restoration task is to output images with enhanced visual quality while maintaining high fidelity to the original scene information. This differs from image generation tasks, which prioritize visual quality. Therefore, to thoroughly evaluate the effectiveness of our method, we utilize reference-based image quality assessment techniques from both distortion and perceptual perspectives, including PSNR, SSIM, and LPIPS, as well as FID.

**Comparison Methods.** In the experiments, we primarily compare with several state-of-the-art methods in image restoration, which fall into two categories: regression model and generative model. Regression models include NAFNet [2], Restormet [53], as well as AirNet [15] and PromptIR [31] proposed for multiple degradation restoration. DiffBIR [17], IR-SDE [21] and Daclip-IR [20] are generative models built upon the diffusion model [9].

### Implementation Details

During the training, we adapt an AdamW optimizer to update the weights of trainable parameters in our model. Before training LoRA for specific degradation, we add skip-connections in the VAE of SD-turbo[39] like [29; 44] and train them with multiple degraded images. We set the initialization

\begin{table}
\begin{tabular}{l c c c c|c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{2}{c}{**Distortion**} & \multicolumn{2}{c}{**Perceptual**} & \multicolumn{2}{c}{**Complexity**} \\ \cline{2-7}  & PSNR\(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & FID \(\downarrow\) & Param /M & Runtime /s \\ \hline SwinIR [16] & 23.37 & 0.731 & 0.354 & 104.37 & 15.8 & 0.66 \\ NAFNet [2] & 26.34 & 0.847 & 0.159 & 55.68 & 67.9 & 0.54 \\ Restormer [53] & 26.43 & 0.850 & 0.157 & 54.03 & 26.1 & 0.14 \\ \hline AirNet [15] & 25.62 & 0.844 & 0.182 & 64.86 & 7.6 & 1.50 \\ PromptIR [31] & 27.14 & 0.859 & 0.147 & 48.26 & 35.6 & 1.19 \\ IR-SDE [21] & 23.64 & 0.754 & 0.167 & 49.18 & 36.2 & 5.07 \\ DiffBIR [17] & 21.01 & 0.618 & 0.263 & 91.03 & 363.2 & 5.95 \\ Daclip-IR [20] & 27.01 & 0.794 & 0.127 & 34.89 & 295.2 & 4.09 \\
**UIR-LoRA (Ours)** & **28.08** & **0.864** & **0.104** & **30.58** & 95.2 & 0.44 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of the restoration results over ten different datasets. The best results are marked in boldface.

learning rate to 2e-4 and decrease it with CosineAnnealingLR. We trained every LoRA for 80K iterations with batch size 8 and we keep the same hyper-parameters when training different LoRAs. The default rank of LoRAs in VAE and Unet is 4 and 8, respectively.

### Multiple Image Restoration

For fair comparisons, all methods are trained and tested on the multiple degradation dataset. The results are presented in Table 1. We can find that our model, UIR-LoRA, considerably surpasses all compared image restoration approaches across four metrics. This indicates that our approach can balance generating clear structures and details while ensuring the restored images closely resemble the original information of the scene. The visual comparison results depicted in Figure 7 also confirm this assertion. Regression models such as NAFNet [2]and Restormer [53], lacking extensive image priors, tend to produce blurred and over-smoothed images, leading to inferior visual outcomes. Conversely, generative models Daclip-IR [20] excessively prioritize perceptual quality, yielding artifacts and noise that diverge from the actual scene information. Our approach integrates the strengths of both categories of methods, enabling strong performance in both distortion and perceptual aspects

### Mixed Image Restoration

To evaluate the transferability of UIR-LoRA, we conduct some experiments on mixed degradation datasets from REDS[25] and LOLBlur [58]. Each image in these two datasets contains more than one type of degradation, like blur, jpeg compression, noise, and low light. We test the mixed degraded images using models trained on multiple degradations and set \(K\) to 2 in the router. As shown in Table 2, our method achieves superior results in both distortion and perceptual quality, particularly on the LOLBlur dataset. We also provide visual comparison results, as illustrated in Figure 4, our approach effectively enhances the low-light image compared to SOTA methods, highlighting its stronger transferability in the wild. More visual results can be found in **Appendix**.

Figure 3: Qualitative comparison on multiple degraded images.

[MISSING_PAGE_FAIL:8]

similarity achieves better outcomes. This suggests that the transferability between different types of degradation is limited and that specific parameters are needed to address their particularities. Furthermore, the selection of the K value also affects the model's performance. When an image has only one type of degradation, a smaller K value can result in comparable performance with lower inference costs. However, for mixed degradations, a larger K value is required to handle the more complex situation.

**Impact of LoRA's Rank.** Within our framework, LoRA is utilized to facilitate the transfer from the pre-trained generative model to the image restoration task. In order to investigate the impact of LoRA's rank on the performance of image restoration, we conduct experiments using deblurring and denoising tasks chosen from ten distinct degradation categories. We set the initial rank to 2 and incrementally increase the value by a factor of 2. The performance changes are depicted in Figure 5. It is evident that as the rank grows, the restoration results improve in distortion and perceptual quality, and at the same time, the number of trainable parameters also increases. Once the rank value exceeds 4, the performance improvement becomes progressively marginal. Therefore, we set the default rank to 4 in our restorer to balance between performance and complexity.

**Impact of Predicted Degradation.**

The resizing operation on input images in CLIP models [20; 35] may lead to inaccurate predictions of degradation types, especially for blurry images. To reduce its negative impact on performance, we introduce a simple way that uses the degradation vector of the image crop without resizing to correct the potential error in the resized image. Table 4 is the comparison conducted on blurry images from GoPro dataset. It can be observed that our model with modified operation has higher accuracy and better performance for deblurring.

## 5 Conclusion

In this paper, we propose a universal image restoration framework based on multiple low-rank adaptation, named UIR-LoRA, from the perspective of multi-domain transfer learning. UIR-LoRA utilizes a pre-trained generative model as the frozen base model and transfers its abundant image priors to different image restoration tasks using the LoRA technique. Moreover, we introduce a LoRA' composition strategy based on the degradation similarity that allows UIR-LoRA applicable for multiple and mixed degradations in the wild. Extensive experiments on universal image restoration tasks demonstrate the effectiveness and better generalization capability of our proposed UIR-LoRA.

## 6 Limitation and Discussion

Although our UIR-LoRA has achieved remarkable performance in image restoration tasks under both multiple and mixed degradations, it still has limitations and problems for further exploration. For instance, adding new trainable parameters into the network for unseen degradations is unavoidable in image restoration tasks, although UIR-LoRA is already more efficient and flexible compared to other approaches.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & PSNR\(\uparrow\) & SSIM \(\uparrow\) & LPIPS \(\downarrow\) & FID \(\downarrow\) & Accuracy \(\uparrow\) \\ \hline Original & 26.66 & 0.839 & 0.159 & 18.72 & 91.6 \\ Modified & 26.87 & 0.842 & 0.155 & 18.42 & 99.2 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The accuracy of predicted degradation type

Figure 5: The impact of LoRA’s rank on deblurring and denoising tasks.

## References

* Agustsson and Timofte [2017] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In _Proceedings of the IEEE conference on computer vision and pattern recognition workshops_, pages 126-135, 2017.
* Chen et al. [2022] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image restoration. In _European conference on computer vision_, pages 17-33. Springer, 2022.
* Chen et al. [2021] Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Chengpeng Chen. Hinet: Half instance normalization network for image restoration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 182-192, 2021.
* Chen et al. [2022] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. _Advances in Neural Information Processing Systems_, 35:16664-16678, 2022.
* Fu et al. [2017] Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao, and John Paisley. Clearing the skies: A deep network architecture for single-image rain removal. _IEEE Transactions on Image Processing_, 26(6):2944-2956, 2017.
* Gong et al. [2017] Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian Reid, Chunhua Shen, Anton Van Den Hengel, and Qinfeng Shi. From motion blur to motion flow: A deep learning solution for removing heterogeneous motion blur. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2319-2328, 2017.
* Gu et al. [2014] Shuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization with application to image denoising. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2862-2869, 2014.
* Guo et al. [2020] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1780-1789, 2020.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In _International conference on machine learning_, pages 2790-2799. PMLR, 2019.
* Hu et al. [2022] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022.
* Karras et al. [2017] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. _arXiv preprint arXiv:1710.10196_, 2017.
* Kumari et al. [2023] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1931-1941, 2023.
* Kupyn et al. [2018] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Jiri Matas. Deblurgan: Blind motion deblurring using conditional adversarial networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8183-8192, 2018.
* Li et al. [2022] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jiancheng Lv, and Xi Peng. All-in-one image restoration for unknown corruption. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17452-17462, 2022.
* Liang et al. [2021] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1833-1844, 2021.

* Lin et al. [2023] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao, and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior. _arXiv preprint arXiv:2308.15070_, 2023.
* Liu et al. [2018] Yun-Fu Liu, Da-Wei Jaw, Shih-Chia Huang, and Jenq-Neng Hwang. Desnownet: Context-aware deep network for snow removal. _IEEE Transactions on Image Processing_, 27(6):3064-3073, 2018.
* Lingmayr et al. [2022] Andreas Lingmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repairt: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11461-11471, 2022.
* Luo et al. [2023] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas B Schon. Controlling vision-language models for universal image restoration. _arXiv preprint arXiv:2310.01018_, 2023.
* Luo et al. [2023] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas B Schon. Image restoration with mean-reverting stochastic differential equations. _International Conference on Machine Learning_, 2023.
* Ma et al. [2022] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongxuan Luo. Toward fast, flexible, and robust low-light image enhancement. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 5637-5646, 2022.
* Martin et al. [2001] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 416-423, 2001.
* Mou et al. [2024] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 4296-4304, 2024.
* Nah et al. [2019] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_, pages 0-0, 2019.
* Nah et al. [2017] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 3883-3891, 2017.
* Pan et al. [2014] Jinshan Pan, Zhe Hu, Zhixun Su, and Ming-Hsuan Yang. Deblurring text images via l0-regularized intensity and gradient prior. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 2901-2908, 2014.
* Pan et al. [2016] Jinshan Pan, Deqing Sun, Hanspeter Pfister, and Ming-Hsuan Yang. Blind image deblurring using dark channel prior. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1628-1636, 2016.
* Parmar et al. [2024] Gaurav Parmar, Taesung Park, Srinivasa Narasimhan, and Jun-Yan Zhu. One-step image translation with text-to-image models. _arXiv preprint arXiv:2403.12036_, 2024.
* Podell et al. [2023] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.
* Potlapalli et al. [2023] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Khan. Promptir: Prompting for all-in-one image restoration. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.

* Qian et al. [2018] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiaying Liu. Attentive generative adversarial network for raindrop removal from a single image. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2482-2491, 2018.
* Qin et al. [2020] Xu Qin, Zhilin Wang, Yuanchao Bai, Xiaodong Xie, and Huizhu Jia. Ffa-net: Feature fusion attention network for single image dehazing. In _Proceedings of the AAAI conference on artificial intelligence_, pages 11908-11915, 2020.
* Qu et al. [2017] Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, and Rynson WH Lau. Deshadownet: A multi-context embedding deep network for shadow removal. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4067-4075, 2017.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International conference on machine learning_, pages 8821-8831. Pmlr, 2021.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in neural information processing systems_, 35:36479-36494, 2022.
* Sauer et al. [2023] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. _arXiv preprint arXiv:2311.17042_, 2023.
* Sener and Koltun [2018] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. _Advances in neural information processing systems_, 31, 2018.
* Shazeer et al. [2017] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. _arXiv preprint arXiv:1701.06538_, 2017.
* Sheikh [2005] H Sheikh. Live image quality assessment database release 2. http://live.ece.utexas.edu/research/quality, 2005.
* Timofte et al. [2017] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, and Lei Zhang. Ntire 2017 challenge on single image super-resolution: Methods and results. In _Proceedings of the IEEE conference on computer vision and pattern recognition workshops_, pages 114-125, 2017.
* Wang et al. [2023] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. _arXiv preprint arXiv:2305.07015_, 2023.
* Wang et al. [2023] Yinglong Wang, Chao Ma, and Jianzhuang Liu. Smartassign: Learning a smart knowledge assignment strategy for deraining and desnowing. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3677-3686, 2023.
* Wei et al. [2018] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. _arXiv preprint arXiv:1808.04560_, 2018.
* Whang et al. [2022] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G Dimakis, and Peyman Milanfar. Deblurring via stochastic refinement. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16293-16303, 2022.
* Wu et al. [2023] Xun Wu, Shaohan Huang, and Furu Wei. Mole: Mixture of lora experts. In _The Twelfth International Conference on Learning Representations_, 2023.

* Xie et al. [2023] Yuan Xie, Shaohan Huang, Tianyu Chen, and Furu Wei. Moec: Mixture of expert clusters. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 13807-13815, 2023.
* Yang et al. [2017] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep joint rain detection and removal from a single image. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1357-1366, 2017.
* Yang et al. [2020] Wenhan Yang, Robby T Tan, Shiqi Wang, Yuming Fang, and Jiaying Liu. Single image deraining: From model-based to data-driven and beyond. _IEEE Transactions on pattern analysis and machine intelligence_, 43(11):4059-4077, 2020.
* Yu et al. [2020] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. _Advances in Neural Information Processing Systems_, 33:5824-5836, 2020.
* Zamir et al. [2022] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 5728-5739, 2022.
* Zamir et al. [2021] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 14821-14831, 2021.
* Zhang et al. [2023] Cheng Zhang, Yu Zhu, Qingsen Yan, Jinqiu Sun, and Yanning Zhang. All-in-one multi-degradation image restoration network via hierarchical degradation representation. In _Proceedings of the 31st ACM International Conference on Multimedia_, pages 2285-2293, 2023.
* Zhang et al. [2017] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. _IEEE transactions on image processing_, 26(7):3142-3155, 2017.
* Zhang et al. [2023] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.
* Zhou et al. [2022] Shangchen Zhou, Chongyi Li, and Chen Change Loy. Lednet: Joint low-light enhancement and deblurring in the dark. In _European conference on computer vision_, pages 573-589. Springer, 2022.

Appendix

### More Details about Datasets

For multiple degradations, we follow Daclip-IR [20] to construct the dataset, which includes a total of ten distinct degradation types: blurry, hazy, JPEG-compression, low-light, noisy, raindrop, rainy, shadowed, snowy, and inpainting. The data sources and data splits for each degradation type are illustrated in Table 5.

For mixed degradations, we utilize images from REDS[25] and LOLBlur[58]to evaluate the transferability of models. We sample 60 images from REDS and 200 images from LOLBlur dataset for testing. The degraded images from REDS dataset feature a variety of realistic scenes and objects, which suffer from both motion blurs and compression. And the images from LOLBlur dataset cover a range of real-world dynamic dark scenarios with mixed degradation of low light and blurs.

### More Visual Results

### Details about Metrics on Multiple Degradation

Figure 6: Qualitative comparison on mixed degraded images from LOLBlur dataset.

\begin{table}
\begin{tabular}{l c c|c c} \hline \hline \multirow{2}{*}{**Dataset**} & \multicolumn{2}{c}{**Train**} & \multicolumn{2}{c}{**Test**} \\ \cline{2-5}  & Sources & Num & Sources & Num \\ \hline Blurry & GoPro[26] & 2 103 & GoPro & 1 111 \\ Hazy & RESIDE-6k[33] & 6 000 & RESIDE-6k & 1 000 \\ JPEG & DIV2K[1] and Flickr2K[43] & 3 550 & LIVE1[42] & 29 \\ Low-light & LOL[46] & 485 & LOL & 15 \\ Noisy & DIV2K and Flickr2K & 3 550 & CBSD68[23] & 68 \\ Raindrop & RainDrop[32] & 861 & RainDrop & 58 \\ Rainy & Rain100H[50] & 1 800 & Rain100H & 100 \\ Shadowed & SRD[34] & 2 680 & SRD & 408 \\ Snowy & Snow100K-L[18] & 1 872 & Snow100K-L & 601 \\ Inpainting & CelebaHQ[12] & 29 900 & CelebaHQ and RePaint[19] & 100 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Details of the datasets with ten different image degradation types

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline  & Blurry & Hazy & JPEG & Low-light & Noisy & Raindrop & Rainy & Shadowed & Snowy & Inpainting & Average \\ \hline SwinIR & 24.49 & 23.49 & 24.44 & 19.59 & 25.13 & 24.64 & 22.07 & 23.97 & 21.86 & 24.05 & 23.37 \\ NAVNet & 26.12 & 24.05 & 26.81 & 22.16 & 27.16 & 30.67 & 27.32 & 24.16 & 25.94 & 29.03 & 26.34 \\ Restormer & 26.34 & 23.75 & 26.90 & 22.17 & 27.25 & 30.85 & 27.91 & 23.33 & 25.98 & 29.88 & 26.43 \\ AirNet & 26.25 & 23.56 & 26.98 & 14.24 & 27.51 & 30.68 & 28.45 & 23.48 & 24.87 & 30.15 & 25.62 \\ PromptIR & 26.50 & 25.19 & 26.95 & **23.14** & 27.56 & **31.35** & 29.24 & 24.06 & 27.23 & 30.22 & 27.14 \\ IR-SDE & 24.13 & 17.44 & 24.21 & 16.07 & 24.82 & 28.49 & 26.64 & 22.18 & 24.70 & 27.56 & 23.64 \\ DIMIR & 22.79 & 20.52 & 22.39 & 16.96 & 21.60 & 23.22 & 21.04 & 22.27 & 20.63 & 18.77 & 21.01 \\ Saclip-IR & **27.03** & 29.53 & 23.70 & 22.09 & 24.36 & 30.81 & **29.41** & 27.27 & 26.83 & 28.94 & 27.01 \\ Ours & 26.66 & **30.28** & **27.15** & 22.45 & **27.74** & 30.51 & 28.26 & **28.63** & **28.09** & **30.88** & **28.06** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of the restoration results over ten different datasets on _PSNR_

Figure 7: Qualitative comparison on mixed degraded images from REDS dataset.

\begin{table}
\begin{tabular}{c c c c c c|c c c c c c} \hline \hline  & Blurry & Hazy & JPEG & Low-light & Noisy & Raindrop & Rainy & Shadowed & Snowy & Inpainting & Average \\ \hline SwinIR & 0.758 & 0.848 & 0.734 & 0.735 & 0.690 & 0.758 & 0.623 & 0.757 & 0.665 & 0.743 & 0.731 \\ NAFNet & 0.804 & 0.926 & 0.780 & 0.809 & 0.768 & 0.924 & 0.848 & 0.839 & 0.869 & 0.901 & 0.847 \\ Restormer & 0.811 & 0.915 & 0.781 & 0.815 & 0.762 & 0.928 & 0.862 & 0.836 & 0.877 & 0.912 & 0.850 \\ AirNet & 0.805 & 0.916 & 0.783 & 0.781 & 0.769 & 0.926 & 0.867 & 0.832 & 0.846 & 0.911 & 0.844 \\ PromptIR & 0.815 & 0.933 & **0.784** & **0.829** & 0.774 & **0.931** & **0.876** & 0.842 & 0.887 & **0.918** & 0.859 \\ IR-SDE & 0.730 & 0.832 & 0.615 & 0.719 & 0.640 & 0.822 & 0.808 & 0.667 & 0.828 & 0.876 & 0.754 \\ DiffBIR & 0.695 & 0.761 & 0.607 & 0.665 & 0.395 & 0.682 & 0.573 & 0.568 & 0.566 & 0.678 & 0.618 \\ Daclip-IR & 0.810 & 0.931 & 0.532 & 0.796 & 0.579 & 0.882 & 0.854 & 0.811 & 0.854 & 0.894 & 0.794 \\ Ours & **0.839** & **0.962** & 0.782 & 0.826 & **0.789** & 0.908 & 0.857 & **0.862** & **0.893** & 0.916 & **0.864** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Comparison of the restoration results over ten different datasets on _SSIM_

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline  & Blurry & Hazy & JPEG & Low-light & Noisy & Raindrop & Rainy & Shadowed & Snowy & Inpainting & Average \\ \hline SwinIR & 0.347 & 0.180 & 0.392 & 0.362 & 0.439 & 0.353 & 0.481 & 0.335 & 0.388 & 0.265 & 0.354 \\ NAFNet & 0.284 & 0.043 & 0.303 & 0.158 & 0.216 & 0.082 & 0.180 & 0.138 & 0.096 & 0.085 & 0.159 \\ Restormer & 0.282 & 0.054 & 0.300 & 0.156 & 0.215 & 0.083 & 0.170 & 0.145 & 0.095 & 0.072 & 0.157 \\ AirNet & 0.279 & 0.063 & 0.302 & 0.321 & 0.264 & 0.095 & 0.163 & 0.145 & 0.112 & 0.071 & 0.182 \\ PromptIR & 0.267 & 0.051 & 0.269 & 0.140 & 0.230 & 0.078 & 0.147 & 0.143 & 0.082 & 0.068 & 0.147 \\ IR-SDE & 0.198 & 0.168 & 0.246 & 0.185 & 0.232 & 0.113 & 0.142 & 0.223 & 0.107 & 0.065 & 0.167 \\ DiffBIR & 0.269 & 0.158 & 0.244 & 0.273 & 0.442 & 0.187 & 0.309 & 0.261 & 0.236 & 0.246 & 0.263 \\ Daclip-IR & **0.140** & 0.037 & 0.317 & **0.114** & 0.272 & 0.068 & **0.085** & 0.118 & 0.072 & **0.047** & 0.127 \\ Ours & 0.159 & **0.021** & **0.204** & 0.126 & **0.153** & **0.048** & 0.112 & **0.103** & **0.070** & 0.056 & **0.105** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison of the restoration results over ten different datasets on _LPIPS_

\begin{table}
\begin{tabular}{c c c c c c|c c c c c} \hline \hline  & Blurry & Hazy & JPEG & Low-light & Noisy & Raindrop & Rainy & Shadowed & Snowy & Inpainting & Average \\ \hline SwinIR & 0.347 & 0.180 & 0.392 & 0.362 & 0.439 & 0.353 & 0.481 & 0.335 & 0.388 & 0.265 & 0.354 \\ NAFNet & 0.284 & 0.043 & 0.303 & 0.158 & 0.216 & 0.082 & 0.180 & 0.138 & 0.096 & 0.085 & 0.159 \\ Restormer & 0.282 & 0.054 & 0.300 & 0.156 & 0.215 & 0.083 & 0.170 & 0.145 & 0.095 & 0.072 & 0.157 \\ AirNet & 0.279 & 0.063 & 0.302 & 0.321 & 0.264 & 0.095 & 0.163 & 0.145 & 0.112 & 0.071 & 0.182 \\ PromptIR & 0.267 & 0.051 & 0.269 & 0.140 & 0.230 & 0.078 & 0.147 & 0.143 & 0.082 & 0.068 & 0.147 \\ IR-SDE & 0.198 & 0.168 & 0.246 & 0.185 & 0.2NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction1 accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper does discuss the limitations of the work in Sections 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The paper provides the full set of assumptions and a complete (and correct) proof in Sections 3.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

**Experimental Result Reproducibility**

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer: [Yes]

Justification: The paper provides a comprehensive description of the experimental setting in Sections 4.1 and implementation details in Sections 4.2, which are crucial for reproducing the main results.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

**Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

Answer: [No]

Justification: Upon acceptance of the paper, we will release the code under an open-source license, which will allow the community to access and verify the experimental results.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper provides detailed information on all training and test aspects, including datasets, metrics, comparison methods, hyperparameters, the type of optimizer used, and other relevant details necessary to understand the results in Sections 4.1 and 4.2. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper reports error bars appropriately and includes correctly defined information regarding the statistical significance of the experiments, ensuring the transparency and reliability of the results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper provides detailed information regarding the computer resources used in Sections 4.1, and time of execution in Table 1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper is in full compliance with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper provides both the potential benefits and the risks associated with the research, ensuring a comprehensive assessment of its societal implications. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not introduce assets that carry a high risk for misuse, therefore, no specific safeguards for data or model release are required. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper meticulously cites all external assets in references, including code, dataset, and models, acknowledging the contributions of their creators and respecting the associated licenses and terms of use. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The novel universal image restoration framework introduced in the paper is well documented, and the documentation is provided alongside the model in Sections 3, offering comprehensive details for replication and application. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not engage in crowdsourcing experiments or research with human subjects therefore, it does not include participant instructions, screenshots, or details about compensation. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]Justification: The paper does not involve research with human subjects, so there are no participant risks to disclose, and no Institutional Review Board (IRB) approvals or equivalent reviews were required.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.