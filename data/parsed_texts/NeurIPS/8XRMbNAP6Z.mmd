# Near-Optimal \(k\)-Clustering in the Sliding Window Model

David P. Woodruff CMU

dwoodruf@cs.cmu.edu &Peilin Zhong

Google Research

peilinz@google.com &Samson Zhou

Texas A&M University

samsonzhou@gmail.com

###### Abstract

Clustering is an important technique for identifying structural information in large-scale data analysis, where the underlying dataset may be too large to store. In many applications, recent data can provide more accurate information and thus older data past a certain time is expired. The sliding window model captures these desired properties and thus there has been substantial interest in clustering in the sliding window model.

In this paper, we give the first algorithm that achieves near-optimal \((1+\varepsilon)\)-approximation to \((k,z)\)-clustering in the sliding window model, where \(z\) is the exponent of the distance function in the cost. Our algorithm uses \(\frac{k}{\min\left(\varepsilon^{4},\varepsilon^{2+z}\right)}\operatorname{ polylog}\frac{n\Delta}{\varepsilon}\) words of space when the points are from \([\Delta]^{d}\), thus significantly improving on works by Braverman et. al. (SODA 2016), Borassi et. al. (NeurIPS 2021), and Epasto et. al. (SODA 2022).

Along the way, we develop a data structure for clustering called an online coreset, which outputs a coreset not only for the end of a stream, but also for all prefixes of the stream. Our online coreset samples \(\frac{k}{\min\left(\varepsilon^{4},\varepsilon^{2+z}\right)}\operatorname{ polylog}\frac{n\Delta}{\varepsilon}\) points from the stream. We then show that any online coreset requires \(\Omega\left(\frac{k}{\varepsilon^{2}}\log n\right)\) samples, which shows a separation from the problem of constructing an offline coreset, i.e., constructing online coresets is strictly harder. Our results also extend to general metrics on \([\Delta]^{d}\) and are near-optimal in light of a \(\Omega\left(\frac{k}{\varepsilon^{2+z}}\right)\) lower bound for the size of an offline coreset.

## 1 Introduction

Clustering is a fundamental procedure frequently used to help extract important structural information from large datasets. Informally, the goal of clustering is to partition the data into \(k\) clusters so that the elements within each cluster have similar properties. Classic formulations of clustering include the \(k\)-median and \(k\)-means problems, which have been studied since the 1950's [60, 50]. More generally, for a set \(X\) of \(n\) points in \(\mathbb{R}^{d}\), along with a metric \(\operatorname{dist}\), a cluster parameter \(k>0\), and an exponent \(z>0\) that is a positive integer, the clustering objective can be defined by

\[\min_{C\subset\mathbb{R}^{d},|C|=k}\sum_{i=1}^{n}\min_{c\in C}\operatorname{ dist}(x_{i},c)^{z}.\]

When \(\operatorname{dist}\) is the Euclidean distance, the problem is known as \((k,z)\)-clustering and more specifically, \(k\)-median clustering and \(k\)-means clustering, when \(z\) is additionally set to \(1\) and \(2\), respectively.

As modern datasets have significantly increased in size, attention has shifted to large-scale computational models, such as the streaming model of computation, that do not require multiple passes over the data. In the (insertion-only) streaming model, the points \(x_{1},\ldots,x_{n}\) of \(X\) arrive sequentially,and the goal is to output an optimal or near-optimal clustering of \(X\) while using space sublinear in \(n\), ideally space \(k\ \mathrm{polylog}(n,d)\), since outputting the cluster centers uses \(k\) words of space, where each word of space is assumed to be able to store an entire input point in \(\mathbb{R}^{d}\). There exist slight variants of the insertion-only streaming model and a long line of active research has been conducted on clustering in these models [42, 21, 44, 43, 23, 17, 37, 39, 1, 11, 59, 46, 14, 27, 10, 25, 61, 29].

The sliding window model.Unfortunately, an important shortcoming of the streaming model is that it ignores the time at which a specific data point arrives and thus it is unable to prioritize recent data over older data. Consequently, the streaming model cannot capture applications in which recent data is more accurate and therefore considered more important than data that arrived prior to a certain time, e.g., Census data or financial markets. Indeed, it has been shown that for a number of applications, the streaming model has inferior performance [4, 52, 57, 62] compared to the sliding window model [33], where only the most recent \(W\) updates in the stream comprise the underlying dataset. Here, \(W>0\) is a parameter that designates the window size of the active data, so that all updates before the \(W\) most recent updates are considered expired, and the goal is to aggregate statistics about the active data using space sublinear in \(W\). In the setting of clustering, where the data stream is \(x_{1},\ldots,x_{n}\subset\mathbb{R}^{d}\), the active data set is \(X=\{x_{n-W+1},\ldots,x_{n}\}\) for \(n\geq W\) and \(X=\{x_{1},\ldots,x_{n}\}\) otherwise. Thus the sliding window model is a generalization of the streaming model, depending on the choice of \(W\), and is especially relevant for time-sensitive settings, such as data summarization [22, 34], event detection in social media [56], and network monitoring [32, 31, 30].

The sliding window model is especially relevant for applications in which computation _must_ be restricted to data that arrived after a certain time. Data privacy laws such as the General Data Protection Regulation (GDPR) mandate that companies cannot retain specific user data beyond a certain duration. For example, the Facebook data policy [36] states that user search histories are retained for \(6\) months, the Apple differential privacy overview [3] states that collected user information is retained for \(3\) months, and the Google data retention policy states that browser information may be stored for up to \(9\) months [41]. These retention polices can be modeled by the sliding window model with the corresponding setting of the window parameter \(W\) and thus the sliding window model has been subsequently studied in a wide range of applications [48, 49, 18, 19, 12, 13, 9, 20, 63, 2, 47, 7].

Clustering in the sliding window model.Because the clustering objective is not well-suited to popular frameworks such as the exponential histogram or the smooth histogram, there has been significant interest in clustering in the sliding window model. We now describe the landscape of clustering algorithms in the sliding window model; these results are summarized in Table 1. In 2003, [5] first gave a \(2^{O(1/\varepsilon)}\)-approximation algorithm for \(k\)-median clustering in the sliding window model using \(O\left(\frac{k}{\varepsilon}W^{2\varepsilon}\log^{2}W\right)\) words of space, where \(\varepsilon\in\left(0,\frac{1}{2}\right)\) is an input parameter. Subsequently, [15] gave an \(O\left(1\right)\)-approximate bicriteria algorithm using \(2k\) centers and \(k^{2}\ \mathrm{polylog}(W)\) space for the \(k\)-median problem in the sliding window model. The question of whether there exists a \(\mathrm{poly}(k\log W)\) space algorithm for \(k\)-clustering on sliding windows remained open until [16] gave constant-factor approximation sliding window algorithms for \(k\)-median and \(k\)-means using \(O\left(k^{3}\log^{6}W\right)\) space and [28] gave constant-factor approximation algorithms for \(k\)-center clustering using \(O\left(k\log\Delta\right)\) space, where \(\Delta\) is the aspect ratio, i.e., the ratio of the largest to smallest distances between any pair of points. Afterwards, [8] gave a \(C\)-approximation algorithm for some constant \(C>2^{14}\), though it should be noted that their main contribution was the first constant-factor approximation algorithm for \(k\)-clustering using space linear in \(k\), i.e., \(k\ \mathrm{polylog}(W,\Delta)\) space, and thus they did not attempt to optimize the constant \(C\). Recently, [35] gave the first \((1+\varepsilon)\)-approximation algorithm for \((k,z)\)-clustering using \(\frac{(kd+d^{C})}{\varepsilon^{3}}\ \mathrm{polylog}\left(W,\Delta,\frac{1}{ \varepsilon}\right)\) words of space, for some constant \(C\geq 7\). Using known dimensionality reduction techniques, i.e., [51], the algorithm's dependence on \(d^{C}\) can be removed in exchange for a \(\frac{1}{\varepsilon^{14}}\ \mathrm{polylog}\left(W,\frac{1}{ \varepsilon}\right)\) overhead. However, neither the \(d^{C}\) dependency nor the \(\frac{1}{\varepsilon^{14}}\ \mathrm{polylog}\left(W,\frac{1}{\varepsilon}\right)\) trade-off is desirable for realistic settings of \(d\) and \(\varepsilon\) for applications of \(k\)-clustering on sliding windows. In particular, recent results have achieved efficient summarizations, i.e., coresets, for \(k\)-median and \(k\)-means clustering in the offline setting using \(\tilde{O}\left(\frac{k}{\varepsilon^{4}}\log n\right)\) words of space [27, 25] when the input is from \([\Delta]^{d}\) and it is known that this is near-optimal, i.e., \(\Omega\left(\frac{k}{\varepsilon^{2+\varepsilon}}\log n\right)\) samples are necessary to form coresets for \((k,z)\)-clustering [45] in that setting. Thus a natural question is to ask whether such near-optimal space bounds can be achieved in the sliding window model.

### Our Contributions

In this paper, we answer the question in the affirmative. That is, we give near-optimal space algorithms for \(k\)-median and \(k\)-means clustering in the sliding window model. In fact, we give more general algorithms for \((k,z)\)-clustering in the sliding window that nearly match the space used by the offline coreset constructions of [27, 25, 26]:

**Theorem 1.1**.: _There exists an algorithm that samples \(\frac{k}{\min(\varepsilon^{4},\varepsilon^{2+z})}\ \mathrm{polylog}\ \frac{n \Delta}{\varepsilon}\) points and with high probability, outputs a \((1+\varepsilon)\)-approximation to \((k,z)\)-clustering for the Euclidean distance on \([\Delta]^{d}\) in the sliding window model._

In particular, our bounds in Theorem1.1 achieve \(\frac{k}{\varepsilon^{4}}\ \mathrm{polylog}\ \frac{n\Delta}{\varepsilon}\) words of space for \(k\)-median clustering and \(k\)-means clustering, i.e., \(z=1\) and \(z=2\), respectively, matching the lower bounds of [25, 45] up to polylogarithmic factors.

Moreover, our algorithm actually produces a coreset, i.e., a data structure that approximately answers the clustering cost of the underlying dataset with respect to any set of \(k\) centers, not just the optimal \(k\) centers.

**Theorem 1.2**.: _There exists an algorithm that samples \(\frac{k}{\min(\varepsilon^{4},\varepsilon^{2+z})}\ \mathrm{polylog}\ \frac{n \Delta}{\varepsilon}\) points and with high probability, outputs a \((1+\varepsilon)\)-coreset to \((k,z)\)-clustering in the sliding window model for general metrics on \([\Delta]^{d}\)._

We emphasize that the guarantees of Theorem1.2 are for general metrics on \([\Delta]^{d}\), such as \(L_{p}\) metrics. Note that in light of the properties of coresets, the guarantee of Theorem1.1 follows from taking a coreset for \((k,z)\)-clustering on Euclidean distances and then using an offline algorithm for \((k,z)\)-clustering for post-processing after the data stream, i.e., see Theorem2.4.

Along the way, we provide a construction for a \((1+\varepsilon)\)-online coreset for \((k,z)\)-clustering for general metrics on \([\Delta]^{d}\). An online coreset for \((k,z)\)-clustering is a data structure on a data stream that will not only approximately answer the clustering cost of the underlying dataset with respect to any set of \(k\) centers, but also approximately answer the clustering cost of _any prefix of the data stream_ with respect to any set of \(k\) centers.

**Theorem 1.3**.: _There exists an algorithm that samples \(\frac{k}{\min(\varepsilon^{4},\varepsilon^{2+z})}\ \mathrm{polylog}\ \frac{n \Delta}{\varepsilon}\) points and with high probability, outputs a \((1+\varepsilon)\)-online coreset for \((k,z)\)-clustering._

We remark that Theorem1.3 further has the attractive property that once a point is sampled into the online coreset at some point in the stream, then the point irrevocably remains in the online coreset. That is, the online coreset essentially satisfies two different definitions of online: 1) the data structure is a coreset for any prefix of the stream and 2) points sampled into the data structure will never be deleted from the data structure.

We further remark that due to leveraging the coreset construction of [27, 25, 26], we can similarly trade a factor of \(\frac{1}{\varepsilon^{x}}\) for a \(\mathrm{poly}(k)\) in the guarantees of Theorem1.1, Theorem1.2, and Theorem1.3.

By contrast, the lower bound by [25] states that any offline coreset construction for \(k\)-means clustering only requires \(\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) points. This lower bound was later strengthened to \(\Omega\left(\frac{k}{\varepsilon^{2+x}}\right)\) points by [45], for which matching upper bounds are given by [27, 25]. Thus our online coreset constructions are near-optimal in the \(k\) and \(\frac{1}{\varepsilon}\) dependencies for \(z>1\) and nearly match the best known offline constructions for \(z=1\).

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Reference & Accuracy & Space & Setting \\ \hline
[5] & \(2^{O(1/\varepsilon)}\) & \(O\left(\frac{k}{\varepsilon^{4}}W^{2}\log^{2}W\right)\) & \(k\)-median, \(\varepsilon\in\left(0,\frac{1}{2}\right)\) \\ \hline
[16] & \(C>2\) & \(O\left(k^{3}\log^{6}W\right)\) & \(k\)-median and \(k\)-means \\ \hline
[34] & \(C>2^{14}\) & \(k\ \mathrm{polylog}(W,\Delta)\) & \((k,z)\)-clustering \\ \hline
[35] & \((1+\varepsilon)\) & \(\frac{(kd+d^{\varepsilon^{2}})}{\varepsilon^{3}}\ \mathrm{polylog}\left(W,\Delta, \frac{1}{\varepsilon}\right)\), \(C\geq 7\) & \((k,z)\)-clustering \\ \hline Our work & \((1+\varepsilon)\) & \(\frac{k}{\min(\varepsilon^{4},\varepsilon^{2+z})}\ \mathrm{polylog}\ \frac{n \Delta}{\varepsilon}\) & \((k,z)\)-clustering \\ \hline \end{tabular}
\end{table}
Table 1: Summary of \((k,z)\)-clustering results in the sliding window model for input points in \([\Delta]^{d}\) on a window of size \(W\)It is thus a natural question to ask whether our polylogarithmic overheads in Theorem1.3 are necessary for an \((1+\varepsilon)\)-online coreset. We show that in fact, a logarithmic overhead is indeed necessary to maintain a \((1+\varepsilon)\)-online coreset.

**Theorem 1.4**.: _Let \(\varepsilon\in(0,1)\). For sufficiently large \(n\), \(d\), and \(\Delta\), there exists a set \(X\subset[\Delta]^{d}\) of \(n\) points \(x_{1},\ldots,x_{n}\) such that any \((1+\varepsilon)\)-online coreset for \(k\)-means clustering on \(X\) requires \(\Omega\left(\frac{k}{\varepsilon^{2}}\log n\right)\) points._

We emphasize that combined with existing offline coreset constructions [25, 26], Theorem1.4 shows a separation between the problems of constructing offline coresets and online coresets. That is, the problem of maintaining a data structure that recovers coresets for all prefixes of the stream is provably harder than maintaining a coreset for an offline set of points.

### Technical Overview

In this section, we give a high-level overview of our techniques. We also describe the limitations of many natural approaches.

Shortcomings of histograms and sensitivity sampling.A first attempt at clustering in the sliding window model might be to adapt the popular exponential histogram [33] and smooth histogram techniques [18]. These frameworks convert streaming algorithms to sliding window algorithms in the case that the objective function is smooth, which informally means that once a suffix of a data stream becomes a good approximation of the overall data stream, then it always remains a good approximation, regardless of the values of new elements that arrive in the stream. Unfortunately, [16] showed that the \(k\)-clustering objective function is not smooth and thus these histogram-based frameworks cannot work. Nevertheless, they gave the first constant-factor approximation by showing that the \(k\)-clustering objective function is almost-smooth using a generalized triangle inequality, which inherently loses constant factors and thus will not suffice for our goal of achieving a \((1+\varepsilon)\)-approximation.

Another approach might be to adapt the popular sensitivity sampling framework of coreset construction [37, 39, 10, 29]. The sensitivity sampling framework assigns a value to each point, called the sensitivity, which intuitively quantifies the "importance" of that point, and then samples each point with probability proportional to its sensitivity. [9] observed that sliding window algorithms can be achieved from _online_ sensitivity sampling, where the importance of each point is measured against the prefix of the stream, and then running the process in reverse at each time, so that more emphasis is placed on the suffix of the sliding window. At a high level, this is the intuition taken by [34, 35], which leverage data structures that prioritize more recent elements of the data stream. However, it is not known how to achieve optimal bounds simply using sensitivity sampling, and indeed the optimal coreset constructions use slightly more nuanced sampling schemes [27, 25].

Sliding window algorithms from online coresets.Instead, we recall an observation by [9], who noted that deterministic constructions for online coresets for linear algebraic problems can be utilized to obtain sliding window algorithms for the corresponding linear algebraic problems. We first extend this observation to randomized constructions for online coresets for \(k\)-clustering problem.

The intuition is quite simple. Given an \((1+\varepsilon)\)-online coreset algorithm for a \(k\)-clustering problem on a data stream of length \(n\) from \(\mathbb{R}^{d}\) that stores \(S(n,d,k,\varepsilon,\delta)\) weights points and succeeds with probability \(1-\delta\), we store the \(S(n,d,k,\varepsilon^{\prime},\delta^{\prime})\) most recent points in the stream, where \(\varepsilon^{\prime}=O\left(\frac{\varepsilon}{\log n}\right)\) and \(\delta^{\prime}=\frac{\delta}{\mathrm{poly}(n)}\). We then feed the \(S(n,d,k,\varepsilon^{\prime},\delta^{\prime})\) points to the online coreset construction in _reverse order of their arrival_. Since the online coreset preserves all costs for all prefixes of its input, then the resulting data structure will preserve all costs for all _suffixes_ of the data stream. To extend this guarantee to the entire stream, including the sliding window, we can then use a standard merge-and-reduce framework. It thus remains to devise a \((1+\varepsilon)\)-online coreset construction for \(k\)-clustering with near-optimal sampling complexity.

Online coreset construction.To that end, our options are quite limited, as to the best of our knowledge, the only offline coreset constructions using \(\tilde{O}\left(\frac{k}{\varepsilon^{4}}\log n\right)\) words of space when the input is from \([\Delta]^{d}\) are due to [27, 25]. Fortunately, although the analyses of correctness for these sampling schemes are quite involved, the constructions themselves are quite accessible. For example, [27] first uses an \((\alpha,\beta)\)-approximation, i.e., a clustering that achieves \(\alpha\)-approximation to the optimal cost but uses \(\beta k\) centers, to partition the underlying dataset \(X\) into disjoint concentric rings around each of the \(\beta k\) centers. These rings are then gathered into groups and it is shown that by independently sampling a fixed number of points with replacement from each of the groups suffices to achieve a \((1+\varepsilon)\)-coreset. Their analysis argues that the contribution of each of the groups toward the overall \(k\)-clustering cost is preserved through an expectation and variance bounding argument, and then taking a sophisticated union bound over a net over the set of possible centers. Thus their argument still holds when each point of the dataset is independently sampled by the data structure with probability proportional to the probability it would have been sampled by the group. Moreover, independently sampling each point with a higher probability can only decrease the variance, so that correctness is retained, though we must also upper bound the number of sampled points. Crucially, independently sampling each point can be implemented in the online setting and the probability of correctness can be boosted to union bound over all times in the stream, which facilitates the construction of our \((1+\varepsilon)\)-online coreset, given an \((\alpha,\beta)\)-approximation.

Consistent \((\alpha,\beta)\)-approximation.It seemingly remains to find \((\alpha,\beta)\)-approximations for \(k\)-clustering at all times in the stream. A natural approach would be to use an algorithm that achieves a \((\alpha,\beta)\)-approximation at a certain time in the stream with constant probability, e.g., [59], boost the probability of success to \(1-\frac{1}{\operatorname{poly}(n)}\), and the union bound to argue correctness over all times in the stream. However, a subtle pitfall here is that the rings and groups in the offline coreset construction of [27] are with respect to a specific \((\alpha,\beta)\)-approximation. Hence their analysis would no longer hold if a point \(x_{t}\) was assigned to cluster \(i_{1}\) at time \(t\) when the sampling process occurs but then assigned to cluster \(i_{2}\) at the end of the stream. Therefore, we require a consistent \((\alpha,\beta)\)-approximation, so that once the algorithm assigns point \(x_{t}\) to cluster \(i\), then the point \(x_{t}\) will always remain in cluster \(i\) even if a newer and closer center is subsequently opened later in the stream. To that end, we invoke a result of [34] that analyzes the popular Meyerson online facility location algorithm, along with a standard guess-and-double approach for estimating the input parameter to the Meyerson subroutine.

Lower bound.The intuition for our lower bound that any \((1+\varepsilon)\)-online coreset for \((k,z)\)-clustering requires \(\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) is somewhat straightforward and in a black-box manner. We first observe that [25] showed the existence of a set \(X\) of \(\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) unit vectors in \(\mathbb{R}^{d}\) such that any coreset with \(o\left(\frac{k}{\varepsilon^{2}}\right)\) samples provably cannot accurately estimate the \((k,z)\)-clustering cost for a set \(C\) of \(k\) unit vectors.

Since an online \((1+\varepsilon)\)-coreset must answer queries on all prefixes of the stream, we embed \(\Omega(\log n)\) instances of \(X\). We first increase the dimension by a \(\log n\) factor so that each of these instances can have disjoint support. We then give each of the instances increasingly exponential weight to force the data structure to sample \(\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) points for each instance. Specifically, we insert \(\tau^{i}\) copies of the \(i\)-th instance of \(X\), where \(\tau>1\) is some constant. Because the weight of the \(i\)-th instance is substantially greater than the sum of the weights of all previous instances, then any \((1+\varepsilon)\)-online coreset must essentially be a \((1+\varepsilon)\)-offline coreset for the \(i\)-th instance, thus requiring \(\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) points for the \(i\)-th instance. This reasoning extends to all \(\Omega(\log n)\) instances, thus showing that any online \((1+\varepsilon)\)-coreset requires \(\Omega\left(\frac{k}{\varepsilon^{2}}\log n\right)\) points.

## 2 Algorithm

In this section, we describe our sliding window algorithm for \(k\)-clustering. We first overview the construction of an online \((1+\varepsilon)\) coreset for \((k,z)\)-clustering under general discrete metrics. We then describe how our online coreset construction for \((k,z)\)-clustering on general discrete metric spaces can be used to achieve near-optimal space algorithms for \((k,z)\)-clustering in the sliding window model.

Online \((1+\varepsilon)\)-coreset.We first recall the following properties from the Meyerson sketch, which we formally introduce in A.

**Theorem 2.1**.: _[_8_]_ _Given an input stream \(x_{1},\ldots,x_{n}\in\mathbb{R}^{d}\) defining a set \(X\subset[\Delta]^{d}\), there exists an online algorithm MultMeyerson that with probability at least \(1-\frac{1}{\operatorname{poly}(n)}\):_1. _on the arrival of each point_ \(x_{i}\)_, assigns_ \(x_{i}\) _to a center in_ \(C\) _through a mapping_ \(\pi:X\to C\)_, where_ \(C\) _contains at most_ \(O\left(2^{2z}k\log n\log\Delta\right)\) _centers_
2. \(\sum_{x\in X}\|x_{i}-\pi(x_{i})\|_{2}^{z}\leq 2^{z+7}\operatorname{Cost}_{|S| \leq k}(X,S)\)__
3. MultMeyerson _uses_ \(O\left(2^{z}k\log^{3}(nd\Delta)\right)\) _words of space_

We also use the following notation, adapted from [27] to the online setting.

Let \(\mathcal{A}\) be an \((\alpha,\beta)\)-approximation for a \(k\)-means clustering on an input set \(X\subseteq[\Delta]^{d}\) and let \(C_{1},\ldots,C_{\beta k}\) be the clusters of \(X\) induced by \(\mathcal{A}\). Suppose the points of \(X\) arrive in a data stream \(S\). For a fixed \(\varepsilon>0\), define the following notions of rings and groups:

* The average cost of cluster \(C_{i}\) is denoted by \(\kappa_{C_{i}}:=\frac{\operatorname{Cost}(C_{i},\mathcal{A})}{|C_{i}|}\).
* For any \(i,j\), the ring \(R_{i,j}\) is the set of points \(x\in C_{i}\) such that \(2^{j}\kappa_{C_{i}}\leq\operatorname{Cost}(x,\mathcal{A})<2^{j+1}\kappa_{C_{i}}\). For any \(j\), \(R_{j}=\cup R_{i,j}\).
* The inner ring \(R_{I}(C_{i})=\cup_{j\leq 2z\log\frac{z}{z}}R_{i,j}\) is the set of points of \(C_{i}\) with cost at most \(\left(\frac{\varepsilon}{z}\right)^{2z}\kappa_{C_{i}}\). More generally for a solution \(\mathcal{S}\), let \(R_{I}^{\mathcal{S}}\) denote the union of the inner rings induced by \(\mathcal{S}\).
* The outer ring \(R_{O}(C_{i})=\cup_{j\geq 2z\log\frac{z}{z}}R_{i,j}\) is the set of points of \(C_{i}\) with cost at least \(\left(\frac{\varepsilon}{z}\right)^{2z}\kappa_{C_{i}}\). More generally for a solution \(\mathcal{S}\), let \(R_{O}^{\mathcal{S}}\) denote the union of the outer rings induced by \(\mathcal{S}\).
* The main ring \(R_{M}(C_{i})\) is the set of points of \(C_{i}\) that are not in the inner or outer rings, i.e., \(R_{M}(C_{i})=C_{i}\setminus(R_{I}(C_{i})\cup R_{O}(C_{i})\).
* For any \(j\), the group \(G_{j,b}\) consists of the \((2^{b-1}+1)\)-th to \((2^{b})\)-th points of each ring \(R_{i,j}\) that arrive in \(S\).
* For any \(j\), we use \(G_{j,\min}\) to denote the union of the groups with the smallest costs, i.e., \[G_{j,\min}=\left\{x|\exists i,x\in R_{i,j},\operatorname{Cost}(R_{i,j}, \mathcal{A})<2\left(\frac{\varepsilon}{4z}\right)^{z}\frac{\operatorname{Cost }(R_{j},\mathcal{A})}{\beta k}\right\}.\]
* The outer groups \(G_{b}^{O}\) partition the outer rings \(R_{O}^{\mathcal{A}}\) so that \[G_{b}^{O}=\left\{x|\exists i,x\in C_{i},\left(\frac{\varepsilon}{4z}\right) ^{z}\frac{\operatorname{Cost}(R_{O}^{\mathcal{A}},\mathcal{A})}{\beta k}\cdot 2^ {b}\leq\operatorname{Cost}(R_{O}(C_{i}),\mathcal{A})<\left(\frac{\varepsilon} {4z}\right)^{z}\frac{\operatorname{Cost}(R_{O}^{\mathcal{A}},\mathcal{A})}{ \beta k}\cdot 2^{b+1}\right\}.\]
* We define \(G_{\min}^{O}=\cup_{b\leq 0}G_{b}^{O}\) and \(G_{\max}^{O}=\cup_{b\geq z\log\frac{4z}{\varepsilon}}G_{b}^{O}\).

```
0: Points \(x_{1},\ldots,x_{n}\in[\Delta]^{d}\)
0: A set \(W\) of weighted points and timestamps
1: Initiate an instance of \((\alpha,\beta)\)-bicriteria algorithm MultMeyerson
2:\(\gamma\leftarrow\frac{C\max(\alpha^{2},\alpha^{\alpha})\beta}{\min(\varepsilon ^{2},\varepsilon^{2})}\log^{2}\frac{1}{\varepsilon}\left(k\log|\mathbb{C}|+ \log\log\frac{1}{\varepsilon}+\log n\right)\log^{2}\frac{1}{\varepsilon}\)
3:\(W\leftarrow\emptyset\)
4:for each point \(x_{t}\), \(t\in[n]\)do
5: Let \(c_{i}\) be the center assigned for \(x_{t}\) by MultMeyerson
6: Let \(2^{j}\leq\|x_{t}-c_{i}\|_{2}^{z}<2^{j+1}\) for \(j\in\mathbb{Z}\)
7: Let \(b\in\mathbb{Z}\) so that the number of points in \(R_{i,j}\) is between \(2^{b-1}+1\) and \(2^{b}\)
8: Let \(r_{t}\) be the number of points in \(G_{j,b}\) at time \(t\)
9:\(p_{x}\leftarrow\min\left(\frac{4}{r_{t}}\cdot\gamma\log n,1\right)\)
10: With probability \(p_{x}\), add \(x\) to \(W\) with timestamp \(t\) and weight \(\frac{1}{p_{x}}\)
11:return\(W\)
```

**Algorithm 1**RingSample

We then adapt the offline coreset construction of [27] to an online setting at the cost of logarithmic overheads, which suffice for our purpose. The algorithm (Algorithm 1) has the following guarantees:

**Lemma 2.2**.: _Let \(\mathbb{C}\) be an \(\mathcal{A}\)-approximate centroid set for a fixed group \(G\). There exists an algorithm RingSample that samples_

\[O\left(\frac{\max(\alpha^{2},\alpha^{2})\beta}{\min(\varepsilon^{2},\varepsilon^ {2})}\log^{2}\frac{1}{\varepsilon}\left(k\log|\mathbb{C}|+\log\log\frac{1}{ \varepsilon}+\log n\right)\log^{2}n\log^{2}\Delta\log^{2}\frac{1}{\varepsilon}\right)\]

_points and with high probability, outputs a \((1+\varepsilon)\)-online coreset for the \(k\)-means clustering problem._

Informally, an approximate centroid set is a set of possible points so that taking the centers from this set generates an approximately accurate solution (see Appendix B for a formal definition). To bound \(\log|\mathbb{C}|\), we construct and apply a terminal embedding to project each point to a lower dimension and then appeal to known bounds for approximate centroid sets in low-dimensional Euclidean, thereby giving our online coreset algorithm with the guarantees of Theorem 1.3.

Sliding window model.We first recall a standard approach for using offline coreset constructions for insertion-only streaming algorithms. Suppose there exists a randomized algorithm that produces an online coreset algorithm that uses \(S(n,\varepsilon,\delta)\) points for an input stream of length \(n\), accuracy \(\varepsilon\), and failure probability \(\delta\), where for the ease of discussion, we omit additional dependencies. A standard approach for using coresets on insertion-only streams is the merge-and-reduce approach, which partitions the stream into blocks of size \(S\left(n,\frac{\varepsilon}{2\log n},\frac{\delta}{\mathrm{poly}(n)}\right)\) and builds a coreset for each block. Each coreset is then viewed as the leaves of a binary tree with height at most \(\log n\), since the binary tree has at most \(n\) leaves. Then at each level of the binary tree, for each node in the level, a coreset of size \(S\left(n,\frac{\varepsilon}{2\log n},\frac{\delta}{\mathrm{poly}(n)}\right)\) is built from the coresets representing the two children of the node. Due to the mergeability property of coresets, the coreset at the root of the tree will be a coreset for the entire stream with accuracy \(\left(1+\frac{\varepsilon}{2\log n}\right)^{\log n}\leq(1+\varepsilon)\) and failure probability \(\delta\).

This approach fails for sliding window algorithms because the elements at the beginning of the data stream can expire, and so coresets corresponding to earlier blocks of the stream may no longer accurate, which would result in the coreset at the root of the tree also no longer being accurate. On the other hand, suppose we partition the stream into blocks consisting of \(S\left(n,\frac{\varepsilon}{2\log n},\frac{\delta}{\mathrm{poly}(n)}\right)\) elements as before, but instead of creating an offline coreset for each block, we can create an online coreset for the elements _in reverse_. That is, since the elements in each block are explicitly stored, we can create offline an artificial stream consisting of the elements in the block in reverse and then give the artificial stream as input to the online coreset construction. Note that if we also first consider the "latter" coreset when merging two coresets, then this effectively reverses the stream. Moreover, by the correctness of the online coreset, our data structure provides correctness over any prefix of the reversed stream, or equivalently, any suffix of the stream and specifically, correctness over the sliding window. We thus further adapt the merge-and-reduce framework to show that randomized online coresets for problems in clustering can also be used to achieve randomized algorithms for the corresponding problems in the sliding window model. We formalize this approach in Algorithm 2.

**Theorem 2.3**.: _Let \(x_{1},\ldots,x_{n}\) be a stream of points in \([\Delta]^{d}\), \(\varepsilon>0\), and let \(X=\{x_{n-W+1},\ldots,x_{n}\}\) be the \(W\) most recent points. Suppose there exists a randomized algorithm that with probability at least \(1-\delta\), outputs an online coreset algorithm for a \(k\)-clustering problem with \(S(n,d,k,\varepsilon,\delta)\) points. Then there exists a randomized algorithm that with probability at least \(1-\delta\), outputs a coreset for the \(k\)-clustering problem in the sliding window model with \(O\left(S\left(n,d,k,\frac{\varepsilon}{\log n},\frac{\delta}{n^{2}}\right) \log n\right)\) points._

By Theorem 1.3 and Theorem 2.3, we have:

**Theorem 2.4**.: _There exists an algorithm that samples \(\frac{k}{\min(\varepsilon^{4},\varepsilon^{2+\varepsilon})}\ \mathrm{polylog}\ \frac{n \Delta}{\varepsilon}\) points and with high probability, outputs a \((1+\varepsilon)\)-coreset to \((k,z)\)-clustering in the sliding window model._

Using an offline algorithm for \((k,z)\)-clustering for post-processing after the data stream, we have Theorem 1.1.

## 3 Experimental Evaluations

In this section, we conduct simple empirical demonstrations as proof-of-concepts to illustrate the benefits of our algorithm. Our empirical evaluations were conducted using Python 3.10 using a 64-bitoperating system on an AMD Ryzen 7 5700U CPU, with 8GB RAM and 8 cores with base clock 1.80 GHz. The general approach to our experiments is to produce a data stream \(S\) that defines dataset \(X\), whose generation we describe below, as well as in Appendix F. We then compare the performance of a simplified version of our algorithm with various state-of-the-art baselines.

Baselines.Our first baseline (denoted off for offline) is the simple Lloyd's algorithm on the entire dataset \(X\), with multiple iterations using the k-means++ initialization. This is a standard approach for finding a good approximation to the optimal clustering cost, because finding the true optimal centers requires exponential time. Because this offline Lloyd's algorithm has access to the entire dataset, the expected behavior is that this algorithm will have the best objective, i.e., smallest clustering cost. However, we emphasize that this algorithm requires storing the entire dataset \(X\) in memory and thus its input size is significantly larger than the sublinear space algorithms.

To compare with the offline Lloyd's algorithm, we run a number of sublinear space algorithms. These algorithms generally perform some sort of processing on the datastream \(X\) to create a coreset \(C\). We normalize the space requirement of these algorithms by permitting each algorithm to store \(m\) points across specific ranges of \(m\). We then run Lloyd's algorithm on the coreset \(C\), with the same number of iterations using the k-means++ initialization.

Our first sublinear space algorithm is uniform sampling on the dataset \(X\). That is, we form \(C\) by uniformly sampling \(m\) points from \(X\), before running Lloyd's algorithm. We use uni to denote this algorithm whose first step is based on uniformly sampling. Our second sublinear space algorithm is the importance sampling approach used by histogram-based algorithms, e.g., [15, 11, 8]. These algorithms perform importance sampling, i.e., sample points into the coreset \(C\) with probability proportional to their distances from existing samples and delete points once the clustering cost of \(C\) is much higher than the clustering cost of the dataset \(X\). We use \(\mathsf{hist(ogram)}\) to denote this algorithm that is based on the histogram frameworks for sliding windows.

Our final algorithm is a simplification of our algorithm. As with the histogram-based algorithm, we perform importance sampling on the stream \(S\) to create the coreset \(C\) of size \(m\). Thus we do not implement the ring and group sampling subroutines in our full algorithm. However, the crucial difference compared to the histogram-based approach is that we forcefully discard points of \(C\) that have expired. We use \(\mathsf{imp}\) to denote this algorithm whose first step is based on importance sampling.

Dataset.We first describe the methodology and experimental setup of our empirical evaluation on a real-world dataset with an amount of synthetic noise before detailing the experimental results. The first component of our dataset consists of the points of the SKIN (Skin Segmentation) dataset \(X^{\prime}\) from the publicly available UCI repository [6], which was also used in the experiments of [8]. The dataset \(X^{\prime}\) consists of \(245,057\) points with four features, where each point refers to a separate image, such that the first three features are constructed over BGR space, and the fourth feature is the label for whether or not the image refers to a skin sample. We subsequently pre-process each dataset to have zero mean and unit standard deviation in each dimension.

We then form our dataset \(X\) by augmenting \(X^{\prime}\) with \(201\) points in four-dimensional space, where \(100\) of these points were drawn from a spherical Gaussian with unit standard deviation in each direction and centered at \((-10,10,0,0)\) and \(100\) of these points were drawn from a spherical Gaussian with unit standard deviation in each direction and centered at \((10,-10,0,0)\). The final point of \(X\) was drawn from a spherical Gaussian with unit standard deviation centered at \((500,500,0,0)\). Thus our dataset \(X\) has dimensions \(n=245,258\) and \(d=4\). We then create the data stream \(S\) by prepending two additional points drawn from spherical Gaussians with standard deviation \(2.75\) centered at \((-10,10,0,0)\) and \((-10,-10,0,0)\) respectively, so that the stream has length \(245,260\). We set the window length to be \(245,258\) in accordance with the "true" data set, so that the first two points of the stream will be expired by the data stream.

Experimental setup.For each of the instances of Lloyd's algorithm, either on the entire dataset \(X\) or the sampled coreset \(C\), we use 10 iterations using the k-means++ initialization. While the offline Lloyd's algorithm stores the entire dataset \(X\) of \(245,258\) points in memory, we only allow each of the sublinear-space algorithms to store a fixed \(m\) points. We compare the algorithms across \(m\in\{5,10,15,20,25,30\}\) and \(k\in\{2,3,4,5,6,7,8,9,10\}\). Note that in the original dataset, each of the points has a label for either skin or non-skin, which would be reasonable for \(k=2\). However, due to the artificial structure possibly induced by the synthetic noise, it also makes sense to other values of \(k\). In particular, preliminary experiments from uniform sampling by the elbow method indicated that \(k=3\) would be a reasonable setting. Thus we fix \(k=3\) while varying \(m\in\{5,10,15,20,25,30\}\) and we arbitrarily fix \(m=25\) while varying \(k\in\{2,3,4,5,6,7,8,9,10\}\).

Experimental results.For each choice of \(m\) and \(k\), we ran each algorithm \(30\) times and tracked the resulting clustering cost. Our algorithm demonstrated superior performance than the other sublinear-space algorithms across all values of \(m\in\{5,10,15,20,25,30\}\) and \(k\in\{2,3,4,5,6,7,8,9,10\}\), and was even quite competitive with the offline Lloyd's algorithm, even though our algorithm only used memory size \(m\leq 30\), while the offline algorithm used memory \(245,258\).

Uniform sampling performed well for \(k=2\), which in some case captures the structure imposed on the data through the skin vs. non-skin label, but for larger \(k\), the optimal solutions start placing centers to handle the synthetic noise, which may not be sampled by uniform sampling. Thus uniform sampling performed relatively poorly albeit quite stably for larger \(k\). In contrast, the histogram-based algorithm performed poorly for small \(k\) across all our ranges of \(m\), due to sampling the extra points in \(S\setminus X\), so that the resulting Lloyd's algorithm on \(C\) moved the centers far away from the optimal centers of \(X\). On the other hand, the histogram-based algorithm performed well for larger \(k\), likely due to additional centers that could be afforded to handle the points in \(S\setminus X\). We plot our results in Figure 1 and defer additional experiments to Appendix F.

## 4 Conclusion

In this paper, we give an algorithm outputs a \((1+\varepsilon)\)-approximation to \((k,z)\)-clustering in the sliding window model, while using \(\frac{k}{\min(\varepsilon^{4},\varepsilon^{2+z})}\) polylog\(\frac{n\Delta}{\varepsilon}\) words of space when the points are from \([\Delta]^{d}\). Our algorithm not only improves on a line of work [5, 16, 34, 8, 35], but also nearly matches the space used by the offline coreset constructions of [27], which is known to be near-optimal in light of a \(\Omega\left(\frac{k}{\varepsilon^{2+z}}\right)\) lower bound for the size of an offline coreset [45].

We also give a lower bound that shows a logarithmic overhead in the number of points is needed to maintain a \((1+\varepsilon)\)-online coreset compared to a \((1+\varepsilon)\)-coreset. That is, we gave in Theorem 1.4 a set \(X\subset[\Delta]^{d}\) of \(n\) points \(x_{1},\ldots,x_{n}\) such that any \((1+\varepsilon)\)-online coreset for \(k\)-means clustering on \(X\) requires \(\Omega\left(\frac{k}{\varepsilon^{2}}\log n\right)\) points. However, this does not rule out whether the \(\log n\) overhead is necessary for \((k,z)\)-clustering in the sliding window model, since a sliding window algorithm does not necessarily need to maintain an online coreset. We leave this question as a possible direction for future work.

## Acknowledgments

David P. Woodruff and Samson Zhou were partially supported by a Simons Investigator Award and by the National Science Foundation under Grant No. CCF-1815840. This work was done in part while Samson Zhou was at Carnegie Mellon University, UC Berkeley, and Rice University.

## References

* [1] Marcel R. Ackermann, Marcus Martens, Christoph Raupach, Kamil Swierkot, Christiane Lammersen, and Christian Sohler. Streamkm++: A clustering algorithm for data streams. _ACM J. Exp. Algorithmics_, 17(1), 2012.
* [2] Miklos Ajtai, Vladimir Braverman, T. S. Jayram, Sandeep Silwal, Alec Sun, David P. Woodruff, and Samson Zhou. The white-box adversarial data stream model. In _PODS '22: International Conference on Management of Data, 2022_, pages 15-27, 2022.
* [3] Apple. [https://images.apple.com/privacy/docs/Differential_Privacy_Overview.pdf](https://images.apple.com/privacy/docs/Differential_Privacy_Overview.pdf).
* [4] Brian Babcock, Shivnath Babu, Mayur Datar, Rajeev Motwani, and Jennifer Widom. Models and issues in data stream systems. In _Proceedings of the Twenty-first ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems_, pages 1-16, 2002.
* [5] Brian Babcock, Mayur Datar, Rajeev Motwani, and Liadan O'Callaghan. Maintaining variance and k-medians over data stream windows. In _Proceedings of the Twenty-Second ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems_, pages 234-243, 2003.
* [6] Rajen Bhatt and Abhinav Dhall. Skin segmentation dataset. UCI Learning Repository.
* [7] Jeremiah Blocki, Seunghoon Lee, Tamalika Mukherjee, and Samson Zhou. Differentially private \(l_{2}\)-heavy hitters in the sliding window model. In _The Eleventh International Conference on Learning Representations, ICLR_, 2023.
* [8] Michele Borassi, Alessandro Epasto, Silvio Lattanzi, Sergei Vassilvitskii, and Morteza Zadimoghaddam. Sliding window algorithms for k-clustering problems. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS_, 2020.

Figure 1: Comparison of average clustering costs made by uniform sampling, histogram-based algorithm, and our coreset-based algorithm across various settings of space allocated to the algorithm, given a synthetic dataset. For comparison, we also include the offline k-means++ algorithm as a baseline, though it is inefficient because it stores the entire dataset.

* [9] Vladimir Braverman, Petros Drineas, Cameron Musco, Christopher Musco, Jalaj Upadhyay, David P. Woodruff, and Samson Zhou. Near optimal linear algebra in the online and sliding window models. In _61st IEEE Annual Symposium on Foundations of Computer Science, FOCS_, pages 517-528, 2020.
* [10] Vladimir Braverman, Dan Feldman, Harry Lang, Adiel Statman, and Samson Zhou. Efficient coreset constructions via sensitivity sampling. In _Asian Conference on Machine Learning, ACML_, pages 948-963, 2021.
* [11] Vladimir Braverman, Gereon Frahling, Harry Lang, Christian Sohler, and Lin F. Yang. Clustering high dimensional dynamic data streams. In _Proceedings of the 34th International Conference on Machine Learning, ICML_, pages 576-585, 2017.
* [12] Vladimir Braverman, Ran Gelles, and Rafail Ostrovsky. How to catch \(l_{2}\)-heavy-hitters on sliding windows. _Theor. Comput. Sci._, 554:82-94, 2014.
* [13] Vladimir Braverman, Elena Grigorescu, Harry Lang, David P. Woodruff, and Samson Zhou. Nearly optimal distinct elements and heavy hitters on sliding windows. In _Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM_, pages 7:1-7:22, 2018.
* [14] Vladimir Braverman, Avinatan Hassidim, Yossi Matias, Mariano Schain, Sandeep Silwal, and Samson Zhou. Adversarial robustness of streaming algorithms through importance sampling. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems, NeurIPS_, pages 3544-3557, 2021.
* [15] Vladimir Braverman, Harry Lang, Keith Levin, and Morteza Monemizadeh. Clustering on sliding windows in polylogarithmic space. In _35th IARCS Annual Conference on Foundation of Software Technology and Theoretical Computer Science, FSTTCS_, pages 350-364, 2015.
* [16] Vladimir Braverman, Harry Lang, Keith Levin, and Morteza Monemizadeh. Clustering problems on sliding windows. In _Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA_, pages 1374-1390, 2016.
* [17] Vladimir Braverman, Adam Meyerson, Rafail Ostrovsky, Alan Roytman, Michael Shindler, and Brian Tagiku. Streaming k-means on well-clusterable data. In _Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA_, pages 26-40, 2011.
* [18] Vladimir Braverman and Rafail Ostrovsky. Smooth histograms for sliding windows. In _48th Annual IEEE Symposium on Foundations of Computer Science (FOCS), Proceedings_, pages 283-293, 2007.
* [19] Vladimir Braverman, Rafail Ostrovsky, and Carlo Zaniolo. Optimal sampling from sliding windows. _J. Comput. Syst. Sci._, 78(1):260-272, 2012.
* 27th International Conference, COCOON, Proceedings_, pages 528-539, 2021.
* [21] Moses Charikar, Liadan O'Callaghan, and Rina Panigrahy. Better streaming algorithms for clustering problems. In _Proceedings of the 35th Annual ACM Symposium on Theory of Computing_, pages 30-39, 2003.
* [22] Jiecao Chen, Huy L. Nguyen, and Qin Zhang. Submodular maximization over sliding windows. _CoRR_, abs/1611.00129, 2016.
* [23] Ke Chen. On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications. _SIAM J. Comput._, 39(3):923-947, 2009.
* [24] Vincent Cohen-Addad, 2022. Private communication.
* [25] Vincent Cohen-Addad, Kasper Green Larsen, David Saulpic, and Chris Schwiegelshohn. Towards optimal lower bounds for k-median and k-means coresets. In _STOC '22: 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1038-1051, 2022.

* [26] Vincent Cohen-Addad, Kasper Green Larsen, David Saulpic, Chris Schwiegelshohn, and Omar Ali Sheikh-Omar. Improved coresets for euclidean k-means. In _NeurIPS_, 2022.
* [27] Vincent Cohen-Addad, David Saulpic, and Chris Schwiegelshohn. A new coreset framework for clustering. In _STOC: 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 169-182, 2021.
* [28] Vincent Cohen-Addad, Chris Schwiegelshohn, and Christian Sohler. Diameter and k-center in sliding windows. In _43rd International Colloquium on Automata, Languages, and Programming, ICALP_, pages 19:1-19:12, 2016.
* [29] Vincent Cohen-Addad, David P. Woodruff, and Samson Zhou. Streaming euclidean k-median and k-means with o(log n) space. _CoRR_, abs/2310.02882, 2023.
* [30] Graham Cormode. The continuous distributed monitoring model. _SIGMOD Rec._, 42(1):5-14, 2013.
* [31] Graham Cormode and Minos N. Garofalakis. Streaming in a connected world: querying and tracking distributed data streams. In _EDBT 2008, 11th International Conference on Extending Database Technology, Proceedings_, page 745, 2008.
* [32] Graham Cormode and S. Muthukrishnan. What's new: finding significant differences in network data streams. _IEEE/ACM Trans. Netw._, 13(6):1219-1232, 2005.
* [33] Mayur Datar, Aristides Gionis, Piotr Indyk, and Rajeev Motwani. Maintaining stream statistics over sliding windows. _SIAM J. Comput._, 31(6):1794-1813, 2002.
* [34] Alessandro Epasto, Silvio Lattanzi, Sergei Vassilvitskii, and Morteza Zadimoghaddam. Submodular optimization over sliding windows. In _Proceedings of the 26th International Conference on World Wide Web, WWW_, pages 421-430, 2017.
* [35] Alessandro Epasto, Mohammad Mahdian, Vahab S. Mirrokni, and Peilin Zhong. Improved sliding window algorithms for clustering and coverage via bucketing-based sketches. In _Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA_, pages 3005-3042, 2022.
* [36] Facebook. [https://www.facebook.com/policy.php](https://www.facebook.com/policy.php).
* [37] Dan Feldman and Michael Langberg. A unified framework for approximating and clustering data. In _Proceedings of the 43rd ACM Symposium on Theory of Computing, STOC_, pages 569-578, 2011.
* [38] Dan Feldman, Melanie Schmidt, and Christian Sohler. Turning big data into tiny data: Constant-size coresets for k-means, pca, and projective clustering. _SIAM J. Comput._, 49(3):601-657, 2020.
* [39] Dan Feldman and Leonard J. Schulman. Data reduction for weighted and outlier-resistant clustering. In _Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms, SODA_, pages 1343-1354, 2012.
* [40] Zhili Feng, Praneeth Kacham, and David P. Woodruff. Strong coresets for subspace approximation and k-median in nearly linear time. _CoRR_, abs/1912.12003, 2019.
* [41] Google. [https://policies.google.com/technologies/retention](https://policies.google.com/technologies/retention).
* [42] Sudipto Guha, Nina Mishra, Rajeev Motwani, and Liadan O'Callaghan. Clustering data streams. In _41st Annual Symposium on Foundations of Computer Science, FOCS_, pages 359-366, 2000.
* [43] Sariel Har-Peled and Akash Kushal. Smaller coresets for k-median and k-means clustering. _Discret. Comput. Geom._, 37(1):3-19, 2007.
* [44] Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In _Proceedings of the 36th Annual ACM Symposium on Theory of Computing_, pages 291-300, 2004.

* [45] Lingxiao Huang, Jian Li, and Xuan Wu. Towards optimal coreset construction for (k, z)-clustering: Breaking the quadratic dependency on k. _CoRR_, abs/2211.11923, 2022.
* [46] Lingxiao Huang and Nisheeth K. Vishnoi. Coresets for clustering in euclidean spaces: importance sampling is nearly optimal. In _Proccedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC_, pages 1416-1429, 2020.
* [47] Rajesh Jayaram, David P. Woodruff, and Samson Zhou. Truly perfect samplers for data streams and sliding windows. In _PODS '22: International Conference on Management of Data_, pages 29-40, 2022.
* [48] Lap-Kei Lee and H. F. Ting. Maintaining significant stream statistics over sliding windows. In _Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA_, pages 724-732, 2006.
* [49] Lap-Kei Lee and H. F. Ting. A simpler and more efficient deterministic scheme for finding frequent items over sliding windows. In _Proceedings of the Twenty-Fifth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems_, pages 290-297, 2006.
* [50] J MacQueen. Classification and analysis of multivariate observations. In _5th Berkeley Symp. Math. Statist. Probability_, pages 281-297, 1967.
* [51] Konstantin Makarychev, Yury Makarychev, and Ilya P. Razenshteyn. Performance of johnson-lindenstrauss transform for \(k\)-means and \(k\)-medians clustering. In _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC_, pages 1027-1038, 2019.
* [52] Gurmeet Singh Manku and Rajeev Motwani. Approximate frequency counts over data streams. _Proc. VLDB Endow._, 5(12):1699, 2012.
* [53] Jiri Matousek. On approximate geometric k-clustering. _Discret. Comput. Geom._, 24(1):61-84, 2000.
* [54] Adam Meyerson. Online facility location. In _42nd Annual Symposium on Foundations of Computer Science, FOCS_, pages 426-431. IEEE Computer Society, 2001.
* [55] Shyam Narayanan and Jelani Nelson. Optimal terminal dimensionality reduction in euclidean space. In _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC_, pages 1064-1069, 2019.
* [56] Miles Osborne, Sean Moran, Richard McCreadie, Alexander von Lunen, Martin D. Sykora, Amparo Elizabeth Cano, Neil Ireson, Craig Macdonald, Iadh Ounis, Yulan He, Tom Jackson, Fabio Ciravegna, and Ann O'Brien. Real-time detection, tracking, and monitoring of automatically discovered events in social media. In _Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL_, pages 37-42, 2014.
* [57] Odysseas Papapetrou, Minos N. Garofalakis, and Antonios Deligiannakis. Sketching distributed sliding-window data streams. _VLDB J._, 24(3):345-368, 2015.
* [58] Christian Sohler and David P. Woodruff. Strong coresets for k-median and subspace approximation: Goodbye dimension. In _59th IEEE Annual Symposium on Foundations of Computer Science, FOCS_, pages 802-813, 2018.
* [59] Zhao Song, Lin F. Yang, and Peilin Zhong. Sensitivity sampling over dynamic geometric data streams with applications to k-clustering. _CoRR_, abs/1802.00459, 2018.
* [60] Hugo Steinhaus et al. Sur la division des corps materiels en parties. _Bull. Acad. Polon. Sci_, 1(804):801, 1956.
* [61] Murad Tukan, Xuan Wu, Samson Zhou, Vladimir Braverman, and Dan Feldman. New coresets for projective clustering and applications. In _International Conference on Artificial Intelligence and Statistics, AISTATS_, pages 5391-5415, 2022.

* [62] Zhewei Wei, Xuancheng Liu, Feifei Li, Shuo Shang, Xiaoyong Du, and Ji-Rong Wen. Matrix sketching over sliding windows. In _Proceedings of the 2016 International Conference on Management of Data, SIGMOD Conference_, pages 1465-1480. ACM, 2016.
* [63] David P. Woodruff and Samson Zhou. Tight bounds for adversarially robust streams and sliding windows via difference estimators. In _62nd IEEE Annual Symposium on Foundations of Computer Science, FOCS_, pages 1183-1196, 2021.

Preliminaries

For a positive integer \(n\), we use the notation \([n]\) to denote the set \(\{1,\ldots,n\}\). Similarly, we use \([\Delta]^{d}\) to denote \(\{1,\ldots,\Delta\}^{d}\). We use \(\mathrm{poly}(n)\) to denote a fixed polynomial in \(n\) with degree determined as necessary by setting the appropriate constants in corresponding variables. Similarly, we use \(\mathrm{polylog}(n)\) to denote \(\mathrm{poly}(\log n)\). We suppress polylogarithmic dependencies by writing \(\hat{O}\left(f(\cdot)\right)=O\left(f(\cdot)\right)\ \mathrm{polylog}\,f(\cdot)\).

For \((k,z)\)-clustering on a set \(X=\{x_{1},\ldots,x_{n}\}\subset\mathbb{R}^{d}\) using a set \(C\) of \(k\) centers and a distance function \(\mathrm{dist}(\cdot,\cdot)\), we define the notation \(\mathrm{Cost}(X,C)=\sum_{i=1}^{n}\min_{e\in C}\mathrm{dist}(x_{i},c)^{z}\). We also define the notation \(\mathrm{Cost}_{|S|\leq k}(X,S):=\min_{S:|S|\leq k}\mathrm{Cost}(X,S)\), so that \(\mathrm{Cost}_{|S|\leq k}\) is the cost of an optimal \((k,z)\)-clustering.

**Definition A.1** (\((\alpha,\beta)\)-approximation).: _We say a set of centers \(C\) provides an \((\alpha,\beta)\)-approximation to the optimal \(k\)-means clustering on a set \(X\) if \(|C|\leq\beta k\) and_

\[\mathrm{Cost}(X,C)\leq\alpha\mathsf{OPT}.\]

**Definition A.2** (Coreset).: _A coreset for \((k,z)\)-clustering on an approximation parameter \(\varepsilon>0\) and a set \(X\) of points \(x_{1},\ldots,x_{n}\in\mathbb{R}^{d}\) with distance function \(\mathrm{dist}\) is a subset \(S\) of weighted points of \(X\) with weight function \(w\) such that for any set \(C\) of \(k\) points, we have_

\[(1-\varepsilon)\sum_{i=1}^{n}\mathrm{dist}(x_{i},C)^{z}\leq\sum_{q\in S}w(q) \,\mathrm{dist}(q,S)^{z}\leq(1+\varepsilon)\sum_{i=1}^{n}\mathrm{dist}(x_{i},C )^{z}.\]

**Definition A.3** (Online Coreset).: _An online coreset for \((k,z)\)-clustering on an approximation parameter \(\varepsilon>0\) and a set \(X\) of points \(x_{1},\ldots,x_{n}\in\mathbb{R}^{d}\) with distance function \(\mathrm{dist}\) is a subset \(S\) of weighted points of \(X\) with weight function \(w\) such that for any set \(C\) of \(k\) points and for any \(t\in[n]\), we have_

\[(1-\varepsilon)\sum_{i=1}^{t}\mathrm{dist}(x_{i},C)^{z}\leq\sum_{q\in S_{t}}w( q)\,\mathrm{dist}(q,S_{t})^{z}\leq(1+\varepsilon)\sum_{i=1}^{t}\mathrm{dist}(x _{i},C)^{z},\]

_where \(S_{t}=S\cap\{X_{1},\ldots,X_{t}\}\), i.e., the subset of \(S\) that has arrived at time \(t\)._

**Theorem A.4** (Bernstein's inequality).: _Let \(X_{1},\ldots,X_{n}\) be independent random variables such that \(\mathbb{E}[X_{i}^{2}]<\infty\) and \(X_{i}\geq 0\) for all \(i\in[n]\). Let \(X=\sum_{i}X_{i}\) and \(\gamma>0\). Then_

\[\mathbf{Pr}\left[X\leq\mathbb{E}[X]-\gamma\right]\leq\exp\left(\frac{-\gamma^{ 2}}{2\sum_{i}\mathbb{E}[X_{i}^{2}]}\right).\]

_If \(X_{i}-\mathbb{E}[X_{i}]\leq\Delta\) for all \(i\), then for \(\sigma_{i}^{2}=\mathbb{E}[X_{i}^{2}]-\mathbb{E}[X_{i}]^{2}\),_

\[\mathbf{Pr}\left[X\geq\mathbb{E}[X]+\gamma\right]\leq\exp\left(\frac{-\gamma^{ 2}}{2\sum_{i}\sigma_{i}^{2}+2\gamma\Delta/3}\right).\]

Meyerson sketch.We briefly review the Meyerson sketch [54] and the relevant properties that we need from the Meyerson sketch. The Meyerson sketch provides an \((\alpha,\beta)\)-approximation to \((k,z)\)-clustering on a data stream of points \(x_{1},\ldots,x_{n}\in[\Delta]^{d}\) with \(\alpha=2^{z+7}\) and \(\beta=O\left(2^{2z}\log n\log\Delta\right)\). Moreover, for our purposes, it provides the crucial property that on the arrival of each point \(x_{i}\), the algorithm irrevocably assigns \(x_{i}\) to one of the \(\beta k\) centers. Specifically, the clustering cost at the end of the stream is computed with respect to the center that \(x_{i}\) is assigned at time \(i\), which may not be the closest center to \(x_{i}\) because the closer center can be opened at a later time.

For the ease of discussion, we describe the Meyerson sketch for \(z=1\); the intuition generalizes naturally to other values of \(z\). The Meyerson sketch performs via a guess-and-double approach, where it first attempts to guess the cost of the optimal clustering cost. Using the guess of the cost, it then turns each point into a center with probability proportional to the distance of that point from the existing centers. This subroutine is illustrated in Algorithm 3. If too many centers have been opened, then the Meyerson sketch determines that the guess for the optimal clustering cost must have been too low and increases the guess. The overall algorithm is given in Algorithm 4.

```
1:Input:\(X\), \(\beta\), \(\gamma\), \(\delta\), \(\gamma\), \(\delta^{\prime}\```
0: Points \(X:=x_{1},\ldots,x_{n}\in\mathbb{R}^{d}\) with aspect ratio \(\Delta\), estimate \(\widehat{\mathsf{OPT}}\geq 0\) such that \(\alpha\mathsf{OPT}\leq\widehat{\mathsf{OPT}}\leq\mathsf{OPT}\) for some \(\alpha\in(0,1)\), failure probability \(\delta\in(0,1)\)
0: A coreset for \(k\)-clustering on \(X\)
1:\(\gamma\gets 2\log\frac{1}{\delta}\)
2:for\(i\in[\gamma]\)do
3:for\(t\in[n]\)do
4:if\(t=1\)then
5:\(M_{i}\gets x_{1}\), \(C_{\mu_{i}}\gets 0\), \(w_{i}(x_{1})=1\)
6:else
7:if\(|M_{i}|\leq 4k(1+\log\Delta)\left(\frac{2^{z+3}}{\alpha^{z}}+1\right)\)then
8: With probability \(\min\left(\frac{k(1+\log\Delta)\operatorname{dist}(x_{t},M_{i})^{z}}{\widehat{ \mathsf{OPT}}},1\right)\), add \(x_{t}\) to \(M_{i}\) with weight \(1\), i.e., \(w_{i}(x_{t})=1\)
9: Otherwise, let \(z=\operatorname{argmin}_{y\in M_{i}}\operatorname{dist}(x_{t},y)\), increment the weight of \(z\), i.e., \(w_{i}(z)\gets w_{i}(z)+1\), and increase \(C_{\mu_{i}}\gets C\mu_{i}\operatorname{dist}(x_{t},z)^{p}\)
10: Let \(j=\operatorname{argmin}_{i:|M_{i}|\leq 4k(1+\log\Delta)\left(\frac{2^{z+3}}{ \alpha^{z}}+1\right)}C_{\mu_{i}}\) be the index of the minimal cost sketch with at most \(4k(1+\log\Delta)\left(\frac{2^{z+3}}{\alpha^{z}}+1\right)\) samples \(\triangleright\)Return FAIL if such \(j\) does not exist
11:return\(\cup_{i\in[\gamma]}M_{i}\), \(w_{j}\), and \(C_{\mu_{j}}\)
```

**Algorithm 3** High probability \(\textsc{Meyerson}(X,\widehat{\mathsf{OPT}},\alpha,\delta,\Delta,z,k)\) sketch

```
0: Points \(X:=x_{1},\ldots,x_{n}\in\mathbb{R}^{d}\) with aspect ratio \(\Delta\), estimate \(\widehat{\mathsf{OPT}}\geq 0\) such that \(\alpha\mathsf{OPT}\leq\widehat{\mathsf{OPT}}\leq\mathsf{OPT}\) for some \(\alpha\in(0,1)\), failure probability \(\delta\in(0,1)\)
0: A coreset for \(k\)-means clustering on \(X\) if \(\widehat{\mathsf{OPT}}\) upper bounds the cost of the optimal clustering
1:\(\gamma\leftarrow\log nd(\Delta^{z})\)
2:for\(i\in[\gamma]\)do
3: Run \(\textsc{Meyerson}\left(X,2^{i},\alpha=\frac{1}{2},\delta,\Delta,z,k\right)\) in parallel
4: Let \(j\) be the minimal index in \([\gamma]\) such that \(\textsc{Meyerson}\) with input \(2^{j}\) has size smaller than \(8k\log\frac{1}{\delta}\left(1+\log\Delta\right)\left(2^{2z+3}+1\right)\) and cost smaller than \(2^{z+6+j}\)
5:return the output for \(\textsc{Meyerson}\left(X,2^{j},\alpha=\frac{1}{2},\delta,\Delta,z,k\right)\)
```

**Algorithm 4** High probability \(\textsc{MultMeyerson}\) sketch

**Theorem 2.1**.: _[_8_]_ _Given an input stream \(x_{1},\ldots,x_{n}\in\mathbb{R}^{d}\) defining a set \(X\subset[\Delta]^{d}\), there exists an online algorithm \(\textsc{MultMeyerson}\) that with probability at least \(1-\frac{1}{\operatorname{poly}(n)}\):_

1. _on the arrival of each point_ \(x_{i}\)_, assigns_ \(x_{i}\) _to a center in_ \(C\) _through a mapping_ \(\pi:X\to C\)_, where_ \(C\) _contains at most_ \(O\left(2^{2z}k\log n\log\Delta\right)\) _centers_
2. \(\sum_{x\in X}\|x_{i}-\pi(x_{i})\|_{2}^{z}\leq 2^{z+7}\operatorname{Cost}_{|S| \leq k}(X,S)\)__
3. \(\textsc{MultMeyerson}\) _uses_ \(O\left(2^{z}k\log^{3}(nd\Delta)\right)\) _words of space_

## Appendix B Online \((1+\varepsilon)\)-Coreset

In this section, we describe how to construct an online \((1+\varepsilon)\) coreset for \((k,z)\)-clustering under general discrete metrics. We first describe the offline coreset construction of [27] and then argue that the construction can be adapted to an online setting at the cost of logarithmic overheads, which suffice for our purpose.

Let \(\mathcal{A}\) be an \((\alpha,\beta)\)-approximation for a \((k,z)\)-clustering on an input set \(X\subseteq[\Delta]^{d}\) and let \(C_{1},\ldots,C_{\beta k}\) be the clusters of \(X\) induced by \(\mathcal{A}\). Suppose the points of \(X\) arrive in a data stream \(S\). For a fixed \(\varepsilon>0\), [27] define the following notions of rings and groups:* The average cost of cluster \(C_{i}\) is denoted by \(\kappa_{C_{i}}:=\frac{\operatorname{Cost}(C_{i},\mathcal{A})}{|C_{i}|}\).
* For any \(i,j\), the ring \(R_{i,j}\) is the set of points \(x\in C_{i}\) such that \[2^{j}\kappa_{C_{i}}\leq\operatorname{Cost}(x,\mathcal{A})<2^{j+1}\kappa_{C_{i}}.\] For any \(j\), \(R_{j}=\cup R_{i,j}\).
* The inner ring \(R_{I}(C_{i})=\cup_{j\leq 2z\log\frac{z}{z}}R_{i,j}\) is the set of points of \(C_{i}\) with cost at most \(\left(\frac{z}{z}\right)^{2z}\kappa_{C_{i}}\). More generally for a solution \(\mathcal{S}\), let \(R_{I}^{\mathcal{S}}\) denote the union of the inner rings induced by \(\mathcal{S}\).
* The outer ring \(R_{O}(C_{i})=\cup_{j\geq 2\log\frac{z}{z}}R_{i,j}\) is the set of points of \(C_{i}\) with cost at least \(\left(\frac{z}{z}\right)^{2z}\kappa_{C_{i}}\). More generally for a solution \(\mathcal{S}\), let \(R_{O}^{\mathcal{S}}\) denote the union of the outer rings induced by \(\mathcal{S}\).
* The main ring \(R_{M}(C_{i})\) is the set of points of \(C_{i}\) that are not in the inner or outer rings, i.e., \(R_{M}(C_{i})=C_{i}\setminus(R_{I}(C_{i})\cup R_{O}(C_{i})\).
* For any \(j\), the group \(G_{j,b}\) consists of the \((2^{b-1}+1)\)-th to \((2^{b})\)-th points of each ring \(R_{i,j}\) that arrive in \(S\).
* For any \(j\), we use \(G_{j,\min}\) to denote the union of the groups with the smallest costs, i.e., \[G_{j,\min}=\left\{x|\exists i,x\in R_{i,j},\operatorname{Cost}(R_{i,j}, \mathcal{A})<2\left(\frac{\varepsilon}{4z}\right)^{z}\frac{\operatorname{Cost} (R_{j},\mathcal{A})}{\beta k}\right\}.\]
* The outer groups \(G_{b}^{O}\) partition the outer rings \(R_{O}^{\mathcal{A}}\) so that \[G_{b}^{O}=\left\{x|\exists i,x\in C_{i},\left(\frac{\varepsilon}{4z}\right)^{ z}\frac{\operatorname{Cost}(R_{O}^{\mathcal{A}},\mathcal{A})}{\beta k}\cdot 2^{b} \leq\operatorname{Cost}(R_{O}(C_{i}),\mathcal{A})<\left(\frac{\varepsilon}{4z} \right)^{z}\frac{\operatorname{Cost}(R_{O}^{\mathcal{A}},\mathcal{A})}{\beta k }\cdot 2^{b+1}\right\}.\]
* We define \(G_{\min}^{O}=\cup_{b\leq 0}G_{b}^{O}\) and \(G_{\max}^{O}=\cup_{b\geq z\log\frac{4z}{z}}G_{b}^{O}\).

We remark that unlike the definition of [27], \(G_{j,\min}\) is a subset of the groups \(G_{j,b}\) with \(b\geq 1\), but we shall nevertheless show that our sampling procedure preserves the cost contributed by each group. We also require the following slight variation of the definition of \(\mathcal{A}\)-approximate centroid set from [53] due to [27].

**Definition B.1** (\(\mathcal{A}\)-approximate centroid set).: _Let \(X\subseteq\mathbb{R}^{d}\) be a set of points, let \(k,z\) be two positive integers, and let \(\varepsilon>0\) be an accuracy parameter. Given a set \(\mathcal{A}\) of centers, we say a set \(\mathbb{C}\) is an \(\mathcal{A}\)-approximate centroid set for \((k,z)\)-clustering on \(X\) if for every set of \(k\) centers \(\mathcal{S}\subseteq\mathbb{R}^{d}\), there exists \(\widetilde{\mathcal{S}}\subseteq\mathbb{R}^{d}\) of \(k\) points such that for all \(x\in X\) with \(\operatorname{Cost}(x,\mathcal{S})\leq\left(\frac{8z}{\varepsilon}\right)^{z }\operatorname{Cost}(x,\mathcal{A})\) or \(\operatorname{Cost}(x,\widetilde{\mathcal{S}})\leq\left(\frac{8z}{ \varepsilon}\right)^{z}\operatorname{Cost}(x,\mathcal{A})\),_

\[|\operatorname{Cost}(x,\mathcal{S})-\operatorname{Cost}(x,\widetilde{ \mathcal{S}})|\leq\frac{\varepsilon}{z\log(z/\varepsilon)}(\operatorname{ Cost}(x,\mathcal{S})-\operatorname{Cost}(x,\mathcal{A}).\]

The following statement is implied by the proof of Theorem 1 in [27].

**Theorem B.2**.: _[_27, 24_]_ _Let \(z>0\) be a constant. Let \(x\in G\) for a group induced by an \((\alpha,\beta)\)-bicriteria assignment \(\mathcal{A}\). For each cluster \(C_{i}\) with \(i\in[\beta k]\), let \(D_{i}=C_{i}\cap G\). Let \(\mathbb{C}\) be an \(\mathcal{A}\)-approximate centroid set for \(G\) and let_

\[\gamma=\frac{C\max(\alpha^{2},\alpha^{2})\beta}{\min(\varepsilon^{2}, \varepsilon^{z})}\log^{2}\frac{1}{\varepsilon}\left(k\log|\mathbb{C}|+\log \log\frac{1}{\varepsilon}+\log n\right)\log^{2}\frac{1}{\varepsilon},\]

_for some sufficiently large constant \(C>0\). Let_

\[\zeta_{x}=\frac{\operatorname{Cost}(D_{i},\mathcal{A})}{|D_{i}|\operatorname{ Cost}(G,\mathcal{A})}\cdot\gamma\log n,\qquad\eta_{x}=\frac{\operatorname{Cost}(x, \mathcal{A})}{\operatorname{Cost}(G,\mathcal{A})}\cdot\gamma\log n.\]

_Suppose each point \(x\in X\) is sampled and reweighted independently into a set \(\Omega_{0}\) with probability \(p_{x}\), where_

\[p_{x}\geq\min(\zeta_{x}+\eta_{x},1).\]

_Let \(\Omega_{1}=\Omega_{0}\setminus(R_{I}(C_{i})\cup(C_{i}\cap\cup_{j}G_{j,\min}) \cup(R_{O}(C_{i})\cap G_{\min}^{O})\)._

_Suppose \(\Omega_{2}\) is the set of centers in \(\mathcal{A}\), where each center \(c_{i}\) with \(i\in[\beta k]\) has weight \(w_{i}\), where \(w_{i}\) is a \((1+\varepsilon)\)-approximation to \(|R_{I}(C_{i})|+|C_{i}\cap\cup_{j}G_{j,\min}|+|R_{O}(C_{i})\cap G_{\min}^{O}|\). Then \((\Omega_{1}\setminus\Omega_{2})\cup\Omega_{2}\) is \((1+\varepsilon)\)-coreset for the \((k,z)\)-clustering problem with probability \(1-\frac{1}{\operatorname{poly}(n)}\)._We first show that the sampling probabilities for each point in the stream by RingSample in Algorithm1 satisfies the conditions of TheoremB.2.

**Lemma B.3**.: _Let \(x\in G\) for a group induced by an \((\alpha,\beta)\)-bicriteria assignment \(\mathcal{A}\) at a time \(t\), with \(t\in[n]\). For each cluster \(C_{i}\) with \(i\in[\beta k]\), let \(D_{i}=C_{i}\cap G\). Let \(\mathbb{C}\) be an \(\mathcal{A}\)-approximate centroid set for \(G\) and let_

\[\gamma=\frac{C\max(\alpha^{2},\alpha^{2})\beta}{\min(\varepsilon^{2}, \varepsilon^{2})}\log^{2}\frac{1}{\varepsilon}\left(k\log|\mathbb{C}|+\log\log \frac{1}{\varepsilon}+\log n\right)\log^{2}\frac{1}{\varepsilon},\]

_for some sufficiently large constant \(C>0\) Let_

\[\zeta_{x}=\frac{\operatorname{Cost}(D_{i},\mathcal{A})}{|D_{i}|\operatorname{ Cost}(G,\mathcal{A})}\cdot\gamma\log n,\qquad\eta_{x}=\frac{\operatorname{Cost }(x,\mathcal{A})}{\operatorname{Cost}(G,\mathcal{A})}\cdot\gamma\log n.\]

_Then the probability \(p_{x}\) that RingSample (Algorithm1) samples each point \(x\) satisfies_

\[p_{x}\geq\min(\zeta_{x}+\eta_{x},1).\]

Proof.: Suppose that \(x\in R_{i,j}\) and \(x\in G_{j,b}\) at time \(t\), for some \(i\in[\beta k]\) in an assignment by \(\mathcal{A}\) from MultMeyerson. Let \(u\) be the time that \(x\) arrived in the stream. By the properties of the Meyerson sketch, i.e., MultMeyerson in Theorem2.1, \(x\) is irrevocably assigned to a cluster \(C_{i}\) with \(i\in[\beta k]\) at time \(u\). Hence, \(x\) must also be assigned to ring \(R_{i,j}\) at time \(u\). Moreover, since the stream is insertion-only, then the number of points in all rings \(R_{i,j}\) for a fixed \(j\) across all \(i\in[\beta k]\) is monotonically non-decreasing. Thus \(x\) must also be assigned to group \(G_{j,b}\) at time \(u\).

Let \(p_{x}\) be the sampling probability of \(x\) by RingSample in Algorithm1 at time \(u\). We have that

\[p_{x}=\min\left(\frac{4}{r_{u}}\cdot\gamma\log n,1\right),\]

where \(r_{u}\) is the number of points in \(G_{j,b}\) at time \(u\). Let \(G^{(u)}_{j,b}\) be the subset of \(G_{j,b}\) that have arrived at time \(u\) and let \(G^{(t)}_{j,b}\) be the subset of \(G_{j,b}\) that have arrived at time \(t\). Let \(c_{i}\) be the center assigned to point \(x\), so that \(\operatorname{Cost}(x,c_{i})=\operatorname{Cost}(x,\mathcal{A})\) and let \(C^{(u)}_{i}\) be the points assigned to \(c_{i}\) at time \(u\). Similarly, let \(D^{(u)}_{i}=C^{(u)}_{i}\cap G^{(u)}_{j,b}\). By the definition of \(R_{i,j}\) and \(G_{j,b}\),

\[\frac{\|x-c_{i}\|_{2}^{2}}{\operatorname{Cost}(G^{(u)}_{j,b},\mathcal{A})} \leq\frac{2^{j+1}}{\operatorname{Cost}(G^{(u)}_{j,b},\mathcal{A})}\leq\frac{2 ^{j+1}}{r_{u}\cdot 2^{j}}=\frac{2}{r_{u}}.\]

Since both the cost of group \(G_{j,b}\) and the number of points in \(D_{i}\) is monotonically non-decreasing over time, then at time \(t\), we have

\[\frac{\zeta_{x}}{\gamma\log n}=\frac{\operatorname{Cost}(D_{i},\mathcal{A})}{ |D_{i}|\operatorname{Cost}(G_{j,b},\mathcal{A})}\leq\frac{2|D_{i}|\|x-c_{i}\| _{2}^{2}}{|D_{i}|\operatorname{Cost}(G^{(t)}_{j,b},\mathcal{A})}\leq\frac{2 \|x-c_{i}\|_{2}^{2}}{\operatorname{Cost}(G^{(u)}_{j,b},\mathcal{A})}\leq\frac{ 4}{r_{u}}.\]

Similarly, we have that due to the monotonicity of the cost of group \(G_{j,b}\) over time,

\[\frac{\eta_{x}}{\gamma\log n}=\frac{\|x-c_{i}\|_{2}^{2}}{\operatorname{Cost}( G^{(t)}_{j,b},\mathcal{A})}\leq\frac{\|x-c_{i}\|_{2}^{2}}{\operatorname{Cost}(G^{(u)} _{j,b},\mathcal{A})}\leq\frac{2}{r_{u}}.\]

Thus for sufficiently large constant \(C\) in the definition of \(\gamma\) in RingSample, we have that

\[p_{x}\geq\min(\zeta_{x}+\eta_{x},1),\]

since \(p_{x}=\min\left(\frac{4}{r_{u}}\cdot\gamma\log n,1\right)\). 

We next justify the space complexity of Algorithm1, i.e., showing that with high probability, an upper bound of the number of samples can be determined.

**Lemma B.4**.: RingSample (Algorithm1) samples

\[O\left(\frac{\max(\alpha^{2},\alpha^{2})\beta}{\min(\varepsilon^{2}, \varepsilon^{2})}\log^{2}\frac{1}{\varepsilon}\left(k\log|\mathbb{C}|+\log \log\frac{1}{\varepsilon}+\log n\right)\log^{2}n\log^{2}\Delta\log^{2}\frac{1} {\varepsilon}\right)\]

_points with high probability._Proof.: Recall that by definition, the groups \(G_{j,b}\) partition the points \(X=x_{1},\ldots,x_{n}\subseteq[\Delta]^{d}\). For a fixed \(j\) and \(b\), let \(Y_{i}\) be an indicator random variable for whether the \(i\)-th point of \(G_{j,b}\) is sampled by RingSample. Then we have \(\mathbb{E}\left[Y_{i}\right]\leq\frac{4}{i}\cdot\gamma\log n\) and similarly \(\mathbb{E}\left[Y_{i}^{2}\right]\leq\frac{4}{i}\cdot\gamma\log n\). By Bernstein's inequality, Theorem A.4, we have that

\[\mathbf{Pr}\left[\sum Y_{i}\geq 80\gamma\log^{2}n\right]\leq\frac{1}{n^{4}}\]

and more generally, we have that \(\sum Y_{i}=O\left(\gamma\log^{2}n\right)\) with high probability. Thus by a union bound over all \(j\) and \(b\), we have that the number of sampled points is at most

\[O\left(\gamma\log^{2}n\log^{2}\Delta\right)=O\left(\frac{1}{\min( \varepsilon^{2},\varepsilon^{2})}\log^{2}\frac{1}{\varepsilon}\left(k\log| \mathbb{C}|+\log\log\frac{1}{\varepsilon}+\log n\right)\log^{2}n\log^{2} \Delta\log^{2}\frac{1}{\varepsilon}\right)\]

for \(\gamma=\frac{C\max(\alpha^{2},\alpha^{*})\beta}{\min(\varepsilon^{2}, \varepsilon^{*})}\log^{2}\frac{1}{\varepsilon}\left(k\log|\mathbb{C}|+\log \log\frac{1}{\varepsilon}+\log n\right)\log^{2}\frac{1}{\varepsilon}\). 

Moreover, note that we can for all \(t\in[n]\), we can explicitly track both \(|G_{j,b}^{(t)}|\) and \(\operatorname{Cost}(G_{j,b}^{(t)},\mathcal{A})\) as the stream is updated, because once the bicriteria algorithm assigns a point to a center in \(\mathcal{A}\), the assignment will remain the same for the rest of the stream. Thus, we have the following:

**Lemma B.5**.: _For each \(j\) and \(b\), there exists an algorithm that maintains both \(|G_{j,b}^{(t)}|\) and \(\operatorname{Cost}(G_{j,b}^{(t)},\mathcal{A})\) for all \(t\in[n]\) using \(O\left(\log(nd\Delta)\right)\) space._

Putting things together, we give the full guarantees of RingSample in Algorithm 1.

**Lemma 2.2**.: _Let \(\mathbb{C}\) be an \(\mathcal{A}\)-approximate centroid set for a fixed group \(G\). There exists an algorithm RingSample that samples_

\[O\left(\frac{\max(\alpha^{2},\alpha^{2})\beta}{\min(\varepsilon^{2}, \varepsilon^{2})}\log^{2}\frac{1}{\varepsilon}\left(k\log|\mathbb{C}|+\log \log\frac{1}{\varepsilon}+\log n\right)\log^{2}n\log^{2}\Delta\log^{2}\frac{1 }{\varepsilon}\right)\]

_points and with high probability, outputs a \((1+\varepsilon)\)-online coreset for the \(k\)-means clustering problem._

Proof.: Consider RingSample. Before claiming the algorithm gives an \((1+\varepsilon)\)-online coreset, we first consider a fixed time \(t\in[n]\). Then correctness at time \(t\) follows from applying Theorem B.2, given Lemma B.3 and Lemma B.5. We then observe that once a center is formed by RingSample, i.e., once a point is sampled, then it irrevocably remains a center in the data structure. Therefore, conditioned on the correctness at time \(t\), then the data structure will always correctly give an \((1+\varepsilon)\)-coreset to the prefix of \(t\) points in the stream at any later point \(t^{\prime}\) in the stream, \(t^{\prime}\in[n]\) with \(t^{\prime}>t\). It thus suffices to argue correctness over all \(t\in[n]\), which requires a simple union bound. The space complexity follows from Lemma B.4 and Lemma B.5. 

To apply Lemma 2.2, we require upper bounding the term \(\log|\mathbb{C}|\). To that end, we first require the following definition of doubling dimension.

**Definition B.6** (Doubling dimension).: _The doubling dimension of a metric space \(X\) with metric \(d\) is the smallest integer \(\ell\) such that for any \(x\in X\), it is possible to cover the ball of radius \(2r\) around \(x\) with \(2^{\ell}\) balls of radius \(r\)._

Observe that general discrete metric spaces with \(n\) points have doubling dimension \(O\left(\log n\right)\) since all points can be covered by \(2^{\log n}\) balls.

We then recall the following result that upper bounds the size \(\log|\mathbb{C}|\) for metric spaces with doubling dimension \(d\).

**Lemma B.7**.: _[_27_]_ _Given a subset \(X\) from a metric space with doubling dimension \(d\), \(\varepsilon>0\), and an \(\alpha\)-approximate solution \(\mathcal{A}\) with at most \(k\ \mathrm{polylog}(n)\) centers, there exists an \(\mathcal{A}\)-approximate centroid set for \(X\) of size \(|X|\cdot\left(\frac{\alpha}{\varepsilon}\right)^{O(d)}\)._

It is known that the Euclidean space has doubling dimension \(\Theta(d)\), which would give a \(d\) dependency on our coreset size. However, [38] showed that the \(d\) dependency can be replaced with \(\frac{k}{\varepsilon^{2}}\), which was subsequently improved by a line of works, e.g., [58, 40, 46], ultimately down to a dependency of \(\frac{1}{\varepsilon^{2}}\log\frac{k}{\varepsilon}\) using the following notion of terminal embeddings:

**Definition B.8** (Terminal embedding).: _Let \(\varepsilon\in(0,1)\) and \(X\subseteq\mathbb{R}^{d}\) be a set of \(n\) points. Then a mapping \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}^{m}\) is a terminal embedding if for all \(x\in X\) and all \(y\in\mathbb{R}^{d}\),_

\[(1-\varepsilon)\|x-y\|_{2}\leq\|f(x)-f(y)\|_{2}\leq(1+\varepsilon)\|x-y\|_{2}.\]

[55] gave a construction of a terminal embedding with \(m=O\left(\frac{1}{\varepsilon^{2}}\log n\right)\) that can be applied in linear space through exhaustive search when polynomial runtime is not required. Thus Lemma 2.2 rows give the following:

**Theorem 1.3**.: _There exists an algorithm that samples \(\frac{k}{\min(\varepsilon^{4},\varepsilon^{2+\varepsilon})}\operatorname{ polylog}\frac{n\Delta}{\varepsilon}\) points and with high probability, outputs a \((1+\varepsilon)\)-online coreset for \((k,z)\)-clustering._

For the purpose of clarity, we emphasize that the algorithm does not use sublinear space, even though the sample complexity is sublinear. Namely, for each stream update, we construct and apply a terminal embedding to project each point to a lower dimension. We then compute the appropriate sampling probability of the projected point, but then sample the original point with the computed sampling probability.

## Appendix C Sliding Window Model

In this section, we describe how our online coreset construction for \((k,z)\)-clustering on general discrete metric spaces can be used to achieve near-optimal space algorithms for \((k,z)\)-clustering in the sliding window model.

We first recall a standard approach for using offline coreset constructions for insertion-only streaming algorithms. Suppose there exists a randomized algorithm that produces an online coreset algorithm that uses \(S(n,\varepsilon,\delta)\) points for an input stream of length \(n\), accuracy \(\varepsilon\), and failure probability \(\delta\), where for the ease of discussion, we omit additional dependencies, such as on the dimension \(d\), the clustering constraint \(k\), the parameter \(z\), or additional parameters for whatever problem the coreset construction may approximate. A standard approach for using coresets on insertion-only streams is the merge-and-reduce approach, which partitions the stream into blocks of size \(S\left(n,\frac{\varepsilon}{2\log n},\frac{\delta}{\operatorname{ polyl}(n)}\right)\) and builds a coreset for each block. Each coreset is then viewed as the leaves of a binary tree with height at most \(\log n\), since the binary tree has at most \(n\) leaves. Then at each level of the binary tree, for each node in the level, a coreset of size \(S\left(n,\frac{\varepsilon}{2\log n},\frac{\delta}{\operatorname{ polyl}(n)}\right)\) is built from the coresets representing the two children of the node. Due to the mergeability property of coresets, the coreset at the root of the tree will be a coreset for the entire stream with accuracy \(\left(1+\frac{\varepsilon}{2\log n}\right)^{\log n}\leq(1+\varepsilon)\) and failure probability \(\delta\). We give an illustration of this approach in Figure 2.

This approach fails for sliding window algorithms because the elements at the beginning of the data stream can expire, and so coresets corresponding to earlier blocks of the stream may no longer accurate, which would result in the coreset at the root of the tree also no longer being accurate. On the other hand, suppose we partition the stream into blocks consisting of \(S\left(n,\frac{\varepsilon}{2\log n},\frac{\delta}{\operatorname{ polyl}(n)}\right)\) elements as before, but instead of creating an offline coreset for each block, we can create an online coreset

Figure 2: Merge and reduce framework on a stream of length \(n\). The coresets at level 1 are the entire blocks. The coresets at level \(i\) for \(i>1\) are each \(\left(1+O\left(\frac{\varepsilon}{2\log n}\right)\right)\)-coresets of the coresets at their children nodes in level \(i-1\).

for the elements _in reverse_. That is, since the elements in each block are explicitly stored, we can create offline an artificial stream consisting of the elements in the block in reverse and then give the artificial stream as input to the online coreset construction. Note that if we also first consider the "latter" coreset when merging two coresets, then this effectively reverses the stream. Moreover, by the correctness of the online coreset, our data structure provides correctness over any prefix of the reversed stream, or equivalently, any suffix of the stream and specifically, correctness over the sliding window.

Indeed, [9] showed that deterministic online coresets for problems in randomized numerical linear algebra can be used to achieve deterministic algorithms for the corresponding problems in the sliding window model. We thus further adapt the merge-and-reduce framework to show that randomized online coresets for problems in clustering can also be used to achieve randomized algorithms for the corresponding problems in the sliding window model. We formalize this approach in Algorithm 2, duplicated below:

```
1:A clustering function \(f\), a set of points \(x_{1},\ldots,x_{n}\subseteq\mathbb{R}^{d}\), accuracy parameter \(\varepsilon>0\), failure probability \(\delta\in(0,1)\), and window size \(W>0\)
2:An approximation of \(f\) on the \(W\) most recent points
3:Let \(\textsc{Coreset}(X,n,d,k,\varepsilon,\delta)\) be an online coreset construction with \(S(n,d,k,\varepsilon,\delta)\) points on a set \(X\subseteq\mathbb{R}^{d}\)
4:\(m\gets O\left(S\left(n,d,k,\frac{\varepsilon}{\log n},\frac{\delta}{n} \right)\log n\right)\)
5:Initialize blocks \(B_{0},B_{1},\ldots,B_{\log n}\leftarrow\emptyset\)
6:for each point \(x_{t}\) with \(t\in[n]\)do
7:if\(B_{0}\) does not contain \(m\) points then
8: Prepend \(x_{t}\) to \(B_{0}\), i.e., \(B_{0}\leftarrow\{x_{t}\}\cup B_{0}\)
9:else
10: Let \(i\) be the smallest index such that \(B_{i}=\emptyset\)
11:\(B_{i}\leftarrow\textsc{Coreset}\left(Y,n,d,k,\frac{\varepsilon}{\log n},\frac{ \delta}{n^{2}}\right)\) for \(Y=B_{0}\cup\ldots\cup B_{i-1}\)\(\triangleright Y\) is an ordered set of weighted points
12:for\(j=0\) to \(j=i-1\)do
13:\(B_{j}\leftarrow\emptyset\)
14:\(B_{0}\leftarrow\{x_{t}\}\)
15:return the ordered set \(B_{\log n}\cup\ldots\cup B_{0}\)
```

**Algorithm 5** Merge-and-reduce framework for randomized algorithms in the sliding window model, using randomized constructions of online coresets

**Theorem 2.3**.: _Let \(x_{1},\ldots,x_{n}\) be a stream of points in \([\Delta]^{d}\), \(\varepsilon>0\), and let \(X=\{x_{n-W+1},\ldots,x_{n}\}\) be the \(W\) most recent points. Suppose there exists a randomized algorithm that with probability at least \(1-\delta\), outputs an online coreset algorithm for a \(k\)-clustering problem with \(S(n,d,k,\varepsilon,\delta)\) points. Then there exists a randomized algorithm that with probability at least \(1-\delta\), outputs a coreset for the \(k\)-clustering problem in the sliding window model with \(O\left(S\left(n,d,k,\frac{\varepsilon}{\log n},\frac{\delta}{n^{2}}\right)\log n\right)\) points._

Proof.: Consider Algorithm 2. Let \(\textsc{Coreset}(X,n,d,k,\varepsilon,\delta)\) be a randomized algorithm that, with probability at least \(1-\delta\), computes an online coreset for a \(k\)-clustering problem \(f\) with \(S(n,d,k,\varepsilon,\delta)\) points.

We first claim that for each \(B_{i}\) is a \(\left(1+\frac{\varepsilon}{\log n}\right)^{i}\) online coreset for \(2^{i-1}m\) points. To that end, observe that \(B_{i}\) can only be non-empty if at some time, \(B_{0}\) contains \(m\) points and \(B_{1},\ldots,B_{i-1}\) are all non-empty. By the correctness of the subroutine Coreset, \(B_{i}\) is a \(\left(1+\frac{\varepsilon}{\log n}\right)\) online coreset for the points in \(B_{0}\cup\ldots\cup B_{i-1}\) at some point during the stream. Hence by induction, \(B_{i}\) is a \(\left(1+\frac{\varepsilon}{\log n}\right)\left(1+\frac{\varepsilon}{\log n} \right)^{i-1}=\left(1+\frac{\varepsilon}{\log n}\right)^{i}\) coreset for \(m+\sum_{j=1}^{i-1}2^{j-1}m=2^{i-1}m\) points.

Now, because Algorithm 2 inserts the newest points at the beginning of \(B_{0}\), then the stream is fed in reverse to the merge-and-reduce procedure. Thus, for any \(W\in[2^{i-1},2^{i})\), \(B_{0}\cup\ldots\cup B_{i}\) provides an online coreset \(k\)-clustering for the \(W\) most recent points in the stream.

To analyze the probability of failure, we remark that there are at most \(n\) points in the stream. For each point, there are at most \(n\) coresets constructed by the subroutine Coreset (in fact, the number of coreset constructions is upper bounded by \(O\left(\log n\right)\)). Since each subroutine is called with failure probability \(\frac{\delta}{n^{2}}\), then by a union bound, the total failure probability is at most \(\delta\).

To analyze the space complexity, note that there are at most \(O\left(\log n\right)\) coreset constructions \(B_{0},\ldots,B_{\log n}\) maintained by the algorithm. Each coreset construction samples \(S\left(n,d,k,\frac{\varepsilon}{\log n},\frac{\delta}{n^{2}}\right)\) points. Hence, the total number of sampled points is \(O\left(S\left(n,d,k,\frac{\varepsilon}{\log n},\frac{\delta}{n^{2}}\right)\log n\right)\). 

By Theorem1.3 and Theorem2.3, we have:

**Theorem 2.4**.: _There exists an algorithm that samples \(\frac{k}{\min(\varepsilon^{4},\varepsilon^{2+\varepsilon})}\ \mathrm{polylog}\ \frac{n \Delta}{\varepsilon}\) points and with high probability, outputs a \((1+\varepsilon)\)-coreset to \((k,z)\)-clustering in the sliding window model._

Using an offline algorithm for \((k,z)\)-clustering for post-processing after the data stream, we have:

**Theorem 1.1**.: _There exists an algorithm that samples \(\frac{k}{\min(\varepsilon^{4},\varepsilon^{2+\varepsilon})}\ \mathrm{polylog}\ \frac{n \Delta}{\varepsilon}\) points and with high probability, outputs a \((1+\varepsilon)\)-approximation to \((k,z)\)-clustering for the Euclidean distance on \([\Delta]^{d}\) in the sliding window model._

## Appendix D Lower Bounds

In this section, we show that any \((1+\varepsilon)\)-online coreset for \((k,z)\)-clustering requires \(\Omega\left(\frac{k}{\varepsilon^{2}}\log n\right)\) points. The intuition is somewhat straightforward and in a black-box manner. [25] showed the existence of a set \(X\) of \(\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) unit vectors such that any sublinear space data structure would not be able to accurately determine \(\mathrm{Cost}(C,X)\) for a set of \(k\) unit vectors \(C\). They thus showed that any offline \((1+\varepsilon)\)-coreset construction for \((k,z)\)-clustering required \(\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) points.

Because an online \((1+\varepsilon)\)-coreset must answer queries on all prefixes of the stream, our goal is to essentially embed \(\Omega(\log n)\) instances of the hard instance of [25] into the stream, which would require \(\Omega\left(\frac{k}{\varepsilon^{2}}\log n\right)\) points. To enforce the data structure to sample \(\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) points for each of the hard instance, we give each of the instances increasingly exponential weight. That is, we give the points in the \(i\)-th instance \(\tau^{i}\) weight for some constant \(\tau>1\), by inserting \(\tau^{i}\) copies of each of the points. Because the weight of the \(i\)-th instance is substantially greater than the sum of the weights of the previous instances, any \((1+\varepsilon)\)-online coreset must essentially be a \((1+\varepsilon)\)-coreset for the \(i\)-th instance, thus requiring \(\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) points for the \(i\)-th instance. This reasoning extends to all of the \(\Omega(\log n)\) instances, thereby giving a lower bound of \(\Omega\left(\frac{k}{\varepsilon^{2}}\log n\right)\) points.

We first recall the following offline coreset lower bound by [25].

**Theorem D.1**.: _[_25_]_ _For \(d=\Theta\left(\frac{k}{\varepsilon^{2}}\right)\), let \(X=e_{1},\ldots,e_{d}\in\mathbb{R}^{2d}\) be the set of elementary vectors. Let \(z\) be a constant and let \(a_{1},\ldots,a_{m}\in\mathbb{R}^{2d}\) with corresponding weights \(w_{1},\ldots,w_{m}\in\mathbb{R}\) be a weighted set \(P\) of points. Then there exists a set of \(k\) unit vectors \(C=c_{1},\ldots,c_{k}\in\mathbb{R}^{2d}\) such that for \(m=o\left(\frac{k}{\varepsilon^{2}}\right)\),_

1. \(\mathrm{Cost}(C,X)=\sum_{i=1}^{d}\min_{j\in[k]}\|e_{i}-c_{j}\|_{2}^{2}\geq 2^ {z/2}d-2^{z/2}\cdot\max(1,z/2)\cdot\sqrt{dk}\)_._
2. \(\mathrm{Cost}(C,P)=\sum_{i=1}^{m}w_{i}\min_{j\in[k]}\|a_{i}-c_{j}\|_{2}^{2}<(1- \varepsilon)(2^{z/2}d-2^{z/2}\cdot\max(1,z/2)\cdot\sqrt{dk})\)_._

We remark that the first property is due to Lemma 31 pf [25] and the second property is due to Lemma 33 and Lemma 34 of [25].

Let \(\gamma=\Theta\left(\log\frac{n}{d^{\prime}}\right)\). Let \(d^{\prime}=\Theta\left(\frac{k}{\varepsilon^{2}}\right)\) be the dimension of the hard instance in TheoremD.1 and set \(d=\gamma d^{\prime}\), so that we can partition the space \(\mathbb{R}^{2d}\) into \(\gamma\) groups of \(2d^{\prime}\) coordinates.

We define a stream by creating \(\gamma\) weighted instances of the hard instance defined in TheoremD.1. Each of the \(\gamma\) hard instances will be embedded into a separate partition of \(2d^{\prime}\) coordinates of \(\mathbb{R}^{2d}\). Namely, the first instance consists of the vectors \(e_{1},\ldots,e_{d^{\prime}}\) being inserted into the stream. By TheoremD.1, any \((1+\varepsilon)\)-coreset must contain \(\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) points. The next instance consists of the vectors \(e_{1+2d^{\prime}},\ldots,e_{3d^{\prime}}\) each being inserted \(\tau=100\) times into the stream. That is, after the vector \(e_{d^{\prime}}\) arrives in the stream from the first hard instance, then \(t\) copies of \(e_{1+2d^{\prime}}\) arrive in the stream, followed by then \(t\) copies of \(e_{2+2d^{\prime}}\), and so forth. Due to the weights of these vectors, any \((1+\varepsilon)\)-coreset must essentially be a \((1+\varepsilon)\)-coreset for the second hard instance and thus contain \(\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) points with support in the second group of \(2d^{\prime}\) coordinates.

More generally, for each \(i\in[\gamma]\), the stream inserts \(t^{i-1}\) copies of \(e_{1+2(i-1)d^{\prime}}\), followed by \(\tau^{i-1}\) copies of \(e_{2+2(i-1)d^{\prime}}\), and so on. The main intuition is that due to the weights of the \(i\)-th group of \(d^{\prime}\) elementary vectors, an \((1+\varepsilon)\)-online coreset must contain a \((1+\varepsilon)\)-coreset for the \(i\)-th hard instance. Moreover, since the \((1+\varepsilon)\)-online coreset must be a coreset for any prefix of the stream, then it needs to be a \((1+\varepsilon)\)-coreset for each of the hard instances. Hence, the online coreset must contain \(\gamma\cdot\Omega\left(\frac{k}{\varepsilon^{2}}\right)=\Omega\left(\frac{k}{ \varepsilon^{2}}\cdot\frac{\log n}{\log\frac{1}{\varepsilon}}\right)\) points.

**Lemma D.2**.: _Let \(\tau=100\). For each integer \(i>0\), let \(S_{i}\) be the stream that consists of \(\tau^{i-1}\) consecutive copies of \(e_{1+2(i-1)d^{\prime}}\), followed by \(\tau^{i-1}\) copies of \(e_{2+2(i-1)d^{\prime}}\), and so on. Let \(S\) be the stream that consists of \(S_{1}\circ S_{2}\circ\ldots\). Then for each \(i\), any \((1+\varepsilon)\)-online coreset after the arrival of \(S_{i}\) must consist of \(i\cdot\Omega\left(\frac{k}{\varepsilon^{2}}\right)\) points._

Proof.: We prove the claim by induction on \(i\). The base case of \(i=1\) follows from Theorem D.1.

Now suppose the claim holds for a fixed \(i-1\). Let \(X_{i}\) be the set of points that have arrived after \(S_{i}\), i.e., \(X_{i}=S_{1}\circ\ldots\circ S_{i}\). Let \(C_{i-1}\) be any \((1+\varepsilon)\)-online coreset for \(S\) after the arrival of \(S_{i-1}\). Let \(P_{i}\) be a set of weighted points sampled during stream \(S_{i}\), so that \(C_{i}=C_{i-1}\cup P_{i}\). Since each point in \(S_{i}\) has weight \(\tau^{i}\), then by scaling the first property of Theorem D.1, we have that there exists a set of \(k\) unit vectors \(U_{i}=c_{1},\ldots,c_{k}\in\mathbb{R}^{2d}\) such that

\[\operatorname{Cost}(U,X_{i}) =\sum_{a=1}^{i}\sum_{b=1}^{d^{\prime}}\tau^{a}\min_{j\in[k]}\|e_{ b+2(a-1)d^{\prime}}-c_{j}\|_{2}^{z}\] \[\geq\sum_{b=1}^{d^{\prime}}\tau^{i}\min_{j\in[k]}\|e_{b+2(i-1)d^{ \prime}}-c_{j}\|_{2}^{z}\] \[\geq(\tau^{i})(2^{z/2}d-2^{z/2}\cdot\max(1,z/2)\cdot\sqrt{dk}). \tag{1}\]

In particular, the unit vectors \(U_{i}=c_{1},\ldots,c_{k}\) have support entirely in the \(i\)-th group of \(2d^{\prime}\) coordinates in \(\mathbb{R}^{2d}\). By the same argument, there exists a set \(U_{i-1}\) with the same properties in the \((i-1)\)-th group of \(2d^{\prime}\) coordinates in \(\mathbb{R}^{2d}\).

By the correctness of the online coreset, we have

\[\operatorname{Cost}(U_{i-1},C_{i-1})\leq(1+\varepsilon)\operatorname{Cost}(U_ {i-1},X_{i-1})=(1+\varepsilon)\sum_{a=1}^{i-1}\operatorname{Cost}(U_{i-1},S_{ a}).\]

Since \(U_{i-1}\) consists of unit vectors and each substream \(S_{a}\) consists of unit vectors, then we have

\[\operatorname{Cost}(U_{i-1},S_{a})\leq 2d^{\prime}\tau^{a}.\]

Thus for \(\varepsilon\in(0,1)\),

\[\operatorname{Cost}(U_{i-1},C_{i-1})\leq 2\sum_{a=1}^{i-1}(2d^{\prime}\tau^{a}) \leq 8d^{\prime}\tau^{i-1}<\frac{1}{10}d^{\prime}\tau^{i},\]

since \(\tau=100\). On the other hand, since \(U_{i-1}\) has support entirely in the \((i-1)\)-th group of \(2d^{\prime}\) coordinates and \(S_{i}\) has support entirely in the \(i\)-th group of \(2d^{\prime}\) coordinates in \(\mathbb{R}^{2d}\), then

\[\operatorname{Cost}(U_{i-1},X_{i})\geq\operatorname{Cost}(U_{i-1},S_{i})\geq 2 d^{\prime}\tau^{i}.\]

Thus for \(C_{i}\) to be a \((1+\varepsilon)\)-online coreset for \(\varepsilon\in(0,1)\), \(C_{i}\) must sample additional points from \(X_{i}\) on top of \(C_{i-1}\). Hence, \(P_{i}\neq\emptyset\).

In particular, let \(P_{i}\) consist of vectors \(y_{1},\ldots,y_{m}\) with weights \(w_{1},\ldots,w_{m}\). Since \(P_{i}\neq\emptyset\), then

\[\operatorname{Cost}(U_{i},C_{i})=\operatorname{Cost}(U,C_{i-1}\cup P_{i})\leq \operatorname{Cost}(U,P_{i}).\]If \(|P_{i}|=o\left(\frac{k}{\varepsilon^{2}}\right)\), then by the second property of Theorem D.1, we have

\[\operatorname{Cost}(U_{i},P_{i})=\sum_{b=1}^{m}\min_{j\in[k]}w_{b}\|y_{b}-c_{j} \|_{2}^{2}<\tau^{i}(1-\varepsilon)(2^{z/2}d-2^{z/2}\cdot\max(1,z/2)\cdot\sqrt{ dk}),\]

which together with Equation 1 contradicts the fact that \(C_{i}\) is an \((1+\varepsilon)\)-online coreset for \(X_{i}\).

Therefore, we have \(|P_{i}|=\Omega\left(\frac{k}{\varepsilon^{2}}\right)\). Moreover, since \(P_{i}\) has disjoint support from \(C_{i-1}\), then by induction,

\[|C_{i}|=|C_{i-1}\cup P_{i}|=|C_{i-1}|+|P_{i}|=i\cdot\Omega\left(\frac{k}{ \varepsilon^{2}}\right).\]

**Theorem 1.4**.: _Let \(\varepsilon\in(0,1)\). For sufficiently large \(n\), \(d\), and \(\Delta\), there exists a set \(X\subset[\Delta]^{d}\) of \(n\) points \(x_{1},\ldots,x_{n}\) such that any \((1+\varepsilon)\)-online coreset for \(k\)-means clustering on \(X\) requires \(\Omega\left(\frac{k}{\varepsilon^{2}}\log n\right)\) points._

Proof.: Let \(\gamma=\Theta\left(\log\frac{n}{d^{\prime}}\right)\). For each \(i\in[\gamma]\), construct the stream \(S_{i}\) as in the statement of Lemma D.2. Observe that \(|S_{i}|=d^{\prime}\cdot t^{i}\) for \(t=100\) and so under the settings of the parameter \(\gamma\) with the appropriate constant, the total length of the stream \(S=S_{1}\circ\ldots\circ S_{\gamma}\) is precisely \(n\). Moreover, by Lemma D.2, any \((1+\varepsilon)\)-online coreset must store \(\gamma\cdot\Omega\left(\frac{k}{\varepsilon^{2}}\right)=\Omega\left(\frac{k}{ \varepsilon^{2}}\log n\right)\) points for \(n=\operatorname{poly}(d)\). 

## Appendix E On the Proof of Theorem b.2

We remark that Theorem 1 of [27] is stated for sampling a fixed number of points with replacement from each group, rather than sampling each point independently without replacement. By contrast, Theorem B.2 is stated for sampling each point independently without replacement. In this section, we briefly outline the proof of Theorem 1 of [27] and how the analysis translates to the statement of Theorem B.2.

At a high level, the coreset construction of [27] first collects rings of an approximate solution \(\mathcal{A}\) of \(k\) points into groups, using a similar approach to that described in Appendix B with \(\beta=1\). The algorithm then computes a coreset for each group first using a procedure GroupSample and then using a procedure SensitivitySample for some of the points not considered by the first procedure. We briefly describe both procedures, as well as how to adapt them to the setting where each point is sampled independently and without replacement.

### Adaptation of Group Sampling

The GroupSample procedure of [27] samples a fixed \(\Lambda_{1}\) number of points from each group \(G\) with probability proportional to the contribution of each corresponding cluster of the point to the group. That is, given clusters \(\widetilde{C_{1}},\ldots,\widetilde{C_{k}}\) induced by \(\mathcal{A}\) on \(G\), GroupSample then performs \(\Lambda_{1}\) rounds of sampling. Each round samples a single point, where a point \(p\in\widetilde{C_{i}}\) is sampled proportional to \(\frac{\operatorname{Cost}(\widetilde{C_{i}},\mathcal{A})}{|C_{i}|\operatorname {Cost}(G,\mathcal{A})}\) and rescaled appropriately. Then GroupSample offers the following guarantees:

**Lemma E.1** (Lemma 2 of [27]).: _Let \((X,\operatorname{dist})\) be a metric space, \(k\), \(z\) be positive integers, \(G\) be a group of clients and \(\mathcal{A}\) be an \(\alpha\)-approximate solution to \((k,z)\)-clustering on \(G\) so that:_

* _For every cluster_ \(\widetilde{C}\) _induced by_ \(\mathcal{A}\) _on_ \(G\)_, all points of_ \(\widetilde{C}\) _contribute the same cost in_ \(\mathcal{A}\) _up to a factor of_ \(2\)_._
* _For all clusters_ \(\widetilde{C}\) _induced by_ \(\mathcal{A}\) _on_ \(G\)_, we have that_ \(\frac{\operatorname{Cost}(G,\mathcal{A})}{2k}\leq\operatorname{Cost}( \widetilde{C},\mathcal{A})\)_._

_Let \(\mathbb{C}\) be an \(\mathcal{A}\)-approximate centroid set for \((k,z)\)-clustering on \(G\)._

_Then there exists a procedure GroupSample that constructs a set \(\Omega\) of size_

\[\Lambda_{1}=O\left(\frac{\max(\alpha^{2},\alpha^{z})\log^{2}\frac{1}{ \varepsilon}}{2^{O(z\log z)}\min(\varepsilon^{2},\varepsilon^{z})}\left(k\log| \mathbb{C}|+\log\log\frac{1}{\varepsilon}+\log n\right)\right),\]_such that with high probability, it simultaneously holds for all sets \(S\) of \(k\) centers that_

\[|\operatorname{Cost}(G,S)-\operatorname{Cost}(\Omega,S)|\leq O\left(\frac{ \varepsilon}{\alpha}\right)(\operatorname{Cost}(G,S)+\operatorname{Cost}(G, \mathcal{A}).\]

We outline the high-level approach of the proof of LemmaE.1 and how can it can adjusted for an \((\alpha,\beta)\)-approximate solution \(\mathcal{A}\), as well as a process that samples each point independently without replacement, rather than using \(\Lambda_{1}\) rounds as GroupSample.

The proof of LemmaE.1 involves further partitioning the points of \(G\) into three subsets, based on the cost induced by the point. Namely, given a set \(S\) of \(k\) centers, a point \(p\) in group \(G\) is categorized as tiny, interesting, or huge, depending on \(\operatorname{Cost}(p,S)\) (though the interesting and huge points actually have a small overlap to allow slack in the proof). [27] applies standard Chernoff bounds to show that the number of sampled points is well-concentrated around its expectation and then applies Bernstein's inequality to show that the clustering costs of the tiny points, the interesting points are well-concentrated around their expectations. In particular, they show that the expected number of sampled points from each cluster \(\widetilde{C_{i}}\) is

\[\frac{\Lambda_{1}\operatorname{Cost}(\widetilde{C_{i}},\mathcal{A})}{ \operatorname{Cost}(G,\mathcal{A})}\geq\frac{\Lambda_{1}}{2k},\]

due to the assumption that for all clusters \(\widetilde{C}\) induced by \(\mathcal{A}\) on \(G\), we have that \(\frac{\operatorname{Cost}(G,\mathcal{A})}{2k}\leq\operatorname{Cost}( \widetilde{C},\mathcal{A})\).

We first remark that if \(\mathcal{A}\) is an \((\alpha,\beta)\)-approximate solution rather than an \(\alpha\)-approximate solution, i.e., if \(\mathcal{A}\) has \(\beta k\) centers rather than \(k\) centers, then the definition of the rings and groups would instead insist that for all clusters \(\widetilde{C}\) induced by \(\mathcal{A}\) on \(G\), we have that \(\frac{\operatorname{Cost}(G,\mathcal{A})}{2\beta k}\leq\operatorname{Cost}( \widetilde{C},\mathcal{A})\). Then by oversampling \(\Lambda_{1}\) by a factor of \(\beta\), i.e., sampling \(\beta\Lambda_{1}\) points would ensure that the expected number of sampled points from each cluster \(\widetilde{C_{i}}\) would be

\[\frac{\beta\Lambda_{1}\operatorname{Cost}(\widetilde{C_{i}},\mathcal{A})}{ \operatorname{Cost}(G,\mathcal{A})}\geq\frac{\beta\Lambda_{1}}{2\beta k}=\frac {\Lambda_{1}}{k}.\]

It then remains to argue the correctness of sampling each point independently without replacement rather than a fixed \(\beta\Lambda_{1}\) number of points, which simply holds by adjusting the applications of the Chernoff bounds and Bernstein's inequality so that there is a separate random variable for each point in the input rather than for each of the \(\Lambda_{1}\) rounds.

### Adaptation of Sensitivity Sampling

The SensitivitySample procedure of [27] samples a fixed \(\Lambda_{2}\) number of points from each group \(G\) with probability proportional to the contribution of the point. Specifically, SensitivitySample then performs \(\Lambda_{2}\) rounds of sampling, where each round samples a point \(p\) in the group \(G\) with probability proportional to \(\frac{\operatorname{Cost}(p,\mathcal{A})}{\operatorname{Cost}(G,\mathcal{A})}\) and reaches the sampled point appropriately. Then SensitivitySample offers the following guarantee:

**Lemma E.2** (Lemma 3 of [27]).: _Let \((X,\operatorname{dist})\) be a metric space, \(k\), \(z\) be positive integers, and \(\mathcal{A}\) be an \(\alpha\)-approximate solution to \((k,z)\)-clustering on \(G\). Let \(\mathbb{C}\) be an \(\mathcal{A}\)-approximate centroid set for \((k,z)\)-clustering on \(G\). Let \(G\) be either a group \(G^{O}_{b}\) or \(G^{O}_{\max}\). Then there exists a procedure SensitivitySample that constructs a set \(\Omega\) of size_

\[\Lambda_{2}=O\left(\frac{2^{O(z\log z)}\alpha^{2}\log^{2}\frac{1}{\varepsilon} }{\varepsilon^{2}}\left(k\log|\mathbb{C}|+\log\log\frac{1}{\varepsilon}+\log n \right)\right),\]

_such that with high probability, it simultaneously holds for all sets \(S\) of \(k\) centers that_

\[|\operatorname{Cost}(G,S)-\operatorname{Cost}(\Omega,S)|\leq O\left(\frac{ \varepsilon}{\alpha z\log\frac{\varepsilon}{\varepsilon}}\right)(\operatorname {Cost}(G,S)+\operatorname{Cost}(G,\mathcal{A}).\]

We outline the high-level approach of the proof of LemmaE.2 and how can it can adjusted for an \((\alpha,\beta)\)-approximate solution \(\mathcal{A}\), as well as a process that samples each point independently without replacement, rather than using \(\Lambda_{2}\) rounds as SensitivitySample.

The proof of Lemma E.2 partitions the points of \(G\) into two categories, based on the cost induced by the point. Given a set \(S\) of \(k\) centers, the close points are the points \(p\) in \(G\) that have \(\operatorname{Cost}(p,S)\leq 4^{*}\cdot\operatorname{Cost}(p,\mathcal{A})\). The far points are the remaining points in \(G\), i.e., the points \(p\) in \(G\) with \(\operatorname{Cost}(p,S)>4^{*}\cdot\operatorname{Cost}(p,\mathcal{A})\).

[27] applies Bernstein's inequality to show that the clustering cost of the close points is well-concentrated around their expectations. We can again adjust the application of Bernstein's inequality so that there is a separate random variable for each point in the input rather than for each of the \(\Lambda_{2}\) samples.

To handle the far points, [27] again uses Bernstein's inequality to show that with high probability, the clustering points of these points with respect to \(S\) can be replaced with the distance to the closest center \(c\in\mathcal{A}\) plus the distance from \(c\) to the closest center in \(S\). Conditioned on this event, the latter distance can then be charged to the remaining points of the cluster from the original dataset, i.e., the remaining points of the cluster not necessarily restricted to group \(G\), which are significantly more numerous and already paying a similar value in \(S\). In particular, Bernstein's inequality utilizes the fact that the second moment of the estimated cost of a cluster \(C\) is at most

\[\frac{\operatorname{Cost}(G,\mathcal{A})}{\Lambda_{2}^{2}}\operatorname{Cost} (C\cap G,\mathcal{A})\leq\frac{2k}{\Lambda_{2}^{2}}(\operatorname{Cost}(C\cap G,\mathcal{A}))^{2},\]

for \(\beta=1\). Thus for general \(\beta\), we recover the same guarantee by oversampling \(\Lambda_{2}\) by a factor of \(\beta\), i.e., sampling \(\beta\Lambda_{2}\) points would ensure that the second moment would be at most \(\frac{2k}{\Lambda_{2}^{2}}\operatorname{Cost}^{2}(C\cap G,\mathcal{A})\). It then remains to argue the correctness of sampling each point independently without replacement rather than a fixed \(\beta\Lambda_{2}\) number of points, which again holds by adjusting the application of Bernstein's inequality so that there is a separate random variable for each point in the input rather than for each of the \(\Lambda_{2}\) rounds.

## Appendix F Additional Experiments on Synthetic Data

We first describe the methodology and experimental setup of our empirical evaluation on a synthetic dataset before detailing the experimental results. To emphasize the benefits of our algorithm against worst-case input, we generate a synthetic dataset that would fully capture the failure cases of previous baselines.

Dataset.We generated our dataset \(X\) consisting of \(200,001\) points on two-dimensional space so that \(100,000\) points were drawn from a spherical Gaussian with standard deviation \(2.75\) centered at \((-10,10)\) and \(100,000\) points were drawn from a spherical Gaussian with standard deviation \(2.75\) centered at \((10,-10)\). The final point of \(X\) was drawn from a spherical Gaussian with standard deviation \(2.75\) centered at \((100000,100000)\). Thus by construction of our synthetic dataset for \(k=3\), the optimal centers should be close to \((-10,10)\), \((10,-10)\), and \((100000,10000)\). We then create the data stream \(S\) by prepending two additional points drawn from spherical Gaussians with standard deviation \(2.75\) centered at \((-100000,100000)\) and \((-100000,-100000)\) respectively. We set the window length to be \(200,001\) in accordance with the "true" data set, so that the first two points of the stream of length \(200,003\) will be expired by the data stream.

Experimental setup.For each of the instances of Lloyd's algorithm, either on the entire dataset \(X\) or the sampled coreset \(C\), we use 3 iterations using the k-means++ initialization. In this case, the offline Lloyd's algorithm requires storing the entire dataset \(X\) in memory and thus its input size is \(200,001\) points. By comparison, we normalize the space requirement of the sublinear-space algorithms by permitting each algorithm to store \(m\in\{3,4,5,6,7,8,9,10,11,12\}\) points. Note that since \(k=3\), it would not be reasonable for \(C\) to have fewer than \(3\) points. We then run Lloyd's algorithm on the coreset \(C\), with 3 iterations using the k-means++ initialization.

By construction of our dataset, we generally expect the uniform sampling algorithm uni to be stable across the various values of \(m\) but perform somewhat poorly, as it will sample points from the large clusters but it will miss the point generated from the Gaussian centered at \((100000,100000)\). Since in our construction the stream \(S\) only contains two more points than the dataset \(X\), the histogram-based algorithm hist will not delete any points. Thus, the resulting coreset \(C\) generated by hist is somewhat likely contain the points generated from the Gaussians centered at and \((-100000,-100000)\) and can perform poorly on the synthetic dataset in these cases. Finally, since we allow the last point of the stream to be the single point of \(X\) far from the two large clusters, then the importance sampling based algorithm imp will sample the last point with high probability once any points of \(C\) have been expired. Hence by the construction of our stream, we expect imp to perform well.

Experimental results.For each choice of \(m\) and \(k\), we ran each algorithm \(50\) times and tracked the resulting clustering cost. As expected by our construction, our algorithm performed significantly better than the other sublinear-space algorithms. In fact, even though our algorithm was only permitted memory size \(m\in\{3,4,5,6,7,8,9,10,11,12\}\), our algorithm was quite competitive with the offline Lloyd's algorithm, which used memory size \(200,001\), i.e., the entire dataset. For \(k\geq 3\), uniform sampling performed relatively poorly but quite stably, because although it never managed to sample the point generated from the Gaussian centered at \((100000,100000)\), the two other Gaussian distributions were sufficiently close that any sampled point would serve as a relatively good center for points generated from the two distributions. Similarly, for fixed \(k=3\) in Figure 2(b), the importance sampling approach used by histogram-based algorithms performed the worse, by multiple orders of magnitude. We expect this is because we did not delete the points in \(S\setminus X\) from \(C\) and thus the resulting Lloyd's algorithm on \(C\) moved the centers far away from the centers of the Gaussian distributions that induced \(X\). A more optimized fine-tuned histogram-based algorithm would have searched for parameters that govern when to delete points from \(S\setminus X\), which have reduced the algorithm down to our main algorithm. We plot our results in Figure 3.

Figure 3: Comparison of average clustering costs made by uniform sampling, histogram-based algorithm, and our coreset-based algorithm across various settings of space allocated to the algorithm, given a synthetic dataset. For comparison, we also include the offline k-means++ algorithm as a baseline, though it is inefficient because it stores the entire dataset. Ranges are not plotted because they would not be visible.