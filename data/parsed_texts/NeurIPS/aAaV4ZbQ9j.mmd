# Navigating Chemical Space with Latent Flows

 Guanghao Wei

Cornell University

gw338@cornell.edu &Yining Huang

Harvard University

Yiinguhuang@hms.harvard.edu &Chenru Duan

Deep Principle, Inc.

duanchenru@gmail.com &Yue Song

California Institute of Technology

yuesong@caltech.edu &Yuanqi Du

Cornell University

yd392@cornell.edu

Equal Contribution.Equal Supervision.This work was completed while the author was at Cornell University.

###### Abstract

Recent progress of deep generative models in the vision and language domain has stimulated significant interest in more structured data generation such as molecules. However, beyond generating new random molecules, efficient exploration and a comprehensive understanding of the vast chemical space are of great importance to molecular science and applications in drug design and materials discovery. In this paper, we propose a new framework, ChemFlow, to traverse chemical space through navigating the latent space learned by molecule generative models through flows. We introduce a dynamical system perspective that formulates the problem as learning a vector field that transports the mass of the molecular distribution to the region with desired molecular properties or structure diversity. Under this framework, we unify previous approaches on molecule latent space traversal and optimization and propose alternative competing methods incorporating different physical priors. We validate the efficacy of ChemFlow on molecule manipulation and single- and multi-objective molecule optimization tasks under both supervised and unsupervised molecular discovery settings. Codes and demos are publicly available on GitHub at [https://github.com/garywei944/ChemFlow](https://github.com/garywei944/ChemFlow).

## 1 Introduction

Designing new functional molecules has been a long-standing challenge in molecular discovery which concerns a wide range of applications in drug design and materials discovery [48, 57]. With the increasing interest in applying deep learning models in scientific problems [60, 70], molecular design has attracted considerable attention given its massively available data and accessible evaluations. Among the developed methods, two paradigms emerge: one paradigm searches for new molecules based on combinatorial optimization approaches respecting the discrete nature of molecules; the other paradigm builds upon the success of deep generative models in approximating the molecular distribution with a given dataset and then generating new molecules from the learned models [8]. Both of the approaches have demonstrated promising results in small molecule, protein, and materials design [61, 26, 41, 69]. Despite the promise, the chemical space is tremendously large with the number of drug-like small molecule compounds estimated to be from \(10^{23}\) to \(10^{60}\)[3, 39]. This necessitates either more efficient searching methods or better understanding about the structure of the chemical space. Following the progress made in studying the latent structure of deep generative models (_e.g._ generative adversarial networks (GANs) [19], variational autoencoders (VAEs) [35],and denoising diffusion models [23]) in computer vision [28; 5; 22; 38], decent efforts have recently been made in understanding the learned latent space of molecule generative models.

Initially, disentangled representation learning becomes a popular paradigm to enforce a structured and interpretable representation [9]. Specifically, each latent dimension is expected to learn a disentangled factor of variation, and tweak the latent vector along the dimension could lead to generating new samples with changes only in one molecular property. However, even if imposing such constraints in the training of molecule generative models, the models still struggle to learn meaningful disentangled factors in the early attempts [10]. In addition to constraining the model training procedure, exploring the structure of pre-trained molecule generative models is more efficient. The main approach developed is to utilize optimization approaches to discover the region in the latent space with the desired molecular property. It often trains a proxy function to map from the latent vector to the property, providing access to gradients for gradient-based optimization [40; 20; 13]. The third line of work also builds upon pre-trained models as well. It leverages one interesting finding such that the learned latent space of molecule generative models is linearly separable [18], which is also widely studied and used as a priori in computer vision [51; 50]. ChemSpace [11] develops a highly efficient approach to use linear classifiers to identify the separation boundary and considers the normal direction to the boundary as the direction of control. Nevertheless, the linear separability assumption may be too strong. It is worth noting that the first line of work does not require labels and can be trained in an unsupervised manner (referred to as unsupervised discovery), while both the second and third lines of work require access to labels to train/identify a guidance model/direction (referred to as supervised discovery).

In this paper, we propose a new framework, _ChemFlow_, based on flows in a dynamical system to efficiently explore the latent structure of molecule generative models. Specifically, we unify previous approaches (gradient-based optimization, linear latent traversal, and disentangled traversal) under the realm of flows that transforms data density along time via a vector field. In contrast to previous linear models, our framework is flexible to learn nonlinear transformations inspired by partial differential equations (PDEs) governing real-world physical systems such as heat and wave equations. We then analyze how different dynamics may bring special properties to solve different tasks. Our framework can also generalize both supervised and unsupervised settings under the same umbrella. Particularly in the under-studied unsupervised setting, we demonstrate a structure diversity potential can be incorporated to find trajectories that maximize the structure change of the molecules (which in turn leads to property change). We conduct extensive experiments with physicochemical, drug-related properties, and protein-ligand binding affinities on both molecule manipulation and (single- and multi-objective) molecule optimization experiments. The experiment results demonstrate the generality of the proposed framework and the effectiveness of alternative methods under this framework to achieve better or comparable results with existing approaches.

## 2 Background

### Navigating Latent Space of Molecules

The latent space \(\mathcal{Z}\) of molecule generative models is often learned through an encoder function \(f_{\theta}(\cdot)\) and a decoder function \(g_{\psi}(\cdot)\) such that the encoder maps the input molecular structures \(\mathbf{x}\in\mathcal{X}\) into an (often) low-dimensional and continuous space (_i.e._ latent space) while the decoder maps the latent vectors \(\mathbf{z}\in\mathcal{Z}\) back to molecular structures \(x^{\prime}\). Note that this encoder-decoder architecture is general and can be realized by popular generative models such as VAEs, flow-based models, GANs, and diffusion models [31; 43; 6; 58; 66]. For simplicity, we focus on VAE-based methods in this paper. To traverse the learned latent space of molecule generative models, two approaches have been proposed: gradient-based optimization and latent traversal.

The gradient-based optimization methods first learn a proxy function \(h(\cdot)\) parameterized by a neural network that provides the direction to traverse [67]. This can be formulated as a gradient flow following the direction of steepest descent of the potential energy function \(h(\cdot)\) and discretized, as follows:

\[\mathbf{d}\mathbf{z}_{t} =-\nabla_{z}h(\mathbf{z}_{t})\mathbf{d}t \tag{1}\] \[\mathbf{z}_{t} =\mathbf{z}_{t-1}-\nabla_{z}h(\mathbf{z}_{t-1})\mathbf{d}t\]

where we take a dynamic system perspective on the evolution of latent samples. The latent traversal approaches leverage the observation of linear separability in the learned latent space of molecule generative models [18]. Since the direction is assumed to be linear, it can be found easily. ChemSpace [11] learns a linear classifier that defines the separation boundary of the molecular properties. Then the normal direction of the boundary provides a linear direction \(\mathbf{n}\in\mathcal{Z}\) for traversing the latent space:

\[\mathbf{z}_{t}=\mathbf{z}_{0}+\mathbf{n}t \tag{2}\]

We notice that the above gradient flow and linear traversal can be analyzed and designed from a dynamical system perspective: linear traversal can be considered as a special case of wave functions, _i.e._, we have \(\nicefrac{{\partial^{2}\mathbf{x}_{t}}}{{\partial^{2}\mathbf{x}_{t-1}}}=\nicefrac{{ \partial^{2}\mathbf{x}_{t}}}{{\partial^{2}t}}=0\) satisfied by wave functions. This connection inspires us to consider designing more dynamical traversal approaches in the latent space.

### Wasserstein Gradient Flows

Gradient flows define the curve \(\mathbf{x}(t)\in\mathbb{R}^{n}\) that evolves in the negative gradient direction of a function \(\mathcal{F}:\mathbb{R}^{n}\rightarrow\mathbb{R}\). The time evolution of the gradient flow is given by the ODE \(\mathbf{x}^{\prime}(t)=-\nabla\mathcal{F}(\mathbf{x}(t))\). Wasserstein gradient flows describe a special type of gradient flow where \(\mathcal{F}\) is set to be the Wasserstein distance. For example, as introduced in Benamou and Brenier [2], the commonly used \(L^{2}\) Wasserstein metric induces a dynamic formulation of optimal transport:

\[W_{2}(\mu,\nu)^{2}=\min_{v,\rho}\Big{\{}\int\int\frac{1}{2}\rho(t,\mathbf{x})|v(t, \mathbf{x})|^{2}\,dt\,d\mathbf{x}:\partial_{t}\rho(t,\mathbf{x})=-\nabla\cdot(v(t,\mathbf{x}) \rho(t,\mathbf{x}))\Big{\}} \tag{3}\]

where \(\mu,\nu\) are two probability measures at the source and target distributions, respectively. Interestingly, if we take the gradient of a potential energy \(\nabla\phi\) as the velocity field applied to a distribution, the time evolution of \(\nabla\phi\) can be seen to minimize the Wasserstein distance and thus follow optimal transport. In Appendix A, we give detailed derivations of how the vector fields minimize the \(L^{2}\) Wasserstein distance and discuss alternative PDEs of the density evolution recovered by Wasserstein gradient flow (_e.g._ Wasserstein gradient flow over the entropy functional recovers heat equation) following the seminal JKO scheme [34].

## 3 Methodology

We present _ChemFlow_ as a unified framework for latent traversals in chemical latent space as latent flows. We parameterize a set of scalar-valued energy functions \(\phi^{k}=\textsc{MLP}_{\theta^{k}}(t,\mathbf{z})\in\mathbb{R}\) and use the

Figure 1: _ChemFlow_ framework: (1) a pre-trained encoder \(f_{\theta}(\cdot)\) and decoder \(g_{\psi}(\cdot)\) that maps between molecules \(\mathbf{x}\) and latent vectors \(\mathbf{z}\), (2) we use a property predictor \(h_{\eta}(\cdot)\) (green box) or a “Jacobian control” (yellow box) as the guidance to learn a vector field \(\nabla_{z}\phi^{k}(t,\mathbf{z}_{t})\) that maximizes the change in certain molecular properties (e.g. plogP, QED) or molecular structures, (3) during the training process, we add additional dynamical regularization on the flow. The learned flows move the latent samples to change the structures and properties of the molecules smoothly. (Better seen in color). The flow chart illustrates a case where a molecule is manipulated into a drug like caffeine.

learned flow \(\nabla_{\mathbf{z}}\phi^{k}\) to traverse the latent samples. The traversal process can be described by the following equation in a Lagrangian way (particle trajectory):

\[\mathbf{z}_{t}=\mathbf{z}_{t-1}+\nabla_{\mathbf{z}}\phi^{k}(t-1,\mathbf{z}_{t-1}) \tag{4}\]

Alternatively, as an Eulerian approach, we can write the time evolution of the density through a pushforward map:

\[\rho_{t}=[\psi_{t}]_{*}\rho_{t-1} \tag{5}\]

where \(\psi_{t}\) defines the time-dependent flow that transforms the densities of latent samples through a probability path. The pushforward measure \([\psi_{t}]_{*}\) induces a change of variable formula for densities [47]:

\[[\psi_{t}]_{*}\rho_{t-1}(\mathbf{z})=\rho_{t-1}(\psi_{t}^{-1}(\mathbf{z}))|\det\Big{[} \frac{\partial\psi_{t}^{-1}(\mathbf{z})}{\partial\mathbf{z}}\Big{]}| \tag{6}\]

In the following, we will introduce how \(\nabla_{\mathbf{z}}\phi^{k}\) is matched to some pre-defined velocities for generating different flows.

### Learning Different Latent Flows

Given a pre-trained molecule generative model \(g_{\psi}:\mathcal{Z}\rightarrow\mathcal{X}\) with prior distribution \(p(\mathbf{z})\), we would like to model \(K\) different semantically disentangled latent trajectories that correspond to different properties of the molecules, numbered by superscript \(k\).

**Hamilton-Jacobi Flows.** One desired property for the latent traversal comes from optimal transport theory such that the transport cost is minimized (i.e. shortest path). This property can be enforced by solving Eq. (3) by Karush-Kuhn-Tucker (KKT) conditions, which will give the optimal solution -- the Hamilton-Jacobi Equation (HJE):

\[\frac{\partial}{\partial t}\phi^{k}(t,\mathbf{z})+\frac{1}{2}||\nabla_{\mathbf{z}}\phi ^{k}(t,\mathbf{z})||^{2}=0 \tag{7}\]

where the velocity field is defined as the flow \(\nabla\phi^{k}\). The HJE can also be interpreted as mass transportation in fluid dynamics, _i.e._, under the velocity field \(\nabla\phi^{k}\), the fluid will evolve to the target distribution with an optimal transportation cost.

We achieve the HJE constraint by matching our flow fields and define the boundary condition as:

\[\mathcal{L}_{r}=\frac{1}{T}\sum_{t=0}^{T-1}\big{(}\frac{\partial}{\partial t} \phi^{k}(t,\mathbf{z})+\frac{1}{2}||\nabla_{\mathbf{z}}\phi^{k}(t,\mathbf{z})||^{2}\big{)} ^{2},\ \mathcal{L}_{\phi}=\sum_{k=0}^{K-1}||\nabla_{\mathbf{z}}\phi^{k}(0,\mathbf{z}_{0})|| _{2}^{2} \tag{8}\]

where \(T\) represents the total number of traversal steps, \(\mathcal{L}_{r}\) restricts the energy to obey our physical constraints, and \(\mathcal{L}_{\phi}\) restricts \(\phi(t,\mathbf{z}_{t})\) to match the initial condition. Our latent traversal can be thus regarded as dynamic optimal transport between distributions of molecules with different properties.

**Wave Flows.** Alternatively, we can pivot the optimal transport property to enforce additional physical and dynamical priors. For example, if we specify the flow to follow wave-like dynamics, we can use the second-order wave equation:

\[\frac{\partial^{2}}{\partial t^{2}}\phi^{k}(t,\mathbf{z})-c^{2}\nabla_{\mathbf{z}}^{2} \phi^{k}(t,\mathbf{z})=0 \tag{9}\]

The above constraint empirically produces highly diverse and realistic trajectories. Our velocity matching objective and boundary condition then become:

\[\mathcal{L}_{r}=\frac{1}{T}\sum_{t=0}^{T-1}||\frac{\partial^{2}}{\partial t^{2 }}\phi^{k}(t,\mathbf{z})-c^{2}\nabla_{\mathbf{z}}^{2}\phi^{k}(t,\mathbf{z})||_{2}^{2},\ \mathcal{L}_{\phi}=\sum_{k=0}K-1||\nabla_{\mathbf{z}}\phi^{k}(0,\mathbf{z}_{0})||_{2}^{2} \tag{10}\]

where \(\mathcal{L}_{r}\) and \(\mathcal{L}_{\phi}\) restrict the physical constraints and the initial condition, respectively. Note that \(\phi^{k}\equiv 0\) is a trivial optimal solution for the above two objectives regarding that both \(\mathcal{L}_{r}\) and \(\mathcal{L}_{\phi}\) are non-negative. To prevent the parameterized \(\phi^{k}\) from converging to such a trivial solution, we introduce more guidance to the loss function in Section 3.2 separately.

**Alternative Flows.** Besides HJE and wave equations, our framework is also general to include other commonly used PDEs that allow for different dynamics along the flow, such as Fokker Planck equation and heat equation. In the experimental section, we will explore the effectiveness of each latent flow in different supervision settings.

### Supervised & Unsupervised Guidance

**Supervised Semantic Guidance.** When an explicit semantic potential energy function or labeled data for the semantic of interest is available, we can use the provided semantic potential to guide the learning of the flow. Firstly, we train a surrogate model \(h_{\eta}:\mathcal{X}\rightarrow\mathbb{R}\) (parameterized by a deep neural network) to predict the corresponding molecular property. Then we use the trained surrogate model as guidance to learn flows that drive the increase of the property for the trajectory of the generated molecules.

\[d=\langle-\nabla_{\mathbf{z}}h_{\eta}(g_{\psi}(\mathbf{z}_{t})),\nabla_{\mathbf{z}}\phi^{k }(t,\mathbf{z}_{t})\rangle,\;\mathcal{L}_{\mathcal{P}}=-\operatorname{sign}(d)\|d \|_{2}^{2} \tag{11}\]

The intuition behind this objective is to learn the vector field \(\mathbf{z}_{t}\) such that it aligns with the direction of the steepest descent (negative gradient) of the objective function. Note that the sign of the dot product matters as it determines minimizing or maximizing the property.

The proposed objective function in the supervised scenario is

\[\mathcal{L}=\mathcal{L}_{r}+\mathcal{L}_{\phi}+\mathcal{L}_{\mathcal{P}}\]

**Unsupervised Diversity Guidance.** When no explicit potential energy function is provided to learn the flow, we need to define a potential energy function that captures the change of molecular properties. As molecular properties are determined by the structures, we devise a potential energy that maximizes the continuous structure change of the generated molecules. Inspired by Song et al. [54], we couple the traversal direction with the Jacobian of the generator to maximize the traversal variations in the molecular space. The perturbation on latent samples can be approximated by the first-order Taylor approximation:

\[g(\mathbf{z}_{t}+\epsilon\nabla_{\mathbf{z}}\phi^{k}(t,\mathbf{z}_{t}))=g(\mathbf{z}_{t})+ \epsilon\frac{\partial g(\mathbf{z}_{t})}{\partial\mathbf{z}_{t}}\nabla_{\mathbf{z}}\phi^{ k}(t,\mathbf{z}_{t})+R_{1}(g(\mathbf{z}_{t})) \tag{12}\]

where \(\epsilon\) denotes perturbation strength, and \(R_{1}(\cdot)\) is the high-order terms. In the unsupervised setting, for sufficiently small \(\epsilon\), if the Jacobian-vector product can cause large variations in the generated sample, the direction is likely to correspond to certain properties of molecules. We therefore introduce such a Jacobian-vector product guidance:

\[\mathcal{L}_{\mathcal{J}}=-\left\|\frac{\partial g(\mathbf{z}_{t})}{\partial\mathbf{z} _{t}}\nabla_{\mathbf{z}}\phi^{k}(t,\mathbf{z}_{t})\right\|_{2}^{2} \tag{13}\]

Compared to the supervised setting which maximizes the change of the molecular properties, it aims to find the direction that causes the maximal change of the structures. This can in turn effectively pushes the initial data distribution to the target one concentrated on the maximum property value. The Jacobian guidance will compete with the dynamical regularization (e.g. wave-like form) on the flow to yield smooth and meaningful traversal paths.

**Disentanglement Regularization.** While the above formulation can encourage smooth dynamics and meaningful output variations, the flows are likely to mine identical directions which all correspond to the maximum Jacobian change. To avoid such a trivial solution, we adopt an auxiliary classifier \(l_{\gamma}\) following Song et al. [54] to predict the flow index and use the cross-entropy loss to optimize it:

\[\mathcal{L}_{k}=\mathcal{L}_{CE}(l_{\gamma}(g_{\psi}(\mathbf{z}_{t});g_{\psi}(\mathbf{ z}_{t+1})),k) \tag{14}\]

Where \(\mathbf{x}_{t}=g(\mathbf{z}_{t})\) is the generated sample from timestep \(t\). We see the extra classifier guidance would encourage each flow to be independent and find distinct properties. For each target property, we compute the Pearson correlation coefficient using a randomly generated test set. This coefficient measures the correlation between the property and a natural sequence (from 1 to time step \(t\)) along the optimization trajectory. We then select the energy network that achieves the highest correlation score for optimizing molecules with that specific property.

The proposed objective function in the unsupervised scenario is

\[\mathcal{L}=\mathcal{L}_{r}+\mathcal{L}_{\phi}+\mathcal{L}_{\mathcal{J}}+ \mathcal{L}_{k}\]

### Connection with Langevin Dynamics for Global Optimization

In scenarios where our flow adheres to the dynamics of the Fokker-Planck equation, our approach may also be interpreted as employing a learned potential energy function to simulate Langevin Dynamicsfor global optimization [15]. Notably, the convergence of Langevin dynamics, particularly at low temperatures, tends to occur around the global minima of the potential energy function [7]. The continuous and discretized Langevin dynamics are as follows:

\[\begin{split}\mathsf{d}\mathbf{z}_{t}&=-\nabla_{z}h_{\eta }(\mathbf{z}_{t})\mathsf{d}t+\sqrt{2}\mathsf{d}\mathbf{w}_{t}\\ \mathbf{z}_{t}&=\mathbf{z}_{t-1}-\nabla_{z}h_{\eta}(\mathbf{z}_{ t-1})\mathsf{d}t+\sqrt{2\mathsf{d}t}\mathcal{N}(0,I)\end{split} \tag{15}\]

**Proposition 3.1**.: _(Global Convergence of Langevin Dynamics, adapted from Gelfand and Mitter [16]). Given a Langevin dynamics in the form of_

\[\mathbf{z}_{t}=\mathbf{z}_{t-1}-a_{t}(\nabla_{z}h_{\eta}(\mathbf{z}_{t-1})+\mathbf{u}_{t})+b_{t }\mathbf{w}_{t}\]

_where \(\mathbf{w}_{t}\) is a \(d\)-dimensional Brownian motion, \(a_{t}\) and \(b_{t}\) are a set of positive numbers with \(a_{T},b_{T}\to 0\), and \(\mathbf{u}_{t}\) is a set of random variables in \(\mathbb{R}^{n}\) denoting noisy measurements of the energy function \(h_{\eta}(\cdot)\). Under mild assumptions, \(\mathbf{z}_{t}\) converges to the set of global minima of \(h_{\eta}(\cdot)\) in probability._

Following Section3.1, the learned latent flow can be used to search for molecules with optimal properties and it converges to the global minimizers of the learned latent potential energy function.

## 4 Experiments

### Experiment Set-up

Datasets & Molecular properties.We extract 4,253,577 molecules from the three commonly used datasets for drug discovery including **MOSES**[46], **ZINC250K**[27], and **ChEMBL**[68]. Molecules are represented by SELFIES strings [37]. All input molecules are padded to the maximum length in the dataset before fitting into the generative model. We consider a total of 8 molecular properties which include _3 general drug-related properties_ -- Quantitative Estimate of Drug-likeness (QED), Synthesis Accessibility (SA), and penalized Octanol-water Partition Coefficient (plogP) and _3 machine learning-based target activities_ -- DRD2, JNK3 and GSK3B [25], _2 simulation-based target activities_ -- docking scores for two human proteins ESR1 and ACAA1. See SectionD.3 for details.

Implementations.We establish our framework by pre-training a VAE model that learns a latent space of molecules that can generate new molecules by decoding latent vectors from the latent space. We adapt the framework in Eckmann et al. [13] which is a basic VAE architecture with molecular SELFIES string representations and an additional MLP model as the surrogate property predictor. See SectionD.5 for all implementation and hyper-parameter details.

Model variants.As discussed in Section3.1, our proposed framework is general to incorporate different dynamical priors to learn the flow. For the experiments, we consider four types of dynamics including _gradient flow (GF)_, _Wave flow (Wave, eq. (9))_, _Hamilton Jacobi flow (HJ, eq. (7))_ and _Langevin Dynamics or equivalently Fokker Planck flow (LD, eq. (15))_.

For the specific molecular properties and evaluations, the readers are kindly referred to AppendixD for details. We also move qualitative evaluations to AppendixF due to space limit.

### Molecule Optimization

Molecule optimization is key in drug design and materials discovery, aiming to identify molecules with optimal properties [4]. Various machine learning methods have accelerated this process [8]. Our discussion focuses on optimization within the latent space of generative models, primarily using gradient-based optimization as outlined in Section2.1. We categorize molecule optimization into three scenarios: (1) unconstrained optimization to identify molecules with the best properties, (2) constrained optimization to find molecules with the best-expected property and similar to specific structures--a common step in the lead optimization process, and (3) multi-objective optimization to simultaneously enhance multiple properties of a molecule.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

Among the quantitative results, it is interesting that the random direction achieves a good relaxed success rate for some properties, we argue this is because of the specific property of the learned latent space. The latent space learned by generative models tend to be smooth such that similar molecular structures are often mapped to close areas in the latent space. In Appendix E, we find that some molecular properties are highly correlated with their latent vector norms, in which a random direction always increases the norm and thus successfully manipulates a portion of molecules by chance.

## 5 Conclusion, Limitation and Future Work

In this paper, we propose a unified framework for navigating chemical space through the learned latent space of molecule generative models. Specifically, we formulate the traversal process as a flow that defines a vector to transport the mass of molecular distribution through time to desired concentrations (e.g. high properties). Two forces (supervised potential guidance and unsupervised structure diversity guidance) are derived to drive the dynamics. We also propose a variety of new physical PDEs on the dynamics which exhibit different properties. We hope this general framework can open up a new research avenue to study the structure and dynamics of the latent space of molecule generative models.

**Limitation and future work.** This work is a preliminary study on small molecules and it may be interesting to see it transfer to larger molecular systems or more specialized systems and properties. Beyond molecules, this approach has the potential to be extended to languages and other data modalities.

## 6 Acknowledgement

Y.D. would like to thank Ziming Liu and Kirill Neklyudov for helpful discussions.

## References

* [1] L. Ambrosio, N. Gigli, and G. Savare. _Gradient flows: in metric spaces and in the space of probability measures_. Springer Science & Business Media, 2005.
* [2] J.-D. Benamou and Y. Brenier. A computational fluid mechanics solution to the monge-kantorovich mass transfer problem. _Numerische Mathematik_, 84(3):375-393, 2000.
* [3] R. S. Bohacek, C. McMartin, and W. C. Guida. The art and practice of structure-based drug design: a molecular modeling perspective. _Medicinal research reviews_, 16(1):3-50, 1996.
* [4] N. Brown, M. Fiscato, M. H. Segler, and A. C. Vaucher. Guacamol: benchmarking models for de novo molecular design. _Journal of chemical information and modeling_, 59(3):1096-1108, 2019.
* [5] C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. Understanding disentangling in \(\beta\)-vae. _ArXiv_, abs/1804.03599, 2018.
* [6] N. D. Cao and T. Kipf. MolGAN: An implicit generative model for small molecular graphs, 2018.
* [7] T.-S. Chiang, C.-R. Hwang, and S. J. Sheu. Diffusion for global optimization in r'n. _SIAM Journal on Control and Optimization_, 25(3):737-753, 1987.
* [8] Y. Du, T. Fu, J. Sun, and S. Liu. Molgensurvey: A systematic survey in machine learning models for molecule design. _arXiv preprint arXiv:2203.14500_, 2022.
* [9] Y. Du, X. Guo, A. Shehu, and L. Zhao. Interpretable molecular graph generation via monotonic constraints. In _Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)_, pages 73-81. SIAM, 2022.
* [10] Y. Du, X. Guo, Y. Wang, A. Shehu, and L. Zhao. Small molecule generation via disentangled representation learning. _Bioinformatics_, 38(12):3200-3208, 2022.
* [11] Y. Du, X. Liu, N. M. Shah, S. Liu, J. Zhang, and B. Zhou. Chemspace: Interpretable and interactive chemical space exploration. _Trans. Mach. Learn. Res._, 2023, 2023.

* [12] A. Z. Dudek, T. Arodz, and J. Galvez. Computational methods in developing quantitative structure-activity relationships (qsar): a review. _Combinatorial chemistry & high throughput screening_, 9(3):213-228, 2006.
* [13] P. Eckmann, K. Sun, B. Zhao, M. Feng, M. Gilson, and R. Yu. Limo: Latent inceptionism for targeted molecule generation. In _International Conference on Machine Learning_, pages 5777-5792. PMLR, 2022.
* [14] T. Fu, W. Gao, C. Coley, and J. Sun. Reinforced genetic algorithm for structure-based drug design. _Advances in Neural Information Processing Systems_, 35:12325-12338, 2022.
* [15] C. W. Gardiner et al. _Handbook of stochastic methods_, volume 3. springer Berlin, 1985.
* [16] S. B. Gelfand and S. K. Mitter. Recursive stochastic algorithms for global optimization in r^d. _SIAM Journal on Control and Optimization_, 29(5):999-1018, 1991.
* [17] L. Goetschalckx, A. Andonian, A. Oliva, and P. Isola. Ganalyze: Toward visual definitions of cognitive image properties. In _ICCV_, 2019.
* [18] R. Gomez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernandez-Lobato, B. Sanchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. _ACS central science_, 4(2):268-276, 2018.
* [19] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio. _Deep learning_, volume 1. MIT Press, 2016.
* [20] R.-R. Griffiths and J. M. Hernandez-Lobato. Constrained bayesian optimization for automatic chemical design using variational autoencoders. _Chemical science_, 11(2):577-586, 2020.
* [21] G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias, and A. Aspuru-Guzik. Objective-reinforced generative adversarial networks (organ) for sequence generation models. _arXiv preprint arXiv:1705.10843_, 2017.
* [22] E. Harkonen, A. Hertzmann, J. Lehtinen, and S. Paris. Ganspace: Discovering interpretable gan controls. _Advances in neural information processing systems_, 33:9841-9850, 2020.
* [23] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* [24] E. Hoogeboom, V. G. Satorras, C. Vignac, and M. Welling. Equivariant diffusion for molecule generation in 3d. In _International conference on machine learning_, pages 8867-8887. PMLR, 2022.
* [25] K. Huang, T. Fu, W. Gao, Y. Zhao, Y. Roohani, J. Leskovec, C. W. Coley, C. Xiao, J. Sun, and M. Zitnik. Artificial intelligence foundation for therapeutic science. _Nature chemical biology_, 18(10):1033-1036, 2022.
* [26] J. B. Ingraham, M. Baranov, Z. Costello, K. W. Barber, W. Wang, A. Ismail, V. Frappier, D. M. Lord, C. Ng-Thow-Hing, E. R. Van Vlack, et al. Illuminating protein space with a programmable generative model. _Nature_, pages 1-9, 2023.
* [27] J. J. Irwin and B. K. Shoichet. Zinc- a free database of commercially available compounds for virtual screening. _Journal of chemical information and modeling_, 45(1):177-182, 2005.
* [28] A. Jahanian, L. Chai, and P. Isola. On the" steerability" of generative adversarial networks. In _International Conference on Learning Representations_, 2019.
* [29] A. Jahanian, L. Chai, and P. Isola. On the" steerability" of generative adversarial networks. _ICLR_, 2020.
* [30] J. H. Jensen. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. _Chemical science_, 10(12):3567-3572, 2019.
* [31] W. Jin, R. Barzilay, and T. Jaakkola. Junction tree variational autoencoder for molecular graph generation. _ICML_, 2018.
* [32] W. Jin, R. Barzilay, and T. Jaakkola. Hierarchical generation of molecular graphs using structural motifs. In _International Conference on Machine Learning_, 2020.
* [33] J. Jo, S. Lee, and S. J. Hwang. Score-based generative modeling of graphs via the system of stochastic differential equations. In _International Conference on Machine Learning_, pages 10362-10383. PMLR, 2022.

* [34] R. Jordan, D. Kinderlehrer, and F. Otto. The variational formulation of the fokker-planck equation. _Siam Journal on Applied Mathematics_, 1996.
* [35] D. P. Kingma and M. Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [36] M. Krenn, F. Hase, A. Nigam, P. Friederich, and A. Aspuru-Guzik. Self-referencing embedded strings (selfies): A 100% robust molecular string representation. _Machine Learning: Science and Technology_, 1, 2019.
* [37] M. Krenn, F. Hase, A. Nigam, P. Friederich, and A. Aspuru-Guzik. Self-referencing embedded strings (selfies): A 100% robust molecular string representation. _Machine Learning: Science and Technology_, 1 (4):045024, 2020.
* [38] M. Kwon, J. Jeong, and Y. Uh. Diffusion models already have a semantic latent space. In _The Eleventh International Conference on Learning Representations_, 2022.
* [39] C. A. Lipinski, F. Lombardo, B. W. Dominy, and P. J. Feeney. Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings. _Advanced drug delivery reviews_, 64:4-17, 2012.
* [40] Q. Liu, M. Allamanis, M. Brockschmidt, and A. Gaunt. Constrained graph variational autoencoders for molecule design. _Advances in neural information processing systems_, 31, 2018.
* [41] H. Loeffler, J. He, A. Tibo, J. P. Janet, A. Voronov, L. Mervin, and O. Engkvist. Reinvent4: Modern ai-driven generative molecule design. 2023.
* [42] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2017.
* [43] K. Madhawa, K. Ishiguro, K. Nakago, and M. Abe. Graphnvp: An invertible flow model for generating molecular graphs. _arXiv preprint arXiv:1905.11600_, 2019.
* [44] D. Misra. Mish: A self regularized non-monotonic activation function. In _British Machine Vision Conference_, 2020.
* [45] W. Peebles, J. Peebles, J.-Y. Zhu, A. Efros, and A. Torralba. The hessian penalty: A weak prior for unsupervised disentanglement. In _ECCV_, 2020.
* [46] D. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. A. Artamonov, V. Aladinskiy, M. Veselov, A. Kadurin, S. I. Nikolenko, A. Aspuru-Guzik, and A. Zhavoronkov. Molecular sets (moses): A benchmarking platform for molecular generation models. _Frontiers in Pharmacology_, 11, 2018.
* [47] D. Rezende and S. Mohamed. Variational inference with normalizing flows. In _International conference on machine learning_, pages 1530-1538. PMLR, 2015.
* [48] B. Sanchez-Lengeling and A. Aspuru-Guzik. Inverse molecular design using machine learning: Generative models for matter engineering. _Science_, 361(6400):360-365, 2018.
* [49] A. Schneuing, Y. Du, C. Harris, A. Jamasb, I. Igashov, W. Du, T. Blundell, P. Lio, C. Gomes, M. Welling, et al. Structure-based drug design with equivariant diffusion models. _arXiv preprint arXiv:2210.13695_, 2022.
* [50] Y. Shen and B. Zhou. Closed-form factorization of latent semantics in gans. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1532-1540, 2021.
* [51] Y. Shen, C. Yang, X. Tang, and B. Zhou. Interfacegan: Interpreting the disentangled face representation learned by gans. _IEEE transactions on pattern analysis and machine intelligence_, 44(4):2004-2018, 2020.
* [52] Y. Song, N. Sebe, and W. Wang. Orthogonal svd covariance conditioning and latent disentanglement. _IEEE T-PAMI_, 2022.
* [53] Y. Song, A. Keller, N. Sebe, and M. Welling. Flow factorzied representation learning. In _NeurIPS_, 2023.
* [54] Y. Song, A. Keller, N. Sebe, and M. Welling. Latent traversals in generative models as potential flows. _arXiv preprint arXiv:2304.12944_, 2023.
* [55] Y. Song, J. Zhang, N. Sebe, and W. Wang. Householder projector for unsupervised latent semantics discovery. In _ICCV_, 2023.

* [56] Y. Song, T. A. Keller, Y. Yue, P. Perona, and M. Welling. Unsupervised representation learning from sparse transformation analysis. _arXiv preprint arXiv:2410.05564_, 2024.
* [57] J. Vamathevan, D. Clark, P. Czodrowski, I. Dunham, E. Ferran, G. Lee, B. Li, A. Madabhushi, P. Shah, M. Spitzer, et al. Applications of machine learning in drug discovery and development. _Nature reviews Drug discovery_, 18(6):463-477, 2019.
* [58] C. Vignac, I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and P. Frossard. Digress: Discrete denoising diffusion for graph generation. In _The Eleventh International Conference on Learning Representations_, 2023.
* [59] A. Voynov and A. Babenko. Unsupervised discovery of interpretable directions in the gan latent space. In _ICML_, 2020.
* [60] H. Wang, T. Fu, Y. Du, W. Gao, K. Huang, Z. Liu, P. Chandak, S. Liu, P. Van Katwyk, A. Deac, et al. Scientific discovery in the age of artificial intelligence. _Nature_, 620(7972):47-60, 2023.
* [61] J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, et al. De novo design of protein structure and function with rdfdiffusion. _Nature_, 620(7976):1089-1100, 2023.
* [62] Y. Xie, C. Shi, H. Zhou, Y. Yang, W. Zhang, Y. Yu, and L. Li. Mars: Markov molecular sampling for multi-objective drug discovery. _ArXiv_, abs/2103.10432, 2021.
* [63] X. Yang, J. Zhang, K. Yoshizoe, K. Terayama, and K. Tsuda. ChemTS: an efficient python library for de novo molecular generation. _Science and technology of advanced materials_, 18(1):972-976, 2017.
* [64] J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In _NIPS_, 2018.
* [65] J. You, B. Liu, R. Ying, V. S. Pande, and J. Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In _Neural Information Processing Systems_, 2018.
* [66] Y. You, R. Zhou, J. Park, H. Xu, C. Tian, Z. Wang, and Y. Shen. Latent 3d graph diffusion. In _The Twelfth International Conference on Learning Representations_, 2024.
* [67] C. Zang and F. Wang. Moflow: an invertible flow model for generating molecular graphs. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 617-626, 2020.
* D1192, 2023.
* [69] C. Zeni, R. Pinsler, D. Zugner, A. Fowler, M. Horton, X. Fu, S. Shysheya, J. Crabbe, L. Sun, J. Smith, et al. Mattergen: a generative model for inorganic materials design. _arXiv preprint arXiv:2312.03687_, 2023.
* [70] X. Zhang, L. Wang, J. Helwig, Y. Luo, C. Fu, Y. Xie, M. Liu, Y. Lin, Z. Xu, K. Yan, et al. Artificial intelligence for science in quantum, atomistic, and continuum systems. _arXiv preprint arXiv:2307.08423_, 2023.
* [71] Z. Zhou, S. M. Kearnes, L. Li, R. N. Zare, and P. F. Riley. Optimization of molecules via deep reinforcement learning. _Scientific Reports_, 9, 2018.
* [72] X. Zhu, C. Xu, and D. Tao. Learning disentangled representations with latent variation predictability. In _ECCV_, 2020.

## Appendix for ChemFlow

* Wasserstein Gradient Flow
* PDE-regularized Latent Space Learning
* Extended Related Work
* Machine Learning for Molecule Generation
* Goal-oriented Molecule Generation
* Image Editing in the Latent Space
* Experiments Details
* Training Dataset
* Molecule Properties
* Training and inference
* Experiments Setup
* Evaluation Metrics
* Additional Baselines
* More Experiment Results
* Latent Space Visualization and Analysis
* Qualitative Evaluations
Wasserstein Gradient Flow

As shown in the main paper, based on the dynamic formulation of optimal transport [2], the \(L_{2}\) Wasserstein distance can be re-written as:

\[W_{2}(\mu_{0},\mu_{1})=\min_{\rho,v}\sqrt{\int\int\rho_{t}(\mathbf{z})|v_{t}(\mathbf{z}) |^{2}\,d\mathbf{z}dt} \tag{16}\]

where \(v_{t}(\mathbf{z})\) is the velocity of the particle at position \(\mathbf{z}\) and time \(t\), and \(\rho_{t}(\mathbf{z})\) is the density \(d\mu(\mathbf{z})=\rho_{t}(\mathbf{z})d\mathbf{z}\). The distance can be optimized by the gradient flow of a certain function on space and time. Consider the functional \(\mathcal{F}:\mathbb{R}^{n}\to\mathbb{R}\) that takes the following form:

\[\mathcal{F}(\mu)=\int U(\rho_{t}(\mathbf{z}))\,d\mathbf{z} \tag{17}\]

The curve is considered as a gradient flow if it satisfies \(\nabla\mathcal{F}=-\frac{d}{dt}\rho_{t}(\mathbf{z})\)[1]. Moving the particles leads to:

\[\frac{d}{dt}\mathcal{F}(\mu)=\int U^{\prime}(\mathbf{z})\frac{d\,\rho_{t}(\mathbf{z}) }{dt}\,d\mathbf{z} \tag{18}\]

The velocity vector satisfies the continuity equation:

\[\frac{d\,\rho_{t}(\mathbf{z})}{dt}=-\nabla\cdot\Big{(}v_{t}(\mathbf{z})\rho_{t}(\mathbf{z} )\Big{)} \tag{19}\]

where \(-\nabla\cdot\Big{(}v_{t}(\mathbf{z})\rho_{t}(\mathbf{z})\Big{)}\) is the tangent vector at point \(\rho_{t}(\mathbf{z})\). Eq. (18) can be simplified to:

\[\begin{split}\frac{d}{dt}\mathcal{F}(\mu)&=\int-U^{ \prime}(\rho_{t}(\mathbf{z}))\nabla\cdot\Big{(}v_{t}(\mathbf{z})\rho_{t}(\mathbf{z})\Big{)} \,d\mathbf{z}\\ &=\int\nabla\Big{(}U^{\prime}(\rho_{t}(\mathbf{z}))\Big{)}v_{t}(\mathbf{z} )\rho_{t}(\mathbf{z})\,d\mathbf{z}\end{split} \tag{20}\]

On the other hand, the calculus of differential geometry gives

\[\frac{d}{dt}\mathcal{F}(\mu)=\mathrm{Diff}\mathcal{F}|_{\rho_{t}}(-\nabla\cdot \Big{(}v_{t}(\mathbf{z})\rho_{t}(\mathbf{z})\Big{)})=\langle\nabla\mathcal{F},-\nabla \cdot\Big{(}v_{t}(\mathbf{z})\rho_{t}(\mathbf{z})\Big{)}\rangle_{f} \tag{21}\]

where \(\langle,\rangle_{f}\) is a Riemannian distance function which is defined as:

\[\langle-\nabla\cdot\Big{(}w_{1}(\mathbf{z})\rho_{t}(\mathbf{z})\Big{)},-\nabla\cdot \Big{(}w_{2}(\mathbf{z})\rho_{t}(\mathbf{z})\Big{)}\rangle_{f}=\int w_{1}(\mathbf{z})w_{2 }(\mathbf{z})f(\mathbf{z})\,d\mathbf{z} \tag{22}\]

This scalar product coincides with the \(W_{2}\) distance according to Benamou and Brenier [2]. Then eq. (20) can be similarly re-written as:

\[\frac{d}{dt}\mathcal{F}(\mu)=\langle-\nabla\cdot\Big{(}\nabla U^{\prime}(\rho_ {t}(\mathbf{z}))\rho_{t}(\mathbf{z})\Big{)},-\nabla\cdot\Big{(}v_{t}(\mathbf{z})\rho_{t}( \mathbf{z})\Big{)}\rangle_{f} \tag{23}\]

So the relation arises as:

\[\nabla\mathcal{F}=-\nabla\cdot\Big{(}\nabla U^{\prime}(\rho_{t}(\mathbf{z}))\rho_{ t}(\mathbf{z})\Big{)} \tag{24}\]

Since we have \(\nabla\mathcal{F}=-\frac{d}{dt}\rho_{t}(\mathbf{z})\), the above equation can be re-written as

\[\frac{d}{dt}\rho_{t}(\mathbf{z})=\nabla\cdot\Big{(}\nabla U^{\prime}(\rho_{t}(\bm {z}))\rho_{t}(\mathbf{z})\Big{)} \tag{25}\]

The above derivations can be alternatively made by the pioneering JKO scheme [34]. This explicitly defines the relation between evolution PDEs of \(\rho_{t}(\mathbf{z})\) and the internal energy \(U\). For our method, we use the gradient of our scalar energy field \(\nabla u(\mathbf{z},t)\) to learn the velocity field which is given by \(U^{\prime}(\rho_{t}(\mathbf{z}))\). Interestingly, driven by certain specific velocity fields \(\nabla u(\mathbf{z},t)\), the evolution of \(\rho(\mathbf{z},t)\) would become some special PDEs. Here we discuss some possibilities:

**Heat Equations.** If we consider the energy function \(U\) as the weighted entropy:

\[U(\rho_{t}(\mathbf{z}))=\rho_{t}(\mathbf{z})\log(\rho_{t}(\mathbf{z})) \tag{26}\]

We would have exactly the heat equation:

\[\frac{d}{dt}\rho_{t}(\mathbf{z})-\frac{d}{d\mathbf{z}^{2}}\rho_{t}(\mathbf{z})=0 \tag{27}\]

Injecting the above equation back into the continuity equation leads to the velocity field \(v_{t}(\mathbf{z})\) as

\[\begin{split}\frac{d\,\rho_{t}(\mathbf{z})}{dt}&=- \nabla\cdot\left(v_{t}(\mathbf{z})\rho_{t}(\mathbf{z})\right)=\frac{d}{d\mathbf{z}^{2}}\rho _{t}(\mathbf{z})\\ v_{t}(\mathbf{z})&=-\frac{\nabla\rho_{t}(\mathbf{z})}{\rho _{t}(\mathbf{z})}=-\nabla\log(\rho_{t}(\mathbf{z}))\end{split} \tag{28}\]

When our \(\nabla u(\mathbf{z},t)\) learns the velocity field \(-\nabla\log(\rho_{t}(\mathbf{z}))\), the evolution of \(\rho(\mathbf{z},t)\) would become heat equations.

**Fokker Planck Equations.** For the energy function defined as:

\[U(\rho_{t}(\mathbf{z}))=-A\cdot\rho_{t}(\mathbf{z})+\rho_{t}(\mathbf{z})\log(\rho_{t}(\bm {z})) \tag{29}\]

we would have the Fokker-Planck equation as

\[\frac{d}{dt}\rho_{t}(\mathbf{z})+\frac{d}{d\mathbf{z}}[\nabla A\rho_{t}(\mathbf{z})]-\frac {d}{d\mathbf{z}^{2}}[\rho_{t}(\mathbf{z})]=0, \tag{30}\]

The velocity field can be similarly derived as

\[v_{t}(\mathbf{z})=\nabla A-\nabla\log(\rho_{t}(\mathbf{z})) \tag{31}\]

For the velocity field \(\nabla A-\nabla\log(\rho_{t}(\mathbf{z}))\), the movement of \(\rho(\mathbf{z},t)\) is the Fokker Planck equation.

**Porous Medium Equations.** If we define the energy function as

\[U(\rho_{t}(\mathbf{z}))=\frac{1}{m-1}\rho_{t}^{m}(\mathbf{z}) \tag{32}\]

Then we would have the porous medium equation where \(m>1\) and the velocity field:

\[\frac{d}{dt}\rho_{t}(\mathbf{z})-\frac{d}{d\mathbf{z}^{2}}\rho_{t}^{m}(\mathbf{z})=0,\;v_ {t}(\mathbf{z})=-m\rho^{m-2}\nabla\rho \tag{33}\]

When the \(\nabla u(\mathbf{z},t)\) learns the velocity \(-m\rho^{m-2}\nabla\rho\), the trajectory of \(\rho(\mathbf{z},t)\) becomes the porous medium equations.

## Appendix B PDE-regularized Latent Space Learning

Our framework can be extended to incorporate the PDE dynamics as part of the training procedure to encourage a more structured representation. To validate the effectiveness, we incorporate a PDE loss such that we expect any path in the latent space to follow specific dynamics (wave equation in our experiment). In addition to the initial setup outlined in Appendix D.5, we further fine-tune the VAE model by applying a PDE regularization loss term, defined as

\[\mathcal{L}=\mathcal{L}_{VAE}+\mathcal{L}_{r}+\mathcal{L}_{\phi}\]

that includes the velocity-matching objective \(\mathcal{L}_{r}\) and boundary condition \(\mathcal{L}_{\phi}\). This PDE-regularized latent space learning can also be adapted to other generative models by replacing \(\mathcal{L}_{VAE}\). We further optimize the model and energy network for 10 epochs across the full training dataset using an AdamW optimizer with a 1e-4 learning rate and a cosine learning rate scheduler, without a warm-up period. All other training parameters remain consistent with those initially described in Appendix D.5.

During fine-tuning, the VAE loss drops from 0.2187 to 0.06288. The results of QED optimization surpass those of two other unsupervised methods, indicating potential areas for future research and refinement.

## Appendix C Extended Related Work

### Machine Learning for Molecule Generation

Molecules are highly discrete objects and two branches of methods are thus developed to design or search new molecules [8]. One idea is to leverage the advancement of deep generative models which approximate the data distribution from a provided dataset of molecules and then sample new molecules from the learned density. This idea inspires a line of work developing deep generative models from variational auto-encoders (VAE) [18; 31], generative adversarial networks (GAN) [21; 6], normalizing flows (NF) [43; 67] and more recently diffusion models [24; 58; 33]. However, to respect the combinatorial nature of molecules, another line of work leverage combinatorial optimization to search new molecules including genetic algorithm (GA) [30], Monte Carlo tree search (MCTS) [63], reinforcement learning (RL) [64], but often with sophisticated optimization objectives beyond simple valid molecules.

### Goal-oriented Molecule Generation

In addition to simply generating valid molecules, a more realistic application is to generate molecules with desired properties [8]. For deep generative model-based methods, it is naturally combined with on-the-fly optimization methods such as gradient-based or Bayesian optimization (in low data regime) as it often maps data to a low-dimensional and smooth latent space thus more friendly for these optimization methods [20]. For methods that do not explicitly reduce the dimensionality of data such as diffusion models, Schneuing et al. [49] propose an evolutionary process to iteratively optimize the generated molecules. As it is observed that the learned latent space exhibits explicit structure [18], Du et al. [11] leverage such property to learn a linear classifier to find the latent direction to optimize the property of given molecules. In opposition to deep generative models, combinatorial optimization methods are often inherently associated with optimization, e.g. reward function in RL, selection criteria in GA, etc [14; 41].

### Image Editing in the Latent Space

Beyond molecule generation, there is a vast literature on the study of the latent space of generative models on images for image editing and manipulation [17; 29; 59; 22; 72; 45; 50; 52; 54; 53; 55; 56]. Here we highlight some representative supervised and unsupervised approaches. Supervised methods usually require pixel-wise annotations. InterfaceGAN [51] leverages face image pairs of different attributes to interpret disentangled latent representations of GANs. Jahanian et al. [29] explores linear and non-linear walks in the latent space under the guidance of user-specified transformation. Compared to supervised methods, unsupervised ones mainly focus on discovering meaningful interpretable directions in the latent space through extra regularization. Voynov and Babenko [59] proposes to jointly learn a set of orthogonal directions and a classifier to learn the distinct interpretable directions. SeFa [50] and HouseholderGAN [55] propose to use the eigenvectors of the (orthogonal) projection matrices as interpretable directions to traverse the latent space. More relevantly, Song et al. [54] proposes to use wave-like potential flows to model the spatiotemporal dynamics in the latent spaces of different generative models.

\begin{table}
\begin{tabular}{l|c|c c c|c c c} \hline \multirow{2}{*}{Method} & \multirow{2}{*}{\(K\)} & \multicolumn{3}{c|}{ploop\(\uparrow\)} & \multicolumn{3}{c}{QED \(\uparrow\)} \\  & & 1st & 2nd & 3rd & 1st & 2nd & 3rd \\ \hline Random & N/A & 3.52 & 3.43 & 3.37 & **0.940** & **0.933** & 0.932 \\ Wave (unsup) & 10 & **5.30** & **5.22** & **5.14** & 0.905 & 0.902 & 0.978 \\ HJ (unsup) & 10 & 4.26 & 4.10 & 4.07 & 0.930 & 0.928Experiments Details

### Baselines

We compare with the following baselines:

* **Random**: we take a linear direction that is sampled from Multi-variant Gaussian distribution in the high dimensional latent space and normalized to unit length for all molecules across all time steps.
* **Random 1D**: we take a unit vector where only 1 randomly selected dimension is either 1 or -1 as the linear direction.
* **ChemSpace**[11]: a separation boundary of the training dataset in latent space w.r.t. the desired property is classified by an Support vector machine (SVM). Then we take the normal vector corresponding to the positive separation as the manipulation direction of control.
* **Gradient Flow (LIMO)**[13]: a VAE-based generative model that encodes the input molecules into SELFIES [36] and auto-regressive on the tokenized molecule. LIMO uses Adam optimizer to reverse optimize on the input latent vector \(z\) whereas Gradient Flow is equivalent to using an SGD optimizer for the same purpose.
* **Evolutionary Algorithm (EA)**: EA is a general framework that leverages random mutation and crossover to select better samples iteratively to search good solutions. In our scenario, we realize an EA-based baseline by perturbing the vector by the directions given by random, ChemSpace and gradient. The pseudocode can be found in Algorithm 3. The results are presented at Appendix D.7.

### Training Dataset

* **ChEMBL**[68] is a database of 2.4M bioactive molecules with drug-like properties, including features like a Natural Product likeness score and annotations for chemical probes and bioactivity measurements.
* **MOSES**[46] is a benchmarking dataset derived from the ZINC [27] Clean Leads collection, containing 2M filtered molecules with specific physicochemical properties, organized into training, test, and unique scaffold test sets to facilitate the evaluation of model performance on novel molecular scaffolds.
* **ZINC250k** is a subset of the ZINC database containing \(\sim\)250,000 commercially available compounds for virtual screening.

### Molecule Properties

We report the following metrics for our experiments:

* **Penalized logP/plogP**: Estimated octanol-water partition coefficient penalized by synthetic accessibility (SA) score and the number of atoms in the longest ring.
* **QED**: Quantitative Estimate of Drug-likeness, a metric that evaluates the likelihood of a molecule being a successful drug based on its pharmacophores and physicochemical properties.
* **SA**: Synthetic Accessibility, a score that predicts the ease of synthesis of a molecule, with lower values indicating easier synthesis.
* **DRD2 activity**: Predicted activity against the D2 dopamine receptor, using machine learning models trained on known bioactivity data.
* **JNK3 activity**: Predicted activity against the c-Jun N-terminal kinase 3, important for developing treatments for neurodegenerative diseases.
* **GSK3B activity**: Predicted activity against Glycogen Synthase Kinase 3 beta, which plays a crucial role in various cellular processes including metabolism and neuronal cell development.
* **ESR1 docking score**: Simulation-based score representing the binding affinity of a molecule to Estrogen Receptor 1, relevant in the context of breast cancer therapies.

* **ACAA1 docking score**: Simulation-based score representing the binding affinity of a molecule to Acetyl-CoA Acyltransferase 1, important for metabolic processes in cells.

#### d.3.1 Misalignment of normalization schemes for penalized logP

We notice that plogP is a commonly reported metric in recent molecule discovery literature but does not share the same normalization scheme. Following Gomez-Bombarelli et al. [18, Eq. 1], the SA scores and a ring penalty term were introduced into the calculation of penalized logP as the following

\[J^{\log\mathrm{P}}(m)=\log\mathrm{P}(m)-\mathrm{SA}(m)-\text{ring-penalty}(m)\]

Each term of \(\log\mathrm{P}(m)\), \(\mathrm{SA}(m)\), and \(\text{ring-penalty}(m)\) are normalized to have zero mean and unit standard derivation across the training data. However, no sufficient details were included in their paper or their released source code on how the \(\text{ring-penalty}(m)\) is computed. Specifically, 3 implementations are widely used in various works.

Penalized by the length of the maximum cycle without normalization ring-penalty\((m)\) is computed as the number of atoms on the longest ring minus 6 in their implementation. Neither \(\log\mathrm{P}(m)\), \(\mathrm{SA}(m)\), or \(\text{ring-penalty}(m)\) is normalized. MolDQN [71] reported their results in this scheme.

Penalized by the length of the maximum cycle with normalization ring-penalty\((m)\) is computed same as without normalization. MARS [62], HierVAE [32], GCPN [65], and ours report plogP using this scheme.

Penalized by number of cyclesAs described by Jin et al. [31, page 7 footnote 3], \(\text{ring-penalty}(m)\) is computed as the number of rings in the molecule that has more than 6 atoms. LIMO reports plogP using this metric.

### Training and inference

We detail our training and inference workflows in Alg. 1 and Alg. D.4, respectively.

```
1:Pre-trained encoder \(f_{\theta}\), decoder \(g_{\psi}\), (optional) classifier \(l_{\gamma}\), timestamps \(T\), # of potential functions \(K\)
2:Initialize \(\phi^{j}(\cdot)\leftarrow\) MLP for \(j=1,\ldots,K\)
3:repeat
4: Sampling: \(\mathbf{z}_{0}=f_{\theta}(x_{0})\), \(t\sim\texttt{Categorical}(T)\), \(k\sim\texttt{Categorical}(K)\)
5:for\(i=1,\ldots,t\)do
6:\(\mathbf{z}_{i+1}=\mathbf{z}_{i}+\nabla_{\mathbf{z}}\phi^{k}(i,\mathbf{z}_{i})\)
7:endfor
8: Decode: \(\mathbf{x}_{t}=g_{\psi}(\mathbf{z}_{t})\), \(\mathbf{x}_{t+1}=g_{\psi}(\mathbf{z}_{t+1})\)
9:if unsupervised then
10: Classification: \(\hat{k}=l_{\gamma}(\mathbf{x}_{t};\mathbf{x}_{t+1})\)
11: Loss: \(\mathcal{L}=\mathcal{L}_{r}+\mathcal{L}_{\phi}+\mathcal{L}_{\mathcal{J}}+ \mathcal{L}_{k}\)
12:else
13: Loss: \(\mathcal{L}=\mathcal{L}_{r}+\mathcal{L}_{\phi}+\mathcal{L}_{\mathcal{P}}\)
14:endif
15: Back-propagation through the Loss \(\mathcal{L}\)
16:until Convergence
```

**Algorithm 1** ChemFlow Training

### Experiments Setup

Pre-trained VAEWe follow the VAE architecture from LIMO consisting of a 128 dimension embedding layer, 1024 latent space size, 3-hidden-layer encoder, and 3-hidden-layer decoder both with 1D batch normalization and non-linear activation functions. The hidden layer sizes are \(\{4096,2048,1024\}\) for the encoder and reversely for the decoder. We empirically find that replacing the ReLU activation function with its newer variant Mish activation function [44] resultsin faster convergence and better validation loss. All the experiments reported in this paper use this Mish-activated variant of VAE.

The VAE is trained using an AdamW [42] optimizer, 0.001 initial learning rate, and 1,024 training batch size. To better prevent the model from being stacked at a sub-optimal local minimum, a cosine learning rate scheduler with a 1e-6 minimum learning rate with periodic restart is applied. The VAE is trained for 150 epochs with 4 restarts on 90% of the training data and validated with the rest 10% data. The checkpoint corresponding to the epoch with the lowest validation loss is selected. Training 150 epochs takes \(\sim\)8 hours on a single RTX 3090 desktop.

Surrogate PredictorThe performance of the surrogate predictor is crucial to the proposed latent traversal framework. To handle chemical properties of different magnitudes, we normalize all chemical properties in the training data to have zero mean and unit variance. Then we use a pre-activation-norm MLP with residual connections as the surrogate predictor. The predictor contains 3 residual blocks of size 1024 and the output dimension is 1. Similar to the LIMO setups, we find that the choice of optimizer and training hyperparameters like learning rate or learning rate scheduler is crucial for successful training. The predictor is trained for 20 epochs on 100,000 randomly generated samples and validated with 10,000 unseen data with SGD optimizer, 0.001 learning rate, and batch size 1000. The epoch with the best validation loss is selected. Training each predictor takes less than a minute.

Energy NetworkWe use an MLP structure to parameterize the energy function (the spatial derivative gives the velocity). The time input \(t\) is embedded with a sinusoidal positional embedding followed by a linear layer. The special input \(x\) is encoded with a linear layer and ReLU activation function. The training of the network uses 9,000 random data and 1,000 unseen data for validation. For unsupervised settings, 10 disentangled potential energy functions are trained for 310 epochs with a batch size of 100. The epoch with the best validation loss is selected. Training an energy network with 10 disentangled potential energy functions takes \(\sim\)40 minutes.

ReproducibilityAll the experiments including baselines are conducted on one RTX 3090 GPU and one Nvidia A100 GPU. All docking scores are computed on one RTX 2080Ti GPU. The code implementation is available at [https://github.com/garywei944/ChemFlow](https://github.com/garywei944/ChemFlow).

### Evaluation Metrics

Success RateThe success rate is used as the evaluation metric for the molecule manipulation task. It first randomly generates \(n\) molecules and traverses each of them in the latent space for \(k\) steps. The success rate is calculated as the percentage of \(k\)-step trajectories that are successful. In our case, we generate 1000 molecules and traverse for 10 steps. The manipulation is successful if the local change in molecular structure is smooth and molecular property is increased. Specifically, we showed two success rates: the strict success rate and the relaxed success rate.

For the strict success rate, manipulation is a success if the molecular property is monotonically increasing, molecular similarity with respect to the previous step is monotonically decreasing, and molecules are diversity on the manipulation trajectory. These constraints are formulated as follows:

\[C_{SP}(x,k,P) =\mathbf{1}[\forall i\in[k],s.t.P(x_{i})-P(x_{i+1})\leq 0],\] \[C_{SS}(x,k,S) =\mathbf{1}[\forall i\in[k],s.t.S(x_{i+1},x_{1})-S(x_{i},x_{1}) \leq 0],\] \[C_{SD}(x,k) =\mathbf{1}[|x_{t}:\forall i\in[k]|>2], \tag{34}\] \[SSR =\frac{1}{|X|}\sum_{x\in X}\mathbf{1}[C_{SP}(x,k,P)\wedge C_{SS}( x,k,S)\wedge C_{SD}(x,k)]\]

where \(\{x_{i}\}_{i=1}^{k}\) is one \(k\)-step manipulation trajectory, \(X\) contains \(n\) manipulation trajectories, \(C\) is the constraint, \(P\) is the property evaluation function, \(S\) is the structure similarity function (Taminoto similarity over Morgan fingerprints). The \(C_{SP}\) constraints that the property of molecules must monotonically increase. The \(C_{SS}\) constraints that the structure is similar in regard to the starting molecule must monotonically decrease. The \(C_{SD}\) constraint that the molecules must at least change twice during the manipulation. SSR calculates the percentage of trajectories that satisfy all success constraints.

The relaxed success rate relaxes some constraints by adding a tolerance interval. It is formulated as follows:

\[C_{SP}(x,k,P) =\mathbf{1}[\forall i\in[k],s.t.P(x_{t})-P(x_{t+1})\leq\epsilon],\] \[C_{SS}(x,k,S) =\mathbf{1}[\forall i\in[k],s.t.S(x_{t+1},x_{1})-S(x_{t},x_{1}) \leq\gamma],\] \[C_{SD}(x,k) =\mathbf{1}[|x_{t}:\forall i\in[k]|>2], \tag{35}\] \[SSR =\frac{1}{|X|}\sum_{x\in X}\mathbf{1}[C_{SP}(x,k,P)\wedge C_{SS}( x,k,S)\wedge C_{SD}(x,k)]\]

The relaxed success rate does not require a monotonic increase of molecular property but sets a tolerance threshold \(\epsilon\). This tolerance threshold \(\epsilon\) is defined as \(5\%\) of the range of property in the training dataset. It also does not require a monotonically decrease of structure similarity with a tolerance threshold \(\gamma\) of 0.1.

### Additional Baselines

Evolutionary Algorithm.We present the Evolutionary Algorithm-based (EA) approach to optimize molecules in the latent space as an additional baseline for the experiment. The pseudocode is provided as Algorithm 3. Table 6 shows the performance of the evolutionary algorithm-based approach on unconstrained optimization. For a fair comparison, all methods in Table 6 have the same number of Oracle calls. The result shows that our methods outperform all EA approaches.

```
1:Input:\(n\) samples select \(k\) per iteration, pre-trained ChemSpace/Gradient direction \(l\), step size \(\alpha\), pre-trained surrogate model \(h\), decoder \(g_{\psi}\)
2:Randomly sample \(n\) latent vectors \(\{\mathbf{z}_{i}^{0}\}_{i=1}^{n}\) in latent space
3:for each iteration \(t=0\) to \(T\)do
4: Evaluate sampled latent vector scores: \(s_{i}^{t}=h(\mathbf{z}_{i}^{t})\) for \(i=1,\ldots,n\)
5: Select top-\(k\) scored latent vectors: \(\{\mathbf{z}_{\text{top}}^{t}\}_{j=1}^{k}\)
6:for each selected vector \(j=0\) to \(k\)do
7: Update \(\mathbf{z}_{\text{top},j}^{t+1}=\mathbf{z}_{\text{top},j}^{t}+\alpha\cdot l+\mathbf{\epsilon}\), where \(\mathbf{\epsilon}\sim\mathcal{N}(0,\mathbf{I})\), \(l\) the evolution direction guided by Random/ChemSpace/Gradient.
8:endfor
9: Randomly sample \(\frac{n}{k}\) samples around each \(\mathbf{z}_{\text{top},j}^{t}\) to generate \(n\) new latent vectors \(\{\mathbf{z}_{i}^{t+1}\}_{i=1}^{n}\)
10:endfor
11: Decode latent vectors: \(x_{i}^{T}=g_{\psi}(\mathbf{z}_{i}^{T})\) for \(i=1,\ldots,n\)
```

**Algorithm 3** Evolutionary Algorithm Augmented OptimizationUnconstrained Molecular Optimization Additional ResultsIn addition to reporting the top 3 scores as presented in Table 1, we computed the mean and standard deviation for the top 100 molecules after unconstrained optimization in Table 7. The table shows that our methods have overall the best optimization performance. In addition, HJ exhibits better performance on mean and standard deviation than on top 3, showing that minimizing the kinetic energy is efficient in pushing the distribution to desired properties.

### More Experiment Results

We conduct more experiments to analyze the performance of the proposed methods systematically. They are referred to and discussed in the main paper.

Pearson correlation score for unsupervised diversity guidance with disentanglement regularization.Tables 8 and 9 present the Pearson correlation scores for the trained energy networks of wave equations and Hamilton-Jacobi equations with disentanglement regularization, respectively. For properties other than synthetic accessibility (SA), we select the network with the highest correlation score to maximize these properties. Conversely, for SA, the network with the lowest correlation score (most negative score) is chosen.

Distribution shift and convergence for plogP optimization.Figure 4 illustrates the distribution shift in plogP optimization, complementing the analysis in Figure 3. Similar to the findings discussed in Section 4.2, both unsupervised methods encounter out-of-distribution (OOD) issues after 400 steps, consistent with those observed with ChemSpace. The Random 1D method does not achieve the expected distribution shift, as it manipulates only one dimension of the latent vector. Figure 5 depicts the convergence trends of each method's improvements. Consistent with the predictions in Proposition 3.1, Langevin Dynamics demonstrates the fastest and most effective convergence among all methods.

\begin{table}
\begin{tabular}{l|c c c|c c c} \hline \hline Method & \multicolumn{3}{c}{plogP \(\uparrow\)} & \multicolumn{3}{c}{QED \(\uparrow\)} \\  & 1st & 2nd & 3rd & 1st & 2nd & 3rd \\ \hline EA (Random) & 2.29 & 1.64 & 1.52 & 0.836 & 0.801 & 0.794 \\ EA (ChemSpace) & 3.79 & 3.79 & 3.79 & 0.933 & 0.931 & 0.931 \\ EA (Gradient Flow) & 3.53 & / & / & 0.930 & 0.929 & 0.929 \\ \hline Wave (spv) & 4.76 & 3.78 & 3.71 & **0.947** & 0.934 & 0.932 \\ Wave (unsup) & **5.30** & **5.22** & **5.14** & 0.905 & 0.902 & 0.978 \\ HJ (spv) & 4.39 & 3.70 & 3.48 & 0.946 & 0.941 & 0.940 \\ HJ (unsup) & 4.26 & 4.10 & 4.07 & 0.930 & 0.928 & 0.927 \\ LD & 4.74 & 3.61 & 3.55 & **0.947** & **0.947** & **0.942** \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Unconstrained plogP, QED maximization of Evolutionary Algorithm.** (SPV denotes supervised scenarios, UNSUP denotes unsupervised scenarios). Random/ChemSpace/Gradient are the evolution directions of EA. Since EA (Gradient Flow) has converged to a single molecule when optimizing plogP, only one value is reported.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Method & plogP \(\uparrow\) & QED \(\uparrow\) & ESR1 Docking \(\downarrow\) & ACAA1 Docking \(\downarrow\) \\ \hline Random & 2.345 ± 0.386 (2.259) & 0.903 ± 0.014 (0.902) & -9.127 ± 0.360 (-9.015) & -8.454 ± 0.316 (-8.390) \\ Gradient Flow & 2.664 ± 0.382 (2.537) & 0.910 ± 0.012 (0.908) & -9.452 ± 0.338 (-9.365) & -8.735 ± 0.337 (-8.650) \\ ChemSpace & 2.580 ± 0.406 (2.446) & 0.907 ± 0.014 (0.906) & -9.523 ± 0.409 (-9.395) & -8.749 ± 0.356 (-8.640) \\ \hline Wave (spv) & 2.536 ± 0.439 (2.388) & 0.903 ± 0.015 (0.898) & **-9.630 ± 0.399 (-9.525)** & -8.764 ± 0.344 (-8.650) \\ Wave (unsup) & 1.736 ± 0.401 (1.610) & 0.845 ± 0.014 (0.840) & -9.074 ± 0.329 (-9.000) & **-8.813 ± 0.265 (-8.745)** \\ HJ (spv) & 2.482 ± 0.397 (2.382) & 0.899 ± 0.017 (0.894) & -9.544 ± 0.322 (-9.460) & -8.792 ± 0.332 (-8.675) \\ HJ (unsup) & **3.405 ± 0.254 (3.377)** & **0.911 ± 0.009 (0.909)** & -9.132 ± 0.321 (-9.090) & -8.668 ± 0.243 (-8.630) \\ LD & 2.463 ± 0.388 (2.399) & 0.905 ± 0.014 (0.903) & -9.400 ± 0.360 (-9.300) & -8.709 ± 0.372 (-8.585) \\ \hline \hline \end{tabular}
\end{table}
Table 7: **MSE of Unconstrained plogP, QED maximization, and docking score minimization.** (SPV denotes supervised scenarios, UNSUP denotes unsupervised scenarios). Each entry in the table follows the format mean ± std (median). Boldface highlights the highest-performing generation for each property within each rank.

[MISSING_PAGE_FAIL:23]

## Appendix E Latent Space Visualization and Analysis

As observed in experiments such that random directions perform surprisingly well on molecule manipulation and optimization tasks, we look into the learned latent space to understand its structure. As the prior of a VAE is an isotropic Gaussian distribution, we first verify if the learned variational poster also follows a Gaussian distribution and we find that it does learn so from the evidence shown in Figure 7, where the norm of the molecule projected to the latent space concentrate around 32 which is around \(\sqrt{d}\) such that the latent dimension \(d\) is 1024. We also visualize in Figure 8 how the properties of the molecules in the training dataset are related to their latent vector norms. Surprisingly, we find a strong correlation between almost all molecular properties and their latent norms. Combining these two evidences, it is not surprising that a random latent vector taking a random direction will change the molecular property smoothly and monotonically. In addition, we further plot when we traverse along a random direction in the latent space, how the change of the norm may correspond to the change of a certain property. Among them, we find that SA is particularly in strong positive correlation with the traversal in Figure 9. Though the emergence of the structure in the latent space is interesting and suggests that better algorithms can be developed to exploit the structure, we leave this to future work.

Additionally, we visualize the traversal trajectory for each different property using both supervised and unsupervised wave flow in Figure 10. The plot shows that almost all trajectories grow towards a unique direction in the t-SNE plot, which implies the disentanglement of learned directions and, thus, molecular properties. In addition, the figures display sinusoidal wave-shape trajectories, indicating the

Figure 4: **Distribution shift for plogP optimization**

[MISSING_PAGE_FAIL:25]

Figure 6: **Distribution shift for QED optimization**

Figure 7: **Latent Vector Norm.** Distribution of the norm of the latent vectors projected from the training dataset onto the learned latent space.

Figure 11: **Molecule Manipulation Trajectory Molecule manipulation by chemspace on plogP.**

Figure 12: **Molecule Manipulation Trajectory Molecule manipulation by gradient flow on plogP.**

Figure 8: **Embedding Norm against Property Value of each path. Norm and property value of molecules along the direction of latent traversal with a random direction. The middle curve shows the mean property value and latent embedding norm for all paths. The shaded area is the standard deviation of property value.**

Figure 10: **t-SNE Visualization of Optimization Trajectories. Optimization Trajectories following supervised and unsupervised wave flow visualized using t-SNE.**

Figure 9: **Embedding Norm against Property Value of each Molecule. Scatter plot of norm and property value of individual molecules in the training set encoded in the latent space.**

Figure 16: **Molecule Optimization Trajectory Molecule optimization by supervised Hamilton-Jacobi flow on QED.**

Figure 14: **Molecule Optimization Trajectory Molecule optimization by random direction on plogP.**

Figure 15: **Molecule Optimization Trajectory Molecule optimization by supervised wave flow on QED.**

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes]" is generally preferable to "[No]", it is perfectly acceptable to answer "[No]" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No]" or "[NA]" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims in the abstract and introduction reflect the scope and contributions of the paper, including the development of the ChemFlow framework for navigating chemical latent spaces and its validation across multiple molecule manipulation and optimization tasks. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes] Justification: The paper discusses limitations such as scaling with billions of available molecules in large databases and generalizing beyond small molecules. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The paper does not invent any new theory but replies on existing theoretical results. The details including assumptions and derivations (referenced in Section 3 and Appendix A), are discussed. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]Justification: All experimental setups are clearly described, including data sources, model parameters, and evaluation criteria. The source code is released at [https://github.com/garywei944/ChemFlow](https://github.com/garywei944/ChemFlow). This should allow for the reproducibility of the results presented. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All the data used in our experiments are open-sourced and we include our codes and instructions to run with README.md in [https://github.com/garywei944/ChemFlow](https://github.com/garywei944/ChemFlow). Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should also include the code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The user should include the code and data submission guidelines (https://nips.

* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper specifies all necessary experimental details, such as data splits, hyperparameters, and types of optimizers, which are essential for understanding and reproducing experimental results, referred in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Statistical significance is discussed with appropriate measures (std) and success rate provided for key results, ensuring the reliability of the findings. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper details the computational resources used, including the type of GPUs and the estimated compute time for experiments in Appendix D.5, which aids in the replication and understanding of the resource requirements. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The research adheres to the NeurIPS Code of Ethics, and ethical considerations, especially concerning dual-use risks and responsible AI practices, are discussed in Section 5. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Both positive and negative societal impacts are considered, with discussions on how the ChemFlow framework could impact drug discovery and potential misuse scenarios. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not introduce new models or datasets with high risks of misuse at this moment; thus, specific safeguards are not necessary. However, general practices for responsible AI are discussed. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper correctly credits all used assets, including datasets and codes, with appropriate references and discusses the terms of use and licenses where applicable. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not introduce new datasets or tools that require additional documentation or licensing information.

Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The research does not involve human subjects or crowdsourcing, making this question not applicable. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No human subject research is conducted; hence IRB approval is not required. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.