# Mixture Weight Estimation and Model Prediction in Multi-source Multi-target Domain Adaptation

 Yuyang Deng

Pennsylvania State University

yzd82@psu.edu

&Ilja Kuzborskij

Google DeepMind

iljak@google.com

&Mehrdad Mahdavi

Pennsylvania State University

mzm616@psu.edu

###### Abstract

We consider the problem of learning a model from multiple heterogeneous sources with the goal of performing well on a new target distribution. The goal of learner is to mix these data sources in a target-distribution aware way and simultaneously minimize the empirical risk on the mixed source. The literature has made some tangible advancements in establishing theory of learning on mixture domain. However, there are still two unsolved problems. Firstly, how to estimate the optimal mixture of sources, given a target domain; Secondly, when there are numerous target domains, how to solve empirical risk minimization (ERM) for each target using possibly unique mixture of data sources in a computationally efficient manner. In this paper we address both problems efficiently and with guarantees. We cast the first problem, mixture weight estimation, as a convex-nonconcave compositional minimax problem, and propose an efficient stochastic algorithm with provable stationarity guarantees. Next, for the second problem, we identify that for certain regimes, solving ERM for each target domain individually can be avoided, and instead parameters for a target optimal model can be viewed as a non-linear function on a space of the mixture coefficients. Building upon this, we show that in the offline setting, a GD-trained overparameterized neural network can provably learn such function to _predict_ the model of target domain instead of solving a designated ERM problem. Finally, we also consider an online setting and propose a label efficient online algorithm, which predicts parameters for new targets given an arbitrary sequence of mixing coefficients, while enjoying regret guarantees.

## 1 Introduction

With a rapidly increasing amount of decentralized data, multiple source domain adaptation has been an important learning scheme in modern machine learning, e.g., in learning with data collected from multiple sources (e.g. crowdsourcing) or learning in distributed systems where the data can be highly heterogeneous such as federated learning. In this learning scenario, given an input space \(\mathcal{X}\) and output space \(\mathcal{Y}\), we assume access to \(N\) sources of data, each with its own underlying distributions \(\mathcal{D}_{j},j\in[N]\) over \(\mathcal{X}\times\mathcal{Y}\). Then, given i.i.d. training samples \(\widehat{\mathcal{D}}_{1},\ldots,\widehat{\mathcal{D}}_{N}\), and a hypothesis space \(\mathcal{H}\), our goal is to learn a model on the combination of these sources, for instance through the Empirical Risk Minimization (ERM) procedure \(\widehat{h}_{\boldsymbol{\alpha}}=\arg\min_{h\in\mathcal{H}}\sum_{j=1}^{N} \alpha(j)\mathcal{L}_{\widehat{\mathcal{D}}_{j}}(h)\), where \(\mathcal{L}_{\widehat{\mathcal{D}}_{j}}(h)\) is the empirical loss of a model \(h\in\mathcal{H}\) over data samples in \(\widehat{\mathcal{D}}_{j}\), and \(\boldsymbol{\alpha}\in\Delta^{N}\) is some mixing parameter, such that predictor \(\widehat{h}_{\boldsymbol{\alpha}}\) entails a good generalization performance on a target domain characterized by a distribution \(\mathcal{T}\), i.e., yielding a small true risk \(\mathcal{L}_{\mathcal{T}}(\widehat{h}_{\boldsymbol{\alpha}})=\int\ell( \widehat{h}_{\boldsymbol{\alpha}}(\mathbf{x}),y)\,\mathrm{d}\mathcal{T}( \mathbf{x},y)\). It is natural to measure the quality of \(\widehat{h}_{\boldsymbol{\alpha}}\) in terms of the excess risk -- namely, the difference between the risk of optimal model for target domain \(h^{*}_{\mathcal{T}}=\arg\min_{h\in\mathcal{H}}\mathcal{L}_{\mathcal{T}}(h)\), and that achieved by \(\widehat{h}_{\boldsymbol{\alpha}}\). Clearly, the performance of \(\widehat{h}_{\boldsymbol{\alpha}}\) will be influenced by several factors, such as the choice of mixing coefficients\(\bm{\alpha}\) to aggregate the empirical losses, capacity of \(\mathcal{H}\), and discrepancy between target and source data distributions. So, in order to design a good procedure for learning \(\widehat{h}_{\bm{\alpha}}\) we need to understand aforementioned trade-offs. Over the years the literature on the multiple source learning has dedicated a considerable attention to this problem [25, 5, 35, 17, 12]. To this end, we consider the following bound on the excess risk of \(\widehat{h}_{\bm{\alpha}}\):

**Theorem 1** (Multi-source learning bound [17]).: _Given \(N\) source data distributions \(\mathcal{D}_{1},\ldots,\mathcal{D}_{N}\) and a target data distribution \(\mathcal{T}\), let \(\widehat{h}_{\bm{\alpha}}=\arg\min_{h\in\mathcal{H}}\sum_{j=1}^{N}\alpha(j) \mathcal{L}_{\widehat{\mathcal{D}}_{j}}(h)\) be the ERM solution with fixed mixture weights \(\bm{\alpha}\in\Delta^{N}\). Then for any \(\nu\geq 0\), with probability at least \(1-4e^{-\nu}\) it holds that_

\[\mathcal{L}_{\mathcal{T}}(\widehat{h}_{\bm{\alpha}})\leq\mathcal{L}_{ \mathcal{T}}(h_{\mathcal{T}}^{*})+\mathcal{C}(\mathcal{H},\bm{\alpha})+\sup_{ h\in\mathcal{H}}\sum\nolimits_{j=1}^{N}\alpha(j)|\mathcal{L}_{\widehat{ \mathcal{D}}_{j}}(h)-\mathcal{L}_{\widehat{\mathcal{D}}_{j}}(h)|+C\sqrt{ \frac{\nu}{2}\sum\nolimits_{j=1}^{N}\frac{\alpha^{2}(j)}{m_{j}}}\]

_where \(C\) is some constant, the complexity term is \(\mathcal{C}(\mathcal{H},\bm{\alpha}):=\sum_{j=1}^{N}\alpha(j)\mathfrak{R}_{j} (\mathcal{H})\) with \(\mathfrak{R}_{j}(\mathcal{H})\) being the Rademacher complexity of \(\mathcal{H}\) w.r.t. data source \(j\), and \(m_{j}\) is the number of samples from source \(j\)._

The above bound indicates that, the generalization ability of a model learnt by ERM on an \(\bm{\alpha}\)-combined sources, depends on the \(\bm{\alpha}\)-weighted sum of target-source discrepancies, and the number of samples drawn from each source. To entail a good generalization on target domain, it naturally motivates us to minimize right hand side of the bound over \(\bm{\alpha}\in\Delta^{N}\) to get a _good_ mixture parameter. In this paper we can cast this idea as solving the following minimax optimization problem:

\[\min_{\bm{\alpha}\in\Delta^{N}}\max_{h\in\mathcal{H}}\sum\nolimits_{j=1}^{N} \alpha(j)|\mathcal{L}_{\widehat{\mathcal{D}}_{j}}(h)-\mathcal{L}_{\widehat{ \mathcal{D}}_{j}}(h)|+C\sqrt{\sum\nolimits_{j=1}^{N}\frac{\alpha^{2}(j)}{m_{j }}},\] (1)

where we drop the complexity term as it becomes identical for all sources by fixing the hypothesis space \(\mathcal{H}\) and bounding it with a computable distribution-independent quantity such as VC dimension [31], or it can be controlled by choice of \(\mathcal{H}\) or through data-dependent regularization. [17] gave a simple algorithm to minimize the bound of theorem 1 for binary classifiers and 0-1 loss, however their algorithm does not extend to a more general setting. [24] also looked at minimization of a similar bound with the goal to find weights for an optimal mixture, but they did not give a practical algorithm, nor a provable convergence guarantee. However, none of these works aimed to solve (1) because of its complex structure, and so an efficient algorithm for solving (1) so far has not been proposed. In particular, the first difficulty with (1) is that it is a convex-nonconcave objective, which means all minimax algorithms that require inner concavity [23, 22, 27, 28] or PL-condition [30] will fail to converge to a stationary point. However, recently the literature on optimization of this type of objectives has recently made a tangible progress: The first provable convex-nonconcave algorithm was proposed by [34], where they consider alternating gradient descent ascent algorithm. Their algorithm is deterministic, but in practice, we favor a stochastic gradient method. The second difficulty in solving (1) is its compositional structure, which means that simply replacing gradient with stochastic gradient in [34] will not retain convergence guarantees. To tackle these two difficulties, we propose a _stochastic corrected gradient descent ascent_ algorithm, with provable convergence guarantee for solving (1). Our method can be viewed as a variant of the Stochastic Gradient Descent-Ascent (SGDA) algorithm, and moreover here we give a positive answer to the question posed by [34], on _whether an algorithm performing simultaneous updates can optimize convex-nonconcave problem?_, which could be interesting by its own right.

The discussion above concerns learning with one target domain, but in practice, a more common scenario is that we have multiple target domains to adapt to. For example, in federated learning [26], millions of users might wish to learn a good model from multiple sources, which can have good performance on their own data distribution. Hence, we propose to study _Multi-source Multi-target Domain Adaptation_ scenario (M\({}^{2}\)DA). Here we assume that we have \(M\) target domains, each of them characterized by its own distribution \(\mathcal{T}_{i},i\in[M]\) over \(\mathcal{X}\times\mathcal{Y}\). Adapting to \(M\) different target domains requires different mixture weights \(\bm{\alpha}_{1},\ldots,\bm{\alpha}_{M}\), either obtained by solving (1), or supplied by the user. Equipped with mixing parameters, next we have to solve \(M\) weighted ERM problems to tailor solutions for each target domain \(\mathcal{T}_{i}\), that is

\[\widehat{h}_{i}=\arg\min_{h\in\mathcal{H}}\sum\nolimits_{j=1}^{N}\alpha_{i}(j) \mathcal{L}_{\widehat{\mathcal{D}}_{j}}(h)\;,\qquad i\in[M].\]Notice that these \(M\) ERM objectives share the same component functions, so we name this problem as a _co-component empirical risk minimization_. A straightforward and naive approach is to solve all \(M\) weighted ERMs individually which becomes computationally inefficient when dealing with a large number of data sources. Nevertheless, given the benign structure of these \(M\) ERM problems, we may inquire whether there is a computationally efficient method for discovering all solutions without the necessity of solving each one individually.

We give an affirmative answer to this question by replacing the _learning_ of the target model by _predicting_ the target model and propose two efficient strategies to learn such predictors (for instance, a neural network). Our algorithm designs are based on the following observation: if we assume that each empirical risk \(\mathcal{L}_{\widehat{\mathcal{D}}_{j}}(h)\) is strongly convex and smooth in parameters of a hypothesis \(h\), then the optimal parameters are given by _a Lipschitz function_ of mixture weights \(\boldsymbol{\alpha}\). More formally, denoting \(\mathbf{w}^{*}(\boldsymbol{\alpha})\) as optimal parameters of a hypothesis \(h\) for \(\boldsymbol{\alpha}\)-weighted ERM, \(\mathbf{w}^{*}(\boldsymbol{\alpha})\) is Lipschitz in \(\boldsymbol{\alpha}\). This means that we can learn function \(\mathbf{w}^{*}(\cdot)\) with, say, a neural network and gradient descent, with provably small generalization error. Moreover, analysis of generalization error allows us to understand when such target model prediction is more efficient than direct learning. In particular, we look at such a _phase transition of efficiency_, and conclude that when the number of targets \(M\) is much larger than number of sources \(N\), i.e, \(M\geq\Omega((1/\epsilon)^{N/2})\), learning to predict solution is more efficient than optimizing to solve all \(M\) ERMs. When \(M\) is relatively smaller, optimizing to solve ERMs is more efficient than learning to predict.

Finally, as a second learning scenario we consider an online learning setting, where mixture weights \(\boldsymbol{\alpha}_{1},...,\boldsymbol{\alpha}_{M}\) are arriving sequentially, and may not even originate from the same distribution. We cast this problem as an online non-parametric regression problem with inexact labels and propose an label-efficient online algorithm to predict models.

Our contributionsThe main contributions of this paper is summerized as follows:

* We study the multi-source domain adaptation problem where there are multiple source domains and we wish to learn a new model given the mixture of source domains, that can perform well on a given target domain. We build upon existing learning-theoretic results on multi-source domain adaptation and design a new algorithm for weighing of source domains, that casts this problem as a a convex-nonconcave minimax optimization problem. In section 2 we give the first stochastic optimization algorithm for this problem which provably converges to a stationary point. The proposed algorithm is the first provably convergent algorithm for a stochastic _compositional_ convex-nonconcave minimax problem.
* We further consider the above adaptation problem with multiple target domains, the Multi-source Multi-target Domain Adaptation (M\({}^{2}\)DA). We observe that these multiple adaptation might share a common structure, which allows us to avoid solving adaptation problem for each target domain individually, and instead we can replace it by direct prediction of parameters for a new problem. We consider offline and online settings for prediction of target parameters, and propose computationally efficient algorithms for both. For the offline setting, in section 3.1 we propose to use a two-layer neural network to learn optimal parameters \(\mathbf{w}^{*}(\boldsymbol{\alpha})\) using bilevel gradient descent. We show that our algorithm can achieve \(O(n^{-\frac{2}{2+N}})\) excess risk. We also identify the regime where our learning based approach is more efficient compared to directly solving each target problem individually.
* Finally, in section 3.2 we focus on scenario where target problems arrive sequentially (and could be dependent) and extend our study of direct target parameter prediction to the online setting. We propose a label-efficient algorithm which enjoys \(O(n^{-\frac{1}{1+N}})\) average regret.

NotationWe introduce some basic definitions and notation that will be used throughout the paper. Let \(\mathbb{B}_{q}^{d}(r)\) be the ball in \(q\)-metric centered at the origin and of radius \(r>0\), and let \(\mathbb{S}^{d-1}(r)=\{\mathbf{x}\in\mathbb{R}^{d}\ :\ \|\mathbf{x}\|_{2}=r\} \subset\mathbb{R}^{d}\) be the \(\ell_{2}\)-norm unit sphere centered at the origin, and finally let \(\mathbb{S}^{d-1}=\mathbb{S}^{d-1}(1)\). In addition, the probability simplex is defined as \(\Delta^{N}=\{\boldsymbol{\alpha}\in[0,1]^{N}:\|\boldsymbol{\alpha}\|_{1}=1\}\). Concatenation of vectors is denoted by parentheses, that is \((\mathbf{w}_{1},\ldots,\mathbf{w}_{m})=[\mathbf{w}_{1}^{\top},\ldots,\mathbf{ w}_{m}^{\top}]^{\top}\). A vector norm \(\|\cdot\|\) is understood as Euclidean norm, while \(\|\mathbf{x}\|_{\infty}=\max_{i}|x_{i}|\). For a matrix \(\mathbf{M}\), \(\|\mathbf{M}\|_{\mathrm{op}}\) denotes its spectral norm while \(\|\mathbf{M}\|_{F}\) is its Frobenius norm. For some \(f:\mathbb{S}^{d-1}\rightarrow\mathbb{R}\) the _empirical semi-norm_ is defined as \(\|f\|_{n}^{2}=\frac{1}{n}(f(\mathbf{x}_{1})^{2}+\cdots+f(\mathbf{x}_{n})^{2})\) and is always taken w.r.t. the training sample \(S\). In addition, for \(g:\mathbb{S}^{d-1}\rightarrow\mathbb{R}\), we define an empirical inner product \(\left\langle f,g\right\rangle_{n}=\frac{1}{n}(f(\mathbf{x}_{1})g(\mathbf{x}_{1} )+\cdots+f(\mathbf{x}_{n})g(\mathbf{x}_{n}))\). At the same time, \(\|f\|_{2}=\|f\|_{L^{2}(P_{X})}^{2}\).

## 2 Mixture Weights Estimation via Convex-nonconcave Minimax Optimization

In this section we focus on a single target domain and present an Algorithm 1 designed to solve a minimax problem (1) to estimate the mixture weights. We assume that hypothesis \(h\) is parameterized by a vector space \(\{\mathbf{w}\in\mathcal{W}\subseteq\mathbb{R}^{d}\}\), and use \(f_{j}(\mathbf{w})=\mathcal{L}_{\widehat{\mathcal{D}}_{j}}(h)\) to denote the empirical risk over data source \(j\). Similarly we define \(f_{\widehat{\mathcal{T}}}(\mathbf{w})=\mathcal{L}_{\widehat{\mathcal{T}}}(h)\). We do the following standard relaxations. First, for the sake of simplicity in computation, we relax the square root on the quadratic term w.r.t. \(\boldsymbol{\alpha}\). Second, since the absolute value function is non-smooth, we shall use the smooth approximation function \(g\) to replace it, e.g., \(g(x)=\sqrt{x^{2}+c}\) where \(c\) is some small number (here \(g(\cdot)\) is smooth approximation of \(|\cdot|\)). These relaxations lead to solving the following compositional convex-nonconcave minimax optimization problem:

\[\min_{\alpha\in\Delta^{N}}\max_{\mathbf{w}\in\mathcal{W}}F(\boldsymbol{ \alpha},\mathbf{w}):=\sum\nolimits_{j=1}^{N}\alpha(j)g(f_{\widehat{\mathcal{T }}}(\mathbf{w})-f_{j}(\mathbf{w}))+C\boldsymbol{\alpha}^{\top}\mathbf{M} \boldsymbol{\alpha}\,\] (2)

where \(\mathbf{M}=\mathrm{diag}\{\frac{1}{m_{1}},\ldots,\frac{1}{m_{N}}\}\). We are interested in developing a stochastic optimization algorithm to solve (2). It is a strongly-convex-nonconcave minimax problem, and it is one of most difficult type of minimax problem due to the absence of inner concavity.

``` Input: Target domain \(\mathcal{T}\), Source domains \(\mathcal{D}_{1},...,\mathcal{D}_{N}\), Initialization variable \(\mathbf{w}^{0}=\mathbf{w}^{-1}\), \(z_{1}^{0},...,z_{N}^{0}\), Positive hyper-parameters \((B,\beta,\eta,\gamma)\) (see theorem 2). for\(t=0,...,T-1\)do  Sample a minibatch \(\xi_{\mathcal{T}}^{t}\) of size \(B\) from target domain \(\mathcal{T}\), and \(\xi_{1}^{t},\ldots,\xi_{N}^{t}\) from source domains \(\mathcal{D}_{1},\ldots,\mathcal{D}_{N}\) \(z_{j}^{t+1}=(1-\beta^{t})\left(z_{j}^{t}+f_{\widehat{\mathcal{T}}}(\mathbf{w} ^{t};\xi_{\mathcal{T}}^{t})-f_{j}(\mathbf{w}^{t};\xi_{j}^{t})-(f_{\widehat{ \mathcal{T}}}(\mathbf{w}^{t-1};\xi_{\mathcal{T}}^{t})-f_{j}(\mathbf{w}^{t-1} ;\xi_{j}^{t}))\right)\) \(+\beta^{t}(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t};\xi_{\mathcal{T}}^{t})-f_{ j}(\mathbf{w}^{t};\xi_{j}^{t}))\).  Compute gradient for \(\mathbf{w}^{t}_{\mathbf{w}}=\sum_{j=1}^{N}\alpha_{j}^{t}\nabla g(z_{j}^{t+1})( \nabla f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t};\xi_{\mathcal{T}}^{t})- \nabla f_{j}(\mathbf{w}^{t};\xi_{j}^{t}))\) \(\mathbf{w}^{t+1}=\mathcal{P}_{\mathcal{W}}\left(\mathbf{w}^{t}+\gamma\mathbf{ g}^{t}_{\mathbf{w}}\right)\)  Make vector \(\mathbf{v}\in\mathbb{R}^{N}\) whose \(j\)th coordinate is \(g(z_{j}^{t})\).  Compute gradient for \(\boldsymbol{\alpha}\): \(\mathbf{g}^{t}_{\boldsymbol{\alpha}}=\mathbf{v}+2C\mathbf{M}\boldsymbol{ \alpha}^{t}\). \(\boldsymbol{\alpha}^{t+1}=\mathcal{P}_{\Delta^{N}}\left(\boldsymbol{\alpha}^{t }-\eta\mathbf{g}^{t}_{\boldsymbol{\alpha}}\right)\) ```

**Algorithm 1**Mixture Weight Estimation

To the best of our knowledge, only Xu _et al._[34] proposed a deterministic algorithm to solve it, but it is still unknown whether a stochastic algorithm can solve it with provable guarantee. We give an affirmative answer to this question, by proposing an algorithm built on celebrated stochastic gradient descent ascent [23]. In addition to nonconvexity nature of 2, another difficulty that arises from the compositional structure of objective is that we cannot simply compute stochastic gradients, namely (with \(\mathbb{E}[\cdot]\equiv\mathbb{E}[\cdot\mid\mathrm{data}]\)):

\[\mathbb{E}[g^{\prime}(f_{\widehat{\mathcal{T}}}(\mathbf{w};\xi_{\mathcal{T}})-f _{j}(\mathbf{w};\xi_{j}))(\nabla f_{\widehat{\mathcal{T}}}(\mathbf{w};\xi_{ \mathcal{T}})-\nabla f_{j}(\mathbf{w};\xi_{j}))]\neq g^{\prime}(f_{\widehat{ \mathcal{T}}}(\mathbf{w})-f_{j}(\mathbf{w}))(\nabla f_{\widehat{\mathcal{T}}}( \mathbf{w})-\nabla f_{j}(\mathbf{w})),\]

where \(\xi_{1},\xi_{2},\ldots\) are independent random elements in sample space \(\Xi=\mathcal{X}\times\mathcal{Y}\) that capture stochasticity of the algorithm. To alleviate this issue, we borrow 'the stochastic corrected gradient' idea from [7], and maintain an auxiliary variable by introducing

\[z_{j}^{t+1}=(1-\beta^{t})\left(z_{j}^{t}+f_{\widehat{\mathcal{T}}}( \mathbf{w}^{t};\xi_{\mathcal{T}}^{t})-f_{j}(\mathbf{w}^{t};\xi_{j}^{t})-(f_{ \widehat{\mathcal{T}}}(\mathbf{w}^{t-1};\xi_{\mathcal{T}}^{t})-f_{j}(\mathbf{ w}^{t-1};\xi_{j}^{t}))\right)\] \[+\beta^{t}(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t};\xi_{\mathcal{ T}}^{t})-f_{j}(\mathbf{w}^{t};\xi_{j}^{t}))\Then, denoting the projection operator onto convex set \(\mathcal{C}\) by \(\mathcal{P}_{\mathcal{C}}(\cdot)\), the update rule becomes

\[\mathbf{w}^{t+1}=\mathcal{P}_{\mathcal{W}}\left(\mathbf{w}^{t}+\gamma\mathbf{g} _{\mathbf{w}}^{t}\right),\qquad\boldsymbol{\alpha}^{t+1}=\mathcal{P}_{\Delta^{ N}}\left(\boldsymbol{\alpha}^{t}-\eta\mathbf{g}_{\boldsymbol{\alpha}}^{t}\right)\.\]

### Convergence Analysis

In this section we are going to present the convergence guarantee for Algorithm 1. We make the following standard assumption on objective in (2).

**Assumption 1**.: _We make the following assumptions on \(g\) and \(f\):_

1. \(g(z)\) _is_ \(G_{g}\) _Lipschitz and_ \(L_{g}\) _smooth._ \(f_{j}(\mathbf{w};\xi)\) _is_ \(G_{f}\) _Lipschitz and_ \(L_{f}\) _smooth,_ \(\forall\mathbf{w}\in\mathcal{W},j\in[N],\,\xi\in\Xi\)_._
2. \(\mathbb{E}\left\|\nabla f_{j}(\mathbf{w};\xi)-\nabla f_{j}(\mathbf{w})\right\| ^{2}\leq\sigma^{2},\forall\mathbf{w}\in\mathcal{W}\)_._
3. \(\max_{\boldsymbol{\alpha}\in\Delta^{N},\mathbf{w}\in\mathcal{W}}F(\boldsymbol {\alpha},\mathbf{w})\leq F_{max}\)_,_ \(\max_{\mathbf{w}\in\mathcal{W}}g(f_{\widehat{\mathcal{T}}}(\mathbf{w})-f_{j}( \mathbf{w}))\leq B_{g},\forall j\in[N]\)_._

Points 1 and 2 of Assumption 1 are standard in the literature on compositional optimization [7]. Point 3 guarantees boundedness of objective value, which can be ensured since we are working in the bounded parameter domain. Assumption 1 also implies the following property of \(F\).

**Proposition 1**.: _Under Assumption 1, \(F(\boldsymbol{\alpha},\mathbf{w})\) is \(L:=\max\left\{4G_{f}^{2}L_{g}+2G_{g}L_{f},\frac{2C}{m_{\min}}\right\}\) smooth, and \(\mu=\frac{2C}{m_{\max}}\) strongly convex in \(\boldsymbol{\alpha}\)._

Next, we consider the following convergence measure:

**Definition 1** (Convergence Measure [34]).: _Given two parameters, \(\boldsymbol{\alpha}\) and \(\mathbf{w}\), we define the following quantity as a stationary gap_

\[\nabla G(\boldsymbol{\alpha},\mathbf{w})=\left(\begin{array}{l}\frac{1}{ \eta}\left(\boldsymbol{\alpha}-\mathcal{P}_{\Delta^{N}}\left(\boldsymbol{ \alpha}-\eta\nabla_{\boldsymbol{\alpha}}F(\boldsymbol{\alpha},\mathbf{w}) \right)\right)\\ \frac{1}{\gamma}\left(\mathbf{w}-\mathcal{P}_{\mathcal{W}}\left(\mathbf{w}+ \gamma\nabla_{\mathbf{w}}F(\boldsymbol{\alpha},\mathbf{w})\right)\right) \end{array}\right)\.\]

Given the nonconcave nature of (2), we are only able to show the convergence to a stationary point. Definition 1 measures the stationarity given parameter pair \((\boldsymbol{\alpha},\mathbf{w})\) by examining how much the parameter will change if we run one step projected gradient descent-ascent on them. Alternatively, one could consider the widely employed _primal function_[23] as a convergence measure, \(\left\|\nabla\Phi(\boldsymbol{\alpha})\right\|\) with \(\Phi(\boldsymbol{\alpha})=\max_{\mathbf{w}\in\mathcal{W}}F(\boldsymbol{\alpha },\mathbf{w})\), but it is ill-suited to express stationarity since \(F(\boldsymbol{\alpha},\cdot)\) is non-concave.

One of our main results, proved in appendix A, establishes the convergence rate of Algorithm 1:

**Theorem 2**.: _Consider Assumption 1 and let \(L\) and \(\mu\) be defined in Proposition 1. Then, letting \(B=\Theta\left(\max\left\{\frac{G_{g}^{2}N\sigma^{2}}{\epsilon^{2}},\frac{ \kappa L\sigma^{2}}{\epsilon^{2}}\right\}\right)\), \(\beta=0.1\), \(\eta=\Theta\left(\frac{\mu}{L^{2}}\right)\), \(\gamma=\Theta\left(\frac{\mu^{3}}{NG_{g}^{2}G_{f}^{2}L^{2}}\right)\), the Algorithm 1 guarantees that_

\[\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}\left\|\nabla G(\boldsymbol{\alpha}^{t}, \mathbf{w}^{t})\right\|^{2}\leq\epsilon^{2}\]

_with the gradient complexity bounded by:_

\[O\left(\frac{\kappa LF_{\max}}{\epsilon^{2}}\cdot\max\left\{\frac{\kappa L \sigma^{2}}{\epsilon^{2}},\frac{G_{g}^{2}N\sigma^{2}}{\epsilon^{2}},1\right\} \right).\]

To the best of our knowledge, this is the first convergence proof for stochastic algorithm on solving strongly-convex-nonconcave problem. We achieve \(O(\epsilon^{-4})\) gradient complexity required to reach an \(\epsilon\) stationary point. In contrast to the most relevant result of [34], they show the rate \(O(\epsilon^{-2})\) for a _deterministic_ Alternating Gradient Projection (AGP) in a strongly-convex-nonconcave setting. Note that our result also positively answers the question posed by [34], on whether some algorithm performing simultaneous instead of alternative updates can optimize strongly-convex-nonconcave minimax problem. Finally, compared to \(O(\epsilon^{-4})\) rate of SGDA given a nonconvex-strongly-concave problem [23], we need roughly same stochastic gradient evaluations.

Multiple Target Domains: Learning to Solve Co-component ERM

Up till now, the main focus was on the problem of learning _good_ mixture parameters given a single target domain. Now we turn to a more general setting where we have \(M\) target domains, each associated with a different data distribution which necessitates per target mixture weights \(\bm{\alpha}_{1},\ldots,\bm{\alpha}_{M}\), either obtained by our algorithm, or provided by the user to guarantee good generalization on individual domains. Next, to get personalized models for these \(M\) domains, we have to solve \(M\) different ERM problems based on these mixture weights:

\[\min_{\mathbf{w}\in\mathcal{W}}f_{\bm{\alpha}_{1}}(\mathbf{w}):=\sum\nolimits _{j=1}^{N}\alpha_{1}(j)f_{j}(\mathbf{w}),\quad\cdots,\quad\min_{\mathbf{w}\in \mathcal{W}}f_{\bm{\alpha}_{M}}(\mathbf{w}):=\sum\nolimits_{j=1}^{N}\alpha_{n }(j)f_{j}(\mathbf{w});\]

A naive way is to solve each of them, which will result in a computational complexity of \(M\) multiplied by the cost required to minimize each individual \(f_{\bm{\alpha}_{i}}\) to a desired precision. We note that such a solution does not exploit the benign structure of these ERM problems: they share the same component functions \((f_{j})_{j}\), and the only difference is in the mixture weights. It naturally motivates us to ask, can we propose an efficient algorithm which avoids solving all these \(M\) co-component ERM problems from scratch? Consider the solution of \(\min_{\mathbf{w}\in\mathcal{W}}f_{\alpha}(\mathbf{w}):=\sum_{j=1}^{N}\alpha( j)f_{j}(\mathbf{w})\) as a function of \(\bm{\alpha}\),

\[\mathbf{w}^{*}(\bm{\alpha}):=\arg\min_{\mathbf{w}\in\mathcal{W}}\sum \nolimits_{j=1}^{N}\alpha(j)f_{j}(\mathbf{w}).\] (3)

Fortunately, if we assume that each source empirical risk \(f_{j}\) is strongly convex and \(L_{f}\) smooth in model parameters, we have the following Lipschitz property (shown in appendix B.1):

**Lemma 1**.: _If each \(f_{j}\) is \(\mu_{f}\) strongly convex and \(L_{f}\) smooth, then \(\mathbf{w}^{*}(\cdot)\) is \(\kappa^{*}=\sqrt{N}G_{f}/\mu_{f}\) Lipschitz._

Some basic algebra shows that in the above example \(\mathbf{w}^{*}(\bm{\alpha})\) is indeed Lipschitz in \(\bm{\alpha}\) with respect to \(\ell^{2}\) metric. The Lipschitz property allows us to learn \(\mathbf{w}^{*}\) efficiently. In particular, learning arbitrary Lipschitz (and bounded) vector-valued function \(\mathbf{w}^{*}:\mathbb{R}^{N}\rightarrow\mathbb{R}^{d}\) is an instance of a well-studied _nonparametric regression_ problem [11]. In the following we will consider algorithms for learning \(\mathbf{w}^{*}\) in both offline and online setting and which are provably capable of estimating \(\mathbf{w}^{*}\) at an almost optimal rate. In offline setting, we assume that we have access to a subset of \(M\) mixture weights, say \(\bm{\alpha}_{1},...,\bm{\alpha}_{n}\), and we shall use a two layer neural network \(\mathbf{h}_{\bm{\theta}}(\cdot)\) to learn \(\mathbf{w}^{*}(\cdot)\). Our algorithm is GD based empirical risk minimization with adaptive label refining. In a nutshell, given an \(\bm{\alpha}\), since we do not have access to \(\mathbf{w}^{*}(\bm{\alpha})\), we will use gradient descent to jointly solve weighted ERM with \(\bm{\alpha}\) to get an approximation of \(\mathbf{w}^{*}(\bm{\alpha})\) as well as optimizing neural network parameters. With a mild distributional assumption on \(\bm{\alpha}\), we show that our algorithm guarantees that the two layer network learns \(\mathbf{w}^{*}(\bm{\alpha})\), that is, it achieves a small excess risk \(\mathbb{E}_{\bm{\alpha}}\left\|\mathbf{h}_{\bm{\theta}}(\bm{\alpha})-\mathbf{w }^{*}(\bm{\alpha})\right\|^{2}\).

In online setting, we assume that we observe an arbitrary sequence \(\bm{\alpha}_{1},\bm{\alpha}_{2},...\) on a simplex, and we wish to predict parameters close to \(\mathbf{w}^{*}(\bm{\alpha}_{1}),\mathbf{w}^{*}(\bm{\alpha}_{2}),...\). As baseline algorithm we will consider a well-known online nonparametric regression that greedily covers the simplex with local online learners and which enjoys almost-optimal regret [13]. However, in the considered online protocol, the algorithm will need access to labels, and revealing each label requires to solve (3) to some desired accuracy. Here we explore a possibility that in practice we might be satisfied with \(\epsilon\)-average regret, while saving the labelling cost. To this end we propose a modification of the algorithm that randomly skips some labels, while incurring a slightly larger regret.

### Offline Setting: Learning Lipschitz function with ReLU Neural Network

In this section we consider offline learning of \(\mathbf{w}^{*}\). The Lipschitzness guarantees that the \(\mathbf{w}^{*}(\bm{\alpha})\) function can be efficiently learnt on finite \(\bm{\alpha}\)s, and generalizable to unseen \(\bm{\alpha}\). Hence, we propose to use a vector-valued two layer ReLU neural network \(\mathbf{h}_{\bm{\theta}}\) to learn \(\mathbf{w}^{*}(\bm{\alpha})\).

We consider a two layer vector-valued neural network \(\mathbf{h}_{\bm{\theta}}:\mathbb{R}^{N}\rightarrow\mathbb{R}^{d}\), \(\mathbf{h}_{\bm{\theta}}(\mathbf{x})=[\mathbf{a}_{1}^{\top}(\mathbf{U}^{1} \mathbf{x})_{+},...,\mathbf{a}_{d}^{\top}(\mathbf{U}^{d}\mathbf{x})_{+}]\), where parameters of the _hidden layer_ are matrices \(\mathbf{U}^{i}\in\mathbb{R}^{m\times N}\), collectively captured by the parameter vector \(\bm{\theta}=(\operatorname{vec}(\mathbf{U}^{1}),\ldots,\operatorname{vec}( \mathbf{U}^{d}))\in\mathbb{R}^{dmN}\). Here \(\mathbf{a}_{i}\in\{\pm 1/\sqrt{m}\}^{m}\) are parameters of the _output layer_. In the following the hidden layer is tuned by Algorithm 2, while parameters of the output layer are fixed throughout training. We assume that at initialization, for each \(\mathbf{U}^{i}\), the first half of its rows are drawn i.i.d. from isotropic standard Gaussian and the remaining half is identical to the first half. Similarly, for each \(\mathbf{a}_{i}\), half of the entries are set to \(-1/\sqrt{m}\) and the rest to \(1/\sqrt{m}\) (we assume that \(m\) is even). This initialization ensures that each output coordinate is \(0\) and so the empirical risk is bounded by a constant at initialization.

We assume that we observe mixture weights \(\bm{\alpha}_{1},\ldots,\bm{\alpha}_{n}\in\Delta^{N}\) i.i.d. according to some underlying distribution \(\mathcal{U}\). Such mixture weights can be obtained by Algorithm 1 and their independence means that samples originating from target domains are independent from each other.

We learn the neural network by solving the following **Bi-level** ERM:

\[\min_{\bm{\theta}}\widehat{\mathcal{R}}(\bm{\theta}):=\frac{1}{n}\sum_{i=1}^{n} \left\|\mathbf{h}_{\bm{\theta}}(\bm{\alpha}_{i})-\mathbf{w}^{*}(\bm{\alpha}_{ i})\right\|^{2},\quad\text{s.t.}\quad\mathbf{w}^{*}(\bm{\alpha}_{i})=\arg \min_{\mathbf{w}\in\mathcal{W}}\sum_{j=1}^{N}\alpha_{i}(j)f_{j}(\mathbf{w}).\] (4)

The parameters of a neural network \(\bm{\theta}\) should have the well-controlled excess risk on unseen \(\bm{\alpha}\):

\[\mathcal{R}(\bm{\theta})=\int_{\Delta^{N}}\left\|\mathbf{h}_{\bm{\theta}}( \bm{\alpha})-\mathbf{w}^{*}(\bm{\alpha})\right\|^{2}\mathrm{d}\mathcal{U}( \bm{\alpha})\.\]

To solve (4), we use a nested loop procedure which performs a GD step on a neural network objective \(\widehat{\mathcal{R}}\), while in the inner loop we approximately find 'labels' \(\mathbf{w}^{*}(\bm{\alpha}_{1}),...,\mathbf{w}^{*}(\bm{\alpha}_{n})\) using a \(K\)-step GD. The entire procedure for solving (4) is described in Algorithm 2. Then, the following theorem shows that the two layer neural net optimized by Algorithm 2 learns \(\mathbf{w}^{*}(\bm{\alpha})\):

``` Input: Number of global iteration \(T\), Number of Iteration for inner problem \(R\). for\(t=1,...,T\)do \(\bm{\theta}^{t+1}=\bm{\theta}^{t}-\eta\nabla_{\bm{\theta}^{t}}\mathbf{h}_{ \bm{\theta}^{t}}(\bm{\alpha}_{i})(\mathbf{h}_{\bm{\theta}^{t}}(\bm{\alpha}_{i })-\mathbf{w}_{i}^{t})\) \(\triangleright\) Neural network parameter update for\(i=1,...,n\)do \(\mathbf{w}_{i}^{t+1}=\texttt{GD}(\mathbf{w}_{i}^{t},\bm{\alpha}_{i},K)\)\(\triangleright\) Label refining by \(K\)-step gradient descent ```

**Algorithm 2**Learning \(\mathbf{w}^{*}\) function by a neural network

**Theorem 3**.: _Let \(\lambda_{0}=N\cdot\mathrm{polylog}(N,n)\). Consider a neural network of with \(m\geq\Omega\left(n^{8+\frac{2}{2+N}}\right)\). Then, for the Algorithm 2 with \(\eta\leq\frac{1}{2}\), \(\gamma=\frac{1}{L_{f}}\), \(T\geq\Omega\left(\frac{n}{N\lambda_{0}^{2}n}\log(n)\right)\), and \(K\geq\Omega\left(\kappa\log\left(\frac{nTnD}{\lambda_{0}}\right)\right)\), the following excess risk bound holds with probability at least \(0.99\):_

\[\mathcal{R}(\bm{\theta}_{T+1})\leq O\left((\kappa^{*})^{2}dn^{-\frac{2}{2+N}} \right).\]

The proof given in appendix B.5 is based on a more-or-less standard Neural Tangent Kernel (NTK) approximation argument [15, 4], namely we use the key fact that predictions made by a GD-trained overparameterized neural network are close to those made by a Kernelized Least-Squares (KLS) predictor (given that the width of the network is sufficiently large). Now, such a KLS GD-trained predictor can learn Lipschitz target functions: It is well known that by learning on a sufficiently large Reproducing kernel Hilbert space (RKHS) (with polynomial spectral decay), one can approximate Lipschitz functions well [8, 2]. Here our goal is to approximate a vector-valued function, however, by treating each output independently we follow existing proofs [14, 21] for scalar-valued Lipschitz regression by GD-trained neural networks and arrive at the same excess risk times \(d\).

**Optimality of our rate.** Here we show that a two-layer neural network trained by a bi-level Gradient Descent (GD) can learn a vector-valued function with \(O(dn^{-\frac{2}{2+N}})\) excess risk. If we ignore the dependency on \(d\), our result matches the minimax rate of learning a scalar valued Lipschitz function [11].

**Algorithm 4**: Label efficient nonparametric online regression

**Efficiency of our learning-based approach.** Given \(M\) mixture weights \((\bm{\alpha}_{i})_{i=1}^{M}\), the baseline naive approach is to solve all \(M\) weighted ERM with gradient descent, to accuracy level \(\epsilon\), which requires \(\Theta(M\kappa\log(1/\epsilon))\) time complexity. Using our approach, we first need to learn a neural network with \(\epsilon\) excess risk, and it needs \(n=\Theta\left((\kappa^{*2}d/\epsilon)^{1+N/2}\right)\) samples, which implies that we need to solve this many weighted ERM problems, resulting a complexity of \(\Theta\left((\kappa^{*2}d/\epsilon)^{1+N/2}\cdot\kappa\log(1/\epsilon)\right)\). Once we learn a neural network, we just need to pay for the inference cost to predict \(\mathbf{w}^{*}\) for each \(\bm{\alpha}_{i}\). Putting things together, the total time complexity is \(\Theta\left((\kappa^{*2}d/\epsilon)^{1+N/2}\cdot\kappa\log(1/\epsilon)+M\right)\). We observe the following regimes:

* When \(M\geq\Omega\left(\frac{(\kappa^{*2}d/\epsilon)^{1+N/2}\kappa\log(1/\epsilon)}{ \kappa\log(1/\epsilon)-1}\right)\), learning is more efficient than solving \(M\) ERMs.
* Otherwise, directly solving \(M\) ERMs is more efficient than learning a model predictor.

Intuitively, when the number of target domains is much larger than number of source domains, our learning based approach is strictly more efficient. It is also interesting to note that our learning based approach can avoid computational overhead of \(M\), but suffers exponential cost from the number of sources \(N\). While solving ERMs avoids the price for \(N\), the computational cost increases linearly in terms of \(M\).

### Online Setting: Label Efficient Nonparametric Online Regression

In previous section we discussed nonparametric offline learning of \(\mathbf{w}^{*}\) with distributional assumption on \(\bm{\alpha}_{1},...,\bm{\alpha}_{M}\). In this section we consider the following online learning protocol with oblivious adversary. Given a known and fixed parameter \(p\in[0,1]\), and an unknown sequence \((\bm{\alpha}_{1},\mathbf{w}^{*}(\bm{\alpha}_{1})),(\bm{\alpha}_{2},\mathbf{w} ^{*}(\bm{\alpha}_{2})),\cdots\in\Delta^{N}\times\mathbb{B}_{2}^{d}(D)\) of inputs and labels, at every round \(t=1,2,\ldots\)

1. the environment reveals mixture weights \(\bm{\alpha}_{t}\in\Delta^{N}\);
2. the learner selects a label \(\hat{\mathbf{w}}_{t}\in\mathbb{B}_{2}^{d}(D)\) and incurs loss \(\ell_{t}\big{(}\hat{\mathbf{w}}_{t}\big{)}=\big{\|}\hat{\mathbf{w}}_{t}- \mathbf{w}^{*}(\bm{\alpha}_{t})\big{\|}^{2}\);
3. the learner samples \(Z_{t}\sim\mathrm{Bern}(p)\) and observes \(\mathbb{I}\left\{Z_{t}=1\right\}\mathbf{w}_{t}\), when \(\mathbf{w}_{t}\) is a GD-optimized approximation of \(\mathbf{w}^{*}(\bm{\alpha}_{t})\).

In particular, we introduce Algorithm 4, a modified version of the online nonparametric regression algorithm proposed by [13]. Algorithm 4 iteratively constructs a packing of \(\Delta^{N}\) using \(\ell^{2}\) balls centered on a subset of previously observed inputs. At each step \(t\), the label (parameters) associated with the current input \(\bm{\alpha}_{t}\) is predicted by averaging the labels of past inputs within the ball whose center \(\bm{\alpha}_{s}\) is closest to \(\bm{\alpha}_{t}\) in the \(\ell^{2}\) metric (note that labels are vector-valued). If \(\bm{\alpha}_{t}\) lies outside the nearest ball, a new ball with center \(\bm{\alpha}_{t}\) is created. The radii \(\varepsilon_{t}\) of all balls shrink at a rate \(t^{-1/(1+N)}\)Note that efficient (log in the number of centers) algorithms for approximate nearest-neighbor search are well-known [19], as well as highly-optimized open-source packages are readily available [16].

In contrast to the original algorithm by [13], where all sequentially observed labels are used to generate predictions (update local learners), our algorithm variant uses only a \(p\)-fraction of labels on average. This reduces label complexity at the cost of increased regret. This approach is referred to as _label-efficient prediction_ in online learning [6, Sec. 6.2] and is beneficial when accessing labels is costly. The following theorem, shown in appendix C, establishes the regret bound of Algorithm 4.

**Theorem 4** (Regret bound).: _Let \(f:\Delta^{N}\rightarrow\mathbb{R}^{d}\) be arbitrary \(\kappa^{*}\)-Lipschitz function with respect to \(\|\cdot\|\) metric and let \(C_{N}>0\) be a metric-dependent constant. Then, Algorithm 4 with \(\varepsilon_{t}=t^{-\frac{1}{1+N}}\) satisfies_

\[\mathbb{E}\left[\sum_{t=1}^{T}\left(\ell_{t}(\hat{\mathbf{w}}_{t})-\ell_{t}(f( \boldsymbol{\alpha}_{t}))\right)\right]\leq(pC_{N}\ln(eT)+4D\kappa^{*})T^{ \frac{N}{1+N}}+(1-p)TD+4pDT(1-\kappa^{-1})^{K}D\]

_Moreover, by definition \(\ell_{t}(\mathbf{w}^{*}(\boldsymbol{\alpha}_{t}))=0\), so choosing \(f(\cdot)=\mathbf{w}^{*}\), the above bound implies that:_

\[\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}[\ell_{t}(\hat{\mathbf{w}}_{t})]\leq 8(pC_{N} \ln(eT)+4D\kappa^{*})T^{-\frac{1}{1+N}}+(1-p)D+4pD\left(1-\kappa^{-1}\right)^ {K}D.\]

The core idea behind the proof (deferred to appendix C) involves maintaining a balance between the regret contribution of each ball and an added regret term arising from approximating the target function using Voronoi partitioning. The regret contribution of each ball is logarithmic in the number of predictions made due to regret of online quadratic optimization [6, p. 42]. Ignoring log factors, the overall regret contribution equals the number of balls, which is essentially governed by the packing number with respect to the \(\ell^{2}\) metric. The additional term in the regret comes from the algorithm's prediction being constant within the Voronoi cells of \(\Delta^{N}\) induced by the current centers (considering that we predict using the nearest neighbor). Thus, an extra term equal to the product of the balls' radius and the Lipschitz constant is incurred. Finally, the label efficient algorithm we present here incurs yet another, \(p\)-dependent terms, which accounts for the missed labels.

**Corollary 1**.: _If our desired average regret is \(\epsilon>0\), then Algorithm 4 has label complexity:_

\[\tilde{O}\left(\max\left\{\left(\frac{D\kappa^{*}}{\epsilon}\right)^{1+N}, \left(\frac{1-\epsilon/D}{\epsilon}\right)^{1+N}\right\}\right)\left(1-\frac{ \epsilon}{D}\right).\]

Note that we recover the standard version of the algorithm (non-label efficient) by trivially setting \(p=1\), which in contrast has label complexity of order \(\tilde{O}(\max\{\left(D\kappa^{*}/\epsilon\right)^{1+N},\left(1/\epsilon \right)^{1+N}\})\), which is strictly larger than the label efficient version as long as \(\epsilon\) is not zero. When our desired regret goes to zero, the label complexity of two algorithms will tend to be the same asymptotically.

## 4 Experiments

To demonstrate the effectiveness of our proposed mixture weights estimation algorithm, we conducted an experiment using MNIST dataset [1] according to the following specifications. We consider a scenario with 15 source distributions, and dividing them into 3 groups. For group 1, it contains 5 source distributions and each distribution contains 100 data (80 for training and 20 for testing) samples which are drawn uniformly randomly from class '0', '1' and '2'. Group 2 and 3 share similar settings but their distributions' data are drawn from class '3', '4', '5', and class '6', '7', '9', '10' respectively. The data generation process is summarized in Table 1.

To demonstrate the effectiveness of our Algorithm 1, we implemented and run experiments with two-layer MLP neural network. We choose four different target setting: (1) target distribution

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline
**Group** & **Classes** & **Domains per Group** & **Samples per Domain** \\ \hline
1 & 0, 1, 2 & 5 & 100 \\ \hline
2 & 3, 4, 5 & 5 & 100 \\ \hline
3 & 6, 7, 8, 9 & 5 & 100 \\ \hline \end{tabular}
\end{table}
Table 1: Classes and samples per domain for each group from Group 1, (2) target distribution from Group 2, (3) target distribution from Group 3, (4) target distribution from the mix of Group 1 and Group 2. We compare three algorithms: 1. Weighted ERM using our learnt weights, 2. ERM on averaged weights and 3. ERM solely on target domain, and presented our findings in Table 2. The results indicate that the accuracy achieved using the learnt alphas outperforms the other two approaches.

## 5 Discussion and Conclusions

In this paper we studied the multi-source multi-target domain adaptation problem. In the first part of the paper we gave an algorithm for adressing a minimax problem, that provably finds good mixture weights of source domains, given a single target domain. In the second part we studied the problem of domain adaptation with multiple target domains, and introduced the co-component ERM problem. We gave two concrete algorithms to solve co-component ERM problem, in offline and online settings. There are several potential future venues for future work, which we briefly discuss below.

Online mixture weight predictionThroughout Section 3, we assumed that the target domains' \(\bm{\alpha}\)s are given. However, it would be interesting if given a new target domain, one could predict _good_ mixture weight in an online fashion, and our Algorithm 1 could serve as an oracle to give the inexact label. Meanwhile, since the Algorithm 1 takes considerable time to converge, a desired online algorithm should also be label-efficient.

The complexity of solving co-component ERMThe co-component ERM problem we introduced in section 3 is interesting from pure optimization perspective. Even though we proposed the learning-based approach to avoid heavy computation, one alternative direction is to develop efficient algorithms to directly solve \(M\) co-component ERM problems, and give upper and lower complexity bounds.

More structure in \(\mathbf{w^{\star}}\) and better phase transition lawsIn this work we considered a basic structure in co-component ERM problems (strong-convexity and smoothness), which gave rise to Lipschitzness of \(\mathbf{w^{\star}}\). Lipschitz class of functions is very large and in general can only be learned at a rate \(\Theta(n^{-\frac{2}{2+N}})\). As discussed in section 3.1 this allowed us to argue that the learning approach is more efficient than solving co-component ERM whenever \(M\geq\Omega((1/\epsilon)^{N/2})\). However, we could potentially obtain better rates (and better laws) of learning \(\mathbf{w^{\star}}\) having more structure in \(\mathbf{w^{\star}}\). For example, assuming that \(\mathbf{w^{\star}}\) is \(H\)-times differentiable, the excess risk would behave as \(\Theta(n^{-\frac{2H}{2H+N}})\)[11]. Thus, the learning approach would be more efficient when \(M\geq\Omega((1/\epsilon)^{N/(2H)})\), that is with potentially much fewer sources than targets. It is an intriguing question to figure our which co-component ERM problems would allows for a nicer structure in \(\mathbf{w^{\star}}\).

Alternative learning based approach for co-component ERMThere may be several other learning based method to solve co-component ERM. One potential approach is Meta learning [10]. The idea is to train a meta model \(\mathbf{w}_{\mathrm{meta}}\) by optimizing a pre-defined meta objective based on some sampled mixture weights, and the goal would be to find a model that can quickly adapt to tasks with different mixture weights \(\bm{\alpha}\). We leave this as a promising open problem.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline  & Target & Target & Target & Target \\  & (Group 1) & (Group 2) & (Group 3) & (mix of Group 1 and 2) \\ \hline Average ERM & 69.9 \% & 40.0 \% & 34.9 \% & 59.9 \% \\ \hline Pure target training & 69.9 \% & 55.0\% & 40.0 \% & 55.0\% \\ \hline Our method & 80.0 \% & 69.9 \% & 55.0 \% & 65.0 \% \\ \hline \hline \end{tabular}
\end{table}
Table 2: Accuracy comparison with two baseline algorithms. Each row represents the accuracy of model learnt by Average ERM, Pure target training or Our method, on different target domain. We can see that the models learnt using mixture weights from our algorithm (Algorithm 1) always yield the best accuracy.

## Acknowledgement

The work of YD and MM was partially supported by NSF CAREER Award # 2239374 and CNS Award #1956276.

## References

* [1] The mnist database of handwritten digits. _http://yann. lecun. com/exdb/mnist/_.
* [2] F. Bach. Breaking the curse of dimensionality with convex neural networks. _Journal of Machine Learning Research_, 18(1):629-681, 2017.
* [3] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. _Journal of Machine Learning Research_, 3(Nov):463-482, 2002.
* [4] P. L. Bartlett, A. Montanari, and A. Rakhlin. Deep learning: a statistical viewpoint. _Acta Numerica_, 2021.
* [5] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from different domains. _Machine Learning_, 79(1-2):151-175, 2010.
* [6] N. Cesa-Bianchi and G. Lugosi. _Prediction, learning, and games_. Cambridge University Press, 2006.
* [7] T. Chen, Y. Sun, and W. Yin. Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization. _IEEE Transactions on Signal Processing_, 69:4937-4948, 2021.
* [8] F. Cucker and D. X. Zhou. _Learning theory: an approximation theory viewpoint_, volume 24. Cambridge University Press, 2007.
* [9] Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. _arXiv preprint arXiv:1810.02054_, 2018.
* [10] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International Conference on Machine Learing (ICML)_, pages 1126-1135. PMLR, 2017.
* [11] L. Gyorfi, M. Kohler, A. Krzyzak, and H. Walk. _A distribution-free theory of nonparametric regression_. Springer, 2006.
* [12] S. Hanneke and S. Kpotufe. A no-free-lunch theorem for multitask learning. _The Annals of Statistics_, 50(6):3119-3143, 2022.
* [13] E. Hazan and N. Megiddo. Online Learning with Prior Knowledge. In _Learning Theory_, pages 499-513. Springer, 2007.
* [14] T. Hu, W. Wang, C. Lin, and G. Cheng. Regularization matters: A nonparametric perspective on overparametrized neural network. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 829-837. PMLR, 2021.
* [15] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: convergence and generalization in neural networks. In _Conference on Neural Information Processing Systems_, 2018.
* [16] J. Johnson, M. Douze, and H. Jegou. Billion-scale similarity search with GPUs. _IEEE Transactions on Big Data_, 7(3):535-547, 2019.
* [17] N. Konstantinov and C. H. Lampert. Robust learning from untrusted sources. In _International Conference on Machine Learing (ICML)_, pages 3488-3498. PMLR, 2019.
* [18] S. Kpotufe and F. Orabona. Regression-tree tuning in a streaming setting. _Conference on Neural Information Processing Systems_, 26, 2013.
* [19] R. Krauthgamer and J. R. Lee. Navigating nets: simple algorithms for proximity search. In _Proceedings of the 15th annual ACM-SIAM Symposium on Discrete algorithms_, pages 798-807. Society for Industrial and Applied Mathematics, 2004.

* [20] I. Kuzborskij and N. Cesa-Bianchi. Nonparametric Online Regression while Learning the Metric. In _Conference on Neural Information Processing Systems_, 2017.
* [21] I. Kuzborskij and Cs. Szepesvari. Learning lipschitz functions by gd-trained shallow overparameterized relu neural networks. arXiv:2212.13848, 2022.
* [22] T. Lin, C. Jin, and M. Jordan. Near-optimal algorithms for minimax optimization. _arXiv preprint arXiv:2002.02417_, 2020.
* [23] T. Lin, C. Jin, and M. I. Jordan. On gradient descent ascent for nonconvex-concave minimax problems. _arXiv preprint arXiv:1906.00331_, 2019.
* [24] Y. Mansour, M. Mohri, J. Ro, A. T. Suresh, and K. Wu. A theory of multiple-source adaptation with limited target labeled data. In Arindam Banerjee and Kenji Fukumizu, editors, _International Conference on Artificial Intelligence and Statistics (AISTATS)_, volume 130 of _Proceedings of Machine Learning Research_, pages 2332-2340. PMLR, 13-15 Apr 2021.
* [25] Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In _Conference on Computational Learning Theory (COLT)_, 2009.
* [26] H. B. McMahan, E. Moore, D. Ramage, and S. Hampson. Communication-efficient learning of deep networks from decentralized data. _arXiv preprint arXiv:1602.05629_, 2016.
* [27] A. Mokhtari, A. Ozdaglar, and S. Pattathil. Convergence rate of o (1/k) for optimistic gradient and extra-gradient methods in smooth convex-concave saddle point problems. _arXiv preprint arXiv:1906.01115_, 2019.
* [28] A. Mokhtari, A. Ozdaglar, and S. Pattathil. Proximal point approximations achieving a convergence rate of o(1/k) for smooth convex-concave saddle point problems: Optimistic gradient and extra-gradient methods. _arXiv preprint arXiv:1906.01115_, 2019.
* [29] Q. Nguyen, M. Mondelli, and G. F. Montufar. Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks. In _International Conference on Machine Learing (ICML)_, pages 8119-8129. PMLR, 2021.
* [30] M. Nouiehed, M. Sanjabi, J. D. Lee, and M. Razaviyayn. Solving a class of non-convex min-max games using iterative first order methods. _arXiv preprint arXiv:1902.08297_, 2019.
* [31] S. Shalev-Shwartz and S. Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge University Press, 2014.
* [32] N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and fast rates. In _Conference on Neural Information Processing Systems_, pages 2199-2207, 2010.
* [33] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge University Press, 2019.
* [34] Z. Xu, H. Zhang, Y. Xu, and G. Lan. A unified single-loop alternating gradient projection algorithm for nonconvex-concave and convex-nonconcave minimax problems. _Mathematical Programming_, pages 1-72, 2023.
* [35] C. Zhang, L. Zhang, and J. Ye. Generalization bounds for domain adaptation. _Conference on Neural Information Processing Systems_, 25, 2012.

Proof of theorem 2: Stationarity of Algorithm 1

### Proof Sketch

To prove the convergence of Algorithm 1 to a stationary point, we first need to relate our convergence measure to actual iterates during the algorithm dynamics. Defining

\[G_{w}(\boldsymbol{\alpha},\mathbf{w})=\frac{1}{\gamma}\left(\mathbf{w}-\mathcal{ P}_{\mathcal{W}}\left(\mathbf{w}+\gamma\nabla_{\mathbf{w}}F(\boldsymbol{\alpha}, \mathbf{w})\right)\right),\;G_{\alpha}(\boldsymbol{\alpha},\mathbf{w})=\frac{1 }{\eta}\left(\boldsymbol{\alpha}-\mathcal{P}_{\Lambda}\left(\boldsymbol{ \alpha}-\eta\nabla_{\boldsymbol{\alpha}}F(\boldsymbol{\alpha},\mathbf{w}) \right)\right)\,,\]

the following statements hold:

\[\mathbb{E}\left\|G_{\alpha}(\boldsymbol{\alpha}^{t},\mathbf{w}^{ t})\right\|^{2}\leq\frac{2}{\eta^{2}}\,\mathbb{E}\left\|\boldsymbol{\alpha}^{t}- \boldsymbol{\alpha}^{t+1}\right\|^{2}+2G_{g}^{2}\sum_{j=1}^{N}\mathbb{E} \left\|z_{j}^{t+1}-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t})-f_{j}(\mathbf{ w}^{t}))\right\|^{2}\,,\] \[\mathbb{E}\left\|G_{w}(\boldsymbol{\alpha}^{t},\mathbf{w}^{t}) \right\|^{2}\leq\frac{2}{\gamma^{2}}\,\mathbb{E}\left\|\mathbf{w}^{t}- \mathbf{w}^{t+1}\right\|^{2}+4G_{g}^{2}\frac{\sigma^{2}}{B}+8G_{f}^{2}L_{g}^{2 }\sum_{j=1}^{N}\alpha^{t}(j)\,\mathbb{E}\left\|z_{j}^{t+1}-(f_{\widehat{ \mathcal{T}}}(\mathbf{w}^{t})-f_{j}(\mathbf{w}^{t}))\right\|^{2}\,.\]

Bounds above show that the gradient norm can be bounded by the difference between two iterates generated by Algorithm 1, and the approximation error of our stochastic correction steps. Hence, we need to quantify how well the proposed stochastic correction step can approximate inner finite functions, i.e., the difference between \(z_{j}^{t+1}\) and \(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t})-f_{j}(\mathbf{w}^{t})\). This gap can be bounded following a standard result from [7]:

\[\mathbb{E}\left\|z_{j}^{t+1}-(f_{\widehat{\mathcal{T}}}(\mathbf{w }^{t})-f_{j}(\mathbf{w}^{t}))\right\|^{2}\leq \,(1-\beta)^{2}\,\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{ \mathcal{T}}}(\mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[+4(1-\beta)^{2}G_{f}^{2}\left\|\mathbf{w}^{t}-\mathbf{w}^{t-1} \right\|^{2}+2\beta^{2}\frac{\sigma^{2}}{B}.\]

Besides above tracking error, we will also need to control the iterates gap of auxiliary variable \(z_{j}^{t}\):

\[\mathbb{E}\left\|z_{j}^{t+1}-z_{j}^{t}\right\|^{2}\leq \,(1-\frac{\beta}{2})^{2}\,\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1} \right\|^{2}+8\left(1+\frac{2}{\beta}\right)G_{f}^{2}\left(\left\|\mathbf{w} ^{t}-\mathbf{w}^{t-1}\right\|^{2}+\left\|\mathbf{w}^{t-1}-\mathbf{w}^{t-2} \right\|^{2}\right)\] \[+8\left(1+\frac{2}{\beta}\right)\beta^{2}G_{f}^{2}\left\| \mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}.\]

It shows that as the primal iterates approach the stationary point (i.e., iterates will not change too much), our auxiliary variable \(z_{j}^{t}\) also will not change too much.

Next, we are going to characterize the upper bound of \(\mathbb{E}\left\|\boldsymbol{\alpha}^{t}-\boldsymbol{\alpha}^{t+1}\right\|^{2}\), \(\mathbb{E}\left\|\mathbf{w}^{t}-\mathbf{w}^{t+1}\right\|^{2}\). To this end, our proof relies on constructing two-level potential functions. Our first level potential function is defined as

\[\hat{F}^{t+1}:= F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^{t+1})-\frac{2}{\eta^{2} \mu}\|\boldsymbol{\alpha}^{t+1}-\boldsymbol{\alpha}^{t}\|^{2}-O\left(\frac{1} {4\gamma}+L_{g}^{2}G_{f}^{4}\gamma+\frac{\eta L^{2}}{2}+\frac{NG_{g}^{2}}{ \mu\eta\beta}\right)\left\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\|^{2}\] \[-O\left(\frac{1}{8\gamma}+\frac{NG_{g}^{2}}{\mu\eta\beta}\right) \left\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}+O\left(\frac{7}{2\eta}+\mu- \frac{\eta L^{2}}{2}\right)\left\|\boldsymbol{\alpha}^{t+1}-\boldsymbol{\alpha }^{t}\right\|^{2}\]

Then we prove the following one-iteration relationship:

\[\mathbb{E}[\hat{F}^{t+1}-\hat{F}^{t}]\geq C_{1}\,\mathbb{E}\left\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\|^{2}+ \frac{1}{8\gamma}\,\mathbb{E}\left\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2 }+\frac{1}{8\gamma}\,\mathbb{E}\left\|\mathbf{w}^{t-1}-\mathbf{w}^{t-2}\right\| ^{2}\] \[-O\left(\gamma+\beta^{2}\gamma\right)\frac{\sigma^{2}}{B}-\left(1 -\frac{\beta}{2}\right)^{2}O\left(\frac{1}{\eta}\right)\sum_{j=1}^{N}\mathbb{E} \left\|z_{j}^{t}-z_{j}^{t-1}\right\|^{2},\]where \(C_{1}\) and \(C_{2}\) are some constant depending on \(\eta,\gamma,L,\mu\). To eliminate the approximation error \(\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t-1})-f_{j}( \mathbf{w}^{t-1}))\right\|^{2}\) and difference of auxiliary iterates \(\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1}\right\|^{2}\), we need construct another potential function \(\tilde{F}^{t+1}\):

\[\tilde{F}^{t+1}:=\tilde{F}^{t+1}-\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t+1}-(f_ {\widehat{\mathcal{T}}}(\mathbf{w}^{t})-f_{j}(\mathbf{w}^{t}))\right\|^{2}-O \left(\frac{(1-\frac{\beta}{2})^{2}}{\beta}\frac{G_{g}^{2}}{\mu^{2}\eta}\right) \sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t+1}-z_{j}^{t}\right\|^{2}\]

Finally, we arrive at the following bounds.

\[\mathbb{E}[\tilde{F}^{t+1}-\tilde{F}^{t}]\geq\frac{C_{1}}{2}\gamma^{2}\, \mathbb{E}\left\|\nabla_{\mathbf{w}}G(\boldsymbol{\alpha}^{t},\mathbf{w}^{t} )\right\|^{2}+\frac{C_{2}}{2}\eta^{2}\,\mathbb{E}\left\|\nabla_{\boldsymbol{ \alpha}}G(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}\]

Conducting telescoping summation will yield the desired result.

### Technical Lemmas

**Proposition 2**.: _If \(g(\cdot)\) is \(G_{g}\) Lipschitz and \(L_{g}\) smooth, and \(f_{\widehat{\mathcal{T}}}(\cdot),f_{1}(\cdot),...,f_{N}(\cdot)\) are all \(G_{f}\) Lipschitz and \(L_{f}\) smooth, then \(F(\boldsymbol{\alpha},\mathbf{w})\) is \(L:=\max\left\{4G_{f}^{2}L_{g}+2G_{g}L_{f},\frac{2C}{m_{\min}}\right\}\) smooth, and \(\mu=\frac{2C}{m_{\max}}\) strongly convex in \(\boldsymbol{\alpha}\)._

Proof.: We first examine the Lipschitzness of gradient of \(F\) w.r.t. \(\boldsymbol{\alpha}\).

\[\nabla_{\boldsymbol{\alpha}}F(\boldsymbol{\alpha},\mathbf{w})=[g(f_{\widehat {\mathcal{T}}}(\mathbf{w})-f_{1}(\mathbf{w})),...,g(f_{\widehat{\mathcal{T}}}( \mathbf{w})-f_{N}(\mathbf{w})))]+2C\mathbf{M}\boldsymbol{\alpha}\]

Hence

\[\left\|\nabla_{\boldsymbol{\alpha}}F(\boldsymbol{\alpha},\mathbf{w})-\nabla_{ \boldsymbol{\alpha}}F(\boldsymbol{\alpha}^{\prime},\mathbf{w})\right\|\leq 2C \left\|\boldsymbol{\alpha}-\boldsymbol{\alpha}^{\prime}\right\|\]

Similarly for gradient of \(F\) w.r.t. \(\mathbf{w}\):

\[\left\|\nabla_{\mathbf{w}}F(\boldsymbol{\alpha},\mathbf{w})- \nabla_{\mathbf{w}}F(\boldsymbol{\alpha},\mathbf{w}^{\prime})\right\|\] \[\leq \sum_{i=1}^{N}\alpha(i)\left\|\nabla g(f_{\widehat{\mathcal{T}}}( \mathbf{w})-f_{i}(\mathbf{w}))(\nabla f_{\widehat{\mathcal{T}}}(\mathbf{w})- \nabla f_{i}(\mathbf{w}))-\nabla g(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{ \prime})-f_{i}(\mathbf{w}^{\prime}))(\nabla f_{\widehat{\mathcal{T}}}( \mathbf{w}^{\prime})-\nabla f_{i}(\mathbf{w}^{\prime}))\right\|\] \[\leq \sum_{i=1}^{N}\alpha(i)\left\|\nabla g(f_{\widehat{\mathcal{T}}}( \mathbf{w})-f_{i}(\mathbf{w}))(\nabla f_{\widehat{\mathcal{T}}}(\mathbf{w})- \nabla f_{i}(\mathbf{w}))-\nabla g(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{ \prime})-f_{i}(\mathbf{w}^{\prime}))(\nabla f_{\widehat{\mathcal{T}}}( \mathbf{w})-\nabla f_{i}(\mathbf{w}))\right\|\] \[+\sum_{i=1}^{N}\alpha(i)\left\|\nabla g(f_{\widehat{\mathcal{T}}}( \mathbf{w}^{\prime})-f_{i}(\mathbf{w}^{\prime}))(\nabla f_{\widehat{\mathcal{ T}}}(\mathbf{w})-\nabla f_{i}(\mathbf{w}))-\nabla g(f_{\widehat{\mathcal{T}}}( \mathbf{w}^{\prime})-f_{i}(\mathbf{w}^{\prime}))(\nabla f_{\widehat{\mathcal{ T}}}(\mathbf{w}^{\prime})-\nabla f_{i}(\mathbf{w}^{\prime}))\right\|\] \[\leq \sum_{i=1}^{N}\alpha(i)L_{g}\left\|f_{\widehat{\mathcal{T}}}( \mathbf{w})-f_{i}(\mathbf{w})-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{\prime} )-f_{i}(\mathbf{w}^{\prime}))\right\|\cdot 2G_{f}+\sum_{i=1}^{N}\alpha(i)2L_{f}\left\| \mathbf{w}-\mathbf{w}^{\prime}\right\|\cdot G_{g}\] \[\leq \sum_{i=1}^{N}\alpha(i)4G_{f}^{2}L_{g}\left\|\mathbf{w}-\mathbf{w} ^{\prime}\right\|+\sum_{i=1}^{N}\alpha(i)2G_{g}L_{f}\left\|\mathbf{w}- \mathbf{w}^{\prime}\right\|.\]

For strong convexity, we compute Hessian w.r.t. \(\boldsymbol{\alpha}\):

\[\nabla_{\boldsymbol{\alpha}}^{2}F(\mathbf{w},\boldsymbol{\alpha})=2C\mathbf{M} \succeq 2C\frac{1}{m_{\max}}\mathbf{I},\]

so \(F(\boldsymbol{\alpha},\mathbf{w})\) is \(2C\mathbf{M}\) strongly convex in \(\boldsymbol{\alpha}\). 

The following lemma bounds the tracking error of the stochastic correction algorithm.

**Lemma 2** (Tracking Error [7]).: _For Algorithm 1, under the assumptions of Theorem 2, the following statement holds true:_

\[\mathbb{E}\left\|z_{j}^{t+1}-(f_{\widehat{\mathcal{T}}}(\mathbf{w} ^{t})-f_{j}(\mathbf{w}^{t}))\right\|^{2}\leq \ (1-\beta)^{2}\,\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{\mathcal{T}}}( \mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[+4(1-\beta)^{2}G_{f}^{2}\left\|\mathbf{w}^{t}-\mathbf{w}^{t-1} \right\|^{2}+2\beta^{2}\frac{\sigma^{2}}{B}.\]An immediate implication of Lemma 2 is the following corollary:

**Corollary 2**.: _For Algorithm 1, under the assumptions of Theorem 2, the following statement holds true:_

\[\mathbb{E}\left\|z_{j}^{T+1}-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^ {T})-f_{j}(\mathbf{w}^{T}))\right\|^{2} \leq(1-\beta)^{2T}\,\mathbb{E}\left\|z_{j}^{0}-(f_{\widehat{ \mathcal{T}}}(\mathbf{w}^{-1})-f_{j}(\mathbf{w}^{-1}))\right\|^{2}\] \[\quad+4\frac{(1-\beta)^{4}}{1-(1-\beta)^{2}}\gamma^{2}G_{f}^{4}G_ {g}^{2}+2\beta^{2}\frac{(1-\beta)^{2}}{1-(1-\beta)^{2}}\frac{\sigma^{2}}{B}.\]

Proof.: Due to Lemma 2 and updating rule for \(\mathbf{w}\), we have

\[\mathbb{E}\left\|z_{j}^{t+1}-(f_{\widehat{\mathcal{T}}}(\mathbf{ w}^{t})-f_{j}(\mathbf{w}^{t}))\right\|^{2} \leq(1-\beta)^{2}\,\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{ \mathcal{T}}}(\mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[\quad+4(1-\beta)^{2}G_{f}^{2}\,\mathbb{E}\left\|\gamma\mathbf{g} _{\mathbf{w}}^{t-1}\right\|^{2}+2\beta^{2}\frac{\sigma^{2}}{B}\] \[\leq(1-\beta)^{2}\,\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{ \mathcal{T}}}(\mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[\quad+4(1-\beta)^{2}G_{f}^{2}\gamma^{2}G_{g}^{2}G_{f}^{2}+2\beta ^{2}\frac{\sigma^{2}}{B}.\]

Then we unroll the recursion in Lemma 2:

\[\mathbb{E}\left\|z_{j}^{T+1}-(f_{\widehat{\mathcal{T}}}(\mathbf{ w}^{T})-f_{j}(\mathbf{w}^{T}))\right\|^{2} \leq(1-\beta)^{2T}\,\mathbb{E}\left\|z_{j}^{0}-(f_{\widehat{ \mathcal{T}}}(\mathbf{w}^{-1})-f_{j}(\mathbf{w}^{-1}))\right\|^{2}\] \[\quad+\sum_{t=0}^{T}(1-\beta)^{2t}\left(4(1-\beta)^{2}\gamma^{2} G_{g}^{2}G_{f}^{4}+2\beta^{2}\frac{\sigma^{2}}{B}\right)\] \[\leq(1-\beta)^{2T}\,\mathbb{E}\left\|z_{j}^{0}-(f_{\widehat{ \mathcal{T}}}(\mathbf{w}^{-1})-f_{j}(\mathbf{w}^{-1}))\right\|^{2}\] \[\quad+\frac{(1-\beta)^{2}}{1-(1-\beta)^{2}}\left(4(1-\beta)^{2} \gamma^{2}G_{g}^{2}G_{f}^{4}+2\beta^{2}\frac{\sigma^{2}}{B}\right).\]

Besides the above lemma, we also need the following bound on tracking error between two consecutive iterates.

**Lemma 3** (Second Order Tracking Error).: _For Algorithm 1, under the assumptions of Theorem 2, the following statement holds true:_

\[\mathbb{E}\left\|z_{j}^{t+1}-z_{j}^{t}\right\|^{2} \leq(1-\frac{\beta}{2})^{2}\,\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1 }\right\|^{2}+8\left(1+\frac{2}{\beta}\right)G_{f}^{2}\left(\left\|\mathbf{w} ^{t}-\mathbf{w}^{t-1}\right\|^{2}+\left\|\mathbf{w}^{t-1}-\mathbf{w}^{t-2} \right\|^{2}\right)\] \[\quad+8\left(1+\frac{2}{\beta}\right)\beta^{2}G_{f}^{2}\left\| \mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}.\]

Proof.: For the ease of presentation, we define the following two auxiliary variables:

\[f_{j}^{t}=f_{i}(\mathbf{w}^{t};\xi_{i}^{t})-f_{j}(\mathbf{w}^{t };\xi_{j}^{t}),\] \[f_{j}^{t\to t-1}=f_{i}(\mathbf{w}^{t};\xi_{i}^{t})-f_{j}(\mathbf{w}^{t };\xi_{j}^{t})-(f_{i}(\mathbf{w}^{t-1};\xi_{i}^{t})-f_{j}(\mathbf{w}^{t-1};\xi _{j}^{t})).\]

According to updating rule of \(z\), we have:

\[z_{j}^{t+1}-z_{j}^{t}=(1-\beta)(z_{j}^{t}-z_{j}^{t-1})+(1-\beta)(f_{j}^{t\to t -1}-f_{j}^{t-1\mapsto t-2})+\beta(f_{j}^{t}-f_{j}^{t-1})\]Taking expectation w.r.t. \(\xi_{j}^{t}\), \(\xi_{i}^{t}\), \(\xi_{j}^{t-1}\) and \(\xi_{i}^{t-1}\) yields:

\[\mathbb{E}\left\|z_{j}^{t+1}-z_{j}^{t}\right\|^{2}\] \[=\mathbb{E}\left\|(1-\beta)(z_{j}^{t}-z_{j}^{t-1})+(1-\beta)(f_{j }^{t\mapsto t-1}-f_{j}^{t-1\mapsto t-2})+\beta(f_{j}^{t}-f_{j}^{t-1})\right\|^{2}\] \[\stackrel{{(\Delta)}}{{\leq}}\left(1+\frac{\beta}{ 2-2\beta}\right)(1-\beta)^{2}\,\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1}+(f_{j}^{ t\mapsto t-1}-f_{j}^{t-1\mapsto t-2})\right\|^{2}\] \[\quad+\left(1+\frac{2-2\beta}{\beta}\right)\left\|\beta(f_{j}^{t }-f_{j}^{t-1})\right\|^{2}\] \[\stackrel{{(\Theta)}}{{\leq}}\left(1-\frac{\beta}{ 2}\right)(1-\beta)\,\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1}+(f_{j}^{t\mapsto t- 1}-f_{j}^{t-1\mapsto t-2})\right\|^{2}+\left(1+\frac{2}{\beta}\right)\beta^{2 }\left\|f_{j}^{t}-f_{j}^{t-1}\right\|^{2}\] \[\stackrel{{(\Lambda)}}{{\leq}}\left(1-\frac{\beta}{ 2}\right)(1-\beta)\left(1+\frac{\beta}{2-2\beta}\right)\mathbb{E}\left\|z_{j} ^{t}-z_{j}^{t-1}\right\|^{2}\] \[\quad+\left(1-\frac{\beta}{2}\right)(1-\beta)\left(1+\frac{2-2 \beta}{\beta}\right)\mathbb{E}\left\|f_{j}^{t\mapsto t-1}-f_{j}^{t-1\mapsto t- 2}\right\|^{2}+\left(1+\frac{2}{\beta}\right)\beta^{2}\left\|f_{j}^{t}-f_{j}^ {t-1}\right\|^{2}\] \[\stackrel{{(\Xi)}}{{\leq}}\left(1-\frac{\beta}{2} \right)^{2}\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1}\right\|^{2}+\underbrace{ \left(1+\frac{2}{\beta}\right)\mathbb{E}\left\|f_{j}^{t\mapsto t-1}-f_{j}^{t-1 \mapsto t-2}\right\|^{2}}_{T_{1}}+\underbrace{\left(1+\frac{2}{\beta}\right) \beta^{2}\left\|f_{j}^{t}-f_{j}^{t-1}\right\|^{2}}_{T_{2}}\]

where in (1) and (3) we use Young's inequality that \(\left\|\mathbf{a}+\mathbf{b}\right\|^{2}\leq(1+a)\left\|\mathbf{a}\right\|^{2 }+(1+\frac{1}{a})\left\|\mathbf{b}\right\|^{2}\). Now we bound \(T_{1}\) as follows:

\[T_{1} \leq 4\left(1+\frac{2}{\beta}\right)\left(\left\|f_{i}(\mathbf{w} ^{t};\xi_{i}^{t})-f_{i}(\mathbf{w}^{t-1};\xi_{i}^{t})\right\|^{2}+\left\|f_{j} (\mathbf{w}^{t};\xi_{j}^{t})-f_{j}(\mathbf{w}^{t-1};\xi_{j}^{t})\right\|^{2}\right)\] \[+4\left(1+\frac{2}{\beta}\right)\left(\left\|f_{i}(\mathbf{w}^{t- 1};\xi_{i}^{t-1})-f_{i}(\mathbf{w}^{t-2};\xi_{i}^{t-1})\right\|^{2}+\left\|f_{j }(\mathbf{w}^{t-1};\xi_{j}^{t-1})-f_{j}(\mathbf{w}^{t-2};\xi_{j}^{t-1})\right\| ^{2}\right)\] \[\leq 8\left(1+\frac{2}{\beta}\right)G_{f}^{2}\left(\left\| \mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}+\left\|\mathbf{w}^{t-1}-\mathbf{w}^ {t-2}\right\|^{2}\right),\]

and for \(T_{2}\):

\[T_{2}\leq 8\left(1+\frac{2}{\beta}\right)\beta^{2}G_{f}^{2}\left\| \mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}.\]

Putting pieces together will conclude the proof. 

The following lemma establishes the difference between the exact gradients computed on \(F\) and the gradients we actually used in Algorithm 1.

**Lemma 4** (Gradient difference).: _For Algorithm 1, under the assumptions of Theorem 2, the following statement holds true:_

\[\mathbb{E}\left\|\mathbf{g}_{\mathbf{w}}^{t}-\nabla_{\mathbf{w}}F( \boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2} \leq 4G_{g}^{2}\frac{\sigma^{2}}{B}+8G_{f}^{2}L_{g}^{2}\sum_{j=1}^{N} \alpha^{t}(j)\,\mathbb{E}\left\|z_{j}^{t+1}-(f_{\widehat{\mathcal{T}}}(\mathbf{ w}^{t})-\nabla f_{j}(\mathbf{w}^{t}))\right\|^{2}\] \[\mathbb{E}\left\|\mathbf{g}_{\boldsymbol{\alpha}}^{t}-\nabla_{ \boldsymbol{\alpha}}F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2} \leq\sum_{j=1}^{N}G_{g}^{2}\,\mathbb{E}\left\|z_{j}^{t+1}-(f_{ \widehat{\mathcal{T}}}(\mathbf{w}^{t})-f_{j}(\mathbf{w}^{t}))\right\|^{2}\]Proof.: By the definition of \(\mathbf{g}_{\mathbf{w}}^{t}\), we have

\[\mathbb{E}\left\|\mathbf{g}_{\mathbf{w}}^{t}-\nabla_{\mathbf{w}}F( \boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}\] \[=\left\|\sum_{j=1}^{N}\alpha^{t}(j)\nabla g(z_{j}^{t+1})(\nabla f_ {\widehat{\mathcal{T}}}(\mathbf{w}^{t};\xi_{i}^{t})-\nabla f_{j}(\mathbf{w}^{ t};\xi_{j}^{t}))-\nabla_{\mathbf{w}}F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t}) \right\|^{2}\] \[\leq 2\,\mathbb{E}\left\|\sum_{j=1}^{N}\alpha^{t}(j)\nabla g(z_{j} ^{t+1})(\nabla f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t};\xi_{i}^{t})-\nabla f _{j}(\mathbf{w}^{t};\xi_{j}^{t})-(\nabla f_{\widehat{\mathcal{T}}}(\mathbf{w}^ {t})-\nabla f_{j}(\mathbf{w}^{t})))\right\|^{2}\] \[\quad+2\,\mathbb{E}\left\|\sum_{j=1}^{N}\alpha^{t}(j)(\nabla g(z _{j}^{t+1})-\nabla g(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t})-\nabla f_{j}( \mathbf{w}^{t}))(\nabla f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t})-\nabla f_{j }(\mathbf{w}^{t}))\right\|^{2}\] \[\leq 4G_{g}^{2}\frac{\sigma^{2}}{B}+8G_{f}^{2}L_{g}^{2}\sum_{j=1}^ {N}\alpha^{t}(j)\,\mathbb{E}\left\|z_{j}^{t+1}-(f_{\widehat{\mathcal{T}}}( \mathbf{w}^{t})-\nabla f_{j}(\mathbf{w}^{t}))\right\|^{2}.\]

where at the last step we use the bounded variance assumption. Similarly by the definition of \(\mathbf{g}_{\boldsymbol{\alpha}}^{t}\) we have:

\[\mathbb{E}\left\|\mathbf{g}_{\boldsymbol{\alpha}}^{t}-\nabla_{ \boldsymbol{\alpha}}F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2} =\mathbb{E}\left\|[g(z_{1}^{t+1}),...,g(z_{N}^{t+1})]-\nabla_{ \mathbf{w}}F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}\] \[\leq\sum_{j=1}^{N}G_{g}^{2}\,\mathbb{E}\left\|z_{j}^{t+1}-(f_{ \widehat{\mathcal{T}}}(\mathbf{w}^{t})-f_{j}(\mathbf{w}^{t}))\right\|^{2}.\]

**Lemma 5** (Connection between stationary measure and iterates).: _For Algorithm 1, if we define_

\[G_{w}(\boldsymbol{\alpha},\mathbf{w})=\frac{1}{\gamma}\left(\mathbf{w}- \mathcal{P}_{\mathcal{W}}\left(\mathbf{w}+\gamma\nabla_{\mathbf{w}}F( \boldsymbol{\alpha},\mathbf{w})\right)\right),G_{\alpha}(\boldsymbol{\alpha}, \mathbf{w})=\frac{1}{\eta}\left(\boldsymbol{\alpha}-\mathcal{P}_{\Lambda} \left(\boldsymbol{\alpha}-\eta\nabla_{\boldsymbol{\alpha}}F(\boldsymbol{ \alpha},\mathbf{w})\right)\right)\,,\]

_then the following statements hold:_

\[\mathbb{E}\left\|G_{\alpha}(\boldsymbol{\alpha}^{t},\mathbf{w}^{ t})\right\|^{2} \leq\frac{2}{\eta^{2}}\,\mathbb{E}\left\|\boldsymbol{\alpha}^{t}- \boldsymbol{\alpha}^{t+1}\right\|^{2}+2G_{g}^{2}\sum_{j=1}^{N}\mathbb{E}\left\| z_{j}^{t+1}-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t})-f_{j}(\mathbf{w}^{t})) \right\|^{2}\,,\] \[\mathbb{E}\left\|G_{w}(\boldsymbol{\alpha}^{t},\mathbf{w}^{t}) \right\|^{2} \leq\frac{2}{\gamma^{2}}\,\mathbb{E}\left\|\mathbf{w}^{t}-\mathbf{w}^{t+1} \right\|^{2}+4G_{g}^{2}\frac{\sigma^{2}}{B}+8G_{f}^{2}L_{g}^{2}\sum_{j=1}^{N} \alpha^{t}(j)\,\mathbb{E}\left\|z_{j}^{t+1}-(f_{\widehat{\mathcal{T}}}( \mathbf{w}^{t})-f_{j}(\mathbf{w}^{t}))\right\|^{2}\,.\]

Proof.: We begin with proving the first statement. According to updating rule we have

\[\mathbb{E}\left\|G_{\boldsymbol{\alpha}}(\boldsymbol{\alpha}^{t}, \mathbf{w}^{t})\right\|^{2}\] \[=\mathbb{E}\left\|\frac{1}{\eta}\left(\boldsymbol{\alpha}^{t}- \mathcal{P}_{\Lambda}\left(\boldsymbol{\alpha}^{t}-\eta\nabla_{\boldsymbol{ \alpha}}F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right)\right)\right\|^{2}\] \[\leq\frac{2}{\eta^{2}}\,\mathbb{E}\left\|\boldsymbol{\alpha}^{t}- \mathcal{P}_{\Lambda}\left(\boldsymbol{\alpha}^{t}-\eta\mathbf{g}_{\boldsymbol{ \alpha}}^{t}\right)\right\|^{2}+\frac{2}{\eta^{2}}\,\mathbb{E}\left\|\mathcal{P} _{\Lambda}\left(\boldsymbol{\alpha}^{t}-\eta\mathbf{g}_{\boldsymbol{\alpha}}^{t }\right)-\mathcal{P}_{\Lambda}\left(\boldsymbol{\alpha}^{t}-\eta\nabla_{ \boldsymbol{\alpha}}F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right)\right\|^{2}\] \[\leq\frac{2}{\eta^{2}}\,\mathbb{E}\left\|\boldsymbol{\alpha}^{t}- \boldsymbol{\alpha}^{t+1}\right\|^{2}+2\,\mathbb{E}\left\|\mathbf{g}_{ \boldsymbol{\alpha}}^{t}-\nabla_{\boldsymbol{\alpha}}F(\boldsymbol{\alpha}^{t}, \mathbf{w}^{t})\right\|^{2}\] \[\leq\frac{2}{\eta^{2}}\,\mathbb{E}\left\|\boldsymbol{\alpha}^{t}- \boldsymbol{\alpha}^{t+1}\right\|^{2}+2G_{g}^{2}\sum_{j=1}^{N}\mathbb{E}\left\|z_ {j}^{t+1}-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t})-f_{j}(\mathbf{w}^{t})) \right\|^{2}\]where at last step we apply Lemma 4. Similarly, for the second statement we have:

\[\mathbb{E}\left\|G_{\mathbf{w}}(\boldsymbol{\alpha}^{t},\mathbf{w}^{ t})\right\|^{2}\] \[=\mathbb{E}\left\|\frac{1}{\gamma}\left(\mathbf{w}^{t}-\mathcal{P} _{\mathcal{W}}\left(\mathbf{w}^{t}+\eta\nabla_{\mathbf{w}}F(\boldsymbol{\alpha }^{t},\mathbf{w}^{t})\right)\right)\right\|^{2}\] \[\leq\frac{2}{\gamma^{2}}\,\mathbb{E}\left\|\mathbf{w}^{t}- \mathcal{P}_{\mathcal{W}}\left(\mathbf{w}^{t}+\gamma\mathbf{g}_{\mathbf{w}}^{ t}\right)\right\|^{2}+\frac{2}{\gamma^{2}}\,\mathbb{E}\left\|\mathcal{P}_{ \mathcal{W}}\left(\mathbf{w}^{t}+\gamma\mathbf{g}_{\mathbf{w}}^{t}\right)- \mathcal{P}_{\mathcal{W}}\left(\mathbf{w}^{t}+\gamma\nabla_{\mathbf{w}}F( \boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right)\right\|^{2}\] \[\leq\frac{2}{\gamma^{2}}\,\mathbb{E}\left\|\mathbf{w}^{t}- \mathbf{w}^{t+1}\right\|^{2}+4G_{g}^{2}\frac{\sigma^{2}}{B}+8G_{f}^{2}L_{g}^{ 2}\sum_{j=1}^{N}\alpha^{t}(j)\,\mathbb{E}\left\|z_{j}^{t+1}-(f_{\widehat{ \mathcal{T}}}(\mathbf{w}^{t})-\nabla f_{j}(\mathbf{w}^{t}))\right\|^{2}.\]

The following lemma connects \(F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^{t+1})\) and \(F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^{t})\).

**Lemma 6** (Dual Variable One Iteration Analysis).: _Let \(L:=\max\left\{4G_{f}^{2}L_{g}+2G_{g}L_{f},2C\right\}\). For Algorithm 1, under the assumptions of Theorem 2, the following statement holds true:_

\[\mathbb{E}[F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^{t+1})-F( \boldsymbol{\alpha}^{t+1},\mathbf{w}^{t})]\] \[-4L_{g}^{2}G_{f}^{2}\gamma\sum_{j=1}^{N}\alpha^{t}(j)(1-\beta)^{ 2}\,\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t-1})-f _{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[-16(1-\beta)^{2}L_{g}^{2}G_{f}^{4}\gamma\,\mathbb{E}\left\| \mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}.\]

Proof.: By the optimality of projection and updating rule of \(\mathbf{w}\), we have:

\[\left\langle\gamma\mathbf{g}_{\mathbf{w}}^{t}+(\mathbf{w}^{t}-\mathbf{w}^{t+1} ),\mathbf{w}^{t}-\mathbf{w}^{t+1}\right\rangle\leq 0\]

Re-arranging terms yields:

\[\left\langle\mathbf{g}_{\mathbf{w}}^{t},\mathbf{w}^{t}-\mathbf{w}^{t+1} \right\rangle\leq-\frac{1}{\gamma}\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\|^{2}\]

Adding and subtracting \(\nabla_{\mathbf{w}}F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\) gives:

\[\left\langle\nabla_{\mathbf{w}}F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t}), \mathbf{w}^{t}-\mathbf{w}^{t+1}\right\rangle+\left\langle\mathbf{g}_{\mathbf{ w}}^{t}-\nabla_{\mathbf{w}}F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t}), \mathbf{w}^{t}-\mathbf{w}^{t+1}\right\rangle\leq-\frac{1}{\gamma}\|\mathbf{w}^ {t+1}-\mathbf{w}^{t}\|^{2}\]

Applying Cauchy-Schwartz inequality yields:

\[\frac{1}{\gamma}\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\|^{2}\leq\left\langle\nabla _{\mathbf{w}}F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t}),\mathbf{w}^{t+1}- \mathbf{w}^{t}\right\rangle+\frac{1}{2\gamma}\left\|\mathbf{w}^{t+1}-\mathbf{ w}^{t}\right\|^{2}+\frac{1}{2}\gamma\left\|\mathbf{g}_{\mathbf{w}}^{t}-\nabla_{ \mathbf{w}}F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}\]

Now, we take expectation over the randomness of \(\zeta_{\mathcal{T}}^{t}\), \(\zeta_{j}^{t}\), and apply Lemma 4 to get:

\[\frac{1}{2\gamma}\,\mathbb{E}\left\|\mathbf{w}^{t+1}-\mathbf{w}^{ t}\right\|^{2} \leq\mathbb{E}\left\langle\nabla_{\mathbf{w}}F(\boldsymbol{\alpha}^{t},\mathbf{ w}^{t}),\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\rangle+2\gamma G_{g}^{2}\frac{ \sigma^{2}}{B}\] \[\quad+4\gamma G_{f}^{2}L_{g}^{2}\sum_{j=1}^{N}\alpha^{t}(j)\, \mathbb{E}\left\|z_{j}^{t+1}-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t})-\nabla f _{j}(\mathbf{w}^{t}))\right\|^{2}.\]Plugging in Lemma 2 yields:

\[\frac{1}{2\gamma}\operatorname{\mathbb{E}}\|\mathbf{w}^{t+1}- \mathbf{w}^{t}\|^{2} \leq\operatorname{\mathbb{E}}\left\langle\nabla_{\mathbf{w}}F( \boldsymbol{\alpha}^{t},\mathbf{w}^{t}),\mathbf{w}^{t+1}-\mathbf{w}^{t} \right\rangle+2\gamma G_{g}^{2}\frac{\sigma^{2}}{B}\] \[\quad+4\gamma G_{f}^{2}L_{g}^{2}\sum_{j=1}^{N}\alpha^{t}(j) \Bigg{(}(1-\beta)^{2}\operatorname{\mathbb{E}}\left\|z_{j}^{t}-(f_{\widehat{ \mathcal{T}}}(\mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[\qquad\qquad\qquad\qquad+4(1-\beta)^{2}G_{f}^{2}\left\|\mathbf{w }^{t}-\mathbf{w}^{t-1}\right\|^{2}+2\beta^{2}\frac{\sigma^{2}}{B}\Bigg{)}\;.\]

On the other hand, from Proposition 2 on the smoothness of \(F\) with parameter \(L\) we have:

\[F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^{t+1})-F(\boldsymbol{ \alpha}^{t+1},\mathbf{w}^{t})\] \[\geq\left\langle\nabla_{\mathbf{w}}F(\boldsymbol{\alpha}^{t+1}, \mathbf{w}^{t}),\,\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\rangle-\frac{L}{2} \left\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\|^{2}\] \[=\left\langle\nabla F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t}),\, \mathbf{w}^{t+1}-\mathbf{w}^{t}\right\rangle-\frac{L}{2}\left\|\mathbf{w}^{t+ 1}-\mathbf{w}^{t}\right\|^{2}+\left\langle\nabla F(\boldsymbol{\alpha}^{t+1}, \mathbf{w}^{t})-\nabla F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t}),\,\mathbf{w}^ {t+1}-\mathbf{w}^{t}\right\rangle\] \[\geq\left\langle\nabla F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t}), \,\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\rangle-\frac{L}{2}\left\|\mathbf{w}^{t +1}-\mathbf{w}^{t}\right\|^{2}\] \[\geq\left\langle\nabla F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t}), \,\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\rangle-\left(\frac{L}{2}+\frac{L^{2} \eta}{2}\right)\left\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\|^{2}-\frac{1}{2 \eta}\left\|\boldsymbol{\alpha}^{t+1}-\boldsymbol{\alpha}^{t}\right\|^{2}\;.\]

Putting pieces together will conclude the proof:

\[\operatorname{\mathbb{E}}[F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^ {t+1})-F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^{t})]\] \[-4L_{g}^{2}G_{f}^{2}\gamma\sum_{j=1}^{N}\alpha^{t}(j)(1-\beta)^{2 }\operatorname{\mathbb{E}}\left\|z_{j}^{t}-(f_{\widehat{\mathcal{T}}}( \mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[-16(1-\beta)^{2}L_{g}^{2}G_{f}^{4}\gamma\operatorname{\mathbb{E} }\left\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}.\]

The following lemma connects \(F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^{t+1})\) and \(F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\).

**Lemma 7** (One Iteration Descent Lemma).: _For Algorithm 1, under the assumptions of Theorem 2, the following statement holds true:_

\[\operatorname{\mathbb{E}}[F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^ {t+1})-F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})]\] \[+\left(\frac{\mu}{2}-\frac{3}{2\eta}-\frac{1}{4\eta(1-\beta)^{2}} \right)\operatorname{\mathbb{E}}\left\|\mathbf{\alpha}^{t+1}-\boldsymbol{ \alpha}^{t}\right\|^{2}+\left(\mu-\frac{1}{2\eta}-\frac{\eta L^{2}}{2}\right) \operatorname{\mathbb{E}}\left\|\boldsymbol{\alpha}^{t}-\boldsymbol{\alpha}^{ t-1}\right\|^{2}\] \[-\sum_{j=1}^{N}(1-\beta)^{2}\left(4\alpha^{t}(j)L_{g}^{2}G_{f}^{2} \gamma+\eta G_{g}^{2}\right)\operatorname{\mathbb{E}}\left\|z_{j}^{t}-(f_{ \widehat{\mathcal{T}}}(\mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[-\left(2\gamma G_{g}^{2}+8\beta^{2}L_{g}^{2}G_{f}^{2}\gamma\right) \frac{\sigma^{2}}{B}.\]Proof.: According to property of projection we have:

\[\left\langle\mathbf{g}_{\alpha}^{t}+\frac{1}{\eta}(\bm{\alpha}^{t+1} -\bm{\alpha}^{t}),\bm{\alpha}-\bm{\alpha}^{t+1}\right\rangle\geq 0\] (5) \[\left\langle\mathbf{g}_{\alpha}^{t}+\frac{1}{\eta}(\bm{\alpha}^{t+ 1}-\bm{\alpha}^{t}),\bm{\alpha}^{t}-\bm{\alpha}^{t+1}\right\rangle\geq 0\] (6) \[\left\langle\mathbf{g}_{\alpha}^{t-1}+\frac{1}{\eta}(\bm{\alpha}^{ t}-\bm{\alpha}^{t-1}),\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\rangle\geq 0\] (7)

Since \(F(\cdot,\mathbf{w})\) is strongly convex for any \(\mathbf{w}\in\mathcal{W}\), we have:

\[F(\bm{\alpha}^{t+1},\mathbf{w}^{t})-F(\bm{\alpha}^{t},\mathbf{w} ^{t}) \geq\left\langle\nabla_{\alpha}F(\bm{\alpha}^{t},\mathbf{w}^{t}),\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\rangle+\frac{\mu}{2}\|\bm{\alpha}^{t +1}-\bm{\alpha}^{t}\|^{2}\] \[\geq\left\langle\nabla_{\alpha}F(\bm{\alpha}^{t},\mathbf{w}^{t}) -\mathbf{g}_{\alpha}^{t-1},\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\rangle+ \frac{\mu}{2}\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\|^{2}\] \[\geq\left\langle\nabla_{\alpha}F(\bm{\alpha}^{t},\mathbf{w}^{t}) -\nabla_{\alpha}F(\bm{\alpha}^{t-1},\mathbf{w}^{t-1}),\bm{\alpha}^{t+1}-\bm{ \alpha}^{t}\right\rangle+\frac{\mu}{2}\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\|^{2}\] \[\quad+\left\langle\mathbf{g}_{\alpha}^{t-1}-\nabla_{\alpha}F( \bm{\alpha}^{t-1},\mathbf{w}^{t-1}),\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\rangle\] \[\quad-\frac{1}{\eta}\left\langle\bm{\alpha}^{t}-\bm{\alpha}^{t-1},\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\rangle\]

where at second inequality we use the fact of (7). For the first dot product term, we observe that:

\[\left\langle\nabla_{\alpha}F(\bm{\alpha}^{t},\mathbf{w}^{t})- \nabla_{\alpha}F(\bm{\alpha}^{t-1},\mathbf{w}^{t-1}),\bm{\alpha}^{t+1}-\bm{ \alpha}^{t}\right\rangle\] (8) \[=\left\langle\nabla_{\alpha}F(\bm{\alpha}^{t},\mathbf{w}^{t})- \nabla_{\alpha}F(\bm{\alpha}^{t},\mathbf{w}^{t-1}),\bm{\alpha}^{t+1}-\bm{ \alpha}^{t}\right\rangle\] \[+\left\langle\nabla_{\alpha}F(\bm{\alpha}^{t},\mathbf{w}^{t-1})- \nabla_{\alpha}F(\bm{\alpha}^{t-1},\mathbf{w}^{t-1}),\Delta\right\rangle\] \[+\left\langle\nabla_{\alpha}F(\bm{\alpha}^{t},\mathbf{w}^{t-1})- \nabla_{\alpha}F(\bm{\alpha}^{t-1},\mathbf{w}^{t-1}),\bm{\alpha}^{t}-\bm{ \alpha}^{t-1}\right\rangle\] \[\geq-\frac{L^{2}\eta}{2}\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\|^{2}- \frac{1}{2\eta}\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\|^{2}-\frac{\eta L^{2}}{2} \|\bm{\alpha}^{t}-\bm{\alpha}^{t-1}\|^{2}-\frac{1}{2\eta}\|\Delta\|^{2}+\mu\| \bm{\alpha}^{t}-\bm{\alpha}^{t-1}\|^{2}\] (9)

where \(\Delta=(\bm{\alpha}^{t+1}-\bm{\alpha}^{t})-(\bm{\alpha}^{t}-\bm{\alpha}^{t-1})\).

For the second dot product, we have:

\[\mathbb{E}\left\langle\mathbf{g}_{\alpha}^{t-1}-\nabla_{\alpha}F (\bm{\alpha}^{t-1},\mathbf{w}^{t-1}),\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\rangle\] \[\geq-\eta(1-\beta)^{2}\,\mathbb{E}\left\|\mathbf{g}_{\alpha}^{t-1 }-\nabla_{\alpha}F(\bm{\alpha}^{t-1},\mathbf{w}^{t-1})\right\|^{2}-\frac{1}{4 \eta(1-\beta)^{2}}\,\mathbb{E}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\|^ {2}\] \[\geq-\eta(1-\beta)^{2}\sum_{j=1}^{N}G_{g}^{2}\,\mathbb{E}\left\|z_ {j}^{t}-(f_{\widehat{T}}(\mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2} -\frac{1}{4\eta(1-\beta)^{2}}\,\mathbb{E}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{ t}\right\|^{2}.\]

where we plug in Lemma 4 at last step.

For the third dot product, we use the following identity:

\[\frac{1}{\eta}\left\langle\bm{\alpha}^{t}-\bm{\alpha}^{t-1},\bm{\alpha}^{t+1}- \bm{\alpha}^{t}\right\rangle=\frac{1}{\eta}\left(\frac{1}{2}\|\bm{\alpha}^{t}- \bm{\alpha}^{t-1}\|^{2}+\frac{1}{2}\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\|^{2}- \frac{1}{2}\|\Delta\|^{2}\right)\] (10)

So we have:

\[\mathbb{E}[F(\bm{\alpha}^{t+1},\mathbf{w}^{t})-F(\bm{\alpha}^{t}, \mathbf{w}^{t})] \geq-\frac{L^{2}\eta}{2}\,\mathbb{E}\left\|\mathbf{w}^{t}-\mathbf{ w}^{t-1}\|^{2}+\left(\frac{\mu}{2}-\frac{1}{\eta}-\frac{1}{4\eta(1-\beta)^{2}} \right)\mathbb{E}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\|^{2}\] \[\quad+\left(\mu-\frac{1}{2\eta}-\frac{\eta L^{2}}{2}\right)\mathbb{E }\left\|\bm{\alpha}^{t}-\bm{\alpha}^{t-1}\right\|^{2}\] \[\quad-\eta(1-\beta)^{2}\sum_{j=1}^{N}G_{g}^{2}\,\mathbb{E}\left\|z_ {j}^{t}-(f_{\widehat{T}}(\mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] (11)Recall Lemma 6 gives the following lower bound of \(\mathbb{E}[F(\bm{\alpha}^{t+1},\mathbf{w}^{t+1})-F(\bm{\alpha}^{t+1},\mathbf{w}^{t })]\):

\[\mathbb{E}[F(\bm{\alpha}^{t+1},\mathbf{w}^{t+1})-F(\bm{\alpha}^{t+1},\mathbf{w}^{t})]\] (12) \[\geq\left(\frac{1}{2\gamma}-\frac{L}{2}-\frac{L^{2}\eta}{2} \right)\mathbb{E}\left\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\|^{2}-\frac{1}{ 2\eta}\,\mathbb{E}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\|^{2}-\left( 2\gamma G_{g}^{2}+8\beta^{2}L_{g}^{2}G_{\hat{f}}^{2}\gamma\right)\frac{\sigma ^{2}}{B}\] \[-4L_{g}^{2}G_{\hat{f}}^{2}\gamma\sum_{j=1}^{N}\alpha^{t}(j)(1- \beta)^{2}\,\mathbb{E}\left\|z_{j}^{t}-(f_{\hat{\mathcal{T}}}(\mathbf{w}^{t-1 })-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[-16(1-\beta)^{2}L_{g}^{2}G_{\hat{f}}^{4}\gamma\,\mathbb{E}\left\| \mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}.\] (13)

Hence, Adding (11) and (13) yields:

\[\mathbb{E}[F(\bm{\alpha}^{t+1},\mathbf{w}^{t+1})-F(\bm{\alpha}^{ t},\mathbf{w}^{t})]\] \[\geq\left(\frac{1}{2\gamma}-\frac{L}{2}-\frac{L^{2}\eta}{2} \right)\mathbb{E}\left\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\|^{2}-\left(16( 1-\beta)^{2}L_{g}^{2}G_{\hat{f}}^{4}\gamma+\frac{L^{2}\eta}{2}\right)\mathbb{ E}\left\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}\] \[+\left(\frac{\mu}{2}-\frac{3}{2\eta}-\frac{1}{4\eta(1-\beta)^{2} }\right)\mathbb{E}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\|^{2}+\left( \mu-\frac{1}{2\eta}-\frac{\eta L^{2}}{2}\right)\mathbb{E}\left\|\bm{\alpha}^ {t}-\bm{\alpha}^{t-1}\right\|^{2}\] \[-\sum_{j=1}^{N}(1-\beta)^{2}\left(4\alpha^{t}(j)L_{g}^{2}G_{\hat{ f}}^{2}\gamma+\eta G_{g}^{2}\right)\mathbb{E}\left\|z_{j}^{t}-(f_{\hat{\mathcal{T}}}( \mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[-\left(2\gamma G_{g}^{2}+8\beta^{2}L_{g}^{2}G_{\hat{f}}^{2}\gamma \right)\frac{\sigma^{2}}{B},\]

which concludes the proof.

**Lemma 8**.: _For Algorithm 1, under the assumptions of Theorem 2, define the following auxiliary quantity \(\hat{F}^{t+1}\):_

\[\hat{F}^{t+1}:= F(\bm{\alpha}^{t+1},\mathbf{w}^{t+1})+s^{t+1}\] \[-\left(\frac{1}{4\gamma}+4L_{g}^{2}G_{\hat{f}}^{4}\gamma+\frac{ \eta L^{2}}{2}+\frac{192NG_{g}^{2}}{\mu\eta\beta}+\frac{96NG_{g}^{2}G_{\hat{ f}}^{2}\beta}{\mu^{2}\eta}\right)\left\|\mathbf{w}^{t+1}-\mathbf{w}^{t} \right\|^{2}\] \[-\left(\frac{1}{8\gamma}+\frac{96NG_{g}^{2}}{\mu\eta\beta}\right) \left\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}+\left(\frac{7}{2\eta}+\mu- \frac{\eta L^{2}}{2}\right)\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\|^{2}\]

_where_

\[s^{t+1}:=-\frac{2}{\eta^{2}\mu}\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\|^{2}\,.\]

_Then the following statement holds:_

\[\mathbb{E}[\hat{F}^{t+1}-\hat{F}^{t}]\geq C_{1}\,\mathbb{E}\left\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\|^{2}+ \frac{1}{8\gamma}\,\mathbb{E}\left\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^ {2}+\frac{1}{8\gamma}\,\mathbb{E}\left\|\mathbf{w}^{t-1}-\mathbf{w}^{t-2} \right\|^{2}\] \[+C_{2}\,\mathbb{E}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\| ^{2}\] \[-(1-\beta)^{2}\left(4L_{g}^{2}G_{\hat{f}}^{2}\gamma+\eta G_{g}^{2 }\right)\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-(f_{\hat{\mathcal{T}}}( \mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[-\left(2\gamma G_{g}^{2}+8\beta^{2}L_{g}^{2}G_{\hat{f}}^{2}\gamma \right)\frac{\sigma^{2}}{B}-\left(1-\frac{\beta}{2}\right)^{2}\frac{4G_{g}^{2} }{\mu^{2}\eta}\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1}\right\|^{2},\]

_where_

\[C_{1} =\frac{1}{4\gamma}-\frac{L}{2}-\frac{3}{2}\eta L^{2}-16L_{g}^{2}G_ {\hat{f}}^{4}\gamma-\frac{192NG_{g}^{2}G_{\hat{f}}^{2}}{\mu^{2}\eta\beta}- \frac{96NG_{g}^{2}G_{\hat{f}}^{2}\beta}{\mu^{2}\eta},\] \[C_{2} =\frac{1}{\eta}+\frac{3}{2}\mu-\frac{\eta L^{2}}{2}-\frac{1}{4\eta (1-\beta)^{2}}-\frac{2L^{2}}{\mu}.\]Proof.: According to (6) and (7):

\[\frac{1}{\eta}\left\langle\Delta^{t+1},\bm{\alpha}^{t}-\bm{\alpha}^{t+1}\right\rangle \geq\left\langle\mathbf{g}_{\alpha}^{t}-\mathbf{g}_{\alpha}^{t-1},\,\bm{\alpha} ^{t+1}-\bm{\alpha}^{t}\right\rangle.\]

If we define \(\mathbf{g}_{\alpha}(z,\bm{\alpha})=\left[g(z_{1}),...,g(z_{N})\right]+2C\mathbf{ M}\bm{\alpha}\).

\[\frac{1}{\eta}\left\langle\Delta^{t+1},\bm{\alpha}^{t}-\bm{\alpha }^{t+1}\right\rangle \geq\left\langle\mathbf{g}_{\alpha}^{t}-\mathbf{g}_{\alpha}(z^{t},\bm{\alpha}^{t}),\,\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\rangle\] \[\geq\left\langle\mathbf{g}_{\alpha}^{t}-\mathbf{g}_{\alpha}(z^{t},\bm{\alpha}^{t}),\,\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\rangle\] \[\quad+\left\langle\mathbf{g}_{\alpha}(z^{t},\bm{\alpha}^{t})- \mathbf{g}_{\alpha}^{t-1},\,\bm{\alpha}^{t+1}-\bm{\alpha}^{t}-(\bm{\alpha}^{t }-\bm{\alpha}^{t})\right\rangle\] \[\quad+\left\langle\mathbf{g}_{\alpha}(z^{t},\bm{\alpha}^{t})- \mathbf{g}_{\alpha}^{t-1},\,(\bm{\alpha}^{t}-\bm{\alpha}^{t-1})\right\rangle\] \[\geq-\frac{G_{g}^{2}}{\mu}\sum_{j=1}^{N}\left\|z_{j}^{t+1}-z_{j}^ {t}\right\|^{2}-\frac{\mu}{4}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\|^ {2}\] \[\quad-\frac{\eta L^{2}}{2}\left\|\bm{\alpha}^{t}-\bm{\alpha}^{t-1 }\right\|^{2}-\frac{1}{2\eta}\left\|\Delta^{t+1}\right\|^{2}+\mu\left\|\bm{ \alpha}^{t}-\bm{\alpha}^{t-1}\right\|^{2}.\]

Since \(\frac{1}{\eta}\left\langle\Delta^{t+1},\bm{\alpha}^{t}-\bm{\alpha}^{t+1} \right\rangle=\frac{1}{2\eta}\left\|\bm{\alpha}^{t}-\bm{\alpha}^{t-1}\right\|^ {2}-\frac{1}{2\eta}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\|^{2}-\frac{ 1}{2\eta}\left\|\Delta^{t+1}\right\|^{2}\) Hence we have:

\[\frac{1}{2\eta}\left\|\bm{\alpha}^{t}-\bm{\alpha}^{t-1}\right\|^ {2}-\frac{1}{2\eta}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\|^{2} \geq-\frac{G_{g}^{2}}{\mu}\sum_{j=1}^{N}\left\|z_{j}^{t+1}-z_{j}^ {t}\right\|^{2}-\frac{\mu}{4}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\|^ {2}\] \[\quad-\frac{\eta L^{2}}{2}\left\|\bm{\alpha}^{t}-\bm{\alpha}^{t-1 }\right\|^{2}+\mu\left\|\bm{\alpha}^{t}-\bm{\alpha}^{t-1}\right\|^{2}\]

Multiplying both sides with \(\frac{4}{\mu\eta}\) yields:

\[\frac{2}{\mu\eta^{2}}\left\|\bm{\alpha}^{t}-\bm{\alpha}^{t-1} \right\|^{2}-\frac{2}{\mu\eta}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\| ^{2} \geq-\frac{4G_{g}^{2}}{\mu^{2}\eta}\sum_{j=1}^{N}\left\|z_{j}^{t+1 }-z_{j}^{t}\right\|^{2}-\frac{1}{\eta}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t} \right\|^{2}\] \[\quad-\frac{2L^{2}}{\mu}\left\|\bm{\alpha}^{t}-\bm{\alpha}^{t-1} \right\|^{2}+\frac{4}{\eta}\left\|\bm{\alpha}^{t}-\bm{\alpha}^{t-1}\right\|^{2},\] (14)

and so, putting all together,

\[\mathbb{E}[F(\bm{\alpha}^{t+1},\mathbf{w}^{t+1})-F(\bm{\alpha}^{t },\mathbf{w}^{t})]\] \[\geq\left(\frac{1}{2\gamma}-\frac{L}{2}-\frac{L^{2}\eta}{2} \right)\mathbb{E}\left\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\|^{2}-\left(16(1 -\beta)^{2}L_{g}^{2}G_{f}^{4}\gamma+\frac{L^{2}\eta}{2}\right)\mathbb{E} \left\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}\] \[+\left(\frac{\mu}{2}-\frac{3}{2\eta}-\frac{1}{4\eta(1-\beta)^{2} }\right)\mathbb{E}\left\|\bm{\alpha}^{t+1}-\bm{\alpha}^{t}\right\|^{2}+\left( \mu-\frac{1}{2\eta}-\frac{\eta L^{2}}{2}\right)\mathbb{E}\left\|\bm{\alpha}^{t }-\bm{\alpha}^{t-1}\right\|^{2}\] \[\quad-\sum_{j=1}^{N}(1-\beta)^{2}\left(4\alpha^{t}(j)L_{g}^{2}G_ {f}^{2}\gamma+\eta G_{g}^{2}\right)\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{f}}( \mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[\quad-\left(2\gamma G_{g}^{2}+8\beta^{2}L_{g}^{2}G_{f}^{2}\gamma \right)\frac{\sigma^{2}}{B}.\]Now evoking Lemma 7 together with (14) yields:

\[\mathbb{E}[F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^{t+1})+s^{t+1}-(F( \boldsymbol{\alpha}^{t},\mathbf{w}^{t})+s^{t})]\] \[\geq\left(\frac{1}{2\gamma}-\frac{L}{2}-\frac{L^{2}\eta}{2}\right) \mathbb{E}\left\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\|^{2}-\left(16(1-\beta )^{2}L_{g}^{2}G_{f}^{4}\gamma+\frac{L^{2}\eta}{2}\right)\mathbb{E}\left\| \mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}\] \[+\left(\frac{\mu}{2}-\frac{3}{2\eta}-\frac{1}{4\eta(1-\beta)^{2} }-\frac{1}{\eta}\right)\mathbb{E}\left\|\boldsymbol{\alpha}^{t+1}-\boldsymbol {\alpha}^{t}\right\|^{2}+\left(\mu-\frac{1}{2\eta}-\frac{\eta L^{2}}{2}+\frac {4}{\eta}-\frac{2L^{2}}{\mu}\right)\mathbb{E}\left\|\boldsymbol{\alpha}^{t}- \boldsymbol{\alpha}^{t-1}\right\|^{2}\] \[-\sum_{j=1}^{N}(1-\beta)^{2}\left(4L_{g}^{2}G_{f}^{2}\gamma+\eta G _{g}^{2}\right)\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{\tau}}(\mathbf{w}^{t-1 })-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[-\left(2\gamma G_{g}^{2}+8\beta^{2}L_{g}^{2}G_{f}^{2}\gamma \right)\frac{\sigma^{2}}{B}-\frac{4G_{g}^{2}}{\mu^{2}\eta}\sum_{j=1}^{N} \mathbb{E}\left\|z_{j}^{t+1}-z_{j}^{t}\right\|^{2}\;.\]

Now we plug in Lemma 3 with the fact that \(\frac{1}{\beta}\geq 1\) and get:

\[\mathbb{E}[F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^{t+1})+h^{t+1}- (F(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})+h^{t})]\] \[-\left(16L_{g}^{2}G_{f}^{4}\gamma+\frac{L^{2}\eta}{2}+\frac{96NG_ {g}^{2}G_{f}^{2}}{\mu^{2}\eta\beta}+\frac{96NG_{g}^{2}G_{f}^{2}\beta}{\mu^{2} \eta}\right)\mathbb{E}\left\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}- \frac{96NG_{g}^{2}G_{f}^{2}}{\mu^{2}\eta\beta}\,\mathbb{E}\left\|\mathbf{w}^{ t-1}-\mathbf{w}^{t-2}\right\|^{2}\] \[+\left(\frac{\mu}{2}-\frac{5}{2\eta}-\frac{1}{4\eta(1-\beta)^{2} }\right)\mathbb{E}\left\|\boldsymbol{\alpha}^{t+1}-\boldsymbol{\alpha}^{t} \right\|^{2}+\left(\mu+\frac{7}{2\eta}-\frac{\eta L^{2}}{2}-\frac{2L^{2}}{ \mu}\right)\mathbb{E}\left\|\boldsymbol{\alpha}^{t}-\boldsymbol{\alpha}^{t-1 }\right\|^{2}\] \[-(1-\beta)^{2}\left(4L_{g}^{2}G_{f}^{2}\gamma+\eta G_{g}^{2} \right)\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{\tau}}(\mathbf{w }^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[-\left(2\gamma G_{g}^{2}+8\beta^{2}L_{g}^{2}G_{f}^{2}\gamma \right)\frac{\sigma^{2}}{B}-\left(1-\frac{\beta}{2}\right)^{2}\frac{4G_{g}^{ 2}}{\mu^{2}\eta}\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1}\right\|^{2}.\]

Recall our definition of potential function \(\hat{F}^{t+1}\)

\[\hat{F}^{t+1}:= F(\boldsymbol{\alpha}^{t+1},\mathbf{w}^{t+1})+h^{t+1}-\left( \frac{1}{4\gamma}+16L_{g}^{2}G_{f}^{4}\gamma-\frac{\eta L^{2}}{2}+\frac{192NG_ {g}^{2}G_{f}^{2}}{\mu^{2}\eta\beta}+\frac{96NG_{g}^{2}G_{f}^{2}\beta}{\mu^{2} \eta}\right)\left\|\mathbf{w}^{t+1}-\mathbf{w}^{t}\right\|^{2}\] \[-\left(\frac{1}{8\gamma}+\frac{96NG_{g}^{2}G_{f}^{2}}{\mu^{2} \eta\beta}\right)\left\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}+\left( \frac{7}{2\eta}+\mu-\frac{\eta L^{2}}{2}-\frac{2L^{2}}{\mu}\right)\left\| \boldsymbol{\alpha}^{t+1}-\boldsymbol{\alpha}^{t}\right\|^{2}.\]

We conclude that:

\[\mathbb{E}[\hat{F}^{t+1}-\hat{F}^{t}]\geq \left(\frac{1}{4\gamma}-\frac{L}{2}-\frac{3}{2}\eta L^{2}-16L_{g}^ {2}G_{f}^{4}\gamma-\frac{192NG_{g}^{2}G_{f}^{2}}{\mu^{2}\eta\beta}-\frac{96NG_ {g}^{2}G_{f}^{2}\beta}{\mu^{2}\eta}\right)\mathbb{E}\left\|\mathbf{w}^{t+1}- \mathbf{w}^{t}\right\|^{2}\] \[+\frac{1}{8\gamma}\,\mathbb{E}\left\|\mathbf{w}^{t}-\mathbf{w}^{t -1}\right\|^{2}+\frac{1}{8\gamma}\,\mathbb{E}\left\|\mathbf{w}^{t-1}-\mathbf{w}^ {t-2}\right\|^{2}\] \[+\left(\frac{1}{\eta}+\frac{3}{2}\mu-\frac{\eta L^{2}}{2}-\frac{1} {4\eta(1-\beta)^{2}}-\frac{2L^{2}}{\mu}\right)\mathbb{E}\left\|\boldsymbol{ \alpha}^{t+1}-\boldsymbol{\alpha}^{t}\right\|^{2}\] \[-(1-\beta)^{2}\left(4L_{g}^{2}G_{f}^{2}\gamma+\eta G_{g}^{2} \right)\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{\tau}}(\mathbf{w }^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[-\left(2\gamma G_{g}^{2}+8\beta^{2}L_{g}^{2}G_{f}^{2}\gamma\right) \frac{\sigma^{2}}{B}-\left(1-\frac{\beta}{2}\right)^{2}\frac{4G_{g}^{2}}{\mu^{2} \eta}\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1}\right\|^{2}.\]

**Lemma 9**.: _For Algorithm 1, under the assumptions of Theorem 2, define the following auxiliary quantity \(\tilde{F}^{t+1}\):_

\[\tilde{F}^{t+1}:=\hat{F}^{t+1}-\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t+1}-(f_{ \widehat{\mathcal{T}}}(\mathbf{w}^{t})-f_{j}(\mathbf{w}^{t}))\right\|^{2}- \frac{(1-\frac{\beta}{2})^{2}}{1-(1-\frac{\beta}{2})^{2}}\frac{4G_{g}^{2}}{\mu ^{2}\eta}\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t+1}-z_{j}^{t}\right\|^{2}.\]

_Let C1, C2 be defined in Lemma 8. If the following conditions hold_

\[\frac{1}{8\gamma}-4(4C_{1}\gamma^{2}L_{g}^{2}N+C_{2}\eta^{2}G_{g}^ {2}N^{2}+N)(1-\beta)^{2}G_{f}^{2}-\frac{(1-\frac{\beta}{2})^{2}}{1-(1-\frac{ \beta}{2})^{2}}\frac{4G_{g}^{2}}{\mu^{2}\eta}\frac{48}{\beta}G_{f}^{2}\geq 0,\] (15) \[1-\left(4C_{1}\gamma^{2}L_{g}^{2}+C_{2}\eta^{2}G_{g}^{2}+4L_{g} ^{2}G_{f}^{2}\gamma+\eta G_{g}^{2}+1\right)(1-\beta)^{2}\geq 0,\] (16) \[\frac{1}{8\gamma}-\frac{(1-\frac{\beta}{2})^{2}}{1-(1-\frac{ \beta}{2})^{2}}\frac{4G_{g}^{2}}{\mu^{2}\eta}\frac{24}{\beta}G_{f}^{2}\geq 0\;,\] (17)

_then the following statement hold:_

\[\mathbb{E}[\tilde{F}^{t+1}-\tilde{F}^{t}]\] \[-\left(2C_{1}\gamma^{2}L_{g}^{2}N\beta^{2}+2C_{2}\eta^{2}G_{g}^{2 }N^{2}\beta^{2}+4\gamma G_{g}^{2}+2\beta^{2}L_{g}^{2}G_{f}^{2}\gamma+2\beta^{2 }\right)\frac{\sigma^{2}}{B}\;.\]

Proof.: According to Lemma 8:

\[\mathbb{E}[\hat{F}^{t+1}-\hat{F}^{t}]\] \[\geq C_{1}\,\mathbb{E}\left\|\mathbf{w}^{t+1}-\mathbf{w}^{t} \right\|^{2}+\frac{1}{8\gamma}\,\mathbb{E}\left\|\mathbf{w}^{t}-\mathbf{w}^{t- 1}\right\|^{2}+\frac{1}{8\gamma}\,\mathbb{E}\left\|\mathbf{w}^{t-1}-\mathbf{w} ^{t-2}\right\|^{2}+C_{2}\,\mathbb{E}\left\|\mathbf{\alpha}^{t+1}-\mathbf{ \alpha}^{t}\right\|^{2}\] \[-\left(1-\beta\right)^{2}\left(4L_{g}^{2}G_{f}^{2}\gamma+\eta G_ {g}^{2}\right)\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{\mathcal{ T}}}(\mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[-\left(2\gamma G_{g}^{2}+8\beta^{2}L_{g}^{2}G_{f}^{2}\gamma \right)\frac{\sigma^{2}}{B}-\left(1-\frac{\beta}{2}\right)^{2}\frac{4G_{g}^{ 2}}{\mu^{2}\eta}\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1}\right\|^{ 2},\]

where

\[C_{1} =\frac{1}{4\gamma}-\frac{L}{2}-\frac{3}{2}\eta L^{2}-16L_{g}^{2} G_{f}^{4}\gamma-\frac{192NG_{g}^{2}G_{f}^{2}}{\mu^{2}\eta\beta}-\frac{96NG_{g}^{2}G_{ f}^{2}\beta}{\mu^{2}\eta},\] \[C_{2} =\frac{1}{\eta}+\frac{3}{2}\mu-\frac{\eta L^{2}}{2}-\frac{1}{4 \eta(1-\beta)^{2}}-\frac{2L^{2}}{\mu}.\]

Now we plug in Lemma 5:

\[\mathbb{E}[\hat{F}^{t+1}-\hat{F}^{t}]\] \[\geq C_{1}\left(\frac{1}{2}\gamma^{2}\,\mathbb{E}\left\|\nabla_{ \mathbf{w}}G(\mathbf{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}-2\gamma^{2}G_{g} ^{2}\frac{\sigma^{2}}{B}-4\gamma^{2}G_{f}^{2}L_{g}^{2}\sum_{j=1}^{N}\alpha^{t }(j)\,\mathbb{E}\left\|z_{j}^{t+1}-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t})- \nabla f_{j}(\mathbf{w}^{t}))\right\|^{2}\right)\] \[+C_{2}\left(\frac{1}{2}\eta^{2}\,\mathbb{E}\left\|\nabla_{ \mathbf{\alpha}}G(\mathbf{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}-\eta^{2}G_{g }^{2}\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t+1}-(f_{\widehat{\mathcal{T}}}( \mathbf{w}^{t})-f_{j}(\mathbf{w}^{t}))\right\|^{2}\right)\] \[-(1-\beta)^{2}\left(4L_{g}^{2}G_{f}^{2}\gamma+\eta G_{g}^{2} \right)\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{\mathcal{T}}}( \mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}-\left(2\gamma G_{g}^{2} +8\beta^{2}L_{g}^{2}G_{f}^{2}\gamma\right)\frac{\sigma^{2}}{B}\] \[-\left(1-\frac{\beta}{2}\right)^{2}\frac{4G_{g}^{2}}{\mu^{2}\eta} \sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1}\right\|^{2}+\frac{1}{8 \gamma}\,\mathbb{E}\left\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}+\frac{1} {8\gamma}\,\mathbb{E}\left\|\mathbf{w}^{t-1}-\mathbf{w}^{t-2}\right\|^{2}\]Plugging in Lemma 2 yields:

\[\mathbb{E}[\hat{F}^{t+1}-\hat{F}^{t}]\geq\frac{C_{1}}{2}\gamma^{2} \,\mathbb{E}\left\|\nabla_{\mathbf{w}}G(\boldsymbol{\alpha}^{t},\mathbf{w}^{t} )\right\|^{2}+C_{2}\frac{1}{2}\eta^{2}\,\mathbb{E}\left\|\nabla_{\boldsymbol {\alpha}}G(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}\] \[-C_{1}4\gamma^{2}G_{f}^{2}L_{g}^{2}\sum_{j=1}^{N}\alpha^{t}(j) \left((1-\beta)^{2}\,\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{\mathcal{T}}}( \mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}+4(1-\beta)^{2}G_{f}^{2} \left\|\mathbf{w}^{t}-\mathbf{w}^{t-1}\right\|^{2}+2\beta^{2}\frac{\sigma^{2}} {B}\right)\] \[-C_{2}\eta^{2}G_{g}^{2}\sum_{j=1}^{N}\left((1-\beta)^{2}\, \mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t-1})-f_{j} (\mathbf{w}^{t-1}))\right\|^{2}+4(1-\beta)^{2}G_{f}^{2}\left\|\mathbf{w}^{t}- \mathbf{w}^{t-1}\right\|^{2}+2\beta^{2}\frac{\sigma^{2}}{B}\right)\] \[-(1-\beta)^{2}\left(4L_{g}^{2}G_{f}^{2}\gamma+\eta G_{g}^{2} \right)\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-(f_{\widehat{\mathcal{T}}}( \mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1}))\right\|^{2}\] \[-\left(2\gamma G_{g}^{2}+8\beta^{2}L_{g}^{2}G_{f}^{2}\gamma+2C_{ 1}\gamma^{2}G_{g}^{2}\right)\frac{\sigma^{2}}{B}\] \[-\left(1-\frac{\beta}{2}\right)^{2}\frac{4G_{g}^{2}}{\mu^{2}\eta }\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1}\right\|^{2}.\]

Rearranging the terms yields:

\[\mathbb{E}[\hat{F}^{t+1}-\hat{F}^{t}]\] \[\geq\frac{C_{1}}{2}\gamma^{2}\,\mathbb{E}\left\|\nabla_{\mathbf{ w}}G(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}+\frac{C_{2}}{2}\eta^{2}\, \mathbb{E}\left\|\nabla_{\boldsymbol{\alpha}}G(\boldsymbol{\alpha}^{t}, \mathbf{w}^{t})\right\|^{2}\] \[\quad+\left(\frac{1}{8\gamma}-(4C_{1}\gamma^{2}L_{g}^{2}+C_{2} \eta^{2}G_{g}^{2}N)4(1-\beta)^{2}G_{f}^{2}\right)\mathbb{E}\left\|\mathbf{w}^ {t}-\mathbf{w}^{t-1}\right\|^{2}\] \[\quad-\left(4C_{1}\gamma^{2}L_{g}^{2}+C_{2}\eta^{2}G_{g}^{2}+4L_{ g}^{2}G_{f}^{2}\gamma+\eta G_{g}^{2}\right)(1-\beta)^{2}\sum_{j=1}^{N}\mathbb{E} \left\|z_{j}^{t}-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{t-1})-f_{j}(\mathbf{ w}^{t-1}))\right\|^{2}\] \[\quad+\frac{1}{8\gamma}\,\mathbb{E}\left\|\mathbf{w}^{t-1}-\mathbf{ w}^{t-2}\right\|^{2}-\left(8C_{1}\gamma^{2}G_{f}^{2}L_{g}^{2}\beta^{2}+2C_{2} \eta^{2}G_{g}^{2}\beta^{2}+2\gamma G_{g}^{2}+8\gamma G_{f}^{2}L_{g}^{2}\beta ^{2}+2C_{1}\gamma^{2}G_{g}^{2}\right)\frac{\sigma^{2}}{B}\] \[\quad-\left(1-\frac{\beta}{2}\right)^{2}\frac{4G_{g}^{2}}{\mu^{2} \eta}\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t}-z_{j}^{t-1}\right\|^{2}.\]

Recall our definition of potential function \(\tilde{F}^{t+1}\):

\[\tilde{F}^{t+1}:=\hat{F}^{t+1}-\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t+1}-(f_{ \widehat{\mathcal{T}}}(\mathbf{w}^{t})-f_{j}(\mathbf{w}^{t}))\right\|^{2}- \frac{(1-\frac{\beta}{2})^{2}}{1-(1-\frac{\beta}{2})^{2}}\frac{4G_{g}^{2}}{\mu ^{2}\eta}\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{t+1}-z_{j}^{t}\right\|^{2}\]

[MISSING_PAGE_EMPTY:26]

then we can have the 'clean' bound:

\[\mathbb{E}[\tilde{F}^{t+1}-\tilde{F}^{t}] \geq\frac{C_{1}}{2}\gamma^{2}\,\mathbb{E}\left\|\nabla_{\mathbf{w}} G(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}+\frac{C_{2}}{2}\eta^{2}\, \mathbb{E}\left\|\nabla_{\boldsymbol{\alpha}}G(\boldsymbol{\alpha}^{t}, \mathbf{w}^{t})\right\|^{2}\] \[\quad-\left(2C_{1}\gamma^{2}L_{g}^{2}N\beta^{2}+2C_{2}\eta^{2}G_ {g}^{2}N^{2}\beta^{2}+4\gamma G_{g}^{2}+2\beta^{2}L_{g}^{2}G_{f}^{2}\gamma+2 \beta^{2}\right)\frac{\sigma^{2}}{B}\,.\]

### Proof of theorem 2

Summing the inequality from \(t=1\) to \(T\) yields:

\[\frac{\mathbb{E}[\tilde{F}^{T}-\tilde{F}^{0}]}{T}+\left(2C_{1} \gamma^{2}L_{g}^{2}N\beta^{2}+2C_{2}\eta^{2}G_{g}^{2}N^{2}\beta^{2}+4\gamma G _{g}^{2}+2\beta^{2}L_{g}^{2}G_{f}^{2}\gamma+2\beta^{2}\right)\frac{\sigma^{2} }{B}\] \[\geq\min\left\{\frac{C_{1}}{2}\gamma^{2},\frac{C_{2}}{2}\eta^{2} \right\}\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left\|\nabla_{\mathbf{w}}G( \boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}+\mathbb{E}\left\|\nabla_{ \boldsymbol{\alpha}}G(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}\]

We compute the upper and lower bound of \(C_{1}\) and \(C_{2}\). For \(C_{1}\)

\[C_{1}=\frac{1}{4\gamma}-\frac{L}{2}-\frac{3}{2}\eta L^{2}-16L_{g}^{2}G_{f}^{4} \gamma-\frac{192NG_{g}^{2}G_{f}^{2}}{\mu^{2}\eta\beta}-\frac{96NG_{g}^{2}G_{f }^{2}\beta}{\mu^{2}\eta}\]

The upper bound \(C_{1}\leq\frac{1}{4\gamma}\) holds trivially. For lower bound, since we choose

\[\gamma=\min\left\{\frac{1}{20L},\frac{1}{60\eta L^{2}},\frac{1}{8\sqrt{10}L_{ g}G_{f}^{2}},\frac{\mu^{2}\eta\beta}{7680NG_{g}^{2}G_{f}^{2}}\right\}\]

we know that \(C_{1}\geq\frac{1}{8\gamma}\).

For \(C_{2}\):

\[C_{2}=\frac{1}{\eta}+\frac{3}{2}\mu-\frac{\eta L^{2}}{2}-\frac{1}{4\eta(1- \beta)^{2}}-\frac{2L^{2}}{\mu}.\]

The upper bound \(C_{2}\leq\frac{1}{\eta}+\frac{3\mu}{2}\) holds trivially. For lower bound, since we choose:

\[\eta=\min\left\{\frac{\sqrt{2}}{3L},\frac{\mu}{36L^{2}}\right\},\beta=0.1\leq 1 -\frac{\sqrt{3}}{2}\]

it holds that \(C_{2}\geq\frac{1}{2\eta}\).

Since \(\frac{1}{8\gamma}\leq C_{1}\leq\frac{1}{4\gamma}\) and \(\frac{1}{2\eta}\leq C_{2}\leq\frac{1}{\eta}+\frac{3\mu}{2}\leq\frac{2}{\eta}\), we have:

\[\frac{\mathbb{E}[\tilde{F}^{T}-\tilde{F}^{0}]}{T}+\left(\frac{1}{ 2}\gamma L_{g}^{2}N\beta^{2}+\frac{4}{\eta}\eta^{2}G_{g}^{2}N^{2}\beta^{2}+4 \gamma G_{g}^{2}+2\beta^{2}L_{g}^{2}G_{f}^{2}\gamma+2\beta^{2}\right)\frac{ \sigma^{2}}{B}\] (18) \[\geq\min\left\{\frac{1}{16}\gamma,\frac{1}{4}\eta\right\}\frac{ 1}{T}\sum_{t=0}^{T-1}\mathbb{E}\left\|\nabla_{\mathbf{w}}G(\boldsymbol{\alpha} ^{t},\mathbf{w}^{t})\right\|^{2}+\mathbb{E}\left\|\nabla_{\boldsymbol{ \alpha}}G(\boldsymbol{\alpha}^{t},\mathbf{w}^{t})\right\|^{2}\] (19)

We then need to verify the conditions (15), (16) and (17) in Lemma 9 can hold under our choice of \(\eta_{\mathbf{x}}\), \(\eta_{\mathbf{y}}\) and \(\beta\). To guarantee (15) holding, we need:

\[\gamma\leq\min\left\{\sqrt{\frac{1}{128L_{g}^{2}N}},\frac{1}{52G_{f}^{2} \left(2\eta G_{g}^{2}N^{2}+N\right)},\frac{\mu^{2}\eta}{307200G_{g}^{2}G_{f}^{2} }\right\}.\]To guarantee (16) holding, we need:

\[\gamma\leq\min\left\{\frac{1}{50L_{g}^{2}},\frac{1}{200L_{g}^{2}G_{f}^{2}}\right\},\eta\leq\frac{1}{100G_{g}^{2}}.\]

To guarantee (17) holding, we need:

\[\gamma\leq\frac{\mu^{2}\eta}{1080G_{g}^{2}G_{f}^{2}}\]

Next we examine how large the \(\mathbb{E}[\tilde{F}^{T}-\tilde{F}^{0}]\) is. By definition of potential function, we have:

\[\mathbb{E}[\tilde{F}^{T}-\tilde{F}^{0}]\] \[=\hat{F}^{T}-\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{T}-(f_{\widehat {\mathcal{T}}}(\mathbf{w}^{T-1})-f_{j}(\mathbf{w}^{T-1}))\right\|^{2}-\frac{(1 -\frac{\beta}{2})^{2}}{1-(1-\frac{\beta}{2})^{2}}\frac{4G_{g}^{2}}{\mu^{2}\eta }\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{T}-z_{j}^{T-1}\right\|^{2}\] \[-\left(\hat{F}^{0}-\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{0}-(f_{ \widehat{\mathcal{T}}}(\mathbf{w}^{-1})-f_{j}(\mathbf{w}^{-1}))\right\|^{2}- \frac{(1-\frac{\beta}{2})^{2}}{1-(1-\frac{\beta}{2})^{2}}\frac{4G_{g}^{2}}{ \mu^{2}\eta}\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{0}-z_{j}^{-1}\right\|^{2}\right)\] \[\leq\hat{F}^{T}-\hat{F}^{0}+\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^ {0}-(f_{\widehat{\mathcal{T}}}(\mathbf{w}^{-1})-f_{j}(\mathbf{w}^{-1}))\right\| ^{2}+\frac{(1-\frac{\beta}{2})^{2}}{\beta-\frac{\beta^{2}}{4}}\frac{4G_{g}^{2} }{\mu^{2}\eta}\sum_{j=1}^{N}\mathbb{E}\left\|z_{j}^{0}-z_{j}^{-1}\right\|^{2}.\]

By our choice \(z_{j}^{0}=z_{j}^{-1}=f_{\widehat{\mathcal{T}}}(\mathbf{w}^{-1})-f_{j}(\mathbf{ w}^{-1})\), \(\mathbf{w}^{0}=\mathbf{w}^{-1}\), we have

\[\mathbb{E}[\tilde{F}^{T}-\tilde{F}^{0}]\leq\hat{F}^{T}-\hat{F}^{0}\]

Next we examine how large the \(\mathbb{E}[\hat{F}^{T}-\hat{F}^{0}]\) is.

\[\mathbb{E}[\hat{F}^{T}-\hat{F}^{0}]\] \[=F(\bm{\alpha}^{T},\mathbf{w}^{T})+s^{T}-\left(\frac{1}{4\gamma}+ 4L_{g}^{2}G_{f}^{4}\gamma+\frac{\eta L^{2}}{2}+\frac{192NG_{g}^{2}}{\mu^{2} \eta\beta}+\frac{96NG_{g}^{2}G_{f}^{2}\beta}{\mu^{2}\eta}\right)\left\| \mathbf{w}^{T}-\mathbf{w}^{T-1}\right\|^{2}\] \[\quad-F(\bm{\alpha}^{0},\mathbf{w}^{0})-s^{0}+\left(\frac{1}{4 \gamma}+4L_{g}^{2}G_{f}^{4}\gamma+\frac{\eta L^{2}}{2}+\frac{192NG_{g}^{2}}{ \mu^{2}\eta\beta}+\frac{96NG_{g}^{2}G_{f}^{2}\beta}{\mu^{2}\eta}\right)\left\| \mathbf{w}^{0}-\mathbf{w}^{-1}\right\|^{2}\] \[\quad+\left(\frac{1}{8\gamma}+\frac{96NG_{g}^{2}}{\mu^{2}\eta \beta}\right)\left\|\mathbf{w}^{-1}-\mathbf{w}^{-2}\right\|^{2}-\left(\frac{ 7}{2\eta}+\mu-\frac{\eta L^{2}}{2}\right)\left\|\bm{\alpha}^{0}-\bm{\alpha}^{ -1}\right\|^{2}\] \[\leq F_{\max}+\left(\frac{7}{2\eta}+\mu-\frac{\eta L^{2}}{2} \right)\left\|\bm{\alpha}^{T}-\bm{\alpha}^{T-1}\right\|^{2}\]

Notice that \(\bm{\alpha}^{-1}=\bm{\alpha}^{0}\), \(\mathbf{w}^{0}=\mathbf{w}^{-1}=\mathbf{w}^{-2}\) we have

\[\mathbb{E}[\hat{F}^{T}-\hat{F}^{0}]\leq F_{\max}+\left(\frac{7}{2\eta}+\mu- \frac{\eta L^{2}}{2}\right)\left\|\bm{\alpha}^{T}-\bm{\alpha}^{T-1}\right\|^{2}\]

According to updating rule,\[\mathbb{E}\left\|\boldsymbol{\alpha}^{T}-\boldsymbol{\alpha}^{T-1}\right\|^ {2} =\mathbb{E}\left\|\eta\mathbf{g}_{\boldsymbol{\alpha}}^{T-1}\right\|^ {2}\leq 2\eta^{2}\,\mathbb{E}\left\|\nabla_{\boldsymbol{\alpha}}F(\boldsymbol{ \alpha}^{t-1},\mathbf{w}^{t-1})\right\|^{2}+2\eta^{2}\sum_{j=1}^{N}G_{g}^{2} \left\|z_{j}^{t}-(f_{\tilde{T}}(\mathbf{w}^{t-1})-f_{j}(\mathbf{w}^{t-1})) \right\|^{2}\] \[\leq 2\eta^{2}(2NB_{g}^{2}+2L^{2})+2\eta^{2}NG_{g}^{2}\left(4 \frac{(1-\beta)^{4}}{1-(1-\beta)^{2}}\gamma^{2}G_{f}^{4}G_{g}^{2}+2\beta^{2} \frac{(1-\beta)^{2}}{1-(1-\beta)^{2}}\frac{\sigma^{2}}{B}\right)\] \[\leq 4\eta^{2}(NB_{g}^{2}+L^{2})+2\eta^{2}NG_{g}^{2}\left(14 \gamma^{2}G_{f}^{4}G_{g}^{2}+2\frac{\sigma^{2}}{B}\right)\]

Putting pieces together we can have the upper bound of \(\mathbb{E}[\tilde{F}^{T+1}-\tilde{F}^{1}]\):

\[\mathbb{E}[\tilde{F}^{T+1}-\tilde{F}^{1}]\leq F_{\max}+\frac{9}{2\eta}\left(4 \eta^{2}(NB_{g}^{2}+L^{2})+2\eta^{2}NG_{g}^{2}\left(14\gamma^{2}G_{f}^{4}G_{g }^{2}+2\frac{\sigma^{2}}{B}\right)\right)\]

Plugging above bound back to (19) yields:

\[\frac{1}{T}\sum_{t=1}^{T}\left\|\nabla G(\boldsymbol{\alpha}^{t}, \mathbf{w}^{t})\right\|^{2}\] \[\leq\max\left\{\frac{16}{\gamma},\frac{4}{\eta}\right\}\cdot \frac{F_{\max}+\frac{9}{2}\left(4\eta(NB_{g}^{2}+L^{2})+2\eta NG_{g}^{2}\left( 14\gamma^{2}G_{f}^{4}G_{g}^{2}+2\frac{\sigma^{2}}{B}\right)\right)}{T}\] \[\quad+\max\left\{\frac{16}{\gamma},\frac{4}{\eta}\right\}\cdot \left(\frac{1}{2}\gamma L_{g}^{2}N\beta^{2}+4\eta G_{g}^{2}N^{2}\beta^{2}+4 \gamma G_{g}^{2}+2\beta^{2}L_{g}^{2}G_{f}^{2}\gamma+2\beta^{2}\right)\frac{ \sigma^{2}}{B}\] \[\leq O\left(\frac{1}{\eta}\cdot\frac{F_{\max}+\left(\eta(NB_{g}^{ 2}+L^{2})+\eta NG_{g}^{2}\left(\gamma^{2}G_{f}^{4}G_{g}^{2}+\frac{\sigma^{2}}{B }\right)\right)}{T}\right)\] \[\quad+O\left(\frac{1}{\eta}\cdot\left(\gamma L_{g}^{2}N\beta^{2}+ \eta G_{g}^{2}N^{2}\beta^{2}+\gamma G_{g}^{2}+\beta^{2}L_{g}^{2}G_{f}^{2} \gamma+2\beta^{2}\right)\frac{\sigma^{2}}{B}\right)\]

Define \(\kappa=L/\mu\). Recall that we choose:

\[\eta=\Theta\left(\frac{1}{\kappa L}\right),\gamma=\Theta\left(\min\left\{ \frac{1}{L},\frac{1}{L_{g}G_{f}^{2}},\frac{\mu^{2}}{NG_{g}^{2}G_{f}^{2}\kappa L }\right\}\right)\]

To guarantee RHS is less than \(\epsilon^{2}\), we need:

\[T=\Omega\left(\max\left\{\frac{F_{\max}}{\eta\epsilon^{2}},\frac{NB_{g}^{2}}{ \epsilon^{2}}\right\}\right),B=\Theta\left(\max\left\{\frac{\mu^{2}L_{g}^{2} \sigma^{2}}{G_{g}^{2}G_{f}^{2}\epsilon^{2}},\frac{G_{g}^{2}N\sigma^{2}}{ \epsilon^{2}},\frac{\mu^{2}}{NG_{f}^{2}\epsilon^{2}},\frac{\mu^{2}L_{g}^{2} \sigma^{2}}{NG_{g}^{2}\epsilon^{2}},\frac{\kappa L\sigma^{2}}{\epsilon^{2}} \right\}\right),\]

which yields the total gradient complexity:

\[O\left(\frac{\kappa LF_{\max}}{\epsilon^{2}}\cdot\max\left\{\frac{\kappa L \sigma^{2}}{\epsilon^{2}},\frac{\mu^{2}L_{g}^{2}\sigma^{2}}{G_{g}^{2}G_{f}^{2} \epsilon^{2}},\frac{G_{g}^{2}N\sigma^{2}}{\epsilon^{2}},1\right\}\right).\]Proofs for \(\mathbf{w}^{*}\) Approximation Algorithm

### Proof of Lemma 1

Proof.: First, we define \(\Phi(\bm{\alpha},\mathbf{w})=\sum_{j=1}^{N}\alpha(j)\cdot f_{j}(\mathbf{w})\). Indeed, \(\Phi(\cdot,\mathbf{w})\) is \(\sqrt{N}G_{f}\) smooth for all \(\mathbf{w}\in\mathcal{W}\). To see this, we consider

\[\left\|\nabla_{\mathbf{w}}\Phi(\bm{\alpha},\mathbf{w})-\nabla_{ \mathbf{w}}\Phi(\bm{\alpha}^{\prime},\mathbf{w})\right\| =\left\|\sum_{i=1}^{N}\alpha_{i}\nabla f_{i}(\mathbf{w})-\sum_{i=1 }^{N}\alpha_{i}^{\prime}\nabla f_{i}(\mathbf{w})\right\|\] \[=\left\|\sum_{i=1}^{N}(\alpha_{i}-\alpha_{i}^{\prime})\nabla f_{i }(\mathbf{w})\right\|\] \[\leq\sum_{i=1}^{N}|\alpha_{i}-\alpha_{i}^{\prime}|\max_{i\in[N]} \left\|\nabla f_{i}(\mathbf{w})\right\|\] \[\leq\left\|\bm{\alpha}-\bm{\alpha}^{\prime}\right\|_{1}G_{f}\] \[\leq\sqrt{N}G_{f}\left\|\bm{\alpha}-\bm{\alpha}^{\prime}\right\|.\]

According to optimality conditions we have:

\[\left\langle\mathbf{w}-\mathbf{w}^{*}(\bm{\alpha}),\nabla_{ \mathbf{w}}\Phi(\bm{\alpha},\mathbf{w}^{*}(\bm{\alpha}))\right\rangle\geq 0,\] \[\left\langle\mathbf{w}-\mathbf{w}^{*}(\bm{\alpha}^{\prime}), \nabla_{\mathbf{w}}\Phi(\bm{\alpha}^{\prime},\mathbf{w}^{*}(\bm{\alpha}^{ \prime}))\right\rangle\geq 0\]

Substituting \(\mathbf{v}\) with \(\mathbf{w}^{*}(\bm{\alpha}^{\prime})\) and \(\mathbf{w}^{*}(\bm{\alpha})\) in the above first and second inequalities respectively yields:

\[\left\langle\mathbf{w}^{*}(\bm{\alpha}^{\prime})-\mathbf{w}^{*}( \bm{\alpha}),\nabla_{\mathbf{w}}\Phi(\bm{\alpha},\mathbf{w}^{*}(\bm{\alpha})) \right\rangle\geq 0,\] \[\left\langle\mathbf{w}^{*}(\bm{\alpha})-\mathbf{w}^{*}(\bm{ \alpha}^{\prime}),\nabla_{\mathbf{w}}\Phi(\bm{\alpha}^{\prime},\mathbf{w}^{*}( \bm{\alpha}^{\prime}))\right\rangle\geq 0\]

Adding up the above two inequalities yields:

\[\left\langle\mathbf{w}^{*}(\bm{\alpha}^{\prime})-\mathbf{w}^{*}( \bm{\alpha}),\nabla_{\mathbf{w}}\Phi(\bm{\alpha},\mathbf{w}^{*}(\bm{\alpha})) -\nabla_{\mathbf{w}}\Phi(\bm{\alpha}^{\prime},\mathbf{w}^{*}(\bm{\alpha}^{ \prime}))\right\rangle\geq 0,\] (20)

Since \(\Phi(\bm{\alpha},\cdot)\) is \(\mu\) strongly convex, we have:

\[\left\langle\mathbf{w}^{*}(\bm{\alpha}^{\prime})-\mathbf{w}^{*}( \bm{\alpha}),\nabla_{\mathbf{w}}\Phi(\bm{\alpha},\mathbf{w}^{*}(\bm{\alpha}^{ \prime}))-\nabla_{\mathbf{w}}\Phi(\bm{\alpha},\mathbf{w}^{*}(\bm{\alpha})) \geq\mu\|\mathbf{w}^{*}(\bm{\alpha}^{\prime})-\mathbf{w}^{*}(\bm{\alpha})\|^ {2}.\] (21)

Adding up (20) and (21) yields:

\[\left\langle\mathbf{w}^{*}(\bm{\alpha}^{\prime})-\mathbf{w}^{*}( \bm{\alpha}),\nabla_{\mathbf{w}}\Phi(\bm{\alpha},\mathbf{w}^{*}(\bm{\alpha}^{ \prime}))-\nabla_{\mathbf{w}}\Phi(\bm{\alpha}^{\prime},\mathbf{w}^{*}(\bm{ \alpha}^{\prime}))\geq\mu\|\mathbf{w}^{*}(\bm{\alpha}^{\prime})-\mathbf{w}^{*} (\bm{\alpha})\|^{2}\]

Finally, using \(\sqrt{N}G_{f}\) smoothness of \(\Phi\) will conclude the proof:

\[\sqrt{N}G_{f}\|\mathbf{w}^{*}(\bm{\alpha}^{\prime})-\mathbf{w}^{ *}(\bm{\alpha})\|\|\bm{\alpha}-\bm{\alpha}^{\prime}\| \geq\mu\|\mathbf{w}^{*}(\bm{\alpha}^{\prime})-\mathbf{w}^{*}(\bm{ \alpha})\|^{2}\] \[\iff\kappa^{*}\|\bm{\alpha}-\bm{\alpha}^{\prime}\| \geq\|\mathbf{w}^{*}(\bm{\alpha}^{\prime})-\mathbf{w}^{*}(\bm{ \alpha})\|.\]

### Preliminaries of the proof of theorem 3

In this section, we introduce some notational preliminaries for the proof for Theorem 3. Following the framework in [21], we introduce three models, neural network predictor, random feature predictor and kernel least square predictor. In the following sections we will use \(w(j)\) to denote the \(j\)th coordinate of vector \(\mathbf{w}\).

Two-layer ReLU neural network (NN) predictorRecall that in Section 3.1 we consider a two layer vector-valued neural network \(\mathbf{h}_{\bm{\theta}}:\mathbb{R}^{N}\rightarrow\mathbb{R}^{d}\):

\[\mathbf{h}_{\bm{\theta}}(\mathbf{x})=[\mathbf{a}_{1}^{\top}(\mathbf{U}^{1} \mathbf{x})_{+},...,\mathbf{a}_{d}^{\top}(\mathbf{U}^{d}\mathbf{x})_{+}]\:,\]

where parameters of the _hidden layer_ are matrices \(\mathbf{U}^{i}\in\mathbb{R}^{m\times N}\), collectively captured by the parameter vector \(\bm{\theta}=(\mathrm{vec}(\mathbf{U}^{1}),\ldots,\mathrm{vec}(\mathbf{U}^{d})) \in\mathbb{R}^{dmN}\), and we also define \(\bm{\theta}^{j}=\mathrm{vec}(\mathbf{U}^{j})\in\mathbb{R}^{Nm}\). Here \(\mathbf{a}_{i}\in\{\pm 1/\sqrt{m}\}^{m}\) are parameters of the _output layer_.

Next, induce the Neural Tangent Feature (NTF) operator, defined as

\[\bm{\phi}_{t}^{j}=\begin{pmatrix}a_{j}(1)\mathbb{I}\{{\bm{u}_{t}^{j}}^{\top}(1) \bm{\alpha}\}\bm{\alpha}\\ \vdots\\ a_{j}(m)\mathbb{I}\{{\bm{u}_{t}^{j}}^{\top}(m)\bm{\alpha}\}\bm{\alpha}\end{pmatrix} \in\mathbb{R}^{dm}\qquad(j,t\in\mathbb{N})\]

where \({\bm{u}_{t}^{j}}^{\top}(r)\) is \(r\)th row of \(\bm{\UpUpUp}_{t}^{j}\). Note that we dropped dependence on \(\bm{\alpha}\) on the left hand side -- it is always assumed that the operator is applied to variable \(\bm{\alpha}\). Also, note that the operator comes by differentiation of the neural network with respect to a \(j\)-th component of the hidden layer, that is \(\bm{\phi}_{t}^{j}(\bm{\alpha})=\nabla_{\bm{\theta}}.\mathbf{h}_{\bm{\theta}}( \bm{\alpha})|_{\bm{\theta}^{j}=\bm{\theta}_{t}^{j}}\). For convenience, we also introduce the following NTF matrix notation, given multiple input vectors \(\bm{\alpha}_{1},\ldots,\bm{\alpha}_{n}\):

\[\bm{\Phi}_{t}^{j}=\left[\bm{\phi}_{t}^{j}(\bm{\alpha}_{1}),\ldots,\bm{\phi}_{ t}^{j}(\bm{\alpha}_{n})\right]\in\mathbb{R}^{dm\times n}\;.\]

Finally, it is not hard to see that the use of NTF operator recovers the neural network predictor:

\[\mathbf{h}_{t}(\bm{\alpha})=\begin{bmatrix}\bm{\phi}_{t}^{1}(\bm{\alpha})^{ \top}\bm{\theta}_{t}^{1}\\ \vdots\\ \bm{\phi}_{t}^{d}(\bm{\alpha})^{\top}\bm{\theta}_{t}^{d}\end{bmatrix}\]

Using the above definitions, we can write least squares objective as

\[\widehat{\mathcal{R}}(\bm{\theta}_{t})=\frac{1}{n}\sum_{i=1}^{n}\left(\mathbf{ h}_{t}(\bm{\alpha}_{i})-\mathbf{w}^{*}(\bm{\alpha}_{i})\right)^{2}=\frac{1}{n} \sum_{i=1}^{n}\sum_{j=1}^{d}\left(\bm{\phi}_{t}^{j}{}^{\top}(\bm{\alpha}_{i}) \bm{\theta}_{t}^{j}-w^{*}(\bm{\alpha}_{i})(j)\right)^{2}\;,\]

and similarly for the pointwise loss function,

\[\widehat{\mathcal{R}}^{j}(\bm{\theta}_{t}^{j})=\frac{1}{n}\sum_{i=1}^{n} \left(h_{t}^{j}(\bm{\alpha}_{i})-w^{*}(\bm{\alpha}_{i})(j)\right)^{2}=\frac{1 }{n}\sum_{i=1}^{n}\left(\bm{\phi}_{t}^{j}{}^{\top}(\bm{\alpha}_{i})\bm{\theta }_{t}^{j}-w^{*}(\bm{\alpha}_{i})(j)\right)^{2}\;,\]

and for the GD update rule:

\[\bm{\theta}_{t+1}^{j}=\bm{\theta}_{t}^{j}-\eta\mathbf{g}_{t}^{j},\text{where }\mathbf{g}_{t}^{j}=\frac{1}{n}\sum_{i=1}^{n}\bm{\phi}_{t}^{j}(\bm{\alpha}_{i })(\mathbf{h}_{t}^{j}(\bm{\alpha}_{i})-w_{i}^{t}(j))\;.\]

Neural Tangent Feature (NTF) predictorFor parameter vector \(\bar{\bm{\theta}}_{t}^{j}\in\mathbb{R}^{Nd}\) and \(\bar{\bm{\theta}}_{t}=(\bar{\bm{\theta}}_{t}^{1},\ldots,\bar{\bm{\theta}}_{t}^ {d})\) (the procedure for obtaining them is discussed later), we have the following NTF predictor defined as

\[\mathbf{h}_{t}^{\text{rf}}=\begin{bmatrix}(\bm{\phi}_{0}^{1}(\bm{\alpha}))^{ \top}\bar{\bm{\theta}}_{t}^{1}\\ \vdots\\ (\bm{\phi}_{0}^{d}(\bm{\alpha}))^{\top}\bar{\bm{\theta}}_{t}^{d}\end{bmatrix}\;.\]

We also use \(h_{t}^{\text{rf},j}(\bm{\alpha})\) to indicate the \(j\)th coordinate of \(\mathbf{h}_{t}^{\text{rf}}(\bm{\alpha})\). Then, the objective of the NTF predictor is

\[\widehat{\mathcal{R}}^{\text{rf}}(\bm{\theta}_{t})=\frac{1}{n}\sum_{i=1}^{n} \left(\mathbf{h}_{t}^{\text{rf}}(\bm{\alpha}_{i})-\mathbf{w}^{*}(\bm{\alpha}_ {i})\right)^{2}=\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{d}\left(\bm{\phi}_{0}^{j}{ }^{\top}(\bm{\alpha}_{i})\bar{\bm{\theta}}_{t}^{j}-w^{*}(\bm{\alpha}_{i})(j) \right)^{2}\;,\]

its pointwise counterpart is

\[\widehat{\mathcal{R}}^{\text{rf},j}(\bm{\theta}_{t}^{j})=\frac{1}{n}\sum_{i=1}^ {n}\left(h_{t}^{\text{rf},j}(\bm{\alpha}_{i})-w^{*}(\bm{\alpha}_{i})(j)\right)^ {2}=\frac{1}{n}\sum_{i=1}^{n}\left(\bm{\phi}_{0}^{j}{}^{\top}(\bm{\alpha}_{i}) \bar{\bm{\theta}}_{t}^{j}-w^{*}(\bm{\alpha}_{i})(j)\right)^{2}\;,\]

and GD update rule is defined as

\[\bar{\bm{\theta}}_{t+1}=\bar{\bm{\theta}}_{t}-\eta\nabla\widehat{\mathcal{R}}^{ \text{rf}}(\bm{\theta}_{t})\;.\]Kernel Least Squares (KLS) predictorIn the following we will work with a kernel function:

**Proposition 3** (Kernel induced by Rectified Linear Unit (ReLU) activation).: _The following kernel function is called the NTK function induced by the ReLU activation:_

\[\kappa(\bm{\alpha},\bm{\alpha}^{\prime})=(\bm{\alpha}^{\top}\bm{\alpha}^{\prime })\int_{\mathbb{R}^{d}}\mathbb{I}\left\{\mathbf{w}^{\top}\bm{\alpha}>0\right\} \mathbb{I}\left\{\mathbf{w}^{\top}\bm{\alpha}^{\prime}>0\right\}\,\mathcal{N}( \mathrm{d}\mathbf{w}\mid\mathbf{0},\mathbf{I}_{d})\qquad(\bm{\alpha},\bm{ \alpha}^{\prime}\in\mathbb{S}^{d-1})\;.\]

_The following holds for \(\kappa\):_

* _It is_ \(\bm{a}\) _reproducing kernel and has analytic form_ \(\kappa(\bm{\alpha},\bm{\alpha}^{\prime})=(\bm{\alpha}^{\top}\bm{\alpha}^{ \prime})(\pi-\arccos(\bm{\alpha}^{\top}\bm{\alpha}^{\prime}))\)__15_._
* \(\sup_{\bm{\alpha},\bm{\alpha}^{\prime}\in\mathbb{S}^{d-1}}\kappa(\bm{\alpha}, \bm{\alpha}^{\prime})\leq 1\)_._
* _Eigenvalues of_ \(\kappa\) _satisfy_ \(\mu_{k}\leq C\,k^{-\frac{d}{2}}\) _for_ \(k\in\mathbb{N}\)__[_2_]__._
* _The NTK matrix is a symmetric matrix_ \(\mathbf{K}\in\mathbb{R}^{n\times n}\) _with entries_ \((\mathbf{K})_{i,j}=\kappa(\bm{\alpha}_{i},\bm{\alpha}_{j})\)_._

_Throughout, \(\mathcal{H}\) is the RKHS induced by \(\kappa\)._

Having established the kernel function, we assume the following about matrix \(\mathbf{K}\):

**Assumption 2** (Smallest eigenvalue of the NTK matrix).: _Assume that there exists fixed \(\lambda_{0}>0\) such that \(\mathbb{P}(\lambda_{\min}(\mathbf{K})\geq\lambda_{0})\geq 1-\delta_{\lambda_{0}}\) and \(\delta_{\lambda_{0}}\in[0,1]\)._

Assumption 2 is fairly standard and often satisfied with sample-dependent lower bounds:

* In particular, [9] shows that \(\lambda_{0}>0\) whenever no two distinct inputs are parallel.
* In a random design setting, [4, Lemma 5.2] show that when inputs are sampled from isotropic Gaussian and \(n\leq d^{C}\) for some activation function-dependent \(C\), \(\lambda_{0}=\Omega(d)\) with probability at least \(1-e^{-\mathcal{O}(n)}\).
* In a random design setting, [29] show that (for a certain well-behaved family of input distributions), \(\lambda_{\min}(\mathbf{K})=\Theta_{\mathbb{P}}(d)\) with high probability. More precisely, their Theorem 3.2 implies that we have \(\lambda_{\min}(\mathbf{K})\geq C\,\mathrm{polylog}(n,d)d\) with probability at least \(1-n^{2}e^{-\mathcal{O}(\sqrt{d})}\).

In addition to the NTK matrix, we define its empirical counterpart for the \(j\)th component of the hidden layer, namely

\[\hat{\mathbf{K}}^{j}=(\bm{\Phi}_{0}^{j})^{\top}\bm{\Phi}_{0}^{j}\in\mathbb{R}^ {n\times n}\;.\]

Then, it is clear that \(\mathbf{K}=\mathbb{E}[\hat{\mathbf{K}}^{j}\mid\mathrm{data}]\;.\)

Now, for some vector \(\mathbf{c}\in\mathbb{R}^{d}\), we define the KLS predictor as

\[\mathbf{h}_{t}^{\mathrm{kls}}(\bm{\alpha})=\begin{bmatrix}\sum_{i=1}^{n}k(\bm {\alpha}_{i},\bm{\alpha})c_{i}^{1}\\ \vdots\\ \sum_{i=1}^{n}k(\bm{\alpha}_{i},\bm{\alpha})c_{i}^{d}\end{bmatrix},\]

We also use \(h_{t}^{\mathrm{kls},j}(\bm{\alpha})\) to indicate the \(j\)th coordinate of \(\mathbf{h}_{t}^{\mathrm{kls}}(\bm{\alpha})\). The objective for KLS predictor is defined as:

\[\widehat{\mathcal{R}}^{\mathrm{kls}}(\mathbf{c}_{t})=\frac{1}{n}\sum_{i=1}^{n} \left(\mathbf{h}_{t}^{\mathrm{kls}}(\bm{\alpha}_{i})-\mathbf{w}^{*}(\bm{ \alpha}_{i})\right)^{2}\]

for the pointwise loss function as:

\[\widehat{\mathcal{R}}^{\mathrm{kls},j}(\mathbf{c}_{t}^{j})=\frac{1}{n}\sum_{i=1 }^{n}\left(\mathbf{h}_{t}^{\mathrm{kls},j}(\bm{\alpha}_{i})-w^{*}(\bm{\alpha}_ {i})(j)\right)^{2}\]

and finally GD update rule is defined as:

\[\mathbf{c}_{t+1}=\mathbf{c}_{t}-\eta\nabla\widehat{\mathcal{R}}^{\mathrm{kls }}(\mathbf{c}_{t})\;.\]

**Proposition 4**.: _Let \(\mathbf{w}_{i}^{t}\) be iterates generated by Algorithm 2. Then the following statement holds true:_

\[\left\|\mathbf{w}_{i}^{t}-\mathbf{w}^{*}(\bm{\alpha}_{i})\right\|^{2}\leq(1- \mu\gamma)^{t}D\]

### Properties of ReLU Networks

In this section we include standard lemmata for analysis of shallow neural networks, and where appropriate we account for the fact that we are working with vector-valued predictors.

**Proposition 5** (Activation patterns [21]).: _Assume that initial parameters of a neural network are chosen as in section 3.1. Consider the set of indices of neurons that changed their activation patterns on input \(\bm{\alpha}\), when \(\bm{\theta}_{0}\) is replaced by some parameters \(\tilde{\bm{\theta}}=(\tilde{\mathbf{u}}_{1},\ldots,\tilde{\mathbf{u}}_{m})\):_

\[P(\tilde{\bm{\theta}},\bm{\alpha})=\left\{k\in[m]|\mathbb{I}\{\tilde{\mathbf{ u}}^{\top}(k)\bm{\alpha}>0\}-\mathbb{I}\{\mathbf{u}_{0}^{\top}(k)\bm{\alpha}>0 \}\neq 0\right\}\qquad(\tilde{\bm{\theta}}\in\mathbb{R}^{Nm},\bm{\alpha}\in \Delta^{N})\;.\]

_Then, for \(\tilde{\bm{\theta}}\) whose components satisfy \(\max_{k}\|\tilde{\mathbf{u}}_{k}-\mathbf{u}_{0,k}\|\leq\rho\) for some fixed \(\rho\geq 0\), and any \(\bm{\alpha}\in\Delta^{N}\), the following facts hold:_

* _For all_ \(k\in P(\tilde{\bm{\theta}},\bm{\alpha})\)_,_ \(|\mathbf{u}_{0,k}^{\top}\bm{\alpha}|\leq\|\tilde{\mathbf{u}}_{k}-\mathbf{u}_{0,k}\|\)_._
* \(\mathbb{E}\left|P(\tilde{\bm{\theta}},\bm{\alpha})\right|\leq m\rho\,.\)__
* _With probability at least_ \(1-2e^{-\nu}\) _for any_ \(\nu>0\)_,_ \(|P(\tilde{\bm{\theta}},\bm{\alpha})|\leq m\rho+\sqrt{m\nu}\,.\)__

_Also_

* \(\left\|\bm{\phi}_{\tilde{\bm{\theta}}}-\bm{\phi}_{0}\right\|\leq\rho+\sqrt{ \frac{\nu}{m}}\)_._
* \(\left\|(\bm{\phi}_{\tilde{\bm{\theta}}}-\bm{\phi}_{0})^{\top}\tilde{\bm{\theta }}\right\|\leq\rho(\sqrt{m}\rho+\sqrt{\nu})\)_._

**Proposition 6** (Bounded gradient).: _The gradients of neural network are bounded:_

\[\left\|\nabla_{\bm{\theta}}\mathbf{h}_{\bm{\theta}}(\bm{\alpha})\right\|^{2} \leq d,\quad\left\|\nabla\widehat{\mathcal{R}}(\bm{\theta})\right\|^{2}\leq 4d \widehat{\mathcal{R}}(\bm{\theta}).\]

Proof.: The proof simply follows by definition:

\[\left\|\nabla_{\bm{\theta}}\mathbf{h}_{\bm{\theta}}(\bm{\alpha})\right\|^{2}= \sum_{j=1}^{d}\left\|\bm{\phi}^{j}(\bm{\alpha})\right\|^{2}=\sum_{j=1}^{d}\sum _{r=1}^{m}\frac{1}{m}\mathbb{I}\left\{\mathbf{u}_{r}^{\top}\bm{\alpha}\right\} \left\|\bm{\alpha}\right\|^{2}\leq d\]

and

\[\left\|\nabla\widehat{\mathcal{R}}(\bm{\theta})\right\|^{2} =\left\|\frac{2}{n}\sum_{i=1}^{n}\nabla_{\bm{\theta}}\mathbf{h} _{\bm{\theta}}(\bm{\alpha}_{i})(\mathbf{h}_{\bm{\theta}}(\bm{\alpha}_{i})- \mathbf{w}^{*}(\bm{\alpha}_{i}))\right\|^{2}\] \[\leq 4\frac{1}{n}\sum_{i=1}^{n}\left\|\nabla_{\bm{\theta}} \mathbf{h}_{\bm{\theta}}(\bm{\alpha}_{i})\right\|^{2}\left\|\mathbf{h}_{\bm{ \theta}}(\bm{\alpha}_{i})-\mathbf{w}^{*}(\bm{\alpha}_{i}))\right\|^{2}\leq 4d \widehat{\mathcal{R}}(\bm{\theta}).\]

**Lemma 10** (Spectral Property of Empirical Kernel Matrix).: _Given a set of parameter \(\bm{\theta}_{t}=\{\mathbf{u}_{t}(1),...,\mathbf{u}_{t}(m)\}\), if we assume \(\max_{r\in[m]}\left\|\mathbf{u}_{t}(r)-\mathbf{u}_{0}(r)\right\|\leq\rho\), then the following statements about the spectral property of its random feature and empirical gram matrix hold true:_

* \(\left\|\bm{\phi}_{t}(\bm{\alpha}_{i})\right\|\leq 1\)_,_ \(\left\|\bm{\Phi}_{t}\right\|\leq\sqrt{n}\) _,_ \(\left\|\bm{\Phi}_{t}-\bm{\Phi}_{0}\right\|\leq\sqrt{n(\rho+\sqrt{\frac{\nu}{m}})}\)__
* \(\left\|\hat{\mathbf{K}}_{t}\right\|\leq\left\|\hat{\mathbf{K}}_{0}\right\|+2n^{ 2}(\rho+\sqrt{\frac{\nu}{m}})\)_,_
* \(\lambda_{\min}(\hat{\mathbf{K}}_{t})\geq\lambda_{\min}(\hat{\mathbf{K}}_{0})-2n ^{2}(\rho+\sqrt{\frac{\nu}{m}})\)_,_
* _If we assume_ \(m\geq\frac{64n^{2}\log(n/\nu^{\prime})}{\lambda_{0}^{2}}\)_, then_ \(\mathbb{P}\left(\lambda_{\min}(\hat{\mathbf{K}}_{0})\geq\frac{1}{2}\lambda_{0} \right)\geq 1-\nu^{\prime}\)_, and_ \(\mathbb{P}\left(\left\|\hat{\mathbf{K}}_{0}\right\|\leq\frac{1}{2}\left\| \mathbf{K}_{0}\right\|\right)\geq 1-\nu^{\prime}\)_._Proof.: We begin with proving \((a)\), the spectral norm \(\bm{\phi}_{t}(\bm{\alpha}_{i})\)

\[\left\|\bm{\phi}_{t}(\bm{\alpha}_{i})\right\|^{2}\leq\sum_{r=1}^{m}\left\|a(r) \mathbb{I}\left\{\mathbf{u}_{t}^{\top}(r)\bm{\alpha}_{i}\right\}\bm{\alpha}_{i} \right\|^{2}\leq 1\]

the spectral bound for \(\bm{\Phi}_{t}\).

\[\left\|\bm{\Phi}_{t}\right\|^{2}=\left\|(\bm{\phi}_{t}(\bm{\alpha}_{1}),\dots, \bm{\phi}_{t}(\bm{\alpha}_{n}))\right\|^{2}\leq\sum_{i=1}^{n}\left\|\bm{\phi}_ {t}(\bm{\alpha}_{i})\right\|_{F}^{2}\leq n,\]

so we know \(\left\|\bm{\Phi}_{t}\right\|\leq\sqrt{n}\).

For \(\left\|\bm{\Phi}_{t}-\bm{\Phi}_{0}\right\|\), by definition we have:

\[\left\|\bm{\Phi}_{t}-\bm{\Phi}_{0}\right\|^{2} \leq\left\|\bm{\Phi}_{t}-\bm{\Phi}_{0}\right\|_{F}^{2}\] \[\leq\sum_{i=1}^{n}\left\|\bm{\phi}_{t}(\bm{\alpha}_{i})-\bm{\phi }_{0}(\bm{\alpha}_{i})\right\|^{2}\] \[\leq\sum_{i=1}^{n}\sum_{r=1}^{m}|a(r)\mathbb{I}\{\mathbf{u}_{t}^ {\top}(r)(\bm{\alpha}_{i})\}-a(r)\mathbb{I}\{\mathbf{u}_{0}^{\top}(r)(\bm{ \alpha}_{i})\}|^{2}\] \[\leq\frac{1}{m}\sum_{i=1}^{n}|P(\bm{\theta}_{t},\bm{\alpha}_{i})|\] \[\leq n(\rho+\sqrt{\frac{\nu}{m}})\;.\]

Now we show the bound on the norm of \(\hat{\mathbf{K}}_{t}\). We first examine \(\left\|\bm{\phi}_{t}^{\top}(\bm{\alpha}_{i})\bm{\phi}_{t}(\bm{\alpha}_{j})- \bm{\phi}_{0}^{\top}(\bm{\alpha}_{i})\bm{\phi}_{0}(\bm{\alpha}_{j})\right\|_{F}\). Consider

\[\left|\bm{\phi}_{t}^{\top}(\bm{\alpha}_{i})\bm{\phi}_{t}(\bm{ \alpha}_{j})-\bm{\phi}_{0}^{\top}(\bm{\alpha}_{i})\bm{\phi}_{0}(\bm{\alpha}_ {j})\right|\] \[= \left|\sum_{r=1}^{m}a(r)\mathbb{I}\{\mathbf{u}_{t}^{\top}(r)\bm {\alpha}_{i}\}\cdot a(r)\mathbb{I}\{\mathbf{u}_{t}^{\top}(r)\bm{\alpha}_{j}\} \bm{\alpha}_{i}^{\top}\bm{\alpha}_{j}-\sum_{r=1}^{m}a(r)\mathbb{I}\{\mathbf{ u}_{0}^{\top}(r)\bm{\alpha}_{i}\}\cdot a(r)\mathbb{I}\{\mathbf{u}_{0}^{\top}(r) \bm{\alpha}_{j}\}\bm{\alpha}_{i}^{\top}\bm{\alpha}_{j}\right|\] \[\leq \frac{1}{m}\sum_{r=1}^{m}\left|\mathbb{I}\{\mathbf{u}_{t}^{\top }(r)\bm{\alpha}_{i}\}\cdot\mathbb{I}\{\mathbf{u}_{t}^{\top}(r)\bm{\alpha}_{j} \}-\mathbb{I}\{\mathbf{u}_{0}^{\top}(r)\bm{\alpha}_{i}\}\cdot\mathbb{I}\{ \mathbf{u}_{0}^{\top}(r)\bm{\alpha}_{j}\}\right|\left|\bm{\alpha}_{i}^{\top} \bm{\alpha}_{j}\right|\] \[= \frac{1}{m}\sum_{r=1}^{m}\left|\left(\mathbb{I}\{\mathbf{u}_{t}^ {\top}(r)\bm{\alpha}_{i}\}-\mathbb{I}\{\mathbf{u}_{0}^{\top}(r)\bm{\alpha}_{ i}\}\right)\cdot\mathbb{I}\{\mathbf{u}_{t}^{\top}(r)\bm{\alpha}_{j}\}- \mathbb{I}\{\mathbf{u}_{0}^{\top}(r)\bm{\alpha}_{i}\}\right.\] \[\quad\times\left(\mathbb{I}\{\mathbf{u}_{0}^{\top}(r)\bm{\alpha}_{ j}\}-\mathbb{I}\{\mathbf{u}_{t}^{\top}(r)\bm{\alpha}_{j}\}\right)\left|\left|\bm{ \alpha}_{i}^{\top}\bm{\alpha}_{j}\right|\] \[= \frac{1}{m}\left(|P(\bm{\theta}_{t},\bm{\alpha}_{i})|+|P(\bm{ \theta}_{t},\bm{\alpha}_{j})|\right)\] \[\leq 2(\rho+\sqrt{\frac{\nu}{m}})\;.\]Notice that due to triangle inequality we have \(\left\|\hat{\mathbf{K}}_{t}\right\|\leq\left\|\hat{\mathbf{K}}_{0}\right\|+ \left\|\hat{\mathbf{K}}_{t}-\hat{\mathbf{K}}_{0}\right\|.\) Now we examine \(\left\|\hat{\mathbf{K}}_{t}-\hat{\mathbf{K}}_{0}\right\|\):

\[\left\|\hat{\mathbf{K}}_{t}-\hat{\mathbf{K}}_{0}\right\| \leq\left\|\hat{\mathbf{K}}_{t}-\hat{\mathbf{K}}_{0}\right\|_{F}\] (22) \[=\sqrt{\sum_{i,j\in[n]}|\boldsymbol{\phi}_{t}^{\top}(\boldsymbol {\alpha}_{i})\boldsymbol{\phi}_{t}(\boldsymbol{\alpha}_{j})-\boldsymbol{\phi}_ {0}^{\top}(\boldsymbol{\alpha}_{i})\boldsymbol{\phi}_{0}(\boldsymbol{\alpha}_{ j})|^{2}}\] (23) \[\leq\sum_{i,j\in[n]}|\boldsymbol{\phi}_{t}^{\top}(\boldsymbol{ \alpha}_{i})\boldsymbol{\phi}_{t}(\boldsymbol{\alpha}_{j})-\boldsymbol{\phi}_ {0}^{\top}(\boldsymbol{\alpha}_{i})\boldsymbol{\phi}_{0}(\boldsymbol{\alpha}_{ j})|\] (24) \[=2n^{2}(\rho+\sqrt{\frac{\nu}{m}}).\] (25)

At last, we prove statement (d). According to Weyl's inequality:

\[\lambda_{\min}(\hat{\mathbf{K}}_{0})\geq\lambda_{\min}(\mathbf{K})-\left\| \hat{\mathbf{K}}_{0}-\mathbf{K}\right\|.\]

According to basic concentration inequality we know that the following statement holds with probability at least \(1-\nu\):

\[|\hat{\mathbf{K}}_{0}(i,j)-\mathbf{K}(i,j)|\leq\frac{2\sqrt{\log(1/\nu^{\prime })}}{\sqrt{m}}\]

Taking union bound over all \(n^{2}\) entries and summing over all entries yields:

\[\left\|\hat{\mathbf{K}}_{0}-\mathbf{K}\right\|\leq\left\|\hat{\mathbf{K}}_{0} -\mathbf{K}\right\|_{F}\leq\sum_{i,j\in[n]}|\hat{\mathbf{K}}_{0}(i,j)-\mathbf{ K}(i,j)|\leq\frac{4n\sqrt{\log(n/\nu^{\prime})}}{\sqrt{m}}\]

To ensure the RHS less than \(\frac{1}{2}\lambda_{0}\), we need

\[m\geq\frac{64n^{2}\log(n/\nu^{\prime})}{\lambda_{0}^{2}}.\]

Last, since \(\left\|\hat{\mathbf{K}}_{0}-\mathbf{K}\right\|\leq\frac{1}{2}\lambda_{0}\), we know that it is also true \(\left\|\hat{\mathbf{K}}_{0}-\mathbf{K}\right\|\leq\frac{1}{2}\sigma_{\max}= \frac{1}{2}\left\|\mathbf{K}\right\|\). 

**Lemma 11** (Deviation of Gradient Computed on Inexact Labels).: _For Algorithm 2, the following statements hold true:_

\[\left\|\nabla_{\boldsymbol{\theta}_{t}^{j}}\widehat{\mathcal{R}} ^{j}(\boldsymbol{\theta}_{t}^{j})-\mathbf{g}_{t}^{j}\right\|^{2}\leq 4(1-\mu \gamma)^{Kt}D,\] \[\left\|\boldsymbol{\Phi}_{t}^{j\top}(\nabla\widehat{\mathcal{R}} ^{j}(\boldsymbol{\theta}_{t}^{j})-\mathbf{g}_{t}^{j})\right\|^{2}\leq\frac{4}{ n}\left(\frac{3}{2}\left\|\mathbf{K}\right\|+2n^{2}\left(\rho+\sqrt{\frac{\nu}{m}} \right)\right)^{2}(1-\mu\gamma)^{Kt}D.\]

Proof.: Recall the definition of \(\nabla_{\boldsymbol{\theta}_{t}}\widehat{\mathcal{R}}(\boldsymbol{\theta}_{t})\) and \(\mathbf{g}_{t}\):

\[\nabla_{\boldsymbol{\theta}_{t}}\widehat{\mathcal{R}}(\boldsymbol{ \theta}_{t}) =\begin{bmatrix}\frac{2}{n}\sum_{i=1}^{n}\boldsymbol{\phi}_{t}^{1}( h_{t}^{1}(\boldsymbol{\alpha}_{i})-\mathbf{w}^{*}(\boldsymbol{\alpha}_{i})(1))\\ \vdots\\ \frac{2}{n}\sum_{i=1}^{n}\boldsymbol{\phi}_{t}^{d}(h_{t}^{d}(\boldsymbol{ \alpha}_{i})-\mathbf{w}^{*}(\boldsymbol{\alpha}_{i})(d))\end{bmatrix}= \begin{bmatrix}\frac{2}{n}\boldsymbol{\Phi}_{t}^{1}(\mathbf{h}_{t}^{1}- \mathbf{w}^{*}(1))\\ \vdots\\ \frac{2}{n}\boldsymbol{\Phi}_{t}^{d}(\mathbf{h}_{t}^{d}-\mathbf{w}^{*}( \boldsymbol{\alpha}))\end{bmatrix},\] \[\mathbf{g}_{t} =\begin{bmatrix}\frac{2}{n}\sum_{i=1}^{n}\boldsymbol{\phi}_{t}^{1 }(h_{t}^{1}(\boldsymbol{\alpha}_{i})-\mathbf{w}_{i}^{t}(1))\\ \vdots\\ \frac{2}{n}\sum_{i=1}^{n}\boldsymbol{\phi}_{t}^{d}(h_{t}^{d}(\boldsymbol{ \alpha}_{i})-\mathbf{w}_{i}^{t}(d))\end{bmatrix}=\begin{bmatrix}\frac{2}{n} \boldsymbol{\Phi}_{t}^{1}(\mathbf{h}_{t}^{1}-\mathbf{w}_{i}^{t}(1))\\ \vdots\\ \frac{2}{n}\boldsymbol{\Phi}_{t}^{d}(\mathbf{h}_{t}^{d}-\mathbf{w}_{i}^{t}(d)) \end{bmatrix}\]where \(\mathbf{w}^{t}(j)=[w_{1}^{t}(j),...,w_{n}^{t}(j)]^{\top}\in\mathbb{R}^{n}\) and \(\mathbf{w}^{*}(j)=[w^{*}(\boldsymbol{\alpha}_{1})(j),...,w^{*}(\boldsymbol{ \alpha}_{n})(j)]^{\top}\in\mathbb{R}^{n}\). Hence

\[\left\|\nabla_{\boldsymbol{\theta}_{t}^{j}}\widehat{\mathcal{R}} ^{j}(\boldsymbol{\theta}_{t}^{j})-\mathbf{g}_{t}^{j}\right\|^{2} =\left\|\frac{2}{n}\boldsymbol{\Phi}_{t}^{j}(\mathbf{h}_{t}^{j}- \mathbf{w}^{*}(j))-\frac{2}{n}\boldsymbol{\Phi}_{t}^{j}\left(\mathbf{h}_{t}^{j} -\mathbf{w}^{t}(j)\right)\right\|^{2}\] \[=\left\|\frac{2}{n}\boldsymbol{\Phi}_{t}^{j}(\mathbf{w}^{t}(j)- \mathbf{w}^{*}(j))\right\|^{2}\] \[\leq 4(1-\mu\gamma)^{Kt}D\;.\]

For the second result we have

\[\left\|\boldsymbol{\Phi}_{t}^{j\top}(\nabla\widehat{\mathcal{R}} ^{j}(\boldsymbol{\theta}_{t}^{j})-\mathbf{g}_{t}^{j})\right\|^{2} =\frac{4}{n^{2}}\left\|\boldsymbol{\Phi}_{t}^{\top}\boldsymbol{ \Phi}_{t}(\mathbf{h}_{t}^{j}-\mathbf{w}^{*}(j))-\boldsymbol{\Phi}_{t}^{j\top }\boldsymbol{\Phi}_{t}^{j}(\mathbf{h}_{t}^{j}-\mathbf{w}^{t}(j))\right\|^{2}\] \[=\frac{4}{n^{2}}\left\|\boldsymbol{\Phi}_{t}^{j\top}\boldsymbol{ \Phi}_{t}^{j}(\mathbf{w}^{t}(j)-\mathbf{w}^{*}(j))\right\|^{2}\] \[\leq\frac{4}{n^{2}}\left\|\boldsymbol{\Phi}_{t}^{j\top}\boldsymbol {\Phi}_{t}^{j}\right\|^{2}n(1-\mu\gamma)^{Kt}D\] \[\leq\frac{4}{n}\left(\frac{3}{2}\left\|\mathbf{K}\right\|+2n^{2}( \rho+\sqrt{\frac{\nu}{m}})\right)^{2}(1-\mu\gamma)^{Kt}D\;.\]

The following theorem shows that Algorithm 2 can converge on empirical risk defined in (4).

**Theorem 5** (Convergence of Algorithm).: _For Algorithm 2, if we choose_

\[m\geq\left(\frac{48nB_{y}}{\lambda_{0}}+\sqrt{\nu}\right)^{2} \frac{256n^{4}}{\lambda_{0}^{2}},\] \[\eta\leq\min\left\{\frac{3n^{3}}{\lambda_{0}}\left(\frac{48nB_{y }}{\lambda_{0}}+\sqrt{\nu}\right),\frac{3\lambda_{0}n^{2}}{256\left(\frac{1}{ 2}\left\|\mathbf{K}\right\|+\frac{\lambda_{0}}{8}\right)^{2}}\right\},\]

_and assume the inner iteration number of Algorithm 1 satisfying_

\[K\geq\max\left\{\kappa\log\left(\frac{\eta\lambda_{0}nT^{2}D}{16B_{y}^{2}n} \right),2\kappa\log\left(\frac{\eta\lambda_{0}\kappa\sqrt{D}}{4B_{y}n}\right) \right\},\] (26)

_then the following statement holds for any \(t\in[T]\) and \(j\in[d]\):_

\[\widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_{t+1}^{j})\leq\left(1-\frac{ \eta\lambda_{0}}{4n}\right)^{t}\widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_ {0}^{j})+\frac{4nC\left(1-\mu\gamma\right)^{K}D}{\eta\lambda_{0}},\]

_with_

\[C=\frac{\lambda_{0}^{2}\eta^{2}}{24n^{4}}+\frac{4\eta^{2}\left(\frac{1}{2} \left\|\mathbf{K}\right\|+\frac{\lambda_{0}}{8}\right)^{2}}{\frac{3}{8}n^{2}}.\]

Proof.: We prove by induction. We make the following inductive hypotheses:

(I) \[\quad\widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_{t}^{j})\leq \left(1-\frac{\eta\lambda_{0}}{4n}\right)^{t-1}\widehat{\mathcal{R}}^{j}( \boldsymbol{\theta}_{0}^{j})+\sum_{s=0}^{t-1}\left(1-\frac{\eta\lambda_{0}}{4 n}\right)^{t-1-s}(1-\mu\gamma)^{s}C(1-\mu\gamma)^{K}D,\] (II) \[\quad\max_{r\in[m]}\left\|\mathbf{u}_{t}^{j}(r)-\mathbf{u}_{t}^{j }(r)\right\|\leq\rho:=\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}.\]where \(C\) is the constant to be determined later. First we are going to show hypothesis (\(\mathbf{II}\)) holding for \(t+1\). First, by our choice of \(\eta\), we know \((1-\mu\gamma)\leq\left(1-\frac{\eta\lambda_{0}}{4n}\right)\). Hence

\[\sum_{s=0}^{t-1}\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{t-1-s}(1-\mu\gamma) ^{s}\leq\frac{4n}{\eta\lambda_{0}}.\]

Now we are going to bound maximal neuron drifting from initialization: for any \(j\in[d]\)

\[\left\|\mathbf{u}_{t+1}^{j}(r)-\mathbf{u}_{0}^{j}(r)\right\|\] \[=\left\|\sum_{t^{\prime}=0}^{t}\eta\mathbf{g}_{t^{\prime}}^{j}(r) \right\|=\eta\left\|\sum_{t^{\prime}=0}^{t}\frac{2}{n}\sum_{i=1}^{n}a_{j}(r) \mathbb{I}\left\{\mathbf{u}_{t}^{j}(r)^{\top}\boldsymbol{\alpha}_{i}\right\} \boldsymbol{\alpha}_{i}(h_{t}^{j}(\boldsymbol{\alpha}_{i})-w_{t}^{i}(j))\right\|\] \[\leq\frac{2\eta}{\sqrt{m}}\sum_{t^{\prime}=0}^{t}\frac{1}{n}\sum _{i=1}^{n}\left(|h_{t}^{j}(\boldsymbol{\alpha}_{i})-w_{*}^{i}(j)|+|w_{*}^{i}( j)-w_{t}^{i}(j)|\right)\] \[\leq\frac{2\eta}{\sqrt{m}}\sum_{t^{\prime}=0}^{t}\left(\sqrt{ \frac{1}{n}\sum_{i=1}^{n}\left(h_{t}^{j}(\boldsymbol{\alpha}_{i})-w_{*}^{i}( j)\right)^{2}}+\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(w_{*}^{i}(j)-w_{t}^{i}(j) \right)^{2}}\right)\] \[\leq\frac{2\eta}{\sqrt{m}}\sum_{t^{\prime}=0}^{t}\left(\sqrt{ \widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_{t^{\prime}}^{j})}+\sqrt{(1- \mu\gamma)^{Kt^{\prime}}D}\right)\] \[\leq 2\eta\sum_{t^{\prime}=0}^{t}\left(\frac{\sqrt{\left(1-\frac{ \eta\lambda_{0}}{4n}\right)^{t^{\prime}-1}\widehat{\mathcal{R}}^{j}( \boldsymbol{\theta}_{0}^{j})}+C^{\frac{4n(1-\mu\gamma)^{K}D}{\eta\lambda_{0}}} }{\sqrt{m}}+\frac{1}{\sqrt{m}}\sqrt{(1-\mu\gamma)^{Kt^{\prime}}D}\right)\] \[\leq 2\eta\left(\frac{8n}{\eta\lambda_{0}}\sqrt{\widehat{\mathcal{ R}}(\boldsymbol{\theta}_{0})}+t\sqrt{C^{\frac{4n(1-\mu\gamma)^{K}D}{\eta \lambda_{0}}}}{\sqrt{m}}+\frac{1}{\sqrt{m}\mu\gamma}\sqrt{(1-\mu\gamma)^{K}D} \right),\]

where at last step we use the fact \(\sqrt{1-a}\leq 1-\frac{a}{2}\) for \(a\in(0,1)\), and triangle inequality to split terms inside square root. Since we choose \(\gamma=\frac{1}{L}\), we have

\[\left\|\mathbf{u}_{t+1}(r)-\mathbf{u}_{t}(r)\right\|\leq 2\eta\left(\frac{8nB_{y }}{\eta\lambda_{0}\sqrt{m}}+t\sqrt{C^{\frac{4n\left(1-\frac{1}{\kappa}\right) ^{K}D}{\eta\lambda_{0}m}}}+\frac{2\kappa\sqrt{(1-\frac{1}{\kappa})^{K}D}}{ \sqrt{m}}\right)\]

According to our choice of \(K\):

\[K\geq\max\left\{\kappa\log\left(\frac{\eta\lambda_{0}nT^{2}CD}{16B_{y}^{2}n} \right),2\kappa\log\left(\frac{\eta\lambda_{0}\kappa\sqrt{D}}{4B_{y}n}\right) \right\},\]

we have:

\[t\sqrt{C\frac{4n\left(1-\frac{1}{\kappa}\right)^{K}D}{\eta\lambda _{0}m}} \leq\frac{8nB_{y}}{\eta\lambda_{0}\sqrt{m}},\] \[\frac{2\kappa\sqrt{(1-\frac{1}{\kappa})^{K}D}}{\sqrt{m}} \leq\frac{8nB_{y}}{\eta\lambda_{0}\sqrt{m}}.\]

Hence we conclude that:

\[\left\|\mathbf{u}_{t+1}(r)-\mathbf{u}_{t}(r)\right\|\leq\frac{48nB_{y}}{ \lambda_{0}\sqrt{m}}.\]

Now we switch to proving hypothesis (\(\mathbf{I}\)). According to updating rule, we have

\[\boldsymbol{\theta}_{t+1}^{j} =\boldsymbol{\theta}_{t}^{j}-\eta\mathbf{g}_{t}^{j}\] \[=\boldsymbol{\theta}_{t}^{j}-\eta\frac{2}{n}\boldsymbol{\Phi}_{t} ^{j}(\mathbf{h}_{t}^{j}-\mathbf{w}^{t}(j))\]where \(\mathbf{w}^{t}(j)=[w_{t}^{1}(j),...,w_{n}^{t}(j)]\). Hence

\[\widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_{t+1}^{j}) =\frac{1}{n}\left\|\boldsymbol{\Phi}_{t+1}^{j}{}^{\top} \boldsymbol{\theta}_{t+1}^{j}-\mathbf{w}^{*}(j)\right\|^{2}\] \[=\frac{1}{n}\left\|\boldsymbol{\Phi}_{t+1}^{j}{}^{\top} \boldsymbol{\theta}_{t+1}^{j}-\boldsymbol{\Phi}_{t}^{j}{}^{\top}\boldsymbol{ \theta}_{t+1}^{j}+\boldsymbol{\Phi}_{t}^{j}{}^{\top}\boldsymbol{\theta}_{t+1} ^{j}-\mathbf{w}^{*}(j)\right\|^{2}\] \[\leq\inf_{p>0}\left(1+\frac{1}{p}\right)\underbrace{\frac{1}{n} \left\|\boldsymbol{\Phi}_{t+1}^{j}{}^{\top}\boldsymbol{\theta}_{t+1}^{j}- \boldsymbol{\Phi}_{t}^{j}{}^{\top}\boldsymbol{\theta}_{t+1}^{j}\right\|^{2}}_ {T_{1}}+(1+p)\underbrace{\frac{1}{n}\left\|\boldsymbol{\Phi}_{t}^{j}{}^{\top} \boldsymbol{\theta}_{t+1}^{j}-\mathbf{w}^{*}(j)\right\|^{2}}_{T_{2}}\] (27)

where \(\mathbf{w}^{*}(j)=[w^{*}(\boldsymbol{\alpha}_{1})(j),...,w^{*}(\boldsymbol{ \alpha}_{n})(j)]\).

For \(T_{1}\):

Notice that:

\[\left|\sum_{r=1}^{m}a_{j}(r)(\mathbb{I}\{\mathbf{u}_{t+1}^{j}(r) ^{\top}\boldsymbol{\alpha}_{i}\}-\mathbb{I}\{\mathbf{u}_{t}^{j}(r)^{\top} \boldsymbol{\alpha}_{i}\})\mathbf{u}_{t}^{\top}(r)\boldsymbol{\alpha}_{i}\right|\] \[\leq\frac{1}{\sqrt{m}}\sum_{r=1}^{m}\left|(\mathbb{I}\{\mathbf{u} _{t+1}^{j}(r)^{\top}\boldsymbol{\alpha}_{i}\}-\mathbb{I}\{\mathbf{u}_{t}^{j}(r) ^{\top}\boldsymbol{\alpha}_{i}\})\mathbf{u}_{t}^{\top}(r)\boldsymbol{\alpha}_{ i}\right|\] \[\leq\frac{1}{\sqrt{m}}\sum_{r=1}^{m}\left|(\mathbb{I}\{\mathbf{u} _{t+1}^{j}(r)^{\top}\boldsymbol{\alpha}_{i}\}-\mathbb{I}\{\mathbf{u}_{t}^{j}(r )^{\top}\boldsymbol{\alpha}_{i}\})\right|\left\|\mathbf{u}_{t}^{j}{}^{\top}(r )\right\|\] \[\leq\frac{1}{\sqrt{m}}(|P(\boldsymbol{\theta}_{t+1},\boldsymbol{ \alpha}_{i})|+|P(\boldsymbol{\theta}_{t},\boldsymbol{\alpha}_{i})|)\left\| \mathbf{u}_{t}^{\top}(r)-\mathbf{u}_{t+1}^{\top}(r)\right\|.\]

According to updating rule, we have:

\[\left\|\mathbf{u}_{t}^{j}{}^{\top}(r)-\mathbf{u}_{t+1}^{j}{}^{ \top}(r)\right\| =\left\|\eta\mathbf{g}_{t}^{j}(r)\right\|\] \[\leq\frac{2\eta\sqrt{\widehat{\mathcal{R}}^{j}(\boldsymbol{ \theta}_{t}^{j})}}{\sqrt{m}}+\frac{2\eta}{\sqrt{m}}\sqrt{(1-\mu\gamma)^{Kt}D}\]

Since we prove that \(\max_{r\in[m]}\left\|\mathbf{u}_{t+1}^{j}(r)-\mathbf{u}_{t}^{j}(r)\right\|\leq\rho\), we have:

\[|P(\boldsymbol{\theta}^{t+1},\boldsymbol{\alpha}_{i})|\leq m\rho+\sqrt{m\nu}.\]

Putting pieces together yields:

\[T_{1} \leq\frac{4}{m}\left(m\rho+\sqrt{m\nu}\right)^{2}\left(\frac{2 \eta\sqrt{\widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_{t}^{j})}}{\sqrt{m}}+ \frac{2\eta}{\sqrt{m}}\sqrt{(1-\mu\gamma)^{Kt}D}\right)^{2}\] \[\leq 4\left(\sqrt{m}\rho+\sqrt{\nu}\right)^{2}\left(\frac{4\eta^{2} \widehat{\mathcal{R}}(\boldsymbol{\theta}^{t})}{m}+\frac{4\eta^{2}}{m}(1-\mu \gamma)^{Kt}D\right).\] (28)For \(T_{2}\):

\[\frac{1}{n}\left\|\boldsymbol{\Phi}_{t}^{j\top}\boldsymbol{\theta}_{t +1}^{j}-\mathbf{w}^{*}(j)\right\|^{2}\] \[=\frac{1}{n}\left\|\boldsymbol{\Phi}_{t}^{j\top}(\boldsymbol{ \theta}_{t}^{j}-\eta\mathbf{g}_{t}^{j})-\mathbf{w}^{*}(j)\right\|^{2}\] \[=\frac{1}{n}\left\|\boldsymbol{\Phi}_{t}^{j\top}(\boldsymbol{ \theta}_{t}^{j}-\frac{2}{n}\eta\boldsymbol{\Phi}_{t}^{j}(\mathbf{h}_{t}^{j}- \mathbf{w}^{*}(j)))-\mathbf{w}^{*}(j)-\boldsymbol{\Phi}_{t}^{j\top}(\frac{2}{n }\eta\boldsymbol{\Phi}_{t}^{j}(\mathbf{w}^{t}(j)-\mathbf{w}^{*}(j)))\right\|^{2}\] \[\leq\inf_{q>0}\left(1+\frac{1}{q}\right)\left(1-\frac{2\eta \lambda_{\min}(\hat{\mathbf{K}}_{t})}{n}\right)^{2}\widehat{\mathcal{R}}^{j}( \boldsymbol{\theta}_{t}^{j})+(1+q)\frac{4\eta^{2}}{n^{3}}\left\|\boldsymbol{ \Phi}_{t}^{j\top}\boldsymbol{\Phi}_{t}^{j}(\mathbf{w}^{t}(j)-\mathbf{w}^{*}(j ))\right\|^{2}\]

We choose \(q=\frac{n}{2\eta\lambda_{\min}(\hat{\mathbf{K}}_{t})}\), and the fact that \((1+a)(1-a)^{2}\leq(1-a)\):

\[T_{2} =\frac{1}{n}\left\|\boldsymbol{\Phi}_{t}^{j\top}\boldsymbol{ \theta}_{t+1}^{j}-\mathbf{w}^{*}(j)\right\|^{2}\] \[\leq\left(1-\frac{2\eta\lambda_{\min}(\hat{\mathbf{K}}_{t})}{n} \right)\widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_{t}^{j})+\left(1+\frac{n} {2\eta\lambda_{\min}(\hat{\mathbf{K}}_{t})}\right)\frac{4\eta^{2}}{n^{3}} \left\|\boldsymbol{\Phi}_{t}^{j\top}\boldsymbol{\Phi}_{t}^{j}(\mathbf{w}^{t}( j)-\mathbf{w}^{*}(j))\right\|^{2}\] \[\leq\left(1-\frac{2\eta\lambda_{\min}(\hat{\mathbf{K}}_{t})}{n} \right)\widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_{t}^{j})\] \[\quad+\left(1+\frac{n}{2\eta\lambda_{\min}(\hat{\mathbf{K}}_{t}) }\right)\frac{4\eta^{2}}{n^{3}}\left(\left\|\hat{\mathbf{K}}_{0}\right\|+2n^{2 }(\rho+\sqrt{\frac{\nu}{m}})\right)^{2}(1-\mu\gamma)^{Kt}D\]

Since we choose \(\eta\leq\frac{n}{2\lambda_{\min}(\hat{\mathbf{K}}_{t})}\), we can re-write the above inequality as :

\[T_{2} \leq\left(1-\frac{2\eta\lambda_{\min}(\hat{\mathbf{K}}_{t}^{j})}{ n}\right)\widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_{t}^{j})+\frac{4\eta}{n^{2} \lambda_{\min}(\hat{\mathbf{K}}_{t})}\left(\left\|\hat{\mathbf{K}}_{0}\right\| +2n^{2}(\rho+\sqrt{\frac{\nu}{m}})\right)^{2}(1-\mu\gamma)^{Kt}D\] \[\leq\left(1-\frac{2\eta\lambda_{\min}(\hat{\mathbf{K}}_{0})}{n}+4 n\eta\left(\rho+\sqrt{\frac{\nu}{m}}\right)\right)\widehat{\mathcal{R}}^{j}( \boldsymbol{\theta}_{t}^{j})+\frac{4\eta\left(\left\|\hat{\mathbf{K}}_{0} \right\|+2n^{2}(\rho+\sqrt{\frac{\nu}{m}})\right)^{2}(1-\mu\gamma)^{Kt}D}{ n^{2}\left(\lambda_{\min}(\hat{\mathbf{K}}_{0})-2n^{2}(\rho+\sqrt{\frac{\nu}{m}}) \right)}\]

Plugging \(T_{1}\) and \(T_{2}\) back to (27), and using the eigenvalue lower bound from Lemma 10 (d) yields:

\[\widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_{t+1}^{j}) \leq\inf_{p>0}\left(1+p\right)4\left(\sqrt{m}\rho+\sqrt{\nu} \right)^{2}\left(\frac{4\eta^{2}\widehat{\mathcal{R}}(\boldsymbol{\theta}^{t}) }{m}+\frac{4\eta^{2}}{m}(1-\mu\gamma)^{Kt}D\right)\] \[+\left(1+\frac{1}{p}\right)\Bigg{[}\left(1-\frac{\eta\lambda_{0}} {n}+4n\eta\left(\rho+\sqrt{\frac{\nu}{m}}\right)\right)\widehat{\mathcal{R}}^ {j}(\boldsymbol{\theta}_{t}^{j})\] \[\qquad\qquad\qquad+\frac{4\eta\left(\left\|\hat{\mathbf{K}}_{0} \right\|+2n^{2}(\rho+\sqrt{\frac{\nu}{m}})\right)^{2}(1-\mu\gamma)^{Kt}D}{n^{2 }\left(\frac{1}{2}\lambda_{0}-2n^{2}(\rho+\sqrt{\frac{\nu}{m}})\right)} \Bigg{]}.\]Now we plug in \(\rho=\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}\)

\[\widehat{\mathcal{R}}^{j}(\bm{\theta}_{t+1}^{j})\leq\inf_{p>0}\left(1 +p\right)4\left(\sqrt{m}\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\nu}\right)^ {2}\left(\frac{4\eta^{2}\widehat{\mathcal{R}}^{j}(\bm{\theta}_{t}^{j})}{m}+ \frac{4\eta^{2}}{m}(1-\mu\gamma)^{Kt}D\right)\\ +\left(1+\frac{1}{p}\right)\Bigg{[}\left(1-\frac{\eta\lambda_{0}} {n}+4n\eta\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right) \right)\widehat{\mathcal{R}}(\bm{\theta}^{t})\\ +\frac{4\eta\left(\left\|\hat{\mathbf{K}}_{0}\right\|+2n^{2}( \frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}})\right)^{2}(1-\mu \gamma)^{Kt}D}{n^{2}\left(\frac{1}{2}\lambda_{0}-2n^{2}(\frac{48nB_{y}}{ \lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}})\right)}\Bigg{]}.\] (29)

According to our choice of \(m\):

\[m\geq\left(\frac{48nB_{y}}{\lambda_{0}}+\sqrt{\nu}\right)^{2}\frac{256n^{4}}{ \lambda_{0}^{2}}\]

we know that

\[4n\eta\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu }{m}}\right)\leq\frac{\eta\lambda_{0}}{4n},\] \[4\left(\sqrt{m}\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\nu} \right)^{2}\frac{4\eta^{2}\widehat{\mathcal{R}}(\bm{\theta}^{t})}{m}\leq\frac {\eta^{2}\lambda_{0}^{2}}{16n^{4}}\widehat{\mathcal{R}}(\bm{\theta}^{t}),\] \[2n^{2}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu }{m}}\right)\leq\frac{\lambda_{0}}{8}.\]

Plugging above inequality back to Eq.(29) yields:

\[\widehat{\mathcal{R}}(\bm{\theta}^{t+1})\leq\inf_{p>0}\left(1+p \right)\left(4\left(\sqrt{m}\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\nu} \right)^{2}\left(\frac{4\eta^{2}}{m}(1-\mu\gamma)^{Kt}D\right)+\frac{\eta^{2} \lambda_{0}^{2}}{16n^{4}}\widehat{\mathcal{R}}(\bm{\theta}^{t})\right)\\ +\left(1+\frac{1}{p}\right)\left(1-\frac{3\eta\lambda_{0}}{4n} \right)\widehat{\mathcal{R}}(\bm{\theta}^{t})+\left(1+\frac{1}{p}\right)\frac{ 4\eta\left(\left\|\hat{\mathbf{K}}_{0}\right\|+\frac{\lambda_{0}}{8}\right)^{ 2}(1-\mu\gamma)^{Kt}D}{\frac{3}{8}\lambda_{0}n^{2}}.\]

According to the fact that:

\[\left(1-\frac{1}{a}\right)\left(1+\frac{1}{2a-1}\right)=\frac{2a-2}{2a-1}\leq 1 -\frac{1}{2a}\]

we choose \(p=\frac{8n}{3\eta\lambda_{0}}-1\), and it yields:

\[\widehat{\mathcal{R}}(\bm{\theta}^{t+1})\leq\frac{8n}{3\eta\lambda _{0}}\left(4\left(\frac{48nB_{y}}{\lambda_{0}}+\sqrt{\nu}\right)^{2}\left( \frac{4\eta^{2}}{m}(1-\mu\gamma)^{Kt}D\right)+\frac{\eta^{2}\lambda_{0}^{2}}{16 n^{4}}\widehat{\mathcal{R}}(\bm{\theta}^{t})\right)\\ +\left(1-\frac{3\eta\lambda_{0}}{8n}\right)\widehat{\mathcal{R}}( \bm{\theta}^{t})+\left(1+\frac{1}{\frac{8n}{3\eta\lambda_{0}}-1}\right)\frac{4 \eta\left(\left\|\hat{\mathbf{K}}_{0}\right\|+\frac{\lambda_{0}}{8}\right)^{ 2}(1-\mu\gamma)^{Kt}D}{\frac{3}{8}\lambda_{0}n^{2}}.\]Using the fact \(\eta\leq\frac{n}{\lambda_{0}}\) yields:

\[\widehat{\mathcal{R}}(\boldsymbol{\theta}^{t+1}) \leq\frac{8n}{3\eta\lambda_{0}}\left(4\left(\frac{48dnB_{y}}{ \lambda_{0}}+\sqrt{\nu}\right)^{2}\left(\frac{4\eta^{2}}{m}(1-\mu\gamma)^{Kt}D \right)\right)\] \[\quad+\left(1-\frac{3\eta\lambda_{0}}{8n}+\frac{\eta\lambda_{0}}{ 6n^{3}}\right)\widehat{\mathcal{R}}(\boldsymbol{\theta}^{t})+\frac{8\eta \left(\left\|\hat{\mathbf{K}}_{0}\right\|+\frac{\lambda_{0}}{8}\right)^{2}(1- \mu\gamma)^{Kt}D}{\frac{3}{8}\lambda_{0}n^{2}}\] \[\leq\left(\frac{48nB_{y}}{\lambda_{0}}+\sqrt{\nu}\right)^{2} \left(\frac{128n\eta}{3\lambda_{0}m}(1-\mu\gamma)^{Kt}D\right)\] \[\quad+\left(1-\frac{3\eta\lambda_{0}}{8n}+\frac{\eta\lambda_{0}}{ 6n^{3}}\right)\widehat{\mathcal{R}}(\boldsymbol{\theta}^{t})+\frac{8\eta \left(\left\|\hat{\mathbf{K}}_{0}\right\|+\frac{\lambda_{0}}{8}\right)^{2}(1- \mu\gamma)^{Kt}D}{\frac{3}{8}\lambda_{0}n^{2}}.\]

Since \(n^{2}\geq\frac{4}{3}\), we know \(\frac{\eta\lambda_{0}}{6n^{3}}\leq\frac{\eta\lambda_{0}}{8n}\). Hence we have:

\[\widehat{\mathcal{R}}(\boldsymbol{\theta}^{t+1})\] \[\leq\left(\frac{48dnB_{y}}{\lambda_{0}}+\sqrt{\nu}\right)^{2} \left(\frac{128n\eta}{3\lambda_{0}m}(1-\mu\gamma)^{Kt}D\right)\] \[\quad+\left(1-\frac{\eta\lambda_{0}}{4n}\right)\widehat{\mathcal{ R}}(\boldsymbol{\theta}^{t})+\frac{8\eta\left(\frac{1}{2}\left\|\mathbf{K} \right\|+\frac{\lambda_{0}}{8}\right)^{2}(1-\mu\gamma)^{Kt}D}{\frac{3}{8} \lambda_{0}n^{2}}\] \[=\left(1-\frac{\eta\lambda_{0}}{4n}\right)\widehat{\mathcal{R}}( \boldsymbol{\theta}^{t})+\frac{4n}{\eta\lambda_{0}}\left(\left(\frac{48nB_{y} }{\lambda_{0}}+\sqrt{\nu}\right)^{2}\frac{32\eta^{2}}{3m}+\frac{2\eta^{2} \left(\frac{1}{2}\left\|\mathbf{K}\right\|+\frac{\lambda_{0}}{8}\right)^{2}}{ \frac{3}{8}n^{2}}\right)(1-\mu\gamma)^{Kt}D.\]

Plugging lower bound of \(m\geq\left(\frac{48nB_{y}}{\lambda_{0}}+\sqrt{\nu}\right)^{2}\frac{256n^{4}}{ \lambda_{0}^{2}}\) yields:

\[\widehat{\mathcal{R}}(\boldsymbol{\theta}^{t+1})\leq\left(1-\frac{\eta \lambda_{0}}{4n}\right)\widehat{\mathcal{R}}(\boldsymbol{\theta}^{t})+\frac{4 n}{\eta\lambda_{0}}\left(\frac{\lambda_{0}^{2}\eta^{2}}{24n^{4}}+\frac{2\eta^{2} \left(\frac{1}{2}\left\|\mathbf{K}\right\|+\frac{\lambda_{0}}{8}\right)^{2}} {\frac{3}{8}n^{2}}\right)(1-\mu\gamma)^{Kt}D.\]

Finally, according to inductive hypothesis **(I)**, we plug in the convergence rate for \(\widehat{\mathcal{R}}(\boldsymbol{\theta}^{t})\) and get

\[\widehat{\mathcal{R}}(\boldsymbol{\theta}^{t+1}) \leq\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{t}\widehat{\mathcal{ R}}(\boldsymbol{\theta}_{0})+\left(1-\frac{\eta\lambda_{0}}{4n}\right) \sum_{s=0}^{t-1}\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{t-1-s}(1-\mu\gamma )^{s}C(1-\mu\gamma)^{K}\,D\] \[+\frac{4n}{\eta\lambda_{0}}\left(\frac{\lambda_{0}^{2}\eta^{2}}{2 4n^{4}}+\frac{2\eta^{2}\left(\frac{1}{2}\left\|\mathbf{K}\right\|+\frac{\lambda _{0}}{8}\right)^{2}}{\frac{3}{8}n^{2}}\right)(1-\mu\gamma)^{Kt}D\] \[\leq\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{t}\widehat{ \mathcal{R}}(\boldsymbol{\theta}_{0})+C\sum_{s=0}^{t-1}\left(1-\frac{\eta \lambda_{0}}{4n}\right)^{t-s}(1-\mu\gamma)^{s}(1-\mu\gamma)^{K}\,D\] \[+\frac{4n}{\eta\lambda_{0}}\left(\frac{\lambda_{0}^{2}\eta^{2}}{2 4n^{4}}+\frac{2\eta^{2}\left(\frac{1}{2}\left\|\mathbf{K}\right\|+\frac{ \lambda_{0}}{8}\right)^{2}}{\frac{3}{8}n^{2}}\right)\left(1-\frac{\eta \lambda_{0}}{4n}\right)^{t-t}(1-\mu\gamma)^{t}(1-\mu\gamma)^{K}D\]

Now we just need to ensure that

\[\frac{\lambda_{0}^{2}\eta^{2}}{24n^{4}}+\frac{2\eta^{2}\left(\frac{1}{2}\left\| \mathbf{K}\right\|+\frac{\lambda_{0}}{8}\right)^{2}}{\frac{3}{8}n^{2}}\leq C.\]

We hence choose \(C\) as:

\[C=\frac{\lambda_{0}^{2}\eta^{2}}{24n^{4}}+\frac{4\eta^{2}\left(\frac{1}{2} \left\|\mathbf{K}\right\|+\frac{\lambda_{0}}{8}\right)^{2}}{\frac{3}{8}n^{2}}\]

[MISSING_PAGE_FAIL:42]

We define the following stacked vector representation :

\[\mathbf{h}_{t+1}^{j}:=[{h_{t+1}^{j}}^{\top}(\boldsymbol{\alpha}_{1}),...,{h_{t+1}^{j}}^{\top}(\boldsymbol{\alpha}_{n})]^{\top}\in\mathbb{R}^{n},\] \[\mathbf{r}_{t}:=[r_{t}^{\top}(\boldsymbol{\alpha}_{1}),...,r_{t}^ {\top}(\boldsymbol{\alpha}_{n})]^{\top}\in\mathbb{R}^{n}.\]

Hence we can write the pointwise difference into the following compact form:

\[\mathbf{h}_{t+1}^{j}-\mathbf{h}_{t+1}^{\text{rf},j} =\left(\mathbf{I}-\frac{2\eta}{n}\boldsymbol{\Phi}_{0}^{j\top} \boldsymbol{\Phi}_{0}^{j}\right)(\mathbf{h}_{t}^{j}-\mathbf{h}_{t}^{\text{rf},j})\] \[+\underbrace{(\boldsymbol{\Phi}_{t+1}^{j}}^{\top}-\boldsymbol{ \Phi}_{t}^{j\top})\boldsymbol{\theta}_{t+1}^{j}\] \[+\eta\underbrace{\boldsymbol{\Phi}_{t}^{j\top}(\nabla_{ \boldsymbol{\theta}_{t}^{j}}\widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_{t }^{j})-\mathbf{g}_{t}^{j})}_{T_{2}}\] \[+\eta\underbrace{\frac{2}{n}\left(\boldsymbol{\Phi}_{0}^{j\top} \boldsymbol{\Phi}_{0}^{j}-\boldsymbol{\Phi}_{t}^{j\top}\boldsymbol{\Phi}_{t}^{ j}\right)}_{T_{3}}\mathbf{r}_{t}.\]

Unrolling the recursion to \(t=0\) yields:

\[\mathbf{h}_{t+1}^{j}-\mathbf{h}_{t+1}^{\text{rf},j} =\sum_{s=0}^{t}(\mathbf{I}-\frac{2\eta}{n}\hat{\mathbf{K}}_{0}^{ j})^{t-s}(\boldsymbol{\Phi}_{s+1}^{j\top}-\boldsymbol{\Phi}_{s}^{j\top}) \boldsymbol{\theta}_{t+1}^{j}+\eta\sum_{s=0}^{t}(\mathbf{I}-\frac{2\eta}{n} \hat{\mathbf{K}}_{0}^{j})^{t-s}\boldsymbol{\Phi}_{s}^{j\top}(\nabla_{ \boldsymbol{\theta}_{t}^{j}}\widehat{\mathcal{R}}(\boldsymbol{\theta}_{s}^{j}) -\mathbf{g}_{s}^{j})\] \[\quad+\eta\sum_{s=0}^{t}(\mathbf{I}-\frac{2\eta}{n}\hat{\mathbf{ K}}_{0}^{j})^{t-s}\frac{2}{n}\left(\hat{\mathbf{K}}_{0}^{j}-\hat{\mathbf{K}}_{s} ^{j}\right)\mathbf{r}_{s}.\]

Taking spectral norm on both sides yields:

\[\left\|\mathbf{h}_{t+1}^{j}-\mathbf{h}_{t+1}^{\text{rf},j}\right\| =\left\|\underbrace{\sum_{s=0}^{t}(\mathbf{I}-\frac{2\eta}{n} \hat{\mathbf{K}}_{0}^{j})^{t-s}(\boldsymbol{\Phi}_{s+1}^{j}}^{\top}- \boldsymbol{\Phi}_{s}^{j\top})\boldsymbol{\theta}_{t+1}^{j}\right\|\] \[\quad+\eta\underbrace{\left\|\sum_{s=0}^{t}(\mathbf{I}-\frac{2 \eta}{n}\hat{\mathbf{K}}_{0}^{j})^{t-s}\mathbf{g}_{s}^{j\top}(\nabla_{ \boldsymbol{\theta}_{t}^{j}}\widehat{\mathcal{R}}^{j}(\boldsymbol{\theta}_{s}^ {j})-\mathbf{g}_{s}^{j})\right\|}_{T_{2}}\] \[\quad+\eta\underbrace{\left\|\sum_{s=0}^{t}(\mathbf{I}-\frac{2 \eta}{n}\hat{\mathbf{K}}_{0}^{j})^{t-s}\frac{2}{n}\left(\hat{\mathbf{K}}_{0}^{ j}-\hat{\mathbf{K}}_{s}^{j}\right)\mathbf{r}_{s}\right\|}_{T_{3}}.\]

We first bound \(T_{1}\) as follows:

\[\left\|\sum_{s=0}^{t}\left(\mathbf{I}-\frac{2\eta}{n}\hat{ \mathbf{K}}_{0}^{j}\right)^{t-s}(\boldsymbol{\Phi}_{s+1}^{j\top}-\boldsymbol{ \Phi}_{s}^{j\top})\boldsymbol{\theta}_{t+1}^{j}\right\| \leq\] \[\leq \sum_{s=0}^{t}\left(1-\frac{2\eta}{n}\lambda_{\min}(\hat{ \mathbf{K}}_{0}^{j})\right)^{t-s}\left\|(\boldsymbol{\Phi}_{s+1}^{j\top}- \boldsymbol{\Phi}_{s}^{j\top})\boldsymbol{\theta}_{t+1}^{j}\right\|\] \[\leq \sum_{s=0}^{t}\left(1-\frac{\lambda_{0}\eta}{n}\right)^{t-s} \left\|(\boldsymbol{\Phi}_{s+1}^{j\top}-\boldsymbol{\Phi}_{s}^{j\top}) \boldsymbol{\theta}_{t+1}^{j}\right\|\]where we apply triangle inequality and Cauchy-Schwartz inequality. To bound \(\left\|(\bm{\Phi}_{s+1}^{\top}-\bm{\Phi}_{s}^{\top})\bm{\theta}_{t+1}\right\|\), we evoke Eq.(28)):

\[\left\|(\bm{\Phi}_{s+1}^{j}{}^{\top}-\bm{\Phi}_{s}^{j}{}^{\top})\bm{\theta}_{t+1 }^{j}\right\|\leq 2\sqrt{n}\left(\frac{48nB_{y}}{\lambda_{0}}+\sqrt{\nu} \right)\left(\frac{2\eta\sqrt{\widehat{\mathcal{R}}^{j}(\bm{\theta}_{s}^{j}) }}{\sqrt{m}}+\frac{2\eta}{\sqrt{m}}\sqrt{(1-\mu\gamma)^{Ks}D}\right)\]

Now we plug in the convergence rate from Theorem 5, and obtain

\[\left\|(\bm{\Phi}_{s+1}^{j}{}^{\top}-\bm{\Phi}_{s}^{j}{}^{\top}) \bm{\theta}_{t+1}^{j}\right\|\] \[\leq 2\sqrt{n}\left(\frac{48nB_{y}}{\lambda_{0}}+\sqrt{\nu} \right)\left(\frac{2\eta\sqrt{\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{s} \widehat{\mathcal{R}}^{j}(\bm{\theta}_{0}^{j})+\frac{4nC(1-\mu\gamma)^{K}D}{ \eta\lambda_{0}}}}{\sqrt{m}}+\frac{2\eta}{\sqrt{m}}\sqrt{(1-\mu\gamma)^{Ks}D}\right)\]

Putting pieces together yields:

\[T_{1} \leq 2\sqrt{n}\left(\frac{48nB_{y}}{\lambda_{0}}+\sqrt{\nu} \right)\sum_{s=0}^{t}(1-\frac{\lambda_{0}\eta}{n})^{t-s}\] \[\quad\times\left(\frac{2\eta\sqrt{\left(1-\frac{\eta\lambda_{0}}{ 4n}\right)^{s}\widehat{\mathcal{R}}(\bm{\theta}_{0})+\frac{4nC(1-\mu\gamma)^{K} D}{\eta\lambda_{0}}}}{\sqrt{m}}+\frac{2\eta}{\sqrt{m}}\sqrt{(1-\mu\gamma)^{Ks}D}\right)\] \[\leq\frac{4\eta\sqrt{n}}{\sqrt{m}}\left(\frac{48nB_{y}}{\lambda_{ 0}}+\sqrt{\nu}\right)\sum_{s=0}^{t}(1-\frac{\lambda_{0}\eta}{n})^{t-s}\] \[\quad\times\left(\sqrt{\left(1-\frac{\eta\lambda_{0}}{4n}\right)^ {s}\widehat{\mathcal{R}}(\bm{\theta}_{0})+\frac{4nC\left(1-\mu\gamma\right)^{K }D}{\eta\lambda_{0}}}+\sqrt{(1-\mu\gamma)^{Ks}D}\right)\] \[\leq\frac{4\eta\sqrt{n}}{\sqrt{m}}\left(\frac{48nB_{y}}{\lambda_{ 0}}+\sqrt{\nu}\right)(1-\frac{\lambda_{0}\eta}{n})^{\frac{s}{2}}\] \[\quad\times\sum_{s=0}^{t}(1-\frac{\lambda_{0}\eta}{n})^{\frac{s} {2}-s}\left(\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{\frac{s}{2}}\sqrt{ \widehat{\mathcal{R}}(\bm{\theta}_{0})}+\sqrt{\frac{4nC\left(1-\mu\gamma\right) ^{K}D}{\eta\lambda_{0}}}+(1-\mu\gamma)^{\frac{s}{2}}\sqrt{(1-\mu\gamma)^{K}D} \right).\]

Using the fact that \(1-\frac{\lambda_{0}\eta}{n}\leq 1-\frac{\lambda_{0}\eta}{4n}\) and our choice of learning rate such that \(1-\mu\gamma\leq 1-\frac{\lambda_{0}\eta}{4n}\), we have:

\[T_{1} \leq\frac{\sqrt{n}4\eta}{\sqrt{m}}\left(\frac{48nB_{y}}{\lambda_{ 0}}+\sqrt{\nu}\right)\] \[\quad\times\left((1-\frac{\lambda_{0}\eta}{n})^{\frac{s}{2}} \frac{8n}{\lambda_{0}\eta}\sqrt{\widehat{\mathcal{R}}^{j}(\bm{\theta}_{0}^{j}) }+\frac{4n}{\lambda_{0}\eta}\sqrt{\frac{4nC\left(1-\mu\gamma\right)^{K}D}{ \eta\lambda_{0}}}+(1-\frac{\lambda_{0}\eta}{n})^{\frac{s}{2}}\frac{8n}{ \lambda_{0}\eta}\sqrt{(1-\mu\gamma)^{K}D}\right).\]

For \(T_{2}\), similarly, we apply triangle and Cauchy-Schwartz inequality:

\[T_{2}\leq\eta\sum_{s=0}^{t}\left(1-\frac{\eta\lambda_{0}}{n}\right)^{t-s}\left\| \bm{\Phi}_{s}^{j}{}^{\top}(\nabla_{\bm{\theta}_{t}^{j}}\widehat{\mathcal{R}}^ {j}(\bm{\theta}_{s}^{j})-\mathbf{g}_{s}^{j})\right\|\]

To bound \(\left\|\bm{\Phi}_{s}^{j}{}^{\top}(\nabla_{\bm{\theta}_{t}^{j}}\widehat{ \mathcal{R}}^{j}(\bm{\theta}_{s}^{j})-\mathbf{g}_{s})\right\|\), we evoke Lemma 11:\[\left\|{\bm{\Phi}_{s}^{j}}^{\top}(\nabla_{\bm{\theta}_{i}^{j}}\widehat{\mathcal{R} }^{j}(\bm{\theta}_{s}^{j})-\mathbf{g}_{s}^{j})\right\|\leq\frac{2}{n}\left( \left\|\hat{\mathbf{K}}_{0}^{j}\right\|+2n^{2}(\rho+\sqrt{\frac{\nu}{m}}) \right)\sqrt{(1-\mu\gamma)^{Ks}D}.\]

Putting the above inequality back to \(T_{2}\) yields:

\[T_{2} \leq\eta\sum_{s=0}^{t}\left(1-\frac{\eta\lambda_{0}}{n}\right)^{t- s}\frac{2}{n}\left(\frac{3}{2}\left\|\mathbf{K}\right\|+2n^{2}(\rho+\sqrt{\frac{ \nu}{m}})\right)\sqrt{(1-\mu\gamma)^{Ks}D}\] \[\leq\eta\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{\frac{t}{2}} \sum_{s=0}^{t}\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{\frac{t}{2}-\frac{s}{ 2}}\frac{2}{n}\left(\frac{3}{2}\left\|\mathbf{K}\right\|+2n^{2}\left(\frac{48 nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)\right)\sqrt{(1-\mu \gamma)^{K}D}\] \[\leq\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{\frac{t}{2}}\frac {16}{\lambda_{0}}\left(\frac{3}{2}\left\|\mathbf{K}\right\|+2n^{2}\left(\frac{4 8nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)\right)\sqrt{(1-\mu \gamma)^{K}D}\]

For \(T_{3}\), we also apply Cauchy-Schwartz and get:

\[T_{3} \leq\eta\sum_{s=0}^{t}(1-\frac{\eta\lambda_{0}}{n})^{t-s}\left\| \frac{2}{n}\left(\hat{\mathbf{K}}_{0}^{j}-\hat{\mathbf{K}}_{s}^{j}\right) \mathbf{r}_{s}\right\|\] \[\leq\eta\frac{2}{n}\sum_{s=0}^{t}(1-\frac{\eta\lambda_{0}}{n})^{t -s}\left\|\left(\hat{\mathbf{K}}_{0}^{j}-\hat{\mathbf{K}}_{s}^{j}\right) \right\|\left\|\mathbf{r}_{s}\right\|\] \[\leq\eta\frac{2}{n}\sum_{s=0}^{t}(1-\frac{\eta\lambda_{0}}{n})^{t -s}2n^{2}(\rho+\sqrt{\frac{\nu}{m}})\sqrt{n}\sqrt{\widehat{\mathcal{R}}^{j}( \bm{\theta}_{t}^{j})}\]

where at last step we plug in empirical gram matrix bound from Lemma 10. Now it suffices to plug in bound for \(\widehat{\mathcal{R}}(\bm{\theta}_{t})\)

\[T_{3} \leq\eta\frac{2}{n}\sum_{s=0}^{t}(1-\frac{\eta\lambda_{0}}{n})^{t -s}2n^{2}(\rho+\sqrt{\frac{\nu}{m}})\sqrt{n}\sqrt{\left(1-\frac{\eta\lambda_{ 0}}{4n}\right)^{s}\widehat{\mathcal{R}}^{j}(\bm{\theta}_{0}^{j})+\frac{4n\left( 1-\mu\gamma\right)^{K}D}{\eta\lambda_{0}}}\] \[\leq\eta\frac{2}{n}2n^{2}(\rho+\sqrt{\frac{\nu}{m}})\sqrt{n} \left((1-\frac{\eta\lambda_{0}}{n})^{\frac{t}{2}}\sum_{s=0}^{t}(1-\frac{\eta \lambda_{0}}{n})^{\frac{t}{2}-\frac{s}{2}}\sqrt{\widehat{\mathcal{R}}^{j}( \bm{\theta}_{0}^{j})}+\frac{8n}{\eta\lambda_{0}}\sqrt{\frac{4n\left(1-\mu \gamma\right)^{K}D}{\eta\lambda_{0}}}\right)\] \[\leq 4\eta\sqrt{n}n\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+ \sqrt{\frac{\nu}{m}}\right)\left((1-\frac{\eta\lambda_{0}}{4n})^{\frac{t}{2}} \frac{8n}{\eta\lambda_{0}}\sqrt{\widehat{\mathcal{R}}^{j}(\bm{\theta}_{0}^{j}) }+\frac{8n}{\eta\lambda_{0}}\sqrt{\frac{4n\left(1-\mu\gamma\right)^{K}D}{\eta \lambda_{0}}}\right).\]

Putting pieces together yields:

\[\left\|\mathbf{h}_{t+1}^{j}-\mathbf{h}_{t+1}^{\text{rf},j}\right\| \leq\frac{\sqrt{n}4\eta}{\sqrt{m}}\left(\frac{48nB_{y}}{\lambda_{0}}+\sqrt{ \nu}\right)\] \[\quad\times\left((1-\frac{\lambda_{0}\eta}{4n})^{\frac{t}{2}} \frac{8n}{\lambda_{0}\eta}\sqrt{\widehat{\mathcal{R}}^{j}(\bm{\theta}_{0}^{j})}+ \frac{4n}{\lambda_{0}\eta}\sqrt{\frac{4nC\left(1-\mu\gamma\right)^{K}D}{\eta \lambda_{0}}}+(1-\frac{\lambda_{0}\eta}{4n})^{\frac{t}{2}}\frac{8n}{\lambda_{0 }\eta}\sqrt{(1-\mu\gamma)^{K}D}\right)\] \[\quad+\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{\frac{t}{2}} \frac{16}{\lambda_{0}}\left(\frac{3}{2}\left\|\mathbf{K}\right\|+2n^{2}\left( \frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)\right)\sqrt{( 1-\mu\gamma)^{K}D}\] \[\quad+4\eta\sqrt{n}n\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+ \sqrt{\frac{\nu}{m}}\right)\left((1-\frac{\eta\lambda_{0}}{4n})^{\frac{t}{2}} \frac{8n}{\eta\lambda_{0}}\sqrt{\widehat{\mathcal{R}}^{j}(\bm{\theta}_{0}^{j})}+ \frac{8n}{\eta\lambda_{0}}\sqrt{\frac{4n\left(1-\mu\gamma\right)^{K}D}{\eta \lambda_{0}}}\right).\]

Let \(\tilde{n}:=\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)\)\[\left\|\mathbf{h}_{t+1}^{j}-\mathbf{h}_{t+1}^{\mathrm{rf},j}\right\| \leq 4\sqrt{n}\tilde{n}\Bigg{(}\left(1-\frac{\eta\lambda_{0}}{4n} \right)^{\frac{j}{2}}\frac{8n}{\lambda_{0}}\sqrt{\widehat{\mathcal{R}}^{j}( \boldsymbol{\theta}_{0}^{j})}\] \[\qquad\qquad\qquad+\frac{4n}{\lambda_{0}}\sqrt{\frac{4nC\left(1- \mu\gamma\right)^{K}D}{\eta\lambda_{0}}}+\left(1-\frac{\eta\lambda_{0}}{4n} \right)^{\frac{j}{2}}\frac{8n}{\lambda_{0}}\sqrt{(1-\mu\gamma)^{K}D}\Bigg{)}\] \[+\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{\frac{j}{2}}\frac{16} {\lambda_{0}}\left(\frac{3}{2}\left\|\mathbf{K}\right\|+2n^{2}\tilde{n}\right) \sqrt{(1-\mu\gamma)^{K}D}\] \[+4\sqrt{n}n\tilde{n}\left(\left(1-\frac{\eta\lambda_{0}}{4n} \right)^{\frac{j}{2}}\frac{8n}{\lambda_{0}}\sqrt{\widehat{\mathcal{R}}^{j}( \boldsymbol{\theta}_{0}^{j})}+\frac{8n}{\lambda_{0}}\sqrt{\frac{4nC\left(1- \mu\gamma\right)^{K}D}{\eta\lambda_{0}}}\right)\] \[\leq\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{\frac{j}{2}}\frac{ 16}{\lambda_{0}}\left(\left(\frac{3}{2}\left\|\mathbf{K}\right\|+2n^{2}\tilde{ n}+2\sqrt{n}n\tilde{n}\right)\sqrt{(1-\mu\gamma)^{K}D}+\left(2\sqrt{n}n^{2} \tilde{n}+2\sqrt{n}n\tilde{n}\right)\sqrt{\widehat{\mathcal{R}}(\boldsymbol{ \theta}_{0})}\right)\] \[\quad+\left(2\sqrt{n}\tilde{n}+4\sqrt{n}n\tilde{n}\right)\frac{8n }{\lambda_{0}}\sqrt{\frac{4nC\left(1-\mu\gamma\right)^{K}D}{\eta\lambda_{0}}}.\]

**Lemma 13** (Neural network parameter and RF parameter coupling).: _For Algorithm 2, the following statement holds for any \(j\in[d]\) and \(t\in[T]\):_

\[\left\|\boldsymbol{\theta}_{t}^{j}-\bar{\boldsymbol{\theta}}_{t}^ {j}\right\|^{2} \leq O\left(\frac{n^{2}}{\lambda_{0}^{2}}\left(\frac{48nB_{y}}{ \lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)+\frac{n^{6}}{\lambda_{0}^{4} }\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)^{2}\right)\] \[\quad+O\left(\eta^{3}T^{2}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{ m}}+\sqrt{\frac{\nu}{m}}\right)^{2}\frac{n^{3}(1-\frac{1}{\kappa})^{K}D}{ \lambda_{0}^{3}}(\lambda_{\max}+\lambda_{0})^{2}+\frac{(1-\frac{1}{\kappa})^ {K}D}{\lambda_{0}^{2}}\right).\]

Proof.: According to updating rule of neural network and random feature predictor, we have

\[\boldsymbol{\theta}_{t+1}^{j}-\bar{\boldsymbol{\theta}}_{t+1}^{j}\] \[=\boldsymbol{\theta}_{t}^{j}-\bar{\boldsymbol{\theta}}_{t}^{j}- \eta(\mathbf{g}_{t}^{j}-\nabla_{\bar{\boldsymbol{\theta}}_{t}^{j}}\mathcal{R}^ {\mathrm{rf},j}(\bar{\boldsymbol{\theta}}_{t}^{j}))\] \[=\boldsymbol{\theta}_{t}^{j}-\bar{\boldsymbol{\theta}}_{t}^{j}- \eta\frac{2}{n}\sum_{i=1}^{n}(\boldsymbol{\phi}_{t}^{j}(\boldsymbol{\alpha}_{i })\mathbf{r}_{t}^{j}(\boldsymbol{\alpha}_{i})-\boldsymbol{\phi}_{0}^{j}( \boldsymbol{\alpha}_{i})\mathbf{r}_{t}^{\mathrm{rf},j}(\boldsymbol{\alpha}_{ i}))-\eta(\mathbf{g}_{t}^{j}-\nabla_{\boldsymbol{\theta}_{t}^{j}}\mathcal{R}^{j}( \bar{\boldsymbol{\theta}}_{t}^{j}))\] \[=\boldsymbol{\theta}_{t}^{j}-\bar{\boldsymbol{\theta}}_{t}^{j}- \eta\frac{2}{n}\sum_{i=1}^{n}(\boldsymbol{\phi}_{t}^{j}(\boldsymbol{\alpha}_{i })\mathbf{r}_{t}^{j}(\boldsymbol{\alpha}_{i})-\boldsymbol{\phi}_{0}( \boldsymbol{\alpha}_{i})\mathbf{r}_{t}^{\mathrm{rf},j}(\boldsymbol{\alpha}_{ i}))-\eta(\mathbf{g}_{t}^{j}-\nabla_{\boldsymbol{\theta}_{t}^{j}}\mathcal{R}^{j}( \bar{\boldsymbol{\theta}}_{t}^{j}))\] \[=\boldsymbol{\theta}_{t}^{j}-\bar{\boldsymbol{\theta}}_{t}^{j}- \eta\frac{2}{n}\sum_{i=1}^{n}(\boldsymbol{\phi}_{t}^{j}(\boldsymbol{\alpha}_{i })-\boldsymbol{\phi}_{0}^{j}(\boldsymbol{\alpha}_{i}))\mathbf{r}_{t}( \boldsymbol{\alpha}_{i})-\eta\frac{2}{n}\sum_{i=1}^{n}\boldsymbol{\phi}_{0}^{j}( \boldsymbol{\alpha}_{i})(\mathbf{r}_{t}(\boldsymbol{\alpha}_{i})-\mathbf{r}_{t}^ {\mathrm{rf}}(\boldsymbol{\alpha}_{i}))-\eta(\mathbf{g}_{t}^{j}-\nabla_{ \boldsymbol{\theta}_{t}^{j}}\mathcal{R}(\bar{\boldsymbol{\theta}}_{t}^{j}))\] \[=\boldsymbol{\theta}_{t}^{j}-\bar{\boldsymbol{\theta}}_{t}^{j}- \eta\frac{2}{n}(\boldsymbol{\Phi}_{t}^{j}(\boldsymbol{\alpha}_{i})- \boldsymbol{\Phi}_{0}^{j}(\boldsymbol{\alpha}_{i}))\mathbf{r}_{t}-\eta\frac{2}{n} \boldsymbol{\Phi}_{0}^{j}(\boldsymbol{\alpha}_{i})(\mathbf{h}_{t}^{j}-\mathbf{h }_{t}^{\mathrm{rf},j})-\eta(\mathbf{g}_{t}^{j}-\nabla_{\boldsymbol{\theta}_{t}^{j} }\mathcal{R}^{j}(\bar{\boldsymbol{\theta}}_{t}^{j})).\]

Now taking norm on both side yields:

\[\left\|\boldsymbol{\theta}_{t+1}^{j}-\bar{\boldsymbol{\theta}}_{t+1}^{j}\right\|= \left\|\boldsymbol{\theta}_{t}^{j}-\bar{\boldsymbol{\theta}}_{t}^{j}\right\|+ \eta\underbrace{\frac{2}{n}\underbrace{\left\|(\boldsymbol{\Phi}_{t}^{j}-\boldsymbol {\Phi}_{0}^{j})\mathbf{r}_{t}^{j}\right\|}}_{T_{1}}+\eta\frac{2}{n}\underbrace{ \left\|\boldsymbol{\Phi}_{0}^{j}(\boldsymbol{\alpha}_{i})(\mathbf{h}_{t}^{j}- \mathbf{h}_{t}^{\mathrm{rf},j})\right\|}_{T_{2}}+\eta\underbrace{\left\| \boldsymbol{\mathcal{g}}_{t}^{j}-\nabla_{\boldsymbol{\theta}_{t}^{j}}\mathcal{R}( \bar{\boldsymbol{\theta}}_{t}^{j})\right\|}_{T_{3}}.\]For \(T_{1}\):

\[T_{1} \leq\left\|(\bm{\Phi}_{t}^{j}-\bm{\Phi}_{0}^{j})\right\|\left\|\mathbf{ r}_{t}^{j}\right\|\] \[\leq\sqrt{n(\rho+\sqrt{\frac{\nu}{m}})}\sqrt{n}\sqrt{\widehat{ \mathcal{R}}^{j}(\bm{\theta}_{t}^{j})}\] \[\leq\sqrt{n(\rho+\sqrt{\frac{\nu}{m}})}\sqrt{n}\sqrt{\left(1- \frac{\eta\lambda_{0}}{4n}\right)^{t}\widehat{\mathcal{R}}^{j}(\bm{\theta}_{0 }^{j})+\frac{4n\left(1-\mu\gamma\right)^{K}D}{\eta\lambda_{0}}}.\]

where we plug in results from Lemma 10 and Theorem 5.

For \(T_{2}\):

\[T_{2} \leq\left\|\bm{\Phi}_{0}^{j}\right\|\left\|\mathbf{h}_{t}^{j}- \mathbf{h}_{t}^{\text{rf},j}\right\|\] \[\leq\sqrt{n}\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{\frac{t}{ 2}}\frac{16}{\lambda_{0}}\left(\left(\frac{3}{2}\left\|\mathbf{K}\right\|+2n^ {2}\tilde{n}+2\sqrt{n}n\tilde{n}\right)\sqrt{(1-\mu\gamma)^{K}D}+\left(2\sqrt{ n}n^{2}\tilde{n}+2\sqrt{n}n\tilde{n}\right)\sqrt{\widehat{\mathcal{R}}(\bm{ \theta}_{0})}\right)\] \[\quad+\sqrt{n}\left(2\sqrt{n}\tilde{n}+4\sqrt{n}n\tilde{n}\right) \frac{8n}{\lambda_{0}}\sqrt{\frac{4nC\left(1-\mu\gamma\right)^{K}D}{\eta \lambda_{0}}}.\]

For \(T_{3}\), we evoke Lemma 11:

\[T_{3}\leq 2\sqrt{(1-\mu\gamma)^{K}D}\leq 2\sqrt{(1-\frac{n\lambda_{0}}{4n})^{ t}(1-\mu\gamma)^{K}D}.\]

Putting pieces together we have:

\[\left\|\bm{\theta}_{t+1}-\bar{\bm{\theta}}_{t+1}\right\|\] \[=\left\|\bm{\theta}_{t}^{j}-\bar{\bm{\theta}}_{t}^{j}\right\|+ \frac{2\eta}{n}\sqrt{n(\rho+\sqrt{\frac{\nu}{m}})}\sqrt{n}\sqrt{\left(1-\frac {\eta\lambda_{0}}{4n}\right)^{t}\widehat{\mathcal{R}}(\bm{\theta}_{0})+\frac {4n\left(1-\mu\gamma\right)^{K}D}{\eta\lambda_{0}}}\] \[\quad+\frac{2\eta}{n}\sqrt{n}\left(1-\frac{\eta\lambda_{0}}{4n} \right)^{\frac{t}{2}}\frac{16}{\lambda_{0}}\] \[\quad\times\left(\left(\frac{3}{2}\left\|\mathbf{K}\right\|+2n^{2 }\tilde{n}+2\sqrt{n}n\tilde{n}\right)\sqrt{(1-\mu\gamma)^{K}D}+\left(2\sqrt{n }n^{2}\tilde{n}+2\sqrt{n}n\tilde{n}\right)\sqrt{\widehat{\mathcal{R}}(\bm{ \theta}_{0})}\right)\] \[\quad+\frac{2\eta}{n}\sqrt{n}\left(2\sqrt{n}\tilde{n}+4\sqrt{n} n\tilde{n}\right)\frac{8n}{\lambda_{0}}\sqrt{\frac{4nC\left(1-\mu\gamma\right)^{K}D}{ \eta\lambda_{0}}}\] \[\quad+\frac{2\eta}{n}2\sqrt{(1-\frac{\eta\lambda_{0}}{4n})^{t}(1 -\mu\gamma)^{K}D}.\]Conducting telescoping sum yields:

\[\left\|\boldsymbol{\theta}_{t+1}-\bar{\boldsymbol{\theta}}_{t+1}\right\|\] \[=\frac{2\eta}{n}\sum_{s=0}^{t+1}n\sqrt{\left(\rho+\sqrt{\frac{ \nu}{m}}\right)}\sqrt{\left(1-\frac{\eta\lambda_{0}}{4n}\right)^{s}\widehat{ \mathcal{R}}^{j}(\boldsymbol{\theta}_{0}^{j})+\frac{4nC\left(1-\mu\gamma \right)^{K}D}{\eta\lambda_{0}}}\] \[\quad+\frac{2\eta}{n}\sum_{s=0}^{t+1}\sqrt{n}\left(1-\frac{\eta \lambda_{0}}{4n}\right)^{\frac{s}{2}}\frac{16}{\lambda_{0}}\] \[\qquad\qquad\times\left(\left(\frac{3}{2}\left\|\mathbf{K} \right\|+2n^{2}\tilde{n}+2\sqrt{n}n\tilde{n}\right)\sqrt{(1-\mu\gamma)^{K}D}+ \left(2\sqrt{n}n^{2}\tilde{n}+2\sqrt{n}n\tilde{n}\right)\sqrt{\widehat{ \mathcal{R}}(\boldsymbol{\theta}_{0})}\right)\] \[\quad+\frac{2\eta}{n}\sum_{s=0}^{t+1}\left(2n\tilde{n}+4n^{2} \tilde{n}\right)\frac{8n}{\lambda_{0}}\sqrt{\frac{4nC\left(1-\mu\gamma\right) ^{K}D}{\eta\lambda_{0}}}\] \[\quad+\frac{2\eta}{n}\sum_{s=0}^{t+1}2\sqrt{(1-\frac{\eta\lambda _{0}}{4n})^{s}(1-\mu\gamma)^{K}D}\] \[\leq n\sqrt{\tilde{n}}\left(\frac{16}{\lambda_{0}}\sqrt{\widehat{ \mathcal{R}}(\boldsymbol{\theta}_{0})}\right)+\eta T\left(\sqrt{\tilde{n}}+ \left(2n\tilde{n}+4n^{2}\tilde{n}\right)\frac{16}{\lambda_{0}}\right)\sqrt{ \frac{4nC\left(1-\mu\gamma\right)^{K}D}{\eta\lambda_{0}}}\] \[\quad+\frac{256\sqrt{n}}{\lambda_{0}^{2}}\left(\left(\frac{3}{2} \left\|\mathbf{K}\right\|+2n^{2}\tilde{n}+2\sqrt{n}n\tilde{n}\right)\sqrt{(1- \mu\gamma)^{K}D}+\left(2\sqrt{n}n^{2}\tilde{n}+2\sqrt{n}n\tilde{n}\right) \sqrt{\widehat{\mathcal{R}}(\boldsymbol{\theta}_{0})}\right)\] \[\quad+\frac{32}{\lambda_{0}}\sqrt{(1-\mu\gamma)^{K}D}\]

Recall that \(\tilde{n}=\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)\) and \(C=\frac{\lambda_{0}^{2}\eta^{2}}{24n^{4}}+\frac{4\eta^{2}\left(\frac{1}{2} \left\|\mathbf{K}\right\|+\frac{\lambda_{0}}{8}\right)^{2}}{\frac{3}{8}n^{2}}\), we conclude that:

\[\left\|\boldsymbol{\theta}_{t+1}-\bar{\boldsymbol{\theta}}_{t+1} \right\|^{2}\] \[\leq O\left(\frac{n^{2}}{\lambda_{0}^{2}}\left(\frac{48nB_{y}}{ \lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)+\eta^{2}T^{2}\left(\frac{48nB _{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)^{2}\frac{n^{4}}{\lambda _{0}^{2}}\frac{n(1-\frac{1}{\kappa})^{K}D}{\eta\lambda_{0}}\frac{\eta^{2}( \lambda_{\max}+\lambda_{0})^{2}}{n^{2}}\right)\] \[\quad+O\left(\frac{n^{6}}{\lambda_{0}^{4}}n^{4}\left(\frac{48nB_{y }}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)^{2}(1-\frac{1}{\kappa})^{ K}D+\frac{n}{\lambda_{0}^{4}}n^{5}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+ \sqrt{\frac{\nu}{m}}\right)^{2}+\frac{(1-\frac{1}{\kappa})^{K}D}{\lambda_{0}^ {2}}\right)\] \[=O\left(\frac{n^{2}}{\lambda_{0}^{2}}\left(\frac{48nB_{y}}{ \lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)+\eta^{3}T^{2}\left(\frac{48nB _{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)^{2}\frac{n^{3}(1-\frac{ 1}{\kappa})^{K}D}{\lambda_{0}^{3}}(\lambda_{\max}+\lambda_{0})^{2}\right)\] \[\quad+O\left(\frac{n^{6}}{\lambda_{0}^{4}}\left(\frac{48nB_{y}}{ \lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)^{2}+\frac{(1-\frac{1}{\kappa })^{K}D}{\lambda_{0}^{2}}\right)\]

**Theorem 6**.: _For Algorithm 2, the following statement holds true for any \(j\in[d]\), \(t\in[T]\):_

\[\sup_{\boldsymbol{\alpha}\in\Delta^{N}}\left(h_{T}^{j}(\boldsymbol {\alpha})-h_{T}^{\mathrm{kls},j}(\boldsymbol{\alpha})\right)^{2}\] \[\leq 2\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}\right)^{2}( \frac{48nB_{y}}{\lambda_{0}}+\sqrt{\nu})^{2}+O\left(\frac{n^{2}}{\lambda_{0}^ {2}}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)+ \frac{n^{6}}{\lambda_{0}^{4}}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+ \sqrt{\frac{\nu}{m}}\right)^{2}\right)\] \[\quad+O\left(\eta^{3}T^{2}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt {m}}+\sqrt{\frac{\nu}{m}}\right)^{2}\frac{n^{3}(1-\frac{1}{\kappa})^{K}D}{ \lambda_{0}^{3}}(\lambda_{\max}+\lambda_{0})^{2}+\frac{(1-\frac{1}{\kappa})^{K }D}{\lambda_{0}^{2}}\right)\]

_If we choose \(K\geq\kappa\log\left(\frac{\eta^{3}T^{2}n^{3}D}{\lambda_{0}^{3}}\right)\), then we have:_

\[\sup_{\boldsymbol{\alpha}\in\Delta^{N}}\left(h_{T}^{j}(\boldsymbol {\alpha})-h_{T}^{\mathrm{kls},j}(\boldsymbol{\alpha})\right)^{2}\leq O\left( \max\left\{\frac{\mathrm{poly}_{3}(n)}{\sqrt{m}},\frac{\mathrm{poly}_{8}(n)}{m} \right\}\right).\]Proof.: We consider the following canonical decomposition:

\[\left(h^{j}(\bm{\alpha})-h^{\mathrm{kls},j}(\bm{\alpha})\right)^{2}\leq 2\left(h_{t}^{j}(\bm{ \alpha})-h_{t}^{\mathrm{rf},j}(\bm{\alpha})\right)^{2}+2\left(h_{t}^{\mathrm{ rf},j}(\bm{\alpha})-h_{t}^{\mathrm{kls},j}(\bm{\alpha})\right)^{2}\]

For the first term, we

\[\left\|h_{t}^{j}(\bm{\alpha})-h_{t}^{\mathrm{rf},j}(\bm{\alpha}) \right\|^{2}\] \[\leq 2\left\|\bm{\phi}_{t}^{j}(\bm{\alpha})^{\top}\bm{\theta}_{t}^ {j}-\bm{\phi}_{0}^{j}(\bm{\alpha})^{\top}\bm{\theta}_{t}^{j}\right\|^{2}+2 \left\|\bm{\phi}_{0}^{j}(\bm{\alpha})^{\top}(\bm{\theta}_{t}^{j}-\bar{\bm{ \theta}}_{t}^{j})\right\|^{2}\] \[\leq 2\rho^{2}(\sqrt{m}\rho+\sqrt{\nu})^{2}+O\left(\frac{n^{2}}{ \lambda_{0}^{2}}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m }}\right)+\frac{n^{6}}{\lambda_{0}^{4}}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{ m}}+\sqrt{\frac{\nu}{m}}\right)^{2}\right)\] \[\quad+O\left(\eta^{3}T^{2}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{ m}}+\sqrt{\frac{\nu}{m}}\right)^{2}\frac{n^{3}(1-\frac{1}{\kappa})^{K}D}{ \lambda_{0}^{3}}(\lambda_{\max}+\lambda_{0})^{2}+\frac{(1-\frac{1}{\kappa})^{ K}D}{\lambda_{0}^{2}}\right)\]

where we plug in Proposition 5 (e) to bound \(\left\|\bm{\phi}_{t}^{j}(\bm{\alpha})^{\top}\bm{\theta}_{t}^{j}-\bm{\phi}_{0}^ {j}(\bm{\alpha})^{\top}\bm{\theta}_{t}^{j}\right\|\), and use Lemma 13 to bound \(\left\|\bm{\phi}_{0}^{j}(\bm{\alpha})^{\top}(\bm{\theta}_{t}^{j}-\bar{\bm{ \theta}}_{t}^{j})\right\|\). Plugging in \(\rho=\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}\) will complete bounding first term:

\[\left\|h_{t}^{j}(\bm{\alpha})-h_{t}^{\mathrm{rf},j}(\bm{\alpha})\right\|^{2}\] \[\leq 2\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}\right)^{2}( \frac{48nB_{y}}{\lambda_{0}}+\sqrt{\nu})^{2}+O\left(\frac{n^{2}}{\lambda_{0}^ {2}}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt{\frac{\nu}{m}}\right)+ \frac{n^{6}}{\lambda_{0}^{4}}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt{m}}+\sqrt {\frac{\nu}{m}}\right)^{2}\right)\] \[\quad+O\left(\eta^{3}T^{2}\left(\frac{48nB_{y}}{\lambda_{0}\sqrt {m}}+\sqrt{\frac{\nu}{m}}\right)^{2}\frac{n^{3}(1-\frac{1}{\kappa})^{K}D}{ \lambda_{0}^{3}}(\lambda_{\max}+\lambda_{0})^{2}+\frac{(1-\frac{1}{\kappa})^{ K}D}{\lambda_{0}^{2}}\right)\]

For the second term, we use the intermediate result from [21]

\[\left\|\mathbf{h}_{t}^{\mathrm{rf},j}(\bm{\alpha})-\mathbf{h}_{t}^{\mathrm{kls },j}(\bm{\alpha})\right\|\leq\sqrt{\frac{\nu}{m}}B_{y}\frac{n}{\lambda_{0}} \left(\frac{24n}{\lambda_{0}}+\frac{1}{2}\right)\]

**Lemma 14** (Approximation of Lipschitz functions on the ball [2, Proposition 6]).: _For \(R\) larger than a constant \(c\) that depends only on \(N\), for any function \(f^{\star}:\mathbb{R}^{N}\to\mathbb{R}\) such that for all \(\mathbf{x},\tilde{\mathbf{x}}\in\mathbb{R}_{q}^{N}\), \(\sup_{\mathbf{x}\in\mathbb{R}_{q}^{N}}|f^{\star}(\mathbf{x})|\leq\Lambda\) and \(|f^{\star}(\mathbf{x})-f^{\star}(\tilde{\mathbf{x}})|\leq\Lambda\|\mathbf{x}- \tilde{\mathbf{x}}\|_{q}\), there exists \(h\in\mathcal{H}\), such that \(||h||_{\mathcal{H}}^{2}\leq R\) and_

\[\sup_{\mathbf{x}\in\mathbb{R}_{q}^{d}(1)}|f^{\star}(\mathbf{x})-h(\mathbf{x})| \leq A(R),\qquad A(R)=c\Lambda\left(\frac{\sqrt{R}}{\Lambda}\right)^{-\frac{2} {d-2}}\ln\left(\frac{\sqrt{R}}{\Lambda}\right)\.\]

**Theorem 7** ([32, Theorem 1]).: _Let \(\mathcal{F}=\left\{f:\mathbb{S}^{d-1}\to[0,b]\right\}\) for some \(b<\infty\). Then with probability at least \(1-e^{-\nu},\nu>0\), for all \(f\in\mathcal{F}\) simultaneously_

\[\|f\|_{2}^{2}\leq\|f\|_{n}^{2}+O\left(\|f\|_{n}\left(\widehat{\mathfrak{R}}( \mathcal{F})+\sqrt{\frac{b\nu}{n}}\right)+\widehat{\mathfrak{R}}^{2}(\mathcal{F} )+\frac{b\nu}{n}\right)\,\]

_where the worst-case empirical Rademacher complexity is defined as_

\[\widehat{\mathfrak{R}}(\mathcal{F})=\sup_{\mathbf{x}_{1},\ldots,\mathbf{x}_{n} \in\mathbb{S}^{d-1}}\mathbb{E}_{\bm{\varepsilon}}\sup_{f\in\mathcal{F}}\left| \frac{1}{n}\sum_{i=1}^{n}\varepsilon_{i}f(\mathbf{x}_{i})\right|\. (\varepsilon_{1},\ldots,\varepsilon_{n}\stackrel{{\mathrm{ iid}}}{{\sim}}\mathrm{unif}\{\pm 1\})\]

**Lemma 15**.: _[Rademacher complexity of kernel predictor class [3] Lemma 22] Consider the following function class with bounded RKHS norm:_

\[\mathcal{F}=\left\{h(\mathbf{x})=\sum_{i=1}^{n}c_{i}k(\mathbf{x},\mathbf{x}_{i} ):h\in\mathcal{H},\left\|h\right\|_{\mathcal{H}}\leq B\right\}.\]

_Then its empirical Rademacher complexity is bounded by:_

\[\widehat{\mathfrak{R}}(\mathcal{F})\leq O\left(\frac{B}{\sqrt{n}}\right).\]

**Lemma 16** (Excess risk of virtual KLS predictor learnt on optimal KLS approximator).: _Let \(h_{*}^{\mathrm{kls},j}\) be the optimal approximator of \(f^{*,j}\) in RKHS, and \(\tilde{h}_{t}^{\mathrm{kls},j}(\bm{\alpha})\) be KLS predictor trained by GD on sample \(\{\bm{\alpha}_{i},h_{*}^{\mathrm{kls},j}(\bm{\alpha}_{i})\}_{i=1}^{n}\). Then the following excess risk bound holds with probability at least \(1-e^{-\nu}\):_

\[\mathbb{E}\left\|\tilde{h}_{t+1}^{\mathrm{kls},j}(\bm{\alpha})-h_{*}^{\mathrm{ kls},j}(\bm{\alpha})\right\|^{2}\leq\left(1-\frac{2\lambda_{0}^{2}\eta}{n} \right)^{t}+O\left(\left(1-\frac{2\lambda_{0}^{2}\eta}{n}\right)^{t}\left( \frac{R}{\sqrt{n}}+\sqrt{\frac{\nu}{n}}\right)+\frac{R^{2}}{n}+\frac{\nu}{n} \right)\,\]

Proof.: Given a fixed function \(h_{*}^{\mathrm{kls},j}\) with \(\left\|h_{*}^{\mathrm{kls},j}\right\|_{\mathcal{H}}^{2}\leq R\), we consider the following class:

\[\widetilde{\mathcal{H}}=\left\{\tilde{h}(\mathbf{x})=h(\mathbf{x})-h_{*}^{ \mathrm{kls},j}(\mathbf{x}):h\in\mathcal{H},\left\|h\right\|_{\mathcal{H}}\leq B\right\}\]

According to constant shift property of Rademacher complexity [33] and Lemma 15, we know

\[\widehat{\mathfrak{R}}(\widetilde{\mathcal{H}})\leq O\left(\frac{B}{\sqrt{n}} +\frac{\sqrt{R}}{\sqrt{n}}\right).\]

According to Theorem 7, we have

\[\mathbb{E}\left\|\tilde{h}_{t}^{\mathrm{kls},j}(\bm{\alpha})-h_{ *}^{\mathrm{kls},j}(\bm{\alpha})\right\|^{2}\leq\frac{1}{n}\sum_{i=1}^{n}\left\| \tilde{h}_{t}^{\mathrm{kls},j}(\bm{\alpha}_{i})-h_{*}^{\mathrm{kls},j}(\bm{ \alpha}_{i})\right\|^{2}\] \[\quad+O\left(\frac{1}{n}\sum_{i=1}^{n}\left\|\tilde{h}_{t}^{ \mathrm{kls},j}(\bm{\alpha}_{i})-h_{*}^{\mathrm{kls},j}(\bm{\alpha}_{i})\right\| ^{2}\left(\frac{B+\sqrt{R}}{\sqrt{n}}+\sqrt{\frac{\nu}{n}}\right)+\frac{(B+ \sqrt{R})^{2}}{n}+\frac{\nu}{n}\right)\,\]

Now, it suffices to bound (i) empirical risk term \(\frac{1}{n}\sum_{i=1}^{n}\left\|\tilde{h}_{t}^{\mathrm{kls},j}(\bm{\alpha}_{i} )-h_{*}^{\mathrm{kls},j}(\bm{\alpha}_{i})\right\|^{2}\) and (ii) RKHS norm of \(\tilde{h}_{t}^{\mathrm{kls},j}\).

For (i), we define \(\widehat{\mathcal{R}}_{t}=\frac{1}{n}\sum_{i=1}^{n}\left\|\tilde{h}_{t}^{ \mathrm{kls},j}(\bm{\alpha}_{i})-h_{*}^{\mathrm{kls},j}(\bm{\alpha}_{i})\right\| ^{2}\) and recall the updating rule of kernel OLS predictor:

\[\tilde{\mathbf{h}}_{t+1}^{\mathrm{kls},j}-\tilde{\mathbf{h}}_{*}^ {\mathrm{kls},j}=\mathbf{Kc}_{t+1}^{j}-\tilde{\mathbf{h}}_{*}^{\mathrm{kls},j} =\mathbf{Kc}_{t}^{j}-\mathbf{K}\frac{2\eta}{n}\mathbf{K}^{\top}( \tilde{\mathbf{h}}_{t}^{\mathrm{kls},j}-\tilde{\mathbf{h}}_{*}^{\mathrm{kls},j})-\tilde{\mathbf{h}}_{*}^{\mathrm{kls},j}\] \[=\left(\mathbf{I}-\frac{2\eta}{n}\mathbf{K}^{\top}\mathbf{K} \right)(\tilde{\mathbf{h}}_{t}^{\mathrm{kls},j}-\tilde{\mathbf{h}}_{*}^{ \mathrm{kls},j})\] \[=\left(\mathbf{I}-\frac{2\eta}{n}\mathbf{K}^{\top}\mathbf{K} \right)^{t}(\tilde{\mathbf{h}}_{0}^{\mathrm{kls},j}-\tilde{\mathbf{h}}_{*}^{ \mathrm{kls},j}).\]

Hence we know that:

\[\widehat{\mathcal{R}}_{t+1}=\left(\mathbf{I}-\frac{2\eta}{n}\mathbf{K}\mathbf{ K}^{\top}\right)^{t}\widehat{\mathcal{R}}_{0}.\]

(ii) RKHS norm of \(\tilde{h}_{t}^{\mathrm{kls},j}\)

\[\left\|\tilde{h}_{t}^{\mathrm{kls},j}\right\|_{\mathcal{H}} =\left\|\left(\mathbf{I}-\left(\mathbf{I}-\frac{2\eta}{n}\mathbf{ K}\right)^{t}\right)\mathbf{y}_{*}\right\|_{\mathbf{K}^{-1}}\] \[=\left\|\left(\mathbf{I}-\left(\mathbf{I}-\frac{2\eta}{n} \mathbf{K}\right)^{t}\right)\mathbf{K}^{-1}\mathbf{Kc}_{*}\right\|\] \[\leq\left\|\mathbf{I}-\left(\mathbf{I}-\frac{2\eta}{n}\mathbf{K} \right)^{t}\right\|\left\|\mathbf{K}^{-1}\mathbf{Kc}_{*}\right\|\] \[=\left\|\mathbf{I}-\left(\mathbf{I}-\frac{2\eta}{n}\mathbf{K} \right)^{t}\right\|\left\|\tilde{h}_{*}^{\mathrm{kls},j}\right\|_{\mathcal{H} }\leq\sqrt{R}.\]

**Lemma 17**.: _[_21_, Lemma 8]_ _Let \(f_{t}^{\kappa},\tilde{f}_{t}^{\kappa}\) be GD-trained KLS predictors (as discussed in appendix B.2) given training samples \((\mathbf{x}_{i},y_{i})_{i=1}^{n}\) and \((\mathbf{x}_{i},\tilde{y}_{i})_{i=1}^{n}\) respectively, where \(y_{i}=f^{\star}(\mathbf{x}_{i})+\varepsilon_{i}\) and \(\tilde{y}_{i}=h(\mathbf{x}_{i})+\varepsilon_{i}\) with \(\|h\|_{\mathcal{H}}^{2}\leq R\) characterized by lemma 14. Then, with \(A(R)\) defined in lemma 14, for any \(t\in\mathbb{N}\),_

\[\|f_{t}^{\kappa}-\tilde{f}_{t}^{\kappa}\|_{n}^{2}\leq A(R)\.\]

### Proof of Theorem 3

Proof.: We notice the following risk decomposition:

\[\mathbb{E}\left\|h_{T}^{j}(\bm{\alpha})-w_{j}^{*}(\bm{\alpha}) \right\|^{2} \leq 4\,\mathbb{E}\left\|h_{T}^{j}(\bm{\alpha})-h_{T}^{\mathrm{ kls},j}(\bm{\alpha})\right\|^{2}+4\,\mathbb{E}\left\|h_{T}^{\mathrm{ kls},j}(\bm{\alpha})-\tilde{h}_{T}^{\mathrm{kls},j}(\bm{\alpha})\right\|^{2}\] \[\quad+4\,\mathbb{E}\left\|\tilde{h}_{t}^{\mathrm{kls},j}(\bm{ \alpha})-h_{*}^{\mathrm{kls},j}(\bm{\alpha})\right\|^{2}+4\,\mathbb{E}\left\|h _{*}^{\mathrm{kls},j}(\bm{\alpha})-w_{j}^{*}(\bm{\alpha})\right\|^{2}\] \[\leq O\left(\max\left\{\frac{\mathrm{poly}_{3}(n)}{\sqrt{m}},\frac {\mathrm{poly}_{8}(n)}{m}\right\}\right)+O\left(\left(1+\frac{c}{\lambda_{0}^{2 }}\right)A(R)^{2}+\frac{c}{\sqrt{n}}\right)\] \[\quad+\left(1-\frac{2\lambda_{0}^{2}\eta}{n}\right)^{T}+O\left( \frac{R}{n}\right)+O\left(\Lambda\left(\frac{\sqrt{R}}{\Lambda}\right)^{-N^{ \frac{2}{2}}}\ln\left(\frac{\sqrt{R}}{\Lambda}\right)\right),\]

where respectively we plug in the NN-KLS coupling result (Theorem 6), Lemma 17, excess risk of KLS (Lemma 16) and approximation error of RKHS to Lipschitz function (Lemma 14). By choosing \(m\geq\Omega(n^{8+\frac{2}{2+N}})\), \(T\geq\Omega\left(\frac{n}{N\lambda_{0}^{2}\eta}\log(n)\right)\) and optimizing over \(R\), we recover the rate:

\[\mathbb{E}\left\|h_{t}^{j}(\bm{\alpha})-w_{j}^{*}(\bm{\alpha})\right\|^{2}\leq O \left(\Lambda^{2}n^{-\frac{2}{2+N}}\right).\]

Hence by summing over all coordinates \(j\in[d]\) we conclude the result:

\[\mathbb{E}\left\|\mathbf{h}_{t}(\bm{\alpha})-\mathbf{w}^{*}(\bm{\alpha})\right\| ^{2}\leq O\left(\Lambda^{2}dn^{-\frac{2}{2+N}}\right)\]

where in our case lemma 1 implies \(\Lambda=\kappa^{*}\). Finally we lower bound \(\lambda_{0}\) using [29, Theorem 3.2] which implies that \(\lambda_{0}=\Omega(\mathrm{polylog}(n,N)N)\) with probability at least \(1-n^{2}e^{-O(\sqrt{N})}\) for a distribution on a simplex.

Regret Analysis of Algorithm 4

In this section, we are going to provide the proof for Theorem 4. Let us first introduce the following definitions that will be used in our proof.

**Definition 2** (Covering and Packing Numbers).: _An \(\varepsilon\)-cover of a set \(S\) w.r.t. some metric \(\|\cdot\|\) is a set \(\{\mathbf{x}^{\prime}_{1},\dots,\mathbf{x}^{\prime}_{n}\}\subseteq S\) such that for each \(\mathbf{x}\in S\) there exists \(i\in\{1,\dots,n\}\) such that \(\|\mathbf{x}-\mathbf{x}^{\prime}_{i}\|\leq\varepsilon\). The covering number \(\mathcal{N}(S,\varepsilon,\|\cdot\|)\) is the smallest cardinality of a \(\varepsilon\)-cover. An \(\varepsilon\)-packing of a set \(S\) w.r.t. some metric \(\|\cdot\|\) is a set \(\{\mathbf{x}^{\prime}_{1},\dots,\mathbf{x}^{\prime}_{m}\}\subseteq S\) such that for any distinct \(i,j\in\{1,\dots,m\}\), we have \(\|\mathbf{x}^{\prime}_{i}-\mathbf{x}^{\prime}_{j}\|>\varepsilon\). The packing number \(\mathcal{M}(S,\varepsilon,\|\cdot\|)\) is the largest cardinality of a \(\varepsilon\)-packing._

It is well known that \(\mathcal{M}(S,2\varepsilon,\|\cdot\|)\leq\mathcal{N}(S,\varepsilon,\|\cdot\|)\leq \mathcal{M}(S,\varepsilon,\|\cdot\|)\).

**Definition 3** (Metric dimension).: _The metric space \((\mathcal{X},\|\cdot\|)\) had metric dimension \(d\), if there exists a constant \(C_{d}\) such that for all \(\varepsilon>0\), \(\mathcal{X}\) has an \(\varepsilon\)-cover at most \(C_{d}\varepsilon^{-d}\)._

Proof.: We start from the standard analysis framework for online non-parametric regression [13; 18; 20]. Let \(S_{t}\) be the value of the variable \(S\) at the end of time \(t\). Hence \(S_{0}=\varnothing\). The functions \(\pi_{t}:\mathcal{X}\to\{1,\dots,t\}\) for \(t=1,2,\dots\) map each data point \(\mathbf{x}\) to its closest center (in norm \(\|\cdot\|\)) in \(S_{t-1}\),

\[\pi_{t}(\boldsymbol{\alpha})=\left\{\begin{array}{ll}\arg\min_{s\in S_{t-1}} \|\boldsymbol{\alpha}-\boldsymbol{\alpha}_{s}\|&\text{if }S_{t-1}\neq\varnothing\\ t&\text{otherwise.}\end{array}\right.\]

The set \(T_{s}\) contain all data points \(\boldsymbol{\alpha}_{t}\) that at time \(t\) belonged to the ball with center \(\boldsymbol{\alpha}_{s}\) and radius \(\varepsilon_{t}\),

\[T_{s}=\{t\,:\,\|\boldsymbol{\alpha}_{t}-\boldsymbol{\alpha}_{s}\|\leq \varepsilon_{t},\,t=s,\dots,T\}\.\]

Finally, \(\mathbf{w}^{\star}_{s}\) is the best fixed prediction for all examples \((\boldsymbol{\alpha}_{t},\mathbf{w}_{t})\) such that \(t\in T_{s}\),

\[\mathbf{w}^{\star}_{s}=\operatorname*{arg\,min}_{\mathbf{w}\in\mathcal{W}} \sum_{t\in T_{s}}\ell_{t}(\mathbf{w})=\frac{1}{|T_{s}|}\sum_{t\in T_{s}} \mathbf{w}_{t}\.\] (30)

We proceed by decomposing the regret into estimation and approximation cumulative errors,

\[R_{T}(f)=\sum_{t=1}^{T}\Big{(}\ell_{t}(\hat{\mathbf{w}}_{t})-\ell_{t}\big{(}f( \boldsymbol{\alpha}_{t})\big{)}\Big{)}=\sum_{t=1}^{T}\Big{(}\ell_{t}(\hat{ \mathbf{w}}_{t})-\ell_{t}\big{(}\mathbf{w}^{\star}_{\pi_{t}(\boldsymbol{ \alpha}_{t})}\big{)}\Big{)}+\sum_{t=1}^{T}\Big{(}\ell_{t}\big{(}\mathbf{w}^{ \star}_{\pi_{t}(\boldsymbol{\alpha}_{t})}\big{)}-\ell_{t}\big{(}f(\boldsymbol {\alpha}_{t})\big{)}\Big{)}\.\]

The estimation term is bounded as

\[\sum_{t=1}^{T}\Big{(}\ell_{t}(\hat{\mathbf{w}}_{t})-\ell_{t}\big{(}\mathbf{w}^ {\star}_{\pi_{t}(\boldsymbol{\alpha}_{t})}\big{)}\Big{)}=\sum_{s\in S_{T}} \sum_{t\in T_{s}}\Big{(}\ell_{t}(\hat{\mathbf{w}}_{t})-\ell_{t}(\mathbf{w}^{ \star}_{s})\Big{)}\]

We study the cumulative regret within a fixed ball \(T_{s}\). We use \(T_{s}(t)\) to denote the set of all points belong to ball with center \(\mathbf{w}_{s}\), at time \(t\). That is,

\[T_{s}(t)=\{j:j\in T_{s},j\leq t\}.\]

One property of \(T_{s}(t)\) is that, if \(\mathbf{x}_{t}\in T_{s}\), then \(|T_{s}(t)|=|T_{s}(t-1)|+1\). We further define \(\tilde{\mathbf{w}}_{t}=\frac{1}{|T_{s}(t)|}\sum_{j\in T_{s}(t)}\mathbf{w}^{ \star}(\boldsymbol{\alpha}_{j})\). According to [6, Lemma 3.1], we have:

\[\sum_{t\in T_{s}}\ell_{t}(\hat{\mathbf{w}}_{t})-\ell_{t}(\mathbf{w}^{\star}_{s })\leq\sum_{t\in T_{s}}\ell_{t}(\hat{\mathbf{w}}_{t})-\ell_{t}(\tilde{\mathbf{ w}}_{t})\]

Recall that \(\hat{\mathbf{w}}_{t}=\frac{1}{|T_{s}(t-1)|}\sum_{j\in T_{s}(t-1)}\mathbf{w}_{j}\). Then,

\[\mathbb{E}[\ell_{t}(\hat{\mathbf{w}}_{t})-\ell_{t}(\tilde{\mathbf{ w}}_{t})] \leq 4D\,\mathbb{E}\,\|\hat{\mathbf{w}}_{t}-\ell_{t}(\tilde{\mathbf{ w}}_{t})\|\] \[=4D\,\mathbb{E}\,\left\|\frac{1}{|T_{s}(t-1)|}\sum_{j\in T_{s}(t-1 )}\mathbb{I}\,\{Z_{j}=1\}\,\mathbf{w}_{j}-\frac{1}{|T_{s}(t)|}\sum_{j\in T_{s }(t)}\mathbf{w}^{\star}(\boldsymbol{\alpha}_{j})\right\|\] \[\leq 4D\,\mathbb{E}\,\left\|\frac{1}{|T_{s}(t-1)|}\sum_{j\in T_{s}(t-1 )}\mathbb{I}\,\{Z_{j}=1\}\,(\mathbf{w}_{j}-\mathbf{w}^{\star}(\boldsymbol{ \alpha}_{j}))\right\|\] \[\quad+4D\,\mathbb{E}\left\|\frac{1}{|T_{s}(t-1)|}\sum_{j\in T_{s}(t -1)}\mathbb{I}\,\{Z_{j}=1\}\,\mathbf{w}^{\star}(\boldsymbol{\alpha}_{j})-\frac {1}{|T_{s}(t)|}\sum_{j\in T_{s}(t)}\mathbf{w}^{\star}(\boldsymbol{\alpha}_{j})\right\|\]According to the convergence rate of GD on strongly-convex and smooth function, the first term is bounded by \(p(1-\frac{1}{\kappa})^{K}D\). For second term:

\[\mathbb{E}\left\|\frac{1}{|T_{s}(t-1)|}\sum_{j\in T_{s}(t-1)} \mathbb{I}\left\{Z_{j}=1\right\}\mathbf{w}^{*}(\boldsymbol{\alpha}_{j})-\frac{ 1}{|T_{s}(t)|}\sum_{j\in T_{s}(t)}\mathbf{w}^{*}(\boldsymbol{\alpha}_{j})\right\|\] \[= \mathbb{E}\left\|\sum_{j\in T_{s}(t-1)}\left(\frac{\mathbb{I} \left\{Z_{j}=1\right\}}{|T_{s}(t-1)|}-\frac{1}{|T_{s}(t)|}\right)\mathbf{w}^{*} (\boldsymbol{\alpha}_{j})\right\|\] \[\leq \sum_{j\in T_{s}(t-1)}\mathbb{E}_{Z_{j}}\left\|\left(\frac{1\left\{ Z_{j}=1\right\}}{|T_{s}(t-1)|}-\frac{1}{|T_{s}(t)|}\right)\mathbf{w}^{*}( \boldsymbol{\alpha}_{j})\right\|\] \[\leq \sum_{j\in T_{s}(t-1)}\left(p\frac{1}{|T_{s}(t-1)||T_{s}(t)|}+(1- p)\frac{1}{|T_{s}(t)|}\right)D\] \[\leq \left(p\frac{1}{|T_{s}(t)|}+(1-p)\right)D\]

where at the second last step we use the fact that \(|T_{s}(t)|=|T_{s}(t-1)|+1\).

\[\sum_{t\in T_{s}}\ell_{t}(\hat{\mathbf{w}}_{t})-\ell_{t}(\mathbf{ w}_{s}^{*}) \leq\sum_{t\in T_{s}}\ell_{t}(\hat{\mathbf{w}}_{t})-\ell_{t}(\tilde {\mathbf{w}}_{t})\] \[\leq 4D\sum_{t\in T_{s}}\left(p\frac{1}{|T_{s}(t)|}+(1-p)\right)D+ 4pD|T_{s}|(1-\frac{1}{\kappa})^{K}D\]

We further sum above statement over all \(s\in S_{T}\):

\[\mathbb{E}\left[\sum_{s\in S_{T}}\sum_{t\in T_{s}}\ell_{t}(\hat{ \mathbf{w}}_{t})-\ell_{t}(\mathbf{w}_{s}^{*})\right] \leq p\sum_{s\in S_{T}}\left(\ln(|T_{s}|)+(1-p)TD+4pD|T_{s}|(1- \frac{1}{\kappa})^{K}D\right)\] \[\leq p|S_{T}|\ln(T)+(1-p)TD+4pDT(1-\frac{1}{\kappa})^{K}D\]

The first inequality is a known bound on the regret under square loss [6, page 43]. We upper bound the size of the final packing \(S_{T}\) as

\[|S_{T}|\leq\mathcal{M}\big{(}B,\varepsilon_{T},\|\cdot\|\big{)}\leq C_{N} \,\varepsilon_{T}^{-N}\]

Thus,

\[\sum_{t=1}^{T}\Big{(}\ell_{t}(\hat{\mathbf{w}}_{t})-\ell_{t}\big{(}\mathbf{w}_ {\pi_{t}(\boldsymbol{\alpha}_{t})}^{*}\big{)}\Big{)}\leq pC_{N}\varepsilon_{T} ^{-N}\ln(T)+(1-p)TD+4pDT(1-\frac{1}{\kappa})^{K}D\] (31)

Next, we bound the approximation term. Using (30) we have

\[\sum_{t=1}^{T}\Big{(}\ell_{t}\big{(}\mathbf{w}_{\pi_{t}(\boldsymbol{\alpha}_{ t})}^{*}\big{)}-\ell_{t}\big{(}f(\boldsymbol{\alpha}_{t})\big{)}\Big{)}\leq\sum_{t=1}^ {T}\Big{(}\ell_{t}\big{(}f(\boldsymbol{\alpha}_{\pi_{t}(\boldsymbol{\alpha}_{t} )})\big{)}-\ell_{t}\big{(}f(\boldsymbol{\alpha}_{t})\big{)}\Big{)}\,.\] (32)

Note that \(\ell_{t}\) is \(4D\)-Lipschitz because \(\mathbf{y}_{t},\hat{\mathbf{y}}_{t}\in\mathbb{B}_{2}(D)\). Hence,

\[\ell_{t}\big{(}f(\boldsymbol{\alpha}_{\pi_{t}(\boldsymbol{\alpha}_{t})})\big{)} -\ell_{t}\big{(}f(\boldsymbol{\alpha}_{t})\big{)}\leq 4D\big{|}f(\boldsymbol{ \alpha}_{\pi_{t}(\boldsymbol{\alpha}_{t})})-f(\boldsymbol{\alpha}_{t})\big{|} \leq 4D\kappa^{*}\|\boldsymbol{\alpha}_{\pi_{t}(\boldsymbol{\alpha}_{t})}- \boldsymbol{\alpha}_{t}\|\leq 4D\varepsilon_{t}\]

by \(\kappa^{*}\)-Lipschitzness of \(f\). Thus, putting all together,

\[\sum_{t=1}^{T}\ell_{t}(\hat{\mathbf{w}}_{t})-\sum_{t=1}^{T}\ell_{t}(f( \boldsymbol{\alpha}_{t}))\leq 8\ln(eT)C_{N}\,\varepsilon_{T}^{-N}+4D\kappa^{*} \sum_{t=1}^{T}\varepsilon_{t}\]and recalling that \(\varepsilon_{t}=t^{-\frac{1}{1+N}}\) we have

\[\sum_{t=1}^{T}t^{-\frac{1}{1+N}}\leq\sum_{t=1}^{T}t^{-\frac{1}{1+N}}\leq\int_{0} ^{T}\tau^{-\frac{1}{1+N}}\;\mathrm{d}\tau=\left(1+\frac{1}{N}\right)T^{\frac{N} {1+N}}\leq 2T^{\frac{N}{1+N}}\]

which completes the proof.