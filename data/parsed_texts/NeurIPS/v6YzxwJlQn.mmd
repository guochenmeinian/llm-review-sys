# Deep Equilibrium Based Neural Operators for Steady-State PDEs

Tanya Marwah\({}^{1}\) Ashwini Pokle \({}^{1}\) J. Zico Kolter \({}^{1,2}\) Zachary C. Lipton \({}^{1}\) Jianfeng Lu \({}^{3}\) Andrej Risteski \({}^{1}\)

\({}^{1}\)Carnegie Mellon University \({}^{2}\) Bosch Center for AI \({}^{3}\) Duke University

{tmarwah,apokle,zicokolter,zlipton,aristesk}@andrew.cmu.edu

jianfeng@math.duke.edu

Equal contribution. Correspondence to tmarwah@andrew.cmu.edu and apokle@andrew.cmu.edu

###### Abstract

Data-driven machine learning approaches are being increasingly used to solve partial differential equations (PDEs). They have shown particularly striking successes when training an operator, which takes as input a PDE in some family, and outputs its solution. However, the architectural design space, especially given structural knowledge of the PDE family of interest, is still poorly understood. We seek to remedy this gap by studying the benefits of weight-ted neural network architectures for steady-state PDEs. To achieve this, we first demonstrate that the solution of most steady-state PDEs can be expressed as a fixed point of a non-linear operator. Motivated by this observation, we propose FNO-DEQ, a deep equilibrium variant of the FNO architecture that directly solves for the solution of a steady-state PDE as the infinite-depth fixed point of an implicit operator layer using a black-box root solver and differentiates analytically through this fixed point resulting in \(\mathcal{O}(1)\) training memory. Our experiments indicate that FNO-DEQ-based architectures outperform FNO-based baselines with \(4\times\) the number of parameters in predicting the solution to steady-state PDEs such as Darcy Flow and steady-state incompressible Navier-Stokes. Finally, we show FNO-DEQ is more robust when trained with datasets with more noisy observations than the FNO-based baselines, demonstrating the benefits of using appropriate inductive biases in architectural design for different neural network based PDE solvers. Further, we show a universal approximation result that demonstrates that FNO-DEQ can approximate the solution to any steady-state PDE that can be written as a fixed point equation.

## 1 Introduction

Partial differential equations (PDEs) are used to model a wide range of processes in science and engineering. They define a relationship of (unknown) function and its partial derivatives. Most PDEs do not admit a closed form solution, and are solved using a variety of classical numerical methods such as finite element [10], finite volume [14], and spectral methods [15, 16]. These methods are often very computationally expensive, both as the ambient dimension grows, and as the desired accuracy increases.

This has motivated a rapidly growing area of research in data-driven approaches to PDE solving. One promising approach involves learning _neural solution operators_[1, 17, 18, 19], which take in the coefficients of a PDE in some family and output its solution--and are trained by examples of coefficient-solution pairs.

While several architectures for this task have been proposed, the design space--in particular taking into account structural properties of the PDEs the operator is trained on--is still largely unexplored. Most present architectures are based on "neuralizing" a classical numerical method. For instance, Li et al. (2020) take inspiration from spectral methods, and introduce FNO: a trained composition of (parametrized) kernels in Fourier space. Brandstetter et al. (2022) instead consider finite-difference methods and generalize them into (learnable) graph neural networks using message-passing.

Our work focuses on families of PDEs that describe the steady-state of a system (that is, there is no time variable). Namely, we consider equations of the form:

\[L(a(x),u(x))=f(x),\qquad\forall x\in\Omega,\] (1)

where \(u:\Omega\to\mathbb{R}^{d_{u}}\), \(a:\Omega\to\mathbb{R}^{d_{a}}\) and \(f:\Omega\to\mathbb{R}^{d_{f}}\) are functions defined over the domain \(\Omega\), and \(L\) is a (possibly non-linear) operator. This family includes many natural PDE families like Poisson equations, electrostatic equations, and steady-state Navier-Stokes.

We take inspiration from classical numerical approaches of fast-converging Newton-like iterative schemes (LeVeque, 2007; Farago and Karatson, 2002) to solve steady-state PDEs, as well as recent theoretical works for elliptic (linear and non-linear PDEs) (Marwah et al., 2021; Chen et al., 2021; Marwah et al., 2022) to hypothesize that very deep, but heavily weight-tied architectures would provide a useful architectural design choice for steady-state PDEs.

In this paper, we show that for steady state equations it is often more beneficial to weight-tie an existing neural operator, as opposed to making the model deeper--thus increasing its size. To this end, we introduce **FNO-DEQ**, a new architecture for solving steady-state PDEs. FNO-DEQ is a deep equilibrium model (DEQ) that utilizes weight-tied FNO layers along with implicit differentiation and root-solvers to approximate the solution of a steady-state PDE. DEQs are a perfect match to the desiderata laid out above: they can be viewed alternately as directly parameterizing the fixed points of some iterative process; or by explicitly expanding some iterative fixed point solver like Newton's or Broyden's method as an infinitely deep, weight-tied model.

Such an architecture has a distinct computational advantage: implicit layer models effectively backpropagate through the infinite-depth network while using only constant memory (equivalent to a single layer's activations). Empirically, we show that for steady-state PDEs, weight-tied and DEQ based models perform better than baselines with \(4\times\) the number of parameters, and are robust to training data noise. In summary, we make the following contributions:

* We show the benefits of weight-tying as an effective architectural choice for neural operators when applied to steady-state PDEs.
* We introduce FNO-DEQ, a FNO based deep equilibrium model (DEQ) that uses implicit layers and root solving to approximate the solution of a steady-state PDE. We further attest to the empirical performance of FNO-DEQ by showing that it performs as well as FNO and its variants with \(4\times\) number of parameters.
* We show that FNO-DEQ and weight tied architectures are more robust to both input and observation noise, thus showing that weight-tying is a useful inductive bias for architectural design for steady-state PDEs.
* By leveraging the universal approximation results of FNO (Kovachki et al., 2021) we show that FNO-DEQ based architectures can universally approximate the solution operator for a wide variety of steady-state PDE families.
* Finally, we create a dataset of pairs of steady-state incompressible Navier-Stokes equations with different forcing functions and viscosities, along with their solutions, which we will make public as a community benchmark for steady-state PDE solvers.

## 2 Related Work

Neural network based approaches for solving PDEs can broadly be divided into two categories. First are hybrid solvers (Bar-Sinai et al., 2019; Kochkov et al., 2021; Hsieh et al., 2019) which use neural networks in conjunction with existing numerical solvers. The main motivation is to not only improve upon the existing solvers, but to also replace the more computationally inefficient parts of the solver with a learned counter part. Second set of approaches are full machine learning based approaches that aim to leverage the approximation capabilities of neural networks (Hornik et al., 1989) to directly learn the dynamics of the physical system from observations.

Hybrid solvers like Hsieh et al. (2019) use a neural network to learn a correction term to correct over an existing hand designed solver for a Poisson equation, and also provide convergence guarantees of their method to the solution of the PDE. However, the experiments in their paper are limited to linear elliptic PDEs. Further, solvers like Bar-Sinai et al. (2019) use neural networks to derive the discretizations for a given PDE, thus enabling the use of a low-resolution grid in the numerical solver. Furthermore, Kochkov et al. (2021) use neural networks to interpolate differential operators between grid points of a low-resolution grid with high accuracy. This work specifically focuses on solving Navier-Stokes equations, their method is more accurate than numerical techniques like Direct Numerical Simulation (DNS) with a low-resolution grid, and is also \(80\times\) more faster. Brandstetter et al. (2022) introduced a message passing based hybrid scheme to train a hybrid solver and also propose a loss term which helps improve the stability of hybrid solvers for time dependent PDEs. However, most of these methods are equation specific, and are not easily transferable to other PDEs from the same family.

The neural network based approach that has recently garnered the most interest by the community is that of the operator learning framework (Chen and Chen, 1995; Kovachki et al., 2021; Lu et al., 2019; Li et al., 2020; Bhattacharya et al., 2021), which uses a neural network to approximate and infinite dimensional operator between two Banach spaces, thus learning an entire family of PDEs at once. Lu et al. (2019) introduces DeepONet, which uses two deep neural networks, referred to as the branch net and trunk net, which are trained concurrently to learn from data. Another line of operator learning framework is that of neural operators Kovachki et al. (2021). The most successful methodology for neural operators being the Fourier neural operators (FNO) (Li et al., 2020). FNO uses convolution based integral kernels which are evaluated in the Fourier space. Future works like Tran et al. (2021) introduce architectural improvements that enables one to train deeper FNO networks, thus increasing their size and improving their the performance on a variety of (time-dependent) PDEs. Moreover, the success of Transformers in domains like language and vision has also inspired transformer based neural operators in works like Li et al. (2022), Hao et al. (2023) and Liu et al. (2022). Theoretical results pertaining to the neural operators mostly include universal approximation results Kovachki et al. (2021), Lanthaler et al. (2022) which show that architectures like FNO and DeepONet can indeed approximate the infinite dimension operators.

In this work, we focus on steady-state equations and show the benefits of weight-tying in improving the performance of FNO for steady-state equations. We show that instead of making a network deeper and hence increasing the size of a network, weight-tied FNO architectures can outperform FNO and its variants \(4\times\) its size. We further introduce FNO-DEQ, a deep equilibrium based architecture to simulate an infinitely deep weight-tied network (by solving for a fixed point) with \(\mathcal{O}(1)\) training memory. Our work takes inspiration from recent theoretical works like Marwah et al. (2021), Chen et al. (2021), Marwah et al. (2022) which derive parametric rates for some-steady state equations, and in fact prove that neural networks can approximate solutions to some families of PDEs with just \(\text{poly}(d)\) parameters, thus evading the curse of dimensionality.

## 3 Preliminaries

We now introduce some key concepts and notation.

**Definition 1** (\(L^{2}(\Omega;\mathbb{R}^{d})\)).: _For a domain \(\Omega\) we denote by \(L^{2}(\Omega;\mathbb{R}^{d})\) the space of square integrable functions \(g:\Omega\to\mathbb{R}^{d}\) such that \(\|g\|_{L^{2}(\Omega)}<\infty\), where \(\|g\|_{L^{2}(\Omega)}=\left(\int_{\Omega}\|g(x)\|_{\ell_{2}}^{2}dx\right)^{1/2}\)._

### Neural Operators

Neural operators (Lu et al., 2019; Li et al., 2020; Bhattacharya et al., 2021; Patel et al., 2021; Kovachki et al., 2023) are a deep learning approach to learning solution operators which map a PDE to its solution. Fourier Neural Operator (FNO) (Li et al., 2020) is a particularly successful recent architecture parametrized as a sequence of kernel integral operator layers followed by non-linear activation functions. Each kernel integral operator layer is a convolution-based kernel function that is instantiated through a linear transformation in Fourier domain, making it less sensitive to the level of spatial discretization. Specifically, an \(L\)-layered FNO \(G_{\theta}:\mathbb{R}^{d_{u}}\rightarrow\mathbb{R}^{d_{u}}\) with learnable parameters \(\theta\), is defined as

\[G_{\theta}:=\mathcal{Q}\circ\mathcal{L}_{L}\circ\mathcal{L}_{L-1}\circ\cdots \circ\mathcal{L}_{1}\circ\mathcal{P}\] (2)

where \(\mathcal{P}:L^{2}(\Omega;\mathbb{R}^{d_{u}})\to L^{2}(\mathbb{R}^{d_{ \cdot}};\mathbb{R}^{d_{u}})\) and \(\mathcal{Q}:L^{2}(\mathbb{R}^{d_{\cdot}};\mathbb{R}^{d_{\cdot}})\to L ^{2}(\mathbb{R}^{d_{\cdot}};\mathbb{R}^{d_{u}})\) are projection operators, and \(\mathcal{L}_{l}:L^{2}(\mathbb{R}^{d_{\cdot}};\mathbb{R}^{d_{\cdot}})\to L ^{2}(\mathbb{R}^{d_{\cdot}};\mathbb{R}^{d_{\cdot}})\) for \(l\in[L]\) is the \(l^{\text{th}}\) FNO layer defined as,

\[\mathcal{L}_{l}\left(v_{l}\right)=\sigma\left(W_{l}v_{l}+b_{l}+\mathcal{K}_{l} (v_{l})\right))\,.\] (3)

Here \(\sigma\) is a non-linear activation function, \(W_{l},b_{l}\) are the \(l^{th}\) layer weight matrix and bias terms. Finally \(\mathcal{K}_{l}\) is the \(l^{th}\) integral kernel operator which is calculated using the Fourier transform as introduced in Li et al. (2020) defined as follows,

\[\mathcal{K}_{l}(v_{l})=\mathcal{F}^{-1}\left(R_{l}\cdot\left(\mathcal{F}v_{l} \right)\right)(x)\qquad\forall x\in\Omega,\] (4)

where \(\mathcal{F}\) and \(\mathcal{F}^{-1}\) are the Fourier transform and the inverse Fourier transform, with \(R_{l}\) representing the learnable weight-matrix in the Fourier domain. Therefore, ultimately, the trainable parameters \(\theta\) is a collection of all the weight matrices and biases, i.e, \(\theta:=\{W_{l},b_{l},R_{l},\cdots,W_{1},b_{1},R_{1}\}\).

### Equilibrium Models

Equilibrium models (Liao et al., 2018; Bai et al., 2019; Revay et al., 2020; Winston and Kolter, 2020) compute internal representations by solving for a fixed point in their forward pass. Specifically, consider a deep feedforward network with \(L\) layers :

\[z^{[i+1]}=f_{\theta}^{[i]}\left(z^{[i]};x\right)\quad\text{for }i=0,...,L-1\] (5)

where \(x\in\mathbb{R}^{n_{z}}\) is the input injection, \(z^{[i]}\in\mathbb{R}^{n_{z}}\) is the hidden state of \(i^{th}\) layer with \(z^{[0]}=\mathbf{0}\), and \(f_{\theta}^{[i]}:\mathbb{R}^{n_{z}\times n_{z}}\mapsto\mathbb{R}^{n_{z}}\) is the feature transformation of \(i^{th}\) layer, parametrized by \(\theta\). Suppose the above model is weight-tied, _i.e.,_\(f_{\theta}^{[i]}=f_{\theta},\forall i\), and \(\lim_{i\rightarrow\infty}f_{\theta}\left(z^{[i]};x\right)\) exists and its value is \(z^{\star}\). Further, assume that for this \(z^{\star}\), it holds that \(f_{\theta}\left(z^{\star};x\right)=z^{\star}\). Then, equilibrium models can be interpreted as the infinite-depth limit of the above network such that \(f_{\theta}^{\infty}\left(z^{\star};x\right)=z^{\star}\).

Under certain conditions2, and for certain classes of \({f_{\theta}}^{3}\), the output \(z^{\star}\) of the above weight-tied network is a fixed point. A simple way to solve for this fixed point is to use fixed point iterations, _i.e.,_ repeatedly apply the update \(z^{[t+1]}=f_{\theta}(z^{[t]};x)\) some fixed number of times, and backpropagate through the network to compute gradients. However, this can be computationally expensive. Deep equilibrium (DEQ) models (Bai et al., 2019) explicitly solve for \(z^{\star}\) through iterative root finding methods like Broyden's method (Broyden, 1965), Newton's method, Anderson acceleration (Anderson, 1965). DEQs use implicit function theorem to directly differentiate through the fixed point \(z^{\star}\) at equilibrium, thus requiring constant memory to backpropagate through an infinite-depth network:

Footnote 2: The fixed point can be reached if the dynamical system is globally contractive. This is usually not true in practice for most choices of \(f_{\theta}\), and divergence is possible.

\[\frac{\partial z^{\star}}{\partial\theta}=\left(I-\frac{\partial f_{\theta}(z^ {\star};x)}{\partial z^{\star}}\right)^{-1}\frac{\partial f_{\theta}(z^{ \star};x)}{\partial\theta}\] (6)

Computing the inverse of Jacobian can quickly become intractable as we deal with high-dimensional feature maps. One can replace the inverse-Jacobian term with an identity matrix _i.e.,_ Jacobian-free (Fung et al., 2022) or an approximate inverse-Jacobian (Geng et al., 2021) without affecting the final performance. There are alternate formulations of DEQs (Winston and Kolter, 2020) that guarantee existence of a unique equilibrium point. However, designing \(f_{\theta}\) for these formulations can be challenging, and in this work we use the formulation by Bai et al. (2019).

## 4 Problem setting

We first formally define the system of steady-state PDEs that we will solve for:

**Definition 2** (Steady-State PDE).: _Given a bounded open set \(\Omega\subset\mathbb{R}^{d}\), a steady-state PDE can be written in the following general form:_

\[L(a(x),u(x))=f(x),\qquad\forall x\in\Omega\] (7)

_Here \(L\) is a continuous operator, the function \(u\in L^{2}\left(\Omega;\mathbb{R}^{d_{u}}\right)\) is the unknown function that we wish to solve for and \(a\in L^{2}\left(\Omega;\mathbb{R}^{d_{u}}\right)\) collects all the coefficients describing the PDE, and \(f\in L^{2}\left(\Omega;\mathbb{R}^{d_{f}}\right)\) is a function independent of \(u\). We will, for concreteness, assume periodic boundary conditions, i.e. \(\forall z\in\mathbb{Z}^{d},\forall x\in\Omega\) we have \(u(x+z)=u(x)\). (Equivalently, \(\Omega:=\mathbb{T}^{d}=[0,2\pi]^{d}\) can be identified with the torus.) 4 Finally, we will denote \(u^{\star}:\Omega\to\mathbb{R}\) as the solution to the PDE._

Footnote 4: This is for convenience of exposition, our methods can readily be extended to other boundary conditions like Dirichet, Neumann etc.

Steady-state models a system at stationarity, _i.e.,_ when some quantity of interest like temperature or velocity no longer changes over time. Classical numerical solvers for these PDEs include iterative methods like Newton updates or conjugate gradient descent, typically with carefully chosen preconditioning to ensure benign conditioning and fast convergence. Furthermore, recent theoretical works (Marwah et al., 2021; Chen et al., 2021; Marwah et al., 2022) show that for many families of PDEs (e.g., steady-state elliptic PDEs that admit a variational formulation), iterative algorithms can be efficiently "neuralized", that is, the iterative algorithm can be represented by a compact neural network, so long as the coefficients of the PDE are also representable by a compact neural network. Moreover, the architectures constructed in these works are heavily weight-tied.

We will operationalize these developments through the additional observation that all these iterative schemes can be viewed as algorithms to find a fixed point of a suitably chosen operator. Namely, we can design an operator \(\mathcal{G}:L^{2}(\Omega;\mathbb{R}^{d_{u}})\times L^{2}(\Omega;\mathbb{R}^{d_ {f}})\to L^{2}(\Omega;\mathbb{R}^{d_{u}})\)5 such that \(u^{\star}=\mathcal{G}(u^{\star},f)\) and a lot of common (preconditioned) first and second-order methods are natural ways to recover the fixed points \(u^{\star}\).

Footnote 5: We note that the choice of defining the operator with the forcing function \(f\) is made for purely expository purposes the operator \(\mathcal{G}\) can be defined as \(\mathcal{G}:L^{2}(\Omega;\mathbb{R}^{d_{u}})\times L^{2}(\Omega;\mathbb{R}^{d _{u}})\to L^{2}(\Omega;\mathbb{R}^{d_{u}})\) as well.

Before describing our architectures, we introduce two components that we will repeatedly use.

**Definition 3** (Projection and embedding layers).: _A projection and embedding layer, respectively \(\mathcal{P}:L^{2}(\Omega;\mathbb{R}^{d_{u}})\times L^{2}(\Omega;\mathbb{R}^{d _{f}})\to L^{2}(\mathbb{R}^{d_{v}};\mathbb{R}^{d_{v}})\times L^{2}(\mathbb{R} ^{d_{v}};\mathbb{R}^{d_{v}})\) and \(\mathcal{Q}:L^{2}(\mathbb{R}^{d_{v}};\mathbb{R}^{d_{v}})\to L^{2}(\mathbb{R}^{ d_{v}};\mathbb{R}^{d_{u}})\), are defined as_

\[\mathcal{P}(v,f) =\left(\sigma\left(W_{P}^{(1)}v+b_{P}^{(1)}\right),\sigma\left(W_ {P}^{(2)}f+b_{P}^{(2)}\right)\right),\] \[\mathcal{Q}(v) =\sigma\left(W_{Q}v+b_{Q}\right)\]

_where \(W_{P}^{(1)}\in\mathbb{R}^{d_{u}\times d_{v}},W_{P}^{(2)}\in\mathbb{R}^{d_{f} \times d_{v}},W_{Q}\in\mathbb{R}^{d_{v}\times d_{u}}\) and \(b_{P}^{(1)},b_{P}^{(2)}\in\mathbb{R}^{d_{v}},b_{Q}\in\mathbb{R}^{d_{u}}\)._

**Definition 4** (Input-injected FNO layer).: _An input-injected FNO layer \(\mathcal{L}:L^{2}(\mathbb{R}^{d_{v}};\mathbb{R}^{d_{v}})\times L^{2}(\mathbb{R }^{d_{v}};\mathbb{R}^{d_{v}})\to L^{2}(\mathbb{R}^{d_{v}};\mathbb{R}^{d_{v}})\) is defined as_

\[\mathcal{L}(v,g):=g+\sigma\left(Wv+b+\mathcal{F}^{-1}(R^{(k)}\cdot(\mathcal{F} v)\right).\] (8)

_where \(W\in\mathbb{R}^{d_{v}\times d_{v}}\), \(b\in\mathbb{R}^{d_{v}}\) and \(R^{(k)}\in\mathbb{R}^{d_{v}\times d_{v}}\) for all \(k\in[K]\) are learnable parameters._

Note the difference between the FNO layer specified above, and the standard FNO layer Equation 3 is the extra input \(g\) to the layer, which in our architecture will correspond to a projection of (some or all) of the PDE coefficients. We also note that this change to the FNO layer also enables us to learn deeper FNO architectures, as shown in Section 5. With this in mind, we can discuss the architectures we propose.

Weight-tied architecture I: Weight-tied FNOThe first architecture we consider is a weighted version of FNO (introduced in Section 3), in which we repeatedly apply (\(M\) times) the same transformation in each layer. More precisely, we have:

**Definition 5** (FNO Weight-Tied).: _We define a \(M\) times weight-tied neural operator \(G_{\theta}^{M}\) as,_

\[G_{\theta}^{M}=\mathcal{Q}\circ\underbrace{\mathcal{B}^{L}\circ\mathcal{B}^{L} \circ\cdots\circ\mathcal{B}^{L}}_{\text{M times}}\circ\mathcal{P}\]such that: \(\mathcal{P},\mathcal{Q}\) are projection and embedding layers as in Definition 3 an \(\mathcal{B}^{L}:L^{2}(\mathbb{R}^{d_{v}};\mathbb{R}^{d_{v}})\times L^{2}(\mathbb{ R}^{d_{v}};\mathbb{R}^{d_{v}})\to L^{2}(\mathbb{R}^{d_{v}};\mathbb{R}^{d_{v}})\) is an \(L\)-layer FNO block, i.e, \(\mathcal{B}^{L}=\mathcal{L}_{L}\circ\mathcal{L}_{L-1}\circ\mathcal{L}_{L-2} \circ\mathcal{L}_{1}\) where for all \(l\in[L]\), \(\mathcal{L}_{l}(\cdot,\mathcal{P}(f))\)6 is an input-injected FNO block as in Definition 4._

Footnote 6: We are abusing the notation somewhat and denoting by \(\mathcal{P}(f)\) the second coordinate of \(\mathcal{P}\), which only depends on \(f\).

Weight-tied architecture II: Fno-deqIn cases where we believe a weight-tied \(G_{\theta}^{M}\) converges to some fixed point as \(M\to\infty\), unrolling \(G_{\theta}^{M}\) for a large \(M\) requires a lot of hardware memory for training: training the model requires one to store intermediate hidden units for each weight-tied layer for backpropagation, resulting in a \(\mathcal{O}(M)\) increase in the amount of memory required.

To this end, we use Deep Equilibrium models (DEQs) which enables us to implicitly train \(G_{\theta}:=\lim_{M\to\infty}G_{\theta}^{M}\) by directly solving for the fixed point by leveraging black-box root finding algorithms like quasi-Newton methods, [1, 13]. Therefore we can think of this approach as an infinite-depth (or infinitely unrolled) weight-tied network. We refer to this architecture as **FNO-DEQ**.

**Definition 6** (Fno-deq).: _Given \(\mathcal{P},\mathcal{Q}\) and \(\mathcal{B}^{L}\) in Definition 5, FNO-DEQ is trained to parametrize the fixed point equation \(\mathcal{B}^{L}\)\((v^{\star},\mathcal{P}(f))=v^{\star}\) and outputs \(u^{\star}=\mathcal{Q}(v^{\star})\)._

Usually, it is non-trivial to differentiate through these black-box root solvers. DEQs enable us to implicitly differentiate through the equilibrium fixed point efficiently without any need to backpropagate through these root solvers, therefore resulting in \(\mathcal{O}(1)\) training memory.

## 5 Experiments

**Network architectures.** We consider the following network architectures in our experiments.

**FNO**: We closely follow the architecture proposed by Li et al. [13] and construct this network by stacking four FNO layers and four convolutional layers, separated by GELU activation [1]. Note that in our current set up, we recover the original FNO architecture if the input to the \(l^{\text{th}}\) layer is the output of \((l-1)^{\text{th}}\) layer _i.e.,_\(v_{l}=\mathcal{B}_{l-1}(v_{l-1})\).

**Improved FNO (FNO++)**: The original FNO architecture suffers from vanishing gradients, which prohibits it from being made deeper [12]. We overcome this limitation by introducing residual connections within each block of FNO, with each FNO block \(\mathcal{B}_{l}\) comprising of three FNO layers \(\mathcal{L}\)_i.e.,_\(\mathcal{B}_{l}=\mathcal{L}_{L_{1}}^{l}\circ\mathcal{L}_{L_{2}}^{l}\circ \mathcal{L}_{L_{3}}^{l}\) and three convolutional layers, where \(\mathcal{L}\) is defined in Eq. (8).

**Weight-tied network (FNO-WT)**: This is the weight-tied architecture introduced in Definition 5, where we initialize \(v_{0}(x)=0\) for all \(x\in\Omega\).

**FNO-DEQ**: As introduced in Definition 6, FNO-DEQ is a weight-tied network where we explicitly solve for the fixed point in the forward pass with a root finding algorithm. We use Anderson acceleration [1] as the root solver. For the backward pass, we use approximate implicit gradients [1] which are light-weight and more stable in practice, compared to implicit gradients computed by inverting Jacobian.

Note that both weight-tied networks and FNO-DEQs leverage weight-tying but the two models differ in the ultimate goal of the forward pass: DEQs explicitly solve for the fixed point during the forward pass, while weight-tied networks trained with backpropagation may or may-not reach a fixed point [13]. Furthermore, DEQs require \(\mathcal{O}(1)\) memory, as they differentiate through the fixed point implicitly, whereas weight-tied networks need to explicitly create the entire computation graph for backpropagation, which can become very large as the network depth increases. Additionally, FNO++ serves as a non weight-tied counterpart to a weight-tied input-injected network. Like weighted networks, FNO++ does not aim to solve for a fixed point in the forward pass.

**Experimental setup.** We test the aforementioned network architectures on two families of steady-state PDEs: Darcy Flow equation and steady-state Navier-Stokes equation for incompressible fluids. For experiments with Darcy Flow, we use the dataset provided by Li et al. [13], and generate our own dataset for steady-state Navier-Stokes. For more details on the datasets and the data generation processes we refer to Sections B.1 and B.2 of the Appendix. For each family of PDE, we train networks under 3 different training setups: clean data, noisy inputs and noisy observations. For experiments with noisy data, both input and observations, we add noise sampled from a sequence of standard Gaussians with increasing values of variance \(\{\mathcal{N}(0,(\sigma_{k}^{2}))\}_{k=0}^{M-1}\), where \(M\) is the total number of Gaussians we sample from. We set \(\sigma_{0}^{2}=0\) and \(\sigma_{\max}^{2}=\sigma_{M-1}^{2}\leq 1/r\), where \(r\) is the resolution of the grid. Thus, the training data includes equal number of PDEs with different levels of Gaussian noise added to their input or observations. We add noise to training data, and always test on clean data. We follow prior work [11] and report relative \(L_{2}\) norm between ground truth \(u^{\star}\) and prediction on test data. The total depth of all networks besides FNO is given by \(6B+4\), where \(B\) is the number of FNO blocks. Each FNO block has 3 FNO layers and convolutional layers. In addition, we include the depth due to \(\mathcal{P}\), \(\mathcal{Q}\), and an additional final FNO layer and a convolutional layer. We further elaborate upon network architectures and loss functions in in Appendix A.

### Darcy Flow

For our first set of experiments we consider stationary Darcy Flow equations, a form of linear elliptic PDE with in two dimensions. The PDE is defined as follows,

\[\begin{split}-\nabla\cdot(a(x)\nabla u(x))&=f(x), \qquad x\in(0,1)^{2}\\ u(x)&=0&\qquad x\in\partial(0,1)^{2}. \end{split}\] (9)

Note that the diffusion coefficient \(a\in L^{\infty}(\Omega)(\Omega;\mathbb{R}_{+})\), i.e., the coefficients are always positive, and \(f\in L^{2}(\Omega;\mathbb{R}^{d_{f}})\) is the forcing term. These PDEs are used to model the steady-state pressure of fluids flowing through a porous media. They can also be used to model the stationary state of the diffusive process with \(u(x)\) modeling the temperature distribution through the space with \(a\) defining the thermal conductivity of the medium. The task is to learn an operator \(G_{\theta}:L^{2}(\Omega;\mathbb{R}^{d_{u}})\times L^{2}(\Omega;\mathbb{R}^{d_ {a}})\to L^{2}(\Omega;\mathbb{R}^{d_{u}})\) such that \(u^{\star}=G_{\theta}(u^{\star},a)\).

We report the results of our experiments on Darcy Flow in Table 1. The original FNO architecture does not improve its performance with increased number of FNO blocks \(\mathcal{B}\). FNO++ with residual connections scales better but saturates at around 4 FNO blocks. In contrast, FNO-WT and FNO-DEQ with just a _single_ FNO block outperform deeper non-weight-tied architectures on clean data and on data with noisy inputs. When observations are noisy, FNO-WT and FNO-DEQ outperform FNO++ with a similar number of parameters, and perform comparably to FNO++ with \(4\times\) parameters.

We also report results on shallow FNO-DEQ, FNO-WT and FNO++ architectures. An FNO block in these shallow networks has a single FNO layer instead of three layers. In our experiments, shallow weight-tied networks outperform non-weight-tied architectures including FNO++ with \(7\times\) parameters on clean data and on data with noisy inputs, and perform comparably to deep FNO++ on noisy observations. In case of noisy observations, we encounter training instability issues in FNO-DEQ. We believe that this shallow network lacks sufficient representation power and cannot accurately solve for the fixed point during the forward pass. These errors in fixed point estimation accumulate over time, leading to incorrect values of implicit gradients, which in turn result in training instability issues.

### Steady-state Navier-Stokes Equations for Incompressible Flow

We consider the steady-state Navier-Stokes equation for an incompressible viscous fluid in the vorticity form defined on a torus, i.e., with periodic boundary condition,

\[\begin{split} u\cdot\nabla\omega&=\nu\Delta\omega+ f,\qquad x\in\Omega\\ \nabla\cdot u&=0&\qquad x\in\Omega\end{split}\] (10)

where \(\Omega:=(0,2\pi)^{2}\), and \(u:\Omega\to\mathbb{R}^{2}\) is the velocity and \(\omega:\Omega\to\mathbb{R}\) where \(\omega=\nabla\times u\), \(\nu\in\mathbb{R}_{+}\) is the viscosity and \(f:\Omega\to\mathbb{R}\) is the external force term. We learn an operator \(G_{\theta}:L^{2}(\Omega;\mathbb{R}^{d_{u}})\times L^{2}(\Omega;\mathbb{R}^{d_ {f}})\to L^{2}(\Omega;\mathbb{R}^{d_{u}})\), such that \(u^{\star}=G_{\theta}(u^{\star},f)\). We train all the models on data with viscosity values \(\nu=0.01\) and \(\nu=0.001\), and create a dataset for steady-state incompressible Navier-Stokes, which we will make public as a community benchmark for steady-state PDE solvers.

Results for Navier-Stokes equation have been reported in Table 2 and Table 3. For both values of viscosity, FNO-DEQ outperforms other architectures for all three cases: clean data, noisy inputs and noisy observations. FNO-DEQ is more robust to noisy inputs compared to non-weight-tied architectures. For noisy inputs, FNO-DEQ matches the test-error of noiseless case in case of viscosity \(0.01\) and almost matches the test-error of noiseless case in case of viscosity \(0.001\). We provide additional results for noise level \(0.004\) in Appendix E. FNO-DEQ and FNO-WT consistently outperform non-weight-tied architectures for higher levels of noise as well.

In general, DEQ-based architectures are slower to train (upto \(\sim\)2.5\(\times\) compared to feedforward networks of similar size) as we solve for the fixed point in the forward pass. However, their inductive bias provides performance gains that cannot be achieved by simply stacking non-weight-tied FNO layers. In general, we observe diminishing returns in FNO++ beyond 4 blocks. Additionally, training the original FNO network on more than 4 FNO blocks is challenging, with the network often diverging during training, and therefore we do not include these results in the paper.

## 6 Universal Approximation and Fast Convergence of FNO-DEQ

Though the primary contribution of our paper is empirical, we show (by fairly standard techniques) that FNO-DEQ is a universal approximator, under mild conditions on the steady-state PDEs. Moreover, we also show that in some cases, we can hope the fixed-point solver can converge rapidly.

As noted in Definition 2, we have \(\Omega:=\mathbb{T}^{d}\). We note that all continuous function \(f\in L^{2}(\Omega;\mathbb{R})\) and \(\int_{\Omega}|f(x)|dx<\infty\) can be written as, \(f(x)=\sum_{\omega\in\mathbb{Z}^{d}}e^{ix^{T}\omega}\hat{f}_{w}\). where \(\{\hat{f}_{\omega}\}_{\omega\in\mathbb{Z}^{d}}\) are the Fourier coefficients of the function \(f\). We define as \(L^{2}_{N}(\Omega)\) as the space of functions such that for all \(f_{N}\in L^{2}_{N}(\Omega)\) with Fourier coefficients that vanish outside a bounded ball. Finally, we define an

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Architecture} & \multirow{2}{*}{Parameters} & \multirow{2}{*}{\#Blocks} & \multicolumn{4}{c}{Test error \(\downarrow\)} \\ \cline{4-6}  & & & \(\sigma^{2}_{\max}=0\) & \((\sigma^{2}_{\max})^{i}=0.001\) & \((\sigma^{2}_{\max})^{t}=0.001\) \\ \hline FNO & 2.37M & 1 & 0.0080 \(\pm\) 5e-4 & 0.0079 \(\pm\) 2e-4 & 0.0125 \(\pm\) 4e-5 \\ FNO & 4.15M & 2 & 0.0105 \(\pm\) 6e-4 & 0.0106 \(\pm\) 4e-4 & 0.0136 \(\pm\) 2e-5 \\ FNO & 7.71M & 4 & 0.2550 \(\pm\) 2e-8 & 0.2557 \(\pm\) 8e-9 & 0.2617 \(\pm\) 2e-9 \\ \hline FNO++ & 2.37M & 1 & 0.0075 \(\pm\) 2e-4 & 0.0075 \(\pm\) 2e-4 & 0.0145 \(\mp\) 7e-4 \\ FNO++ & 4.15M & 2 & 0.0065 \(\pm\) 2e-4 & 0.0065 \(\pm\) 9e-5 & 0.0117 \(\pm\) 5e-5 \\ FNO++ & 7.71M & 4 & 0.0064 \(\pm\) 2e-4 & 0.0064 \(\pm\) 2e-4 & **0.0109 \(\pm\) 5e-4** \\ S-FNO++ & 1.78M & 0.66 & 0.0093 \(\pm\) 5e-4 & 0.0094 \(\pm\) 7e-4 & 0.0402 \(\pm\) 6e-3 \\ \hline FNO-WT & 2.37M & 1 & **0.0055 \(\pm\) 1e-4** & **0.0056 \(\pm\) 5e-5** & 0.0112 \(\pm\) 4e-4 \\ FNO-DEQ & 2.37M & 1 & **0.0055 \(\pm\) 1e-4** & **0.0056 \(\pm\) 7e-5** & 0.0112 \(\pm\) 4e-4 \\ \hline S-FNO-WT & 1.19M & 0.33 & 0.0057 \(\pm\) 3e-5 & 0.0057 \(\pm\) 5e-5 & 0.0112 \(\pm\) 1e-4 \\ S-FNO-DEQ & 1.19M & 0.33 & 0.0056 \(\pm\) 4e-5 & 0.0056 \(\pm\) 5e-5 & 0.0136 \(\pm\) 0.011 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on Darcy flow: clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being \((\sigma^{2}_{\max})^{i}\) and \((\sigma^{2}_{\max})^{t}\), respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2. Here, S-FNO++, S-FNO-WT and S-FNO-DEQ are shallow versions of FNO++, FNO-WT and FNO-DEQ respectively.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Architecture} & \multirow{2}{*}{Parameters} & \multirow{2}{*}{\#Blocks} & \multicolumn{4}{c}{Test error \(\downarrow\)} \\ \cline{4-6}  & & & \(\sigma^{2}_{\max}=0\) & \((\sigma^{2}_{\max})^{i}=0.001\) & \((\sigma^{2}_{\max})^{t}=0.001\) \\ \hline FNO & 2.37M & 1 & 0.184 \(\pm\) 0.002 & 0.218 \(\pm\) 0.003 & 0.184 \(\pm\) 0.001 \\ FNO & 4.15M & 2 & 0.162 \(\pm\) 0.024 & 0.176 \(\pm\) 0.004 & 0.152 \(\pm\) 0.005 \\ FNO & 7.71M & 4 & 0.157 \(\pm\) 0.012 & 0.187 \(\pm\) 0.004 & 0.166 \(\pm\) 0.013 \\ \hline FNO++ & 2.37M & 1 & 0.199 \(\pm\) 0.001 & 0.230 \(\pm\) 0.001 & 0.197 \(\pm\) 0.001 \\ FNO++ & 4.15M & 2 & 0.154 \(\pm\) 0.005 & 0.173 \(\pm\) 0.003 & 0.154 \(\pm\) 0.006 \\ FNO++ & 7.71M & 4 & 0.151 \(\pm\) 0.003 & 0.165 \(\pm\) 0.004 & 0.149 \(\pm\) 0.003 \\ \hline FNO-WT & 2.37M & 1 & 0.151 \(\pm\) 0.007 & 0.173 \(\pm\) 0.017 & **0.126 \(\pm\) 0.027** \\ FNO-DEQ & 2.37M & 1 & **0.128 \(\pm\) 0.004** & **0.144 \(\pm\) 0.007** & 0.136 \(\pm\) 0.003 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on incompressible steady-state Navier-Stokes (viscosity=0.001): clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being \((\sigma^{2}_{\max})^{i}\) and \((\sigma^{2}_{\max})^{t}\), respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2.

orthogonal projection operator \(\Pi_{N}:L^{2}(\Omega)\to L^{2}_{N}(\Omega)\), such that for all \(f\in L^{2}(\Omega)\) we have,

\[f_{n}=\Pi_{N}(f)=\Pi_{N}\left(\sum_{\omega\in\mathbb{Z}^{d}}f_{\omega}e^{ix^{T} \omega}\right)=\sum_{\left\|\omega\right\|_{\infty}\leq N}\hat{f}_{\omega}e^{ ix^{T}\omega}.\] (11)

That is, the projection operator \(\Pi_{N}\) takes an infinite dimensional function and projects it to a finite dimensional space. We prove the following universal approximation result:

**Theorem 1**.: _Let \(u^{\star}\in L^{2}(\Omega;\mathbb{R}^{d_{u}})\) define the solution to a steady-state PDE in Definition 2, Then there exists an operator \(\mathcal{G}:L^{2}(\Omega;\mathbb{R}^{d_{u}})\times L^{2}(\Omega;\mathbb{R}^{d _{f}})\to L^{2}(\Omega;\mathbb{R}^{d_{u}})\) such that, \(u^{\star}=\mathcal{G}(u^{\star},f)\). Furthermore, for every \(\epsilon>0\) there exists an \(N\in\mathbb{N}\) such that for compact sets \(K_{u}\subset L^{2}(\Omega;\mathbb{R}^{d_{u}})\) and \(K_{f}\subset L^{2}(\Omega;\mathbb{R}^{d_{f}})\) there exists a neural network \(G_{\theta}:L^{2}_{N}(\Omega;\mathbb{R}^{d_{u}})\times L^{2}_{N}(\Omega; \mathbb{R}^{d_{f}})\to L^{2}_{N}(\Omega;\mathbb{R}^{d_{u}})\) with parameters \(\theta\), such that,_

\[\sup_{u\in K_{u},f\in K_{f}}\|u^{\star}-G_{\theta}(\Pi_{N}u^{\star},\Pi_{N}f) \|_{L^{2}(\Omega)}\leq\epsilon.\]

The proof for the above theorem is relatively straightforward and provided in Appendix C. The proof uses the fact that \(u^{\star}\) is a fixed-point of the operator \(G(u,f)=u-(L(u)-f)\), allowing us to use the the results in Kovachki et al. (2021) that show a continuous operator can be approximated by a network as defined in Equation 2. Note that the choice of \(G\) is by no means unique: one can "universally approximate" any operator \(G(u,f)=u-A(L(u)-f)\), for a continuous operator \(A\). Such a \(G\) can be thought of as a form of "preconditioned" gradient descent, for a preconditioner \(A\). For example, a Newton update has the form \(G(u,f)=u-L^{\prime}(u)^{-1}\left(L(u)-f\right)\), where \(L^{\prime}:L^{2}(\Omega;\mathbb{R}^{d_{u}})\to L^{2}(\Omega;\mathbb{R}^{d_{u}})\) is the Frechet derivative of the operator \(L\).

The reason this is relevant is that the DEQ can choose to universally approximate a fixed-point equation for which the fixed-point solver it is trained with also converges rapidly. As an example, the following classical result shows that under Lax-Milgram-like conditions (a kind of strong convexity condition), Newton's method converges doubly exponentially fast:

**Lemma 1**(Farago and Karatson (2002), Chapter 5).: _Consider the PDE defined Definition 2, such that \(d_{u}=d_{v}=d_{f}=1\). such that \(L^{\prime}(u)\) defines the Frechet derivative of the operator \(L\). If for all \(u,v\in L^{2}(\Omega;\mathbb{R})\) we have \(\|L^{\prime}(u)v\|_{L^{2}(\Omega)}\geq\lambda\|v\|_{L^{2}(\Omega)}\) and \(\|L^{\prime}(u)-L^{\prime}(v)\|_{L^{2}(\Omega)}\leq\Lambda\|u-v\|_{L^{2}( \Omega)}\) for \(0<\lambda\leq\Lambda<\infty\), then for the Newton update, \(u_{t+1}\gets u_{t}-L^{\prime}(u_{t})^{-1}\left(L(u_{t})-f\right),\) with \(u_{0}\in L^{2}(\Omega;\mathbb{R})\), there exists an \(\epsilon>0\), such that \(\|u_{T}-u^{\star}\|_{L^{2}(\Omega)}\leq\epsilon\) if \(T\geq\log\left(\log\left(\frac{1}{\epsilon}\right)/\log\left(\frac{2\lambda^ {2}}{\Lambda\|L(u_{0})-f\|_{L^{2}(\Omega)}}\right)\right).\)_

For completeness, we include the proof of the above lemma in the Appendix (Section D). We note that the conditions of the above lemma are satisfied for elliptic PDEs like Darcy Flow, as well as many variational non-linear elliptic PDEs (e.g., those considered in Marwah et al. (2022)). Hence, we can expect FNO-DEQs to quickly converge to the fixed point, since they employ quasi-Newton methods like Broyden and Anderson methods (Broyden, 1965; Anderson, 1965).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Architecture} & \multirow{2}{*}{Parameters} & \multirow{2}{*}{\#Blocks} & \multicolumn{4}{c}{Test error \(\downarrow\)} \\ \cline{4-6}  & & & \(\sigma^{2}_{\max}=0\) & \((\sigma^{2}_{\max})^{i}=0.001\) & \((\sigma^{2}_{\max})^{t}=0.001\) \\ \hline FNO & 2.37M & 1 & 0.181 \(\pm\) 0.005 & 0.186 \(\pm\) 0.003 & 0.178 \(\pm\) 0.006 \\ FNO & 4.15M & 2 & 0.138 \(\pm\) 0.007 & 0.150 \(\pm\) 0.006 & 0.137 \(\pm\) 0.012 \\ FNO & 7.71M & 4 & 0.152 \(\pm\) 0.006 & 0.163 \(\pm\) 0.002 & 0.151 \(\pm\) 0.008 \\ \hline FNO++ & 2.37M & 1 & 0.188 \(\pm\) 0.002 & 0.207 \(\pm\) 0.004 & 0.187 \(\pm\) 0.003 \\ FNO++ & 4.15M & 2 & 0.139 \(\pm\) 0.004 & 0.153 \(\pm\) 0.002 & 0.140 \(\pm\) 0.005 \\ FNO++ & 7.71M & 4 & 0.130 \(\pm\) 0.005 & 0.151 \(\pm\) 0.004 & 0.128 \(\pm\) 0.009 \\ \hline FNO-WT & 2.37M & 1 & 0.099 \(\pm\) 0.007 & 0.101 \(\pm\) 0.007 & 0.130 \(\pm\) 0.044 \\ FNO-DEQ & 2.37M & 1 & **0.088 \(\pm\) 0.006** & **0.099 \(\pm\) 0.007** & **0.116 \(\pm\) 0.011** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on incompressible steady-state Navier-Stokes (viscosity=0.01): clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being \((\sigma^{2}_{\max})^{i}\) and \((\sigma^{2}_{\max})^{t}\), respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2.

Conclusion

In this work, we demonstrate that the inductive bias of deep equilibrium models--and weight-tied networks in general--makes them ideal architectures for approximating neural operators for steady-state PDEs. Our experiments on steady-state Navier-Stokes equation and Darcy flow equations show that weight-tied models and FNO-DEQ perform outperform FNO models with \(\sim 4\times\) the number of parameters and depth. Our findings indicate that FNO-DEQ and weight-tied architectures are, in general, more robust to both input and observation noise compared to non-weight-tied architectures, including FNO. We believe that our results complement any future progress in the design and development of PDE solvers (Tran et al., 2021; Li et al., 2022b) for steady-state PDEs, and hope that our work motivates the study of relevant inductive biases that could be used to improve them.

## 8 Acknowledgements

TM is supported by CMU Software Engineering Institute via Department of Defense under contract FA8702-15-D-0002. AP is supported by a grant from the Bosch Center for Artificial Intelligence. ZL gratefully acknowledges the NSF (FAI 2040929 and IIS2211955), UPMC, Highmark Health, Abridge, Ford Research, Mozilla, the PwC Center, Amazon AI, JP Morgan Chase, the Block Center, the Center for Machine Learning and Health, and the CMU Software Engineering Institute (SEI) via Department of Defense contract FA8702-15-D-0002, for their generous support of ACMI Lab's research. JL is supported in part by NSF award DMS-2012286, and AR is supported in part by NSF awards IIS-2211907, CCF-2238523, Amazon Research Award, and the CMU/PwC DT&I Center.

## References

* Anderson (1965) Donald G Anderson. Iterative procedures for nonlinear integral equations. _Journal of the ACM (JACM)_, 1965.
* Anil et al. (2022) Cem Anil, Ashwini Pokle, Kaiqiu Liang, Johannes Treutlein, Yuhuai Wu, Shaojie Bai, J Zico Kolter, and Roger B Grosse. Path independent equilibrium models can better exploit test-time computation. _Advances in Neural Information Processing Systems_, 35:7796-7809, 2022.
* Bai et al. (2019) Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. _Advances in Neural Information Processing Systems_, 32, 2019.
* Bar-Sinai et al. (2019) Yohai Bar-Sinai, Stephan Hoyer, Jason Hickey, and Michael P Brenner. Learning data-driven discretizations for partial differential equations. _Proceedings of the National Academy of Sciences_, 116(31):15344-15349, 2019.
* Batchelor (1967) Cx K Batchelor and George Keith Batchelor. _An introduction to fluid dynamics_. Cambridge university press, 1967.
* Bhattacharya et al. (2021) Kaushik Bhattacharya, Bamdad Hosseini, Nikola B Kovachki, and Andrew M Stuart. Model reduction and neural networks for parametric PDEs. _The SMAI journal of computational mathematics_, 7:121-157, 2021.
* Boyd (2001) John P Boyd. _Chebyshev and Fourier spectral methods_. Courier Corporation, 2001.
* Brandstetter et al. (2022) Johannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural pde solvers. _arXiv preprint arXiv:2202.03376_, 2022.
* Broyden (1965) Charles G Broyden. A class of methods for solving nonlinear simultaneous equations. _Mathematics of computation_, 1965.
* Chen and Chen (1995) Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems. _IEEE Transactions on Neural Networks_, 6(4):911-917, 1995.
* Chen et al. (2021) Ziang Chen, Jianfeng Lu, and Yulong Lu. On the representation of solutions to elliptic pdes in barron spaces. _Advances in neural information processing systems_, 34:6454-6465, 2021.
* Chen et al. (2021)Gideon Dresdner, Dmitriri Kochkov, Peter Norgaard, Leonardo Zepeda-Nunez, Jamie A. Smith, Michael P. Brenner, and Stephan Hoyer. Learning to correct spectral methods for simulating turbulent flows. 2022. doi: 10.48550/ARXIV.2207.00556. URL https://arxiv.org/abs/2207.00556.
* Farago and Karatson (2002) Istvan Farago and Janos Karatson. _Numerical solution of nonlinear elliptic problems via preconditioning operators: Theory and applications_, volume 11. Nova Publishers, 2002.
* Fung et al. (2022) Samy Wu Fung, Howard Heaton, Qiuwei Li, Daniel McKenzie, Stanley Osher, and Wotao Yin. Jfb: Jacobian-free backpropagation for implicit networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 6648-6656, 2022.
* Geng et al. (2021) Zhengyang Geng, Xin-Yu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training implicit models. _Advances in Neural Information Processing Systems_, 34:24247-24260, 2021.
* Hao et al. (2023) Zhongkai Hao, Zhengyi Wang, Hang Su, Chenyang Ying, Yinpeng Dong, Songming Liu, Ze Cheng, Jian Song, and Jun Zhu. Gnot: A general neural operator transformer for operator learning. In _International Conference on Machine Learning_, pages 12556-12569. PMLR, 2023.
* Hendrycks and Gimpel (2016) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.
* Hornik et al. (1989) Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. _Neural networks_, 2(5):359-366, 1989.
* Hsieh et al. (2019) Jun-Ting Hsieh, Shengjia Zhao, Stephan Eismann, Lucia Mirabella, and Stefano Ermon. Learning neural pde solvers with convergence guarantees. _arXiv preprint arXiv:1906.01200_, 2019.
* Kochkov et al. (2021) Dmitrii Kochkov, Jamie A Smith, Ayya Alieva, Qing Wang, Michael P Brenner, and Stephan Hoyer. Machine learning-accelerated computational fluid dynamics. _Proceedings of the National Academy of Sciences_, 118(21):e2101784118, 2021.
* Kopriva (2009) David A Kopriva. _Implementing spectral methods for partial differential equations: Algorithms for scientists and engineers_. Springer Science & Business Media, 2009.
* Kovachki et al. (2021a) Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra. On universal approximation and error bounds for Fourier neural operators. _The Journal of Machine Learning Research_, 22(1):13237-13312, 2021a.
* Kovachki et al. (2021b) Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces. _arXiv preprint arXiv:2108.08481_, 2021b.
* Kovachki et al. (2023) Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to PDEs. _Journal of Machine Learning Research_, 24(89):1-97, 2023.
* Lanthaler et al. (2022) Samuel Lanthaler, Siddhartha Mishra, and George E Karniadakis. Error estimates for deeponets: A deep learning framework in infinite dimensions. _Transactions of Mathematics and Its Applications_, 6(1):tnac001, 2022.
* LeVeque (2007) Randall J LeVeque. _Finite difference methods for ordinary and partial differential equations: steady-state and time-dependent problems_. SIAM, 2007.
* Li et al. (2022a) Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations' operator learning. _arXiv preprint arXiv:2205.13671_, 2022a.
* Li et al. (2020a) Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. _arXiv preprint arXiv:2010.08895_, 2020a.
* Li et al. (2020b) Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. _arXiv preprint arXiv:2003.03485_, 2020b.
* Li et al. (2020b)Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural operator with learned deformations for pdes on general geometries. _arXiv preprint arXiv:2207.05209_, 2022b.
* Liao et al. [2018] Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, Kijung Yoon, Xaq Pitkow, Raquel Urtasun, and Richard Zemel. Reviving and improving recurrent back-propagation. In _International Conference on Machine Learning_, pages 3082-3091. PMLR, 2018.
* Liu et al. [2022] Xinliang Liu, Bo Xu, and Lei Zhang. Mitigating spectral bias for the multiscale operator learning with hierarchical attention. October 2022. URL http://arxiv.org/abs/2210.10890.
* Lu et al. [2019] Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. _arXiv preprint arXiv:1910.03193_, 2019.
* Marwah et al. [2021] Tanya Marwah, Zachary Lipton, and Andrej Risteski. Parametric complexity bounds for approximating PDEs with neural networks. _Advances in Neural Information Processing Systems_, 34:15044-15055, 2021.
* Marwah et al. [2022] Tanya Marwah, Zachary C Lipton, Jianfeng Lu, and Andrej Risteski. Neural network approximations of PDEs beyond linearity: Representational perspective. _arXiv preprint arXiv:2210.12101_, 2022.
* Moukalled et al. [2016] Fadl Moukalled, Luca Mangani, Marwan Darwish, F Moukalled, L Mangani, and M Darwish. _The finite volume method_. Springer, 2016.
* Patankar and Spalding [1983] Suhas V Patankar and D Brian Spalding. A calculation procedure for heat, mass and momentum transfer in three-dimensional parabolic flows. In _Numerical prediction of flow, heat transfer, turbulence and combustion_, pages 54-73. Elsevier, 1983.
* Patel et al. [2021] Ravi G Patel, Nathaniel A Trask, Mitchell A Wood, and Eric C Cyr. A physics-informed operator regression framework for extracting data-driven continuum models. _Computer Methods in Applied Mechanics and Engineering_, 373:113500, 2021.
* Revay et al. [2020] Max Revay, Ruigang Wang, and Ian R Manchester. Lipschitz bounded equilibrium networks. _arXiv preprint arXiv:2010.01732_, 2020.
* Tran et al. [2021] Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng Soon Ong. Factorized Fourier neural operators. _arXiv preprint arXiv:2111.13802_, 2021.
* Winston and Kolter [2020] Ezra Winston and J Zico Kolter. Monotone operator equilibrium networks. _Advances in neural information processing systems_, 33:10718-10728, 2020.

## Appendix A Implementation Details

Training details.We train all the networks for 500 epochs with Adam optimizer. The learning rate is set to 0.001 for Darcy flow and 0.005 for Navier-Stokes. We use learning rate weight decay of 1e-4 for both Navier-Stokes and Darcy flow. The batch size is set to 32. In case of Darcy flow, we also use cosine annealing for learning rate scheduling. We run all our experiments on a combination of NVIDIA RTX A6000, NVIDIA GeForce RTX 2080 Ti and 3080 Ti. All networks can easily fit on a single NVIDIA RTX A6000, but training time varies between the networks.

For FNO-DEQ, we use Anderson solver (Anderson, 1965) to solve for the fixed point in the forward pass. The maximum number of Anderson solver steps is kept fixed at 32 for Dary Flow, and 16 for Navier Stokes. For the backward pass, we use phantom gradients (Geng et al., 2021) which are computed as:

\[u^{\star}=\tau G_{\theta}(u^{\star},a)+(1-\tau)u^{\star}\] (12)

where \(\tau\) is a tunable damping factor and \(u^{\star}\) is the fixed point computed using Anderson solver in the forward pass. This step can be repeated \(S\) times. We use \(\tau=0.5\) and \(S=1\) for Darcy Flow, and \(\tau=0.8\) and \(S=3\) for Navier-Stokes.

For the S-FNO-DEQ used in Table 1, we use Broyden's method (Broyden, 1965) to solve for the fixed point in the forward pass and use exact implicit gradients, computed through implicit function theorem as shown in Eq. (6), for the backward pass through DEQ. The maximum number of solver steps is fixed at 32.

For weight-tied networks, we repeatedly apply the FNO block to the input \(12\) times for Darcy flow, and \(6\) times for Navier-Stokes.

Network architecture details.The width of an FNO layer set to 32 across all the networks. Additionally, we retain only 12 Fourier modes in FNO layer, and truncate higher Fourier modes. We use the code provided by Li et al. (2020) to replicate the results for FNO, and construct rest of the networks on top of this as described in Sec. 5.

## Appendix B Datasets

### Darcy Flow

As mentioned in Sec. 5 we use the dataset provided by Li et al. (2020) for our experiments with steady-state Darcy-Flow.

All the models are trained on 1024 data samples and tested on 500 samples. The resolution of original images is \(421\times 421\) which we downsample to \(85\times 85\) for our experiments. For experiments with noisy inputs/observations, the variance of Gaussian noise that we add to PDEs are [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3].

### Steady-State Incompressible Fluid Navier-Stoke

\[u\cdot\nabla\omega =\nu\Delta\omega+f, x\in\Omega\] \[\nabla\cdot u =0 x\in\Omega\]

To generate the dataset for steady-state Navier-Stokes, instead of solving the steady state PDE using steady-state solvers like the SIMPLE algorithm Patankar and Spalding (1983), we first choose the solution \(\omega^{\star}:=\nabla\times u^{\star}\) of the PDE and then generate the corresponding equation, i.e. calculate the corresponding force term \(f=u^{\star}\cdot\nabla\omega^{\star}-\nu\Delta\omega^{\star}\).

To generate the solutions \(\omega^{\star}\), we forward propagate a relatively simple initial distribution of \(\omega_{0}\) (sampled from a Gaussian random field) through a time-dependent Navier-Stokes equation in the vorticity form for a short period of time. This ensures our dataset contains solutions \(\omega^{\star}\) that are richand complex. Precisely, recall the Navier-Stokes equations in their vorticity form:

\[\partial_{t}\omega(x,t)+u(x,t)\cdot\nabla\omega(x,t) =\nu\Delta\omega(x,t)+g(x)\qquad x\in(0,2\pi)^{2},t\in[0,T]\] (13) \[\nabla\cdot u(x,t) =0\qquad x\in(0,2\pi)^{2},t\in[0,T]\] \[\omega(x,0) =\omega_{0}(x)\qquad x\in(0,2\pi)^{2}\]

where \(g(x)=\nabla\times\tilde{g}(x)\) and \(\tilde{g}(x)=\sin(5x_{1})\hat{x}_{2}\) is a divergence free forcing term and \(x=(x_{1},x_{2})\) are the two coordinates of the input vector. We forward propagate the equations Equation 13 using a pseudo-spectral method using the functions provided in JAX-CFD (Kochkov et al., 2021; Dresdener et al., 2022) package. The initial vorticity \(\omega_{0}\) is sampled from a Gaussian random field \(\mathcal{N}(0,(5^{3/2}(I+25\Delta)^{-2.5}))\), which is then made divergence free. We forward propagate the Navier-Stokes equation in Equation 13 for time \(T=0.5\) with \(dt=0.002\) to get \(\omega(1,x)\), which we choose as the solution to the steady-state PDE in Equation 10, i.e, \(\omega^{\star}\) for Equation 10.

Subsequently, we use the stream function \(\Psi\)(Batchelor and Batchelor, 1967) to calculate \(u=(\partial\Psi/\partial x_{1},\partial\Psi/\partial x_{2})\) by solving the Poisson equation \(\Delta\Psi=\omega\) in the Fourier domain. Furthermore, since \(f=u^{\star}\cdot\nabla\omega^{\star}-\nu\Delta\omega^{\star}\) we use the stream function to calculate \((f_{1},f_{2})\), i.e., the different components of the force term.

We use \(4500\) training samples and \(500\) testing samples. The input to the network is the vector field \(\tilde{f}=(f_{1},f_{2})\) and we learn a map that outputs the vorticity \(\omega^{\star}\). The resolution of grid used to

Figure 1: Samples from Darcy Flow

generate the dataset is \(256\times 256\) which we downsample to \(128\times 128\) while training the models. For experiments with noisy inputs/observations, we consider two values of maximum variance of Gaussian noise: 1e-3 and 4e-3. The variances of the Gaussian noise that we add to the PDEs for the latter case are [0, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 2e-3, 4e-3]. However, when conducting experiments with a variance of 1e-3, we exclude the last two values of variance from this list.

## Appendix C Proof of Universal Approximation

The proof of the universal approximation essentially follows from the result on the universal approximation capabilities of FNO layers in Kovachki et al. (2021), applied to \(\mathcal{G}(v,f)=v-(Lv-f)\). For the sake of completeness, we reiterate the key steps.

For simplicity, we will assume that \(d_{u}=d_{v}=d_{f}=1\). (The results straightforwardly generalize.) We will first establish some key technical lemmas and introduce some notation and definitions useful for the proof for Theorem 1.

**Definition 7**.: _An operator \(T:L^{2}(\Omega;\mathbb{R})\to L^{2}(\Omega;\mathbb{R})\) is continuous at \(u\in L^{2}(\Omega;\mathbb{R})\) if for every \(\epsilon>0\), there exists a \(\delta>0\), such that for all \(v\in L^{2}(\Omega)\) with \(\|u-v\|_{L^{2}(\Omega)}\leq\delta\), we have \(\|L(u)-L(v)\|_{L^{2}(\Omega)}\leq\epsilon\)._

Figure 2: Samples from Steady-state Navier-Stokes dataset with viscosity \(0.001\). Each triplet visualizes the inputs \(f_{1}\), \(f_{2}\) and the ground truth output i.e. \(\omega^{\star}\).

First, we approximate the infinite dimensional operator \(\mathcal{G}:L^{2}(\Omega)\times L^{2}(\Omega)\to L^{2}(\Omega)\) by projecting the functions in \(L^{2}(\Omega)\) to a finite-dimensional approximation \(L^{2}_{N}(\Omega)\), and considering the action of the operator on this subspace. The linear projection we use is the one introduced in Equation 11. More precisely we show the following result,

**Lemma 2**.: _Given a continuous operator \(L:L^{2}(\Omega)\to L^{2}(\Omega)\) as defined in Equation 1, let us define an operator \(\mathcal{G}:L^{2}(\Omega)\times L^{2}(\Omega)\to L^{2}(\Omega)\) as \(\mathcal{G}(v,f):=v-(L(v)-f)\). Then, for every \(\epsilon>0\) there exists an \(N\in\mathbb{N}\) such that for all \(v,f\) in any compact set \(K\subset L^{2}(\Omega)\), the operator \(\mathcal{G}_{N}=\Pi_{N}\mathcal{G}(\Pi_{N}v,\Pi_{N}f)\) is an \(\epsilon\)-approximation of \(\mathcal{G}(v,f)\), i.e., we have,_

\[\sup_{v,f\in K}\left\|\mathcal{G}(v,f)-\mathcal{G}_{N}(v,f)\right\|_{L^{2}( \Omega)}\leq\epsilon.\]

Proof.: Note that for an \(\epsilon>0\) there exists an \(N=N(\epsilon,d)\) such that for all \(v\in K\) we have

\[\sup_{v\in K}\left\|v-\Pi_{N}v\right\|_{L^{2}(\Omega)}\leq\epsilon.\]

Figure 3: Samples from Steady-state Navier-Stokes dataset with viscosity \(0.01\). Each triplet visualizes the inputs \(f_{1}\), \(f_{2}\) and the ground truth output i.e. \(\omega^{\star}\).

[MISSING_PAGE_FAIL:17]

From Kovachki et al. (2021a), Theorem 2.4 we know that there exists an FNO network \(G_{\theta^{L}}\) of the form defined in Equation 2 such that for all \(v\in K\), where \(K\) is a compact set, there exists an \(\epsilon^{L}\) we have

\[\sup_{v\in K}\|\Pi_{N}L(\Pi_{N}v)-G_{\theta^{L}}\|_{L^{2}(\Omega)}\leq\epsilon ^{L}\] (19)

Finally, note that from Lemma D.1 in Kovachki et al. (2021a), we have that for any \(v\in K\), there exists an FNO layers \(G_{\theta^{f}}\in L^{2}(\Omega)\) and \(G_{\theta^{v}}\in L^{2}(\Omega)\) defined in Equation 3 such that

\[\sup_{v\in K}\|\Pi_{N}v-G_{\theta^{v}}\|_{L^{2}(\Omega)}\leq\epsilon^{v}\] (20)

and

\[\sup_{f\in K}\|\Pi_{N}f-G_{\theta^{f}}\|_{L^{2}(\Omega)}\leq\epsilon^{f}\] (21)

for \(\epsilon^{v}>0\) and \(\epsilon^{f}>0\).

Therefore there exists an \(\tilde{\epsilon}>\) such that there is an FNO network \(G_{\theta}:L^{2}(\Omega)\times L^{2}(\Omega)\to L^{2}(\Omega)\) where \(\theta:=\{\theta^{L},\theta^{v},\theta^{f}\}\) such that

\[\sup_{v\in K,f\in L^{2}(\Omega)}\|\mathcal{G}_{N}(v,f)-G_{\theta}(v,f)\|_{L^{2 }(\Omega)}\leq\tilde{\epsilon}\] (22)

Now, since we know that \(u^{\star}\) is the fixed point of the operator \(\mathcal{G}\) we have from Lemma 2 and Equation 22,

\[\|\mathcal{G}(u^{\star},f)-G_{\theta}(u^{\star},f)\|_{L^{2}(\Omega)} \leq\|u^{\star}-\mathcal{G}_{N}(u^{\star},f)\|_{L^{2}(\Omega)}+\| \mathcal{G}_{N}(u^{\star},f)-G_{\theta}(u^{\star},f)\|_{L^{2}(\Omega)}\] \[\leq\tilde{\epsilon}+\epsilon.\]

## Appendix D Fast Convergence for Newton Method

**Definition 8** (Frechet Derivative in \(L^{2}(\Omega)\)).: _For a continuous operator \(F:L^{2}(\Omega)\to L^{2}(\Omega)\), the Frechet derivative at \(u\in L^{2}(\Omega)\) is a linear operator \(F^{\prime}(u):L^{2}(\Omega)\to L^{2}(\Omega)\) such that for all \(v\in L^{2}(\Omega)\) we have_

\[\lim_{\|v\|_{L^{2}(\Omega)}\to 0}\frac{\|F(u+v)-F(u)-F^{\prime}(u)(v)\|_{L^{2}( \Omega)}}{\|v\|_{L^{2}(\Omega)}}=0.\]

**Lemma 3**.: _Given the operator \(L:L^{2}(\Omega)\to L^{2}(\Omega)\) with Frechet derivative \(L^{\prime}\), such that for all \(u,v\in L^{2}(\Omega)\), we have \(\|L^{\prime}(u)(v)\|_{L^{2}(\Omega)}\geq\lambda\|v\|_{L^{2}(\Omega)}\), then \(L^{\prime}(u)^{-1}\) exists and we have, for all \(v_{1},v_{2}\in L^{2}(\Omega)\):_

1. \(\|L^{\prime}(u)^{-1}(v_{1})\|_{L^{2}(\Omega)}\leq\frac{1}{\lambda}\|v_{1}\|_{ L^{2}(\Omega)}\)_._
2. \(\|v_{1}-v_{2}\|_{L^{2}(\Omega)}\leq\frac{1}{\lambda}\|L(v_{1})-L(v_{2})\|_{L^{2}( \Omega)}\)__

Proof.: Note that for all \(u,v^{\prime}\in L^{2}(\Omega)\) we have,

\[\|L^{\prime}(u)v^{\prime}\|_{L^{2}(\Omega)}\geq\lambda\|v^{\prime}\|_{L^{2}( \Omega)}\]

Taking \(v=L^{\prime}(u)^{-1}(v^{\prime})\), we have

\[\|L^{\prime}(u)\left(L^{\prime}(u)^{-1}(v)\right)\|_{L^{2}(\Omega)} \geq\lambda\|L^{-1}(u)(v)\|_{L^{2}(\Omega)}\] \[\implies\frac{1}{\lambda}\|v\|_{L^{2}(\Omega)} \geq\|L^{-1}(u)(v)\|_{L^{2}(\Omega)}.\]

For part \(2\), note that there exists a \(c\in[0,1]\) such that

\[\|L(v_{1})-L(v_{2})\|_{L^{2}(\Omega)}\geq\inf_{c\in[0,1]}\|L^{\prime}(cv_{1}+( 1-c)v_{2})\|_{2}\|v_{1}-v_{2}\|_{L^{2}(\Omega)}\geq\lambda\|v_{1}-v_{2}\|_{L^{2 }(\Omega)}.\]We now show the proof for Lemma 4. The proof is standard and can be found in Farago and Karatson (2002), however we include the complete proof here for the sake of completeness.

We restate the Lemma here for the convenience of the reader.

**Lemma 4** (Farago and Karatson (2002), Chapter 5).: _Consider the PDE defined Definition 2, such that \(d_{u}=d_{v}=d_{f}=1\). such that \(L^{\prime}(u)\) defines the Frechet derivative of the operator \(L\). If for all \(u,v\in L^{2}(\Omega;\mathbb{R})\) we have \(\|L^{\prime}(u)v\|_{L^{2}(\Omega)}\geq\lambda\|v\|_{L^{2}(\Omega)}\)7 and \(\|L^{\prime}(u)-L^{\prime}(v)\|_{L^{2}(\Omega)}\leq\lambda\|u-v\|_{L^{2}( \Omega)}\) for \(0<\lambda\leq\Lambda<\infty\), then for the Newton update, \(u_{t+1}\gets u_{t}-L^{\prime}(u_{t})^{-1}\left(L(u_{t})-f\right),\) with \(u_{0}\in L^{2}(\Omega;\mathbb{R})\), there exists an \(\epsilon>0\), such that \(\|u_{T}-u^{\star}\|_{L^{2}(\Omega)}\leq\epsilon\) if 8\(T\geq\log\left(\log\left(\frac{1}{\epsilon}\right)/\log\left(\frac{2\lambda^{2} }{\lambda\|L(u_{0})-f\|_{L^{2}(\Omega)}}\right)\right).\)_

Footnote 7: We note that this condition is different from the condition on the inner-product in the submitted version of the paper, which had. \(\langle L^{\prime}(u),v\rangle_{L^{2}(\Omega)}\geq\lambda\|v\|_{L^{2}(\Omega)}\).

Footnote 8: We note that this rate is different from the one in the submitted version of the paper.

Proof of Lemma 4.: Re-writing the updates in Lemma 4 as,

\[u_{t+1} =u_{t}+p_{t}\] (23) \[L^{\prime}(u_{t})p_{t} =-(L(u_{t})-f)\] (24)

Now, upper bounding \(L(u_{t+1})-f\) for all \(x\in\Omega\) we have,

\[L(u_{t+1}(x))-f(x)\] \[=L(u_{t}(x))-f(x)+\int_{0}^{1}\left(L^{\prime}(u_{t}(x)+t(u_{t+1} (x)-u_{t}(x)))\right)(u_{t+1}(x)-u_{t}(x))dt\] \[=L(u_{t}(x))-f(x)+L^{\prime}(u_{t}(x))p_{t}(x)+\int_{0}^{1}\left( L^{\prime}(u_{t}(x)+t(u_{t+1}(x)-u_{t}(x)))-L^{\prime}(u_{t}(x))\right)p_{t}(x)dt\] \[=\int_{0}^{1}\left(L^{\prime}(u_{t}(x)+t(u_{t+1}(x)-u_{t}(x)))-L ^{\prime}(u_{t}(x))\right)p_{t}(x)dt\]

where we use Equation 24 in the final step.

Taking \(L^{2}(\Omega)\) norm on both sides and using the fact that \(\|L^{\prime}(u)-L^{\prime}(v)\|_{L^{2}(\Omega)}\leq\Lambda\|u-v\|_{L^{2}( \Omega)}\), we have

\[\|L(u_{t+1})-f\|_{L^{2}(\Omega)}\leq\int_{0}^{1}\Lambda t\|u_{t+1}-u_{t}\|_{L^ {2}(\Omega)}\|p_{t}\|_{L^{2}(\Omega)}dt\]

Noting that for all \(x\in\Omega\), we have \(u_{t+1}-u_{t}=p_{t}\), and using the fact that for all \(u,v\)\(\|L^{\prime}(u)^{-1}v\|_{L^{2}(\Omega)}\leq\frac{1}{\lambda}\|v\|_{L^{2}( \Omega)}\) we have, \(\|L^{\prime}(u)p_{t}\|_{L^{2}(\Omega)}\leq\frac{1}{\lambda}\|p_{t}\|_{L^{2}( \Omega)}\)

\[\|L(u_{t+1})-f\|_{L^{2}(\Omega)} \leq\int_{0}^{1}\Lambda t\|u_{t+1}-u\|_{L^{2}(\Omega)}\|p_{t}\|_{L ^{2}(\Omega)}dt\] \[\leq\Lambda/2\|p_{t}\|_{L^{2}(\Omega)}^{2}\] \[\leq\Lambda/2\|-L^{\prime}(u_{t})^{-1}(L(u_{t})-f)\|_{L^{2}( \Omega)}^{2}\] \[\leq\frac{\Lambda}{2\lambda^{2}}\|L(u_{t})-f)\|_{L^{2}(\Omega)}^{2}\]

where we use the result from Lemma 3 in the last step.

Therefore we have

\[\|L(u_{t+1})-f\|_{L^{2}(\Omega)} \leq\left(\frac{\Lambda}{2\lambda^{2}}\right)^{2^{t}-1}\left(L(u_{ 0})-f\right)^{2^{t}}\] \[\implies\|L(u_{t+1})-f\|_{L^{2}(\Omega)} \leq\left(\frac{\Lambda}{2\lambda^{2}}\right)^{2^{t}-1}\left(L(u_ {0})-L(u^{\star})\right)^{2^{t}}\] \[\implies\|u_{t+1}-u^{\star}\|_{L^{2}(\Omega)} \leq\frac{1}{\lambda}\left(\frac{\Lambda}{2\lambda^{2}}\right)^{2^ {t}-1}\|L(u_{0})-L(u^{\star})\|_{L^{2}(\Omega)}^{2^{t}}\,.\]Therefore, if

\[\frac{\Lambda}{2\lambda^{2}}\|L(u_{0})-L(u^{*})\|_{L^{2}(\Omega)}\leq 1,\]

then we have

\[\|u_{t+1}-u^{*}\|_{L^{2}(\Omega)}\leq\epsilon,\]

for

\[T\geq\log\left(\log\left(\frac{1}{\epsilon}\right)/\log\left( \frac{2\lambda^{2}}{\Lambda\|L(u_{0})-f\|_{L^{2}(\Omega)}}\right)\right).\]

## Appendix E Additional experimental results

We provide additional results for Navier-Stokes equation for noisy inputs and observations in Table 4 and Table 5. For these experiments, the maximum variance of Gaussian noise added to inputs and observations is 0.004. We observe that weight-tied FNO and FNO-DEQ outperform non-weight-tied architectures.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Architecture} & \multirow{2}{*}{Parameters} & \multirow{2}{*}{\#Blocks} & \multicolumn{4}{c}{Test error \(\downarrow\)} \\ \cline{4-6}  & & & \(\sigma_{\max}^{2}=0\) & \((\sigma_{\max}^{2})^{i}=0.004\) & \((\sigma_{\max}^{2})^{t}=0.004\) \\ \hline FNO & 2.37M & 1 & 0.184 \(\pm\) 0.002 & 0.238 \(\pm\) 0.008 & 0.179 \(\pm\) 0.004 \\ FNO & 4.15M & 2 & 0.162 \(\pm\) 0.024 & 0.196 \(\pm\) 0.011 & 0.151 \(\pm\) 0.010 \\ FNO & 7.71M & 4 & 0.157 \(\pm\) 0.012 & 0.216 \(\pm\) 0.002 & 0.158 \(\pm\) 0.009 \\ \hline FNO++ & 2.37M & 1 & 0.199 \(\pm\) 0.001 & 0.255 \(\pm\) 0.002 & 0.197 \(\pm\) 0.004 \\ FNO++ & 4.15M & 2 & 0.154 \(\pm\) 0.005 & 0.188 \(\pm\) 0.006 & 0.157 \(\pm\) 0.006 \\ FNO++ & 7.71M & 4 & 0.151 \(\pm\) 0.003 & 0.184 \(\pm\) 0.008 & 0.147 \(\pm\) 0.004 \\ \hline FNO-WT & 2.37M & 1 & 0.151 \(\pm\) 0.007 & 0.183 \(\pm\) 0.026 & 0.129 \(\pm\) 0.018 \\ FNO-DEQ & 2.37M & 1 & **0.128 \(\pm\) 0.004** & **0.159 \(\pm\) 0.005** & **0.121 \(\pm\) 0.015** \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Results on incompressible Steady-State Navier-Stokes (viscosity=0.001)**: clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being \((\sigma_{\max}^{2})^{i}\) and \((\sigma_{\max}^{2})^{t}\), respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Architecture} & \multirow{2}{*}{Parameters} & \multirow{2}{*}{\#Blocks} & \multicolumn{4}{c}{Test error \(\downarrow\)} \\ \cline{4-6}  & & & \(\sigma_{\max}^{2}=0\) & \((\sigma_{\max}^{2})^{i}=0.004\) & \((\sigma_{\max}^{2})^{t}=0.004\) \\ \hline FNO & 2.37M & 1 & 0.181 \(\pm\) 0.005 & 0.207 \(\pm\) 0.003 & 0.178 \(\pm\) 0.008 \\ FNO & 4.15M & 2 & 0.138 \(\pm\) 0.007 & 0.163 \(\pm\) 0.003 & 0.137 \(\pm\) 0.006 \\ FNO & 7.71M & 4 & 0.152 \(\pm\) 0.006 & 0.203 \(\pm\) 0.055 & 0.151 \(\pm\) 0.008 \\ \hline FNO++ & 2.37M & 1 & 0.188 \(\pm\) 0.002 & 0.217 \(\pm\) 0.001 & 0.187 \(\pm\) 0.005 \\ FNO++ & 4.15M & 2 & 0.139 \(\pm\) 0.004 & 0.170 \(\pm\) 0.005 & 0.138 \(\pm\) 0.005 \\ FNO++ & 7.71M & 4 & 0.130 \(\pm\) 0.005 & 0.168 \(\pm\) 0.007 & 0.126 \(\pm\) 0.007 \\ \hline FNO-WT & 2.37M & 1 & 0.099 \(\pm\) 0.007 & 0.159 \(\pm\) 0.029 & 0.123 \(\pm\) 0.023 \\ FNO-DEQ & 2.37M & 1 & **0.088 \(\pm\) 0.006** & **0.104 \(\pm\) 0.001** & **0.116 \(\pm\) 0.005** \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Results on incompressible Steady-State Navier-Stokes (viscosity=0.01)**: clean data (Col 4), noisy inputs (Col 5) and noisy observations (Col 6) with max variance of added noise being \((\sigma_{\max}^{2})^{i}\) and \((\sigma_{\max}^{2})^{t}\), respectively. Reported test error has been averaged on three different runs with seeds 0, 1, and 2.

[MISSING_PAGE_FAIL:21]

Figure 4: Training and Test Loss Curves for Steady-State Navier-Stokes with viscosity \(0.01\). The \(x\) axis is the number of epochs and \(y\) axis is the MSE loss in \(\log\) scale. Note that while all the models converge to approximately the same MSE loss value while training, DEQs and weight-tied networks get a better test loss in fewer epochs.