# Solving Linear Inverse Problems Provably via

Posterior Sampling with Latent Diffusion Models

 Litu Rout  Negin Raoof  Giannis Daras

Constantine Caramanis  Alexandros G. Dimakis  Sanjay Shakkottai

The University of Texas at Austin

Email:{litu.rout,neginmr,giannisdaras,constantine,sanjay.shakkottai}utexas.edu, dimakis@austin.utexas.edu

###### Abstract

We present the first framework to solve linear inverse problems leveraging pre-trained _latent_ diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to _pixel-space_ diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.

## 1 Introduction

We study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising, inpainting, compressed sensing and super-resolution. There are two classes of approaches for inverse problems: supervised methods where a restoration model is trained to solve the task at hand [37, 39, 56, 31], and unsupervised methods that use the prior learned by a generative model to guide the restoration process [52, 40, 5, 33, 11, 26]; see also the survey of Ongie et al. [36] and references therein.

The second family of unsupervised methods has gained popularity because: (i) general-domain foundation generative models have become widely available, (ii) unsupervised methods do not require any training to solve inverse problems and leverage the massive data and compute investment of pre-trained models and (iii) generative models _sample_ from the posterior-distribution, mitigating certain pitfalls of likelihood-maximization methods such as bias in the reconstructions [35, 24] and regression to the mean [23, 22].

Diffusion models have emerged as a powerful new approach to generative modeling [47, 48, 49, 20, 29, 18, 54]. This family of generative models works by first corrupting the data distribution \(p_{0}(\bm{x}_{0})\) using an Ito Stochastic Differential Equation (SDE), \(\mathrm{d}\bm{x}=\bm{f}(\bm{x},t)\mathrm{d}t+g(t)\mathrm{d}\bm{w}\), and then by learning the score-function, \(\nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t})\), at all levels \(t\), using Denoising Score Matching (DSM) [21, 53]. The seminal result of Anderson [1] shows that we can reverse the corruption process, i.e., start with noise and then sample from the data distribution, by running another Ito SDE. The SDE that corrupts the data is often termed as Forward SDE and its reverse as Reverse SDE [49]. The latter depends on the score-function \(\nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t})\) that we learn through DSM. In [8, 9], the authors provided a non-asymptotic analysis for the sampling of diffusion models when the score-function is only learned approximately.

The success of diffusion models sparked the interest to investigate how we can use them to solve inverse problems. Song et al. [49] showed that given measurements \(\bm{y}=\mathcal{A}\bm{x}_{0}+\sigma_{y}\bm{n}\), we canprovably sample from the distribution \(p_{0}(\bm{x}_{0}|\bm{y})\) by running a modified Reverse SDE that depends on the unconditional score \(\nabla_{\bm{x}_{i}}\log p_{t}(\bm{x}_{t})\) and the term \(\nabla_{\bm{x}_{i}}\log p(\bm{y}|\bm{x}_{t})\). The latter term captures how much the current iterate explains the measurements and it is intractable even for linear inverse problems without assumptions on the distribution \(p_{0}(x_{0})\)[11; 14]. To deal with the intractability of the problem, a series of approximation algorithms have been developed [22; 11; 2; 13; 26; 10; 6; 46; 12; 27] for solving (linear and non-linear) inverse problems with diffusion models. These algorithms use pre-trained diffusion models as flexible priors for the data distribution to effectively solve problems such as inpainting, deblurring, super-resolution among others.

Recently, diffusion models have been generalized to learn to invert non-Markovian and non-linear corruption processes [16; 15; 3]. One instance of this generalization is the family of Latent Diffusion Models (LDMs) [41]. LDMs project the data into some latent space, \(\bm{z}_{0}=\mathcal{E}(\bm{x}_{0})\), perform the

Figure 1: Overall pipeline of our proposed framework from left to right. Given an image (**left**) and a user defined mask (**center**), our algorithm inpaints the masked region (**right**). The known part of the images are unaltered (see Appendix C for web demo and image sources).

diffusion in the latent space and use a decoder, \(\mathcal{D}(\bm{z}_{0})\), to move back to the pixel space. LDMs power state-of-the-art foundation models such as Stable Diffusion [41] and have enabled a wide-range of applications across many data modalities including images [41], video [4], audio [30] and medical domain distributions (e.g., for MRI and proteins) [38; 51]. Unfortunately, none of the existing algorithms for solving inverse problems works with Latent Diffusion Models. Hence, to use a foundation model, such as Stable Diffusion, for some inverse problem, one needs to perform finetuning for each task of interest.

In this paper, we present the first framework to solve general inverse problems with pre-trained _latent_ diffusion models. Our main idea is to extend DPS by adding an extra gradient update step to guide the diffusion process to sample latents for which the decoding-encoding map is not lossy. By harnessing the power of available foundation models, we are able to outperform previous approaches without finetuning across a wide range of problems (see Figure 1 and 2).

**Our contributions are as follows:**

1. We show how to use Latent Diffusion Models models (such as Stable Diffusion) to solve linear inverse problem when the degradation operator is known.
2. We theoretically analyze our algorithm and show provable sample recovery in a linear model setting with two-step diffusion processes.
3. We achieve a new state-of-the-art for solving inverse problems with latent diffusion models, outperforming previous approaches for inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.2 Footnote 2: The source code is available at: https://github.com/LituRout/PSLD and a web application for image inpainting is available at: https://huggingface.co/spaces/PSLD/PSLD.

## 2 Background and Method

**Notation:** Bold lower-case \(\bm{x}\), bold upper-case \(\bm{X}\), and normal lower case \(x\) denote a vector, a matrix, and a scalar variable, respectively. We denote by \(\odot\) element-wise multiplication. \(\bm{D}(\bm{x})\) represents a diagonal matrix with entries \(\bm{x}\). We use \(\mathcal{E}(.)\) for the encoder and \(\mathcal{D}(.)\) for the decoder. \(\mathcal{E}\sharp p\) is a pushforward measure of \(p\), i.e., for every \(\bm{x}\in p\), the sample \(\mathcal{E}(\bm{x})\) is a sample from \(\mathcal{E}\sharp p\). We use arrows in Section 3 to distinguish random variables of the forward (\(\rightarrow\)) and the reverse process (\(\leftarrow\)).

The standard diffusion modeling framework involves training a network, \(\bm{s}_{\theta}(\bm{x}_{t},t)\), to learn the score-function, \(\nabla_{\bm{x}_{t}}\log p_{t}(\bm{x}_{t})\), at all levels \(t\), of a stochastic process described by an Ito SDE:

\[\mathrm{d}\bm{x}=\bm{f}(\bm{x},t)\mathrm{d}t+g(t)\mathrm{d}\bm{w},\] (1)

where \(\bm{w}\) is the standard Wiener process. To generate samples from the trained model, one can run the (unconditional) Reverse SDE, where the score-function is approximated by the trained neural network. Given measurements \(\bm{y}=\mathcal{A}x_{0}+\sigma_{y}\bm{n}\), one can sample from the distribution \(p_{0}(\bm{x}_{0}|\bm{y})\) by running the conditional Reverse SDE given by:

\[\mathrm{d}\bm{x}=\big{(}\bm{f}(\bm{x},t)-g^{2}(t)\left(\nabla_{\bm{x}_{t}} \log p_{t}(\bm{x}_{t})+\nabla_{\bm{x}_{t}}\log p(\bm{y}|\bm{x}_{t})\right) \big{)}\,\mathrm{d}t+g(t)\mathrm{d}\bm{w}.\] (2)

As mentioned, \(\nabla_{\bm{x}_{t}}\log p(\bm{y}|\bm{x}_{t})\) is intractable for general inverse problems. One of the most effective approximation methods is the DPS algorithm proposed by Chung et al. [11]. DPS assumes that:

\[p(\bm{y}|\bm{x}_{t})\approx p\left(\bm{y}|\hat{\bm{x}}_{0}\coloneqq\mathbb{E}[ \bm{x}_{0}|\bm{x}_{t}]\right)=\mathcal{N}(\bm{y};\mu=\mathcal{A}\mathbb{E}[ \bm{x}_{0}|\bm{x}_{t}],\Sigma=\sigma_{y}^{2}I).\] (3)

Essentially, DPS substitutes the unknown clean image \(\bm{x}_{0}\) with its conditional expectation given the noisy input, \(\mathbb{E}[\bm{x}_{0}|\bm{x}_{t}]\). Under this approximation, the term \(p(\bm{y}|\bm{x}_{t})\) becomes tractable.

The theoretical properties of the DPS algorithm are not well understood. In this paper, we analyze DPS in a linear model setting where the data distribution lives in a low-dimensional subspace, and show that DPS actually samples from \(p(\bm{x}_{0}|\bm{y})\) (Section A.1). Then, we provide an _algorithm_ (Section 2.1) and its _analysis_ to sample from \(p(\bm{x}_{0}|\bm{y})\) using latent diffusion models (Section 3.2). Importantly, our analysis suggests that our algorithm enjoys the same theoretical guarantees while avoiding the curse of ambient dimension observed in pixel-space diffusion models including DPS. Using experiments (Section 4), we show that our algorithm allows us to use powerful foundation models and solve linear inverse problems, outperforming previous unsupervised approaches without the need for finetuning.

### Method

In Latent Diffusion Models, the diffusion occurs in the latent space. Specifically, we train a model \(\bm{s}_{\theta}(\bm{z}_{t},t)\) to predict the score \(\nabla_{\bm{z}_{t}}\log p_{t}(\bm{z}_{t})\), of a diffusion process:

\[\mathrm{d}\bm{z}=\bm{f}(\bm{z},t)\mathrm{d}t+g(t)\mathrm{d}\bm{w},\] (4)

where \(\bm{z}_{0}=\mathcal{E}(\bm{x}_{0})\) for some encoder function \(\mathcal{E}(\cdot):\mathbb{R}^{d}\rightarrow\mathbb{R}^{k}\). During sampling, we start with \(\bm{z}_{T}\), we run the Reverse Diffusion Process and then we obtain a clean image by passing \(\bm{z}_{0}\sim p_{0}(\bm{z}_{0}|\bm{z}_{T})\) through a decoder \(\mathcal{D}:\mathbb{R}^{k}\rightarrow\mathbb{R}^{d}\).

Although Latent Diffusion Models underlie some of the most powerful foundation models for image generation, existing algorithms for solving inverse problems with diffusion models do not apply for LDMs. The most natural extension of the DPS idea would be to approximate \(p(\bm{y}|\bm{z}_{t})\) with:

\[p(\bm{y}|\bm{z}_{t})\approx p(\bm{y}|\bm{x}_{0}=\mathcal{D}\left(\mathbb{E}[ \bm{z}_{0}|\bm{z}_{t}]\right)),\] (5)

i.e., to approximate the unknown clean image \(\bm{x}_{0}\) with the decoded version of the conditional expectation of the clean latent \(\bm{z}_{0}\) given the noisy latent \(\bm{z}_{t}\). However, as we show experimentally in Section 4, this idea does not work. The failure of the "vanilla" extension of the DPS algorithm for latent diffusion models should not come as a surprise. The fundamental reason is that the encoder is a many-to-one mapping. Simply put, there are many latents \(\bm{z}_{0}\) that correspond to encoded versions of images that explain the measurements. Taking the gradient of the density given by (5) could be pulling \(\bm{z}_{t}\) towards any of these latents \(\bm{z}_{0}\), potentially in different directions. On the other hand, the score-function is pulling \(\bm{z}_{t}\) towards a specific \(\bm{z}_{0}\) that corresponds to the best denoised version of \(\bm{z}_{t}\).

To address this problem, we propose an extra term that penalizes latents that are not fixed-points of the composition of the decoder-function with the encoder-function. Specifically, we approximate the intractable \(\nabla\log p(\bm{y}|\bm{z}_{t})\) with:

\[\nabla_{\bm{z}_{t}}\log p(\bm{y}|\bm{z}_{t})=\underbrace{\nabla_{\bm{z}_{t}} \log p(\bm{y}|\hat{\bm{x}}_{0}=\mathcal{D}\left(\mathbb{E}[\bm{z}_{0}|\bm{z}_ {t}]\right))}_{\text{DPS vanilla extension}}+\gamma_{t}\underbrace{\nabla_{\bm{z}_{t}} \left|\left|\mathbb{E}[\bm{z}_{0}|\bm{z}_{t}]-\mathcal{E}(\mathcal{D}( \mathbb{E}[\bm{z}_{0}|\bm{z}_{t}]))\right|\right|^{2}}_{\text{``goodness'' of $\bm{z}_{0}$}}.\] (6)

We refer to this approximation as Goodness Modified Latent DPS (GML-DPS). Intuitively, we guide the diffusion process towards latents such that: i) they explain the measurements when passed through the decoder, and ii) they are fixed points of the decoder-encoder composition. The latter is useful to make sure that the generated sample remains on the manifold of real data. However, it does not penalize the reverse SDE for generating other latents \(\bm{z}_{0}\) as long as \(\mathcal{D}(\bm{z}_{0})\) lies on the manifold of natural images. Even in the linear case (see Section 3), this can lead to inconsistency at the boundary of the mask in the pixel space. The linear theory in Section 3 suggests that we can circumvent this problem by introducing the following gluing objective. In words, the gluing objective penalizes decoded images having a discontinuity at the boundary of the mask.

\[\nabla_{\bm{z}_{t}}\log p(\bm{y}|\bm{z}_{t}) =\underbrace{\nabla_{\bm{z}_{t}}\log p(\bm{y}|\bm{x}_{0}= \mathcal{D}\left(\mathbb{E}[\bm{z}_{0}|\bm{z}_{t}]\right))}_{\text{DPS vanilla extension}}\] \[+\gamma_{t}\underbrace{\nabla_{z_{t}}\left|\left|\mathbb{E}[\bm{z }_{0}|\bm{z}_{t}]-\mathcal{E}(\mathcal{A}^{T}\bm{y}+(\bm{I}-\mathcal{A}^{T} \mathcal{A})\mathcal{D}(\mathbb{E}[\bm{z}_{0}|\bm{z}_{t}]))\right|\right|^{2}}_ {\text{``gluing'' of $\bm{z}_{0}$}}.\] (7)

The gluing objective is critical for our algorithm as it ensures that the denoising update, measurement-matching update, and the gluing update point to the same optima in the latent space. We refer to this approximation (7) as Posterior Sampling with Latent Diffusion (PSLD). In the next Section 3, we provide an analysis of these gradient updates, along with the associated algorithms.

**Remark 2.1**.: Consider the optimization problem of projecting onto the measurements:

\[\min_{\bm{x}_{0}} \|\hat{\bm{x}}_{0}-\bm{x}_{0}\|_{2}^{2}\] subject to \[\mathcal{A}\bm{x}_{0}=\bm{y},\]

In the linear setting, the optimal solution is given by \(x_{0}^{*}=\mathcal{A}^{T}(\mathcal{A}\mathcal{A}^{T})^{-1}\bm{y}+(\hat{\bm{x}} _{0}-\mathcal{A}^{T}(\mathcal{A}\mathcal{A}^{T})^{-1}(\mathcal{A}\hat{\bm{x}} _{0}))\). Now further suppose that the measurement rows are orthogonal, i.e. \(\mathcal{A}\mathcal{A}^{T}=\bm{I}_{l}\). This condition holds for some natural linear inverse problems like inpainting. Suppose that we want to update the latent vector \(\bm{z}_{t}\) such that \(\mathbb{E}[\bm{z}_{0}|\bm{z}_{t}]=\mathcal{E}(x_{0}^{*})\); this ensures that the gradientsresulting from the two terms in (7) both point to the same optima in the latent space. Equivalently, we want to solve the following minimization problem: \(\min_{\bm{z}_{t}}\left\|\mathbb{E}[\bm{z}_{0}|\bm{z}_{t}]-\mathcal{E}(x_{0}^{*}) \right\|_{2}^{2}\). Substituting \(\mathcal{E}(x_{0}^{*})=\mathcal{E}(\mathcal{A}^{T}\bm{y}+(\bm{\hat{x}}_{0}- \mathcal{A}^{T}\mathcal{A}\hat{\bm{x}}_{0}))=\mathcal{E}(\mathcal{A}^{T}\bm{y }+(\bm{I}-\mathcal{A}^{T}\mathcal{A})\hat{\bm{x}}_{0})\), and \(\hat{\bm{x}}_{0}=\mathcal{D}(\mathbb{E}[\bm{z}_{0}|\bm{z}_{t}])\), we can thus interpret the gluing objective in (7) as a one step of gradient descent of this loss \(\left\|\mathbb{E}[\bm{z}_{0}|\bm{z}_{t}]-\mathcal{E}(x_{0}^{*})\right\|_{2}^{2}\) with respect to \(z_{t}\). Note that, if there was no latent space, our gluing would be equivalent to a projection on the measurements, but now because of the encoder and decoder, it is not.

## 3 Theoretical Results

As discussed in Section 2, diffusion models consist of two stochastic processes: the forward and reverse processes, each governed by Ito SDEs. For implementation purposes, these SDEs are discretized over a finite number of (time) steps, and the diffusion takes place using a transition kernel. The forward process starts from \(\overrightarrow{\bm{x}}_{0}^{\prime}\sim p(\overrightarrow{\bm{x}}_{0}^{\prime})\) and gradually adds noise, i.e., \(\overrightarrow{\bm{x}}_{t+1}^{\prime}=\sqrt{1-\beta_{t}}\overrightarrow{\bm{x }}_{t}+\sqrt{\beta_{t}}\bm{e}\) where \(\beta_{t}\in[0,1]\) and \(\beta_{t}\geq\beta_{t-1}\) for \(t=0,\dots,T-1\). The reverse process is initialized with \(\overrightarrow{\bm{x}}_{T}\sim\mathcal{N}\left(\bm{0},\bm{I}_{d}\right)\) and generates \(\overrightarrow{\bm{x}}_{t-1}=\mu_{\theta}(\overrightarrow{\bm{x}}_{t},t)+ \sqrt{\beta_{t}}\bm{e}\). In the last step, \(\mu_{\theta}(\overleftarrow{\bm{x}}_{1},1)\) is displayed without the noise.

In this section, we consider the diffusion discretized to two steps (\((\overrightarrow{\bm{x}}_{0}^{\prime},\overrightarrow{\bm{x}}_{1}^{\prime})\)), and a Gaussian transition kernel that arises from the Ornstein-Uhlenbeck (OU) process. We choose this setup because it captures essential components of complex diffusion processes without raising unnecessary complications in the analysis. We provide a principled analysis of **Algorithm 1** and **Algorithm 2** in a linear model setting with this two-step diffusion process under assumptions that guarantee exact reconstruction is possible in principle. A main result of our work is to prove that in this setting we can solve inverse problems perfectly. As we show, this requires some novel algorithmic ideas that are suggested by our theory. In Section 4, we then show that these algorithmic ideas are much more general, and apply to large-scale real-world applications of diffusion models that use multiple steps (\((\overrightarrow{\bm{x}}_{0}^{\prime},\overrightarrow{\bm{x}}_{1}^{\prime}, \cdots,\overrightarrow{\bm{x}}_{T}^{\prime})\), where \(T=1000\)), and moreover do not satisfy the recoverability assumptions. We provide post-processing details of **Algorithm 2** in Appendix C.1. All proofs are given in Appendix B.

### Problem Setup

The goal is to show that posterior sampling algorithms (such as DPS) can provably solve inverse problems in a perfectly recoverable setting. To show exact recovery, we analyze two-step diffusion processes in a linear model setting similar to [42, 7], where the images (\(\overrightarrow{\bm{x}}_{0}^{\prime}\in\mathbb{R}^{d}\)) reside in a linear subspace of the form \(\overrightarrow{\bm{x}}_{0}^{\prime}=\mathcal{S}\overrightarrow{\bm{w}}_{0}^{ \prime},\mathcal{S}\in\mathbb{R}^{d\times l},\overrightarrow{\bm{w}}_{0}^{ \prime}\in\mathbb{R}^{l}\), and \(\sigma_{y}=0\). Here, \(\mathcal{S}\) is a tall thin matrix with \(rank(\mathcal{S})=l\leq d\) that lifts any latent vector \(\overrightarrow{\bm{w}}_{0}^{\prime}\sim\mathcal{N}\left(\bm{0},\bm{I}_{l}\right)\) to the image space with ambient dimension \(d\). Given the measurements \(\bm{y}=\mathcal{A}\overrightarrow{\bm{x}}_{0}^{\prime}+\sigma_{y}\bm{n}\), \(\mathcal{A}\in\mathbb{R}^{l\times d},\bm{n}\in\mathbb{R}^{l}\), the goal is to sample from \(p_{0}(\overrightarrow{\bm{x}}_{0}^{\prime}|\bm{y})\) using a pre-trained latent diffusion model. In the inpainting task, the measurement operator \(\mathcal{A}\) is such that \(\mathcal{A}^{T}\mathcal{A}\) is a diagonal matrix \(\bm{D}(\bm{m})\), where \(\bm{m}\) is the masking vector with elements set to 1 where data is observed and 0 where data is masked (see Appendix B for further details). Recall that in latent diffusion models, the diffusion takes place in the latent space of a pre-trained Variational Autoencoder (VAE). Following the common practice [41], we consider a setting where the latent vector of the VAE is \(k\)-dimensional and the latent distribution is a standard Gaussian \(\mathcal{N}\left(\bm{0},\bm{I}_{k}\right)\). Our analysis shows that the proposed **Algorithm 2** provably solves inverse problems under the following assumptions.

**Assumption 3.1**.: The columns of the data generating model \(\mathcal{S}\) are orthonormal, i.e., \(\mathcal{S}^{T}\mathcal{S}=\bm{I}_{l}\).

**Assumption 3.2**.: The measurement operator \(\mathcal{A}\) satisfies \((\mathcal{A}\mathcal{S})^{T}(\mathcal{A}\mathcal{S})\succ\bm{0}\).

These assumptions have previously appeared, e.g., [42]. While **Assumption 3.1** is mild and can be relaxed at the expense of (standard) mathematical complications, **Assumption 3.2** indicates that \((\mathcal{A}\mathcal{S})^{T}(\mathcal{A}\mathcal{S})\) is a positive definite matrix. The latter ensures that there is enough energy left in the measurements for perfect reconstruction. More precisely, any subset of \(l\) coordinates exactly determines the remaining \((d-l)\) coordinates of \(\overrightarrow{\bm{x}_{0}}\). The underlying assumption is that there _exists_ a solution and it is _unique_[42]. Thus, the theoretical question becomes how close the recovered sample is to this groundtruth sample from the true posterior. Alternatively, one may consider other types of posteriors and prove that the generated samples are close to this posterior in distribution. However, this does not guarantee that the exact groundtruth sample is recovered. Therefore, motivated by prior works [42, 7], we analyze posterior sampling in a two-step diffusion model and answer a fundamental question: _Can a pre-trained latent diffusion model provably solve inverse problems in a perfectly recoverable setting?_

### Posterior Sampling using Latent Diffusion Model

In this section, we analyze two approximations: GML-DPS based on (6), and PSLD based on (7), displayed in **Algorithm 2**. We consider the case where the latent distribution of the VAE is in the same space as the latent distribution of the data generating model, i.e., \(k=l\), and normalize \(\gamma_{i}=1\) (as this is immaterial in the linear setting). In **Proposition 3.3**, we provide analytical solutions for the encoder and the decoder of the VAE.

**Proposition 3.3** (Variational Autoencoder).: _Suppose **Assumption 3.1** holds. For an encoder \(\mathcal{E}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{k}\) and a decoder \(\mathcal{D}:\mathbb{R}^{k}\rightarrow\mathbb{R}^{d}\), denote by \(\mathcal{L}\left(\phi,\omega\right)\) the training objective of VAE:_

\[\arg\min_{\phi,\omega}\mathcal{L}\left(\phi,\omega\right)\coloneqq\mathbb{E}_ {\overrightarrow{\bm{x}_{0}}\sim p}\left[\left\|\mathcal{D}(\mathcal{E}( \overrightarrow{\bm{x}_{0}};\phi);\omega)-\overrightarrow{\bm{x}_{0}}\right\| _{2}^{2}\right]+\lambda KL\left(\mathcal{E}\sharp p,\mathcal{N}(\bm{0},\bm{I }_{k})\right),\]

_then the combination of \(\mathcal{E}(\overrightarrow{\bm{x}_{0}};\phi)=\mathcal{S}^{T}\overrightarrow{ \bm{x}_{0}}\) and \(\mathcal{D}(\overleftarrow{\bm{x}_{0}};\omega)=\mathcal{S}\overleftarrow{ \bm{x}_{0}}\) is a minimizer of \(\mathcal{L}\left(\phi,\omega\right)\)._

Using the encoder \(\mathcal{E}(\overrightarrow{\bm{x}_{0}};\phi)=\mathcal{S}^{T}\overrightarrow{ \bm{x}_{0}}\), we can use the analytical solution \(\bm{\theta}^{*}\) of the LDM obtained in **Theorem A.1**. To verify that \(\bm{\theta}^{*}\) recovers the true subspace \(p\left(\overrightarrow{\bm{x}_{0}}\right)\), we compose the decoder \(\mathcal{D}(\overleftarrow{\bm{x}_{0}};\omega)=\mathcal{S}\overleftarrow{ \bm{x}_{0}}\) with the generator of the LDM, i.e., \(\overleftarrow{\bm{x}_{0}}=\mathcal{D}\left(\bm{\theta}^{*}\overleftarrow{ \bm{x}_{1}}\right)=\mathcal{D}\left(\bm{I}_{k}\overleftarrow{\bm{x}_{1}} \right)=\mathcal{S}\overleftarrow{\bm{x}_{1}}\). Since \(\overleftarrow{\bm{x}_{1}}\sim\mathcal{N}\left(\bm{0},\bm{I}_{k}\right)\) and \(\mathcal{S}\) is the data generating model, this shows that \(\overleftarrow{\bm{x}_{0}}\) is a sample from \(p(\overrightarrow{\bm{x}_{0}})\). Thus we have the following.

**Theorem 3.4** (Generative Modeling using Diffusion in Latent Space).: _Suppose **Assumption 3.1** holds. Let the optimal solution of the latent diffusion model be_

\[\bm{\theta}^{*}=\arg\min_{\bm{\theta}}\mathbb{E}_{\overrightarrow{\bm{x}_{0} },\overrightarrow{\bm{\epsilon}}}\left[\left\|\hat{\mu}_{1}\left(\overrightarrow {\bm{x}_{1}}(\overrightarrow{\bm{x}_{0}},\overrightarrow{\bm{\epsilon}}), \overrightarrow{\bm{x}_{0}}\right)-\mu_{\theta}\left(\overrightarrow{\bm{x}_{1 }}\left(\overrightarrow{\bm{x}_{0}},\overrightarrow{\bm{\epsilon}}\right) \right)\right\|^{2}\right].\]

_For a fixed variance \(\beta>0\), if \(\mu_{\bm{\theta}}\left(\overrightarrow{\bm{x}_{1}}\left(\overrightarrow{\bm{x }_{0}},\overrightarrow{\bm{\epsilon}}\right)\right)\coloneqq\bm{\theta} \overrightarrow{\bm{x}_{1}}\left(\overrightarrow{\bm{x}_{0}},\overrightarrow{ \bm{\epsilon}}\right)\), then the closed-form solution is \(\bm{\theta}^{*}=\sqrt{1-\beta}\bm{I}_{k}\), which after normalization by \(\frac{1}{\sqrt{1-\beta}}\) and composition with the decoder \(\mathcal{D}\left(\overleftarrow{\bm{x}_{0}};\omega\right)=\mathcal{S} \overleftarrow{\bm{x}_{0}}\) recovers the true subspace of \(p\left(\overrightarrow{\bm{x}_{0}}\right)\)._

With this optimal \(\bm{\theta}^{*}\), we can now prove exact sample recovery using GML-DPS (6).

**Theorem 3.5** (Posterior Sampling using Goodness Modified Latent DPS).: _Let \(\bm{Assumptions 3.1}\) and 3.2 hold. Let \(\sigma_{j},\forall j=1,\ldots,r\), denote the singular values of \((\mathcal{A}\mathcal{S})^{T}(\mathcal{A}\mathcal{S})\), and let_

\[\bm{\theta}^{*}=\arg\min_{\bm{\theta}}\mathbb{E}_{\overrightarrow{\bm{x}_{0}}, \overrightarrow{\bm{\epsilon}}}\left[\left\|\hat{\mu}_{1}\left(\overrightarrow{ \bm{x}_{1}}(\overrightarrow{\bm{x}_{0}},\overrightarrow{\bm{\epsilon}}), \overrightarrow{\bm{x}_{0}}\right)-\mu_{\theta}\left(\overrightarrow{\bm{x}_{1 }}\left(\overrightarrow{\bm{x}_{0}},\overrightarrow{\bm{\epsilon}}\right)\right) \right\|^{2}\right].\]

_Given a partially known image \(\overrightarrow{\bm{x}_{0}}\sim p(\overrightarrow{\bm{x}_{0}})\), any fixed variance \(\beta\in(0,1)\), then with the (unique) step size \(\eta_{i}^{j}=1/2\sigma_{j},j=1,2,\ldots,r\), the GML-DPS Algorithm (6) samples from the true posterior \(p(\overrightarrow{\bm{x}_{0}}|y)\) and exactly recovers the groundtruth sample, i.e., \(\overleftarrow{\bm{x}_{0}}=\overrightarrow{\bm{x}_{0}}\)._

**Theorem 3.5** shows that GML-DPS (6) recovers the true sample using an LDM. This approach, however, requires the step size \(\eta\) to be chosen _coordinate-wise_ in a specific manner. Also, multiple natural images could have the same measurements in the pixel space. This is a reasonable concern forLDMs due to one-to-many mappings of the decoder. Note that the _goodness objective_ (Section 2.1) cannot help in this scenario because it assigns uniform probability to many of these latents \(\overleftarrow{\bm{x}_{1}}\) for which \(\nabla_{\overleftarrow{\bm{x}_{1}}}\big{|}\big{|}\big{|}\widehat{\bm{x}}_{0}( \overleftarrow{\bm{x}_{1}})\big{|}-\mathcal{E}(\mathcal{D}(\overleftarrow{\bm {x}_{0}}(\overleftarrow{\bm{x}_{1}})))\big{|}\big{|}^{2}=0\). These challenges motivate the _gluing objective_ in **Theorem 3.6**. This is crucial for two reasons. First, we show that it helps recover the true sample even when the step size \(\eta\) is chosen arbitrarily. Second, it assigns all the probability mass to the desired (unique) solution in the pixel space.

**Theorem 3.6** (Posterior Sampling using Diffusion in Latent Space).: _Let **Assumptions 3.1 and 3.2 hold. Let \(\sigma_{j},\forall j=1,\ldots,r\) denote the singular values of \((\mathcal{AS})^{T}(\mathcal{AS})\) and let_

\[\bm{\theta}^{*}=\arg\min_{\bm{\theta}}\mathbb{E}_{\overleftarrow{\bm{x}_{0}}, \overleftarrow{\bm{x}}}\left[\left\|\tilde{\mu}_{1}\left(\overleftarrow{\bm {x}_{1}}(\overleftarrow{\bm{x}_{0}},\overleftarrow{\bm{e}}),\overrightarrow{ \bm{x}_{0}}\right)-\mu_{\theta}\left(\overrightarrow{\bm{x}_{1}}\left( \overleftarrow{\bm{x}_{0}},\overleftarrow{\bm{e}}\right)\right)\right\|^{2} \right].\]

_Given a partially known image \(\overrightarrow{\bm{x}_{0}}\sim p(\overrightarrow{\bm{x}_{0}})\), any fixed variance \(\beta\in(0,1)\), and any positive step sizes \(\eta_{i}^{j},j=1,2,\ldots,r\), the PSLD Algorithm 2 samples from the true posterior \(p(\overrightarrow{\bm{x}_{0}}|y)\) and exactly recovers the groundtruth sample, i.e., \(\bm{\bar{x}}_{0}=\overrightarrow{\bm{x}_{0}}\)._

The important distinction between **Theorem 3.5** and **Theorem 3.6** is that the former requires the _exact_ step size while the latter works for any finite step size. Combining denoising, measurement-consistency (with a scalar \(\eta\)), and gluing updates, we have

\[\overleftarrow{\bm{x}_{0}}=\bm{\theta}^{*}\overleftarrow{\bm{x}_{1}}-\eta \nabla_{\overleftarrow{\bm{x}_{1}}}\left\|\mathcal{A}\mathcal{D}(\overleftarrow {\bm{x}_{0}}(\overleftarrow{\bm{x}_{1}}))-\bm{y}\right\|_{2}^{2}-\nabla_{ \overleftarrow{\bm{x}_{1}}}\left\|\overleftarrow{\bm{x}_{0}}(\overleftarrow{ \bm{x}_{1}})-\mathcal{E}(\mathcal{A}^{T}\mathcal{A}\overrightarrow{\bm{x}_{0}}+( \bm{I}_{d}-\mathcal{A}^{T}\mathcal{A})\mathcal{D}(\overleftarrow{\bm{x}_{0}}( \overleftarrow{\bm{x}_{1}})))\right\|_{2}^{2}.\]

When \(\eta\) is chosen arbitrarily, then the third term guides the reverse SDE towards the optimal solution \(\overrightarrow{\bm{x}_{0}}\). When the reverse SDE generates the exact same groundtruth sample, i.e., \(\mathcal{D}(\overleftarrow{\bm{x}_{1}}(\overleftarrow{\bm{x}_{0}}))= \overrightarrow{\bm{x}_{0}}\), then the third term becomes zero. For all other samples, it penalizes the reverse SDE. Thus, it forces the reverse SDE to recover the true underlying sample irrespective of the value of \(\eta\).

We draw the following key insights from our **Theorem 3.6**: **Curse of ambient dimension:** In order to run posterior sampling using diffusion in the pixel space, the gradient of the measurement error needs to be computed in the \(d\)-dimensional ambient space. Therefore, DPS algorithm suffers from the curse of ambient dimension. On the other hand, our algorithm uses diffusion in the latent space, and therefore avoids the curse of ambient dimension. **Large-scale foundation model:** We propose a posterior sampling algorithm which offers the provision to use large-scale foundation models, and it provably solves general linear inverse problems. **Robustness to measurement step:** The gluing objective makes our algorithm robust to the choice of step size \(\eta\). Furthermore, it allows the same (scalar) step size across all the coordinates of \(\overrightarrow{\bm{x}_{0}}\).

## 4 Experimental Evaluation

We experiment with in-distribution and out-of-distribution datasets. For in-distribution, we conduct our experiments on a subset of the FFHQ dataset [25] (downscaled to \(256\times 256^{3}\), denoted by FFHQ 256). For out-of-distribution, we use images from the web and ImageNet dataset [17] (resized to \(256\times 256\), denoted by ImageNet 256). To make a fair comparison, we use the same validation subset and follow the same masking strategy as the baseline DPS [11]. It is important to note that our main contribution is an algorithm that can leverage any latent diffusion model. We test our algorithm with two pre-trained latent diffusion models: (i) the Stable Diffusion model that is trained on multiple subsets of the LAION dataset [44; 45]; and (ii) the Latent Diffusion model (LDM-VQ-4) trained on the FFHQ \(256\) dataset [41]. The DPS model is similarly trained from scratch for 1M steps using 49k FFHQ \(256\) images, which excludes the first 1K images used as validation set.

**Inverse Problems.** We experiment with the following task-specific measurement operators from the baseline DPS [11]: (i) Box inpainting uses a mask of size 128x128 at the center. (ii) Random inpainting chooses a drop probability uniformly at random between \((0.2,0.8)\) and applies this drop

\begin{table}
\begin{tabular}{l l l} \hline \hline Method & PSLD (Ours) & DPS [11] \\ \hline \(2\times\) & **0.185** & 0.220 \\ \(3\times\) & **0.220** & 0.247 \\ \(4\times\) & **0.233** & 0.291 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quantitative super-resolution (using measurement operator from [32]) results on FFHQ \(256\) validation samples [25; 11]. We use PSLD with Stable Diffusion. Table shows LPIPS (\(\downarrow\)).

probability to all the pixels. (iii) Super-resolution downsamples images at \(4\times\) scale. (iv) Gaussian blur convolves images with a Gaussian blur kernel. (v) Motion blur convolves images with a motion blur kernel. We also experiment with these additional operators from RePaint [32]: (vi) Super-resolution downsamples images at \(2\times\), \(3\times\), and \(4\times\) scale. (vii) Denoising has Gaussian noise with \(\sigma=0.05\). (viii) Destriping has vertical and horizontal stripes in the input images.

**Evaluation.** We compare the performance of our PSLD algorithm with the state-of-the-art DPS algorithm [11] on random inpainting, box inpainting, denoising, Gaussian deblur, motion deblur, arbitrary masking, and super-resolution tasks. We show that PSLD outperforms DPS, both in-distribution and out-of-distribution datasets, using the Stable Diffusion v-1.5 model pre-trained on the LAION dataset. We also test PSLD with LDM-VQ-4 trained on FFHQ \(256\), to compare with DPS trained on the same data distribution. Note that the LDM-v4 is a latent-based model released prior to Stable Diffusion. Therefore, it does not match the performance of Stable Diffusion in solving inverse problems. However, it shows the general applicability of our framework to leverage an LDM in posterior sampling. Since Stable Diffusion v-1.5 is trained with an image resolution of \(512\times 512\), we apply the forward operator after upsampling inputs to \(512\times 512\), run posterior sampling at \(512\times 512\), and then downsample images to the original \(256\times 256\) resolution for a fair comparison with DPS. We observed a similar performance while applying the masking operator at \(256\times 256\) and upscaling to \(512\times 512\) before running PSLD. More implementation details are provided in Appendix C.1.

**Metrics.** We use the commonly used Learned Perceptual Image Patch Similarity (LPIPS), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Metric (SSIM), and Frechet Inception Distance4 (FID) metrics for quantitative evaluation.

Footnote 4: https://github.com/mseitzer/pytorch-fid

**Results.** Figure 2 shows the inpainting results on out-of-distribution samples. This experiment was performed on commercial platforms that use (to the best of our knowledge) Stable diffusion and additional proprietary models. This evaluation was performed on models deployed in May 2023 and may change as commercial providers improve their platforms.

The qualitative advantage of PSLD is clearly demonstrated in Figures 2, 3, 4, 15 and 16. In Figure 5, we compare PSLD and DPS in random inpainting task for varying percentage of dropped pixels. Quantitatively, PSLD outperforms DPS in commonly used metrics: LPIPS, PSNR, and SSIM.

In our PSLD algorithm, we use Stable Diffusion v1.5 model and (zero-shot) test it on inverse problems. Table 6 compares the quantitative results of PSLD with related works on random inpainting, box inpainting, super-resolution, and Gaussian deblur tasks. PSLD significantly outperforms previous approaches on the relatively easier random inpainting task, and it is better or comparable on harder tasks. Table 4 draws a comparison between PSLD and the strongest baseline (among the compared methods) on out-of-distribution images. Table 1 shows the super-resolution results using nearest-neighbor kernels from [32] on FFHQ 256 validation dataset. Observe that PSLD outperforms state-of-the-art methods across diverse tasks and standard evaluation metrics.

In Table 3, we compare PSLD (using LDM-VQ-4) and DPS on random and box inpainting tasks with the same operating resolution (\(256\times 256\)) and training distributions (FFHQ 256). Although the LDM model exceeds DPS performance in box inpainting, it is comparable in random inpainting. As expected, using a more powerful pre-trained model such as Stable Diffusion is beneficial in

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{Inpaint (random)} & \multicolumn{2}{c}{Inpaint (box)} & \multicolumn{2}{c}{SR (\(4\times\))} & \multicolumn{2}{c}{Gaussian Deblur} \\ \cline{2-9} Method & FID (\(\downarrow\)) & LPIPS (\(\downarrow\)) & FID (\(\downarrow\)) & LPIPS (\(\downarrow\)) & FID (\(\downarrow\)) & LPIPS (\(\downarrow\)) & FID (\(\downarrow\)) & LPIPS (\(\downarrow\)) \\ \hline PSLD (Ours) & **21.34** & **0.096** & 43.11 & **0.167** & **34.28** & **0.201** & **41.53** & **0.221** \\ \hline DPS [11] & 33.48 & 0.212 & **35.14** & 0.216 & 39.35 & 0.214 & 44.05 & 0.257 \\ DDRM [26] & 69.71 & 0.587 & 42.93 & 0.204 & 62.15 & 0.294 & 74.92 & 0.332 \\ MCG [13] & 29.26 & 0.286 & 40.11 & 0.309 & 87.64 & 0.520 & 101.2 & 0.340 \\ PnP-ADMM [6] & 123.6 & 0.692 & 151.9 & 0.406 & 66.52 & 0.353 & 90.42 & 0.441 \\ Score-SDE [50] & 76.54 & 0.612 & 60.06 & 0.331 & 96.72 & 0.563 & 109.0 & 0.403 \\ ADMM-TV & 181.5 & 0.463 & 68.94 & 0.322 & 110.6 & 0.428 & 186.7 & 0.507 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Quantitative inpainting results on FFHQ \(256\) validation set [25, 11]. We use Stable Diffusion v-1.5 and the measurement operators as in DPS [11]. As shown, our PSLD model outperforms DPS since it is able to leverage the power of the Stable Diffusion foundation model.

reconstruction-see Table 6. This highlights the significance of our PSLD algorithm that has the provision to incorporate a powerful foundation model with no extra training costs for solving inverse problems. Importantly, PSLD uses latent-based diffusion, and thus it avoids the curse of ambient dimension (**Theorem 3.6**), while still achieving comparable results to the state-of-the-art method DPS [11] that has been trained on the same dataset. Additional experimental evaluation is provided in Appendix C.

## 5 Conclusion

In this paper, we leverage latent diffusion models to solve general linear inverse problems. While previously proposed approaches only apply to pixel-space diffusion models, our algorithm allows us to use the image prior learned by latent-based foundation generative models. We provide a principled analysis of our algorithm in a linear two-step diffusion setting, and use insights from this analysis to design a modified objective (goodness and gluing). This leads to our algorithm - Posterior

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{Random inpaint + denoise \(\sigma=0.00\)} & \multicolumn{3}{c}{Random inpaint + denoise \(\sigma=0.05\)} \\ \cline{2-7} Method & PSNR (\(\uparrow\)) & SSIM (\(\uparrow\)) & LPIPS (\(\downarrow\)) & PSNR (\(\uparrow\)) & SSIM (\(\uparrow\)) & LPIPS (\(\downarrow\)) \\ \hline PSLD (Ours) & **34.02** & **0.951** & **0.083** & **33.71** & **0.943** & **0.096** \\ DPS [11] & 31.41 & 0.884 & 0.171 & 29.49 & 0.844 & 0.212 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Quantitative results of random inpainting and denoising on FFHQ \(256\)[25; 11] using Stable Diffusion v-1.5. Note that DPS is trained on FFHQ \(256\). The results show that our method PSLD generalizes well to out-of-distribution samples even without finetuning.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{Inpaint (random)} & \multicolumn{3}{c}{Inpaint (box)} \\ \cline{2-7} Method & PSNR (\(\uparrow\)) & SSIM (\(\uparrow\)) & LPIPS (\(\downarrow\)) & PSNR (\(\uparrow\)) & SSIM (\(\uparrow\)) & LPIPS (\(\downarrow\)) \\ \hline PSLD (Ours) & **30.31** & **0.851** & 0.221 & **24.22** & **0.819** & **0.158** \\ DPS [11] & 29.49 & 0.844 & **0.212** & 23.39 & 0.798 & 0.214 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative inpainting results on FFHQ \(256\) validation set [25; 11]. We use the _latent diffusion_ (LDM-VQ-4) trained on FFHQ \(256\). Note that in this experiment PSLD and DPS use diffusion models trained on the same dataset. As shown, PSLD with LDM-VQ-4 as diffusion model outperforms DPS in box inpainting and has comparable performance in random inpainting.

Figure 2: Inpainting results in general domain images from the web (see Appendix C for image sources). Our model compared to state-of-art commercial inpainting services that leverage the same foundation model (Stable Diffusion v-1.5).

Sampling with Latent Diffusion (PSLD) - that experimentally outperforms state-of-art baselines on a wide variety of tasks including random inpainting, block inpainting, denoising, destriping, and super-resolution.

**Limitations.** Our evaluation is based on Stable Diffusion which was trained on the LAION dataset. Biases in this dataset and foundation model will be implicitly affecting our algorithm. Our method can work with any LDM and we expect new foundation models trained on better datasets like [19] to mitigate these issues. Second, we have not explored how to use latent-based foundation models to solve non-linear inverse problems. Our method builds on the DPS approximation (which performs well on non-linear inverse problems), and hence we believe our method can also be similarly extended.

Figure 4: Inpainting (random and box) results on out-of-distribution samples, \(256\times 256\) (see Appendix C for image sources). We use PSLD with Stable Diffusion v-1.5 as generative foundation model.

Figure 5: Comparing DPS and PSLD performance in random inpainting on FFHQ 256 [25, 11], as the percentage of masked pixels increases. PSLD with Stable Diffusion outperforms DPS.

Figure 3: **Left panel:** Random Inpainting on images from FFHQ 256 [25] using PSLD with Stable Diffusion v-1.5. Notice the text in the top row and the facial expression in the bottom row. **Right panel:** Block (\(128\times 128\)) inpainting, using the LDM-VQ-4 model trained on FFHQ \(256\)[25]. Notice the glasses in the top row and eyes in the bottom row.

## Acknowledgements

This research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace team for providing us GPU support for the demo of our work.

## References

* (1) Brian D.O. Anderson. "Reverse-time diffusion equation models". In: _Stochastic Processes and their Applications_ 12.3 (1982), pp. 313-326 (page 1).
* (2) Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. "Single-Shot Adaptation using Score-Based Models for MRI Reconstruction". In: _International Society for Magnetic Resonance in Medicine, Annual Meeting_. 2022 (page 2).
* (3) Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. "Cold Diffusion: Inverting arbitrary image transforms without noise". In: _arXiv preprint arXiv:2208.09392_ (2022) (page 2).
* (4) Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. "Align your latents: High-resolution video synthesis with latent diffusion models". In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2023, pp. 22563-22575 (page 3).
* (5) Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. "Compressed sensing using generative models". In: _International Conference on Machine Learning_. PMLR. 2017, pp. 537-546 (page 1).
* (6) Stanley H Chan, Xiran Wang, and Omar A Elgendy. "Plug-and-play ADMM for image restoration: Fixed-point convergence and applications". In: _IEEE Transactions on Computational Imaging_ 3.1 (2016), pp. 84-98 (pages 2, 8, 24, 27).
* (7) Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. "Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data". In: _arXiv preprint arXiv:2302.07194_ (2023) (pages 5, 6).
* (8) Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. "Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions". In: _arXiv preprint arXiv:2209.11215_ (2022) (page 1).
* (9) Sitan Chen, Giannis Daras, and Alexandros G Dimakis. "Restoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers". In: _arXiv preprint arXiv:2303.03384_ (2023) (page 1).
* (10) Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. "Ilvr: Conditioning method for denoising diffusion probabilistic models". In: _arXiv preprint arXiv:2108.02938_ (2021) (page 2).
* (11) Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. "Diffusion Posterior Sampling for General Noisy Inverse Problems". In: _The Eleventh International Conference on Learning Representations_. 2023. url: https://openreview.net/forum?id=OnD9zGAGT0k (pages 1-3, 7-10, 15, 18, 20, 22, 24-31).
* (12) Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. "Direct Diffusion Bridge using Data Consistency for Inverse Problems". In: _arXiv preprint arXiv:2305.19809_ (2023) (page 2).
* (13) Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. "Improving Diffusion Models for Inverse Problems using Manifold Constraints". In: _Advances in Neural Information Processing Systems_. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. url: https://openreview.net/forum?id=nJJjv0JDjju (pages 2, 8, 24, 27, 31).
* (14) Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. "Score-guided intermediate layer optimization: Fast langevin mixing for inverse problem". In: _arXiv preprint arXiv:2206.09104_ (2022) (page 2).

* [15] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar. "Soft diffusion: Score matching for general corruptions". In: _arXiv preprint arXiv:2209.05442_ (2022) (page 2).
* [16] Mauricio Delbracio and Peyman Milanfar. "Inversion by direct iteration: An alternative to denoising diffusion for image restoration". In: _arXiv preprint arXiv:2303.11435_ (2023) (page 2).
* [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. "Imagenet: A large-scale hierarchical image database". In: _2009 IEEE conference on computer vision and pattern recognition_. Ieee. 2009, pp. 248-255 (pages 7, 21, 22, 27-29).
* [18] Prafulla Dhariwal and Alexander Nichol. "Diffusion models beat gans on image synthesis". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 8780-8794 (page 1).
* [19] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. "DataComp: In search of the next generation of multimodal datasets". In: _arXiv preprint arXiv:2304.14108_ (2023) (page 10).
* [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models". In: _Advances in Neural Information Processing Systems_ 33 (2020), pp. 6840-6851 (page 1).
* [21] Aapo Hyvarinen and Peter Dayan. "Estimation of non-normalized statistical models by score matching." In: _Journal of Machine Learning Research_ 6.4 (2005) (page 1).
* [22] Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. "Robust compressed sensing mri with deep generative priors". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 14938-14954 (pages 1, 2).
* [23] Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price. "Instance-optimal compressed sensing via posterior sampling". In: _arXiv preprint arXiv:2106.11438_ (2021) (page 1).
* [24] Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price. "Fairness for Image Generation with Uncertain Sensitive Attributes". In: _Proceedings of the 38th International Conference on Machine Learning_. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 18-24 Jul 2021, pp. 4721-4732. url: https://proceedings.mlr.press/v139/jalal21b.html (page 1).
* [25] Tero Karras, Samuli Laine, and Timo Aila. "A style-based generator architecture for generative adversarial networks". In: _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 2019, pp. 4401-4410 (pages 7-10, 24-28, 31).
* [26] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. "Denoising Diffusion Restoration Models". In: _Advances in Neural Information Processing Systems_ (pages 1, 2, 8, 24, 27, 31).
* [27] Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael Elad. "GSURE-Based Diffusion Model Training with Corrupted Data". In: _arXiv preprint arXiv:2305.13128_ (2023) (page 2).
* [28] Bahjat Kawar, Gregory Vaksman, and Michael Elad. "SNIPS: Solving noisy inverse problems stochastically". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 21757-21769 (page 31).
* [29] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. "Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation". In: _International Conference on Machine Learning_. PMLR. 2022, pp. 11201-11228 (page 1).
* [30] Haobe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. "Audioldm: Text-to-audio generation with latent diffusion models". In: _arXiv preprint arXiv:2301.12503_ (2023) (page 3).
* [31] Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang. "Coherent Semantic Attention for Image Inpainting". In: _2019 IEEE/CVF International Conference on Computer Vision (ICCV)_ (Oct. 2019). doi: 10.1109/iccv.2019.00427. URL: http://dx.doi.org/10.1109/ICCV.2019.00427 (page 1).
* [32] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. "Repaint: Inpainting using denoising diffusion probabilistic models". In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2022, pp. 11461-11471 (pages 7, 8, 30).

* [33] Gary Mataev, Peyman Milanfar, and Michael Elad. "DeepRED: Deep image prior powered by RED". In: _Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops_. 2019, pp. 0-0 (page 1).
* [34] Xiangming Meng and Yoshiyuki Kabashima. "Diffusion model based posterior sampling for noisy linear inverse problems". In: _arXiv preprint arXiv:2211.12343_ (2022) (pages 27, 31).
* [35] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. "Pulse: Self-supervised photo upsampling via latent space exploration of generative models". In: _Proceedings of the ieee/cvf conference on computer vision and pattern recognition_. 2020, pp. 2437-2445 (page 1).
* [36] Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca Willett. "Deep learning techniques for inverse problems in imaging". In: _IEEE Journal on Selected Areas in Information Theory_ 1.1 (2020), pp. 39-56 (page 1).
* [37] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. "Context encoders: Feature learning by inpainting". In: _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2016, pp. 2536-2544 (page 1).
* [38] Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso. "Brain imaging generation with latent diffusion models". In: _Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings_. Springer. 2022, pp. 117-126 (page 3).
* [39] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation". In: _arXiv preprint arXiv:2008.00951_ (2020) (page 1).
* [40] Yaniv Romano, Michael Elad, and Peyman Milanfar. "The little engine that could: Regularization by denoising (RED)". In: _SIAM Journal on Imaging Sciences_ 10.4 (2017), pp. 1804-1844 (pages 1, 31).
* [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. "High-resolution image synthesis with latent diffusion models". In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2022, pp. 10684-10695 (pages 2, 3, 5, 7).
* [42] Litu Rout, Advait Parulekar, Constantine Caramanis, and Sanjay Shakkottai. "A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models". In: _arXiv preprint arXiv:2302.01217_ (2023) (pages 5, 6, 15).
* [43] Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi. _Palette: Image-to-Image Diffusion Models_. 2022. arXiv: 2111.05826 [cs.CV] (page 31).
* [44] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. _LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs_. 2021. arXiv: 2111.02114 [cs.CV] (page 7).
* [45] Christoph Schuhmann et al. _LAION-5B: An open large-scale dataset for training next generation image-text models_. 2022. arXiv: 2210.08402 [cs.CV] (page 7).
* [46] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. "Pseudoinverse-guided diffusion models for inverse problems". In: _International Conference on Learning Representations_. 2023 (pages 2, 31).
* [47] Yang Song and Stefano Ermon. "Generative modeling by estimating gradients of the data distribution". In: _Advances in Neural Information Processing Systems_ 32 (2019) (page 1).
* [48] Yang Song and Stefano Ermon. "Improved techniques for training score-based generative models". In: _Advances in neural information processing systems_ 33 (2020), pp. 12438-12448 (page 1).
* [49] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. "Score-Based Generative Modeling through Stochastic Differential Equations". In: _International Conference on Learning Representations_. 2021 (page 1).
* [50] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. "Score-Based Generative Modeling through Stochastic Differential Equations". In: _International Conference on Learning Representations_ (pages 8, 24, 27).

* [51] Yu Takagi and Shinji Nishimoto. "High-resolution image reconstruction with latent diffusion models from human brain activity". In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2023, pp. 14453-14463 (page 3).
* [52] Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. "Plug-and-play priors for model based reconstruction". In: _2013 IEEE Global Conference on Signal and Information Processing_. IEEE. 2013, pp. 945-948 (page 1).
* [53] Pascal Vincent. "A connection between score matching and denoising autoencoders". In: _Neural computation_ 23.7 (2011), pp. 1661-1674 (page 1).
* [54] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. "Imagen editor and editbench: Advancing and evaluating text-guided image inpainting". In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2023, pp. 18359-18369 (pages 1, 21).
* [55] Yinhuai Wang, Jiwen Yu, and Jian Zhang. "Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model". In: _The Eleventh International Conference on Learning Representations_. 2023. url: https://openreview.net/forum?id=mRiedgMtNTQ (page 31).
* [56] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. "Free-Form Image Inpainting With Gated Convolution". In: _2019 IEEE/CVF International Conference on Computer Vision (ICCV)_ (Oct. 2019). DOI: 10.1109/iccv.2019.00457. URL: http://dx.doi.org/10.1109/ICCV.2019.00457 (page 1).

Additional Theoretical Results

**Notation and Measurement Matrix.** We elaborate on the structure of the measurement matrix \(\mathcal{A}\in\mathbb{R}^{l\times d}\). In our setting, we are considering linear inverse problems. Thus, this matrix is a pixel selector and consists of a subset of the rows from the \(d\times d\) identity matrix (the rows that are present correspond to the indices of the selected pixels from the image \(\overrightarrow{\bm{x}_{0}^{j}}\in\mathbb{R}^{d}\)). Given this structure, it immediately follows that \(\mathcal{A}^{T}\mathcal{A}\) is a \(d\times d\) matrix that has the interpretation of a pixel selection _mask_. Specifically, \(\mathcal{A}^{T}\mathcal{A}\) is a \(d\times d\) diagonal matrix \(\bm{D}(\bm{m})\), where the elements of \(\bm{m}\) are set to 1 where data (pixel) is observed and 0 where data (pixel) is masked. Without the loss of generality, we suppose that the first \(k\) coordinates are known.

### Posterior Sampling using Pixel-space Diffusion Model

We first consider the reverse process, starting with \(\overleftarrow{\bm{x}_{1}}\sim\mathcal{N}\left(\bm{0},\bm{I}_{d}\right)\), and borrow a result from [42] to show that the sample \(\overleftarrow{\bm{x}_{0}}\) generated by the reverse process is a valid image from \(p(\overrightarrow{\bm{x}_{0}^{j}})\).

**Theorem A.1** (Generative Modeling using Diffusion in Pixel Space, [42]).: _Suppose **Assumption 3.1** holds. Let_

\[\bm{\theta}^{*}=\arg\min_{\bm{\theta}}\mathbb{E}_{\overrightarrow{\bm{x}_{0} },\,\bm{\varepsilon}}\left[\left\|\tilde{\mu}_{1}\left(\overrightarrow{\bm{x }_{1}^{j}}(\overrightarrow{\bm{x}_{0}^{j}},\overrightarrow{\bm{\epsilon}}), \overrightarrow{\bm{x}_{0}^{j}}\right)-\mu_{\bm{\theta}}\left(\overrightarrow{ \bm{x}_{1}^{j}}(\overrightarrow{\bm{x}_{0}^{j}},\overrightarrow{\bm{\epsilon}} )\right)\right\|^{2}\right].\]

_For a fixed variance \(\beta>0\), if \(\mu_{\bm{\theta}}\left(\overrightarrow{\bm{x}_{1}}\left(\overrightarrow{\bm{x }_{0}^{j}},\overrightarrow{\bm{\epsilon}}\right)\right)\coloneqq\bm{\theta} \overrightarrow{\bm{x}_{1}}\left(\overrightarrow{\bm{x}_{0}^{j}}, \overrightarrow{\bm{\epsilon}}\right)\), then the closed-form solution \(\bm{\theta}^{*}\) is \(\sqrt{1-\beta}\bm{S}\bm{S}^{T}\), which after normalization by \(1/\sqrt{1-\beta}\) recovers the true subspace of \(p\left(\overrightarrow{\bm{x}_{0}^{j}}\right)\)._

Though this establishes that \(\overleftarrow{\bm{x}_{0}}\) generated by the reverse process is a valid image from \(p(\overrightarrow{\bm{x}_{0}^{j}})\), it is not necessarily a sample from the posterior \(p(\overrightarrow{\bm{x}_{0}^{j}}|\bm{y})\) that satisfies the measurements. To accomplish this we perform one additional step of gradient descent for every step of the reverse process. This gives us **Algorithm 1**, the DPS algorithm. The next theorem shows that the reverse SDE guided by these measurements (3) recovers the true underlying sample5.

Footnote 5: While the DPS Algorithm [11] uses a scalar step size \(\zeta_{i}\) at each step, this does not suffice for exact recovery. However, by generalizing to allow a different step size per coordinate, we can show sample recovery. Thus, in this section, we denote \(\zeta_{i}^{j}\) to be the step size at step \(i\) and coordinate \(j\), \(1\leq j\leq r\). Also note that the step index \(i\) is vacuous in this section, as we consider a two-step diffusion process (i.e., \(i\) is always 1).

**Theorem A.2** (Posterior Sampling using Diffusion in Pixel Space).: _Suppose **Assumption 3.1** and **Assumption 3.2** hold. Let us denote by \(\sigma_{j},\forall j=1,\ldots,r\), the singular values of \((\mathcal{AS})^{T}(\mathcal{AS})\) and_

\[\bm{\theta}^{*}=\arg\min_{\bm{\theta}}\mathbb{E}_{\overrightarrow{\bm{x}_{0} },\,\bm{\varepsilon}}\left[\left\|\tilde{\mu}_{1}\left(\overrightarrow{\bm{x }_{1}^{j}}(\overrightarrow{\bm{x}_{0}^{j}},\overrightarrow{\bm{\epsilon}}), \overrightarrow{\bm{x}_{0}^{j}}\right)-\mu_{\bm{\theta}}\left(\overrightarrow{ \bm{x}_{1}^{j}}(\overrightarrow{\bm{x}_{0}^{j}},\overrightarrow{\bm{\epsilon}} )\right)\right\|^{2}\right].\]

_Given a partially known image \(\overrightarrow{\bm{x}_{0}^{j}}\sim p(\overrightarrow{\bm{x}_{0}^{j}})\), a fixed variance \(\beta>0\), there exists a step size \(\zeta_{i}^{j}=1/2\sigma_{j}\) for all the coordinates of \(\overrightarrow{\bm{x}_{0}^{j}}\) such that **Algorithm 1** samples from the true posterior \(p(\overrightarrow{\bm{x}_{0}}|y)\) and exactly recovers the groundtruth sample, i.e., \(\overleftarrow{\bm{x}_{0}}=\overrightarrow{\bm{x}_{0}^{j}}\)._

## Appendix B Technical Proofs

This section contains proofs of all the theorems and propositions presented in the main body of the paper. For clarity, we restate the theorems more formally with precise mathematical details.

### Proof of Theorem a.2

**Theorem B.1** (Posterior Sampling using Diffusion in Pixel Space).: _Suppose **Assumption 3.1** and **Assumption 3.2** hold. Let us denote by \(\bm{\sigma}=\{\sigma_{j}\}_{j=1}^{k}\) the singular values of \((\mathcal{AS})^{T}(\mathcal{AS})\), i.e. \((\mathcal{AS})^{T}(\mathcal{AS})=\bm{U}\Sigma\bm{V}^{T}\coloneqq\bm{U}\bm{D}( \bm{\sigma})\bm{V}^{T},\bm{U}\in\mathbb{R}^{k\times k},\bm{V}\in\mathbb{R}^{k \times k}\) and_

\[\bm{\theta}^{*}=\arg\min_{\bm{\theta}}\mathbb{E}_{\overrightarrow{\bm{x}_{0}^{j} },\,\bm{\varepsilon}}\left[\left\|\tilde{\mu}_{1}\left(\overrightarrow{\bm{x} _{1}^{j}}(\overrightarrow{\bm{x}_{0}^{j}},\overrightarrow{\bm{\epsilon}}), \overrightarrow{\bm{x}_{0}^{j}}\right)-\mu_{\theta}\left(\overrightarrow{\bm{x}_ {1}^{j}}(\overrightarrow{\bm{x}_{0}^{j}},\overrightarrow{\bm{\epsilon}})\right) \right\|^{2}\right].\]_Suppose \(\overrightarrow{\bm{x}}_{0}^{\prime}\sim p(\overrightarrow{\bm{x}}_{0}^{\prime})\). Given measurements \(y=\mathcal{A}\overrightarrow{\bm{x}}_{0}^{\prime}\) and a fixed variance \(\beta\in(0,1)\), there exists a matrix step size6\(\bm{\zeta}=(1/2)(\mathcal{SU})\bm{D}(\bm{\zeta}_{i})(\mathcal{SU})^{T},\bm{\zeta}_{i}= \{\zeta_{i}^{j}=1/\sigma_{j}\}_{j=1}^{k}\) for all the coordinates of \(\overrightarrow{\bm{x}}_{0}^{\prime}\) such that **Algorithm 1** samples from the true posterior \(p(\overrightarrow{\bm{x}}_{0}|y)\) and exactly recovers the groundtruth sample, i.e., \(\overrightarrow{\bm{x}}_{0}=\overrightarrow{\bm{x}}_{0}^{\prime}\)._

Footnote 6: We use the term step size in a more general way than is normally used. In this case, the step size is a pre-conditioning positive definite matrix, whose eigenvalue magnitudes correspond to the scalar step sizes per coordinate along an appropriately rotated basis. This general form is needed and with carefully selected (unique) eigenvalues; otherwise the DPS algorithm fails to converge to the groundtruth sample. We will later see that for our PSLD Algorithm in Theorem 3.6, we can revert to the commonly used notion of step size (a single scalar), as any finite step size (including a single scalar common across all coordinates) suffices for proving recovery.

Proof.: Our goal is to show that \(\overleftarrow{\bm{x}}_{0}=\overrightarrow{\bm{x}}_{0}^{\prime}\), where \(\overleftarrow{\bm{x}}_{0}\) is returned by **Algorithm 1**. Recall that the reverse process starts with \(\overleftarrow{\bm{x}}_{1}\sim\mathcal{N}\left(\bm{0},\bm{I}_{d}\right)\) and generates the following:

\[\overleftarrow{\bm{x}}_{0} =\bm{\theta}^{*}\overleftarrow{\bm{x}}_{1}-\bm{\zeta}\nabla_{ \overleftarrow{\bm{x}}_{1}}\left\|\mathcal{A}\overleftarrow{\bm{x}}_{0}( \overleftarrow{\bm{x}}_{1})-\bm{y}\right\|_{2}^{2}\] \[=\bm{\theta}^{*}\overleftarrow{\bm{x}}_{1}-\bm{\zeta}\nabla_{ \overleftarrow{\bm{x}}_{1}}\left\|\mathcal{A}\mathcal{S}^{T}\overleftarrow{\bm {x}}_{1}-\bm{y}\right\|_{2}^{2}\] \[=\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-2\bm{\zeta }\left(\mathcal{A}\mathcal{S}\mathcal{S}^{T}\right)^{T}\left(\mathcal{A} \mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-\bm{y}\right)\] \[=\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-2\mathcal{S }\mathcal{S}\mathcal{S}^{T}\mathcal{A}^{T}\mathcal{A}\mathcal{S}\mathcal{S}^{T }\overleftarrow{\bm{x}}_{1}+2\mathcal{U}\mathcal{S}\mathcal{S}\mathcal{S}^{T} \mathcal{A}^{T}\bm{y}\] \[=\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-2\mathcal{ Q}\mathcal{S}\mathcal{S}^{T}\mathcal{A}^{T}\mathcal{A}\mathcal{S}\mathcal{S}^{T} \overleftarrow{\bm{x}}_{1}+2\mathcal{U}\mathcal{S}\mathcal{S}^{T}\mathcal{A}^{T }\mathcal{A}\overrightarrow{\bm{x}}_{0}^{\prime}\] \[=\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-2\mathcal{ Q}\mathcal{S}\mathcal{S}^{T}\mathcal{A}^{T}\mathcal{A}\mathcal{S}\mathcal{S}^{T} \overleftarrow{\bm{x}}_{1}+2\mathcal{U}\mathcal{S}\mathcal{S}^{T}\mathcal{A}^{T }\mathcal{A}\mathcal{S}\overrightarrow{\bm{x}}_{0}^{\prime}.\]

Now, we use the singular value decomposition of \((\mathcal{A}\mathcal{S})^{T}(\mathcal{A}\mathcal{S})\) with left singular vectors in \(\bm{U}\in\mathbb{R}^{k\times k}\), right singular vectors in \(\bm{V}\in\mathbb{R}^{k\times k}\), and singular values \(\bm{\sigma}=[\sigma_{1},\ldots,\sigma_{k}]\) in \(\Sigma=\bm{D}(\bm{\sigma})\). Thus, the above expression becomes

\[\overleftarrow{\bm{x}}_{0} =\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-2\mathcal{ Q}\mathcal{S}\bm{U}\Sigma\bm{V}^{T}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}+2 \mathcal{U}\mathcal{S}\bm{U}\Sigma\bm{V}^{T}\overrightarrow{\bm{x}}_{0}^{\prime}\] \[=\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-2\mathcal{ Q}\mathcal{S}\bm{U}\Sigma\bm{V}^{T}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}+2 \mathcal{U}\mathcal{S}\bm{U}\Sigma\bm{V}^{T}\mathcal{S}^{T}\overleftarrow{\bm{ x}}_{0}^{\prime}\] \[=\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-2(\mathcal{ SU})\bm{D}(\bm{\zeta}_{i})(\mathcal{S}\bm{U})^{T}\mathcal{S}\bm{U}\Sigma\bm{V}^{T} \mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}+2(\mathcal{S}\bm{U})\bm{D}(\bm{ \zeta}_{i})(\mathcal{S}\bm{U})^{T}\mathcal{S}\bm{U}\Sigma\bm{V}^{T}\overrightarrow {\bm{x}}_{0}^{\prime}\] \[\overset{(i)}{=}\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1} -2(\mathcal{S}\bm{U})\bm{D}(\bm{\zeta}_{i})\bm{U}^{T}\bm{U}\Sigma\bm{U}^{T} \mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}+2(\mathcal{S}\bm{U})\bm{D}(\bm{ \zeta}_{i})\bm{U}^{T}\bm{U}\Sigma\bm{U}^{T}\overrightarrow{\bm{x}}_{0}^{\prime}\] \[=\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-2(\mathcal{ SU})\bm{D}(\bm{\zeta}_{i})\Sigma\bm{U}^{T}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}+2( \mathcal{S}\bm{U})\bm{D}(\bm{\zeta}_{i})\Sigma\bm{U}^{T}\overrightarrow{\bm{x}} _{0}^{\prime}\] \[=\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-2\mathcal{ S}\bm{U}\bm{D}(\bm{\zeta}_{i})\bm{D}(\bm{\sigma})\bm{U}^{T}\mathcal{S}^{T} \overleftarrow{\bm{x}}_{1}+2\mathcal{S}\bm{U}\bm{D}(\bm{\zeta}_{i})\bm{D}( \bm{\sigma})\bm{U}^{T}\overrightarrow{\bm{x}}_{0}^{\prime}\] \[=\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-2\mathcal{ S}\bm{U}\bm{D}(\bm{\zeta}_{i})\bm{D}(\bm{\sigma})\bm{U}^{T}\mathcal{S}^{T} \overleftarrow{\bm{x}}_{1}+2\mathcal{S}\bm{U}\bm{D}(\bm{\zeta}_{i})\bm{D}(\bm{ \sigma})\bm{U}^{T}\overrightarrow{\bm{x}}_{0}^{\prime}\] \[=\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-2\mathcal{ S}\bm{U}\bm{D}(\bm{\zeta}_{i}\odot\bm{\sigma})\bm{U}^{T}\mathcal{S}^{T} \overleftarrow{\bm{x}}_{1}+2\mathcal{S}\bm{U}\bm{D}(\bm{\zeta}_{i}\odot\bm{ \sigma})\bm{U}^{T}\overrightarrow{\bm{x}}_{0}^{\prime},\]

where (i) is due to **Assumption 3.1** and (ii) uses **Assumption 3.2**. By choosing \(\zeta_{i}^{j}\) as half the inverse of the non-zero singular values of \((\mathcal{A}\mathcal{S})^{T}(\mathcal{A}\mathcal{S})\), i.e., \(\zeta_{i}^{j}=1/2\sigma_{i}\ \forall i=1,\ldots,k\), we obtain

\[\overleftarrow{\bm{x}}_{0} =\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-\mathcal{S} \bm{U}\bm{U}^{T}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}+\mathcal{S}\bm{U} \bm{U}^{T}\overrightarrow{\bm{x}}_{0}^{\prime}\] \[=\mathcal{S}\mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}-\mathcal{S} \mathcal{S}^{T}\overleftarrow{\bm{x}}_{1}+\mathcal{S}\overrightarrow{\bm{x}}_{0} ^{\prime}=\overrightarrow{\bm{x}}_{0}^{\prime},\]

which completes the statement of the theorem. 

### Proof of Proposition 3.3

**Proposition B.2** (Variational Autoencoder).: _Suppose **Assumption 3.1** holds. For an encoder \(\mathcal{E}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{k}\) and a decoder \(\mathcal{D}:\mathbb{R}^{k}\rightarrow\mathbb{R}^{d}\), denote by \(\mathcal{L}\left(\phi,\omega\right)\) the training objective of VAE:_

\[\arg\min_{\phi,\omega}\mathcal{L}\left(\phi,\omega\right)\coloneqq \mathbb{E}_{\overrightarrow{\bm{x}}_{0}^{\prime}\sim p}\left[\left\|\mathcal{D}( \mathcal{E}(\overrightarrow{\bm{xProof.: To show that the encoder \(\mathcal{E}(\overrightarrow{\bm{x}_{0}};\phi)=\mathcal{S}^{T}\overrightarrow{\bm{x} _{0}}\) and the decoder \(\mathcal{D}(\overrightarrow{\bm{z}_{0}};\omega)=\mathcal{S}\widehat{\bm{z}_{0}}\) minimize the VAE training objective \(\mathcal{L}\left(\phi,\omega\right)\), we begin with the first part of the loss, which is also called _reconstruction error_\(\mathcal{L}_{recon}\left(\phi,\omega\right)\). Substituting \(\mathcal{E}(\overrightarrow{\bm{x}_{0}};\phi)=\mathcal{S}^{T}\overrightarrow {\bm{x}_{0}}\) and \(\mathcal{D}(\overrightarrow{\bm{z}_{0}};\omega)=\mathcal{S}\widehat{\bm{z}_{0}}\), we have

\[\mathcal{L}_{recon}\left(\phi,\omega\right) \coloneqq\mathbb{E}_{\overrightarrow{\bm{x}_{0}}\sim p}\left[ \left\|\mathcal{D}(\mathcal{E}(\overrightarrow{\bm{x}_{0}};\phi);\omega)- \overrightarrow{\bm{x}_{0}}\right\|_{2}^{2}\right]\] \[=\mathbb{E}_{\overrightarrow{\bm{x}_{0}}\sim p}\left[\left\| \mathcal{D}(\mathcal{S}^{T}\overrightarrow{\bm{x}_{0}};\omega)- \overrightarrow{\bm{x}_{0}}\right\|_{2}^{2}\right]\] \[=\mathbb{E}_{\overrightarrow{\bm{x}_{0}}\sim p}\left[\left\| \mathcal{S}\mathcal{S}^{T}\overrightarrow{\bm{x}_{0}}-\overrightarrow{\bm{x} _{0}}\right\|_{2}^{2}\right]\]

Using the fact that \(\overrightarrow{\bm{x}_{0}}\) lives in a linear subspace, we arrive at

\[\mathcal{L}_{recon}\left(\phi,\omega\right) =\mathbb{E}_{\overrightarrow{\bm{x}_{0}}\sim p}\left[\left\| \mathcal{S}\mathcal{S}^{T}\mathcal{S}\overrightarrow{\bm{z}_{0}}-\mathcal{S} \overrightarrow{\bm{z}_{0}}\right\|_{2}^{2}\right]\] \[\stackrel{{(i)}}{{=}}\mathbb{E}_{\overrightarrow{ \bm{z}_{0}}\sim\mathcal{N}(\bm{0},\bm{I}_{k})}\left[\left\|\mathcal{S} \overrightarrow{\bm{z}_{0}}-\mathcal{S}\overrightarrow{\bm{z}_{0}}\right\|_{2 }^{2}\right]=0,\]

where (i) is due to **Assumption 3.1**. Now, we analyze the distribution loss. Note that the KL-divergence between two Gaussian distributions with moments \((\mu_{1},\sigma_{1})\) and \((\mu_{2},\sigma_{2})\) is given by

\[KL\left(\mathcal{N}(\mu_{1},\sigma_{1}),\mathcal{N}(\mu_{2},\sigma_{2}) \right)=\log\left(\frac{\sigma_{2}}{\sigma_{1}}\right)+\frac{\sigma_{1}^{2}+ \left(\mu_{1}-\mu_{2}\right)^{2}}{2\sigma_{2}^{2}}-\frac{1}{2}.\]

Since \(\mathcal{E}\left(\bm{x}_{0}\right)=\mathcal{S}^{T}\bm{x}_{0}=\mathcal{S}^{T} \mathcal{S}\bm{z}_{0}=\bm{z}_{0}\), the distribution loss becomes:

\[\mathcal{L}_{dist}\left(\phi\right)\coloneqq KL\left(\mathcal{E}\sharp p, \mathcal{N}(\bm{0},\bm{I}_{k})\right)=KL\left(\mathcal{N}(\bm{0},\bm{I}_{k}), \mathcal{N}(\bm{0},\bm{I}_{k})\right)=0.\]

### Proof of Theorem 3.4

**Theorem B.3** (Generative Modeling using Diffusion in Latent Space).: _Suppose **Assumption 3.1** holds. Let the optimal solution of the latent diffusion model be_

\[\bm{\theta}^{*}=\arg\min_{\bm{\theta}}\mathbb{E}_{\overrightarrow{\bm{z}_{0} },\overrightarrow{\bm{z}}}\left[\left\|\widehat{\mu}_{1}\left(\overrightarrow{ \bm{z}_{1}}^{\prime}(\overrightarrow{\bm{z}_{0}},\overrightarrow{\bm{\epsilon}} ),\overrightarrow{\bm{z}_{0}}\right)-\mu_{\theta}\left(\overrightarrow{\bm{z}_{1 }}^{\prime}\left(\overrightarrow{\bm{z}_{0}},\overrightarrow{\bm{\epsilon}} \right)\right)\right\|^{2}\right].\]

_For a fixed variance \(\beta>0\), if \(\mu_{\bm{\theta}}\left(\overrightarrow{\bm{z}_{1}}\left(\overrightarrow{\bm{z }_{0}},\overrightarrow{\bm{\epsilon}}\right)\right)\coloneqq\bm{\theta} \overrightarrow{\bm{z}_{1}}\left(\overrightarrow{\bm{z}_{0}},\overrightarrow{ \bm{\epsilon}}\right)\), then the closed-form solution is \(\bm{\theta}^{*}=\sqrt{1-\beta}\bm{I}_{k}\), which after normalization by \(\frac{1}{\sqrt{1-\beta}}\) and composition with the decoder \(\mathcal{D}\left(\overleftarrow{\bm{z}_{0}};\omega\right)\coloneqq\mathcal{S} \overleftarrow{\bm{z}_{0}}\) recovers the true subspace of \(p\left(\overrightarrow{\bm{x}_{0}}\right)\)._

Proof.: In latent diffusion models, the training is performed in the latent space of a pre-trained VAE. If the VAE is chosen from **Proposition 3.3**, then the training objective becomes:

\[\min_{\bm{\theta}} \mathbb{E}_{\overrightarrow{\bm{x}_{0}},\overrightarrow{\bm{ \epsilon}}}\left[\left\|\widehat{\mu}_{1}(\overrightarrow{\bm{z}_{1}}^{\prime} \left(\mathcal{E}(\overrightarrow{\bm{x}_{0}}),\overrightarrow{\bm{\epsilon}} \right),\mathcal{E}(\overrightarrow{\bm{x}_{0}})\right)-\mu_{\bm{\theta}}\left( \overrightarrow{\bm{z}_{1}}^{\prime}\left(\mathcal{E}(\overrightarrow{\bm{x}_{0} }),\overrightarrow{\bm{\epsilon}}\right)\right)\right\|^{2}\right]\] \[=\mathbb{E}_{\overrightarrow{\bm{z}_{0}},\overrightarrow{\bm{\epsilon }}}\left[\left\|\widehat{\mu}_{1}(\overrightarrow{\bm{z}_{1}}^{\prime} (\overrightarrow{\bm{z}_{0}},\overrightarrow{\bm{\epsilon}}),\overrightarrow{\bm{z}_{0 }})-\mu_{\bm{\theta}}\left(\overrightarrow{\bm{z}_{1}}^{\prime}\left( \overrightarrow{\bm{z}_{0}},\overrightarrow{\bm{\epsilon}}\right)\right)\right\|^{2}\right]\] \[=\mathbb{E}_{\overrightarrow{\bm{z}_{0}},\overrightarrow{\bm{\epsilon }}}\left[\left\|\overrightarrow{\bm{z}_{0}}-\mu_{\bm{\theta}}\left( \overrightarrow{\bm{z}_{1}}^{\prime}\left(\overrightarrow{\bm{z}_{0}},\overrightarrow{ \bm{\epsilon}}\right)\right)\right\|^{2}\right]=\mathbb{E}_{\overrightarrow{ \bm{z}_{0}},\overrightarrow{\bm{\epsilon}}}\left[\left\|\overrightarrow{\bm{z}_{0} }-\bm{\theta}\overrightarrow{\bm{z}_{1}}^{\prime}\left(\overrightarrow{\bm{z}_{0}}, \overrightarrow{\bm{\epsilon}}\right)\right\|^{2}\right]\] \[=\mathbb{E}_{\overrightarrow{\bm{z}_{0}},\overrightarrow{\bm{\epsilon }}}\left[\left\|\overrightarrow{\bm{z}_{0}}-\bm{\theta}\left(\overrightarrow{\bm{z}_{ 0}}\sqrt{1-\beta}+\sqrt{\beta}\overrightarrow{\bm{\epsilon}}\right)\right\|^{2}\right]\] \[=\mathbb{E}_{\overrightarrow{\bm{z}_{0}}\sim p}\left[\sum_{i=1}^{k} \left(\overrightarrow{\bm{z}_{0,i}}-\bm{\theta}_{i}^{T}\left(\overrightarrow{\bm{z}_{0}} \sqrt{1-\beta}+\overrightarrow{\bm{\epsilon}}\sqrt{\beta}\right)\right)^{2} \right],\]

[MISSING_PAGE_EMPTY:18]

**Theorem 3.4** to perform one step of _denoising_ by the reverse SDE. In the second step, it runs one step of gradient descent to satisfy the _measurements_ in the pixel space. Finally, it takes one step of gradient descent on the _goodness_ objective, which acts as a regularizer to ensure that the reconstructed image lies on the data manifold.

This can be formalized as:

\[\overleftarrow{\bm{z}_{0}^{\prime}} =\bm{\theta}^{*}\overleftarrow{\bm{z}_{1}}-\bm{\eta}\nabla_{ \overleftarrow{\bm{z}_{1}}}\left\|\mathcal{AD}(\overleftarrow{\bm{z}_{0}}( \overleftarrow{\bm{z}_{1}}))-\bm{y}\right\|_{2}^{2};\] (8) \[\overleftarrow{\bm{z}_{0}} =\arg\min_{\overleftarrow{\bm{z}_{0}^{\prime}}}\left|\left| \overleftarrow{\bm{z}_{0}^{\prime}}-\mathcal{E}(\mathcal{D}(\overleftarrow{\bm {z}_{0}^{\prime}}))\right|\right|_{2}^{2},\] (9)

In practice, solving (9) can be difficult, and can be approximated via gradient descent. In our analysis however, we analyze the exact system of equations above, as (9) has a closed-form solution in the linear setting.

**Theorem B.4** (Posterior Sampling using Goodness Modified Latent DPS).: _Suppose **Assumptions 3.1** and **Assumption 3.2** hold. Denote by \(\bm{\sigma}=\{\sigma_{j}\}_{j=1}^{k}\) the singular values of \((\mathcal{AS})^{T}(\mathcal{AS})\), i.e., \((\mathcal{AS})^{T}(\mathcal{AS})=\bm{U}\Sigma\bm{U}^{T}\coloneqq\bm{U}\bm{D}( \bm{\sigma})\bm{U}^{T},\bm{U}\in\mathbb{R}^{k\times k}\), and let_

\[\bm{\theta}^{*}=\arg\min_{\bm{\theta}}\mathbb{E}_{\bm{z}_{0}^{\prime},\, \mathcal{E}}\left[\left\|\hat{\bm{\mu}}_{1}\left(\overrightarrow{\bm{z}_{1}^ {\prime}}(\overrightarrow{\bm{z}_{0}^{\prime}},\overrightarrow{\bm{\epsilon} }),\overrightarrow{\bm{z}_{0}^{\prime}}\right)-\mu_{\theta}\left( \overrightarrow{\bm{z}_{1}^{\prime}}(\overrightarrow{\bm{z}_{0}^{\prime}}, \overleftarrow{\bm{\epsilon}})\right)\right\|_{2}^{2}\right].\]

_Suppose \(\overrightarrow{\bm{x}_{0}}\sim p(\overrightarrow{\bm{x}_{0}^{\prime}})\). Given measurements \(\bm{y}=\mathcal{A}\overrightarrow{\bm{x}_{0}^{\prime}}\) and any fixed variance \(\beta\in(0,1)\), then with the (unique) step size \(\bm{\eta}=(1/2)\bm{U}\bm{D}(\bm{\eta}_{i})\bm{U}^{T},\bm{\eta}_{i}=\{\eta_{i} ^{j}=1/2\sigma_{j}\}_{j=1}^{k}\), the GML-DPS algorithm (6) samples from the true posterior \(p(\overrightarrow{\bm{x}_{0}^{\prime}}|y)\) and exactly recovers the groundtruth sample, i.e., \(\overleftarrow{\bm{x}_{0}}=\overrightarrow{\bm{x}_{0}^{\prime}}\)._

Proof.: We start with the measurement consistency update (8) and then show that the solution obtained from (8) is already a minimizer of (9). Therefore, we have

\[\overleftarrow{\bm{z}_{0}^{\prime}} =\bm{\theta}^{*}\overleftarrow{\bm{z}_{1}}-\bm{\eta}\nabla_{ \overleftarrow{\bm{z}_{1}}}\left\|\mathcal{AD}(\overleftarrow{\bm{z}_{0}}( \overleftarrow{\bm{z}_{1}}))-\bm{y}\right\|_{2}^{2}\] \[=\overleftarrow{\bm{z}_{1}}-\bm{\eta}\nabla_{\overleftarrow{\bm{z }_{1}}}\left\|\mathcal{AD}(\bm{I}_{k}\overleftarrow{\bm{z}_{1}})-\bm{y}\right\| _{2}^{2}\] \[=\overleftarrow{\bm{z}_{1}}-\bm{\eta}\nabla_{\overleftarrow{\bm{z }_{1}}}\left\|\mathcal{AS}\overleftarrow{\bm{z}_{1}})-\bm{y}\right\|_{2}^{2}\] \[=\overleftarrow{\bm{z}_{1}}-\bm{\eta}\nabla_{\overleftarrow{\bm{z }_{1}}}\left\|\mathcal{AS}\overleftarrow{\bm{z}_{1}}-\bm{y}\right\|_{2}^{2}\] \[=\overleftarrow{\bm{z}_{1}}-2\bm{\eta}\mathcal{S}^{T}\mathcal{A}^{T }\left(\mathcal{AS}\overleftarrow{\bm{z}_{1}}-\bm{y}\right)\] \[=\overleftarrow{\bm{z}_{1}}-2\bm{\eta}\mathcal{S}^{T}\mathcal{A}^{T }\mathcal{A}\mathcal{AS}\overleftarrow{\bm{z}_{1}}+2\bm{\eta}\mathcal{S}^{T} \mathcal{A}^{T}\mathcal{A}\overrightarrow{\bm{x}_{0}^{\prime}}\] \[=\overleftarrow{\bm{z}_{1}}-2\bm{\eta}\mathcal{S}^{T}\mathcal{A}^{T }\mathcal{AS}\overleftarrow{\bm{z}_{1}}+2\bm{\eta}\mathcal{S}^{T}\mathcal{A}^{T }\mathcal{A}\overrightarrow{\bm{x}_{0}^{\prime}},\]

where (i) is due to **Assumption 3.1**. By **Assumption 3.2**, \((\mathcal{AS})^{T}(\mathcal{AS})\) is a positive definite matrix and can be written as \(\bm{U}\Sigma\bm{U}^{T}\):

\[\overleftarrow{\bm{z}_{0}^{\prime}} =\overleftarrow{\bm{z}_{1}}-2\bm{\eta}\bm{U}\Sigma\bm{U}^{T} \overleftarrow{\bm{z}_{1}}+2\bm{\eta}\bm{U}\Sigma\bm{U}^{T}\overleftarrow{\bm {z}_{0}^{\prime}}\] \[=\overleftarrow{\bm{z}_{1}}-2\bm{U}\bm{D}(\bm{\eta}_{i})\bm{U}^{T} \bm{U}\Sigma\bm{U}^{T}\overleftarrow{\bm{z}_{1}}+2\bm{U}\bm{D}(\bm{\eta}_{i}) \bm{U}^{T}\bm{U}\Sigma\bm{U}^{T}\overleftarrow{\bm{z}_{0}^{\prime}}\] \[=\overleftarrow{\bm{z}_{1}}-2\bm{U}\bm{D}(\bm{\eta}_{i})\Sigma\bm{ U}^{T}\overleftarrow{\bm{z}_{1}}+2\bm{U}\bm{D}(\bm{\eta}_{i})\Sigma\bm{U}^{T} \overrightarrow{\bm{z}_{0}^{\prime}}\] \[=\overleftarrow{\bm{z}_{1}}-2\bm{U}\bm{D}(\bm{\eta}_{i})\bm{D}( \bm{\sigma})\bm{U}^{T}\overleftarrow{\bm{z}_{1}}+2\bm{U}\bm{D}(\bm{\eta}_{i}) \bm{D}(\bm{\sigma})\bm{U}^{T}\overrightarrow{\bm{z}_{0}^{\prime}}\] \[=\overleftarrow{\bm{z}_{1}}-2\bm{U}\bm{D}(\bm{\eta}_{i}\odot\bm{ \sigma})\bm{U}^{T}\overleftarrow{\bm{z}_{1}}+2\bm{U}\bm{D}(\bm{\eta}_{i}\odot \bm{\sigma})\bm{U}^{T}\overrightarrow{\bm{z}_{0}^{\prime}}.\]

Since \(\eta_{j}^{i}=1/2\sigma_{j}\), the above expression further simplifies to

\[\overleftarrow{\bm{z}_{0}^{\prime}}=\overleftarrow{\bm{z}_{1}}-\bm{U}\bm{U}^{T} \overleftarrow{\bm{z}_{1}}+\bm{U}\bm{U}^{T}\overrightarrow{\bm{z}_{0}}= \overrightarrow{\bm{z}_{0}^{\prime}}.\]

Next, we show that \(\overleftarrow{\bm{z}_{0}^{\prime}}\) is already a minimizer of (9). This is a direct consequence of the encoder-decoder architecture of the VAE: \(\mathcal{E}(\mathcal{D}(\overleftarrow{\bm{z}_{0}^{\prime}}))=\mathcal{S}^{T} \mathcal{S}\overleftarrow{\bm{z}_{0}^{\prime}}=\overleftarrow{\bm{z}_{0}^{ \prime}}\). Hence, \(\left|\left|\overleftarrow{\bm{z}_{0}^{\prime}}-\mathcal{E}(\mathcal{D}( \overleftarrow{\bm{z}_{0}^{\prime}}))\right|\right|^{2}=0\), andconsequently \(\xleftarrow{\overline{z}}_{0}=\xleftarrow{\overline{z}}_{0}^{\prime}-\gamma\nabla_{ \xleftarrow{\overline{z}}_{1}}\left|\left|\xleftarrow{\overline{z}}_{0}- \mathcal{E}(\mathcal{D}(\xleftarrow{\overline{z}}_{0}))\right|\right|^{2}= \overline{z}_{0}^{\lambda}\). Thus, the reconstructed sample becomes \(\xleftarrow{\overline{x}}_{0}=\mathcal{D}(\xleftarrow{\overline{z}}_{0})= \mathcal{S}\xleftarrow{\overline{z}}_{0}^{\lambda}=\xleftarrow{\overline{x}}_ {0}^{\lambda}\).

Furthermore, as \(\left|\left|\xleftarrow{\overline{z}}_{0}^{\prime}-\mathcal{E}(\mathcal{D}( \xleftarrow{\overline{z}}_{0}))\right|\right|^{2}=0\) for all \(\xleftarrow{\overline{z}}_{0}^{\prime}\), it is evident that the goodness objective cannot rectify the error incurred in the measurement update (8). For this reason, GML-DPS algorithm (6) requires the exact step size to sample from the posterior. 

Beyond the linear setting, we also refer to Table 5 for experiments supporting this result.

### Proof of Theorem 3.6

Different from GML-DPS, PSLD **Algorithm 2** replaces the goodness objective (6) with the gluing objective (7), which can be formalized as:

\[\xleftarrow{\overline{z}}_{0}^{\prime} =\theta^{\star}\xleftarrow{\overline{z}}_{1}-\eta\nabla_{\xleftarrow {\overline{z}}_{1}}\left\|\mathcal{A}\mathcal{D}(\xleftarrow{\overline{z}}_{0}( \xleftarrow{\overline{z}}_{1}))-\yiv\right\|_{2}^{2};\] (10) \[\xleftarrow{\overline{z}}_{0} =\arg\min_{\xleftarrow{\overline{z}}_{0}^{\prime}}\left|\left| \xleftarrow{\overline{z}}_{0}^{\prime}-\mathcal{E}(\mathcal{A}^{T}\mathcal{A} \xleftarrow{\overline{z}}_{0}^{\prime}+(\IIII-\mathcal{A}^{T}\mathcal{A}) \mathcal{D}(\xleftarrow{\overline{z}}_{0}^{\prime}))\right|\right|^{2}.\] (11)

We again remind that solving the minimization problem (11) is hard in general, and can be _approximated_ by gradient descent as typically followed in practice [11]. However, in a linear model setting, (11) has a closed-form solution which we derive to prove exact recovery.

**Theorem B.5** (Posterior Sampling using Diffusion in Latent Space).: _Let **Assumptions 3.1** and 3.2 hold. Let \(\sigma_{j},\forall j=1,\ldots,r\) denote the singular values of \((\mathcal{A}\mathcal{S})^{T}(\mathcal{A}\mathcal{S})\) and let_

\[\theta^{\star}=\arg\min_{\theta}\mathbb{E}_{\xleftarrow{\overline{z}}_{0}^{ \prime},\,\mathcal{E}}\left[\left\|\hat{\mu}_{1}\left(\xleftarrow{\overline{z }}_{1}^{\prime}({\overline{z}}_{0}^{\prime},\xleftarrow{\overline{z}}_{0}^{ \prime}),\xleftarrow{\overline{z}}_{0}^{\prime}\right)-\mu_{\theta}\left( \xleftarrow{\overline{z}}_{1}^{\prime}({\overline{z}}_{0}^{\prime},\xleftarrow {\overline{\epsilon}})\right)\right\|^{2}\right].\]

_Suppose \(\xleftarrow{\overline{x}}_{0}\sim p(\xleftarrow{\overline{x}}_{0}^{\prime})\). Given measurements \(\y=\mathcal{A}\xleftarrow{\overline{x}}_{0}^{\prime}\), any fixed variance \(\beta\in(0,1)\), and any positive step sizes \(\eta_{i}^{j},j=1,2,\ldots,r\), the PSLD Algorithm 2 samples from the true posterior \(p(\xleftarrow{\overline{x}}_{0}^{\prime}|y)\) and exactly recovers the groundtruth sample, i.e., \(\xleftarrow{\overline{x}}_{0}={\overline{x}}_{0}^{\prime}\)._

Proof.: Following the proof in Appendix B.4, we have

\[\xleftarrow{\overline{z}}_{0}^{\prime} =\theta^{\star}\xleftarrow{\overline{z}}_{1}-\eta\nabla_{ \xleftarrow{\overline{z}}_{1}}\left\|\mathcal{A}\mathcal{D}(\xleftarrow{ \overline{z}}_{0}(\xleftarrow{\overline{z}}_{1}))-\yiv\right\|_{2}^{2}\] \[=\xleftarrow{\overline{z}}_{1}-\eta\nabla_{\xleftarrow{\overline{ z}}_{1}}\left\|\mathcal{A}\xleftarrow{\overline{z}}_{1}-\yiv\right\|_{2}^{2}\] \[=\xleftarrow{\overline{z}}_{1}-2\eta\nabla^{T}\mathcal{A}^{T}( \mathcal{A}\xleftarrow{\overline{z}}_{1}-\yiv)\] \[=\xleftarrow{\overline{z}}_{1}-2\eta\nabla^{T}\mathcal{A}^{T} \mathcal{A}\xleftarrow{\overline{z}}_{1}+2\eta\nabla^{T}\mathcal{A}^{T} \mathcal{A}\xleftarrow{\overline{z}}_{0}^{\prime}\] \[=\xleftarrow{\overline{z}}_{1}-2\eta\nabla^{T}\mathcal{A}^{T} \mathcal{A}\xleftarrow{\overline{z}}_{1}+2\eta\nabla^{T}\mathcal{A}^{T} \mathcal{A}\xleftarrow{\overline{z}}_{0}^{\prime}.\]

We use the above expression to derive a closed-form solution to the minimization problem (11):

\[\mathbf{0} =\nabla_{\xleftarrow{\overline{z}}_{0}^{\prime}}\left\|\xleftarrow{ \overline{z}}_{0}^{\prime}-\mathcal{S}^{T}(\mathcal{A}^{T}\mathcal{A}\xleftarrow{ \overline{z}}_{0}^{\prime}+(\IIII-\mathcal{A}^{T}\mathcal{A})\xleftarrow{ \overline{z}}_{0}^{\prime})\right\|_{2}^{2}\] \[=\nabla_{\xleftarrow{\overline{z}}_{0}^{\prime}}\left\|\xleftarrow{ \overline{z}}_{0}^{\prime}-\mathcal{S}^{T}\mathcal{A}^{T}\mathcal{A}\xleftarrow{ \overline{z}}_{0}^{\prime}-\mathcal{S}^{T}(\III-\mathcal{A}^{T}\mathcal{A}) \xleftarrow{\overline{z}}_{0}^{\prime}\right\|_{2}^{2}\] \[=\nabla_{\xleftarrow{\overline{z}}_{0}^{\prime}}\left\|\xleftarrow{ \overline{z}}_{0}^{\prime}-\mathcal{S}^{T}\mathcal{A}^{T}\mathcal{A}\xleftarrow{ \overline{z}}_{0}^{\prime}-\mathcal{S}^{T}\xleftarrow{\overline{z}}_{0}^{\prime} +\mathcal{S}^{T}\mathcal{A}^{T}\mathcal{A}\xleftarrow{\overline{z}}_{0}^{ \prime}\right\|_{2}^{2}\] \[=2\left(\III-\mathcal{S}^{T}\mathcal{S}+\mathcal{S}^{T}\mathcal{A}^{T} \mathcal{A}\xleftarrow{\mathcal{S}}\right)\left(\xleftarrow{\overline{z}}_{0}^{ \prime}-\mathcal{S}^{T}\mathcal{A}^{T}\mathcal{A}\xleftarrow{\overline{z}}_{0}^{ \prime}-\mathcal{S}^{T}\xleftarrow{\overline{z}}_{0}^{\prime}+\mathcal{S}^{T} \mathcal{A}^{T}\mathcal{A}\xleftarrow{\overline{z}}_{0}^{\prime}\right)\right)\] \[=2\mathcal{S}^{T}\mathcal{A}^{T}\mathcal{A}\mathcal{S}\left(\xleftarrow{ \overline{S}}^{T}\mathcal{A}^{T}\mathcal{A}\xleftarrow{\overline{z}}_{0}^{ \prime}-\mathcal{S}^{T}\mathcal{A}^{T}\mathcal{A}\xleftarrow{\overline{z}}_{0}^{ \prime})\right),\]

where the last step is due to **Assumption 3.1**. Thus, we have

\[\xleftarrow{\overline{z}}_{0}=\arg\min_{\xleftarrow{\overline{z}}_{0}^{\prime}} \left|\xleftarrow{\overline{z}}_{0}^{\prime}-\mathcal{E}(\mathcal{A}^{T}\mathcal{A} \xleftarrow{\overline{z}}_{0}^{\prime}+(\IIII-\mathcal{A}^{T}\mathcal{A}) \mathcal{D}(\xleftarrow{\overline{z}}_{0}^{\prime}))\right|_{2}^{2}={\overline{z}}_ {0}^{\prime},\]which produces \(\widehat{\bm{x}}_{0}=\mathcal{D}(\widehat{\bm{z}}_{0})=\mathcal{D}(\bm{\overline{ x}}_{0}^{\prime})=\mathcal{S}\bm{\overline{x}}_{0}^{\prime}=\bm{\overline{x}}_{0}^{\prime}\). 

It is worth highlighting that PSLD exactly recovers the groundtruth sample irrespective of the choice of the step size \(\eta\), whereas GML-DPS requires the step size to be exactly \(\bm{\eta}=(1/2)\bm{U}\bm{D}(\bm{\eta}_{i})\bm{U}^{T}\).

## Appendix C Additional Experiments

### Implementation Details

For inpainting tasks, we note that the PSLD sampler generates missing parts (by design of our gluing objective) that are consistent with the known portions of the image, i.e., \(\widehat{\bm{x}}_{0}=\mathcal{A}^{T}\mathcal{A}\bm{\overline{x}}_{0}^{\prime} +(\bm{I}_{d}-\mathcal{A}^{T}\mathcal{A})\mathcal{D}(\widehat{\bm{x}}_{0})\). This is different from the DPS sampler, which generates the whole image which may not match the observations exactly. In other words, in the last of step of our algorithm, the observations are glued onto the corresponding parts of the generated image, leaving the unmasked portions untouched [54]. This sometimes creates edge effects which are then removed by post-processing the glued image through the encoder and decoder of the SD model, i.e. running one last step of our algorithm. Figure 2 illustrates that gluing the observations in commercial services still leads to visually inconsistent results (e.g. head in top row) unlike our method.

For all other tasks, such as motion deblur, Gaussian deblur, and super-resolution, this last step is not needed, as there is no box inpainting, i.e., \(\widehat{\bm{x}}_{0}=\mathcal{D}(\widehat{\bm{z}}_{0})\). Furthermore, we use the same measurement operator \(\mathcal{A}\) and its transpose \(\mathcal{A}^{T}\) as provided by the DPS code repository8. However, since Stable Diffusion v1.5 generates images of size \(512\times 512\) resolution and DPS operates at \(256\times 256\), we adjust the size of the kernels used in PSLD to ensure that both the methods use the same amount of information while sampling from the posterior. During evaluation, we downsample PSLD generated images from \(512\times 512\) to \(256\times 256\) to compare with DPS at the same resolution.

Footnote 8: https://github.com/DPS2022/diffusion-posterior-sampling/blob/main/guided_diffusion/measurements.py

**PSLD (Stable Diffusion-V1.5 ):** We run **Algorithm 2** with Stable Diffusion version 1.5 as the foundation model9. We use a fixed \(\eta=1\) and \(\gamma=0.1\). Since we study posterior sampling of images without conditioning on _text_ inputs, we pass an empty string to the Stable Diffusion foundation model, which accepts texts as an input argument. For better performance, we recommend using the latest pretrained weights.

Footnote 9: https://huggingface.co/runwayml/stable-diffusion-v1-5

**PSLD (LDM-VQ-4 ):** This is the same sampling algorithm as before but with a different latent diffusion model, LDM-VQ-410, which contains pretrained weights for FFHQ 25611 and large-scale text-to-image generative model12. We keep the hyperparameters same (\(\eta=1\) and \(\gamma=0.1\)). For each task, we provide hyper-parameter details in our codebase13. Although we have tested our framework with these two latent-diffusion-models, one may experiment with other latent-diffusion-models available in the same repository.

Footnote 10: https://github.com/CompVis/latent-diffusion

Footnote 11: https://ommer-lab.com/files/latent-diffusion/ffhq.zip

Footnote 12: https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt

Footnote 13: https://github.com/LituRout/PSLD

**DPS:** We use the original source code provided by the authors14.

Footnote 14: https://github.com/DPS2022/diffusion-posterior-sampling

**OOD images are sourced online:**

1. Figure 1: the original images are generated by Stable Diffusion v-2.115.
2. Figure 2 first row: Walking example from the web.
3. Figure 2 second row, Obama-Biden image from the web.
4. Figure 2 third row, Fisherman from ImageNet 256 [17].
5. Figure 4 first row: Racoon image from the web.
6. Figure 4 second row: Fisherman from ImageNet 256 [17].
7. Figure 15: Celebrity face from the web.

### Additional Experimental Evaluation

Here, we provide additional results to support our theoretical claims on various inverse problems.

Figures 6, 7, 8, and 9 show the inpainting results of user defined masks obtained from our PSLD inpainting web demo. Note that the foundation model used in this demo is a generic model. For better performance on specific images, we recommend finetuning the foundation model on this class and then running posterior sampling using our web demo: https://huggingface.co/spaces/PSLD/PSLD.

Figure 10 and 11 illustrate **super-resolution** (\(4\times\)) of in-distribution samples from the validation set of FFHQ 256. Observe that the samples generated by DPS are far from the groundtruth sample. On the other hand, the samples generated by PSLD closely capture the perceptual quality of the groundtruth sample. In other words, one may identify (b) and (c) as images of two different individuals, whereas (b) and (d) of the same individual. We attribute this _photorealism_ of our method to the power of Stable Diffusion foundation model and the ability to use the knowledge of the VAE encoder-decoder in the gluing objective.

In addition, we test on out-of-distribution samples from ImageNet [17] validation set. Figure 12 and Figure 13 show the results in **motion deblur** and **Gaussian deblur**, respectively. By leveraging the foundation model Stable Diffusion v1.5, our PSLD method clearly outperforms DPS [11] in the general domain. Further, Figures 14, 15, and 16 show reconstruction of general domain samples for **random inpainting**, **super-resolution**, and **destriping** tasks, respectively. In all these tasks, the samples generated by PSLD are closer to the groundtruth sample than the ones generated by DPS. Figure 17 shows the results on image colorization. Table 5 and Table 6 show the quantitative results. Table 7 draws a comparison between the latent-DPS and PSLD algorithms, and shows that the PSLD objective enhances the reconstruction performance.

In Table 8, we compare the runtime and NFEs of PSLD with prior works. PSLD-SD (trained on LAION-5B) takes 776 s to generate 512x512 images. To compare with other methods that generate 256x256 images, we divide our runtime by 4. All the other methods use diffusion models trained on FFHQ and produce 256x256 images.

Figure 6: Results from the web application of our PSLD algorithm, \(512\times 512\). The original image (1) is generated by Stable Diffusion v-2.1 with the prompt,_A dinner date between a robot couple during sunset_.

Figure 8: Results from the web application of our PSLD algorithm, \(512\times 512\). The original image (1) is generated by Stable Diffusion v-2.1 with the prompt,_A teddy bear showing stop sign at the traffic_.

Figure 7: Results from the web application of our PSLD algorithm, \(512\times 512\). The original image (1) is generated by Stable Diffusion v-2.1 with the prompt,_A panda wearing a spiderman costume_.

## Appendix D Additional Discussion

**Curse of ambient dimension:** DPS [11] suffers from the curse of ambient dimension because in this method, gradients are computed in the pixel space with dimension \(d\). However, latent-based methods such as PSLD compute gradients in the latent dimension \(k\), and hence the computation is more efficient. Furthermore, applying the chain rule on VAE and running diffusion in the latent space is less expensive than running diffusion in pixel space directly. In practice, the computational complexity of Stable Diffusion model (\(\sim 4GB\)) is higher (roughly 6 times) than the computational complexity of the encoder-decoder model (\(\sim 700MB\)). Therefore, applying the chain rule in the encoder-decoder and running diffusion in the latent space is less expensive than applying diffusion models in the pixel space directly.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{Inpaint (random)} & \multicolumn{2}{c}{SR (\(4\times\))} & \multicolumn{2}{c}{Gaussian Deblur} \\ \cline{2-7} Method & PSNR (\(\uparrow\)) & SSIM (\(\uparrow\)) & PSNR (\(\uparrow\)) & SSIM (\(\uparrow\)) & PSNR (\(\uparrow\)) & SSIM (\(\uparrow\)) \\ \hline PSLD (Ours) & **33.71** & **0.943** & **30.73** & **0.867** & **30.10** & **0.843** \\ GML-DPS (Ours) & 29.49 & 0.844 & 29.77 & 0.860 & 29.21 & 0.820 \\ \hline DPS [11] & 25.23 & **0.851** & 25.67 & 0.852 & 24.25 & 0.811 \\ DDRM [26] & 9.19 & 0.319 & 25.36 & 0.835 & 23.36 & 0.767 \\ MCG [13] & 21.57 & 0.751 & 20.05 & 0.559 & 6.72 & 0.051 \\ PnP-ADMM [6] & 8.41 & 0.325 & 26.55 & 0.865 & 24.93 & 0.812 \\ Score-SDE [50] & 13.52 & 0.437 & 17.62 & 0.617 & 7.12 & 0.109 \\ ADMM-TV & 22.03 & 0.784 & 23.86 & 0.803 & 22.37 & 0.801 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Quantitative random inpainting results on FFHQ \(256\) validation set [25, 11]. We use Stable Diffusion (v1.5) trained on LAION.

Figure 9: Results from the web application of our PSLD algorithm, \(512\times 512\). The original image (1) is generated by Stable Diffusion v-2.1 with the prompt,_A cute dog playing with a toy teddy bear on the lawn_.

Figure 10: Super-resolution results on images from FFHQ 256 [25; 11] (in distribution).

Figure 11: Super-resolution results on FFHQ 256 [25; 11] (in distribution).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{2}{c}{SR (4\(\times\))} & \multicolumn{2}{c}{Gaussian Deblur} \\ \cline{2-5} Method & FID (\(\downarrow\)) & LPIPS (\(\downarrow\)) & FID (\(\downarrow\)) & LPIPS (\(\downarrow\)) \\ \hline PSLD (Ours) & **34.28** & **0.201** & **41.53** & **0.221** \\ \hline DPS [11] & 39.35 & 0.214 & 44.05 & 0.257 \\ DDRM [26] & 62.15 & 0.294 & 74.92 & 0.332 \\ MCG [13] & 87.64 & 0.520 & 101.2 & 0.340 \\ PnP-ADMM [6] & 66.52 & 0.353 & 90.42 & 0.441 \\ Score-SDE [50] & 96.72 & 0.563 & 109.0 & 0.403 \\ ADMM-TV & 110.6 & 0.428 & 186.7 & 0.507 \\ \hline \hline \end{tabular} 
\begin{tabular}{l c c c} \hline \hline  & \multicolumn{2}{c}{SR (4\(\times\))} & \multicolumn{2}{c}{Gaussian Deblur} \\ \cline{2-5} Method & PSNR (\(\uparrow\)) & SSIM (\(\uparrow\)) & PSNR (\(\uparrow\)) & SSIM (\(\uparrow\)) \\ \hline PSLD (Ours) & **30.73** & **0.867** & **30.10** & **0.843** \\ GML-DPS (Ours) & 29.77 & 0.860 & 29.21 & 0.820 \\ \hline DMPS [34] & 27.63 & - & 25.41 & - \\ DPS [11] & 25.67 & 0.852 & 24.25 & 0.811 \\ DDRM [26] & 25.36 & 0.835 & 23.36 & 0.767 \\ MCG [13] & 20.05 & 0.559 & 6.72 & 0.051 \\ PnP-ADMM [6] & 26.55 & 0.865 & 24.93 & 0.812 \\ Score-SDE [50] & 17.62 & 0.617 & 7.12 & 0.109 \\ ADMM-TV & 23.86 & 0.803 & 22.37 & 0.801 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Additional quantitative results on FFHQ \(256\) validation set [25, 11].

Figure 12: Motion deblur results on ImageNet 256 [17] (out-of-distribution).

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & \multicolumn{3}{c}{Inpaint (box)} \\ \cline{2-4} Method & PSNR (\(\uparrow\)) & SSIM (\(\uparrow\)) & LPIPS (\(\downarrow\)) \\ \hline PSLD & **24.22** & **0.819** & **0.158** \\ latent-DPS & 17.58 & 0.780 & 0.21 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Latent-DPS and PSLD methods evaluated on FFHQ \(256\) validation set [25, 11]. We use the _latent diffusion_ (LDM-VQ-4) trained on FFHQ \(256\). Latent-DPS is a special case of PSLD algorithm when \(\gamma=0\).

Figure 13: Gaussian deblur results on ImageNet 256 [17] (out-of-distribution).

Figure 14: Random inpainting results on ImageNet 256 [17] (out-of-distribution).

Figure 16: Destriping results on out-of-distribution samples from the web, \(256\times 256\). (**Top row**) Horizontal destriping: LPIPS of PSLD=0.244 and DPS [11]=0.613. (**Bottom row**) Vertical destriping: LPIPS of PSLD=0.255, DPS [11]=0.597.

Figure 15: Super-resolution (using nearest neighbor kernel from [32]) results on out-of-distribution samples from the web, \(256\times 256\) (see Table 1 for LPIPS of these images).

## Appendix A

Figure 17: Additional colorization results on images from FFHQ 256 [25; 11]. PSLD generates photo-realistic color, whereas DPS [11] generates overly saturated images.

\begin{table}
\begin{tabular}{l l} \hline \hline Method & Runtime (s) \\ \hline PSLD-LDM & 187.00 \\ PSLD-LDM (LAION-400M) & 190.00 \\ PSLD-SD (LAION-5B) & 194.25 \\ \hline DMPS [34] & 67.02 \\ DPS [11] & 180.00 \\ DDNM+ [55] & 18.5 \\ DDRM [26] & 2.15 \\ MCG [13] & 193.71 \\ \hline \hline Method & NFEs \\ \hline PSLD (Ours) & 100 to 1000 \\ \hline DPS [11] & 1000 \\ DDRM [26] & 20 \\ RED [40] & 500 \\ IIGDM [46] & 20 to 100 \\ Palette [43] & 1000 \\ Regression & 1 \\ SNIPS [28] & 1000 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Runtime (top) and NFEs (bottom) of different posterior sampling algorithms. Runtimes are computed for the super-resolution task.