# Causal Effect Identification

in Uncertain Causal Networks

 Sina Akbari

EPFL, Switzerland

sina.akbari@epfl.ch

&Fateme Jamshidi

EPFL, Switzerland

fateme.jamshidi@epfl.ch

&Ehsan Mokhtarian

EPFL, Switzerland

ehsan.mokhtarian@epfl.ch

&Matthew J. Vowels

University of Lausanne, Switzerland

matthew.vowels@unil.ch

&Jalal Etesami

TUM, Germany

j.etesami@tum.de

&Negar Kiyavash

EPFL, Switzerland

negar.kiyavash@epfl.ch

###### Abstract

Causal identification is at the core of the causal inference literature, where complete algorithms have been proposed to identify causal queries of interest. The validity of these algorithms hinges on the restrictive assumption of having access to a correctly specified causal structure. In this work, we study the setting where a probabilistic model of the causal structure is available. Specifically, the edges in a causal graph exist with uncertainties which may, for example, represent degree of belief from domain experts. Alternatively, the uncertainty about an edge may reflect the confidence of a particular statistical test. The question that naturally arises in this setting is: Given such a probabilistic graph and a specific causal effect of interest, what is the subgraph which has the highest plausibility and for which the causal effect is identifiable? We show that answering this question reduces to solving an NP-complete combinatorial optimization problem which we call the edge ID problem. We propose efficient algorithms to approximate this problem and evaluate them against both real-world networks and randomly generated graphs.

## 1 Introduction

A large proportion of questions of interest in various fields, including but not limited to psychology, social sciences, behavioral sciences, medical research, epidemiology, economy, etc., are causal in nature [33, 24, 8]. In order to estimate causal effects, the gold standard is performing controlled interventions and experiments. Unfortunately, such experiments can be prohibitively expensive, unethical, or impractical [10, 12]. In contrast, non-experimental data are comparatively abundant, and no expensive interventions are required to generate such data. This has motivated the development of numerous techniques for understanding whether a causal query can be answered using observational data. Specifically, if a particular causal query is _identifiable_, it means it can be expressed as a function of the observational distribution and thus estimated from observational data (at least in principle).

A significant body of the causal inference literature is dedicated to the identification problem [23, 6, 29, 26, 24]. In particular, [13] presented a complete algorithmic approach to decide the identifiability of a specific query and proved that Pearl's do calculus is complete. Furthermore, [26] provided graphical criteria to decide the identifiability based on the _hedge_ criterion. The problem of identifiability was later generalized to deal with interventional data [21, 20], selection bias [1], context-specific independence relations [31, 15] and causal bandits [16]. However, all of these results hinge on the full specification of the causal structure, i.e., access to a correctly specified Acyclic Directed Mixed Graph (ADMG) that models the causal dynamics of the system. This requirement is restrictive in a number of ways. (i) The causal identification problem is concerned with inference from theobservational data, but the ADMG cannot be inferred from the observational distribution alone. (ii) Structure learning methods rely heavily on statistical tests [27, 9, 5, 4, 17], which are prone to errors arising from lack of sufficient data and method-specific limitations [25]. This can in turn result in misspecification of the causal structure.

A notable work to address the second challenge is [22], where the authors proposed a causal discovery method that assigns a probability to the existence of each edge, as opposed to making "hard" decisions based on a single or limited number of tests. This is achieved by updating the posterior probability of a direct link between two variables, denoted as \(X\) and \(Y\), using conditional dependence tests that incorporate independent evidence. The authors demonstrate that the posterior probability denoted as \(p_{i}\), of a link between \(X\) and \(Y\) after performing \(i\) dependence tests (\(d_{j}\), where \(j\) ranges from \(1\) to \(i\)) is given by the following equation:

\[p_{i}=\frac{p_{i-1}d_{i}}{p_{i-1}d_{i}+(1-p_{i-1})(G+1-d_{i})},\]

where \(G\) represents a factor ranging from \(0\) to \(1\), and it can be interpreted as the measure of "importance" or relevance associated with the truthfulness of each test, denoted as \(d_{i}\). Other closely linked work include [7] and [4], where an optimal transport (OT)-based causal discovery approach is proposed that assigns a score to each edge of the graph denoting the strength of that edge.

While previous research, such as [22], has focused on probabilistic learning of causal graphs, the problem of causal effect identification on these graphs remains unexplored. This paper introduces a novel approach to address causal effect identification in a scenario where only a probabilistic model of the causal structure is available, as opposed to a complete specification of the structure itself. For instance, an ADMG \(\mathcal{G}\) is given along with probabilities assigned to each edge of \(\mathcal{G}\). An example is shown in Figure 0(a). These probabilities could represent uncertainties arising from statistical tests or the strength of belief of domain experts concerning the plausibility of the existence of an edge. Under this setting, each ADMG on the set of vertices of \(\mathcal{G}\) is assigned its own plausibility score. Since the causal structure is not deterministic anymore, answering questions such as "_is the causal effect \(P(Y|do(X))\) identifiable?_" also becomes probabilistic in nature. One can compare the overall plausibility of different subgraphs in which the causal effect is identifiable and then select the graph which maximizes the plausibility. In this work, for a specific causal query \(P(Y|do(X))\), we first answer the question, "Which graph has the highest plausibility among those compliant with the probabilistic ADMG model that renders \(P(Y|do(X))\) identifiable?" The answer to this question allows us to quantify how confident we are about the causal identification task given the combination of the data at hand and the corresponding probabilistic model.

As the causal identification task is carried out through an identification formula that is based on the causal structure, our second focus is on deriving an identification formula for a given causal query that holds with the highest probability. This problem poses a greater challenge compared to the former, as a single identification formula can be valid for various graphs. Consequently, the probability that a particular identification formula is valid for a causal query becomes an aggregate probability across all graphs where this formula holds. We shall illustrate this point in more detail through Example 1 in Section 2. The intricacy of characterizing all valid graphs for a given formula makes it an intractable and open problem. To surmount this obstacle, we establish that if an identification formula is valid for a causal graph, it remains valid for all edge-induced subgraphs of that graph. This allows us to form a surrogate problem (see Problem 2 in Section 2.1) that recovers a causal graph with the highest aggregated probability of its subgraphs. The probability of the resulting graph provides a lower bound on the highest probable identification formula.

Both problems discussed in this work are aimed at evaluating the plausibility of performing causal identification for a specific query given a dataset and a non-deterministic model describing the causal structure. To sum up, our main contributions are as follows.

1. We study the problem of causal identifiability in probabilistic causal models, where there are uncertainties about the existence of edges and whether a given causal effect is identifiable. More precisely, we consider two problems: 1) finding the most probable graph that renders a desired causal query identifiable, and 2) finding the graph with the highest aggregate probability over its edge-induced subgraphs that renders a desired causal query identifiable.

2. We show that both aforementioned problems reduce to a special combinatorial optimization problem which we call the _edge ID problem_. Moreover, we prove that the edge ID problem is NP-complete, and thus, so are both of the problems.
3. We propose several exact and heuristic algorithms for the aforementioned problems and evaluate their performances through different experiments.

## 2 Preliminaries

We utilize small letters for variables and capital letters for sets of variables. Calligraphic letters are used to denote graphs. An acyclic directed mixed graph (ADMG) \(\mathcal{G}=(V^{\mathcal{G}},E_{d}^{\mathcal{G}},E_{b}^{\mathcal{G}})\) is defined as an acyclic graph on the vertices \(V^{\mathcal{G}}\), where \(E_{d}^{\mathcal{G}}\subseteq V^{\mathcal{G}}\times V^{\mathcal{G}}\) and \(E_{b}^{\mathcal{G}}\subseteq\binom{V^{\mathcal{G}}}{2}\) are the set of directed and bidirected edges among the vertices, respectively. With slight abuse of notation, if \(e\in E_{d}^{\mathcal{G}}\cup E_{b}^{\mathcal{G}}\), we write \(e\in\mathcal{G}\). We use \(\mathcal{G}^{\prime}\subseteq\mathcal{G}\) when \(\mathcal{G}^{\prime}\) is an edge-induced subgraph of \(\mathcal{G}\), i.e., \(\mathcal{G}^{\prime}=(V^{\mathcal{G}^{\prime}},E_{d}^{\mathcal{G}^{\prime}},E _{b}^{\mathcal{G}^{\prime}})\), where \(V^{\mathcal{G}^{\prime}}=V^{\mathcal{G}}\) and \(E_{i}^{\mathcal{G}^{\prime}}\subseteq E_{i}^{\mathcal{G}}\) for \(i\in\{b,d\}\). We denote by \(\mathcal{G}[X]\) the vertex-induced subgraph of \(\mathcal{G}\) over the subset of vertices \(X\subseteq V^{\mathcal{G}}\). For a set of vertices \(X\), we denote by \(\mathit{Anc}_{\mathcal{G}}(X)\) the set of vertices in \(\mathcal{G}\) that have a directed path to \(X\). Note that \(X\subseteq\mathit{Anc}_{\mathcal{G}}(X)\). Let \(P_{X}(Y)\) be a shorthand for \(P(Y|do(X))\), and \(P^{M}(\cdot)\) denote the distribution of variables described by the causal model \(M\).

**Definition 2.1** (Identifiability [24]).: Given a causal ADMG \(\mathcal{G}=(V^{\mathcal{G}},E_{d}^{\mathcal{G}},E_{b}^{\mathcal{G}})\), and two disjoint subsets of variables \(X,Y\subseteq V^{\mathcal{G}}\), the causal effect of \(X\) on \(Y\), denoted by \(P_{X}(Y)\), is identifiable in \(\mathcal{G}\) if \(P_{X}^{M_{1}}(Y)=P_{X}^{M_{2}}(Y)\) for any two models \(M_{1}\) and \(M_{2}\) that induce \(\mathcal{G}\) and \(P^{M_{1}}(V^{\mathcal{G}})=P^{M_{2}}(V^{\mathcal{G}})>0\).

**Definition 2.2** (Valid identification formula).: For a causal ADMG \(\mathcal{G}\) over variables \(V^{\mathcal{G}}\) and a causal query \(P_{X}(Y)\), we say a functional \(\mathcal{F}\) defined on the probability space over \(V^{\mathcal{G}}\) is a valid identification formula for \(P_{X}(Y)\) in \(\mathcal{G}\) if \(P_{X}^{M_{1}}(Y)=P_{X}^{M_{2}}(Y)=\mathcal{F}(P^{M_{1}}(V^{\mathcal{G}}))= \mathcal{F}(P^{M_{2}}(V^{\mathcal{G}}))\) for any two models \(M_{1}\) and \(M_{2}\) that induce \(\mathcal{G}\) and \(P^{M_{1}}(V^{\mathcal{G}})=P^{M_{2}}(V^{\mathcal{G}})>0\).

For any query \(P_{X}(Y)\), let \([\mathcal{G}]_{Id(P_{X}(Y))}\) denote the set of subgraphs of \(\mathcal{G}\) in which \(P_{X}(Y)\) is identifiable (note that if \(\mathcal{G}\) is complete graph, \([\mathcal{G}]_{Id(P_{X}(Y))}\) is the set of all graphs in which \(P_{X}(Y)\) is identifiable.) We denote by \(Q[Y]\) the causal effect of \(V\setminus Y\) on \(Y\), i.e., \(Q[Y]\!=\!P(Y|do(V\setminus Y))\).

**Definition 2.3** (District).: For ADMG \(\mathcal{G}=(V^{\mathcal{G}},E_{d}^{\mathcal{G}},E_{b}^{\mathcal{G}})\), let \(\mathcal{G}_{\leftrightarrow}\) denote the edge-induced subgraph of \(\mathcal{G}\) over its bidirected edges. \(X\subseteq V^{\mathcal{G}}\) is a district (aka c-component) in \(\mathcal{G}\) if \(\mathcal{G}_{\leftrightarrow}[X]\) is connected.

**Definition 2.4** (Hedge [26]).: Let \(\mathcal{G}\) be an ADMG, and \(Y\subsetneq X\) be two subsets of its vertices, where \(Y\) is a district in \(\mathcal{G}[Y]\). Vertices \(X\) form a hedge for \(Q[Y]\) if \(X\) is a district in \(\mathcal{G}[X]\) and \(\mathit{Anc}_{\mathcal{G}[X]}(Y)=X\)1.

Footnote 1: We showed in our previous work [2] that this intuitive definition is equivalent to the standard definition of hedge in [26].

**Definition 2.5** (Maximal hedge [2]).: For ADMG \(\mathcal{G}\) with set vertices \(Y\), let \(X\) be the union of all hedges formed for \(Q[Y]\). Graph \(\mathcal{G}[X]\), denoted by \(\mathbf{MH}(\mathcal{G},Y)\), is called the maximal hedge for \(Q[Y]\).

As an example, both sets \(\{t,x\}\) and \(\{z,x\}\) form a hedge for \(Q[x]\) in \(\mathcal{G}\) in Figure 0(a), and \(\mathcal{G}[\{x,z,t\}]\) is the maximal hedge for \(Q[x]\).

### Problem setup

Let \(\mathcal{G}=(V^{\mathcal{G}},E_{d}^{\mathcal{G}},E_{b}^{\mathcal{G}})\) be an ADMG, where \(V^{\mathcal{G}}\) is the set of vertices each representing an observed variable of the system, \(E_{d}^{\mathcal{G}}\) is the set of directed edges, and \(E_{b}^{\mathcal{G}}\) is the set of bidirected edges among \(V^{\mathcal{G}}\). We know _a priori_ that the true ADMG describing the system is an edge-induced subgraph of \(\mathcal{G}^{2}\), and we are given a probability map that indicates for each subgraph of \(\mathcal{G}\) such as \(\mathcal{G}_{s}\), with what probability \(\mathcal{G}_{s}\) is the true causal ADMG of the system. We denote this probability as \(P(\mathcal{G}_{s})\). For instance, if edge probabilities \(p_{e}\) are assumed to be mutually independent, \(P(\mathcal{G}_{s})\) takes the form:

\[P(\mathcal{G}_{s})=\prod_{e\in\mathcal{G}_{s}}p_{e}\prod_{e\notin\mathcal{G}_ {s}}(1-p_{e}).\] (1)

In what follows, we will refer to \(P(\mathcal{G}_{s})\) simply as the probability of the ADMG \(\mathcal{G}_{s}\). The first problem of our interest is formally defined as follows.

**Problem 1**.: _We consider the problem of finding the most probable edge-induced subgraph of \(\mathcal{G}\), in which the causal effect \(Q[Y]\)3 is identifiable. That is, the goal is to find the ADMG \(\mathcal{G}^{*}\) defined by_

Footnote 3: Note that in case such information is not available, one can simply take \(\mathcal{G}\) to be a complete graph over both its directed and bidirected edges, as every graph is a subgraph of the complete graph. Even when a certain \(\mathcal{G}\) is available but the investigator is uncertain about it, the complete graph can serve as a conservative choice to eliminate uncertainty.

\[\mathcal{G}^{*}:=\operatorname*{arg\,max}_{\mathcal{G}_{s}\subseteq\mathcal{G},\mathcal{G}_{s}\in[\mathcal{G}]_{td(Q[Y])}}P(\mathcal{G}_{s}).\] (2)

We will prove in Lemma 3 that if \(Q[Y]\) is identifiable in \(\mathcal{G}\), then it is also identifiable in every edge-induced subgraph of \(\mathcal{G}\). In other words, if \(\mathcal{G}\) is a feasible solution to the above optimization problem, so are all its edge-induced subgraphs. Furthermore, the same identification functional that is valid w.r.t. \(\mathcal{G}\), is also valid w.r.t. every subgraph of \(\mathcal{G}\). Let us illustrate this first on an example.

**Example 1**.: _Consider the ADMG in Figure 0(a). With the given edge probabilities and assuming independence among the edge probabilities, the subgraph of \(\mathcal{G}\) illustrated in Figure 0(b) has probability \(0.7\times 0.7\times 0.1=0.049,\) whereas the subgraph of Figure 0(c) has probability \(0.3\times 0.3\times 0.9=0.081\) (see Eq. (1)). If we were to solve Problem 1, we would choose \(\mathcal{G}_{2}\) over \(\mathcal{G}_{1}\), as it has a higher probability. Now consider identification formulas in \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\), respectively:_

\[\mathcal{F}_{1}:Q[Y]=P(Y|X),\qquad\mathcal{F}_{2}:Q[Y]=\sum_{Z,T}P(Y|X,Z,T)P(Z, T).\]

\(\mathcal{F}_{1}\) _is a valid identification formula for any edge-induced subgraph of \(\mathcal{G}_{1}\) (see Lemma 3). Analogously, \(\mathcal{F}_{2}\) is valid for all edge-induced subgraphs of \(\mathcal{G}_{2}\). If we consider the aggregate probability of the subgraphs of \(\mathcal{G}_{1}\) and \(\mathcal{G}_{2}\), i.e.,_

\[\sum_{\mathcal{G}\subseteq\mathcal{G}_{1}}P(\mathcal{\hat{G}})=1-0.9=0.1,\quad \text{vs.}\quad\sum_{\mathcal{G}\subseteq\mathcal{G}_{2}}P(\mathcal{\hat{G}} )=(1-0.7)\times(1-0.7)=0.09,\]

_then we should prefer choosing \(\mathcal{G}_{1}\) over \(\mathcal{G}_{2}\), as its identification formula \(\mathcal{F}_{1}\) is more likely to be valid than \(\mathcal{F}_{2}\) considering the fact that for all its subgraphs, the identification functional \(\mathcal{F}_{1}\) is still valid._

_Plausibility_ of a certain identification functional \(\mathcal{F}\) is the sum of the probabilities of all graphs in which \(\mathcal{F}\) is valid given the query of interest. Finding the most plausible identification formula for a given query requires computing the plausibility of all formulae. Since the space of all formulae is intractable, alternatively, we can enumerate all valid formulae for a given graph which changes the problem's search space to the space of all graphs. However, this is yet another challenging and, to the best of our knowledge, an open problem. Instead, we proposed a surrogate problem that yields a lower bound for the most plausible identification formula and identifies a causal graph where this formula is valid. To do so, we use the result of Lemma 3 that shows when an identification functional is valid in a causal graph, it is also valid in all its edge-induced subgraphs.

**Problem 2**.: _Consider the problem of finding the edge-induced subgraph \(\mathcal{H}^{*}\) of \(\mathcal{G}\) with a maximum aggregate probability of its subgraphs, in which \(Q[Y]\) is identifiable. Formally,_

\[\mathcal{H}^{*}:=\operatorname*{arg\,max}_{\mathcal{G}_{s}\subseteq\mathcal{G},\mathcal{G}_{s}\in[\mathcal{G}]_{td(Q[Y])}}\sum_{\mathcal{G}\subseteq\mathcal{ G}_{s}}P(\mathcal{\hat{G}}).\] (3)In other words, we are looking for a causal graph \(\mathcal{H}^{*}\) with the maximum aggregate probability of its subgraphs among the graphs in \([\mathcal{G}]_{Id(Q[Y])}\), i.e., the graphs in which \(Q[Y]\) is identifiable. Running an identification algorithm on \(\mathcal{H}^{*}\) yields an identification formula for \(Q[Y]\), which is valid at least with the aggregate probability of the subgraphs of \(\mathcal{H}^{*}\). Therefore, Problem 2 is a surrogate for recovering the identification formula with the highest plausibility. In the sequel, for simplicity, we study Problems 1 and 2 under the following assumption. However, as proved in Appendix C, our results are valid in a more general setting where we allow only for perfect negative or positive correlations among the edges. An example of a perfect negative correlation between two edges is that both cannot exist simultaneously. Appendix C.1 discusses the significance of this generalization.

**Assumption 2.1**.: The edges of \(\mathcal{G}\) are mutually independent, i.e., the probability of a subgraph \(\mathcal{G}_{s}\) of \(\mathcal{G}\) is of the form (1).

**Remark 2.1**.: It is noteworthy that our results are not limited to causal queries of the form \(Q[Y]=P(Y|do(V^{\mathcal{G}}\setminus Y))\). They can be applied to general causal queries of the form \(P_{X}(Y)\) if the set \(\mathit{Anc}_{\mathcal{G}\setminus X}(Y)\) is known. This is because the causal query \(P_{X}(Y)\) can be expressed as \(\sum_{\mathit{Anc}_{\mathcal{G}\setminus X}(Y)\setminus Y}Q[\mathit{Anc}_{ \mathcal{G}\setminus X}(Y)]\), where \(\mathit{Anc}_{\mathcal{G}\setminus X}(Y)\) is the set of ancestors of \(Y\) in \(\mathcal{G}\) after removing the vertices of \(X\). Furthermore, \(P_{X}(Y)\) is identifiable in \(\mathcal{G}\) if and only if \(Q[\mathit{Anc}_{\mathcal{G}\setminus X}(Y)]\) is identifiable in \(\mathcal{G}\)[30; 26; 14]. Note that the assumption that \(\mathit{Anc}_{\mathcal{G}\setminus X}(Y)\) is known is not equivalent to precluding uncertainty on the directed edges (as in the case of fixing the edge probabilities to 0 or 1), but it rather imposes a perfect correlation type of constraint. Consider for instance the three graphs of Figure 2, where all of them share the same set \(\mathit{Anc}_{\mathcal{G}\setminus X}(Y)=\{z,t\}\). In fact, knowing this set forces a constraint of the type that if the edge \(z\to y\) does not exist, the path \(z\to t\to y\) must.

## 3 Reduction to edge ID problem and establishing complexity

We begin this section with the following lemma, to which we referred before. Thereafter, we discuss the hardness of the two problems considered in this work. For any causal query \(P_{X}(Y)\) and ADMG \(\mathcal{G}\), if \(\mathcal{F}\) is a valid identification formula for \(P_{X}(Y)\) in \(\mathcal{G}\) (Def. 2.2), then \(\mathcal{F}\) is a valid identification formula for \(P_{X}(Y)\) in any \(\mathcal{G}^{\prime}\subseteq\mathcal{G}\). All proofs are presented in Appendix A. In what follows, we first formally define the edge ID problem and then show the equivalence of Problems 1 and 2 to the edge ID problem under Assumption 2.1.

**Definition 3.1** (Edge ID problem).: For ADMG \(\mathcal{G}=(V^{\mathcal{G}},E^{\mathcal{G}}_{d},E^{\mathcal{G}}_{b})\), a set of non-negative edge weights \(W_{\mathcal{G}}=\{w_{e}\geq 0|e\in\mathcal{G}\}\), and a causal query \(Q[Y]\) for \(Y\subseteq V^{\mathcal{G}}\), the objective of the edge ID problem is to find the set of edges \(E^{*}\subseteq E^{\mathcal{G}}_{d}\cup E^{\mathcal{G}}_{b}\) with minimum aggregated weight (cost), such that \(Q[Y]\) is identifiable in the graph resulting from removing \(E^{*}\) from \(\mathcal{G}\). Formally,

\[E^{*}:=\underset{E\subseteq E^{\mathcal{G}}_{d}\cup E^{\mathcal{G}}_{b}}{\arg \min}\sum_{e\in E}w_{e},\quad\mathbf{s.t.}\quad\mathcal{G}^{\prime}=(V^{ \mathcal{G}},E^{\mathcal{G}}_{d}\setminus E,E^{\mathcal{G}}_{b}\setminus E) \in[\mathcal{G}]_{Id(Q[Y])}.\] (4)

That is, the cost of removing a set of edges from \(\mathcal{G}\) is the sum of the weights of each individual edge.

The following result unifies the two problems considered in this work by establishing their equivalence to the edge ID problem. This is done by transforming Problems 1 and 2 with multiplicative objectives into the edge ID problem that has an additive objective.

**Lemma 3.2**.: _Under Assumption 2.1, Problem 1 is equivalent to the edge ID problem with the edge weights chosen to be the log propensity ratios, i.e., \(w_{e}=\max\{0,\log(\frac{p_{e}}{1-p_{e}})\}\), \(\forall e\in\mathcal{G}\). Moreover, Problem 2 is equivalent to the edge ID problem with the choice of weights \(w_{e}=-\log(1-p_{e})\), \(\forall e\in\mathcal{G}\). That is, an instance of Problems 1 and 2 can be reduced to an instance of the edge ID problem in polynomial time, and vice versa._

As we mentioned earlier, the equivalence of these three problems can be established in more general settings than what is described under Assumption 2.1. We refer the interested reader to Appendix C for a discussion on one such setting. The following result shows no polynomial-time algorithm for solving these three problems exists unless \(\mathrm{P}=\mathrm{NP}\).

**Theorem 3.3**.: _The edge ID problem is NP-complete._

Theorem 3.3 is established through a reduction from another NP-complete problem, namely the _minimum-cost intervention problem_ (MCIP) [2; 3], which we shall discuss in Section 4.3. Theorem3.3 is a key result which shows the hardness of recovering the most plausible graph in which a specified causal effect of interest is identifiable.

**Corollary 3.4**.: _Problems 1 and 2 are NP-complete under Assumption 2.1._

It is noteworthy that the size of the problem depends on the number of vertices of \(\mathcal{G}\), i.e., \(|V^{\mathcal{G}}|\), and the number of edges of \(\mathcal{G}\) with finite weight, i.e., \(|E^{\mathcal{G}}|=|E^{\mathcal{G}}_{d}|+|E^{\mathcal{G}}_{b}|\). Since the ID algorithm (function ID of [26]) runs in time \(\mathcal{O}(|V^{\mathcal{G}}|^{2})\), the brute-force algorithm that tests the identifiability of \(Q[Y]\) in every edge-induced subgraph of \(\mathcal{G}\) and chooses the one with the minimum weight of deleted edges runs in time \(\mathcal{O}(2^{|E^{\mathcal{G}}|}|V^{\mathcal{G}}|^{2})\). In the next Section, we present various algorithmic approaches for solving or approximating the solutions to these problems.

## 4 Algorithmic approaches

We first present a recursive approach for solving the edge ID problem in Section 4.1, described in Algorithm 1. Since the problem itself is NP-complete, Algorithm 1 runs in exponential time in the worst case. In Section 4.2, we present heuristic approximations of the edge ID problem, which runs in cubic time in the worst case. These heuristics can also be used as a pre-process to reduce the runtime of Alg. 1 by providing an upper bound that can be fed into Alg. 1 to prune the search space. Finally, in Section 4.3, we present a reduction of edge ID to yet another NP-complete problem, namely minimum-cost intervention problem [2], which allows us to use the algorithms designed for that problem to solve edge ID. Our simulations in Section 5 evaluate these approaches against each other.

### Recursive exact algorithm

This approach is described in Algorithm 1. The inputs to the algorithm are an ADMG \(\mathcal{G}\) along with edge weights \(W_{\mathcal{G}}\), a set of vertices \(Y\) corresponding to the causal query \(Q[Y]\), an upper bound \(\omega^{ub}\) on the aggregate weight (cost) of the optimal solution, and a threshold \(\omega^{th}\), an upper bound on the acceptable cost of a solution. The closer \(\omega^{ub}\) is to the optimal cost, the quicker Algorithm 1 will find the solution. If no upper bound is known, the algorithm can be initiated with \(\omega^{ub}=\infty\). However, we shall discuss a few approaches to find a good upper bound \(\omega^{ub}\) in the following Section. Note that when \(\omega^{th}=0\), Algorithm 1 will output the optimal solution. Otherwise, as soon as a feasible solution with weight less than \(\omega^{th}\) is found, the algorithm terminates (line 13).

The algorithm begins with calling subroutine **MH** in line 2, which constructs the maximal hedge for \(Q[Y]\), denoted by \(\mathcal{H}\). We discuss this subroutine in detail in Appendix B. Throughout the rest of the algorithm, we only consider the edges in \(\mathcal{H}\), as the other edges do not alter the identifiability. If there is no hedge formed for \(Q[Y]\), i.e., \(\mathcal{H}=\mathcal{G}[Y]\), there is no need to remove any edges from \(\mathcal{G}\) and the effect is already identified. Otherwise, we remove the edge with the lowest weight (\(e\)) from \(\mathcal{H}\) and recursively call the algorithm to find the solution after removing the edge \(e\), unless the weight of \(e\) is already higher than the upper bound \(\omega^{ub}\), which means no feasible solutions exist for the provided upper bound. Whenever a feasible solution is found, the upper bound \(\omega^{ub}\) is updated to the lowest weight among all the solutions weights discovered so far. This, in turn, helps the algorithm prune the exponential search space during the next iterations to reduce the runtime. As soon as a solution with a weight less than the acceptable threshold, i.e., \(\omega^{th}\), is found, the algorithm returns the solution. Otherwise, \(w_{e}\) is updated to infinity so that it never gets removed (line 14). This is due to the fact that we have already explored all the solutions involving \(e\).

Figure 2: Three different graphs that share the same set \(Anc_{\mathcal{G}}(\{y\})=\{z,t\}\).

### Heuristic algorithms

In this section, we present two heuristic algorithms for approximating the solution to the edge ID problem. These algorithms can also be used to efficiently find the upper bound \(\omega^{ub}\), which could be fed as an input to Algorithm 1.

```
1:functionHEID(\(\mathcal{G},Y,W_{\mathcal{G}}\))
2:\(\mathcal{G}^{\prime}\leftarrow\textbf{MH}(\mathcal{G},Y)\),
3:\(Z\leftarrow\{z\in V^{\mathcal{G}^{\prime}}|zy\in Y:\{z,y\}\in E_{b}^{ \mathcal{G}^{\prime}}\}\setminus Y\),
4:\(\mathcal{H}\leftarrow\) induced subgraph of \(\mathcal{G}^{\prime}\) on its directed edges.
5:\(W_{\mathcal{H}}\leftarrow\{w_{e}\in W_{\mathcal{G}}|e\in\mathcal{H}\}\), \(V^{\mathcal{H}}\leftarrow V^{\mathcal{H}}\cup\{y^{*},z^{*}\}\)
6:for\(z\in Z\)do\(E^{\mathcal{H}}\leftarrow E^{\mathcal{H}}\cup(z^{*},z)\), \(W_{\mathcal{H}}\gets W_{\mathcal{H}}\cup\{w_{(z^{*},z)}=\sum_{y}w_{\{z,y\}}\}\)
7:for\(y\in Y\)do\(E^{\mathcal{H}}\leftarrow E^{\mathcal{H}}\cup(y,y^{*})\), \(W_{\mathcal{H}}\gets W_{\mathcal{H}}\cup\{w_{(y,y^{*})}=\infty\}\)
8:\(E\gets MinCut(\mathcal{H},W_{\mathcal{H}},z^{*},y^{*})\)
9:return\((E,\sum_{e\in E}w_{e})\) ```

**Algorithm 2** Heuristic algorithm for edge ID.

Let \(Z=\{z\in V^{\mathcal{G}}|\;\exists\;y\in Y:\{z,y\}\in E_{b}^{\mathcal{G}}\} \setminus Y\) denote the set of vertices that have at least one common bidirected edge with a vertex in \(Y\). Any hedge formed for \(Q[Y]\) contains at least one vertex of \(Z\). As a result, to eliminate all the hedges formed for \(Q[Y]\), it suffices to ensure that none of the vertices in \(Z\) appear in such a hedge. To this end, for any \(z\in Z\), it suffices to either remove all the bidirected edges between \(z\) and \(Y\) or eliminate all the directed paths from \(z\) to \(Y\). The problem of eliminating all directed paths from \(Z\) to \(Y\) can be cast as a minimum cut problem between \(Z\) and \(Y\) in the edge-induced subgraph of \(\mathcal{G}\) over its directed edges. To add the possibility of removing the bidirected edges between \(Z\) and \(Y\), we add an auxiliary vertex \(z^{*}\) to the graph and draw a directed edge from \(z^{*}\) to every \(z\in Z\) with weight \(w=\sum_{y\in Y}w_{\{z,y\}}\), i.e., the sum of the weights of all bidirected edges between \(z\) and \(Y\). Note that \(z\) can have bidirected edges to multiple vertices in \(Y\). We then solve the minimum cut problem for \(z^{*}\) and \(Y\). If an edge between \(z^{*}\) and \(z\in Z\) is included in the solution to this minimum cut problem, it is mapped to remove all the bidirected edges between \(z\) and \(Y\) in the main problem. Note that we can run the algorithm on the maximal hedge formed for \(Q[Y]\) in \(\mathcal{G}\) rather than \(\mathcal{G}\) itself. This heuristic is presented as Algorithm 2.

```
1:functionHEID(\(\mathcal{G},Y,W_{\mathcal{G}}\))
2:\(\mathcal{G}^{\prime}\leftarrow\textbf{MH}(\mathcal{G},Y)\),
3:\(Z\leftarrow\{z\in V^{\mathcal{G}^{\prime}}|zy\in Y:\{z,y\}\in E_{b}^{ \mathcal{G}^{\prime}}\}\setminus Y\),
4:\(\mathcal{H}\leftarrow\) induced subgraph of \(\mathcal{G}^{\prime}\) on its directed edges.
5:\(W_{\mathcal{H}}\leftarrow\{w_{e}\in W_{\mathcal{G}}|e\in\mathcal{H}\}\), \(V^{\mathcal{H}}\leftarrow\mathcal{H}\cup\{y^{*},z^{*}\}\)
6:for\(z\in Z\)do\(E^{\mathcal{H}}\leftarrow E^{\mathcal{H}}\cup(z^{*},z)\), \(W_{\mathcal{H}}\gets W_{\mathcal{H}}\cup\{w_{(z^{*},z)}=\sum_{y}w_{\{z,y\}}\}\)
7:for\(y\in Y\)do\(E^{\mathcal{H}}\leftarrow E^{\mathcal{H}}\cup(y,y^{*})\), \(W_{\mathcal{H}}\gets W_{\mathcal{H}}\cup\{w_{(y,y^{*})}=\infty\}\)
8:\(E\gets MinCut(\mathcal{H},W_{\mathcal{H}},z^{*},y^{*})\)
9:return\((E,\sum_{e\in E}w_{e})\) ```

**Algorithm 3** Heuristic algorithm for edge ID.

An analogous approach which goes through solving an undirected minimum cut on the edge-induced subgraph of \(\mathcal{G}\) over its bidirected edges is presented in Algorithm 4 in Appendix D. As mentioned earlier, these algorithms can be used either as standalone algorithms to approximate the solution to the edge ID problem, or as a pre-processing step to find an upper bound \(\omega^{ub}\) for Algorithm 1.

Complexity and performance.The aforementioned heuristic algorithms can be broken down into two primary components: constructing an instance of the minimum-cut problem, and solving the latter. Both algorithms follow similar steps for constructing the minimum-cut problem. For reference,Alg. 2 starts with identifying the maximal hedge formed for \(Q[Y]\), which is cubic-time in the worst case [2], followed by forming a directed graph \(\mathcal{H}\) over the vertices of the maximal hedge. Forming this graph, which inherits its vertices and directed edges from \(\mathcal{G}\), is linear in time. Lastly, there exist several minimum-cut-maximum-flow algorithms that exhibit cubic-time performance. Therefore, our heuristic algorithms have a cubic runtime in the worst case. On the other hand, given the hardness result we provided in the previous section, and in light of the fact that there is a polynomial-time reduction from the minimum hitting set problem to MCIP [3], the edge ID problem is NP-hard to approximate within a factor better than logarithmic [11]. Since our heuristic algorithms run in polynomial time, they are expected to be suboptimal by at least a logarithmic factor in the worst case. However, as we shall see in our simulations, both algorithms achieve near-optimal results on random graphs.

### Alternative approach: reduction to MCIP

As an alternative approach to the algorithms discussed so far, we present a reduction of the edge ID problem to another NP-complete problem, i.e., the minimum-cost intervention problem (_MCIP_) introduced in [2]. This reduction allows us to exploit algorithms designed for MCIP to solve our problems. The formal definition of MCIP is as follows.

**Definition 4.1** (Mcip).: Suppose \(\mathcal{G}=(V^{\mathcal{G}},E_{d}^{\mathcal{G}},E_{b}^{\mathcal{G}})\) is an ADMG, \(C:V^{\mathcal{G}}\rightarrow\mathbbm{R}^{\geq 0}\) is a cost function mapping each vertex of \(\mathcal{G}\) to a non-negative cost, and \(Y\subseteq V^{\mathcal{G}}\). The objective of the minimum-cost intervention problem for identifying the causal effect \(Q[Y]\) is to find the subset \(A\subseteq V^{\mathcal{G}}\) with the minimum aggregate cost such that \(Q[Y]\) is identifiable after intervening on the set \(A\).

The reduction from edge ID to MCIP is based on a transformation from ADMG \(\mathcal{G}\) to another ADMG \(\mathcal{H}\), where each edge in \(\mathcal{G}\) is represented by a vertex in \(\mathcal{H}\). This transformation is based on the causal query \(Q[Y]\), and it maps the identifiability of \(Q[Y]\) in \(\mathcal{G}\) to the identifiability of \(Q[Y^{mcip}]\) in \(\mathcal{H}\), where \(Y^{mcip}\) is a set of vertices in \(\mathcal{H}\). This transformation satisfies the following property; removing a set of edges \(E^{*}\) in \(\mathcal{G}\) makes \(Q[Y]\) identifiable if and only if intervening on the corresponding vertices of \(E^{*}\) in \(\mathcal{H}\) makes \(Q[Y^{mcip}]\) identifiable. More precisely, after this transformation, solving the edge ID problem for \(Q[Y]\) in \(\mathcal{G}\) is equivalent to solving MCIP for \(Q[Y^{mcip}]\) in \(\mathcal{H}\). The complete details of this transformation can be found in Appendix A.2. An example of this reduction is shown in Figure 3, where \(Q[\{y_{1},y_{2}\}]\) in \(\mathcal{G}\) (Figure 2(a)) is mapped to \(Q[\{y_{1},y_{2},y_{2}^{12}\}]\) in \(\mathcal{H}\) (Figure 2(b)), where \(\{y_{1},y_{2},y_{2}^{12}\}\) is a district, and the set of all vertices of \(\mathcal{H}\) forms a hedge for it. The vertices of \(\mathcal{H}\) corresponding to each edge in \(\mathcal{G}\) are indicated with the same color and have the same weight (cost). We assign infinity cost to them to avoid intervening on the remaining vertices in \(\mathcal{H}\). It is straightforward to see that the solution to the edge ID problem in \(\mathcal{G}\) with the query \(Q[Y=\{y_{1},y_{2}\}]\) would be to remove the edge with the lowest weight. This is because after removing any edge in \(\mathcal{G}\), no hedge remains for \(Q[Y]\). Similarly, in \(\mathcal{H}\), the solution to MCIP with the query \(Q[Y^{mcip}=\{y_{1},y_{2},y_{2}^{12}\}]\) is to intervene on the vertex with the lowest cost among \(Z=\{z_{11}^{d},x_{21}^{d},x_{12}^{b},y_{12}^{b},z_{22}^{b}\}\). This is because after intervening on any vertex in \(Z\), no hedge remains for \(Q[Y^{mcip}]\). The following result establishes the link between the edge ID problem in \(\mathcal{G}\) and MCIP in \(\mathcal{H}\).

Figure 3: Reduction from edge ID to MCIP.

**Proposition 4.2**.: _There exists a polynomial-time reduction from edge ID to MCIP and vice versa._

In particular, the time complexity of the transformation from edge ID to MCIP scales linearly in size of \(\mathcal{G}\), and is cubic in \(|Y|\). More precisely, the time complexity is \(\mathcal{O}(|V^{\mathcal{G}}|+|E_{d}^{\mathcal{G}}|+|E_{b}^{\mathcal{G}}|+|Y|^{ 3})\). For further details, refer to the construction in Appendix A.2.

## 5 Experiments

We evaluate the proposed heuristic algorithms 2 (HEID-1) and 4 (HEID-2), as well as the exact algorithm 1 (EDGEID), where the upper-bound \(\omega^{ub}\) for EDGEID is set to be the minimum cost found by HEID-1 or -2. Furthermore, given the reduction of the edge ID problem to the MCIP problem described in Section 4.3, we also evaluate the two approximation and one exact algorithms from [2] (MCIP-H1, MCIP-H2, and MCIP-exact, respectively). Experimental results are provided for Problem 1, and analogous results for Problem 1 are provided in Appendix E.3. All experiments were carried out on an Intel i9-9900K CPU running at 3.6GHz4.

Footnote 4: To replicate our findings, our Python implementations are accessible via https://github.com/SinAkkbarii/Causal-Effect-Identification-in-Uncertain-Causal-Networks and https://github.com/matthewvowels1/NeurIPS_testbed_ADMGs.

**Simulations:** The algorithms are evaluated for graphs with between 5 and 250 vertices. For a given number of vertices, we uniformly sample 50 ADMG structures from a library of graphs which are non-isomorphic to each other. Edges for each of these 100 graphs are sampled with probability of \(\log(n)/n\), where \(n\) is the number of (observable) vertices, to impose sparsity (thus pragmatically reducing the search space). For each graph, we sample directed and bidirected edge probabilities \(p_{e}\) uniformly between 0.51 and \(1.0\lx@math@degree\). The problem is then converted into edge ID according to Lemma 3.2. The vertices in the graphs are topologically sorted and the outcome \(Y\) is selected to be the last vertex in the topological ordering. We then check whether a solution exists in principle by removing all finite cost edges and checking for identifiability. If not, a new graph is sampled to avoid evaluating the algorithms on graphs with no solution. For each of these 50 probabilistic ADMGs, we run the algorithms and record the resulting runtime and the associated cost of the solution. If the runtime exceeds 3 minutes, we abort and log that the algorithm has failed to find a solution.

Results are presented in Figure 4. Runtimes and costs are shown for the subset of graphs for which all algorithms found a solution (to facilitate comparison). Runtimes for each algorithm are shown in Fig. 3(a), where it can be seen that our proposed HEID-1 and HEID-2 heuristic algorithms have negligible runtime, followed by the MCIP variants. Interestingly, the exact algorithm EDGEID outperformed the MCIP algorithms on larger graphs, for which the transformation time from the edge ID problem to the MCIP increases with the size of the graph. In contrast, EDGEID had large runtime variance which depended heavily on the specifics of the graph under evaluation, particularly for graphs with fewer vertices. The costs for each graph are shown in Fig. 3(b), and here we see, as expected, the lowest cost is achieved by the two exact algorithms, EDGEID and MCIP-exact, followed closely by the heuristic algorithms. Figure 3(c) shows the fraction of evaluations for which the runtime exceeded 3 minutes (applicable to the exact algorithms). In general, and owing to the sparsity

Figure 4: Experimental results for runtime, solution costs, fraction of graphs for which no solution was found, and fraction of graphs for which runtime limit of 3 minutes was exceeded. Error bars for runtime and cost graphs indicate 5th and 95th percentiles. Best viewed in color.

penalty in our graph generation mechanism, the cost of identified solutions falls with the number of vertices. However, among the exact algorithms, EDGEID, exceeds the 3-minute runtime more often than the MCIP-Exact, regardless of the number of vertices and despite the fact that EDGEID is quicker at finding a solution when it does so. Overall, HEID-1 was the most consistent in finding a solution, having a short runtime, and achieving a close to optimal cost.

Real-world graphs: We also apply the algorithms to four real-world datasets. The first 'Psych' (22 nodes & 70 directed edges) concerns the putative structure from a causal discovery algorithm Structural Agnostic Model [18] using data collected as part of the Health and Relationships Project [32]. The other three 'Barley' (48 nodes & 84 directed edges), 'Water' (32 nodes & 66 directed edges), and 'Alarm' (37 nodes & 46 directed edges) come from the bnlearn python package [28]. For all four graphs, and as with the simulations described above, we introduce bidirected edges with a sparsity constraint of \(\log(n)/n\), and simulate expert domain knowledge by randomly assigning directed and bidirected edge probabilities between 0.51 and 1. The outcome \(Y\) is selected to be the last vertex in the topological ordering. For these results, we provide the runtime (limited to 500 seconds) and cost, as well as the ratio of graph plausibility before and after selecting a subgraph in which the effect is identifiable \(P(\hat{\mathcal{G}}^{*})/P(\mathcal{G})\). This ratio is 1.0 if the effect is identifiable in the original graph and decreases according to the plausibility of an identified subgraph.

Results are shown in Table 1. In cases where MCIP-exact and/or EDGEID did not exceed the runtime limit, it can be seen that HEID-2 and MCIP-H2 achieved equivalent to optimal cost and ratio. Runtimes for MCIP variants exceeded the HEID variants owing to the required transformation. EDGEID timed out on all but the Alarm structure, whereas MCIP-exact only timed out on the Psych structure, indicating that the MCIP-exact is more consistent (this also corroborates Figure 4c).

## 6 Concluding remarks

Researchers in causal inference are often faced with graphs for which the effect of interest is not identifiable. It is common to identify a target effect by assuming ignorability. A less drastic and more reasonable approach would be to relax this assumption by identifying the most plausible subgraph, given uncertainty about the structure as we suggested in this work. We presented a number of algorithms for finding the most probable/plausible probabilistic ADMG in which the target causal effect is identifiable. We provided an analysis of the complexity of the problem, and an experimental comparison of runtimes, solution costs, and failure rates. We noted that our heuristic algorithms, Alg. 2 and Alg. 4 performed remarkably well across all metrics. In terms of limitations, we made the assumption that the edges in \(\mathcal{G}\) are mutually independent (Assumption 2.1). Future work should explore the case where this assumption does not hold. In this work, we studied the problem of point identification of the target effect. We envision that a similar methodology can be applied to partial identification. Finally, it is worth noting that the external validity of the derived subgraph (i.e., whether or not the subgraph is correctly specified with respect to the corresponding real-world process) is not guaranteed. As such, practitioners using such approaches are encouraged to do so with caution, particularly for research involving human participants.

## Acknowledgements

This research was in part supported by the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40_180545 and Swiss SNF project 200021_204355 /1.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Algorithm**} & \multicolumn{2}{c}{**Psych**} & \multicolumn{3}{c}{**Barley**} & \multicolumn{3}{c}{**Alarm**} & \multicolumn{3}{c}{**Water**} \\  & Time & Cost & Ratio & Time & Cost & Ratio & Time & Cost & Ratio & Time & Cost & Ratio \\ \hline HEID-1 & 0.0019 & 2.648 & 0.07 & 0.0026 & 0.081 & 0.92 & 0.0004 & 0.0 & 1.0 & 0.0019 & 1.02 & 0.36 \\ HEID-2 & 0.0019 & 1.806 & 0.16 & 0.0026 & 0.081 & 0.92 & 0.0003 & 0.0 & 1.0 & 0.0017 & 0.42 & 0.66 \\ MCIP-H1 & 0.0136 & 2.648 & 0.07 & 0.0140 & 0.081 & 0.92 & 0.0027 & 0.0 & 1.0 & 0.0124 & 1.02 & 0.36 \\ MCIP-H2 & 0.0133 & 1.806 & 0.16 & 0.0131 & 0.081 & 0.92 & 0.0029 & 0.0 & 1.0 & 0.0113 & 0.42 & 0.66 \\ MCIP-exact & - & - & - & 0.0099 & 0.081 & 0.92 & 0.0028 & 0.0 & 1.0 & 0.0221 & 0.42 & 0.66 \\ EDGEID & - & - & - & - & - & - & - & 0.0005 & 0.0 & 1.0 & - & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: Time (seconds), cost, and ratio \(P(\hat{\mathcal{G}}^{*})/P(\mathcal{G})\) for seven algorithms over four real-world datasets. A dash - indicates the maximum runtime (500 seconds) exceeded.

## References

* [1] Amir Mohammad Abouei, Ehsan Mokhtarian, and Negar Kiyavash. s-id: Causal effect identification in a sub-population. _AAAI_, 2024.
* [2] Sina Akbari, Jalal Etesami, and Negar Kiyavash. Minimum cost intervention design for causal effect identification. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 258-289. PMLR, 2022.
* [3] Sina Akbari, Jalal Etesami, and Negar Kiyavash. Experimental design for causal effect identification. _arXiv:2205.02232_, 2023.
* [4] Sina Akbari, Luca Ganassali, and Negar Kiyavash. Causal discovery via monotone triangular transport maps. In _NeurIPS 2023 Workshop Optimal Transport and Machine Learning_, 2023.
* [5] Sina Akbari, Ehsan Mokhtarian, AmirEmad Ghassami, and Negar Kiyavash. Recursive causal structure learning in the presence of latent variables and selection bias. _Advances in Neural Information Processing Systems_, 34:10119-10130, 2021.
* [6] Joshua Angrist and Guido Imbens. Identification and estimation of local average treatment effects. _Econometrica_, 62(2):467-475, 1994.
* [7] Ricardo Baptista, Youssef Marzouk, Rebecca E. Morrison, and Olivier Zahm. Learning non-gaussian graphical models via hessian scores and triangular transport, 2023.
* [8] Elias Bareinboim, Juan D. Correa, Duligur Ibeling, and Thomas Icard. On Pearl's hierarchy and the foundations of causal inference. _ACM Special Reports_, 2020.
* [9] Diego Colombo, Marloes H. Maathuis, Markus Kalisch, and Thomas S. Richardson. Learning high-dimensional directed acyclic graphs with latent and selection variables. _The Annals of Statistics_, pages 294-321, 2012.
* [10] Angus Deaton and Nancy Cartwright. Understanding and misundertstanding randomized controlled trials. _Social Science and Medicine_, 210:2-21, 2018.
* [11] Uriel Feige. A threshold of ln n for approximating set cover. _Journal of the ACM (JACM)_, 45(4):634-652, 1998.
* [12] Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical models. _Frontiers in Genetics_, 10, 2019.
* [13] Yimin Huang and Marco Valtorta. Pearl's calculus of intervention is complete. _Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI)_, 2006.
* [14] Amin Jaber, Jiji Zhang, and Elias Bareinboim. Causal identification under Markov equivalence: Completeness results. In _International Conference on Machine Learning_, pages 2981-2989. PMLR, 2019.
* [15] Fateme Jamshidi, Sina Akbari, and Negar Kiyavash. Causal imitability under context-specific independence relations. _arXiv preprint arXiv:2306.00585_, 2023.
* [16] Fateme Jamshidi, Jalal Etesami, and Negar Kiyavash. Confounded budgeted causal bandits. _3rd Conference on Causal Learning and Reasoning_, 2024.
* [17] Fateme Jamshidi, Luca Ganassali, and Negar Kiyavash. On sample complexity of conditional independence testing with von mises estimator with application to causal discovery. _arXiv preprint arXiv:2310.13553_, 2023.
* [18] Diviyan Kalainathan, Olivier Goudet, Isabelle Guyon, David Lopez-Paz, and Michele Sebag. Structural agnostic modeling: Adversarial learning of causal graphs. _arXiv:1803.04929v3_, 2020.
* [19] Richard M. Karp. Reducibility among combinatorial problems. In _Complexity of computer computations_, pages 85-103. Springer, 1972.

* [20] Yaroslav Kivva, Ehsan Mokhtarian, Jalal Etesami, and Negar Kiyavash. Revisiting the general identifiability problem. In _Uncertainty in Artificial Intelligence_, pages 1022-1030. PMLR, 2022.
* [21] Sanghack Lee, Juan D Correa, and Elias Bareinboim. General identifiability with arbitrary surrogate experiments. In _Uncertainty in artificial intelligence_, pages 389-398. PMLR, 2020.
* [22] Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. _Advances in neural information processing systems_, 12, 1999.
* [23] Judea Pearl. Aspects of graphical models connceted with causality. _Proceedings of the 49th Session of the International Statistical Institute_, pages 399-401, 1993.
* [24] Judea Pearl. _Causality_. Cambridge University Press, Cambridge, 2009.
* [25] Rajen D. Shah and Jonas Peters. The hardness of conditional independence testing and the generalised covariance measure. _The Annals of Statistics_, 48(3), 2020.
* [26] Ilya Shpitser and Judea Pearl. Identification of joint interventional distributions in recursive semi-Markovian causal models. In _Proceedings of the National Conference on Artificial Intelligence_, volume 21, page 1219. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.
* [27] Peter Spirtes, Clark N. Glymour, and D. Scheines, Richardd Heckerman. _Causation, prediction, and search_. MIT press, 2000.
* Library for Bayesian network learning and inference. _Python Library_, 2020.
* [29] Jin Tian and Judea Pearl. A general identification condition for causal effects. _AAAI_, 2002.
* [30] Jin Tian and Judea Pearl. On the testable implications of causal models with hidden variables. In _Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence_, pages 519-527, 2002.
* [31] Santtu Tikka, Antti Hyttinen, and Juha Karvanen. Identifying causal effects via context-specific independence relations. _Advances in neural information processing systems_, 32, 2019.
* [32] Debra Umberson. Health and relationships project. _Inter-university consortium for political and social research_, 2014-2015.
* Causal Inference for Observational and Experimental Data_. Springer International, New York, 2011.

**Appendix**

The appendices are organized as follows. Formal proofs of the results stated in the main text are presented in Section A. In Section B, we describe the algorithm to recover the maximal hedge formed for a certain query (Def. 2.5), which is used as a subroutine of Algorithm 1. A generalization of Assumption 2.1 is discussed in Section C. Section D provides further details of the heuristic algorithms discussed in the main text. Further evaluations and experimental conditions for our proposed algorithms are presented in Section E.

## Appendix A Formal proofs

We begin with presenting the proofs of Lemma 3 and Lemma 3. Proofs of Theorem 3.3 and Proposition 4 appear at the end of Sections A.1 and A.2, respectively.

For any causal query \(P_{X}(Y)\) and ADMG \(\mathcal{G}\), if \(\mathcal{F}\) is a valid identification formula for \(P_{X}(Y)\) in \(\mathcal{G}\) (Def. 2.2), then \(\mathcal{F}\) is a valid identification formula for \(P_{X}(Y)\) in any \(\mathcal{G}^{\prime}\subseteq\mathcal{G}\).

Proof.: Let \(\mathcal{H}\subseteq\mathcal{G}\) be an arbitrary edge-induced subgraph of \(\mathcal{G}\). Let \(\mathcal{F}\) be an identification formula for \(P_{X}(Y)\) in \(\mathcal{G}\), i.e., for any model \(M\) that induces \(\mathcal{G}\),

\[P_{X}^{M}(Y)=\mathcal{F}(P^{M}(V^{\mathcal{G}})).\] (5)

By definition, \(P_{X}(Y)\) is identifiable in \(\mathcal{G}\). As a result, there exists and identification formula such as \(\mathcal{F}^{\prime}\) that can be derived for \(P_{X}(Y)\) in \(\mathcal{G}\), using a sequence of do calculus rules and basic probability manipulations. Note that this means for any model \(M\) that induces \(\mathcal{G}\),

\[P_{X}^{M}(Y)=\mathcal{F}^{\prime}(P^{M}(V^{\mathcal{G}})).\] (6)

Note that an immediate corollary of Equations 5 and 6 is that for any model \(M\) that induces \(\mathcal{G}\),

\[\mathcal{F}(P^{M}(V^{\mathcal{G}}))=\mathcal{F}^{\prime}(P^{M}(V^{\mathcal{G} })).\] (7)

Now, we first show that this sequence of actions (combination of do calculus rules and probability manipulations) is valid in \(\mathcal{H}\). Note that the basic probability manipulations are graph-independent. It only suffices to show that any applied do calculus rule w.r.t. \(\mathcal{G}\) can also be applied w.r.t. \(\mathcal{H}\). The validity conditions of all three do calculus rules are based on certain d-separations. As a result, it suffices to show that if a d-separation relation is valid in \(\mathcal{G}\), it is also valid in \(\mathcal{H}\). To do so, it suffices to show that if all paths between \(Z_{1}\) and \(Z_{2}\) are blocked in \(\mathcal{G}\) given \(W\), they are blocked in \(\mathcal{H}\) too, for arbitrary disjoint sets of vertices \(Z_{1},Z_{2},W\subseteq V^{\mathcal{G}}\). Take an arbitrary path, \(p\), between \(Z_{1}\) and \(Z_{2}\) in \(\mathcal{H}\). Since \(\mathcal{H}\subseteq\mathcal{G}\), this path exists in \(\mathcal{G}\). Since \(Z_{1}\) and \(Z_{2}\) are d-separated given \(W\) in \(\mathcal{G}\), the path \(p\) is blocked by \(W\). As a result, any path between \(Z_{1}\) and \(Z_{2}\) in \(\mathcal{H}\) is blocked by \(W\). Therefore, any do-calculus rule applied in \(\mathcal{G}\), can also be applied in \(\mathcal{H}\). Hence, \(\mathcal{F}^{\prime}\) is a valid identification formula for \(P_{X}(Y)\). That is, for any model \(M\) that induces \(\mathcal{H}\),

\[P_{X}^{M}(Y)=\mathcal{F}^{\prime}(P^{M}(V^{\mathcal{H}})).\] (8)

Now note that any model \(M\) that induces \(\mathcal{H}\), i.e., is compatible with \(\mathcal{H}\), is also compatible with \(\mathcal{G}\). Also, \(V^{\mathcal{G}}=V^{\mathcal{H}}\). As a result, from Equations 7 and 8, we know that for any model \(M\) that induces \(\mathcal{H}\),

\[P_{X}^{M}(Y)=\mathcal{F}(P^{M}(V^{\mathcal{H}})).\]

By definition, \(\mathcal{F}\) is a valid identification formula for \(P_{X}(Y)\) in \(\mathcal{H}\)

\begin{table}
\begin{tabular}{c|c} \hline
**Symbol** & **Description** \\ \hline \(V^{\mathcal{G}}\) & Vertices of \(\mathcal{G}\) \\ \(E_{b}^{\mathcal{G}}\) & The set of bidirected edges of \(\mathcal{G}\) \\ \(E_{d}^{\mathcal{G}}\) & The set of directed edges of \(\mathcal{G}\) \\ \(Ancg_{c}(X)\) & Ancestors of \(X\) in \(\mathcal{G}\) \\ \(\mathcal{M}(\mathcal{G})\) & The set of the all compatible models with \(\mathcal{G}\) \\ \(p_{e}\) & Probability of edge \(e\) \\ \(w_{e}\) & Weight of edge \(e\) \\ \(P_{X}(Y)\) & Causal effect of \(X\) on \(Y\) \\ \hline \end{tabular}
\end{table}
Table 2: Table of notations

**Lemma 3.2**.: _Under Assumption 2.1, Problem 1 is equivalent to the edge ID problem with the edge weights chosen to be the log propensity ratios, i.e., \(w_{e}=\max\{0,\log(\frac{p_{e}}{1-p_{e}})\}\), \(\forall e\in\mathcal{G}\). Moreover, Problem 2 is equivalent to the edge ID problem with the choice of weights \(w_{e}=-\log(1-p_{e})\), \(\forall e\in\mathcal{G}\). That is, an instance of Problems 1 and 2 can be reduced to an instance of the edge ID problem in polynomial time, and vice versa._

Proof.: _Problem 1._ First consider an arbitrary graph \(\mathcal{G}_{1}\in[\mathcal{G}]_{Id(Q[Y])}\) such that \(\mathcal{G}_{1}\) has an edge \(e\) with \(p_{e}<1/2\). Let \(G_{2}\) denote the graph \(G_{1}\) after removing \(e\). Lemma 3 implies that \(\mathcal{G}_{2}\in[\mathcal{G}]_{Id(Q[Y])}\). According to Equation 1, we have \(P(\mathcal{G}_{2})=\frac{1-p_{e}}{p_{e}}P(\mathcal{G}_{1})>P(\mathcal{G}_{1})\) (since \(p_{e}<1/2\)). As a result, the solution \(\mathcal{G}^{*}\) to Problem 1 (Eq. 2) has no edges with probability less than \(1/2\). We can therefore rewrite Problem 1 as:

\[\mathcal{G}^{*}:=\operatorname*{arg\,max}_{\begin{subarray}{c}\mathcal{G}_{ s}\subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{Id(Q[Y])}\end{subarray}}P(\mathcal{G}_{s})= \operatorname*{arg\,max}_{\begin{subarray}{c}\mathcal{G}_{s}\subseteq \mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{Id(Q[Y])}\end{subarray}}P(\mathcal{G}_{s}) \quad\mathbf{s.t.}\quad\forall e\in\mathcal{G}_{s}:\;p_{e}\geq\frac{1}{2}.\]

Or equivalently, we can always assume that we start with a graph \(\mathcal{G}\) that has no edges with probability less than \(1/2\), as otherwise we can remove all of those edges and the problem does not change. This indeed is equivalent to choosing weight (cost) 0 for those edges in the equivalent edge ID problem. Now assuming that the edges have probability at least \(1/2\),

\[\mathcal{G}^{*} =\operatorname*{arg\,max}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{Id(Q[Y])}\end{subarray}}P(\mathcal{G}_{s})\] \[=\operatorname*{arg\,max}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{Id(Q[Y])}\end{subarray}}\log(P(\mathcal{G}_{s}))\] \[=\operatorname*{arg\,max}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{Id(Q[Y])}\end{subarray}}\log(\prod_{e\in \mathcal{G}_{s}}p_{e}\prod_{e\notin\mathcal{G}_{s}}(1-p_{e}))\] \[=\operatorname*{arg\,max}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{Id(Q[Y])}\end{subarray}}\sum_{e\in\mathcal{G}_ {s}}\log(p_{e})+\sum_{e\notin\mathcal{G}_{s}}\log(1-p_{e}))\] \[=\operatorname*{arg\,max}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{Id(Q[Y])}\end{subarray}}\sum_{e\in\mathcal{G} _{s}}\log(p_{e})+\sum_{e\notin\mathcal{G}_{s}}\log(1-p_{e}))+\sum_{e\in \mathcal{G}_{s}}\log(1-p_{e}))-\sum_{e\in\mathcal{G}_{s}}\log(1-p_{e}))\]

Since \(\sum_{e\notin\mathcal{G}_{s}}\log(1-p_{e}))+\sum_{e\in\mathcal{G}_{s}}\log(1- p_{e}))\) is a constant value that does not depend on \(\mathcal{G}_{s}\), it can be ignored in the maximization and we have:

\[\mathcal{G}^{*}=\operatorname*{arg\,max}_{\begin{subarray}{c} \mathcal{G}_{s}\subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{Id(Q[Y])}\end{subarray}}\sum_{e\in\mathcal{G} _{s}}\log(p_{e})-\sum_{e\in\mathcal{G}_{s}}\log(1-p_{e}))\] \[=\operatorname*{arg\,min}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{Id(Q[Y])}\end{subarray}}\sum_{e\in\mathcal{G} _{s}}\log(\frac{p_{e}}{1-p_{e}}))\] \[=\operatorname*{arg\,min}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{Id(Q[Y])}\end{subarray}}\sum_{e\notin \mathcal{G}_{s}}\log(\frac{p_{e}}{1-p_{e}})).\]

From the formulation above, it is clear that if we assign the weight \(w_{e}=\log(\frac{p_{e}}{1-p_{e}})\) to each edge \(e\in E^{\mathcal{G}}\), we will have an instance of the edge ID problem. Note that for edges with probability higher than \(1/2\), \(\log(\frac{p_{e}}{1-p_{e}})\geq 0\), and this assignment of edge weights satisfies the positivity requirement. For the opposite direction, note that the procedure explained above is reversible by the choice of probabilities \(p_{e}=\frac{\exp{(w_{e})}}{1+\exp{(w_{e})}}\), which is a value between \(1/2\) and \(1\).

_Problem 2_.: First note that under Assumption 2.1, for any graph \(\mathcal{G}_{s}\),

\[\sum_{\hat{\mathcal{G}}\subseteq\mathcal{G}_{s}}P(\hat{\mathcal{G}})=\prod_{e \notin\mathcal{G}_{s}}(1-p_{e})[\sum_{\hat{E}\subseteq E^{\mathcal{G}_{s}}} \prod_{e\in\hat{E}}p_{e}\prod_{e\notin\hat{E}}(1-p_{e})]=\prod_{e\notin \mathcal{G}_{s}}(1-p_{e}).\]This is because the inner summation goes over all the possible subsets of \(E^{\mathcal{G}_{s}}\), and the summation adds up to 1. Therefore, we can rewrite Problem 2 (Eq. 3)as

\[\mathcal{H}^{*} =\operatorname*{arg\,max}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{I\neq{d(Q[Y])}}\end{subarray}}\sum_{ \hat{\mathcal{G}}\subseteq\mathcal{G}_{s}}P(\hat{\mathcal{G}})\] \[=\operatorname*{arg\,max}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{I\neq{d(Q[Y])}}\end{subarray}}\prod_{e\notin \mathcal{G}_{s}}(1-p_{e})\] \[=\operatorname*{arg\,max}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{I\neq{d(Q[Y])}}\end{subarray}}\log(\prod_{e \notin\mathcal{G}_{s}}(1-p_{e}))\] \[=\operatorname*{arg\,max}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{I\neq{d(Q[Y])}}\end{subarray}}\sum_{e\notin \mathcal{G}_{s}}\log(1-p_{e})\] \[=\operatorname*{arg\,min}_{\begin{subarray}{c}\mathcal{G}_{s} \subseteq\mathcal{G},\\ \mathcal{G}_{s}\in[\mathcal{G}]_{I\neq{d(Q[Y])}}\end{subarray}}\sum_{e\notin \mathcal{G}_{s}}-\log(1-p_{e}).\]

With the same reasoning as before, assigning the weights \(w_{e}=-\log(1-p_{e})\) to each edge \(e\in E^{\mathcal{G}}\), we end up with an instance of the edge ID problem. Note that again \(0\leq-\log(1-p_{e})\leq\infty\). It is noteworthy that this procedure is also reversible with the choice of edge probabilities \(p_{e}=1-\exp{(-w_{e})}\), which reduces the edge ID problem to an instance of Problem 2. Again note that \(0\leq 1-\exp{(-w_{e})}\leq 1\) for any non-negative \(w_{e}\). 

### Reduction from MCIP to edge ID

**Theorem 3.3**.: _The edge ID problem is NP-complete._

First note that edge ID is in the class of NP problems, since the ID algorithm [26] can verify a solution in polynomial time. To prove Theorem 3.3, we first present a polynomial-time reduction from MCIP to the edge ID problem. It has been shown that the minimum vertex cover problem6 can be reduced to MCIP in polynomial time [2]. Combining the two reductions, we show that there exists a polynomial-time reduction from the minimum vertex cover problem to the edge ID problem. Since the minimum vertex cover problem is known to be NP-complete [19], it follows that the edge ID problem is also NP-complete.

Footnote 6: We later showed that there is a polynomial-time reduction from the minimum hitting set problem to MCIP, resulting in stronger hardness results [3].

We propose the following reduction from MCIP to the edge ID problem. Assume we want to solve MCIP given ADMG \(\mathcal{G}=(V^{\mathcal{G}},E^{\mathcal{G}}_{d},E^{\mathcal{G}}_{b})\), query \(Q[Y]\), and the intervention costs \(C(v)\) for \(v\in V^{\mathcal{G}}\). We construct a graph, denoted by \(\mathcal{H}=\mathcal{T}_{1}(\mathcal{G},Y)\), through the following steps.

1. For every vertex \(x\in V^{\mathcal{G}}\setminus Y\), add two vertices \(x^{1},x^{2}\) to \(V^{\mathcal{H}}\).
2. For any bidirected edge \(\{x,z\}\in E^{\mathcal{G}}_{b}\) where \(x\in V^{\mathcal{G}}\setminus Y\) and \(z\in V^{\mathcal{G}}\), add the bidirected edge \(\{x^{2},z^{2}\}\) to \(E^{\mathcal{H}}_{b}\).
3. For any directed edge \((x,z)\in E^{\mathcal{G}}_{d}\) where \(x\in V^{\mathcal{G}}\setminus Y\) and \(z\in V^{\mathcal{G}}\), add the directed edge \((x^{1},z^{1})\) to \(E^{\mathcal{H}}_{d}\).
4. For any bidirected edge \(\{y_{1},y_{2}\}\in E^{\mathcal{G}}_{b}\) where \(y_{1},y_{2}\in Y\), add the bidirected edge \(\{y_{1},y_{2}\}\) to \(E^{\mathcal{H}}_{b}\).
5. For every \(x^{1},x^{2}\in V^{\mathcal{G}}\setminus Y\), draw the two edges \(\{x^{1},x^{2}\}\in E^{\mathcal{H}}_{b}\) and \((x^{2},x^{1})\in E^{\mathcal{H}}_{d}\). Furthermore, the weight of \(\{x^{1},x^{2}\}\) is \(C(x)\).
6. The costs of the all other edges in \(\mathcal{H}\) are assigned to be infinite.

With abuse of notation, for any vertex \(x\in V^{\mathcal{G}}\setminus Y\), we define \(\mathcal{T}_{1}(x)=\{x^{2},x^{1}\}\in E^{\mathcal{H}}_{b}\), where \(\{x^{2},x^{1}\}\) is the bidirected edge in \(\mathcal{H}\) that corresponds to \(x\) in \(\mathcal{G}\), and inherits the same weight (cost).

**Example 2**.: _Consider graph \(\mathcal{G}\) in Figure 4(a). Vertices \(x\) and \(z\) are mapped to \(x^{1},x^{2}\), and \(z^{1},z^{2}\), respectively. Both a directed and a bidirected edge are drawn between these pairs. The bidirected edge \(\{x^{1},x^{2}\}\) is assigned the weight \(C(x)=c_{x}\), and the bidirected edge \(\{z^{1},z^{2}\}\) is assigned the weight \(C(z)=c_{z}\). Infinite weights are assigned to the rest of the edges in \(\mathcal{H}\) (Figure 4(b))._

**Proposition A.1**.: _Suppose \(\mathcal{G}^{\prime}\) is an ADMG, \(Y\subseteq V^{\mathcal{G}^{\prime}}\) is a set of its vertices such that \(Y\) is a district in \(\mathcal{G}^{\prime}[Y]\), and \(\mathcal{H}^{\prime}=\mathcal{T}_{1}(\mathcal{G}^{\prime},Y)\). Consider \(X\subseteq V^{\mathcal{G}^{\prime}}\setminus Y\) as an arbitrary subset of vertices of \(\mathcal{G}^{\prime}\), and define \(\mathcal{G}=\mathcal{G}^{\prime}[V^{\mathcal{G}^{\prime}}\setminus X]\). Let \(E^{\prime\prime}_{b}=\{e\in E^{\mathcal{H}^{\prime}}_{b}|\exists v\in X,e= \mathcal{T}_{1}(v)\}\) and define \(E^{\mathcal{H}}_{b}=E^{\mathcal{H}^{\prime}}_{b}\setminus E^{\prime\prime}_{b}\). Let \(\mathcal{H}\) be the edge-induced subgraph of \(\mathcal{H}^{\prime}\) defined as \(\mathcal{H}=(V^{\mathcal{H}^{\prime}},E^{\mathcal{H}}_{d},E^{\mathcal{H}}_{b})\). \(Q[Y]\) is identifiable in \(\mathcal{G}\) if and only if \(Q[Y]\) is identifiable in \(\mathcal{H}\)._

Proof.: We prove the contrapositive, i.e., \(Q[Y]\) is not identifiable in \(\mathcal{G}\) iff \(Q[Y]\) is not identifiable in \(\mathcal{H}\). Note that by construction, \(Y\) is a district in both \(\mathcal{G}[Y]\) and \(\mathcal{H}[Y]\). That is, it suffices to show that there exists a hedge formed for \(Q[Y]\) in \(\mathcal{G}\) iff there exists a hedge formed for \(Q[Y]\) in \(\mathcal{H}\).

To this end, we first prove the following claim. Let \(W\in V^{\mathcal{H}}\) form a hedge for \(Q[Y]\). If \(x^{1}\in W\) for some \(x\in V^{\mathcal{G}^{\prime}}\), then \(x^{2}\in W\) and vice versa. That is, the two vertices \(x^{1}\) and \(x^{2}\) corresponding to the same vertex \(x\) in \(V^{\mathcal{G}^{\prime}}\) appear only simultaneously in any hedge. To see this, note that by construction, \(x^{1}\) is the only child of \(x^{2}\). By definition of hedge, if \(x^{2}\in W\), then it has a directed path to \(Y\) within \(\mathcal{H}[W]\), and this path can only go through \(x^{1}\). For the other direction, note that \(x^{1}\) has only one bidirected edge, which is with \(x^{2}\). Again, by definition of hedge, if \(x^{1}\in W\), then it has a bidirected path to \(Y\) within \(\mathcal{H}[W]\), and this path can only go through \(x^{2}\). Hence, in the sequel, when there is a hedge \(W\) formed for \(Q[Y]\) in \(\mathcal{H}\), we will without loss of generality assume that there exists a set of variables \(Z\subseteq V^{G^{\prime}}\) such that \(W=Z^{1}\cup Z^{2}\cup Y\), where \(Z^{1}=\{z^{1}|z\in Z\}\) and \(Z^{2}=\{z^{2}|z\in Z\}\).

_If part._ Let \(W=Z^{1}\cup Z^{2}\cup Y\) form a hedge for \(Q[Y]\) in \(\mathcal{H}\). First note that since none of the bidirected edges between \(Z^{1}\) and \(Z^{2}\) are removed in \(\mathcal{H}\), by construction, all vertices \(Z\) are present in \(\mathcal{G}\), i.e., \(Z\subseteq V^{\mathcal{G}}\). Now we show that \(Z\cup Y\) forms a hedge for \(Q[Y]\) in \(\mathcal{G}\). To this end, we prove \(\mathcal{G}[Z\cup Y]\) is a district and \(Z\cup Y=\text{{Anc}}_{\mathcal{G}[Z\cup Y]}(Y)\). First note that any vertex in \(Z^{1}\) has only one bidirected edge to a vertex in \(Z^{2}\). That is, if we consider the edge-induced subgraph of \(\mathcal{H}[W]\) over its bidirected edges, vertices of \(Z^{1}\) are leaf nodes. As a result, \(Z^{2}\cup Y\) must be connected in this graph. That is, \(Z^{2}\cup Y\) is a district in \(\mathcal{H}[Z^{2}\cup Y]\). This implies by construction of \(\mathcal{H}\) that \(\mathcal{G}[Z\cup Y]\) is a single district. With a similar reasoning, note that vertices in \(Z^{2}\) have no parents. As result, \(Z^{1}\cup Y=\text{{Anc}}_{\mathcal{H}[Z^{1}\cup Y]}(Y)\) (since the directed paths cannot go through \(Z^{2}\)). Again, by construction, the edge-induced subgraph of \(\mathcal{G}[Z\cup Y]\) over its directed edges is a copy of \(\mathcal{H}[Z^{1}\cup Y]\). As a result, \(Z\cup Y=\text{{Anc}}_{\mathcal{G}[Z\cup Y]}(Y)\).

_Only if part._ Let \(Z\cup Y\) form a hedge for \(Q[Y]\) in \(\mathcal{G}\), where \(Z\subseteq V^{\mathcal{G}}\setminus Y\). Define \(Z^{1}=\{z^{1}|z\in Z\}\) and \(Z^{2}=\{z^{2}|z\in Z\}\). We show that \(Z^{1}\cup Z^{2}\cup Y\) forms a hedge for \(Q[Y]\) in \(\mathcal{H}\). First, by definition of hedge, \(\text{{Anc}}_{\mathcal{G}[Z\cup Y]}(Y)=Z\cup Y\). Since the edge-induced subgraph of \(\mathcal{H}[Z^{1}\cup Y]\) is a copy of \(\mathcal{G}[Z\cup Y]\) by construction, we know \(\text{{Anc}}_{\mathcal{G}[Z^{1}\cup Y]}(Y)=Z^{1}\cup Y\). Further, each vertex \(z^{2}\in Z^{2}\) is a parent of \(z^{1}\in Z^{1}\). As a result, \(\text{{Anc}}_{\mathcal{G}[Z^{1}\cup Z^{2}\cup Y]}(Y)=Z^{1}\cup Z^{2}\cup Y\). Now it suffices to show that \(Z^{1}\cup Z^{2}\cup Y\) is a district in \(\mathcal{H}[Z^{1}\cup Z^{2}\cup Y]\). By definition of hedge, \(Z\cup Y\) is a district in \(\mathcal{G}[Z\cup Y]\). By construction of \(\mathcal{H}\), exactly the same bidirected edges (and therefore bidirected paths) exist in \(\mathcal{H}[Z^{2}\cup Y]\). Therefore, \(Z^{2}\cup Y\) is a district in \(\mathcal{H}[Z^{2}\cup Y]\). Now note that by construction of \(\mathcal{H}^{\prime}\), each vertex \(z^{1}\in Z^{1}\) has a bidirected edge to \(z^{2}\in Z^{2}\). And by definition of \(\mathcal{G}\) and \(\mathcal{H}\), since the

Figure 5: Reduction of MCIP to edge IDvertices \(Z\) exist in \(\mathcal{G}\), none of these edges are removed in \(\mathcal{H}\). As a result, \(Z^{1}\cup Z^{2}\cup Y\) is a district in \(\mathcal{H}[Z^{1}\cup Z^{2}\cup Y]\), which completes the proof.

Proof of Theorem 3.3.: A polynomial-time reduction from MCIP to the edge ID problem follows immediately from Proposition A.1. MCIP is shown to be NP-complete [2]. As a result, the edge ID problem is Np-complete. 

### Reduction from edge ID to MCIP

**Proposition 4.2**.: _There exists a polynomial-time reduction from edge ID to MCIP and vice versa._

To prove Proposition 4.2, we begin with presenting a transformation \(\mathcal{T}_{2}(\mathcal{G},Y)\) which is in the core of reduction from edge ID to MCIP.

Suppose we want to solve the edge ID problem given ADMG \(\mathcal{G}=(V^{\mathcal{G}},E_{d}^{\mathcal{G}},E_{b}^{\mathcal{G}})\), query \(Q[Y]\), and edge weights \(W_{\mathcal{G}}=\{w_{e}|e\in\mathcal{G}\}\). Let \(X=V^{\mathcal{G}}\setminus Y\) denote the set of vertices of \(\mathcal{G}\) excluding \(Y\). We define the transformation \((\mathcal{H},Y^{mcp})=\mathcal{T}_{2}(\mathcal{G},Y)\) where \(\mathcal{H}=(V^{\mathcal{H}},E_{d}^{\mathcal{H}},E_{b}^{\mathcal{H}})\) is an ADMG and \(Y^{mcp}\subseteq V^{\mathcal{H}}\) as follows. Note that \(V^{\mathcal{H}}\) will consist of two disjoint set of vertices, namely \(V^{\mathcal{H}}_{top}\) and \(V^{\mathcal{H}}_{bot}\), i.e., \(V^{\mathcal{H}}=V^{\mathcal{H}}_{top}\cup V^{\mathcal{H}}_{bot}\).

1. Begin with \(V^{\mathcal{H}}_{top}=V^{\mathcal{H}}_{bot}=\emptyset,Y^{mcp}=\emptyset\). For any vertex \(v\in V^{\mathcal{G}}\), add a vertex \(v\) to \(V^{\mathcal{H}}_{top}\) with cost \(C(v)=\infty\). If \(v\in Y\), add \(v\) to \(Y^{mcp}\).
2. For any directed edge \((v_{i},v_{j})\in E_{d}^{\mathcal{G}}\) with weight \(w^{d}_{ij}\) in \(\mathcal{G}\), add a new vertex \(v^{d}_{ij}\) to \(V^{\mathcal{H}}_{top}\), with cost \(C(v^{d}_{ij})=w^{d}_{ij}\), where \[v^{d}_{ij}=\begin{cases}x^{d}_{ij}&\text{if $v_{i},v_{j}\in X$},\\ z^{d}_{ij}&\text{if $v_{i}\in Y$ or $v_{j}\in Y$},\\ y^{d}_{ij}&\text{if both $v_{i},v_{j}\in Y$}.\end{cases}\] Draw directed edges \((v_{i},v^{d}_{ij})\) and \((v^{d}_{ij},v_{j})\). Further, draw a bidirected edge between \(v_{i}\) and \(v^{d}_{ij}\).
3. For any bidirected edge \(\{x_{i},x_{j}\}\in E_{b}^{\mathcal{G}}\) with weight \(w^{b}_{ij}\), add a new vertex, \(x^{b}_{ij}\) to \(V^{\mathcal{H}}_{top}\) with cost \(C(x^{b}_{ij})=w^{b}_{ij}\). Add two bidirected edges \(\{x_{i},x^{b}_{ij}\}\) and \(\{x_{j},x^{b}_{ij}\}\). Further, draw two directed edges \((x^{b}_{ij},x_{i})\) and \((x^{b}_{ij},x_{j})\) in \(\mathcal{H}\).
4. For any bidirected edge \(\{x_{i},y_{j}\}\) with weight \(w^{b}_{ij}\), add a new vertex \(z^{b}_{ij}\) to \(V^{\mathcal{H}}_{top}\) with cost \(C(z^{b}_{ij})=w^{b}_{ij}\). Draw bidirected edges \(\{z^{b}_{ij},x_{i}\}\) and \(\{z^{b}_{ij},y_{j}\}\). Then draw a directed edge from \(z^{b}_{ij}\) to \(x_{i}\).
5. For any bidirected edge between \(\{y_{i},y_{j}\}\in E_{b}^{\mathcal{G}}\) with weight \(w^{b}_{ij}\) in \(\mathcal{G}\), add a new vertex, \(y^{b}_{ij}\) to \(V^{\mathcal{H}}_{top}\) with cost \(C(y^{b}_{ij})=w^{b}_{ij}\). Draw bidirected edges \(\{y^{b}_{ij},y_{i}\}\) and \(\{y^{b}_{ij},y_{j}\}\). Further, for any \(x\in X\), draw a directed edge from \(y^{b}_{ij}\) to \(x\).
6. Let \(y_{1}\prec...\prec y_{k}\) denote a topological ordering among vertices of \(Y\). For every pair \(\{y_{i},y_{j}\}\) of vertices of \(Y\), where \(i<j\), add vertices \(y^{ij}_{i},y^{ij}_{i+1},\ldots,y^{ij}_{j}\) to \(V^{\mathcal{H}}_{bot}\). Add \(y^{ij}_{j}\) to \(Y^{mcp}\). Draw the directed edges \((y_{k},y^{ij}_{k})\) for every \(i\leq k\leq j\). Draw the directed edges \((y^{ij}_{k},y^{ij}_{i})\) for every \(i<k<j\), and the directed edge \((y^{ij}_{i},y^{ij}_{j})\). Draw a bidirected edge between \(y_{j}\) and \(y^{ij}_{i}\). Further, for any bidirected edge \(\{y_{k},y_{i}\}\in E_{b}^{\mathcal{G}}\) where \(i\leq k,l\leq j\), add a new vertex \(y^{ij}_{kl}\) to \(V^{\mathcal{H}}_{bot}\), draw two bidirected edges \(\{y^{ij}_{kl},y^{ij}_{k}\}\) and \(\{y^{ij}_{kl},y^{ij}_{l}\}\), and a directed edge \((y^{ij}_{kl},y^{b}_{ij})\). The costs of the all of the vertices in \(V^{\mathcal{H}}_{bot}\) are infinite.

With abuse of notation, for any bidirected edge \(e^{b}_{ij}=\{v_{i},v_{j}\}\in E_{b}^{\mathcal{G}}\) and any directed edge \(e^{d}_{ij}=(v_{i},v_{j})\in E_{d}^{\mathcal{G}}\), we define \(\mathcal{T}_{2}(e^{b}_{ij})=v^{b}_{ij}\) and \(\mathcal{T}_{2}(e^{d}_{ij})=v^{d}_{ij}\), respectively, where \(v^{b}_{ij},v^{d}_{ij}\in V^{\mathcal{H}}\) are the vertices representing their corresponding edges.

We will utilize the following results to prove Proposition 4.2. More precisely, Lemmas 2 through 9 are used to prove Proposition A.10, which in turn is used to prove Proposition 4.2.

**Lemma A.2**.: _Suppose \(\mathcal{G}\) is an ADMG, \(Y\) is a set of its vertices, and \((\mathcal{H},Y^{mcip})=\mathcal{T}(\mathcal{G},Y)\). Each vertex \(y\in Y^{mcip}\) is a district in \(\mathcal{H}\)._

Proof.: It suffices to show that for every pair of \(v_{1},v_{2}\in Y^{mcip}\) there is no bidirected edge between them in \(\mathcal{H}\). Suppose first that \(v_{1},v_{2}\in Y\). Any bidirected edge between \(v_{1}\) and \(v_{2}\) in \(\mathcal{G}\) (if it exists) is removed in step (e) of the transformation, and none of the steps (a) through (f) add a bidirected edge between them. Otherwise, at least one of \(v_{1},v_{2}\), w.l.o.g. \(v_{1}\), is in \(Y^{mcip}\setminus Y\). Suppose w.l.o.g. that \(v_{1}=y_{j}^{ij}\). From step (f) of the transformation \(\mathcal{T}\), we know that \(v_{1}\) has bidirected edges only to vertices \(y_{kj}^{ij}\), where none of them is a member of \(Y^{mcip}\). 

**Lemma A.3**.: _Suppose \(\mathcal{G}\) is an ADMG, \(Y\) is a set of its vertices, and \((\mathcal{H},Y^{mcip})=\mathcal{T}_{2}(\mathcal{G},Y)\). Suppose there is a hedge formed for \(Q[y]\) in \(\mathcal{H}\), where \(y\in Y\). Let \(H\) denote the set of vertices of this hedge. \(H\) does not include any of the vertices added in the step (f) of the transformation. That is, \(H\cap V_{bot}^{\mathcal{H}}=\emptyset\)._

Proof.: Define \(V_{1}=\{y_{kl}^{ij}\in V_{bot}^{\mathcal{H}},\forall i,j,k,l\}\), and \(V_{2}=V_{bot}^{\mathcal{H}}\setminus V_{1}\). By construction of \(\mathcal{H}\), the vertices of \(V_{2}\) have directed edges only to vertices in \(V_{2}\). Therefore, for each vertex \(v\in V_{2}\), we have \(v\notin\textit{Anc}_{\mathcal{H}[H]}(y)\). As a result, \(V_{2}\cap H=\emptyset\), since by definition of hedge, any vertex of \(H\) is an ancestor of \(y\) in \(\mathcal{H}[H]\). Now, consider an arbitrary vertex \(v\in V_{1}\). By construction of \(\mathcal{H}\), if there exists a bidirected edge \(\{v,v^{\prime}\}\in E_{b}^{\mathcal{H}}\), we must have that \(v^{\prime}\in V_{2}\). Therefore, if \(v\in H\), there must be at least one vertex \(v^{\prime}\in V_{2}\cap H\). Since we proved \(V_{2}\cap H=\emptyset\), \(v\) cannot be in \(H\). Consequently, \(V_{1}\cap H=\emptyset\).

**Lemma A.4**.: _Suppose \(\mathcal{G}\) is an ADMG, \(Y\) is a set of its vertices, and \((\mathcal{H},Y^{mcip})=\mathcal{T}(\mathcal{G},Y)\). Suppose there is a hedge formed for \(Q[y_{j}^{ij}]\) in \(\mathcal{H}\), where \(y_{i},y_{j}\in Y\) and \(y_{j}^{ij}\) is the vertex corresponding to the pair \((y_{i},y_{j})\) added in step (f) of the transform \(\mathcal{T}\). Let \(H\) denote the set of vertices of this hedge. If \(v\in H\cap V_{bot}^{\mathcal{H}}\), then \(v\) has the superscript \(ij\), that is, \(v\) is either one of the vertices \(y_{k}^{ij}\), or one of the vertices \(y_{kl}^{ij}\), where \(i\leq k,l\leq j\). In the latter case, \(y_{kl}^{b}\in H\)._

Proof.: Define \(V_{1}=\{y_{kl}^{mn}\in V_{bot}^{\mathcal{H}},\forall m,n,k,l\}\), and \(V_{2}=V_{bot}^{\mathcal{H}}\setminus V_{1}\). Suppose \(V_{1}^{*}=\{v_{kl}^{ij}\in V_{bot}^{\mathcal{H}},\forall k,l\}\) and \(V_{2}^{*}=\{v_{k}^{ij}\in V_{bot}^{\mathcal{H}},\forall k\}\). Also define \(V_{1}^{\prime}=V_{1}\setminus V_{1}^{*}\), \(V_{2}^{\prime}=V_{2}\setminus V_{2}^{*}\). For the first part of the claim, it suffices to show that \(V_{1}^{\prime}\cap H=\emptyset,V_{2}^{\prime}\cap H=\emptyset\). By construction of \(\mathcal{H}\), the vertices of \(V_{2}^{\prime}\) do not have any child out of \(V_{2}^{\prime}\). Therefore, \(V_{2}^{\prime}\cap Anc_{\mathcal{H}[H]}(y_{j}^{ij})=\emptyset\). This implies that \(V_{2}^{{}^{\prime}}\cap H=\emptyset\). Now let \(v_{1}^{i^{{}^{\prime}}j^{{}^{\prime}}}\) be an arbitrary vertex in \(V_{1}^{{}^{\prime}}\). By construction of \(\mathcal{H}\), \(v_{1}^{i^{{}^{\prime}}j^{{}^{\prime}}}\) has bidirected edges only to vertices of \(V_{2}^{\prime}\). This implies that if \(v_{1}^{i^{{}^{\prime}}j^{{}^{\prime}}}\in H\), there must be at least one vertex of \(V_{2}^{\prime}\) in \(H\) which is in contradiction with \(V_{2}^{\prime}\cap H=\emptyset\). Therefore, \(v_{1}^{i^{{}^{\prime}}j^{{}^{\prime}}}\notin H\). Since \(v_{1}^{i^{{}^{\prime}}j^{{}^{\prime}}}\) is an arbitrary vertex in \(V_{1}^{{}^{\prime}}\), we conclude \(V_{1}^{\prime}\cap H=\emptyset\).

Now, we prove that if \(v\in H\) is one of the vertices \(y_{kl}^{ij}\), we have \(y_{kl}^{b}\in H\). Since \(y_{kl}^{ij}\in H\), there exists a directed path from \(y_{kl}^{ij}\) to \(y_{j}^{ij}\) in \(\mathcal{H}[H]\). Since \(y_{kl}^{b}\) is the only child of \(y_{kl}^{ij}\), the aforementioned path passes through \(y_{kl}^{b}\). Therefore, \(y_{kl}^{b}\in H\).

**Lemma A.5**.: _Suppose \(\mathcal{G}^{\prime}=(V^{\mathcal{G}^{\prime}},E_{d}^{\mathcal{G}^{\prime}},E_{b} ^{\mathcal{G}^{\prime}})\) is an ADMG, \(Y\subseteq V^{\mathcal{G}^{\prime}}\) is a set of its vertices, and \((\mathcal{H}^{\prime},Y^{mcip})=\mathcal{T}(\mathcal{G}^{\prime},Y)\). Let \(E_{d}^{\prime\prime}\subseteq E_{d}^{\mathcal{G}^{\prime}}\) and \(E_{b}^{\prime\prime}\subseteq E_{b}^{\mathcal{G}^{\prime}}\) be arbitrary edges of \(\mathcal{G}\), and define \(E_{d}^{\mathcal{G}}=E_{d}^{\mathcal{G}^{\prime}}\setminus E_{d}^{\prime\prime}\), \(E_{b}^{\mathcal{G}}=E_{b}^{\mathcal{G}^{\prime}}\setminus E_{b}^{\prime\prime}\). Define \(\mathcal{G}=(V^{\mathcal{G}},E_{d}^{\mathcal{G}},E_{b}^{\mathcal{G}})\) and \(\mathcal{H}=\mathcal{H}^{\prime}[V^{\mathcal{H}^{\prime}}\setminus V^{\prime}]\), where \(V^{\mathcal{G}}=V^{\mathcal{G}^{\prime}}\) and \(V^{\prime}=\{v\in V^{\mathcal{H}^{\prime}}|2e\in E_{b}^{\mathcal{G}}\cup E_{d}^{ \prime\prime},v=\mathcal{T}_{2}(e)\}\). Suppose there is a hedge formed for \(Q[y_{j}^{ij}]\) in \(\mathcal{H}\) for some \(i,j\). Let \(H\) denote the set of vertices of this hedge in \(\mathcal{H}\). The set of vertices \(Y^{*}=\{y_{k}|y_{k}^{ij}\in H\}\) is a district in \(\mathcal{G}[Y]\). Moreover, \(H_{top}=Anc_{\mathcal{H}[H_{top}]}(Y^{*})\), where \(H_{top}=H\cap V_{top}^{\mathcal{H}}\)._Proof.: First we prove that \(Y^{*}\) is a district in \(\mathcal{G}[Y]\). Consider an arbitrary vertex \(y_{k}^{ij}\) in \(H\). By definition of hedge, there exists a bidirected path, \(p_{1}\), between \(y_{k}^{ij}\) and \(y_{j}^{ij}\) in \(\mathcal{H}[H]\). Let \(Y^{ij}\) denotes the set of vertices in \(H\) such that their superscript is \(ij\). Lemma A.4 implies that \(H\subseteq V_{top}^{\mathcal{H}}\cup Y^{ij}\). Furthermore, by construction of \(\mathcal{H}\), there is only one bidirected edge between \(Y^{ij}\) and \(H\setminus Y^{ij}\), which is \(\{y_{j},y_{i}^{ij}\}\). Therefore, all of the vertices on the path \(p_{1}\) are in \(Y^{ij}\). Now, we define \(Y_{1}^{{}^{\prime}}=\{y_{k}|y_{k}^{ij}\in p_{1}\}\) and \(Y_{2}^{{}^{\prime}}=\{y_{kl}^{h}|y_{kl}^{ij}\in p_{1}\}\), i.e., the \(V_{top}^{\mathcal{H}}\) counterparts of the vertices in \(p_{1}\). Since the vertices on \(p_{1}\) are in \(H\), \(Y_{1}^{\prime}\subseteq Y^{*}\). From Lemma A.4, we know that if \(y_{kl}^{ij}\in H\), then, \(y_{kl}^{b}\in H\). It implies that \(Y_{2}^{{}^{\prime}}\subseteq H\). As a result, \(Y_{1}^{\prime}\) and \(Y_{2}^{{}^{\prime}}\) are both vertices of \(\mathcal{H}\). Now if we replace all the vertices in \(p_{1}\) with their corresponding counterpart in \(Y_{1}^{\prime}\cup Y_{2}^{\prime}\), we arrive at a bidirected path \(p_{2}\) between \(y_{k}\) and \(y_{j}\) in \(\mathcal{H}[Y_{1}^{\prime}\cup Y_{2}^{\prime}]\) (as by construction the same edges exist in \(V_{top}^{\mathcal{H}}\)). By definition of \(\mathcal{G}\) and \(\mathcal{H}\), if a vertex \(y_{kl}^{b}\) exists in \(\mathcal{H}\), the corresponding edge \(\{y_{k},y_{l}\}\) exists in \(\mathcal{G}\). As a result, a bidirected path between \(y_{k}\) and \(y_{l}\) exists in \(\mathcal{G}[Y_{1}^{\prime}]\). Noting that \(y_{k}\) is an arbitrary vertex in \(Y^{*}\) and \(Y_{1}^{\prime}\subseteq Y^{*}\), this implies that all of the vertices of \(Y^{*}\) are in the same district as \(y_{j}\) in \(\mathcal{G}[Y^{*}]\), which completes the proof.

Next, we prove that \(H_{top}=Anc_{\mathcal{H}[H_{top}]}(Y^{*})\). To this end, it suffices to show that there is a directed path form an arbitrary vertex \(v\in H_{top}\) to \(Y^{*}\) in \(\mathcal{H}[H_{top}]\). Since \(H\) forms a hedge for \(Q[y_{j}^{ij}]\) in \(\mathcal{H}\), there exists a directed path from \(v\) to \(y_{j}^{ij}\) in \(\mathcal{H}[H]\). This path must go through the only parent of \(y_{j}^{ij}\), which is \(y_{i}^{ij}\). Then, the last vertex on the path is one of the parents of \(y_{i}^{ij}\). If this parent is \(y_{i}\), we are done as we have a directed path from \(v\) to \(y_{i}\), where \(y_{i}\in Y^{*}\) and it has no ancestors in \(H\setminus H_{top}\). Otherwise, let this parent be \(y_{k}^{ij}\) for some \(i<k<j\). Now the last vertex on the path before \(y_{k}^{ij}\) must be \(y_{k}\), which is the only parent of \(y_{k}^{ij}\). Note that by definition of \(Y^{*}\), \(y_{k}\in Y^{*}\). Therefore, \(v\) has a directed path to \(Y^{*}\) in \(\mathcal{H}[H_{top}]\). 

**Lemma A.6**.: _Suppose \(\mathcal{G}=(V^{\mathcal{G}},E_{d}^{\mathcal{G}},E_{b}^{\mathcal{G}})\) is an ADMG, \(Y\) is a set of its vertices, and \((\mathcal{H},Y^{micip})=\mathcal{T}_{2}(\mathcal{G},Y)\). Suppose there is a hedge formed for \(Q[y]\) in \(\mathcal{H}\) for some \(y\in Y^{micip}\). Let \(H\) denote the set of vertices of this hedge. Then \(H\cap X\neq\emptyset\), where \(X=V^{\mathcal{G}}\setminus Y\)._

Proof.: Since \(H\) forms a hedge for \(Q[y]\) in \(\mathcal{H}\), there exists a vertex \(h\in H\) such that \(\{y,h\}\in E_{b}^{\mathcal{H}}\). There are two possibilities for \(y\in Y^{micip}\):

* \(y=y_{i}\in Y\). From Lemma A.4 we have \(h\notin V_{bot}^{\mathcal{H}}\). Therefore, by construction of \(\mathcal{H}\), \(h=y_{ij}^{k}\) for some \(j\).
* \(y=y_{j}^{ij}\in V_{bot}^{\mathcal{H}}\). By construction of \(\mathcal{H}\), \(h=y_{kj}^{ij}\) for some \(k\). Vertex \(h\) must have a directed path to \(y\) in \(H\) by definition of hedge, which must go through the only child of \(h\), i.e., \(y_{kl}^{b}\).

In both cases, we showed that there exists a vertex \(v=y_{ij}^{b}\in H\) for some \(i,j\). By definition of hedge, there is a bidirected path, \(p\), from \(v\) to \(y\) in \(\mathcal{H}\) because \(v\in Anc_{\mathcal{H}}(y)\). Since all of the children of \(v\) are in \(X\), there is at least one vertex in \(X\) on path \(p\). Therefore, \(H\) includes at least one vertex of \(X\).

**Lemma A.7**.: _[Inverse transform preserves hedges.] Suppose \(\mathcal{G}^{\prime}=(V^{\mathcal{G}^{\prime}},E_{d}^{\mathcal{G}^{\prime}},E_{ b}^{\mathcal{G}^{\prime}})\) is an ADMG, \(Y\subseteq V^{\mathcal{G}^{\prime}}\) is a set of its vertices, and \((\mathcal{H}^{\prime},Y^{micip})=\mathcal{T}_{2}(\mathcal{G}^{\prime},Y)\). Let \(E_{d}^{\prime\prime}\subseteq E_{d}^{\mathcal{G}^{\prime}}\) and \(E_{b}^{\prime\prime}\subseteq E_{b}^{\mathcal{G}^{\prime}}\) be arbitrary edges of \(\mathcal{G}\), and define \(E_{d}^{\mathcal{G}}=E_{d}^{\mathcal{G}^{\prime}}\setminus E_{d}^{\prime\prime}\), \(E_{b}^{\mathcal{G}}=E_{b}^{\mathcal{G}^{\prime}}\setminus E_{b}^{\prime\prime}\). Define \(\mathcal{G}=(V^{\mathcal{G}},E_{d}^{\mathcal{G}},E_{b}^{\mathcal{G}})\) and \(\mathcal{H}=\mathcal{H}^{\prime}[V^{\mathcal{H}^{\prime}}\setminus V^{\prime}]\), where \(V^{\mathcal{G}}=V^{\mathcal{G}^{\prime}}\) and \(V^{\prime}=\{v\in V^{\mathcal{H}}\,|\exists e\in E_{b}^{\prime\prime}\cup E_{d} ^{\prime\prime},v=\mathcal{T}_{2}(e)\}\). Let \(W\subseteq V_{top}^{\mathcal{H}}\) be a set of vertices of \(\mathcal{H}\). Let \(W_{s}\subseteq W\cap V^{\mathcal{G}}\) be a subset of \(W\) such that \(W_{s}\) are vertices of \(\mathcal{G}\) as well. Consider the inverse transform of \(\mathcal{H}[W]\) in the ADMG \(\mathcal{G}\), i.e., for any \(v=v_{ij}^{b}\in W\), delete \(v\) and all edges incident to it and draw a bidirected edge between \(v_{i}\) and \(v_{j}\), and for any \(v=v_{ij}^{d}\), delete \(v\) and all edges incident to it and draw a directed edge from \(v_{i}\) to \(v_{j}\). Let the resulting subgraph (which is a subgraph of \(\mathcal{G}\)) be denoted by \(\mathcal{G}[W^{-1}]\) with the set of vertices \(W^{-1}\subseteq V^{\mathcal{G}}\). If \(Anc_{\mathcal{H}[W]}(W_{s})=W\), then \(Anc_{\mathcal{G}[W^{-1}]}(W_{s})=W^{-1}\). Moreover, if \(W\) is a district in \(\mathcal{H}[W]\), then \(W^{-1}\) is a district in \(\mathcal{G}[W^{-1}]\).

Proof.: First, we show that if \(Anc_{\mathcal{H}[W]}(W_{s})=W\), then \(Anc_{\mathcal{G}[W^{-1}]}(W_{s})=W^{-1}\). Let \(v\) be an arbitrary vertex in \(W^{-1}\). Vertex \(v\) is in \(W\) because \(W^{-1}\subseteq W\). Since \(v\in W\) and \(v\in Anc_{\mathcal{H}[W]}(W_{s})\), \(v\) has a directed path \(v\rightarrow\ldots v_{i}\to v_{j}^{d}\to v_{j}\cdots\to w\), denoted by \(l\), to a vertex \(w\in W_{s}\) in \(\mathcal{H}[W]\). For each vertex \(v_{ij}^{d}\) on path \(l\), we have \(v_{i},v_{j}\in\mathcal{G}[W^{-1}]\) and since \(v_{ij}^{d}\in V^{\mathcal{H}}\), by definition of \(\mathcal{G}\) and \(\mathcal{H}\), there exists \((v_{i},v_{j})\in E_{d}^{\mathcal{G}}\) s.t. \(i\prec j\), and consequently, \((v_{i},v_{j})\in E_{d}^{\mathcal{G}[W^{-1}]}\). Therefore, there exists a directed path from \(v\) to \(w\) in \(\mathcal{G}[W^{-1}]\). Noting that \(v\) is an arbitrary vertex in \(W^{-1}\), we conclude that \(Anc_{\mathcal{G}[W^{-1}]}(W_{s})=W^{-1}\).

Now, we prove that if \(W\) is a district in \(\mathcal{H}[W]\), then \(W^{-1}\) is a district in \(\mathcal{G}[W^{-1}]\). Consider two vertices \(v_{1},v_{2}\in W^{-1}\). Since \(v_{1},v_{2}\in W\) and \(W\) is a district, there exists a bidirected path \(v_{1}\leftrightarrow\ldots v_{i}\leftrightarrow v_{ij}^{b}\leftrightarrow v _{j}\cdots\leftrightarrow v_{2}\), denoted by \(p\), between \(v_{1}\) and \(v_{2}\) in \(\mathcal{H}[W]\). Each vertex \(v_{ij}^{b}\) on path \(p\) is in \(\mathcal{H}\) and \(v_{i},v_{j}\in\mathcal{G}[W^{-1}]\). By definition of \(\mathcal{G}\) and \(\mathcal{H}\), we have \(\{v_{i},v_{j}\}\in E_{b}^{\mathcal{G}}\). Therefore, \(\{v_{i},v_{j}\}\in E_{b}^{\mathcal{G}[W^{-1}]}\). Then, there is a bidirected path between \(v_{1}\) and \(v_{2}\) in \(\mathcal{G}[W^{-1}]\). Since \(v_{1}\) and \(v_{2}\) are two arbitrary vertices in \(W^{-1}\), it implies that \(W^{-1}\) is a district in \(\mathcal{G}[W^{-1}]\). 

**Lemma A.8**.: _[Transform preserves hedges.] Suppose \(\mathcal{G}^{\prime}=(V^{\mathcal{G}^{\prime}},E_{d}^{\mathcal{G}^{\prime}},E_ {b}^{\mathcal{G}^{\prime}})\) is an ADMG, \(Y\subseteq V^{\mathcal{G}^{\prime}}\) is a set of its vertices, and \((\mathcal{H}^{\prime},Y^{\text{mcip}})=\mathcal{T}_{2}(\mathcal{G}^{\prime},Y)\). Let \(E_{d}^{\mathcal{G}}\subseteq E_{d}^{\mathcal{G}^{\prime}}\) and \(E_{b}^{\prime\prime}\subseteq E_{b}^{\mathcal{G}^{\prime}}\) be arbitrary edges of \(\mathcal{G}\), and define \(E_{d}^{\mathcal{G}}=E_{d}^{\mathcal{G}^{\prime}}\setminus E_{d}^{\prime\prime}\), \(E_{b}^{\mathcal{G}}=E_{b}^{\mathcal{G}^{\prime}}\setminus E_{b}^{\prime\prime}\). Define \(\mathcal{G}=(V^{\mathcal{G}},E_{d}^{\mathcal{G}},E_{b}^{\mathcal{G}})\) and \(\mathcal{H}=\mathcal{H}^{\prime}[V^{\mathcal{H}}\setminus V^{\prime}]\), where \(V^{\mathcal{G}}=V^{\mathcal{G}^{\prime}}\) and \(V^{\prime}=\{v\in V^{\mathcal{H}}\,|\exists e\in E_{b}^{\prime\prime}\cup E_ {d}^{\prime\prime},v=\mathcal{T}_{2}(e)\}\). Let \(W\subseteq V^{\mathcal{G}}\) be a set of vertices of \(\mathcal{G}\) such that \(W\setminus Y\neq\emptyset\). Let \(W_{s}\subseteq W\) be a subset of \(W\). Let the transformed graph of \(\mathcal{G}[W]\) under \(\mathcal{T}_{2}\) be denoted by \(\mathcal{H}^{\prime\prime}\), where \(\mathcal{H}^{\prime\prime}\subseteq\mathcal{H}\). Define \(W^{*}=V_{top}^{\mathcal{H}^{\prime\prime}}\). If \(Anc_{\mathcal{G}[W]}(W_{s})=W\), then \(Anc_{\mathcal{H}[W^{*}]}(W_{s})=W^{*}\). Moreover, if \(W\) is a district in \(\mathcal{G}[W]\), then \(W^{*}\) is a district in \(\mathcal{H}[W^{*}]\).

Proof.: First, we prove that if \(Anc_{\mathcal{G}[W]}(W_{s})=W\), then \(Anc_{\mathcal{H}[W^{*}]}(W_{s})=W^{*}\). Take an arbitrary vertex \(v\in W^{*}\). There are two possibilities for \(v\):

* \(v\in W\). That is, vertex \(v\) is in \(\mathcal{G}[W]\).
* \(v\notin W\). This implies that \(v\) represents an edge \(e\) between two vertices \(v_{i}\) and \(v_{j}\) in \(\mathcal{G}[W]\). There are three possibilities for \(e\):
* \(e=(v_{i},v_{j})\). By construction of \(\mathcal{H}\), \(v\) is parent of \(v_{j}\) in \(\mathcal{H}[W^{*}]\), where \(v_{j}\) is a vertex of \(\mathcal{G}[W]\).
* \(e=\{v_{i},v_{j}\}\) and \(v_{i}\in X\) or \(v_{j}\in X\). In this case, \(v\) is parent of at least one of \(v_{i}\) and \(v_{j}\) in \(\mathcal{H}[W^{*}]\), w.l.o.g. \(v_{i}\), where \(v_{i}\) is a vertex of \(\mathcal{G}[W]\).
* \(e=\{v_{i},v_{j}\}\) and \(v_{i},v_{j}\in Y\). By construction of \(\mathcal{H}\), \(v\) is parent of all vertices in \(V^{\mathcal{G}}\setminus Y\). Since \(W\setminus Y\neq\emptyset\), there exists a vertex \(x\) in \(\mathcal{G}[W]\) such that \(v\) is a parent of \(x\). In all three cases above, we proved that there exists a vertex \(x\in W\) such that \(v\) is a parent of \(x\).

Therefore, we showed that any vertex \(v\in W^{*}\) either is itself a vertex in \(W\) or is a parent of a vertex in \(W\). As a result, it suffices to show that every \(w\in W\) has a directed path to \(W_{s}\) in \(\mathcal{H}[W^{*}]\). We know that \(w\) has a directed path to \(W_{s}\) in \(\mathcal{G}[W]\) such as \(p\). Take an arbitrary pair of consecutive vertices on this path, such as \(v_{1}\) and \(v_{2}\). The directed edge \((v_{1},v_{2})\) exists in \(\mathcal{G}[W]\). As a result, the directed path \(v_{1}\to v_{12}^{d}\to v_{2}\) exists in \(\mathcal{H}[W^{*}]\). Starting at \(w\) and repeating this argument for every pair of consecutive vertices on \(p\), we conclude that there exists a directed path from \(w\) to \(W_{s}\), which completes the proof.

Now, we show that if \(W\) is a district in \(\mathcal{G}[W]\), then \(W^{*}\) is a district in \(\mathcal{H}[W^{*}]\). Take an arbitrary vertex \(v\in W^{*}\). There are two possibilities for \(v\):

* \(v\in W\). That is, \(v\) is a vertex in \(\mathcal{G}[W]\).

* \(v\notin W\). In this case, at least one of the vertices \(v\) represents an edge \(e\) between two vertices \(v_{i}\) and \(v_{j}\) in \(\mathcal{G}[W]\). By construction of \(\mathcal{H}\), \(v\) is connected to at least one of \(v_{i}\) or \(v_{j}\), w.l.o.g. \(v_{i}\), by a bidirected edge, where \(v_{i}\in W\).

We showed that any vertex \(v\in W^{*}\) either is in \(W\), or is connected to a vertex in \(W\) through a bidirected edge. Therefore, it suffices to show that for any two vertices \(w_{1},w_{2}\in W\) there exists a bidirected path between \(w_{1}\) and \(w_{2}\) in \(\mathcal{H}[W^{*}]\). Since \(w_{1},w_{2}\in W\), there is a bidirected path, \(p\), between \(w_{1}\) and \(w_{2}\) in \(\mathcal{G}[W]\). Take an arbitrary pair of consecutive vertices on this path, such as \(v_{1}\) and \(v_{2}\). The bidirected edge \(\{v_{1},v_{2}\}\) exists in \(\mathcal{G}[W]\). As a result, the bidirected path \(v_{1}\leftrightarrow v_{12}^{b}\leftrightarrow v_{2}\) exists in \(\mathcal{H}[W^{*}]\). Starting at \(w\) and repeating this argument for every pair of consecutive vertices on \(p\), we conclude that there exists a bidirected path from \(w_{1}\) to \(w_{2}\), which completes the proof. 

**Lemma A.9**.: _Suppose \(\mathcal{G}\) is an ADMG, and \(Y\) is a subset of its vertices. Also let \(Y^{*}\) be a district in \(\mathcal{G}[Y]\). If the set of vertices \(H\) form a hedge for \(Q[Y^{*}]\), then \(H\setminus Y\neq\emptyset\)._

Proof.: Assume by contradiction \(H\setminus Y=\emptyset\), i.e., \(H\subseteq Y\). By definition of hedge, we know \(H\setminus Y^{*}\neq\emptyset\). Take an arbitrary vertex \(v\in H\setminus Y^{*}\). Furthermore, \(v\in Y\setminus Y^{*}\) because \(H\subseteq Y\). Since \(H\) forms a hedge for \(Q[Y^{*}]\), \(H\) is a district in \(\mathcal{G}[H]\). Therefore, there exists a bidirected path between \(v\) and a vertex \(y^{*}\in Y^{*}\) in \(Q[Y]\) which is in contradiction with the assumption that \(Y^{*}\) is a district in \(\mathcal{G}[Y]\). 

**Proposition A.10**.: _Suppose \(\mathcal{G}^{\prime}=(V^{\mathcal{G}^{\prime}},E_{d}^{\mathcal{G}^{\prime}},E_ {b}^{\mathcal{G}^{\prime}})\) is an ADMG, \(Y\subseteq V^{\mathcal{G}^{\prime}}\) is a set of its vertices, and \((\mathcal{H}^{\prime},Y^{mcip})=\mathcal{T}_{2}(\mathcal{G}^{\prime},Y)\). Let \(E_{d}^{\prime\prime}\subseteq E_{d}^{\mathcal{G}^{\prime}}\) and \(E_{b}^{\prime\prime}\subseteq E_{b}^{\mathcal{G}^{\prime}}\) be arbitrary edges of \(\mathcal{G}\), and define \(E_{d}^{\mathcal{G}}=E_{d}^{\mathcal{G}^{\prime}}\setminus E_{d}^{\prime\prime}\), \(E_{b}^{\mathcal{G}}=E_{b}^{\mathcal{G}^{\prime}}\setminus E_{b}^{\prime\prime}\). \(Q[Y]\) is identifiable in \(\mathcal{G}=(V^{\mathcal{G}},E_{d}^{\mathcal{G}},E_{b}^{\mathcal{G}})\) if and only if \(Q[Y^{mcip}]\) is identifiable in \(\mathcal{H}=\mathcal{H}^{\prime}[V^{\mathcal{H}^{\prime}}\setminus V^{\prime}]\), where \(V^{\mathcal{G}}=V^{\mathcal{G}^{\prime}}\) and \(V^{\prime}=\{v\in V^{\mathcal{H}^{\prime}}|\exists e\in E_{b}^{\prime\prime} \cup E_{d}^{\prime\prime},v=\mathcal{T}_{2}(e)\}\)._

Proof.: We prove the contrapositive, i.e., \(Q[Y]\) is not identifiable in \(\mathcal{G}\) iff \(Q[Y^{mcip}]\) is not identifiable in \(\mathcal{H}\).

_If part._ Suppose \(Q[Y^{mcip}]\) is not identifiable in \(\mathcal{H}\). That is, there exists a hedge formed for \(Q[Y^{mcip}]\) in \(\mathcal{H}\). From Lemma A.2, this hedge is formed for \(Q[y^{\prime}]\) for some \(y^{\prime}\in Y^{mcip}\). Denote the set of vertices of this hedge by \(H\). We consider two possibilities separately:

* \(y^{\prime}=y_{i}\), where \(y_{i}\in Y\). From Lemma A.3, \(H\subseteq V_{top}^{\mathcal{H}}\). Taking \(W=H\) in Lemma A.7, \(W^{-1}\) is a set of vertices in \(\mathcal{G}\) such that \(Anc_{\mathcal{G}[W^{-1}]}(y)=W^{-1}\), and \(W^{-1}\) is a district in \(\mathcal{G}\). Now take \(Y^{*}\) to be the district of \(\mathcal{G}[Y]\) that includes \(y_{i}\). By definition of hedge, \(\mathcal{G}[W^{-1}\cup Y^{*}]\) forms a hedge for \(Q[Y^{*}]\) in \(\mathcal{G}\). Note that from Lemma A.6, \(W^{-1}\setminus Y\neq\emptyset\). As a result, \(Q[Y]\) is not identifiable in \(\mathcal{G}\).
* \(y^{\prime}=y_{j}^{ij}\), where \(y_{i},y_{j}\in Y\) and \(y^{\prime}\) is one of the vertices added to \(\mathcal{H}\) in the last step of the transformation \(\mathcal{T}\) (step (f)). Define the set \(Y^{*}=\{y_{k}|y_{k}^{ij}\in H\}\). From Lemma A.5, \(Y^{*}\) is a district in \(\mathcal{G}\), and therefore a district in \(\mathcal{G}[Y]\). As a result, it suffices to show that there exists a hedge formed for \(Q[Y^{*}]\) in \(\mathcal{G}\). Now define \(H_{top}=H\cap V_{top}^{\mathcal{H}}\). By definition of hedge, \(H\) is a district in \(\mathcal{H}[H]\), i.e., it is connected over its bidirected edges. By construction of \(\mathcal{H}\), there is only one bidirected edge between the vertices in \(H_{top}\) and \(H\setminus H_{top}\), which is the bidirected edge between \(y_{j}\) and \(y_{i}^{ij}\). Therefore, this edge is a cut set that partitions the graph \(\mathcal{H}[H]\) into two connected components \(\mathcal{H}[H_{top}]\) and \(\mathcal{H}[H\setminus H_{top}]\). That is, \(\mathcal{H}[H_{top}]\) is connected over its bidirected edges and therefore \(H_{top}\) is a district in \(\mathcal{H}[H_{top}]\). Further, from Lemma A.5, \(H_{top}=Anc_{\mathcal{H}[H_{top}]}(Y^{*})\). Noting that \(H_{top}\subseteq V_{top}^{\mathcal{H}}\), taking \(W=H_{top}\) in Lemma A.7, \(W^{-1}\) is a district in \(\mathcal{G}\) and \(Anc_{\mathcal{G}[W^{-1}]}(Y^{*})=W^{-1}\). Note that from Lemma A.6, \(W^{-1}\setminus Y\neq\emptyset\). Therefore, the set of vertices \(W^{-1}\) form a hedge for \(Q[Y^{*}]\) in \(\mathcal{G}\). Hence, \(Q[Y]\) is not identifiable in \(\mathcal{G}\).

_Only if part._ Suppose \(Q[Y]\) is not identifiable in \(\mathcal{G}\). It implies that there exists a district of \(\mathcal{G}[Y]\) such as \(Y^{*}\) such that there is a hedge formed for \(Q[Y^{*}]\) in \(\mathcal{G}\). Let \(H\) denote the set of vertices of this hedge. From Lemma A.9, \(H\setminus Y\neq\emptyset\). Define \(W^{*}\) as in Lemma A.8, that is the transform \(\mathcal{T}(\mathcal{G}[H],Y^{*})\)without step (f) (only on the vertices of \(V^{\mathcal{H}}_{top}\)). Note that \(Y^{*}\subseteq W^{*}\). We consider the following two cases separately:

* \(Y^{*}=\{y\}\), that is, \(Y^{*}\) is a single vertex. From Lemma A.8, \(W^{*}\) is a district in \(\mathcal{H}[W^{*}]\), and \(Anc_{\mathcal{H}[W^{*}]}(y)=W^{*}\). By definition of hedge, the vertices \(W^{*}\) form a hedge for \(Q[y]\) in \(\mathcal{H}\). Note that \(y\in Y^{mclip}\), and from Lemma A.2 it is a district of \(\mathcal{H}[Y^{mclip}]\). As a result, \(Q[Y^{mclip}]\) is not identifiable in \(\mathcal{H}\).
* \(|Y^{*}|\geq 2\). Let \(y_{i}\) and \(y_{j}\) be the first and the last vertices of \(Y^{*}\) in the topological order. Define \(Y^{ij*}=\{y_{k}^{ij}|y_{k}\in Y^{*}\}\cup\{y_{kl}^{ij}|y_{k},y_{l}\in Y^{*}\}\). \(Y^{ij*}\) are the vertices in \(V^{\mathcal{H}}_{bot}\) with superscript \(ij\) corresponding to the vertices in \(Y^{*}\). Note that \(y_{i}^{ij},y_{j}^{ij}\in Y^{ij*}\), since \(y_{i},y_{j}\in Y^{*}\). Since \(y_{j}^{ij}\in Y^{mclip}\) and from Lemma A.2\(y_{j}^{ij}\) is a district in \(\mathcal{H}[Y^{mclip}]\), it suffices to show that there is a hedge formed for \(y_{j}^{ij}\) in \(\mathcal{H}\). We show that the vertices \(W=W^{*}\cup Y^{ij*}\) form a hedge for \(y_{j}^{ij}\) in \(\mathcal{H}\). From Lemma A.8, \(Anc_{\mathcal{H}[W^{*}]}(Y^{*})=W^{*}\), that is, all of the vertices in \(W^{*}\) are ancestors of \(Y^{*}\) in \(\mathcal{H}[W^{*}]\), and therefore in \(\mathcal{H}[W]\). Also, the vertices \(y_{kl}^{ij}\) in \(Y^{ij*}\) have a direct edge to their corresponding vertex in \(W^{*}\), i.e., \(y_{kl}^{b}\), and therefore are ancestors of \(Y^{*}\) in \(\mathcal{H}[W]\) as well. Further, each vertex in \(Y^{*}\) such as \(y_{k}\) is a parent of \(y_{k}^{ij}\), which is in turn a parent of \(y_{i}^{ij}\) (or is \(y_{i}^{ij}\) itself if \(k=i\).) Finally, \(y_{i}^{ij}\) has a directed edge to \(y_{j}^{ij}\) by construction. As a result, all of the vertices \(W\) have a direct path to \(y_{j}^{ij}\) in \(\mathcal{H}[W]\). That is, \(Anc_{\mathcal{H}[W]}(y_{j}^{ij})=W\). It now remains to show that \(W\) is a district in \(\mathcal{H}[W]\). From Lemma A.8, \(W^{*}\) is a district in \(\mathcal{H}[W^{*}]\). As a result, the vertices \(W^{*}\) are connected through bidirected edges in \(\mathcal{H}[W]\). There is a bidirected edge between \(y_{j}\) and \(y_{i}^{ij}\) by construction of \(\mathcal{H}\). It suffices to show that for any \(v\in Y^{ij*}\), there exists a bidirected path between \(v\) and \(y_{i}^{ij}\) in \(\mathcal{H}[W]\). A vertex \(y_{kl}^{ij}\in Y^{ij*}\) (with double subscript, which are due to the bidirected edges among \(Y^{*}\)) has bidirected edges to \(y_{k}^{ij}\) and \(y_{l}^{ij}\), which are both in \(Y^{ij*}\) by definition. Now take an arbitrary vertex \(y_{k}^{ij}\in Y^{ij*}\) (with single subscript, due to vertices in \(Y^{*}\)). We know \(y_{k}\in Y^{*}\), as \(y_{k}^{ij}\in Y^{ij*}\), by definition of \(Y^{ij*}\). \(Y^{*}\) is a district in \(\mathcal{G}[Y^{*}]\). That is, there exists a bidirected path from \(y_{k}\) to \(y_{i}\) in \(\mathcal{G}[Y^{*}]\). From Lemma A.8 by taking \(W=Y^{*}\), there is a bidirected path \(p\) from \(y_{k}\) to \(y_{i}\) in \(\mathcal{H}[Y^{*}\cup\{y_{lm}|y_{l},y_{m}\in Y^{*}\}]\). By construction of \(\mathcal{H}\), if we replace each vertex \(v\) on \(p\) by \(v^{ij}\), we achieve a bidirected path \(p^{\prime}\) with vertices in \(Y^{ij*}\) from \(y_{k}^{ij}\) to \(y_{i}^{ij}\), which completes the proof.

Proof of Proposition 4.2.: The reduction from the edge ID problem to MCIP was shown through the proof of Proposition A.10. The opposite direction is an immediate corollary of Proposition A.1. 

**Corollary A.11**.: _The edge ID problem and MCIP are equivalent._

## Appendix B Maximal hedge

Herein, we present the algorithm for recovering the maximal hedge formed for \(Q[Y]\) in a given ADMG \(\mathcal{G}\) (see Definition 2.5). Maximal hedge was initially defined in [2] under the name _hedge hull_. We adopt the same definition, and when \(\mathcal{G}[Y]\) comprises several districts, we define the maximal hedge as the union of the hedge hulls formed for each district of \(\mathcal{G}[Y]\). As a result, the complete procedure of recovering the maximal hedge for a query \(Q[Y]\), summarized in Algorithm 3, finds the maximal hedge formed for each district \(Y_{i}\) of \(\mathcal{G}[Y]\) and returns the union of them. This procedure is used as a subroutine **MH** in Algorithm 1. The function **HHull** is in fact Algorithm 1 borrowed from [2]. This function is proven to recover the union of all hedges formed for \(Y_{i}\), where \(Y_{i}\) is one of the districts of \(\mathcal{G}[Y]\) (see Lemma 6 of [2]).

## Appendix C Towards generalizing Assumption 2.1

Lemma 3.2 states the equivalence of Problems 1 and 2 with the edge ID problem under Assumption 2.1. However, as mentioned in the main text, this equivalence holds in the more general setting where we allow for perfect negative correlations among edges. As an example, consider the graph of Figure 6. Suppose that the performed statistical independence tests show that the two variables \(z\) and \(y\) are independent of each other with high confidence. As a result, the expert believes that the edges \((z,x)\) and \((x,y)\) must not exist simultaneously, as otherwise the causal path from \(z\) to \(y\) would make them dependent. In such cases, the belief of the expert can be modeled as probabilities \(p\) and \(q\) assigned to the existence of the edges \((z,x)\) and \((x,y)\), respectively, as well as a perfect negative correlation between them.

Note that the aforementioned constraint, i.e., that the edges do not exist simultaneously, can be specified for any number of edges, not limited to two edges only. For instance, the expert might believe at least one of the edges along a causal path of length \(n\) must not exist in the true ADMG describing the system. This belief can be modeled as an extra constraint in the optimization of Equations 2 and 3. We show that with the specification of such negative correlations, Problems 1 and 2 can still be cast as an instance of the edge ID problem. Therefore, the results presented in this work are valid in this setting as well.

**Assumption C.1**.: The edges in \(\mathcal{G}\) are assigned probabilities \(p_{e},\forall e\in\mathcal{G}\), and perfect negative correlations are defined among subsets of edges. More precisely, for any subset \(E\subseteq E_{d}^{\mathcal{G}}\cup E_{b}^{\mathcal{G}}\), there is either 1) no constraint (mutually independent), or 2) the constraint that at least one of the edges in \(E\) must not exist in the true ADMG (perfect negative correlation).

**Proposition C.1**.: _Under Assumption C.1, there exists a reduction from Problems 1 and 2 to the edge ID problem and vice versa with the time complexity in the order of \(\mathcal{O}(|C|\cdot|V^{\mathcal{G}}|+|E_{d}^{\mathcal{G}}\cup E_{b}^{\mathcal{ G}}|)\), where \(C\) is the set of perfect correlation constraints._

Proof.: First note that we proved the equivalence of Problems 1 and 2 with the edge ID problem without the perfect correlation constraints in Lemma 3.2. As a result, under assumption C.1, i.e., by adding the perfect correlation constraints, Problems 1 and 2 are equivalent to a modified edge ID problem with those constraints. But we claim that there exists and instance of the original unconstrained edge ID problem which is equivalent to these problems. To see this, first note that we know from Corollary A.11 that the edge ID problem is equivalent to MCIP. Therefore, it suffices to show that there exists an instance of MCIP which is equivalent to the constrained edge ID mentioned above. To this end, consider the transform \(\mathcal{T}_{2}(\mathcal{G},Y)\) introduced in Section A.2. This transformation

Figure 6: An example where the expert is aware that there is no causal path from \(z\) to \(y\), e.g., because \(z\perp\!\!\!\!\perp y\) with high confidence.

maps an instance of the edge ID problem to an instance of MCIP. Applying this transformation to the constrained edge ID problem, we can map the constrained edge ID to an instance of MCIP with extra constraints, with transforming the constraints as well. That is, if for instance, there is a perfect negative correlation among the edges \(e_{1},e_{2}\) in \(\mathcal{G}\), this constraint is mapped to a negative perfect correlation on the corresponding vertices in \(\mathcal{H}\), namely \(\mathcal{T}_{2}(e_{1}),\mathcal{T}_{2}(e_{2})\). In words, this constraint would be that at least one of \(\mathcal{T}_{2}(e_{1})\) and \(\mathcal{T}_{2}(e_{2})\) must be intervened upon. We show that such constraints can be integrated into the original definition of MCIP.

Suppose we have an MCIP problem in ADMG \(\mathcal{G}\) with query \(Q[Y]\), with the extra constraint that at least one of the vertices \(X\subseteq V^{\mathcal{G}}\) must be intervened upon. Consider the example of \(X=\{x_{1},x_{2},x_{3}\}\) in Figure 7. We build a new ADMG \(\mathcal{G}^{\prime}\) by adding vertices \(\{x^{\prime}|x\in X\}\), i.e., a new vertex corresponding to each vertex in \(X\), along with an auxiliary vertex \(\hat{y}\) to \(\mathcal{G}\). We fix a random ordering over the vertices of \(X\), and denote the set of vertices of \(X\) as \(x_{1},...,x_{m}\). We add the directed edges \((x_{i},x_{i}^{\prime})\) to \(\mathcal{G}^{\prime}\), as well as the bidirected edges \(\{x_{i},x_{i}^{\prime}\}\). Further, we draw directed edges \((x_{i}^{\prime},x_{i+1}^{\prime})\) for every \(1\leq i<m\). Finally, we draw the directed edge \((x_{m}^{\prime},\hat{y})\) and the bidirected edge \(\{x_{1},\hat{y}\}\). Refer to the graph in Figure 7 for an example with \(X=\{x_{1},x_{2},x_{3}\}\). Note that the set \(X\cup X^{\prime}\cup\{\hat{y}\}\) forms a hedge for \(Q[\hat{y}]\), where \(X^{\prime}=\{x^{\prime}|x\in X\}\) Now it suffices to set the cost of intervention on vertices of \(X^{\prime}\) to infinity, and consider MCIP for the query \(Q[Y\cup\{\hat{y}\}]\) in \(\mathcal{G}^{\prime}\). It is straightforward to see that the objective of this problem would be to find the minimum cost intervention for identification of \(Q[Y]\), with the constraint that at least one of the vertices in \(X\) must be intervened on. Note that as soon as one vertex in \(X\) gets intervened upon, there is no hedge left for \(Q[\hat{y}]\). Also it is noteworthy that adding this structure does not add any new hedges formed for \(Q[Y]\), since the structure only includes new descendants for \(X\) which have no directed paths to \(Y\). Also note that the vertices \(X^{\prime}\) and \(\hat{y}\) are specific to the very constraint corresponding to the set of vertices \(X\). For any constraint, we add such a structure to \(\mathcal{G}\). The number of vertices (and therefore the time complexity) is at most in the order \(\mathcal{O}(|C|\cdot|V^{\mathcal{G}}|)\), where \(C\) is the set of constraints.

### Further applications

The relaxation provided in this Appendix allows the approaches proposed in this work to be applicable to a more general set of problems. Herein, we discuss one such application.

Let us assume we run our algorithm which returns the subgraph with the highest probability, \(\mathcal{G}_{1}\). However, the probability that \(\mathcal{G}_{1}\) is the true causal structure describing the system might be too low. In such a case, the researcher might be interested in having a ranking of most probable graphs (for instance, the 10 most probable graphs), rather than only one most probable graph. This could be helpful for instance, when a unique identification formula is valid in a few of these graphs, or the researcher is interested in identifying more than one causal query. The methods discussed in this work along with the relaxation proposed in this appendix provide the tools to recover such a ranking (of the most probable graphs in which a query is identifiable). To see this, note that based on what we proposed in this Appendix, perfect negative correlation constraints can be added to the edge ID problem without additional computational cost. So we begin by solving the original problem, which yields a graph \(\mathcal{G}_{1}\). We then solve it for a second time (i.e., re-run the algorithm), with the only difference that we add the perfect negative correlation constraint over the set of all edges of \(\mathcal{G}_{1}\) (i.e., we force the algorithm to exclude at least one of the edges of \(\mathcal{G}_{1}\).) The solution to this problem (let us call it \(\mathcal{G}_{2}\)) is the highest probability graph among all subgraphs except \(\mathcal{G}_{1}\), i.e., it is the second highest

Figure 7: Integrating the perfect negative correlation constraint into MCIP.

probability graph in which the query is identifiable. Continuing in this manner, running the algorithm \(n\) times would give us a ranking of the \(n\) highest probability graphs.

## Appendix D Heuristic algorithms

Algorithm 2 was devised considering the fact that every hedge formed for \(Q[Y]\) must include a vertex that has a bidirected edge to \(Y\). As mentioned in Section 4.2, an analogous approach, summarized in Algorithm 4, uses the fact that any hedge formed for \(Q[Y]\) must include a parent of \(Y\).

Let \(Y\subseteq V^{\mathcal{G}}\) be a set of vertices of \(\mathcal{G}\) such that \(\mathcal{G}[Y]\) comprises of only one district. Let \(Z:=\{z\in V^{\mathcal{G}}|\exists\ y\in Y:(z,y)\in E_{d}^{\mathcal{G}}\setminus Y\) denote the set of vertices that have at least one directed edge to a vertex in \(Y\), i.e., the parents of \(Y\) excluding \(Y\). Any hedge formed for \(Q[Y]\) contains at least one vertex of \(Z\). As a result, in order to eliminate all the hedges formed for \(Q[Y]\), it suffices to ensure that none of the vertices in \(Z\) appear in the final hedge. To this end, for any \(z\in Z\), it suffices to either remove all the directed edges between \(z\) and \(Y\), or eliminate all the bidirected paths from \(z\) to \(Y\). The problem of eliminating all bidirected paths from \(Z\) to \(Y\) can be cast as a minimum cut problem between \(Z\) and \(Y\) in the edge-induced subgraph of \(\mathcal{G}\) over its bidirected edges. To add the possibility of removing the directed edges between \(Z\) and \(Y\), we add an auxiliary vertex \(z^{*}\) to the graph and draw a bidirected edge between \(z^{*}\) and every \(z\in Z\) with weight \(w=\sum_{y\in Y}w_{(z,y)}\), i.e., the sum of the weights of all directed edges between \(z\) and \(Y\). Note that \(z\) can have directed edges to multiple vertices in \(Y\). We then solve the minimum cut problem for \(z^{*}\) and \(Y\). If an edge between \(z^{*}\) and \(z\in Z\) is in the solution to this min-cut problem, it translates to removing all the directed edges from \(z\) to \(Y\) in the original problem. Note that we can run the algorithm on the maximal hedge formed for \(Q[Y]\) in \(\mathcal{G}\) rather than \(\mathcal{G}\) itself.

```
1:functionHEID2(\(\mathcal{G},Y,W_{\mathcal{G}}\))
2:\(\mathcal{G}^{\prime}\leftarrow\mathbf{MH}(\mathcal{G},Y)\)
3:\(Z\leftarrow\{z\in V^{\mathcal{G}^{\prime}}|\exists y\in Y:(z,y)\in E_{d}^{ \mathcal{G}^{\prime}}\}\setminus Y\)
4:\(\mathcal{H}\leftarrow\) The induced subgraph of \(\mathcal{G}^{\prime}\) on its bidirected edges.
5:\(W_{\mathcal{H}}\leftarrow\{w_{e}\in W_{\mathcal{G}}|e\in\mathcal{H}\}\)
6:\(V^{\mathcal{H}}\gets V^{\mathcal{H}}\cup\{y^{*},z^{*}\}\)
7:for\(z\in Z\)do
8:\(E^{\mathcal{H}}\gets E^{\mathcal{H}}\cup\{z^{*},z\}\)
9:\(W_{\mathcal{H}}\gets W_{\mathcal{H}}\cup\{w_{\{z^{*},z\}}=\sum_{y}w_{(z,y)}\}\)
10:for\(y\in Y\)do
11:\(E^{\mathcal{H}}\gets E^{\mathcal{H}}\cup\{y,y^{*}\}\)
12:\(W_{\mathcal{H}}\gets W_{\mathcal{H}}\cup\{w_{\{y,y^{*}\}}=\infty\}\)
13:\(E\gets MinCut(\mathcal{H},W_{\mathcal{H}},z^{*},y^{*})\)
14:return\((E,\sum_{e\in E}w_{e})\) ```

**Algorithm 4** Heuristic algorithm 2.

## Appendix E Experiments

Noting that the synthetic/simulation results in the main paper were for graphs with a \(\log(n)/n\) sparsity constraint, we begin this section by providing a set a results on the simulated graphs without the sparsity penalty for comparison. Then, we provide information about the causal discovery algorithm used to derive the psychology 'Psych' real-world graph. We also provide experimental results for Problem 2 formulation in Section E.3.

### Additional simulation results without sparsity constraint

The simulation results for graphs generated without the sparsity constraint are shown in Figure 8. These results illustrate monotonic increases in runtime and cost as the number of nodes increases. Our proposed heuristic algorithms (HEID-1 and HEID-2) maintain runtimes less than 0.5 seconds even for 250 nodes. In contrast, the two exact algorithms (MCIP-exact and EDGEID) exceed the three minute runtime limit at only 20 nodes, and the MCIP heuristic variants (MCIP-H1 and MCIP-H2) have runtimes which increase exponentially with the number of nodes. These results highlight the efficiency of our proposed heuristic algorithms to find solutions with equivalent cost with significantly faster runtimes.

### Psychology graph discovery

The settings for deriving the putative structure used on the psychology real-world graph are provided in Table 3.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Parameter** & **Value** \\ \hline Learning Rate & 0.01 \\ DAG Penalty & True \\ DAG Penalty Weight & 0.05 \\ Number of Runs & 50 \\ Train Epochs & 3000 \\ Test Epochs & 800 \\ Mixed Data & True \\ hlayers & 2 \\ dhlayers & 2 \\ lambda1 & 10 \\ lambda2 & 0.001 \\ dlr & 0.001 \\ linear & False \\ nh & 20 \\ dnh & 200 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyperparameter settings for the Structural Agnostic Model used to generate the putative (directed) structure for the ‘Psych’ real-world dataset.

Figure 8: Experimental results (for graphs generated without the sparsity constraint) for runtime, solution costs, fraction of graphs for which no solution was found, and fraction of graphs for which runtime limit of 3 minutes was exceeded. Error bars for runtime and cost graphs indicate 5th and 95th percentiles. Best viewed in color.

### Simulation results for Problem 2 formulation

The experimental setup is exactly as in the main text (the results depicted in Figure 4), except that the probabilities are chosen in the range \([0.01,1]\) instead of \([0.51,1]\), and we use the weight mapping corresponding to Problem 2 as described in Lemma 3.2. That is, we map each probability \(p_{e}\) to the weight \(-\log(1-p_{e})\) in the corresponding edge ID problem.

The simulation results are presented in Figure 9. Runtimes and costs are shown for the subset of graphs for which all algorithms found a solution (to facilitate comparison). Runtimes for each algorithm are shown in Fig. 8(a), where it can be seen that our proposed HEID-1 and HEID-2 heuristic algorithms have negligible runtime. In contrast, EDGEID had large runtime variance which depended heavily on the specifics of the graph under evaluation, particularly for graphs with fewer vertices. The costs for each graph are shown in Fig. 8(b). Figure 8(c) shows the fraction of evaluations for which the runtime exceeded 3 minutes (applicable to the exact algorithms). In general, and owing to the sparsity penalty in our graph generation mechanism, the cost of identified solutions falls with the number of vertices. Overall, HEID-1 was both the most consistent in terms of finding a solution, having a short runtime, and achieving a close to optimal cost.

Figure 9: Experimental results for runtime, solution costs, fraction of graphs for which no solution was found, and fraction of graphs for which runtime limit of 3 minutes was exceeded. Error bars for runtime and cost graphs indicate 5th and 95th percentiles. Best viewed in color.