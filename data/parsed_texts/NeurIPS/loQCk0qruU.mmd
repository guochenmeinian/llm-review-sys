# Be Confident in What You Know: Bayesian Parameter Efficient Fine-Tuning of Vision Foundation Models

 Deep Shankar Pandey \({}^{\dagger}\) Spandan Pyakurel \({}^{\dagger}\) Qi Yu

Rochester Institute of Technology

{dp7972,sp1468,qi.yu}@rit.edu

Corresponding author, \({}^{\dagger}\) equal contribution

###### Abstract

Large transformer-based foundation models have been commonly used as pre-trained models that can be adapted to different challenging datasets and settings with state-of-the-art generalization performance. Parameter efficient fine-tuning (PEFT) provides promising generalization performance in adaptation while incurring minimum computational overhead. However, adaptation of these foundation models through PEFT leads to accurate but severely underconfident models, especially in few-shot learning settings. Moreover, the adapted models lack accurate fine-grained uncertainty quantification capabilities limiting their broader applicability in critical domains. To fill out this critical gap, we develop a novel lightweight Bayesian Parameter Efficient Fine-Tuning (referred to as Bayesian-PEFT) framework for large transformer-based foundation models. The framework integrates state-of-the-art PEFT techniques with two Bayesian components to address the under-confidence issue while ensuring reliable prediction under challenging few-shot settings. The first component performs base rate adjustment to strengthen the prior belief corresponding to the knowledge gained through pre-training, making the model more confident in its predictions; the second component builds an evidential ensemble that leverages belief regularization to ensure diversity among different ensemble components. Our thorough theoretical analysis justifies that the Bayesian components can ensure reliable and accurate few-shot adaptations with well-calibrated uncertainty quantification. Extensive experiments across diverse datasets, few-shot learning scenarios, and multiple PEFT techniques demonstrate the outstanding prediction and calibration performance by Bayesian-PEFT.

## 1 Introduction

Transformer-based foundation models have been developed as general models with state-of-the-art generalization performance [32, 66, 52, 28]. These models leverage the rich meta-knowledge acquired during the pre-training stage to effectively adapt to complex downstream tasks [32], where pre-training is usually performed on massive-scale annotated datasets (_e.g.,_[35, 55]) through supervised learning[32, 66, 28] or self-supervised learning [52]. To achieve effective adaption, various parameter-efficient fine-tuning (PEFT) approaches have been developed [38, 27, 9, 54] that introduce a small number of tunable parameters either within or outside of the backbone architecture to ensure good generalization capability while incurring little computational overhead because most parts of (or the entire) backbone architecture is frozen during fine-tuning [25, 59, 67]. Bias-fine tuning [9], a representative partial tuning-based PEFT, only fine-tunes the bias parameters to downstream tasks. Adapter [51] and side-tune [72] fine-tuning techniques are instances of extra module-based PEFT that introduce extra parameterized modules and fine-tune them based on the downstream tasks. Visual Prompt-tuning [32] (VPT) follows the popular prompt learning paradigm by introducing a learnableprompt that is fine-tuned on the downstream task knowledge keeping the pre-trained backbone frozen.

Despite the attractive generalization performance, most foundation models adapted to downstream few-shot tasks through PEFT exhibit a somewhat surprising and undesirable behavior that may prevent them from being applied to many critical domains. Figure 1 (b) shows the predictive accuracy versus the Expected Calibration Error (ECE) of a foundation model after performing few-shot adaptation on CIFAR-100 using a series of representative PEFT methods, including VPT [32], Adapter [51], Bias Fine Tuning [71], and Side-tune [72]. It is clear that the adapted model is able to provide accurate predictions even after fine-tuned on limited training samples. For example, all PEFT methods help to boost the model's accuracy to over 75% using just 5-shot fine-tuning and the accuracy reaches 80% after 10-shot fine-tuning. However, the adapted model is very poorly calibrated as shown by the large ECE scores, which are consistently over 0.3 across all the fine-tuning methods. Increasing the fine-tuning size does not show clear improvement and sometimes even hurts the calibration performance. While one may expect the poor ECE to be caused by over-confidence as we fine-tune a large foundation model using very limited training samples, the detailed ECE plots as shown in Figure 2 (a)-(c) reveal that the model is in fact severely under-confident. For example, after adapting to the 1-shot training dataset, the VPT fine-tuned model can already achieve a test accuracy close to \(50\%\) but is under-confident in almost all its predictions leading to an ECE score over \(0.45\). The under-confidence issue is observed for all representative PEFT methods across different datasets and various few-shot learning settings as evidenced by our experiments.

The under-confident few-shot adaptation behavior of foundation models closely mimics how human experts with rich domain knowledge in their own disciplines tend to make _conservative_ decisions when facing new tasks that deviate from their own expertise. Analogous to their human counterparts, the rich prior knowledge gained through the pre-training stage of foundational models overshadows the relatively limited knowledge obtained through few-shot fine-tuning, which prevents them from making more confident predictions in the downstream tasks. Unreliable uncertainty (_i.e.,_ confidence) quantification makes the predictions provided by these models less trustworthy, which may limit applying the promising "pre-train-then-finetune" paradigm to many critical domains. As shown in Figure 2 (a)-(c), the fine-tuned model seldom makes any predictions with confidence over 80%, making it difficult to leverage these predictions in any high-stakes decision-making process.

The need to balance between the rich prior knowledge gained through pre-training and the new knowledge obtained through few-shot adaptation inspires us to investigate the under-confidence issue from the Bayesian perspective. In particular, we propose to look into the predictive behavior of the few-shot adapted foundation model through the lenses of evidential learning [56], which is built upon Bayesian theorem and subjective logic (SL) theory [33]. As part of the recent advances in modern Bayesian modeling, evidential learning provides a cost-effective way to perform Bayesian inference with the capability to quantify fine-grained second-order uncertainty [57]. By leveraging the important

Figure 1: Accuracy Vs. ECE on CIFAR100 few-shot adaptation from different PEFT methods

theoretical connection between fine-grained uncertainty and model accuracy [46], we unveil the underlying reason that supports the good predictive performance of a few-shot adapted foundation model and the root cause for the under-confident behavior. Drawing from this important insight as outlined above, we propose to integrate the modern PEFT techniques into a novel lightweight Bayesian framework, referred to as Bayesian-PEFT (B-PEFT), aiming to achieve highly reliable and accurate few-shot adaptations with well-calibrated and trustworthy uncertainty quantification.

The proposed Bayesian framework offers two important components to address the under-confidence issue while ensuring reliable prediction under challenging few-shot settings. The first component makes novel adjustments to the base rates introduced by the SL theory to strengthen the prior belief corresponding to the knowledge gained through pre-training. Meanwhile, the adjustment does not change the relative order of the belief assigned to different classes, which ensures that the model accuracy is maintained. Our theoretical analysis shows that the proposed base rate adjustment strategy leads to more confident predictions (by increasing the gaps between the belief assigned to the ground-truth class and the rest) without compromising the model's accuracy. Figure 1(d) shows that B-PEFT significantly improves the model calibration. To further enhance the reliability of both prediction accuracy and uncertainty quantification when performing few-shot adaptation, the second component performs Bayesian model averaging by building a diversity-inducing evidential ensemble. In addition to using different random initialization of the PEFT components, diversity is further enhanced through incorrect belief regularization that penalizes a model for assigning a high belief to a non-ground-truth class. By controlling the strength of belief regularization, different ensemble components are guided to learn from diverse features in the data space, where a light penalty allows an ensemble component to learn the common discriminative features while a heavy one will force an ensemble component to learn rare features to avoid errors on the difficult data samples. A deeper theoretical analysis of the proposed diversity-induced evidential ensemble is equivalent to Stein Variational Gradient Descent (SVGD) based ensembles [13]. Experiments on multiple challenging few-shot learning tasks justify the superior performance of B-PEFT over state-of-the-art PEFT baselines, in terms of both prediction accuracy and uncertainty calibration performance. Our contributions can be summarized as follows:

* We identify the severe under-confidence issue of pre-trained foundation models after performing parameter-efficient fine-tuning over few-shot datasets. The fine-grained uncertainty analysis through evidential learning and SL theory reveals the root cause for their good predictive performance while being under-confident.
* We develop a novel lightweight Bayesian framework that integrates state-of-the-art PEFT techniques with two Bayesian components to address the under-confidence issue while ensuring reliable prediction under challenging few-shot settings. The first component performs base rate adjustment to strengthen the prior belief corresponding to the knowledge gained through pre-training, making the model more confident in its predictions; the second component builds an evidential ensemble that leverages belief regularization to ensure diversity among different ensemble components.
* We perform thorough theoretical analysis to justify why the proposed Bayesian components can ensure reliable and accurate few-shot adaptations with well-calibrated uncertainty quantification.
* We carry out experiments with 4 benchmark datasets, 5 different few-shot settings, and 3 parameter efficient fine-tuning techniques that demonstrate the effectiveness of the developed model.

## 2 Related Works

Parameter Efficient Fine Tuning of Foundation Models.Transformer-based foundation models [63; 15] have been developed as an improvement over traditional convolution-based architectures

Figure 2: PEFT on the \(1\)-shot CIFAR100 dataset: all existing PEFT techniques exhibit severe under-confidence while the proposed B-PEFT reduces the ECE by almost an order of magnitude.

[29; 31] for computer vision tasks. The transformer-based models achieve strong generalization performance [40] after training on large datasets. Moreover, the pre-trained transformers can be fine-tuned in limited data settings leading to state-of-the-art performance [32; 27]. As a computation, memory, and parameter-efficient alternative to full fine-tuning of such large pre-trained transformers, different Parameter Efficient Fine Tuning (PEFT) approaches have been developed. PEFT techniques freeze most of the large transformer backbone, fine-tune the remaining backbone parameters and/or introduce lightweight extra modules for adapting to the downstream task. Existing approaches can broadly be categorized as extra-module-based [72; 54], partial-tuning-based [71; 9], and visual prompt tuning-based [68; 68; 28] methods. Extra-module-based methods (_e.g.,_ Adapter [51], side-tune[72]) introduce small additional learnable modules and keep the pre-rained backbone frozen. Partial-tuning-based methods (_e.g.,_ Bias [9]) keep a large portion of the backbone frozen, and fine-tune only part of the foundation model to downstream tasks. Visual prompt-based PEFT methods (VPT) introduce a learnable prompt variable along with a learnable classification head over the fixed pre-trained backbone to be adapted to the downstream task. VPT [32] has shown significant improvement over other PEFT techniques, and can even outperform full fine-tuning in multiple datasets/settings [28].

Calibration in Deep Learning ModelsCalibration methods have been increasingly explored to achieve trustworthy deep-learning models. Post-hoc calibration methods [26; 43; 73] aim to learn a calibration map for a standard trained deep learning model such that the map can transform the poorly calibrated probabilities to calibrated probabilities. Regularization-based calibration approaches introduce explicit regularization (such as with L\({}_{2}\) regularization [26], entropy regularization [50]), or implicit regularization (such as with focal loss [39]) during training to ensure that the trained model is calibrated. Data augmentation methods such as Label smoothing [44], and mixup training [60][74] have also been explored for developing calibrated deep learning models. Recent survey [65] provides a discussion of the most relevant works towards developing calibrated deep learning models. Most existing calibration methods are designed to tackle the over-confidence issue, which is more commonly observed for large models trained from limited data due to overfitting. We observe that fine-tuned foundation models exhibit severe under-confidence in their predictions, where existing calibration techniques are less effective. To this end, we propose a lightweight Bayesian framework that fills this critical gap.

Few-Shot Adaptation and Relationship with Meta-Learning.In this work, we consider few-shot adaption with a focus on \(N\)-way \(K\)-shot classification [64; 22], where the model is presented with a few-shot training set with \(N\)-class, each having \(K\) examples. For instance, 1-shot Cifar100 training set consists of 1 sample from each of the 100 classes. The model is then evaluated on the test set, which is identical to the query set in the meta-testing tasks [58]. It is worth to note that meta-learning (_e.g.,_ matching networks [64], MAML [16], VERSA [23], PLATIPUS [17]) leverages an episodic learning paradigm to achieve few-shot adaptation, where both meta-training and meta-testing are done on the task level in an episodic fashion [69; 37] with a large number of \(N\)-way \(K\)-shot training tasks. In this work, we consider more challenging few-shot adaptation tasks (_e.g.,_ 100-way 1-shot in Cifar100 and 102-way 1-shot in Flowers102) compared to the commonly used 5-way 1-shot meta-learning tasks. We leverage the power of the pre-trained foundation models, which eliminates the need of task based episodic meta-training. From a meta-learning perspective, the pre-training phase for the foundation model could be viewed as performing meta-knowledge acquisition similar to meta-training. The pre-trained model can be seen as an expert equipped with the meta-knowledge, and parameter-efficient fine-tuning performs quick adaptation to the downstream tasks, analogous to the support-set based adaptation done in meta-testing.

## 3 Bayesian Parameter-Efficient Fine-Tuning of Foundation Models

We start by introducing some fundamental concepts from evidential learning, which will serve as key building blocks in the proposed Bayesian-PEFT framework. We then detail the two Bayesian components: base rate adjustment to address under-confidence and diversity-inducing evidential ensemble to improve the reliability on both prediction accuracy and uncertainty quantification.

### Preliminaries

Evidential Deep Learning (EDL) [56] introduces a computationally efficient framework to transform deterministic deep learning (DL) models into uncertainty-aware models. The key idea is to introduce a higher-order conjugate prior distribution over the predicted likelihood distribution and train the DL model to output parameters of the higher-order distribution. Towards classification, EDL models[56; 11] introduce Dirichlet prior distribution for the multinomial likelihood distribution. Specifically, the output softmax layer of the DL model is replaced by a monotonic, non-negative transformation function (_e.g.,_ ReLU, SoftPlus, or \(\exp\)) to obtain the evidence for different classes that are transformed into the Dirichlet parameters. Mathematically, for a DL model \(f_{\theta}(\cdot)\), and an input sample \(\mathbf{x}\), we have

\[e_{i}=\mathcal{E}\big{(}f_{\theta}(\mathbf{x})\big{)}_{i}\quad\alpha_{i}=e_{i }+a_{i}\times W\] (1)

where \(e_{i}\) is the output evidence for the \(i^{\text{th}}\) class from the model \(f_{\theta}(\cdot)\) and input sample \(\mathbf{x}\), \(a_{i}\) is the base rate for the \(i^{\text{th}}\) class, \(W\) is the non-informative prior weight usually set to the number of classes, \(\mathcal{E}\) is the non-negative transformation function, and \(\alpha_{i}\) parameterizes a Dirichlet distribution. Existing EDL works usually adopt a non-informative base rate of \(a_{i}=\frac{1}{N}\forall i\in[1,N]\). Furthermore, a multinomial distribution \(\texttt{{Mult}}(y|\mathbf{p})\) over labels is parameterized as \(\mathbb{E}[p_{i}]=\frac{\alpha_{i}}{S},\) where the total Dirichlet Strength \(S=\sum_{i=1}^{N}\alpha_{i}\).

An evidential model can be trained via a Type-II Maximum Likelihood-based evidential loss \(\mathcal{L}^{\texttt{Log}}(\bm{x},\bm{y})\)[56] with KL regularization that penalizes evidence assigned to non-ground-truth classes:

\[\mathcal{L}_{\texttt{evid}}(\bm{x},\bm{y})=\mathcal{L}^{\texttt{Log}}(\bm{x}, \bm{y})+\lambda\text{KL}\big{(}\texttt{Dir}(\bm{p}|\bm{\tilde{\alpha}})|| \texttt{Dir}(\bm{p}|\bm{1})\big{)}\] (2)

where \(\bm{\tilde{\alpha}}=\bm{y}+(\bm{1}-\bm{y})\odot\bm{\alpha}\). Once trained, the evidential model can predict an evidence vector \(\bm{e}\) = \((e_{1},e_{2},...e_{N})^{\top}\) for a given test sample \(\bm{x}\). From the predicted evidence, we obtain the model's belief (\(\mathbf{b}\)) over different classes, the correct belief \(b_{\texttt{cer}}\), incorrect belief \(b_{\texttt{ine}}\), and vacuity \(u\) as

\[\mathbf{b}=\frac{\mathbf{e}}{S}\quad,\quad b_{\texttt{cer}}=\sum\mathbf{y} \odot\mathbf{b}\quad,\quad b_{\texttt{inc}}=\sum(\mathbf{1}-\mathbf{y})\odot \mathbf{b}\quad,\quad u=\frac{N}{S},\] (3)

where vacuity \(u\) is a second-order uncertainty [33] that captures the model's lack of knowledge in its prediction; \(\mathbf{b}_{\texttt{cer}}\) and \(\mathbf{b}_{\texttt{inc}}\) quantify model accuracy and error, respectively. However, neither \(\mathbf{b}_{\texttt{cer}}\) nor \(\mathbf{b}_{\texttt{inc}}\) can be evaluated without the ground-truth label, which is not available in the testing phase. Existing theoretical work has established an important connection between \(\mathbf{b}_{\texttt{inc}}\) and dissonance [46], which is another second-order uncertainty [33] that can be quantified without the ground-truth label. More specifically, dissonance \(\texttt{di}\) is can be evaluated as

\[\texttt{{dis}}=\sum_{n=1}^{N}\Big{(}b_{n}\frac{\sum_{j\neq n}b_{j}Bal(b_{j},b_ {n})}{\sum_{j\neq n}b_{j}}\Big{)},\quad Bal(b_{j},b_{n})=\begin{cases}2\frac{ \min(b_{j},b_{n})}{b_{j}+b_{n}},&\text{if }b_{i}b_{j}>0\\ 0,&\text{otherwise}\end{cases}\] (4)

where \(Bal(\cdot,\cdot)\) is the relative mass balance function between two belief masses. The dissonance essentially captures the conflicting belief assigned to different classes [57].

### Strengthening the prior belief through base rate adjustment

To gain a deeper understanding of the under-confident few-shot adaptation behavior of foundation models, we perform fine-grained uncertainty analysis using the predicted evidence from an evidential model. To this end, we replace the softmax layer in a VPT fine-tuned transformer model with an exponential-based evidential head that outputs non-negative evidence for different classes. We analyze the output evidence from the evidential model that reveals some interesting insights.

Why is the model accurate?First, we observe that the relative order of the evidence assigned to different classes is accurate. This implies that the model outputs relatively greater evidence for the correct class compared to all other classes that ensure the model's strong predictive performance. To more precisely quantify the model's accuracy, we adapt the lower bound of incorrect belief theorem developed for meta-learning [46] to evaluate the model accuracy through its predicted dissonance:

**Theorem 1**.: _Consider an evidential model that outputs incorrect belief of \(b_{\texttt{inc}}\) and the dissonance in the beliefs is \(\texttt{{dis}}\). Then, the incorrect belief of the model will be at least half of the dissonance for all predictions from the evidential model._

\[\frac{1}{2}\texttt{{dis}}\leq b_{\texttt{inc}}\quad\text{where}\quad 0\leq \texttt{{dis}}\leq 1\quad\&\quad 0\leq b_{\texttt{inc}}\leq 1\] (5)

Figure 2(a) shows the test accuracy vs. dissonance curve, which is aligned with the relationship between the incorrect belief and the dissonance given in the theorem above, where a low dissonance implies a low \(b_{\texttt{inc}}\) (or a high accuracy). From all the testing samples, we observe relatively low dissonance and the highest is only slightly above 0.7 as shown in the figure. We further evaluate the Area Under the Curve of the Accuracy vs. \((1-\texttt{{dis}})\) and obtain an AUC of 0.82 as shown in Figure 2(b). This implies the model is able to clearly discriminate the ground-truth label from the rest without much confusion (_i.e.,_ low dissonance) which ensures its good prediction accuracy.

Why is the model under-confident?Despite being able to assign relatively more evidence to the correct label over the rest, it is also interesting to observe that the model generally assigns very low evidence to all the labels, including the correct one. Figure 2(c) shows the evidence distribution of one representative test data sample from Cifar100. As can be seen, most classes are assigned very low evidence that is close to zero. The ground-truth class is assigned higher evidence, but it is far from sufficient to make the prediction confident. The resultant confidence is only 0.025 while the vacuity is extremely high at 0.875, implying that the model _believes_ it has very limited knowledge of the data sample despite it correctly identifying the correct label. Figure 2(d) shows the predicted vacuity over all the test samples, most of which are assigned a very high vacuity. This confirms the overly conservative behavior of the model, where _the low confidence is primarily due to the insufficient allocation of evidence_ in its predictions. On the other hand, since the model is fairly accurate, it is reasonable to believe that the model underestimates the contribution of the rich prior knowledge gained through pre-training.

Base rate adjustment to strength the prior belief.The fine-grained uncertainty analysis through the lenses of evidential learning not only explains the good predictive performance of the few-shot adapted model through PEFT but also unveils the root cause for its under-confident behavior, which is under-estimation of the contribution from the prior knowledge to the downstream task. While the classical Bayes' theorem offers a principal idea to address the issue, which is to strengthen the prior belief, there is a lack of practical way to achieve this. To this end, we propose to leverage the base rate introduced by the subjective logic theory [33] as an effective vehicle to adjust the prior belief gained through pre-training. According to (1), adjusting the base rate has the effect of changing the Dirichlet parameter \(\alpha\), which will change the confidence for the prediction given by \(\max_{i}\mathbb{E}[p_{i}]\). However, base rate adjustment needs to meet two key requirements: (1) the relative order of the Dirichlet parameters assigned to different classes should be preserved so that the predictive performance of the model remains unaffected, (2) the gap between the Dirichlet parameters for different classes is transformed such that the model becomes more confident in its predictions, making it well-calibrated. To meet these requirements, we we propose a transformation function \(\mathcal{A}_{m}\) to the model's output evidence such that the model is well calibrated without any compromise in the generalization performance:

\[\boldsymbol{\alpha}=\mathcal{A}_{m}\big{(}f_{\theta}(\mathbf{x_{i}})\big{)}= \mathbf{e}+W\boldsymbol{\chi}\quad,\quad\chi_{i}=a_{i}^{\text{adj}}=\Big{(} \frac{e_{i}-e_{\texttt{min}}}{e_{\texttt{min}}}\Big{)}^{m}\] (6)

where \(\boldsymbol{\chi}=(\chi_{1},\chi_{2},...\chi_{N})^{\top}\) is the adjusted base rate, and \(m\geq 1\) controls the base rate transformation. The adjusted base rate \(\boldsymbol{\chi}\) considers evidence of all classes as a reference via \(e_{\texttt{min}}\), and transforms the gap between different class evidences such that the model is well calibrated.

**Lemma 2**.: _The base-rate adjusted model that uses learnable base rate \(\boldsymbol{\chi}=(\chi_{1},\chi_{2},...,\chi_{N})^{\top}\) has the same generalization performance compared to the model using fixed base rate of \(a_{i}=\frac{1}{N}\forall i\in[1,N]\)_

**Theorem 3**.: _For any \(m\geq 1\), the transformation function \(\mathcal{A}_{m}\) transforms the base rate for the class with the highest evidence \(e_{\texttt{max}}\) and class with the second highest evidence \(e_{\texttt{2nd}}\) such that the gap in Dirichlet parameters between the two classes is non-decreasing._

**Remark**.: Theorem 3 ensures that the expected probability \(\mathbb{E}[p_{i}]\) for the predicted class \(i\) has an increased gap with the rest of the classes, which results in an increase of the model's confidence. Therefore, if the prediction is accurate, the model's calibration performance will be improved. Meanwhile, Lemma 2 ensures that the good prediction accuracy of the model is maintained by the proposed base rate adjustment strategy. The detailed proofs are given in Appendix D.

### Building A Diversity Induced Evidential Ensemble

The second Bayesian component of the proposed B-PEFT framework aims to further improve the reliability of both prediction accuracy and uncertainty quantification when performing few-shot adap

Figure 3: 1-shot Cifar10 results and evidence vacuity trends

tation. It performs Bayesian model averaging by building a diversity-inducing evidential ensemble. The ensemble of deep learning models (_i.e.,_ deep ensemble) [20, 36] can effectively improve the generalization performance of deep learning models. Moreover, deep ensembles can capture the model uncertainty [36, 53] via the agreement-disagreement between the ensemble components. Model uncertainty essentially captures the uncertainty in the model parameters, which is denoted as \(\theta\) of the graphical model of B-PEFT as shown in Figure 4(b). The schematic diagram of B-PEFT is shown in Figure 4(a). The model uncertainty can be leveraged to evaluate the reliability of fine-grained uncertainty output by the evidential model.

The effectiveness of the ensembles has been empirically demonstrated across multiple datasets/settings [30] with theoretical guarantees [2]. However, standard deep ensembling leads to limited diversity among the ensemble components as it only considers the random initialization of components. We propose a novel diversity-inducing ensembling scheme for the evidential models. Similar to the deep ensemble [36], we also consider randomly initialized evidential models. We additionally train each ensemble component with different strengths for incorrect evidence regularization along with evidential loss objective. The overall objective for each ensemble component is identical to (2).

However, each ensemble component is trained with different incorrect evidence (or belief) regularization strengths (_i.e.,_ different components place different priorities for the minimization of incorrect evidence over the maximization of correct evidence) which leads to diversity among the components. Since each component's priority for minimizing the incorrect evidence is different, the components focus on different attributes/features in the data that help the model avoid overfitting to an identical set of discriminative features. As a result, the proposed evidential ensembling scheme implicitly pushes the ensemble components away from each other, making it equivalent to the repulsive force in the Stein Variational Gradient Descent (SVGD) [12, 13].

**Lemma 4**.: _For given incorrect evidence regularization \(\mathcal{L}_{\text{reg}}^{\text{inc}}\), and E ensemble components with regularization strengths \(\lambda_{p},p\in[1,P]\), the ensemble components in the evidence space are implicitly pushed away from each other by a force \(\lambda_{p}\nabla\mathcal{L}_{\text{reg}}^{\text{inc}}\) that acts identical to the repulsive force in Stein Variational Gradient Descent (SVGD) based ensembles._

Remark.The detailed proof is given in the Appendix. We present an intuitive visualization of the update of the evidential ensemble model for different strengths (\(\lambda_{1}<\lambda_{2}<...<\lambda_{P}\)) of incorrect evidence regularization for different seeds in Figure 5. Each ensemble component aims to maximize the likelihood (direction \(\overrightarrow{A}\)) and minimize incorrect

Figure 4: (a) Schematic diagram and (b) Graphical model of the B-PEFT model

Figure 5: Illustration of ensemble diversity achieved through incorrect belief regularization with different strength

evidence (direction \(\overrightarrow{B}\)). The strengths of incorrect evidence regularization (direction \(\overrightarrow{B}\)) are different for each ensemble component that acts as an implicit repulsive force among the ensemble components, ensuring that they are diverse from each other. Different from the SVGD-based ensemble, in our proposed model, the particles do not need to explicitly communicate with each other making our proposed approach computationally efficient, scalable, and generalizable.

## 4 Experiments and Results

Experiment setup, datasets, and baselines.We consider \(K\)-shot adaptation (_i.e.,_ the dataset has \(K\) examples per class in the training set) with Cifar10 [1], Cifar100 [1], Food101 [8], and Flowers102 [45] datasets. For instance, the \(2\)-shot Cifar100 dataset has \(2\) examples per class leading to a total of 200 labeled training samples. For all datasets and experiments, the training set is a few-shot dataset, and the evaluation is done on the standard test set available with benchmark datasets. Details of the few-shot training datasets along with additional experiment details are presented in the Appendix E. We consider large pre-trained vision transformer with ViT backbone [15] and consider Visual Prompt Tuning (VPT) [32], along with bias fine-tuning [9] and adapter fine-tuning [72] as the PEFT techniques (We use VPT as the representative PEFT where not specified due to its superior performance). We consider accuracy-preserving post-hoc calibration techniques including Temperature Scaling (TS) [26], Parameterized Temperature Scaling (PTS) [61], and Isotonic Regression (IR-MC) [6] as the baseline calibration techniques.

Prediction and calibration performance.We first consider standard cross-entropy (CE)-based PEFT of the supervised pre-trained ViT model on few-shot datasets. We present the accuracy and calibration results of VPT in Table 1 (a). We observe that the straightforward adaption of the models leads to accurate but under-confident models as indicated by a high ECE. The evidential models as shown in Table 1 (b) have comparable or better generalization performance across the datasets/settings. However, these models are also severely under-confident similar to CE-based models indicated by high ECE and accuracy-confidence trends (see Figure (a)a, (b)b in the Appendix). The overall performance of the calibrated evidential model using base-rate adjustment is presented in Table 1 (c). As can be seen, the accuracy remains the same as the adjusted base rate expands the gap between evidence of the class and preserves the relative order in the predicted class evidence. It effectively tackles the under-confidence issue, which leads to a significant improvement in the overall ECE performance across the datasets and settings (also see Figure (c)c in the Appendix). Table 1 (d) shows

\begin{table}
\begin{tabular}{l|c|c|c|c c|c c} \hline \hline \multirow{2}{*}{K (Shot)} & \multicolumn{2}{c|}{**Cifar10**} & \multicolumn{2}{c|}{**Cifar100**} & \multicolumn{2}{c|}{**Food101**} & \multicolumn{2}{c}{**Flowers102**} \\ \cline{2-9}  & Accuracy \(\uparrow\) & ECE \(\downarrow\) & Accuracy \(\uparrow\) & ECE \(\downarrow\) & Accuracy \(\uparrow\) & ECE \(\downarrow\) & Accuracy \(\uparrow\) & ECE \(\downarrow\) \\ \hline \multicolumn{9}{c}{**(a) Standard Model**} \\ \hline
1-Shot & \(69.578_{\pm 1.31}\) & \(0.437_{\pm 0.010}\) & \(48.637_{\pm 0.757}\) & \(0.393_{\pm 0.008}\) & \(35.702_{\pm 1.055}\) & \(0.263_{\pm 0.009}\) & \(88.161_{\pm 0.91}\) & \(0.61_{\pm 0.004}\) \\
2-Shot & \(81.771_{\pm 1.33}\) & \(0.400_{\pm 0.016}\) & \(64.501_{\pm 0.030}\) & \(0.494_{\pm 0.002}\) & \(53.954_{\pm 0.659}\) & \(0.39_{\pm 0.004}\) & \(93.462_{\pm 1.072}\) & \(0.55_{\pm 0.006}\) \\
5-Shot & \(88.707_{\pm 0.423}\) & \(0.255_{\pm 0.008}\) & \(76.758_{\pm 0.525}\) & \(0.517_{\pm 0.001}\) & \(65.856_{\pm 0.197}\) & \(0.424_{\pm 0.002}\) & \(97.363_{\pm 0.165}\) & \(0.472_{\pm 0.013}\) \\
10-Shot & \(91.061_{\pm 0.217}\) & \(0.212_{\pm 0.005}\) & \(80.720_{\pm 0.329}\) & \(0.501_{\pm 0.003}\) & \(71.566_{\pm 0.069}\) & \(0.444_{\pm 0.003}\) & \(98.244_{\pm 0.114}\) & \(0.439_{\pm 0.018}\) \\
20-Shot & \(92.678_{\pm 0.37}\) & \(0.166_{\pm 0.004}\) & \(82.608_{\pm 0.206}\) & \(0.487_{\pm 0.004}\) & \(74.914_{\pm 0.178}\) & \(0.460_{\pm 0.003}\) & \(98.431_{\pm 0.100}\) & \(0.425_{\pm 0.017}\) \\ \hline \multicolumn{9}{c}{**(b) Evidential Model**} \\ \hline
1-Shot & \(70.197_{\pm 1.013}\) & \(0.557_{\pm 0.011}\) & \(51.127_{\pm 0.045}\) & \(0.499_{\pm 0.004}\) & \(36.297_{\pm 1.407}\) & \(0.349_{\pm 0.014}\) & \(89.225_{\pm 1.03}\) & \(0.846_{\pm 0.004}\) \\
2-Shot & \(81.613_{\pm 1.716}\) & \(0.553_{\pm 0.01}\) & \(65.545_{\pm 0.349}\) & \(0.620_{\pm 0.004}\) & \(52.855_{\pm 0.551}\) & \(0.485_{\pm 0.005}\) & \(95.071_{\pm 0.413}\) & \(0.874_{\pm 0.006}\) \\
5-Shot & \(88.764_{\pm 0.896}\) & \(0.391_{\pm 0.015}\) & \(77.561_{\pm 0.716}\) & \(0.744_{\pm 0.006}\) & \(65.135_{\pm 0.27}\) & \(0.536_{\pm 0.005}\) & \(97.602_{\pm 0.199}\) & \(0.686_{\pm 0.02}\) \\
10-Shot & \(92.014_{\pm 0.353}\) & \(0.388_{\pm 0.006}\) & \(81.561_{\pm 0.291}\) & \(0.765_{\pm 0.002}\) & \(70.863_{\pm 0.261}\) & \(0.673_{\pm 0.003}\) & \(98.326_{\pm 0.233}\) & \(0.444_{\pm 0.008}\) \\
20-Shot & \(93.029_{\pm 0.239}\) & \(0.360_{\pm 0.015}\) & \(83.100_{\pm 0.184}\) & \(0.782_{\pm 0.001}\) & \(72.060_{\pm 0.309}\) & \(0.599_{\pm 0.003}\) & \(98.708_{\pm 0.014}\) & \(0.411_{\pm 0.013}\) \\ \hline \hline \multicolumn{9}{c}{**(c) Base-rate adjusted Evidential Model** (Cabibrated Evidential Model)**} \\ \hline
1-Shot & \(70.197_{\pm 1.013}\) & \(0.027_{\pm 1.127}\) & \(0.035_{\pm 0.037}\) & \(0.074_{\pm 0.004}\) & \(36.297_{\pm 1.407}\) & \(0.081_{\pm 0.011}\) & \(89.225_{\pm 1.03}\) & \(0.025_{\pm 0.004}\) \\
2-Shot & \(81.613_{\pm 1.716}\) & \(0.040_{\pm 0.013}\) & \(65.545_{\pm 0.339}\) & \(0.08_{\pm 0.003}\) & \(52.855_{\pm 0.551}\) & \(0.063_{\pm 0.006}\) & \(95.071_{\pm 0.413}\) & \(0.023_{\pm 0.003}\) \\
5-Shot & \(88.764_{\pm 0.896}\) & \(0.028_{\pm 0.006}\) & \(77.561_{\pm 0.716}\) & \(0.044_{\pm 0.002}\) & \(65.135_{\pm 0.270}\) & \(0.037_{\pm 0.003}\) & \(97.602_{\pm 0.199}\) & \(0.015_{\pm 0.002}\) \\
10-Shot & \(92.014_{\pm 0.353}\

[MISSING_PAGE_FAIL:9]

7. We observe that when only a single component outputs a low vacuity for the OOD samples in Figure 7 (a-b), the variance of the ensemble is high, implying a high model uncertainty. When all the components output a high vacuity to the OOD sample in Figure 7 (c), the variance is also low, implying a low model uncertainty.

Ablation study.We carry out ablation with \(1\)-shot Cifar100 dataset to study the impact of the base rate transformation order \(m\) (Section 3.2) for different strengths of incorrect evidence regularization on the calibration performance. As we increase \(m\), the probability gap between classes improves, leading to more confident predictions. However, the model starts to become overconfident for large \(m\) values (see Figure 8).Moreover, with an increase in incorrect evidence regularization strength, the optimal \(m\) value decreases to a smaller value (_e.g.,_ optimal \(m\) = 2.0 for \(\lambda=0.1\), and optimal \(m=1.5\) for \(\lambda\) = 10). Choosing a proper \(m\) leads to the best-calibrated evidential model. We present additional results studying the impact of \(m\) in Appendix F.1.

Limited by space, we provide results comparing meta-learning methods on standard few-shot tasks in Appendix F.4, discuss impact of various components (number of classes, data size, and unfrozen parameters) in Appendix F.5, and include additional experiments and comparisons, including OOD settings, in Appendix F.6. Moreover, we carry out additional ablation experiments to study the impact of incorrect evidence regularization strength (Appendix F.2), and the impact of ensemble components (Appendix F.3). We further carry out experiments and discuss applying PEFT to foundation models pre-trained in a self-supervised fashion (Appendix G). We discuss the societal impact and limitations of the work in Appendices H and I, respectively.

## 5 Conclusion

In this work, we focus on transformer-based large vision foundation models and investigate different parameter-efficient fine-tuning techniques for effective few-shot adaptation. We observe that the existing models are severely under-confident, especially in challenging datasets and settings. Moreover, existing models lack fine-grained uncertainty quantification capabilities. We extend the models to uncertainty-aware evidential models, and resort to the evidential framework to develop a novel Bayesian parameter efficient fine-tuning (B-PEFT) framework that integrates evidence-based base rate adjustment to addresses the under-confidence and a diversity inducing evidential ensemble technique to further improve the reliability in model prediction and uncertainty quantification. The B-PEFT framework possesses theoretically sound properties to ensure its superior generalization capability and robust calibration behavior. We carry out intensive experiments across different benchmark datasets and diverse few-shot settings that demonstrate the outstanding performance of B-PEFT.

Figure 8: Impact of \(m\)

Figure 6: (a-b): Vacuity distribution of a single model and (c-d): variance distribution of ensemble models for 1/5 shots cifar10 as In-Distribution and Cifar100 as Out-of-Distribution dataset

Figure 7: Qualitative analysis of OOD samples for 1-shot adaptation of Cifar10 as ID dataset and Cifar100 as OOD dataset

## Acknowledgments

This research was supported in part by an NSF IIS award IIS-1814450. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agency. We would like to thank the anonymous reviewers for their constructive comments.

## References

* [1] Krizhevsky Alex. Learning multiple layers of features from tiny images. _https://www. cs. toronto. edu/kriz/learning-features-2009-TR. pdf_, 2009.
* [2] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. _arXiv preprint arXiv:2012.09816_, 2020.
* [3] Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. Deep evidential regression. _Advances in Neural Information Processing Systems_, 33:14927-14937, 2020.
* [4] Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. _arXiv preprint arXiv:2002.06470_, 2020.
* [5] Wentao Bao, Qi Yu, and Yu Kong. Evidential deep learning for open set action recognition. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13349-13358, 2021.
* [6] Eugene Berta, Francis Bach, and Michael Jordan. Classifier calibration with roc-regularized isotonic regression. In _International Conference on Artificial Intelligence and Statistics_, pages 1972-1980. PMLR, 2024.
* [7] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In _International conference on machine learning_, pages 1613-1622. PMLR, 2015.
* mining discriminative components with random forests. In _European Conference on Computer Vision_, 2014.
* [9] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for efficient on-device learning. _Advances in Neural Information Processing Systems_, 33:11285-11297, 2020.
* [10] Bertrand Charpentier, Oliver Borchert, Daniel Zugner, Simon Geisler, and Stephan Gunnemann. Natural posterior network: Deep bayesian predictive uncertainty for exponential family distributions. In _International Conference on Learning Representations_, 2022.
* [11] Bertrand Charpentier, Daniel Zugner, and Stephan Gunnemann. Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts. _Advances in Neural Information Processing Systems_, 33:1356-1367, 2020.
* [12] Francesco D'Angelo and Vincent Fortuin. Repulsive deep ensembles are bayesian. _Advances in Neural Information Processing Systems_, 34:3451-3465, 2021.
* [13] Francesco D'Angelo, Vincent Fortuin, and Florian Wenzel. On stein variational neural network ensembles. _arXiv preprint arXiv:2106.10760_, 2021.
* [14] Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. Laplace redux-effortless bayesian deep learning. _Advances in Neural Information Processing Systems_, 34:20089-20103, 2021.
* [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.

* Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _Proceedings of the 34th International Conference on Machine Learning-Volume 70_, pages 1126-1135. JMLR. org, 2017.
* Finn et al. [2018] Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In _Advances in Neural Information Processing Systems_, pages 9516-9527, 2018.
* Franchi et al. [2024] Gianni Franchi, Olivier Laurent, Maxence Leguery, Andrei Bursuc, Andrea Pilzer, and Angela Yao. Make me a bnn: A simple strategy for estimating bayesian uncertainty from pre-trained models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12194-12204, 2024.
* Gal and Ghahramani [2016] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In _international conference on machine learning_, pages 1050-1059. PMLR, 2016.
* Ganaie et al. [2022] Mudasir A Ganaie, Minghui Hu, Ashwani Kumar Malik, Muhammad Tanveer, and Ponnuthurai N Suganthan. Ensemble deep learning: A review. _Engineering Applications of Artificial Intelligence_, 115:105151, 2022.
* Gao et al. [2024] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. _International Journal of Computer Vision_, 132(2):581-595, 2024.
* Gidaris and Komodakis [2018] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4367-4375, 2018.
* Gordon et al. [2018] Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard E Turner. Meta-learning probabilistic inference for prediction. _arXiv preprint arXiv:1805.09921_, 2018.
* Grant et al. [2018] Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-based meta-learning as hierarchical bayes. In _International Conference on Learning Representations_, 2018.
* Gu et al. [2021] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. Ppt: Pre-trained prompt tuning for few-shot learning. _arXiv preprint arXiv:2109.04332_, 2021.
* Guo et al. [2017] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330. PMLR, 2017.
* Han et al. [2023] Cheng Han, Qifan Wang, Yiming Cui, Zhiwen Cao, Wenguan Wang, Siyuan Qi, and Dongfang Liu. E^ 2vpt: An effective and efficient approach for visual prompt tuning. _arXiv preprint arXiv:2307.13770_, 2023.
* Han et al. [2024] Cheng Han, Qifan Wang, Yiming Cui, Wenguan Wang, Lifu Huang, Siyuan Qi, and Dongfang Liu. Facing the elephant in the room: Visual prompt tuning or full finetuning? _arXiv preprint arXiv:2401.12902_, 2024.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Huang et al. [2017] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. _arXiv preprint arXiv:1704.00109_, 2017.
* Huang et al. [2017] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.
* Jia et al. [2022] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In _European Conference on Computer Vision_, pages 709-727. Springer, 2022.

* [33] Audun Josang. _Subjective logic_, volume 3. Springer, 2016.
* [34] Audun Josang, Jin-Hee Cho, and Feng Chen. Uncertainty characteristics of subjective opinions. In _2018 21st International Conference on Information Fusion (FUSION)_, pages 1998-2005. IEEE, 2018.
* [35] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _International journal of computer vision_, 128(7):1956-1981, 2020.
* [36] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in neural information processing systems_, 30, 2017.
* [37] Hae Beom Lee, Hayeon Lee, Donghyun Na, Saehoon Kim, Minseop Park, Eunho Yang, and Sung Ju Hwang. Learning to balance: Bayesian meta-learning for imbalanced and out-of-distribution tasks. In _Eighth International Conference on Learning Representations, ICLR 2020_. ICLR, 2020.
* [38] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. _arXiv preprint arXiv:2104.08691_, 2021.
* [39] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In _Proceedings of the IEEE international conference on computer vision_, pages 2980-2988, 2017.
* [40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* [41] Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. _Advances in neural information processing systems_, 31, 2018.
* [42] Aryan Mobiny, Pengyu Yuan, Supratik K Moulik, Naveen Garg, Carol C Wu, and Hien Van Nguyen. Dropconnect is effective in modeling uncertainty of bayesian deep networks. _Scientific reports_, 11(1):5458, 2021.
* [43] Azadeh Sadat Mozafari, Hugo Siqueira Gomes, Wilson Leao, Steeven Janny, and Christian Gagne. Attended temperature scaling: a practical approach for calibrating deep neural networks. _arXiv preprint arXiv:1810.11586_, 2018.
* [44] Rafael Muller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? _Advances in neural information processing systems_, 32, 2019.
* [45] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _2008 Sixth Indian conference on computer vision, graphics & image processing_, pages 722-729. IEEE, 2008.
* [46] Deep Shankar Pandey and Qi Yu. Multidimensional belief quantification for label-efficient meta-learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 14391-14400, June 2022.
* [47] Deep Shankar Pandey and Qi Yu. Evidential conditional neural processes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 9389-9397, 2023.
* [48] Deep Shankar Pandey and Qi Yu. Learn to accumulate evidence from all training samples: Theory and practice. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 26963-26989. PMLR, 23-29 Jul 2023.
* [49] Tim Pearce, Felix Leibfried, and Alexandra Brintrup. Uncertainty in neural networks: Approximately bayesian ensembling. In _International conference on artificial intelligence and statistics_, pages 234-244. PMLR, 2020.

* [50] Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. _arXiv preprint arXiv:1701.06548_, 2017.
* [51] Jonas Pfeiffer, Andreas Ruckle, Clifton Poth, Aishwarya Kamath, Ivan Vulic, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. Adapterhub: A framework for adapting transformers. _arXiv preprint arXiv:2007.07779_, 2020.
* [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [53] Rahul Rahaman et al. Uncertainty quantification and deep ensembles. _Advances in Neural Information Processing Systems_, 34:20063-20075, 2021.
* [54] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. _Advances in neural information processing systems_, 30, 2017.
* [55] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses. _arXiv preprint arXiv:2104.10972_, 2021.
* [56] Murat Sensoy, Lance Kaplan, and Melih Kandemir. Evidential deep learning to quantify classification uncertainty. _Advances in neural information processing systems_, 31, 2018.
* [57] Weishi Shi, Xujiang Zhao, Feng Chen, and Qi Yu. Multifaceted uncertainty estimation for label-efficient deep learning. _Advances in neural information processing systems_, 33, 2020.
* [58] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5, 2023.
* [60] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* [61] Christian Tomani, Daniel Cremers, and Florian Buettner. Parameterized temperature scaling for boosting the expressive power in post-hoc uncertainty calibration. In _In European Conference on Computer Vision (ECCV)_, 2022.
* [62] Dennis Ulmer. A survey on evidential deep learning for single-pass uncertainty estimation. _arXiv preprint arXiv:2110.03051_, 2021.
* [63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [64] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. _Advances in neural information processing systems_, 29, 2016.
* [65] Cheng Wang. Calibration in deep learning: A survey of the state-of-the-art. _arXiv preprint arXiv:2308.01222_, 2023.
* [66] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, and Wen Gao. Large-scale multi-modal pre-trained models: A comprehensive survey. _Machine Intelligence Research_, 20(4):447-482, 2023.

* [67] Jingyuan Wen, Yutian Luo, Nanyi Fei, Guoxing Yang, Zhiwu Lu, Hao Jiang, Jie Jiang, and Zhao Cao. Visual prompt tuning for few-shot text classification. In _Proceedings of the 29th International Conference on Computational Linguistics_, pages 5560-5570, 2022.
* [68] Liqi Yan, Cheng Han, Zenglin Xu, Dongfang Liu, and Qifan Wang. Prompt learns prompt: exploring knowledge-aware generative prompt collaboration for video captioning. In _Proceedings of international joint conference on artificial intelligence (IJCAI)_, pages 1622-1630, 2023.
* [69] Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn. Bayesian model-agnostic meta-learning. In _Advances in Neural Information Processing Systems_, pages 7332-7342, 2018.
* [70] Yu Yu, Chao-Han Huck Yang, Jari Kolehmainen, Prashanth G Shivakumar, Yile Gu, Sungho Ryu Roger Ren, Qi Luo, Aditya Gourav, I-Fan Chen, Yi-Chieh Liu, et al. Low-rank adaptation of large language model rescoring for parameter-efficient speech recognition. In _2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)_, pages 1-8. IEEE, 2023.
* [71] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. _arXiv preprint arXiv:2106.10199_, 2021.
* [72] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning: a baseline for network adaptation via additive side networks. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16_, pages 698-714. Springer, 2020.
* [73] Jize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning. In _International conference on machine learning_, pages 11117-11128. PMLR, 2020.
* [74] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves calibration. In _International Conference on Machine Learning_, pages 26135-26160. PMLR, 2022.
* [75] Xujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. Uncertainty aware semi-supervised learning on graph data. _Advances in Neural Information Processing Systems_, 33:12827-12836, 2020.
* [76] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16816-16825, 2022.
* [77] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. _International Journal of Computer Vision_, 130(9):2337-2348, 2022.

Supplementary Material

## Appendix

Table of Contents

* A Organization of the Appendix
* B Summary of the Symbols
* C Further Discussion on Uncertainty-Aware Deep Learning
* C.1 Existing Uncertainty Quantification Methods in Deep Learning
* C.2 Evidential Deep Learning Models for Classification
* C.3 Evidential models vs. Standard Bayesian Models
* D Proofs of Theoretical Results
* D.1 Proof of Lemma 2
* D.2 Proof of Theorem 3
* D.3 Connection with SVGD-based Bayesian Ensembling and Proof of Lemma 4
* E Dataset and Implementation Details
* F Additional Experiments
* F.1 Impact of \(m\) on Expected Calibration Error
* F.2 Impact of Incorrect Evidence Regularization Strength (\(\lambda\))
* F.3 Impact of Ensemble Components
* F.4 Few Shot Learning Results
* F.5 Impact of Different Components
* F.6 Additional Experiments and Comparison
* G Calibration Behavior of Self-Supervised Model
* H Societal Impact
* I Limitations and Future WorkOrganization of the Appendix

* In Section B, we present the table with summary of the key symbols used in this work.
* In Section C, we present related works in uncertainty-aware deep learning, describe evidential deep learning for classification in details, and discuss some key advantages of using evidential models to address the under-confidence issue over standard Bayesian models.
* In Section D, we present proof of all our theoretical claims.
* In Section E, we present the details of hyperparameters and additional experiment details.
* In Section F, we present additional experiment results including the impact of m for calibration performance, comparison results with meta-learning methods, the impact of the number of classes, data size, and unfrozen parameters, comparison in OOD settings, the impact of incorrect evidence regularization strength, the results of additional PEFT methods, and the impact of ensemble components.
* In Section G, we discuss the calibration and accuracy behavior for PEFT of large foundation models pre-trained in a self-supervised fashion.
* In Section H, we discuss societal impact of our work.
* In Section I, we discuss the limitations of our work and present potential future direction.

The source code for the experiments carried out in this work is attached in the supplementary materials and is available at the link: https://github.com/rtimininglab/B-PEFT

## Appendix B Summary of the Symbols

## Appendix C Further Discussion on Uncertainty-Aware Deep Learning

### Existing Uncertainty Quantification Methods in Deep Learning

Accurate quantification of predictive uncertainty is essential for the development of trustworthy Deep Learning (DL) models. To this end, DL models have been augmented to become uncertainty-aware using a variety of approaches such as ensemble-based approaches [36; 49], bayesian neural networks based approaches [42; 19; 7], and deterministic neural network based approaches [56; 11; 3]. Deep ensemble techniques [36; 49] construct an ensemble of neural networks, and the agreement/disagreement across the ensemble components is used to quantify different uncertainties. Alternatively, Bayesian neural networks [19][7][42] have been developed that consider a Bayesian formalism (_e.g.,_ bayes-by-backprop [7], dropout during test [19]) to quantify different uncertainties.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Symbol** & **Definition** \\ \hline \(\mathbf{x}\) & Input sample vector \\ \hline \(\mathbf{y}\) & Ground truth label as one hot vector \\ \hline \(\mathbf{e},e_{i}\) & The evidence vector, and the evidence for class \(i\) \\ \hline \(a_{i}\) & Fixed base rate for class \(i\), usually set to \(a_{i}=\frac{1}{N}\) \\ \hline \(\alpha_{i}\) & Dirichlet parameter value for class \(i\) \\ \hline \(S=\sum_{i=1}^{N}\alpha_{i}\) & The Dirichlet Strength \\ \hline \(\mathbf{(b},b_{\text{cer}},b_{\text{inc}})\) & The belief vector, Correct belief, and the Incorrect belief \\ \hline \(N\) & Number of classes \\ \hline \(b_{i}\) & Belief for class i \\ \hline \(u\) & Vacuity output by the model \\ \hline \(\mathtt{dis}\) & Dissonance output by the model \\ \hline \(\mathcal{E}\) & Non-negative evidential transformation function (we use exp) \\ \hline \(\chi_{i}\) & Learnable base rate for class \(i\) \\ \hline \(\mathbf{\chi}\) & Learnable base rate vector \\ \hline \(\lambda\) & Incorrect evidence regularization strength \\ \hline \(\odot\) & Element wise multiplication between two vectors \\ \hline \(W\) & Non-informative prior weight \\ \hline \hline \end{tabular}
\end{table}
Table 4: Summary of the symbols and their definitionsABNN [18] introduces Bayesian normalization layers after training of deep learning models, and requires additional training of these layers in a post-hoc manner. Deterministic neural network-based approaches [57; 41; 10] extend the existing neural network to become uncertainty-aware and enable the networks to quantify fine-grained uncertainties with a single forward pass through the network. Evidential deep learning models [56; 5; 75; 10; 10; 62], an instance of deterministic approaches, introduce a conjugate higher-order evidential prior for the likelihood distribution to enable the model to express the fine-grained uncertainties in both classification[56; 11] and regression problems [3; 47]. Towards classification, evidential models [56; 5; 75] introduce higher-order evidential Dirichlet prior to the multinomial likelihood that enables the deterministic neural network model to capture different uncertainty characteristics. In what follows, we first provide some additional details on using evidential learning model to perform classification. We then highlight some important advantage of using evidential models over standard Bayesian models in uncertainty quantification.

### Evidential Deep Learning Models for Classification

Evidential Deep Learning models, based on Subjective Logic theory [33], aim to train the model such that for any new input sample, the model can make predictions, as well as output fine-grained uncertainty information (via vacuity [56] and dissonance [34]). Towards capturing fine-grained uncertainty for classification problems, EDL models assume that the label for each sample is obtained from a generative process with a Dirichlet prior and a multinomial likelihood. The parameters for the Dirichlet prior express the vacuity and belief masses for uncertainty estimation. The conjugacy between the Dirichlet prior and the multinomial likelihood is explored, and different evidential losses are introduced for model training and inference [48]. In this work, we consider Type-II Maximum Likelihood-based evidential loss \(\mathcal{L}^{\mathsf{Log}}(\bm{x},\bm{y})\)[56] with incorrect evidence regularization \(\mathcal{L}^{\mathsf{inc}}_{\mathsf{reg}}(\bm{x},\bm{y})\) given by [56]

\[\mathcal{L}_{\mathsf{evid}}(\bm{x},\bm{y})=\mathcal{L}^{\mathsf{Log}}(\bm{x},\bm{y})+\lambda\times\mathcal{L}^{\mathsf{inc}}_{\mathsf{reg}}(\bm{x},\bm{y})\] (7)

We replace the softmax layer in the head of the VPT model with \(\exp\) activation function. To avoid _zero evidence regions_, we also include the correct evidence regularization \(\mathcal{L}_{\mathsf{cor}}(\bm{x},\bm{y})=-\lambda_{\mathsf{cor}}\log(\alpha_ {gt}-1)\) ( \(\lambda_{\mathsf{cor}}\) is the magnitude of the model's vacuity) in model training objective. The evidential model outputs evidence vector \(\bm{e}=(e_{1},e_{2},...e_{N})\) for a given input \(\bm{x}\) and corresponding ground truth label of \(\bm{y}\). Based on the evidence, Dirichlet parameters are obtained as \(\alpha_{i}=e_{i}+1\). The Type-II Maximum likelihood-based evidential loss is given by

\[\mathcal{L}^{\mathsf{Log}}(\bm{x},\bm{y})=-\ln\int\text{Mult}(\bm{y}|\bm{p}) \text{Dir}(\bm{p}|\bm{\alpha})\text{d}\bm{p}=\log S-\sum_{k=1}^{K}y_{k}\log \alpha_{k}\quad S=\sum_{k=1}^{K}\alpha_{k}\] (8)

The incorrect evidence regularization guides the model to minimize the evidence for all classes other than the ground truth class and can take one of the following forms

1. KL-based incorrect evidence regularization term as in EDL [56] \[\mathcal{L}^{\mathsf{EDL}}_{\mathsf{reg}}(\bm{x},\bm{y}) =\text{KL}\big{(}\text{Dir}(\bm{p}|\bm{\tilde{\alpha}})\|\|\text {Dir}(\bm{p}|\bm{1})\big{)}\] (9) Where \(\bm{\tilde{\alpha}}=\bm{y}+(\bm{1}-\bm{y})\odot\bm{\alpha}=(\tilde{\alpha}_{1 },\tilde{\alpha}_{2},...\tilde{\alpha}_{N})\) parameterize a dirichlet distribution, \(\tilde{\alpha}_{i=gt}=1,\tilde{\alpha}_{i}=\alpha_{i}\forall i\neq gt\), and \(\odot\) represents element-wise product. Here, the KL regularization term encourages the Dirichlet distribution based on the incorrect evidence i.e., \(\text{Dir}(\bm{p}|\bm{\tilde{\alpha}})\) to be flat which is possible when there is no incorrect evidence.
2. Incorrect evidence sum based regularization as in ADL [57] \[\mathcal{L}^{\mathsf{HDL}}_{\mathsf{reg}}(\bm{x},\bm{y})=\sum_{k=1}^{K}\big{(} \mathbf{e}\odot(\bm{1}-\mathbf{y})\big{)}_{k}=\sum_{k=1}^{K}e_{k}\times(1-y_{ k})\] (10)
3. Incorrect belief-sum based regularization as in Units-ML [46] \[\mathcal{L}^{\mathsf{Units}}_{\mathsf{reg}}(\bm{x},\bm{y})=\sum_{k=1}^{K}\big{(} \frac{\mathbf{e}}{S}\odot(\bm{1}-\mathbf{y})\big{)}_{k}=\sum_{k=1}^{K}\frac{e_ {k}}{S}\times(1-y_{k})\] (11)All three regularizations guide the model to minimize the incorrect evidence(ideally close to zero). In our experiments, we consider KL-based incorrect evidence regularization.

### Evidential models vs. Standard Bayesian Models

As compared with the Bayesian-inspired models, evidential learning offers two key properties that allow us to formulate a principled solution to address the unique under-confident behavior of the PEFT methods. First, thanks to its evidence-based fine-grained uncertainty decomposition capability, we can separate two distinct sources of second-order uncertainty, including vacuity and dissonance. Different from the commonly used first-order uncertainty (e.g., entropy), these two second-order uncertainty serve as a key tool to understand why PEFT methods are both accurate (with a low dissonance) while being under-confident (with a high vacuity). This key insight suggests that these methods systematically under-estimate the contribution from the prior knowledge to the downstream task. While the classical Bayes' theorem offers a principal idea to address the issue, which is to strengthen the prior belief, there is a lack of practical way to achieve this. As the second key property, evidential learning allows us to leverage the base rate, which is rooted in the subjective logic theory as an effective vehicle to adjust the prior belief gained through pre-training. To this end, we propose a transformation function in Eq. (6) to adjust the base rate that leads to the increase of the model confidence while maintaining the predictive accuracy of the model as guaranteed by our theoretical results in Lemma 2 and Theorem 3. Furthermore, we develop belief-based diversity for ensemble of evidential models leading to the the B-PEFT model. In theory, evidential deep learning model could be augmented with the Bayesian normalization layers [18] or Bayesian neural networks [7] as an alternative to belief-based diversity of B-PEFT. We leave exploration of different techniques for diversity for Bayesian evidential model as a potential future work.

## Appendix D Proofs of Theoretical Results

In this section, we provide the proofs of the major theoretical results presented in the main paper.

### Proof of Lemma 2

Proof.: Consider an input sample \(\bm{x}\) for which the model outputs the evidence \((e_{1},e_{2},...,e_{N})^{\top}\). Let \(e_{\mathtt{max}}=\max(e_{1},e_{2},...,e_{N})\), and \(e_{\mathtt{min}}=\min(e_{1},e_{2},...,e_{N})\). Here, \(e_{\mathtt{max}}\geq e_{i}\geq e_{\mathtt{min}}\forall i\in[1,N]\). For the evidential model with fixed base rate of \(a_{i}=\frac{1}{N}\forall i\in[1,N]\), the model's predicted class is given by \(e_{\mathtt{pred}}=\arg\max(e_{1}+a_{1}\times W,e_{2}+a_{2}\times W,...,e_{N}+ a_{N}\times W)=\arg\max(e_{1}+1,e_{2}+1,...,e_{N}+1)=\mathtt{Index}(e_{\mathtt{max}})\). For the calibrated model with learnable \(\bm{\chi}=(\chi_{1},\chi_{2},...,\chi_{N})^{\top}\), the model's predicted class is given by \(c^{\mathtt{new}}_{\mathtt{pred}}=\arg\max(\alpha_{1},\alpha_{2},...,\alpha_{N })=\arg\max(e_{1}+\chi_{1}\times W,e_{2}+\chi_{2}\times W,...,e_{N}+\chi_{N} \times W)=\arg\max(e_{1}+N(\frac{e_{1}}{e_{\mathtt{min}}})-N,e_{2}+N(\frac{e_{ 2}}{e_{\mathtt{min}}})-N,...,e_{N}+N(\frac{e_{N}}{e_{\mathtt{min}}})-N)\). Since \(e_{\mathtt{max}}\geq e_{i}\geq e_{\mathtt{min}}\forall i\in[1,N]\), \(\alpha_{\mathtt{max}}\geq\alpha_{i}\geq\alpha_{\mathtt{min}}\forall i\in[1,N]\), and \(c^{\mathtt{new}}_{\mathtt{pred}}=c_{\mathtt{pred}}\) 

### Proof of Theorem 3

Proof.: Consider an input sample \(\mathbf{x}\) for which the model outputs the evidence \((e_{1},e_{2},...,e_{N})^{\top}\). Let \(e_{\mathtt{max}}=\max(e_{1},e_{2},...e_{N})\), \(e_{\mathtt{min}}=\min(e_{1},e_{2},...e_{N})\), and \(e_{\mathtt{max}}\geq e_{\mathtt{2nd}}\geq,...,\geq e_{\mathtt{min}}\). For the evidential model with a fixed base rate of \(a_{i}=\frac{1}{N}\forall i\in[1,N]\), the difference between the Dirichlet parameters for class with maximum evidence and class with the second maximum evidence is given by \(\alpha_{\mathtt{max}}-\alpha_{\mathtt{2nd}}=e_{\mathtt{max}}+a_{\mathtt{max} }W-e_{\mathtt{2nd}}-a_{\mathtt{2nd}}W=e_{\mathtt{max}}-e_{\mathtt{2nd}}\) as \(a_{i}=\frac{1}{N}\forall i\in[1,N]\).

For the calibrated model with learnable \(\bm{\chi}=(\chi_{1},\chi_{2},...,\chi_{N})\), the difference between the Dirichlet parameters for class with maximum evidence and class with the second maximum evidence is given by \(\alpha_{\mathtt{max}}-\alpha_{\mathtt{2nd}}=e_{\mathtt{max}}+\chi_{\mathtt{max }}W-e_{\mathtt{2nd}}-\chi_{\mathtt{2nd}}W=(e_{\mathtt{max}}-e_{\mathtt{2nd}})+ (\chi_{\mathtt{max}}-\chi_{\mathtt{2nd}})W\). Now,

\[\chi_{\mathtt{max}}=\left(\frac{e_{\mathtt{max}}-e_{\mathtt{min}}}{e_{ \mathtt{min}}}\right)^{m}=\left(\frac{e_{\mathtt{max}}}{e_{\mathtt{min}}}-1 \right)^{m}\quad\&\quad\chi_{\mathtt{2nd}}=\left(\frac{e_{\mathtt{2nd}}-e_{ \mathtt{min}}}{e_{\mathtt{min}}}\right)^{m}=\left(\frac{e_{\mathtt{2nd}}}{e_ {\mathtt{min}}}-1\right)^{m}\] (12) \[\text{Or, }\left(\chi_{\mathtt{max}}-\chi_{\mathtt{2nd}}\right)=\left( \frac{e_{\mathtt{max}}}{e_{\mathtt{min}}}-1\right)^{m}-\left(\frac{e_{\mathtt{2nd }}}{e_{\mathtt{min}}}-1\right)^{m}\] (13)

Since \(\frac{e_{i}}{e_{\mathtt{min}}}\geq 1\forall i\in[1,N]\), and \(e_{\mathtt{max}}\geq e_{\mathtt{2nd}}\geq,...,\geq e_{\mathtt{min}}\), \((\chi_{\mathtt{max}}-\chi_{\mathtt{2nd}})\geq 0\forall m>0\). For \(e_{\mathtt{max}}>e_{\mathtt{2nd}}\), \(\&m>0\), \(\chi_{\mathtt{max}}-\chi_{\mathtt{2nd}}>0\). Thus, with the proposed learnable base rate, the gap between the two largest Dirichlet parameters is maintained whenever \(m=0\) and/or \(e_{\mathtt{max}}=e_{2\mathtt{nd}}\). Moreover, whenever \(m\geq 1\) and \(e_{\mathtt{max}}>e_{2\mathtt{nd}}\), the Dirichlet parameter gap between the two classes is increased by a factor of \(\left(\frac{e_{\mathtt{max}}}{e_{\mathtt{min}}}-1\right)^{m}-\left(\frac{e_{ \mathtt{max}}}{e_{\mathtt{min}}}-1\right)^{m}\). 

### Connection with SVGD-based Bayesian Ensembling and Proof of Lemma 4

We first carry out an analysis of the update in Stein Variational Gradient Descent (SVGD) based ensembling [12, 13] that reveals the repulsive force acting among the ensemble components that pushes the particles away and introduces diversity. We then consider ensemble components with different strengths of incorrect evidence regularization \(\mathcal{L}_{\mathtt{reg}}^{\mathtt{inc}}\) and analyze the update to the evidential model in the evidence space that reveals a repulsive diversity-enforcing force acting identical to the SVGD based ensemble.

SVGD update involves randomly initializing the particles and iteratively updating the particles to match the target distribution, which is summarized below.

**Input:**\(\{x_{i}^{0}\}_{i=1}^{N}\): A set of initial parameters, and target distribution density function \(p(x)\)

**For \(L\) iterations**, iteratively update the particles as

* \(x_{i}^{l+1}=x_{i}^{l}+\epsilon_{l}\hat{\phi}^{*}(x_{i}^{l})\), where \(\hat{\phi}^{*}(x)=\frac{1}{E}\sum_{e=1}^{E}k(x_{e}^{l},x)\nabla_{x_{e}^{l}} \log p(x_{e}^{l})+\nabla_{x_{e}^{l}}k(x_{e}^{l},x)\)

Here, \(\epsilon_{l}\) is the step size at iteration \(l\), and \(k(\cdot,\cdot)\) is the kernel function that measures similarity.

**Output:**\(\{x_{i}^{0}\}_{i=1}^{N}\): A set of initial parameters, and target distribution density function \(p(x)\)

For given incorrect evidence regularization \(\mathcal{L}_{\mathtt{reg}}^{\mathtt{inc}}\), and P ensemble components with regularization strengths \(\lambda_{p},p\in[1,P]\), the ensemble components in the evidence space are implicitly pushed away from each other by a force \(\lambda_{p}\nabla\mathcal{L}_{\mathtt{reg}}^{\mathtt{inc}}\) that acts identical to the repulsive force in Stein Variational Gradient Descent (SVGD) based ensembles.

Proof.: For simplicity, consider RBF kernel for \(k(\cdot,\cdot)\)_i.e.,_\(k(a,b)=\exp\left(\frac{-1}{h}(a-b)^{2}\right)\), two particles \(x_{1},x_{2}\), and analyze their updates in SVGD based ensembling. At iteration \(l\), the update to particle \(x_{2}\) is given by

\[\hat{\phi^{*}}(x_{2}^{l})=\frac{1}{2}\Big{(}k(x_{2}^{l},x_{1}^{l})\nabla_{x_{ 1}^{l}}\log p(x_{1}^{l})+k(x_{2}^{l},x_{2}^{l})\nabla_{x_{2}^{l}}\log p(x_{2} ^{l})+\nabla_{x_{1}^{l}}k(x_{1}^{l},x_{2}^{l})+\nabla_{x_{2}^{l}}k(x_{1}^{l}, x_{2}^{l})\Big{)}\]

In the above update, \(\overrightarrow{Q}=k(x_{2}^{l},x_{1}^{l})\nabla_{x_{1}^{l}}\log p(x_{1}^{l}) +k(x_{2}^{l},x_{2}^{l})\nabla_{x_{2}^{l}}\log p(x_{2}^{l})\) aims to guide the particles in the direction that maximizes the likelihood, and the update direction \(\overrightarrow{R}=\nabla_{x_{1}^{l}}k(x_{1}^{l},x_{2}^{l})+\nabla_{x_{2}^{l} }k(x_{1}^{l},x_{2}^{l})\) acts as the repulsive force. Considering the repulsive force

\[\overrightarrow{R} =\nabla_{x_{1}^{l}}k(x_{1}^{l},x_{2}^{l})+\nabla_{x_{2}^{l}}k(x_ {1}^{l},x_{2}^{l})=\nabla_{x_{1}^{l}}\exp\left(\frac{-1}{h}(x_{1}^{l}-x_{2}^ {l})^{2}\right)+\nabla_{x_{2}^{l}}\exp\left(\frac{-1}{h}(x_{1}^{l}-x_{2}^{l}) ^{2}\right)\] \[=\frac{2}{h}(x_{2}^{l}-x_{1}^{l})k(x_{1}^{l},x_{2}^{l})\]

As can be seen, the repulsive force \(\overrightarrow{R}\) pushes the particle \(x_{2}^{l}\) in the direction away from particle \(x_{1}^{l}\) that introduces diversity. With more particles, each particle is updated in the direction that maximizes the likelihood, and the particle is pushed away from all other particles (by force \(R\)).

Next, consider ensemble components with different strengths of incorrect evidence regularization \(\mathcal{L}_{\mathtt{reg}}^{\mathtt{inc}}\) to analyze the update to the evidential model in the evidence space. For simplicity, we consider the incorrect evidence sum-based regularization similar to ADL without correct evidence regularization (The analysis is valid for all incorrect evidence regularization and for models with correct evidence regularization). For a model with incorrect evidence regularization, the overall evidential loss is given by:

\[\mathcal{L}_{\mathtt{evid}}(\bm{x},\bm{y})=\mathcal{L}^{\mathtt{Log}}(\bm{x},\bm{y})+\lambda\times\mathcal{L}_{\mathtt{reg}}^{\mathtt{ ADL}}(\bm{x},\bm{y})=\log S-\sum_{k=1}^{K}y_{k}\log \alpha_{k}+\lambda\times\sum_{k=1}^{K}e_{k}\times(1-y_{k})\]The gradient of the loss with respect to logits (the output head layer, where \(e_{k}=\exp(o_{k})\)) is given by

\[\text{grad}_{k} =\frac{\partial\mathcal{L}^{\text{Log}}(\mathbf{x},\mathbf{y})}{ \partial o_{k}}+\frac{\partial\mathcal{L}^{\text{ADL}}_{reg}(\mathbf{x}, \mathbf{y})}{\partial o_{k}}=\Big{(}\frac{1}{S}-\frac{y_{k}}{\alpha_{k}}\Big{)} \frac{\partial e_{k}}{\partial o_{k}}+\lambda\times(1-y_{k})\times\frac{ \partial e_{k}}{\partial o_{k}}\] (14) \[=\Big{(}\frac{1}{S}-\frac{y_{k}}{\alpha_{k}}+\lambda(1-y_{k}) \Big{)}e_{k}=\Big{(}\frac{1}{S}-\frac{y_{k}}{\alpha_{k}}+\lambda(1-y_{k}) \Big{)}e_{k}\] (15)

Consider \(K\) class classification problem. The gradient update to the logit layer for the evidential model is given by

\[\text{grad}_{k}= e_{k}\times\begin{bmatrix}\frac{1}{S}-\frac{y_{1}}{\alpha_{1}}\\ \frac{1}{S}-\frac{y_{2}}{\alpha_{2}}\\...\\ \frac{1}{S}-\frac{y_{K}}{\alpha_{K}}\end{bmatrix}+e_{k}\times\lambda\times \begin{bmatrix}1-y_{1}\\ 1-y_{2}\\...\\ 1-y_{K}\end{bmatrix}\] (16) \[=\overrightarrow{A}+\lambda\times\overrightarrow{B}\] (17)

Here, \(y_{k}\in[0,1],y_{k}=1\) if \(k=\text{gt},\quad\text{and}\quad y_{k}=0\quad\text{otherwise}\). Moreover, the \(\lambda\) value is varied, and different \(\lambda\) values lead to different evidential models.

The update force \(\overrightarrow{A}\) pushes the evidential model in the direction that maximizes the likelihood (similar to \(\overrightarrow{Q}\) in SVGD-based update), and the force \(\overrightarrow{B}\) implicitly pushes the ensemble components away from each other (similar to the repulsive force \(\overrightarrow{R}\) in SVGD-based update). Each component moves in \(\overrightarrow{B}\) direction with a different force determined by the incorrect evidence regularization strength \(\lambda\) that ensures that the ensemble components are diverse. Due to the different strengths of incorrect evidence regularization, each ensemble component places a different priority level for minimization of incorrect evidence over acquiring correct evidence, which ensures that the ensemble components remain diverse. 

## Appendix E Dataset and Implementation Details

We consider ViT model backbone [15] that is pre-trained in a supervised fashion, and 4 benchmark datasets of Cifar10 [1], Cifar100 [1], Food101 [8], and Flowers102 [45]. We consider few-shot adaptation with \(K\)-shot classification problem (We experiment with \(K\) values of \(1\), \(2\), \(5\), \(10\), and \(20\)). The few-shot training set is constructed by randomly selecting \(K\) samples per class from the training set of the benchmark datasets. We consider \(2\)-shot validation set for all datasets and settings. We train the model on the few-shot training set, use the \(2\)-shot validation set for hyperparameter tuning, and evaluate all models on the benchmark test set with all the test set samples. We augment the few-shot training set and the few-shot validation set with resize, random horizontal flip, and cropping. The dataset details are also presented in Table 5. We train all the models for \(50\) epochs on the few-shot training dataset with a batch size of \(64\) samples at each iteration and evaluate the model on the benchmark test set. The evidence is in the range [0, infinity], and some stability issues could potentially arise in extreme cases when the logit output is extremely low (i.e. close to negative infinity). In our experiments, we did not observe the stability issue. Still, the issue can arise in some extreme cases for which a small delta in the denominator could be introduced or the network's logits could be bounded to be greater than a small negative value. For the calibration baseline model of Parameterized Temperature Scaling (PTS) [61], we consider a 2-layer neural network with 128 nodes in the hidden layer (we carry out hyperparameter tuning with 1-layer, 2-layer, and 3-layer networks and select the best model), and train with a learning rate of 0.00001. For Temperature Scaling [26], we optimize for the temperature hyperparameter using Adam optimizer, and a learning rate of 0.01 (We also experiment with SGD optimizer, and learning rates of 0.1, 1.0, 0.01, and 0.0001 to select the best performing model). The evidential models use incorrect evidence regularization strength of \((0,0.1,1.0,10.0,100.0,\) and \(1000.0)\). For Isotonic Regression [6], we consider multi-class setting and sklearn package. We use VPT [32] as the representative PEFT where not specified due to its superior performance. The key model performance results are averaged across 5 different runs to present the mean and the standard deviation. The experiments use Pytorch and are carried out on a workstation with NVIDIA RTX A6000 GPU.

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

Figure 11: Accuracy-Confidence trends in 1-shot Cifar100 Results

Figure 10: Visualization of the impact of m for \(5\)-Shot Cifar100 dataset using reliability plots

[MISSING_PAGE_FAIL:25]

[MISSING_PAGE_FAIL:26]

**Data size.** We observe that the model's accuracy increases with more training samples (see Table 1 where we vary shots from 1 to 20). The under-confidence remains, even with further increase in training samples. To this end, we conduct additional experiments on Cifar100 by increasing the training samples per class to 500 and observe the under-confidence issue despite the increase in the accuracy. The trend is summarized in Figure 15 (a-b). We see an increase in accuracy and a decrease in ECE. However, even with 500 samples per class, the under-confidence issue remains. Further, we report the accuracy and ECE of the fully fine-tuned model (fine-tuning of all the parameters) for 1 shot cifar100 in Table 11 where we observe a decrease in accuracy while the calibration issue remains. We observe that full fine-tuning leads to overconfidence behavior, hurting the generalization performance, as seen in Table 11 and reliability plot as presented in Figure 14.

**Number of classes:** To study the impact of number of classes, we formulate 5-way 1 shot, 10-way 1 shot, and 100-way 1 shot tasks using Cifar100. The results are presented in Table 12. As we decrease the number of shots from 100 to 5, we see an increase in accuracy and a decrease in ECE. We observe that the model is more accurate as tasks become easier (indicated by fewer classes _i.e.,_ lower \(N\) value in Table 12). However, the under-confidence issue remains.

**Number of unfrozen parameters:** We conduct additional experiments on Cifar100 100-way 1-shot tasks by varying the number of prompts for 1) shallow prompt: prompt added to the input only and 2) deep prompt: prompt added to all Transformer encoder layers' input as well. The accuracy and ECE trends are presented in Figure 15 (c-d). As can be seen, with the increase in the number of prompts for both shallow and deep prompts, there are fluctuations in accuracy and ECE performance. However, the under-confidence issue persists for all the cases.

### Additional Experiments and Comparison

In this section, we carry out additional experiments to study the OOD performance of our model, and compare our model with additional methods present in literature.

**OOD Performance:** The current work, being an instance of fine-grained uncertainty quantification works, could potentially help in OOD detection. To this end, we present the OOD results of Cifar10 as in-distribution dataset and Cifar100 as out-of-distribution dataset with AUROC, FPR95, AUPR metrics for our model on 100-way 1-shot and 100-way 5-shot Cifar100 tasks in Table 13. As seen, B-PEFT performs better than PEFT, and with more training data, the model's OOD detection capabilities improve. Even with only 5 samples/class (i.e. 100-way 5-shot Cifar100 task), the model can achieve an AUROC of 92.58.

**Model Comparison:** We first carry out experiments with cosine classifier [22] without training for the 100-way 1-shot task on Cifar100. The cosine classifier has comparable generalization performance (accuracy) in comparison to VPT based model (see Table 14). However, looking at the ECE, the miscalibration issue is even higher than VPT based model. Hence, the simple solution (cosine classifier) does not ensure calibrated predictions. We also carry out experiments with test time augmentation [4] and VPT fine tuning with LoRA [70], on 100-way 1-shot Cifar100 dataset. Towards comparison with Bayesian inspired methods [14], we use Laplace approximation on last layer of the model using Kronecker Product and Diagonalization represented by KronLaplace and DiagLaplace in Table 14. As can be seen, these methods also suffer from the under-confidence issue when straightforwardly extended to the VPT. B-PEFT achieves much better generalization and calibration performance than these baselines.

Figure 15: (a-b) Accuracy-EEC trend with the number of parameters, (c-d): Accuracy-ECE Trend with data size

[MISSING_PAGE_FAIL:28]

predictions, and lack of fine-grained uncertainty quantification capabilities. We develop a novel Bayesian Evidential model: B-PEFT that addresses the weaknesses of existing PEFT for pre-trained foundation models. Being an instance of the PEFT, our developed model enables the large foundation models to be adapted to challenging few-shot problems in a parameter-efficient and computationally efficient manner with limited memory requirements and energy footprint. Moreover, the developed model improves the generalization performance and the model's predictions are calibrated ensuring trustworthiness. Finally, the model has fine-grained uncertainty quantification capabilities which are highly desirable when applying these models in real-world safety-critical scenarios. Overall, our developed B-PEFT is expected to have a strong positive societal impact.

## Appendix I Limitations and Future Work

In this work, we investigate the calibration of transformer-based foundation models under few-shot adaptation using various parameter-efficient fine-tuning methods. We focus on fine-tuning supervised pre-trained models for few-shot learning. We note that there are other self-supervised pre-trained models that show promising results for various benchmark datasets. We investigate the calibration of CLIP, a representative method, under few-shot adaptation using the two most popular parameter-efficient fine-tuning methods: prompt and adapter. Our preliminary results demonstrate that the few-shot performance does not consistently increase in comparison to zero-shot. Similarly, prompt-based fine-tuning has relatively better calibration than adapter-based fine-tuning. As an extension of this work, we will investigate the calibration performance of self-supervised foundation models. Additionally, it could be interesting to study the calibration performance of PEFT for tasks beyond image classification, _e.g.,_ to other modalities such as audio and language foundation models. For instance, the ideas developed in this work could potentially be used in situations where the PEFT leads to mis-calibrated models and the developed model requires trustworthy fine-grained uncertainty quantification capabilities. If these data modalities are also modeled using transformers and follow the parameter-efficient fine-tuning paradigm in performing downstream tasks, we expect the proposed approach can benefit them in a similar way.

Figure 17: Different Shot adaptation: Calibration performance of CLIP based model on cifar100 dataset using prompt and adapter-based fine-tuning

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract and introduction present the paper's contribution and scope, and match the theoretical and empirical results in the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of the proposed work have been discussed in Section I.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Complete proof of all the theoretical claims is presented.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Details for reproducibility along with link to the code is provided.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Only benchmark datasets and publicly available models are used for training and evaluation.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experimental setting and details are provided.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All the key results present the mean and standard deviation of five trials.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details of all compute resources used in experiments is provided
9. **Code Of Ethics**Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The work confirms to the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The societal impact is discussed in Section H.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The work poses no obvious high risk for misuse.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: References and citations are provided as required.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The link to the source code and resources is provided.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing and research with human subjects is involved in this research work.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No crowdsourcing and research with human subjects is involved in this research work.