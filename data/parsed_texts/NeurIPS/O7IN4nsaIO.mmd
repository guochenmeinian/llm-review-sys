# Achieving Near-Optimal Convergence for Distributed Minimax Optimization with Adaptive Stepsizes

 Yan Huang

College of Control Science and Engineering

Zhejiang University, China

huangyan5616@zju.edu.cn &Xiang Li

Department of Computer Science

ETH Zurich, Switzerland

xiang.li@inf.ethz.ch &Yipeng Shen

College of Control Science and Engineering

Zhejiang University, China

22332074@zju.edu.cn &Niao He

Department of Computer Science

ETH Zurich, Switzerland

niao.he@inf.ethz.ch &Jinming Xu

College of Control Science and Engineering

Zhejiang University, China

jimmyxu@zju.edu.cn

###### Abstract

In this paper, we show that applying adaptive methods directly to distributed minimax problems can result in non-convergence due to inconsistency in locally computed adaptive stepsizes. To address this challenge, we propose D-AdaST, a Distributed Adaptive minimax method with Stepsize Tracking. The key strategy is to employ an adaptive stepsize tracking protocol involving the transmission of two extra (scalar) variables. This protocol ensures the consistency among stepsizes of nodes, eliminating the steady-state error due to the lack of coordination of stepsizes among nodes that commonly exists in vanilla distributed adaptive methods, and thus guarantees exact convergence. For nonconvex-strongly-concave distributed minimax problems, we characterize the specific transient times that ensure time-scale separation of stepsizes and quasi-independence of networks, leading to a near-optimal convergence rate of \(\tilde{\mathcal{O}}\left(\epsilon^{-(4+\delta)}\right)\) for any small \(\delta>0\), matching that of the centralized counterpart. To our best knowledge, D-AdaST is the _first_ distributed adaptive method achieving near-optimal convergence without knowing any problem-dependent parameters for nonconvex minimax problems. Extensive experiments are conducted to validate our theoretical results.

## 1 Introduction

Distributed optimization has seen significant research progress over the last decade, resulting in numerous algorithms (Nedic and Ozdaglar, 2009; Yuan et al., 2016; Lian et al., 2017; Pu and Nedic, 2021). However, the traditional focus of distributed optimization has primarily been on minimization tasks. With the rapid growth of machine learning research, various applications have emerged that go beyond simple minimization, such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014; Gulrajani et al., 2017), robust optimization (Mohri et al., 2019; Sinha et al., 2017), adversary training of neural networks (Wang et al., 2021), fair machine learning (Madras et al., 2018), and justto name a few. These tasks typically involve a minimax structure as follows:

\[\min_{x\in\mathcal{X}}\,\max_{y\in\mathcal{Y}}f\left(x,y\right),\]

where \(\mathcal{X}\subseteq\mathbb{R}^{p}\), \(\mathcal{Y}\subseteq\mathbb{R}^{d}\), and \(x,y\) are the primal and dual variables to be learned, respectively. One of the simplest yet effective methods for solving the above minimax problem is Gradient Descent Ascent (GDA) (Dem'yanov and Pevnyi, 1972; Nemirovski et al., 2009) which alternately performs stochastic gradient descent for the primal variable and stochastic gradient ascent for the dual variable. This approach has demonstrated its effectiveness in solving minimax problems, especially for convex-concave objectives (Hsieh et al., 2021; Daskalakis et al., 2021; Antonakopoulos et al., 2021), i.e., the function \(f(\cdot,y)\) is convex for any \(y\in\mathcal{Y}\), and \(f(x,\cdot)\) is concave for any \(x\in\mathcal{X}\).

Adaptive gradient methods, such as AdaGrad (Duchi et al., 2011), Adam (Kingma and Ba, 2014), and AMSGrad (Reddi et al., 2018), are often integrated with GDA to effectively solve minimax problems with theoretical guarantees in convex-concave settings (Diakonikolas, 2020; Antonakopoulos et al., 2021; Ene and Le Nguyen, 2022). These adaptive methods are capable of adjusting stepsizes based on historical gradient information, making it robust to hyper-parameters tuning and can converge without requiring to know problem-dependent parameters (a characteristic often referred to as being "parameter-agnostic"). However, in the nonconvex regime, it has been shown by Lin et al. (2020); Yang et al. (2022) that it is necessary to have a time-scale separation in stepsizes between the minimization and maximization processes to ensure the convergence of GDA and GDA-based adaptive algorithms. In particular, the stepsize ratio between primal and dual variables needs to be smaller than a threshold depending on the properties of the problem such as the smoothness and strong-concavity parameters (Li et al., 2022; Guo et al., 2021; Huang et al., 2021), which are often unknown or difficult to estimate in real-world tasks, such as training deep neural networks.

Applying GDA-based adaptive methods into decentralized settings poses additional challenges due to the presence of inconsistency in locally computed adaptive stepsizes. In particular, it has been shown that the inconsistency of stepsizes can result in non-convergence in federated learning with heterogeneous computation speeds (Wang et al., 2020; Sharma et al., 2023). This is mainly due to the lack of a central node coordinating the stepsizes of nodes in distributed settings, making it difficult to converge, as observed in minimization problems (Liggett, 2022; Chen et al., 2023b). As a result, the following question arises naturally:

_"Can we design an adaptive minimax method that ensures the time-scale separation and consistency of stepsizes with provable convergence in fully distributed settings?"_

**Contributions.** In this paper, we aim to propose a distributed adaptive method for efficiently solving nonconvex-strongly-concave (NC-SC) minimax problems. The contributions are threefold:

* We construct counterexamples showing that directly applying adaptive methods designed for centralized problems will lead to inconsistencies in locally computed adaptive stepsizes, resulting in non-convergence in distributed settings. To tackle this issue, we propose the _first_ distributed adaptive minimax method, named D-AdaST, that incorporates an efficient stepsize tracking mechanism to maintain consistency across local stepsizes, which involves transmission of merely two extra (scalar) variables. The proposed algorithm exhibits time-scale separation in stepsizes and parameter-agnostic capability in fully distributed settings.
* Theoretically, we prove that D-AdaST is able to achieve a near-optimal convergence rate of \(\tilde{\mathcal{O}}\left(\epsilon^{-(4+\delta)}\right)\) with arbitrarily small \(\delta>0\) to find an \(\epsilon\)-stationary point for distributed NC-SC minimax problems. In contrast, we also prove the existence of a constant steady-state error in both the lower and upper bounds for GDA-based distributed minimax algorithms when being directly integrated with the adaptive stepsize rule without the stepsize tracking mechanism. Moreover, we explicitly characterize the transient times that ensure time-scale separation and quasi-independence of network, respectively.
* We conduct extensive experiments on real-world datasets to verify our theoretical findings and the effectiveness of D-AdaST on a variety of tasks, including robust training of neural networks and optimizing Wasserstein GANs. In all tasks, we demonstrate the superiority of D-AdaST over several vanilla distributed adaptive methods across various graphs, initial stepsizes and data distributions (see also additional experiments in Appendix A).

### Related Works

**Distributed nonconvex minimax methods.** In the realm of federated learning, Deng and Mahdavi (2021) introduce Local SGDA algorithm combining FedAvg/Local SGD with stochastic GDA and show an \(\tilde{\mathcal{O}}\left(\epsilon^{-6}\right)\) sample complexity for NC-SC objective functions. Sharma et al. (2022) provide improved complexity result of \(\tilde{\mathcal{O}}\left(\epsilon^{-4}\right)\) matching that of the lower bound of first-order algorithms for both NC-SC and nonconvex-Polyak-Lojasiewicz (NC-PL) settings (Li et al., 2021; Zhang et al., 2021). Yang et al. (2022) integrate Local SGDA with stochastic gradient estimators to eliminate the data heterogeneity. More recently, Zhang et al. (2023) adopt compressed momentum methods with Local SGD to increase the communication efficiency of the algorithm. For decentralized nonconvex minimax problems, Liu et al. (2020) study the training of GANs using decentralized optimistic stochastic gradient and provide non-asymptotic convergence with fixed stepsizes. Tsaknakis et al. (2020) propose a double-loop decentralized SGDA algorithm with gradient tracking techniques (Pu and Nedic, 2021) and achieve \(\tilde{\mathcal{O}}\left(\epsilon^{-4}\right)\) sample complexity. With a stronger assumption of average smoothness, some studies employ variance reduction techniques to accelerate convergence (Zhang et al., 2021; Chen et al., 2022; Xian et al., 2021; Tarzanagh et al., 2022; Wu et al., 2023; Chen et al., 2024; Zhang et al., 2024), which require more memory and computational resources due to the need for larger batch-sizes or full gradient evaluations. However, all the above-mentioned methods use a fixed or uniformly decaying stepsize, requiring the prior knowledge of smoothness and concavity.

**(Distributed) adaptive minimax methods.** For centralized nonconvex minimax problems, Yang et al. (2022) show that, even in deterministic settings, GDA-based methods necessitate the time-scale separation of the stepsizes for primal and dual updates. Many attempts have been made for ensuring the time-scale separation requirement (Lin et al., 2020; Yang et al., 2022; Bot and Bohm, 2023; Huang et al., 2023). However, these methods typically come with the prerequisite of having knowledge about problem-dependent parameters, which can be a significant drawback in practical scenarios. To this end, Yang et al. (2022) introduce a nested adaptive algorithm named NeAda that achieves parameter-agnosticism by incorporating an inner loop to effectively maximize the dual variable, which can obtain an optimal sample complexity of \(\tilde{\mathcal{O}}\left(\epsilon^{-4}\right)\) when the strong-concavity parameter is known. More recently, Li et al. (2023) introduce TiAda, a single-loop parameter-agnostic adaptive algorithm for nonconvex minimax optimization which employs separated exponential factors on the adaptive primal and dual stepsizes, improving upon NeAda on the noise-adaptivity. There has been few works dedicated to adaptive minimax optimization in federated learning settings. For instance, Huang et al. (2024) introduces a federated adaptive algorithm that integrates the stepsize rule of Adam with full-client participation, resembling the centralized counterpart. Ju et al. (2023) study a federated Adam algorithm for fair federated learning where the objective function is properly weighted to account for heterogeneous updates among nodes. To the best of our knowledge, it is still unknown how one can design an adaptive minimax method capable of fulfilling the time-scale separation requirement and being parameter-agnostic in _fully distributed settings_.

**Notations.** Throughout this paper, we denote by \(\mathbb{E}\left[\cdot\right]\) the expectation of a random variable, \(\left\|\cdot\right\|\) the Frobenius norm, \(\left\langle\cdot,\cdot\right\rangle\) the inner product of two vectors, \(\odot\) the Hadamard product (entry wise), \(\otimes\) the Kronecker product. We denote by \(\mathbf{1}\) the all-ones vector, \(\mathbf{I}\) the identity matrix and \(\mathbf{J}=\mathbf{1}\mathbf{1}^{T}/n\) the averaging matrix with \(n\) dimension. For a vector or matrix \(A\) and constant \(\alpha\), we denote \(A^{\alpha}\) the entry-wise exponential operations. We denote \(\Phi\left(x\right):=f\left(x,y^{*}\left(x\right)\right)\) as the primal function where \(y^{*}\left(x\right)=\underset{y\in\mathcal{Y}}{\operatorname{argmax}}f\left(x, y\right)\), and \(\mathcal{P}_{\mathcal{Y}}\left(\cdot\right)\) as the projection operation onto set \(\mathcal{Y}\).

## 2 Distributed Adaptive Minimax Methods

We consider the distributed minimax problem collaboratively solved by a set of agents over a network. The overall objective of the agents is to solve the following finite-sum problem:

\[\min_{x\in\mathbb{R}^{p}}\ \max_{y\in\mathcal{Y}}f\left(x,y\right)=\frac{1}{n} \sum_{i=1}^{n}\underbrace{\mathbb{E}_{\xi_{i}\sim\mathcal{D}_{i}}\left[F_{i} \left(x,y;\xi_{i}\right)\right]}_{:=f_{i}\left(x,y\right)},\] (1)

where \(f_{i}:\mathbb{R}^{p+d}\rightarrow\mathbb{R}\) is the local private loss function accessible only by the associated node \(i\in\mathcal{N}=\{1,2,\cdots,n\}\), \(\mathcal{Y}\subset\mathbb{R}^{d}\) is closed and convex, and \(\xi_{i}\sim\mathcal{D}_{i}\) denotes the data sample locally stored at node \(i\in\mathcal{N}\) with distribution \(\mathcal{D}_{i}\). We consider a graph \(\mathcal{G}=\left(\mathcal{V},\mathcal{E}\right)\), here, \(\mathcal{V}=\{1,2,...,n\}\) represents the set of agents, and \(\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}\) denotes the set of edges consisting of ordered pairs \((i,j)\)representing the communication link from node \(j\) to node \(i\). For node \(i\), we define \(\mathcal{N}_{i}=\{j\mid(i,j)\in\mathcal{E}\}\) as the set of its neighboring nodes. Before proceeding to the discussion of distributed algorithms, we first introduce the following notations for brevity:

\[\mathbf{x}_{k}:=\left[x_{1,k},x_{2,k},\cdots,x_{n,k}\right]^{T}\in\mathbb{R}^{n \times p},\ \mathbf{y}_{k}:=\left[y_{1,k},y_{2,k},\cdots,y_{n,k}\right]^{T}\in\mathbb{R}^ {n\times d},\]

where \(x_{i,k}\in\mathbb{R}^{p},y_{i,k}\in\mathcal{Y}\) denote the primal and dual variable of node \(i\) at each iteration \(k\), and

\[\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right) :=\left[\cdots,\nabla_{x}F_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^{x} \right),\cdots\right]^{T},\] \[\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right) :=\left[\cdots,\nabla_{y}F_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^{y} \right),\cdots\right]^{T}\]

are the corresponding partial stochastic gradients with i.i.d. samples \(\xi_{k}^{x},\xi_{k}^{y}\) in a compact form.

Next, we will first explain the pitfalls of directly applying centralized adaptive stepsize rules to decentralized settings, and then introduce our newly proposed solution to address the challenge.

### Non-Convergence of Direct Extensions

For the distributed minimax optimization problem as depicted in (1) involving NC-SC objective functions, we will show shortly that the Distributed Stochastic Gradient Descent Ascent (D-SGDA) method may not converge due to the inability of time-scale separation with constant stepsizes (c.f., Figure 1), which is also observed in centralized settings (Lin et al., 2020; Yang et al., 2022b). To address this issue, one can adopt the adaptive stepsize rule used in centralized TiAda (Li et al., 2023) for each individual node, which is renowned for its ability to adaptively fulfill the time-scale separation requirements. As a result, we arrive at the following Distributed TiAda (D-TiAda) algorithm.

\[\mathbf{x}_{k+1} =W\left(\mathbf{x}_{k}-\gamma_{x}V_{k+1}^{-\alpha}\nabla_{x}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right),\] (2a) \[\mathbf{y}_{k+1} =\mathcal{P}_{\mathcal{Y}}\left(W\left(\mathbf{y}_{k}+\gamma_{y} U_{k+1}^{-\beta}\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right) \right)\right),\] (2b)

where \(\gamma_{x}\) and \(\gamma_{y}\) are the stepsizes, \(W\) is a doubly-stochastic weight matrix induced by graph \(\mathcal{G}\)(Xiao et al., 2006) (c.f., Assumption 4), and

\[V_{k+1}^{-\alpha}=\mathrm{diag}\left\{v_{i,k+1}^{-\alpha}\right\}_{i=1}^{n}, \quad U_{k+1}^{-\beta}=\mathrm{diag}\left\{u_{i,k+1}^{-\beta}\right\}_{i=1}^{ n},\] (3)

with \(v_{i,k+1}=\max\left\{m_{i,k+1}^{x},m_{i,k+1}^{y}\right\},u_{i,k+1}=m_{i,k+1}^{y}\), and

\[m_{i,k+1}^{x}=m_{i,k}^{x}+\left\|\nabla_{x}F_{i}\left(x_{i,k},y_{i,k};\xi_{i,k }^{x}\right)\right\|^{2},\ m_{i,k+1}^{y}=m_{i,k}^{y}+\left\|\nabla_{y}F_{i} \left(x_{i,k},y_{i,k};\xi_{i,k}^{y}\right)\right\|^{2}\] (4)

are the local accumulated gradient norm. Note that we impose a maximum operator in the preconditioner \(v_{i,k}\), and employ different stepsize decaying rates, i.e., \(0<\beta<\alpha<1\), for the primal and

Figure 1: Comparison among D-SGDA, D-TiAda and D-AdaST for NC-SC quadratic objective function (6) with \(n=2\) nodes and \(\gamma_{x}=\gamma_{y}\). In (a), it shows the trajectories of primal and dual variables of the algorithms, the points on the black dash line are stationary points of \(f\). In (b), it shows the convergence of \(\left\|\nabla_{x}f\left(x_{k},y_{k}\right)\right\|^{2}\) over the iterations. In (c), it shows the convergence of the inconsistency of stepsizes, \(\zeta_{v}^{2}\) defined in (8), over the iterations. Notably, \(\zeta_{v}^{2}\) fails to converge for D-TiAda and \(\zeta_{v}^{2}=0\) for non-adaptive D-SGDA.

dual variables, respectively. Such design allows to balance the updates of \(x\) and \(y\), and achieves the desired time-scale separation without requiring any knowledge of parameters (Li et al., 2023).

However, in the distributed setting, such direct extension may fail to converge to a stationary point because \(v_{i,k}\) and \(u_{i,k}\) can be inconsistent due to the difference of local objective functions \(f_{i}\), In particular, we can rewrite the above vanilla distributed optimization algorithm (2) in the sense of average system of primal variables as below,

\[\bar{x}_{k+1} =\underbrace{\bar{x}_{k}-\gamma_{x}\bar{v}_{k}^{-\alpha}\frac{ \mathbf{1}^{T}}{n}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x} \right)}_{\text{adaptive descent}}-\underbrace{\frac{\left(\boldsymbol{\tilde{v}}_{ k+1}^{-\alpha}\right)^{T}}{n}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x} \right)}_{\text{inconsistency}},\] (5)

where \(\left(\boldsymbol{\tilde{v}}_{k}^{-\alpha}\right)^{T}:=\left[\cdots,v_{i,k}^{ -\alpha}-\bar{v}_{k}^{-\alpha},\cdots\right]\), \(\bar{x}_{k}:=\mathbf{1}^{T}\mathbf{x}_{k}/n\) and \(\bar{v}_{k}:=1/n\sum_{i=1}^{n}v_{i,k}\).

It is evident that, in comparison to centralized adaptive methods, an unexpected term (i.e., \(\boldsymbol{\tilde{v}}_{k}\)) on the right-hand side (RHS) arises due to inconsistencies. This term introduces inaccuracies in the directions of gradient descent, degrading the optimization performance. The theorem presented below reveals a gap near the stationary points in a properly designed counterexample, indicating the non-convergence of D-TiAda. The proof is available in Appendix B.3.

**Theorem 1**.: _There exists a distributed minimax problem in the form of Problem (1) and certain initialization such that after running D-TiAda with any \(0<\beta<0.5<\alpha<1\) and \(\gamma_{x},\gamma_{y}>0\), it holds that for any \(t=0,1,2,\ldots\), we have,_

\[\parallel\nabla_{x}f(x_{t},y_{t})\parallel=\parallel\nabla_{x}f(x_{0},y_{0}) \parallel,\quad\parallel\nabla_{y}f(x_{t},y_{t})\parallel=\parallel\nabla_{y} f(x_{0},y_{0})\parallel,\]

_where \(\parallel\nabla_{x}f(x_{0},y_{0})\parallel\) and \(\parallel\nabla_{y}f(x_{0},y_{0})\parallel\) can be arbitrarily large depending on the initialization._

**Remark 1**.: _The counterexample we constructed consists of three nodes, forming a complete graph. Without the stepsize tracking, D-TiAda will remain stationary, and the iterates will not progress if initiated along a specific line. In this counterexample, the only stationary point is at \((0,0)\), but initial points along the line (c.f., Eq. (72)) can be positioned arbitrarily far away from this stationary point, implying the non-convergence of D-TiAda with certain initialization._

Apart from the counterexample discussed in Theorem 1, we also experimentally observe the divergence of of D-SGDA and D-TiAda even in a simple scenario involving only two connected agents. This phenomenon is illustrated in Figure 1 and the functions are depicted as follows:

\[f_{1}\left(x,y\right) =-\frac{9}{20}y^{2}+\frac{3}{5}y-x+xy-\frac{1}{2}x^{2},\] (6) \[f_{2}\left(x,y\right) =-\frac{9}{20}y^{2}+\frac{3}{5}y-x+2xy-2x^{2}.\]

It is not difficult to verify that the points on the line \(3y=5x+2\) are stationary points of \(f\left(x,y\right)=1/2\left(f_{1}\left(x,y\right)+f_{2}\left(x,y\right)\right)\). It follows from Figure 1(a) and 1(b) that D-SGDA does not converge to a stationary point because of the lack of time-scale separation, and D-TiAda also fails to converge due to stepsize inconsistency, as shown in Figure 1(c). In contrast, the utilization of the stepsize tracking protocol in D-AdaST ensures convergence to a stationary point, with the inconsistency in stepsizes gradually diminishing (c.f., Lemma 9). These two motivating examples effectively highlight the challenges associated with applying adaptive minimax algorithms to distributed settings.

### The Proposed D-AdaST Algorithm

To address the issue of stepsize inconsistency across different nodes, we propose the following Distributed Adaptive minimax optimization algorithm with Stepsize Tracking protocol, termed D-AdaST, which allows us to asymptotically eliminate the stepsize inconsistency in a decentralized manner over networks. The pseudo-code for the algorithm is summarized in Algorithm 1, and can be rewritten in a compact form as follows:

\[\mathbf{m}_{k+1}^{x} =W\left(\mathbf{m}_{k}^{x}+\mathbf{h}_{k}^{x}\right),\] (7a) \[\mathbf{m}_{k+1}^{y} =W\left(\mathbf{m}_{k}^{y}+\mathbf{h}_{k}^{y}\right),\] (7b) \[\mathbf{x}_{k+1} =W\left(\mathbf{x}_{k}-\gamma_{x}V_{k+1}^{-\alpha}\nabla_{x}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right),\] (7c) \[\mathbf{y}_{k+1} =\mathcal{P}_{\mathcal{Y}}\left(W\left(\mathbf{y}_{k}+\gamma_{y}U_ {k+1}^{-\beta}\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y} \right)\right)\right),\] (7d)where \(\mathbf{m}_{k}^{x}=[\cdots,m_{i,k}^{x},\cdots]^{T}\), \(\mathbf{m}_{k}^{y}=[\cdots,m_{i,0}^{y},\cdots]^{T}\) denote the tracking variables for the accumulated global gradient norm, i.e., for \(z\in\{x,y\}\),

\[\frac{\mathbf{1}^{T}}{n}\mathbf{m}_{k+1}^{z}=\frac{1}{n}\sum\nolimits_{i=1}^{ n}\left(\sum\nolimits_{t=0}^{k}\left\|g_{i,t}^{z}\right\|^{2}+m_{i,0}^{z}\right)\]

while \(\bm{h}_{k}^{z}=[\cdots,\parallel g_{i,k}^{z}\parallel^{2},\cdots]^{T}\), and \(V_{k},U_{k}\) are diagonal matrices with \(v_{i,k}=\max\left\{m_{i,k}^{x},m_{i,k}^{y}\right\}\) and \(u_{i,k}=m_{i,k}^{x}\). Note that we also provide a variant of D-AdaST with coordinate-wise adaptive stepsizes in Algorithm 2, along with its convergence analysis in Appendix B.5.

```
0:\(x_{i,0}\in\mathbb{R}^{p}\), \(y_{i,0}\in\mathcal{Y}\), buffers \(m_{i,0}^{x}=m_{i,0}^{y}=c>0\), stepsizes \(\gamma_{x},\gamma_{y}>0\), exponential factors \(0<\beta<\alpha<1\) and weight matrix \(W\).
1:for iteration \(k=0,1,\cdots\), each node \(i\in[n]\), do
2: Sample i.i.d. \(g_{i,k}^{x}=\nabla_{x}F_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^{x}\right)\) and \(g_{i,k}^{y}=\nabla_{y}F_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^{y}\right)\).
3: Accumulate the gradient norm: \[m_{i,k+1}^{x}=m_{i,k}^{x}+\left\|g_{i,k}^{x}\right\|^{2},\ m_{i,k+1}^{y}=m_{i,k}^{y}+ \left\|g_{i,k}^{y}\right\|^{2}.\]
4: Compute the ratio: \[\psi_{i,k+1}=(m_{i,k+1}^{x})^{\alpha}/\max\left\{(m_{i,k+1}^{x})^{\alpha},(m_{ i,k+1}^{y})^{\alpha}\right\}\leqslant 1.\]
5: Update primal and dual variables locally: \[x_{i,k+1}=x_{i,k}-\gamma_{x}\psi_{i,k+1}\left(m_{i,k+1}^{x}\right)^{-\alpha}g_ {i,k}^{x},\ y_{i,k+1}=y_{i,k}+\gamma_{y}(m_{i,k+1}^{y})^{-\beta}g_{i,k}^{y}.\]
6: Communicate adaptive stepsizes and decision variables with neighbors: \[\left\{m_{i,k+1}^{x},m_{i,k+1}^{y},x_{i,k+1},y_{i,k+1}\right\}\leftarrow\sum \limits_{j\in\mathcal{N}_{i}}W_{i,j}\left\{m_{j,k+1}^{x},m_{j,k+1}^{y},x_{j,k+ 1},y_{j,k+1}\right\}\text{.}\]
7: Projection of dual variable on the set \(\mathcal{Y}\): \(y_{i,k+1}\leftarrow\mathcal{P}_{\mathcal{Y}}\left(y_{i,k+1}\right)\).
8:endfor ```

**Algorithm 1** Distributed Adaptive Minimax Method with Stepsize Tracking (D-AdaST)

## 3 Convergence Analysis

In this section, we present the main convergence results for the proposed D-AdaST algorithm and compare it with D-TiAda to show the effectiveness of the proposed stepsize tracking protocol.

To this end, letting \(\bar{u}_{k}:=1/n\sum_{i=1}^{n}u_{i,k}\), we define the following metrics to evaluate the level of inconsistency of stepsizes among nodes, which are ensured to be bounded by Assumption 3.

\[\zeta_{v}^{2}:=\sup\limits_{i\in[n],k>0}\left\{\left(v_{i,k}^{-\alpha}-\bar{v }_{k}^{-\alpha}\right)^{2}/\left(\bar{v}_{k}^{-\alpha}\right)^{2}\right\},\ \zeta_{u}^{2}:=\sup\limits_{i\in[n],k>0}\left\{\left(u_{i,k}^{-\beta}-\bar{u}_{k }^{-\beta}\right)^{2}/\left(\bar{u}_{k}^{-\beta}\right)^{2}\right\}.\] (8)

### Assumptions

We consider the NC-SC setting of Problem (1) with the following assumptions that are commonly used in the existing works (c.f., Remark 2 and Remark 3). Notably, for the function and algorithm class determined by the assumptions of this work, Li et al. (2021) derived a lower complexity bound of \(\Omega\left(\epsilon^{-4}\right)\) and proved that such a dependency on \(\epsilon\) is optimal (c.f., Remark 2).

**Assumption 1** (\(\mu\)-strong concavity in \(y\)).: _Each objective function \(f_{i}\left(x,y\right)\) is \(\mu\)-strongly concave in \(y\), i.e., \(\forall x\in\mathbb{R}^{p}\), \(\forall y,y^{\prime}\in\mathcal{Y}\) and \(\mu>0\),_

\[f_{i}\left(x,y\right)-f_{i}\left(x,y^{\prime}\right)\geqslant\left\langle\nabla_{ y}f_{i}\left(x,y\right),y-y^{\prime}\right\rangle+\frac{\mu}{2}\left\|\ y-y^{\prime}\right\|^{2}.\] (9)

**Assumption 2** (Joint smoothness).: _Each objective function \(f_{i}\left(x,y\right)\) is \(L\)-smooth in \(x\) and \(y\), i.e., \(\forall x,x^{\prime}\in\mathbb{R}^{p}\) and \(\forall y,y^{\prime}\in\mathcal{Y}\), there exists a constant \(L\) such that for \(z\in\left\{x,y\right\}\),_

\[\left\|\nabla_{z}f_{i}\left(x,y\right)-\nabla_{z}f_{i}\left(x^{\prime},y^{ \prime}\right)\right\|^{2}\leqslant L^{2}\left(\left\|x-x^{\prime}\right\|^{2} +\left\|y-y^{\prime}\right\|^{2}\right).\] (10)

_Furthermore, \(f_{i}\) is second-order Lipschitz continuous for \(y\), i.e., for \(z\in\left\{x,y\right\}\),_

\[\left\|\nabla_{zy}^{2}f_{i}\left(x,y\right)-\nabla_{zy}^{2}f_{i}\left(x^{ \prime},y^{\prime}\right)\right\|^{2}\leqslant L^{2}\left(\left\|x-x^{\prime} \right\|^{2}+\left\|y-y^{\prime}\right\|^{2}\right).\] (11)

**Remark 2**.: _Assumption 1 does not require the convexity in \(x\) and the objective function thus can be nonconvex. Assumption 1 and 2 ensure that \(y^{*}(\cdot)\) is smooth (c.f., Lemma 2), which is essential for achieving (near) optimal convergence rate (Chen et al., 2021; Li et al., 2023). Besides, it can be verified that the constructed 'hard' examples for obtaining the lower complexity bound in Li et al. (2021) satisfy the above second-order Lipschitz continuity (11) on \(y\), implying that the achievable optimal complexity for the function and algorithm class considered in this work is \(\mathcal{O}\left(\epsilon^{-4}\right)\)._

**Assumption 3** (Stochastic gradient).: _For i.i.d. sample \(\xi_{i}\), the stochastic gradient of each \(i\) is unbiased, i.e., \(\forall x\in\mathbb{R}^{p},y\in\mathcal{Y}\), \(\mathbb{E}_{\xi_{i}}\left[\nabla_{z}F_{i}\left(x,y;\xi_{i}\right)\right]= \nabla_{z}f_{i}\left(x,y\right)\), for \(z\in\left\{x,y\right\}\), and there is a constant \(C>0\) such that \(\left\|\nabla_{z}F_{i}\left(x,y;\xi_{i}\right)\right\|\leqslant C\)._

**Remark 3**.: _Assumption 3 on unbiased stochastic gradient is widely used for establishing convergence rates of both minimization and minimax optimization methods with AdaGrad (Kavis et al., 2022; Li et al., 2023) or Adam (Zou et al., 2019; Chen et al., 2023a; Huang et al., 2024) adaptive stepsize. We note that under Assumption 2, this assumption can be easily satisfied in many real-world tasks by imposing constraints on the compact domain of \(f\), e.g., neural networks with rectified activation (Dinh et al., 2017) and GANs with projections on the critic (Gulrajani et al., 2017)._

Next, we make the following assumption on the underlying graph to ensure its connectivity.

**Assumption 4** (Graph connectivity).: _The weight matrix \(W\) induced by graph \(\mathcal{G}\) is doubly stochastic, i.e., \(W\mathbf{1}=\mathbf{1},\mathbf{1}^{T}W=\mathbf{1}^{T}\) and \(\rho_{W}:=\left\|W-\mathbf{J}\right\|_{2}^{2}<1\)._

Note that one can always find a proper weight matrix \(W\) compliant to the graph that satisfies Assumption 4 once the underlying graph is undirected and connected. For instance, the weight matrix can be easily determined based on the Metropolis-Hastings protocol (Xiao et al., 2006). Moreover, this assumption is more general than that in Lian et al. (2017); Borodich et al. (2021) in the sense that \(W\) is not required to be symmetric, implying that certain directed graphs can be included in this assumption, e.g., directed ring and exponential graphs (Ying et al., 2021).

### Main Results

We are now ready to present the key convergence results in terms of the primal function \(\Phi\left(x\right):=f\left(x,y^{*}\left(x\right)\right)\) with \(y^{*}\left(x\right)=\underset{y\in\mathcal{Y}}{\operatorname{argmax}}f\left( x,y\right)\), whose proofs can be found in Appendix B.4.

**Theorem 2**.: _Suppose Assumption 1-4 hold. Let \(0<\beta<\alpha<1\) and the total iteration \(K\) satisfy_

\[\Omega\left(\max\left\{\left(\frac{\gamma_{x}^{2}\kappa^{4}}{\gamma_{y}^{2}} \right)^{\frac{1}{\alpha-\beta}},\;\left(\frac{1}{\left(1-\rho_{W}\right)^{2}} \right)^{\max\left\{\frac{1}{\alpha},\frac{1}{\beta}\right\}}\right\}\right)\] (12)

_with \(\kappa:=L/\mu\) to ensure time-scale separation and quasi-independence of the network. For D-AdaST, we have 1_

Footnote 1: The complete convergence result can be found in (75) in Appendix.

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla\Phi\left(\bar{x}_{k} \right)\right\|^{2}\right]=\tilde{\mathcal{O}}\left(\frac{1}{K^{1-\alpha}}+ \frac{1}{\left(1-\rho_{W}\right)^{\alpha}K^{\alpha}}\right)+\tilde{\mathcal{O }}\left(\frac{1}{K^{1-\beta}}+\frac{1}{\left(1-\rho_{W}\right)K^{\beta}} \right).\] (13)

**Remark 4** (Near-optimal convergence).: _Theorem 2 implies that if the total number of iterations satisfies the conditions (12), the proposed D-AdaST algorithm converges to a stationary point exactly for Problem (1) with an \(\tilde{\mathcal{O}}\left(\epsilon^{-\left(4+\delta\right)}\right)\) sample complexity for arbitrarily small \(\delta>0\), e.g., letting\(\alpha=0.5+\delta/\left(8+2\delta\right)\) and \(\beta=0.5-\delta/\left(8+2\delta\right)\). It is worth noting that this rate is near-optimal compared to the existing lower bound of \(\Omega\left(\epsilon^{-4}\right)\)(Li et al., 2021) for a class of smooth NC-SC functions. Moreover, this result recovers the centralized TiAda algorithm (Li et al., 2023) as a special case, i.e., setting \(\rho_{W}=0\), without assuming the existence of interior optimal point (c.f., Assumption 3.3 Li et al. (2023)). To the best of our knowledge, there is no existing fully parameter-agnostic method that achieves a convergence rate of \(\tilde{\mathcal{O}}\left(\epsilon^{-4}\right)\), even in a centralized setting._

**Remark 5** (Parameter-agnostic property and transient times).: _The above results show that D-AdaST converges without requiring to know any problem-dependent parameters, i.e., \(L\), \(\mu\) and \(\rho_{W}\), or tuning the initial stepsize \(\gamma_{x}\) and \(\gamma_{y}\), and is thus parameter-agnostic. Moreover, we explicitly characterize the transient times (c.f., Eq. (12)) that ensure time-scale separation and quasi-independence of the network, respectively. Indeed, we can see that if \(\alpha\) and \(\beta\) are close to each other, the time required for time-scale separation to occur increases significantly, which has been observed in (Li et al., 2023). On the other hand, if \(\alpha\) and \(\beta\) are relatively large, then \(\tilde{\mathcal{O}}\left(1/K^{1-\alpha}+1/K^{1-\beta}\right)\) dominates the other terms, indicating independence on the network. These observations highlight the trade-offs between the convergence rate and the required duration of the transition phase._

For proper comparison, we also derive an upper bound for D-TiAda as follows. Together with the lower bound in Theorem 1, we demonstrate that without the stepsize tracking mechanism, the inconsistency among local stepsizes prevents D-TiAda from converging in the distributed setting.

**Corollary 1**.: _Under the same conditions of Theorem 2. For the proposed D-TiAda, we have_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla\Phi\left( \bar{x}_{k}\right)\right\|^{2}\right] =\tilde{\mathcal{O}}\left(\frac{1}{K^{1-\alpha}}+\frac{1}{\left( 1-\rho_{W}\right)^{\alpha}K^{\alpha}}\right)\] (14) \[+\tilde{\mathcal{O}}\left(\frac{1}{K^{1-\beta}}+\frac{1}{\left( 1-\rho_{W}\right)K^{\beta}}\right)+\tilde{\mathcal{O}}\left(\left(\zeta_{v}^{ 2}+\kappa^{2}\zeta_{u}^{2}\right)C^{2}\right).\]

## 4 Experiments

In this section, we conduct experiments to validate the theoretical findings and demonstrate the effectiveness of the proposed algorithm on real-world machine learning tasks. We compare the proposed D-AdaST with the distributed variants of AdaGrad (Duchi et al., 2011), TiAda (Li et al., 2023) and NeAda (Yang et al., 2022b), namely D-AdaGrad, D-TiAda and D-NeAda, respectively. These experiments run across multiple nodes with different networks, and we consider heterogeneous distributions of local objective functions/datasets. For example, each node can only access samples with a subset of labels on MNIST and CIFAR-10 datasets, which is a common scenario in decentralized and federated learning tasks (Sharma et al., 2023; Huang et al., 2022). The experiments cover three main tasks: synthetic function, robust training of the neural network, and training of Wasserstein GANs (Heusel et al., 2017). For the exponential factors of stepsize, we set \(\alpha=0.6\) and \(\beta=0.4\) for both D-TiAda and D-AdaST. More detailed settings and additional experiments with different initial stepsizes, data distributions and choices of \(\alpha\) and \(\beta\) can be found in Appendix A.

**Synthetic example.** We consider a distributed minimax problem with the following NC-SC local objective functions over exponential networks with \(n=50\) (\(\rho_{W}=0.71\)) and \(n=100\) (\(\rho_{W}=0.75\)).

\[f_{i}\left(x,y\right)=-\frac{1}{2}y^{2}+L_{i}xy-\frac{L_{i}^{2}}{2}x^{2}-2L_{i }x+L_{i}y,\] (15)

where \(L_{i}\sim\mathcal{U}\left(1.5,2.5\right)\). The local gradient of each node is computed with an additive \(\mathcal{N}\left(0,0.1\right)\) Gaussian noise. It follows from Figure 2 (a) and 2 (b) that the proposed D-AdaST algorithm outperforms other distributed adaptive methods for both initial stepsize settings, especially in cases with a favorable initial stepsize ratio, as illustrated in plots (b) and (d) where \(\gamma_{x}/\gamma_{y}=0.2\). Similar observation can be found in Figure 2 (c) and 2 (d), demonstrating the effectiveness of D-AdaST.

**Robust training of neural networks.** Next, we consider the task of robust training of neural networks, in the presence of adversarial perturbations on data samples (Sharma et al., 2022; Deng and Mahdavi, 2021). The problem can be formulated as \(\min\limits_{x}\max\limits_{y}1/n\sum_{i=1}^{n}f_{i}\left(x;\xi_{i}+y\right)- \eta\left\|y\right\|^{2}\), where \(x\) denotes the parameters of the model, \(y\) denotes the perturbation and \(\xi_{i}\) denotes the data sample of node \(i\). Note that if \(\eta\) is large enough, the problem is NC-SC. We conduct experiments on MNIST dataset over different networks, e.g., ring graph, exponential (exp.) graph (Ying et al., 2021) and dense graph with \(n/2\) edges for each node. We consider a heterogeneous scenario in which each node possesses only two distinct classes of labeled samples, resulting in heterogeneity among the local datasets across nodes, while the data is i.i.d within each node.

In Figure 3, we compare D-AdaST with D-AdaGrad, D-TiAda and D-NeAda, using adaptive stepsizes in AdaGrad (first row) and Adam (second row, name suffixed with Adam) respectively, it can be observed from the first three columns that the proposed D-AdaST outperforms the others on three different graphs and it is not very sensitive to the graph connectivity (i.e., \(\rho_{W}\)), demonstrating the quasi-independence of network as indicated in Theorem 2. It should be noted that Adam-like algorithms exhibit more fluctuations in the later stages of optimization as the gradient norm vanishes, leading to an inevitable increase in the Adam stepsize as the optimization process converges (Kingma and Ba, 2014). In plots (d) and (h), we further demonstrate that D-AdaST can scale efficiently with respect to the number of nodes, while keeping a constant batch-size of 64 for each node. This showcases the algorithm's ability to handle large-scale distributed scenarios effectively.

**Generative Adversarial Networks.** We further illustrate the effectiveness of D-AdaST on another popular task of training GANs, which has a generator and a discriminator used to generate and distinguish samples respectively (Goodfellow et al., 2014). In this experiment, we train Wasserstein GANs (Gulrajani et al., 2017) on CIFAR-10 dataset in a decentralized setting where each discriminator is 1-Lipschitz and has access to only two classes of samples. We compare the inception score of D-AdaST with D-Adam and D-TiAda adopting Adam-like stepsizes in Figure 4. It can be observed from the figure that D-AdaST achieves higher inception scores in three cases with different initial stepsizes, and has a small score loss as the initial step size changes. We believe that this example shows the great potential of D-AdaST in solving real-world problems.

Figure 3: Comparison of the algorithms on training robust CNN on MNIST dataset. The first row shows the results of AdaGrad-like stepsize, and the second row is for Adam-like stepsize. For the first three columns, we compare the algorithms on _different graphs_ with \(n=20\). For the last column, we show the scalability of D-AdaST in terms of number of nodes. Initial stepsizes are set as \(\gamma_{x}=0.01,\gamma_{y}=0.1\) for AdaGrad-like stepsize, and \(\gamma_{x}=0.1,\gamma_{y}=0.1\) for Adam-like stepsize.

Figure 2: Performance comparison of algorithms on quadratic functions over exponential graphs with node counts \(n=\{50,100\}\) and _different initial stepsizes_ (\(\gamma_{y}=0.1\)).

## 5 Conclusion

We introduced a new distributed adaptive minimax method, D-AdaST, designed to tackle the issue of non-convergence in nonconvex-strongly-concave minimax problems caused by the inconsistencies among locally computed adaptive stepsizes. Vanilla distributed adaptive methods could suffer from such inconsistencies, as highlighted by the carefully designed counterexamples for demonstrating their potential non-convergence. In contrast, our proposed method employs an efficient adaptive stepsize tracking protocol that not only ensures the time-scale separation, but also guarantees stepsize consistency among nodes and thus effectively eliminates steady-state errors. Theoretically, we showed that D-AdaST can achieve a near-optimal convergence rate of \(\tilde{\mathcal{O}}\left(\epsilon^{-(4+\delta)}\right)\) with any arbitrarily small \(\delta>0\). Extensive experiments on both real-world and synthetic datasets have been conducted to validate our theoretical findings across various scenarios.

## Acknowledgments

The work of Huang, Shen and Xu has been supported by the National Key R&D Program of China under Grant No. 2022YFB3102100, and in parts by National Natural Science Foundation of China under Grants 62373323, 62088101. The work of Li and He has been supported by the ETH research grant and Swiss National Science Foundation (SNSF) Starting Grant.

## References

* Antonakopoulos et al. (2021) Antonakopoulos, K., Belmega, V. E., and Mertikopoulos, P. (2021). Adaptive extra-gradient methods for min-max optimization and games. In _ICLR 2021-9th International Conference on Learning Representations_, pages 1-28.
* Borodich et al. (2021) Borodich, E., Beznosikov, A., Sadiev, A., Sushko, V., Savelyev, N., Takac, M., and Gasnikov, A. (2021). Decentralized personalized federated min-max problems. _arXiv preprint arXiv:2106.07289_.
* Bot and Bohm (2023) Bot, R. I. and Bohm, A. (2023). Alternating proximal-gradient steps for (stochastic) nonconvex-concave minimax problems. _SIAM Journal on Optimization_, 33(3):1884-1913.
* Chen et al. (2023a) Chen, C., Shen, L., Liu, W., and Luo, Z.-Q. (2023a). Efficient-adam: Communication-efficient distributed adam. _IEEE Transactions on Signal Processing_.
* Chen et al. (2022) Chen, L., Ye, H., and Luo, L. (2022). A simple and efficient stochastic algorithm for decentralized nonconvex-strongly-concave minimax optimization. _arXiv preprint arXiv:2212.02387_.
* Chen et al. (2024) Chen, L., Ye, H., and Luo, L. (2024). An efficient stochastic algorithm for decentralized nonconvex-strongly-concave minimax optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 1990-1998. PMLR.
* Chen et al. (2021) Chen, T., Sun, Y., and Yin, W. (2021). Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. _Advances in Neural Information Processing Systems_, 34:25294-25307.
* Chen et al. (2023b) Chen, X., Karimi, B., Zhao, W., and Li, P. (2023b). On the convergence of decentralized adaptive gradient methods. In _Asian Conference on Machine Learning_, pages 217-232. PMLR.

Figure 4: Training GANs on CIFAR-10 dataset over exponential graphs with \(n=10\) nodes.

Daskalakis, C., Skoulakis, S., and Zampetakis, M. (2021). The complexity of constrained min-max optimization. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 1466-1478.
* Dem'yanov and Pevnyi (1972) Dem'yanov, V. F. and Pevnyi, A. B. (1972). Numerical methods for finding saddle points. _USSR Computational Mathematics and Mathematical Physics_, 12(5):11-52.
* Deng and Mahdavi (2021) Deng, Y. and Mahdavi, M. (2021). Local stochastic gradient descent ascent: Convergence analysis and communication efficiency. In _International Conference on Artificial Intelligence and Statistics_, pages 1387-1395. PMLR.
* Diakonikolas (2020) Diakonikolas, J. (2020). Halpern iteration for near-optimal and parameter-free monotone inclusion and strong solutions to variational inequalities. In _Conference on Learning Theory_, pages 1428-1451. PMLR.
* Dinh et al. (2017) Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. (2017). Sharp minima can generalize for deep nets. In _International Conference on Machine Learning_, pages 1019-1028. PMLR.
* Duchi et al. (2011) Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. _Journal of machine learning research_, 12(7).
* Ene and Le Nguyen (2022) Ene, A. and Le Nguyen, H. (2022). Adaptive and universal algorithms for variational inequalities with optimal convergence. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 6559-6567.
* Goodfellow et al. (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. _Advances in neural information processing systems_, 27.
* Gulrajani et al. (2017) Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. (2017). Improved training of wasserstein gans. _Advances in neural information processing systems_, 30.
* Guo et al. (2021) Guo, Z., Xu, Y., Yin, W., Jin, R., and Yang, T. (2021). A novel convergence analysis for algorithms of the adam family. _arXiv preprint arXiv:2112.03459_.
* Heusel et al. (2017) Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. (2017). Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30.
* Hsieh et al. (2021) Hsieh, Y.-P., Mertikopoulos, P., and Cevher, V. (2021). The limits of min-max optimization algorithms: Convergence to spurious non-critical sets. In _International Conference on Machine Learning_, pages 4337-4348. PMLR.
* Huang et al. (2024) Huang, F., Wang, X., Li, J., and Chen, S. (2024). Adaptive federated minimax optimization with lower complexities. In _International Conference on Artificial Intelligence and Statistics_, pages 4663-4671. PMLR.
* Huang et al. (2023) Huang, F., Wu, X., and Hu, Z. (2023). Adagda: Faster adaptive gradient descent ascent methods for minimax optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 2365-2389. PMLR.
* Huang et al. (2021) Huang, F., Wu, X., and Huang, H. (2021). Efficient mirror descent ascent methods for nonsmooth minimax problems. _Advances in Neural Information Processing Systems_, 34:10431-10443.
* Huang et al. (2022) Huang, Y., Sun, Y., Zhu, Z., Yan, C., and Xu, J. (2022). Tackling data heterogeneity: A new unified framework for decentralized SGD with sample-induced topology. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 9310-9345. PMLR.
* Ju et al. (2023) Ju, L., Zhang, T., Toor, S., and Hellander, A. (2023). Accelerating fair federated learning: Adaptive federated adam. _arXiv preprint arXiv:2301.09357_.
* Kavis et al. (2022) Kavis, A., Levy, K. Y., and Cevher, V. (2022). High probability bounds for a class of nonconvex algorithms with adagrad stepsize. In _International Conference on Learning Representations_.
* Krizhevsky et al. (2012)Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_.
* Li et al. (2022) Li, H., Farnia, F., Das, S., and Jadbabaie, A. (2022). On convergence of gradient descent ascent: A tight local analysis. In _International Conference on Machine Learning_, pages 12717-12740. PMLR.
* Li et al. (2021) Li, H., Tian, Y., Zhang, J., and Jadbabaie, A. (2021). Complexity lower bounds for nonconvex-strongly-concave min-max optimization. _Advances in Neural Information Processing Systems_, 34:1792-1804.
* Li et al. (2023) Li, X., YANG, J., and He, N. (2023). Tiada: A time-scale adaptive algorithm for nonconvex minimax optimization. In _The Eleventh International Conference on Learning Representations_.
* Lian et al. (2017) Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J. (2017). Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. _Advances in Neural Information Processing Systems_, 30.
* Liggett (2022) Liggett, B. (2022). Distributed learning with automated stepsizes.
* Lin et al. (2020) Lin, T., Jin, C., and Jordan, M. (2020). On gradient descent ascent for nonconvex-concave minimax problems. In _International Conference on Machine Learning_, pages 6083-6093. PMLR.
* Liu et al. (2020) Liu, M., Zhang, W., Mroueh, Y., Cui, X., Ross, J., Yang, T., and Das, P. (2020). A decentralized parallel algorithm for training generative adversarial nets. _Advances in Neural Information Processing Systems_, 33:11056-11070.
* Madras et al. (2018) Madras, D., Creager, E., Pitassi, T., and Zemel, R. (2018). Learning adversarially fair and transferable representations. In _International Conference on Machine Learning_, pages 3384-3393. PMLR.
* Mohri et al. (2019) Mohri, M., Sivek, G., and Suresh, A. T. (2019). Agnostic federated learning. In _International Conference on Machine Learning_, pages 4615-4625. PMLR.
* Nedic and Ozdaglar (2009) Nedic, A. and Ozdaglar, A. (2009). Distributed subgradient methods for multi-agent optimization. _IEEE Transactions on Automatic Control_, 54(1):48-61.
* Nedic et al. (2010) Nedic, A., Ozdaglar, A., and Parrilo, P. A. (2010). Constrained consensus and optimization in multi-agent networks. _IEEE Transactions on Automatic Control_, 55(4):922-938.
* Nemirovski et al. (2009) Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. (2009). Robust stochastic approximation approach to stochastic programming. _SIAM Journal on optimization_, 19(4):1574-1609.
* Pu and Nedic (2021) Pu, S. and Nedic, A. (2021). Distributed stochastic gradient tracking methods. _Mathematical Programming_, 187(1):409-457.
* Reddi et al. (2018) Reddi, S. J., Kale, S., and Kumar, S. (2018). On the convergence of adam and beyond. In _International Conference on Learning Representations_.
* Sharma et al. (2023) Sharma, P., Panda, R., and Joshi, G. (2023). Federated minimax optimization with client heterogeneity. _arXiv preprint arXiv:2302.04249_.
* Sharma et al. (2022) Sharma, P., Panda, R., Joshi, G., and Varshney, P. (2022). Federated minimax optimization: Improved convergence analyses and algorithms. In _International Conference on Machine Learning_, pages 19683-19730. PMLR.
* Sinha et al. (2017) Sinha, A., Namkoong, H., Volpi, R., and Duchi, J. (2017). Certifying some distributional robustness with principled adversarial training. _arXiv preprint arXiv:1710.10571_.
* Tarzanagh et al. (2022) Tarzanagh, D. A., Li, M., Thrampoulidis, C., and Oymak, S. (2022). Fednest: Federated bilevel, minimax, and compositional optimization. In _International Conference on Machine Learning_, pages 21146-21179. PMLR.
* Tsaknakis et al. (2020) Tsaknakis, I., Hong, M., and Liu, S. (2020). Decentralized min-max optimization: Formulations, algorithms and applications in network poisoning attack. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 5755-5759. IEEE.
* Tikhonov and Arsenkin (2017)Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H. V. (2020). Tackling the objective inconsistency problem in heterogeneous federated optimization. _Advances in neural information processing systems_, 33:7611-7623.
* Wang et al. (2021) Wang, J., Zhang, T., Liu, S., Chen, P.-Y., Xu, J., Fardad, M., and Li, B. (2021). Adversarial attack generation empowered by min-max optimization. _Advances in Neural Information Processing Systems_, 34:16020-16033.
* Wu et al. (2023) Wu, X., Sun, J., Hu, Z., Zhang, A., and Huang, H. (2023). Solving a class of non-convex minimax optimization in federated learning. In _Thirty-seventh Conference on Neural Information Processing Systems_.
* Xian et al. (2021) Xian, W., Huang, F., Zhang, Y., and Huang, H. (2021). A faster decentralized algorithm for nonconvex minimax problems. _Advances in Neural Information Processing Systems_, 34:25865-25877.
* Xiao et al. (2006) Xiao, L., Boyd, S., and Lall, S. (2006). Distributed average consensus with time-varying metropolis weights. _Automatica_, 1:1-4.
* Yang et al. (2022a) Yang, H., Liu, Z., Zhang, X., and Liu, J. (2022a). Sagda: Achieving \(\mathcal{O}\left(\varepsilon^{-2}\right)\) communication complexity in federated min-max learning. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, _Advances in Neural Information Processing Systems_, volume 35, pages 7142-7154. Curran Associates, Inc.
* Yang et al. (2022b) Yang, J., Li, X., and He, N. (2022b). Nest your adaptive algorithm for parameter-agnostic nonconvex minimax optimization. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K., editors, _Advances in Neural Information Processing Systems_.
* Yang et al. (2022c) Yang, J., Orvieto, A., Lucchi, A., and He, N. (2022c). Faster single-loop algorithms for minimax optimization without strong concavity. In _International Conference on Artificial Intelligence and Statistics_, pages 5485-5517. PMLR.
* Ying et al. (2021) Ying, B., Yuan, K., Chen, Y., Hu, H., Pan, P., and Yin, W. (2021). Exponential graph is provably efficient for decentralized deep training. _Advances in Neural Information Processing Systems_, 34:13975-13987.
* Yuan et al. (2016) Yuan, K., Ling, Q., and Yin, W. (2016). On the convergence of decentralized gradient descent. _SIAM Journal on Optimization_, 26(3):1835-1854.
* Zhang et al. (2023) Zhang, S., Choudhury, S., Stich, S. U., and Loizou, N. (2023). Communication-efficient gradient descent-accent methods for distributed variational inequalities: Unified analysis and local updates. _arXiv preprint arXiv:2306.05100_.
* Zhang et al. (2021a) Zhang, S., Yang, J., Guzman, C., Kiyavash, N., and He, N. (2021a). The complexity of nonconvex-strongly-concave minimax optimization. In _Uncertainty in Artificial Intelligence_, pages 482-492. PMLR.
* Zhang et al. (2021b) Zhang, X., Liu, Z., Liu, J., Zhu, Z., and Lu, S. (2021b). Taming communication and sample complexities in decentralized policy evaluation for cooperative multi-agent reinforcement learning. _Advances in Neural Information Processing Systems_, 34:18825-18838.
* Zhang et al. (2024) Zhang, X., Mancino-Ball, G., Aybat, N. S., and Xu, Y. (2024). Jointly improving the sample and communication complexities in decentralized stochastic minimax optimization. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 20865-20873.
* Zhou et al. (2018) Zhou, D., Chen, J., Cao, Y., Tang, Y., Yang, Z., and Gu, Q. (2018). On the convergence of adaptive gradient methods for nonconvex optimization. _arXiv preprint arXiv:1808.05671_.
* Zou et al. (2019) Zou, F., Shen, L., Jie, Z., Zhang, W., and Liu, W. (2019). A sufficient condition for convergences of adam and rmsprop. In _Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition_, pages 11127-11135.

Additional Experiments

In this section, we provide detailed experimental settings and perform additional experiments on the task of training robust neural networks with different choices of hyper-parameters. All experiments are deployed in a server with Intel Xeon E5-2680 v4 CPU @ 2.40GHz and 8 Nvidia RTX 3090 GPUs, and implemented using distributed communication package _torch.distributed_ in PyTorch 2.0, where each process serves as a node, and we use inter-process communication to mimic communication between nodes. For the AdaGrad-like algorithms considered in the experiments of training neural networks, similar to the Adam-like stepsize, we adopt a coordinate-wise adaptive stepsize rule as commonly used in existing centralized adaptive methods (Yang et al., 2022; Li et al., 2023). Moreover, since we attempt to develop a parameter-agnostic algorithm that does not need much effort in tuning hyper-parameters, we set \(\alpha=0.6\) and \(\beta=0.4\) for all tasks in the main text, and evaluate the effect of the choices of \(\alpha\) and \(\beta\) on the performance of D-AdaST individually in an additional experiment on the synthetic objective function as shown in Appendix A.4.

### Experimental details

**Communication topology.** For the experiments in the main text, we utilize three commonly used communication topologies: indirect ring, exponential graph and dense graph. An indirect ring is a sparse graph in which each node is sequentially connected to form a ring, with only two neighbors per node. Exponential graph (Ying et al., 2021) is a directed graph where each node is connected to nodes at distances of \(2^{0},2^{1}...,2^{\log n}\). Exponential graphs achieve a good balance between the degree and connectivity of the graph. A dense graph is an indirect graph where each node is connected to nodes at distances of \(1,2,4,...,n\). We also consider directed ring and fully connected graphs, which are more sparsely and densely connected, respectively, in the additional experiments.

**Robust training of neural network.** In this task, we train CNNs with three convolutional layers and one fully connected layer on MNIST dataset containing images of 10 classes. Each layer adopts batch normalization and ELU activation. The total batch-size is 1280, and the batch-size of each node during training is \(1280/n\). For Adam-like algorithms, we set the first and second moment parameters as \(\beta_{1}=0.9,\beta_{2}=0.999\) respectively. Since NeAda is a double-loop algorithm, for fair comparison, we imply D-AdaGrad and D-Adam using 15 iterations of inner loop in this task.

**Generative Adversarial Networks.** In this task, we train Wasserstein GANs on CIFAR-10 dataset, where the model used for discriminator is a four layer CNNs, and for generator is a four layer CNNs with transpose convolution layers. The total batch-size is 1280, and the batch-size of each node during training is 128 with 10 nodes. For Adam-like algorithms, we use \(\beta_{1}=0.5,\beta_{2}=0.9\). To obtain the inception score, we use 8000 artificially generated samples to feed the previously trained inception network.

### Additional experiments on robust training of neural network.

In this part, we conduct additional experiments on robust training of CNNs on MNIST dataset considering a variety of settings. We compare the convergence performance of D-AdaST with D-AdaGrad, D-TiAda and D-NeAda using adaptive stepsizes of AdaGrad and Adam. Unless otherwise specified, the total batch-size is set to 1280; the initial stepsizes for \(x\) and \(y\) are assigned as \(\gamma_{x}=0.01,\gamma_{y}=0.1\) for AdaGrad-like algorithms, and \(\gamma_{x}=\gamma_{y}=0.1\) for Adam-like algorithms. Specifically, we consider two extra graphs that are more sparse and more dense, respectively in Figure 5, e.g., directed ring and fully-connected (fc) graphs. We consider more initial stepsizes settings for \(x\) and \(y\) respectively in Figure 6. Further, we also consider different data distributions where each node has samples from 4 of the 10 classes in Figure 7. Finally, we perform a comparison experiment with 40 nodes in Figure 8. Under all settings, the proposed D-AdaST outperforms the others, demonstrating the superiority of D-AdaST.

### Additional experiments on training GANs

We provide additional experiments of training GANs on a more complicated dataset CIFAR-100 to further illustrate the effectiveness of the proposed D-AdaST, as shown in Figure 9. We use the entire training set of CIFAR-100 with coarse labels (20 classes) to train GANs over networks, where each node is assigned with four distinct classes of labeled samples. Under the same settings as in Figure 4 (a), it can be observed that D-AdaST outperforms the others in terms of the inception score. Together with other experimental results in the main text, we believe that we have demonstrated the effectiveness of the proposed D-AdaST method and its potential for further real-world applications.

Figure 5: Performance comparison of training CNN on MNIST with \(n=20\) nodes over _directed ring and fully connected graphs_.

Figure 8: Performance comparison of training CNN on MNIST with \(n=40\) nodes over exponential and dense graphs.

Figure 6: Performance comparison of training CNN on MNIST with \(n=20\) nodes with _different initial stepsizes_\(\gamma_{x}\) and \(\gamma_{y}\).

Figure 7: Performance comparison of training CNN on MNIST with \(n=20\) nodes over exponential and dense graphs where each node has _4 sample classes_.

### Additional experiments with different choices of \(\alpha\) and \(\beta\)

In this part, we evaluate the effect of the choices of \(\alpha\) and \(\beta\) on the performance of D-AdaST. In particular, we provide an additional experimental result on the synthetic quadratic objective functions (15) with a larger ratio of initial stepsizes, i.e., \(\gamma_{x}/\gamma_{y}=20\) (indicating faster minimization and slower maximization processes at the beginning). As shown in Figure 10, it can be observed that the transient time (iteration before the inflection point) becomes longer as \(\alpha-\beta\) decreases, while the convergence rate is relatively faster, which is consistent with Theorem 2 and the result in the centralized TiAda algorithm (c.f., Figure 5, Li et al., 2023).

## Appendix B Proof of the main results

We recall here some definitions used in the main text. The averaged variables and the inconsistency are defined as follows:

\[\bar{x}_{k}:=\frac{\mathbf{1}^{T}}{n}\mathbf{x}_{k},\quad\bar{v}_{ k}:=\frac{1}{n}\sum_{i=1}^{n}v_{i,k},\quad\left(\boldsymbol{\tilde{v}}_{k}^{- \alpha}\right)^{T}:=\left[\cdots,v_{i,k}^{-\alpha}-\bar{v}_{k}^{-\alpha}, \cdots\right],\] \[\bar{y}_{k}:=\frac{\mathbf{1}^{T}}{n}\mathbf{y}_{k},\quad\bar{u}_ {k}:=\frac{1}{n}\sum_{i=1}^{n}u_{i,k},\quad\left(\boldsymbol{\tilde{u}}_{k}^{- \beta}\right)^{T}:=\left[\cdots,u_{i,k}^{-\beta}-\bar{u}_{k}^{-\beta},\cdots \right].\]

Figure 10: Performance comparison of D-AdaST on quadratic functions over an exponential graph of \(n=50\) nodes with different choices of \(\alpha\) and \(\beta\).

Figure 9: Performance comparison of D-AdaST with D-Adam and D-TiAda adopting Adam-like stepsizes for training GANs on CIFAR-100 with coarse labels over the exponential graph consisting of \(n=10\) nodes under initial stepsizes \(\gamma_{x}=\gamma_{y}=0.001\).

The inconsistency of stepsizes of the primal and dual variables is defined as follows:

\[\zeta_{v}^{2}:=\sup_{i\in[n],k>0}\left\{\left(v_{i,k}^{-\alpha}-\bar{v}_{k}^{- \alpha}\right)^{2}/\left(\bar{v}_{k}^{-\alpha}\right)^{2}\right\},\ \zeta_{u}^{2}:=\sup_{i\in[n],k>0}\left\{\left(u_{i,k}^{-\beta}-\bar{u}_{k}^{- \beta}\right)^{2}/\left(\bar{u}_{k}^{-\beta}\right)^{2}\right\}.\]

**Proof Sketch.** The convergence analysis of the main results in Theorem 2 is mainly based on carefully analyzing the average system as shown in (5), and the difference between the distributed system and the averaged system. In general, under Assumption 1-4, we first give a telescoped descent lemma from \(0\) to \(K-1\) iterations in Lemma 3, which is upper bounded by the following key error terms:

* \(S_{1}:=\frac{1}{nK}\sum_{k=0}^{K-1}\mathbb{E}\left[\bar{v}_{k+1}^{-\alpha}\left\| \nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2 }\right]\): The asymptotically decaying terms by adopting adaptive stepsize;
* \(S_{2}:=\frac{1}{nK}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\mathbf{x}_{k}- \mathbf{1}\bar{x}_{k}\right\|^{2}+\left\|\mathbf{y}_{k}-\mathbf{1}\bar{y}_{k }\right\|^{2}\right]\): The consensus error of \(x\) and \(y\) between the distributed system and the average system;
* \(S_{3}:=\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[f\left(\bar{x}_{k},y^{*} \left(\bar{x}_{k}\right)\right)-f\left(\bar{x}_{k},\bar{y}_{k}\right)\right]\): The optimality gap in dual variable \(y\);
* \(S_{4}:=\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\frac{\left(\bar{v}_{ k+1}^{-\alpha}\right)^{T}}{n\bar{v}_{k+1}^{-\alpha}}\nabla_{x}F\left( \mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right]\): The inconsistency of stepsize of \(x\).

Next, we prove the contraction properties of these terms in Lemma 4-8 and Lemma 9 respectively. Finally, these results are integrated into the descent lemma to complete the proof. We note that the proof is not trivial in the sense that these terms are coupled and therefore are needed to be carefully analyzed. This proof can also be adapted to analyze the coordinate-wise adaptive stepsize variant of D-AdaST as explained in Appendix B.5, which is of independent interest.

### Supporting lemmas

In this part, we provide several supporting lemmas that have been shown in the existing literature, which are essential to the subsequent convergence analysis.

**Lemma 1** (Lemma A.2 in Yang et al. (2022b)).: _Let \(\left\{x_{t}\right\}_{t=0}^{T-1}\) be a sequence of non-negative real numbers, \(x_{0}>0\) and \(\alpha\in(0,1)\). Then we have,_

\[\left(\sum_{t=0}^{T-1}x_{t}\right)^{1-\alpha}\leqslant\sum_{t=0}^{T-1}\frac{x _{t}}{\left(\sum_{k=0}^{t}x_{k}\right)^{\alpha}}\leqslant\frac{1}{1-\alpha} \left(\sum_{t=0}^{T-1}x_{t}\right)^{1-\alpha}.\] (16)

_When \(\alpha=0\), we have_

\[\sum_{t=0}^{T-1}\frac{x_{t}}{\left(\sum_{k=0}^{t}x_{k}\right)^{\alpha}} \leqslant 1+\log\left(\frac{\sum_{t=0}^{T-1}x_{t}}{x_{0}}\right).\] (17)

**Lemma 2**.: _Suppose Assumption 1 and 2 hold. Define \(\varPhi\left(x\right):=f\left(x,y^{*}\left(x\right)\right)\) as the envelope function and \(y^{*}\left(x\right)=\operatorname*{argmax}_{y\in\mathcal{Y}}f\left(x,y\right)\). Then, we have,_

* \(\varPhi\left(\cdot\right)\) _is_ \(L_{\varPhi}\)_-smooth with_ \(L_{\varPhi}=L\left(1+\kappa\right)\)_, and_ \(\nabla\varPhi\left(x\right)=\nabla_{x}f\left(x,y^{*}\left(x\right)\right)\) _(c.f., Lemma_ 4.3 _in Lin et al. (_2020_));_
* \(y^{*}\left(\cdot\right)\) _is_ \(\kappa\)_-Lipschitz and_ \(\hat{L}\)_-smooth with_ \(\hat{L}=\kappa\left(1+\kappa\right)^{2}\)_(c.f., Lemma_ 2 _in Chen et al. (_2021_))._

### Key Lemmas

In this subsection, we give the key lemmas to help the analysis of the main results. For simplicity, we define \(\Delta_{k}:=\left\|\mathbf{x}_{k}-\mathbf{1}\bar{x}_{k}\right\|^{2}+\left\| \mathbf{y}_{k}-\mathbf{1}\bar{y}_{k}\right\|^{2}\) as the consensus error for primal and dual variables. Then, we have the following lemmas.

**Lemma 3** (Descent lemma).: _Suppose Assumption 1-4 hold. Then, we have_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla\Phi\left( \bar{x}_{k}\right)\right\|^{2}\right]\] (18) \[\leqslant\frac{8C^{2\alpha}\left(\Phi^{\max}-\Phi^{*}\right)}{ \gamma_{x}K^{1-\alpha}}-\frac{4}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\| \nabla_{x}f\left(\bar{x}_{k},\bar{y}_{k}\right)\right\|^{2}\right]\] \[+8\gamma_{x}L_{\Phi}\left(1+\zeta_{v}^{2}\right)\underbrace{ \frac{1}{nK}\sum_{k=0}^{K-1}\mathbb{E}\left[\bar{v}_{k+1}^{-\alpha}\left\| \nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2 }\right]}_{S_{1}}+8L^{2}\underbrace{\frac{1}{nK}\sum_{k=0}^{K-1}\mathbb{E} \left[\Delta_{k}\right]}_{S_{2}}\] \[+8\kappa L\underbrace{\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E} \left[f\left(\bar{x}_{k},y^{*}\left(\bar{x}_{k}\right)\right)-f\left(\bar{x}_ {k},\bar{y}_{k}\right)\right]}_{S_{3}}+16\underbrace{\frac{1}{K}\sum_{k=0}^{K- 1}\mathbb{E}\left[\left\|\frac{\left(\widetilde{\mathbf{v}}_{k+1}^{-\alpha} \right)^{T}}{n\bar{v}_{k+1}^{-\alpha}}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{ y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right]}_{S_{4}},\]

_where \(\kappa:=L/\mu\) is the condition number of the function in \(y\), \(\Phi^{\max}=\max\limits_{x}\Phi\left(x\right),\Phi^{*}=\min\limits_{x}\Phi \left(x\right)\)._

Proof.: By the smoothness of \(\Phi\) given in Lemma 2, i.e.,

\[\Phi\left(\bar{x}_{k+1}\right)-\Phi\left(\bar{x}_{k}\right)\leqslant\left\langle \nabla\Phi\left(\bar{x}_{k}\right),\bar{x}_{k+1}-\bar{x}_{k}\right\rangle+ \frac{L_{\Phi}}{2}\left\|\bar{x}_{k+1}-\bar{x}_{k}\right\|^{2},\]

and noticing that the scalar \(\bar{v}_{k},\bar{u}_{k}\) are random variables, we have

\[\mathbb{E}\left[\frac{\Phi\left(\bar{x}_{k+1}\right)-\Phi\left( \bar{x}_{k}\right)}{\gamma_{x}\bar{v}_{k+1}^{-\alpha}}\right]\] (19) \[\leqslant-\mathbb{E}\left[\left\langle\nabla\Phi\left(\bar{x}_{k }\right),\frac{\mathbf{1}^{T}}{n}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k };\xi_{k}\right)\right\rangle\right]-\mathbb{E}\left[\left\langle\nabla\Phi \left(\bar{x}_{k}\right),\frac{\left(\widetilde{\mathbf{v}}_{k+1}^{-\alpha} \right)^{T}}{n\bar{v}_{k+1}^{-\alpha}}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{ y}_{k};\xi_{k}^{x}\right)\right\rangle\right]\] \[+\frac{\gamma_{x}L_{\Phi}}{2}\mathbb{E}\left[\frac{1}{\bar{v}_{ k+1}^{-\alpha}}\left\|\left(\frac{\bar{v}_{k+1}^{-\alpha}\mathbf{1}^{T}}{n}+ \frac{\left(\widetilde{\mathbf{v}}_{k+1}^{-\alpha}\right)^{T}}{n}\right) \nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2} \right],\]

where we have used the definition of \(\bar{x}_{k+1}\) as presented in (5). Then, we bound the inner-product terms on the RHS. Firstly,

\[-\mathbb{E}\left[\left\langle\nabla\Phi\left(\bar{x}_{k}\right), \frac{\mathbf{1}^{T}}{n}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k} ^{x}\right)\right\rangle\right]\] (20) \[=-\mathbb{E}\left[\left\langle\nabla\Phi\left(\bar{x}_{k}\right), \frac{\mathbf{1}^{T}}{n}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k}\right)- \frac{\mathbf{1}^{T}}{n}\nabla_{x}F\left(\mathbf{1}\bar{x}_{k},\mathbf{1}\bar{ y}_{k}\right)+\frac{\mathbf{1}^{T}}{n}\nabla_{x}F\left(\mathbf{1}\bar{x}_{k}, \mathbf{1}\bar{y}_{k}\right)\right\rangle\right]\] \[\leqslant\frac{1}{4}\mathbb{E}\left[\left\|\nabla\Phi\left(\bar{x }_{k}\right)\right\|^{2}\right]+\mathbb{E}\left[\left\|\frac{\mathbf{1}^{T}}{n} \nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k}\right)-\frac{\mathbf{1}^{T}}{n} \nabla_{x}F\left(\mathbf{1}\bar{x}_{k},\mathbf{1}\bar{y}_{k}\right)\right\|^{2 }\right]\] \[+\frac{1}{2}\left(\mathbb{E}\left[\left\|\nabla\Phi\left(\bar{x }_{k}\right)-\nabla_{x}f\left(\bar{x}_{k},\bar{y}_{k}\right)\right\|^{2} \right]-\mathbb{E}\left[\left\|\nabla\Phi\left(\bar{x}_{k}\right)\right\|^{2 }\right]-\mathbb{E}\left[\left\|\nabla_{x}f\left(\bar{x}_{k},\bar{y}_{k} \right)\right\|^{2}\right]\right)\] \[\leqslant-\frac{1}{4}\mathbb{E}\left[\left\|\nabla\Phi\left(\bar{x }_{k}\right)\right\|^{2}\right]+\frac{L^{2}}{n}\mathbb{E}\left[\Delta_{k} \right]+\frac{L^{2}}{2}\mathbb{E}\left[\left\|\bar{y}_{k}-y^{*}\left(\bar{x}_{k }\right)\right\|^{2}\right]-\frac{1}{2}\mathbb{E}\left[\left\|\nabla_{x}f \left(\bar{x}_{k},\bar{y}_{k}\right)\right\|^{2}\right].\]

wherein the last inequality we have used the smoothness of the objective functions. Then, for the second inner-product in (19), using Young's inequality we get

\[-\mathbb{E}\left[\left\langle\nabla\Phi\left(\bar{x}_{k}\right), \frac{\left(\widetilde{\mathbf{v}}_{k+1}^{-\alpha}\right)^{T}}{n\bar{v}_{k+1}^{- \alpha}}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\rangle\right]\] (21) \[\leqslant\frac{1}{8}\mathbb{E}\left[\left\|\nabla\Phi\left(\bar{x }_{k}\right)\right\|^{2}\right]+2\mathbb{E}\left[\left\|\frac{\left( \widetilde{\mathbf{v}}_{k+1}^{-\alpha}\right)^{T}}{n\bar{v}_{k+1}^{-\alpha}} \nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2} \right].\]

[MISSING_PAGE_FAIL:19]

Proof.: With the help of Lemma 1 and Assumption 3, taking the primal variable \(x\) as an example, and noticing that \(v_{i,0}>0,i\in[n]\), we have

\[\frac{1}{K}\,\sum_{k=0}^{K-1}\mathbb{E}\left[\bar{v}_{k+1}^{-\alpha }\left\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right) \right\|^{2}\right]\] \[=\frac{1}{K}\sum_{k=0}^{K-1}\frac{1}{n}\sum_{i=1}^{n}\frac{\left\| \nabla_{x}F_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^{x}\right)\right\|^{2}}{\bar{v} _{k+1}^{\alpha}}\] \[\leqslant\frac{1}{K}\sum_{k=0}^{K-1}\frac{1}{n}\sum_{i=1}^{n} \frac{\left\|\nabla_{x}F_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^{x}\right)\right\| ^{2}}{\left(\sum_{t=0}^{k}\frac{1}{n}\sum_{j=1}^{n}\left\|\nabla_{x}F_{j} \left(x_{j,t},y_{j,t};\xi_{j,t}^{x}\right)\right\|^{2}\right)^{\alpha}}\] \[\leqslant\frac{1}{1-\alpha}\frac{1}{K}\left(\sum_{k=0}^{K-1} \frac{1}{n}\sum_{i=1}^{n}\left\|\nabla_{x}F_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^ {x}\right)\right\|^{2}\right)^{1-\alpha}\leqslant\frac{C^{2-2\alpha}}{\left(1 -\alpha\right)K^{\alpha}}.\]

The similar result can be obtained for dual variable \(y\) and we thus complete the proof. 

Next, we bound the the consensus error term \(S_{2}\) in the following lemma.

**Lemma 5**.: _Suppose Assumption 1-4 hold. Then, we have_

\[\frac{1}{K}\sum_{k=0}^{K}\mathbb{E}\left[\Delta_{k}\right]\leqslant \frac{2\mathbb{E}\left[\Delta_{0}\right]}{\left(1-\rho_{W}\right)K}\] (27) \[+\frac{8n\rho_{W}\gamma_{x}^{2}\left(1+\zeta_{v}^{2}\right)}{ \left(1-\rho_{W}\right)^{2}}\left(\frac{C^{2-4\alpha}}{\left(1-2\alpha\right) K^{2\alpha}}\mathbb{I}_{\alpha<1/2}+\frac{1+\log v_{K}-\log v_{1}}{K\bar{v}_{1}^{2 \alpha-1}}\mathbb{I}_{\alpha\geqslant 1/2}\right)\] \[+\frac{8n\rho_{W}\gamma_{y}^{2}\left(1+\zeta_{u}^{2}\right)}{ \left(1-\rho_{W}\right)^{2}}\left(\frac{C^{2-4\beta}}{\left(1-2\beta\right) K^{2\beta}}\mathbb{I}_{\beta<1/2}+\frac{1+\log u_{K}-\log u_{1}}{K\bar{u}_{1}^{2 \beta-1}}\mathbb{I}_{\beta\geqslant 1/2}\right),\]

_where \(\mathbb{I}_{[\cdot]}\in\{0,1\}\) is the indicator for specific condition, and the initial consensus error \(\Delta_{0}\) can be set to \(0\) with proper initialization._

Proof.: By the updating rule of the primal variable, we have

\[\mathbb{E}\left[\left\|\mathbf{x}_{k+1}-\mathbf{1}\bar{x}_{k+1} \right\|^{2}\right]\] (28) \[=\mathbb{E}\left[\left\|W\left(\mathbf{x}_{k}-\gamma_{x}V_{k+1}^ {-\alpha}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right) \right)-\mathbf{J}\left(\mathbf{x}_{k}-\gamma_{x}V_{k+1}^{-\alpha}\nabla_{x}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right)\right\|^{2}\right]\] \[\leqslant\frac{1+\rho_{W}}{2}\mathbb{E}\left[\left\|\mathbf{x}_{k }-\mathbf{1}\bar{x}_{k}\right\|^{2}\right]+\frac{2\gamma_{x}^{2}\left(1+\rho_{W }\right)\rho_{W}}{1-\rho_{W}}\mathbb{E}\left[\bar{v}_{k+1}^{-2\alpha}\left\| \nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right]\] \[+\frac{2\gamma_{x}^{2}\left(1+\rho_{W}\right)\rho_{W}}{1-\rho_{W }}\mathbb{E}\left[\left\|\left(V_{k+1}^{-\alpha}-\bar{v}_{k+1}^{-\alpha} \mathbf{I}\right)\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x} \right)\right\|^{2}\right],\]

where we have used Young's inequality. Then, by the definition of \(\zeta_{v}\) in (8), we have

\[\mathbb{E}\left[\left\|\left(V_{k+1}^{-\alpha}-\bar{v}_{k+1}^{-\alpha}\mathbf{I }\right)\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right) \right\|^{2}\right]\leqslant\zeta_{v}^{2}\mathbb{E}\left[\bar{v}_{k+1}^{-2 \alpha}\left\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x} \right)\right\|^{2}\right],\] (29)

and thus

\[\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\mathbf{x}_{k+1}-\mathbf{1 }\bar{x}_{k+1}\right\|^{2}\right]\] (30) \[\leqslant\frac{2}{1-\rho_{W}}\mathbb{E}\left[\left\|\mathbf{x}_{k }-\mathbf{1}\bar{x}_{k}\right\|^{2}\right]+\frac{8\gamma_{x}^{2}\rho_{W}\left(1+ \zeta_{v}^{2}\right)}{\left(1-\rho_{W}\right)^{2}}\sum_{k=0}^{K-1}\mathbb{E} \left[\bar{v}_{k+1}^{-2\alpha}\left\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y} _{k};\xi_{k}^{x}\right)\right\|^{2}\right]\!.\]Then, we bound the last term on the RHS of the above inequality by Lemma 4. For the case \(\alpha<1/2\), by Assumption 3 we have

\[\begin{split}&\sum_{k=0}^{K-1}\mathbb{E}\left[\bar{v}_{k+1}^{-2 \alpha}\left\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x} \right)\right\|^{2}\right]\\ &=\sum_{k=0}^{K-1}\sum_{i=1}^{n}\mathbb{E}\left[\frac{\left\| \nabla_{x}F_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^{x}\right)\right\|^{2}}{\bar{v}_ {k+1}^{2\alpha}}\right]\leqslant\frac{n\left(KC^{2}\right)^{1-2\alpha}}{(1-2 \alpha)}.\end{split}\] (31)

For the case \(\alpha\geqslant 1/2\), with the help of Lemma 1, we have

\[\begin{split}&\sum_{k=0}^{K-1}\mathbb{E}\left[\bar{v}_{k+1}^{-2 \alpha}\left\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x} \right)\right\|^{2}\right]\\ &=\sum_{k=0}^{K-1}\sum_{i=1}^{n}\mathbb{E}\left[\frac{\left\| \nabla_{x}F_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^{x}\right)\right\|^{2}}{\bar{v} _{k+1}\cdot\bar{v}_{k+1}^{2\alpha-1}}\right]\leqslant\frac{n\left(1+\log v_{T }-\log v_{1}\right)}{\bar{v}_{1}^{2\alpha-1}}.\end{split}\] (32)

For the dual variable, we have

\[\begin{split}\mathbf{y}_{k+1}&=\mathcal{P}_{ \mathcal{Y}}\left(W\left(\mathbf{y}_{k}+\gamma_{y}U_{k+1}^{-\beta}\nabla_{y}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right)\right)\\ &=W\mathbf{y}_{k}+\gamma_{y}\nabla_{y}\hat{G}\end{split}\]

where

\[\nabla_{y}\hat{G}=\frac{1}{\gamma_{y}}\left(\mathcal{P}_{\mathcal{Y}}\left(W \left(\mathbf{y}_{k}+\gamma_{y}U_{k+1}^{-\beta}\nabla_{y}F\left(\mathbf{x}_{k },\mathbf{y}_{k};\xi_{k}^{y}\right)\right)\right)-W\mathbf{y}_{k}\right).\]

Then, using Young's inequality with parameter \(\lambda\), we have

\[\begin{split}&\mathbb{E}\left[\left\|\mathbf{y}_{k+1}-\mathbf{1} \bar{y}_{k+1}\right\|^{2}\right]\\ &=\mathbb{E}\left[\left\|W\mathbf{y}_{k}+\gamma_{y}\nabla_{y} \hat{G}-\mathbf{J}\left(W\mathbf{y}_{k}+\gamma_{y}\nabla_{y}\hat{G}\right) \right\|^{2}\right]\\ &\leqslant(1+\lambda)\,\rho_{W}\mathbb{E}\left[\left\|\mathbf{y} _{k}-\mathbf{J}\mathbf{y}_{k}\right\|^{2}\right]\\ &+\left(1+\frac{1}{\lambda}\right)\mathbb{E}\left[\left\|\mathcal{ P}_{\mathcal{Y}}\left(W\left(\mathbf{y}_{k}+\gamma_{y}U_{k+1}^{-\beta}\nabla_{y}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right)\right)-W \mathbf{y}_{k}\right\|^{2}\right]\\ &\leqslant\frac{1+\rho_{W}}{2}\mathbb{E}\left[\left\|\mathbf{y} _{k}-\mathbf{J}\mathbf{y}_{k}\right\|^{2}\right]\\ &+\frac{1+\rho_{W}}{1-\rho_{W}}\mathbb{E}\left[\left\|\mathcal{ P}_{\mathcal{Y}}\left(W\left(\mathbf{y}_{k}+\gamma_{y}U_{k+1}^{-\beta}\nabla_{y}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right)\right)-W \mathbf{y}_{k}\right\|^{2}\right].\end{split}\]

Noticing that \(W\mathbf{y}_{k}=\mathcal{P}_{\mathcal{Y}}\left(W\mathbf{y}_{k}\right)\) holds for convex set \(\mathcal{Y}\), we get

\[\begin{split}&\mathbb{E}\left[\left\|\mathbf{y}_{k+1}-\mathbf{1} \bar{y}_{k+1}\right\|^{2}\right]\\ &\leqslant\frac{1+\rho_{W}}{2}\mathbb{E}\left[\left\|\mathbf{y} _{k}-\mathbf{J}\mathbf{y}_{k}\right\|^{2}\right]\\ &+\frac{1+\rho_{W}}{1-\rho_{W}}\mathbb{E}\left[\left(\left\| \mathcal{P}_{\mathcal{Y}}\left(W\left(\mathbf{y}_{k}+\gamma_{y}U_{k+1}^{-\beta} \nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right) \right)-\mathcal{P}_{\mathcal{Y}}\left(W\mathbf{y}_{k}\right)\right\|\right)^{2 }\right]\\ &\leqslant\frac{1+\rho_{W}}{2}\mathbb{E}\left[\left\|\mathbf{y} _{k}-\mathbf{J}\mathbf{y}_{k}\right\|^{2}\right]+\frac{1+\rho_{W}}{1-\rho_{W}} \mathbb{E}\left[\left\|\gamma_{y}U_{k+1}^{-\beta}\nabla_{y}F\left(\mathbf{x}_{k },\mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}\right]\\ &\leqslant\frac{1+\rho_{W}}{2}\mathbb{E}\left[\left\|\mathbf{y} _{k}-\mathbf{J}\mathbf{y}_{k}\right\|^{2}\right]+\frac{4\gamma_{y}^{2}\left(1+ \zeta_{u}^{2}\right)}{(1-\rho_{W})}\mathbb{E}\left[\bar{u}_{k+1}^{-2\beta} \left\|\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right\| ^{2}\right],\end{split}\]where we have used the non-expansiveness of projection operator. Then, we have

\[\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\mathbf{y}_{k}-\mathbf{1} \bar{y}_{k}\right\|^{2}\right]\] \[\leqslant\frac{2}{1-\rho_{W}}\mathbb{E}\left[\left\|\mathbf{y}_{0 }-\mathbf{J}\mathbf{y}_{0}\right\|^{2}\right]+\frac{8\gamma_{y}^{2}\left(1+ \zeta_{u}^{2}\right)}{\left(1-\rho_{W}\right)^{2}}\sum_{k=0}^{K-1}\mathbb{E} \left[\bar{u}_{k+1}^{-2\beta}\left\|\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_ {k};\xi_{k}^{y}\right)\right\|^{2}\right]\]

Similar to the primal variable, we can bound the last term above, which completes the proof. 

Next, we need to bound the term \(S_{3}\) i.e., the optimality gap in dual variable. The intuition of the proof relies on the adaptive two time-scale protocol, that is, for given \(\alpha\) and \(\beta\), we try to find the threshold of the iterations \(k_{0}\), after which the inner sub-problem can be well solved (faster) to ensure that the computation of outer sub-problem can be solved accurately (slower). In specific, we suppose that there is a constant \(G\) such that \(\bar{u}_{k}\leqslant G\) hold for \(k=0,1,\cdots,k_{0}-1\), then the analysis is divided into two phases.

**Lemma 6** (First phase).: _Suppose Assumption 1-4 hold. If \(\bar{u}_{k}\leqslant G,k=0,1,\cdots,k_{0}-1\), then we have_

\[\sum_{k=0}^{k_{0}-1}\mathbb{E}\left[f\left(\bar{x}_{k},y^{*} \left(\bar{x}_{k}\right)\right)-f\left(\bar{x}_{k},\bar{y}_{k}\right)\right]\] (33) \[\leqslant\sum_{k=0}^{k_{0}-1}\mathbb{E}\left[E_{1,k}\right]+ \frac{\gamma_{x}^{2}\kappa^{2}\left(1+\zeta_{v}^{2}\right)G^{2\beta}}{n\mu \gamma_{y}^{2}}\sum_{k=0}^{k_{0}-1}\mathbb{E}\left[\bar{v}_{k+1}^{-2\alpha} \left\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right) \right\|^{2}\right]\] \[+\frac{\gamma_{y}\left(1+\zeta_{u}^{2}\right)}{n}\sum_{k=0}^{k_{ 0}-1}\mathbb{E}\left[\bar{u}_{k+1}^{-\beta}\left\|\nabla_{y}F\left(\mathbf{x} _{k},\mathbf{y}_{k};\xi_{k}\right)\right\|^{2}\right]+\frac{4\kappa L}{n}\sum _{k=0}^{k_{0}-1}\mathbb{E}\left[\left\|\mathbf{x}_{k}-\mathbf{1}\bar{x}_{k} \right\|^{2}\right]\] \[+\frac{4}{\mu}\sum_{k=0}^{k_{0}-1}\mathbb{E}\left[\left\|\frac{ \bar{\mathbf{u}}_{k+1}^{-\beta}}{n\bar{u}_{k+1}^{-\beta}}\nabla_{y}F\left( \mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}\right]+C\sum_{k=0 }^{k_{0}-1}\mathbb{E}\left[\sqrt{\frac{1}{n}\left\|\mathbf{y}_{k}-\mathbf{1} \bar{y}_{k}\right\|^{2}}\right]\!,\]

_where_

\[E_{1,k}:=\frac{1-3\mu\gamma_{y}\bar{u}_{k+1}^{-\beta}/4}{2\gamma_{y}\bar{u}_{ k+1}^{-\beta}n}\left\|\mathbf{y}_{k}-\mathbf{1}y^{*}\left(\bar{x}_{k}\right) \right\|^{2}-\frac{\left\|\mathbf{y}_{k+1}-\mathbf{1}y^{*}\left(\bar{x}_{k+1} \right)\right\|^{2}}{\left(2+\mu\gamma_{y}\bar{u}_{k+1}^{-\beta}\right)\gamma_ {y}\bar{u}_{k+1}^{-\beta}n}.\] (34)

Proof.: Using Young's inequality with parameter \(\lambda_{k}\), we get

\[\frac{1}{n}\left\|\mathbf{y}_{k+1}-\mathbf{1}\bar{y}^{*}\left( \bar{x}_{k+1}\right)\right\|^{2}\] (35) \[\leqslant\frac{\left(1+\lambda_{k}\right)}{n}\left\|\mathbf{y}_{k +1}-\mathbf{1}y^{*}\left(\bar{x}_{k}\right)\right\|^{2}+\left(1+\frac{1}{ \lambda_{k}}\right)\left\|y^{*}\left(\bar{x}_{k}\right)-y^{*}\left(\bar{x}_{k+1 }\right)\right\|^{2}.\]

Recalling that \(\mathbf{y}_{k+1}=\mathcal{P}_{\mathcal{Y}}\left(W\left(\mathbf{y}_{k}+\gamma_{ y}U_{k+1}^{-\beta}\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y} \right)\right)\right)\), we further define

\[\mathbf{\hat{y}}_{k+1}=W\left(\mathbf{y}_{k}+\gamma_{y}U_{k+1}^{-\beta}\nabla_{ y}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right).\]

Then, for the first term on the RHS of (35), by the non-expansiveness property of projection operator \(\mathcal{P}_{\mathcal{Y}}(\cdot)\) (c.f., Lemma 1 in (Nedic et al., 2010)), we have

\[\frac{1}{n}\left\|\mathbf{y}_{k+1}-\mathbf{1}y^{*}\left(\bar{x}_{ k}\right)\right\|^{2}\] (36) \[\leqslant\frac{1}{n}\left\|\mathbf{\hat{y}}_{k+1}-\mathbf{1}y^{*} \left(\bar{x}_{k}\right)\right\|^{2}-\frac{1}{n}\left\|\mathbf{y}_{k+1}- \mathbf{\hat{y}}_{k+1}\right\|^{2}\] \[-\frac{1}{n}\sum_{i=1}^{n}2\left\langle\gamma_{y}\bar{u}_{k+1}^{- \beta}g_{i,k}^{y},y_{i,k}-y^{*}\left(\bar{x}_{k}\right)\right\rangle-\frac{1}{n} \sum_{i=1}^{n}2\left\langle\gamma_{y}\left(u_{i,k+1}^{-\beta}-\bar{u}_{k+1}^{- \beta}\right)g_{i,k}^{y},y_{i,k}-y^{*}\left(\bar{x}_{k}\right)\right\rangle\!,\]where we have used Young's inequality and strong-concavity of \(f_{i}\), and

\[\begin{split}&\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[-2\left\langle \left(\frac{u_{i,k+1}^{-\beta}-\bar{u}_{k+1}^{-\beta}}{\bar{u}_{k+1}^{-\beta}} \right)g_{i,k}^{y},y_{i,k}-y^{*}\left(\bar{x}_{k}\right)\right\rangle\right]\\ &\leqslant\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\frac{8}{\mu} \left\|\left(\frac{u_{i,k+1}^{-\beta}-\bar{u}_{k+1}^{-\beta}}{\bar{u}_{k+1}^{- \beta}}\right)g_{i,k}^{y}\right\|^{2}+\frac{\mu}{8}\left\|y_{i,k}-y^{*}\left( \bar{x}_{k}\right)\right\|^{2}\right].\end{split}\] (39)

For the consensus error of dual variable on the objective function, using strong-concavity of \(f_{i}\) and Jensen's inequality, we have

\[\begin{split}&\frac{1}{n}\sum_{i=1}^{n}-2\left(f_{i}\left(\bar{x }_{k},\bar{y}_{k}\right)-f_{i}\left(\bar{x}_{k},y_{i,k}\right)\right)\\ &\leqslant\frac{1}{n}\sum_{i=1}^{n}2\left\langle\nabla_{y}f_{i} \left(\bar{x}_{k},\bar{y}_{k}\right),y_{i,k}-\bar{y}_{k}\right\rangle-\frac{ \mu}{n}\left\|\mathbf{y}_{k}-\mathbf{1}\bar{y}_{k}\right\|^{2}\\ &\leqslant 2C\frac{1}{n}\sum_{i=1}^{n}\left\|y_{i,k}-\bar{y}_{k} \right\|\leqslant 2C\sqrt{\frac{1}{n}\left\|\mathbf{y}_{k}-\mathbf{1}\bar{y}_{k} \right\|^{2}}.\end{split}\] (40)Letting \(\lambda_{k}=\mu\gamma_{y}\bar{u}_{k+1}^{-\beta}/2\), we get

\[\begin{split}&\mathbb{E}\left[f\left(\bar{x}_{k},\bar{y}^{\ast} \left(\bar{x}_{k}\right)\right)-f\left(\bar{x}_{k},\bar{y}_{k}\right)\right]\\ &\leqslant\mathbb{E}\left[\frac{1-3\mu\gamma_{y}\bar{u}_{k+1}^{- \beta}/4}{2\gamma_{y}\bar{u}_{k+1}^{-\beta}n}\left\|\mathbf{y}_{k}-\mathbf{1}y ^{\ast}\left(\bar{x}_{k}\right)\right\|^{2}-\frac{\left\|\mathbf{y}_{k+1}- \mathbf{1}y^{\ast}\left(\bar{x}_{k+1}\right)\right\|^{2}}{\left(2+\mu\gamma_{y }\bar{u}_{k+1}^{-\beta}\right)\gamma_{y}\bar{u}_{k+1}^{-\beta}n}\right]\\ &+\frac{\gamma_{x}^{2}\kappa^{2}\left(1+\zeta_{v}^{2}\right)G^{2 \beta}}{n\mu\gamma_{y}^{2}}\mathbb{E}\left[\bar{v}_{k+1}^{-2\alpha}\left\| \nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2 }\right]\\ &+\frac{\gamma_{y}\left(1+\zeta_{u}^{2}\right)}{n}\sum_{i=1}^{n} \mathbb{E}\left[\bar{u}_{k+1}^{-\beta}\left\|\nabla_{y}F\left(\mathbf{x}_{k}, \mathbf{y}_{k};\xi_{k}\right)\right\|^{2}\right]+\frac{4\kappa L}{n}\mathbb{E }\left[\left\|\mathbf{x}_{k}-\mathbf{1}\bar{y}_{k}\right\|^{2}\right]\\ &+\frac{4}{\mu}\mathbb{E}\left[\left\|\frac{\boldsymbol{\bar{u}} _{k+1}^{-\beta}}{n\bar{u}_{k+1}^{-\beta}}\nabla_{y}F\left(\mathbf{x}_{k}, \mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}\right]+C\mathbb{E}\left[\sqrt{ \frac{1}{n}\left\|\mathbf{y}_{k}-\mathbf{1}\bar{y}_{k}\right\|^{2}}\right]. \end{split}\] (41)

By the \(\kappa\)-smoothness of \(y^{\ast}\), we have

\[\begin{split}&\left\|y^{\ast}\left(\bar{x}_{k+1}\right)-y^{\ast} \left(\bar{x}_{k}\right)\right\|^{2}\\ &\leqslant\kappa^{2}\left\|\bar{x}_{k+1}-\bar{x}_{k}\right\|^{2} \\ &=\kappa^{2}\left\|\gamma_{x}\bar{v}_{k+1}^{-\alpha}\frac{\mathbf{ 1}^{T}}{n}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}\right)- \gamma_{x}\frac{\left(\boldsymbol{\bar{v}}_{k+1}^{-\alpha}\right)^{T}}{n} \nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2} \\ &\leqslant\frac{2\gamma_{x}^{2}\kappa^{2}\left(1+\zeta_{v}^{2} \right)\bar{v}_{k+1}^{-2\alpha}}{n}\left\|\nabla_{x}F\left(\mathbf{x}_{k}, \mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}.\end{split}\] (42)

Telescoping the obtained terms from \(0\) to \(k_{0}-1\) and noticing that \(\bar{u}_{k}\leqslant G\) for \(k\leqslant k_{0}-1\) we complete the proof. 

For the second phase, i.e., \(k\geqslant k_{0}\), we have the following lemma.

**Lemma 7** (Second phase).: _Suppose Assumption 1-4 hold. If \(\bar{u}_{k}\leqslant G,k=0,1,\cdots,k_{0}-1\), then we have_

\[\begin{split}&\sum_{k=k_{0}}^{K-1}\mathbb{E}\left[f\left(\bar{x }_{k},\bar{y}^{\ast}\left(\bar{x}_{k}\right)\right)-f\left(\bar{x}_{k},\bar{y} _{k}\right)\right]\\ &\leqslant\sum_{k=k_{0}}^{K-1}\mathbb{E}\left[E_{1,k}\right]+ \frac{8\gamma_{x}^{2}\kappa^{2}\left(1+\zeta_{v}^{2}\right)}{\mu\gamma_{y}^{2} G^{2\alpha-2\beta}}\sum_{k=k_{0}}^{K-1}\left\|\nabla_{x}f\left(\bar{x}_{k}, \bar{y}_{k}\right)\right\|^{2}\\ &+\left(\frac{8\gamma_{x}^{2}\kappa^{2}L^{2}\left(1+\zeta_{v}^{2 }\right)}{n\mu\gamma_{y}^{2}G^{2\alpha-2\beta}}+\frac{4\kappa L}{n}\right) \sum_{k=k_{0}}^{K-1}\mathbb{E}\left[\Delta_{k}\right]\\ &+\frac{\gamma_{y}\left(1+\zeta_{u}^{2}\right)}{n}\mathbb{E} \left[\bar{u}_{k+1}^{-\beta}\left\|\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_ {k};\xi_{k}\right)\right\|^{2}\right]+C\sum_{k=k_{0}}^{K-1}\mathbb{E}\left[ \sqrt{\frac{1}{n}\left\|\mathbf{y}_{k}-\mathbf{1}\bar{y}_{k}\right\|^{2}} \right]\\ &+\frac{\gamma_{x}^{2}\left(1+\zeta_{v}^{2}\right)}{\gamma_{y} \bar{v}_{1}^{\alpha-\beta}}\left(\kappa^{2}+\frac{2\gamma_{x}^{2}\left(1+\zeta_{ v}^{2}\right)C^{2}\hat{L}^{2}}{\mu\gamma_{y}\bar{v}_{1}^{2\alpha-\beta}}\right) \sum_{k=k_{0}}^{K-1}\mathbb{E}\left[\frac{\bar{v}_{k+1}^{-\alpha}}{n}\left\| \nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2 }\right]\\ &+\frac{4\gamma_{x}\kappa\left(1+\zeta_{v}\right)C^{2}}{\mu\gamma_{ y}\bar{v}_{1}^{\alpha}}\mathbb{E}\left[\bar{u}_{k}^{\beta}\right]+\frac{4}{\mu} \sum_{k=k_{0}}^{K-1}\mathbb{E}\left[\left\|\frac{\boldsymbol{\bar{u}}_{k+1}^{- \beta}}{n\bar{u}_{k+1}^{-\beta}}\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_{k}; \xi_{k}^{x}\right)\right\|^{2}\right]\!.\end{split}\] (43)Proof.: Firstly, by the non-expansiveness of projection operator, we have

\[\begin{split}&\|y_{i,k+1}-y^{*}\left(\bar{x}_{k+1}\right)\|^{2}\\ &\leqslant\|\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k+1}\right)\|^{2} -\|y_{i,k+1}-\hat{y}_{i,k+1}\|^{2}\\ &=\|\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right)\|^{2}+\|y^{*} \left(\bar{x}_{k+1}\right)-y^{*}\left(\bar{x}_{k}\right)\|^{2}\\ &-2\left\langle\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right),y^{* }\left(\bar{x}_{k+1}\right)-y^{*}\left(\bar{x}_{k}\right)\right\rangle\\ &=\|\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right)\|^{2}+\|y^{*} \left(\bar{x}_{k+1}\right)-y^{*}\left(\bar{x}_{k}\right)\|^{2}\\ &-2\left(\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right)\right)^{T }\nabla y^{*}\left(\bar{x}_{k}\right)\left(\bar{x}_{k+1}-\bar{x}_{k}\right)^{ T}\\ &-2\left(\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right)\right)^{T }\left(y^{*}\left(\bar{x}_{k+1}\right)-y^{*}\left(\bar{x}_{k}\right)-\nabla y^ {*}\left(\bar{x}_{k}\right)\left(\bar{x}_{k+1}-\bar{x}_{k}\right)^{T}\right). \end{split}\] (44)

Then, for the first inner-product term on the RHS, letting \(\nabla_{x}\tilde{F}_{k}=\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k }\right)-\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k}\right)\), we get

\[\begin{split}&-2\left(\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k} \right)\right)^{T}\nabla y^{*}\left(\bar{x}_{k}\right)\left(\bar{x}_{k+1}-\bar {x}_{k}\right)^{T}\\ &=2\gamma_{x}\left(\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right) \right)^{T}\nabla y^{*}\left(\bar{x}_{k}\right)\left(\nabla_{x}F\left(\mathbf{ x}_{k},\mathbf{y}_{k}\right)\right)^{T}\left(\frac{\mathbf{1}\bar{v}_{k+1}^{-\alpha}}{n}+ \frac{\mathbf{\tilde{v}}_{k+1}^{-\alpha}}{n}\right)\\ &+2\gamma_{x}\left(\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right) \right)^{T}\nabla y^{*}\left(\bar{x}_{k}\right)\left(\nabla_{x}\tilde{F}_{k} \right)^{T}\left(\frac{\mathbf{1}\bar{v}_{k+1}^{-\alpha}}{n}+\frac{\mathbf{ \tilde{v}}_{k+1}^{-\alpha}}{n}\right)\\ &\leqslant 2\gamma_{x}\kappa\left\|\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k} \right)\right\|\left(\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k}\right) \right)^{T}\left(\frac{\mathbf{1}\bar{v}_{k+1}^{-\alpha}}{n}+\frac{\mathbf{ \tilde{v}}_{k+1}^{-\alpha}}{n}\right)\right\|\\ &+2\gamma_{x}\left(\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right) \right)^{T}\nabla y^{*}\left(\bar{x}_{k}\right)\left(\nabla_{x}\tilde{F}_{k} \right)^{T}\left(\frac{\mathbf{1}\bar{v}_{k+1}^{-\alpha}}{n}+\frac{\mathbf{ \tilde{v}}_{k+1}^{-\alpha}}{n}\right).\end{split}\] (45)

wherein the last inequality we have used the fact that \(y^{*}\) is \(\kappa\)-Lipschitz. Then, using Young's inequality with parameter \(\lambda_{k}\), we get

\[\begin{split}&-2\left(\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right) \right)^{T}\nabla y^{*}\left(\bar{x}_{k}\right)\left(\bar{x}_{k+1}-\bar{x}_{k} \right)^{T}\\ &\leqslant\lambda_{k}\left\|\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k} \right)\right\|^{2}\\ &+\frac{2\gamma_{x}^{2}\bar{v}_{k+1}^{-2\alpha}\kappa^{2}}{\lambda _{k}}\left(\left\|\frac{\mathbf{1}^{T}}{n}\nabla_{x}F\left(\mathbf{x}_{k}, \mathbf{y}_{k}\right)\right\|^{2}+\left\|\frac{\left(\mathbf{\tilde{v}}_{k+1}^ {-\alpha}\right)^{T}}{n\bar{v}_{k+1}^{-\alpha}}\nabla_{x}F\left(\mathbf{x}_{k}, \mathbf{y}_{k}\right)\right\|^{2}\right)\\ &+2\gamma_{x}\left(\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right) \right)^{T}\nabla y^{*}\left(\bar{x}_{k}\right)\left(\nabla_{x}\tilde{F}_{k} \right)^{T}\left(\frac{\mathbf{1}\bar{v}_{k+1}^{-\alpha}}{n}+\frac{\mathbf{ \tilde{v}}_{k+1}^{-\alpha}}{n}\right).\end{split}\] (46)

For the second inner-product term on the RHS, noticing that \(y^{*}\) is \(\hat{L}=\kappa\left(1+\kappa\right)^{2}\) smooth given in Lemma 2, we have

\[\begin{split}&\leq 2\left\|\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k} \right)\right\|\left\|y^{*}\left(\bar{x}_{k}\right)-y^{*}\left(\bar{x}_{k+1} \right)+\nabla y^{*}\left(\bar{x}_{k}\right)\left(\bar{x}_{k+1}-\bar{x}_{k} \right)^{T}\right\|\\ &\leqslant 2\left\|\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k} \right)\right\|\frac{\hat{L}}{2}\left\|\bar{x}_{k+1}-\bar{x}_{k}\right\|^{2}\\ &\leqslant\gamma_{x}^{2}\hat{L}\left\|\hat{y}_{i,k+1}-y^{*}\left( \bar{x}_{k}\right)\right\|\left\|\left(\frac{\bar{v}_{k+1}^{-\alpha}\mathbf{1}^{T }}{n}+\frac{\left(\mathbf{\tilde{v}}_{k+1}^{-\alpha}\right)^{T}}{n}\right) \nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2} \\ &\leqslant\gamma_{x}^{2}\hat{L}\left\|\hat{y}_{i,k+1}-y^{*}\left( \bar{x}_{k}\right)\right\|\left\|\frac{\bar{v}_{k+1}^{-2\alpha}\left(1+\zeta_{v} ^{2}\right)C}{n}\left\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x} \right)\right\|\\ &\leqslant\tau\gamma_{x}^{2}\bar{v}_{k+1}^{-2\alpha}\left(1+ \zeta_{v}^{2}\right)C^{2}\hat{L}\left\|\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k} \right)\right\|^{2}+\frac{\gamma_{x}^{2}\bar{v}_{k+1}^{-2\alpha}\left(1+ \zeta_{v}^{2}\right)\hat{L}}{\tau n}\left\|\nabla_{x}F\left(\mathbf{x}_{k}, \mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2},\end{split}\] (47)wherein the last inequality we have used Young's inequality with parameter \(\tau\). Plugging the obtained inequalities into (44), we get

\[\begin{split}&\|y_{i,k+1}-y^{*}\left(\bar{x}_{k+1}\right)\|^{2}\\ &\leqslant\left(1+\lambda_{k}+\tau\gamma_{x}^{2}\bar{v}_{k+1}^{-2 \alpha}\left(1+\zeta_{v}^{2}\right)C^{2}\hat{L}\right)\|\hat{y}_{i,k+1}-y^{*} \left(\bar{x}_{k}\right)\|^{2}\\ &+\frac{\gamma_{x}^{2}\bar{v}_{k+1}^{-2\alpha}}{n}\left(2\kappa^ {2}+\frac{\hat{L}}{\tau}\right)\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{ k};\xi_{k}\right)\|^{2}\\ &+\frac{2\gamma_{x}^{2}\bar{v}_{k+1}^{-2\alpha}\kappa^{2}}{ \lambda_{k}}\left(\left\|\frac{\mathbf{1}^{T}}{n}\nabla_{x}F\left(\mathbf{x}_ {k},\mathbf{y}_{k}\right)\right\|^{2}+\left\|\frac{\left(\mathbf{\tilde{v}}_{k +1}^{-\alpha}\right)^{T}}{n\bar{v}_{k+1}^{-\alpha}}\nabla_{x}F\left(\mathbf{x }_{k},\mathbf{y}_{k}\right)\right\|^{2}\right)\\ &+2\gamma_{x}\left(\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right) \right)^{T}\nabla y^{*}\left(\bar{x}_{k}\right)\left(\nabla_{x}\hat{F}\right)^ {T}\left(\frac{\mathbf{1}\bar{v}_{k+1}^{-\alpha}}{n}+\frac{\mathbf{\tilde{v}} _{k+1}^{-\alpha}}{n}\right).\end{split}\] (48)

Setting the parameters for Young's inequalities we used as follows,

\[\lambda_{k}=\frac{\mu\gamma_{y}\bar{v}_{k+1}^{-\beta}}{4},\quad\tau=\frac{\mu \gamma_{y}v_{0}^{2\alpha-\beta}}{4\gamma_{x}^{2}\left(1+\zeta_{v}^{2}\right)C ^{2}\hat{L}},\] (49)

then we get

\[\begin{split}&\|y_{i,k+1}-y^{*}\left(\bar{x}_{k+1}\right)\|^{2} \\ &\leqslant\left(1+\frac{\mu\gamma_{y}\bar{u}_{k+1}^{-\beta}}{2} \right)\|\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right)\|^{2}\\ &+\frac{\gamma_{x}^{2}\left(1+\zeta_{v}^{2}\right)}{n}\left(2 \kappa^{2}+\frac{4\gamma_{x}^{2}\left(1+\zeta_{v}^{2}\right)C^{2}\hat{L}^{2}} {\mu\gamma_{y}\bar{v}_{0}^{2\alpha-\beta}}\right)\bar{v}_{k+1}^{-2\alpha}\left\| \nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}\right)\right\|^{2}\\ &+\frac{8\gamma_{x}^{2}\bar{v}_{k+1}^{-2\alpha}\kappa^{2}}{\mu \gamma_{y}\bar{u}_{k+1}^{-\beta}}\left(\left\|\frac{\mathbf{1}^{T}}{n}\nabla_ {x}F\left(\mathbf{x}_{k},\mathbf{y}_{k}\right)\right\|^{2}+\left\|\frac{ \left(\mathbf{\tilde{v}}_{k+1}^{-\alpha}\right)^{T}}{n\bar{v}_{k+1}^{-\alpha }}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k}\right)\right\|^{2}\right)\\ &+2\gamma_{x}\left(\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right) \right)^{T}\nabla y^{*}\left(\bar{x}_{k}\right)\left(\nabla_{x}\tilde{F}_{k} \right)^{T}\left(\frac{\mathbf{1}\bar{v}_{k+1}^{-\alpha}}{n}+\frac{\mathbf{ \tilde{v}}_{k+1}^{-\alpha}}{n}\right).\end{split}\] (50)

Recalling that

\[\begin{split}&\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\frac{1}{ \gamma_{y}\bar{u}_{k+1}^{-\beta}}\left\|\hat{y}_{i,k+1}-\bar{y}^{*}\left(\bar{x }_{k}\right)\right\|^{2}\right]\\ &\leqslant\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[\frac{1-3\mu \gamma_{y}\bar{u}_{k+1}^{-\beta}/4}{\gamma_{y}\bar{u}_{k+1}^{-\beta}}\left\|y _{i,k}-\bar{y}^{*}\left(\bar{x}_{k}\right)\right\|^{2}\right]+\frac{8\kappa L} {n}\mathbb{E}\left[\left\|\mathbf{x}_{k}-\mathbf{1}\bar{y}_{k}\right\|^{2}\right] \\ &+\frac{2\gamma_{y}\left(1+\zeta_{u}^{2}\right)}{n}\mathbb{E} \left[\bar{u}_{k+1}^{-\beta}\left\|\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y} _{k};\xi_{k}\right)\right\|^{2}\right]-\mathbb{E}\left[2\left(f\left(\bar{x}_ {k},\bar{y}^{*}\left(\bar{x}_{k}\right)\right)-f\left(\bar{x}_{k},\bar{y}_{k} \right)\right)\right]\\ &+\frac{8}{\mu}\mathbb{E}\left[\left\|\frac{\mathbf{\tilde{u}}_{k +1}^{-\beta}}{n\bar{u}_{k+1}^{-\beta}}\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{ y}_{k};\xi_{k}^{\prime}\right)\right\|^{2}\right]+2C\mathbb{E}\left[\sqrt{ \frac{1}{n}\left\|\mathbf{y}_{k}-\mathbf{1}\bar{y}_{k}\right\|^{2}}\right], \end{split}\]

[MISSING_PAGE_EMPTY:27]

Then, for the term \(\mathbb{E}\left[E_{3,k}\right]\), noticing that \(\bar{u}_{k+1}\leqslant\bar{v}_{k+1}\) and \(\bar{v}_{k+1}\geqslant\bar{v}_{1}\), we have

\[\sum_{k=k_{0}}^{K-1}\mathbb{E}\left[E_{3,k}\right]\] (54) \[\leqslant\sum_{k=k_{0}}^{K-1}\mathbb{E}\left[\frac{\gamma_{x}^{2} \left(1+\zeta_{v}^{2}\right)}{n\gamma_{y}}\left(\kappa^{2}+\frac{2\gamma_{x}^{ 2}\left(1+\zeta_{v}^{2}\right)C^{2}\hat{L}^{2}}{\mu\gamma_{y}\bar{v}_{1}^{2 \alpha-\beta}}\right)\frac{\bar{v}_{k+1}^{-2\alpha}}{\bar{u}_{k+1}^{-\beta}} \left\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\| ^{2}\right]\] \[\leqslant\frac{\gamma_{x}^{2}\left(1+\zeta_{v}^{2}\right)}{ \gamma_{y}\bar{v}_{1}^{\alpha-\beta}}\left(\kappa^{2}+\frac{2\gamma_{x}^{2} \left(1+\zeta_{v}^{2}\right)C^{2}\hat{L}^{2}}{\mu\gamma_{y}\bar{v}_{1}^{2 \alpha-\beta}}\right)\sum_{k=k_{0}}^{K-1}\mathbb{E}\left[\frac{\bar{v}_{k+1}^ {-\alpha}}{n}\left\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x }\right)\right\|^{2}\right].\]

For the term \(E_{4,k}\), we denote

\[e_{k}:=\frac{\gamma_{x}}{\gamma_{y}\bar{u}_{k+1}^{-\beta}}\left(\frac{1}{n} \sum_{i=1}^{n}\left(\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right)\right)^{T} \right)\nabla y^{*}\left(\bar{x}_{k}\right)\left(\nabla_{x}\tilde{F}_{k} \right)^{T}\left(\frac{1}{n}+\frac{\tilde{\bm{v}}_{k+1}^{-\alpha}}{n\bar{v}_{ k+1}^{-\alpha}}\right),\]

then we have

\[\left|e_{k}\right| \leqslant\frac{\gamma_{x}\kappa}{\gamma_{y}\bar{u}_{k+1}^{-\beta }}\frac{1}{n}\sum_{i=1}^{n}\left\|\hat{y}_{i,k+1}-y^{*}\left(\bar{x}_{k}\right) \right\|\left\|\left(\nabla_{x}\tilde{F}_{k}\right)^{T}\left(\frac{1}{n}+ \frac{\tilde{\bm{v}}_{k+1}^{-\alpha}}{n\bar{v}_{k+1}^{-\alpha}}\right)\right\|\] (55) \[\leqslant\frac{\gamma_{x}\kappa\left(1+\zeta_{v}\right)}{\gamma_{ y}\sqrt{n}\bar{u}_{k+1}^{-\beta}}\left(\frac{1}{n}\sum_{i=1}^{n}\frac{1}{\mu} \left\|\nabla_{y}f\left(\bar{x}_{k},\hat{y}_{i,k+1}\right)-\nabla_{y}f\left( \bar{x}_{k},y^{*}\right)\right\|\right)\left\|\nabla_{x}\tilde{F}\right\|\] \[\leqslant\underbrace{\frac{2\gamma_{x}\kappa\left(1+\zeta_{v} \right)C^{2}\bar{u}_{K}^{\beta}}{\mu\gamma_{y}}}_{M},\]

where we have used the Lipschitz continuity of \(y^{*}\) given in Lemma 2 and Assumption 3. Then, noticing that \(\mathbb{E}\left[\nabla_{x}\tilde{F}_{k}\right]=0\), we obtain

\[\sum_{k=k_{0}}^{K-1}\mathbb{E}\left[E_{4,k}\right] =\sum_{k=k_{0}}^{K-1}\mathbb{E}\left[e_{k}\bar{v}_{k+1}^{-\alpha}\right]\] (56) \[=\mathbb{E}\left[e_{k_{0}}\bar{v}_{k_{0}+1}^{-\alpha}\right]+ \underbrace{\sum_{k=k_{0}+1}^{K-1}\mathbb{E}\left[e_{k}\bar{v}_{k}^{-\alpha} \right]}_{0}+\sum_{k=k_{0}+1}^{K-1}\mathbb{E}\left[-e_{k}\underbrace{\left( \bar{v}_{k}^{-\alpha}-\bar{v}_{k+1}^{-\alpha}\right)}_{>0}\right]\] \[\leqslant\mathbb{E}\left[M\bar{v}_{k_{0}+1}^{-\alpha}\right]+ \sum_{k=k_{0}+1}^{K-1}\mathbb{E}\left[M\left(\bar{v}_{k}^{-\alpha}-\bar{v}_{k+1 }^{-\alpha}\right)\right]\] \[\leqslant 2\mathbb{E}\left[M\bar{v}_{k_{0}+1}^{-\alpha}\right] \leqslant\frac{4\gamma_{x}\kappa\left(1+\zeta_{v}\right)C^{2}}{\mu\gamma_{y} \bar{v}_{1}^{\alpha}}\mathbb{E}\left[\bar{u}_{K}^{\beta}\right].\]

Therefore, combining the obtained inequalities, we complete the proof. 

Now, it remains to bound the term \(E_{1,k}\).

**Lemma 8**.: _Suppose Assumption 1-4 hold. Then, we have_

\[\sum_{k=0}^{K-1}\mathbb{E}\left[E_{1,k}\right]\leqslant\frac{1}{2\gamma_{y} \bar{u}_{1}^{-\beta}n}\left\|\mathbf{y}_{0}-\mathbf{1}y^{*}\left(\bar{x}_{0} \right)\right\|^{2}+\frac{2\left(4\beta C^{2}\right)^{2+\frac{1}{1-\beta}}}{\mu^ {3+\frac{1}{1-\beta}}\gamma_{y}^{2+\frac{1}{1-\beta}}\bar{u}_{1}^{2-2\beta}}.\] (57)Proof.: Recalling the definition of \(E_{1,k}\) as given in (34), we have

\[\begin{split}&\sum_{k=0}^{K-1}\mathbb{E}\left[\frac{1-3\mu\gamma_{y} \bar{u}_{k+1}^{-\beta}/4}{2\gamma_{y}\bar{u}_{k+1}^{-\beta}n}\left\|\mathbf{y}_ {k}-\mathbf{1}y^{*}\left(\bar{x}_{k}\right)\right\|^{2}-\frac{\left\|\mathbf{y }_{k+1}-\mathbf{1}y^{*}\left(\bar{x}_{k+1}\right)\right\|^{2}}{\left(2+\mu \gamma_{y}\bar{u}_{k+1}^{-\beta}\right)\gamma_{y}\bar{u}_{k+1}^{-\beta}n}\right] \\ &\leqslant\frac{1-3\mu\gamma_{y}\bar{u}_{1}^{-\beta}/4}{2\gamma_{ y}\bar{u}_{1}^{-\beta}n}\left\|\mathbf{y}_{0}-\mathbf{1}y^{*}\left(\bar{x}_{0} \right)\right\|^{2}\\ &+\sum_{k=1}^{K-1}\mathbb{E}\left[\left(\frac{1-3\mu\gamma_{y} \bar{u}_{k+1}^{-\beta}/4}{2\gamma_{y}\bar{u}_{k+1}^{-\beta}n}-\frac{1}{2n \gamma_{y}\bar{u}_{k}^{-\beta}\left(2+\mu\gamma_{y}\bar{u}_{k}^{-\beta}\right) }\right)\left\|\mathbf{y}_{k}-\mathbf{1}y^{*}\left(\bar{x}_{k}\right)\right\|^ {2}\right]\\ &\leqslant\frac{1-3\mu\gamma_{y}\bar{u}_{1}^{-\beta}/4}{2\gamma_ {y}\bar{u}_{1}^{-\beta}n}\left\|\mathbf{y}_{0}-\mathbf{1}y^{*}\left(\bar{x}_{0 }\right)\right\|^{2}\\ &+\sum_{k=1}^{K-1}\mathbb{E}\left[\left(\frac{1}{2\gamma_{y}\bar{ u}_{k+1}^{-\beta}}-\frac{1}{4\gamma_{y}\bar{u}_{k}^{-\beta}}-\frac{\mu}{8}+ \underbrace{\frac{\mu}{2\left(2+\mu\gamma_{y}\bar{u}_{k}^{-\beta}\right)}- \frac{\mu}{2}}_{<0}\right)\frac{1}{n}\left\|\mathbf{y}_{k}-\mathbf{1}y^{*} \left(\bar{x}_{k}\right)\right\|^{2}\right]\!.\end{split}\] (58)

Next, we show that the term \(\frac{1}{2\gamma_{y}\bar{u}_{k+1}^{-\beta}}-\frac{1}{2\gamma_{y}\bar{u}_{k}^{- \beta}}-\frac{\mu}{8}\) is positive for only a constant number of iterations. If the term is positive at iteration \(k\), then we have

\[\begin{split} 0&<\frac{\bar{u}_{k+1}^{\beta}}{2\gamma_{y} }-\frac{\bar{u}_{k}^{\beta}}{2\gamma_{y}}-\frac{\mu}{8}\\ &\leqslant\bar{u}_{k}^{\beta}\frac{\left(1+\left\|\nabla_{y}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}/n\bar{u}_{k} ^{\beta}\right)^{\beta}}{2\gamma_{y}}-\frac{\bar{u}_{k}^{\beta}}{2\gamma_{y}} -\frac{\mu}{8}\\ &\leqslant\bar{u}_{k}^{\beta}\frac{\left(1+\beta\left\|\nabla_{y} F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}/n\bar{u}_{k} \right)}{2\gamma_{y}}-\frac{\bar{u}_{k}^{\beta}}{2\gamma_{y}}-\frac{\mu}{8}\\ &=\frac{\beta\left\|\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_{k };\xi_{k}^{y}\right)\right\|^{2}}{2\gamma_{y}n\bar{u}_{k}^{1-\beta}}-\frac{\mu }{8},\end{split}\] (59)

wherein the last inequality we used Bernoulli's inequality. Then we have the following two conditions,

\[\begin{cases}\frac{1}{n}\left\|\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_{k };\xi_{k}\right)\right\|^{2}\geqslant\frac{\gamma_{y}\bar{u}_{k+1}^{1-\beta}}{4 \beta}\geqslant\frac{\gamma_{y}\bar{u}_{1}^{1-\beta}}{4\beta},\\ \frac{4\beta G^{2}}{\mu\gamma_{y}}\geqslant\frac{4\beta\left\|\nabla_{y}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}}{\mu \gamma_{y}n}\geqslant\bar{u}_{k+1}^{1-\beta},\end{cases}\] (60)

which implies that we have at most

\[\left(\frac{4\beta C^{2}}{\mu\gamma_{y}}\right)^{\frac{1}{1-\beta}}\frac{4 \beta}{\mu\gamma_{y}\bar{u}_{1}^{1-\beta}}\] (61)

constant number of iterations when the term is positive. Furthermore, when the term is positive, by the inequality (59), we have

\[\begin{split}&\left(\frac{1}{2\gamma_{y}\bar{u}_{k+1}^{-\beta}}- \frac{1}{2\gamma_{y}\bar{u}_{k}^{-\beta}}-\frac{\mu}{8}\right)\frac{1}{n} \left\|\mathbf{y}_{k}-\mathbf{1}y^{*}\left(\bar{x}_{k}\right)\right\|^{2}\\ &\leqslant\frac{\beta\left\|\nabla_{y}F\left(\mathbf{x}_{k}, \mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}}{2\gamma_{y}n\bar{u}_{1}^{1-\beta }}\frac{1}{n}\left\|\mathbf{y}_{k}-\mathbf{1}y^{*}\left(\bar{x}_{k}\right) \right\|^{2}\\ &\leqslant\frac{\beta C^{2}}{2\mu^{2}\gamma_{y}\bar{u}_{1}^{1- \beta}}\frac{1}{n}\sum_{i=1}^{n}\left\|\nabla_{y}f_{i}\left(\bar{x}_{k},y_{i,k }\right)-\nabla_{y}f_{i}\left(\bar{x}_{k},y^{*}\right)\right\|^{2}\\ &\leqslant\frac{2\beta C^{4}}{\mu^{2}\gamma_{y}\bar{u}_{1}^{1- \beta}},\end{split}\] (62)where we have used the concavity of \(f_{i}\) in \(y\) and Assumption 3. Then, we have

\[\begin{split}&\sum_{k=1}^{K-1}\mathbb{E}\left[\left(\frac{1}{2\gamma_{y} \bar{u}_{k+1}^{-\beta}}-\frac{1}{2\gamma_{y}\bar{u}_{k}^{-\beta}}-\frac{\mu}{8 }\right)\frac{1}{n}\left\|\mathbf{y}_{k}-\mathbf{1}y^{*}\left(\bar{x}_{k} \right)\right\|^{2}\right]\\ &\leqslant\frac{2\beta C^{4}}{\mu^{2}\gamma_{y}\bar{u}_{1}^{1- \beta}}\left(\frac{4\beta C^{2}}{\mu\gamma_{y}}\right)^{\frac{1}{1-\beta}}\frac {4\beta}{\mu\gamma_{y}\bar{u}_{1}^{1-\beta}}\\ &\leqslant\frac{2\left(4\beta C^{2}\right)^{2+\frac{1}{1-\beta}} }{\mu^{3+\frac{1}{1-\beta}}\gamma_{y}^{2+\frac{1}{1-\beta}}\bar{u}_{1}^{2-2 \beta}},\end{split}\] (63)

which completes the proof. 

Next, we show in the following lemma that the inconsistency terms, as described in (5), exhibit asymptotic convergence for the proposed D-AdaST algorithm.

**Lemma 9** (Convergence of inconsistency terms).: _Suppose Assumption 1-4 hold. For the proposed D-AdaST in Algorithm 1, we have_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\frac{\left( \mathbf{\tilde{u}}_{k+1}^{-\alpha}\right)^{T}}{n\bar{u}_{k+1}^{-\alpha}}\nabla _{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right] \leqslant\sqrt{\frac{1}{n^{1-\alpha}}\left(\frac{4\rho_{W}}{\left(1-\rho_{W} \right)^{2}}\right)^{\alpha}}\frac{\left(1+\zeta_{v}\right)\zeta_{v}C^{2- \alpha}}{\left(1-\alpha\right)K^{\alpha}},\] (64)

_and_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\frac{\left( \mathbf{\tilde{u}}_{k+1}^{-\beta}\right)^{T}}{n\bar{u}_{k+1}^{-\beta}}\nabla _{y}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}\right] \leqslant\sqrt{\frac{1}{n^{1-\beta}}\left(\frac{4\rho_{W}}{\left(1-\rho_{W} \right)^{2}}\right)^{\beta}}\frac{\left(1+\zeta_{u}\right)\zeta_{u}C^{2- \beta}}{\left(1-\beta\right)K^{\beta}}.\] (65)

Proof.: By the definition of \(v_{i,k}\) in (3), we have

\[\begin{split}&\mathbb{E}\left[\left\|\frac{\left(\mathbf{\tilde{ v}}_{k+1}^{-\alpha}\right)^{T}}{n\bar{v}_{k+1}^{-\alpha}}\nabla_{x}F\left( \mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right]\\ &\leqslant\mathbb{E}\left[\frac{1}{n^{2}}\sum_{i=1}^{n}\left( \bar{v}_{k+1}^{\alpha}-v_{i,k+1}^{\alpha}\right)^{2}\frac{\left\|g_{i,k}^{x} \right\|^{2}}{v_{i,k+1}^{2\alpha}}\right]\\ &\leqslant\mathbb{E}\left[\frac{1}{n^{2}}\sum_{i=1}^{n}\left( \bar{v}_{k+1}^{\alpha}-v_{i,k+1}^{\alpha}\right)^{2}\frac{\left\bar{v}_{k+1}^{ \alpha}\right\|g_{i,k}^{x}\right\|^{2}}{\bar{v}_{k+1}^{\alpha}}\right].\end{split}\] (66)Noticing that \(\frac{\left\|\bar{v}_{k+1}^{2}-v_{i,k+1}^{\alpha}\right\|}{v_{k,k+1}^{\alpha}} \leqslant\zeta_{v}\), we have

\[\mathbb{E}\left[\left\|\frac{\left(\bar{\bm{v}}_{k+1}^{-\alpha} \right)^{T}}{n\bar{v}_{k+1}^{-\alpha}}\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{ y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right]\] \[\leqslant\mathbb{E}\left[\frac{1}{n^{2}}\sum_{i=1}^{n}\left(\bar{v }_{k+1}^{\alpha}-v_{i,k+1}^{\alpha}\right)^{2}\left(\frac{\bar{v}_{k+1}^{ \alpha}-v_{i,k+1}^{\alpha}}{v_{i,k+1}^{2\alpha}}+\frac{1}{v_{i,k+1}^{\alpha}} \right)\frac{\left\|g_{i,k}^{x}\right\|^{2}}{\bar{v}_{k+1}^{\alpha}}\right]\] \[\leqslant\mathbb{E}\left[\frac{1}{n^{2}}\sum_{i=1}^{n}\frac{ \left(\bar{v}_{k+1}^{\alpha}-v_{i,k+1}^{\alpha}\right)^{2}}{v_{i,k+1}^{2 \alpha}}\left|\bar{v}_{k+1}^{\alpha}-v_{i,k+1}^{\alpha}\right|\frac{\left\|g_ {i,k}^{x}\right\|^{2}}{\bar{v}_{k+1}^{\alpha}}\right]\] (67) \[+\mathbb{E}\left[\frac{1}{n^{2}}\sum_{i=1}^{n}\frac{\left|\bar{v }_{k+1}^{\alpha}-v_{i,k+1}^{\alpha}\right|}{v_{i,k+1}^{\alpha}}\left|\bar{v}_ {k+1}^{\alpha}-v_{i,k+1}^{\alpha}\right|\frac{\left\|g_{i,k}^{x}\right\|^{2}} {\bar{v}_{k+1}^{\alpha}}\right]\] \[\leqslant\left(1+\zeta_{v}\right)\zeta_{v}\mathbb{E}\left[\frac{1 }{n}\sum_{i=1}^{n}\left|\bar{v}_{k+1}^{\alpha}-v_{i,k+1}^{\alpha}\right|\frac {1}{n}\sum_{i=1}^{n}\frac{\left\|g_{i,k}^{x}\right\|^{2}}{\bar{v}_{k+1}^{ \alpha}}\right].\]

By Lemma 4, we get

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\frac{\left( \bar{\bm{v}}_{k+1}^{-\alpha}\right)^{T}}{n\bar{v}_{k+1}^{-\alpha}}\nabla_{x}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right]\] (68) \[\leqslant\left(1+\zeta_{v}\right)\zeta_{v}\mathbb{E}\left[\frac{1 }{n}\sum_{i=1}^{n}\left|\bar{v}_{k+1}^{\alpha}-v_{i,k+1}^{\alpha}\right|\frac{ 1}{K}\sum_{k=0}^{K-1}\frac{\frac{1}{n}\sum_{i=1}^{n}\left\|g_{i,k}^{x}\right\| ^{2}}{\bar{v}_{k+1}^{\alpha}}\right]\] \[\leqslant\left(1+\zeta_{v}\right)\zeta_{v}\mathbb{E}\left[\frac{1 }{n}\sum_{i=1}^{n}\left|\bar{v}_{k+1}^{\alpha}-v_{i,k+1}^{\alpha}\right|\right] \frac{C^{2-2\alpha}}{\left(1-\alpha\right)K^{\alpha}}\] \[\leqslant\left(1+\zeta_{v}\right)\zeta_{v}\sqrt{\frac{1}{n} \mathbb{E}\left[\left\|\bm{v}_{k+1}-\bm{1}\bar{v}_{k+1}\right\|^{2\alpha} \right]}\frac{C^{2-2\alpha}}{\left(1-\alpha\right)K^{\alpha}}.\]

Next, for the term of inconsistency of the stepsize \(\left\|\bm{v}_{k}-\bm{1}\bar{v}_{k}\right\|^{2}\), we consider two cases due to the max operator we used. At iteration \(k\), for the case \(\mathbf{m}_{k}^{x}\geqslant\mathbf{m}_{k}^{y}\) with \(\left\|\mathbf{m}_{0}^{x}-\bm{1}\bar{m}_{0}^{x}\right\|^{2}=0\), we have

\[\mathbb{E}\left[\left\|\bm{v}_{k+1}-\bm{1}\bar{v}_{k+1}\right\|^{ 2}\right]=\mathbb{E}\left[\left\|\mathbf{m}_{k+1}^{x}-\bm{1}\bar{m}_{k+1}^{x} \right\|^{2}\right]\] (69) \[=\mathbb{E}\left[\left\|(W-\mathbf{J})\left(\mathbf{m}_{k}^{x}- \bm{1}\bar{m}_{k}^{x}\right)+\eta_{k}\left(W-\mathbf{J}\right)\bm{h}_{k}^{x} \right\|^{2}\right]\] \[\leqslant\frac{1+\rho_{W}}{2}\mathbb{E}\left[\left\|\mathbf{m}_ {k}^{x}-\bm{1}\bar{m}_{k}^{x}\right\|^{2}\right]+\frac{\left(1+\rho_{W}\right) \rho_{W}}{1-\rho_{W}}\mathbb{E}\left[\left\|\bm{h}_{k}^{x}\right\|^{2}\right]\] \[\leqslant\left(\frac{1+\rho_{W}}{2}\right)^{k}\mathbb{E}\left[ \left\|\mathbf{m}_{0}^{x}-\bm{1}\bar{m}_{0}^{x}\right\|^{2}\right]+\frac{nC^{2 }\left(1+\rho_{W}\right)\rho_{W}}{1-\rho_{W}}\sum_{t=0}^{k}\left(\frac{1+ \rho_{W}}{2}\right)^{k-t}\] \[\leqslant\frac{2nC^{2}\left(1+\rho_{W}\right)\rho_{W}}{\left(1- \rho_{W}\right)^{2}}.\]

For the case \(\mathbf{m}_{k}^{x}<\mathbf{m}_{k}^{y}\), with \(\left\|\mathbf{m}_{0}^{y}-\bm{1}\bar{m}_{0}^{y}\right\|^{2}=0\),

\[\mathbb{E}\left[\left\|\bm{v}_{k+1}-\bm{1}\bar{v}_{k+1}\right\|^{2}\right]= \mathbb{E}\left[\left\|\mathbf{m}_{k+1}^{y}-\bm{1}\bar{m}_{k+1}^{y}\right\|^{2 }\right]\leqslant\frac{2nC^{2}\left(1+\rho_{W}\right)\rho_{W}}{\left(1-\rho_{W }\right)^{2}},\] (70)Combining these two cases, and using Lemma 4 and the fact \(\|\bm{v}_{k}^{\alpha}-\mathds{1}\bar{v}_{k}^{\alpha}\|^{2}\leqslant\|\bm{v}_{k}- \bm{1}\bar{v}_{k}\|^{2\alpha}\) for \(\alpha\in(0,1)\), we obtain the result for primal decision variable. Following the same proof, we can also derive the result for dual decision variable. We thus complete the proof. 

We further give the following lemma to show that the inconsistency of stepsize remains uniformly bounded for the vanilla D-TiAda algorithm as given in (2).

**Lemma 10** (Inconsistency for D-TiAda).: _Suppose Assumption 1-4 hold. Then, for D-TiAda, we have_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\frac{\left( \tilde{\bm{v}}_{k+1}^{-\alpha}\right)^{T}}{n\bar{v}_{k+1}^{-\alpha}}\nabla_{x }F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right] \leqslant\zeta_{v}^{2}C^{2},\] (71) \[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\frac{\left( \tilde{\bm{u}}_{k+1}^{-\beta}\right)^{T}}{n\bar{u}_{k+1}^{-\beta}}\nabla_{y}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}\right] \leqslant\zeta_{u}^{2}C^{2}.\]

Proof.: By the definition of inconsistency of stepsizes in (8) and Assumption 3 on bounded gradient, we immediately get the result. 

### Proof of Theorem 1

Proof of Theorem 1.: Consider a complete graph with 3 nodes where the functions corresponding to the nodes are as follows:

\[f_{1}(x,y) =-\frac{1}{2}y^{2}+xy-\frac{1}{2}x^{2},\] \[f_{2}(x,y) =f_{3}(x,y) =-\frac{1}{2}y^{2}-(1+\frac{1}{a}+\frac{1}{b})xy-\frac{1}{2}x^{2},\]

where \(a=2^{\frac{-1}{2\alpha-1}}\) and \(b=2^{\frac{-1}{2\beta-1}}\).

Notice that the only stationary point of \(f(x,y)=(f_{1}(x,y)+f_{2}(x,y)+f_{3}(x,y))/3\) is \((0,0)\). We denote \(g_{i,k}^{x}=\nabla_{x}f_{i}(x_{k},y_{k})\) and \(g_{i,k}^{y}=\nabla_{y}f_{i}(x_{k},y_{k})\).

Now we consider points initialized in line

\[y=-\frac{1+a}{a+\frac{a}{b}}x,\] (72)

where we have

\[g_{1,0}^{x} =y_{0}-x_{0}=-\frac{2ab+a+b}{ab+a}x_{0}\] \[g_{2,0}^{x} =g_{3,0}^{x}=-\left(1+\frac{1}{b}+\frac{1}{a}\right)y_{0}-x_{0}= \frac{2ab+a+b}{a^{2}(b+1)}x_{0}\] \[g_{1,0}^{y} =x_{0}-y_{0}=\frac{2ab+a+b}{ab+a}x_{0}\] \[g_{2,0}^{y} =g_{2,0}^{y}=-\frac{2ab+a+b}{ab(b+1)}x_{0}.\]

Note that by our assumptions of the range of \(\alpha\) and \(\beta\), we have \(a<b\). Thus, we have

\[|g_{1,0}^{x}|=|g_{1,0}^{y}|\quad\text{and}\quad|g_{2,0}^{x}|>|g_{2,0}^{y}|,\]which means \(g_{2,0}^{x}\) would be chosen in the maximum operator in the denominator of TiAda stepsize for \(x\). Therefore, after one step, we have

\[x_{1} =x_{0}-\eta^{x}\underbrace{\left(\frac{g_{1,0}^{x}}{\left(\left|g_{ 1,0}^{x}\right|^{2}\right)^{\alpha}}+\frac{g_{2,0}^{x}}{\left(\left|g_{2,0}^{x} \right|^{2}\right)^{\alpha}}+\frac{g_{3,0}^{x}}{\left(\left|g_{3,0}^{x}\right| ^{2}\right)^{\alpha}}\right)}_{=0}\] \[y_{1} =y_{0}-\eta^{y}\underbrace{\left(\frac{g_{1,0}^{y}}{\left(\left|g_ {1,0}^{y}\right|^{2}\right)^{\beta}}+\frac{g_{2,0}^{y}}{\left(\left|g_{2,0}^{y} \right|^{2}\right)^{\beta}}+\frac{g_{3,0}^{y}}{\left(\left|g_{3,0}^{y}\right| ^{2}\right)^{\beta}}\right)}_{=0}.\]

Next, we will use induction to show that \(x\) and \(y\) will stay in \(x_{0}\) and \(y_{0}\) for any iteration. Assuming for all iterations \(k\) in \(1,\ldots,t\), \(x_{k}=x_{0}\) and \(y_{k}=y_{0}\), then we have in next step

\[x_{t+1}=x_{t}-\eta^{x}\left(\frac{g_{1,0}^{x}}{\left(t\cdot\left|g_{1,0}^{x} \right|^{2}\right)^{\alpha}}+\frac{g_{2,0}^{x}}{\left(t\cdot\left|g_{2,0}^{x} \right|^{2}\right)^{\alpha}}+\frac{g_{3,0}^{x}}{\left(t\cdot\left|g_{3,0}^{x} \right|^{2}\right)^{\alpha}}\right).\]

Note that \(g_{1,0}^{x}=-a\cdot g_{2,0}^{x}\). Then, we get

\[x_{t+1} =x_{t}-\eta^{x}\left(\frac{-p\cdot g_{2,0}^{x}}{t^{\alpha}\cdot a ^{2\alpha}\cdot|g_{2,0}^{x}|^{2\alpha}}+\frac{2g_{2,0}^{x}}{t^{\alpha}\cdot|g _{2,0}^{x}|^{2\alpha}}\right)\] \[=x_{t}-\frac{g_{2,0}^{x}}{t^{\alpha}\cdot|g_{2,0}^{x}|^{2\alpha}} \underbrace{\left(2-a^{1-2\alpha}\right)}_{=0\text{ (by definition of $a$)}}\] \[=x_{t}.\]

Similarly, we can show that \(y_{t+1}=y_{t}\). Therefore all iterates will stay at \((x_{0},y_{0})\) if initialized at line \(y=-\frac{ab+b}{ab+a}x\), which implies that the initial gradient norm can be arbitrarily large by picking \(x_{0}\) to be large. 

### Proof of Theorem 2 and Corollary 1

Proof of Theorem 2.: Combining the results obtained in Lemma 6, 7 and 8, we get

\[\sum_{k=0}^{K-1}\mathbb{E}\left[f\left(\bar{x}_{k},y^{*}\left( \bar{x}_{k}\right)\right)-f\left(\bar{x}_{k},\bar{y}_{k}\right)\right]\] \[=\sum_{k=0}^{k_{0}-1}\mathbb{E}\left[f\left(\bar{x}_{k},y^{*} \left(\bar{x}_{k}\right)\right)-f\left(\bar{x}_{k},\bar{y}_{k}\right)\right]+ \sum_{k=k_{0}}^{K-1}\mathbb{E}\left[f\left(\bar{x}_{k},y^{*}\left(\bar{x}_{k} \right)\right)-f\left(\bar{x}_{k},\bar{y}_{k}\right)\right]\] \[\leqslant\frac{1}{2\gamma_{y}\bar{u}_{1}^{-\beta}n}\mathbb{E} \left[\left\|\mathbf{y}_{0}-\mathbf{1}y^{*}\left(\bar{x}_{0}\right)\right\|^{2 }\right]+\frac{2\left(4\beta C^{2}\right)^{2+\frac{1}{1-\beta}}}{\mu^{3+\frac{1 }{1-\beta}}\gamma_{y}^{2+\frac{1}{1-\beta}}\bar{u}_{1}^{2-2\beta}}\] \[+\frac{2\gamma_{x}^{2}\kappa^{2}\left(1+\zeta_{v}^{2}\right)G^{2 \beta}}{n\mu\gamma_{y}^{2}}\sum_{k=0}^{k_{0}-1}\mathbb{E}\left[\bar{v}_{k+1}^ {-2\alpha}\left\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x} \right)\right\|^{2}\right]\] (73) \[+\frac{\gamma_{y}\left(1+\zeta_{u}^{2}\right)}{n}\sum_{k=0}^{K-1} \mathbb{E}\left[\bar{u}_{k+1}^{-\beta}\left\|\nabla_{y}F\left(\mathbf{x}_{k}, \mathbf{y}_{k};\xi_{k}\right)\right\|^{2}\right]+C\sum_{k=0}^{K-1}\mathbb{E} \left[\sqrt{\frac{1}{n}\left\|\mathbf{y}_{k}-\mathbf{1}\bar{y}_{k}\right\|^{2 }}\right]\] \[+\frac{4}{\mu}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\frac{\bar{ \mathbf{u}}_{k+1}^{-\beta}}{n\bar{u}_{k+1}^{-\beta}}\nabla_{y}F\left(\mathbf{x} _{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}\right]+\frac{8\gamma_{x}^{2} \kappa^{2}\left(1+\zeta_{v}^{2}\right)}{\mu\gamma_{y}^{2}G^{2\alpha-2\beta}} \sum_{k=k_{0}}^{K-1}\left\|\nabla_{x}f\left(\bar{x}_{k},\bar{y}_{k}\right) \right\|^{2}\] \[+\frac{\sqrt{2}\kappa^{2}\kappa^{2}L^{2}\left(1+\zeta_{v}^{2} \right)}{n\mu\gamma_{y}^{2}G^{2\alpha-2\beta}}+\frac{4\kappa L}{n}\right)\sum_{ k=0}^{K-1}\mathbb{E}\left[\Delta_{k}\right]+\frac{4\gamma_{x}\kappa\left(1+\zeta_{v} \right)C^{2}}{\mu\gamma_{y}\bar{v}_{1}^{\alpha}}\mathbb{E}\left[\bar{u}_{K}^{ \beta}\right]\] \[+\frac{\gamma_{x}^{2}\left(1+\zeta_{v}^{2}\right)}{\gamma_{y}\bar{ v}_{1}^{\alpha-\beta}}\left(\kappa^{2}+\frac{2\gamma_{x}^{2}\left(1+\zeta_{v}^{2} \right)C^{2}\hat{L}^{2}}{\mu\gamma_{y}\bar{v}_{1}^{2\alpha-\beta}}\right)\sum_{ k=k_{0}}^{K-1}\mathbb{E}\left[\frac{\bar{v}_{k+1}^{-\alpha}}{n}\left\|\nabla_{x}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right].\]Letting the separation point between the two phases discussed in Lemma 6 and 7 satisfy

\[G=\left(\frac{16\left(1+\zeta_{v}^{2}\right)\gamma_{\pi}^{2}\kappa^{4}}{\gamma_{y} ^{2}}\right)^{\frac{1}{2\alpha-2\beta}},\] (74)

then, plugging above inequality into (18), with the help of Lemma 4-8 and Lemma 9, we get

\[\begin{split}&\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\| \nabla\Phi\left(\bar{x}_{k}\right)\right\|^{2}\right]\\ &\leqslant E_{0}+E_{G}+E_{W}+\frac{8C^{2\alpha}\left(\Phi^{ \max}-\Phi^{\ast}\right)}{\gamma_{\pi}K^{1-\alpha}}\\ &+\frac{32\gamma_{\pi}\kappa^{3}\left(1+\zeta_{v}\right)C^{2+2 \beta}}{\gamma_{y}\bar{v}_{1}^{\alpha}K^{1-\beta}}+\frac{8\kappa L\gamma_{y} \left(1+\zeta_{u}^{2}\right)C^{2-2\beta}}{\left(1-\beta\right)K^{\beta}}\\ &+\left(\gamma_{\pi}L_{\Phi}+\frac{\kappa^{3}L\gamma_{\pi}^{2}}{ \gamma_{y}\bar{v}_{1}^{\alpha-\beta}}+\frac{2\gamma_{\pi}^{4}\kappa^{2}\left( 1+\zeta_{v}^{2}\right)C^{2}\hat{L}^{2}}{\gamma_{y}^{2}\bar{v}_{1}^{3\alpha-2 \beta}}\right)\frac{8\left(1+\zeta_{v}^{2}\right)C^{2-2\alpha}}{\left(1-\alpha \right)K^{\alpha}}\\ &+\sqrt{\frac{1}{n^{1-\alpha}}\left(\frac{4\rho_{W}}{\left(1- \rho_{W}\right)^{2}}\right)^{\alpha}}\frac{16\left(1+\zeta_{v}\right)\zeta_{v }C^{2-\alpha}}{\left(1-\alpha\right)K^{\alpha}}\\ &+\sqrt{\frac{1}{n^{1-\beta}}\left(\frac{4\rho_{W}}{\left(1- \rho_{W}\right)^{2}}\right)^{\beta}}\frac{32\kappa^{2}\left(1+\zeta_{u}\right) \zeta_{u}C^{2-\beta}}{\left(1-\beta\right)K^{\beta}}\\ &+8\kappa LC\sqrt{\frac{8\rho_{W}\gamma_{y}^{2}\left(1+\zeta_{u} ^{2}\right)}{\left(1-\rho_{W}\right)^{2}}\left(\frac{C^{2-4\beta}}{\left(1-2 \beta\right)K^{2\beta}}\mathbb{I}_{\beta<1/2}+\frac{1+\log u_{K}-\log v_{1}}{ K\bar{u}_{1}^{2\beta-1}}\mathbb{I}_{\beta\geqslant 1/2}\right)},\end{split}\] (75)

where \(\hat{L}=\kappa\left(1+\kappa\right)^{2},L_{\Phi}=L\left(1+\kappa\right)\), and

\[\begin{split} E_{0}&:=\frac{4\kappa L}{K\gamma_{y }\bar{u}_{1}^{-\beta}n}\mathbb{E}\left[\left\|\mathbf{y}_{0}-\mathbf{1}y^{\ast }\left(\bar{x}_{0}\right)\right\|^{2}\right]+\frac{16\kappa^{2}\left(4\beta C ^{2}\right)^{2+\frac{1}{1-\beta}}}{K\mu^{2+\frac{1}{1-\beta}}\gamma_{y}^{2+ \frac{1}{1-\beta}}\bar{u}_{1}^{2-2\beta}},\\ E_{G}&:=\frac{16\gamma_{x}^{2}\kappa^{4}\left(1+ \zeta_{v}^{2}\right)G^{2\beta}}{\gamma_{y}^{2}}\left(\frac{C^{2-4\alpha}}{ \left(1-2\alpha\right)K^{2\alpha}}\mathbb{I}_{\alpha<1/2}+\frac{1+\log v_{K} -\log v_{1}}{K\bar{v}_{1}^{2\alpha-1}}\mathbb{I}_{\alpha\geqslant 1/2}\right), \\ E_{W}&:=\frac{32\left(8\kappa L+3L^{2}\right)\rho_{W }\gamma_{x}^{2}\left(1+\zeta_{v}^{2}\right)}{\left(1-\rho_{W}\right)^{2}} \left(\frac{C^{2-4\alpha}}{\left(1-2\alpha\right)K^{2\alpha}}\mathbb{I}_{ \alpha<1/2}+\frac{1+\log v_{K}-\log v_{1}}{K\bar{v}_{1}^{2\alpha-1}}\mathbb{I}_ {\alpha\geqslant 1/2}\right)\\ &+\frac{32\left(8\kappa L+3L^{2}\right)\rho_{W}\gamma_{y}^{2} \left(1+\zeta_{u}^{2}\right)}{\left(1-\rho_{W}\right)^{2}}\left(\frac{C^{2-4 \beta}}{\left(1-2\beta\right)K^{2\beta}}\mathbb{I}_{\beta<1/2}+\frac{1+\log u _{K}-\log v_{1}}{K\bar{u}_{1}^{2\beta-1}}\mathbb{I}_{\beta\geqslant 1/2}\right). \end{split}\]

Letting the total iteration \(K\) satisfy the conditions given in (12) such that the terms \(E_{0}\), \(E_{G}\) and \(E_{W}\) are dominated by the others, we thus complete the proof. 

Proof of Corollary 1.: With the help of Lemma 10, we can directly adapt the proof of Theorem 2 to get the result in (14). 

### Extend the proof to coordinate-wise stepsize

In this subsection, we show how to extend our convergence analysis of D-AdaST to the coordinate-wise adaptive stepsize (Zhou et al., 2018) variant. We first present this variant in Algorithm 2, which can be rewritten in a compact form with the Hadamard product denoted by \(\odot\).

**Algorithm 2**D-AdaST with coordinate-wise adaptive stepsize

```
0:\(x_{i,0}\in\mathbb{R}^{p}\), \(y_{i,0}\in\mathcal{Y}\), buffers \(m_{i,0}^{x},m_{i,0}^{y}>0\), stepsizes \(\gamma_{x},\gamma_{y}>0\) and \(0<\beta<\alpha<1\).
1:for iteration \(k=0,1,\cdots\), each node \(i\in[n]\), do
2: Sample i.i.d \(\xi_{i,k}^{x}\) and \(\xi_{i,k}^{y}\), compute: \[g_{i,k}^{x}=\nabla_{x}f_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^{x}\right),\ g_{i,k}^{y}= \nabla_{y}f_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^{y}\right).\]
3: Accumulate the gradient with Hadamard product: \[m_{i,k+1}^{x}=m_{i,k}^{x}\circ g_{i,k}^{x},\ m_{i,k+1}^{y}=m_{i,k}^{y}+g_{i,k}^ {y}\circ g_{i,k}^{y}\]
4: Compute the ratio: \[\psi_{i,k+1}=\left\|m_{i,k+1}^{x}\right\|^{2\alpha}/\max\left\{\left\|m_{i,k+ 1}^{x}\right\|^{2\alpha},\left\|m_{i,k+1}^{y}\right\|^{2\alpha}\right\}\leqslant 1.\]
5: Update primal and dual variables locally: \[x_{i,k+1} =x_{i,k}-\gamma_{x}\psi_{i,k+1}\left(m_{i,k+1}^{x}\right)^{- \alpha}\odot g_{i,k}^{x},\] \[y_{i,k+1} =y_{i,k}+\gamma_{y}\left(m_{i,k+1}^{y}\right)^{-\beta}\odot g_{i, k}^{y}.\]
6: Communicate parameters with neighbors: \[\left\{m_{i,k+1}^{x},m_{i,k+1}^{y},x_{i,k+1},y_{i,k+1}\right\}\leftarrow\sum_{j \in\mathcal{N}_{i}}W_{i,j}\left\{m_{j,k+1}^{x},m_{j,k+1}^{y},x_{j,k+1},y_{j,k+ 1}\right\}\!.\]
7: Projection of dual variable on to set \(\mathcal{Y}\): \(y_{i,k+1}\leftarrow\mathcal{P}_{\mathcal{Y}}\left(y_{i,k+1}\right)\).
8:endfor ```

**Algorithm 2**D-AdaST with coordinate-wise adaptive stepsize

\[\mathbf{m}_{k+1}^{x} =W\left(\mathbf{m}_{k}^{x}+\mathbf{h}_{k}^{x}\right),\] (76a) \[\mathbf{m}_{k+1}^{y} =W\left(\mathbf{m}_{k}^{y}+\mathbf{h}_{k}^{y}\right),\] (76b) \[\mathbf{x}_{k+1} =W\left(\mathbf{x}_{k}-\gamma_{x}V_{k+1}^{-\alpha}\odot\nabla_{ x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right),\] (76c) \[\mathbf{y}_{k+1} =\mathcal{P}_{\mathcal{Y}}\left(W\left(\mathbf{y}_{k}+\gamma_{y} U_{k+1}^{-\beta}\odot\nabla_{y}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right) \right)\right),\] (76d)

where

\[\boldsymbol{h}_{k}^{x}=\left[\cdots,g_{i,k}^{x}\odot g_{i,k}^{x},\cdots \right]^{T}\in\mathbb{R}^{n\times p},\ \boldsymbol{h}_{k}^{y}=\left[\cdots,g_{i,k}^{y}\odot g_{i,k}^{y},\cdots \right]^{T}\in\mathbb{R}^{n\times d},\]

and the matrices \(U_{k}^{\alpha}\) and \(V_{k}^{\beta}\) are redefined as follows:

\[V_{k}^{-\alpha}=\left[\cdots,v_{i,k}^{-\alpha},\cdots\right]^{T}, \ \left[v_{i,k}\right]_{j}=\max\left\{\left[m_{i,k}^{x}\right]_{j},\left[m_{i,k}^ {y}\right]_{j}\right\},\ j\in[p]\,,\] (77) \[U_{k}^{-\beta}=\left[\cdots,u_{i,k}^{-\beta},\cdots\right]^{T}, \ \left[u_{i,k}\right]_{j}=\left[m_{i,k}^{y}\right]_{j},\ j\in[d]\,,\]

where \(\left[\cdot\right]_{j}\) denotes the \(j\)-th element of a vector.

Recalling the definitions of inconsistency of stepsize in (8), we give the following notations:

\[\tilde{V}_{k} =V_{k}-\bar{v}_{k}\mathbf{1}\mathbf{1}_{p}^{T},\ \bar{v}_{k}=\frac{1}{np}\sum_{i=1}^{n}\sum_{j}^{p}V_{ij},\ \bar{v}_{i,k}=\frac{1}{p}\sum_{j}^{p}V_{ij},\ \bar{v}_{j,k}=\frac{1}{n}\sum_{i=1}^{n}V_{ij},\] (78) \[\tilde{U}_{k} =U_{k}-\bar{u}_{k}\mathbf{1}\mathbf{1}_{p}^{T},\,\bar{u}_{k}= \frac{1}{nd}\sum_{i=1}^{n}\sum_{j}^{d}U_{ij},\ \bar{u}_{i,k}=\frac{1}{d}\sum_{j}^{d}U_{ij},\ \bar{u}_{j,k}=\frac{1}{n}\sum_{i=1}^{n}U_{ij},\]and

\[\zeta_{V}^{2} =\sup_{k\geqslant 0}\left\{\frac{\left\|V_{k}^{-\alpha}-\bar{v}_{k}^{- \alpha}\mathbf{1}\mathbf{1}_{p}^{T}\right\|^{2}}{np\left(\bar{v}_{k}^{-\alpha} \right)^{2}}\right\},\ \hat{\zeta}_{v}^{2}=\sup_{k\geqslant 0}\left\{\frac{\left\|V_{k}^{- \alpha}-\left(V_{k}\mathbf{J}_{p}\right)^{-\alpha}\right\|^{2}}{np\left(\bar{v }_{k}^{-\alpha}\right)^{2}}\right\},\] \[\zeta_{U}^{2} =\sup_{k\geqslant 0}\left\{\frac{\left\|U_{k}^{-\beta}-\bar{u}_{k}^{- \beta}\mathbf{1}\mathbf{1}_{d}^{T}\right\|^{2}}{nd\left(\bar{u}_{k}^{-\beta} \right)^{2}}\right\},\ \hat{\zeta}_{u}^{2}=\sup_{k\geqslant 0}\left\{\frac{\left\|U_{k}^{- \beta}-\left(U_{k}\mathbf{J}_{d}\right)^{-\beta}\right\|^{2}}{nd\left(\bar{u} _{k}^{-\beta}\right)^{2}}\right\}.\]

Building upon the established definitions of coordinate-wise stepsize inconsistency, the subsequent lemma is presented to show the non-convergence of the inconsistency term compared to Lemma 9.

**Lemma 11** (Inconsistency, coordinate-wise).: _Suppose Assumption 1-4 hold. For the proposed D-AdaST algorithm, we have_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\frac{\mathbf{ 1}^{T}}{n\bar{v}_{k+1}^{-\alpha}}\tilde{V}_{k+1}^{-\beta}\odot\nabla_{x}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}\right]\] (79) \[\leqslant 2\left(1+\zeta_{v}\right)\zeta_{v}\sqrt{\frac{1}{n^{1- \alpha}}\left(\frac{4C^{2}\rho_{W}}{\left(1-\rho_{W}\right)^{2}}\right)^{ \alpha}}\frac{C^{2-2\alpha}}{\left(1-\alpha\right)K^{\alpha}}+2np\hat{\zeta}_{ v}^{2}C^{2}\]

_and_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\frac{\mathbf{ 1}^{T}}{n\bar{v}_{k+1}^{-\beta}}\tilde{U}_{k+1}^{-\beta}\odot\nabla_{y}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}\right]\] (80) \[\leqslant 2\left(1+\zeta_{u}\right)\zeta_{u}\sqrt{\frac{1}{n^{1- \beta}}\left(\frac{4C^{2}\rho_{W}}{\left(1-\rho_{W}\right)^{2}}\right)^{\beta }}\frac{C^{2-2\beta}}{\left(1-\beta\right)K^{\beta}}+2nd\hat{\zeta}_{u}^{2}C ^{2}.\]

_In contrast, for D-TaAda, we have_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\frac{\mathbf{ 1}^{T}}{n\bar{v}_{k+1}^{-\alpha}}\tilde{V}_{k+1}^{-\alpha}\odot\nabla_{x}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right] \leqslant p\zeta_{V}^{2}C^{2},\] (81) \[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\frac{\mathbf{ 1}^{T}}{n\bar{u}_{k+1}^{-\beta}}\tilde{U}_{k+1}^{-\beta}\odot\nabla_{y}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{y}\right)\right\|^{2}\right] \leqslant d\zeta_{U}^{2}C^{2}.\]

Proof.: For the coordinate-wise adaptive stepsize, with the definitions of Frobenius norm and Hadamard product, we have

\[\mathbb{E}\left[\left\|\frac{\mathbf{1}^{T}}{n\bar{v}_{k+1}^{- \alpha}}\tilde{V}_{k+1}^{-\alpha}\odot\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{ y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right]\] (82) \[=\mathbb{E}\left[\left\|\frac{\mathbf{1}^{T}}{n\bar{v}_{k+1}^{- \alpha}}\left(V_{k+1}^{-\alpha}-\left(V_{k+1}\mathbf{J}\right)^{-\alpha}+\left( V_{k+1}\mathbf{J}\right)^{-\alpha}-\bar{v}_{k+1}^{-\alpha}\mathbf{1}\mathbf{1}_{p}^{T} \right)\odot\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right) \right\|^{2}\right]\] \[\leqslant 2\mathbb{E}\left[\left\|\frac{\mathbf{1}^{T}}{n\bar{v}_{k+1 }^{-\alpha}}\left(\left(V_{k+1}\mathbf{J}\right)^{-\alpha}-\bar{v}_{k+1}^{- \alpha}\mathbf{1}\mathbf{1}_{p}^{T}\right)\odot\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right]\] \[+2\mathbb{E}\left[\left\|\frac{\mathbf{1}^{T}}{n\bar{v}_{k+1}^{- \alpha}}\left(V_{k+1}^{-\alpha}-\left(V_{k+1}\mathbf{J}\right)^{-\alpha}\right) \odot\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\| ^{2}\right].\]For the first term on the RHS, according to the definitions given in (78), we have

\[\begin{split}&\mathbb{E}\left[\left\|\frac{\mathbf{1}^{T}}{n\bar{v}_{k+1 }^{-\alpha}}\left(\left(V_{k+1}\mathbf{J}\right)^{-\alpha}-\bar{v}_{k+1}^{- \alpha}\mathbf{1}\mathbf{1}_{p}^{T}\right)\odot\nabla_{x}F\left(\mathbf{x}_{k },\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right]\\ &\leqslant\mathbb{E}\left[\frac{1}{n^{2}\bar{v}_{k+1}^{-2\alpha}} \sum_{i=1}^{n}\left(\bar{v}_{i,k+1}^{\alpha}-\bar{v}_{k+1}^{\alpha}\right)^{2} \left\|\nabla_{x}f_{i}\left(x_{i,k},y_{i,k};\xi_{i,k}^{x}\right)\right\|^{2} \right].\end{split}\] (83)

Then, for the second part, we have

\[\begin{split}&\mathbb{E}\left[\left\|\frac{\mathbf{1}^{T}}{n\bar{v}_{k+ 1}^{-\alpha}}\left(V_{k+1}^{-\alpha}-\left(V_{k+1}\mathbf{J}\right)^{-\alpha} \right)\odot\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right) \right\|^{2}\right]\\ &\leqslant\frac{1}{n}\mathbb{E}\left[\left\|\frac{V_{k+1}^{- \alpha}-\left(V_{k+1}\mathbf{J}\right)^{-\alpha}}{\bar{v}_{k+1}^{-\alpha}} \right\|^{2}\left\|\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x} \right)\right\|^{2}\right]\\ &\leqslant p\hat{\zeta}_{v}^{2}\mathbb{E}\left[\left\|\nabla_{x}F \left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right]. \end{split}\] (84)

where the term \(\hat{\zeta}_{v}^{2}\) is not guaranteed to be convergent because the stepsizes between the different dimensions of each node are not consistent. Then, similar to the proof of Lemma 9, we can obtain the result presented in (79).

Next, noticing that for D-TiAda,

\[\mathbb{E}\left[\left\|\frac{\mathbf{1}^{T}}{n\bar{v}_{k+1}^{-\alpha}}\tilde{ V}_{k+1}^{-\alpha}\odot\nabla_{x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x} \right)\right\|^{2}\right]\leqslant\frac{1}{n}\mathbb{E}\left[\left\|\frac{ \tilde{V}_{k+1}^{-\alpha}}{\bar{v}_{k+1}^{-\alpha}}\right\|^{2}\left\|\nabla_{ x}F\left(\mathbf{x}_{k},\mathbf{y}_{k};\xi_{k}^{x}\right)\right\|^{2}\right] \leqslant p\hat{\zeta}_{V}^{2}C^{2},\] (85)

and using Lemma 9, we complete the proof. 

**Theorem 3**.: _Suppose Assumption 1-4 hold. Let \(0<\beta<\alpha<1\) and the total iteration satisfy_

\[K=\Omega\left(\max\left\{\left(\frac{\gamma_{x}^{2}\kappa^{4}}{\gamma_{y}^{2}} \right)^{\frac{1}{\alpha-\beta}},\ \ \ \left(\frac{1}{\left(1-\rho_{W}\right)^{2}}\right)^{\max\left\{\frac{1}{\alpha}, \frac{1}{\beta}\right\}}\right\}\right).\]

_to ensure time-scale separation and quasi-independence of network. For D-AdaST with coordinate-wise adaptive stepsize, we have_

\[\begin{split}&\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\| \nabla\Phi\left(\bar{x}_{k}\right)\right\|^{2}\right]\\ &=\tilde{\mathcal{O}}\left(\frac{1}{K^{1-\alpha}}+\frac{1}{\left( 1-\rho_{W}\right)^{\alpha}K^{\alpha}}+\frac{1}{K^{1-\beta}}+\frac{1}{\left(1- \rho_{W}\right)K^{\beta}}\right)+\mathcal{O}\left(n\left(p\hat{\zeta}_{v}^{2}+ \kappa^{2}d\hat{\zeta}_{u}^{2}\right)C^{2}\right).\end{split}\] (86)

Proof.: With the help of Lemma 11 and the obtained result (75) in the proof of Theorem 2, we can derive the convergence results for D-AdaST with coordinate-wise adaptive stepsize. 

**Remark 6**.: _In Theorem 3, we show that the coordinate-wise variant of D-AdaST exhibits a steady-state error in its upper bound. This error depends on the number of nodes and the dimension of the problem, which stems from the stepsize inconsistency in each dimension of the local decision variables for each node (c.f., Line 3 of Algorithm 2)._

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The contributions and scope of this work have been accurately discussed. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes], Justification: We have carefully discussed the limitations of this work in terms of assumptions and main results. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We have provided a full set of assumptions and complete proof for the theoretical results. See Section 3 and Appendix B.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided detailed experimental settings and reproducibility information for the experiments of this work. See Appendix A. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code of this work is included in the supplementary. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have provided detailed experimental settings in Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Multiple runs with averaging are used to produce the experimental curves in this work. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided sufficient information on the computer resources in Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This work conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel are negative. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: NA Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The license/copyright information of the code and dataset in this paper is clear. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code of this paper is included in the supplementary. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: NA Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: NA Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.