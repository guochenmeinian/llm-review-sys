# Unleashing the Power of Randomization in

Auditing Differentially Private ML

 Krishna Pillutla\({}^{1}\) & Galen Andrew\({}^{1}\) & Peter Kairouz\({}^{1}\)

**H. Brendan McMahan\({}^{1}\)** & **Alina Oprea\({}^{1,2}\)** & **Sweoong Oh\({}^{1,3}\)**

\({}^{1}\)Google Research & \({}^{2}\)Northeastern University & \({}^{3}\)University of Washington

###### Abstract

We present a rigorous methodology for auditing differentially private machine learning algorithms by adding multiple carefully designed examples called canaries. We take a first principles approach based on three key components. First, we introduce Lifted Differential Privacy (LiDP) which expands the definition of differential privacy to handle randomized datasets. This gives us the freedom to design randomized canaries. Second, we audit LiDP by trying to distinguish between the model trained with \(K\) canaries versus \(K-1\) canaries in the dataset, leaving one canary out. By drawing the canaries i.i.d., LiDAR can leverage the symmetry in the design and reuse each privately trained model to run multiple statistical tests, one for each canary. Third, we introduce novel confidence intervals that take advantage of the multiple test statistics by adapting to the empirical higher-order correlations. Together, this new recipe demonstrates significant improvements in sample complexity, both theoretically and empirically, using synthetic and real data. Further, recent advances in designing stronger canaries can be readily incorporated into the new framework.

## 1 Introduction

Differential privacy (DP), introduced in [22], has gained widespread adoption by governments, companies, and researchers by formally ensuring plausible deniability for participating individuals. This is achieved by guaranteeing that a curious observer of the output of a query cannot be confident in their answer to the following binary hypothesis test: _did a particular individual participate in the dataset or not?_ For example, introducing sufficient randomness when training a model on a certain dataset ensures a desired level of differential privacy. This in turn ensures that an individual's sensitive information cannot be inferred from the trained model with high confidence. However, calibrating the right amount of noise can be a challenging process. It is easy to make mistakes in implementing a DP mechanism due to intricacies involving micro-batching, sensitivity analysis, and privacy accounting. Even with correct implementation, there are several incidents of published DP algorithms with miscalculated privacy guarantees that falsely report higher levels of privacy [16, 34, 40, 47, 57, 58]. Data-driven approaches to auditing a mechanism for a violation of a claimed privacy guarantee can significantly mitigate the danger of unintentionally leaking sensitive data.

Popular approaches for auditing privacy share three common components [e.g. 31, 32, 45, 69, 33]. Conceptually, these approaches are founded on the definition of DP and involve producing counterexamples that potentially violate the DP condition. Algorithmically, this leads to the standard recipe of injecting a single carefully designed example, referred to as a _canary_, and running a statistical hypothesis test for its presence from the outcome of the mechanism. Analytically, a high-confidence bound on the DP condition is derived by calculating the confidence intervals of the corresponding Bernoulli random variables from \(n\) independent trials of the mechanism.

Recent advances adopt this standard approach and focus on designing stronger canaries to reduce the number of trials required to successfully audit DP [e.g., 31; 32; 33; 45; 69]. However, each independent trial can be as costly as training a model from scratch; refuting a false claim of \((\varepsilon,\delta)\)-DP with a minimal number of samples is of utmost importance. In practice, standard auditing can require training on the range of thousands to hundreds of thousands of models [e.g., 60]. Unfortunately, under the standard recipe, we are fundamentally limited by the \(1/\sqrt{n}\) sample dependence of the Bernoulli confidence intervals.

**Contributions.** We break this \(1/\sqrt{n}\) barrier by rethinking auditing from first principles.

1. **Lifted DP**: We propose to audit an equivalent definition of DP, which we call Lifted DP in SS3.1. This gives an auditor the freedom to design a counter-example consisting of _random_ datasets and rejection sets. This enables adding random canaries, which is critical in the next step. Theorem 3 shows that violation of Lifted DP implies violation of DP, justifying our framework.
2. **Auditing Lifted DP with Multiple Random Canaries**: We propose adding \(K>1\) canaries under the alternative hypothesis and comparing it against a dataset with \(K-1\) canaries, leaving one canary out. If the canaries are deterministic, we need \(K\) separate null hypotheses; each hypothesis leaves one canary out. Our new recipe overcomes this inefficiency in SS3.2 by drawing _random_ canaries independently from the same distribution. This ensures the exchangeability of the test statistics, allowing us to reuse each privately trained model to run multiple hypothesis tests in a principled manner. This is critical in making the confidence interval sample efficient.
3. **Adaptive Confidence Intervals**: Due to the symmetry of our design, the test statistics follow a special distribution that we call _eXchangeable Bernoulli (XBern)_. Auditing privacy boils down to computing confidence intervals on the average test statistic over the \(K\) canaries included in the dataset. If the test statistics are independent, the resulting confidence interval scales as \(1/\sqrt{nK}\). However, in practice, the dependence is non-zero and unknown. We propose a new and principled family of confidence intervals in SS3.3 that adapts to the empirical higher-order correlations between the test statistics. This gives significantly smaller confidence intervals when the actual dependence is small, both theoretically (Proposition 4) and empirically.
4. **Numerical Results**: We audit an unknown Gaussian mechanism with black-box access and demonstrate (up to) \(16\times\) improvement in the sample complexity. We also show how to seamlessly lift recently proposed canary designs in our recipe to improve the sample complexity on real data.

## 2 Background

We describe the standard recipe to audit DP. Formally, we adopt the so-called add/remove definition of differential privacy; our constructions also seamlessly extend to other choices of neighborhoods as we explain in SSB.2.

**Definition 1** (Differential privacy).: A pair of datasets, \((D_{0},D_{1})\), is said to be _neighboring_ if their sizes differ by one and the datasets differ in one entry: \(|D_{1}\setminus D_{0}|+|D_{0}\setminus D_{1}|=1\). A randomized mechanism \(\mathcal{A}:\mathcal{Z}^{*}\to\mathcal{R}\) is said to be \((\varepsilon,\delta)\)_-Differentially Private_ (DP) for some \(\varepsilon\geq 0\) and \(\delta\in[0,1]\) if it satisfies \(\mathbb{P}_{\mathcal{A}}(\mathcal{A}(D_{1})\in R)\leq e^{\varepsilon}\, \mathbb{P}_{\mathcal{A}}(\mathcal{A}(D_{0})\in R)+\delta\), which is equivalent to

\[\varepsilon\geq\log\left(\mathbb{P}(\mathcal{A}(D_{1})\in R)-\delta\right)- \log\mathbb{P}(\mathcal{A}(D_{0})\in R)\,,\] (1)

for all pairs of neighboring datasets, \((D_{0},D_{1})\), and all measurable sets, \(R\subset\mathcal{R}\), of the output space \(\mathcal{R}\), where \(\mathbb{P}_{\mathcal{A}}\) is over the randomness internal to the mechanism \(\mathcal{A}\). Here, \(\mathcal{Z}^{*}\) is a space of datasets.

For small \(\varepsilon\) and \(\delta\), one cannot infer from the output whether a particular individual is in the dataset or not with a high success probability. For a formal connection, we refer to [35]. This naturally leads to a standard procedure for auditing a mechanism \(\mathcal{A}\) claiming \((\varepsilon,\delta)\)-DP: present \((D_{0},D_{1},R)\in\mathcal{Z}^{*}\times\mathcal{Z}^{*}\times\mathcal{R}\) that violates Eq. (1) as a piece of evidence. Such a counter-example confirms that an adversary attempting to test the participation of an individual will succeed with sufficient probability, thus removing the potential for plausible deniability for the participants.

**Standard Recipe: Adding a Single Canary.** When auditing DP model training (using e.g. DP-SGD [1; 54]), the following recipe is now standard for designing a counter-example \((D_{0},D_{1},R)\)[31; 32; 33; 45; 69]. A training dataset \(D_{0}\) is assumed to be given. This ensures that the model under scrutiny matches the use-case and is called a _null hypothesis_. Next, under a corresponding _alternative hypothesis_, a neighboring dataset \(D_{1}=D_{0}\cup\{c\}\) is constructed by adding a single carefully-designedexample \(c\in\mathcal{Z}\), known as a _canary_. Finally, Eq. (1) is evaluated with a choice of \(R\) called a _rejection set_. For example, one can reject the null hypothesis (and claim the presence of the canary) if the loss on the canary is smaller than a fixed threshold; \(R\) is a set of models satisfying this rejection rule.

**Bernoulli Confidence Intervals.** Once a counter-example \((D_{0},D_{1},R)\) is selected, we are left to evaluate the DP condition in Eq. (1). Since the two probabilities in the condition cannot be directly evaluated, we rely on the samples of the output from the mechanism, e.g., models trained with DP-SGD. This is equivalent to estimating the expectation, \(\mathbb{P}(\mathcal{A}(D)\in R)\) for \(D\in\{D_{0},D_{1}\}\), of a Bernoulli random variable, \(\mathbb{I}(\mathcal{A}(D)\in R)\), from \(n\) i.i.d. samples. Providing high confidence intervals for Bernoulli distributions is a well-studied problem with several off-the-shelf techniques, such as Clopper-Pearson, Jeffreys, Bernstein, and Wilson intervals. Concretely, let \(\hat{\mathbb{P}}_{n}(\mathcal{A}(D)\in R)\) denote the empirical probability of the model falling in the rejection set in \(n\) independent runs. The standard intervals scale as \(|\mathbb{P}(\mathcal{A}(D_{0})\in R)-\hat{\mathbb{P}}_{n}(\mathcal{A}(D_{0}) \in R)|\leq C_{0}n^{-1/2}\) and \(|\mathbb{P}(\mathcal{A}(D_{1})\in R)-\hat{\mathbb{P}}_{n}(\mathcal{A}(D_{1}) \in R)|\leq C_{1}n^{-1/2}\) for constants \(C_{0}\) and \(C_{1}\) independent of \(n\). If \(\mathcal{A}\) satisfies a claimed \((\varepsilon,\delta)\)-DP in Eq. (1), then the following finite-sample lower bound holds with high confidence:

\[\varepsilon\geq\hat{\varepsilon}_{n}=\log\left(\hat{\mathbb{P}}_{n}(\mathcal{ A}(D_{1})\in R)-\tfrac{C_{1}}{\sqrt{n}}-\delta\right)-\log\left(\hat{ \mathbb{P}}_{n}(\mathcal{A}(D_{0})\in R)+\tfrac{C_{0}}{\sqrt{n}}\right)\,.\] (2)

\(\text{Auditing}(\varepsilon,\delta)\)-DP amounts to testing the violation of this condition. This is fundamentally limited by the \(n^{-1/2}\) dependence of the Bernoulli confidence intervals. Our goal is to break this barrier.

**Notation.** While the DP condition is symmetric in \((D_{0},D_{1})\), we use \(D_{0},D_{1}\) to refer to specific hypotheses. For symmetry, we need to check both conditions: Eq. (1) and its counterpart with \(D_{0},D_{1}\) interchanged. We omit this second condition for notational convenience. We use the shorthand \([k]:=\{1,2,\ldots,k\}\). We refer to random variables by boldfaced letters (e.g. \(\bm{D}\) is a random dataset).

**Related Work.** We provide a detailed survey in Appendix A. A stronger canary (and its rejection set) can increase the RHS of (1). The resulting hypothesis test can tolerate larger confidence intervals, thus requiring fewer samples. This has been the focus of recent breakthroughs in privacy auditing in [32, 39, 41, 45, 60]. They build upon membership inference attacks [e.g. 12, 53, 68] to measure memorization. Our aim is not to innovate on this front. Instead, our framework can seamlessly adopt recently designed canaries and inherit their strengths as demonstrated in SS5 and SS6.

Random canaries have been used in prior work, but for making the canary out-of-distribution in a computationally efficient manner. No variance reduction is achieved by such random canaries. Adding multiple (deterministic) canaries has been explored in literature but for different purposes. [32, 39] include multiple copies of the same canary to make the canary easier to detect while paying for group privacy since the paired datasets differ in multiple entries (see SS3.2 for a detailed discussion). [42, 69] propose adding multiple distinct canaries to reuse each trained model for multiple hypothesis tests. However, each canary is no stronger than a single canary case, and the resulting auditing suffers from group privacy. When computing the lower bound on \(\varepsilon\), however, group privacy is ignored and the test statistics are assumed to be independent without rigorous justification. [2] avoids group privacy in the federated scenario where the adversary has the freedom to return a canary gradient update of choice. The prescribed random gradient shows good empirical performance. The confidence interval is not rigorously derived. Our recipe for injecting multiple canaries _without_ a group privacy cost with _rigorous_ confidence intervals can be incorporated into these works to give provable lower bounds.

In an independent and concurrent work, Steinke et al. [56] also consider auditing with randomized canaries that are Poisson-sampled, i.e., each canary is included or excluded independently with equal probability. Their recipe involves computing an empirical lower bound by comparing the accuracy (rather than the full confusion matrix as in our case) from the possibly dependent guesses with the worst-case randomized response mechanism. This allows them to use multiple dependent observations from a single trial to give a high probability lower bound on \(\varepsilon\). Their confidence intervals, unlike the ones we give here, are non-adaptive and worst-case.

## 3 A New Framework for Auditing DP Mechanisms with Multiple Canaries

We define Lifted DP, a new definition of privacy that is equivalent to DP (SS3.1). This allows us to define a new recipe for auditing with multiple random canaries, as opposed to a single deterministic canary in the standard recipe, and reuse each trained model to run multiple correlated hypothesis testsin a principled manner (SS3.2). The resulting test statistics form a vector of _dependent but exchangeable_ indicators (which we call an eXchangeable Bernoulli or XBern distribution), as opposed to a single Bernoulli distribution. We leverage this exchangeability to give confidence intervals for the XBern distribution that can potentially improve with the number of injected canaries (SS3.3). The pseudocode of our approach is provided in Algorithm 1.

### From DP to Lifted DP

To enlarge the design space of counter-examples, we introduce an equivalent definition of DP.

**Definition 2** (Lifted differential privacy).: Let \(\mathcal{P}\) denote a joint probability distribution over \((\bm{D}_{0},\bm{D}_{1},\bm{R})\) where \((\bm{D}_{0},\bm{D}_{1})\in\mathcal{Z}^{*}\times\mathcal{Z}^{*}\) is a pair of _random_ datasets that are neighboring (as in the standard definition of neighborhood in Definition 1) with probability one and let \(\bm{R}\subset\mathcal{R}\) denote a _random_ rejection set. We say that a randomized mechanism \(\mathcal{A}:\mathcal{Z}^{*}\to\mathcal{R}\) satisfies \((\varepsilon,\delta)\)_-Lifted Differential Privacy_ (LiDP) for some \(\varepsilon\geq 0\) and \(\delta\in[0,1]\) if, for all \(\mathcal{P}\) independent of \(\mathcal{A}\), we have

\[\mathbb{P}_{\mathcal{A},\mathcal{P}}(\mathcal{A}(\bm{D}_{1})\in\bm{R}) \leq e^{\varepsilon}\,\mathbb{P}_{\mathcal{A},\mathcal{P}}(\mathcal{A }(\bm{D}_{0})\in\bm{R})+\delta\,.\] (3)

In Appendix A.3, we discuss connections between Lifted DP and other existing extensions of DP, such as Bayesian DP and Pufferfish, that also consider randomized datasets. The following theorem shows that LiDP is equivalent to the standard DP, which justifies our framework of checking the above condition; if a mechanism \(\mathcal{A}\) violates the above condition then it violates \((\varepsilon,\delta)\)-DP.

**Theorem 3**.: _A randomized algorithm \(\mathcal{A}\) is \((\varepsilon,\delta)\)-LiDP iff \(\mathcal{A}\) is \((\varepsilon,\delta)\)-DP._

A proof is provided in Appendix B. In contrast to DP, LiDP involves probabilities over both the internal randomness of the algorithm \(\mathcal{A}\) and the distribution \(\mathcal{P}\) over \((\bm{D}_{0},\bm{D}_{1},\bm{R})\). This gives the auditor greater freedom to search over a _lifted_ space of joint distributions over the paired datasets and a rejection set; hence the name Lifted DP. Auditing LiDP amounts to constructing a _randomized_ (as emphasized by the boldface letters) counter-example \((\bm{D}_{0},\bm{D}_{1},\bm{R})\) that violates (3) as evidence.

### From a Single Deterministic Canary to Multiple Random Canaries

Our strategy is to turn the LiDP condition in Eq. (3) into another condition in Eq. (4) below; this allows the auditor to reuse samples, running multiple hypothesis tests on each sample. This derivation critically relies on our carefully designed recipe that incorporates three crucial features: \((a)\) binary hypothesis tests between pairs of stochastically coupled datasets containing \(K\) canaries and \(K-1\) canaries, respectively, for some fixed integer \(K\), \((b)\) sampling those canaries i.i.d. from the same distribution, and (\(c\)) choice of rejection sets, where each rejection set only depends on a single left-out canary. We introduce the following recipe, also presented in Algorithm 1.

We fix a given training set \(D\) and a canary distribution \(P_{\text{canary}}\) over \(\mathcal{Z}\). This ensures that the model under scrutiny is close to the use case. Under the alternative hypothesis, we train a model on a randomized training dataset \(\bm{D}_{1}=D\cup\{\bm{c}_{1},\dots,\bm{c}_{K}\}\), augmented with \(K\) random canaries drawn i.i.d. from \(P_{\text{canary}}\). Conceptually, this is to be tested against \(K\) leave-one-out (LOO) null hypotheses. Under the \(k^{\text{th}}\) null hypothesis for each \(k\in[K]\), we construct a coupled dataset, \(\bm{D}_{0,k}=D\cup\{\bm{c}_{1},\dots,\bm{c}_{k-1},\bm{c}_{k+1},\dots\bm{c}_{K}\}\), with \(K-1\) canaries, leaving the \(k^{\text{th}}\) canary out. This coupling of \(K-1\) canaries ensures that \((\bm{D}_{0,k},\bm{D}_{1})\) is neighboring with probability one. For each left-out canary, the auditor runs a binary hypothesis test with a choice of a random rejection set \(\bm{R}_{k}\). We restrict \(\bm{R}_{k}\) to depend only on the canary \(\bm{c}_{k}\) that is being tested and not the index \(k\). For example, \(\bm{R}_{k}\) can be the set of models achieving a loss on the canary \(\bm{c}_{k}\) below a predefined threshold \(\tau\).

The goal of this LOO construction is to reuse each trained private model to run multiple tests such that the averaged test statistic has a smaller variance for a _given number of models_. Under the standard definition of DP, one can still use the above LOO construction but with fixed and deterministic canaries. This gives no variance gain because evaluating \(\mathbb{P}(\mathcal{A}(D_{0,k})\in R_{k})\) in Eq. (1) or its averaged counterpart \((1/K)\sum_{k=1}^{K}\mathbb{P}(\mathcal{A}(D_{0,k})\in R_{k})\) requires training one model to get one sample from the test statistic \(\mathbb{I}(\mathcal{A}(D_{0,k})\in R_{k})\). The key ingredient in _reusing trained models_ is randomization.

We build upon the LiDP condition in Eq. (3) by noting that the test statistics are exchangeable for i.i.d. canaries. Specifically, we have for any \(k\in[K]\) that \(\mathbb{P}(\mathcal{A}(\bm{D}_{0,k})\in\bm{R}_{k})=\mathbb{P}(\mathcal{A}(\bm {D}_{0,K})\in\bm{R}_{K})=\mathbb{P}(\mathcal{A}(\bm{D}_{0,K})\in\bm{R}_{j}^{ \prime})\) for any canary \(\bm{c}_{j}^{\prime}\) drawn i.i.d. from \(P_{\text{canary}}\) and its corresponding rejection set \(\bm{R}^{\prime}_{j}\) that are statistically independent of \(\bm{D}_{0,K}\). Therefore, we can rewrite the right side of Eq. (3) using \(m\) i.i.d. _test canaries_\(\bm{c}^{\prime}_{1},\ldots,\bm{c}^{\prime}_{m}\sim P_{\text{canary}}\) and a single trained model \(\mathcal{A}(\bm{D}_{0,K})\) as

\[\tfrac{1}{K}\sum_{k=1}^{K}\mathbb{P}_{\mathcal{A},\mathcal{P}}(\mathcal{A}(\bm {D}_{1})\in\bm{R}_{k}) \leq \tfrac{e^{\varepsilon}}{m}\sum_{j=1}^{m}\mathbb{P}_{\mathcal{A}, \mathcal{P}}(\mathcal{A}(\bm{D}_{0,K})\in\bm{R}^{\prime}_{j})+\delta\,.\] (4)

Checking this condition is sufficient for auditing LiDP and, via Theorem 3, for auditing DP. For each model trained on \(\bm{D}_{1}\), we record the test statistics of \(K\) (correlated) binary hypothesis tests. This is denoted by a random vector \(\bm{x}=(\mathbb{I}(\mathcal{A}(\bm{D}_{1})\in\bm{R}_{k}))_{k=1}^{K}\in\{0,1\}^ {K}\), where \(\bm{R}_{k}\) is a rejection set that checks for the presence of the \(k^{\text{th}}\) canary. Similar to the standard recipe, we train \(n\) models to obtain \(n\) i.i.d. samples \(\bm{x}^{(1)},\ldots,\bm{x}^{(n)}\in\{0,1\}^{K}\) to estimate the left side of (4) using the empirical mean:

\[\hat{\bm{\mu}}_{1}:=\tfrac{1}{n}\sum_{i=1}^{n}\tfrac{1}{K}\sum_{k=1}^{K}\bm{x }^{(i)}_{k} \in[0,1]\;,\] (5)

where the subscript one in \(\hat{\bm{\mu}}_{1}\) denotes that this is the empirical first moment. Ideally, if the \(K\) tests are independent, the corresponding confidence interval is smaller by a factor of \(\sqrt{K}\). In practice, the \(K\) tests are correlated and the size of the confidence interval depends on their correlation. We derive principled confidence intervals that leverage the empirically measured correlations in SS3.3. We can define \(\bm{y}\in\{0,1\}^{m}\) and its mean \(\hat{\bm{\nu}}_{1}\) analogously for the null hypothesis. We provide pseudocode in Algorithm 1 as an example guideline for applying our recipe to auditing DP training, where \(f_{\theta}(z)\) is the loss evaluated on an example \(z\) for a model \(\theta\). \(\operatorname{XBernLower}()\) and \(\operatorname{XBernUpper}()\) respectively return the lower and upper adaptive confidence intervals from SS3.3. We instantiate Algorithm 1 with concrete examples of canary design in SS5.

```
0: Sample size \(n\), number of canaries \(K\), number of null tests \(m\), DP mechanism \(\mathcal{A}\), training set \(D\), canary generating distribution \(P_{\text{canary}}\), threshold \(\tau\), failure probability \(\beta\), privacy \(\delta\in[0,1]\).
1:for\(i=1,\ldots,n\)do
2: Randomly generate \(K+m\) canaries \(\{\bm{c}_{1},\ldots,\bm{c}_{K},\bm{c}^{\prime}_{1},\ldots,\bm{c}^{\prime}_{m}\}\) i.i.d. from \(P_{\text{canary}}\).
3:\(\bm{D}_{0}\gets D\cup\{\bm{c}_{1},\ldots,\bm{c}_{K-1}\}\), \(\bm{D}_{1}\gets D\cup\{\bm{c}_{1},\ldots,\bm{c}_{K}\}\)
4: Train two models \(\bm{\theta}_{0}\leftarrow\mathcal{A}(\bm{D}_{0})\) and \(\bm{\theta}_{1}\leftarrow\mathcal{A}(\bm{D}_{1})\)
5: Record test statistics \(\bm{x}^{(i)}\leftarrow\big{(}\mathbb{I}(f_{\bm{\theta}_{1}}(\bm{c}_{k})<\tau) \big{)}_{k=1}^{K}\) and \(\bm{y}^{(i)}\leftarrow\big{(}\mathbb{I}(f_{\bm{\theta}_{0}}(\bm{c}^{\prime}_{ j})<\tau)\big{)}_{j=1}^{m}\)
6: Set \(\underline{p}_{1}\leftarrow\operatorname{XBernLower}\big{(}\{\bm{x}^{(i)}\}_{i \in[n]},\beta/2\big{)}\) and \(\overline{\bm{p}}_{0}\leftarrow\operatorname{XBernUpper}\big{(}\{\bm{y}^{(i) }\}_{i\in[n]},\beta/2\big{)}\)
7:Return\(\hat{\varepsilon}_{n}\leftarrow\log\big{(}(\underline{p}_{1}-\delta)/ \overline{\bm{p}}_{0}\big{)}\) and a guarantee that \(\mathbb{P}(\varepsilon<\hat{\varepsilon}_{n})\leq\beta\). ```

**Algorithm 1** Auditing Lifted DP

**Our Recipe vs. Multiple Deterministic Canaries.** An alternative with deterministic canaries would be to test between \(D_{0}\) with no canaries and \(D_{1}\) with \(K\) canaries such that we can get \(K\) samples from a single trained model on \(D_{0}\), one for each of the \(K\) test statistics \(\{\mathbb{I}(\mathcal{A}(D_{0})\in R_{k})\}_{k=1}^{K}\). However, due to the fact that \(D_{1}\) and \(D_{0}\) are now at Hamming distance \(K\), this suffers from group privacy; we are required to audit for a much larger privacy leakage of \((K\varepsilon,((e^{K\varepsilon}-1)/(e^{\varepsilon}-1))\delta)\)-DP. Under the (deterministic) LOO construction, this translates into \((1/K)\sum_{k=1}^{K}\mathbb{P}(\mathcal{A}(D_{1})\in R_{k})\leq e^{K \varepsilon}(1/K)\sum_{k=1}^{K}\mathbb{P}(\mathcal{A}(D_{0})\in R_{k})+((e^{K \varepsilon}-1)/(e^{\varepsilon}-1))\delta\), where if one canary violates the group privacy condition then the average also violates it. We can reuse a single trained model to get \(K\) test statistics in the above condition, but each canary is distinct and not any stronger than the one from \((\varepsilon,\delta)\)-DP auditing. One cannot obtain stronger counterexamples without sacrificing the sample gain. For example, we can repeat the same canary \(K\) times as proposed in [32]. This makes it easier to detect the canary, making a stronger counter-example, but there is no sample gain as we only get one test statistic per trained model. With deterministic canaries, there is no way to avoid this group privacy cost while our recipe does not incur it.

### From Bernoulli Intervals to Higher-Order Exchangeable Bernoulli (XBern) Intervals

The LiDP condition in Eq. (4) critically relies on the canaries being sampled i.i.d. and the rejection set only depending on the corresponding canary. For such a symmetric design, auditing boils down to deriving a Confidence Interval (CI) for a special family of distributions that we call _Exchangeable Bernoulli (XBern)_. We derive the CI for the alternate hypothesis, i.e., the left side of Eq. (4). The CI under the null hypothesis is analogous.

Recall that \(\bm{x}_{k}:=\mathbb{I}(\mathcal{A}(\bm{D}_{1})\in\bm{R}_{k})\) denotes the test statistic for the \(k^{\text{th}}\) canary. By the symmetry of our design, \(\bm{x}\in\{0,1\}^{K}\) is an _exchangeable_ random vector that is distributed as an exponential family. Further, the distribution of \(\bm{x}\) is fully defined by a \(K\)-dimensional parameter \((\mu_{1},\ldots,\mu_{K})\) where \(\mu_{\ell}\) is the \(\ell^{\text{th}}\) moment of \(\bm{x}\). We call this family XBern. Specifically, this implies permutation invariance of the higher-order moments: \(\mathbb{E}[\bm{x}_{j_{1}}\cdots\bm{x}_{j_{\ell}}]=\mathbb{E}[\bm{x}_{k_{1}} \cdots\bm{x}_{k_{\ell}}]\) for any distinct sets of indices \((j_{l})_{l\in[\ell]}\) and \((k_{l})_{l\in[\ell]}\) for any \(\ell\leq K\). For example, \(\mu_{1}:=\mathbb{E}[(1/K)\sum_{k=1}^{K}\bm{x}_{k}]\), which is the LHS of (4).

Using samples from this XBern, we aim to derive a CI on \(\mu_{1}\) around the empirical mean \(\hat{\bm{\mu}}_{1}\) in (5). Bernstein's inequality applied to our test statistic \(\bm{m}_{1}:=(1/K)\sum_{k=1}^{K}\bm{x}_{k}\) gives,

\[|\hat{\bm{\mu}}_{1}-\mu_{1}| \leq \sqrt{\frac{2\log(2/\beta)}{n}\mathrm{Var}(\bm{m}_{1})}+\frac{2 \log(2/\beta)}{3n}\,,\] (6)

w.p. at least \(1-\beta\). Bounding \(\mathrm{Var}(\bm{m}_{1})\leq\mu_{1}(1-\mu_{1})\) since \(\bm{m}_{1}\in[0,1]\) a.s. and numerically solving the above inequality for \(\mu_{1}\) gives a CI that scales as \(1/\sqrt{n}\) -- see Figure 2 (left). We call this the **1st-order Bernstein bound**, as it depends only on the 1st moment \(\mu_{1}\). Our strategy is to measure the (higher-order) correlations between \(\bm{x}_{k}\)'s to derive a tighter CI that adapts to the given instance. This idea applies to any standard CI. We derive and analyze the higher order Bernstein intervals here, and experiments use Wilson intervals from Appendix C; see also Figure 2 (right).

Concretely, we can leverage the 2nd order correlation by expanding

\[\mathrm{Var}(\bm{m}_{1})=\tfrac{1}{K}(\mu_{1}-\mu_{2})+(\mu_{2}-\mu_{1}^{2}) \quad\text{where}\quad\mu_{2}:=\tfrac{1}{K(K-1)}\sum_{k_{1}<k_{2}\in[K]} \mathbb{E}\left[\bm{x}_{k_{1}}\bm{x}_{k_{2}}\right].\]

Ideally, when the second order correlation \(\mu_{2}-\mu_{1}^{2}=\mathbb{E}[\bm{x}_{1}\bm{x}_{2}]-\mathbb{E}[\bm{x}_{1}] \mathbb{E}[\bm{x}_{2}]\) equals \(0\), we have \(\mathrm{Var}(\bm{m}_{1})=\mu_{1}(1-\mu_{1})/K\), a factor of \(K\) improvement over the worst-case. Our higher-order CIs adapt to the _actual level of correlation_ of \(\bm{x}\) by further estimating the 2nd moment \(\mu_{2}\) from samples. Let \(\overline{\bm{\mu}}_{2}\) be the first-order Bernstein upper bound on \(\mu_{2}\) such that \(\mathbb{P}(\mu_{2}\leq\overline{\bm{\mu}}_{2})\geq 1-\beta\). On this event,

\[\mathrm{Var}(\bm{m}_{1}) \leq \tfrac{1}{K}(\mu_{1}-\overline{\bm{\mu}}_{2})+(\overline{\bm{\mu} }_{2}-\mu_{1}^{2})\,.\] (7)

Combining this with (6) gives us the **2nd-order Bernstein bound** on \(\mu_{1}\), valid w.p. \(1-2\beta\). Since \(\overline{\bm{\mu}}_{2}\lesssim\hat{\bm{\mu}}_{2}+1/\sqrt{n}\) where \(\hat{\bm{\mu}}_{2}\) is the empirical estimate of \(\mu_{2}\), the 2nd-order bound scales as

\[|\mu_{1}-\hat{\bm{\mu}}_{1}| \lesssim \sqrt{\frac{1}{nK}}+\sqrt{\frac{1}{n}|\hat{\bm{\mu}}_{2}-\hat{ \bm{\mu}}_{1}^{2}|}+\frac{1}{n^{3/4}}\,,\] (8)

where constants and log factors are omitted. Thus, our 2nd-order CI can be as small as \(1/\sqrt{nK}+1/n^{3/4}\) (when \(\hat{\bm{\mu}}_{1}^{2}\approx\bm{\mu}_{2}\)) or as large as \(1/\sqrt{n}\) (in the worst-case). With small enough correlations of \(|\hat{\bm{\mu}}_{2}-\hat{\bm{\mu}}_{1}^{2}|=O(1/K)\), this suggests a choice of \(K=O(\sqrt{n})\) to get CI of \(1/n^{3/4}\). In practice, the correlation is controlled by the design of the canary. For the Gaussian mechanism with random canaries, the correlation indeed empirically decays as \(1/K\), as we see in SS4. Thus, the CI decreases monotonically with \(K\). This is also true for the CI under the null hypothesis with \(K\) replaced by \(m\).

However, there is a qualitative difference between the null and alternate hypotheses. Estimation under the alternate hypothesis incurs a larger bias as the number \(K\) of canaries increases -- this is due

Figure 1: **Bias illustration**: Consider the sum query \(z_{1}+z_{2}\) with 2 inputs. Its DP version produces a point in the blue circle w.h.p. due to the noise \(\bm{\xi}\sim\mathcal{N}(0,\bm{I})\) scaled by \(\sigma\). When auditing with a random canary \(\bm{c}\), it contributes additional randomness (red disc) leading to a smaller effective privacy parameter \(\varepsilon\).

Figure 2: **Left**: The Bernstein CI \([\underline{\bm{\mu}},\overline{\bm{\mu}}]\) can be found by equating the two sides of Bernstein’s inequality in Eq. (6). **Right**: The asymptotic Wilson CI is a tightening of the Bernstein CI with a smaller \(1/\sqrt{n}\) coefficient (shown here) and no \(1/n\) term.

to the additional randomness from adding more canaries into the training process, as illustrated in Figure 1. The optimal choice of \(K\) balances the bias and variance. On the other hand, estimation under the null hypothesis incurs no bias and a larger number \(m\) of test canaries always helps. We return to the bias-variance tradeoff in SS4.

**Higher-order intervals.** We can recursively apply this method by expanding the variance of higher-order statistics, and derive higher-order Bernstein bounds. The next recursion uses empirical 3\({}^{\text{rd}}\) and 4\({}^{\text{th}}\) moments \(\hat{\bm{\mu}}_{3}\) and \(\hat{\bm{\mu}}_{4}\) to get the **4\({}^{\text{th}}\)-order Bernstein bound** which scales as

\[|\mu_{1}-\hat{\bm{\mu}}_{1}|\lesssim\sqrt{\frac{1}{nK}}+\sqrt{\frac{1}{n}}\,| \hat{\bm{\mu}}_{2}-\hat{\bm{\mu}}_{1}^{2}|+\frac{1}{n^{3/4}}|\hat{\bm{\mu}}_{4 }-\hat{\bm{\mu}}_{2}^{2}|^{1/4}+\frac{1}{n^{7/8}}\,.\] (9)

Ideally, when the 4\({}^{\text{th}}\)-order correlation is small enough, \(|\hat{\bm{\mu}}_{4}-\hat{\bm{\mu}}_{2}^{2}|=O(1/K)\), (along with the 2nd-order correlation, \(\hat{\bm{\mu}}_{2}-\hat{\bm{\mu}}_{1}^{2}\)) this 4th-order CI scales as \(1/n^{7/8}\) with a choice of \(K=O(n^{3/4})\) improving upon the 2nd-order CI of \(1/n^{3/4}\). We can recursively derive even higher-order CIs, but we find in SS4 that the gains diminish rapidly. In general, the \(\ell^{\text{th}}\) order Bernstein bounds achieve CIs scaling as \(1/n^{(2\ell-1)/2\ell}\) with a choice of \(K=O(n^{(\ell-1)/\ell})\). This shows that the higher-order CI decreases in the order \(\ell\) of the correlations used; we refer to Appendix C for details.

**Proposition 4**.: _For any positive integer \(\ell\) that is a power of two and \(K=\lceil n^{(\ell-1)/\ell}\rceil\), suppose we have \(n\) samples from a \(K\)-dimensional XBern distribution with parameters \((\mu_{1},\dots,\mu_{K})\). If all \(\ell^{\text{th}}\)-order correlations scale as \(1/K\), i.e., \(|\mu_{2\ell^{\prime}}-\mu_{\ell^{\prime}}^{2}|=O(1/K)\), for all \(\ell^{\prime}\leq\ell\) and \(\ell^{\prime}\) is a power of two, then the \(\ell^{\text{th}}\)-order Bernstein bound is \(|\mu_{1}-\hat{\bm{\mu}}_{1}|=O(1/n^{(2\ell-1)/(2\ell)})\)._

## 4 Simulations: Auditing the Gaussian Mechanism

**Setup.** We consider a simple sum query \(q(D)=\sum_{z\in D}z\) over the unit sphere \(\mathcal{Z}=\{z\in\mathbb{R}^{d}\,:\,\|z\|_{2}=1\}\). We want to audit a Gaussian mechanism that returns \(q(D)+\sigma\bm{\xi}\) with standard Gaussian \(\bm{\xi}\sim\mathcal{N}(0,\bm{I}_{d})\) and \(\sigma\) calibrated to ensure \((\varepsilon,\delta)\)-DP. We assume black-box access, where we do not know what mechanism we are auditing and we only access it through samples of the outcomes. A white-box audit is discussed in SSA.1. We apply our new recipe with canaries sampled uniformly at random from \(\mathcal{Z}\). Following standard methods [e.g. 23], we declare that a canary \(\bm{c}_{k}\) is present if \(\bm{c}_{k}^{\top}\mathcal{A}(\bm{D})>\tau\) for a threshold \(\tau\) learned from separate samples. For more details and additional results, see Appendix E. A broad range of values of \(K\) (between 32 and 256 in Figure 3 middle) leads to good performance in auditing LiDP. We use \(K=\sqrt{n}\) (as suggested by our analysis in (8)), \(m=K\) test canaries, and the 2\({}^{\text{nd}}\)-order Wilson estimator (as gains diminish rapidly afterward) as a reliable default setting.

**Sample Complexity Gains.** In Figure 3 (left), the proposed approach of injecting \(K\) canaries with the 2\({}^{\text{nd}}\) order Wilson interval (denoted "LiDP +2\({}^{\text{nd}}\)-Order Wilson") reduces the number of trials, \(n\), needed to reach the same empirical lower bound, \(\hat{\varepsilon}\), by \(4\times\) to \(16\times\), compared to the baseline of injecting a single canary (denoted "DP+Wilson"). We achieve \(\hat{\varepsilon}_{n}=0.85\) with \(n=4096\) (while the baseline requires \(n=65536\)) and \(\hat{\varepsilon}_{n}=0.67\) with \(n=1024\) (while the baseline requires \(n=4096\)).

Figure 3: **Left**: For Gaussian mechanisms, the proposed LiDP-based auditing with \(K\) canaries provides a significant gain in the required number of trials to achieve a desired level of lower bound \(\hat{\varepsilon}\) on the privacy. **Center**: Increasing the number of canaries trades off the bias and the variance, with our prescribed \(K=\sqrt{n}\) achieving a good performance. **Right**: Increasing the dimension makes the canaries less correlated, thus achieving smaller confidence intervals, and larger \(\hat{\varepsilon}\). We shade the standard error over 25 repetitions.

**Number of Canaries and Bias-Variance Tradeoffs.** In Fig. 3 (middle), LiDP auditing with \(2^{\text{nd}}\)/\(4^{\text{th}}\)-order CIs improve with increasing canaries up to a point and then decreases. This is due to a bias-variance tradeoff, which we investigate further in Figure 4 (left). Let \(\hat{\varepsilon}(K,\ell)\) denote the empirical privacy lower bound with \(K\) canaries using an \(\ell^{\text{th}}\) order interval, e.g., the baseline is \(\hat{\varepsilon}(1,1)\). Let the bias from injecting \(K\) canaries and the variance gain from \(\ell^{\text{th}}\)-order interval respectively be

\[\Delta\text{Bias}(K):=\hat{\varepsilon}(K,1)-\hat{\varepsilon}(1,1)\,,\quad \text{and}\quad\Delta\text{Var}(K,\ell):=\hat{\varepsilon}(K,\ell)-\hat{ \varepsilon}(K,1)\,.\] (10)

In Figure 4 (left), the gain \(\Delta\text{Bias}(K)\) from bias is negative and gets worse with increasing \(K\); when testing for each canary, the \(K-1\) other canaries introduce more randomness that makes the test more private and hence lowers the \(\hat{\varepsilon}\) (see also Figure 1. The gain \(\Delta\text{Var}(K)\) in variance is positive and increases with \(K\) before saturating. This improved "variance" of the estimate is a key benefit of our framework. The net improvement \(\hat{\varepsilon}(K,\ell)-\hat{\varepsilon}(1,1)\) is a sum of these two effects. This trade-off between bias and variance explains the concave shape of \(\hat{\varepsilon}\) in \(K\).

Next, we see from Figure 5 that a larger number \(m\) of test canaries always helps, i.e., it does not incur a similar bias-variance tradeoff. Indeed, this is because larger \(m\) does not lead to any additional bias, as discussed in SS3.3. However, the gains quickly saturate, so \(m=K\) is a reliable default, especially for \(K\geq 16\) or so; we refer to Appendix E for more plots and details.

**Correlation between Canaries.** Based on (8), this improvement in the variance can further be examined by looking at the term \(|\hat{\mu}_{2}-\hat{\mu}_{1}^{2}|\) that leads to a narrower \(2^{\text{nd}}\)-order Wilson interval. The log-log plot of this term in Figure 4 (middle) is nearly parallel to the dotted \(1/K\) line (slope = \(-0.93\)), meaning that it decays roughly as \(1/K\) (note that the log-log plot of \(y=cx^{\alpha}\) is a straight line with slope \(a\)). This indicates that we get close to a \(1/\sqrt{nK}\) confidence interval as desired. Similarly, we get that \(|\hat{\mu}_{4}-\hat{\mu}_{2}^{2}|\) decays roughly as \(1/K\) (slope = \(-1.05\)). However, the \(4^{\text{th}}\)-order estimator offers only marginal additional improvements in the small \(\varepsilon\) regime (see Appendix E). Thus, the gain diminishes rapidly in the order of our estimators.

Figure 4: **Left**: Separating the effects of bias and variance in auditing LiDP; cf. definition (10). **Center & Right**: The correlations between the test statistics of the canaries decrease with \(K\) and \(d\), achieving smaller CIs.

Figure 5: **Left**: There is no bias-variance tradeoff with the number \(m\) of test canaries: larger \(m\) always gives a better empirical \(\hat{\varepsilon}\) but the results saturate quickly. This experiment is for the Gaussian mechanism. **Center & Right**: The proposed LiDP-based auditing procedure gives significant improvements similar to Figure 3 when auditing the Laplace mechanism with canaries sampled randomly from the unit \(\ell_{1}\) ball.

**Effect of Dimension on the Bias and Variance.** As the dimension \(d\) increases, the LiDP-based lower bound becomes tighter monotonically as we show in Figure 3 (right). This is due to both the bias gain, as in (10), improving (less negative) and the variance gain improving (more positive). With increasing \(d\), the variance of our estimate reduces because the canaries become less correlated. Guided by Eq. (8,9), we measure the relevant correlation measures \(|\hat{\bm{\mu}}_{2}-\hat{\bm{\mu}}_{1}^{2}|\) and \(|\hat{\bm{\mu}}_{4}-\hat{\bm{\mu}}_{2}^{2}|\) in Figure 4 (right). Both decay approximate as \(1/d\) (slope = \(-1.06\) and \(-1.00\) respectively, ignoring the outlier at \(d=10^{6}\)). This suggests that, for Gaussian mechanisms, the corresponding XBern distribution resulting from our recipe behaves favorably as \(d\) increases.

**Auditing Other Mechanisms.** The proposed LiDP-based auditing recipe is agnostic to the actual privacy mechanism. We audit a Laplace mechanism with canaries drawn uniformly at random over the unit \(\ell_{1}\) ball in Figure 4 (center and right). The results are qualitatively similar to those of the Gaussian mechanism, leading to an eight-fold improvement in the sample complexity over the baseline.

## 5 Lifting Existing Canary Designs

Several prior works (e.g., 32; 41; 45; 60), focus on designing stronger canaries to improve the lower bound on \(\varepsilon\). We provide two concrete examples of how to _lift_ these canary designs to be compatible with our framework while inheriting their strengths. We refer to Appendix D for further details.

We impose two criteria on the distribution \(P_{\text{canary}}\) over canaries for auditing LiDP. First, the injected canaries are easy to detect, so that the probabilities on the left side of (4) are large and those on the right side are small. Second, a canary \(\bm{c}\sim P_{\text{canary}}\), if included in the training of a model \(\theta\), is unlikely to change the membership of \(\theta\in R_{\bm{c}^{\prime}}\) for an independent canary \(\bm{c}^{\prime}\sim P_{\text{canary}}\). Existing canary designs already impose the first condition to audit DP using (1). The second condition ensures that the canaries are uncorrelated, allowing our adaptive CIs to be smaller, as we discussed in SS3.3.

**Data Poisoning via Tail Singular Vectors.** ClipBKD [32] adds as canaries the tail singular vector of the input data (e.g., images). This ensures that the canary is out of distribution, allowing for easy detection. We lift ClipBKD by defining the distribution \(P_{\text{canary}}\) as the uniform distribution over the \(p\) tail singular vectors of the input data. If \(p\) is small relative to the dimension \(d\) of the data, then they are still out of distribution, and hence, easy to detect. For the second condition, the orthogonality of the singular vectors ensures the interaction between canaries is minimal, as measured empirically.

**Random Gradients.** The approach of [2] samples random vectors (of the right norm) as canary gradients, assuming a grey-box access where we can inject gradients. Since random vectors are nearly orthogonal to any fixed vector in high dimensions, their presence is easy to detect with a dot product. Similarly, any two i.i.d. canary gradients are roughly orthogonal, leading to minimal interactions.

## 6 Experiments

We compare the proposed LiDP auditing recipe relative to the standard one for DP training of machine learning models. Our code is available online.1

Footnote 1: https://github.com/google-research/federated/tree/master/lidp_auditing

**Setup.** We test with two classification tasks: FMNIST [65] is a 10-class grayscale image classification dataset, while Purchase-100 is a sparse tabular dataset with \(600\) binary features and \(100\) classes [19; 53]. We train a linear model and a multi-layer perceptron (MLP) with 2 hidden layers using DP-SGD [1] to achieve \((\varepsilon,10^{-5})\)-DP with varying values of \(\varepsilon\). The training is performed using cross-entropy for a fixed epoch budget and a batch size of \(100\). We refer to Appendix F for details.

**Auditing.** We audit the LiDP using the two types of canaries from SS5: data poisoning and random gradient canaries. We vary the number \(K\) of canaries and the number \(n\) of trials. We track the empirical lower bound obtained from the Wilson family of confidence intervals. We compare this with auditing DP, which coincides with auditing LiDP with \(K=1\) canary. We audit _only the final model_ in all the experiments.

**Sample Complexity Gain.** Table 1 shows the reduction in the sample complexity from auditing LiDP. For each canary type, auditing LiDP is better \(11\) out of the \(12\) settings considered. The average improvement (i.e., the harmonic mean over the table) for data poisoning is \(2.3\times\), while for randomgradients, it is \(3.0\times\). Since each trial is a full model training run, this improvement can be quite significant in practice. This improvement can also be seen visually in Figure 6 (left two).

**Number of Canaries.** We see from Figure 6 (right two) that LiDP auditing on real data behaves similarly to Figure 3 in SS4. We also observe the bias-variance tradeoff in the case of data poisoning (center right). The choice \(K=\sqrt{n}\) is competitive with the best value of \(K\), validating our heuristic. Overall, these results show that the insights from SS4 hold even with the significantly more complicated DP mechanism involved in training models privately.

**Additional Comparisons.** We show in SSF.4 that our LiDP auditing with generally outperforms the Bayesian auditing approach of Zanella-Beguelin et al. (2019). This also suggests an interesting future research direction: adapt Bayesian credible intervals for LiDP auditing to get the best of both worlds.

## 7 Conclusion

We introduce a new framework for auditing differentially private learning. Diverging from the standard practice of adding a single deterministic canary, we propose a new recipe of adding multiple i.i.d. random canaries. This is made rigorous by an expanded definition of privacy that we call LiDAR. We provide novel higher-order confidence intervals that can automatically adapt to the level of correlation in the data. We empirically demonstrate that there is a potentially significant gain in sample dependence of the confidence intervals, achieving a favorable bias-variance tradeoff.

Although any rigorous statistical auditing approach can benefit from our framework, it is not yet clear how other popular approaches (e.g. 12) for measuring memorization can be improved with randomization. Bridging this gap is an important practical direction for future research. It is also worth considering how our approach can be adapted to audit the diverse definitions of privacy in machine learning (e.g. 28).

**Broader Impact.** Auditing private training involves a trade-off between the computational cost and the tightness of the guarantee. This may not be appropriate for all practical settings. For deployment in production, it is worth further studying approaches with minimal computational overhead (e.g. 2; 12).

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{**Dataset / Model**} & \multicolumn{3}{c}{**C.I/**} & \multicolumn{3}{c}{**Data Poisoning Canary**} & \multicolumn{3}{c}{**Random Gradient Canary**} \\ \cline{3-10}  & **(Wilson)** & \(\varepsilon=2\) & \(\varepsilon=4\) & \(\varepsilon=8\) & \(\varepsilon=16\) & \(\varepsilon=2\) & \(\varepsilon=4\) & \(\varepsilon=8\) & \(\varepsilon=16\) \\ \hline \multirow{2}{*}{**FMNIST / Linear**} & 2nd-Ord. & \(0.68\) & \(3.31\) & \(2.55\) & \(4.38\) & \(2.69\) & \(3.34\) & \(5.98\) & \(5.06\) \\  & 4th-Ord. & \(0.42\) & \(2.68\) & \(2.29\) & \(3.76\) & \(2.66\) & \(2.70\) & \(7.75\) & \(4.19\) \\ \hline \multirow{2}{*}{**FMNIST / MLP**} & 2nd-Ord. & \(4.80\) & \(1.46\) & \(2.95\) & \(1.60\) & \(4.62\) & \(2.88\) & \(2.33\) & \(5.46\) \\  & 4th-Ord. & \(4.42\) & \(2.49\) & \(2.37\) & \(1.30\) & \(3.95\) & \(2.81\) & \(2.02\) & \(4.60\) \\ \hline \multirow{2}{*}{**Purchase / MLP**} & 2nd-Ord. & \(3.14\) & \(1.06\) & \(1.41\) & \(9.28\) & \(1.41\) & \(0.71\) & \(4.30\) & \(2.84\) \\  & 4th-Ord. & \(2.84\) & \(1.09\) & \(1.36\) & \(6.99\) & \(1.29\) & \(0.42\) & \(4.35\) & \(2.38\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: The (multiplicative) improvement in the sample complexity from auditing LiDP with \(K=16\) canaries compared to auditing DP with \(n=1000\) trials. We determine this factor by linearly interpolating/extrapolating \(\hat{\varepsilon}_{n}\); cf. Figure 6 (left) for a visual representation of these numbers. For instance, an improvement of \(3.31\) means LiDP needs \(n\approx 1000/3.31\approx 302\) trials to reach the same empirical lower bound that DP reaches at \(n=1000\).

Figure 6: **Left two**: LiDP-based auditing with \(K>1\) canaries achieves the same lower bound \(\hat{\varepsilon}\) on the privacy loss with fewer trials. **Right two**: LiDP auditing is robust to \(K\); the prescribed \(K=\sqrt{n}\) is a reliable default.

## Acknowledgements

We acknowledge Lang Liu for helpful discussions regarding hypothesis testing and conditional probability, Matthew Jagielski for help in debugging the experiments with data poisoning canaries, and Zeba Islam for help in simplifying proofs involving combinatorics.

## References

* Abadi et al. [2016] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep Learning with Differential Privacy. In _Proc. of the 2016 ACM SIGSAC Conf. on Computer and Communications Security (CCS'16)_, pages 308-318, 2016.
* Andrew et al. [2023] G. Andrew, P. Kairouz, S. Oh, A. Oprea, H. B. McMahan, and V. Suriyakumar. One-shot Empirical Privacy Estimation for Federated Learning. _arXiv preprint arXiv:2302.03098_, 2023.
* Askin et al. [2022] O. Askin, T. Kutta, and H. Dette. Statistical Quantification of Differential Privacy: A Local Approach. In _2022 IEEE Symposium on Security and Privacy (SP)_, pages 402-421. IEEE, 2022.
* Balle et al. [2020] B. Balle, G. Barthe, M. Gaboardi, J. Hsu, and T. Sato. Hypothesis Testing Interpretations and Renyi Differential Privacy. In _The 23rd International Conference on Artificial Intelligence and Statistics_, volume 108, pages 2496-2506. PMLR, 2020.
* Barthe et al. [2012] G. Barthe, B. Kopf, F. Olmedo, and S. Zanella Beguelin. Probabilistic Relational Reasoning for Differential Privacy. _ACM SIGPLAN Notices_, 47(1):97-110, 2012.
* Barthe et al. [2013] G. Barthe, G. Danezis, B. Gregoire, C. Kunz, and S. Zanella-Beguelin. Verified Computational Differential Privacy with Applications to Smart Metering. In _2013 IEEE 26th Computer Security Foundations Symposium_, pages 287-301. IEEE, 2013.
* Barthe et al. [2014] G. Barthe, M. Gaboardi, E. J. G. Arias, J. Hsu, C. Kunz, and P.-Y. Strub. Proving differential privacy in Hoare logic. In _2014 IEEE 27th Computer Security Foundations Symposium_, pages 411-424. IEEE, 2014.
* Bhaskar et al. [2011] R. Bhaskar, A. Bhowmick, V. Goyal, S. Laxman, and A. Thakurta. Noiseless Database Privacy. In _International Conference on the Theory and Application of Cryptology and Information Security_, pages 215-232. Springer, 2011.
* Bichsel et al. [2021] B. Bichsel, S. Steffen, I. Bogunovic, and M. Vechev. DP-Sniper: Black-Box Discovery of Differential Privacy Violations using Classifiers. In _2021 IEEE Symposium on Security and Privacy (SP)_, pages 391-409. IEEE, 2021.
* Bun and Steinke [2016] M. Bun and T. Steinke. Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds. In _Theory of Cryptography Conference_, pages 635-658. Springer, 2016.
* Bun et al. [2018] M. Bun, J. Ullman, and S. Vadhan. Fingerprinting Codes and the Price of Approximate Differential Privacy. _SIAM Journal on Computing_, 47(5):1888-1938, 2018.
* Carlini et al. [2019] N. Carlini, C. Liu, U. Erlingsson, J. Kos, and D. Song. The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks. In _USENIX Security Symposium_, volume 267, 2019.
* Carlini et al. [2021] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, A. Oprea, and C. Raffel. Extracting Training Data from Large Language Models. In _30th USENIX Security Symposium (USENIX Security 2021)_, 2021.
* Carlini et al. [2022] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer. Membership Inference Attacks from First Principles. In _IEEE Symposium on Security and Privacy (SP)_, pages 1519-1519, Los Alamitos, CA, USA, May 2022. IEEE Computer Society. doi: 10.1109/SP46214.2022.00090. URL https://doi.ieeecomputersociety.org/10.1109/SP46214.2022.00090.
* Carlini et al. [2023] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying Memorization Across Neural Language Models. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=TatRHT_1cK* Chen and Machanavajjhala [2015] Y. Chen and A. Machanavajjhala. On the privacy properties of variants on the sparse vector technique. _arXiv preprint arXiv:1508.07306_, 2015.
* Ding et al. [2018] Z. Ding, Y. Wang, G. Wang, D. Zhang, and D. Kifer. Detecting violations of differential privacy. In _Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security_, pages 475-489, 2018.
* Dixit et al. [2013] K. Dixit, M. Jha, S. Raskhodnikova, and A. Thakurta. Testing Lipschitz Property over Product Distribution and its Applications to Statistical Data Privacy. In _Theory of Cryptography Conference_, pages 418-436. Springer, 2013.
* DMDave [2014] W. C. DMDave, Todd B. Acquire valued shoppers challenge, 2014. URL https://kaggle.com/competitions/acquire-valued-shoppers-challenge.
* Dudley [2014] R. M. Dudley. _Uniform Central Limit Theorems_, volume 142. Cambridge university press, 2014.
* Dwork and Rothblum [2016] C. Dwork and G. N. Rothblum. Concentrated Differential Privacy. _arXiv preprint arXiv:1603.01887_, 2016.
* Dwork et al. [2006] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating Noise to Sensitivity in Private Data Analysis. In _Proc. of the Third Conf. on Theory of Cryptography (TCC)_, pages 265-284, 2006. URL http://dx.doi.org/10.1007/11681878_14.
* Dwork et al. [2015] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan. Robust Traceability from Trace Amounts. In _2015 IEEE 56th Annual Symposium on Foundations of Computer Science_, pages 650-669. IEEE, 2015.
* Gaboardi et al. [2013] M. Gaboardi, A. Haeberlen, J. Hsu, A. Narayan, and B. C. Pierce. Linear Dependent Types for Differential Privacy. In _Proceedings of the 40th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages_, pages 357-370, 2013.
* Gilbert and McMillan [2018] A. C. Gilbert and A. McMillan. Property Testing for Differential Privacy. In _2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 249-258. IEEE, 2018.
* Guo et al. [2022] C. Guo, B. Karrer, K. Chaudhuri, and L. van der Maaten. Bounding Training Data Reconstruction in Private (Deep) Learning. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 8056-8071, 17-23 Jul 2022.
* Han et al. [2020] Y. Han, J. Jiao, and T. Weissman. Minimax estimation of divergences between discrete distributions. _IEEE Journal on Selected Areas in Information Theory_, 1(3):814-823, 2020.
* Hannun et al. [2021] A. Hannun, C. Guo, and L. van der Maaten. Measuring Data Leakage in Machine-Learning Models with Fisher Information. In _UAI_, pages 760-770, 2021.
* Hayes et al. [2023] J. Hayes, S. Mahloujifar, and B. Balle. Bounding Training Data Reconstruction in DP-SGD, 2023.
* Homer et al. [2008] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig. Resolving individuals contributing trace amounts of DNA to highly complex mixtures using high-density SNP genotyping microarrays. _PLoS genetics_, 4(8):e1000167, 2008.
* Hyland and Tople [2019] S. L. Hyland and S. Tople. An Empirical Study on the Intrinsic Privacy of SGD. _arXiv preprint arXiv:1912.02919_, 2019.
* Jagielski et al. [2020] M. Jagielski, J. Ullman, and A. Oprea. Auditing differentially private machine learning: How private is private SGD? _Advances in Neural Information Processing Systems_, 33:22205-22216, 2020.
* Jayaraman and Evans [2019] B. Jayaraman and D. Evans. Evaluating Differentially Private Machine Learning in Practice. In _USENIX Security Symposium_, pages 1895-1912, 2019.

* [34] M. Johnson. fix prng key reuse in differential privacy example. https://github.com/google/jax/pull/3646, 2020.
* [35] P. Kairouz, S. Oh, and P. Viswanath. The composition theorem for differential privacy. In _International conference on machine learning_, pages 1376-1385. PMLR, 2015.
* [36] D. Kifer and A. Machanavajjhala. Pufferfish: A framework for mathematical privacy definitions. _ACM Transactions on Database Systems (TODS)_, 39(1):1-36, 2014.
* [37] T. Kutta, O. Askin, and M. Dunsche. Lower Bounds for Renyi Differential Privacy in a Black-Box Setting. _arXiv preprint arXiv:2212.04739_, 2022.
* [38] X. Liu and S. Oh. Minimax Optimal Estimation of Approximate Differential Privacy on Neighboring Databases. In _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [39] F. Lu, J. Munoz, M. Fuchs, T. LeBlond, E. Zaresky-Williams, E. Raff, F. Ferraro, and B. Testa. A General Framework for Auditing Differentially Private Machine Learning. In _NeurIPS_, 2022.
* [40] M. Lyu, D. Su, and N. Li. Understanding the sparse vector technique for differential privacy. _arXiv preprint arXiv:1603.01699_, 2016.
* [41] S. Maddock, A. Sablayrolles, and P. Stock. CANIFE: Crafting canaries for empirical privacy measurement in federated learning. In _ICLR_, 2023.
* [42] M. Malek Esmaeili, I. Mironov, K. Prasad, I. Shilov, and F. Tramer. Antipodes of label differential privacy: Pate and alibi. _Advances in Neural Information Processing Systems_, 34:6934-6945, 2021.
* [43] F. D. McSherry. Privacy integrated queries: an extensible platform for privacy-preserving data analysis. In _Proceedings of the 2009 ACM SIGMOD International Conference on Management of data_, pages 19-30. ACM, 2009.
* [44] I. Mironov. Renyi Differential Privacy. In _30th IEEE Computer Security Foundations Symposium, CSF 2017, Santa Barbara, CA, USA, August 21-25, 2017_, pages 263-275. IEEE Computer Society, 2017.
* [45] M. Nasr, S. Songi, A. Thakurta, N. Papernot, and N. Carlini. Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning. In _2021 IEEE Symposium on Security and Privacy (SP)_, pages 866-882. IEEE, 2021.
* [46] M. Nasr, J. Hayes, T. Steinke, B. Balle, F. Tramer, M. Jagielski, N. Carlini, and A. Terzis. Tight Auditing of Differentially Private Machine Learning. _arXiv preprint arXiv:2302.07956_, 2023.
* [47] M. Park. Bug-fix. https://github.com/mijunggi/vips_code/commit/4e32042b66c960af618722a43c32f3f2dda2730c, 2018.
* [48] N. Ponomareva, H. Hazimeh, A. Kurakin, Z. Xu, C. Denison, H. B. McMahan, S. Vassilvitskii, S. Chien, and A. Thakurta. How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy. _arXiv Preprint_, 2023.
* [49] M. A. Rahman, T. Rahman, R. Laganiere, N. Mohammed, and Y. Wang. Membership Inference Attack against Differentially Private Deep Learning Model. _Trans. Data Priv._, 11(1):61-79, 2018.
* [50] J. Reed and B. C. Pierce. Distance makes the types grow stronger: a calculus for differential privacy. In _ACM Sigplan Notices_, volume 45, pages 157-168. ACM, 2010.
* [51] I. Roy, S. T. Setty, A. Kilzer, V. Shmatikov, and E. Witchel. Airavat: Security and privacy for MapReduce. In _NSDI_, volume 10, pages 297-312, 2010.
* [52] S. Sankararaman, G. Obozinski, M. I. Jordan, and E. Halperin. Genomic privacy and limits of individual detection in a pool. _Nature genetics_, 41(9):965-967, 2009.

* [53] R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning models. In _2017 IEEE symposium on security and privacy (SP)_, pages 3-18. IEEE, 2017.
* [54] S. Song, K. Chaudhuri, and A. D. Sarwate. Stochastic gradient descent with differentially private updates. In _2013 IEEE Global Conference on Signal and Information Processing_, pages 245-248. IEEE, 2013.
* [55] S. Song, Y. Wang, and K. Chaudhuri. Pufferfish privacy mechanisms for correlated data. In _Proceedings of the 2017 ACM International Conference on Management of Data_, pages 1291-1306, 2017.
* [56] T. Steinke, M. Nasr, and M. Jagielski. Privacy auditing with one (1) training run. _arXiv preprint arXiv:2305.08846_, 2023.
* [57] T. Stevens, I. C. Ngong, D. Darais, C. Hirsch, D. Slater, and J. P. Near. Backpropagation Clipping for Deep Learning with Differential Privacy. _arXiv preprint arXiv:2202.05089_, 2022.
* [58] F. Tramer. Tensorflow privacy issue #153: Incorrect comparison between privacy amplification by iteration and DP-SGD. https://github.com/tensorflow/privacy/issues/153, 2020.
* [59] F. Tramer, R. Shokri, A. San Joaquin, H. Le, M. Jagielski, S. Hong, and N. Carlini. Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets. In _Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security_, CCS '22, page 2779-2792, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450394505. doi: 10.1145/3548606.3560554. URL https://doi.org/10.1145/3548606.3560554.
* [60] F. Tramer, A. Terzis, T. Steinke, S. Song, M. Jagielski, and N. Carlini. Debugging differential privacy: A case study for privacy auditing. _arXiv preprint arXiv:2202.12219_, 2022.
* [61] A. Triastcyn and B. Faltings. Bayesian Differential Privacy for Machine Learning. In _International Conference on Machine Learning_, pages 9583-9592. PMLR, 2020.
* [62] M. C. Tschantz, D. Kaynar, and A. Datta. Formal verification of differential privacy for interactive systems. _Electronic Notes in Theoretical Computer Science_, 276:61-79, 2011.
* [63] S. Vadhan. The complexity of differential privacy. In _Tutorials on the Foundations of Cryptography_, pages 347-450. Springer, 2017.
* [64] G. Valiant and P. Valiant. Estimating the unseen: improved estimators for entropy and other properties. _Journal of the ACM (JACM)_, 64(6):1-41, 2017.
* [65] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. _arXiv Preprint_, 2017.
* [66] B. Yang, I. Sato, and H. Nakagawa. Bayesian Differential Privacy on Correlated Data. In _Proceedings of the 2015 ACM SIGMOD international conference on Management of Data_, pages 747-762, 2015.
* [67] J. Ye, A. Maddi, S. K. Murakonda, V. Bindschaedler, and R. Shokri. Enhanced Membership Inference Attacks against Machine Learning Models. _arXiv preprint arXiv:2111.09679_, 2021.
* [68] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha. Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting. In _2018 IEEE 31st computer security foundations symposium (CSF)_, pages 268-282. IEEE, 2018.
* [69] S. Zanella-Beguelin, L. Wutschitz, S. Tople, A. Salem, V. Ruhle, A. Paverd, M. Naseri, and B. Kopf. Bayesian Estimation of Differential Privacy. _arXiv preprint arXiv:2206.05199_, 2022.

## Appendix

### Table of Contents

* 1 Related Work
	* 1.1 Auditing Private Machine Learning with Strong Canaries
	* 1.2 Improving Statistical Trade-offs in Auditing
	* 1.3 Connections to Other Notions of Differential Privacy
* 2 Properties of Lifted DP and Further Details
	* 2.1 Equivalence Between DP and LiDAR
	* 2.2 Auditing LiDAR with Different Notions of Neighborhood
* 3 Confidence Intervals for Exchangeable Bernoulli Means
	* 3.1 Non-Asymptotic Confidence Intervals
	* 3.2 Asymptotic Confidence Intervals
	* 3.3 Scaling of Higher-Order Bernstein Bounds (Proposition 4)
* 4 Canary Design for Lifted DP: Details
	* 4.1 Data Poisoning
	* 4.2 Random Gradients
* 5 Simulations with the Gaussian Mechanism: Details and More Results
	* 5.1 Experiment Setup
	* 5.2 Additional Experimental Results
* 6 Experiments: Details and More Results
	* 6.1 Training Details: Datasets, Models
	* 6.2 DP and Auditing Setup
	* 6.3 Miscellaneous Details
	* 6.4 Additional Experimental Results
Related Work

Prior to [17], privacy auditing required some access to the description of the mechanism. [5; 62; 7; 24; 43; 50; 51; 63] provide platforms with a specific set of functions to be used to implement the mechanism, where the end-to-end privacy of the source code can be automatically verified. Closest to our setting is the work of [18], where statistical testing was first proposed for privacy auditing, given access to an oracle that returns the exact probability measure of the mechanism. However, the guarantee is only for a relaxed notion of DP from [8] and the run-time depends super-linearly on the size of the output domain.

The pioneering work of [17] was the first to propose practical methods to audit privacy claims given a black-box access to a mechanism. The focus was on simple queries that do not involve any training of models. This is motivated by [16; 40], where even simple mechanisms falsely reported privacy guarantees. For such mechanisms, sampling a large number, say \(500,000\) in [17], of outputs is computationally easy, and no effort was made in [17] to improve the statistical trade-off. However, for private learning algorithms, training such a large number of models is computationally prohibitive. The main focus of our framework is to improve this statistical trade-off for auditing privacy. We first survey recent breakthroughs in designing stronger canaries, which is orthogonal to our main focus.

### Auditing Private Machine Learning with Strong Canaries

Recent breakthroughs in auditing are accelerated by advances in privacy attacks, in particular membership inference. An attacker performing membership inference would like to determine if a particular data sample was part of the training set. Early work on membership inference [11; 23; 30; 52] considered algorithms for statistical methods, and more recent work demonstrates black-box attacks on ML algorithms [14; 33; 53; 67; 68]. [33] compares different notions of privacy by measuring privacy empirically for the composition of differential privacy [35], concentrated differential privacy [10; 21], and Renyi differential privacy [44]. This is motivated by [49], which compares different DP mechanisms by measuring the success rates of membership inference attacks. [31] attempts to measure the intrinsic privacy of stochastic gradient descent (without additional noise as in DP-SGD) using canary designs from membership inference attacks. Membership inference attacks have been shown to have higher success when the adversary can poison the training dataset [59].

A more devastating privacy attack is the extraction or reconstruction of the training data, which is particularly relevant for generative models such as large language models (LLMs). Several papers showed that LLMs tend to memorize their training data [13; 15], allowing an adversary to prompt the generative models and extract samples of the training set. The connection between the ability of an attacker to perform training data reconstruction and DP guarantees has been shown in recent work [26; 29].

When performing privacy auditing, a stronger canary design increases the success of the adversary in the distinguishing test and improves the empirical privacy bounds. The resulting hypothesis test can tolerate larger confidence intervals and requires less number of samples. Recent advances in privacy auditing have focused on designing such stronger canaries. [32] designs data poisoning canaries, in the direction of the lowest variance of the training data. This makes the canary out of distribution, making it easier to detect. [45] proposes attack surfaces of varying capabilities. For example, a gradient attack canary returns a gradient of choice when accessed by DP-SGD. It is shown that, with more powerful attacks, the canaries become stronger and the lower bounds become higher. [60] proposes using an example from the baseline training dataset, after changing the label, and introduces a search procedure to find a strong canary. More recently, [46] proposes a significantly improved auditing scheme for DP-SGD under a white-box access model where \((i)\) the auditor knows that the underlying mechanism is DP-SGD with a spherical Gaussian noise with unknown variance, and \((ii)\) all intermediate models are revealed. Since each coordinate of the model update provides an independent sample from the same Gaussian distribution, sample complexity is dramatically improved. [41] proposes CANIFE, a novel canary design method that finds a strong data poisoning canary adaptively under the federated learning scenario.

Prior work in this space shows that privacy auditing can be performed via privacy attacks, such as membership inference or reconstruction, and strong canary design results in better empirical privacy bounds. We emphasize that our aim is not to innovate on optimizing canary design for performingprivacy auditing. Instead, our framework can seamlessly adopt recently designed canaries and inherit their strengths as demonstrated in SS5 and SS6.

### Improving Statistical Trade-offs in Auditing

Eq. (2) points to two orthogonal directions that can potentially improve the sample dependence: designing stronger canaries and improving the sample dependence of the confidence intervals. The former was addressed in the previous section. There is relatively less work in improving the statistical dependence, which our framework focuses on.

Given a pair of neighboring datasets, \((D_{0},D_{1})\), and for a query with discrete output in a finite space, the statistical trade-off of estimating privacy parameters was studied in [25] where a plug-in estimator is shown to achieve an error of \(O(\sqrt{de^{2\varepsilon}/n})\), where \(d\) is the size of the discrete output space. [38] proposes a sub-linear sample complexity algorithm that achieves an error scaling as \(\sqrt{de^{\varepsilon}/n\log n}\), based on polynomial approximation of a carefully chosen degree to optimally trade-off bias and variance motivated by [27, 64]. Similarly, [37] provides a lower bound for auditing Renyi differential privacy. [9] trains a classifier for the binary hypothesis test and uses the classifier to design rejection sets. [3] proposes local search to find the rejection set efficiently. More recently, [69] proposes numerical integration over a larger space of false positive rate and true positive rate to achieve better sample complexity of the confidence region in the two-dimensional space. Our framework can be potentially applied to this confidence region scenario, which is an important future research direction.

### Connections to Other Notions of Differential Privacy

Similar to Lifted DP, a line of prior works [36, 55, 61, 66] generalizes DP to include a distributional assumption over the dataset. However, unlike Lifted DP, they are motivated by the observation that DP is not sufficient for preserving privacy when the samples are highly correlated. For example, upon releasing the number of people infected with a highly contagious flu within a tight-knit community, the usual Laplace mechanism (with sensitivity one) is not sufficient to conceal the likelihood of one member getting infected when the private count is significantly high. One could apply group differential privacy to hide the contagion of the entire population, but this will suffer from excessive noise. Ideally, we want to add a noise proportional to the _expected_ size of the infection. Pufferfish privacy, introduced in [36], achieves this with a generalization of DP that takes into account prior knowledge of a class of potential distributions over the dataset. A special case of \(\varepsilon\)-Pufferfish with a specific choice of parameters recovers a special case of our Lifted DP in Eq. (3) with \(\delta=0\)[36, Section 3.2], where a special case of Theorem 3 has been proven for pure DP [36, Theorem 3.1]. However, we want to emphasize that our Lifted DP is motivated by a completely different problem of auditing differential privacy and is critical to breaking the barriers in the sample complexity.

## Appendix B Properties of Lifted DP and Further Details

### Equivalence Between DP and LiDP

In contrast to the usual \((\varepsilon,\delta)\)-DP in Eq. (1), the probability in LiDP is over both the internal randomness of the algorithm \(\mathcal{A}\) and the distribution \(\mathcal{P}\) over the triplet \((\bm{D}_{0},\bm{D}_{1},\bm{R})\). Since we require that the definition holds only for lifted distributions \(\mathcal{P}\) that are independent of the algorithm \(\mathcal{A}\), it is easy to show its equivalence to the usual notion of \((\varepsilon,\delta)\)-DP.

**Theorem 3**.: _A randomized algorithm \(\mathcal{A}\) is \((\varepsilon,\delta)\)-LiDP iff \(\mathcal{A}\) is \((\varepsilon,\delta)\)-DP._

Proof.: Suppose \(\mathcal{A}\) is \((\varepsilon,\delta)\)-LiDP. Fix a pair of neighboring datasets \(D_{0},D_{1}\) and an outcome \(R\subset\mathcal{R}\). Define \(\mathcal{P}_{D_{0},D_{1},R}\) as the point mass on \((D_{0},D_{1},R)\), i.e.,

\[\text{d}\mathcal{P}_{D_{0},D_{1},R}(D_{0}^{\prime},D_{1}^{\prime},R^{\prime}) =\mathbbm{1}(D_{0}^{\prime}=D_{0},\,D_{1}^{\prime}=D_{1},\,R^{\prime}=R)\,,\]

so that \(\mathbb{P}_{\mathcal{A},\mathcal{P}_{D_{0},D_{1},R}}(\mathcal{A}(\bm{D}_{0}) \in\bm{R})=\mathbb{P}(\mathcal{A}(D_{0})\in R)\) and similarly for \(D_{1}\). Then, applying the definition of \((\varepsilon,\delta)\)-LiDP w.r.t. the distribution \(\mathcal{P}_{D_{0},D_{1},R}\) gives

\[\mathbb{P}_{\mathcal{A}}(\mathcal{A}(D_{1})\in R)\leq e^{\varepsilon}\, \mathbb{P}_{\mathcal{A}}(\mathcal{A}(D_{0})\in R)+\delta\,.\]

Since this holds for any neighboring datasets \(D_{0},D_{1}\) and outcome set \(R\), we get that \(\mathcal{A}\) is \((\varepsilon,\delta)\)-DP.

Conversely, suppose that \(\mathcal{A}\) is \((\varepsilon,\delta)\)-DP. For any distribution \(\mathcal{P}\) over pairs of neighboring datasets \(\bm{D}_{0},\bm{D}_{1}\) and outcome set \(\bm{R}\), we have by integrating the DP definition

\[\int\mathbb{P}(\mathcal{A}(D_{1})\in R)\,\mathrm{d}\mathcal{P}(D_{0},D_{1},R) \leq e^{\varepsilon}\int\mathbb{P}(\mathcal{A}(D_{0})\in R)\,\mathrm{d} \mathcal{P}(D_{0},D_{1},R)+\delta\,.\] (11)

Next, we use the law of iterated expectation to get

\[\mathbb{P}_{\mathcal{A},\mathcal{P}}\big{(}\mathcal{A}(\bm{D}_{0 })\in\bm{R}\big{)} =\mathbb{E}_{\mathcal{A},\mathcal{P}}\left[\mathbb{I}\big{(} \mathcal{A}(\bm{D}_{0})\in\bm{R}\big{)}\right]\] \[=\int\mathbb{E}_{\mathcal{A}}\left[\mathbb{I}\big{(}\mathcal{A}( D_{0})\in\bm{R}\big{)}\,\big{|}\bm{D}_{0}=D_{0},\bm{R}=R\right]\,\mathrm{d} \mathcal{P}(D_{0},D_{1},R)\] \[=\int\mathbb{P}_{\mathcal{A}}\big{(}\mathcal{A}(D_{0})\in R\big{)} \,\mathrm{d}\mathcal{P}(D_{0},D_{1},R)\,,\]

where \((*)\) followed from the independence of \(\mathcal{A}\) and \(\mathcal{P}\). Plugging this and the analogous expression for \(\mathbb{P}(\mathcal{A}(\bm{D}_{1})\in\bm{R})\) into (11) gives us that \(\mathcal{A}\) is \((\varepsilon,\delta)\)-LiDP. 

### Auditing LiDP with Different Notions of Neighborhood

We describe how to modify the recipe of SS3 for other notions of neighborhoods of datasets. The notion of the neighborhood in Definition 1 is also known as the "add-or-remove" neighborhood.

**Replace-one Neighborhood.** Two datasets \(D,D^{\prime}\in\mathcal{Z}^{*}\) are considered neighboring if \(|D|=|D^{\prime}|\) and \(|D\setminus D^{\prime}|=|D^{\prime}\setminus D|=1\). Roughly speaking, this leads to privacy guarantees that are roughly twice as strong as the add-or-remove notion of the neighborhood in Definition 1, as the worst-case sensitivity of the operation is doubled. We refer to [48, 63] for more details. Just like Definition 1, Definition 2 can also be adapted to this notion of neighborhood.

**Auditing LiDP with Replace-one Neighborhood.** The recipe of SS3.2 can be slightly modified for this notion of neighborhood. The main difference is that the null hypothesis must now use \(K\) canaries as well, with one fresh canary.

The alternative hypothesis is the same -- we train a model on a randomized training dataset \(\bm{D}_{1}=D\cup\{\bm{c}_{1},\dots,\bm{c}_{K}\}\) augmented with \(K\) random canaries drawn i.i.d. from \(\mathcal{P}_{\text{canary}}\). Under the \(j^{\text{th}}\) null hypothesis for each \(j\in[K]\), we construct a coupled dataset \(\bm{D}_{0,j}=D\cup\{\bm{c}_{1},\dots,\bm{c}_{j-1},\bm{c}^{\prime}_{j},\bm{c}_ {j+1},\dots,\bm{c}_{K}\}\), where \(\bm{c}^{\prime}_{j}\) is a fresh canary drawn i.i.d. from \(\mathcal{P}_{\text{canary}}\). This coupling ensures that \((\bm{D}_{0,j},\bm{D}_{1})\) are neighboring with probability one. We restrict the rejection region \(\bm{R}_{j}\) to now depend only on \(\bm{c}_{j}\) and not in the index \(j\), e.g., we test for the present of \(\bm{c}_{j}\).2

Footnote 2: Note that the test could have depended on \(\bm{c}^{\prime}_{j}\) as well, but we do not need it here.

Note the symmetry of the setup. Testing for the presence of \(\bm{c}_{j}\) in \(\bm{D}_{0,j}\) is exactly identical to testing for the presence of \(\bm{c}^{\prime}_{j}\) in \(\bm{D}_{1}\). Thus, we can again rewrite the LiDP condition as

\[\frac{1}{K}\sum_{k=1}^{K}\mathbb{P}(\mathcal{A}(\bm{D}_{1})\in\bm{R}_{k}) \leq \frac{e^{\varepsilon}}{m}\sum_{j=1}^{m}\mathbb{P}(\mathcal{A}(\bm{ D}_{1})\in\bm{R}^{\prime}_{j})+\delta\,.\] (12)

Note the subtle difference between (4) and (12): both sides depend only on \(\mathcal{A}(\bm{D}_{1})\) and we have completely eliminated the need to train models on \(K-1\) canaries.

From here on, the rest of the recipe is identical to SS3 and Algorithm 1; we construct XBern confidence intervals for both sides of (12) and get a lower bound \(\hat{\varepsilon}\).

## Appendix C Confidence Intervals for Exchangeable Bernoulli Means

We give a rigorous definition of the multivariate Exchangeable Bernoulli (XBern) distributions and derive their confidence intervals. We also give proof of the correctness of the confidence intervals.

**Definition 5** (XBern Distributions).: A random vector \((\bm{x}_{1},\dots,\bm{x}_{K})\in\{0,1\}^{K}\) is said to be distributed as XBern\({}_{K}(\mu_{1},\dots,\mu_{K})\) if:

* \(\bm{x}_{1},\dots,\bm{x}_{K}\) is exchangeable, i.e., the vector \((\bm{x}_{1},\dots,\bm{x}_{K})\) is identical in distribution to \((\bm{x}_{\pi(1)},\dots,\bm{x}_{\pi(K)})\) for any permutation \(\pi:[K]\to[K]\), and,
* for each \(\ell=1,\dots,K\), we have \(\mathbb{E}[\bm{m}_{\ell}]=\mu_{\ell}\), where \[\bm{m}_{\ell}:=\frac{1}{\binom{K}{\ell}}\sum_{j_{1}<\dots<j_{\ell} \in[K]}\bm{x}_{j_{1}}\cdots\bm{x}_{j_{\ell}}\,.\] (13)

We note that the XBern\({}_{K}\) distribution is fully determined by its \(K\) moments \(\mu_{1},\dots,\mu_{K}\). For \(K=1\), XBern\({}_{1}(\mu_{1})=\text{Bernoulli}(\mu_{1})\) is just the Bernoulli distribution.

The moments \(\bm{m}_{\ell}\) satisfy a computationally efficient recurrence.

**Proposition 6**.: _Let \(\bm{x}\sim\text{XBern}_{K}(\mu_{1},\dots,\mu_{K})\). We have the following for all \(1\leq\ell\leq K\bm{m}_{1}\):_

\[\bm{m}_{\ell}=\frac{\binom{K\bm{m}_{1}}{\ell}}{\binom{K}{\ell}} \qquad\text{and}\qquad\bm{m}_{\ell+1}=\bm{m}_{\ell}\left(\frac{K\bm{m}_{1}- \ell}{K-\ell}\right)\,.\] (14)

Computing the first \(\ell\) moments can thus be done in \(O(K+\ell)\) time rather than \(O(K^{\ell})\) by naively computing the sum in (13).

Proof of Proposition 6.: We show that this holds for any fixed vector \((x_{1},\dots,x_{K})\in\{0,1\}^{K}\) and their corresponding moments \(m_{1},\dots,m_{K}\) as defined in (13). Define the sums \(s_{1}:=\sum_{j=1}^{K}x_{j}=Km_{1}\) and

\[s_{\ell}:=\binom{K}{\ell}\sum_{j_{1}<\dots<j_{\ell}}x_{j_{1}} \cdots x_{j_{\ell}}\,.\] (15)

We can compute \(s_{\ell}\) by a counting argument: each term in the sum is non-zero only if all \(\ell\) of the \(x_{j_{1}},\dots,x_{j_{\ell}}\) are non-zero. The number of non-zero terms is equal to number of ways of selecting \(\ell\) items from a total of \(\sum_{j=1}^{K}x_{j}=s_{1}\) items, i.e.,

\[s_{\ell}=\binom{s_{1}}{\ell}=\binom{Km_{1}}{\ell}\,.\] (16)

Combining Equations (15) and (16) completes the proof. 

**Notation.** In this section, we are interested in giving confidence intervals on the mean \(\mu_{1}\) of a XBern\({}_{K}(\mu_{1},\dots,\mu_{K})\) random variable from \(n\) i.i.d. observations:

\[\bm{x}^{(1)},\dots,\bm{x}^{(n)}\stackrel{{\text{i.i.d.}}}{{\sim}} \text{XBern}_{K}(\mu_{1},\dots,\mu_{K})\,.\]

We will define the confidence intervals using the empirical mean

\[\hat{\bm{\mu}}_{1}=\frac{1}{n}\sum_{i=1}^{n}\bm{m}_{1}^{(i)}\quad \text{where}\quad\bm{m}_{1}^{(i)}=\frac{1}{K}\sum_{j=1}^{K}\bm{x}_{j}^{(i)}\,,\]

as well as the higher-order moments for \(\ell\in[K]\),

\[\hat{\bm{\mu}}_{\ell}=\frac{1}{n}\sum_{i=1}^{n}\bm{m}_{\ell}^{(i)} \quad\text{where}\quad\bm{m}_{\ell}^{(i)}=\frac{1}{\binom{K}{\ell}}\sum_{j_{1}< \dots<j_{\ell}\in[K]}\bm{x}_{j_{1}}^{(i)}\cdots\bm{x}_{j_{\ell}}^{(i)}\,.\]

### Non-Asymptotic Confidence Intervals

We start by giving non-asymptotic confidence intervals for the XBern distributions based on the Bernstein bound, recalled below [e.g. 20, Thm. 1.3.2].

**Lemma 7** (Bernstein inequality).: _Let \(\bm{x}_{1},\ldots,\bm{x}_{n}\) be independent random variables with \(\mathbb{E}[\bm{x}_{i}]=0\), \(\mathrm{Var}(\bm{x}_{i})\leq\sigma^{2}\), and \(|\bm{x}_{i}|\leq b\) almost surely. Let \(\bm{\mu}_{1}:=(1/n)\sum_{i=1}^{n}\bm{x}_{i}\). For any \(t>0\), we have_

\[\mathbb{P}(\bm{\mu}_{1}>t)\leq\exp\left(-\frac{nt^{2}}{2\sigma^{2}+2bt/3} \right)\,.\]

_Equivalently, we have with probability at least \(1-\beta\) that_

\[\bm{\mu}_{1}\leq\sqrt{\frac{2\sigma^{2}}{n}\log\frac{1}{\beta}+\frac{b^{2}}{9 n^{2}}\log^{2}\frac{1}{\beta}}+\frac{b}{3n}\log\frac{1}{\beta}\leq\sqrt{\frac{2 \sigma^{2}}{n}\log\frac{1}{\beta}}+\frac{2b}{3n}\log\frac{1}{\beta}\,.\]

The last simplification is obtained by using \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\) for scalars \(a,b\geq 0\).

#### c.1.1 First-Order Bernstein Intervals

The first-order Bernstein interval only depends on the empirical mean \(\bm{\mu}_{1}\) and is given in Algorithm 2.

**Proposition 8**.: _Consider Algorithm 2 with inputs \(n\) i.i.d. samples \(\bm{x}^{(1)},\ldots,\bm{x}^{(n)}\)\(\stackrel{{ i.i.d.}}{{\sim}}\)\(\underline{XBern}_{K}(\mu_{1},\ldots,\mu_{K})\) for some \(K\geq 1\). Then, its outputs \(\underline{\bm{\mu}}_{1},\overline{\bm{\mu}}_{1}\) satisfy \(\mathbb{P}(\mu_{1}\leq\underline{\bm{\mu}}_{1})\geq 1-\beta\) and \(\mathbb{P}(\mu_{1}\geq\overline{\bm{\mu}}_{1})\geq 1-\beta\)._

Proof.: Applying Bernstein's inequality to \(\bm{m}_{1}:=(1/K)\sum_{j=1}^{K}\bm{x}_{j}\), we have with probability \(1-\beta\) that

\[\mu_{1}-\hat{\bm{\mu}}_{1}\leq\sqrt{\frac{2\mathrm{Var}(\bm{m}_{1})}{n}\log \frac{1}{\beta}}+\frac{2}{3n}\log\frac{1}{\beta}\leq\sqrt{\frac{2\mu_{1}(1- \mu_{1})}{n}\log\frac{1}{\beta}}+\frac{2}{3n}\log\frac{1}{\beta}\,,\]

where we used that \(\mathrm{Var}(\bm{m}_{1})\leq\mu_{1}(1-\mu_{1})\) since \(\bm{m}_{1}\in[0,1]\) a.s. We see from Figure 2 that \(\overline{\bm{\mu}}_{1}\) is that largest value of \(\mu_{1}\) that satisfies the above inequality, showing that it is an upper confidence bound. Similarly, we get that \(\underline{\bm{\mu}}_{1}\) is a valid lower confidence bound.

#### c.1.2 Second-Order Bernstein Intervals

The second-order Bernstein interval only depends on the first two empirical moments \(\bm{\mu}_{1}\) and \(\bm{\mu}_{2}\). It is given in Algorithm 3. The algorithm is based on the calculation

\[\begin{split}\operatorname{Var}(\bm{m}_{1})&=\mathbb{ E}[\bm{m}_{1}^{2}]-\mu_{1}^{2}\\ &=\mathbb{E}\left[\frac{1}{K^{2}}\sum_{j=1}^{K}\bm{x}_{j}^{2}+ \frac{2}{K^{2}}\sum_{j_{1}<j_{2}\in[K]}\bm{x}_{j_{1}}\bm{x}_{j_{2}}\right]-\mu _{1}^{2}\\ &=\frac{\mu_{1}}{K}-\mu_{1}^{2}+\frac{K-1}{K}\mu_{2}\,,\end{split}\] (17)

where we used \(\bm{x}_{j}^{2}=\bm{x}_{j}\) since it is an indicator.

**Proposition 9**.: _Consider Algorithm 3 with inputs \(n\) i.i.d. samples \(\bm{x}^{(1)},\ldots,\bm{x}^{(n)}\stackrel{{ iid.}}{{\sim}}\text{ XBern}_{K}(\mu_{1},\ldots,\mu_{K})\) for some \(K\geq 2\). Then, its outputs \(\underline{\bm{\mu}}_{1},\overline{\bm{\mu}}_{1}\) satisfy \(\mathbb{P}(\mu_{1}\geq\underline{\bm{\mu}}_{1})\geq 1-\beta\) and \(\mathbb{P}(\mu_{1}\leq\overline{\bm{\mu}}_{1})\geq 1-\beta\) and \(\mathbb{P}(\underline{\bm{\mu}}_{1}\leq\mu_{1}\leq\overline{\bm{\mu}}_{1}) \leq 1-\frac{3\beta}{2}\)._

Proof.: Algorithm 3 computes the correct 2nd moment \(\bm{m}_{2}^{(i)}\) due to Proposition 6. Applying Bernstein's inequality to \(\bm{m}_{2}\), we get \(\mathbb{P}(\mu_{2}\leq\overline{\bm{\mu}}_{2})\geq 1-\beta/2\) (see also the proof of Proposition 8). Next, from Bernstein's inequality applied to \(\bm{m}_{1}\), we leverage (17) to say that with probability at least \(1-\beta/2\), we have

\[\mu_{1}-\hat{\bm{\mu}}_{1}\leq\frac{2}{3n}\log\frac{2}{\beta}+\sqrt{\frac{2}{ n}\log\frac{2}{\beta}\left(\frac{\mu_{1}}{K}-\mu_{1}^{2}+\frac{K-1}{K}\mu_{2} \right)}\,.\]

Together with the result on \(\overline{\bm{\mu}}_{2}\), we have with probability at least \(1-\beta\) that

\[\mu_{1}-\hat{\bm{\mu}}_{1}\leq\frac{2}{3n}\log\frac{2}{\beta}+\sqrt{\frac{2} {n}\log\frac{2}{\beta}\left(\frac{\mu_{1}}{K}-\mu_{1}^{2}+\frac{K-1}{K} \overline{\bm{\mu}}_{2}\right)}\,.\]We can verify that the output \(\overline{\bm{\mu}}_{1}\) is the largest value of \(\mu_{1}\leq 1\) that satisfies the above inequality. Similarly, \(\underline{\bm{\mu}}_{1}\) is obtained as a lower Bernstein bound on \(\bm{m}_{1}\) with probability at least \(1-\beta\). By the union bound, \(\underline{\bm{\mu}}_{1}\leq\mu_{1}\leq\overline{\bm{\mu}}_{1}\) holds with probability at least \(1-3\beta/2\), since we have three invocations of Bernstein's inequality, each with a failure probability of \(\beta/2\). 

#### c.1.3 Fourth-Order Bernstein Intervals

The fourth-order Bernstein interval depends on the first four empirical moments \(\bm{\mu}_{1},\dots,\bm{\mu}_{4}\). It is given in Algorithm 4. The derivation of the interval is based on the calculation

\[\begin{split}\operatorname{Var}(\bm{m}_{2})&= \mathbb{E}[\bm{m}_{2}^{2}]-\mu_{2}^{2}\\ &=\frac{2\mu_{2}}{K(K-1)}-\mu_{2}^{2}+\frac{4(K-2)}{K(K-1)}\mu_{ 3}+\frac{(K-2)(K-3)}{K(K-1)}\mu_{4}\\ &=\frac{2\mu_{2}(1-\mu_{2})}{K(K-1)}+\frac{4(K-2)}{K(K-1)}(\mu_{ 3}-\mu_{2}^{2})+\frac{(K-2)(K-3)}{K(K-1)}(\mu_{4}-\mu_{2}^{2})\\ &=:\sigma_{2}^{2}(\mu_{2},\mu_{3},\mu_{4})\,.\end{split}\] (18)

**Proposition 10**.: _Consider Algorithm 4 with inputs \(n\) i.i.d. samples \(\bm{x}^{(1)},\dots,\bm{x}^{(n)}\stackrel{{ i.i.d.}}{{\sim}} \text{XBern}_{K}(\mu_{1},\dots,\mu_{K})\) for some \(K\geq 4\). Then, its outputs \(\bm{\mu}_{1},\overline{\bm{\mu}}_{1}\) satisfy \(\mathbb{P}(\mu_{1}\geq\underline{\bm{\mu}}_{1})\geq 1-\beta\) and \(\mathbb{P}(\mu_{1}\leq\overline{\bm{\mu}}_{1})\geq 1-\beta\) and \(\mathbb{P}(\underline{\bm{\mu}}_{1}\leq\mu_{1}\leq\overline{\bm{\mu}}_{1}) \leq 1-\frac{5\beta}{4}\)._Proof.: Algorithm 4 computes the correct moments \(\bm{m}_{\ell}^{(i)}\) for \(\ell\leq 4\) due to Proposition 6. Applying Bernstein's inequality to \(\bm{m}_{3}\) and \(\bm{m}_{4}\), we get \(\mathbb{P}(\mu_{\ell}\leq\overline{\bm{\mu}}_{\ell})\geq 1-\beta/4\) for \(\ell=3,4\) (see also the proof of Proposition 8).

Next, from Bernstein's inequality applied to \(\bm{m}_{2}\), we have with probability at least \(1-\beta/4\) that

\[\mu_{2}-\hat{\bm{\mu}}_{2}\leq\frac{2}{3n}\log\frac{4}{\beta}+\sqrt{\frac{2 \sigma_{2}^{2}\left(\mu_{2},\mu_{3},\mu_{4}\right)}{n}\log\frac{4}{\beta}}\,.\]

Combining this with the results on \(\overline{\bm{\mu}}_{3},\overline{\bm{\mu}}_{4}\) with the union bound, we get with probability at least \(1-3\beta/4\) that

\[\mu_{2}-\hat{\bm{\mu}}_{2}\leq\frac{2}{3n}\log\frac{4}{\beta}+\sqrt{\frac{2 \sigma_{2}^{2}\left(\mu_{2},\overline{\bm{\mu}}_{3},\overline{\bm{\mu}}_{4} \right)}{n}\log\frac{4}{\beta}}\,.\]

Finally, plugging this into a Bernstein bound on \(\bm{m}_{1}\) using the variance calculation from (17) (also see the proof of Proposition 9) completes the proof. 

### Asymptotic Confidence Intervals

We derive asymptotic versions of the Algorithms 2 to 4 using the Wilson confidence interval.

The Wilson confidence interval is a tightening of the constants for the Bernstein confidence interval

\[\mu_{1}-\hat{\bm{\mu}}_{1}\leq\sqrt{\frac{2\log(1/\beta)}{n}\operatorname{ Var}(\bm{m}_{1})}+\frac{2}{3n}\log\frac{1}{\beta}\quad\text{to}\quad\mu_{1}- \hat{\bm{\mu}}_{1}\leq\sqrt{\frac{Z_{\beta}^{2}}{n}\operatorname{Var}(\bm{m} _{1})}\,,\]

where \(Z_{\beta}\) is the \((1-\beta)\)-quantile of the standard Gaussian. Essentially, this completely eliminates the \(1/n\) term, while the coefficient of the \(1/\sqrt{n}\) term improves from \(\sqrt{2\log(1/\beta)}\) to \(Z_{\beta}\) -- see Figure 2.

The Wilson approximation holds under the assumption that \((\mu_{1}-\hat{\bm{\mu}}_{1})/\sqrt{\operatorname{Var}(\bm{m}_{1})/n}\stackrel{{ \mathrm{d}}}{{\approx}}\mathcal{N}(0,1)\) and using a Gaussian confidence interval. This can be formalized by the central limit theorem.

**Lemma 11** (Lindeberg-Levy Central Limit Theorem).: _Consider a sequence of independent random variables \(\bm{y}^{(1)},\bm{y}^{(2)},\ldots\) with finite moments \(\mathbb{E}[\bm{y}^{(i)}]=\mu<\infty\) and \(\mathbb{E}(\bm{y}^{(i)}-\mu)^{2}=\sigma^{2}<\infty\) for each \(i\). Then, the empirical mean \(\hat{\bm{\mu}}_{1}^{(n)}=(1/n)\sum_{i=1}^{n}\bm{y}^{(i)}\) based on \(n\) samples satisfies_

\[\lim_{n\to\infty}\mathbb{P}\left(\frac{\hat{\bm{\mu}}_{1}^{(n)}-\mu}{\sigma/ \sqrt{n}}>t\right)=\mathbb{P}_{\bm{\xi}\sim\mathcal{N}(0,1)}\left(\bm{\xi}>t\right)\]

_for all \(t\in\mathbb{R}\). Consequently, we have,_

\[\lim_{n\to\infty}\mathbb{P}\left(\mu-\hat{\bm{\mu}}_{1}^{(n)}> \sigma Z_{\beta}/\sqrt{n}\right)\geq 1-\beta\quad\text{and}\quad\lim_{n\to\infty} \mathbb{P}\left(\hat{\bm{\mu}}_{1}^{(n)}-\mu>\sigma Z_{\beta}/\sqrt{n}\right) \geq 1-\beta\,.\] (19)

The finite moment requirement above is satisfied in our case because all our random variables are bounded between \(0\) and \(1\). The asymptotic confidence interval implied by (19) is known as the **Wilson confidence interval**. It states that

\[|\mu-\hat{\bm{\mu}}_{1}^{(n)}|\leq\frac{\sigma Z_{\beta/2}}{\sqrt{n}}\,.\]

w.p. \(1-\beta\) as \(n\to\infty\).

We give the Wilson-variants of Algorithms 2 to 4 respectively in Algorithms 5 to 7. Apart from the fact that the Wilson intervals are tighter, we can also solve the equations associated with the Wilson intervals in closed form as they are simply quadratic equations (i.e., without the need for numerical root-finding). The following proposition shows their correctness.

**Proposition 12**.: _Consider \(n\) i.i.d. samples \(\bm{x}^{(1)},\ldots,\bm{x}^{(n)}\stackrel{{ i.i.d.}}{{\sim}}X\text{Bern} _{K}(\mu_{1},\ldots,\mu_{K})\) as inputs to Algorithms 5 to 7. Then, their outputs \(\underline{\bm{\mu}}_{1},\overline{\bm{\mu}}_{1}\) satisfy \(\lim_{n\to\infty}\mathbb{P}(\mu_{1}\leq\underline{\bm{\mu}}_{1})\geq 1-\beta\) and \(\lim_{n\to\infty}\mathbb{P}(\mu_{1}\geq\overline{\bm{\mu}}_{1})\geq 1-\beta\) and \(\lim_{n\to\infty}\mathbb{P}(\underline{\bm{\mu}}_{1}\leq\mu_{1}\leq\overline {\bm{\mu}}_{1})\leq 1-C\beta\) if_1. \(K\geq 1\) _and_ \(C=2\) _for Algorithm_ 5_,_
2. \(K\geq 2\) _and_ \(C=3/2\) _for Algorithm_ 6_, and_
3. \(K\geq 4\) _and_ \(C=5/4\) _for Algorithm_ 7_._

We omit the proof as it is identical to those of Propositions 8 to 10 except that it uses the Wilson interval from Lemma 11 rather than the Bernstein interval.

### Scaling of Higher-Order Bernstein Bounds (Proposition 4)

We now re-state and prove Proposition 4.

**Proposition 4**.: _For any positive integer \(\ell\) that is a power of two and \(K=\lceil n^{(\ell-1)/\ell}\rceil\), suppose we have \(n\) samples from a \(K\)-dimensional XBern distribution with parameters \((\mu_{1},\ldots,\mu_{K})\). If all \(\ell^{\prime\text{th}}\)-order correlations scale as \(1/K\), i.e., \(|\mu_{2\ell^{\prime}}-\mu_{\ell^{\prime}}^{2}|=O(1/K)\), for all \(\ell^{\prime}\leq\ell\) and \(\ell^{\prime}\) is a power of two, then the \(\ell^{\text{th}}\)-order Bernstein bound is \(|\mu_{1}-\hat{\boldsymbol{\mu}}_{1}|=O(1/n^{(2\ell-1)/(2\ell)})\)._

Proof.: We are given \(n\) samples from an XBern distribution \(\boldsymbol{x}\in\{0,1\}^{K}\) with parameters \((\mu_{1},\ldots,\mu_{K})\), where \(\mu_{\ell}:=\mathbb{E}[\boldsymbol{m}_{\ell}]\) with

\[\boldsymbol{m}_{\ell} := \frac{1}{K(K-1)\cdots(K-\ell+1)}\sum_{j_{1}<j_{2}<\ldots<j_{\ell} \in[K]}\boldsymbol{x}_{j_{1}}\cdots\boldsymbol{x}_{j_{\ell}}\;.\]

By exchangeability, it also holds that \(\mu_{\ell}=\mathbb{E}[\boldsymbol{x}_{1}\cdots\boldsymbol{x}_{\ell}]\). Assuming a confidence level \(1-\beta<1\) and \(K=\lceil n^{(\ell-1)/\ell}\rceil\), the \(1\)st-order Bernstein bound gives

\[|\mu_{1}-\hat{\boldsymbol{\mu}}_{1}| = O\left(\sqrt{\frac{\sigma_{1}^{2}}{n}}\right)\;,\] (20)

where \(\sigma_{\ell}^{2}:=\mathrm{Var}(\boldsymbol{m}_{\ell})\). Expanding \(\sigma_{\ell}^{2}\), it is easy to show that it is dominated by the \(2\ell\)th-order correlation \(|\mu_{2\ell}-\mu_{\ell}^{2}|\):

\[\sigma_{\ell}^{2} = O\left(\frac{1}{K}+|\mu_{2\ell}-\mu_{\ell}^{2}|\right)\;\;=\; \;O\left(\frac{1}{K}+|\hat{\boldsymbol{\mu}}_{2\ell}-\hat{\boldsymbol{\mu}}_ {\ell}^{2}|+\sqrt{\frac{\sigma_{2\ell}^{2}}{n}}\right)\;.\]

Note that our Bernstein confidence interval does not use the fact that the higher-order correlations are small. We only use that assumption to bound the resulting size of the confidence interval in the analysis. Applying the assumption that all the higher order correlations are bounded by \(1/K\), i.e., \(|\mu_{2\ell^{\prime}}-\mu_{\ell^{\prime}}^{2}|=O(1/K)\), we get that \(|\hat{\boldsymbol{\mu}}_{2\ell^{\prime}}-\hat{\boldsymbol{\mu}}_{\ell^{\prime} }^{2}|=O(1/K+\sqrt{\sigma_{2\ell^{\prime}}^{2}/n})\). Applying this recursively into (20), we get that

\[|\mu_{1}-\hat{\boldsymbol{\mu}}_{1}| = O\left(\sqrt{\frac{1}{nK}}+\frac{\sigma_{\ell}^{1/\ell}}{n^{(2 \ell-1)/(2\ell)}}\right)\;,\]

for any \(\ell\) that is a power of two. For an \(\ell^{\text{th}}\)-order Bernstein bound, we only use moment estimates up to \(\ell\) and bound \(\sigma_{\ell}^{2}\leq 1\). The choice of \(K=n^{(\ell-1)/\ell}\) gives the desired bound: \(|\mu_{1}-\hat{\boldsymbol{\mu}}_{1}|=O(1/n^{(2\ell-1)/(2\ell)})\). 

## Appendix D Canary Design for Lifted DP: Details

The canary design employed in the auditing of the usual \((\varepsilon,\delta)\)-DP can be easily extended to create distributions over canaries to audit LiDP. We give some examples for common classes of canaries.

**Setup.** We assume a supervised learning setting with a training dataset \(D_{\text{train}}=\{(x_{i},y_{i})\}_{i=1}^{N}\) and a held-out dataset \(D_{\text{val}}=\{(x_{i},y_{i})\}_{i=N+1}^{N+N^{\prime}}\) of pairs of input \(x_{i}\in\mathcal{X}\) and output \(y_{i}\in\mathcal{Y}\). We then aim to minimize the average loss

\[F(\theta)=\frac{1}{N}\sum_{i=1}^{N}L((x_{i},y_{i}),\theta)\,,\] (21)where \(L(z,\theta)\) is the loss incurred by model \(\theta\) on input-output pair \(z=(x,y)\).

In the presence of canaries \(c_{1},\ldots,c_{k}\), we instead aim to minimize the objective

\[F_{\text{canary}}(\theta;c_{1},\ldots,c_{k})=\frac{1}{N}\left(\sum_{i=1}^{N}L((x _{i},y_{i}),\theta)+\sum_{j=1}^{k}L_{\text{canary}}(c_{j},\theta)\right)\,,\] (22)

where \(L_{\text{canary}}(c,\theta)\) is the loss function for a canary \(c\) -- this may or may not coincide with the usual loss \(L\).

**Goals of Auditing DP.** The usual practice is to set \(D_{0}=D_{\text{train}}\) and \(D_{1}=D_{0}\cup\{c\}\) and \(R\equiv R_{c}\) for a canary \(c\). Recall from the definition of \((\varepsilon,\delta)\)-DP in (1), we have

\[\varepsilon\geq\sup_{c\in\mathcal{C}}\log\left(\frac{\mathbb{P}(\mathcal{A}( D_{1})\in R_{c})-\delta}{\mathbb{P}(\mathcal{A}(D_{0})\in R_{c})}\right)\,,\]

for some class \(\mathcal{C}\) of canaries. The goal then is to find canaries \(c\) that approximate the sup over \(\mathcal{C}\). Since this goal is hard, one usually resorts to finding canaries whose effect can be "easy to detect" in some sense.

**Goals of Auditing LiDP.** Let \(P_{\text{canary}}\) denote a probability distribution over a set \(\mathcal{C}\) of allowed canaries. We sample \(K\) canaries \(\bm{c}_{1},\ldots,\bm{c}_{K}\stackrel{{\text{i.i.d.}}}{{\sim}}P_ {\text{canary}}\) and set

\[\bm{D}_{0}=D_{\text{train}}\cup\{\bm{c}_{1},\ldots,\bm{c}_{K-1}\},\quad\bm{D}_ {1}=D_{\text{train}}\cup\{\bm{c}_{1},\ldots,\bm{c}_{K}\},\quad\bm{R}=R_{\bm{c }_{K}}\,.\]

From the definition of \((\varepsilon,\delta)\)-LiDP in (3), we have

\[\varepsilon\geq\sup_{P_{\text{canary}}}\log\left(\frac{\mathbb{P}(\mathcal{A} (\bm{D}_{1})\in\bm{R})-\delta}{\mathbb{P}(\mathcal{A}(\bm{D}_{0})\in\bm{R})} \right)\,,\]

for some class \(\mathcal{C}\) of canaries. The goal then is to approximate the distribution \(P_{\text{canary}}\) for each choice of the canary set \(\mathcal{C}\). Since this is hard, we will attempt to define a distribution over canaries that are easy to detect (similar to the case of auditing DP). Following the discussion in SS3.3, auditing LiDP benefits the most when the canaries are uncorrelated. To this end, we will also impose the restriction that a canary \(\bm{c}\sim P_{\text{canary}}\), if included in the training of a model \(\theta\), is unlikely to change the membership of \(\theta\in R_{\bm{c}^{\prime}}\) for an i.i.d. canary \(\bm{c}^{\prime}\sim P_{\text{canary}}\) that is independent of \(\bm{c}\).

We consider two choices of the canary set \(\mathcal{C}\) (as well as the outcome set \(R_{c}\) and the loss \(L_{\text{canary}}\)): data poisoning, and random gradients.

### Data Poisoning

We describe the data poisoning approach known as ClipBKD [32] that is based on using the tail singular vectors of the input data matrix and its extension to auditing LiDP.

Let \(X=(x_{1}^{\top};\cdots;x_{N}^{\top})\in\mathbb{R}^{N\times d}\) denote the matrix with the datapoints \(x_{i}\in\mathbb{R}^{d}\) as rows. Let \(X=\sum_{i=1}^{\min\{N,d\}}\sigma_{i}u_{i}v_{i}^{\top}\) be the singular value decomposition of \(X\) with \(\sigma_{1}\leq\sigma_{2}\leq\cdots\) be the singular values arranged in ascending order. Let \(Y\) denote the set of allowed labels.

For this section, we take the set of allowed canaries \(\mathcal{C}=\{\alpha v_{1},\alpha v_{2},\ldots,\alpha v_{\min\{N,d\}}\}\times \mathcal{Y}\) as the set of the right singular vector of \(X\) scaled by a given factor \(\alpha>0\) together with any possible target from \(\mathcal{Y}\). We take \(L_{\text{canary}}(c,\theta)=L(c,\theta)\) to be the usual loss function, and the output set \(R_{c}\) to be the loss-thresholded set

\[R_{c}:=\left\{\theta\in\mathcal{R}\,:\,L(c,\theta)\leq\tau\right\},\] (23)

for some threshold \(\tau\).

**Auditing DP.** The ClipBKD approach [32] uses a canary with input \(\alpha v_{1}\), the singular vector corresponding to the smallest singular value, scaled by a parameter \(\alpha>0\). The label is taken as \(y^{\star}(\alpha v_{1})\), where

\[y^{\star}(x)=\operatorname*{arg\,max}_{y\in\mathcal{Y}}L((x,y),\theta_{0}^{ \star})\]

is the target that has the highest loss on input \(x\) under the empirical risk minimizer \(\theta_{0}^{\star}=\operatorname*{arg\,min}_{\theta\in\mathcal{R}}F(\theta)\). Since a unique \(\theta_{0}^{\star}\) is not guaranteed for deep nets nor can we find it exactly,we train 100 models with different random seeds and pick the class \(y\) that yields the highest average loss over these runs.

**Auditing LiDP.** We extend ClipBKD to define a probability distribution over a given number \(p\) of canaries. We take

\[P_{\text{canary}}=\text{Uniform}\big{(}\{c_{1},\dots,c_{p}\}\big{)}\quad\text{ with}\quad c_{j}=\big{(}\alpha v_{j},y^{\star}(\alpha v_{j})\big{)}\,,\] (24)

i.e., \(P_{\text{canary}}\) is the uniform distribution over the \(p\) singular vectors corresponding to the smallest singular values.

### Random Gradients

The update poisoning approach of [2] relies of supplying gradients \(c\sim\text{Uniform}(B_{\mathcal{R}}(0,r))\) that are uniform on the Euclidean ball of a given radius \(r\). This is achieved by setting the loss of the canary as

\[L_{\text{canary}}(c,\theta)=\langle c,\theta\rangle,\quad\text{so that}\quad \nabla_{\theta}L_{\text{canary}}(c,\theta)=c\,,\]

is the desired vector \(c\).

The set \(R_{c}\) is a threshold of the dot product

\[R_{c}=\{\theta\in\mathcal{R}\,:\,\langle c,\theta\rangle\leq\tau\}\] (25)

for a given threshold \(\tau\). This set is analogous to the loss-based thresholding of (23) in that both can be written as \(L_{\text{canary}}(c,\theta)\leq\tau\).

**Auditing DP and LiDP.** The random gradient approach of [2] relies on defining a distribution \(P_{\text{canary}}\equiv\text{Uniform}(B_{\mathcal{R}}(0,r))\) over canaries. It can be used directly to audit LiDP.

## Appendix E Simulations with the Gaussian Mechanism: Details and More Results

Here, we give the full details and additional results of auditing the Gaussian mechanism with synthetic data in SS4.

### Experiment Setup

Fix a dimension \(d\) and a failure probability \(\beta\in(0,1)\). Suppose we have a randomized algorithm \(\mathcal{A}\) that returns a noisy sum of its inputs with the goal of simulating the Gaussian mechanism. Concretely, the input space \(\mathcal{Z}=\{z\in\mathbb{R}^{d}\,:\,\|z\|_{2}\leq 1\}\) is the unit ball in \(\mathbb{R}^{d}\). Given a finite set \(D\in\mathcal{Z}^{*}\), we sample a vector \(\boldsymbol{\xi}\sim\mathcal{N}(0,\sigma^{2}\boldsymbol{I}_{d})\) of a given variance \(\sigma^{2}\) and return

\[\mathcal{A}(D)=\boldsymbol{\xi}+\sum_{z\in D}z\,.\]

Figure 7: Comparing the Binomial proportion confidence intervals. We sample \(m\sim\text{Binomial}(n,p)\) for \(p=0.1\) and \(n\) varying and find the \(95\%\) confidence interval \([\underline{\boldsymbol{p}}_{n},\overline{\boldsymbol{p}}_{n}]\). We plot the widths \(p-\underline{\boldsymbol{p}}_{n}\) and \(\overline{\boldsymbol{p}}_{n}-p\) versus \(n\). We find that all confidence intervals are nearly equivalent once \(n\) is larger than \(\approx 1/\min\{p,1-p\}^{2}\).

To isolate the effect of the canaries, we set our original dataset \(D=\{\mathbf{0}_{d}\}\) as a singleton with the vector of zeros in \(\mathbb{R}^{d}\). Since we are in the blackbox setting, we do not assume that this is known to the auditor.

**DP Upper Bound.** The non-private version of our function computes the sum \(D\mapsto\sum_{z\in D}z\). where each \(z\in D\) is a canary. Hence, the \(\ell_{2}\) sensitivity of the operation is \(\Delta_{2}=\max_{x\in D}\|x\|_{2}=1\), as stated in SS4.

Since we add \(\boldsymbol{\xi}\sim\mathcal{N}(0,\sigma^{2}I_{d})\), it follows that the operation \(\mathcal{A}(\cdot)\) is \(\big{(}\alpha,\alpha/(2\sigma^{2})\big{)}\)-RDP for every \(\alpha>1\). Thus, \(\mathcal{A}(\cdot)\) is \((\varepsilon_{\delta},\delta)\)-DP where

\[\varepsilon_{\delta}\hskip-2.845276pt\leq\inf_{\alpha>1}\left\{\frac{\alpha}{ 2\sigma^{2}}+\frac{1}{\alpha-1}\log\frac{1}{\alpha\delta}+\log\left(1-\frac{1 }{\alpha}\right)\right\}\,,\]

based on [4, Thm. 21]. This can be shown to be bounded above by \(\frac{1}{\sigma}\sqrt{2\log\frac{1}{\delta}}+\frac{1}{2\sigma^{2}}\)[44, Prop. 3]. By Theorem 3, it follows that the operation \(\mathcal{A}(\cdot)\) is also \((\varepsilon_{\delta},\delta)\)-LiDP.

**Auditing LiDP.** We follow the recipe of Algorithm 1. We set the rejection region \(\boldsymbol{R}=R_{\tau}(\boldsymbol{c}_{K})\) as a function of the canary \(\boldsymbol{c}_{K}\) that differs between \(\boldsymbol{D}_{0}\) and \(\boldsymbol{D}_{1}\), where

\[R_{\tau}(\boldsymbol{c}_{j}):=\left\{u\in\mathbb{R}^{d}\,:\,\langle u, \boldsymbol{c}_{j}\rangle\geq\tau\right\}\,,\] (26)

and \(\tau\in\mathbb{R}\) is a tuned threshold.

Figure 8: Effect of the number \(n\) of trials on the empirical lower bound \(\hat{\varepsilon}\) from auditing the Gaussian mechanism for DP and LiDP. The shaded are denotes the standard error over 25 random seeds.

We evaluate empirical privacy auditing methods by how large the lower bound \(\hat{\varepsilon}\) is -- the higher the lower bound, the better the confidence interval.

**Methods Compared.** An empirical privacy auditing method is defined by the type of privacy auditing (DP or LiDP) and the type of confidence intervals. We compare the following auditing methods:

* **DP + Wilson**: We audit the usual \((\varepsilon,\delta)\)-DP with \(K=1\) canary. This corresponds exactly to auditing LiDP with \(K=1\). We use the 1st-Order Wilson confidence intervals for a fair comparison with the other LiDP auditing methods. This performs quite similarly to the other intervals used in the literature, cf. Figure 7.
* **LiDP + 1st-Order Wilson**: We audit LiDP with \(K\) canaries with the 1st-Order Wilson confidence interval. This method cannot leverage the shrinking of the confidence intervals from higher-order estimates.
* **LiDP + 2nd/4th-Order Wilson**: We audit LiDP with \(K>1\) canaries using the higher-order Wilson confidence intervals. Unless stated otherwise, we use \(m=K\) test canaries.

**Parameters of the Experiment.** We vary the following parameters in the experiment:

* Number of trials \(n\in\{2^{8},2^{10},\cdots,2^{16}\}\).
* Number of canaries \(k\in\{1,2,2^{2},\ldots,2^{10}\}\).
* Dimension \(d\in\{10^{2},10^{3},\ldots,10^{6}\}\).

Figure 9: Effect of the number \(k\) of canaries on the empirical lower bound \(\hat{\varepsilon}\) from auditing the Gaussian mechanism for DP and LiDP. The shaded are denotes the standard error over 25 random seeds.

* DP upper bound \(\varepsilon\in\{1,2,4,8\}\).

We fix the DP parameter \(\delta=10^{-5}\) and the failure probability \(\beta=0.05\).

**Tuning the threshold \(\tau\).** For each confidence interval scheme, we repeat the estimation of the lower bound \(\hat{\varepsilon}(\tau)\) for a grid of thresholds \(\tau\in\Gamma\) on a holdout set of \(n\) trials. We fix the best threshold \(\tau^{*}=\arg\max_{\tau\in\Gamma}\hat{\varepsilon}(\tau)\) that gives the largest lower bound \(\hat{\varepsilon}(\tau)\) from the grid \(\Gamma\). We then fix the threshold \(\tau^{*}\) and report numbers over a fresh set of \(n\) trials.

**Randomness and Repetitions.** We repeat each experiment \(25\) times (after fixing the threshold) with different random seeds and report the mean and standard error.

### Additional Experimental Results

**Effect of the Number \(m\) of Test Canaries.** In Figure 13, we vary the number \(m\) of test canaries while keeping the number \(K\) of training canaries fixed. We see that the 2nd and 4th-order Wilson estimators lead to monotonically improving lower bound \(\hat{\varepsilon}\) as \(m\) increases. However, these improvements soon flatten out between \(m=8\) and \(m=32\) depending on other parameters. In particular, the default value of \(m=K\) is nearly or equal to the best value of \(m\) for \(K\geq 16\).

We give additional experimental results, expanding on the plots shown in Figures 3 and 4:

* Figure 8 shows the effect of varying the number of trials \(n\), similar to Figure 3 (left).

Figure 10: Effect of the data dimension \(d\) on the empirical lower bound \(\hat{\varepsilon}\) from auditing the Gaussian mechanism for DP and LiDP. The shaded area denotes the standard error over 25 random seeds.

* Figure 9 shows the effect of varying the number of canaries \(k\), similar to Figure 3 (middle).
* Figure 10 shows the effect of varying the data dimension \(d\), similar to Figure 3 (right).
* Figure 11 shows the effect of varying the number of canaries \(k\) on the moment estimates, similar to Figure 4 (right).
* Figure 12 shows the effect of varying the data dimension \(d\) on the moment estimates, similar to Figure 4 (right).

We observe that the insights discussed in SS4 hold across a wide range of the parameter values. In addition, we make the following observations.

**The benefit of higher-order confidence estimators.** We see from Figures 8 to 10 that the higher-order Wilson estimators lead to larger relative improvements at smaller \(\varepsilon\). On the other hand, they perform similarly at large \(\varepsilon\) (e.g., \(\varepsilon=8\)) to the lower-order estimators.

**4th-Order Wilson vs. 2nd-Order Wilson.** We note that the 4th-order Wilson interval outperforms the 2nd-order Wilson interval at \(\varepsilon=1\), while the opposite is true at large \(\varepsilon=8\). At intermediate values of \(\varepsilon\), both behave very similarly. We suggest the 2nd-order Wilson interval as a default because it is nearly as good as or better than the 4th-order variant across the board, but is easier to implement.

Figure 11: Effect of the number \(k\) of canaries on the moment estimates employed by the higher-order Wilson intervals in auditing LiDP.

## Appendix F Experiments: Details and More Results

We describe the detailed experimental setup here.

### Training Details: Datasets, Models

We consider two datasets, FMNIST and Purchase-100. Both are multiclass classification datasets trained with the cross-entropy loss using stochastic gradient descent (without momentum) for a fixed epoch budget.

Figure 12: Effect of the data dimension on the moment estimates employed by the higher-order Wilson intervals in auditing LiDP.

Figure 13: Effect of varying the number \(m\) of test canaries with the number \(K\) of canaries inserted in the training being fixed.

* **FMNIST**: FMNIST or FashionMNIST [65] is a classification of \(28\times 28\) grayscale images of various articles of clothing into 10 classes. It contains 60K train images and 10K test images. The dataset is available under the MIT license. We experiment with two models: a linear model and a multi-layer perceptron (MLP) with 2 hidden layers of dimension 256 each. We train each model for 30 epochs with a batch size of 100 and a fixed learning rate of 0.02 for the linear model and 0.01 for the MLP.
* **Purchase-100**: The Purchase dataset is based on Kaggle's "acquire valued shoppers" challenge [19] that records the shopping history of 200K customers. The dataset is available publicly on Kaggle but the owners have not created a license as far as we could tell. We use the preprocessed version of [53]3 where the input is a 600-dimensional binary vector encoding the shopping history. The classes are obtained by grouping the records into 100 clusters using \(k\)-means. We use a fixed subsample of 20K training points and 5K test points. The model is a MLP with 2 hidden layers of 256 units each. It is trained for 100 epochs with a batch size of 100 and a fixed learning rate of 0.05. Footnote 3: https://github.com/privacytrustlab/datasets

### DP and Auditing Setup

We train each dataset-model pair with DP-SGD [1]. The noise level is calibrated so that the entire training algorithm satisfies \((\varepsilon,\delta)\)-differential privacy with \(\varepsilon\) varying and \(\delta=10^{-5}\) fixed across all experiments. We tune the per-example gradient clip norm for each dataset, model, and DP parameter \(\varepsilon\) so as to maximize the validation accuracy.

**Auditing Setup.** We follow the LiDP auditing setup described in Appendix B.2. Recall that auditing LiDP with \(K=1\) canaries and corresponds exactly with auditing DP. We train \(n\in\{125,250,500,1000\}\) trials, where each trial corresponds to a model trained with \(K\) canaries in each run. We try two methods for canary design (as well as their corresponding rejection regions), as discussed in Appendix D: data poisoning and random gradients.

With data poisoning for FMNIST, we define the canary distribution \(\mathcal{P}_{\text{canary}}\) as the uniform distribution over the last \(p=284\) principal components (i.e., principal components \(500\) to \(784\)). For Purchase-100, we use the uniform distribution over the last \(p=300\) principal components (i.e., principal components \(300\) to \(600\)).

For both settings, we audit _only the final model_, assuming that we do not have access to the intermediate models. This corresponds to the blackbox auditing setting for data poisoning and a graybox setting for random gradient canaries.

### Miscellaneous Details

**Hardware.** We run each job on an internal compute cluster using only CPUs (i.e., no hardware accelerators such as GPUs were used). Each job was run with 8 CPU cores and 16G memory.

### Additional Experimental Results

#### f.4.1 Comparison to Bayesian Auditing of DP

We compare the proposed LiDP auditing approach with the Bayesian approach of Zanella-Beguelin et al. [69]. This approach constructs a posterior distribution over the true and false positive rates by updating the non-informative Jeffreys priors using empirical observations. They use Bayesian credible intervals (as opposed to confidence intervals in our case) computed using numerical quadrature.

We compare the proposed LiDP auditing with \(K=m>1\) with the Bayesian approach of [69] with \(K=m=1\) in Figure 14 using the code open-sourced by the authors.4 Owing to the increased runtime in the numerical estimation of the \(\varepsilon\) of the Bayesian approach, we tune its threshold \(\tau\) to separate the null and alternate hypotheses (cf. line 5 of Algorithm 1) for the Clopper-Pearson interval.

Footnote 4: https://github.com/microsoft/responsible-ai-toolbox-privacy

We observe that the LiDP auditing approach generally provides improved estimates when compared to this baseline. Note that the reason behind the improvement for the Bayesian approach of [69] is by capture the joint distribution of the true and false positive rates, while the improvement behind LiDP

[MISSING_PAGE_FAIL:34]

Figure 16: Test accuracy versus the number of canaries \(K\). We plot the mean over \(1000\) training runs (the standard error in under \(10^{-5}\)). Adding multiple canaries to audit LiDAR does not have any impact on the final test accuracy of the model trained with DP.

Figure 17: Experimental results for FMNIST linear model (top two: varying \(n\), bottom two: varying \(K\)).

Figure 18: Experimental results for FMNIST MLP model (top two: varying \(n\), bottom two: varying \(K\)).

Figure 19: Experimental results for Purchase-100 MLP model (top two: varying \(n\), bottom two: varying \(K\)).