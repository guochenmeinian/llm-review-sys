# Multi-scale Diffusion Denoised Smoothing

Jongheon Jeong Jinwoo Shin

Korea Advanced Institute of Science and Technology (KAIST)

Daejeon, South Korea

{jongheonj,jinwoos}@kaist.ac.kr

###### Abstract

Along with recent diffusion models, randomized smoothing has become one of a few tangible approaches that offers adversarial robustness to models at scale, _e.g._, those of large pre-trained models. Specifically, one can perform randomized smoothing on any classifier via a simple "denoise-and-classify" pipeline, so-called _denoised smoothing_, given that an accurate denoiser is available - such as diffusion model. In this paper, we present scalable methods to address the current trade-off between certified robustness and accuracy in denoised smoothing. Our key idea is to "selectively" apply smoothing among multiple noise scales, coined _multi-scale smoothing_, which can be efficiently implemented with a single diffusion model. This approach also suggests a new objective to compare the _collective_ robustness of multi-scale smoothed classifiers, and questions which representation of diffusion model would maximize the objective. To address this, we propose to further fine-tune diffusion model (a) to perform consistent denoising whenever the original image is recoverable, but (b) to generate rather diverse outputs otherwise. Our experiments show that the proposed multi-scale smoothing scheme, combined with diffusion fine-tuning, not only allows strong certified robustness at high noise scales but also maintains accuracy close to non-smoothed classifiers. Code is available at https://github.com/jh-jeong/smoothing-multiscale.

## 1 Introduction

Arguably, one of the important lessons in modern deep learning is the effectiveness of massive data and model scaling [38, 74], which enabled many breakthroughs in recent years [8, 53, 55, 7]. Even with the largest amount of data available on the web and computational budgets, however, the _worst-case_ behaviors of deep learning models are still challenging to regularize. For example, large language models often leak private information in training data [10], and one can completely fool superhuman-level Go agents with few meaningless moves [40]. Such unintentional behaviors can be of critical concerns in deploying deep learning into real-world systems, and the risk has been increasing as the capability of deep learning continues to expand.

In this context, _adversarial robustness_[61, 9] has been a seemingly-close milestone towards reliable deep learning. Specifically, neural networks are even fragile to small, "imperceptible" scales of noise when it comes to the worst-case, and consequently there have been many efforts to obtain neural networks that are robust to such noise [47, 3, 75, 15, 65]. Although it is a reasonable premise that we humans have an inherent mechanism to correct against adversarial examples [21, 26], yet so far the threat still persists in the context of deep learning, _e.g._, even in recent large vision-language models [48], and is known as hard to avoid without a significant reduction in model performance [66, 64].

_Randomized smoothing_[41, 15], the focus in this paper, is currently one of a few techniques that has been successful in obtaining adversarial robustness from neural networks. Specifically, it constructs a _smoothed classifier_ by taking a majority vote from a "base" classifier, _e.g._, a neural network, over random Gaussian input noise. The technique is notable for its _provable_ guarantees on adversarialrobustness, _i.e._, it is a _certified defense_[44], and its scalability to arbitrary model architectures. For example, it was the first certified defense that could offer adversarial robustness on ImageNet [56]. The scalablity of randomized smoothing has further expanded by Salman et al. [58], which observed that randomized smoothing can be applied to any pre-trained classifiers by prepending a denoiser model, dubbed _denoised smoothing_. Combined with the recent _diffusion-based models_[31], denoised smoothing could provide the current state-of-the-arts in \(\ell_{2}\)-certified robustness [11, 69, 44].

Despite its desirable properties, randomized smoothing in practice is also at odds with model accuracy [15], similarly to other defenses [47, 76], which makes it burdening to be applied in the real-world. For example, the variance of Gaussian noise, or _smoothing factor_, is currently a crucial hyperparameter to increase certified robustness at the cost of accuracy. The fundamental trade-off between accuracy and adversarial robustness has been well-evidenced in the literature [66, 75, 64], but it has been relatively under-explored whether they may or may not be applicable in the context of randomized smoothing. For example, Tramer et al. [64] demonstrate that pursuing \(\varepsilon\)-uniform adversarial robustness in neural networks may increase their vulnerability against _invariance attacks_, _i.e._, some semantics-altering perturbations possible inside the \(\varepsilon\)-ball: yet, it is still unclear whether such a vulnerability would be inevitable at non-uniformly robust models such as smoothed classifiers.

In attempt to understand the accuracy-robustness trade-off in randomized smoothing, efforts have been made to increase the certified robustness a given smoothed classifier can provide, _e.g._, through a better training method [58, 73, 35, 36, 37]. For example, Salman et al. [57] have employed adversarial training [47] in the context of randomized smoothing, and Jeong and Shin [35] have proposed to train a classifier with consistency regularization. Motivated by the trade-off among different smoothing factors, some works have alternatively proposed to perform smoothing with _input-dependent_ factors [1, 13, 67]: unfortunately, subsequent works [60, 36] have later shown that such schemes do not provide valid robustness certificates, reflecting its brittleness in overcoming the trade-off.

Contribution.In this paper, we develop a practical method to overcome the current frontier of accuracy-robustness trade-off in randomized smoothing, particularly upon the architectural benefits that the recent _diffusion denoised smoothing_[11] can offer. Specifically, we first propose to aggregate multiple smoothed classifiers of different smoothing factors to obtain their collective robustness (see Figure 1(a)), which leverages the scale-free nature of diffusion denoised smoothing. In this way, the model can decide which smoothed classifier to use for each input, maintaining the overall accuracy. Next, we fine-tune a diffusion model for randomized smoothing through the "denoise-and-classify" pipeline (see Figure 1(b)), as an efficient alternative of the full fine-tuning of (potentially larger) pre-trained classifiers. Here, we identify the _over-confidence_ of diffusion models, _e.g._, to a specific class given certain backgrounds, as a major challenge towards accurate-yet-robust randomized smoothing, and design a regularization objective to mitigate the issue.

In our experiments, we evaluate our proposed schemes on CIFAR-10 [39] and ImageNet [56], two of standard benchmarks for certified \(\ell_{2}\)-robustness, particularly considering practical scenarios of applying diffusion denoised smoothing to large pre-trained models such as CLIP [53]. Overall, the

Figure 1: An overview of the proposed approaches, (a) _cascaded randomized smoothing_ (Section 3.2 and (b) _diffusion calibration_ (Section 3.3) to attain a better trade-off between accuracy and certified robustness in randomized smoothing, upon on the recent _diffusion denoised smoothing_ scheme [11].

results consistently highlight that (a) the proposed multi-scale smoothing scheme upon the recent diffusion denoised smoothing [11] can significantly improve the accuracy of smoothed inferences while maintaining their certified robustness at larger radii, and (b) our fine-tuning scheme of diffusion models can additively improve the results - not only in certified robustness but also in accuracy - by simply replacing the denoiser model without any further adaptation. For example, we could improve certifications from a diffusion denoised smoothing based classifier by \(30.6\%\to 72.5\%\) in clean accuracy, while also improving its certified robustness at \(\varepsilon=2.0\) by \(12.6\%\to 14.1\%\). We observe that the collective robustness from our proposed multi-scale smoothing does not always correspond to their individual certified robustness, which has been a major evaluation in the literature, but rather to their "calibration" across models: which opens up a new direction to pursue for a practical use of randomized smoothing.

## 2 Preliminaries

**Adversarial robustness and randomized smoothing.** For a given classifier \(f:\mathcal{X}\to\mathcal{Y}\), where \(\mathbf{x}\in\mathcal{X}\subseteq\mathbb{R}^{d}\) and \(y\in\mathcal{Y}:=\{1,\cdots,K\}\), _adversarial robustness_ refers to the behavior of \(f\) in making consistent predictions at the _worst-case_ perturbations under semantic-preserving restrictions. Specifically, for samples from a data distribution \((\mathbf{x},y)\sim p_{\mathrm{data}}(\mathbf{x},y)\), it requires \(f(\mathbf{x}+\boldsymbol{\delta})=y\) for _every_ perturbation \(\boldsymbol{\delta}\) that a threat model defines, _e.g._, an \(\ell_{2}\)-ball \(\|\boldsymbol{\delta}\|_{2}\leq\varepsilon\). One of ways to quantify adversarial robustness is to measure the following _average minimum-distance_ of adversarial perturbations [9], _i.e._, \(R(f;p_{\mathrm{data}}):=\mathbb{E}_{(\mathbf{x},y)\sim p_{\mathrm{data}}} \left[\min_{f(\mathbf{x}^{\prime})\neq y}\|\mathbf{x}^{\prime}-\mathbf{x}\|_ {2}\right].\)

The essential challenge in achieving adversarial robustness stems from that evaluating this (and further optimizing on it) is usually infeasible. _Randomized smoothing_[41; 15] bypasses this difficulty by constructing a new classifier \(\hat{f}\) from \(f\) instead of letting \(f\) to directly model the robustness: specifically, it transforms the base classifier \(f\) with a certain _smoothing measure_, where in this paper we focus on the case of Gaussian distributions \(\mathcal{N}(0,\sigma^{2}\mathbf{I})\):

\[\hat{f}(\mathbf{x}):=\operatorname*{arg\,max}_{c\in\mathcal{Y}}\mathbb{P}_{ \hat{\delta}\sim\mathcal{N}(0,\sigma^{2}\mathbf{I})}\left[f(\mathbf{x}+ \boldsymbol{\delta})=c\right].\] (1)

Then, the robustness of \(\hat{f}\) at \((\mathbf{x},y)\), namely \(R(\hat{f};\mathbf{x},y)\), can be lower-bounded in terms of the _certified radius_\(\underline{R}(\hat{f},\mathbf{x},y)\), _e.g._, Cohen et al. [15] showed that the following bound holds, which is tight for \(\ell_{2}\)-adversarial threat models:

\[R(\hat{f};\mathbf{x},y)\geq\sigma\cdot\Phi^{-1}(p_{\hat{f}}(\mathbf{x},y))=: \underline{R}(\hat{f},\mathbf{x},y),\quad\text{where}\quad p_{\hat{f}}( \mathbf{x},y):=\mathbb{P}_{\boldsymbol{\delta}}[f(\mathbf{x}+\boldsymbol{ \delta})=y],\] (2)

provided that \(\hat{f}(\mathbf{x})=y\), otherwise \(R(\hat{f};\mathbf{x},y):=0\).1 Here, we remark that the formula for certified radius is essentially a function of \(p_{\hat{f}}\), which is the _accuracy_ of \(f(\mathbf{x}+\boldsymbol{\delta})\) over \(\boldsymbol{\delta}\).

Footnote 1: \(\Phi\) denotes the cumulative distribution function of \(\mathcal{N}(0,1^{2})\).

**Denoised smoothing.** Essentially, randomized smoothing requires \(f\) to make accurate classification of Gaussian-corrupted inputs. A possible design of \(f\) in this regard is to concatenate a Gaussian denoiser, say \(\mathsf{denoise}(\cdot)\), with any standard classifier \(f_{\mathtt{std}}\), so-called _denoised smoothing_[58]:

\[f(\mathbf{x}+\boldsymbol{\delta}):=f_{\mathtt{std}}(\mathsf{denoise}(\mathbf{ x}+\boldsymbol{\delta})).\] (3)

Under this design, an ideal denoiser \(\mathsf{denoise}(\cdot)\) should "accurately" recover \(\mathbf{x}\) from \(\mathbf{x}+\boldsymbol{\delta}\), _i.e._, \(\mathsf{denoise}(\mathbf{x}+\boldsymbol{\delta})\approx\mathbf{x}\) (in terms of their semantics to perform classification) with high probability of \(\boldsymbol{\delta}\sim\mathcal{N}(0,\sigma^{2}\mathbf{I})\). Denoised smoothing offers a more scalable framework for randomized smoothing, considering that (a) standard classifiers (rather than those specialized to Gaussian noise) are nowadays easier to obtain in the paradigm of large pre-trained models, and (b) the recent developments in diffusion models [31] has supplied denoisers strong enough for the framework. In particular, Lee [42] has firstly explored the connection between diffusion models and randomized smoothing; Carlini et al. [11] has further observed that latest diffusion models combined with a pre-trained classifier provides a state-of-the-art design of randomized smoothing.

**Diffusion models.** In principle, _diffusion models_[59; 31] aims to generate a given data distribution \(p_{\mathrm{data}}(\mathbf{x})\) via an iterative denoising process from a Gaussian noise \(\hat{\mathbf{x}}_{T}\sim\mathcal{N}(0,T^{2}\mathbf{I})\) for a certain \(T>0\). Specifically, it first assumes the following diffusion process which maps \(p_{\mathrm{data}}\) to \(\mathcal{N}(0,T^{2}\mathbf{I})\): \(\mathrm{d}\mathbf{x}_{t}=\boldsymbol{\mu}(\mathbf{x}_{t},t)\mathrm{d}t+ \sigma(t)\mathrm{d}\mathbf{w}_{t}\), where \(t\in[0,T]\), and \(\mathbf{w}_{t}\) denotes the standard Brownian motion. Basedon this, diffusion models first train a _score model_\(\mathbf{s}_{\phi}(\mathbf{x},t)\approx\nabla\log p_{t}(\mathbf{x})\) via score matching [34], and use the model to solve the probabilistic flow from \(\hat{\mathbf{x}}_{T}\sim\mathcal{N}(0,T^{2}\mathbf{I})\) to \(\mathbf{x}_{0}\) for sampling. The score estimator \(\mathbf{s}_{\phi}(\mathbf{x},t)\) is often parametrized by a _denoiser_\(D(\mathbf{x};\sigma(t))\) in practice, _viz._, \(\nabla\log p_{t}(\mathbf{x})=(D(\mathbf{x};\sigma(t))-\mathbf{x})/\sigma(t)^{2}\), which establishes its close relationship with denoised smoothing.

## 3 Method

Consider a classification task from \(\mathcal{X}\) to \(\mathcal{Y}\) where training data \(\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\}\sim p_{\mathrm{data}}(\mathbf{x},y)\) is available, and let \(f:\mathcal{X}\rightarrow\mathcal{Y}\) be a classifier. We denote \(\hat{f}_{\sigma}\) to be the smoothed classifier of \(f\) with respect to the smoothing factor \(\sigma>0\), as defined by (1). In this work, we aim to better understand how the _accuracy-robustness trade-off_ of \(\hat{f}_{\sigma}\) occurs, with a particular consideration of the recent denoised smoothing scheme. Generally speaking, the trade-off implies the following: for a given model, there exists a sample that the model gets "wrong" as it is optimized for (adversarial) robustness on another sample. In this respect, we start by taking a closer look at what it means by a model gets wrong, particularly when it is from randomized smoothing.

### Over-smoothing and over-confidence in randomized smoothing

Consider a smoothed classifier \(\hat{f}_{\sigma}\), and suppose there exists a sample \((\mathbf{x},y)\) where \(\hat{f}_{\sigma}\) makes an error: _i.e._, \(\hat{f}_{\sigma}(\mathbf{x})\neq y\). Our intuition here is to separate possible scenarios of \(\hat{f}_{\sigma}(\mathbf{x})\) making an error into two distinct cases, based on the _prediction confidence_ of \(\hat{f}_{\sigma}(\mathbf{x})\). Specifically, we define the _confidence_ of a smoothed classifier \(\hat{f}_{\sigma}\) at \(\mathbf{x}\) based on the definition of randomized smoothing (1) and (2):

\[p_{\hat{f}_{\sigma}}(\mathbf{x}):=\max_{y}\ p_{\hat{f}_{\sigma}}(\mathbf{x},y) =\max_{y}\ \mathbb{P}_{\boldsymbol{\delta}\sim\mathcal{N}(0,\sigma^{2}\mathbf{I})}[f( \mathbf{x}+\boldsymbol{\delta})=y].\] (4)

Intuitively, this notion of "smoothed" confidence measures how _consistent_ the base classifier \(f\) is in classifying \(\mathbf{x}+\boldsymbol{\delta}\) over Gaussian noise \(\boldsymbol{\delta}\sim\mathcal{N}(0,\sigma^{2}\mathbf{I})\). In cases when \(f\) is modeled by denoised smoothing, achieving high confidence requires the denoiser \(D\) to accurately "bounce-back" a given noisy image \(\mathbf{x}+\boldsymbol{\delta}\) into one that falls into class \(y\) with high probability over \(\boldsymbol{\delta}\).

Given a smoothed confidence \(p:=p_{\hat{f}_{\sigma}}(\mathbf{x})\), we propose to distinguish two cases of model errors, which are both peculiar to randomized smoothing, by considering a certain threshold \(p_{0}\) on \(p\). Namely, we interpret an error of \(\hat{f}_{\sigma}\) at \(\mathbf{x}\) either as (a) \(p\leq p_{0}\): the model is _over-smoothing_, or (b) \(p>p_{0}\): the model is having an _over-confidence_ on the input:

1. **Over-smoothing (\(p\leq p_{0}\)):** On one hand, it is unavoidable that the mutual information \(I(\mathbf{x}+\boldsymbol{\delta};y)\) between input and its class label absolutely decrease from smoothing with larger variance \(\sigma^{2}\), although using larger \(\sigma\) can increase the maximum certifiable radius of \(\hat{f}_{\sigma}\) in practice (2). Here, a "well-calibrated" smoothed classifier should output a prediction close to the uniform distribution across \(\mathcal{Y}\), leading to a low prediction confidence. In terms of denoised smoothing, the expectation is clearer: as \(\mathbf{x}+\boldsymbol{\delta}\) gets closer to the pure Gaussian noise, the denoiser \(\bar{D}\) should generate more diverse outputs hence in \(\mathcal{Y}\) as well. Essentially, this corresponds to an accuracy-robustness trade-off from choosing a specific \(\sigma\) for a given (_e.g._, information-theoretic) capacity of data.
2. **Over-confidence (\(p>p_{0}\)):** On the other hand, it is also possible for a model \(\hat{f}_{\sigma}\) to be incorrect but with a _high confidence_. Compared to the over-smoothing case, this scenario rather signals a "miscalibration" and corresponds to a trade-off from _model biases_: even across smoothed models with a fixed \(\sigma\), the balance between accuracy and robustness can be different depending on how each model assigns robustness in its decision boundary per-sample basis. When viewed in terms of denoised smoothing, this occurrence can reveal an implicit bias of the denoiser function \(D\), _e.g._, that of diffusion models. For example, a denoiser might be trained to adapt to some spurious cues in training data, _e.g._, their backgrounds, as also illustrated in Figure 1(b).

In the subsequent sections, Section 3.2 and 3.3, we introduce two methods to exhibit better accuracy-robustness trade-off in randomized smoothing, each of which focuses on the individual scenarios of over-smoothing and over-confidence, respectively. Specifically, Section 3.2 proposes to use a _cascaded inference_ of multiple smoothed classifiers across different smoothing factors to mitigate the limit of using a single smoothing factor. Next, in Section 3.3, we propose to calibrate diffusion models to reduce its over-confidence particularly in denoised smoothing.

### Cascaded randomized smoothing

To overcome the trade-off between accuracy and certified robustness from _over-smoothing_, _i.e._, from choosing a specific \(\sigma\), we propose to combine _multiple_ smoothed classifiers with different \(\sigma\)'s. In a nutshell, we design a pipeline of smoothed inferences that each input (possibly with different noise resilience) can adaptively select which model to use for its prediction. Here, the primary challenge is to make it "correct", so that the proposed pipeline does not break the existing statistical guarantees on certified robustness that each smoothed classifier makes.

Specifically, we now assume \(K\) distinct smoothing factors, say \(0<\sigma_{1}<\cdots<\sigma_{K}\), and their corresponding smoothed classifiers of \(f\), namely \(\hat{f}_{\sigma_{1}},\cdots,\hat{f}_{\sigma_{K}}\). For a given input \(\mathbf{x}\), our desiderata is (a) to maximize robustness certification at \(\mathbf{x}\) based on the smoothed inferences available from the individual models, say \(p_{\hat{f}_{\sigma_{1}}}(\mathbf{x}),\cdots,p_{\hat{f}_{\sigma_{K}}}(\mathbf{x})\), while (b) minimizing the access to each of the models those require a separate Monte Calro integration in practice. In these respects, we propose a simple "predict-or-abstain" policy, coined _cascaded randomized smoothing_:2

Footnote 2: We also discuss several other possible (and more sophisticated) designs in Appendix E.

\[\mathtt{casc}(\mathbf{x};\{\hat{f}_{\sigma_{i}}\}_{i=1}^{K}):=\begin{cases} \hat{f}_{\sigma_{K}}(\mathbf{x})&\text{if }\ p_{\hat{f}_{\sigma_{K}}}(\mathbf{x})>p_{0},\\ \mathtt{casc}(\mathbf{x};\{\hat{f}_{\sigma_{i}}\}_{i=1}^{K-1})&\text{if }\ p_{\hat{f}_{ \sigma_{K}}}(\mathbf{x})\leq p_{0}\text{ and }K>1,\end{cases}\] (5)

where \(\mathtt{ABSTAIN}\notin\mathcal{Y}\) denotes an artificial class to indicate "undecidable". Intuitively, the pipeline starts from computing \(\hat{f}_{\sigma_{K}}(\mathbf{x})\), the model with highest \(\sigma\), but takes its output only if its (smoothed) confidence \(p_{\hat{f}_{\sigma_{K}}}(\mathbf{x})\) exceeds a certain threshold \(p_{0}\):3 otherwise, it tries a smaller noise scale, say \(\sigma_{K-1}\) and so on, applying the same abstention policy of \(p_{\hat{f}_{\sigma}}(\mathbf{x})\leq p_{0}\). In this way, it can early-stop the computation at higher \(\sigma\) if it is confident enough, so it can maintain higher certified robustness, while avoiding unnecessary accesses to other models of smaller \(\sigma\).

Footnote 3: In our experiments, we simply use \(p_{0}=0.5\) for all \(\sigma\)â€™s. See Appendix C.3 for an ablation study with \(p_{0}\).

Next, we ask whether this pipeline can indeed provide a robustness certification: _i.e._, how much one can ensure \(\mathtt{casc}(\mathbf{x}+\boldsymbol{\delta})=\mathtt{casc}(\mathbf{x})\) in its neighborhood \(\boldsymbol{\delta}\). Theorem 3.1 below shows that one can indeed enjoy the most certified radius from \(\hat{f}_{\sigma_{k}}\) where \(\mathtt{casc}(\mathbf{x})=:\hat{y}\) halts, as long as the preceding models are either keep abstaining or output \(\hat{y}\) over \(\boldsymbol{\delta}\):4

Footnote 4: The proof of Theorem 3.1 is provided in Appendix D.1.

**Theorem 3.1**.: _Let \(\hat{f}_{\sigma_{1}},\cdots,\hat{f}_{\sigma_{K}}:\mathcal{X}\rightarrow\mathcal{Y}\) be smoothed classifiers with \(0<\sigma_{1}<\cdots<\sigma_{K}\). Suppose \(\mathtt{casc}(\mathbf{x};\{\hat{f}_{\sigma_{i}}\}_{i=1}^{K})=:\hat{y}\in \mathcal{Y}\) halts at \(\hat{f}_{\sigma_{k}}\) for some \(k\). Consider any \(\underline{p}\) and \(\overline{p}_{i,c}\in[0,1]\) that satisfy the following:_ **(a)**_\(\underline{p}\leq p_{\hat{f}_{\sigma_{k}}}(\mathbf{x},\hat{y})\), and_ **(b)**_\(\overline{p}_{k^{\prime},c}\geq p_{\hat{f}_{\sigma_{k}}}(\mathbf{x},c)\) for \(k^{\prime}\!\!>k\) and \(c\in\mathcal{Y}\). Then, it holds that \(\mathtt{casc}(\mathbf{x}+\boldsymbol{\delta};\{\hat{f}_{\sigma_{i}}\}_{i=1}^{K })=\hat{y}\) for any \(\|\boldsymbol{\delta}\|<R\), where:_

\[R:=\min\left\{\sigma_{k}\cdot\Phi^{-1}\Big{(}\underline{p}\Big{)},\min_{ \begin{subarray}{c}y\neq\hat{y}\\ k^{\prime}>k\end{subarray}}\left\{\sigma_{k^{\prime}}\cdot\Phi^{-1}\left(1- \overline{p}_{k^{\prime},y}\right)\right\}\right\}.\] (6)

Overall, the proposed multi-scale smoothing scheme (and Theorem 3.1) raise the importance of "abstaining well" in randomized smoothing: if a smoothed classifier can perfectly detect and abstain its potential errors, one could overcome the trade-off between accuracy and certified robustness by joining a more accurate model afterward. The option to abstain in randomized smoothing was originally adopted to make its statistical guarantees correct in practice. Here, we extend this usage to also rule out less-confident predictions for a more conservative decision making. As discussed in Section 3.1, now the _over-confidence_ becomes a major challenge in this matter: _e.g._, such samples can potentially bypass the abstention policy of cascaded smoothing, which motivates our fine-tuning scheme presented in Section 3.3.

**Certification.** We implement our proposed cascaded smoothing to make "statistically consistent" predictions across different noise samples, considering a certain _significance level_\(\alpha\) (_e.g._, \(\alpha=0.001\)): in a similar fashion as Cohen et al. [15]. Roughly speaking, for a given input \(\mathbf{x}\), it makes predictions only when the \((1-\alpha)\)-confidence interval of \(p_{\hat{f}}(\mathbf{x})\) does not overlap with \(p_{0}\) upon \(n\)_i.i.d._ noise samples (otherwise it abstains). The more details can be found in Appendix D.2.

### Calibrating diffusion models through smoothing

Next, we move on to the _over-confidence_ issue in randomized smoothing: _viz._, \(\hat{f}_{\sigma}\) often makes errors with a high confidence to wrong classes. We propose to fine-tune a given \(\hat{f}_{\sigma}\) to make rather _diverse_ outputs when it misclassifies, _i.e._, towards a more "calibrated" \(\hat{f}_{\sigma}\). By doing so, we aim to cast the issue of over-confidence as that of _over-smoothing_: which can be easier to handle in practice, _e.g._, by abstaining. In this work, we particularly focus on fine-tuning only the _denoiser model_\(D\) in the context of denoised smoothing, _i.e._, \(f:=f_{\mathtt{std}}\circ D\) for a standard classifier \(f_{\mathtt{std}}\): this offers a more scalable approach to improve certified robustness of pre-trained models, given that fine-tuning the entire classifier model in denoised smoothing can be computationally prohibitive in practice.

Specifically, given a base classifier \(f:=f_{\mathtt{std}}\circ D\) and training data \(\mathcal{D}\), we aim to fine-tune \(D\) to improve the certified robustness of \(\hat{f}_{\sigma}\). To this end, we leverage the _confidence_ information of the backbone classifier \(f_{\mathtt{std}}\) fairly assuming it as an "oracle" - which became somewhat reasonable given the recent off-the-shelf models available - and apply different losses depending on the confidence information per-sample basis. We propose two losses in this matter: (a) _Brier loss_ for either correct or under-confident samples, and (b) _anti-consistency loss_ for incorrect, over-confident samples.

Brier loss.For a given training sample \((\mathbf{x},y)\), we adopt the Brier (or "squared") loss [6] to regularize the denoiser function \(D\) to promote the confidence of \(f_{\mathtt{std}}(D(\mathbf{x}+\boldsymbol{\delta}))\) towards \(y\), which can be beneficial to increase the smoothed confidence \(p_{\hat{f}_{\sigma}}(\mathbf{x})\) that impacts the certified robustness at \(\mathbf{x}\). Compared to the cross-entropy (or "log") loss, a more widely-used form in such purpose, we observe that the Brier loss can be favorable in such fine-tuning of \(D\) through \(f_{\mathtt{std}}\), in a sense that the loss is less prone to "over-optimize" the confidence at values closer to \(1\).

Here, an important detail is that we do not apply the regularization to incorrect-yet-confident samples: _i.e._, whenever \(f_{\mathtt{std}}(D(\mathbf{x}+\boldsymbol{\delta}))\neq y\) and \(p_{\mathtt{std}}(\boldsymbol{\delta}):=\max_{c}F_{\mathtt{std},c}(D(\mathbf{x }+\boldsymbol{\delta}))>p_{0}\), where \(F_{\mathtt{std}}\) is the soft prediction of \(f_{\mathtt{std}}\). This corresponds to the case when \(D(\mathbf{x}+\boldsymbol{\delta})\) rather outputs a "realistic" off-class sample, which will be handled by the anti-consistency loss we propose. Overall, we have:

\[L_{\mathtt{Brier}}(\mathbf{x},y):=\mathbb{E}_{\boldsymbol{\delta}}[\mathbf{1} [\hat{y}_{\boldsymbol{\delta}}=y\;\;\text{or}\;\;p_{\mathtt{std}}(\boldsymbol {\delta})\leq p_{0}]\cdot\|F_{\mathtt{std}}(D(\mathbf{x}+\boldsymbol{\delta}) )-\mathbf{e}_{y}\|^{2}],\] (7)

where we denote \(\hat{y}_{\boldsymbol{\delta}}:=f(\mathbf{x}+\boldsymbol{\delta})\) and \(\mathbf{e}_{y}\) is the \(y\)-th unit vector in \(\mathbb{R}^{|\mathcal{Y}|}\).

Anti-consistency loss.On the other hand, the anti-consistency loss aims to detect whether the sample is over-confident, and penalizes it accordingly. The challenge here is that identifying over-confidence in a smoothed classifier requires checking for \(p_{\hat{f}_{\sigma}}(\mathbf{x})>p_{0}\) (4), which can be infeasible during training. We instead propose a simpler condition to this end, which only takes two independent Gaussian noise, say \(\boldsymbol{\delta}_{1},\boldsymbol{\delta}_{2}\sim\mathcal{N}(0,\sigma^{2} \mathbf{I})\). Specifically, we identify \((\mathbf{x},y)\) as over-confident whenever (a) \(\hat{y}_{1}:=f(\mathbf{x}+\boldsymbol{\delta}_{1})\) and \(\hat{y}_{2}:=f(\mathbf{x}+\boldsymbol{\delta}_{2})\) match, while (b) they are incorrect, _i.e._, \(\hat{y}_{1}\neq y\). Intuitively, such a case signals that the denoiser \(D\) is often making a complete flip to the semantics of \(\mathbf{x}+\boldsymbol{\delta}\) (see Figure 1(b) for an example), which we aim to penalize. A care should be taken, however, considering the possibility that \(D(\mathbf{x}+\boldsymbol{\delta})\) indeed generates an in-distribution sample that falls into a different class: in this case, penalizing it may result in a decreased robustness of that sample. In these respects, our design of anti-consistency loss forces the two samples simply to have different predictions, by keeping at least one prediction as the original, while penalizing the counterpart. Denoting \(\mathbf{p}_{1}:=F_{\mathtt{std}}(D(\mathbf{x}+\boldsymbol{\delta}_{1}))\) and \(\mathbf{p}_{2}:=F_{\mathtt{std}}(D(\mathbf{x}+\boldsymbol{\delta}_{2}))\), we again apply the squared loss on \(\mathbf{p}_{1}\) and \(\mathbf{p}_{2}\) to implement the loss design, as the following:

\[L_{\mathtt{AC}}(\mathbf{x},y):=\mathbf{1}[\hat{y}_{1}=\hat{y}_{2}\;\text{and} \;\hat{y}_{1}\neq y]\cdot(\|\mathbf{p}_{1}-\mathtt{sg}(\mathbf{p}_{1})\|^{2}+ \|\mathbf{p}_{2}\|^{2}),\] (8)

where \(\mathtt{sg}(\cdot)\) denotes the stopping gradient operation.

Overall objective.Combining the two proposed losses, _i.e._, the Brier loss and anti-consistency loss, defines a new regularization objective to add upon any pre-training objective for the denoiser \(D\):

\[L(D):=L_{\mathtt{Denoiser}}+\lambda\cdot(L_{\mathtt{Brier}}+\alpha\cdot L_{ \mathtt{AC}}),\] (9)

where \(\lambda,\alpha>0\) are hyperparameters. Here, \(\alpha\) denotes the relative strength of \(L_{\mathtt{AC}}\) over \(L_{\mathtt{Brier}}\) in its regularization.5 Remark that increasing \(\alpha\) would give more penalty on over-confident samples, which would lead the model to make more abstentions: therefore, this results in an increased accuracy particularly in cascaded smoothing (Section 3.2).

## 4 Experiments

We verify the effectiveness of our proposed schemes, (a) _cascaded smoothing_ and (b) _diffusion calibration_, mainly on CIFAR-10 [39] and ImageNet [56]: two standard datasets for an evaluation of certified \(\ell_{2}\)-robustness. We provide the detailed experimental setups, _e.g_., training, datasets, hyperparameters, computes, _etc_., in Appendix B.

**Baselines.** Our evaluation mainly compares with _diffusion denoised smoothing_[11], the current state-of-the-art methodology in randomized smoothing. We additionally compare with two other training baselines from the literature, by considering models with the same classifier architecture but without the denoising step of Carlini et al. [11]. Specifically, we consider (a) _Gaussian training_[15], which trains a classifier with Gaussian augmentation; and (b) _Consistency_[35], which additionally regularizes the variance of predictions over Gaussian noise in training. We select the baselines assuming practical scenarios where the training cost of the classifier side is crucial: other existing methods for smoothed classifiers often require much more costs, _e.g_., \(8\) times over Gaussian [57; 73].

**Setups.** We follow Carlini et al. [11] for the choice of diffusion models: specifically, we use the 50M-parameter \(32\times 32\) diffusion model from Nichol and Dhariwal [52] for CIFAR-10, and the 552M-parameter \(256\times 256\) unconditional model from Dhariwal and Nichol [18] for ImageNet. For the classifier side, we use ViT-B/16 [63] pre-trained via CLIP [53] throughout our experiments. For uses we fine-tune the model on each of CIFAR-10 and ImageNet via FT-CLIP [19], resulting in classifiers that achieve 98.1% and 85.2% in top-1 accuracy on CIFAR-10 and ImageNet, respectively. Following the prior works, we mainly consider \(\sigma\in\{0.25,0.50,1.00\}\) for smoothing in our experiments.

**Evaluation metrics.** We consider two popular metrics in the literature when evaluating certified robustness of smoothed classifiers: (a) the _approximate certified test accuracy_ at \(r\): the fraction of the test set which Certify[15] classifies correctly with the radius larger than \(r\) without abstaining,and (b) the _average certified radius_ (ACR) [73]: the average of certified radii on the test set \(\mathcal{D}_{\texttt{test}}\) while assigning incorrect samples as 0: _viz._, \(\mathrm{ACR}:=\frac{1}{|\mathcal{D}_{\texttt{test}}|}\sum_{(\mathbf{x},y)\in \mathcal{D}_{\texttt{test}}}[\mathrm{CR}(f,\sigma,\mathbf{x})\cdot\mathds{1}_{ \tilde{f}_{(\mathbf{x})=y}}]\), where \(\mathrm{CR}(\cdot)\) denotes the certified radius that Certify returns. Throughout our experiments, we use \(n=10,000\) noise samples to certify robustness for both CIFAR-10 and ImageNet. We follow [15] for the other hyperparameters to run Certify, namely by \(n_{0}=100\), and \(\alpha=0.001\). In addition to certified accuracy, we also compare the _empirical accuracy_ of smoothed classifiers. Here, we define empirical accuracy by the fraction of test samples those are either (a) certifiably correct, or (b) abstained but correct in the _clean_ classifier: which can be a natural alternative especially at denoised smoothing. For this comparison, we use \(n=100\) to evaluate empirical accuracy.

Unlike the evaluation of Carlini et al. [11], however, we do not compare cascaded smoothing with the _envelop accuracy curve_ over multiple smoothed classifiers at \(\sigma\in\{0.25,0.50,1.00\}\): although the envelope curve can be a succinct proxy to compare methods, it can be somewhat misleading and unfair to compare the curve directly with an individual smoothed classifier. This is because the curve does not really construct a concrete classifier on its own: it additionally assumes that each test sample has prior knowledge on the value of \(\sigma\in\{0.25,0.5,1.0\}\) to apply, which is itself challenging to infer. This is indeed what our proposal of cascaded smoothing addresses.

### Results

**Cascaded smoothing.** Figure 2 visualizes the effect of cascaded smoothing we propose in the plots of certified accuracy, and the detailed results are summarized in Table 1 and 2 as "+ Cascading". Overall, we observe that the certified robustness that cascaded smoothing offers can be highly desirable over the considered single-scale smoothed classifiers. Compared to the single-scale classifiers at highest \(\sigma\), _e.g._, \(\sigma=1.0\) in Table 1 and 2, our cascaded classifiers across \(\sigma\in\{0.25,0.50,1.00\}\) absolutely improve the certified accuracy at all the range of \(\varepsilon\) by incorporating more accurate predictions from classifiers, _e.g._, of lower \(\sigma\in\{0.25,0.50\}\). On the opposite side, _e.g._, compared to Carlini et al. [11] at \(\sigma=0.25\), the cascaded classifiers provide competitive certified clean accuracy (\(\varepsilon=0\)), _e.g._, 89.5% _vs._ 85.1% on CIFAR-10, while being capable of offering a wider range of robustness certificates. Those considerations are indeed reflected quantitatively in terms of the improvements in ACRs. The existing gaps in the clean accuracy are in principle due to the errors in higher-\(\sigma\) classifiers: _i.e._, a better calibration, to let them better abstain, could potentially reduce the gaps.

**Diffusion calibration.** Next, we evaluate the effectiveness of our proposed diffusion fine-tuning scheme: "+ Calibration" in Table 1 and 2 report the results on CIFAR-10 and ImageNet, respectively, and Figure 3 plots the CIFAR-10 results for each of \(\sigma\in\{0.25,0.50,1.00\}\). Overall, on both CIFAR-10 and ImageNet, we observe that the proposed fine-tuning scheme could _uniformly_ improve certified accuracy across the range considered, even including the clean accuracy. This confirms that the tuning could essentially improve the accuracy-robustness trade-off rather than simply moving along itself. As provided in Table 3, we remark that simply pursuing only the Brier loss (7) may achieve a better ACR in overall, but with a decreased accuracy: it is the role of the anti-consistency loss (8) to balance between the two, consequently to achieve a better trade-off afterwards.

**Empirical accuracy.** In practical scenarios of adopting smoothed classifiers for inference, it is up to users to decide how to deal with the "abstained" inputs. Here, we consider a possible candidate of simply outputting the _standard_ prediction instead for such inputs: in this way, the output could be noted as an "uncertified" prediction, while possibly being more accurate, _e.g._, for in-distribution

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{ImageNet, ViT-B/16@224} & \multicolumn{3}{c}{Empirical accuracy (\%)} & \multicolumn{3}{c}{Certified accuracy at \(\varepsilon\) (\%)} \\ \cline{2-10}  & \(\sigma\) & IN-1K & IN-R & IN-A & 0.0 & 0.5 & 1.0 & 1.5 & 2.0 & ACR \\ \hline Carlini et al. [11] & 0.25 & 80.9 & 69.3 & 35.3 & 78.8 & 61.4 & & & 0.517 \\ \hline Carlini et al. [11] & 0.50 & 79.5 & 67.5 & 32.3 & 65.8 & 52.2 & 38.6 & 23.6 & 0.703 \\ **+ Cascading (ours)** & (0.25-0.50) & **83.5** & **69.6** & **41.3** & **75.0** & **52.6** & **39.0** & 22.8 & **0.720** \\ **+ Calibration (ours)** & (0.25-0.50) & **83.8** & **69.8** & **41.7** & **76.6** & **54.6** & **39.8** & 23.0 & **0.743** \\ \hline Carlini et al. [11] & 1.00 & 77.2 & 64.3 & 32.0 & 30.6 & 25.8 & 20.6 & 17.0 & 12.6 & 0.538 \\ **+ Cascading (ours)** & (0.25-1.00) & **82.6** & **69.8** & **40.5** & **69.0** & **42.4** & **26.6** & **19.0** & **14.6** & **0.752** \\ **+ Calibration (ours)** & (0.25-1.00) & **83.2** & **69.5** & **40.8** & **72.5** & **44.0** & **27.5** & **19.9** & **14.1** & **0.775** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of (a) certified accuracy, (b) empricial accuracy, and (c) average certified radius (ACR) on ImageNet. We set our result bold-faced whenever it achieves the best upon baselines.

inputs. Specifically, in Table 1 and 2, we consider a fixed standard classifier of CLIP-finetuned ViT-B/16 on CIFAR-10 (or ImageNet), and compare the empirical accuracy of smoothed classifiers on CIFAR-10, -10-C [27] and -10.1 [54] (or ImageNet, -R [28], and -A [30]). Overall, the results show that our smoothed models could consistently outperform even in terms of empirical accuracy while maintaining high certified accuracy, _i.e_., they abstain only when necessary. We additionally observe in Appendix C.2 that the empirical accuracy of our smoothed model can be further improved by considering an "ensemble" with the clean classifier: _e.g_., the ensemble improves the accuracy of our cascaded classifier (\(\sigma\in\{0.25,0.5\}\)) on CIFAR-10-C by \(88.8\%\to 95.0\%\), even outperforming the accuracy of the standard classifier of \(93.4\%\).

### Ablation study

In Table 3, we compare our result with other training baselines as well as some ablations for a component-wise analysis, particularly focusing on their performance in cascaded smoothing across \(\sigma\in\{0.25,0.50,1.00\}\) on CIFAR-10. Here, we highlight several remarks from the results, and provide the more detailed study, _e.g_., the effect of \(p_{0}\) (5), in Appendix C.3.

**Cascading from other training.** "Gaussian" and "Consistency" in Table 3 report the certified robustness of cascaded classifiers where each of single-scale models is individually trained by the method. Even while their (certified) clean accuracy of \(\sigma=0.25\) models are competitive with those of denoising-based models, their collective accuracy significantly degraded after cascading, and interestingly the drop is much more significant on "Consistency", although it did provide more robustness at larger \(\varepsilon\). Essentially, for a high clean accuracy in cascaded smoothing the individual classifiers should make consistent predictions although their confidence may differ: the results imply that individual training of classifiers, without a shared denoiser, may break this consistency. This supports an architectural benefit of denoised smoothing for a use as cascaded smoothing.

**Cross-entropy _vs_. Brier.** "+ Cross-entropy" in Table 3 considers an ablation of the Brier loss (7) where the loss is replaced by the standard cross-entropy: although it indeed improves ACR compared to Carlini et al. [11] and achieves the similar clean accuracy with "+ Brier loss", its gain in overall certified robustness is significantly inferior to that of Brier loss as also reflected in the worse ACR. The superiority of the "squared" loss instead of the "log" loss suggests that it may not be necessary to optimize the confidence of individual denoised image toward a value strictly close to 1, which is reasonable considering the severity of noise usually considered for randomized smoothing.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multicolumn{6}{c}{Error rates (\%, \(\downarrow\))} \\ \cline{2-9} Model (\(\sigma=1.0\)) & \(p\leq p_{0}\) & \(p>p_{0}\) & ACR (\(\uparrow\)) \\ \hline Carlini et al. [11] & 43.8 & **7.6** & 0.498 \\ \hline
**Cascading** & 5.0 & 14.5 & 0.579 \\
**+ Anti-consist.** & 4.7 & 11.4 & 0.566 \\
**+ Brier loss** & **3.6** & 16.8 & **0.645** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of ACR and certified accuracy rates on CIFAR-10 decomposed into (a) over-smoothing (\(p\leq p_{0}\)) and (b) over-confidence (\(p>p_{0}\)), also with ACR.

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline \multicolumn{1}{l}{Cascaded, \(\sigma\in\{0.25,0.5,1.0\}\)} & \multicolumn{6}{c}{Certified accuracy at \(\varepsilon\) (\%)} \\ \cline{2-10} Training & ACR & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 & 1.25 & 1.50 & 1.75 \\ \hline Gaussian [15] & 0.470 & 70.7 & 46.9 & 31.7 & 23.7 & 16.8 & 12.4 & 9.2 & 6.5 \\ Consistency [35] & 0.548 & 57.6 & 42.7 & 32.3 & 26.3 & 22.4 & 18.1 & 14.6 & 11.1 \\ \hline Carlini et al. [11] & 0.579 & 80.5 & 52.8 & 37.4 & 28.0 & 21.7 & 16.8 & 11.8 & 8.5 \\
**+ Cross-entropy** & 0.605 & 77.9 & 52.8 & 39.6 & 29.1 & 22.2 & 18.7 & 13.5 & 9.5 \\
**+ Brier loss** & **0.666** & 77.6 & **55.7** & **41.7** & **32.7** & **26.8** & **21.7** & **16.3** & **11.9** \\
**+ Anti-consist.** (\(\alpha=1.0\)) & 0.645 & 79.6 & 53.5 & 40.5 & 31.2 & 25.0 & 20.1 & 16.2 & 11.4 \\
**+ Anti-consist.** (\(\alpha=2.0\)) & 0.625 & 80.1 & 54.6 & 39.2 & 30.6 & 24.0 & 18.5 & 14.5 & 9.9 \\
**+ Anti-consist.** (\(\alpha=1.0\)) & 0.566 & **83.9** & 55.1 & 37.8 & 25.8 & 19.8 & 14.5 & 11.5 & 8.1 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of ACR and certified accuracy of cascaded smooth classifiers on CIFAR-10 over different training and ablations. We use \(\sigma\in\{0.25,0.5,1.0\}\) for cascaded smoothing. Bold and underline indicate the best and runner-up, respectively. Model reported in Table 1 is marked as grey.

Anti-consistency loss.The results marked as "+ Anti-consist." in Table 3, on the other hand, validates the effectiveness of the anti-consistency loss (8). Compared to "+ Brier loss", which is equivalent to the case when \(\alpha=0.0\) in (9), we observe that adding anti-consistency results in a slight decrease in ACR but with an increase in clean accuracy. The increased accuracy of cascaded smoothing indicates that the fine-tuning could successfully reduce over-confidence, and this tendency continues at larger \(\alpha=2.0\). Even with the decrease in ACR over the Brier loss, the overall ACRs attained are still superior to others, confirming the effectiveness of our proposed loss in total (9).

Classifier fine-tuning.In Table 4, we compare our proposed diffusion fine-tuning (Section 3.3) with another possible scheme of _classifier_ fine-tuning, of which the effectiveness has been shown by Carlini et al. [11]. Overall, we observe that fine-tuning both classifiers and denoiser can bring their complementary effects to improve denoised smoothing in ACR, and the diffusion fine-tuning itself could obtain a comparable gain to the classifier fine-tuning. The (somewhat counter-intuitive) effectiveness of diffusion fine-tuning confirms that denoising process can be also biased as well as classifiers to make over-confident errors. We further remark two aspects where classifier fine-tuning can be less practical, especially when it comes with the cascaded smoothing pipeline we propose:

1. To perform cascaded smoothing at multiple noise scales, classifier fine-tuning would require separate runs of training for optimal models per scale, also resulting in multiple different models to load - which can be less scalable in terms of memory efficiency.
2. In a wider context of denoised smoothing, the classifier part is often assumed to be a model at scale, even covering cases when it is a "black-box" model, _e.g._, public APIs. Classifier fine-tuning, in such cases, can become prohibitive or even impossible.

With respect to (a) and (b), denoiser fine-tuning we propose offers a more efficient inference architecture: it can handle multiple noise scales jointly with a single diffusion model, while also being applicable to the extreme scenario when the classifier model is black-box.

Over-smoothing and over-confidence.As proposed in Section 3.1, errors from smoothed classifiers can be decomposed into two: _i.e._, \(p\leq p_{0}\) for over-smoothing, and \(p>p_{0}\) over-confidence, respectively. In Table 5, we further report a detailed breakdown of the model errors on CIFAR-10, assuming \(\sigma=1.0\). Overall, we observe that over-smoothing can be the major source of errors especially at such a high noise level, and our proposed cascading dramatically reduces the error. Although we find cascading could increase over-confidence from accumulating errors through multiple inferences, our diffusion fine-tuning could alleviate the errors jointly reducing the over-smoothing as well.

## 5 Conclusion and Discussion

Randomized smoothing has been traditionally viewed as a somewhat less-practical approach, perhaps due to its cost in inference time and impact on accuracy. In another perspective, to our knowledge, it is currently one of a few existing approaches that is prominent in pursuing _adversarial robustness at scale_, _e.g._, in a paradigm where training cost scales faster than computing power. This work aims to make randomized smoothing more practical, particularly concerning on scalable scenarios of robustness large pre-trained classifiers. We believe our proposals in this respect, _i.e._, _cascaded smoothing_ and _diffusion calibration_, can be a useful step towards building safer AI-based systems.

Limitation.A practical downside of randomized smoothing is in its increased inference cost, mainly from the majority voting procedure per inference. Our method also essentially possesses this practical bottleneck, and the proposed multi-scale smoothing scheme may further increase the cost from taking multiple smoothed inferences. Yet, we note that randomized smoothing itself is equipped with many practical axes to reduce its inference cost by compensating with abstention: for example, one can reduce the number of noise samples, _e.g._, to \(n=100\). It would be an important future direction to explore practices for a better trade-off between the inference cost and robustness of smoothed classifiers, which could eventually open up a feasible way to obtain adversarial robustness at scale.

Broader impact.Deploying deep learning based systems into the real-world, especially when they are of security-concerned [12, 72], poses risks for both companies and customers, and we researchers are responsible to make this technology more reliable through research towards _AI safety_[2, 29]. _Adversarial robustness_, that we focus on in this work, is one of the central parts of this direction, but one should also recognize that adversarial robustness is still a bare minimum requirement for reliable deep learning. The future research should also explore more diverse notions of AI Safety to establish a realistic sense of security for practitioners, _e.g._, monitoring and alignment research, to name a few.

## Acknowledgments and Disclosure of Funding

This work was partly supported by Center for Applied Research in Artificial Intelligence (CARAI) grant funded by Defense Acquisition Program Administration (DAPA) and Agency for Defense Development (ADD) (UD230017TD), and by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)). We are grateful to the Center for AI Safety (CAIS) for generously providing compute resources that supported a significant portion of the experiments conducted in this work. We thank Kyuyoung Kim and Sihyun Yu for the proofreading of our manuscript, and Kyungmin Lee for the initial discussions. We also thank the anonymous reviewers for their valuable comments to improve our manuscript.

## References

* Alfarra et al. [2022] Motasem Alfarra, Adel Bibi, Philip HS Torr, and Bernard Ghanem. Data dependent randomized smoothing. In _Uncertainty in Artificial Intelligence_, pages 64-74. PMLR, 2022.
* Amodei et al. [2016] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane. Concrete problems in AI safety. _arXiv preprint arXiv:1606.06565_, 2016.
* Athalye et al. [2018] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 274-283, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.press/v80/athaye18a.html.
* Balunovic and Vechev [2020] Mislav Balunovic and Martin Vechev. Adversarial training and provable defenses: Bridging the gap. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=SJxSDxrKDr.
* Bonferroni [1936] Carlo Bonferroni. Teoria statistica delle classi e calcolo delle probabilita. _Pubblicazioni del R Istituto Superiore di Scienze Economice e Commericiali di Firenze_, 8:3-62, 1936.
* Brier [1950] Glenn W Brier. Verification of forecasts expressed in terms of probability. _Monthly weather review_, 78(1):1-3, 1950.
* Brohan et al. [2022] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as I can, not as I say: Grounding language in robotic affordances. In _6th Annual Conference on Robot Learning_, 2022.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901, 2020.
* Carlini et al. [2019] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, and Aleksander Madry. On evaluating adversarial robustness. _arXiv preprint arXiv:1902.06705_, 2019.
* Carlini et al. [2021] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In _30th USENIX Security Symposium_, pages 2633-2650, 2021.
* Carlini et al. [2023] Nicholas Carlini, Florian Tramer, Krishnamurthy Dj Dvijotham, Leslie Rice, Mingjie Sun, and J Zico Kolter. (Certified!) adversarial robustness for free! In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=JLg54HW7j.
* Caruana et al. [2015] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In _Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1721-1730, 2015.
* Chen et al. [2021] Chen Chen, Kezhi Kong, Peihong Yu, Juan Luque, Tom Goldstein, and Furong Huang. Insta-RS: Instance-wise randomized smoothing for improved robustness and accuracy. _arXiv preprint arXiv:2103.04436_, 2021.
* Clopper and Pearson [1934] Charles J Clopper and Egon S Pearson. The use of confidence or fiducial limits illustrated in the case of the binomial. _Biometrika_, 26(4):404-413, 1934.

* Cohen et al. [2019] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 1310-1320, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/cohen19c.html.
* Croce and Hein [2020] Francesco Croce and Matthias Hein. Provable robustness against all adversarial \(l_{p}\)-perturbations for \(p\geq 1\). In _International Conference on Learning Representations_, 2020.
* Croce et al. [2020] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. RobustBench: a standardized adversarial robustness benchmark. _arXiv preprint arXiv:2010.09670_, 2020.
* Dhariwal and Nichol [2021] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 8780-8794. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/49ad23dec9fa4bd8d77d02681df5cfa-Paper.pdf.
* Dong et al. [2022] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Gu Shuyang, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. CLIP itself is a strong fine-tuner: Achieving 85.7% and 88.0% top-1 accuracy with ViT-B and ViT-L on ImageNet. _arXiv preprint arXiv:2212.06138_, 2022.
* Eiras et al. [2021] Francisco Eiras, Motasem Alfarra, M. Pawan Kumar, Philip H. S. Torr, Puneet K. Dokania, Bernard Ghanem, and Adel Bibi. ANCER: Anisotropic certification via sample-wise volume maximization, 2021.
* Elsayed et al. [2018] Gamaleldin Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alexey Kurakin, Ian Goodfellow, and Jascha Sohl-Dickstein. Adversarial examples that fool both computer vision and time-limited humans. _Advances in Neural Information Processing Systems_, 31, 2018.
* Fischer et al. [2020] Marc Fischer, Maximilian Baader, and Martin Vechev. Certified defense to image transformations via randomized smoothing. _Advances in Neural information processing systems_, 33:8404-8417, 2020.
* Gehr et al. [2018] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In _IEEE Symposium on Security and Privacy_, 2018.
* Goodman [1965] Leo A Goodman. On simultaneous confidence intervals for multinomial proportions. _Technometrics_, 7(2):247-254, 1965.
* Gowal et al. [2019] Sven Gowal, Krishnamurthy Dj Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. Scalable verified training for provably robust image classification. In _IEEE/CVF International Conference on Computer Vision_, pages 4842-4851, 2019.
* Guo et al. [2022] Chong Guo, Michael Lee, Guillaume Leclerc, Joel Daepello, Yug Rao, Aleksander Madry, and James Dicarlo. Adversarially trained neural representations are already as robust as biological neural representations. In _International Conference on Machine Learning_, pages 8072-8081. PMLR, 2022.
* Hendrycks and Dietterich [2019] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=HJz6tiCq7m.
* Hendrycks et al. [2021] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8340-8349, October 2021.
* Hendrycks et al. [2021] Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ML safety. _arXiv preprint arXiv:2109.13916_, 2021.
* Hendrycks et al. [2021] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15262-15271, June 2021.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.

* Horvath et al. [2022] Miklos Z. Horvath, Mark Niklas Mueller, Marc Fischer, and Martin Vechev. Boosting randomized smoothing with variance reduced classifiers. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=mHu2vIds_-b.
* compositional architectures for randomized smoothing. In _ICLR 2022 Workshop on Socially Responsible Machine Learning_, 2022.
* Hyvarinen and Dayan [2005] Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* Jeong and Shin [2020] Jongheon Jeong and Jinwoo Shin. Consistency regularization for certified robustness of smoothed classifiers. _Advances in Neural Information Processing Systems_, 33:10558-10570, 2020.
* Jeong et al. [2021] Jongheon Jeong, Sejun Park, Minkyu Kim, Heung-Chang Lee, Doguk Kim, and Jinwoo Shin. SmoothMix: Training confidence-calibrated smoothed classifiers for certified robustness. In _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=nlEQMVBD359.
* Jeong et al. [2023] Jongheon Jeong, Seojin Kim, and Jinwoo Shin. Confidence-aware training of smoothed classifiers for certified robustness. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 8005-8013, 2023. doi: 10.1609/aaai.v37i7.25968. URL https://ojs.aaai.org/index.php/AAAI/article/view/25968.
* Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.
* Krizhevsky [2009] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Department of Computer Science, University of Toronto, 2009.
* Lan et al. [2022] Li-Cheng Lan, Huan Zhang, Ti-Rong Wu, Meng-Yu Tsai, I-Chen Wu, and Cho-Jui Hsieh. Are AlphaZero-like agents robust to adversarial perturbations? In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=yZ_01Za0Czv.
* Lecuyer et al. [2019] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to adversarial examples with differential privacy. In _2019 IEEE Symposium on Security and Privacy (SP)_, pages 656-672. IEEE, 2019.
* Lee [2021] Kyungmin Lee. Provable defense by denoised smoothing with learned score function. In _ICLR Workshop on Security and Safety in Machine Learning Systems_, 2021.
* Levine and Feizi [2021] Alexander J Levine and Soheil Feizi. Improved, deterministic smoothing for \(l_{1}\) certified robustness. In _International Conference on Machine Learning_, pages 6254-6264. PMLR, 2021.
* Li et al. [2020] Linyi Li, Xiangyu Qi, Tao Xie, and Bo Li. SoK: Certified robustness for deep neural networks. _arXiv preprint arXiv:2009.04131_, 2020.
* Li et al. [2022] Linyi Li, Jiawei Zhang, Tao Xie, and Bo Li. Double sampling randomized smoothing. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 13163-13208. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/li22aa.html.
* Liu et al. [2020] Chizhou Liu, Yunzhen Feng, Ranran Wang, and Bin Dong. Enhancing certified robustness via smoothed weighted ensembling, 2020.
* Madry et al. [2018] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
* Mao et al. [2023] Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, and Carl Vondrick. Understanding zero-shot adversarial robustness for large-scale models. In _International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=P4bXcawRi5J.
* Mirman et al. [2018] Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably robust neural networks. In _International Conference on Machine Learning_, volume 80, pages 3578-3586, 10-15 Jul 2018. URL http://proceedings.mlr.press/v80/mirman18b.html.

* Mohapatra et al. [2020] Jeter Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Higher-order certification for randomized smoothing. _Advances in Neural Information Processing Systems_, 33:4501-4511, 2020.
* Mueller et al. [2021] Mark Niklas Mueller, Mislav Balunovic, and Martin Vechev. Certify or predict: Boosting certified robustness with compositional architectures. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=USCNapootw.
* Nichol and Dhariwal [2021] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 8162-8171. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/nichol21a.html.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* Recht et al. [2018] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do CIFAR-10 classifiers generalize to CIFAR-10?, 2018.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. _International Journal of Computer Vision_, 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
* Salman et al. [2019] Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In _Advances in Neural Information Processing Systems 32_, pages 11289-11300. Curran Associates, Inc., 2019.
* Salman et al. [2020] Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico Kolter. Denoised smoothing: A provable defense for pretrained classifiers. _Advances in Neural Information Processing Systems_, 33:21945-21957, 2020.
* Sohl-Dickstein et al. [2015] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* Sukenk et al. [2022] Peter Sukenk, Aleksei Kuvshinov, and Stephan Gunnemann. Intriguing properties of input-dependent randomized smoothing. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 20697-20743. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/sukeni-k22a.html.
* Szegedy et al. [2014] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks, 2014.
* Torralba et al. [2008] Antonio Torralba, Rob Fergus, and William T. Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 30(11):1958-1970, 2008. doi: 10.1109/TPAMI.2008.128.
* Touvron et al. [2021] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 10347-10357. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/touvron21a.html.
* Tramer et al. [2020] Florian Tramer, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and Jorn-Henrik Jacobsen. Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations. In _International Conference on Machine Learning_, pages 9561-9571. PMLR, 2020.
* Tramer et al. [2020] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses. In _Advances in Neural Information Processing Systems_, volume 33, 2020.

* Tsipras et al. [2019] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=Syxab30cY7.
* Wang et al. [2021] Lei Wang, Runtian Zhai, Di He, Liwei Wang, and Li Jian. Pretrain-to-finetune adversarial training via sample-wise randomized smoothing, 2021. URL https://openreview.net/forum?id=Teia22myPIu.
* Wong and Kolter [2018] Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In _International Conference on Machine Learning_, volume 80, pages 5286-5295, 10-15 Jul 2018. URL http://proceedings.mlr.press/v80/wong18a.html.
* Xiao et al. [2023] Chaowei Xiao, Zhongzhu Chen, Kun Jin, Jiongxiao Wang, Weili Nie, Mingyan Liu, Anima Anandkumar, Bo Li, and Dawn Song. DensePure: Understanding diffusion models for adversarial robustness. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=p7hv0J6Gq0i.
* Yang et al. [2020] Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized smoothing of all shapes and sizes. In _International Conference on Machine Learning_, pages 10693-10705. PMLR, 2020.
* Yang et al. [2022] Zhuolin Yang, Linyi Li, Xiaojun Xu, Bhavya Kailkhura, Tao Xie, and Bo Li. On the certified robustness for ensemble models and beyond. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=tUa4REjGjTf.
* Yurtsever et al. [2020] Ekim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda. A survey of autonomous driving: Common practices and emerging technologies. _IEEE Access_, 8:58443-58469, 2020.
* Zhai et al. [2020] Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh, and Liwei Wang. MACER: Attack-free and scalable robust training via maximizing certified radius. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=rJx1Na4Fwr.
* Zhai et al. [2022] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling Vision Transformers. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12104-12113, 2022.
* Zhang et al. [2019] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 7472-7482, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/zhang19p.html.
* Zhang et al. [2020] Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=Skxuk1rfwB.
* Zhang et al. [2022] Huan Zhang, Shiqi Wang, Kaidi Xu, Linyi Li, Bo Li, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. General cutting planes for bound-propagation-based neural network verification. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=ShaJAcofjc.

Related Work

In the context of aggregating multiple smoothed classifiers, several works have previously explored on ensembling smoothed classifiers of the same noise scale [46, 32, 71], in attempt to boost their certified robustness. Our proposed multi-scale framework for randomized smoothing considers a different setup of aggregating smoothed classifiers of different noise scales, with a novel motivation of addressing the current accuracy-robustness trade-off in randomized smoothing. More in this respect, Mueller et al. [51] have considered a selective inference scheme between a "certification" network (for robustness) and a "core" network (for accuracy), and Horvath et al. [33] have adapted the framework into randomized smoothing. Our approach explores an orthogonal direction, and can be viewed as directly improving the "certification" network here: in this way, we could offer improvements not only in empirical but also in _certified_ accuracy as a result.

Independently to our method, there have been approaches to further tighten the certified lower-bound that randomized smoothing guarantees [50, 70, 43, 45, 69]. Mohapatra et al. [50] have shown that higher-order information of smoothed classifiers, _e.g._, its gradient on inputs, can tighten the certification of Gaussian-smoothed classifiers, beyond that only utilizes the zeroth-order information of smoothed confidence. In the context of denoised smoothing [58, 11], Xiao et al. [69] have recently proposed a multi-step, multi-round scheme for diffusion models to improve accuracy of denoising at randomized smoothing. Such approaches could be also incorporated to our method to further improve certification. Along with the developments of randomized smoothing, there have been also other attempts in certifying deep neural networks against adversarial examples [23, 68, 49, 25, 76, 16, 4, 77], where a more extensive survey on the field can be found in Li et al. [44].

## Appendix B Experimental Details

### Datasets

**CIFAR-10**[39] consist of 60,000 images of size 32\(\times\)32 pixels, 50,000 for training and 10,000 for testing. Each of the images is labeled to one of 10 classes, and the number of data per class is set evenly, _i.e._, 6,000 images per each class. By default, we do not apply any data augmentation but the input normalization with mean \([0.5,0.5,0.5]\) and standard deviation \([0.5,0.5,0.5]\), following the standard training configurations of diffusion models. The full dataset can be downloaded at https://www.cs.toronto.edu/~kriz/cifar.html.

**CIFAR-10-C**[27] are collections of 75 replicas of the CIFAR-10 test datasets (of size 10,000), which consists of 15 different types of common corruptions each of which contains 5 levels of corruption severities. Specifically, the datasets includes the following corruption types: (a) _noise_: Gaussian, shot, and impulse noise; (b) _blur_: defocus, glass, motion, zoom; (c) _weather_: snow, frost, fog, bright; and (d) _digital_: contrast, elastic, pixel, JPEG compression. In our experiments, we evaluate test errors on CIFAR-10-C for models trained on the "clean" CIFAR-10 datasets, where the error values are averaged across different corruption types per severity level. The full datasets can be downloaded at https://github.com/hendrycks/robustness.

**CIFAR-10.1**[54] is a reproduction of the CIFAR-10 test set that are separately collected from Tiny Images dataset [62]. The dataset consists of 2,000 samples for testing, and designed to minimize distribution shift relative to the original CIFAR-10 dataset in their data creation pipelines. The full dataset can be downloaded at https://github.com/modestyachts/CIFAR-10.1, where we use the "v6" version for our experiments among those given in the repository.

**ImageNet**[56], also known as ILSVRC 2012 classification dataset, consists of 1.2 million high-resolution training images and 50,000 validation images, which are labeled with 1,000 classes. As a data pre-processing step, we apply a 256\(\times\)256 center cropping for both training and testing images after re-scaling the images to have 256 in their shorter edges, making it compatible with the pre-processing of ADM [18], the backbone diffusion model. Similarly to CIFAR-10, all the images are normalized with mean \([0.5,0.5,0.5]\) and standard deviation \([0.5,0.5,0.5]\). A link for downloading the full dataset can be found in http://image-net.org/download.

**ImageNet-R**[28] consists of 30,000 images of various artistic renditions for 200 (out of 1,000) ImageNet classes: _e.g._, art, cartoons, deviantart, graffiti, embroidery, graphics, origami, paintings, patterns, plastic objects, plush objects, sculptures, sketches, tattoos, toys, video game renditions,and so on. To perform an evaluation of ImageNet classifiers on this dataset, we apply masking on classifier logits for the 800 classes those are not in ImageNet-R. The full dataset can be downloaded at https://github.com/hendrycks/imagenet-r.

**ImageNet-A**[30] consists of 7,500 images of 200 ImageNet classes those are "adversarially" filtered, by collecting natural images that causes wrong predictions from a pre-trained ResNet-50 model. To perform an evaluation of ImageNet classifiers on this dataset, we apply masking on classifier logits for the 800 classes those are not in ImageNet-A. The full dataset can be downloaded at https://github.com/hendrycks/natural-adv-examples.

### Training

**CLIP fine-tuning.** We follow the training configuration suggested by FT-CLIP [19] in fine-tuning the CLIP ViT-B/16@224 model, where the full script is specified at https://github.com/LightDXY/FT-CLIP. The same configuration is applied to all the datasets considered for fine-tuning, namely CIFAR-10, ImageNet, and Gaussian-corrupted versions of CIFAR-10 (for baselines such as "Gaussian" and "Consistency"), which can be done by resizing the datasets into \(224\times 224\) as a data pre-processing. We fine-tune all these tested models for 50 epochs on each of the considered datasets.

**Diffusion fine-tuning.** Similarly to the CLIP fine-tuning, we follow the training details given by Nichol and Dhariwal [52] in fine-tuning diffusion models, which are specified in https://github.com/openai/improved-diffusion. More concretely, here we fine-tune each pre-trained diffusion model by "resuming" the training following its configuration, but with an added regularization term we propose upon the original training loss. We use the 50M-parameter \(32\times 32\) diffusion model from Nichol and Dhariwal [52] for CIFAR-10,6 and the 552M-parameter \(256\times 256\) unconditional model from Dhariwal and Nichol [18] for ImageNet7 to initialize each fine-tuning run. We fine-tune each model for 50K training steps, using batch size 128 and 64 for CIFAR-10 and ImageNet, respectively.

Footnote 6: https://github.com/openai/improved-diffusion

Footnote 7: https://github.com/openai/guided-diffusion

### Hyperparameters

Unless otherwise noted, we use \(p_{0}=0.5\) for _cascaded smoothing_ throughout our experiments. We mainly consider two configurations of cascaded smoothing: (a) \(\sigma\in\{0.25,0.50,1.00\}\), and (b) \(\sigma\in\{0.25,0.50\}\), each of which denoted as "(0.25-1.00)" and "(0.25-0.50)" in tables, respectively. For _diffusion calibration_, on the other hand, we use \(\alpha=1.0\) by default. We use \(\lambda=0.01\) on CIFAR-10 and \(\lambda=0.005\) on ImageNet, unless noted: we halve the regularization strength \(\lambda\) on ImageNet considering its batch size used in the fine-tuning, _i.e._, 128 (for CIFAR-10) _vs._ 64 (for ImageNet). In other words, we follow \(\lambda=0.01\cdot(\texttt{batch\_size}/128)\). Among the baselines considered, "Consistency" [35] requires two hyperparameters: the coefficient for the consistency term \(\eta\) and the entropy term \(\gamma\). Following those considered by Jeong and Shin [35], we fix \(\gamma=0.5\) and \(\eta=5.0\) throughout our experiments.

### Computing infrastructure

Overall, we conduct our experiments with a cluster of 8 NVIDIA V100 32GB GPUs and 8 instances of a single NVIDIA A100 80GB GPU. All the CIFAR-10 experiments are run on a single NVIDIA A100 80GB GPU, including both the diffusion fine-tuning and the smoothed inference procedures. For the ImageNet experiments, we use 8 NVIDIA V100 32GB GPUs per run. In our computing environment, it takes _e.g._, \(\sim\)10 seconds and \(\sim\)5 minutes per image (we use \(n=10,000\) for each inference) to perform a single pass of smoothed inference on CIFAR-10 and ImageNet, respectively. For a single run of fine-tuning diffusion models, we observe \(\sim\)1.5 days of training cost on CIFAR-10 with a single NVIDIA A100 80GB GPU, and that of \(\sim\)4 days on ImageNet using a cluster of 8 NVIDIA V100 32GB GPUs, to run 50K training steps.

At inference time, our proposed cascaded smoothing may introduce an extra computational overhead as the cost for the increased accuracy, _i.e._, from taking multiple times of smoothed inferences. Nevertheless, remark that the actual overhead per input in practice can be different depending on the early-stopping result of cascaded smoothing. For example, in our experiments, we observe that the average inference time to process the CIFAR-10 test set via cascaded smoothing is only \(\sim 1.5\times\) compared to standard (single-scale) smoothed inferences, even considering \(\sigma\in\{0.25,0.5,1.0\}\).

## Appendix C Additional Results

### Comparison with input-dependent smoothing

In a sense that the proposed cascaded smoothing results in different smoothing factors \(\sigma\) per-sample basis, our approach can be also viewed in the context of _input-dependent randomized smoothing_[1, 67, 13, 20], a line of attempts to improve certified robustness by applying different smoothing factor \(\sigma\) conditioned on input. From being more flexible in assigning \(\sigma\), however, these approaches commonly require more assumptions for their certification, making them hard to compare directly upon the standard evaluation protocol, _e.g._, that we considered in our experiments. For example, Sukenfik et al. [60] have proved that such a scheme of freely assigning \(\sigma\) does not provide valid robustness certificates in the standard protocol, and [1, 20] in response have suggested to further assume that all the previous test samples during evaluation can be memorized to predict future samples, which can affect the practical relevance of the overall evaluation protocol.

To address the issue, Sukenfik et al. [60] have also proposed a fix of [1] to discard its test-time dependency, by assuming a more strict restriction on the variation of \(\sigma\). Nevertheless, they also report that the benefit of input-dependent \(\sigma\) then becomes quite limited, supporting the challenging nature of input-dependent smoothing. As shown in Figure 4 that additionally compares our method with [60], we indeed confirm that cascaded smoothing offers significantly wider robustness certificates with a better accuracy trade-off. This is unprecedented to the best of our knowledge, which could be achieved from our completely new approach of data-dependent smoothing: _i.e._, by combining multiple smoothed classifiers through abstention, moving away from the previous attempts that are focused on obtaining a single-step smoothing.

### Clean ensemble for empirical accuracy

In this appendix, we demonstrate that predictions from smoothed classifiers can not only useful to obtain adversarial robustness, but also to improve out-of-distribution robustness of _clean_ classifiers, which suggests a new way to utilize randomized smoothing. Specifically, consider a smoothed classifier \(\tilde{f}\), and its smoothed confidences at \(\mathbf{x}\), say \(p_{f}(y|\mathbf{x})\) for \(y\in\mathcal{Y}\). Next, we consider prediction confidences of a "clean" prediction, namely \(p_{f}(y|\mathbf{x})\). Remark that \(p_{f}(y|\mathbf{x})\) can be interpreted in terms of _logits_, namely \(\log p_{f}(\mathbf{x}|y)\), of a neural network based model \(f\) as follows:

\[p_{f}(y|\mathbf{x})=\frac{p_{f}(\mathbf{x},y)}{p_{f}(\mathbf{x})}=\frac{p(y) \cdot p_{f}(\mathbf{x}|y)}{\sum_{y\in\mathcal{Y}}p_{f}(\mathbf{x},y)}.\] (10)

Note that the prior \(p(y)\) is usually assumed to be the uniform distribution \(\mathcal{U}(\mathcal{Y})\). Here, we consider a variant of this prediction, by replacing the prior \(p(y)\) by a _mixture_ of \(p(y)\) and a _smoothed prediction_

[MISSING_PAGE_FAIL:19]

### Ablation study

Effect of \(\lambda\).In Table 7, we jointly ablate the effect of tuning \(\lambda\in\{0.001,0.01,0.1\}\) upon the choices of \(\alpha\in\{1.0,2.0,4.0,8.0\}\), by comparing its certified accuracy of cascaded smoothing as well as their empirical accuracy on CIFAR-10 test set. Overall, we observe that increasing \(\lambda\) has an effect of improving robustness at large radii, with a slight decrease in the clean accuracy as well as the empirical accuracy, where using higher \(\alpha\) could compensate this at some extent. Although we generally observe effectiveness of the proposed regularization for a wide range of \(\lambda\), using higher values of \(\lambda\), _e.g._, \(\lambda=0.1\), often affects the base diffusion model and results in a degradation in accuracy without a significant gain in its certified robustness.

Effect of \(p_{0}\).Table 8, on the other hand, reports the effect of \(p_{0}\) we introduced for cascaded smoothing. Recall that we use \(p_{0}=0.5\) throughout our experiments by default. We observe that using a bit of higher values of \(p_{0}\), _e.g._, \(p_{0}=0.6\) could be beneficial to improve the clean accuracy of cascaded smoothing, at the cost of robustness at larger radii: for example, it improves the certified accuracy at \(r=0.0\) by \(81.1\to 83.9\). The overall certified robustness starts to decrease as \(p_{0}\) further increases, due to its effect of increasing abstention rate. One could still observe the consistent improvement in the empirical accuracy as \(p_{0}\) increases, _i.e._, by also considering abstain as a valid prediction, _i.e._, using high values of \(p_{0}\) could be useful in certain scenarios. We think a more detailed choice of \(p_{0}\) per \(\sigma\) can further boost the performance of cascaded smoothing, which is potentially an important practical consideration.

Critical \(\sigma\).Although it is not required for cascaded smoothing to guarantee the stability of smoothed confidences across \(\sigma\) (_e.g._, outside of \(\{0.25,0.5,1.0\}\) in our experiments), we observe that smoothed confidences from denoised smoothing usually interpolate smoothly between different \(\sigma\)'s, at least for the diffusion denoised smoothing pipeline. In this respect, we consider a new concept of _critical_\(\sigma\) for each sample, by measuring the threshold of \(\sigma\) where its confidence goes below \(p_{0}=0.5\). Figure 6 examines this measure on CIFAR-10 training samples and plots its distribution for each class as histograms. Interestingly, we observe that the distributions are often significantly biased, _e.g._, the "Ship" class among CIFAR-10 classes relatively obtains much higher critical \(\sigma\) - possibly due to its peculiar background of, _e.g._, ocean.

Per-\(\sigma\) distribution from cascading.To further verify that per-sample \(\sigma\)'s from cascaded smoothing are diverse enough to achieve better robustness-accuracy trade-off, we examine in Figure 5 the actual histograms of the resulting \(\sigma\in\{\texttt{ABSTAIN},0.25,0.50,1.00\}\) on CIFAR-10, also varying \(p_{0}\in\{0.5,0.6,0.7,0.8,0.9\}\). Overall, we observe that (a) the distributions of \(\sigma_{x}\) indeed widely cover the total range of \(\sigma\) for every \(p_{0}\)'s considered, and (b) using higher \(p_{0}\) can further improve diversity of the distribution while its chance to be abstained can be also increased.

## Appendix D Technical Details

### Proof of Theorem 3.1

Our proof is based on the fact that any smoothed classifier \(\hat{f}\) composed with \(\Phi^{-1}(\cdot)\) is always Lipschitz-continuous, which is shown by Salman et al. [57]:

Figure 6: Comparison of per-class histograms of _critical_\(\sigma\): the minimum value of \(\sigma\) for each input where the smoothed confidence at true class falls below \(0.5\). We use the whole CIFAR-10 training samples for the analysis. Each image is smoothed with \(n=1,000\) samples of Gaussian noise, while iterating \(\sigma\) within the range \(\sigma\in[0.0,2.0]\). Images with critical \(\sigma\) larger than \(2.0\) are truncated.

**Lemma D.1** (followed by Salman et al. [57]).: _Let \(\Phi(a):=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{a}\exp\left(-\frac{1}{2}s^{2}\right) \mathrm{d}s\) be the cumulative distribution function of standard Gaussian. For any function \(f:\mathbb{R}^{d}\to[0,1]\), the map \(\mathbf{x}\mapsto\sigma\cdot\Phi^{-1}(p_{\hat{f}_{\sigma}}(\mathbf{x}))\) is 1-Lipschitz for any \(\sigma>0\)._

Given Lemma D.1, the robustness certificate by Cohen et al. [15] in (2) (of the main text) can be viewed as computing the region of \(p_{\hat{f}_{\sigma}}(\mathbf{x}+\boldsymbol{\delta};y)>0.5\), _i.e._, where it is guaranteed to return \(y\) over other (binary) smoothed classifiers \(p_{\hat{f}_{\sigma}}(\mathbf{x}+\boldsymbol{\delta};y^{\prime})\) of \(y\neq y^{\prime}\).

In general, as in our proposed _cascaded smoothing_, one can consider a more "strict" region defined by \(p_{\hat{f}_{\sigma}}(\mathbf{x}+\boldsymbol{\delta};y)>p_{0}\) for some \(p_{0}\geq 0.5\). Here, the region corresponds to a policy that \(\hat{f}_{\sigma}\) abstains whenever \(p_{\hat{f}_{\sigma}}(\mathbf{x})\leq p_{0}\). Accordingly, the certified radius one can obtain from this policy becomes:

\[R_{p_{0}}(\hat{f}_{\sigma};\mathbf{x},y):=\sigma\cdot\left(\Phi^{-1}(p_{\hat{ f}_{\sigma}}(\mathbf{x},y))-\Phi^{-1}(p_{0})\right).\] (13)

With this notation, we restate Theorem 3.1 into a tighter form to show, which do not consider neither \(\underline{p}\) nor \(\overline{p}\) upon the knowledge of \(p_{\hat{f}_{\sigma_{k}}}\) and implies Theorem 3.1 as a consequence:

**Theorem 3.1** (restated).: _Let \(\hat{f}_{\sigma_{1}},\cdots,\hat{f}_{\sigma_{K}}:\mathcal{X}\to\mathcal{Y}\) be smoothed classifiers with \(0<\sigma_{1}<\cdots<\sigma_{K}\). Suppose \(\mathtt{case}(\mathbf{x};\{\hat{f}_{\sigma_{i}}\}_{i=1}^{K})=:\hat{y}\in \mathcal{Y}\) halts at \(\hat{f}_{\sigma_{k}}\) for some \(k\). Then, it holds that \(\mathtt{case}(\mathbf{x}+\boldsymbol{\delta};\{\hat{f}_{\sigma_{i}}\}_{i=1}^{ K})=\hat{y}\) for \(\|\boldsymbol{\delta}\|<R\), where:_

\[R:=\min\left\{R_{p_{0}}(\hat{f}_{\sigma_{i}};\mathbf{x},\hat{y}),R^{-}\right\},\text{ and }R^{-}:=\min_{\begin{subarray}{c}y\neq\hat{y}\\ k^{\prime}>k\end{subarray}}\left\{\min\left\{0,-R_{p_{0}}(\hat{f}_{\sigma_{k^ {\prime}}};\mathbf{x},y)\right\}\right\}.\] (14)

Proof.: By the definition of \(\mathtt{case}(\mathbf{x})\), one can ensure \(\mathtt{case}(\mathbf{x}+\boldsymbol{\delta})=\hat{y}\) if the following holds:

* \(\hat{f}_{\sigma_{k}}(\mathbf{x}+\boldsymbol{\delta})=\hat{y}\) with \(p_{\hat{f}_{\sigma_{k}}}(\mathbf{x}+\boldsymbol{\delta},\hat{y})>p_{0}\), and
* \(p_{\hat{f}_{\sigma_{k^{\prime}}}}(\mathbf{x}+\boldsymbol{\delta})<p_{0}\) or \(\hat{f}_{\sigma_{k^{\prime}}}(\mathbf{x}+\boldsymbol{\delta})=\hat{y}\), for every \(k^{\prime}=k+1,\cdots,K\).

Remark that the condition (a) is satisfied by \(\boldsymbol{\delta}\) within \(\|\boldsymbol{\delta}\|<R_{p_{0}}(\hat{f}_{\sigma_{k}};\mathbf{x},\hat{y})\). Specifically, one can consider the "certifiable" region of \(\boldsymbol{\delta}\) at \(\hat{f}_{\sigma_{k}}\) as follows:

\[\mathcal{R}_{k}=\{\boldsymbol{\delta}\in\mathcal{X}:\|\boldsymbol{\delta}\|<R _{p_{0}}(\hat{f}_{\sigma_{k}};\mathbf{x},\hat{y})\}.\] (15)

For the condition (b), on the other hand, note that the condition is equivalent to that:

* \(p_{\hat{f}_{\sigma_{k^{\prime}}}}(\mathbf{x}+\boldsymbol{\delta};y)<p_{0}\) for \(y\neq\hat{y}\), and for every \(k^{\prime}=k+1,\cdots,K\).

Fix \(k^{\prime}\in\{k+1,\cdots,K\}\). From the Lipschitzedness of \(\mathbf{x}\mapsto\sigma\cdot\Phi^{-1}(p_{\hat{f}_{\sigma}}(\mathbf{x};y))\) by Lemma D.1, it is clear to show that \(\boldsymbol{\delta}\)'s inside the following region \(\mathcal{R}_{k^{\prime}}\) satisfies (\(\mathbf{b^{\prime}}\)):

\[\mathcal{R}_{k^{\prime}} =\bigcap_{y\neq\hat{y}}\left\{\boldsymbol{\delta}\in\mathcal{X}: \|\boldsymbol{\delta}\|<\min\left\{0,-R_{p_{0}}(\hat{f}_{\sigma_{k^{\prime}}}; \mathbf{x},y)\right\}\right\}\] (16) \[=\left\{\boldsymbol{\delta}\in\mathcal{X}:\|\boldsymbol{\delta}\| <\min_{y\neq\hat{y}}\left\{\min\left\{0,-R_{p_{0}}(\hat{f}_{\sigma_{k^{\prime}}}; \mathbf{x},y)\right\}\right\}\right\}.\] (17)

Lastly, the result is followed by considering the intersection of \(\mathcal{R}_{k}\)'s which represent a certifiable region where \(\mathtt{case}(\mathbf{x}+\boldsymbol{\delta};\{\hat{f}_{\sigma}\}_{i=1}^{K})= \hat{y}\):

\[\mathcal{R}:=\mathcal{R}_{k}\cap(\cap_{k^{\prime}\geq k+1}\mathcal{R}_{k^{ \prime}})=\{\boldsymbol{\delta}\in\mathcal{X}:\|\boldsymbol{\delta}\|<R\}.\] (18)

Once we have the restated version of Theorem 3.1 above, the original statement follows from an observation that the new radius by introducing either \(\underline{p}\) or \(\overline{p}\), say \(\underline{R}\), always lower-bounds \(R\) of the above: _i.e._, \(\underline{R}\leq R\) for any choices of \(\underline{p}\) and \(\overline{p}\), thus certifiable for the region as well.

### Certification and prediction

For practical uses of cascaded smoothing, where the exact values of \(p_{\hat{f}_{\sigma_{k}}}(\mathbf{x},y)\) are not available, one has to estimate the smoothed confidence values as per Theorem 3.1, _i.e._, by properly lower-bound (or upper-bound) the confidence values. More concretely, one is required to obtain (a) \(\underline{p}_{\hat{f}_{\sigma_{k}}}(\mathbf{x})\) at the halting stage \(k\), and (b) \(\overline{p}_{\hat{f}_{\sigma_{k^{\prime}}}}(\mathbf{x},y)\) for \(k^{\prime}=k+1,\cdots,K\) and \(y\in\mathcal{Y}\).

For both estimations, we use Monte Carlo algorithm with _n i.i.d._ Gaussian samples, say \(\{\boldsymbol{\delta}_{i}\}\), and perform a majority voting across \(\{f_{\sigma}(\mathbf{x}+\boldsymbol{\delta}_{i})\}\) to obtain a histogram on \(\mathcal{Y}\) of \(n\) trials:

1. To estimate a lower-bound \(\underline{p}_{\hat{f}_{\sigma_{k}}}(\mathbf{x})\), we follow the statistical procedure by Cohen et al. [15], namely \(\textsc{Certify}(n,n_{0},\alpha)\), which additionally takes two hyper-parameters \(n_{0}\) and \(\alpha\): here, \(n_{0}\) denotes the number of samples to initially make a guess to estimate a smoothed prediction, and \(\alpha\) is the significance level of the Clopper-Pearson confidence interval [14]. Concretely, it sets \(\underline{p}_{\hat{f}_{\sigma_{k}}}\) as the lower confidence level of \(p_{\hat{f}_{\sigma_{k}}}\) with coverage \(1-\alpha\).
2. For \(\overline{p}_{\hat{f}_{\sigma_{k^{\prime}}}}(\mathbf{x},y)\), on the other hand, we first obtain voting counts from \(n\) Gaussian trials, say \(\mathbf{c}_{\mathbf{x}^{\prime}}^{(\sigma_{k^{\prime}})}\), and adopt Goodman confidence interval to bound multinomial proportions [24] with the significance level \(\alpha\). Specifically, one can set \(\overline{p}_{\hat{f}_{\sigma_{k^{\prime}}}}(\mathbf{x},y)\) for \(y\in\mathcal{Y}\) by the \((1-\alpha)\)-upper level of the (multinomial) confidence interval. This can be implemented by using, _e.g._, \(\textsc{multinomial\_proportions\_confint}\) of the \(\textsc{statsmodels}\) library.

We make an additional treatment on the significance level \(\alpha\): unless the pipeline halts at the first stage \(K\), cascaded smoothing makes a \((K-k+1)\)-consecutive testing of confidence intervals to make a prediction. In attempt to maintain the overall significance level of this prediction as \(\alpha\), one can apply the Bonferroni correction [5] on the significance level of individual testing, specifically by considering \(\frac{\alpha}{(K-k+1)}\) as the adjusted significance level (instead of using \(\alpha\)) at the stage \(k\).

## Appendix E Other Possible Designs

In this appendix, we present two alternative designs of multi-scale randomized smoothing we have also considered other than _cascaded smoothing_ proposed in the main text. Specifically, here we additionally propose two different policies of aggregating multiple smoothed classifiers, each of which dubbed as the (a) _max-radius policy_, and (b) _focal smoothing_, respectively. Overall, both design do provide valid certified guarantees combining the accuracy-robustness trade-offs across multiple smoothing factors, often attaining a better robustness even compared to cascaded smoothing when \(n\), the number of noise samples, is large enough.

The common drawback in the both approaches here, however, is the necessity to access every smoothed confidence, which can be crucial in practice in both terms of efficiency and accuracy. Specifically, for given \(K\) smoothed classifiers, this implies the exact \(K\)-times increase in the inference cost, which makes it much costly on average compared to cascaded smoothing. The need to access and compare with \(K\) smoothed confidence in practice also demands more accurate estimations of the confidence values to avoid abstaining. For example, these approaches can significantly degrade the certified accuracy in practice where \(n\) cannot be large unlike the certification phase: which again makes our cascaded smoothing more favorable over the approaches presented here.

### Max-radius policy

Consider \(K\) smoothed classifiers, say \(\hat{f}_{\sigma_{1}},\cdots,\hat{f}_{\sigma_{K}}:\mathcal{X}\to\mathcal{Y}\), where \(0<\sigma_{1}<\cdots<\sigma_{K}\). The _max-radius_ policy, say \(\textsc{maxR}(\cdot)\), can be another simple policy one can consider for given \(\{\hat{f}_{\sigma_{i}}\}\):

\[\textsc{maxR}(\mathbf{x};\{\hat{f}_{\sigma_{i}}\}_{i=1}^{K}):=\hat{f}_{\sigma^ {*}}(\mathbf{x}),\text{ where }\sigma^{*}:=\operatorname*{arg\,max}_{\sigma_{i}} \sigma_{i}\cdot\Phi^{-1}\left(p_{\hat{f}_{\sigma_{i}}}(\mathbf{x})\right).\] (19)

Again, given the Lipschitzedness associated with the smoothed confidences \(p_{\hat{f}_{\sigma}}(\mathbf{x},y)\), it is easy to check the following robustness certificate for \(\textsc{maxR}(\mathbf{x};\{\hat{f}_{\sigma_{i}}\}_{i=1}^{K})\):

**Theorem E.1**.: _Let \(\hat{f}_{\sigma_{1}},\cdots,\hat{f}_{\sigma_{K}}:\mathcal{X}\rightarrow\mathcal{Y}\) be smoothed classifiers with \(0<\sigma_{1}<\cdots<\sigma_{K}\). If \(\mathtt{maxR}(\mathbf{x};\{\hat{f}_{\sigma_{i}}\}_{i=1}^{K})=:\hat{y}\in \mathcal{Y}\), it holds that \(\mathtt{maxR}(\mathbf{x}+\bm{\delta};\{\hat{f}_{\sigma_{i}}\}_{i=1}^{K})=\hat {y}\;\) for \(\|\bm{\delta}\|<R\), where:_

\[R:=\frac{1}{2}\cdot\left(\sigma^{*}\cdot\Phi^{-1}\left(p_{\hat{f}_{\sigma^{*} }}(\mathbf{x})\right)-\max_{\begin{subarray}{c}y\neq\hat{y}\\ i=1,\cdots,K\end{subarray}}\sigma_{i}\cdot\Phi^{-1}\left(p_{\hat{f}_{\sigma_{ i}}}(\mathbf{x};y)\right)\right).\] (20)

Remark that the certified radius given by (20) recovers the guarantee of Cohen et al. [15] if \(K=1\).

### Focal smoothing

In this method, coined _focal smoothing_, we take a somewhat different approach to achieve multi-scale smoothing. Here, now we assume two different levels of smoothing factor, say \(\sigma_{0}>\sigma_{1}>0\), and consider inputs that contains noise with _spatially different_ magnitudes. Specifically, we consider a binary mask \(M\in\{0,1\}^{d}\) and an input corrupted by the following:

\[\mathtt{focal}(\mathbf{x},M;\sigma_{0},\sigma_{1}):=\mathbf{x}+(\sigma_{0} \cdot(1-M)+\sigma_{1}\cdot M)\odot\bm{\epsilon},\] (21)

where \(\odot\) denotes the element-wise product and \(\bm{\epsilon}\sim\mathcal{N}(0,\mathbf{I})\). In this way, each of corrupted input \(\hat{\mathbf{x}}\) gets a "hint" to recover the original content of \(\mathbf{x}\) by observing a less-corrupted region marked by \(M\).

Randomized smoothing over such _anisotropic_ noise has been recently studied [22; 20]: in a nutshell, smoothing over noises specified by (21) could produce an _ellipsoid-shaped_ certified region with two axes of length \(\sigma_{0}\Phi^{-1}(p)\) and \(\sigma_{1}\Phi^{-1}(p)\) for its smoothed confidence \(p\), respectively.

In terms of certified radius, however, this implies that one can still certify only the region limited by the shorter axes (of length \(\sigma_{1}\Phi^{-1}(p)\)). It turns out that such a limitation can be avoided by considering an "ensemble" of \(K\) multiple orthogonal masks, say \(M_{1},\cdots,M_{K}\), so that the shorter axes of ellipsoids from the masks do not overlap each other. Specifically, we observe the following:

**Theorem E.2**.: _Let \(M_{1},\cdots,M_{K}\in\{0,1\}^{d}\) be \(K\) orthogonal masks and \(\sigma_{0}>\sigma_{1}>0\) be smoothing factors, and \(p_{k}(\mathbf{x})\) be the smoothed confidence from inputs corrupted by \(\mathtt{focal}(\mathbf{x},M_{k};\sigma_{0},\sigma_{1})\), for a given binary classifier \(f_{k}:\mathbb{R}^{d}\rightarrow[0,1]\). Define \(\hat{p}:=\frac{1}{K}\sum_{k}p_{k}\), and suppose \(\hat{p}(\mathbf{x})>0.5\). Then, it is certified that \(\hat{p}(\mathbf{x}+\bm{\delta})>0.5\) for \(\|\bm{\delta}\|<R_{\mathbf{b}^{*}}\), where \(\mathbf{b}^{*}\) is the solution of the following optimization:_

\[\underset{\mathbf{b}}{\text{minimize}} R_{\mathbf{b}}:=\frac{1}{K}\sum_{k}\frac{a_{k}}{\sqrt{t^{2}b_{k}+1}}\] (22) _subject to_ \[\sum_{k}b_{k}=1,\] \[b_{k}\geq 0,\;k=1,2,\cdots,K,\]

_where \(a_{k}:=\sigma_{0}\Phi^{-1}(p_{k})\) for \(k=1,\cdots,K\), and \(t^{2}:=\left(\frac{\sigma_{0}^{2}}{\sigma_{1}^{2}}-1\right)\)._

In Algorithm 1, we propose an efficient 1-dimensional grid-search based optimization to solve (22), based on observations given as Lemma E.3 and E.4: each of which identifies the condition of \(b_{k}\) when it attains either the maximum value or zero, respectively. By solving the optimization, one could obtain a certified radius \(R\) that the aggregated smoothing can guarantee consistent prediction.

**Lemma E.3**.: _Consider the constrained optimization (22) with respect to \(\mathbf{b}=(b_{1},\cdots,b_{K})\). Let \(M_{\mathbf{a}}:=\{m\in[K]:a_{m}=\max_{k}a_{k}\}\) and \(\mathbf{b}^{*}\) be an optimal solution of (22). Then, it holds that \(b_{m}^{*}=b_{m^{\prime}}^{*}>0\) for any \(m,m^{\prime}\in M_{\mathbf{a}}\)._

Proof.: First, we show that there exists at least one \(m\in M_{\mathbf{a}}\) where \(b_{m}^{*}\) is nonzero, _i.e._, \(b_{m}^{*}>0\). Suppose the contrary, _i.e._, \(b_{k}^{*}=0\) for all \(k\in M_{\mathbf{a}}\). Denote the corresponding objective value as \(R^{*}\). Given the condition \(\sum_{k}b_{k}=1\), there always exists \(i\notin M\) where \(b_{i}>0\). Now, consider swapping the allocations of \(b_{m}\) and \(b_{i}\) for some \(m\in M_{\mathbf{a}}\) and denote the corresponding objective value \(R^{\prime}\)Then, we have:

\[R^{\prime}-R^{*} =\frac{1}{K}\left(\frac{a_{k}}{\sqrt{t^{2}b_{j}+1}}+\frac{a_{j}}{ \sqrt{t^{2}b_{k}+1}}-\frac{a_{k}}{\sqrt{t^{2}b_{k}+1}}-\frac{a_{j}}{\sqrt{t^{2}b _{j}+1}}\right)\] \[=\frac{1}{K}\left(a_{k}\left(\frac{1}{\sqrt{t^{2}b_{j}+1}}-\frac{1 }{\sqrt{t^{2}b_{k}+1}}\right)+a_{j}\left(\frac{1}{\sqrt{t^{2}b_{k}+1}}-\frac{1 }{\sqrt{t^{2}b_{j}+1}}\right)\right)\] \[<\frac{1}{K}\left(a_{k}\left(\frac{1}{\sqrt{t^{2}b_{j}+1}}-\frac{1 }{\sqrt{t^{2}b_{k}+1}}\right)+a_{k}\left(\frac{1}{\sqrt{t^{2}b_{k}+1}}-\frac{1 }{\sqrt{t^{2}b_{j}+1}}\right)\right)=0,\] (23)

given that \(a_{k}>a_{j}\) and \(\frac{1}{\sqrt{t^{2}b_{j}+1}}<\frac{1}{\sqrt{t^{2}b_{k}+1}}=1\). This means \(R^{\prime}<R^{*}\), contradicting the optimality of the initial solution \(\mathbf{b}^{*}\) with \(b_{k}=0\) for all \(k\in M_{\mathbf{a}}\). Thus, there must exist at least one \(b_{m}^{*}>0\) for some \(m\in M_{\mathbf{a}}\).

Next, we show \(b_{m}^{*}=b_{m^{\prime}}^{*}\) for any \(m,m^{\prime}\in M_{\mathbf{a}}\). Suppose there exist such \(m,m^{\prime}\in M_{\mathbf{a}}\) where \(b_{m}^{*}\neq b_{m^{\prime}}^{*}\). Due to the symmetry, one can construct another optimal allocation \(\mathbf{b}^{\prime}\) by swapping the two allocations \(b_{m}^{*}\) and \(b_{m^{\prime}}^{*}\), as it does not affect the optimality. Now consider another allocation \(\bar{\mathbf{b}}\) by averaging the two allocations:

\[\bar{b}_{m}=\bar{b}_{m^{\prime}}=\frac{b_{m}^{*}+b_{m^{\prime}}^{*}}{2}.\] (24)

Let the corresponding objective value \(\bar{R}\). Note that \(R\) is strictly convex, which it implies that \(\bar{R}<R^{*}\), contradicting the optimality of the initial solution \(\mathbf{b}^{*}\). Thus, it holds \(b_{m}^{*}=b_{m^{\prime}}^{*}\) for any \(m,m^{\prime}\in M_{\mathbf{a}}\). 

**Lemma E.4**.: _Consider the constrained optimization (22) with \(\mathbf{b}^{*}\) as an optimal solution. Denote \(B:=b_{m}^{*}=b_{m^{\prime}}^{*}>0\) for all \(m,m^{\prime}\in M_{\mathbf{a}}\). Then, \(b_{k}^{*}=0\) if and only if \(a_{k}\leq\frac{A}{(t^{2}B+1)^{3/2}}\)._

Proof.: Suppose \(b_{k}^{*}=0\). From the Karush-Kuhn-Tucker (KKT) stationarity condition, we have \(\frac{-a_{k}t^{2}}{K}+\lambda-\mu_{k}=0\). Since \(\lambda,\mu_{k}\geq 0\), we obtain \(\lambda\geq\frac{a_{k}t^{2}}{K}\). Now, consider the stationarity condition for \(m\in M\): \(-\frac{At^{2}}{K(\sqrt{t^{2}B+1})^{3}}+\lambda=0\). Substituting the lower bound for \(\lambda\), we have \(a_{k}\leq\frac{A}{(t^{2}B+1)^{3/2}}\). Conversely, suppose \(a_{k}\leq\frac{A}{(t^{2}B+1)^{3/2}}\). Then, \(b_{k}^{*}>0\) contradict to that \(b_{k}^{*}=\frac{1}{t^{2}}\left(\left(\frac{a_{k}}{A}\right)^{2/3}\left(t^{2}B+ 1)-1\right)\leq 0\), hence \(b_{k}^{*}=0\).