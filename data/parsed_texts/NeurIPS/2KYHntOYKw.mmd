# Cooperative Learning for Cost-Adaptive Inference

Xingli Fang

Computer Science

North Carolina State University

&Richard Bradford

Collins Aerospace

&Jung-Eun Kim

Computer Science

North Carolina State University

###### Abstract

We propose a cooperative training framework for deep neural network architectures that enables the runtime network depths to change to satisfy dynamic computing resource requirements. In our framework, the number of layers participating in computation can be chosen dynamically to meet performance-cost trade-offs at inference runtime. Our method trains two Teammate nets and a Leader net, and two sets of Teammate sub-networks with various depths through knowledge distillation. The Teammate nets derive sub-networks and transfer knowledge to them, and to each other, while the Leader net guides Teammate nets to ensure accuracy. The approach trains the framework atomically at once instead of individually training various sizes of models; in a sense, the various-sized networks are all trained at once, in a "package deal." The proposed framework is not tied to any specific architecture but can incorporate any existing models/architectures, therefore it can maintain stable results and is insensitive to the size of a dataset's feature map. Compared with other related approaches, it provides comparable accuracy to its full network while various sizes of models are available.

## 1 Introduction

As deep neural network architectures continue to grow in size and complexity, it is increasingly costly to store and execute them. The trend is especially challenging in resource-constrained environments, such as embedded platforms or edge devices, where having a succinct model is a precondition for meeting time and space constraints. Hence, to enable systems to function in dynamically changing running environments and platforms under diverse resource allowances, we propose a novel training framework for adaptive real-time inferences. Note that, for the purpose of this work, the resource cost matters only in inference but not in training.

To achieve efficient models, several authors took the approach of re-designing the neural network architecture to have fewer depths or widths, or to have more computation-efficient kernels [15, 5, 12, 31, 11]. Another approach involved removing/deactivating insignificant neurons, as shown in [29, 40, 25]. Yet another alternative is knowledge distillation [8] which trains a smaller-size network by learning the distribution explicitly or implicitly from an original full-size network of high accuracy. As opposed to such static approaches, which individually customize a model for each specific task/platform or a certain inference path in advance, some other efforts provided various options to accommodate different platforms by scaling widths and depths as shown in [7, 4, 35]. Although the aforementioned approaches can trade off performance and resource with different model sizes, they must _pre_-define the requirements of computing resources in advance. Moreover, those architectures must be trained multiple times, specifically for each individual model size.

Unlike the existing work, our approach is able to provide an agile prediction at any inference cost on the fly; also, it is not tied to any specific architecture but can adopt any refined models. We propose a novel pipeline of knowledge distillation, namely, _Cooperative_ training framework, which employs a cohort of three cooperating networks, two _Teammate nets_ and _Leader net_. Teammate networks offerthe soft labels to each other, and each of them derives sub-networks and transfers knowledge to the sub-networks. The Leader network is an auxiliary network learning from true labels and guiding Teammate nets to ensure accuracy. In particular, our approach trains the framework atomically at once instead of individually training various sizes of models; it does not simply train with labels and losses but also learns from numerous smaller networks at the same time. So, in a sense, the various-sized networks are all trained at once, in a "package deal," maintaining accuracy that is competitive with the original model.

## 2 Related Work

### Static Approach to Compact Networks

To obtain a compact neural network, one approach is to re-design its architecture with fewer depths or widths, or with more computation-efficient kernels. Some manual network designs, such as SqueezeNet series [15; 5] and MobileNet series [12; 31; 11], have been applied to explore more efficient model architectures. Another way to reduce redundancy from existing models is to estimate the importance of neurons in a neural network and remove unimportant neurons. Some works explored how to estimate the importance, such as [29; 10], or deactivate unimportant neurons of a large network with sparse regularization [40] or greedy algorithm [25]. Another approach is knowledge distillation [8] which transfers the distribution of large networks to the light networks explicitly.

### Scalable Architectures

As to scalable structures, prior works such as VGG series [32; 2], EfficientNet series [35; 34], MobileNet series [12; 31; 11] and others [30; 13] have proposed some architectures with scalable depth or width to meet the accuracy and static resource constraints. For instance, MobileNet series proposed three versions of MobileNet and adjusted the channel size of the main calculation cost part. One more example is ResNet [7] which adjusts the depth of the network by adjusting the number of repetitive convolution blocks at different stages divided by the down-sampling layers.

### Adaptive Neural Networks

The whole idea of adaptive neural networks is to train a network to fit in a given environment and platform with various computing resources available to meet the dynamic constraints of real-world applications. An intuitive approach to training smaller networks of various sizes from a full-sized model is attaching intermediate (early) output branches on the forward pass, and training all the branches together as presented in [36] or [43]. [36] trained networks with multiple early exits. In [43], early exits imitate the main output's distribution to achieve better performance. However, such frameworks provide only a few choices, and the models' capacity or thresholds on the confidence need to be fixed in advance. More recent ideas are to re-design the structure of networks and optimize the training. [16] developed an adaptive deep neural network architecture that gradually adds layers to provide flexible options available concerning different timing constraints. [14] designed a stepped structure, MSD-Net, to produce time-adaptive classification results. [38] proposed RANet that offers an exiting branch at each stage and gradually adds more layers. In other studies, the width of networks is explored. [42; 22] tried training adaptive width networks by knowledge distillation. [41] proposed US-Net for selecting any channel width of convolution networks trained with self-distillation. Similarly, [9] trained a Dynamic BERT with dynamic depths and widths. Based on US-Net, [39] proposed MutualNet and pointed out that multi-scale inputs can help an adaptive model

Figure 1: Overview of different knowledge distillations. (d) is enlarged in Fig. 2(b).

be trained better. [20] proposed a CGM module making a decision on pruning a layer by considering the outputs of preceding layers and required scales. Li et al. [21] built DS-Net, including an online network and target sub-network, with a dynamic gating to choose a proper channel pruning rate.

### Knowledge Distillation

The lottery ticket hypothesis [3] suggests that the existing refined neural networks have the potential to retain comparable accuracy even after removing or deactivating some nodes. One well-known way to achieve this is knowledge distillation. The original approach to knowledge distillation is to train a smaller-size neural network against a full-size neural network's outputs so that the small-size network can perform competitively. Moreover, [43] introduced a knowledge distillation method to train parameter-shared networks by creating multiple branches for different costs of inference. [42] applied a similar training policy at the channel level and switchable batch normalization to solve the problem of different means and variances of the aggregated feature led by ever-changing input channel size in preceding layers. [41] extended this adaptability from only several fixed ratios to any channel pruning rate according to the sandwich rule (see Sec. 4.3). [44] let two models from the same architecture learn each other's distribution of outputs to enhance the performance of both models. [28] introduced a teaching assistant network (an intermediate-size homogeneous network) between the teacher and student network to help the student network achieve more stable and better performance. [6] proposed an ensemble mutual learning and explored how to produce a good ensemble output.

## 3 Background

### Knowledge Distillation

In a typical knowledge distillation [8] as in Fig. 1(a), there is a teacher and a student network. A teacher net is trained first, and then its outputs and accurate labels "teach" (_i.e._, train) a student net. The training process for the student network is described as,

\[\mathcal{L}_{s}=(1-\lambda)\mathcal{L}_{ce}+\lambda\mathcal{L}_{kl}\] (1)

where \(\mathcal{L}_{ce}\) is a cross-entropy loss, \(\mathcal{L}_{kl}\) is Kullback-Leibler divergence loss [18], and \(\lambda\) is to balance two losses. Before being fed into a softmax activation function of the teacher and student net, let each last input be \(a_{t}\) and \(a_{s}\), respectively. Then, the output of teacher net is \(y_{t}=\mathsf{softmax}(a_{t}/\tau)\), where \(\tau\), as the temperature, is to adjust the smoothness of probability distribution, and that of student net is \(y_{s}=\mathsf{softmax}(a_{s}/\tau)\). Thus, \(\mathcal{L}_{kl}\) is represented as,

\[\mathcal{L}_{kl}=\tau^{2}KLDiv(y_{s},y_{t})\] (2)

where \(KLDiv(\cdot,\cdot)\) is Kullback-Leibler divergence loss function and \(\tau\) is typically greater than or equal to \(1\). \(\mathcal{L}_{ce}\) is formulated as,

\[\mathcal{L}_{ce}=CE(y_{s},y_{true})\] (3)

where \(CE(\cdot,\cdot)\) is a cross entropy loss function and \(y_{true}\) is a true label from supervised dataset.

### Mutual Knowledge Distillation

With an observation that logits of one model can help another model improve its performance, _mutual knowledge distillation_ (deep mutual learning) was proposed by [44]. As depicted in Fig. 1(b), two models can learn from each other using formula (1), and significant improvement has been shown in each model's performance.

### Self-Distillation

In self-distillation (inplace distillation), the teacher net becomes the student net itself, in which a student net is a subset of the full net (Fig. 1(c).) There is no pre-trained reference net since there is only a single net to be trained. According to [42; 41; 43; 39; 21], a common loss calculation is,

\[\mathcal{L}_{sl}=CE(y_{full},y_{true})+\lambda\sum_{i=1}^{n}\tau^{2}KLDiv(y_{ sub,i},y_{full})\] (4)

where \(y_{full}\) is the output from a full network, \(y_{sub,i}\) is the output from \(i\)-th sub-network, \(n\) is the total number of sub-networks designed manually prior to training, and \(\lambda\) is a hyper-parameter to balance the two types of losses.

[MISSING_PAGE_FAIL:4]

[MISSING_PAGE_FAIL:5]

Interactive LearningWith only self-learning, there is no source to produce soft labels the full network can learn, possibly leading to imbalanced performances between the sub-networks. For the issue, we employ two _Teammate networks_ offering soft labels for each other. A teammate net, as a full network, is tagged along with a cohort of self-learned sub-nets as shown in Fig. 2(b). It is called _interactive learning_ since the two are learning from each other. The two Teammate nets may or may not be from the same architecture. Let \(y_{apt,a}\) and \(y_{apt,b}\) be the outputs produced by the two Teammate nets. The same scaling factor is chosen to generate their sub-nets. Then, the loss of interactive learning for the target network can be represented as:

\[\mathcal{L}_{apt\_cl,a}=\sum_{s_{i}\in\mathcal{S}^{{}^{\prime}}}KLDiv(y_{apt, a}(x,s_{i}),y_{apt,b}(x,s_{i}))\] (15)

where \(\mathcal{S}^{{}^{\prime}}\) is a set of scaling factors including \(s_{i}=1.0\). As one Teammate network takes a turn to learn from the other, we just swap the "teacher" and the "student" for the arguments of Kullback-Leibler divergence loss calculation in (15) as,

\[\mathcal{L}_{apt\_cl,b}=\sum_{s_{i}\in\mathcal{S}^{{}^{\prime}}}KLDiv(y_{apt, b}(x,s_{i}),y_{apt,a}(x,s_{i}))\] (16)

On top of that, since two networks that learn from each other can also improve each other's performance as shown [44], the combined loss is calculated as follows:

\[\mathcal{L}_{apt\_cl}=\mathcal{L}_{apt\_cl,a}+\mathcal{L}_{apt\_cl,b}\] (17)

Guided LearningEven with self-learning and interactive learning, we observed that a balance of losses may still not be preserved. Hence, we introduce _Leader Network_ which only learns the knowledge from the true labels. As a result, a full-size sub-net in the cohort of a Teammate net can learn the distribution from this highly accurate net as follows:

\[\mathcal{L}_{apt\_cl}=CE(y_{teader},y_{true})+\sum_{i\in\{a,b\}}KLDiv(y_{apt,i} (x,1.0),y_{teader})\] (18)

where \(y_{teader}\) denotes the output by the Leader net, and \(i\) denotes which Teammate net is used for calculation.

Total lossFinally, all three losses are combined together. The total loss is formulated as follows:

\[\mathcal{L}_{apt\_total}=\sum_{i\in\{a,b\}}\mathcal{L}_{apt\_st,i}+\mathcal{ L}_{apt\_cl}+\mathcal{L}_{apt\_ol}\] (19)

where \(\mathcal{L}_{apt\_sl,i}\) is Teammate network \(i\)'s self-learning loss. Note that we stop the gradients of the outputs produced by networks when we use them as labels.

## 5 Experiments

### Experimental Setting

#### 5.1.1 Datasets

Cifar-100 [17]:the images are zero-padded in 4 directions, and \(32\times 32\) crops are randomly sampled or their horizontal flips normalized with the per-channel mean and standard deviation. Due to the smaller image size than ImageNet, the first downsampling in ResNet is replaced by a convolution with \(3\times 3\) kernel and a stride of \(2\) before the global average pooling.

Tiny ImageNet [19]:Tiny ImageNet is derived from ImageNet [1]. Since labels of testing images are not publicly available, the validation images are used for evaluation. To make the number of input channels the same among all samples, the gray-scale images are converted into RGB images. For training, we horizontally flip the images randomly for data augmentation while keeping the original format of validation images.

#### 5.1.2 Network architectures for comparisons

Baselinewe use a ResNet152 [7] trained only with cross-entropy loss as a baseline. For comparisons, the number of layers in each stage is adjusted to the same proportion as the sub-network that we generate.

BranchyNetBranchyNet [36] creates muti-branch networks by adding additional exits. While ResNet110 was adapted in the original, we adapted and fine-tuned ResNet152 by adding one more stage. We implemented 3 branches for each stage. Each 1st, 2nd, and 3rd branch includes 3, 2, and 1 convolutional blocks, respectively. A global average pooling layer and a fully-connected layer follow the convolutional blocks in each branch. BranchyNet takes a weighted sum of the cross-entropy losses over all exits. Since precise settings for weights are not presented in the original paper, we set the weights as \(\{0.4,0.6,0.8,1.0\}\) from the first exit to the final.

Byot'Be your own teacher' method [43] is also designed to achieve a multi-cost inference by employing additional exits in the early stages. Its loss function and training method make it different from BranchyNet. In addition to training each exit, BYOT also applies the knowledge distillation in part and Euclidean distance to train the exits with the knowledge of the final exit to better train the outputs of the exits.

#### 5.1.3 Hyper-parameters

For learning rates and epochs: we train a model for 200 epochs using learning rates of \(\{1\times 10^{-1},1\times 10^{-2},1\times 10^{-3},1\times 10^{-4}\}\) - the 75th, 130th and 180th epochs are changing points of learning rates. The learning rate of the first epoch is set to \(1\times 10^{-2}\) to warm up the network. For optimizer, the optimizer we adapt is SGD. We set the momentum to \(0.9\) and weights decay to \(5\times 10^{-4}\). We report results with the model at the final training epoch.

#### 5.1.4 Platforms and running environments

The models are trained on the GPU cluster server running on Ubuntu with various GPU resources including NVIDIA RTX A6000, NVIDIA RTX 6000, NVIDIA RTX 5000, NVIDIA GeForce GTX 1080 Ti, and NVIDIA GeForce GTX 750 Ti. For evaluation, we run models on python 3.7 on Windows 11, and use NVIDIA GeForce RTX 2060 to measure performance metrics. For measuring the number of parameters and FLOPs, we use the python package _THOP_ to count them. The inference time is evaluated with the mini-batch size of \(1\) input which is the same as the data size in the datasets on NVIDIA GeForce RTX 2060. We run 100 times to warm up the GPU before we start any evaluation, and compute the inference time using the average of 1,000 times' results. All models are implemented and trained in Pytorch.

Figure 3: Performance and costs of different methods on CIFAR100 (top row) and Tiny ImageNet (bottom row.) (Only the full size of Baseline is shown in the charts since the subnetsâ€™ accuracies are incomparable to the others.)

### Experiments on CIFAR-100

In this section, we present the experimental results on CIFAR-100. Both BranchyNet and BYOT have four exits including the original exit. We display accuracy according to # FLOPs, # parameters and inference time in Fig. 3 (top row) and Table 1 left half. Some result points show that BranchyNet and BYOT perform better than our approach, COOP, in terms of accuracy. That is because these two methods yield hierarchies of features for classifier, which is empirically shown by a series of works such as FCN [24] - that is, multi-scale feature maps' fusion is beneficial to the whole network's performance. Another factor, which is supported by GoogLeNet [33], is that early exits can improve model training to some extent. However, for inference, multi-scale outputs may show unpredictable and unstable performance drops depending on the feature maps' sizes. We will discuss this later in Sec. 5.3. Our approach, COOP, achieves accuracy close to Baseline. Although its performance does not outperform BranchyNet and BYOT at all measurement points, there are three strengths of COOP: i) It provides more flexible options for computing capacity, ii) the size of the output feature map is consistent with the size of the original network so that our method is an architecture-free (general) method, and iii) it provides a predictable and smoother decrease in accuracy as the model's depth decreases.

### Experiments on Tiny Imagenet

In this section, we present the experimental results on Tiny ImageNet in Fig. 3 (bottom row) and Table 1 right half. Since the size of feature maps is larger than that in CIFAR-100, we can examine the impact of feature maps' size by comparing the results on the two different datasets. One thing to note in the right half Table 1 is that BYOT shows significantly poorer performance on \(1^{st}\) and \(2^{nd}\) exits compared with the results on CIFAR-100. This is because the feature map in Tiny ImageNet is \(64\times 64\), which is four times as large as that in CIFAR-100. Hence, the much smaller feature map size greatly degrades the performance of BYOT. On the other hand, COOP and BranchyNet show a more stable trend, since all branches are independent in the training phase. However, BranchyNet's loss function contains hyper-parameters that need to be set manually and fine-tuned, and the exits and loss functions have to be carefully designed case by case. In contrast, COOP is more convenient since it does not need to be "re-designed" when other existing refined networks are applied. Moreover, although the accuracy of the model may not be the best in all cases, it shows the highest accuracy in most of the middle range of scaling factors. In light of the motivation of the work which would utilize compact and efficient models, the trend shows the kind of utility we are seeking.Overall, COOP generally shows less degraded performance when the depth of the model decreases.

### Ablation Study

We explore and show the impacts of the components contributing to the network's performance with CIFAR-100.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{**CIFAR-100**} & \multicolumn{4}{c}{**TinyImageNet**} \\
**Network** & **Exe. Portion** & **\#Param.** & **\#FLOPs** & **Inf. (ms)** & **Acc.(\%)** & **\#Param.** & **\#FLOPs** & **Inf. (ms)** & **Acc.(\%)** \\ \hline \multirow{4}{*}{Baseline} & 0.2\(\times\) & 20.788M & 1.006 & 9.55 & 7.65 & 20.98M & 4.02G & 11.70 & 6.07 \\  & 0.4\(\times\) & 28.88M & 1.585 & 1.09 & 10.63 & 20.98M & 6.31G & 18.09 & 9.50 \\  & 0.6\(\times\) & 37.26M & 2.22G & 19.84 & 26.04 & 37.46M & 8.88M & 24.30 & 15.01 \\  & 0.8\(\times\) & 45.43M & 2.81G & 23.62 & 47.84 & 45.63M & 11.24G & 30.06 & 32.57 \\  & 1.0\(\times\) & 58.34M & 3.54G & 27.84 & 77.71 & 58.55M & 14.13G & 35.29 & 60.98 \\ \hline \multirow{4}{*}{BranchyNet} & \(1^{st}\) exit & 243.36K & 223.18M & 2.33 & 62.75 & 243.36K & 89.65M & 2.67 & 42.05 \\  & 2\({}^{st}\) exit & 1.21M & 488.98M & 4.41 & 73.76 & 1.21M & 1.96G & 5.07 & 48.25 \\  & 3\({}^{st}\) exit & 5.29M & 1.04G & 8.51 & 79.71 & 5.29M & 4.16G & 10.05 & 57.46 \\  & 4\({}^{th}\) exit & 58.34M & 3.74G & 28.88 & 80.51 & 58.55M & 14.95G & 36.81 & 60.35 \\ \hline \multirow{4}{*}{BYOT} & \(1^{st}\) exit & 18.39M & 644.25M & 5.20 & 71.85 & 18.39M & 2.58G & 6.60 & 24.62 \\  & 2\({}^{st}\) exit & 26.45M & 1.42G & 11.89 & 80.02 & 26.45M & 5.68G & 13.49 & 52.05 \\  & 3\({}^{st}\) exit & 69.90M & 4.11G & 35.62 & 81.95 & 69.90M & 16.45G & 42.60 & 64.57 \\  & 4\({}^{th}\) exit & 85.07M & 4.38G & 34.95 & 81.95 & 85.07M & 17.51G & 47.35 & 64.01 \\ \hline \multirow{4}{*}{COOP} & 0.2\(\times\) & 20.78M & 1.00G & 9.88 & 73.80 & 20.98M & 4.02G & 11.39 & 51.86 \\  & 0.4\(\times\) & 28.88M & 1.58G & 14.99 & 76.89 & 29.08M & 6.31G & 18.02 & 56.22 \\ \cline{1-1}  & 0.6\(\times\) & 37.26M & 2.22G & 20.55 & 78.11 & 37.46M & 8.88G & 24.53 & 56.56 \\ \cline{1-1}  & 0.8\(\times\) & 45.43M & 2.81G & 24.80 & 77.47 & 45.63M & 11.24G & 29.29 & 54.65 \\ \hline \multirow{4}{*}{} & 1.0\(\times\) & 58.34M & 3.54G & 28.12 & 75.81 & 58.55M & 14.18G & 35.04 & 52.19 \\ \cline{1-1} \cline

#### 5.4.1 Impact of Multi-Model Training

For multi-model training, we employed 4 sub-networks to train. Scaling factors of the sub-nets are sampled in two ways:

* STATIC: static values are set, \([1.0,0.7,0.4,0.2]\).
* RANDOM: other than the deepest (\(1.0\times\)) and shallowest net (\(0.2\times\)), two scaling factors are randomly sampled from \([0.3,0.9]\) with a step of \(0.1\) in every training epoch.

The approaches' comparison result with Baseline is shown in Fig. 4(a). Considering Baseline is merely trained by cross-entropy loss, the result shows that the multi-model training strategies can significantly help sub-networks' performance in most cases. In addition, RANDOM outperforms STATIC, which demonstrates how the random depths sampling provides stable performance in any size of the model by training various sub-networks. Accordingly, in the following results in the section, we employ random sampling for scaling factors. Fig. 4(b) shows the results of training with SFSL on top of RANDOM, which we call SFSL. It clearly shows that SFSL enhances the overall performance. It is noticeable that even the smaller sub-networks show very much enhanced performance by SFSL, which explains the impact of weighting smaller networks' loss more in the overall loss.

Impact of Teammate and Leader networksWe show the impact of Teammate and Leader nets with the following approaches,

* two Teammate nets in total. This corresponds to _Interactive Learning_ introduced in Sec. 4.4.
* COOP: the proposed, _Cooperative learning_ framework, that is, Leader net is added on top of TEAMMT.

The results are in Table 2 and Fig. 4(c). A general trend is that adding Teammate nets enhances performance (SFSL vs. TEAMMT), and adding Leader net also does (TEAMMT vs. COOP.) In particular, Interactive learning (TEAMMT) helps the network achieve better performance in all scaling factors. Cooperative learning (COOP) even boosts the performance further in a wide range of factors.

## 6 Conclusion

In this paper, we demonstrate that our approach (Cooperative training framework) dynamically resizes networks to meet resource constraints while offering more size options, applying seamlessly to various neural network architectures with minimal additional inference cost. Our approach efficiently trains multiple different-sized subnetworks simultaneously, as evidenced by experiments where even smaller-sized networks perform comparably to the full-sized network.

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline  & \multicolumn{5}{c}{Acc.(\%)} \\ \cline{3-7} Network & Member & \(0.2\times\) & \(0.4\times\) & \(0.6\times\) & \(0.8\times\) & \(1.0\times\) \\ \hline SFSL & - & 70.89 & 75.85 & 76.33 & 75.52 & 73.59 \\ \hline TEAMOT & Teammate & 72.40 & 75.46 & 76.51 & 76.89 & **76.52** \\ \hline COOP & Leader & **1/3** & **1/3** & **1/3** & **1/3** & 79.04 \\  & Teammate & **73.80** & **76.89** & **78.11** & **77.47** & 75.81 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance comparison (accuracy) of different methods for adaptive inference.

Figure 4: Ablation experiments

[MISSING_PAGE_FAIL:10]

- 86, 1951.
* [19] Y. Le and X. Yang. Tiny imagenet visual recognition challenge. _CS 231N_, 7(7):3, 2015.
* [20] S. Lee, S. Chang, and N. Kwak. Urnet: User-resizable residual networks with conditional gating module. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 4569-4576, 2020.
* [21] C. Li, G. Wang, B. Wang, X. Liang, Z. Li, and X. Chang. Dynamic slimmable network. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8607-8617, 2021.
* [22] C. Li, G. Wang, B. Wang, X. Liang, Z. Li, and X. Chang. Ds-net++: Dynamic weight slicing for efficient inference in cnns and vision transformers. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(4):4430-4446, 2022.
* [23] F. Li, G. Li, X. He, and J. Cheng. Dynamic dual gating neural networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5330-5339, 2021.
* [24] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3431-3440, 2015.
* [25] J.-H. Luo, J. Wu, and W. Lin. Thinet: A filter level pruning method for deep neural network compression. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_, Oct 2017.
* [26] C. Maddison, A. Mnih, and Y. Teh. The concrete distribution: A continuous relaxation of discrete random variables. 04 2017.
* [27] C. J. Maddison, D. Tarlow, and T. Minka. A\({}^{*}\) Sampling. In _Advances in Neural Information Processing Systems_, page 3086-3094, 2014.
* [28] S. I. Mirzadeh, M. Farajtabar, A. Li, N. Levine, A. Matsukawa, and H. Ghasemzadeh. Improved knowledge distillation via teacher assistant. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 5191-5198, 2020.
* [29] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz. Importance estimation for neural network pruning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2019.
* [30] Z. Pan, J. Cai, and B. Zhuang. Stitchable neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16102-16112, 2023.
* [31] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2018.
* [32] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition, 2015.
* [33] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1-9, 2015.
* [34] M. Tan and Q. Le. Efficientnetv2: Smaller models and faster training. In _International conference on machine learning_, pages 10096-10106. PMLR, 2021.
* [35] M. Tan and Q. V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _ICML_, pages 6105-6114, 2019.
* [36] S. Teerapittayanon, B. McDanel, and H. Kung. Branchynet: Fast inference via early exiting from deep neural networks. In _2016 23rd International Conference on Pattern Recognition (ICPR)_, pages 2464-2469, 2016.
* [37] Z. Xie, Z. Zhang, X. Zhu, G. Huang, and S. Lin. Spatially adaptive inference with stochastic feature sampling and interpolation. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 531-548, 2020.
* [38] L. Yang, Y. Han, X. Chen, S. Song, J. Dai, and G. Huang. Resolution adaptive networks for efficient inference. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2369-2378, 2020.

* Yang et al. [2020] T. Yang, S. Zhu, C. Chen, S. Yan, M. Zhang, and A. Willis. Mutualnet: Adaptive convnet via mutual learning from network width and resolution. In _European conference on computer vision_, pages 299-315, 2020.
* Ye et al. [2018] J. Ye, X. Lu, Z. Lin, and J. Z. Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=HJ94fqApW.
* Yu and Huang [2019] J. Yu and T. S. Huang. Universally slimmable networks and improved training techniques. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2019.
* Yu et al. [2019] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang. Slimmable neural networks. In _International Conference on Learning Representations_, 2019.
* Zhang et al. [2019] L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2019.
* Zhang et al. [2018] Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu. Deep mutual learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2018.