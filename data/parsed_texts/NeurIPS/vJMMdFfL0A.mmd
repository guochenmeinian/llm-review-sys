# The Benefits of Balance:

From Information Projections to Variance Reduction

 Lang Liu

These authors contributed equally to this work.

Ronak Mehta1

Soumik Pal

Zaid Harchaoui

University of Washington

###### Abstract

Data balancing across multiple modalities and sources appears in various forms in foundation models in machine learning and AI, _e.g._ in CLIP and DINO. We show that data balancing across modalities and sources actually offers an unsuspected benefit: variance reduction. We present a non-asymptotic statistical bound that quantifies this variance reduction effect and relates it to the eigenvalue decay of Markov operators. Furthermore, we describe how various forms of data balancing in contrastive multimodal learning and self-supervised clustering can be better understood, and even improved upon, owing to our variance reduction viewpoint.

## 1 Introduction

Deep neural networks have shown remarkable success at learning task-specific representations of data when provided supervision from massive amounts of labeled training examples. Recent trends, however, have shifted toward task-agnostic, universal representations that may be easily fine-tuned or even have zero-shot capabilities out of the box. Supervised learning, _stricto sensu_, is too limited a framework for these billion-parameter, data-hungry models, and a question at the heart of modern machine learning is learning from unlabeled, partially labeled, or weakly labeled data.

This need has paved the way for the current generation of self-supervised learning (SSL) approaches that circumvent the need for large amounts of "strong" labels. In SSL, a model is trained on a generic pseudo-task suited for unlabeled data, such as relating image-caption pairs or augmentations of the same image. Despite modern foundation models such as DINO [10] and CLIP [17] being trained in this fashion, many aspects of SSL remain mysterious.

In particular, the training process of self-supervised models often transcends the rules of the standard empirical risk minimization (ERM) toolkit. ERM combines two well-understood techniques: minibatch sampling and gradient-based optimization using backpropagation. On the other hand, SSL adds clever, yet less-understood techniques to the training pipeline. To illustrate this, consider a minibatch of independent and identically distributed (i.i.d.) training examples \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\sim P\), where \(P\) is a joint probability measure on sample spaces \(\mathcal{X}\times\mathcal{Y}\) (e.g. feature-label or image-caption pairs) and let \(P_{n}=\frac{1}{n}\sum_{i=1}^{n}\delta_{(X_{i},Y_{i})}\) be the empirical distribution. For a model parameterized by \(\theta\in\mathbb{R}^{d}\) with loss function \(h_{\theta}\), a stochastic learning algorithm involves computing the minibatch loss

\[\mathbb{E}_{P_{n}}\left[h_{\theta}(X,Y)\right]=\frac{1}{n}\sum_{i=1}^{n}h_{ \theta}(X_{i},Y_{i})\] (1)

and backpropagating through it to produce a minibatch stochastic gradient estimate. The algorithm then proceeds with the stochastic gradient descent (SGD) or a variant thereof (e.g., Adam, SGD with momentum, etc). Self-supervised methods often modify this recipe by _intervening_ on the optimization algorithm in a minibatch-specific way.

For example, SwaV (Caron et al., 2020) passes the minibatch examples through the model's encoder and clusters output vectors to generate pseudo-labels for a prediction task. In teacher-student architectures such as BYOL (Grill et al., 2020) and DINO (Caron et al., 2021), the minibatch is passed through two networks, where the "student" network is updated via backpropagation and the "teacher" network is updated by cloning the student's weights in regular intervals. In CLIP (Radford et al., 2021), a model optimizes the sum of two cross-entropy losses, where the predicted class probabilities on example \(i\) are generated by comparison to all other elements of the minibatch. While introducing such interventions into the procedure has clearly proven useful practically, it remains conceptually unclear what exactly is being optimized by the learning algorithm.

In this work, we aim to gain a better theoretical understanding of the objectives and algorithms underlying these empirically effective recipes. In particular, we want to shed a theoretical light on their precise benefits over traditional learning methods. We show that such recipes often enjoy an unsuspected benefit: reducing the variance of the empirical minibatch objective.

Concretely, we formalize the model updates described above as two phrases. Let \(Z_{1},\ldots,Z_{n}\) be a minibatch containing data points of arbitrary type (e.g. unlabeled images). In the first phase, this original data source is mapped (possibly using a model parameterized by \(\theta\)) to another minibatch \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\) of _derived_ pairs in \(\mathcal{X}\times\mathcal{Y}\). For example, in SwaV, each \(Z_{i}\) is an image, and we derive \((X_{i},Y_{i})\) by setting \(X_{i}=Z_{i}\) and letting \(Y_{i}\) be the pseudo-label based on clustering the vector representations of the images. In CLIP, each \(Z_{i}\) is an image-caption pair, and we derive \((X_{i},Y_{i})\) by simply letting \(X_{i}\) be the image and \(Y_{i}\) be the caption. Note that \(Y_{i}\) is _not_ a label in the traditional sense in neither of these examples. In the second phase, we use the model to compute a probability distribution \(P_{n,\theta}\) over \(\mathcal{X}\times\mathcal{Y}\), and perform a stochastic gradient update for the objective

\[\mathbb{E}_{P_{n,\theta}}\left[h_{\theta}(X,Y)\right].\] (2)

This reduces to empirical risk minimization on the minibatch objective (1) when \(Z=(X,Y)\) (each data point is originally observed in \(\mathcal{X}\times\mathcal{Y}\)) and \(P_{n,\theta}=P_{n}\) (the empirical distribution of the data is used, regardless of the model). Beyond this setting, one specific example of \(P_{n,\theta}\) has been applied across various families of self-supervised learning (as detailed in Sec. 2), which we refer to as _data balancing_ or simply _balancing_, the primary subject of this work.

For a probability measure \(Q\) on \(\mathcal{X}\times\mathcal{Y}\), let \(Q_{X}\) and \(Q_{Y}\) be the respective marginals on \(\mathcal{X}\) and \(\mathcal{Y}\) and let \(Q_{X|Y}\) and \(Q_{Y|X}\) denote the respective conditional distributions. Given fixed _target_ marginal distributions \(P_{X}\) on \(\mathcal{X}\) and \(P_{Y}\) on \(\mathcal{Y}\), balancing refers to repeatedly applying the operations

\[R\mapsto\operatorname*{arg\,min}_{Q:Q_{X}=P_{X}}\operatorname{KL}(Q\|R)\quad \text{and}\quad R\mapsto\operatorname*{arg\,min}_{Q:Q_{X}=P_{Y}}\operatorname {KL}(Q\|R),\] (3)

in an alternating fashion. After enough iterations, the resulting probability measure approximately marginalizes to \(P_{X}\) and \(P_{Y}\) in each variable. When \(\mathcal{X}\) and \(\mathcal{Y}\) are finite with \(|\mathcal{X}|=m\) and \(|\mathcal{Y}|=l\), these operations reduce to rescaling the rows of an \((m\times l)\)-matrix by \(P_{X}/R_{X}\) or its columns by \(P_{Y}/R_{Y}\). This algorithm has a decades-old history and is known in other contexts as Sinkhorn-Knopp matrix scaling (Sinkhorn, 1967), iterative proportional or biproportional fitting (Johnston and Pattie, 1993), and raking-ratio estimation (Thompson, 2000). The marginals \(P_{X}\) and \(P_{Y}\) can represent auxiliary information or inductive bias from users, such as the desire for balanced clusters.

Returning to \(P_{n,\theta}\) in (2), we show in Sec. 2 that both self-labeling and contrastive approaches in SSL implicitly define \(P_{n,\theta}\) by the following steps: 1) constructing a method-specific "initial" measure \(P_{n}^{(0)}\) on \(\mathcal{X}\times\mathcal{Y}\), then 2) applying \(k\) iterations of the operations (3) to generate a sequence \(P_{n}^{(0)},\ldots,P_{n}^{(k)}\), and finally, 3) setting \(P_{n,\theta}:=P_{n}^{(k)}\). In other words, these methods embed a _learnable_ balancing operation in their objectives. A natural question to consider is: if the marginals one uses accurately represent the ones of the true probability measure \(P\) governing the data, are balanced quantities "better behaved" than their unbalanced counterparts? If so, in what way?

Inspired by this question, we fix the model parameter \(\theta\) (thus dropping the subscript from the quantities above) and analyze the fluctuations of the unbalanced and balanced objectives. The formal problem statement is as follows. Let \(P_{n}^{(0)}=P_{n}\) and \(P_{n}^{(k)}\) denote the output of \(k\geq 1\) iterations of data balancing (see Sec. 3 for the precise definition). Finally, letting \(h:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\) be a fixed function of interest, we define the population parameter \(\varphi\) and its \(k\)-step _balanced estimator_\(\varphi_{n}^{(k)}\) by

\[\varphi:=\mathbb{E}_{P}\left[h(X,Y)\right]\quad\text{and}\quad\varphi_{n}^{(k)} :=\mathbb{E}_{P_{n}^{(k)}}\left[h(X,Y)\right].\] (4)Our goal is to establish theoretical guarantees on the mean squared error (MSE) \(\mathbb{E}_{P}[(\varphi_{n}^{(k)}-\varphi)^{2}]\) of estimating \(\varphi\) using \(\varphi_{n}^{(k)}\), with an informative dependence on the sample size \(n\), number of iterations \(k\), target marginals \((P_{X},P_{Y})\), and test function \(h\). We are particularly interested in its comparison to the direct estimator based on the empirical measure \(\varphi_{n}^{(0)}=\frac{1}{n}\sum_{i=1}^{n}h(X_{i},Y_{i})\), as to quantify the effect of the auxiliary information \((P_{X},P_{Y})\). Our analysis uncovers two surprising facts. Firstly, while originally proposed for a different purpose, balancing reduces the variance of the empirical estimate. Secondly, while the balancing iterations are nonlinear operations on the input measure, the variance reduction can be precisely quantified using the spectral decay of two linear Markov operators: the conditional means given \(X\) and \(Y\), respectively.

Contributions.In Sec. 2, we detail the mathematical connection between balancing and the modern representation learning techniques mentioned above. In Sec. 3, we prove a new upper bound on the MSE of the balancing estimator \(\varphi_{n}^{(k)}\). The bound decomposes into an \(O(n^{-1})\) first-order variance term and an \(O(k^{6}n^{-3/2})\) second-order term. The first-order term is shown to have a strict improvement over the empirical measure baseline with a fine-grained dependence on the spectra of two particular Markov operators. The second-order term can be used to compute the asymptotic variance reduction for statistical efficiency comparisons. Our proof technique relies on a recursion decomposition for balancing-based estimators, which may be of independent interest. In Sec. 4, we illustrate how insights from our analysis can be practically applied to CLIP-type objectives and evaluation setups.

## 2 Data Balancing in Practice

To demonstrate a precise connection to (2), we describe how a collection of training examples \(Z_{1},\ldots,Z_{n}\) observed in an original data space \(\mathcal{Z}\) (e.g. grayscale images) is mapped to a probability measure \(P_{n,\theta}\). Using the framework introduced in Sec. 1, this amounts to specifying four components: 1) the map from the original data into the derived sample spaces \(\mathcal{X}\) and \(\mathcal{Y}\), 2) the initial measure \(P_{n}^{(0)}\), 3) the function \(h\), and 4) the target marginals \((P_{X},P_{Y})\) for this measure to fit. From that point, the iterations of (3) produce \(P_{n}^{(1)},\ldots,P_{n}^{(k)}\), and we set \(P_{n,\theta}:=P_{n}^{(k)}\). For ease of presentation, we hide the dependence of \(P_{n}^{(k)}=P_{n,\theta}\) and \(h\equiv h_{\theta}\) on the model parameter \(\theta\). See Fig. 1 for examples of different choices of the sample spaces \(\mathcal{X}\) and \(\mathcal{Y}\).

Example 1: Self-Supervised Clustering.Balancing is used in discriminative clustering and self-supervised clustering; see (Jones et al., 2022; Asano et al., 2020; Caron et al., 2020) for variations on this theme. We describe the swapped prediction task of Caron et al. (2020) for concreteness but emphasize that clustering of this form is used as an intermediate step (or as the task itself) in many SSL pseudo-tasks. At a high level, this approach involves passing elements of a minibatch through two encoders to generate vector representations. These representations are then clustered separately, and the features from one encoder predict the cluster label from the other encoders. Denote the encoders \(f_{\theta_{s}}:\mathcal{Z}\rightarrow\mathbb{R}^{r}\) and \(f_{\theta_{s}}:\mathcal{Z}\rightarrow\mathbb{R}^{r}\), colloquially known as the _student_ and _teacher_ networks, respectively. Here, we let \(\{Z_{i}\}_{i=1}^{n}\) be a minibatch of \(n\) images, with

\[\mathcal{X}=\{Z_{1},\ldots,Z_{n}\}\quad\text{ and }\quad\mathcal{Y}=\{1, \ldots,l\}\,,\]

where \(m=n\) and the elements of \(\mathcal{Y}\) index learnable cluster representation vectors \(c_{1},\ldots,c_{l}\in\mathbb{R}^{r}\). Thus, we consider the overall parameter vector to be \(\theta:=(\theta_{s},\theta_{t},c_{1},\ldots,c_{l})\). Given temperature hyperparameters \(\epsilon,\tau>0\), the initial measure and loss function are given by the expressions

\[P_{n}^{(0)}(x,y)\propto e^{f_{\theta_{s}}(x)^{\top}c_{y}/\epsilon}\quad\text{ and}\quad h(x,y)=\log\frac{e^{f_{\theta_{s}}(x)^{\top}c_{y}/\tau}}{\sum_{y^{ \prime}=1}^{l}e^{f_{\theta_{s}}(x)^{\top}c_{y^{\prime}}/\tau}}.\]

Directly optimizing \(\sum_{x,y}P_{n}^{(0)}(x,y)h(x,y)\) without any constraints would lead to collapse, so it is balanced before optimization. The target marginals \(P_{X}\) and \(P_{Y}\) are given by the discrete uniform measures on \(\mathcal{X}\) and \(\mathcal{Y}\). This formulation is often derived by solving an optimal transport problem with the Sinkhorn-Knopp algorithm to assign soft cluster labels, the iterative solution result from this procedure is precisely \(P_{n}^{(k)}\). The intuition behind the choice of uniform marginal \(P_{X}\) is that each data point has an equal amount of mass to be allotted, whereas \(P_{Y}\) captures that the cluster sizes are equal. The number of iterations \(k\) is selected based on optimization considerations.

Example 2: Contrastive Learning.Contrastive Language-Image Pre-Training (Radford et al., 2021), or CLIP, is an architecture with an image encoder and a text encoder that map to a joint embedding space. Trained using image-caption pairs, the loss promotes representations such that images and text that are paired in the minibatch are close, whereas those that are not paired are far. The latter aspect (promoting dissimilarity of unpaired images/text) is what prevents collapse in this framework. To our knowledge, our interpretation of the CLIP objective as an implicit data balancing procedure is novel. Under this interpretation, we demonstrate that the objective is in fact a nonlinear function of \(P_{n,\theta}\), whereas its gradient will have a linear form similar to (2). In this case, each \(Z_{i}=(X_{i},Y_{i})\), where \(X_{i}\) is an image and \(Y_{i}\) is an associated caption. We have that

\[\mathcal{X}=\left\{X_{1},\dots,X_{n}\right\}\quad\text{and}\quad\mathcal{Y}= \left\{Y_{1},\dots,Y_{n}\right\},\]

so that \(m=n\). Consider an image encoder \(f_{\theta_{I}}:\mathcal{X}\mapsto\mathbb{R}^{r}\) and text encoder \(f_{\theta_{T}}:\mathcal{Y}\mapsto\mathbb{R}^{r}\) with parameter vector \(\theta=(\theta_{I},\theta_{T})\). The initial, unnormalized measure and the (in this case, vector-valued) function \(h\) are chosen based on these encoded representations:

\[P_{n}^{(\text{\tiny{0}})}(x,y)\propto e^{f_{\theta_{I}}(x)^{\top}f_{\theta_{T }}(y)}\quad\text{and}\quad h(x,y)=\nabla_{\theta}(f_{\theta_{I}}(x)^{\top}f_{ \theta_{T}}(y)).\] (5)

While we usually interpret \(h\) as a loss function, we will show below that the CLIP loss depends nonlinearly on \(P_{n,\theta}\), while the gradient has a linear dependence. If we believe, as in Example 1, that the target marginals \((P_{X},P_{Y})\) of the images and the text should be roughly uniform, we can apply the balancing iterations (3) with the target marginals being the uniform distributions over \(\mathcal{X}\) and \(\mathcal{Y}\), respectively. Because there is no preference for starting the iterations with the \(\mathcal{X}\) or \(\mathcal{Y}\) dimension first, we may consider both orderings. Let \(Q_{n}^{(\text{\tiny{1}})}\) be one iteration of balancing in the \(\mathcal{Y}\) dimension and \(R_{n}^{(\text{\tiny{1}})}\) represent one such iteration in the \(\mathcal{X}\) dimension. Then the original CLIP objective \(L_{n}^{\mathrm{CLIP}}\) can be recovered (up to an additive constant) as

\[L_{n}^{\mathrm{CLIP}} :=-\frac{1}{2}\sum_{i=1}^{n}\left[\log\frac{P_{n}^{(\text{\tiny{0 }})}(X_{i},Y_{i})}{\sum_{x}P_{n}^{(\text{\tiny{0}})}(x,Y_{i})}+\log\frac{P_{n} ^{(\text{\tiny{0}})}(X_{i},Y_{i})}{\sum_{y}P_{n}^{(\text{\tiny{0}})}(X_{i},y)}\right]\] \[=-\frac{1}{2}\sum_{i=1}^{n}\left[\log Q_{n}^{(\text{\tiny{1}})}(X _{i},Y_{i})+\log R_{n}^{(\text{\tiny{1}})}(X_{i},Y_{i})\right]-\log n.\] (6)

The measure \(P_{n,\theta}=\frac{1}{2}Q_{n}^{(\text{\tiny{1}})}+\frac{1}{2}R_{n}^{(\text{ \tiny{1}})}\) is constructed in this case by averaging the outputs of one iteration of balancing under each modality. Taking the gradient of (6) with respect to \(\theta\) (whose dependence is contained in \((Q_{n}^{(\text{\tiny{1}})},R_{n}^{(\text{\tiny{1}})})\)) recovers the expression for \(h\) in (5). The objective is often interpreted as an average of cross-entropy loss terms, each representing the prediction of one modality's original pair from the other. In our formulation, \(L_{n}^{\mathrm{CLIP}}\) can also be viewed as an average negative log-likelihood under the \(Q_{n}^{(\text{\tiny{1}})}\) and \(R_{n}^{(\text{\tiny{1}})}\). It is also of interest to study the effect of using \(Q_{n}^{(\text{\tiny{1}})}\) and \(R_{n}^{(\text{\tiny{1}})}\) for \(k\geq 0\) in general, as we show in Sec. 4.

Figure 1: **Data Balancing Examples:** Each panel shows a possible distribution \(Q\) on different choices of \((\mathcal{X},\mathcal{Y})\). The orange histograms are the target marginal \(P_{Y}\). **Left:**\(Q(x,y)\) is the affinity of an image \(x\) for cluster \(y\). **Center:**\(Q(x,y)\) is the similarity of an image \(x\) to a text caption \(y\). **Right:**\(Q(x,y)\) is the proportion of substring matches between a text caption \(x\) and a keyword \(y\).

**Example 3: Metadata Curation.** Here, we consider balancing an entire training set, as opposed to a particular minibatch. At the billion-parameter scale, dataset design can be the primary factor that differentiates performance between foundation models (Fang et al., 2013; Xu et al., 2024; Gadre et al., 2023). One general approach used in both the original CLIP dataset (Radford et al., 2021) and an open-source replication (Xu et al., 2024) is metadata curation, wherein a text dataset (possibly captions for images) is synthesized using a list of keywords \(\{y_{1},\ldots,y_{l}\}\) so that

\[\mathcal{X}=\left\{Z_{1},\ldots,Z_{n}\right\},\quad\mathcal{Y}=\left\{y_{1}, \ldots,y_{l}\right\},\]

meaning that \(m=n\). The keywords are matched to texts within \(\mathcal{X}\) via substring matching. While the approach of Xu et al. (2024) (dubbed MetaCLIP) pools all matched keywords on every text to measure the "distribution" of keywords, we consider a version in which each text \(Z_{i}\) can only be labeled with a single keyword \(y_{j}\). This allows for a true joint probability measure on \(\mathcal{X}\times\mathcal{Y}\). The marginal distribution of observed keywords is initially long-tailed (see Fig. 4) (e.g., "the" will match many more texts than "xylophone"). In both Radford et al. (2021) and Xu et al. (2024), the data are resampled so that this distribution of keywords over matches is closer to uniformity, i.e. keywords with many matches have their associated texts downsampled during the dataset creation process. While the probability measure may not be computed explicitly (due to scale), this adjustment of the keyword distribution can be viewed as a single iteration of balancing (3) applied to the \(\mathcal{Y}\) marginal. For tasks such as language modeling, we have

\[P_{n}^{\text{\tiny(0)}}(x,y)=P_{n}(x,y)\quad\text{and}\quad h(x,y)=\ell_{ \theta}(x),\] (7)

where \(\ell_{\theta}(x)\) denotes the loss of a model evaluated at a single text \(x\in\mathcal{X}\) (notice that the keyword is not used). We elucidate this connection by applying direct balancing on a subset of the ImageNet-Captions dataset in Sec. 4, observing the effect on downstream model performance.

Motivated by these scenarios, we address the statistical problem outlined in Sec. 1 by analyzing balancing-based estimators. We then return to examples mentioned above in Sec. 4, illustrating how the theoretical analysis can be translated to algorithmic variants.

## 3 Theoretical Analysis of Variance Reduction

We now present theoretical guarantees on the mean squared error (MSE) of the data-balanced estimator \(\varphi_{n}^{\text{\tiny(k)}}\) and highlight relevant points in the proofs. For readers' convenience, a notation table (Tab. 1) is in Appx. \(\mathrm{A}\). We first give context on the main innovations of the analysis and then outline its high-level steps. These innovations include relating the nonlinear iterations of balancing over probability measures to linear operators on a vector space and using a singular value decomposition of these operators to quantify their effect after a finite number of iterations. Furthermore, by scaling the number of iterations appropriately, we can characterize the estimator using the limit of balancing iterations, which is an object of interest in applications including optimal transport.

**Preliminaries.** Recall the setting introduced in Sec. 1, in which we consider sample spaces \((\mathcal{X},\mathcal{Y})\), along with true and unknown joint distribution \(P\) on \(\mathcal{X}\times\mathcal{Y}\) with known marginals \((P_{X},P_{Y})\). For ease of presentation, we assume that \(|\mathcal{X}|=|\mathcal{Y}|=m\), although the arguments do not rely on equal support sizes. We make the following assumption throughout, which is usually satisfied by the desired marginals \(P_{X}\) and \(P_{Y}\), such as in the uniform cases discussed in Sec. 2: the target marginals \(P_{X}(x)>0\) and \(P_{Y}(y)>0\) for all \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\). We define \(P_{n}^{\text{\tiny(0)}}=P_{n}\) as the empirical measure and for \(k\geq 1\) construct

\[P_{n}^{\text{\tiny(k)}}(x,y):=\begin{cases}\frac{P_{X}(x)}{P_{n}^{\text{\tiny(k )}}(x)}\cdot P_{n}^{\text{\tiny(k-1)}}(x,y)&\text{$k$ odd}\\ \frac{P_{Y}(y)}{P_{n}^{\text{\tiny(k-1)}}(y)}\cdot P_{n}^{\text{\tiny(k-1)}}(x, y)&\text{$k$ even}\end{cases}.\] (8)

By direct computation, we see that the iterations in (8) are equivalent to applying (3) for \(k\) odd and even, respectively. See Fig. 2 (left) for a visualization of this procedure. The iterations are well-defined for all \(k\) under the event that \(\text{Supp}(P_{n,X})=\text{Supp}(P_{X})\) and \(\text{Supp}(P_{n,Y})=\text{Supp}(P_{Y})\), i.e., all observed row counts and column counts are non-empty.2To provide background, the scheme of alternating the operators (8) is often seen as an iterative algorithm to solve the problem

\[\min_{Q\in\Pi(P_{X},P_{Y})}\operatorname{KL}(Q\|P_{n}^{{}_{(0)}}),\] (9)

where \(\Pi(P_{X},P_{Y})\) denotes the set of probability measures on \(\mathcal{X}\times\mathcal{Y}\) that marginalize to \(P_{X}\) and \(P_{Y}\) in each variable and \(\operatorname{KL}(\cdot\|\cdot)\) denotes the Kullback-Leibler divergence. The iterations (8) are based on the alternating minimization approach of solving

\[P_{n}^{{}_{(k)}}(x,y):=\begin{cases}\operatorname*{arg\,min}_{\{Q:Q_{X}=P_{X} \}}\operatorname{KL}(Q\|P_{n}^{{}_{(k-1)}})&\text{$k$ odd}\\ \operatorname*{arg\,min}_{\{Q:Q_{Y}=P_{Y}\}}\operatorname{KL}(Q\|P_{n}^{{}_{( k-1)}})&\text{$k$ even}\end{cases},\]

which inspires the viewpoint of balancing as alternating _information projections_. As we show in Appx. C, the iterations of (8) can equivalently be defined using the KL, reverse KL, or \(\chi^{2}\)-divergences. This viewpoint is relevant as previously, efforts have been made (e.g. in Bickel et al. (1991)) to analyze the variance reduction afforded by the solution to (9) directly. However, quantifying the variance reduction (in terms of properties of \(P\)) using this approach is challenging, as there is no closed-form expression for the solution of (9). A key mathematical outcome of our analysis is that the closed-form expressions of the projections (8) can be used to compute the reduction in mean squared error at each iteration. Thus, by letting \(k\equiv k(n)\to\infty\) (scaled appropriately against \(n\)), we can determine the reduction for the solution of (9) for large \(n\). This is the subject of Thm. 1.

**From Information Projections to Orthogonal Projections.** First, we will show that the variance reduction resulting from each nonlinear iteration of (8) is associated with a linear operator applied to \(h\). Thus, instead of analyzing the alternating information projections over probability measures, we may use familiar tools to understand alternating orthogonal projections in a vector space. To define them, we first let \(\mathbf{L}^{2}(P)\) to be the set of functions \(h:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\) satisfying \(\mathbb{E}_{P}\left[h^{2}(X,Y)\right]<\infty\). Even though \(\mathcal{X}\times\mathcal{Y}\) is finite, working within \(\mathbf{L}^{2}(P)\) will be analytically convenient. Let \(\mathbf{L}^{2}(P_{X})\) be the subspace of \(\mathbf{L}^{2}(P)\) containing functions that only depend on the first argument \(x\in\mathcal{X}\) and define \(\mathbf{L}^{2}(P_{Y})\) analogously. These are the solid-colored subspaces in Fig. 2 (right). Next, let \(\mu_{X}:\mathbf{L}^{2}(P)\to\mathbf{L}^{2}(P_{X})\) and \(\mu_{Y}:\mathbf{L}^{2}(P)\to\mathbf{L}^{2}(P_{Y})\) be defined as, for any \(h\in\mathbf{L}^{2}(P)\),

\[\mu_{X}h=\operatorname*{arg\,min}_{f\in\mathbf{L}^{2}(P_{X})}\mathbb{E}_{P} \left[(h(X,Y)-f(X))^{2}\right]\implies\left[\mu_{X}h\right](x,y):=\mathbb{E}_ {P}\left[h(X,Y)|X\right](x)\]

The operator \(\mu_{X}\) is an orthogonal projection onto \(\mathbf{L}^{2}(P_{X})\). The orthogonal projection operator \(\mu_{Y}\) onto \(\mathbf{L}^{2}(P_{Y})\) is defined analogously. We may also define the conditional _debiasing_ operators \(\mathcal{C}_{X}=I-\mu_{X}\) and \(\mathcal{C}_{Y}=I-\mu_{Y}\), which each project onto the orthogonal complements of \(\mathbf{L}^{2}(P_{X})\) and

Figure 2: **Data Balancing.** Nonlinear and linear operators associated with each iteration of (8). **Left:** Visualization of the exact iterations of (8) in the space of probability measures. The blue set contains joint distributions with \(\mathcal{X}\)-marginal equal to \(P_{X}\), whereas the orange set contains joint distributions with \(\mathcal{Y}\)-marginal equal to \(P_{Y}\). **Right:** Visualization of \(\mathbf{L}^{2}(P)\), the operators defining (11), and the singular values given in (13).

\(\mathbf{L}^{2}(P_{Y})\), visualized as subspaces with dotted border in Fig. 2 (right). To understand the importance of the conditional mean and debiasing operators, we give a recursive formula that forms the backbone of our analysis. Define \(\mu_{k}=\mu_{X}\) for \(k\) odd and \(\mu_{k}=\mu_{Y}\) for \(k\) even, and define \(\mathcal{C}_{k}\) similarly. Thus, by using the notation \(Q(h):=\mathbb{E}_{Q}[h(X,Y)]\), we have by linearity of expectation that

\[[P_{n}^{{(k)}}-P](h) =[P_{n}^{{(k)}}-P](\mathcal{C}_{k}h)+ \overbrace{[P_{n}^{{(k)}}-P](\mu_{k}h)}^{=0}\] \[=[P_{n}^{{(k-1)}}-P](\mathcal{C}_{k}h)+[P_{n}^{{(k)}}-P_{n}^{{ (k-1)}}](\mathcal{C}_{k}h)\] \[=\underbrace{[P_{n}^{{(0)}}-P](\mathcal{C}_{1} \ldots\mathcal{C}_{k}h)}_{\text{first-order term}}+\underbrace{\sum_{\ell=1}^{k}[P_{n}^{ {(0)}}-P_{n}^{{(\ell-1)}}](\mathcal{C}_{\ell}\ldots\mathcal{C}_{k}h)}_{ \text{higher-order terms}}.\] (10)

To justify the first line, we discuss the case when \(k\) is odd. Notice that \(\mu_{X}h\) is only a function of \(X\), so its expectation only depends on \(P_{X}\) that is equal to \(P_{n,X}^{{(k)}}\) (the \(\mathcal{X}\)-marginal of \(P_{n}^{{(k)}}\)) by (8). The last line follows by unrolling the previous step \(k-1\) times. This recursive expansion is proven formally in Prop. 15 in Appx. D. Given the expansion, the mean squared error can be computed by taking the expectation of squared (10). We show that the second moment of the first-order term in (10) is equal to \(\sigma_{k}^{2}/n\) where

\[\sigma_{0}^{2}:=\mathbb{V}\mathrm{ar}(h)\text{ and }\sigma_{k}^{2}:=\mathbb{V} \mathrm{ar}(\mathcal{C}_{1}\ldots\mathcal{C}_{k}h)\text{ for }k\geq 1,\] (11)

and all other terms are \(O(k^{6}n^{-3/2})\). Thus, by exactly computing the constant in the dominating term, we may quantify the asymptotic variance reduction. Our first main result concerns the higher-order terms and shows that it is indeed dominated by the first-order term. Note that the empirical mean \(\varphi_{n}^{{(0)}}=\frac{1}{n}\sum_{i=1}^{n}h(X_{i},Y_{i})\) is unbiased, and so its MSE is equal to \(\sigma_{0}^{2}/n\). Define in addition

\[p_{\star}:=\min\{\min_{x}P_{X}(x),\min_{y}P_{Y}(y)\}\]

which measures the non-uniformity of the target marginals. We have that \(p_{\star}\) is positive because both \(P_{X}\) and \(P_{Y}\) are positive. We now state the first main result.

**Theorem 1**.: _For a sequence of data balancing estimators \((\varphi_{n}^{{(k)}})_{k\geq 1}\) as defined in (4), there exists an absolute constant \(C>0\) and distribution dependent constant \(s\in[0,1)\) and such the following holds for \(\sigma_{\text{gap}}^{2}=\sigma_{0}^{2}-\sigma_{k}^{2}\): For \(n\geq C[\log_{2}(2n/p_{\star})+m\log{(n+1)}]/p_{\star}^{2}\) and \(k\geq 1\), we have_

\[\mathbb{E}_{P}\left[(\varphi_{n}^{{(k)}}-\varphi)^{2} \right]\leq\frac{\sigma_{0}^{2}-\sigma_{\text{gap}}^{2}}{n}+O\left(\frac{s^{ k}}{n}\right)+\tilde{O}\left(\frac{k^{6}}{n^{3/2}}\right).\] (12)

The quantities \(\sigma_{\text{gap}}^{2}\) and \(s\) are quantified toward the end of this section and are dependent on eigendecays of the conditional mean operators for each variable under \(P\). Furthermore, \(\sigma_{\text{gap}}^{2}>0\) except for the pathological case of \(\mu_{X}h\) being a constant function. Showing Thm. 1 boils down to showing that the higher-order term in (10) is \(O(n^{-1})\) with high probability. Using the expression (8) and assuming that \(\ell\geq 1\) is odd, we see that

\[[P_{n}^{{(\ell)}}-P_{n}^{{(\ell-1)}}](\mathcal{C}_{ \ell}\ldots\mathcal{C}_{k}h)=\sum_{x,y}\left[\frac{P_{X}(x)}{P_{n,X}^{{(\ell-1 )}}(x)}-1\right]\cdot[\mathcal{C}_{\ell}\ldots\mathcal{C}_{k}h](x,y)P_{n}^{{( \ell-1)}}(x,y).\]

The first (blue) term in the product quantifies the disagreement between the \(\mathcal{X}\)-marginal of \(P_{n}^{{(\ell-1)}}\) and the true marginal, which can be bounded in terms of \(\mathrm{KL}(P_{n,X}^{{(0)}}\|P_{X})\) and is shown to be \(O(n^{-1/2})\) with high probability via techniques from information theory. The second (orange) term can be unrolled recursively in a similar fashion to (10) itself, which will consequently be \(O(n^{-1/2})\) as well; this is the most technical part of the analysis (see Appx. D.3). Our analysis also yields a bound for the sensitivity of balancing to misspecified marginals; see Appx. D.5.

Given Thm. 1, a natural next step is to quantify the gap between \(\sigma_{0}^{2}\) and \(\sigma_{k}^{2}\), which requires finer-grained properties of \(\mathcal{C}_{X}\) and \(\mathcal{C}_{Y}\). Notably, we show that as \(k\to\infty\), \(\sigma_{k}^{2}\) approaches a limiting value. Thus, via (12), by using \(k=o(n^{1/12})\) obtains asymptotic variance of the solution to (9). This contrasts with Albertus and Berthet (2019), in which the dependence of a quantity similar to (12) is exponential in \(k\), meaning that \(k=o(\log(n))\) is required for convergence under this argument.

From Orthogonal Projections to Variance Reduction.We now clarify what is precisely meant by the "spectrum" of the conditional mean operators \(\mu_{X}\) and \(\mu_{Y}\). As proven using a _singular value decomposition_ (Prop. 3) in Appx. B.1, there exists a basis \(\{\alpha_{j}\}_{j=1}^{m}\) of \(\mathbf{L}^{2}(P_{X})\), a basis \(\{\beta_{j}\}_{j=1}^{m}\) of \(\mathbf{L}^{2}(P_{Y})\), and real values \(\{s_{j}\}_{j=1}^{m}\), that satisfy

\[\mu_{Y}\alpha_{j}=s_{j}\beta_{j}\text{ and }\mu_{X}\beta_{j}=s_{j}\alpha_{j} \text{ for }j\in\{1,\ldots,m\}\,.\] (13)

Furthermore, \(\alpha_{1}=\mathbf{1}_{\mathcal{X}}\) and \(\beta_{1}=\mathbf{1}_{\mathcal{Y}}\) leading to the equality \(\langle f,\alpha_{1}\rangle_{\mathbf{L}^{2}(P_{X})}=\mathbb{E}_{P_{X}}\left[f (X)\right]\). Finally, \(s_{1}=1\) and \(s_{j}\) is non-negative and non-increasing in \(j\). For a concrete example, consider \(m=2\), in which case \(P\) can be written as a matrix in \(\mathbb{R}^{2\times 2}\) and elements of \(\mathbf{L}^{2}(P_{X})\) and \(\mathbf{L}^{2}(P_{X})\) are vectors in \(\mathbb{R}^{2}\). Then, in the case of uniform marginals, we can verify directly that (13) can be satisfied by setting

\[\alpha_{1}=\beta_{1}=\begin{bmatrix}1\\ 1\end{bmatrix},\alpha_{2}=\beta_{2}=\begin{bmatrix}1\\ -1\end{bmatrix},\text{ and }P=\frac{1}{4}\begin{bmatrix}1+s&1-s\\ 1-s&1+s\end{bmatrix}\] (14)

for \(s=s_{2}\) (the second largest singular value). Thus, as \(s\to 1\), the distribution becomes "fully dependent" as \(Y\) and \(X\) are completely determined by one another. As \(s\to 0\), \(P\) approaches the product measure. Geometrically, because \(\alpha_{1}=\beta_{1}\), we know that the angle \(a\) between the subspaces \(\mathbf{L}^{2}(P_{X})\) and \(\mathbf{L}^{2}(P_{Y})\) is given by the angle between \(\alpha_{2}\) and \(\beta_{2}\). By computing their inner product in \(\mathbf{L}^{2}(P)\), we have that \(\langle\alpha_{2},\beta_{2}\rangle_{\mathbf{L}^{2}(P)}=\langle P,\alpha_{2} \beta_{2}^{\top}\rangle=s=\cos a\). Thus, \(s=0\) indicates orthogonality of these subspaces, alluding to the independence of \(X\) and \(Y\) (see the right panel of Fig. 2).

Returning to \(m\geq 2\), we consider the following as a sufficient condition for variance reduction: the operators \(\mu_{X}\) and \(\mu_{Y}\) have a positive spectral gap, i.e., \(s_{2}<s_{1}\). Note that this assumption is satisfied when \(P(x,y)>0\) for all \((x,y)\in\mathcal{X}\times\mathcal{Y}\) by the Perron-Frobenius Theorem [12, Chapter 8]. Using the intuition from Fig. 2, this rules out pathological cases such as \(Y\) being a deterministic function of \(X\). Under the spectral gap condition, the singular values \(\{s_{j}\}_{j=2}^{m}\) that are strictly less than \(1\) will determine a geometric rate of decay in variance given in Cor. 2. The left and right singular functions \(\alpha_{j}:\mathcal{X}\to\mathbb{R}\) and \(\beta_{j}:\mathcal{Y}\to\mathbb{R}\) will define a useful coordinate system to represent projections of \(h\) when analyzing \(\varphi_{n}^{(h)}\).

Indeed, let \(\bar{h}=P(h)\) be the centered test function. Because \(\mu_{X}\bar{h}\in\mathbf{L}^{2}(P_{X})\) and \(\mu_{Y}\bar{h}\in\mathbf{L}^{2}(P_{Y})\), we may decompose this function on the two bases to write

\[\mu_{X}\bar{h}=\sum_{j=1}^{m}u_{j}\alpha_{j}\quad\text{and}\quad\mu_{Y}\bar{h }=\sum_{j=1}^{m}v_{j}\beta_{j}.\] (15)

Cor. 2 below relates the (normalized) variance \(\sigma_{k}^{2}\) of the first-order term to the one of the sample mean \(\varphi_{n}^{(0)}\). In fact, it shows that the variance reduction \(\sigma_{0}^{2}-\sigma_{k}^{2}\) decays geometrically to the quantity

\[\sigma_{\text{gap}}^{2}:=\sum_{j=2}^{m}\left[u_{j}^{2}+\frac{(v_{j}-s_{j}u_{j })^{2}}{1-s_{j}^{2}}\right].\]

For simplicity, we only present the result for \(k\) even, i.e., \(\sigma_{2t}^{2}\).

**Corollary 2**.: _The variance reduction achieved by \(t+1\) iterations of the \(\mathcal{C}_{Y}\mathcal{C}_{X}\) operator can be quantified as_

\[\sigma_{0}^{2}-\sigma_{2(t+1)}^{2}=\sigma_{\text{gap}}^{2}-\sum_{j=2}^{m}\frac {s_{j}^{2}(v_{j}-s_{j}u_{j})^{2}}{1-s_{j}^{2}}s_{j}^{4t}=\sum_{j=2}^{m}\left[ u_{j}^{2}+(1-s_{j}^{4t+2})\frac{(v_{j}-s_{j}u_{j})^{2}}{1-s_{j}^{2}}\right].\]

Intuitively, the operators \(\mathcal{C}_{X}\) and \(\mathcal{C}_{Y}\) are the main sources of the variance reduction via orthogonality. Since \(\alpha_{1}=\mathbf{1}_{\mathcal{X}}\), we can see that the reduction will always be strictly positive as long as \(\mu_{X}\bar{h}\) is not a constant function. Finally, using \(s:=s_{2}\geq s_{j}\) for \(j\geq 2\) gives the second term in Thm. 1.

## 4 Numerical Illustrations

We illustrate how data balancing manifests in the motivating examples mentioned in Sec. 2 with experiments with CLIP-type models. We focus here on zero-shot image classification tasks. Details on these experiments, and additional ones including linear probing and zero-shot retrieval, as well as an empirical investigation of the sensitivity to misspecified marginals, are all contained in Appx. E. Code to reproduce the data and experiments can be found at https://github.com/ronakdm/balancing.

Model, Datasets, and Evaluation.Throughout, we consider training variants of CLIP models (see Sec. 2), which require a dataset of image-caption pairs. For the training set, we use the ImageNet-Captions dataset (Fang et al., 2013), which pairs images from ImageNet (Deng et al., 2009) that were taken from Flickr with their original captions. In the notation of Sec. 2, the model is specified by selecting an image encoder \(f_{\theta_{I}}\) and a text encoder \(f_{\theta_{T}}\). In all cases, we use a fixed image/text encoder as a base vector representation and compose it with a trainable feed-forward neural network, i.e., \(f_{\theta}=f_{\theta^{\text{head}}}^{\text{head}}\circ f^{\text{base}}\). We fix the base image encoder as CLIP ViT-B/32 architecture pre-trained on LAION-2B (Schuhmann et al., 2022), and vary the base text encoder across embedding models of varying quality: GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2019), and CLIP-based encodings. When two CLIP encoders are used for the base image/text vector representation, they are taken from separate CLIP models (i.e. the base representations are not dependent). We evaluate models based on zero-shot classification performance using the standard CLIP inference procedure: for any image \(x\), a label \(c\in\{1,\dots,C\}\) is predicted by associating to each \(c\) a natural language prompt \(y_{c}\), and predicting the scores \(s(x)=(s_{1}(x),\dots,s_{C}(x))\), with

\[s_{c}(x)=\frac{e^{\left\langle f_{\theta_{I}}(x),f_{\theta_{T}}(y_{c})\right\rangle /\tau}}{\sum_{c^{\prime}=1}^{C}e^{\left\langle f_{\theta_{I}}(x),f_{\theta_{T} }(y_{c^{\prime}})\right\rangle/\tau}}\] (16)

for a temperature \(\tau>0\). Multiple prompting strategies can be used depending on the evaluation dataset, for which we average embeddings before applying (16). We use the public CLIP Benchmark repository, using the datasets CIFAR-10, CIFAR-100, STL-10, with their default caption sets.

Data Balancing Effects.Fig. 3 shows the zero-shot classification performance (in terms of average per-class recall) of variants depending on whether the _contrastive learning_ objective from Sec. 2 is used or not. One iteration of balancing already leads to improvement in terms of downstream performance. Multiple balancing iterations lead to further improvements. See Appx. E for more details on this experiment, and for analogous ones with linear probing and zero-shot retrieval.

Fig. 4 then shows how balancing can be used to adjust an entire pre-training set to given marginals based on metadata, as described in Sec. 2 in the _metadata curation_ example. After balancing, the target marginal has less than \(2\) orders of difference. In terms of downstream performance, data balancing leads to some improvement in the smaller batch regime (\(m=512\)) when curating the dataset. See Appx. E for more details on this experiment.

Figure 3: **Zero-Shot Classification Performance across Embeddings, Batch Sizes, and Objectives. The three vertical panels describe different choices of the text encoder \(f_{\theta_{T}}\) which increases in quality from left to right; that is, pre-trained GPT-2, BERT, and CLIP embeddings, respectively. Within each vertical panel, examples include batch sizes \(m=128\) and \(m=512\). Rows indicate various evaluation datasets from CIFAR-10, CIFAR-100, and STL-10. The \(y\)-axis of each plot indicates average per-class recall, whereas the \(x\)-axis indicates training iterations at the given batch size.**

Related WorkSelf-supervised learning has witnessed a surge of recent interest as datasets and computing hardware allow for larger, more capable models (see Balestriero et al. (2023) and references therein). While we highlight in this paper the connections between data balancing and contrastive learning (Radford et al., 2021), we acknowledge that data balancing can also be related to "self-distillation" approaches more broadly (Grill et al., 2020; Chen and He, 2021; Oquab et al., 2024).

Historical motivations for data balancing include census or survey data, in which \(P_{n}\) is a cross-tabulation of (a limited number of) paired observations and the target marginals were estimated from large amounts of unpaired observations (Deming and Stephan, 1940; Ireland and Kullback, 1968). This situation is not unlike the present day--yet at a different scale--in which the amount of unstructured single-modality data (such as images) still dwarfs the amount of high-quality multimodal data (Gadre et al., 2023). Bickel et al. (1991) proved classical asymptotic results on balancing estimators. Linear operators similar to the ones we use in Sec. 3 also appear in their analysis. More recently, Albertus and Berthet (2019) studied such estimators from an asymptotic empirical process viewpoint. Our theoretical results significantly improve on those from Albertus and Berthet (2019) primarily in the dependence of the number of iterations \(k\) on the sample size \(n\) to achieve convergence guarantees (from logarithmic to polynomial).

Matrix scaling is a popular algorithm for solving entropy-regularized optimal transport (EOT). We refer to Peyre and Cuturi (2019) for a survey. See also Courty et al. (2017); Shen et al. (2018); Peng et al. (2019) for interesting methods based on EOT in machine learning. Entropy-regularized optimal transport was one of the original inspirations for SSL techniques such as SwaV (see Sec. 2). While EOT is itself a deterministic optimization problem, a related statistical problem is the large-sample limits of EOT solutions when the marginal measures are estimated from data (Mena and Niles-Weed, 2019; Genevay et al., 2019; Klatt et al., 2020). We emphasize that, while this line of work shares the matrix scaling algorithm with our setting, the statistical problem is entirely distinct; in statistical EOT, the target marginal distributions are computed from observations of independent, unpaired data, and the initial measure can be computed from the cost function. In our setting, the data are dependent, forming the random initial measure \(P_{n}\), whereas \(P_{X}\) and \(P_{Y}\) are fixed auxiliary information.

## 5 Conclusion

We showed how several disparate techniques used towards the training of foundation models are instances of a data balancing algorithm, which has the unsuspected benefit of reducing the variance of learning objectives involving multiple sources of data. We proved a new non-asymptotic bound on the mean-squared error of balanced estimators as they adjust to the given marginals. We also highlight the key roles of conditional expectation operators in quantifying that variance reduction effect. Finally, we translated the marginal balancing interpretation of several training practices for foundation models into algorithmic variants that warrant further investigation. Exploring variants incorporating prior information on the data sources is also an interesting venue for future work.

Figure 4: **Balancing and Metadata Curation.** Depiction of balancing and metadata curation (Example 3 in Sec. 2) on ImageNet-Captions dataset, in which \(\mathcal{X}\) represents image-caption pairs and \(\mathcal{Y}\) represents keywords. **Left:** Observed marginal \(P_{n,Y}\) (orange) and \(P_{Y}\) (blue), which are sorted by order of increasing probability. **Right:** Zero-shot evaluation of an embedding model trained using the standard CLIP loss original versus the balanced training set.

AcknowledgementsThe authors are grateful to G. Ilharco, M. Wortsman, K. Pillutla, L. Schmidt, and J. Wellner for fruitful discussions related to this work. This work was supported by NSF DMS-2023166, CCF-2019844, DMS-2134012, PIMS 20240827-PRN01, NIH, and IARPA 2022-22072200003. Part of this work was done while L. Liu was with the University of Washington, and while R. Mehta and Z. Harchaoui were visiting the Simons Institute for the Theory of Computing.

Broader ImpactWhile this paper is of a theoretical nature, the web-scale pre-training sets used to train foundation models can affect not only the biases of the models themselves but also the behavior of individuals who interact with them. In the case of representation learning, unrefined Internet data may lead to non-uniform performance among protected attributes such as gender, age, etc. For generative models, individuals of all ages may be influenced by harmful images or textual output. Studying the relationship between the balancing procedures considered in this paper and more holistic model evaluations presents a valuable direction for follow-up work.

## References

* Albertus and Berthet (2019) M. Albertus and P. Berthet. Auxiliary information: The raking-ratio empirical process. _Electronic Journal of Statistics_, 13(1), 2019.
* Asano et al. (2020) Y. Asano, C. Rupprecht, and A. Vedaldi. Self-labelling via simultaneous clustering and representation learning. In _ICLR_, 2020.
* Balestriero et al. (2023) R. Balestriero, M. Ibrahim, V. Sobal, A. Morcos, S. Shekhar, T. Goldstein, F. Bordes, A. Bardes, G. Mialon, Y. Tian, A. Schwarzschild, A. G. Wilson, J. Geiping, Q. Garrido, P. Fernandez, A. Bar, H. Pirsiavash, Y. LeCun, and M. Goldblum. A cookbook of self-supervised learning. _arXiv preprint_, 2023.
* Bickel et al. (1991) P. J. Bickel, Y. Ritov, and J. A. Wellner. Efficient estimation of linear functionals of a probability measure \(P\) with known marginal distributions. _The Annals of Statistics_, 1991.
* Caron et al. (2020) M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In _NeurIPS_, 2020.
* Caron et al. (2021) M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In _ICCV_, 2021.
* Chen and He (2021) X. Chen and K. He. Exploring simple Siamese representation learning. In _CVPR_, 2021.
* Courty et al. (2017) N. Courty, R. Flamary, A. Habrard, and A. Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. In _NeurIPS_, 2017.
* Cover (1999) T. M. Cover. _Elements of Information Theory_. John Wiley & Sons, 1999.
* Deming and Stephan (1940) W. E. Deming and F. F. Stephan. On a least squares adjustment of a sampled frequency table when the expected marginal totals are known. _Annals of Mathematical Statistics_, 11, 1940.
* Deng et al. (2009) J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In _CVPR_, 2009.
* Devlin et al. (2019) J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _ACL_, 2019.
* Everingham et al. (2007) M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results, 2007.
* Fang et al. (2013) A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language-image pre-training (CLIP). In _ICML_, 2013.
* Gadre et al. (2023) S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, E. Orgad, R. Entezari, G. Daras, S. M. Pratt, V. Ramanujan, Y. Bitton, K. Marathe, S. Mussmann, R. Vencu, M. Cherti, R. Krishna, P. W. Koh, O. Saukh, A. Ratner, S. Song, H. Hajishirzi, A. Farhadi, R. Beaumont, S. Oh, A. Dimakis, J. Jitsev, Y. Carmon, V. Shankar, and L. Schmidt. DataComp: In search of the next generation of multimodal datasets. In _NeurIPS_, 2023.
* Goyal et al. (2019)A. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyre. Sample complexity of Sinkhorn divergences. In _AISTATS_, 2019.
* Gohberg et al. [1990] I. Gohberg, S. Goldberg, and M. Kaashoek. _Classes of Linear Operators Vol. 1_. Springer, 1990.
* Grill et al. [2020] J.-B. Grill, F. Strub, F. Altche, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, B. Piot, k. kavukcuoglu, R. Munos, and M. Valko. Bootstrap your own latent: A new approach to self-supervised learning. In _NeurIPS_, 2020.
* Hodosh et al. [2013] M. Hodosh, P. Young, and J. Hockenmaier. Framing image description as a ranking task: Data, models and evaluation metrics. _Journal of Artificial Intelligence Research_, 2013.
* Horn and Johnson [2013] R. A. Horn and C. R. Johnson. _Matrix Analysis_. Cambridge University Press, 2013.
* Ireland and Kullback [1968] C. T. Ireland and S. Kullback. Contingency tables with given marginals. _Biometrika_, 1968.
* Johnston and Pattie [1993] R. J. Johnston and C. J. Pattie. Entropy-maximizing and the iterative proportional fitting procedure. _The Professional Geographer_, 45, 1993.
* Jones et al. [2022] C. Jones, V. Roulet, and Z. Harchaoui. Discriminative clustering with representation learning with any ratio of labeled to unlabeled data. _Statistics and Computing_, 2022.
* Kingma and Ba [2015] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* Klatt et al. [2020] M. Klatt, C. Tameling, and A. Munk. Empirical regularized optimal transport: Statistical theory and applications. _SIAM Journal on Mathematics of Data Science_, 2020.
* Lin et al. [2015] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollar. Microsoft COCO: Common Objects in Context, 2015.
* Maji et al. [2013] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-Grained Visual Classification of Aircraft. Technical report, University of Oxford, 2013.
* Mena and Niles-Weed [2019] G. Mena and J. Niles-Weed. Statistical bounds for entropic optimal transport: Sample complexity and the central limit theorem. In _NeurIPS_, 2019.
* Nutz [2021] M. Nutz. Introduction to Entropic Optimal Transport. _Lecture notes, Columbia University_, 2021.
* Oquab et al. [2024] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. HAZIZA, F. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski. DINOV2: Learning robust visual features without supervision. _Transactions on Machine Learning Research_, 2024.
* Peng et al. [2019] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang. Moment matching for multi-source domain adaptation. In _ICCV_, 2019.
* Peyre and Cuturi [2019] G. Peyre and M. Cuturi. Computational Optimal Transport: With Applications to Data Science. _Foundations and Trends in Machine Learning_, 11, 2019.
* Radford et al. [2019] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners, 2019.
* Radford et al. [2021] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, 2021.
* Schuhmann et al. [2022] C. Schuhmann, R. Beaumont, R. Vencu, C. W. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. R. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk, and J. Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In _NeurIPS_, 2022.
* Shen et al. [2018] J. Shen, Y. Qu, W. Zhang, and Y. Yu. Wasserstein distance guided representation learning for domain adaptation. In _AAAI_, 2018.
* Shen et al. [2019]R. Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. _American Mathematical Monthly_, 74(4), 1967.
* Thompson [2000] M. E. Thompson. _Theory of Sample Surveys_. Chapman & Hall, 2000.
* Xu et al. [2024] H. Xu, S. Xie, X. Tan, P.-Y. Huang, R. Howes, V. Sharma, S.-W. Li, G. Ghosh, L. Zettlemoyer, and C. Feichtenhofer. Demystifying CLIP data. In _ICLR_, 2024.

## Appendix

### Table of Contents

* 1 Notation
* 2 Linear Operators and Variance Reduction
	* 2.1 Singular Value Decomposition
	* 2.2 Proof of Main Results
* 3 From Information Projections to Data Balancing
	* 3.1 Balancing as Information Projections
	* 3.2 Proof of Main Results
* 4 Statistical Analysis of Balancing Estimators
	* 4.1 Recursion of Estimation Error
	* 4.2 Technical Tools & Intermediate Results
	* 4.3 Analysis of Higher-Order Term
	* 4.4 Proof of Main Results
	* 4.5 Misspecified Marginal Distributions
* 5 Experimental Details
	* 5.1 Datasets
	* 5.2 Model Specification and Hyperparameters
	* 5.3 Compute Environment
	* 5.4 CLIP and Multi-CLIP
	* 5.5 Metadata Curation
	* 5.6 Additional Experiments
* 6 NeurIPS Paper Checklist

## Appendix A Notation

## Appendix B Linear Operators and Variance Reduction

This section is dedicated to establishing the variance reduction result in Cor. 2 by employing properties of the Markov operators introduced in Sec. 3. In the first part, we establish Prop. 3, the singular value decomposition that defines the quantities appearing in Cor. 2. In the second part, we quantify the difference between \(\sigma_{0}^{2}\) and \(\sigma_{k}^{2}\) for even and odd iterations of \(k\).

### Singular Value Decomposition

Recall the conditional mean operators \(\mu_{X}\) and \(\mu_{Y}\) from Sec. 3,

\[[\mu_{X}h](x):=\mathbb{E}\left[h(X,Y)|X|\right)(x)\text{ and }[\mu_{Y}h](y):= \mathbb{E}\left[h(X,Y)|Y|\right)(y),\]

with the corresponding debiasing (a.k.a. centering) operators defined by \(\mathcal{C}_{X}=I-\mu_{X}\) and \(\mathcal{C}_{Y}=I-\mu_{Y}\).

**Proposition 3**.: _There exists a basis \(\{\alpha_{j}\}_{j=1}^{m}\) of \(\mathbf{L}^{2}(P_{X})\), a basis \(\{\beta_{j}\}_{j=1}^{m}\) of \(\mathbf{L}^{2}(P_{Y})\), and real values \(\{s_{j}\}_{j=1}^{m}\), which satisfy:_

\[\mu_{Y}\alpha_{j}=s_{j}\beta_{j}\text{ and }\mu_{X}\beta_{j}=s_{j}\alpha_{j} \text{ for }j\in\{1,\ldots,m\}\,,\] (17)

\(\alpha_{1}=\mathbf{1}_{\mathcal{X}}\)_, \(\beta_{1}=\mathbf{1}_{\mathcal{Y}}\), \(s_{1}=1\) and \(s_{j}\) is non-negative and non-increasing in \(j\)._

\begin{table}
\begin{tabular}{c c} \hline \hline
**Symbol** & **Description** \\ \hline \(\mathcal{X}\), \(\mathcal{Y}\) & Sample spaces for two data sources. \\  & Support sizes \(m=|\mathcal{X}|\) and \(l=|\mathcal{Y}|\). \\ \(m\), \(l\) & We sometimes assume \(m=l\) for ease of presentation. \\ \(P\) & Probability measure on \(\mathcal{X}\times\mathcal{Y}\) (the data-generating distribution). \\ \(n\) & Sample size. \\ \((X_{1},Y_{1}),\ldots,(X_{n},Y_{n})\) & Independent and identically distributed sample from \(P\). \\ \(P_{n}\) & Empirical measure of \(\{(X_{i},Y_{i})\}_{i=1}^{n}\). \\ \(Q_{X},Q_{Y}\) & Marginals of measure \(Q\) on \(\mathcal{X}\times\mathcal{Y}\), eg. \(R_{X}\), \(P_{Y_{i},X}\), etc. \\ Supp(\(Q\)) & For measure \(Q\) over \(\mathcal{Z}\), the set of values \(z\in\mathbb{Z}\) such that \(Q(z)>0\). \\ \(Q(h)\) & The expected value of \(h\) under \(Q_{i}\) or \(\mathbb{Z}_{Q}\left[h(X,Y)\right]\). \\ \hline \((P_{n}^{\omega_{n}})_{k\geq 1}\) & Sequence of iterations of (8). \\ \(k\) & Iteration count of (8). \\ \(\mathcal{S}\) & The event \(\{\text{Supp}(P_{n,X})=\text{Supp}(P_{X})\text{ and Supp}(P_{n,Y})=\text{Supp}(P_{Y})\}\). \\ \(h\) & Test function \(h:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\) of interest. \\ \(\varphi\) & The estimated \(\sum_{x}h(x,y)P(x,y)\). \\ \(\varphi_{n}^{(\omega_{n}^{\omega})}\) & The estimator \(\sum_{x,y}h(x,y)P_{n}^{(\omega_{1}^{\omega})}(x,y)\), when well-defined. \\ \(\tilde{\varphi}_{n}^{(\omega_{n}^{\omega})}\) & The estimator \(\tilde{\varphi}_{n}^{(\omega_{1}^{\omega})}=\varphi_{n}^{(\omega_{1}^{\omega})} \mathbb{I}_{S}+\varphi_{n}^{(\omega_{1}^{\omega})}\mathbb{I}_{S}\). \\ \(\tilde{\Theta}_{n}^{(\omega_{1}^{\omega})}(h)\) & Normalized error \(\sqrt{n}(\tilde{\varphi}_{n}^{(\omega_{1}^{\omega}-\varphi)}\). \\ \(V_{n}^{\omega_{1}^{\omega}}(h)\) & Remainder defined in Prop. 15. \\ \(\tilde{h}\) & Centered function \(h-\mathbb{E}_{P}\left[h\right]\). \\ \(\sigma_{k}^{2}\) & Variance term \(\mathbb{E}_{P}\left[(\mathcal{C}_{1},\ldots\mathcal{C}_{k}h)^{2}\right]\). \\ \(p_{*}\) & \(\min\{\min_{k}P_{X}(x),\min_{k}P_{Y}(y)\}\). \\ \hline \(\mathbf{L}^{2}(P)\) & Functions \(h:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\) (as \(\mathcal{X}\times\mathcal{Y}\) is finite). \\ \(\mathbf{L}^{2}(P_{X}),\mathbf{L}^{2}(P_{Y})\) & Subspaces of \(\mathbf{L}^{2}(P)\) containing functions only of \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\), respectively. \\ \(\mu_{X},\mu_{Y}\) & Conditional expectation operators \([\mu_{X}h](x):=\mathbb{E}_{P}\left[h(X,Y)|X|\right)(x)\) \\  & and \([\mu_{Y}h](y):=\mathbb{E}_{P}\left[h(X,Y)|Y|\right](y)\). \\ \(\mathcal{C}_{\mathcal{X}},\mathcal{C}_{Y}\) & Debiasing/centering operators \(\mathcal{C}_{X}=I-\mu_{X}\) and \(\mathcal{C}_{Y}=I-\mu_{Y}\). \\ \(\mu_{k},\mathcal{C}_{k}\) & \((\mu_{X},\mathcal{C}_{X})\) for \(k\) odd and \([\mu_{Y},\mathcal{C}_{Y})\) for \(k\) even. \\ \(\{s_{j}\}_{j=1}^{m}\) & Singular values in Prop. 3. \\ \(\{\alpha_{j}\}_{j=1}^{m},\{\beta_{j}\}_{j=1}^{m}\) & Bases for \(\mathbf{L}^{2}(P_{X})\) and \(\mathbf{L}^{2}(P_{Y})\) in Prop. 3. \\ \hline \hline \end{tabular}
\end{table}
Table 1: Notation used throughout the paper.

Proof.: When \(\mu_{X}\) is restricted to \(\mathbf{L}^{2}(P_{Y})\) and \(\mu_{Y}\) is restricted to \(\mathbf{L}^{2}(P_{X})\), these operators are in fact adjoint in \(\mathbf{L}^{2}(P)\), as by the tower property we have the relation

\[\langle f,\mu_{X}g\rangle_{\mathbf{L}^{2}(P_{X})}=\mathbb{E}\left[f(X)\mathbb{ E}\left[g(Y)|X|\right]\right]=\mathbb{E}\left[\mathbb{E}\left[f(X)|Y|\,g(Y) \right]=\langle\mu_{Y}f,g\rangle_{\mathbf{L}^{2}(P_{Y})}.\]

Since \(\mu_{Y}:\mathbf{L}^{2}(P_{X})\to\mathbf{L}^{2}(P_{Y})\) is a compact linear operator, by Gohberg et al. (1990, Section IV.1 Theorem 1.1) and Gohberg et al. (1990, Section IV.1 Corollary 1.2), we have that \(\mu_{Y}\) admits a singular value decomposition satisfying (17). Next, we show that \(s_{1}\leq 1\) and that \(\mathbf{1}_{\mathcal{X}}\) is an eigenvector of \(\mu_{X}\mu_{Y}:\mathbf{L}^{2}(P_{X})\to\mathbf{L}^{2}(P_{X})\) with eigenvalue \(1\), which confirms that \(s_{1}=1\) and \(\alpha_{1}=\mathbf{1}_{\mathcal{X}}\) by the definition of singular values (arguing symmetrically achieves \(\beta_{1}=\mathbf{1}_{\mathcal{Y}}\)). By the variational representation of singular values (Gohberg et al., 1990, Section IV.1 Equation (2)), we have that

\[\sup_{f:\|f\|_{\mathbf{L}^{2}(P_{X})}=1}\|\mu_{Y}f\|_{\mathbf{L}^{2}(P_{Y})}=s _{1}.\]

Consider any \(f\in\mathbf{L}^{2}(P_{X})\) such that \(\|f\|_{\mathbf{L}^{2}(P_{X})}=1\). Define the conditional probability \(P_{X|Y}(x|y)=P(x,y)/P_{Y}(y)\) which is well-defined by assumption. Then, by the Cauchy-Schwarz inequality in \(\mathbf{L}^{2}(P_{X|Y})\),

\[\|\mu_{Y}f\|_{\mathbf{L}^{2}(P_{Y})}^{2} =\sum_{y\in\mathcal{Y}}\left(\sum_{x\in\mathcal{X}}f(x)P_{X|Y}(x| y)\right)^{2}P_{Y}(y)\] \[\leq\sum_{y\in\mathcal{Y}}\sum_{x\in\mathcal{X}}f^{2}(x)P_{X|Y}( x|y)P_{Y}(y)\] \[=\sum_{x\in\mathcal{X}}f^{2}(x)\sum_{y\in\mathcal{Y}}P(x,y)\] \[=\|f\|_{\mathbf{L}^{2}(P_{X})}^{2}=1.\]

This proves that \(s_{1}\leq 1\). For equality, notice that \(\mu_{X}\mu_{Y}\,\mathbf{1}_{\mathcal{X}}=\mu_{X}\,\mathbf{1}_{\mathcal{Y}}= \mathbf{1}_{\mathcal{X}}\), completing the proof. 

### Proof of Main Results

From Prop. 3, we establish two bases \(\left\{\alpha_{j}\right\}_{j=1}^{m}\) and \(\left\{\beta_{j}\right\}_{j=1}^{m}\) of \(\mathbf{L}^{2}(P_{X})\) and \(\mathbf{L}^{2}(P_{Y})\), respectively. These bases span the range of the operators \(\mu_{X}\) and \(\mu_{Y}\). We will consider the repeated application of the operator \(\mathcal{C}_{Y}\mathcal{C}_{X}\), a sequence of two centering operations on some function \(h\in\mathbf{L}^{2}(P)\), and compare

\[\mathbb{E}\left[((\mathcal{C}_{Y}\mathcal{C}_{X})^{t}\bar{h})^{2}\right]\text{ against }\mathbb{E}\left[\bar{h}^{2}\right]\]

for \(\bar{h}=h-\mathbb{E}_{P}\left[h\right]\). We establish the main result by measuring the reduction in variance from a single application, in terms of the coordinates of the function of interest on each of the two subspaces. We will then observe how these coordinates change iteration-to-iteration to give the final result.

**Lemma 4**.: _For any \(h\in\mathbf{L}^{2}(P)\) such that \(\mathbb{E}_{P}\left[h\right]=0\), let_

\[\mu_{X}h=\sum_{j=1}^{m}u_{j}\alpha_{j}\text{ and }\mu_{Y}h=\sum_{j=1}^{m}v_{j} \beta_{j}.\]

_Then, we have that_

\[\mathbb{E}\left[(\mathcal{C}_{Y}\mathcal{C}_{X}h)^{2}\right]=\mathbb{E}\left[h ^{2}\right]-\sum_{j=2}^{m}u_{j}^{2}-\sum_{j=2}^{m}(v_{j}-s_{j}u_{j})^{2}.\]

Proof.: By orthogonality, we have that

\[\mathbb{E}\left[(\mathcal{C}_{Y}\mathcal{C}_{X}h)^{2}\right] =\mathbb{E}\left[((I-\mu_{Y})\mathcal{C}_{X}h)^{2}\right]\] \[=\mathbb{E}\left[(\mathcal{C}_{X}h)^{2}\right]-2\mathbb{E}\left[( \mathcal{C}_{X}h)(\mu_{Y}\mathcal{C}_{X}h)\right]+\mathbb{E}\left[(\mu_{Y} \mathcal{C}_{X}h)^{2}\right]\] \[=\mathbb{E}\left[(\mathcal{C}_{X}h)^{2}\right]-2P_{Y}((\mu_{Y} \mathcal{C}_{X}h)^{2})+P_{Y}((\mu_{Y}\mathcal{C}_{X}h)^{2})\] \[=\mathbb{E}\left[(\mathcal{C}_{X}h)^{2}\right]-P_{Y}((\mu_{Y} \mathcal{C}_{X}h)^{2})\] \[=\mathbb{E}\left[h^{2}\right]-P_{X}((\mu_{X}h)^{2})-P_{Y}((\mu_{Y} \mathcal{C}_{X}h)^{2}).\]Because \(P(h)=0\), it holds by the tower property of conditional expectation that \(P_{X}(\mu_{X}h)=0\), which implies that

\[u_{1}=\langle\mu_{X}h,\alpha_{1}\rangle_{\mathbf{L}^{2}(P_{X})}=0\implies P_{X}(( \mu_{X}h)^{2})=\sum_{j=2}^{m}u_{j}^{2}.\]

For the second term, observe that \(P_{X}(\mathcal{C}_{X}h)=0\), so it holds by the tower property that \(P_{Y}(\mu_{Y}\mathcal{C}_{X}h)=0\), so

\[P_{Y}((\mu_{Y}\mathcal{C}_{X}h)^{2})=\sum_{j=2}^{m}\left(\left\langle\mu_{Y} \mathcal{C}_{X}h,\beta_{j}\right\rangle_{\mathbf{L}^{2}(P_{Y})}\right)^{2}.\]

Next, we compute the term in the square by applying Prop. 3:

\[\langle\mu_{Y}\mathcal{C}_{X}h,\beta_{j}\rangle_{\mathbf{L}^{2}(P _{Y})} =\langle\mu_{Y}h,\beta_{j}\rangle_{\mathbf{L}^{2}(P_{Y})}-\langle \mu_{Y}\mu_{X}h,\beta_{j}\rangle_{\mathbf{L}^{2}(P_{Y})}\] \[=v_{j}-\left\langle\mu_{Y}\sum_{k=1}^{m}u_{k}\alpha_{k},\beta_{j }\right\rangle_{\mathbf{L}^{2}(P_{Y})}\] \[=v_{j}-s_{j}u_{j},\]

which completes the proof. 

Lem. 4 ensures that we have a reduction on each iteration, with a formula that depends on the coordinates of the function on each subspace. Because these coordinates change every iteration, we track them in the next lemma. Define \(h_{0}=\bar{h}\) and \(h_{t+1}=(\mathcal{C}_{Y}\mathcal{C}_{X})h_{t}\), along with the constants \(\left\{u_{t,j}\right\}_{j=1}^{m}\) and \(\left\{v_{t,j}\right\}_{j=1}^{m}\) given by

\[\mu_{X}h_{t}=\sum_{j=1}^{m}u_{t,j}\alpha_{j}\text{ and }\mu_{Y}h_{t}=\sum_{j=1} ^{m}v_{t,j}\beta_{j}.\]

We have the following.

**Lemma 5**.: _For all \(t\geq 0\), it holds that_

\[u_{t+1,j} =s_{j}^{2}u_{t,j}-s_{j}v_{t,j},\] \[v_{t+1,j} =0.\]

Proof.: Fix any \(j\in[m]\), and use Prop. 3 to write

\[u_{t+1,j} =\langle\mu_{X}\mathcal{C}_{Y}\mathcal{C}_{X}h_{t},\alpha_{j} \rangle_{\mathbf{L}^{2}(P_{X})}\] \[=\langle\mu_{X}(I-\mu_{X}-\mu_{Y}+\mu_{Y}\mu_{X})h_{t},\alpha_{j }\rangle_{\mathbf{L}^{2}(P_{X})}\] \[=\langle\mu_{X}\mu_{Y}\mu_{X}h_{t},\alpha_{j}\rangle_{\mathbf{L} ^{2}(P_{X})}-\langle\mu_{X}\mu_{Y}h_{t},\alpha_{j}\rangle_{\mathbf{L}^{2}(P_{ X})}\] \[=\left\langle\mu_{X}\mu_{Y}\sum_{k=1}^{m}u_{t,k}\alpha_{k},\alpha _{j}\right\rangle_{\mathbf{L}^{2}(P_{X})}-\left\langle\mu_{X}\sum_{k=1}^{m}v _{t,k}\beta_{k},\alpha_{j}\right\rangle_{\mathbf{L}^{2}(P_{X})}\] \[=s_{j}^{2}u_{t,j}-s_{j}v_{t,j},\]

which proves the first part of the claim. For the second part, note that \(\mu_{Y}\mathcal{C}_{Y}=0\), so \(\langle\mu_{Y}\mathcal{C}_{Y}\mathcal{C}_{X}h_{t},\alpha_{j}\rangle_{\mathbf{ L}^{2}(P_{Y})}=0\). 

Using Lem. 4 and Lem. 5, we can simply accumulate the reduction incurred on every iteration.

**Proposition 6**.: _Define the constants \((u_{j})_{j=1}^{m}\) and \((v_{j})_{j=1}^{m}\) by_

\[\mu_{X}\bar{h}=\sum_{j=1}^{m}u_{j}\alpha_{j}\text{ and }\mu_{Y}\bar{h}=\sum_{j=1}^{m }v_{j}\beta_{j}.\]

_Then, we may quantify the variance reduction achieved by \(t+1\) iterations of the \(\mathcal{C}_{Y}\mathcal{C}_{X}\) operator as_

\[\mathbb{E}\left[\bar{h}^{2}\right]-\mathbb{E}\left[((\mathcal{C} _{Y}\mathcal{C}_{X})^{t+1}\bar{h})^{2}\right] =\sum_{j=2}^{m}\left\{u_{j}^{2}+(v_{j}-s_{j}u_{j})^{2}\left[1+ \frac{s_{j}^{2}(1-s_{j}^{4t})}{1-s_{j}^{2}}\right]\right\}\] \[\to\sum_{j=2}^{m}\left[u_{j}^{2}+\frac{(v_{j}-s_{j}u_{j})^{2}}{1- s_{j}^{2}}\right]\]

_as \(t\to\infty\)._

Proof.: Apply Lem. 4\((t+1)\)-times so that

\[\mathbb{E}\left[((\mathcal{C}_{Y}\mathcal{C}_{X})^{t+1}\bar{h})^ {2}\right] =\mathbb{E}\left[\bar{h}^{2}\right]-\sum_{j=2}^{m}\sum_{\tau=0}^{t }\left[(1+s_{j}^{2})u_{\tau,j}^{2}+v_{\tau,j}^{2}-2s_{j}u_{\tau,j}v_{\tau,j}\right]\] \[=\mathbb{E}\left[\bar{h}^{2}\right]-\sum_{j=2}^{m}\left[v_{0,j}^{ 2}-2s_{j}u_{0,j}v_{0,j}+\sum_{\tau=0}^{t}(1+s_{j}^{2})u_{\tau,j}^{2}\right]\]

as by Lem. 5, we have that \(v_{\tau,j}=0\) for \(\tau>0\). Next, we unroll the definition of \(u_{\tau,j}\) so that

\[u_{\tau,j} =s_{j}^{2}u_{\tau-1,j}-s_{j}v_{\tau-1,j}\] \[=s_{j}^{2}(s_{j}^{2}u_{\tau-2,j}-s_{j}v_{\tau-2,j})-s_{j}v_{\tau- 1,j}\] \[=s_{j}^{2\tau-2}(s_{j}^{2}u_{0,j}-s_{j}v_{0,j})\]

for \(\tau>0\), yielding

\[\mathbb{E}\left[\bar{h}^{2}\right]-\mathbb{E}\left[((\mathcal{C} _{Y}\mathcal{C}_{X})^{t+1}\bar{h})^{2}\right]\] \[=\sum_{j=2}^{m}\left[u_{0,j}^{2}+(v_{0,j}-s_{j}u_{0,j})^{2}+(1+s_ {j}^{2})(s_{j}^{2}u_{0,j}-s_{j}v_{0,j})^{2}\sum_{\tau=1}^{t}(s_{j}^{4})^{\tau- 1}\right]\] \[=\sum_{j=2}^{m}\left[u_{0,j}^{2}+(v_{0,j}-s_{j}u_{0,j})^{2}+(1+s_ {j}^{2})(s_{j}^{2}u_{0,j}-s_{j}v_{0,j})^{2}\sum_{\tau=0}^{t-1}(s_{j}^{4})^{\tau }\right]\] \[=\sum_{j=2}^{m}\left[u_{0,j}^{2}+(v_{0,j}-s_{j}u_{0,j})^{2}+\frac {s_{j}^{2}(1+s_{j}^{2})(v_{0,j}-s_{j}u_{0,j})^{2}(1-s_{j}^{4t})}{1-s_{j}^{4}}\right]\] \[=\sum_{j=2}^{m}\left[u_{0,j}^{2}+(v_{0,j}-s_{j}u_{0,j})^{2}+\frac {s_{j}^{2}(v_{0,j}-s_{j}u_{0,j})^{2}(1-s_{j}^{4t})}{1-s_{j}^{2}}\right].\]

Substitute \(u_{0,j}=u_{j}\) and \(v_{0,j}=v_{j}\) to complete the proof. 

We also present the corresponding result for \(k\) odd. The proof follows similarly by repeated application of the operator \(\mathcal{C}_{Y}\mathcal{C}_{X}\). However, the iterations will be compared to \(\sigma_{1}^{2}=\mathbb{E}_{P}\left[(\mathcal{C}_{X}\bar{h})^{2}\right]\), as we consider \(\mathcal{C}_{X}\bar{h}\) as the "first" iteration to this process.

**Proposition 7**.: _Define the constants \((u_{j})_{j=1}^{m}\) by_

\[\mu_{Y}\mathcal{C}_{X}\bar{h}=\sum_{j=1}^{m}u_{j}\beta_{j}.\]_Then, we may quantify the variance reduction achieved by \(t+1\) iterations of the \(\mathcal{C}_{X}\mathcal{C}_{Y}\) operator as_

\[\mathbb{E}\left[(\mathcal{C}_{X}\bar{h})^{2}\right]-\mathbb{E} \left[((\mathcal{C}_{X}\mathcal{C}_{Y})^{t+1}\mathcal{C}_{X}\bar{h})^{2}\right] =\sum_{j=2}^{m}\left\{u_{j}^{2}+(s_{j}u_{j})^{2}\left[1+\frac{s_{ j}^{2}(1-s_{j}^{4t})}{1-s_{j}^{2}}\right]\right\}\] \[\to\sum_{j=2}^{m}\left(\frac{1+s_{j}^{2}}{1-s_{j}^{2}}\right)u_{j} ^{2}\]

_as \(t\to\infty\)._

In order to have full monotonicity, we also need that \(\sigma_{0}^{2}\geq\sigma_{1}^{2}\). This follows by orthogonality, as

\[\sigma_{0}^{2}=\mathbb{E}\left[\bar{h}^{2}\right]=\mathbb{E}\left[(\mathcal{C }_{X}\bar{h})^{2}\right]+\mathbb{E}\left[(\mu_{X}\bar{h})^{2}\right]=\sigma_{1 }^{2}+\mathbb{E}\left[(\mu_{X}\bar{h})^{2}\right]\geq\sigma_{1}^{2}.\] (18)

Thus, we can combine Prop. 7 and (18) to fully quantify the relationship between \(\sigma_{0}^{2}\) and \(\sigma_{k}^{2}\) for \(k\) odd.

## Appendix C From Information Projections to Data Balancing

This section is dedicated to deriving three representations of the balancing procedure as projections in various statistical divergences, as shown in Fig. 2.

We consider two sets of probability measures denoted by \(\Pi_{X}=\{Q:Q_{X}=P_{X}\}\) and \(\Pi_{Y}=\{Q:Q_{Y}=P_{Y}\}\). The marginal matching steps are written as projections in terms of a statistical divergence \(D\) (precisely, an \(f\)-divergence) in the form

\[\frac{P_{X}}{P_{n,X}^{(k-1)}}\otimes P_{n}^{(k-1)}=\operatorname*{arg\,min}_{ Q\in\Pi_{X}}D(Q\|P_{n}^{(k-1)}),\quad\frac{P_{Y}}{P_{n,Y}^{(k-1)}}\otimes R= \operatorname*{arg\,min}_{Q\in\Pi_{Y}}D(Q\|P_{n}^{(k-1)}).\]

We provide the derivations for three common choices of \(D\): Kullback-Leibler (KL), reverse KL, and \(\chi^{2}\). Using this viewpoint, and simply assuming the positivity of the marginal measures \(P_{X}\) and \(P_{Y}\), we derive an upper bound in Prop. 14 that is _constant_ in \(k\). This is an improvement over the recent work of Albertus and Berthet (2019), in which they show an upper bound that scales _exponentially_ in \(k\).

The KL representation will be used in the proof of Prop. 14, which (recalling the sequence \((P_{n}^{(k)})_{k\geq 1}\) from (8)), controls the error between \(P_{n,Y}^{(k)}\) and \(P_{Y}\) for \(k\) odd and \(P_{n,X}^{(k)}\) and \(P_{X}\) for \(k\) even.

### Balancing as Information Projections

#### c.1.1 Projection in KL-Divergence

**Proposition 8**.: _Assume that \(P_{X}\ll R_{X}\) and \(P_{Y}\ll R_{Y}\), and define_

\[Q^{\star}:=\operatorname*{arg\,min}_{Q\in\Pi_{X}}\operatorname{KL}(Q\|R),\quad P ^{\star}:=\operatorname*{arg\,min}_{Q\in\Pi_{Y}}\operatorname{KL}(Q\|R).\] (19)

_Then, it holds that_

\[Q^{\star}(x,y)=\begin{cases}P_{X}(x)R_{Y|X}(y|x)&\text{ if }R_{X}(x)>0\\ 0&\text{ if }R_{X}(x)=0\end{cases}\] (20)

_and_

\[P^{\star}(x,y)=\begin{cases}P_{Y}(y)R_{X}(x|y)&\text{ if }R_{Y}(y)>0\\ 0&\text{ if }R_{Y}(y)=0\end{cases}.\] (21)Proof.: In the case that \(Q(x,y)=0\), we apply the convention that \(0\log 0=0\). Consider the case \(Q^{\star}\), the projection of \(R\) onto \(\Pi_{X}\). Write

\[\operatorname{KL}(Q\|R) =\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}Q(x,y)\log\tfrac{Q_{Y \mid X}(y|x)Q_{X}(x)}{R_{Y\mid X}(y|x)R_{X}(x)}\] \[=\sum_{x\in\mathcal{X}}Q_{X}(x)\left[\sum_{y\in\mathcal{Y}}Q_{Y \mid X}(y|x)\log\tfrac{Q_{Y\mid X}(y|x)Q_{X}(x)}{R_{Y\mid X}(y|x)R_{X}(x)}\right]\] \[=\sum_{x\in\mathcal{X}}Q_{X}(x)\left[\sum_{y\in\mathcal{Y}}Q_{Y \mid X}(y|x)\log\tfrac{Q_{Y\mid X}(y|x)}{R_{Y\mid X}(y|x)}+\sum_{y\in\mathcal{Y }}Q_{Y\mid X}(y|x)\log\tfrac{Q_{X}(x)}{R_{X}(x)}\right]\] \[=\sum_{x\in\mathcal{X}}Q_{X}(x)\left[\sum_{y\in\mathcal{Y}}Q_{Y \mid X}(y|x)\log\tfrac{Q_{Y\mid X}(y|x)}{R_{Y\mid X}(y|x)}\right]+\sum_{x\in \mathcal{X}}Q_{X}(x)\log\tfrac{Q_{X}(x)}{R_{X}(x)}\] \[=\sum_{x\in\mathcal{X}}Q_{X}(x)\operatorname{KL}(Q_{Y\mid X}( \cdot|x)\|R_{Y\mid X}(\cdot|x))+\operatorname{KL}(Q_{X}\|R_{X})\] \[=\sum_{x\in\mathcal{X}}P_{X}(x)\operatorname{KL}(Q_{Y\mid X}( \cdot|x)\|R_{Y\mid X}(\cdot|x))+\operatorname{KL}(P_{X}\|R_{X}),\]

where the last line is due to the marginal constraint \(Q\in\Pi_{X}\). For the above to be well defined, we need that \(P_{X}\ll R_{X}\) so that \(\operatorname{KL}(P_{X}\|R_{X})<+\infty\). The above is minimized when \(Q_{Y\mid X}(y|x)=R_{Y\mid X}(y|x)\) for all \((x,y)\in\mathcal{X}\times\mathcal{Y}\) such that \(Q_{X}(x)=P_{X}(x)>0\). The case of \(P^{\star}\) follows analogously when using that \(P_{Y}\ll R_{Y}\). 

#### c.1.2 Projection in Reverse KL-Divergence

**Proposition 9**.: _Assume that \(P_{Y}\ll R_{X}\) and \(P_{Y}\ll R_{Y}\), and define_

\[Q^{\star}:=\operatorname*{arg\,min}_{Q\in\Pi_{X}}\operatorname{KL}(R\|Q),\quad P ^{\star}:=\operatorname*{arg\,min}_{Q\in\Pi_{Y}}\operatorname{KL}(R\|Q).\] (22)

_Then, it holds that_

\[Q^{\star}(x,y)=\begin{cases}P_{X}(x)R_{Y\mid X}(y|x)&\text{ if }R_{X}(x)>0\\ 0&\text{ if }R_{X}(x)=0\end{cases}\] (23)

_and_

\[P^{\star}(x,y)=\begin{cases}P_{Y}(y)R_{X}(x|y)&\text{ if }R_{Y}(y)>0\\ 0&\text{ if }R_{Y}(y)=0\end{cases}.\] (24)

Proof.: In the case that \(R(x,y)=0\), we apply the convention that \(0\log 0=0\). Note that minimizing \(\operatorname{KL}(R\|Q)\) over \(Q\) is equivalent to minimizing \(-\sum_{x,y}R(x,y)\log Q(x,y)\) (i.e. the cross entropy). Consider the case \(Q^{\star}\), the projection of \(R\) onto \(\Pi_{X}\). Because \(R\ll Q\) for \(\operatorname{KL}(R\|Q)<+\infty\) to hold, we have that \(R(x)>0\implies Q(x)>0\), so that \(Q_{Y\mid X}(y|x)\) is well-defined. Write

\[-\sum_{x,y}R(x,y)\log Q(x,y)\] \[=-\sum_{x\in\mathcal{X}}R_{X}(x)\log Q_{X}(x)-\sum_{x\in\mathcal{ X}}R(x)\sum_{y\in\mathcal{Y}}R_{Y\mid X}(y|x)\log Q_{Y\mid X}(y|x)\] \[=-\sum_{x\in\mathcal{X}}R_{X}(x)\log P_{X}(x)+\sum_{x\in\mathcal{ X}}R_{X}(x)\left[-\sum_{y\in\mathcal{Y}}R_{Y\mid X}(y|x)\log Q_{Y\mid X}(y|x) \right].\]

The second first term does not depend on \(Q\) due to the marginal constraint \(Q\in\Pi_{X}\). The second term is the expectation of the cross entropy from \(R_{Y\mid X}\) to \(Q_{Y\mid X}\) over \(R_{X}\), which is minimized if \(R_{Y\mid X}=Q_{Y\mid X}\). We have specified \(Q_{Y\mid X}\) and \(Q_{X}\), completing the proof.

#### c.1.3 Projection in \(\chi^{2}\)-Divergence

Let \(\mathbf{1}\) denote the function that is identically equal to \(1\). Consider the following optimization problem, which is the subject of the subsequent lemmas:

\[\min_{\xi\in\mathcal{A}_{X}}\left\|\mathbf{1}-\xi\right\|_{\mathbf{L}^{2}(R)}^{2 },\] (25)

where

\[\mathcal{A}_{X}:=\left\{f:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\text{ satisfying }\sum_{y\in\mathcal{Y}}f(x,y)R(x,y)=P_{X}(x)\text{ for any }x\in\mathcal{X}\right\}.\]

**Lemma 10**.: _Assume that \(P_{X}\ll R_{X}\), and define The problem (25) is feasible, and its solution can be written as_

\[\xi^{\star}=\mathcal{C}_{X}^{R}(\mathbf{1}-f)+f\]

_for any \(f\in\mathbf{L}^{2}(R)\), where the linear operator \(\mathcal{C}_{X}^{R}\) is specified by_

\[[\mathcal{C}_{X}^{R}g](x,y)=g(x,y)-\sum_{y^{\prime}\in\mathcal{Y}}g(x,y^{ \prime})R_{Y|X}(y^{\prime}|x).\]

Proof.: First, we establish feasibility by letting

\[f(x,y):=\begin{cases}P_{X}(x)/R_{X}(x)&\text{ if }R_{X}(x)>0\\ 1&\text{ otherwise}\end{cases}.\]

This function does not depend on the second input \(y\). Because we assumed that \(P_{X}\ll R_{X}\), we have that the terms of \(f(x,y)\) for which \(R_{X}(x)=0\) do not affect whether \(\sum_{y\in\mathcal{Y}}f(x,y)R(x,y)=P_{X}(x)\), because \(P_{X}(x)=0\) in these cases. In the remainder of this proof, we will show that (25) is an affine projection problem, and find its solution by converting it to a subspace projection problem. Indeed, consider \(f_{1},\ldots,f_{r}\in\mathcal{A}_{X}\), and \(\alpha_{1},\ldots,\alpha_{r}\in\mathbb{R}\) such that \(\sum_{j=1}^{r}\alpha_{j}=1\). Then,

\[\sum_{y\in\mathcal{Y}}\left[\sum_{j=1}^{r}\alpha_{j}f_{j}(x,y)\right]\cdot R( x,y)=\sum_{j=1}^{r}\alpha_{j}\left[\sum_{y\in\mathcal{Y}}f_{j}(x,y)R(x,y) \right]=P_{X}(x),\]

indicating that \(\sum_{j=1}^{r}\alpha_{j}f_{j}(x,y)\in\mathcal{A}_{X}\) and \(\mathcal{A}_{X}\) is an affine subset of \(\mathbf{L}^{2}(R)\). Define

\[\mathcal{S}_{X}:=\left\{g:\mathcal{X}\times\mathcal{Y}\to\mathbb{R}\text{ satisfying }\sum_{y\in\mathcal{Y}}g(x,y)R(x,y)=0\text{ for any }x\in\mathcal{X}\right\}.\]

Then, for any \(f\in\mathcal{A}_{X}\), we have that \(g\in\mathcal{S}_{X}\) if and only if \(g+f\in\mathcal{A}_{X}\). Taking any \(f\in\mathcal{A}_{X}\), letting \(\phi^{\star}\) be the solution of

\[\min_{\phi\in\mathcal{S}_{X}}\left\|\mathbf{1}-f-\phi\right\|_{\mathbf{L}^{2} (R)}^{2},\] (26)

we will have that \(\phi^{\star}+f\) will be the solution of (25). The remainder of the proof is showing that \(\phi^{\star}=\mathcal{C}_{X}^{R}(\mathbf{1}-f)\).

First, define the operator \(\mu_{X}^{R}\) by \([\mu_{X}g](x,y)=\sum_{y^{\prime}\in\mathcal{Y}}g(x,y^{\prime})R_{Y|X}(y^{ \prime}|x)\), and note (by factoring out \(R_{X}(x)\)) that \(g\in\mathcal{S}_{X}\) if and only if \(\mu_{X}^{R}g=0\). In addition, \(\mu_{X}^{R}g\) is linear and idempotent as \(\mu_{X}^{R}\mu_{X}^{R}g=\mu_{X}^{R}g\), so it is a projection operator in \(\mathbf{L}^{2}(R)\). Thus, \(\mathcal{S}_{X}\) is the orthogonal complement of \(\operatorname{range}(\mu_{X}^{R})\), and the solution of (26) is given by \((I-\mu_{X}^{R})(\mathbf{1}-f)=\mathcal{C}_{X}^{R}(\mathbf{1}-f)\), because \(\mathcal{C}_{X}^{R}=I-\mu_{X}^{R}\). The claim is proved. 

**Lemma 11**.: _Assume that \(P_{X}\ll R_{X}\). Define_

\[Q^{\star}:=\operatorname*{arg\,min}_{Q\in\Pi_{X}}\chi^{2}(Q\|R).\] (27)

_and let \(\xi^{\star}\) be the solution of problem (25). Then,_

\[Q^{\star}(x,y)=\xi^{\star}(x,y)R(x,y)=\begin{cases}P_{X}(x)R_{Y|X}(y|x)&\text{ if }R_{X}(x)>0\\ 0&\text{ if }R_{X}(x)=0\end{cases}.\] (28)Proof.: First, by reparametrizing the problem (27) as finding \(\xi\) such that \(Q(x,y)=\xi(x,y)R(x,y)\), we can compute its solution by solving

\[\min_{\xi\in\mathcal{A}_{X},\xi\geq 0}\|\mathbf{1}-\xi\|_{\mathbf{L}^{2}(R)}^{2}\,,\] (29)

Notice that we also have a non-negativity constraint, as opposed to (25). If \(\xi^{\star}\) solves (25) and happens to be non-negative, then we have that \(\xi^{\star}\) solves (29) as well and the first equality of (28) is satisfied by definition. We show the second equality of (28) by direct computation, which also establishes the non-negativity of \(\xi^{\star}\) simultaneously.

Apply Lem. 10 with

\[f(x,y):=\begin{cases}P_{X}(x)/R_{X}(x)&\text{if }R_{X}(x)>0\\ 1&\text{otherwise}\end{cases}.\]

so that

\[\xi^{\star}(x,y) =\mathcal{C}_{X}^{R}\left(\mathbf{1}-f\right)(x,y)+f(x,y)\] \[=\left[\sum_{z\in\mathcal{Y}}f(x,z)R_{Y|X}(z|x)-f(x,y)\right]+f(x,y)\] \[=f(x,y^{\prime})\]

for any \(y^{\prime}\in\mathcal{Y}\). Thus, the likelihood ratio of \(Q^{\star}\) with respect to \(R\) is a marginal reweighting. Accordingly,

\[Q^{\star}(x,y)=\xi^{\star}(x,y)R(x,y)=\begin{cases}P_{X}(x)R_{Y|X}(y|x)&\text {if }R_{X}(x)>0\\ 0&\text{if }R_{X}(x)=0\end{cases},\]

completing the proof. 

**Proposition 12**.: _Assume that \(P_{X}\ll R_{X}\) and \(P_{Y}\ll R_{Y}\). Define_

\[Q^{\star}:=\operatorname*{arg\,min}_{Q\in\Pi_{X}}\chi^{2}(Q\|R),\quad P^{\star }:=\operatorname*{arg\,min}_{Q\in\Pi_{Y}}\chi^{2}(Q\|R).\] (30)

_Then, it holds that_

\[Q^{\star}(x,y) =\begin{cases}P_{X}(x)R_{Y|X}(y|x)&\text{if }R_{X}(x)>0\\ 0&\text{if }R_{X}(x)=0\end{cases}\] \[P^{\star}(x,y) =\begin{cases}P_{Y}(y)R_{X|Y}(x|y)&\text{if }R_{Y}(y)>0\\ 0&\text{if }R_{Y}(y)=0\end{cases}.\] (31)

Proof.: The first equality of (31) follows by the claim of Lem. 11. The second equality follows by repeating the argument of Lem. 10 and Lem. 11 with \((X,x)\) and \((Y,y)\) swapped. 

### Proof of Main Results

We may now control the errors of the ratio of marginals using the projection interpretation established in the previous sections. Recall the event \(\mathcal{S}\) as defined in Tab. 1. The following result, the monotonicity of the marginal violation terms in terms of \(\operatorname{KL}\), will be useful in the bound.

**Proposition 13**.: _[_11_, Proposition 6.10]_ _Under the event \(\mathcal{S}\), it holds that_

\[\operatorname{KL}(P_{n,X}^{{(0)}}\|P_{X})\geq \operatorname{KL}(P_{Y}\|P_{n,Y}^{{(1)}})\geq \operatorname{KL}(P_{n,X}^{{(2)}}\|P_{X})\geq\ldots\]

We give the following result for \(\mathcal{X}\) and the analogous claim holds on \(\mathcal{Y}\).

**Proposition 14**.: _Assume that \(P_{n,X}(x)>0\) for all \(x\in\mathcal{X}\). It holds that_

\[\max_{x\in\mathcal{X}}\left|\frac{P_{X}(x)}{P_{n,X}^{{(k-1)}}(x)}-1\right| \leq\begin{cases}\max\{n-1,1\}&\text{if }k=1\\ \max\{1/p_{\star}^{2}-1,1\}&\text{if }k>1.\end{cases}\] (32)_In addition, we have that_

\[\max_{x\in\mathcal{X}}\left|\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\right|\leq\begin{cases} n\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|P_{X})}&\text{if }k=1\\ \frac{1}{p_{\star}^{2}}\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|P_{X})}& \text{if }k>1\end{cases}.\]

_Moreover, when \(\operatorname{KL}(P_{n,X}\|P_{X})\leq p_{\star}^{2}/2\), we have_

\[\max_{x\in\mathcal{X}}\left|\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\right|\leq \frac{2}{p_{\star}}\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|P_{X})}.\] (33)

Proof.: We first show that \(P_{n}^{(k-1)}(x)\geq 1/n\) for \(k=1\) and \(P_{n}^{(k-1)}(x)\geq p_{\star}^{2}\) for \(k>1\). In the case that \(k=1\), the result follows directly from the event \(\mathcal{S}\). For \(k>1\) such that \(k\) is odd, we have that for \(x\in\mathcal{X}\),

\[P_{n,X}^{(k-1)}(x) =\sum_{y\in\mathcal{Y}}P_{n}^{(k-1)}(x,y)=\sum_{y\in\mathcal{Y}} \frac{P_{Y}(y)}{P_{n,Y}^{(k-2)}(y)}P_{n}^{(k-2)}(x,y)\] \[\geq p_{\star}\sum_{y\in\mathcal{Y}}P_{n}^{(k-2)}(x,y)=p_{\star}P _{n,X}^{(k-2)}(x)=p_{\star}P_{X}(x)\geq p_{\star}^{2}.\]

The result for \(k\) even can be proven similarly. We now proceed to prove the inequalities given in the statement, which will rely on the lower bound above.

**Proving the first inequality.** Then, for any \(x\in\mathcal{X}\),

\[\left|\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\right|=\max\left\{\frac{P_{X}(x)}{ P_{n,X}^{(k-1)}(x)}-1,1-\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}\right\}\leq\begin{cases} \max\{n-1,1\}&\text{if }k=1\\ \max\{1/p_{\star}^{2}-1,1\}&\text{if }k>1\end{cases},\]

which is the desired result for the first inequality.

**Proving the second and third inequalities.** Consider an odd \(k\geq 1\). By the definition of total variation distance, it holds that

\[\max_{x\in\mathcal{X}}\left|P_{X}(x)-P_{n,X}^{(k-1)}(x)\right|\leq\operatorname {TV}(P_{n,X}^{(k-1)},P_{X}).\]

According to Pinsker's inequality, we have that \(\operatorname{TV}(P_{n,X}^{(k-1)},P_{X})\leq\sqrt{\frac{1}{2}\operatorname{ KL}(P_{n,X}^{(k-1)}\|P_{X})}\), and so we have that

\[\max_{x\in\mathcal{X}}\left|P_{X}(x)-P_{n,X}^{(k-1)}(x)\right|\leq\sqrt{\frac {1}{2}\operatorname{KL}(P_{n,X}^{(k-1)}\|P_{X})}\leq\sqrt{\frac{1}{2} \operatorname{KL}(P_{n,X}^{(0)}\|P_{X})},\]

where the last inequality follows by the monotonicity of Sinkhorn iterations given in Prop. 13. We apply the lower bounds to write

\[\max_{x\in\mathcal{X}}\left|\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\right|\leq \begin{cases}n\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|P_{X})}&\text{if }k=1\\ \frac{1}{p_{\star}^{2}}\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|P_{X})}& \text{if }k>1\end{cases}.\]

Finally, when \(\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|P_{X})}\leq p_{\star}/2\), we have that \(\max_{x\in\mathcal{X}}\left|P_{X}(x)-P_{n,X}^{(k-1)}(x)\right|\leq p_{\star}/2\) and thus

\[\min_{x\in\mathcal{X}}P_{n,X}^{(k-1)}(x)\geq\min_{x\in\mathcal{X}}P_{X}(x)- \max_{x\in\mathcal{X}}\left|P_{n,X}^{(k-1)}(x)-P_{X}(x)\right|\geq\frac{p_{ \star}}{2}.\]

Hence,

\[\max_{x\in\mathcal{X}}\left|\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\right|\leq \frac{\max_{x\in\mathcal{X}}\left|P_{n,X}^{(k-1)}(x)-P_{X}(x)\right|}{\min_{x \in\mathcal{X}}P_{n,X}^{(k-1)}(x)}\leq\frac{2}{p_{\star}}\sqrt{\frac{1}{2} \operatorname{KL}(P_{n,X}\|P_{X})}.\]Now, for \(k\) even, set \(k=2t\) for \(t\geq 0\). We have that

\[\max_{y\in\mathcal{X}}\left|P_{n,Y}^{(2t-1)}(y)-P_{Y}(y)\right|\leq\mathrm{TV}( P_{n,Y}^{(2t-1)},P_{Y})\leq\sqrt{\frac{1}{2}\operatorname{KL}(P_{Y}\|P_{n,Y}^{(2t-1) })}.\]

Invoke Prop. 13 once again to achieve

\[\sqrt{\frac{1}{2}\operatorname{KL}(P_{Y}\|P_{n,Y}^{(2t-1)})}\leq\sqrt{\frac{1} {2}\operatorname{KL}(P_{n,X}\|P_{X})},\]

which completes the proof. 

## Appendix D Statistical Analysis of Balancing Estimators

This section contains the proof of the main result, namely Thm. 1. We first consolidate notation and then give a broad outline of the proof for readability. Let the expectation of a function \(h\) under a probability measure \(Q\) on \(\mathcal{X}\times\mathcal{Y}\) by denoted by

\[Q(h)=\sum_{x\in\mathcal{X},y\in\mathcal{Y}}h(x,y)Q(x,y)\]

so that

\[\varphi_{n}^{(k)}=P_{n}^{(k)}(h),\quad\varphi=P(h),\]

and

\[\mathcal{G}_{n}^{(k)}(h)=\sqrt{n}[P_{n}^{(k)}-P](h)=\sqrt{n}(P_{n}^{(k)}(h)-P (h)).\] (34)

Recalling in addition that \(\mathcal{C}_{k}=\mathcal{C}_{X}\) for \(k\) odd and \(\mathcal{C}_{k}=\mathcal{C}_{Y}\) for \(k\) even. The event

\[\mathcal{S}:=\left\{\text{Supp}(P_{n,X})=\text{Supp}(P_{X})\text{ and }\text{Supp}(P_{n,Y})=\text{Supp}(P_{Y})\right\},\] (35)

is used for purely technical reasons in many results.

Proof Outline.: We first establish that the recursion formula

\[[P_{n}^{(k)}-P](h)=[P_{n}^{(k-1)}-P](\mathcal{C}_{k}h)+V_{n}^{(k-1)}(\mathcal{ C}_{k}h)\]

holds in Prop. 15, where

\[V_{n}^{(k-1)}(h)=\begin{cases}\sum_{x,y}\left(\frac{P_{X}}{P_{n,X}^{(k-1)}}(x )-1\right)h(x,y)P_{n}^{(k-1)}(x,y)&k\text{ odd}\\ \sum_{x,y}\left(\frac{P_{Y}}{P_{n,Y}^{(k-1)}}(y)-1\right)h(x,y)P_{n}^{(k-1)}(x,y)&k\text{ even}\end{cases}.\] (36)

The quantity \(V_{n}^{(k-1)}(\mathcal{C}_{k}h)\) describes an error term that accumulates for each iteration of balancing, which explains why \(k\) must be scaled appropriately against \(n\) to ensure the error does not accumulate too fast. Applying the recursion repeatedly to the balanced sequence \((P_{n}^{(k)})_{k\geq 1}\) and unrolling the recursion, we see that when \(k\) is odd,

\[[P_{n}^{(k)}-P](h) =[P_{n}^{(k-1)}-P](\mathcal{C}_{X}h)+V_{n}^{(k-1)}(\mathcal{C}_ {X}h)\] \[=[P_{n}^{(k-2)}-P](\mathcal{C}_{Y}\mathcal{C}_{X}h)+V_{n}^{(k-2)} (\mathcal{C}_{Y}\mathcal{C}_{X}h)+V_{n}^{(k-1)}(\mathcal{C}_{X}h)\] \[=\underbrace{[P_{n}^{(0)}-P](\mathcal{C}_{1}\dots\mathcal{C}_{k}h )}_{\text{first-order term}}+\underbrace{\sum_{\ell=1}^{k}V_{n}^{(\ell-1)}( \mathcal{C}_{\ell}\dots\mathcal{C}_{k}h)}_{\text{higher-order term}}\] (37)

Additionally, let \(h_{\ell,k}:=\mathcal{C}_{\ell}\dots\mathcal{C}_{k}h\), so that the first-order term can be written as \(P_{n}^{(0)}(h_{1,k})-P(h_{1,k})\) higher-order term can also be written as \(\sum_{\ell=1}^{k}V_{n}^{(\ell-1)}(h_{\ell,k})\). Because our original goal is to upper bound the mean squared error, we use the expansion above to write

\[\mathbb{E}\left|P_{n}^{(k)}(h)-P(h)\right|^{2}\leq\mathbb{E}\left|P _{n}^{(0)}(h_{1,k})-P(h_{1,k})\right|^{2}\] \[\quad+2\mathbb{E}\left|P_{n}^{(0)}(h_{1,k})-P(h_{1,k})\right| \left|\sum_{\ell=1}^{k}V_{n}^{(\ell-1)}(h_{\ell,k})\right|+\mathbb{E}\left| \sum_{\ell=1}^{k}V_{n}^{(\ell-1)}(h_{\ell,k})\right|^{2}\]

Regarding the first term, we have that \(\mathbb{E}\left|P_{n}^{(0)}(h_{1,k})-P(h_{1,k})\right|^{2}=\sigma_{k}^{2}/n\), which is the dominant term in Thm. 1. Thus, the remaining challenge of the proof will be to upper bound the cross term and squared term and show its dependence on \(n\). The dominant term of these two will be the cross term, as we will essentially show that \(\left|P_{n}^{(0)}(h_{1,k})-P(h_{1,k})\right|\) is \(O(n^{-1/2})\) with high probability and that \(\left|\sum_{\ell=1}^{k}V_{n}^{(\ell-1)}(h_{\ell,k})\right|\) is in fact \(O(n^{-1})\) with high probability. As stated in Sec. 3, a key intermediate result in controlling the higher-order term is Prop. 14, whose proof is given in Appx. \(\mathbb{C}\). The remaining subsections walk through these steps in detail.

### Recursion of Estimation Error

We first recall that the sequence \((P_{n}^{(k)})_{k\geq 1}\) can be computed with the following formula:

\[P_{n}^{(0)}(x,y):=P_{n}(x,y)\text{ and }P_{n}^{(k)}(x,y):=\begin{cases}\frac{P_{X}}{P_{n }^{(k-1)}}(x)P_{n}^{(k-1)}(x,y)&\text{$k$ odd}\\ \frac{P_{X}^{(k)}}{P_{n,Y}^{(k-1)}}(y)P_{n}^{(k-1)}(x,y)&\text{$k$ even}\end{cases}.\] (38)

Prop. 15 establishes the conditions under which these steps are well-defined (i.e. \(P_{n,X}^{(k-1)}(x)>0\) and \(P_{n,Y}^{(k-1)}(y)>0\)).

**Proposition 15**.: _Let \((P_{n}^{(k)})_{k\geq 1}\), be a sequence computed according to (8). These iterations are well-defined under the event \(\mathcal{S}\), and for \(\mathbb{G}_{n}^{(k)}\) defined in (34) and \(V_{n}^{(k)}\) defined in (36), it holds that_

\[\mathbb{G}_{n}^{(k)}(h)=\mathbb{G}_{n}^{(k-1)}(h)+\sqrt{n}V_{n}^{(k-1)}(h).\] (39)

_and_

\[\mathbb{G}_{n}^{(k)}(h)=\mathbb{G}_{n}^{(k-1)}(\mathcal{C}_{k}h)+\sqrt{n}V_{n }^{(k-1)}(\mathcal{C}_{k}h).\] (40)

Proof.: First, assume that \(P_{n,X}^{(k-1)}(x)>0\) and \(P_{n,Y}^{(k-1)}(y)>0\) for all \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\) so that we may establish the recursion, which we will show by induction toward the end of the proof.

Consider the following steps in the case that \(k\) is odd:

\[P_{n}^{(k)}(h)\] \[=\sum_{x,y}h(x,y)P_{n}^{(k)}(x,y)=\sum_{x,y}h(x,y)\frac{P_{X}}{P_ {n,X}^{(k-1)}}(x)P_{n}^{(k-1)}(x,y) \text{by (\ref{eq:10}) for $k$ odd}\] \[=\sum_{x,y}1\cdot h(x,y)P_{n}^{(k-1)}(x,y)+\sum_{x,y}\left[\frac {P_{X}}{P_{n,X}^{(k-1)}}(x)-1\right]\cdot h(x,y)P_{n}^{(k-1)}(x,y)\] \[=P_{n}^{(k-1)}(h)+V_{n}^{(k-1)}(h).\]

Arguing analogously for \(k\) even and subtracting \(P(h)\) on both sides, we have that

\[[P_{n}^{(k)}-P](h)=[P_{n}^{(k-1)}-P](h)+V_{n}^{(k-1)}(h).\] (41)

We refer to this as the "unccentered" recursion, which proves (39).

We can then establish the following "centered" recursion using the following decomposition in the case of \(k\) odd.

\[[P_{n}^{(k)}-P](h)\] \[=[P_{n}^{(k)}-P](\mathcal{C}_{X}h)+[P_{n}^{(k)}-P](\mu_{X}h) h=\mathcal{C}_{X}h+\mu_{X}h\] \[=[P_{n}^{(k-1)}-P](\mathcal{C}_{X}h)+V_{n}^{(k-1)}(\mathcal{C}_{ X}h)+[P_{n}^{(k)}-P](\mu_{X}h) \text{apply (\ref{eq:10}) to $\mathcal{C}_{X}h$}\] \[=[P_{n}^{(k-1)}-P](\mathcal{C}_{X}h)+V_{n}^{(k-1)}(\mathcal{C}_{ X}h). P_{n}^{(k)}(\mu_{X}h)=P(\mu_{X}h)\]

The last line follows because \(\mu_{X}h\) is only a function on \(\mathcal{X}\), and due to the definition of the marginal rebalancing iterations, \(P_{n,X}^{(k)}=P_{X}\). This gives the desired formula by substituting (34).

We proceed to show that the iterations are well-defined. We will in fact show that \(P_{n,X}^{(k-1)}(x)>0\) and \(P_{n,Y}^{(k-1)}(y)>0\) for all \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\). For \(k=1\), \(P_{n,X}^{(0)}(x)=P_{n,X}(x)>0\) and \(P_{n,Y}^{(0)}(y)=P_{n,Y}(y)>0\) for all \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\) this holds under the event \(\mathcal{S}\) by assumption. We argue by induction that this holds for all \(k>1\). Assume that the claim is true for \(\{1,\ldots,k-1\}\), and that \(k\) is even. Then,

\[P_{n,X}^{(k-1)}(x) =P_{X}(x)>0,\] \[P_{n,Y}^{(k-1)}(y) =\sum_{x\in\mathcal{X}}P_{n}^{(k-1)}(x,y)=\sum_{x\in\mathcal{X}} \frac{P_{X}}{P_{n,X}^{(k-2)}}(x)P_{n}^{(k-2)}(x,y)\] \[\geq\min_{x\in\mathcal{X}}\frac{P_{X}}{P_{n,X}^{(k-2)}}(x)\cdot P _{n,Y}^{(k-2)}(y)>0\]

as \(P_{n,X}^{(k-2)}(x)>0\) and \(P_{n,Y}^{(k-2)}(y)>0\) by the inductive hypothesis. Arguing analogously for \(k\) odd achieves the claim.

### Technical Tools & Intermediate Results

Having established the backbone of the argument, we collect in this subsection some useful tools that are used in the remainder of the proofs.

The following result follows from the method of types in information theory and will be helpful in deriving the dependence of the higher-order term on \(n\).

**Theorem 16**.: _[_Cover_,_ 1999, Theorem 11.2.1]_ _Let \(\nu\) be a discrete probability measure supported on \(m\) atoms. Let \(U_{1},\ldots,U_{n}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\nu\) and \(\nu_{n}\) be the associated empirical measure. Then, we have for any \(\epsilon>0\) that_

\[\mathbb{P}\left(\operatorname{KL}(\nu_{n}\|\nu)\geq\epsilon\right)\leq 2^{-n \left(\epsilon-m\frac{\log(n+1)}{n}\right)}.\]

We then provide a result that counts the number of terms that appear when repeatedly centering via the operators \(\mathcal{C}_{1},\ldots,\mathcal{C}_{k}\). This formalizes the pattern

\[\mathcal{C}_{X} =I-\mu_{X}\] \[\mathcal{C}_{Y}\mathcal{C}_{X} =I-\mu_{X}-\mu_{Y}+\mu_{Y}\mu_{X}\] \[\mathcal{C}_{X}\mathcal{C}_{Y}\mathcal{C}_{X} =I-\mu_{X}-\mu_{Y}+\mu_{Y}\mu_{X}+\mu_{X}\mu_{Y}-\mu_{X}\mu_{Y}\mu _{X},\]

and so on. This will be useful when bounding \(h_{\ell,k}\) uniformly.

**Lemma 17**.: _For any \(k\geq 1\) and \(\ell\in\{1,\ldots,k\}\),_

\[\mathcal{C}_{\ell}\ldots\mathcal{C}_{k} =I-\sum_{\tau=0}^{(k-\ell-1)/2}(\mu_{X}\mu_{Y})^{\tau}\mu_{X}- \sum_{\tau=0}^{(k-\ell-1)/2}(\mu_{Y}\mu_{X})^{\tau}\mu_{Y}\] \[\quad+\sum_{\tau=1}^{(k-\ell)/2}(\mu_{X}\mu_{Y})^{\tau}+\sum_{ \tau=1}^{(k-\ell)/2}(\mu_{Y}\mu_{X})^{\tau}+(-1)^{k-\ell+1}\mu_{\ell}\ldots\mu _{k},\]

_where the sum \(\sum_{\tau=i}^{j}\) is 0 when \(i>j\) and is \(\sum_{\tau=i}^{\lfloor j\rfloor}\) when \(j\) is not an integer by convention._

Proof.: We prove the claim by backward induction on \(\ell\), for the case that \(k\) is odd. In the case \(\ell=k\), the claim holds because \(\mathcal{C}_{k}=I-\mu_{k}\). Next, for any \(\ell<k\), assume that the stated result holds for \(\{\ell+1,\ldots,k\}\). Then, if \(\ell\) is also odd (so that \(\mu_{\ell}=\mu_{X}\)),

\[\mathcal{C}_{\ell}\ldots\mathcal{C}_{k} =\mathcal{C}_{\ell}\mathcal{C}_{\ell+1}\ldots\mathcal{C}_{k}\] \[=I-\sum_{\tau=0}^{(k-\ell-2)/2}(\mu_{X}\mu_{Y})^{\tau}\mu_{X}- \sum_{\tau=0}^{(k-\ell-2)/2}(\mu_{Y}\mu_{X})^{\tau}\mu_{Y}\] \[\quad+\sum_{\tau=1}^{(k-\ell-1)/2}(\mu_{X}\mu_{Y})^{\tau}+\sum_{ \tau=1}^{(k-\ell-1)/2}(\mu_{Y}\mu_{X})^{\tau}+\mu_{Y}\underbrace{\cdot\cdot \cdot}_{k-\ell\text{ terms}}\mu_{X}\] \[\quad-\mu_{X}+\sum_{\tau=0}^{(k-\ell-2)/2}(\mu_{X}\mu_{Y})^{\tau} \mu_{X}+\sum_{\tau=0}^{(k-\ell-2)/2}\mu_{X}(\mu_{Y}\mu_{X})^{\tau}\mu_{Y}\] \[\quad-\sum_{\tau=1}^{(k-\ell-1)/2}(\mu_{X}\mu_{Y})^{\tau}-\sum_{ \tau=1}^{(k-\ell-1)/2}\mu_{X}(\mu_{Y}\mu_{X})^{\tau}-(\mu_{X}\mu_{Y})^{(k-\ell )/2}\mu_{X}\]

The red terms and blue terms cancel out to zero. This leaves

\[\mathcal{C}_{\ell}\ldots\mathcal{C}_{k} =I-\sum_{\tau=0}^{(k-\ell-2)/2}(\mu_{X}\mu_{Y})^{\tau}\mu_{X}-\sum_ {\tau=0}^{(k-\ell-2)/2}(\mu_{Y}\mu_{X})^{\tau}\mu_{Y}\] \[\quad+\sum_{\tau=1}^{(k-\ell-1)/2}(\mu_{Y}\mu_{X})^{\tau}+(\mu_{Y }\mu_{X})^{(k-\ell)/2}\] \[\quad+\sum_{\tau=0}^{(k-\ell-2)/2}\mu_{X}(\mu_{Y}\mu_{X})^{\tau} \mu_{Y}+(-1)^{k-\ell+1}\mu_{\ell}\ldots\mu_{k}\]wherein we combine the red terms and re-index the blue terms to get

\[\mathcal{C}_{\ell}\ldots\mathcal{C}_{k} =I-\sum_{\tau=0}^{(k-\ell-2)/2}(\mu_{X}\mu_{Y})^{\tau}\mu_{X}-\sum_ {\tau=0}^{(k-\ell-2)/2}(\mu_{Y}\mu_{X})^{\tau}\mu_{Y}\] \[\quad+\sum_{\tau=1}^{(k-\ell)/2}(\mu_{Y}\mu_{X})^{\tau}+\sum_{\tau =1}^{(k-\ell)/2}(\mu_{X}\mu_{Y})^{\tau}+(-1)^{k-\ell+1}\mu_{\ell}\ldots\mu_{k}.\]

Finally, because \(k-\ell\) is even when \(k\) is odd and \(\ell\) is odd, we can set the upper bound of the first two sums to \((k-\ell-1)/2\) without changing the number of terms. This proves the desired result. The result can be proved similarly when \(\ell\) is even. As a result, we have proved the claim for any odd k and \(\ell\leq k\). Similar arguments can be used for the case of \(k\) even and \(\ell\leq k\). 

### Analysis of Higher-Order Term

Returning to the outline at the start of this section, we may now bound the higher-order remainder term in (37), namely

\[\sum_{\ell=1}^{k}V_{n}^{(\ell-1)}(h_{\ell,k})=\sum_{\ell=1}^{k}V_{n}^{(\ell-1 )}(\mathcal{C}_{\ell}\ldots\mathcal{C}_{k}h),\]

depends on controlling the quantity \(V_{n}^{(k-1)}\) in the summation, which we recall for convenience:

\[V_{n}^{(k-1)}(h)=\begin{cases}\sum_{x,y}\left(\frac{P_{X}}{P_{n,X}^{(k-1)}}(x) -1\right)h(x,y)P_{n}^{(k-1)}(x,y)&\text{$k$ odd}\\ \sum_{x,y}\left(\frac{P_{Y}}{P_{n,Y}^{(k-1)}}(y)-1\right)h(x,y)P_{n}^{(k-1)}(x,y)&\text{$k$ even}\end{cases}.\] (42)

Because we have established uniform control over the functions \(P_{X}/P_{n,X}^{(k-1)}-1\) and \(P_{Y}/P_{n,Y}^{(k-1)}-1\), via Prop. 14 in Appx. \(\mathrm{C}\) we can now bound the full remainder in Prop. 20.

We also make use of the following intermediate result, which controls how large the \(\ell_{\infty}\)-norm of the function \(h\) can grow after centering.

**Lemma 18**.: \(\left\lVert h_{\ell,k}\right\rVert_{\infty}\leq 2(k-\ell+1)\left\lVert h \right\rVert_{\infty}\)_._

Proof.: Apply Lem. 17 and the triangle inequality, so that we only need to count the number of terms that appear in the sums, adding \(2\) for the first and last term in the expression. We subtract \(1\) from the total, as one of either \((k-\ell)/2\) or \((k-\ell+1)/2\) will be a fraction. This yields \(2(k-\ell+1)\) terms total, the desired result. 

We upper bound the sum in Prop. 20. To do so, we introduce some notation. Consider \(B_{1}\) and \(B_{2}\) defined by

\[B_{1}:=M_{1}\quad\text{ and }\quad B_{2}:=\max_{2\leq\ell\leq k}M_{\ell} \quad\text{ for }\quad M_{\ell}:=\begin{cases}\max_{x\in\mathcal{X}}\left|\frac{P_{X}(x)}{P_{n,X}^{(k-1)}(x)}-1\right|&\text{$\ell$ odd}\\ \max_{y\in\mathcal{Y}}\left|\frac{P_{Y}(y)}{P_{n,Y}^{(k-1)}(y)}-1\right|&\text{$ \ell$ even}\end{cases}\]

for \(k\geq 1\). We also enumerate the sample spaces as \(\mathcal{X}=\{x_{1},\ldots,x_{m}\}\) and \(\mathcal{Y}=\{y_{1},\ldots,y_{m}\}\), and define the function

\[\mathbf{1}_{jk}(x,y):=\begin{cases}\mathds{1}\left\{x=x_{j}\right\}&\text{$k$ odd}\\ \mathds{1}\left\{y=y_{j}\right\}&\text{$k$ even}\end{cases}.\]

This is an indicator function on the \(j\)-th element of either \(\mathcal{X}\) or \(\mathcal{Y}\) depending on whether \(k\) is odd or even. Finally, for any function \(h\), use (under the event \(\mathcal{S}\)) recall the empirical process notation

\[\mathbb{G}_{n}^{(k)}(h):=\sqrt{n}\left(P_{n}^{(k)}(h)-P(h)\right).\] (43)

Using this notation, we can rewrite the recursion in terms of the quantity \(\mathbb{G}_{n}^{(k)}(h)\) itself. This is established in the following lemma.

**Lemma 19**.: _For \(k\) odd, it holds that_

\[\mathbb{G}_{n}^{{}^{(k)}}(h)=\mathbb{G}_{n}^{{}^{(k-1)}}( \mathcal{C}_{X}h)+\sum_{j=1}^{m}\left[\frac{P_{X}(x_{j})}{P_{n,X}^{{}^{(k-1)}}( x_{j})}-1\right]\mathbb{G}_{n}^{{}^{(k-1)}}(\mathcal{C}_{X}h\,\mathbf{1}_{jk}),\]

_whereas for \(k\) even, it holds that_

\[\mathbb{G}_{n}^{{}^{(k)}}(h)=\mathbb{G}_{n}^{{}^{(k-1)}}( \mathcal{C}_{Y}h)+\sum_{j=1}^{m}\left[\frac{P_{Y}(y_{j})}{P_{n,Y}^{{}^{(k-1)}}( y_{j})}-1\right]\mathbb{G}_{n}^{{}^{(k-1)}}(\mathcal{C}_{Y}h\,\mathbf{1}_{jk}),\]

Proof.: We give the proof for \(k\) odd. By (40) from Prop. 15 and by the definition of \(\mathbb{G}_{n}^{{}^{(k)}}(h)\), we need only show that \(P(\mathcal{C}_{X}h\mathbf{1}_{jk})=0\). Indeed,

\[\mathbb{E}\left[\mathcal{C}_{X}h\mathbf{1}_{jk}|X\right](x)= \begin{cases}\mathbb{E}\left[\mathcal{C}_{X}h|X\right](x_{j})&\text{ if }x=x_{j}\\ 0&\text{ if }x\neq x_{j}\end{cases}.\]

But \(\mathbb{E}\left[\mathcal{C}_{X}h|X\right](x_{j})=0\) by definition of \(\mathcal{C}_{X}\). Taking an expectation over \(P_{X}\) gives that \(P(\mathcal{C}_{X}h\mathbf{1}_{jk})=0\), which implies the desired result. The proof for \(k\) even follows symmetrically. 

The higher-order term in (37), can be bounded using Prop. 20.

**Proposition 20**.: _For any \(k\geq 1\), the following holds under the event \(\mathcal{S}\):_

\[\sqrt{n}\sum_{\ell=1}^{k}|V_{n}^{{}^{(\ell-1)}}( \mathcal{C}_{\ell}\ldots\mathcal{C}_{k}h)| \leq\sum_{j=1}^{m}\left(B_{1}\left|\mathbb{G}_{n}^{{}^{(0)}}( h_{1,k}\mathbf{1}_{j\ell})\right|+B_{2}\sum_{\ell=2}^{k}|\mathbb{G}_{n}^{{}^{(0)}}( h_{\ell,k}\mathbf{1}_{j\ell})|\right)\] \[\quad+mB_{2}\left\|h\right\|_{\infty}\sqrt{n}k(k-1)[B_{1}+B_{2}(k +1)/3].\]

Proof.: First, for any \(\ell\in\{1,\ldots,k\}\), recall the notation \(h_{\ell,k}:=\mathcal{C}_{\ell}\ldots\mathcal{C}_{k}h\). By (39) from Prop. 15 and by Lem. 19, we have that for \(\ell\) odd,

\[\sqrt{n}V_{n}^{{}^{(\ell-1)}}(h_{\ell,k})=\sum_{j=1}^{m}\left[ \frac{P_{X}}{P_{n,X}^{{}^{(\ell-1)}}}(x_{j})-1\right]\mathbb{G}_{n}^{{}^{(\ell -1)}}(h_{\ell,k}\mathbf{1}_{j\ell}).\] (44)

Using the statement above, we have that

\[\sqrt{n}\left|V_{n}^{{}^{(\ell-1)}}(h_{\ell,k})\right|\leq M_{ \ell}\sum_{j=1}^{m}\left|\mathbb{G}_{n}^{{}^{(\ell-1)}}(h_{\ell,k}\,\mathbf{1} _{j\ell})\right|.\]

The bound above holds for \(\ell\) even as well. Then, using the (39) from Prop. 15 again, we have that for \(\ell\geq 2\),

\[[P_{n}^{{}^{(\ell-1)}}-P](h_{\ell,k}\mathbf{1}_{j\ell})=[P_{n}^{{}^{(\ell-2)}} -P](h_{\ell,k}\mathbf{1}_{j\ell})+V_{n}^{{}^{(\ell-2)}}(h_{\ell,k} \mathbf{1}_{j\ell})\]

which implies that

\[\left|\mathbb{G}_{n}^{{}^{(\ell-1)}}(h_{\ell,k}\mathbf{1}_{j \ell})\right| \leq\left|\mathbb{G}_{n}^{{}^{(\ell-2)}}(h_{\ell,k} \mathbf{1}_{j\ell})\right|+\sqrt{n}\left|V_{n}^{{}^{(\ell-2)}}(h_{\ell,k} \mathbf{1}_{j\ell})\right|\] \[\leq\left|\mathbb{G}_{n}^{{}^{(0)}}(h_{\ell,k} \mathbf{1}_{j\ell})\right|+M_{1}\sqrt{n}P_{n}^{{}^{(0)}}(\left|h_{\ell,k} \mathbf{1}_{j\ell}\right|+\ldots+M_{\ell}\sqrt{n}P_{n}^{{}^{(\ell-2)}}(\left|h _{\ell,k}\right|\mathbf{1}_{j\ell})\] \[\leq\left|\mathbb{G}_{n}^{{}^{(0)}}(h_{\ell,k} \mathbf{1}_{j\ell})\right|+2\left\|h\right\|_{\infty}\sqrt{n}\left[B_{1}+B_{2} (\ell-1)\right](k-\ell+1),\] (45)

[MISSING_PAGE_EMPTY:29]

**Proposition 21**.: _It holds that \(P(\mathcal{S}^{c})\leq 2m(1-p_{\star})^{n}\). Moreover, for any \(\delta\in(0,1)\), we have_

\[\mathbb{E}_{P}\left[\left(P_{n}(h)-P(h)\right)^{2}\mathbf{1}_{\mathcal{S}^{c}} \right]\leq 4\left\|h\right\|_{\infty}^{2}\min\left\{2m(1-p_{\star})^{n}, \delta\right\}+\frac{2\log(2/\delta)}{n}\left\|h\right\|_{\infty}^{2}2m(1-p_{ \star})^{n}.\]

Proof.: Define \(\mathcal{F}_{X}:=\{\text{Supp}(P_{n,X})\neq\text{Supp}(P_{X})\}\) and \(\mathcal{F}_{Y}:=\{\text{Supp}(P_{n,Y})\neq\text{Supp}(P_{Y})\}\), so that \(\mathcal{S}^{c}=\mathcal{F}_{X}\cup\mathcal{F}_{Y}\). We first control the probability of \(\mathcal{F}_{X}\). Let \(F_{j}:=\{P_{n,X}(x_{j})=0\}\) for \(j\in[m]\). We then obtain \(\mathcal{F}_{X}=\cup_{j=1}^{m}F_{j}\), which implies by the union bound that

\[P(\mathcal{F}_{X})\leq\sum_{j=1}^{m}P(F_{j})=\sum_{j=1}^{m}(1-P_{X}(x_{j}))^{n }\leq m(1-p_{\star})^{n}.\]

Similarly, we have that \(P(\mathcal{F}_{Y})\leq m(1-p_{\star})^{n}\) and thus \(P(\mathcal{S}^{c})\leq 2m(1-p_{\star})^{n}\), which gives the first claim.

To control the expectation, consider any \(\delta>0\), and define the event

\[\mathcal{E}_{\delta}:=\left\{\left|P_{n}^{(0)}(h)-P(h)\right|\leq\sqrt{\frac{2 \log\left(2/\delta\right)}{n}}\left\|h\right\|_{\infty}\right\}.\]

By Hoeffding's inequality, it holds that \(P(\mathcal{E}_{\delta})\geq 1-\delta\). Furthermore, we get

\[\mathbb{E}[\mathds{1}_{\mathcal{S}^{c}}(P_{n}^{(0)}(h)-P(h))^{2}] =\mathbb{E}[\mathds{1}_{\mathcal{S}^{c}}\mathds{1}_{\mathcal{E}_ {\delta}^{c}}(P_{n}^{(0)}(h)-P(h))^{2}]+\mathbb{E}[\mathds{1}_{\mathcal{S}^{c }}\mathds{1}_{\mathcal{E}_{\delta}}(P_{n}^{(0)}(h)-P(h))^{2}]\] \[\leq 4\left\|h\right\|_{\infty}^{2}\mathbb{E}[\mathds{1}_{ \mathcal{S}^{c}}\mathds{1}_{\mathcal{E}_{\delta}^{c}}]+\frac{2\log\left(2/ \delta\right)}{n}\left\|h\right\|_{\infty}^{2}\mathbb{E}[\mathds{1}_{\mathcal{ S}^{c}}\mathds{1}_{\mathcal{E}_{\delta}}]\] \[\leq 4\left\|h\right\|_{\infty}^{2}\min\{P(\mathcal{S}^{c}),P( \mathcal{E}_{\delta}^{c})\}+\frac{2\log\left(2/\delta\right)}{n}\left\|h \right\|_{\infty}^{2}P(\mathcal{S}^{c})\] \[\leq 4\left\|h\right\|_{\infty}^{2}\min\{2m(1-p_{\star})^{n}, \delta\}+\frac{2\log\left(2/\delta\right)}{n}\left\|h\right\|_{\infty}^{2}2m(1 -p_{\star})^{n}.\]

In order to bound the terms appearing in (48), we introduce the events \(\mathcal{E}_{1}^{\delta}\), \(\mathcal{E}_{2}^{\delta}\), and \(\mathcal{E}_{3}^{\delta}\), defined by

\[\mathcal{E}_{1}^{\delta} :=\left\{\max\left\{\mathrm{KL}(P_{n,X}\|P_{X}),\mathrm{KL}(P_{n, Y}\|P_{Y})\right\}\leq\frac{1}{n}\log_{2}\frac{2}{\delta}+m\frac{\log(n+1)}{n}\right\}\] \[\mathcal{F}_{\ell}^{\delta} :=\left\{\left|\mathbb{G}_{n}^{{}^{(0)}}(h_{t,k}\,\mathbf{1}_{j \ell})\right|\leq\sqrt{2\log(2mk/\delta)}2(k-\ell+1)\left\|h\right\|_{\infty} \right\},\quad\ell=1,\ldots,k,\;j=1,\ldots,m\] \[\mathcal{E}_{2}^{\delta} :=\bigcap_{\ell=1}^{k}\mathcal{F}_{\ell}^{\delta}\] \[\mathcal{E}_{3}^{\delta} :=\left\{\left|\mathbb{G}_{n}^{{}^{(0)}}(h_{1,k})\right|\leq\sqrt {2\log(2/\delta)}2k\left\|h\right\|_{\infty}\right\}.\]

The events are constructed such that \(\mathbb{P}(\mathcal{E}_{1}^{\delta})\geq 1-\delta\), \(\mathbb{P}(\mathcal{E}_{2}^{\delta})\geq 1-\delta\), and \(\mathbb{P}(\mathcal{E}_{3}^{\delta})\geq 1-\delta\), as we used in the upcoming proofs of Lem. 23, Lem. 22, and Thm. 24.

**Lemma 22** (Squared term bound).: _Let \(T_{2}\) be defined as in (49). For any \(\delta>0\), assuming that \(n\geq 2[\log_{2}(2/\delta)+m\log(n+1)]/p_{\star}^{2}\), we have that_

\[\mathbb{E}_{P}\left[T_{2}^{2}\mathds{1}_{\mathcal{S}}\right]\leq \frac{2\left\|h\right\|_{\infty}^{2}m^{2}k^{2}}{p_{\star}^{2}}\left[\log_{2}(2/ \delta)+m\log(n+1)\right]^{2-1\left\{k=1\right\}}\;\times\] \[\left[\left(4n+\frac{k-1}{p_{\star}^{2}}\left(n+2+\frac{k+1}{p_{ \star}^{2}}\right)\right)^{2}\delta+\frac{8}{n^{2}}\left(\sqrt{2\log\frac{2mk}{ \delta}}(k+1)+\frac{(k-1)(k+4)}{p_{\star}^{2}}\right)^{2}\right].\]

Proof.: The following computations are done under the event \(\mathcal{S}\). First, apply Prop. 20 to write

\[\left|T_{2}\right| \leq\frac{1}{\sqrt{n}}\sum_{j=1}^{m}\left(B_{1}\left|\mathbb{G}_{ n}^{{}^{(0)}}(h_{1,k}\,\mathbf{1}_{j\ell})\right|+B_{2}\sum_{\ell=2}^{k}\left| \mathbb{G}_{n}^{{}^{(0)}}(h_{\ell,k}\,\mathbf{1}_{j\ell})\right|\right)\;+\] \[mB_{2}\left\|h\right\|_{\infty}k(k-1)[B_{1}+B_{2}(k+1)/3].\] (50)We decompose on the event \(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}\). Note that by Thm. 16, we have that \(\mathbb{P}(\mathcal{E}_{1}^{\delta})\geq 1-\delta\). It follows from Hoeffding's inequality, the union bound, and boundedness of \(\left\|h_{\ell,k}\,\mathbf{1}_{j\ell}\right\|\) by Lem. 18 that \(\mathbb{P}(\mathcal{E}_{2}^{\delta})\geq 1-\delta\) As a result, \(\mathbb{P}(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta})\geq 1-2\delta\).

Bound \(\left|T_{2}\right|\) under the event \(\mathcal{S}\backslash(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta})\).In this case, we apply (32) from Prop. 14 to get \(B_{1}\leq n\) and \(B_{2}\leq 1/p_{\star}^{2}\), along with the universal bounds from Lem. 18:

\[\frac{1}{\sqrt{n}}\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{1,k}\, \mathbf{1}_{j\ell})\right| \leq 2\left\|h_{1,k}\right\|_{\infty}\leq 4k\left\|h\right\|_{\infty}\] \[\frac{1}{\sqrt{n}}\sum_{\ell=2}^{k}\left|\mathbb{G}_{n}^{{}_{(0) }}(h_{\ell,k}\,\mathbf{1}_{j\ell})\right| \leq 2\sum_{\ell=2}^{k}\left\|h_{\ell,k}\right\|_{\infty}\leq\sum_{ \ell=2}^{k}4(k-\ell+1)\left\|h\right\|_{\infty}=2k(k-1)\left\|h\right\|_{\infty}\]

so that by plugging into (50),

\[\left|T_{2}\right|\leq\left\|h\right\|_{\infty}mk\left[4n+\frac{k-1}{p_{\star} ^{2}}\left(n+2+\frac{k+1}{3p_{\star}^{2}}\right)\right],\]

and in turn,

\[\mathbb{E}_{P}\left[T_{2}^{2}\mathds{1}_{\mathcal{S}\backslash(\mathcal{E}_{1 }^{\delta}\cap\mathcal{E}_{2}^{\delta})}\right]\leq 2\left\|h\right\|_{\infty}^{2}m^{2}k ^{2}\left[4n+\frac{k-1}{p_{\star}^{2}}\left(n+2+\frac{k+1}{3p_{\star}^{2}} \right)\right]^{2}\delta.\] (51)

Bound \(\left|T_{2}\right|\) under the event \(\mathcal{S}\cap\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}\).In this case, we may use that \(n\geq 2[\log_{2}(2/\delta)+m\log(n+1)]/p_{\star}^{2}\) apply (33) from Prop. 14 to get

\[\max\left\{B_{1},B_{2}\right\}\leq\frac{2}{p_{\star}}\sqrt{\frac{1}{2}\operatorname {KL}(P_{n,X}\|P_{X})}\leq\frac{1}{p_{\star}\sqrt{n}}\sqrt{2\log_{2}(2/\delta)+ 2m\log(n+1)}\]

and the bounds based on \(\mathcal{E}_{2}^{\delta}\) which give

\[\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{1,k}\,\mathbf{1}_{j\ell})\right| \leq\sqrt{2\log\frac{2mk}{\delta}}2k\left\|h\right\|_{\infty}\] \[\sum_{\ell=2}^{k}\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{\ell,k}\, \mathbf{1}_{j\ell})\right| \leq\sum_{\ell=2}^{k}\sqrt{2\log\frac{2mk}{\delta}}2(k-\ell+1) \left\|h\right\|_{\infty}\leq\sqrt{2\log\frac{2mk}{\delta}}k(k-1)\left\|h \right\|_{\infty},\]

By plugging into (50),

\[\left|T_{2}\right| \leq\frac{2m\left\|h\right\|_{\infty}\sqrt{2\log(2mk/\delta) \left[2\log_{2}(2/\delta)+2m\log(n+1)\right]}}{np_{\star}}k(k+1)+\] (52) \[\quad\frac{m\left\|h\right\|_{\infty}\left[2\log_{2}(2/\delta)+ 2m\log(n+1)\right]}{3np_{\star}^{2}}k(k-1)(k+4)\] (53) \[\leq\frac{4mk\left\|h\right\|_{\infty}\left[\log_{2}(2/\delta)+2m \log(n+1)\right]^{1-\mathds{1}\left\{k=1\right\}/2}}{np_{\star}^{2}}\times\] (54) \[\quad\left[p_{\star}\sqrt{2\log{(2mk/\delta)}}(k+1)+(k-1)(k+4) \right].\] (55)

In turn,

\[\mathbb{E}_{P}\left[T_{2}^{2}\mathds{1}_{\mathcal{S}\backslash( \mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta})}\right] \leq\frac{16\left\|h\right\|_{\infty}^{2}m^{2}k^{2}\left[\log_{2}(2/ \delta)+m\log(n+1)\right]^{2-\mathds{1}\left\{k=1\right\}}}{n^{2}p_{\star}^{4}}\times\] \[\quad\left[p_{\star}\sqrt{2\log(2mk/\delta)}(k+1)+(k-1)(k+4) \right]^{2}.\] (56)

Combining together both (56) and (51) and using that \([\log_{2}(2/\delta)+2m\log(n+1)]\geq 1\), we have that

\[\mathbb{E}_{P}\left[T_{2}^{2}\mathds{1}_{\mathcal{S}}\right]\leq \frac{2\left\|h\right\|_{\infty}^{2}m^{2}k^{2}}{p_{\star}^{2}}\left[\log_{2}(2/ \delta)+m\log(n+1)\right]^{2-\mathds{1}\left\{k=1\right\}}\times\] \[\left[\left(4n+\frac{k-1}{p_{\star}^{2}}\left(n+2+\frac{k+1}{p_{ \star}^{2}}\right)\right)^{2}\delta+\frac{8}{n^{2}}\left(\sqrt{2\log(2mk/\delta) }(k+1)+\frac{(k-1)(k+4)}{p_{\star}^{2}}\right)^{2}\right],\]

the result as desired.

**Lemma 23** (Cross term bound).: _Let \(T_{1}\) and \(T_{2}\) be defined as in (49). For any \(\delta>0\), assuming that \(n\geq 2[\log_{2}(2/\delta)+m\log(n+1)]/p_{\star}^{2}\), we have that_

\[\mathbb{E}_{P}\left[T_{1}T_{2}\mathds{1}_{\mathcal{S}}\right]\] \[\leq\frac{2mk^{2}\left\|h\right\|_{\infty}^{2}\sqrt{2\log(2/ \delta)}\left[\log_{2}(2/\delta)+2m\log(n+1)\right]^{1-1\left\{k=1\right\}/2} }{p_{\star}^{2}}\times\] \[\left[\frac{p_{\star}\sqrt{2\log\left(2mk/\delta\right)}(k+1)+(k- 1)(k+4)}{n^{3/2}}+6\left(4np_{\star}^{2}+(k-1)\left(n+2+\frac{k+1}{p_{\star}^{2 }}\right)\right)\delta\right],\]

Proof.: The following computations are done under the event \(\mathcal{S}\). First, apply Prop. 20 to write

\[|T_{1}T_{2}| \leq\frac{1}{\sqrt{n}}\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{1,k} )\right|\left[\frac{1}{\sqrt{n}}\sum_{j=1}^{m}\left(B_{1}\left|\mathbb{G}_{n} ^{{}_{(0)}}(h_{1,k}\,\mathbf{1}_{j\ell})\right|+B_{2}\sum_{\ell=2}^{k}\left| \mathbb{G}_{n}^{{}_{(0)}}(h_{\ell,k}\,\mathbf{1}_{j\ell})\right|\right)\right.+\] \[\left.mB_{2}\left\|h\right\|_{\infty}k(k-1)[B_{1}+B_{2}(k+1)/3] \right]\!.\] (57)

We decompose on the event \(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}\cap\mathcal{E}_{3}^{\delta}\). Note that by Thm. 16 and that \(n\geq\log_{2}(2/\delta)+m\log(n+1)\), we have that \(\mathbb{P}(\mathcal{E}_{1}^{\delta})\geq 1-\delta\). It follows by Hoeffding's inequality and the union bound that \(\mathbb{P}(\mathcal{E}_{2}^{\delta})\geq 1-\delta\). Similarly, we also have by Hoeffding's inequality that \(\mathbb{P}(\mathcal{E}_{3}^{\delta})\geq 1-\delta\). As a result, \(\mathbb{P}(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}\cap\mathcal{E }_{3}^{\delta})\geq 1-3\delta\).

Bound \(|T_{1}T_{2}|\) under the event \(\mathcal{S}\backslash(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta} \cap\mathcal{E}_{3}^{\delta})\).In this case, we apply (32) from Prop. 14 to get \(B_{1}\leq n\) and \(B_{2}\leq 1/p_{\star}^{2}\), along with the universal bounds from Lem. 18:

\[\frac{1}{\sqrt{n}}\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{1,k})\right| \leq 2\left\|h_{1,k}\right\|_{\infty}\leq 4k\left\|h\right\|_{\infty}\] \[\frac{1}{\sqrt{n}}\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{1,k}\, \mathbf{1}_{j\ell})\right| \leq 2\left\|h_{1,k}\right\|_{\infty}\leq 4k\left\|h\right\|_{\infty}\] \[\frac{1}{\sqrt{n}}\sum_{\ell=2}^{k}\left|\mathbb{G}_{n}^{{}_{(0)}}( h_{\ell,k}\,\mathbf{1}_{j\ell})\right| \leq 2\sum_{\ell=2}^{k}\left\|h_{\ell,k}\right\|_{\infty}\leq\sum_{\ell =2}^{k}4(k-\ell+1)\left\|h\right\|_{\infty}=2k(k-1)\left\|h\right\|_{\infty},\]

so that by plugging into (57),

\[|T_{1}T_{2}|\leq 4k^{2}\left\|h\right\|_{\infty}^{2}m\left[4n+\frac{k-1}{p_{ \star}^{2}}\left(n+2+\frac{k+1}{3p_{\star}^{2}}\right)\right],\]

and in turn,

\[\mathbb{E}_{P}\left[T_{1}T_{2}\mathds{1}_{\mathcal{S}\backslash(\mathcal{E}_{ 1}^{\delta}\cap\mathcal{E}_{2}^{\delta}\cap\mathcal{E}_{3}^{\delta})}\right] \leq\frac{12k^{2}\left\|h\right\|_{\infty}^{2}m}{p_{\star}^{2}}\left[4np_{ \star}^{2}+(k-1)\left(n+2+\frac{k+1}{3p_{\star}^{2}}\right)\right]\delta.\] (58)

Bound \(|T_{1}T_{2}|\) under the event \(\mathcal{S}\cap\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}\cap \mathcal{E}_{3}^{\delta}\).In this case, we may use that \(n\geq 2[\log_{2}(2/\delta)+m\log(n+1)]/p_{\star}^{2}\) apply (33) from Prop. 14 to get

\[\max\left\{B_{1},B_{2}\right\}\leq\frac{2}{p_{\star}}\sqrt{\frac{1}{2}\operatorname {KL}(P_{n,X}\|P_{X})}\leq\frac{1}{\sqrt{n}}\frac{1}{p_{\star}}\sqrt{2\log_{2}( 2/\delta)+2m\log(n+1)}\]

and the bounds based on \(\mathcal{E}_{2}^{\delta}\cap\mathcal{E}_{2}^{\delta}\cap\mathcal{E}_{3}^{\delta}\) which give

\[\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{1,k})\right| \leq\sqrt{2\log(2/\delta)}2k\left\|h\right\|_{\infty}\] \[\leq\sqrt{2\log(2mk/\delta)}2k\left\|h\right\|_{\infty}\] \[\sum_{\ell=2}^{k}\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{\ell,k}\, \mathbf{1}_{j\ell})\right| \leq\sum_{\ell=2}^{k}\sqrt{2\log\frac{2mk}{\delta}}2(k-\ell+1) \left\|h\right\|_{\infty}\leq\sqrt{2\log\frac{2mk}{\delta}}k(k-1)\left\|h \right\|_{\infty},\]By plugging into (57),

\[\left|T_{2}\right| \leq\frac{m\left\|h\right\|_{\infty}\sqrt{2\log(2mk/\delta)\left[2 \log_{2}(2/\delta)+2m\log(n+1)\right]}}{np_{\star}}k(k+1)\ +\] \[\quad\frac{m\left\|h\right\|_{\infty}\left[2\log_{2}(2/\delta)+2m \log(n+1)\right]}{3np_{\star}^{2}}k(k-1)(k+4)\] \[\leq\frac{mk\left\|h\right\|_{\infty}\left[\log_{2}(2/\delta)+2m \log(n+1)\right]^{1-\mathds{1}\{k=1\}/2}}{np_{\star}^{2}}\ \times\] \[\quad\left[p_{\star}\sqrt{2\log(2mk/\delta)}(k+1)+(k-1)(k+4)\right]\] \[\left|T_{1}T_{2}\right| \leq\frac{2mk^{2}\left\|h\right\|_{\infty}^{2}\sqrt{2\log(2/\delta )}\left[\log_{2}(2/\delta)+2m\log(n+1)\right]^{1-\mathds{1}\{k=1\}/2}}{n^{3/2} p_{\star}^{2}}\times\] \[\quad\left[p_{\star}\sqrt{2\log(2mk/\delta)}(k+1)+(k-1)(k+4) \right],\]

In turn,

\[\mathbb{E}_{P}\left[T_{2}^{2}\mathds{1}_{S\setminus(\xi_{1}^{s} \cap\xi_{2}^{s}\cap\xi_{3}^{s})}\right] \leq\frac{2mk^{2}\left\|h\right\|_{\infty}^{2}\sqrt{2\log(2/\delta )}\left[\log_{2}(2/\delta)+2m\log(n+1)\right]^{1-\mathds{1}\{k=1\}/2}}{n^{3/2} p_{\star}^{2}}\times\] \[\quad\left[p_{\star}\sqrt{2\log(2mk/\delta)}(k+1)+(k-1)(k+4) \right],\] (59)

Combining together both (59) and (58) and using that \([\log_{2}(2/\delta)+2m\log(n+1)]\geq 1\), we have that

\[\mathbb{E}_{P}\left[T_{1}T_{2}\mathds{1}_{S}\right]\] \[\leq\frac{2mk^{2}\left\|h\right\|_{\infty}^{2}\sqrt{2\log(2/ \delta)}\left[\log_{2}(2/\delta)+2m\log(n+1)\right]^{1-\mathds{1}\{k=1\}/2}}{ p_{\star}^{2}}\times\] \[\quad\left[\frac{p_{\star}\sqrt{2\log(2mk/\delta)}(k+1)+(k-1)(k+ 4)}{n^{3/2}}+6\left(4np_{\star}^{2}+(k-1)\left(n+2+\frac{k+1}{p_{\star}^{2}} \right)\right)\delta\right],\]

the result as desired. 

We now combine the previous results to prove Thm. 24.

**Theorem 24**.: _For a sequence of rebalanced distributions \((\tilde{P_{n}}^{(\lambda)})_{k\geq 1}\), there exists an absolute constant \(C>0\) such that when \(n\geq C[\log_{2}(2n/p_{\star})+m\log(n+1)]/p_{\star}^{2}\),_

\[\mathbb{E}_{P}[(\tilde{P_{n}}^{(k)}(h)-P(h))^{2}]\leq\frac{\sigma_{k}^{2}}{n}+ \frac{CB}{n^{3/2}},\] (60)

_where_

\[B=\frac{\sqrt{\log\left(2n/p_{\star}\right)}m^{2}k^{4}\left\|h\right\|_{ \infty}^{2}}{p_{\star}^{2}}\ \left(\log_{2}\frac{2n}{p_{\star}}+m\log \left(n+1\right)\right)^{2-\mathds{1}\{k\}}\ \left(\log\frac{2mkn}{p_{\star}}+\frac{(k-1)^{2}}{p_{\star}^{2}}\right).\]

Proof.: We apply the decomposition (47), and subsequently handle the second term using bounds on the terms in (48). Set \(\delta=p_{\star}^{1}/n^{4}\). We apply Lem. 22 and Lem. 23 with this choice of \(\delta\), so that there exists an absolute constants \(\tilde{C}\), \(C_{1}\), and \(C_{2}\) such that

\[\mathbb{E}_{P}\left[T_{1}T_{2}\mathds{1}_{S}\right] \leq C_{1}\frac{\left\|h\right\|_{\infty}^{2}m^{2}k^{3}\sqrt{\log (2n/p_{\star})}}{n^{3/2}p_{\star}^{2}}\left[\log_{2}(2n/p_{\star})+m\log(n+1) \right]^{1-\mathds{1}\{k=1\}/2}\ \times\] \[\quad\left(\log\frac{2mnk}{p_{\star}}+\frac{k-1}{p_{\star}^{2}}\right)\] \[\mathbb{E}_{P}\left[T_{2}^{2}\mathds{1}_{S}\right] \leq C_{2}\frac{\left\|h\right\|_{\infty}^{2}m^{2}k^{4}}{n^{2}p_{ \star}^{2}}\left[\log_{2}(2n/p_{\star})+m\log(n+1)\right]^{2-\mathds{1}\{k=1 \}}\ \times\] \[\quad\left(\log\frac{2mnk}{p_{\star}}+\frac{(k-1)^{2}}{p_{\star}^ {2}}\right),\]when \(n\geq\tilde{C}[\log_{2}(2n/p_{\star})+m\log{(n+1)}]/p_{\star}^{2}\). This then implies that there is an absolute constant \(C_{3}\) such that

\[\mathbb{E}_{P}\left[\left(\tilde{P_{n}}^{{}^{(k)}}(h)-P(h) \right)^{2}\right]\] \[\leq\mathbb{E}_{P}\left[\left(P_{n}^{{}^{(0)}}(h)-P(h) \right)^{2}\mathds{1}_{\mathcal{S}^{c}}\right]+\frac{\sigma_{k}^{2}}{n}\,+\] \[\frac{C_{3}\left\|h\right\|_{\infty}^{2}m^{2}k^{4}\sqrt{\log(2n/p _{\star})}}{n^{3/2}p_{\star}^{2}}\left[\log_{2}\frac{2n}{p_{\star}}+m\log(n+1) \right]^{2-1\left\{k=1\right\}}\left(\log\frac{2mnk}{p_{\star}}+\frac{(k-1)^{2 }}{p_{\star}^{2}}\right).\]

Next, we apply Prop. 21 with the same choice of \(\delta\). Because \(2[\log_{2}(2/\delta)+m\log(n+1)]\geq\log(m/\delta)\) and \(-\log(1-p_{\star})\geq p_{\star}\geq p_{\star}^{2}\), we have that \(n\geq\log(\delta/m)/\log(1-p_{\star})\), which implies that \(m(1-p_{\star})^{n}\leq\delta\). Combining with the display above, we have that there exists an absolute constant \(C>0\) such that

\[\mathbb{E}_{P}\left[\left(\tilde{P_{n}}^{{}^{(k)}}(h)-P(h) \right)^{2}\right] \leq\frac{\sigma_{k}^{2}}{n}+\frac{C\left\|h\right\|_{\infty}^{2 }m^{2}k^{4}\sqrt{\log(2n/p_{\star})}}{n^{3/2}p_{\star}^{2}}\] \[\times\left[\log_{2}(2/\delta)+m\log(n+1)\right]^{2-1\left\{k=1 \right\}}\left(\log\frac{2mnk}{p_{\star}}+\frac{(k-1)^{2}}{p_{\star}^{2}} \right),\]

which is the claimed result. 

While not shown in the main text, similar techniques to those used above can also control the bias of \(\tilde{P_{n}}^{{}^{(k)}}(h)\) as in Thm. 25. Interestingly, this bias is of order \(O(n^{-2})\) which confirms the intuition that even thought \(\tilde{P_{n}}^{{}^{(k)}}(h)\) may be biased, the dominant term is the variance.

**Theorem 25**.: _For a sequence of rebalanced distributions \((P^{{}^{(k)}})_{k\geq 1}\), there exists an absolute constant \(C>0\) such that when \(n\geq C[\log_{2}(2n/p_{\star})+m\log{(n+1)}]/p_{\star}^{2}\),_

\[\left|\mathbb{E}_{P}[\tilde{P_{n}}^{{}^{(k)}}(h)-P(h)]\right|^{2}\leq\frac{CB }{n^{2}},\] (61)

_where \(B\) is as defined in Thm. 24._

Proof.: First, apply the decomposition (47) so that

\[\left|\mathbb{E}_{P}\left[\tilde{P_{n}}^{{}^{(k)}}(h)-P(h)\right]\right|\leq \left|\mathbb{E}_{P}\left[(P_{n}(h)-P(h))\,\mathds{1}_{\mathcal{S}^{c}}\right] \right|+\left|\mathbb{E}_{P}\left[(P_{n}^{{}^{(k)}}(h)-P(h))\,\mathds{1}_{ \mathcal{S}}\right]\right|.\]

By using the argument of Prop. 21, we have that

\[\left|\mathbb{E}_{P}\left[P_{n}(h)-P(h)\right]\mathds{1}_{\mathcal{S}^{c}} \right|\leq 2\left\|h\right\|_{\infty}\min\left\{2m(1-p_{\star})^{n},\delta \right\}+\sqrt{\frac{2\log(2/\delta)}{n}}\left\|h\right\|_{\infty}2m(1-p_{ \star})^{n}.\]

Then, by the recursion formula Equation (37), we have that

\[\sqrt{n}\left|\mathbb{E}_{P}\left[(P_{n}^{{}^{(k)}}(h)-P(h))\, \mathds{1}_{\mathcal{S}}\right]\right|\] \[=\left|\mathbb{E}_{P}\left[\mathbb{G}_{n}^{{}^{(k)}}(h)\mathds{1 }_{\mathcal{S}}\right]\right|=\left|\mathbb{E}_{P}\left[(1-\mathds{1}_{ \mathcal{S}^{c}})\mathbb{G}_{n}^{{}^{(0)}}(\mathcal{C}_{1}\ldots\mathcal{C}_ {k}h)+\sqrt{n}\mathds{1}_{\mathcal{S}}\sum_{\ell=1}^{k}V_{n}^{{}^{(\ell-1)}}( \mathcal{C}_{\ell}\ldots\mathcal{C}_{k}h)\right]\right|.\]

Because \(\mathbb{G}_{n}^{{}^{(0)}}(\mathcal{C}_{1}\ldots\mathcal{C}_{k}h)\) has zero mean, it follows that

\[\sqrt{n}\left|\mathbb{E}_{P}\left[(P_{n}^{{}^{(k)}}(h)-P(h))\,\mathds{1}_{ \mathcal{S}}\right]\right|\leq\left|\mathbb{E}_{P}\left[\mathds{1}_{ \mathcal{S}^{c}}\mathbb{G}_{n}^{{}^{(0)}}(\mathcal{C}_{1}\ldots\mathcal{C}_ {k}h)\right]\right|+\sqrt{n}\left|\mathbb{E}_{P}\left[\mathds{1}_{\mathcal{S} }T_{2}\right]\right|\]

We have by Hoeffding's inequality that \(\mathbb{P}(\mathcal{E}_{\mathcal{S}}^{\delta})\geq 1-\delta\), and that by Lem. 18 that \(\mathbb{G}_{n}^{{}^{(0)}}(\mathcal{C}_{1}\ldots\mathcal{C}_{k}h)\leq 4k\sqrt{n}\left\|h\right\|_{\infty}\) universally. As a result, applying Prop. 21 once again,

\[\left|\mathbb{E}_{P}\left[\mathds{1}_{\mathcal{S}^{c}}\mathbb{G} _{n}^{{}^{(0)}}(\mathcal{C}_{1}\ldots\mathcal{C}_{k}h)\right]\right|\] \[\leq\left|\mathbb{E}_{P}\left[\mathds{1}_{\mathcal{S}^{c}} \mathds{1}_{\mathcal{S}^{c}}\mathbb{G}_{n}^{{}^{(0)}}(\mathcal{C}_{1}\ldots \mathcal{C}_{k}h)\right]\right|+\left|\mathbb{E}_{P}\left[\mathds{1}_{ \mathcal{S}^{c}}\mathds{1}_{\mathcal{S}^{c}}\mathbb{G}_{n}^{{}^{(0)}}( \mathcal{C}_{1}\ldots\mathcal{C}_{k}h)\right]\right|\] \[\leq 4k\sqrt{n}\left\|h\right\|_{\infty}\min\left\{2m(1-p_{\star})^{ n},\delta\right\}+\sqrt{2\log(2/\delta)}2k\left\|h\right\|_{\infty}2m(1-p_{\star})^{n}.\]Using a similar argument to Lem. 22, we have that under \(\mathcal{S}\backslash(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta})\) (which occurs with probability no more than \(2\delta\)),

\[\left|T_{2}\right|\leq\left\|h\right\|_{\infty}mk\left[4n+\frac{k-1}{p_{\star}^ {2}}\left(n+2+\frac{k+1}{3p_{\star}^{2}}\right)\right],\]

and that under \(\mathcal{S}\cap\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}\) (which occurs with probability at least \(1-2\delta\)),

\[\left|T_{2}\right|\leq\frac{4mk\left\|h\right\|_{\infty}\left[\log _{2}(2/\delta)+2m\log(n+1)\right]^{1-\mathds{1}(k=1)/2}}{np_{\star}^{2}}\] \[\quad\quad\left[p_{\star}\sqrt{2\log(2mk/\delta)}(k+1)+(k-1)(k+4) \right].\]

Applying the decomposition \(\left|\mathbb{E}_{P}\left[\mathds{1}_{\mathcal{S}}T_{2}\right]\right|\leq \left|\mathbb{E}_{P}\left[\mathds{1}_{\mathcal{S}\backslash(\mathcal{E}_{1}^{ \delta}\cap\mathcal{E}_{2}^{\delta})}T_{2}\right]\right|+\left|\mathbb{E}_{P} \left[\mathds{1}_{\mathcal{S}\cap\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^ {\delta}}T_{2}\right]\right|\) and setting \(\delta=\frac{p_{\star}^{2}}{n^{2}}\) achieves the desired result. 

### Misspecified Marginal Distributions

We now adapt the main results to cases in which the marginal distributions \((P_{X},P_{Y})\) are misspecified, in that the user is provided marginal distributions \((\hat{P}_{X,\epsilon},\hat{P}_{Y,\epsilon})\) which satisfy the following structure.

**Assumption 1**.: _There exist fixed probability mass functions \(\hat{P}_{X}\) and \(\hat{P}_{Y}\) for some \(\epsilon\in[0,1)\),_

\[\hat{P}_{X,\epsilon}=(1-\epsilon)P_{X}+\epsilon\hat{P}_{X}\text{ and }\hat{P}_{Y,\epsilon}=(1-\epsilon)P_{Y}+\epsilon\hat{P}_{Y}.\]

_We also have the existence of the positive quantity_

\[\hat{p}_{\star}:=\min\{\min_{x}\hat{P}_{X}(x),\min_{y}\hat{P}_{Y}(y)\}>0.\]

Given the existence of \(\hat{p}_{\star}>0\), we may also define

\[\hat{p}_{\star,\epsilon}=\min\{\min_{x}\hat{P}_{X,\epsilon}(x),\min_{y}\hat{P} _{Y,\epsilon}(y)\}\geq\epsilon\hat{p}_{\star}+(1-\epsilon)p_{\star}>0.\]

To be precise, the iterations of balancing follow \(\hat{P}_{n}^{{(0)}}=P_{n}\) and

\[\hat{P}_{n}^{{(k)}}(x,y):=\begin{cases}\frac{\hat{P}_{X, \epsilon}(x)}{\hat{P}_{n,X}^{{(k-1)}}(x)}\cdot\hat{P}_{n}^{{ (k-1)}}(x,y)&\text{$k$ odd}\\ \frac{\hat{P}_{Y,\epsilon}(y)}{\hat{P}_{n,Y}^{{(k-1)}}(y)}\cdot\hat{P}_{n}^{{ (k-1)}}(x,y)&\text{$k$ even}\end{cases}.\] (62)

We start by deriving a result similar to Prop. 15. Since \(\epsilon<1\), the (possibly misspecified) target marginals \(\hat{P}_{X,\epsilon}(x)>0\) and \(\hat{P}_{Y,\epsilon}(y)>0\) for all \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\). Define

\[\hat{V}_{n}^{{(k-1)}}(h):=\begin{cases}\sum_{x,y}\left(\frac{ \hat{P}_{X,\epsilon}}{\hat{P}_{n,X}^{{(k-1)}}(x)}(x-1)\right)h(x,y)\hat{P}_{n} ^{{(k-1)}}(x,y)&\text{$k$ odd}\\ \sum_{x,y}\left(\frac{\hat{P}_{Y,\epsilon}}{\hat{P}_{n,Y}^{{(k-1)}}(y)}(y-1) \right)h(x,y)\hat{P}_{n}^{{(k-1)}}(x,y)&\text{$k$ even}\end{cases}\] (63)

and

\[\hat{\mathbb{G}}_{n}^{{(k)}}(h):=\sqrt{n}\left(\hat{P}_{n}^{{ (k)}}(h)-P(h)\right).\]

The format of this section will be to derive results analogous to the building blocks of the previous section. From that point, the computations from Appx. D.4 will achieve the desired result. For the sake of comparison to Thm. 1 we consider error terms containing \(\epsilon\) only by their dependence on \((\epsilon,k,n,\hat{p}_{\star,\epsilon})\).

#### d.5.1 Intermediate Results

**Proposition 26**.: _Let \((\hat{P}_{n}^{{(k)}})_{k\geq 1}\) be a sequence computed according to (62). Define_

\[c^{2}=\max\left\{\chi^{2}(\hat{P}_{X}\|P_{X}),\chi^{2}(\hat{P}_{Y}\|P_{Y})\right\}.\]

_These iterations are well-defined under the event \(\mathcal{S}\), and for \(\mathbb{G}_{n}^{{(k)}}\) defined in (43), it holds that_

\[\hat{\mathbb{G}}_{n}^{{(k)}}(h)=\hat{\mathbb{G}}_{n}^{{ (k-1)}}(h)+\sqrt{n}\hat{V}_{n}^{{(k-1)}}(h)\] (64)

_and_

\[\hat{\mathbb{G}}_{n}^{{(k)}}(h)=\hat{\mathbb{G}}_{n}^{{ (k-1)}}(\mathcal{C}_{k}h)+\sqrt{n}\hat{V}_{n}^{{ (k-1)}}(\mathcal{C}_{k}h)+\begin{cases}\sqrt{n}[\hat{P}_{X,\epsilon}-P_{X}]( \mu_{X}h)&\text{if $k$ odd}\\ \sqrt{n}[\hat{P}_{Y,\epsilon}-P_{Y}](\mu_{Y}h)&\text{if $k$ even}\end{cases}.\] (65)

_Furthermore,_

\[\left|\hat{\mathbb{G}}_{n}^{{(k)}}(h)\right| \leq\left|\hat{\mathbb{G}}_{n}^{{(k-1)}}( \mathcal{C}_{k}h)\right|+\sqrt{n}\left|\hat{V}_{n}^{{ (k-1)}}(\mathcal{C}_{k}h)\right|+c\left\|h\right\|_{\mathbb{L}^{2}(P)}\sqrt{n \epsilon}\] \[=\left|\hat{\mathbb{G}}_{n}^{{(k-1)}}( \mathcal{C}_{k}h)\right|+\sqrt{n}\left|\hat{V}_{n}^{{ (k-1)}}(\mathcal{C}_{k}h)\right|+O\left(\sqrt{n\epsilon}\right).\] (66)

Proof.: The proof that \(\hat{P}_{n,X}^{{(k-1)}}(x)>0\) and \(\hat{P}_{n,Y}^{{(k-1)}}(y)>0\) for all \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\) follows the exact same steps as in the proof of Prop. 15. We take this for granted and establish the recursion.

Consider the following steps in the case that \(k\) is odd:

\[\hat{P}_{n}^{{(k)}}(h) =\sum_{x,y}h(x,y)\hat{P}_{n}^{{(k)}}(x,y)=\sum_{x,y}h(x,y) \frac{\hat{P}_{X,\epsilon}}{\hat{P}_{n,X}^{{(k-1)}}}(x)\hat{P}_{n}^{{ (k-1)}}(x,y)\] \[=\sum_{x,y}1\cdot h(x,y)\hat{P}_{n}^{{(k-1)}}(x,y)+ \sum_{x,y}\left[\frac{\hat{P}_{X,\epsilon}}{\hat{P}_{n,X}^{{(k-1)}}}(x)-1 \right]\cdot h(x,y)\hat{P}_{n}^{{(k-1)}}(x,y)\] \[=\hat{P}_{n}^{{(k-1)}}(h)+\hat{V}_{n}^{{ (k-1)}}(h).\]

Subtracting \(P(h)\) on both sides, we have that

\[[\hat{P}_{n}^{{(k)}}-P](h)=[\hat{P}_{n}^{{ (k-1)}}-P](h)+\hat{V}_{n}^{{(k-1)}}(h).\] (67)

This proves the uncentered recursion formula given in (64). We then show the centered version.

\[[\hat{P}_{n}^{{(k)}}-P](h)\] \[=[\hat{P}_{n}^{{(k)}}-P](\mathcal{C}_{X}h)+[\hat{P}_{n}^{ {(k)}}-P](\mu_{X}h)\] \[=[\hat{P}_{n}^{{(k)}}-P](\mathcal{C}_{X}h)+[\hat{P}_{X, \epsilon}-P_{X}](\mu_{X}h)\] \[=[\hat{P}_{n}^{{(k-1)}}-P](\mathcal{C}_{X}h)+\hat{V}_{n}^{ {(k-1)}}(\mathcal{C}_{X}h)+[\hat{P}_{X,\epsilon}-P_{X}](\mu_{X}h).\]

Next, we bound the additional error term. By the Cauchy-Schwarz inequality,

\[[\hat{P}_{X,\epsilon}-P_{X}](\mu_{X}h) \leq\left\|\frac{\hat{P}_{X,\epsilon}}{P_{X}}-\mathbf{1}\right\|_ {\mathbf{L}^{2}(P_{X})}\cdot\left\|\mu_{X}h\right\|_{\mathbf{L}^{2}(P_{X})}\] \[=\sqrt{\chi^{2}(\hat{P}_{X,\epsilon}\|P_{X})}\cdot\left\|\mu_{X}h \right\|_{\mathbf{L}^{2}(P_{X})}\] \[\leq\sqrt{\chi^{2}(\hat{P}_{X,\epsilon}\|P_{X})}\cdot\left\|h \right\|_{\mathbf{L}^{2}(P)},\]

as \(\mu_{X}\) is an orthogonal projection in \(\mathbf{L}^{2}(P)\). Using convexity of \(f\)-divergences, we have that

\[\chi^{2}(\hat{P}_{X,\epsilon}\|P_{X})\leq\epsilon\chi^{2}(\hat{P}_{X}\|P_{X})+( 1-\epsilon)\chi^{2}(P_{X}\|P_{X})=\epsilon\chi^{2}(\hat{P}_{X}\|P_{X}).\]

This achieves the desired result. 

Using similar ideas, we then prove an analog of Lem. 19.

**Lemma 27**.: _For \(k\) odd, it holds that_

\[\sqrt{n}\hat{V}_{n}^{{}_{(k-1)}}(\mathcal{C}_{X}h)=\sum_{j=1}^{m}\left(\frac{\hat {P}_{X,\epsilon}}{\hat{P}_{n,X}^{{}_{(k-1)}}}(x_{j})-1\right)\hat{\mathbb{G}}_{n }^{{}_{(k-1)}}(\mathcal{C}_{X}h\,\mathbf{1}_{jk}),\]

_whereas for \(k\) even, it holds that_

\[\sqrt{n}\hat{V}_{n}^{{}_{(k-1)}}(\mathcal{C}_{Y}h)=\sum_{j=1}^{m}\left(\frac{ \hat{P}_{Y,\epsilon}}{\hat{P}_{n,Y}^{{}_{(k-1)}}}(x_{j})-1\right)\hat{\mathbb{ G}}_{n}^{{}_{(k-1)}}(\mathcal{C}_{Y}h\,\mathbf{1}_{jk}).\]

Proof.: We give the proof for \(k\) odd. We claim that we need only show that \(P(\mathcal{C}_{X}h\mathbf{1}_{jk})=0\). This would show that

\[\sqrt{n}\hat{V}_{n}^{{}_{(k-1)}}(\mathcal{C}_{X}h) =\sum_{x,y}\left(\frac{\hat{P}_{X,\epsilon}}{\hat{P}_{n,X}^{{}_{( k-1)}}}(x)-1\right)[\mathcal{C}_{X}h](x,y)\hat{P}_{n}^{{}_{(k-1)}}(x,y)\] \[=\sqrt{n}\sum_{j=1}^{m}\sum_{x,y}\left(\frac{\hat{P}_{X,\epsilon} }{\hat{P}_{n,X}^{{}_{(k-1)}}}(x)-1\right)[\mathcal{C}_{X}h\,\mathbf{1}_{jk}](x,y)\hat{P}_{n}^{{}_{(k-1)}}(x,y)\] \[=\sqrt{n}\sum_{j=1}^{m}\sum_{x,y}\left(\frac{\hat{P}_{X,\epsilon} }{\hat{P}_{n,X}^{{}_{(k-1)}}}(x_{j})-1\right)[\mathcal{C}_{X}h\,\mathbf{1}_{jk }](x,y)\hat{P}_{n}^{{}_{(k-1)}}(x,y)\] \[=\sum_{j=1}^{m}\left(\frac{\hat{P}_{X,\epsilon}}{\hat{P}_{n,X}^{ {}_{(k-1)}}}(x_{j})-1\right)\sqrt{n}\sum_{x,y}[\mathcal{C}_{X}h\,\mathbf{1}_{ jk}](x,y)\hat{P}_{n}^{{}_{(k-1)}}(x,y)\] \[=\sum_{j=1}^{m}\left(\frac{\hat{P}_{X,\epsilon}}{\hat{P}_{n,X}^{ {}_{(k-1)}}}(x_{j})-1\right)\hat{\mathbb{G}}_{n}^{{}_{(k-1)}}(\mathcal{C}_{X} h\,\mathbf{1}_{jk}),\]

where \(P(\mathcal{C}_{X}h\mathbf{1}_{jk})=0\) is employed in the last step. Now the result follows from (65) in Prop. 26 and the definition of \(\hat{\mathbb{G}}_{n}^{{}_{(k)}}(h)\). To prove the claim, as in Lem. 19, write

\[\mathbb{E}\left[\mathcal{C}_{X}h\mathbf{1}_{jk}|X\right](x)=\begin{cases} \mathbb{E}\left[\mathcal{C}_{X}h|X\right](x_{j})&\text{ if }x=x_{j}\\ 0&\text{ if }x\neq x_{j}\end{cases}.\]

But \(\mathbb{E}\left[\mathcal{C}_{X}h|X\right](x_{j})=0\) by definition of \(\mathcal{C}_{X}\). Taking an expectation over \(P_{X}\) gives that \(P(\mathcal{C}_{X}h\mathbf{1}_{jk})=0\), which implies the desired result. The proof for \(k\) even follows symmetrically. 

For the remainder of the argument, we see that (66) can be unrolled so that

\[\left|\hat{\mathbb{G}}_{n}^{{}_{(k)}}(h)\right|\leq\underbrace{|\mathbb{G}_{ n}^{{}_{(0)}}(\mathcal{C}_{1}\ldots\mathcal{C}_{k}h)|}_{\text{first-order term}}+\underbrace{\sqrt{n}\sum_{\ell=1}^{k}\left|\hat{V}_{n}^{{}_{(\ell-1)}}( \mathcal{C}_{\ell}\ldots\mathcal{C}_{k}h)\right|}_{\text{higher-order term}}+ \underbrace{O(k\sqrt{n\epsilon})}_{\text{physpecification}},\] (68)

where we use that \(\mathbb{G}_{n}^{{}_{(0)}}=\hat{\mathbb{G}}_{n}^{{}_{(0)}}\).

Next, we need to bound \(\left|\hat{V}_{n}^{{}_{(\ell-1)}}(\mathcal{C}_{\ell}\ldots\mathcal{C}_{k}h)\right|\), in particular accounting for the marginal violation term. We follow similar steps as in the analysis of the higher-order term in Appx. D.3.

**Proposition 28**.: _Assume that \(P_{n,X}(x)>0\) for all \(x\in\mathcal{X}\). It holds that_

\[\max_{x\in\mathcal{X}}\left|\frac{\hat{P}_{X,\epsilon}(x)}{\hat{P}_{n,X}^{{}_{ (k-1)}}(x)}-1\right|\leq\begin{cases}\max\{n-1,1\}&\text{if }k=1\\ \max\{1/\hat{p}_{\ast,\epsilon}^{2}-1,1\}&\text{if }k>1.\end{cases}\] (69)

_In addition, we have that_

\[\max_{x\in\mathcal{X}}\left|\frac{\hat{P}_{X,\epsilon}(x)}{\hat{P}_{n,X}^{{}_{ (k-1)}}(x)}-1\right|\leq\begin{cases}O\left(n\sqrt{\log\frac{1}{1-\epsilon}} \right)+n\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|P_{X})}&\text{if }k=1\\ O\left(\frac{1}{\hat{p}_{\ast,\epsilon}^{2}}\sqrt{\log\frac{1}{1-\epsilon}}\right)+ \frac{1}{\hat{p}_{\ast,\epsilon}^{2}}\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X} \|P_{X})}&\text{if }k>1\end{cases},\]_Moreover, when \(\operatorname{KL}(P_{n,X}\|P_{X})\leq\frac{\hat{P}_{2,*}^{(k-1)}}{8}\) and \(\epsilon\leq 1-\exp\left(-\frac{\hat{p}_{2,*}^{(k-1)}}{8}\right)\), we have_

\[\max_{x\in\mathcal{X}}\left|\frac{\hat{P}_{X,\epsilon}(x)}{\hat{P}_{n,X}^{(k-1 )}(x)}-1\right|\leq O\left(\tfrac{1}{\hat{p}_{*,\epsilon}}\sqrt{\log\frac{1}{1- \epsilon}}\right)+\frac{2}{\hat{p}_{*,\epsilon}}\sqrt{\frac{1}{2}\operatorname {KL}(P_{n,X}\|P_{X})}.\] (70)

Proof.: First, observe that \(\hat{P}_{n,X}^{(0)}(x)=P_{n,X}^{(0)}(x)\geq 1/n\) under the event \(\mathcal{S}\). For \(k>1\) such that \(k\) is odd, we have that for \(x\in\mathcal{X}\),

\[\hat{P}_{n,X}^{(k-1)}(x) =\sum_{y\in\mathcal{Y}}\hat{P}_{n}^{(k-1)}(x,y)=\sum_{y\in \mathcal{Y}}\frac{\hat{P}_{Y,\epsilon}(y)}{\hat{P}_{n,Y}^{(k-2)}(y)}\hat{P}_{ n}^{(k-2)}(x,y)\] \[\geq\hat{p}_{*,\epsilon}\sum_{y\in\mathcal{Y}}\hat{P}_{n}^{(k-2) }(x,y)=\hat{p}_{*,\epsilon}\hat{P}_{n,X}^{(k-2)}(x)=\hat{p}_{*,\epsilon}\hat{ P}_{X,\epsilon}(x)\geq\hat{p}_{*,\epsilon}^{2}.\]

The result for \(k\) even can be proven similarly. We now prove the inequalities listed in the statement using on the lower bounds above.

**Proving the first inequality.** For any \(x\in\mathcal{X}\),

\[\left|\frac{\hat{P}_{X,\epsilon}(x)}{\hat{P}_{n,X}^{(k-1)}(x)}-1\right|=\max \left\{\frac{\hat{P}_{X,\epsilon}(x)}{\hat{P}_{n,X}^{(k-1)}(x)}-1,1-\frac{\hat {P}_{X,\epsilon}(x)}{\hat{P}_{n,X}^{(k-1)}(x)}\right\}\leq\left\{\begin{matrix} \max\{n-1,1\}&\text{if $k=1$}\\ \max\{1/\hat{p}_{*,\epsilon}^{2}-1,1\}&\text{if $k>1$}\end{matrix}\right.,\]

which is the desired result.

**Proving the second and third inequalities.** Consider an odd \(k\geq 1\). By the definition of total variation distance, it holds that

\[\max_{x\in\mathcal{X}}\left|\hat{P}_{X,\epsilon}(x)-\hat{P}_{n,X}^{(k-1)}(x) \right|\leq\operatorname{TV}(\hat{P}_{n,X}^{(k-1)},\hat{P}_{X,\epsilon}).\]

According to Pinsker's inequality, we have that \(\operatorname{TV}(\hat{P}_{n,X}^{(k-1)},\hat{P}_{X,\epsilon})\leq\sqrt{\frac {1}{2}\operatorname{KL}(\hat{P}_{n,X}^{(k-1)}\|\hat{P}_{X,\epsilon})}\), and so we have that

\[\max_{x\in\mathcal{X}}\left|\hat{P}_{X,\epsilon}(x)-\hat{P}_{n,X}^{(k-1)}(x) \right|\leq\sqrt{\frac{1}{2}\operatorname{KL}(\hat{P}_{n,X}^{(k-1)}\|\hat{P}_ {X,\epsilon})}\leq\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}^{(0)}\|\hat{P}_ {X,\epsilon})},\]

where the last inequality follows by the monotonicity of Sinkhorn iterations given in Prop. 13. Notice that the remaining term is \(\operatorname{KL}(P_{n,X}^{(0)}\|\hat{P}_{X,\epsilon})=\operatorname{KL}(P_{n,X}\|\hat{P}_{X,\epsilon})\), which may not decay to zero as \(n\to\infty\). Because \(\epsilon<1\), write

\[\operatorname{KL}(P_{n,X}\|\hat{P}_{X,\epsilon}) =\sum_{x\in\mathcal{X}}P_{n,X}(x)\log\frac{P_{n,X}(x)}{(1-\epsilon )P_{X}(x)+\epsilon\hat{P}_{X}(x)}\] \[\leq\sum_{x\in\mathcal{X}}P_{n,X}(x)\log\frac{P_{n,X}(x)}{(1- \epsilon)P_{X}(x)}\] \[=\operatorname{KL}(P_{n,X}\|P_{X})+\log\frac{1}{1-\epsilon}\] \[\implies\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|\hat{P}_{X, \epsilon})} \leq\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|P_{X})}+\sqrt{ \frac{1}{2}\log\frac{1}{1-\epsilon}}.\]

We can then apply the lower bounds

\[\max_{x\in\mathcal{X}}\left|\frac{\hat{P}_{X,\epsilon}(x)}{\hat{P}_{n,X}^{(k-1 )}(x)}-1\right|\leq\left\{\begin{matrix}n\left(\sqrt{\frac{1}{2} \operatorname{KL}(P_{n,X}\|P_{X})}+\sqrt{\frac{1}{2}\log\frac{1}{1-\epsilon}} \right)&\text{if $k=1$}\\ \frac{1}{\hat{p}_{*,\epsilon}^{2}}\left(\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|P_{X})}+\sqrt{\frac{1}{2}\log\frac{1}{1-\epsilon}}\right)&\text{if $k>1$}\end{matrix}\right..\]

Finally, combining the arguments above, we have that

\[\max_{x\in\mathcal{X}}\left|\hat{P}_{X,\epsilon}(x)-\hat{P}_{n,X}^ {(k-1)}(x)\right| \leq\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|P_{X})}+\sqrt{ \frac{1}{2}\log\frac{1}{1-\epsilon}}\] \[\leq\frac{\hat{p}_{*,\epsilon}}{4}+\frac{\hat{p}_{*,\epsilon}}{4}= \frac{\hat{p}_{*,\epsilon}}{2},\]where the last step invoked the assumption that

\[\operatorname{KL}(P_{n,X}\|P_{X})\leq\frac{\hat{p}_{\star,\epsilon}^{2}}{8}\quad \text{and}\quad\epsilon\leq 1-\exp\left(-\frac{\hat{p}_{\star,\epsilon}^{2}}{8} \right).\]

This means that

\[\min_{x\in\mathcal{X}}\hat{P}_{n,X}^{{}^{(k-1)}}(x)\geq \min_{x\in\mathcal{X}}\hat{P}_{X,\epsilon}(x)-\max_{x\in\mathcal{X}}\left|\hat{ P}_{n,X}^{{}^{(k-1)}}(x)-\hat{P}_{X,\epsilon}(x)\right|\geq\frac{\hat{p}_{\star, \epsilon}}{2}.\]

Hence,

\[\max_{x\in\mathcal{X}}\left|\frac{\hat{P}_{X,\epsilon}(x)}{\hat{P}_{n,X}^{{}^{ (k-1)}}(x)}-1\right|\leq\frac{\max_{x\in\mathcal{X}}\left|\hat{P}_{n,X}^{{}^{(k -1)}}(x)-\hat{P}_{X,\epsilon}(x)\right|}{\min_{x\in\mathcal{X}}\hat{P}_{n,X}^{ {}^{(k-1)}}(x)}\leq\frac{2}{\hat{p}_{\star,\epsilon}}\sqrt{\frac{1}{2} \operatorname{KL}(P_{n,X}\|\hat{P}_{X,\epsilon})}.\]

Now, for \(k\) even, set \(k=2t\) for \(t\geq 0\). We have that

\[\max_{y\in\mathcal{Y}}\left|\hat{P}_{n,Y}^{{}^{(2t-1)}}(y)- \hat{P}_{Y,\epsilon}(y)\right|\leq\operatorname{TV}(\hat{P}_{n,Y}^{{}^{(2t-1) }},\hat{P}_{Y,\epsilon})\leq\sqrt{\frac{1}{2}\operatorname{KL}(\hat{P}_{Y, \epsilon}\|\hat{P}_{n,Y}^{{}^{(2t-1)}})}.\]

Invoke Prop. 13 once again to achieve

\[\sqrt{\frac{1}{2}\operatorname{KL}(\hat{P}_{Y,\epsilon}\|\hat{P}_{n,Y}^{{}^{( 2t-1)}})}\leq\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|\hat{P}_{X,\epsilon}) }\leq\sqrt{\frac{1}{2}\operatorname{KL}(P_{n,X}\|P_{X})}+\sqrt{\frac{1}{2} \log\frac{1}{1-\epsilon}},\]

which completes the proof. 

Proceeding with similar steps, define the quantities

\[\hat{B}_{1}:=\hat{M}_{1}\quad\text{ and }\quad\hat{B}_{2}:=\max_{2\leq\ell\leq k} \hat{M}_{\ell}\quad\text{ for }\quad\hat{M}_{\ell}:=\begin{cases}\max_{x\in\mathcal{X}}\left| \frac{\hat{P}_{X,\epsilon}(x)}{\hat{P}_{n,X}^{{}^{(t-1)}}(x)}-1 \right|&\ell\text{ odd}\\ \max_{y\in\mathcal{Y}}\left|\frac{\hat{P}_{Y,\epsilon}(y)}{\hat{P}_{n,Y}^{{}^{ (\ell-1)}}(y)}-1\right|&\ell\text{ even}\end{cases}.\]

We must now establish an analog of Prop. 20.

**Proposition 29**.: _For any \(k\geq 1\), the following holds under the event \(\mathcal{S}\):_

\[\sqrt{n}\sum_{\ell=1}^{k}\left|\hat{V}_{n}^{{}^{(\ell-1)}}( \mathcal{C}_{\ell}\ldots\mathcal{C}_{k}h)\right| \leq\sum_{j=1}^{m}\left(\hat{B}_{1}\left|\mathbb{G}_{n}^{{}^{(0)} }(h_{1,k}\mathbf{1}_{j\ell})\right|+\hat{B}_{2}\sum_{\ell=2}^{k}\left| \mathbb{G}_{n}^{{}^{(0)}}(h_{\ell,k}\mathbf{1}_{j\ell})\right|\right)\] \[\quad+m\hat{B}_{2}\left\|h\right\|_{\infty}\sqrt{n}k(k-1)[\hat{B} _{1}+\hat{B}_{2}(k+1)/3].\]

Proof.: This proof largely follows the argument of Prop. 20, while accounting for the misspecified marginal error. Using again the notation \(h_{\ell,k}:=\mathcal{C}_{\ell}\ldots\mathcal{C}_{k}h\), it follows from Lem. 27 that, for odd \(\ell\),

\[\sqrt{n}\hat{V}_{n}^{{}^{(\ell-1)}}(h_{\ell,k})=\sum_{j=1}^{m} \left[\frac{\hat{P}_{X,\epsilon}}{\hat{P}_{n,X}^{{}^{(\ell-1)}}}(x_{j})-1 \right]\hat{\mathbb{G}}_{n}^{{}^{(\ell-1)}}(h_{\ell,k}\mathbf{1}_{j\ell}) \leq\hat{M}_{\ell}\sum_{j=1}^{m}\left|\hat{\mathbb{G}}_{n}^{{}^{(\ell-1)}}(h_{ \ell,k}\mathbf{1}_{j\ell})\right|.\]

The bound above holds for \(\ell\) even as well. Then, using (64) from Prop. 26 along with the triangle inequality, we have that for \(\ell\geq 2\),

\[\left|[\hat{P}_{n}^{{}^{(\ell-1)}}-P](h_{\ell,k}\mathbf{1}_{j\ell})\right| \leq\left|\hat{P}_{n}^{{}^{(\ell-2)}}-P](h_{\ell,k}\mathbf{1}_{j\ell})\right|+ \left|\hat{V}_{n}^{{}^{(\ell-2)}}(h_{\ell,k}\mathbf{1}_{j\ell})\right|\]

which implies that

\[\left|\hat{\mathbb{G}}_{n}^{{}^{(\ell-1)}}(h_{\ell,k}\mathbf{1}_{j \ell})\right|\] (71) \[\leq\left|\hat{\mathbb{G}}_{n}^{{}^{(\ell-2)}}(h_{\ell,k} \mathbf{1}_{j\ell})\right|+\sqrt{n}\left|\hat{V}_{n}^{{}^{(\ell-2)}}(h_{\ell,k} \mathbf{1}_{j\ell})\right|\] \[\leq\left|\hat{\mathbb{G}}_{n}^{{}^{(0)}}(h_{\ell,k} \mathbf{1}_{j\ell})\right|+\hat{M}_{1}\sqrt{n}\hat{P}_{n}^{{}^{(0)}}(\left|h_{ \ell,k}\left|\mathbf{1}_{j\ell}\right)+\ldots+\hat{M}_{\ell}\sqrt{n}\hat{P}_{n} ^{{}^{(\ell-2)}}(\left|h_{\ell,k}\right|\mathbf{1}_{j\ell})\] \[\leq\left|\hat{\mathbb{G}}_{n}^{{}^{(0)}}(h_{\ell,k} \mathbf{1}_{j\ell})\right|+2\left\|h\right\|_{\infty}\sqrt{n}\left[\hat{B}_{1}+ \hat{B}_{2}(\ell-1)\right](k-\ell+1),\] (72)

[MISSING_PAGE_FAIL:40]

a higher-order term compared to other components of the bound.

Next, we must control the left-hand side of (73). We perform the decomposition based on (68):

\[\mathbb{E}_{P}\left[\left(\hat{P}_{n}^{(k)}(h)-P(h)\right)^{2} \mathds{1}_{S}\right]\] \[\leq\mathbb{E}_{P}\left[T_{1}^{2}\mathds{1}_{S}\right]+2\mathbb{E }_{P}\left[\left|\left[T_{1}\hat{T}_{2}\right|\mathds{1}_{S}\right]+\mathbb{E} _{P}\left[\hat{T}_{2}^{2}\mathds{1}_{S}\right]\right.\] (76) \[\quad+O(k\sqrt{\epsilon})\cdot\mathbb{E}_{P}\left[\left(\left[T_{ 1}\right]+\left|\hat{T}_{2}\right|\right)\mathds{1}_{S}\right]+O(k^{2}\epsilon)\] (77)

for

\[T_{1}:=[P_{n}-P](\mathcal{C}_{1}\ldots\mathcal{C}_{k}h)\text{ and }\hat{T}_{2}:= \sum_{\ell=1}^{k}\left|\hat{V}_{n}^{(\ell-1)}(\mathcal{C}_{\ell}\ldots\mathcal{ C}_{k}h)\right|.\] (78)

Recall the events \(\mathcal{E}_{1}^{\delta}\) and \(\mathcal{E}_{2}^{\delta}\) and \(\mathcal{E}_{3}^{\delta}\) from Appx. 4. To perform this computation efficiently, we will split the bounds on each term into two components. In particular, we will show that

* Under the event \(\mathcal{S}\cap\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}:\left| \hat{T}_{2}\right|\leq\mathcal{T}_{2}+E_{2}\),
* Under the event \(\mathcal{S}\backslash(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}): \left|\hat{T}_{2}\right|\leq\mathcal{T}_{2}^{c}+E_{2}^{c}\),
* Under the event \(\mathcal{S}\cap\mathcal{E}_{3}^{\delta}:\left|T_{1}\right|\leq\mathcal{T}_{1}\),
* Under the event \(\mathcal{S}\backslash\mathcal{E}_{3}^{\delta}:\left|T_{1}\right|\leq \mathcal{T}_{1}^{c}\),

where any term denoted with "\(E\)" will represent all error terms that include \(\epsilon\) and will be written in big-\(O\) notation. There are no errors for the bounds on \(T_{1}\), as this term does not depend on the misspecified marginals. The idea is that for the "\(\mathcal{T}_{2}\)" terms we may reuse the bounds derived in Appx. 4 by simply replacing \(p_{\star}\) with \(\hat{p}_{\star,\epsilon}\). This is due to the fact that the dependence of the analogous terms from Appx. 4 depend on \(p_{\star}\) only through Prop. 14; similarly, the corresponding terms in this section depend on \(\hat{p}_{\star,\epsilon}\) through Prop. 28. We return to the terms in (76) and (77).

Decomposing on \(\mathcal{E}_{3}^{\delta}\) will result in a bound of the form

\[O(k\sqrt{\epsilon})\cdot\mathbb{E}_{P}\left[\left|T_{1}\right| \mathds{1}_{S}\right]\leq O(k\sqrt{\epsilon})\cdot\left(\delta\mathcal{T}_{1} ^{c}+\mathcal{T}_{1}\right).\]

Decomposing on \(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}\) will result in a bounds of the form

\[\mathbb{E}_{P}\left[\hat{T}_{2}^{2}\mathds{1}_{S}\right] \leq 2\delta(\mathcal{T}_{2}^{c})^{2}+\mathcal{T}_{2}^{2}+\tilde{O} \left(\delta\left((E_{2}^{c})^{2}+E_{2}^{c}\mathcal{T}_{2}^{c}\right)+\left( E_{2}^{2}+E_{2}\mathcal{T}_{2}\right)\right)\] \[O(k\sqrt{\epsilon})\cdot\mathbb{E}_{P}\left[\left|\hat{T}_{2} \mathds{1}_{S}\right] \leq O(k\sqrt{\epsilon})\cdot\left(\delta\left(\mathcal{T}_{2}^{c}+E_{2} ^{c}\right)+\mathcal{T}_{2}+E_{2}\right).\]

Finally, decomposing on \(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}\cap\mathcal{E}_{3}^{\delta}\) will result in a bound of the form

\[\mathbb{E}_{P}\left[\left|T_{1}\hat{T}_{2}\right|\mathds{1}_{S} \right] \leq 3\delta\mathcal{T}_{1}^{c}\mathcal{T}_{2}^{c}+\mathcal{T}_{1} \mathcal{T}_{2}+\tilde{O}\left(\delta\mathcal{T}_{1}^{c}E_{2}^{c}+\mathcal{T} _{1}E_{2}\right).\]

The leading terms \(2\delta(\mathcal{T}_{2}^{c})^{2}+\mathcal{T}_{2}^{2}\) and \(3\delta\mathcal{T}_{1}^{c}\mathcal{T}_{2}^{c}+\mathcal{T}_{1}\mathcal{T}_{2}\) from both bounds should have the exact same form as the terms in Lem. 22 and Lem. 23, with \(p_{\star}\) replaced by \(\hat{p}_{\star,\epsilon}\), thus retaining the same dependence on \((n,k)\). By setting \(\delta=\hat{p}_{\star,\epsilon}^{\star}/n^{4}\), we will achieve a similar result to Thm. 24, i.e., that

\[\mathbb{E}_{P}\left[\left(\hat{P}_{n}^{(k)}(h)-P(h)\right)^{2} \mathds{1}_{S}\right]\] \[\leq\frac{\sigma_{k}^{2}}{n}+\tilde{O}\left(\frac{k^{6}}{n^{3/2}}\right)\] \[+\tilde{O}\Big{(}(\hat{p}_{\star,\epsilon}/n)^{4}\left(E_{2}^{c}( E_{2}^{c}+\mathcal{T}_{2}^{c})\right)+E_{2}\left(E_{2}+\mathcal{T}_{2}\right)+( \hat{p}_{\star,\epsilon}/n)^{4}\mathcal{T}_{1}^{c}E_{2}^{c}+\mathcal{T}_{1}E_{2 }\Big{)}.\] (79) \[+\tilde{O}\left(k\sqrt{\epsilon}\left((\hat{p}_{\star,\epsilon}/n)^ {4}\mathcal{T}_{1}^{c}+\mathcal{T}_{1}+(\hat{p}_{\star,\epsilon}/n)^{4} \left(\mathcal{T}_{2}^{c}+E_{2}^{c}\right)+\mathcal{T}_{2}+E_{2}\right)+k^{2} \epsilon\right).\] (80)

It remains to quantify the \(\tilde{O}\) terms by computing the order of the 6 constants \((\mathcal{T}_{2},E_{2},\mathcal{T}_{2}^{c},E_{2}^{c},\mathcal{T}_{1},\mathcal{T}_{ 1}^{c})\). We follow similar steps to Lem. 22 and Lem. 23 to achieve this.

**Lemma 30**.: _For \(\delta=(\hat{p}_{\star,\epsilon}/n)^{4}\), assume that \(n\geq 8[\log_{2}(2/\delta)+m\log(n+1)]/\hat{p}_{\star,\epsilon}^{2}\) and \(\epsilon\leq 1-\exp\left(-\frac{\hat{p}_{\star,\epsilon}^{2}}{8}\right)\). Then, it holds that_

\[\mathcal{T}_{2}^{c} =\tilde{O}\left(\frac{k^{2}}{n\hat{p}_{\star,\epsilon}^{2}}\left( n+\frac{k}{\hat{p}_{\star,\epsilon}^{2}}\right)\right),\quad E_{2}^{c}=0\] \[\mathcal{T}_{2} =\tilde{O}\left(\frac{k^{3}}{n\hat{p}_{\star,\epsilon}^{2}} \right),\quad E_{2}=\tilde{O}\left(\frac{k^{3}}{\hat{p}_{\star,\epsilon}^{2}} \left(\sqrt{\frac{1}{n}\log\frac{1}{1-\epsilon}}+\log\frac{1}{1-\epsilon} \right)\right).\]

Proof.: The following computations are done under the event \(\mathcal{S}\). First, apply Prop. 29 to write

\[\sqrt{n}\left|\hat{T}_{2}\right| \leq\sum_{j=1}^{m}\left(\hat{B}_{1}\left|\mathbb{G}_{n}^{{}_{(0 )}}(h_{1,k}\:\mathbf{1}_{j\ell})\right|+\hat{B}_{2}\sum_{\ell=2}^{k}\left| \mathbb{G}_{n}^{{}_{(0)}}(h_{\ell,k}\:\mathbf{1}_{j\ell})\right|\right)\] \[\quad+m\hat{B}_{2}\left\|h\right\|_{\infty}k(k-1)[\hat{B}_{1}+ \hat{B}_{2}(k+1)/3].\] (81)

We decompose on the event \(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}\).

Bound \(|T_{2}|\) under the event \(\mathcal{S}\backslash(\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta})\).In this case, we apply (69) from Prop. 28 to get \(\hat{B}_{1}\leq n\) and \(\hat{B}_{2}\leq 1/\hat{p}_{\star,\epsilon}^{2}\), along with the universal bounds from Lem. 18:

\[\frac{1}{\sqrt{n}}\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{1,k}\: \mathbf{1}_{j\ell})\right| \leq 2\left\|h_{1,k}\right\|_{\infty}\leq 4k\left\|h\right\|_{\infty}\] \[\frac{1}{\sqrt{n}}\sum_{\ell=2}^{k}\left|\mathbb{G}_{n}^{{}_{(0)} }(h_{\ell,k}\:\mathbf{1}_{j\ell})\right| \leq 2\sum_{\ell=2}^{k}\left\|h_{\ell,k}\right\|_{\infty}\leq\sum_{ \ell=2}^{k}4(k-\ell+1)\left\|h\right\|_{\infty}=2k(k-1)\left\|h\right\|_{\infty}\]

so that by plugging into (81),

\[\left|\hat{T}_{2}\right| \leq\underbrace{\left\|h\right\|_{\infty}mk\left[4n+\frac{k-1}{ \hat{p}_{\star,\epsilon}^{2}}\left(n+2+\frac{k+1}{3\hat{p}_{\star,\epsilon}^{ 2}}\right)\right]}_{\mathcal{T}_{2}^{\zeta}}+\underbrace{0}_{E_{2}^{\zeta}}.\]

Bound \(|T_{2}|\) under the event \(\mathcal{S}\cap\mathcal{E}_{1}^{\delta}\cap\mathcal{E}_{2}^{\delta}\).In this case, we may use that \(n\geq 8/\hat{p}_{\star,\epsilon}^{2}\) (because \([\log_{2}(2/\delta)+m\log(n+1)]\geq 1\) for \(\delta\in(0,1)\)) and apply (70) from Prop. 28 to get

\[\max\left\{\hat{B}_{1},\hat{B}_{2}\right\} \leq O\left(\frac{1}{\hat{p}_{\star,\epsilon}}\sqrt{\log\frac{1}{1- \epsilon}}\right)+\frac{2}{\hat{p}_{\star,\epsilon}}\sqrt{\frac{2\log_{2}(2/ \delta)+2m\log(n+1)}{2n}}\]

The bounds based on \(\mathcal{E}_{2}^{\delta}\) give

\[\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{1,k}\:\mathbf{1}_{j\ell})\right| \leq\sqrt{2\log\frac{2mk}{\delta}2k\left\|h\right\|_{\infty}}\] \[\sum_{\ell=2}^{k}\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{\ell,k}\: \mathbf{1}_{j\ell})\right| \leq\sum_{\ell=2}^{k}\sqrt{2\log\frac{2mk}{\delta}}2(k-\ell+1) \left\|h\right\|_{\infty}\leq\sqrt{2\log\frac{2mk}{\delta}}k(k-1)\left\|h \right\|_{\infty}.\]

By plugging into (81), we can reuse the steps in the bound from (55) (for all terms without \(\epsilon\)) to write

\[\left|\hat{T}_{2}\right| \leq\frac{4mk\left\|h\right\|_{\infty}\left[\log_{2}(2/\delta)+2m \log(n+1)\right]^{1-1\left\{k=1\right\}/2}}{n\hat{p}_{\star,\epsilon}^{2}}\times\] \[\quad\left[\hat{p}_{\star,\epsilon}\sqrt{2\log\left(2mk/\delta \right)}(k+1)+(k-1)(k+4)\right]+E_{2},\]

so that

\[\mathcal{T}_{2} =\frac{4mk\left\|h\right\|_{\infty}\left[\log_{2}(2/\delta)+2m \log(n+1)\right]^{1-1\left\{k=1\right\}/2}}{n\hat{p}_{\star,\epsilon}^{2}}\] \[\quad\times\left[\hat{p}_{\star,\epsilon}\sqrt{2\log\left(2mk/ \delta\right)}(k+1)+(k-1)(k+4)\right].\]We compute \(E_{2}\) by using that

\[\max\left\{\hat{B}_{1},\hat{B}_{2}\right\} \leq O\left(\frac{1}{\hat{p}_{\star,\epsilon}}\sqrt{\log\frac{1}{1- \epsilon}}\right)+\tilde{O}\left(\frac{1}{\hat{p}_{\star,\epsilon}\sqrt{n}}\right)\] \[\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{1,k}\:\mathbf{1}_{j\ell})\right| \leq\tilde{O}\left(k\right)\] \[\sum_{\ell=2}^{k}\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{\ell,k}\: \mathbf{1}_{j\ell})\right| \leq\tilde{O}\left(k^{2}\right),\]

which gives

\[E_{2}=\tilde{O}\left(\frac{k^{3}}{\hat{p}_{\star,\epsilon}^{2}} \left(\sqrt{\frac{1}{n}\log\frac{1}{1-\epsilon}}+\log\frac{1}{1-\epsilon} \right)\right).\]

We now make the corresponding argument for the term \(T_{1}\).

**Lemma 31**.: _For \(\delta=(\hat{p}_{\star,\epsilon}/n)^{4}\), it holds that_

\[\mathcal{T}_{1}^{c}=\tilde{O}(k),\quad\mathcal{T}_{1}=\tilde{O} \left(\frac{k}{\sqrt{n}}\right).\]

Proof.: The following computations are done under the event \(\mathcal{S}\).

Bound \(\left|T_{1}\right|\) under the event \(\mathcal{S}\backslash\mathcal{E}_{3}^{\delta}\).Here we simply apply a universal bound on the empirical process term:

\[\frac{1}{\sqrt{n}}\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{1,k})\right| \leq 2\left\|h_{1,k}\right\|_{\infty}\leq 4k\left\|h\right\|_{\infty},\]

so that \(\mathcal{T}_{1}^{c}=4k\left\|h\right\|_{\infty}\)

Bound \(\left|T_{1}\right|\) under the event \(\mathcal{S}\cap\mathcal{E}_{3}^{\delta}\).Now, we may use the definition of the event \(\mathcal{E}_{3}^{\delta}\) to achieve

\[\frac{1}{\sqrt{n}}\left|\mathbb{G}_{n}^{{}_{(0)}}(h_{1,k})\right| \leq\sqrt{\frac{2\log(2/\delta)}{n}}2k\left\|h\right\|_{\infty}=\mathcal{T}_{ 1}.\]

Knowing that \(E_{2}^{c}=0\), we simplify (79) and (79) to read

\[\tilde{O}\Big{(}E_{2}\left(E_{2}+\mathcal{T}_{2}+\mathcal{T}_{1} \right)\Big{)}\] \[\tilde{O}\left(k\sqrt{\epsilon}\left((\hat{p}_{\star,\epsilon}/n )^{4}\mathcal{T}_{1}^{c}+\mathcal{T}_{1}+(\hat{p}_{\star,\epsilon}/n)^{4} \mathcal{T}_{2}^{c}+\mathcal{T}_{2}+E_{2}\right)+k^{2}\epsilon\right).\]

We now combine the bounds from the previous two lemmas to compute (79) and (80) to state the main result.

**Theorem 32**.: _Let Asm. 1 be true with error \(\epsilon\in[0,1)\). For a sequence of rebalanced distributions \((\hat{P}_{n}^{{}_{(k)}})_{k\geq 1}\), there exists an absolute constant \(C>0\) such that when \(n\geq C[\log_{2}(2n/\hat{p}_{\star,\epsilon})+m\log{(n+1)}]/\min\left\{p_{ \star},\hat{p}_{\star,\epsilon}\right\}^{2}\), we have that_

\[\mathbb{E}_{P}\left[\left(\hat{P}_{n}^{{}_{(k)}}(h)-P(h)\right)^{ 2}\mathds{1}_{\mathcal{S}}\right]+\mathbb{E}_{P}\left[\left(P_{n}(h)-P(h) \right)^{2}\mathds{1}_{\mathcal{S}^{c}}\right]\leq\frac{\sigma_{k}^{2}}{n}+ \tilde{O}\left(\frac{k^{6}}{n^{3/2}}\right)\] \[+\tilde{O}\left(\frac{k^{4}}{\hat{p}_{\star,\epsilon}^{2}}\left( \sqrt{\frac{1}{n}\log\frac{1}{1-\epsilon}}+\log\frac{1}{1-\epsilon}\right) \left[\frac{k^{2}}{\hat{p}_{\star,\epsilon}^{2}}\left(\sqrt{\frac{1}{n}\log \frac{1}{1-\epsilon}}+\log\frac{1}{1-\epsilon}+\frac{1}{n}\right)+\frac{1}{ \sqrt{n}}\right]\right)\] \[+\tilde{O}\left(k^{2}\left[\sqrt{\epsilon}\left(\frac{\hat{p}_{ \star,\epsilon}^{4}}{n^{4}}+\frac{1}{\sqrt{n}}+\frac{\hat{p}_{\star,\epsilon}^{2 }k}{n^{4}}\left(n+\frac{k^{2}}{\hat{p}_{\star,\epsilon}^{2}}\right)+\frac{k^ {2}}{\hat{p}_{\star,\epsilon}^{2}}\left[\frac{1}{n}+\sqrt{\frac{1}{n}\log \frac{1}{1-\epsilon}}+\log\frac{1}{1-\epsilon}\right]\right)+\epsilon\right] \right)\!.\]Experimental Details

We provide the full experimental details of the experimental results from Sec. 4. We report additional evaluations on downstream tasks with linear probing and zero-shot retrieval. Finally, we give illustrations of the sensitivity to misspecified marginals, and of the convergence to the given marginals.

### Datasets

#### Pre-Training Data.

The pre-training data was taken from the public ImageNet-Captions dataset (Fang et al., 2013). We subset the dataset by selecting the 250 classes that were most frequent in the dataset, resulting in 174,594 images and associated Flickr captions. The exact images used and their associated captions are given in the code supplement.

#### Evaluation Data.

We perform zero-shot classification (as described in Sec. 4), zero-shot retrieval, and linear probing with various image classification and image-caption datasets. We used the default class captions (for classification) and default linear probing parameters from the CLIP Benchmark repo. The datasets (test splits) used were:

* **CIFAR-10:** 10,000 colored natural images labeled with one of 10 classes.
* **CIFAR-100:** 10,000 colored natural images labeled with one of 100 classes.
* **STL-10:** 80,000 colored natural images labeled with one of 10 classes.
* **MS-COCO:** 41,000 colored natural images with associated captions.
* **Flickr8k:** 8,000 colored natural images with associated captions.
* **Rendered SST2:** 1,821 images of typed natural language with sentiment label (2 classes).
* **VOC2007:** 4,952 colored natural images labeled with one of 20 classes.
* **FGVC Aircraft:** 34,000 colored natural images labeled with one of 102 classes.

Evaluation scripts using the various embedding models (described below) are provided.

### Model Specification and Hyperparameters

#### Architecture and Implementation.

The models considered CLIP models (Radford et al., 2021), and are specified by pairs of encoders \((f_{\theta},g_{\theta})\), representing images and text, respectively. The encoders decompose into \(f_{\theta}=f_{\theta}^{\text{head}}\circ f_{\theta}^{\text{base}}\) (similarly for \(g_{\theta}\)) where \(f_{\theta}^{\text{base}}\) denotes a base image encoder and \(f_{\theta}^{\text{head}}\) denotes a trainable head model. The head models are feed-forward networks with two hidden layers, 256 hidden units, and 128-dimensional output representations. Their input dimensions may be 512 or 768, depending on whether a CLIP model or BERT/GPT-2 model is used as the base. For the image base/foundation models, we use the open-source OpenCLIP implementation of the ViT-B/32 model with the laion2b_s34b_b79k model tag. For the text encoder, we use the encoder of the variant of the ViT-B/32 with tag datacomp_xl_s13b_b90k. For the other text encoders the Huggingface implementations of GPT-2 and BERT were used.

#### Optimizer.

For optimization, models were trained with stochastic gradient descent (SGD) with the learning rate tuned along the grid \(\left\{1^{-3},3^{-3},1^{-2},3^{-2},1^{-1}\right\}\) and a fixed weight decay parameter of 0.01. Momentum-variants such as Adam (Kingma and Ba, 2015) were not used to isolate the effect of varying losses as described in Sec. 4.

### Compute Environment

Experiments were run on a CPU/GPU workstation with 12 virtual cores, 126G of memory, and four NVIDIA TITAN Xp GPUs with 12G memory each. The code was written in Python 3 and we use PyTorch for automatic differentiation. The OpenCLIP and CLIP Benchmark repositories were used for zero-shot evaluation.

### CLIP and Multi-CLIP

We considered in the contrastive learning example from Sec. 2 - see (6) in particular - a variant of the CLIP objective in which either zero, or one, or more than one balancing iterations are performed (see (6)), via optimizing

\[L_{n}^{(k)}=-\frac{1}{2}\sum_{i=1}^{n}\left[\log Q_{n}^{(k)}(X_{i},Y_{i})+\log R _{n}^{(k)}(X_{i},Y_{i})\right].\] (82)

This contrasts the single-iteration variant \(L_{n}^{(1)}\) which in fact reduces to the original CLIP loss. Because these iterations are applied in the objective, backpropagation occurs _through_ each iteration.

In Fig. 3, we plot the zero-shot classification performance (in terms of average per-class recall) of the variants trained on \(L_{n}^{(0)}\) (the normalized initial measure, _No Balancing_), \(L_{n}^{(1)}\) (the original CLIP loss, _CLIP balancing_), and \(L_{n}^{(2)}\) (the two-iteration CLIP loss, _Multi-CLIP balancing_). We also vary the quality of the text encoder \(f_{\theta_{T}}\), observing an overall accuracy trend of GPT-2 \(\prec\) BERT \(\prec\) CLIP across variants, which is to be expected given the base representation quality of each model. Interestingly, there is an improvement stemming from performing multiple balancing iterations across choices of the text embedding, the batch size \(m\), and the evaluation dataset.

### Metadata Curation

We considered in the metadata curation example from Sec. 2 how to use balancing to adjust the entire pre-training set, in the spirit of Xu et al. (2024). The target marginal \(P_{Y}\) is selected by choosing a threshold for which frequent keywords have their probability mass truncated, and the probability measure is normalized to sum to one. In Fig. 4, we show the observed marginal \(P_{n,Y}\) and the target marginal \(P_{Y}\) sorted in increasing order (left). The original marginal on \(\mathcal{Y}\) has approximately \(5\) orders of magnitude of difference between the most and least probable keyword. After balancing, the target marginal has less than \(2\) orders of difference. To see how this affects downstream performance, we plot the zero-shot classification accuracy over training iterations in Fig. 4 (right) when using the original dataset (orange) and using the metadata-balanced dataset (blue). We observe moderate improvement especially in the small batch regime (\(m=512\)) when curating the dataset.

### Additional Experiments

In this section, we provide 1) a synthetic data example that helps elucidate the role of the spectral decomposition introduced in Sec. 3, and 2) additional evaluations on downstream tasks such as zero-shot retrieval and linear probing. For the latter, we maintain the experimental settings as used in the zero-shot classification example from Sec. 4 (Fig. 3). That is, we train variants of CLIP models (see Sec. 2) on the ImageNet-Captions dataset (Fang et al., 2013). As before, we use a fixed image/text encoder as a base vector representation and compose it with a trainable feed-forward neural network, i.e., \(f_{\theta}=f_{\theta}^{\text{head}}\circ f^{\text{base}}\), for \(\theta=\theta_{I}\) (images) or \(\theta=\theta_{T}\) (text). For the base text embeddings, we maintain three levels of model quality: GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2019), and CLIP-based encodings.

**Baseline Comparisons.** We present a synthetic data example to understand the role of the singular values \(s_{2},\dots,s_{m}\) and compare our approach to simple baselines that make use of \((P_{X},P_{Y})\). We also consider misspecification of these target marginals, in that they are chosen by the user but are not the marginal distributions of the data-generating distribution \(P\). First, while one can verify by hand that (14) is a distribution for which \(s_{2}=s\), we construct a more general example for \(m\geq 2\). We leave the construction to the end of this example. For controllable misspecification, we define \(\epsilon\in[0,0.5]\) to be the _misspecification_ level, so that the corrupted target marginals are set to be

\[\tilde{P}_{X}:=(1-\epsilon)P_{X}+\epsilon\hat{P}_{X}\text{ and }\tilde{P}_{Y}:=(1- \epsilon)P_{Y}+\epsilon\hat{P}_{Y},\] (83)

where \(\hat{P}_{X}\) and \(\hat{P}_{Y}\) are drawn independently and randomly from the \(\text{Dirichlet}(\mathbf{1}_{m})\) distribution (i.e. uniformly over the probability simplex on \(m\) atoms). Finally, other than the empirical measure \(P_{n}\), we define one additional baseline; the _importance weighted independently (IPWI)_ estimator is defined as

\[P_{n}^{\text{IPWI}}(x,y)=\frac{\tilde{P}_{X}(x)}{P_{n,X}(x)}\frac{\tilde{P}_{ Y}(y)}{P_{n,Y}(y)}P_{n}(x,y).\] (84)This estimator simply reweighs all cells of the empirical measure by the likelihood ratio from each observed marginal to the target marginal. Note that the result may not even be a probability measure, as it may not sum to one. Observe the comparative performance in (see Fig. 5). We notice in particular that the naive \(P_{n}^{\mathrm{IPWI}}\) is outperformed by empirical measure uniformly over \(s\), as by applying both reweighings simultaneously, the estimator does not satisfy either marginal constraint. On the other hand, under the maximum amount of target marginal corruption (\(\epsilon=0.5\)), the balancing-based estimator suffers an approximately half-order of magnitude in MSE. When \(s\approx 1\), the MSE of the balancing estimator decreases significantly. We hypothesize that this is because the data sources \(X\) and \(Y\) are nearly a function of one another, and if this function is estimable to high precision by a small amount of data, then a single marginal can identify the entire joint distribution via pushforward calculations. That being said, it is important to note that the quantities \(u_{j}\) and \(v_{j}\) in (15) also depend on \(s\), so it is difficult to control the singular values without controlling the respective bases.

As for the construction of the probability mass function and test function, let \(\mathbf{I}_{m}\) and \(\mathbf{1}_{m\times m}\) denote the identity matrix and matrix of ones in \(\mathbb{R}^{m\times m}\). For any \(s\in(0,1)\) and \(m\geq 1\), consider the probability mass matrix \(P\) given by

\[P=\frac{1}{m}\left[\frac{1}{m}\,\mathbf{1}_{m\times m}+s\left(\mathbf{I}_{m}- \frac{1}{m}\,\mathbf{1}_{m\times m}\right)\right].\]

The eigenvalues of the first matrix in the squared brackets are \((1,0,\ldots,0)\), as it is a rank \(1\) matrix for which \(\mathbf{1}_{m}\) is an eigenvector. The second matrix in the square brackets is the centering matrix (the projection matrix that subtracts the mean of a vector's components from the entire vector). Multiplied by \(s\), it has eigenvalues \((0,s,\ldots,s)\) where \(0\) is associated to the eigenvector \(\mathbf{1}_{m}\). Thus, the matrix in its entirety has eigenvalues \((1/m,s/m,\ldots,s/m)\), where the scaling factor ensures that \(P\) sums to one. The relation (13) holds for this choice of \(P\) and uniform marginals, with \(s_{2}=\ldots=s_{m}=s\). Thus, by tuning \(s\), we may control the level of dependence between \(X\) and \(Y\). Finally, because \(\mathcal{X}\) and \(\mathcal{Y}\) are finite, we can also specify the test function \(h\) via an \(m\times m\) table indexed by \(i\) (meaning \(x_{i}\)) and \(j\) (meaning \(y_{j}\)). We let \(h(x_{i},y_{j})=|Z_{ij}|\) where the \(Z_{ij}\) are independently drawn from a standard normal distribution. The resulting mean squared error is estimated with 200 seeds at \(n=300\).

Zero-Shot Retrieval.In this evaluation, we assess the ability of the learned representations to match queries from one modality to their counterparts in another modality. We are given a test sets \(\mathcal{X}_{\text{test}}=\left\{x_{1},\ldots,x_{M}\right\}\) of images and \(\mathcal{Y}_{\text{test}}=\left\{y_{1},\ldots,y_{N}\right\}\) of texts in natural language. We are also given a matrix of annotations \(A\in\left\{0,1\right\}^{M\times N}\) where \(A_{ij}=1\) if and only if \(y_{j}\) is a "relevant" caption

Figure 5: **Baseline Comparisons across Dependence and Misspecification Levels. Each line refers to a combination of an estimation method (the empirical probability measure \(P_{n}\), the estimator \(P_{n}^{\mathrm{IPWI}}\) from (84), or the balancing estimator \(P_{n}^{(k)}\) for \(k=8\)) and a noise level on the provided marginals (see (83)). The \(y\)-axis shows the mean squared error of estimating a linear functional. The \(x\)-axis represents the dependence level \(s=s_{2}\) (i.e. the leading singular value other than \(s_{1}=1\)).**

Figure 6: **Zero-Shot Retrieval Performance across Embeddings and Objectives.** The three vertical panels describe different choices of the text encoder \(f_{\theta_{T}}\) which increases in quality from left to right; that is, pre-trained GPT-2, BERT, and CLIP embeddings, respectively. Rows indicate various datasets, either MS-COCO or Flickr8k. evaluated under recall at \(K=5\) for image and text retrieval, respectively. The \(y\)-axis of each plot indicates the metric (see (85)) for either image or text retrieval, whereas the \(x\)-axis indicates training iterations at batch size \(512\).

for image \(x_{i}\) (and vice versa). Given a particular query \(y\in\mathcal{Y}_{\text{test}}\), we define the top-\(K\) neighborhood of \(y\) as

\[\mathcal{N}_{K}(y;\theta)=\operatorname*{arg\,max}_{S\subseteq[M]:|S|=K}\sum_{ i\in S}\langle f_{\theta_{I}}(x_{i}),f_{\theta_{T}}(y)\rangle,\]

i.e. the images in the test set that have the closest embeddings under the given model. Then, we may define the _average recall at \(K\) for image retrieval_ metric as

\[\operatorname{AverageRecall}_{K}(\theta):=\frac{1}{N}\sum_{j=1}^{N}\frac{ \sum_{i\in\mathcal{N}_{K}(y_{j};\theta)}A_{ij}}{\sum_{i^{\prime}\in[M]}A_{i^{ \prime}j}}.\] (85)

In words, the metric evaluates the retrieval system's ability to detect relevant items in the dataset, in this case by comparing the closeness of the image-text representations. We can analogously define the _average recall at \(K\) for text retrieval_ metric by swapping the role of \(x\) and \(y\) above. The results for both retrieval metrics on the MS-COCO [Lin et al., 2015] and Flickr8k [Hodosh et al., 2013] benchmarks are given in Fig. 6.

Linear Probe.Here, we evaluate the quality of the model's encoders by fine-tuning a single linear layer on top of the learned representations for a classification task. In the case of linear probing via image classification, we use only the image encoder \(f_{\theta_{I}}\). We are given a training set \(\{(x_{1},c_{1}),\dots,(x_{N},c_{N})\}\) of image-label pairs, where each \(c_{i}\in\{1,\dots,C\}\). We fix the model

Figure 7: **Linear Probe Performance across Embeddings and Objectives. The three vertical panels describe different choices of the text encoder \(f_{\theta_{T}}\) which increases in quality from left to right; that is, pre-trained GPT-2, BERT, and CLIP embeddings, respectively. Rows indicate various evaluation datasets from Rendered SST2, VOC2007, and FGVC Aircraft. The \(y\)-axis of each plot indicates the average per-class recall, whereas the \(x\)-axis indicates training iterations at batch size \(512\).**

parameter \(\theta_{I}\) and solve the regularized multinomial cross entropy (MCE) objective

\[\min_{W\in\mathbb{R}^{C\times r}}\left[L_{\text{MCE}}(W):=-\frac{1}{N}\sum_{i=1}^ {N}[\text{LogSoftmax}(Wf_{\theta_{I}}(x_{i}))]_{c_{i}}+\frac{\lambda}{2}\left\|W \right\|_{F}^{2}\right],\]

where \(\lambda>0\) is a regularization parameter, \(\left\|\cdot\right\|_{F}\) denotes the Frobenius norm on \(\mathbb{R}^{C\times r}\) and \(\text{LogSoftmax}:\mathbb{R}^{C}\rightarrow\mathbb{R}^{C}\) is given by \(\text{LogSoftmax}(z)=z-\log\sum_{j=1}^{C}\exp(z_{j})\). This results in a classifier

\[g(x):=\operatorname*{arg\,max}_{j\in[C]}[Wf_{\theta_{I}}(x)]_{j},\]

which can then be evaluated using standard accuracy metrics on a held-out test set. The image classification results for the Rendered SST2 (Radford et al., 2021), VOC2007 (Everingham et al., 2007), and FGVC Aircraft (Maji et al., 2013) benchmarks are given in Fig. 7.

Empirical Marginals in CLIP Balancing.To further clarify how the iterative balancing procedure is baked into the CLIP losses, recall from (82) that the objectives decompose into two terms, which depend on \(Q_{n}^{{}^{(k)}}\) and \(R_{n}^{{}^{(k)}}\) which differ only based on whether balancing to fit \(P_{Y}\) or to fit \(P_{X}\) is applied first, respectively. Thus, for any model parameterized by \(\theta\) and any number of iterations \(k\), there are four marginal distributions of interest: \(Q_{\theta,X}^{{}^{(k)}}\), \(Q_{\theta,Y}^{{}^{(k)}}\), \(R_{\theta,X}^{{}^{(k)}}\), and \(R_{\theta,Y}^{{}^{(k)}}\). Based on the order of iterations, we have that \(Q_{\theta,Y}^{{}^{(1)}}=R_{\theta,Y}^{{}^{(2)}}=P_{Y}\), and \(R_{\theta,X}^{{}^{(1)}}=Q_{\theta,X}^{{}^{(2)}}=P_{X}\). This is illustrated in Fig. 8. We see that after only a few iterations, both marginal distributions converge to the uniform distribution.

Figure 8: **Empirical Marginals of CLIP Contrast Matrix.** Depiction of the probability measures \(Q_{n}^{{}^{(k)}}\) and \(R_{n}^{{}^{(k)}}\) as described in (82) from Sec. 2. The orange bars correspond to the observed marginal after fitting to the target uniform distribution on the given iteration. **Left:**\(Q_{n}^{{}^{(0)}}\) and \(R_{n}^{{}^{(0)}}\), where neither marginal is set to uniform. **Center:**\(Q_{n}^{{}^{(1)}}\) and \(R_{n}^{{}^{(1)}}\), which corresponds to the original CLIP loss. **Right:**\(Q_{n}^{{}^{(2)}}\) and \(R_{n}^{{}^{(2)}}\), which correspond to two iterations of the balancing procedure within the loss. The blue bars are slightly non-uniform.

## Appendix F NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Theoretical claims, the focus of this paper, are supported with proofs. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We clarify that the setting studied has some dissimilarities with practice in Sec. 2. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: This is done for all theoretical statements. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Code is provided to reproduce the main results and an extensive experimental details section is written. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: This is written in the public repo provided in Sec. 4. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, we even give a list of the specific images of ImageNet used to train the multimodal models. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our evaluation metrics are shown with all seeds and their mean plotted in the corresponding figures. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please see Appx. E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: NA Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The work is primarily theoretical and retrospective. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: We do not release general-purpose models, only small-scale illustrative ones. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We site all software and models used in the study in Appx. E. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide a list of notebooks and scripts to illustrate our method and connect it to existing software repositories such as OpenCLIP. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: NA Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: NA Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.