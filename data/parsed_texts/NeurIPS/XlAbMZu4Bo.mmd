# Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length

Xuezhe Ma\({}^{\pi}\)  Xiaomeng Yang\({}^{\mu\ast}\)  Wenhan Xiong\({}^{\mu}\)  Beidi Chen\({}^{\kappa}\)  Lili Yu\({}^{\mu}\)

Hao Zhang\({}^{\delta}\)  Jonathan May\({}^{\pi}\)  Luke Zettlemoyer\({}^{\mu}\)  Omer Levy\({}^{\mu}\)  Chunting Zhou\({}^{\mu\ast}\)

\({}^{\mu}\)AI at Meta

\({}^{\pi}\)University of Southern California

\({}^{\kappa}\)Carnegie Mellon University

\({}^{\delta}\)University of California San Diego

Equal Contribution. Correspondence to chuntinz@meta.com

###### Abstract

The quadratic complexity and weak length extrapolation of Transformers limits their ability to scale to long sequences, and while sub-quadratic solutions like linear attention and state space models exist, they empirically underperform Transformers in pretraining efficiency and downstream task accuracy. We introduce Megalodon, an neural architecture for efficient sequence modeling with unlimited context length. Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability and stability, including _complex exponential moving average (CEMA)_, _timestep normalization_ layer, _normalized attention_ mechanism and _pre-norm with two-hop residual_ configuration. In a controlled head-to-head comparison with Llama2, Megalodon achieves better efficiency than Transformer in the scale of 7 billion parameters and 2 trillion training tokens. Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). The improvements of Megalodon over Transformers are robust throughout a range of benchmarks across different tasks and modalities.

**Code**: https://github.com/XuezheMax/megalodon

## 1 Introduction

In many real-world applications, such as multi-turn conversation, long-document comprehension, and video generation, large language models (LLMs) must efficiently process long sequential data, understand internal long-range dynamics, and generate coherent output. The Transformer architecture (Vaswani et al., 2017), despite its remarkable capabilities, faces challenges with quadratic computational complexity and limited inductive bias for length generalization, making it inefficient for long sequence modeling (Wang et al., 2024; Zhou et al., 2024). Even with recently proposed distributed attention solutions (Li et al., 2023; Liu et al., 2024), computing a single training step of a 7B parameter model over a 1M-token sequence is more than 100 times slower than performing the equivalent computation using 256 separate sequences of 4K tokens each.

Techniques like efficient attention mechanisms (Tay et al., 2020; Ma et al., 2021) and structured state space models (Gu et al., 2022; Poli et al., 2023; Gu and Dao, 2023) have been introduced to overcome these limitations, aiming to enhance scalability and performance. However, the practical application of these methods still falls short of Transformers (Tay et al., 2022; Gu and Dao, 2023). This work introduces an unlimited context model that outperforms the canonical Transformer architecture on real-world language modeling.

We introduce Megalodon, an improved Mega architecture (Ma et al., 2023), which harnesses the gated attention mechanism with the classical exponential moving average (EMA) (Hunter, 1986) approach (SS2). To further improve the capability and efficiency of Megalodon on large-scale long-context pretraining, we propose multiple novel technical components. First, Megalodon introduces the _complex exponential moving average (CEMA)_ component, which extends the multi-dimensional damped EMA in Mega to the complex domain (SS3.1). Then, Megalodon proposes the _timestep normalization_ layer, which generalizes the group normalization layer (Wu and He, 2018) to auto-regressive sequence modeling tasks to allow normalization along the sequential dimension (SS3.2). To improve large-scale pretraining stability, Megalodon further proposes _normalized attention_ (SS3.3), together with _pre-norm with two-hop residual_ configuration by modifying the widely-adopted pre- and post-normalization methods (SS3.4). By simply chunking input sequences into fixed blocks, as is done in Mega-chunk (Ma et al., 2023), Megalodon achieves linear computational and memory complexity in both model training and inference.

Empirically, we demonstrate the potential of Megalodon as a general architecture for modeling long sequences, by evaluating its performance across multiple scales of language modeling, as well as downstream domain-specific tasks. Through a direct comparison with Llama2, while controlling for data and compute, Megalodon-7B significantly outperforms the state-of-the-art variant of Transformer used to train Llama2-7B (Touvron et al., 2023) on both training perplexity (Figure 1) and across downstream benchmarks (Table 1). Evaluation on long-context modeling, including perplexity in various context lengths up to 2M and long-context QA tasks in Scrolls (Parisotto et al., 2020) prove Megalodon's ability to model sequences of unlimited length. Additional experimental results on small/medium-scale benchmarks, including LRA (Tay et al., 2021), ImageNet (Deng et al., 2009), Speech Commands (Warden, 2018), WikiText-103 (Merity et al., 2017) and PG19 (Rae et al., 2019), demonstrate the robust improvements of Megalodon across scales and modalities.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline
**Model** & **Size** & **Tokens** & **Context** & **MMLU** & **BoolQ** & **HellaSw** & **PIQA** & **SIQA** & **WinoG** & **Arce-** & **Avc-** & **NQ** & **TQA** \\ \hline Mamba & 3B & 0.67 & 2K & 26.2 & 71.0 & 71.0 & 78.1 & – & 65.9 & 68.2 & 41.7 & – & – \\ RwKV & 7B & 1.1T & 4K & – & – & 70.8 & 77.3 & – & 68.4 & 74.9 & 46.1 & – & – \\ \hline MPT & 7B & 1T & 4K & 26.8 & 75.0 & 76.4 & 80.6 & 48.5 & 68.3 & 70.2 & 42.6 & 20.8 & 50.4 \\ Mistral & 7B & – & 16K & 60.1 & **83.2** & **81.3** & **82.2** & 47.0 & **74.2** & 80.0 & **54.9** & 23.2 & 62.5 \\ Gemma & 8B & 6T & 8K & **64.3** & **83.2** & 81.2 & 81.2 & **51.8** & 72.3 & **81.5** & 53.2 & 23.0 & 63.4 \\ Llama2 & 13B & 2T & 4K & 54.8 & 81.7 & 80.7 & 80.5 & 50.3 & 72.8 & 77.3 & 49.4 & **31.2** & **65.1** \\ \hline Llama2 & 7B & 2T & 4K & 45.3 & 77.4 & 77.2 & 78.8 & 48.3 & 69.2 & 75.2 & 45.9 & 25.7 & 58.5 \\ Megalodon & 7B & 2T & 32K & 49.8 & 80.5 & 77.5 & 80.1 & 49.6 & 71.4 & 79.8 & 53.1 & 25.7 & 60.5 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Performance on standard academic benchmarks, compared to open-source base models. We reported model size, context length and total data tokens during model pretraining. – indicates that the number was not reported in the original paper.**

Figure 1: **Negative log-likelihood for Megalodon-7B, Llama2-7B and Llama2-13B.**

Background: Moving Average Equipped Gated Attention (Mega)

In this section, we setup notations, briefly review the key components in the Mega architecture (Ma et al., 2023), and discuss the existing problems in Mega.

Following the notations in Mega, we use \(\bm{X}=\{\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{n}\}\in\mathbb{R}^{n \times d}\) and \(\bm{Y}=\{\mathbf{y}_{1},\mathbf{y}_{2},\ldots,\mathbf{y}_{n}\}\in\mathbb{R}^{n \times d}\) to denote the input and output sequences with length \(n\), and assume the representations of the input and output sequences have the same dimension \(d\).

### Multi-dimensional Damped EMA

Mega embeds an EMA component into the calculation of the attention matrix to incorporate inductive biases across the timestep dimension. Concretely, the multi-dimensional damped EMA first expands each dimension of the input sequence \(\bm{X}\) individually into \(h\) dimensions via an expansion matrix \(\bm{\beta}\in\mathbb{R}^{d\times h}\), then applies damped EMA to the \(h\)-dimensional hidden space. Formally, for each dimension \(j\in\{1,2,\ldots,d\}\):

\[\mathbf{u}_{t}^{(j)} =\bm{\beta}_{j}\mathbf{x}_{t,j}\] (1) \[\mathbf{h}_{t}^{(j)} =\bm{\alpha}_{j}\odot\mathbf{u}_{t}^{(j)}+(1-\bm{\alpha}_{j} \odot\bm{\delta}_{j})\odot\mathbf{h}_{t-1}^{(j)}\] \[\mathbf{y}_{t,j} =\bm{\eta}_{j}^{T}\mathbf{h}_{t}^{(j)}\]

where \(\mathbf{u}_{t}^{(j)}\in\mathbb{R}^{h}\) is the expanded \(h\)-dimensional vector for the \(j\)-th dimension at timestep \(t\). \(\bm{\alpha}\in(0,1)^{d\times h}\), \(\bm{\delta}\in(0,1)^{d\times h}\) are the decaying and damping factors, respectively. \(\mathbf{h}_{t}^{(j)}\in\mathbb{R}^{h}\) is the EMA hidden state for the \(j\)-th dimension at timestep \(t\). \(\bm{\eta}\in\mathbb{R}^{d\times h}\) is the projection matrix to map the \(h\)-dimensional hidden state back to \(1\)-dimensional output \(\mathbf{y}_{t,j}\in\mathbb{R}\).

### Moving Average Equipped Gated Attention

In the gated attention mechanism in Mega, the output from EMA (1) is used to compute the shared representation (Hua et al., 2022), because it encodes contextual information through EMA. Subsequently, Mega introduces the reset gate, the update gate, and computes the candidate activation with the update gate and the residual connection. The technical details are provided in Appendix A.

### Existing Problems in Mega

To reduce the quadratic complexity in the full attention mechanism, Mega simply split the sequences of queries, keys and values in (14-16) into chunks of length \(c\). The attention in (17) is individually applied to each chunk, yielding linear complexity \(O(kc^{2})=O(nc)\). Technically, the EMA sub-layer in Mega helps capture local contextual information near each token, mitigating the problem of losing contextual information beyond chunk boundaries in the chunk-wise attention.

Despite the impressive successes of Mega, it still suffers its own problems: i) the performance of Mega with chunk-wise attention still fails behind the one with full attention, due to the limited expressiveness of the EMA sub-layer in Mega. ii) for different tasks and/or data types, there are architectural divergences in the final Mega architectures. For example, different normalization layers, normalization patterns (pre-norm vs. post-norm) and attention functions (\(f(\cdot)\) in (17)) are applied to different data types (see Ma et al. (2023) for details). iii) There are no empirical evidences showing that Mega is scalable for large-scale pretraining.

## 3 Megalodon

To address the aforementioned problems of Mega, in this section we describe the novel technical advancements of Megalodon.

### CEMA: Extending Multi-dimensional Damped EMA to Complex Domain

As discussed in Ma et al. (2023), the EMA component can be regarded as a simplified state space model with diagonal state matrix. Directly inspired from Gu et al. (2022), as almost all matrices diagonalize over the complex plane, a straight-forward idea to improve EMA capability is to extend to work over the complex number system \(\mathbb{C}\). We propose the _complex exponential moving average (CEMA)_, which re-writes Eq. (1):

\[\mathbf{h}_{t}^{(j)} =\bm{\alpha}_{j}(\cos\theta_{j}+i\sin\theta_{j})\odot\mathbf{u}_{t }^{(j)}+(1-\bm{\alpha}_{j}\odot\bm{\delta}_{j})(\cos\theta_{j}+i\sin\theta_{j}) \odot\mathbf{h}_{t-1}^{(j)}\] \[\mathbf{y}_{t,j} =\mathrm{Re}(\bm{\eta}_{j}^{T}\mathbf{h}_{t}^{(j)})\] (2)

where \(\bm{\alpha}\), \(\bm{\delta}\in\mathbb{R}^{d\times h}\) are the real number parameters same as in EMA. Different from EMA, \(\bm{\eta}\in\mathbb{C}^{d\times h}\) in CEMA are complex numbers. \(\theta_{j}\in\mathbb{R}^{h},\ j\in\{1,2,\ldots,d\}\) are the \(h\) arguments. To uniformly space the \(h\) arguments over the period \(2\pi\), we parameterize \(\theta_{j}\) as:

\[\theta_{j,k}=\frac{2\pi k}{h}\omega_{j},\quad\forall k\in\{1,2,\ldots,h\}\] (3)

where the learnable parameter \(\omega\in\mathbb{R}^{d}\) depicts the \(d\) base angles. By decaying the absolute value of each \(h_{t}\), CEMA preserves the decaying structure in kernel weights, which is a key principle to the success of convolutional models on long sequence modeling (Li et al., 2023c).

### Timestep Normalization

Despite the impressive performance of Layer Normalization combined with Transformer, it is obvious that layer normalization cannot directly reduce the internal covariate shift along the spatial dimension (a.k.a timestep or sequential dimension) (Ioffe and Szegedy, 2015). Group Normalization (Wu and He, 2018) normalizes hidden representations both along the timestep dimension and a subset of the feature dimension, which has obtained improvements over Layer Normalization on a range of computer vision tasks. However, it cannot be directly applied to Transformer on auto-regressive sequence modeling, due to the leakage of future information via the mean and variance across the timestep dimension.

In Megalodon, we extend Group Normalization to the auto-regressive case by computing the cumulative mean and variance. Formally, suppose an input sequence \(\bm{X}=\{\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{n}\}\in\mathbb{R}^{ n\times d}\), and \(k\) groups along the feature dimension with \(d_{g}=d/k\) elements per group. Then, the mean and variance of the first group at timestep \(t\in\{1,2,\ldots,n\}\) are:

\[\mu_{t}=\frac{1}{t*d_{g}}\sum_{i=1}^{t}\sum_{j=1}^{d_{g}}x_{i,j},\qquad\sigma_ {t}^{2}=\frac{1}{t*d_{g}}\sum_{i=1}^{t}\sum_{j=1}^{d_{g}}(x_{i,j}-\mu_{t})^{2}\] (4)

Figure 2 illustrates Layer Normalization and Timestep Normalization. To efficiently and precisely calculate the cumulative mean and variance in each timestep, we provide hardware-friendly implementation on modern hardware (GPU) (see Appendix B.1).

### Normalized Attention in Megalodon

Previous studies have investigated the saturation and instability issues in the original scaled dot-product attention (17). A number of novel techniques have emerged to modify the scaled dot-product attention, among which normalized attention mechanisms, such as (scaled-) cosine attention (Luo et al., 2018; Liu et al., 2022) and QK-normalization (Henry et al., 2020), have stood out for the simplicity and effectiveness.

Figure 2: **Normalization methods**. The elements in blue or pink are the regions to compute means and variances. We omit the batch dimension for simplicity.

Directly inspired from these normalized attention mechanisms, we propose the normalized attention mechanism specifically defined for Mega to improve its stability. Formally,

\[\bm{X}^{\prime} =\mathrm{CEMA}(\bm{X}) \in\mathbb{R}^{n\times d}\] (5) \[\bm{Z} =\bm{X}^{\prime}W_{z}+b_{z},\quad\bm{Z}^{\prime}=\frac{\bm{Z}}{ \|\bm{Z}\|} \in\mathbb{R}^{n\times z}\] (6) \[\bm{Q} =\bm{\kappa}_{q}\odot\bm{Z}^{\prime}+\bm{\mu}_{q} \in\mathbb{R}^{n\times z}\] (7) \[\bm{K} =\bm{\kappa}_{k}\odot\bm{Z}^{\prime}+\bm{\mu}_{k} \in\mathbb{R}^{n\times z}\] (8)

where \(\bm{Q}\) and \(\bm{K}\) are computed by using the normalized shared representation \(\bm{Z}^{\prime}\) instead of \(\bm{Z}\). Note that we remove the SiLU (Ramachandran et al., 2017) activation function \(\phi_{\mathrm{silu}}\) in (13), because the normalization on \(\bm{Z}\) has incorporated non-linearity into \(\bm{Z}^{\prime}\). Then the attention operation in (17) has been changed to:

\[\bm{O}=f_{\mathrm{softmax}}\left(\bm{Q}\bm{K}^{T}\right)\bm{V} \in\mathbb{R}^{n\times v}\] (9)

As we use learnable \(\bm{\kappa}_{q}\), \(\bm{\kappa}_{k}\) in (7) and (8), we can remove the scaled term \(\tau(\bm{X})\). In addition, we found that with the normalized attention, the softmax function \(f_{\mathrm{softmax}}\) obtains the best or at least comparable performance on different tasks and data modalities (see Appendix C). Hence, throughout this paper we use softmax as the default attention function.

### Pre-Norm with Two-hop Residual

Normalization configurations are crucial in stably training deep architectures, and pre-normalization (Xiong et al., 2020) has become the default normalization configuration because of its better convergence properties than post-normalization in the original Transformer architecture (Vaswani et al., 2017). However, extensive studies have investigated the instability issue of pre-normalization when scaling up model size (Davis et al., 2021; Liu et al., 2022). Formally, a Transformer-based block in pre-normalization can be formulated as (shown in Figure 3 (b)):

\[\hat{\bm{Y}} =\mathrm{Attention}(\mathrm{Norm}(\bm{X}))+\bm{X}\] \[\bm{Y} =\mathrm{FFN}(\mathrm{Norm}(\hat{\bm{Y}}))+\hat{\bm{Y}}\] \[=\mathrm{FFN}(\mathrm{Norm}(\hat{\bm{Y}}))+\mathrm{Attention}( \mathrm{Norm}(\bm{X}))+\bm{X}\] (10)

where the output \(\bm{Y}\) is the sum of the input \(\bm{X}\) and the output of each component in one block. Hence, the range and/or variance of \(\bm{Y}\) keeps increasing for deeper blocks, causing the instability issue. In

Figure 3: Illustration of the Megalodon architecture. Figure (a) shows a sketch of one Megalodon layer. Figure (b) and (c) display the configurations of pre-norm and pre-norm with two-hop residual, respectively.

the original Mega architecture, the update gate \(\bm{\varphi}\)(19) is used for a gated residual connection (21) to mitigate this problem (Parisotto et al., 2020; Xu et al., 2020). However, the update gate \(\bm{\varphi}\) introduces more model parameters and the instability issue still exists when scaling up model size to 7 billion.

Megalodon introduces a new configuration named _pre-norm with two-hop residual_, which simply re-arranges the residual connections in each block (shown in Figure 3 (c):

\[\hat{\bm{Y}} =\mathrm{Attention}(\mathrm{Norm}(\bm{X}))+\bm{X}\] \[\bm{Y} =\mathrm{FFN}(\mathrm{Norm}(\hat{\bm{Y}}))+\bm{X}\] (11)

where the input \(\bm{X}\) is reused as the residual connection of the FFN layer. Since \(\hat{\bm{Y}}\) is directly followed by a normalization layer, we remove the update gate \(\bm{\varphi}\) and use standard residual connection. The graphical architecture of a Megalodon sub-layer is visualized in Figure 3 (a). Note that the Timestep Normalization is only applied before the attention layer. Before the FFN layer, we still use Layer Normalization. The reasons are two-fold: i) Layer Normalization is faster than Timestep Normalization; ii) the output vector of each token from the attention layer is a mixture of vectors from contextual tokens via attention weights. Hence, normalizing the attention output along the feature dimension is similar to indirectly normalize along the timestep dimension.

### 4-Dimensional Parallelism in Distributed LLM Pretraining

Efficient distributed training algorithm is essential to train a large-scale language model, and several parallelization mechanisms have been introduced. The three most commonly used parallelism strategies are data, tensor (Shoeybi et al., 2019) and pipeline parallelism (Huang et al., 2019). However, the 3-dimensional parallelism is still insufficient to scale up the context length of LLMs (Li et al., 2023; Liu et al., 2024).

Benefiting from the chunk-wise attention in Megalodon, we can efficiently parallelize it along the new timestep/sequence dimension, which is orthogonal to all the aforementioned three parallelism dimensions. In Megalodon, the only communications between devices in one chunk-parallel group are the last hidden state of CEMA and the cumulative mean and variance of Timestep Normalization in each block. Using asynchronous communication, we can minimize the overhead of chunk parallelization by hiding the communication costs in the computation of other components inside the same block and/or other blocks.

## 4 Experiments

To evaluate the scalability and efficiency of Megalodon on long-context sequence modeling, we scale up Megalodon to 7-billion model size and apply it to large-scale language model pretraining on 2 trillion tokens. We also conduct experiments on small/medium-scale sequence modeling benchmarks, including Long Range Arena (LRA) (Tay et al., 2021), raw speech classification on Speech Commands (Warden, 2018), image classification on ImageNet-1K (Deng et al., 2009), and language-modeling on WikiText-103 (Merity et al., 2017) and PG19 (Rae et al., 2019). 2 Empirically, Megalodon significantly outperforms all the state-of-the-art baseline models on these tasks across various data modalities.

Footnote 2: Some results are provided in Appendix C, due to space limits.

### LLM Pretraining

Architectural DetailsIn our Megalodon-7B model, we adopt most of architectural hyperparameters from Llama2-7B to ensure fair comparison: Megalodon-7B consists of 32 blocks, with feature dimension \(d=4096\). Following Llama2, we use the SwiGLU activation function (Shazeer, 2020) in the feed-forward layer, and rotary positional embedding (RoPE, Su et al. (2021)). We set the attention chunk size \(c=4096\), which is the same as the pretraining context length in Llama2. Benefiting from the attention gate (\(\gamma\) in (18)), we use a much smaller number of attention heads \(h=4\) in Megalodon-7B, comparing to \(h=32\) in Llama2-7B. In addition, we apply pre-norm with two-hop residual (SS3.4), using Timestep Normalization (SS3.2) and Layer Normalization (Ba et al., 2016), while Llama2 models apply pre-normalization with RMSNorm (Zhang and Sennrich, 2019).

Data and Pretraining DetailsWe use the same mix of publicly available data from Llama2, ensuring that the model are trained on exactly the same 2-trillion tokens. We also use the same tokenizer as Llama2, whose vocabulary size is \(32\)K.

We trained Megalodon-7B using the AdamW optimizer (Loshchilov and Hutter, 2019), with \(\beta_{1}=0.9\), \(\beta_{2}=0.95\), \(\epsilon=1e-8\). The learning rate is \(3.5e-4\) and cosine learning rate schedule is applied with warmup of \(2500\) steps. We use a weight decay of \(0.1\) and gradient clipping of \(1.0\), and no dropout is applied during training. The context length in pretraining is \(32\)K (4 attention chunks). The global batch size is 4M tokens, and is distributed on 256 NVIDIA A100 GPUs (16K tokens per A100). We set data parallel size to 128, chunk parallel size to 2 and tensor parallel size to 1.

Data and Computation EfficiencyWe evaluate the efficiency of Megalodon w.r.t both the data and computation perspectives. For data efficiency, we display the negative log-likelihood (NLL) for Megalodon-7B, Llama2-7B and Llama2-13B w.r.t processed tokens during training in Figure 1. Megalodon-7B obtains significantly better (lower) NLL than Llama2-7B under the same amount of training tokens, demonstrating better data efficiency. Moreover, Megalodon suffers less training spikes than the Transformer-based architecture in Llama2. Note that at the first 1/4 of the pretraining process (\(<500\)B tokens), the NLL of Megalodon-7B is slightly worse than Llama2-7B. We found that the main reason is that we increased the base \(\theta\) of RoPE from \(10,000\) in Llama2 to \(100,000\) in Megalodon, which slows down model convergence at the beginning of the pretraining process. At the end, Megalodon reaches a training loss of 1.70, landing mid-way between Llama2-7B (1.75) and Llama2-13B (1.67).

For computation efficiency, we conduct experiments of running Llama2-7B and Megalodon-7B using the same amount of computational resources and comparing their training speed under various context lengths. Specifically, we execute each experiment to train a model with global batch size 4M tokens distributed on 256 NVIDIA A100 GPUs (16K tokens per A100) and calculate the word/token per second (WPS) to measure the training speed. Figure 4 illustrates the average WPS per device of Llama2-7B and Megalodon-7B using 4K and 32K context lengths, respectively. For Llama2 models, we accelerate the computation of full attention with Flash-Attention V2 (Dao, 2024). Under 4K context length, Megalodon-7B is slightly slower (about \(6\%\)) than Llama2-7B, due to the introduction of CEMA and Timestep Normalization. When we scale up context length to 32K, Megalodon-7B is significantly faster (about \(32\%\)) than Llama2-7B, demonstrating the computation efficiency of Megalodon for long-context pretraining. In addition, Megalodon-7B-\(32\)K, which utilizes chunk parallelism (SS3.5), achieves about \(94\%\) utilization of Megalodon-7B-4K.

### Short-Context Evaluation on Academic Benchmarks

We compare Megalodon-7B to Llama2 models on standard academic benchmarks with short contexts (\(<4\)K tokens), closely following the settings in Llama2 (Touvron et al., 2023). The benchmarks are grouped into the categories listed below:

* **Commonsense Reasoning** (0-shot): HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-e and -c (Clark et al., 2018).
* **World Knowledge** (5-shot): NaturalQuestions (NQ, Kwiatkowski et al. (2019)) and TriviaQA (TQA, Joshi et al. (2017)).
* **Reading Comprehension** (0-shot): BoolQ (Clark et al., 2019).
* **Popular aggregated results** (5-shot): MMLU (Hendrycks et al., 2020).

Table 1 summarizes the results of Megalodon and Llama2 on these academic benchmarks, together with other open-source base models, including MPT (MosaicML, 2023), RWKV (Peng et al., 2023), Mampa (Gu and Dao, 2023), Mistral (Jiang et al., 2023) and Gemma (Mesnard et al., 2024). Pretrained on the same 2T tokens, Megalodon-7B surpasses Llama2-7B across all the

Figure 4: Average WPS per device.

benchmarks. On some tasks, Megalodon-7B achieves comparable or even better performance with Llama2-13B. Note that Mistral-7B and Gemma-8B were pretrained on much larger datasets than Megalodon-7B, hence the results are not directly comparable.

### Long-Context Evaluation

Perplexity over Long SequencesTo demonstrate the capability of Megalodon to make use of very long contexts to improve next-token prediction, we start by conducting the evaluation of valid perplexity on different context lengths. Concretely, we construct a validation dataset which consists of 1,920 selected books. Each of these books contains sequences with at least 2M tokens. The validation dataset is constructed by first randomly shuffling all the files and then concatenating them. Figure 5 shows the perplexity (PPL) of the validation dataset in various context lengths ranging from 4K to 2M. We observe that the PPL decreases monotonically with context length, validating the effectiveness and robustness of Megalodon on modeling extremely long sequences.

Long-Context QA tasks in ScrollsNext, we evaluate Megalodon on long-context open-book question answering (QA) tasks in the Scrolls dataset (Shaham et al., 2022), including NarrativeQA (Kocisky et al., 2018), Qasper (Daisigi et al., 2021) and QMSum (Zhong et al., 2021). Following Xiong et al. (2023), we use a simple prompt {CONTEXT} Q: {QUESTION} A: for all the tasks, and evaluate 0-shot F1-score on NarrativeQA, 2-shot F1-score on Qasper and 1-shot geometric-ROUGE3 on QMSum. Table 2 lists the results of Megalodon-7B, together with other open-source long-context models in the scale of 7B, namely Xgen-7B-8K (Nijkamp et al., 2023), MPT-7B-8K (MosaicML, 2023), YaRN-7B-128k (Peng et al., 2024), Llama2-7B-4K (Touvron et al., 2023) and Llama2-7B-32K (Llama2-L, Xiong et al. (2023)). Megalodon-7B obtains the best F1 on NarrativeQA, and competitive results with Llama2-7B Long. It should be noticed that Llama2-7B Long extends the context length of Llama2-7B from 4K to 32K by continually pretraining it on additional 500B tokens from long-context data.

Footnote 3: Geometric mean of ROUGE-1, 2 and L.

### Instruction Finetuning

To evaluation the generalization capability of Megalodon on instruction following and alignment, We finetune the base model of Megalodon-7B on a proprietary instruction-alignment data under a controlled setting. We did not apply any RLHF techniques to further finetune it. Table 3 summarizes the performance of chat models in 7B scale on MT-Bench4. Megalodon exhibits superior performance on MT-Bench compared to Vicuna (Chiang et al., 2023), and comparable performance to Llama2-Chat, which utilizes RLHF for further alignment instruction finetuned Megalodon in Appendix D.

Figure 5: PPL in various context lengths.

\begin{table}
\begin{tabular}{l r r r} \hline \hline
**Model** & **NaQA** & **Qasper** & **QMSum** \\ \hline Xgen & 17.4 & 20.5 & 6.8 \\ MPT & 18.8 & 24.7 & 8.8 \\ Yarn & 20.9 & 26.2 & 11.4 \\ \hline Llama2 & 18.8 & 19.8 & 10.1 \\ Llama2-L\({}^{*}\) & 23.5 & **28.3** & **14.5** \\ \hline Megalodon & **23.9** & 28.0 & 13.1 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Results on Scrolls. \({}^{*}\) Llama2-L (Xiong et al., 2023) continually trains Llama2 on 500B tokens for length extension.**

### Evaluation on Medium-Scale Benchmarks

ImageNet ClassificationTo evaluate Megalodon on image classification task, we conduct experiments on the Imagenet-1K (Deng et al., 2009) dataset, which consists of 1.28M training images and 50K validation images from 1000 classes. We mostly follow DeiT's approach of applying several data augmentation and regularization methods that facilitate the training process, and adopt most the hyperparameters from Ma et al. (2023). For classification task, we replace the timestep normalization with the standard group normalization method. Top-1 accuracy on the validation set is reported in Table 4 to assess various models. Megalodon obtains about \(1.3\)% accuracy improvement over DeiT-B (Touvron et al., 2021), and \(0.8\)%. improvement over Mega(Ma et al., 2023).

Auto-regressive Language Modeling on PG-19We also evaluate Megalodon on auto-regressive language modeling on the medium-scale PG19 (Rae et al., 2019) datasets. We use the same vocabulary from Block-Recurrent Transformer (Hutchins et al., 2022) and adopt most of its hyper-parameters to train a Megalodon model with 1.3B parameters. Table 5 illustrate the word-level perplexity (PPL) of Megalodon on PG-19, together with previous state-of-the-art models, including Compressive Transformer (Rae et al., 2020), Perceiver AR (Hawthorne et al., 2022), Block-Recurrent Transformer (Hutchins et al., 2022) and MegaByte (Yu et al., 2024). Megalodon significantly outperforms all the baselines.

## 5 Conclusion

We have introduced Megalodon, an improved Mega architecture with multiple novel technical components, including complex exponential moving average (CEMA), the timestep normalization layer, normalized attention and pre-norm with two-hop residual configuration, to improve its capability, efficiency and scalability. Through a direct comparison with Llama2, Megalodon achieves impressive improvements on both training perplexity and across downstream benchmarks. Importantly, experimental results on long-context modeling demonstrate Megalodon's ability to model sequences of unlimited length. Additional experiments on small/medium-scale benchmarks across different data modalities illustrate the robust improvements of Megalodon, which lead to a potential direction of future work to apply Megalodon for large-scale multi-modality pretraining.

## Acknowledgments

We thank Sadhika Malladi, Zihao Ye, Dacheng Li and Rulin Shao for their helpful feedback and discussion during this work.

## References

* Ba et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* Baevski and Auli (2018) Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In _International Conference on Learning Representations_, 2018.
* Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In _Proceedings of the AAAI conference on artificial intelligence_, pages 7432-7439, 2020.
* Baevski et al. (2018)

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Model** & **\#Param.** & **Acc.** \\ \hline ResNet-152 & 60M & 78.3 \\ ViT-B & 86M & 77.9 \\ DeiT-B & 86M & 81.8 \\ Mega & 90M & 82.3 \\ \hline Megalodon & 90M & **83.1** \\ \hline \hline \end{tabular}
\end{table}
Table 4: **(ImageNet-1K) Top-1 accuracy.**

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Model** & **\#Param.** & **Val** & **Test** \\ \hline Compressive Trans. & – & 43.4 & 33.6 \\ Perceiver AR & 975M & 45.9 & 28.9 \\ Block-Recurrent Trans. & 1.3B & – & 26.5 \\ MegaByte & 1.3B & 42.8 & 36.4 \\ \hline Megalodon & 1.3B & **29.5** & **25.4** \\ \hline \hline \end{tabular}
\end{table}
Table 5: **(PG-19) Word-level perplexity.**Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.
* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. _arXiv preprint arXiv:1905.10044_, 2019.
* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* Cooley and Tukey (1965) James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series. _Mathematics of computation_, 19(90):297-301, 1965.
* Dai et al. (2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 2978-2988, 2019.
* Dao (2024) Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In _International Conference on Learning Representations (ICLR-2024)_, 2024.
* Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-2021)_, pages 4599-4610, Online, June 2021. Association for Computational Linguistics.
* Davis et al. (2021) Jared Q Davis, Albert Gu, Krzysztof Choromanski, Tri Dao, Christopher Re, Chelsea Finn, and Percy Liang. Catformer: Designing stable transformers via sensitivity analysis. In _International Conference on Machine Learning_, pages 2489-2499. PMLR, 2021.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Fu et al. (2023) Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In _The Eleventh International Conference on Learning Representations (ICLR-2023)_, 2023.
* Gu and Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.
* Gu et al. (2022a) Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In _International Conference on Learning Representations (ICLR-2022)_, 2022a.
* Gu et al. (2022b) Albert Gu, Ankit Gupta, Karan Goel, and Christopher Re. On the parameterization and initialization of diagonal state space models. _arXiv preprint arXiv:2206.11893_, 2022b.
* Hanson and Pratt (1988) Stephen Hanson and Lorien Pratt. Comparing biases for minimal network construction with back-propagation. _Advances in neural information processing systems_, 1, 1988.
* Hawthorne et al. (2022) Curtis Hawthorne, Andrew Jaegle, Catalina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, et al. General-purpose, long-context autoregressive modeling with perceiver ar. In _International Conference on Machine Learning_, pages 8535-8558. PMLR, 2022.
* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.
* Henry et al. (2020) Alex Henry, Prudhvi Raj Dachapally, Shubham Shataram Pawar, and Yuxuan Chen. Query-key normalization for transformers. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 4246-4253, 2020.
* Heng et al. (2020)Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In _International Conference on Machine Learning (ICML-2022)_, pages 9099-9117. PMLR, 2022.
* Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Hunter (1986) J Stuart Hunter. The exponentially weighted moving average. _Journal of quality technology_, 18(4):203-210, 1986.
* Hutchins et al. (2022) DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent transformers. _Advances in neural information processing systems_, 35:33248-33261, 2022.
* Ioffe and Szegedy (2015) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning (ICML-2015)_, pages 448-456. pmlr, 2015.
* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_. Association for Computational Linguistics, 2017.
* Kahan (1965) William Kahan. Pracniques: further remarks on reducing truncation errors. _Communications of the ACM_, 8(1):40, 1965.
* Kocisky et al. (2018) Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge. _Transactions of the Association for Computational Linguistics_, 6:317-328, 2018.
* Krizhevsky et al. (2009) Alex Krizhevsky et al. Learning multiple layers of features from tiny images. _Technical Report. University of Toronto_, 2009.
* Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:453-466, 2019.
* Li et al. (2023a) Bonan Li, Yinhan Hu, Xuecheng Nie, Congying Han, Xiangjian Jiang, Tiande Guo, and Luoqi Liu. Dropkey for vision transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 22700-22709, June 2023a.
* Li et al. (2023b) Dacheng Li, Rulin Shao, Anze Xie, Eric P Xing, Joseph E Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. Lightseq:: Sequence level parallelism for distributed training of long context transformers. In _Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)_, 2023b.
* Li et al. (2023c) Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? In _International Conference on Learning Representations (ICLR-2023)_, 2023c.
* Linsley et al. (2018) Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learning long-range spatial dependencies with horizontal gated recurrent units. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* Liu et al. (2024) Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. In _International Conference on Learning Representations (ICLR-2024)_, 2024.
* Liu et al. (2020)* Liu et al. (2022) Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12009-12019, 2022.
* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations_, 2019.
* Luo et al. (2018) Chunjie Luo, Jianfeng Zhan, Xiaohe Xue, Lei Wang, Rui Ren, and Qiang Yang. Cosine normalization: Using cosine similarity instead of dot product in neural networks. In _27th International Conference on Artificial Neural Networks (ICANN-2018)_, pages 382-391. Springer, 2018.
* Ma et al. (2021) Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. _Advances in Neural Information Processing Systems_, 34:2441-2453, 2021.
* Ma et al. (2023) Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In _The Eleventh International Conference on Learning Representations_, 2023.
* Maas et al. (2011) Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies_, pages 142-150, 2011.
* Merity et al. (2017) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In _International Conference on Learning Representations (ICLR-2017)_, 2017.
* Mesnard et al. (2017) Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Leonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amelie Heliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clement Cergy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryy Michalewski, Ian Tenney, Ivan Grischenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikula, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clement Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Dennis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024.
* MosaicML (2023) MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.
* Nangia and Bowman (2018) Nikita Nangia and Samuel Bowman. Listops: A diagnostic dataset for latent tree learning. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop_, pages 92-99, 2018.
* Nijkamp et al. (2023) Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Krycsinski, Lidiya Murakhovs'ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Joty, and Caiming Xiong. Xgen-7b technical report, 2023.
* Parisotto et al. (2020) Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning. In _International conference on machine learning_, pages 7487-7498. PMLR, 2020.
* PogAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* Peng et al. (2023) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. _arXiv preprint arXiv:2305.13048_, 2023.
* Peng et al. (2024) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. In _International Conference on Learning Representations (ICLR-2024)_, 2024.
* Poli et al. (2023) Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. In _International conference on machine learning (ICML-2023)_. PMLR, 2023.
* Radev et al. (2013) Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network corpus. _Language Resources and Evaluation_, 47(4):919-944, 2013.
* Rae et al. (2019) Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. _arXiv preprint_, 2019. URL https://arxiv.org/abs/1911.05507.
* Rae et al. (2020) Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modeling. In _International Conference on Learning Representations (ICLR-2020)_, 2020.
* Ramachandran et al. (2017) Prajit Ramachandran, Barret Zoph, and Quoc V Le. Swish: a self-gated activation function. _arXiv preprint arXiv:1710.05941_, 7(1):5, 2017.
* Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.
* Sap et al. (2019) Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. _arXiv preprint arXiv:1904.09728_, 2019.
* Shaham et al. (2022) Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP-2022)_, pages 12007-12021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
* Shazeer (2020) Noam Shazeer. Glu variants improve transformer. _arXiv preprint arXiv:2002.05202_, 2020.
* Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. _arXiv preprint arXiv:1909.08053_, 2019.
* Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _arXiv preprint arXiv:2104.09864_, 2021.
* Tay et al. (2020) Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. _arXiv preprint arXiv:2009.06732_, 2020.
* Tay et al. (2021) Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k.
* Tay et al. (2022) Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q. Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling?, 2022.
* Tay et al. (2020)Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _International Conference on Machine Learning_, pages 10347-10357. PMLR, 2021.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017.
* Wang et al. (2024) Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi. Beyond the limits: A survey of techniques to extend the context length in large language models, 2024.
* Warden (2018) Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. _arXiv preprint arXiv:1804.03209_, 2018.
* Welford (1962) B. P. Welford. Note on a method for calculating corrected sums of squares and products. _Technometrics_, 4(3):419-420, 1962.
* Wu and He (2018) Yuxin Wu and Kaiming He. Group normalization. In _Proceedings of the European conference on computer vision (ECCV-2018)_, pages 3-19, 2018.
* Xiong et al. (2020) Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In _International Conference on Machine Learning_, pages 10524-10533. PMLR, 2020.
* Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajiwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. _arXiv preprint arXiv:2309.16039_, 2023.
* Xu et al. (2020) Hongfei Xu, Qiuhui Liu, Deyi Xiong, and Josef van Genabith. Transformer with depth-wise lstm. _arXiv preprint arXiv:2007.06257_, 2020.
* Yu et al. (2024) Lili Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers. _Advances in Neural Information Processing Systems_, 36, 2024.
* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL-2019)_. Association for Computational Linguistics, 2019.
* Zhang and Sennrich (2019) Biao Zhang and Rico Sennrich. Root mean square layer normalization. _Advances in Neural Information Processing Systems_, 32, 2019.
* Zhong et al. (2021) Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutthia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based multi-domain meeting summarization. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-2021)_, pages 5905-5921, Online, June 2021. Association for Computational Linguistics.
* Zhou et al. (2024) Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou. Transformers can achieve length generalization but not robustly, 2024.

## Appendix A Background: Mosing Average Equipped Gated Attention

In the gated attention mechanism in Mega, the output from EMA (1) is used to compute the shared representation (Hua et al., 2022)\(\bm{Z}\):

\[\bm{X}^{\prime} =\mathrm{EMA}(\bm{X}) \in\mathbb{R}^{n\times d}\] (12) \[\bm{Z} =\phi_{\mathrm{silu}}(\bm{X}^{\prime}W_{z}+b_{z}) \in\mathbb{R}^{n\times z}\] (13)

where \(\bm{X}^{\prime}\) can be regarded as the updated or contextual input, because it encodes contextual information through EMA. Then, the query and key sequences are computed by applying per-dimension scalars and offsets to \(\bm{Z}\), and the value sequence is from the original \(\bm{X}\):

\[\bm{Q} =\bm{\kappa}_{q}\odot\bm{Z}+\bm{\mu}_{q} \in\mathbb{R}^{n\times z}\] (14) \[\bm{K} =\bm{\kappa}_{k}\odot\bm{Z}+\bm{\mu}_{k} \in\mathbb{R}^{n\times z}\] (15) \[\bm{V} =\phi_{\mathrm{silu}}(\bm{X}W_{v}+b_{v}) \in\mathbb{R}^{n\times v}\] (16)

where \(\bm{\kappa}_{q}\), \(\bm{\mu}_{q}\), \(\bm{\kappa}_{k}\), \(\bm{\mu}_{k}\in\mathbb{R}^{z}\) are the learnable scalars and offsets of queries and keys, respectively. \(v\) is the expanded intermediate dimension for the value sequence. The output of attention is computed as follows:

\[\bm{O} =f\left(\frac{\bm{Q}\bm{K}^{T}}{\tau(\bm{X})}\right)\bm{V} \in\mathbb{R}^{n\times v}\] (17)

Subsequently, Mega introduces the reset gate \(\bm{\gamma}\), the update gate \(\bm{\varphi}\), and computes the candidate activation \(\hat{\bm{H}}\) and final output \(\bm{Y}\):

\[\bm{\gamma} =\phi_{\mathrm{silu}}(\bm{X}^{\prime}W_{\gamma}+b_{\gamma}) \in\mathbb{R}^{n\times v}\] (18) \[\bm{\varphi} =\phi_{\mathrm{sigmoid}}(\bm{X}^{\prime}W_{\varphi}+b_{\varphi}) \in\mathbb{R}^{n\times d}\] (19) \[\hat{\bm{H}} =\phi_{\mathrm{silu}}(\bm{X}^{\prime}W_{h}+(\bm{\gamma}\odot\bm{ O})U_{h}+b_{h}) \in\mathbb{R}^{n\times d}\] (20) \[\bm{Y} =\bm{\varphi}\odot\hat{\bm{H}}+(1-\bm{\varphi})\odot\bm{X} \in\mathbb{R}^{n\times d}\] (21)

with the update gate \(\bm{\varphi}\) and the residual connection \(\bm{X}\).

## Appendix B Implementation Details

### Efficient Fused CUDA Operators Implementation

Fused AttentionWe implemented a fused attention operator to improve the efficiency, mainly by fusing the causal mask, softmax function and dropout operation (if necessary). The fused implementation reduces the IO costs from global memory for the attention weight. For attention dropout, we adopt the dropout-before-softmax scheme in DropKey (Li et al., 2023), which applies the dropout mask on the input attention matrix of the softmax function. Concretely, we fill the values of the attention matrix at dropout mask positions to \(-\infty\) before feeding it into the softmax function. One important advantage of this dropout-before-softmax scheme comparing to the standard attention dropout is that the computation of the gradients in back-propagation is independent with the applied dropout mask.

Efficient FFTConvWe also provide an efficient fused implementation of the FFTConv operator. Similar with the FlashConv in H3 (Fu et al., 2023), we fused the real number FFT (RFFT), its inverse (IRFFT) and implemented the Cooley-Tukey FFT algorithm (Cooley and Tukey, 1965) in the CUDA shared memory. Similar with the FlashConv in H3 (Fu et al., 2023), we fused the real number FFT (RFFT), its inverse (IRFFT) and the element-wise multiplication, and implemented the Cooley-Tukey FFT algorithm (Cooley and Tukey, 1965) in CUDA's shared memory. Our implementation is able to accommodate up to 16K tokens in the limited shared memory of an A100 GPU.

Timestep NormalizationFor the TimestepNorm operator, we have an efficient implementation to improve both its speed and numerical stability. To compute the cumulative mean and variance for each of the timesteps, our implementation distributed the threads in each CUDA block in both the timestep/sequence dimension and the feature dimension to balance the parallelism of the algorithm and the performance of the global memory access. To improve numerical stability, we used the Welford algorithm (Welford, 1962) to compute the cumulative mean and variance and the Kahan Summation (Kahan, 1965) to reduce the numerical error from summation.

### Plus 1 Reparameterization in Normalization Layers

In the normalization methods, two learnable parameters \(\gamma\) and \(\beta\) are introduced to scale and shift the normalized value:

\[y=\gamma\frac{x-\mu}{\sigma}+\beta\] (22)

where \(\mu\) and \(\sigma^{2}\) are the mean and variance of the input \(x\) across the pre-defined dimensions. Initialization of \(\gamma\) and \(\beta\) is crucial for model performance and stability. The standard implementation of normalization layers, such as PyTorch (Paszke et al., 2019), initializes \(\gamma\) and \(\beta\) to vectors of ones and zeros, respectively, to preserve the mean and variance of the normalized inputs at the beginning of training.

This standard implementation, however, suffers a problem when weight decay regularization is applied to prevent overfitting (Hanson and Pratt, 1988). Technically, the weight decay regularization pushes the values of model parameters towards smaller magnitudes. In the context of normalization methods, weight decay pushes the values in \(\gamma\) towards zero, which diverges from its initialization of one. This may prevent the model from learning the true scale of the data distribution, and may cause numerical stability issues as well.

To address this problem, we used the _plus 1 reparameterization5_ of the scale parameter \(\gamma\):

Footnote 5: Similar idea in the blog: https://medium.com/@ohadrubin/exploring-weight-decay-in-layer-normalization-challenges-and-a-reparameterization-solution-add4l12c24950

\[y=(\gamma+1)\frac{x-\mu}{\sigma}+\beta\] (23)

where \(\gamma\) is initialized to zero. Under weight decay, \(\gamma\) remains centered around zero, resulting in a desirable scale of \(\gamma+1\) around one.

## Appendix C Experiments on Small-Scale Benchmarks

We conducted small-scale experiments on five benchmarks across various data modalities, including text, audio and image. To demonstrate the robustness of the Megalodon architecture on different tasks and data types, we used a single unified architecture with minimal architectural divergence in

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline
**Models** & **ListOps** & **Text** & **Retrieval** & **Image** & **Pathfinder** & **Path-X** & **Avg.** \\ \hline Transformer & 37.11 & 65.21 & 79.14 & 42.94 & 71.83 & ✗ & 59.24 \\ Reformer & 37.27 & 56.10 & 53.40 & 38.07 & 68.50 & ✗ & 50.67 \\ Linformer & 35.70 & 53.94 & 52.27 & 38.56 & 76.34 & ✗ & 51.36 \\ BigBird & 36.05 & 64.02 & 59.29 & 40.83 & 74.87 & ✗ & 55.01 \\ Luna-\(256\) & 37.98 & 65.78 & 79.56 & 47.86 & 78.55 & ✗ & 61.95 \\ S4 & 59.10 & 86.53 & 90.94 & 88.48 & 94.01 & 96.07 & 85.86 \\ \hline Mega-chunk & 58.76 & 90.19 & 90.97 & 85.80 & 94.41 & 93.81 & 85.66 \\ Mega & 63.14 & 90.43 & 91.25 & **90.44** & 96.01 & 97.98 & 88.21 \\ \hline Megalodon-chunk & 62.23 & **90.53** & 91.74 & 87.11 & 96.89 & 97.21 & 87.62 \\ Megalodon & **63.79** & 90.48 & **91.76** & 89.42 & **98.13** & **98.17** & **88.63** \\ \hline \hline \end{tabular}
\end{table}
Table 6: (**Long Range Arena**) Accuracy on the full suite of long range arena (LRA) tasks. Results of previous models are reported in Ma et al. (2023).

all the experiments: softmax attention function, rotary positional embedding, pre-norm with two-hop residual, and timestep Normalization (Group Normalization for classification). We adopt (almost) all the architectural and training hyperparameters from the corresponding experiments of the original Mega(Ma et al., 2023).

### Long Range Arena (LRA)

Long Range Arena (LRA) benchmark (Tay et al., 2021) is designed for evaluating sequence models under the long-context scenario. They collect six tasks in this benchmark which are ListOps (Nangia and Bowman, 2018), byte-level text classification (Text; Maas et al. (2011)), byte-level document retrieval (Retrieval; Radev et al. (2013)), image classification on sequences of pixels (Image; Krizhevsky et al. (2009)), Pathfinder (Linsley et al., 2018) and its extreme long version (Path-X; Tay et al. (2021)). These tasks consist of input sequences ranging from 1K to 16K tokens and span across a variety of data types and modalities.

Table 6 compares Megalodon against several baselines, including Transformer and its efficient variants, the state space model S4 (Gu et al., 2022), and the original Mega model. Following Ma et al. (2023), we also evaluate Megalodon-chunk on each task, by setting the chunk size \(c=128\) for all the tasks, except Path-X where \(c=4096\). With chunk-wise attention, Megalodon-chunk substantially outperforms Mega-chunk on all the six tasks. In addition, Megalodon significantly narrows the gap between chunk-wise attention and full attention.

### Raw Speech Classification

To evaluate the capability of Megalodon on the long-range modeling of speech signals, we apply Megalodon to classify raw speech (with length 16000), rather than using traditional preprocessing (e.g. convert to MFCC features). Following Ma et al. (2023), we perform speech classification on the SC10 subset of the Speech Commands dataset (Warden, 2018), and set attention chunk size \(c=1000\). As shown in Table 7, our Megalodon model with 300K parameters achieves an accuracy of 98.14, which is significantly better than 97.50 from S4 and 96.92 from Mega with similar model size.

### Auto-regressive Language Modeling

We also evaluate Megalodon on auto-regressive language modeling on the small-scale WikiText-103 (Merity et al., 2017) datasets. Following Baevski and Auli (2018), we adopt adaptive softmax and input embeddings and use a vocabulary of 260K tokens for WikiText-103 and train a Megalodon model with about 250M parameters with attention chunk size \(c=2048\). Table 8 illustrate the word-level perplexity (PPL) of Megalodon on WikiText-103, together with previous state-of-the-art models, including Transformer (Baevski and Auli, 2018), Transformer-XL (Dai et al., 2019), S4 (Gu et al., 2022) and Mega(Ma et al., 2023). Megalodon significantly outperforms all the baselines on both the two datasets.

## Appendix D Model Outputs from Instruction-finetuned Megalodon

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Model** & **\#Param.** & **Acc.** \\ \hline Transformer & 786K & ✗ \\ S4 & 300K & 97.50 \\ Mega & 300K & 96.92 \\ Mega (big) & 476K & 97.30 \\ \hline Megalodon & 300K & **98.14** \\ \hline \hline \end{tabular}
\end{table}
Table 7: (**SC-Raw**) Accuracy.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Model** & **\#Param.** & **PPL** \\ \hline Transformer & 247M & 18.66 \\ Transformer-XL & 257M & 18.30 \\ S4 & 249M & 20.95 \\ Mega & 252M & 18.07 \\ \hline Megalodon & 252M & **17.23** \\ \hline \hline \end{tabular}
\end{table}
Table 8: (**WikiText-103**) Word-level PPL.

What is the central dogma of molecular biology? What processes are involved? Who named this?

The central dogma of molecular biology describes the flow of genetic information from DNA to RNA to proteins. The three processes involved in the central dogma are transcription, translation, and translocation.

The central dogma was first proposed by Francis Crick, James Watson, and Maurice Wilkins in the 1950s, and was later formalized by Crick in a 1958 paper titled "On the Central Dogma of Molecular Biology". The term "central dogma" was coined by Crick to describe the flow of genetic information from DNA to RNA to proteins.

Identify and fix one incorrect fact in your previous response.

The correct name of the person who proposed the central dogma of molecular biology is James Watson, not Maurice Wilkins. James Watson, along with Francis Crick, first proposed the central dogma in the 1950s, and Maurice Wilkins was a co-recipient of the Nobel Prize in Physiology or Medicine in 1962 for his contributions to the discovery of the double helix structure of DNA.

Act as a math teacher. I will provide some mathematical equations or concepts, and it will be your job to explain them in easy-to-understand terms. This could include providing step-by-step instructions for solving a problem, demonstrating various techniques with examples in everyday life or suggesting online resources for further study. My first request is "I need help understanding how probability works."

Probability is a branch of mathematics that deals with the likelihood of an event occurring. It is often expressed as a number between 0 and 1, where 0 represents an impossible event and 1 represents a certain event.

For example, if you were not to dari a fix-sided die, the probability of rolling a 1 would be 1/6, or about 17%. This is because there is only one way for the die to land on a lot of the six possible outcomes.

To calculate probability, you can use the formula:

Probability = Number of ways the event can occur / Total number of possible outcomes

For example, if you were to ask "What is the probability of flipping heads on a fair coin toss?", the answer would be 1/2, or 50%. This is because there is only one way for the coin to land heads-up out of the two possible outcomes (heads or tails).

There are many different ways to interpret probability, and it can be used in a variety of real-world applications such as gambling, insurance, and statistics.

I hope this helps to give you a basic understanding of probability. Let me know if you have any questions or need further clarification.

What are the differences between Riemannian geometry and euclidean geometry?

Riemamian geometry and Euclidean geometry are both branches of mathematics that study the properties of space, but they differ in their assumptions and methods.

Euclidean geometry is based on the assumption that the space is flat and flat Euclidean space is the simplest non-empty connected flat metric space. It studies the properties of space using concepts such as distance, angle, and parallelism. It is named after the Greek mathematician Euclid, who developed the foundations of the field in his book "Elements".

Riemannian geometry, on the other hand, is based on the assumption that the space is curved. It studies the properties of space using concepts such as distance, angle, and parallelism, but in a more general setting, and it is named after the German mathematician Bernhard Riemann.

In Riemannian geometry, the concept of a metric is used to measure the distance between two points in a curved space. A metric is a mathematical function that assigns a distance to each segment in a space. The most common metric in Riemannian geometry is the distance function induced by a Riemannian metric, which is a mathematical function that assigns a distance to each segment in a space. In summary, Euclidean geometry is a special case of Riemannian geometry, where the metric is the flat Euclidean metric. Riemannian geometry is a more general field that studies the properties of curved spaces, and it is used in many areas of mathematics, physics and other sciences.

I hope this helps to give you a basic understanding of the differences between Riemannian geometry and Euclidean geometry. Let me know if you have any questions or need further clarification.

Evaluate the following movie reviews on a scale of 1 to 5, with 1 being very negative, 3 being neutral, and 5 being very positive: 1. This movie released on Nov. 18, 2019, was phenomenal. The cinematography, the acting, the plot - everything was top-notch. 2.

Never before have I been so disappointed with a movie. The plot was predictable and the characters were one-dimensional. In my opinion, this movie is the worst one to have been released in 2022, 3. The movie was okay. There were some parts I enjoyed, but there were also parts that felt lackluster. This is a movie that was released in Feb 2018 and seems to be quite ordinary. Return the answer as a JSON array of integers.

[5, 1, 3]

Update your previous reply by including the release date as part of the JSON content.

[ {"release_date": "Nov 18, 2019", "rating": 5}, {"release_date": "2022", "rating": 1}, {"release_date": "Feb 2018", "rating": 3} ]

Figure 6: Model outputs from instruction-finetuned Megalodon.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: see the abstract Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: No theoretical results involved.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: see appendix Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: see appendix Guidelines: * The answer NA means that paper does not include experiments requiring code.

* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: see appendix Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).

* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: see experiments and appendix Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used.

* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.