What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement

Yotam Alexander

Equal contribution

Nimrod De La Vega

Equal contribution

Noam Razin

Nadav Cohen

Tel Aviv University

{yotama,nimrodd,noamrazin}@mail.tau.ac.il, cohennadav@cs.tau.ac.il

###### Abstract

The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution _if and only if_ the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about the relation between deep learning and real-world data.1

Footnote 1: Due to lack of space, a significant portion of the paper is deferred to the appendices. We refer the reader to [2] for a self-contained version of the text.

## 1 Introduction

Deep learning is delivering unprecedented performance when applied to data modalities involving images, text and audio. On the other hand, it is known both theoretically and empirically [53, 1] that there exist data distributions over which deep learning utterly fails. The question of _what makes a data distribution suitable for deep learning_ is a fundamental open problem in the field.

A prevalent family of deep learning architectures is that of _locally connected neural networks_. It includes, among others: _(i)_ convolutional neural networks, which dominate the area of computer vision; _(ii)_ recurrent neural networks, which were the most common architecture for sequence (_e.g._ text and audio) processing, and are experiencing a resurgence by virtue of S4 models [26]; and _(iii)_ local variants of self-attention neural networks [46]. Conventional wisdom postulates that data distributions suitable for locally connected neural networks are those exhibiting a "local nature," and there have been attempts to formalize this intuition [66, 28, 15]. However, to the best of our knowledge, there are no characterizations providing necessary and sufficient conditions for a data distribution to be suitable to a locally connected neural network.

A seemingly distinct scientific discipline tying distributions and computational models is _quantum physics_. There, distributions of interest are described by _tensors_, and the associated computational models are _tensor networks_. While there is shortage in formal tools for assessing the suitability of data distributions to deep learning architectures, there exists a widely accepted theory that allows for assessing the suitability of tensors to tensor networks. The theory is based on the notion of _quantum_entanglement_, which quantifies dependencies that a tensor admits under partitions of its axes (for a given tensor \(\mathcal{A}\) and a partition of its axes to sets \(\mathcal{K}\) and \(\mathcal{K}^{c}\), the entanglement is a non-negative number quantifying the dependence that \(\mathcal{A}\) admits between \(\mathcal{K}\) and \(\mathcal{K}^{c}\)).

In this paper, we apply the foregoing theory to a tensor network equivalent to a certain locally connected neural network, and derive theorems by which fitting a tensor is possible if and only if the tensor admits low entanglement under certain _canonical partitions_ of its axes. We then consider the tensor network in a machine learning context, and find that its ability to attain low approximation error, _i.e._ to express a solution with low population loss, is determined by its ability to fit a particular tensor defined by the data distribution, whose axes correspond to features. Combining the latter finding with the former theorems, we conclude that a _locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low entanglement under canonical partitions of features_. Experiments with different datasets corroborate this conclusion, showing that the accuracy of common locally connected neural networks (including modern convolutional, recurrent, and local self-attention neural networks) is inversely correlated to the entanglement under canonical partitions of features in the data (the lower the entanglement, the higher the accuracy, and vice versa).

The above results bring forth a recipe for enhancing the suitability of a data distribution to locally connected neural networks: given a dataset, search for an arrangement of features which leads to low entanglement under canonical partitions, and then arrange the features accordingly. Unfortunately, the above search is computationally prohibitive. However, if we employ a certain correlation-based measure as a surrogate for entanglement, _i.e._ as a gauge for dependence between sides of a partition of features, then the search converts into a succession of _minimum balanced cut_ problems, thereby admitting use of well-established graph theoretical tools, including ones designed for large scale [29; 57]. We empirically evaluate this approach on various datasets, demonstrating that it substantially improves prediction accuracy of common locally connected neural networks (including modern convolutional, recurrent, and local self-attention neural networks).

The data modalities to which deep learning is most commonly applied -- namely ones involving images, text and audio -- are often regarded as natural (as opposed to, for example, tabular data fusing heterogeneous information). We believe the difficulty in explaining the suitability of such modalities to deep learning may be due to a shortage in tools for formally reasoning about natural data. Concepts and tools from physics -- a branch of science concerned with formally reasoning about natural phenomena -- may be key to overcoming said difficulty. We hope that our use of quantum entanglement will encourage further research along this line.

## 2 Preliminaries

For simplicity, the main text treats locally connected neural networks whose input data is one dimensional (_e.g._ text or audio). We defer to Appendix J an extension of the analysis and experiments to models intaking data of arbitrary dimension (_e.g._ two-dimensional images). Due to lack of space, we also defer our review of related work to Appendix A.

We use \(\left\lVert\cdot\right\rVert\) and \(\left\langle\cdot,\cdot\right\rangle\) to denote the Euclidean (Frobenius) norm and inner product, respectively. We shorthand \([N]:=\{1,\ldots,N\}\), where \(N\in\mathbb{N}\). The complement of \(\mathcal{K}\subseteq[N]\) is denoted by \(\mathcal{K}^{c}\), _i.e._\(\mathcal{K}^{c}:=[N]\setminus\mathcal{K}\).

### Tensors and Tensor Networks

For our purposes, a _tensor_ is an array with multiple axes \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\), where \(N\in\mathbb{N}\) is its _order_ and \(D_{1},\ldots,D_{N}\in\mathbb{N}\) are its _axes lengths_. The \((d_{1},\ldots,d_{N})\)'th entry of \(\mathcal{A}\) is denoted \(\mathcal{A}_{d_{1},\ldots,d_{N}}\).

_Contraction_ between tensors is a generalization of multiplication between matrices. Two matrices \(\mathbf{A}\in\mathbb{R}^{D_{1}\times D_{2}}\) and \(\mathbf{B}\in\mathbb{R}^{D^{\prime}_{1}\times D^{\prime}_{2}}\) can be multiplied if \(D_{2}=D^{\prime}_{1}\), in which case we get a matrix in \(\mathbb{R}^{D_{1}\times D^{\prime}_{2}}\) holding \(\sum_{d=1}^{D_{2}}\mathbf{A}_{d,d}\cdot\mathbf{B}_{d,d^{\prime}_{2}}\) in entry \((d_{1},d^{\prime}_{2})\in[D_{1}]\times[D^{\prime}_{2}]\). More generally, two tensors \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) and \(\mathcal{B}\in\mathbb{R}^{D^{\prime}_{1}\times\cdots\times D^{\prime}_{N^{ \prime}}}\) can be contracted along axis \(n\in[N]\) of \(\mathcal{A}\) and \(n^{\prime}\in[N^{\prime}]\) of \(\mathcal{B}\) if \(D_{n}=D^{\prime}_{n^{\prime}}\), in which case we get a tensor of size \(D_{1}\times\cdots D_{n-1}\times D_{n+1}\times\cdots\times D_{N}\times D^{\prime }_{1}\times\cdots\times D^{\prime}_{n^{\prime}-1}\times D^{\prime}_{n^{\prime} +1}\cdots\times D^{\prime}_{N^{\prime}}\) holding \(\sum_{d=1}^{D_{n}}\mathcal{A}_{d_{1},\ldots,d_{n-1},d,d_{n+1},\ldots,d_{N}} \cdot\mathcal{B}_{d^{\prime}_{1},\ldots,d^{\prime}_{n^{\prime}-1},d,d^{\prime }_{n^{\prime}+1},\ldots,d^{\prime}_{N^{\prime}}}\) in the entry indexed by \(\{d_{k}\in[D_{k}]\}_{k\in[N]\setminus\{n\}}\) and \(\{d^{\prime}_{k}\in[D^{\prime}_{k}]\}_{k\in[N^{\prime}]\setminus\{n^{\prime}\}}\).

_Tensor networks_ are prominent computational models for fitting (_i.e._ representing) tensors. More specifically, a tensor network is a weighted graph that describes formation of a (typically high-order)tensor via contractions between (typically low-order) tensors. As customary (_cf._[42]), we will present tensor networks via graphical diagrams to avoid cumbersome notation -- see Figure 1 for details.

### Quantum Entanglement

In quantum physics, the distribution of possible states for a multi-particle ("many body") system is described by a tensor, whose axes are associated with individual particles. A key property of the distribution is the dependence it admits under a given partition of the particles (_i.e._ between a given set of particles and its complement). This dependence is formalized through the notion of _quantum entanglement_, defined using the distribution's description as a tensor -- see Definition 1 below.

Quantum entanglement lies at the heart of a widely accepted theory which allows assessing the ability of a tensor network to fit a given tensor (_cf._[16, 35]). In Section 3 we specialize this theory to a tensor network equivalent to a certain locally connected neural network. The specialized theory will be used (in Section 4) to establish our main theoretical contribution: a necessary and sufficient condition for when the locally connected neural network is capable of accurate prediction over a data distribution.

**Definition 1**.: For a tensor \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) and subset of its axes \(\mathcal{K}\subseteq[N]\), let \(\llbracket\mathcal{A};\mathcal{K}\rrbracket\in\mathbb{R}\Pi_{n\in\mathcal{K} }\cap\mathbb{I}_{n\in\mathcal{K}^{c}}\,D_{n}\) be the arrangement of \(\mathcal{A}\) as a matrix where rows correspond to axes \(\mathcal{K}\) and columns correspond to the remaining axes \(\mathcal{K}^{c}:=[N]\setminus\mathcal{K}\). Denote by \(\sigma_{1}\geq\cdots\geq\sigma_{D_{\mathcal{K}}}\in\mathbb{R}_{\geq 0}\) the singular values of \(\llbracket\mathcal{A};\mathcal{K}\rrbracket\), where \(D_{\mathcal{K}}:=\min\{\prod_{n\in\mathcal{K}}D_{n},\prod_{n\in\mathcal{K}^{ c}}D_{n}\}\). The _quantum entanglement_2 of \(\mathcal{A}\) with respect to the partition \((\mathcal{K},\mathcal{K}^{c})\) is the entropy of the distribution \(\{\rho_{d}:=\sigma_{d}^{2}/\sum_{d^{\prime}=1}^{D_{\mathcal{K}}}\sigma_{d^{ \prime}}^{2}\}_{d=1}^{D_{\mathcal{K}}}\), _i.e._\(QE(\mathcal{A};\mathcal{K}):=-\sum_{d=1}^{D_{\mathcal{K}}}\rho_{d}\ln(\rho_{d})\). By convention, if \(\mathcal{A}=0\) then \(QE(\mathcal{A};\mathcal{K})=0\).

Footnote 2: There exist multiple notions of entanglement in quantum physics (see, _e.g._, [35]). The one we consider is the most common, known as _entanglement entropy_.

## 3 Low Entanglement Under Canonical Partitions Is Necessary and Sufficient for Fitting Tensor

In this section, we prove that a tensor network equivalent to a certain locally connected neural network can fit a tensor if and only if the tensor admits low entanglement under certain canonical partitions of its axes. We begin by introducing the tensor network (Section 3.1). Subsequently, we establish the necessary and sufficient condition required for it to fit a given tensor (Section 3.2). For conciseness, the treatment in this section is limited to one-dimensional (sequential) models; see Appendix J.1 for an extension to arbitrary dimensions.

### Tensor Network Equivalent to a Locally Connected Neural Network

Let \(N\in\mathbb{N}\), and for simplicity suppose that \(N=2^{L}\) for some \(L\in\mathbb{N}\). We consider a tensor network with an underlying perfect binary tree graph of height \(L\), which generates \(\mathcal{W}_{\mathrm{TN}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\). Figure 2(a) provides its diagrammatic definition. For simplicity of presentation, the lengths of axes corresponding to inner (non-open) edges are taken to all be equal to some \(R\in\mathbb{N}\), referred to as the _width_ of the tensor network.

Figure 1: Tensor networks form a graphical language for fitting (_i.e._ representing) tensors through tensor contractions. **Tensor network definition:** Every node in a tensor network is associated with a tensor, whose order is equal to the number of edges emanating from the node. An edge connecting two nodes specifies contraction between the tensors associated with the nodes (Section 2.1), where the weight of the edge signifies the respective axes lengths. Tensor networks may also contain open edges, _i.e._ edges that are connected to a node on one side and are open on the other. The number of such open edges is equal to the order of the tensor produced by contracting the tensor network. **Illustrations:** Presented are exemplar tensor network diagrams of: **(a)** an order \(N\) tensor \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\); **(b)** a vector-matrix multiplication between \(\mathbf{M}\in\mathbb{R}^{D_{1}\times D_{2}}\) and \(\mathbf{v}\in\mathbb{R}^{D_{2}}\), which results in the vector \(\mathbf{M}\mathbf{v}\in\mathbb{R}^{D_{1}}\); and **(c)** a tensor network generating \(\mathcal{W}\in\mathbb{R}^{D_{1}\times D_{2}\times D_{3}}\).

As identified by previous works, the tensor network depicted in Figure 2(a) is equivalent to a certain locally connected neural network (with polynomial non-linearity -- see, _e.g._, [12; 10; 35; 49]). In particular, contracting the tensor network with vectors \(\mathbf{x}^{(1)}\in\mathbb{R}^{D_{1}},\ldots,\mathbf{x}^{(N)}\in\mathbb{R}^{D_{N}}\), as illustrated in Figure 2(b), can be viewed as a forward pass of the data instance \((\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)})\) through a locally connected neural network, whose hidden layers are of width \(R\). This computation results in a scalar equal to \(\left\langle\otimes_{n=1}^{N}\mathbf{x}^{(n)},\mathcal{W}_{\mathrm{TN}}\right\rangle\), where \(\otimes\) stands for the outer product.3 In light of its equivalence to a locally connected neural network, we will refer to the tensor network as a _locally connected tensor network_. We note that for the equivalent neural network to be practical (in terms of memory and runtime), the width \(R\) needs to be of moderate size (typically no more than a few thousands). Specifically, \(R\) cannot be exponential in the order \(N\), meaning \(\ln(R)\) needs to be much smaller than \(N\).

Footnote 3: For any \(\left\{\mathbf{x}^{(n)}\in\mathbb{R}^{D_{n}}\right\}_{n=1}^{N}\), the outer product \(\otimes_{n=1}^{N}\mathbf{x}^{(n)}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) is defined element-wise by \(\left[\otimes_{n=1}^{N}\mathbf{x}^{(n)}\right]_{d_{1},\ldots,d_{N}}=\prod_{n =1}^{N}\mathbf{x}^{(n)}_{d_{n}}\), where \(d_{1}\in[D_{1}],\ldots,d_{N}\in[D_{N}]\).

By virtue of the locally connected tensor network's equivalence to a deep neural network, it has been paramount for the study of expressiveness and generalization in deep learning [12; 9; 10; 13; 14; 54; 34; 35; 3; 30; 31; 36; 47; 48; 49; 50]. Although the equivalent deep neural network (which has polynomial non-linearity) is less common than other neural networks (_e.g._, ones with ReLU non-linearity), it has demonstrated competitive performance in practice [8; 11; 55; 58; 22]. More importantly, its theoretical analyses, through the equivalence to the locally connected tensor network, brought forth numerous insights that were demonstrated empirically and led to development of practical tools for common locally connected architectures. Continuing this line, we will demonstrate our theoretical insights through experiments with widespread convolutional, recurrent and local self-attention architectures (Section 4.3), and employ our theory for deriving an algorithm that enhances the suitability of a data distribution to said architectures (Section 5).

### Necessary and Sufficient Condition for Fitting Tensor

Herein we show that the ability of the locally connected tensor network (defined in Section 3.1) to fit (_i.e._ represent) a given tensor is determined by the entanglements that the tensor admits under the below-defined _canonical partitions_ of \([N]\). Note that each canonical partition comprises a subset of contiguous indices, so low entanglement under canonical partitions can be viewed as a formalization of locality.

**Definition 2**.: The _canonical partitions_ of \([N]=[2^{L}]\) (illustrated in Figure 4 of Appendix B) are:

\[\mathcal{C}_{N}:= \left\{\left(\mathcal{K},\mathcal{K}^{c}\right):\,\mathcal{K}= \left\{2^{L-l}\cdot(n-1)+1,\ldots,2^{L-l}\cdot n\right\},\,l\in\left\{0, \ldots,L\right\},\;n\in\left[2^{l}\right]\right\}.\]

By appealing to known upper bounds on the entanglements that a given tensor network supports [16; 35], we now establish that if the locally connected tensor network can fit a given tensor, that tensor

Figure 2: The analyzed tensor network equivalent to a locally connected neural network. **(a)** We consider a tensor network adhering to a perfect binary tree connectivity with \(N=2^{L}\) leaf nodes, for \(L\in\mathbb{N}\), generating \(\mathcal{W}_{\mathrm{TN}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\). Axes corresponding to open edges are indexed such that open edges descendant to any node of the tree have contiguous indices. The lengths of axes corresponding to inner (non-open) edges are equal to \(R\in\mathbb{N}\), referred to as the _width_ of the tensor network. **(b)** Contracting \(\mathcal{W}_{\mathrm{TN}}\) with vectors \(\mathbf{x}^{(1)}\in\mathbb{R}^{D_{1}},\ldots,\mathbf{x}^{(N)}\in\mathbb{R}^{D _{N}}\) produces \(\left\langle\otimes_{n=1}^{N}\mathbf{x}^{(n)},\mathcal{W}_{\mathrm{TN}}\right\rangle\). Performing these contractions from leaves to root can be viewed as a forward pass of a data instance \((\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)})\) through a certain locally connected neural network (with polynomial non-linearity; see, _e.g._, [12; 10; 35; 49]). Accordingly, we call the tensor network generating \(\mathcal{W}_{\mathrm{TN}}\) a _locally connected tensor network_.

must admit low entanglement under the canonical partitions of its axes. Namely, suppose that \(\mathcal{W}_{\mathrm{TN}}\) -- the tensor generated by the locally connected tensor network -- well-approximates an order \(N\) tensor \(\mathcal{A}\). Then, Theorem 1 below shows that the entanglement of \(\mathcal{A}\) with respect to every canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) cannot be much larger than \(\ln(R)\) (recall that \(R\) is the width of the locally connected tensor network, and that in practical settings \(\ln(R)\) is much smaller than \(N\)), whereas the expected entanglement of a random tensor with respect to \((\mathcal{K},\mathcal{K}^{c})\) is on the order of \(\min\{|\mathcal{K}|,|\mathcal{K}^{c}|\}\) (which is linear in \(N\) for most canonical partitions).

In the other direction, Theorem 2 below implies that low entanglement under the canonical partitions is not only necessary for a tensor to be fit by the locally connected tensor network, but also sufficient.

**Theorem 1**.: _Let \(\mathcal{W}_{\mathrm{TN}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) be a tensor generated by the locally connected tensor network defined in Section 3.1, and let \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\). For any \(\epsilon\in[0,\|\mathcal{A}\|/4]\), if \(\|\mathcal{W}_{\mathrm{TN}}-\mathcal{A}\|\leq\epsilon\), then for all canonical partitions \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) (Definition 2) it holds that \(QE(\mathcal{A};\mathcal{K})\leq\ln(R)+\frac{2\epsilon}{\|\mathcal{A}\|}\cdot \ln(D_{\mathcal{K}})+2\sqrt{\frac{2\epsilon}{\|\mathcal{A}\|}}\), where \(D_{\mathcal{K}}:=\min\{\prod_{n\in\mathcal{K}}D_{n},\prod_{n\in\mathcal{K}^{c} }D_{n}\}\).4 In contrast, a random tensor \(\mathcal{A}^{\prime}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\), drawn according to the uniform distribution over the set of unit norm tensors, satisfies \(\mathbb{E}[QE(\mathcal{A}^{\prime};\mathcal{K})]\geq\min\{|\mathcal{K}|,| \mathcal{K}^{c}|\}\cdot\ln(\min_{n\in[N]}D_{n})+\ln\bigl{(}\frac{1}{2}\bigr{)} -\frac{1}{2}\) for all canonical partitions \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\)._

Footnote 4: If \(\mathcal{A}=0\), then \(\epsilon=0\). In this case, the expression \(\epsilon/\|\mathcal{A}\|\) is by convention equal to zero.

Proof sketch (proof in Appendix M.2).: In general, the entanglements that a tensor network supports can be upper bounded through cuts in its graph [16, 35]. For the locally connected tensor network, these bounds imply that \(QE(\mathcal{W}_{\mathrm{TN}};\mathcal{K})\leq\ln(R)\) for any canonical partition \((\mathcal{K},\mathcal{K}^{c})\). The upper bounds on the entanglements of \(\mathcal{A}\) then follow by showing that if \(\mathcal{W}_{\mathrm{TN}}\) and \(\mathcal{A}\) are close, then so are their entanglements. The lower bounds on the expected entanglements of a random tensor are derived based on a characterization from [52]. 

**Theorem 2**.: _Let \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) and \(\epsilon>0\). Suppose that for all canonical partitions \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) (Definition 2) it holds that \(QE(\mathcal{A};\mathcal{K})\leq\frac{\epsilon^{2}}{(2N-3)\|\mathcal{A}\|^{2}} \cdot\ln(R)\).5 Then, there exists an assignment for the tensors constituting the locally connected tensor network (defined in Section 3.1) such that it generates \(\mathcal{W}_{\mathrm{TN}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) satisfying \(\|\mathcal{W}_{\mathrm{TN}}-\mathcal{A}\|\leq\epsilon\)._

Footnote 5: When the approximation error \(\epsilon\) tends to zero, the sufficient condition in Theorem 2 requires entanglements to approach zero, unlike the necessary condition in Theorem 1 which requires entanglements to become no greater than \(\ln(R)\). This is unavoidable. However, if for all canonical partitions \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) the singular values of \(\llbracket\mathcal{A};\mathcal{K}\rrbracket\) trailing after the \(R\)’th one are small, then we can also guarantee an assignment for the locally connected tensor network satisfying \(\|\mathcal{W}_{\mathrm{TN}}-\mathcal{A}\|\leq\epsilon\), while \(QE(\mathcal{A};\mathcal{K})\) can be on the order of \(\ln(R)\) for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\). See Appendix C for details.

Proof sketch (proof in Appendix M.3).: We show that if \(\mathcal{A}\) has low entanglement under a canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\), then the singular values of \(\llbracket\mathcal{A};\mathcal{K}\rrbracket\) must decay rapidly (recall that \(\llbracket\mathcal{A};\mathcal{K}\rrbracket\) is the arrangement of \(\mathcal{A}\) as a matrix where rows correspond to axes indexed by \(\mathcal{K}\) and columns correspond to the remaining axes). The approximation guarantee is then obtained through a construction from [25], which is based on truncated singular value decompositions of every \(\llbracket\mathcal{A};\mathcal{K}\rrbracket\) for \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\). 

## 4 Low Entanglement Under Canonical Partitions Is Necessary and Sufficient for Accurate Prediction

In this section we consider the locally connected tensor network from Section 3.1 in a machine learning setting. We show that attaining low population loss amounts to fitting a tensor defined by the data distribution, whose axes correspond to features (Section 4.1). Applying the theorems of Section 3.2, we then conclude that the locally connected tensor network is capable of accurate prediction if and only if the data distribution admits low entanglement under canonical partitions of features (Section 4.2). This conclusion is corroborated through experiments, demonstrating that the performance of common locally connected neural networks (including convolutional, recurrent, and local self-attention neural networks) is inversely correlated with the entanglement under canonical partitions of features in the data (Section 4.3). For conciseness, the treatment in this section is limited to one-dimensional (sequential) models and data; see Appendix J.2 for an extension to arbitrary dimensions.

### Accurate Prediction Is Equivalent to Fitting Data Tensor

As discussed in Section 3.1, the locally connected tensor network generating \(\mathcal{W}_{\mathrm{TN}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) is equivalent to a locally connected neural network, whose forward pass over a data instance \((\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)})\) yields \(\left\langle\diamondsuit_{n=1}^{N}\mathbf{x}^{(n)},\mathcal{W}_{\mathrm{TN}}\right\rangle\), where \(\mathbf{x}^{(1)}\in\mathbb{R}^{D_{1}},\ldots,\mathbf{x}^{(N)}\in\mathbb{R}^{D_ {N}}\). Motivated by this fact, we consider a binary classification setting, in which the label \(y\) of the instance \((\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)})\) is either \(1\) or \(-1\), and the prediction \(\hat{y}\) is taken to be the sign of the output of the neural network, _i.e._\(\hat{y}=\mathrm{sign}\big{(}\big{\langle}\diamondsuit_{n=1}^{N}\mathbf{x}^{(n)}, \mathcal{W}_{\mathrm{TN}}\big{\rangle}\big{)}\).

Suppose we are given a training set of labeled instances \(\big{\{}\big{(}(\mathbf{x}^{(1,m)},\ldots,\mathbf{x}^{(N,m)}),y^{(m)}\big{)} \big{\}}_{m=1}^{M}\) drawn i.i.d. from some distribution, and we would like to learn the parameters of the neural network through the soft-margin support vector machine (SVM) objective, _i.e._ by optimizing:

\[\min_{\|\mathcal{W}_{\mathrm{TN}}\|\leq B}\frac{1}{M}\sum\nolimits_{m=1}^{M} \max\Bigl{\{}0,1-y^{(m)}\big{\langle}\diamondsuit_{n=1}^{N}\mathbf{x}^{(n,m)},\mathcal{W}_{\mathrm{TN}}\big{\rangle}\Bigr{\}}\,, \tag{1}\]

for a predetermined constant \(B>0\). We assume instances are normalized, _i.e._ the distribution is such that all vectors constituting an instance have norm no greater than one. We also assume that \(B\leq 1\). In this case \(\bigl{|}y^{(m)}\big{\langle}\diamondsuit_{n=1}^{N}\mathbf{x}^{(n,m)},\mathcal{ W}_{\mathrm{TN}}\big{\rangle}\bigr{|}\leq 1\), so our optimization problem can be expressed as \(\min_{\|\mathcal{W}_{\mathrm{TN}}\|\leq B}1-\left\langle\mathcal{D}_{\mathrm{ emp}},\mathcal{W}_{\mathrm{TN}}\right\rangle\), where

\[\mathcal{D}_{\mathrm{emp}}:=\frac{1}{M}\sum\nolimits_{m=1}^{M}y^{(m)}\cdot \diamondsuit_{n=1}^{N}\mathbf{x}^{(n,m)} \tag{2}\]

is referred to as the _empirical data tensor_. This means that the accuracy over the training data is determined by how large the inner product \(\left\langle\mathcal{D}_{\mathrm{emp}},\mathcal{W}_{\mathrm{TN}}\right\rangle\) is.

Disregarding the degenerate case of \(\mathcal{D}_{\mathrm{emp}}=0\) (in which the optimized objective is constant), the inner products \(\left\langle\mathcal{D}_{\mathrm{emp}},\mathcal{W}_{\mathrm{TN}}\right\rangle\) and \(\left\langle\mathcal{D}_{\mathrm{emp}}/\|\mathcal{D}_{\mathrm{emp}}\|,\mathcal{ W}_{\mathrm{TN}}\right\rangle\) differ by only a multiplicative (positive) constant, so fitting the training data amounts to optimizing \(\max_{\|\mathcal{W}_{\mathrm{TN}}\|\leq B}\left\langle\mathcal{D}_{\mathrm{emp} }/\|\mathcal{D}_{\mathrm{emp}}\|,\mathcal{W}_{\mathrm{TN}}\right\rangle\). If \(\mathcal{W}_{\mathrm{TN}}\) can represent some \(\mathcal{W}\), then it can also represent \(c\cdot\mathcal{W}\) for every \(c\in\mathbb{R}\). Thus, we may equivalently optimize \(\max_{\mathcal{W}_{\mathrm{TN}}}\left\langle\mathcal{D}_{\mathrm{emp}}/\| \mathcal{D}_{\mathrm{emp}}\|,\mathcal{W}_{\mathrm{TN}}/\|\mathcal{W}_{\mathrm{ TN}}\|\right\rangle\) and multiply the result by \(B\). Fitting the training data therefore boils down to minimizing \(\left\|\frac{\mathcal{W}_{\mathrm{TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}-\frac{ \mathcal{D}_{\mathrm{emp}}}{\|\mathcal{D}_{\mathrm{emp}}\|}\right\|\). In other words, the accuracy achievable over the training data is determined by the extent to which \(\frac{\mathcal{W}_{\mathrm{TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}\) can fit the normalized empirical data tensor \(\frac{\mathcal{D}_{\mathrm{emp}}}{\|\mathcal{D}_{\mathrm{emp}}\|}\).

The arguments above are independent of the training set size \(M\), and in fact apply to the population loss as well, in which case \(\mathcal{D}_{\mathrm{emp}}\) is replaced by the _population data tensor_:

\[\mathcal{D}_{\mathrm{pop}}:=\mathbb{E}_{(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{( N)}),y}\bigl{[}y\cdot\diamondsuit_{n=1}^{N}\mathbf{x}^{(n)}\bigr{]}\,. \tag{3}\]

Disregarding the degenerate case of \(\mathcal{D}_{\mathrm{pop}}=0\) (_i.e._ that in which the population loss is constant), it follows that the achievable accuracy over the population is determined by the extent to which \(\frac{\mathcal{W}_{\mathrm{TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}\) can fit the normalized population data tensor \(\frac{\mathcal{D}_{\mathrm{pop}}}{\|\mathcal{D}_{\mathrm{pop}}\|}\). We refer to the minimal distance from it as the _suboptimality in achievable accuracy_.

**Definition 3**.: In the context of the classification setting above, the _suboptimality in achievable accuracy_ is \(\mathrm{SubOpt}:=\min_{\mathcal{W}_{\mathrm{TN}}}\left\|\frac{\mathcal{W}_{ \mathrm{TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}-\frac{\mathcal{D}_{\mathrm{pop}}}{ \|\mathcal{D}_{\mathrm{pop}}\|}\right\|\).

### Necessary and Sufficient Condition for Accurate Prediction

In the classification setting of Section 4.1, by invoking Theorems 1 and 2 from Section 3.2, we conclude that the suboptimality in achievable accuracy is small if and only if the population data tensor \(\mathcal{D}_{\mathrm{pop}}\) admits low entanglement under the canonical partitions of its axes (Definition 2). This is formalized in Corollary 1 below. The quantum entanglement of \(\mathcal{D}_{\mathrm{pop}}\) with respect to an arbitrary partition of its axes \((\mathcal{K},\mathcal{K}^{c})\), where \(\mathcal{K}\subseteq[N]\), is a measure of dependence between the data features indexed by \(\mathcal{K}\) and those indexed by \(\mathcal{K}^{c}\) -- see Appendix D for intuition behind this. Thus, Corollary 1 implies that the suboptimality in achievable accuracy is small if and only if the data admits low dependence under the canonical partitions of features. Since canonical partitions comprise a subset with contiguous indices, we obtain a formalization of the intuition by which data distributions suitable for locally connected neural networks are those exhibiting a "local nature."

Directly evaluating the conditions required by Corollary 1 -- low entanglement under canonical partitions for \(\mathcal{D}_{\mathrm{pop}}\) -- is impractical, since: _(i)_\(\mathcal{D}_{\mathrm{pop}}\) is defined via an unknown data distribution(Equation (3)); and _(ii)_ computing the entanglements involves taking singular value decompositions of matrices with size exponential in the number of input variables \(N\). Fortunately, as Proposition 2 in Appendix E shows, \(\mathcal{D}_{\mathrm{pop}}\) is with high probability well-approximated by the empirical data tensor \(\mathcal{D}_{\mathrm{emp}}\). Moreover, the entanglement of \(\mathcal{D}_{\mathrm{emp}}\) under any partition can be computed efficiently, without explicitly storing or manipulating an exponentially large matrix -- see Appendix F for an algorithm (originally proposed in [40]). Overall, we obtain an efficiently computable criterion (low entanglement under canonical partitions for \(\mathcal{D}_{\mathrm{emp}}\)), that with high probability is both necessary and sufficient for low suboptimality in achievable accuracy (see Corollary 2 in Appendix E for a formalization).

**Corollary 1**.: _Consider the classification setting of Section 4.1, and let \(\epsilon\in[0,1/4]\). If there exists a canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) (Definition 2) under which \(QE(\mathcal{D}_{\mathrm{pop}};\mathcal{K})>\ln(R)+2\epsilon\cdot\ln(D_{ \mathcal{K}})+2\sqrt{2\epsilon}\), where \(R\) is the width of the locally connected tensor network and \(D_{\mathcal{K}}:=\min\{\prod_{n\in\mathcal{K}}D_{n},\prod_{n\in\mathcal{K}^{c} }D_{n}\}\), then \(\mathrm{SubOpt}>\epsilon\). Conversely, if for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) it holds that \(QE(\mathcal{D}_{\mathrm{pop}};\mathcal{K})\leq\frac{\epsilon^{2}}{8N-12}\cdot \ln(R)\), then \(\mathrm{SubOpt}\leq\epsilon\)._

Proof sketch (proof in Appendix M.4).: The result follows from Theorems 1 and 2 after accounting for the normalization of \(\mathcal{W}_{\mathrm{TN}}\) in the definition of \(\mathrm{SubOpt}\) (Definition 3). 

### Empirical Demonstration

Corollary 2 establishes that, with high probability, the locally connected tensor network (from Section 3.1) can achieve high prediction accuracy if and only if the empirical data tensor (Equation (2)) admits low entanglement under canonical partitions of its axes. We corroborate our formal analysis through experiments, demonstrating that its conclusions carry over to common locally connected architectures. Namely, applying convolutional neural networks, S4 (a popular recurrent neural network; see [26]), and a local self-attention model [46] to different datasets, we show that the achieved test accuracy is inversely correlated with the entanglements of the empirical data tensor under canonical partitions. Below is a description of experiments with one-dimensional (_i.e._ sequential) models and data. Additional experiments with two-dimensional (imagery) models and data are given in Appendix J.2.3.

Discerning the relation between entanglements of the empirical data tensor and performance (prediction accuracy) of locally connected neural networks requires datasets admitting different entanglements. A potential way to acquire such datasets is as follows. First, select a dataset on which locally connected neural networks perform well, in the hopes that it admits low entanglement under canonical partitions; natural candidates are datasets comprising images, text or audio. Subsequently, create "shuffled" variants of the dataset by repeatedly swapping the position of two features chosen at random.6 This erodes the original arrangement of features in the data, and is expected to yield higher entanglement under canonical partitions.

Footnote 6: It is known that as the number of random position swaps goes to infinity, the arrangement of the features converges to a random permutation [18].

We followed the blueprint above for a binary classification version of the Speech Commands audio dataset [64]. Figure 3 presents test accuracies achieved by a convolutional neural network, S4, and a local self-attention model, as well as average entanglement under canonical partitions of the empirical data tensor, against the number of random feature swaps performed to create the dataset. As expected, when the number of swaps increases, the average entanglement under canonical partitions becomes higher. At the same time, in accordance with our theory, the prediction accuracies of the locally connected neural networks substantially deteriorate, showing an inverse correlation with the entanglement under canonical partitions.

## 5 Enhancing Suitability of Data to Locally Connected Neural Networks

Our analysis (Sections 3 and 4) suggests that a data distribution is suitable for locally connected neural networks if and only if it admits low entanglement under canonical partitions of features. Motivated by this observation, we derive a preprocessing algorithm aimed to enhance the suitability of a data distribution to locally connected neural networks (Section 5.1 and Appendix G). Empirical evaluations demonstrate that it significantly improves prediction accuracies of common locally connected neural networks on various datasets (Section 5.2). For conciseness, the treatment in this section is limited to one-dimensional (sequential) models and data; see Appendix J.3 for an extension to arbitrary dimensions.

### Search for Feature Arrangement With Low Entanglement Under Canonical Partitions

Our analysis naturally leads to a recipe for enhancing the suitability of a data distribution to locally connected neural networks: given a dataset, search for an arrangement of features which leads to low entanglement under canonical partitions, and then arrange the features accordingly. Formally, suppose we have \(M\in\mathbb{N}\) training instances \(\left\{\left((\mathbf{x}^{(1,m)},\ldots,\mathbf{x}^{(N,m)}),y^{(m)}\right) \right\}_{m=1}^{M}\), where \(y^{(m)}\in\{1,-1\}\) and \(\mathbf{x}^{(n,m)}\in\mathbb{R}^{D}\) for \(n\in[N],m\in[M]\), with \(D\in\mathbb{N}\). Assume without loss of generality that \(N\) is a power of two (if this is not the case we may add constant features as needed). The aforementioned recipe boils down to a search for a permutation \(\pi:[N]\rightarrow[N]\), which when applied to feature indices leads the empirical data tensor \(\mathcal{D}_{\mathrm{emp}}\) (Equation (2)) to admit low entanglement under the canonical partitions of its axes (Definition 2).

A greedy realization of the foregoing search is as follows. Initially, partition the features into two equally sized sets \(\mathcal{K}_{1,1}\subset[N]\) and \(\mathcal{K}_{1,2}:=[N]\setminus\mathcal{K}_{1,1}\) such that the entanglement of \(\mathcal{D}_{\mathrm{emp}}\) with respect to \((\mathcal{K}_{1,1},\mathcal{K}_{1,2})\) is minimal. That is, find \(\mathcal{K}_{1,1}\in\operatorname*{argmin}_{\mathcal{K}_{C}[N],|\mathcal{K}|= N/2}QE(\mathcal{D}_{\mathrm{emp}};\mathcal{K})\). The permutation \(\pi\) will map \(\mathcal{K}_{1,1}\) to coordinates \(\{1,\ldots,\frac{N}{2}\}\) and \(\mathcal{K}_{1,2}\) to \(\{\frac{N}{2}+1,\ldots,N\}\). Then, partition \(\mathcal{K}_{1,1}\) into two equally sized sets \(\mathcal{K}_{2,1}\subset\mathcal{K}_{1,1}\) and \(\mathcal{K}_{2,2}:=\mathcal{K}_{1,1}\setminus\mathcal{K}_{2,1}\) such that the average of entanglements induced by these sets is minimal, _i.e._\(\mathcal{K}_{2,1}\in\operatorname*{argmin}_{\mathcal{K}\subset\mathcal{K}_{1,1},| \mathcal{K}|=\mathcal{K}_{1,1}|/2}\frac{1}{2}\left[QE(\mathcal{D}_{\mathrm{emp }};\mathcal{K})+QE(\mathcal{D}_{\mathrm{emp}};\mathcal{K}_{1,1}\setminus \mathcal{K})\right]\). The permutation \(\pi\) will map \(\mathcal{K}_{2,1}\) to coordinates \(\{1,\ldots,\frac{N}{4}\}\) and \(\mathcal{K}_{2,2}\) to \(\{\frac{N}{4}+1,\ldots,\frac{N}{2}\}\). A partition of \(\mathcal{K}_{1,2}\) into two equally sized sets \(\mathcal{K}_{2,3}\) and \(\mathcal{K}_{2,4}\) is obtained similarly, where \(\pi\) will map \(\mathcal{K}_{2,3}\) to coordinates \(\{\frac{N}{2}+1,\ldots,\frac{3N}{4}\}\) and \(\mathcal{K}_{2,4}\) to \(\{\frac{3N}{4}+1,\ldots,N\}\). Continuing in the same fashion, until we reach subsets \(\mathcal{K}_{L,1},\ldots,\mathcal{K}_{L,N}\) consisting of a single feature index each, fully specifies the permutation \(\pi\).

Unfortunately, the step lying at the heart of the above scheme -- finding a balanced partition that minimizes average entanglement -- is computationally prohibitive, and we are not aware of any tools that alleviate the computational difficulty. However, as discussed in Appendix G, if one replaces entanglement with an appropriate surrogate measure, then each search for a balanced partition minimizing average entanglement converts into a _minimum balanced cut problem_, which enjoys a wide array of established approximation tools [29]. We thus obtain a practical algorithm for enhancing the suitability of a data distribution to locally connected neural networks.

### Experiments

We empirically evaluate our feature rearrangement method, detailed in Appendix G, using common locally connected neural networks -- a convolutional neural network, S4 (popular recurrent neural network; see [26]), and a local self-attention model [46] -- over randomly permuted audio datasets (Section 5.2.1) and several tabular datasets (Section 5.2.2). For brevity, we defer some experiments and implementation details to Appendices K and L. Additional experiments with two-dimensional data are given in Appendix J.3.3.

#### 5.2.1 Randomly Permuted Audio Datasets

Section 4.3 demonstrated that audio data admits low entanglement under canonical partitions of features, and that randomly permuting the position of features leads this entanglement to increase,

Figure 3: The prediction accuracies of common locally connected neural networks are inversely correlated with the entanglements of the data under canonical partitions of features, in compliance with our theory (Sections 4.1 and 4.2). **Left:** Average entanglement under canonical partitions (Definition 2) of the empirical data tensor (Equation (2)), for binary classification variants of the Speech Commands audio dataset [64] obtained by performing random position swaps between features. **Right:** Test accuracies achieved by a convolutional neural network (CNN) [17], S4 (a popular class of recurrent neural networks; see [26]), and a local self-attention model [46], against the number of random feature swaps performed to create the dataset. **All:** Reported are the means and standard deviations of the quantities specified above, taken over ten different random seeds. See Appendix J.2.3 for experiments over (two-dimensional) image data and Appendix L for implementation details.

while substantially degrading the prediction accuracy of locally connected neural networks. A sensible test for our method is to evaluate its ability to recover performance lost due to the random permutation of features.

For the Speech Commands dataset [64], Table 1 compares prediction accuracies of locally connected neural networks on the data: _(i)_ subject to a random permutation of features; _(ii)_ attained after rearranging the randomly permuted features via our method; and _(iii)_ attained after rearranging the randomly permuted features via IGTD [67] -- a heuristic scheme designed for convolutional neural networks (see Appendix A). As can be seen, our method leads to significant improvements, surpassing those brought forth by IGTD. Note that the performance lost due to the random permutation of features is not entirely recovered.7 We believe this relates to phenomena outside the scope of the theory underlying our method (Sections 3 and 4), for example translation invariance in data being beneficial in terms of generalization. Investigation of such phenomena and suitable modification of our method are regarded as promising directions for future work.

Footnote 7: Accuracies on the original data are \(59.8\), \(69.6\) and \(48.1\) for CNN, S4 and Local-Attention, respectively.

The number of features in the audio dataset used in the experiment above is 2048. We demonstrate the scalability of Algorithm 2 by including in Appendix K an experiment over audio data with 50,000 features. In this experiment, instances of the minimum balanced cut problem encountered in Algorithm 2 entail graphs with up to \(25\cdot 10^{8}\) edges. They are solved using the well-known edge sparsification algorithm of [57] that preserves weights of cuts, allowing for configurable compute and memory consumption (the more resources are consumed, the more accurate the solution will be).

#### 5.2.2 Tabular Datasets

The prediction accuracies of locally connected neural networks on tabular data, _i.e._ on data in which features are arranged arbitrarily, is known to be subpar [56]. Tables 2 and 5 report results of experiments with locally connected neural networks over standard tabular benchmarks (namely "semeion", "isolet" and "dna" [60]), demonstrating that arranging features via our method leads to significant improvements in prediction accuracies, surpassing improvements brought forth by IGTD (a heuristic scheme designed for convolutional neural networks [67]). Note that our method does not lead to state of the art prediction accuracies on the evaluated benchmarks.8 However, the results suggest that it renders locally connected neural networks a viable option for tabular data. This option is particularly appealing in when the number of features is large settings, where many alternative approaches (_e.g._ ones involving fully connected neural networks) are impractical.

Footnote 8: XGBoost, _e.g._, achieves prediction accuracies \(91\), \(95.2\) and \(96\) over semeion, isolet and dna, respectively.

## 6 Conclusion

### Summary

The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks -- a prevalent family of deep learning architectures that includes as special cases convolutional neural networks, recurrent neural networks (in particular the recent S4 models) and local self-attention models -- we address this problem by

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Randomly Permuted & Our Method & IGTD \\ \hline CNN & \(5.5\pm 0.5\) & \(\mathbf{18}\pm 1.6\) & \(5.7\pm 0.6\) \\ S4 & \(9.4\pm 0.8\) & \(\mathbf{30.1}\pm 1.8\) & \(12.8\pm 1.3\) \\ Local-Attention & \(7.8\pm 0.5\) & \(\mathbf{12.9}\pm 0.6\) & \(7\pm 0.6\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Arranging features of randomly permuted audio data via our method (detailed in Appendix G) significantly improves the prediction accuracies of locally connected neural networks. Reported are test accuracies (mean and standard deviation over ten random seeds) of a convolutional neural network (CNN), S4 (a popular recurrent neural network; see [26]), and a local self-attention model [46], over the Speech Commands dataset [64] subject to different arrangements of features: _(i)_ a random arrangement; _(ii)_ an arrangement provided by applying our method to the random arrangement; and _(iii)_ an arrangement provided by applying an adaptation of IGTD [67] — a heuristic scheme designed for convolutional neural networks — to the random arrangement. For each model, we highlight (in boldface) the highest mean accuracy if the difference between that and the second-highest mean accuracy is statistically significant (namely, is larger than the standard deviation corresponding to the former). Our method leads to significant improvements in prediction accuracies, surpassing the improvements brought forth by IGTD. See Appendix K for experiments demonstrating its scalability and implementation details.

adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction (_i.e._ can express a solution with low population loss) over a data distribution _if and only if_ the data distribution admits low quantum entanglement under certain canonical partitions of features. Experiments with widespread locally connected neural networks corroborate this finding.

Our theory suggests that the suitability of a data distribution to locally connected neural networks may be enhanced by arranging features such that low entanglement under canonical partitions is attained. Employing a certain surrogate for entanglement, we show that this arrangement can be implemented efficiently, and that it leads to substantial improvements in the prediction accuracies of common locally connected neural networks on various datasets.

### Limitations and Future Work

#### 6.2.1 Neural network architecture

We theoretically analyzed a locally connected neural network with polynomial non-linearity, by employing its equivalence to a tensor network (_cf._ Section 3.1). Accounting for neural networks with connectivities beyond those considered (_e.g._ connectivities that are non-local or ones involving skip connections) is an interesting topic for future work. It requires modification of the equivalent tensor network and corresponding modification of the definition of canonical partitions, similarly to the analysis in Appendix J. Another valuable direction is to account for neural networks with other non-linearities, _e.g._ ReLU. We believe this may be achieved through a generalized notion of tensor networks, successfully used in past work to analyze such architectures [9].

#### 6.2.2 Objective function

The analysis in Section 4 assumes a binary soft-margin SVM objective. Extending it to other objective functions, _e.g._ multi-class SVM, may shed light on the relation between the objective and the requirements for a data distribution to be suitable to neural networks.

#### 6.2.3 Textual data

Our experiments (in Section 4.3 and Appendix J.2.3) show that the necessary and sufficient condition we derived for a data distribution to be suitable to a locally connected neural network -- namely, low quantum entanglement under canonical partitions of features -- is upheld by audio and image datasets. This falls in line with the excellent performance of locally connected neural networks over these data modalities. In contrast, high performant architectures for textual data are typically non-local [61]. Investigating the quantum entanglements that textual data admits and, in particular, under which partitions they are low, may allow designing more efficient architectures with connectivity tailored to textual data.

### Outlook

The data modalities to which deep learning is most commonly applied -- namely ones involving images, text and audio -- are often regarded as natural (as opposed to, for example, tabular data fusing heterogeneous information). We believe the difficulty in explaining the suitability of such modalities to deep learning may be due to a shortage in tools for formally reasoning about natural data. Concepts and tools from physics -- a branch of science concerned with formally reasoning about natural phenomena -- may be key to overcoming said difficulty. We hope that our use of quantum entanglement will encourage further research along this line.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{Dataset: semeion} & \multicolumn{3}{c}{Dataset: isolet} \\ \cline{2-7}  & Baseline & Our Method & IGTD & Baseline & Our Method & IGTD \\ \hline CNN & \(77.7\pm 1.4\) & \(80.0\pm 1.8\) & \(78.9\pm 1.9\) & \(91.0\pm 0.6\) & \(\mathbf{92.5}\pm 0.4\) & \(92.0\pm 0.6\) \\ S4 & \(82.5\pm 1.1\) & \(\mathbf{89.7}\pm 0.5\) & \(86.0\pm 0.7\) & \(92.3\pm 0.4\) & \(\mathbf{93.4}\pm 0.3\) & \(92.7\pm 0.5\) \\ Local-Attention & \(60.9\pm 4.9\) & \(\mathbf{78.0}\pm 1.7\) & \(67.8\pm 2.6\) & \(82.0\pm 1.6\) & \(\mathbf{89.0}\pm 0.6\) & \(85.7\pm 1.9\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Arranging features of tabular datasets via our method (detailed in Appendix G) significantly improves the prediction accuracies of locally connected neural networks. Reported are results of experiments analogous to those of Table 1, but with the “semeion” and “isolet” tabular classification datasets [60]. Since to the arrangement of features in a tabular dataset is intended to be arbitrary, we regard as a baseline the prediction accuracies attained with a random permutation of features. For each combination of dataset and model, we highlight (in boldface) the highest mean accuracy if the difference between that and the second-highest mean accuracy is statistically significant (namely, is larger than the standard deviation corresponding to the former). As in the experiment of Table 1, rearranging the features according to our method leads to significant improvements in prediction accuracies, surpassing the improvements brought forth by IGTD. See Appendix K for experiments with an additional tabular dataset (“dna”) and implementation details.

## Acknowledgments and Disclosure of Funding

This work was supported by a Google Research Scholar Award, a Google Research Gift, the Yandex Initiative in Machine Learning, the Israel Science Foundation (grant 1780/21), the Tel Aviv University Center for AI and Data Science, the Adelis Research Fund for Artificial Intelligence, Len Blavatnik and the Blavatnik Family Foundation, and Amnon and Anat Shashua. NR is supported by the Apple Scholars in AI/ML PhD fellowship.

## References

* [1] Emmanuel Abbe and Colin Sandon. Provable limitations of deep learning. _arXiv preprint arXiv:1812.06369_, 2018.
* [2] Yotam Alexander, Nimrod De La Vega, Noam Razin, and Nadav Cohen. What makes data suitable for a locally connected neural network? a necessary and sufficient condition based on quantum entanglement. _arXiv preprint arXiv:2303.11249_, 2023.
* [3] Emilio Rafael Balda, Arash Behboodi, and Rudolf Mathar. A tensor analysis on dense connectivity via convolutional arithmetic circuits. _Preprint_, 2018.
* [4] Rajendra Bhatia. _Matrix analysis_. Springer-Verlag, 1997.
* [5] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. In _International Conference on Machine Learning (ICML)_, pages 605-614, 2017.
* [6] Alon Brutzkus, Amir Globerson, Eran Malach, Alon Regev Netser, and Shai Shalev-Schwartz. Efficient learning of CNNs using patch based features. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 2336-2356. PMLR, 17-23 Jul 2022. URL [https://proceedings.mlr.press/v162/brutzkus22a.html](https://proceedings.mlr.press/v162/brutzkus22a.html).
* [7] Song Cheng, Jing Chen, and Lei Wang. Information perspective to probabilistic modeling: Boltzmann machines versus born machines. _Entropy_, 20(8):583, 2018.
* [8] Nadav Cohen and Amnon Shashua. Simnets: A generalization of convolutional networks. _Advances in Neural Information Processing Systems (NeurIPS), Deep Learning Workshop_, 2014.
* [9] Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions. _International Conference on Machine Learning (ICML)_, 2016.
* [10] Nadav Cohen and Amnon Shashua. Inductive bias of deep convolutional networks through pooling geometry. _International Conference on Learning Representations (ICLR)_, 2017.
* [11] Nadav Cohen, Or Sharir, and Amnon Shashua. Deep simnets. _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016.
* [12] Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. _Conference On Learning Theory (COLT)_, 2016.
* [13] Nadav Cohen, Or Sharir, Yoav Levine, Ronen Tamari, David Yakira, and Amnon Shashua. Analysis and design of convolutional networks via hierarchical tensor decompositions. _Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) Special Issue on Deep Learning Theory_, 2017.
* [14] Nadav Cohen, Ronen Tamari, and Amnon Shashua. Boosting dilated convolutional networks with mixed tensor decompositions. _International Conference on Learning Representations (ICLR)_, 2018.
* [15] Ian Convy, William Huggins, Haoran Liao, and K Birgitta Whaley. Mutual information scaling for tensor network machine learning. _Machine learning: science and technology_, 3(1):015017, 2022.
* [16] Shawn X Cui, Michael H Freedman, Or Sattah, Richard Stong, and Greg Minton. Quantum max-flow/min-cut. _Journal of Mathematical Physics_, 57(6):062206, 2016.
* [17] Wei Dai, Chia Dai, Shuhui Qu, Juncheng Li, and Samarjit Das. Very deep convolutional neural networks for raw waveforms. In _2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)_, pages 421-425. IEEE, 2017.
* [18] Persi Diaconis and Mehrdad Shashashani. Generating a random permutation with random transpositions. _Zeitschrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete_, 57(2):159-179, 1981.

* [19] Simon S Du and Surbhi Goel. Improved learning of one-hidden-layer convolutional neural networks with overlaps. _arXiv preprint arXiv:1805.07798_, 2018.
* [20] Simon S Du, Jason D Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. In _International Conference on Machine Learning (ICML)_, pages 1339-1348, 2018.
* [21] Anatoly Dymarsky and Kirill Pavlenko. Tensor network to learn the wave function of data. _Physical Review Research_, 4(4):043111, 2022.
* [22] Timo Felser, Marco Trenti, Lorenzo Sestini, Alessio Gianelle, Davide Zuliani, Donatella Lucchesi, and Simone Montangero. Quantum-inspired machine learning on high-energy physics data. _npj Quantum Information_, 7(1):1-8, 2021.
* [23] Michael R Garey and David S Johnson. Computers and intractability. _A Guide to the Theory of NP-Completeness_, 1979.
* [24] Rong Ge, Yunwei Ren, Xiang Wang, and Mo Zhou. Understanding deflation process in over-parametrized tensor decomposition. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [25] Lars Grasedyck. Hierarchical singular value decomposition of tensors. _SIAM journal on matrix analysis and applications_, 31(4):2029-2054, 2010.
* [26] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. _International Conference on Learning Representations (ICLR)_, 2022.
* [27] Siu-Wai Ho and Raymond W Yeung. The interplay between entropy and variational distance. _IEEE Transactions on Information Theory_, 56(12):5906-5929, 2010.
* [28] Zhih-Ahn Jia, Lu Wei, Yu-Chun Wu, Guang-Can Guo, and Guo-Ping Guo. Entanglement area law for shallow and deep quantum neural network states. _New Journal of Physics_, 22(5):053022, 2020.
* [29] George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs. _SIAM Journal on scientific Computing_, 20(1):359-392, 1998.
* [30] Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural networks. _International Conference on Learning Representations (ICLR)_, 2018.
* [31] Valentin Khrulkov, Oleksii Hrinchuk, and Ivan Oseledets. Generalized tensor models for recurrent neural networks. _International Conference on Learning Representations (ICLR)_, 2019.
* [32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [33] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
* [34] Yoav Levine, Or Sharir, and Amnon Shashua. Benefits of depth for long-term memory of recurrent networks. _International Conference on Learning Representations (ICLR) Workshop_, 2018.
* [35] Yoav Levine, David Yakira, Nadav Cohen, and Amnon Shashua. Deep learning and quantum entanglement: Fundamental connections with implications to network design. _International Conference on Learning Representations (ICLR)_, 2018.
* [36] Yoav Levine, Or Sharir, Nadav Cohen, and Amnon Shashua. Quantum entanglement in deep learning architectures. _To appear in Physical Review Letters_, 2019.
* [37] Jingling Li, Yanchao Sun, Jiahao Su, Taiji Suzuki, and Furong Huang. Understanding generalization in deep learning via tensor methods. In _International Conference on Artificial Intelligence and Statistics_, pages 504-515. PMLR, 2020.
* [38] Sirui Lu, Marton Kanasz-Nagy, Ivan Kukuljan, and J Ignacio Cirac. Tensor networks and efficient descriptions of classical data. _arXiv preprint arXiv:2103.06872_, 2021.
* [39] Eran Malach and Shai Shalev-Shwartz. A provably correct algorithm for deep learning that actually works. _arXiv preprint arXiv:1803.09522_, 2018.
* [40] John Martyn, Guifre Vidal, Chase Roberts, and Stefan Leichenauer. Entanglement and tensor networks for supervised image classification. _arXiv preprint arXiv:2007.06082_, 2020.
* [41] Kento Nozawa and Issei Sato. Empirical evaluation and theoretical analysis for representation learning: A survey. _arXiv preprint arXiv:2204.08226_, 2022.

* [42] Roman Orus. A practical introduction to tensor networks: Matrix product states and projected entangled pair states. _Annals of physics_, 349:117-158, 2014.
* [43] Samet Oymak and Mahdi Soltanolkotabi. End-to-end learning of a convolutional neural network via deep tensor decomposition. _arXiv preprint arXiv:1805.06523_, 2018.
* [44] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In _NIPS-W_, 2017.
* [45] Giovanni Puccetti. Measuring linear correlation between random vectors. _Available at SSRN 3116066_, 2022.
* [46] Jack Rae and Ali Razavi. Do transformers need deep long-range memory? In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, Online, July 2020. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.acl-main.672](https://www.aclweb.org/anthology/2020.acl-main.672).
* [47] Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by norms. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [48] Noam Razin, Asaf Maman, and Nadav Cohen. Implicit regularization in tensor factorization. _International Conference on Machine Learning (ICML)_, 2021.
* [49] Noam Razin, Asaf Maman, and Nadav Cohen. Implicit regularization in hierarchical tensor factorization and deep convolutional neural networks. _International Conference on Machine Learning (ICML)_, 2022.
* [50] Noam Razin, Tom Verbin, and Nadav Cohen. On the ability of graph neural networks to model interactions between vertices. _arXiv preprint arXiv:2211.16494_, 2022.
* [51] Lorenzo Rosasco, Mikhail Belkin, and Ernesto De Vito. On learning with integral operators. _J. Mach. Learn. Res._, 11:905-934, mar 2010. ISSN 1532-4435.
* [52] Siddhartha Sen. Average entropy of a quantum subsystem. _Physical review letters_, 77(1):1, 1996.
* [53] Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning. In _International Conference on Machine Learning_, pages 3067-3075. PMLR, 2017.
* [54] Or Sharir and Amnon Shashua. On the expressive power of overlapping architectures of deep learning. _International Conference on Learning Representations (ICLR)_, 2018.
* [55] Or Sharir, Ronen Tamari, Nadav Cohen, and Amnon Shashua. Tensorial mixture models. _arXiv preprint arXiv:1610.04167_, 2016.
* [56] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. _Information Fusion_, 81:84-90, 2022.
* [57] Daniel A Spielman and Shang-Hua Teng. Spectral sparsification of graphs. _SIAM Journal on Computing_, 40(4):981-1025, 2011.
* [58] E Miles Stoudenmire. Learning relevant features of data with multi-scale tensor networks. _Quantum Science and Technology_, 3(3):034003, 2018.
* [59] Edwin Stoudenmire and David J Schwab. Supervised learning with tensor networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2016.
* [60] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. Openml: networked science in machine learning. _SIGKDD Explorations_, 15(2):49-60, 2013. doi: 10.1145/2641190.2641198. URL [http://doi.acm.org/10.1145/2641190.264119](http://doi.acm.org/10.1145/2641190.264119).
* [61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [62] Qingcan Wang et al. Exponential convergence of the deep neural network approximation for analytic functions. _arXiv preprint arXiv:1807.00297_, 2018.
* [63] Xiang Wang, Chenwei Wu, Jason D Lee, Tengyu Ma, and Rong Ge. Beyond lazy training for over-parameterized tensor decomposition. _Advances in Neural Information Processing Systems_, 33:21934-21944, 2020.

* [64] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. _arXiv preprint arXiv:1804.03209_, 2018.
* [65] Dmitry Yarotsky. Error bounds for approximations with deep relu networks. _Neural Networks_, 94:103-114, 2017.
* [66] Ya-Hui Zhang. Entanglement entropy of target functions for image classification and convolutional neural network. _arXiv preprint arXiv:1710.05520_, 2017.
* [67] Yitan Zhu, Thomas Brettin, Fangfang Xia, Alexander Partin, Maulik Shukla, Hyunseung Yoo, Yvonne A Evrard, James H Doroshow, and Rick L Stevens. Converting tabular data into images for deep learning with convolutional neural networks. _Scientific reports_, 11(1):1-11, 2021.

Related Work

Characterizing formal properties of data distributions that make them suitable for deep learning is a major open problem in the field. A number of papers provide sufficient conditions on a data distribution which imply that it is learnable by certain neural networks [5, 65, 39, 19, 20, 43, 62, 6]. However, these sufficient conditions are restrictive, and are not argued to be necessary for any aspect of learning (_i.e._ for expressiveness, optimization or generalization). To the best of our knowledge, this paper is the first to derive a verifiable condition on a data distribution that is both necessary and sufficient for aspects of learning to be achievable by a neural network. We note that the condition we derive resembles past attempts to quantify the structure of data via quantum entanglement and mutual information [40, 15, 38, 7, 66, 28, 21]. However, such quantifications have not been formally related to learnability by neural networks.

The current paper follows a long line of research employing tensor networks as theoretical models for studying deep learning. This line includes works analyzing the expressiveness of different neural network architectures [12, 55, 9, 10, 54, 14, 34, 3, 35, 30, 36, 31, 50], their generalization [37], and the implicit regularization induced by their optimization [47, 48, 49, 63, 24]. Similarly to prior works we focus on expressiveness, yet our approach differs in that we incorporate the data distribution into the analysis and tackle the question of what makes data suitable for deep learning.

The algorithm we propose for enhancing the suitability of data to locally connected neural networks can be considered a form of representation learning. Representation learning is a vast field, far too broad to survey here (for an overview see [41]). Our algorithm, which learns a representation via rearrangement of features in the data, is complementary to most representation learning methods in the literature. A notable method that is also based on feature rearrangement is IGTD [67] -- a heuristic scheme designed for convolutional neural networks. In contrast to IGTD, our algorithm is theoretically grounded. Moreover, we demonstrate empirically in Section 5 that it leads to higher prediction accuracies.

## Appendix B Illustration of Canonical Partitions

## Appendix C Impossibility Result for Improving the Sufficient Condition in Theorem 2

The sufficient condition in Theorem 2 (from Section 3.2) for approximating \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) requires entanglements to approach zero as the desired approximation error \(\epsilon\) does, in contrast to the necessary condition in Theorem 1 where they approach \(\ln(R)\). As Proposition 1 shows, this is unavoidable in the absence of further knowledge regarding \(\mathcal{A}\). However, if for all canonical partitions \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) the singular values of \([\![\mathcal{A};\mathcal{K}]\!]\) trailing after the \(R\)'th one are small, then we can also guarantee an assignment for the locally connected tensor network satisfying \(\|\mathcal{W}_{\mathrm{TN}}-\mathcal{A}\|\leq\epsilon\), while \(QE(\mathcal{A};\mathcal{K})\) can be on the order of \(\ln(R)\) for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\). Indeed, this follows directly from a result in [25], which we restate as Lemma 7 for convenience.

**Proposition 1**.: _Let \(f:\mathbb{R}^{2}\rightarrow\mathbb{R}_{\geq 0}\) be monotonically increasing in its second variable. Suppose that the following statement holds: if a tensor \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) satisfies \(QE(\mathcal{A};\mathcal{K})\leq f(\|\mathcal{A}\|,\epsilon)\) for all canonical partitions \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) (Definition 2), then there exists an assignment for the tensors constituting the locally connected tensor network (defined in Section 3.1) for which

Figure 4: The canonical partitions of \([N]\), for \(N=2^{L}\) with \(L\in\mathbb{N}\). Every \(l\in\{0,\ldots,L\}\) contributes \(2^{l}\) canonical partitions, the \(n\)’th one induced by \(\mathcal{K}=\{2^{L-l}\cdot(n-1)+1,\ldots,2^{L-l}\cdot n\}\).

\(\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) _uplo

**Lemma 1**.: _Consider the classification setting of Section 4.1, and let \(\mathcal{K}\subseteq[N]\). If the features indexed by \(\mathcal{K}\) are independent of those indexed by \(\mathcal{K}^{c}\) (i.e. \((\mathbf{x}^{(n)})_{n\in\mathcal{K}}\) are independent of \((\mathbf{x}^{(n)})_{n\in\mathcal{K}^{c}}\)), conditioned on the label \(y\), then \(QE(\mathcal{D}_{\mathrm{pop}};\mathcal{K})\leq\ln(2)\). Furthermore, if the features are also independent of \(y\), then \(QE(\mathcal{D}_{\mathrm{pop}};\mathcal{K})=0\)._

Proof.: For brevity, denote \(\mathbf{X}:=(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)})\), \(\mathbf{X}_{\mathcal{K}}:=(\mathbf{x}^{(n)})_{n\in\mathcal{K}}\), and \(\mathbf{X}_{\mathcal{K}^{c}}:=(\mathbf{x}^{(n)})_{n\in\mathcal{K}^{c}}\). By the law of total expectation, the entry of \(\mathcal{D}_{\mathrm{pop}}\) corresponding to an index tuple \((i_{1},\ldots,i_{N})\in[D_{1}]\times\cdots\times[D_{N}]\) satisfies:

\[(\mathcal{D}_{\mathrm{pop}})_{i_{1},\ldots,i_{N}} =\mathbb{E}_{\mathbf{X},y}\Big{[}y\cdot\prod\nolimits_{n=1}^{N} \mathbf{x}_{i_{n}}^{(n)}\Big{]}\] \[=\mathbb{P}(y=1)\cdot\mathbb{E}\mathbf{x}\Big{[}\prod\nolimits_{n =1}^{N}\mathbf{x}_{i_{n}}^{(n)}\big{|}y=1\Big{]}-\mathbb{P}(y=-1)\cdot\mathbb{ E}\mathbf{x}\Big{[}\prod\nolimits_{n=1}^{N}\mathbf{x}_{i_{n}}^{(n)}\big{|}y=-1 \Big{]}\,.\]

Thus, if the features indexed by \(\mathcal{K}\) are independent of those indexed by \(\mathcal{K}^{c}\) given \(y\), it holds that:

\[(\mathcal{D}_{\mathrm{pop}})_{i_{1},\ldots,i_{N}}= \mathbb{P}(y=1)\cdot\mathbb{E}\mathbf{x}_{\mathcal{K}}\Big{[}\prod \nolimits_{n\in\mathcal{K}}\mathbf{x}_{i_{n}}^{(n)}\big{|}y=1\Big{]}\cdot \mathbb{E}\mathbf{x}_{\mathcal{K}^{c}}\Big{[}\prod\nolimits_{n\in\mathcal{K}^{ c}}\mathbf{x}_{i_{n}}^{(n)}\big{|}y=1\Big{]} \tag{4}\]

This implies that we can write \(\big{[}\mathcal{D}_{\mathrm{pop}};\mathcal{K}\big{]}\in\mathbb{R}\Pi_{n\in \mathcal{K}}\,D_{n}\times\Pi_{n\in\mathcal{K}^{c}}\,D_{n}\) -- the arrangement of \(\mathcal{D}_{\mathrm{pop}}\) as a matrix whose rows correspond to axes indexed by \(\mathcal{K}\) and columns correspond to the remaining axes -- as a weighted sum of two outer products. Specifically, define \(\mathbf{v}_{\mathcal{K}},\mathbf{u}_{\mathcal{K}}\in\mathbb{R}\Pi_{n\in \mathcal{K}}\,D_{n}\) to be the vectors holding for each possible indexing tuple \((i_{n})_{n\in\mathcal{K}}\) the values \(\mathbb{E}_{\mathbf{X}_{\mathcal{K}}}\big{[}\prod\nolimits_{n\in\mathcal{K}} \mathbf{x}_{i_{n}}^{(n)}\big{|}y=1\big{]}\) and \(\mathbb{E}_{\mathbf{X}_{\mathcal{K}}}\big{[}\prod\nolimits_{n\in\mathcal{K}} \mathbf{x}_{i_{n}}^{(n)}\big{|}y=-1\big{]}\), respectively (with the arrangement of entries in the vectors being consistent with the rows of \(\big{[}\mathcal{D}_{\mathrm{pop}};\mathcal{K}\big{]}\)). Furthermore, let \(\mathbf{v}_{\mathcal{K}^{c}},\mathbf{u}_{\mathcal{K}^{c}}\in\mathbb{R}^{\Pi_{ n\in\mathcal{K}^{c}}\,D_{n}}\) be defined analogously by replacing \(\mathcal{K}\) with \(\mathcal{K}^{c}\) (with the arrangement of entries in the vectors being consistent with the columns of \(\big{[}\mathcal{D}_{\mathrm{pop}};\mathcal{K}\big{]}\)). Then, by Equation (4):

\[\big{[}\mathcal{D}_{\mathrm{pop}};\mathcal{K}\big{]}=\mathbb{P}(y=1)\cdot \mathbf{v}_{\mathcal{K}}\otimes\mathbf{v}_{\mathcal{K}^{c}}-\mathbb{P}(y=-1) \cdot\mathbf{u}_{\mathcal{K}}\otimes\mathbf{u}_{\mathcal{K}^{c}}\,.\]

Since each outer product forms a rank one matrix, the subaddititvity of rank implies that the rank of \(\big{[}\mathcal{D}_{\mathrm{pop}};\mathcal{K}\big{]}\) is at most two. As a result, \(\big{[}\mathcal{D}_{\mathrm{pop}};\mathcal{K}\big{]}\) has at most two non-zero singular values and \(QE(\mathcal{D}_{\mathrm{pop}};\mathcal{K})\leq\ln(2)\) (the entropy of a distribution is at most the natural logarithm of the size of its support).

If, in addition, all features in the data are independent of the label \(y\), then \(\mathbf{v}_{\mathcal{K}}=\mathbf{u}_{\mathcal{K}}\) and \(\mathbf{v}_{\mathcal{K}^{c}}=\mathbf{u}_{\mathcal{K}^{c}}\), meaning \(\big{[}\mathcal{D}_{\mathrm{pop}};\mathcal{K}\big{]}=(\mathbb{P}(y=1)- \mathbb{P}(y=-1))\cdot\mathbf{v}_{\mathcal{K}}\otimes\mathbf{v}_{\mathcal{K}^ {c}}\). In this case, the rank of \(\big{[}\mathcal{D}_{\mathrm{pop}};\mathcal{K}\big{]}\) is at most one, so \(QE(\mathcal{D}_{\mathrm{pop}};\mathcal{K})=0\). 

## Appendix E Efficiently Computable Criterion for Low Suboptimality in Achievable Accuracy

In this appendix, we provide the formal claims deferred from Section 4.2. Specifically, Proposition 2 shows that \(\mathcal{D}_{\mathrm{pop}}/\big{\|}\mathcal{D}_{\mathrm{pop}}\big{\|}\) is, with high probability, well-approximated by the normalized empirical data tensor \(\mathcal{D}_{\mathrm{emp}}/\big{\|}\mathcal{D}_{\mathrm{emp}}\big{\|}\). Corollary 2 establishes the efficiently computable criterion (low entanglement under canonical partitions for \(\mathcal{D}_{\mathrm{emp}}\)), that with high probability is both necessary and sufficient for low suboptimality in achievable accuracy.

**Proposition 2**.: _Consider the classification setting of Section 4.1, and let \(\delta\in(0,1)\) and \(\gamma>0\). If the training set size \(M\) satisfies \(M\geq\frac{2\ln(2)}{\|\mathcal{D}_{\mathrm{pop}}\|^{2}\gamma^{2}}\), then with probability at least \(1-\delta\):_

\[\Big{\|}\frac{\mathcal{D}_{\mathrm{pop}}}{\big{\|}\mathcal{D}_{\mathrm{pop}} \big{\|}}-\frac{\mathcal{D}_{\mathrm{emp}}}{\|\mathcal{D}_{\mathrm{emp}}\|} \Big{\|}\leq\gamma\]

Proof sketch (proof in Appendix 0.A.5).: A standard generalization of the Hoeffding inequality to random vectors in a Hilbert space yields a high probability bound on \(\|\mathcal{D}_{\mathrm{emp}}-\mathcal{D}_{\mathrm{pop}}\|\), which is then translated to a bound on the normalized tensors.

**Corollary 2**.: _Consider the setting and notation of Corollary 1, with \(\epsilon\in(0,1/6]\). For \(\delta\in(0,1)\), suppose that the training set size \(M\) satisfies \(M\geq\frac{8\ln(\frac{2}{3})}{\|\mathcal{D}_{\mathrm{emp}}\|^{2}\epsilon^{2}}\). Then, with probability at least \(1-\delta\) the following hold. First, if there exists a canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) (Definition 2) under which \(QE(\mathcal{D}_{\mathrm{emp}};\mathcal{K})>\ln(R)+3\epsilon\cdot\ln(D_{\mathcal{ K}})+2\sqrt{3\epsilon}\), then:_

\[\mathrm{SubOpt}>\epsilon\,.\]

_Second, if for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) it holds that \(QE(\mathcal{D}_{\mathrm{emp}};\mathcal{K})\leq\frac{\epsilon^{2}}{32N-48}\cdot \ln(R)\), then:_

\[\mathrm{SubOpt}\leq\epsilon\,.\]

_Moreover, the conditions above on the entanglements of \(\mathcal{D}_{\mathrm{emp}}\) can be evaluated efficiently (in \(\mathcal{O}(DNM^{2}+NM^{3})\) time \(\mathcal{O}(DNM+M^{2})\) and memory, where \(D:=\max_{n\in[N]}D_{n}\))._

Proof sketch (proof in Appendix M.6).: Implied by Corollary 1, Proposition 2 with \(\gamma=\frac{\epsilon}{2}\) and Algorithm 1 in Appendix F. 

## Appendix F Efficiently Computing Entanglements of the Empirical Data Tensor

For a given tensor, its entanglement with respect to a partition of axes (Definition 1) is determined by the singular values of its arrangement as a matrix according to the partition. Since the empirical data tensor \(\mathcal{D}_{\mathrm{emp}}\) (Equation (2)) has size exponential in the number of features \(N\), it is infeasible to naively compute its entanglement (or even explicitly store it in memory). Fortunately, as shown in [40], the specific form of the empirical data tensor admits an efficient algorithm for computing entanglements, without explicitly manipulating an exponentially large matrix. Specifically, the algorithm runs in \(\mathcal{O}(DNM^{2}+M^{3})\) time and requires \(\mathcal{O}(DNM+M^{2})\) memory, where \(D:=\max_{n\in[N]}D_{n}\) is the maximal feature dimension (_i.e._ axis length of \(\mathcal{D}_{\mathrm{emp}}\)), \(N\) is the number of features in the data (_i.e._ number of axes that \(\mathcal{D}_{\mathrm{emp}}\) has), and \(M\) is the number of training instances. For completeness, we outline the method in Algorithm 1 while referring the interested reader to Appendix A in [40] for further details.

Practical Algorithm via Surrogate for Entanglement

To efficiently implement the scheme from Section 5.1, we replace entanglement with a surrogate measure of dependence. The surrogate is based on the Pearson correlation coefficient for multivariate features [45],9 and its agreement with entanglement is demonstrated empirically in Appendix I. Theoretically supporting this agreement is left for future work.

Footnote 9: For completeness, Appendix H provides a formal definition of the multivariate Pearson correlation.

**Definition 4**.: Given a set of \(M\in\mathbb{N}\) instances \(\mathcal{X}:=\left\{(\mathbf{x}^{(1,m)},\ldots,\mathbf{x}^{(N,m)})\in(\mathbb{ R}^{D})^{N}\right\}_{m=1}^{M}\), denote by \(p_{n,n^{\prime}}\) the multivariate Pearson correlation between features \(n,n^{\prime}\in[N]\). For \(\mathcal{K}\subseteq[N]\), the _surrogate entanglement_ of \(\mathcal{X}\) with respect to the partition \((\mathcal{K},\mathcal{K}^{c})\), denoted \(SE(\mathcal{X};\mathcal{K})\), is the sum of absolute values of Pearson correlation coefficients between pairs of features, the first belonging to \(\mathcal{K}\) and the second to \(\mathcal{K}^{c}:=[N]\setminus\mathcal{K}\). That is, \(SE(\mathcal{X};\mathcal{K}):=\sum_{n\in\mathcal{K},n^{\prime}\in\mathcal{K}^{ c}}|p_{n,n^{\prime}}|\).

As shown by Proposition 3 below, replacing entanglement with surrogate entanglement in the scheme from Section 5.1 converts each search for a balanced partition minimizing average entanglement into a _minimum balanced cut problem_. Although the minimum balanced cut problem is NP-hard (see, _e.g._, [23]), it enjoys a wide array of well-established approximation tools, particularly ones designed for large scale [29, 57]. We therefore obtain a practical algorithm for enhancing the suitability of a data distribution to locally connected neural networks -- see Algorithm 2.

**Proposition 3**.: _For any \(\bar{\mathcal{K}}\subseteq[N]\) of even size, the following optimization problem can be framed as a minimum balanced cut problem over a complete graph with \(|\bar{\mathcal{K}}|\) vertices:_

\[\min_{\mathcal{K}\subset\mathcal{K},|\mathcal{K}|=|\bar{\mathcal{K}}|/2}\frac{ 1}{2}\Big{[}SE\big{(}\mathcal{X};\mathcal{K}\big{)}+SE\big{(}\mathcal{X};\bar {\mathcal{K}}\setminus\mathcal{K}\big{)}\Big{]}\,. \tag{5}\]

_Specifically, there exists a complete undirected weighted graph with vertices \(\bar{\mathcal{K}}\) and edge weights \(w:\bar{\mathcal{K}}\times\bar{\mathcal{K}}\rightarrow\mathbb{R}\) such that for any \(\mathcal{K}\subset\bar{\mathcal{K}}\), the weight of the cut in the graph induced by \(\bar{\mathcal{K}}\) -- \(\sum_{n\in\mathcal{K},n^{\prime}\in\bar{\mathcal{K}}\setminus\mathcal{K}}w(\{n,n^{\prime}\})\) -- is equal, up to an additive constant, to the term minimized in Equation (5), i.e. to \(\frac{1}{2}\big{[}SE\big{(}\mathcal{X};\mathcal{K}\big{)}+SE\big{(}\mathcal{X };\bar{\mathcal{K}}\setminus\mathcal{K}\big{)}\big{]}\)._

Proof.: Consider the complete undirected graph whose vertices are \(\bar{\mathcal{K}}\) and where the weight of an edge \(\{n,n^{\prime}\}\in\bar{\mathcal{K}}\times\bar{\mathcal{K}}\) is \(w(\{n,n^{\prime}\})=|p_{n,n^{\prime}}|\) (recall that \(p_{n,n^{\prime}}\) stands for the multivariate Pearson correlation between features \(n\) and \(n^{\prime}\) in \(\mathcal{X}\)). For any \(\mathcal{K}\subset\bar{\mathcal{K}}\) it holds that:

\[\sum\nolimits_{n\in\mathcal{K},n^{\prime}\in\bar{\mathcal{K}}\setminus \mathcal{K}}w(\{n,n^{\prime}\})=\frac{1}{2}\Big{[}SE\big{(}\mathcal{X}; \mathcal{K}\big{)}+SE\big{(}\mathcal{X};\bar{\mathcal{K}}\setminus\mathcal{K }\big{)}\Big{]}-\frac{1}{2}SE\big{(}\mathcal{X};\bar{\mathcal{K}}\big{)}\,,\]

where \(\frac{1}{2}SE\big{(}\mathcal{X};\bar{\mathcal{K}}\big{)}\) does not depend on \(\mathcal{K}\). This concludes the proof. 

## Appendix H Definition of the Multivariate Pearson Correlation from [45]

Appendix G introduces a surrogate measure for entanglement based on the multivariate Pearson correlation from [45]. For completeness, this appendix provides its formal definition.

Given a set of \(M\in\mathbb{N}\) instances \(\mathcal{X}:=\left\{(\mathbf{x}^{(1,m)},\ldots,\mathbf{x}^{(N,m)})\in( \mathbb{R}^{D})^{N}\right\}_{m=1}^{M}\), let \(\Sigma^{(n)}\) be the empirical covariance matrix of feature \(n\in[N]\) and \(\Sigma^{(n,n^{\prime})}\) be the empirical cross-covariance matrix of features \(n,n^{\prime}\in[N]\), _i.e._:

\[\Sigma^{(n)} :=\frac{1}{M}\sum\nolimits_{m-1}^{M}\!\left(\mathbf{x}^{(n,m)}- \mu^{(n)}\right)\otimes\left(\mathbf{x}^{(n,m)}-\mu^{(n)}\right),\] \[\Sigma^{(n,n^{\prime})} :=\frac{1}{M}\sum\nolimits_{m-1}^{M}\!\left(\mathbf{x}^{(n,m)}- \mu^{(n)}\right)\otimes\left(\mathbf{x}^{(n^{\prime},m)}-\mu^{(n^{\prime})}

**Algorithm 2** Enhancing Suitability of Data to Locally Connected Neural Networks

## Appendix I Entanglement and Surrogate Entanglement Are Strongly Correlated

In Appendix G, we introduced a surrogate entanglement measure (Definition 4) to facilitate efficient implementation of the feature arrangement search scheme from Section 5.1. Figure 5 supports the viability of the chosen surrogate, demonstrating empirically that it is strongly correlated with entanglement of the empirical data tensor (Definition 1 and Equation (2)).

## Appendix J Extension to Arbitrary Dimensional Models and Data

In this appendix, we extend our theoretical analysis and experiments, including the algorithm for enhancing the suitability of data to locally connected neural networks, from one-dimensional (sequential) models and data to \(P\)-dimensional models and data (such as two-dimensional image data or three-dimensional video data), for \(P\in\mathbb{N}\). Specifically, Appendix J.1 extends Section 3, Appendix J.2 extends Section 4 and Appendix J.3 extends Section 5.

To ease presentation, we consider \(P\)-dimensional data instances whose feature vectors are associated with coordinates \((n_{1},\ldots,n_{P})\in[N]^{P}\), where \(N=2^{L}\) for some \(L\in\mathbb{N}\) (if this is not the case we may add constant features as needed).

Figure 5: Surrogate entanglement (Definition 4) is strongly correlated with the entanglement (Definition 1) of the empirical data tensor. Presented are average entanglement and average surrogate entanglement under canonical partitions, admitted by the Speech Commands audio datasets [64] considered in Figure 3. Remarkably, the Pearson correlation between the quantities is \(0.974\). For further details see caption of Figure 3 as well as Appendix L.

### Low Entanglement Under Canonical Partitions Is Necessary and Sufficient for Fitting Tensor

We introduce the locally connected tensor network equivalent to a locally connected neural network that operates over \(P\)-dimensional data (Appendix J.1.1). Subsequently, we establish a necessary and sufficient condition required for it to fit a given tensor (Appendix J.1.2), generalizing the results of Section 3.2.

#### j.1.1 Tensor Network Equivalent to a Locally Connected Neural Network

For \(P\)-dimensional data, the locally connected tensor network we consider (defined in Section 3.1 for one-dimensional data) has an underlying perfect \(2^{P}\)-ary tree graph of height \(L\). We denote the tensor it generates by \(\mathcal{W}_{\text{TN}}^{P}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}\). Figure 6(a) provides its diagrammatic definition. As in the one-dimensional case, the lengths of axes corresponding to inner edges are taken to be \(R\in\mathbb{N}\), referred to as the width of the tensor network.

The axes of \(\mathcal{W}_{\text{TN}}^{P}\) are associated with \(P\)-dimensional coordinates through a bijective function \(\mu:[N]^{P}\to[N^{P}]\).

**Definition 5**.: We say that a bijective function \(\mu:[N]^{P}\to[N^{P}]\) is _compatible_ with the locally connected tensor network if, for any node in the tensor network, the coordinates mapped to indices of \(\mathcal{W}_{\text{TN}}^{P}\)'s axes descendant to that node form a contiguous \(P\)-dimensional cubic block in \([N]^{P}\) (_e.g._, square block when \(P=2\)) -- see Figure 6(b) for an illustration. With slight abuse of notation, for \(\mathcal{K}\subseteq[N]^{P}\) we denote \(\mu(\mathcal{K}):=\{\mu(n_{1},\ldots,n_{P}):(n_{1},\ldots,n_{P})\in\mathcal{ K}\}\subseteq[N^{P}]\).

Contracting the locally connected tensor network with \(\{\mathbf{x}^{(n_{1},\ldots,n_{P})}\in\mathbb{R}^{D_{\mu(n_{1},\ldots,n_{P})}} \}_{n_{1},\ldots,n_{P}\in[N]}\) according to a compatible \(\mu\), as depicted in Figure 6(c), can be viewed as a forward pass of the data instance \(\{\mathbf{x}^{(n_{1},\ldots,n_{P})}\}_{n_{1},\ldots,n_{P}\in[N]}\) through a locally connected neural network (with polynomial non-linearity), which produces the scalar \(\langle\otimes_{n=1}^{N^{P}}\mathbf{x}^{\mu^{-1}(n)},\mathcal{W}_{\text{TN}}^ {P}\rangle\) (see, _e.g._, [12, 10, 35, 49]).

Figure 6: The analyzed tensor network equivalent to a locally connected neural network operating over \(P\)-dimensional data, for \(P=2\). **(a)** The tensor network adheres to a perfect \(2^{P}\)-ary tree connectivity with \(N^{P}\) leaf nodes, where \(N=2^{L}\) for some \(L\in\mathbb{N}\), and generates \(\mathcal{W}_{\text{TN}}^{P}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}\). Axes corresponding to open edges are indexed such that open edges descendant to any node of the tree have contiguous indices. The lengths of axes corresponding to inner (non-open) edges are equal to \(R\in\mathbb{N}\), referred to as the width of the tensor network. **(b)** Exemplar \(\mu:[N]^{P}\to[N^{P}]\) compatible with the locally connected tensor network (Definition 5), mapping \(P\)-dimensional coordinates to axes indices of \(\mathcal{W}_{\text{PR}}^{P}\). **(c)** Contracting \(\mathcal{W}_{\text{TN}}^{P}\) with vectors \(\{\mathbf{x}^{(n_{1},\ldots,n_{P})}\}_{n_{1},\ldots,n_{P}\in[N]}\) according to a compatible \(\mu\) produces \(\langle\otimes_{n=1}^{N^{P}}\mathbf{x}^{\mu^{-1}(n)},\mathcal{W}_{\text{TN}}^ {P}\rangle\). Performing these contractions can be viewed as a forward pass of a certain locally connected neural network (with polynomial non-linearity) over the data instance \(\{\mathbf{x}^{(n_{1},\ldots,n_{P})}\}_{n_{1},\ldots,n_{P}\in[N]}\) (see, _e.g._, [12, 10, 35, 49]).

#### j.1.2 Necessary and Sufficient Condition for Fitting Tensor

The ability of the locally connected tensor network, defined in Appendix J.1.1, to fit (_i.e._ represent) a tensor is determined by the entanglements that the tensor admits under partitions of its axes, induced by the following canonical partitions of \([N]^{P}\).

**Definition 6**.: The _canonical partitions_ of \([N]^{P}\), illustrated in Figure 7 for \(P=2\), are:10

Footnote 10: For sets \(\mathcal{S}_{1},\ldots,\mathcal{S}_{P}\), we denote their Cartesian product by \(\times_{p=1}^{P}\mathcal{S}_{p}\).

\[\mathcal{C}_{N}^{P}:= \Big{\{}(\mathcal{K},\mathcal{K}^{c}):\mathcal{K}=\times_{p=1}^{ P}\big{\{}2^{L-l}\cdot(n_{p}-1)+1,\ldots,2^{L-l}\cdot n_{p}\big{\}},\] \[l\in\big{\{}0,\ldots,L\big{\}},\;n_{1},\ldots,n_{P}\in\big{[}2^{ l}\big{]}\Big{\}}\,.\]

With the definition of canonical partitions for \(P\)-dimensional data in place, Theorem 3 generalizes Theorem 1. In particular, suppose that \(\mathcal{W}_{\text{TN}}^{P}\) -- the tensor generated by the locally connected tensor network -- well-approximates \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}\). Then, given a compatible \(\mu:[N]^{P}\to[N^{P}]\) (Definition 5), Theorem 3 establishes that the entanglement of \(\mathcal{A}\) with respect to \(\big{(}\mu(\mathcal{K}),\mu(\mathcal{K})^{c}\big{)}\), where \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}^{P}\), cannot be much larger than \(\ln(R)\), whereas the expected entanglement attained by a random tensor with respect to \(\big{(}\mu(\mathcal{K}),\mu(\mathcal{K})^{c}\big{)}\) is on the order of \(\min\{|\mathcal{K}|,|\mathcal{K}^{c}|\}\) (which is linear in \(N^{P}\) for some canonical partitions).

In the other direction, Theorem 4 implies that low entanglement under partitions of axes induced by canonical partitions of \([N]^{P}\) is not only necessary for a tensor to be fit by the locally connected tensor network, but also sufficient.

**Theorem 3**.: _Let \(\mathcal{W}_{\text{TN}}^{P}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}\) be a tensor generated by the locally connected tensor network defined in Appendix J.1.1, and \(\mu:[N]^{P}\to[N^{P}]\) be a compatible map from \(P\)-dimensional coordinates to axes indices of \(\mathcal{W}_{\text{TN}}^{P}\) (Definition 5). For any \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}\) and \(\epsilon\in[0,\|\mathcal{A}\|/4]\), if \(\|\mathcal{W}_{\text{TN}}^{P}-\mathcal{A}\|\leq\epsilon\), then for all canonical partitions \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}^{P}\) (Definition 6):_

\[QE(\mathcal{A};\mu(\mathcal{K}))\leq\ln(R)+\frac{2\epsilon}{\|\mathcal{A}\|} \cdot\ln(D_{\mu(\mathcal{K})})+2\sqrt{\frac{2\epsilon}{\|\mathcal{A}\|}}\,, \tag{6}\]_where \(D_{\mu(\mathcal{K})}:=\min(\prod_{n\in\mu(\mathcal{K})}D_{n},\prod_{n\in\mu( \mathcal{K})^{c}}D_{n})\). In contrast, a random \(\mathcal{A}^{\prime}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}\), drawn according to the uniform distribution over the set of unit norm tensors, satisfies for all canonical partitions \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}^{P}_{N}\):_

\[\mathbb{E}[QE(\mathcal{A}^{\prime};\mu(\mathcal{K}))]\geq\min\{|\mathcal{K}|, |\mathcal{K}^{c}|\}\cdot\ln\bigl{(}\min_{n\in[N^{P}]}D_{n}\bigr{)}+\ln\biggl{(} \frac{1}{2}\biggr{)}-\frac{1}{2}\,. \tag{7}\]

Proof sketch (proof in Appendix M.7).: The proof is analogous to that of Theorem 1. 

**Theorem 4**.: _Let \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}\) and \(\epsilon>0\). Suppose that for all canonical partitions \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}^{P}_{N}\) (Definition 6) it holds that \(QE(\mathcal{A};\mu(\mathcal{K}))\leq\frac{\epsilon^{2}}{(2N^{P}-3)|\mathcal{A} |^{2}}\cdot\ln(R)\), where \(\mu:[N]^{P}\to[N^{P}]\) is compatible with the locally connected tensor network (Definition 5). Then, there exists an assignment for the tensors constituting the locally connected tensor network (defined in Appendix J.1.1) such that it generates \(\mathcal{W}^{P}_{\text{TN}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}\) satisfying:_

\[\|\mathcal{W}^{P}_{\text{TN}}-\mathcal{A}\|\leq\epsilon\,.\]

Proof sketch (proof in Appendix M.8).: The claim follows through a reduction from the locally connected tensor network for \(P\)-dimensional data to that for one-dimensional data (defined in Section 3.1), _i.e._ from perfect \(2^{P}\)-ary to perfect binary tree tensor networks. Specifically, we consider a modified locally connected tensor network for one-dimensional data, where axes corresponding to different inner edges can vary in length (as opposed to all having length \(R\)). We then show, by arguments analogous to those in the proof of Theorem 2, that it can approximate \(\mathcal{A}\) while having certain inner axes, related to the canonical partitions of \([N]^{P}\), of lengths at most \(R\). The proof concludes by establishing that, any tensor represented by such a locally connected tensor network for one-dimensional data can be represented via the locally connected tensor network for \(P\)-dimensional data (where the length of each axis corresponding to an inner edge is \(R\)). 

### Low Entanglement Under Canonical Partitions Is Necessary and Sufficient for Accurate Prediction

In this appendix, we consider the locally connected tensor network from Appendix J.1.1 in a machine learning setting, and extend the results and experiments of Section 4 from one-dimensional to \(P\)-dimensional models and data.

#### j.2.1 Accurate Prediction Is Equivalent to Fitting Data Tensor

The locally connected tensor network generating \(\mathcal{W}^{P}_{\text{TN}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}\) is equivalent to a locally connected neural network operating over \(P\)-dimensional data (see Appendix J.1.1). Specifically, a forward pass of the latter over \(\{\mathbf{x}^{(n_{1},\ldots,n_{P})}\in\mathbb{R}^{D_{\mu(n_{1},\ldots,n_{P})} }\}_{n_{1},\ldots,n_{P}\in[N]}\) yields \(\langle\otimes_{n=1}^{N^{P}}\mathbf{x}^{\mu^{-1}(n)},\mathcal{W}^{P}_{\text{ TN}}\rangle\), for a compatible \(\mu:[N]^{P}\to[N^{P}]\) (Definition 5). Suppose we are given a training set of labeled instances \(\bigl{\{}\{\mathbf{x}^{((n_{1},\ldots,n_{P}),m)}\}_{n_{1},\ldots,n_{P}\in[N]}, y^{(m)}\bigr{\}}_{m=1}^{M}\) drawn i.i.d. from some distribution, where \(y^{(m)}\in\{1,-1\}\) for \(m\in[M]\). Learning the parameters of the neural network through the soft-margin support vector machine (SVM) objective amounts to optimizing:

\[\min_{\|\mathcal{W}^{P}_{\text{TN}}\|\leq B}\frac{1}{M}\sum\nolimits_{m=1}^{M }\max\Bigl{\{}0,1-y^{(m)}\bigl{\langle}\otimes_{n=1}^{N^{P}}\mathbf{x}^{(\mu^ {-1}(n),m)},\mathcal{W}^{P}_{\text{TN}}\bigr{\rangle}\Bigr{\}}\,,\]

for a predetermined constant \(B>0\). This objective generalizes Equation (1) from one-dimensional to \(P\)-dimensional model and data. Assume that instances are normalized, _i.e._\(\|\mathbf{x}^{((n_{1},\ldots,n_{P}),m)}\|\leq 1\) for all \(n_{1},\ldots,n_{P}\in[N],m\in[M]\), and that \(B\leq 1\). By a derivation analogous to that succeeding Equation (1) in Section 4.1, if follows that minimizing the SVM objective is equivalent to minimizing \(\bigl{\|}\frac{\mathcal{W}^{P}_{\text{TN}}}{\|\mathcal{W}^{P}_{\text{TN}}\|}- \frac{\mathcal{D}^{P}_{\text{TN}}}{\|\mathcal{D}^{P}_{\text{TN}}\|}\bigr{\|}\), where:

\[\mathcal{D}^{P}_{\text{emp}}:=\frac{1}{M}\sum\nolimits_{m=1}^{M}y^{(m)}\cdot \otimes_{n=1}^{N^{P}}\mathbf{x}^{(\mu^{-1}(n),m)} \tag{8}\]

extends the notion of _empirical data tensor_ to \(P\)-dimensional data. In other words, the accuracy achievable over the training data is determined by the extent to which \(\frac{\mathcal{W}^{P}_{\text{emp}}}{\|\mathcal{W}^{P}_{\text{TN}}\|}\) can fit the normalized empirical data tensor \(\frac{\mathcal{D}^{P}_{\text{emp}}}{\|\mathcal{D}^{P}_{\text{emp}}\|}\).

The same arguments apply to the population loss, in which case \(\mathcal{D}^{P}_{\text{emp}}\) is replaced by the _population data tensor_:

\[\mathcal{D}^{P}_{\text{pop}}:=\mathbb{E}_{\{\mathbf{x}^{(n_{1},\ldots,n_{P})}\}_ {n_{1},\ldots,n_{P}\in[N],y}}\Big{[}y\cdot\otimes_{n=1}^{N^{P}}\mathbf{x}^{\mu ^{-1}(n)}\Big{]}\,. \tag{9}\]

The achievable accuracy over the population is therefore determined by the extent to which \(\frac{\mathcal{W}^{P}_{\text{IN}}}{\|\mathcal{W}^{P}_{\text{IN}}\|}\) can fit the normalized population data tensor \(\frac{\mathcal{D}^{P}_{\text{emp}}}{\|\mathcal{D}^{P}_{\text{emp}}\|}\). Accordingly, we refer to the minimal distance from it as the _supobitmality in achievable accuracy_, generalizing Definition 3 from Section 4.1.

**Definition 7**.: In the context of the classification setting above, the _suboptimality in achievable accuracy_ is:

\[\mathrm{SubOpt}^{P}:=\min_{\mathcal{W}^{P}_{\text{IN}}}\left\|\frac{\mathcal{W }^{P}_{\text{IN}}}{\|\mathcal{W}^{P}_{\text{IN}}\|}-\frac{\mathcal{D}^{P}_{ \text{pop}}}{\|\mathcal{D}^{P}_{\text{pop}}\|}\right\|.\]

#### j.2.2 Necessary and Sufficient Condition for Accurate Prediction

In the classification setting of Appendix J.2.1, by invoking Theorems 3 and 4 from Appendix J.1.2, we conclude that the suboptimality in achievable accuracy is small if and only if the population (empirical) data tensor \(\mathcal{D}^{P}_{\text{pop}}\) (\(\mathcal{D}^{P}_{\text{emp}}\)) admits low entanglement under the canonical partitions of features (Definition 6). Specifically, we establish Corollary 3, Proposition 4 and Corollary 4, which generalize Corollary 1, Proposition 2 and Corollary 2 from Section 4.2, respectively, to \(P\)-dimensional model and data.

**Corollary 3**.: _Consider the classification setting of Appendix J.2.1, and let \(\epsilon\in[0,1/4]\). If there exists a canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}^{P}_{N}\) (Definition 6) under which \(QE\big{(}\mathcal{D}^{P}_{pop};\mu(\mathcal{K})\big{)}>\ln(R)+2\epsilon\cdot\ln (D_{\mu(\mathcal{K})})+2\sqrt{2\epsilon}\), where \(D_{\mu(\mathcal{K})}:=\min\{\prod_{n\in\mu(\mathcal{K})}D_{n},\prod_{n\in\mu( \mathcal{K})^{c}}D_{n}\}\), then:_

\[\mathrm{SubOpt}^{P}>\epsilon\,.\]

_Conversely, if for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}^{P}_{N}\) it holds that \(QE\big{(}\mathcal{D}^{P}_{\text{pop}};\mu(\mathcal{K})\big{)}\leq\frac{ \epsilon^{2}}{8N^{P}-12}\cdot\ln(R)\), then:_

\[\mathrm{SubOpt}^{P}\leq\epsilon\,.\]

Proof.: Implied by Theorems 3 and 4 after accounting for \(\mathcal{W}^{P}_{\text{TN}}\) being normalized in the suboptimality in achievable accuracy, as done in the proof of Corollary 1. 

**Proposition 4**.: _Consider the classification setting of Appendix J.2.1, and let \(\delta\in(0,1)\) and \(\gamma>0\). If the training set size \(M\) satisfies \(M\geq\frac{2\ln(\frac{3}{2})}{\|\mathcal{D}^{P}_{\text{emp}}\|^{2}\epsilon^{2}}\), then with probability at least \(1-\delta\):_

\[\left\|\frac{\mathcal{D}^{P}_{\text{pop}}}{\|\mathcal{D}^{P}_{\text{pop}}\|}- \frac{\mathcal{D}^{P}_{\text{emp}}}{\|\mathcal{D}^{P}_{\text{emp}}\|}\right\|\leq\gamma\]

Proof.: The claim is established by following steps identical to those in the proof of Proposition 2. 

**Corollary 4**.: _Consider the setting and notation of Corollary 3, with \(\epsilon\in(0,\frac{1}{6}]\). For \(\delta\in(0,1)\), suppose that the training set size \(M\) satisfies \(M\geq\frac{8\ln(\frac{3}{2})}{\|\mathcal{D}^{P}_{\text{emp}}\|^{2}\epsilon^{2}}\). Then, with probability at least \(1-\delta\) the following hold. First, if there exists a canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}^{P}_{N}\) (Definition 6) under which \(QE\big{(}\mathcal{D}^{P}_{\text{emp}};\mu(\mathcal{K})\big{)}>\ln(R)+3\epsilon \cdot\ln(D_{\mu(\mathcal{K})})+2\sqrt{3\epsilon}\), then:_

\[\mathrm{SubOpt}^{P}>\epsilon\,.\]

_Second, if for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}^{P}_{N}\) it holds that \(QE\big{(}\mathcal{D}^{P}_{\text{emp}};\mu(\mathcal{K})\big{)}\leq\frac{ \epsilon^{2}}{32N^{P}-48}\cdot\ln(R)\), then:_

\[\mathrm{SubOpt}^{P}\leq\epsilon\,.\]

_Moreover, the conditions above on the entanglements of \(\mathcal{D}^{P}_{\text{emp}}\) can be evaluated efficiently (in \(\mathcal{O}(DN^{P}M^{2}+N^{P}M^{3})\) time \(\mathcal{O}(DN^{P}M+M^{2})\) and memory, where \(D:=\max_{n\in[N^{P}]}D_{n}\))._

Proof.: Implied from Corollary 3, Proposition 4 with \(\gamma=\frac{\epsilon}{2}\) and Algorithm 1 in Appendix F by following steps identical to those in the proof of Corollary 2.

#### j.2.3 Empirical Demonstration

Figure 8 extends the experiments reported by Figure 3 in Section 4.3 from one-dimensional (sequential) audio data to two-dimensional image data. Specifically, it demonstrates that the prediction accuracies of convolutional neural networks over a variant of CIFAR10 [33] is inversely correlated with the entanglements that the data admits under canonical partitions of features (Definition 6), _i.e._ with the entanglements of the empirical data tensor under partitions of its axes induced by canonical partitions.

### Enhancing Suitability of Data to Locally Connected Neural Networks

We extend the preprocessing algorithm from Section 5, aimed to enhance the suitability of a data distribution to locally connected neural networks, from one-dimensional to \(P\)-dimensional models and data (Appendices J.3.1 and J.3.2). Empirical evaluations demonstrate that it significantly improves prediction accuracy of common architectures (Appendix J.3.3).

#### j.3.1 Search for Feature Arrangement With Low Entanglement Under Canonical Partitions

Suppose we have \(M\in\mathbb{N}\) training instances \(\big{\{}\{\mathbf{x}^{((n_{1},\ldots,n_{P}),m)}\}_{n_{1},\ldots,n_{P}\in[N]}, y^{(m)}\big{\}}_{m=1}^{M}\), where \(\mathbf{x}^{((n_{1},\ldots,n_{P}),m)}\in\mathbb{R}^{D}\) and \(y^{(m)}\in\{1,-1\}\) for \(n_{1},\ldots,n_{P}\in[N],m\in[M]\), with \(D\in\mathbb{N}\). For models intaking \(P\)-dimensional data, the recipe for enhancing the suitability of a data distribution to locally connected neural networks from Section 5.1 boils down to finding a permutation \(\pi:[N]^{P}\to[N]^{P}\), which when applied to feature coordinates leads the empirical data tensor \(\mathcal{D}^{P}_{\text{emp}}\) (Equation (8)) to admit low entanglement under canonical partitions (Definition 6).11

Footnote 11: Enhancing the suitability of a data distribution with instances of dimension different than \(P\) to \(P\)-dimensional models is possible by first arbitrarily mapping features to coordinates in \([N]^{P}\), and then following the scheme for rearranging \(P\)-dimensional data.

A greedy realization, analogous to that outlined in Section 5.1, is as follows. Initially, partition the features into \(2^{P}\) equally sized disjoint sets \(\{\mathcal{K}_{1,(k_{1},\ldots,k_{P})}\subset[N]^{P}\}_{k_{1},\ldots,k_{P}\in [2]}\) such that the average of \(\mathcal{D}^{P}_{\text{emp}}\)'s entanglements induced by these sets is minimal. That is, find an element of:

\[\operatorname*{argmin}_{\begin{subarray}{c}\{\mathcal{K}^{\prime}_{k_{1}, \ldots,k_{P}}\subset[N]^{P}\}_{k_{1},\ldots,k_{P}\in[2]}\\ \text{s.t. }\cup_{k_{1},\ldots,k_{P}\in[2]}\mathcal{K}^{\prime}_{k_{1},\ldots,k_ {P}=[N]^{P}},\\ \forall k_{1},\ldots,k_{P},k^{\prime}_{1},\ldots,k^{\prime}_{P}\in[2]\ | \mathcal{K}^{\prime}_{k_{1},\ldots,k_{P}}|=|\mathcal{K}^{\prime}_{k^{\prime}_{ 1},\ldots,k^{\prime}_{P}}|\end{subarray}}\frac{1}{2^{P}}\sum\nolimits_{k_{1}, \ldots,k_{P}\in[2]}QE\big{(}\mathcal{D}^{P}_{\text{emp}};\mu(\mathcal{K}^{ \prime}_{k_{1},\ldots,k_{P}})\big{)}\,,\] \[\operatorname*{s.t.}\cup_{k_{1},\ldots,k_{P}\in[2]}\mathcal{K}^{ \prime}_{k_{1},\ldots,k_{P}=[N]^{P}}_{k_{1},\ldots,k_{P}\in[2]},\] \[\forall k_{1},\ldots,k_{P},k^{\prime}_{1},\ldots,k^{\prime}_{P} \in[2]\ |\mathcal{K}^{\prime}_{k_{1},\ldots,k_{P}}|=|\mathcal{K}^{\prime} _{k^{\prime}_{1},\ldots,k^{\prime}_{P}}|\]

where \(\mu:[N]^{P}\to[N^{P}]\) is a compatible map from coordinates in \([N]^{P}\) to axes indices (Definition 5). The permutation \(\pi\) will map each \(\mathcal{K}_{1,(k_{1},\ldots,k_{P})}\) to coordinates \(\chi^{P}_{p=1}\{\frac{N}{2}\cdot(k_{p}-1)+1,\ldots,\frac{N}{2}\cdot k_{p}\}\), for \(k_{1},\ldots,k_{P}\in[2]\). Then, partition similarly each of \(\{\mathcal{K}_{1,(k_{1},\ldots,k_{P})}\}_{k_{1},\ldots,k_{P}\in[2]}\) into \(2^{P}\) equally sized disjoint sets. Continuing in the same fashion, until we reach subsets \(\{\mathcal{K}_{L,(k_{1},\ldots,k_{P})}\}_{k_{1},\ldots,k_{P}\in[N]}\) consisting of a single feature coordinate each, fully specifies the permutation \(\pi\).

Figure 8: The prediction accuracies of convolutional neural networks are inversely correlated with the entanglements of image data under canonical partitions of features, in compliance with our theory (Appendices J.1 and J.2). This figure is identical to Figure 3, except that the measurements were carried over a binary classification version of the CIFAR10 image dataset, as opposed to a one-dimensional (sequential) audio dataset, using three different convolutional neural network architectures. For further details see caption of Figure 3 as well as Appendix L.

As in the case of one-dimensional models and data (Section 5.1), the step lying at the heart of the above scheme -- finding a balanced partition into \(2^{P}\) sets that minimizes average entanglement -- is computationally prohibitive. In the next supappendix we will see that replacing entanglement with surrogate entanglement brings forth a practical implementation.

#### j.3.2 Practical Algorithm via Surrogate for Entanglement

To efficiently implement the scheme from Appendix J.3.1, we replace entanglement with surrogate entanglement (Definition 4), which for \(P\)-dimensional data is straightforwardly defined as follows.

**Definition 8**.: Given a set of \(M\in\mathbb{N}\) instances \(\mathcal{X}:=\big{\{}\{\mathbf{x}^{((n_{1},\ldots,n_{P}),m)}\in\mathbb{R}^{D} \}_{n_{1},\ldots,n_{P}\in[N]}\big{\}}_{m=1}^{M}\), denote by \(p_{(n_{1},\ldots,n_{P}),(n^{\prime}_{1},\ldots,n^{\prime}_{P})}\) the multivariate Pearson correlation between features \((n_{1},\ldots,n_{P})\in[N]^{P}\) and \((n^{\prime}_{1},\ldots,n^{\prime}_{P})\in[N]^{P}\). For \(\mathcal{K}\subseteq[N]^{P}\), the _surrogate entanglement_ of \(\mathcal{X}\) with respect to the partition \((\mathcal{K},\mathcal{K}^{c})\), denoted \(SE(\mathcal{X};\mathcal{K})\), is the sum of Pearson correlation coefficients between pairs of features, the first belonging to \(\mathcal{K}\) and the second to \(\mathcal{K}^{c}:=[N]^{P}\setminus\mathcal{K}\). That is:

\[SE\big{(}\mathcal{X};\mathcal{K}\big{)}=\sum\nolimits_{(n_{1},\ldots,n_{P}) \in\mathcal{K},(n^{\prime}_{1},\ldots,n^{\prime}_{P})\in\mathcal{K}^{c}}P_{(n _{1},\ldots,n_{P}),(n^{\prime}_{1},\ldots,n^{\prime}_{P})}\,.\]

Analogously to the case of one-dimensional data (Appendix G), Proposition 5 shows that replacing the entanglement with surrogate entanglement in the scheme from Appendix J.3.1 converts each search for a balanced partition minimizing average entanglement into a _minimum balanced \(2^{P}\)-cut problem_ (see, _e.g._, [23]), similarly to the minimum balanced (2-) cut problem, it enjoys a wide array of well-established approximation implementations, particularly ones designed for large scale [29, 57]. We therefore obtain a practical algorithm for enhancing the suitability of a data distribution with \(P\)-dimensional instances to locally connected neural networks -- see Algorithm 3.

**Proposition 5**.: _For any \(\bar{\mathcal{K}}\subseteq[N]^{P}\) of size divisible by \(2^{P}\), the following optimization problem can be framed as a minimum balanced \(2^{P}\)-cut problem over a complete graph with \(|\bar{\mathcal{K}}|\) vertices:_

\[\min_{\begin{subarray}{c}\{\mathcal{K}_{k_{1},\ldots,k_{P}}\subset\bar{ \mathcal{K}}\}_{k_{1},\ldots,k_{P}\in[2]}\\ s.t.\;\cup_{k_{1},\ldots,k_{P}\in[2]}\mathcal{K}_{k_{1},\ldots,k_{P}=\bar{ \mathcal{K}},}\\ \forall k_{1},\ldots,k_{P},k^{\prime}_{1},\ldots,k_{P}\in[2]\mid\mathcal{K}_{ k_{1},\ldots,k_{P}}\mid=|\mathcal{K}_{k^{\prime}_{1},\ldots,k^{\prime}_{P}}| \end{subarray}}}\frac{1}{2^{P}}\sum\nolimits_{k_{1},\ldots,k_{P}\in[2]}SE( \mathcal{X};\mathcal{K}_{k_{1},\ldots,k_{P}})\,. \tag{10}\]

_Specifically, there exists a complete undirected weighted graph with vertices \(\bar{\mathcal{K}}\) and edge weights \(w:\bar{\mathcal{K}}\times\bar{\mathcal{K}}\to\mathbb{R}\) such that, for any partition of \(\bar{\mathcal{K}}\) into equally sized dis joint sets \(\{\mathcal{K}_{k_{1},\ldots,k_{P}}\}_{k_{1},\ldots,k_{P}\in[2]}\), the weight of the \(2^{P}\)-cut in the graph induced by them -- \(\frac{1}{2}\sum_{k_{1},\ldots,k_{P}\in[2]}\sum_{(n_{1},\ldots,n_{P})\in\mathcal{ K}_{k_{1},\ldots,k_{P}},(n^{\prime}_{1},\ldots,n^{\prime}_{P})\in\bar{\mathcal{K}} \setminus\mathcal{K}_{k_{1},\ldots,k_{P}}}w(\{(n_{1},\ldots,n_{P}),(n^{\prime} _{1},\ldots,n^{\prime}_{P})\})\) -- is equal, up to multiplicative and additive constants, to the term minimized in Equation (5)._

Proof.: Consider the complete undirected graph whose vertices are \(\bar{\mathcal{K}}\) and where the weight of an edge \(\{(n_{1},\ldots,n_{P}),(n^{\prime}_{1},\ldots,n^{\prime}_{P})\}\in\bar{ \mathcal{K}}\times\bar{\mathcal{K}}\) is:

\[w(\{(n_{1},\ldots,n_{P}),(n^{\prime}_{1},\ldots,n^{\prime}_{P})\})=p_{(n_{1}, \ldots,n_{P}),(n^{\prime}_{1},\ldots,n^{\prime}_{P})}\]

(recall that \(p_{(n_{1},\ldots,n_{P}),(n^{\prime}_{1},\ldots,n^{\prime}_{P})}\) stands for the multivariate Pearson correlation between features \((n_{1},\ldots,n_{P})\) and \((n^{\prime}_{1},\ldots,n^{\prime}_{P})\) in \(\mathcal{X}\)). For any partition of \(\bar{\mathcal{K}}\) into equally sized disjoint sets \(\{\bar{\mathcal{K}}_{k_{1},\ldots,k_{P}}\}_{k_{1},\ldots,k_{P}\in[2]}\), it holds that:

\[\frac{1}{2}\sum_{k_{1},\ldots,k_{P}\in[2]}\sum_{(n_{1},\ldots,n_{ P})\in\mathcal{K}_{k_{1},\ldots,k_{P}},(n^{\prime}_{1},\ldots,n^{\prime}_{P}) \in\bar{\mathcal{K}}\setminus\mathcal{K}_{k_{1},\ldots,k_{P}}}w(\{(n_{1}, \ldots,n_{P}),(n^{\prime}_{1},\ldots,n^{\prime}_{P})\})\] \[=\frac{1}{2}\sum_{k_{1},\ldots,k_{P}\in[2]}SE(\mathcal{X}; \mathcal{K}_{k_{1},\ldots,k_{P}})-\frac{1}{2}SE\big{(}\mathcal{X};\bar{ \mathcal{K}}\big{)}\,,\]

where \(\frac{1}{2}SE\big{(}\mathcal{X};\bar{\mathcal{K}}\big{)}\) does not depend on \(\{\mathcal{K}_{k_{1},\ldots,k_{P}}\}_{k_{1},\ldots,k_{P}\in[2]}\). This concludes the proof. 

#### j.3.3 Experiments

We supplement the experiments from Section 5.2 for one-dimensional models and data, by empirically evaluating Algorithm 3 using two-dimensional convolutional neural networks over randomly permuted image data. Specifically, Table 3 presents experiments analogous to those of Table 1 from Section 5.2.1 over the CIFAR10 [33] image dataset. For brevity, we defer some implementation details to Appendix L.

## Appendix K Further Experiments

Table 4 supplements the experiment of Table 1 from Section 5.2.1, demonstrating that Algorithm 2 can be applied to data with a large number of features (_e.g._ with tens of thousands of features). In the experiment, instances of the minimum balanced cut problem solved as part of Algorithm 2 entail graphs with up to \(25\cdot 10^{8}\) edges. They are solved using the well-known edge sparsification algorithm of [57] that preserves weights of cuts, allowing for configurable compute and memory consumption (the more resources are consumed, the more accurate the solution will be). In contrast, this approach is not directly applicable for improving the efficiency of IGTD.

\begin{table}
\begin{tabular}{l c c} \hline \hline  & Randomly Permuted & Our Method \\ \hline CNN & \(15.0\pm 1.6\) & \(\mathbf{65.6}\pm 1.1\) \\ S4 & \(18.2\pm 0.5\) & \(\mathbf{82.2}\pm 0.4\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Our feature rearrangement method (Algorithm 2 in Appendix G) can be efficiently applied to data with a large number of features. Reported are the results of an experiment identical to that of Table 1, but over a version of the Speech Commands [64] dataset in which every instance has 50,000 features. Instances of the minimum balanced cut problem solved as part of Algorithm 2 entail graphs with up to \(25\cdot 10^{8}\) edges. They are solved using the well-known edge sparsification algorithm of [57] that preserves weights of cuts, allowing for configurable compute and memory consumption (the more resources are consumed, the more accurate the solution will be). As can be seen, Algorithm 2 leads to significant improvements in prediction accuracies. See Appendix L for further implementation details.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Randomly Permuted & Algorithm 3 & IGTD \\ \hline CNN & \(35.1\pm 0.5\) & \(\mathbf{38.2}\pm 0.4\) & \(36.2\pm 0.7\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Arranging features of randomly permuted image data via Algorithm 3 significantly improves the prediction accuracies of convolutional neural networks. Reported are the results of experiments analogous to those of Table 1, carried out over the CIFAR10 image dataset (as opposed to an audio dataset) using Algorithm 3. For further details see caption of Table 1 as well as Appendix L.

Table 5 supplements Table 2 from Section 5.2.2 by reporting results of experiments with an additional tabular benchmark -- "dna".

## Appendix L Further Implementation Details

We provide implementation details omitted from our experimental reports (Section 4.3, Section 5, Appendix J.2.3, Appendix J.3.3 and Appendix K). Source code for reproducing our results and figures, based on the PyTorch [44] framework, can be found at [https://github.com/nmd95/data_suitable_lc_nn_code](https://github.com/nmd95/data_suitable_lc_nn_code). All experiments were run on a single Nvidia RTX A6000 GPU.

### Empirical Demonstration of Theoretical Analysis (Figures 3, 5, and 8)

#### l.1.1 Figures 3 and 5

**Dataset**: The SpeechCommands dataset [64] contains raw audio segments of length up to 16000, split into 84843 train and 11005 test segments. We zero-padded all audio segments to have a length of 16000 and resampled them using sinc interpolation (default PyTorch implementation). We allocated 11005 audio segments from the train set for validation, _i.e._ the dataset was split into 73838 train, 11005 validation and 11005 test audio segments, and created a binary one-vs-all classification version of SpeechCommands by taking all audio segments labeled by the class "33" (corresponding to the word "yes"), and sampling an equal amount of segments from the remaining classes (this process was done separately for the train, validation and test sets). The resulting balanced classification dataset had 5610 train, 846 validation and 838 test segments. Lastly, we resampled all audio segments in the dataset from 16000 to 4096 features using sinc interpolation.

**Random feature swaps:** Starting with the original order of features, we created increasingly "shuffled" versions of the dataset by randomly swapping the position of features. For each number of random position swaps \(k\in\{0,250,\dots,4000\}\), we created ten datasets, whose features were subject to \(k\) random position swaps between features, using different random seeds.

**Quantum entanglement measurement**: Each reported value in the plot is the average of entanglements with respect to canonical partitions (Definition 2) corresponding to levels \(l=1,2,3,4,5,6\). We used Algorithm 1 described in Appendix F on two mini-batches of size 500, randomly sampled from the train set. As customary (_cf._[59]), every input feature \(x\) was embedded using the following sine-cosine scheme:

\[\phi(x):=(\sin(\pi\theta x),\cos(\pi\theta x))\in\mathbb{R}^{2}\,,\]

with \(\theta=0.085\).

**Neural network architectures**:

* **CNN**: An adaptation of the M5 architecture from [17], which is designed for audio data. Our implementation is based on [https://github.com/danielajisafe/Audio_WaveForm_Paper_Implementation](https://github.com/danielajisafe/Audio_WaveForm_Paper_Implementation).
* **S4**: Official implementation of [26] with a hidden dimension of 128 and 4 layers.
* **Local-Attention**: Adaptation of the local-attention model from [46], as implemented in [https://github.com/lucidrains/local-attention](https://github.com/lucidrains/local-attention). We use a multi-layer perceptron (MLP) for mapping continuous (raw) inputs to embeddings, which are fed as input the the local-attention model. For classification, we collapse the spatial dimension of the network's output using mean-pooling and pass the result into an MLP classification head. The network had attention dimension 128 (with 2 heads of dimension 64), depth 8, hidden dimension of MLP blocks 341

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \multicolumn{3}{c}{Dataset: dna} \\ \cline{2-4}  & Baseline & Our Method & IGTD \\ \hline CNN & \(82.5\pm 1.7\) & \(\mathbf{91.2}\pm 1.1\) & \(87.4\pm 1.1\) \\ S4 & \(86.4\pm 1.7\) & \(89.1\pm 3.7\) & \(89.9\pm 1.1\) \\ Local-Attention & \(79.2\pm 4.0\) & \(85.7\pm 4.5\) & \(82.7\pm 3.2\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Arranging features of tabular datasets via our method (detailed in Appendix G) significantly improves the prediction accuracies of locally connected neural networks. Reported are results of experiments identical to those of Table 2, but over the “dna” tabular classification dataset [60]. For further details, see caption of Table 2 and Appendix L.

(computed automatically by the library based on the attention dimension) and local-attention window size 10.

**Training and evaluation:** The binary cross-entropy loss was minimized via the Adam optimizer [32] with default \(\beta_{1},\beta_{2}\) coefficients. Batch sizes were chosen to be the largest powers of two that fit in the GPU memory. The duration of optimization was 300 epochs for all models (number of epochs was taken large enough such that the train loss plateaued). After the last training epoch, the model which performed best on the validation set was chosen, and its average test accuracy is reported. Additional optimization hyperparameters are provided in Table 6. We note that for S4 [26], in accordance with its official implementation, we use a cosine annealing learning rate scheduler.

#### i.1.2 Figure 8

**Dataset**: We created one-vs-all binary classification datasets based on CIFAR10 [33] as follows. All images were converted to grayscale using the PyTorch default implementation. Then, we allocated 7469 images from the train set for validation, _i.e._ the dataset was split into 42531 train, 7469 validation and 1000 test images. We took all images labeled by the class "0" (corresponding to images of airplanes), and uniformly sampled an equal amount of images from the remaining classes (this process was done separately for the train, validation and test sets). The resulting balanced classification dataset had 8506 train, 1493 validation and 2001 test images.

**Random feature swaps:** We created increasingly "shuffled" versions of the dataset according to the protocol described in Appendix L.1.1.

**Quantum entanglement measurement**: Each reported value in the plot is the average entanglements with respect to canonical partitions (Definition 6) corresponding to levels \(l=1,2\).

**Neural network architectures**:

* **CNN**: Same architecture used for the experiments of Table 2 (see Appendix L.2.3), but with one-dimensional convolutional layers replaced with two-dimensional convolutional layers. See Table 7 for the exact architectural hyperparameters.
* **VGG**: Adaptation of the VGG16 architecture, as implemented in the PyTorch framework, for binary classification tasks on grayscale images. Specifically, the number of input channels is set to one and the number of output classes is set to two.
* **MobileNet**: Adaptation of the MobileNet V2 architecture, as implemented in the PyTorch framework, for binary classification on grayscale images. Specifically, the number of input channels is set to one and the number of output classes is set to two.

**Training and evaluation**: The binary cross-entropy loss was minimized for 150 epochs via the Adam optimizer [32] with default \(\beta_{1},\beta_{2}\) coefficients (number of epochs was taken large enough such that the train loss plateaued). Batch size was chosen to be the largest power of two that fit in the

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & **Optimizer** & **Learning Rate** & **Weight Decay** & **Batch Size** \\ \hline CNN & Adam & 0.001 & 0.0001 & 128 \\ S4 & AdamW & 0.001 & 0.01 & 64 \\ Local-Attention & Adam & 0.0005 & 0 & 32 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Optimization hyperparameters for the experiments of Figure 3.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Hyperparameter** & **Value** \\ \hline Stride & (3, 3) \\ Kernel size & (3, 3) \\ Pooling window size & (3, 3) \\ Number of blocks & 8 \\ Hidden dimension & 32 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Architectural hyperparameters for the convolutional neural network (referred to as “CNN”) used in the experiments of Figure 8 and Table 3.

GPU memory. After the last training epoch, the model which performed best on the validation set was chosen, and its average test accuracy is reported. Additional optimization hyperparameters are provided in Table 8.

### Enhancing Suitability of Data to Locally Connected Neural Networks (Tables 1, 2, 3, 4, and 5)

#### l.2.1 Feature Rearrangement Algorithms

Algorithm 2, 3 and IGTD [67] were applied to the training set. Subsequently, the learned feature rearrangement was used for both the validation and test data. In instances where three-way cross-validation was used, distinct rearrangements were learned for each separately.

**Algorithm 2 and Algorithm 3**: Approximate solutions to the minimum balanced cut problems were obtained using the METIS graph partitioning algorithm [29], as implemented in [https://github.com/networkx/networkx-metis](https://github.com/networkx/networkx-metis).

**IGTD [67]**: The original IGTD implementation supports rearranging data only into two-dimensional images. We adapted its implementation to support one-dimensional (sequential) data for the experiments of Tables 1 and 2.

#### l.2.2 Randomly Permuted Audio Datasets (Table 1)

**Dataset**: To facilitate efficient experimentation, we downsampled all audio segments in SpeechCommands to have 2048 features. Furthermore, for the train, validation and test sets separately, we used 20% of the audio segments available for each class.

**Neural network architectures**:

* **CNN**: Same architecture used for the experiments of Figure 3 (see Appendix L.1.1).
* **S4**: Same architecture used for the experiments of Figure 3 (see Appendix L.1.1).
* **Local-Attention**: Same architecture used in the experiments of Figure 3, but with a depth 4 network (we reduced the depth to allow for more efficient experimentation).

**Training and evaluation:** The cross-entropy loss was minimized via the Adam optimizer [32] with default \(\beta_{1},\beta_{2}\) coefficients. Batch sizes were chosen to be the largest powers of two that fit in the GPU memory. The duration of optimization was 200, 200 and 450 epochs for the CNN, S4 and Local-Attention models, respectively (number of epochs was taken large enough such that the train loss plateaued). After the last training epoch, the model which performed best on the validation set was chosen, and its average test accuracy is reported. Additional optimization hyperparameters are provided in Table 9. We note that for S4 [26], in accordance with its official implementation, we use a cosine annealing learning rate scheduler.

#### l.2.3 Tabular Datasets (Tables 2 and 5)

**Datasets**: The datasets "dna", "semeion" and "isolet" are all from the OpenML repository [60]. For each dataset we split the samples intro three folds, which were used for evaluation according to a

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & **Optimizer** & **Learning Rate** & **Weight Decay** & **Batch Size** \\ \hline CNN & Adam & 0.001 & 0.0001 & 128 \\ VGG & Adam & 0.0001 & 0.0001 & 64 \\ MobileNet & Adam & 0.001 & 0.0001 & 64 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Optimization hyperparameters for the experiments of Figure 8.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & **Optimizer** & **Learning Rate** & **Weight Decay** & **Batch Size** \\ \hline CNN & Adam & 0.001 & 0.0001 & 128 \\ S4 & AdamW & 0.001 & 0.01 & 64 \\ Local-Attention & Adam & 0.0001 & 0 & 32 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Optimization hyperparameters for the experiments of Table 1.

standard three-way cross-validation protocol. That is, for each of the three folds, we used one third of the data as a test set and the remaining for train and validation. One third of the samples in the remaining folds (not used for testing) were allocated for the validation set.

**Neural network architectures**:

* **CNN**: We used a ResNet adapted for tabular data. It consisted of residual blocks of the following form: \[\text{Block}(\mathbf{x})=\text{dropout}(\mathbf{x}+\text{BN}(\text{maxpool}( \text{ReLU}(\text{conv}(\mathbf{x})))))\,.\] After applying a predetermined amount of residual blocks, a global average pooling and fully connected layers were used to output the prediction. The architectural hyperparameters are specified in Table 10.
* **S4**: Same architecture used for the experiments of Figure 3 (see Appendix L.1.1), but with a hidden dimension of 64.
* **Local-Attention**: Same architecture used for the experiments of Figure 3 (see Appendix L.1.1), but with 4 attention heads of dimension 32 and a local-attention window size of 25.

**Training and evaluation**: The cross-entropy loss was minimized for 300 epochs via the Adam optimizer [32] with default \(\beta_{1},\beta_{2}\) coefficients (number of epochs was taken large enough such that the train loss plateaued). Batch sizes were chosen to be the largest powers of two that fit in the GPU memory. After the last training epoch, the model which performed best according to the validation sets was chosen, and test accuracy was measured on the test set. The reported accuracy is the average over the three folds. Additional optimization hyperparameters are specified in Table 11. We note that for S4 [26], in accordance with its official implementation, we use a cosine annealing learning rate scheduler.

#### l.2.4 Randomly Permuted Image Datasets (Table 3)

**Dataset**: The data acquisition process followed the protocol described in Appendix L.1.2, except that the data was not converted into a binary one-vs-all classification dataset.

**Neural network architectures**:

* **CNN**: Same architecture used for the experiments of Figure 8 (see Appendix L.1.2).

**Training and evaluation**: The cross-entropy loss was minimized for 500 epochs via the Adam optimizer [32] with default \(\beta_{1},\beta_{2}\) coefficients (number of epochs was taken large enough such that the train loss plateaued). Batch size was chosen to be the largest power of two that fit in the GPU memory. After the last training epoch, the model which performed best on the validation set was chosen, and its average test accuracy is reported. Additional optimization hyperparameters are provided in Table 12.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Hyperparameter** & **Value** \\ \hline Stride & 3 \\ Kernel size & 3 \\ Pooling window size & 3 \\ Number of blocks & 8 \\ Hidden dimension & 32 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Architectural hyperparameters for the convolutional neural network used in the experiments of Tables 2 and 4.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & **Optimizer** & **Learning Rate** & **Weight Decay** & **Batch Size** \\ \hline CNN & Adam & 0.001 & 0.0001 & 64 \\ S4 & AdamW & 0.001 & 0.01 & 64 \\ Local-Attention & Adam & 0.00005 & 0 & 64 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Optimization hyperparameters for the experiments of Table 2.

#### l.2.5 Randomly Permuted Audio Datasets With a Large Number of Features (Table 4)

**Dataset**: The data acquisition process followed the protocol described in Appendix L.1.1, except that the data was not transformed into a binary one-vs-all classification dataset and the audio segments were upsampled from 16,000 to 50,000.

**Edge Sparsification**: To facilitate running the METIS graph partitioning algorithm over the minimum balanced cut problems encountered as part of Algorithm 2, we first removed edges from the graph using the spectral sparsification algorithm of [57]. Specifically, we used the official Julia implementation ([https://github.com/danspielman/Laplacians.jl](https://github.com/danspielman/Laplacians.jl)) with hyperparameter \(\epsilon=0.15\).

**Neural network architectures**:

* **CNN**: Same architecture used for the experiments of Table 2 (see Appendix L.2.3).
* **S4**: Same architecture used for the experiments of Table 1 (see Appendix L.1.1), but with a hidden dimension of 32 (we reduced the hidden dimension due to GPU memory considerations).

**Training and evaluation**: The cross-entropy loss was minimized for 200 epochs via the Adam optimizer [32] with default \(\beta_{1},\beta_{2}\) coefficients (number of epochs was taken large enough such that the train loss plateaued). Batch sizes were chosen to be the largest powers of two that fit in the GPU memory. After the last training epoch, the model which performed best on the validation set was chosen, and its average test accuracy is reported. Additional optimization hyperparameters are provided in Table 13. We note that for S4 [26], in accordance with its official implementation, we use a cosine annealing learning rate scheduler.

## Appendix M Deferred Proofs

### Useful Lemmas

Below we collect a few useful results, which we will use in our proofs.

**Lemma 2**.: _We denote the vector of singular values of a matrix \(\mathbf{X}\) (arranged in decreasing order) by \(S(\mathbf{X})\). For any \(\mathbf{A},\mathbf{B}\in\mathbb{R}^{D_{1}\times D_{2}}\) it holds that:_

\[\|S(\mathbf{A})-S(\mathbf{B})\|\leq\|S(\mathbf{A}-\mathbf{B})\|\]

Proof.: See Theorem III.4.4 of [4]. 

**Lemma 3**.: _Let \(\mathcal{P}=\{p_{1},...,p_{N}\},\mathcal{Q}=\{q_{1},...,q_{N}\}\) be two probability distributions supported on \([N]\), and denote by \(TV(\mathcal{P},\mathcal{Q}):=\frac{1}{2}\sum_{n=1}^{N}\lvert p_{n}-q_{n}\rvert\) their total variation distance. If for \(\epsilon\in(0,1/2)\) it holds that \(TV(\mathcal{P},\mathcal{Q})\leq\epsilon\), then:_

\[\lvert H(\mathcal{P})-H(\mathcal{Q})\rvert\leq H_{b}(\epsilon)+\epsilon\cdot \ln(N)\,,\]

_where \(H(\mathcal{P}):=-\sum_{n=1}^{N}p_{n}\ln(p_{n})\) is the entropy of \(\mathcal{P}\), and \(H_{b}(c):=-c\cdot\ln(c)-(1-c)\cdot\ln(1-c)\) is the binary entropy of a Bernoulli distribution parameterized by \(c\in[0,1]\)._

Proof.: See, _e.g._, Theorem 11 of [27].

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & **Optimizer** & **Learning Rate** & **Weight Decay** & **Batch Size** \\ \hline CNN & Adam & 0.001 & 0.0001 & 64 \\ S4 & AdamW & 0.001 & 0.01 & 64 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Optimization hyperparameters for the experiments of Table 4.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & **Optimizer** & **Learning Rate** & **Weight Decay** & **Batch Size** \\ \hline \multirow{2}{*}{CNN} & \multirow{2}{*}{Adam} & 0.001 & \multirow{2}{*}{0.0001} & \multirow{2}{*}{128} \\  & & (multiplied by 0.1 after 300 epochs) & & & \\ \hline \hline \end{tabular}
\end{table}
Table 12: Optimization hyperparameters for the experiments of Table 3.

**Lemma 4** (Hoeffding inequality in Hilbert space).: _Let \(X_{1},..,X_{N}\) be an i.i.d. sequence of random variables whose range is some separable Hilbert space \(\mathcal{H}\). Suppose that \(\mathbb{E}[X_{n}]=0\) and \(\|X_{n}\|\leq c\) for all \(n\in[N]\). Then, for all \(t\geq 0\):_

\[Pr\left(\left\|\frac{1}{n}\sum\nolimits_{n=1}^{N}X_{n}\right\|\geq t\right)\leq 2 \exp\left(-\frac{Nt^{2}}{2c^{2}}\right)\,,\]

_where \(\left\|\cdot\right\|\) refers to the Hilbert space norm._

Proof.: See Section 2.4 of [51]. 

**Lemma 5** (adapted from [35]).: _Let \(G=(V,E)\) be the perfect binary tree graph, with vertices \(V\) and edges \(E\), underlying the locally connected tensor network that generates \(\mathcal{W}_{\mathrm{TN}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) (defined in Section 3.1). For \(\mathcal{K}\subseteq[N]\), let \(V_{\mathcal{K}}\subseteq V\) and \(V_{K^{c}}\subseteq\mathcal{V}\) be the leaves in \(G\) corresponding to axes indices \(\mathcal{K}\) and \(\mathcal{K}^{c}\) of \(\mathcal{W}_{\mathrm{TN}}\), respectively. Lastly, given a cut \((A,B)\) of \(V\), i.e. \(A\subseteq V\) and \(B\subseteq V\) are disjoint and \(A\cup B=V\), denote by \(C(A,B):=\{\{u,v\}\in E:u\in A,v\in B\}\) the edge-cut set. Then:_

\[\mathrm{rank}([\![\mathcal{W}_{\mathrm{TN}};\mathcal{K}]\!])\leq\min\nolimits _{\text{cut}\,(A,B)\text{ of }V\text{ s.t. }V_{\mathcal{K}}\subseteq A,V_{ \mathcal{K}^{c}}\subseteq B}R^{|C(A,B)|}\,.\]

_In particular, if \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\), then:_

\[\mathrm{rank}([\![\mathcal{W}_{\mathrm{TN}};\mathcal{K}]\!])\leq R\,.\]

Proof.: See Claim 1 in [35] for the upper bound on the rank of \([\![\mathcal{W}_{\mathrm{TN}};\mathcal{K}]\!]\) for any \(\mathcal{K}\subseteq[N]\). If \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\), since there exists a cut \((A,B)\) of \(V\) such that \(V_{\mathcal{K}}\subseteq A\) and \(V_{\mathcal{K}^{c}}\subseteq B\) whose edge-cut set is of a singleton, we get that \(\mathrm{rank}([\![\mathcal{W}_{\mathrm{TN}};\mathcal{K}]\!])\leq R\). 

**Lemma 6** (adapted from [35]).: _Let \(G=(V,E)\) be the perfect \(2^{P}\)-ary tree graph, with vertices \(V\) and edges \(E\), underlying the locally connected tensor network that generates \(\mathcal{W}_{\mathrm{TN}}^{P}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}\) (defined in Appendix J.1.1). Furthermore, let \(\mu:[N]^{P}\to[N^{P}]\) be a compatible map from \(P\)-dimensional coordinates to axes indices of \(\mathcal{W}_{\mathrm{TN}}^{P}\) (Definition 5). For \(\mathcal{J}\subseteq[N^{P}]\), let \(V_{\mathcal{J}}\subseteq V\) and \(V_{\mathcal{J}^{c}}\subseteq\mathcal{V}\) be the leaves in \(G\) corresponding to axes indices \(\mathcal{J}\) and \(\mathcal{J}^{c}\) of \(\mathcal{W}_{\mathrm{TN}}^{P}\), respectively. Lastly, given a cut \((A,B)\) of \(V\), i.e. \(A\subseteq V\) and \(B\subseteq V\) are disjoint and \(A\cup B=V\), denote by \(C(A,B):=\{\{u,v\}\in E:u\in A,v\in B\}\) the edge-cut set. Then:_

\[\mathrm{rank}(\big{[}\![\mathcal{W}_{\mathrm{TN}}^{P};\mathcal{J}]\!])\leq \min\nolimits_{\text{cut}\,(A,B)\text{ of }V\text{ s.t. }V_{\mathcal{J}} \subseteq A,V_{\mathcal{J}^{c}}\subseteq B}R^{|C(A,B)|}\,.\]

_In particular, if \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}^{P}\), then:_

\[\mathrm{rank}([\![\mathcal{W}_{\mathrm{TN}};\mu(\mathcal{K})]\!])\leq R\,.\]

Proof.: See Claim 1 in [35] for the upper bound on the rank of \(\big{[}\![\mathcal{W}_{\mathrm{TN}}^{P};\mathcal{J}]\!\big{]}\) for any \(\mathcal{J}\subseteq[N^{P}]\). If \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}^{P}\), since there exists a cut \((A,B)\) of \(V\) such that \(V_{\mu(\mathcal{K})}\subseteq A\) and \(V_{\mu(K)^{c}}\subseteq B\) whose edge-cut set is of a singleton, we get that \(\mathrm{rank}(\big{[}\![\mathcal{W}_{\mathrm{TN}}^{P};\mu(\mathcal{K})]\!])\leq R\). 

**Lemma 7** (adapted from Theorem 3.18 in [25]).: _Let \(\mathcal{A}\in\mathbb{R}^{D_{1}\times\ldots\times D_{N}}\) and \(\epsilon>0\). For each canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\), let \(\sigma_{\mathcal{K},1}\geq\ldots\sigma_{K,D_{\mathcal{K}}}\) be the singular values of \([\![\mathcal{A};\mathcal{K}]\!]\), where \(D_{\mathcal{K}}:=\min\{\prod_{n\in\mathcal{K}}D_{n},\prod_{n\in\mathcal{K} ^{c}}D_{n}\}\). Let \((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N}}\in\mathbb{R}^{\mathcal{C}_{ N}}\) be an assignment of an integer to each \(\mathcal{K}\in\mathcal{C}_{N}\). For any such assignment, consider the set of tensors with \(\mathrm{Hierarchical\ Tucker\ (HT)}\) rank at most \((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N}}\) as follows:_

\[HT\left((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N}}\right):=\left\{ \mathcal{V}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}:\forall\mathcal{K} \in\mathcal{C}_{N},\mathrm{rank}(\![\mathcal{V};\mathcal{K}]\!]\leq n_{ \mathcal{K}}\right\}\,.\]

_Suppose that for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) it holds that \(\sqrt{\sum_{d=n_{\mathcal{K}}+1}^{D_{\mathcal{K}}}\sigma_{\mathcal{K},d}^{2}} \leq\frac{\epsilon}{\sqrt{2N-3}}\). Then, there exists \(\mathcal{W}\in HT\left((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N}}\right)\) satisfying:_

\[\left\|\mathcal{W}-\mathcal{A}\right\|\leq\epsilon\,.\]

_In particular, if for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) it holds that \(\sqrt{\sum_{d=R+1}^{D_{\mathcal{K}}}\sigma_{\mathcal{K},d}^{2}}\leq\frac{ \epsilon}{\sqrt{2N-3}}\), then there exists \(\mathcal{W}_{\mathrm{TN}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) generated by the locally connected tensor network (defined in Section 3.1) satisfying:_

\[\left\|\mathcal{W}_{\mathrm{TN}}-\mathcal{A}\right\|\leq\epsilon\,.\]

**Lemma 8** (adapted from Theorem 3.18 in [25]).: _Let \((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N}}\in\mathbb{N}^{\mathcal{C}_{N}}\) be an assignment of an integer to each \(\mathcal{K}\in\mathcal{C}_{N}\). For any such assignment, consider the set of tensors with Hierarchical Tucker (HT) rank at most \((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N}}\) as follows:_

\[HT\left((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N}}\right):=\left\{ \mathcal{V}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}:\forall\mathcal{K}\in \mathcal{C}_{N},\mathrm{rank}([\mathcal{V};\mathcal{K}])\leq n_{\mathcal{K}} \right\}\,.\]

_Consider a locally connected tensor network with varying widths \((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N}}\), i.e. a tensor network conforming to a perfect binary tree graph in which the lengths of inner axes are as follows (in contrast to all being equal to \(R\) as in the locally connected tensor network defined in Section 3.1). An axis corresponding to an edge that connects a node with descendant leaves indexed by \(\mathcal{K}\) to its parent is assigned the length \(n_{\mathcal{K}}\). Then, every \(\mathcal{A}\in HT\left((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N}}\right)\) can be represented by said locally connected tensor network with varying widths, meaning there exists an assignment to the tensors of the tensor network such that it generates \(\mathcal{A}\). In particular, if \(n_{K}=R\) for all \(\mathcal{K}\in\mathcal{C}_{N}\), then \(\mathcal{A}\) can be generated by the locally connected tensor network with all inner axes being of length \(R\)._

**Lemma 9**.: _Let \(\mathcal{V},\mathcal{W}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) be tensors such that \(\left\|\mathcal{V}\right\|=\left\|\mathcal{W}\right\|=1\) and \(\left\|\mathcal{V}-\mathcal{W}\right\|<\frac{1}{2}\). Then, for any \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) it holds that:_

\[\left|QE(\mathcal{V};\mathcal{K})-QE(\mathcal{W};\mathcal{K})\right|\leq H_{b} (\left\|\mathcal{V}-\mathcal{W}\right\|)+\left\|\mathcal{V}-\mathcal{W}\right\| \cdot\ln(D_{\mathcal{K}})\,,\]

_where \(H_{b}(c):=-(c\cdot\ln(c)+(1-c)\cdot\ln(1-c))\) is the binary entropy of a Bernoulli distribution parameterized by \(c\in[0,1]\), and \(\mathcal{D}_{\mathcal{K}}:=\min\{\prod_{n\in\mathcal{K}}D_{n},\prod_{n\in \mathcal{K}^{c}}D_{n}\}\)._

Proof.: For any matrix \(\mathbf{M}\in\mathbb{R}^{D_{1}\times D_{2}}\) with \(D:=\min\{D_{1},D_{2}\}\), let \(S(\mathbf{M})=(\sigma_{\mathbf{M},1},...,\sigma_{\mathbf{M},D})\) be the vector consisting of its singular values. First note that

\[\left\|\mathcal{V}-\mathcal{W}\right\|=\left\|S(\left[\mathcal{V}-\mathcal{W} ;\mathcal{K}\right])\right\|\leq\left\|\mathcal{V}-\mathcal{W}\right\|.\]

So by Lemma 2

\[\left\|S(\left[\mathcal{V};\mathcal{K}\right]-S(\left[\mathcal{W};\mathcal{K} \right]))\right\|\leq\left\|\mathcal{V}-\mathcal{W}\right\|.\]

Let \(v_{1}\geq\cdots\geq v_{D_{\mathcal{K}}}\) and \(w_{1}\geq\cdots\geq w_{D_{\mathcal{K}}}\) be the singular values of \(\left[\mathcal{V};\mathcal{K}\right]\) and \(\left[\mathcal{W};\mathcal{K}\right]\), respectively. We have by the Cauchy-Schwarz inequality that

\[\left(\sum_{d=1}^{D_{\mathcal{K}}}|w_{d}^{2}-v_{d}^{2}|\right)^{2}=\left(\sum _{d=1}^{D_{\mathcal{K}}}|w_{d}-v_{d}|\cdot|w_{d}+v_{d}|\right)^{2}\leq\left( \sum_{d=1}^{D_{\mathcal{K}}}(w_{d}-v_{d})^{2}\right)\left(\sum_{d=1}^{D_{ \mathcal{K}}}(w_{d}+v_{d})^{2}\right)\,.\]

Now the first term is upper bounded by \(\left\|\mathcal{V}-\mathcal{W}\right\|^{2}\), and for the second we have

\[\sum_{d=1}^{D_{\mathcal{K}}}(w_{d}+v_{d})^{2}=\sum_{d=1}^{D_{\mathcal{K}}}w_{d }^{2}+\sum_{d=1}^{D_{\mathcal{K}}}v_{d}^{2}+2v_{d}w_{d}=2+2\sum_{d=1}^{D_{ \mathcal{K}}}v_{d}w_{d}\leq 4\,,\]

where we use the fact that \(\left\|\mathcal{V}\right\|=\left\|\mathcal{W}\right\|=1\), and again Cuachy-Schwarz. Overall we have:

\[\sum_{d=1}^{D_{\mathcal{K}}}|w_{d}^{2}-v_{d}^{2}|\leq 2\left\|\mathcal{V}- \mathcal{W}\right\|.\]

Note that the left hand side of the inequality above equals twice the total variation distance between the distributions defined by \(\{w_{d}^{2}\}_{d=1}^{D_{\mathcal{K}}}\) and \(\{v_{d}^{2}\}_{d=1}^{D_{\mathcal{K}}}\). Therefore by Lemma 3 we have:

\[\left|QE(\mathcal{V};\mathcal{K})-QE(\mathcal{W};\mathcal{K})\right|=\left|H( \{w_{d}^{2}\})-H(\{v_{d}^{2}\})\right|\leq\left\|\mathcal{V}-\mathcal{W} \right\|\cdot\ln(D_{\mathcal{K}})+H_{b}(\left\|\mathcal{V}-\mathcal{W}\right\| )\,.\]

**Lemma 10**.: _Let \(\mathcal{P}=\{p(x)\}_{x\in[S]}\), where \(S\in\mathbb{N}\), be a probability distribution, and denote its entropy by \(H(\mathcal{P}):=\mathbb{E}_{x\sim\mathcal{P}}[\ln\left(1/p(x)\right)]\). Then, for any \(0<a<1\), there exists a subset \(T\subseteq[S]\) such that \(Pr_{x\sim\mathcal{P}}(T^{c})\leq a\) and \(\left|T\right|\leq e^{\frac{H(\mathcal{P})}{a}}\)._

Proof.: By Markov's inequality we have for any \(0<a<1\):

\[Pr_{x\sim\mathcal{P}}\left(\left\{x:e^{-\frac{H(\mathcal{P})}{a}}\geq p(x) \right\}\right)=Pr_{x\sim\mathcal{P}}\left(\left\{x:\ln\left(\frac{1}{p(x)} \right)\geq\frac{H(\mathcal{P})}{a}\right\}\right)\leq a\,.\]

Let \(T:=\left\{x:e^{-\frac{H(\mathcal{P})}{a}}\leq p(x)\right\}\subseteq[S]\). Note that

\[e^{-\frac{H(\mathcal{P})}{a}}|T|\leq\sum\nolimits_{x\in T}p(x)\leq\sum \nolimits_{x\in[S]}p(x)=1\,,\]

and so \(\left|T\right|\leq e^{\frac{H(\mathcal{P})}{a}}\) and \(Pr_{x\sim\mathcal{P}}(T^{c})\leq a\), as required.

### Proof of Theorem 1

If \(\mathcal{A}=0\) the theorem is trivial, since then \(QE(\mathcal{A};\mathcal{K})=0\) for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\), so we can assume \(\mathcal{A}\neq 0\). We have:

\[\bigg{\|}\frac{\mathcal{W}_{\mathrm{TN}}}{\|\mathcal{W}_{\mathrm{ TN}}\|}-\frac{\mathcal{A}}{\|\mathcal{A}\|} \bigg{\|} =\frac{1}{\|\mathcal{A}\|}\,\bigg{\|}\frac{\|\mathcal{A}\|}{\| \mathcal{W}_{\mathrm{TN}}\|}\cdot\mathcal{W}_{\mathrm{TN}}-\mathcal{A}\bigg{\|}\] \[\leq\frac{1}{\|\mathcal{A}\|}\,\bigg{(}\bigg{\|}\frac{\|\mathcal{ A}\|}{\|\mathcal{W}_{\mathrm{TN}}\|}-1\bigg{|}\cdot\|\mathcal{W}_{\mathrm{TN}}\|+ \|\mathcal{W}_{\mathrm{TN}}-\mathcal{A}\|\bigg{)}\] \[=\frac{1}{\|\mathcal{A}\|}\,(\||\mathcal{A}\|-\|\mathcal{W}_{ \mathrm{TN}}\|+\|\mathcal{W}_{\mathrm{TN}}-\mathcal{A}\|)\] \[\leq\frac{2\epsilon}{\|A\|}\,.\]

Now, let \(\hat{\mathcal{A}}:=\frac{\mathcal{A}}{\|\mathcal{A}\|}\) and \(\hat{\mathcal{W}}_{\mathrm{TN}}=\frac{\mathcal{W}_{\mathrm{TN}}}{\|\mathcal{ W}_{\mathrm{TN}}\|}\) be normalized versions of \(\mathcal{A}\) and \(\mathcal{W}_{\mathrm{TN}}\), respectively, and let \(c=\frac{2\epsilon}{\|A\|}\). Note that \(c<\frac{2\|\mathcal{A}\|}{4}\frac{1}{\|\mathcal{A}\|}=\frac{1}{2}\), and therefore by Lemma 9 we have:

\[|QE(\mathcal{A};\mathcal{K})-QE(\mathcal{W}_{\mathrm{TN}};\mathcal{ K})| =\Big{|}QE\Big{(}\hat{\mathcal{A}};\mathcal{K}\Big{)}-QE\Big{(} \hat{\mathcal{W}}_{\mathrm{TN}};\mathcal{K}\Big{)}\Big{|}\] \[\leq c\cdot\ln(D_{\mathcal{K}})+H_{b}(c)\,.\]

By Lemma 5 we have that

\[QE\Big{(}\hat{\mathcal{W}}_{\mathrm{TN}};\mathcal{K}\Big{)}\leq\ln(\mathrm{ rank}(\llbracket\mathcal{W}_{\mathrm{TN}};\mathcal{K}\rrbracket))\leq\ln(R)\,,\]

and therefore

\[QE\Big{(}\hat{\mathcal{A}};\mathcal{K}\Big{)}\leq\ln(R)+c\ln(D_{\mathcal{K}})+ H_{b}(c)\,.\]

Substituting \(c=\frac{2\epsilon}{\|A\|}\) and invoking the elementary inequality \(H_{b}(x)\leq 2\sqrt{x}\) we obtain

\[QE(\mathcal{A};\mathcal{K})\leq\ln(R)+\frac{2\epsilon}{\|\mathcal{A}\|}\cdot \ln(D_{\mathcal{K}})+2\sqrt{\frac{2\epsilon}{\|\mathcal{A}\|}}\,,\]

as required.

As for the expected entanglements of a random tensor, let \(\mathcal{A}^{\prime}\in\mathbb{R}^{D_{1}\times\dots\times D_{N}}\) be drawn according to the uniform distribution over the set of unit norm tensors. According to [52], for any canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) it holds that:

\[\mathbb{E}[QE(\mathcal{A}^{\prime};\mathcal{K})]=\sum\nolimits_{k=D_{\mathcal{ K}}^{\prime}+1}^{D_{\mathcal{K}}\cdot D_{\mathcal{K}}^{\prime}}\frac{1}{k}- \frac{D_{\mathcal{K}}-1}{2D_{\mathcal{K}}^{\prime}}\,,\]

where \(D_{\mathcal{K}}^{\prime}:=\max\{\prod_{n\in\mathcal{K}}D_{n},\prod_{n\in \mathcal{K}^{c}}D_{n}\}\) and recall that \(D_{\mathcal{K}}:=\min\{\prod_{n\in\mathcal{K}}D_{n},\prod_{n\in\mathcal{K}^{c} }D_{n}\}\). By the elementary inequality of \(\sum_{k=q}^{p}1/k\geq\ln(p)-\ln(q)\) we therefore have that:

\[\mathbb{E}[QE(\mathcal{A}^{\prime};\mathcal{K})] \geq\ln(D_{\mathcal{K}}\cdot D_{\mathcal{K}}^{\prime})-\ln(D_{ \mathcal{K}}^{\prime}+1)-\frac{D_{\mathcal{K}}-1}{2D_{\mathcal{K}}^{\prime}}\] \[=\ln(D_{\mathcal{K}})+\ln\!\left(\frac{D_{\mathcal{K}}^{\prime} }{D_{\mathcal{K}}^{\prime}+1}\right)-\frac{D_{\mathcal{K}}-1}{2D_{\mathcal{K} }^{\prime}}\,.\]

Since \(D_{\mathcal{K}}\leq D_{\mathcal{K}}^{\prime}\) and \(D_{\mathcal{K}}\geq 1\), it holds that \((D_{\mathcal{K}}-1)/2D_{\mathcal{K}}^{\prime}\leq 1/2\) and \(\ln(D_{\mathcal{K}}^{\prime}/(D_{\mathcal{K}}^{\prime}+1))\geq\ln(1/2)\). Thus:

\[\mathbb{E}[QE(\mathcal{A}^{\prime};\mathcal{K})]\geq\ln(D_{\mathcal{K}})+\ln \!\left(\frac{1}{2}\right)-\frac{1}{2}\geq\min\{|\mathcal{K}|,|\mathcal{K}^{c} |\}\cdot\ln(\min_{n\in[N]}D_{n})+\ln\!\left(\frac{1}{2}\right)-\frac{1}{2}\,.\]

### Proof of Theorem 2

Consider, for each canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\), the distribution

\[\mathcal{P}_{\mathcal{K}}=\left\{p_{\mathcal{K}}(i):=\frac{\sigma_{\mathcal{K},i }^{2}}{\|\mathcal{A}\|^{2}}\right\}_{i\in[D_{\mathcal{K}}]}\,,\]

where \(\sigma_{\mathcal{K},1}\geq\sigma_{\mathcal{K},2}\geq...\geq\sigma_{\mathcal{K},D_{\mathcal{K}}}\) are the singular values of \([\![\mathcal{A};\mathcal{K}]\!]\) (note that \(\frac{1}{\|\mathcal{A}\|^{2}}\sum_{j}\sigma_{\mathcal{K},j}^{2}=\frac{\| \mathcal{A}\|^{2}}{\|\mathcal{A}\|^{2}}=1\) so \(\mathcal{P}_{\mathcal{K}}\) is indeed a probability distribution). Denoting by \(H(\mathcal{P}_{\mathcal{K}}):=\mathbb{E}_{i\sim\mathcal{P}_{\mathcal{K}}}[\ln \left(1/p_{\mathcal{K}}(i)\right)]\) the entropy of \(\mathcal{P}_{\mathcal{K}}\), by assumption:

\[QE(\mathcal{A};\mathcal{K})=H(\mathcal{P}_{\mathcal{K}})\leq\frac{\epsilon^{2 }}{\|\mathcal{A}\|^{2}(2N-3)}\ln(R)\,,\]

for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\). Thus, taking \(a=\frac{\epsilon^{2}}{\|\mathcal{A}\|^{2}(2N-3)}\) we obtain by Lemma 10 that there exists a subset \(T_{\mathcal{K}}\subseteq[D_{\mathcal{K}}]\) such that

\[\mathcal{P}_{\mathcal{K}}(T_{\mathcal{K}}^{c})\leq\frac{\epsilon^{2}}{(2N-3) \|\mathcal{A}\|^{2}}\,,\]

and \(|T_{\mathcal{K}}|\leq e^{\frac{H(\mathcal{P}_{\mathcal{K}})}{c}}=e^{\ln(R)}=R\). Note that

\[\mathcal{P}_{\mathcal{K}}(T_{\mathcal{K}})\leq\sum\nolimits_{i=1}^{R}\frac{ \sigma_{i}^{2}}{\|\mathcal{A}\|^{2}}\,.\]

Since this holds for any subset of cardinality at most \(R\). Taking complements we obtain

\[\sum\nolimits_{i=R+1}^{D_{\mathcal{K}}}\frac{\sigma_{i}^{2}}{\|\mathcal{A}\|^ {2}}\leq\mathcal{P}_{\mathcal{K}}(T_{\mathcal{K}}^{c})\,,\]

so

\[\sqrt{\sum\nolimits_{i=R+1}^{D_{\mathcal{K}}}\sigma_{\mathcal{K},i}^{2}}\leq \frac{\epsilon}{\sqrt{(2N-3)}}\,.\]

We can now invoke Lemma 7, which implies that there exists some \(\mathcal{W}_{\mathrm{TN}}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N}}\) generated by the locally connected tensor network satisfying:

\[\|\mathcal{W}_{\mathrm{TN}}-\mathcal{A}\|\leq\epsilon\,.\]

### Proof of Corollary 1

Notice that the entanglements of a tensor are invariant to multiplication by a constant. In particular, \(QE(\mathcal{D}_{\mathrm{pop}};\mathcal{K})=QE(\mathcal{D}_{\mathrm{pop}}/\| \mathcal{D}_{\mathrm{pop}}\|;\mathcal{K})\) for any \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\). Hence, if there exists a canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) under which \(QE(\mathcal{D}_{\mathrm{pop}};\mathcal{K})>\ln(R)+2\epsilon\cdot\ln(D_{ \mathcal{K}})+2\sqrt{2\epsilon}\), then Theorem 1 implies that \(\min_{\mathcal{W}_{\mathrm{TN}}}\|\mathcal{W}_{\mathrm{TN}}-\mathcal{D}_{ \mathrm{pop}}/\|\mathcal{D}_{\mathrm{pop}}\|\|>\epsilon\). Now, for any non-zero \(\mathcal{W}\in\mathbb{R}^{D_{1}\times\cdots D_{N}}\) generated by the locally connected tensor network, one can also represent \(\mathcal{W}/\|\mathcal{W}\|\) by multiplying any of the tensors constituting the tensor network by \(1/\|\mathcal{W}\|\) (contraction is a multilinear operation). Thus:

\[\mathrm{SubOpt}:=\min_{\mathcal{W}_{\mathrm{TN}}}\!\!\left\|\frac{\mathcal{W }_{\mathrm{TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}-\frac{\mathcal{D}_{\mathrm{ pop}}}{\|\mathcal{D}_{\mathrm{pop}}\|}\right\|\geq\min_{\mathcal{W}_{\mathrm{ TN}}}\!\!\left\|\mathcal{W}_{\mathrm{TN}}-\frac{\mathcal{D}_{\mathrm{pop}}}{\| \mathcal{D}_{\mathrm{pop}}\|}\right\|>\epsilon\,,\]

which concludes the first part of the claim, _i.e._ the necessary condition for low suboptimality in achievable accuracy.

For the sufficient condition, if for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) it holds that \(QE(\mathcal{D}_{\mathrm{pop}};\mathcal{K})\leq\frac{\epsilon^{2}}{8N-12}\cdot \ln(R)\), then by Theorem 2 there exists an assignment for the locally connected tensor network such that \(\|\mathcal{W}_{\mathrm{TN}}-\mathcal{D}_{\mathrm{pop}}/\|\mathcal{D}_{\mathrm{ pop}}\|\|\leq\epsilon/2\). From the triangle inequality we obtain:

\[\left\|\frac{\mathcal{W}_{\mathrm{TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}-\frac{ \mathcal{D}_{\mathrm{pop}}}{\|\mathcal{D}_{\mathrm{pop}}\|}\right\|\leq\left\| \mathcal{W}_{\mathrm{TN}}-\frac{\mathcal{D}_{\mathrm{pop}}}{\|\mathcal{D}_{ \mathrm{pop}}\|}\right\|+\left\|\mathcal{W}_{\mathrm{TN}}-\frac{\mathcal{W}_{ \mathrm{TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}\right\|\leq\frac{\epsilon}{2}+ \left\|\mathcal{W}_{\mathrm{TN}}-\frac{\mathcal{W}_{\mathrm{TN}}}{\|\mathcal{W }_{\mathrm{TN}}\|}\right\|. \tag{11}\]Since \(\|\mathcal{W}_{\mathrm{TN}}-\mathcal{D}_{\mathrm{pop}}/\|\mathcal{D}_{\mathrm{pop}} \|\leq\epsilon/2\) it holds that \(\|\mathcal{W}_{\mathrm{TN}}\|\leq 1+\epsilon/2\). Combined with the fact that \(\big{\|}\mathcal{W}_{\mathrm{TN}}-\frac{\mathcal{W}_{\mathrm{TN}}}{\|\mathcal{ W}_{\mathrm{TN}}\|}\big{\|}=\|\mathcal{W}_{\mathrm{TN}}\|-1\), we get that \(\big{\|}\mathcal{W}_{\mathrm{TN}}-\frac{\mathcal{W}_{\mathrm{TN}}}{\|\mathcal{ W}_{\mathrm{TN}}\|}\big{\|}\leq\epsilon/2\). Plugging this into Equation (11) yields:

\[\bigg{\|}\frac{\mathcal{W}_{\mathrm{TN}}}{\|\mathcal{W}_{\mathrm{ TN}}\|}-\frac{\mathcal{D}_{\mathrm{pop}}}{\|\mathcal{D}_{\mathrm{pop}}\|} \bigg{\|}\leq\epsilon\,,\]

and so \(\mathrm{SubOpt}:=\min_{\mathcal{W}_{\mathrm{TN}}}\big{\|}\frac{\mathcal{W}_{ \mathrm{TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}-\frac{\mathcal{D}_{\mathrm{pop}}} {\|\mathcal{D}_{\mathrm{pop}}\|}\big{\|}\leq\epsilon\). 

### Proof of Proposition 2

We have the identity

\[\bigg{\|}\frac{\mathcal{D}_{\mathrm{pop}}}{\|\mathcal{D}_{\mathrm{pop}}\|}- \frac{\mathcal{D}_{\mathrm{emp}}}{\|\mathcal{D}_{\mathrm{emp}}\|}\bigg{\|}= \bigg{\|}\frac{\mathcal{D}_{\mathrm{pop}}\|\mathcal{D}_{\mathrm{emp}}\|- \mathcal{D}_{\mathrm{emp}}\|\mathcal{D}_{\mathrm{pop}}\|}{\|\mathcal{D}_{ \mathrm{pop}}\|\|\mathcal{D}_{\mathrm{emp}}\|}\bigg{\|}=\]

By the triangle inequality the above is bounded by

\[\frac{\|\mathcal{D}_{\mathrm{pop}}-\mathcal{D}_{\mathrm{emp}}\|}{\|\mathcal{D}_ {\mathrm{pop}}\|}+\frac{\|\mathcal{D}_{\mathrm{pop}}\|-\|\mathcal{D}_{\mathrm{ em}}\|}{\|\mathcal{D}_{\mathrm{pop}}\|}\,.\]

For \(m\in[M]\), let \(\mathcal{X}^{(m)}=y^{(m)}\cdot\otimes_{n\in[N]}\mathbf{x}^{(n,m)}-\mathcal{D} _{\mathrm{pop}}\). These are i.i.d. random variables with \(\mathbb{E}[\mathcal{X}^{(m)}]=0\) and \(\|\mathcal{X}^{(m)}\|\leq 2\) for all \(m\in[M]\). Note that

\[\bigg{\|}\frac{1}{M}\sum\nolimits_{m=1}^{M}\mathcal{X}^{(m)}\bigg{\|}=\| \mathcal{D}_{\mathrm{emp}}-\mathcal{D}_{\mathrm{pop}}\|\,,\]

so by Lemma 4 with \(c=2,t=\frac{\|\mathcal{D}_{\mathrm{pop}}\|\gamma}{2}\), assuming \(M\geq\frac{2\ln(\frac{2}{\delta})}{\|\mathcal{D}_{\mathrm{pop}}\|^{2}\gamma^{2}}\) we have with probability at least \(1-\delta\)

\[\||\mathcal{D}_{\mathrm{pop}}\|-\|\mathcal{D}_{\mathrm{emp}}\|\|\leq\| \mathcal{D}_{\mathrm{pop}}-\mathcal{D}_{\mathrm{emp}}\|\leq\frac{\|\mathcal{D} _{\mathrm{pop}}\|\gamma}{2}\,,\]

and therefore

\[\frac{\|\mathcal{D}_{\mathrm{pop}}-\mathcal{D}_{\mathrm{emp}}\|}{\|\mathcal{ D}_{\mathrm{pop}}\|}+\frac{\||\mathcal{D}_{\mathrm{pop}}\|-\|\mathcal{D}_{ \mathrm{emp}}\|}{\|\mathcal{D}_{\mathrm{pop}}\|}\leq\gamma\,.\]

### Proof of Corollary 2

First, by Proposition 2, for \(M\geq\frac{8\ln(\frac{2}{\delta})}{\|\mathcal{D}_{\mathrm{pop}}\|^{2}\epsilon^ {2}}\) we have that with probability at least \(1-\delta\):

\[\bigg{\|}\frac{\mathcal{D}_{\mathrm{pop}}}{\|\mathcal{D}_{\mathrm{pop}}\|}- \frac{\mathcal{D}_{\mathrm{emp}}}{\|\mathcal{D}_{\mathrm{emp}}\|}\bigg{\|} \leq\frac{\epsilon}{2}\,. \tag{12}\]

Now, we establish the necessary condition on the entanglements of \(\mathcal{D}_{\mathrm{emp}}\). Assume that there exists a canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\) under which

\[QE(\mathcal{D}_{\mathrm{emp}};\mathcal{K})>\ln(R)+3\epsilon\cdot\ln(D_{ \mathcal{K}})+2\sqrt{3\epsilon}\,.\]

We may view \(\mathcal{D}_{\mathrm{emp}}\) as the population data tensor for the uniform data distribution over the training instances \(\big{\{}\big{(}(\mathbf{x}^{(1,m)},\ldots,\mathbf{x}^{(N,m)}),y^{(m)}\big{)} \big{\}}_{m=1}^{M}\). Thus, Corollary 1 implies that

\[\min_{\mathcal{W}_{\mathrm{TN}}}\bigg{\|}\frac{\mathcal{W}_{\mathrm{ TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}-\frac{\mathcal{D}_{\mathrm{emp}}}{\| \mathcal{D}_{\mathrm{emp}}\|}\bigg{\|}>1.5\epsilon\,.\]

Using the triangle inequality after adding and subtracting \(\frac{\mathcal{D}_{\mathrm{pan}}}{\|\mathcal{D}_{\mathrm{pop}}\|}\), it follows that

\[\min_{\mathcal{W}_{\mathrm{TN}}}\bigg{\{}\bigg{\|}\frac{\mathcal{W}_{\mathrm{ TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}-\frac{\mathcal{D}_{\mathrm{pop}}}{\| \mathcal{D}_{\mathrm{pop}}\|}\bigg{\|}+\bigg{\|}\frac{\mathcal{D}_{\mathrm{pop} }}{\|\mathcal{D}_{\mathrm{pop}}\|}-\frac{\mathcal{D}_{\mathrm{emp}}}{\| \mathcal{D}_{\mathrm{emp}}\|}\bigg{\|}\bigg{\}}>1.5\epsilon\,.\]Hence, combined with Equation (12) this concludes the current part of the proof:

\[\mathrm{SubOpt}:=\min_{\mathcal{W}_{\mathrm{TN}}}\biggl{\|}\frac{\mathcal{W}_{ \mathrm{TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}-\frac{\mathcal{D}_{\mathrm{pop}}} {\|\mathcal{D}_{\mathrm{pop}}\|}\biggr{\|}>1.5\epsilon-\biggl{\|}\frac{ \mathcal{D}_{\mathrm{pop}}}{\|\mathcal{D}_{\mathrm{pop}}\|}-\frac{\mathcal{D}_{ \mathrm{emp}}}{\|\mathcal{D}_{\mathrm{emp}}\|}\biggr{\|}\geq 1.5\epsilon-\frac{ \epsilon}{2}=\epsilon.\]

We turn our attention to the sufficient condition on the entanglements of \(\mathcal{D}_{\mathrm{emp}}\). Suppose that \(QE(\mathcal{D}_{\mathrm{emp}};\mathcal{K})\leq\frac{\epsilon^{2}}{32N-48}\cdot \ln(R)\) for all canonical partitions \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}\). Invoking Corollary 1 while viewing \(\mathcal{D}_{\mathrm{emp}}\) as the population data tensor for the uniform data distribution over the training instances \(\bigl{\{}\bigl{(}(\mathbf{x}^{(1,m)},\ldots,\mathbf{x}^{(N,m)}),y^{(m)}\bigr{)} \bigr{\}}_{m=1}^{M}\), we get that

\[\min_{\mathcal{W}_{\mathrm{TN}}}\biggl{\|}\frac{\mathcal{W}_{\mathrm{TN}}}{ \|\mathcal{W}_{\mathrm{TN}}\|}-\frac{\mathcal{D}_{\mathrm{emp}}}{\|\mathcal{D }_{\mathrm{emp}}\|}\biggr{\|}\leq\frac{\epsilon}{2}.\]

Combined with Equation (12), and by the triangle inequality, we conclude:

\[\mathrm{SubOpt} :=\min_{\mathcal{W}_{\mathrm{TN}}}\biggl{\|}\frac{\mathcal{W}_{ \mathrm{TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}-\frac{\mathcal{D}_{\mathrm{pop}}} {\|\mathcal{D}_{\mathrm{pop}}\|}\biggr{\|}\] \[\leq\min_{\mathcal{W}_{\mathrm{TN}}}\biggl{\|}\frac{\mathcal{W}_{ \mathrm{TN}}}{\|\mathcal{W}_{\mathrm{TN}}\|}-\frac{\mathcal{D}_{\mathrm{emp}}} {\|\mathcal{D}_{\mathrm{emp}}\|}\biggr{\|}+\Bigl{\|}\frac{\mathcal{D}_{\mathrm{ pop}}}{\|\mathcal{D}_{\mathrm{pop}}\|}-\frac{\mathcal{D}_{\mathrm{emp}}}{\| \mathcal{D}_{\mathrm{emp}}\|}\biggr{\|}\] \[\leq\epsilon\,.\]

Lastly, the fact that the entanglements of \(\mathcal{D}_{\mathrm{emp}}\) can be evaluated efficiently is discussed in Appendix F. 

### Proof of Theorem 3

If \(\mathcal{A}=0\) the theorem is trivial, since then \(QE(\mathcal{A};\mu(\mathcal{K}))=0\) for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}^{P}\), so we can assume \(\mathcal{A}\neq 0\). We have:

\[\biggl{\|}\frac{\mathcal{W}_{\mathrm{TN}}^{P}}{\|\mathcal{W}_{ \mathrm{TN}}\|}-\frac{\mathcal{A}}{\|\mathcal{A}\|}\biggr{\|} =\frac{1}{\|\mathcal{A}\|}\left\|\frac{\|\mathcal{A}\|}{\|\mathcal{ W}_{\mathrm{TN}}^{P}\|}\cdot\mathcal{W}_{\mathrm{TN}}^{P}-\mathcal{A}\right\|\] \[\leq\frac{1}{\|\mathcal{A}\|}\left(\biggl{\|}\frac{\|\mathcal{A} \|}{\|\mathcal{W}_{\mathrm{TN}}^{P}\|}-1\biggr{\|}\cdot\|\mathcal{W}_{\mathrm{ TN}}^{P}\|+\|\mathcal{W}_{\mathrm{TN}}^{P}-\mathcal{A}\|\right)\] \[=\frac{1}{\|\mathcal{A}\|}\left(\bigl{\|}\mathcal{A}\|-\|\mathcal{ W}_{\mathrm{TN}}^{P}\|\bigr{\|}+\|\mathcal{W}_{\mathrm{TN}}^{P}-\mathcal{A}\|\right)\] \[\leq\frac{2\epsilon}{\|\mathcal{A}\|}\,.\]

Now, let \(\hat{\mathcal{A}}:=\frac{\mathcal{A}}{\|\mathcal{A}\|}\) and \(\hat{\mathcal{W}}_{\mathrm{TN}}^{P}=\frac{\mathcal{W}_{\mathrm{TN}}^{P}}{\| \mathcal{W}_{\mathrm{TN}}^{P}\|}\) be normalized versions of \(\mathcal{A}\) and \(\mathcal{W}_{\mathrm{TN}}^{P}\), respectively, and let \(c=\frac{2\epsilon}{\|\mathcal{A}\|}\). Note that \(c<\frac{2\|\mathcal{A}\|}{4}\frac{1}{\|\mathcal{A}\|}=\frac{1}{2}\), and therefore by Lemma 9 we have:

\[\begin{split}|QE(\mathcal{A};\mu(\mathcal{K}))-QE\bigl{(}\mathcal{ W}_{\mathrm{TN}}^{P};\mu(\mathcal{K}))|&=\bigl{|}QE\Bigl{(} \hat{\mathcal{A}};\mu(\mathcal{K})\Bigr{)}-QE\Bigl{(}\hat{\mathcal{W}}_{ \mathrm{TN}}^{P};\mu(\mathcal{K})\Bigr{)}\Bigr{|}\\ &\leq c\cdot\ln(D_{\mu(\mathcal{K})})+H_{b}(c)\,.\end{split}\]

By Lemma 6 we have that

\[QE\Bigl{(}\hat{\mathcal{W}}_{\mathrm{TN}}^{P};\mu(\mathcal{K})\Bigr{)}\leq\ln( \mathrm{rank}\bigl{(}\bigl{[}\mathcal{W}_{\mathrm{TN}}^{P};\mu(\mathcal{K}) \bigr{]}\bigr{)})\leq\ln(R)\,,\]

and therefore

\[QE\Bigl{(}\hat{\mathcal{A}};\mu(\mathcal{K})\Bigr{)}\leq\ln(R)+c\ln(D_{\mu( \mathcal{K})})+H_{b}(c)\,.\]

Substituting \(c=\frac{2\epsilon}{\|\mathcal{A}\|}\) and invoking the elementary inequality \(H_{b}(x)\leq 2\sqrt{x}\) we obtain

\[QE(\mathcal{A};\mu(\mathcal{K}))\leq\ln(R)+\frac{2\epsilon}{\|\mathcal{A}\|} \cdot\ln(D_{\mu(\mathcal{K})})+2\sqrt{\frac{2\epsilon}{\|\mathcal{A}\|}}\,,\]

as required.

As for Equation (7), let \(\mathcal{A}^{\prime}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}\) be drawn according to the uniform distribution over the set of unit norm tensors. According to [52], for any canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}^{P}\) it holds that:

\[\mathbb{E}[QE(\mathcal{A}^{\prime};\mu(\mathcal{K}))]=\sum\nolimits_{k=D_{\mu( \mathcal{K})}^{D_{\mu(\mathcal{K})}^{\prime}}+1}^{D_{\mu(\mathcal{K})}^{\prime} \cdot D_{\mu(\mathcal{K})}^{\prime}}\frac{1}{k}-\frac{D_{\mu(\mathcal{K})}-1}{ 2D_{\mu(\mathcal{K})}^{\prime}}\,,\]

where we denote \(D_{\mu(\mathcal{K})}^{\prime}:=\max\{\prod_{n\in\mu(\mathcal{K})}D_{n},\prod_{n \in\mu(\mathcal{K})^{c}}D_{n}\}\) and recall that \(D_{\mu(\mathcal{K})}:=\min\{\prod_{n\in\mu(\mathcal{K})}D_{n},\prod_{n\in\mu( \mathcal{K})^{c}}D_{n}\}\). By the elementary inequality of \(\sum_{k=q}^{p}1/k\geq\ln(p)-\ln(q)\) we therefore have that:

\[\mathbb{E}[QE(\mathcal{A}^{\prime};\mu(\mathcal{K}))] \geq\ln(D_{\mu(\mathcal{K})}\cdot D_{\mu(\mathcal{K})}^{\prime})- \ln(D_{\mu(\mathcal{K})}^{\prime}+1)-\frac{D_{\mu(\mathcal{K})}-1}{2D_{\mu( \mathcal{K})}^{\prime}}\] \[=\ln(D_{\mu(\mathcal{K})})+\ln\Biggl{(}\frac{D_{\mu(\mathcal{K})} ^{\prime}}{D_{\mu(\mathcal{K})}^{\prime}+1}\Biggr{)}-\frac{D_{\mu(\mathcal{K})} -1}{2D_{\mu(\mathcal{K})}^{\prime}}\,.\]

Since \(D_{\mu(\mathcal{K})}\leq D_{\mu(\mathcal{K})}^{\prime}\) and \(D_{\mu(\mathcal{K})}\geq 1\), we have that:

\[\frac{D_{\mu(\mathcal{K})}-1}{2D_{\mu(\mathcal{K})}^{\prime}}\leq\frac{1}{2}\, \,\,\,\,\ln\Biggl{(}\frac{D_{\mu(\mathcal{K})}^{\prime}}{D_{\mu(\mathcal{K})}^ {\prime}+1}\Biggr{)}\geq\ln\Biggl{(}\frac{1}{2}\Biggr{)}\,.\]

Thus:

\[\mathbb{E}[QE(\mathcal{A}^{\prime};\mu(\mathcal{K}))] \geq\ln(D_{\mu(\mathcal{K})})+\ln\Biggl{(}\frac{1}{2}\Biggr{)}- \frac{1}{2}\] \[\geq\min\{|\mathcal{K}|,|\mathcal{K}^{c}|\}\cdot\ln(\min_{n\in[N ^{P}]}D_{n})+\ln\Biggl{(}\frac{1}{2}\Biggr{)}-\frac{1}{2}\,.\]

### Proof of Theorem 4

Let \(\mathcal{C}_{N^{P}}\) be the one-dimensional canonical partitions of \([N^{P}]\) (Definition 2). Note that \(\mu(\mathcal{C}_{N}^{P}):=\{\mu(\mathcal{K}):\mathcal{K}\in\mathcal{C}_{N}^{P}\} \subseteq\mathcal{C}_{N^{P}}\). For an assignment \((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N^{P}}}\in\mathbb{N}^{\mathcal{C }_{N^{P}}}\) of integers to one-dimensional canonical partitions \(\mathcal{K}\in\mathcal{C}_{N^{P}}\), we consider the set of tensors whose matricization with respect to each \(\mathcal{K}\in\mathcal{C}_{N^{P}}\) has rank at most \(n_{\mathcal{K}}\). This set is also known in the literature as the set of tensors with _Hierarchical Tucker (HT) rank_ at most \((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N^{P}}}\) (_cf._[25]). Accordingly, we denote it by:

\[HT\left((n_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{N^{P}}}\right):=\left\{ \mathcal{V}\in\mathbb{R}^{D_{1}\times\cdots\times D_{N^{P}}}:\forall\mathcal{ K}\in\mathcal{C}_{N^{P}},\mathrm{rank}([\mathcal{V};\mathcal{K}])\leq n_{ \mathcal{K}}\right\}\,.\]

Now, define \((n_{\mathcal{K}}^{*})_{\mathcal{K}\in\mathcal{C}_{N^{P}}}\in\mathbb{N}^{ \mathcal{C}_{N^{P}}}\) by:

\[\forall\mathcal{K}\in\mathcal{C}_{N^{P}}:\,\,n_{\mathcal{K}}^{*}=\begin{cases} R&\text{if }\,\,\mu^{-1}(\mathcal{K})\in\mathcal{C}_{N}^{P}\\ \mathcal{D}_{\mathcal{K}}&\text{if }\mu^{-1}(\mathcal{K})\notin\mathcal{C}_{N}^{P} \end{cases}\,,\]

where \(D_{\mathcal{K}}:=\min\{\prod_{n\in\mathcal{K}}D_{n},\prod_{n\in\mathcal{K}^{ c}}D_{n}\}\). We show that for any tensor \(\mathcal{A}\) that satisfies for all \(\mathcal{K}\in\mathcal{C}_{N}^{P}\):

\[QE(\mathcal{A};\mu(\mathcal{K}))\leq\frac{\epsilon^{2}}{(2N^{P}-3)\| \mathcal{A}\|^{2}}\cdot\ln(R)\,,\]

there exists a tensor \(\mathcal{V}\in HT\left((n_{\mathcal{K}}^{*})_{\mathcal{K}\in\mathcal{C}_{N^{P }}}\right)\) such that \(\|\mathcal{A}-\mathcal{V}\|\leq\epsilon\). Consider, for each canonical partition \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}_{N}^{P}\), the distribution

\[\mathcal{P}_{\mathcal{K}}=\left\{p_{\mathcal{K}}(i):=\frac{\sigma_{\mathcal{ K},i}^{2}}{\|\mathcal{A}\|^{2}}\right\}_{i\in[D_{\mathcal{K}}]}\,,\]

where \(\sigma_{\mathcal{K},1}\geq\sigma_{\mathcal{K},2}\geq...\geq\sigma_{\mathcal{ K},D_{\mathcal{K}}}\) are the singular values of \([\mathcal{A};\mu(\mathcal{K})]\) (note that \(\frac{1}{\|\mathcal{A}\|^{2}}\sum_{j}\sigma_{\mathcal{K},j}^{2}=\frac{\| \mathcal{A}\|^{2}}{\|\mathcal{A}\|^{2}}=1\) so \(\mathcal{P}_{\mathcal{K}}\) is indeed a probability distribution). Denoting by \(H(\mathcal{P}_{\mathcal{K}}):=\mathbb{E}_{i\sim\mathcal{P}_{\mathcal{K}}}[\ln \left(1/p_{\mathcal{K}}(i)\right)]\) the entropy of \(\mathcal{P}_{\mathcal{K}}\), by assumption:

\[QE(\mathcal{A};\mu(\mathcal{K}))=H(\mathcal{P}_{\mathcal{K}})\leq\frac{ \epsilon^{2}}{\|\mathcal{A}\|^{2}(2N^{P}-3)}\ln(R)\,,\]for all \((\mathcal{K},\mathcal{K}^{c})\in\mathcal{C}^{P}_{N}\). Thus, taking \(a=\frac{\epsilon^{2}}{\|\mathcal{A}\|^{2}(2N^{P}-3)}\) we get by Lemma 10 that there exists a subset \(T_{\mathcal{K}}\subseteq[D_{\mathcal{K}}]\) such that

\[\mathcal{P}_{\mathcal{K}}(T^{c}_{\mathcal{K}})\leq\frac{\epsilon^{2}}{(2N^{P}- 3)\|\mathcal{A}\|^{2}}\,,\]

and \(|T_{\mathcal{K}}|\leq e^{\frac{H(\mathcal{P}_{\mathcal{K}})}{a}}=e^{\ln(R)}=R\). Note that

\[\mathcal{P}_{\mathcal{K}}(T_{\mathcal{K}})\leq\sum\nolimits_{i=1}^{R}\frac{ \sigma_{i}^{2}}{\|\mathcal{A}\|^{2}}\,.\]

Since this holds for any subset of cardinality at most \(R\). Taking complements we obtain

\[\sum\nolimits_{i=R+1}^{D_{\mathcal{K}}}\frac{\sigma_{i}^{2}}{\|\mathcal{A}\|^ {2}}\leq\mathcal{P}_{\mathcal{K}}(T^{c}_{\mathcal{K}})\,,\]

so

\[\sqrt{\sum\nolimits_{i=R+1}^{D_{\mathcal{K}}}\sigma_{\mathcal{K},i}^{2}}\leq \frac{\epsilon}{\sqrt{(2N^{P}-3)}}\,.\]

We can now invoke Lemma 7 (note that if \(\mu^{-1}(\mathcal{K})\notin\mathcal{C}^{P}_{N}\), then the requirements of Lemma 7 are trivially fullfiled with respect to the partition \((\mathcal{K},\mathcal{K}^{c})\) since \(n^{*}_{\mathcal{K}}=D_{\mathcal{K}}\)), which implies that there exists some \(\mathcal{W}\in HT\left((n^{*}_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{NP}}\right)\) satisfying:

\[\|\mathcal{W}-\mathcal{A}\|\leq\epsilon\,,\]

as required.

The proof concludes by establishing that for any tensor \(\mathcal{W}\in HT\left((n^{*}_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{NP}}\right)\), there exists assignment for the tensors constituting the \(P\)-dimensional locally connected tensor network (defined in Figure 6) such that it generates \(\mathcal{W}\).

To see why this is the case, note that by Lemma 8 any tensor \(\mathcal{W}\in HT\left((n^{*}_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{NP}}\right)\) can be represented by a (one-dimensional) locally connected tensor network with varying widths \((n^{*}_{\mathcal{K}})_{\mathcal{K}\in\mathcal{C}_{NP}}\), _i.e._ a tensor network conforming to a perfect binary tree graph in which the lengths of inner axes are as follows: an axis corresponding to an edge that connects a node with descendant leaves indexed by \(\mathcal{K}\) to its parent is assigned the length \(n^{*}_{\mathcal{K}}\). We can obtain an equivalent representation of any such tensor as a \(P\)-dimensional locally connected tensor network (described in Appendix J.1.1) via the following procedure. For each node at level \(l\in\{0,P,2P,\ldots,(L-1)P\}\) of the tree (recall \(N=2^{L}\) for \(L\in\mathbb{N}\)), contract it with its descendant nodes at levels \(\{l+1,\ldots,l+(P-1)\}\).12 This results in a new tensor network whose underlying graph is a perfect \(2^{P}\)-ary tree and the remaining edges all correspond to inner axes of lengths equal to \(n^{*}_{\mathcal{K}}=R\) for \(\mathcal{K}\in\mathcal{C}^{P}_{N}\), _i.e._ in a representation of \(\mathcal{W}\) as a \(P\)-dimensional locally connected tensor network. 

Footnote 12: For a concrete example, let \(N=2^{L}=4\) and \(P=2\) (_i.e._\(L=2\)). In this case, the perfect binary tree underlying the one-dimensional locally connected tensor network of varying widths is of height \(L\cdot P=4\) and has \(N^{P}=16\) leaves. It is converted into a perfect \(4\)-ary tree tensor network of height \(L=2\) by contracting the root with its two children and the nodes at level two with their children.