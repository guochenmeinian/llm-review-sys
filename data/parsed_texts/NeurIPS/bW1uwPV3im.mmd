# LoRA: A Logical Reasoning Augmented Dataset for Visual Question Answering

 Jingying Gao

School of Computer Science & Engineering

The University of New South Wales

jingying.gao@unsw.edu.au

&Qi Wu

School of Computer & Mathematical Sciences

The University of Adelaide

qi.wu01@adelaide.edu.au

&Alan Blair

School of Computer Science & Engineering

The University of New South Wales

a.blair@unsw.edu.au

&Maurice Pagnucco

School of Computer Science & Engineering

The University of New South Wales

morri@cse.unsw.edu.au

###### Abstract

The capacity to reason logically is a hallmark of human cognition. Humans excel at integrating multimodal information for logical reasoning, as exemplified by the Visual Question Answering (VQA) task, which is a challenging multimodal task. VQA tasks and large vision-and-language models aim to tackle reasoning problems, but the accuracy, consistency and integrity of the generated answers is hard to evaluate in the absence of a VQA dataset that can offer formal, comprehensive and systematic complex logical reasoning questions. To address this gap, we present LoRA, a novel Logical Reasoning Augmented VQA dataset that requires formal and complex description logic reasoning based on a food-and-kitchen knowledge base. Our main objective in creating LoRA is to enhance the complex and formal logical reasoning capabilities of VQA models, which are not adequately measured by existing VQA datasets. We devise strong and flexible programs to automatically generate 200,000 diverse description logic reasoning questions based on the SROIQ Description Logic, along with realistic kitchen scenes and ground truth answers. We fine-tune the latest transformer VQA models and evaluate the zero-shot performance of the state-of-the-art large vision-and-language models on LoRA. The results reveal that LoRA presents a unique challenge in logical reasoning, setting a systematic and comprehensive evaluation standard.12

Footnote 1: LoRA Dataset Project page: https://lora-vqa.github.io/

Footnote 2: The LoRA Dataset code is available at: https://github.com/CarolineGao/LoRA-Dataset.git

## 1 Introduction

Logical reasoning is a fundamental hallmark of human cognition that enables us to understand and solve various problems, from everyday life to scientific research. Logical reasoning is the ability to analyze and identify logical relationships and derive conclusions or solutions based on known information or conditions [6, 3]. Humans are adept at integrating multimodal data for logical reasoning, such as visual, linguistic, or auditory information. A challenging task in artificial intelligence that requires such multimodal logical reasoning is Visual Question Answering (VQA)[1]. However, existing VQA datasets lack questions with complex logical structure, limiting the ability of these models to perform complex inference and multi-step reasoning. In this work, we introduce a new VQA dataset LoRA (Logical Reasoning Augmented Dataset), to address these challenges andexplore the logical reasoning capabilities of VQA and large vision-and-language models, and how they perform as the logical difficulty increases.

State-of-the-art VQA models have made significant strides in tackling reasoning problems that hinge on visual relationships and basic logical constructs such as 'and', 'or', and 'not'. Nonetheless, they struggle to perform complex inference and multi-step reasoning [40, 36, 27, 11]. Large vision-and-language models can execute complex multi-step reasoning. However, even the most advanced models still often produce logical errors. They exhibit a tendency to fabricate facts when facing uncertain information [37, 14]. It is unclear to what extent they understand the task, are capable of reasoning logically and generalizing, even if they perform well on the benchmark.

This can be largely attributed to the limitations of existing VQA datasets[40, 23], which primarily offer questions with basic logical connections and not only fail to challenge the models to reason at higher levels of complexity, but also neglect to consider the logical difficulty and the performance of different approaches with increasing complexity. We studied and examined the logical syntactic complexity of the questions in several widely-used VQA datasets from 2016 to 2022 [1, 28, 20, 21, 35, 33, 25, 31, 9, 26] based on various logical categories and confirmed this limitation.

We introduce LoRA, a Logic Reasoning Augmented VQA Dataset, which addresses these issues with a focus on formal, complex, and diverse logical reasoning. The LoRA Dataset comprises 200,000 sophisticated and diverse logical reasoning questions based on the formal Description Logic SROIQ, based on realistic kitchen scenes, with ground truth answers, and logical prompt annotations. We devised multiple strong and flexible tools to automatically generate the LoRA dataset. The logical problems in LoRA span a wide and diverse range of complex logic, divided into three levels of increasing difficulty from simple to complex. This progression provides a more comprehensive evaluation of existing VQA methods, assessing to what extent their logical reasoning abilities have evolved, and highlighting for which logical reasoning tasks their performance is superior.

One example from our LoRA dataset is shown in Figure 1. These questions contain a number of different types of complex logical reasoning relationships, such as conjunction, disjunction, negation and "if... then" rule-based reasoning, etc. In order to answer such questions involving multiple different logical relationships, multi-step logical reasoning is required, based on diverse types of logical inferences.

Our experiments show that current multimodal methods[19, 32, 24, 39, 13, 7, 38] fail to achieve satisfactory performance on the LoRA dataset and their performance decreases with the increasing difficulty level of the questions. The evaluation encompasses end-to-end training, transformer models within a fine-tuning setting, and large vision-and-language models in both zero-shot and few-shot learning settings on LoRA.

In summary, our contributions are three-fold: (1) we present LoRA, a novel VQA dataset that challenges the state-of-the-art models to solve 200,000 multimodal logical reasoning questions of high complexity and diversity; (2) our dataset comes with automated scripts that enable researchers to create and enrich their own logical questions, facilitating the growth and variety of logical reasoning datasets; and, (3) we leverage a formal definition of logical difficulty based on Description Logic to systematically evaluate the logical reasoning abilities of existing VQA and large vision-and-language models across different levels of complexity, revealing important directions for future research.

Figure 1: Sample image and questions from LoRA. Test questions are a combination of logical reasoning including rule-based conditional logic, conjunction, and negation.

[MISSING_PAGE_FAIL:3]

based on the formal Description Logic SROIQ[2], along with ground truth answers, logical prompt annotations, and 100,000 realistic kitchen scenes.

The LoRA dataset is created automatically via multiple flexible automated scripts. Figure 2 provides an overview of the LoRA generation pipelines. The dataset creation process includes five steps: (1) constructing the ontology, (2) formal description logic definitions, (3) automatically generating questions, (4) dynamically queried answers, (5) automatically generating images.

In constructing the LoRA dataset, we first created a kitchen domain-specific ontology and defined its logic syntax based on Description Logic. We implemented a script to automatically generate complex reasoning questions, enabling arbitrary expansion of question quantity and complexity. Furthermore, we designed a dynamic query algorithm for the automatic retrieval of answers. For realistic visual context, we developed a script that can be executed within the Blender[5] environment to automatically generate realistic kitchen scene images corresponding to the questions. The ground truth answers are intentionally obfuscated by noise distractors to increase the level of difficulty. Last but not least, logical operators to construct the questions are included as logical prompt annotations. It should be emphasized that our dataset creation approach and pipeline are domain-agnostic, allowing adaptation to diverse knowledge areas.

### The Ontology

We used OWLReady2[22] to create our initial ontology (Knowledge Base, KB), and drew inspiration from public food ontologies such as FoodOn [10] and FoodKG [15]. This ontology was framed using the industry-standard OWL format. The ontology defines domain knowledge through three elements: concepts (e.g., Food, Fruit), roles that represent atomic relations, (e.g., hasTaste); and individuals (e.g., apple). The ontology[2] is defined as KB = (A, T), where: KB = ABox + TBox.

The **ABox** contains specific instances of the TBox concepts (e.g., apple, banana) and their attributes, covering both visible (e.g., red) and invisible attributes (e.g., sweet). The **TBox** defines concepts (e.g., Food, Fruit, Vegetable, Meal), along with relationships between them(e.g., isSubClassOf, isMadeFrom). Additionally, Semantic Web Rule Language (SWRL)[18] rules are defined in the TBox for conditional logical reasoning.

The ontology provides formal representations and definitions for food and kitchen domain knowledge. The ontology provides three advantages: (1) it provides a rich vocabulary and knowledge base for question generation, (2) it standardizes and maps concepts to objects in images, (3) it enhances reasoning capabilities by enabling inference based on logical rules and knowledge. Our approach and pipeline to create the dataset is generalizable. It can work with any ontology (knowledge base) that adheres to the OWL specifications, which is the standard W3C Web Ontology Language.

### Formal Description Logic Definition

Description Logic (DL)[2] is a family of formalisms for representing and reasoning with ontological knowledge. We use Description Logic syntax to formulate the logical questions in LoRA, as defined in Definition 1. Description Logic provides a standard way to construct the logical components of the questions, enabling diverse and sophisticated logical reasoning.

**Definition 1** (Description Logic Syntax).: _Let C, D be concepts, R is a role (possibly inverse), P is a simple role (possibly inverse), n is a non-negative integer; then C \(\sqcap\) D, C \(\sqcup\) D, \(\sqcap\) C, \(\exists\) R.C, \(\forall\) R.C, \(\geq\) nP.C, \(\leq\) nP.C, are also concepts (Table 2)._

### Automatically Generated Questions

We propose an algorithm that generates complex logical reasoning questions by unrolling the on

\begin{table}
\begin{tabular}{|l|l|} \hline \multicolumn{2}{|c|}{Description Logic Syntax} \\ \hline Logic Categories & Syntax \\ \hline atomic concept & C, D \\ atomic role & R \\ transitive role & \(R\in R^{+}\) \\ conjunction & \(C\sqcap D\) \\ disjunction & \(C\sqcup D\) \\ negation & \(\neg C\) \\ concept inclusion & C \(\sqsubseteq D\) \\ existential restriction & \(\exists R.C\) \\ universal restriction & \(\forall R.C\) \\ number restrictions & \(\geq nP.C\) or \(\leq nP.C\) \\ \hline \end{tabular}
\end{table}
Table 2: Description Logic Grammar.

tology into dataframes and populating a matrix with concepts, roles, attributes, logical operators, and formulas. The questions are formed by randomly placing logical operators in the matrix, ensuring diverse reasoning types and difficulty levels. The generated questions exhibit human-like logic, involving both multimodal information and commonsense knowledge as well as symbolic logic.

Logical questions are automatically generated using a five-step procedure: 1) unroll the ontology, 2) generate logical questions with logical operators, 3) generate conditional logical questions with rules, 4) filter rules to avoid repetition, and 5) enrich question diversity using synonymous phrases.

#### 3.3.1 Unroll Ontology

We propose an algorithm that unrolls the ontology into a table format in three steps. First, we recursively extract the class hierarchy, relationship tuples, and object pairs from the ontology. For example, Vegetable is a subclass of Food, which is a subclass of Thing; (hasColor, Vegetable, Color) is a relationship tuple; and (carrot, orange) is an object pair. Second, we collapse the hierarchy into a list of dictionaries recursively, where each dictionary contains the entity and relationship information at each level. Finally, we transform the list of dictionaries into a dataframe with five columns: object classes, attribute classes, entities, attributes, and relationships. This dataframe is the basic building block for generating questions.

Our algorithm can unroll any ontology that is based on or can be converted to owlready2 [22, 17], not only our customized ontology. This is because the framework we utilized to construct the ontologies is the industry-standard OWL format. It can work with any other domain ontology (knowledge base) that adheres to the OWL specifications, which is the standard W3C Web Ontology Language [29].

#### 3.3.2 Generate Logical Questions with Logical Operators

We propose an automated script to generate large numbers of logical questions with logical operators based on Description Logic (DL) syntax. We first define nine foundational logical operators based on DL syntax, which include AND, OR, NOT, inclusion, existential restriction, etc., shown in Table 3. These operators form a basic logical library that enables us to construct logical questions following the formal DL syntax.

To generate questions from an unrolled ontology represented as a table, we randomly select concepts, roles and entities to form a matrix of sentence blocks. We then apply logical operators to connect two or three sentence blocks within the matrix, resulting in logical questions with varying levels of complexity. Each row in the table corresponds to a branch of class, relation and entity pairs. By traversing the table, we can randomly combine class, relation and entity elements with diverse logical operators to create intricate questions with two or three layers of logical relationships.

The question template for the three-layer logical relationship is as follows: "\([QuestionTypes]\) Which \(|\) How many \(|\) Are there + \(\langle concepts\rangle\) + \(\langle\) sentence block 1 \(\rangle\) + logical operator + \(\langle\) sentence block 2\(\rangle\) + logical operator + \(\langle\) sentence block 3\(\rangle\)?" One such question could be: "\([Which]\)\(\langle\) vegetables \(\rangle\) in the picture exhibit \(\langle\) a conical shape \(\rangle\), **and**\(\langle\) are rich in nutrient iron \(\rangle\), **or**\(\langle\) possess internal seeds \(\rangle\)?"

To illustrate, the logical operators used are not confined to [not], [and], [or], but also include formal description logic syntax defined above as a benchmark. Different logic syntax are inserted into the matrix, for instance, using \(\geq nP.C\), we can generate a question like "which vegetables in the picture contain at least two different tastes, and are rich in cellulose?"; C \(\sqsupseteq\) D: class hierarchy: e.g. "Which foods in the picture are meat but can provide high protein and shaped solids?"; R- inverse role: "Which food in the image can be used to cook a meal for vegetarians?".

#### 3.3.3 Generate Conditional Logical Questions with Rules

We extend our question generation pipeline to produce seven additional complex logical questions based on SWRL rules, illustrated in Table 4. For example, we use the following question template based on a SWRL rule: to cook [attribute] \(\langle concept\rangle\), if we do not have \(\langle object\rangle\), can we use other \(\langle objects\rangle\) in the image instead? Question examples: to cook [spicy] \(\langle noodles\rangle\), if we do not have \(\langle pepper\rangle\), can we use other items in the image instead?

To generate complex logical questions, we need both the unrolled ontology table and the specific question templates. We design question templates based on SWRL rules and select and replace the 

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

### Question and Answer Types Analysis

**Question Types:** We categorize the question types in the LORA dataset into seven groups shown in Figure 3 based on the head of each question template: What, Which, How many, Is there..., Are there..., If... then, and Why...? The distribution of each question type in the dataset is shown in Figure 3, which also presents the data distribution based on ontology types and the question distribution based on logical reasoning types. The lengths of the logical questions range from eight to thirty words.

**Answer Types:** The answers within the dataset fall into four distinct categories: yes/no responses; individual or multiple objects (e.g., pumpkin, cheese); numerical values; and a small proportion of specialized responses, including answers to 'why' questions that incorporate relations and attributes. Examples of such specialized responses include "because it has juicy taste". or instances where the answer is "None". The length of the answers ranges from two words to a maximum of 23 words, varying based on the answer type.

Supplementary material Section A offers a more comprehensive data analysis, encompassing the analysis of the ontology, question types, and the distribution of question logic in LoRA. Section A.3 of the supplementary material provides examples of question logical complexity and answer inference complexity in LoRA, ranging from 3 to 9 required inference steps.

## 5 Baseline Experiments and Analysis

### Baseline Experiments

The VQA dataset was evaluated and analyzed via different VQA baseline models and large vision-and-language models. The evaluation results are shown in Table 5. In our methodology, we categorize the queries according to their intricacy, on a scale of 1 to 9. A score of 1 signifies a relatively straightforward query, while a score of 9 implies the necessity for more than nine logical deductions to infer the answer. In addition, a study of human performance was conducted, details of which can be found in Supplementary Section E.2.

**Ablation Study.** In order to carry out an ablation study and evaluate the capacity of baseline models to provide accurate answers without image data, question data, or external knowledge, we examined three models: a "deaf" CNN [4] model, a "blind" LSTM [16] model, and a CNN+LSTM model that made predictions based on visual and linguistic features, but without a knowledge base. The performance of all three models was found to be unsatisfactory, underscoring the importance of integrating visual, question, and knowledge base information for effective VQA reasoning.

**End-to-End Training Baseline.**

We trained MAC network [19] end-to-end on the LoRA dataset, inputting both visual and question language features to estimate the accuracy for each type of logical question. The MAC network struggled with rule-based and complex logical questions compared to simple ones, resulting in 64.6% accuracy.

**Fine-tuning Baseline.** Moreover, we trained and fine-tuned the VisualBERT model [24], based on the transformer architecture, on the LoRA dataset. Visual features were extracted using a pre-trained detectron2 model[34], while BERT[8] was used for question features. These image and language features were fed into vision-and-language fusion layers to predict final answers. Despite

Figure 3: Dataset Statistics: Ontology types, question types, and logical reasoning types.

the significant improvement in performance brought by the transformer of VisualBERT over other models, it displayed a decrease in its ability to answer complex questions as reasoning complexity increased, from 85.1% to 57.8%.

**Zero-shot Baselines.**

We evaluated the zero-shot performance of cutting-edge large vision-and-language models (VLMs), specifically, MiniGPT4 [39], Multimodal-GPT [13], InstructBLIP [7], and Multimodal Chain-of-Thought (MMCoT) [38] across three model types: in-context learning, instruct-tuning, and zero-shot CoT, using our LoRA dataset. We used the questions from the LoRA dataset as the prompts, with the accompanying images serving as the input images for these models for zero-shot experiments. Each question in the dataset not only guided the model in the direction of the answer but also implicitly highlighted the logical construct being evaluated. Our results revealed that all four models exhibited basic VQA capabilities, managing to answer questions that do not require much logical reasoning. They could also handle simple negation questions, but inconsistently. However, limitations were evident. The models occasionally ignore the visual information in the image and generate answers solely based on textual knowledge, especially MiniGPT4. Furthermore, they faltered in providing comprehensive responses involving multiple objects, often only listing one or two. Moreover, they are inconsistent and tend to fabricate answers that are often incongruent with the image content. Among the four models, InstructBLIP outperforms the other models with an average accuracy of 41.2%, but it performs poorly on complex logical questions that require more than six inference steps, achieving only 30.5% accuracy. MiniGPT4 struggles with negation questions that involve invisible attributes and complex logical questions involving multimodal inference. MMCoT exhibited suboptimal performance in VQA logical reasoning tasks, producing reasoning chains with logical inconsistencies.

**Few-shot Baseline.**

Furthermore, we evaluated Multimodal Chain of Thought based on the few-shot setting. We provided the logical operator that constructed each question as the prompting context to the model when asking it to answer each question. Using the logical operators of the questions as prompts improved the performance by 5.87% compared to not using them. Further analysis shows that logical prompts can help multimodal models stimulate logical thinking better.

### Error Analysis

The following analysis investigates the different behaviors and performance of VQA models and multimodal LLM models, delving deeper into the underlying reasons behind the observed model behavior, and elucidates the potential factors that contribute to the strengths or weaknesses of a particular model.

#### 5.2.1 MAC and VisualBERT's Challenges in Complex Logical Reasoning

The MAC network and VisualBERT model, despite their groundbreaking advancements in VQA, primarily rely on deep neural networks to derive implicit representations from the provided data. Deep learning models, by their very nature, often lack the inherent capacity to reason in an explicit, step-by-step manner that might be required for intricate logical constructs. Instead, these models tend to predict answers based on the patterns and associations they have learned. While this approach works efficiently for numerous tasks, it may not provide the rigorous, step-by-step logical reasoning required for our dataset, especially when questions increase in complexity.

#### 5.2.2 Multimodal LLMs Baselines' Challenges in Complex Logical Reasoning

Building on our observations from the zero-shot and few-shot experiments, we further analyzed challenges faced by large vision-and-language models (VLMs). We observed that large vision-and-language models demonstrate deficiencies in handling logical constructs, especially with operators like negation and multiple logical operators appearing in a question. For example, when presented with negation-centric queries, these models tend to produce keyword-driven positive responses, overlooking inherent logical relationships. Such tendencies underscore that, despite expansive training data, these models lack genuine "logical understanding".

[MISSING_PAGE_FAIL:10]

## References

* [1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. VQA: Visual Question Answering. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 2425-2433, 2015.
* [2] Franz Baader. _The description logic handbook: Theory, implementation and applications_. Cambridge university press, 2003.
* [3] Janette B Benson. _Encyclopedia of infant and early childhood development_. Elsevier, 2020.
* [4] Rahul Chauhan, Kamal Kumar Ghanshala, and RC Joshi. Convolutional neural network (CNN) for image detection and recognition. In _2018 First International Conference on Secure Cyber Computing and Communication (ICSCCC)_, pages 278-282. IEEE, 2018.
* a 3D modelling and rendering package_. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. (https://www.blender.org).
* [6] John Corcoran. Completeness of an ancient logic. _The Journal of Symbolic Logic_, 37(4):696-702, 1972.
* [7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning, 2023.
* [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [9] Yang Ding, Jing Yu, Bang Liu, Yue Hu, Mingxin Cui, and Qi Wu. Mukea: Multimodal knowledge extraction and accumulation for knowledge-based visual question answering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5089-5098, 2022.
* [10] Damion M. Dooley, Emma J. Griffiths, Gurinder S. Gosal, Pier L. Buttigieg, Robert Hoehndorf, Matthew C. Lange, Lynn M. Schriml, Fiona S. L. Brinkman, and William W. L. Hsiao. FoodOn: a harmonized food ontology to increase global food traceability, quality control and data integration. _npj Science of Food_, 2(1):23, 2018.
* [11] Thomas Eiter, Nelson Higuera, Johannes Oetsch, and Michael Pritz. A neuro-symbolic ASP pipeline for visual question answering. _Theory and Practice of Logic Programming_, 22(5):739-754, 2022.
* [12] Jingying Gao, Alan Blair, and Maurice Pagnucco. A symbolic-neural reasoning model for visual question answering. In _2023 International Joint Conference on Neural Networks (IJCNN)_, pages 1-9. IEEE, 2023.
* [13] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-GPT: A vision and language model for dialogue with humans. _arXiv preprint arXiv:2305.04790_, 2023.
* [14] Muhammad Usman Hadi, R Qureshi, A Shah, M Irfan, A Zafar, MB Shaikh, N Akhtar, J Wu, and S Mirjalili. A survey on large language models: Applications, challenges, limitations, and practical usage. _TechRxiv_, 2023.
* [15] Steven Haussmann, Oshani Seneviratne, Yu Chen, Yarden Ne'eman, James Codella, Ching-Hua Chen, Deborah L McGuinness, and Mohammed J Zaki. FoodKG: a semantics-driven knowledge graph for food recommendation. In _The Semantic Web-ISWC 2019: 18th International Semantic Web Conference, Auckland, New Zealand_, pages 146-162. Springer, 2019. (https://foodkg.github.io).
* [16] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural Computation_, 9(8):1735-1780, 1997.

* [17] Matthew Horridge, Simon Jupp, Georgina Moulton, Alan Rector, Robert Stevens, and Chris Wroe. A practical guide to building OWL Ontologies using Protege 4 and CO-ODE Tools Edition 1.2. _The University of Manchester_, 2009.
* [18] Ian Horrocks, Peter F Patel-Schneider, Harold Boley, Said Tabet, Benjamin Grosof, Mike Dean, et al. SWRL: A semantic web rule language combining OWL and RuleML. _W3C Member submission_, 21(79):1-31, 2004.
* [19] Drew A Hudson and Christopher D Manning. Compositional attention networks for machine reasoning. _arXiv preprint arXiv:1803.03067_, 2018.
* [20] Drew A Hudson and Christopher D Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6700-6709, 2019.
* [21] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 2901-2910, 2017.
* [22] Jean-Baptiste Lamy. Owlready: Ontology-oriented programming in python with automatic classification and high level constructs for biomedical ontologies. _Artificial intelligence in medicine_, 80:11-28, 2017.
* [23] Feng Li, Hao Zhang, Yi-Fan Zhang, Shilong Liu, Jian Guo, Lionel M Ni, PengChuan Zhang, and Lei Zhang. Vision-language intelligence: Tasks, representation learning, and large models. _arXiv preprint arXiv:2203.01922_, 2022.
* [24] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBert: A simple and performant baseline for vision and language. _arXiv preprint arXiv:1908.03557_, 2019.
* [25] Zechen Li and Anders Sogaard. Qlevr: A diagnostic dataset for quantifational language and elementary visual reasoning. _arXiv preprint arXiv:2205.03075_, 2022.
* [26] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In _The 36th Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [27] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. _arXiv preprint arXiv:1904.12584_, 2019.
* [28] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-VQA: A visual question answering benchmark requiring external knowledge. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3195-3204, 2019.
* [29] Deborah L McGuinness, Frank Van Harmelen, et al. OWL web ontology language overview. _W3C recommendation_, 10(10):2004, 2004.
* [30] Eric Prud'hommeaux and Andy Seaborne. SPARQL Query Language for RDF. W3C Recommendation, 2008.
* [31] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In _European Conference on Computer Vision_, pages 146-162. Springer, 2022.
* [32] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers. _arXiv preprint arXiv:1908.07490_, 2019.
* [33] Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. FVQA: Fact-based visual question answering. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 40(10):2413-2427, 2017.

* [34] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.
* [35] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. CLEVER: Collision events for video representation and reasoning. _arXiv preprint arXiv:1910.01442_, 2019.
* [36] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-symbolic VQA: Disentangling reasoning from vision and language understanding. _Advances in Neural Information Processing Systems_, 31, 2018.
* [37] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: A survey. _arXiv preprint arXiv:2304.00685_, 2023.
* [38] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. _arXiv preprint arXiv:2302.00923_, 2023.
* [39] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.00685_, 2023.
* [40] Yeyun Zou and Qiyu Xie. A survey on VQA: Datasets and approaches. In _2020 2nd International Conference on Information Technology and Computer Application (ITCA)_, pages 289-297. IEEE, 2020.