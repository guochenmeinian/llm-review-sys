[MISSING_PAGE_EMPTY:1]

building block for constructing discriminative models that are fast, gradient-free, and Bayesian.

## 1 Introduction

Modern machine learning methods attempt to learn functions of complex data (e.g., images, audio, text) to predict information associated with that data, such as discrete labels (Bernardo et al., 2007). Deep neural networks (DNNs) have demonstrated success in this domain, owing to their universal function approximation properties (Park and Sandberg, 1991) and scalable optimization algorithms for training them (Amari, 1993). Despite their performance and scalability, DNNs do not provide well calibrated predictions and uncertainty estimates (Wang et al., 2021; Shao et al., 2020). This limits the applicability and reliability of using DNNs in safety-critical applications like autonomous driving, medicine, and disaster response (Papamarkou et al., 2024).

Here we introduce a gradient-free variational learning algorithm for a probabilistic variant of a two-layer, feedforward neural network -- the conditional mixture network or CMN -- and measure its performance on supervised learning benchmarks. This method rests on coordinate ascent variational inference (CAVI) (Wainwright et al., 2008; Hoffman et al., 2013) and hence we name it CAVI-CMN. CAVI-CMN maintains the predictive accuracy and scalability of an architecture-matched feedforward neural network fit with maximum likelihood estimation, while maintaining full distributions over its parameters and generating calibrated predictions, as measured in relationship to state-of-the-art Bayesian methods like the No U-Turn Sampler (NUTS) algorithm for Hamiltonian Monte Carlo (Hoffman et al., 2014) and black-box variational inference (Ranganath et al., 2014).

We summarize the contributions of this work below:

* Introduce and derive a coordinate ascent variational inference scheme for the conditional mixture network, which we term CAVI-CMN.
* CAVI-CMN matches, and sometimes exceeds, the performance of maximum likelihood estimation (MLE) in terms of predictive accuracy, while maintaining probabilistic benefits like high log predictive density and low calibration error. This is shown across a suite of 8 different supervised classification tasks.
* CAVI-CMN requires drastically less time to converge and overall runtime than the other state-of-the-art Bayesian methods like NUTS and BBVI.

## 2 Methods

Here, we introduce a variant of the Mixture-of-Experts (MoE) model (Jacobs et al., 1991) that makes its parameters amenable to gradient-free Bayesian learning. Jacobs et al. (1991) originally introduced MoEs as a way to improve the performance of neural networks by combining the strengths of multiple specialized models (Gormley and Fruhwirth-Schnatter, 2019). Non-Bayesian approaches to MoE typically rely on maximum likelihood estimation (MLE) (Jacobs et al., 1991), which can suffer from overfitting and poor generalization due to the lack of regularization mechanisms (Bishop and Svenskn, 2003).

The approach we propose, CAVI-CMN takes advantage of the conditional conjugacy of a mixture of linear experts, along with Polya-Gamma (PG) augmentation (Polson et al., 2013) for the softmax layers, to make all parameters amenable to variational Bayesian inference. We use coordinate ascent variational inference (CAVI) to obtain posteriors over the weights of both the individual experts and the gating network (Bishop and Nasrabadi, 2006; Blei et al., 2017), without resorting to costly gradient or sampling computations.

### The conditional mixture network

The conditional mixture network maps from a continuous input vector \(\bm{x}_{0}\in\mathbb{R}^{d}\) to its label \(y\in\{1,\ldots,L\}\). This is achieved with two layers: a conditional mixture of linear experts, which outputs a joint continuous-discrete latent \(\big{(}\bm{x}_{1}\in\mathbb{R}^{h},z_{1}\in\{1,\ldots,K\}\big{)}\) and a multinomial logistic regression or softmax layer, which maps from the continuous latent \(\bm{x}_{1}\) to the corresponding label \(y\). Given a dataset of input-label pairs \((\bm{X}_{0},Y)=\{\bm{x}_{0}^{n},y^{n}\}_{n=1}^{N}\), the CMN defines a joint distribution over labels \(Y\), latents \(\bm{X}_{1},Z_{1}\), and parameters \(\bm{\Theta}\):\[p(\bm{Y},\bm{X}_{1},\bm{Z}_{1},\bm{\Theta}|\bm{X}_{0}) =p\left(\bm{\Theta}\right)\prod_{n=1}^{N}p_{\bm{\beta}_{1}}\left(y^ {n}|\bm{x}_{1}\right)p_{\bm{\lambda}_{1}}\left(\bm{x}_{1}^{n}|\bm{x}_{0}^{n},z _{1}^{n}\right)p_{\bm{\beta}_{0}}\left(z_{1}^{n}|\bm{x}_{0}^{n}\right)\] (1) \[p\left(\bm{\Theta}\right) =p\left(\bm{\beta}_{1}\right)p\left(\bm{\beta}_{0}\right)p\left( \bm{\lambda}_{1}\right)\]

where \(p_{\bm{\lambda}_{1}}\left(\bm{x}_{0}^{n}|\bm{x}_{0}^{n},z_{1}^{n}\right)\) refers to a mixture of linear models with parameters \(\bm{\lambda}_{1}=\bm{A}_{1:K},\bm{\Sigma}_{1:K}^{-1}\), while \(p_{\bm{\beta}_{0}}\left(z_{1}^{n}|\bm{x}_{0}^{n}\right)\)is a multinomial logistic (softmax) layer that outputs a probability over discrete latent \(z_{1}^{n}\), which selects which of \(K\) experts to use in predicting \(\bm{x}_{1}^{n}\). \(p_{\bm{\beta}_{1}}\left(y^{n}|\bm{x}_{1}\right)\) parameterizes a final (softmax) likelihood over the label \(y^{n}\). A Bayesian network representation of the two layer CMN architecture is shown in Figure 1.

### Coordinate ascent variational inference with conjugate priors

In this section we summarize a variational approach for inverting the probabilistic model described in Equation (1). We posit the following approximate posterior over latents and parameters:

\[p\left(\bm{X}_{1},\bm{Z}_{1},\bm{\Theta}|Y,\bm{X}\right)\approx q\left(\bm{ \Theta}\right)\prod_{n=1}^{N}q\left(z_{1}^{n}\right)q\left(\bm{x}_{1}^{n}|z_{1 }^{n}\right)\qquad\quad q\left(\bm{\Theta}\right)=q\left(\bm{\beta}_{1} \right)q\left(\bm{\beta}_{0}\right)q(\bm{\lambda}_{1})\] (2)

where \(q\left(\bm{x}_{1}^{n},z_{1}^{n}\right)\) corresponds to an approximate posterior over continuous \(\bm{x}_{1}\) and discrete \(z_{1}\) latent variables.

The mean-field factorized form of the approximate posterior (Svensen, 2003), combined with conjugate priors over the parameters \(\bm{\Theta}\) (see Appendix A for their form), allows us to derive conditionally-conjugate updates for \(q\left(\bm{X}_{1},Z_{1}\right)\) and \(q\left(\bm{\Theta}\right)\). We use an iterative update scheme for the parameters of the approximate posterior, often referred to as variational Bayesian expectation maximisation (VBEM) (Beal, 2003) or coordinate ascent variational inference (CAVI) (Bishop and Nasrabadi, 2006). This consists in alternating updates to the posterior over latents and the posterior over parameters, split into a variational E-step and a variational M-step. Each step maximizes the evidence lower bound (ELBO), conditioned on the current setting of the other factor (i.e., \(q\left(\bm{X}_{1},Z_{1}\right)\) or \(q\left(\bm{\Theta}\right)\)).

Figure 1: A Bayesian network representation of the two-layer conditional mixture network, with an input-output pair \(\bm{x}_{0}^{n},y^{n}\) and latent variables \(\bm{x}_{1}^{n},z_{1}^{n}\). Observations are shaded nodes, while latents and parameters are transparent.

Update to latents ('E-step')

\[q_{t}\left(\bm{x}_{1}^{n},z_{1}^{n}\right) \propto\exp\left\{\mathbb{E}_{q_{t-1}(\bm{\Theta})}\left[\ln p_{\bm{ \Theta}}(y^{n},\bm{x}_{1}^{n},z_{1}^{n}|\bm{x}_{0}^{n})\right]\right\}\] (3) Update to parameters ('M-step')

\[q_{t}\left(\bm{\Theta}\right) \propto\exp\left\{\mathbb{E}_{q_{t-1}(\bm{x}_{1}^{n},z_{1}^{n})} \left[\ln p_{\bm{\Theta}}(y^{n},\bm{x}_{1}^{n},z_{1}^{n}|\bm{x}_{0}^{n})\right]\right\}\]

The functional forms of these equations and the PG augmentation scheme needed to turn them into conditionally-conjugate updates, are given in detail in Appendix A and Appendix B.

## 3 Results

We fit CAVI-CMN on several real and synthetic datasets and compared it to three alternative inference methods for fitting the parameters of the CMN:

**MLE**: -- We obtained point estimates for the parameters \(\bm{\Theta}\) of the CMN using maximum-likelihood estimation (backpropagation to minimize the negative log likelihood).
**NUTS-HMC**: -- The No-U-Turn Sampler (NUTS), an extension to Hamiltonian Monte Carlo (HMC) that incorporates adaptive step sizes (Hoffman et al., 2014). This provides samples from a posterior distribution over \(\bm{\Theta}\).
**BBVI**: -- Black-Box Variational Inference (BBVI) method (Ranganath et al., 2014). BBVI maximizes the evidence lower bound (ELBO) using stochastic estimation of its gradients with respect to variational parameters.

Appendix C contains details of the hyperparameters used for each inference algorithm.

### Predictive performance and efficiency

We fit all the inference algorithms on the Pinwheels dataset (Johnson et al., 2016) and 7 datasets from the UCI Machine Learning repository (Kelly et al., 2024). The upper row of Figure 2 visualizes three

Figure 2: Performance and runtime results of the different inference algorithms on the ‘Pinwheel’ dataset from Johnson et al. (2016). The standard deviation (vertical lines) of the performance metric is depicted together with the mean estimate (circles) over different model initializations. The top row of subplots show test accuracy (top left); log predictive density (top center), and expected calibration error (top right) as a function of training set size. The bottom row shows runtime metrics as a function of increasing training set size: the number of iterations required to achieve convergence (lower left); and the total runtime (in seconds, lower right). See Appendix G for details on run-time metrics.

different performance metrics for the Pinwheels dataset as a function of the size of the training set. The CAVI-based approach achieves competitive test accuracy to MLE, as well as comparable log predictive density (LPD) and expected calibration error (ECE) to the other two Bayesian methods; all three Bayesian approaches outperform maximum likelihood estimation in LPD and ECE. This finding holds for 6 of the 7 datasets we tested (see Appendix E), and also holds across training set sizes, indicating robust sample efficiency and calibration. To further study the probabilistic performance of CAVI-CMN, we computed the widely applicable information criterion (WAIC), an approximate estimate of leave-one-out cross-validation (Vehtari et al., 2017; Watanabe and Opper, 2010). Table 1 shows the WAIC scores for all methods evaluated on 7 UCI datasets. The CAVI-CMN approach consistently provided higher WAIC scores compared to the MLE algorithm, and WAIC scores that were on par with BBVI and NUTS.

The bottom row of Figure 2 shows that across training set sizes, all three gradient-based algorithms 4 exhibit an increase in runtime as the number of training data increases (which also scales the number of parameters for BBVI and CAVI). However the rate of increase varies significantly across different algorithms, with CAVI-CMN approach showing the best scaling behavior, both in terms of steps-to-convergence and absolute runtime. CAVI-CMN's runtime also scales competitively with MLE and BBVI along two other dimensions of model complexity: input dimension \(d\) and number of expert learners \(K\) (see Appendix F).

Footnote 4: We excluded NUTS from this analysis because its runtime was in general always orders of magnitude larger than the other methods

Thus, CAVI-CMN retains the probabilistic benefits of state-of-the-art Bayesian methods, as measured by metrics like test accuracy, LPD, and ECE, while also offering substantial advantages in terms of computational efficiency.

## 4 Conclusion

We introduced CAVI-CMN, a computationally efficient Bayesian approach for conditional mixture networks (CMN) that outperforms maximum likelihood estimation (MLE) in terms of predictive performance and calibration, as measured by LPD and ECE, and is competitive in terms of test accuracy on held out data. CAVI-CMN offers significant computational advantages over other Bayesian methods like Black-Box Variational Inference (BBVI) and the No-U-Turn Sampler (NUTS). While NUTS excels in inference quality, its computational cost is prohibitive for complex models. BBVI, though efficient, converges slower and has overall slower run-time than CAVI when applied to the CMN model.

The benchmark results demonstrate that CAVI-CMN matches the performance of BBVI and NUTS in terms of predictive accuracy, log-predictive density, and expected calibration error, while being considerably faster. The conjugate-exponential form of CAVI-CMN also makes it amenable to online learning with mini-batches of data, suggesting the extension of CAVI-CMN to deeper architectures and larger datasets. Overall, CAVI-CMN presents a promising tool for building fast, gradient-free and scalable Bayesian machine learning models.

## Acknowledgments and Disclosure of Funding

We thank the anonymous reviewers for their valuable feedback. The authors would like to thank the members of the VERSES Machine Learning Foundations and Intelligent Systems groups for critical discussions and feedback that improved the quality of this work, with special thanks to Tommaso Salvatori, Tim Verbelen, Magnus Koudahl, Toon Van de Maele, Hampus Linander, and Karl Friston.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} \hline  & Rice & Breast Cancer & Waveform & Vehicle Silh. & Banknote & Sonar & Iris \\ \hline CAVI & -0.1820 & -0.0504 & **-0.2921** & **-0.3281** & -0.0206 & -0.1544 & -0.0747 \\ \hline MLE & -0.3599 & -0.3133 & -0.5759 & -0.7437 & -0.3133 & -0.3133 & -0.5514 \\ \hline NUTS & **-0.1278** & **-0.0324** & -0.3753 & -0.3767 & **-0.0110** & **-0.0306** & **-0.0413** \\ \hline BBVI & -0.1739 & -0.0763 & -0.3618 & -0.4154 & -0.0382 & -0.0583 & -0.1544 \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of widely-applicable information criterion (WAIC) for different methods evaluated on 7 different UCI datasets. The highest WAIC score for each dataset is highlighted in boldface.

## References

* Cammeo and Osmancik (2019) Rice (Cammeo and Osmancik). UCI Machine Learning Repository, 2019. DOI: https://doi.org/10.24432/C5MW4Z.
* Amari (1993) Shun-ichi Amari. Backpropagation and stochastic gradient descent method. _Neurocomputing_, 5(4-5):185-196, 1993.
* Beal (2003) Matthew James Beal. _Variational algorithms for approximate Bayesian inference_. University of London, University College London (United Kingdom), 2003.
* Bernardo et al. (2007) JM Bernardo, MJ Bayarri, JO Berger, AP Dawid, D Heckerman, AFM Smith, and M West. Generative or discriminative? getting the best of both worlds. _Bayesian statistics_, 8(3):3-24, 2007.
* Bishop and Nasrabadi (2006) Christopher M Bishop and Nasser M Nasrabadi. _Pattern recognition and machine learning_, volume 4. Springer, 2006.
* Bishop and Svenskn (2003) Christopher M. Bishop and Markus Svenskn. Bayesian hierarchical mixtures of experts. In _UAI'03 Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence_, pages 57-64. Morgan Kaufmann Publishers Inc., 2003. ISBN 0-127-05664-5.
* Blei et al. (2017) David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. _Journal of the American statistical Association_, 112(518):859-877, 2017.
* Breiman and Stone (1988) L. Breiman and C.J. Stone. Waveform Database Generator (Version 1). UCI Machine Learning Repository, 1988. DOI: https://doi.org/10.24432/C5CS3C.
* 485, 2019. doi: 10.1214/19-STS712. URL https://doi.org/10.1214/19-STS712.
* Gormley and Fruhwirth-Schnatter (2019) Isobel Claire Gormley and Sylvia Fruhwirth-Schnatter. Mixture of experts models. In _Handbook of mixture analysis_, pages 271-307. Chapman and Hall/CRC, 2019.
* Hoffman et al. (2013) Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. _Journal of Machine Learning Research_, 2013.
* Hoffman et al. (2014) Matthew D Hoffman, Andrew Gelman, et al. The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo. _J. Mach. Learn. Res._, 15(1):1593-1623, 2014.
* Jacobs et al. (1991) Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. _Neural computation_, 3(1):79-87, 1991.
* Johnson et al. (2016) Matthew J Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta. Composing graphical models with neural networks for structured representations and fast inference. _Advances in neural information processing systems_, 29, 2016.
* Kelly et al. (2024) Markelle Kelly, Rachel Longjohn, and Kolby Nottingham. The uci machine learning repository, 2024. URL https://archive.ics.uci.edu. https://archive.ics.uci.edu.
* Linderman et al. (2015) Scott Linderman, Matthew J Johnson, and Ryan P Adams. Dependent multinomial models made easy: Stick-breaking with the polya-gamma augmentation. _Advances in neural information processing systems_, 28, 2015.
* Lohweg (2013) Volker Lohweg. Banknote Authentication. UCI Machine Learning Repository, 2013. DOI: https://doi.org/10.24432/C55P57.
* Moworth and Shepherd (2016) Pete Moworth and Barry Shepherd. Sttalog (Vehicle Silhouettes). UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5HG6N.
* Papamakrou et al. (2024) Theodore Papamakrou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin, Philipp Hennig, Aliaksandr Hubin, et al. Position paper: Bayesian deep learning in the age of large-scale ai. _arXiv preprint arXiv:2402.00809_, 2024.
* Papamakrou et al. (2015)Jooyoung Park and Irwin W Sandberg. Universal approximation using radial-basis-function networks. _Neural computation_, 3(2):246-257, 1991.
* Polson et al. [2013] Nicholas G Polson, James G Scott, and Jesse Windle. Bayesian inference for logistic models using polya-gamma latent variables. _Journal of the American statistical Association_, 108(504):1339-1349, 2013.
* Ranganath et al. [2014] Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In _Artificial intelligence and statistics_, pages 814-822. PMLR, 2014.
* Sejnowski and Gorman [2013] Terry Sejnowski and R. Gorman. Connectionist Bench (Sonar, Mines vs. Rocks). UCI Machine Learning Repository. DOI: https://doi.org/10.24432/C5T01Q.
* Shao et al. [2020] Zhihui Shao, Jianyi Yang, and Shaolei Ren. Calibrating deep neural network classifiers on out-of-distribution datasets. _arXiv preprint arXiv:2006.08914_, 2020.
* Bishop Markus Svensen [2003] Christopher M Bishop Markus Svensen. Bayesian hierarchical mixtures of experts. In _To appear in: Uncertainty in Artificial Intelligence: Proceedings of the Nineteenth Conference_, page 1, 2003.
* Vehtari et al. [2017] Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation using leave-one-out cross-validation and waic. _Statistics and computing_, 27:1413-1432, 2017.
* Wainwright et al. [2008] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. _Foundations and Trends(r) in Machine Learning_, 1(1-2):1-305, 2008.
* Wang et al. [2021] Deng-Bao Wang, Lei Feng, and Min-Ling Zhang. Rethinking calibration of deep neural networks: Do not be afraid of overconfidence. _Advances in Neural Information Processing Systems_, 34:11809-11820, 2021.
* Watanabe and Opper [2010] Sumio Watanabe and Manfred Opper. Asymptotic equivalence of bayes cross validation and widely applicable information criterion in singular learning theory. _Journal of machine learning research_, 11(12), 2010.
* Wolberg et al. [1995] William Wolberg, Olvi Mangasarian, Nick Street, and W. Street. Breast Cancer Wisconsin (Diagnostic). UCI Machine Learning Repository, 1995. DOI: https://doi.org/10.24432/C5DW2B.
* Zhuang et al. [2020] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. _Advances in neural information processing systems_, 33:18795-18806, 2020.

## Code availability

The code for using CAVI and the other 3 methods to fit the CMN model on the pinwheel and UCI datasets is available on the 'cavi-cmn' repository, hosted on the VersesTech GitHub organization: https://github.com/VersesTech/cavi-cmn.

## Appendix A Coordinate ascent variational inference for conditional mixture networks

In this section we detail a variational approach for inverting the probabilistic model described in Equation (1) and computing an approximate posterior over latents and parameters specified as

\[p\left(\boldsymbol{X}_{1},\boldsymbol{Z}_{1},\boldsymbol{\Theta}|Y, \boldsymbol{X}\right)=\frac{p\left(Y,\boldsymbol{X}_{1},\boldsymbol{Z}_{1}, \boldsymbol{\Theta},\boldsymbol{X}\right)}{p\left(Y|\boldsymbol{X}\right)} \approx q\left(\boldsymbol{\Theta}\right)\prod_{n=1}^{N}q\left(z_{1}^{n} \right)q\left(\boldsymbol{x}_{1}^{n}|z_{1}^{n}\right)\] (4)

where \(q\left(\boldsymbol{x}_{1}^{n}|z_{1}^{n}\right)\) corresponds to a component specific multivariate normal distribution, and \(q\left(z_{1}^{n}\right)\) to a multinomial distribution. Importantly, the approximate posterior over parameters \(q\left(\boldsymbol{\Theta}\right)\) further factorizes (Svensen, 2003) as

\[q\left(\boldsymbol{\Theta}\right) =\prod_{l=1}^{L-1}q\left(\boldsymbol{\beta}_{l,1}\right)\prod_{k= 1}^{K-1}q\left(\boldsymbol{\beta}_{k,0}\right)\underbrace{\prod_{j=1}^{K}q \left(\boldsymbol{A}_{j},\boldsymbol{\Sigma}_{j}^{-1}\right)}_{=q\left( \boldsymbol{\lambda}_{1}\right)}\] (5) \[q\left(\boldsymbol{\beta}_{l,1}\right) =\mathcal{N}\left(\boldsymbol{\beta}_{l,1};\boldsymbol{\mu}_{l, 1},\boldsymbol{\Sigma}_{l,1}\right)\] \[q\left(\boldsymbol{\beta}_{k,0}\right) =\mathcal{N}\left(\boldsymbol{\beta}_{k,0};\boldsymbol{\mu}_{k,0 },\boldsymbol{\Sigma}_{k,0}\right)\] \[q\left(\boldsymbol{A}_{j}|\boldsymbol{\Sigma}_{j}^{-1}\right) =\mathcal{M}\mathcal{N}\left(\boldsymbol{A}_{j};\boldsymbol{M}_{ j},\boldsymbol{\Sigma}_{j},\boldsymbol{V}_{j}\right)\] \[q\left(\boldsymbol{\Sigma}_{j}^{-1}\right) =\prod_{i=1}^{h}\Gamma\left(\sigma_{i,j}^{-2};a_{j},b_{i,j}\right)\]

We use the following conjugate priors for the parameters of the linear experts \(\boldsymbol{\lambda}_{1}=\left(\boldsymbol{A}_{1:K},\boldsymbol{\Sigma}_{1:K} ^{-1}\right)\) and the regression coefficients \(\boldsymbol{\beta}_{0},\boldsymbol{\beta}_{1}\):

\[p\left(\boldsymbol{A}_{k}|\boldsymbol{\Sigma}_{k}^{-1}\right) =\mathcal{M}\mathcal{N}\left(\boldsymbol{A}_{k};\boldsymbol{0}, \boldsymbol{\Sigma}_{k},v_{0}\boldsymbol{I}_{d+1}\right)\] (6) \[p\left(\boldsymbol{\Sigma}_{k}^{-1}\equiv\text{diag}\left( \boldsymbol{\sigma}_{k}^{-2}\right)\right) =\prod_{i=1}^{h}\Gamma\left(\sigma_{k,i}^{-2};a_{0},b_{0}\right)\] \[p\left(\boldsymbol{\beta}_{k,0}\right) =\mathcal{N}\left(\boldsymbol{\beta}_{k,0};\boldsymbol{0},\sigma_ {0}^{2}\boldsymbol{I}_{d+1}\right)\] \[p\left(\boldsymbol{\beta}_{l,1}\right) =\mathcal{N}\left(\boldsymbol{\beta}_{l,1};\boldsymbol{0},\sigma _{1}^{2}\boldsymbol{I}_{h+1}\right)\]

The above form of the approximate posterior, in combination with the conjugate priors in Equation (6), allows us to define tractable conditionally conjugate updates for each factor. This becomes evident from the following expression for the evidence lower bound (ELBO) on the marginal log likelihood

\[\mathcal{L}(q)=\mathbb{E}_{q\left(\boldsymbol{X}_{1},\boldsymbol{Z}_{1}\right) q\left(\boldsymbol{\Theta}\right)}\left[\sum_{n=1}^{N}\ln\frac{p_{\boldsymbol{ \Theta}}\left(y^{n},\boldsymbol{x}_{1}^{n},z_{1}^{n}|\boldsymbol{x}_{0}^{n} \right)}{q\left(z_{1}^{n}\right)q\left(\boldsymbol{x}_{1}^{n}|z_{1}^{n} \right)}\right]+\mathbb{E}_{q\left(\boldsymbol{\Theta}\right)}\left[\ln\frac{p \left(\boldsymbol{\beta}_{1}\right)p\left(\boldsymbol{\beta}_{0}\right)p \left(\boldsymbol{\lambda}_{1}\right)}{q\left(\boldsymbol{\beta}_{1}\right) q\left(\boldsymbol{\beta}_{0}\right)q\left(\boldsymbol{\lambda}_{1}\right)}\right]\] (7)

We maximize the ELBO using an iterative update scheme for the parameters of the approximate posterior, often referred to as variational Bayesian expectation maximisation (VBEM) (Beal, 2003) or coordinate ascent variational inference (CAVI) (Bishop and Nasrabadi, 2006; Blei et al., 2017). The procedure consists of two parts:First, we fix the posterior over the parameters (to randomly initialized values). Given the posterior over parameters, we update the posterior over latent variables (variational E-step) as

\[\begin{split} q_{t}\left(\bm{x}_{1}^{n}|z_{1}^{n}\right)& \propto\exp\left\{\mathbb{E}_{q_{t-1}(\bm{\beta}_{1})q_{t-1}(\bm{ \lambda}_{1})}\left[\ln p_{\bm{\beta}_{1}}\left(y^{n}|\bm{x}_{1}^{n}\right)+ \ln p_{\bm{\lambda}_{1}}\left(\bm{x}_{1}^{n}|\bm{x}_{0}^{n},z_{1}^{n}\right) \right]\right\}\\ q_{t}\left(z_{1}^{n}\right)&\propto\exp\left\{ \mathbb{E}_{q_{t-1}(\bm{\Theta})}\left[\left\langle\ln p_{\bm{\beta}_{1},\bm{ \lambda}_{1}}\left(y^{n},\bm{x}_{1}^{n}|\bm{x}_{0}^{n},z_{1}^{n}\right) \right\rangle_{q_{t}\left(\bm{x}_{1}^{n}|z_{1}^{n}\right)}+\ln p_{\bm{\beta}_ {0}}\left(z_{1}^{n}|\bm{x}_{0}^{n}\right)\right]\right\}\end{split}\] (8)

Second, the posterior over latents that was updated in the E-step, is used to update the posterior over parameters (variational M-step) as

\[\begin{split} q_{t}\left(\bm{\beta}_{1}\right)& \propto\exp\left\{\sum_{n=1}^{N}\mathbb{E}_{q_{t}\left(\bm{x}_{1}^{n},z_{1}^{ n}\right)}\left[\ln p_{\bm{\beta}_{1}}\left(y^{n}|\bm{x}_{1}^{n}\right) \right]\right\}\\ q_{t}\left(\bm{\beta}_{0}\right)&\propto\exp\left\{ \sum_{n=1}^{N}\mathbb{E}_{q_{t}\left(z_{1}^{n}\right)}\left[\ln p_{\bm{\beta}_ {1}}\left(z_{1}^{n}|\bm{x}_{0}^{n}\right)\right]\right\}\\ q_{t}\left(\bm{\lambda}_{1}\right)&\propto\exp\left\{ \sum_{n=1}^{N}\mathbb{E}_{q_{t}\left(\bm{x}_{1}^{n},z_{1}^{n}\right)}\left[\ln p _{\bm{\lambda}_{1}}\left(\bm{x}_{1}^{n}|z_{1}^{n},\bm{x}_{0}^{n}\right)\right] \right\}\end{split}\] (9)

In the variational inference literature, the distinction between latents and parameters is often described in terms of 'local' vs 'global' latent variables (Hoffman et al., 2013), where local variables are datapoint-specific, and global variables are shared across datapoints. To detail the form of the updates to the parameters of the linear experts in Equation (9), i.e. \(q_{t}(\bm{\lambda}_{1})=q_{t}(\bm{A}_{1:K},\bm{\Sigma}_{1:K}^{-1})\), first we note the form of the approximate posteriors over the latent variables \(q(\bm{X}_{1},Z_{1})\):

\[q\left(\bm{X}_{1}|Z_{1}\right) =\prod_{n=1}^{N}\prod_{k=1}^{K}\mathcal{N}(\bm{x}_{1}^{n};\bm{ \mu}_{k,1}^{n},\bm{\Sigma}_{k,1}^{n})\] \[q\left(Z_{1}\right) =\prod_{n=1}^{N}\text{Cat}(z_{1}^{n};\bm{\gamma}^{n})\] (10)

The update to the \(k^{\text{th}}\) expert's parameters \(q(\bm{A}_{k},\bm{\Sigma}_{k}^{-1})\) can written in terms of weighted updates to the Matrix Normal Gamma's canonical parameters \(\bm{M}_{k},\bm{V}_{k},a_{k}\) and \(b_{k}\), where the weights are provided by the sufficient statistics of \(\left\{q\left(\bm{x}_{1}^{1}|z_{1}^{1}\!=\!k\right),q\left(\bm{x}_{1}^{2}|z_{1}^ {2}\!=\!k\right),\ldots,q\left(\bm{x}_{1}^{N}|z_{1}^{N}\!=\!k\right)\right\}\):

\[\begin{split}\bm{V}_{k}^{-1}&=\bm{V}_{k,0}^{-1}+ \sum_{n=1}^{N}\gamma_{k}^{n}\bm{x}_{0}^{n}\left(\bm{x}_{0}^{n}\right)^{\top}\\ \bm{M}_{k}&=\left(\bm{M}_{k,0}\bm{V}_{k,0}^{-1}+ \sum_{n=1}^{N}\gamma_{k}^{n}\bm{\mu}_{k,1}^{n}\left(\bm{x}_{0}^{n}\right)^{\top} \right)\bm{V}_{k}\\ a_{k}&=a_{k,0}+\frac{\sum_{n=1}^{N}\gamma_{k}^{n}}{ 2}\\ b_{i,k}&=b_{i,k,0}+\frac{1}{2}\left(\sum_{n=1}^{N} \gamma_{k}^{n}\left[\bm{\Sigma}_{k,1}^{n}+\bm{\mu}_{k,1}^{n}(\bm{\mu}_{k,1}^{ n})^{\top}\right]_{ii}-\left[\bm{M}_{k}\bm{V}_{k}^{-1}\bm{M}_{k}^{T}\right]_{ii}+ \left[\bm{M}_{k,0}\bm{V}_{k,0}^{-1}\bm{M}_{k,0}^{T}\right]_{ii}\right)\end{split}\] (11)

where the notation \(\left[\cdot\right]_{ii}\) selects the \(i^{\text{th}}\) element of the diagonal of the matrix in the brackets.

However, the update equations described in Equation (8) and in the first two lines of Equation (9) for \(q(\bm{\beta}_{0}),q(\bm{\beta}_{1})\) are not computationally tractable without an additional approximation, known as Polya-Gamma augmentation of the multinomial distribution. The full details of the augmentation procedure are described below in Appendix B. Here we will briefly sketch the main steps and describe the high level, augmented update equations. The Polya-Gamma augmentation introduces datapoint-specific auxiliary variables \((\bm{\omega}_{1}^{n},\bm{\omega}_{0}^{n})\), that help us transform the log-probability of the multinomial distribution into a quadratic function (Polson et al., 2013; Linderman et al., 2015) over coefficients \((\bm{\beta}_{1},\bm{\beta}_{0})\), and latents \(\bm{x}_{1}^{n}\). This quadratic form enables tractable update of \(q\left(\bm{x}_{1}^{n}|z_{1}^{n}\right)\) in the form of a multivariate normal distribution, and a tractable updating of posteriors over coefficients \(q\left(\bm{\beta}_{1}\right)\) and \(q\left(\bm{\beta}_{0}\right)\).

With the introduction of the auxiliary variables the variational expectation and maximisation steps are expressed as

Update latents ('E-step')

\[q_{t}\left(\bm{x}_{1}^{n}|z_{1}^{n}\right) \propto\exp\left\{\mathbb{E}_{q_{t-1}(\bm{\beta}_{1})q_{t-1}( \bm{\lambda}_{1})}\left[\left\langle l\left(y^{n},\bm{x}_{1}^{n},\bm{\omega}_ {1}^{n},\bm{\beta}_{1}\right)\right\rangle_{q_{t-1}\left(\bm{\omega}_{1}^{n}| y^{n}\right)}+\ln p_{\bm{\lambda}_{1}}\left(\bm{x}_{1}^{n}|\bm{x}_{0}^{n},z_{1}^{n} \right)\right]\right\}\] \[q_{t}\left(\bm{\omega}_{1}|y^{n}\right) \propto p\left(\bm{\omega}_{1}^{n}|y_{n}\right)\exp\left\{\mathbb{E }_{q_{t-1}(\bm{\beta}_{1})q_{t}\left(\bm{x}_{1}^{n}|z_{1}^{n}\right)}\left[l \left(y^{n},\bm{x}_{1}^{n},\bm{\omega}_{1}^{n},\bm{\beta}_{1}\right)\right]\right\}\] \[q_{t}\left(\bm{\omega}_{0}|z_{1}^{n}\right) \propto p\left(\bm{\omega}_{0}^{n}|z_{1}^{n}\right)\exp\left\{ \mathbb{E}_{q_{t-1}(\bm{\beta}_{0})}\left[l\left(z_{1}^{n},\bm{x}_{0}^{n},\bm {\omega}_{0}^{n},\bm{\beta}_{0}\right)\right]\right\}\] \[q_{t}\left(z_{1}^{n}\right) \propto\exp\left\{\mathbb{E}_{q_{t-1}(\bm{\Theta})}\left[\bar{l}_ {z_{1}^{n},t}\left(y^{n},\bm{\beta}_{1}\right)+R_{z_{1}^{n},t}\left(\bm{x}_{0} ^{n},\bm{\lambda}_{1}\right)+\bar{l}_{t}\left(z_{1}^{n},\bm{x}_{0}^{n},\bm{ \beta}_{0}\right)\right]\right\}\]

Update parameters ('M-step')

\[q_{t}\left(\bm{\beta}_{1}\right) \propto\exp\left\{\sum_{n=1}^{N}\mathbb{E}_{q_{t}\left(\bm{x}_{1} ^{n},z_{1}^{n}\right)q_{t}\left(\bm{\omega}_{1}^{n}|y^{n}\right)}\left[l(y^{n},\bm{x}_{1}^{n},\bm{\beta}_{1},\bm{\omega}_{1}^{n})\right]\right\}\] \[q_{t}\left(\bm{\beta}_{0}\right) \propto\exp\left\{\sum_{n=1}^{N}\mathbb{E}_{q_{t}\left(z_{1}^{n} \right)q_{t}\left(\bm{\omega}_{0}^{n}|z_{1}^{n}\right)}\left[l(z_{1}^{n},\bm{x }_{0}^{n},\bm{\beta}_{0},\bm{\omega}_{0}^{n})\right]\right\}\]

where we skipped the terms whose form did not change. \(R_{z_{1}^{n},t}\left(\bm{x}_{0}^{n},\bm{\lambda}_{1}\right)\) reflects a contribution to \(q(z_{1}^{n})\) that depends on the expected log partition of the linear (Matrix Normal Gamma) likelihood \(p_{\bm{\lambda}_{1}}(\bm{x}_{1}^{n}|\bm{x}_{0}^{n},z_{1}^{n})\). Note that the updates to each subset of posteriors (latents or parameters) have an analytic form due to the conditional conjugacy of the model. Importantly, both priors and posterior of the auxiliary variables are Polya-Gamma distributed (Polson et al., 2013).

Finally, in the above update equations, we have replaced instances of the multinomial distribution \(p\left(z|\bm{x},\bm{\beta}\right)\) with its augmented form \(p\left(\omega|z\right)e^{l(z,\bm{x},\bm{\mu},\bm{\beta})}\) where the function \(l\left(\cdot\right)\) is quadratic with respect to the coefficients \(\bm{\beta}\) and the input variables \(\bm{x}\), leading to tractable update equations.

## Appendix B Variational Bayesian Multinomial Logistic Regression

In this section, we focus on a single multinomial logistic regression model (not in the context of the CMN), but the ensuing variational update scheme derived in Appendix B.4 is applied in practice to both the gating network's parameters \(\bm{\beta}_{0}\) as well as those of the final output likelihood for the class label \(\bm{\beta}_{1}\).

### Stick-breaking reparameterization of a multinomial distribution

Multinomial logistic regression considers the probability that an outcome variable \(y\) belongs to one of \(K\) mutually-exclusive classes or categories. The probability of \(y\) belonging to the \(k^{\text{th}}\) class is given by the categorical likelihood:\[p(y=k|\bm{x},\bm{\beta})=p_{k}\] (13)

The problem of multinomial logistic regression is to identify or estimate the values of regression coefficients \(\bm{\beta}\) that explain the relationship between some dataset of given continuous input regressors \(\bm{X}=(\bm{x}^{1},\bm{x}^{2},\ldots,\bm{x}^{N})\) and corresponding categorical labels \(Y=(y^{1},y^{2},\ldots,y^{N}),y^{n}\in 1,2,\ldots,K\).

We can use a stick-breaking construction to parameterize the likelihood over \(y\) using a set of \(K-1\) stick-breaking coefficients: \(\bm{\pi}=(\pi_{1},\ldots,\pi_{K-1})\). Each coefficient is parameterized with an input regressor \(\bm{x}\), and a corresponding set of regression weights \(\bm{\beta}_{j}\). Stick-breaking coefficient \(\pi_{j}\) is then given by a sigmoid transform of the product of the regression weights and the input regressors:

\[\pi_{j} =\sigma\left(\bm{\beta}_{j}\left[\bm{x};1\right]\right)\;,\] (14) \[\text{where}\;\;\sigma\left(\bm{\beta}_{j}\left[\bm{x};1\right]\right) =\frac{1}{1+\exp\left\{-\bm{\beta}_{j}\left[\bm{x};1\right] \right\}}\;,\] \[\text{and}\;\;\bm{\beta}_{j}\left[\bm{x};1\right] =\sum_{i=1}^{d}w_{j,i}x_{i}+a_{j}.\]

The outcome likelihood is then obtained via stick breaking transform5 as follows

Footnote 5: This blog post has helpful discussion on the stick-breaking form of the multinomial logistic likelihood and provides more intuition behind its functional form.

\[p_{k}=\pi_{K}\prod_{j=1}^{K-1}(1-\pi_{j})=\sigma\left(\bm{\beta}_{K}\left[\bm{ x};1\right]\right)\prod_{j=1}^{K-1}\left(1-\sigma\left(\bm{\beta}_{j}\left[ \bm{x};1\right]\right)\right)=\prod_{j=1}^{K-1}\frac{\exp\left\{\bm{\beta}_{j} \left[\bm{x};1\right]\right\}}{1+\exp\left\{\bm{\beta}_{j}\left[\bm{x};1\right] \right\}}\] (15)

where \(\pi_{K}=1\), and \(\bm{\beta}_{K}=\vec{0}\).

Finally, we can express the likelihood in the form of a Categorical distribution as

\[\text{Cat}\left(y;\bm{x},\bm{\beta}\right)=\prod_{k=1}^{K-1}\frac{\left(\exp \left\{\bm{\beta}_{k}\left[\bm{x};1\right]\right\}\right)^{\delta_{k,y}}}{ \left(1+\exp\left\{\bm{\beta}_{k}\left[\bm{x};1\right]\right\}\right)^{N_{k,y }}}\;.\] (16)

where \(N_{k,y}=1\) for \(k\leq y\), and \(N_{k,y}=0\) otherwise (or \(N_{k,y}=1-\sum_{j=1}^{k-1}\delta_{j,y}\)), and \(\delta_{k,y}=1\) for \(k=y\) and is zero otherwise.

### Polya-Gamma augmentation

The _Polya-Gamma augmentation_ scheme (Polson et al., 2013; Linderman et al., 2015; Durante and Rigon, 2019) is defined as

\[\frac{\left(e^{\psi}\right)^{a}}{\left(1+e^{\psi}\right)^{b}}=2^{-b}e^{\kappa \psi}\int_{0}^{\infty}e^{-\omega\psi^{2}/2}p(\omega)\,\mathrm{d}\omega\] (17)

where \(\kappa=a-b/2\) and \(p\left(\omega|b,0\right)\) is the density of the Polya-Gamma distribution \(\text{PG}(b,0)\) which does not depend on \(\psi\). The useful properties of the Polya-Gamma are the exponential tilting property expressed as

\[PG(\omega;b,\psi)=\frac{e^{-\omega\psi^{2}/2}PG(\omega;b,0)}{\mathbb{E}\left[ e^{-\omega\psi^{2}/2}\right]}\] (18)the expected value of \(\omega\), and \(e^{-\omega\psi^{2}/2}\) given as

\[\begin{split}\mathbb{E}\left[\omega\right]&=\int_{o}^{ \infty}\omega PG(\omega;b,\psi)\,\mathrm{d}\omega=\frac{b}{2\psi}\tanh\left( \frac{\psi}{2}\right),\\ \mathbb{E}\left[e^{-\omega\psi^{2}/2}\right]&=\cosh^ {-b}\left(\frac{\psi}{2}\right)\end{split}\] (19)

and the Kulback-Leibler divergence between \(q\left(\omega\right)=PG(\omega;b,\psi)\) and \(p\left(\omega\right)=PG(\omega;b,0)\) obtained as

\[D_{KL}\left[q\left(\omega\right)||p\left(\omega\right)\right]=-\mathbb{E} \left[\omega\right]\frac{\psi^{2}}{2}+b\ln\cosh\left(\frac{\psi}{2}\right)=- \frac{b\psi}{4}\tanh\left(\frac{\psi}{2}\right)+b\ln\cosh\left(\frac{\psi}{2} \right)\;.\] (20)

We can express the likelihood function in Equation (16) using the augmentation as

\[\begin{split} p(y,\bm{\omega}|\bm{\psi})&=p\left( y|\bm{\psi}\right)p\left(\bm{\omega}|y,\bm{\psi}\right)=\prod_{k=1}^{K-1}2^{-b_{k,y}}e^{ \kappa_{k,y}\psi_{k}-\omega_{k}\psi_{k}^{2}/2}\text{PG}(\omega_{k};b_{k,y},0) \\ p\left(y|\bm{\psi}\right)&=\prod_{k=1}^{K-1}2^{-b_{ k,y}}e^{\kappa_{k,y}\psi_{k}}\int_{0}^{\infty}e^{-\omega_{k}\psi_{k}^{2}/2} \text{PG}(\omega_{k};b_{k,y},0)\,\mathrm{d}\omega_{k}\\ p\left(\bm{\omega}|y,\bm{\psi}\right)&=\prod_{k=1}^ {K-1}\text{PG}\left(\omega_{k};b_{k,y},\psi_{k}\right)\end{split}\] (21)

where \(b_{k,y}\equiv N_{k,y}\), \(\kappa_{k,y}=\delta_{k,y}-N_{k,y}/2\), and \(\psi_{k}=\bm{\beta_{k}}\left[\bm{x};1\right]\). Given a prior distribution \(p\left(\bm{\psi}\right)=p\left(\bm{\beta}\right)p\left(\bm{x}\right)\), we can write the joint \(p\left(y,\bm{\omega},\bm{\psi}\right)\) as

\[\begin{split} p\left(y,\bm{\omega},\bm{\psi}\right)& =p\left(\bm{\omega}|y\right)p\left(\bm{\psi}\right)e^{l\left(y, \bm{\psi},\bm{\omega}\right)}\;,\\ l\left(y,\bm{\psi},\bm{\omega}\right)&=\sum_{k=1}^ {K-1}l_{k}\left(y,\psi_{k},\omega_{k}\right)\;,\\ l_{k}\left(y,\psi_{k},\omega_{k}\right)&=\kappa_{ y,k}\psi_{k}-b_{y,k}\ln 2-\omega_{k}\psi_{k}^{2}/2\;.\end{split}\] (22)

### Evidence lower-bound

Given a set of observations \(\bm{\mathcal{D}}=\left(y^{1},\ldots,y^{N}\right)\) the augmented joint distribution can be expressed as

\[p\left(\bm{\mathcal{D}},\bm{\Omega},\bm{X},\bm{\beta}\right)=p\left(\bm{\beta }\right)\prod_{n=1}^{N}p\left(\bm{x}^{n}\right)p\left(\bm{\omega}^{n}|y^{n} \right)e^{l\left(y^{n},\bm{\psi}^{n},\bm{\omega}^{n}\right)}\]

We can express the evidence lower-bound (ELBO) as

\[\begin{split}\mathcal{L}(q)&=E_{q\left(\bm{\Omega} \right)q\left(\bm{X}\right)q\left(\bm{\beta}\right)}\left[-\ln q\left(\bm{ \beta}\right)+\sum_{n=1}^{N}\ln\frac{p\left(y^{n},\bm{\psi}^{n},\bm{\omega}^{n }\right)}{q\left(\bm{\omega}^{n}\right)q\left(\bm{x}^{n}\right)}\right]\\ &=E_{q\left(\bm{\Omega}\right)q\left(\bm{\beta}\right)}\left[\ln \frac{p\left(\bm{\beta}\right)}{q\left(\bm{\beta}\right)}+\sum_{n=1}^{N}l \left(y^{n},\bm{\psi}^{n},\bm{\omega}^{n}\right)+\ln\frac{p\left(\bm{\omega }^{n}|y^{n}\right)}{q\left(\bm{\omega}^{n}\right)}+\ln\frac{p\left(\bm{x}^{n} \right)}{q\left(\bm{\omega}^{n}\right)}\right]\\ &\geq\ln p\left(\bm{\mathcal{D}}\right)\end{split}\] (23)where we use the following forms for the approximate posterior

\[q\left(\boldsymbol{\Omega}|Y\right) =\prod_{n=1}^{N}q\left(\boldsymbol{\omega}^{n}|y^{n}\right)=\prod_{n =1}^{N}\prod_{k=1}^{K-1}PG\left(b_{k,y^{n}},\xi_{k,n}\right)\,\] \[q\left(\boldsymbol{X}\right) =\prod_{n=1}^{N}q\left(\boldsymbol{x}^{n}\right)=\prod_{n=1}^{N} \mathcal{N}\left(\boldsymbol{x}^{n};\boldsymbol{\mu}^{n},\boldsymbol{\Sigma}^{ n}\right)\,\] (24) \[q\left(\boldsymbol{\beta}\right) =\prod_{k=1}^{K-1}\mathcal{N}\left(\boldsymbol{\beta}_{k}; \boldsymbol{\mu}_{k},\boldsymbol{\Sigma}_{k}\right)\.\]

### Coordinate ascent variational inference for the PG-augmented model

The mean-field assumption in Equation (24) allows the implementation of a simple CAVI algorithm (Wainwright et al., 2008; Beal, 2003; Hoffman et al., 2013; Blei et al., 2017) which sequentially maximizes the evidence lower bound in Equation (23) with respect to each factor in \(q\left(\boldsymbol{\Omega}|Y\right)\,q\left(\boldsymbol{X}\right)\,q\left( \boldsymbol{\beta}\right)\), via the following updates:

Update to latents ('E-step')

\[q^{\left(t,l\right)}\left(\boldsymbol{x}^{n}\right) \propto p\left(\boldsymbol{x}^{n}\right)\exp\left\{\mathbb{E}_{q ^{\left(t-1\right)}\left(\boldsymbol{\beta}\right)q^{\left(t,l-1\right)} \left(\boldsymbol{\omega}^{n}\right)}\left[l\left(y^{n},\boldsymbol{\psi}^{n},\boldsymbol{\omega}^{n}\right)\right]\right\}\] \[q^{\left(t,l\right)}\left(\omega_{k}^{n}|y^{n}\right) \propto p\left(\omega_{k}^{n}|y^{n}\right)\exp\left\{\mathbb{E}_{q ^{\left(t-1\right)}\left(\boldsymbol{\beta}\right)q^{\left(t,l\right)}\left( \boldsymbol{x}^{n}\right)}\left[l_{k}\left(y^{n},\psi_{k}^{n},\omega_{k}^{n} \right)\right]\right\}\] \[\forall n \in\left\{1,\ldots,N\right\},\text{ and for }q^{\left(t,0\right)} \left(\boldsymbol{\omega}^{n}|y^{n}\right)=q^{\left(t-1,L\right)}\left( \boldsymbol{\omega}^{n}|y^{n}\right)\] (25)

Update to parameters ('M-step')

\[q^{\left(t\right)}\left(\boldsymbol{\beta}_{k}\right) \propto\exp\left\{\sum_{n=1}^{N}\mathbb{E}_{q^{\left(t\right)} \left(\boldsymbol{x}^{n}\right)q^{\left(t\right)}\left(\boldsymbol{\omega}^{n} \right|y^{n}\right)}\left[l\left(y^{n},\boldsymbol{\psi}^{n},\boldsymbol{\omega }^{n}\right)\right]\right\}\]

at each iteration \(t\), and multiple local iteration \(l\) during the variational expectation step--until the convergence of the ELBO.

Specifically, the update equations for the parameters of the latents (the 'E-step') are:

\[q^{\left(t,l\right)}\left(\boldsymbol{x}^{n}\right) \propto\mathcal{N}\left(\boldsymbol{x}^{n};0,-2\boldsymbol{\lambda }_{2,0}\right)\exp\left\{\sum_{k=1}^{K}\kappa_{k,y^{n}}\text{Tr}\left( \boldsymbol{\mu}_{k}^{\left(t-1\right)}\left[\boldsymbol{x}^{n};1\right]^{T} \right)-\frac{\left\langle\omega_{k}\right\rangle}{2}\text{Tr}\left( \boldsymbol{M}_{k}^{\left(t-1\right)}\left[\boldsymbol{x}^{n};1\right]\left[ \boldsymbol{x}^{n};1\right]^{T}\right)\right\}\] \[\boldsymbol{\lambda}_{1}^{\left(n,t,l\right)} =\sum_{k=1}^{K-1}\left\{\kappa_{k,y^{n}}\left[\boldsymbol{\mu}_{ k}^{\left(t-1\right)}\right]_{1:D}-\left\langle\omega_{k}^{n}\right\rangle_{t,l-1} \left[\boldsymbol{M}_{k}^{\left(t-1\right)}\right]_{D+1,1:D}\right\}\] \[\boldsymbol{\lambda}_{2}^{\left(n,t,l\right)} =\boldsymbol{\lambda}_{2,0}-\frac{1}{2}\sum_{k=1}^{K-1}\langle \omega_{k}^{n}\rangle_{t,l-1}\left[\boldsymbol{M}_{k}\right]_{1:D,1:D}\] \[\boldsymbol{M}_{k}^{\left(t-1\right)} =\boldsymbol{\Sigma}_{k}^{\left(t-1\right)}+\boldsymbol{\mu}_{k} ^{\left(t-1\right)}\left[\boldsymbol{\mu}_{k}^{\left(t-1\right)}\right]^{T}\] (26)

and

\[q^{\left(t,l\right)}\left(\omega_{k}^{n}|y^{n}\right) \propto e^{-\omega_{k}^{n}\langle\psi_{k}^{2}\rangle/2}PG\left( \omega_{k}^{n};b_{k,y^{n}},0\right)\] \[\xi_{k}^{n} =\sqrt{\mathbb{E}_{q^{\left(t-1\right)}\left(\boldsymbol{\beta} \right)q^{\left(t,l\right)}\left(\boldsymbol{x}^{n}\right)}\left[\psi_{k}^{2} \right]}\] \[\xi_{k}^{n} =\sqrt{\text{Tr}\left(\boldsymbol{M}_{k}^{\left(t-1\right)} \hat{\boldsymbol{M}}^{\left(n,t,l\right)}\right)}\] (27) \[\text{where }\hat{\boldsymbol{M}}^{\left(n,t,l\right)} =\begin{pmatrix}\boldsymbol{M}^{\left(n,t,l\right)}&\boldsymbol{ \mu}^{\left(n,t,l\right)}\\ \left[\boldsymbol{\mu}^{\left(n,t,l\right)}\right]^{T}&1\end{pmatrix}\,\text{ and }\boldsymbol{M}^{\left(n,t,l\right)}= \boldsymbol{\Sigma}^{\left(n,t,l\right)}+\boldsymbol{\mu}^{\left(n,t,l\right)} \left[\boldsymbol{\mu}^{\left(n,t,l\right)}\right]^{T}.\]Similarly, for the parameter updates ('M-step') we get

\[q^{(t)}\left(\bm{\beta}_{k}\right) \propto\mathcal{N}\left(\bm{\beta}_{k};0,-2\bm{\lambda}_{2,0}^{ \prime}\right)\exp\left\{\sum_{n=1}^{N}\kappa_{k,y^{n}}\text{Tr}\left(\hat{\bm{ \mu}}^{(n,t)}\bm{\beta}_{k}^{T}\right)-\frac{\langle\omega_{k}\rangle_{t}^{n}}{2 }\text{Tr}\left(\hat{\bm{M}}_{i}^{(t)}\bm{\beta}_{k}\bm{\beta}_{k}^{T}\right)\right\}\] \[\bm{\lambda}_{k,1}^{(t)} =\sum_{i}\kappa_{k,y^{n}}\hat{\bm{\mu}}^{(n,t)}\] (28) \[\bm{\lambda}_{k,2}^{(t)} =\bm{\lambda}_{2,0}^{\prime}-\frac{1}{4}\sum_{n=1}^{N}\frac{b_{k,y^{n}}}{\xi_{k,t}^{(n,t)}}\tanh\left(\frac{\xi_{k}^{(n,t)}}{2}\right)\hat{\bm {M}}^{(n,t)}\]

where \(\hat{\bm{\mu}}^{(n,t)}=\left[\bm{\mu}^{(n,t)};1\right]\).

## Appendix C Hyperparameters

### Common hyperparameters

For the Bayesian methods (CAVI, NUTS, and BBVI), we used the same form for the CMN priors (see Equation (6) for their parameterization) and fixed the prior parameters to the following values, used for all datasets: \(v_{0}=10\), \(a_{0}=2\), \(b_{0}=1\), \(\sigma_{0},\sigma_{1}=5\). For all datasets, we fixed the dimension of the continuous latent \(\bm{z}_{1}\) to be \(h=L-1\), where \(L\) is the number of classes. For the Pinwheels dataset (see Appendix D.1 below), we set the number of linear experts (and hence the dimension of the discrete latent \(\bm{z}_{1}\)) at \(K=10\), while for all other datasets we used \(K=20\).

### Posterior Initialization for CAVI-CMN

We initialized the posterior parameters of the Matrix Normal Gamma distributions for each linear expert in the conditional mixture layer of the network in the following way:

* Each element \(M_{ij}\) of the posterior mean matrix \(\bm{M}_{k\in 1:K}\) was independently sampled from a uniform distribution \(\mathcal{U}(\frac{-3}{\sqrt{d}},\frac{3}{\sqrt{d}})\), where \(\mathcal{U}(lb,ub)\) represents the uniform distribution with bounds \(lb\) and \(ub\), and \(d\) is the input dimension.
* The initial posterior values of \(\bm{V}_{k\in 1:K}\) were set to identity matrices.
* The initial posteriors for \(a_{k\in 1:K}\) and \(b_{k\in 1:K}\) were set to 2.0 and 1.0, respectively.

We initialized the posterior parameters of the two Multinomial Logistic Regressions (one for the conditional mixture layer, one for the terminal layer) in the following way:

* The posterior mean and covariance of the \(k^{\text{th}}\) row of stick-breaking weights were multivariate normal distributions \(\mathcal{N}(\bm{\beta}_{k};\bm{\mu}_{k},\bm{\Sigma}_{k})\) with the following parameters \(\bm{\mu}_{k}=\left(0,0,\ldots,-\log(K-k)\right)\), \(\Sigma_{k}=0.01\,I_{D}\). The mean of the \(k^{\text{th}}\) bias term was initialized to a'stick-breaking correction term' \(-\log(K-k)\) in order to induce a flat prior over the \(K\) categories. In the absence of this correction, the stick-breaking parameterization assigns non-uniform probability across categories, whereby categories with lower indices in the ordering \(k\in(1,2,\ldots,K)\) are assigned higher likelihood _a priori_.

### Maximum Likelihood Estimation

For gradient-based optimization of the loss function (the negative log likelihood), we used the AdaBelief optimizer with parameters set to its default values as introduced in Zhuang et al. (2020) (\(\alpha=1e-3\), \(\beta_{1}=0.9\), \(\beta_{2}=0.999\)), and ran the optimization for \(20,000\) steps. This implements deterministic gradient descent, not stochastic gradient descent, because we fit the model in 'full-batch' mode, i.e., without splitting the data into mini-batches and updating model parameters using noisy gradient estimates.

### No U-Turn Sampler

Markov Chain Monte Carlo converges in distribution to samples from a target distribution, so for this method we obtain samples from a joint distribution \(p(\bm{A}_{1:K},\bm{\Sigma}_{1:K}^{-1},\bm{\beta}_{0},\bm{\beta}_{1}|Y,\bm{X}_{0})\) that approximate the true posterior. We used 800 warm-up steps, 16 independent chains, and 64 samples for each chain.

### Black box variational inference

While BBVI does not require conjugate relationships in the generative model, we use the same CMN model and variational distributions as we use for CAVI-CMN, in order to ensure fair comparison. For stochastic optimization, we used the AdaBelief optimizer with learning rate \(\alpha=5e{-3}\ \beta_{1}=0.9\), \(\beta_{2}=0.999\), used 8 samples to estimate the ELBO gradient (the num_particles argument of the Trace_ELBO() class), and ran the optimizer for \(20,000\) steps).

## Appendix D Dataset Descriptions

We fit all inference methods using different training set sizes, where each next training set was twice as large as the previous. This was done in order to study the robustness of performance in the low data regime. For each training size, we used the same test-set to evaluate performance. The test set was ensured to have the same relative class frequencies as in the training set(s). For each inference method and examples set size, we fit using the same batch of training data, but with 16 randomly-initialized models (different initial posterior samples or parameters).

### Pinwheels Dataset

The pinwheels dataset is a synthetic dataset designed to test a model's ability to handle nonlinear decision boundaries and data with non-Gaussian densities. The dataset consists of multiple clusters arranged in a pinwheel pattern, posing a challenging task for mixture models (Johnson et al., 2016) due to the curved and elongated spatial distributions of the data. The structure of the pinwheels dataset is determined by 4 parameters: the number of clusters or distinct spirals; the angular deviation, which defines how far the spiralling clusters deviate from the origin; the tangential deviation, which defines the noise variance of 2-D points within each cluster; and the angular rate, which determines the curvature of each spiral. For evaluating the four methods (CAVI-CMN, MLE, BBVI, and NUTS) on the synthetic pinwheels dataset, we generated a dataset with 5 clusters, with an angular deviation of 0.7, tangential deviation of 0.3 and angular rate of 0.2. We selected these values by looking at the maximum achieved test accuracy across all the methods for different parameter combinations and tried to upper-bound it 80%, which provides a low enough signal-to-noise ratio to be able to meaningfully show differences in probabilistic metrics like calibration and WAIC. For pinwheels, we trained using train sizes 50 to 1600, doubling the number of training examples at each successive training set size. We tested using 500 held-out test examples generated using the same parameters as used for the training set(s).

### Waveform Domains Dataset

The Waveform Domains dataset consists of synthetic data generated to classify three different waveform patterns, where each class is described by 21 continuous attributes (Breiman and Stone, 1988). For waveform domains, we fit each model on train sizes ranging from 60 to 3840 examples, and tested on a held-out size of 1160 examples. See here for more information about the dataset.

### Vehicle Silhouettes Dataset

This dataset involves classifying vehicle silhouettes into one of four types (bus, van, or two car models) based on features extracted from 2D images captured at various angles (Mowforth and Shepherd). We fit each model on train sizes ranging from 20 to 650 examples, and tested on a held-out size of 205 examples. See here for more information about the dataset.

[MISSING_PAGE_FAIL:16]

Figure 4: Performance and runtime results of the different models on the ‘Vehicle Silhouettes’ dataset. Descriptions of each subplot are same as in the Figure 2 legend.

Figure 3: Performance and runtime results of the different models on the ‘Waveform Domains’ dataset. Descriptions of each subplot are same as in the Figure 2 legend.

Figure 5: Performance and runtime results of the different models on the ‘Rice’ dataset. Descriptions of each subplot are same as in the Figure 2 legend.

Figure 6: Performance and runtime results of the different models on the ‘Breast Cancer’ dataset. Descriptions of each subplot are same as in the Figure 2 legend.

Figure 8: Performance and runtime results of the different models on the ‘Banknote Authentication’ dataset. Descriptions of each subplot are same as in the Figure 2 legend.

Figure 7: Performance and runtime results of the different models on the ‘Connectionist Bench (Sonar, Mines vs. Rocks)’ dataset. Descriptions of each subplot are same as in the Figure 2 legend..

\(q(\bm{x}_{1}^{n}|z_{1}^{n})=\mathcal{N}(\bm{x}_{1}^{n};\bm{\mu}_{1}^{n},\bm{ \Sigma}_{1}^{n})\). Running the CAVI algorithm involves operations (like matrix inversions and matrix-vector products) whose (naive) complexity is quadratic in matrix size. This explains the nonlinear scaling of runtime as a function of \(h\), the dimension of \(\bm{X}_{1}\). There are several ways to address this issue:

* **Low-Rank Approximations**: Use low-rank approximations to the covariance matrix \(\bm{\Sigma}_{1}^{n}\) (e.g., Cholesky or eigendecompositions).
* **Diagonal Covariance Structure**: Further constrain the covariance structure of \(q(\bm{x}_{1}^{n})\) by forcing the latent dimensions to be independent in the posterior, i.e., \(q(\bm{x}_{1}^{n}|z_{1}^{n})=\prod_{i=1}^{h}\mathcal{N}(x_{i,1}^{n};\mu_{i,1}^ {n},(\sigma_{i,1}^{n})^{2})\). This would then mean that the number of parameters to store would only grow as \(K(2h)\) in the size of the dataset, rather than as \(K(h+O(h^{2}))\).
* **Full Mean-Field Approximation**: Enforce a full mean-field approximation between \(\bm{X}_{1}\) and \(Z_{1}\), so that one only needs to store \(q(\bm{x}_{1}^{n})q(z_{1}^{n})\) rather than \(q(\bm{x}_{1}^{n}|z_{1}^{n})q(z_{1}^{n})\). This would reduce the number of multivariate normal parameters that would have to be stored and operated upon by a factor of \(K\).
* **Shared Conditional Covariance Structure**: Assume that the conditional covariance structure is shared across all training data points, i.e., \(\bm{\Sigma}_{1}^{n}=\bm{\Sigma}_{1}\), for all \(n\in\{1,2,\dots,n\}\).

All of these adjustments would help mitigate the quadratic runtime scaling of CAVI-CMN as the latent dimension \(h\) increases.

## Appendix G Model Convergence Determination

For each inference algorithm, the number of iterations taken to converge was determined by running each algorithm for a sufficiently high number of gradient (respectively, CAVI update) steps such that the ELBO (or log likelihood - LL - for MLE) stopped significantly changing. This was determined (through anecdotal inspection over many different initializations and runs across the different UCI datasets) to be 20,000 gradient steps for BBVI, 20,000 gradient steps for MLE, and 500 combined CAVI update steps for CAVI-CMN. To determine the time taken to sufficiently converge, we recorded the value of the ELBO or LL at each iteration, and fit an exponential decay function to the negative of each curve. The parameters of the estimated exponential decay were then used to determine the time at which the curve decayed to 95% decay of its value. This time was reported as the number of steps taken to converge.

Figure 9: Relative scaling of fitting time in seconds for Maximum Likelihood, BBVI, and CAVI, as a function of the number of parameters. The number of parameters itself was manipulated in three illustrative ways: changing the input dimension \(d\), changing the number of linear experts \(K\) in the conditional mixture layer, and changing the dimensionality of the continuous latent variable \(h\).