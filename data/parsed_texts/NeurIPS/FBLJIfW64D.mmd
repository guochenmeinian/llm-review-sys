# Dimension-free deterministic equivalents and scaling laws for random feature regression

 Leonardo Defilippis

Departement d'Informatique

Ecole Normale Superieure - PSL & CNRS

leonardo.defilippis@ens.psl.eu&Bruno Loureiro

Departement d'Informatique

Ecole Normale Superieure - PSL & CNRS

bruno.loureiro@ens.psl.eu&Theodor Misiakiewicz

Department of Statistics and Data Science

Yale University

theodor.misiakiewicz@yale.edu

###### Abstract

In this work we investigate the generalization performance of random feature ridge regression (RFRR). Our main contribution is a general deterministic equivalent for the test error of RFRR. Specifically, under a certain concentration property, we show that the test error is well approximated by a closed-form expression that only depends on the feature map eigenvalues. Notably, our approximation guarantee is non-asymptotic, multiplicative, and independent of the feature map dimension--allowing for infinite-dimensional features. We expect this deterministic equivalent to hold broadly beyond our theoretical analysis, and we empirically validate its predictions on various real and synthetic datasets. As an application, we derive sharp excess error rates under standard power-law assumptions of the spectrum and target decay. In particular, we provide a tight result for the smallest number of features achieving optimal minimax error rate.

## 1 Introduction

At odds with classical statistical intuition, overparametrized neural networks are able to generalize while perfectly interpolating the training data. This observation, which defies the canonical mathematical understanding of generalization based on complexity measures and uniform convergence, appears surprising at first (Zhang et al., 2017). However, recent progress in our mathematical understanding of generalization has taught us that this _benign overfitting_ property of overparametrized neural networks is shared by a plethora of simpler learning tasks (Bartlett et al., 2021; Belkin, 2021). Among them, the investigation of the following class of _random feature models_ has been at the forefront of this progress:

\[\mathcal{F}_{\text{RF}}=\Big{\{}\hat{f}(\bm{x};\bm{a})=\frac{1}{\sqrt{\rho}} \sum_{j\in[p]}a_{j}\varphi(\bm{x},\bm{w}_{j})\ :\ \bm{a}=(a_{j})_{j\in[p]}\in\mathbb{R}^{p}\Big{\}}.\] (1)

Here \(\bm{x}\in\mathcal{X}\) denotes the inputs and \(\bm{W}=(\bm{w}_{j})_{j\in[p]}\) a set of weight vectors which are taken to be random \(\bm{w}_{j}\in\mathcal{W}\subseteq\mathbb{R}^{d}\sim_{\text{iid}}\mu_{w}\). Hence, as the name suggests the feature map \(\varphi:\mathcal{X}\times\mathcal{W}\rightarrow\mathbb{R}\) defines a random function. A few examples of random feature maps include the fully connected neural network features \(\varphi(\bm{x},\bm{w})=\sigma(\langle\bm{w},\bm{x}\rangle)\), \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\), and the convolutional features with global average pooling \(\varphi(\bm{x},\bm{w})=\nicefrac{{1}}{{d}}\sum_{\ell=1}^{d}\sigma(\langle \bm{w},g_{\ell}\cdot\bm{x}\rangle)\) where \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\) and\(g_{\ell}\cdot\bm{x}=(x_{\ell+1},\ldots,x_{d},x_{1},\ldots,x_{\ell})\) is the \(\ell\)-shift operator with cyclic boundary conditions (both with \(\mathcal{X}=\mathbb{R}^{d}\)). Random features (Balcan et al., 2006; Rahimi and Recht, 2007) were originally introduced as a computationally efficient approximation for the limiting kernel: \[K(\bm{x},\bm{x}^{\prime})=\mathbb{E}_{\bm{w}\sim\mu_{w}}\left[\varphi(\bm{x}, \bm{w})\varphi(\bm{x}^{\prime},\bm{w})\right].\] (2) Although this can reduce the computational cost of kernel methods, it introduces an approximation error. Rahimi and Recht (2008) showed that in a supervised setting with \(n\) samples, \(p=O(n)\) features are sufficient to achieve an excess risk \(O(n^{-\nicefrac{{1}}{{n}}})\). Rudi and Rosasco (2017) improved this result under standard power-law assumptions on the asymptotic kernel spectrum, showing that for fast decays, less features are needed to achieve the minimax rate. In Section 4 we will revisit this question, where we will derive a tight result for the minimum number of features. More recently, the random feature model has gained in popularity as a proxy model for studying the generalization properties of two-layer neural networks in the lazy regime of training (Jacot et al., 2018; Chizat et al., 2019). Indeed, for particular choices of feature maps such as \(\varphi(\bm{x},\bm{w})=\sigma(\langle\bm{w},\bm{x}\rangle)\), it can also be seen as a two-layer neural network with fixed first-layer weights. Exact asymptotic results for the generalization error of eq. (1) were derived for different supervised learning tasks under the proportional scaling regime \(n,p=\Theta(d)\) in (Mei and Montanari, 2022; Gerace et al., 2021; Dhifallah and Lu, 2020; Hu and Lu, 2023; Goldt et al., 2022; Loureiro et al., 2022; Bosch et al., 2023, 2024; Schroder et al., 2023, 2024) and under more general polynomial scaling \(n,p=\Theta(d^{\kappa})\) in (Simon et al., 2023, Aguirre-Lopez et al., 2024; Hu et al., 2024). As discussed above, these works played a fundamental role in our current mathematical understanding of the relationship between overparametrization and generalization, demystifying different phenomena such as double descent Belkin et al. (2019) and benign overfitting Bartlett et al. (2020). It also led to fundamental separation results between lazy and trained networks (Ghorbani et al., 2019, 2020; Mei et al., 2022), recently motivating the investigation of corrections to the random limit (Ba et al., 2022; Dandi et al., 2023; Moniri et al., 2024; Cui et al., 2024). With the exception of (Simon et al., 2023), which is based on non-rigorous arguments, the results in all the works cited above are derived in the asymptotic limit of large data dimension. However, the relative scaling of the \(n,p,d\) is fundamentally artificial, and in practice it is hard to unambiguously define the regime of interest. Our main goal in this manuscript is to provide a dimension-free characterization of the generalization error allowing us to give tight answers to questions which cannot be addressed asymptotically. More precisely, our main contributions are:

1. Under a concentration assumption on the feature map eigenfunctions, we prove a non-asymptotic deterministic approximation for the RFRR risk \(\mathcal{R}_{\mathrm{test}}\approx\mathsf{R}_{n,p}\) which is independent of the feature map dimension. More precisely, with high-probability over the input data and random weights: \[|\mathcal{R}_{\mathrm{test}}-\mathsf{R}_{n,p}|\leq\tilde{O}(p^{-1/2}+n^{-1/2}) \cdot\mathsf{R}_{n,p}.\] (3) where the _deterministic equivalent_\(\mathsf{R}_{n,p}\) can be computed by solving a set of self-consistent equations of the type \(x=f(x)\), with \(f\) a contractive map. This result unifies the long list of asymptotic formulas in the RFRR literature, and proves a conjecture by Simon et al. (2023). We numerically validate the results on various real and synthetic datasets. The precise statement of the theorem and the assumptions are discussed in Section 3.
2. Leveraging our formula, we investigate the error scaling laws in a setting where the target function and feature spectrum decay as a power-law, also known as _source and capacity_ conditions. We provide a full picture of the different scaling regimes and the cross-overs between them, summarized in Figure 2. Our result is closely related to the neural scaling laws literature (Kaplan et al., 2020), and provides the first rigorous, non-linear extension of (Bahiri et al., 2024; Maloney et al., 2022).
3. We provide a sharp expression for the minimum number of features required to achieve the minimax optimal decay rate of Caponnetto and De Vito (2007), closing the gap of previous lower-bounds in the literature (Rudi and Rosasco, 2017). Further related works --Deterministic equivalents have been derived for a wide range of learning problems, such as ridge regression (Dobriban and Wager, 2018; Hastie et al., 2022; Cheng and Montanari, 2022; Wei et al., 2022), kernel regression (Misiakiewicz and Saeed, 2024), shallow (Liaoand Couillet, 2018; Mei and Montanari, 2022; Chouard, 2022; Bach, 2024; Atanasov et al., 2024) and deep random feature regression (Fan and Wang, 2020; Schroder et al., 2023, 2024; Chouard, 2023) and spiked random features (Wang et al., 2024). Scaling laws under source and capacity conditions were studied by several authors in the context of kernel ridge regression (Bordelon et al., 2020; Spigler et al., 2020; Cui et al., 2022; Simon et al., 2023; Li et al., 2023; Misiakiewicz and Mei, 2022; Favero et al., 2021; Cagnetta et al., 2023; Dohmatob et al., 2024) and classification (Cui et al., 2023).

## 2 Setting

In this work, we focus on the generalization properties of the random feature class \(\mathcal{F}_{\text{RF}}\) defined in eq. (1) in a supervised regression setting. More precisely, consider a data set \(\mathcal{D}=\{(\bm{x}_{i},y_{i})_{i\in[n]}\}\) composed of \(n\) independent and identically distributed samples from a joint distribution \(\mu_{x,y}\) on \(\mathcal{X}\times\mathbb{R}\). Let \(f_{\star}(\bm{x})=\mathbb{E}[y|\bm{x}]\) denote the target function. We assume \(f_{\star}\in L_{2}(\mu_{x})\), where \(\mu_{x}\) is the marginal distribution over \(\mathcal{X}\). Moreover, we assume the noise \(\varepsilon\coloneqq y-f_{\star}(\bm{x})\) has zero mean and finite variance \(\mathbb{E}[\varepsilon^{2}]=\sigma_{\varepsilon}^{2}<\infty\). Note this is equivalent to:

\[y_{i}=f_{\star}(\bm{x}_{i})+\varepsilon_{i},\qquad\qquad f_{\star}\in L_{2}( \mu_{x}).\] (4)

Given the training data, we are interested in the properties of the minimiser:

\[\hat{\bm{a}}_{\lambda}(\bm{Z},\bm{y})\coloneqq\operatorname*{arg\,min}_{\bm{a }\in\mathbb{R}^{p}}\Big{\{}\sum_{i\in[n]}\Big{(}y_{i}-\hat{f}(\bm{x}_{i};\bm{a })\Big{)}^{2}+\lambda\|\bm{a}\|_{2}^{2}\Big{\}}=(\bm{Z}^{\top}\bm{Z}+\lambda \bm{I}_{p})^{-1}\bm{Z}^{\top}\bm{y},\] (5)

where we have defined the feature matrix \(\bm{Z}_{ij}=p^{-\nicefrac{{1}}{{2}}}\varphi(\bm{x}_{i};\bm{w}_{j})\) and the label vectors \(\bm{y}=(y_{i})_{i\in[n]}\). In particular, we are interested in its capacity of generalising to unseen data, as quantified by the _excess population risk_:

\[\mathcal{R}(f_{\star},\bm{X},\bm{W},\bm{\varepsilon},\lambda)\coloneqq\mathbb{ E}_{\bm{x}\sim\mu_{x}}\Big{[}\Big{(}f_{\star}(\bm{x})-\hat{f}(\bm{x};\hat{\bm{a}}_{ \lambda})\Big{)}^{2}\Big{]}.\] (6)

It will be convenient to decompose the excess risk above in terms of the standard bias and variance:

\[\mathcal{R}(f_{\star};\bm{X},\bm{W},\lambda)\coloneqq\mathbb{E}_{\bm{ \varepsilon}}\left[\mathcal{R}(f_{\star};\bm{X},\bm{W},\bm{\varepsilon},\lambda )\right]=\mathcal{B}(f_{\star};\bm{X},\bm{W},\lambda)+\mathcal{V}(\bm{X},\bm{ W},\lambda),\] (7)

where:

\[\mathcal{B}(f_{\star};\bm{X},\bm{W},\lambda) \coloneqq\mathbb{E}_{\bm{x}\sim\mu_{x}}\left[\Big{(}f_{\star}(\bm{ x})-\mathbb{E}_{\bm{\varepsilon}}[\hat{f}(\bm{x};\hat{\bm{a}}_{\lambda})]\Big{)}^{2} \right],\] (8) \[\mathcal{V}(\bm{X},\bm{W},\lambda) \coloneqq\mathbb{E}_{\bm{x}\sim\mu_{x}}\left[\operatorname{Var}_{ \bm{\varepsilon}}(\hat{f}(\bm{x};\hat{\bm{a}}_{\lambda}))\right].\] (9)

Note that to simplify the exposition, we have explicitly taken an expectation over the training data noise \(\bm{\varepsilon}=(\varepsilon_{i})_{i\in[n]}\). Indeed, it can be shown that the excess risk eq. (6) concentrates on its expectation over \(\bm{\varepsilon}\) under mild assumptions (see for example Misiakiewicz and Saeed (2024)).

## 3 Deterministic equivalents

The excess risk eq. (6) is a function of the covariates \(\bm{X}\) and the weights \(\bm{W}\), and therefore it is a random quantity. Our main result in what follows is a sharp characterization of the bias and variance in terms of a _deterministic equivalent_ depending only on the model parameters and spectral properties of the features. Consider a square-integrable \(\varphi\in L_{2}(\mathcal{X}\times\mathcal{W})\), and define the Fredholm integral operator \(\mathbb{T}:L_{2}(\mathcal{X})\to\mathcal{V}\subseteq L_{2}(\mathcal{W})\):

\[\mathbb{T}h(\bm{w})\coloneqq\int_{\mathcal{X}}\varphi(\bm{x};\bm{w})h(\bm{x}) \mu_{x}(\mathrm{d}\bm{x}),\qquad\quad\forall h\in L_{2}(\mathcal{X}),\] (10)

where we define \(\mathcal{V}=\operatorname{Im}(\mathbb{T})\). This is a compact operator, and therefore can be diagonalized:

\[\mathbb{T}=\sum_{k=1}^{\infty}\xi_{k}\psi_{k}\phi_{k}^{\star},\] (11)where \((\xi_{k})_{k\geq 1}\subseteq\mathbb{R}\) are the eigenvalues and \((\psi_{k})_{k\geq 1}\) and \((\phi_{k})_{k\geq 1}\) are orthonormal bases of \(L_{2}(\mathcal{X})\) and \(\mathcal{V}\) respectively:

\[\langle\psi_{k},\psi_{k^{\prime}}\rangle_{L_{2}(\mathcal{X})}=\delta_{kk^{ \prime}},\qquad\quad\langle\phi_{k},\phi_{k^{\prime}}\rangle_{L_{2}(\mathcal{ W})}=\delta_{kk^{\prime}}.\] (12)

Without loss of generality, we assume the eigenvalues are ordered in non-increasing absolute values \(|\xi_{1}|\geq|\xi_{2}|\geq\ldots\), and for simplicity of presentation we assume that all eigenvalues are non-zero, i.e., \(\ker(\mathbb{T})=\{0\}\). Denote \(\bm{\Sigma}=\operatorname{diag}(\xi_{1}^{2},\xi_{2}^{2},\ldots)\in\mathbb{R}^ {\infty\times\infty}\) the diagonal matrix of the squared eigenvalues. Similarly, since \(f_{\star}\in L_{2}(\mu_{x})\), it admits the following decomposition in \((\psi)_{k\geq 1}\):

\[f_{\star}=\sum_{k\geq 1}\bm{\beta}_{\star,k}\psi_{k}\] (13)

Our formal results will assume the following concentration property over the eigenfunctions.

**Assumption 3.1** (Concentration of the eigenfunctions).: _Denote the (infinite-dimensional) random vectors1\(\bm{\psi}:=(\xi_{k}\psi_{k}(\bm{x}))_{k\geq 1}\) and \(\bm{\phi}:=(\xi_{k}\phi_{k}(\bm{w}))_{k\geq 1}\). There exists a constant \(\mathsf{C}_{x}>0\) such that for any deterministic p.s.d. matrix \(\bm{A}\in\mathbb{R}^{\infty\times\infty}\), i.e. a linear operator acting on an infinite-dimensional Hilbert space, with \(\operatorname{Tr}(\bm{\Sigma}\bm{A})<\infty\), we have_

Footnote 1: Note that we can consider both \(\bm{\psi}\) and \(\bm{\phi}\) random elements of the Hilbert space \(\ell_{2}\) with distribution induced by \(\bm{x}\sim\mu_{x}\) and \(\bm{w}\sim\mu_{w}\), where \(\mathbb{E}[\bm{\psi}\bm{\psi}^{\dagger}]=\mathbb{E}[\bm{\phi}\bm{\phi}^{ \dagger}]=\bm{\Sigma}\) and \(\operatorname{Tr}(\bm{\Sigma})<\infty\).

\[\mathbb{P}\left(\left|\bm{\psi}^{\mathsf{T}}\bm{A}\bm{\psi}- \operatorname{Tr}(\bm{\Sigma}\bm{A})\right|\geq 1\cdot\|\bm{\Sigma}^{1/2}\bm{A}\bm{\Sigma}^{1/2}\|_{F}\right) \leq\mathsf{C}_{x}\exp\left\{-t/\mathsf{C}_{x}\right\},\] (14) \[\mathbb{P}\left(\left|\bm{\phi}^{\mathsf{T}}\bm{A}\bm{\phi}- \operatorname{Tr}(\bm{\Sigma}\bm{A})\right|\geq 1\cdot\|\bm{\Sigma}^{1/2}\bm{A}\bm{\Sigma}^{1/2}\|_{F}\right) \leq\mathsf{C}_{x}\exp\left\{-t/\mathsf{C}_{x}\right\}.\] (15)

While Assumption 3.1 is restrictive and will not be satisfied by many non-linear settings, it covers a number of popular theoretical models studied in the literature: 1) independent sub-Gaussian entries, 2) verifying a log-Sobolev inequality or convex Lipschitz concentration (see Cheng and Montanari (2022)). We expect that Assumption 3.1 can be relaxed using the same procedure as in Misiakiewicz and Saeed (2024) to cover classical examples such as data and weights uniformly distributed on the sphere or hypercube. Such a relaxation is involved and we leave it to future work. We will further assume that:

**Assumption 3.2**.: _There exists \(\mathsf{m}\in\mathbb{N}\) such that_

\[p\xi_{\mathsf{m}+1}^{2}\leq\frac{\lambda}{n}\sum_{k=\mathsf{m}+1}^{\infty}\xi _{k}^{2}.\] (16)

_Furthermore, we will assume that for some \(\mathsf{C}_{*}>0\) that we have_

\[\frac{\operatorname{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1})}{\operatorname {Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{-2})}\leq\mathsf{C}_{*},\qquad\quad \quad\frac{\langle\bm{\beta}_{\star},(\bm{\Sigma}+\nu_{2})^{-1}\bm{\beta}_{ \star}\rangle}{\nu_{2}\langle\bm{\beta}_{\star},(\bm{\Sigma}+\nu_{2})^{-2}\bm {\beta}_{\star}\rangle}\leq\mathsf{C}_{*}.\] (17)

Assumption 3.2 is technical, and we believe it can be removed at the cost of a more involved analysis. For instance, eq. (16) is always satisfied for \(\xi_{k}^{2}\propto k^{-\alpha}\) if we take \(\mathsf{m}=O(p^{2})\). Condition (17) was also considered in Cheng and Montanari (2022), and is satisfied in many settings of interest, for example under source and capacity conditions \(\beta_{k}\asymp k^{-\beta}\) and \(\xi_{k}^{2}\asymp k^{-\alpha}\) considered in Section 4.

Main result --Our main result concerns a dimension-free characterization of the risk eq. (6) in terms of deterministic equivalents. We start by defining them.

**Definition 1** (Deterministic equivalents).: _Given integers \(n,p\), covariance matrix \(\bm{\Sigma}\) and regularization parameter \(\lambda\geq 0\). Consider the parameter \(\nu_{2}\in\mathbb{R}_{>0}\) defined as the unique solution of the self-consistent equation:_

\[1+\frac{n}{p}-\sqrt{\left(1-\frac{n}{p}\right)^{2}+4\frac{\lambda}{p\nu_{2}}}= \frac{2}{p}\mathrm{Tr}\left(\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1}\right),\] (18)

_and \(\nu_{1}\in\mathbb{R}_{>0}\) is given by:_

\[\nu_{1}:=\frac{\nu_{2}}{2}\left[1-\frac{n}{p}+\sqrt{\left(1-\frac{n}{p}\right)^ {2}+4\frac{\lambda}{p\nu_{2}}}\right].\] (19)_We introduce the short-hand:_

\[\Upsilon(\nu_{1},\nu_{2}) \coloneqq\frac{p}{n}\left[\left(1-\frac{\nu_{1}}{\nu_{2}}\right)^{2}+ \left(\frac{\nu_{1}}{\nu_{2}}\right)^{2}\frac{\operatorname{Tr}(\bm{\Sigma}^{2} (\bm{\Sigma}+\nu_{2})^{-2})}{p-\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+ \nu_{2})^{-2})}\right],\] (20) \[\chi(\nu_{2}) \coloneqq\frac{\operatorname{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2} )^{-2})}{p-\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{-2})}.\] (21)

_Then, the deterministic equivalents for the bias, variance and test error are defined as:_

\[\mathsf{B}_{n,p}(\bm{\beta}_{*},\lambda) \coloneqq\frac{\nu_{2}^{2}}{1-\Upsilon(\nu_{1},\nu_{2})}\Big{[} \langle\bm{\beta}_{*},(\bm{\Sigma}+\nu_{2})^{-2}\bm{\beta}_{*}\rangle+\chi( \nu_{2})\langle\bm{\beta}_{*},\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-2}\bm{\beta} _{*}\rangle\Big{]},\] (22) \[\mathsf{V}_{n,p}(\lambda) \coloneqq\sigma_{\varepsilon}^{2}\frac{\Upsilon(\nu_{1},\nu_{2}) }{1-\Upsilon(\nu_{1},\nu_{2})},\] (23) \[\mathsf{R}_{n,p}(\bm{\beta}_{*},\lambda) \coloneqq\mathsf{B}_{n,p}(\bm{\beta}_{*},\lambda)+\mathsf{V}_{n,p} (\lambda).\] (24)

Our main result provides precise conditions for when the deterministic equivalents defined in definition 1 are a good approximation for the test error eq. (6), as a function of the dimensions \(n,p\), feature covariance \(\bm{\Sigma}\), and regularization \(\lambda>0\). More precisely, the approximation rates will depend on them through the following quantities:

\[r_{\bm{\Sigma}}(k) \coloneqq\frac{\sum_{j\geq k}\xi_{j}^{2}}{\xi_{k}^{2}},\quad M_{ \bm{\Sigma}}(k)\coloneqq 1+\frac{r_{\bm{\Sigma}}(\lfloor\eta_{*}\cdot k\rfloor) \lor k}{k}\log\left(r_{\bm{\Sigma}}(\lfloor\eta_{*}\cdot k\rfloor)\lor k \right),\] (25) \[\rho_{\kappa}(p) \coloneqq 1+\frac{p\cdot\xi_{\lfloor\eta_{*}\cdot p\rfloor}^{2}}{ \kappa}M_{\bm{\Sigma}}(p),\] (26) \[\widetilde{\rho}_{\kappa}(n,p) \coloneqq 1+1[n\leq p/\eta_{*}]\cdot\left\{\frac{n\xi_{\lfloor\eta_{*} \cdot n\rfloor}^{2}}{\kappa}+\frac{n}{p}\cdot\rho_{\kappa}(p)\right\}M_{\bm{ \Sigma}}(n),\] (27)

Below we denote \(C_{a_{1},\dots,a_{k}}\) constants that only depend on the values of \(\{a_{i}\}_{i\in[k]}\). We use \(a_{i}=`*\)' to denote the dependency on the constants in Assumptions 3.1 and 3.2.

**Theorem 3.3** (Test error of RFRR).: _Under Assumptions 3.1, 3.2 and for any \(D,K>0\), there exist constants \(\eta_{*}\in(0,1/2)\) and \(C_{*,D,K}>0\) such that the following holds. For any \(n,p\geq C_{*,D,K}\), regularization \(\lambda>0\), and target function \(f_{\star}\in L_{2}(\mu_{x})\), if_

\[\lambda\geq n^{-K},\qquad\gamma_{\lambda}\geq p^{-K},\qquad\qquad \widetilde{\rho}_{\lambda}(n,p)^{5/2}\cdot\log^{3/2}(n) \leq K\sqrt{n},\] (28) \[\widetilde{\rho}_{\lambda}(n,p)^{2}\cdot\rho_{\gamma_{+}}(p)^{8 }\cdot\log^{4}(p) \leq K\sqrt{p},\] (29)

_then with probability at least \(1-n^{-D}-p^{-D}\), we have_

\[|\mathcal{R}(f_{\star};\bm{X},\bm{W},\lambda)-\mathsf{R}_{n,p}(\bm{\beta}_{*}, \lambda)|\leq C_{*,D,K}\cdot\mathcal{E}(n,p)\cdot\mathsf{R}_{n,p}(\bm{\beta}_ {*},\lambda),\] (30)

_where \(\mathsf{R}_{n,p}(\bm{\beta}_{*},\lambda)\) has been defined in eq. (24) and:_

\[\gamma_{\lambda}=\frac{p\lambda}{n}+\sum_{k=m+1}^{\infty}\xi_{k}^{2},\qquad \qquad\gamma_{+}=p\nu_{1}+\sum_{k=m+1}^{\infty}\xi_{k}^{2}.\] (31)

_and the approximation rate is given by_

\[\mathcal{E}(n,p) :=\frac{\widetilde{\rho}_{\lambda}(n,p)^{6}\log^{7/2}(n)}{\sqrt{n }}+\frac{\widetilde{\rho}_{\lambda}(n,p)^{2}\cdot\rho_{\gamma_{+}}(p)^{8} \log^{7/2}(p)}{\sqrt{p}}.\] (32)

For typical settings, with regularly varying spectrum, \(\rho_{\kappa}(p)\lesssim\log(p)^{C}/\kappa\) and \(\widetilde{\rho}_{\kappa}(n,p)\lesssim\log(n\wedge p)^{C}/\kappa\). In this case, the approximation rate scales as \(\mathcal{E}(n,p)=\tilde{O}(n^{-1/2}+p^{-1/2})\), which matches the optimal rates expected from local law fluctuations. A few remarks on this theorem are in order:

1. Theorem 3.3 provides fully non-asymptotic approximation bounds for the population risk and its deterministic equivalent. They hold pointwise and for a large class of functions. In particular, they do not require probabilistic assumptions over the target function coefficients \(\bm{\beta}_{\star}\), as for instance in (Dobriban and Wager, 2018; Richards et al., 2021; Wu and Xu, 2020).

2. They are not explicitly dependent on the feature map dimension.
3. They are multiplicative, and therefore relative to the scale of the risk. In particular, they hold even if \(\mathcal{R}\asymp n^{-\gamma}\), which will be crucial to the discussion in section 4.
4. Theorem 3.3 is considerably more general than previous results. First, it extends the dimension-free results of Cheng and Montanari (2022) for well-specified ridge regression and Misiakiewicz and Saeed (2024) for KRR (see \(p\to\infty\) discussion below) to the case of feature maps \(\varphi:\mathcal{X}\times\mathcal{W}\to\mathbb{R}\), which, as discussed in Section 2, comprises several cases of interest in machine learning. Moreover, the deterministic equivalent recovers as particular cases the asymptotic results derived under proportional \(n,p=\Theta(d)\)Mei and Montanari (2022); Loureiro et al. (2022); Schroder et al. (2023) and polynomial \(n,p=\Theta(d^{\kappa})\)Xiao et al. (2022); Hu et al. (2024); Aguirre-Lopez et al. (2024) scaling.
5. The bounds depend on \(\lambda^{-1}\) and \(\lambda_{>m}^{-1}\). Following similar arguments as in Cheng and Montanari (2022); Misiakiewicz and Saeed (2024), this assumption could be removed at the cost of a lengthier analysis and worse rates \(n^{-C}+p^{-C}\) with \(C<1/2\). Figure 1 illustrates Theorem 3.3 in two different settings with real and synthetic data. On the left, we show the population risk of learning a single-index target function with a spiked random features model. This model was recently shown to be equivalent to the first-step of training in a fully-connected two-layer network Ba et al. (2022), and it was recently studied by several authors Moniri et al. (2024); Cui et al. (2024); Wang et al. (2024). On the right, we apply our formulas directly to a real data set. In both cases, the theoretical curves show excellent agreement with the numerical simulations. In Appendix C we present additional plots, together with a discussion of how these plots were generated.

Particular limits --We now discuss some particular limits of interest of the deterministic equivalent eq. (24). First, note that at the interpolation threshold \(n=p\), we have \(1-\Upsilon(\nu_{1},\nu_{2})\sim\sqrt{\lambda}\). Therefore, the risk \(\mathsf{R}_{n,p}\sim\lambda^{-1/2}\) diverges as \(\lambda\to 0^{+}\), a well-known behaviour known as the _interpolation peak_ in the random feature literature Hastie et al. (2022); Mei and Montanari (2022); Gerace et al. (2021) and observed in neural networks Spigler et al. (2019); Nakkiran et al. (2021).

Another limit of interest is \(p\to\infty\) where, in the generic case, the features span an infinite-dimensional RKHS. Typically, the resulting kernel will be universal, implying it can approximate any function in \(L_{2}(\mu_{x})\). In this limit, the risk bottleneck is given by the finite amount of data \(n\).

**Corollary 3.4** (Kernel limit).: _In the \(p\to\infty\) limit both \(\nu_{1}\) and \(\nu_{2}\) converge to a single \(\nu_{\mathsf{K}}\) which is the unique positive solution to the following self-consistent equation_

\[n-\frac{\lambda}{\nu_{\mathsf{K}}}=\operatorname{Tr}\bigl{(}\bm{\Sigma}(\bm{ \Sigma}+\nu_{\mathsf{K}})^{-1}\bigr{)}.\] (33)

_Moreover, the bias eq. (22) and variance eq. (23) terms simplify to:_

\[\mathsf{B}_{\mathsf{K},n}(\bm{\beta}_{*},\lambda)=\frac{\nu_{\mathsf{K}}^{2} \langle\bm{\beta}_{*},(\bm{\Sigma}+\nu_{\mathsf{K}})^{-2}\bm{\beta}_{*}\rangle} {1-\frac{1}{n}\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{\mathsf{K}}) ^{-2})},\quad\quad\mathsf{V}_{\mathsf{K},n}(\lambda)=\sigma_{\varepsilon}^{2} \frac{\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{\mathsf{K}})^{-2})}{ n-\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{\mathsf{K}})^{-2})}.\] (34)

_We denote the corresponding test error \(\mathsf{R}_{\mathsf{K},n}(\bm{\beta}_{*},\lambda)=\mathsf{B}_{\mathsf{K},n}( \bm{\beta}_{*},\lambda)+\mathsf{V}_{\mathsf{K},n}(\lambda)\)._

Note that eq. (23) exactly agrees with the dimension-free deterministic equivalents for kernel methods in Cheng and Montanari (2022), Misiakiewicz and Saeed (2024). Finally, the third limit of interest is the \(n\to\infty\) where data is abundant. In this case, the empirical risk eq. (5) converge to the population risk, and therefore the bottleneck in the risk is given by the capacity of the random feature class \(\mathcal{F}_{\text{RF}}\) eq. (1) to approximate the target \(f_{*}\).

**Corollary 3.5** (Approximation limit).: _In the \(n\to\infty\) limit, we have \(\nu_{1}\to 0\) and \(\nu_{2}\to\nu_{\mathsf{A}}\) satisfying the following simplified self-consistent equation:_

\[p=\operatorname{Tr}\bigl{(}\bm{\Sigma}(\bm{\Sigma}+\nu_{\mathsf{A}})^{-1} \bigr{)}.\] (35)

_Moreover, the bias eq. (22) and variance eq. (23) terms simplify to:_

\[\mathsf{B}_{\mathsf{A},p}(\bm{\beta}_{*})=\nu_{\mathsf{A}}\langle\bm{\beta}_{ *},(\bm{\Sigma}+\nu_{\mathsf{A}})^{-1}\bm{\beta}_{*}\rangle,\qquad\qquad \mathsf{V}_{\mathsf{A},n}=0.\] (36)

_We denote the risk in this case \(\mathsf{R}_{\mathsf{A},p}(\bm{\beta}_{*})=\mathsf{B}_{\mathsf{A},p}(\bm{\beta }_{*})\), which as expected does not depend on \(\lambda\)._

## 4 Scaling laws

Our exact characterization of the excess risk in Theorem 3.3 shows that the bottleneck in the model performance stems either from its approximation capacity (as measured by the "width" \(p\)) and the availability of data (as measured by the number of samples \(n\)). In other words, for a fixed data budget \(n\), increasing \(p\) might not improve the error besides a certain point, yielding a waste of computational resources. This raises an important question: _given a fixed data budged \(n\), what is the optimal choice of model size \(p_{*}\)?_

Context --This is a fundamental question in the random feature literature, and was investigated already in the pioneering works of Rahimi and Recht (2007, 2008), who showed that to achieve an excess risk of \(O(n^{-1/2})\) requires at most \(p=O(n)\) features. This upper bound was considerably refined by Rudi and Rosasco (2017) under classical power law scaling assumptions, also known as _source_ and _capacity_ conditions in the kernel literature:

\[\operatorname{Tr}\!\bm{\Sigma}^{1/\alpha}<\infty,\qquad\qquad\qquad\quad|| \bm{\Sigma}^{-r}\bm{\beta}_{*}||_{2}<\infty.\] (37)

where \(\alpha\in(1,\infty)\) and \(r\in(0,\infty)\), with the case \(r=\nicefrac{{1}}{{2}}\) corresponding to \(f_{*}\) belonging to the RKHS of the asymptotic random feature kernel eq. (2). The optimal minmax rate \(O\left(n^{-\frac{2\alpha r}{2\alpha r+1}}\right)\) for ridge regression under source and capacity conditions were obtained by Caponnetto and De Vito (2007). Rudi and Rosasco (2017) showed that this optimal rate can be attained by the random feature hypothesis eq. (1) with \(p>p_{0}=O\left(n^{\frac{\alpha-1+2r}{1+2\alpha r}}\right)\) features. However, this is only an upper bound, and understanding how tight it is, as well as the full picture in the hard regime \(r\in(0,\nicefrac{{1}}{{2}})\), remains an open question. In this section, we leverage our tight characterization of the excess risk in Theorem 3.3 to provide a sharp answer to this question.

Results --Without loss of generality, we can assume the covariance is a diagonal matrix \(\bm{\Sigma}=\operatorname{diag}(\xi_{k}^{2})_{k\geq 1}\), and we consider the case where the exponents exactly saturate the source and capacity conditions eq. (37):

\[\xi_{k}^{2} =k^{-\alpha}, \beta_{*,k} =k^{-\frac{1+2\alpha r}{2}}.\] (38)

Further, we assume a relative scaling of the number of features \(p\) and the regularization \(\lambda\) with the number of samples \(n\):

\[p=n^{q}, \lambda =n^{-(\ell-1)}.\] (39)

with \(q\geq 0\) and \(\ell\geq 0\).

**Theorem 4.1** (Excess risk rates).: _Under source and capacity conditions eq.38 and scaling assumptions eq.39, the deterministic equivalent eq.24 rate is given by:_

\[\mathsf{R}_{n,p}(\bm{\beta}_{*},\lambda)=\Theta\left(n^{-\gamma_{\mathcal{B}}( \ell,q)}+\sigma_{e}^{2}n^{-\gamma_{\mathcal{V}}(\ell,q)}\right)=\Theta\left(n^ {-\gamma(\ell,q)}\right),\] (40)

_where \(\gamma(\ell,q)\coloneqq\gamma_{\mathcal{B}}(\ell,q)\wedge\gamma_{\mathcal{V}} (\ell,q)\) for non-zero noise variance \(\sigma_{\varepsilon}^{2}\neq 0\), otherwise \(\gamma(\ell,q)=\gamma_{\mathcal{B}}(\ell,q)\). The exponents \(\gamma_{\mathcal{B}}\) and \(\gamma_{\mathcal{V}}\) are respectively the decay rates of the bias and variance terms eqs.22 and 23, and are explicitly given by_

\[\gamma_{\mathcal{B}} \coloneqq\left[2\alpha\left(\frac{\ell}{\alpha}\wedge q\wedge 1 \right)(r\wedge 1)\right]\wedge\left[\left(2\alpha\left(r\wedge\frac{1}{2} \right)-1\right)\left(\frac{\ell}{\alpha}\wedge q\wedge 1\right)+q\right],\] (41) \[\gamma_{\mathcal{V}} \coloneqq 1-\left(\frac{\ell}{\alpha}\wedge q\wedge 1\right).\] (42)

**Remark 4.1**.: _Under the scaling in eqs.38 and 39, one can check that the approximation rates \(\mathcal{E}(n,p)\) in Theorem3.3 are vanishing for \(\ell\leq\alpha+\nicefrac{{1}}{{12}}\) if \(q\geq 1\), and for \(\ell\leq q((\alpha+\nicefrac{{1}}{{16}})\lor\nicefrac{{1}}{{16}}(\alpha-1))\) if \(q<1\), which includes the optimal vertical line \(\ell=\ell_{\star}\). Hence, for these regions of scaling, Theorem3.3 readily implies that the excess risk eq.6 indeed has the decay rates described in Theorem4.1. As discussed in the previous section, we expect that these approximation guarantees can be improved to include a larger region of decay rates, but we leave it to future work._

A detailed derivation of the result above from the deterministic equivalent characterization from Theorem3.3 is discussed in AppendixD. The expressions in eq.41 are easier to visualise in a diagram. Figure2 shows the excess risk exponent \(\gamma(\ell,q)\) as a function of the parameters \(\ell\) and \(q\), in the case where \(\sigma_{\varepsilon}^{2}\neq 0\) for \(r\geq\nicefrac{{1}}{{2}}\) (left) and \(r<\nicefrac{{1}}{{2}}\) (right). Note that the key difference between the diagrams is the presence of an additional region for \(r\geq\nicefrac{{1}}{{2}}\).2 Defining the following shorthand:

Footnote 2: Recall that these two cases correspond to the target function \(f_{\star}\) belonging (\(r\geq\nicefrac{{1}}{{2}}\)) or not (\(r<\nicefrac{{1}}{{2}}\)) to the RKHS spanned by the asymptotic kernel

\[\ell_{\star}\coloneqq\frac{\alpha}{2\alpha(r\wedge 1)+1},\qquad q_{\star} \coloneqq 1-\ell_{\star}(2r\wedge 1),\qquad\hat{q}\coloneqq\frac{1}{\alpha(2r \wedge 1)+1}=q_{\star}\lor\frac{1}{\alpha+1}\] (43)

we can identify two main regions in the \((\ell,q)\) plane, corresponding to a trade-off between the bias \(\gamma_{\mathcal{B}}\) and variance \(\gamma_{\mathcal{V}}\) terms:

1. **Variance dominated region** (\(\gamma_{\mathcal{V}}<\gamma_{\mathcal{B}}\)): if \(\ell>\ell_{\star}\), \(q>\hat{q}\) and \(p>\lambda\), the excess risk is dominated by the variance term, provided the number of samples is large enough \(n\gg\sigma_{\varepsilon}^{-\nicefrac{{1}}{{\gamma_{\mathcal{B}}(\ell,q)}}- \gamma_{\mathcal{V}}(\ell,q)}\).3 Inside this region it is possible to further distinguish between two regimes:* **slow decay regime** (orange and brown): for \(\ell<\alpha\) and \(q<1\) (\(p\ll n\)), \(\gamma_{\mathcal{V}}=1-(\nicefrac{{\epsilon}}{{\alpha}}\wedge q)\), hence the decay depends on the interplay between regularization strength and number of random features and it is slower as \((\ell/\alpha\wedge q)\) increases;
* **plateau regime** (red): for \(\ell\geq\alpha\) and \(q\geq 1\) (\(p\geq n\)) the excess risk converges to a constant value and does not decay as \(n\) increases.
* **Bias dominated region** (\(\gamma_{\mathcal{V}}>\gamma_{\mathcal{B}}\)): if \(\ell<\ell_{\star}\), \(q<\hat{q}\) and \(p<\lambda\), the excess risk is dominated by the bias term, whose decay is faster as \((\nicefrac{{\epsilon}}{{\alpha}}\wedge q)\) increases (cyan, emerald and teal). Note that in the limit of large number of random features \(p\to\infty\), we recover the same rates found by Cui et al. (2022) for kernel ridge regression. Of particular interest is the rate for which the excess risk decays the fastest with the number of samples \(n\), and what is the minimum number of random features \(p_{\star}\) required to achieve this rate.

**Corollary 4.2** (Optimal rates).: _The optimal excess risk rate achieved by the random features hypothesis eq. (1) under source and capacity conditions eq. (38) and scaling assumptions eq. (39):_

\[\gamma_{\star}=\max_{\ell,q}\gamma(\ell,q)=\frac{2\alpha(r\wedge 1)}{2\alpha (r\wedge 1)+1},\] (44)

_and it is attained for:_

\[\begin{cases}\lambda=\lambda_{\star}\coloneqq n^{-(\ell_{\star}-1)}&\text{ for }r\geq\nicefrac{{1}}{{2}},\\ p\geq p_{\star}\coloneqq n^{q_{\star}}=\lambda_{\star}&\text{ for }r\geq \nicefrac{{1}}{{2}},\\ \end{cases}\] (45)

\[\begin{cases}\lambda=\lambda_{\star}&\text{ for }\left\{\lambda\leq\lambda_{ \star}\right.\\ p\geq p_{\star}=\left(\lambda_{\star}^{-1}n\right)^{\nicefrac{{1}}{{\alpha}}}& \text{ for }r<\nicefrac{{1}}{{2}}\end{cases}\] (46)

_corresponding to the bold red line (\(\bullet\)) in Fig. 2. In particular, the minimal number of random features \(p_{\star}=n^{q_{\star}}\) required to achieve the optimal rate \(\gamma_{\star}\) is given by:_

\[q_{\star}=1-\frac{\alpha(2r\wedge 1)}{2\alpha(r\wedge 1)+1}\] (47)

_and corresponds to the bold red dot (\(\bullet\)) in Fig. 2._

A few comments on Corollary 4.2 are in place. 1) The optimal excess error rate eq. (44) is consistent with the minimax optimal rates for ridge regression from Caponnetto and De Vito (2007), as also discussed by Rudi and Rosasco (2017). 2) The minimal number of random features \(p_{\star}=n^{q_{\star}}\) in eq. (47) achieving the optimal rate eq. (44) in the \(r\geq\nicefrac{{1}}{{2}}\) regime is strictly smaller than the lower bound \(p>p_{0}\) of Rudi and Rosasco (2017). More precisely, letting \(p_{0}=n^{q_{0}}\), for \(r\in[\nicefrac{{1}}{{2}},1)\):

\[q_{0}-q_{\star}=\frac{2(1-r)(\alpha-1)}{2\alpha r+1}>0,\qquad\text{for all }\alpha>1.\] (48)

Relationship to scaling laws --The empirical observation that the performance of large scale neural networks decreases as a power law with respect to the number of samples, parameter and computing time has sparked a renewed wave of interest in the theoretical investigation of power laws (Kaplan et al., 2020). Despite being a mature topic in the statistical learning literature, different recent works have turned to the study of linear models under source and capacity conditions as a playground to understand the emergence of different bottlenecks in the excess error rates (Bahi et al., 2024; Maloney et al., 2022).

The model studied in these works is given by ridge regression on data \(y_{i}=\langle\boldsymbol{\beta}_{\star},\boldsymbol{x}_{i}\rangle\) with \(\boldsymbol{x}\sim\mathcal{N}(\boldsymbol{0},\operatorname{diag}((\nicefrac{{ \epsilon}}{{k}})^{\alpha}))\) and \(\boldsymbol{\beta}_{\star}\sim\mathcal{N}(\boldsymbol{0},\nicefrac{{1}}{{d}} \boldsymbol{I}_{d})\) with a linear projection model \(\hat{f}(\boldsymbol{x},\boldsymbol{a})=\langle\boldsymbol{a},\boldsymbol{W} \boldsymbol{x}\rangle\), where \(\boldsymbol{W}\) is an i.i.d. Gaussian matrix. Note this model is a particular case of the one studied here, corresponding to a linear feature map and random target function. Moreover, since the variance of the target is constant, the source is entirely determined by the capacity \(\alpha\) of the asymptotic kernel, here controlled by the decay of the covariance of the input data.

The approximation limit from Corollary 3.5 and the kernel limit from Corollary 3.4 are known in this literature as _Variance_ and _Resolution limited regimes_, respectively (Bahi et al., 2024). They correspond precisely to the bottlenecks in the excess risk arising from the limited approximation capacity of the random feature model or the limited availability of training data. As this model is a particular case of ours, the rates in the variance limited regime can also be obtained from Theorem 4.1, and correspond to particular cases in Fig. 2, see Appendix E for a detailed discussion. Contemporary to our work, Atanasov et al. (2024) has extended the analysis in this linear model to the case where \(\bm{\beta}_{\star}\) also has a power-law decay, and provided a comprehensive discussion of the different scaling regimes for this model. Their rates can be put in a one-to-one correspondence with the rates derived in section 4. We refer the interested reader to Section VI.6 of Atanasov et al. (2024) for a detailed discussion of this relationship. We stress, however, that beside being rigorous, our results hold for features in infinite-dimensional Hilbert spaces and are not restricted to a particular asymptotic limit in the dimensions.

Complementary to the sample and model complexity bottlenecks, Kaplan et al. (2020) also observed the emergence of computational scaling laws in the risk as a function of flops used in training. A recent line of work has investigated this question on the aforementioned linear random feature model under different training algorithms, such as gradient flow (Bordelon et al., 2024) and SGD (Paquette et al., 2024; Lin et al., 2024; Bordelon et al., 2024). Due to the simplicity of this setting, the risk of ridge regression with a particular choice of regularization \(\lambda\) is closely related to the risk of different descent algorithms for least-squares at a fixed running horizon (Ali et al., 2019, 2020; Sonthalia et al., 2024). A similar analogy allows us to compare our results to the ones obtained in (Paquette et al., 2024; Lin et al., 2024). In particular, our setting cover three of the phases identified by Paquette et al. (2024), and correspond to the result in Theorem 4.1 with \(\lambda=1\) (\(\ell=1\)). Similarly, the rates of Lin et al. (2024) are obtained by taking \(\lambda\) to be the inverse of the learning rate. A detailed connection to this line of work is discussed in Appendix E.

## 5 Conclusion

In this paper, we have investigated the generalization properties of random feature models, deriving a non-asymptotic deterministic equivalent for the risk of random feature ridge regression--which recovers (and unifies) previous asymptotic findings as special limits. Our results provide a rigorous multiplicative approximation rate, enabling us to analyze error scaling laws under source and capacity conditions, and offers a complete view of the different scaling regimes and their cross-overs. Our analysis relies on Assumption 3.1 which, while popular in theoretical investigations, excludes more realistic random feature models, such as \(\varphi(\bm{x},\bm{w})=\sigma(\bm{x}^{\top}\bm{w})\) with \(\bm{x},\bm{w}\) Gaussian vectors and non-linear \(\sigma\). Although restrictive, this assumption allowed us to derive tight multiplicative approximation bounds for a generic random feature model with infinite-dimensional features--which was essential for obtaining the rigorous excess risk rates that are the primary motivation of our work. We further note that numerical simulations in Figure 1 and Appendix C suggest that the predictions of Theorem 3.3 remain accurate much beyond Assumption 3.1. We consider lifting this technical condition--e.g., by following the approach in Misiakiewicz and Saeed (2024)--to be an important direction for future research.

Figure 3: Excess risk eq. (6) of RFRR as a function of the number of samples \(n\) under source and capacity conditions eq. (37) and power-law assumptions \(\lambda=n^{-(\ell-1)}\), \(p=n^{q}\), with noise variance \(\sigma_{e}^{2}=0.1\). Solid lines are obtained from the deterministic equivalent Theorem 3.3. In the figure on the left, points are finite size numerical experiments. Dashed and dotted lines are the analytical rates from Theorem 4.1, stated in the legend. The colour scheme corresponds to the regions of Fig. 2.

#### Acknowledgements

We would like to thank Yasaman Bahri, Hugo Cui and Florent Krzakala for stimulating discussions. BL & LD acknowledges funding from the _Choose France - CNRS AI Rising Talents_ program.

## References

* Aguirre-Lopez et al. (2024) Fabian Aguirre-Lopez, Silvio Franz, and Mauro Pastore. Random features and polynomial rules. _arXiv preprint arXiv:2402.10164_, 2024.
* Ali et al. (2019) Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least squares regression. In _The 22nd international conference on artificial intelligence and statistics_, pages 1370-1378. PMLR, 2019.
* Ali et al. (2020) Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic gradient flow for least squares. In _International conference on machine learning_, pages 233-244. PMLR, 2020.
* Atanasov et al. (2024) Alexander B. Atanasov, Jacob A. Zavatone-Veth, and Cengiz Pehlevan. Scaling and renormalization in high-dimensional regression. _arXiv preprint arXiv:2405.00592_, 2024.
* Ba et al. (2022) Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 37932-37946. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/f7e7fabd73b3df96c54a320862afcbf78-Paper-Conference.pdf.
* Bach (2024) Francis Bach. High-dimensional analysis of double descent for linear regression with random projections. _SIAM Journal on Mathematics of Data Science_, 6(1):26-50, 2024. doi: 10.1137/23M1558781. URL https://doi.org/10.1137/23M1558781.
* Bahri et al. (2024) Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws. _Proceedings of the National Academy of Sciences_, 121(27):e2311878121, July 2024. doi: 10.1073/pnas.2311878121. URL https://www.pnas.org/doi/abs/10.1073/pnas.2311878121.
* Balcan et al. (2006) Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. Kernels as features: On kernels, margins, and low-dimensional mappings. _Machine Learning_, 65(1):79-94, 2006.
* Bartlett et al. (2020) Peter L. Bartlett, Philip M. Long, Gabor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_, 117(48):30063-30070, 2020. doi: 10.1073/pnas.1907378117. URL https://www.pnas.org/doi/abs/10.1073/pnas.1907378117.
* Bartlett et al. (2021) Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. Deep learning: a statistical viewpoint. _Acta Numerica_, 30:87-201, 2021. doi: 10.1017/S0962492921000027.
* Belkin (2021) Mikhail Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation. _Acta Numerica_, 30:203-248, 2021. doi: 10.1017/S0962492921000039.
* Belkin et al. (2019) Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. _Proceedings of the National Academy of Sciences_, 116(32):15849-15854, August 2019. doi: 10.1073/pnas.1903070116. URL https://www.pnas.org/doi/full/10.1073/pnas.1903070116.
* Bordelon et al. (2020) Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 1024-1034. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/bordelon20a.html.
* Bordelon et al. (2024) Blake Bordelon, Alexander Atanasov, and Cengiz Pehlevan. A dynamical model of neural scaling laws. _arXiv preprint arXiv:2402.01092_, 2024.
* Bordelon et al. (2020)David Bosch, Ashkan Panahi, and Babak Hassibi. Precise asymptotic analysis of deep random feature models. In Gergely Neu and Lorenzo Rosasco, editors, _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pages 4132-4179. PMLR, 12-15 Jul 2023a. URL https://proceedings.mlr.press/v195/bosch23a.html.
* Bosch et al. (2023) David Bosch, Ashkan Panahi, Ayca Ozcelikkale, and Devdatt Dubhashi. Random features model with general convex regularization: A fine grained analysis with precise asymptotic learning curves. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent, editors, _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_, volume 206 of _Proceedings of Machine Learning Research_, pages 11371-11414. PMLR, 25-27 Apr 2023b. URL https://proceedings.mlr.press/v206/bosch23a.html.
* Cagnetta et al. (2023) Francesco Cagnetta, Alessandro Favero, and Matthieu Wyart. What can be learnt with wide convolutional neural networks? In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 3347-3379. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/cagnetta23a.html.
* Caponnetto and De Vito (2007) Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. _Foundations of Computational Mathematics_, 7(3):331-368, 2007.
* Cheng and Montanari (2022) Chen Cheng and Andrea Montanari. Dimension free ridge regression. _arXiv preprint arXiv:2210.08571_, 2022.
* Chizat et al. (2019) Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf.
* Chouard (2022) Clement Chouard. Quantitative deterministic equivalent of sample covariance matrices with a general dependence structure. _arXiv preprint arXiv:2211.13044_, 2022.
* Chouard (2023) Clement Chouard. Deterministic equivalent of the conjugate kernel matrix associated to artificial neural networks. _arXiv preprint arXiv:2306.05850_, 2023.
* Cui et al. (2022) Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Generalization error rates in kernel regression: the crossover from the noiseless to noisy regime. _Journal of Statistical Mechanics: Theory and Experiment_, 2022(11):114004, nov 2022. doi: 10.1088/1742-5468/ac9829. URL https://dx.doi.org/10.1088/1742-5468/ac9829.
* Cui et al. (2023) Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Error scaling laws for kernel classification under source and capacity conditions. _Machine Learning: Science and Technology_, 4(3):035033, aug 2023. doi: 10.1088/2632-2153/acf041. URL https://dx.doi.org/10.1088/2632-2153/acf041.
* Cui et al. (2024) Hugo Cui, Luca Pesce, Yatin Dandi, Florent Krzakala, Yue Lu, Lenka Zdeborova, and Bruno Loureiro. Asymptotics of feature learning in two-layer networks after one gradient-step. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, _Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine Learning Research_, pages 9662-9695. PMLR, 21-27 Jul 2024. URL https://proceedings.mlr.press/v235/cui24d.html.
* Dandi et al. (2023) Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. How two-layer neural networks learn, one (giant) step at a time. _arXiv preprint arXiv:2305.18270_, 2023.
* Dhifallah and Lu (2020) Oussama Dhifallah and Yue M. Lu. A precise performance analysis of learning with random features. _arXiv preprint arXiv:2008.11904_, 2020.
* Dobriban and Wager (2018) Edgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: Ridge regression and classification. _The Annals of Statistics_, 46(1):247-279, 2018. ISSN 00905364, 21688966. URL https://www.jstor.org/stable/26542784.

Elvis Dohmatob, Yunzhen Feng, and Julia Kempe. Model collapse demystified: The case of regression. _arXiv preprint arXiv:2402.07712_, 2024.
* Fan and Wang (2020) Zhou Fan and Zhichao Wang. Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 7710-7721. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/572201a4497b0bbf02d4f279b09ec30d-Paper.pdf.
* Favero et al. (2021) Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimensionality in convolutional teacher-student scenarios. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 9456-9467. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/4e8eaf897c638d519710b1691121f8cb-Paper.pdf.
* Gerace et al. (2021) Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Generalisation error in learning with random features and the hidden manifold model. _Journal of Statistical Mechanics: Theory and Experiment_, 2021(12):124013, dec 2021. doi: 10.1088/1742-5468/ac3ae6. URL https://dx.doi.org/10.1088/1742-5468/ac3ae6.
* Ghorbani et al. (2019) Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy training of two-layers neural network. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/file/c133f1bbb634af68c5088f3438848bfd-Paper.pdf.
* Ghorbani et al. (2020) Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks outperform kernel methods? In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 14820-14830. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/a9df2255ad642b923d95503b9a7958d8-Paper.pdf.
* Goldt et al. (2022) Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. The gaussian equivalence of generative models for learning with shallow neural networks. In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors, _Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference_, volume 145 of _Proceedings of Machine Learning Research_, pages 426-471. PMLR, 16-19 Aug 2022. URL https://proceedings.mlr.press/v145/goldt22a.html.
* 986, 2022. doi: 10.1214/21-AOS2133. URL https://doi.org/10.1214/21-AOS2133.
* Hu and Lu (2023) Hong Hu and Yue M. Lu. Universality laws for high-dimensional learning with random features. _IEEE Transactions on Information Theory_, 69(3):1932-1964, 2023. doi: 10.1109/TIT.2022.3217698.
* Hu et al. (2024) Hong Hu, Yue M. Lu, and Theodor Misiakiewicz. Asymptotics of random feature regression beyond the linear scaling regime. _arXiv preprint arXiv:2403.08160_, 2024.
* Jacot et al. (2018) Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1ifa34e62bb8a6ec6b91d2462f5a-Paper.pdf.
* Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* Lecun et al. (1998) Yan Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
* Lill et al. (2019)Yicheng Li, haobo Zhang, and Qian Lin. On the asymptotic learning curves of kernel ridge regression under power-law decay. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 49341-49364. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/9adc8ada9183f4b9a007a02773fd8114-Paper-Conference.pdf.
* Liao and Couillet [2018] Zhenyu Liao and Romain Couillet. On the spectrum of random features maps of high dimensional data. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 3063-3071. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/liao18a.html.
* Lin et al. [2024] Licong Lin, Jingfeng Wu, Sham M Kakade, Peter L Bartlett, and Jason D Lee. Scaling laws in linear regression: Compute, parameters, and data. _arXiv preprint arXiv:2406.08466_, 2024.
* Loureiro et al. [2022] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc Mezard, and Lenka Zdeborova. Learning curves of generic features maps for realistic datasets with a teacher-student model. _Journal of Statistical Mechanics: Theory and Experiment_, 2022(11):114001, nov 2022. doi: 10.1088/1742-5468/ac9825. URL https://dx.doi.org/10.1088/1742-5468/ac9825.
* Maloney et al. [2022] Alexander Maloney, Daniel A. Roberts, and James Sully. A solvable model of neural scaling laws. _preprint arXiv:2210.16859_, 2022.
* Mei and Montanari [2022] Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. _Communications on Pure and Applied Mathematics_, 75(4):667-766, 2022. doi: https://doi.org/10.1002/cpa.22008. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.22008.
* Mei et al. [2022] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random feature and kernel methods: Hypercontractivity and kernel matrix concentration. _Applied and Computational Harmonic Analysis_, 59:3-84, 2022. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2021.12.003. URL https://www.sciencedirect.com/science/article/pii/S1063520321001044. Special Issue on Harmonic Analysis and Machine Learning.
* Misiakiewicz and Mei [2022] Theodor Misiakiewicz and Song Mei. Learning with convolution and pooling operations in kernel methods. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 29014-29025. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/baBaee784ffe0813890288b33444eda-Paper-Conference.pdf.
* Misiakiewicz and Saeed [2024] Theodor Misiakiewicz and Basil Saeed. A non-asymptotic theory of kernel ridge regression: deterministic equivalents, test error, and gcv estimator. _arXiv preprint arXiv:2403.08938_, 2024.
* Moniri et al. [2024] Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban. A theory of non-linear feature learning with one gradient step in two-layer neural networks. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, _Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine Learning Research_, pages 36106-36159. PMLR, 21-27 Jul 2024. URL https://proceedings.mlr.press/v235/moniri24a.html.
* Nakkiran et al. [2021] Preetum Nakkiran, Gal Kaplan, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: where bigger models and more data hurt. _Journal of Statistical Mechanics: Theory and Experiment_, 2021(12):124003, dec 2021. doi: 10.1088/1742-5468/ac3a74. URL https://dx.doi.org/10.1088/1742-5468/ac3a74.
* Paquette et al. [2024] Elliot Paquette, Courtney Paquette, Lechao Xiao, and Jeffrey Pennington. 4+3 phases of compute-optimal neural scaling laws. _arXiv preprint arXiv:2405.15074_, 2024.
* Rahimi and Recht [2007] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, _Advances in Neural Information Processing Systems_, volume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf.
* Recht et al. [2018]Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, _Advances in Neural Information Processing Systems_, volume 21. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/paper_files/paper/2008/file/Oefe32849d230d7f53049ddc4a4b0c60-Paper.pdf.
* Richards et al. [2021] Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge(less) regression under general source condition. In Arindam Banerjee and Kenji Fukumizu, editors, _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 3889-3897. PMLR, 13-15 Apr 2021. URL https://proceedings.mlr.press/v130/richards21b.html.
* Rudi and Rosasco [2017] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/61b1fb3f59e28c67f3925f3c79be81a1-Paper.pdf.
* Schroder et al. [2023] Dominik Schroder, Hugo Cui, Daniil Dmitriev, and Bruno Loureiro. Deterministic equivalent and error universality of deep random features learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 30285-30320. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/schroder23a.html.
* Schroder et al. [2024] Dominik Schroder, Daniil Dmitriev, Hugo Cui, and Bruno Loureiro. Asymptotics of learning with deep structured (Random) features. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, _Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine Learning Research_, pages 43862-43894. PMLR, 21-27 Jul 2024. URL https://proceedings.mlr.press/v235/schroder24a.html.
* Simon et al. [2023a] James B. Simon, Madeline Dickens, Dhruva Karkada, and Michael R. DeWeese. The eigenlearning framework: A conservation law perspective on kernel regression and wide neural networks. _arXiv preprint arXiv:2110.03922_, 2023a.
* Simon et al. [2023b] James B. Simon, Dhruva Karkada, Nikhil Ghosh, and Mikhail Belkin. More is better in modern machine learning: when infinite overparameterization is optimal and overfitting is obligatory. _arXiv preprint arXiv:2311.14646_, 2023b.
* Sonthalia et al. [2024] Rishi Sonthalia, Jackie Lok, and Elizaveta Rebrova. On regularization via early stopping for least squares regression. _arXiv preprint arXiv:2406.04425_, 2024.
* Spigler et al. [2019] S Spigler, M Geiger, S d'Ascoli, L Sagun, G Biroli, and M Wyart. A jamming transition from under- to over-parametrization affects generalization in deep learning. _Journal of Physics A: Mathematical and Theoretical_, 52(47):474001, oct 2019. doi: 10.1088/1751-8121/ab4c8b. URL https://dx.doi.org/10.1088/1751-8121/ab4c8b.
* Spigler et al. [2020] Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods: empirical data versus teacher-student paradigm. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(12):124001, dec 2020. doi: 10.1088/1742-5468/abc61d. URL https://dx.doi.org/10.1088/1742-5468/abc61d.
* Tropp et al. [2015] Joel A Tropp et al. An introduction to matrix concentration inequalities. _Foundations and Trends(r) in Machine Learning_, 8(1-2):1-230, 2015.
* Vershynin [2010] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. _arXiv preprint arXiv:1011.3027_, 2010.
* Wang et al. [2024] Zhichao Wang, Denny Wu, and Zhou Fan. Nonlinear spiked covariance matrices and signal propagation in deep neural networks. _arXiv preprint arXiv:2402.10127_, 2024.
* Wang et al. [2020]

[MISSING_PAGE_FAIL:16]

## Appendix A Background on deterministic equivalents

We consider a feature vector \(\bm{f}\in\mathbb{R}^{q}\), \(q\in\mathbb{N}\cup\{\infty\}\), with covariance matrix \(\bm{\Sigma}=\mathbb{E}[\bm{f}\bm{f}^{\mathsf{T}}]\). We denote \(\gamma_{1}^{2}\geq\gamma_{2}^{2}\geq\gamma_{3}^{2}\geq\cdots\) the eigenvalues of \(\bm{\Sigma}\) in non-increasing order. In the case of infinite-dimensional features \(q=\infty\), we will further assume that \(\operatorname{Tr}(\bm{\Sigma})<\infty\), i.e., we consider \(\bm{\Sigma}\) to be a trace-class self-adjoint operator.

We assume that the feature \(\bm{f}\) satisfies the following assumption.

**Assumption A.1** (Feature concentration).: _There exist \(\mathsf{c}_{*},\mathsf{C}_{*}>0\) such that for any p.s.d. matrix \(\bm{A}\in\mathbb{R}^{q\times q}\) with \(\operatorname{Tr}(\bm{\Sigma}\bm{A})<\infty\), we have_

\[\mathbb{P}\left(\left|\bm{f}^{\mathsf{T}}\bm{A}\bm{f}-\operatorname{Tr}(\bm{ \Sigma}\bm{A})\right|\geq t\cdot\|\bm{\Sigma}^{1/2}\bm{A}\bm{\Sigma}^{1/2}\|_{ F}\right)\leq\mathsf{C}_{*}\exp\left\{-\mathsf{c}_{*}t\right\}.\] (49)

Recall that we denote \(C_{a_{1},\ldots,a_{k}}\) constants that only depend on the values of \(\{a_{i}\}_{i\in[k]}\). We use \(a_{i}=`*\) to denote the dependency on the constants \(\mathsf{c}_{*},\mathsf{C}_{*}\) from Assumption A.1.

We will further define the following two quantities.

**Definition 2** (Effective regularization).: _For an integer \(n\), covariance \(\bm{\Sigma}\), and regularization \(\lambda\geq 0\), we define the effective regularization \(\lambda_{*}\) associated to model \((n,\bm{\Sigma},\lambda)\) to be the unique non-negative solution to the equation_

\[n-\frac{\lambda}{\lambda_{*}}=\operatorname{Tr}\bigl{(}\bm{\Sigma}(\bm{ \Sigma}+\lambda_{*})^{-1}\bigr{)}.\] (50)

Throughout this appendix, we assume that \(\lambda>0\). The existence and uniqueness follow from noticing that the left-hand side is monotonically increasing in \(\lambda_{*}\) while the right-hand side is monotonically decreasing. We consider the change of variable \(\mu_{*}:=\mu_{*}(\lambda)=\lambda/\lambda_{*}\), such that \(\mu_{*}\) is the unique non-negative solution of

\[\mu_{*}=\frac{n}{1+\operatorname{Tr}(\bm{\Sigma}(\mu_{*}\bm{\Sigma}+\lambda) ^{-1})}.\] (51)

Both \(\mu_{*}\) and \(\lambda_{*}\) are increasing functions with \(\lambda\).

**Definition 3** (Intrinsic dimension).: _For a covariance matrix \(\bm{\Sigma}\in\mathbb{R}^{q\times q}\) with eigenvalues in nonincreasing order \(\gamma_{1}^{2}\geq\gamma_{2}^{2}\geq\gamma_{3}^{2}\geq\cdots\), we define the intrinsic dimension \(r_{\bm{\Sigma}}(k)\) at level \(k\in\mathbb{N}\) of \(\bm{\Sigma}\) to be the intrinsic dimension of the covariance matrix \(\bm{\Sigma}_{\geq k}=\operatorname{diag}(\gamma_{k}^{2},\gamma_{k+1}^{2},\ldots)\), i.e., the covariance matrix projected orthogonally to the top \(k-1\) eigenspaces, which is given by_

\[r_{\bm{\Sigma}}(k):=\frac{\operatorname{Tr}(\bm{\Sigma}_{\geq k})}{\|\bm{ \Sigma}_{\geq k}\|_{\mathrm{op}}}=\frac{\sum_{j=k}^{q}\gamma_{j}^{2}}{\gamma_ {k}^{2}}.\]

The intrinsic dimension of \(\bm{\Sigma}_{\geq k}\) captures the number of dimensions of \(\bm{\Sigma}_{\geq k}\) that have significant spectral content, i.e., \(\gamma_{j}^{2}\approx\|\bm{\Sigma}_{\geq k}\|_{\mathrm{op}}\) (see (Tropp et al., 2015, Chapter 7) for further background).

We are given \(n\) i.i.d. features \((\bm{f}_{i})_{i\in[n]}\), and we denote \(\bm{F}=[\bm{f}_{1},\ldots,\bm{f}_{n}]^{\mathsf{T}}\in\mathbb{R}^{n\times q}\) the feature matrix. The train and test errors are functionals of the feature matrix \(\bm{F}\). In particular, they depend on the following resolvent matrix

\[\bm{R}=(\bm{F}^{\mathsf{T}}\bm{F}+\lambda\mathbf{I}_{q})^{-1}.\]

In this section we consider functionals that depend on products of \(\bm{F}\), \(\bm{R}\) and deterministic matrices. For a general p.s.d. matrix \(\bm{A}\in\mathbb{R}^{q\times q}\), define the functionals

\[\Phi_{1}(\bm{F};\bm{A},\lambda) :=\operatorname{Tr}\left(\bm{A}\bm{\Sigma}^{1/2}(\bm{F}^{\mathsf{T }}\bm{F}+\lambda)^{-1}\bm{\Sigma}^{1/2}\right),\] \[\Phi_{2}(\bm{F};\lambda) :=\operatorname{Tr}\left(\frac{\bm{F}^{\mathsf{T}}\bm{F}}{n}(\bm{ F}^{\mathsf{T}}\bm{F}+\lambda)^{-1}\right),\] \[\Phi_{3}(\bm{F};\bm{A},\lambda) :=\operatorname{Tr}\left(\bm{A}\bm{\Sigma}^{1/2}(\bm{F}^{\mathsf{ T}}\bm{F}+\lambda)^{-1}\bm{\Sigma}(\bm{F}^{\mathsf{T}}\bm{F}+\lambda)^{-1}\bm{ \Sigma}^{1/2}\right),\] \[\Phi_{4}(\bm{F};\bm{A},\lambda) :=\operatorname{Tr}\left(\bm{A}\bm{\Sigma}^{1/2}(\bm{F}^{\mathsf{T }}\bm{F}+\lambda)^{-1}\frac{\bm{F}^{\mathsf{T}}\bm{F}}{n}(\bm{F}^{\mathsf{T}} \bm{F}+\lambda)^{-1}\bm{\Sigma}^{1/2}\right).\]These functionals are well approximated by quantities proportional to

\[\Psi_{1}(\lambda_{*};\bm{A}) :=\operatorname{Tr}\left(\bm{A}\bm{\Sigma}(\bm{\Sigma}+\lambda_{*}) ^{-1}\right),\] \[\Psi_{2}(\lambda_{*}) :=\frac{1}{n}\operatorname{Tr}\left(\bm{\Sigma}(\bm{\Sigma}+ \lambda_{*})^{-1}\right),\] \[\Psi_{3}(\lambda_{*};\bm{A}) :=\frac{1}{n}\cdot\frac{\operatorname{Tr}(\bm{A}\bm{\Sigma}^{2}( \bm{\Sigma}+\lambda_{*})^{-2})}{n-\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{ \Sigma}+\lambda_{*})^{-2})}.\]

Without loss of generality, we can assume that \(\operatorname{Tr}(\bm{A}\bm{\Sigma})<\infty\) for \(\Phi_{1}\), as otherwise \(\Phi_{1}(\bm{F};\bm{A},\lambda)=\Psi_{1}(\mu_{*};\bm{A},\lambda)=\infty\) almost surely, and \(\operatorname{Tr}(\bm{A}\bm{\Sigma}^{2})<\infty\) for \(\Phi_{3}\) and \(\Phi_{4}\), as otherwise \(\Phi_{j}(\bm{F};\bm{A},\lambda)=\Psi_{j}(\mu_{*};\bm{A},\lambda)=\infty\), \(j=3,4\), almost surely.

Our relative approximation bound will depend on the covariance matrix \(\bm{\Sigma}\) through

\[\rho_{\lambda}(n)=1+\frac{n\gamma_{\lfloor\eta_{*}\cdot n\rfloor}^{2}}{\lambda }\left\{1+\frac{r_{\bm{\Sigma}}(\lfloor\eta_{*}\cdot n\rfloor)\lor n}{n}\log \left(r_{\bm{\Sigma}}(\lfloor\eta_{*}\cdot n\rfloor)\lor n\right)\right\},\] (52)

where \(\eta_{*}\in(0,1/2)\) is a constant that will only depend on \(\mathsf{c}_{*},\mathsf{C}_{*}\) and we used the convention that \(\gamma_{\lfloor\eta_{*}\cdot n\rfloor}^{2}=0\) if \(\lfloor\eta_{*}\cdot n\rfloor>q\).

The following theorem gathers the approximation guarantees for the different functionals stated above, and is obtained by modifying (Misiakiewicz and Saeed, 2024, Theorem 4).

**Theorem A.2** (Dimension-free deterministic equivalents).: _Assume the features \((\bm{f}_{i})_{i\in[n]}\) satisfy Assumption A.1 with some constants \(\mathsf{c}_{x},\mathsf{C}_{x},\beta>0\). For any \(D,K>0\), there exist constants \(\eta:=\eta_{x}\in(0,1/2)\) (only depending on \(\mathsf{c}_{x},\mathsf{C}_{x},\beta\)), \(C_{D,K}>0\) (only depending on \(K,D\)), and \(C_{x,D,K}>0\) (only depending on \(\mathsf{c}_{x},\mathsf{C}_{x},\beta,D,K\)), such that the following holds. For all \(n\geq C_{D,K}\) and \(\lambda>0\), if it holds that_

\[\lambda\cdot\rho_{\lambda}(n)\geq\|\bm{\Sigma}\|_{\mathrm{op}}\cdot n^{-K}, \qquad\rho_{\lambda}(n)^{5/2}\log^{3/2}(n)\leq K\sqrt{n},\] (53)

_then for any p.s.d. matrix \(\bm{A}\), we have with probability at least \(1-n^{-D}\) that_

\[\left|\Phi_{1}(\bm{F};\bm{A},\lambda)-\frac{\lambda_{*}}{\lambda} \Psi_{1}(\lambda_{*};\bm{A})\right| \leq C_{x,D,K}\frac{\rho_{\lambda}(n)^{5/2}\log^{3/2}(n)}{\sqrt{n }}\cdot\frac{\lambda_{*}}{\lambda}\Psi_{1}(\lambda_{*};\bm{A}),\] (54) \[\left|\Phi_{2}(\bm{F};\lambda)-\Psi_{2}(\lambda_{*})\right| \leq C_{x,D,K}\frac{\rho_{\lambda}(n)^{5/2}\log^{3/2}(n)}{\sqrt{n }}\Psi_{2}(\lambda_{*}),\] (55) \[\left|\Phi_{3}(\bm{F};\bm{A},\lambda)-\left(\frac{n\lambda_{*}}{ \lambda}\right)^{2}\Psi_{3}(\mu_{*};\bm{A},\lambda)\right| \leq C_{x,D,K}\frac{\rho_{\lambda}(n)^{6}\log^{5/2}(n)}{\sqrt{n }}\cdot\left(\frac{n\lambda_{*}}{\lambda}\right)^{2}\Psi_{3}(\mu_{*};\bm{A}, \lambda),\] (56) \[\left|\Phi_{4}(\bm{F};\bm{A},\lambda)-\Psi_{3}(\lambda_{*};\bm{A })\right| \leq C_{x,D,K}\frac{\rho_{\lambda}(n)^{6}\log^{3/2}(n)}{\sqrt{n }}\Psi_{3}(\lambda_{*};\bm{A}).\] (57)

Proof of Theorem a.2.: The only difference between this theorem and (Misiakiewicz and Saeed, 2024, Theorem 4) comes from the definition of \(\rho_{\lambda}(n)\). This new definition is obtained by slightly modifying the proof bounding the operator norm of \(\bm{\Sigma}^{1/2}\bm{R}\bm{\Sigma}^{1/2}\) from (Cheng and Montanari, 2022, Lemma 7.2) and (Misiakiewicz and Saeed, 2024, Lemma 1). In particular, we will simply modify step 2 in the proof of (Misiakiewicz and Saeed, 2024, Lemma 1). Consider \(\bm{F}_{+}=[\bm{f}_{+,1},\ldots,\bm{f}_{+,n}]^{\mathsf{T}}\in\mathbb{R}^{n \times(q-k_{*})}\) where \(\bm{f}_{+,i}\) correspond to the projection orthogonal to the top \(k_{*}:=\lfloor\eta_{*}n\rfloor-1\) eigenspaces with covariance matrix

\[\bm{\Sigma}_{+}:=\mathbb{E}[\bm{f}_{+,i}\bm{f}_{+,i}^{\mathsf{T}}]=\mathrm{ diag}(\gamma_{k_{*}}^{2},\gamma_{k_{*}+1}^{2},\ldots,\gamma_{q}^{2}).\]

Then, denoting \(\bm{S}=\sum_{i\in[n]}\bm{S}_{i}\) with \(\bm{S}_{i}:=\bm{f}_{+,i}\bm{f}_{i,+}^{\mathsf{T}}\), we have with probability at least \(1-n^{-D}\), for any \(i\in[n]\),

\[\|\bm{S}_{i}\|_{\mathrm{op}}\leq\operatorname{Tr}(\bm{\Sigma}_{+})+C_{*,D}\cdot \log(n)\sqrt{\gamma_{k_{*}}^{2}\operatorname{Tr}(\bm{\Sigma}_{+})}\leq \operatorname{Tr}(\bm{\Sigma}_{+})\left(1+C_{*,D}\frac{\log(n)}{\sqrt{r_{\bm{ \Sigma}}(k_{*})}}\right)=:L_{n}.\]Denoting \(\widetilde{\bm{S}}=\sum_{i\in[n]}\widetilde{\bm{S}}_{i}\) with \(\widetilde{\bm{S}}_{i}:=\bm{S}_{i}\mathbbm{1}_{\|\bm{S}_{i}\|_{\mathrm{op}}\leq L _{n}}\), so that \(\widetilde{\bm{S}}=\bm{S}\) with probability at least \(1-n^{-D}\), we obtain

\[\|\widetilde{\bm{S}}\|_{\mathrm{op}}\leq nL_{n}\gamma_{k_{*}}^{2} =:v_{n},\qquad\quad\|\mathbb{E}[\widetilde{\bm{S}}]\|_{\mathrm{op}}\leq n\| \mathbb{E}[\bm{S}_{i}]\|_{\mathrm{op}}=n\gamma_{k_{*}}^{2}.\]

Therefore, applying the matrix Bernstein's inequality with intrinsic dimension (Tropp et al., 2015, Theorem 7.3.1) to \(\widetilde{\bm{S}}\) gives that with probability at least \(1-n^{-D}\),

\[\|\widetilde{\bm{S}}\|_{\mathrm{op}} \leq n\gamma_{k_{*}}^{2}+C_{D}\left(\sqrt{v_{n}}+L_{n}\right) \sqrt{\log(r_{\bm{\Sigma}}(k_{*})n)}\] \[\leq n\gamma_{k_{*}}^{2}+C_{D}L_{n}\log(r_{\bm{\Sigma}}(k_{*})n)\] \[\leq n\gamma_{k_{*}}^{2}\left\{1+\frac{r_{\bm{\Sigma}}(k_{*})}{n }\left(1+C_{*,D}\frac{\log(n)}{\sqrt{r_{\bm{\Sigma}}(k_{*})}}\right)\log(r_{ \bm{\Sigma}}(k_{*})n)\right\}.\]

Note that by the condition of our theorem, \(\log(n)\leq K\sqrt{n}\), and therefore

\[\|\widetilde{\bm{S}}\|_{\mathrm{op}} \leq C_{*,D,K}\cdot n\gamma_{k_{*}}^{2}\left\{1+\frac{r_{\bm{ \Sigma}}(k_{*})}{n}\left(1+\sqrt{\frac{n}{r_{\bm{\Sigma}}(k_{*})}}\right)\log( r_{\bm{\Sigma}}(k_{*})n)\right\}\] \[\leq C_{*,D,K}\cdot n\gamma_{k_{*}}^{2}\left\{1+\frac{r_{\bm{ \Sigma}}(k_{*})\lor n}{n}\log(r_{\bm{\Sigma}}(k_{*})\lor n)\right\}.\]

Following the rest of the argument in (Misiakiewicz and Saeed, 2024, Lemma 1) we obtain \(\rho_{\lambda}(n)\) in Eq. (52). 

## Appendix B Proof of the deterministic equivalent for RFRR

In this appendix, we prove the approximation guarantees stated in Theorem 3.3 between the test error of RFRR and its deterministic equivalent. We start in Section B.1 by introducing background and notations that we will use throughout the proof. Section B.2 introduces key results on the covariance matrix and the fixed points. We then leverage these results to prove deterministic equivalents for different functionals of \(\bm{Z}=(\sigma(\langle\bm{x}_{i},\bm{w}_{j}\rangle))_{i\in[n],j\in[p]}\in \mathbb{R}^{n\times p}\) conditional on \((\bm{w}_{j})_{j\in[p]}\) in Section B.3, and functionals of \(\bm{F}=(\xi_{k}\phi_{k}(\bm{w}_{j}))_{j\in[p],k\geq 1}\in\mathbb{R}^{p\times\infty}\) in Section B.4. Given these deterministic equivalents, we prove our approximation guarantees for the variance term in Section B.5, and for the bias term in B.6. Finally, we deffer the proof of some technical results to Section B.7.

### Preliminaries

Recall that throughout the paper, we will keep track of the parameters of the problem \((n,p,\bm{\Sigma},\lambda,\sigma_{\varepsilon}^{2})\). For the other constants \(\mathsf{C}_{*},K,D\), we will denote \(C_{a_{1},a_{2},\ldots,a_{k}}\) constants that only depend on the values of \(\{a_{i}\}_{i\in[k]}\). We use \(a_{i}=\cdot*\)' to denote the dependency on the constant \(\mathsf{C}_{*}\) appearing in Assumption 3.1 and Assumption 3.2.

Throughout this appendix, we will directly work in the 'feature space'

\[\bm{g}_{i}:=(\psi_{k}(\bm{x}_{i}))_{k\geq 1},\qquad\quad\text{and}\qquad\quad \bm{f}_{j}:=(\xi_{k}\phi_{k}(\bm{w}_{j}))_{k\geq 1},\]

with distribution induced by \(\bm{x}_{i}\sim\mu_{x}\) and \(\bm{w}_{j}\sim\mu_{w}\). We will denote the covariate feature and weight feature matrices by

\[\bm{G}:=[\bm{g}_{1},\ldots,\bm{g}_{n}]^{\mathsf{T}}\in\mathbb{R}^{n\times \infty},\qquad\qquad\bm{F}:=[\bm{f}_{1},\ldots,\bm{f}_{p}]^{\mathsf{T}}\in \mathbb{R}^{p\times\infty}.\]

We denote the random feature weight vector

\[\bm{z}_{i}:=\frac{1}{\sqrt{p}}[\sigma(\langle\bm{w}_{1},\bm{x}_{i}\rangle), \ldots,\sigma(\langle\bm{w}_{p},\bm{x}_{i}\rangle)]=\frac{1}{\sqrt{p}}\bm{F} \bm{g}_{i}\in\mathbb{R}^{p},\]

and the associated feature matrix

\[\bm{Z}=[\bm{z}_{1},\ldots,\bm{z}_{n}]^{\mathsf{T}}=\frac{1}{\sqrt{p}}\bm{G}\bm{F }^{\mathsf{T}}\in\mathbb{R}^{n\times p}.\]

Note that \(\bm{f}\) has covariance matrix

\[\bm{\Sigma}:=\mathbb{E}[\bm{f}\bm{f}^{\mathsf{T}}]=\mathrm{diag}(\xi_{1}^{2}, \xi_{2}^{2},\xi_{3}^{2},\ldots).\]We will further introduce the covariance matrix of \(\bm{z}\) conditional on the weight feature matrix \(\bm{F}\) (i.e., conditional on \((\bm{w}_{j})_{j\in[p]}\))

\[\widehat{\bm{\Sigma}}_{\bm{F}}:=\mathbb{E}_{\bm{z}}\left[\bm{z}\bm{z}^{\top} \middle|\bm{F}\right]=\frac{1}{p}\bm{F}\bm{F}^{\top}\in\mathbb{R}^{p\times p}.\] (58)

Note that under Assumption 3.1, the features \(\bm{z}\) and \(\bm{f}\) satisfy the following assumption.

**Assumption B.1** (Concentration of the features \(\bm{z}\) and \(\bm{f}\)).: _There exists a constant \(\mathsf{C}_{*}>0\) such that for any weight feature matrix \(\bm{F}\in\mathbb{R}^{p\times\infty}\) and deterministic p.s.d. matrix \(\bm{A}\in\mathbb{R}^{p\times p}\) with \(\operatorname{Tr}(\widehat{\bm{\Sigma}}_{\bm{F}}\bm{A})<\infty\), we have_

\[\mathbb{P}_{\bm{z}\mid\bm{F}}\left(\left|\bm{z}^{\top}\bm{A}\bm{z}- \operatorname{Tr}(\widehat{\bm{\Sigma}}_{\bm{F}}\bm{A})\right|\geq t\cdot \left\|\widehat{\bm{\Sigma}}^{1/2}_{\bm{F}}\bm{A}\widehat{\bm{\Sigma}}^{1/2}_{ \bm{F}}\right\|_{F}\right)\leq\mathsf{C}_{*}\exp\left\{-t/\mathsf{C}_{x} \right\},\] (59)

_and for any deterministic p.s.d. matrix \(\bm{B}\in\mathbb{R}^{\infty\times\infty}\) with \(\operatorname{Tr}(\bm{\Sigma}\bm{B})<\infty\),_

\[\mathbb{P}_{\bm{f}}\left(\left|\bm{f}^{\top}\bm{B}\bm{f}- \operatorname{Tr}(\bm{\Sigma}\bm{B})\right|\geq t\cdot\left\|\bm{\Sigma}^{1/ 2}\bm{B}\bm{\Sigma}^{1/2}\right\|_{F}\right)\leq\mathsf{C}_{*}\exp\left\{-t/ \mathsf{C}_{x}\right\}.\] (60)

We will assume in the rest of this appendix that Assumption B.1 holds. Using the notations introduced above, we restate our setting below. Recall that we consider learning a target function \(h_{*}(\bm{g}):=\bm{g}^{\top}\bm{\beta}_{*}\) from i.i.d. samples \((y_{i},\bm{g}_{i})_{i\in[n]}\) with

\[y_{i}=\bm{g}_{i}^{\top}\bm{\beta}_{*}+\varepsilon_{i},\]

where \(\varepsilon_{i}\) are independent noise with \(\mathbb{E}[\varepsilon_{i}]=0\) and \(\mathbb{E}[\varepsilon_{i}^{2}]=\sigma_{\varepsilon}^{2}\). Denote \(\bm{y}=(y_{1},\ldots,y_{n})\) the vector containing the labels. We fit this data using a random feature model with i.i.d. random weight features \((\bm{f}_{j})_{j\in[p]}\)

\[\hat{f}(\bm{g})=\frac{1}{\sqrt{p}}\bm{g}^{\top}\bm{F}^{\top}\bm{a},\qquad\quad \bm{a}\in\mathbb{R}^{p}.\] (61)

We fit the parameter \(\bm{a}\) using random feature ridge regression (RFRR)

\[\hat{\bm{a}}_{\lambda}=\operatorname*{arg\,min}_{\bm{a}\in\mathbb{R}^{p}}\left\{ \|\bm{y}-\bm{Z}\bm{a}\|_{2}^{2}+\lambda\|\bm{a}\|_{2}^{2}\right\}=(\bm{Z}^{ \top}\bm{Z}+\lambda)^{-1}\bm{Z}^{\top}\bm{y}.\] (62)

The test error is then given by

\[\mathcal{R}_{\mathrm{test}}(h_{*};\bm{G},\bm{F},\lambda) :=\mathbb{E}_{\bm{z}}\left\{\mathbb{E}_{\bm{g}}\left[\left(\bm{g }^{\top}\bm{\beta}_{*}-\frac{1}{\sqrt{p}}\bm{g}^{\top}\bm{F}^{\top}\hat{\bm{a} }_{\lambda}\right)^{2}\right]\right\}\] (63) \[=\mathcal{B}(\bm{\beta}_{*};\bm{G},\bm{F},\lambda)+\mathcal{V}( \bm{G},\bm{F},\lambda),\]

where the bias and variance terms are given explicitly by

\[\mathcal{B}(\bm{\beta}_{*};\bm{G},\bm{F},\lambda) =\|\bm{\beta}_{*}-p^{-1/2}\bm{F}^{\top}(\bm{Z}^{\top}\bm{Z}+ \lambda)^{-1}\bm{Z}^{\top}\bm{G}\bm{\beta}_{*}\|_{2}^{2},\] (64) \[\mathcal{V}(\bm{G},\bm{F},\lambda) =\sigma_{\varepsilon}^{2}\cdot\operatorname{Tr}\bigl{(}\widehat{ \bm{\Sigma}}_{\bm{F}}\bm{Z}^{\top}\bm{Z}(\bm{Z}^{\top}\bm{Z}+\lambda)^{-2} \bigr{)}.\] (65)

Note that both the bias and variance terms are random quantities that depend on the random matrices \(\bm{G},\bm{F}\). The goal of this appendix is to prove _non-asymptotic_ and _multiplicative_ approximation guarantees between these two terms and deterministic quantities that only depend on the parameters of the model \((n,p,\bm{\Sigma},\bm{\beta}_{*},\lambda,\sigma_{\varepsilon}^{2})\), i.e., we will show that with high probability

\[\left|\mathcal{B}(\bm{\beta}_{*};\bm{G},\bm{F},\lambda)-\mathsf{ B}_{n,p}(\bm{\beta}_{*},\lambda)\right| =\widetilde{O}\left(n^{-1/2}+p^{-1/2}\right)\cdot\mathsf{B}_{n,p}( \bm{\beta}_{*},\lambda),\] \[\left|\mathcal{V}(\bm{G},\bm{F},\lambda)-\mathsf{V}_{n,p}(\lambda)\right| =\widetilde{O}\left(n^{-1/2}+p^{-1/2}\right)\cdot\mathsf{V}_{n,p} (\lambda),\]

where \(\mathsf{B}_{n,p}(\bm{\beta}_{*},\lambda)\) and \(\mathsf{V}_{n,p}(\lambda)\) are defined in Eqs (22) and (23), and the approximation rates \(\widetilde{O}(\cdot)\) are explicit in terms of the model parameters.

The proof of these approximation guarantees will proceed in two steps. We first show that the bias and variance terms conditional on \(\bm{F}\) are well approximated by functionals that only depend on \(\bm{F}\), i.e.,

\[\left|\mathcal{B}(\bm{\beta}_{*};\bm{G},\bm{F},\lambda)-\widetilde{ \mathcal{B}}(\bm{\beta}_{*};\bm{F},\lambda)\right| =\widetilde{O}\left(n^{-1/2}\right)\cdot\widetilde{\mathcal{B}}( \bm{\beta}_{*};\bm{F},\lambda),\] (66) \[\left|\mathcal{V}(\bm{G},\bm{F},\lambda)-\widetilde{\mathcal{V}}( \bm{F},\lambda)\right| =\widetilde{O}\left(n^{-1/2}\right)\cdot\widetilde{\mathcal{V}}( \bm{F},\lambda).\]We then show that \(\widetilde{\mathcal{B}}(\bm{\beta}_{*};\bm{F},\lambda)\) and \(\widetilde{\mathcal{V}}(\bm{F},\lambda)\) are well approximated by \(\mathsf{B}_{n,p}(\bm{\beta}_{*},\lambda)\) and \(\mathsf{V}_{n,p}(\lambda)\) with

\[\begin{split}\left|\widetilde{\mathcal{B}}(\bm{\beta}_{*};\bm{F}, \lambda)-\mathsf{B}_{n,p}(\bm{\beta}_{*},\lambda)\right|&= \widetilde{O}\left(p^{-1/2}\right)\cdot\mathsf{B}_{n,p}(\bm{\beta}_{*}, \lambda),\\ \left|\widetilde{\mathcal{V}}(\bm{F},\lambda)-\mathsf{V}_{n,p}( \lambda)\right|&=\widetilde{O}\left(p^{-1/2}\right)\cdot\mathsf{V }_{n,p}(\lambda).\end{split}\] (67)

For each of these two steps, we will apply general results showing deterministic equivalents for functionals of (possibly infinite-dimensional) random matrices proved in Misiakiewicz and Saeed (2024) (see Appendix A and in particular Theorem A.2 for some background). Note that when writing the proof, we will directly show Eq. (66) assuming that \(\bm{F}\) is in some good event \(\bm{F}\in\mathcal{A}_{\mathcal{F}}\), where \(\mathbb{P}_{\bm{F}}(\mathcal{A}_{\mathcal{F}})\geq 1-p^{-D}\), so that we can immediately write functionals with regularization parameter that does not depend on \(\widehat{\bm{\Sigma}}_{\bm{F}}\), and therefore the rate will be directly \(\widetilde{O}\left(n^{-1/2}+p^{-1/2}\right)\). However, we can reorganize the proof to indeed get the separate contributions (66) and (67) to the approximation error rate.

The rest of Appendix B is devoted to implementing this proof strategy. We start in the next three sections by introducing key technical results which we will use in the analysis of the bias and variance terms.

### Fixed points, feature covariance matrix, and tail rank

Recall that our deterministic equivalents will depend on the fixed points \((\nu_{1},\nu_{2})\in\mathbb{R}^{2}_{>0}\) stated in Definition 1. Furthermore, our approximation guarantees will depend on the covariance matrix \(\bm{\Sigma}\) through \(\rho_{\kappa}(p)\) and \(\widetilde{\rho}_{\kappa}(n,p)\) which we restate below for convenience

\[M_{\bm{\Sigma}}(k) =1+\frac{r_{\bm{\Sigma}}(\lfloor\eta_{*}\cdot k\rfloor)\lor k}{k} \log\left(r_{\bm{\Sigma}}(\lfloor\eta_{*}\cdot k\rfloor)\lor k\right),\] (68) \[\rho_{\kappa}(p) =1+\frac{p\cdot\xi_{\lfloor\eta_{*}\cdot p\rfloor}^{2}}{\kappa} M_{\bm{\Sigma}}(p),\] (69) \[\widetilde{\rho}_{\kappa}(n,p) =1+1[n\leq p/\eta_{*}]\cdot\left\{\frac{n\xi_{\lfloor\eta_{*} \cdot n\rfloor}^{2}}{\kappa}+\frac{n}{p}\cdot\rho_{\kappa}(p)\right\}M_{\bm{ \Sigma}}(n),\] (70)

where \(\eta_{*}\in(0,1/4)\) is a constant that only depends on \(\mathsf{C}_{*}\) appearing in Assumption B.1, and \(r_{\bm{\Sigma}}(k)\) is the intrinsic dimension of \(\bm{\Sigma}\) at level \(k\) (see Definition 3)

\[r_{\bm{\Sigma}}(k)=\frac{\sum_{j=k}^{p}\xi_{j}^{2}}{\xi_{k}^{2}}.\]

Observe that \(\widetilde{\rho}_{\kappa}(n,p)\) is well defined for \(n\to\infty\) while \(p\) stays constant with \(\widetilde{\rho}_{\kappa}(\infty,p)=1\), and for \(p\to\infty\) while \(n\) stays constant with \(\widetilde{\rho}_{\kappa}(n,\infty)=\rho_{\kappa}(n)\) (under the conditions in our setting that \(\rho_{\kappa}(p)\leq K\sqrt{p}\) for some constant \(K\)).

In this section, we introduce and prove properties on the fixed point \((\nu_{1},\nu_{2})\in\mathbb{R}^{2}_{>0}\) and the weight feature matrix \(\bm{F}\) which we will use to prove deterministic equivalents.

Feature covariance matrix.The features \(\bm{z}_{i}\in\mathbb{R}^{p}\) conditional on \(\bm{F}\) are i.i.d. random vectors with covariance \(\widehat{\bm{\Sigma}}_{\bm{F}}=\bm{F}\bm{F}^{\mathsf{T}}/p\). We first show that with high probability over \(\bm{F}\), this feature covariance matrix has eigenvalues and intrinsic dimensions bounded by the ones of \(\bm{\Sigma}\).

Denote \((\hat{\xi}_{1}^{2},\hat{\xi}_{2}^{2},\ldots\hat{\xi}_{p}^{2})\) the \(p\) eigenvalues of \(\widehat{\bm{\Sigma}}_{\bm{F}}\) in nonincreasing order. Applying the definition of the intrinsic dimension at level \(k\) to \(\widehat{\bm{\Sigma}}_{\bm{F}}\), we have for any \(k=1,\ldots,p\),

\[r_{\widehat{\bm{\Sigma}}_{\bm{F}}}(k)=\frac{\sum_{j=k}^{p}\hat{\xi}_{j}^{2}}{ \hat{\xi}_{k}^{2}}.\] (71)

Applying Theorem A.2 directly to functionals of \(\bm{Z}\) conditional on \(\bm{F}\), the approximation guarantees depend on

\[\widehat{\rho}_{\lambda}(n)=1+\frac{n\cdot\hat{\xi}_{\lfloor\eta_{*}\cdot n \rfloor}^{2}}{\lambda}\left\{1+\frac{r_{\widehat{\bm{\Sigma}}_{\bm{F}}}( \lfloor\eta_{*}\cdot n\rfloor)\lor n}{n}\log\left(r_{\widehat{\bm{\Sigma}}_{\bm {F}}}(\lfloor\eta_{*}\cdot n\rfloor)\lor n\right)\right\},\] (72)which simply corresponds to \(\rho_{\lambda}\) defined in Eq. (52) applied to \(\widehat{\bm{\Sigma}}_{\bm{F}}\) where we recall that \(\hat{\xi}_{[\eta_{*}\cdot n]}^{2}=0\) if \(\lfloor\eta_{*}\cdot n\rfloor>p\). The next lemma shows that with high probability over \(\bm{F}\), we have \(\widehat{\rho}_{\lambda}(n)\lesssim\widehat{\rho}_{\lambda}(n,p)\) for all \(n\in\mathbb{N}\).

**Lemma B.2** (Feature covariance matrix).: _Assume the feature vectors \(\{\bm{f}_{j}\}_{j\in[p]}\) satisfy Assumption B.1. Then for any \(D,K>0\), there exist constants \(\eta_{*}\in(0,1/4)\) and \(C_{*,D,K}>0\) such that the following holds. For any \(p\geq C_{*,D,K}\), the event_

\[\widetilde{\mathcal{A}}_{\bm{F}}=\left\{\bm{F}\in\mathbb{R}^{p\times\infty}\ :\ \| \widehat{\bm{\Sigma}}_{\bm{F}}\|_{\mathrm{op}}\geq\frac{1}{2},\ \ \ \widehat{\rho}_{\lambda}(n)\leq C_{*,D,K}\cdot\widetilde{\rho}_{\lambda}(n,p), \ \ \forall n\in\mathbb{N},\lambda\in\mathbb{R}\right\}\] (73)

_holds with probability at least \(1-p^{-D}\)._

We defer the proof of this lemma to Section B.7.1.

High-degree part of the feature matrix \(\bm{F}\).Recall that \((\nu_{1},\nu_{2})\in\mathbb{R}_{>0}^{2}\) are the solutions to the fixed point equations stated in Definition 1. In order to get approximation guarantees when \(p\nu_{1}\to 0\) as \(n\to\infty\), we will analyze separately the top eigenspaces of \(\bm{F}\) from the rest. For an integer \(\mathsf{m}\in\mathbb{N}\), we split the feature vector \(\bm{f}_{j}=[\bm{f}_{0,j},\bm{f}_{+,j}]\) where \(\bm{f}_{0,j}\in\mathbb{R}^{\mathsf{m}}\) corresponds to the top \(\mathsf{m}\) coordinates with covariance

\[\bm{\Sigma}_{0}:=\mathbb{E}[\bm{f}_{0,j}\bm{f}_{0,j}^{\mathsf{T}}]=\mathrm{ diag}(\xi_{1}^{2},\xi_{2}^{2},\ldots,\xi_{\mathsf{m}}^{2}),\]

and \(\bm{f}_{+,j}\in\mathbb{R}^{\infty}\) corresponds to the high degree features orthogonal to the top \(\mathsf{m}\) eigenspaces. We denote their covariance

\[\bm{\Sigma}_{+}:=\mathbb{E}[\bm{f}_{+,j}\bm{f}_{+,j}^{\mathsf{T}}]=\mathrm{ diag}(\xi_{\mathsf{m}+1}^{2},\xi_{\mathsf{m}+2}^{2},\ldots).\]

We split the weight feature matrix into \(\bm{F}=[\bm{F}_{0},\bm{F}_{+}]\), where

\[\bm{F}_{0}=[\bm{f}_{0,1},\ldots,\bm{f}_{0,p}]^{\mathsf{T}}\in\mathbb{R}^{p \times m},\ \ \ \ \ \ \ \ \bm{F}_{+}=[\bm{f}_{+,1},\ldots,\bm{f}_{+,p}]^{\mathsf{T}}\in\mathbb{R}^{p \times\infty}.\]

We will use that for \(\mathsf{m}\) chosen such that \(p\cdot\xi_{\mathsf{m}+1}\ll\mathrm{Tr}(\bm{\Sigma}_{+})\), we have with high probability

\[\|\bm{F}_{+}\bm{F}_{+}^{\mathsf{T}}-\mathrm{Tr}(\bm{\Sigma}_{+})\cdot\mathrm{ I}_{p}\|_{\mathrm{op}}\lesssim\sqrt{p\cdot\xi_{\mathsf{m}+1}\mathrm{Tr}(\bm{ \Sigma}_{+})}\ll\mathrm{Tr}(\bm{\Sigma}_{+}),\]

and therefore

\[\bm{F}\bm{F}^{\mathsf{T}}+\kappa\approx\bm{F}_{0}\bm{F}_{0}^{\mathsf{T}}+ \gamma(\kappa),\]

where we defined the function

\[\gamma(\kappa):=\kappa+\mathrm{Tr}(\bm{\Sigma}_{+}).\]

To simplify the final statement of our results, we assume that we can choose \(\mathsf{m}\) such that \(p^{2}\xi_{\mathsf{m}+1}^{2}\leq\gamma(p\lambda/n)\). Note that \(\nu_{1}\geq\lambda/n\) from the fixed point equations and \(\gamma(p\lambda/n)\leq\gamma(p\nu_{1})\) (e.g., see Equation (76)). For convenience, we will further denote

\[\gamma_{+}:=\gamma(p\nu_{1}),\ \ \ \ \ \ \ \ \ \ \ \gamma_{\lambda}:=\gamma(p \lambda/n).\] (74)

**Lemma B.3** (Concentration of high-degree part of \(\bm{F}\)).: _Assume that \((\bm{f}_{+,j})_{j\in[p]}\) satisfy Assumption B.1 and \(\mathsf{m}\in\mathbb{N}\) is chosen such that \(p^{2}\xi_{\mathsf{m}+1}^{2}\leq\gamma(p\lambda/n)\). Then for any \(D>0\), there exists a constant \(C_{*,D}>0\) such that with probability at least \(1-p^{-D}\),_

\[\|\bm{F}_{+}\bm{F}_{+}^{\mathsf{T}}-\mathrm{Tr}(\bm{\Sigma}_{+})\cdot\mathrm{ I}_{p}\|_{\mathrm{op}}\leq C_{*,D}\frac{\log^{3}(p)}{\sqrt{p}}\gamma_{\lambda}\leq C _{*,D}\frac{\log^{3}(p)}{\sqrt{p}}\gamma_{+}.\]

This lemma follows directly from [14, Proposition 9].

Effective regularization and fixed points.Conditional on \(\bm{F}\), the bias and variance are functionals of the random matrix \(\bm{Z}\) which has \(n\) i.i.d. rows with regularization parameter \(\lambda\) and covariance \(\widehat{\bm{\Sigma}}_{\bm{F}}\). The deterministic approximations to these functionals depend on an "effective regularization" \(\tilde{\nu}_{1}\) associated to \((n,\widehat{\bm{\Sigma}}_{\bm{F}},\lambda)\) (see Definition 2 in Appendix A for background) which is given as the unique non-negative solution to the equation

\[n-\frac{\lambda}{\tilde{\nu}_{1}}=\mathrm{Tr}\big{(}\widehat{\bm{\Sigma}}_{\bm{F }}\big{(}\widehat{\bm{\Sigma}}_{\bm{F}}+\tilde{\nu}_{1}\big{)}^{-1}\big{)}.\]Note that the right-hand side can be rewritten as

\[\mathrm{Tr}\big{(}\widehat{\bm{\Sigma}}_{\bm{F}}\big{(}\widehat{\bm{ \Sigma}}_{\bm{F}}+\tilde{\nu}_{1}\big{)}^{-1}\big{)}=\mathrm{Tr}\big{(}\bm{F}\bm {F}^{\mathsf{T}}(\bm{F}\bm{F}^{\mathsf{T}}+p\tilde{\nu}_{1})^{-1}\big{)},\] (75)

which is itself of functional of the random matrix \(\bm{F}\) which has \(p\) i.i.d. rows with regularization parameter \(p\tilde{\nu}_{1}\) and covariance \(\bm{\Sigma}\). Note that \(\tilde{\nu}_{1}\) is a random variable depending itself on \(\bm{F}\). However we will show that it concentrates on a deterministic value \(\nu_{1}\). We therefore introduce a second effective regularization \(\nu_{2}\) associated to \((p,\bm{\Sigma},p\nu_{1})\) given as the unique non-negative solution of the equation

\[p-\frac{p\nu_{1}}{\nu_{2}}=\mathrm{Tr}\left(\bm{\Sigma}(\bm{ \Sigma}+\nu_{2})^{-1}\right).\]

The functional (75) is then well approximated by

\[\mathrm{Tr}\big{(}\bm{F}\bm{F}^{\mathsf{T}}(\bm{F}\bm{F}^{ \mathsf{T}}+p\tilde{\nu}_{1})^{-1}\big{)}=(1+o_{p,\widetilde{p}}(1))\cdot \mathrm{Tr}\big{(}\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1}\big{)}.\]

This motivates to define \((\nu_{1},\nu_{2})\in\mathbb{R}_{>0}^{2}\) as the unique non-negative solutions to the coupled fixed point equations

\[\begin{split} n-\frac{\lambda}{\nu_{1}}& =\mathrm{Tr}\left(\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1}\right), \\ p-\frac{p\nu_{1}}{\nu_{2}}&=\mathrm{Tr}\left(\bm{ \Sigma}(\bm{\Sigma}+\nu_{2})^{-1}\right).\end{split}\] (76)

Writing \(\nu_{1}\) as a function of \(\nu_{2}\) indeed produces the equations stated in Definition 1.

To show that \(\tilde{\nu}_{1}\) concentrates on \(\nu_{1}\), we define the following fixed points \((\tilde{\nu}_{1},\tilde{\nu}_{2})\in\mathbb{R}_{>0}^{2}\) to be the unique positive solutions to the random equations

\[\begin{split} n-\frac{\lambda}{\tilde{\nu}_{1}}& =\mathrm{Tr}\big{(}\widehat{\bm{\Sigma}}_{\bm{F}}\big{(}\widehat {\bm{\Sigma}}_{\bm{F}}+\tilde{\nu}_{1}\big{)}^{-1}\big{)},\\ p-\frac{p\tilde{\nu}_{1}}{\tilde{\nu}_{2}}&= \mathrm{Tr}\left(\bm{\Sigma}(\bm{\Sigma}+\tilde{\nu}_{2})^{-1}\right).\end{split}\] (77)

The following proposition shows that \((\tilde{\nu}_{1},\tilde{\nu}_{2})\) is well approximated by \((\nu_{1},\nu_{2})\) with high probability.

**Proposition B.4** (Concentration of the fixed points).: _Assume that \((\bm{f}_{j})_{j\in[p]}\) satisfy Assumption B.1. Then for any \(D,K>0\), there exist constants \(\eta_{*}\in(0,1/4)\) and \(C_{*,D,K}>0\) such that the following holds. Let \(\rho_{\kappa}(p)\) and \(\widetilde{\rho}_{\kappa}(n,p)\) be defined as per Eqs. (26) and (25), and \(\gamma_{\lambda}\) and \(\gamma_{+}\) as per Eq. (74). For any \(p\geq C_{*,D,K}\) and \(\lambda>0\), if it holds that_

\[\gamma_{\lambda}\geq p^{-K},\qquad\quad\widetilde{\rho}_{\lambda}(n,p)\cdot \rho_{\gamma_{\lambda}}(p)^{5/2}\log^{4}(p)\leq K\sqrt{p},\] (78)

_then with probability at least \(1-p^{-D}\), we have_

\[\max\left\{\frac{|\tilde{\nu}_{2}-\nu_{2}|}{\nu_{2}},\frac{| \tilde{\nu}_{1}-\nu_{1}|}{\nu_{1}}\right\}\leq C_{*,D,K}\cdot\mathcal{E}_{\nu }(p),\]

_where we defined_

\[\mathcal{E}_{\nu}(p):=\frac{\widetilde{\rho}_{\lambda}(n,p)\cdot \rho_{\gamma_{+}}(p)^{5/2}\log^{3}(p)}{\sqrt{p}}.\]

The proof of this proposition can be found in Section B.7.2.

To study functionals of \(\bm{Z}\) conditional on \(\bm{F}\), we will assume that \(\bm{F}\) belongs to the good event

\[\mathcal{A}_{\mathcal{F}}:=\widetilde{\mathcal{A}}_{\mathcal{F}} \cap\left\{\bm{F}\in\mathbb{R}^{p\times\infty}\ \ :\ \ |\tilde{\nu}_{1}-\nu_{1}|\leq C_{*,D,K}\cdot\mathcal{E}_{\nu}(p)\cdot\nu_{1} \right\},\] (79)

where \(\widetilde{\mathcal{A}}_{\mathcal{F}}\) is defined in Lemma B.2. In particular, as long as \(p\geq C_{*,D,K}\) and condition (78) hold, then \(\mathbb{P}(\mathcal{A}_{\mathcal{F}})\geq 1-2p^{-D}\) by Lemma B.2 and Proposition B.4.

Truncated fixed point.As mentioned above, we will separate the analysis of the low-degree part of the feature matrix \(\bm{F}_{0}\) and the high-degree part \(\bm{F}_{+}\). For the high-degree part, we will simply use the concentration stated in Lemma B.3. For the low degree part, we will study functional of \(\bm{F}_{0}\) with regularization \(\gamma_{+}=p\nu_{1}+\operatorname{Tr}(\bm{\Sigma}_{+})\). We therefore introduce an effective regularization \(\nu_{2,0}\) associated to the model \((p,\bm{\Sigma}_{0},\gamma_{+})\), i.e., the unique positive solution to the equation

\[p-\frac{\gamma_{+}}{\nu_{2,0}}=\operatorname{Tr}\bigl{(}\bm{\Sigma}_{0}(\bm{ \Sigma}_{0}+\nu_{2,0})^{-1}\bigr{)}.\] (80)

Intuitively, \(\nu_{2,0}\) will be closed to \(\nu_{2}\) as soon as \(\lambda_{\max}(\bm{\Sigma}_{+})\ll\nu_{2}\) as

\[n-\frac{p\nu_{1}}{\nu_{2}}\approx\operatorname{Tr}\left(\bm{\Sigma}_{0}(\bm{ \Sigma}_{0}+\nu_{2})^{-1}\right)+\frac{\operatorname{Tr}(\bm{\Sigma}_{+})}{ \nu_{2}},\]

and uniqueness of the positive solution \(\nu_{2,0}\). This is formalized in the following lemma.

**Lemma B.5** (Truncated fixed point).: _Let \(\mathfrak{m}\) be chosen such that \(p^{2}\xi_{\mathfrak{m}+1}^{2}\leq\gamma_{\lambda}\), then_

\[\frac{|\nu_{2,0}-\nu_{2}|}{\nu_{2}}\leq\frac{1}{p}.\] (81)

_Furthermore, there exists an absolute constant \(C>0\) such that_

\[\left|\operatorname{Tr}\bigl{(}\bm{\Sigma}_{0}(\bm{\Sigma}_{0}+\nu_{2,0})^{-1 }\bigr{)}+\frac{\operatorname{Tr}(\bm{\Sigma}_{+})}{\nu_{2,0}}-\operatorname {Tr}\bigl{(}\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1}\bigr{)}\right|\leq\frac{C}{ p}\mathrm{Tr}\bigl{(}\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1}\bigr{)}.\] (82)

Proof of Lemma b.5.: The first bound (81) follows directly from [14, Lemma 6]. For the second inequality, we decompose this difference into

\[\left|\operatorname{Tr}\bigl{(}\bm{\Sigma}_{0}(\bm{\Sigma}_{0}+ \nu_{2,0})^{-1}\bigr{)}+\frac{\operatorname{Tr}(\bm{\Sigma}_{+})}{\nu_{2,0}}- \operatorname{Tr}\bigl{(}\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1}\bigr{)}\right|\] \[\leq \ |\nu_{2,0}-\nu_{2}|\operatorname{Tr}\bigl{(}\bm{\Sigma}_{0}(\bm {\Sigma}_{0}+\nu_{2,0})^{-1}(\bm{\Sigma}_{0}+\nu_{2})^{-1}\bigr{)}+\frac{\xi_{ \mathfrak{m}+1}^{2}+|\nu_{2,0}-\nu_{2}|}{\nu_{2,0}}\mathrm{Tr}\bigl{(}\bm{ \Sigma}_{+}(\bm{\Sigma}_{+}+\nu_{2})^{-1}\bigr{)}\] \[\leq \ \left\{\frac{\xi_{\mathfrak{m}+1}^{2}}{\nu_{2,0}}+\frac{|\nu_{2,0 }-\nu_{2}|}{\nu_{2,0}}\right\}\mathrm{Tr}\bigl{(}\bm{\Sigma}(\bm{\Sigma}+\nu_ {2})^{-1}\bigr{)}\] \[\leq \ \frac{3}{p}\mathrm{Tr}\bigl{(}\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{ -1}\bigr{)},\]

where we used that \(\xi_{\mathfrak{m}+1}^{2}/\nu_{2,0}\leq\gamma_{+}/(p^{2}\nu_{2,0})\leq 1/p\) by the assumption on \(\mathfrak{m}\) and identity (80). 

### Deterministic equivalents for functionals of \(\mathbf{Z}\) conditional on \(\mathbf{F}\)

As mentioned in Section B.1, we will first show that the test error concentrates over \(\bm{Z}\) conditional on \(\bm{F}\) on some quantity that only depends on \(\widehat{\bm{\Sigma}}_{\bm{F}}\). The bias and variance terms can be written in terms of the following three functionals of the feature matrix \(\bm{Z}\): for a general p.s.d. matrix \(\bm{A}\in\mathbb{R}^{p\times p}\) and positive scalar \(\kappa>0\), define

\[\Phi_{3}(\bm{Z};\kappa) :=\operatorname{Tr}\left(\frac{\bm{Z}^{\mathsf{T}}\bm{Z}}{n}(\bm{Z} ^{\mathsf{T}}\bm{Z}+\kappa)^{-1}\right),\] \[\Phi_{4}(\bm{Z};\bm{A},\kappa) :=\,\left(\bm{A}\widehat{\bm{\Sigma}}_{\bm{F}}^{1/2}(\bm{Z}^{ \mathsf{T}}\bm{Z}+\kappa)^{-1}\frac{\bm{Z}^{\mathsf{T}}\bm{Z}}{n}(\bm{Z}^{ \mathsf{T}}\bm{Z}+\kappa)^{-1}\widehat{\bm{\Sigma}}_{\bm{F}}^{1/2}\right),\] (83)

where we recall that \(\bm{Z}\) has i.i.d. rows with covariance \(\widehat{\bm{\Sigma}}_{\bm{F}}=\bm{F}\bm{F}^{\mathsf{T}}/p\). We show that these functional are well approximated by functionals of \(\bm{F}\) that can be written in terms of the following functionals:

\[\widetilde{\Phi}_{2}(\bm{F};\kappa) :=\operatorname{Tr}\left(\frac{\bm{F}^{\mathsf{T}}\bm{F}}{p}(\bm{F} ^{\mathsf{T}}\bm{F}+\kappa)^{-1}\right),\] (84) \[\widetilde{\Phi}_{5}(\bm{F};\bm{A},\kappa) :=\frac{1}{n}\cdot\frac{\widetilde{\Phi}_{6}(\bm{F};\bm{A},\kappa) }{n-\widetilde{\Phi}_{6}(\bm{F};\bm{1},\kappa)},\] \[\widetilde{\Phi}_{6}(\bm{F};\bm{A},\kappa) :=\operatorname{Tr}\left(\bm{A}(\bm{F}\bm{F}^{\mathsf{T}})^{2}( \bm{F}\bm{F}^{\mathsf{T}}+\kappa)^{-2}\right).\]The following proposition gather the approximation guarantees for \(\Phi_{2},\Phi_{3},\Phi_{4}\) listed in Eq. (83).

**Proposition B.6** (Deterministic equivalents for \(\Phi(\bm{Z})\) conditional on \(\bm{F}\)).: _Under Assumption B.1 and assuming that \(\bm{F}\in\mathcal{A}_{\mathcal{F}}\) defined in Eq. (79), for any \(D,K>0\), there exist constants \(\eta_{*}\in(0,1/4)\), \(C_{D,K}>0\), and \(C_{*,D,K}>0\) such that the followings holds. Let \(\rho_{\kappa}(p)\) and \(\widetilde{\rho}_{\kappa}(n,p)\) be defined as per Eqs. (26) and (25). For any \(n\geq C_{D,K}\) and regularization parameter \(\lambda>0\), if it holds that_

\[\lambda\geq n^{-K}, \widetilde{\rho}_{\lambda}(n,p)^{5/2}\log^{3/2}(n)\leq K\sqrt{n},\] (85) \[\widetilde{\rho}_{\lambda}(n,p)^{2}\cdot\rho_{\gamma_{+}}(p)^{5/ 2}\log^{3}(p)\leq K\sqrt{p},\]

_then for any p.s.d. matrix \(\bm{A}\in\mathbb{R}^{p\times p}\) (independent of \(\bm{Z}|\bm{F}\)), with probability at least \(1-n^{-D}\) on \(\bm{Z}\) conditional on \(\bm{F}\), we have_

\[\left|\Phi_{2}(\bm{Z};\lambda)-\frac{p}{n}\widetilde{\Phi}_{2}( \bm{F};p\nu_{1})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{1}(n,p)\cdot\frac{p}{n}\widetilde{ \Phi}_{2}(\bm{F};p\nu_{1}),\] (86) \[\left|\Phi_{3}(\bm{Z};\bm{A},\lambda)-\left(\frac{n\nu_{1}}{ \lambda}\right)^{2}\widetilde{\Phi}_{5}(\bm{F};\bm{A},p\nu_{1})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{1}(n,p)\cdot\left(\frac{n\nu_{1} }{\lambda}\right)^{2}\widetilde{\Phi}_{5}(\bm{F};\bm{A},p\nu_{1}),\] (87) \[\left|\Phi_{4}(\bm{Z};\bm{A},\lambda)-\widetilde{\Phi}_{5}(\bm{F}; \bm{A},p\nu_{1})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{1}(n,p)\cdot\widetilde{\Phi}_{5}( \bm{F};\bm{A},p\nu_{1}),\] (88)

_where the approximation rate is given by_

\[\mathcal{E}_{1}(n,p):=\frac{\widetilde{\rho}_{\lambda}(n,p)^{6}\log^{5/2}(n)} {\sqrt{n}}+\frac{\widetilde{\rho}_{\lambda}(n,p)^{2}\cdot\rho_{\gamma_{+}}(p) ^{5/2}\log^{3}(p)}{\sqrt{p}}.\] (89)

Proposition B.6 is a consequence of [10, Theorem 4] (see Appendix A and Theorem A.2 for background) and Proposition B.4. We defer its proof to Section B.7.3. Note that the term \(\widetilde{O}(p^{-1/2})\) in the approximation rate \(\mathcal{E}_{1}(n,p)\) defined in Eq. (89) comes from comparing \(p\nu_{1}\) with \(p\widetilde{\nu}_{1}\) and is equal to \(\widetilde{\rho}_{\lambda}(n,p)\cdot\mathcal{E}_{\nu}(n,p)\) where \(\mathcal{E}_{\nu}(n,p)\) is defined in Proposition B.4. If instead, we compared to functionals with regularization \(p\widetilde{\nu}_{1}\), then the approximation rate in Proposition B.6 would scale \(\widetilde{O}(n^{-1/2})\) as expected.

In the analysis of the bias term, we will further need to show deterministic equivalents in the case where \(\bm{A}\) is itself a random matrix uncorrelated (but not independent) to \(\bm{Z}|\bm{F}\). The following proposition gather these approximation guarantees and is a consequence of [10, Lemma 10] and Proposition B.4.

**Proposition B.7** (Deterministic equivalents for \(\Phi(\bm{Z})\), uncorrelated numerator).: _Assume the same setting as Proposition B.6 and the same conditions (85). Consider a deterministic vector \(\bm{v}\in\mathbb{R}^{p}\) and a random vector \(\bm{u}=(u_{i})_{i\in[n]}\) with i.i.d. entries and \(\mathbb{E}[u_{i}]=0\), \(\mathbb{E}[u_{i}^{2}]=1\), and \(\mathbb{E}[\bm{z}_{i}u_{i}|\bm{F}]=\bm{0}\). Then with probability at least \(1-n^{-D}\) on \(\bm{Z}\) conditional on \(\bm{F}\), we have_

\[\left|\langle\bm{u},\bm{Z}(\bm{Z}^{\mathsf{T}}\bm{Z}+\lambda)^{-1 }\widehat{\bm{\Sigma}}_{\bm{F}}(\bm{Z}^{\mathsf{T}}\bm{Z}+\lambda)^{-1}\bm{Z}^ {\mathsf{T}}\bm{u}\rangle-n\widetilde{\Phi}_{5}(\bm{F};\bm{1},p\nu_{1})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{2}(n,p),\] (90) \[\left|\langle\bm{u},\bm{Z}(\bm{Z}^{\mathsf{T}}\bm{Z}+\lambda)^{-1 }\widehat{\bm{\Sigma}}_{\bm{F}}(\bm{Z}^{\mathsf{T}}\bm{Z}+\lambda)^{-1}\bm{v} \rangle\right|\leq C_{*,D,K}\cdot\mathcal{E}_{2}(n,p)\cdot\frac{n\nu_{1}}{ \lambda}\sqrt{\widetilde{\Phi}_{5}(\bm{F};\overline{\bm{v}}\bm{v}^{\mathsf{T} },p\nu_{1})},\] (91)

_where we denoted \(\overline{\bm{v}}:=\widehat{\bm{\Sigma}}_{\bm{F}}^{-1/2}\bm{v}\) and the approximation rate is given by_

\[\mathcal{E}_{2}(n,p):=\frac{\widetilde{\rho}_{\lambda}(n,p)^{6}\log^{7/2}(n)}{ \sqrt{n}}+\frac{\widetilde{\rho}_{\lambda}(n,p)^{2}\cdot\rho_{\gamma_{+}}(p)^{ 5/2}\log^{3}(p)}{\sqrt{p}}.\] (92)

We defer the proof of Proposition B.7 to Section B.7.3.

### Deterministic equivalents for functionals of \(\mathbf{F}\)

After replacing the bias and variance terms by their deterministic equivalents over the randomness in \(\bm{Z}|\bm{F}\), we obtain functionals in terms of \(\widetilde{\Phi}_{2}(\bm{F};p\nu_{1})\) and \(\widetilde{\Phi}_{5}(\bm{F};\bm{A},p\nu_{1})\) listed in Eq. (84). As mentioned in Section B.2, we will analyze the low-degree and high degree part of the feature matrix separately. Using Lemma B.3, we can replace the high-degree part \(\bm{F}_{+}\bm{F}_{+}^{\mathsf{T}}\) by a deterministic matrix, which results in a regularization parameter \(\gamma(p\nu_{1})=p\nu_{1}+\mathrm{Tr}(\bm{\Sigma}_{+})\).

The functionals of \(\bm{F}_{0}\) can be written in terms of the following quantities: for any deterministic matrix \(\bm{B}\in\mathbb{R}^{m\times m}\), define

\[\widetilde{\Phi}_{1}(\bm{F}_{0};\bm{B},\kappa) =\operatorname{Tr}\left(\bm{B}\bm{\Sigma}_{0}^{1/2}(\bm{F}_{0}^{ \mathsf{T}}\bm{F}_{0}+\gamma(\kappa))^{-1}\bm{\Sigma}_{0}^{1/2}\right),\] (93) \[\widetilde{\Phi}_{2}(\bm{F}_{0};\kappa) =\operatorname{Tr}\left(\frac{\bm{F}_{0}^{\mathsf{T}}\bm{F}_{0}}{ p}(\bm{F}_{0}^{\mathsf{T}}\bm{F}_{0}+\gamma(\kappa))^{-1}\right),\] \[\widetilde{\Phi}_{3}(\bm{F}_{0};\bm{B},\kappa) =\operatorname{Tr}\left(\bm{B}_{0}\bm{\Sigma}_{0}^{1/2}(\bm{F}_{0 }^{\mathsf{T}}\bm{F}_{0}+\gamma(\kappa))^{-1}\bm{\Sigma}_{0}(\bm{F}_{0}^{ \mathsf{T}}\bm{F}_{0}+\gamma(\kappa))^{-1}\bm{\Sigma}_{0}^{1/2}\right),\] \[\widetilde{\Phi}_{4}(\bm{F}_{0};\bm{B},\kappa) =\left(B\bm{\Sigma}_{0}^{1/2}(\bm{F}_{0}^{\mathsf{T}}\bm{F}_{0}+ \gamma(\kappa))^{-1}\frac{\bm{F}_{0}^{\mathsf{T}}\bm{F}_{0}}{p}(\bm{F}_{0}^{ \mathsf{T}}\bm{F}_{0}+\gamma(\kappa))^{-1}\bm{\Sigma}^{1/2}\right).\]

We show that these functionals can be well approximated by the deterministic functions that can be written in terms of

\[\Psi_{1}(\nu;\bm{B}) =\operatorname{Tr}\left(\bm{B}\bm{\Sigma}_{0}(\bm{\Sigma}_{0}+\nu )^{-1}\right),\] (94) \[\Psi_{2}(\nu) =\frac{1}{p}\text{Tr}\left(\bm{\Sigma}_{0}(\bm{\Sigma}_{0}+\nu)^ {-1}\right),\] \[\Psi_{3}(\nu;\bm{B}) =\frac{1}{p}\cdot\frac{\operatorname{Tr}(\bm{B}\bm{\Sigma}_{0}^{ 2}(\bm{\Sigma}_{0}+\nu)^{-2})}{p-\operatorname{Tr}(\bm{\Sigma}_{0}^{2}(\bm{ \Sigma}_{0}+\nu)^{-2})}.\]

Recall that for \(\kappa=p\nu_{1}\), we denote \(\gamma_{+}:=\gamma(p\nu_{1})\) and \(\nu_{2,0}\) the effective regularization associated to model \((p,\bm{\Sigma}_{0},\gamma_{+})\). The following proposition gather the approximation guarantees for \(\widetilde{\Phi}_{1},\widetilde{\Phi}_{2},\widetilde{\Phi}_{3},\widetilde{\Phi }_{4}\) listed in Eq. (93).

**Proposition B.8** (Deterministic equivalents for \(\bm{F}_{0}\)).: _Under Assumption B.1, for any \(D,K>0\), there exist constants \(\eta_{*}\in(0,1/4)\), \(C_{D,K}>0\), and \(C_{*,D,K}>0\) such that the followings holds. Let \(\rho_{\kappa}(p)\) be defined as per Eq. (26). For any \(p\geq C_{D,K}\) and \(\lambda>0\), if it holds that_

\[\gamma_{+}\geq p^{-K},\qquad\qquad\rho_{\gamma_{+}}(p)^{5/2}\log^{3/2}(p)\leq K \sqrt{p},\] (95)

_then for any deterministic p.s.d. matrix \(\bm{B}\in\mathbb{R}^{m\times m}\), with probability at least \(1-p^{-D}\), we have_

\[\left|\widetilde{\Phi}_{1}(\bm{F}_{0};\bm{B},p\nu_{1})-\frac{\nu_ {2,0}}{\gamma_{+}}\Psi_{1}(\nu_{2,0};\bm{B})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{3}(p)\cdot\frac{\nu_{2,0}}{ \gamma_{+}}\Psi_{1}(\nu_{2,0};\bm{B}),\] (96) \[\left|\widetilde{\Phi}_{2}(\bm{F}_{0};p\nu_{1})-\Psi_{2}(\nu_{2, 0})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{3}(p)\cdot\Psi_{2}(\nu_{2,0}),\] (97) \[\left|\widetilde{\Phi}_{3}(\bm{F}_{0};\bm{B},p\nu_{1})-\left( \frac{p\nu_{2,0}}{\gamma_{+}}\right)^{2}\Psi_{3}(\nu_{2,0};\bm{B})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{3}(p)\cdot\left(\frac{p\nu_{2,0} }{\gamma_{+}}\right)^{2}\Psi_{3}(\nu_{2,0};\bm{B}),\] (98) \[\left|\widetilde{\Phi}_{4}(\bm{F}_{0};\bm{B},p\nu_{1})-\Psi_{3}( \nu_{2,0};\bm{B})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{3}(p)\cdot\Psi_{3}(\nu_{2,0};\bm{B }),\] (99)

_where the approximation rate is given by_

\[\mathcal{E}_{3}(p):=\frac{\rho_{\gamma_{+}}(p)^{6}\log^{3}(p)}{\sqrt{p}}.\] (100)

This proposition is obtained by directly applying Theorem A.2 with no modifications.

Again, in the analysis of the bias term, because we separated the analysis of the low-degree and high-degree parts \(\bm{F}_{0}\) and \(\bm{F}_{+}\), we will further need deterministic equivalents when \(\bm{B}\) is itself a random matrix uncorrelated but not independent to \(\bm{F}\). We gather the associated deterministic equivalents in the following proposition.

**Proposition B.9** (Deterministic equivalents for \(\bm{F}_{0}\), uncorrelated numerator).: _Assume the same setting as Proposition B.8 and the same conditions (95). Consider a deterministic vector \(\bm{v}\in\mathbb{R}^{m}\) and a random vector \(\bm{u}=(u_{j})_{j\in[p]}\) with i.i.d. entries and \(\mathbb{E}[u_{j}]=0\), \(\mathbb{E}[u_{j}^{2}]=1\), and \(\mathbb{E}[\bm{f}_{0,j}u_{j}]=\bm{0}\).__Then with probability at least \(1-p^{-D}\), we have_

\[\left|\frac{1}{p}\langle\bm{u},(\bm{F}_{0}\bm{F}_{0}^{\mathsf{T}}+ \gamma_{+})^{-2}\bm{u}\rangle-\frac{1}{(p\nu_{2,0})^{2}}\frac{1}{1-\frac{1}{p} \mathrm{Tr}(\bm{\Sigma}_{0}^{2}(\bm{\Sigma}_{0}+\nu_{2,0})^{-2})}\right|\leq C_{ \ast,D,K}\cdot\mathcal{E}_{4}(p)\cdot\frac{1}{\gamma_{+}^{2}},\] (101) \[\left|\frac{1}{p}\langle\bm{u},(\bm{F}_{0}\bm{F}_{0}^{\mathsf{T}} +\gamma_{+})^{-2}\bm{F}_{0}\bm{v}\rangle\right|\leq C_{\ast,D,K}\cdot\mathcal{E }_{4}(p)\frac{p\nu_{2,0}}{\gamma_{+}^{2}}\sqrt{\Psi_{3}(\nu_{2,0};\overline{\bm {v}\bm{v}^{\mathsf{T}}})},\] (102)

_where we denoted \(\overline{\bm{v}}:=\bm{\Sigma}_{0}^{-1/2}\bm{v}\) and the approximation rate is given by_

\[\mathcal{E}_{4}(p):=\frac{\rho_{\gamma_{+}}(p)^{6}\log^{7/2}(p)}{\sqrt{p}}.\] (103)

This proposition is a direct consequence of (Misiakiewicz and Saeed, 2024, Lemma 10).

Throughout the proofs, with a slight abuse of notations, we will denote functionals \(\widetilde{\Phi}_{i}(\bm{F}_{0};\bm{B},\kappa)\), \(i\in[4]\), the functionals listed in Eq. (93) applied to the truncated feature matrix \(\bm{F}_{0}\), with covariance \(\bm{\Sigma}_{0}\), regularization \(\gamma(\kappa)\), and deterministic matrix \(\bm{B}\in\mathbb{R}^{m\times m}\), and \(\widetilde{\Phi}_{i}(\bm{F};\bm{B},\kappa)\), \(i\in[4]\), the functionals applied to the full feature matrix \(\bm{F}\in\mathbb{R}^{p\times\infty}\), where \(\bm{\Sigma}_{0}\) is replaced by \(\bm{\Sigma}\), the regularization parameter is \(\kappa\), and the deterministic matrix \(\bm{B}\in\mathbb{R}^{\infty\times\infty}\). Similarly, we will us the notation \(\Psi_{i}(\nu_{2,0};\bm{B}),i\in[3]\) for the truncated deterministic functionals (94), and the notation \(\Psi_{i}(\nu_{2};\bm{B}),i\in[3]\) to denote the full functionals with \(\bm{\Sigma}_{0}\) replaced by \(\bm{\Sigma}\) and \(\bm{B}\in\mathbb{R}^{\infty\times\infty}\).

### Approximation guarantee for the variance term

Recall the expressions for the variance term

\[\mathcal{V}(\bm{G},\bm{F},\lambda)=\sigma_{\varepsilon}^{2}\cdot\mathrm{Tr} \big{(}\widehat{\bm{\Sigma}}_{\bm{F}}\bm{Z}^{\mathsf{T}}\bm{Z}(\bm{Z}^{\mathsf{ T}}\bm{Z}+\lambda)^{-2}\big{)},\]

and its associated deterministic equivalent

\[\mathsf{V}_{n,p}(\lambda) =\sigma_{\varepsilon}^{2}\frac{\Upsilon(\nu_{1},\nu_{2})}{1- \Upsilon(\nu_{1},\nu_{2})},\] (104) \[\Upsilon(\nu_{1},\nu_{2}) =\frac{p}{n}\left[\left(1-\frac{\nu_{1}}{\nu_{2}}\right)^{2}+ \left(\frac{\nu_{1}}{\nu_{2}}\right)^{2}\frac{\mathrm{Tr}(\bm{\Sigma}^{2}(\bm {\Sigma}+\nu_{2})^{-2})}{p-\mathrm{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{- 2})}\right].\] (105)

We prove in this section an approximation guarantee between \(\mathcal{V}(\bm{G},\bm{F},\lambda)\) and \(\mathsf{V}_{n,p}(\lambda)\). For convenience, we state a separate theorem for this term.

**Theorem B.10** (Deterministic equivalent for the variance term).: _Assume the features \((\bm{z}_{i})_{i\in[n]}\) and \((\bm{f}_{j})_{j\in[p]}\) satisfy Assumption B.1, and the covariance \(\bm{\Sigma}\) and target coefficients \(\bm{\beta}_{\ast}\) satisfy Assumption 3.2. Then, for any \(D,K>0\), there exist constants \(\eta_{\ast}\in(0,1/2)\) and \(C_{\ast},D,K>0\) such that the following holds. Let \(\rho_{\kappa}\) and \(\widetilde{\rho}_{\kappa}\) be defined as per Eqs. (26) and (25). For any \(n,p\geq C_{\ast,D,K}\) and \(\lambda>0\), if it holds that_

\[\lambda\geq n^{-K},\qquad\gamma_{\lambda}\geq p^{-K},\qquad\qquad \widetilde{\rho}_{\lambda}(n,p)^{5/2}\cdot\log^{3/2}(n) \leq K\sqrt{n},\] (106) \[\widetilde{\rho}_{\lambda}(n,p)^{2}\cdot\rho_{\gamma_{+}}(p)^{7} \cdot\log^{4}(p) \leq K\sqrt{p},\]

_then with probability at least \(1-n^{-D}-p^{-D}\), we have_

\[|\mathcal{V}(\bm{G},\bm{F},\lambda)-\mathsf{V}_{n,p}(\lambda)|\leq C_{\ast,D,K} \cdot\mathcal{E}_{V}(n,p)\cdot\mathsf{V}_{n,p}(\lambda),\]

_where the approximation rate is given by_

\[\mathcal{E}_{V}(n,p):=\frac{\widetilde{\rho}_{\lambda}(n,p)^{6}\log^{5/2}(n)}{ \sqrt{n}}+\frac{\widetilde{\rho}_{\lambda}(n,p)^{2}\cdot\rho_{\gamma_{+}}(p) ^{7}\log^{3}(p)}{\sqrt{p}}.\] (107)

Proof of Theorem b.10.: First, note that \(\mathcal{V}(\bm{G},\bm{F},\lambda)\) can be written in terms of the functional \(\Phi_{4}\) defined in Eq. (83):

\[\mathcal{V}(\bm{G},\bm{F},\lambda)=\sigma_{\varepsilon}^{2}\cdot n\Phi_{4}(\bm{Z };\mathbf{I},\lambda).\]Recall that \(\mathcal{A}_{\mathcal{F}}\) is the event defined in Eq. (79). Under the assumptions of Theorem B.10, we can apply Lemma B.2 and Proposition B.4 to obtain

\[\mathbb{P}(\mathcal{A}_{\mathcal{F}})\geq 1-p^{-D}.\]

Hence, applying Proposition B.6 for \(\bm{F}\in\mathcal{A}_{\mathcal{F}}\) and via union bound, we obtain that with probability at least \(1-p^{-D}-n^{-D}\),

\[\Big{|}n\Phi_{4}(\bm{Z};\mathbf{I},\lambda)-n\widetilde{\Phi}_{5}(\bm{F}; \mathbf{I},p\nu_{1})\Big{|}\leq C_{*,D,K}\cdot\mathcal{E}_{1}(p,n)\cdot n \widetilde{\Phi}_{5}(\bm{F};\mathbf{I},p\nu_{1}),\] (108)

where \(\mathcal{E}_{1}(n,p)\) is defined in Eq. (89) and we recall the expressions

\[n\widetilde{\Phi}_{5}(\bm{F};\mathbf{I},p\nu_{1})=\frac{\widetilde{\Phi}_{6}( \bm{F};\mathbf{I},p\nu_{1})}{n-\widetilde{\Phi}_{6}(\bm{F};\mathbf{I},p\nu_{1} )},\qquad\widetilde{\Phi}_{6}(\bm{F};\mathbf{I},p\nu_{1})=\mathrm{Tr}\big{(}( \bm{F}\bm{F}^{\mathsf{T}})^{2}(\bm{F}\bm{F}^{\mathsf{T}}+p\nu_{1})^{-2}\big{)}.\]

Let us decompose \(\widetilde{\Phi}_{6}(\bm{F};\mathbf{I},p\nu_{1})\) into

\[\widetilde{\Phi}_{6}(\bm{F};\mathbf{I},p\nu_{1})=\mathrm{Tr}(\bm{F}\bm{F}^{ \mathsf{T}}(\bm{F}\bm{F}^{\mathsf{T}}+p\nu_{1})^{-1})-p\nu_{1}\mathrm{Tr}(\bm{ F}\bm{F}^{\mathsf{T}}(\bm{F}\bm{F}^{\mathsf{T}}+p\nu_{1})^{-2}).\]

From Lemma B.11 stated below, we have with probability at least \(1-p^{-D}\)

\[\left|\frac{1}{p}\mathrm{Tr}(\bm{F}\bm{F}^{\mathsf{T}}(\bm{F}\bm {F}^{\mathsf{T}}+p\nu_{1})^{-1})-\Psi_{2}(\nu_{2})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{3}(p)\cdot\Psi_{2}(\nu_{2}),\] \[\left|\frac{1}{p}\mathrm{Tr}(\bm{F}\bm{F}^{\mathsf{T}}(\bm{F}\bm {F}^{\mathsf{T}}+p\nu_{1})^{-2})-\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1})\right| \leq C_{*,D,K}\cdot\rho_{\gamma_{+}}(p)\mathcal{E}_{3}(p)\cdot\Psi_ {3}(\nu_{2};\bm{\Sigma}^{-1}),\]

where \(\mathcal{E}_{3}(p)\) is the approximation rate defined in Eq. (100). Note that

\[p\Psi_{2}(\nu_{2})-p^{2}\nu_{1}\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1}) =\mathrm{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1})-\frac{\nu_{1} \mathrm{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-2})}{1-\frac{1}{p}\mathrm{Tr}( \bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{-2})}\] \[=p\left\{1-\frac{\nu_{1}}{\nu_{2}}-\frac{\nu_{1}}{\nu_{2}}\frac{ 1-\frac{\nu_{1}}{\nu_{2}}-\frac{1}{p}\mathrm{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+ \nu_{2})^{-2})}{1-\frac{1}{p}\mathrm{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2}) ^{-2})}\right\}\] \[=p\left\{\left(1-\frac{\nu_{1}}{\nu_{2}}\right)^{2}+\left(\frac{ \nu_{1}}{\nu_{2}}\right)^{2}\frac{\mathrm{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu _{2})^{-2})}{p-\mathrm{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{-2})}\right\}\] \[=n\Upsilon(\nu_{1},\nu_{2}).\]

Combining the above displays, we deduce that

\[\left|\widetilde{\Phi}_{6}(\bm{F};\mathbf{I},p\nu_{1})-n\Upsilon( \nu_{1},\nu_{2})\right| \leq C_{*,D,K}\cdot\rho_{\gamma_{+}}(p)\mathcal{E}_{3}(p)\cdot \left[p\Psi_{2}(\nu_{2})+p^{2}\nu_{1}\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1})\right]\] \[\leq C_{*,D,K}\cdot\rho_{\gamma_{+}}(p)\mathcal{E}_{3}(p)\cdot \left[n\Upsilon(\nu_{1},\nu_{2})+2p^{2}\nu_{1}\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1 })\right]\]

Using Eq. (120) in Lemma B.14 stated in Section B.7, we conclude that with probability at least \(1-p^{-D}\),

\[\left|\widetilde{\Phi}_{6}(\bm{F};\mathbf{I},p\nu_{1})-n\Upsilon(\nu_{1},\nu_{ 2})\right|\leq C_{*,D,K}\cdot\rho_{\gamma_{+}}(p)\mathcal{E}_{3}(p)\cdot n \Upsilon(\nu_{1},\nu_{2}).\] (109)

Finally, by simple algebra, we have

\[\left|n\widetilde{\Phi}_{5}(\bm{F};\mathbf{I},p\nu_{1})-\frac{ \Upsilon(\nu_{1},\nu_{2})}{1-\Upsilon(\nu_{1},\nu_{2})}\right|\] \[\leq\] \[\leq \left\{\left|n\widetilde{\Phi}_{5}(\bm{F};\mathbf{I},p\nu_{1})- \frac{\Upsilon(\nu_{1},\nu_{2})}{1-\Upsilon(\nu_{1},\nu_{2})}\right|+\frac{1}{1 -\Upsilon(\nu_{1},\nu_{2})}\right\}\cdot C_{*,D,K}\cdot\rho_{\gamma_{+}}(p) \mathcal{E}_{3}(p)\frac{\Upsilon(\nu_{1},\nu_{2})}{1-\Upsilon(\nu_{1},\nu_{2})}.\] (110)

Note that by Eq. (119) in Lemma B.14, we have

\[\frac{\Upsilon(\nu_{1},\nu_{2})}{1-\Upsilon(\nu_{1},\nu_{2})}\leq(1-\Upsilon(\nu_ {1},\nu_{2}))^{-1}\leq C_{*}\cdot\widetilde{\rho}_{\lambda}(n,p).\]Hence rearranging the terms in Eq. (110) and using from conditions (106) and that \(p\geq C_{*,D,K}\), we obtain with probability at least \(1-p^{-D}\) that

\[\left|n\widetilde{\Phi}_{5}(\bm{F};\mathbf{I},p\nu_{1})-\frac{\Upsilon(\nu_{1}, \nu_{2})}{1-\Upsilon(\nu_{1},\nu_{2})}\right|\leq C_{*,D,K}\cdot\widetilde{ \rho}_{\lambda}(n,p)\rho_{\gamma_{+}}(p)\mathcal{E}_{3}(p)\frac{\Upsilon(\nu_{ 1},\nu_{2})}{1-\Upsilon(\nu_{1},\nu_{2})}.\] (111)

Using an union bound and combining bounds Eqs. (108) and (111), we obtain with probability at least \(1-n^{-D}-p^{-D}\), that

\[\left|\mathcal{V}(\bm{G},\bm{F},\lambda)-\mathsf{V}_{n,p}(\lambda)\right|\] \[\leq\,\left|\mathcal{V}(\bm{G},\bm{F},\lambda)-\sigma_{\varepsilon }^{2}\cdot n\widetilde{\Phi}_{5}(\bm{F};\mathbf{I},p\nu_{1})\right|+\sigma_{ \varepsilon}^{2}\left|n\widetilde{\Phi}_{5}(\bm{F};\mathbf{I},p\nu_{1})- \frac{\Upsilon(\nu_{1},\nu_{2})}{1-\Upsilon(\nu_{1},\nu_{2})}\right|\] \[\leq C_{*,D,K}\cdot\left\{\mathcal{E}_{1}(p,n)+\widetilde{\rho} _{\lambda}(n,p)\rho_{\gamma_{+}}(p)\mathcal{E}_{3}(p)\right\}\cdot\left[ \sigma_{\varepsilon}^{2}\cdot n\widetilde{\Phi}_{5}(\bm{F};\mathbf{I},p\nu_{1} )+\mathsf{V}_{n,p}(\lambda)\right]\] \[\leq C_{*,D,K}\cdot\left\{\mathcal{E}_{1}(p,n)+\widetilde{\rho} _{\lambda}(n,p)\rho_{\gamma_{+}}(p)\mathcal{E}_{3}(p)\right\}\cdot\mathsf{V}_ {n,p}(\lambda),\]

where we used Eq. (111) and conditions (106) in the last line. Replacing the rates \(\mathcal{E}_{j}\) by their expressions conclude the proof of this theorem. 

**Lemma B.11**.: _Under the setting of Theorem B.10 and assuming the same conditions (106), we have with probability at least \(1-p^{-D}\),_

\[\left|\frac{1}{p}\mathrm{Tr}(\bm{F}\bm{F}^{\mathsf{T}}(\bm{F}\bm {F}^{\mathsf{T}}+p\nu_{1})^{-1})-\Psi_{2}(\nu_{2})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{3}(p)\cdot\Psi_{2}(\nu_{2}),\] (112) \[\left|\frac{1}{p}\mathrm{Tr}(\bm{F}\bm{F}^{\mathsf{T}}(\bm{F}\bm {F}^{\mathsf{T}}+p\nu_{1})^{-2})-\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1})\right| \leq C_{*,D,K}\cdot\rho_{\gamma_{+}}(p)\mathcal{E}_{3}(p)\cdot \Psi_{3}(\nu_{2};\bm{\Sigma}^{-1}),\] (113)

_where \(\mathcal{E}_{3}(p)\) is the approximation rate defined in Eq. (100)._

The proof of this lemma can be found in Section B.7.4.

### Approximation guarantee for the bias term

Recall the expression for the bias term

\[\mathcal{B}(\bm{\beta}_{*};\bm{G},\bm{F},\lambda)=\|\bm{\beta}_{*}-p^{-1/2}\bm {F}^{\mathsf{T}}(\bm{Z}^{\mathsf{T}}\bm{Z}+\lambda)^{-1}\bm{Z}^{\mathsf{T}}\bm {G}\bm{\beta}_{*}\|_{2}^{2},\]

and its associated deterministic equivalent

\[\chi(\nu_{2}) =\frac{\mathrm{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-2})}{p- \mathrm{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{-2})},\] \[\mathsf{B}_{n,p}(\bm{\beta}_{*},\lambda) =\frac{\nu_{2}^{2}}{1-\Upsilon(\nu_{1},\nu_{2})}\Big{[}\langle \bm{\beta}_{*},(\bm{\Sigma}+\nu_{2})^{-2}\bm{\beta}_{*}\rangle+\chi(\nu_{2}) \langle\bm{\beta}_{*},\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-2}\bm{\beta}_{*} \rangle\Big{]}.\]

We prove in this section an approximation guarantee between \(\mathcal{B}(\bm{\beta}_{*};\bm{G},\bm{F},\lambda)\) and \(\mathsf{B}_{n,p}(\bm{\beta}_{*},\lambda)\). For convenience, we state a separate theorem for this term.

**Theorem B.12** (Deterministic equivalent for the bias term).: _Assume the features \((\bm{z}_{i})_{i\in[n]}\) and \((\bm{f}_{j})_{j\in[p]}\) satisfy Assumption B.1, and that there exists \(\mathsf{m}\in\mathbb{N}\) such that \(p^{2}\xi_{\mathsf{m}}^{2}\leq\gamma_{\mathsf{m}}(n\lambda/p)\). Then, for any \(D,K>0\), there exist constants \(\eta_{*}\in(0,1/2)\) and \(C_{*,D,K}>0\) such that the following holds. Let \(\rho_{\kappa}\) and \(\widetilde{\rho}_{\kappa}\) be defined as per Eqs. (26) and (25), and recall that \(\gamma_{\lambda}:=\gamma_{\mathsf{m}}(n\lambda/p)\) and \(\gamma_{+}:=\gamma_{\mathsf{m}}(p\nu_{1})\). For any \(n,p\geq C_{*,D,K}\) and \(\lambda>0\), if it holds that_

\[\lambda\geq n^{-K},\qquad\gamma_{\lambda}\geq p^{-K},\qquad\qquad \widetilde{\rho}_{\lambda}(n,p)^{5/2}\cdot\log^{3/2}(n) \leq K\sqrt{n},\] (114) \[\widetilde{\rho}_{\lambda}(n,p)^{2}\cdot\rho_{\gamma_{+}}(p)^{8} \cdot\log^{4}(p) \leq K\sqrt{p},\]

_then with probability at least \(1-n^{-D}-p^{-D}\), we have_

\[\left|\mathcal{B}(\bm{\beta}_{*};\bm{G},\bm{F},\lambda)-\mathsf{B}_{n,p}(\bm{ \beta}_{*},\lambda)\right|\leq C_{*,D,K}\cdot\mathcal{E}_{B}(n,p)\cdot\mathsf{ B}_{n,p}(\bm{\beta}_{*},\lambda),\]

_where the approximation rate is given by_

\[\mathcal{E}_{B}(n,p):=\frac{\widetilde{\rho}_{\lambda}(n,p)^{6}\log^{7/2}(n)}{ \sqrt{n}}+\frac{\widetilde{\rho}_{\lambda}(n,p)^{2}\cdot\rho_{\gamma_{+}}(p)^{8 }\log^{7/2}(p)}{\sqrt{p}}.\] (115)Before starting the proof, let us introduce some notations. First, define \(\mathsf{P}_{\bm{F}}\) the projection onto the span of \(\bm{F}\), and \(\mathsf{P}_{\perp,\bm{F}}\) the projection orthogonal to the span of \(\bm{F}\), i.e.,

\[\mathsf{P}_{\bm{F}}:=(\bm{F}^{\mathsf{T}}\bm{F})^{\dagger}\bm{F}^{\mathsf{T}}\bm {F},\qquad\qquad\mathsf{P}_{\perp,\bm{F}}=\mathbf{I}-\mathsf{P}_{\bm{F}}.\]

We can decompose the feature \(\bm{g}\) with respect to the orthogonal sum of these two subspaces

\[\bm{g}=\mathsf{P}_{\bm{F}}\bm{g}+\mathsf{P}_{\perp,\bm{F}}\bm{g}=\sqrt{p}(\bm{ F}^{\mathsf{T}}\bm{F})^{\dagger}\bm{F}^{\mathsf{T}}\bm{z}+\mathsf{P}_{\perp,\bm{F}} \bm{g}.\]

We define \(\bm{r}:=\mathsf{P}_{\perp,\bm{F}}\bm{g}\) and \(\bm{R}=[\bm{r}_{1},\ldots,\bm{r}_{n}]^{\mathsf{T}}\in\mathbb{R}^{n\times\infty}\). Similarly, we can decompose the target function

\[h_{*}(\bm{g})=\langle\bm{\beta}_{*},\bm{g}\rangle=\langle\bm{\beta}_{\bm{F}}, \bm{z}\rangle+\langle\bm{\beta}_{\perp,\bm{F}},\bm{r}\rangle,\]

where we introduced

\[\bm{\beta}_{\bm{F}}:=\sqrt{p}\bm{F}(\bm{F}^{\mathsf{T}}\bm{F})^{\dagger}\bm{ \beta}_{*},\qquad\qquad\bm{\beta}_{\perp,\bm{F}}=\mathsf{P}_{\perp,\bm{F}}\bm {\beta}_{*}.\]

Note that in particular \(\mathbb{E}[\bm{z}\langle\bm{r},\bm{\beta}_{\perp,\bm{F}}\rangle]=\bm{0}\) by orthogonality.

Proof of Theorem b.12.: **Step 0: Decomposing the bias term.**

Note that using the notations introduced above, we can decompose the bias term into

\[\mathcal{B}(\bm{\beta}_{*};\bm{G},\bm{F},\lambda) =\|\bm{\beta}_{*}-p^{-1/2}\bm{F}^{\mathsf{T}}(\bm{Z}^{\mathsf{T}} \bm{Z}+\lambda)^{-1}\bm{Z}^{\mathsf{T}}\bm{G}\bm{\beta}_{*}\|_{2}^{2}\] \[=\frac{1}{p}\left\|\bm{F}^{\mathsf{T}}\left(\bm{\beta}_{\bm{F}}-( \bm{Z}^{\mathsf{T}}\bm{Z}+\lambda)^{-1}\bm{Z}^{\mathsf{T}}(\bm{Z}\bm{\beta}_{ \bm{F}}+\bm{R}\bm{\beta}_{\perp,\bm{F}})\right)\right\|_{2}^{2}+\|\bm{\beta}_{ \perp,\bm{F}}\|_{2}^{2}\] \[=T_{1}-2T_{2}+T_{3}+\|\bm{\beta}_{\perp,\bm{F}}\|_{2}^{2}.\]

where we denoted

\[T_{1} :=\lambda^{2}\langle\bm{\beta}_{\bm{F}},(\bm{Z}^{\mathsf{T}}\bm{Z }+\lambda)^{-1}\widehat{\bm{\Sigma}}_{\bm{F}}(\bm{Z}^{\mathsf{T}}\bm{Z}+ \lambda)^{-1}\bm{\beta}_{\bm{F}}\rangle,\] \[T_{2} :=\lambda\langle\bm{\beta}_{\bm{F}},(\bm{Z}^{\mathsf{T}}\bm{Z}+ \lambda)^{-1}\widehat{\bm{\Sigma}}_{\bm{F}}(\bm{Z}^{\mathsf{T}}\bm{Z}+\lambda)^ {-1}\bm{Z}^{\mathsf{T}}\bm{R}\bm{\beta}_{\perp,\bm{F}}\rangle,\] \[T_{3} :=\langle\bm{\beta}_{\perp,\bm{F}},\bm{R}^{\mathsf{T}}\bm{Z}(\bm{ Z}^{\mathsf{T}}\bm{Z}+\lambda)^{-1}\widehat{\bm{\Sigma}}_{\bm{F}}(\bm{Z}^{ \mathsf{T}}\bm{Z}+\lambda)^{-1}\bm{Z}^{\mathsf{T}}\bm{R}\bm{\beta}_{\perp,\bm {F}}\rangle.\]

We proceed similarly to the proof for the variance term, by first considering the deterministic equivalent over \(\bm{Z}\) conditional on \(\bm{F}\), and then over \(\bm{F}\). We omit some repetitive details for the sake of brevity.

**Step 1: Deterministic equivalent over \(\bm{Z}\) conditional on \(\bm{F}\).**

First note that, denoting \(\tilde{\bm{A}}_{*}=\widehat{\bm{\Sigma}}_{\bm{F}}^{-1/2}\bm{\beta}_{\bm{F}} \bm{\beta}_{\bm{F}}^{\mathsf{T}}\widehat{\bm{\Sigma}}_{\bm{F}}^{-1/2}\), we have

\[T_{1}=\lambda^{2}\Phi_{3}(\bm{Z};\tilde{\bm{A}}_{*},\lambda).\]

Furthermore, \(T_{3}\) and \(T_{2}\) correspond respectively to the terms (90) and (91) in Proposition B.7 with \(\bm{v}=\bm{\beta}_{\bm{F}}\) and \(\bm{u}=\bm{R}\bm{\beta}_{\perp,\bm{F}}\) where

\[\mathbb{E}[\bm{z}_{i}u_{i}]=\mathbb{E}[\bm{z}_{i}\langle\bm{r}_{i},\bm{\beta}_{ \perp,\bm{F}}\rangle]=0,\qquad\quad\mathbb{E}[u_{i}^{2}]=\|\bm{\beta}_{\perp, \bm{F}}\|_{2}^{2}.\]

Thus, under the assumptions of Theorem B.12, we can apply Propositions B.8 and B.7 to obtain (via union bound) that with probability at least \(1-n^{-D}-p^{-D}\),

\[\left|T_{1}-(n\nu_{1})^{2}\widetilde{\Phi}_{5}(\bm{F};\tilde{\bm{A }}_{*},p\nu_{1})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{1}(p,n)\cdot(n\nu_{1})^{2} \widetilde{\Phi}_{5}(\bm{F};\tilde{\bm{A}}_{*},p\nu_{1}),\] \[\left|T_{2}\right| \leq C_{*,D,K}\cdot\mathcal{E}_{2}(p,n)\cdot\sqrt{\|\bm{\beta}_{ \perp,\bm{F}}\|_{2}^{2}\cdot(n\nu_{1})^{2}\widetilde{\Phi}_{5}(\bm{F};\tilde{\bm{A }}_{*},p\nu_{1})},\] \[\left|T_{3}-\|\bm{\beta}_{\perp,\bm{F}}\|_{2}^{2}\cdot n \widetilde{\Phi}_{5}(\bm{F};\mathbf{I},p\nu_{1})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{2}(p,n)\cdot\|\bm{\beta}_{\perp, \bm{F}}\|_{2}^{2}.\]

Hence we deduce that

\[\left|\mathcal{B}(\bm{\beta}_{*};\bm{G},\bm{F},\lambda)-(n\nu_{1}) ^{2}\widetilde{\Phi}_{5}(\bm{F};\tilde{\bm{A}}_{*},p\nu_{1})-\|\bm{\beta}_{\perp, \bm{F}}\|_{2}^{2}\cdot n\widetilde{\Phi}_{5}(\bm{F};\mathbf{I},p\nu_{1})-\|\bm{ \beta}_{\perp,\bm{F}}\|_{2}^{2}\right|\] \[\leq C_{*,D,K}\cdot\{\mathcal{E}_{1}(n,p)+\mathcal{E}_{2}(n,p)\} \cdot\left[(n\nu_{1})^{2}\widetilde{\Phi}_{5}(\bm{F};\tilde{\bm{A}}_{*},p\nu_{1})+ \|\bm{\beta}_{\perp,\bm{F}}\|_{2}^{2}\right].\]Let us simplify these terms. Recall that

\[n\widetilde{\Phi}_{5}(\bm{F};\tilde{\bm{A}}_{*},p\nu_{1})=\frac{\widetilde{\Phi}_{ 6}(\bm{F};\tilde{\bm{A}}_{*},p\nu_{1})}{n-\widetilde{\Phi}_{6}(\bm{F};\bm{\Gamma },p\nu_{1})},\qquad\quad n\widetilde{\Phi}_{5}(\bm{F};\bm{\Gamma},p\nu_{1})\frac{ \widetilde{\Phi}_{6}(\bm{F};\bm{\Gamma},p\nu_{1})}{n-\widetilde{\Phi}_{6}(\bm{ F};\bm{\Gamma},p\nu_{1})}.\]

For the term involving \(\tilde{\bm{A}}_{*}\), we can rewrite it as

\[\nu_{1}^{2}\widetilde{\Phi}_{6}(\bm{F};\tilde{\bm{A}}_{*},p\nu_{1 }) =\nu_{1}^{2}\langle\bm{\beta}_{\bm{F}},\widehat{\bm{\Sigma}}_{\bm{F }}^{-1}(\bm{F}^{\mathsf{T}}\bm{F})^{2}(\bm{F}^{\mathsf{T}}\bm{F}+p\nu_{1})^{-2} \bm{\beta}_{\bm{F}}\rangle\] \[=(p\nu_{1})^{2}\langle\bm{\beta}_{*},(\bm{F}^{\mathsf{T}}\bm{F})^ {\dagger}(\bm{F}^{\mathsf{T}}\bm{F})(\bm{F}^{\mathsf{T}}\bm{F}+p\nu_{1})^{-2} (\bm{F}^{\mathsf{T}}\bm{F})(\bm{F}^{\mathsf{T}}\bm{F})^{\dagger}\bm{\beta}_{*}\rangle\] \[=(p\nu_{1})^{2}\langle\bm{\beta}_{*},(\bm{F}^{\mathsf{T}}\bm{F}+p \nu_{1})^{-2}\bm{\beta}_{*}\rangle-\|\bm{\beta}_{\perp,\bm{F}}\|_{2}^{2}.\]

Hence the terms involving \(\|\bm{\beta}_{\perp,\bm{F}}\|_{2}^{2}\) cancel out and we obtain

\[(n\nu_{1})^{2}\widetilde{\Phi}_{5}(\bm{F};\tilde{\bm{A}}_{*},p\nu_{1})+\|\bm{ \beta}_{\perp,\bm{F}}\|_{2}^{2}\cdot n\widetilde{\Phi}_{5}(\bm{F};\bm{\Gamma },p\nu_{1})+\|\bm{\beta}_{\perp,\bm{F}}\|_{2}^{2}=(p\nu_{1})^{2}\frac{\langle \bm{\beta}_{*},(\bm{F}^{\mathsf{T}}\bm{F}+p\nu_{1})^{-2}\bm{\beta}_{*}\rangle} {1-\frac{1}{n}\widetilde{\Phi}_{6}(\bm{F};\bm{\Gamma},p\nu_{1})}.\]

Combining the above displays, we deduce that with probability at least \(1-n^{-D}-p^{-D}\),

\[\left|\mathcal{B}(\bm{\beta}_{*};\bm{G},\bm{F},\lambda)-(p\nu_{1 })^{2}\frac{\langle\bm{\beta}_{*},(\bm{F}^{\mathsf{T}}\bm{F}+p\nu_{1})^{-2} \bm{\beta}_{*}\rangle}{1-\frac{1}{n}\widetilde{\Phi}_{6}(\bm{F};\bm{\Gamma},p \nu_{1})}\right|\] (116) \[\leq C_{*,D,K}\cdot\{\mathcal{E}_{1}(n,p)+\mathcal{E}_{2}(n,p)\} \cdot(p\nu_{1})^{2}\frac{\langle\bm{\beta}_{*},(\bm{F}^{\mathsf{T}}\bm{F}+p \nu_{1})^{-2}\bm{\beta}_{*}\rangle}{1-\frac{1}{n}\widetilde{\Phi}_{6}(\bm{F}; \bm{\Gamma},p\nu_{1})}.\]

**Step 2: Deterministic equivalents over \(\bm{F}\).**

Following the same steps as Eq. (110) in the proof of Theorem B.10, we have with probability at least \(1-p^{-D}\),

\[\left|(1-n^{-1}\widetilde{\Phi}_{6}(\bm{F};\bm{\Gamma},p\nu_{1}))^{-1}-(1- \Upsilon(\nu_{1},\nu_{2}))^{-1}\right|\leq C_{*,D,K}\cdot\widetilde{\rho}_{ \lambda}(n,p)\rho_{\gamma_{+}}(p)\mathcal{E}_{3}(p)\cdot(1-\Upsilon(\nu_{1}, \nu_{2}))^{-1}.\]

Furthermore, from Lemma B.13 stated below, with probability at least \(1-p^{-D}\),

\[\left|(p\nu_{1})^{2}\langle\bm{\beta}_{*},(\bm{F}^{\mathsf{T}}\bm{F}+p\nu_{1}) ^{-2}\bm{\beta}_{*}\rangle-\widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*}, \lambda)\right|\leq C_{*,D,K}\cdot\rho_{\gamma_{+}}(p)^{2}\mathcal{E}_{4}(p) \cdot\widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*},\lambda),\]

where \(\widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*},\lambda)\) is defined in Eq. (118). Combining these two bounds and recalling conditions (114), we obtain

\[\left|(p\nu_{1})^{2}\frac{\langle\bm{\beta}_{*},(\bm{F}^{\mathsf{ T}}\bm{F}+p\nu_{1})^{-2}\bm{\beta}_{*}\rangle}{1-\frac{1}{n}\widetilde{\Phi}_{6}( \bm{F};\bm{\Gamma},p\nu_{1})}-\frac{\widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*}, \lambda)}{1-\Upsilon(\nu_{1},\nu_{2})}\right|\] \[\leq C_{*,D,K}\left\{\widetilde{\rho}_{\lambda}(n,p)\rho_{\gamma _{+}}(p)\mathcal{E}_{3}(p)+\rho_{\gamma_{+}}(p)^{2}\mathcal{E}_{4}(p)\right\} \cdot\frac{\widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*},\lambda)}{1-\Upsilon(\nu _{1},\nu_{2})}.\]

Noting that \(\mathsf{B}_{n,p}(\bm{\beta}_{*},\lambda)=\widetilde{\mathsf{B}}_{n,p}(\bm{\beta} _{*},\lambda)/(1-\Upsilon(\nu_{1},\nu_{2}))\), we can combine this bound with Eq. (116) to obtain via union bound that with probability at least \(1-n^{-D}-p^{-D}\),

\[|\mathcal{B}(\bm{\beta}_{*};\bm{G},\bm{F},\lambda)-\mathsf{B}_{n,p}(\bm{\beta} _{*},\lambda)|\] \[\leq C_{*,D,K}\left\{\mathcal{E}_{1}(n,p)+\mathcal{E}_{2}(n,p)+ \widetilde{\rho}_{\lambda}(n,p)\rho_{\gamma_{+}}(p)\mathcal{E}_{3}(p)+\rho_{ \gamma_{+}}(p)^{2}\mathcal{E}_{4}(p)\right\}\cdot\mathsf{B}_{n,p}(\bm{\beta}_{*},\lambda).\]

Replacing the rates \(\mathcal{E}_{j}\) by their expressions conclude the proof of this theorem. 

**Lemma B.13**.: _Under the setting of Theorem B.12 and assuming the same conditions (114), we have with probability at least \(1-p^{-D}\),_

\[\left|(p\nu_{1})^{2}\langle\bm{\beta}_{*},(\bm{F}^{\mathsf{T}}\bm{F}+p\nu_{1})^{-2 }\bm{\beta}_{*}\rangle-\widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*},\lambda) \right|\leq C_{*,D,K}\cdot\rho_{\gamma_{+}}(p)^{2}\mathcal{E}_{4}(p)\cdot \widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*},\lambda),\] (117)

_where \(\mathcal{E}_{3}(p)\) and \(\mathcal{E}_{4}(p)\) are the approximation rates defined in Eqs. (100) and (103), and we denoted_

\[\widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*},\lambda):=\nu_{2}^{2}\Big{[} \langle\bm{\beta}_{*},(\bm{\Sigma}+\nu_{2})^{-2}\bm{\beta}_{*}\rangle+\chi(\nu_{2 })\langle\bm{\beta}_{*},\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-2}\bm{\beta}_{*} \rangle\Big{]}.\] (118)

The proof of Lemma B.13 can be found in Section B.7.5.

### Technical results

In this section, we prove the technical results that were deferred from the previous sections. We start with a lemma that gathers useful bounds on the deterministic functionals.

**Lemma B.14**.: _There exists a constant \(C_{*}>0\) such that_

\[(1-\Upsilon(\nu_{1},\nu_{2}))^{-1}\leq C_{*}\cdot\widetilde{\rho}_{\lambda}(n, p),\] (119)

_where we recall that \(\Upsilon(\nu_{1},\nu_{2})\) is defined as per Eq. (105). Furthermore, under Assumption 3.2, we have_

\[p\nu_{1}\frac{\operatorname{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-2})}{p- \operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{-2})} \leq C_{*}\cdot n\Upsilon(\nu_{1},\nu_{2}),\] (120)

\[p\nu_{1}\frac{\langle\bm{\beta}_{*},\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-2}\bm{ \beta}_{*}\rangle}{p-\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{ -2})} \leq C_{*}\cdot\widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*},\lambda),\] (121)

_where \(\widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*},\lambda)\) is defined as per Eq. (118) in Lemma B.13._

Proof of Lemma b.14.: **Step 1: Equation (119).**

Note that we have

\[\Upsilon(\nu_{1},\nu_{2})\leq\frac{1}{n}\operatorname{Tr}(\bm{\Sigma}(\bm{ \Sigma}+\nu_{2})^{-1})\leq\frac{p}{n}.\]

In particular, if \(n\geq p/\eta_{*}\), then we can simply write

\[(1-\Upsilon(\nu_{1},\nu_{2}))^{-1}\leq\frac{1}{1-\eta_{*}}=C_{*}.\]

For \(n\leq p/\eta_{*}\), note that using the first identity in Eq. (76), we have

\[(1-\Upsilon(\nu_{1},\nu_{2}))^{-1}\leq\left(1-\frac{1}{n}\operatorname{Tr}( \bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1})\right)^{-1}\leq\frac{n\nu_{1}}{\lambda }\leq C_{*}\rho_{\lambda}(n).\]

Combining the previous two displays, we obtain Eq. (119).

**Step 2: Equation (120).**

We rewrite the left-hand side as

\[p\nu_{1}\frac{\operatorname{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2} )^{-2})}{p-\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{-2})} \leq\frac{p\nu_{1}}{p-\operatorname{Tr}(\bm{\Sigma}(\bm{\Sigma}+ \nu_{2})^{-1})}\operatorname{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-2})\] \[=\operatorname{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1})- \operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{-2})\] \[\leq\left(1-\frac{1}{\mathsf{C}_{*}}\right)\operatorname{Tr}(\bm {\Sigma}(\bm{\Sigma}+\nu_{2})^{-1}),\]

where we uses the second of the identities (76) in the second line, and Assumption 3.2 in the third line. Hence,

\[p\nu_{1}\frac{\operatorname{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2} )^{-2})}{p-\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{-2})} \leq(\mathsf{C}_{*}-1)\cdot\left\{\operatorname{Tr}(\bm{\Sigma}( \bm{\Sigma}+\nu_{2})^{-1})-p\nu_{1}\frac{\operatorname{Tr}(\bm{\Sigma}(\bm{ \Sigma}+\nu_{2})^{-2})}{p-\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{ 2})^{-2})}\right\}\] \[=(\mathsf{C}_{*}-1)\cdot n\Upsilon(\nu_{1},\nu_{2}).\]

**Step 3: Equation (121).**

Similarly, we get again using Eq. (76) and Assumption 3.2 that

\[p\nu_{1}\frac{\langle\bm{\beta}_{*},\bm{\Sigma}(\bm{\Sigma}+\nu_ {2})^{-2}\bm{\beta}_{*}\rangle}{p-\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma }+\nu_{2})^{-2})} \leq\nu_{2}\left\{\langle\bm{\beta}_{*},(\bm{\Sigma}+\nu_{2})^{-1} \bm{\beta}_{*}\rangle-\nu_{2}\langle\bm{\beta}_{*},\bm{\Sigma}^{2}(\bm{\Sigma}+ \nu_{2})^{-2}\bm{\beta}_{*}\rangle\right\}\] \[\leq\left(1-\frac{1}{\mathsf{C}_{*}}\right)\cdot\nu_{2}\langle\bm {\beta}_{*},(\bm{\Sigma}+\nu_{2})^{-1}\bm{\beta}_{*}\rangle,\]

and therefore

\[p\nu_{1}\frac{\langle\bm{\beta}_{*},\bm{\Sigma}(\bm{\Sigma}+\nu _{2})^{-2}\bm{\beta}_{*}\rangle}{p-\operatorname{Tr}(\bm{\Sigma}^{2}(\bm{ \Sigma}+\nu_{2})^{-2})} \leq(\mathsf{C}_{*}-1)\left\{\nu_{2}\langle\bm{\beta}_{*},(\bm{ \Sigma}+\nu_{2})^{-1}\bm{\beta}_{*}\rangle-p\nu_{1}\frac{\langle\bm{\beta}_{* },\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-2}\bm{\beta}_{*}\rangle}{p- \operatorname{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{-2})}\right\}\] \[=\widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*},\lambda),\]

which concludes the proof of this lemma.

#### b.7.1 Feature covariance matrix

Proof of Lemma b.2.: Recall that we consider \(\widehat{\bm{\Sigma}}_{\bm{F}}=p^{-1}\bm{F}\bm{F}^{\mathsf{T}}\), with \(\bm{F}=[\bm{f}_{1},\dots,\bm{f}_{p}]^{\mathsf{T}}\in\mathbb{R}^{p\times\infty}\) and the \(\bm{f}_{i}\) are i.i.d. random vectors satisfying Assumption B.1. For any integers \(k_{2}\geq k_{1}\geq 1\), we split the weight feature matrix into \(\bm{F}=[\bm{F}_{1},\bm{F}_{2},\bm{F}_{3}]\), where \(\bm{F}_{1}=[\bm{f}_{1,1},\dots,\bm{f}_{1,p}]^{\mathsf{T}}\in\mathbb{R}^{p\times k _{1}}\) with \(\bm{f}_{1,j}\) the first \(k_{1}\) coordinates of the feature vector \(\bm{f}_{j}\), \(\bm{F}_{2}=[\bm{f}_{2,1},\dots,\bm{f}_{2,p}]^{\mathsf{T}}\in\mathbb{R}^{p\times (k_{2}-k_{1})}\) with \(\bm{f}_{2,j}\) the next \(k_{2}-k_{1}\) coordinates of \(\bm{f}_{j}\), and \(\bm{F}_{3}=[\bm{f}_{3,1},\dots,\bm{f}_{3,p}]^{\mathsf{T}}\in\mathbb{R}^{p\times\infty}\) contains the rest of the coordinates. In other words, we split the feature vector into \(\bm{f}_{j}=[\bm{f}_{1,j},\bm{f}_{2,j},\bm{f}_{3,j}]\). Denote

\[\bm{\Sigma}_{1} =\mathbb{E}[\bm{f}_{1,j}\bm{f}_{1,j}^{\mathsf{T}}]=\operatorname{ diag}(\xi_{1}^{2},\dots,\xi_{k_{1}}^{2})\in\mathbb{R}^{k_{1}\times k_{1}},\] \[\bm{\Sigma}_{2} =\mathbb{E}[\bm{f}_{2,j}\bm{f}_{2,j}^{\mathsf{T}}]= \operatorname{diag}(\xi_{k_{1}+1}^{2},\dots,\xi_{k_{2}}^{2})\in \mathbb{R}^{(k_{2}-k_{1})\times(k_{2}-k_{1})},\] \[\bm{\Sigma}_{3} =\mathbb{E}[\bm{f}_{3,j}\bm{f}_{3,j}^{\mathsf{T}}]= \operatorname{diag}(\xi_{k_{2}+1}^{2},\xi_{k_{2}+2}^{2},\dots) \in\mathbb{R}^{\infty\times\infty}.\]

We decompose the feature covariance matrix into

\[\frac{\bm{F}\bm{F}^{\mathsf{T}}}{p}=\frac{\bm{F}_{1}\bm{F}_{1}^{\mathsf{T}}}{p} +\frac{\bm{F}_{2}\bm{F}_{2}^{\mathsf{T}}}{p}+\frac{\bm{F}_{3}\bm{F}_{3}^{ \mathsf{T}}}{p}.\] (122)

#### Step 1: Bounding the eigenvalues of \(\bm{F}_{1}\) and \(\bm{F}_{2}\).

Introduce the whitened matrices

\[\overline{\bm{F}}_{1}=\bm{F}_{1}\bm{\Sigma}_{1}^{-1/2}=[\overline{\bm{f}}_{1,1 },\dots,\overline{\bm{f}}_{1,p}]^{\mathsf{T}},\qquad\quad\overline{\bm{F}}_{2} =\bm{F}_{2}\bm{\Sigma}_{2}^{-1/2}=[\overline{\bm{f}}_{2,1},\dots,\overline{\bm {f}}_{2,p}]^{\mathsf{T}},\]

so that the feature vectors \(\overline{\bm{f}}_{1,j}\) and \(\overline{\bm{f}}_{2,j}\) have covariance \(\mathbf{I}_{k_{1}}\) and \(\mathbf{I}_{k_{2}-k_{1}}\) respectively. We have

\[\|\overline{\bm{F}}_{1}^{\mathsf{T}}\overline{\bm{F}}_{1}/p-\mathbf{I}_{k_{1}} \|_{\mathrm{op}}=\sup_{\bm{v}\in\mathbb{R}^{k_{1}},\|\bm{v}\|_{2}=1}\left| \frac{1}{p}\sum_{j\in[p]}\langle\bm{v},\overline{\bm{f}}_{1,j}\rangle^{2}-1\right|\]

Denote \(Z_{j}:=\langle\bm{v},\overline{\bm{f}}_{1,j}\rangle^{2}-1\). Then we have from Equation (60) applied to \(\bm{B}=\bm{\Sigma}_{1}^{-1/2}\bm{v}\bm{v}^{\mathsf{T}}\bm{\Sigma}_{1}^{-1/2}\), for any integer \(q\geq 1\),

\[\mathbb{E}[|Z_{j}|^{q}]\leq q\mathsf{C}_{*}\int_{0}^{\infty}t^{q-1}e^{-\varepsilon _{x}t}\mathrm{d}t=\frac{\mathsf{C}_{*}q!}{\mathsf{c}_{*}^{q}}.\]

Hence we can apply Bernstein's inequality for centered sub-exponential random variable and obtain

\[\mathbb{P}\left(\left|\frac{1}{p}\sum_{j\in[p]}Z_{j}\right|\geq\varepsilon/2 \right)\leq 2\exp\left\{-c_{*}p\cdot\min(\varepsilon^{2},\varepsilon)\right\}.\] (123)

Following the proof of [23, Theorem 5.39], we deduce that there exist constants \(C_{*},C_{*,D}>0\) such that with probability at least \(1-p^{-D}\),

\[\|\overline{\bm{F}}_{1}^{\mathsf{T}}\overline{\bm{F}}_{1}/p-\mathbf{I}_{k_{1}} \|_{\mathrm{op}}\leq C_{*}\sqrt{\frac{k_{1}}{p}}+C_{*,D}\sqrt{\frac{\log(p)}{p}}.\]

In particular, there exists \(\eta_{*}\in(0,1/4)\) and \(C_{*,D}>0\) such that for \(p\geq C_{*,D}\) and via union bound, we have with probability at least \(1-p^{-D}\) (reparametrizing \(D\)), that for any \(k_{1}\leq\lfloor\eta_{*}\cdot p\rfloor\),

\[\|\overline{\bm{F}}_{1}^{\mathsf{T}}\overline{\bm{F}}_{1}/p-\mathbf{I}_{k_{1}} \|_{\mathrm{op}}\leq 1/2.\]

From Eq. (122), we deduce that the \(k_{1}\)-th eigenvalue of \(\widehat{\bm{\Sigma}}_{\bm{F}}\) for \(k_{1}<\lfloor\eta_{*}\cdot p\rfloor\) is lower bounded by

\[\hat{\xi}_{k_{1}}^{2}\geq\lambda_{\min}\left(\frac{\bm{F}_{1}\bm{F}_{1}^{ \mathsf{T}}}{p}\right)\geq\xi_{k_{1}}^{2}\cdot\lambda_{\min}\left(\frac{\overline {\bm{F}}_{1}\bm{F}_{1}^{\mathsf{T}}}{p}\right)\geq\frac{\xi_{k_{1}}^{2}}{2},\] (124)

and

\[\lambda_{\max}\left(\frac{\bm{F}_{1}\bm{F}_{1}^{\mathsf{T}}}{p}\right)\leq\frac{3 }{2}\xi_{1}^{2}=\frac{3}{2}.\]Similarly for \(\overline{\bm{F}}_{2}\), we have with probability at least \(1-p^{-D}\) that for any \(k_{1}<k_{2}\leq\lfloor\eta_{*}\cdot p\rfloor\),

\[\|\overline{\bm{F}}_{2}^{\mathsf{T}}\overline{\bm{F}}_{2}/p-\mathbf{I}_{k_{2}-k _{1}}\|_{\mathrm{op}}\leq 1/2,\]

and therefore

\[\lambda_{\max}\left(\frac{\bm{F}_{2}\bm{F}_{2}^{\mathsf{T}}}{p}\right)\leq \frac{3}{2}\xi_{k_{1}+1}^{2}.\] (125)

#### Step 2: Bounding the eigenvalues of \(\bm{F}\).

Equation (124) provides a lower bound on the eigenvalues \(\hat{\xi}_{k}^{2}\) of \(\widehat{\bm{\Sigma}}_{\bm{F}}\) up to \(k<\lfloor\eta_{*}\cdot p\rfloor\). Let's upper bound the \(p\) eigenvalues of \(\widehat{\bm{\Sigma}}_{\bm{F}}\). From now on, set \(k_{2}=p_{*}-1\) where \(p_{*}:=\lfloor\eta_{*}\cdot p\rfloor\).

For the contribution of \(\|\bm{F}_{+}\|\), we use the matrix Bernstein's inequality as in (Misiakiewicz and Saeed, 2024, Lemma 1) (see proof of Theorem A.2 in Appendix A) and obtain that for \(p\geq C_{K}\), with probability at least \(1-p^{-D}\),

\[\frac{1}{p}\|\bm{F}_{3}\bm{F}_{3}^{\mathsf{T}}\|_{\mathrm{op}}\leq C_{*,D,K} \cdot\xi_{p_{*}}^{2}\left\{1+\frac{r_{\bm{\Sigma}}(p_{*})\lor p}{p}\log\left(r _{\bm{\Sigma}}(p_{*})\lor p\right)\right\}.\]

By the min-max theorem, we have with probability at least \(1-p^{-D}\) for all \(k_{1}<p_{*}-1\),

\[\hat{\xi}_{k_{1}+1}^{2}\leq\lambda_{\max}\left(\frac{\bm{F}_{2}\bm{F}_{2}^{ \mathsf{T}}}{p}+\frac{\bm{F}_{3}\bm{F}_{3}^{\mathsf{T}}}{p}\right)\leq\frac{3} {2}\xi_{k_{1}+1}^{2}+C_{*,D,K}\cdot\xi_{p_{*}}^{2}\left\{1+\frac{r_{\bm{ \Sigma}}(p_{*})\lor p}{p}\log\left(r_{\bm{\Sigma}}(p_{*})\lor p\right)\right\},\]

where we used Eq. (125). For \(k\geq p_{*}\), we simply use that

\[\hat{\xi}_{k}^{2}\leq\frac{1}{p}\|\bm{F}_{3}\bm{F}_{3}^{\mathsf{T}}\|_{\mathrm{ op}}\leq\frac{3}{2}\xi_{k}^{2}+C_{*,D,K}\cdot\xi_{p_{*}}^{2}\left\{1+\frac{r_{ \bm{\Sigma}}(p_{*})\lor p}{p}\log\left(r_{\bm{\Sigma}}(p_{*})\lor p\right) \right\}.\]

We deduce from the above two displays that with probability at least \(1-p^{-D}\), we have for any \(k\leq p\)

\[\hat{\xi}_{k}^{2}\leq C_{*,K,D}\cdot\left\{\xi_{k}^{2}+\xi_{p_{*}}^{2}\cdot M _{\bm{\Sigma}}(p)\right\},\] (126)

where we recall that we defined

\[M_{\bm{\Sigma}}(k):=1+\frac{r_{\bm{\Sigma}}(\lfloor\eta_{*}\cdot k\rfloor) \lor k}{k}\log\left(r_{\bm{\Sigma}}(\lfloor\eta_{*}\cdot k\rfloor)\lor k \right).\]

#### Step 3: Bounding \(r_{\widehat{\bm{\Sigma}}_{\bm{F}}}(k)\) for \(k\leq p\).

Recall that the intrinsic dimension \(r_{\widehat{\bm{\Sigma}}_{\bm{F}}}(k)\) at level \(k\leq p\) is given by

\[r_{\widehat{\bm{\Sigma}}_{\bm{F}}}(k)=\frac{\sum_{j=k}^{p}\hat{\xi}_{j}^{2}}{ \hat{\xi}_{k}^{2}}.\]

First note that for \(p\geq k\geq\lfloor\eta_{*}\cdot p\rfloor\), we simply use that and the eigenvalues are nonincreasing to get

\[\frac{\sum_{j=k}^{p}\hat{\xi}_{j}^{2}}{\hat{\xi}_{k}^{2}}\leq(p+1-k)\leq C(1- \eta_{*})p\leq C\frac{1-\eta_{*}}{\eta_{*}}k.\]

For \(k\leq p_{*}-1\), we use that from Eq. (124) with probability at least \(1-p^{-D}\),

\[\frac{\sum_{j=k}^{p}\hat{\xi}_{j}^{2}}{\hat{\xi}_{k}^{2}}\leq\frac{2}{\hat{\xi }_{k}^{2}}\left(\frac{3}{2}\sum_{j=k}^{\lfloor\eta_{*}\cdot p\rfloor-1}\xi_{k }^{2}+\sum_{j=\lfloor\eta_{*}\cdot p\rfloor}^{p}\hat{\xi}_{j}^{2}\right).\]

Let's bound the second term on the right-hand side. Notice that

\[\sum_{j=\lfloor\eta_{*}\cdot p\rfloor+1}^{p}\hat{\xi}_{j}^{2}=\min_{\bm{V}} \mathrm{Tr}(\widehat{\bm{\Sigma}}_{\bm{F}}(\mathbf{I}-\bm{V}\bm{V}^{\mathsf{T}})) \leq\frac{1}{p}\mathrm{Tr}(\bm{F}_{3}\bm{F}_{3}^{\mathsf{T}}),\]where the minimization is over \(\bm{V}\in\mathbb{R}^{p\times(\lfloor\eta_{*}\cdot p\rfloor-1)}\) with \(\bm{V}^{\mathsf{T}}\bm{V}=\mathbf{I}_{\lfloor\eta_{*}\cdot p\rfloor-1}\) and the second inequality is obtained by taking \(\bm{V}\) orthogonal to the matrix \([\bm{F}_{1},\bm{F}_{2}]\). We can rewrite

\[\frac{1}{p}\mathrm{Tr}(\bm{F}_{3}\bm{F}_{3}^{\mathsf{T}})-\mathrm{Tr}(\bm{ \Sigma}_{3})=\frac{1}{p}\sum_{j\in[p]}\|\bm{f}_{3,j}\|_{2}^{2}-\mathrm{Tr}(\bm {\Sigma}_{3}).\]

Introduce \(Z_{j}:=\|\bm{f}_{3,j}\|_{2}^{2}-\mathrm{Tr}(\bm{\Sigma}_{3})\). By Assumption B.1 with \(\bm{B}=\mathrm{diag}(\bm{0},\bm{\mathrm{I}})\) (identity on the subspace \(\bm{\Sigma}_{3}\) and \(0\) otherwise), we have

\[\mathbb{E}[|Z_{j}|^{q}]\leq q\mathsf{C}_{x}\|\bm{\Sigma}_{3}\|_{F}^{q}\int_{0 }^{\infty}t^{q-1}e^{-\mathsf{c}_{*}t}\mathrm{d}t\leq\mathsf{C}_{x}\cdot \mathrm{Tr}(\bm{\Sigma}_{3})^{q}\frac{q!}{\mathsf{c}_{x}^{q}}.\]

We therefore we can apply Bernstein's inequality (123) again and we get

\[\mathbb{P}\left(\left|\frac{1}{p}\sum_{j\in[p]}Z_{j}\right|\geq t\cdot\mathrm{ Tr}(\bm{\Sigma}_{3})\right)\leq 2\exp(-c_{*}\min(pt^{2},pt)).\]

Hence, for any \(p\geq C_{*,D}\) with probability at least \(1-p^{-D}\),

\[\frac{1}{p}\mathrm{Tr}(\bm{F}_{3}\bm{F}_{3}^{\mathsf{T}})\leq 2\mathrm{Tr}(\bm{ \Sigma}_{3}).\]

Combining the above display with the previous inequalities yields with probability at least \(1-p^{-D}\) that for any \(k\leq\lfloor\eta_{*}\cdot p\rfloor\),

\[r_{\widehat{\bm{\Sigma}}_{\bm{F}}}(k)=\frac{\sum_{j=k}^{p}\hat{\xi}_{j}^{2}}{ \hat{\xi}_{k}^{2}}\leq C\frac{\sum_{j=k}^{\infty}\xi_{j}^{2}}{\xi_{k}^{2}}.\]

We deduce that there exist a constant \(C_{*}>0\) such that with probability at least \(1-p^{-D}\), we have for any \(n_{*}:=\lfloor\eta_{*}\cdot n\rfloor\leq p\)

\[r_{\widehat{\bm{\Sigma}}_{\bm{F}}}(n_{*})\lor n\leq C_{*}\cdot r_{\bm{\Sigma}} (n_{*})\lor n,\] (127)

where \(r_{\bm{\Sigma}}(n)\) is the effective rank associated to \(\bm{\Sigma}\).

**Step 4: Concluding the proof.**

For any \(n_{*}=\lfloor\eta_{*}\cdot n\rfloor>p\), we have \(\widehat{\rho}_{\lambda}(n)=\widetilde{\rho}_{\lambda}(n,p)=1\). Hence, we only need to consider the case \(n_{*}\leq p\). Using Eqs. (126) and (127), we get

\[\widehat{\rho}_{\lambda}(n) =1+\frac{n\hat{\xi}_{n_{*}}^{2}}{\lambda}\left\{1+\frac{r_{ \widehat{\bm{\Sigma}}_{\bm{F}}}(n_{*})\lor n}{n}\log\left(r_{\widehat{\bm{ \Sigma}}_{\bm{F}}}(n_{*})\lor n\right)\right\}\] \[\leq 1+C_{*,D,K}\cdot\left\{\frac{n\xi_{n_{*}}^{2}}{\lambda}+ \frac{n}{p}\cdot\frac{p\cdot\xi_{p_{*}}}{\lambda}M_{\bm{\Sigma}}(p)\right\}M_ {\bm{\Sigma}}(n)\] \[\leq 1+C_{*,D,K}\cdot\left\{\frac{n\xi_{n_{*}}^{2}}{\lambda}+ \frac{n}{p}\cdot\rho_{\lambda}(p)\right\}M_{\bm{\Sigma}}(n),\]

which concludes the proof. 

#### b.7.2 Concentration of the fixed points

Proof of Proposition b.4.: Recall that \((\tilde{\nu}_{1},\tilde{\nu}_{2})\in\mathbb{R}_{>0}^{2}\) are the unique solution to the random fixed point equations (77). From the first equation, we have the following bounds on \(\tilde{\nu}_{1}\):

\[\frac{p\lambda}{n}\leq p\tilde{\nu}_{1}\leq\frac{p\|\widehat{\bm{\Sigma}}_{\bm {F}}\|_{\mathrm{op}}+p\lambda}{n}.\]

From Lemma B.2, we have with probability at least \(1-p^{-D}\) that \(\|\widehat{\bm{\Sigma}}_{\bm{F}}\|_{\mathrm{op}}\leq C_{*,D,K}\leq p\). Hence, by the uniform concentration over \(\kappa\in[p\lambda/n,(p^{3}+p\lambda)/n]\) in Lemma B.15 stated below and an union bound, we deduce that with probability at least \(1-p^{-D}\),

\[\left|\mathrm{Tr}\big{(}\bm{F}\bm{F}^{\mathsf{T}}(\bm{F}\bm{F}^{ \mathsf{T}}+p\tilde{\nu}_{1})^{-1}\big{)}-\mathrm{Tr}\big{(}\bm{\Sigma}(\bm{ \Sigma}+\tilde{\nu}_{2})^{-1}\big{)}\right| \leq C_{*,K,D}\frac{\rho_{\gamma_{*}}(p)^{5/2}\log^{3}(p)}{ \sqrt{p}}\mathrm{Tr}\big{(}\bm{\Sigma}(\bm{\Sigma}+\tilde{\nu}_{2})^{-1}\big{)}\] \[=:\widetilde{\mathcal{E}}_{2}\cdot\mathrm{Tr}\big{(}\bm{\Sigma}( \bm{\Sigma}+\tilde{\nu}_{2})^{-1}\big{)}.\]Therefore, we can rewrite the fixed equations (77) as

\[n-\frac{\lambda}{\tilde{\nu}_{1}} =\operatorname{Tr}\bigl{(}\bm{\Sigma}\bigl{(}\bm{\Sigma}+\tilde{\nu }_{2}\bigr{)}^{-1}\bigr{)}\cdot(1+\delta(\bm{F})),\] \[p-\frac{p\tilde{\nu}_{1}}{\tilde{\nu}_{2}} =\operatorname{Tr}\left(\bm{\Sigma}(\bm{\Sigma}+\tilde{\nu}_{2})^ {-1}\right).\]

where with probability at least \(1-p^{-D}\), we have \(|\delta(\bm{F})|\leq\widetilde{\mathcal{E}}(p)\). Therefore, by condition (78) and \(p\geq C_{*,D,K}\)

\[C_{*}\widetilde{\mathcal{E}}(p)\cdot\widetilde{\rho}_{\lambda}(n,p)\leq\frac{ C_{*,D,K}}{\log(p)}\leq\frac{1}{2},\]

and we can directly use Lemma B.17 stated below to obtain with probability at least \(1-p^{-D}\),

\[\frac{|\tilde{\nu}_{1}-\nu_{1}|}{\nu_{1}}\leq C_{*}\cdot\mathcal{E}_{2}(p) \cdot\widetilde{\rho}_{\lambda}(n,p), \frac{|\tilde{\nu}_{2}-\nu_{2}|}{\nu_{2}}\leq C_{*}\cdot\mathcal{E}_{2}(p) \cdot\widetilde{\rho}_{\lambda}(n,p).\]

This concludes the proof of this proposition. 

For any \(\kappa\geq 0\), denote \(\nu_{2}(\kappa)\in\mathbb{R}_{>0}\) the unique positive solution to

\[p-\frac{\kappa}{\nu_{2}(\kappa)}=\operatorname{Tr}\bigl{(}\bm{\Sigma}(\bm{ \Sigma}+\nu_{2}(\kappa))^{-1}\bigr{)}.\] (128)

We will further define analogously to Eq. (80) the truncated fixed point

\[p-\frac{\gamma(\kappa)}{\nu_{2,0}(\kappa)}=\operatorname{Tr}\bigl{(}\bm{ \Sigma}_{0}(\bm{\Sigma}_{0}+\nu_{2,0}(\kappa))^{-1}\bigr{)},\] (129)

where we recall that we denoted \(\gamma(\kappa)=\kappa+\operatorname{Tr}(\bm{\Sigma}_{+})\). It will be convenient to recall the notations

\[\widetilde{\Phi}_{2}(\bm{F};\kappa) =\frac{1}{p}\mathrm{Tr}\bigl{(}\bm{F}\bm{F}^{\mathsf{T}}(\bm{F} \bm{F}^{\mathsf{T}}+\kappa)^{-1}\bigr{)},\] \[\widetilde{\Phi}_{2}(\bm{F}_{0};\gamma(\kappa)) =\frac{1}{p}\mathrm{Tr}\bigl{(}\bm{F}_{0}\bm{F}_{0}^{\mathsf{T}}( \bm{F}_{0}\bm{F}_{0}^{\mathsf{T}}+\gamma(\kappa))^{-1}\bigr{)},\]

and the deterministic equivalents

\[\Psi_{2}(\nu_{2}(\kappa)) =\frac{1}{p}\mathrm{Tr}\bigl{(}\bm{\Sigma}(\bm{\Sigma}+\nu_{2}( \kappa))^{-1}\bigr{)},\] \[\Psi_{2}(\nu_{2,0}(\kappa)) =\frac{1}{p}\mathrm{Tr}\bigl{(}\bm{\Sigma}_{0}(\bm{\Sigma}_{0}+ \nu_{2,0}(\kappa))^{-1}\bigr{)}.\]

The next lemma show that \(\widetilde{\Phi}_{2}(\bm{F},\kappa)\) concentrates on \(\Psi_{2}(\nu_{2}(\kappa))\) uniformly on an interval of \(\kappa\).

**Lemma B.15**.: _Under the setting of Proposition B.4 and for any \(D,K\geq 0\), there exist constants \(\eta_{*}\in(0,1/4)\) and \(C_{*,D,K}>0\) such that the following holds. For any \(p\geq C_{*,D,K}\) and \(\lambda>0\), if it holds that_

\[\gamma_{\lambda}=\gamma(p\lambda/n)\geq p^{-K},\qquad\qquad\rho_{\gamma_{ \lambda}}(p)^{5/2}\log^{3/2}(p)\leq K\sqrt{p},\] (130)

_then with probability at least \(1-p^{-D}\), we have for any \(\kappa\in[p\lambda/n,p(p^{2}+\lambda)/n]\),_

\[\Bigl{|}\widetilde{\Phi}_{2}(\bm{F};\kappa)-\Psi_{2}(\nu_{2}(\kappa))\Bigr{|} \leq C_{*,D,K}\frac{\rho_{\gamma(\kappa)}(p)^{5/2}\log^{3}(p)}{\sqrt{p}}\Psi_ {2}(\nu_{2}(\kappa)).\] (131)

Proof of Lemma b.15.: Throughout the proof we assume that we are working on the event

\[\|\bm{F}_{+}\bm{F}_{+}^{\mathsf{T}}-\gamma(0)\mathbf{I}_{p}\|_{\mathrm{op}} \leq C_{*,D}\frac{\log^{3}(p)}{\sqrt{p}}\gamma(p\lambda/n),\]

which happens with probability at least \(1-p^{-D}\) by Lemma B.3 via union bound. Note that \(\gamma(\kappa)\geq\gamma(p\lambda/n)\) for all the \(\kappa\in[p\lambda/n,p(p^{2}+\lambda)/n]\). Furthermore, we assume that \(p\geq C_{*,D}\) chosen large enough so that \(\|\bm{F}_{+}\bm{F}_{+}^{\mathsf{T}}-\gamma(0)\mathbf{I}_{p}\|_{\mathrm{op}} \leq 1/2\cdot\gamma(p\lambda/n)\) so that

\[\bm{F}\bm{F}^{\mathsf{T}}+\kappa\succeq\frac{1}{2}\gamma(\kappa).\]The proof will proceed via a standard union bound argument over \(\kappa\) in the interval \([p\lambda/n,p(p^{2}+\lambda)/n]\). Let us first prove Eq. (131) for a fixed \(\kappa\). We first simplify the functional by rewriting it as

\[\widetilde{\Phi}_{2}(\bm{F};\kappa) =1-\frac{\kappa}{p}\mathrm{Tr}\big{(}(\bm{F}\bm{F}^{\mathsf{T}}+ \kappa)^{-1}\big{)}\] \[=1-\frac{\kappa}{p}\mathrm{Tr}\big{(}(\bm{F}_{0}\bm{F}_{0}^{ \mathsf{T}}+\gamma(\kappa))^{-1}\big{)}+\frac{\kappa}{p}\Delta,\]

where we denoted

\[|\Delta| =\Big{|}\mathrm{Tr}\big{(}(\bm{F}\bm{F}^{\mathsf{T}}+\kappa)^{-1} \big{)}-\mathrm{Tr}\big{(}(\bm{F}_{0}\bm{F}_{0}^{\mathsf{T}}+\gamma(\kappa))^ {-1}\big{)}\Big{|}\] \[=\Big{|}\mathrm{Tr}\big{(}(\bm{F}\bm{F}^{\mathsf{T}}+\kappa)^{-1 }(\bm{F}_{1}\bm{F}_{1}^{\mathsf{T}}-\gamma(0)\bm{\mathrm{I}}_{p})(\bm{F}_{0} \bm{F}_{0}^{\mathsf{T}}+\gamma(\kappa))^{-1}\big{)}\Big{|}\] \[\leq C_{*,D}\frac{\log^{3}(p)}{\sqrt{p}}\,\Big{|}\mathrm{Tr} \big{(}(\bm{F}_{0}\bm{F}_{0}^{\mathsf{T}}+\gamma(\kappa))^{-1}\big{)}\Big{|}\,.\]

Using again the above identity, we rewrite

\[\frac{1}{p}\mathrm{Tr}\big{(}(\bm{F}_{0}\bm{F}_{0}^{\mathsf{T}}+\gamma(\kappa) )^{-1}\big{)}=\frac{1}{\gamma(\kappa)}-\frac{1}{\gamma(\kappa)}\widetilde{ \Phi}_{2}(\bm{F}_{0};\gamma(\kappa)).\]

We can now apply Eq. (55) in Theorem A.2: under the assumption of Proposition B.4 and recalling the conditions (130), we have with probability at least \(1-p^{-D}\),

\[\Big{|}\widetilde{\Phi}_{2}(\bm{F}_{0};\gamma(\kappa))-\Psi_{2}(\nu_{2,0}( \kappa))\Big{|}\leq C_{*,D,K}\frac{\rho_{\gamma(\kappa)}(p)^{5/2}\log^{3/2}(p )}{\sqrt{p}}\Psi_{2}(\nu_{2,0}(\kappa)).\]

Combining the above displays, we obtain that with probability at least \(1-p^{-D}\)

\[\quad\Big{|}\widetilde{\Phi}_{2}(\bm{F};\kappa)-\Psi_{2}(\nu_{2, 0}(\kappa))-\frac{\mathrm{Tr}(\bm{\Sigma}_{+})}{p\nu_{2,0}}\Big{|}\] \[\leq\frac{\kappa}{\gamma(\kappa)}\,\Big{|}\widetilde{\Phi}_{2}( \bm{F}_{0};\gamma(\kappa))-\Psi_{2}(\nu_{2,0}(\kappa))\Big{|}+\kappa|\Delta|\] \[\leq C_{*,D,K}\frac{\rho_{\gamma(\kappa)}(p)^{5/2}\log^{3/2}(p)}{ \sqrt{p}}\Psi_{2}(\nu_{2,0}(\kappa))+C_{*,D}\frac{\log^{3}(p)}{\sqrt{p}}\, \bigg{|}\frac{\kappa}{p}\mathrm{Tr}\big{(}(\bm{F}_{0}\bm{F}_{0}^{\mathsf{T}}+ \gamma(\kappa))^{-1}\big{)}\bigg{|}\,,\]

where we used in the first inequality identity (129) to get

\[\left(1-\frac{\kappa}{\gamma(\kappa)}\right)\left(1-\frac{1}{p}\mathrm{Tr}( \bm{\Sigma}_{0}(\bm{\Sigma}_{0}+\nu_{2,0})^{-1})\right)=\frac{\mathrm{Tr}(\bm {\Sigma}_{+})}{p\nu_{2,0}}.\]

Further note that using condition (130), we can simplify the right-hand side

\[\Big{|}\kappa\mathrm{Tr}\big{(}(\bm{F}_{0}\bm{F}_{0}^{\mathsf{T} }+\gamma(\kappa))^{-1}\big{)}\Big{|} \leq\frac{\kappa}{\gamma(\kappa)}\,|1-\Psi_{2}(\nu_{2,0}(\kappa ))|+C_{*,D,K}\cdot K\cdot\Psi_{2}(\nu_{2,0}(\kappa))\] \[\leq C_{*,D,K}\left\{\Psi_{2}(\nu_{2,0}(\kappa))+\frac{\mathrm{Tr}( \bm{\Sigma}_{+})}{p\nu_{2,0}}\right\}.\]

We can now use Eq. (82) in Lemma B.5 applied to \(\nu_{2}(\kappa)\) and \(\nu_{2,0}(\kappa)\) to concluded that with probability at least \(1-p^{-D}\),

\[\Big{|}\widetilde{\Phi}_{2}(\bm{F};\kappa)-\Psi_{2}(\nu_{2}(\kappa))\Big{|} \leq\widetilde{\mathcal{E}}(p)\cdot\Psi_{2}(\nu_{2}(\kappa)),\qquad\widetilde{ \mathcal{E}}(p):=C_{*,D,K}\frac{\rho_{\gamma(\kappa)}(p)^{5/2}\log^{3}(p)}{ \sqrt{p}}.\] (132)

We now consider a \(p^{-P}\)-grid \(\mathcal{P}_{n}\) of the interval \(\kappa\in[p\lambda/n,p(p^{2}+\lambda)/n]\), which contains at most \(p^{P+3}\) points. We can use a union bound over \(\kappa\in\mathcal{P}_{n}\) and reparametrizing \(D^{\prime}=D+P+3\), so that with probability at least \(1-p^{-D}\), Equation (132) holds for any \(\kappa\in\mathcal{P}_{n}\). Then for any point \(\kappa_{1}\in[p\lambda/n,p(Kp^{2}+\lambda)/n]\), denote \(\kappa_{0}\in\mathcal{P}_{n}\) its closest point. Then by Lemma B.16 stated below, we have

\[\quad\Big{|}\widetilde{\Phi}_{2}(\bm{F};\kappa_{1})-\Psi_{2}(\nu_{ 2}(\kappa_{1}))\Big{|}\] \[\leq \,\Big{|}\widetilde{\Phi}_{2}(\bm{F};\kappa_{1})-\widetilde{\Phi} _{2}(\bm{F};\kappa_{0})\Big{|}+\Big{|}\widetilde{\Phi}_{2}(\bm{F};\kappa_{0})- \Psi_{2}(\nu_{2}(\kappa_{0}))\Big{|}+|\Psi_{2}(\nu_{2}(\kappa_{0}))-\Psi_{2}( \nu_{2}(\kappa_{1}))|\] \[\leq \,p^{CK}|\kappa_{1}-\kappa_{0}|\widetilde{\Phi}_{2}(\bm{F};\kappa_ {0})+\widetilde{\mathcal{E}}(p)\cdot\Psi_{2}(\nu_{2}(\kappa_{0}))+p^{CK}|\kappa_{ 1}-\kappa_{0}|\Psi_{2}(\nu_{2}(\kappa_{1}))\] \[\leq \,\Big{(}p^{CK-P}+\widetilde{\mathcal{E}}(p)\Big{)}\left(1+p^{CK-P }+\widetilde{\mathcal{E}}(p)\right)\cdot\Psi_{2}(\nu_{2}(\kappa_{1})).\]where we used that \(|\kappa_{1}-\kappa_{0}|\leq p^{-P}\). Taking \(P=CK+1\) concludes the proof. 

**Lemma B.16**.: _Under the setting of Proposition B.4 and for any \(D,K\geq 0\), there exist constants \(\eta_{*}\in(0,1/4)\), \(C>0\) and \(C_{*,D}>0\) such that the following holds. For any \(p\geq 1\) and \(\lambda>0\), it holds that_

\[\gamma(p\lambda/n)=\frac{p\lambda}{n}+\mathrm{Tr}(\mathbf{\Sigma}_{+})\geq p^{ -K},\] (133)

_then for any \(\kappa_{0},\kappa_{1}\geq p\lambda/n\), we have_

\[\left|\frac{\Psi_{2}(\nu_{2}(\kappa_{1}))}{\Psi_{2}(\nu_{2}(\kappa_{0}))}-1 \right|\leq p^{CK}|\kappa_{1}-\kappa_{0}|.\] (134)

_Furthermore, if \(p\geq C_{*,D}\), then we have with probability \(1-p^{-D}\) that for any \(\kappa_{1},\kappa_{2}\geq p\lambda/n\),_

\[\left|\frac{\widetilde{\Phi}_{2}(\bm{F};\kappa_{1})}{\widetilde{\Phi}_{2}(\bm {F};\kappa_{0})}-1\right|\leq p^{CK}|\kappa_{1}-\kappa_{0}|.\] (135)

Proof of Lemma b.16.: Using the identity (128), we can decompose the first difference into

\[\left|\frac{\Psi_{2}(\nu_{2}(\kappa_{1}))}{\Psi_{2}(\nu_{2}(\kappa _{0}))}-1\right| =\frac{\left|\frac{\kappa_{0}}{\nu_{2}(\kappa_{0})}-\frac{\kappa_ {1}}{\nu_{2}(\kappa_{1})}\right|}{p\Psi_{2}(\nu_{2}(\kappa_{0}))}\] \[\leq\frac{1}{\nu_{2}(\kappa_{0})\cdot p\Psi_{2}(\nu_{2}(\kappa_{0 }))}\left\{|\kappa_{1}-\kappa_{0}|+\left|\frac{\nu_{2}(\kappa_{0})}{\nu_{2}( \kappa_{1})}-1\right|\right\}.\]

From condition (133) and the proof of (Misiakiewicz and Saeed, 2024, Lemma 11), we have

\[\frac{1}{\nu_{2}(\kappa_{0})\cdot p\Psi_{2}(\nu_{2}(\kappa_{0}))} \leq p^{3+2K},\] \[\left|\frac{\nu_{2}(\kappa_{0})}{\nu_{2}(\kappa_{1})}-1\right| \leq p^{2+2K}|\kappa_{1}-\kappa_{0}|.\]

Combining the above inequalities, we deduce that there exists a constant \(C>0\) such that

\[\left|\frac{\Psi_{2}(\nu_{2}(\kappa_{1}))}{\Psi_{2}(\nu_{2}(\kappa_{0}))}-1 \right|\leq p^{CK}|\kappa_{1}-\kappa_{0}|.\]

For the second inequality (135), we rewrite the difference as

\[\left|\frac{\widetilde{\Phi}_{2}(\bm{F};\kappa_{1})}{\widetilde{ \Phi}_{2}(\bm{F};\kappa_{0})}-1\right| =\frac{\mathrm{Tr}(\bm{F}\bm{F}^{\mathsf{T}}(\bm{F}\bm{F}^{\mathsf{ T}}+\kappa_{1})^{-1}(\bm{F}\bm{F}^{\mathsf{T}}+\kappa_{0})^{-1})}{ \mathrm{Tr}(\bm{F}\bm{F}\bm{F}^{\mathsf{T}}+\kappa_{0})^{-1})}|\kappa_{1}- \kappa_{0}|\] \[\leq\|(\bm{F}\bm{F}^{\mathsf{T}}+\kappa_{1})^{-1}\|_{\mathrm{op}}| \kappa_{1}-\kappa_{0}|.\]

Hence, recalling Lemma B.3 and condition (133), for any \(p\geq C_{*,D}\), we get with probability at least \(1-p^{-D}\) and for all \(\kappa_{1},\kappa_{2}\geq p\lambda/n\),

\[\left|\frac{\widetilde{\Phi}_{2}(\bm{F};\kappa_{1})}{\widetilde{\Phi}_{2}(\bm{ F};\kappa_{0})}-1\right|\leq\frac{2}{\gamma(\kappa_{1})}|\kappa_{1}-\kappa_{0}| \leq n^{K}|\kappa_{1}-\kappa_{0}|,\]

which concludes the proof of this lemma. 

We consider \((\nu_{1}^{\varepsilon},\nu_{2}^{\varepsilon})\in\mathbb{R}_{\geq 0}\) the unique positive solutions of the perturbed equations:

\[n-\frac{\lambda}{\nu_{1}^{\varepsilon}} =\mathrm{Tr}\left(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{ \varepsilon})^{-1}\right)(1+\varepsilon),\] (136) \[p-\frac{p\nu_{1}^{\varepsilon}}{\nu_{2}^{\varepsilon}} =\mathrm{Tr}\left(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{ \varepsilon})^{-1}\right).\] (137)

**Lemma B.17**.: _Let \(\eta_{*}\in(0,1/4)\) be chosen as in Proposition B.4. Then there exists \(C_{*},C^{\prime}_{*}>0\) such that for any \(\varepsilon\in\mathbb{R}\) with_

\[|\varepsilon|\cdot C_{*}\cdot\widetilde{\rho}_{\lambda}(n,p)\leq\frac{1}{2},\]

_then_

\[\left|\frac{\nu_{1}^{\varepsilon}-\nu_{1}^{0}}{\nu_{1}^{0}}\right|\leq C^{ \prime}_{*}\cdot\widetilde{\rho}_{\lambda}(n,p)\cdot|\varepsilon|,\qquad \qquad\left|\frac{\nu_{2}^{\varepsilon}-\nu_{2}^{0}}{\nu_{2}^{0}}\right|\leq C ^{\prime}_{*}\cdot\widetilde{\rho}_{\lambda}(n,p)\cdot|\varepsilon|.\]

Proof of Lemma b.17.: For convenience, we introduce the notations

\[\delta_{1}:=\frac{\nu_{1}^{\varepsilon}-\nu_{1}^{0}}{\nu_{1}^{ \varepsilon}},\qquad\qquad\delta_{2}:=\frac{\nu_{2}^{\varepsilon}-\nu_{2}^{0 }}{\nu_{2}^{\varepsilon}}.\]

We first consider the second equation (137) and subtract the identities for \((\nu_{1}^{\varepsilon},\nu_{2}^{\varepsilon})\) and \((\nu_{1}^{0},\nu_{2}^{0})\) to obtain

\[p\left(\frac{\nu_{1}^{0}}{\nu_{2}^{0}}-\frac{\nu_{1}^{\varepsilon}}{\nu_{2}^{ \varepsilon}}\right)+\mathrm{Tr}\left(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{ 2}^{0})^{-1}\right)-\mathrm{Tr}\left(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^ {\varepsilon})^{-1}\right)\]

\[=p\left[\frac{\nu_{1}^{0}}{\nu_{2}^{0}}\delta_{2}-\frac{\nu_{1}^{\varepsilon}} {\nu_{2}^{\varepsilon}}\delta_{1}\right]+\delta_{2}\cdot\nu_{2}^{\varepsilon} \mathrm{Tr}(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{0})^{-1}(\mathbf{\Sigma }+\nu_{2}^{\varepsilon})^{-1})=0.\]

Hence, we obtain the first identity

\[\delta_{2}=\delta_{1}\frac{(\nu_{1}^{\varepsilon}/\nu_{2}^{ \varepsilon})}{(\nu_{1}^{0}/\nu_{2}^{0})+\frac{\nu_{2}^{\varepsilon}}{p} \mathrm{Tr}(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{0})^{-1}(\mathbf{\Sigma }+\nu_{2}^{\varepsilon})^{-1})}.\] (138)

We now turn to the first equation (136): we obtain similarly

\[\lambda\left(\frac{1}{\nu_{1}^{0}}-\frac{1}{\nu_{1}^{ \varepsilon}}\right)=\mathrm{Tr}\left(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{ 2}^{\varepsilon})^{-1}\right)(1+\varepsilon)-\mathrm{Tr}\left(\mathbf{\Sigma }(\mathbf{\Sigma}+\nu_{2}^{0})^{-1}\right)\] \[\Longrightarrow\ \frac{\lambda}{\nu_{1}^{0}}\delta_{1}=\delta_{2}\cdot \nu_{2}^{\varepsilon}\mathrm{Tr}(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{0}) ^{-1}(\mathbf{\Sigma}+\nu_{2}^{\varepsilon})^{-1})(1+\varepsilon)+ \varepsilon\mathrm{Tr}\left(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{0})^{-1 }\right).\]

Hence, rearranging the term and recalling the first identity (136), we get the second identity

\[\delta_{1}\left[\frac{\lambda}{\nu_{1}^{0}}+(1+\varepsilon)\frac{\nu_{2}^{ \varepsilon}\mathrm{Tr}(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{0})^{-1}( \mathbf{\Sigma}+\nu_{2}^{\varepsilon})^{-1})\cdot(\nu_{1}^{\varepsilon}/\nu_{ 2}^{\varepsilon})}{(\nu_{1}^{0}/\nu_{2}^{0})+\frac{\nu_{2}^{\varepsilon}}{p} \mathrm{Tr}(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{0})^{-1}(\mathbf{\Sigma }+\nu_{2}^{\varepsilon})^{-1})}\right]=\varepsilon\mathrm{Tr}\left(\mathbf{ \Sigma}(\mathbf{\Sigma}+\nu_{2}^{0})^{-1}\right).\] (139)

From this identity, we directly have

\[|\delta_{1}|\leq|\varepsilon|\cdot\frac{\nu_{1}^{0}}{\lambda}\mathrm{Tr}( \mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{0})^{-1}).\]

Note that for \(n\geq p/\eta_{*}\), we simply have by Eq. (136) that

\[\frac{\nu_{1}^{0}}{\lambda}\mathrm{Tr}(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2} ^{0})^{-1})=\frac{\mathrm{Tr}(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{0})^{ -1})}{n-\mathrm{Tr}(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{0})^{-1})}\leq \frac{p}{n-p}\leq\frac{\eta_{*}}{1-\eta_{*}},\]

where we use that \(\mathrm{Tr}(\mathbf{\Sigma}(\mathbf{\Sigma}+\nu_{2}^{0})^{-1})\leq p\) by Eq. (137). For \(n\leq p/\eta_{*}\), let's denote \(\mu_{1}^{0}=\lambda/\nu_{1}^{0}\). Rewriting Eq. (136), we get that

\[\mu_{*}=\frac{n}{1+\mathrm{Tr}(\mathbf{\Sigma}(\mu_{*}\mathbf{\Sigma}+\mu_{*} \nu_{2}^{0})^{-1})}.\]

Hence

\[\frac{\nu_{1}^{0}}{\lambda}\mathrm{Tr}(\mathbf{\Sigma}(\mathbf{ \Sigma}+\nu_{2}^{0})^{-1}) =\mathrm{Tr}(\mathbf{\Sigma}_{<\lfloor\eta_{*}\cdot n\rfloor}( \mu_{1}^{0}\mathbf{\Sigma}_{<\lfloor\eta_{*}\cdot n\rfloor}+\mu_{1}^{0}\nu_{2} ^{0})^{-1})+\mathrm{Tr}(\mathbf{\Sigma}_{\geq\lfloor\eta_{*}\cdot n\rfloor}( \mu_{1}^{0}\mathbf{\Sigma}_{\geq\lfloor\eta_{*}\cdot n\rfloor}+\mu_{1}^{0}\nu_{ 2}^{0})^{-1})\] \[\leq\frac{\eta_{*}\cdot n}{\mu_{1}^{0}}+\frac{\mathrm{Tr}(\mathbf{ \Sigma}_{\geq\lfloor\eta_{*}\cdot n\rfloor})}{\mu_{1}^{0}\nu_{2}^{0}}\] \[\leq\eta_{*}\left\{1+\mathrm{Tr}(\mathbf{\Sigma}(\mu_{*}\mathbf{ \Sigma}+\mu_{*}\nu_{2}^{0})^{-1})\right\}+\frac{\mathrm{Tr}(\mathbf{\Sigma}_{ \geq\lfloor\eta_{*}\cdot n\rfloor})}{\lambda},\]where we use in the last inequality that \(\nu_{1}^{0}/\nu_{2}^{0}\leq 1\) and the definition of the effective rank. Rearranging the terms we obtain

\[\frac{\nu_{1}^{0}}{\lambda}\mathrm{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2}^{0})^{-1 })\leq\frac{1}{1-\eta_{*}}\left\{1+\frac{\mathrm{Tr}(\bm{\Sigma}_{\geq\lfloor \eta_{*}\cdot n\rfloor})}{\lambda}\right\}.\]

Combining the above bounds we deduce that

\[|\delta_{1}|\leq C_{*}|\varepsilon|\cdot\left\{1+\mathbbm{1}_{n\leq p/\eta_{*} }\frac{\mathrm{Tr}(\bm{\Sigma}_{\geq\lfloor\eta_{*}\cdot n\rfloor})}{\lambda} \right\}\leq C_{*}\cdot\widetilde{\rho}_{\lambda}(n,p)\cdot|\varepsilon|.\]

By assumption \(C_{*}\cdot\widetilde{\rho}_{\lambda}(n,p)\cdot|\varepsilon|\leq 1/2\) and therefore \(\nu_{1}^{\varepsilon}\leq 2\nu_{1}^{0}\). We conclude the first inequality

\[\left|\frac{\nu_{1}^{\varepsilon}-\nu_{1}^{0}}{\nu_{1}^{0}}\right|\leq\frac{ \nu_{1}^{\varepsilon}}{\nu_{1}^{0}}|\delta_{1}|\leq C_{*}\cdot\widetilde{ \rho}_{\lambda}(n,p)\cdot|\varepsilon|.\]

Recalling Eq. (138), we have directly

\[|\delta_{2}|\leq|\delta_{1}|\frac{\nu_{1}^{\varepsilon}\nu_{2}^{0}}{\nu_{1}^ {0}\nu_{2}^{\varepsilon}}\quad\Longrightarrow\quad\left|\frac{\nu_{2}^{ \varepsilon}-\nu_{2}^{0}}{\nu_{2}^{0}}\right|\leq\left|\frac{\nu_{1}^{ \varepsilon}-\nu_{1}^{0}}{\nu_{1}^{0}}\right|,\]

which concludes the proof. 

#### b.7.3 Proof of deterministic equivalents for functionals on \(\mathbf{Z}\)

Proof of Proposition b.6.: Recall that \(\widehat{\rho}_{\lambda}(n)\) is defined in Eq. (72) and that for \(\bm{F}\in\mathcal{A}_{\mathcal{F}}\), we have \(\widehat{\rho}_{\lambda}(n)\leq C_{*,D,K}\widetilde{\rho}_{\lambda}(n,p)\) for all \(n\in\mathbb{N}\). Under the assumptions of Proposition b.6, we can apply Theorem A.2 to \(\bm{Z}\) conditional on \(\bm{F}\) to obtain that with probability at least \(1-n^{-D}\),

\[\left|\Phi_{2}(\bm{Z};\lambda)-\frac{p}{n}\widetilde{\Phi}_{2}(\bm {F};p\tilde{\nu}_{1})\right| \leq C_{*,D,K}\frac{\widetilde{\rho}_{\lambda}(n,p)^{5/2}\log^{ 3/2}(n)}{\sqrt{n}}\cdot\frac{p}{n}\widetilde{\Phi}_{2}(\bm{F};p\tilde{\nu}_{1}),\] \[\left|\Phi_{3}(\bm{Z};\bm{A},\lambda)-\left(\frac{n\tilde{\nu}_{ 1}}{\lambda}\right)^{2}\widetilde{\Phi}_{5}(\bm{F};\bm{A},p\tilde{\nu}_{1})\right| \leq C_{*,D,K}\frac{\widetilde{\rho}_{\lambda}(n,p)^{6}\log^{ 5/2}(n)}{\sqrt{n}}\cdot\left(\frac{n\tilde{\nu}_{1}}{\lambda}\right)^{2} \widetilde{\Phi}_{5}(\bm{F};\bm{A},p\tilde{\nu}_{1}),\] \[\left|\Phi_{4}(\bm{Z};\bm{A},\lambda)-\widetilde{\Phi}_{5}(\bm{F} ;\bm{A},p\tilde{\nu}_{1})\right| \leq C_{*,D,K}\frac{\widetilde{\rho}_{\lambda}(n,p)^{6}\log^{ 3/2}(n)}{\sqrt{n}}\cdot\widetilde{\Phi}_{5}(\bm{F};\bm{A},p\tilde{\nu}_{1}),\]

where \(\tilde{\nu}_{1}\) is the solution of the fixed point equation (77). We conclude the proof using Lemma B.18 stated below and that by condition (85), we have \(C_{*,D,K}\cdot\widetilde{\rho}_{\lambda}(n,p)\mathcal{E}_{\nu}(p)\leq C_{*,D,K }K\). 

Proof of Proposition b.7.: Again, under the assumptions of the proposition and for \(\bm{F}\in\mathcal{A}_{\mathcal{F}}\), we can apply (Misiakiewicz and Saeed, 2024, Lemma 10) to get

\[\left|\langle\bm{u},\bm{Z}(\bm{Z}^{\mathsf{T}}\bm{Z}+\lambda)^{-1}\widehat{ \bm{\Sigma}}_{\bm{F}}(\bm{Z}^{\mathsf{T}}\bm{Z}+\lambda)^{-1}\bm{Z}^{\mathsf{T}} \bm{u}\rangle-n\widetilde{\Phi}_{5}(\bm{F};\mathbf{I},p\tilde{\nu}_{1})\right| \leq C_{*,D,K}\frac{\widetilde{\rho}_{\lambda}(n,p)^{6}\log^{7/2}(n)}{\sqrt{n}},\]

\[\left|\langle\bm{u},\bm{Z}(\bm{Z}^{\mathsf{T}}\bm{Z}+\lambda)^{-1}\widehat{ \bm{\Sigma}}_{\bm{F}}(\bm{Z}^{\mathsf{T}}\bm{Z}+\lambda)^{-1}\bm{Z}^{\mathsf{T} }\bm{v}\rangle\right|\leq C_{*,D,K}\frac{\widetilde{\rho}_{\lambda}(n,p)^{6} \log^{7/2}(n)}{\sqrt{n}}\cdot\frac{n\nu_{1}}{\lambda}\sqrt{\widetilde{\Phi}_{5 }(\bm{F};\overline{\bm{v}}\bm{v}^{\mathsf{T}},p\tilde{\nu}_{1})}.\]

We conclude by combining these inequalities with Lemma B.18. 

**Lemma B.18**.: _For \(\bm{F}\in\mathcal{A}_{\mathcal{F}}\), we have_

\[\left|\widetilde{\Phi}_{2}(\bm{F};p\tilde{\nu}_{1})-\widetilde{\Phi}_{2}(\bm{F} ;p\nu_{1})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{\nu}(p)\cdot\widetilde{\Phi}_{2}(\bm{F};p \nu_{1}),\] (140)

\[\left|\widetilde{\Phi}_{5}(\bm{F};\bm{A},p\tilde{\nu}_{1})-\widetilde{\Phi}_{5}( \bm{F};\bm{A},p\nu_{1})\right| \leq C_{*,D,K}\cdot\widetilde{\rho}_{\lambda}(n,p)\mathcal{E}_{\nu}(p) \cdot\widetilde{\Phi}_{5}(\bm{F};\bm{A},p\nu_{1}).\] (141)

_and_

\[\left|\left(\frac{n\tilde{\nu}_{1}}{\lambda}\right)^{2}\widetilde{\Phi}_{5}(\bm {F};\bm{A},p\tilde{\nu}_{1})-\left(\frac{n\nu_{1}}{\lambda}\right)^{2} \widetilde{\Phi}_{5}(\bm{F};\bm{A},p\nu_{1})\right|\] (142) \[\leq C_{*,D,K}\cdot\widetilde{\rho}_{\lambda}(n,p)\mathcal{E}_{\nu }(p)\cdot\left(\frac{n\nu_{1}}{\lambda}\right)^{2}\widetilde{\Phi}_{5}(\bm{F};\bm{A },p\nu_{1}).\]Proof of Lemma b.18.: For convenience, we denote \(\kappa:=\nu_{1}\), \(\kappa^{\prime}:=\tilde{\nu}_{1}\), and \(\bm{\Gamma}:=\bm{\widehat{\Sigma}}_{\bm{F}}\). For Eq. (140), we simply use that

\[\left|p\widetilde{\Phi}_{2}(\bm{F};p\tilde{\nu}_{1})-p\widetilde{ \Phi}_{2}(\bm{F};p\nu_{1})\right| =\left|\mathrm{Tr}\left(\bm{\Gamma}(\bm{\Gamma}+\kappa^{\prime})^ {-1}\right)-\mathrm{Tr}\left(\bm{\Gamma}(\bm{\Gamma}+\kappa)^{-1}\right)\right|\] \[=\left|\kappa-\kappa^{\prime}\right|\mathrm{Tr}\left(\bm{\Gamma} (\bm{\Gamma}+\kappa^{\prime})^{-1}(\bm{\Gamma}+\kappa)^{-1}\right)\] \[\leq\frac{|\kappa^{\prime}-\kappa|}{\kappa^{\prime}}\mathrm{Tr} \left(\bm{\Gamma}(\bm{\Gamma}+\kappa)^{-1}\right)\] \[\leq C_{*,D,K}\cdot\mathcal{E}_{\nu}(p)\cdot p\widetilde{\Phi}_ {2}(\bm{F};p\nu_{1}).\]

Similarly, by simple algebra, we have

\[\left|\widetilde{\Phi}_{6}(\bm{F};\bm{A},p\tilde{\nu}_{1})- \widetilde{\Phi}_{6}(\bm{F};\bm{A},p\nu_{1})\right|\] \[=\left|\mathrm{Tr}\big{(}\bm{A}\bm{\Gamma}^{2}(\bm{\Gamma}+ \kappa^{\prime})^{-2}\big{)}-\mathrm{Tr}\big{(}\bm{A}\bm{\Gamma}^{2}(\bm{ \Gamma}+\kappa)^{-2}\big{)}\right|\] \[\leq 2|\kappa-\kappa^{\prime}|\mathrm{Tr}\big{(}\bm{A}\bm{\Gamma} ^{3}(\bm{\Gamma}+\kappa^{\prime})^{-2}(\bm{\Gamma}+\kappa)^{-2}\big{)}+|\kappa ^{2}-(\kappa^{\prime})^{2}|\mathrm{Tr}\big{(}\bm{A}\bm{\Gamma}^{3}(\bm{\Gamma }+\kappa^{\prime})^{-2}(\bm{\Gamma}+\kappa)^{-2}\big{)}\] \[\leq\frac{|\kappa-\kappa^{\prime}|}{\kappa^{\prime}}\left\{2+ \frac{|\kappa+\kappa^{\prime}|}{\kappa^{\prime}}\right\}\mathrm{Tr}\big{(} \bm{A}\bm{\Gamma}^{2}(\bm{\Gamma}+\kappa)^{-2}\big{)}\] \[\leq C_{*,D,K}\cdot\mathcal{E}_{\nu}(p)\cdot\widetilde{\Phi}_{6} (\bm{F};\bm{A},p\nu_{1}).\] (143)

Furthermore, note that by the identity (77),

\[\widetilde{\Phi}_{6}(\bm{F};\mathbf{I},p\tilde{\nu}_{1})\left(n- \widetilde{\Phi}_{6}(\bm{F};\mathbf{I},p\tilde{\nu}_{1})\right)^{-1}\leq \widetilde{\Phi}_{2}(\bm{F};p\tilde{\nu}_{1})\left(n-\widetilde{\Phi}_{2}(\bm{ F};p\tilde{\nu}_{1})\right)^{-1}=\frac{\tilde{\nu}_{1}}{\lambda}\widetilde{\Phi}_{2}( \bm{F};p\tilde{\nu}_{1}).\]

Furthermore, we have from the previous computation and simple algebra that

\[\frac{\tilde{\nu}_{1}}{\lambda}\widetilde{\Phi}_{2}(\bm{F};p\tilde{\nu}_{1}) \leq(1+C_{*,D,K}\mathcal{E}_{\nu}(p))\cdot\frac{\nu_{1}}{\lambda}\mathrm{Tr} (\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1})\leq C_{*,D,K}\cdot\widetilde{\rho}_{ \lambda}(n,p),\]

where we used the same argument as in the proof of Lemma B.17 in the last inequality as well as condition (85). The proof of Eqs. (142) and (141) from simple algebra from the above displays and (143). 

#### b.7.4 Proof of Lemma b.11

Proof of Lemma b.11.: For convenience, we introduce the notations

\[\bm{G}_{\bm{F}}:=(\bm{F}\bm{F}^{\mathsf{T}}+p\nu_{1})^{-1},\qquad\bm{G}_{0}:=( \bm{F}_{0}\bm{F}_{0}^{\mathsf{T}}+\gamma_{+})^{-1},\qquad\bm{R}_{0}:=(\bm{F}_{0 }^{\mathsf{T}}\bm{F}_{0}+\gamma_{+})^{-1},\] (144)

where we recall that we denoted \(\gamma_{+}=\gamma(p\nu_{1})=p\nu_{1}+\mathrm{Tr}(\bm{\Sigma}_{+})\). Recall that for the first approximation guarantee, we denote

\[\widetilde{\Phi}_{2}(\bm{F};p\nu_{1})=\frac{1}{p}\mathrm{Tr}(\bm{F}\bm{F}^{ \mathsf{T}}\bm{G}_{\bm{F}}),\qquad\qquad\widetilde{\Phi}_{2}(\bm{F}_{0};p\nu_{ 1})=\frac{1}{p}\mathrm{Tr}(\bm{F}_{0}\bm{F}_{0}^{\mathsf{T}}\bm{G}_{0}),\]

and the deterministic equivalents

\[\Psi_{2}(\nu_{2})=\frac{1}{p}\mathrm{Tr}(\bm{\Sigma}(\bm{\Sigma}+\nu_{2})^{-1 }),\qquad\quad\Psi_{2}(\nu_{2,0})=\frac{1}{p}\mathrm{Tr}(\bm{\Sigma}_{0}(\bm{ \Sigma}_{0}+\nu_{2,0})^{-1}).\]

For the second approximation guarantee, recall that we denote

\[\widetilde{\Phi}_{4}(\bm{F};\bm{\Sigma}^{-1},p\nu_{1})=\frac{1}{p}\mathrm{Tr}( \bm{F}\bm{F}^{\mathsf{T}}\bm{G}_{\bm{F}}^{2}),\qquad\qquad\widetilde{\Phi}_{4} (\bm{F};\bm{\Sigma}_{0}^{-1},p\nu_{1})=\frac{1}{p}\mathrm{Tr}(\bm{F}_{0}\bm{F}_{ 0}^{\mathsf{T}}\bm{G}_{0}^{2}),\]

and their associated deterministic equivalents

\[\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1})=\frac{1}{p}\cdot\frac{\mathrm{Tr}(\bm{ \Sigma}(\bm{\Sigma}+\nu_{2})^{-2})}{p-\mathrm{Tr}(\bm{\Sigma}^{2}(\bm{\Sigma}^{ 2}+\nu_{2})^{-2})},\qquad\quad\Psi_{3}(\nu_{2,0};\bm{\Sigma}_{0}^{-1})=\frac{1} {p}\cdot\frac{\mathrm{Tr}(\bm{\Sigma}_{0}(\bm{\Sigma}_{0}+\nu_{2,0})^{-2})}{p- \mathrm{Tr}(\bm{\Sigma}_{0}^{2}(\bm{\Sigma}_{0}^{2}+\nu_{2,0})^{-2})}.\]

We separate the analysis of the low-degree part \(\bm{F}_{0}\) from the high-degree part \(\bm{F}_{+}\). To remove the dependency on the high-degree part \(\bm{F}_{+}\), we recall that by Lemma B.3, we have with probability at least \(1-p^{-D}\)

\[\|\bm{\Delta}\|_{\mathrm{op}}\leq C_{*,D}\frac{\log^{3}(p)}{\sqrt{p}}\gamma_{+},\] (145)where we denoted \(\bm{\Delta}:=\bm{F}_{+}\bm{F}_{+}-\mathrm{Tr}(\bm{\Sigma}_{+})\cdot\bm{I}_{p}\) and we recall that \(\gamma_{+}=\gamma(p\nu_{1})=p\nu_{1}+\mathrm{Tr}(\bm{\Sigma}_{+})\). In particular, taking \(p\geq C_{*,D}\), we have \(\|\bm{\Delta}\|_{\mathrm{op}}\leq\frac{1}{2}\gamma_{+}\) and therefore

\[\|\bm{G}_{\bm{F}}\|_{\mathrm{op}}\leq 2\|\bm{G}_{0}\|_{\mathrm{op}}\leq\frac{2}{ \gamma_{+}}.\] (146)

**Step 1: Bound on \(|\widetilde{\Phi}_{2}(\bm{F};p\nu_{1})-\Psi_{2}(\nu_{2})|\).**

First note that we have the identity

\[\widetilde{\Phi}_{2}(\bm{F};p\nu_{1})=1-\nu_{1}\mathrm{Tr}(\bm{G}_{\bm{F}})=1 -\nu_{1}\mathrm{Tr}(\bm{G}_{0})+\nu_{1}\Theta,\]

where we denoted \(\Theta=\mathrm{Tr}(\bm{G}_{0})-\mathrm{Tr}(\bm{G}_{\bm{F}})\). By Eqs. (145) and (146), we have

\[|\Theta|=|\mathrm{Tr}(\bm{G}_{\bm{F}}\bm{\Delta}\bm{G}_{0})|\leq C_{*,D}\frac {\log^{3}(p)}{\sqrt{p}}\cdot\mathrm{Tr}(\bm{G}_{0}).\] (147)

Using again the above identity, we have

\[\frac{1}{p}\mathrm{Tr}(\bm{G}_{0})=\frac{1}{\gamma_{+}}-\frac{1}{\gamma_{+}} \widetilde{\Phi}_{2}(\bm{F}_{0};p\nu_{1}).\] (148)

Under the assumption of the lemma, we can apply Proposition B.8 and obtain with probability at least \(1-p^{-D}\) that

\[\left|\widetilde{\Phi}_{2}(\bm{F}_{0};p\nu_{1})-\Psi_{2}(\nu_{2,0})\right| \leq C_{*,D,K}\cdot\mathcal{E}_{3}(p)\cdot\Psi_{2}(\nu_{2,0}),\] (149)

where \(\mathcal{E}_{3}(p)\) is defined in Eq. (100). Furthermore note that using identity (80), we have

\[\frac{\mathrm{Tr}(\bm{\Sigma}_{+})}{\gamma_{+}}+\frac{p\nu_{1}}{\gamma_{+}} \Psi_{2}(\nu_{2,0})=\Psi_{2}(\nu_{2,0})+\frac{\mathrm{Tr}(\bm{\Sigma}_{+})}{p \nu_{2,0}},\]

and that by Eq. (82) in Lemma B.5,

\[\left|\Psi_{2}(\nu_{2,0})+\frac{\mathrm{Tr}(\bm{\Sigma}_{+})}{p\nu_{2,0}}- \Psi_{2}(\nu_{2})\right|\leq\frac{C}{p}\Psi_{2}(\nu_{2}).\] (150)

Thus combining Eqs. (147), (149) and (150), we obtain

\[|\widetilde{\Phi}_{2}(\bm{F};p\nu_{1})-\Psi_{2}(\nu_{2})| \leq\nu_{1}|\Theta|+\frac{p\nu_{1}}{\gamma_{+}}\left|\widetilde{ \Phi}_{2}(\bm{F}_{0};p\nu_{1})-\Psi_{2}(\nu_{2,0})\right|+\left|\Psi_{2}(\nu_ {2,0})+\frac{\mathrm{Tr}(\bm{\Sigma}_{+})}{p\nu_{2,0}}-\Psi_{2}(\nu_{2})\right.\] \[\leq C_{*,D,K}\left\{\frac{\log^{3}(p)}{\sqrt{p}}+\mathcal{E}_{3 }(p)+\frac{1}{p}\right\}\left[\nu_{1}\mathrm{Tr}(\bm{G}_{0})+\frac{p\nu_{1}}{ \gamma_{+}}\Psi_{2}(\nu_{2,0})+\Psi_{2}(\nu_{2})\right],\]

with probability at least \(1-p^{-D}\) via union bound. Using condition (106), we can simplify the right-hand side using identity (148) and bounds (149) and (150) to get

\[\kappa_{1}\mathrm{Tr}(\bm{G}_{0}) \leq\frac{p\nu_{1}}{\gamma_{+}}\left|1-\Psi_{2}(\nu_{2,0})\right| +C_{*,D,K}K\Psi_{2}(\nu_{2,0})\] \[\leq C_{*,D,K}\left\{\Psi_{2}(\nu_{2,0})+\frac{\mathrm{Tr}(\bm{ \Sigma}_{+})}{p\nu_{2,0}}\right\}\leq C_{*,D,K}\cdot\Psi_{2}(\nu_{2}).\]

This concludes the proof of the first part of this lemma.

**Step 2: Bound on \(|\widetilde{\Phi}_{4}(\bm{F};\bm{\Sigma}^{-1},p\nu_{1})-\Psi_{3}(\nu_{2};\bm{ \Sigma}^{-1})|\).**

We proceed similarly as in the first part and omit some repetitive details. First note that we can rewrite

\[\widetilde{\Phi}_{4}(\bm{F};\bm{\Sigma}^{-1},p\nu_{1}) =\frac{1}{p}\mathrm{Tr}(\bm{G}_{\bm{F}})-\nu_{1}\mathrm{Tr}(\bm{G }_{\bm{F}}^{2})\] \[=\frac{1}{p}\mathrm{Tr}(\bm{G}_{0})-\nu_{1}\mathrm{Tr}(\bm{G}_{0}^ {2})+\Theta\] \[=\frac{\mathrm{Tr}(\bm{\Sigma}_{+})}{\gamma_{+}^{2}}\left(1- \widetilde{\Phi}_{2}(\bm{F}_{0};p\nu_{1})\right)+\frac{p\nu_{1}}{\gamma_{+}} \widetilde{\Phi}_{4}(\bm{F}_{0};\bm{\Sigma}_{0}^{-1},p\nu_{1})+\Theta,\]where

\[\begin{split}|\Theta|&=\frac{1}{p}\left|\mathrm{Tr}( \bm{G_{F}})-\mathrm{Tr}(\bm{G}_{0})+p\nu_{1}\mathrm{Tr}(\bm{G}_{0}^{2})-p\nu_{ 1}\mathrm{Tr}(\bm{G}_{F}^{2})\right|\\ &\leq C_{*,D}\frac{\log^{3}(p)}{\sqrt{p}}\cdot\left[\frac{1}{p} \mathrm{Tr}(\bm{G}_{0})+\nu_{1}\mathrm{Tr}(\bm{G}_{0}^{2})\right].\end{split}\] (151)

Again, by Proposition B.8, we get that with probability at least \(1-p^{-D}\) that

\[\begin{split}\left|\widetilde{\Phi}_{2}(\bm{F}_{0};p\nu_{1})- \Psi_{2}(\nu_{2,0})\right|&\leq C_{*,D,K}\cdot\mathcal{E}_{3}(p) \cdot\Psi_{2}(\nu_{2,0}),\\ \left|\widetilde{\Phi}_{4}(\bm{F}_{0};\bm{\Sigma}_{0}^{-1},p\nu_{ 1})-\Psi_{3}(\nu_{2,0};\bm{\Sigma}_{0}^{-1})\right|&\leq C_{*,D, K}\cdot\mathcal{E}_{3}(p)\cdot\Psi_{3}(\nu_{2,0};\bm{\Sigma}_{0}^{-1}).\end{split}\] (152)

Furthermore, note that by Lemma B.19 stated below, we have

\[\left|\frac{\mathrm{Tr}(\bm{\Sigma}_{+})}{\gamma_{+}^{2}}\left(1-\Psi_{2}(\nu _{2,0})\right)+\frac{p\nu_{1}}{\gamma_{+}}\Psi_{3}(\nu_{2,0};\bm{\Sigma}_{0}^ {-1})-\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1})\right|\leq C\frac{\rho_{\gamma_{+}}( p)}{p}\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1}).\] (153)

Combining Eqs. (151), (152) and (153), we deduce via union bound that with probability at least \(1-p^{-D}\)

\[\begin{split}&|\widetilde{\Phi}_{4}(\bm{F};\bm{\Sigma}^{-1},p \nu_{1})-\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1})|\\ &\leq C_{*,D,K}\cdot\mathcal{E}_{3}(p)\left[\frac{1}{p}\mathrm{ Tr}(\bm{G}_{0})+\nu_{1}\mathrm{Tr}(\bm{G}_{0}^{2})+\frac{\mathrm{Tr}(\bm{ \Sigma}_{+})^{2}}{\gamma_{+}^{2}}\Psi_{2}(\nu_{2,0})+\frac{p\nu_{1}}{\gamma_{+ }}\Psi_{3}(\nu_{2,0};\bm{\Sigma}_{0}^{-1})+\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1}) \right].\end{split}\]

Let us simplify the right hand side. First, from the proof of Lemma B.19, we have

\[\frac{\mathrm{Tr}(\bm{\Sigma}_{+})^{2}}{\gamma_{+}^{2}}\Psi_{2}(\nu_{2,0})+ \frac{p\nu_{1}}{\gamma_{+}}\Psi_{3}(\nu_{2,0};\bm{\Sigma}_{0}^{-1})\leq C\rho _{\gamma_{+}}(p)\cdot\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1}).\]

Combining the above two displays with \(\mathcal{E}_{3}(p)\leq K\) from conditions (106) yields

\[\frac{1}{p}\mathrm{Tr}(\bm{G}_{0})-\nu_{1}\mathrm{Tr}(\bm{G}_{0}^{2})\leq C_ {*,D,K}\cdot\rho_{\gamma_{+}}(p)\cdot\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1}).\]

Finally, note that using Eq. (152) and again \(\mathcal{E}_{3}(p)\leq K\) that

\[\nu_{1}\mathrm{Tr}(\bm{G}_{0}^{2})\leq\frac{p\nu_{1}}{\gamma_{+}}\widetilde{ \Phi}_{4}(\bm{F};\bm{\Sigma}_{0}^{-1/2},p\nu_{1})\leq C_{*,D,K}\Psi_{3}(\nu_{2 };\bm{\Sigma}^{-1}),\]

which concludes the proof of this lemma. 

**Lemma B.19**.: _Assuming that \(p^{2}\xi_{m}^{2}\leq\gamma_{+}\), we have_

\[\left|\frac{\mathrm{Tr}(\bm{\Sigma}_{+})}{\gamma_{+}^{2}}\left(1-\Psi_{2}(\nu _{2,0})\right)+\frac{p\nu_{1}}{\gamma_{+}}\Psi_{3}(\nu_{2,0};\bm{\Sigma}_{0}^ {-1})-\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1})\right|\leq C\frac{\rho_{\gamma_{+}}( p)}{p}\Psi_{3}(\nu_{2};\bm{\Sigma}^{-1}).\]

Proof of Lemma b.19.: For convenience, we introduce

\[\Upsilon(\nu_{2,0}):=\frac{1}{p}\mathrm{Tr}(\bm{\Sigma}_{0}^{2}(\bm{\Sigma}_{ 0}+\nu_{2,0})^{-2}),\qquad\quad\Upsilon(\nu_{2}):=\frac{1}{p}\mathrm{Tr}(\bm{ \Sigma}^{2}(\bm{\Sigma}+\nu_{2})^{-2}).\]

Using that

\[\frac{1}{p}\mathrm{Tr}(\bm{\Sigma}_{0}(\bm{\Sigma}_{0}+\nu_{2,0})^{-2})=\frac {1}{\nu_{2,0}}\left\{\Psi_{2}(\nu_{2,0})-\Upsilon(\nu_{2,0})\right\},\]

we obtain by simple algebra and using identity (80) that

\[\frac{\mathrm{Tr}(\bm{\Sigma}_{+})}{\gamma_{+}^{2}}\left(1-\Psi_{2}(\nu_{2,0}) \right)+\frac{p\nu_{1}}{\gamma_{+}}\Psi_{3}(\nu_{2,0};\bm{\Sigma}_{0}^{-1})= \frac{1}{\nu_{2,0}}\frac{\Psi_{2}(\nu_{2,0})+\frac{\mathrm{Tr}(\bm{\Sigma}_{+})}{ p\nu_{2,0}}-\Upsilon(\nu_{2,0})}{p(1-\Upsilon(\nu_{2,0}))}.\]

Following the proof of (Misiakiewicz and Saeed, 2024, Lemma 7), we get that

\[|\Upsilon(\nu_{2,0})-\Upsilon(\nu_{2})|\leq 10\frac{p\xi_{m}^{2}}{\gamma_{+}}\leq \frac{C}{p},\]

\[(1-\Upsilon(\nu_{2,0}))^{-1}\leq(1-\Psi_{2}(\nu_{2,0}))^{-1}=\frac{p\nu_{2,0}}{ \gamma_{+}}\leq C\rho_{\gamma_{+}}(p).\]

Recalling Eq. (82) in Lemma B.5, we can conclude the proof using simple algebraic manipulations similarly to (Misiakiewicz and Saeed, 2024, Lemma 7).

[MISSING_PAGE_FAIL:44]

Hence, combining the previous two displays and using that \(\mathcal{E}_{3}(p)\leq K\) by conditions (114), we have

\[\left|\bm{\beta}_{0}^{\mathsf{T}}\widetilde{\bm{R}}_{00}\bm{\beta}_{0}-\nu_{2,0} \Psi_{1}(\nu_{2,0};\bm{A}_{0})\right|\leq C_{*,D,K}\cdot\left\{\frac{\log^{3}( p)}{\sqrt{p}}+\mathcal{E}_{3}(p)\right\}\nu_{2,0}\Psi_{1}(\nu_{2,0};\bm{A}_{0}).\] (154)

Similarly, denoting \(\widetilde{\bm{G}}_{0}=(\bm{F}_{0}\bm{F}_{0}^{\mathsf{T}}+p\nu_{1})^{-1}\), we can rewrite the third term as

\[\bm{\beta}_{+}^{\mathsf{T}}\widetilde{\bm{R}}_{++}\bm{\beta}_{+}=\bm{\beta}_{+ }^{\mathsf{T}}\left(\bm{F}_{+}^{\mathsf{T}}\widetilde{\bm{G}}_{0}\bm{F}_{+}+1 \right)^{-1}\bm{\beta}_{+}=\|\bm{\beta}_{+}\|_{2}^{2}-\Theta_{++},\]

where with probability at least \(1-p^{-D}\),

\[\Theta_{++} =\bm{\beta}_{+}^{\mathsf{T}}\bm{F}_{+}^{\mathsf{T}}\widetilde{G} _{0}^{1/2}\left(\widetilde{\bm{G}}_{0}^{1/2}\bm{F}_{+}\bm{F}_{+}^{\mathsf{T}} \widetilde{\bm{G}}_{0}^{1/2}+1\right)^{-1}\widetilde{\bm{G}}_{0}^{1/2}\bm{F}_ {+}\bm{\beta}_{+}\] (155) \[\leq\|(\bm{F}_{+}\bm{F}_{+}^{\mathsf{T}}+\widetilde{\bm{G}}_{0}^ {-1})^{-1}\|_{\mathrm{op}}\cdot\|\bm{F}_{+}\bm{\beta}_{+}\|_{2}^{2}\] \[\leq\frac{C}{\gamma_{+}}\cdot p\|\bm{\Sigma}_{+}^{1/2}\bm{\beta} _{+}\|_{2}^{2}\leq\frac{C}{p}\|\bm{\beta}_{+}\|_{2}^{2},\]

where we used the same concentration argument as in Step 3 of the proof of Lemma B.2.

Finally, we rewrite

\[\bm{\beta}_{0}^{\mathsf{T}}\widetilde{\bm{R}}_{0+}\bm{\beta}_{+}=-\bm{\beta}_ {0}^{\mathsf{T}}\left(\bm{F}_{0}^{\mathsf{T}}\widetilde{\bm{G}}_{+}\bm{F}_{0}+ 1\right)^{-1}\bm{F}_{0}^{\mathsf{T}}(\bm{F}_{+}\bm{F}_{+}^{\mathsf{T}}+p\nu_{ 1})^{-1}\bm{F}_{+}\bm{\beta}_{+}.\]

Hence, using Eqs. (154) and (155) as well as conditions (114), we obtain

\[|\bm{\beta}_{0}^{\mathsf{T}}\widetilde{\bm{R}}_{0+}\bm{\beta}_{+}|\leq C_{*,D, K}\cdot\frac{1}{\sqrt{p}}\left\{\nu_{2,0}\Psi_{1}(\nu_{2,0};\bm{A}_{0})+\|\bm{ \beta}_{+}\|_{2}^{2}\right\}.\] (156)

Finally, note that by Lemma B.20 stated below, we have

\[\left|\nu_{2,0}\langle\bm{\beta}_{0},(\bm{\Sigma}_{0}+\nu_{2,0})^{-1}\bm{ \beta}_{0}\rangle+\|\bm{\beta}_{+}\|_{2}^{2}-\nu_{2}\Psi_{1}(\nu_{2};\bm{A}_{ *})\right|\leq\frac{C}{p}\nu_{2}\Psi_{1}(\nu_{2};\bm{A}_{*}).\] (157)

Combining Eqs. (154), (155), (156), and (157), we deduce that with probability at least \(1-p^{-D}\),

\[\left|(p\nu_{1})\widetilde{\Phi}_{1}(\bm{F};\bm{A}_{*},p\nu_{1})-\nu_{2}\Psi_ {1}(\nu_{2};\bm{A}_{*})\right|\leq C_{*,D,K}\cdot\left\{\frac{\log^{3}(p)}{ \sqrt{p}}+\mathcal{E}_{3}(p)\right\}\cdot\nu_{2}\Psi_{1}(\nu_{2};\bm{A}_{*}).\] (158)

#### Step 2: Bounding term \(\widetilde{\Phi}_{4}(\bm{F};\bm{A}_{*},p\nu_{1})\).

We rewrite this term as

\[\langle\bm{\beta}_{*},\bm{R}\bm{F}^{\mathsf{T}}\bm{F}\bm{R}\bm{\beta}_{*} \rangle=\langle\bm{\beta}_{*},\bm{F}^{\mathsf{T}}\bm{G}_{0}^{2}\bm{F}\bm{\beta }_{*}\rangle+\Theta,\]

where

\[|\Theta| =\left|\langle\bm{\beta}_{*},\bm{F}^{\mathsf{T}}(\bm{G}_{\bm{F}}^ {2}-\bm{G}_{0}^{2})\bm{F}\bm{\beta}_{*}\rangle\right|\] \[=\left|\langle\bm{\beta}_{*},\bm{F}^{\mathsf{T}}\bm{G}_{0}\left( -2\bm{\Delta}\bm{G}_{\bm{F}}+\bm{\Delta}\bm{G}_{\bm{F}}^{2}\bm{\Delta}\right) \bm{G}_{0}\bm{F}\bm{\beta}_{*}\rangle\right|\leq C_{*,D,K}\frac{\log^{3}(d)}{p} \langle\bm{\beta}_{*},\bm{F}^{\mathsf{T}}\bm{G}_{0}^{2}\bm{F}\bm{\beta}_{*}\rangle.\] (159)

Let us decompose

\[\langle\bm{\beta}_{*},\bm{F}^{\mathsf{T}}\bm{G}_{0}^{2}\bm{F}\bm{\beta}_{*} \rangle=\langle\bm{\beta}_{0},\bm{F}_{0}^{\mathsf{T}}\bm{G}_{0}^{2}\bm{F}_{0} \bm{\beta}_{0}\rangle+2\langle\bm{\beta}_{+},\bm{F}_{+}^{\mathsf{T}}\bm{G}_{0}^ {2}\bm{F}_{0}\bm{\beta}_{0}\rangle+\langle\bm{\beta}_{+},\bm{F}_{+}^{\mathsf{T}} \bm{G}_{0}^{2}\bm{F}_{+}\bm{\beta}_{+}\rangle.\]

For the first term, we have directly from Proposition B.8 that

\[\left|\langle\bm{\beta}_{0},\bm{F}_{0}^{\mathsf{T}}\bm{G}_{0}^{2}\bm{F}_{0} \bm{\beta}_{0}\rangle-p\Psi_{3}(\nu_{2,0};\bm{A}_{0})\right|\leq C_{*,D,K} \cdot\mathcal{E}_{3}(p)\cdot p\Psi_{3}(\nu_{2,0};\bm{A}_{0}).\] (160)

For the two other terms, notice that they correspond to the terms (101) and (102) in Proposition B.9 with \(\bm{v}=\bm{\beta}_{0}\) and \(\bm{u}=\bm{F}_{+}\bm{\beta}_{+}\), where

\[\mathbb{E}[\bm{f}_{0,j}\langle\bm{f}_{+,j},\bm{\beta}_{+}\rangle]=0,\qquad\quad \mathbb{E}[u_{i}^{2}]=\|\bm{\Sigma}_{+}^{1/2}\bm{\beta}_{*}\|_{2}^{2}.\]Hence, by Proposition B.9, we have with probability at least \(1-p^{-D}\) that

\[\begin{split}&\left|\langle\bm{\beta}_{+},\bm{F}_{+}^{\top}\bm{G}_{ 0}^{2}\bm{F}_{+}\bm{\beta}_{+}\rangle-\frac{1}{\nu_{2,0}^{2}}\frac{\|\bm{ \Sigma}_{+}^{1/2}\bm{\beta}_{+}\|_{2}^{2}}{p-\operatorname{Tr}(\bm{\Sigma}_{0 }^{2}(\bm{\Sigma}_{0}+\nu_{2,0})^{-2})}\right|\leq C_{*,D,K}\cdot\mathcal{E}_{ 4}(p)\cdot\frac{p\|\bm{\Sigma}_{+}^{1/2}\bm{\beta}_{+}\|_{2}^{2}}{\gamma_{+}^{2 }},\\ &\left|\langle\bm{\beta}_{+},\bm{F}_{+}^{\top}\bm{G}_{0}^{2}\bm{F }_{0}\bm{\beta}_{0}\rangle\right|\leq C_{*,D,K}\cdot\mathcal{E}_{4}(p)\cdot\| \bm{\Sigma}_{+}^{1/2}\bm{\beta}_{+}\|_{2}\frac{p^{2}\nu_{2,0}}{\gamma_{+}^{2} }\sqrt{\Psi_{3}(\nu_{2,0};\bm{A}_{0})}.\end{split}\] (161)

Furthermore, by Lemma B.20 stated below

\[\left|\frac{\langle\bm{\beta}_{0},\bm{\Sigma}_{0}(\bm{\Sigma}_{0}+\nu_{2,0})^ {-2}\bm{\beta}_{0}\rangle+\langle\bm{\beta}_{+},\bm{\Sigma}_{+}\bm{\beta}_{+} \rangle/\nu_{2,0}^{2}}{p-\operatorname{Tr}(\bm{\Sigma}_{0}^{2}(\bm{\Sigma}_{0 }+\nu_{2,0})^{-2})}-p\Psi_{3}(\nu_{2};\bm{A}_{*})\right|\leq C\frac{\rho_{ \gamma_{+}}(p)}{p}\cdot p\Psi_{3}(\nu_{2};\bm{A}_{*}).\] (162)

Combining Eqs. (154), (155), (156), and (157), we deduce that with probability at least \(1-p^{-D}\),

\[\left|p\widetilde{\Phi}_{4}(\bm{F};\bm{A}_{*},p\nu_{1})-p\Psi_{3}(\nu_{2};\bm {A}_{*})\right|\leq C_{*,D,K}\cdot\rho_{\gamma_{+}}(p)^{2}\mathcal{E}_{4}(p) \cdot p\Psi_{3}(\nu_{2};\bm{A}_{*}).\] (163)

where we used that

\[\frac{p\nu_{2,0}}{\gamma_{+}}\sqrt{\frac{p\|\bm{\Sigma}_{+}^{1/2} \bm{\beta}_{*}\|_{2}^{2}}{\gamma_{+}^{2}}p\Psi_{3}(\nu_{2,0};\bm{A}_{0})}\leq C \rho_{\gamma_{+}}(p)\left\{\frac{\|\bm{\Sigma}_{+}^{1/2}\bm{\beta}_{*}\|_{2}^ {2}/\nu_{2,0}^{2}}{p-\operatorname{Tr}(\bm{\Sigma}_{0}^{2}(\bm{\Sigma}_{0}+ \nu_{2,0})^{-2})}+p\Psi_{3}(\nu_{2,0};\bm{A}_{0})\right\},\] \[\frac{p\|\bm{\Sigma}_{+}^{1/2}\bm{\beta}_{+}\|_{2}^{2}}{\gamma_{+ }^{2}}\leq C\rho_{\gamma_{+}}(p)^{2}\frac{\|\bm{\Sigma}_{+}^{1/2}\bm{\beta}_{* }\|_{2}^{2}/\nu_{2,0}^{2}}{p-\operatorname{Tr}(\bm{\Sigma}_{0}^{2}(\bm{\Sigma }_{0}+\nu_{2,0})^{-2})}.\]

#### Step 3: Combining the terms.

From Eqs. (158) and (163), we have with probability at least \(1-p^{-D}\) that

\[\begin{split}&\left|(p\nu_{1})^{2}\langle\bm{\beta}_{*},\bm{R}^{2} \bm{\beta}_{*}\rangle-\nu_{2}\Psi_{1}(\nu_{2};\bm{A}_{*})+(p\nu_{1})p\Psi_{3}( \nu_{2};\bm{A}_{*})\right|\\ &\leq C_{*,D,K}\cdot\rho_{\gamma_{+}}(p)^{2}\mathcal{E}_{4}(p) \cdot\left[\nu_{2}\Psi_{1}(\nu_{2};\bm{A}_{*})+(p\nu_{1})p\Psi_{3}(\nu_{2};\bm {A}_{*})\right].\end{split}\]

First, it is straightforward to verify that indeed

\[\nu_{2}\Psi_{1}(\nu_{2};\bm{A}_{*})-(p\nu_{1})p\Psi_{3}(\nu_{2};\bm{A}_{*})= \widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*},\lambda).\]

Then by Eq. (121) in Lemma B.14, we conclude that with probability at least \(1-p^{-D}\),

\[\left|(p\nu_{1})^{2}\langle\bm{\beta}_{*},\bm{R}^{2}\bm{\beta}_{*}\rangle- \widetilde{\mathsf{B}}_{n,p}(\bm{\beta}_{*},\lambda)\right|\leq C_{*,D,K}\cdot \rho_{\gamma_{+}}(p)^{2}\mathcal{E}_{4}(p)\cdot\widetilde{\mathsf{B}}_{n,p}( \bm{\beta}_{*},\lambda),\]

which concludes the proof of this lemma. 

**Lemma B.20**.: _Assuming that \(p^{2}\xi_{\eta}^{2}\leq\gamma_{+}\), we have_

\[\left|\nu_{2,0}\langle\bm{\beta}_{0},(\bm{\Sigma}_{0}+\nu_{2,0})^{-1}\bm{\beta}_ {0}\rangle+\|\bm{\beta}_{+}\|_{2}^{2}-\nu_{2}\Psi_{1}(\nu_{2};\bm{A}_{*})\right| \leq\frac{C}{p}\nu_{2}\Psi_{1}(\nu_{2};\bm{A}_{*}),\]

\[\left|\frac{\langle\bm{\beta}_{0},\bm{\Sigma}_{0}(\bm{\Sigma}_{0}+\nu_{2,0})^{-2 }\bm{\beta}_{0}\rangle+\langle\bm{\beta}_{+},\bm{\Sigma}_{+}\bm{\beta}_{+} \rangle/\nu_{2,0}^{2}}{p-\operatorname{Tr}(\bm{\Sigma}_{0}^{2}(\bm{\Sigma}_{0 }+\nu_{2,0})^{-2})}-p\Psi_{3}(\nu_{2};\bm{A}_{*})\right| \leq C\frac{\rho_{\gamma_{+}}(p)}{p}\cdot p\Psi_{3}(\nu_{2};\bm{A}_{*}).\]

Proof of Lemma b.20.: This lemma follows from the same arguments as in the proofs of Lemma B.5 and Lemma B.19. 

## Appendix C Details of the numerical illustration

In this section we provide further examples of comparison between the excess risk computed using the deterministic equivalent in Theorem 3.3 and numerical simulations, together with details about their realization. Results from numerical experiments are obtained averaging over 20-50 seeds. The data dimension is \(d=100\), with the exception of experiments involving real data (see Appendix C.4) and the ones realized considering the Gaussian design described in Appendix C.2.

### Self-consistent equations

To solve Equations 18 and 19 numerically, the following approach has been employed. From equation 19,

\[\sqrt{\left(1-\frac{n}{p}\right)^{2}+4\frac{\lambda}{p\nu_{2}}}=2\frac{\nu_{1}}{ \nu_{2}}-1+\frac{n}{p}.\] (164)

Substituting this expression in equation 18, we obtain

\[2\left(1-\frac{\nu_{1}}{\nu_{2}}\right)=\frac{2}{p}\mathrm{Tr}( \boldsymbol{\Sigma}(\boldsymbol{\Sigma}+\nu_{2})^{-1})\] (165) \[\stackrel{{\nu_{2}\geq 0}}{{\Longrightarrow}}\nu_{2}= \nu_{1}+\frac{\nu_{2}}{p}\mathrm{Tr}(\boldsymbol{\Sigma}(\boldsymbol{\Sigma}+ \nu_{2})^{-1}).\] (166)

Therefore, the parameters \(\nu_{1}\) and \(\nu_{2}\) have been computed by iterating

\[\nu_{1}^{t+1} =\frac{\nu_{2}^{t}}{2}\left[1-\frac{n}{p}+\sqrt{\left(1-\frac{n}{ p}\right)^{2}+4\frac{\lambda}{p\nu_{2}^{t}}}\right],\] (167) \[\nu_{2}^{t+1} =\nu_{1}^{t+1}+\frac{\nu_{2}^{t}}{p}\mathrm{Tr}(\boldsymbol{ \Sigma}(\boldsymbol{\Sigma}+\nu_{2}^{t})^{-1}),\] (168)

until a chosen tolerance \(\epsilon\) was reached.

### Gaussian design

Figures 3, 4 and 9 have been realized considering Gaussian design for the vectors in 'feature space' defined in Appendix B.1. In particular, fixed \(\boldsymbol{\Sigma}\) and \(\boldsymbol{\beta}_{*}\), we have drawn

\[\{\boldsymbol{g}_{i}\}_{i\in[n]}\sim_{\mathrm{i.i.d.}}\mathcal{N}(\boldsymbol{ 0},\boldsymbol{I}),\quad\{\boldsymbol{f}_{j}\}_{j\in[p]}\sim_{\mathrm{i.i.d.}} \mathcal{N}(\boldsymbol{0},\boldsymbol{\Sigma}),\]

and consequently \(\{y_{i}=\boldsymbol{\beta}_{*}^{\top}\boldsymbol{g}_{i}\}_{i\in[n]}\). Then the random feature estimator can be computed according to eqs. (61) and (62). In the figures produced with this setting, the elements of \(\boldsymbol{\beta}_{*}\) and \(\mathrm{diag}(\boldsymbol{\Sigma})\) follow power-laws truncated at the component \(10^{4}\).

### Empirical diagonalization

Whenever the data probability distribution \(\mu_{x}\) or the weights distribution \(\mu_{w}\) are unknown (e.g. in all cases involving real data), we estimated the matrix \(\boldsymbol{\Sigma}\) and the vector \(\boldsymbol{\beta}_{*}\) following the procedure described in this section and summarized in Algorithm 1. Consider a data set of \(N\) covariates

Figure 4: Relative difference between the excess risk (eq. (6)) of random features ridge regression from numerical simulation and its deterministic equivalent (Theorem 3.3), with regularization strength \(\lambda=0.1\), and noise variance \(\sigma_{\varepsilon}^{2}=0.1\). The relative error is \(O((n\wedge p)^{-1/2})\), in agreement with eq. (32). The simulations are made following the procedure described in appendix C.2, with \(\xi_{k}=k^{-1.2}\) and \(\beta_{*,k}=k^{-1.46}\); **(left)**\(p=3000\) fixed (right) \(n=3000\) fixed.

\(\{\bm{x}_{i}\}_{i\in[N]}\) drawn from \(\mu_{x}\) and \(N\) noiseless labels \(\{y_{i}=f_{*}(\bm{x}_{i})\}_{i\in[N]}\), and a set of \(P\) weights \(\{\bm{w}_{j}\}_{j\in[P]}\). In Figures 1, 5, 6, 7, 8, for which this procedure was used, we take both \(N,P=10^{4}\) (with the exception of Fig. 7 (right), where \(P=8000\)) and approximate \(\mu_{x}\) and \(\mu_{w}\) respectively with the empirical distributions \(\tilde{\mu}_{x}=\sum_{i=1}^{N}N^{-1}\delta(\bm{x}-\bm{x}_{i})\) and \(\tilde{\mu}_{w}=\sum_{j=1}^{P}P^{-1}\delta(\bm{w}-\bm{w}_{j})\). Then eq. (2) becomes

\[K(\bm{x},\bm{x}^{\prime})=\mathbb{E}_{\bm{w}\sim\mu_{w}}\left[ \varphi(\bm{x},\bm{w})\varphi(\bm{x}^{\prime},\bm{w})\right]=\sum_{j=1}^{P}P^{ -1}\varphi(\bm{x},\bm{w}_{j})\varphi(\bm{x}^{\prime},\bm{w}_{j}),\] (169)

and since eq. (11) implies \(\mathbb{E}_{\bm{x}}K(\bm{x},\bm{x}^{\prime})\psi_{k}(\bm{x})=\xi_{k}^{2}\psi_ {k}(\bm{x}^{\prime})\), defining the Gram matrix \(\bm{K}^{\text{emp}}\in\mathbb{R}^{N\times N}\) with elements \(\tilde{K}_{ii^{\prime}}=K(\bm{x}_{i},\bm{x}_{i^{\prime}})N^{-1}\) and the vectors \(\bm{\tilde{\psi}}^{k}=(\psi_{k}(\bm{x}_{1}),\ldots,\psi_{k}(\bm{x}_{N}))^{\top}\), we can write the following eigenvalue problems, for \(k\in[N]\):

\[\bm{\tilde{K}}\bm{\tilde{\psi}}_{k}=\tilde{\xi}_{k}^{2}\bm{\tilde{\psi}}_{k}.\] (170)

We then constructed the matrix \(\tilde{\bm{\Sigma}}=\mathrm{diag}(\tilde{\xi}_{1}^{2},\ldots,\tilde{\xi}_{N}^{2})\) and used it as an approximation of \(\bm{\Sigma}\). One should note that in this situation \(\mathbb{E}_{\bm{x}}\psi_{k}(\bm{x})\psi_{k^{\prime}}(\bm{x})=\delta_{kk^{ \prime}}\) corresponds to \(\bm{\tilde{\psi}}_{k}\bm{\tilde{\psi}}_{k^{\prime}}=N\delta_{kk^{\prime}}\).

Similarly, eq. (13) implies \(\beta_{*,k}=\mathbb{E}_{\bm{x}}\left[f_{*}(\bm{x})\psi_{k}(\bm{x})\right]\), which can be approximated by

\[\tilde{\beta}_{k}=N^{-1}\bm{y}^{\top}\bm{\tilde{\psi}}_{k}.\] (171)

```
0:\(\{\bm{x}_{i}\}_{i\in[N]}\sim_{\text{i.i.d.}}\mu_{x}\), \(\{y_{i}=f_{*}(\bm{x}_{i})\}_{i\in[N]}\), \(\{\bm{w}_{j}\}_{j\in[P]}\sim_{\text{i.i.d.}}\mu_{w}\)
0:\(\bm{\Sigma},\bm{\beta}_{*},\mathsf{R}_{n,p}(\bm{\beta}_{*},\lambda)\) for\(i,i^{\prime}\in\{1,\ldots,N\}\)do \(\tilde{K}_{ii^{\prime}}\leftarrow(NP)^{-1}\sum_{j=1}^{P}\varphi(\bm{x}_{i},\bm{w} _{j})\varphi(\bm{x}_{i^{\prime}},\bm{w}_{j})\) endfor \(\{(\tilde{\xi}_{k},\bm{\tilde{\psi}}_{k})_{k\in[N]}\}\leftarrow\mathrm{eig}( \bm{\tilde{K}})\)\(\triangleright\)\(\bm{\tilde{\psi}}_{k}\bm{\tilde{\psi}}_{k^{\prime}}\stackrel{{!}}{{=}}N\delta_{kk^{ \prime}}\) for\(k\in\{1,\ldots,N\}\)do \(\tilde{\beta}_{k}\gets N^{-1}\bm{y}^{\top}\bm{\tilde{\psi}}_{k}\) endfor \(\bm{\Sigma}\leftarrow\mathrm{diag}(\tilde{\xi}_{1},\ldots,\tilde{\xi}_{N})\) \(\bm{\beta}_{*}\leftarrow(\beta_{1},\ldots,\beta_{N})^{\top}\) Iterate eqs. (167-168) up to tolerance \(\epsilon\)  Compute the deterministic equivalent for the excess risk (22-24) ```

**Algorithm 1** Empirical diagonalization

### Real data

We performed numerical simulations sampling the training data from the MNIST data set Lecun et al. (1998) and the FashionMNIST data set Xiao et al. (2017), standardizing both covariates and labels, reshaping the images into vectors with \(d=748\). Results are shown in Figures 1 (right) and 7 (left).

### Trained network

In Figure 7 (right), we apply the procedure described in Appendix C.3 to the trained weights of a two layer neural network with hidden layer of size \(p\)

\[\hat{f}(\bm{x};\bm{W},\bm{a})=\frac{1}{\sqrt{p}}\sum_{j=1}^{p}a_{j}\varphi( \langle\bm{x},\bm{w}_{j}\rangle).\]

At initialization, the weights are randomly drawn; then, after sampling a training dataset \(\bm{X}_{\text{tr}}\in\mathbb{R}^{n_{\text{tr}}\times d}\), \(\bm{y}_{\text{tr}}\in\mathbb{R}^{n_{\text{tr}}}\), the weights of the first layer are trained using gradient descent, iterating for \(t=1,\ldots,T\) the following

\[\bm{W}_{t+1}=\bm{W}_{t}+\frac{\eta}{n_{\text{tr}}}\bm{X}_{\text{tr}}^{\top} \left[\left((\bm{y}_{\text{tr}}-\hat{f}(\langle\bm{X}_{\text{tr}};\bm{W}_{t}, \bm{a}\rangle)^{\top}\bm{a}\right)\odot\varphi^{\prime}(\bm{X}_{\text{tr}}\bm{W }_{t}^{\top})\right],\] (172)

where \(\eta\) is the learning rate, \(\odot\) is the Hadamard product, \(\hat{f}\) and \(\varphi\) are applied component-wise; finally, the weights of the second layer are minimized using ridge regression, as in eq. (5).

Figure 5: Excess risk eq. (6) of random features ridge regression. Solid lines are obtained from the deterministic equivalent in Theorem 3.3, and points are numerical simulations, with the different curves denoting different regularization strengths \(\lambda\geq 0\). Training data \((\bm{x}_{i},y_{i})_{i\in[n]}\), sampled from a teacher-student model \(y_{i}=\tanh(\langle\beta,\bm{x}_{i}\rangle)+\varepsilon_{i}\), \(\sigma_{\varepsilon}^{2}=0.1\), with random feature map \(\varphi(\bm{x},\bm{w})=\mathrm{ReLU}(\langle\bm{w},\bm{x}\rangle)\). Both covariates \(\{\bm{x}_{i}\}\) and weights \(\{\bm{w}_{i}\}\) are uniformly sampled from the \(d\)-dimensional spheres respectively with radius \(\sqrt{d}\) and \(1\). (**Left**) Excess risk as a function of \(n\), with \(p=600\) fixed. (**Right**) Excess risk as a function of \(p\), with \(n=300\) fixed.

Figure 6: Excess risk eq. (6) of random features ridge regression. Solid lines are obtained from the deterministic equivalent in Theorem 3.3, and points are numerical simulations, with the different curves denoting different regularization strengths \(\lambda\geq 0\). Training data \((\bm{x}_{i},y_{i})_{i\in[n]}\), sampled from a teacher-student model \(y_{i}=\tanh(\langle\beta,\bm{x}_{i}\rangle)+\varepsilon_{i}\), \(\sigma_{\varepsilon}^{2}=0.1\), \(\bm{x}_{i}\sim_{\text{i.i.d.}}\mathcal{N}(0,\bm{I}_{d})\), with a spiked random feature map \(\varphi(\bm{x},\bm{w})=\mathrm{erf}(\langle\bm{w}+u\bm{v},\bm{x}\rangle)\) where \(\bm{w}\sim\mathcal{N}(0,d^{-1}\bm{I}_{d})\), \(\bm{v}\in\mathbb{R}^{d}\sim\mathcal{N}(0,d^{-1}\bm{I}_{d})\), and \(u\sim\mathcal{N}(0,1)\). (**Left**) Excess risk as a function of \(n\), with \(p=500\) fixed. (**Right**) Excess risk as a function of \(p\), with \(n=300\) fixed.

[MISSING_PAGE_EMPTY:50]

[MISSING_PAGE_EMPTY:51]

allowing us to use (173).

In conclusion, as \(n\to\infty\),

\[\nu_{1} \approx\begin{cases}O\left(n^{-\alpha}\right),\quad\text{for $q>1$ and $\ell>\alpha$},\\ n^{-\ell},\quad\text{otherwise}\end{cases}\] (176) \[\nu_{2} \approx O\left(n^{-\alpha\left(1\wedge q\wedge\nicefrac{{\ell}}{{ \alpha}}\right)}\right),\] (177)

in particular

\[\frac{\nu_{1}}{\nu_{2}}=\begin{cases}1-O\left(\nu_{2}^{-\nicefrac{{1}}{{ \alpha}}}n^{-q}\right),\quad\text{for $q>1\wedge\nicefrac{{\ell}}{{ \alpha}}$}\\ O\left(n^{-\ell+\alpha q}\right)=o(1)\quad\text{otherwise}\end{cases}.\] (178)

### Variance term

Considering the results (176-178), we can write eq. (20) as

\[\Upsilon(\nu_{1},\nu_{2}) =\frac{p}{n}\left[\left(1-\frac{\nu_{1}}{\nu_{2}}\right)^{2}+ \left(\frac{\nu_{1}}{\nu_{2}}\right)^{2}\frac{T_{22}^{0}(\nu_{2})}{p-T_{22}^ {0}(\nu_{2})}\right]\] (179) \[=\begin{cases}n^{-1}O(\nu_{2}^{-\nicefrac{{1}}{{\alpha}}})=O \left(n^{-\left(1-\left(1\wedge\nicefrac{{1}}{{\alpha}}\right)\right)}\right),\quad\text{for $q>1\wedge\nicefrac{{\ell}}{{\alpha}}$}\\ n^{-\left(1-q\right)}(1+o(1))\quad\text{otherwise}\end{cases}\] (180)

One could notice, using the integral approximation of the Riemann sum \(T_{22}^{0}(\nu_{2})\) given in (173), that \(1-\Upsilon(\nu_{1},\nu_{2})=O(1)\) for any choice of \(\ell\) and \(q\). Hence, the variance term given by (23) decays with \(n\) with rate

\[\gamma_{\mathcal{V}}(\ell,q)=1-\left(\frac{\ell}{\alpha}\wedge q \wedge 1\right).\]

### Bias term

Using again (176-178), we can compute the rate of \(\chi(\nu_{2})\) defined in eq. (21), as \(n\to\infty\):

\[\chi(\nu_{2})=\frac{T_{12}^{0}(\nu_{2})}{p-T_{22}^{0}(\nu_{2}))} =n^{-q}O\left(\nu_{2}^{-1-\nicefrac{{1}}{{\alpha}}}\right)\left( 1+n^{-q}O\left(\nu_{2}^{-\nicefrac{{1}}{{\alpha}}}\right)\right)\] \[=n^{-q}O\left(\nu_{2}^{-1-\nicefrac{{1}}{{\alpha}}}\right)\]

Using the integral approximation given in (173), one could verify that \(p-T_{22}^{0}(\nu_{2})=O(p)\) for any choice of \(\ell\) and \(q\).

The deterministic equivalent for the bias term, given in eq. (22), can be written as

\[\mathsf{B}_{n,p}(\bm{\beta}_{*},\lambda) =\frac{\nu_{2}^{2}}{1-\Upsilon(\nu_{1},\nu_{2})}\left(T_{2r,2}^{1 }(\nu_{2})+\chi(\nu_{2})T_{2r+1,2}^{1}(\nu_{2})\right)\] (181) \[=O\left(\nu_{2}^{2}\right)O\left(\nu_{2}^{2(r-1\wedge 0)}+n^{-q} \nu_{2}^{-1-\nicefrac{{1}}{{\alpha}}+(2r-1)\wedge 0}\right)\] (182) \[=O\left(\nu_{2}^{2(r\wedge 1)}+n^{-q}\nu_{2}^{-\nicefrac{{1}}{{ \alpha}}+2(r\wedge\nicefrac{{1}}{{\alpha}})}\right)\] (183)

where we have used (175) to compute the scalings of the terms \(T^{1}\delta\gamma\) and the fact that \(1-\Upsilon(\nu_{1},\nu_{2})=O(1)\).

From eq. (183), and using the result (177), it is straightforward to see that the decay rate of the bias term is given by

\[\gamma_{\mathcal{B}}=\left[2\alpha\left(\frac{\ell}{\alpha}\wedge q\wedge 1 \right)(r\wedge 1)\right]\wedge\left[\left(2\alpha\left(r\wedge\frac{1}{2} \right)-1\right)\left(\frac{\ell}{\alpha}\wedge q\wedge 1\right)+q\right].\] (184)

Examples of the results of Theorem 4.1 and Corollary 4.2 are shown in Fig. 3 and 9.

### Details of Remark 4.1

In order to extend the results of Theorem 4.1 and Corollary 4.2 to the excess risk defined in (6), in this section we compute the intervals for \(\ell\) and \(q\) such that the assumptions of Theorem 3.3 hold and the approximation rates \(\mathcal{E}(n,p)\) are vanishing, under source and capacity conditions.

Given \(n\), \(p=n^{q}\), \(\lambda^{-(\ell-1)}\) and \(\nu_{2}\) as in (177) assumption 3.2 is verified for \(\mathrm{m}=n^{q+\ell}\) and \(\mathsf{C}_{*}=O(\nu_{2}^{-1})\). In fact

\[\sum_{k=\mathrm{m}+1}^{\infty}\xi_{k}^{2}>\int_{\mathrm{m}+1}^{ \infty}x^{-\alpha}=\frac{(\mathrm{m}+1)^{1-\alpha}}{\alpha-1}\] (185)

and the inequality in (16) holds if

\[n^{2q}(\mathrm{m}+1)^{-\alpha}\leq n^{q-\ell}\frac{(\mathrm{m}+1 )^{1-\alpha}}{\alpha-1}\implies\mathrm{m}\geq n^{q+\ell}(\alpha-1)-1.\] (186)

The inequalities in (17) can be written as

\[\mathsf{C}_{*} \geq\frac{T_{11}^{0}(\nu_{2})}{T_{22}^{0}(\nu_{2})}=O(1),\] (187) \[\mathsf{C}_{*} \geq\frac{T_{2r,1}^{1}(\nu_{2})}{\nu_{2}T_{2r,2}^{1}(\nu_{2})}=O \left(\nu_{2}^{-((0\vee(2r-1))\wedge 1)}\right).\] (188)

Figure 9: Excess risk eq. (6) of random features ridge regression as a function of the number of samples \(n\) under source and capacity conditions eq. (37) and power-law assumptions \(\lambda=n^{-(\ell-1)}\), \(p=n^{q}\), with noise variance \(\sigma_{\varepsilon}^{2}=1\), obtained from the deterministic equivalent Theorem 3.3. Dashed and dotted lines are the analytical rates from Theorem 4.1, stated in the legend. The colour scheme is the following: variance dominated region: orange and brown for the slow decay regime; cyan for the bias dominated region; shades of green for the optimal decay (red lines in Fig. 2 (right). In particular we show: **(left)** the crossover between the orange and teal regions in Fig. 2 at fixed regularization and \(r<\nicefrac{{1}}{{2}}\); **(right)** the optimal decay rate along the horizontal red line line in Fig. 2 at \(q=q_{*}\) and \(r<\nicefrac{{1}}{{2}}\), for any \(\lambda\leq\lambda_{*}\), included the non regularized case.

Then, introducing \(\eta_{*}\in(0,\nicefrac{{1}}{{2}})\), we can compute the following quantities of interest (introduced in eqs. (25) to (27)):

\[r_{\boldsymbol{\Sigma}}(\lfloor\eta_{*}\cdot k\rfloor) =\frac{\operatorname{Tr}(\boldsymbol{\Sigma}_{\geq\lfloor\eta_{*} \cdot k\rfloor})}{||\boldsymbol{\Sigma}_{\geq\lfloor\eta_{*}\cdot k\rfloor}||_{ \operatorname{op}}}=O(\lfloor\eta_{*}\cdot k\rfloor)=O(k)\] (189) \[M_{\boldsymbol{\Sigma}}(k) \coloneqq 1+\frac{r_{\boldsymbol{\Sigma}}(\lfloor\eta_{*}\cdot k \rfloor)\lor k}{k}\log\left(r_{\boldsymbol{\Sigma}}(\lfloor\eta_{*}\cdot k \rfloor)\lor k\right)=1+O(\log k),\] (190) \[\rho_{\kappa}(p) \coloneqq 1+\frac{p\cdot\xi_{\lfloor\eta_{*}p\rfloor}^{2}}{\kappa}M_{ \boldsymbol{\Sigma}}(p)=1+O\left(\frac{n^{q(1-\alpha)}}{\kappa}\log n\right),\] (191) \[\widetilde{\rho}_{\kappa}(n,p) \coloneqq 1+\mathbb{1}[n\leq p/\eta_{*}]\cdot\left\{\frac{n\xi_{ \lfloor\eta_{*}n\rfloor}^{2}}{\kappa}+\frac{n}{p}\cdot\rho_{\kappa}(p) \right\}M_{\boldsymbol{\Sigma}}(n)\] (192) \[\stackrel{{ n\geq\eta_{*}^{1/(1-q)}}}{{=}}1+\mathbb{1 }[q\geq 1]O\left(\frac{n^{1-\alpha}}{\kappa}\log n\right).\] (193)

Similarly, considering \(\nu_{1}\) scaling as in eq. (176), we have that eq. (31)

\[\gamma_{\lambda} =\frac{p\lambda}{n}+\sum_{k=m+1}^{\infty}\xi_{k}^{2}=O\left(n^{q- \ell}+n^{(q+\ell)(1-\alpha)}\right),\] (194) \[\gamma_{+} =p\nu_{1}+\sum_{k=m+1}^{\infty}\xi_{k}^{2}=O\left(\mathbb{1}[q \geq 1]n^{q-(\ell\wedge\alpha)}+\mathbb{1}[q<1]n^{q-\ell}+n^{(q+\ell)(1- \alpha)}\right)\] (195) \[=O\left(\mathbb{1}[q\geq 1]n^{q-(\ell\wedge\alpha)}+\mathbb{1} \left[\frac{\ell}{\alpha}(2-\alpha)\leq q<1\right]n^{q-\ell}+1\left[q<\frac{ \ell}{\alpha}(2-\alpha)\right]n^{(q+\ell)(1-\alpha)}\right).\] (196)

The last step is a consequence of

\[q-(\ell\wedge\alpha)>(q+\ell)(1-\alpha) \implies q>\frac{\ell}{\alpha}\underbrace{(1-\alpha)}_{<0}+ \left(\frac{\ell}{\alpha}\wedge 1\right),\] (197) \[q-\ell>(q+\ell)(1-\alpha) \implies q>\frac{\ell}{\alpha}(2-\alpha)\] (198)

Fixing \(K>0\) and considering the, we consider condition (28):

\[\lambda\geq n^{-K} \iff\ell\leq 1+K,\] (199) \[\gamma_{\lambda}\geq p^{-K} \iff\begin{cases}\ell\leq q(1+K)&\lor\left\{\ell\leq\frac{q(1+K- \alpha)}{\alpha-1}\right.\\ q>\frac{(2-a)}{a}\ell&\lor\left\{q<\frac{(2-a)}{a}\ell\right.\end{cases}\quad,\] (200) \[\widetilde{\rho}_{\lambda}(n,p)^{5/2}\cdot\log^{3/2}(n)\leq\,K \sqrt{n} \iff\ell>\mathbb{1}[q\geq 1]\left(\alpha+\frac{1}{5}\right),\] (201)

and, similarly, condition (29) is satisfied if, for \(q\geq 1\)

\[\left(1+O\left(n^{\ell-\alpha}\log n\right)\right)^{2}\left(O\left(1+n^{(\ell \wedge\alpha)-q\alpha}\log n\right)\right)^{8}q\log^{4}n\leq Kn^{\nicefrac{{ 3}}{{2}}}\] (202) \[\implies 2(\ell-\alpha)\lor 0<\frac{q}{2}\implies\ell<\frac{q}{4}+\alpha,\] (203)

while, for \(q<1\), if

\[\begin{cases}1>q\geq\frac{\ell}{\alpha}(2-\alpha)&\lor\left\{q<\frac{\ell}{ \alpha}(2-\alpha)\right.\\ \left(1+O\left(n^{\ell-q\alpha}\right)\right)^{8}q\log^{4}n\leq Kn^{\nicefrac{{ 3}}{{2}}}&\lor\left\{q<\frac{\ell}{\alpha}(2-\alpha)\right.\\ \left(1+O\left(n^{\ell(\alpha-1)}\right)\right)^{8}q\log^{4}n\leq Kn^{ \nicefrac{{3}}{{2}}}\end{cases}\] (204)

\[\implies\left\{\begin{matrix}1>q\geq\frac{\ell}{\alpha}(2-\alpha)& \lor\left\{q<\frac{\ell}{\alpha}(2-\alpha)\right.\\ \ell<\frac{q}{16(\alpha-1)}\end{matrix}\right.\\ \left.\left(205\right)\right.\\ \left.\left(206\right)\right.\\ \left.\left(207\right)\right.\\ \left.\left(208\right)\right.\\ \left.\left(209\right)\right.\\ \left.\left(210\right)\right.\\ \left.\left(220\right)\right.\\ \left.\left(209\right)\right.\\ \left.\left(211\right)\right.\\ \left.\left(221\right)\right.\\ \left.\left(222\right)\right.\\ \left.\left(230\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(241\right)\right.\\ \left.\left(242\right)\right.\\ \left.\left(243\right)\right.\\ \left.\left(244\right)\right.\\ \left.\left(244\right)\right.\\ \left.\left(245\right)\right.\\ \left.\left(246\right)\right.\\ \left.\left(247\right)\right.\\ \left.\left(248\right)\right.\\ \left.\left(249\right)\right.\\ \left.\left(249\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(241\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(242\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(241\right)\right.\\ \left.\left(242\right)\right.\\ \left.\left(243\right)\right.\\ \left.\left(244\right)\right.\\ \left.\left(245\right)\right.\\ \left.\left(246\right)\right.\\ \left.\left(247\right)\right.\\ \left.\left(248\right)\right.\\ \left.\left(249\right)\right.\\ \left.\left(249\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(241\right)\right.\\ \left.\left(242\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(240\right)\right.\\ \left.\left(241\right)\right.\\ \left.\left(242\right)\right.\\ \left.\left(243\right)\right.\\ \left.\left(244\right)\right.\\ \left.\left(245\right)\right.\\ \left.\left(246\right)\right.\\ \left.\left(247\right)\right.\\ \left.\left(248\right)\right.\\where the last step is a consequence of

\[\frac{\alpha}{2-\alpha}\leq\alpha+\frac{1}{16}\leq\frac{1}{16(\alpha -1)}, \text{for }\alpha\geq\overline{\alpha}\coloneqq\frac{15+\sqrt{353}}{32} \approx 1.05588,\] (207) \[\frac{\alpha}{2-\alpha}>\alpha+\frac{1}{16}>\frac{1}{16(\alpha-1)}, \text{for }\alpha<\overline{\alpha}.\] (208)

Finally, the approximation rate defined in remark 4.1 is

\[\mathcal{E}(n,p)= \left(n^{-\nicefrac{{1}}{{2}}}+\mathbbm{1}[q\geq 1]\tilde{O} \left(n^{6(\ell-\alpha)-\nicefrac{{1}}{{2}}}\right)\right)+\] (209) \[\left(n^{-\nicefrac{{q}}{{2}}}+\mathbbm{1}[q\geq 1]\tilde{O} \left(n^{2(\ell-\alpha)-\nicefrac{{q}}{{2}}}\right)\right)\left(1+\tilde{O} \left(\frac{n^{8q(1-\alpha)}}{\gamma_{+}^{8}}\right)\right),\] (210)

where the second term vanishes under the conditions in (203) and (206), and the first term vanishes by further assuming, for \(q\geq 1\)

\[\ell<\alpha+\frac{1}{12}.\] (211)

## Appendix E Comparison with neural scaling laws

In this appendix we discuss the relationship between our results and the recent literature of the theory of neural scaling laws with linear models. We adopt a notation close to ours, with dictionary to their notation given in Table 1 and Table 2.

Bahri et al. (2024) and Maloney et al. (2022) have considered a model where with Gaussian input data and linear target function:

\[f_{\star}(\bm{x}_{i}) =\langle\bm{\beta}_{\star},\bm{x}_{i}\rangle, \bm{x}_{i}\sim\mathcal{N}(0,\bm{\Lambda}), i\in[n]\] (212)

The covariance matrix \(\bm{\Lambda}=\mathrm{diag}(\lambda_{k})_{k\in[d]}\) is taken to be diagonal, with eigenvalues following a power-law scaling:

\[\lambda_{k}\sim\left(\frac{d}{k}\right)^{\alpha}, k\in[d]\] (213)

with \(\alpha>1\) and the target weights are assumed to be random Gaussian vectors \(\bm{\beta}_{\star}\sim\mathcal{N}(0,\nicefrac{{1}}{{d}}I_{d})\). In particular, note that \(\mathrm{Tr}\bm{\Lambda}\sim d\) for \(d\to\infty\). Given the training data, they consider least-squares regression in the class of linear random features predictor:

\[\hat{f}(\bm{x};\bm{a})=\langle\bm{a},\bm{W}\bm{x}\rangle\] (214)

where \(\bm{W}\in\mathbb{R}^{p\times d}\) is a Gaussian random matrix elements in \(\mathcal{N}(0,\nicefrac{{1}}{{d}}I_{d})\).5

Footnote 5: In Maloney et al. (2022), \(\bm{\beta}_{\star}\) has variance \(\sigma_{w}/d\), the spectrum has a scale \(\lambda_{-}\) and the random projection \(\bm{W}\) has variance \(\sigma_{w}/d\). Here we take \(\sigma_{w}=\lambda_{-}=\sigma_{u}=1\) since it is irrelevant to the discussion.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline  & This work & Bahri et al. (2024) & Maloney et al. (2022) & Atanasov et al. (2024) \\ \hline Input dimension & \(d\) & \(d\) & \(M\) & \(D\) \\ \hline Number of features & \(p\) & \(P\) & \(N\) & \(N\) \\ \hline Number of samples & \(n\) & \(D\) & \(T\) & \(P\) \\ \hline Capacity & \(\alpha\) & \(1+\tilde{\alpha}\) & \(1+\tilde{\alpha}\) & \(\alpha\) \\ \hline Source & \(r\) & \(\nicefrac{{1}}{{2}}(1-\nicefrac{{1}}{{(\tilde{\alpha}+1)}})\) & \(\nicefrac{{1}}{{2}}(1-\nicefrac{{1}}{{(\tilde{\alpha}+1)}})\) & \(r\) \\ \hline Target decay (in \(L_{2}\)) & \(\alpha r+\nicefrac{{1}}{{2}}\) & \(\nicefrac{{1}}{{2}}(1+\tilde{\alpha})\) & \(\nicefrac{{1}}{{2}}(1+\tilde{\alpha})\) & \(\alpha r+\nicefrac{{1}}{{2}}\) \\ \hline \end{tabular}
\end{table}
Table 1: Dictionary of notation between the source and capacity conditions defined in eq. (38) and the scalings in different neural scaling laws works. Note that since Bahri et al. (2024), Maloney et al. (2022) also employ the greek letter \(\alpha\), we denote theirs by \(\tilde{\alpha}\) to avoid confusion.

This setting is a particular case of the one introduced in Section 2. In particular, it satisfies particular source and capacity conditions eq. (38). To see this, note that the feature population covariance is identical to the input data covariance:

\[\mathbb{E}[\bm{W}\bm{x}\bm{x}^{\top}\bm{W}^{\top}]=\nicefrac{{1}}{{d}}\bm{\Lambda}\] (215)

Therefore, we can identify \(\bm{\Sigma}=\nicefrac{{1}}{{d}}\bm{\Lambda}\) which has \(\operatorname{Tr}\bm{\Sigma}<\infty\) for \(\alpha>1\) in the limit \(d\to\infty\). Therefore, the features satisfy a capacity condition with scaling \(\alpha\). Moreover, the asymptotic kernel is simply the linear kernel:

\[K(\bm{x},\bm{x}^{\prime})=\mathbb{E}_{\bm{w}}[\langle\bm{w},\bm{x}\rangle \langle\bm{w},\bm{x}^{\prime}\rangle]=\frac{\langle\bm{x},\bm{x}^{\prime} \rangle}{d}.\] (216)

Since the target variance is constant, this is equivalent to a source condition with:

\[r=\frac{1}{2}\left(1-\frac{1}{\alpha}\right)\] (217)

Since \(\alpha\in(1,\infty)\), we are always in the hard regime \(r\in(0,\nicefrac{{1}}{{2}})\) where the target does not belong to the RKHS \(\mathcal{H}=\mathbb{R}^{d}\). Indeed, since \(\bm{\beta}_{\star}\sim\mathcal{N}(0,\nicefrac{{1}}{{d}}I_{d})\), we have:

\[||f_{\star}||_{\mathcal{H}}^{2}=\sum_{k=1}^{d}\beta_{\star,k}^{2}\lambda_{k} \sim d^{\alpha-1}\] (218)

which indeed diverges as \(d\to\infty\). Moreover, note that least-squares regression correspond to the case \(\ell=\infty\).

From the discussion above, the bias term scalings from Bahri et al. (2024) (resolution limited regime) and Maloney et al. (2022) (underparametrized regime \(n\gg p\), _i.e._\(q\ll 1\), and overparametrized regime \(n\ll p\), _i.e._\(q\gg 1\)), correspond to a vertical cross-section on the large \(\ell\) region of Fig. 2 (Right). Indeed, we recover exactly the rate of the _label term_ in eqs. (167)-(168) of Maloney et al. (2022):

\[\mathcal{B}(f_{\star},\bm{X},\bm{W},\bm{\varepsilon},\lambda)=O \left(n^{-2\alpha rq}\right)=O\left(p^{-(\alpha-1)}\right), n\gg p,\] (219) \[\mathcal{B}(f_{\star},\bm{X},\bm{W},\bm{\varepsilon},\lambda)=O \left(n^{-2\alpha r}\right)=O\left(n^{-(\alpha-1)}\right), n\ll p.\] (220)

Similarly, it is possible to recover the rates for the _noise term_ (first two results in eq. (86) of Maloney et al. (2022)) as the vertical cross-section on the large \(\ell\) region of Figure 2 for the rates of the variance term. In particular:

\[\mathcal{V}(f_{\star},\bm{X},\bm{W},\bm{\varepsilon},\lambda)=\begin{cases}O \left(\sigma_{\varepsilon}^{2}n^{-(1-q)}\right)=O\left(\sigma_{\varepsilon}^ {2}\frac{p}{n}\right),&n\gg p\\ O\left(\sigma_{\varepsilon}^{2}n^{0}\right),&p\gg n\end{cases}.\] (221)

Comparison with the SGD rates from Paquette et al. (2024), Lin et al. (2024) --Furthermore, in the linear noiseless target setting, lifting the condition in eq. (217), it is possible to compare our results to the compute-optimal rates for the risk obtained through stochastic gradient descent in Paquette et al. (2024). In particular, we consider unitary batch-size and the correspondence between the number of iterations of stochastic gradient descent and the number of samples \(n\) in ridge regression. Then, defining \(\hat{\gamma}\) the decay rate of the compute-optimal curves for the risk \(\tilde{\mathcal{R}}\asymp n^{-\hat{\gamma}}\) in Paquette et al. (2024), corresponding to the compute-optimal number of features \(p\asymp\tilde{p}=:n^{\tilde{q}}\),6

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline  & This work & Bordelon et al. (2024) & Lin et al. (2024) & Paquette et al. (2024) \\ \hline Input dimension & \(d\) & \(D\) & \(d\) & \(v\) \\ \hline Number of features & \(p\) & \(N\) & \(M\) & \(d\) \\ \hline Number of samples & \(n\) & \(P\) & \(N\) & \(r\) \\ \hline Capacity & \(\alpha\) & \(b\) & \(a\) & \(2\tilde{\alpha}\) \\ \hline Source & \(r\) & \(\nicefrac{{(a-1)}}{{2b}}\) & \(\nicefrac{{(b-1)}}{{2a}}\) & \(\nicefrac{{(2\tilde{\alpha}+2\beta-1)}}{{4\tilde{\alpha}}}\) \\ \hline Target decay (in \(L_{2}\)) & \(\alpha r+\nicefrac{{1}}{{2}}\) & \(\nicefrac{{a}}{{2}}\) & \(\nicefrac{{b}}{{2}}\) & \(\tilde{\alpha}+\beta\) \\ \hline \end{tabular}
\end{table}
Table 2: Dictionary of notation between the source and capacity conditions defined in eq. (38) and the scalings in different neural scaling laws works. Note that since Paquette et al. (2024) also employs the greek letter \(\alpha\), we denote theirs by \(\tilde{\alpha}\) to avoid confusion.

coincides with \(\gamma_{\mathcal{B}}(\ell=1,\hat{q})\) in Theorem 4.1, _i.e._ with fixed regularization parameter \(\lambda=1\) (\(\ell=1\)). In particular, consider the following regions in the phase diagram provided in their work:7

Footnote 7: The Phases Ib, Ic and IV correspond to \(\alpha<1\), _i.e._ to an activation \(\sigma\notin L_{2}\).

* Phase Ia (\(r<\nicefrac{{1}}{{2}}\)): \[\hat{q} =\frac{1}{\alpha},\] (222) \[\hat{\gamma} =2r=2\alpha\hat{q}r=\gamma_{\mathcal{B}}(1,\hat{q});\] (223)
* Phase II (\(r>\nicefrac{{1}}{{2}}\) and \(r<1-\nicefrac{{1}}{{2\alpha}}<1\)): \[\hat{q} =\frac{1+2\alpha r-\alpha}{\alpha}<1,\] (224) \[\hat{\gamma} =2r=\frac{\alpha-1}{\alpha}+\hat{q}=\gamma_{\mathcal{B}}(1,\hat{ q});\] (225)
* Phase III (\(r>\nicefrac{{1}}{{2}}\) and \(r>1-\nicefrac{{1}}{{2\alpha}}\)): \[\hat{q} =1,\] (226) \[\hat{\gamma} =\frac{2\alpha-1}{\alpha}=\frac{\alpha-1}{\alpha}+\hat{q}=\gamma _{\mathcal{B}}(1,\hat{q}).\] (227)

We emphasize that, in Phases Ia and II, \(\hat{\gamma}=\max_{q}\gamma_{\mathcal{B}}(1,q)\), while, in Phase III, \(\hat{\gamma}\leq\max_{q}\gamma_{\mathcal{B}}(1,q)\). Hence, the compute-optimal decay rate of the risk for stochastic gradient descent is equal or smaller than the largest rate achievable by RFRR with fixed regularization \(\lambda=1\) and therefore always smaller than the optimal one in Corollary 4.2.

A similar setting has been investigated by the recent work Lin et al. (2024), providing scaling laws for the excess risk obtained by stochastic gradient descent with stepsize schedule \(\eta_{t}=\eta/2^{t\log\nicefrac{{n}}{{n}}}\), for \(t=1,...,n\).8 Under the same source and capacity conditions, assuming \(r\in(0,\nicefrac{{1}}{{2}})\) and \(\eta=O(1)\), the result in their Theorem 4.2 may be rephrased as follows:

Footnote 8: The stepsize in Lin et al. (2024) is denoted by the greek letter \(\gamma\), which we changed to \(\eta\) to avoid confusion with the symbol we use for the risk decay rate.

\[\mathbb{E}_{\bm{X},\bm{\varepsilon}}\mathcal{R}(f_{\star},\bm{X},\bm{W},\bm{\varepsilon},\eta)\asymp n^{-\gamma_{\mathrm{SGD}}(\eta,p)},\] (228) \[\gamma_{\mathrm{SGD}}(\eta,p)=\left[2\alpha r\left(\frac{1+\log_ {n}\eta}{\alpha}\wedge\log_{n}p\right)\right]\wedge\left[1-\left(\frac{1+\log_ {n}\eta}{\alpha}\wedge\log_{n}p\right)\right].\] (229)

Hence, choosing \(p\asymp n^{q}\) and \(\eta\asymp n^{\ell-1}\), _i.e._\(\eta\asymp\lambda^{-1}\), provided \(\eta=O(1)\implies\ell<1\), this result recovers precisely the same rates as in our Theorem 4.1.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims in the abstract and introduction are supported by mathematical proofs or numerical results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: All mathematical proofs include clear assumptions reflecting the scope of applicability. Numerical results are replicated for different data sets and choices of feature maps. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Clear assumptions are provided in an "assumption environment". Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have included a detailed discussion of how all the numerical experiments were conducted in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We judge the code is too simple to be released, and that we give enough information for the reproducibility of the numerical plots. All data sets used in the numerical experiments are either synthetic or open source. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/Guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have added a detailed discussion of the experiments in the captions and in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The numerical agreement between theory and experiments is so good that error bars are unnecessary. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: The experiments are simple illustrations of the theorems. They are simple enough to be ran on a standard laptop in a a few hours. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This work is of theoretical nature, and therefore has no major ethical implications. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This work is of theoretical nature, and therefore has no relevant societal impacts. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This work is of theoretical nature, and therefore has no risk of misusage. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All resources used from other works are properly acknowledged. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets are introduced in this paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve crowdsourcing. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work is of theoretical nature and does not have potential risks to the people involved. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.