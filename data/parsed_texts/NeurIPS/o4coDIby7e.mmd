# Measuring Goal-Directedness

 Matt MacDermott

Imperial College London

&James Fox

University of Oxford

London Initiative for Safe AI

&Francesco Belardinelli

Imperial College London

&Tom Everitt

Google DeepMind

###### Abstract

We define _maximum entropy goal-directedness (MEG)_, a formal measure of goal-directedness in causal models and Markov decision processes, and give algorithms for computing it. Measuring goal-directedness is important, as it is a critical element of many concerns about harm from AI. It is also of philosophical interest, as goal-directedness is a key aspect of agency. MEG is based on an adaptation of the maximum causal entropy framework used in inverse reinforcement learning. It can measure goal-directedness with respect to a known utility function, a hypothesis class of utility functions, or a set of random variables. We prove that MEG satisfies several desiderata and demonstrate our algorithms with small-scale experiments 1.

Footnote 1: A notebook that can be used to apply our algorithms in your own tabular MDPs is available at https://colab.research.google.com/drive/1p_RA7Ee18GDwx6x3M4v45R6L84koa4rf_

## 1 Introduction

In order to build more useful AI systems, a natural inclination is to try to make them more _agentic_. But while agents built from language models are touted as the next big advance [20], agentive systems have been identified as a potential source of individual [4], systemic [17], and catastrophic [21] risks. Agency is thus a key focus of behavioural evaluations [22, 23] and governance [24]. Some prominent researchers have even called for a shift towards designing explicitly non-agentic systems [4, 1].

A critical aspect of agency is the ability to pursue goals. Indeed, the _standard theory of agency_ defines agency as the capacity for intentional action - action that can be explained in terms of mental states such as goals [10]. But when are we justified in ascribing such mental states? According to Daniel Dennett's instrumentalist philosophy of mind [18], we are justified when doing so is useful for predicting a system's behaviour.

This paper's key contribution is a method for formally measuring goal-directedness based on Dennett's idea. Since pursuing goals is about having a particular causal effect on the environment2, defining our measure in a causal model is natural. Causal models are general enough to encompass most frameworks popular among ML practitioners, such as single-decision prediction, classification, and regression tasks, as well as multi-decision (partially observable) Markov decision processes. They also offer enough structure to usefully model many ethics and safety problems [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25].

Maximum entropy goal-directedness (MEG) operationalises goal-directedness as follows, illustrated by the subsequent running example.

A variable \(D\) in a causal model is _goal-directed_ with respect to a utility function \(\mathcal{U}\) to the extent that the conditional probability distribution of \(D\) is well-predicted by the hypothesis that \(D\) is optimising \(\mathcal{U}\).

**Example 1**.: A mouse begins at the centre of a gridworld (Figure 0(a)). It observes that a block of cheese is located either to the right or left (\(S\)) with equal probability, proceeds either away from it or towards it (\(D\)), and thus either obtains the cheese or does not (\(T\)).

Suppose that the mouse moves left when the cheese is to the left and right when it is to the right, thus consistently obtaining the cheese. Intuitively, this behaviour seems goal-directed, but can we quantify how much? Figure 1 gives an overview of our procedure. We first model the system of interest as a causal Bayesian network (Figure 0(b)) with variables \(S\) for the cheese's position, \(D\) for the mouse's movement, and \(T\) for whether or not the mouse obtains the cheese. Then, we identify a candidate decision variable \(D\) and target variable \(T\), hypothesising that the mouse optimises a utility function that depends on \(T\) (Figure 0(c)). Finally, we form a model of what behaviour we should expect from \(D\) if it is indeed optimising \(U\) and then measure how well this model predicts \(D\)'s observed behaviour (Figure 0(d)).

Ziebart (2010)'s maximum causal entropy (MCE) framework suggests a promising way to construct a model for expected behaviour under a given utility function. However, there are several obstacles to applying it to our problem: it cannot measure the predictive usefulness of _known_ utility functions, and it only finds the most predictive _linear_ utility function. In practice, arbitrary known utility functions can be plugged in, but the results are _not scale-invariant_. We overcome these difficulties by returning to first principles and deriving an updated version of the MCE framework.

In summary, our contributions are four-fold. We (i) adapt the MCE framework to derive _maximum entropy goal-directedness_ (MEG), a philosophically-motivated measure of goal-directedness with respect to known utility functions, and show that it satisfies several key desiderata (Section 3); (ii) we extend MEG to measure goal-directedness in cases without a known utility function (Section 4); (iii) we provide algorithms for measuring MEG in MDPs (Section 5); and (iv) we demonstrate the algorithms empirically (Section 6).

Related Work.Inverse reinforcement learning (IRL) (Ng and Russell, 2000) focuses on the question of _which_ goal a system is optimising, whilst we are interested in _to what extent_ the system can be seen as optimising a goal. We are not the first to ask this question. Some works take a Bayesian approach inspired by Dennett's intentional stance.3 Most closely related to our work is Orseau et al. (2018), which applies Bayesian IRL in POMDPs using a Solomonoff prior over utility functions and an \(\varepsilon\)-greedy model of behaviour. This lets them infer a posterior probability distribution over whether an observed system is a (goal-directed) "agent" or "just a device". Our approach distinguishes itself from these by considering arbitrary variables in a causal model and deriving our behaviour model from the principle of maximum entropy. Moreover, since our algorithms can take advantage of differentiable classes of utility functions, our approach may be amenable to scaling up using deep neural networks. Oesterheld (2016) combines the intentional stance with Bayes' theorem in cellular automata, but does not consider specific models of behaviour. Like us, Kenton et al. (2023) consider goal-directedness in

Figure 1: Computing maximum entropy goal-directedness (MEG).

a causal graph. However, they require variables to be manually labelled as _mechanisms_ or _object-level_, and only provide a binary distinction between agentic and non-agentic systems (see Appendix A for a detailed comparison). Biehl and Virgo (2022); Virgo et al. (2021) propose a definition of agency in Moore machines based on whether a system's internal state can be interpreted as beliefs about the hidden states of a POMDP.

## 2 Background

We use capital letters for random variables \(V\), write \(\operatorname{dom}(V)\) for their domain (assumed finite), and use lowercase for outcomes \(v\in\operatorname{dom}(V)\). Boldface denotes sets of variables \(\bm{V}=\{V_{1},\ldots,V_{n}\}\), and their outcomes \(\bm{v}\in\operatorname{dom}(\bm{V})=\bigtimes_{i}\operatorname{dom}(V_{i})\). Parents and descendants of \(V\) in a graph are denoted by \(\mathbf{Pa}_{V}\) and \(\mathbf{Dese}_{V}\), respectively (where \(\mathbf{pa}_{V}\) and \(\mathbf{dese}_{V}\) are their instantiations).

Causal Bayesian networks (CBNs) are a class of probabilistic graphical models used to represent causal relationships between random variables (Pearl, 2009).

**Definition 2.1** (Causal Bayesian network).: A _Bayesian network_\(M=(G,P)\) over a set of variables \(\bm{V}=\{V_{1},\ldots,V_{n}\}\) consists of a joint probability distribution \(P\) which factors according to a directed acyclic graph (DAG) \(G\), i.e., \(P(V_{1},\ldots,V_{n})=\prod_{i=1}^{n}P(V_{i}\mid\mathbf{Pa}_{V_{i}})\), where \(\mathbf{Pa}_{V_{i}}\) are the parents of \(V_{i}\) in \(G\). A Bayesian network is _causal_ if its edges represent direct causal relationships, or formally if the result of an intervention \(\operatorname{do}(\bm{X}=\bm{x})\) for any \(\bm{X}\subseteq\bm{V}\) can be computed using the _truncated factorisation formula_: \(P(\bm{v}\mid\operatorname{do}(\bm{X}=\bm{x}))=\prod_{i:v_{i}\notin\bm{x}}P(v_{ i}\mid\mathbf{pa}_{v_{i}})\) if \(\bm{v}\) is consistent with \(\bm{x}\) or \(P(\bm{v}\mid\operatorname{do}(\bm{X}=x))=0\) otherwise.

Figure 0(b) depicts Example 1 as a CBN, showing the causal relationships between the location of the cheese (\(S\)), the mouse's behavioural response (\(D\)), and whether the mouse obtains the cheese (\(T\)).

We are interested in to what extent a set of random variables in a CBN can be seen as goal-directed. That is, to what extent can we interpret them as _decisions_ optimising a _utility function_? In other words, we are interested in moving from a CBN to a causal influence diagram (CID), a type of probabilistic graphical model that explicitly identifies decision and utility variables.

**Definition 2.2** (Causal Influence Diagram (Everitt et al., 2021)).: A _causal influence diagram_ (CID) \(M=(G,P)\) is a CBN where the variables \(\bm{V}\) are partitioned into decision \(\bm{D}\), chance \(\bm{X}\), and utility variables \(\bm{U}\). Instead of a full joint distribution over \(\bm{V}\), \(P\) consists of conditional probability distributions (CPDs) for each _non-decision_ variable \(V\in\bm{V}\setminus\bm{D}\).

A CID can be combined with a _policy_\(\pi\), which specifies a CPD \(\pi_{D}\) for each decision variable \(D\), in order to obtain a full joint distribution. We call the sum of the utility variables the _utility function_ and denote it \(\mathcal{U}=\sum_{U\in\bm{U}}U\). Policies are evaluated by their total expected utility \(\mathbb{E}_{\pi}[\mathcal{U}]\). We write \(\pi^{\text{unif}}\) for the uniformly random policy.

CIDs can model a broad class of decision problems, including Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs) (Everitt et al., 2021).

**Example 2** (POMDP).: A mouse begins at the centre of the grid, with a block of cheese located either at the far left or the far right (Figure 2). The mouse does not know its position or the position of the cheese (\(S_{1}\)) but can smell which direction the cheese is in (\(O_{1}\)) and decide which way to move (\(D_{1}\)). Next step, the mouse again smells the direction of the cheese (\(O_{2}\)) and again chooses which way to proceed (\(D_{2}\)).

## 3 Measuring goal-directedness with respect to a known utility function

Maximum Entropy Goal-directedness.Dennett's instrumentalist approach to agency says that we can ascribe mental states (such as utilities) to a system to the extent that doing so is useful for predicting its behaviour (Dennett, 1989). To operationalise this, we need a model of what behaviour is predicted by a utility function. According to the _principle of maximum entropy_(Jaynes, 1957), we should choose a probability distribution with the highest entropy distribution satisfying our requirements, thus minimising unnecessary assumptions (following Occam's razor). We can measure the entropy of a policy by the expected entropy of its decision variables conditional on their parents \(H_{\pi}(\bm{D}\mid\mid\mathbf{Pa}_{D})=-\sum_{D\in\mathcal{D}}\mathbb{E}_{d, \mathbf{Pa}_{D}\sim P_{\pi}}\log\pi_{D}(d\mid\mathbf{Pa}_{D})\). This is Ziebart et al. (2010)'s _causal entropy_, which we usually refer to as just the entropy of \(\pi\).

In our setting, the relevant constraint is expected utility. To avoid assuming that only optimal agents are goal-directed, we construct a set of models of behaviour which covers all levels of competence an agent optimising utility \(\mathcal{U}\) could have. We define the set of _attainable expected utilities_ in a CID as \(\operatorname{att}(\mathcal{U})=\{u\in\mathbb{R}\mid\exists\pi\in\Pi\left( \mathbb{E}_{\pi}\left[\mathcal{U}\right]=u\right)\}\).

**Definition 3.1** (Maximum entropy policy set, known utility function).: Let \(M=(G,P)\) be a CID with decision variables \(\bm{D}\) and utility function \(\mathcal{U}\). The _maximum entropy policy set for \(u\in\operatorname{att}(\mathcal{U})\)_ is \(\Pi^{\text{maxent}}_{\mathcal{U},u}\coloneqq\operatorname*{argmax}_{\pi\mid \mathbb{E}_{\pi}\left[\mathcal{U}\right]=u}H_{\pi}(\bm{D}\mid\textbf{Pa}_{D})\). The _maximum entropy policy set for \(\mathcal{U}\)_ is the set of maximum entropy policies for _any_ attainable expected utility \(\Pi^{\text{maxent}}_{\mathcal{U}}\coloneqq\bigcup_{u\in\operatorname{att}( \mathcal{U})}\Pi^{\text{maxent}}_{\mathcal{U},u}\).

For each attainable expected utility, \(\Pi^{\text{maxent}}_{\mathcal{U}}\) contains the highest entropy policy which attains it. In MDPs, this policy is unique \(\pi^{\text{maxent}}_{\mathcal{U},u}\) and can be found with backwards induction (see Section 5).

We measure predictive accuracy using cross-entropy, as is common in ML. We subtract the predictive accuracy of the uniform distribution, so that we measure predictive accuracy relative to random chance. This makes MEG always non-negative.

**Definition 3.2** (Maximum entropy goal-directedness, known utility function).: Let \(M\coloneqq(G,P)\) be a CID with decision variables \(\bm{D}\) and utility function \(\mathcal{U}\). The _maximum entropy goal-directedness_ (MEG) of a policy \(\pi\) with respect to \(\mathcal{U}\) is

\[\operatorname{MEG}_{\mathcal{U}}(\pi)\coloneqq\max_{\pi^{\text{maxent}}\in \Pi^{\text{maxent}}_{\mathcal{U}}}\mathbb{E}_{\pi}\left[\sum_{D\in\bm{D}} \left(\log\pi^{\text{maxent}}(D\mid\textbf{Pa}_{D})-\log\frac{1}{\left| \operatorname{dom}(D)\right|}\right)\right].\] (1)

As we will see, in a Markov Decision Process, this is equivalent to maximising predictive accuracy by varying the rationality parameter \(\beta\) in the'soft-optimal' policy \(\pi^{\text{soft}}_{\beta}\) used in maximum entropy reinforcement learning [1].4

Footnote 4: A noteworthy property of Definition 3.2 is that a policy that _pessimises_ a utility function can be just as goal-directed as a policy that _optimises_ it. This fits with the intuitive notion that we are measuring how likely a policy’s performance on a particular utility function is to be an accident. However, it might be argued that a policy should instead be considered _negatively_ goal-directed with respect to a utility function it pessimises. We could adjust Definition 3.2 to achieve this by multiplying by the sign of \(\mathbb{E}_{\pi}\left[\mathcal{U}\right]-\mathbb{E}_{\pi^{\text{min}}}\left[ \mathcal{U}\right]\), making the goal-directedness of any policy that performs worse than random negative. For simplicity, we use the unsigned measure in this paper.

If instead of having access to a policy \(\pi\), we have access to a set of full trajectories \(\{(\textbf{pa}^{i}_{D_{1}},D^{i}_{1},\ldots,\textbf{pa}_{D^{i}_{n}},D^{i}_{n}) \}_{i}\), the expectation \(\mathbb{E}_{\pi}\) in Equation (1) can be replaced with an average over the trajectory set. This is an unbiased and consistent estimate of \(\operatorname{MEG}_{\mathcal{U}}(\pi)\) for the policy \(\pi\) generating the trajectories.

Example.Consider a policy \(\pi\) in Example 1 that proceeds towards the cheese with probability \(0.8\). How goal-directed is this policy with respect to the utility function \(\mathcal{U}\) that gives \(+1\) for obtaining the cheese and \(-1\) otherwise?

To compute \(\operatorname{MEG}_{\mathcal{U}}(\pi)\), we first find the maximum entropy policy set \(\Pi^{\text{maxent}}_{\mathcal{U}}\), and then take the maximum predictive accuracy with respect to \(\pi\). In a single-decision setting, for each attainable expected utility \(u\) there is a unique \(\pi^{\text{maxent}}_{\mathcal{U},u}\) which has the form of a Boltzmann policy \(\pi^{\text{maxent}}_{\mathcal{U},u}(d\mid s)=\frac{\exp(\beta\cdot\mathbb{E}[ U|d,s])}{\sum_{d^{\prime}}\exp(\beta\cdot\mathbb{E}[U|d^{\prime},s])}\) (cf. Theorem 5.1). The rationality parameter \(\beta=\beta(u)\) can be varied to get the correct expected utility. Predictive accuracy with respect to \(\pi\) is maximised by \(\pi^{\text{maxent}}_{\mathcal{U},0.8}\), which has a rationality parameter of \(\beta=\log 2\). The expected logprob of a prediction of this policy is

Figure 2: Sequential multi-decision mouse example.

\(\mathbb{E}_{\pi}\left[\log\pi_{\mathcal{U},0.8}^{\text{maxert}}(D\mid\mathbf{Pa}_{D} )\right]=-0.50\), while the expected logprob of a uniform prediction is \(\log(\frac{1}{2})=-0.69\). So we get that \(\operatorname{MEG}_{\mathcal{U}}(\pi)=-0.50-(-0.69)=0.19\). For comparison, predictive accuracy for the optimal policy \(\pi^{*}\) is maximised when \(\beta=\infty\), and has \(\operatorname{MEG}_{\mathcal{U}}(\pi^{*})=0-(-0.69)=0.69\).

Properties.We now show that MEG satisfies three important desiderata. First, since utility functions are usually only defined up to translation and rescaling, a measure of goal-directedness with respect to a utility function should be translation and scale invariant. MEG satisfies this property:

**Proposition 3.1** (Translation and scale invariance).: _Let \(M_{1}\) be a CID with utility function \(\mathcal{U}_{1}\), and let \(M_{2}\) be an identical CID but with utility function \(\mathcal{U}_{2}=a\cdot\mathcal{U}_{1}+b\), for some \(a,b\in\mathbb{R}\), with \(a\neq 0\). Then for any policy \(\pi\), \(\operatorname{MEG}_{\mathcal{U}_{1}}(\pi)=\operatorname{MEG}_{\mathcal{U}_{2 }}(\pi)\)._

Second, goal-directedness should be minimal when actions are chosen completely at random and maximal when uniquely optimal actions are chosen.

**Proposition 3.2** (Bounds).: _Let \(M\) be a CID with utility function \(\mathcal{U}\). Then for any policy \(\pi\) we have \(0\leq\operatorname{MEG}_{\mathcal{U}}(\pi)\leq\sum_{D\in\mathcal{D}}\log(| \operatorname{dom}(D)|)\), with equality in the lower bound if \(\pi\) is the uniform policy, and equality in the upper bound if \(\pi\) is the unique optimal (or anti-optimal) policy with respect to \(\mathcal{U}\)._

Note that MEG has a natural interpretation as the amount of evidence provided for a goal-directed policy over a purely random policy. The larger a decision problem, the more opportunity there is to see this evidence, so the higher MEG can be.

Third, a system can never be goal-directed towards a utility function it cannot affect.

**Proposition 3.3** (No goal-directedness without causal influence).: _Let \(M=(G,P)\) be a CID with utility function \(\mathcal{U}\) and decision variables \(\boldsymbol{D}\) such that, \(\textbf{Desc}(\boldsymbol{D})\cap\textbf{Pa}_{\boldsymbol{U}}=\emptyset\). Then \(\operatorname{MEG}_{\mathcal{U}}(\boldsymbol{D})=0\)._

Comparison to MCE IRLOur method is closely related to MCE IR (Ziebart et al., 2010) (see Gleave and Toyer (2022) for a useful primer). In this subsection, we discuss the key similarities and differences.

The MCE IRL method seeks to find a utility function that explains the policy \(\pi\). It starts by identifying a set of \(n\) linear features \(f_{i}\) and seeks a model policy that imitates \(\pi\) as far as these features are concerned but otherwise is as random as possible. It thus applies the principle of maximum entropy with \(n\) linear constraints. The form of the model policy involves a weighted sum of these features. In a single-decision example, it takes the form

\[\pi^{\text{MCE}}(d\mid s)=\frac{\exp\left(\mathbb{E}\left[\sum_{i}w_{i}f_{i} \mid d,s\right]\right)}{\sum_{d^{\prime}}\exp\left(\mathbb{E}\left[w_{i}f_{i} \mid d^{\prime},s\right]\right)}.\] (2)

The weights \(w_{i}\) are interpreted as a utility function over the features \(f_{i}\). MCE IRL can, therefore, only return a linear utility function.

In contrast, our method seeks to measure the goal-directedness of \(\pi\) with respect to an arbitrary utility function \(\mathcal{U}\), linear or otherwise. Rather than constructing a single maximum entropy policy with \(n\) linear constraints, we construct a class of maximum entropy policies, each with a different single constraint on the expected utility.

A naive alternative to defining the goal-directedness of \(\pi\) with respect to \(\mathcal{U}\) as the maximum predictive accuracy across \(\mathcal{U}\)'s maximum policy set, we could simply plug in our utility function \(\mathcal{U}\) to \(\pi^{\text{MCE}}\) from Equation (2), and use that to measure predictive accuracy. If \(\mathcal{U}\) is linear in the features \(f_{i}\), we could substitute in the appropriate weights, but even if not, we could still replace \(\sum_{i}w_{i}f_{i}\) with \(\mathcal{U}\). Indeed, this is often done with nonlinear utility functions in _deep_ MCE IRL (Wulfmeier et al., 2015).

However, this would not have a formal justification, and we would run into a problem: scale non-invariance. Plugging in \(2\cdot\mathcal{U}\) would result in a more sharply peaked \(\pi^{\text{MCE}}\) than \(\mathcal{U}\); in Example 1, we would get that the mouse is more goal-directed towards \(2\cdot\mathcal{U}\) than \(\mathcal{U}\), with a predictive accuracy (measured by negative cross-entropy) of -0.018 vs -0.13. In contrast, constructing separate maximum entropy policies for each expected utility automatically handles this issue. The policy in \(\Pi^{\text{maxert}}_{2\cdot\mathcal{U}}\) which maximises predictive accuracy for \(\pi\) has an inversely scaled rationality parameter \(\beta^{\prime}=\frac{\beta}{2}\) compared to the maximally predictive policy in \(\Pi^{\text{maxert}}_{2\cdot\mathcal{U}}\). In other words, they are the same policy, and we get that \(\operatorname{MEG}_{\mathcal{U}}(\pi)=\operatorname{MEG}_{2\cdot\mathcal{U}}( \pi)=0.19\) (cf. Proposition 3.1).

Measuring goal-directedness without a known utility function

In many cases where we want to apply MEG, we may not know exactly what utility function the system could be optimising. For example, we might suspect that a content recommender is trying to influence a user's preferences, but may not have a clear hypothesis as to in what way. Therefore, in this section, we extend our definitions for measuring goal-directedness to the case where the utility function is unknown.

We first extend CIDs to include multiple possible utility functions.

**Definition 4.1**.: A _parametric-utility CID_ (CID) \(M^{\Theta}\) is a set of CIDs \(\{M^{\theta}\mid\theta\in\Theta\}\) which differ only in the CPDs of their utility variables.

In effect, a parametric CID is a CID with a parametric class of utility functions \(\mathcal{U}^{\Theta}\).

The maximum entropy policy set from Definition 3.1 is extended accordingly to include maximum entropy policies _for each utility function_ and each attainable expected utility with respect to it.

**Definition 4.2** (Maximum entropy policy set, unknown utility function).: Let \(M^{\Theta}=(G,P)\) be a parametric-utility CID with decision variables \(\bm{D}\) and utility function \(\mathcal{U}^{\Theta}\). The _maximum entropy policy set for \(\mathcal{U}^{\Theta}\)_ is the set of maximum entropy policies for any attainable expected utility for any utility function in the class: \(\Pi^{\text{maxent}}_{\mathcal{U}^{\Theta}}\coloneqq\bigcup_{\theta\in\Theta,u \in\text{att}(\mathcal{U}^{\Theta})}\Pi^{\text{maxent}}_{\mathcal{U}^{\theta },u}\).

**Definition 4.3** (Meg, unknown utility function).: Let \(M^{\Theta}=(G,P)\) be a parametric-utility CID with decision variables \(\bm{D}\) and utility function \(\mathcal{U}^{\Theta}\). The _maximum entropy goal-directedness_ of a policy \(\pi\) with respect to \(\mathcal{U}^{\Theta}\) is \(\operatorname{MEG}_{\mathcal{U}^{\Theta}}(\pi)=\max_{\mathcal{U}\in\mathcal{U }^{\Theta}}\operatorname{MEG}_{\mathcal{U}}(\pi)\).

**Definition 4.4** (Meg, target variables).: Let \(M=(G,P)\) be a CBN with variables \(\bm{V}\). Let \(\bm{D}\subseteq\bm{V}\) be a hypothesised set of decision variables and \(\bm{T}\subseteq V\) be a hypothesised set of _target_ variables. The _maximum entropy goal-directedness_ of \(\bm{D}\) with respect to \(\bm{T}\), \(\operatorname{MEG}_{\bm{T}}(\bm{D})\), is the goal-directedness of \(\pi=P(\bm{D}\mid\mathbf{Pa}_{D})\) in the parametric CID with decisions \(\bm{D}\) and utility functions \(\mathcal{U}:\operatorname{dom}(\bm{T})\to\mathbb{R}\) (i.e., the set of all utility functions over \(T\)).

For example, if we only suspected that the mouse in Example 1 was optimising some function of the cheese \(T\), but didn't know which one, we could apply Definition 4.4 to consider the goal-directedness towards \(T\) under any utility function defined on \(T\). Thanks to translation and scale invariance (Proposition 3.1), there are effectively only three utility functions to consider: those that provide higher utility to cheese than not cheese, those that do the opposite, and those that are indifferent.

Note that \(\bm{T}\) has to include some descendants of \(\bm{D}\), in order to enable positive MEG (Proposition 3.3). However, it is not necessary that \(\bm{T}\) consists of _only_ descendants of \(\bm{D}\) (i.e., \(\bm{T}\) need not be a subset of \(\mathbf{Desc}(\bm{D})\)). For example, goal-conditional agents take an instruction as part of their input \(\mathbf{Pa}_{D}\). The goal-directedness of such agents can only be fully appreciated by including the instruction in \(\bm{T}\).

Pseudo-terminal goals.Definition 4.4 enable us to state a result about a special kind of instrumental goal. It is well known that an agent that optimises some variable has an instrumental incentive to control any variables which mediate between the two [10]. However, since the agent might want the mediators to take different values in different circumstances, it need not appear goal-directed with respect to the mediators. Theorem 4.1 shows that in the special case where the mediators _d-separate_ the decision from the downstream variable, the decision appears at least as goal-directed with respect to the mediators as with respect to the target.

**Theorem 4.1** (Pseudo-terminal goals).: _Let \(M=((\bm{V},\bm{E}),P)\) be a CBN. Let \(\bm{D},\bm{T},\bm{S}\subseteq\bm{V}\) such that \(\bm{D}\perp\bm{T}\mid\bm{S}\). Then \(\operatorname{MEG}_{\bm{T}}(\bm{D})\leq\operatorname{MEG}_{\bm{S}}(\bm{D})\)._

For example, in Figure 2, the agent must be at least as goal-directed towards \(S_{3}\) as it is towards \(U_{3}\), since \(S_{3}\) blocks all paths (i.e. d-separates) from \(\{D_{1},D_{2}\}\) to \(U_{3}\).

Intuitively, this comes about because, in such cases, a rational agent wants the mediators to take the same values regardless of circumstances, making the instrumental control incentive indistinguishable from a terminal goal. This means we do not have to look arbitrarily far into the future to find evidence of goal-directedness. An agent that is goal-directed with respect to next week must also be goal-directed with respect to tomorrow.

Computing MEG in Markov Decision Processes

In this section, we give algorithms for computing MEG in MDPs. First, we define what an MDP looks like as a CID. We then establish that soft value iteration can be used to construct our maximum entropy policies, and give algorithms for computing MEG when the utility function is known or unknown.

Note that in order to run these algorithms, we do not need an explicit causal model, and so we do not have to worry about hidden confounders. We do, however, need black-box access to the environment dynamics, i.e., the ability to run different policies in the environment and measure whatever variables we are considering utility functions over.

**Definition 5.1**.: A _Markov Decision Process_ (MDP) is a CID with variables \(\{S_{t},D_{t},U_{t}\}_{t=1}^{n}\), decisions \(\bm{D}=\{D_{t}\}_{t=1}^{n}\) and utilities \(\bm{U}=\{U_{t}\}_{t=1}^{n}\), such that for \(t\) between 1 and \(n\), \(\textbf{Pa}_{D_{t}}=\{S_{t}\}\), \(\textbf{Pa}_{U_{t}}=\{S_{t}\}\), while \(\textbf{Pa}_{S_{t}}=\{S_{t-1},D_{t-1}\}\) for \(t>1\), and \(\textbf{Pa}_{S_{1}}=\emptyset\).

Constructing Maximum Entropy PoliciesIn MDPs, Ziebart's soft value iteration algorithm can be used to construct maximum entropy policies satisfying a set of linear constraints. We apply it to construct maximum entropy policies satisfying expected utility constraints.

**Definition 5.2** (Soft Q-Function).: Let \(M=(G,P)\) be an MDP. Let \(\beta\in\mathbb{R}\setminus\{0\}\). For each \(D_{t}\in\bm{D}\) we define the _soft Q-function_\(Q_{\beta,n}^{\text{soft}}:\operatorname{dom}(D_{t})\times\operatorname{dom}( \textbf{Pa}_{D_{t}})\to\mathbb{R}\) via the recursion:

\[Q_{\beta,n}^{\text{soft}}(d_{n}\mid\textbf{pa}_{n}) =\mathbb{E}\left[U_{t}+\frac{1}{\beta}\operatorname{logsumexp}( \beta\cdot Q_{\beta,t+1}^{\text{soft}}(\cdot\mid\textbf{Pa}_{D_{t+1}}))\right| d_{t},\textbf{pa}_{t+1}\right]\quad\text{ for }t<n,\] \[Q_{\beta,n}^{\text{soft}}(d_{n}\mid\textbf{pa}_{n}) =\mathbb{E}\left[U_{n}\mid d_{n},\textbf{pa}_{n}\right],\]

where \(\operatorname{logsumexp}\beta(Q_{\beta,t+1}^{\text{soft}}(\cdot\mid\textbf{Pa }_{D_{t+1}}))=\log\sum_{d_{t+1}\in\operatorname{dom}(D_{t+1})}\exp(\beta Q_{ \beta,t+1}^{\text{soft}}(d_{t+1}\mid\textbf{Pa}_{D_{t+1}}))\).

Using the soft Q-function, we show that there is a unique \(\pi\in\Pi^{\text{maxent}}_{\bm{U},u}\) for each \(\mathcal{U}\) and \(u\) in MDPs.

**Theorem 5.1** (Maximum entropy policy in MDPs).: _Let \(M=(G,P)\) be an MDP with utility function \(\mathcal{U}\), and let \(u\in\operatorname{att}(\mathcal{U})\) be an attainable expected utility. Then there exists a unique maximum entropy policy \(\pi^{\text{maxent}}_{u}\in\Pi^{\text{maxent}}_{\mathcal{U},u}\), and it has the form_

\[\pi^{\text{maxent}}_{u,t}(d_{t}\mid\textbf{pa}_{t})=\pi^{\text{soft}}_{\beta,t }(d_{t}\mid\textbf{pa}_{t})\coloneqq\begin{cases}\frac{\exp(\beta\cdot Q_{ \beta,t}^{\text{soft}}(d_{t}\mid\textbf{pa}_{t}))}{\sum_{d^{\prime}\in \operatorname{dom}(D_{t})}\exp(\beta\cdot Q_{\beta,t}^{\text{soft}}(d_{t}^{ \prime}\mid\textbf{pa}_{t}))},&\text{if }\beta\neq 0\\ \pi^{\text{unif}}(d_{t}\mid\textbf{pa}_{t}),&\text{if }\beta=0.\end{cases}\] (3)

_where \(\beta=\operatorname{argmax}_{\beta^{\prime}\in\mathbb{R}\cup\{\infty,-\infty \}}\sum_{t}\mathbb{E}_{\pi}\left[\log(\pi^{\text{soft}}_{\beta^{\prime},t}(d _{t}\mid\textbf{pa}_{t}))\right]\)._

We refer to a policy of the form of \(\pi_{\beta}\) as a soft-optimal policy with a rationality parameter of \(\beta\).

Known Utility FunctionTo apply Definition 3.1 to measure the goal-directedness of a policy \(\pi\) in a CID \(M\) with respect to a utility function \(\mathcal{U}\), we need to find the maximum entropy policy in \(\Pi^{\text{maxent}}_{\mathcal{U}}\) which best predicts \(\pi\). We can use Theorem 5.1 to derive an algorithm that finds \(\pi^{\text{maxent}}_{u}\) for any \(u\in\operatorname{att}(\mathcal{U})\).

Fortunately, we do not need to consider each policy in \(\Pi^{\text{maxent}}_{\mathcal{U},u}\) individually. We know the form of \(\pi^{\text{maxent}}_{u}\), and only the real-valued rationality parameter \(\beta\) varies depending on \(u\).

The gradient of the predictive accuracy with respect to \(\beta\) is

\[\nabla_{\beta}\mathbb{E}_{\pi}\left[\sum_{D\in\bm{D}}\left(\log\pi^{\text{ soft}}_{\beta}(D\mid\textbf{Pa}_{\bm{D}})-\log\frac{1}{\mid\operatorname{ dom}(D)\mid}\right)\right]=\mathbb{E}_{\pi}\left[\mathcal{U}\right]-\mathbb{E}_{\pi^{ \text{soft}}_{\beta}}\left[\mathcal{U}\right].\]

The predictive accuracy is a concave function of \(\beta\), so we can apply gradient ascent to find the global maximum in \(\beta\), which is the same as finding the maximum in \(u\).

\(\operatorname{MEG}_{\mathcal{U}}(\pi)\) can therefore be found by alternating between applying the soft value iteration of Definition 5.2 to find \(\pi^{\text{maxent}}_{\beta}\), computing \(\mathbb{E}_{\pi}\left[\mathcal{U}\right]-\mathbb{E}_{\pi^{\text{maxent}}_{ \beta}}\left[\mathcal{U}\right]\), and taking a gradient step. See Algorithm 1.

If the \(\beta\) that maximises predictive accuracy is \(\infty\) or \(-\infty\), which can happen if \(\pi\) is optimal or anti-optimal with respect to \(\mathcal{U}\), then the algorithm can never reach the (nonetheless finite) value of \(\operatorname{MEG}_{\mathcal{U}}(\pi)\), but will still converge in the limit.

Unknown-utility algorithmTo find unknown-utility MEG, we consider soft-optimal policies for various utility functions \(\mathcal{U}\) and maximise predictive accuracy with respect to \(\mathcal{U}\) as well as \(\beta\). Let \(\mathcal{U}^{\Theta}\) be a differentiable class of functions, such as a neural network, and write \(\pi^{\text{soft}}_{\theta,\beta}\) for a soft-optimal policy with respect to \(\mathcal{U}\in\mathcal{U}^{\Theta}\), and \(Q^{\text{soft}}_{\theta,\beta}\) for the corresponding soft Q-function. We can take the derivative of the predictive accuracy with respect to \(\theta\) and get \(\mathbb{E}_{\pi}\left[\nabla_{\theta}\mathcal{U}\right]-\mathbb{E}_{\pi^{ \text{uncent}}_{\beta}}\left[\nabla_{\theta}\mathcal{U}\right]\). Algorithm 2 extends Algorithm 1 to this case.

```
0:
0: Parametric MDP \(M_{\Theta}\) over differentiable class of utility functions, policy \(\pi\)
0:\(\mathrm{MEG}_{\mathcal{U}_{\Theta}}(\pi)\)
1: Initialise utility parameter \(\theta\), rationality parameter \(\beta\), set learning rate \(\alpha\).
2:repeat
3: Apply soft value iteration to find \(Q^{\text{soft}}_{\theta,\beta}\)\(\triangleright\) Definition 5.2
4:\(\pi^{\text{soft}}_{\theta,\beta}\leftarrow\text{softmax}(\beta\cdot Q^{\text{ soft}}_{\theta,\beta})\)
5:\(g_{\beta}\leftarrow\left(\mathbb{E}_{\pi}\left[\mathcal{U}^{\theta}\right]- \mathbb{E}_{\pi^{\text{soft}}_{\beta}}\left[\mathcal{U}^{\theta}\right]\right)\)
6:\(g_{\theta}\leftarrow\left(\mathbb{E}_{\pi}\left[\nabla_{\theta}\mathcal{U}^{ \theta}\right]-\mathbb{E}_{\pi^{\text{soft}}_{\beta}}\left[\nabla_{\theta} \mathcal{U}^{\theta}\right]\right)\)
7:\(\beta\leftarrow\beta+\alpha\cdot g_{\beta}\)
8:\(\theta\leftarrow\beta+\alpha\cdot g_{\theta}\)
9:until Stopping condition
10:return\(\mathbb{E}_{\pi}\left[\sum_{D\in\bm{D}}\left(\log\pi^{\text{soft}}_{\theta, \beta}(D\mid\textbf{Pa}_{D})-\log\frac{1}{|\text{dom}(D)|}\right)\right]\) ```

**Algorithm 2** Unknown-utility MEG in MDPs

An important caveat is that if \(\mathcal{U}^{\theta}\) is a non-convex function of \(\theta\) (e.g. a neural network), Algorithm 2 need not converge to a global maximum. In general, the algorithm provides a _lower bound_ for \(\mathrm{MEG}_{\mathcal{U}_{\theta}}(\pi)\), and hence for \(\mathrm{MEG}_{\mathcal{T}}(\pi)\) where \(\bm{T}=\textbf{Pa}_{\mathcal{U}^{\Theta}}\). In practice, we may want to estimate the soft Q-function and expected utilities with Monte Carlo or variational methods, in which case the algorithm provides an _approximate_ lower bound on goal-directedness.

## 6 Experimental Evaluation

We carried out two experiments5 to measure known-utility MEG with respect to the environment reward function and unknown-utility MEG with respect to a hypothesis class of utility functions. We used an MLP with a single hidden layer of size 256 to define a utility function over states.

Footnote 5: Code available at https://github.com/mattmacdermott1/measuring-goal-directedness

Our experiments measured MEG for various policies in the CliffWorld environment from the seals suite (Gleave et al., 2020). Cliffworld (Figure 2(a)) is a 4x10 gridworld MDP in which the agent starts in the top left corner and aims to reach the top right while avoiding the cliff along the top row. With probability \(0.3\), wind causes the agent to move upwards by one more square than intended. The environment reward function gives \(+10\) when the agent is in the (yellow) goal square, \(-10\) for the (dark blue) cliff squares, and \(-1\) elsewhere. The dotted yellow line indicates a length 3 goal region.

MEG vs Optimality of policyIn our first experiment, we measured the goal-directedness of policies of varying degrees of optimality by considering \(\varepsilon\)-greedy policies for \(\varepsilon\) in the range \(0.1\) to \(0.9\). Figure 2(b) shows known- and unknown utility MEG for each policy.6 Predictably, the goal-directedness with respect to the environment reward decreased toward \(0\) as the policy became less optimal. So did unknown-utility MEG - since, as \(\varepsilon\) increases, the policy becomes increasingly uniform, it does not appear goal-directed with respect to _any_ utility function over states.

Footnote 6: Known-utility MEG is deterministic. Unknown-utility MEG depends on the random initialisation of the neural network, so we show the mean of several runs. Full details are given in Appendix D.3.

MEG vs Task difficultyIn our second experiment, we measured the goal-directedness of optimal policies for reward functions of varying difficulty. We extended the goal region of Cliffworld to run for either 1, 2, 3 or 4 squares along the top row and back column and considered optimal policies for each reward function. Figure 2(c) shows Cliffworld with a goal region of length 3. Figure 2(b) shows the results. Goal-directedness with respect to the true reward function decreased as the task became easier to complete. A way to interpret this is that as the number of policies which do well on a reward function increases, doing well on that reward function provides less and less evidence for deliberate optimisation. In contrast, unknown-utility MEG stays high even as the environment reward becomes easier to satisfy. This is because the optimal policy proceeds towards the _nearest_ goal-squares and, hence, it appears strongly goal-directed with respect to a utility function which gives high reward to only those squares. Since this narrower utility function is more difficult to do well on than the environment reward function, doing well on it provides more evidence for goal-directedness. In Appendix D.3, we visualise the policies in question to make this more explicit. We also give tables of results for both experiments.

## 7 Limitations

Environment AccessAlthough computing MEG does not require an explicit causal model (cf. Section 5), it does require the ability to run various policies in the environment of interest - we cannot compute MEG purely from observational data.

IntractabilityWhile MEG can be computed with gradient descent, doing so may well be computationally intractable in high dimensional settings. In this paper, we conduct only preliminary experiments - larger experiments based on real-world data may explore how serious these limitations are in practice.

Choice of variablesMEG is highly dependent on which variables we choose to measure goal-directedness with respect to. At one extreme, all (deterministic) policies have maximal goal

Figure 3: (a) The CliffWorld environment. (b) MEG of \(\varepsilon\)-greedy policies for varying \(\varepsilon\). MEG decreases as the policy gets less optimal. (c) MEG for optimal policies for various reward functions. Known-utility MEG decreases as the goal gets easier to satisfy, but unknown-utility MEG stays higher because the optimal policies also do well with respect to a narrower goal.

directedness with respect to their own actions (given an expressive enough class of utility functions). This means that, for example, it would not be very insightful to compute the goal-directedness of a language model with respect to the text it outputs 7. At the other extreme, if a policy is highly goal-directed towards some variable \(T\) that we do not think to measure, MEG may be misleadingly low. Relatedly, MEG may also be affected by whether we use a binary variable for \(T\) (or the decisions \(D\)) rather than a fine-grained one with many possible outcomes. We should, therefore, think of MEG as measuring what _evidence_ a set of variables provides about a policy's goal-directedness. A lack of evidence does not necessarily mean a lack of goal-directedness.

Footnote 7: Any proposed way of measuring goal-directedness needs a way to avoid the trivial result where all policies are seen to be maximising an idiosyncratic utility function that ‘overfits’ to that policy’s behaviour. MEG avoids this by allowing us to measure goal-directedness with respect to a set of variables that excludes the system’s actions (for example, states in an MDP). An alternative approach is to penalise more complex utility functions.

Distributional shiftsMEG measures how predictive a utility function is of a system's behaviour _on distribution_, and distributional shifts can lead to changes in MEG. A consequence of this is that two almost identical policies can be given arbitrarily different goal-directedness scores, for example, if they take different actions in the start state of an MDP and thus spend time in different regions of the state space. It may be that by considering changes to a system's behaviour under interventions, as Kenton et al. (2023) do, we can distinguish "true" goals from spurious goals, where the former predict behaviour well across distributional shifts, while the latter happen to predict behaviour well on a particular distribution (perhaps because they correlate with true goals) (Di Langosco et al., 2022). We leave this to future work.

Behavioural vs mechanistic approachesThe latter two points suggest that mechanistic approaches to understanding a system's goal-directedness could have advantages over behavioural approaches like MEG. For example, suppose we could reverse engineer the algorithms learned by a neural network. In that case, it may be possible to infer which variables the system is goal-directed with respect to and do so in a way which is robust to distributional shifts. However, mechanistic interpretability of neural networks (Elhage et al., 2021) is an ambitious research agenda that is still in its infancy, and for now, behavioural approaches appear more tractable.

Societal implicationsAn empirical measure of goal-directedness could enable researchers and companies to keep better track of how goal-directed LLMs and other systems are. This is important for dangerous capability evaluations (Shevlane et al., 2023; Phuong et al., 2024) and governance (Shavit et al., 2023). A potential downside is that it could enable bad actors to create more dangerous systems by optimising for goal-directedness. We judge that this does not contribute much additional risk, given that bad actors can already optimise a system to pursue actively harmful goals.

## 8 Conclusion

We proposed maximum entropy goal-directedness (MEG), a formal measure of goal-directedness in CIDs and CBNs, grounded in the philosophical literature and the maximum causal entropy principle. Developing such measures is important because many risks associated with advanced artificial intelligence come from goal-directed behaviour. We proved that MEG satisfies several key desiderata, including scale invariance and being zero with respect to variables that can't be influenced, and that it gives insights about instrumental goals. On the practical side, we adapted algorithms from the maximum causal entropy framework for inverse reinforcement learning to measure goal-directedness with respect to nonlinear utility functions in MDPs. The algorithms handle both a single utility function and a differentiable class of utility functions. The algorithms were used in some small-scale experiments measuring the goal-directedness of various policies in MDPs. In future work, we plan to develop MEG's practical applications further. In particular, we hope to apply MEG to neural network interpretability by measuring the goal-directedness of a neural network agent with respect to a hypothesis class of utility functions taking the network's hidden states as input, thus taking more than just the system's behaviour into account.

Acknowledgements

The authors would like to thank Ryan Carey, David Hyland, Laurent Orseau, Francis Rhys Ward, and several anonymous reviewers for useful feedback. This research is supported by the UKRI Centre for Doctoral Training in Safe and Trusted AI (EP/S023356/1), and by the EPSRC grant "An Abstraction-based Technique for Safe Reinforcement Learning" (EP/X015823/1). Fox acknowledges the support of the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines and Systems (EP/S024050/1).

## References

* Bengio (2023) Yoshua Bengio. AI Scientists: Safe and Useful AI? https://yoshuabengio.org/2023/05/07/ai-scientists-safe-and-useful-ai/, 2023.
* Biehl and Virgo (2022) Martin Biehl and Nathaniel Virgo. Interpreting systems as solving pomdps: a step towards a formal understanding of agency. In _International Workshop on Active Inference_, pages 16-31. Springer, 2022.
* Chan et al. (2023) Alan Chan, Rebecca Salganik, Alva Markelius, Chris Pang, Nitarshan Rajkumar, Dmitrii Krasheninnikov, Lauro Langosco, Zhonghao He, Yawen Duan, Micah Carroll, et al. Harms from increasingly agentic algorithmic systems. In _Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency_, pages 651-666, 2023.
* Dennett (1989) Daniel C Dennett. _The intentional stance_. MIT press, 1989.
* Dennett (2017) Daniel C Dennett. _From bacteria to Bach and back: The evolution of minds_. WW Norton & Company, 2017.
* Dennett (2023) Daniel C Dennett. The problem with counterfeit people. _The Atlantic_, 16, 2023.
* Di Langosco et al. (2022) Lauro Langosco Di Langosco, Jack Koch, Lee D Sharkey, Jacob Pfau, and David Krueger. Goal misgeneralization in deep reinforcement learning. In _International Conference on Machine Learning_, pages 12004-12019. PMLR, 2022.
* Elhage et al. (2021) Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 1(1):12, 2021.
* Everitt et al. (2021a) Tom Everitt, Ryan Carey, Eric D Langlois, Pedro A Ortega, and Shane Legg. Agent incentives: A causal perspective. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 11487-11495, 2021a.
* Everitt et al. (2021b) Tom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna. Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective. _Synthese_, 198 (Suppl 27):6435-6467, 2021b.
* Fox et al. (2023) James Fox, Matt MacDermott, Lewis Hammond, Paul Harrenstein, Alessandro Abate, and Michael Wooldridge. On imperfect recall in multi-agent influence diagrams. In Proceedings Nineteenth conference on _Theoretical Aspects of Rationality and Knowledge_, volume 379 of _Electronic Proceedings in Theoretical Computer Science_, pages 201-220, 2023.
* Gabriel et al. (2024) Iason Gabriel, Arianna Manzini, Geoff Keeling, Lisa Anne Hendricks, Verena Rieser, Hasan Iqbal, Nenad Tomasev, Ira Ktena, Zachary Kenton, Mikel Rodriguez, et al. The ethics of advanced AI assistants. _arXiv preprint arXiv:2404.16244_, 2024.
* Gleave and Toyer (2022) Adam Gleave and Sam Toyer. A primer on maximum causal entropy inverse reinforcement learning. 2022.
* Gleave et al. (2020) Adam Gleave, Pedro Freire, Steven Wang, and Sam Toyer. seals: Suite of environments for algorithms that learn specifications. https://github.com/HumanCompatibleAI/seals, 2020.
* Haarnoja et al. (2017) Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In _International conference on machine learning_, pages 1352-1361. PMLR, 2017.
* Haarnoja et al. (2018)Joseph Halpern and Max Kleiman-Weiner. Towards formal definitions of blameworthiness, intention, and moral responsibility. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* Jaynes [1957] Edwin T Jaynes. Information theory and statistical mechanics. _Physical review_, 106(4):620, 1957.
* Kenton et al. [2023] Zachary Kenton, Ramana Kumar, Sebastian Farquhar, Jonathan Richens, Matt MacDermott, and Tom Everitt. Discovering agents. _Artificial Intelligence_, page 103963, 2023.
* Kusner et al. [2017] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. _Advances in neural information processing systems_, 30, 2017.
* MacDermott et al. [2023] Matt MacDermott, Tom Everitt, and Francesco Belardinelli. Characterising decision theories with mechanised causal graphs. _arXiv preprint arXiv:2307.10987_, 2023.
* Ng and Russell [2000] Andrew Y Ng and Stuart Russell. Algorithms for inverse reinforcement learning. _Proc. of 17th International Conference on Machine Learning, 2000_, pages 663-670, 2000.
* Ngo et al. [2022] Richard Ngo, Lawrence Chan, and Soren Mindermann. The alignment problem from a deep learning perspective. _The Twelfth International Conference on Learning Representations_, 2022.
* Oesterheld [2016] Caspar Oesterheld. Formalizing preference utilitarianism in physical world models. _Synthese_, 193(9):2747-2759, 2016.
* Orseau et al. [2018] Laurent Orseau, Simon McGregor McGill, and Shane Legg. Agents and devices: A relative definition of agency. _arXiv preprint arXiv:1805.12387_, 2018.
* Pearl [2009] Judea Pearl. _Causality_. Cambridge university press, 2009.
* Phuong et al. [2024] Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson, et al. Evaluating frontier models for dangerous capabilities. _arXiv preprint arXiv:2403.13793_, 2024.
* Richens and Everitt [2024] Jonathan Richens and Tom Everitt. Robust agents learn causal world models. _The Twelfth International Conference on Learning Representations_, 2024.
* Richens et al. [2022] Jonathan Richens, Rory Beard, and Daniel H Thompson. Counterfactual harm. _Advances in Neural Information Processing Systems_, 35:36350-36365, 2022.
* Schlosser [2019] Markus Schlosser. Agency. In Edward N. Zalta, editor, _The Stanford Encyclopedia of Philosophy_. Metaphysics Research Lab, Stanford University, Winter 2019 edition, 2019.
* Shavit et al. [2023] Yonadav Shavit, Sandhini Agarwal, Miles Brundage, Steven Adler, Cullen O'Keefe, Rosie Campbell, Teddy Lee, Pamela Mishkin, Tyna Eloundou, Alan Hickey, et al. Practices for governing agentic AI systems. _Research Paper, OpenAI, December_, 2023.
* Shevlane et al. [2023] Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, et al. Model evaluation for extreme risks. _arXiv preprint arXiv:2305.15324_, 2023.
* Virgo et al. [2021] Nathaniel Virgo, Martin Biehl, and Simon McGregor. Interpreting dynamical systems as bayesian reasoners. In _Joint European conference on machine learning and knowledge discovery in databases_, pages 726-762. Springer, 2021.
* Wachter et al. [2017] Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. _Harv. JL & Tech._, 31:841, 2017.
* Wang et al. [2024] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. _Frontiers of Computer Science_, 18(6):1-26, 2024.
* Ward et al. [2024a] Francis Ward, Francesca Toni, Francesco Belardinelli, and Tom Everitt. Honesty is the best policy: defining and mitigating AI deception. _Advances in Neural Information Processing Systems_, 36, 2024a.
* Wachter et al. [2024b]Francis Rhys Ward, Matt MacDermott, Francesco Belardinelli, Francesca Toni, and Tom Everitt. The reasons that agents act: Intention and instrumental goals. In _Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems_, pages 1901-1909, 2024b.
* Weirich (2020) Paul Weirich. Causal Decision Theory. In Edward N. Zalta, editor, _The Stanford Encyclopedia of Philosophy_. Metaphysics Research Lab, Stanford University, Winter 2020 edition, 2020.
* Wulfmeier et al. (2015) Markus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse reinforcement learning. _arXiv preprint arXiv:1507.04888_, 2015.
* Xu and Rivera (2024) Dylan Xu and Juan-Pablo Rivera. Towards measuring goal-directedness in AI systems. _arXiv preprint arXiv:2410.04683_, 2024.
* Ziebart (2010) Brian D Ziebart. _Modeling purposeful adaptive behavior with the principle of maximum causal entropy_. Carnegie Mellon University, 2010.
* Ziebart et al. (2010) Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of maximum causal entropy. In _Proceedings of the 27th International Conference on International Conference on Machine Learning_, pages 1255-1262, 2010.

## Appendix A Comparison to Discovering Agents

This paper is inspired by Kenton et al. (2023), who proposed a causal discovery algorithm for identifying agents in causal models, inspired by Daniel Dennett's view of agents as systems "moved by reasons" (Dennett, 1989). Our approach has several advantages over theirs, which we enumerate below.

**Mechanism variables.**Kenton et al. (2023) assume access to a _mechanised structural causal model_, which augments an ordinary causal model with _mechanism variables_ which parameterise distributions of ordinary object-level variables. An agent is defined as a system that adapts to changes in the mechanism of its environment. However, the question of what makes a variable a mechanism is left undefined, and indeed, the same causal model can be expressed either with or without mechanism variables, leading their algorithm to give a different answer. For example, Example 1 has identical causal structure to the mouse example in Kenton et al. (2023), but without any variables designated as mechanisms. Their algorithm says the version with mechanism variables contains an agent while the version without does not, despite them being essentially the same causal model. Figure 4 shows our example depicted as a mechanised structural causal model. We fix this arbitrariness by making our definition in ordinary causal Bayesian networks.

**Utility variables.** Their algorithm assumes that some variables in the model represent agents' utilities. We bring this more in line with the philosophical motivation by treating utilities as hypothesised mental states with which we augment our model.

**Predictive accuracy.**Kenton et al. (2023)'s approach formalises Dennett's idea of agents as systems "moved by reasons". We build on this idea but bring it more in line with Dennett's notion of what it means for a system to be moved by a reason - that the reason is useful for predicting its behaviour.

**Gradualist vs Essentialist.** The predictive error viewpoint gives us a continuous measure of goal-directedness rather than a binary notion of agency, which is more befitting of the gradualist view of agents which inspired it.

**Practicality.** Their algorithm is theoretical rather than something that can be applied in practice. Instead, ours is straightforward to implement, as we demonstrate in Section 6. This opens up a range of potential applications, including behavioural evaluations and interpretability of ML models.

**Interventional distributions.** The primary drawback of MEG is that it doesn't necessarily generalise outside of the distribution. Running MEG on interventional distributions may fix this. We leave an extension of MEG to interventional distributions for future work.

## Appendix B Proofs of MEG Properties

**Proposition 3.1** (Translation and scale invariance).: _Let \(M_{1}\) be a CID with utility function \(\mathcal{U}_{1}\), and let \(M_{2}\) be an identical CID but with utility function \(\mathcal{U}_{2}=a\cdot\mathcal{U}_{1}+b\), for some \(a,b\in\mathbb{R}\), with \(a\neq 0\). Then for any policy \(\pi\), \(\mathrm{MEG}_{\mathcal{U}_{1}}(\pi)=\mathrm{MEG}_{\mathcal{U}_{2}}(\pi)\)._

Proof.: Since MEG is defined by maximising over a maximum entropy policy set, showing that two utility functions have the same maximum entropy policy set is enough to show that they result in the same MEG for every policy.

Figure 4: Example 1 can be equally well represented with a CBN (a) or mechanised CBN (b), but Kenton et al. (2023)’s algorithm only identifies an agent in (b). (c) shows the resulting mechanised CID. In contrast, MEG is unchanged between (b) and (c). Note also that the causal discovery algorithm identifies \(T\) as a utility variable, where where MEG adds a new utility child to \(T\).

If \(\pi\in\Pi_{t_{1}}^{\text{maxent}}\), then \(\pi\in\Pi_{t_{1},u_{1}}^{\text{maxent}}\) for some \(u_{1}\in\operatorname{att}(\mathcal{U}_{1})\), i.e. \(\pi\) is a maximum entropy policy satisfying the constraint that \(\mathbb{E}_{\pi}\left[\mathcal{U}_{1}\right]=u_{1}\). \(\pi\) thus also satisfies the constraint that \(\mathbb{E}_{\pi}\left[\mathcal{U}_{2}\right]=u_{2}\coloneqq a\cdot u_{1}+b\). Note that any policy satisfying \(\mathbb{E}_{\pi}\left[\mathcal{U}_{2}\right]=u_{2}\) also satisfies \(\mathbb{E}_{\pi}\left[\mathcal{U}_{1}\right]=u_{1}\) because the map \(x\mapsto a\cdot x+b\) is injective (since \(a\neq 0\)). Thus \(\pi\) must also be a maximum entropy policy satisfying \(\mathbb{E}_{\pi}\left[\mathcal{U}_{2}\right]=u_{2}\), and so \(\pi\in\Pi_{t\mathcal{U}_{2},u_{2}}^{\text{maxent}}\) and thus \(\pi\in\Pi_{t\mathcal{U}_{2}}^{\text{maxent}}\).

The converse is similar. 

**Proposition 3.2** (Bounds).: _Let \(M\) be a CID with utility function \(\mathcal{U}\). Then for any policy \(\pi\) we have \(0\leq\operatorname{MEG}_{\mathcal{U}}(\pi)\leq\sum_{D\in\mathcal{D}}\log(| \operatorname{dom}(D)|)\), with equality in the lower bound if \(\pi\) is the uniform policy, and equality in the upper bound if \(\pi\) is the unique optimal (or anti-optimal) policy with respect to \(\mathcal{U}\)._

Proof.: Recall that

\[\operatorname{MEG}_{\mathcal{U}}(\pi)=\max_{\pi^{\text{maxent}}\in\Pi_{ \mathcal{U}}^{\text{maxent}}}\mathbb{E}_{\pi}\left[\sum_{D\in\bm{D}}\left( \log\pi^{\text{maxent}}(D\mid\textbf{Pa}_{D})-\log\frac{1}{|\operatorname{ dom}(D)\mid}\right)\right].\]

For the lower bound, first note that the uniform policy \(\pi^{\text{unif}}\) is always an element of \(\Pi_{\mathcal{U}}^{\text{maxent}}\), since it is the maximum entropy way to attain a utility of \(\mathbb{E}_{\pi^{\text{unif}}}\left[\mathcal{U}\right]\). It follows that

\[\operatorname{MEG}_{\mathcal{U}}(\pi)\geq \mathbb{E}_{\pi}\left[\sum_{D\in\bm{D}}\left(\log\pi^{\text{unif} }(D\mid\textbf{Pa}_{D})-\log\frac{1}{|\operatorname{dom}(D)\mid}\right)\right]\] (4) \[=\mathbb{E}_{\pi}\left[\sum_{D\in\bm{D}}\left(\log\frac{1}{| \operatorname{dom}(D)\mid}-\log\frac{1}{|\operatorname{dom}(D)\mid}\right)\right]\] (5) \[=0,\] (6)

which shows both the lower bound and the fact that we attain it when \(\pi=\pi^{\text{unif}}\).

For the upper bound, note that the \(\log\pi^{\text{maxent}}(D\mid\textbf{Pa}_{D})\) term is at most \(0\), so

\[\operatorname{MEG}_{\mathcal{U}}(\pi)\leq \mathbb{E}_{\pi}\left[\sum_{D\in\bm{D}}\left(0-\log\frac{1}{| \operatorname{dom}(D)\mid}\right)\right]\] (7) \[=\sum_{D\in\bm{D}}\log(|\operatorname{dom}(D)|)\] (8)

To see that \(\pi\) being a unique optimal (or anti-optimal) policy is a sufficient condition for attaining the upper bound, note that there always exists a deterministic optimal and anti-optimal policy in a CID, so a unique such policy must be deterministic. Further, \(\pi\) must be in \(\Pi_{t\mathcal{U}}^{\text{maxent}}\), since there is no higher entropy way to get the same expected utility. Then since \(\mathbb{E}_{\pi}\left[\log\pi^{\text{maxent}}(D\mid\textbf{Pa}_{D})\right]=0\), choosing \(\pi^{\text{maxent}}=\pi\) obtains the upper bound.

**Proposition 3.3** (No goal-directedness without causal influence).: _Let \(M=(G,P)\) be a CID with utility function \(\mathcal{U}\) and decision variables \(\bm{D}\) such that, \(\textbf{Desc}(\bm{D})\cap\textbf{Pa}_{\bm{U}}=\emptyset\). Then \(\operatorname{MEG}_{\mathcal{U}}(\bm{D})=0\)._

Proof.: Since \(\mathcal{U}\) is not a descendant of \(\bm{D}\), it follows from the Markov property of causal Bayesian networks that \(\mathcal{U}\perp\bm{D}\mid\textbf{Pa}_{\bm{D}}\). That means all policies achieve the same expected utility \(u\). So, the maximum entropy policy set \(\Pi_{\mathcal{U}}^{\text{maxent}}\) contains only the uniform policy. We get that

\[\operatorname{MEG}_{\mathcal{U}}(\pi)=-\mathbb{E}\left[\sum_{D\in\bm{D}}\log \frac{1}{|\operatorname{dom}(D)\mid}-\log\frac{1}{|\operatorname{dom}(D)\mid} \right]=0.\qed\]

**Theorem 4.1** (Pseudo-terminal goals).: _Let \(M=((\bm{V},\bm{E}),P)\) be a CBN. Let \(\bm{D},\bm{T},\bm{S}\subseteq\bm{V}\) such that \(\bm{D}\perp\bm{T}\mid\bm{S}\). Then \(\mathrm{MEG}_{\bm{T}}(\bm{D})\leq\mathrm{MEG}_{\bm{S}}(\bm{D})\)._

Proof.: We will show that the maximum entropy policy set \(\Pi^{\text{maxent}}_{\bm{U}^{\bm{T}}}\) (where \(\bm{U}^{\bm{T}}\) is the set of all utility functions over \(\bm{T}\)) is a subset of \(\Pi^{\text{maxent}}_{\bm{U}^{\bm{S}}}\), so the maximum predictive accuracy taken over the latter upper bounds the maximum predictive accuracy taken over the former.

Let \(\pi\in\Pi^{\text{maxent}}_{\bm{U}^{\bm{T}}}\), so \(\pi=\pi^{\text{maxent}}_{\bm{U},u}\) for some \(\mathcal{U}^{T}\in\bm{U}^{\bm{T}}\). If we can find a utility function \(\mathcal{U}^{S}\in\bm{U}^{\bm{S}}\) such that for all \(\pi\), \(\mathbb{E}_{\pi}\left[\mathcal{U}^{S}\right]=\mathbb{E}_{\pi}\left[\mathcal{U} ^{T}\right]\), then a maximum entropy policy with \(\mathbb{E}_{\pi}\left[\mathcal{U}^{T}\right]=u\) must also be a maximum entropy policy with \(\mathbb{E}_{\pi}\left[\mathcal{U}^{S}\right]=u\). It would follow that \(\pi\in\Pi^{\text{maxent}}_{\bm{U}^{\bm{T}}}\) and so \(\Pi^{\text{maxent}}_{\bm{U}^{\bm{S}}}\subseteq\Pi^{\text{maxent}}_{\bm{U}^{ \bm{T}}}\).

To construct such a utility function, let \(\mathcal{U}^{\bm{S}}(\bm{s})=\sum_{t}P(\bm{T}=\bm{t}\mid\bm{S}=\bm{s})\mathcal{ U}^{\bm{T}}(t)\). Note that since \(\bm{D}\perp\bm{T}\mid\bm{S}\), \(P(\bm{T}=\bm{t}\mid\bm{S}=\bm{s})\) is not a function of \(\pi\). Then for any \(\pi\),

\[\mathbb{E}_{\pi}\left[\mathcal{U}^{\bm{T}}\right] =\sum_{\bm{t}}P_{\pi}(\bm{t})\mathcal{U}^{\bm{T}}(\bm{t})\] \[=\sum_{\bm{s}}P_{\pi}(\bm{s})\sum_{\bm{t}}P_{\pi}(\bm{t}\mid\bm{s })\mathcal{U}^{\bm{T}}(\bm{t})\] \[=\sum_{\bm{s}}P_{\pi}(\bm{s})\sum_{\bm{t}}P(\bm{t}\mid\bm{s}) \mathcal{U}^{\bm{T}}(\bm{t})\quad\text{ (since }\bm{D}\perp\bm{T}\mid\bm{S})\] \[=\sum_{s}P_{\pi}(\bm{s})\mathcal{U}^{\bm{S}}(\bm{s})\] \[=\mathbb{E}_{\pi}\left[\mathcal{U}^{\bm{S}}\right].\]

## Appendix C Proof of Theorem 5.1

**Lemma C.1**.: _Let \(\pi_{\beta\mathcal{U}}\) denote a soft-optimal with respect to utility function \(\mathcal{U}\), defined as in Theorem 5.1, with rationality parameter \(\beta\). Then for any \(\alpha\in\mathbb{R}\), we have that \(\pi_{\alpha\cdot\beta\mathcal{U}}=\pi_{\beta,\alpha\cdot\mathcal{U}}\)._

Proof.: If either \(\alpha\) or \(\beta\) are equal to \(0\), both policies are uniform and we are done. Otherwise, write \(Q^{\text{soft}}_{\beta\mathcal{U}}\) for the soft-Q function for \(\mathcal{U}\) with rationality parameter \(\beta\). We first show that \(Q^{\text{soft}}_{\beta,\alpha\cdot\mathcal{U}}=\alpha\cdot Q^{\text{soft}}_{ \alpha\cdot\beta\mathcal{U}}\) by backwards induction.

First, \(Q^{\text{soft}}_{\beta,\alpha\cdot\mathcal{U},n}=\mathbb{E}\left[\alpha\cdot U _{n}\mid d_{n},\textbf{pa}_{n}\right]=\alpha\mathbb{E}\left[U_{n}\mid d_{n}, \textbf{pa}_{n}\right]=\alpha\cdot Q^{\text{soft}}_{\alpha\cdot\beta,\mathcal{ U},n}\).

And for \(t<n\), assuming the inductive hypothesis holds for \(t+1\),

\[Q^{\text{soft}}_{\beta,\alpha\cdot\mathcal{U},t}(d_{t}\mid\textbf{ pa}_{t}) =\mathbb{E}\left[\alpha\cdot U_{t}+\frac{1}{\beta}\operatorname{ logsumexp}(\beta\cdot Q^{\text{soft}}_{\beta,\alpha\cdot\mathcal{U},t+1}( \cdot\mid\textbf{pa}_{D_{t+1}}))\middle|d_{t},\textbf{pa}_{t+1}\right]\] \[=\alpha\cdot\mathbb{E}\left[U_{t}+\frac{1}{\alpha\cdot\beta} \operatorname{logsumexp}(\beta\cdot\alpha\cdot Q^{\text{soft}}_{\alpha\cdot \beta,\mathcal{U},t+1}(\cdot\mid\textbf{pa}_{D_{t+1}}))\middle|d_{t},\textbf{ pa}_{t+1}\right]\] \[=\alpha\cdot Q^{\text{soft}}_{\alpha\cdot\beta\mathcal{U},t}.\]

Then, substituting this into the definition of \(\pi_{\alpha\cdot\beta\mathcal{U}}\) we get\[\pi_{\beta,\alpha\,\mathcal{U}} =\text{softmax}\left(\beta\cdot Q_{\beta,\alpha\cdot\mathcal{U}}^{ \text{soft}}\right)\] \[=\text{softmax}\left(\beta\cdot\alpha\cdot Q_{\alpha\cdot\beta \cdot\mathcal{U}}^{\text{soft}}\right)\] \[=\text{softmax}\left(\alpha\cdot\beta\cdot Q_{\alpha\cdot\beta \cdot\mathcal{U}}^{\text{soft}}\right)\] \[=\pi_{\alpha\cdot\beta\,\mathcal{U}}.\]

**Theorem 5.1** (Maximum entropy policy in MDPs).: _Let \(M=(G,P)\) be an MDP with utility function \(\mathcal{U}\), and let \(u\in\operatorname{att}(\mathcal{U})\) be an attainable expected utility. Then there exists a unique maximum entropy policy \(\pi_{u}^{\text{maxent}}\in\Pi_{\mathcal{U},u}^{\text{maxent}}\), and it has the form_

\[\pi_{u,t}^{\text{maxent}}(d_{t}\mid\textbf{pa}_{t})=\pi_{\beta,t}^{\text{soft} }(d_{t}\mid\textbf{pa}_{t}):=\begin{cases}\frac{\exp(\beta\cdot Q_{\beta,t}^{ \text{soft}}(d_{t}\mid\textbf{pa}_{t}))}{\sum_{d^{\prime}\in\operatorname{dom} (D_{t})}\exp(\beta\cdot Q_{\beta,t}^{\text{soft}}(d_{t}^{\prime}\mid\textbf{pa }_{t}))},&\text{if }\beta\neq 0\\ \pi^{\text{unif}}(d_{t}\mid\textbf{pa}_{t}),&\text{if }\beta=0.\end{cases}\] (3)

_where \(\beta=\operatorname{argmax}_{\beta^{\prime}\in\mathbb{R}\cup\{\infty,-\infty \}}\sum_{t}\mathbb{E}_{\pi}\left[\log(\pi_{\beta^{\prime},t}^{\text{soft}}(d_{t }\mid\textbf{pa}_{t}))\right]\)._

Proof.: In an MDP, the expected utility is a linear function of the policy, so the attainable utility set is a closed interval \(\operatorname{att}(\mathcal{U})=[u_{\min},u_{\max}]\). We first consider the case where \(u\in(u_{\min},u_{\max})\).

In this case, we are seeking the maximum entropy policy in an MDP with a linear constraint satisfiable by a full support policy (since \(u\) is an interior point), so we can invoke Ziebart's result on the form of such policies (Ziebart, 2010; Ziebart et al., 2010; Gleave and Toyer, 2022). In particular, our feature is the utility \(\mathcal{U}\). We get that the maximum entropy policy is a soft-Q policy for a utility function \(\beta\cdot\mathcal{U}\) with a rationality parameter of \(1\), where \(\beta=\operatorname{argmax}_{\beta^{\prime}\in\mathbb{R}}\sum_{t}\mathbb{E}_{ \pi}\left[\log(\pi_{\beta^{\prime}}^{\text{soft}}(d_{t}\mid\textbf{pa}_{t}))\right]\). By Lemma C.1 this can be restated as a soft-Q policy for \(\mathcal{U}\) with a rationality parameter of \(\beta\). It follows from Ziebart that \(\beta=\operatorname{argmax}_{\beta^{\prime}\in\mathbb{R}}\pi_{\beta^{\prime}} ^{\text{soft}}\), and allowing \(\beta=\infty\) or \(-\infty\) does not change the argmax.

In the case where \(u\in\{u_{\min},u_{\max}\}\), it's easy to show that the maximum entropy policy which attains \(u\) randomises uniformly between maximal value actions (for \(u_{\max}\)) or minimal value actions (for \(u_{\min}\)). These policies can be expressed as \(\lim_{\beta\to\infty}\pi_{\beta}^{\text{soft}}\) and \(\lim_{\beta\to-\infty}\pi_{\beta}^{\text{soft}}\) respectively. 

## Appendix D Experimental Details

### Tables of results

\begin{tabular}{l c c} \hline  & Known Utility & Unknown Utility \\ \hline \(k=1\) & \(37.8\) & \(34.3\pm 2.6\) \\ \hline \(k=2\) & \(21.4\) & \(32.1\pm 0.5\) \\ \hline \(k=3\) & \(16.8\) & \(33.6\pm 0.5\) \\ \hline \(k=4\) & \(18.9\) & \(35.4\pm 0.6\) \\ \hline \end{tabular}

\begin{tabular}{l c c} \hline  & Known Utility & Unknown Utility \\ \hline \(\varepsilon=0.1\) & \(2.4\) & \(26.1\pm 0.11\) \\ \hline \(\varepsilon=0.2\) & \(1.5\) & \(17.4\pm 0.2\) \\ \hline \(\varepsilon=0.3\) & \(0.95\) & \(11.0\pm 0.25\) \\ \hline \(\varepsilon=0.4\) & \(0.50\) & \(6.2\pm 0.08\) \\ \hline \(\varepsilon=0.5\) & \(0.20\) & \(2.9\pm 0.06\) \\ \hline \(\varepsilon=0.6\) & \(0.04\) & \(1.0\pm 0.003\) \\ \hline \(\varepsilon=0.7\) & \(0.003\) & \(0.10\pm 0.002\) \\ \hline \(\varepsilon=0.8\) & \(0.001\) & \(0.10\pm 0.003\) \\ \hline \(\varepsilon=0.9\) & \(0.008\) & \(0.091\pm 0.007\) \\ \hline \end{tabular}

### Visualising optimal policies for different lengths of goal region.

Figure 4(a) and Figure 4(b) show the occupancy measures for an optimal policy for k=1 and k=4 respectively, where \(k\) is the length of the goal region in squares. Although the goal region is larger in the latter case, the optimal policy consistently aims for the same sub-region. This explains why unknown-utility MEG is higher than MEG with respect to the environment reward. The policy does just as well on a utility function whose goal-region is limited to the nearer goal squares as it does on the environment reward, but fewer policies do well on this utility function, so doing well on it constitutes more evidence for goal-directedness.

### Further details

The experiments were carried out on a personal laptop with the following specs:

* _Hardware model:_ LENOVO20N2000RUK
* _Processor:_ Intel(R) Core(TM) i7-8665U CPU @ 1.90GHz, 2112 Mhz, 4 Core(s), 8 Logical Processor(s)
* _Memory:_ 24.0 GB

We used an environment from the SEALS library8, and adapted an algorithm from the imitation library9. Both are released under the MIT license.

Footnote 8: https://github.com/HumanCompatibleAI/seals

Footnote 9: https://github.com/HumanCompatibleAI/imitation

For information on hyperparameters, see the code.

Figure 5: Occupancy measures

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The contributions of the paper are listed at the end of the introduction, with references to the relevant sections of the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section 7 contains a section dedicated to limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Proofs of all theoretical results are given in the supplementary material.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Details of the experiments are describe in Section 6. Code is included in the supplementary material.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code is included in the supplementary material.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Full details are provided via the code.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report two sigma error bars for unknown-utility MEG experiments. Known-utility MEG experiments are deterministic, so we do not report error bars.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Information on compute resources is given in Appendix D.3.

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted conforms to the code of ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Section 7 contains a section discussing societal impacts.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not release such data or models.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The creators of the libraries we used are credited, the licenses are respected, and explicitly mentioned in Appendix D.3.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.