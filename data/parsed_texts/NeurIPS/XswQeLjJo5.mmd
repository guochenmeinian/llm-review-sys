# Unraveling the Gradient Descent Dynamics of Transformers

 Bingqing Song

University of Minnesota, Twin Cities

song0409@umn.edu

&Boran Han

Amazon Web Services

boranhan@amazon.com

Shuai Zhang

Amazon Web Services

shuaizs@amazon.com

&Jie Ding

University of Minnesota, Twin Cities

dingj@umn.edu

&Mingyi Hong

University of Minnesota, Twin Cities

mhong@umn.edu

The work of B. Song was partially done while interning at Amazon Web Services.

###### Abstract

While the Transformer architecture has achieved remarkable success across various domains, a thorough theoretical foundation explaining its optimization dynamics is yet to be fully developed. In this study, we aim to bridge this understanding gap by answering the following two core questions: (1) Which types of Transformer architectures allow Gradient Descent (GD) to achieve guaranteed convergence? and (2) Under what initial conditions and architectural specifics does the Transformer achieve rapid convergence during training? By analyzing the loss landscape of a single Transformer layer using Softmax and Gaussian attention kernels, our work provides concrete answers to these questions. Our findings demonstrate that, with appropriate weight initialization, GD can train a Transformer model (with either kernel type) to achieve a global optimal solution, especially when the input embedding dimension is large. Nonetheless, certain scenarios highlight potential pitfalls: training a Transformer using the Softmax attention kernel may sometimes lead to suboptimal local solutions. In contrast, the Gaussian attention kernel exhibits a much favorable behavior. Our empirical study further validate the theoretical findings.

## 1 Introduction

Transformer model architectures have become popular in machine learning, delivering remarkable performance across a wide array of tasks. From natural language processing (Vaswani et al., 2017; Beltagy et al., 2020) to computer vision (Dosovitskiy et al., 2020), these models have set new standards in performance and efficiency. Popular models include BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2020), GPT models (Radford et al., 2019; Brown et al., 2020) and ViT (Dosovitskiy et al., 2020). Despite their empirical success, a comprehensive understanding of their optimization process remains elusive. As highlighted in Liu et al. (2020), the training of large Transformers can sometimes result in deteriorated performance. It is therefore critical to develop theoretical insights for researchers and practitioners to better understand the practical performance of Transformers. However, the complexity of their architectures, coupled with the non-convex natureof the associated optimization problems, has made the theoretical analysis of these models very challenging.

The optimization landscape can be pivotal for understanding a certain type of neural network and providing the practical guidance (Liu et al., 2020). Existing literature offers numerous studies on achieving zero-loss solutions in networks with ReLU activation. These studies encompass various network structures, including fully-connected, convolutional, and residual networks, as explored in (Jain et al., 2017), (Jin et al., 2021), and (Danilova et al., 2022). They delve into the analysis of network optimization landscapes and provide assurances of rapid global convergence when using gradient descent (GD) or stochastic gradient descent (SGD) algorithms. For instance, in Du et al. (2019), the authors focus on fully-connected networks and ResNets with smooth activation functions, and they have demonstrated that global convergence can be achieved using GD with a network size proportional to \(\mathcal{O}\big{(}\text{poly}(N)\big{)}\), where \(N\) is the sample size. Similarly, (Allen-Zhu et al., 2019) show that ReLU fully-connected networks with at least \(\mathcal{O}\big{(}\text{poly}(N)\big{)}\) neurons can achieve global convergence using GD or SGD. From a statistical perspective, (Li et al., 2023) have shown that for two-layer ReLU neural networks (with input dimension \(p\)) that admit a sparse subnetwork representation, a sample size of \(O(\log^{4}(p/\delta))\) can guarantee the global convergence with probability at least \(\delta\) using GD. Despite this extensive body of work on traditional architectures, it is not clear what conditions we need (e.g. network size, optimizer, initialization) to ensure training Transformer models to find high-quality solutions.

Compared to traditional deep learning architectures, Transformers incorporate a unique level of intricacy through their attention kernel (Vaswani et al., 2017), which is designed to effectively handle sequence inputs. This mechanism incorporates Softmax activation to the inner products of query and key vectors, and this inherently non-convex operation poses considerable challenges to theoretical analysis. Consequently, existing frameworks for analyzing the convergence of classical deep learning models are not directly applicable to Transformers. Further, many recent works have pointed out that the performance of Transformers depends on a number of factors such as the choice of kernel function, initialization, choice of optimizers, and forms of token embeddings (Huang et al., 2020; Pan and Li, 2023; Shazeer, 2020; Li et al., 2018; Tian et al., 2023). In deep learning, these factors have been studied in a line works. For example, Li et al. (2018) show that the good training performance is not universal ; skip connections have the effect of smoothing the training landscape, and the Adam algorithm tends to follow a more direct trajectory towards optimal solutions compared to SGD. Therefore, it is imperative to understand what kind of conditions, including initialization, network structure, data properties, and optimizer choices, will lead to high-performing Transformers.

In this work, we will delve into the intricacies of attention kernels, discussing both their advantages and limitations in the context of model optimization. The main contributions of this work are threefold.

* We derive the conditions that will make the one-layer Softmax attention Transformer reach global optimality with vanilla gradient descent. The convergence guarantee is largely attributed to the linear layer (\(W^{\dagger}\)) in the attention mechanism.
* We investigate the attention kernel's effectiveness, revealing Gaussian attention achieves zero training loss, while Softmax can lead to non-optimal stationary points.
* Our experiments validate that Softmax attention Transformers converge slower and present more challenging training landscapes than Gaussian counterparts, potentially leading to more local optimal solutions.

## 2 Related Work

A number of research works have focused on the theoretical analysis and interpretation of Transformer models, revealing crucial insights into their practical performance.

Liu et al. (2020) showed that heavy reliance on the residual branch in multi-layer Transformer models can lead to training instability, which amplifies small parameter perturbations, causing significant disturbances in the model's output. In Bhojanapalli et al. (2020), the authors illustrated the existence of a low-rank bottleneck in Transformer models with sufficiently large embedding and hidden size (\(D=d\)). However, this work focuses on the representation ability of large size attention, while falling short of analyzing Transformer models from an optimization perspective. In Noci et al. (2022),the authors explored rank collapse issues in token representations and their impact on training. The authors discussed the origin of the phenomenon of rank collapse and proposed depth-dependent scaling of residual branches as a potential solution. They specifically investigated scenarios where token rank equals one, which can hinder Transformer training. Their findings demonstrate the occurrence of the vanishing gradient issue, however, this work does not comprehensively characterize the vanishing gradient problem throughout the entire training process.

A recent work Wu et al. (2024) analyzes the convergence behavior of shallow Transformer, which builds a convergence theory of shallow Transformer with realistic structure and initialization, but they do not provide roles for different matrices in the convergence. However, the focus of our paper is different from Wu et al. (2024). We not only derive the global convergence analysis (Our Theorem 2), but also investigates the role of different variables in optimization.

Some other works focus on improving the optimization of Transformers empirically. Huang et al. (2020) have proposed an initialization strategy such that no warm-up or layer normalization is needed to train Transformers efficiently; in Shazeer (2020), the GLU variant of token embedding has been showed to be better than plain embedding in the optimization of Transformer models with Softmax attention kernel. It is worth noting that the above works all primarily focus on empirical investigations into the training of Transformer models, lacking a comprehensive theoretical analysis of the underlying mechanisms.

Some recent research has focused on the convergence analysis of Transformer-based models within the in-context learning (ICL) framework. For instance, Huang et al. (2023); Zhang et al. (2023) explores the learning dynamics of a one-layer Transformer with Softmax attention trained via gradient descent to learn linear function classes in-context. However, this line of study primarily addresses the general convergence performance of Transformers within the ICL setting and does not delve into the role of individual variables. More specifically, these works analyze the convergence of in-context training, where a prompt is constructed with all the training samples and a single test sample. The goal of these works is to achieve the zero test loss (in expectation) by optimizing over the loss function modeled by the prompt. On the other hand, our analysis is based on standard empirical loss minimization, which does not involve any prompt construction.

## 3 Notations and Problem Description

In this section, we define the structure of the Transformer model and describe the training problem. We consider a one-layer attention Transformer model with multiple heads and a dataset with \(N\) samples. Each data sample consists of \(n\) discrete tokens, each with embedding dimension \(D\). We denote the dataset as \(\{(X_{i},y_{i})\}_{i=1}^{N}\), where \(X_{i}\in\mathbb{R}^{n\times D}\), and \(y_{i}\in\mathbb{R}^{n}\) is the label of the dataset. The output from the Transformer model is the prediction of the label. The Transformer structure is formulated as follows:

\[\mathrm{Attention}(W_{h}^{Q},W_{h}^{K},W_{h}^{V};X_{i}):=S(W_{h}^{ Q},W_{h}^{K};X_{i}W_{h}^{V}\] (1) \[\mathsf{MH}(W^{Q},W^{K},W^{V};X_{i}):=\mathrm{Concat}\left(\mathrm{ head}_{1},\ldots,\mathrm{head}_{H}\right)\cdot W^{O},\] \[\text{where }\mathrm{head}_{h}:=\mathrm{Attention}(W_{h}^{Q},W_{h}^ {K},W_{h}^{V};X_{i}),h=1,\cdots,H.\] (2)

In the above notation, \(W_{h}^{Q},W_{h}^{K}\in\mathbb{R}^{D\times d}\) is the query weight matrix and key weight matrix, respectively; \(W_{h}^{V}\in\mathbb{R}^{D\times d}\) is the value weight matrix; these matrices are the main optimization variables throughout the paper. Further \(W^{O}\in\mathbb{R}^{Hd\times 1}\) is a fixed matrix, representing the weight of the output layer; \(H\) is the number of attention heads; \(S(\cdot)\) is a kernel function of variables \(W^{Q},W^{K}\) and input \(X_{i}\). \(\mathrm{Attention}(\cdot)\) is the attention head function; \(\mathsf{MH}(\cdot)\) represents the multi-head attention function. For example, with the Softmax attention Vaswani et al. (2017), \(S(\cdot)\) can be written as:

\[S\left(W_{h}^{Q},W_{h}^{K};X_{i}\right):=\mathrm{Softmax}\left( \frac{X_{i}W_{h}^{Q}\left(X_{i}W_{h}^{K}\right)^{\top}}{\sqrt{d}}\right)\] (3)

where for a given \(n\times n\) matrix \(Z\), \(\mathrm{Softmax}(Z):=[\mathrm{Softmax}(Z_{1}),\cdots,\mathrm{Softmax}(Z_{n})]\). Throughout, let us denote \(S(\cdot)_{kj}\) as the element of \(k\)-th row and \(j\)-th column in matrix \(S(\cdot)\). Let \(X_{ik}\). \(\in\mathbb{R}^{D}\) denote the embedding of the \(k\)-th token in data \(X_{i}\), which is the \(k\)-th row of matrix \(X_{i}\). The structure of Transformer model can be found in Fig 1, where we denote \(S_{ih}:=S\left(W_{h}^{Q},W_{h}^{K};X_{i}\right)\).

Based on the above Transformer model, we consider minimizing the following empirical \(\ell_{2}\) loss function for the entire data set \(\{X_{i},y_{i}\}_{i=1}^{N}\):

\[\min_{M}\frac{1}{2}\sum_{i=1}^{N}\|\mathsf{MH}(M;X_{i})-y_{i}\|^{2},\] (4)

where \(M:=(W^{Q},W^{K},W^{V})\) is the set of variables that can be optimized.

For notation simplicity, next we define the vector version of the Transformer model given in Equation (1), for the entire dataset \(\{(X_{i},y_{i})\}_{i=1}^{N}\). Towards this end, let \(X\in\mathbb{R}^{Nn\times D}\) denote the column-stacked matrix of each single data \(X_{i}\). Similarly, define the stacked label \(y\in\mathbb{R}^{Nn}\). Then we can define:

\[\mathsf{MH}(M;X):=\begin{pmatrix}S_{11}X_{1}&\cdots&S_{1H}X_{1}\\ \cdots&\cdots&\cdots\\ S_{N1}X_{N}&\cdots&S_{NH}X_{N}\end{pmatrix}\cdot\mathrm{diag}(W_{1}^{V}, \cdots,W_{H}^{V})\cdot W^{O},\] (5)

\(i=1,2,\cdots,N,h=1,2,\cdots,H\) for simplicity.

Thus the empirical loss function given in Equation (4) can be simplified as

\[\min_{M}\frac{1}{2}\|\mathsf{MH}(M;X)-y\|^{2}.\] (6)

For more notations in the following sections, we will use subscript \(t\) to represent the variables in \(t\)-th iteration, e.g, \(M_{t}:=\{W_{t}^{Q},W_{t}^{K},W_{t}^{V}\}\). Similarly, we denote \(B_{t}\) as the matrix \(B\) at \(t\)-th iteration.

It is important to note that, in the above description and throughout the paper, we model the Transformer training problem by using a single-layer Transformer, with a regression loss. In practice Transformer models can exhibit greater complexity (different loss functions, multiple layers, etc). For example, the text classification task has an additional mean pooling layer followed by the output of the Transformer structure. Further, they usually contain downstream MLP modules. However, we choose to use the simplified version due to the following reasons:

First, the primary objective of this work is to understand how different attention kernels affect the training dynamics of the Transformers, so we do not include the layer normalization in our model. In fact, in the literature, many works that analyze popular network structures also do not consider layer normalization. For example, in (Huang et al., 2023; Zhang et al., 2023), both analyze the convergence performance of Transformers but normalization is not considered.

Second, we do not include the downstream MLP module in our work since we are interested in the role of self-attention layer in convergence analysis, and the single-attention model is also the standard model used in (Huang et al., 2023; Zhang et al., 2023). Further, the analysis of MLP is standard in literature (Allen-Zhu et al., 2019; Du et al., 2019; Nguyen and Mondelli, 2020). And it is worth noting that our choice to focus on a one-layer Transformer is consistent with other works that similarly aim to investigate the core training dynamics of Transformers, e.g, in (Tian et al., 2023), a single-layer Transformer is considered as a basic model.

Figure 1: One head in Transformer architecture with Soft-max Attention.

Convergence Analysis

In this section, we present our theoretical analysis for solving problem (6). We focus on the behavior of the vanilla GD algorithm for optimizing the variable set \(M\), where \(M\subset\{W^{Q},W^{K},W^{V}\}\). Below we summarize our results.

**Common convergence conditions with Softmax Attention**: When the activation function \(S(\cdot)\) is either the Softmax or Gaussian function, and the embedding dimension \(D\) is at least \(\mathcal{O}(Nn)\), optimizing Equation (6) can achieve a global optimal solution when \(M=\{W^{V}\}\) and \(M=\{W^{Q},W^{K},W^{V}\}\).

**Different behavior between Softmax and Gaussian Kernel Attention**. When \(S(\cdot)\) is Gaussian and the embedding dimension \(D\) is at least \(\mathcal{O}(Nn)\), convergence to global optimal is also ensured for \(M=\{W^{Q}\}\). Interestingly, under the same conditions of large \(D\), convergence to global optimal is _not_ guaranteed when \(S(\cdot)\) is Softmax.

In the subsequent sections, we will elaborate on these convergence results in detail, providing a deeper understanding of the nuances in Transformer behavior under varying configurations. To set up our analysis, we introduce \(\underline{\lambda}^{V}\) as the smallest eigenvalue of \(W^{V}_{0}\), \(\underline{\lambda}^{B}\) as the smallest eigenvalue of \(B_{0}\), \(\bar{\lambda}^{Q}_{h},\bar{\lambda}^{K}_{h},\bar{\lambda}^{V}\) as the largest singular value of matrix \(W^{Q}_{h,0},W^{K}_{h,0},W^{V}\), respectively. We denote \(\|\cdot\|_{2}\) as \(\ell_{2}\) norm and \(\|\cdot\|_{F}\) as Frobenius norm. Further, we denote \(\sigma_{\max}(\cdot)\) and \(\sigma_{\min}(\cdot)\) as the largest and smallest singular value of a matrix, respectively. For any vector \(v\), let \(\min(|v|)\) denote the smallest absolute value of vector \(v\).

### Convergence to global optimal

First, we examine the role of \(W^{V}\) in the optimization of multi-head attention network structure. Our analysis demonstrates that with the hidden dimension \(HD\geq Nn\) and proper initialization, the global optimal solution of (6) can be found using a vanilla gradient descent algorithm. The initialization requires that the matrix \(B_{0}\) has full rank. Our first result shows that, overparameterized Transformer can be trained to global optimal solution.

**Theorem 1**.: _Consider problem (4) with \(S(\cdot)\) being instantiated as the Softmax kernel given in (3). Consider the following update for the variable \(M=\{W^{V}\}\): \(W^{V}_{t+1}=W^{V}_{t}-\eta\nabla_{W^{V}}f(M_{t};X)\), where \(\eta>0\) is the stepsize._

_Suppose \(W^{Q}_{0}\) and \(W^{K}_{0}\) are initialized such that \(\underline{\lambda}^{B}>0\). Then we have:_

\[f\left(M_{t};X\right)\leq\left(1-\eta\alpha\right)^{t}f\left(M_{0};X\right),\] (7)

_where \(\alpha:=\|W^{O}\|^{2}(\underline{\lambda}^{B})^{2}>0\); \(\eta>0\) is defined in Appendix 1.3, and chosen such as \(\eta\alpha<1\)._

**Remark 1**.: _The aforementioned theorem focuses on the convergence behavior when only \(W^{V}\) is being updated. We further elaborate on the initial conditions ensuring \(\underline{\lambda}^{B}>0\)._

_Note that \(\underline{\lambda}^{B}>0\) implies that the objective function \(f\) exhibits a landscape that is nearly convex, which is crucial for optimization. By definition, this condition implies that \(B_{0}\) has full rank, which can be fulfilled by selecting appropriate \(W^{Q}_{0}\) and \(W^{K}_{0}\), plus having large enough embedding size, satisfying \(D\geq Nn/H\). We refer the readers to Appendix 1.3 for the derivation of this condition, which can be guaranteed by random initialization with high probability._

Furthermore, it is important to note that our work aligns with existing literature on the subject of embedding size in Transformer models. For example, in [1], the authors restrict their focus to the simplified case of \(N=1,H=1\). They establish the necessary condition for Softmax attention to overcome its low-rank bottleneck, which requires \(D\geq n\). In our analysis, we derive a similar necessary condition on Transformer model size (\(D\geq n\times(N/H)\)) to guarantee the global convergence when a Transformer model is trained with GD.

In Theorem 1, we have illustrated the case where only updating \(W^{V}\) already leads to global convergence. However, in practice, all parameters \(W^{V},W^{Q},W^{K}\) are updated. This case is more challenging to analyze due to the non-linearity introduced by the Softmax function. Next, we show that a similar result in Theorem 1 still holds when all the parameters are updated simultaneously.

**Theorem 2**.: _Consider problem (4), with \(S(\cdot)\) being instantiated as the Softmax kernel. Consider the GD update where \(M=\{W^{Q},W^{K},W^{V}\}\): Suppose \(\underline{\lambda}^{B}>0\), and the initialization \(M_{0}\) satisfy_

\[\frac{n^{2}\sqrt{NH}\|X\|_{F}^{5}\sum\limits_{h=1}^{H}\left((\bar{\lambda}_{h}^{ Q})^{2}+(\bar{\lambda}_{h}^{K})^{2}\right)\bar{\lambda}^{V}}{\|W^{O}\|_{2}\cdot( \underline{\lambda}^{B})^{2}\min\left(\bar{\lambda}_{h}^{Q},\bar{\lambda}_{h}^ {K},\underline{\lambda}^{B}\right)}\times\|\mathsf{MH}(M_{0};X)-y\|_{2}\leq\nu.\] (8)

_Then there exists stepsize \(\eta>0\), such that_

\[f\left(M_{t};X\right)\leq\left(1-\eta\beta\right)^{t}f\left(M_{0};X\right),\] (9)

_where \(\beta:=\|W^{O}\|^{2}(\underline{\lambda}^{B})^{2}>0\), and the constants \(\eta,\nu\) are defined in Appendix 1.3._

**Remark 2**.: _In the stated theorem, we simplify our analysis by excluding the downstream MLP module in the typical Transformer model, since it is easy to combine the model in Equation (2) with downstream MLP layers. Further, it can be directly showed that the Transformer with MLP will lead to the **same** convergence rate of the optimization problem as updating \(W^{Q},W^{K},W^{V}\) only. To illustrate this, consider the following Transformer model:_

\[G\left(W^{Q},W^{K},W^{V};X_{i}\right)=\mathsf{MH}(W^{Q},W^{K},W^{V};X_{i}) \cdot W^{1}W^{2}\cdots W^{L},\] (10)

_where \(W^{l}\in\mathbb{R}^{n_{l-1}\times n_{l}}\), and \(n_{0}=d^{O}\). Based on the Transformer model defined in Equation (10), we have the following corollary._

**Corollary 1**.: _Consider problem \(\min\limits_{M}\frac{1}{2}\|G(M;X)-y\|^{2}\), with \(G(\cdot)\) being defined in Equation (10) and \(S(\cdot)\) being instantiated as the Softmax kernel. Suppose that the MLP module satisfies:_

\[n_{1}\geq n_{2}\cdots\geq n_{L}.\]

_Consider the following GD update (where \(M=\{W^{Q},W^{K},W^{V},W^{1},\cdots,W^{L}\}\)): Suppose \(\underline{\lambda}^{B}>0\). Then, there exists a step size \(\eta>0\) and initialization weight \(M_{0}\), such that the loss function linearly converges to \(0\)._

**Remark 3**.: _The above theorem and corollary describe the global convergence guarantee when \(W^{Q},W^{K}\) and \(W^{V}\) are updated. This is in line with the insights gained from Theorem 1. However, the conditions for initialization are more stringent, and the optimization landscape becomes inherently more complex due to the involvement of the Softmax attention through \(W^{Q}\) and \(W^{K}\)._

_To ensure the initial condition 8, we have two options: 1) Initializing \(M_{0}\) such that \(\|\mathsf{MH}(M_{0};X)-y\|_{F}\) is small, which implies that the optimization starts in a region close to the global optimal solution and that the initial weight is close to the global optimal solution; 2) Balancing between \(W^{O}\) and \(W^{V}\), in the sense that \(\|W^{O}\|_{2}\) is large and \(\bar{\lambda}^{V}\) is small. For a detailed account of these initialization strategies, please refer to Appendix 1.3._

_Finally, we need to point out that for Transformers with Gaussian kernel attention, we can derive similar convergence results as long as the attention kernel maintains full rank and weights are initialized appropriately. Here we do not include the theoretical statement since it is similar to the result for Softmax attention._

### Softmax vs Gaussian kernel: Softmax attention Transformers may exhibit slower convergence.

In the previous section, we explored the global convergence of training Transformer models. However, from Theorem 2, it was not clear what roles do matrices \(W^{Q}\) and \(W^{K}\) play in the entire convergence process, since Theorem 1 indicates that optimizing \(W^{V}\) alone already ensures the desired convergence. Nevertheless, it is the matrices \(W^{K}\) and \(W^{Q}\) that truly represent the power of a Transformer model, because they are used to extract token correlations.

To study how well a Transformer model can extract the token correlation, in this section, we will study the GD dynamics for Transformer models, where only \(W^{K}\) and \(W^{Q}\) are optimized (while fixing \(W^{V}\)). If optimizing these two parameters alone can still achieve zero training loss, then we claim that the input token correlation can be optimally extracted by the Transformer model.

#### 4.2.1 Notations

To begin our study, let us define that Gaussian kernel to be an \(n\times n\) matrix, where its \(k\)-th row and \(j\)-th column of is given by:

\[S\left(W_{h}^{Q},W_{h}^{K};X_{i}\right)_{kj}=\exp\left(-\frac{1}{\sqrt{d}}\left( X_{ik}.W_{h}^{Q}-X_{ij}.W_{h}^{K}\right)^{2}\right)\] (11)

Since the training dynamics/gradients of variables \(W^{Q}\) and \(W^{K}\) have the same property in (3) and (11), we will only concentrate on optimizing \(W^{Q}\).

With some abuse of notation, define a matrix \(C\) for Softmax attention and Gaussian kernel attention, respectively. Softmax attention: \(C_{ih}:=\frac{X_{i}W_{h}^{Q}\left(X_{i}W_{h}^{K}\right)^{\top}}{\sqrt{d}}\in \mathbb{R}^{n\times n}\).

Gaussian kernel attention: \(C_{ih}\in\mathbb{R}^{n\times n};\;\left(C_{ih}\right)_{kj}=-\frac{\|X_{ik}.W _{h}^{Q}-X_{ij}.W_{h}^{K}\|^{2}}{2\sqrt{d}}\).

For both Softmax attention and Gaussian kernel attention:

\[C_{i}\in\mathbb{R}^{n\times Hn}=\left[C_{i1},C_{i2},\cdots,C_{iH}\right];\;C \in\mathbb{R}^{Nn\times Hn}=\left[C_{1}^{\top},C_{2}^{\top},\cdots,C_{N}^{ \top}\right]^{\top}.\]

Using the above notation, the activation function \(S(\cdot)\) in (3) and (11) can be related to the matrices \(C\)'s in the following manner:

Softmax attention \(:S_{ih}=\operatorname{Softmax}\left(C_{ih}\right),\;\text{Gaussian attention}:\left(S_{ih}\right)_{kj}=\exp\big{(}\left(C_{ih}\right)_{kj}\big{)}\).

Additionally, note that \(C\) is a function of variables \(M\). Therefore we will sometimes use \(C(M)\) when we need to emphasize the dependency of \(C\) on \(M\).

#### 4.2.2 Main Results

Next, we will outline the conditions under which GD can still successfully find global optimal solutions for Transformers with Gaussian kernel attention (when only \(W^{Q}\) is updated), while under the same set of conditions, but with Softmax kernel attention, GD fails.

**Theorem 3**.: _Solve problem (4) with the following GD update (with \(M=\{W^{Q}\}\)): \(W_{t+1}^{Q}=W_{t}^{Q}-\eta\nabla_{W^{Q}}f(M_{t};X)\). Suppose \(\delta_{h}:=\sigma_{\min}(\frac{\partial C(M_{0})}{\partial W_{h}^{Q}})>0,\; \forall\;h\in[1,2,\cdots,H]\), and the initialization condition further satisfies_

\[\frac{n\|X\|_{F}^{5}\left(\bar{\lambda}_{h}^{Q}+\bar{\lambda}_{h}^{K}\right) \exp\left(\frac{9}{4}\|X\|_{F}^{2}\big{(}(\bar{\lambda}_{h}^{Q})^{2}+(\bar{ \lambda}_{h}^{K})^{2}\big{)}\right)}{\big{(}\min(|V^{\prime}W^{O}|)\big{)}^{2} \cdot\min(\delta_{h},\bar{\lambda}_{h}^{Q})}\times\bar{\lambda}^{V}\|W^{O}\|_ {2}\cdot\|\mathsf{MH}\left(M_{0};X\right)-y\|_{2}\leq\nu^{\prime},\] (12)

\(\nu^{\prime}\) _is defined in Appendix_ 1.5_._

_(1) When_ \(S(\cdot)\) _is a Gaussian kernel function, there exists a stepsize_ \(\eta\) _and a positive constant_ \(\gamma\)_, such that_

\[f\left(M_{t};X\right)\leq\left(1-\eta\gamma\right)^{t}f\left(M_{0};X\right),\] (13)

_where_ \(\gamma,\eta\) _are defined in Appendix_ 1.5_._

_(2) When_ \(S(\cdot)\) _is a Softmax function, suppose_ \(W_{t}^{Q}\) _is bounded during the training phase, then there exists stepsize_ \(\eta\)_, such that_

\[f\left(M_{t};X\right)\leq f\left(M_{0};X\right)-\eta^{\prime}\sum_{r=0}^{t-1} \|\nabla_{W^{Q}}f\left(M_{r};X\right)\|^{2},\] (14)

_where_ \(\eta^{\prime}\) _is defined in Appendix_ 1.5_._

**Remark 4**.: _First, it's important to note that the parameter size must satisfy \(Dd\geq Nn^{2}\) for \(\delta>0\) to hold. It is crucial to emphasize the fundamental distinction in convergence outcomes between Transformers employing Gaussian kernel attention and those utilizing Softmax attention under these conditions. With equivalent initialization conditions, training Transformers equipped with Gaussian kernel attention achieves global convergence using gradient descent (GD). Second, it is essential to emphasize that the dimension size \(Dd\geq Nn^{2}\) is similar to the findings of works that have analyzed the convergence performance of over-parameterized neural networks Allen-Zhu et al. (2019); Du et al. (2019). The total number of samples, consisting of \(N\) samples each with \(n\) tokens, can be calculated as \(Nn\). Meanwhile, the total feature dimension is \(Dd\). The inequality implies that the width of theparameters is at least \(\mathcal{O}(N)\), a relationship also illustrated in Nguyen and Mondelli (2020). The proof consists of two basic steps. The first step is to derive the closed form gradient of loss function over variable \(W^{Q}\). Intuitively, the gradient of Softmax attention is much more complicated than the Gaussian attention Transformer, which will lead to a more complicated landscape and more local solutions. The second step is to analyze the gradient of Transformers with both kernels and the same initialization Equation (12). For Gaussian attention Transformer, it can be iteratively shown during the gradient descent training: 1) The variable \(W^{Q}\) is bounded; 2) The PL condition holds (i.e, the optimization landscape remains near-convex); 3) The loss function decreases linearly. For the Softmax attention Transformer, there is no guarantee that the PL condition holds during iterative gradient descent update._

In part (2), we demonstrate that the PL condition does not hold. In particular, we identify an initial solution that satisfies all the conditions given in Theorem 3, yet fails to satisfy the PL condition. Therefore, in this case, GD leads to vanishing gradients without being able to find a global optimal solution. The details of this specific example are provided below.

**Example:** Consider Transformer with Softmax attention, and \(N=1,n=2,H=1\). Let us first write down the close form of the gradient over \(W_{1}^{Q}\):

\[\frac{\partial f\left(M_{0};X_{1}\right)}{\partial W_{1}^{Q}}=\frac{1}{ \sqrt{d}}X_{1}^{\top}\frac{\partial f\left(M_{0};X_{1}\right)}{\partial C_{1 1}}X_{1}W_{1,0}^{K}\]

Next, we show there exists \(W^{O},W^{V},X_{1},W_{1,0}^{Q},W_{1,0}^{K}\) such that the loss function is non-zero with Equation (12) satisfied, while

\[\frac{\partial f\left(M_{0};X_{1}\right)}{\partial C_{11}}=\mathbf{0}\in \mathbb{R}^{2\times 2}.\]

Denote \(L:=\frac{\partial f\left(M_{0};X_{1}\right)}{\partial\mathsf{MH}\left(M_{0}; X_{1}\right)}\left(W^{O}\right)^{\top}\left(X_{1}W_{0}^{V}\right)^{\top}\in \mathbb{R}^{2\times 2}\), \(\frac{\partial f\left(M_{0};X_{1}\right)}{\partial C_{11}}\) can be expressed as follows:/

\[\left(\frac{\partial f\left(M_{0};X_{1}\right)}{\partial C_{11}}\right)_{11}= \delta\cdot\left(L_{11}-L_{12}\right),\;\left(\frac{\partial f\left(M_{0};X_{ 1}\right)}{\partial C_{11}}\right)_{12}=\delta\cdot\left(L_{12}-L_{11}\right),\delta\text{ is some constant.}\]

Next, we will give the value of \(W^{O},W_{0}^{V}\) to show the case where GD leads to vanishing gradient. Let \(D=d=2\), \(W^{O}=(\frac{1}{a},\frac{1}{a}),X_{1}=\begin{pmatrix}1&0\\ 0&1\end{pmatrix}\), and \(W_{0}^{V}=\begin{pmatrix}2a&a\\ a&2a\end{pmatrix}\), where \(a\) is a constant. It is easy to show that there exists \(W_{1}^{Q}\) and \(W_{1}^{K}\) such that Equation (12) holds. Further, it is easy to verify that for this scenario, the following holds:

\[L_{11}=L_{12},L_{21}=L_{22}.\] (15)

Next, we can easily deduce that \(\left(\frac{\partial f\left(M_{0};X_{1}\right)}{\partial C_{11}}\right)_{11}= \left(\frac{\partial f\left(M_{0};X_{1}\right)}{\partial C_{11}}\right)_{12}=0\). Similarly, we can demonstrate that \(\left(\frac{\partial f\left(M_{0};X_{1}\right)}{\partial C_{11}}\right)_{21}= \left(\frac{\partial f\left(M_{0};X_{1}\right)}{\partial C_{11}}\right)_{22}=0\). Consequently, we have \(\frac{\partial f\left(M_{0};X_{1}\right)}{\partial W_{1}^{Q}}=\mathbf{0}\). However, if \(y_{1}\) satisfies that \(\frac{\partial f\left(M_{0};X_{1}\right)}{\partial\mathsf{MH}\left(M_{0};X_{ 1}\right)}\neq\mathbf{0}\), it follows \(f(M_{0};X_{1})\neq 0\), which means \(M_{0}\) is not global optimal solution.

## 5 Experiment: Softmax v.s. Gaussian

In this section, we present numerical results to illustrate the behaviors of Transformers models with Softmax attention and Gaussian kernel attention across various tasks.

### Dataset

We investigate two distinct tasks: Text Classification using the IMDb review dataset (Maas et al., 2011) and Pathfinder (Linsley et al., 2018). While both tasks involve processing long sequences, they exhibit different characteristics. Text Classification is a well-known NLP task that focuses on discerning relationships among token embeddings, while the Pathfinder task prioritizes capturing spatial information within the input pixels.

Figure 2: Test performance on text classification task with different attention kernels

### Model and Experiment Method

We follow the experiment setting in [Chen et al., 2021]. For both tasks, we employ a 2-layer Transformer model with the following specifications: embedding dimension \(D=64\), hidden dimension \(d=128\), and number of attention heads \(H=2\). To align the model with the classification task, we use an additional mean pooling layer as the final layer. We determine the batch size based on available memory constraints. Specifically, we set a batch size of 16 for the Text Classification task with a learning rate of \(1\times 10^{-4}\), and a batch size of 128 for the Pathfinder task with a learning rate of \(2\times 10^{-4}\). For optimization, we use Stochastic Gradient Descent (SGD) for the Text Classification task and Adam for the Pathfinder task. We conduct two types of experiments.

In the first experiment, we plot the test accuracy and test loss within the training steps with both Softmax and Gaussian kernel attention on both tasks. We repeat the training for \(10\) times and make the shadow plot on the test performance.

In our second experiment, the training process consists of two stages: In the first stage, we train the Transformer model equipped with Softmax attention (defined in Equation (3)) for 8,000 steps. In the second stage, we continue training from the pre-trained model for an additional 500 steps, with the option of using either Softmax or Gaussian kernel. To explore the optimization landscape around the trained model, we employed a technique inspired by Li et al. (2018). We select two parameter directions, specifically the \(W^{Q}\) and \(W^{K}\) matrices in the first Transformer layer. These two directions, denoted as \(d_{1},d_{2}\), are centered at the trained model \(M\), and represent the parameter space of \(W^{Q},W^{K}\), respectively. We evaluate the loss function on the set \(\{M+0.02(r-25)d_{1}+0.02(s-25)d_{2}\}\), where \(r,s\in[1,2,\cdots,50]\). The above set is the neighborhood of the trained model \(M\), and we chose the evaluation stepsize as \(0.02\) along the two directions \(d_{1},d_{2}\), with the total steps limit as \(100\). Within this parameter space, we plot a 3-D surface representing the landscape around the trained model.

### Results

#### 5.3.1 Test Loss & Accuracy Curve comparison

To begin with, we present some observations in our first experiment. We plot the test performance of these two tasks on Transformers with two different types of attention. From Fig 2 and Fig 3, we can conclude that in both tasks, Transformers with Gaussian kernel attention exhibit faster convergence and higher test accuracy than Softmax attention with the same model size and learning rate. Especially,

Figure 4: The loss landscapes on text classification task and Pathfinder task. For both tasks, we use the two-stage training in Section 5.2 with the same training hyperparameters, while the only difference is the attention structure in the second training stage. The two axes represent the two directions \(d_{1}\) and \(d_{2}\) as defined in Section 5.2.

Figure 3: Test performance on pathfinder task with different attention kernelstraining Transformers with Softmax attention in the Pathfinder task can lead to unstable performance as indicated in Fig 3. The test accuracy has a significantly higher variance at the same training epoch. Further, the worst test accuracy after \(20,000\) epochs is around \(0.58\) for the Softmax attention Transformer, compared with \(0.62\) for the Gaussian kernel Transformer. These observations align with the experiment results in (Chen et al., 2021) and (Tay et al., 2020), where Transformers with different attention kernels are trained with the same model size and learning rate, while Softmax attention Transformers show instability in a few tasks.

#### 5.3.2 Optimization Landscape Comparison

In Figure 4, we present a comparison of the optimization landscape between Transformers with Softmax and Gaussian kernel attention. Notably, we observe distinct differences in the training landscapes of these two attention types for both tasks. We follow the visualization method described in Section 5.2. We conduct a visualization of the optimization landscape around the trained models after a two-stage training process, with identical learning rates, network sizes, and training epochs. Keeping all other factors consistent, the disparity in the landscape provides a direct representation of the difference in the attention structure during the optimization procedure. With Softmax attention, the landscape appears more complicated compared with Gaussian kernel attention. This complexity can be interpreted as the presence of a greater number of local optima in the optimization landscape, suggesting that Transformers utilizing Softmax attention may encounter more challenges in reaching global optimal solutions. In contrast, the landscape with the Gaussian kernel is flatter. This observation aligns with our earlier findings in Figure 2 and Figure 3, where Softmax attention exhibited certain convergence issues. These observations also provide empirical evidence supporting our Theorem 3, which reflects in a slightly different perspective the complicated optimization landscape within the Softmax kernel.

## 6 Conclusion and Future Work

In conclusion, our study addresses critical gaps in our understanding of why Transformer models perform exceptionally well in a variety of machine learning tasks. Our work also provides a nuanced understanding of the advantages and disadvantages of using classical Softmax attention in Transformers. We find that while shallow Softmax attention Transformers can achieve global convergence with overparameterization, there are scenarios where this attention structure can lead to local solutions. However, those issues can be mitigated by the Gaussian kernel-based attention. In our work, we need strong initialization and large embedding size, i.e, \(HD\geq Nn\) to obtain the global convergence, which exhibits a gap towards real case. In the future work, we will investigate how to relax the assumptions.

## 7 Acknowledgment

The work of B. Song was partially done while interning at Amazon Web Services. M. Hong holds concurrent appointments as an Amazon Scholar and as a faculty at the University of Minnesota. This paper describes their work performed at Amazon. The work of Jie Ding was supported in part by the Army Research Office Early Career Program Award under grant number W911NF2310315.

## References

* Allen-Zhu et al. (2019) Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization. In _International conference on machine learning_, pages 242-252. PMLR, 2019.
* Beltagy et al. (2020) I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.
* Bhojanapalli et al. (2020) S. Bhojanapalli, C. Yun, A. S. Rawat, S. Reddi, and S. Kumar. Low-rank bottleneck in multi-head attention models. In _International conference on machine learning_, pages 864-873. PMLR, 2020.
* Brown et al. (2020) T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Bhojanapalli et al. (2020)Y. Chen, Q. Zeng, H. Ji, and Y. Yang. Skyformer: Remodel self-attention with gaussian kernel and nystr\(\backslash\)" om method. _Advances in Neural Information Processing Systems_, 34:2122-2135, 2021.
* Danilova et al. (2022) M. Danilova, P. Dvurechensky, A. Gasnikov, E. Gorbunov, S. Guminov, D. Kamzolov, and I. Shibaev. Recent theoretical advances in non-convex optimization. In _High-Dimensional Optimization and Probability: With a View Towards Data Science_, pages 79-163. Springer, 2022.
* Devlin et al. (2018) J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Dosovitskiy et al. (2020) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* Du et al. (2019) S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pages 1675-1685. PMLR, 2019.
* He et al. (2020) P. He, X. Liu, J. Gao, and W. Chen. Deberta: Decoding-enhanced bert with disentangled attention. _arXiv preprint arXiv:2006.03654_, 2020.
* Huang et al. (2020) X. S. Huang, F. Perez, J. Ba, and M. Volkovs. Improving transformer optimization through better initialization. In _International Conference on Machine Learning_, pages 4475-4483. PMLR, 2020.
* Huang et al. (2023) Y. Huang, Y. Cheng, and Y. Liang. In-context convergence of transformers. _arXiv preprint arXiv:2310.05249_, 2023.
* Jain et al. (2017) P. Jain, P. Kar, et al. Non-convex optimization for machine learning. _Foundations and Trends(r) in Machine Learning_, 10(3-4):142-363, 2017.
* Jin et al. (2021) C. Jin, P. Netrapalli, R. Ge, S. M. Kakade, and M. I. Jordan. On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points. _Journal of the ACM (JACM)_, 68(2):1-29, 2021.
* Langley (2000) P. Langley. Crafting papers on machine learning. In P. Langley, editor, _Proceedings of the 17th International Conference on Machine Learning (ICML 2000)_, pages 1207-1216, Stanford, CA, 2000. Morgan Kaufmann.
* Li et al. (2023) G. Li, G. Wang, and J. Ding. Provable identifiability of two-layer relu neural networks via lasso regularization. _IEEE Transactions on Information Theory_, 2023.
* Li et al. (2018) H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein. Visualizing the loss landscape of neural nets. _Advances in neural information processing systems_, 31, 2018.
* Linsley et al. (2018) D. Linsley, J. Kim, V. Veerabadaran, C. Windolf, and T. Serre. Learning long-range spatial dependencies with horizontal gated recurrent units. _Advances in neural information processing systems_, 31, 2018.
* Liu et al. (2020) L. Liu, X. Liu, J. Gao, W. Chen, and J. Han. Understanding the difficulty of training transformers. _arXiv preprint arXiv:2004.08249_, 2020.
* Liu et al. (2019) Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* Maas et al. (2011) A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies_, pages 142-150, 2011.
* Nguyen and Mondelli (2020) Q. N. Nguyen and M. Mondelli. Global convergence of deep networks with one wide layer followed by pyramidal topology. _Advances in Neural Information Processing Systems_, 33:11961-11972, 2020.
* Noci et al. (2022) L. Noci, S. Anagnostidis, L. Biggio, A. Orvieto, S. P. Singh, and A. Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. _Advances in Neural Information Processing Systems_, 35:27198-27211, 2022.
* Nemi et al. (2019)Y. Pan and Y. Li. Toward understanding why adam converges faster than sgd for transformers. _arXiv preprint arXiv:2306.00204_, 2023.
* Radford et al. (2019) A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* Shazeer (2020) N. Shazeer. Glu variants improve transformer. _arXiv preprint arXiv:2002.05202_, 2020.
* Tay et al. (2020) Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler. Long range arena: A benchmark for efficient transformers. _arXiv preprint arXiv:2011.04006_, 2020.
* Tian et al. (2023) Y. Tian, Y. Wang, B. Chen, and S. Du. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. _arXiv preprint arXiv:2305.16380_, 2023.
* Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Wu et al. (2024) Y. Wu, F. Liu, G. Chrysos, and V. Cevher. On the convergence of encoder-only shallow transformers. _Advances in Neural Information Processing Systems_, 36, 2024.
* Zhang et al. (2023) R. Zhang, S. Frei, and P. L. Bartlett. Trained transformers learn linear models in-context. _arXiv preprint arXiv:2306.09927_, 2023.

Appendix

### Notations

Recall that we have defined the structure of a single Transformer model in Equation (1) and Equation (5). We will further define a few notations before we introduce a few useful lemmas that are needed in our proof.

(1) Operator: Denote \(\operatorname{vec}(\cdot)\) as the vectorization operator on a matrix; \(\otimes\) as Kronecker product operator; \(\odot\) as the element product. Denote \(\Upsilon(\cdot)\) as a matrix operator, such that for any matrix \(X\) without zero element

\[\Upsilon\left(X_{m\times n}\right)=\left[\begin{array}{ccc}1/x_{11}&\cdots&1 /x_{1n}\\ \vdots&\ddots&\vdots\\ 1/x_{m1}&\cdots&1/x_{mn}\end{array}\right]_{m\times n}\] (16)

(2) Matrix: Denote \(\mathbb{I}\) as the identity matrix. Define matrix \(\mathbb{E}\) and \(E\) as following:

\[\mathbb{E}=\left[\begin{array}{ccc}E&&\\ &\ddots&\\ &&E\end{array}\right]_{Hn\times Hn},\quad E=\left[\begin{array}{ccc}1&\cdots &1\\ \vdots&\ddots&\vdots\\ 1&\cdots&1\end{array}\right]_{n\times n}.\]

Define matrix \(\mathbb{P}_{h}\) as following: \(\mathbb{P}_{h}=\left(\ldots,E_{n\times n}^{h},\ldots\right),\;h=1,\cdots,H\).

(3) Matrix in Transformer: Define the following matrix \(C\) related to the attention layer

Softmax kernel:

\[C_{ih}=\frac{X_{i}W_{h}^{Q}\left(X_{i}W_{h}^{K}\right)^{\top}}{ \sqrt{d}},\;S_{ih}=\operatorname{Softmax}(C_{ih})\] (17) Gaussian kernel: \[\left(C_{ih}\right)_{kj}=-\frac{\left\|X_{ik}.W_{h}^{Q}-X_{ij}.W_{h}^{K} \right\|^{2}}{2\sqrt{d}},\;(S_{ih})_{kj}=\exp\left((C_{ih})_{kj}\right)\] (18) \[C_{i}=\left[C_{i1},\cdots,C_{iH}\right],\;S_{i}=\left[S_{i1}, \cdots,S_{iH}\right]\] (19)

Define matrix \(V_{i}^{\prime}\) for each data \(X_{i}\):

\[V_{i}^{\prime}=\left[\begin{array}{ccc}X_{i}W_{1}^{V}&&\\ &\ddots&\\ &&X_{i}W_{H}^{V}\end{array}\right]_{Hn\times d},\;V=[V_{1}^{\top},\cdots,V_{N} ^{\top}]^{\top}.\] (20)

Next, let us introduce several useful lemma which leads to Theorem 2:

### Lemmas of Theorem 2

**Lemma 1**.: \[(1)\;\frac{\partial f(M;X)}{\partial W^{V}}=B^{\top}\left(\mathsf{MH }(M;X)-y\right)\left(W^{O}\right)^{\top}\] (21) \[(2)\;\operatorname{vec}\left(\frac{\partial f(M;X)}{\partial W^ {V}}\right)=\left\langle(W^{O})^{\top}\otimes B,\operatorname{vec}(\mathsf{ MH}(M;X)-y)\right\rangle\] \[=\left(\mathbb{I}_{Hd}\otimes B^{\top}\right)\cdot\left(W^{O} \otimes\mathbb{I}_{N}\right)\cdot\left(\mathsf{MH}(M;X)-y\right)\] (22) \[(3)\;\frac{\partial f(M;X)}{\partial W_{h}^{Q}}=\frac{1}{\sqrt{d }}X^{\top}\mathbb{P}_{h}\frac{\partial f(M;X)}{\partial C}XW_{h}^{K}=\sum_{i=1 }^{N}\frac{1}{\sqrt{d}}X_{i}^{\top}\mathbb{P}_{h}\frac{\partial f(M;X_{i})}{ \partial C_{i}}X_{i}W_{h}^{K}\] (23) \[(4)\;\frac{\partial f(M;X_{i})}{\partial C_{i}}=\left(\left( \mathsf{MH}(M;X_{i})-y_{i}\right)\left(W^{O}\right)^{\top}\left(V_{i}^{\prime }\right)^{\top}\right)\odot S_{i}\] (24) \[-\left(\left(\left(\mathsf{MH}(M;X_{i})-y_{i}\right)\left(W^{O} \right)^{\top}\left(V_{i}^{\prime}\right)^{\top}\right)\odot S_{i}\odot \Upsilon\big{(}(\exp C_{i})\mathbb{E}\big{)}\right)\mathbb{E}^{\top}\right) \odot\exp C_{i}\] (25) \[(5)\;\frac{\partial f(M;X)}{\partial C}=\operatorname{diag} \left(\frac{\partial f(M;X_{1})}{\partial C_{1}},\cdots,\frac{\partial f(M;X_{N })}{\partial C_{N}}\right)\] (26)

**Remark:** The above lemma derives the closed form of the gradient of objective over \(W^{V},W^{Q}\). Notice that we can derive the derivative of \(W^{K}\) in the same way as \(W^{Q}\) due to symmetry, so we do not include the derivation here. Some of the lemmas here refers https://say-hello2y.github.io/2022-09-07/attention-gradient

**Lemma 2**.: _Consider updating \(W^{Q},W^{K},W^{V}\) at iteration \(t\). Suppose \(\sigma_{\max}(W^{Q})\), \(\sigma_{\max}(W^{K})\), \(\sigma_{\max}(W^{V})\) are bounded during in the optimization phase, then we have the following conclusion:_

\[(1)\;\|d(S_{i})\|_{F}\leq\phi_{i}\|d(W^{Q})\|_{F},\quad\text{where }\phi_{i}=\frac{n}{\sqrt{d}}\left\|X_{i}\right\|_{F}^{2}\sqrt{\sum_{h=1}^{H} \sigma_{\max}^{2}\left(W_{h}^{K}\right)}\] (27) \[(2)\;\|d(S_{i})\|_{F}\leq\psi_{i}\|d(W^{K})\|_{F},\quad\text{where }\psi_{i}= \frac{n}{\sqrt{d}}\left\|X_{i}\right\|_{F}^{2}\sqrt{\sum_{h=1}^{H}\sigma_{\max }^{2}\left(W_{h}^{Q}\right)}.\] (28) \[(3)\;\|d(S_{i})\|_{F}\leq\sqrt{\phi_{i}^{2}+\psi_{i}^{2}}\cdot\|d (W^{Q}),d(W^{K})\|_{F}.\] (29) \[(4)\;\|\frac{\partial f(M;X_{i})}{\partial W^{Q}}\|_{F}\leq Q_{i} \|\mathsf{MH}\left(M;X_{i}\right)-y_{i}\|_{F},\] \[\text{where }Q_{i}=n\sqrt{H}\left\|X_{i}\right\|_{F}^{3}\left\|W^{Q }\right\|_{2}\sqrt{\sum_{h=1}^{H}\sigma_{\max}^{2}\left(W_{h}^{K}\right)} \cdot\sigma_{\max}\left(W^{V}\right).\] (30) \[(5)\;\|\frac{\partial f(M;X_{i})}{\partial W^{K}}\|_{F}\leq K_{i} \|\mathsf{MH}\left(M;X_{i}\right)-y_{i}\|_{F},\] \[\text{where }K_{i}=n\sqrt{H}\left\|X_{i}\right\|_{F}^{3}\left\|W^{Q }\right\|_{2}\sqrt{\sum_{h=1}^{H}\sigma_{\max}^{2}\left(W_{h}^{Q}\right)} \cdot\sigma_{\max}\left(W^{V}\right).\] (31)

**Lemma 3**.: _Consider updating \(W^{Q},W^{K},W^{V}\) at iteration \(t\). Suppose \(\sigma_{\max}(W^{Q})\), \(\sigma_{\max}(W^{K})\), \(\sigma_{\max}(W^{V})\) are bounded during in the optimization phase, then we have the following conclusion:_

\[(1)\;\|\mathsf{MH}(M_{t+1};X)-\mathsf{MH}(M_{t};X)\|_{F}\leq Z\|M_{t+1}-M_{t} \|_{F},\text{where }Z\text{ is some positive constant}.\] (32) \[(2)\;\|\nabla f\left(M_{t+1};X\right)-\nabla f\left(M_{t};X\right) \|_{2}\leq G\|M_{t+1}-M_{t}\|_{F},\text{where }G\text{ is some positive constant}.\] (33)

**Lemma 4**.: _Let \(f:\mathbb{R}^{n}\rightarrow\mathbb{R}\) be a second order differentiable function. Let \(x,y\in\mathbb{R}^{n}\) be given, and assume that \(\|\nabla f(z)-\nabla f(x)\|_{2}\leq C\|z-x\|_{2}\) for every \(z=x+t(y-x)\) with \(t\in[0,1]\). Then,_

\[f(y)\leq f(x)+\langle\nabla f(x),y-x\rangle+\frac{C^{\prime}}{2}\|x-y\|^{2}.\]

**Lemma 5**.: _For matrix \(A\in\mathbb{R}^{k\times l},B\in\mathbb{R}^{l\times m},C\in\mathbb{R}^{m\times n}\)._

\[\operatorname{vec}(ABC) =(I_{n}\otimes AB)\operatorname{vec}(C)=\left(C^{\mathrm{T}}B^{ \mathrm{T}}\otimes I_{k}\right)\operatorname{vec}(A)\] \[\operatorname{vec}(AB) =(I_{m}\otimes A)\operatorname{vec}(B)=\left(B^{\mathrm{T}} \otimes I_{k}\right)\operatorname{vec}(A)\] \[\operatorname{vec}(A\odot B) =\operatorname{vec}(A)\odot\operatorname{vec}(B).\]

### Proof of Theorem 2

**Proof Sketch of Theorem 2:**

The main idea of the proof follows from (Nguyen and Mondelli, 2020). Let us first recall a few notations. \(\bar{\lambda}^{V}:=\frac{2}{3}\big{(}1+\sigma_{\max}(W_{0}^{V})\big{)},\;\underline {\lambda}^{B}:=\sigma_{\min}(B_{0})\). Using GD update rule, we aim to iteratively show

\[\left\{\begin{array}{l}\sigma_{\max}(W_{r}^{V})\leq\frac{3}{2}\bar{\lambda} ^{V},r\in\{0,\ldots,t\},\\ \sigma_{\max}(W_{r}^{Q})\leq\frac{3}{2}\bar{\lambda}^{Q},r\in\{0,\ldots,t\},\\ \sigma_{\max}(W_{r}^{K})\leq\frac{3}{2}\bar{\lambda}^{K},r\in\{0,\ldots,t\}, \\ \sigma_{\min}\left(B_{r}\right)\geq\frac{1}{2}\bar{\lambda}^{B},r\in\{0,\ldots,t\},\\ f\left(M_{r};X\right)\leq(1-\eta\mu)^{r}f\left(M_{0},X\right),\;r\in\{0,\ldots,t\} \end{array}\right.\] (34)Denote \(\mu:=\frac{1}{4}(\Delta^{B})^{2}\|W^{O}\|_{2}^{2}\). Let us discuss about the value of \(\mu\). We know \(W^{O}\in\mathbb{R}^{Hd\times 1}\), \(B_{0}^{\top}\in\mathbb{R}^{HD\times N_{n}}\). We require \(\mu>0\), i.e, \(\underline{\lambda}^{B}>0\), which implies \(B\) has full row rank. For simplicity, let us consider the \(H=1\) case. Recall the definition of \(B\):

\[B:=\left(\begin{array}{c}S_{11}X_{1}\\ \cdots\\ S_{N1}X_{N}\end{array}\right)\]

Suppose we initialize \(W_{1}^{Q},W_{1}^{K}\) such that each \(S_{i1}\in\mathbb{R}^{n\times n}\) is full rank, then we can easily show that \(\mathrm{rank}(S_{i1}X_{i})=n\) if \(X_{i}\) has full row rank. Suppose embedding dimension \(D\) is large, with certain assumption on \(X\), we can show \(B\) has full row rank. For example, if each \(X_{i}\) follows standard Gaussian distribution with \(D>>N\), then \(\mathrm{rank}(B)=Nn\) with probability \(1\) if we initialize \(W_{1}^{Q},W_{1}^{K}\) such that \(S_{i1}\) is full rank.

Further, let us assume that \(\sum\limits_{h=1}^{H}(\bar{\lambda}_{h}^{Q})^{2}>1\), \(\sum\limits_{h=1}^{H}(\bar{\lambda}_{h}^{K})^{2}>1\), and initialization condition satisfies:

\[\frac{54n^{2}\sqrt{NH}\|X\|_{F}^{6}\bar{\lambda}^{V}\big{(}\sum \limits_{h=1}^{H}(\bar{\lambda}_{h}^{Q})^{2}+(\bar{\lambda}_{h}^{K})^{2}\big{)} }{(\underline{\lambda}^{B})^{2}\|W^{O}\|_{2}\min\big{(}\bar{\lambda}_{h}^{Q}, \bar{\lambda}_{h}^{K},1,\underline{\lambda}^{B}\big{)}}\leq 1\] (35)

**Remark 5**.: _The initialization condition can be satisfied if \(\|W^{O}\|_{2}\) is large and \(\sigma_{\max}(W^{V})\) is small. \(\nu\) in Equation (8) is \(\frac{1}{54}\)._

It is clear that Equation (34) holds when \(t=0\). Suppose it holds at iteration \(t\), we prove it holds at iteration \(t+1\).

\[\left\|W_{r+1}^{V}-W_{0}^{V}\right\|_{F}\overset{(i)}{\leq}\sum \limits_{s=0}^{r}\left\|W_{r+1}^{V}-W_{r}^{V}\right\|_{F}=\eta\sum\limits_{s=0 }^{r}\|\nabla_{W^{V}}f\left(M_{t};X\right)\|_{F}\] \[\overset{(ii)}{\leq}\eta\sum\limits_{s=0}^{r}\|B_{r}\|_{F}\|W^{O }\|_{2}\left\|\mathsf{MH}(M_{r};X)-y\right\|_{2}\overset{(iii)}{\leq}\eta\|B_ {r}\|_{F}\|W^{O}\|_{2}\sum\limits_{s=0}^{r}\left(1-\eta\mu\right)^{\text{*/2} }\left\|\mathsf{MH}(M_{0};X)-y\right\|_{2},\]

where (i) uses the triangle inequality; (ii) plugs in the expression of \(\nabla_{W^{V}}f\left(M_{t};X\right)\) and uses the Cauchy-Schwartz inequality; (iii) is because we assume the loss function \(f(\cdot)\) linearly decreases until \(t\)-th iteration. Let \(u=\sqrt{1-\eta\mu}\). So we have

\[\eta\left\|B_{r}\right\|_{F}\left\|W^{O}\right\|_{2}\sum\limits_{ s=0}(1-\eta\mu)^{s/2}\left\|\mathsf{MH}\left(M_{0};X\right)-y\right\|_{2}\] \[\leq\frac{1}{\mu}\|B_{r}\|_{F}\|W^{O}\|\frac{1-u^{r+1}}{1-u}(1-u^ {2})\left\|\mathsf{MH}(M_{0};X)-y\right\|_{2}\] \[=\frac{1}{\mu}\|\left[S_{r,1}X_{1},\cdots,S_{r,N}X_{N}\right]\|_{F }\|W^{O}\|\frac{1-u^{r+1}}{1-u}(1-u^{2})\left\|\mathsf{MH}(M_{0};X)-y\right\| _{2}\] (36) \[\overset{(i)}{\leq}\frac{2n\sqrt{HN}}{\mu}\|X\|_{F}\|W^{O}\|_{2} \|\mathsf{MH}(M_{0};X)-y\|_{F}\overset{(ii)}{\leq}1,\] (37)

where (i) is because each element in \(S_{r,i}\) has magnitude at most \(1\) and \(\|S_{r,i}\|_{F}\leq n\sqrt{H}\), then by Cuachy-Schwartz inequality, we have \(\|B\|_{r}\leq\sqrt{HN}\|X\|_{F}\); (ii) is due to the initialization condition. Then by Weyl's inequality, there is

\[\sigma_{\max}\left(W_{r+1}^{V}\right)\leq\sigma_{\max}(W_{0}^{V})+1=\frac{3}{2} \bar{\lambda}^{V}.\]Similarly, let us derive the upper bound for \(\sigma_{\max}(W_{h,r}^{Q})\).

\[\left\|W_{h,r+1}^{Q}-W_{h,0}^{Q}\right\|_{F}\overset{(i)}{\leq} \sum_{s=0}^{r}\left\|W_{h,r+1}^{Q}-W_{h,r}^{Q}\right\|_{F}=\eta\sum_{s=0}^{r} \left\|\nabla_{W_{h}^{Q}}f\left(M_{t};X\right)\right\|_{F}\] \[\overset{(ii)}{\leq}\eta\sum_{s=0}^{r}\sqrt{\sum_{i=1}^{N}Q_{i}^ {2}}\left\|\mathsf{MH}\left(M_{r};X\right)-y\right\|_{2}\overset{(iii)}{\leq} \eta\sqrt{\sum_{i=1}^{N}Q_{i}^{2}}\sum_{s=0}^{r}(1-\eta\mu)^{s/2}\left\| \mathsf{MH}\left(M_{0};X\right)-y\right\|_{2}\] \[\leq\frac{\sqrt{\sum_{i=1}^{N}Q_{i}^{2}}}{\mu}\frac{1-u^{r+1}}{1- u}\left(1-u^{2}\right)\left\|\mathsf{MH}\left(M_{0};X\right)-y\right\|_{2}\] \[\leq\frac{2\sqrt{\sum_{i=1}^{N}Q_{i}^{2}}}{\mu}\left\|\mathsf{MH }\left(M_{0};X\right)-y\right\|_{2}\overset{(iv)}{\leq}\frac{1}{2}\bar{ \lambda}_{h}^{Q},\]

where (i) uses triangle inequality; (ii) uses Lemma 2 (4); (iii) comes from the assumption that loss function \(f(\cdot)\) linearly decreases until \(t\)-th iteration; (iv) is due to the initialization condition Equation (35). Similarly, we can show

\[\eta\sqrt{\sum_{i=1}^{N}K_{i}^{2}}\sum_{s=0}^{r}(1-\eta\mu)^{s/2} \left\|\mathsf{MH}\left(M_{0};X\right)-y\right\|_{2}\] \[\leq\frac{2\sqrt{\sum_{i=1}^{N}K_{i}^{2}}}{\mu}\left\|\mathsf{MH }\left(M_{0};X\right)-y\right\|_{2}\leq\frac{1}{2}\bar{\lambda}_{h}^{K}.\] (38)

Then by Weyl's inequality, there is

\[\sigma_{\max}\left(W_{h,t+1}^{Q}\right)\leq\sigma_{\max}(W_{h,0}^{Q})+\frac{1} {2}\bar{\lambda}_{h}^{Q}=\frac{3}{2}\bar{\lambda}_{h}^{Q};\;\sigma_{\max}\left( W_{h,t+1}^{K}\right)\leq\sigma_{\max}(W_{h,0}^{K})+\frac{1}{2}\bar{\lambda}_{h}^{K}= \frac{3}{2}\bar{\lambda}_{h}^{K}.\]

Now we aim to bound the eigenvalues of \(B_{r+1}\).

\[\left\|B_{r+1}-B_{0}\right\|_{F}\leq\sum_{s=0}^{r}\left\|B_{s+1}- B_{s}\right\|_{F}=\sum_{i=1}^{N}\sum_{s=0}^{r}\left\|S_{i,s+1}X_{i}-S_{i,s}X_{i} \right\|_{F}\] \[\overset{(i)}{\leq}\eta\sum_{i=1}^{N}\sum_{s=0}^{r}\left\|X_{i} \right\|_{F}\cdot\sqrt{\phi_{i}^{2}+\psi_{i}^{2}}\cdot\left\|\left(\nabla_{W^{ Q}}f(M_{s};X_{i}),\nabla_{W^{K}}f(M_{s};X_{i})\right)\right\|_{F}\;\;,\] \[\overset{(iv)}{\leq}\sum_{i=1}^{N}\sum_{s=0}^{r}\left\|X_{i} \right\|_{F}\cdot\sqrt{\phi_{i}^{2}+\psi_{2}^{2}}\cdot\sqrt{Q_{i}^{2}+K_{i}^{2 }}\cdot\left\|\mathsf{MH}\left(M_{s},X_{i}\right)-y_{i}\right\|_{F}\] \[\overset{(v)}{\leq}\eta\sum_{s=0}^{r}\sqrt{\sum_{i=1}^{N}\left\| X_{i}\right\|_{F}^{2}(\phi_{i}^{2}+\psi_{i}^{2})(Q_{i}^{2}+K_{i}^{2})}(1-\eta\mu)^{s/2} \left\|\mathsf{MH}\left(M_{0};X\right)-y\right\|_{2}\]

where (i) and (ii) uses triangle inequality and Cauchy-Schwartz inequality; (iii) comes from Lemma 2 (5); (iv) uses Lemma 2 and Cauchy-Schwartz inequality; (v) comes from Cauchy-Schwartz inequality. Together with our initialization condition, we have

\[\left\|B_{r+1}-B_{0}\right\|_{F}\leq\frac{1}{\mu}\sqrt{\sum_{i=1} ^{N}\left\|X_{i}\right\|_{F}^{2}(\phi_{i}^{2}+\psi_{i}^{2})(Q_{i}^{2}+K_{i}^{2 })}\cdot\frac{1-u^{r+1}}{1-u}\left\|\mathsf{MH}\left(M_{0};X\right)-y\right\|_ {2}\] \[\leq\frac{2}{\mu}\sqrt{\sum_{i=1}^{N}\left\|X_{i}\right\|_{F}^{2} (\phi_{i}^{2}+\psi_{i}^{2})(Q_{i}^{2}+K_{i}^{2})}\left\|\mathsf{MH}\left(M_{0}; X\right)-y\right\|_{2}\overset{(i)}{\leq}\frac{1}{2}\bar{\lambda}^{B},\]where (i) comes from the initialization condition 35. By Weyl's inequality, we can derive the bound for the singular values of \(B_{t}\):

\[\sigma_{\min}(B_{r+1})\geq\sigma_{\min}(B_{0})-\left\|B_{r+1}-B_{0}\right\|_{F} \geq\frac{1}{2}\lambda^{B}.\]

The final step is to show the last inequality holds. Since we have already showed \(\sigma_{\max}(W_{h}^{Q}),\sigma_{\max}(W_{h}^{K}),\sigma_{\max}(W_{h}^{V})\) are bounded, by Lemma 3 (2) we can conclude that:

\[\left\|\nabla f\left(M_{t+1};X\right)-\nabla f\left(M_{t}\right)\right\|_{2} \leq G\|M_{t+1}-M_{t}\|_{F}\]

Thus by Lemma 4, we choose \(\eta<\frac{1}{2G}\), then the following hold true:

\[f\left(M_{t+1};X\right) =f\left(M_{t}-\eta\nabla f(M_{t};X);X\right)\] \[\overset{(i)}{\leq}f\left(M_{t};X\right)-\eta\left\|\nabla f \left(M_{t};X\right)\right\|^{2}+\frac{G}{2}\eta^{2}\left\|\nabla f\left(M_{t };X\right)\right\|^{2}\] \[\overset{(ii)}{\leq}f\left(M_{t};X\right)-\frac{1}{2}\eta\left\| \nabla f\left(M_{t};X\right)\right\|^{2}\] \[\overset{(iii)}{\leq}f\left(M_{t};X\right)-\frac{1}{2}\eta\left\| \frac{\partial f\left(M_{t};X\right)}{\partial W^{V}}\right\|^{2}\] \[\overset{(iv)}{\leq}f\left(M_{t};X\right)-\frac{1}{2}\eta\|W^{O} \otimes B_{t}^{\top}\left(\operatorname{vec}(\mathsf{MH}(M_{t};X)-y)\right) \|^{2}\] \[\overset{(v)}{\leq}f\left(M_{t};X\right)-\frac{1}{8}\eta\|W^{O} \|_{2}^{2}(\lambda^{B})^{2}\cdot f(M_{t};X)\] \[=(1-\frac{1}{4}\|W^{O}\|_{2}^{2}(\lambda^{B})^{2})\cdot f(M_{t};X)\] \[\overset{(vi)}{=}(1-\eta\mu)f(M_{t};X),\]

where (i) uses Lemma 4; (ii) is because we set \(\eta<\frac{1}{2G}\); (iii) only considers the gradient over \(W^{V}\); (iv) plugs in the closed form gradient in Lemma 1; (v) uses the property of smallest singular value and induction assumption; (vi) comes from the definition of \(\mu\).

### Lemma for Theorem 3

The following lemmas all consider the Transformers with Gaussian kernel attention 11.

**Lemma 6**.: \[(1)\;\frac{\partial f(M;X)}{\partial W^{V}}=B^{\top}\left(\mathsf{MH }(M;X)-y\right)\left(W^{O}\right)^{\top}\] (39) \[(2)\;\operatorname{vec}\left(\frac{\partial f(M;X)}{\partial W^ {V}}\right)=\left\langle(W^{O})^{\top}\otimes B,\operatorname{vec}(\mathsf{ MH}(M;X)-y)\right\rangle\] \[=\left(\mathbb{I}_{Hd}\otimes B^{\top}\right)\cdot\left(W^{O} \otimes\mathbb{I}_{N}\right)\cdot\left(\mathsf{MH}(M;X)-y\right)\] (40) \[(3)\;\frac{\partial f(M;X)}{\partial W^{Q}_{h}}=\frac{\partial f (M;X)}{\partial C}\cdot\frac{\partial C}{\partial W^{Q}_{h}}=\sum_{i=1}^{N} \frac{\partial f(M;X_{i})}{\partial C_{i}}\cdot\frac{\partial C_{i}}{ \partial W^{Q}_{h}}\] (41) \[(4)\;\frac{\partial f(M;X_{i})}{\partial C_{i}}=\left(\left( \mathsf{MH}(M;X_{i})-y_{i}\right)\left(W^{O}\right)^{\top}\left(V^{\prime}_{i }\right)^{\top}\right)\odot S_{i}\] (42) \[(5)\;\frac{\partial f(M;X)}{\partial C}=\left[\frac{\partial f(M ;X_{1})}{\partial C_{1}},\cdots,\frac{\partial f(M;X_{N})}{\partial C_{N}} \right]^{\top}\] (43)

**Lemma 7**.: _Consider updating \(W^{Q},W^{K},W^{V}\) at iteration \(t\). Suppose \(\sigma_{\max}(W^{Q})\), \(\sigma_{\max}(W^{K})\), \(\sigma_{\max}(W^{V})\) are bounded during in the optimization phase, then we have the following conclusion:_

\[(1)\|d(C_{ih})\|_{F}\leq\sqrt{\frac{2n}{d}}\left\|X_{i}\right\|_{F }^{2}\sqrt{\sigma_{\max}^{2}\left(W_{h}^{Q}\right)+\sigma_{\max}^{2}\left(W_{h }^{K}\right)}\|d(W_{h}^{Q})\|_{F}\] (44) \[(2)\left\|d\left(\frac{\partial C_{ih}}{\partial W_{h}^{Q}} \right)\right\|_{F}\leq\sqrt{n}\|X_{i}\|_{F}^{2}\cdot\|d(W_{h}^{Q})\|_{F}.\] (45) \[(3)\|\frac{\partial f(M;X_{i})}{\partial C_{i}}\|_{F}\geq\min|V_{i }W^{O}|\cdot\min S_{i}\cdot\|\mathsf{MH}\left(M;X_{i}\right)-y_{i}\|_{2},\] (46) \[\text{where }R_{i}=\left(\mathsf{MH}\left(M;X_{i}\right)-y_{i} \right)\left(W^{O}\right)^{\top}\left(V_{i}^{\prime\prime}\right)^{\top}.\] (47) \[(4)\left\|\frac{\partial f(M;X_{i})}{\partial W_{h}^{Q}}\right\| _{F}\leq Q_{i}^{\prime}\|\mathsf{MH}(M;X_{i})-y_{i}\|_{2},\] (48) \[Q_{i}^{\prime}=\sqrt{\frac{2n}{d}}\left\|X_{i}\right\|_{F}^{3}\| W^{O}\|_{2}\sigma_{\max}(W^{V})\sqrt{\sigma_{\max}^{2}\left(W_{h}^{Q}\right)+ \sigma_{\max}^{2}\left(W_{h}^{K}\right)}\] (49)

_where \(\min|V^{\prime}W^{O}|\) is the smallest absolute value of each element in vector \(V^{\prime}W^{O}\); \(\min S\) is the smallest element in matrix \(S\)._

**Lemma 8**.: _Consider updating \(W^{Q},W^{K},W^{V}\) at iteration \(t\). Suppose \(\sigma_{\max}(W^{Q})\), \(\sigma_{\max}(W^{K})\), \(\sigma_{\max}(W^{V})\) are bounded during in the optimization phase, then we have the following conclusion:_

\[\|\mathsf{MH}(M_{t+1};X)-\mathsf{MH}(M_{t};X)\|_{F}\leq Z^{\prime}\|M_{t+1}- M_{t}\|_{F},\text{where }Z^{\prime}\text{ is some positive constant.}\] (50) \[\|\nabla f\left(M_{t+1};X\right)-\nabla f\left(M_{t};X\right)\|_{ 2}\leq G^{\prime}\|M_{t+1}-M_{t}\|_{F},\text{where }G^{\prime}\text{ is some positive constant.}\] (51)

### Proof Sketch of Theorem 3.

(1)Using GD update rule, we aim to iteratively show

\[\left\{\begin{array}{l}\sigma_{\max}(W_{r}^{Q})\leq\frac{3}{2}\bar{\lambda} ^{Q},r\in\{0,\ldots,t\},\\ \sigma_{\min}\left(\frac{\partial C_{h}\left(M_{r}\right)}{\partial W_{h}^{Q}} \right)\geq\frac{1}{2}\delta,r\in\{0,\ldots,t\},\\ \min S_{r}\geq\kappa,r\in\{0,\ldots,t\},\\ f\left(M_{r};X\right)\leq(1-\eta\gamma)^{r}f\left(M_{0},X\right),\ r\in\{0, \ldots,t\}\end{array}\right.\] (52)

Denote \(\gamma:=\frac{1}{2}\delta^{2}\kappa^{2}\left(\min\left|V^{\prime}W^{O}\right| \right)^{2}\). Let us discuss about the value of \(\gamma\). We know \(W^{O}\in\mathbb{R}^{Hd^{V}\times 1}\), \(B_{0}^{\top}\in\mathbb{R}^{HD\times Nn}\), where \(Hd>1,HD>Nn\). We require \(\gamma>0\), i.e, \(\delta>0,\kappa>0,\min\left|V^{\prime}W^{O}\right|>0\). It is clear that \(\kappa>0\) can hold as long as \(W_{h}^{Q}\) is bounded. And it is easy to show that if \(X_{i}\neq\mathbf{0}\), we can always choose \(W^{V}\) and \(W^{O}\), such that \(\min\left|V^{\prime}W^{O}\right|>0\). Since \(\frac{\partial C_{h}\left(M\right)}{\partial W_{h}^{Q}}\in\mathbb{R}^{Nn^{2} \times Dd}\), suppose we initialize \(W_{h}^{Q},W_{h}^{K}\) such that \(\mathrm{rank}(\frac{\partial C_{h}\left(M_{0}\right)}{\partial W_{h}^{Q}})=Nn^ {2}\), then we have \(\sigma_{\min}\left(\frac{\partial C_{h}\left(M_{0}\right)}{\partial W_{h}^{Q}} \right)\geq\delta\) for some positive constant \(\delta\). Further, we assume the initialization condition satisfies:

\[\frac{8n\|X\|_{F}^{5}\|W^{O}\|_{2}\bar{\lambda}^{V}(\bar{\lambda} _{h}^{Q}+\bar{\lambda}_{h}^{K})\exp\left(\frac{9}{4}\|X\|_{F}^{2}\left(\left( \bar{\lambda}_{h}^{Q}\right)^{2}+\left(\bar{\lambda}_{h}^{K}\right)^{2}\right) \right)}{\delta^{2}\left(\min\left(|V^{\prime}W^{O}|\right)\right)^{2}\cdot \min\left(\delta,\bar{\lambda}_{h}^{Q}\right)}\left\|\mathrm{MH}\left(M_{0};X \right)-y\right\|_{2}\leq 1\] (53)

**Remark 6**.: _The initialization condition can be satisfied if \(\|W^{O}\|_{2}\) is large and \(\sigma_{\max}(W^{V})\) is small. \(\nu^{\prime}\) in Equation (12) is \(\frac{1}{8}\)._Similar to the proof of Theorem 2, we use induction to prove the theorem. Equation (52) holds when \(t=0\). Suppose it holds at iteration \(t\), we prove it holds at iteration \(t+1\).

\[\left\|W_{h,r+1}^{Q}-W_{h,0}^{Q}\right\|_{F}\stackrel{{ (i)}}{{\leq}}\sum_{s=0}^{r}\left\|W_{h,r+1}^{Q}-W_{h,r}^{Q}\right\|_{F}=\eta \sum_{s=0}^{r}\left\|\nabla_{W_{h}^{Q}}f\left(M_{t};X\right)\right\|_{F}\] \[\stackrel{{(ii)}}{{\leq}}\eta\sum_{s=0}^{r}\sqrt{ \sum_{i=1}^{N}{Q^{\prime}_{i}}^{2}}\left\|\mathsf{MH}(M_{r};X)-y\right\|_{2} \stackrel{{(iii)}}{{\leq}}\eta\sqrt{\sum_{i=1}^{N}{Q^{\prime}_{i }}^{2}}\sum_{s=0}^{r}\left(1-\eta\gamma\right)^{s/2}\left\|\mathsf{MH}(M_{0}; X)-y\right\|_{2},\]

where (i) uses triangle inequality; (ii) comes from Lemma 7 and Cauchy-Schwartz inequality; (iii) is from the induction assumption that loss function \(f(\cdot)\) linearly decreases until \(t\)-th iteration. Let \(u=\sqrt{1-\eta\gamma}\). So we have

\[\left\|W_{h,r+1}^{Q}-W_{h,0}^{Q}\right\|_{F}\leq\eta\sqrt{\sum_{i =1}^{N}{Q^{\prime}_{i}}^{2}}\sum_{s=0}^{r}(1-\eta\gamma)^{s/2}\left\|\mathsf{MH }\left(M_{0};X\right)-y\right\|_{2}\] (54) \[\leq\frac{1}{\gamma}\sqrt{\sum_{i=1}^{N}{Q^{\prime}_{i}}^{2}} \frac{1-u^{r+1}}{1-u}(1-u^{2})\left\|\mathsf{MH}(M_{0};X)-y\right\|_{2}\] \[\leq\frac{2\sqrt{\sum_{i=1}^{N}{Q^{\prime}_{i}}^{2}}}{\gamma} \left\|\mathsf{MH}(M_{0};X)-y\right\|_{F}\stackrel{{(i)}}{{\leq }}\frac{1}{2}\bar{\lambda}_{h}^{Q},\] (55)

where (i) comes from the initialization condition. Then by Weyl's inequality, there is

\[\sigma_{\max}\left(W_{h,t+1}^{Q}\right)\leq\sigma_{\max}(W_{h,0}^ {Q})+\frac{1}{2}\bar{\lambda}_{h}^{Q}=\frac{3}{2}\bar{\lambda}_{h}^{Q}.\] \[\stackrel{{(ii)}}{{\leq}}\eta\sqrt{n}\|X\|_{F}^{2} \sum_{s=0}^{r}\left\|\nabla_{W_{h}^{Q}}f\left(M_{s};X\right)\right\|_{F}\] \[\stackrel{{(iii)}}{{\leq}}\eta\sqrt{n}\|X\|_{F}^{2} \sum_{s=0}^{r}\sqrt{\sum_{i=1}^{N}{Q^{\prime}_{i}}^{2}}\left\|\mathsf{MH} \left(M_{s};X\right)-y\right\|_{2}\] \[\stackrel{{(iv)}}{{\leq}}\eta\sqrt{n}\|X\|_{F}^{2} \sqrt{\sum_{i=1}^{N}{Q^{\prime}_{i}}^{2}}\sum_{s=0}^{r}(1-\eta\gamma)^{s/2} \left\|\mathsf{MH}\left(M_{0};X\right)-y\right\|_{2},\] \[\leq\frac{2}{\gamma}\sqrt{n}\|X\|_{F}^{2}\sqrt{\sum_{i=1}^{N}{Q^ {\prime}_{i}}^{2}}\left\|\mathsf{MH}\left(M_{0};X\right)-y\right\|_{2}\] \[\stackrel{{(v)}}{{\leq}}\frac{1}{2}\delta,\]

where (i) uses triangle inequality; (ii) applies Lemma 7 (2) and Cauchy-Schwartz inequality; (iii) uses Lemma 7 (4); (iv) applies the induction assumption that the loss function \(f(\cdot)\) linearly decreases until \(t\)-th iteration; (v) comes from the initialization condition. Then by Weyl's inequality, there is

\[\sigma_{\max}\left(\frac{\partial{C_{h}}\left(M_{t+1}\right)}{\partial W_{h}^{ Q}}\right)\geq\sigma_{\max}\left(\frac{\partial{C_{h}}\left(M_{0}\right)}{ \partial W_{h}^{Q}}\right)-\frac{1}{2}\delta=\frac{1}{2}\delta.\]

For each element in \(S_{ih}\), we have close form

\[S\left(W_{h}^{Q},W_{h}^{K};X_{i}\right)_{kj}=\exp\left(-\frac{1}{2\sqrt{d}} \left\|X_{ik}\cdot W_{h}^{Q}-X_{ij}\cdot W_{h}^{K}\right\|^{2}\right)\]Since we have already showed that \(\sigma_{\max}\left(W_{h,r}^{Q}\right)\leq\frac{3}{2}\bar{\lambda}_{h}^{Q}\), it follows directly each element in matrix \(S_{t}\) is lower bounded by some constant \(\kappa\) for any \(t\). Now we derive the expression of \(\kappa\):

\[\exp\left(-\frac{1}{2\sqrt{d}}\left\|X_{ik}\cdot W_{h,t}^{Q}-X_{ij} \cdot W_{h}^{K}\right\|^{2}\right)\] \[\overset{(i)}{\geq}\exp\left(-\frac{1}{\sqrt{d}}\big{(}\|X_{ik} \cdot W_{h,t}^{Q}\|^{2}+\|X_{ij}\cdot W_{h}^{K}\|^{2}\big{)}\right)\] \[\overset{(ii)}{\geq}\exp\left(-\frac{1}{\sqrt{d}}\big{(}\frac{9} {4}(\bar{\lambda}_{h}^{Q})^{2}\|X_{ik}\cdot\|^{2}+(\bar{\lambda}_{h}^{K})^{2} \|X_{ij}\cdot\|^{2}\big{)}\right)\] \[\overset{(iii)}{\geq}\exp\left(-\frac{9}{4}\|X\|_{F}^{2}\big{(}( \bar{\lambda}_{h}^{Q})^{2}+(\bar{\lambda}_{h}^{K})^{2}\big{)}\right)\] \[:=\kappa,\]

where (i) uses Cauchy-Schwartz inequality; (ii) applies the induction assumption \(\sigma_{\max}(W_{h,t}^{Q})\leq\frac{3}{2}\bar{\lambda}_{h}^{Q}\) and property of singular value; (iii) is because \(d\geq 1\). Thus, we have \(\min S_{t}\geq\kappa\). Finally, we aim to show \(f\left(M_{t+1};X\right)\leq(1-\eta\gamma)f\left(M_{t},X\right)\). By Lemma 8, since we have showed that \(\sigma_{\max}(W_{h}^{Q})\) is bounded, we can directly derive that

\[\left\|\nabla f\left(M_{t+1};X\right)-\nabla f\left(M_{t};X\right) \right\|_{2}\] \[=\left\|\nabla_{W_{h}^{Q}}f\left(M_{t+1};X\right)-\nabla_{W_{h}^ {Q}}f\left(M_{t};X\right)\right\|_{2}\] \[\leq G^{\prime}\|M_{t+1}-M_{t}\|_{F}\]

Finally, by Lemma 4, choose \(\eta<\frac{1}{2G^{\prime}}\), we have the following holds:

\[f\left(M_{t+1};X\right) =f\left(M_{t}-\eta\nabla f(M_{t};X);X\right)\] \[\overset{(i)}{\leq}f\left(M_{t};X\right)-\eta\left\|\nabla_{W^{ Q}}f\left(M_{t};X\right)\right\|^{2}+\frac{G^{\prime}}{2}\eta^{2}\left\|\nabla_{W ^{Q}}f\left(M_{t};X\right)\right\|^{2}\] \[\overset{(ii)}{\leq}f\left(M_{t};X\right)-\frac{1}{2}\eta\left\| \nabla_{W_{h}^{Q}}f\left(M_{t};X\right)\right\|^{2}\] \[\overset{(iii)}{=}f\left(M_{t};X\right)-\frac{1}{2}\eta\left\| \frac{\partial f(M_{t};X)}{\partial C(M_{t})}\cdot\left(\frac{\partial C(M_{t })}{\partial W_{h}^{Q}}\right)\right\|_{F}^{2}\] \[\overset{(iv)}{\leq}f\left(M_{t};X\right)-\frac{1}{4}\eta\delta^ {2}\left\|\frac{\partial f\left(M_{t};X\right)}{\partial C\left(M_{t}\right)} \right\|_{F}^{2}\] \[\overset{(v)}{\leq}f\left(M_{t};X\right)-\frac{1}{4}\eta\delta^{2} \left\|\left(\left(\mathsf{MH}\left(M;X\right)-y\right)\left(W^{O}\right)^{ \top}\left(V^{\prime}\right)^{\top}\right)\odot S\right\|_{F}^{2}\] \[\overset{(vi)}{\leq}f\left(M_{t};X\right)-\frac{1}{4}\eta\delta ^{2}\kappa^{2}\cdot\left(\min|V^{\prime}W^{O}|\right)^{2}\left\|\mathsf{MH} \left(M_{0};X\right)-y\right\|_{2}^{2}\] \[\overset{(vii)}{=}(1-\eta\gamma)f(M_{t};X),\]

where (i) uses Lemma 4 (2); (ii) is because we choose \(\eta<\frac{1}{2G^{\prime}}\); (iii) writes down the expression of gradient according to chain rule in Lemma 6; (iv) uses the induction assumption \(\sigma_{\max}\left(\frac{\partial C_{h}\left(M_{t+1}\right)}{\partial W_{h}^{ Q}}\right)\geq\frac{1}{2}\delta\) and property of singular value; (v) uses Lemma 6 (4); (vi) comes from Lemma 7 (3); (vii) uses the definition of \(\gamma\).

(2)Next, we show the convergence result for Transformer with Softmax kernel with only \(W^{Q}\) updated. Since we assume parameters are all bounded during optimization phase, by Lemma 8, we can easily show that there exists constant \(G^{\prime}\) (see xx for details), such that

\[\left\|\nabla_{W_{h}^{Q}}f\left(M_{t+1};X\right)-\nabla_{W_{h}^{Q}}f\left(M_{t}; X\right)\right\|_{2}\leq G^{\prime}\left\|M_{t+1}-M_{t}\right\|_{F}\] (56)Then by Lemma 4, choose \(\eta^{\prime}<\frac{1}{2G^{\prime}}\) we have

\[f\left(M_{t+1};X\right) =f\left(M_{t}-\eta\nabla f\left(M_{t};X\right);X\right)\] \[\leq f\left(M_{t};X\right)-\eta^{\prime}\left\|\nabla f\left(M_{t};X \right)\right\|^{2}+\frac{G^{\prime}}{2}\eta^{\prime 2}\left\|\nabla f\left(M_{t};X \right)\right\|^{2}\] \[\leq f\left(M_{t};X\right)-\frac{1}{2}\eta^{\prime}\left\|\nabla f \left(M_{t};X\right)\right\|^{2}\]

### Proof of Lemma in Section 1.2

**Proof of Lemma 2 (1).**

Proof.: **Step 1:** When \(W^{Q},W^{K}\) are updated, we aim to prove

\[\|d(S_{i})\|_{F}\leq n\|d(C_{i})\|_{F}.\]

**Step 2:** We aim to show \(\|d(C_{i})\|_{F}\leq\frac{n}{\sqrt{d}}\left\|X_{i}\right\|_{F}^{2}\sqrt{\sum \limits_{h=1}^{H}\sigma_{\max}^{2}\left(W_{h}^{K}\right)}\cdot\left\|d\left(W ^{Q}\right)\right\|_{F}\). Combine the above two steps, we can derive the bound in Equation (27).

Proof of Step 1: First, we can write down the closed form of the differential of \(S_{i}\):

\[\|d(S_{i})\|_{F}=\|S_{i}\odot d(C_{i})-S_{i}\odot\Upsilon((\exp C_{i})\mathbb{ E})\odot d(\exp(C_{i})\mathbb{E}))\|_{F}\] (57)

We reorganize the terms on the right side of Equation (57), we have the following equation:

\[\|d(S_{i})\|_{F} =\|S_{i}\odot\left(d(C_{i})-\Upsilon((\exp C_{i})\mathbb{E}) \odot d((\exp C_{i})\mathbb{E})\right)\|_{F}\] \[=\|S_{i}\odot\left(d(C_{i})-\Upsilon((\exp C_{i})\mathbb{E}) \odot((\exp C_{i})\odot d(C_{i}))\,\mathbb{E}\right)\|_{F}\] (58)

Since \(C_{i}=[C_{i1},\cdots,C_{iH}]\), we will investigate each \(C_{ih},\ h=1,2,\cdots,H\). We focus on the term \(d(C_{i})-\Upsilon((\exp C_{i})\mathbb{E})\odot(\exp(C_{i})\odot d(C_{i}))\, \mathbb{E}\) in Equation (58). We write down the close form of the element in the \(k\)-th row and \(j\)-th column:

\[[d(C_{ih})-\Upsilon(\exp(C_{ih})\mathbb{E})\odot(\exp(C_{ih}) \odot d(C_{ih}))\,\mathbb{E}]_{kj}\] (59) \[\overset{(i)}{=}\left(1-\frac{\exp\left(C_{ihkj}\right)}{\sum \limits_{j=1}^{n}\exp\left(C_{ihkj}\right)}\right)d(C_{ihkj})-\frac{\sum \limits_{p\neq j}\exp\left(C_{ihkp}\right)d(C_{ihkp})}{\sum\limits_{j=1}^{n} \exp\left(C_{ihkj}\right)}\] (60) \[\overset{(ii)}{\leq}\sqrt{\left(1-\frac{\exp\left(C_{ihkj} \right)}{\sum\limits_{j=1}^{n}\exp\left(C_{ihkj}\right)}\right)^{2}+\sum \limits_{p\neq j}\left(\frac{\exp(C_{ihkp})}{\sum\limits_{j=1}^{n}\exp\left(C_ {ihkj}\right)}\right)^{2}}\cdot\sqrt{\sum\limits_{j=1}^{n}\left(d(C_{ihkj}) \right)^{2}}\] (61) \[\overset{(iii)}{\leq}\sqrt{n}\|d(C_{ihk})\|_{F},\] (62)

where (i) is expand the closed form of Equation (59); (ii) uses the Cauchy-Schwartz inequality; (iii) is because each element in the square root in (ii) is upper bounded by \(1\). With Equation (61), we can easily show

\[\|d(C_{ih})-\Upsilon((\exp C_{ih})\mathbb{E})\odot((\exp C_{ih}) \odot d(C_{ih}))\,\mathbb{E}\|_{F}\leq\sqrt{n}\sqrt{\sum\limits_{k=1}^{n}\sum \limits_{j=1}^{n}\|d(C_{ihk})\|_{F}^{2}}\leq n\|d(C_{ih})\|_{F}\] (63)

Since every element in \(S_{i}\) has magnitude less than \(1\), we have

\[\|d(S_{i})\|_{F}=\|S_{i}\odot(d\left(C_{i}\right)-\Upsilon\left(( \exp C_{i})\,\mathbb{E}\right)\odot((\exp C_{i})\odot d\left(C_{i}\right))\, \mathbb{E})\|_{F}\] (64) \[\leq\|d(C_{ih})-\Upsilon((\exp C_{ih})\mathbb{E})\odot((\exp C_{ih })\odot d(C_{ih}))\,\mathbb{E}\|_{F}\] (65) \[\overset{(i)}{\leq}n\sqrt{H}\|d(C_{i})\|_{F},\] (66)where (i) is from Cauchy-Schawatz inequality.

Proof of Step 2: We aim to show \(\left\|d\left(C_{i}\right)\right\|_{F}\leq\frac{n}{\sqrt{d}}\left\|X_{i}\right\|_ {F}^{2}\sqrt{\sum_{h=1}^{H}\sigma_{\max}^{2}\left(W_{h}^{K}\right)}\cdot\left\|d \left(W^{Q}\right)\right\|_{F}\). Similarly, we investigate \(\left\|d(C_{ih})\right\|_{F},\;h=1,2,\cdots,H\). We have

\[\|d(C_{ih})\|_{F}=\left\|\frac{X_{i}d(W_{h}^{Q})\left(X_{i}W_{h}^{K}\right)^{ \top}}{\sqrt{d}}\right\|_{F}\leq\frac{1}{\sqrt{d}}\|X_{i}\|_{F}^{2}\sigma_{ \max}(W_{h}^{K})\|d(W_{h}^{Q})\|_{F}\] (67)

Then plug the above inequality to Equation (63), we can derive

\[\|d(S_{ih})\|_{F}\leq\frac{n}{\sqrt{d}}\|X_{i}\|_{F}^{2}\sigma_{\max}\left(W_{ h}^{K}\right)\left\|d(W_{h}^{Q})\right\|_{F}\] (68)

Thus by Cauchy-Schwartz inequality, it is easy to show

\[\|d(S_{i})\|_{F}\leq\frac{n}{\sqrt{d}}\|X_{i}\|_{F}^{2}\sqrt{\sum_{h=1}^{H} \sigma_{\max}^{2}\left(W_{h}^{K}\right)}\cdot\left\|d\left(W^{Q}\right)\right\| _{F}.\]

**Proof of Lemma 2 (4).**

Proof.: We first write down the close form of gradient of \(f(\cdot)\) over \(W_{h}^{Q}\) by Lemma 1, and derive the upper bound of the norm of the gradient.

\[\left\|\frac{\partial f(M;X_{i})}{\partial W_{h}^{Q}}\right\|_{F}=\left\|\frac {1}{\sqrt{d}}X_{i}^{\top}\frac{\partial f(M;X_{i})}{\partial C_{i}}\mathbb{P} _{h}^{\top}X_{i}W_{h}^{K}\right\|_{F}\leq\|X_{i}\|_{F}^{2}\sigma_{\max}(W_{h}^ {K})\left\|\frac{\partial f\left(M;X_{i}\right)}{\partial C_{i}}\right\|_{F}\] (69)

By Lemma 1, there is

\[\frac{\partial f(M;X_{i})}{\partial C_{i}}=\left(\left(\mathsf{MH }(M;X_{i})-y_{i}\right)\left(W^{O}\right)^{\top}\left(V_{i}^{\prime}\right)^ {\top}\right)\odot S_{i}\] \[-\left(\left(\left(\mathsf{MH}(M;X_{i})-y_{i}\right)\left(W^{O} \right)^{\top}\left(V_{i}^{\prime}\right)^{\top}\right)\odot S_{i}\odot \Upsilon\big{(}(\exp C_{i})\mathbb{E})\big{)}\right)\mathbb{E}^{\top}\right) \odot\exp C_{i}\] (70)

Denote \(R_{i}=\left(\mathsf{MH}\left(M;X_{i}\right)-y_{i}\right)\left(W^{O}\right)^{ \top}\left(V_{i}^{\prime}\right)^{\top},\;R_{i}=\left[R_{i1},\cdots,R_{iH}\right]\). Write down the close form of the element in the \(k\)-th row and \(j\)-th column:

\[\left[R_{ih}S_{ih}-\left(\left(R_{ih}\odot C_{ih}\odot\Upsilon \left(\exp C_{ih}\right)\mathbb{E}\right)\right)\mathbb{E}^{\top}\right)\odot \left(\exp C_{ih}\right)\right]_{kj}\] \[=R_{ihkj}S_{ihkj}-\frac{\exp(C_{ihkj})\sum\limits_{j=1}^{n}R_{ihkj }S_{ihkj}}{\sum\limits_{j=1}^{n}\exp(C_{ihkj})}\] \[=\left(S_{ihkj}-\frac{\left(\exp C_{ihkj}\right)S_{ihkj}}{\sum \limits_{j=1}^{n}\exp(C_{ihkj})}\right)\cdot R_{ihkj}-\sum\limits_{p\neq j} \frac{\left(\exp C_{ihkp}\right)S_{ihkj}}{\sum\limits_{j=1}^{n}\exp(C_{ihkp})}R _{ihkp}\] \[\overset{(i)}{\leq}\sqrt{\left(1-\frac{\exp\left(C_{ihkj}\right)} {\sum_{j=1}^{n}\exp\left(C_{ihkj}\right)}\right)^{2}+\sum\limits_{p\neq j} \left(\frac{\exp\left(C_{ihkp}\right)}{\sum\limits_{j=1}^{n}\exp\left(C_{ihkj} \right)}\right)^{2}}\cdot\|R_{ihk}\|_{F}\] \[\overset{(ii)}{\leq}\sqrt{n}\|R_{ihk}\|_{F}\]where (1) is due to the Cauchy-Schwartz inequality; (ii) is because each element within the squre root term in (i) has magnitude at most \(1\). Thus, we can further derive

\[\left\|\frac{\partial f\left(M;X_{i}\right)}{\partial C_{ih}}\right\| _{F}=\left\|R_{ih}\odot S_{ih}-\left(\left(R_{ih}\odot S_{ih}\odot\Upsilon((\exp C _{ih})\mathbb{E})\right)\mathbb{E}^{\top}\right)\odot\exp C_{ih}\right\|_{F}\] \[\stackrel{{(i)}}{{\leq}}\sqrt{n}\sum_{k=1}^{n}\sum_{ j=1}^{n}\|R_{ihk}\|_{F}\stackrel{{(ii)}}{{\leq}}n\|R_{ih}\|_{F}\] \[\stackrel{{(iii)}}{{\leq}}n\|X_{i}\|_{F}\|W^{O}\|_{ 2}\sigma_{\max}(W_{h}^{V})\|\mathsf{MH}(M;X_{i})-y_{i}\|_{2},\]

where (i) if from the bound in Equation (71); (ii) comes from Cauchy-Schwatz inwquality; (iii) uses the property of Frobenious norm. Thus, by Cauchy-Schwartz inequality, we can derive the upper bound for \(\left\|\frac{\partial f\left(M;X_{i}\right)}{\partial C_{i}}\right\|_{F}\).

\[\left\|\frac{\partial f\left(M;X_{i}\right)}{\partial C_{i}}\right\|_{F}\leq n \sqrt{H}\|X_{i}\|_{F}\|W^{O}\|_{2}\sigma_{\max}(W^{V})\|\mathsf{MH}(M;X_{i})-y _{i}\|_{2}\] (71)

So plug the above inequality into Equation (69), we can derive the upper bound for \(\left\|\frac{\partial f\left(M;X_{i}\right)}{\partial W_{h}^{Q}}\right\|_{F}\):

\[\left\|\frac{\partial f\left(M;X_{i}\right)}{\partial W_{h}^{Q}} \right\|_{F}\leq\left\|X_{i}\right\|_{F}^{2}\sigma_{\max}\left(W_{h}^{K} \right)\left\|\frac{\partial f\left(M;X\right)}{\partial C_{i}}\right\|_{F}\] \[\leq n\sqrt{H}\left\|X_{i}\right\|_{F}^{3}\left\|W^{O}\right\|_{2} \sigma_{\max}\left(W_{h}^{K}\right)\sigma_{\max}\left(W_{h}^{V}\right)\left\| \mathsf{MH}\left(M;X_{i}\right)-y_{i}\right\|_{2}\] \[\leq n\sqrt{H}\left\|X_{i}\right\|_{F}^{3}\left\|W^{O}\right\|_{2 }\sqrt{\sum_{h=1}^{H}\sigma_{\max}^{2}\left(W_{h}^{K}\right)}\sigma_{\max} \left(W^{V}\right)\left\|\mathsf{MH}\left(M;X_{i}\right)-y_{i}\right\|_{2}\]

**Proof of Lemma 3 (1).** By Mean Value Theorem and Cauchy-Schwartz inequality,

\[\left|f\left(M_{t+1};X_{i}\right)-f\left(M_{t};X_{i}\right)\right|\] \[=\left\langle\frac{\partial f(M_{t}^{\prime};X_{i})}{\partial W},M_{t+1}-M_{t}\right\rangle\] \[\leq\sqrt{\left\|\frac{\partial f(M_{t}^{\prime};X_{i})}{\partial W ^{Q}}\right\|^{2}+\left\|\frac{\partial f(M_{t}^{\prime};X_{i})}{\partial W ^{K}}\right\|^{2}+\left\|\frac{\partial f(M_{t};X_{i})}{\partial W^{V}} \right\|^{2}}\|M_{t+1}-M_{t}\|_{F},\] (72)

where \(M_{t}^{\prime}\) is between \(M_{t}\) and \(M_{t+1}\). We can derive the upper bound of the norm of \(\nabla_{W^{V}}f(M;X_{i})\):

\[\left\|\frac{\partial f(M_{t};X_{i})}{\partial W^{V}}\right\|_{F}=\|B_{i}^{ \top}\left(\mathsf{MH}(M_{t};X_{i})-y_{i}\right)\left(W^{O}\right)^{\top}\|_{F}\]

\[\leq\|B_{i}\|_{F}\|\mathsf{MH}(M_{t};X_{i})-y_{i}\|_{F}\|W^{O}\|_{2}\]

\[\leq n\sqrt{H}\|X_{i}\|_{F}\|W^{O}\|_{2}\|\mathsf{MH}(M_{t};X_{i})-y_{i}\|_{F}\] (73)

By Lemma 2, we know

\[\left\|f\left(M_{t+1};X_{i}\right)-f\left(M_{t};X_{i}\right)\right\|_{2}\leq \sqrt{Q_{i}^{2}+K_{i}^{2}+n^{2}H\sigma_{\max}^{2}(X_{i})\|W^{O}\|^{2}}\|M_{t+1 }-M_{t}\|_{F}\]

\[:=Z_{i}\|M_{t+1}-M_{t}\|_{F}\] (74)

Therefore, together with Equation (73), we have

\[\|f\left(M_{t+1};X\right)-f\left(M_{t};X\right)\|_{2}\leq N\sqrt{\max_{i}Q_{i }^{2}+\max_{i}K_{i}^{2}+n^{2}H\max_{i}\|X_{i}\|_{F}^{2}}\|M_{t+1}-M_{t}\|_{F}\]

\[:=Z\|M_{t+1}-M_{t}\|_{F}\] (75)

**Proof of Lemma 3 (2).**Proof.: By triangle inequality, we have

\[\left\|\nabla_{W}f(M_{t+1};X)-\nabla_{W}f(M_{t};X)\right\|_{F}\] \[\leq\left\|\nabla_{W^{\mathcal{O}}}f(M_{t+1};X)-\nabla_{W^{ \mathcal{O}}}f(M_{t+1};X)\right\|_{F}+\left\|\nabla_{W^{\mathcal{K}}}f(M_{t+1}; X)-\nabla_{W^{\mathcal{K}}}f(M_{t+1};X)\right\|_{F}\] \[\quad+\left\|\nabla_{W^{\mathcal{V}}}f(M_{t+1};X)-\nabla_{W^{ \mathcal{V}}}f(M_{t+1};X)\right\|_{F}\] (76) \[\leq\sum_{i=1}^{N}\big{(}\left\|\nabla_{W^{\mathcal{O}}}f\left(M_{ t+1};X\right)-\nabla_{W^{\mathcal{O}}}f\left(M_{t+1};X\right)\right\|_{F}+\left\| \nabla_{W^{\mathcal{K}}}f\left(M_{t+1};X\right)-\nabla_{W^{\mathcal{K}}}f \left(M_{t+1};X\right)\right\|_{F}\] (77) \[\quad+\left\|\nabla_{W^{\mathcal{V}}}f\left(M_{t+1};X\right)- \nabla_{W^{\mathcal{V}}}f\left(M_{t+1};X\right)\right\|_{F}\big{)}\] (78)

**Step 1:** Derive upper bound for

\[\left\|\nabla_{W^{\mathcal{O}}}f(M_{t+1};X_{i})\right)-\nabla_{W^{\mathcal{O}} }f(M_{t};X_{i}))\right\|_{F}=\|\operatorname{vec}(\nabla_{W^{\mathcal{O}}}f(M _{t+1};X_{i}))-\operatorname{vec}(\nabla_{W^{\mathcal{O}}}f(M_{t};X_{i}))\|_{2}.\]

First, we give the vectorized expression of \(\nabla_{W^{\mathcal{O}}}f\left(M_{t};X_{i}\right)\). Recall we denote \(U_{i}=\left(\left(\mathsf{MH}\left(M;X_{i}\right)-y_{i}\right)\left(W^{O} \right)^{\top}\left(V_{i}^{\prime}\right)^{\top}\right)\odot S_{i}\). By Lemma 1, we can derive the close form of \(\operatorname{vec}(\nabla_{W^{\mathcal{O}}}f(M_{t};X_{i}))\):

\[\operatorname{vec}(\nabla_{W^{\mathcal{O}}}f(M;X_{i}))\overset{( i)}{=}\operatorname{vec}(U_{i})-\operatorname{vec}\left((U_{i}\odot\Upsilon( (\exp C_{i})\mathbb{E}))\right)\mathbb{E}^{\top})\odot\operatorname{vec}(\exp C _{i})\] \[\overset{(ii)}{=}\operatorname{vec}(U_{i})-\left(\mathbb{E} \otimes\mathbb{I}_{n}\right)\operatorname{vec}(U_{i})\odot\operatorname{vec} \left(\Upsilon((\exp C_{i})\mathbb{E})\right)\odot\operatorname{vec}(\exp C _{i})\] \[\overset{(iv)}{=}\operatorname{vec}(U_{i})-\left(\mathbb{E} \otimes\mathbb{I}_{n}\right)\operatorname{vec}(U_{i})\odot\operatorname{vec} (S_{i})\] \[\overset{(v)}{=}\mathbb{I}_{n^{2}H}\operatorname{vec}(U_{i}) \odot\operatorname{vec}(\mathbf{1}_{n}\mathbf{1}_{nH}^{\top})-\left(\mathbb{E }\otimes\mathbb{I}_{n}\right)\operatorname{vec}(U_{i})\odot\operatorname{vec} (S_{i})\] \[\overset{(vi)}{=}\big{(}\mathbb{I}_{n^{2}H}-\left(\mathbb{E} \otimes\mathbb{I}_{n}\right)\big{)}\operatorname{vec}(U_{i})\odot \operatorname{vec}(\mathbf{1}_{n}\mathbf{1}_{nH}^{\top}-S_{i}),\] (79)

where (i) uses the Lemma 1; (ii) and (iii) comes from the property of vectorization in Lemma 5; (vi) uses the definition of \(S_{i}\) ; (v) gives an equivalent expression of \(\operatorname{vec}(U_{i})\); (vi) reorganizes (v). Further, it is easy to verify that:

\[\left\|U_{i}\right\|_{F}=\left\|\left(\left(\mathsf{MH}\left(M;X_ {i}\right)-y_{i}\right)\left(W^{O}\right)^{\top}\left(V_{i}^{\prime}\right)^{ \top}\right)\odot S_{i}\right\|_{F}\leq\left\|R_{i}\right\|_{F}\] \[=\left(\left\|\mathsf{MH}\left(M;X_{i}\right)\right\|_{2}+\|y_{i }\|_{2})\|W^{O}\|_{2}\|X_{i}\|_{F}\sigma_{\max}(W^{V})\] \[\leq\left(n\sqrt{H}\sigma_{\max}(W^{V})\|X_{i}\|_{F}\|W^{O}\|_{2}+ \|y_{i}\|_{2}\right)\left\|W^{O}\right\|_{2}\left\|X_{i}\right\|_{F}\sigma_{ \max}\left(W^{V}\right)\] \[\leq\left(n\sqrt{H}\sigma_{\max}\left(W^{V}\right)\|X\|_{F}\left\| W^{O}\right\|_{2}+\|y\|_{2}\right)\left\|W^{O}\right\|_{2}\left\|X\right\|_{F} \sigma_{\max}\left(W^{V}\right)\] \[:=\bar{R}\] (80)

Next, let us derive upper bound for \(\left\|\nabla_{W^{\mathcal{O}}}f\left(M_{t+1};X_{i}\right)-\nabla_{W^{\mathcal{ O}}}f\left(M_{t+1};X_{i}\right)\right\|_{F}\).

\[\left\|\nabla_{W^{\mathcal{O}}}f(M_{t+1};X_{i})-\nabla_{W^{\mathcal{O}}}f(M_{t+ 1};X_{i})\right\|_{F}\] \[\overset{(i)}{=}\left\|\big{(}\mathbb{I}_{n^{2}H}-\left( \mathbb{E}\otimes\mathbb{I}_{n}\right)\big{)}\big{(}\operatorname{vec}(U_{i,t+1} )\odot\operatorname{vec}(S_{i,t+1})-\operatorname{vec}(U_{i,t})\odot \operatorname{vec}(S_{i,t+1})+\operatorname{vec}(U_{i,t})\odot\operatorname{ vec}(S_{i,t+1})-\operatorname{vec}(U_{i,t})\odot\operatorname{vec}(S)\] \[=\left\|\big{(}\mathbb{I}_{n^{2}H}-\left(\mathbb{E}\otimes \mathbb{I}_{n}\right)\big{)}\big{(}\operatorname{vec}(U_{i,t+1})\odot \operatorname{vec}(S_{i,t+1})-\operatorname{vec}(U_{t})\odot\operatorname{vec}(S_{ i,t+1})+\operatorname{vec}(U_{i,t})\odot\operatorname{vec}(S_{i,t+1})- \operatorname{vec}(U_{i,t})\odot\operatorname{vec}(S)\] \[\overset{(ii)}{\leq}\|\mathbb{I}_{n^{2}H}-\left(\mathbb{E} \otimes\mathbb{I}_{n}\right)\|_{F}\bigg{(}\left\|\operatorname{vec}(U_{i,t+1}-U_{ i,t})\right\|_{F}+\|U_{i,t}\|_{F}\|S_{i,t+1}-S_{i,t}\|_{F}\bigg{)}\] \[\overset{(iii)}{\leq}n\sqrt{H}\left(\left\|\operatorname{vec}(U_{i,t+ 1}-U_{i,t})\right\|_{F}+\bar{R}\left\|S_{i,t+1}-S_{i,t}\right\|_{F}\right)\] \[\overset{(iv)}{=}n\sqrt{H}\big{(}\|R_{i,t+1}\odot S_{i,t+1}-R_{i,t} \odot S_{i,t}\|_{F}+\bar{R}\|S_{i,t+1}-S_{i,t}\|_{F}\big{)}\] \[=n\sqrt{H}\big{(}\|(R_{i,t+1}\odot S_{i,t+1}-R_{i,t}\odot S_{i,t+1}+R _{i,t}\odot S_{i,t+1}-R_{i,t}\odot S_{i,t})\|_{F}+\bar{R}\|S_{i,t+1}-S_{i,t}\| _{F}\big{)}\] \[\overset{(v)}{\leq}n\sqrt{H}\big{(}\|(R_{i,t+1}-R_{i,t})\odot S_{i,t+ 1}\|_{F}+\|R_{i,t}\odot S_{i,t+1}-R_{i,t}\odot S_{t})\|_{F}+\bar{R}\|S_{i +1}-S_{i}\|_{F}\big{)}\] \[\overset{(vi)}{\leq}n\sqrt{H}\big{(}\|R_{i,t+1}-R_{i,t}\|_{F}+\| R_{i,t}\|_{F}\|S_{i,t+1}-S_{i,t}\|_{F}+\bar{R}\|S_{i,t+1}-S_{i,t}\|\big{)},\] (81)where (i) plugs in the expression in Equation (79); (ii) uses the fact that each element in \(S_{i,i+1}\) has magnitude at most \(1\), and Cauchy-Schwartz inequality; (iii) comes from the definition of \(\mathbb{I},\mathbb{E}\) and \(\bar{R}\); (iv) uses the definition of \(U_{i,t}\); (v) is because triangle inequality; (vi) uses the fact that each element in \(S_{i,t+1}\) has magnitude at most \(1\), and Cauchy-Schwartz inequality. Next, we aim to derive upper bound of \(\left\|R_{i,t+1}-R_{i,t}\right\|_{F}\) in Equation (81).

\[\|R_{i,t+1}-R_{i,t}\|_{F}=\left\|\left(\mathsf{MH}(M_{t+1};X_{i}) -y_{i}\right)W^{O}(V^{\prime}_{i,t+1})-\left(\mathsf{MH}(M_{t};X_{i})-y_{i} \right)W^{O}(V^{\prime}_{i,t})\right\|_{F}\] \[=\|\left(\mathsf{MH}(M_{t+1};X_{i})-y_{i}\right)W^{O}(V^{\prime} _{i,t+1})-\left(\mathsf{MH}(M_{t};X_{i})-y_{i}\right)W^{O}(V^{\prime}_{i,t+1})+\] \[\quad\left(\mathsf{MH}(M_{t};X_{i})-y_{i}\right)W^{O}(V^{\prime} _{i,t+1})-\left(\mathsf{MH}(M_{t};X_{i})-y_{i}\right)W^{O}(V^{\prime}_{i,t}) \|_{F}\] \[\stackrel{{(i)}}{{\leq}}\|\left(\mathsf{MH}(M_{t+1} ;X_{i})-\mathsf{MH}(M_{t};X_{i})\right)(V^{\prime}_{i,t+1})W^{O}\|_{F}+\| \left(\mathsf{MH}(M_{t};X_{i})-y_{i}\right)(V^{\prime}_{i,t+1}-V^{\prime}_{i,t })W^{O}\|_{F}\] \[\stackrel{{(ii)}}{{\leq}}Z_{i}\|M_{t+1}-M_{t}\|_{F} \|\|X_{i}\|_{F}\sigma_{\max}(W^{V})\|W^{O}\|_{2}\] \[\quad+\left(\|\mathsf{MH}(M_{t+1};X_{i})\|_{F}+\|y_{i}\|_{2} \right)\|X_{i}\|_{F}\|W^{V}_{t+1}-W^{V}_{t}\|_{F}\|W^{O}\|_{2}\] \[\stackrel{{(iii)}}{{\leq}}Z_{i}\|X_{i}\left\|_{F} \sigma_{\max}\left(W^{V}\right)\right\|W^{O}\|_{2}+\left(n\sqrt{H}\sigma_{ \max}\left(W^{V}\right)\left\|X_{i}\right\|_{F}\left\|W^{O}\right\|_{2}+\left\| y_{i}\right\|_{2}\right)\left\|X_{i}\right\|_{F}\|W^{O}\|_{2}\] \[\quad\times\|M_{t+1}-M_{t}\|_{F}\] \[:=P_{i}\|M_{t+1}-M_{t}\|_{F},\] (82)

where (i) is because of the triangle inequality; (ii) uses the definition of \(Z_{i}\) in Equation (74), Cauchy-Schwartz inequality and triangle inequality; (iii) uses the Cauchy-Schwartz inequality; (iv) reorganizes the terms in (iii). Plug Equation (82) into Equation (81), we can finally derive the bound for \(\left\|\nabla_{W^{Q}}f\left(M_{t+1};X_{i}\right)-\nabla_{W^{Q}}f\left(M_{t+1}; X_{i}\right)\right\|_{F}\).

\[\|\nabla_{W^{Q}}f(M_{t+1};X_{i})-\nabla_{W^{Q}}f(M_{t+1};X_{i}) \|_{F}\] \[\stackrel{{(i)}}{{\leq}}n\sqrt{H}\left(\left\|R_{i, t+1}-R_{i,t}\right\|_{F}+\left\|R_{i,t}\right\|_{F}\left\|S_{i,t+1}-S_{i,t} \right\|_{F}+\bar{R}\left\|S_{i,t+1}-S_{i,t}\right\|\right)\] \[\stackrel{{(ii)}}{{\leq}}n\sqrt{H}P_{i}\|M_{t+1}-M_{ t}\|_{F}+2\bar{R}n\sqrt{H}\|S_{i,t+1}-S_{i,t}\|_{F}\] \[\stackrel{{(iii)}}{{\leq}}n\sqrt{H}P_{i}\|M_{t+1}-M_{ t}\|_{F}+2\bar{R}n\sqrt{H}\sqrt{\phi_{i}^{2}+\psi_{i}^{2}}\|M_{t+1}-M_{t}\|_{F}\] \[:=L^{Q}_{i}\|M_{t+1}-M_{t}\|_{F},\]

where (i) is from Equation (81); (ii) uses the definition of \(\bar{R}\) in Equation (80); (iii) comes from Lemma 3 (3). Since \(W^{Q}\) and \(W^{K}\) are symmetric in the Transformer structure, similarly, we can derive \(L^{K}_{i}\).

**Step 2:** In this step, we aim to derive bound for \(\left\|\nabla_{W^{v}}f\left(M_{t+1};X_{i}\right)-\nabla_{W^{v}}f\left(M_{t};X_{ i}\right)\right\|_{F}\).

\[\left\|\nabla_{W^{V}}f(M_{t+1};X_{i})-\nabla_{W^{V}}f(M_{t};X_{i}) \right\|_{F}\] \[\overset{(i)}{=}\left\|B_{i,t+1}^{\top}\left(\mathsf{MH}\left(M_{t +1};X_{i}\right)-y\right)\left(W^{O}\right)^{\top}-B_{i,t}^{\top}\left(\mathsf{ MH}\left(M_{t};X_{i}\right)-y_{i}\right)\left(W^{O}\right)^{\top}\right\|_{F}\] \[\overset{(ii)}{\leq}\left\|B_{i,t+1}^{\top}\left(\mathsf{MH} \left(M_{t+1};X_{i}\right)-y_{i}\right)\left(W^{O}\right)^{\top}-B_{i,t+1}^{ \top}\left(\mathsf{MH}\left(M_{t};X_{i}\right)-y_{i}\right)\left(W^{O}\right)^ {\top}\right\|_{F}\] \[\quad+\left\|B_{i,t+1}^{\top}\left(\mathsf{MH}\left(M_{t};X_{i} \right)-y_{i}\right)\left(W^{O}\right)^{\top}-B_{i,t}^{\top}\left(\mathsf{MH} \left(M_{t};X_{i}\right)-y_{i}\right)\left(W^{O}\right)^{\top}\right\|_{F}\] \[\overset{(iii)}{\leq}\left\|B_{i,t+1}\right\|_{F}\left\|\left| \mathsf{MH}\left(M_{t+1};X_{i}\right)-\mathsf{MH}\left(M_{t};X_{i}\right) \right\|_{F}\left\|W^{O}\right\|_{2}+\left\|B_{i,t+1}-B_{i,t}\right\|_{F}\| \mathsf{MH}\left(M_{t};X_{i}\right)-y_{i}\|_{F}\|W^{O}\|_{2}\] \[\overset{(iv)}{\leq}n\sqrt{H}\|X_{i}\|_{F}\|W^{O}\|_{2}Z_{i}\|M_{ t+1}-M_{t}\|_{F}+\|S_{i,t+1}-S_{i,t}\|_{F}\|X_{i}\|_{F}\|W^{O}\|_{2}\left(\left\| \mathsf{MH}\left(M_{t+1};X_{i}\right)\right\|_{F}+\|y_{i}\|_{2}\right)\] \[\overset{(v)}{\leq}\sqrt{\phi_{i}^{2}+\psi_{i}^{2}}\left\|X_{i} \right\|_{F}\left\|W^{O}\right\|_{2}\left(n\sqrt{H}\sigma_{\max}\left(W^{V} \right)\|X_{i}\|_{F}\left\|W^{O}\right\|_{2}+\|y_{i}\|_{2}\right)\|M_{t+1}-M_{ t}\|_{F}\] \[\quad+n\sqrt{H}\left\|W^{O}\right\|_{2}\left\|X_{i}\right\|_{F}Z_ {i}\left\|M_{t+1}-M_{t}\right\|_{F}\] \[\overset{(vi)}{\leq}\left(\sqrt{\phi_{i}^{2}+\psi_{i}^{2}}\left\| X_{i}\right\|_{F}\left\|W^{O}\right\|_{2}\left(n\sqrt{H}\sigma_{\max}\left(W^{V} \right)\|X_{i}\|_{F}\left\|W^{O}\right\|_{2}+\|y_{i}\|_{2}\right)+n\sqrt{H} \left\|W^{O}\right\|_{2}Z_{i}\right)\|M_{t+1}-M_{t}\|_{F}\] \[:=L_{i}^{V}\|M_{t+1}-M_{t}\|_{F}\]

where (i) is from Lemma 1 (1); (ii) uses triangle inequality; (iii) uses Cauchy-Schwartz inequality; (iv) comes from the definition of \(B_{i,t}\), \(Z_{i}\)(in Equation (74)), Cauchy-Schwartz inequality and triangle inequality; (v) comes from Lemma 2 (3) and Cauchy-Schwartz inequality; (vi) reorganizes (v).

Now we combine the result in **Step 1** and **Step 2**, and plug into Equation (78), we can finally derive

\[\left\|\nabla_{W}f\left(M_{t+1};X\right)-\nabla_{W}f\left(M_{t};X \right)\right\|_{F}\leq\sum_{i=1}^{N}(L_{i}^{Q}+L_{i}^{K}+L_{i}^{V})\|M_{t+1}- M_{t}\|_{F}\] \[\leq N(\max_{i}L_{i}^{Q}+\max_{i}L_{i}^{K}+\max_{i}L_{i}^{V})\|M_ {t+1}-M_{t}\|_{F}\] (83) \[:=G\|M_{t+1}-M_{t}\|_{F}.\] (84)

### Proof of Lemma in Section 1.4

Proof.: **Proof of Lemma 6 (1)**: We consider the differential of the element in the \(k\)-th row and \(j\)-th column. First, let us write down the closed form of each element:

\[(C_{ih})_{kj}=-\|X_{ik}.W_{h}^{Q}-X_{ij}.W_{h}^{K}\|^{2}/2\sqrt{d}\]

Next, we consider the differential of each element over \(W_{h}^{Q}\):

\[d\left(C_{ih}\right)_{kj}=-\frac{1}{2\sqrt{d}}\left(\left\|X_{ ik.}\big{(}W_{h}^{Q}+d(W_{h}^{Q})\big{)}-X_{ij.}W_{h}^{K}\right\|^{2}-\left\|X_{ ik.}(W_{h}^{Q})-X_{ij.}W_{h}^{K}\right\|^{2}\right)\] \[=-\frac{1}{\sqrt{d}}\langle X_{ik.}d(W_{h}^{Q}),X_{ik.}W_{h}^{Q}-X _{ij.}W_{h}^{K}\rangle+o\big{(}d(W_{h}^{Q})\big{)},\]

where \(o(d(W_{h}^{Q}))\) denotes the higher order of \(d(W_{h}^{Q})\). Leave out the higher order differential term, we derive

\[\|d\left(C_{ih}\right)_{kj}\|_{F}\leq\frac{1}{\sqrt{d}}\left(\|X_{ ik.}\|_{2}\|d(W_{h}^{Q})\|_{F}\cdot\sigma_{\max}(W_{h}^{Q})\|X_{ik.}\|_{2}+\|d(W_{h}^{Q} )\|_{F}\cdot\sigma_{\max}(W_{h}^{K})\|X_{ik.}\|_{2}\|X_{ij.}\|_{2}\right)\] \[\leq\frac{1}{\sqrt{d}}\|X_{ik.}\|_{2}\|d(W_{h}^{Q})\|_{F}(\sigma_{ \max}(W_{h}^{Q})\|X_{ik.}\|_{2}+\sigma_{\max}(W_{h}^{K})\|X_{ij.}\|_{2})\] \[\leq\frac{1}{\sqrt{d}}\|X_{ik.}\|_{2}\sqrt{\sigma_{\max}^{2}(W_{h }^{Q})+\sigma_{\max}^{2}(W_{h}^{K})}\cdot\sqrt{\|X_{ik.}\|_{2}^{2}+\|X_{ij.}\|_{2} ^{2}}\|d(W_{h}^{Q})\|_{F}\]\[\|d\left(C_{ih}\right)\|_{F}=\sum_{k=1}^{n}\sum_{j=1}^{n}\|d(C_{ih})_{kj}\| _{F}^{2}\] \[\leq\frac{1}{\sqrt{d}}\sum_{k=1}^{n}\sum_{j=1}^{n}\|X_{ik}\cdot\|_ {2}\sqrt{\sigma_{\max}^{2}\left(W_{h}^{Q}\right)+\sigma_{\max}^{2}\left(W_{h}^{ K}\right)}\cdot\sqrt{\left\|X_{ik}\cdot\right\|_{2}^{2}+\left\|X_{ij}\cdot\right\|_{2}^ {2}}\|d(W_{h}^{Q})\|_{F}\] \[\leq\frac{1}{\sqrt{d}}\sqrt{\sigma_{\max}^{2}\left(W_{h}^{Q} \right)+\sigma_{\max}^{2}\left(W_{h}^{K}\right)}\cdot\sqrt{\sum_{k=1}^{n}\|X_ {ik}\cdot\|_{F}^{2}}\cdot\sqrt{\sum_{k=1}^{n}(n\|X_{ik}\cdot\|_{2}^{2}+\sum_{j =1}^{n}\|X_{ij}\cdot\|_{F}^{2})}\|d(W_{h}^{Q})\|_{F}\] \[=\frac{1}{\sqrt{d}}\sqrt{\sigma_{\max}^{2}\left(W_{h}^{Q} \right)+\sigma_{\max}^{2}\left(W_{h}^{K}\right)}\cdot\|X_{i}\|_{F}\cdot\sqrt{ 2n}\|X_{i}\|_{F}\|d(W_{h}^{Q})\|_{F}\] \[=\sqrt{\frac{2n}{d}}\|X_{i}\|_{F}^{2}\sqrt{\sigma_{\max}^{2}\left( W_{h}^{Q}\right)+\sigma_{\max}^{2}\left(W_{h}^{K}\right)}\|d(W_{h}^{Q})\|_{F}\]

Proof.: **Proof of Lemma 7 (2)**: First, let us write down the closed form of \(\frac{\partial(C_{ih})_{kj}}{\partial W_{h}^{Q}}\). We have

\[\frac{\partial(C_{ih})_{kj}}{\partial W_{h}^{Q}}=-(X_{ik}.W_{h}^{Q}-X_{ij}.W_{ h}^{K})\mathbb{I}_{d}\otimes X_{ik}.\] (85)

Thus, we can derive upper bound for \(\left\|d\left(\frac{\partial\left(C_{ih}\right)_{kj}}{\partial W_{h}^{Q}} \right)\right\|_{F}\):

\[\left\|d\left(\frac{\partial\left(C_{ih}\right)_{kj}}{\partial W_ {h}^{Q}}\right)\right\|_{F}=\left\|-\left(X_{ik}.(W_{h}^{Q}+d(W_{h}^{Q}))-X_{ ij}.W_{h}^{K}\right)\mathbb{I}_{d}\otimes X_{ik}+\left(X_{ik}.W_{h}^{Q}-X_{ij}.W_{ h}^{K}\right)\mathbb{I}_{d}\otimes X_{ik}.\right\|_{F}/\sqrt{d}\] \[=\|X_{ik}.d(W_{h}^{Q})\mathbb{I}_{d}\otimes X_{ik}.\|_{F}/\sqrt{d}\] \[\leq\|X_{ik}.\|_{2}^{2}\|\mathbb{I}_{d}\|_{F}\|d(W_{h}^{Q})\|_{F}/ \sqrt{d}\] \[=\|X_{ik}.\|_{2}^{2}\|d(W_{h}^{Q})\|_{F}\] (86)

Thus, we have the following:

\[\left\|d\left(\frac{\partial\left(C_{ih}\right)}{\partial W_{h}^{ Q}}\right)\right\|_{F}\leq\sum_{k=1}^{n}\sum_{j=1}^{n}\left\|d\left(\frac{ \partial\left(C_{ih}\right)_{kj}}{\partial W_{h}^{Q}}\right)\right\|_{F}\] \[\leq\|d(W_{h}^{Q})\|_{F}\sum_{k=1}^{n}\sum_{j=1}^{n}\|X_{ik}.\|_{ 2}^{2}\] \[\leq n\|X_{i}\|_{F}^{2}\|d(W_{h}^{Q})\|_{F}\]

Proof.: **Proof of Lemma 7 (3)**:

\[\left\|\frac{\partial f\left(M;X_{i}\right)}{\partial C_{i}}\right\|_{F}= \left\|\left(\mathsf{MH}\left(M;X_{i}\right)-y_{i}\right)\left(W^{O}\right)^{ \top}\left(V_{i}^{\prime}\right)^{\top}\right)\odot S_{i}\right\|_{F}\] \[\geq\min|V_{i}^{\prime}W^{O}|\cdot\min|S_{i}|\cdot\|\mathsf{MH} \left(M;X_{i}\right)-y_{i}\|_{2}.\]Proof.: **Proof of Lemma 7 (4)**:

\[\left\|\frac{\partial f(M;X_{i})}{\partial W_{h}^{Q}}\right\|_{F}= \left\|\operatorname{vec}\left(\frac{\partial f(M;X_{i})}{\partial W_{h}^{Q}} \right)\right\|_{2}=\left\|\operatorname{vec}\left(\frac{\partial f(M;X_{i})}{ \partial C_{i}}\right)\cdot\frac{\partial C_{i}}{\partial W_{h}^{Q}}\right\|_{2}\] \[\leq\left\|\frac{\partial f(M;X_{i})}{\partial C_{i}}\right\|_{F} \cdot\left\|\frac{\partial C_{i}}{\partial W_{h}^{Q}}\right\|_{2}\] \[=\left\|\left(\left(\mathsf{MH}\left(M;X_{i}\right)-y_{i}\right) \left(W^{O}\right)^{\top}\left(V_{i}^{\prime}\right)^{\top}\right)\odot S_{i} \right\|_{F}\cdot\sqrt{\frac{2n}{d}}\left\|X_{i}\right\|_{F}^{2}\sqrt{\sigma_{ \max}^{2}\left(W_{h}^{Q}\right)+\sigma_{\max}^{2}\left(W_{h}^{K}\right)}\] \[\leq\sqrt{\frac{2n}{d}}\left\|X_{i}\right\|_{F}^{3}\left\|W^{O} \right\|_{2}\sigma_{\max}(W^{V})\sqrt{\sigma_{\max}^{2}\left(W_{h}^{Q}\right) +\sigma_{\max}^{2}\left(W_{h}^{K}\right)}\left\|\mathsf{MH}\left(M;X_{i}\right) -y_{i}\right\|_{2}\]

Proof.: **Proof of Lemma 8 (1)**: The proof is similar to the proof of Lemma 3 (1). So we do not include the details here. We can similarly derive

\[\left\|f\left(M_{t+1};X\right)-f\left(M_{t};X\right)\right\|_{2} \leq N\sqrt{\max_{i}Q_{i}^{\prime 2}+\max_{i}K_{i}^{\prime 2}+n^{2}H\max_{i} \left\|X_{i}\right\|_{F}^{2}\left\|W^{O}\right\|_{2}^{2}}\left\|M_{t+1}-M_{t} \right\|_{F}\] \[:=Z^{\prime}\left\|M_{t+1}-M_{t}\right\|_{F}\] (87)

Proof.: **Proof of Lemma 8 (2)**: By triangle inequality, we have

\[\left\|\nabla_{W}f(M_{t+1};X)-\nabla_{W}f(M_{t};X)\right\|_{F}\] \[\leq\left\|\nabla_{W^{Q}}f(M_{t+1};X)-\nabla_{W^{Q}}f(M_{t+1};X) \right\|_{F}+\left\|\nabla_{W^{K}}f(M_{t+1};X)-\nabla_{W^{K}}f(M_{t+1};X) \right\|_{F}\] \[\quad+\left\|\nabla_{W^{V}}f(M_{t+1};X)-\nabla_{W^{V}}f(M_{t+1}; X)\right\|_{F}\] \[\leq\sum_{i=1}^{N}\left(\left\|\nabla_{W^{Q}}f\left(M_{t+1};X \right)-\nabla_{W^{Q}}f\left(M_{t+1};X\right)\right\|_{F}+\left\|\nabla_{W^{K }}f\left(M_{t+1};X\right)-\nabla_{W^{K}}f\left(M_{t+1};X\right)\right\|_{F}\right.\] \[\quad+\left\|\nabla_{W^{v}}f\left(M_{t+1};X\right)-\nabla_{W^{v }}f\left(M_{t+1};X\right)\left\|{}_{F}\right)\] (88)

**Step 1:** Derive upper bound for

\[\|\nabla_{W^{Q}}f(M_{t+1};X_{i}))-\nabla_{W^{Q}}f(M_{t};X_{i}))\|_{F}=\| \operatorname{vec}(\nabla_{W^{Q}}f(M_{t+1};X_{i}))-\operatorname{vec}(\nabla_{ W^{Q}}f(M_{t};X_{i}))\|_{2}.\]

First, we give the vectorized expression of \(\nabla_{W^{Q}}f\left(M_{t};X_{i}\right)\). Recall we denote \(U_{i}=\left(\left(\mathsf{MH}\left(M;X_{i}\right)-y_{i}\right)\left(W^{O} \right)^{\top}\left(V_{i}^{\prime}\right)^{\top}\right)\odot S_{i}\). By Lemma 6, we can derive the close form of \(\operatorname{vec}(\nabla_{W^{Q}}f(M_{t};X_{i}))\):

\[\operatorname{vec}(\nabla_{W^{Q}}f(M;X_{i}))\overset{(i)}{=}\operatorname{ vec}(U_{i})\cdot\operatorname{vec}\left(\frac{\partial C_{i}}{\partial W_{h}^{Q}}\right)\] (89)

Further, recall we have defined \(\bar{R}\) and the following inequality holds:

\[\|U_{i}\|_{F}\leq\bar{R}\] (90)Next, let us derive upper bound for \(\left\|\nabla_{W^{Q}}f\left(M_{t+1};X_{i}\right)-\nabla_{W^{Q}}f\left(M_{t+1};X_{i} \right)\right\|_{F}\).

\[\left\|\nabla_{W^{Q}}f(M_{t+1};X_{i})-\nabla_{W^{Q}}f(M_{t+1};X_{i} )\right\|_{F}\] \[\overset{(i)}{=}\left\|\operatorname{vec}(U_{i,t+1})\cdot\left( \frac{\partial C_{i}(M_{t+1})}{\partial W_{h}^{Q}}\right)-\operatorname{vec}(U _{i,t})\cdot\left(\frac{\partial C_{i}(M_{t})}{\partial W_{h}^{Q}}\right) \right\|_{F}\] \[=\left\|\operatorname{vec}(U_{i,t+1})\cdot\left(\frac{\partial C _{i}(M_{t+1})}{\partial W_{h}^{Q}}\right)-\operatorname{vec}(U_{i,t+1})\cdot \left(\frac{\partial C_{i}(M_{t})}{\partial W_{h}^{Q}}\right)\right.\] (91) \[\quad+\operatorname{vec}(U_{i,t+1})\cdot\left(\frac{\partial C_{ i}(M_{t})}{\partial W_{h}^{Q}}\right)-\operatorname{vec}(U_{i,t})\cdot\left(\frac{ \partial C_{i}(M_{t})}{\partial W_{h}^{Q}}\right)\Bigg{\|}_{F}\] \[\overset{(ii)}{\leq}\left\|\operatorname{vec}(U_{i,t+1})\right\| _{2}\left\|\frac{\partial C_{i}\left(M_{t+1}\right)}{\partial W_{h}^{Q}}-\frac {\partial C_{i}\left(M_{t}\right)}{\partial W_{h}^{Q}}\right\|_{2}+\left\| \operatorname{vec}(U_{i,t+1}-U_{i,t})\right\|_{2}\left\|\frac{\partial C_{i} \left(M_{t}\right)}{\partial W_{h}^{Q}}\right\|_{2}\] \[\overset{(iii)}{\leq}\left.\bar{R}\sqrt{n}\left\|X_{i}\right\|_{F }^{2}\cdot\left\|d\left(W_{h}^{Q}\right)\right\|_{F}+\left\|\operatorname{vec }\left(U_{i,t+1}-U_{i,t}\right)\right\|_{F}\cdot\sqrt{n}\left\|X_{i}\right\|_ {F}^{2}\cdot\left(\sigma_{\max}\left(W_{h}^{Q}\right)+\sigma_{\max}\left(W_{h }^{K}\right)\right)\right.\] \[\leq\bar{R}\sqrt{n}\left\|X_{i}\right\|_{F}^{2}\cdot\left\|d \left(W_{h}^{Q}\right)\right\|_{F}+\sqrt{n}\left\|X_{i}\right\|_{F}^{2}\cdot \left(\sigma_{\max}\left(W_{h}^{Q}\right)+\sigma_{\max}\left(W_{h}^{K}\right) \right)\left\|R_{i,t+1}\odot S_{i,t+1}-R_{i,t}\odot S_{i,t}\right\|_{F}\] \[\leq\bar{R}\sqrt{n}\left\|X_{i}\right\|_{F}^{2}\cdot\left\|d \left(W_{h}^{Q}\right)\right\|_{F}+\sqrt{n}\left\|X_{i}\right\|_{F}^{2}\cdot \left(\sigma_{\max}\left(W_{h}^{Q}\right)+\sigma_{\max}\left(W_{h}^{K}\right)\right)\] \[\leq\bar{R}\sqrt{n}\left\|X_{i}\right\|_{F}^{2}\cdot\left\|d \left(W_{h}^{Q}\right)\right\|_{F}+\sqrt{n}\left\|X_{i}\right\|_{F}^{2}\cdot \left(\sigma_{\max}\left(W_{h}^{Q}\right)+\sigma_{\max}\left(W_{h}^{K}\right)\right)\] \[\quad\times\left(\left\|R_{i,t+1}-R_{i,t}\right\|_{F}+\left\|R_{i,t}\right\|_{F}\left\|S_{i,t+1}-S_{i,t}\right\|_{F}\right)\] (92)

Next, we aim to derive upper bound of \(\left\|R_{i,t+1}-R_{i,t}\right\|_{F}\) in Equation (92). Similar to the derivation in Equation (81), we can derive

\[\left\|R_{i,t+1}-R_{i,t}\right\|_{F}\overset{(iv)}{\leq}\left(Z_{ i}^{\prime}\left\|X_{i}\right\|_{F}\sigma_{\max}\left(W^{V}\right)\left\|W^{O} \right\|_{2}+\left(n\sqrt{H}\sigma_{\max}\left(W^{V}\right)\left\|X_{i}\right\| _{F}\left\|W^{O}\right\|_{2}+\left\|y_{i}\right\|_{2}\right)\left\|X_{i} \right\|_{F}\left\|W^{O}\right\|_{2}\right)\] \[\quad\times\left\|M_{t+1}-M_{t}\right\|_{F}\] \[:=P_{i}^{\prime}\|M_{t+1}-M_{t}\|_{F},\] (93)

Plug Equation (82) into Equation (92), we can finally derive the bound for \(\left\|\nabla_{W^{Q}}f\left(M_{t+1};X_{i}\right)-\nabla_{W^{Q}}f\left(M_{t+1};X _{i}\right)\right\|_{F}\).

\[\left\|\nabla_{W^{Q}}f(M_{t+1};X_{i})-\nabla_{W^{Q}}f(M_{t+1};X_{ i})\right\|_{F}\] \[\leq\bar{R}\sqrt{n}\left\|X_{i}\right\|_{F}^{2}\cdot\left\|d\left( W_{h}^{Q}\right)\right\|_{F}+\sqrt{n}\left\|X_{i}\right\|_{F}^{2}\cdot\left( \sigma_{\max}\left(W_{h}^{Q}\right)+\sigma_{\max}\left(W_{h}^{K}\right)\right)\] \[\quad\times\left(\left\|R_{i,t+1}-R_{i,t}\right\|_{F}+\left\|R_{ i,t}\right\|_{F}\left\|S_{i,t+1}-S_{i,t}\right\|_{F}\right)\] \[\leq\bar{R}\sqrt{n}\left\|X_{i}\right\|_{F}^{2}\cdot\left\|M_{t+1}- M_{t}\right\|_{F}+\sqrt{n}\left\|X_{i}\right\|_{F}^{2}\cdot\left(\sigma_{\max} \left(W_{h}^{Q}\right)+\sigma_{\max}\left(W_{h}^{K}\right)\right)\] \[\quad\times\left(P_{i}^{\prime}\|M_{t+1}-M_{t}\|_{F}+\sqrt{n}\bar{ R}\left\|X_{i}\right\|_{F}^{2}\cdot\left(\sigma_{\max}\left(W_{h}^{Q} \right)+\sigma_{\max}\left(W_{h}^{K}\right)\right)\|M_{t+1}-M_{t}\|_{F}\right)\] \[:=L_{i}^{Q^{\prime}}\|M_{t+1}-M_{t}\|_{F},\] (94)

and plug into Equation (78), we can finally derive

\[\left\|\nabla_{W}f\left(M_{t+1};X\right)-\nabla_{W}f\left(M_{t};X \right)\right\|_{F}\leq\sum_{i=1}^{N}(L_{i}^{Q}+L_{i}^{K}+L_{i}^{V})\|M_{t+1}-M_ {t}\|_{F}\] \[\leq N(\max_{i}L_{i}^{Q}+\max_{i}L_{i}^{K}+\max_{i}L_{i}^{V})\|M_{t +1}-M_{t}\|_{F}\] (95) \[:=G\|M_{t+1}-M_{t}\|_{F}.\]

## 2 NeurIPS paper checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer:[Yes] Justification: See Theorem 1,2,3 Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please see our conclusion 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: See Appendix, which provides proof for each Theorem.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See experiment setting in Section 5.2 Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: We do not include the open access to code. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 5.2 for experiment setting. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Please see Fig 2 and Fig 3. We have a 1-\(\sigma\) error bar. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: We do not include the compute resources detail. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper has no harm in the research process or negative social impact. The paper is anonymous. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer:[NA] Justification: There is no societal impact Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification:The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the code framework we use Chen et al. [2021]. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.