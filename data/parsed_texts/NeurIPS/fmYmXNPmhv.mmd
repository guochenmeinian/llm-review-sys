# Permutation Equivariant Neural Functionals

 Allan Zhou\({}^{1}\)  Kaien Yang\({}^{1}\)  Kaylee Burns\({}^{1}\)  Adriano Cardace\({}^{2}\)  Yiding Jiang\({}^{3}\)

Samuel Sokota\({}^{3}\)  J. Zico Kolter\({}^{3}\)  Chelsea Finn\({}^{1}\)

\({}^{1}\)Stanford University \({}^{2}\)University of Bologna \({}^{3}\)Carnegie Mellon University

ayz@cs.stanford.edu

###### Abstract

This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as _neural functional networks_ (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building _permutation equivariant_ neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are _NF-Layers_ (neural functional layers) that we constrain to be permutation equivariant through an appropriate parameter sharing scheme. In our experiments, we find that permutation equivariant neural functionals are effective on a diverse set of tasks that require processing the weights of MLPs and CNNs, such as predicting classifier generalization, producing "winning ticket" sparsity masks for initializations, and classifying or editing implicit neural representations (INRs). In addition, we provide code for our models and experiments1.

Footnote 1: https://github.com/AllanYangZhou/nfn

## 1 Introduction

As deep neural networks have become increasingly prevalent across various domains, there has been a growing interest in techniques for processing their weights and gradients as data. Example applications include learnable optimizers for neural network training [3, 56, 2, 44], extracting information from implicit neural representations of data [61, 45, 58], corrective editing of network weights [57, 11, 46], policy evaluation [24], and Bayesian inference given networks as evidence [60]. We refer to functions of a neural network's weight-space (such as weights, gradients, or sparsity masks) as _neural functionals_; when these functions are themselves neural networks, we call them _neural functional networks_ (NFNs).

In this work, we design neural functional networks by incorporating relevant symmetries directly into the architecture, following a general line of work in "geometric deep learning" [8, 54, 34, 5]. For neural functionals, the symmetries of interest are transformations of a network's weights that preserve the network's behavior. In particular, we focus on _neuron permutation symmetries_, which are those that arise from the fact that the neurons of hidden layers have no inherent order.

Neuron permutation symmetries are simplest in feedforward networks, such as multilayer perceptrons (MLPs) and basic convolutional neural networks (CNNs). These symmetries are induced by the fact that the neurons in each hidden layer of a feedforward network can be arbitrarily permuted without changing its behavior [27]. In MLPs, permuting the neurons in hidden layer \(i\) corresponds topermuting the rows of the weight matrix \(W^{(i)}\), and the columns of the next weight matrix \(W^{(i+1)}\) as shown on the left-hand side of Figure 1. Note that the same permutation must be applied to the rows \(W^{(i)}\) and columns of \(W^{(i+1)}\), since applying _different_ permutations generally changes network behavior and hence does not constitute a neuron permutation symmetry.

We introduce a new framework for constructing neural functional networks that are invariant or equivariant to neuron permutation symmetries. Our framework extends a long line of work on permutation equivariant architectures [52, 68, 25, 63, 41] that design equivariant layers for a particular permutation symmetry of interest. Specifically, we introduce neural functional layers (NF-Layers) that operate on weight-space features (see Figure 1) while being equivariant to neuron permutation symmetries. Composing these NF-Layers with pointwise non-linearities produces equivariant neural functionals.

We propose different NF-Layers depending on the assumed symmetries of the input weight-space: either only the hidden neurons of the feedforward network can be permuted (hidden neuron permutation, HNP), or all neurons, including inputs and outputs, can be permuted (neuron permutation, NP). Although the HNP assumption is typically more appropriate, the corresponding NF-Layers can be parameter inefficient and computationally infeasible in some settings. In contrast, NF-Layers derived under NP assumptions often lead to much more efficient architectures, and, when combined with a positional encoding scheme we design, can even be effective on tasks that require breaking input and output symmetry. For situations where invariance is required, we also define invariant NF-Layers that can be applied on top of equivariant weight-space features.

Finally, we investigate the applications of permutation equivariant neural functionals on tasks involving both feedforward MLPs and CNNs. Our first two tasks require (1) predicting the test accuracy of CNN image classifiers and (2) classifying implicit neural representations (INRs) of images and 3D shapes. We then evaluate NFNs on their ability to (3) predict good sparsity masks for initializations (also called _winning tickets_[19]), and on (4) a weight-space "style-editing" task where the goal is to modify the content an INR encodes by directly editing its weights. In multiple experiments across these diverse settings, we find that permutation equivariant neural functionals consistently outperform non-equivariant methods and are effective for solving weight-space tasks.

**Relation to DWSNets.** The recent work of Navon et al. [45] recognized the potential of leveraging weight-space symmetries to build equivariant architectures on deep weight-spaces; they characterize a weight-space layer which is mathematically equivalent to our NF-Layer in the HNP setting. Their work additionally studies interesting universality properties of the resulting equivariant architectures, and demonstrates strong empirical results for a suite of tasks that require processing the weights of MLPs. Our framework additionally introduces the NP setting, where we make stronger symmetry assumptions to develop equivariant layers with improved parameter efficiency and practical scalability. We also extend our NFN variants to process convolutional neural networks (CNNs) as input, leading to applications such as predicting the generalization of CNN classifiers (Section 3.1).

Figure 1: The internal operation of our permutation equivariant neural functionals (NFNs). The NFN processes the input weights through a series of equivariant NF-Layers, with each one producing _weight-space features_ with varying numbers of channels. In this example, a neuron permutation symmetry simultaneously permutes the rows of \(W^{(2)}\) and the columns of \(W^{(3)}\). This permutation propagates through the NFN in an equivariant manner.

## 2 Equivariant neural functionals

We begin by setting up basic concepts related to (hidden) neuron permutation symmetries, before defining the equivariant NF-Layers in Sec. 2.2 and invariant NF-Layers in Sec. 2.3.

### Preliminaries

Consider an \(L\)-layer feedforward network having \(n_{i}\) neurons at layer \(i\), with \(n_{0}\) and \(n_{L}\) being the input and output dimensions, respectively. The network is parameterized by weights \(W=\big{\{}\,W^{(i)}\in\mathbb{R}^{n_{i}\times n_{i-1}}\mid i\in\llbracket 1..L \rrbracket\,\big{\}}\) and biases \(v=\big{\{}\,v^{(i)}\in\mathbb{R}^{n_{i}}\mid i\in\llbracket 1..L \rrbracket\,\big{\}}\). We denote the combined collection \(U\coloneqq(W,v)\) belonging to weight-space, \(\mathcal{U}\coloneqq\mathcal{W}\times\mathcal{V}\).

Since the neurons in a hidden layer \(i\in\{1,\cdots,L-1\}\) have no inherent ordering, the network is invariant to the symmetric group \(S_{n_{i}}\) of permutations of the neurons in layer \(i\). This reasoning applies to every hidden layer, so the network is invariant to \(\tilde{\mathcal{S}}\coloneqq S_{n_{1}}\times\cdots\times S_{n_{L-1}}\), which we refer to as the **hidden neuron permutation** (HNP) group. Under the stronger assumption that the input and output neurons are also unordered, the network is invariant to \(\mathcal{S}\coloneqq S_{0}\times\cdots\times S_{n_{L}}\), which we refer to as the **neuron permutation** (NP) group. We focus on the NP setting throughout the main text, and treat the HNP case in Appendix B. See Table 1 for a concise summary of the relevant notation for each symmetry group we consider.

Consider an MLP and a permutation \(\sigma=(\sigma_{0},\cdots,\sigma_{L})\in\mathcal{S}\). The action of the neuron permutation group is to permute the rows of each weight matrix \(W^{(i)}\) by \(\sigma_{i}\), and the columns by \(\sigma_{i-1}\). Each bias vector \(v^{(i)}\) is also permuted by \(\sigma_{i}\). So the action is \(\sigma U\coloneqq(\sigma W,\sigma v)\), where:

\[[\sigma W]^{i}_{jk}=W^{(i)}_{\sigma_{i}^{-1}(j),\sigma_{i-1}^{-1}(k)},\quad[ \sigma v]^{i}_{j}=v^{(i)}_{\sigma_{i}^{-1}(j)}.\] (1)

Until now we have used \(U=(W,v)\) to denote actual weights and biases, but the inputs to a neural functional layer could be any weight-space _feature_ such as a gradient, sparsity mask, or the output of a previous NF-Layer (Figure 1). Moreover, we may consider inputs with \(c\geq 1\) feature channels, belonging to \(\mathcal{U}^{c}=\bigoplus_{i=1}^{c}\mathcal{U}\), the direct sum of \(c\) copies of \(\mathcal{U}\). Concretely, each \(U\in\mathcal{U}^{c}\) consists of weights \(W=\big{\{}\,W^{(i)}\in\mathbb{R}^{n_{i}\times n_{i-1}\times c}\mid i\in \llbracket 1..L\rrbracket\,\big{\}}\) and biases \(v=\big{\{}\,v^{(i)}\in\mathbb{R}^{n_{i}\times c}\mid i\in\llbracket 1..L \rrbracket\,\big{\}}\), with the channels in the final dimension. The action defined in Eq. 1 extends to the multiple channel case if we define \(W^{(i)}_{jk}:=W^{(i)}_{j,k,:}\in\mathbb{R}^{c}\) and \(v^{(i)}_{j}:=v^{(i)}_{j,:}\in\mathbb{R}^{c}\).

The focus of this work is on making neural functionals that are equivariant (or invariant) to neuron permutation symmetries. Letting \(c_{i}\) and \(c_{o}\) be the number of input and output channels, we refer to a function \(f:\mathcal{U}^{c_{i}}\to\mathcal{U}^{c_{o}}\) as \(\mathcal{S}\)**-equivariant** if \(\sigma f(U)=f(\sigma U)\) for all \(\sigma\in\mathcal{S}\) and \(U\in\mathcal{U}^{c_{i}}\), where the action of \(\mathcal{S}\) on the input and output spaces is defined by Eq. 1. Similarly, a function \(f:\mathcal{U}^{c}\to\mathbb{R}\) is \(\mathcal{S}\)**-invariant** if \(f(\sigma U)=f(U)\) for all \(\sigma\) and \(U\).

If \(f,g\) are equivariant, then their composition \(f\circ g\) is also equivariant; if \(g\) is equivariant and \(f\) is invariant, then \(f\circ g\) is invariant. Since pointwise nonlinearities are already permutation equivariant, our remaining task is to design a _linear_ NF-Layer that is \(\mathcal{S}\)-equivariant. We can then construct equivariant neural functionals by stacking these NF-Layers with pointwise nonlinearities.

\begin{table}
\begin{tabular}{c|c|c|c|c}
**Group** & **Abbrv** & **Permutable layers** & \multicolumn{2}{c|}{**Equivariant NF-Layer**} \\  & & & Signature & Parameter count \\ \hline \(\mathcal{S}=\prod_{i=0}^{L}S_{n_{i}}\) & NP & All layers & \(H:\mathcal{U}^{c_{i}}\to\mathcal{U}^{c_{o}}\) & \(O(c_{i}c_{o}L^{2})\) \\ \(\tilde{\mathcal{S}}=\prod_{i=1}^{L-1}S_{n_{i}}\) & HNP & Hidden layers & \(\tilde{H}:\mathcal{U}^{c_{i}}\to\mathcal{U}^{c_{o}}\) & \(O\left(c_{i}c_{o}(L+n_{0}+n_{L})^{2}\right)\) \\ — & — & None & \(T:\mathcal{U}^{c_{i}}\to\mathcal{U}^{c_{o}}\) & \(c_{i}c_{o}\dim(\mathcal{U})^{2}\) \\ \end{tabular}
\end{table}
Table 1: Permutation symmetries of \(L\)-layer feedforward networks with \(n_{0},\ldots,n_{L}\) neurons at each layer. All feedforward networks are invariant under hidden neuron permutations (HNP), while NP assumes that input and output neurons can also be permuted. We show the corresponding equivariant NF-Layers which process weight-space features from \(\mathcal{U}\), with \(c_{i}\) input channels and \(c_{o}\) output channels.

### Equivariant NF-Layers

We now construct a linear \(\mathcal{S}\)-equivariant layer that serves as a key building block for neural functional networks. In the single channel case, we begin with generic linear layers \(T(\cdot;\theta):\text{vec}(U)\mapsto\theta\text{vec}(U)\), where \(\text{vec}(U)\in\mathbb{R}^{\dim(U)}\) is \(U\) flattened as a vector and \(\theta\in\mathbb{R}^{\dim(U)\times\dim(U)}\) is a matrix of parameters. We show in Appendix B.2 that **any**\(\mathcal{S}\)-equivariant \(T(\cdot;\theta)\) must satisfy a system of constraints on \(\theta\) known as equivariant _parameter sharing_. We derive this parameter sharing by partitioning the entries of \(\theta\) by the orbits of their indices under the action of \(\mathcal{S}\), with parameters shared in each orbit [54]. Table 7 of the appendix describes the parameter sharing in detail.

Equivariant parameter sharing reduces the matrix-vector product \(\theta\text{vec}(U)\) to the NF-Layer we now present. For simplicity we ignore \(\mathcal{V}\) and assume here that \(\mathcal{U}=\mathcal{W}\) and defer the full form to Eq. 3 in the appendix. Then \(H:\mathcal{W}^{c_{i}}\rightarrow\mathcal{W}^{c_{o}}\) maps input \(\big{(}W^{(1)},\cdots,W^{(L)}\big{)}\) to \(\big{(}H(W)^{(1)},\cdots,H(W)^{(L)}\big{)}\). Recall that the inputs are not necessarily weights, but could be arbitrary weight-space features including the output of a previous NF-Layer. For \(W^{(i)}\in\mathbb{R}^{n_{i}\times n_{i-1}\times c_{i}}\), the corresponding output is \(H(W)^{(i)}\in\mathbb{R}^{n_{i}\times n_{i-1}\times c_{o}}\) with entries computed:

\[H(W)^{(i)}_{jk}=\left(\sum_{s}a^{i,s}W^{(s)}_{\star,\star}\right)+b^{i,i}W^{( i)}_{\star,k}+b^{i,i-1}W^{(i-1)}_{k,\star}+c^{i,i}W^{(i)}_{j,\star}+c^{i,i+1}W^{( i+1)}_{\star,j}+d^{i}W^{(i)}_{jk}.\] (2)

Note that the terms involving \(W^{(i-1)}\) or \(W^{(i+1)}\) should be omitted for \(i=0\) and \(i=L\), respectively, and \(\star\) denotes summation or averaging over either the rows or columns. Recall that in the multi-channel case, each \(W^{(i)}_{jk}\) is a vector in \(\mathbb{R}^{c_{i}}\) so each parameter is a \(c_{o}\times c_{i}\) matrix. We also provide a concrete pseudocode description of \(H\) in Appendix A. Figure 2 visually illustrates the NF-Layer in the single-channel case, showing how the row or column sums from each input contribute to each output. To gain intuition for the operation of \(H\), it is straightforward to check \(\mathcal{S}\)-equivariance:

**Proposition 1**.: _The NF-Layer \(H:\mathcal{U}^{c_{i}}\rightarrow\mathcal{U}^{c_{o}}\) (Eq. 2 and Eq. 3) is \(\mathcal{S}\)-equivariant, where the group's action on input and output spaces is defined by Eq. 1. Moreover, any linear \(\mathcal{S}\)-equivariant map \(T:\mathcal{U}^{c_{i}}\rightarrow\mathcal{U}^{c_{o}}\) is equivalent to \(H\) for some choice of parameters \(a,b,c,d\)._

Proof (sketch).: We can verify that \(H\) satisfies the equivariance condition \([\sigma H(W)]^{(i)}_{jk}=H(\sigma W)^{(i)}_{jk}\) for any \(i,j,k\) by expanding each side of the equation using the definitions of the layer and action

Figure 2: A permutation equivariant NF-Layer takes in weight-space features as input (bottom) and outputs transformed features (top), while respecting the neuron permutation symmetries of feedforward networks. This illustrates the computation of a single output element \(H(W)^{i}_{jk}\), defined in Eq. 2. Each output is a weighted combination of rows or column sums of the input weights, which preserves permutation symmetry. The first term contributes a weighted combination of row-and-column sums from _every_ input weight, though this is omitted for visual clarity.

(Eq. 1). Moreover, Appendix B.2 shows that any \(\mathcal{S}\)-equivariant linear map \(T(\cdot,\theta)\) must have the same equivariant parameter sharing as \(H\), meaning that it must be equivalent to \(H\) for some choice of parameter \(a,b,c,d\). 

Informally, the above proposition tells us that \(H\) can express any linear \(\mathcal{S}\)-equivariant function of a weight-space. Since \(\tilde{\mathcal{S}}\) is a subgroup of \(\mathcal{S}\), \(H\) is also \(\tilde{\mathcal{S}}\)-equivariant. However, it does not express every possible linear \(\tilde{\mathcal{S}}\)-equivariant function. We derive the full \(\tilde{\mathcal{S}}\)-equivariant NF-Layer \(\tilde{H}:\mathcal{U}\rightarrow\mathcal{U}\) in Appendix C.

Table 1 summarizes the number of parameters (after parameter sharing) under different symmetry assumptions. While in general a linear layer \(T(\cdot;\theta):\mathcal{U}^{c_{i}}\rightarrow\mathcal{U}^{c_{n}}\) has \(c_{i}c_{o}\dim(\mathcal{U})^{2}\) parameters, the equivariant NF-Layers have significantly fewer free parameters due to parameter sharing. The \(\mathcal{S}\)-equivariant layer \(H\) has \(O\left(c_{i}c_{o}L^{2}\right)\), while the \(\tilde{\mathcal{S}}\)-equivariant layer \(\tilde{H}\) has \(O\left(c_{i}c_{o}(L+n_{0}+n_{L})^{2}\right)\) parameters. The latter's quadratic dependence on input and output dimensions can be prohibitive in some settings, such as in classification where the number of outputs can be tens of thousands.

**Extension to convolutional weight-spaces.** In convolution layers, since neurons correspond to spatial _channels_, we let \(n_{i}\) denote the number of channels at the \(i^{\mathrm{th}}\) layer. Each bias \(v^{(i)}\in\mathbb{R}^{n_{i}}\) has the same dimensions as in the fully connected case, so only the convolution filter needs to be treated differently since it has additional spatial dimension(s) that cannot be permuted. For example, consider a 1D CNN with filters \(W=\big{\{}\ W^{(i)}\in\mathbb{R}^{n_{i}\times n_{i-1}\times w}\ \big{|}\ i\in \llbracket 1..L\rrbracket\big{\}}\), where \(n_{i}\times n_{i-1}\) are the output and input channel dimensions and \(w\) is the filter width. We let \(W^{(i)}_{jk}\coloneqq W^{(i)}_{jk,:}\in\mathbb{R}^{w}\) denote the \(k^{\mathrm{th}}\) filter in the \(j^{\mathrm{th}}\) output channel, then define the \(\mathcal{S}\)-action the same way as in Eq. 1.

We immediately observe the similarities to multi-channel features: both add dimensions that are not permuted by the group action. In fact, suppose we have \(c\)-channel features \(U\in\mathcal{U}^{c}\) where \(\mathcal{U}\) is the weight-space of a 1D CNN. Then we combine the filter and channel dimensions of the weights, with \(W^{(i)}\in\mathbb{R}^{n_{i}\times n_{i-1}\times(cw)}\). This allows us to use the multi-channel NF-Layer \(H:\mathcal{U}^{wc_{i}}\rightarrow\mathcal{U}^{wc_{o}}\). Any further channel dimensions, such as those for 2D convolutions, can also be folded into the channel dimension.

It is common for CNNs in image classification to follow convolutional layers with pooling and fully connected (FC) layers, which opens the question of defining the \(\mathcal{S}\)-action when layer \(\ell\) is FC and layer \(\ell-1\) is convolutional. If global spatial pooling removes all spatial dimensions from the output of \(\ell-1\) (as in e.g., ResNets [26] and the Small CNN Zoo [64]), then we can verify that the existing action definitions work without modification. We leave more complicated situations (e.g., when nontrivial spatial dimensions are flattened as input to FC layers) to future work.

**IO-encoding.** The \(\mathcal{S}\)-equivariant layer \(H\) is more parameter efficient than \(\tilde{H}\) (Table 1), but its NP assumptions are typically too strong. To resolve this problem, we can add either learned or fixed (sinusoidal) position embeddings to the columns of \(W^{(1)}\) and the rows of \(W^{(L)}\) and \(v^{(L)}\); this breaks the symmetry at input and output neurons even when using \(\mathcal{S}\)-equivariant layers. In our experiments, we find that IO-encoding makes \(H\) competitive or superior to \(\tilde{H}\), while using a fraction of the parameters.

### Invariant NF-Layers

Invariant neural functionals can be designed by composing multiple equivariant NF-Layers with an invariant NF-Layer, which can then be followed by an MLP. We define an \(\mathcal{S}\)-invariant2 layer \(P:\mathcal{U}\rightarrow\mathbb{R}^{2L}\) by simply summing or averaging the weight matrices and bias vectors across any axis that has permutation symmetry, i.e., \(P(U)=\left(W^{(1)}_{\star,\star},\cdots,W^{(L)}_{\star,\star},v^{(1)}_{\star},\cdots,v^{(L)}_{\star}\right).\) We can then apply a fully connected layer to the output of \(P\) to produce an invariant vector of arbitrary size. In fact, this combination expresses every possible linear invariant function on \(\mathcal{U}\):

Footnote 2: We define the analogous \(\tilde{\mathcal{S}}\)-invariant layer \(\tilde{P}\) in Eq. 20 of the appendix.

**Proposition 2**.: _Any \(\mathcal{S}\)-invariant linear function \(\mathcal{U}\rightarrow\mathbb{R}^{d}\) can be expressed in the form \(f\circ P\), for some choice of linear \(f:\mathbb{R}^{2L}\rightarrow\mathbb{R}^{d}\). Proof: See section B.3._

## 3 Experiments

Our experiments evaluate permutation equivariant neural functionals on a variety of tasks that require either invariance (predicting CNN generalization and extracting information from INRs) or equivariance (predicting "winning ticket" sparsity masks and weight-space editing of INR content).

Throughout the experiments, we construct neural functional networks (NFNs) using the NF-Layers described in the previous section. Although the specific design varies depending on the task, we will broadly refer to our permutation equivariant NFNs as \(\text{NFN}_{\text{NP}}\) and \(\text{NFN}_{\text{HNP}}\), depending on which NF-Layer variant they use (see Table 1). We also evaluate a "pointwise" ablation of our equivariant NF-Layer that ignores interactions between weights by only using the last term of Eq. 2, computing \(H(W)^{i}_{jk}\coloneqq d^{i}W^{(i)}_{jk}\). We refer to NFNs that use this pointwise NF-Layer as \(\text{NFN}_{\text{PT}}\).

Using our benchmarks, we compare the performance of NFNs with other methods for processing weights, including standard MLPs that operate on the flattened weight inputs. To encourage permutation equivariance, we optionally augment the MLP's training with permutations using Eq. 1. On relevant datasets we also compare with the recently developed DWSNets [47], an equivariant architecture similar to \(\text{NFN}_{\text{HNP}}\), and with \(\text{in}\text{2vec}\)[12], a recent non-equivariant approach for learning useful representations from weights.

### Predicting CNN generalization from weights

Why deep neural networks generalize despite being heavily overparameterized is a longstanding research problem in deep learning. One recent line of work has investigated the possibility of directly predicting the test accuracy of the models from the weights [64; 16]. The goal is to study generalization in a data-driven fashion and ultimately identify useful patterns from the weights.

Prior methods develop various strategies for extracting potentially useful features from the weights before using them to predict the test accuracy [29; 67; 64; 30; 42]. However, using hand-crafted features could fail to capture intricate correlations between the weights and test accuracy. Instead, we explore using neural functionals to predict test accuracy from the _raw weights_ of feedforward convolutional neural networks (CNN) from the _Small CNN Zoo_ dataset [64], which contains thousands of CNN weights trained on several datasets with a shared architecture, but varied optimization hyperparameters. We compare the predictive power of \(\text{NFN}_{\text{HNP}}\) and \(\text{NFN}_{\text{NP}}\) against a method of Unterthiner et al. [64] that trains predictors on statistical features extracted from each weight and bias, and refer to it as StatNN. To measure the predictive performance of each method, we use _Kendall's_\(\tau\)[31], a popular rank correlation metric with values in \([-1,1]\).

In Table 2, we show the results on two challenging subsets of Small CNN Zoo corresponding to CNNs trained on CIFAR-10-GS and SVHN-GS (GS stands for grayscaled). We see that \(\text{NFN}_{\text{HNP}}\) consistently performs the best on both datasets by a significant margin, showing that having access to the full weights can increase predictive power over hand-designed features as in StatNN. Because the input and output dimensionalities are small on these datasets, \(\text{NFN}_{\text{HNP}}\) only uses moderately more (\(\sim 1.4\times\)) parameters than \(\text{NFN}_{\text{NP}}\) with equivalent depth and channel dimensions, while having significantly better performance.

### Classifying implicit neural representations of images and 3D shapes

Given the rise of implicit neural representations (INRs) that encode data such as images and 3D-scenes [61; 43; 7; 49; 58; 45; 14; 15], it is natural to wonder how to extract information about the

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \(\text{NFN}_{\text{HNP}}\) & \(\text{NFN}_{\text{NP}}\) & StatNN \\ \hline CIFAR-10-GS & \(\mathbf{0.934\pm 0.001}\) & \(0.922\pm 0.001\) & \(0.915\pm 0.002\) \\ SVHN-GS & \(\mathbf{0.931\pm 0.005}\) & \(0.856\pm 0.001\) & \(0.843\pm 0.000\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Test \(\tau\) of generalization prediction methods on the Small CNN Zoo [64], which contains the weights and test accuracies of many small CNNs trained on different datasets, such as CIFAR-10-GS or SVHN-GS. \(\text{NFN}_{\text{HNP}}\) outperforms other methods on both datasets. Uncertainties indicate max and min over two runs.

original data directly from INR weights. Compared to discrete signal representations (pixels, voxels, etc.), the advantage of directly processing INRs is that one can easily be agnostic to varying signal sizes and resolutions.

In this task, our goal is to classify the contents of INRs given only the weights as input. We consider datasets of SIRENs [58] that encode images (MNIST [38], FashionMNIST [66], and CIFAR [35]) and 3D shapes (ShapeNet-10 and ScanNet-10 [53]). For image datasets each SIREN network represents the mapping from pixel coordinate to RGB (or grayscale) value for a single image, while for 3D shapes each network is a signed (or unsigned) distance function encoding a single shape. Each dataset of SIREN weights is split into training, validation, and testing sets. We construct and train invariant neural functionals to classify the INRs, and compare their performance against the MLP and MLP\({}_{\text{Aug}}\) baselines, which are three-layer MLPs with ReLU activations and 1,000 hidden units per layer.

As Navon et al. [47] test the performance of DWSNets on their independently constructed 2D-image INR datasets, we also present the results of training DWSNets on our own 2D-image INR datasets. Table 3 show that \(\text{NFN}_{\text{NP}}\) consistently achieves the highest test accuracies of any method on the 2D-image tasks. More broadly, equivariant architectures significantly outperform the non-equivariant MLP approaches, even with permutation data augmentations.

For the 3D-shape datasets we also report the performance of intr2vec [12], a recent non-equivariant method for classifying 3D shapes from INR weights. Note that intr2vec's original setting assumes that all INRs in a dataset are trained from the same shared initialization, whereas our problem setting makes no such assumption and allows INRs to be trained from random and independent initializations. As expected, Table 4 shows that \(\text{NFN}_{\text{INP}}\) and \(\text{NFN}_{\text{NP}}\) achieve significantly higher test accuracies than intr2vec, as well as the non-equivariant MLP baselines.

In addition to superior generalization, Appendix Table 16 shows that the NFNs are also better at fitting the training data compared to non-equivariant architectures. The MLPs achieve low train accuracy, even with an equal number of parameters as the NFNs. Interestingly, \(\text{NFN}_{\text{NP}}\) matches or exceeds \(\text{NFN}_{\text{INP}}\) performance on both CIFAR-10 and the 3D-shape datasets while using fewer parameters (e.g., \(35\%\) as many parameters on CIFAR-10). Finally, we note that no weight space methods (including NFN) match the performance of near state-of-the-art methods on discrete data representations such as 2D image arrays and point clouds (see Section E.2 for details). All weight space methods still lack the _geometric_ inductive biases that, e.g., convolutional networks have in image tasks.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & \(\text{NFN}_{\text{INP}}\) & \(\text{NFN}_{\text{NP}}\) & MLP & MLP\({}_{\text{Aug}}\) & intr2vec[12] \\ \hline ShapeNet-10 & \(86.9\pm 0.860\) & \(\mathbf{88.7\pm 0.461}\) & \(25.4\pm 0.121\) & \(33.8\pm 0.126\) & \(39.1\pm 0.385\) \\ \hline ScanNet-10 & \(64.1\pm 0.572\) & \(\mathbf{65.9\pm 1.10}\) & \(32.9\pm 0.351\) & \(45.5\pm 0.126\) & \(38.2\pm 0.409\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Classification test accuracies (%) for datasets of implicit neural representations (INRs) of either ShapeNet-10 [6] or ScanNet-10 [10]. Our equivariant NFNs outperform the MLP baselines and recent non-equivariant methods such as intr2vec [12]. Uncertainties indicate standard error over three runs.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & \(\text{NFN}_{\text{INP}}\) & \(\text{NFN}_{\text{NP}}\) & DWSNets & MLP & MLP\({}_{\text{Aug}}\) \\ \hline MNIST & \(92.5\pm 0.071\) & \(\mathbf{92.9\pm 0.218}\) & \(74.4\pm 0.143\) & \(14.5\pm 0.035\) & \(21.0\pm 0.172\) \\ \hline FashionMNIST & \(72.7\pm 1.53\) & \(\mathbf{75.6\pm 1.07}\) & \(64.8\pm 0.685\) & \(12.5\pm 0.111\) & \(15.9\pm 0.181\) \\ \hline CIFAR-10 & \(44.1\pm 0.471\) & \(\mathbf{46.6\pm 0.072}\) & \(41.5\pm 0.431\) & \(16.9\pm 0.250\) & \(18.9\pm 0.432\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Classification test accuracies (%) for implicit neural representations of MNIST, FashionMNIST, and CIFAR-10. Equivariant architectures such as NFNs and DWSNets [47] outperform the non-equivariant MLP baselines, even when the MLP has permutation augmentations. Our \(\text{NFN}_{\text{NP}}\) variant consistently outperforms all other methods across each dataset. Uncertainties indicate standard error over three runs.

### Predicting "winning ticket" masks from initialization

The Lottery Ticket Hypothesis [19; 20], LTH] conjectures the existence of _winning tickets_, or sparse initializations that train to the same final performance as dense networks, and showed their existence in some settings through iterative magnitude pruning (IMP). IMP retroactively finds a winning ticket by pruning _trained_ models by magnitude; however, finding the winning ticket from only the initialization without training remains challenging.

We demonstrate that permutation equivariant neural functionals are a promising approach for finding winning tickets at initialization by learning over datasets of initializations and their winning tickets. Let \(U_{0}\in\mathcal{U}\) be an initialization and let the sparsity mask \(M\in\{0,1\}^{\text{dm}(\mathcal{U})}\) be a winning ticket for the initialization, with zeros indicating that the corresponding entries of \(U_{0}\) should be pruned. The goal is to predict a winning ticket \(\hat{M}\) given a held out initialization \(U_{0}\), such that the MLP initialized with \(U_{0}\) and sparsity pattern \(\hat{M}\) will achieve a high test accuracy after training.

We construct a conditional variational autoencoder [32; 59; cVAE] that learns a generative model of the winning tickets conditioned on initialization and train on datasets of (initialization, ticket) pairs found by one step of IMP with a sparsity level of \(P_{m}=0.95\) for both MLPs trained on MNIST and CNNs trained on CIFAR-10. Table 5 compares the performance of tickets predicted by equivariant neural functionals against IMP tickets and random tickets. We generate random tickets by randomly sampling sparsity mask entries from Bernoulli\((1-P_{m})\). In this setting, NFN\({}_{\text{INP}}\) is prohibitively parameter inefficient, but NFN\({}_{\text{NP}}\) is able to recover test accuracies that are close to that of IMP pruned networks in CIFAR-10 and MNIST, respectively. Somewhat surprisingly, NFN\({}_{\text{PT}}\) performs just as well as the other NFNs, indicating that one can approach IMP performance in these settings without considering interactions between weights or layers. Appendix E.4 further analyzes how NFN\({}_{\text{PT}}\) learns to prune.

### Weight-space style editing

Another potentially useful application of neural functionals is to edit (i.e., transform) the weights of a given INR to alter the content that it encodes. In particular, the goal of this task is to edit the weights of a trained SIREN to alter its encoded image (Figure 3). We evaluate two editing tasks:

\begin{table}
\begin{tabular}{l c c} \hline \hline Method & Contrast & Dilate \\  & (CIFAR-10) & (MNIST) \\ \hline MLP & \(0.031\) & \(0.306\) \\ MLPAug & \(0.029\) & \(0.307\) \\ NFN\({}_{\text{PT}}\) & \(0.029\) & \(0.197\) \\ NFN\({}_{\text{HNP}}\) & \(\mathbf{0.021}\) & \(\mathbf{0.070}\) \\ NFN\({}_{\text{NP}}\) & \(\mathbf{0.020}\) & \(\mathbf{0.068}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Test mean squared error (lower is better) between weight-space editing methods and ground-truth image-space transformations.

Figure 3: In weight-space style editing, an NFN directly edits the weights of an INR to alter the content it encodes. In this example, the NFN edits the weights to dilate the encoded image.

(1) making MNIST digits thicker via image dilation (**Dilate**), and (2) increasing image contrast on CIFAR-10 (**Contrast**). Both of these tasks require neural functionals to process _the relationships between different pixels_ to successfully solve the task.

To produce training data for this task, we use standard image processing libraries [28, OpenCV] to dilate or increase the contrast of the MNIST and CIFAR-10 images, respectively. The training objective is to minimize the mean squared error between the image generated by the NFN-edited INR and the image produced by image processing. We construct equivariant neural functionals to edit the INR weights, and compare them against MLP-based neural functionals with and without permutation augmentation.

Table 6 shows that permutation equivariant neural functionals (NFN\({}_{\text{HNP}}\) and NFN\({}_{\text{NP}}\)) achieve significantly better test MSE when editing held out INRs compared to other methods, on both the Dilate (MNIST) and Contrast (CIFAR-10) tasks. In other words, they produce results that are closest to the "ground truth" image-space processing operations for each task. The pointwise ablation NFN\({}_{\text{PT}}\) performs significantly worse, indicating that accounting for interactions between weights and layers is important to accomplishing these tasks. Figure 4 shows random qualitative samples of editing by different methods below the original (pre-edit) INR. We observe that NFNs are more effective than MLP\({}_{\text{Aug}}\) at dilating MNIST digits and increasing the contrast in CIFAR-10 images.

## 4 Related work

The permutation symmetries of neurons have been a topic of interest in the context of loss landscapes and model merging [21; 4; 62; 17; 1]. Other works have analyzed the degree of learned permutation symmetry in networks that process weights [64] and studied ways of accounting for symmetries when measuring or encouraging diversity in the weight-space [13]. However, these symmetries have not been a key consideration in architecture design for processing weight-space objects [2; 39; 23; 36; 69; 13; 33]. Instead, existing approaches try to encourage permutation equivariance through data augmentation [51; 44]. In contrast, this work directly encodes the equivariance of the weight-space into our architecture design, which can result in much higher data and computational efficiency, as evidenced by the success of convolutional neural networks [37].

Our work follows a long line of literature that incorporates structure and symmetry into neural network architectures [37; 8; 54; 34; 9; 18]. This includes works that design permutation equivariant layers, originally for processing sets and graphs [52; 68; 25; 41]. More generally, equivariant layers have been developed for processing arbitrary rank-\(k\) tensors (and correspondingly, hypergraphs) under higher-order permutation actions [40; 63; 48]. In this context, we can view feedforward networks as graphs that factor into a special layered structure where each (hidden) layer of neurons is an independently permutable set of nodes, and the weights are adjacency matrices specifying the edge weights between nodes of adjacent layers. Then our equivariant NF-Layers give the maximal set

Figure 4: Random qualitative samples of INR editing behavior on the Dilate (MNIST) and Contrast (CIFAR-10) editing tasks. The first row shows the image produced by the original INR, while the rows below show the result of editing the INR weights with an NFN. The difference between MLP neural functionals and equivariant neural functionals is especially pronounced on the more challenging Dilate tasks, which require modifying the geometry of the image. In the Contrast tasks, the MLP baseline produces dimmer images compared to the ground truth, which is especially evident in the second and third columns.

of linear functions on these special graph structures. As discussed in Section 1, Navon et al. [47] recently developed an equivariant weight-space layer that is equivalent to our NF-Layer in the HNP setting. Our work introduces the NP setting to improve parameter efficiency and scalability over the HNP setting, and extends beyond the fully connected case to handle convolutional weight-space inputs.

## 5 Conclusion

This paper proposes a novel symmetry-inspired framework for the design of neural functional networks (NFNs), which process weight-space features such as weights, gradients, and sparsity masks. Our framework focuses on the permutation symmetries that arise in weight-spaces due to the particular structure of neural networks. We introduce two equivariant NF-Layers as building blocks for NFNs, which differ in their underlying symmetry assumptions and parameter efficiency, then use them to construct a variety of permutation equivariant neural functionals. Experimental results across diverse settings demonstrate that permutation equivariant neural functionals outperform prior methods and are effective for solving weight-space tasks.

**Limitations and future work.** Although we believe this framework is a step toward the principled design of effective neural functionals, there remain multiple directions for improvement. One such direction would concern extending the NF-Layers beyond feedforward weight spaces, in order to process the weights of more complex architectures such as ResNets [26] and Transformers [65]. Another useful direction would involve reducing the activation sizes produced by NF-Layers, in order to scale neural functionals to process the weights of very large networks. Finally, improvements to NF-Layers could account for the other symmetries of neural network weight spaces, such as scaling symmetries in ReLU networks [22].

## Acknowledgements

We thank Andy Jiang and Will Dorrell for helpful early discussions about theoretical concepts, and Yoonho Lee and Eric Mitchell for reviewing an early draft of this paper. AZ and KB are supported by the NSF Graduate Research Fellowship Program. This work was also supported by Apple.

## References

* [1] S. K. Ainsworth, J. Hayase, and S. Srinivasa. Git re-basin: Merging models modulo permutation symmetries. _arXiv preprint arXiv:2209.04836_, 2022.
* [2] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, and N. De Freitas. Learning to learn by gradient descent by gradient descent. _Advances in neural information processing systems_, 29, 2016.
* [3] S. Bengio, Y. Bengio, J. Cloutier, and J. Gescei. On the optimization of a synaptic learning rule. In _Optimality in Biological and Artificial Networks?_, pages 281-303. Routledge, 2013.
* [4] J. Brea, B. Simsek, B. Illing, and W. Gerstner. Weight-space symmetry in deep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss landscape. _arXiv preprint arXiv:1907.02911_, 2019.
* [5] M. M. Bronstein, J. Bruna, T. Cohen, and P. Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _arXiv preprint arXiv:2104.13478_, 2021.
* [6] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University -- Princeton University -- Toyota Technological Institute at Chicago, 2015.
* [7] Z. Chen and H. Zhang. Learning implicit fields for generative shape modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5939-5948, 2019.

* [8] T. Cohen and M. Welling. Group equivariant convolutional networks. In _International conference on machine learning_, pages 2990-2999. PMLR, 2016.
* [9] T. S. Cohen, M. Geiger, J. Kohler, and M. Welling. Spherical CNNs. _arXiv preprint arXiv:1801.10130_, 2018.
* [10] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Niessner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 5828-5839, 2017.
* [11] N. De Cao, W. Aziz, and I. Titov. Editing factual knowledge in language models. _arXiv preprint arXiv:2104.08164_, 2021.
* [12] L. De Luigi, A. Cardace, R. Spezialetti, P. Zama Ramirez, S. Salti, and L. Di Stefano. Deep learning on implicit neural representations of shapes. In _International Conference on Learning Representations (ICLR)_, 2023.
* [13] L. Deutsch, E. Nijkamp, and Y. Yang. A generative model for sampling high-performance and diverse weights for neural networks. _arXiv preprint arXiv:1905.02898_, 2019.
* [14] E. Dupont, Y. W. Teh, and A. Doucet. Generative models as distributions of functions. _arXiv preprint arXiv:2102.04776_, 2021.
* [15] E. Dupont, H. Kim, S. Eslami, D. Rezende, and D. Rosenbaum. From data to functa: Your data point is a function and you should treat it like one. _arXiv preprint arXiv:2201.12204_, 2022.
* [16] G. Eilertsen, D. Jonsson, T. Ropinski, J. Unger, and A. Ynnerman. Classifying the classifier: dissecting the weight space of neural networks. _arXiv preprint arXiv:2002.05688_, 2020.
* [17] R. Entezari, H. Sedghi, O. Saukh, and B. Neyshabur. The role of permutation invariance in linear mode connectivity of neural networks. _arXiv preprint arXiv:2110.06296_, 2021.
* [18] M. Finzi, M. Welling, and A. G. Wilson. A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups. In _International Conference on Machine Learning_, pages 3318-3328. PMLR, 2021.
* [19] J. Frankle and M. Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. _arXiv preprint arXiv:1803.03635_, 2018.
* [20] J. Frankle, G. K. Dziugaite, D. M. Roy, and M. Carbin. Stabilizing the lottery ticket hypothesis. _arXiv preprint arXiv:1903.01611_, 2019.
* [21] T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson. Loss surfaces, mode connectivity, and fast ensembling of DNNs. _Advances in neural information processing systems_, 31, 2018.
* [22] C. Godfrey, D. Brown, T. Emerson, and H. Kvinge. On the symmetries of deep learning models and their internal representations. _Advances in Neural Information Processing Systems_, 35:11893-11905, 2022.
* [23] D. Ha, A. Dai, and Q. V. Le. Hypernetworks. _arXiv preprint arXiv:1609.09106_, 2016.
* [24] J. Harb, T. Schaul, D. Precup, and P. Bacon. Policy evaluation networks. _CoRR_, abs/2002.11833, 2020. URL https://arxiv.org/abs/2002.11833.
* [25] J. Hartford, D. Graham, K. Leyton-Brown, and S. Ravanbakhsh. Deep models of interactions across sets. In _International Conference on Machine Learning_, pages 1909-1918. PMLR, 2018.
* [26] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. _CoRR, abs/1512_, 3385:2, 2015.
* [27] R. Hecht-Nielsen. On the algebraic structure of feedforward network weight spaces. In _Advanced Neural Computers_, pages 129-135. Elsevier, 1990.
* [28] Itseez. Open source computer vision library. https://github.com/itseez/opencv, 2015.

* [29] Y. Jiang, D. Krishnan, H. Mobahi, and S. Bengio. Predicting the generalization gap in deep networks with margin distributions. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=HJlQfnCqKX.
* [30] Y. Jiang, P. Natekar, M. Sharma, S. K. Aithal, D. Kashyap, N. Subramanyam, C. Lassance, D. M. Roy, G. K. Dziugaite, S. Gunasekar, et al. Methods and analysis of the first competition in predicting generalization of deep learning. In _NeurIPS 2020 Competition and Demonstration Track_, pages 170-190. PMLR, 2021.
* [31] M. G. Kendall. A new measure of rank correlation. _Biometrika_, 30(1/2):81-93, 1938.
* [32] D. P. Kingma and M. Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [33] B. Knyazev, M. Drozdzal, G. W. Taylor, and A. Romero Soriano. Parameter prediction for unseen deep architectures. _Advances in Neural Information Processing Systems_, 34:29433-29448, 2021.
* [34] R. Kondor and S. Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. In _International Conference on Machine Learning_, pages 2747-2755. PMLR, 2018.
* [35] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [36] D. Krueger, C.-W. Huang, R. Islam, R. Turner, A. Lacoste, and A. Courville. Bayesian hypernetworks. _arXiv preprint arXiv:1710.04759_, 2017.
* [37] Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series. _The handbook of brain theory and neural networks_, 3361(10):1995, 1995.
* [38] Y. LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2, 2010.
* [39] K. Li and J. Malik. Learning to optimize. _arXiv preprint arXiv:1606.01885_, 2016.
* [40] H. Maron, H. Ben-Hamu, N. Shamir, and Y. Lipman. Invariant and equivariant graph networks. _arXiv preprint arXiv:1812.09902_, 2018.
* [41] H. Maron, O. Litany, G. Chechik, and E. Fetaya. On learning sets of symmetric elements. In _International conference on machine learning_, pages 6734-6744. PMLR, 2020.
* [42] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. _The Journal of Machine Learning Research_, 22(1):7479-7551, 2021.
* [43] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks: Learning 3d reconstruction in function space. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4460-4470, 2019.
* [44] L. Metz, J. Harrison, C. D. Freeman, A. Merchant, L. Beyer, J. Bradbury, N. Agrawal, B. Poole, I. Mordatch, A. Roberts, et al. Velo: Training versatile learned optimizers by scaling up. _arXiv preprint arXiv:2211.09760_, 2022.
* [45] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: representing scenes as neural radiance fields for view synthesis (2020). _arXiv preprint arXiv:2003.08934_, 2020.
* [46] E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning. Fast model editing at scale. _arXiv preprint arXiv:2110.11309_, 2021.
* [47] A. Navon, A. Shamsian, I. Achituve, E. Fetaya, G. Chechik, and H. Maron. Equivariant architectures for learning in deep weight spaces. _arXiv preprint arXiv:2301.12780_, 2023.

* [48] H. Pan and R. Kondor. Permutation equivariant layers for higher order interactions. In _International Conference on Artificial Intelligence and Statistics_, pages 5987-6001. PMLR, 2022.
* [49] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 165-174, 2019.
* [50] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [51] W. Peebles, I. Radosavovic, T. Brooks, A. A. Efros, and J. Malik. Learning to learn with generative models of neural network checkpoints. _arXiv preprint arXiv:2209.12892_, 2022.
* [52] C. Qi, H. Su, K. Mo, and L. Guibas. Pointnet: deep learning on point sets for 3d classification and segmentation. cvpr (2017). _arXiv preprint arXiv:1612.00593_, 2016.
* [53] C. Qin, H. You, L. Wang, C.-C. J. Kuo, and Y. Fu. Pointdan: A multi-scale 3d domain adaption network for point cloud representation. _Advances in Neural Information Processing Systems_, 32, 2019.
* [54] S. Ravanbakhsh, J. Schneider, and B. Poczos. Equivariance through parameter-sharing. In _International conference on machine learning_, pages 2892-2901. PMLR, 2017.
* [55] A. Rogozhnikov. Einops: Clear and reliable tensor manipulations with einstein-like notation. In _International Conference on Learning Representations_, 2022.
* [56] T. P. Runarsson and M. T. Jonsson. Evolution and design of distributed learning rules. In _2000 IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks. Proceedings of the First IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks (Cat. No. 00_, pages 59-63. IEEE, 2000.
* [57] A. Sinitsin, V. Plokhotnyuk, D. Pyrkin, S. Popov, and A. Babenko. Editable neural networks. _arXiv preprint arXiv:2004.00345_, 2020.
* [58] V. Sitzmann, J. Martel, A. Bergman, D. Lindell, and G. Wetzstein. Implicit neural representations with periodic activation functions. _Advances in Neural Information Processing Systems_, 33:7462-7473, 2020.
* [59] K. Sohn, H. Lee, and X. Yan. Learning structured output representation using deep conditional generative models. _Advances in neural information processing systems_, 28, 2015.
* [60] S. Sokota, H. Hu, D. J. Wu, J. Z. Kolter, J. N. Foerster, and N. Brown. A fine-tuning approach to belief state modeling. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=ckZY7DGa7FQ.
* [61] K. O. Stanley. Compositional pattern producing networks: A novel abstraction of development. _Genetic programming and evolvable machines_, 8:131-162, 2007.
* [62] N. Tatro, P.-Y. Chen, P. Das, I. Melnyk, P. Sattigeri, and R. Lai. Optimizing mode connectivity via neuron alignment. _Advances in Neural Information Processing Systems_, 33:15300-15311, 2020.
* [63] E. H. Thiede, T. S. Hy, and R. Kondor. The general theory of permutation equivarant neural networks and higher order graph variational encoders. _arXiv preprint arXiv:2004.03990_, 2020.
* [64] T. Unterthiner, D. Keysers, S. Gelly, O. Bousquet, and I. Tolstikhin. Predicting neural network accuracy from weights. _arXiv preprint arXiv:2002.11448_, 2020.
* [65] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.

* [66] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.
* [67] S. Yak, J. Gonzalvo, and H. Mazzawi. Towards task and architecture-independent generalization gap predictors. _arXiv preprint arXiv:1906.01550_, 2019.
* [68] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. Salakhutdinov, and A. Smola. Deep sets. doi: 10.48550. _arXiv preprint arXiv:1703.06114_, 2017.
* [69] C. Zhang, M. Ren, and R. Urtasun. Graph hypernetworks for neural architecture search. _arXiv preprint arXiv:1810.05749_, 2018.

[MISSING_PAGE_EMPTY:15]

[MISSING_PAGE_EMPTY:16]

with the same dimensions. The learnable parameters are in blue:

\[H(U)\coloneqq (Y,z)\in\mathcal{W}\times\mathcal{V}\] (3) \[Y^{(i)}_{jk}\coloneqq \sum_{s}\left(a^{i,s}_{\varphi}W^{(s)}_{\star,\star}+a^{i,s}_{ \phi}v^{(s)}_{\star}\right)+b^{i,i}_{\phi}W^{(i)}_{\star,k}+b^{i,i-1}_{\phi}W^{ (i-1)}_{k,\star}\] \[+b^{i}_{\phi}v^{(i)}_{j}+c^{i,i}_{\phi}W^{(i)}_{j,\star}+c^{i,i+1 }_{\phi}W^{(i+1)}_{\star,j}+c^{i}_{\phi}v^{(i-1)}_{k}+d^{i}_{\phi}W^{(i)}_{jk}\] \[z^{(i)}_{j}\coloneqq \sum_{s}\left(a^{i,s}_{\varphi}W^{(s)}_{\star,\star}+a^{i,s}_{ \psi}v^{(s)}_{\star}\right)+b^{i,i}_{\varphi}W^{(i)}_{j,\star}+b^{i,i+1}_{ \varphi}W^{(i+1)}_{\star,j}+b^{i}_{\psi}v^{(i)}_{j},\]

where \(\star\) denotes summation or averaging over a dimension.

### Proof of Proposition 1

Here we show that the layer defined in Eq. 3 is not only \(\mathcal{S}\)-equivariant, it can express _every_ linear equivariant function \(\mathcal{U}\to\mathcal{U}\). Our strategy is to first consider general linear layers \(T(\cdot;\theta):\mathcal{U}\to\mathcal{U}\) parameterized by \(\theta\in\Theta\), and then constrain to \(\theta\) such that \(T(\cdot;\theta)\) is \(\mathcal{S}\)-equivariant. Applying this constraint will directly yield Eq. 3.

For our purposes, it is sometimes convenient to distinguish layer, row, and column indices of entries in \(U\), so we split the parameters \(\theta=(\vartheta,\phi,\varphi,\psi)\) and write \(T:\mathcal{U}\to\mathcal{U}\) in the form:

\[T(U;\theta) \coloneqq(Y(U),z(U))\quad\in\mathcal{W}\times\mathcal{V}=\mathcal{U}\] (4) \[Y(U)^{(i)}_{jk} \coloneqq\sum_{s=1}^{L}\sum_{p=1}^{n_{s}}\sum_{q=1}^{n_{s-1}} \vartheta^{ijk}_{spq}W^{(s)}_{pq}+\sum_{s=1}^{L}\sum_{p=1}^{n_{s}}\phi^{ijk}_{ sp}v^{(s)}_{p}\] (5) \[z(U)^{(i)}_{j} \coloneqq\sum_{s=1}^{L}\sum_{p=1}^{n_{s}}\sum_{q=1}^{n_{s-1}} \varphi^{ij}_{spq}W^{(s)}_{pq}+\sum_{s=1}^{L}\sum_{p=1}^{n_{s}}\psi^{ij}_{sp}v^ {(s)}_{p}.\] (6)

\begin{table}
\begin{tabular}{c|c c c c}  & \(s=i-1\) & \(s=i\) & other \(s\) \\ \hline \(\vartheta^{ijk}_{spq}\) & \(\begin{cases}a^{i,i-1}_{\phi}&k\neq p\\ b^{i,i-1}_{\phi}&k=p\end{cases}\) & \(\begin{cases}a^{i,i}_{\phi}&j\neq p,k\neq q\\ b^{i,i}_{\phi}&j=p,k\neq q\\ c^{i,i}_{\phi}&j=p,k=q\\ d^{i}_{\phi}&j=p,k=q\end{cases}\) & \(\begin{cases}a^{i,i+1}_{\phi}&j\neq q\\ c^{i,i+1}_{\phi}&j\neq q\\ c^{i,i}_{\phi}&j=q\end{cases}\) & \(a^{i,s}_{\phi}\) \\ \hline \(\varphi^{ij}_{spq}\) & \(\begin{cases}a^{i,i-1}_{\varphi}&j\neq p\\ b^{i,i-1}_{\varphi}&j=p\end{cases}\) & \(\begin{cases}a^{i,i}_{\varphi}&j\neq q\\ b^{i,i}_{\varphi}&j=q\end{cases}\) & \(a^{i,s}_{\varphi}\) \\ \hline \(\psi^{ij}_{sp}\) & \(\begin{cases}a^{i,i}_{\psi}&j\neq p\\ b^{i,i}_{\psi}&j=p\end{cases}\) & \(a^{i,s}_{\psi}\) \\ \hline \end{tabular}
\end{table}
Table 7: \(\mathcal{S}\)-equivariant parameter sharing for linear maps \(\text{vec}(U)\mapsto\theta\text{vec}(U)\). Parameter sharing is a system of constraints on the entries of \((\vartheta,\phi,\varphi,\psi)=\theta\). Each table is organized by the layer indices \((i,s)\). For example, the first table says that for any \((i,s)\) where \(s=i-1\), we constrain \(\vartheta^{ijk}_{spq}=\vartheta^{i,j^{\prime},k^{\prime}}_{s,p^{\prime},q^{ \prime}}=a^{i,i-1}_{\vartheta}\) for any \(j,k,p,q\) and \(j^{\prime},k^{\prime},p^{\prime},q^{\prime}\) where \(k\neq p\) and \(k^{\prime}\neq p^{\prime}\). After parameter sharing, we observe that there are only a constant number of free parameters for each \((i,s)\) pair, adding up to \(O\left(L^{2}\right)\) parameters total.

We introduce the notation \(U_{\alpha}\) to identify individual entries of \(U\). Here \(\alpha\) is a tuple of length two or three, for indexing into either a weight or bias. We denote the space of valid index tuples of \(\mathcal{W}\) and \(\mathcal{V}\) by \(\mathbb{W}\) and \(\mathbb{V}\), respectively, and define \(\mathbb{U}\coloneqq\mathbb{W}\cup\mathbb{V}\) as the combined index space of \(\mathcal{U}\). For example, if \(\alpha=(i,j,k)\in\mathbb{W}\), then \(U_{\alpha}=W_{jk}^{(i)}\).

We can then define the index space \(\mathbb{I}\coloneqq\mathbb{U}\times\mathbb{U}\) for parameters \(\theta\in\Theta\). We use \(\theta_{\beta}^{\alpha}\) to index an entry of \(\theta\) with upper and lower indices \([\alpha,\beta]\in\mathbb{I}\,\). For example, if \(\alpha=(i,j,k)\in\mathbb{W}\) and \(\beta=(s,p,q)\in\mathbb{W}\), we have \(\theta_{\beta}^{\alpha}=\vartheta_{spq}^{ijk}\).

By flattening \(U\) into a vector, we can think of \(\theta\) as a matrix with indices \(\alpha\) and \(\beta\). Eq. 4 can be rewritten in the flattened form:

\[T(U;\theta)_{\alpha}=\sum_{\beta}\theta_{\beta}^{\alpha}U_{\beta}.\] (7)

In the flattened representation, we can write the action of \(\sigma\in\mathcal{S}\) on \(\text{vec}(U)\) by a permutation matrix \(P_{\sigma}\). Equivariance requires that \(P_{\sigma}\theta\text{vec}(U)=\theta P_{\sigma}\text{vec}(U)\) for any \(\sigma\in\mathcal{S}\). Since the input \(U\) can be anything, we get the following constraint on \(\theta\):

\[P_{\sigma}\theta=\theta P_{\sigma},\quad\forall\sigma\in\mathcal{S}.\] (8)

We can re-express the action of \(\mathcal{S}\) on \(\mathcal{U}\) (Eq. 1) as an action on the index space \(\mathbb{U}\). For \(\alpha\in\mathbb{U}\), we write \(\sigma(\alpha)\) which is defined:

\[\sigma(i,j,k) =(i,\sigma_{i}(j),\sigma_{i-1}(k))\quad(i,j,k)\in\mathbb{W}\] (9) \[\sigma(i,j) =(i,\sigma_{i}(j))\quad(i,j)\in\mathbb{V},\] (10)

for any \(\sigma\in\mathcal{S}\). We extend this definition into an action of \(\mathcal{S}\) on \(\mathbb{I}\):

\[\sigma\left[\alpha,\beta\right]\coloneqq[\sigma\alpha,\sigma\beta],\quad \left[\alpha,\beta\right]\in\mathbb{U}\times\mathbb{U}.\] (11)

We can now rewrite the equivariance constraint in Eq. 8 using indices \(\alpha,\beta\):

\[\left[P_{\sigma}\theta\right]_{\beta}^{\alpha}=\theta_{\beta}^{\sigma^{-1}( \alpha)}=\theta_{\sigma(\beta)}^{\alpha}=\left[\theta P_{\sigma}\right]_{ \beta}^{\alpha}.\] (12)

By relabeling \(\alpha\leftarrow\sigma^{-1}(\alpha)\), we can rewrite this condition \(\theta_{\beta}^{\alpha}=\theta_{\sigma(\beta)}^{\sigma(\alpha)}\). Hence, equivariance is equivalent to requiring that \(\theta\) share parameters within orbits under the action of \(\mathcal{S}\) on its indices \(\alpha,\beta\)[54, Prop 3.1]. We characterize these orbits in Sec. B.2.1, yielding Table 7 which gives the parameter sharing as a system of constraints on the indices of \(\theta\). These constraints characterize the space of \(\mathcal{S}\)-equivariant \(T(\cdot,\theta)\). In Sec. B.2.2, we conclude the proof by showing that under parameter sharing, \(T(\cdot,\theta)\) is equivalent to Eq. 3.

#### b.2.1 Parameter orbits and sharing

We now partition the parameters of \(\theta\) into orbits under the \(\mathcal{S}\)-action on its index space, in order to generate the parameter sharing constraints given in Table 7. The index space of \(\theta\) is \(\mathbb{I}=\mathbb{U}\times\mathbb{U}\). There are four subsets of \(\mathbb{I}\):

1. \(\mathbb{I}^{WW}\coloneqq\mathbb{W}\times\mathbb{W}\): Contains \([\alpha,\beta]=[(i,j,k),(s,p,q)]\), indexing parameters \(\vartheta_{spq}^{ijk}\).
2. \(\mathbb{I}^{WW}\coloneqq\mathbb{W}\times\mathbb{V}\): Contains \([\alpha,\beta]=[(i,j,k),(s,p)]\), indexing parameters \(\phi_{sp}^{ijk}\).
3. \(\mathbb{I}^{VW}\coloneqq\mathbb{V}\times\mathbb{W}\): Contains \([\alpha,\beta]=[(i,j),(s,p,q)]\), indexing parameters \(\phi_{spq}^{ij}\).
4. \(\mathbb{I}^{VV}\coloneqq\mathbb{V}\times\mathbb{V}\): Contains \([\alpha,\beta]=[(i,j),(s,p)]\), indexing parameters \(\psi_{sp}^{ij}\).

Consider the block of indices \(\mathbb{I}^{WW}=\mathbb{W}\times\mathbb{W}\), containing \([\alpha,\beta]=[(i,j,k),(s,p,q)]\) indexing parameters \(\vartheta_{\beta}^{\alpha}\). Since the \(\mathcal{S}\)-action never changes the layer indices \((i,s)\), we can independently consider orbits within sub-blocks of indices \(\mathbb{I}^{WW}_{i,s}=\{\,[(i,j,k),(s,p,q)]\mid\forall j,k,p,q\,\}\). The number of orbits within each sub-block \(\mathbb{I}^{WW}_{i,s}\) depends on the relationship between the layer indices \(i\) and \(s\): they are either the same layer (\(s=i\)), they are adjacent (\(s=i-1\) or \(s=i+1\)), or they are non-adjacent (\(s\notin\{i-1,i,i+1\}\)). We now analyze the orbits of sub-blocks for a few cases.

If \(s=i-1\), then choose any two indices \(\left[\alpha^{(1)},\beta^{(1)}\right],\left[\alpha^{(2)},\beta^{(2)}\right]\in \mathbb{I}_{i,s}^{WW}\) where the first satisfies \(p\neq k\) and the second satisfies \(p=k\). Then the orbits of each index are:

\[\text{Orbit}\left(\left[\alpha^{(1)},\beta^{(1)}\right]\right)= \left\{\,\left[(i,j,k),(s,p,q)\right]\,|\;\forall j,k,p,q:p\neq k\,\right\}\] (13) \[\text{Orbit}\left(\left[\alpha^{(2)},\beta^{(2)}\right]\right)= \left\{\,\left[(i,j,k),(s,p,q)\right]\,|\;\forall j,k,p,q:p=k\,\right\}.\] (14)

We see that these two orbits actually partition the entire sub-block of indices \(\mathbb{I}_{i,s}^{WW}\), with each orbit characterized by whether or not \(p=k\). We introduce the parameters \(a_{\vartheta}^{i,i-1}\) (for the first orbit) and \(b_{\vartheta}^{i,i-1}\) (for the second orbit). Under equivariant parameter sharing, all parameters of \(\vartheta\) corresponding \(\mathbb{I}_{i,s}^{WW}\) are equal to either \(a_{\vartheta}^{i,i-1}\) or \(b_{\vartheta}^{i,i-1}\), depending on whether \(p=k\) or \(p\neq k\).

If \(s=i+1\), we instead choose any two indices where the first satisfies \(j\neq q\) and the second satisfies \(j=q\). Then the sub-block of indices \(\mathbb{I}_{i,s}^{WW}\) is again partitioned into two orbits:

\[\left\{\,\left[(i,j,k),(s,p,q)\right]\,|\;\forall j,k,p,q:j\neq q\,\right\}, \text{ and }\left\{\,\left[(i,j,k),(s,p,q)\right]\,|\;\forall j,k,p,q:j=q\,\right\}\] (15)

depending on the condition \(j=q\). We name two parameters \(a_{\vartheta}^{i,i+1}\) and \(c_{\vartheta}^{i,i+1}\) for this sub-block, with one for each orbit.

We can repeat this process for sub-blocks of \(\mathbb{I}^{WW}\) where \(i=s\) and \(s\notin\{i-1,i,i+1\}\), as well as for the other three blocks of \(\mathbb{I}\). Table 7 shows the complete parameter sharing constraints on \(\theta\) resulting from partitioning all possible sub-blocks into orbits.

**Number of parameters.** We also note that every layer pair \((i,s)\) introduces only a constant number of parameters: the number of parameters in each cell of Table 7 has no dependence on the input, output, or hidden dimensions of \(\mathcal{U}\). Hence the number of distinct parameters after parameter sharing simply grows with the number of layer pairs, i.e. \(O\left(L^{2}\right)\).

#### b.2.2 Equivalence to equivariant NF-Layer definition

All that remains is to show that the map \(T(\cdot;\theta):\mathcal{U}\to\mathcal{U}\) with \(\mathcal{S}\)-equivariant parameter sharing (Table 7) is equivalent to the NF-Layer \(H\) we defined in Eq. 3.

Consider a single term from Eq. 4 where \(s=i-1\). Substituting using the constraints of Table 7, we simplify:

\[\sum_{p,q}\vartheta_{i-1,p,q}^{i,j,k}W_{pq}^{(i-1)} =a^{i,i-1}\sum_{q}\sum_{k\neq p}W_{p,q}^{(i-1)}+b^{i,i-1}\sum_{q} W_{k,q}^{(i-1)}\] (16) \[=a^{i,i-1}W_{\star,\star}^{(i-1)}+(b^{i,i-1}-a^{i,i-1})W_{k,\star }^{(i-1)}.\]

We can then reparameterize \(b^{i,i-1}\gets b^{i,i-1}-a^{i,i-1}\), resulting in two terms that appear in Eq. 3. We can simplify every term of Eq. 4 in a similar manner using the parameter sharing of Table 7, reducing the general layer to the \(\mathcal{S}\)-equivariant NF-Layer.

### Proof of Proposition 2

Analogous to Sec. B.2, we begin with general parameterized linear maps \(T(\cdot;\theta):\mathcal{U}\to\mathbb{R}^{d}\) and characterize the parameter sharing implied by \(\mathcal{S}\)-invariance. We then show that \(T(\cdot,\theta)\) under these constraints is equivalent to \(f\circ P\), where \(f:\mathbb{R}^{2L}\to\mathbb{R}^{d}\) is a parameterized linear map (i.e., a fully connected layer) and \(P(U)=\left(W_{\star,\star}^{(1)},\cdots,W_{\star,\star}^{(L)},v_{\star}^{(1)},\cdots,v_{\star}^{(L)}\right)\). We will assume \(d=1\) for the rest of this section, but it is straightforward to generalize our arguments to arbitrary \(d\).

In general, we can parameterize linear maps \(\mathcal{U}\to\mathbb{R}\) by \(\theta\in\mathbb{R}^{\dim(\mathcal{U})}\), where:

\[T(U,\theta)=\sum\theta_{\alpha}U_{\alpha},\] (17)

where \(\alpha\) is an index for either weights or biases as in Sec. B.2. The invariance constraint \(T(U)=T(\sigma U)\) implies:

\[\theta_{\alpha}=\theta_{\sigma(\alpha)},\] (18)

where \(\sigma(\alpha)\) is defined by Eq. 9.

We can now characterize the orbits of the indices of \(\theta\) under \(\mathcal{S}\). In this case, the orbits (and resulting parameter sharing) are relatively simple. For a fixed \(i\), all \((i,j,k)\in\mathbb{W}\) form an orbit, and we introduce a shared parameter \(a^{i}\). Similarly, for each \(i\) all \((i,j)\in\mathbb{V}\) form an orbit, and we introduce a shared parameter \(b^{i}\) for each. We can then rewrite the layer:

\[T(U;\theta)=\sum_{i}\left(a^{i}\sum_{jk}W^{(i)}_{jk}+b^{i}\sum_{j}v^{(i)}_{j} \right)=\sum_{i}a^{i}W^{(i)}_{*,*}+b^{i}v^{(i)}_{*}.\] (19)

This is simply a linear combination of the output of \(P(U)\) parameterized by \(\{(a^{i},b^{i})\}\), and can be written as the composition \(f\circ P\) where \(f\) is a fully connected layer.

## Appendix C NF-Layers for the HNP setting

### Equivariant NF-Layer

Because an expression for the \(\tilde{\mathcal{S}}\)-equivariant NF-Layer analogous to Eq. 3 would be unwieldy, we instead define the layer in terms of its parameter sharing (Tables 8-11) on \(\theta\).

We can derive HNP-equivariant parameter sharing of \(\theta\) using a similar strategy to Sec. B.2.1: we partition the index spaces \(\mathbb{I}^{WW},\mathbb{I}^{VV},\mathbb{I}^{VW},\mathbb{I}^{VV}\) into orbits under the action of \(\tilde{\mathcal{S}}\), and share parameters within each corresponding orbit of \(\vartheta,\phi,\varphi,\psi\). The resulting parameter sharing is different from the NP-setting because while the action of \(\mathcal{S}\) on \(\mathcal{U}\) could permute the rows and columns of every weight and bias, the action of \(\tilde{\mathcal{S}}\) on \(\mathcal{U}\) does not affect the columns of \(W^{(1)}\) or the rows of \(W^{(L)},v^{(L)}\), which correspond to input and output dimensions (respectively).

The orbits are again analyzed within sub-blocks defined by the values of the layer indices \((i,s)\). As with the NP setting, there are broadly four types of sub-blocks based on whether \(i=s\), \(s=i-1\), \(s=i+1\), or \(s\notin\{i-1,i,i+1\}\). However, there are now additional considerations based on whether \(i\) or \(s\) is an input or output layer. For example, consider the sub-block of \(\mathbb{I}^{WW}\) where \(i=s=1\), which we denote \(\mathbb{I}^{WW}_{1,1}\). The action on the indices in this sub-block can be written \(\sigma\left[\alpha,\beta\right]=[(1,\sigma_{1}(j),k),(1,\sigma_{1}(p),q)]\). Importantly, the column indices \(k,q\) are never permuted since they correspond to the input layer. We see that \(\mathbb{I}^{WW}_{1,1}\) contains two orbits _for each_\(k\in\llbracket 1..n_{0}\rrbracket\) and \(q\in\llbracket 1..n_{0}\rrbracket\), with the two orbits characterized by whether or not \(j=p\). Hence we have \(2n_{0}^{2}\) orbits and Table 8 introduces \(2n_{0}^{2}\) parameters \(\left\{\begin{array}{l}a^{1,1,k,q}_{\vartheta},b^{1,1,k,q}_{\vartheta}\end{array} \right|k,q\in\llbracket 1..n_{0}\rrbracket\) \(\left\}\) for this sub-block of parameters.

Now consider another sub-block of \(\mathbb{I}^{WW}\) where \(1<i=s<L\). Now the action of \(\tilde{\mathcal{S}}\) on indices in this sub-block can be written \(\sigma\left[\alpha,\beta\right]=[(i,\sigma_{i}(j),\sigma_{i-1}(k)),(i,\sigma_ {i}(p),\sigma_{i-1}(q))]\). Then we have a total of two orbits characterized by whether or not \(k=p\), rather than \(2n_{0}^{2}\) orbits for the \(i=1\) case. Tables 8-11 present the complete parameter sharing for each of \(\vartheta,\phi,\varphi,\psi\), resulting from analyzing every possible orbit within any sub-block of \(\mathbb{I}^{WW},\mathbb{I}^{WV},\mathbb{I}^{VW},\mathbb{I}^{VV}\).

### Invariant NF-Layer

While the NP-invariant NF-Layer sums over the rows and columns of every weight and bias, under HNP assumptions there is no need to sum over the columns of \(W^{(1)}\) (inputs) or the rows of \(W^{(L)},v^{(L)}\) (outputs). So the HNP invariant NF-Layer \(\tilde{P}:\mathcal{U}\rightarrow\mathbb{R}^{2L+n_{0}+2n_{L}}\) is defined:

\[\tilde{P}(U)=\left(P(U),W^{(1)}_{*,:},W^{(L)}_{*,*},v^{(L)}\right),\] (20)

where \(W^{(1)}_{*,:}\) and \(W^{(L)}_{:,*}\) denote summing over only the rows or only the columns of the matrix, respectively. Note that \(\tilde{P}\) satisfies \(\tilde{\mathcal{S}}\)-invariance without satifying \(\mathcal{S}\)-invariance.

## Appendix D Additional experimental details

### Predicting generalization

The model we use consists of three equivariant NF-Layers with 16, 16, and 5 channels respectively. We apply ReLU activations after each linear NF-Layer. The resulting weight-space features are passed

\begin{table}
\begin{tabular}{c|c|c|c|c} \multicolumn{4}{c}{\(\vartheta^{ijk}_{spq}\)} \\ \hline \multirow{3}{*}{\(s=i-1\)} & \(i=2\) & \(2<i<L\) & \(i=L\) \\ \cline{2-5}  & \(\begin{cases}a_{\vartheta}^{2,1,q}&k\neq p\\ b_{\vartheta}^{2,1,q}&k=p\end{cases}\) & \(\begin{cases}a_{\vartheta}^{i,i-1}&k\neq p\\ b_{\vartheta}^{i,i-1}&k=p\end{cases}\) & \(\begin{cases}a_{\vartheta}^{L,L-1,j}&k\neq p\\ b_{\vartheta}^{L,L-1,j}&k=p\end{cases}\) \\ \hline \multirow{3}{*}{\(s=i\)} & \(i=1\) & \(1<i<L\) & \(i=L\) \\ \cline{2-5}  & \(\begin{cases}a_{\vartheta}^{1,1,k,q}&j\neq p\\ b_{\vartheta}^{1,1,k,q}&j=p\end{cases}\) & \(\begin{cases}a_{\vartheta}^{i,i}&j\neq p,k\neq q\\ b_{\vartheta}^{i,j}&j=p,k\neq q\\ c_{\vartheta}^{i,i}&j\neq p,k=q\end{cases}\) & \(\begin{cases}a_{\vartheta}^{L,L,j,p}&k\neq q\\ c_{\vartheta}^{L,L,j,p}&k=q\end{cases}\) \\ \hline \multirow{3}{*}{\(s=i+1\)} & \(i=1\) & \(1<i<L-1\) & \(i=L-1\) \\ \cline{2-5}  & \(\begin{cases}a_{\vartheta}^{1,2,k}&j\neq q\\ c_{\vartheta}^{1,2,k}&j=q\end{cases}\) & \(\begin{cases}a_{\vartheta}^{1,2}&j\neq q\\ c_{\vartheta}^{1,2}&j=q\end{cases}\) & \(\begin{cases}a_{\vartheta}^{L-1,L,p}&j\neq q\\ c_{\vartheta}^{L-1,L,p}&j=q\end{cases}\) \\ \hline \multirow{3}{*}{\(\text{other}\)\(s\)} & \(i=1,1<s<L\) & \(i=1,s=L\) & \(1<i<L,s=L\) \\ \cline{2-5}  & \(a_{\vartheta}^{1,s,k}\) & \(a_{\vartheta}^{1,L,k,p}\) & \(a_{\vartheta}^{i,L,p}\) \\ \cline{2-5}  & \(1<i<L,s=1\) & \(i=L,s=1\) & \(i=L,1<s<L\) \\ \cline{2-5}  & \(a_{\vartheta}^{i,1,q}\) & \(a_{\vartheta}^{L,1,j,q}\) & \(a_{\vartheta}^{L,s,j}\) \\ \cline{2-5}  & \(1<i<L,1<s<L\) & & & \\ \cline{2-5}  & \(a_{\vartheta}^{i,s}\) & & & \\ \end{tabular}
\end{table}
Table 8: HNP-equivariant parameter sharing on \(\vartheta\subseteq\theta\), corresponding to the NF-Layer \(\tilde{H}:\mathcal{U}\rightarrow\mathcal{U}\).

\begin{table}
\begin{tabular}{c|c|c|c} \multicolumn{4}{c}{\(\phi^{ijk}_{spq}\)} \\ \hline \multirow{3}{*}{\(s=i-1\)} & & \(1<i<L\) & \(i=L\) \\ \cline{2-5}  & & \(\begin{cases}a_{\vartheta}^{i,i-1}&k\neq p\\ b_{\vartheta}^{i,i-1}&k=p\end{cases}\) & \(\begin{cases}a_{\vartheta}^{L,L-1,j}&k\neq p\\ b_{\vartheta}^{L,L-1,j}&k=p\end{cases}\) \\ \hline \multirow{3}{*}{\(s=i\)} & \(i=1\) & \(1<i<L\) & \(i=L\) \\ \cline{2-5}  & \(\begin{cases}a_{\vartheta}^{1,1,k}&j\neq p\\ b_{\vartheta}^{1,1,k}&j=p\end{cases}\) & \(\begin{cases}a_{\vartheta}^{1,i}&j\neq p\\ b_{\vartheta}^{i,i}&j=p\end{cases}\) & \(b_{\vartheta}^{L,L,j,p}\) \\ \hline \multirow{3}{*}{\(s=i\)} & \(i=1,1<s<L\) & \(i=1,s=L\) & \(1<i<L,s=L\) \\ \cline{2-5}  & \(a_{\vartheta}^{1,s,k}\) & \(a_{\vartheta}^{1,L,k,p}\) & \(a_{\vartheta}^{i,L,p}\) \\ \cline{1-1} \cline{2-5}  & \(1<i<L,1\leq s<L\) & \(i=L,1\leq s<L\) & \\ \cline{1-1} \cline{2-5}  & \(a_{\vartheta}^{i,s}\) & \(a_{\vartheta}^{L,s,j}\) & \\ \cline{1-1} \cline{2-5}  & \(1<i<L,1<s<L\) & & & \\ \cline{1-1} \cline{2-5}  & \(a_{\vartheta}^{i,s}\) & & & \\ \end{tabular}
\end{table}
Table 9: HNP-equivariant parameter sharing on \(\phi\subset\theta\), corresponding to the NF-Layer \(\tilde{H}:\mathcal{U}\rightarrow\mathcal{U}\).

into an invariant NF-Layer with mean pooling. The output of the invariant NF-Layer is flattened and projected to \(\mathbb{R}^{1,000}\). The resulting vector is then passed through an MLP with two hidden layers, each with 1,000 units and ReLU activations. The output is linearly projected to a scalar and passed through a sigmoid function. Since the output of the model can be interpreted as a probability, we train the model with binary cross-entropy with hyperparameters outlined in Table 12. The model is trained for 50 epochs with early stopping based on \(\tau\) on the validation set, which takes \(1\) hour on a Titan RTX GPU.

### Predicting "winning ticket" masks from initialization

Concretely, the encoder learns the posterior distribution \(q_{\theta}(Z\mid U_{0},M)\) where \(Z\in\mathbb{R}^{\dim(\mathcal{U})\times C}\) is the latent variable for the winning tickets and \(C\) is the number of latent channels. The decoder learns \(p_{\theta}(M\mid U_{0},Z)\), and both encoder and decoder are implemented using our equivariant NF-Layers. For the prior \(p(Z)\) we choose the isometric Gaussian distribution, and train using the evidence lower bound (ELBO):

\[\mathcal{L}_{\theta}(M,U_{0})=\mathbb{E}_{z\sim q_{\theta}(\cdot\mid U_{0},M)} \Big{[}\ln p_{\theta}(M\mid U_{0},z)\Big{]}-\text{D}_{\text{KL}}\Big{(}q_{ \theta}(\cdot\mid U_{0},M)\mid\mid p(\cdot)\Big{)}.\]

\begin{table}
\begin{tabular}{c|c|c|c} \multicolumn{4}{c}{\(\varphi_{spq}^{ij}\)} \\ \hline \multirow{3}{*}{\(s=i\)} & \(i=1\) & \(1<i<L\) & \(i=L\) \\ \cline{2-4}  & \(\begin{cases}a_{\varphi}^{1,1,q}&j\neq p\\ b_{\varphi}^{1,1,k}&j=p\end{cases}\) & \(\begin{cases}a_{\varphi}^{i,i}&j\neq p\\ b_{\varphi}^{i,\varphi}&j=p\end{cases}\) & \(b_{\varphi}^{L,L,j,p}\) \\ \hline \multirow{3}{*}{\(s=i+1\)} & & \(1\leq i<L-1\) & \(i=L-1\) \\ \cline{2-4}  & & \(\begin{cases}a_{\varphi}^{i,i+1}&j\neq p\\ b_{\varphi}^{i,i+1}&j=p\end{cases}\) & \(\begin{cases}a_{\varphi}^{i,-1,L,p}&j\neq p\\ b_{\varphi}^{L-1,L,p}&j=p\end{cases}\) \\ \hline \multirow{3}{*}{\(\text{other }s\)} & \(1\leq i<L,s=1\) & \(1\leq i<L,1<s<L\) & \(1\leq i<L,s=L\) \\ \cline{2-4}  & \(a_{\varphi}^{i,s,q}\) & \(a_{\varphi}^{i,s}\) & \(a_{\varphi}^{i,L,p}\) \\ \cline{1-1} \cline{2-4}  & \(i=L,s=1\) & \(i=L,1<s<L\) & \\ \cline{1-1} \cline{2-4}  & \(a_{\varphi}^{L,1,j,q}\) & \(a_{\varphi}^{L,s,j}\) & \\ \end{tabular}
\end{table}
Table 10: HNP-equivariant parameter sharing on \(\varphi\subset\theta\), corresponding to the NF-Layer \(\tilde{H}:\mathcal{U}\to\mathcal{U}\).

\begin{table}
\begin{tabular}{c|c|c} \multicolumn{3}{c}{\(\psi_{sp}^{ij}\)} \\ \hline \multirow{3}{*}{\(s=i\)} & \multirow{3}{*}{\(\begin{cases}a_{\varphi}^{i,i}&j\neq p\\ b_{\varphi}^{i,i}&j=p\end{cases}\)} \\ \cline{2-3}  & & \(\begin{cases}a_{\varphi}^{i,i}&j\neq p\\ b_{\varphi}^{i,i}&j=p\end{cases}\) & \(b_{\psi}^{L,L,j,p}\) \\ \hline \multirow{3}{*}{\(\text{other }s\)} & \(1\leq i<L,s=L\) & \(i=L,1\leq s<L\) & \(1\leq i<L,1\leq s<L\) \\ \cline{2-3}  & \(a_{\psi}^{i,L,p}\) & \(a_{\psi}^{L,s,j}\) & \(a_{\psi}^{i,s}\) \\ \end{tabular}
\end{table}
Table 11: HNP-equivariant parameter sharing on \(\psi\subset\theta\), corresponding to the NF-Layer \(\tilde{H}:\mathcal{U}\to\mathcal{U}\).

The initialization and sparsity mask are concatenated so the input to the encoder \(q_{\theta}\) is \((U,M)\in\mathbb{R}^{\dim(\mathcal{U})\times 2}\). After the bottleneck, we concatenate the latent variables and the original mask along the channels, i.e. the decoder input is \((U_{0},Z)\in\mathbb{R}^{\dim(\mathcal{U})\times(C+1)}\).

The first dataset uses three-layer MLPs with 128 hidden units trained on MNIST and the second uses CNNs with three convolution layers (128 channels) and 2 fully-connected layers trained on CIFAR-10. In each dataset, we include 400 pairs for training and hold out 50 for evaluation. The hyperparameter details are in Table 13. The encoder and decoder models contain 4 equivariant NF-Layers with 64 hidden channels within each layer. The latent variable is 5 dimensions. Training takes 5H on a Titan RTX GPU.

### Classifying INRs

We use SIREN [58] for our INRs of CIFAR-10, FashionMNIST, and MNIST. For the SIREN models, we used a three-layer architecture with 32 hidden neurons in each layer. We trained the SIRENs for 5,000 steps using Adam optimizer with a learning rate of \(5\times 10^{-5}\). Datasets were split into 45,000 training images, 5,000 validation images, and 10,000 (MNIST, CIFAR-10) or 20,000 (FashionMNIST) test images. We trained 10 copies (MNIST, FashionMNIST) or 20 copies (CIFAR-10) of SIRENs on each training image with different initializations, and a single SIREN on each validation and test image. No additional data augmentation was applied. For 3D shape classification, we adopt the same protocol introduced in [12], and we train each SIREN to fit the Unsigned Distance Function (UDF) value of points sampled around a shape. Each SIREN is composed of a single hidden layer with 128 neurons. We use Adam as an optimizer and we train for 1,000 steps.

We also trained neural functionals with three equivariant NF-Layers + ReLU activations, each with 512 channels, followed by invariant NF-Layers (mean pooling) and a three-layer MLP head with 1,000 hidden units and ReLU activation. Dropout was applied to the MLP head only. For the NFN IO-encoding, we used sinusoidal position encoding with a maximum frequency of 10 and 6 frequency bands (dimension 13). The training hyperparameters are shown in Table 14, and training took \(\sim 4\)H on a Titan RTX GPU.

We also experimented with larger MLPs (4,000 and 8,000 hidden units per layer) that have parameter counts comparable to those of the NFNs, but found that it did not significantly increase test accuracy, as shown in Table 16.

### Weight-space style editing

For weight-space editing, we use the same INRs as the ones used for classification but we do not augment the dataset with additional INRs. Let \(U_{i}\) be the INR weights for the \(i^{\text{th}}\) image and \(\text{SIREN}(x,y;U)\) be the output of the INR parameterized by \(U\) at coordinates \((x,y)\). We edit the INR

\begin{table}
\begin{tabular}{l r} \hline \hline
**Name** & **Values** \\ \hline Optimizer & Adam \\ Learning rate & \(1\times 10^{-3}\) \\ Batch size & [4, 8] \\ Epoch & 200 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Hyperparameters for predicting LTH on MNIST and CIFAR-10.

\begin{table}
\begin{tabular}{l r} \hline \hline
**Name** & **Values** \\ \hline Optimizer & Adam \\ Learning rate & \(1\times 10^{-4}\) \\ Batch size & 32 \\ Training steps & \(2\times 10^{5}\) \\ MLP dropout & \(0.5\) \\ \hline \hline \end{tabular}
\end{table}
Table 14: Hyperparameters for classifying INRs on MNIST and CIFAR-10 using neural functionals.

weights \(U^{\prime}_{i}=U_{i}+\gamma\cdot\text{NFN}(U_{i})\), and \(\gamma\) is a learned scalar initialized to \(0.01\). Letting \(f_{i}(x,y)\) be the pixel values of the ground truth edited image (obtained from image-space processing), the objective is to minimize mean squared error:

\[\mathcal{L}\left(\text{NFN}\right)=\frac{1}{N\cdot d^{2}}\sum_{i=1}^{N}\sum_{x,y}^{d}\|\text{SIREN}\left(x,y;U^{\prime}\right)-f_{i}(x,y)\|_{2}^{2}.\] (21)

Note that since the SIREN itself is differentiable, the loss can be directly backpropagated through \(U^{\prime}\) to the parameters of the NFN.

The neural functionals contain 3 equivariant NF-Layers with 128 channels, one invariant NF-Layer (mean pooling) followed by 4 linear layers with 1,000 hidden neurons. Every layer uses ReLU activation. The training hyperparameters can be found in Table 15, and training takes \(\sim 1\) hour on a Titan RTX GPU.

## Appendix E Additional experiments and analysis

### Analyzing train vs test accuracies in INR classification

Table 16 shows the training and test accuracies of our NFNs and other weight space methods across the benchmarks we consider in this paper. An interesting observation is that in addition to better test performance, NFN typically achieve much higher _training_ accuracies compared to the MLPs. \(\text{MLP}_{\text{Aug}}\), the MLP variant trained with permutation augmentations, especially struggles to achieve low training error. This indicates that training architectures to respect neuron permutation symmetries is extremely challenging, perhaps because these symmetry groups are extremely large [1]. Therefore, encoding equivariance or invariance via architecture design, as we do with NFNs, is much more effective.

### Non-weight-space baselines for image classification

For the image and 3D shape classification tasks discussed in Sec. 3.2, we compared the neural functionals to non-weight-space baselines, namely residual networks [26] for MNIST, FashionMNIST, and CIFAR-10, and PointNet [52] for ShapeNet-10 and ScanNet-10. The results, shown in Table 16, indicate that applying state of the art architectures to pixels or point clouds is more effective than any weight space method. This may be because these architectures can leverage geometric properties of image and shape classification tasks. While neural functionals handle weight space symmetries, it remains an open problem to incorporate the geometric symmetries of common machine learning problems into weight space methods.

### Ablating positional encoding

We ablate the effect of IO-encoding on the \(\text{NFN}_{\text{NP}}\) architectures, in the 2D-INR classification and style editing tasks. The results, reported in Tables 17 and 18, show that IO-encoding adds a very small (sometimes negligible) boost to \(\text{NFN}_{\text{NP}}\) performance, though it never hurts. Since \(\text{NFN}_{\text{NP}}\) often performs as well as or better than \(\text{NFN}_{\text{INP}}\), this indicates that even the base NP variant can solve many weight-space tasks without needing to break that symmetry.

### Interpreting learned lottery ticket masks

We further analyze the behavior of \(\text{NFN}_{\text{PT}}\) on lottery ticket mask prediction by plotting the mask score predicted for a given initialization value at each layer. To make the visualization clear we train

\begin{table}
\begin{tabular}{l r} \hline \hline
**Name** & **Values** \\ \hline Optimizer & Adam \\ Learning rate & \(1\times 10^{-3}\) \\ Batch size & 32 \\ Training steps & \(5\times 10^{4}\) \\ \hline \hline \end{tabular}
\end{table}
Table 15: Hyperparameters for weight-space style editing using neural functionals.

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline \hline  & & MNIST & FashionMNIST & CIFAR-10 & ShapeNet-10 & ScanNet-10 \\ \hline \multirow{2}{*}{NFN\({}_{\text{HNP}}\)} & Train & \(94.9\pm 0.579\) & \(82.3\pm 2.78\) & \(75.5\pm 0.810\) & \(100\pm 0.0\) & \(100.0\pm 0.0\) \\  & Test & \(92.5\pm 0.071\) & \(72.7\pm 1.53\) & \(44.1\pm 0.471\) & \(86.9\pm 0.860\) & \(64.1\pm 0.572\) \\ \hline \multirow{2}{*}{NFN\({}_{\text{NP}}\)} & Train & \(95.0\pm 0.115\) & \(81.8\pm 0.868\) & \(66.0\pm 0.694\) & \(100.0\pm 0.0\) & \(100.0\pm 0.0\) \\  & Test & \(\mathbf{92.9\pm 0.218}\) & \(\mathbf{75.6\pm 1.07}\) & \(\mathbf{46.6\pm 0.072}\) & \(\mathbf{88.7\pm 0.461}\) & \(\mathbf{65.9\pm 1.10}\) \\ \hline \multirow{2}{*}{MLP} & Train & \(42.4\pm 2.44\) & \(44.5\pm 2.17\) & \(23.7\pm 2.39\) & \(100.0\pm 0.0\) & \(100.0\pm 0.0\) \\  & Test & \(14.5\pm 0.035\) & \(12.5\pm 0.111\) & \(16.9\pm 0.250\) & \(25.4\pm 0.121\) & \(32.9\pm 0.351\) \\ \hline \multirow{2}{*}{MLP\({}_{\text{Aug}}\)} & Train & \(20.5\pm 0.401\) & \(14.9\pm 1.45\) & \(19.1\pm 1.75\) & \(34.0\pm 0.0\) & \(42.7\pm 0.012\) \\  & Test & \(21.0\pm 0.172\) & \(15.9\pm 0.181\) & \(18.9\pm 0.432\) & \(33.8\pm 0.126\) & \(45.5\pm 0.126\) \\ \hline \multirow{2}{*}{MLP-4000} & Train & \(72.6\pm 1.39\) & \(-\) & \(30.4\pm 0.521\) & \(-\) & \(-\) \\  & Test & \(15.5\pm 0.090\) & \(-\) & \(17.1\pm 0.120\) & \(-\) & \(-\) \\ \hline \multirow{2}{*}{MLP-4000\({}_{\text{Aug}}\)} & Train & \(19.4\pm 1.39\) & \(-\) & \(20.5\pm 0.333\) & \(-\) & \(-\) \\  & Test & \(21.1\pm 0.010\) & \(-\) & \(19.3\pm 0.325\) & \(-\) & \(-\) \\ \hline \multirow{2}{*}{MLP-8000} & Train & \(77.8\pm 1.74\) & \(-\) & \(35.9\pm 0.868\) & \(-\) & \(-\) \\  & Test & \(15.8\pm 0.014\) & \(-\) & \(17.3\pm 0.280\) & \(-\) & \(-\) \\ \hline \multirow{2}{*}{MLP-8000\({}_{\text{Aug}}\)} & Train & \(20.3\pm 3.30\) & \(-\) & \(18.1\pm 0.347\) & \(-\) & \(-\) \\  & Test & \(21.3\pm 0.075\) & \(-\) & \(19.6\pm 0.060\) & \(-\) & \(-\) \\ \hline \multirow{2}{*}{inr2vec[12]} & Train & \(-\) & \(-\) & \(-\) & \(99.0\pm 0.0\) & \(93.8\pm 0.090\) \\  & Test & \(-\) & \(-\) & \(-\) & \(39.1\pm 0.385\) & \(38.2\pm 0.409\) \\ \hline \hline ResNet-18 & Test & \(99.21\pm 0.28\) & \(95.48\pm 0.13\) & \(93.95\pm 0.08\) & \(-\) & \(-\) \\ \hline PointNet & Test & \(-\) & \(-\) & \(-\) & \(94.3\) & \(72.7\) \\ \hline \hline \end{tabular}
\end{table}
Table 16: Classification train and test accuracies (%) on datasets of 2D INRS (MNIST, FashionMNIST, and CIFAR-10) and 3D INRs (ShapeNet-10 and ScanNet-10). Our equivariant NFNs outperform the MLP baselines, even when the MLPs have permutation augmentations to encourage invariance, and even when the MLPs are significantly larger to match NFN’s parameter counts (MLP-4000/8000). NFNs also outperform recent non-equivariant methods developed for 3D INRs such as irr2vec [12]. ResNet-18 and PointNet operate on gridded or pointcloud signals, instead of weight space representations, and their performance is given for reference. PointNet results were obtained directly from Qi et al. [52]. Where given, uncertainties indicate standard error over three runs.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & \(\text{NFN}_{\text{NP}}\) & \(\text{NFN}_{\text{NP}}\) (no positional encoding) \\ \hline MNIST & \(92.9\pm 0.218\) & \(92.9\pm 0.077\) \\ \hline CIFAR-10 & \(46.6\pm 0.072\) & \(46.5\pm 0.160\) \\ \hline FashionMNIST & \(75.6\pm 1.07\) & \(73.4\pm 0.701\) \\ \hline \hline \end{tabular}
\end{table}
Table 17: Classification test accuracies (%) when ablating the positional encoding for implicit neural representations (INRs) of MNIST, FashionMNIST, and CIFAR-10. Uncertainties indicate standard error over three runs.

\(\text{NFN}_{\text{PT}}\) on MLP mask prediction without layer norm, which can be viewed as a scalar function of the initialization \(f^{(i)}:\mathbb{R}\rightarrow\mathbb{R}\) for each layer \(i\). Figure 5 plots, for a fixed latent value, the predicted mask score as a function of the initialization value (low mask scores are pruned, while high mask scores are not). These plots suggest that, in the MLP setting, neural functionals are learning something similar to magnitude pruning of the initialization. In our setting, this turns out to be a strong baseline for lottery ticket mask prediction: the test accuracy of models pruned with the modified network is 95.0%.

Figure 5: Mask scores vs weight magnitude for modified \(\text{NFN}_{\text{PT}}\).

### Predicting MLP generalization from weights

In addition to predicting generalization on the Small CNN Zoo benchmark (Section 3.1), we also construct our own datasets to evaluate predicting generalization on MLPs. Specifically, we study three- and five-layer MLPs with 128 units in each hidden layer. For each of the two architectures, we train 2,000 MLPs on MNIST with varying optimization hyperparameters, and save 10 randomly-selected checkpoints from each run to construct a dataset of 20,000 (weight, test accuracy) pairs. Runs are partitioned according to a \(90\%\) / \(10\%\) split for training and testing.

We evaluate \(\text{NFN}_{\text{HNP}}\) and \(\text{NFN}_{\text{NP}}\) on this task and compare them to the StatNN baseline [64] which predicts test accuracy from hand-crafted features extracted from the weights. Table 19 shows that \(\text{NFN}_{\text{NP}}\) and StatNN are broadly comparable, while \(\text{NFN}_{\text{HNP}}\) consistently outperform other methods across both datasets in two measures of correlation: Kendall's tau and \(R^{2}\). These results confirm that processing the raw weights with permutation equivariant neural functionals can lead to greater predictive power when assessing generalization from weights.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & & \(\text{NFN}_{\text{HNP}}\) & \(\text{NFN}_{\text{NP}}\) & StatNN \\ \hline \multirow{2}{*}{\(\tau\)} & 3-Layer & \(\mathbf{0.876\pm 0.003}\) & \(0.859\pm 0.002\) & \(0.854\pm 0.002\) \\  & 5-Layer & \(\mathbf{0.871\pm 0.001}\) & \(0.855\pm 0.001\) & \(0.860\pm 0.001\) \\ \hline \multirow{2}{*}{\(R^{2}\)} & 3-Layer & \(\mathbf{0.957\pm 0.003}\) & \(0.9424\pm 0.003\) & \(0.937\pm 0.002\) \\  & 5-Layer & \(\mathbf{0.956\pm 0.002}\) & \(0.947\pm 0.001\) & \(0.950\pm 0.001\) \\ \hline \hline \end{tabular}
\end{table}
Table 19: Kendall’s \(\tau\) coefficient and \(R^{2}\) between predicted and actual test accuracies of three- and five-layer MLPs trained on MNIST. Our equivariant neural functionals outperform the baseline from [64] which predicts generalization using only simple weight statistics as features. Uncertainties indicate standard error over five runs.