# Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction

Xingyu Xu

Carnegie Mellon University

&Yuejie Chi

Carnegie Mellon University

xingyuxu@andrew.cmu.edu

###### Abstract

In a great number of applications, the goal is to infer an unknown image from a small number of noisy measurements collected from a known and possibly nonlinear forward model describing certain sensing or imaging modality, which is often ill-posed. Score-based diffusion models, thanks to their impressive empirical success, have emerged as an appealing candidate of an expressive prior in image reconstruction. In order to accommodate diverse tasks at once, it is of great interest to develop efficient, consistent and robust algorithms that incorporate _unconditional_ score functions of an image prior distribution in conjunction with flexible choices of forward models. This work develops an algorithmic framework for employing score-based diffusion models as an expressive data prior in nonlinear inverse problems with general forward models. Motivated by the plug-and-play framework in the imaging community, we introduce a diffusion plug-and-play method (DPnP) that alternatively calls two samplers, a proximal consistency sampler based solely on the likelihood function of the forward model, and a denoising diffusion sampler based solely on the score functions of the image prior. The key insight is that denoising under white Gaussian noise can be solved _rigorously_ via both stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers using the same set of score functions trained for generation. We establish both asymptotic and non-asymptotic performance guarantees of DPnP, and provide numerical experiments to illustrate its promise in various tasks. To the best of our knowledge, DPnP is the first provably-robust posterior sampling method for nonlinear inverse problems using unconditional diffusion priors.

## 1 Introduction

In a great number of sensing and imaging applications, the paramount goal is to infer an unknown image \(x^{\star}\in\mathbb{R}^{d}\) from a collection of measurements \(y\in\mathbb{R}^{m}\) that are possibly noisy, incomplete, and even nonlinear. Examples include restoration tasks such as inpainting, super-resolution, denoising, as well as imaging tasks such as magnetic resonance imaging [1], optical imaging [2], microscopy imaging [16], radar and sonar imaging [11], and many more.

Due to sensing and resource constraints, the problem of image reconstruction is often ill-posed, where the desired resolution of the unknown image overwhelms the set of available observations. Consequently, this necessitates the need of incorporating prior information regarding the unknown image to assist the reconstruction process. Over the years, numerous types of prior information have been considered and adopted, from hand-crafted priors such as subspace or sparsity constraints [15, 14], to data-driven ones prescribed in the form of neural networks [17, 1]. These priors can be regarded as some sort of generative models for the unknown image, which postulate the high-dimensional image admits certain parsimonious representation in a low-dimensional data manifold. It is desirable that the generative models are sufficiently expressive to capture thediversity and structure of the image class of interest, yet nonetheless, still lead to image reconstruction problems that are computationally tractable.

**Score-based diffusion models as an image prior.** Recent years have seen tremendous progress on generative artificial intelligence (AI), where it is possible to generate new data samples -- such as images, audio, text -- at unprecedented resolution and scale from a target distribution given training data. Diffusion models, originally proposed by [25], are among one of the most successful frameworks, underneath popular content generators such as DALL-E [12], Stable Diffusion [13], Imagen [26], and many others. Roughly speaking, score-based diffusion models convert noise into samples that resemble those from a target data distribution, by forming the reverse Markov diffusion process only using the score functions of the data contaminated at various noise levels [14, 1, 15]. In particular, [14] developed a unified framework to interpret score-based diffusion models as reversing certain Stochastic Differential Equations (SDE) using either SDE or probability flow Ordinary Differential Equations (ODE), leading to stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers, respectively. While the DDIM-type sampler is more amenable to acceleration, the DDPM-type sampler tends to generate images of higher quality and diversity when running for a large number of steps [14].

Thanks to the expressive power of score-based diffusion models in generating complex and fine-grained images, they have emerged as a plausible candidate of an expressive prior in image reconstruction [26, 27] via the lens of _Bayesian posterior sampling_. To accommodate diverse applications with various image characteristics and imaging modalities, it is desirable to develop _plug-and-play_ methods that do not require training from scratch or end-to-end training for every new imaging task. Nonetheless, despite a flurry of recent efforts, existing algorithms either are computationally expensive [28, 29], inconsistent [25, 26, 27], or confined to linear inverse problems [28, 29]. Therefore, a natural question arises:

_Can we develop a practical, consistent and robust algorithm that incorporates score-based diffusion models as an image prior with general (possibly nonlinear) forward models?_

### Our contribution

This paper provides an affirmative answer to this question, by developing an algorithmic framework to sample from the posterior distribution of images, where score-based diffusion models are employed as an expressive image prior in nonlinear inverse problems with general forward models. Specifically, our contributions are as follows.

* _Diffusion plug-and-play for posterior sampling._ Motivated by the plug-and-play [28] framework in the imaging community, we introduce a diffusion plug-and-play method (DPnP) that alternatively calls two samplers, a _proximal consistency sampler_ that aims to generate samples that are more consistent with the measurements, and a _denoising diffusion sampler_ that focuses on sampling from the posterior distribution of an easier problem -- image denoising under white Gaussian noise -- to enforce the prior constraint. Our method is _modular_, in the sense that the _proximal consistency sampler_ is solely based on the likelihood function of the forward model, and the denoising diffusion sampler is based solely on the score functions of the image prior.
* _Posterior sampling for image denoising._ While the proximal consistency sampler can be borrowed somewhat straightforwardly from existing literature such as the Metropolis-adjusted Langevin algorithm [10], the denoising diffusion sampler, on the other hand, has not been addressed in the literature to the best of our knowledge. Our key insight is that this can be solved via both stochastic (i.e., DDPM-type) or deterministic (i.e., DDIM-type) samplers by carefully choosing the forward SDEs and discretizing the resulting reversal SDE or ODE using the exponential integrator [16]. Importantly, the denoising diffusion samplers use the same set of unconditional score functions for generation, making it readily implementable without additional training.

Figure 1: Solving linear and nonlinear inverse problems with Diffusion Plug-and-Play (DPnP).

* _Theoretical guarantees._ We establish both asymptotic and non-asymptotic performance guarantees of the proposed DPnP method. Asymptotically, we verify the correctness of our method by proving that DPnP converges to the conditional distribution of \(x^{\star}\) given measurements \(y\), assuming exact unconditional score estimates of the image prior. We next establish a non-asymptotic convergence theory of DPnP, where its performance degenerates gracefully with respect to the errors of the samplers, due to, e.g., score estimation errors and limited sampling steps. To the best of our knowledge, this provides the _first provably-robust_ method for _nonlinear_ inverse problems using unconditional score-based diffusion priors.

We further provide numerical experiments to illustrate its promise in solving both linear and nonlinear image reconstruction tasks, such as super-resolution, phase retrieval, and quantized sensing. Due to its plug-and-play nature, we expect it to be of broad interest to a wide variety of inverse problems.

**Related works.** Given its interdisciplinary nature, our work sits at the intersection of generative modeling, computational imaging, optimization and sampling. Due to space limits, we postpone the discussion of related works to Appendix A.

**Notation.** Let \(p_{x}\) denote the probability distribution of \(x\), and \(p_{x}(\cdot|y)\) denotes the conditional distribution of \(x\) given \(y\). We use \(X\stackrel{{\mathrm{(d)}}}{{=}}Y\) to denote random variables \(X\) and \(Y\) are equivalent in distribution. The matrix \(I_{d}\) denotes an identity matrix of dimension \(d\). For two probability distributions with density \(p(x)\) and \(q(x)\), the total variation distance between them is \(\mathsf{TV}(p,q)\coloneqq\int|p(x)-q(x)|\mathrm{d}x\). The \(\chi^{2}\)-divergence of \(p\) to \(q\) is \(\chi^{2}(p\,\|\,q)\coloneqq\int\frac{(p(x)-q(x))^{2}}{q(x)}\mathrm{d}x\).

## 2 Score-based generative models

In this section, we set up the preliminary on diffusion-based generative models, which we will be relying upon to develop our algorithm. The key components consist of a _forward_ process, which diffuses the data distribution \(p^{\star}\) to the standard normal distribution by gradually injecting noise into the samples, and a _backward_ process, which reverses the forward process so that it can transform the standard normal distribution to the data distribution \(p^{\star}\). To facilitate understanding, it will be convenient to formulate these processes in continuous time. For discrete-time formulation and implementation, please refer to Appendix E.

### The forward process and score functions

The continuous-time forward diffusion follows the Ornstein-Uhlenbeck (OU) process, defined by the Stochastic Differential Equation (SDE) [21]:

\[\mathrm{d}X_{\tau}=-X_{\tau}\mathrm{d}\tau+\sqrt{2}\,\mathrm{d}B_{\tau},\quad \tau\geq 0,\quad X_{0}\sim p^{\star},\] (1)

where \((B_{\tau})_{\tau\geq 0}\) is the standard \(d\)-dimensional Brownian motion. It can be shown that [19, 10] the marginal distribution of \(X_{\tau}\) for \(\tau\geq 0\) is

\[X_{\tau}\stackrel{{\mathrm{(d)}}}{{=}}\mathrm{e}^{-\tau}X_{0}+ \sqrt{1-\mathrm{e}^{-2\tau}}\varepsilon,\quad X_{0}\sim p^{\star},\; \varepsilon\sim\mathcal{N}(0,I_{d}).\] (2)

It is then clear that the limiting distribution \(X_{\infty}\sim\mathcal{N}(0,I_{d})\) as \(\tau\to\infty\), i.e., the OU process diffuses \(X_{0}\sim p^{\star}\) to the standard normal distribution. The score function of \(X_{\tau}\) is defined by

\[s(\tau,x)=\nabla\log p_{X_{\tau}}(x).\] (3)

An enlightening property [17] of the score function is that it can be interpreted as the minimum mean-squared error (MMSE) estimate of \(\varepsilon_{t}\) given \(x_{t}=x\), fueled by Tweedie's formula:

\[s(\tau,x)=-\frac{1}{\sqrt{1-\mathrm{e}^{-2\tau}}}\underbrace{\mathbb{E}_{X_{0 }\sim p^{\star},\,\varepsilon\sim\mathcal{N}(0,I_{d})}\big{(}\varepsilon\,| \,\mathrm{e}^{-\tau}X_{0}+\sqrt{1-\mathrm{e}^{-2\tau}}\varepsilon=x\big{)}}_{= \varepsilon(\tau,x)}\] (4)

Consequently, this makes it possible to estimate the score functions via learning to denoise [14], by estimating the denoising function \(\varepsilon(\tau,\cdot)\), as typically done in practice [1].

### The reverse process and sampling

To enable sampling, one needs to "reverse" the forward diffusion process. Fortunately, it is possible to leverage classical theory [1, 1] to reverse the SDE, and apply discretization to the time-reversal processes to collect samples. We shall describe two popular approaches below, corresponding to stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers respectively following primarily the framework set forth in [21].

**Time-reversed SDEs and probability flow ODEs.** Let us begin with the more general theory of _reversing_ SDEs, which will be useful in future sections. Consider a SDE given by

\[\mathrm{d}M_{\tau}=\alpha M_{\tau}\mathrm{d}\tau+\sqrt{\beta}\mathrm{d}B_{\tau},\quad\tau\geq 0,\quad M_{0}\sim p_{M_{0}},\] (5)

where \(\alpha\in\mathbb{R}\) and \(\beta>0\) are constants. For any positive time \(\tau_{\infty}>0\), define the reversed time parameter

\[\tau^{\mathsf{rev}}\coloneqq\tau^{\mathsf{rev}}(\tau)=\tau_{\infty}-\tau.\] (6)

We are now ready to describe the time-reversed processes.

1. The _time-reversed SDE_ of (5) on the time interval \([0,\tau_{\infty}]\) is defined as \[\mathrm{d}M^{\mathsf{rev}}_{\tau^{\mathsf{rev}}}=(-\alpha M^{\mathsf{rev}}_{ \tau^{\mathsf{rev}}}+\beta\nabla\log p_{M_{\tau^{\mathsf{rev}}}}(M^{\mathsf{ rev}}_{\tau^{\mathsf{rev}}}))\,\mathrm{d}\tau+\sqrt{\beta}\mathrm{d}\tilde{B}_{ \tau},\ \tau\in[0,\tau_{\infty}],\,M^{\mathsf{rev}}_{\tau_{\infty}}\sim p_{M_{\tau_{ \infty}}},\] (7) where \(\tilde{B}\) is an independent copy of \(B\), i.e., another Brownian motion. It is a classical result [1] that the reversed process \(M^{\mathsf{rev}}\) shares the same path distribution as \(M\), i.e., \((M^{\mathsf{rev}}_{\tau})_{\tau\in[0,\tau_{\infty}]}\stackrel{{ (\mathrm{d})}}{{=}}(M_{\tau})_{\tau\in[0,\tau_{\infty}]}\). In other words, the joint distribution of \((M^{\mathsf{rev}}_{\tau_{1}},M^{\mathsf{rev}}_{\tau_{2}},\cdots,M^{\mathsf{rev }}_{\tau_{k}})\) for any \(0\leq\tau_{1}\leq\tau_{2}\leq\cdots\leq\tau_{k}\leq\tau_{\infty}\), for any integer \(k\geq 1\), coincides with that of \((M_{\tau_{1}},M_{\tau_{2}},\cdots,M_{\tau_{k}})\).
2. In place of the reversed SDE in (7), it is possible to consider the following probability flow ODE [1, SSDK\({}^{+}\)21]: \[\mathrm{d}M^{\mathsf{rev}}_{\tau^{\mathsf{rev}}}=\left(-\alpha M^{\mathsf{ rev}}_{\tau^{\mathsf{rev}}}+\frac{\beta}{2}\nabla\log p_{M_{\tau^{\mathsf{rev}}}}( \tau^{\mathsf{rev}},M^{\mathsf{rev}}_{\tau^{\mathsf{rev}}})\right)\mathrm{d} \tau,\ \tau\in[0,\tau_{\infty}],\,M^{\mathsf{rev}}_{\tau_{\infty}}\sim p_{M_{\tau_{ \infty}}}.\] (8) The reversed ODE satisfies a slightly weaker guarantee than that of the reversed SDE, which nevertheless suffices for most practical purposes [2]: \(M^{\mathsf{rev}}_{\tau}\stackrel{{(\mathrm{d})}}{{=}}M_{\tau},\quad \tau\in[0,\tau_{\infty}]\). Note that the reversed ODE only guarantees identical marginal distribution for each \(M^{\mathsf{rev}}_{\tau}\), whereas the reversed SDE guarantees identical joint distribution.

Specializing the above to the OU process (1) with proper discretization then leads to popular samplers used for generation, as follows.

**DDPM-type stochastic samplers.** Specializing the time-reversed SDE (7) to the OU process gives

\[\mathrm{d}X^{\mathsf{rev}}_{\tau^{\mathsf{rev}}}=\big{(}X^{\mathsf{rev}}_{ \tau^{\mathsf{rev}}}+2s(\tau^{\mathsf{rev}},X^{\mathsf{rev}}_{\tau^{\mathsf{ rev}}})\big{)}\mathrm{d}\tau+\sqrt{2}\mathrm{d}\tilde{B}_{\tau},\quad\tau\in[0, \tau_{\infty}],\quad X^{\mathsf{rev}}_{\tau_{\infty}}\sim p_{X_{\tau_{\infty}}}.\]

As \(\tau_{\infty}\to\infty\), it can be seen from (2) that \(p_{X_{\tau_{\infty}}}\) converges to \(\mathcal{N}(0,I_{d})\). Thus the solution of the above SDE can be approximated by initializing \(X^{\mathsf{rev}}_{\tau_{\infty}}\sim\mathcal{N}(0,I_{d})\) instead. The DDDPM sampler [1] can be viewed as a discretization of this SDE [2].

**DDIM-type deterministic samplers.** On the other hand, the probability flow ODE (8) for the OU process reads as

\[\mathrm{d}X^{\mathsf{rev}}_{\tau^{\mathsf{rev}}}=\big{(}X^{\mathsf{rev}}_{ \tau^{\mathsf{rev}}}+s(\tau^{\mathsf{rev}},X^{\mathsf{rev}}_{\tau^{\mathsf{ rev}}})\big{)}\mathrm{d}\tau,\quad\tau\in[0,\tau_{\infty}],\quad X^{\mathsf{ rev}}_{\tau_{\infty}}\sim p_{X_{\tau_{\infty}}}.\] (9)

Again, as \(\tau_{\infty}\to\infty\), one may approximate the initialization with \(X^{\mathsf{rev}}_{\tau_{\infty}}\sim\mathcal{N}(0,I_{d})\). It is known that the popular DDIM sampler [2, 2] is a discretization of this ODE [15]. The ODE-based deterministic samplers allow more aggressive choice of discretization schedules, as well as fast ODE solvers [15], enabling significantly accelerated sampling process compared to the SDE-based stochastic samplers.

## 3 Posterior sampling via diffusion plug-and-play

We are interested in solving (possibly nonlinear) inverse problems, where the aim is to infer an unknown image \(x^{\star}\in\mathbb{R}^{d}\) from its measurements \(y\in\mathbb{R}^{m}\),

\[y=\mathcal{A}(x^{\star})+\xi,\]

where \(\mathcal{A}:\mathbb{R}^{d}\to\mathbb{R}^{m}\) is the measurement operator underneath the forward model, and \(\xi\) denotes measurement noise. We focus on the Bayesian setting where the prior information of \(x^{\star}\) is provided in the form of some prior distribution \(p^{\star}(\cdot)\), i.e.,

\[x^{\star}\sim p^{\star}(x),\] (10)The _posterior distribution_ given measurements \(y\) is defined as

\[p^{\star}(x|y)\propto p^{\star}(x)\,p(y|x^{\star}=x)=p^{\star}(x)\,\mathrm{e}^{ \mathcal{L}(x;y)}.\] (11)

Here, \(\mathcal{L}(\cdot;y)\) is the log-likelihood function of the measurements. Notwithstanding, our framework allows flexible choices of the forward model and the noise distributions. In addition, while this formulation is derived from probabilistic interpretations, it also subsumes the "reward-guided" or "loss-guided" setting [13], where \(\mathcal{L}\) can be viewed as a reward function or a negative loss function, both of which characterize preference over structural properties of \(x^{\star}\).

**Assumption on the forward model.** Throughout the paper, for simplicity, we make the following mild assumption on \(\mathcal{L}\), which is applicable to many applications of interest.

**Assumption 1**.: _We assume \(\mathcal{L}(\cdot\,;\,y)\) is differentiable almost everywhere, and \(\sup_{x\in\mathbb{R}^{d}}\mathcal{L}(x;y)<\infty\)._

**Goal.** Our goal is to sample \(\widehat{x}\) from the posterior distribution \(\widehat{x}\sim p^{\star}(\cdot\,|\,y)\) given estimates \(\widehat{s}(\tau,x)\) (resp. \(\widehat{\varepsilon}(\tau,x)\)) of the _unconditional_ score functions \(s(\tau,x)\) (resp. the noise function \(\varepsilon(\tau,x)\)) in (3), assuming knowledge of the likelihood function \(\mathcal{L}(\cdot;y)\).

### Key ingredient: score-based denoising posterior sampling

We begin with an inspection on one of the most fundamental inverse problems: denoising under white Gaussian noise. As shall be elucidated shortly, the denoising diffusion samplers turn out to be an important building block in our algorithm for general inverse problems.

**Image denoising under white Gaussian noise.** Suppose that we have access to a noisy version of \(x^{\star}\sim p^{\star}\) contaminated by white Gaussian noise, given by

\[x_{\mathsf{noisy}}=x^{\star}+\xi,\quad\xi\sim\mathcal{N}(0,\eta^{2}I_{d}),\] (12)

where \(\eta>0\) is the noise intensity _assumed to be known_. Our goal is to sample from \(p^{\star}(\cdot\,|\,x_{\mathsf{noisy}})\) given the score estimates \(\widehat{s}_{t}(x)\) (resp. the noise estimates \(\widehat{\varepsilon}_{t}(x)\)). We will develop our score-based denoising posterior sampler, termed \(\mathsf{DDS}\), with two variants, \(\mathsf{DDS}\)-\(\mathsf{DDPM}\) and \(\mathsf{DDS}\)-\(\mathsf{DDIM}\), which can be viewed as analogues of the well-known \(\mathsf{DDPM}\) and \(\mathsf{DDIM}\) samplers in unconditional score-based sampling respectively. Before proceeding, it is worth highlighting that the two variants will be derived from different forward diffusion processes, since we observe the resulting variants empirically lead to more competitive performance.

**A stochastic \(\mathsf{DDPM}\)-type sampler via heat flow.** We begin with a stochastic \(\mathsf{DDPM}\)-type sampler for denoising, termed \(\mathsf{DDS}\)-\(\mathsf{DDPM}\). We divide our development into the following steps.

1. _Step 1: introducing the heat flow._ Let us introduce a _heat flow_ with initial distribution \(p^{\star}\), defined by the following SDE: \[\mathrm{d}Y_{\tau}=\mathrm{d}B_{\tau},\quad\tau\geq 0,\quad Y_{0}\sim p^{ \star},\] (13) where \((B_{\tau})_{\tau\geq 0}\) is the standard \(d\)-dimensional Brownian motion. The solution of (13) is simply \[Y_{\tau}=Y_{0}+B_{\tau},\quad\tau\geq 0.\] (14) Since \(B_{\tau}\sim\mathcal{N}(0,\tau I_{d})\), it readily follows that \(B_{\eta^{2}}\stackrel{{\mathrm{(d)}}}{{=}}\xi\), which together with \(Y_{0}\sim p^{\star}\) yield the important observation that \(x_{\mathsf{noisy}}=x^{\star}+\xi\) can be viewed as an endpoint of the heat flow, in the sense that \(x_{\mathsf{noisy}}=x^{\star}+\xi\stackrel{{\mathrm{(d)}}}{{=}}Y_ {\eta^{2}}\).
2. _Step 2: reversing the heat flow._ Following similar reasonings in Section 2, the next step boils down to reverse the heat flow (13). The time-reversal of the heat flow SDE (13) is (cf. (7)) given by \[\mathrm{d}Y_{\eta^{2}-\tau}^{\mathsf{rev}}=\nabla\log_{Y_{\eta^{2}-\tau}}(Y_{ \eta^{2}-\tau}^{\mathsf{rev}})\mathrm{d}\tau+\mathrm{d}\tilde{B}_{\tau},\quad \tau\in[0,\eta^{2}],\quad Y_{\eta^{2}}^{\mathsf{rev}}\sim p_{Y_{\eta^{2}}},\] (15) where \((\tilde{B}_{\tau})_{\tau\geq 0}\) is an independent copy of \((B_{\tau})_{\tau\geq 0}\). As introduced earlier, the virtue of the time-reversed SDE (15) is that it produces a process \(Y_{\tau}^{\mathsf{rev}}\) with the same _path_ distribution as \(Y_{\tau}\), i.e., \((Y_{\tau}^{\mathsf{rev}})_{\tau\in[0,\eta^{2}]}\stackrel{{\mathrm{ (d)}}}{{=}}(Y_{\tau})_{\tau\in[0,\eta^{2}]}\). In particular, the joint distribution of \((Y_{0}^{\mathsf{rev}},Y_{\eta^{2}}^{\mathsf{rev}})\) is the same as that of \((Y_{0},Y_{\eta^{2}})\stackrel{{\mathrm{(d)}}}{{=}}(x^{\star},x_{ \mathsf{noisy}})\). This implies that the conditional distribution \(p^{\star}(\cdot\,|\,x_{\mathsf{noisy}})\) is the same as \(p_{Y_{0}^{\mathsf{rev}}}(\cdot\,|\,Y_{\eta^{2}}^{\mathsf{rev}}=x_{\mathsf{noisy}})\). Surprisingly, the latter admits a simple interpretation: \(p_{Y_{0}^{\mathsf{rev}}}(\cdot\,|\,Y_{\eta^{2}}^{\mathsf{rev}}=x_{\mathsf{noisy}})\) is the distribution of \(Y_{0}^{\mathsf{rev}}\) when we initialize (15) with \(Y_{\eta^{2}}^{\mathsf{rev}}=x_{\mathsf{noisy}}\)! Therefore, sampling the posterior \(p^{\star}(\cdot\,|\,x_{\mathsf{noisy}})\) amounts to solving the following simple SDE: \[\mathrm{d}Y_{\eta^{2}-\tau}^{\mathsf{rev}}=\nabla\log p_{Y_{\eta^{2}-\tau}}(Y_ {\eta^{2}-\tau}^{\mathsf{rev}})\mathrm{d}\tau+\mathrm{d}\tilde{B}_{\tau}, \quad\tau\in[0,\eta^{2}],\quad Y_{\eta^{2}}^{\mathsf{rev}}=x_{\mathsf{noisy}}.\] (16)3. _Step 3: connecting the score functions._ It is now immediate to arrive at our proposed stochastic sampler DDS-DDPM by discretization of this SDE (16), which requires knowledge of the score functions \(\nabla\log p_{Y_{\tau}}(\cdot)\). A key observation is that they can in fact be computed from the score function \(s(\tau,x)\) (cf. (3)), due to the following lemma, whose proof is provided in Appendix D.1. [Score function of \(Y_{\tau}\)] For \(\tau\geq 0\), we have \[\nabla\log p_{Y_{\tau}}(x)=\frac{1}{\sqrt{1+\tau}}s\left(\frac{1}{2}\log(1+ \tau),\,\frac{x}{\sqrt{1+\tau}}\right).\] The resulting sampler, DDS-DDPM, is summarized in Algorithm 2 (deferred in the appendix) using a discretization procedure with an exponential integrator [22].

A deterministic DDIM-type sampler via OU processWe next develop a deterministic DDIM-type sampler for denoising, termed DDS-DDIM.

1. _Step 1: introducing a posterior-initialized OU processTo sample from the posterior distribution \(p^{\star}(\cdot|x_{\mathsf{noisy}})\), we first introduce a random variable \(w\) which has (unconditional) distribution \[p_{w}(x)\coloneqq p^{\star}(x^{\star}=x\,|\,x^{\star}+\xi=x_{\mathsf{noisy}}),\] (17) in the same form of the desired posterior distribution \(p^{\star}(\cdot|x_{\mathsf{noisy}})\). Here, since the noisy observation \(x_{\mathsf{noisy}}\) is given, we regard it as fixed.We then further introduce \(z=w-x_{\mathsf{noisy}}\), which is a "centered" version of \(w\), whose distribution is \[p_{z}(x)\coloneqq p_{w}(x+x_{\mathsf{noisy}})=p^{\star}(x^{\star}=x+x_{ \mathsf{noisy}}\,|\,x^{\star}+\xi=x_{\mathsf{noisy}}).\] The OU process with initial distribution \(p_{z}\) is defined by the SDE: \[\mathrm{d}Z_{\tau}=-Z_{\tau}\mathrm{d}\tau+\mathrm{d}B_{\tau},\quad\tau\geq 0,\quad Z_{0}\sim p_{z},\] (18) where \(B_{\tau}\) is the standard \(d\)-dimensional Brownian motion. As in (2), the marginal distribution of \(Z_{\tau}\) is given by \[Z_{\tau}\stackrel{{\mathrm{(d)}}}{{=}}\mathrm{e}^{-\tau}Z_{0}+ \sqrt{1-\mathrm{e}^{-2\tau}}\varepsilon,\quad Z_{0}\sim p_{z},\;\varepsilon \sim\mathcal{N}(0,I_{d}),\quad\tau\geq 0.\] (19)
2. _Step 2: reversing the OU processFollowing similar reasonings in Section 2, reversing the OU process (18) will enable us to generate samples \(z\sim p_{z}\). Then we can set \(w=z+x_{\mathsf{noisy}}\), which, by definition, has distribution \(p_{w}\) defined in (17), and is a sample from the desired posterior distribution \(p^{\star}(\cdot|x_{\mathsf{noisy}})\). We are thus led to solve the time-reversed probability flow ODE (cf. (8)) of (18), given by_ \[\mathrm{d}Z_{\tau^{\mathsf{rev}}}^{\mathsf{rev}}=\big{(}Z_{\tau^{\mathsf{rev} }}^{\mathsf{rev}}+\nabla\log p_{Z_{\tau^{\mathsf{rev}}}}(\tau^{\mathsf{rev}},Z_{\tau^{\mathsf{rev}}}^{\mathsf{rev}})\big{)}\mathrm{d}\tau,\quad\tau\in[0, \tau_{\infty}],\quad Z_{\tau_{\infty}}^{\mathsf{rev}}\sim\mathcal{N}(0,I_{d}), \quad\tau^{\mathsf{rev}}=\tau_{\infty}-\tau.\] (20)
3. _Step 3: connecting the score functions._ We are now one step away from our proposed deterministic sampler DDS-DDIM, which is derived by discretization of the ODE (20). We need to know the score functions \(\nabla\log p_{Z_{\tau}}(\cdot)\), which again can be computed from the score function \(s(\tau,x)\) (cf. (3)), as documented by the following lemma, whose proof is provided in Appendix D.2. [Score function of \(Z_{\tau}\)] For \(\tau\geq 0\), we have \[\nabla\log p_{Z_{\tau}}(x)=-\frac{\mathrm{e}^{2\tau}x}{\eta^{2}+\mathrm{e}^{2 \tau}-1}+\frac{\mathrm{e}^{\tau-\tilde{\tau}}\eta^{2}}{\eta^{2}+\mathrm{e}^{2 \tau}-1}s\left(\tilde{\tau},\,\mathrm{e}^{-\tilde{\tau}}x_{\mathsf{noisy}}+ \frac{\mathrm{e}^{\tau-\tilde{\tau}}\eta^{2}x}{\eta^{2}+\mathrm{e}^{2\tau}- 1}\right),\] (21) where \[\tilde{\tau}\coloneqq\tilde{\tau}(\tau)=\frac{1}{2}\log\left(\frac{\eta^{2}( \mathrm{e}^{2\tau}-1)}{\eta^{2}+\mathrm{e}^{2\tau}-1}+1\right).\] (22) After plugging this into (20) and solving the ODE for \(Z_{\tau}^{\mathsf{rev}}\), we see that \(Z_{0}^{\mathsf{rev}}+x_{\mathsf{noisy}}\) is the desired sample from the posterior distribution \(p^{\star}(\cdot|x_{\mathsf{noisy}})\), as argued before. Numerically, the ODE (20) is solved by discretization with an exponential integrator [22], resulting in the sampler DDS-DDIM as summarized in Algorithm 3 (deferred in the appendix).

``` Input: Measurements \(y\in\mathbb{R}^{m}\), log-likelihood function \(\mathcal{L}(\cdot;y)\) of the forward model, score estimates \(\widehat{s}\), annealing schedule \((\eta_{k})_{0\leq k\leq K}\). Initialization: Sample \(\widehat{x}_{0}\sim\mathcal{N}(0,\frac{\eta_{0}}{4}I_{d})\) Alternating sampling: for\(k=0,1,2,\ldots,K-1\)do 1. Proximal consistency sampler: Sample \(\widehat{x}_{k+\frac{1}{2}}\propto\exp\left(\mathcal{L}(\cdot\,;\,y)-\frac{1}{ 2\eta_{k}^{2}}\|\cdot-\widehat{x}_{k}\|^{2}\right)\) using subroutine \(\mathsf{PCS}(\widehat{x}_{k},y,\mathcal{L},\eta_{k})\) (Alg. 4). 2. Denoising diffusion sampler: Sample \(\widehat{x}_{k+1}\sim\exp\left(\log p^{*}(x)-\frac{1}{2\eta_{k}^{2}}\|x- \widehat{x}_{k+\frac{1}{2}}\|^{2}\right)\) using subroutine \(\mathsf{DDS-DDPM}(\widehat{x}_{k+\frac{1}{2}},\widehat{s},\eta_{k})\) (Alg. 2) or \(\mathsf{DDS-DDIM}(\widehat{x}_{k+\frac{1}{2}},\widehat{s},\eta_{k})\) (Alg. 3). Output:\(\widehat{x}_{K}\). ```

**Algorithm 1** Diffusion Plug-and-Play (DPnP)

### Our algorithm: diffusion plug-and-play

Now we turn to the general setting where the measurement operator \(\mathcal{A}\) is arbitrary. From the factorization of posterior distribution in (11), one intuitively understands that a posterior sampler must obey two constraints simultaneously: (i) the _data prior constraint_, corresponding to the first factor \(p^{*}(x)\), which imposes that the posterior sampler should be less likely to sample at those points where \(p^{*}(x)\) is small; (ii) the _measurement consistency constraint_, corresponding to the second factor \(\mathrm{e}^{\mathcal{L}(x;y)}\), which imposes that \(\mathcal{A}(x)\approx y\).

**Diffusion plug-and-play (DPnP).** We will apply the idea of alternatively enforcing these two constraints from a sampling perspective in the same spirit of [20, 19, 18]. Our algorithm, dubbed diffusion plug-and-play (DPnP), alternates between two samplers, the denoising diffusion sampler (\(\mathsf{DDS}\)) and the proximal consistency sampler (\(\mathsf{PCS}\)), which can be viewed as the substitutes for the proximal operator and the gradient step respectively. Given the iterate \(\widehat{x}_{k}\) and the _annealing_ parameter \(\eta_{k}\) at the \(k\)-th iteration, DPnP proceeds with the following two steps:

1. [leftmargin=*]
2. _Proximal consistency sampler to enforce the measurement consistency constraint._ DPnP draws a sample \(\widehat{x}_{k+\frac{1}{2}}\) from the distribution proportional to \(\exp\left(\mathcal{L}(x\,;\,y)-\frac{1}{2\eta_{k}^{2}}\|x-\widehat{x}_{k}\|^ {2}\right)\) to promote the image to be consistent with the measurements. This step, which we denote as the _proximal consistency sampler_, can be achieved by small modifications of standard algorithms such as Metropolis-Adjusted Langevin Algorithm (MALA) [14] given in Algorithm 4 (deferred in the appendix).
3. _Denoising diffusion sampler to enforce the data prior constraint._ DPnP next draws a sample \(\widehat{x}_{k+1}\) from the distribution proportional to \[\exp\left(-\big{(}-\log p^{*}(x)+\frac{1}{2\eta_{k}^{2}}\|x-\widehat{x}_{k+ \frac{1}{2}}\|^{2}\big{)}\right)\propto p^{*}(x^{*}=x\,|\,x^{*}+\eta_{k}w= \widehat{x}_{k+\frac{1}{2}})\] (23) to promote the image to be consistent with the prior, where \(w\sim\mathcal{N}(0,I_{d})\). The last step, which follows from the Bayes' rule, makes it clear that this step can be precisely achieved by the denoising diffusion sampler (developed in Section 3.1) using solely the unconditional score function.

Combining both steps lead to the proposed DPnP method described in Algorithm 1. Some comments about the proposed DPnP method are in order.

* The proximal consistency sampler \(\mathsf{PCS}\) can be viewed as a "soft" version of the proximal point method [13]. This can be seen from a first-order approximation: the maximum likelihood of the distribution \(\exp\left(\mathcal{L}(\cdot;y)-\frac{1}{2\eta_{k}^{2}}\|\cdot-\widehat{x}_{k} \|^{2}\right)\) is attained at the point \(x^{\prime}\in\mathbb{R}^{d}\) satisfying \[\nabla_{x^{\prime}}\mathcal{L}(x^{\prime};y)-\frac{1}{\eta_{k}^{2}}(x^{\prime }-\widehat{x}_{k})=0,\quad\Longrightarrow\quad x^{\prime}=\widehat{x}_{k}+\eta _{k}^{2}\nabla_{x^{\prime}}\mathcal{L}(x^{\prime};y)\approx\widehat{x}_{k}+ \eta_{k}^{2}\nabla_{\widehat{x}_{k}}\mathcal{L}(\widehat{x}_{k};y),\] Therefore, the proximal consistency sampler draws random samples "concentrated" around \(x^{\prime}\), which approximates the implicit proximal point update, akin to a gradient step at \(\widehat{x}_{k}\).
* On the other end, the denoising posterior sampler \(\mathsf{DDS}\) can be regarded as a "soft" version of the proximal operator. In particular, when \(p^{*}\) is supported on a low-dimensional manifold \(\mathcal{M}\), it forces \(x\) to reside in \(\mathcal{M}\), like the proximal map. To see this, note that denoising posterior distribution vanishes outside \(\mathcal{M}\) by (23).

* The proximal consistency sampler \(\mathsf{PCS}\) admits a simple form when the forward model \(\mathcal{A}\) is linear, i.e. \(\mathcal{A}(x)=Ax\) for some matrix \(A\in\mathbb{R}^{m\times d}\), and the measurement noise \(\xi\sim\mathcal{N}(0,\Sigma)\) is Gaussian. In this situation, the proximal consistency sampler \(\mathsf{PCS}\) can be implemented directly by \[\widehat{x}_{k+\frac{1}{2}}=\mathsf{PCS}(\widehat{x}_{k},y,\mathcal{L},\eta_{ k})=\widetilde{x}_{k}+\widetilde{\Sigma}_{k}^{1/2}w_{k},\quad w_{k}\sim\mathcal{N}(0,I_{d}),\] where \(\widetilde{x}_{k}=\left(A^{\top}\Sigma^{-1}A+\frac{1}{\eta_{k}^{2}}I_{d} \right)^{-1}\left(A^{\top}\Sigma^{-1}y+\frac{1}{\eta_{k}^{2}}\widehat{x}_{k}\right)\), and \(\widetilde{\Sigma}_{k}=\left(A^{\top}\Sigma^{-1}A+\frac{1}{\eta_{k}^{2}}I_{d} \right)^{-1}\).

## 4 Theoretical analysis

In this section, we establish both asymptotic and non-asymptotic performance guarantees of \(\mathsf{DPnP}\).

**Asymptotic consistency.** We begin with the asymptotic consistency of \(\mathsf{DPnP}\) in the theorem below.

**Theorem 1** (Asymptotic consistency of \(\mathsf{DPnP}\)).: _Assume the score function estimate \(\widehat{s}(\tau,\cdot)\) is accurate, i.e., \(\widehat{s}(\tau,x)=s(\tau,x)\), and assume the ODE/SDEs in \(\mathsf{DDS}\) and \(\mathsf{PCS}\) are solved exactly. Let \((\varepsilon_{l})_{l\geq 0}\) be a decreasing sequence of positive numbers satisfying \(\lim_{l\to\infty}\varepsilon_{l}=0\), and \((k_{l})_{l\geq 0}\) be an increasing sequence of integers with \(k_{0}=0\). Set the annealing schedule \(\eta_{k}=\varepsilon_{l}\), for \(k_{l-1}\leq k<k_{l},\;l=1,2,\cdots\). Let \(\min_{l^{\prime}=1,2,\cdots}|k_{l^{\prime}}-k_{l^{\prime}-1}|\to\infty\), the output \(\widetilde{x}_{k_{l}}\) of \(\mathsf{DPnP}\) converges in distribution to the posterior distribution \(p^{\star}(\cdot|y)\) for \(l\to\infty\)._

In words, Theorem 1 establishes the asymptotic consistency of \(\mathsf{DPnP}\) under fairly mild assumptions on the forward model (cf. Assumption 1): as long as the sampled distributions of \(\mathsf{DDS}\) and \(\mathsf{PCS}\) are exact, then running \(\mathsf{DPnP}\) with a slowly diminishing annealing schedule of \(\{\eta_{k}\}\) will output samples approaching the desired posterior distribution \(p^{\star}(\cdot|y)\) when the number of iterations \(l\) goes to infinity.

**Non-asymptotic error analysis.** We now step away from the idealized setting when the sampled distributions of \(\mathsf{DDS}\) and \(\mathsf{PCS}\) are exact. In practice, there are many sources of errors that can influence the sampled distributions of \(\mathsf{DDS}\) and \(\mathsf{PCS}\), e.g., the discretization error arising from numerically solving ODE/SDE, and the score estimation error. In effect, these non-idealities will make \(\mathsf{PCS}\) and \(\mathsf{DDS}\)_inexact_. That is, the distribution they generate will slightly deviate from the distribution they ought to sample from. In this paper, we model such deviations by the _total variation_ distance from the distribution generated by \(\mathsf{PCS}\) (resp. \(\mathsf{DDS}\)) to the ideal distribution proportional to \(\exp(\mathcal{L}(x;y)-\frac{1}{2\eta_{k}^{2}}\|x-\widehat{x}_{k}\|^{2})\) (resp. \(p^{\star}(x^{*}=x|x^{*}+\eta_{k}\varepsilon=\widehat{x}_{k+\frac{1}{2}})\)) uniformly over all iterations. Analyzing these errors is out of the scope of this paper, and we point the interested readers to parallel lines of works, e.g., [1, 1, 2], among many others. In our analysis, we will assume a black-box bound for the total variation errors of \(\mathsf{PCS}\) and \(\mathsf{DDS}\), which can be combined with existing analyses of the respective samplers to bound the iteration complexity of \(\mathsf{DPnP}\).

**Theorem 2** (Non-asymptotic robustness of \(\mathsf{DPnP}\)).: _With the notation in \(\mathsf{DPnP}\) (Algorithm 1), set \(\eta_{k}\equiv\eta>0\). Under Assumption 1, there exists \(\lambda\coloneqq\lambda(p^{\star},\mathcal{L},\eta)\in(0,1)\), such that the following holds. Define a stationary distribution \(\pi_{\eta}\) by \(\pi_{\eta}(x)\propto p^{\star}(x)q_{\eta}(x),\) where \(q_{\eta}\) is defined by_

\[q_{\eta}(x)\coloneqq\mathrm{e}^{\mathcal{L}(\cdot\,;\,y)}\ast p_{\eta \varepsilon}(x)=\frac{1}{(2\pi)^{d/2}\eta^{d}}\int\mathrm{e}^{\mathcal{L}(x^{ \prime};\,y)-\frac{1}{2\eta^{d}}\|x-x^{\prime}\|^{2}}\mathrm{d}x^{\prime}, \quad\varepsilon\sim\mathcal{N}(0,I_{d}),\] (24)

_where \(\ast\) denotes convolution. If \(\mathsf{PCS}\) has error at most \(\varepsilon_{\mathsf{PCS}}\) in total variation and \(\mathsf{DDS}\) has error at most \(\varepsilon_{\mathsf{DDS}}\) in total variation per iteration, then for any accuracy goal \(\varepsilon_{\mathsf{acc}}>0\), with \(K\asymp\frac{\log(1/\varepsilon_{\mathsf{acc}})}{1-\lambda}\), we have_

\[\mathsf{TV}(p_{\widehat{x}_{K}},\pi_{\eta})\lesssim\varepsilon_{\mathsf{acc}} \sqrt{\chi^{2}(p_{\widehat{x}_{1}}\,\|\,\pi_{\eta})}+\frac{1}{1-\lambda}( \varepsilon_{\mathsf{DDS}}+\varepsilon_{\mathsf{PCS}})\log\left(\frac{1}{ \varepsilon_{\mathsf{acc}}}\right).\] (25)

Before interpreting Theorem 2, we observe that \(q_{0}(x)=\mathrm{e}^{\mathcal{L}(x;y)}\), thus \(\pi_{0}(x)\propto p^{\star}(x)\mathrm{e}^{\mathcal{L}(x;y)}\) coincides with the desired posterior distribution \(p^{\star}(\cdot|y)\). Thus Theorem 2 tells us that, assuming a constant annealing schedule \(\eta_{k}=\eta\), the output of \(\mathsf{DPnP}\) converges in total variation to the distribution \(\pi_{\eta}\), which is a distorted version of the desired posterior distribution up to level \(\eta\), with sufficiently many iterations. A few remarks are in order.

**Non-diminishing \(\eta\).** It can be seen from Theorem 2 that even with a nonzero \(\eta\), \(\mathsf{DPnP}\) already enforces the data prior strictly. On the other hand, the measurement consistency is distorted by an order of \(\eta\). This is usually tolerable, since the measurements are themselves contaminated by noise, thus when \(\eta\) is smaller than the noise level, the distortion would be tolerable. In practice, it is beneficial to choose an annealing schedule of \(\{\eta_{k}\}\), which will be elaborated in Section 5.

**Provable robustness.** Theorem 2 indicates the performance of DPnP degenerates gracefully in the presence of sampling errors. To the best of our knowledge, this is the first provably consistent and robust posterior sampling method for nonlinear inverse problems using score-based diffusion priors.

## 5 Numerical experiments

We provide preliminary numerical evidence to corroborate the promise of DPnP in solving both linear and nonlinear image reconstruction tasks. We denote DPnP with the subroutines DDS-DDPM and DDS-DDIM as DPnP-DDPM and DPnP-DDIM respectively.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{Super-resolution} & \multicolumn{2}{c}{Phase retrieval} & \multicolumn{2}{c}{Quantized sensing} & \multicolumn{2}{c}{Time} \\  & \multicolumn{2}{c}{(4x, linear)} & \multicolumn{2}{c}{(nonlinear)} & \multicolumn{2}{c}{(nonlinear)} & \multicolumn{2}{c}{per sample} \\ Algorithm & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) \\ \hline DPnP-DDIM (ours) & \(\mathbf{0.301}\) & \(\mathbf{24.2}\) & \(\mathbf{0.376}\) & \(\mathbf{22.4}\) & \(\mathbf{0.293}\) & \(\mathbf{24.2}\) & \(\sim 90\)s \\ DPS [CKM\({}^{+}\)23] & \(0.331\) & \(23.1\) & \(0.490\) & \(17.4\) & \(0.367\) & \(21.7\) & \(\sim 60\)s \\ LGD-MC (\(n=5\)) [SZY\({}^{+}\)23] & \(0.318\) & \(23.9\) & \(0.522\) & \(16.4\) & \(0.317\) & \(23.9\) & \(\sim 60\)s \\ ReSample (pixel-based) [SKZ\({}^{+}\)23] & \(0.313\) & \(23.9\) & - & - & \(0.318\) & \(22.6\) & \(\sim 70\)s \\ \hline \hline \end{tabular}
\end{table}
Table 3: Evaluation of solving inverse problems on FFHQ \(256\times 256\) validation dataset (1k samples). Despite considerable efforts to optimize parameters, pixel-based ReSample did not generate meaningful results for phase retrieval.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Input & DPS & LGD-MC & ReSample & DPnP-DDPM & DPnP-DDIM & Ground truth \\ \hline \hline \end{tabular}
\end{table}
Table 2: Samples of different algorithms for quantized sensing.

**Experimental setups.** We compare DPnP with the state-of-the-art algorithms including DPS [CKM\({}^{+}\)23], LGD-MC [SZY\({}^{+}\)23], and pixel-based ReSample [SKZ\({}^{+}\)23] for super-resolution (linear), phase retrieval (nonlinear), and quantized sensing (nonlinear). Definitions of these forward measurement models are in Appendix G.2. The annealing schedule \(\{\eta_{k}\}\) of DPnP is fixed across _all_ tasks (Appendix H.2), while DPS, LGD-MC, and ReSample are fine-tuned with reasonable effort for best performance. All experiments are run on a single Nvidia L40 GPU. More details and experiments are in Appendix G.

**Sample images.** We present the sample images of different algorithms for the most complicated task of phase retrieval. For phase retrieval, Fourier transform is performed to the image with a coded mask, and only the magnitude of the Fourier transform is taken as the measurement [SEC\({}^{+}\)15]. Due to the nonlinearity of the operation of taking magnitude, the forward model is nonlinear. The samples generated by different algorithms are shown in Table 1.

We also present the sample images for the nonlinear problem of quantized sensing. In quantized sensing, each pixel of the image is randomly dithered and then quantized to one bit per channel. Nonlinearity of quantizing renders this forward model nonlinear. The samples generated by different algorithms are shown in Table 2.

**Evaluation.** We evaluate the performance of DPnP on the FFHQ validation dataset [KLA19] and the ImageNet validation dataset [RDS\({}^{+}\)15]. Since DPnP-DDIM has similar performance with DPnP-DDPM but admits much faster implementation, only DPnP-DDIM is evaluated. The LPIPS and PSNR are shown in Table 3 and Table 4. These two metrics are arguably the more relevant ones for solving inverse problems. For comparison under other metrics such as FID, SSIM, cf. Appendix G.4.

It can be seen that, DPnP is capable of solving both linear and nonlinear problems, and, in comparison with prior state-of-the-art, performs better in recovering fine and criser details.

## 6 Discussion

This paper sets forth a rigorous and versatile algorithmic framework called DPnP for solving nonlinear inverse problems via posterior sampling, using image priors prescribed by score-based diffusion models with general forward models. DPnP alternates between two sampling steps implemented by DDS and PCS, to promote consistency with the data prior constraint and the measurement constraint respectively. We provide both asymptotic and non-asymptotic convergence guarantees, establishing DPnP as the first provably consistent and robust score-based diffusion posterior sampling method for general nonlinear inverse problems.

### Acknowledgments and Disclosure of Funding

This work is supported in part by Office of Naval Research under N00014-19-1-2404, and by National Science Foundation under DMS-2134080 and ECCS-2126634. X. Xu is also gratefully supported by the Axel Berny Presidential Graduate Fellowship at Carnegie Mellon University.

## References

* [AGS05] L. Ambrosio, N. Gigli, and G. Savare. _Gradient flows: in metric spaces and in the space of probability measures_. Springer Science & Business Media, 2005.
* [And82] B. D. Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{Super-resolution} & \multicolumn{2}{c}{Phase retrieval} & \multicolumn{2}{c}{Quantized sensing} & \multicolumn{2}{c}{Time} \\  & (4x, linear) & \multicolumn{2}{c}{(nonlinear)} & \multicolumn{2}{c}{(nonlinear)} & \multicolumn{2}{c}{per sample} \\ \cline{2-7} Algorithm & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & LPIPS \(\downarrow\) & PSNR \(\uparrow\) & \\ \hline DPnP-DDIM (ours) & \(\mathbf{0.416}\) & \(\mathbf{21.6}\) & \(\mathbf{0.562}\) & \(\mathbf{13.4}\) & \(\mathbf{0.363}\) & \(\mathbf{23.0}\) & \(\sim 240\)s \\ DPS [CKM\({}^{+}\)23] & \(0.473\) & \(20.2\) & \(0.677\) & \(\mathbf{13.4}\) & \(0.542\) & \(18.7\) & \(\sim 150\)s \\ LGD-MC (\(n=5\)) [SZY\({}^{+}\)23] & \(\mathbf{0.416}\) & \(20.9\) & \(0.592\) & \(12.8\) & \(0.384\) & \(22.3\) & \(\sim 150\)s \\ ReSample (pixel-based) [SKZ\({}^{+}\)23] & \(0.464\) & \(20.1\) & - & - & \(0.414\) & \(19.8\) & \(\sim 180\)s \\ \hline \hline \end{tabular}
\end{table}
Table 4: Evaluation of solving inverse problems on ImageNet \(256\times 256\) validation dataset (1k samples). Despite considerable efforts to optimize parameters, pixel-based ReSample did not generate meaningful results for phase retrieval.

[BB23] C. A. Bouman and G. T. Buzzard. Generative plug and play: Posterior sampling for inverse problems. _arXiv preprint arXiv:2306.07233_, 2023.
* [BCSB18] G. T. Buzzard, S. H. Chan, S. Sreehari, and C. A. Bouman. Plug-and-play unplugged: Optimization-free reconstruction using consensus equilibrium. _SIAM Journal on Imaging Sciences_, 11(3):2001-2020, 2018.
* [BDBDD24] J. Benton, V. De Bortoli, A. Doucet, and G. Deligiannidis. Nearly \(d\)-linear convergence bounds for diffusion models via stochastic localization. In _The Twelfth International Conference on Learning Representations_, 2024.
* [BJPD17] A. Bora, A. Jalal, E. Price, and A. G. Dimakis. Compressed sensing using generative models. In _International conference on machine learning_, pages 537-546. PMLR, 2017.
* [Bou23a] N. Bourbaki. _Theories spectrales Chapitres 1 et 2_. Springer, 2023.
* [Bou23b] N. Bourbaki. _Theories spectrales: Chapitres 3 a 5_. Springer Nature, 2023.
* [CCL\({}^{+}\)22] S. Chen, S. Chewi, J. Li, Y. Li, A. Salim, and A. R. Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. _arXiv preprint arXiv:2209.11215_, 2022.
* [CCL\({}^{+}\)23] S. Chen, S. Chewi, H. Lee, Y. Li, J. Lu, and A. Salim. The probability flow ODE is provably fast. _Neural Information Processing Systems_, 2023.
* [CDC23] F. Coeurdoux, N. Dobigeon, and P. Chainais. Plug-and-play split Gibbs sampler: embedding deep generative priors in Bayesian inference. _arXiv preprint arXiv:2304.11134_, 2023.
* [CDD23] S. Chen, G. Daras, and A. Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for DDIM-type samplers. In _International Conference on Machine Learning_, pages 4462-4484, 2023.
* [CICM23] G. Cardoso, Y. J. E. Idrissi, S. L. Corff, and E. Moulines. Monte carlo guided diffusion for Bayesian linear inverse problems. _arXiv preprint arXiv:2308.07983_, 2023.
* [CKM\({}^{+}\)23] H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye. Diffusion posterior sampling for general noisy inverse problems. In _International Conference on Learning Representations_, 2023.
* [CLA\({}^{+}\)21] S. Chewi, C. Lu, K. Ahn, X. Cheng, T. Le Gouic, and P. Rigollet. Optimal dimension dependence of the metropolis-adjusted langevin algorithm. In _Conference on Learning Theory_, pages 1260-1300. PMLR, 2021.
* [CLL23] H. Chen, H. Lee, and J. Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. In _International Conference on Machine Learning_, pages 4735-4763, 2023.
* [CLS15] E. J. Candes, X. Li, and M. Soltanolkotabi. Phase retrieval via Wirtinger flow: Theory and algorithms. _IEEE Transactions on Information Theory_, 61(4):1985-2007, 2015.
* [CLY23] H. Chung, S. Lee, and J. C. Ye. Fast diffusion sampler for inverse problems by geometric decomposition. _arXiv preprint arXiv:2303.05754_, 2023.
* [CR12] E. Candes and B. Recht. Exact matrix completion via convex optimization. _Communications of the ACM_, 55(6):111-119, 2012.
* [Don06] D. L. Donoho. Compressed sensing. _IEEE Transactions on Information Theory_, 52(4):1289-1306, 2006.
* [Doo42] J. L. Doob. The Brownian movement and stochastic equations. _Annals of Mathematics_, 43(2):351-369, 1942.

* [Dru17] D. Drusvyatskiy. The proximal point method revisited. _arXiv preprint arXiv:1712.06038_, 2017.
* [DS24] Z. Dou and Y. Song. Diffusion posterior sampling for linear inverse problem solving: A filtering perspective. In _International Conference on Learning Representations_, 2024.
* [Efr11] B. Efron. Tweedie's formula and selection bias. _Journal of the American Statistical Association_, 106(496):1602-1614, 2011.
* [Eva12] L. C. Evans. _An introduction to stochastic differential equations_, volume 82. American Mathematical Soc., 2012.
* [FBS24] Z. Fang, S. Buchanan, and J. Sulam. What's in a prior? Learned proximal networks for inverse problems. In _The Twelfth International Conference on Learning Representations_, 2024.
* [FSR\({}^{+}\)23] B. T. Feng, J. Smith, M. Rubinstein, H. Chang, K. L. Bouman, and W. T. Freeman. Score-based diffusion models as principled priors for inverse imaging. In _2023 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 10486-10497, 2023.
* [GJP\({}^{+}\)24] S. Gupta, A. Jalal, A. Parulekar, E. Price, and Z. Xun. Diffusion posterior sampling is computationally intractable. _arXiv preprint arXiv:2402.12727_, 2024.
* [GL10] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In _Proceedings of the 27th international conference on international conference on machine learning_, pages 399-406, 2010.
* [GMJS22] A. Graikos, N. Malkin, N. Jojic, and D. Samaras. Diffusion models as plug-and-play priors. _Advances in Neural Information Processing Systems_, 35:14715-14728, 2022.
* [HJA20] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [HSMC17] J. Huang, M. Sun, J. Ma, and Y. Chi. Super-resolution image reconstruction for high-density three-dimensional single-molecule microscopy. _IEEE Transactions on Computational Imaging_, 3(4):763-773, 2017.
* [Hyv05] A. Hyvarinen. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* [KEES22] B. Kawar, M. Elad, S. Ermon, and J. Song. Denoising diffusion restoration models. _Advances in Neural Information Processing Systems_, 35:23593-23606, 2022.
* [Key81] R. Keys. Cubic convolution interpolation for digital image processing. _IEEE transactions on acoustics, speech, and signal processing_, 29(6):1153-1160, 1981.
* [KHR23] F. Koehler, A. Heckett, and A. Risteski. Statistical efficiency of score matching: The view from isoperimetry. _International Conference on Learning Representations_, 2023.
* [KLA19] T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4401-4410, 2019.
* [KVE21] B. Kawar, G. Vaksman, and M. Elad. Stochastic image denoising by sampling from the posterior distribution. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1866-1875, 2021.
* [LBA\({}^{+}\)22] R. Laumont, V. D. Bortoli, A. Almansa, J. Delon, A. Durmus, and M. Pereyra. Bayesian imaging using plug & play priors: when Langevin meets Tweedie. _SIAM Journal on Imaging Sciences_, 15(2):701-737, 2022.

* [LDP07] M. Lustig, D. Donoho, and J. M. Pauly. Sparse MRI: The application of compressed sensing for rapid MR imaging. _Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine_, 58(6):1182-1195, 2007.
* [LHE\({}^{+}\)24] G. Li, Y. Huang, T. Efimov, Y. Wei, Y. Chi, and Y. Chen. Accelerating convergence of score-based diffusion models, provably. _arXiv preprint arXiv:2403.03852_, 2024.
* [LHW24] G. Li, Z. Huang, and Y. Wei. Towards a mathematical theory for consistency training in diffusion models. _arXiv preprint arXiv:2402.07802_, 2024.
* [LST21] Y. T. Lee, R. Shen, and K. Tian. Structured logconcave sampling with a restricted Gaussian oracle. In _Conference on Learning Theory_, pages 2993-3050. PMLR, 2021.
* [LWCC23] G. Li, Y. Wei, Y. Chen, and Y. Chi. Towards faster non-asymptotic convergence for diffusion-based generative models. _arXiv preprint arXiv:2306.09251_, 2023.
* [LZB\({}^{+}\)22] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. DPM-Solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. _Advances in Neural Information Processing Systems_, 35:5775-5787, 2022.
* [MLE21] V. Monga, Y. Li, and Y. C. Eldar. Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing. _IEEE Signal Processing Magazine_, 38(2):18-44, 2021.
* [MSKV24] M. Mardani, J. Song, J. Kautz, and A. Vahdat. A variational perspective on solving inverse problems with diffusion models. In _The Twelfth International Conference on Learning Representations_, 2024.
* [MV19] O. Mangoubi and N. K. Vishnoi. Nonconvex sampling with the metropolis-adjusted langevin algorithm. In _Conference on learning theory_, pages 2259-2293. PMLR, 2019.
* [PEPC10] L. C. Potter, E. Ertin, J. T. Parker, and M. Cetin. Sparsity and compressed sensing in radar imaging. _Proceedings of the IEEE_, 98(6):1006-1020, 2010.
* [PRS\({}^{+}\)24] C. Pabbaraju, D. Rohatgi, A. P. Sevekari, H. Lee, A. Moitra, and A. Risteski. Provable benefits of score matching. _Advances in Neural Information Processing Systems_, 36, 2024.
* [PW24] Y. Polyanskiy and Y. Wu. _Information theory: From coding to learning_. Cambridge university press, 2024.
* [RBL\({}^{+}\)22] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [RDN\({}^{+}\)22] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with CLIP latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [RDS\({}^{+}\)15] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. _International Journal of Computer Vision (IJCV)_, 115(3):211-252, 2015.
* [REM17] Y. Romano, M. Elad, and P. Milanfar. The little engine that could: Regularization by denoising (RED). _SIAM Journal on Imaging Sciences_, 10(4):1804-1844, 2017.
* [RR98] G. O. Roberts and J. S. Rosenthal. Optimal scaling of discrete approximations to Langevin diffusions. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 60(1):255-268, 1998.
* [RS18] E. T. Reehorst and P. Schniter. Regularization by denoising: Clarifications and new interpretations. _IEEE transactions on computational imaging_, 5(1):52-67, 2018.

* [SC97] L. Saloff-Coste. _Lectures on finite Markov chains_, pages 301-413. Springer Berlin Heidelberg, Berlin, Heidelberg, 1997.
* [Sch12] H. Schaefer. _Banach Lattices and Positive Operators_, volume 215. Springer Science & Business Media, 2012.
* [SCS\({}^{+}\)22] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* [SDWMG15] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265, 2015.
* [SE19] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in Neural Information Processing Systems_, 32, 2019.
* [SEC\({}^{+}\)15] Y. Shechtman, Y. C. Eldar, O. Cohen, H. N. Chapman, J. Miao, and M. Segev. Phase retrieval with application to optical imaging: a contemporary overview. _IEEE signal processing magazine_, 32(3):87-109, 2015.
* [SKZ\({}^{+}\)23] B. Song, S. M. Kwon, Z. Zhang, X. Hu, Q. Qu, and L. Shen. Solving inverse problems with latent diffusion models via hard data consistency. _arXiv preprint arXiv:2307.08123_, 2023.
* [SME20] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2020.
* [SSDK\({}^{+}\)21] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. _International Conference on Learning Representations_, 2021.
* [SSXE21] Y. Song, L. Shen, L. Xing, and S. Ermon. Solving inverse problems in medical imaging with score-based generative models. In _International Conference on Learning Representations_, 2021.
* [SVMK22] J. Song, A. Vahdat, M. Mardani, and J. Kautz. Pseudoinverse-guided diffusion models for inverse problems. In _International Conference on Learning Representations_, 2022.
* [SWC\({}^{+}\)23] Y. Sun, Z. Wu, Y. Chen, B. T. Feng, and K. L. Bouman. Provable probabilistic imaging using score-based generative priors. _arXiv preprint arXiv:2310.10835_, 2023.
* [SZY\({}^{+}\)23] J. Song, Q. Zhang, H. Yin, M. Mardani, M.-Y. Liu, J. Kautz, Y. Chen, and A. Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. In _International Conference on Machine Learning_, pages 32483-32498. PMLR, 2023.
* 1728, 1994.
* [TYT\({}^{+}\)22] B. L. Trippe, J. Yim, D. Tischer, D. Baker, T. Broderick, R. Barzilay, and T. Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. _arXiv preprint arXiv:2206.04119_, 2022.
* [TZ24] W. Tang and H. Zhao. Contractive diffusion probabilistic models. _arXiv preprint arXiv:2401.13115_, 2024.
* [UVL18] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Deep image prior. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 9446-9454, 2018.
* [VBW13] S. V. Venkatakrishnan, C. A. Bouman, and B. Wohlberg. Plug-and-play priors for model based reconstruction. In _IEEE Global Conference on Signal and Information Processing_, pages 945-948. IEEE, 2013.

* [VDC19] M. Vono, N. Dobigeon, and P. Chainais. Split-and-augmented Gibbs sampler-application to large-scale inference problems. _IEEE Transactions on Signal Processing_, 67(6):1648-1661, 2019.
* [Vin11] P. Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* [Wib18] A. Wibisono. Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem. In _Conference on Learning Theory_, pages 2093-3027. PMLR, 2018.
* [WTN\({}^{+}\)23] L. Wu, B. Trippe, C. Naesseth, D. Blei, and J. P. Cunningham. Practical and asymptotically exact conditional sampling in diffusion models. _Advances in Neural Information Processing Systems_, 36, 2023.
* [ZC22] Q. Zhang and Y. Chen. Fast sampling of diffusion models with exponential integrator. In _The Eleventh International Conference on Learning Representations_, 2022.

Related works

**Algorithmic unrolling and plug-and-play image reconstruction.** Composite optimization algorithms, which aim to minimize the sum of a measurement fidelity term and a regularization term promoting desirable solution structures, have been the backbone of inverse problem solvers. To unleash the power of deep learning, [10] advocates the perspective of algorithmic unrolling, which turns an iterative algorithm into concatenations of linear and nonlinear layers like in a neural network. [23] recognized that the proximal mapping step in many composite optimization algorithms can be regarded as a denoiser or denoising operator with respect to the given prior, and proposed to "plug in" alternative denoisers, in particular state-of-the-art deep learning denoisers, leading to a class of popular algorithms known as plug-and-play methods [17]; see [14] for a review.

**Regularization by denoising and score matching.**[24] pointed out a connection between score matching and image denoising, which is a consequence of the Tweedie's formula [13]. The regularization by denoising (RED) framework [14] follows the plug-and-play framework to minimize a regularized objective function, where the regularizer is defined based on the plug-in image denoiser; [15] later clarified that the RED framework can be interpreted as score matching by denoising using the Tweedie's formula. [16] developed a stochastic image denoiser for posterior sampling of image denoising using annealed Langevin dynamics. [12] provided a framework to learn exact proximal operators for inverse problems.

**Plug-and-play posterior sampling.** Motivated by the need to characterize the uncertainty, tackling image reconstruction as posterior sampling from a Bayesian perspective is another important approach. Our method is inspired by the plug-and-play framework but takes on a sampling perspective, exploiting the connection between optimization and sampling [25]. Along similar lines, [15, 16] proposed Bayesian counterparts of plug-and-play for posterior sampling, where they leveraged the connection to score matching for sampling from the image prior, but did not consider score-based diffusion models for the image prior, which is a key aspect of ours; see also [26]. [18] extended the split Gibbs sampler [27] in the plug-and-play framework, and advocated the use of score-based diffusion models such as DDPM [1] for image denoising based on heuristic observations. In contrast, we rigorously derive the denoising diffusion samplers from first principles, unraveling critical gaps from naive applications of the generative samplers to denoising, and offer theoretical guarantees on the correctness of our approach.

**Score-based diffusion models as image priors.** Several representative methods for solving inverse problems using score-based diffusion priors alternates between taking steps along the diffusion process and projecting onto the measurement constraint, e.g., [17, 18, 19, 20]. However, these approaches do not possess asymptotic consistency guarantees. [24] proposed to use multiple Monte Carlo samples to reduce bias. On the other hand, [17] developed Monte Carlo guided diffusion methods for Bayesian linear inverse problems which tend to be computationally expensive, and [14] recently introduced a filtering perspective and applied particle filtering. Although asymptotically consistent, these approaches are limited to linear inverse problems. [28, 29] introduced sequential Monte Carlo (SMC) algorithms for conditional sampling using unconditional diffusion models that are asymptotically exact. [26] developed a variational perspective that connects to the regularization by denoising framework. [27] showed that the worst-case complexity of diffusion posterior sampling can take super-polynomial time regardless of the algorithm in use.

**Theory of diffusion models and score matching.** A number of recent papers have studied the non-asymptotic convergence rates of popular diffusion samplers, including but not limited to stochastic DDPM-type samplers [17, 18, 19, 20], deterministic DDIM-type samplers [16, 17, 18], and accelerated samplers [15, 17]. In addition, the statistical efficiency of score matching has also been investigated [14, 15].

## Appendix B Discrete-time formulation of diffusion processes

The discrete-time forward and backward diffusion processes can be understood as two processes constructed in the following manner:

1. a forward process \[x_{0}\to x_{1}\rightarrow\cdots\to x_{T}\] that starts with samples from the target image distribution and diffuses into a noise distribution (e.g., standard Gaussians) by gradually injecting noise into the samples;2. a reverse process \[x_{T}^{\mathsf{rew}}\to x_{T-1}^{\mathsf{rew}}\to\cdots\to x_{0}^{\mathsf{rew}}\] that starts from pure noise (e.g., standard Gaussians) and converts it into samples whose distribution is close to the target image distribution.

Consider the forward Markov process in \(\mathbb{R}^{d}\) that starts with a sample from the data distribution \(p_{X}\), and adds noise over the trajectory according to equationparentequation

\[x_{0} \sim p^{\star},\] (26a) \[x_{t} =\sqrt{1-\beta_{t}}\,x_{t-1}+\sqrt{\beta_{t}}\,w_{t},\qquad 1\leq t \leq T,\] (26b)

where \(\{w_{t}\}_{1\leq t\leq T}\)'s are independent standard Gaussian vectors, i.e., \(w_{t}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathcal{N}(0,I_{d})\), and \(\{\beta_{t}\in(0,1)\}\) describes the noise-injection rates used in each step. Therefore, we can write \(x_{t}\) equivalently as

\[x_{t}\coloneqq\sqrt{\bar{\alpha}_{t}}\,x_{0}+\sqrt{1-\bar{\alpha}_{t}}\, \varepsilon_{t},\quad\varepsilon_{t}\sim\mathcal{N}(0,I_{d}),\quad t=0,1, \cdots,T.\] (27)

Here, \((\bar{\alpha}_{t})_{t=0,1,\cdots,T}\) is the _schedule_ of diffusion given by

\[\alpha_{t}\coloneqq 1-\beta_{t},\qquad\bar{\alpha}_{t}\coloneqq\prod_{k=1}^{t }\alpha_{k},\qquad 1\leq t\leq T.\] (28)

Clearly, it verifies that \(1\geq\bar{\alpha}_{0}>\bar{\alpha}_{1}>\cdots>\bar{\alpha}_{T}>0\). As long as \(\bar{\alpha}_{T}\) is vanishing, it is easy to observe that the distribution of \(x_{T}\) approaches \(\mathcal{N}(0,I_{d})\).

**Score functions.** As will be seen, in order to sample from \(p^{\star}\), it turns out to be sufficient to learn the score functions of \(p_{x_{t}}\) at each step of the forward process, defined as

\[s_{t}^{\star}(x)=\nabla\log p_{x_{t}}(x),\qquad t=0,1,\cdots,T.\] (29)

As in the continuous time, the score function can be viewed as a MMSE estimate:

\[s_{t}^{\star}(x)=-\frac{1}{\sqrt{1-\bar{\alpha}_{t}}}\underbrace{\mathbb{E}_{ x_{0}\sim p^{\star},\,\varepsilon_{t}\sim\mathcal{N}(0,I_{d})}(\varepsilon_{t} \,|\,\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\varepsilon_{t}=x)}_ {=:\varepsilon_{t}^{\star}(x)}.\] (30)

Comparing (27) and (2), it can be checked that the discrete-time diffusion process can be embedded into the continuous-time one via the time change

\[t\mapsto\frac{1}{2}\log\frac{1}{\bar{\alpha}_{t}},\]

in the sense that

\[x_{t}^{\star}\stackrel{{\mathrm{(d)}}}{{=}}X_{\frac{1}{2}\log \frac{1}{\bar{\alpha}_{t}}}.\]

This establishes a correspondence between the continuous-time formulation and the discrete-time formulation.

Within the discrete-time formulation, we assume the score function estimates are given as \(\widehat{s}_{t}(\cdot):\mathbb{R}^{d}\to\mathbb{R}^{d}\), \(t=1,\ldots,T\) such that \(\widehat{s}_{t}\approx s_{t}^{\star}\) in analogy with the continuous-time counterpart.

## Appendix C Details of algorithm subroutines

With the discrete-time perspective established in Appendix B, we are now ready to present the detailed description and the implementation of our algorithms DDS-DDPM (Algorithm 2), DDS-DDIM (Algorithm 3), and PCS (Algorithm 4).

## Appendix D Score functions of diffusion denoising samplers

### Proof of Lemma 1

Proof.: The marginal distribution (14) of the heat flow can be written as

\[Y_{\tau}\stackrel{{\mathrm{(d)}}}{{=}}Y_{0}+\sqrt{\tau} \varepsilon,\quad Y_{0}\sim p^{\star},\;\varepsilon\sim\mathcal{N}(0,I_{d}).\] (31)

Comparing (2) and (31), it is not hard to check that

\[Y_{\tau}\stackrel{{\mathrm{(d)}}}{{=}}\sqrt{1+\tau}X_{\frac{1}{2} \log(1+\tau)}.\]

**Input**: noisy data \(x_{\mathsf{noisy}}\in\mathbb{R}^{d}\), score estimates \(\widehat{s}:=\{\widehat{s}_{t}(\cdot):\mathbb{R}^{d}\to\mathbb{R}^{d},t=1,\dots,T\}\) or noise estimates \(\widehat{\varepsilon}=\{\widehat{\varepsilon}_{t}(\cdot):\mathbb{R}^{d}\to \mathbb{R}^{d},t=1,\dots,T\}\), and noise level \(\eta>0\).

**Scheduling**: Compute the diffusion schedule \((\tau_{t})_{0\leq t\leq T^{\prime}}\) by

\[\tau_{t}=\bar{\alpha}_{t}^{-1}-1,\quad 0\leq t\leq T^{\prime},\]

where

\[T^{\prime}\coloneqq\max\left\{t:0\leq t\leq T,\,\bar{\alpha}_{t}>\frac{1}{ \eta^{2}+1}\right\}.\]

**Initialization**: Set \(\widehat{x}_{T^{\prime}}=x_{\mathsf{noisy}}\).

**Diffusion**: **for**\(t=T^{\prime},T^{\prime}-1,\dots,1\)**do**

\[\widehat{x}_{t-1}=\widehat{x}_{t}-2(\sqrt{\tau_{t}}-\sqrt{\tau_{t-1}})\, \widehat{\varepsilon}_{t}+\sqrt{\tau_{t}-\tau_{t-1}}\,w_{t},\quad w_{t}\sim \mathcal{N}(0,I_{d}).\]

where

\[\widehat{\varepsilon}_{t}\coloneqq\widehat{\varepsilon}_{t}(\sqrt{\bar{ \alpha}_{t}}\,\widehat{x}_{t})=-\frac{1}{\sqrt{1-\bar{\alpha}_{t}}}\widehat{ s}_{t}\left(\sqrt{\bar{\alpha}_{t}}\,\widehat{x}_{t}\right).\]

**Output**: \(\widehat{x}_{0}\).

**Input**: noisy data \(x_{\mathsf{noisy}}\in\mathbb{R}^{d}\), score estimates \(\widehat{s}:=\{\widehat{s}_{t}(\cdot):\mathbb{R}^{d}\to\mathbb{R}^{d},t=1, \dots,T\}\) or noise estimates \(\widehat{\varepsilon}=\{\widehat{\varepsilon}_{t}(\cdot):\mathbb{R}^{d}\to \mathbb{R}^{d},t=1,\dots,T\}\), and noise level \(\eta>0\).

**Scheduling**: Compute the diffusion schedule \((\bar{u}_{t})_{0\leq t\leq T^{\prime}}\) by

\[\bar{u}_{t}=\frac{(\eta^{2}+1)\bar{\alpha}_{t}-1}{\eta^{2}+\bar{\alpha}_{t}-1 },\quad 0\leq t\leq T^{\prime},\]

where

\[T^{\prime}\coloneqq\max\left\{t:0\leq t\leq T,\,\bar{\alpha}_{t}>\frac{1}{ \eta^{2}+1}\right\}.\]

**Initialization**: Draw \(z_{T^{\prime}}\sim\mathcal{N}(0,I_{d})\).

**Diffusion**: **for**\(t=T^{\prime},T^{\prime}-1,\dots,1\)**do**

\[z_{t-1}=\frac{\sqrt{(\eta^{2}-1)\bar{u}_{t-1}+1}}{\sqrt{(\eta^{2}-1)\bar{u}_{ t}+1}}z_{t}+\sqrt{(\eta^{2}-1)\bar{u}_{t-1}+1}\cdot\big{(}h(\eta,\bar{u}_{t-1})-h( \eta,\bar{u}_{t})\big{)}\widehat{\varepsilon}_{t},\]

where

\[h(\eta,u) \coloneqq-\arctan\frac{\eta}{\sqrt{u^{-1}-1}},\] \[\widehat{\varepsilon}_{t} \coloneqq\widehat{\varepsilon}_{t}\left(\sqrt{\bar{\alpha}_{t}}x_ {\mathsf{noisy}}+\frac{\eta^{2}\sqrt{\bar{u}_{t}\bar{\alpha}_{t}}z_{t}}{( \eta^{2}-1)\bar{u}_{t}+1}\right)=-\frac{1}{\sqrt{1-\bar{\alpha}_{t}}}\widehat{ s}_{t}\left(\sqrt{\bar{\alpha}_{t}}x_{\mathsf{noisy}}+\frac{\eta^{2}\sqrt{\bar{u}_{t} \bar{\alpha}_{t}}z_{t}}{(\eta^{2}-1)\bar{u}_{t}+1}\right).\]

**Output**: \(x_{\mathsf{noisy}}+z_{0}\).

**Algorithm 2** Denoising Diffusion Sampler (stochastic) DDS-DDPM\((x_{\mathsf{noisy}},\widehat{s},\eta)\)

Denote \(\theta=\frac{1}{2}\log(1+\tau)\) as a short-hand. We have

\[p_{Y_{\tau}}(x)=p_{\sqrt{1+\tau}X_{\theta}}(x)\propto p_{X_{\theta}}\left( \frac{1}{\sqrt{1+\tau}}x\right).\]

Therefore it follows that

\[\nabla\log p_{Y_{\tau}}(x)=\nabla_{x}\log p_{X_{\theta}}\left(\frac{1}{\sqrt{1 +\tau}}x\right)=\frac{1}{\sqrt{1+\tau}}s\left(\theta,\frac{1}{\sqrt{1+\tau}}x \right),\]

where we used the definition \(s(\theta,\cdot)=\nabla\log p_{X_{\theta}}(\cdot)\). Plugging the definition \(\theta=\frac{1}{2}\log(1+\tau)\) into the above equation yields the desired result.

**Algorithm 4** Proximal Consistency Sampler \(\mathsf{PCS}(x,y,\mathcal{L},\eta)\) (adapted from Metropolis-Adjusted Langevin Algorithm [12])

``` Input: starting point \(x\in\mathbb{R}^{d}\), measurements \(y\in\mathbb{R}^{m}\), log-likelihood function of the forward model \(\mathcal{L}(\cdot;y)\), proximal parameter \(\eta>0\). Hyperparameter: Langevin stepsize \(\gamma\), and the number of iterations \(N\). Initialization: \(z_{0}=x\). Update: for\(n=0,1,\cdots,N-1\)do  1. **One step of discretized Langevin**: Set \(r=\mathrm{e}^{-\gamma/\eta^{2}}\), and \[z_{n+\frac{1}{2}}=rz_{n}+(1-r)x+\eta^{2}(1-r)\nabla_{z_{n}}\mathcal{L}(z_{n};y )+\eta\sqrt{1-r^{2}}w_{n},\quad w_{n}\sim\mathcal{N}(0,I_{d}).\] This is equivalent to drawing \(z_{n+\frac{1}{2}}\) from a distribution with density \(Q(\cdot;z_{n})\), where \[Q(z^{\prime};z)=\frac{1}{(2\pi(1-r^{2}))^{d/2}}\exp\left(-\frac{\left\|z^{ \prime}-\left(rz+(1-r)x+\eta^{2}(1-r)\nabla_{z}\mathcal{L}(z;y)\right)\right\| ^{2}}{2(1-r^{2})}\right).\]
2. Metropolis adjustment: Compute \[q=\frac{\exp\left(\mathcal{L}(z_{n+\frac{1}{2}};y)-\frac{1}{2\eta^{2}}\|z_{n+ \frac{1}{2}}-x\|^{2}\right)}{\exp\left(\mathcal{L}(z_{n};y)-\frac{1}{2\eta^{2} }\|z_{n}-x\|^{2}\right)}\cdot\frac{Q(z_{n};z_{n+\frac{1}{2}})}{Q(z_{n+\frac{1 }{2}};z_{n})},\] and set \[z_{n+1}=\begin{cases}z_{n+\frac{1}{2}},&\text{with probability }\min(1,q),\\ z_{n}&\text{with probability }1-\min(1,q).\end{cases}\] Output: \(z_{N}\). ```

**Algorithm 4** Proximal Consistency Sampler \(\mathsf{PCS}(x,y,\mathcal{L},\eta)\) (adapted from Metropolis-Adjusted Langevin Algorithm [12])

### Proof of Lemma 2

Proof.: We first compute the probability density function of \(z\). Recall that \(z=w-x_{\mathsf{noisy}}\), thus applying Bayes rule yields

\[p_{z}(x)=p_{w}(x+x_{\mathsf{noisy}}) =p^{\star}(x^{\star}=x+x_{\mathsf{noisy}}|x^{\star}+\xi=x_{ \mathsf{noisy}})\] \[=\frac{p^{\star}(x+x_{\mathsf{noisy}})p_{\xi}(-x)}{p_{x^{\star}+ \xi}(x_{\mathsf{noisy}})}\propto p^{\star}(x+x_{\mathsf{noisy}})p_{\xi}(-x),\]

where \(\xi\sim\mathcal{N}(0,\eta^{2}I_{d})\). It is straightforward to compute

\[p_{\xi}(-x)=\frac{1}{(2\pi)^{d/2}\eta^{d}}\mathrm{e}^{-\frac{1}{2\eta^{2}}\|x \|^{2}},\]

therefore

\[p_{z}(x)\propto p^{\star}(x+x_{\mathsf{noisy}})\mathrm{e}^{-\frac{1}{2\eta^{2 }}\|x\|^{2}}.\] (32)

We proceed to compute the probability density function of \(Z_{\tau}\). According to (19), it follows that

\[p_{Z_{\tau}}(x) =p_{\mathrm{e}^{-\tau}z}*p_{\sqrt{1-\mathrm{e}^{-2\tau}}\varepsilon }(x)\] \[=\int p_{\mathrm{e}^{-\tau}z}(x^{\prime})\,p_{\sqrt{1-\mathrm{e}^ {-2\tau}}\varepsilon}(x-x^{\prime})\mathrm{d}x^{\prime}\] \[\propto\int p^{\star}(x_{\mathsf{noisy}}+\mathrm{e}^{\tau}x^{ \prime})\exp\left(-\frac{1}{2\eta^{2}}\|\mathrm{e}^{\tau}x^{\prime}\|^{2} \right)\exp\left(-\frac{1}{2(1-\mathrm{e}^{-2\tau})}\|x-x^{\prime}\|^{2} \right)\mathrm{d}x^{\prime},\] \[\propto\int p^{\star}(x^{\prime})\exp\left(-\frac{1}{2\eta^{2}} \|x^{\prime}-x_{\mathsf{noisy}}\|^{2}\right)\exp\left(-\frac{1}{2(1-\mathrm{e} ^{-2\tau})}\|x-\mathrm{e}^{-\tau}(x^{\prime}-x_{\mathsf{noisy}})\|^{2}\right) \mathrm{d}x^{\prime},\] (33)

where \(*\) denotes convolution, the penultimate line follows from (32) and the last line follow from the change of variable \(x^{\prime}\mapsto\mathrm{e}^{-\tau}(x^{\prime}-x_{\mathsf{noisy}})\). One may exercise some brute force to verify that

\[\exp\left(-\frac{1}{2\eta^{2}}\|x^{\prime}-x_{\mathsf{noisy}}\|^{2}\right)\exp \left(-\frac{1}{2(1-\mathrm{e}^{-2\tau})}\|x-\mathrm{e}^{-\tau}(x^{\prime}-x_{ \mathsf{noisy}})\|^{2}\right)\]\[=\exp\left(-\frac{\mathrm{e}^{2\tau}\|x\|^{2}}{2(\eta^{2}+ \mathrm{e}^{2\tau}-1)}\right)\exp\left(-\frac{1}{2(1-\mathrm{e}^{-2\tilde{\tau}}) }\Big{\|}\mathrm{e}^{-\tilde{\tau}}x_{\mathsf{noisy}}+\frac{\mathrm{e}^{\tau- \tilde{\tau}}\eta^{2}x}{\eta^{2}+\mathrm{e}^{2\tau}-1}-\mathrm{e}^{-\tilde{\tau }}x^{\prime}\Big{\|}^{2}\right)\] \[\propto\exp\left(-\frac{\mathrm{e}^{2\tau}\|x\|^{2}}{2(\eta^{2}+ \mathrm{e}^{2\tau}-1)}\right)p_{\sqrt{1-\mathrm{e}^{-2\tilde{\tau}}}\varepsilon} \left(\mathrm{e}^{-\tilde{\tau}}x_{\mathsf{noisy}}+\frac{\mathrm{e}^{\tau- \tilde{\tau}}\eta^{2}x}{\eta^{2}+\mathrm{e}^{2\tau}-1}-\mathrm{e}^{-\tilde{ \tau}}x^{\prime}\right),\]

where \(\tilde{\tau}\) is as defined in (22). Plug this back into (33), we see

\[p_{Z_{\tau}}(x) \propto\exp\left(-\frac{\mathrm{e}^{2\tau}\|x\|^{2}}{2(\eta^{2}+ \mathrm{e}^{2\tau}-1)}\right)\int p^{\star}(x^{\prime})p_{\sqrt{1-\mathrm{e}^{ -2\tilde{\tau}}}\varepsilon}\left(\mathrm{e}^{-\tilde{\tau}}x_{\mathsf{noisy}}+ \frac{\mathrm{e}^{\tau-\tilde{\tau}}\eta^{2}x}{\eta^{2}+\mathrm{e}^{2\tau}-1}- \mathrm{e}^{-\tilde{\tau}}x^{\prime}\right)\mathrm{d}x^{\prime}\] \[\propto\exp\left(-\frac{\mathrm{e}^{2\tau}\|x\|^{2}}{2(\eta^{2}+ \mathrm{e}^{2\tau}-1)}\right)\int p^{\star}(\mathrm{e}^{\tilde{\tau}}x^{\prime })p_{\sqrt{1-\mathrm{e}^{-2\tilde{\tau}}}\varepsilon}\left(\mathrm{e}^{- \tilde{\tau}}x_{\mathsf{noisy}}+\frac{\mathrm{e}^{\tau-\tilde{\tau}}\eta^{2}x }{\eta^{2}+\mathrm{e}^{2\tau}-1}-x^{\prime}\right)\mathrm{d}x^{\prime}\] \[\propto\exp\left(-\frac{\mathrm{e}^{2\tau}\|x\|^{2}}{2(\eta^{2}+ \mathrm{e}^{2\tau}-1)}\right)p_{\mathrm{e}^{-\tilde{\tau}}x_{0}}*p_{\sqrt{1- \mathrm{e}^{-2\tilde{\tau}}}\varepsilon}\left(\mathrm{e}^{-\tilde{\tau}}x_{ \mathsf{noisy}}+\frac{\mathrm{e}^{\tau-\tilde{\tau}}\eta^{2}x}{\eta^{2}+ \mathrm{e}^{2\tau}-1}\right)\] \[\propto\exp\left(-\frac{\mathrm{e}^{2\tau}\|x\|^{2}}{2(\eta^{2}+ \mathrm{e}^{2\tau}-1)}\right)p_{X_{\tau}}\left(\mathrm{e}^{-\tilde{\tau}}x_{ \mathsf{noisy}}+\frac{\mathrm{e}^{\tau-\tilde{\tau}}\eta^{2}x}{\eta^{2}+ \mathrm{e}^{2\tau}-1}\right),\]

where the second line applies the change of variable \(x^{\prime}\mapsto\mathrm{e}^{\tilde{\tau}}x^{\prime}\) in the integral, the penultimate line follows from \(p_{\mathrm{e}^{-\tilde{\tau}}x_{0}}(x^{\prime})\propto p^{\star}(\mathrm{e}^{ \tilde{\tau}}x^{\prime})\) (since \(x_{0}\sim p^{\star}\)), and the last line follows from \(X_{\tilde{\tau}}\stackrel{{(\mathrm{d})}}{{=}}\mathrm{e}^{-\tilde{ \tau}}x_{0}+\sqrt{1-\mathrm{e}^{-2\tilde{\tau}}}\varepsilon\).

Finally, from the above formula, we obtain

\[\nabla\log p_{Z_{\tau}}(x) =\nabla_{x}\left(-\frac{\mathrm{e}^{2\tau}\|x\|^{2}}{2(\eta^{2}+ \mathrm{e}^{2\tau}-1)}\right)+\nabla_{x}\log p_{X_{\tau}}\left(\mathrm{e}^{- \tilde{\tau}}x_{\mathsf{noisy}}+\frac{\mathrm{e}^{\tau-\tilde{\tau}}\eta^{2}x }{\eta^{2}+\mathrm{e}^{2\tau}-1}\right)\] \[=-\frac{\mathrm{e}^{2\tau}x}{\eta^{2}+\mathrm{e}^{2\tau}-1}+\frac {\mathrm{e}^{\tau-\tilde{\tau}}\eta^{2}}{\eta^{2}+\mathrm{e}^{2\tau}-1}s\left( \tilde{\tau},\,\mathrm{e}^{-\tilde{\tau}}x_{\mathsf{noisy}}+\frac{\mathrm{e}^ {\tau-\tilde{\tau}}\eta^{2}x}{\eta^{2}+\mathrm{e}^{2\tau}-1}\right),\]

where we used the definition \(s(\tilde{\tau},\cdot)=\nabla\log p_{X_{\tau}}(\cdot)\). 

## Appendix E Discretization via the exponential integrator

### General form of the exponential integrator

Consider a SDE of the form:

\[\mathrm{d}M_{\tau}=\big{(}v(\tau)M_{\tau}+f(\tau,M_{\tau})\big{)}\mathrm{d} \tau+\sqrt{\beta}\mathrm{d}B_{\tau},\quad\tau\in[0,\tau_{\infty}],\quad M_{0} \sim p_{M_{0}},\]

where \(v:[0,\tau_{\infty}]\to\mathbb{R}\), \(f:[0,\tau_{\infty}]\times\mathbb{R}^{d}\to\mathbb{R}^{d}\) are deterministic functions, and \(\beta>0\) is a constant. Given discretization time points \(0=\tau_{0}\leq\tau_{1}\leq\cdots\leq\tau_{k}\leq\tau_{\infty}\), a naive way to discretize the SDE is

\[M_{\tau_{i+1}}-M_{\tau_{i}}\approx\big{(}v(\tau_{i})M_{\tau_{i}}+f(\tau_{i},M_{ \tau_{i}})\big{)}(\tau_{i+1}-\tau_{i})+\sqrt{\beta}\sqrt{\tau_{i+1}-\tau_{i}} \varepsilon_{i},\quad i=0,1,\cdots,k-1,\]

where \(\varepsilon_{i}\sim\mathcal{N}(0,I_{d})\) is a standard \(d\)-dimensional Gaussian random vector which is independent of \(M_{\tau_{i}}\). Although this approach is straightforward, it has the drawback that the linear term \(v(\tau)M_{\tau}\) is discretized rather crude. For example, for the OU process where \(v\equiv-1\), \(f\equiv 0\), \(\beta=2\), the SDE can be solved analytically as in (2), while the above approach still has a discretization error.

A more accurate discretization, known to significantly improve the quality of score-based generative models, is given by the _exponential integrator_[22], which preserves the linear term and discretizes the SDE to

\[\mathrm{d}\widehat{M}_{\tau}=\big{(}v(\tau)\widehat{M}_{\tau}+f(\tau_{i}, \widehat{M}_{\tau_{i}})\big{)}\mathrm{d}\tau+\sqrt{\beta}\mathrm{d}B_{\tau}, \quad\tau\in[\tau_{i},\tau_{i+1}],\quad i=0,1,\cdots,k,\]

with initialization \(\widehat{M}_{0}\sim p_{M_{0}}\). On each time interval \([\tau_{i},\tau_{i+1}]\), this is simply a linear SDE, which can be explicitly solved by

\[\widehat{M}_{\tau}\stackrel{{(\mathrm{d})}}{{=}}\mathrm{e}^{V( \tau)-V(\tau_{i})}\widehat{M}_{\tau_{i}}+\left(\int_{\tau_{i}}^{\tau}\mathrm{e}^{V( \tau)-V(\tilde{\tau})}\mathrm{d}\tilde{\tau}\right)f(\tau_{i},\widehat{M}_{\tau _{i}})+\sqrt{\beta}\left(\int_{\tau_{i}}^{\tau}\mathrm{e}^{2(V(\tau)-V(\tilde{ \tau}))}\mathrm{d}\tilde{\tau}\right)^{1/2}\varepsilon_{i},\]

where \(V\) is the antiderivative of \(v\):

\[V(\tau)=\int_{0}^{\tau}v(\tilde{\tau})\mathrm{d}\tilde{\tau}.\]Taking \(\tau=\tau_{i+1}\), we obtain

\[\widehat{M}_{\tau_{i+1}} \stackrel{{\rm(d)}}{{=}} \mathrm{e}^{V(\tau_{i+1})-V(\tau_{i})}\widehat{M}_{\tau_{i}}+\left( \int_{\tau_{i}}^{\tau_{i+1}}\mathrm{e}^{V(\tau_{i+1})-V(\tilde{\tau})}\mathrm{d} \tilde{\tau}\right)f(\tau_{i},\widehat{M}_{\tau_{i}})\] \[+\sqrt{\beta}\mathrm{e}^{V(\tau_{i+1})}\left(\int_{\tau_{i}}^{ \tau_{i+1}}\mathrm{e}^{2(V(\tau_{i+1})-V(\tilde{\tau}))}\mathrm{d}\tilde{\tau} \right)^{1/2}\varepsilon_{i},\] (34)

which provides an iterative formula to compute \(\widehat{M}_{\tau_{i+1}}\).

### Discretization of DDS-DDPM

Plug the expression of \(\nabla\log p_{Y_{\tau}}\) in Lemma 1 into (16), and use the notation \(\tau^{\mathsf{rev}}=\eta^{2}-\tau\), we obtain, for \(\tau\in[0,\eta^{2}]\), that

\[\mathrm{d}Y_{\tau^{\mathsf{rev}}}^{\mathsf{rev}} =\frac{1}{\sqrt{1+\tau^{\mathsf{rev}}}}s\Big{(}\frac{1}{2}\log(1+ \tau^{\mathsf{rev}}),\,\frac{Y_{\tau^{\mathsf{rev}}}^{\mathsf{rev}}}{\sqrt{1+ \tau^{\mathsf{rev}}}}\Big{)}\,\mathrm{d}\tau+\mathrm{d}\tilde{B}_{\tau}\] \[=-\frac{1}{\sqrt{\tau^{\mathsf{rev}}}}\varepsilon^{\mathsf{cont}} \bigg{(}\frac{1}{2}\log(1+\tau^{\mathsf{rev}}),\,\frac{Y_{\tau^{\mathsf{rev}} }^{\mathsf{rev}}}{\sqrt{1+\tau^{\mathsf{rev}}}}\Big{)}\,\mathrm{d}\tau+ \mathrm{d}\tilde{B}_{\tau}.\]

**Choosing discretization time points.** To discretize this SDE, we first choose the discretization time points. Recalling (27), it is most reasonable to discretize at those time points \(0\leq\tau_{0}^{\mathsf{rev}}\leq\cdots\leq\tau_{T^{\mathsf{rev}}}^{\mathsf{ rev}}\leq\eta^{2}\) which satisfy

\[\frac{1}{2}\log(1+\tau_{t}^{\mathsf{rev}})=\frac{1}{2}\log\frac{1}{\bar{\alpha }_{t}},\quad 0\leq t\leq T^{\prime}.\]

This solves to

\[\tau_{t}^{\mathsf{rev}}=\bar{\alpha}_{t}^{-1}-1.\] (35)

The requirement that \(\tau_{t}^{\mathsf{rev}}\leq\eta^{2}\) translates to \(\bar{\alpha}_{t}\geq\frac{1}{1+\eta^{2}}\), which yields the following choice of \(T^{\prime}\):

\[T^{\prime}\coloneqq\max\left\{t:0\leq t\leq T,\,\bar{\alpha}_{t}>\frac{1}{ \eta^{2}+1}\right\}.\] (36)

**Applying the exponential integrator.** Now we apply the exponential integrator to discretize the SDE on each time interval \(\tau^{\mathsf{rev}}\in[\tau_{t-1},\tau_{t}]\), \(t=1,\cdots,T^{\prime}\) as follows:

\[\mathrm{d}\widehat{Y}_{\tau^{\mathsf{rev}}}^{\mathsf{rev}} =-\frac{1}{\sqrt{\tau^{\mathsf{rev}}}}\varepsilon^{\mathsf{cont}} \Bigg{(}\frac{1}{2}\log(1+\tau_{t}^{\mathsf{rev}}),\,\frac{\widehat{Y}_{\tau_ {t}^{\mathsf{rev}}}^{\mathsf{rev}}}{\sqrt{1+\tau_{t}^{\mathsf{rev}}}^{\mathsf{ rev}}}\Bigg{)}\,\mathrm{d}\tau+\mathrm{d}\tilde{B}_{\tau},\] \[=-\frac{1}{\sqrt{\tau^{\mathsf{rev}}}}\varepsilon_{t}^{*}\left( \frac{\widehat{Y}_{\tau_{t}^{\mathsf{rev}}}^{\mathsf{rev}}}{\sqrt{1+\tau_{t}^{ \mathsf{rev}}}}\right)\,\mathrm{d}\tau+\mathrm{d}\tilde{B}_{\tau}\] \[=-\frac{1}{\sqrt{\tau^{\mathsf{rev}}}}\varepsilon_{t}^{*}\left( \sqrt{\bar{\alpha}_{t}}\widehat{Y}_{\tau_{t}^{\mathsf{rev}}}^{\mathsf{rev}} \right)\,\mathrm{d}\tau+\mathrm{d}\tilde{B}_{\tau}.\]

The SDE can be integrated directly on \(\tau^{\mathsf{rev}}\in[\tau_{t-1},\tau_{t}]\) (see also (34), with \(v\equiv 0\)), yielding

\[\widehat{Y}_{\tau_{t-1}^{\mathsf{rev}}}^{\mathsf{rev}} =\widehat{Y}_{\tau_{t}^{\mathsf{rev}}}^{\mathsf{rev}}-2(\sqrt{\tau _{t}^{\mathsf{rev}}}-\sqrt{\tau_{t-1}^{\mathsf{rev}}})\cdot\varepsilon_{t}^{*} \Big{(}\sqrt{\bar{\alpha}_{t}}\widehat{Y}_{\tau_{t}^{\mathsf{rev}}}^{\mathsf{ rev}}\Big{)}+\int_{\eta^{2}-\tau_{t-1}}^{\eta^{2}-\tau_{t-1}}d\tilde{B}_{\tau} \mathrm{d}\tau\] \[\stackrel{{\rm(d)}}{{=}}\widehat{Y}_{\tau_{t}^{ \mathsf{rev}}}^{\mathsf{rev}}-2(\sqrt{\tau_{t}^{\mathsf{rev}}}-\sqrt{\tau_{t-1 }^{\mathsf{rev}}})\cdot\varepsilon_{t}^{*}\Big{(}\sqrt{\bar{\alpha}_{t}} \widehat{Y}_{\tau_{t}^{\mathsf{rev}}}^{\mathsf{rev}}\Big{)}+\sqrt{\tau_{t}^{ \mathsf{rev}}-\tau_{t-1}^{\mathsf{rev}}}w_{t},\]

where \(w_{t}\sim\mathcal{N}(0,I_{d})\) is independent of \(\widehat{Y}_{\tau_{t}^{\mathsf{rev}}}^{\mathsf{rev}}\). Set \(\widehat{x}_{t}=\widehat{Y}_{\tau_{t}^{\mathsf{rev}}}^{\mathsf{rev}}\), we obtain

\[\widehat{x}_{t-1}\stackrel{{\rm(d)}}{{=}}\widehat{x}_{t}-2(\sqrt{ \tau_{t}}-\sqrt{\tau_{t-1}})\cdot\varepsilon_{t}^{*}\big{(}\sqrt{\bar{\alpha}_{t }}\widehat{x}_{t}\big{)}+\sqrt{\tau_{t}-\tau_{t-1}}w_{t},\quad w_{t}\sim \mathcal{N}(0,I_{d}),\] (37)

which is exactly the update equation in Algorithm 2, except that \(\varepsilon_{t}^{*}\) is replaced by the noise estimate \(\widehat{\varepsilon}_{t}\).

### Discretization of DDS-DDIM

Plug in the expression of \(s_{Z}\) in Lemma 2 into the probability flow ODE (20), we obtain

\[\mathrm{d}Z^{\mathsf{rev}}_{\tau^{\mathsf{rev}}} =\frac{\eta^{2}-1}{\eta^{2}+\mathrm{e}^{2\tau^{\mathsf{rev}}}-1}Z ^{\mathsf{rev}}_{\tau^{\mathsf{rev}}}\mathrm{d}\tau+\frac{\mathrm{e}^{\tau^{ \mathsf{rev}}-\bar{\tau}(\tau^{\mathsf{rev}})}\eta^{2}}{\eta^{2}+\mathrm{e}^{2 \tau^{\mathsf{rev}}}-1}s\bigg{(}\bar{\tau}(\tau^{\mathsf{rev}}),\,\mathrm{e}^{- \bar{\tau}(\tau^{\mathsf{rev}})}x_{\mathsf{noisy}}+\frac{\mathrm{e}^{\tau^{ \mathsf{rev}}-\bar{\tau}(\tau^{\mathsf{rev}})}\eta^{2}x}{\eta^{2}+\mathrm{e}^{2 \tau^{\mathsf{rev}}}-1}\bigg{)}\,\mathrm{d}\tau\] \[=\frac{\eta^{2}-1}{\eta^{2}+\mathrm{e}^{2\tau^{\mathsf{rev}}}-1}Z ^{\mathsf{rev}}_{\tau^{\mathsf{rev}}}\mathrm{d}\tau-\frac{\mathrm{e}^{2\tau^{ \mathsf{rev}}}}{\mathrm{e}^{2\tau^{\mathsf{rev}}}-1}\varepsilon^{\mathsf{cont }}\bigg{(}\bar{\tau}(\tau^{\mathsf{rev}}),\,\mathrm{e}^{-\bar{\tau}(\tau^{ \mathsf{rev}})}x_{\mathsf{noisy}}+\frac{\mathrm{e}^{\tau^{\mathsf{rev}}-\bar{ \tau}(\tau^{\mathsf{rev}})}\eta^{2}x}{\eta^{2}+\mathrm{e}^{2\tau^{\mathsf{ rev}}}-1}\bigg{)}\,\mathrm{d}\tau,\]

where the second line used the definition (22).

**Choosing discretization time points.** Similar to the derivation in Appendix E.2, we discretize at time points \(0=\tau_{0}^{\mathsf{rev}}\leq\tau_{1}^{\mathsf{rev}}\leq\cdots\leq\tau_{T}^{ \mathsf{rev}}\leq\eta^{2}\), which obey

\[\bar{\tau}(\tau_{t}^{\mathsf{rev}})=\frac{1}{2}\log\frac{1}{\bar{\alpha}_{t}}, \quad t=0,1,\ldots,T^{\prime},\] (38)

which solves to

\[\tau_{t}^{\mathsf{rev}}=\frac{1}{2}\log\frac{\eta^{2}+\bar{\alpha}_{t}-1}{( \eta^{2}+1)\bar{\alpha}_{t}-1}.\] (39)

To make this well-defined, we require

\[\frac{\eta^{2}+\bar{\alpha}_{t}-1}{(\eta^{2}+1)\bar{\alpha}_{t}-1}>0,\]

which is equivalent to

\[\bar{\alpha}_{t}>\frac{1}{1+\eta^{2}}.\]

This leads to the same choice of \(T^{\prime}\) as in (36). We also set

\[\tau_{\infty}=\tau_{T^{\prime}}^{\mathsf{rev}}.\]

It is convenient to introduce a notation for the corresponding discrete schedule of \(\tau_{t}^{\mathsf{rev}}\), denoted by

\[\bar{u}_{t}=\mathrm{e}^{-2\tau_{t}^{\mathsf{rev}}}=\frac{(\eta^{2}+1)\bar{ \alpha}_{t}-1}{\eta^{2}+\bar{\alpha}_{t}-1},\quad t=0,1,\cdots,T^{\prime}.\]

**Applying the exponential integrator.** Now we apply the exponential integrator, which discretizes the ODE on each time interval \(\tau^{\mathsf{rev}}\in[\tau_{t-1},\tau_{t}]\), \(t=1,\cdots,T^{\prime}\), as

\[\mathrm{d}\widehat{Z}^{\mathsf{rev}}_{\tau^{\mathsf{rev}}} =\frac{\eta^{2}-1}{\eta^{2}+\mathrm{e}^{2\tau^{\mathsf{rev}}}-1} \widehat{Z}^{\mathsf{rev}}_{\tau^{\mathsf{rev}}}\mathrm{d}\tau-\frac{\mathrm{ e}^{2\tau^{\mathsf{rev}}}}{\mathrm{e}^{2\tau^{\mathsf{rev}}}-1}\varepsilon^{\mathsf{ cont}}\bigg{(}\bar{\tau}(\tau_{t}^{\mathsf{rev}}),\,\mathrm{e}^{-\bar{\tau}( \tau_{t}^{\mathsf{rev}})}x_{\mathsf{noisy}}+\frac{\mathrm{e}^{\tau_{t}^{\mathsf{ rev}}-\bar{\tau}(\tau_{t}^{\mathsf{rev}})}\eta^{2}\widehat{Z}^{\mathsf{rev}}_{ \tau^{\mathsf{rev}}}}{\eta^{2}+\mathrm{e}^{2\tau_{t}^{\mathsf{rev}}}-1}\bigg{)} \,\mathrm{d}\tau\] \[=\frac{\eta^{2}-1}{\eta^{2}+\mathrm{e}^{2\tau^{\mathsf{rev}}}-1} \widehat{Z}^{\mathsf{rev}}_{\tau^{\mathsf{rev}}}\mathrm{d}\tau-\frac{\mathrm{ e}^{2\tau^{\mathsf{rev}}}}{\mathrm{e}^{2\tau^{\mathsf{rev}}}-1}\varepsilon^{\star}_{t} \bigg{(}\sqrt{\bar{\alpha}_{t}}x_{\mathsf{noisy}}+\frac{\sqrt{\bar{u}_{t}}\sqrt{ \bar{\alpha}_{t}}\eta^{2}\widehat{Z}^{\mathsf{rev}}_{\tau^{\mathsf{rev}}}}{ (\eta^{2}-1)\bar{u}_{t}+1}\bigg{)}\,\mathrm{d}\tau,\]

where the second line follows from (38), and the last line follows from dividing both the denominator and the numerator in the fraction inside \(\widehat{\varepsilon}_{t}\) by \(\mathrm{e}^{2\tau_{t}^{\mathsf{rev}}}\). This is a first-order linear ODE on \(\tau^{\mathsf{rev}}\in[\tau_{t-1},\tau_{t}]\), which can be solved explicitly (cf. (34)) by

\[\widehat{Z}^{\mathsf{rev}}_{\tau^{\mathsf{rev}}} =\frac{\sqrt{(\eta^{2}-1)\mathrm{e}^{-2\tau^{\mathsf{rev}}}+1}}{ \sqrt{(\eta^{2}-1)\bar{u}_{t}+1}}\widehat{Z}^{\mathsf{rev}}_{\tau_{t}^{\mathsf{ rev}}}\] \[\qquad+\sqrt{(\eta^{2}-1)\mathrm{e}^{-2\tau^{\mathsf{rev}}}+1} \cdot\big{(}h(\eta,\mathrm{e}^{-2\tau^{\mathsf{rev}}})-h(\eta,\bar{u}_{t}) \big{)}\cdot\varepsilon^{\star}_{t}\bigg{(}\sqrt{\bar{\alpha}_{t}}x_{\mathsf{ noisy}}+\frac{\sqrt{\bar{u}_{t}}\sqrt{\bar{\alpha}_{t}}\eta^{2}\widehat{Z}^{ \mathsf{rev}}_{\tau^{\mathsf{rev}}}}{(\eta^{2}-1)\bar{u}_{t}+1}\bigg{)}\,,\]

for \(\tau^{\mathsf{rev}}\in[\tau_{t-1},\tau_{t}]\), where

\[h(\eta,u)\coloneqq-\arctan\frac{\eta}{\sqrt{u^{-1}-1}}.\]Plug in \(\tau^{\mathsf{rev}}=\tau_{t-1}\) in the above solution, and set \(z_{t}=\widetilde{Z}^{\mathsf{rev}}_{\tau_{t}^{\mathsf{rev}}}\), we obtain

\[z_{t-1} =\frac{\sqrt{(\eta^{2}-1)\bar{u}_{t-1}+1}}{\sqrt{(\eta^{2}-1)\bar{u }_{t}+1}}z_{t}\] (40) \[\qquad+\sqrt{(\eta^{2}-1)\bar{u}_{t-1}+1}\cdot\left(h(\eta,\bar{ u}_{t-1})-h(\eta,\bar{u}_{t})\right)\cdot\varepsilon_{t}^{\star}\left(\sqrt{ \bar{u}_{t}}x_{\mathsf{noisy}}+\frac{\sqrt{\bar{u}_{t}}\sqrt{\bar{\alpha}_{t}} \eta^{2}z_{t}}{(\eta^{2}-1)\bar{u}_{t}+1}\right).\]

The initialization, which should ideally be \(z_{T^{\prime}}=\widetilde{Z}^{\mathsf{rev}}_{\tau_{\mathsf{pos}}}\sim p_{Z_{ \mathsf{pos}}}\), is approximated by \(z_{T^{\prime}}\sim\mathcal{N}(0,I_{d})\). This is exactly the update equation and the initialization in Algorithm 3, except that \(\varepsilon_{t}^{\star}\) is replaced by the noise estimate \(\widehat{\varepsilon}_{t}\).

### Discretization of PCS

We first note that the Metropolis-adjustment step in PCS (cf. Algorithm 4) is standard following the classical form of MALA [14]. Therefore, we focus on explaining the Langevin step. Recall the continuous-time Langevin dynamics for sampling from the distribution \(\exp(\mathcal{L}(\cdot;y)-\frac{1}{2\eta^{2}}\|\cdot-x\|^{2})\):

\[\mathrm{d}Z_{\tau}=-\nabla_{Z_{\tau}}\mathcal{L}(Z_{\tau};y)\mathrm{d}\tau+ \frac{1}{\eta^{2}}(Z_{\tau}-x)\mathrm{d}\tau+\sqrt{2}\mathrm{d}B_{\tau},\quad \tau\geq 0,\quad Z_{0}\sim\mathcal{N}(0,I_{d}).\] (41)

The classical form of MALA, as in [14], performs one step of a straightforward discretization of (41) as the Langevin step, as follows:

\[z_{n+\frac{1}{2}}\approx z_{n}-\gamma\nabla_{z_{n}}\mathcal{L}(z_{n};y)+\frac {\gamma}{\eta^{2}}(z_{n}-x)+\sqrt{2\gamma}w_{n},\quad w_{n}\sim\mathcal{N}(0, I_{d}).\]

In our setting, due to the presence of the linear drift term \(\frac{1}{\eta^{2}}(Z_{\tau}-x)\), which can be quite large when \(\eta\) is small, we apply the exponential integrator instead. Set the discretization time points \(\tau_{n}=n\gamma\), the exponential integrator reads as

\[\mathrm{d}Z_{\tau}=-\nabla_{Z_{n\gamma}}\mathcal{L}(Z_{n\gamma};y)\mathrm{d} \tau+\frac{1}{\eta^{2}}(Z_{\tau}-x)\mathrm{d}\tau+\sqrt{2}\mathrm{d}B_{\tau}, \quad n\gamma\leq\tau\leq(n+1)\gamma.\]

Solve this linear SDE on \(n\gamma\leq\tau\leq(n+1)\gamma\) directly (see also (34)) to obtain

\[Z_{(n+1)\gamma}\stackrel{{\mathrm{(d)}}}{{=}}rZ_{n\gamma}+(1-r)x +\eta^{2}(1-r)\nabla_{Z_{n\gamma}}\mathcal{L}(Z_{n\gamma};y)+\eta\sqrt{1-r^{2} }w_{n},\quad w_{n}\sim\mathcal{N}(0,I_{d}),\]

where \(r\coloneqq\mathrm{e}^{-\gamma/\eta^{2}}\). This is the same as the update equation for the Langevin step in PCS (cf. Algorithm 4).

## Appendix F Proof of main theorems

### Proof of Theorem 1

Proof.: We first collect the asymptotic correctness of our subroutines PCS and DDS in the following two lemmas. The correctness of PCS is actually well-known, see e.g., [13, Corollary 2].

**Lemma 3** (Correctness of PCS).: _Under Assumption 1, with notation in Algorithm 4, in the continuous-time limit: \(\gamma\to 0\), \(N\to\infty\), the algorithm PCS outputs samples with distribution \(\propto\exp(\mathcal{L}(\cdot;y)+\frac{1}{2\eta}\|\cdot-x\|^{2})\)._

The next lemma guarantees the correctness of DDS with exact unconditional score functions.

**Lemma 4** (Correctness of DDS).: _Assume the score function estimation \(\widehat{s}_{t}\) is accurate, i.e. \(\widehat{s}_{t}=s_{t}^{\star}\). In the continuous-time limit: \(T\to\infty,\bar{\alpha}_{T}\to 0,\frac{\bar{\alpha}_{t-1}}{\bar{\alpha}_{t}}\to 1\), uniformly in \(t\), both_ DDS-DDIM _and_ DDS-DDPM _output \(x\) obeying the posterior distribution \(p^{\star}(x^{\star}=x\,|\,x^{\star}+\eta\varepsilon=x_{\mathsf{noisy}})\), \(\varepsilon\sim\mathcal{N}(0,I_{d})\)._

The proof of Theorem 1 is based on two lemmas on the one-step transition kernel of DPnP and the asymptotic behavior of the transition kernel, which we will present soon. First, we set up some notations. Denote

\[p_{\eta}(x)\coloneqq p_{x^{\star}\sim p^{\star},\varepsilon\sim\mathcal{N}(0, I_{d})}(x^{\star}+\eta\varepsilon=x)=\frac{1}{(2\pi)^{d/2}\eta^{d}}\int p^{\star}(z) \mathrm{e}^{-\frac{1}{2\pi^{2}}\|x-z\|^{2}}\mathrm{d}z.\]From the first equality, it is clear that \(p_{\eta}\to p^{\star}\) when \(\eta\to 0^{+}\). We will also use the notation \(q_{\eta}\) defined in (24), which we recall here:

\[q_{\eta}(x)\coloneqq\frac{1}{(2\pi)^{d/2}\eta^{d}}\int\mathrm{e}^{\mathcal{L}(z ;y)-\frac{1}{2\eta^{d}}\|x-z\|^{2}}\mathrm{d}z.\]

In virtue of the Assumption 1, we know that \(q_{\eta}\) is finite for all \(x\in\mathbb{R}^{d}\).

For convenience, we introduce a notation for application of transition kernels. For a probability distribution \(p(x)\) and a probability transition kernel \(K(x,x^{\prime})\), denote by \(p\circ K\) the probability distribution given by

\[p\circ K(x^{\prime})=\int p(x)K(x,x^{\prime})\mathrm{d}x.\]

The first lemma characterizes the one-step behavior of DPnP in terms of Markov transition kernels.

**Lemma 5**.: _Under the settings of Lemma 4 and Lemma 3, the one-step transition kernel of DPnP with \(\eta_{k}=\eta\) is given by:_

\[K_{\textsf{DPnP},\eta}(x,x^{\prime})=\left(\int\frac{q_{0}(z)}{p_{\eta}(z)} \mathrm{e}^{-\frac{1}{2\eta^{d}}\|z-x\|^{2}-\frac{1}{2\eta^{d}}\|z-x^{\prime} \|^{2}}\mathrm{d}z\right)\frac{p^{\star}(x^{\prime})}{q_{\eta}(x)}.\]

_In other words, if \(\widehat{x}_{k}\) has distribution \(p_{\widehat{x}_{k}}\), then the distribution of \(\widehat{x}_{k+1}\) is_

\[p_{\widehat{x}_{k+1}}(x^{\prime})=p_{\widehat{x}_{k}}\circ K_{\textsf{DPnP}, \eta}(x)=\int p_{\widehat{x}_{k}}(x)K_{\textsf{DPnP},\eta}(x,x^{\prime}) \mathrm{d}x.\]

The proof is postponed to Appendix F.3. The next lemma analyzes the ergodic properties of the Markov chain with transition kernel \(K_{\textsf{DPnP},\eta}\). These properties are known [1] but scattered in different literatures, so we will provide a brief proof to be self-contained.

**Lemma 6**.: _The Markov transition kernel \(K_{\textsf{DPnP},\eta}\) has the following properties:_

1. _(Stationary distribution.) Let_ \(\pi_{\eta}\) _be the probability distribution defined by_ \[\pi_{\eta}(x)=c_{\eta}p^{\star}(x)q_{\eta}(x),\] _where_ \(c_{\eta}>0\) _is the normalization constant such that_ \(\int\pi_{\eta}(x)\mathrm{d}x=1\)_. Then_ \(K_{\textsf{DPnP},\eta}\) _is reversible with stationary distribution_ \(\pi_{\eta}\)_._
2. _(Convergence.) For any initial distribution_ \(p\)_, the distribution of the Markov chain with kernel_ \(K_{\textsf{DPnP},\eta}\) _converges to_ \(\pi_{\eta}\)_:_ \[\mathsf{TV}(p\circ K_{\textsf{DPnP},\eta}^{(n)},\,\pi_{\eta})\to 0,\quad n \to\infty,\] (42) _where_ \(K_{\textsf{DPnP},\eta}^{(n)}\) _is the_ \(n\)_-step transition kernel of_ \(K_{\textsf{DPnP},\eta}\)_._

The proof is postponed to Appendix F.4. We now show how to prove Theorem 1 with the above two lemmas. With the annealing schedule in Theorem 1, between steps \(k_{l-1}\leq k<k_{l}\), which consist of consecutive \((k_{l}-k_{l-1})\) steps, the transition kernel of one-step of DPnP is \(K_{\textsf{DPnP},\varepsilon_{l}}\). As \((k_{l}-k_{l-1})\to\infty\), Lemma 6 implies that

\[\mathsf{TV}(p_{\widehat{x}_{k_{l}}},\,\pi_{\varepsilon_{l}})=\mathsf{TV}(p_{ \widehat{x}_{k_{l-1}}}\circ K_{\textsf{DPnP},\varepsilon_{l}}^{(k_{l}-k_{l-1 })},\,\pi_{\varepsilon_{l}})\to 0.\]

Under the assumption in Theorem 1 that \(\varepsilon_{l}\to 0\), we let \(l\to\infty\) to see \(\lim_{l\to\infty}\pi_{\varepsilon_{l}}=c_{0}p^{\star}(\cdot)\mathrm{e}^{ \mathcal{L}(\cdot;y)}=p^{\star}(\cdot|y)\), thus \(p_{\widehat{x}_{k_{l}}}\to p^{\star}(\cdot|y)\), as claimed.

### Proof of Lemma 4

Proof.: For \(\mathsf{DDS}\)-\(\mathsf{DDPM}\), we note that under the continuous-time limit in Lemma 4, the discretization time points given by (35) verify

\[\tau_{0}^{\mathsf{rew}}=0,\quad\sup_{0\leq t\leq T^{\prime}-1}|\tau_{t}^{\mathsf{ rew}}-\tau_{t+1}^{\mathsf{rew}}|\to 0,\quad\tau_{T^{\prime}}^{\mathsf{rew}} \rightarrow\left(\frac{1}{1+\eta^{2}}\right)^{-1}-1=\eta^{2},\quad T^{\prime} \rightarrow\infty.\]

Therefore, these discretization time points \(0=\tau_{0}^{\mathsf{rew}}\leq\cdots\leq\tau_{T^{\prime}}^{\mathsf{rew}}\leq \eta^{2}\) form a partition of \([0,\eta^{2}]\), which becomes infinitely fine in the continuous-time limit. Thus the discretized integrator (37) converges to the solution of the SDE (16), which, as we have already argued in Appendix D, produces samples obeying the denoising posterior distribution \(p^{\star}(\cdot|x_{\text{noisy}})\), as claimed.

The proof for \(\mathsf{DDS}\)-\(\mathsf{DDPM}\) follows similarly, by observing that the discretization time points in (35) form an infinitely fine partition of \([0,\infty)\) in the continuous-time limit. 

### Proof of Lemma 5

Proof.: The proof is based on computing the transition kernel of the two subroutines. We claim that

1. Sampling with probability density proportional to \(\exp(\mathcal{L}(\cdot;y)-\frac{1}{2\eta^{2}}\|\cdot-x\|^{2})\) is equivalent to applying the following Markov transition kernel \[K_{\mathsf{POS},\eta}(x,x^{\prime})=\frac{1}{q_{\eta}(x)}\mathrm{e}^{ \mathcal{L}(x^{\prime};y)-\frac{1}{2q_{\eta}^{2}}\|x^{\prime}-x\|^{2}}.\]
2. Sampling with probability \(p^{\star}(x^{\star}\,|\,x^{\star}+\eta\varepsilon=x)\), where \(\varepsilon\sim\mathcal{N}(0,I_{d})\), is equivalent to applying the following Markov transition kernel: \[K_{\mathsf{DDS},\eta}(x,x^{\prime})=\frac{1}{p_{\eta}(x)}p^{\star}(x^{\prime} )\mathrm{e}^{-\frac{1}{2q_{\eta}^{2}}\|x^{\prime}-x\|^{2}}.\]

It is then clear that

\[K_{\mathsf{DPnp},\eta}(x,x^{\prime})=\int K_{\mathsf{POS},\eta}(x,z)K_{ \mathsf{DDS},\eta}(z,x^{\prime})\mathrm{d}z=\left(\int\frac{q_{0}(z)}{p_{\eta} (z)}\mathrm{e}^{-\frac{1}{2q_{\eta}^{2}}\|z-z\|^{2}-\frac{1}{2q_{\eta}^{2}}\|z -x^{\prime}\|^{2}}\mathrm{d}z\right)\frac{p^{\star}(x^{\prime})}{q_{\eta}(x)},\]

as desired. We now prove the above two claims. For (i), note that by (23), we know \(K_{\mathsf{DDS},\eta}(x,\cdot)\propto p^{\star}(\cdot)\mathrm{e}^{-\frac{1}{2 q_{\eta}^{2}}\|\cdot-x\|^{2}}\). Thus it suffices to compute the normalization constant, which is

\[\int p^{\star}(x^{\prime})\mathrm{e}^{-\frac{1}{2q_{\eta}^{2}}\|x^{\prime}-x\| ^{2}}\mathrm{d}x^{\prime}=p_{\eta}(x),\]

by the definition of \(p_{\eta}\). Therefore

\[K_{\mathsf{DDS},\eta}(x,x^{\prime})=\frac{1}{p_{\eta}(x)}p^{\star}(x^{\prime}) \mathrm{e}^{-\frac{1}{2q_{\eta}^{2}}\|x^{\prime}-x\|^{2}},\]

as claimed. The proof of (ii) follows similarly. 

### Proof of Lemma 6

Proof.: We first introduce a fundamental lemma [14], which provides a simple method to bound the total variation between two distributions.

**Lemma 7** (Data-processing inequality).: _Let \(p,q\) be two probability distributions, and \(K\) be a probability transition kernel. Then_

\[\mathsf{TV}(p\circ K,q\circ K)\leq\mathsf{TV}(p,q).\]

We now prove the two items in Lemma 6 separately.

Proof of (i).: We first show that \(\pi_{\eta}\) is well-defined, i.e., \(\int p^{\star}(x)q_{n}\eta(x)\mathrm{d}x<\infty\). This can be seen from Assumption 1, which implies \(q_{\eta}(x)\lesssim\int\mathrm{e}^{-\frac{1}{2\eta^{2}}\|x-z\|^{2}}\mathrm{d}z\lesssim 1\), hence

\[\int p^{\star}(x)q_{\eta}(x)\mathrm{d}x\lesssim\int p^{\star}(x)\mathrm{d}x=1.\]To show that \(K_{\mathsf{DPnpP},\eta}\) is reversible with stationary distribution \(\pi_{\eta}\), it suffices to verify

\[\pi_{\eta}(x)K_{\mathsf{DPnpP},\eta}(x,x^{\prime})=\pi_{\eta}(x^{\prime})K_{ \mathsf{DPnpP},\eta}(x^{\prime},x),\quad\forall x,x^{\prime}\in\mathbb{R}^{d}.\]

However, it is easily checked that both sides are equal to

\[c_{\eta}\left(\int\frac{q_{0}(z)}{p_{\eta}(z)}\mathrm{e}^{-\frac{1}{2p_{\eta} ^{2}}\|z-x\|^{2}-\frac{1}{2q^{2}}\|z-x^{\prime}\|^{2}}\mathrm{d}z\right)p^{ \star}(x^{\prime})p^{\star}(x).\]

Proof of (ii).: We define an auxiliary Markov transition kernel \(K_{\mathsf{aux},\eta}=K_{\mathsf{ODS},\eta}\circ K_{\mathsf{PCS},\eta}\). More explicitly,

\[K_{\mathsf{aux},\eta}(x,x^{\prime})=\int K_{\mathsf{ODS},\eta}(x,x)K_{\mathsf{ PCS},\eta}(z,x^{\prime})\mathrm{d}z=\left(\int\frac{p^{\star}(z)}{q_{\eta}(z)} \mathrm{e}^{-\frac{1}{2q^{2}}\|z-x\|^{2}-\frac{1}{2q^{2}}\|z-x^{\prime}\|^{2 }}\mathrm{d}z\right)\frac{\mathrm{e}^{\mathcal{L}(x^{\prime};y)}}{p_{\eta}(x)}.\] (43)

It is easy to see that

\[p\circ K_{\mathsf{DPnp},\eta}^{(n)}=p\circ K_{\mathsf{PCS},\eta}\circ K_{ \mathsf{aux},\eta}^{(n-1)}\circ K_{\mathsf{ODS},\eta}.\] (44)

Thus we are led to investigate the ergodic properties of \(K_{\mathsf{aux},\eta}\). Similar to the proof of item (i) above, it is not hard to show that \(K_{\mathsf{aux},\eta}\) is reversible with respect to the stationary distribution

\[\mu_{\eta}(x)\coloneqq c_{\eta}p_{\eta}(x)q_{0}(x)=c_{\eta}p_{\eta}(x) \mathrm{e}^{\mathcal{L}(x;y)}.\]

Moreover, one may check that

\[\pi_{\eta}=\mu_{\eta}\circ K_{\mathsf{ODS},\eta}.\] (45)

It is apparent that \(\mu(x^{\prime})>0\) and \(K_{\mathsf{aux},\eta}(x,x^{\prime})/\mu_{\eta}(x^{\prime})>0\) for all \(x,x^{\prime}\in\mathbb{R}^{d}\). By [11, Corollary 1], such a Markov transition kernel obeys, for any probability distribution \(q\), that

\[\mathsf{TV}(q\circ K_{\mathsf{aux},\eta}^{(n)},\ \mu_{\eta})\to 0,\quad n\to\infty.\]

In view of (44) and (45), we set \(q=p\circ K_{\mathsf{PCS},\eta}\) and invoke the data-processing inequality to obtain

\[\mathsf{TV}(p\circ K_{\mathsf{DPnp},\eta}^{(n)},\ \pi_{\eta}) =\mathsf{TV}(q\circ K_{\mathsf{aux},\eta}^{(n-1)}\circ K_{ \mathsf{ODS},\eta},\ \mu_{\eta}\circ K_{\mathsf{ODS},\eta})\] \[\leq\mathsf{TV}(q\circ K_{\mathsf{aux},\eta}^{(n-1)},\ \mu_{\eta})\] \[\to 0,\]

as \(n\to\infty\). This completes the proof. 

### Proof of Theorem 2

Proof.: Denote by \(\tilde{K}_{\mathsf{PCS},\eta}\) and \(\tilde{K}_{\mathsf{ODS},\eta}\) and the transition kernels for \(\mathsf{PCS}\) and for \(\mathsf{DDS}\), respectively. Note that these may deviate from the transition kernels \(K_{\mathsf{PCS},\eta}\) and \(K_{\mathsf{ODS},\eta}\) defined for the idealized asymptotic setting in Appendix F. We have

\[\mathsf{TV}(p_{\tilde{x}_{N}},\pi_{\eta}) =\mathsf{TV}(p_{\tilde{x}_{N-\frac{1}{2}}}\circ\tilde{K}_{\mathsf{ODS },\eta},\ \pi_{\eta})\] \[\leq\mathsf{TV}(p_{\tilde{x}_{N-\frac{1}{2}}}\circ K_{\mathsf{ODS },\eta},\ \pi_{\eta})+\mathsf{TV}(p_{\tilde{x}_{N-\frac{1}{2}}}\circ K_{\mathsf{ODS },\eta},\ p_{\tilde{x}_{N-\frac{1}{2}}}\circ\tilde{K}_{\mathsf{ODS},\eta})\] \[\leq\mathsf{TV}(p_{\tilde{x}_{N-\frac{1}{2}}}\circ K_{\mathsf{ODS },\eta},\ \pi_{\eta})+\varepsilon_{\mathsf{DDS}},\]

where the second line is triangle inequality, and the third line follows from the assumption in Theorem 2 that \(\mathsf{DDS}\) has error at most \(\varepsilon_{\mathsf{ODS}}\) in total variation, by taking the input of \(\mathsf{DDS}\) to be \(\tilde{x}_{N-\frac{1}{2}}\).

Similarly, from \(p_{\tilde{x}_{N-\frac{1}{2}}}=p_{\tilde{x}_{N-1}}\circ\tilde{K}_{\mathsf{PCS},\eta}\) and the assumption that \(\mathsf{PCS}\) has error at most \(\varepsilon_{\mathsf{PCS}}\) in total variation, we can show

\[\mathsf{TV}(p_{\tilde{x}_{N-\frac{1}{2}}}\circ K_{\mathsf{ODS},\eta},\ \pi_{\eta})\leq\mathsf{TV}(p_{\tilde{x}_{N-1}}\circ K_{\mathsf{PCS},\eta}\circ K _{\mathsf{ODS},\eta},\ \pi_{\eta})+\varepsilon_{\mathsf{PCS}}=\mathsf{TV}(p_{\tilde{x}_{N-1}}\circ K_{ \mathsf{DPnp},\eta},\ \pi_{\eta})+\varepsilon_{\mathsf{PCS}}.\]

The above two inequalities together imply

\[\mathsf{TV}(p_{\tilde{x}_{N}},\pi_{\eta})\leq\mathsf{TV}(p_{\tilde{x}_{N-1}} \circ K_{\mathsf{DPnp},\eta},\ \pi_{\eta})+\varepsilon_{\mathsf{DDS}}+\varepsilon_{\mathsf{PCS}}.\]

Iterating this process, we obtain

\[\mathsf{TV}(p_{\tilde{x}_{N}},\pi_{\eta})\leq\mathsf{TV}(p_{\tilde{x}_{1}} \circ K_{\mathsf{DPnp},\eta}^{(N-1)},\ \pi_{\eta})+(N-1)(\varepsilon_{\mathsf{ODS}}+\varepsilon_{\mathsf{PCS}}).\] (46)

It remains to bound \(\mathsf{TV}(p_{\tilde{x}_{1}}\circ K_{\mathsf{DPnp},\eta}^{(N-1)},\ \pi_{\eta})\). For this, we need the following two lemmas.

**Lemma 8** (Comparing TV and \(\chi^{2}\)-divergence, [10]).: _For any two distributions \(p,q\), we have_

\[\mathsf{TV}(p,q)\leq\sqrt{\chi^{2}(p\,\|\,q)}.\]

**Lemma 9** (\(\chi^{2}\)-contractivity of \(K_{\mathsf{DPnp},\eta}\)).: _There exists some \(\lambda\coloneqq\lambda(p^{\star},\mathcal{L},\eta)\in(0,1)\), such that for any probability distribution \(p(x)\), we have_

\[\chi^{2}(p\circ K_{\mathsf{DPnp},\eta}^{(N)}\,\|\,\pi_{\eta})\leq\lambda^{2N} \chi^{2}(p\,\|\,\pi_{\eta}).\]

A form of Lemma 9 is well-known for Markov chains with countable state spaces, but relatively few sources provide a complete proof for the abstract setting we consider here with continuous state space. For sake of completeness, we prove Lemma 9 in Appendix F.6.

Combining the above two lemmas, we obtain

\[\mathsf{TV}(p_{\widehat{x}_{1}}\circ K_{\mathsf{DPnp},\eta}^{(N-1)},\;\pi_{ \eta})\leq\sqrt{\chi^{2}(p_{\widehat{x}_{1}}\circ K_{\mathsf{DPnp},\eta}^{(N- 1)}\,\|\,\pi_{\eta})}\leq\lambda^{N-1}\sqrt{\chi^{2}(p_{\widehat{x}_{1}}\,\| \,\pi_{\eta})}.\]

Plug this into (46), we obtain

\[\mathsf{TV}(p_{\widehat{x}_{N}},\pi_{\eta})\leq\lambda^{N-1}\sqrt{\chi^{2}(p_ {\widehat{x}_{1}}\,\|\,\pi_{\eta})}+(N-1)(\varepsilon_{\mathsf{ODS}}+ \varepsilon_{\mathsf{PCS}}).\]

With \(N\asymp\frac{\log(1/\varepsilon_{\mathsf{NCS}})}{1-\lambda}\) such that \(\lambda^{N-1}\leq\exp\big{(}-(N-1)(1-\lambda)\big{)}\leq\varepsilon_{\mathsf{ acc}}\), the desired result readily follows. 

### Proof of Lemma 9

Proof.: We need a few fundamental properties of reversible Markov chains, which are collected below.

First we set up some notations. Define the Hilbert space \(L^{2}(\pi)\) to be the space of square-integrable functions with respect to measure \(\pi\), i.e., those functions \(f:\mathbb{R}^{d}\to\mathbb{C}\) such that

\[\|f\|_{L^{2}(\pi)}\coloneqq\left(\int|f(x)|^{2}\pi(x)\mathrm{d}x\right)^{1/2}<\infty.\]

The first well-known property [11] offers a way to represent a reversible transition kernel as a self-adjoint operator (infinite-dimensional symmetric matrix).

**Lemma 10** (Self-adjoint representation of reversible Markov operator).: _Assume \(K(x,x^{\prime})\) is a Markov transition kernel that is reversible with respect to the stationary distribution \(\pi(x)\). Then the integral operator \(\mathcal{K}:L^{2}(\pi)\to L^{2}(\pi)\) defined by_

\[\mathcal{K}f(x)=\int K(x,x^{\prime})f(x^{\prime})\mathrm{d}x^{\prime}\]

_is self-adjoint and compact. For any probability distribution \(p(x)\) such that \(\int\frac{p^{2}(x)}{\pi(x)}\mathrm{d}x<\infty\), we have_

\[\int p(x)\cdot\mathcal{K}f(x)\mathrm{d}x=\int p\circ K(x^{\prime})f(x^{\prime} )\mathrm{d}x^{\prime}.\]

_Moreover, the eigenvalues of \(\mathcal{K}\) are the same as those of \(K\)._

The following theorem is a generalization of the classical Perron-Frobenius theory for finite-dimensional transition matrix to strictly positive operators. The form we present here can be found in [11, Theorem V.6.6]; see also [12, Theorem III.6.7] for a more elementary treatment which can also be adapted to the form we need.

**Theorem 3** (Jentzsch).: _Let \(K(x,x^{\prime})\) be a Markov transition kernel. If \(K(x,x^{\prime})>0\) for any \(x,x^{\prime}\in\mathbb{R}^{d}\), then \(K\) has a unique stationary distribution \(\pi\). Moreover, \(1\) is a simple eigenvalue of \(K\), with \(\pi\) being the only left eigenfunction, and the constant function \(1\) being the only right eigenfunction. In addition, there exists \(\lambda\in(0,1)\) such that any other eigenvalue of \(K\) has modulus no larger than \(\lambda\)._We are now ready to prove Lemma 9. We divide the proof into the following steps.

**Step 1: controlling the eigenvalues of \(\mathcal{K}_{\mathsf{DPnP},\eta}\).** Recall the auxiliary kernel \(K_{\mathsf{aux},\eta}\) defined in (43). It is a standard result in linear algebra or function analysis [11] that \(K_{\mathsf{aux},\eta}=K_{\mathsf{PCS},\eta}\circ K_{\mathsf{ODS},\eta}\) has same eigenvalues as \(K_{\mathsf{DPnP},\eta}=K_{\mathsf{ODS},\eta}\circ K_{\mathsf{PCS},\eta}\). From (43), it is easy to check \(K_{\mathsf{aux},\eta}(x,x^{\prime})>0\), thus Theorem 3 implies \(1\) is a simple eigenvalue of \(\mathcal{K}_{\mathsf{DPnP},\eta}\). Moreover, there exists \(\lambda\coloneqq\lambda(p^{\star},\mathcal{L},\eta)\in(0,1)\), such that any other eigenvalue of \(K_{\mathsf{aux},\eta}\) has modulus no larger than \(\lambda\).

Since \(K_{\mathsf{DPnP},\eta}\) has the same eigenvalues as \(K_{\mathsf{aux},\eta}\), and, by Lemma 10, the operator \(\mathcal{K}_{\mathsf{DPnP},\eta}\) also has the same eigenvalues as these two, we conclude that \(\mathcal{K}_{\mathsf{DPnP},\eta}\) is a self-adjoint compact operator on \(L^{2}(\pi_{\eta})\), of whom \(1\) is a simple eigenvalue. Moreover, any other eigenvalue of \(\mathcal{K}_{\mathsf{DPnP},\eta}\) has modulus no larger than \(\lambda\).

**Step 2: establishing the contractivity of \(\mathcal{K}_{\mathsf{DPnP},\eta}\) in \(L^{2}(\pi_{\eta})\).** It is easy to verify that the constant function \(\mathbf{1}\), which takes value \(1\) for any \(x\in\mathbb{R}^{d}\), is a eigenfunction of \(\mathcal{K}_{\mathsf{DPnP},\eta}\) associated to the simple eigenvalue \(1\), thus is the only (up to scaling) eigenfunction associated to that eigenvalue. It is also a unit-length eigenfunction, since \(\|\mathbf{1}\|_{L^{2}(\pi_{\eta})}=(\int 1\cdot\pi_{\eta}(x)dx)^{1/2}=1\). Therefore, the operator \(\mathcal{K}_{\mathsf{DPnP},\eta}-\mathbf{1}\mathbf{1}^{\top}\) is a self-adjoint operator whose eigenvalues have modulus no larger than \(\lambda\), where \(\mathbf{1}\mathbf{1}^{\top}\) is the orthogonal projection onto \(\mathbf{1}\) in \(L^{2}(\pi_{\eta})\), defined by

\[\mathbf{1}\mathbf{1}^{\top}f(x)\equiv\int f(x^{\prime})\pi_{\eta}(x^{\prime}) \mathrm{d}x^{\prime},\quad\forall x\in\mathbb{R}^{d}.\]

Using the fact that \(\mathcal{K}_{\mathsf{DPnP},\eta}\mathbf{1}\mathbf{1}^{\top}=\mathbf{1} \mathbf{1}^{\top}\mathcal{K}_{\mathsf{DPnP},\eta}=\mathbf{1}\mathbf{1}^{\top}\), one may show \((\mathcal{K}_{\mathsf{DPnP},\eta}-\mathbf{1}\mathbf{1}^{\top})^{N}=\mathcal{ K}_{\mathsf{DPnP},\eta}^{(N)}-\mathbf{1}\mathbf{1}^{\top}\) by expanding the product, see e.g. [10]. Consequently, \(\mathcal{K}_{\mathsf{DPnP},\eta}^{N}-\mathbf{1}\mathbf{1}^{\top}\) is a self-adjoint operator whose eigenvalues have modulus no larger than \(\lambda^{N}\), i.e.,

\[\left\|\mathcal{K}_{\mathsf{DPnP},\eta}^{N}-\mathbf{1}\mathbf{1}^{\top} \right\|_{L^{2}(\pi_{\eta})\to L^{2}(\pi_{\eta})}\leq\lambda^{N},\] (47)

where \(\|\cdot\|_{L^{2}(\pi_{\eta})\to L^{2}(\pi_{\eta})}\) denotes the operator norm on \(L^{2}(\pi_{\eta})\).

**Step 3: bounding the inner product of \(p\circ K_{\mathsf{DPnP},\eta}^{(N)}-\pi_{\eta}\) with any square-integrable function.** Note that when \(\chi^{2}(p\,\|\,\pi_{\eta})=\infty\), the conclusion is trivially true. For the rest part of the proof, we assume \(\chi^{2}(p\,\|\,\pi_{\eta})<\infty\). Now, for any \(f\in L^{2}(\pi_{\eta})\), by applying Lemma 10 iteratively, we obtain

\[\int p\circ K_{\mathsf{DPnP},\eta}^{(N)}(x)f(x)\mathrm{d}x =\int p(x^{\prime})\mathcal{K}_{\mathsf{DPnP},\eta}^{N}f(x^{ \prime})\mathrm{d}x^{\prime}\] \[=\int p(x^{\prime})\cdot(\mathcal{K}_{\mathsf{DPnP},\eta}^{N}- \mathbf{1}\mathbf{1}^{\top})f(x^{\prime})\mathrm{d}x^{\prime}+\int p(x^{\prime })\mathbf{1}\mathbf{1}^{\top}f(x^{\prime})\mathrm{d}x^{\prime}\] \[=\int p(x^{\prime})\cdot(\mathcal{K}_{\mathsf{DPnP},\eta}^{N}- \mathbf{1}\mathbf{1}^{\top})f(x^{\prime})\mathrm{d}x^{\prime}+\int f(x^{\prime })\pi_{\eta}(x^{\prime})\mathrm{d}x^{\prime},\]

where the last line follows from the definition of \(\mathbf{1}\mathbf{1}^{\top}\) and \(\int p(x^{\prime})\mathrm{d}x^{\prime}=1\). Rearrange the terms to see

\[\int\big{(}p\circ K_{\mathsf{DPnP},\eta}^{(N)}(x)-\pi_{\eta}(x)\big{)}f(x) \mathrm{d}x=\int p(x^{\prime})\cdot(\mathcal{K}_{\mathsf{DPnP},\eta}^{N}- \mathbf{1}\mathbf{1}^{\top})f(x^{\prime})\mathrm{d}x^{\prime}.\] (48)

In particular, taking \(p=\pi_{\eta}\) yields

\[0=\int\pi_{\eta}(x^{\prime})\cdot(\mathcal{K}_{\mathsf{DPnP},\eta}^{N}- \mathbf{1}\mathbf{1}^{\top})f(x^{\prime})\mathrm{d}x^{\prime}.\] (49)

Substract (49) from (48), and then take absolute value, we obtain

\[\left|\int\big{(}p\circ K_{\mathsf{DPnP},\eta}^{(N)}(x)-\pi_{\eta} (x)\big{)}f(x)\mathrm{d}x\right|\] \[=\left|\int\big{(}p(x^{\prime})-\pi_{\eta}(x^{\prime})\big{)} \cdot(\mathcal{K}_{\mathsf{DPnP},\eta}^{N}-\mathbf{1}\mathbf{1}^{\top})f(x^{ \prime})\mathrm{d}x^{\prime}\right|\] \[\leq\left(\int\frac{\big{(}p(x^{\prime})-\pi_{\eta}(x^{\prime}) \big{)}^{2}}{\pi_{\eta}(x)}\mathrm{d}x\right)^{1/2}\cdot\left\|(\mathcal{K}_{ \mathsf{DPnP},\eta}^{N}-\mathbf{1}\mathbf{1}^{\top})f(x^{\prime})\right\|_{L^{2} (\pi_{\eta})}\]\[\leq\sqrt{\chi^{2}(p\,\|\,\pi_{\eta})}\cdot\lambda^{N}\|f\|_{L^{2}(\pi_{ \eta})}.\] (50)

**Step 4: choosing an appropriate square-integrable function.** Now, set

\[f(x)=\frac{p\circ K^{(N)}_{\mathsf{DPnP},\eta}(x)-\pi_{\eta}(x)}{\pi_{\eta}(x)}.\]

It is easily checked that

\[\int\big{(}p\circ K^{(N)}_{\mathsf{DPnP},\eta}(x)-\pi_{\eta}(x) \big{)}f(x)\mathrm{d}x =\chi^{2}(p\circ K^{(N)}_{\mathsf{DPnP},\eta}\,\|\,\pi_{\eta}),\] \[\|f\|_{L^{2}(\pi_{\eta})} =\sqrt{\chi^{2}(p\circ K^{(N)}_{\mathsf{DPnP},\eta}\,\|\,\pi_{ \eta})}.\]

Plug these equations into (50), we obtain

\[\chi^{2}(p\circ K^{(N)}_{\mathsf{DPnP},\eta}\,\|\,\pi_{\eta})\leq\lambda^{2N} \chi^{2}(p\,\|\,\pi_{\eta}),\]

as claimed. 

## Appendix G Additional numerical results

### Implementation details

_Score functions used._ We use the same pre-trained score functions as in [9].3

Footnote 3: https://github.com/DPS2022/diffusion-posterior-sampling

_Normalization._ All images are normalized in the usual way to fit into the range \([-1,1]\).

_Parameters of our algorithm._ We choose the same annealing schedule across all tasks. Please see Appendix H for a detailed discussion.

_Parameters of comparison methods._ We made our best effort to fine-tune the other algorithms within a reasonable amount of time for each task. We list the paramters, following the notations in the original paper [9, 10], as follows.

* Super-resolution: For DPS, the learning rate is set to \(0.6\). For LGD-MC, the MC sampling variance \(r_{t}=0.05\), the loss coefficient \(\lambda=10^{-3}\), and the learning rate is set to \(60.0\). For ReSample, the stochastic resampling variance parameter \(\gamma=10\) for FFHQ and \(\gamma=4.0\) for ImageNet.
* Phase retrieval: For DPS, the stepsize is set to \(0.8\). For LGD-MC, the MC sampling variance \(r_{t}=0.05\), the loss coefficient \(\lambda=10^{-3}\), and the learning rate is set to \(400.0\).
* Quantized sensing: For DPS, the stepsize is set to \(100.0\). For LGD-MC, the MC sampling variance \(r_{t}=0.05\), the loss coefficient \(\lambda=2\times 10^{-5}\), and the learning rate is set to \(500.0\). For ReSample, the stochastic resampling variance parameter \(\gamma=4\) for FFHQ and \(\gamma=3.5\) for ImageNet.

### Forward measurement operators

_Super-resolution._ The forward model for super-resolution is the usual bicubic downsampling operator [11], which is a linear operator (in fact, a block Hankel matrix). We use a downsampling ratio of \(4\) in all our experiments. The measurement noise is set to be white Gaussian, with variance \(0.2\). Note that the noise variance is moderately larger than that in [9] to better reflect the scenario in practical inverse problems.

_Phase retrieval._ We consider phase retrieval with a coded mask, which is a classical inverse problem [9]. For a \(256\times 256\) image \(x\) (for each color channel) in our experiments, we first generate a random mask \(M\in\mathbb{R}^{256\times 256}\) (which is shared across color channels), then apply Fourier transform \(\mathcal{F}\) to \(M\odot x\), where \(\odot\) denotes the Hadamard (entrywise) product, and finally preserve only the magnitudes of the Fourier transform. Formally, the forward measurement operator is \(\mathcal{A}(x)=\operatorname{mag}(\mathcal{F}(M\odot x))\), where \(\operatorname{mag}(\cdot)\) computes the entrywise magnitude of a matrix with complex entries. The measurement noise is again set to be white Gaussian, with variance \(0.2\).

\[Q(\mathsf{pixel})=\begin{cases}1,&\text{with probability }\frac{e^{\text{pixel}/\theta}}{1+e^{\text{pixel}/ \theta}}\\ -1,&\text{with probability }\frac{1}{1+e^{\text{pixel}/\theta}},\end{cases}\]

where \(\text{pixel}\in[-1,1]\) is the value of each pixel in each channel. The measurements in quantized sensing are therefore one-bit-per-channel images. The dithering level \(\theta\) is set to \(0.4\) in our experiments.

### Sample images for other inverse problems

_Super-resolution._ The samples generated by different algorithms are shown in Table 5.

Across different tasks, linear and nonlinear, it can be seen that \(\mathsf{DPnP}\) has stronger capability of reconstructing the image with higher fidelity to the fine details.

### Additional performance metrics

_Computation time in terms of Neural Function Estimations (NFEs)._ In additional to the clock time statistics in the main text, we also measure the computational cost per sample of different algorithms in terms of the number of Neural Function Estimations, i.e., the number of calls to score functions. The results are in Table 6. Note that the NFEs for \(\mathsf{DPnP}\) depends on the initialization, the annealing schedule (and the number of timesteps for \(\mathsf{DPnP}\)-\(\mathsf{DDIM}\)). We provide typical numbers of NFEs with the choice of parameters given in Appendix H and with a suitable number of timesteps.

_Frechet Inception Distance (FID) and Structural Similarity Index Measure (SSIM)._ We also compare the FID and SSIM of different algorithms across different tasks. The results are shown in Table 7 and Table 8. It should be pointed out that FID is arguably not a very relevant notion to measure the quality of solving inverse problems, as accurately solving inverse problems means that the generated distribution is close to the _conditional_, i.e., _posterior_ distribution of the image, while FID only measure the closeness to the _unconditional_, i.e., _prior_ distribution of the image.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Algorithm & \(\mathsf{DPnP}\)-\(\mathsf{DDIM}\) & \(\mathsf{DPnP}\)-\(\mathsf{DDPM}\) & \(\mathsf{DPS}\) & \(\mathsf{LGD}\)-\(\mathsf{MC}\) & \(\mathsf{ReSample}\) \\ \hline NFEs & \(\sim 1500\) & \(\sim 3000\) & \(1000\) & \(1000\) & \(1000\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Number of NFEs for different algorithms.

\begin{table}
\begin{tabular}{l l l l l l} \hline \hline Input & \(\mathsf{DPS}\) & \(\mathsf{LGD}\)-\(\mathsf{MC}\) & \(\mathsf{ReSample}\) & \(\mathsf{DPnP}\)-\(\mathsf{DDPM}\) & \(\mathsf{DDIM}\) & \(\mathsf{Ground}\) \\ \hline \(\mathsf{DPnP}\) & \(\mathsf{DPnP}\) & \(\mathsf{DPnP}\) & \(\mathsf{DDIM}\) & \(\mathsf{Ground}\) \\ \hline \(\mathsf{DPnP}\) & \(\mathsf{DPnP}\) & \(\mathsf{DPnP}\) & \(\mathsf{DPnP}\) & \(\mathsf{Ground}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Samples of different algorithms for super-resolution (4x).

## Appendix H Ablation studies

### Initialization

In the Algorithm 1, the initial guess \(\widehat{x}_{0}\) is set to be a properly scaled Gaussian random vector. However, from Theorem 2 it can be infered that using a heuristic posterior sampler as the initializer could decrease \(\chi^{2}(p_{\widehat{x}_{1}}\parallel\pi_{\eta})\), hence potentially improve the convergence speed of DPnP. By using existing algorithms like DPS or LGD-MC as initializer, DPnP can improve upon the results of existing algorithms towards the correct posterior distribution efficiently and provably. In our experiments, we find it helpful to initialize DPnP with LGD-MC, which accelerates the algorithm significantly.

### Annealing schedule

We discuss the choice of the annealing schedule \(\eta_{k}\) in DPnP (Algorithm 1). As seen in the theoretical analysis (Theorem 2), if we set all the \(\eta_{k}\equiv\eta\) for some constant \(\eta>0\), then DPnP converges to a distribution \(\pi_{\eta}\), which can be regarded as a version of the posterior distribution \(p^{\star}(\cdot|y)\) distorted by an order of \(O(\eta)\). The smaller \(\eta\) is, the more accurate the final distribution will be. On the other hand, it was also seen that in many cases, the spectral gap is \(\Omega(\eta)\), hence the convergence time is \(O(\frac{1}{\eta})\). Therefore, smaller \(\eta\) would make it take longer to converge.4

Footnote 4: Strictly speaking, while the number of iterations required to converge increases as \(\eta\) gets smaller, the computational complexity per iteration will decrease. However, in experiments, we observed that the latter effect is not strong enough to compensate for the increase in overall complexity caused by the former.

To strike a balance between the accuracy and the convergence rate, we find it empirically successful to adapt an gradually decreasing schedule for \(\eta_{k}\), similar to [1]. In the first few iterations, we set \(\eta_{k}\) to be a large constant. After this initial phase, we decrease \(\eta_{k}\) slowly, eventually to \(\eta_{N}\) which is chosen to be a small constant. An example of such an annealing schedule is

\[\eta_{0} =\eta_{1}=\cdots=\eta_{K_{0}},\quad\eta_{0}>0\text{ is a large constant},\] \[\eta_{k} =(\eta_{K}/\eta_{0})^{\frac{k-K_{0}}{K-K_{0}}}\eta_{0},\quad K_{ 0}<k\leq K,\quad\eta_{K}>0\text{ a small constant},\]

where \(K_{0}<K\) is the length of the initial phase, which can be chosen as, e.g. \(K_{0}=K/5\). For all the numerical experiments, we set \(\eta_{0}=0.4\), \(\eta_{N}=0.15\), \(K_{0}=4\), \(K=20\).

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{Super-resolution} & \multicolumn{2}{c}{Phase retrieval} & \multicolumn{2}{c}{Quantized sensing} \\  & \multicolumn{2}{c}{(4x, linear)} & \multicolumn{2}{c}{(nonlinear)} & \multicolumn{2}{c}{(nonlinear)} \\ \cline{2-7} Algorithm & FID \(\downarrow\) & SSIM \(\uparrow\) & FID \(\downarrow\) & SSIM \(\uparrow\) & FID \(\downarrow\) & SSIM \(\uparrow\) \\ \hline DPnP-DDIM (ours) & \(\mathbf{36.3}\) & \(\mathbf{0.668}\) & \(\mathbf{46.5}\) & \(\mathbf{0.631}\) & \(\mathbf{37.3}\) & \(\mathbf{0.712}\) \\ DFS [2] & \(38.6\) & \(0.636\) & \(52.0\) & \(0.494\) & \(42.1\) & \(0.601\) \\ LGD-MC (\(n=5\)) [2] & \(36.8\) & \(0.651\) & \(82.3\) & \(0.414\) & \(40.3\) & \(0.639\) \\ ReSample (pixel-based) [2] & \(40.2\) & \(0.641\) & - & - & \(40.0\) & \(0.657\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: FID and SSIM of solving inverse problems on FFHQ \(256\times 256\) validation dataset (1k samples).

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{2}{c}{Super-resolution} & \multicolumn{2}{c}{Phase retrieval} & \multicolumn{2}{c}{Quantized sensing} \\  & \multicolumn{2}{c}{(4x, linear)} & \multicolumn{2}{c}{(nonlinear)} & \multicolumn{2}{c}{(nonlinear)} \\ \cline{2-7} Algorithm & FID \(\downarrow\) & SSIM \(\uparrow\) & FID \(\downarrow\) & SSIM \(\uparrow\) & FID \(\downarrow\) & SSIM \(\uparrow\) \\ \hline DPnP-DDIM (ours) & \(47.5\) & \(\mathbf{0.510}\) & \(\mathbf{73.5}\) & \(0.289\) & \(\mathbf{43.2}\) & \(\mathbf{0.623}\) \\ DPS [2] & \(61.4\) & \(0.496\) & \(92.7\) & \(\mathbf{0.318}\) & \(82.4\) & \(0.459\) \\ LGD-MC (\(n=5\)) [2] & \(\mathbf{46.2}\) & \(0.503\) & \(89.8\) & \(0.234\) & \(46.8\) & \(0.563\) \\ ReSample (pixel-based) [2] & \(68.3\) & \(0.427\) & - & - & \(53.7\) & \(0.491\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: FID and SSIM of solving inverse problems on ImageNet \(256\times 256\) validation dataset (1k samples).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction provides explicit description of the contributions and the context. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the scope of our work and provide ablation studies. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide clear statement of assumptions for our theory. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide algorithm tables and experimental setups with a reasonable level of detail. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: The code is still in preparation to meet publication standard. We promise to release it after this work is accepted. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experimental settings are given in detail. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The computational cost of reporting error bars is too high. Most of the closely related works do not include them. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All experiments are done on a single Nvidia L40 GPU. The time of execution is reported. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We fully honor the NeurIPS Code of Ethics in this work. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: While our provably robust algorithm may potentially improve the fieldity of diffusion-based methods in solving inverse problems, it awaits future research before anything special can be said about potential societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No data or models is released by this work. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All assets are credited properly. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.