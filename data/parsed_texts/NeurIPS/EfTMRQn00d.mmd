# Streamer: Streaming Representation Learning and Event Segmentation in a Hierarchical Manner

 Ramy Mounir  Sujal Vijayaraghavan  Sudeep Sarkar

Department of Computer Science and Engineering, University of South Florida, Tampa

{ramy, sujal, sarkar}@usf.edu

###### Abstract

We present a novel self-supervised approach for hierarchical representation learning and segmentation of perceptual inputs in a streaming fashion. Our research addresses how to semantically group streaming inputs into chunks at various levels of a hierarchy while simultaneously learning, for each chunk, robust global representations throughout the domain. To achieve this, we propose STREAMER, an architecture that is trained layer-by-layer, adapting to the complexity of the input domain. In our approach, each layer is trained with two primary objectives: making accurate predictions into the future and providing necessary information to other levels for achieving the same objective. The event hierarchy is constructed by detecting prediction error peaks at different levels, where a detected boundary triggers a bottom-up information flow. At an event boundary, the encoded representation of inputs at one layer becomes the input to a higher-level layer. Additionally, we design a communication module that facilitates top-down and bottom-up exchange of information during the prediction process. Notably, our model is fully self-supervised and trained in a streaming manner, enabling a single pass on the training data. This means that the model encounters each input only once and does not store the data. We evaluate the performance of our model on the egocentric EPIC-KITCHENS dataset, specifically focusing on temporal event segmentation. Furthermore, we conduct event retrieval experiments using the learned representations to demonstrate the high quality of our video event representations. Illustration videos and code are available on our project page: https://ramymounir.com/publications/streamer.

## 1 Computational theory

In temporal event analysis, an event is defined as "a segment in time that is perceived by an observer to have a _beginning_ and an _end_" [59]. Events could be described by a sequence of constituent events of relatively finer detail, thus forming a hierarchical structure. The end of an event and the beginning of the next is a _segmentation boundary_, marking an event transition. Segmentation boundaries in the lower levels of the hierarchy represent event transitions at relatively granular scales, whereas boundaries in higher levels denote higher-level event transitions.

We propose a structurally self-evolving model to learn the hierarchical representation of such events in a self-supervised streaming fashion through predictive learning. Structural evolution refers to the model's capability to create learnable layers _ad hoc_ during training. One may argue that existing deep learning architectures are compositional in nature, where high-level features are composed of lower-level features, forming a hierarchy of features. However, it is important to distinguish between a feature hierarchy and an event hierarchy: an event hierarchy is similar to a part/whole hierarchy in the sense that each event has clear boundaries that reflect the beginning and the end of a coherent chunk of information. One may also view the hierarchy as a redundancy pooling mechanism, whereinformation grouped as one event is considered redundant for a higher level and can be summarized into a single representation for higher-level processing.

Our model is capable of generating a hierarchy of event segments (Figure 1) by learning unique semantic representations for each event type directly from video frames. This is achieved through predictive learning, which models the causal structure of events. These learned representations are expressive enough to enable video snippet retrieval across videos. Each level in the hierarchy selectively groups inputs from the level below to form coherent event representations, which are then sent to the level above. As a result, the hierarchy exhibits temporally aligned boundaries, with each level containing a subset of the boundaries detected in the lower level.

As often prescribed [28; 24], we impose the following biologically-plausible constraints on our learning algorithm:

1. The learning algorithm should be **continuous and online**. Most existing learning algorithms offer batch-based offline learning. However, the learning in the neocortex occurs continuously in a streaming fashion while seeing each datapoint only once
2. The learning should involve the ability to make **high-order predictions** by "incorporating contextual information from the past. The network needs to dynamically determine how much temporal context is needed to make the best predictions" [24] (Section 1.1)
3. Learning algorithms should be **self-supervised** and should not assume labels for training [37]; instead, they should be able to figure out the learning objective from patterns and causal structures within the data
4. The learning should stem from a **universal general-purpose algorithm**. This is supported by observations of the brain circuitry showing that all neocortical regions are doing the same task in a repeated structure of cells [24]. Therefore, there should be no need for a global loss function (_i.e._, end-to-end training with high-level labels); local learning rules should suffice (Section 2)

### Predictive learning

Predictive learning refers to the brain's ability to generate predictions about future events based on past experiences. It is a fundamental process in human cognition that guides perception, action, and thought [58; 31]. The discrepancy between the brain's predictions and the observed perceptual

Figure 1: Comparison of STREAMR’s hierarchical output to single-level ground truth annotations from EPIC-KITCHENS. The ground truth contains redundant narrations for successive annotations (_e.g._, _add chicken_\(\blacksquare\), \(\blacksquare\)); STREAMER identifies such instances as a single high level event (\(\blacksquare\)). (Narrations from ground truth)

inputs forms a useful training signal for optimizing cortical functions: if a model can predict into the future, it implies that it has learned the underlying causal structure of the surrounding environment. Theories of cognition hypothesize that the brain only extracts and selects features from the previous context that help in minimizing future prediction errors, thus making the sensory cortex optimized for prediction of future input [51]. A measure of intelligence can be formulated as the ability of a model to generate accurate, long-range future prediction [53].To this end, we design an architecture with the main goal of minimizing the prediction error, also referred to as maximizing the _model evidence_ in Bayesian inference according to the free energy principle [19; 18].

Event segmentation theory (EST) suggests that desirable properties such as event segmentation emerge as a byproduct of minimizing the prediction loss [59]. Humans are capable of _chunking_ streaming perceptual inputs into events (and _chunking_ spatial regions into objects [14]) to allow for memory consolidation and event retrieval for better future predictions. EST breaks down streaming sensory input into chunks by detecting event boundaries as transient peaks in the prediction error. The detected boundaries trigger a process of transitioning (_i.e._, shifting) to a new event model whereby the current event model is saved in the event schemata, and a different event model is retrieved, or a new one initialized to better explain the new observations. One challenge in implementing a computational model of EST is encoding long-range dependencies from the previous context to allow for contextualized representations and accurate predictions. To address this challenge, we construct a hierarchy of event models operating at different time-scales, predicting future events with varying granularity. This hierarchical structure enables the prediction function at any layer to extract context information dynamically from any other layer, enhancing prediction during inference (learning constraint 2). Recent approaches [42; 40; 41; 55] inspired by EST have focused on event boundary detection using predictive learning. However, these methods typically train a single level and do not support higher-order predictions.

### Hierarchical event models

A single-level predictive model considers events that occur only at a single level of granularity rendering them unable to encode long-range, higher-order causal relationships in complex events. Conversely, a high-level representation does not contain the level of detail needed for accurately predicting low-level actions; it only encodes a high-level conceptual understanding of the action. Therefore, a hierarchy of event models is necessary to make predictions accurately at different levels of granularity [37; 26]. It is necessary to continuously predict future events at different levels of granularity, where low-level event models encode highly detailed information to perform short-term prediction and high-level event models encode conceptual low-detail features to perform long-term prediction.

EST identifies event boundaries based on transient peaks in the prediction error. To learn a hierarchical structure, we extend EST: we use event models at the boundaries in a layer as inputs to the layer above. The prediction error of each layer determines event demarcation, regulating the number of inputs pooled and sent to the layer above. This enables dynamic access to long-range context for short-term prediction, as required. This setup results in stacked predictive layers that perform the same prediction process with varying timescales subjective to their internal representations.

### Cross-layer communication

As noted in Section 1.2, coarsely detailed long-range contexts come from higher layers (the top-left block of Figure 2), and highly detailed short-range contexts come from lower layers (the bottom-left block of Figure 2), both of which are crucial to predict future events accurately. Therefore, the prediction at each layer should be conditioned upon its own representation and those of the other layers (Equation (2)). These two types of contexts can be derived by minimizing the prediction error at different layers. Hence, making perfect predictions is not the primary goal of this model but rather continuously improving its overall predictive capability.

#### 1.3.1 Contextualized inference

A major challenge in current architectures is modeling long-range temporal dependencies between inputs. Most research has focused on modifying recurrent networks [27; 10] or extending the sequence length of transformers [11; 57] to mitigate the problem of vanishing and exploding gradientsin long-range backpropagation through time [44]. Instead, we solve this problem by allowing the multi-level context representations to be shared across layers during inference. It is worth noting that this type of inference is rarely used in typical deep learning paradigms, where the top-down influence only comes from backpropagating a supervised loss signal (_i.e._, top-down optimization). Biologically-inspired architectures such as PredNet [39] utilize top-down inference connections to improve low-level predictions (_i.e._, frames); however, these predictive coding architectures send the prediction error signal from each low-level observation (_i.e._, each frame) to higher levels which prevents the network from explicitly building hierarchical levels with varying degrees of context granularity.

#### 1.3.2 Contextualized optimization

Contextualized inference improves prediction, which is crucial for event boundary detection. However, we also aim to learn rich, meaningful representations. In Section 1, we noted that a 'parent' event could consist of multiple interchangeable low-level events. For instance, making a sandwich can involve spreading butter or adding cheese. From a high-level, using either ingredient amounts to the same parent event: "making a sandwich". Despite their visual differences, the prediction network must embed meaning and learn semantic similarities between these low-level events (_i.e._, spreading butter and adding cheese).

We implement this through "contextualized optimization" of events (Section 2.2), where each layer aligns the input representations from the lower level to minimize its own prediction loss using its context. It must be noted that the contextualization from higher layers (Figure 2, bottom-right) is balanced by the predictive inference at the lower levels (Figure 2, top-right), which visually distinguishes the interchangeable events. This balance of optimization embeds meaningful representations into the distinct low-level representations without collapsing the model. These representations can also be utilized for event retrieval at different hierarchical levels (Figure 5). Unlike other representation learning frameworks that employ techniques like exponential moving average (EMA) or asymmetric branches to prevent model collapse [7; 21; 9], we ensure that higher layers remain grounded in predicting lower-level inputs through bottom-up optimization. 1

Footnote 1: A more detailed literature survey is in the Appendix

## 2 Algorithm

Our goal is to incrementally build a stack of identical layers over the course of the learning, where each layer communicates with the layers above and below it. The layers are created as needed and are trained to function at different timescales; the output events from layer \(l\) become the inputs to the layer \((l+1)\), as illustrated in Figure 3. We describe the model and its connections for a single layer \(l\)

Figure 2: Given a stream of inputs at any layer, our model combines them and generates a bottleneck representation, which becomes the input to the level above it. The cross-layer communication could be broken down into top-down and bottom-up contextualized inference (left) and optimization (right).

but the same structure applies to all the layers in the model (learning constraint 4). In what follows, we describe the design of a mathematical model for a single predictive layer that is capable of (1) encoding temporal input into unique semantic representations (the _event model_) contextualized by previous events, (2) predicting the location of event boundaries (event demarcation), and (3) allowing for communication with other existing layers in the prediction stack to minimize its own prediction loss.

### Temporal encoding

Let \(\mathcal{X}^{(l)}=\{\bm{x}_{(t-m)}^{(l)},\ldots,\bm{x}_{t}^{(l)}\}\) be a set of \(m\) inputs to a layer \(l\) at discrete time steps in the range \((t-m,t]\) where each input \(\bm{x}_{i}\in\mathbb{R}^{d}\). First, we aim to generate an "_event model_"2\(\bm{z}^{(l)}\) which is a single bottleneck representation of the given inputs \(\mathcal{X}^{(l)}\). To accomplish this, we define a function \(f^{(l)}:\mathbb{R}^{m\times d}\mapsto\mathbb{R}^{d}\) with temporally shared learnable weights \(\bm{\Phi}^{(l)}\) to evaluate the importance of each input in \(\mathcal{X}^{(l)}\) for solving the prediction task at hand, as expressed in Equation (1).

Footnote 2: We use ‘event model’ and ‘representation’ interchangeably; terminologies defined in the appendix.

\[\bm{z}^{(l)}=f^{(l)}(\mathcal{X}^{(l)};\bm{\Phi}^{(l)})\] (1)

This event model will be trained to extract information from \(\mathcal{X}^{(l)}\) that is helpful for hierarchical prediction. Ideally, the bottleneck representation should encode top-down semantics, which allow for event retrieval and a bottom-up interpretation of the input to minimize the prediction loss of the following input. The following subsection describes the learning objective to accomplish this encoding task.

### Temporal prediction

At the core of our architecture is the prediction block, which serves two purposes: event demarcation and cross-layer communication. As previously mentioned, our architecture is built on the premise that minimizing the prediction loss is the only needed objective function for hierarchical event segmentation and representation learning.

Cross-layer communicationallows the representation \(\bm{z}^{(l)}\) to utilize information from higher \(\{\bm{z}^{(l+1)},\ldots,\bm{z}^{(L)}\}\) and lower layers \(\{\bm{z}^{(1)},\ldots,\bm{z}^{(l-1)}\}\) when predicting the next input at layer \(l\)

Figure 3: A diagram illustrating information flow across stacked identical layers. Each layer compares its prediction \(\hat{\bm{x}}_{t}\) with the input \(\bm{x}_{t}\) received from the layer below. If the prediction error \(\mathcal{L}_{\text{pred}}\) is over a threshold \(\mu_{t-1}\), the current representation \(\bm{z}_{t-1}\) becomes the input to the layer above, and the working set is reset with \(\bm{x}_{t}\); otherwise, \(\bm{x}_{t}\) is appended to the working set \(\mathcal{X}_{t}\)

where \(L\) is the total number of layers. Let \(\mathcal{Z}_{t}=\{\bm{z}_{t}^{(1)},\ldots,\bm{z}_{t}^{(L)}\}\) be a set of event models where each element is the output of the temporal encoding function \(f\) at its corresponding layer as expressed in Equation (1). Note that the same time variable \(t\) is used for representation across layers for simplicity; however, each layer operates in its own subjective timescale. Let \(p^{(l)}:\mathbb{R}^{L\times d}\mapsto\mathbb{R}^{d}\) be a function of \(\mathcal{Z}\) to predict the next input at layer \(l\) as expressed in Equation (2)

\[\hat{\bm{x}}_{t+1}^{(l)}=p^{(l)}(\mathcal{Z}_{t};\bm{\Psi}^{(l)})\] (2)

where \(\bm{\Psi}^{(l)}\) denotes the learnable parameters of the predictor at layer \(l\). The difference between the layer's prediction \(\hat{\bm{x}}_{t+1}^{(l)}\) and the actual input \(\bm{x}_{t+1}^{(l)}\) is minimized, allowing the gradients to flow back into the \(f^{(\cdot)}\) functions to modify each layer's representation as expressed in Equation (3).

\[\begin{split}\mathcal{L}_{\text{ped}}(\hat{\bm{x}}_{t+1}^{(l)}, \bm{x}_{t+1}^{(l)})&=\hat{\bm{x}}_{t+1}^{(l)}\sim\bm{x}_{t+1}^{(l )}\\ \bm{\Phi}^{*(i)},\bm{\Psi}^{*(l)}&\leftarrow\operatorname* {arg\,min}_{\bm{\Phi}^{(i)},\bm{\Psi}^{(l)}}\mathcal{L}_{\text{ped}}(\hat{\bm {x}}_{t+1}^{(l)},\bm{x}_{t+1}^{(l)})\qquad\forall\;i\in\{1\ldots L\}\end{split}\] (3)

The symbol \(\sim\) represents an appropriate distance measure between two vectors.

Event demarcationis the process of detecting event boundaries by using the prediction loss, \(\mathcal{L}_{\text{ped}}\), from Equation (3). As noted earlier, according to EST, when a boundary is detected, an event model transition occurs, and a new event model is used to explain the previously unpredictable observations. Instead of saving the event model to the event schemata at boundary locations as described in EST, we use it as a detached input (denoted by \(\operatorname{sg}[\cdot]\)) to train the predictive model of the layer above it (_i.e.,_\(\bm{x}^{(l+1)}\equiv\operatorname{sg}[\bm{z}^{(l)}]\)). We compute the running average of the prediction loss with a window of size \(w\), expressed by Equation (5), and assume that a boundary is detected when the new prediction loss is higher than the smoothed prediction loss, as expressed by the decision function in Equation (4).

\[\delta(\bm{x}_{t}^{(l)};\mu_{t-1}^{(l)})=\begin{cases}1&\text{if }\mathcal{L}_{ \text{ped}}(\hat{\bm{x}}_{t}^{(l)},\bm{x}_{t}^{(l)})>\mu_{t-1}^{(l)}\\ 0&\text{otherwise}\end{cases}\] (4)

where the running average is given by

\[\mu_{t}^{(l)}=\frac{1}{w}\sum_{i=t-w}^{t}\mathcal{L}_{\text{pred}}(\hat{\bm{x }}_{i}^{(l)},\bm{x}_{i}^{(l)})\] (5)

### Hierarchical gradient normalization

It is necessary to scale the gradients differently from conventional gradient updates because of the hierarchical nature of the model and its learning based on dynamic temporal contexts. There are three variables influencing the amount of accumulation of gradients:

1. The **relative timescale** between each layer is determined by the number of inputs. For instance, let the event encoder in layer \((l-2)\) have seen \(a=|\mathcal{X}^{(l-2)}|\) inputs, that at layer \((l-1)\) have seen \(b=|\mathcal{X}^{(l-1)}|\) inputs, and that at \(l\) have seen \(c=|\mathcal{X}^{(l)}|\) inputs. Then the input to layer \(l\) is a result of seeing a total of \((a\cdot b\cdot c)\). This term can then be used to scale up the learning at any level \(l\), expressed as \(\prod_{i=1}^{l}|\mathcal{X}^{(i)}|\).
2. The **reach of influence** of each level's representation on a given level's encoder is influenced by its distance from another. For instance, if the input to \(f^{(l)}\) comes from the levels \(\{l+2,l+1,l,l-1,l-2\}\), then the weight of learning should be centered at \(l\) and diminish as the distance increases. Such a weight at any level \(l\) is given by \(\alpha^{-|l-r|}\,\forall\,r\in\{-2,-1,0,1,2\}\). To ensure that the learning values sum up to 1 when this scaling is applied, the weights are normalized to add up to 1 as \(\frac{\alpha^{-|l-r|}}{\sum_{i=1}^{l}\alpha^{-|l-i|}}\).
3. The encoder receives **accumulated feedback** from predictors of all the layers; therefore the change in prediction loss with respect to encoder parameters in any given layer should be normalized by the total number of layers, given by \(\frac{1}{L}\).

The temporal encoding model can be learned by scaling its gradients as expressed by the scaled Jacobian \(\bm{J}^{\prime}_{\mathcal{L}}(\bm{\Phi})\) in Equation (6).

\[\bm{J}^{\prime}_{\mathcal{L}}(\bm{\Phi})=\bm{C}\circ\bm{J}_{\mathcal{L}}(\bm{ \Phi})=\begin{bmatrix}c_{1,1}\frac{\partial\mathcal{L}^{(1)}}{\partial\bm{\Phi} ^{(1)}}&\dots&c_{1,L}\frac{\partial\mathcal{L}^{(1)}}{\partial\bm{\Phi}^{(L)} }\\ \vdots&\ddots&\vdots\\ c_{L,1}\frac{\partial\mathcal{L}^{(L)}}{\partial\bm{\Phi}^{(1)}}&\dots&c_{L,L} \frac{\partial\mathcal{L}^{(L)}}{\partial\bm{\Phi}^{(L)}}\end{bmatrix}\] (6)

where

\[c_{l,r}=\underbrace{\frac{1}{L}}_{\text{feedback}}\cdot\underbrace{\frac{ \alpha^{-|l-r|}}{\sum_{i=1}^{L}\alpha^{-|l-i|}}}_{\text{reach of influence}}\cdot\underbrace{\prod_{i=1}^{l}|\mathcal{X}^{(i)}|}_{\text{ timescale}}\] (7)

Similarly, the temporal prediction model's gradients are controlled with scaling factors as expressed in Equation (8).

\[\bm{J}^{\prime}_{\mathcal{L}}(\bm{\Psi})=\bm{S}\circ\bm{J}_{\mathcal{L}}(\bm {\Psi})=\begin{bmatrix}s_{1}\frac{\partial\mathcal{L}^{(1)}}{\partial\bm{\Psi} ^{(1)}}&\dots&s_{L}\frac{\partial\mathcal{L}^{(L)}}{\partial\bm{\Psi}^{(L)}} \end{bmatrix}\] (8)

where

\[s_{l}=\underbrace{\frac{1}{\sum_{i=1}^{L}\alpha^{-|l-i|}}}_{\text{reach of influence}}\cdot\underbrace{\prod_{i=1}^{l}|\mathcal{X}^{(i)}|}_{\text{ timescale}}\] (9)

## 3 Implementation

### Training details

We resize video frames to \(128\times 128\times 3\) and use a 4-layer CNN autoencoder (only for the first level) to project every frame to a single feature vector of dimension \(1024\) for temporal processing. For predictive-based models (STREAMER and LSTM+AL), we sample frames at 2 fps, whereas for clustering-based models, we use a higher sampling rate (5 fps) to reduce noise during clustering. We use cosine similarity as the distance measure (\(\sim\)) and use the Adam optimizer with a constant learning rate of \(1e-4\) for training. We do not use batch normalization, regularization (_i.e._, dropout, weight decay), learning rate schedule, or data augmentation during training. We use transformer encoder architecture for functions \(f\) and \(p\); however, ablations show different architectural choices. A window size \(w\) of 50 inputs (timescale respective) is used to compute the running average in Equation 5, and a new layer \((l+1)\) is added to the stack after layer \((l)\) has processed 50K inputs.

### Delayed gradient stepping and distributed learning

Unlike our proposed approach, conventional deep learning networks do not utilize high-level outputs in the intermediate-level predictions. Since our model includes a top-down inference component, such that a lower level (_e.g._, \((l)\)) backpropagates its loss gradients into the temporal encoding functions of a higher level (_e.g._, \(f^{(>l)}\)), we cannot apply the gradients immediately after loss calculation at layer \((l)\). Therefore, we allow for scaled (_i.e._, Section 2.3 and Equation (8)) gradients to accumulate at all layers, then perform a single gradient step when the highest layer \(L\) backpropagates its loss.

In our streaming hierarchical learning approach, event demarcation is based on the data (_i.e._, some events are longer than others), posing a challenge for traditional parallelization schemes. We cannot directly batch events as inputs because each layer operates on a different subjective timeline. Therefore, each model is trained separately on a single stream of video data, and the models' parameters are averaged periodically during training. We train eight parallel streams on different sets of videos and average the models' parameters every 1K frames.

### Datasets and comparisons

In our training and evaluation, we use two large-scale egocentric datasets: Ego4D [12] and EPIC-KITCHENS 100 [20]. Ego4D is a collection of videos with a total of 3670 hours of daily-life activity collected from 74 worldwide locations. EPIC-KITCHENS 100 contains 100 hours of egocentric video, 90K action segments, and 20K unique narrations. We train our model in a self-supervised layer-by-layer manner (using only RGB frames and inputting them exactly once) on a random 20% subset of Ego4D and 80% of EPIC-KITCHENS, then evaluate on the rest 20% of EPIC-KITCHENS. We define two evaluation protocols: Protocol 1 divides EPIC-KITCHENS such that the 20% test split comes from kitchens that have not been seen in the training set, whereas Protocol 2 ensures that the kitchens in the test set are also in the training set.

We compare our method with TW-FINCH [48], Offline ABD [16], Online ABD [16], and LSTM+AL [1]. ABD, to the best of our knowledge, is the state of the art in unsupervised event segmentation. Clustering-based event segmentation models do not evaluate on egocentric datasets due to the challenges of camera motion and noise. Most clustering based-approaches use pre-trained or optical-flow-based features, which are not effective when clustered in an egocentric setting. We re-implement ABD due to the unavailability of official code and use available official implementations for the other methods.

### Evaluation metrics and protocols

Event segmentationThe 20K unique _narration_s in EPIC-KITCHENS include different labels referring to the same actions (_e.g._, turn tap on, turn on tap); therefore we cannot evaluate the labeling performance of the model. We follow the protocol of LSTM+AL [1] to calculate the Jaccard index (IoU) and mean over frames (MoF) of the one-to-one mapping between the ground truth and predicted segments. Unlike LSTM+AL [1], which uses Hungarian matching to find the one-to-one mapping, we design a generalized recursive algorithm called Hierarchical Level Reduction (HLR) which finds a one-to-one mapping between the ground truth events and (a single-layer or multi-layer hierarchical) predicted events. A detailed explanation of the algorithm can be found in Supplementary Material.

Representation qualityTo assess the quality of the learned representations, we use the large language model (LLM) GPT 3.5 to first create a dataset of events labels ranked by semantic similarity according to the LLM. In particular, we generate 1K data points sampled from EPIC-KITCHENS, where each data point comprises a 'query' narration and a set of 10 'key' narrations, and each key is ranked by its similarity to the query. We then retrieve the features for each event in the comparison and compute the appropriate vector similarity measure, and accordingly rank each key event. This rank list is then compared with the LLM ranking to report the MSE and the Levenshtein edit distance between them. Examples of LLM similarity rankings are available in Supplementary Material.

### Experiments

We evaluate STREAMER's performance of event segmentation and compare it with streaming and clustering-based SOTA methods as shown in Table 1. Our findings show that the performance of a single layer in STREAMER's hierarchy (the best-performing layer out of three per video) and the

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multirow{2}{*}{**Backbone**} & \multirow{2}{*}{**Pretrained**} & \multirow{2}{*}{**Layers**} & \multicolumn{2}{c}{**Protocol 1**} & \multicolumn{2}{c}{**Protocol 2**} \\ \cline{5-8}  & & & & **MoF \(\uparrow\)** & **IoU**\(\uparrow\) & **MoF**\(\uparrow\) & **IoU**\(\uparrow\) \\ \hline LSTM+AL [1] & VGG16 [50] & ImageNet [46] & 1 & 0.694 & 0.417 & 0.659 & 0.442 \\ \hline TW-FINCH [47] & \multirow{3}{*}{**MTRN [60]**} & \multirow{3}{*}{**EPIC 50 [12]**} & 1 & 0.707 & 0.443 & 0.692 & 0.442 \\ Offline ABD [16] & & & 1 & 0.704 & 0.438 & 0.699 & 0.432 \\ Online ABD [16] & & & 1 & 0.610 & 0.496 & 0.605 & 0.487 \\ \hline STREAMER & 4-layer CNN & - & 1 & **0.759** & **0.508** & **0.754** & **0.489** \\  & & & 3 & **0.736** & **0.511** & **0.729** & **0.494** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Event segmentation comparison of MoF and average IoU, evaluated on EPIC-KITCHENS. None of the methods listed below requires labels.3 The column ‘Layers’ refers to the number of layers evaluated against the ground truth: 1 reports the performance of the best layer in the prediction hierarchy, whereas 3 uses the proposed Hierarchical Level Reduction algorithm for evaluation.

full 3-layer hierarchy outperform all other state of the art using IoU and MoF metrics on both testing protocols. It is worth noting that all the other methods use a large CNN backbone with supervised pre-trained weights (some on the same test dataset: EPIC-KITCHENS), whereas our model is trained from scratch using random initialization with a simple 4-layer CNN. We show comparative qualitative results in Figure 4. More qualitative results are provided in Supplementary Material.

Additionally, we evaluate the quality of event representation in Table 2. We show that self-supervising STREAMER from randomly initialized weights outperforms most clustering-based approaches with pre-trained weights; we perform on par with TW-FINCH when using supervised EPIC-KITCHENS pre-trained weights. Qualitative results of retrieval for the first three nearest neighbors on all the events in the test split are shown in Figure 5. More qualitative results are reported in Supplementary Material.

AblationsWe investigate three main aspects of STREAMER (Table 3): (1) varying the architecture of the temporal encoding model \(f\), (2) varying the predictor function \(p\), and (3) experimenting with the'reach of influence' parameter \(\alpha\) in Equation 7. Our findings suggest that STREAMER is robust to different architectural choices of \(f\). Our experiments also illustrate the importance of the cross-layer communication of \(p\): simply taking the average of \(\mathcal{Z}\) as the prediction performs worse than applying a layer-specific MLP to the average; using a transformer to retrieve context from other layers dynamically performs the best. Finally, adjusting the reach of influence by gradient scaling improves the segmentation performance.

To determine the quality of the backbone features learned by STREAMER, we run ablations of using our 4-layer pretrained CNN features on SoTA clustering methods. The results, plotted in Figure 6, show significant improvement in the average mean over frames (MoF) performance of

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Method** & **Weights** & **MSE \(\downarrow\)** & **LD \(\downarrow\)** \\ \hline \multicolumn{4}{c}{**Supervised**} \\ \hline \multirow{2}{*}{TW-FINCH} & EPIC & 1.00 & **0.67** \\ \cline{2-4}  & IN & 1.018 & 0.710 \\ \hline \multirow{2}{*}{Offline ABD} & EPIC & 1.02 & 0.71 \\ \cline{2-4}  & IN & 1.005 & 0.708 \\ \hline \multirow{2}{*}{Online ABD} & EPIC & 1.00 & 0.70 \\ \cline{2-4}  & IN & 1.039 & 0.704 \\ \hline \multicolumn{4}{c}{**No supervision**} \\ \hline STREAMER & - & **0.967** & 0.695 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Retrieval evaluation based on MSE and the Levenshtein edit distance (LD) of the features. All experiments are on EPIC-KITCHENS.3

Figure 4: Qualitative comparisons of event segmentation. The Gantt chart shows a more accurate alignment of STREAMER’s predictions with the ground truth compared to other methods.

event segmentation on the EPIC-KITCHENS dataset. This improvement could be attributed to the robust representations learned by the encoder through hierarchical predictive learning. In particular, since these features are learned through top-down optimization, the CNN backbone is able to predict longer events at higher levels, thus improving the features and contextualization quality.

## 4 Conclusion

In conclusion, we present STREAMER, a self-supervised and structurally evolving hierarchical temporal segmentation model that is shown to perform well on egocentric videos and is robust to hyperparameter variations. The learned representations are hierarchical in nature, representing events at different levels of granularity and semantics. As part of this, we design a gradient scaling mechanism necessary for such hierarchical frameworks with varying time-scales.

STREAMER adheres to several biologically-inspired constraints and exhibits the ability to process long previous contexts in a streaming manner, seeing each input exactly once. Our method is designed to be trained in a streaming manner which allows models to perform inference simultaneously during training, appealing to applications that require real-time adaptability [32]. We demonstrate its ability to perform event segmentation on large egocentric videos of varying perceptual conditions and demonstrate the quality of the representations through event retrieval and similarity ranking experiments.

**Broader impact and limitations** STREAMER requires large amounts of data to model complex high-level causal structures, and the training time increases as a layer is added. However, as self-supervised learning is becoming of increasing essence, new models must be able to continually learn from large, unlabeled data from constantly evolving domains. STREAMER caters to such online learning paradigms by fully exploiting large unlabelled video data. A much broader impact of this method extends to multi-modal data and domains beyond egocentric videos.

## Acknowledgements

This research was supported by the US National Science Foundation Grants CNS 1513126 and IIS 1956050. The authors would like to thank Margrate Selwaness for her help with results visualizations.

## References

* [1]S. N. Aakur and S. Sarkar (2019) A perceptual prediction framework for self supervised event segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1197-1206. Cited by: SS1.
* [2]H. Ahn and D. Lee (2021) Refining action segmentation with hierarchical video representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16302-16310. Cited by: SS1.
* [3]J. A. Alayrac, P. Bojanowski, N. Agrawal, J. Sivic, I. Laptev, and S. Lacoste-Julien (2016) Unsupervised learning from narrated instruction videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4575-4583. Cited by: SS1.
* [4]P. Bojanowski, R. Lajugie, F. Bach, I. Laptev, J. Ponce, C. Schmid, and J. Sivic (2014) Weakly supervised action labeling in videos under ordering constraints. In Computer Vision-ECCV 2014-13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 628-643. Cited by: SS1.
* [5]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* [6]M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin (2020) Unsupervised learning of visual features by contrasting cluster assignments. Advances in neural information processing systems33, pp. 9912-9924. Cited by: SS1.
* [7]M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin (2021) Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9650-9660. Cited by: SS1.
* [8]C. Caucheteux, A. Gramfort, and J. King (2023) Evidence of a predictive coding hierarchy in the human brain listening to speech. Nature Human Behaviour, pp. 1-12. Cited by: SS1.
* [9]X. Chen and K. He (2021) Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 15750-15758. Cited by: SS1.
* [10]K. Cho, B. V. Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio (2014) Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078. Cited by: SS1.
* [11]Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov (2019) Transformer-xl: attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860. Cited by: SS1.
* [12]D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price, et al. (2018) Scaling egocentric vision: the epic-kitchens dataset. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 720-736. Cited by: SS1.
* [13]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018) Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Cited by: SS1.
* [14]J. J. DiCarlo and D. D. Cox (2007) Untangling invariant object recognition. Trends in cognitive sciences11 (8), pp. 333-341. Cited by: SS1.
* [15]L. Ding and C. Xu (2018) Weakly-supervised action segmentation with iterative soft boundary assignment. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6508-6516. Cited by: SS1.
* [16]Z. Du, X. Wang, G. Zhou, and Q. Wang (2022) Fast and unsupervised action boundary detection for action segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3323-3332. Cited by: SS1.

* [17] Yazan Abu Farha and Jurgen Gall. Ms-tcn: Multi-stage temporal convolutional network for action segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 3575-3584, 2019.
* [18] Karl Friston. The free-energy principle: a unified brain theory? _Nature reviews neuroscience_, 11(2):127-138, 2010.
* [19] Karl Friston and Stefan Kiebel. Predictive coding under the free-energy principle. _Philosophical transactions of the Royal Society B: Biological sciences_, 364(1521):1211-1221, 2009.
* [20] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18995-19012, 2022.
* [21] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.
* [22] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In _Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops_, pages 0-0, 2019.
* [23] Tengda Han, Weidi Xie, and Andrew Zisserman. Memory-augmented dense predictive coding for video representation learning. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16_, pages 312-329. Springer, 2020.
* [24] Jeff Hawkins and Subutai Ahmad. Why neurons have thousands of synapses, a theory of sequence memory in neocortex. _Frontiers in neural circuits_, page 23, 2016.
* [25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16000-16009, 2022.
* [26] Geoffrey Hinton. How to represent part-whole hierarchies in a neural network. _Neural Computation_, pages 1-40, 2022.
* [27] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* [28] Kjell Jorgen Hole and Subutai Ahmad. A thousand brains: toward biologically constrained ai. _SN Applied Sciences_, 3(8):743, 2021.
* [29] De-An Huang, Li Fei-Fei, and Juan Carlos Niebles. Connectionist temporal modeling for weakly supervised action labeling. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 137-153. Springer, 2016.
* [30] Yuchi Ishikawa, Seito Kasai, Yoshimitsu Aoki, and Hirokatsu Kataoka. Alleviating over-segmentation errors by detecting action boundaries. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pages 2322-2331, 2021.
* [31] Moritz Koster, Ezgi Kayhan, Miriam Langeloh, and Stefanie Hoehl. Making sense of the world: infant learning from a predictive processing perspective. _Perspectives on psychological science_, 15(3):562-571, 2020.
* [32] Sonu Shreshtha Kshitiz, Ramy Mounir, Mayank Vatsa, Richa Singh, Saket Anand, Sudeep Sarkar, and Sevaram Mali Parihar. Long-term monitoring of bird flocks in the wild.
* [33] Anna Kukleva, Hilde Kuehne, Fadime Sener, and Jurgen Gall. Unsupervised learning of action classes with continuous temporal embedding. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12066-12074, 2019.
* [34] Sateesh Kumar, Sanjay Haresh, Awais Ahmed, Andrey Konin, M Zeeshan Zia, and Quoc-Huy Tran. Unsupervised action segmentation by joint representation learning and online clustering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 20174-20185, 2022.
* [35] Colin Lea, Michael D Flynn, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional networks for action segmentation and detection. In _proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 156-165, 2017.

* [36] Colin Lea, Austin Reiter, Rene Vidal, and Gregory D Hager. Segmental spatiotemporal cnns for fine-grained action segmentation. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14_, pages 36-52. Springer, 2016.
* [37] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. _Open Review_, 62, 2022.
* [38] William Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video prediction and unsupervised learning. _arXiv preprint arXiv:1605.08104_, 2016.
* [39] William Lotter, Gabriel Kreiman, and David D. Cox. Deep predictive coding networks for video prediction and unsupervised learning. In _ICLR (Poster)_. OpenReview.net, 2017.
* [40] Ramy Mounir, Sathyanarayanan Aakur, and Sudeep Sarkar. Self-supervised temporal event segmentation inspired by cognitive theories. In _Advanced Methods and Deep Learning in Computer Vision_, pages 405-448. Elsevier, 2022.
* [41] Ramy Mounir, Roman Gula, Jorn Theuerkauf, and Sudeep Sarkar. Spatio-temporal event segmentation for wildlife extended videos. In _International Conference on Computer Vision and Image Processing_, pages 48-59. Springer, 2021.
* [42] Ramy Mounir, Ahmed Shahabaz, Roman Gula, Jorn Theuerkauf, and Sudeep Sarkar. Towards automated ethogramming: Cognitively-inspired event segmentation for streaming wildlife video monitoring. _International Journal of Computer Vision_, pages 1-31, 2023.
* [43] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.
* [44] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In _International conference on machine learning_, pages 1310-1318. Pmlr, 2013.
* [45] Alexander Richard, Hilde Kuehne, and Juergen Gall. Weakly supervised action learning with rnn based fine-to-coarse modeling. In _Proceedings of the IEEE conference on Computer Vision and Pattern Recognition_, pages 754-763, 2017.
* [46] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115:211-252, 2015.
* [47] Saquib Sarfraz, Naila Murray, Vivek Sharma, Ali Diba, Luc Van Gool, and Rainer Stiefelhagen. Temporally-weighted hierarchical clustering for unsupervised action segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11225-11234, 2021.
* [48] Saquib Sarfraz, Vivek Sharma, and Rainer Stiefelhagen. Efficient parameter-free clustering using first neighbor relations. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8934-8943, 2019.
* [49] Mike Zheng Shou, Stan Weixian Lei, Weiyao Wang, Deepti Ghadiyaram, and Matt Feiszli. Generic event boundary detection: A benchmark for event segmentation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8075-8084, 2021.
* [50] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* [51] Yosef Singer, Yayoi Teramoto, Ben DB Willmore, Jan WH Schnupp, Andrew J King, and Nicol S Harper. Sensory cortex is optimized for prediction of future input. _elife_, 7:e31557, 2018.
* [52] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. _arXiv preprint arXiv:2201.08239_, 2022.
* [53] Trond A Tjostheim and Andreas Stephens. Intelligence as accurate prediction. _Review of Philosophy and Psychology_, pages 1-25, 2021.
* [54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.

* [55] Xiao Wang, Jingen Liu, Tao Mei, and Jiebo Luo. Coseg: Cognitively inspired unsupervised generic event segmentation. _IEEE Transactions on Neural Networks and Learning Systems_, 2023.
* [56] Shuchen Wu, Noemi Elteto, Ishita Dasgupta, and Eric Schulz. Learning structure from the ground up--hierarchical representation learning by chunking. _Advances in Neural Information Processing Systems_, 35:36706-36721, 2022.
* [57] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. _Advances in neural information processing systems_, 32, 2019.
* [58] Daniel Yon, Cecilia Heyes, and Clare Press. Beliefs and desires in the predictive brain. _Nature Communications_, 11(1):4404, 2020.
* [59] Jeffrey M Zacks, Nicole K Speer, Khena M Swallow, Todd S Braver, and Jeremy R Reynolds. Event perception: a mind-brain perspective. _Psychological bulletin_, 133(2):273, 2007.
* [60] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in videos. In _Proceedings of the European conference on computer vision (ECCV)_, pages 803-818, 2018.

**Supplementary Material**

## Appendix A Hierarchical level reduction

Since the ground truth annotations are provided as a single level of event annotations, it is not possible to compare them with rich hierarchical event segmentation predicted by STREAMER. For a given video and its ground truth annotations and the predicted annotations, several one-to-one mappings between them exist; we desire to find the one with the highest average IoU. In addition, it is necessary to ensure that the resulting one-to-one mapping does not contain temporally overlapping predicted annotations.

To solve this optimization problem, we design a hierarchical level reduction (HLR) algorithm that reduces multiple layers of hierarchical events down to a single layer by selecting prediction events that maximize IoU with the ground truth while ensuring no overlap of events during reduction. We design HLR as a recursive greedy optimization strategy. At each recursive level of Algorithm 1, multiple ground truth events are competing to be assigned to a single predicted event, so HLR returns the best of the two options (line 18): (1) assigning the event to the ground truth with the highest IoU, and (2) averaging the outputs of same recursive function over all children of the predicted event.

```
1:\(\mathcal{A}^{L}\): a list of predicted events at the highest level \(L\)
2:\(\mathcal{G}\): a list of ground truth event annotations
3:
4:The overall IoU of the resulting hierarchy reduction
5:procedureFindMatches(\(\mathcal{A}^{l},\mathcal{G}\))
6:forall\(\boldsymbol{g}\in\mathcal{G}\)do
7:\(\text{max\_ious}\leftarrow\{\text{IoU}(\boldsymbol{g},\boldsymbol{a})\mid \forall\,\boldsymbol{a}\in\mathcal{A}^{l}\}\)
8:\(\boldsymbol{a}^{*}\leftarrow\arg\max\left(\text{max\_ious}\right)\)
9:\(\boldsymbol{a}^{*}\,\text{matches.push}(\boldsymbol{g})\)
10:\(\boldsymbol{a}^{*}\,\text{ious.push}(\text{ious}[\boldsymbol{a}^{*}])\)
11:endfor
12:forall\(\boldsymbol{a}\in\mathcal{A}^{l}\)do
13:FindMatches(\(\boldsymbol{a}\,\text{children},\,\boldsymbol{a}\,\text{matches}\))
14:endfor
15:endprocedure
16:procedureReduceLevels(\(\boldsymbol{a}\))
17:if\(|\boldsymbol{a}\,\text{matches}|=1\)thenreturn\(\boldsymbol{a}\,\text{ious}[0]\)
18:endif
19:\(h\leftarrow\max(\boldsymbol{a}\,\text{.ious})\)
20:if\(|\boldsymbol{a}\,\text{.children}|=0\)thenreturn\(h\)
21:else
22:return\(\max(h,\text{mean}(\{\text{ReduceLevels}(c))\mid\forall\,c\in\boldsymbol{a}\,\text{children}\})\)
23:endif
24:endprocedure
25:FindMatches(\(\mathcal{A}^{L},\mathcal{G}\))
26:return\(\text{mean}(\{\text{ReduceLevels}(\boldsymbol{a})\mid\forall\, \boldsymbol{a}\in\mathcal{A}^{L}\})\) ```

**Algorithm 1** : **Hierarchy Level Reduction**. Given a list of the highest level annotations \(\mathcal{A}^{L}\) from the predicted hierarchy and the ground truth annotations \(\mathcal{G}\), this algorithm finds the optimal match of the predicted annotations across the hierarchy with the ground truth while avoiding any temporal overlap between events.
For models generating a hierarchical structure of events, the proposed Hierarchical Level Reduction (HLR) algorithm could be applied for comparison and evaluation. On the other hand, methods that generate a single layer of event segments can directly compare to our 1-layer evaluation reported in Table 1.

## Appendix B Qualitative results

This section contains more qualitative results of STEAMER. A main argument of our paper is that the ground truth annotations (narrations) of events in EPIC-KITCHENS are not consistent and are sometimes redundant. Figure 7 illustrates one such case. Figure 8 shows an example of STREAMER's hierarchical annotations. We refer the reader to the supplementary video for more examples.

Given a video snippet of an event (which we will refer to as a 'query'), can the model retrieve semantically similar video snippets from across the dataset? To determine this, we perform event retrieval by representation: we first generate a representation for a random query which is then compared with the representations of events from all the videos in the dataset. Based on an appropriate similarity measure as required by the model, we select the top-few nearest matches and qualitatively examine the result.

Figure 9 shows an example of the top-three similar matches compared with ABD. Figure 10 shows more examples of STREAMER's event retrieval, displaying the best of the top three matches. Distance in feature space is calculated by the cosine similarity for our method and the Euclidean distance for ABD.

## Appendix C Retrieval: quantitative analysis

In addition to the qualitative results, we perform more quantitative experiments on retrieval. As described in the main text of this work, we use the large language model (LLM) GPT 3.5 to create a dataset of event labels from EPIC-KITCHENS ranked by the semantic similarity. The dataset contains 1K comparisons where each comparison comprises a 'query' narration and a set of 10 'key' narrations, and each key is ranked by its similarity to the query according to the LLM. The keys are ranked according to the distance of their representations in the feature space. The two rank lists are compared based on 1) Mean Squared Error (MSE) and 2) the Levenshtein edit distance. Listing 1

Figure 7: This figure illustrates the effect of inconsistent ground truth on the model’s evaluation performance. In this segment of a video from EPIC-KITCHENS, the ground truth consists of the same narration annotated thrice in succession (_open bag_\(\blacksquare\), _still opening bag_\(\blacksquare\), _still opening bag_\(\blacksquare\)). Although our model could correctly detect this narration to its entirety (the middle row \(\blacksquare\)), its IoU is low, thus affecting its overall evaluation score. Such inconsistencies and redundancies are prevalent throughout the dataset.

shows the prompt used to generate the dataset and Table 5 shows some examples of LLM similarity rankings in the created dataset.

```
1prompt=f"""
2Givenalistofphrasepairs,computethesemanticssimilarity
3betweenthephrasesineachpairandrankinthecontinuous
4rangeof0tot10where10ismostsimilar.
5Thelists:{queries}
6Justreturnalistofdecimalnumbers.Noexplanation.|n""" ```

Listing 1: The LLM prompt used to generate the dataset for retrieval quality evaluation (ranks divided by 10 to be in \((0,1)\))

## Appendix D Implementation details

Let \(k\) be the total number of narrations in the ground truth for a given video plus one (for the background).

TW-FINCH, LSTM-ALThe official implementations of TW-FINCH and LSTM-AL are available on GitHub (TW-FINCH, LSTM-AL). We use the provided code to run our comparisons.

For LSTM-AL, the required number of clusters for each video being clustered was set to \(k\). For LSTM-AL, the order of peak detection was set to \(2\) frames (sampled at 2 fps) to optimize the best results.

ABD, both offline and online clustering versions, had to be re-implemented based on the implementation details of the paper. For offline clustering, the window size was set to 5, the order of non-max suppression to 10, and the average number of actions to \(k\). For the online clustering variant, the window size was set to 15, the order of non-max suppression to 40, and the lower quantile to \(0.25\) as prescribed by the paper.

Figure 8: Given a sequence of temporal perceptual inputs (_e.g._, video), our model learns to represent them at varying levels of detail. This figure illustrates the predictions made by our model on a video from EPIC-KITCHENS at three levels: the highest level (the top row in the bar chart) captures a high-level, low-detail concept (seasoning vegetables); the middle row captures events at relatively finer detail (mixing vegetables and adding salt); and the last row captures the events in much more granular detail. Video available as supplementary.

## Appendix E Literature survey

In this section we provide a more detailed review of related works. We start by discussing single-layered event segmentation and boundary detection works, followed by a summary of existing ideas, inspirations and implementations of hierarchical models. We end our survey by relating to successful prediction-based models for representation learning.

### Event segmentation

Supervised event segmentationOne effective approach to event segmentation is to label every frame with a class then fully supervise parameterized learning models (_e.g._, neural networks) to classify each frame. This eventually groups frames into events; however, a major drawback of these methods is the cost of fine-grained frame annotations. Additionally, fully supervised methods fail to generalize well due to the labels being in a closed set. Different model variations and approaches have been tested, such as using an encoder-decoder temporal convolutional network (ED-TCN) [35], multi-stage temporal convolution network (MS-TCN) [17], or a spatiotemporal CNN model [36]. HASR [2] refines the output predictions of existing segmentation models by utilizing segment-level representations, whereas ASRF [30] improves the segmentation performances by regressing the action boundary probability using representations with a wide temporal receptive field.

Weakly-supervised event segmentationIn order to avoid the costly process of direct frame labeling, researchers have developed weakly-supervised methods that utilize metadata (_e.g._, captions or narrations) to guide the training process instead of relying on explicit training labels. These approaches have been explored in various studies [45, 4, 15, 29, 3], aiming to reduce the dependency on labeled data. However, one limitation is that the required metadata may not always present in the dataset, which restricts their applicability to many real-world scenarios.

Unsupervised event segmentationTo fully eliminate the need for labeling, some approaches [48, 16] attempt to cluster high-level features of frames, which also eliminates training. The segmentation

Figure 9: A comparison of top-three nearest neighbors retrievals with ABD.

performance is directly proportional to the quality of features used in clustering. Therefore when using optical flow-based features, performance will suffer if applied to moving camera videos (_e.g._, egocentric). Note that in our comparisons we provide these approaches with EPIC-KITCHENS supervised features to enhance their performance. Other methods, such as CTE [33] and TOT [34] utilize the order of actions and decode consistent labels using the Viterbi algorithm.

Event boundary detectionOther works (more similar to ours) have formulated the segmentation problem as a boundary detection problem. Generic Event Boundary Detection (GEBD [49]) proposed to use the difference in context embedding before and after each frame to detect boundaries as peaks. Other works [1, 55] use inspiration from [59] to detect boundaries as peaks in the prediction error signal. These approaches use only low-order prediction (based on pretrained supervised high-level features) to form a single layer of event representation and fail to perform higher-order predictions.

### Hierarchical structure modeling

ChunkingRecent work [56] provide a way to chunk data to learn a hierarchical structure by explicitly forming a dictionary of patterns at each level. These methods can be challenging to scale up to real data due to the enormous number of event possibilities to be stored, thus demonstrated only on toy dataset examples of text and vision. Additionally, quantized inputs are required to form these

Figure 10: Random queries and the corresponding best matches, chosen from a set of top-three candidates for each query on EPIC-KITCHENS.

patterns. In contrast, STREAMER does not save patterns but uses learnable networks to encode and predict future inputs. We demonstrate our results on challenging egocentric data.

GLOM and H-JepaHinton [26] proposed an idea paper for "GLOM", which attempts to build a part-whole hierarchy by assuming a cortical column for each patch in an image, where each layer in every column receives contributions from nearby columns at higher and lower levels. The hierarchy, or parse tree, is formed by forcing consensus laterally between each level, of each column, using a similarity function. Although we share the same motivation with GLOM, we significantly differ in execution. Instead of using a similarity function to force forming "islands of agreement", STREAMER relies on prediction error to discover events and explicitly send their representation to higher level in a dynamically evolving structure (_i.e._, layer-by-layer). Perhaps we share more algorithmic similarity with LeCun's [37] idea paper, "H-JEPA". The paper proposes to use a hierarchical joint embedding predictive architecture which uses self-supervised predictive learning to predict at different timescales at various levels in the hierarchy. This is also supported by evidence from neuroscience [8] suggesting the importance of long-range forecast representations in improving brain-mapping functionality. H-JEPA suggests that higher levels predict further into the future than lower levels; however, it does not provide a working algorithm for segmentation or cross-layer communication. It is to be noted that both works by Hinton and LeCun only provide ideas and not algorithms/results.

### Predictive modeling

Natural language processingSelf-supervised prediction of inputs has shown tremendous success in the field of NLP, specifically masked language models. Recent large language models [13; 5; 52; 54] are based on the core idea of using context to predict missing inputs. The guiding principle of training these models is that successfully predicting missing inputs implies encoding good representation of the context.

Computer visionA similar principle is used to train image representation learning models. Masked Auto Encoder (MAE [25]) is trained to predict missing image patches using a transformer architecture, thus performing better on downstream tasks using the enhanced representations. A family of representation learning methods [21; 6; 9; 7; 43] learn useful representations by predicting augmented views of the input. These augmented views attempt to simulate real-world augmentations (_e.g._, cropping simulates a moving camera), therefore these methods may also be viewed as predicting future frames in a video sequence. Similar ideas can also be seen applied to video representations learning. PredNet [38] attempts to implement cognitively-inspired predictive coding theories for future frame prediction. Other methods [22; 23] use dense predictive coding to learn video representations for retrieval and fine-tuning on downstream tasks. Unlike STREAMER, these methods do not generalize to learning hierarchical representations using predictive learning.

### Relevant datasets

STREAMER is a self-supervised architecture that relies on predictive learning for hierarchical segmentation. In our model, higher-order predictive layers receive sparser learning signals than lower-order layers, because the first layer directly predicts frames, whereas higher layers only receive events that cannot be predicted at lower levels. Short videos do not allow for higher order predictions and learning long-term temporal dependencies. Therefore, training higher levels in the hierarchy requires a large dataset (_i.e._, total number of hours) and longer videos (_i.e._, average video duration) in order to model high-level events. These requirements constraint the choice of datasets on which we can run and evaluate STREAMER.

Based on our review of available datasets for both egocentric and exocentric settings, as shown in the Table 4, many of the available datasets, typically used in event segmentation, do not provide long enough videos. MovieNet and NewsNet are two large datasets with long videos but have not yet been released to the public. In addition, MovieNet does not contain action segments; it contains coarse scene segments. The only available options to train and evaluate STREAMER is large-scale egocentric datasets, where the available datasets provide large enough scale (_i.e._, total number of hours) with a long average duration per video for streaming and high-order temporal prediction.

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_EMPTY:22]