# Maximization of Average Precision for

Deep Learning with Adversarial Ranking Robustness

 Gang Li

Texas A&M University

College Station, USA

gang-li@tamu.edu

&Wei Tong

General Motors

Warren, USA

wei.tong@gm.com

&Tianbao Yang

Texas A&M University

College Station, USA

tianbao-yang@tamu.edu

###### Abstract

This paper seeks to address a gap in optimizing Average Precision (AP) while ensuring adversarial robustness, an area that has not been extensively explored to the best of our knowledge. AP maximization for deep learning has widespread applications, particularly when there is a significant imbalance between positive and negative examples. Although numerous studies have been conducted on adversarial training, they primarily focus on robustness concerning accuracy, ensuring that the average accuracy on adversarially perturbed examples is well maintained. However, this type of adversarial robustness is insufficient for many applications, as minor perturbations on a single example can significantly impact AP while not greatly influencing the accuracy of the prediction system. To tackle this issue, we introduce a novel formulation that combines an AP surrogate loss with a regularization term representing adversarial ranking robustness, which maintains the consistency between ranking of clean data and that of perturbed data. We then devise an efficient stochastic optimization algorithm to optimize the resulting objective. Our empirical studies, which compare our method to current leading adversarial training baselines and other robust AP maximization strategies, demonstrate the effectiveness of the proposed approach. Notably, our methods outperform a state-of-the-art method (TRADES) by more than 4% in terms of robust AP against PGD attacks while achieving 7% higher AP on clean data simultaneously on CIFAR10 and CIFAR100. The code is available at: https://github.com/GangLii/Adversarial-AP

## 1 Introduction

AP measures the precision of a model at different recall levels, offering a more nuanced understanding of the trade-offs between precision and recall. Optimizing AP for deep learning is of vital importance, especially in cases with highly imbalanced datasets. In such situations, accuracy alone can be misleading, as a model may perform well on the majority class but struggle with the minority class, thereby offering a superficially high accuracy score. In contrast, AP serves as a ranking metric that is particularly attuned to errors at the top of the ranking list, which makes it a more appropriate metric for applications dealing with highly imbalanced datasets. For example, deep AP maximization has been crucial in enhancing molecular property prediction performance, contributing to a winning solution in the MIT AICures challenge [54].

However, existing approaches of AP maximization are not robust against adversarial examples. It is notoriously known that deep neural networks (DNN) are vulnerable to adversarial attacks, where small, carefully-crafted perturbations to the input data can cause the model to produce incorrect predictions [46; 16]. These perturbations are often imperceptible to humans but can significantly impact the model's performance. Tremendous studies have been conducted to improve the adversarial robustness of DNN. A popular strategy to achieve adversarial robustness is through adversarial training [27; 59; 53; 45; 21; 2; 35; 44; 47; 15], which injects adversarial examples into the trainingthat are generated by various attack methods. Nevertheless, almost all existing methods focus on robustness concerning accuracy, ensuring that the average accuracy on adversarially perturbed examples is well maintained. This type of adversarial robustness is insufficient for many applications with highly imbalanced datasets, as minor perturbations on a single example can significantly impact AP while not greatly influencing the accuracy of the prediction system (cf. an example in Figure 1). This presents a significant challenge for adversarially robust AP maximization.

In this paper, we conduct a comprehensive study on how to imbue AP maximization with adversarial robustness. There are several technical and practical concerns in the design of adversarial training methods for AP maximization to enjoy three nice properties: (i) capability to trade off between AP on a set of clean data and the robustness of the model on the perturbed data; (ii) robustness in terms of ranking performance instead of accuracy against adversarial perturbations; (ii) consistency of attacks between training and inference. The first property is obvious in light of existing works on adversarial training [59]. The importance of the second property has been explained in the previous paragraph. The third property is tricky as it does not exist in existing adversarial training methods. The adversarial attack is usually applied to an individual data during the inference phase. Hence, we expect that maintaining the consistency between the attacks generated in training process and that in the inference phase will help boost the performance. However, this will cause a dilemma for achieving pointwise attack and listwise robustness in a unified framework.

To acquire these properties in a unified framework, we draw inspiration from prior adversarial training methods through robust regularization, and integrate two distinct design elements. We examine robust objectives that combine an AP surrogate loss on the clean data and a regularization term depending on the perturbed data. The two unique design features are (i) a new listwise adversarial regularization defined by a divergence between two distributions that represent the top one probabilities of a set of clean data and their perturbed versions; (ii) a non-zero-sum game approach, which integrates pointwise adversarial attacks with the proposed listwise adversarial regularization. This will ensure the attack consistency between training and inference. Our contributions are summarized below.

* We propose a non-zero-sum game optimization formulation for AP maximization with adversarial ranking robustness, which achieves listwise defense against pointwise attacks.
* We propose an efficient stochastic algorithm for solving the resulting objective, which integrates traditional adversarial sample generation methods with a state-of-the-art deep AP maximization algorithm without requiring a large batch size.
* We conduct extensive experiments to compare with existing leading adversarial training baselines, and ablation studies to compare different approaches shown in Table 1 for adversarial AP maxi

\begin{table}
\begin{tabular}{c|c|c|c||c|c|c} \hline Approaches & Objective & Regularization & Optimization & Ranking Robustness & Trade-off & Consistent Attack \\ \hline Adap\_MM & MinMax AP Loss & No & Zero-sum Game & Yes & No & No \\ \hline Adap\_ZZ & AP Loss + Reg. & Pointwise & Zero-sum Game & No & Yes & Yes \\ \hline Adap\_LZ & AP Loss + Reg. & Listwise & Zero-sum Game & Yes & Yes & No \\ \hline Adap\_LN & AP Loss + Reg. & Listwise & Non-zero-sum Game & Yes & Yes & Yes \\ \hline Adap\_LPN & AP Loss + Reg. & Listwise + Pointwise & Non-zero-sum Game & Yes & Yes & Yes \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of different approaches for Adversarial AP Maximization (AdAP). Red indicates new features proposed in this paper.

Figure 1: Top are predictions on clean images of a robust model trained by a state-of-the-art adversarial training method TRADES [59] and our method on CIFAR-10 data for detecting trucks. Bottom are predictions on the same set of images with only one example adversarially perturbed, which is generated by PGD following a black-box attack. The results of TRADES (left) indicate that slightly changing one example could dramatically impact AP but not on the accuracy. The results of our approach (right) demonstrate that our solution is more robust to adversarial data in terms of ranking and AP score. The dashed blue line indicates the decision boundary at score 0.5.

mization. We conclude that AdAP_LN and AdAP_LPN achieve the best performance among all methods corroborating the effectiveness of the proposed techniques.

## 2 Related Work

**Adversarial Robustness.** To safeguard deep neural networks (DNNs) from adversarial attacks, a variety of adversarial defense techniques have been proposed in the literature, including (1) detection for defense [37; 48; 38; 24]; (2) input transformations for defense [25; 17; 39; 18; 58]; (3) adversarial training [1; 47; 15; 4]. Among these techniques, adversarial training has been demonstrated to be one of the most effective approaches [27; 59; 41; 45; 21; 14; 53]. Among these, [27] is the first to theoretically study and justify adversarial training by solving a min-max formulation for training adversarially robust models for deep learning. [59] presents an objective function that strike a balance between accuracy and robustness in light of their theoretical tight upper bound on the robust error. However, these previous methods focus on how to improve the robustness concerning accuracy, which is not sufficient for highly imbalanced data. [52] considers adversarial training for imbalanced data by combining a minimax weighted loss and a contrastive loss of feature representations. [60] considers adversarial ranking in the context of retrieval and proposes maximum-shift-distance attack that pushes an embedding vector as far from its original position as possible and uses it in a triplet loss for optimization. [20] presents a study on adversarial AUC optimization by reformulating the original tightly coupled objective as an instance-wise form for adversarial training. Nevertheless, none of these methods enjoy three nice properties simultaneously, i.e., adversarial ranking robustness, trade-off between AP and robustness, and consistent attacks between training and inference.

**Average Precision Optimization.** For imbalanced classification and information retrieval problems, AP optimization has attracted significant attention in the literature [13; 36; 31; 5; 10; 29; 28; 55]. To maximize the AP score for big data, some works employ stochastic optimization with mini-batch averaging to compute an approximate gradient of the AP function or its smooth approximation [6; 36; 42; 4]. These methods typically rely on a large mini-batch size for good performance. In contrast, [34] proposes a novel stochastic algorithm that directly optimizes a surrogate function of AP and provides theoretical convergence guarantee, without the need for a large mini-batch size. Then [50] further improve the stochastic optimization of AP by developing novel stochastic momentum methods with a better iteration complexity of \(O(1/\epsilon^{4})\). However, these approaches of AP maximization are vulnerable to adversarial examples created by introducing small perturbations to natural examples. The question of how to boost model's AP under adversarial perturbations while maintaining AP on clean data is still unresolved.

## 3 Preliminaries

For simplicity of exposition, we consider binary classification problems. However, the discussions and algorithms can be easily extended to mean AP for multi-class or multi-label classification problems. Let \(\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}\) denote the set of all training examples with \(\mathbf{x}_{i}\) being an input data and \(y_{i}\in\{-1,1\}\) being its associated label. Denoted by \(h(\mathbf{x})=h_{\mathbf{w}}(\mathbf{x})\) the predictive function (e.g., a deep neural network), whose parameters are \(\mathbf{w}\in\mathbb{R}^{d}\). Denote by \(\mathbb{I}(\cdot)\) an indicator function of a predicate. Denoted by \(\|\cdot\|=\|\cdot\|_{p}\) the \(L_{p}\)-norm where \(p\in(1,\infty]\). Denoted by \(\mathbb{B}(0,\epsilon)=\{x:\|x\|\leq\epsilon\}\) the \(L_{p}\)-norm ball centered at 0 with radius \(\epsilon\). Let \(\mathcal{D}_{+}\) and \(\mathcal{D}_{-}\) be the subsets of \(\mathcal{D}\) with only positive examples and negative examples, respectively. Let \(n_{+}=|\mathcal{D}_{+}|\) denote the number of positive examples. Denote by \(r(\mathbf{x}_{i},\mathcal{D})=\sum_{\mathbf{x}_{j}\in\mathcal{D}}\mathbb{I}(h _{\mathbf{w}}(\mathbf{x}_{j})\geq h_{\mathbf{w}}(\mathbf{x}_{i}))\) the rank of \(\mathbf{x}_{i}\) in a set \(\mathcal{D}\) according to the prediction scores in descending order, i.e., the number of examples that are ranked higher than \(\mathbf{x}_{i}\) including itself. Let \(\ell(h_{\mathbf{w}}(\mathbf{x}),y)\) denote a pointwise loss function, e.g., the cross-entropy loss.

### AP Maximization

According to its definition, AP can be expressed as: \(\frac{1}{n_{+}}\sum_{\mathbf{x}_{i}\in\mathcal{D}_{+}}\frac{r(\mathbf{x}_{i},\mathcal{D}_{+})}{r(\mathbf{x}_{i},\mathcal{D})}\)[3]. Many different approaches have been developed for AP maximization [9; 36; 13; 4; 34]. We follow a recent work [34], which proposes an efficient stochastic algorithm for AP maximization based on differentiable surrogate loss minimization. In particular, the rank function is approximated by using a surrogate loss of the indicator function \(\mathbb{I}(h_{\mathbf{w}}(\mathbf{x}_{j})\geq h_{\mathbf{w}}(\mathbf{x}_{i}))\)[34], yielding the following problem:

\[\min_{\mathbf{w}}P(\mathbf{w})=-\frac{1}{n_{+}}\sum_{\mathbf{x}_{i}\in \mathcal{D}_{+}}\frac{\sum\limits_{s=1}^{n}\mathbb{I}(y_{s}=1)\ell(\mathbf{w}; \mathbf{x}_{s};\mathbf{x}_{i})}{\sum\limits_{s=1}^{n}\ell(\mathbf{w};\mathbf{x }_{s};\mathbf{x}_{i})},\] (1)where \(\ell(\cdot)\) denotes a smooth surrogate loss function, e.g., the squared hinge loss \(\ell(\mathbf{w};\mathbf{x}_{s};\mathbf{x}_{i})=(\max\{m-(h_{\mathbf{w}}(\mathbf{x }_{i})-h_{\mathbf{w}}(\mathbf{x}_{s})),0\})^{2}\), where \(m\) is a margin parameter.

To tackle the computational complexity of computing the stochastic gradient of \(P(\mathbf{w})\), [34] has formulated this problem as a finite-sum coupled compositional optimization of the form \(\frac{1}{n_{+}}\sum_{\mathbf{x}_{i}\in\mathcal{D}_{+}}f(\varphi(\mathbf{w}; \mathbf{x}_{i}))\), where \(\varphi(\mathbf{w};\mathbf{x}_{i})=[\varphi_{1}(\mathbf{w};\mathbf{x}_{i}), \varphi_{2}(\mathbf{w};\mathbf{x}_{i})]\), \(\varphi_{1}(\mathbf{w};\mathbf{x}_{i})=\sum_{s=1}^{n}\mathbb{I}(y_{s}=1)\ell (\mathbf{w};\mathbf{x}_{s};\mathbf{x}_{i})/n\), \(\varphi_{2}(\mathbf{w};\mathbf{x}_{i})=\sum_{s=1}^{n}\ell(\mathbf{w};\mathbf{ x}_{s};\mathbf{x}_{i})/n\), \(f([\varphi_{1},\varphi_{2}])=-\varphi_{1}/\varphi_{2}\). To compute a stochastic gradient estimator, their algorithm maintains two moving average estimators \(\mathbf{u}_{\mathbf{x}_{i}}^{1}\) and \(\mathbf{u}_{\mathbf{x}_{i}}^{2}\) for \(\varphi_{1}(\mathbf{w};\mathbf{x}_{i})\) and \(\varphi_{2}(\mathbf{w};\mathbf{x}_{i})\). At iteration \(t\), two mini-batches are sampled \(\mathcal{B}^{+}\in\mathcal{D}_{+}\) and \(\mathcal{B}\in\mathcal{D}\). The two estimators \(\mathbf{u}_{\mathbf{x}_{i}}^{1}\) and \(\mathbf{u}_{\mathbf{x}_{i}}^{2}\) are updated by Equation (2) for \(\mathbf{x}_{i}\in\mathcal{B}^{+}\):

\[\begin{split}\mathbf{u}_{\mathbf{x}_{i}}^{1}&=(1- \gamma_{1})\mathbf{u}_{\mathbf{x}_{i}}^{1}+\gamma_{1}\frac{1}{|\mathcal{B}|} \sum_{\mathbf{x}_{j}\in\mathcal{B}}\ell(\mathbf{w}_{t};\mathbf{x}_{j}, \mathbf{x}_{i})\mathbb{I}(y_{j}=1)\\ \mathbf{u}_{\mathbf{x}_{i}}^{2}&=(1-\gamma_{1}) \mathbf{u}_{\mathbf{x}_{i}}^{2}+\gamma_{1}\frac{1}{|\mathcal{B}|}\sum_{ \mathbf{x}_{j}\in\mathcal{B}}\ell(\mathbf{w}_{t};\mathbf{x}_{j},\mathbf{x}_{i }),\end{split}\] (2)

where \(\gamma_{1}\in(0,1)\) is a moving average parameter. With these stochastic estimators, an stochastic estimate of \(\nabla P(\mathbf{w})\) is given by Equation (3)

\[\widehat{\nabla}_{\mathbf{w}_{t}}P(\mathbf{w}_{t})=\frac{1}{|\mathcal{B}^{+}| }\sum_{\mathbf{x}_{i}\in\mathcal{B}^{+}}\sum_{\mathbf{x}_{j}\in\mathcal{B}} \frac{(\mathbf{u}_{\mathbf{x}_{i}}^{1}-\mathbf{u}_{\mathbf{x}_{i}}^{2}\mathbb{ I}(y_{j}=1))\nabla\ell(\mathbf{w}_{t};\mathbf{x}_{j},\mathbf{x}_{i})}{| \mathcal{B}|(\mathbf{u}_{\mathbf{x}_{i}}^{2})^{2}}.\] (3)

Then the model parameter can be updated similar to SGD, momentum-based methods, or Adam.

### Pointwise Attacks and Pointwise Adversarial Regularization

An adversarial attack is usually applied to a specific example \(\mathbf{x}\) such that its class label is changed from its original label (non-targeted attack) or to a specific label (targeted attack). We refer to this kind of attack as pointwise attack. Various pointwise attacking methods have been proposed, including, FGSM [16], Basic Iterative Method [23], PGD [27], JSMA [32], DeepFool [30], and CW attack [8]. While our approach is agnostic to any pointwise attacks, we restrict our discussion to optimization-based non-target attacks, \(\delta=\arg\min_{\|\delta\|\leq\epsilon}G(\mathbf{w},\mathbf{x}+\delta,y)\), where \(G\) is an appropriate function, e.g., \(G(\mathbf{w},\mathbf{x}+\delta,y)=-\ell(h_{\mathbf{w}}(\mathbf{x}+\delta),y)\). A classic adversarial training method is to solve a robust optimization problem that integrates the training loss with the adversarial attack, e.g.,

\[\min_{\mathbf{w}}\frac{1}{n}\sum_{i=1}^{n}\max_{\|\delta_{i}\|\leq\epsilon} \ell(h_{\mathbf{w}}(\mathbf{x}_{i}+\delta_{i}),y_{i}).\] (4)

A deficiency of this approach is that it may not capture the trade-off between natural and robust errors [49]. To address this issue, robust regularization methods have been proposed, whose objective consists of a regular surrogate loss of error rate on clean data and a robust regularization term that accounts for the adversarial robustness. Different robust regularizations have been proposed [59; 45; 21]. A state-of-the-art approach is TRADES [59], whose objective is formulated by:

\[\min_{\mathbf{w}}\frac{1}{n}\sum_{i=1}^{n}\left\{\ell(h_{\mathbf{w}}(\mathbf{ x}_{i}),y_{i})+\lambda\max_{\|\delta_{i}\|\leq\epsilon}L(h_{\mathbf{w}}(\mathbf{x}_{i}),h_{ \mathbf{w}}(\mathbf{x}_{i}+\delta_{i}))\right\},\] (5)

where \(\lambda>0\) is a regularization parameter and \(L(\cdot,\cdot)\) is an appropriate divergence function, e.g., cross-entropy loss between two predicted probabilities [59].

## 4 Adversarial AP Maximization

First, we discuss two straightforward approaches. The first method (referred to as AdAP_MM in Table 1) is to replace the loss in (4) as an AP surrogate loss yielding the following:

\[\min_{\mathbf{w}}\max_{\|\delta\|\leq\epsilon}-\frac{1}{n_{+}}\sum_{\mathbf{x} _{i}\in\mathcal{D}_{+}}\frac{\sum_{s=1}^{n}\mathbb{I}(y_{s}=1)\ell(\mathbf{w}, \mathbf{x}_{s}+\delta_{s},\mathbf{x}_{i}+\delta_{i})}{\sum_{s=1}^{n}\ell( \mathbf{w},\mathbf{x}_{s}+\delta_{s},\mathbf{x}_{i}+\delta_{i})}.\] (6)

The second method (referred to as AdAP_PZ in Table 1) is to simply replace the first term in (5) with an AP surrogate loss:

\[\min_{\mathbf{w}}P(\mathbf{w})+\frac{\lambda}{n}\sum_{i=1}^{n}\max_{\|\delta_{i }\|\leq\epsilon}L(h_{\mathbf{w}}(\mathbf{x}_{i}),h_{\mathbf{w}}(\mathbf{x}_{i}+ \delta_{i})).\] (7)The limitations of the first method are that it may not capture the trade-off between AP and robustness and the adversarial attacks during the training are inter-dependent, which is not consistent with pointwise attacks generated during the inference phase. While the second method is able to trade off between AP and robustness, the pointwise regularization is not suitable for tackling imbalanced datasets, as an perturbation on a single data point from a minority class does not change the pointwise regularization too much but could degrade the AP significantly.

### Listwise Adversarial Regularization

To address the deficiencies of the above straightforward approaches, we draw inspiration from TRADES and propose a listwise adversarial regularization to replace the pointwise regularization. The key property of the listwise adversarial regularization is that it should capture the divergence between the ranking result of the clean data and that of the perturbed data. To this end, we leverage the top-one probability proposed in the literature of learning to rank [7].

**Definition 1**: _The top one probability of an data \(\mathbf{x}_{i}\) represents the probability of it being ranked on the top, given the scores of all the examples i.e., \(p_{t}(\mathbf{x}_{i})=\frac{\exp(h_{\mathbf{w}}(\mathbf{x}_{i}))}{\sum_{j=1} \exp(h_{\mathbf{w}}(\mathbf{x}_{j}))}\)._

With the definition of top-one probability, we define the listwise adversarial regularization as the divergence between two listwise probabilities, i.e., \(\{p_{t}(\mathbf{x}_{i})\}_{i=1}^{n}\) for the clean examples and \(\{p_{t}(\mathbf{x}_{i}+\delta_{i})\}_{i=1}^{n}\) for the perturbed examples. Different metrics can be used to measure the divergence between two distributions, e.g., KL divergence between \(\{p_{t}(\mathbf{x}_{i})\}_{i=1}^{n}\) and \(\{p_{t}(\mathbf{x}_{i}+\delta_{i})\}_{i=1}^{n}\), KL divergence between \(\{p_{t}(\mathbf{x}_{i}+\delta_{i})\}_{i=1}^{n}\) and \(\{p_{t}(\mathbf{x}_{i})\}_{i=1}^{n}\), and their symmetric version Jensen-Shannon divergence. For illustration purpose, we consider the KL divergence:

\[R(\mathbf{w},\delta,\mathcal{D})=\sum\nolimits_{i=1}^{n}p_{t}(\mathbf{x}_{i}) \log p_{t}(\mathbf{x}_{i})-p_{t}(\mathbf{x}_{i})\log p_{t}(\mathbf{x}_{i}+ \delta_{i}).\] (8)

To further understand the above listwise regularization, we conduct a theoretical analysis similar to [59] by decomposing the robust error into two components. The difference from [59] is that we need to use the misranking error. We consider the misranking error as \(\mathcal{R}_{nat}=\mathbb{E}_{\mathbf{x}_{i}\sim\mathcal{D}_{+}}\mathbb{I}\{h (\mathbf{x}_{i})\leq\max_{\mathbf{x}_{j}\in\mathcal{D}_{-}}h(\mathbf{x}_{j})\}\), which measures how likely a positive example is ranked below a negative example [43]. To characterize the robust ranking error under the attack of bounded \(\epsilon\) perturbation, we define \(\mathcal{R}_{rob}=\mathbb{E}_{\mathbf{x}_{i}\sim\mathcal{D}_{+}}\mathbb{I}\{ \exists\delta_{i},\delta_{j}\in\mathbb{B}(0,\epsilon),h(\mathbf{x}_{i}+\delta _{i})\leq\max_{\mathbf{x}_{j}\in\mathcal{D}_{-}}h(\mathbf{x}_{j}+\delta_{j})\}\). It is worth noting that \(\mathcal{R}_{nat}\leq\mathcal{R}_{rob}\) is always satisfied, and in particular, \(\mathcal{R}_{nat}=\mathcal{R}_{rob}\) if \(\epsilon=0\). We show that the \(\mathcal{R}_{rob}\) can be decomposed into \(\mathcal{R}_{nat}\) and \(\mathcal{R}_{bdy}\) in Theorem 1.

**Theorem 1**: \(\mathcal{R}_{rob}=\mathcal{R}_{nat}+\mathcal{R}_{bdy}\)_, where the second term is called the boundary error \(\mathcal{R}_{bdy}=\mathbb{E}_{\mathbf{x}_{i}\sim\mathcal{D}_{+}}\mathbb{I}\{h (\mathbf{x}_{i})>\max_{\mathbf{x}_{j}\in\mathcal{D}_{-}}h(\mathbf{x}_{j})\} \mathbb{I}\{\exists\delta_{i},\delta_{j}\in\mathbb{B}(0,\epsilon),h(\mathbf{ x}_{i}+\delta_{i})\leq\max_{\mathbf{x}_{j}\in\mathcal{D}_{-}}h(\mathbf{x}_{j}+ \delta_{j})\}\)._

**Remark:** It is clear that the boundary error measures the divergence between two ranked list \(\{h_{\mathbf{w}}(\mathbf{x}_{i})\}_{i=1}^{n}\) and \(\{h_{\mathbf{w}}(\mathbf{x}_{i}+\delta_{i})\}_{i=1}^{n}\), which provides an explanation of the divergence between the top-one probabilities on the clean data and on the perturbed data.

Since we are optimizing AP, we use an AP surrogate loss as a surrogate of the misranking error \(\mathcal{R}_{nat}\) on the clean data. Finally, it yields in a robust objective \(P(\mathbf{w})+R(\mathbf{w},\delta,\mathcal{D})\). A question remained is how to generate the perturbations. A simple strategy is to use the robust optimization approach that solves a zero-sum game: \(\min_{\mathbf{x}_{\delta}\in\Omega^{n}}P(\mathbf{w})+R(\mathbf{w},\delta, \mathcal{D})\) (referred to as AdAP_LZ in Table 1). However, since \(R(\mathbf{w},\delta,\mathcal{D})\) is a listwise regularization, the resulting perturbations \(\{\tilde{\delta}_{i}\}_{i=1}^{n}\) are inter-dependent, which is not consistent with the pointwise attacks generated in the inference phase. To address this issue, we decouple the defense and the attack by solving a non-zero-sum game:

\[\min_{\mathbf{w}}\quad P(\mathbf{w})+\lambda R(\mathbf{w},\delta,\mathcal{D})\] (9)

\[\max_{\|\delta_{i}\|\leq\epsilon}\quad\sum\nolimits_{i=1}^{n}G(\mathbf{w}, \mathbf{x}_{i}+\delta_{i},y_{i}),\]

where the attacks are pointwise attacks generated for individual examples separately, e.g., FGSM, PGD. We refer to the above method as AdAP_LN. Finally, we experiment with another method by adding a pointwise regularization into the objective for optimizing the model parameter \(\mathbf{w}\), i.e.,

\[\min_{\mathbf{w}}\quad P(\mathbf{w})+\lambda(R(\mathbf{w},\delta,\mathcal{D}) +\frac{1}{n}\sum_{i=1}^{n}L(h_{\mathbf{w}}(\mathbf{x}_{i}),h_{\mathbf{w}}( \mathbf{x}_{i}+\delta_{i})))\] (10)

This method referred to as AdAP_LPN imposes a stronger adversarial regularization on the model.

```
1:Initialize \(\mathbf{w},\mathbf{u}_{\mathbf{x}}^{1},\mathbf{u}_{\mathbf{x}}^{2},\mathbf{u},\gamma_{ 1},\gamma_{2}\)
2:for\(t=1,\ldots,T\)do
3: Draw a batch of \(B_{+}\) positive samples denoted by \(\mathcal{B}_{+}\).
4: Draw a batch of \(B\) samples denoted by \(\mathcal{B}\).
5:for\(\mathbf{x}_{i}\in\mathcal{B}\)do
6: Initialize \(\delta_{i}\sim\alpha\cdot\mathcal{N}(0,1)\)
7:for\(m=1,\ldots,M\)do
8: Update \(\delta_{i}=\Pi_{\|\cdot\|\leq\epsilon}(\delta_{i}+\eta_{2}\cdot sign(\nabla_{ \delta_{i}}G(\mathbf{w}_{t},\mathbf{x}_{i}+\delta_{i},y_{i})))\), where \(\Pi_{\Omega}(\cdot)\) is the projection operator.
9:endfor
10:endfor
11: For each \(\mathbf{x}_{i}\in\mathcal{B}_{+}\), update \(\mathbf{u}_{\mathbf{x}_{i}}^{1}\) and \(\mathbf{u}_{\mathbf{x}_{i}}^{2}\) by Equation (2)
12: Update \(\mathbf{u}\) by Equation (11) and compute \(\widehat{\nabla}_{\mathbf{w}_{t}}R(\mathbf{w}_{t},\delta,\mathcal{D})\) by Equation (12)
13: Compute stochastic gradient estimator \(\nabla_{\mathbf{w}_{t}}=\widehat{\nabla}_{\mathbf{w}_{t}}P(\mathbf{w}_{t})+ \lambda\widehat{\nabla}_{\mathbf{w}_{t}}R(\mathbf{w}_{t},\delta,\mathcal{D})\)
14: Update \(\mathbf{w}_{t+1}\) by using SGD, momentum-methods or Adam.
15:endfor ```

**Algorithm 1** Stochastic Algorithm for Solving Adap_LN in (9)

### Algorithms for Adversarial AP Maximization

Below, we will discuss efficient algorithms for adversarial AP maximization employing different objectives. Due to limit of space, we focus attention on solving (9). Equal efforts have been spent on solving other Adap formulations with algorithms presented in the appendix.

Since there are two players one for minimizing over \(\mathbf{w}\) and one for maximizing over \(\delta\), we adopt the alternating optimization framework that first optimizes \(\delta\) for generating attacks of sampled data and then optimizes for \(\mathbf{w}\) based on the sampled clean data and perturbed data. The optimization for \(\delta\) is following the existing methods in the literature. We use the PGD method for illustration purpose. The major technical challenge lies at computing a gradient estimator of the listwise regularization \(R(\mathbf{w},\delta,\mathcal{D})\) in terms of \(\mathbf{w}\). We consider \(\delta\) to be fixed and rewrite \(R(\mathbf{w},\delta,\mathcal{D})\) as:

\[\frac{\sum_{i=1}^{n}\exp(h(\mathbf{x}_{i}))(h(\mathbf{x}_{i})-h(\mathbf{x}_{i }+\delta_{i}))}{\sum_{j=1}^{n}\exp(h(\mathbf{x}_{j}))}-\log\sum_{j=1}^{n}\exp( h(\mathbf{x}_{j}))+\log\sum_{j=1}^{n}\exp(h(\mathbf{x}_{j}+\delta_{j})).\]

Note that calculating the gradient of \(R(\mathbf{w},\delta,\mathcal{D})\) in terms of \(\mathbf{w}\) directly, which includes the prediction scores of all samples, is not feasible. To tackle this challenge, we cast \(R(\mathbf{w},\delta,\mathcal{D})\) into a compositoal function and borrow the technique of stochastic compositional optimization. To this end, we define:

\[g(\mathbf{w})=[g_{1}(\mathbf{w}),g_{2}(\mathbf{w}),g_{3}(\mathbf{ w})]^{\top}\] \[=\bigg{[}\frac{1}{n}\sum_{i=1}^{n}\exp(h(\mathbf{x}_{i}))(h( \mathbf{x}_{i})-h(\mathbf{x}_{i}+\delta_{i})),\frac{1}{n}\sum_{i=1}^{n}\exp(h (\mathbf{x}_{i})),\frac{1}{n}\sum_{i=1}^{n}\exp(h(\mathbf{x}_{i}+\delta_{i}) )\bigg{]}^{\top}.\]

Let \(F(g)=\frac{g_{1}}{g_{2}}-\log g_{2}+\log g_{3}\). Then we write \(R(\mathbf{w},\delta,\mathcal{D})=F(g(\mathbf{w}))=\frac{g_{1}(\mathbf{w})}{g_{ 2}(\mathbf{w})}-\log g_{2}(\mathbf{w})+\log g_{3}(\mathbf{w})\). The gradient of \(R(\mathbf{w};\delta,\mathcal{D})\) is given by:

\[\nabla_{\mathbf{w}}R(\mathbf{w},\delta,\mathcal{D})=\nabla_{\mathbf{w}}g( \mathbf{w})^{\top}\nabla F(g(\mathbf{w}))=\nabla_{\mathbf{w}}g(\mathbf{w})^{ \top}\bigg{(}\frac{1}{g_{2}(\mathbf{w})},\frac{-g_{1}(\mathbf{w})-g_{2}( \mathbf{w})}{(g_{2}(\mathbf{w}))^{2}},\frac{1}{g_{3}(\mathbf{w})}\bigg{)}^{ \top}.\]

The major cost for computing \(\nabla_{\mathbf{w}}R(\mathbf{w},\delta,\mathcal{D})\) lies at evaluating \(g(\mathbf{w})\) and its gradient \(\nabla_{\mathbf{w}}g(\mathbf{w})\), which involves passing through all examples in \(\mathcal{D}\). In order to develop a stochastic algorithm, we will approximate these quantities by using a mini-batch of random samples. The gradient \(\nabla_{\mathbf{w}}g(\mathbf{w})\) can be simply approximated by the stochastic gradient, i.e.,

\[\widehat{\nabla}_{\mathbf{w}}g(\mathbf{w})=\left(\begin{array}{c}\frac{1}{B} \sum_{\mathbf{x}_{i}\in\mathcal{B}}\exp(h(\mathbf{x}_{i}))((h(\mathbf{x}_{i}) -h(\mathbf{x}_{i}+\delta_{i})+1)\nabla_{\mathbf{w}}h(\mathbf{x}_{i})-\nabla_{ \mathbf{w}}h(\mathbf{x}_{i}+\delta_{i}))\\ \frac{1}{B}\sum_{\mathbf{x}_{i}\in\mathcal{B}}\exp(h(\mathbf{x}_{i}))\nabla_{ \mathbf{w}}h(\mathbf{x}_{i})\\ \frac{1}{B}\sum_{\mathbf{x}_{i}\in\mathcal{B}}\exp(h(\mathbf{x}_{i}+\delta_{i}) )\nabla_{\mathbf{w}}h(\mathbf{x}_{i}+\delta_{i})\end{array}\right),\]

where \(\mathcal{B}\) denote a set of \(B\) random samples from \(\mathcal{D}\). Due to the non-linear dependence of \(\nabla_{\mathbf{w}}R(\mathbf{w},\delta,\mathcal{D})\) on \(g(\mathbf{w})\), we cannot simply use their mini-batch estimator in the calculation as it will yield a large optimization error [51]. To reduce the optimization error [34; 33], we will maintain a vector \(\mathbf{u}=[u_{1},u_{2},u_{3}]\) for tracking \([g_{1}(\mathbf{w}),g_{2}(\mathbf{w}),g_{3}(\mathbf{w})]\). The vector \(\mathbf{u}\) is updated by Equation (11), where \(\gamma_{2}\in(0,1)\) is a parameter. An estimate of \(\nabla_{\mathbf{w}}R(\mathbf{w},\delta,\mathcal{D})\) is given by Equation (12):

\[\mathbf{u}=(1-\gamma_{2})\mathbf{u}+\gamma_{2}\left(\begin{array}{c}\frac{1}{B} \sum_{\mathbf{x}_{i}\in\mathcal{B}}\exp(h(\mathbf{x}_{i}))(h(\mathbf{x}_{i})-h( \mathbf{x}_{i}+\delta_{i}))\\ \frac{1}{B}\sum_{\mathbf{x}_{i}\in\mathcal{B}}\exp(h(\mathbf{x}_{i}))\\ \frac{1}{B}\sum_{\mathbf{x}_{i}\in\mathcal{B}}\exp(h(\mathbf{x}_{i}+\delta_{i}) )\end{array}\right)\] (11)

\[\widehat{\nabla}_{\mathbf{w}}R(\mathbf{w},\delta,\mathcal{D})=\widehat{\nabla }_{\mathbf{w}}g(\mathbf{w})^{\top}\bigg{(}\frac{1}{u_{2}},\frac{-u_{1}-u_{2}} {(u_{2})^{2}},\frac{1}{u_{3}}\bigg{)}^{\top}.\] (12)

Finally, we are ready to present the detailed steps of the proposed algorithm for solving AdAP_LZ in (9) in Algorithm 1, which employs PGD for generating adversarial attacks and stochastic compositional optimization techniques for updating the model parameter \(\mathbf{w}\).

## 5 Experiments

In this section, we perform extensive experiments to evaluate the proposed approaches against white-box and black-box attacks on diverse imbalanced datasets. In order to provide a comprehensive understanding of our methods, we conduct experiments from three other perspectives: (1) trade-off between robustness and average precision; (2) a close look at the effect of the adversarial perturbations; (3) ablation study of different strategies for adversarial AP maximization.

**Datasets.** We conduct experiments on four distinct datasets sourced from various domains. These encompass CIFAR-10 and CIFAR-100 datasets [22], CelebA dataset [26] and the BDD100K dataset [57]. For CIFAR-10 and CIFAR100, we adopt one versus-all approach to construct imbalanced classification tasks. It should be noted that even for clean data, the task of distinguishing between 1 positive class and 99 negative classes in CIFAR100 is challenging. Hence, in our experiments, we employ the 20 superclass labels, of which we choose the first 10 superclasses for constructing 10 one-vs-all binary classification tasks to verify the adversarial robustness on the CIFAR100 dataset. We split the training dataset into train/validation sets at 80%/20% ratio, and use the testing dataset for testing. CelebA dataset is a large-scale face attributes dataset that contains over 200,000 images of celebrities, each of which is annotated with 40 facial attributes. In our experiments, we choose two attributes with the highest imbalanced ratio, i.e., Gray_hair and Mustache, to show the superiority of our method. We adopt the recommended training/validation/testing split. BDD100K dataset is a large-scale diverse driving video dataset, which also has collected image-level annotation on six weather conditions and six scene types. In our experiments, we choose two kinds of weather conditions, i.e., Rainy and Cloudy, and two kinds of scene types, i.e., Tunnel and Residential, which are more imbalanced than others. Since the official testing dataset is not handy, we take the official validation set as the testing data, and split the training dataset into train/validation sets at 80%/20% ratio. The statistics for each task in datasets are presented in Table 6 in the appendix.

**Baselines.** In all experiments, we compare our methods (AdAP_LN, AdAP_LPN) with the following baseline methods: (1) PGD [27], which solves a MiniMax objective directly to enhance the adversarial robustness in terms of accuracy; (2) TRADES [59], which considers the trade-off between robustness and accuracy by minimizing a form of regularized surrogate loss; (3) MART [53], which explores the effect of misclassified examples to the robustness of adversarial training. Furthermore, we include two normal training methods, namely CE minimization and AP maximization by [34], as references.

**Experimental Details.** We employ the ResNet18 [19] as the backbone network in our experiments. This choice is based on the fact that all the tasks involved in our study are binary classifications, and ResNet18 is considered to be expressive enough for the purpose. For all methods, with mini-batch size as 128, we tune learning rate in {1e-3,1e-4,1e-5} with standard Adam optimizer. We set the weight decay to 2e-4 for the CIFAR10 and CIFAR100 datasets and 1e-5 for the CelebA and BDD100k datasets. In the case of the CIFAR10 and CIFAR100 datasets, we run each method for a total of 60 epochs. For the CelebA and BDD100k datasets, we run each method for 32 epochs. The learning rate decay happens at 50% and 75% epochs by a factor of 10. For MART, AdAP_LN and AdAP_LPN, we tune the regularization parameter \(\lambda\) in {0.1, 0.4, 0.8, 1, 4, 8, 10}. For TRADES, we tune the regularization parameter in {1, 4, 8, 10, 40, 80, 100}, since they favor larger weights to obtain better robustness. In addition, for AdAP_LN and AdAP_LPN, we tune its moving average parameters \(\gamma_{1},\gamma_{2}\) in {0.1, 0.9}. Similarly, we tune the moving average parameters \(\gamma_{1}\) for AP maximization in {0.1, 0.9}. We set margin parameter in the surrogate loss of AP as 0.6 for all methods that use the AP surrogate loss. For all adversarial training methods, we apply 6 projected gradient ascent steps to generate adversarial samples in the training stage, and the step size is 0.01. We choose \(L_{\infty}\) norm to bound the perturbation within the limit of \(\epsilon=8/255\), as it is commonly used in the literature.

All the models in our experiments are trained from scratch, except direct AP maximization which is fine-tuned on a pretrained CE model as in [34]. For each experiment, we repeat three times with different random seeds, then report average and standard deviation.

### Robustness against White-box Attacks

In this part, we evaluate the robustness of all models against PGD and APGD [12] white box attacks that have full access to model parameters. Specifically, we utilized 10-step PGD and APGD attack to generate adversarial perturbations constrained by the same perturbation limit \(\epsilon=8/255\). Given that APGD is a step size-free method, we set the step size for PGD to 0.007. For the adversarial training methods, hyperparameters and models are chosen based on the robust average precision (AP) metric on validation datasets, and their corresponding performances on test datasets are reported. For normal training methods, models are chosen based on the clean average precision. The results are presented in Table 2 and 3. Since we run all the classes of CIFAR10 and the first 10 classes of CIFAR100 to verify the effectiveness of our method, we report the mean average precision over the ten classes in the tables, and the performance on each class is shown in the appendix.

From Table 2 and 3, we can observe that our proposed methods outperform other baselines consistently by a large margin while maintaining notably higher clean AP scores. It is striking to see that our methods improve robust AP by \(2\sim 7\) percent on all datasets, compared with adversarial training methods which are concerning accuracy. The results also show that normal training methods (CE min. and AP max.) are vulnerable to adversarial attacks, while they can achieve pretty high clean AP. This is consistent with the observation from other literature [46; 16]. When comparing results in Table 3 with that in Table 2, we can see that APGD exhibits stronger attacks than PGD, as demonstrated in [12]. However, the superiority of our proposed methods still remains evident.

### Robustness against Black-box Attacks

In this part, we examine all the models against adversarial black-box attacks. To achieve this, we utilize the model trained with CE loss minimization on clean datasets as an attacker model. So we do not include the performance of models of CE loss minimization here. With the well-trained model from CE loss minimization, we craft adversarial test images by PGD attack with 20 iterations and step size is 0.003, under perturbation limit \(\epsilon=8/255\). The models evaluated here are the same model evaluated in 5.1. The results are summarized in Table 4. Results show that our method exhibits significantly superior robustness against black-box attacks on all tasks, with evident advantages against white-box attacks shown in 5.1. From Table 4, we can also observe that the normal training method, i.e., AP maximization, is also susceptible when confronting adversarial black-box attacks.

### Trade-off between Robustness and AP

As argued in various research studies [59; 40; 56], there exists a trade-off between accuracy and adversarial robustness. That is saying when one model boosts its adversarial performance, it may result in a decrease in its clean performance. In this part, we aim to study the trade-off between robustness and average precision and provide clear visualizations of the phenomenon. To accomplish this, we tune the weight of the regularization for the regularization-based approaches, i.e. TRADES, MART, AdAP_LN and AdAP_LPN, then show how their robust AP and clean AP changes. We evaluate the models at the last epoch to ensure that all methods reach a convergence point. For TRADES, AdAP_LN and AdAP_LPN, we tune the weight introduced in Experimental Details part, as well as 0.01, to better illustrate the trade-off trend. We also include non-regularization-based approaches as a point in this graph.

Based on Figure 2, we can observe that for TRADES, AdAP_LN and AdAP_LPN, as the weight of regularization increases, the clean AP decreases while the robust AP increases at first, which is consistent with the observation in [59]. But for MART, the trend is not clear as it is not a trade-off based approach. However, as the weight of regularization continues increasing, both the clean AP and robust AP decrease. This is because when the model places excessive emphasis on the regularization term, it may overlook the actual objective. Notably, our proposed methods place more towards the upper-right than other baselines, which indicates that our method is able to achieve better both robust and clean AP simultaneously.

### Visualizing the Behavior after Adversarial Perturbations

To gain a deeper understanding of our approach, we have a close look at how defense models' predictions change after introducing adversarial perturbations. To be more specific, we compare our AdAP_LN method with another robust baseline, TRADES, to examine how their predictions are 

[MISSING_PAGE_FAIL:9]

run all these five approaches on Cifar10, CIFAR100 and BDD100K datasets. The experimental settings and the hyperparameters for AdAP_LN and AdAP_LPN are the same as those in 5.1. The hyperparameters we tune for AdAP_LZ are the same as those for AdAP_LN. For AdAP_PZ, we tune the weight parameter in {1, 4, 8, 10, 40, 80, 100}. For AdAP_MM, we tune \(\gamma_{1}\) in {0.1,0.9}. The results are presented in Table 5. We can see that (i) robust regularization approaches are better than the minimax approach (AdAP_MM) (ii) the non-zero-sum game approaches (AdAP_LPN, AdAP_LN) are usually better than the zero-sum game approach (AdAP_LZ); (iii) combining listwise and pointwise adversarial regularization in AdAP_LPN could bring significant boost in term of both robust AP and clean AP.

**Insensitivity to Batch Size.** We investigate the proposed approximation method for \(g(\mathbf{w})\) by varying the mini-batch size for AdAP_LN algorithm and report results in Figure 4 in the appendix. We can see that AdAP_LN does not require a very large batch size and is generally not sensitive to the mini-batch size, which implies the effectiveness of our approximation method.

**Sensitivity to \(\gamma_{1},\gamma_{2}\).** We study the sensitivity of the hyper-parameters \(\gamma_{1},\gamma_{2}\) for proposed AdAP_LN algorithm. From the results in Table 8 in the appendix, we can observe that \(\gamma_{1},\gamma_{2}\) have a significant impact on the performance. However, when we tune \(\gamma_{1},\gamma_{2}\) in {\(0.1,0.9\)}, it is able to achieve relatively good performance but not always the optimal one.

**Empirical convergence analysis.** We report the convergence curves of proposed AdAP_LN algorithm in Figure5 in the appendix, which demonstrates the convergence of the algorithm.

**Time efficiency.** We compare the training time efficiency of the proposed method with different algorithms in Table 7 in the appendix. We can observe that (i) adversarial training methods are generally more time-consuming than natural training; (ii) our proposed AdAP_LN and AdAP_LPN methods cost a little more time than traditional PGD method but much less time than TRADES.

## 6 Conclusions

In this paper, we have proposed a novel solution for maximizing average precision with adversarial ranking robustness. The proposed formulation is robust in terms of listwise ranking performance against individual adversarial perturbations and is able to trade off between average precision on clean data and adversarial ranking robustness. We have developed an efficient stochastic algorithm to solve the resulting objective. Extensive experimental results on various datasets demonstrate that the proposed method can achieve significantly improved adversarial robustness in terms of AP, compared with other strong adversarial training baselines. It would be interesting to extend our method to construct more robust systems with average precision as the objective, such as object detection systems and medical diagnosis systems.

## Acknowledgements

We thank anonymous reviewers for constructive comments. G. Li and T. Yang were partially supported by NSF Career Award 2246753, NSF Grant 2246757, NSF Grant 2246756, NSF Grant 2306572, and GM gift funding.

Figure 3: Visualization of modelsâ€™ predictions after perturbation

Figure 2: Visualization of the trade-off between robustness and average precision. For the sake of clarity in these figures, we defer the detailed values to Table 9 in the appendix to additionally demonstrate the correlation between robust AP/clean AP and \(\lambda\).

## References

* [1]S. Addepalli, V. BS, A. Baburaj, G. Sriramananan, and R. P. Babu (2020) Towards achieving adversarial robustness by enforcing feature consistency across bit planes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1020-1029. Cited by: SS1.
* [2]T. Bai, J. Luo, J. Zhao, B. Wen, and Q. Wang (2021) Recent advances in adversarial training for adversarial robustness. CoRRabs/2102.01356. Cited by: SS1.
* [3]K. Boyd, K. H. Eng, and C. D. Page (2013) Area under the precision-recall curve: point estimates and confidence intervals. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13, pp. 451-466. Cited by: SS1.
* [4]A. Brown, W. Xie, V. Kalogeiton, and A. Zisserman (2020) Smooth-ap: smoothing the path towards large-scale image retrieval. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IX 16, pp. 677-694. Cited by: SS1.
* [5]C. Burges, R. Ragno, and Q. Le (2006) Learning to rank with nonsmooth cost functions. Advances in neural information processing systems19. Cited by: SS1.
* [6]F. Cakir, K. He, X. Xia, B. Kulis, and S. Sclaroff (2019) Deep metric learning to rank. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1861-1870. Cited by: SS1.
* [7]Z. Cao, T. Qin, T. Liu, M. Tsai, and H. Li (2007) Learning to rank: from pairwise approach to listwise approach. In Proceedings of the 24th international conference on Machine learning, pp. 129-136. Cited by: SS1.
* [8]N. Carlini and D. Wagner (2017) Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp), pp. 39-57. Cited by: SS1.
* [9]K. Chen, J. Li, W. Lin, J. See, J. Wang, L. Duan, Z. Chen, C. He, and J. Zou (2019) Towards accurate one-stage object detection with ap-loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5119-5127. Cited by: SS1.
* [10]W. Chen, T. Liu, Y. Lan, Z. Ma, and H. Li (2009) Ranking measures and loss functions in learning to rank. Advances in Neural Information Processing Systems22. Cited by: SS1.
* [11]J. Cohen, E. Rosenfeld, and Z. Kolter (2019) Certified adversarial robustness via randomized smoothing. In international conference on machine learning, pp. 1310-1320. Cited by: SS1.
* [12]F. Croce and M. Hein (2020) Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206-2216. Cited by: SS1.
* [13]E. Eban, M. Schain, A. Mackey, A. Gordon, R. Rifkin, and G. Elidan (2017) Scalable learning of non-decomposable objectives. In Artificial intelligence and statistics, pp. 832-840. Cited by: SS1.
* [14]L. Engstrom, A. Ilyas, and A. Athalye (2018) Evaluating and understanding the robustness of adversarial logit pairing. arXiv preprint arXiv:1807.10272. Cited by: SS1.
* [15]I. Goodfellow, J. Shlens, and C. Szegedy (2015) Explaining and harnessing adversarial examples. In International Conference on Learning Representations, Cited by: SS1.
* [16]I. J. Goodfellow, J. Shlens, and C. Szegedy (2014) Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572. Cited by: SS1.
* [17]C. Guo, M. Rana, M. Cisse, and L. Van Der Maaten (2017) Countering adversarial images using input transformations. arXiv preprint arXiv:1711.00117. Cited by: SS1.

* Gupta and Rahtu [2019] Puneet Gupta and Esa Rahtu. Cidefence: Defeating adversarial attacks by fusing class-specific image inpainting and image denoising. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 6708-6717, 2019.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hou et al. [2022] Wenzheng Hou, Qianqian Xu, Zhiyong Yang, Shilong Bao, Yuan He, and Qingming Huang. Adauc: End-to-end adversarial auc optimization against long-tail problems. In _International Conference on Machine Learning_, pages 8903-8925. PMLR, 2022.
* Kannan et al. [2018] Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. _arXiv preprint arXiv:1803.06373_, 2018.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Kurakin et al. [2018] Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In _Artificial intelligence safety and security_, pages 99-112. Chapman and Hall/CRC, 2018.
* Liu et al. [2019] Jiayang Liu, Weiming Zhang, Yiwei Zhang, Dongdong Hou, Yujia Liu, Hongyue Zha, and Nenghai Yu. Detection based defense against adversarial examples from the steganalysis point of view. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4825-4834, 2019.
* Liu et al. [2019] Zihao Liu, Qi Liu, Tao Liu, Nuo Xu, Xue Lin, Yanzhi Wang, and Wujie Wen. Feature distillation: Dnn-oriented jpeg compression against adversarial examples. In _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 860-868. IEEE, 2019.
* Liu et al. [2015] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_, December 2015.
* Madry et al. [2018] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.
* Mohapatra et al. [2014] Pritish Mohapatra, CV Jawahar, and M Pawan Kumar. Efficient optimization for average precision svm. _Advances in Neural Information Processing Systems_, 27, 2014.
* Mohapatra et al. [2018] Pritish Mohapatra, Michal Rolinek, CV Jawahar, Vladimir Kolmogorov, and M Pawan Kumar. Efficient optimization for rank-based loss functions. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 3693-3701, 2018.
* Moosavi-Dezfooli et al. [2016] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2574-2582, 2016.
* Narasimhan et al. [2019] Harikrishna Narasimhan, Andrew Cotter, and Maya Gupta. Optimizing generalized rate metrics with three players. _Advances in Neural Information Processing Systems_, 32, 2019.
* Papernot et al. [2016] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In _2016 IEEE European symposium on security and privacy (EuroS&P)_, pages 372-387. IEEE, 2016.
* Qi et al. [2021] Qi Qi, Zhishuai Guo, Yi Xu, Rong Jin, and Tianbao Yang. An online method for a class of distributionally robust optimization with non-convex objectives. _Advances in Neural Information Processing Systems_, 34:10067-10080, 2021.
* Qi et al. [2021] Qi Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, and Tianbao Yang. Stochastic optimization of areas under precision-recall curves with provable convergence. _Advances in Neural Information Processing Systems_, 34:1752-1765, 2021.

* [35] Zhuang Qian, Kaizhu Huang, Qiu-Feng Wang, and Xu-Yao Zhang. A survey of robust adversarial training in pattern recognition: Fundamental, theory, and methodologies, 2022.
* [36] Tao Qin, Tie-Yan Liu, and Hang Li. A general approximation framework for direct optimization of information retrieval measures. _Information retrieval_, 13:375-397, 2010.
* [37] Yao Qin, Nicholas Frosst, Sara Sabour, Colin Raffel, Garrison Cottrell, and Geoffrey Hinton. Detecting and diagnosing adversarial images with class-conditional capsule reconstructions. _arXiv preprint arXiv:1907.02957_, 2019.
* [38] Yuxian Qiu, Jingwen Leng, Cong Guo, Quan Chen, Chao Li, Minyi Guo, and Yuhao Zhu. Adversarial defense through network profiling based path extraction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4777-4786, 2019.
* [39] Edward Raff, Jared Sylvester, Steven Forsyth, and Mark McLean. Barrage of random transforms for adversarially robust defense. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6528-6537, 2019.
* [40] Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding and mitigating the tradeoff between robustness and accuracy. _arXiv preprint arXiv:2002.10716_, 2020.
* [41] Alexander Robey, Luiz Chamon, George J Pappas, Hamed Hassani, and Alejandro Ribeiro. Adversarial robustness with semi-infinite constrained learning. _Advances in Neural Information Processing Systems_, 34:6198-6215, 2021.
* [42] Michal Rolinek, Vit Musil, Anselm Paulus, Marin Vlastelica, Claudio Michaelis, and Georg Martius. Optimizing rank-based metrics with blackbox differentiation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7620-7630, 2020.
* [43] Cynthia Rudin. The p-norm push: A simple convex ranking algorithm that concentrates at the top of the list. 2009.
* [44] Samuel Henrique Silva and Peyman Najafirad. Opportunities and challenges in deep learning adversarial robustness: A survey. _CoRR_, abs/2007.00753, 2020.
* [45] Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, et al. Guided adversarial attack for evaluating and enhancing adversarial defenses. _Advances in Neural Information Processing Systems_, 33:20297-20308, 2020.
* [46] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _arXiv preprint arXiv:1312.6199_, 2013.
* [47] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In _International Conference on Learning Representations_, 2014.
* [48] Guanhong Tao, Shiqing Ma, Yingqi Liu, and Xiangyu Zhang. Attacks meet interpretability: Attribute-steered detection of adversarial samples. _Advances in Neural Information Processing Systems_, 31, 2018.
* [49] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. _arXiv preprint arXiv:1805.12152_, 2018.
* [50] Guanghui Wang, Ming Yang, Lijun Zhang, and Tianbao Yang. Momentum accelerates the convergence of stochastic auprc maximization. In _International Conference on Artificial Intelligence and Statistics_, pages 3753-3771. PMLR, 2022.
* [51] Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions. _Mathematical Programming_, 161:419-449, 2017.

* [52] Wentao Wang, Han Xu, Xiaorui Liu, Yaxin Li, Bhavani Thurasingham, and Jiliang Tang. Imbalanced adversarial training with reweighting. In _2022 IEEE International Conference on Data Mining (ICDM)_, pages 1209-1214, 2022.
* [53] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In _International Conference on Learning Representations_, 2020.
* [54] Zhengyang Wang, Meng Liu, Youzhi Luo, Zhao Xu, Yaochen Xie, Limei Wang, Lei Cai, Qi Qi, Zhuoning Yuan, Tianbao Yang, et al. Advanced graph and sequence neural networks for molecular property prediction and drug discovery. _Bioinformatics_, 38(9):2579-2586, 2022.
* [55] Peisong Wen, Qianqian Xu, Zhiyong Yang, Yuan He, and Qingming Huang. Exploring the algorithm-dependent generalization of auprc optimization with list stability. _Advances in Neural Information Processing Systems_, 35:28335-28349, 2022.
* [56] Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaudhuri. A closer look at accuracy vs. robustness. _Advances in neural information processing systems_, 33:8588-8601, 2020.
* [57] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2636-2645, 2020.
* [58] Jianhe Yuan and Zhihai He. Ensemble generative cleaning with feedback loops for defending adversarial attacks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 581-590, 2020.
* [59] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In _International conference on machine learning_, pages 7472-7482. PMLR, 2019.
* [60] Mo Zhou, Zhenxing Niu, Le Wang, Qilin Zhang, and Gang Hua. Adversarial ranking attack and defense. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIV 16_, pages 781-799. Springer, 2020.

More Experimental Results

All experiments in our paper are run across 16 NVIDIA A10 GPUs and 10 NVIDIA A30 GPUs. We present the statistics for each task in our experiments in Table 6.

### Time efficiency of our proposed algorithm

Time efficiency is typically a concern for adversarial training techniques. To order to investigate the time efficiency of our proposed method, we've conducted some experiments to show time efficiency comparison with different algorithms. In the experiment, we set the parameters, which could affect training time, exactly the same(e.g. batch size as 128, total epochs as 60, adversarial samples are generated with 6 projected gradient ascent steps) and run all the models over three times on the Class_0 task of CIFAR10. From the table 7, we can observe that (i) adversarial training methods are generally more time-consuming than natural training; (ii) our proposed AdAP_LN and AdAP_LPN methods cost a little more time than traditional PGD method but much less time than TRADES. This is because, to generate adversarial samples in training, TRADES is solving the maximization of KL divergence between the probabilities predicted with clean data and perturbed data(i.e. \(\max_{\|\delta\|\leq\epsilon}\sum_{k}h(x)_{k}\log h(x)_{k}-h(x)_{k}\log h(x+ \delta)_{k}\), where \(h(x)_{k}\) and \(h(x+\delta)_{k}\) are predicted probabilities for class k on clean data and perturbed data respectively, which requires two forward propagations and one backpropagation in each projected gradient ascent step. However, PGD and our proposed methods are directly solving maximization of Cross Entropy(i.e. \(\max_{\|\delta\|\leq\epsilon}-\log h(x+\delta)_{y}\)), which only needs one forward propagation and one backpropagation in each projected gradient ascent step. In adversarial training, since each gradient descent step w.r.t w requires multiple gradient ascent steps w.r.t \(\delta\), the computational expense primarily stems from the projected gradient ascent steps, which can be also observed by comparing the efficiency of CE Min. with PGD.

### Sensitivity of proposed algorithm to batch size

In our proposed algorithm, one essential step is the approximation of \(g(\mathbf{w})\). Due to the non-linear dependence of \(\nabla_{\mathbf{w}}R(\mathbf{w},\delta,\mathcal{D})\) on \(g(\mathbf{w})\), we cannot simply use their mini-batch estimator in the calculation as it will yield a large optimization error [51]. Instead, we maintain a vector \(\mathbf{u}=[u_{1},u_{2},u_{3}]\) for tracking \([g_{1}(\mathbf{w}),g_{2}(\mathbf{w}),g_{3}(\mathbf{w})]\) to reduce the optimization error. The vector \(\mathbf{u}\) is updated by Equation (11). To offer more empirical results on this, we have conducted some experiments to investigate the proposed AdAP_LN's sensitivity to batch sizes on CIFAR10 dataset. The results are shown in Figure 4. We can observe that our method does not require a very large batch size to achieve a good performance and is generally not sensitive to batch size, which implies the effectiveness of our approximation method.

\begin{table}
\begin{tabular}{l l l l} Dataset & Train & Validation & Test \\ \hline CIFAR-10 & 40000 (10\%) & 10000 (10\%) & 10000 (10\%) \\ CIFAR-100 & 40000 (5\%) & 10000 (5\%) & 10000 (5\%) \\ \hline BDD100K(tunned) & 55890 (0.18\%) & 13973 (0.19\%) & 10000 (0.27\%) \\ BDD100K(residential) & 55890 (11.56\%) & 13973 (11.56\%) & 10000 (12.53\%) \\ BDD100K(rainy) & 55890 (7.26\%) & 13973 (7.26\%) & 10000 (7.38\%) \\ BDD100K(cloudy) & 55890 (6.99\%) & 13973 (6.98\%) & 10000 (7.38\%) \\ \hline CelebA(gray\_hair) & 162770 (4.24\%) & 19867 (4.87\%) & 19962 (3.19\%) \\ CelebA(mustache) & 162770 (4.08\%) & 19867 (5.05\%) & 19962 (3.87\%) \\ \hline \end{tabular}
\end{table}
Table 6: Datasets statistics. The percentage in parenthesis represents the proportion of positive samples.

\begin{table}
\begin{tabular}{l|c c c c} \hline Methods & Run 1 & Run 2 & Run 3 & Average \\ \hline CE Min. & 563 & 5668 & 5685 & 565.67 \\ AP Max. & 5598 & 5908 & 5988 & 559.33\% \\ \hline PGD & 28335 & 28048 & 2803s & 2813.33\% \\ TRADES & 42038 & 4182s & 4179s & 4188.00s \\ MAART & 31928 & 32055 & 31948 & 3197.00s \\ \hline AdAP\_LN & 3227s & 3213s & 3211s & 3217.00s \\ AdAP\_LPN & 3234s & 3219s & 3218s & 3223.67s \\ \hline \end{tabular}
\end{table}
Table 7: Training efficiency comparison 

### Sensitivity of proposed algorithm to \(\gamma_{1}\) and \(\gamma_{2}\)

In our main experiments, the hyper-parameters \(\gamma_{1},\gamma_{2}\) for our proposed algorithms are tuned in \(\{0.1,0.9\}\) as mentioned in Section 5. To study the sensitivity of these parameters, we conducted some more experiments on CIFAR10 and BDD100K datasets. The results are summarized in Table 8. From the table, we can observe that \(\gamma_{1},\gamma_{2}\) have a significant impact on the performance. However, when we tune \(\gamma_{1},\gamma_{2}\) in \(\{0.1,0.9\}\), it is able to achieve relatively good performance but not always the optimal one.

### Empirical convergence analysis

Since the theoretical convergence analysis of our proposed algorithm is very challenging, deriving the convergence would be a significant work by itself. Instead, we provides some empirical results to show the convergence of the proposed algorithm. Specifically, we set \(\lambda=1,\gamma_{1}=0.1,\gamma_{2}=0.1\) and run AdAP_LN algorithm on CIFAR10 dataset and BDD100K dataset for a total of 120 epochs and 80 epochs, respectively. We evaluate the training loss after each epoch and report the loss values. We present the AP loss (i.e., \(P(\mathbf{w})\) in Equation 9) and regularization term ( i.e., \(R(\mathbf{w},\delta,D)\) in Equation 9) separately, as well as the summation of the two losses. For each experiment, we repeat three times with different random seeds. The results are shown in Figure 5, which demonstrates the convergence of our algorithm.

Figure 4: Illustration for insensitivity of AdAP_LN to batch size

Figure 5: Training convergence curves on CIFAR10 and BDD100K dataset

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

Figure 8: Visualization of the trade-off between robustness and average precision on BDD100K dataset

Figure 6: Visualization of the trade-off between robustness and average precision on CIFAR10 dataset

Figure 7: Visualization of the trade-off between robustness and average precision on CIFAR100 dataset

Figure 9: Visualization of the trade-off between robustness and average precision on CelebA dataset

[MISSING_PAGE_FAIL:20]

where \(\gamma_{1}\in(0,1)\) is a parameter. And the stochastic gradient \(\widehat{\nabla}_{\mathbf{w}}P(\mathbf{w},\delta)\) is computed by:

\[\widehat{\nabla}_{\mathbf{w}}P(\mathbf{w},\delta)=\frac{1}{|\mathcal{B}^{+}|} \sum_{\mathbf{x}_{i}\in\mathcal{B}^{+}}\sum_{\mathbf{x}_{j}\in\mathcal{B}}\frac {(\mathbf{u}_{\mathbf{x}_{i}}^{\delta_{1}}-\mathbf{u}_{\mathbf{x}_{i}}^{ \delta_{2}}\mathbb{I}(\mathbf{y}_{j}=1))\nabla_{\mathbf{w}}\ell(\mathbf{w}, \mathbf{x}_{j}+\delta_{j},\mathbf{x}_{i}+\delta_{i})}{|\mathcal{B}|(\mathbf{u} _{\mathbf{x}_{i}}^{\delta_{2}})^{2}}.\] (17)

We present the whole algorithm for solving Adap_MM in (14) in Algorithm 2.

The objective for Adap_LZ is defined as

\[\min_{\mathbf{w}}\max_{\delta\in\Omega^{n}}P(\mathbf{w})+R(\mathbf{w},\delta, \mathcal{D})=\min_{\mathbf{w}}\big{\{}P(\mathbf{w})+\max_{\delta\in\Omega^{n} }R(\mathbf{w},\delta,\mathcal{D})\big{\}}\] (18)

Since the listwise adversarial regularization includes all the samples in the dataset, the optimization for \(\delta\) is also challenging. We extend the approach for computing an estimator of \(\nabla_{\mathbf{w}}R(\mathbf{w},\delta,\mathcal{D})\) employed in Section 4.2 to estimate \(\nabla_{\delta}R(\mathbf{w},\delta,\mathcal{D})\). To avoid redundancy, we give the estimate directly by

\[\widehat{\nabla}_{\delta}R(\mathbf{w},\delta,\mathcal{D})=\widehat{\nabla}_{ \delta}g(\delta;\mathbf{w})^{\top}\bigg{(}\frac{1}{u_{2}},\frac{-u_{1}-u_{2}}{ (u_{2})^{2}},\frac{1}{u_{3}}\bigg{)}^{\top},\] (19)

where \(g(\delta;\mathbf{w})\) has the same definition as \(g(\mathbf{w})\) in Section 4.2, \(\mathbf{u}=[u_{1},u_{2},u_{3}]\) denotes a vector for tracking \([g_{1}(\mathbf{w}),g_{2}(\mathbf{w}),g_{3}(\mathbf{w})]\). The whole algorithm for solving Adap_LZ is presented in Algorithm 5.

```
1:Initialize \(\mathbf{w}\), \(\mathbf{u}_{\mathbf{x}}^{\delta_{1}},\mathbf{u}_{\mathbf{x}}^{\delta_{2}},\gamma_ {1}\)
2:for\(t=1,\ldots,T\)do
3: Draw a batch of \(B_{+}\) positive samples denoted by \(\mathcal{B}_{+}\).
4: Draw a batch of \(B\) samples denoted by \(\mathcal{B}\).
5: For \(\mathbf{x}_{i}\in\mathcal{B}\cup\mathcal{B}_{+}\), initialize \(\delta_{i}\sim\alpha\cdot\mathcal{N}(0,1)\)
6:for\(m=1,\ldots,M\)do
7: For each \(\mathbf{x}_{i}\in\mathcal{B}_{+}\), update \(\mathbf{u}_{\mathbf{x}_{i}}^{\delta_{1}}\) and \(\mathbf{u}_{\mathbf{x}_{i}}^{\delta_{2}}\) by Equation (16)
8: Compute \(\widehat{\nabla}_{\delta}P(\mathbf{w}_{t},\delta)\) by Equation (15)
9: Update \(\delta_{i}=\Pi_{\|\cdot\|\leq\epsilon}(\delta_{i}+\eta_{2}\cdot sign(\widehat {\nabla}_{\delta_{i}}P(\mathbf{w}_{t},\delta)))\), where \(\Pi_{\Omega}(\cdot)\) is the projection operator.
10:endfor
11: For each \(\mathbf{x}_{i}\in\mathcal{B}_{+}\), update \(\mathbf{u}_{\mathbf{x}_{i}}^{\delta_{1}}\) and \(\mathbf{u}_{\mathbf{x}_{i}}^{\delta_{2}}\) by Equation (16)
12: Compute stochastic gradient estimator \(\nabla_{\mathbf{w}_{t}}=\widehat{\nabla}_{\mathbf{w}_{t}}P(\mathbf{w}_{t}, \delta)\) by Equation (17)
13: Update \(\mathbf{w}_{t+1}\) by using SGD, momentum-methods or Adam.
14:endfor ```

**Algorithm 2** Stochastic Algorithm for solving Adap_MM in (14)

```
1:Initialize \(\mathbf{w}\), \(\mathbf{u}_{\mathbf{x}}^{\delta_{1}},\mathbf{u}_{\mathbf{x}}^{\delta_{2}},\gamma_ {1}\)
2:for\(t=1,\ldots,T\)do
3: Draw a batch of \(B_{+}\) positive samples denoted by \(\mathcal{B}_{+}\).
4: Draw a batch of \(B_{+}\) positive samples denoted by \(\mathcal{B}_{+}\).
5: Draw a batch of \(B_{+}\) positive samples denoted by \(\mathcal{B}\).
6: Draw a batch of \(B\) samples denoted by \(\mathcal{B}\).
7:for\(\mathbf{x}_{i}\in\mathcal{B}\)do
8: Initialize \(\delta_{i}\sim\alpha\cdot\mathcal{N}(0,1)\)
9:for\(m=1,\ldots,M\)do
10: Update \(\delta_{i}=\Pi_{\|\cdot\|\leq\epsilon}(\delta_{i}+\eta_{2}\cdot sign(\nabla_{ \delta_{i}}G(\mathbf{w}_{t},\mathbf{x}_{i}+\delta_{i},y_{i})))\), where \(\Pi_{\Omega}(\cdot)\) is the projection operator.
11:endfor
12:endfor
13: For each \(\mathbf{x}_{i}\in\mathcal{B}_{+}\), update \(\mathbf{u}_{\mathbf{x}_{i}}^{1}\) and \(\mathbf{u}_{\mathbf{x}_{i}}^{2}\) by Equation (2)
14: Update \(\mathbf{u}\) by Equation (11) and compute \(\widehat{\nabla}_{\mathbf{w}_{t}}R(\mathbf{w}_{t},\delta,\mathcal{D})\) by Equation (12)
15: Compute stochastic gradient estimator: \[\nabla_{\mathbf{w}_{t}}=\widehat{\nabla}_{\mathbf{w}_{t}}P(\mathbf{w}_{t})+ \lambda(\widehat{\nabla}_{\mathbf{w}_{t}}R(\mathbf{w}_{t},\delta,\mathcal{D})+ \frac{1}{B}\sum_{\mathbf{x}_{i}\in\mathcal{B}}L(h_{\mathbf{w}_{t}}(\mathbf{x}_{i} ),h_{\mathbf{w}_{t}}(\mathbf{x}_{i}+\delta_{i})))\]
16: Update \(\mathbf{w}_{t+1}\) by using SGD, momentum-methods or Adam.
17:endfor ```

**Algorithm 3** Stochastic Algorithm for solving Adap_LPN in (10)

```
1:Initialize \(\mathbf{w},\mathbf{u}^{1}_{\mathbf{x}},\mathbf{u}^{2}_{\mathbf{x}},\mathbf{u},\gamma_{1}\)
2:for\(t=1,\ldots,T\)do
3: Draw a batch of \(B_{+}\) positive samples denoted by \(\mathcal{B}_{+}\).
4: Draw a batch of \(B\) samples denoted by \(\mathcal{B}\).
5:for\(\mathbf{x}_{i}\in\mathcal{B}\)do
6: Initialize \(\delta_{i}\sim\alpha\cdot\mathcal{N}(0,1)\)
7:for\(m=1,\ldots,M\)do
8: Update \(\delta_{i}=\Pi_{\|\cdot\|\leq\epsilon}(\delta_{i}+\eta_{2}\cdot sign(\nabla_{ \delta_{i}}L(h_{\mathbf{w}_{t}}(\mathbf{x}_{i}),h_{\mathbf{w}_{t}}(\mathbf{x}_ {i}+\delta_{i}))))\), where \(\Pi_{\Omega}(\cdot)\) is the projection operator.
9:endfor
10:endfor
11: For each \(\mathbf{x}_{i}\in\mathcal{B}_{+}\), update \(\mathbf{u}^{1}_{\mathbf{x}_{i}}\) and \(\mathbf{u}^{2}_{\mathbf{x}_{i}}\) by Equation (2)
12: Update \(\mathbf{u}\) by Equation (11) and compute \(\widehat{\nabla}_{\mathbf{w}_{t}}P(\mathbf{w}_{t})\) by Equation (3)
13: Compute stochastic gradient estimator \[\nabla_{\mathbf{w}_{t}}=\widehat{\nabla}_{\mathbf{w}_{t}}P(\mathbf{w}_{t})+ \frac{\lambda}{B}\sum_{\mathbf{x}_{i}\in\mathcal{B}}L(h_{\mathbf{w}_{t}}( \mathbf{x}_{i}),h_{\mathbf{w}_{t}}(\mathbf{x}_{i}+\delta_{i}))\]
14: Update \(\mathbf{w}_{t+1}\) by using SGD, momentum-methods or Adam.
15:endfor ```

**Algorithm 4** Stochastic Algorithm for solving Adap_PZ in (7)

```
1:Initialize \(\mathbf{w},\mathbf{u}^{1}_{\mathbf{x}},\mathbf{u}^{2}_{\mathbf{x}},\mathbf{u}, \gamma_{1},\gamma_{2}\)
2:for\(t=1,\ldots,T\)do
3: Draw a batch of \(B_{+}\) positive samples denoted by \(\mathcal{B}_{+}\).
4: Draw a batch of \(B\) samples denoted by \(\mathcal{B}\).
5: For \(\mathbf{x}_{i}\in\mathcal{B}\), initialize \(\delta_{i}\sim\alpha\cdot\mathcal{N}(0,1)\)
6:for\(m=1,\ldots,M\)do
7: Update \(\mathbf{u}\) by Equation (11) and compute \(\widehat{\nabla}_{\delta}R(\mathbf{w}_{t},\delta,\mathcal{D})\) by Equation (19)
8: Update \(\delta_{i}=\Pi_{\|\cdot\|\leq\epsilon}(\delta_{i}+\eta_{2}\cdot sign(\widehat {\nabla}_{\delta_{i}}R(\mathbf{w}_{t},\delta,\mathcal{D})))\), where \(\Pi_{\Omega}(\cdot)\) is the projection operator.
9:endfor
10: For each \(\mathbf{x}_{i}\in\mathcal{B}_{+}\), update \(\mathbf{u}^{1}_{\mathbf{x}_{i}}\) and \(\mathbf{u}^{2}_{\mathbf{x}_{i}}\) by Equation (2)
11: Update \(\mathbf{u}\) by Equation (11) and compute \(\widehat{\nabla}_{\mathbf{w}_{t}}R(\mathbf{w}_{t},\delta,\mathcal{D})\) by Equation (12)
12: Compute stochastic gradient estimator \(\nabla_{\mathbf{w}_{t}}=\widehat{\nabla}_{\mathbf{w}_{t}}P(\mathbf{w}_{t})+ \lambda\widehat{\nabla}_{\mathbf{w}_{t}}R(\mathbf{w}_{t};\delta)\)
13: Update \(\mathbf{w}_{t+1}\) by using SGD, momentum-methods or Adam.
14:endfor ```

**Algorithm 5** Stochastic Algorithm for solving Adap_LZ