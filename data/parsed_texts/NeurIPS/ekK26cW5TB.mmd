# AUCSeg: AUC-oriented Pixel-level Long-tail Semantic Segmentation

 Boyu Han\({}^{1,2}\) Qianqian Xu\({}^{1,3}\) Zhiyong Yang\({}^{2}\) Shilong Bao\({}^{2}\)

Peisong Wen\({}^{1,2}\) Yangbangyan Jiang\({}^{2}\) Qingming Huang\({}^{2,1,4}\)

\({}^{1}\) Key Lab. of Intelligent Information Processing, Institute of Computing Technology, CAS

\({}^{2}\) School of Computer Science and Tech., University of Chinese Academy of Sciences

\({}^{3}\) Peng Cheng Laboratory

\({}^{4}\) Key Laboratory of Big Data Mining and Knowledge Management, CAS

{hanboyu23z,xuqianqian,wenpeisong20z}@ict.ac.cn,

{yangzhiyong21,baoshilong,jiangyangbangyan,qmhuang}@ucas.ac.cn

Corresponding authors.

###### Abstract

The _Area Under the ROC Curve (AUC)_ is a well-known metric for evaluating instance-level long-tail learning problems. In the past two decades, many AUC optimization methods have been proposed to improve model performance under long-tail distributions. In this paper, we explore AUC optimization methods in the context of pixel-level long-tail semantic segmentation, a much more complicated scenario. This task introduces two major challenges for AUC optimization techniques. On one hand, AUC optimization in a pixel-level task involves complex coupling across loss terms, with structured inner-image and pairwise inter-image dependencies, complicating theoretical analysis. On the other hand, we find that mini-batch estimation of AUC loss in this case requires a larger batch size, resulting in an unaffordable space complexity. To address these issues, we develop a pixel-level AUC loss function and conduct a dependency-graph-based theoretical analysis of the algorithm's generalization ability. Additionally, we design a _Tail-Classes Memory Bank (T-Memory Bank)_ to manage the significant memory demand. Finally, comprehensive experiments across various benchmarks confirm the effectiveness of our proposed AUCSeg method. The code is available at https://github.com/boyuh/AUCSeg.

## 1 Introduction

Semantic segmentation aims to categorize each pixel within an image into a specific class, which is a fundamental task in image processing and computer vision [33, 51, 75]. Over the past decades, substantial efforts [58, 23, 43, 96] have advanced the field of semantic segmentation. The mainstream paradigm is to develop innovative network architectures that encode more discriminative features for dense pixel-level classifications. Typical backbones include CNN-based [58, 14, 80] and newly emerging Transformer-based methods [108, 83, 65, 15, 32], which have achieved the state-of-the-art (SOTA) performance. Beyond this direction, researchers [24, 66, 48, 41] have recently realized the _Pixel-level Long-tail issue in Semantic Segmentation (PLSS)_, as shown at the top of Figure 1. Similar to the flaws of traditional long-tail problems, the major classes will dominate the model learning process, causing the model to overlook the segmentation of minority classes in an image. Several remedies have been proposed to alleviate this [49, 61, 10, 56, 104, 92, 78]. For example, [77] introduces a category-wise variation technique inversely proportional to distribution to achieve balanced segmentation; [66] introduces a sequence-based generative adversarial network for imbalanced medical image segmentation, and [41] develops a re-weighting scheme for semi-supervised segmentation.

Currently, mainstream studies fall into two camps. One is to develop carefully designed backbones for long-tail distributions but leave the effect of loss functions unconsidered. The other is to conduct empirical studies on the loss functions without exploring their theoretical impact on the generalization performance. A question then arises naturally:

_Can we find a theoretically grounded loss function for PLSS on top of SOTA backbones?_

This paper provides an affirmative answer from the AUC perspective and proposes a novel framework called _AUC-oriented Pixel-level Long-tail Semantic Segmentation (AUC-Seg)_. Specifically, AUC indicates the likelihood that a positive sample scores higher than a negative one, which has been proven to be **insensitive** to data distribution [86, 101]. Applying AUC to **instance-level long-tail** classifications has shown promising progress in the machine learning community [86, 100, 88, 68]. Motivated by its success, this paper starts an early trial to study AUC optimization for PLSS. The primary concern is to study its effectiveness for PLSS from a theoretical perspective. The key challenge is that the standard techniques for generalization analysis [62, 8, 21] require the loss function to be expressed as a sum of independent terms. Unfortunately, the proposed loss function does not satisfy this assumption due to the dual effect of structured inner-image dependency and pairwise inter-image dependency. This complicated structure poses a big challenge to understanding its generalization behavior. To address this, we decompose the loss function into inner-image and inter-image terms. On top of this reformulation, we deploy the dependency graph [103] to decouple the interdependency. Finally, we reach a bound of \(\widetilde{\mathcal{O}}\left(\tau\sqrt{\log\left(\tau Nk\right)/N}\right)\), where \(\tau\) behaves like an indicator for imbalance degree, and \(k\) denotes the number of pixels in each image. This suggests optimizing AUC loss could ensure a promising performance under PLSS.

Back to the practical AUC learning process, we realize that the stochastic gradient optimization (SGD) for structured pixel-level tasks imposes a greater computational burden compared to instance-level long-tail problems. Specifically, the SGD algorithm of AUC requires **at least one sample from each class in each mini-batch**[99, 85]. In light of this, the primary choice is to adopt the so-called stratified sampling on all images [70, 60, 86] for mini-batch generation (See Equation (7)). Unfortunately, as shown in Figure 3(a) and (b), this is hard to implement under PLSS because **pixel-level labels are densely coupled in each image**. Meanwhile, as shown in Proposition 1, we also argue that directly using random sampling to include all classes would require an extremely large batch size. This leads to unaffordable GPU memory demands for optimization, as described in the experiments in Appendix G.6.

To alleviate this, a novel _Tail-class Memory Bank (T-Memory Bank)_ is carefully designed. The main idea is to identify those missing pixel-level classes in each randomly generated mini-batch and then complete these absences using stored historical class information from the T-Memory Bank. This enables efficient optimization of AUCSeg with a light memory usage, enhancing the scalability of our proposed method, as shown in Figure 1. Finally, comprehensive empirical studies consistently speak to the efficacy of our proposed AUCSeg.

Our main contributions are summarized as follows:

* This paper starts the first attempt to explore the potential of AUC optimization in pixel-level long-tail problems.

Figure 1: Statistic information of pixel number for each class in the Cityscapes training set and the performance of previous methods (DeepLabV3+, HRNet and STDC) compared to our method (AUC-Seg). Our method aims to improve overall performance, particularly for tail classes. The dashed lines represent mIoU values for each class, while the solid lines represent the average mIoU for the head, middle, and tail classes.

* We theoretically demonstrate the generalization performance of AUCSeg in semantic segmentation. To our knowledge, this area remains underexplored in the machine-learning community.
* We introduce a Tail-class Memory Bank to reduce the optimization burden for pixel-level AUC learning.

## 2 Related Work

### Semantic Segmentation

Semantic segmentation is a subtask of computer vision, which has seen significant development since the inception of FCN [58]. The most common framework for semantic segmentation networks is the encoder-decoder. For the encoder, researchers typically use general models such as ResNet [37] and ResNeXt [84]. As the segmentation tasks become more challenging, some specialized networks have emerged, such as HRNet [75], ICNet [105], and multimodal networks [76; 42]. For the decoder, a series of studies focus on strengthening edge features [23; 107], capturing global context [43; 31; 45], and enhancing the receptive field [96; 67; 12; 13]. Recently, the transformer has shown immense potential, surpassing previous methods. A series of methods related to Vision Transformer [108; 83; 65; 15; 74] are proposed. SegNeXt [32], which is the current _state-of-the-art (SOTA)_ method, possesses the same powerful feature extraction capabilities as the Vision Transformer and the same low computational requirements as CNN. Apart from improving the network, some research [48; 56; 24; 11; 10; 66; 39] is directed toward addressing the issue of class imbalance in semantic segmentation. However, the effectiveness of these methods is not significant. In this paper, we aim to improve the performance of long-tailed semantic segmentation from an AUC optimization perspective.

### AUC Optimization

The development of AUC Optimization can be divided into two periods: the machine learning era and the deep learning era. As a pioneering study, [20] ushers in the era of AUC in machine learning. It studies the necessity of AUC research, which points out that AUC maximization and error rate minimization are inconsistent. After that, AUC gains significant attention in linear fields such as Logistic Regression [38] and SVM [46; 47]. Then researchers begin to explore the online [106; 28] and stochastic [94; 63] optimization extensions of the AUC maximization problem. Research from the perspectives of generalization analysis [2; 73; 17] and consistency analysis [1; 29] provides theoretical support for AUC optimization algorithms. [57] is the first to extend AUC optimization to deep neural networks, ushering in the era of AUC in deep learning. Meanwhile, a series of AUC variants [87; 86; 68; 88; 69; 90] emerge, gradually enriching AUC optimization algorithms. Furthermore, in practice, AUC optimization demonstrates its effectiveness in various class-imbalanced tasks, such as recommendation systems [5; 6; 4; 7], disease prediction [79; 30], domain adaptation [89], and adversarial training [40; 91].

Despite significant progress, existing studies of AUC optimization mainly pay attention to the instance-level imbalanced classification tasks. This paper starts an early trial to introduce AUC optimization to semantic segmentation. However, due to the high complexity of pixel-level multi-class AUC optimization, such a goal cannot be attained by simply using the current techniques in the AUC community.

## 3 Preliminaries

In this section, we briefly introduce the semantic segmentation task and the AUC optimization problem.

### Semantic Segmentation Training Framework

Let \(\mathcal{D}=\{(\mathbf{X}^{i},\mathbf{Y}^{i})_{i=1}^{n}|\mathbf{X}^{i}\in \mathbb{R}^{H\times W\times 3},\mathbf{Y}^{i}\ \in\mathbb{R}^{H\times W\times K}\}\) be the training dataset, where \(H\) and \(W\) represent the height and width of the images, and \(K\) denotes the total number of classes. Let \(f_{\theta}\) be a semantic segmentation model (\(\theta\) is the model parameters), which commonly follows an encoder-decoder backbone [75; 105; 108; 83; 32]. Let \(\hat{\mathbf{Y}}^{i}=f_{\theta}(\mathbf{X}^{i})\in\mathbb{R}^{H\times W\times K}\) be the dense pixel-level prediction, _i.e._,

\[\hat{\mathbf{Y}}^{i}=f_{\theta}(\mathbf{X}^{i})=f_{\theta}^{d}(f_{\theta}^{e}( \mathbf{X}^{i})),\] (1)

where the encoder \(f_{\theta}^{e}\) extracts features from the image \(\mathbf{X}^{i}\), and then the decoder \(f_{\theta}^{d}\) predicts each pixel based on extracted features and outputs a dense segmentation map with the same size as \(\mathbf{Y}^{i}\).

Furthermore, let \(\mathbf{Y}^{i}_{u,v}\) and \(\hat{\mathbf{Y}}^{i}_{u,v}\) represent the ground truth and prediction of the \((u,v)\)-th pixel of the \(i\)-th image, respectively. To train the model \(f_{\theta}\), most current studies [14; 83; 65; 83] usually adopt the _cross-entropy (CE)_ loss:

\[\ell_{ce}:=\frac{1}{n}\sum_{i=1}^{n}\sum_{u=0}^{H-1}\sum_{v=0}^{W-1}\left[- \sum_{c=1}^{K}\mathbf{Y}^{ic}_{u,v}\log(\hat{\mathbf{Y}}^{ic}_{u,v})\right],\] (2)

where \(\mathbf{Y}^{ic}_{u,v}\) and \(\hat{\mathbf{Y}}^{ic}_{u,v}\) are the one-hot encoding of ground truth and the prediction of pixel \((\mathbf{X}^{i}_{u,v},\mathbf{Y}^{i}_{u,v})\) in class \(c\), respectively.

### AUC Optimization

_Area under the Receiver Operating Characteristic Curve (AUC)_ is a well-known ranking performance metric for **binary classification** task, which measures the probability that a positive instance has a higher score than a negative one [35]:

\[AUC(f_{\theta})=\mathbb{P}\left(f_{\theta}(\mathbf{X}^{+})>f_{\theta}(\mathbf{ X}^{-})|y^{+}=1,y^{-}=0\right),\] (3)

where \((\mathbf{X}^{+},y^{+})\) and \((\mathbf{X}^{-},y^{-})\) represent positive and negative samples, respectively. When \(AUC\to 1\), it indicates that the classifier can perfectly separate positive and negative samples.

According to [87; 88; 86], given finite datasets, maximizing \(AUC(h_{\theta})\) is usually realized by maximizing its unbiased empirical estimation:

\[A\hat{U}C(h_{\theta})=1-\frac{1}{n^{+}n^{-}}\sum_{i=1}^{n^{+}}\sum_{j=1}^{n^{ -}}\ell(h_{\theta}(\mathbf{X}^{+})-h_{\theta}(\mathbf{X}^{-})),\] (4)

where \(\ell\) is a differentiable surrogate loss [86] measuring the ranking error between two samples, \(n^{+}\) and \(n^{-}\) denote the number of positive and negative samples, respectively.

Moreover, we can directly optimize the following problem for AUC maximization:

\[\min_{\theta}\frac{1}{n^{+}n^{-}}\sum_{i=1}^{n^{+}}\sum_{j=1}^{n^{-}}\ell(h_ {\theta}(\mathbf{X}^{+})-h_{\theta}(\mathbf{X}^{-})).\] (5)

Note that, AUC has achieved significant progress in long-tailed classification [100; 88; 68]. Due to the limitations of space, we refer interested readers to the literature [86; 99] for more introductions to AUC. However, most existing studies merely focus on the **instance-level or image-level** problems. Inspired by its distribution-insensitive property [26], this paper starts an early trial to introduce AUC to PLSS.

## 4 AUC-Oriented Semantic Segmentation

In this section, we introduce our proposed AUCSeg method for semantic segmentation. A brief overview is provided in Figure 2. AUCSeg is a generic optimization method that can be directly applied to any SOTA backbone for semantic segmentation. Specifically, AUCSeg includes two crucial components: **(1) AUC optimization** where a theoretically grounded loss function is explored for PLSS and **(2) Tail-class Memory Bank**, an effective augmentation scheme to ensure efficient optimization of the proposed AUC loss. In what follows, we will go into more detail about them. For clarity, we include a table of symbol definitions in Appendix A.

### Pixel-level AUC Optimization

Semantic segmentation is a multi-class classification task. Therefore, to apply AUC, we follow a popular multi-class AUC manner, _i.e._, the One vs. One (ovo) strategy [64; 34; 86], which is an average of binary AUC score introduced in Section 3.2. Specifically, on top of the notation of Section 3.1, we further denote \(\mathcal{D}^{p}=\{(\mathbf{X}_{u,v}^{i},\mathbf{Y}_{u,v}^{i})|i\in[1,n],u\in[0,H -1],v\in[0,W-1]\}\) as the set of all pixels; the \(j\)-th element (\(j\in[1,n\times(H-1)\times(W-1])\)) in \(\mathcal{D}^{p}\) is abbreviated as \((\mathbf{X}_{j}^{p},\mathbf{Y}_{j}^{p})\) for convenience. Given the model prediction \(f_{\theta}=(f_{\theta}^{(1)},\dots,f_{\theta}^{(K)})\), \(\forall c\in[K]\), \(f_{\theta}^{(c)}\in[0,1]\), where \(f_{\theta}^{(c)}\) serves as a continuous score function supporting class \(c\), \(AUC_{seg}^{owo}\) calculates the average of binary AUC scores for every class pair:

\[AUC_{seg}^{ovo}=\frac{1}{K(K-1)}\sum_{c=1}^{K}\sum_{c\neq c^{\prime}}AUC_{cc^{ \prime}}(f_{\theta}),\] (6)

\[AUC_{cc^{\prime}}(f_{\theta})=\mathbb{P}(f_{\theta}^{(c)}(\mathbf{X}_{m}^{p} )>f_{\theta}^{(c)}(\mathbf{X}_{n}^{p})|\mathbf{Y}_{m}^{p}=c,\mathbf{Y}_{n}^{p} =c^{\prime}).\]

To this end, as introduced in Section 3.2, the goal is to minimize the following unbiased empirical risk:

\[\ell_{auc}:=\sum_{c=1}^{K}\sum_{c^{\prime}\neq c}\sum_{\mathbf{X}_{m}^{p} \in\mathcal{N}_{c}}\sum_{\mathbf{X}_{n}^{c}\in\mathcal{N}_{c^{\prime}}}\frac{ 1}{|\mathcal{N}_{c}||\mathcal{N}_{c^{\prime}}|}\ell_{sq}^{c,c^{\prime},m,n},\] (7)

where we adopt the widely used square loss \(\ell_{sq}(x)=(1-x)^{2}\) as the surrogate loss [29]; \(\ell_{sq}^{c,c^{\prime},m,n}:=\ell_{sq}(f_{\theta}^{(c)}(\mathbf{X}_{m}^{p} )-f_{\theta}^{(c)}(\mathbf{X}_{n}^{p}))\); \(\mathcal{N}_{c}=\{\mathbf{X}_{k}^{p}|\mathbf{Y}_{k}^{p}=c\}\) represents the set of pixels with label \(c\) in the set \(\mathcal{D}^{p}\), and \(|\mathcal{N}_{c}|\) denotes the size of the set.

### Generalization Bound

In this section, we explore the theoretical guarantees of the AUC loss function in semantic segmentation tasks and demonstrate that AUCSeg can generalize well to unseen data.

A key challenge is that standard techniques for generalization analysis [62; 8; 21] require the loss function to be expressed as a sum of independent terms. Unfortunately, the proposed loss function does

Figure 2: An overview of AUCSeg.

not satisfy this assumption because there are **two layers of interdependency among the loss terms**. On one hand, semantic segmentation can be considered a structured prediction problem [16], where couplings between output substructures within a given image create the first layer of interdependency. On the other hand, the AUC loss creates a pairwise coupling between positive and negative pixels, so any pixel pairs sharing the same positive/negative instance are interdependent, resulting in the second layer of interdependency.

We present our main result in the following theorem and the proof is deferred to Appendix B.

**Theorem 1** (Generalization Bound for AUCSeg).: _Let \(\mathbb{E}_{\mathcal{D}}\left[\hat{\mathcal{L}}_{\mathcal{D}}\left(f\right)\right]\) be the population risk of \(\hat{\mathcal{L}}_{\mathcal{D}}\left(f\right)\). Assume \(\mathcal{F}\subseteq\{f:\mathcal{X}\rightarrow\mathbb{R}^{H\times W\times K}\}\), where \(H\) and \(W\) represent the height and width of the image, and \(K\) represents the number of categories, \(\hat{\mathcal{L}}^{(i)}\) is the risk over \(i\)-th sample, and is \(\mu\)-Lipschitz with respect to the \(l_{\infty}\) norm, (i.e. \(\|\hat{\mathcal{L}}(x)-\hat{\mathcal{L}}(y)\|_{\infty}\leq\mu\cdot\|x-y\|_{\infty}\)). There exists three constants \(A>0\), \(B>0\) and \(C>0\), the following generalization bound holds with probability at least \(1-\delta\) over a random draw of i.i.d training data (at the image-level):_

\[\left|\hat{\mathcal{L}}_{\mathcal{D}}\left(f\right)-\mathbb{E}_{ \mathcal{D}}\left[\hat{\mathcal{L}}_{\mathcal{D}}\left(f\right)\right]\right| \leq\frac{8}{N}+ \frac{\eta_{\text{inner}}+\eta_{\text{inter}}}{\sqrt{N}}\sqrt{A \log\left(2B\mu\tau Nk+C\right)}\] \[+3\left(\sqrt{\frac{1}{2N}}+K\sqrt{1-\frac{1}{N}}\right)\sqrt{ \log\left(\frac{4K(K-1)}{\delta}\right)},\]

_where_

\[\eta_{\text{inner}}=\frac{48\mu\tau\ln N}{N},\quad\eta_{\text{inter}}=2\sqrt{ 2}\tau,\quad\tau=\left(\max_{c\in[K]}\frac{n_{\max}^{(c)}}{n_{\text{mean}}^{ (c)}}\right)^{2},\]

\(n_{\max}^{(c)}=\max_{\mathbf{X}}n(\mathbf{X}^{(c)})\)_, \(n_{mean}^{(c)}=\sum_{i=1}^{N}n(\mathbf{X}_{i}^{(c)})\), \(N=|\mathcal{D}|\), \(k=H\times W\) and \(\mathbf{X}^{(c)}\) represents the pixel of class \(c\) in image \(\mathbf{X}\)._

**Remark 1**.: We achieve a bound of \(\widetilde{\mathcal{O}}\left(\tau\sqrt{\log\left(\tau Nk\right)/N}\right)\), indicating reliable generalization with a large training set. Here, \(\tau\) represents the degree of pixel-level imbalance. More interestingly, even though we have \(k\) classifiers for every single image, the generalization bound only has an algorithm dependent on \(k\), suggesting that pixel-level prediction doesn't hurt generalization too much.

### Tail-class Memory Bank

**Motivation.** Although we have examined the effectiveness of AUC for PLSS from the theoretical point of view, there is **a practical challenge** when conducting AUC optimization for semantic segmentation, as discussed in Section 1. Specifically, the stochastic AUC optimization, as defined in Equation (7), requires **at least one sample from each class** in a mini-batch. In **instance-level AUC optimization**, recent studies [87; 86] often use a stratified sampling technique that generates batches consistent with the original class distribution, as shown in Figure 3(a). Such a strategy will work well when each image belongs to a unique category (say, 'Banana', 'Apple', or 'Lemon') in traditional classifications. Yet it cannot apply to pixel-level cases because each sample involves multiple and coupled labels, making it hard to split them for stratified sampling, as illustrated in Figure 3(b). Meanwhile, we also provide a bound (Proposition 1) to show that simply adopting random sampling will suffer from an overlarge batch size \(B\), making an unaffordable GPU memory burden.

**Proposition 1**.: _Consider a dataset \(\mathcal{D}\) that includes images with \(K\) different pixel categories. Let \(p_{i}\) represent the probability of observing a pixel with label \(i\) in a given image. Randomly select \(B\) images from \(\mathcal{D}\) as training data, where_

\[B=\Omega\left(\frac{\log(\delta/K)}{\log(1-\min_{i}p_{i})}\right).\]

_Then with probability at least \(1-\delta\), for any \(c\in[K]\), there exists \(\mathbf{X}\) in the training data that contains pixels of label \(c\)._

**Remark 2**.: The proof is deferred to Appendix C. Proposition 1 suggests that the value of \(B\) is inversely proportional to \(\min_{i}p_{i}\). Note that \(p_{i}\) will be smaller as the long-tail degree becomes more severe, leading to a larger \(B\). For example, in terms of the Cityscapes dataset with \(K=19\) classes, assuming \(\delta=0.01\) and \(\min_{i}p_{i}=1\%\), \(B\) should be at least \(759\) to guarantee that each class of pixels appears at least once with a high probability. This results in a significant strain on GPU memory.

To address this, considering that the tail-class samples generally have less opportunity to be included in a mini-batch and are often more crucial for final performance, we thus develop a novel Tail-class Memory Bank (T-Memory Bank) to efficiently optimize Equation (7) and manage GPU usage effectively. As depicted in Figure 3(c), the high-level ideas of the T-Memory Bank are as follows: 1) identify missing tail classes of all images involved in a mini-batch and 2) randomly replace some pixels in the image with missing classes based on stored historical class information in T-Memory Bank. In this sense, we can obtain an approximated batch-version of Equation (7),,

\[\tilde{\ell}_{auc}:=\sum_{c=1}^{K}\sum_{\begin{subarray}{c}c^{\prime}\neq \ell\\ n_{c}\epsilon_{r}\neq\emptyset\end{subarray}}\sum_{\begin{subarray}{c}\mathbf{ X}_{m}^{p}\in\mathcal{N}_{c}\cup\mathcal{T}_{c}\\ \mathbf{X}_{c}^{p}\in\mathcal{N}_{c^{\prime}}\cup\mathcal{T}_{c^{\prime}} \end{subarray}}\frac{1}{|\mathcal{N}_{c}||\mathcal{N}_{c^{\prime}}|}\tilde{ \ell}_{sq}^{c,c^{\prime},m,n}\] (8)

where \(\mathcal{N}_{c}\) and \(\mathcal{T}_{c}\) represent the set of pixels with label \(c\) in the original image and those pixels stored in the T-Memory Bank, respectively; \(\tilde{\ell}_{sq}^{c,c^{\prime},m,n}:=\ell_{sq}(f_{\theta}^{(c)}(\tilde{ \mathbf{X}}_{m}^{p})-f_{\theta}^{(c)}(\tilde{\mathbf{X}}_{n}^{p}))\); \(\tilde{\mathbf{X}}^{p}\) represents the sample after replacing some pixels with tail classes pixels from the T-Memory Bank.

**Detailed Components.** As shown in Figure 2, T-Memory Bank comprises three main parts: **(1) Memory Branch** stores a set with \(S_{M}\) (the Memory Size) images for each tail class. We define the set as \(\mathcal{M}=\{\mathcal{M}_{c_{1}},\dots,\mathcal{M}_{c_{n_{t}}}\}\), where \(\mathcal{C}_{t}=\{c_{i}\}_{i=1}^{n_{t}}\) denotes the labels of tail classes, and \(n_{t}\) is the total number of selected tail classes; **(2) Retrieve Branch** selects pixels from the Memory Branch to supplement the missing tail classes and **(3) Store Branch** updates the Memory Branch whenever a new image arrives. Algorithm 1 summarizes a short version of AUCSeg equipped with T-Memory Bank. **Please refer to the detailed version in Appendix D.** Note that we introduce CE loss as a regularization term for our proposed AUCSeg, which is widely used in the AUC community [99] to pursue robust feature learning. Experiments demonstrate that the performance is insensitive to the regularization weight \(\lambda\), as shown in Figure 4(d).

At the start of training, the Memory Branch is empty. In this case, we only calculate the loss function \(\ell\) for the classes present in the mini-batch, while the Retrieve Branch will not take any action. Meanwhile, the Store Branch will continuously append pixel data of tail classes to the Memory Branch. As the Memory Branch reaches its maximum capacity \(S_{M}\), we adopt a random replacement strategy to update the Store Branch (Lines \(5\) to \(6\) in Algorithm 1 or Lines \(6\) to \(11\) in Algorithm 2).

As the training process progresses, if the Memory Branch is not empty, the Retrieve Branch kicks in to count the missing classes in each image of the mini-batch, denoted as \(\mathcal{C}_{miss}\). It then calculates the number of classes needed to be added for optimization, \(n_{sample}=\lceil|\mathcal{C}_{miss}|\times R_{S}\rceil\). Here, we introduce a tunable sample ratio \(R_{S}\) to strike a trade-off between the original and missing tail-class semantic information. Finally, it uniformly retrieves the corresponding pixels of \(n_{sample}\) missing classes from the Memory Branch, resizes them by the resize ratio \(R_{R}\), and randomly selects positions to overwrite (Lines \(7\) to \(8\) in Algorithm 1 or Lines \(13\) to \(18\) in Algorithm 2).

Figure 3: Instance-level and pixel-level task sampling.

Discussions.We recognize that Memory Bank [82; 36] has achieved great success in deep learning. However, our T-Memory Bank behaves differently compared to earlier studies. **The key difference is that the Memory Bank and T-Memory Bank are designed for different tasks.** The goal of the previous Memory Bank is to facilitate the traditional classifications by storing instance-level or image-level features, while our T-Memory Bank specifically stores the original pixels for each object. This strategy is particularly beneficial for our PLSS task. Additionally, the T-Memory Bank enables AUCSg without substantially increasing GPU workload as well as the number of samples per mini-batch by selectively replacing non-essential pixels. Our experiments, detailed in Appendix G.6, include a comparison of GPU overhead. For more discussion on the T-Memory Bank, please refer to Appendix E.

## 5 Experiments

In this section, we describe some details of the experiments and present our results. **Due to space limitations, please refer to Appendix F, Appendix G and Appendix H for an extended version.**

### Experimental Setups

The experiment includes three benchmark datasets: Cityscapes [19], ADE20K [109], and COCO-Stuff 164K [9]. We use SegNeXt [32] as the backbone for our model and the _mean of Intersection over Union (mIoU)_ as the evaluation metric. We compare our method with \(13\) **recent advancements and \(6\) long-tail approaches** in semantic segmentation. All the long-tail methods also use SegNeXt as the backbone. To ensure fairness, we re-implement the listed methods using their publicly shared code and test them on the same hardware. Detailed introductions are deferred to Appendix F.

### Overall Performance

Table 1 shows the quantitative performance comparisons. We draw the following conclusions: First, most current algorithms perform poorly in long-tail scenarios. Specifically, performance drops sharply from head to tail classes. For instance, the performance gap for PointRend and OCRNet on the Cityscapes reaches up to \(40\%\). Second, models using long-tail approaches generally achieve better results than those that do not. However, they still fail to produce satisfactory outcomes. One possible reason is that these long-tail approaches focus on reweighting, giving too much attention to the tail classes and leading to overfitting. Additionally, our proposed AUCSeg method surpasses all competitors in most metrics. This success is due to the appealing properties of AUC. Our method consistently outperforms the runner-up by \(+1.21\%\), \(+0.75\%\), and \(+0.38\%\) in tail classes mIoU across the datasets. Overall mIoU also improves by \(+1.05\%\), \(+0.27\%\), and \(+0.31\%\). In some Head/Middle

metrics, AUCSeg does not achieve the best performance. Even in these cases, AUCSeg still secures the runner-up status. We analyze the performance trade-off between head and tail classes in Appendix H. These experimental results underscore the effectiveness of our proposed method. We further present the results for each tail class in Appendix G.1, and analyze the reasons for the varying performance improvements of tail classes across the three datasets in Appendix G.2.

Figure 4 displays the qualitative results on the Cityscapes validation set. Benefiting from our proposed AUC and T-Memory Bank techniques, AUCSeg segments objects in tail classes more accurately. It correctly distinguishes between bicycles and motorcycles and successfully identifies distant traffic lights, which other methods overlook. More qualitative results can be found in Appendix G.3.

### Backbone Extension

In Section 5.1, we select the current SOTA SegNeXt as the backbone. Nevertheless, AUCSeg can also adapt to other backbones, consistently delivering effective results. Table 2 presents the experimental results of AUCSeg when using DeepLabV3+, EMANet, OCRNet, and ISANet as backbones. These results reveal significant improvements in both overall mIoU and tail classes mIoU with AUCSeg. Notably, on ISANet, the increases are \(5.54\%\) and \(6.49\%\). Moreover, AUCSeg enhances performance across various model sizes and different pixel-level long-tail problems, as detailed in Appendix G.4 and Appendix G.4. **This demonstrates the superiority of our proposed AUCSeg for long-tailed semantic segmentation.**

### Ablation studies

We perform several ablation studies to test the effectiveness of different modules and hyperparameters. All experiments are conducted on the **ADE20K** validation set.

\begin{table}
\begin{tabular}{c|c c c c c c c|c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{**ADE20K**} & \multicolumn{3}{c|}{**Cityscapes**} & \multicolumn{3}{c|}{**COCO-Stuff 164K**} & \multicolumn{3}{c}{**Tail**} \\  & **Overall** & **Head** & **Middle** & **Tail** & **Overall** & **Head** & **Middle** & **Tail** & **Overall** & **Head** & **Middle** & **Tail** \\ \hline DeepLabV3+ [14] & 31.95 & 75.88 & 51.96 & 26.01 & 66.53 & 90.11 & 57.16 & 54.36 & 29.11 & 51.11 & 32.93 & 24.82 \\ EncNet [102] & 32.12 & 75.34 & 51.60 & 26.32 & 71.34 & 91.62 & 60.76 & 63.03 & 27.31 & 49.89 & 30.41 & 23.09 \\ FastFCN [80] & 29.78 & 74.20 & 49.44 & 23.86 & 63.97 & 90.37 & 52.43 & 51.22 & 28.37 & 50.60 & 32.52 & 23.96 \\ EMANet [55] & 32.83 & 75.77 & 50.03 & 27.36 & 70.93 & 91.69 & 60.61 & 61.97 & 28.48 & 49.73 & 29.97 & 24.85 \\ DANet [27] & 33.83 & 74.62 & 51.01 & 28.52 & 67.57 & 89.66 & 55.30 & 54.26 & 26.83 & 49.60 & 31.14 & 22.29 \\ HRNet [71] & 31.83 & 75.35 & 49.98 & 26.19 & 73.40 & 91.98 & 65.79 & 64.00 & 28.65 & 48.00 & 30.74 & 25.16 \\ OCRNet [97] & 29.64 & 74.00 & 49.40 & 23.72 & 69.65 & 90.24 & 63.18 & 50.21 & 28.67 & 51.04 & 32.41 & 24.33 \\ DNLNet [93] & 33.24 & 75.90 & 51.16 & 27.69 & 76.08 & 91.98 & 59.90 & 61.66 & 30.23 & 50.71 & 33.05 & 26.41 \\ PointRend [50] & 17.77 & 61.18 & 37.60 & 11.46 & 60.67 & 89.79 & 53.92 & 41.49 & 11.17 & 21.17 & 13.64 & 9.04 \\ BiSeNetV2 [95] & 10.26 & 60.38 & 28.72 & 4.10 & 73.04 & 92.00 & 63.52 & 64.93 & 10.30 & 34.96 & 12.71 & 5.92 \\ ISANet [98] & 29.53 & 74.34 & 48.77 & 23.64 & 70.63 & 91.67 & 61.50 & 60.43 & 26.37 & 48.87 & 30.78 & 21.86 \\ STDC [25] & 30.17 & 73.36 & 48.02 & 24.58 & 76.30 & 92.58 & 65.09 & 71.94 & 29.83 & 51.74 & 33.40 & 25.61 \\ SegNeXt [32] & 47.45 & 80.54 & **60.35** & 43.28 & 82.41 & **94.08** & 72.46 & 80.92 & 42.42 & **57.05** & 41.71 & 40.33 \\ \hline VS [49] & 24.72 & 75.30 & 48.02 & 17.86 & 55.40 & 92.16 & 52.52 & 26.36 & 24.27 & 47.80 & 30.38 & 19.19 \\ LA [61] & 31.16 & 77.07 & 53.43 & 24.77 & 62.75 & 92.98 & 64.79 & 35.09 & 28.56 & 49.67 & 33.16 & 24.21 \\ LDAN [10] & 33.11 & 74.06 & 51.26 & 27.65 & 65.95 & 92.72 & 69.27 & 40.17 & 42.39 & 56.85 & 41.59 & 40.34 \\ Focal Loss [56] & 47.68 & 80.54 & 59.04 & 43.73 & 82.44 & 93.90 & **72.79** & 80.89 & 41.98 & 56.87 & 41.51 & 39.79 \\ DisAlign [104] & 48.15 & 80.33 & 59.14 & 44.31 & 81.94 & 93.61 & 72.12 & 80.36 & 42.10 & 55.20 & 41.24 & 40.28 \\ BLV [71] & 46.76 & 79.96 & 58.96 & 42.67 & 81.81 & 93.84 & 71.83 & 80.05 & 42.17 & 56.83 & 41.52 & 40.06 \\ \hline AUCSeg (Ours) & **49.20** & **80.59** & **59.45** & **45.52** & **82.71** & **93.91** & **72.72** & **81.67** & **42.73** & **56.95** & **41.93** & **40.72** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Quantitative results on **Cityscapes, ADE20K** and **COCO-Stuff 164K** val set in terms of mIoU (%). The champion and the runner-up are highlighted in **bold** and underline.

\begin{table}
\begin{tabular}{c|c|c c} \hline \hline
**Backbone** & **AUCSeg** & **Overall** & **Tail** \\ \hline \multirow{2}{*}{DeepLabV3+} & \(\bigtimes\) & 31.95 & 26.01 \\  & \(\bigvee\) & **36.13** & **31.10** \\ \hline \multirow{2}{*}{EMANet} & \(\bigtimes\) & 32.83 & 27.36 \\  & \(\bigvee\) & **36.32** & **31.39** \\ \hline \multirow{2}{*}{OCRNet} & \(\bigtimes\) & 29.64 & 23.72 \\  & \(\bigvee\) & **34.82** & **29.75** \\ \hline \multirow{2}{*}{ISANet} & \(\bigtimes\) & 29.53 & 23.64 \\  & \(\bigvee\) & **35.07** & **30.13** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results of AUCSeg using different backbones in terms of mIoU (%).

**The Effectiveness of AUC Optimization and T-Memory Bank.** Table 3 details our step-by-step ablation study on the AUC and T-Memory Bank components of AUCSeg. Compared to the baseline SegNeXt, AUC enhances performance by \(+1.01\%\) and \(+1.42\%\) in overall and tail classes. The T-Memory Bank further addresses the imbalance issue, yielding improvements of \(+1.75\%\) overall and \(+2.24\%\) in tail classes. Our results also show that employing T-Memory Bank without AUC yields no significant improvements, underscoring the necessity of AUC optimization and TMB are deferred to Appendix G.6, G.7, G.8, and G.9.

**Ablation Study on Hyper-Parameters.** Figure 4(a) ablates the maximum number of images stored per class in the Memory Branch, referred to as Memory Size (\(S_{M}\)). For the ADE20K dataset, optimal performance occurs when \(S_{M}=5\), and performance shows little sensitivity to changes in \(S_{M}\). Figure 4(b) ablates the Sample Ratio (\(R_{S}\)), the fraction of classes sampled from the Memory Branch relative to the total number of missing tail classes. Figure 4(c) ablates the Resize Ratio (\(R_{R}\)), the scaling factor for the sampled pixels. For ADE20K, the best result is obtained when \(R_{S}=0.05\) and \(R_{R}=0.4\). A potential reason for their small value is that the original image is overwritten when \(R_{S}\) and \(R_{R}\) are too large, resulting in poor training performance. Figure 4(d) ablates the weight \(\lambda\) for \(\ell_{ce}\) and \(\ell_{ave}\), with \(\lambda=\frac{1}{4}\) providing slightly better results. The influence of \(\lambda\) on performance is minimal. Detailed results from this hyper-parameter ablation study are available in Appendix G.10 and G.11.

## 6 Conclusion

This paper explores AUC optimization in the context of PLSS tasks. To begin with, we theoretically study the generalization performance of AUC-oriented PLSS by overcoming the two-layer coupling issue across the loss terms of AUCSeg therein. The corresponding results show that applying AUC optimization to PLSS could also enjoy a promising performance. Subsequently, we propose a novel T-Memory Bank to reduce the significant memory demand for the mini-batch optimization of AUCSeg. Finally, comprehensive experiments suggest the effectiveness of our proposed AUCSeg.

Figure 4: Qualitative results on the **Cityscapes** val set. Red rectangles highlight and magnify the image details in the lower left corner.

Figure 5: Ablation Study on Hyper-Parameters.

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline
**Model** & **AUC** & **TMB** & **Overall** & **Tail** \\ \hline SegNeXt & & & 47.45 & 43.28 \\ SegNeXt+AUC & ✓ & & 48.46 & 44.70 \\ SegNeXt+TMB & & ✓ & 47.86 & 43.86 \\ AUCSeg & ✓ & ✓ & **49.20** & **45.52** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation study on the effectiveness of AUC Optimization and T-Memory Bank (TMB) in terms of mIoU (%).

## Acknowledgments

This work was supported in part by the National Key R&D Program of China under Grant 2018AAA0102000, in part by National Natural Science Foundation of China: 62236008, U21B2038, U23B2051, 61931008, 62122075, 92370102, 62406305, 62471013 and 62476068, in part by Youth Innovation Promotion Association CAS, in part by the Strategic Priority Research Program of the Chinese Academy of Sciences, Grant No. XDB0680000, in part by the Innovation Funding of ICT, CAS under Grant No.E000000, in part by the Postdoctoral Fellowship Program of CPSF under Grant GZB20240729 and GZB20230732, and in part by the China Postdoctoral Science Foundation under Grant No.2023M743441.

## References

* [1] Shivani Agarwal. Surrogate regret bounds for bipartite ranking via strongly proper losses. _The Journal of Machine Learning Research_, 15(1):1653-1674, 2014.
* [2] Shivani Agarwal, Thore Graepel, Ralf Herbrich, Sariel Har-Peled, Dan Roth, and Michael I Jordan. Generalization bounds for the area under the roc curve. _Journal of Machine Learning Research_, 6(4), 2005.
* [3] Massih-Reza Amini and Nicolas Usunier. _Learning with partially labeled and interdependent data_. Springer, 2015.
* [4] Shilong Bao, Qianqian Xu, Ke Ma, Zhiyong Yang, Xiaochun Cao, and Qingming Huang. Collaborative preference embedding against sparse labels. In _ACMMM_, pages 2079-2087, 2019.
* [5] Shilong Bao, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, and Qingming Huang. Rethinking collaborative metric learning: Toward an efficient alternative without negative sampling. _IEEE TPAMI_, 45(1):1017-1035, 2022.
* [6] Shilong Bao, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming Huang. The minority matters: A diversity-promoting collaborative metric learning algorithm. In _NeurIPS_, pages 2451-2464, 2022.
* [7] Shilong Bao, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming Huang. Improved diversity-promoting collaborative metric learning for recommendation. _IEEE TPAMI_, 2024.
* [8] Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. _JMLR_, 3(Nov):463-482, 2002.
* [9] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In _CVPR_, pages 1209-1218, 2018.
* [10] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. In _NeurIPS_, 2019.
* [11] Robin Chan, Matthias Rottmann, Fabian Huger, Peter Schlicht, and Hanno Gottschalk. Application of decision rules for handling class imbalance in semantic segmentation. _arXiv preprint arXiv:1901.08394_, 2019.
* [12] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. _IEEE TPAMI_, 40(4):834-848, 2017.
* [13] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. _arXiv preprint arXiv:1706.05587_, 2017.
* [14] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In _ECCV_, pages 801-818, 2018.
* [15] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In _CVPR_, pages 1290-1299, 2022.
* [16] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A general framework for consistent structured prediction with implicit loss embeddings. _JMLR_, 21(98):1-67, 2020.

* [17] Stephan Clemencon, Gabor Lugosi, and Nicolas Vayatis. Ranking and empirical minimization of u-statistics. _The Annals of Statistics_, 2008.
* [18] MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. https://github.com/open-mmlab/mmsegmentation, 2020.
* [19] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In _CVPR_, pages 3213-3223, 2016.
* [20] Corinna Cortes and Mehryar Mohri. Auc optimization vs. error rate minimization. In _NeurIPS_, 2003.
* [21] Felipe Cucker and Steve Smale. On the mathematical foundations of learning. _Bulletin of the American mathematical society_, 39(1):1-49, 2002.
* [22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, pages 248-255, 2009.
* [23] Henghui Ding, Xudong Jiang, Ai Qu Liu, Nadia Magnenat Thalmann, and Gang Wang. Boundary-aware feature propagation for scene segmentation. In _ICCV_, pages 6819-6829, 2019.
* [24] Rongsheng Dong, Xiaoquan Pan, and Fengying Li. Denseu-net-based semantic segmentation of small objects in urban remote sensing images. _IEEE Access_, 7:65347-65356, 2019.
* [25] Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng Luo, and Xiaolin Wei. Rethinking bisenet for real-time semantic segmentation. In _CVPR_, pages 9716-9725, 2021.
* [26] Tom Fawcett. An introduction to roc analysis. _Pattern recognition letters_, 27(8):861-874, 2006.
* [27] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In _CVPR_, pages 3146-3154, 2019.
* [28] Wei Gao, Rong Jin, Shenghuo Zhu, and Zhi-Hua Zhou. One-pass auc optimization. In _ICML_, pages 906-914, 2013.
* [29] Wei Gao and Zhi-Hua Zhou. On the consistency of auc pairwise optimization. _arXiv preprint arXiv:1208.0645_, 2012.
* [30] Damian Gola, Jeannette Erdmann, Bertram Muller-Myshok, Heribert Schunkert, and Inke R Konig. Polygenic risk scores outperform machine learning methods in predicting coronary artery disease status. _Genetic epidemiology_, 44(2):125-138, 2020.
* [31] Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, and Shi-Min Hu. Beyond self-attention: External attention using two linear layers for visual tasks. _IEEE TPAMI_, 45(5):5436-5447, 2022.
* [32] Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhenging Liu, Ming-Ming Cheng, and Shi-Min Hu. Segnext: Rethinking convolutional attention design for semantic segmentation. In _NeurIPS_, pages 1140-1156, 2022.
* [33] Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai Zhang, Ralph R Martin, Ming-Ming Cheng, and Shi-Min Hu. Attention mechanisms in computer vision: A survey. _Computational visual media_, 8(3):331-368, 2022.
* [34] David J Hand and Robert J Till. A simple generalisation of the area under the roc curve for multiple class classification problems. _Machine learning_, 45:171-186, 2001.
* [35] James A Hanley and Barbara J McNeil. The meaning and use of the area under a receiver operating characteristic (roc) curve. _Radiology_, 143(1):29-36, 1982.
* [36] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _CVPR_, pages 9729-9738, 2020.
* [37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.
* [38] Alan Herschtal and Bhavani Raskutti. Optimising area under the roc curve using gradient descent. In _ICML_, pages 385-392, 2004.
* [39] Md Sazzad Hossain, John M Betts, and Andrew P Paplinski. Dual focal loss to address class imbalance in semantic segmentation. _Neurocomputing_, 462:69-87, 2021.

* [40] Wenzheng Hou, Qianqian Xu, Zhiyong Yang, Shilong Bao, Yuan He, and Qingming Huang. Adauc: End-to-end adversarial auc optimization against long-tail problems. In _ICML_, pages 8903-8925, 2022.
* [41] Hanzhe Hu, Fangyun Wei, Han Hu, Qiwei Ye, Jinshi Cui, and Liwei Wang. Semi-supervised semantic segmentation via adaptive equalization learning. In _NeurIPS_, pages 22106-22118, 2021.
* [42] Cong Hua, Qianqian Xu, Shilong Bao, Zhiyong Yang, and Qingming Huang. Reconboost: Boosting can achieve modality reconciliation. In _ICML_, pages 19573-19597, 2024.
* [43] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In _ICCV_, pages 603-612, 2019.
* [44] Svante Janson. Large deviations for sums of partly dependent random variables. _Random Structures & Algorithms_, 24:234-248, 2004.
* [45] Zhenchao Jin, Xiaowei Hu, Lingting Zhu, Luchuan Song, Li Yuan, and Lequan Yu. Idrnet: Intervention-driven relation network for semantic segmentation. In _NeurIPS_, 2024.
* [46] Thorsten Joachims. A support vector method for multivariate performance measures. In _ICML_, pages 377-384, 2005.
* [47] Thorsten Joachims. Training linear svms in linear time. In _SIGKDD_, pages 217-226, 2006.
* [48] Michael Kampffmeyer, Arnt-Borre Salberg, and Robert Jenssen. Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks. In _CVPR workshops_, pages 1-9, 2016.
* [49] Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis. Label-imbalanced and group-sensitive classification under overparameterization. _NeurIPS_, pages 18970-18983, 2021.
* [50] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering. In _CVPR_, pages 9799-9808, 2020.
* [51] Fahad Lateef and Yassine Ruichek. Survey on semantic segmentation using deep learning techniques. _Neurocomputing_, 338:321-348, 2019.
* [52] Antoine Ledent, Yunwen Lei, and Marius Kloft. Improved generalisation bounds for deep learning through \(l_{\infty}\) covering numbers. _arXiv preprint arXiv:1905.12430_, 2019.
* [53] Feiran Li, Qianqian Xu, Shilong Bao, Zhiyong Yang, Runmin Cong, Xiaochun Cao, and Qingming Huang. Size-invariance matters: Rethinking metrics and losses for imbalanced multi-object salient object detection. In _ICML_, pages 28989-29021, 2024.
* [54] Shaojie Li and Yong Liu. Towards sharper generalization bounds for structured prediction. In _NeurIPS_, pages 26844-26857, 2021.
* [55] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, and Hong Liu. Expectation-maximization attention networks for semantic segmentation. In _ICCV_, pages 9167-9176, 2019.
* [56] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In _ICCV_, pages 2980-2988, 2017.
* [57] Mingrui Liu, Zhuoning Yuan, Yiming Ying, and Tianbao Yang. Stochastic auc maximization with deep neural networks. In _ICLR_, 2020.
* [58] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In _CVPR_, pages 3431-3440, 2015.
* [59] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. In _ICLR_, 2018.
* [60] Xiangrui Meng. Scalable simple random sampling and stratified sampling. In _ICML_, pages 531-539, 2013.
* [61] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar. Long-tail learning via logit adjustment. In _ICLR_, 2020.
* [62] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of machine learning_. MIT press, 2018.

* [63] Michael Natole, Yiming Ying, and Siwei Lyu. Stochastic proximal algorithms for auc maximization. In _ICML_, pages 3710-3719, 2018.
* [64] Foster Provost and Pedro Domingos. Tree induction for probability-based ranking. _Machine learning_, 52:199-215, 2003.
* [65] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In _ICCV_, pages 12179-12188, 2021.
* [66] Mina Rezaei, Haojin Yang, and Christoph Meinel. Recurrent generative adversarial network for learning imbalanced medical image semantic segmentation. _Multimedia Tools and Applications_, 79(21-22):15329-15348, 2020.
* [67] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention_, pages 234-241, 2015.
* [68] Huiyang Shao, Qianqian Xu, Zhiyong Yang, Shilong Bao, and Qingming Huang. Asymptotically unbiased instance-wise regularized partial auc optimization: Theory and algorithm. In _NeurIPS_, pages 38667-38679, 2022.
* [69] Huiyang Shao, Qianqian Xu, Zhiyong Yang, Peisong Wen, Gao Peifeng, and Qingming Huang. Weighted roc curve in cost space: Extending auc to cost-sensitive learning. In _NeurIPS_, pages 17357-17368, 2024.
* [70] Ravindra Singh, Naurang Singh Mangat, Ravindra Singh, and Naurang Singh Mangat. Stratified sampling. _Elements of survey sampling_, pages 102-144, 1996.
* [71] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In _CVPR_, pages 5693-5703, 2019.
* [72] Nicolas Usunier, Massih R Amini, and Patrick Gallinari. Generalization error bounds for classifiers trained with interdependent data. In _NeurIPS_, 2005.
* [73] Nicolas Usunier, Massih-Reza Amini, and Patrick Gallinari. A data-dependent generalisation error bound for the auc. In _ICML Workshop_, 2005.
* [74] Haonan Wang, Qixiang Zhang, Yi Li, and Xiaomeng Li. Allspark: Reborn labeled features from unlabeled in transformer for semi-supervised semantic segmentation. In _CVPR_, 2024.
* [75] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. _IEEE TPAMI_, 43(10):3349-3364, 2020.
* [76] Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, and Yunhe Wang. Multimodal token fusion for vision transformers. In _CVPR_, pages 12186-12195, 2022.
* [77] Yuchao Wang, Jingjing Fei, Haochen Wang, Wei Li, Tianpeng Bao, Liwei Wu, Rui Zhao, and Yujun Shen. Balancing logit variation for long-tailed semantic segmentation. In _CVPR_, pages 19561-19573, 2023.
* [78] Zitai Wang, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, and Qingming Huang. A unified generalization analysis of re-weighting and logit-adjustment for imbalanced learning. In _NeurIPS_, pages 48417-48430, 2023.
* [79] Andrew Westcott, Dante PI Capaldi, David G McCormack, Aaron D Ward, Aaron Fenster, and Grace Parraga. Chronic obstructive pulmonary disease: thoracic ct texture analysis and machine learning to predict pulmonary ventilation. _Radiology_, 293(3):676-684, 2019.
* [80] Huikai Wu, Junge Zhang, Kaiqi Huang, Kongming Liang, and Yizhou Yu. Fastfcn: Rethinking dilated convolution in the backbone for semantic segmentation. _arXiv preprint arXiv:1903.11816_, 2019.
* [81] Yu-Huan Wu, Yun Liu, Le Zhang, Ming-Ming Cheng, and Bo Ren. Edn: Salient object detection via extremely-downsampled network. _IEEE Tip_, 31:3125-3136, 2022.
* [82] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In _CVPR_, pages 3733-3742, 2018.
* [83] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In _NeurIPS_, pages 12077-12090, 2021.

* [84] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In _CVPR_, pages 1492-1500, 2017.
* [85] Tianbao Yang and Yiming Ying. Auc maximization in the era of big data and ai: A survey. _ACM computing surveys_, 55(8):1-37, 2022.
* [86] Zhiyong Yang, Qianqian Xu, Shilong Bao, Xiaochun Cao, and Qingming Huang. Learning with multiclass auc: Theory and algorithms. _IEEE TPAMI_, 44(11):7747-7763, 2021.
* [87] Zhiyong Yang, Qianqian Xu, Shilong Bao, Yuan He, Xiaochun Cao, and Qingming Huang. When all we need is a piece of the pie: A generic framework for optimizing two-way partial auc. In _ICML_, pages 11820-11829, 2021.
* [88] Zhiyong Yang, Qianqian Xu, Shilong Bao, Yuan He, Xiaochun Cao, and Qingming Huang. Optimizing two-way partial auc with an end-to-end framework. _IEEE TPAMI_, 2022.
* [89] Zhiyong Yang, Qianqian Xu, Shilong Bao, Peisong Wen, Yuan He, Xiaochun Cao, and Qingming Huang. Auc-oriented domain adaptation: From theory to algorithm. _IEEE TPAMI_, 2023.
* [90] Zhiyong Yang, Qianqian Xu, Xiaochun Cao, and Qingming Huang. Task-feature collaborative learning with application to personalized attribute prediction. _IEEE TPAMI_, 43(11):4094-4110, 2020.
* [91] Zhiyong Yang, Qianqian Xu, Wenzheng Hou, Shilong Bao, Yuan He, Xiaochun Cao, and Qingming Huang. Revisiting auc-oriented adversarial training with loss-agnostic perturbations. _IEEE TPAMI_, 2023.
* [92] Zhiyong Yang, Qianqian Xu, Zitai Wang, Sicong Li, Boyu Han, Shilong Bao, Xiaochun Cao, and Qingming Huang. Harnessing hierarchical label distribution variations in test agnostic long-tail recognition. In _ICML_, pages 56624-56664, 2024.
* [93] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disentangled non-local neural networks. In _ECCV_, pages 191-207, 2020.
* [94] Yiming Ying, Longyin Wen, and Siwei Lyu. Stochastic online auc maximization. In _NeurIPS_, 2016.
* [95] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu, Chunhua Shen, and Nong Sang. Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation. _IJCV_, 129:3051-3068, 2021.
* [96] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. _arXiv preprint arXiv:1511.07122_, 2015.
* [97] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In _ECCV_, pages 173-190, 2020.
* [98] Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. Ocnet: Object context for semantic segmentation. _IJCV_, 129(8):1-24, 2021.
* [99] Zhuoning Yuan, Zhishuai Guo, Nitesh Chawla, and Tianbao Yang. Compositional training for end-to-end deep auc maximization. In _ICLR_, 2021.
* [100] Zhuoning Yuan, Yan Yan, Milan Sonka, and Tianbao Yang. Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification. In _ICCV_, pages 3040-3049, 2021.
* [101] Zhuoning Yuan, Dixian Zhu, Zi-Hao Qiu, Gang Li, Xuanhui Wang, and Tianbao Yang. Libauc: A deep learning library for x-risk optimization. In _KDD_, pages 5487-5499, 2023.
* [102] Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. Context encoding for semantic segmentation. In _CVPR_, pages 7151-7160, 2018.
* [103] Rui-Ray Zhang and Massih-Reza Amini. Generalization bounds for learning under graph-dependence: A survey. _Machine Learning_, pages 1-31, 2024.
* [104] Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, and Jian Sun. Distribution alignment: A unified framework for long-tail visual recognition. In _CVPR_, pages 2361-2370, 2021.
* [105] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic segmentation on high-resolution images. In _ECCV_, pages 405-420, 2018.

* [106] Peilin ZHAO, Steven CH HOI, Rong JIN, and Tianbo YANG. Online auc maximization. In _ICML_, pages 233-240, 2011.
* [107] Mingmin Zhen, Jinglu Wang, Lei Zhou, Shiwei Li, Tianwei Shen, Jiaxiang Shang, Tian Fang, and Long Quan. Joint semantic segmentation and boundary detection using iterative pyramid contexts. In _CVPR_, pages 13666-13675, 2020.
* [108] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In _CVPR_, pages 6881-6890, 2021.
* [109] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In _CVPR_, pages 633-641, 2017.
* [110] Ding-Xuan Zhou. The covering number in learning theory. _Journal of Complexity_, 18:739-767, 2002.

###### Contents

* A Symbol Definitions
* B Generalization Bounds and Its Proofs
* B.1 Preliminary Lemmas
* B.2 Key Lemmas
* B.3 Proof of the Main Result
* C Proof for Propositions of Tail-class Memory Bank
* D Details of T-Memory Bank Algorithm
* E More Discussions about T-Memory Bank
* E.1 Discussion on the Improved Version of Stratified Sampling
* E.2 Discussion on Why the T-Memory Bank Works
* E.3 Discussion on AUC and Contrastive Learning from the Perspective of the Loss Function
* F Additional Experimental Settings
* F.1 Datasets
* F.2 Implementation Details
* F.3 Competitors
* G Additional Experimental Results
* G.1 Per-tail-class Results
* G.2 Performance Differences Across Different Datasets
* G.3 More Qualitative Results
* G.4 Backbone Extension of Different Model Sizes
* G.5 Backbone Extension of Different Pixel-level Long-tail Problems
* G.6 Spatial Resource Consumption
* G.7 Results of Different AUC Surrogate Losses and Calculation Methods
* G.8 Results of the Comparison Between PMB and TMB
* G.9 Results of Different Memory Bank Update Strategies
* G.10 Detailed Results of the Ablation Study on Hyper-Parameters
* G.11 Results of the Ablation Study on the Impact of Batch Size
* H More Discussions About AUCSeg
* I Broader Impact

Symbol Definitions

In this section, Table 4 includes a summary of key notations and descriptions in this work.

## Appendix B Generalization Bounds and Its Proofs

### Preliminary Lemmas

**Lemma 1** (Jensen's Inequality).: _If \(X\) is a random variable and \(\varphi\) is a convex function, then_

\[\varphi\left(\mathbb{E}\left[X\right]\right)\leq\mathbb{E}\left[\varphi\left(X \right)\right].\] (9)

**Assumption 1**.: Assume that \(\ell\) is \(\mu\)-Lipschitz continuous, that is

\[\left|\ell\left(t\right)-\ell\left(s\right)\right|\leq\mu|t-s|.\] (10)

Assumption 1 is a pretty mild assumption. The square loss \(\ell_{sq}\left(x\right)=\left(1-x\right)^{2}\) satisfies Assumption 1.

**Lemma 2**.: _The empirical Rademacher complexity of function \(g\) with respect to the predictor \(f\) is defined as:_

\[\hat{\mathfrak{H}}_{\mathcal{F}}(g)=\mathbb{E}_{\sigma}[\sup_{f\in\mathcal{F} }\frac{1}{N}\sum_{i=1}^{N}\sigma_{i}g(f^{(i)})].\] (11)

_where \(\mathcal{F}\subseteq\{f:\mathcal{X}\rightarrow\mathbb{R}^{K}\}\) is a family of predictors, and \(N\) refers to the size of the dataset, and \(\sigma_{i}\)s are independent uniform random variables taking values in \(\{-1,+1\}\). The random variables \(\sigma_{i}\) are called Rademacher variables._

\begin{table}
\begin{tabular}{l l} \hline \hline
**Notations** & **Descriptions** \\ \hline \(\mathcal{D}\) & Training dataset. \\ \(H/W\) & The height/width of the images in dataset \(\mathcal{D}\). \\ \(N\) & The number of image samples in dataset \(\mathcal{D}\). \\ \(K\) & The total number of classes in dataset \(\mathcal{D}\). \\ \((\mathbf{X}^{i},\mathbf{Y}^{i})\) & The \(i\)-th sample and its ground-truth in dataset \(\mathcal{D}\), where \(i\in[1,n]\), \(\mathbf{X}^{i}\in\mathbb{R}^{H\times W\times 3},\mathbf{Y}^{i}\in\mathbb{R}^{H \times W\times K}\). \\ \(f_{\theta}\) & The semantic segmentation model, where \(\theta\) is is parameter. \\ \(f_{\theta}^{c}/f_{\theta}^{d}\) & The encoder/decoder in the semantic segmentation model \(f_{\theta}\). \\ \(\mathbf{Y}^{i}\) & The prediction of model \(f_{\theta}\), where \(\hat{\mathbf{Y}}^{i}=f_{\theta}(\mathbf{X}^{i})\in\mathbb{R}^{H\times W\times K}\). \\ \((\mathbf{X}^{i}_{v,v^{\prime}},\mathbf{Y}^{i}_{w,v}/\hat{\mathbf{Y}}^{i}_{w,v})\) & The sample and its ground-truth/prediction of the \((u,v)\)-th pixel of the \(i\)-th image. \\ \(\mathbf{Y}^{i}_{w,v^{\prime}}\mathbf{Y}^{i}_{w,v}\) & The ground-truth one-hot encoding/prediction of pixel \((\mathbf{X}^{i}_{w,v^{\prime}},\mathbf{Y}^{i}_{w,v})\) in class \(c\). \\ \(\mathcal{D}^{p^{p}}\) & The set of all pixels in dataset \(\mathcal{D}\). \\ \((\mathbf{X}^{p}_{v},\mathbf{Y}^{p}_{v})\) & The \(j\)-th element in \(\mathcal{D}^{p}\), where \(j\in[1,n\times(H-1)\times(W-1)]\). \\ \(f_{\theta}^{(c)}\) & The continuous score function supporting class \(c\). \\ \(\mathcal{N}_{c}\) & The set of pixels with label \(c\) in the set \(\mathcal{D}^{p}\), where \(\mathcal{N}_{c}=\{\mathbf{X}^{p}_{h}|\mathbf{Y}^{p}_{h}=c\}\). \\ \(|\mathcal{A}|\) & The number of elements in set \(\mathcal{A}\). \\ \(\mu\) & The Lipschitz constant. \\ \(\Omega(\cdot)\) & The lower bound. \\ \(\mathcal{N}_{c}/\mathcal{T}_{c}\) & The set of pixels with label \(c\) in the original image/those pixels stored in the T-Memory Bank. \\ \(\mathbf{X}^{p}\) & The sample after replacing some pixels with tail classes pixels from the T-Memory Bank. \\ \(n_{t}\) & The number of tail classes. \\ \(\mathcal{C}_{t}\) & The labels of tail classes, where \(\mathcal{C}_{t}=\{c_{t}\}_{i=1}^{n_{t}}\). \\ \(\mathcal{M}\) & The Memory Branch, where \(\mathcal{M}=\{\mathcal{M}_{c_{t}},\ldots\mathcal{M}_{c_{n_{t}}}\}\). \\ \(S_{M}\) & The memory size. \\ \(R_{S}\) & The sample ratio. \\ \(R_{R}\) & The resize ratio. \\ \(T_{max}\) & The max iteration. \\ \(N_{b}\) & The batch size. \\ \hline \hline \end{tabular}
\end{table}
Table 4: A summary of key notations and descriptions in this work.

**Lemma 3**.: _Let \(\mathbb{E}\left[g\right]\) and \(\hat{\mathbb{E}}\left[g\right]\) represent the expected risk and empirical risk, and \(\mathcal{F}\subseteq\left\{f:\mathcal{X}\rightarrow\mathbb{R}^{K}\right\}\). Then with probability at least \(1-\delta\) over the draw of an i.i.d. sample \(S\) of size \(n\), the generalization bound holds:_

\[\sup_{f\in\mathcal{F}}\left(\mathbb{E}\left[g\left(f\right)\right]-\hat{ \mathbb{E}}\left[g\left(f\right)\right]\right)\leq 2\hat{\mathfrak{R}}_{ \mathcal{F}}\left(g\right)+3\sqrt{\frac{\log\frac{2}{\delta}}{2n}}.\] (12)

**Definition 1** (Fractional Independent Vertex Cover, and Fractional Chromatic Number [103]).: Let a graph be \(G=(V,E)\).

(1) A **fractional vertex cover** of \(G\) is a family \(\left\{\left(F_{j},\omega_{j}\right)\right\}_{j}\) of pairs \(\left(F_{j},\omega_{j}\right)\), where \(F_{j}\subseteq V(G)\), \(\omega_{j}\in(0,1]\), and \(\sum_{j:v\in F_{j}}\omega_{j}=1,\forall v\in V(G)\).

(2) An **independent set** of \(G\) is a set of vertices in \(G\) with no two adjacent. Let \(\mathcal{I}(G)\) denote the set of independent sets of \(G\).

(3) A fractional vertex cover is a **fractional independent vertex cover**\(\left\{\left(I_{j},\omega_{j}\right)\right\}_{j}\) of \(G\) if \(\forall j\), \(I_{j}\in\mathcal{I}(G)\).

(4) A **fractional coloring** of a graph \(G\) is a mapping \(g:\mathcal{I}(G)\rightarrow(0,1]\) such that \(\sum_{I\in\mathcal{I}(G):v\in I}g(I)\geq 1,\forall v\in V(G)\). The **fractional chromatic number**\(\chi_{f}(G)\) is the minimum of the value \(\sum_{I\in\mathcal{I}(G)}g(I)\) over fractional colorings of \(G\).

Notably, the minimum of \(\sum_{j}\omega_{j}\) over all fractional independent vertex covers \(\left\{(I_{j},\omega_{j})\right\}_{j}\) of \(G\) is the fractional chromatic number \(\chi_{f}(G)\).

**Definition 2** (Dependency Graph [44]).: An (undirected) graph \(G=(V,E)\) is called a dependency graph associated with a random vector (or random variables) \(\mathbf{x}=(x_{1},\ldots,x_{m})\) if

(1) \(V(G)=[m]\).

(2) For all disjoint vertex sets \(I,J\subseteq[m]\), if \(I,J\) are not adjacent in \(G\), then random variables \(\left\{x_{i}\right\}_{i\in I}\) and \(\left\{x_{j}\right\}_{j\in J}\) are independent.

A useful result is Janson's decomposition property [44], which combines the concept of dependency graphs with fractional independent vertex covers. The property states that if interdependent random variables \((x_{i})_{i\in[m]}\) is associated with a dependency graph \(G\) with a fractional independent vertex cover \((I_{j},\omega_{j})_{j\in[J]}\), then, the sum of the interdependent variables, can be equivalently decomposed into a weighted sum of sums of independent variables:

\[\sum_{i=1}^{m}x_{i}=\sum_{i=1}^{m}\sum_{j=1}^{J}\omega_{j}\mathbf{1}_{i\in I_{ j}}x_{i}=\sum_{j=1}^{J}\omega_{j}\sum_{i\in I_{j}}x_{i}.\] (13)

### Key Lemmas

**Lemma 4**.: _Let \(\mathcal{D}\) represents the training set, and \(+/-\) respectively denote the categories \(c/c^{\prime}\). The function \(f_{i,j}^{(+)}\) represents the score function for the \((i,j)\)-th pixel in category \(c\). For a set \(\mathcal{A}\), \(\left|\mathcal{A}\right|\) denotes the number of elements in the set. We have:_

\[\begin{split}\mathbb{E}_{\mathcal{D}}\left[c_{auc}^{c,^{\prime} }\right]&=\mathbb{E}_{\mathcal{D}}\left[\sum_{\mathbf{x}_{m}^{ \prime}\in\mathcal{N}_{c}}\sum_{\mathbf{x}_{n}^{\prime}\in\mathcal{N}_{c^{ \prime}}}\frac{1}{\left|\mathcal{N}_{c}\right|\left|\mathcal{N}_{c^{\prime}} \right|}\ell_{sq}^{c,c^{\prime},m,n}\big{|}c,c^{\prime}\right]\\ &=\mathop{\mathbb{E}}_{\mathbf{X}_{1}\sim\mathcal{D}}\left[\tilde{ \ell}_{+,-}^{uner}\right]+\mathop{\mathbb{E}}_{\mathbf{X}_{1},\mathbf{X}_{2} \sim\mathcal{D}}\left[\tilde{\ell}_{+,-}^{uner}\right].\end{split}\] (14)

[MISSING_PAGE_FAIL:20]

Denote \(\rho(x)\) by \(\frac{|\mathcal{D}|n\left(\mathbf{X}^{+}\right)n\left(\mathbf{X}^{-}\right)}{| \mathcal{N}_{+}||\mathcal{N}_{-}|}\). Therefore,

\[||g-\tilde{g}||_{\infty,S} \leq 2\mu\max_{\left(\mathbf{X},\mathbf{Y}\right)\in S}\max_{ \left(i,j\right)\in\mathcal{X}}\rho\left(x\right)\left|f_{i,j}^{\left(+ \right)}\left(\mathbf{X},\mathbf{Y}\right)-\tilde{f}_{i,j}^{\left(+\right)} \left(\mathbf{X},\mathbf{Y}\right)\right|\] (19) \[\leq 2\mu\rho_{\max}\max_{\left(\mathbf{X},\mathbf{Y}\right)\in S }\max_{\left(i,j\right)\in\mathcal{X}}\left|f_{i,j}^{\left(+\right)}\left( \mathbf{X},\mathbf{Y}\right)-\tilde{f}_{i,j}^{\left(+\right)}\left(\mathbf{X}, \mathbf{Y}\right)\right|\] \[=2\mu\rho_{\max}||f-\tilde{f}||_{\infty,S},\]

where \(\rho_{\max}\triangleq\max_{\mathbf{X}}\rho\left(\mathbf{X}\right)\).

Define a \(\frac{\epsilon}{2\mu\rho_{\max}}\)-covering of the class \(\mathcal{F}\) with \(||\cdot||_{\infty}\) norm:

\[\left\{\mathcal{C}_{1},\ldots,\mathcal{C}_{N}\right\},\]

with

\[N=\mathcal{N}\left(\mathcal{F},||\cdot||_{\infty},S,\epsilon/2\mu\rho_{\max} \right).\] (20)

There exists a \(f^{\mathcal{C}_{k}}=\left\{\left\{f_{i,j}^{\mathcal{C}_{k}}\right\}:\left(i,j \right)\in P\left(\mathbf{X}\right)\right\}\), such that for any \(f=\left\{\left\{f_{i,j}^{\left(+\right)}\right\}:\left(i,j\right)\in P\left( \mathbf{X}\right)\right\}\in\mathcal{C}_{k}\cap\mathcal{F}\):

\[\max_{\left(i,j\right)\in P\left(\mathbf{X}\right)}|f_{i,j}^{\left(+\right)}- f_{i,j}^{\mathcal{C}_{k}}|\leq\frac{\epsilon}{2\mu\rho_{\max}},\] (21)

which implies that

\[\max_{\left(i,j\right)\in P\left(\mathbf{X}\right)}|g_{f}-g_{f^{\mathcal{C}_{ k}}}|\leq 2\mu\rho_{\max}\cdot\frac{\epsilon}{2\mu\rho_{\max}}=\epsilon,\] (22)

where \(g_{f}=\ell\circ f\) and \(g_{f^{\mathcal{C}_{k}}}=\ell\circ f^{\mathcal{C}_{k}}\).

Denote

\[\mathcal{C}_{g,i}=\left\{g_{f}:\max_{\left(i,j\right)\in P\left(\mathbf{X} \right)}|g_{f}-g_{f^{\mathcal{C}_{k}}}|\leq\epsilon\right\},\] (23)

then \(\left\{\mathcal{C}_{g,1},\ldots,\mathcal{C}_{g,N}\right\}\) realizes an \(\epsilon\)-covering of \(\ell\circ f\). Hence, the minimum size of the \(\epsilon\)-covering is at most \(N\). Mathematically, we then have:

\[\mathcal{N}\left(\ell\circ\mathcal{F},||\cdot||_{\infty},S,\epsilon\right) \leq\mathcal{N}\left(\mathcal{F},||\cdot||_{\infty},S,\epsilon/2\mu\rho_{\max }\right).\] (24)

This completed the proof. 

**Lemma 6** ([52]).: _Let \(\mathcal{F}\) be a real-valued function class taking values in \([0,1]\), and assume that \(0\in\mathcal{F}\). Let \(S\) be a finite sample of size \(n\). For any \(2\leq p\leq\infty\), we have the following relationship between the Rademacher complexity \(\mathfrak{R}\left(\mathcal{F}\right)\) and the covering number \(\mathcal{N}\left(\mathcal{F},||\cdot||_{p},S,\epsilon\right)\)._

\[\mathfrak{R}\left(\mathcal{F}\right)\leq\inf_{\alpha>0}\left(4\alpha+\frac{12} {\sqrt{n}}\int_{\alpha}^{1}\sqrt{\log\mathcal{N}\left(\mathcal{F},||\cdot||_{p },S,\epsilon\right)}d\epsilon\right).\] (25)

**Lemma 7** ([72, 3]).: _Given a sample \(\tilde{\mathcal{D}}=\left\{\left(\tilde{x}_{i},\tilde{y}_{i}\right)\right\}_{i=1 }^{m}\) where \(\tilde{x}_{i}\in\tilde{\mathcal{X}}\), \(\tilde{y}_{i}\in\tilde{\mathcal{Y}}\) and \(\tilde{\mathcal{D}}\) is associated with a dependency graph \(G\), where \(\chi_{f}(G)\) is its fractional chromatic number, and a loss function \(L:\tilde{\mathcal{X}}\times\tilde{\mathcal{Y}}\times\tilde{\mathcal{F}}\to[0,M]\), where \(\tilde{\mathcal{F}}=\left\{\tilde{f}:\tilde{\mathcal{X}}\right\}\). Then, for any \(\delta\in(0,1)\), the following generalization bound holds with probability at least \(1-\delta\):_

\[\forall\tilde{f}\in\tilde{\mathcal{F}},R(\tilde{f})\leq\widehat{R}_{\widetilde{ D}}(\tilde{f})+2\mathfrak{R}_{\widetilde{D}}^{*}(L\circ\tilde{\mathcal{F}})+3M \sqrt{\frac{\chi_{f}(G)}{2m}\log\left(\frac{2}{\delta}\right)},\] (26)

_where \(\widehat{\mathfrak{R}}_{\widetilde{D}}^{*}(L\circ\tilde{\mathcal{F}})\) is the empirical fractional Rademacher complexity of the loss space._

**Lemma 8**.: _Given a sample \(\tilde{\mathcal{D}}=\left\{\left(\tilde{x}_{i},\tilde{y}_{i}\right)\right\}_{i=1 }^{2m}\) where \(\tilde{x}_{i}\in\tilde{\mathcal{X}}\), \(\tilde{y}_{i}\in\tilde{\mathcal{Y}}\) and \(\tilde{\mathcal{D}}\) is associated with a dependency graph \(G\), where \(\chi_{f}(G)\) is its fractional chromatic number, and each fractional independent vertex cover contains two independent samples, one positive and one negative. Under these conditions, \(\chi_{f}(G)\) satisfies:_

\[\chi_{f}(G)=2\left(2m-1\right)\] (27)Proof.: The calculation of the fractional chromatic number can be transformed into finding how many groups can be formed where each group contains \(m\) ordered pairs of positive and negative samples. In each group, each sample appears only once, and there are no duplicate ordered pairs of positive and negative samples across all groups.

For example, in a dataset where \(2m=4\), there exist \(6\) groups:

Group 1: \[(1,2),(3,4)\] Group 2: \[(2,1),(4,3)\] Group 3: \[(1,3),(2,4)\] Group 4: \[(3,1),(4,2)\] Group 5: \[(1,4),(2,3)\] Group 6: \[(4,1),(3,2)\]

Therefore, \(\chi_{f}(G)=6\).

For \(2m\) samples, we can extract \(A_{2m}^{2}\) ordered pairs of positive and negative samples. Since these samples have an equal status in the dataset, they appear the same number of times among these \(A_{2m}^{2}\) pairs.

After selecting the first group from these \(A_{2m}^{2}\) pairs, \(A_{2m}^{2}-m\) pairs of positive and negative samples remain. The frequency of each sample appearing in these remaining pairs remains equal.

We continue to select the second group, and so on, until the last group. The frequency of each sample in the remaining pairs still remains equal.

Thus, we can find \(\frac{A_{2m}^{2}}{m}\) groups, _i.e._, \(\chi_{f}(G)=2(2m-1)\).

This completed the proof. 

### Proof of the Main Result

**Restate of Theorem 1** (Generalization Bound for AUCSeg).: Let \(\mathbb{E}_{\mathcal{D}}\left[\hat{\mathcal{L}}_{\mathcal{D}}\left(f\right)\right]\) be the population risk of \(\hat{\mathcal{L}}_{\mathcal{D}}\left(f\right)\). Assume \(\mathcal{F}\subseteq\left\{f:\mathcal{X}\rightarrow\mathbb{R}^{H\times W \times K}\right\}\), where \(H\) and \(W\) represent the height and width of the image, and \(K\) represents the number of categories, \(\hat{\mathcal{L}}^{(i)}\) is the risk over \(i\)-th sample, and is \(\mu\)-Lipschitz with respect to the \(l_{\infty}\) norm, (_i.e._\(\|\hat{\mathcal{L}}(x)-\hat{\mathcal{L}}(y)\|_{\infty}\leq\mu\cdot\|x-y\|_{\infty}\)). There exists three constants \(A>0\), \(B>0\) and \(C>0\), the following generalization bound holds with probability at least \(1-\delta\) over a random draw of i.i.d training data (at the image-level):

\[\left|\hat{\mathcal{L}}_{\mathcal{D}}\left(f\right)-\mathbb{E}_{ \mathcal{D}}\left[\hat{\mathcal{L}}_{\mathcal{D}}\left(f\right)\right]\right| \leq\frac{8}{N}+ \frac{\eta_{\text{inner}}+\eta_{\text{inter}}}{\sqrt{N}}\sqrt{A \log\left(2B\mu\tau Nk+C\right)}\] \[+3\left(\sqrt{\frac{1}{2N}}+K\sqrt{1-\frac{1}{N}}\right)\sqrt{ \log\left(\frac{4K(K-1)}{\delta}\right)},\]

where

\[\eta_{\text{inner}}=\frac{48\mu\tau\ln N}{N},\quad\eta_{\text{inter}}=2\sqrt{ 2}\tau,\]

\[\tau=\left(\max_{c\in[K]}\frac{n_{\max}^{(c)}}{n_{\text{mean}}^{(c)}}\right) ^{2},\]

\(n_{\max}^{(c)}=\max_{\mathbf{X}}n(\mathbf{X}^{(c)})\), \(n_{mean}^{(c)}=\sum_{i=1}^{N}n(\mathbf{X}_{i}^{(c)})\), \(N=|\mathcal{D}|\), \(k=H\times W\) and \(\mathbf{X}^{(c)}\) represents the pixel of class \(c\) in image \(\mathbf{X}\).

Proof.: First, we find that the calculation of pair-wise AUC requires both positive and negative samples. These two samples can come from the same image or from two different images. Therefore,we transform the original problem into two sub-problems:

\[\left|\hat{\mathcal{L}}_{\mathcal{D}}\left(f\right)-\mathbb{E}_{ \mathcal{D}}\left[\hat{\mathcal{L}}_{\mathcal{D}}\left(f\right)\right]\right|\] \[=\sup_{f\in\mathcal{F}}\left[\frac{1}{K(K-1)}\sum_{c,c^{\prime}} \ell_{anc}^{c,c^{\prime}}-\frac{1}{K(K-1)}\sum_{c,c^{\prime}}\mathbb{E}_{ \mathcal{D}}\left[\ell_{anc}^{c,c^{\prime}}\right]\right]\] \[\stackrel{{\eqref{eq:L1}}}{{\leq}}\frac{1}{K(K-1)} \sum_{c,c^{\prime}}\left[\sup_{f\in\mathcal{F}}\left(\ell_{anc}^{c,c^{\prime}}- \mathbb{E}_{\mathcal{D}}\left[\ell_{anc}^{c,c^{\prime}}\right]\right)\right]\] (28) \[\stackrel{{\eqref{eq:L2}}}{{=}}\frac{1}{K(K-1)} \sum_{c,c^{\prime}}\left[\underbrace{\sup_{f\in\mathcal{F}}\left(\ell_{+,-}^ {\text{inner}}-\mathbb{E}_{\mathcal{D}}\left[\bar{\ell}_{+,-}^{\text{inner}} \right]\right)}_{\text{Part 1}}+\underbrace{\sup_{f\in\mathcal{F}}\left(\ell_{+,-}^ {\text{inter}}-\mathbb{E}_{\mathcal{D}}\left[\bar{\ell}_{+,-}^{\text{inter}} \right]\right)}_{\text{Part 2}}\right],\]

where \(+/-\) respectively denote the categories \(c/c^{\prime}\).

For Part 1, we use the complexity measure technique of the covering number.

Assuming \(\log\mathcal{N}\left(\mathcal{F},||\cdot||_{\infty},S,\epsilon\right)\leq \frac{A}{\epsilon^{2}}\log\left[\frac{B}{\epsilon}\cdot|\mathcal{D}|\cdot k+C\right]\), where \(k=H\times W\) is the pixel count in an image. \(A\), \(B\) and \(C\) represent constants. Based on Lemma 5, we have:

\[\log\mathcal{N}\left(\ell\circ\mathcal{F},||\cdot||_{\infty},S, \epsilon\right) \leq\log\mathcal{N}\left(\mathcal{F},||\cdot||_{\infty},S, \epsilon/2\mu\rho_{\max}\right)\] (29) \[\leq\frac{4A\mu^{2}\rho_{\max}^{2}}{\epsilon^{2}}\log\left[\frac {2B\mu\rho_{\max}}{\epsilon}\cdot|\mathcal{D}|\cdot k+C\right],\]

\[\rho_{\max} =\max_{\mathbf{X}}\frac{|\mathcal{D}|\,n\left(\mathbf{X}^{+} \right)n\left(\mathbf{X}^{-}\right)}{|\mathcal{N}_{+}|\,|\mathcal{N}_{-}|}\] \[=\frac{1}{|\mathcal{D}|}\cdot\frac{n_{\max}^{+}\cdot n_{\max}^{ -}}{n_{\text{mean}}^{+}\cdot n_{\text{mean}}^{-}}\] \[\leq\frac{1}{|\mathcal{D}|}\cdot\tau\]

where:

\[n_{\max}^{+}=\max_{\mathbf{X}\in\mathcal{X}}n(\mathbf{X}^{+}),\] \[n_{\max}^{-}=\max_{\mathbf{X}\in\mathcal{X}}n(\mathbf{X}^{-}),\] \[n_{mean}^{+}=\sum_{i=1}^{|D|}n(\mathbf{X}_{i}^{+}),\] \[n_{mean}^{-}=\sum_{i=1}^{|D|}n(\mathbf{X}_{i}^{-}),\]

and \(\tau=\left(\max_{c\in[K]}\frac{n_{\max}^{(c)}}{n_{\max}^{(c)}}\right)^{2}\).

Denoted by \(a:=4A\mu^{2}\rho_{\max}^{2}\), \(b:=2B\mu\rho_{\max}\left|\mathcal{D}\right|k\) and \(c:=C\). Based on Lemma 6 and Lemma A.3 in [54], we have:

\[\begin{split}\hat{\mathfrak{R}}\left(\ell\circ\mathcal{F}\right)& \leq\inf_{\alpha>0}\left(4\alpha+\frac{12}{\sqrt{\left| \mathcal{D}\right|}}\int_{\alpha}^{1}\sqrt{\log\mathcal{N}\left(\ell\circ \mathcal{F},||\cdot||_{\infty},S,\epsilon\right)}d\epsilon\right)\\ &\leq\inf_{\alpha>0}\left(4\alpha+\frac{12}{\sqrt{\left|\mathcal{D }\right|}}\int_{\alpha}^{1}\frac{\sqrt{a\log\left(b/\epsilon+c\right)}}{ \epsilon}d\epsilon\right)\\ &\leq\frac{4}{\left|\mathcal{D}\right|}+\frac{12}{\sqrt{\left| \mathcal{D}\right|}}\int_{1/\left|\mathcal{D}\right|}^{1}\frac{\sqrt{a\log \left(b\left|\mathcal{D}\right|+c\right)}}{\epsilon}d\epsilon\\ &=\frac{4}{\left|\mathcal{D}\right|}+\frac{12\ln\left|\mathcal{D }\right|}{\sqrt{\left|\mathcal{D}\right|}}\sqrt{a\log\left(b\left|\mathcal{D} \right|+c\right)}.\end{split}\] (30)

Substituting this result into Lemma 3, with probability at least \(1-\delta\), we have

\[\begin{split}\sup_{f\in\mathcal{F}}&\left(\ell_{+,- }^{\text{inner}}-\mathbb{E}_{\mathcal{D}}\left[\tilde{\ell}_{+,-}^{\text{inner }}\right]\right)\\ &\leq\frac{8}{\left|\mathcal{D}\right|}+\frac{48\mu\tau\ln\left| \mathcal{D}\right|}{\left|\mathcal{D}\right|^{1.5}}\sqrt{A\log\left(2B\mu\tau \left|\mathcal{D}\right|k+C\right)}+3\sqrt{\frac{\log\left(\frac{4K(K-1)}{ \delta}\right)}{2\left|\mathcal{D}\right|}}.\end{split}\] (31)

For Part 2, we use the complexity measure technique of the fractional chromatic number and covering number.

According to Lemma 7, calculating the generalization bound only requires knowing the chromatic complexity. Based on Equation (13), we have

\[\hat{\mathfrak{R}}\left(\ell\circ\mathcal{F}\right)=\frac{1}{\left|\mathcal{D }\right|\left(\left|\mathcal{D}\right|-1\right)}\mathbb{E}_{\sigma}\left[\sum_ {j\in[J]}w_{j}\left(\sup_{\begin{subarray}{c}f\in\mathcal{F}\\ (i_{1},j_{1})\in I_{j}\\ (i_{2},j_{2})\in I_{j}\end{subarray}}\sigma_{i,j}^{1.2}\,\rho(x)\ell_{sq} \left(\tilde{f}_{i_{1},j_{1}}^{(+)},\tilde{f}_{i_{2},j_{2}}^{(+)},\mathbf{X}, \mathbf{Y}\right)\right)\right],\] (32)

where \(\rho(x)=\frac{\left|\mathcal{D}\right|\left(\left|\mathcal{D}\right|-1\right)n \left(\mathbf{X}^{+}\right)n\left(\mathbf{X}^{-}\right)}{\left|\mathcal{N}_{ +}\right|\left|\mathcal{N}_{-}\right|}\), and \(\omega_{j}\) denotes the weight assigned to the subset in the fractional vertex cover.

Similar to Part 1, using the covering number we can get

\[\begin{split}\mathfrak{R}_{j}=\sup_{f\in\mathcal{F}}& \sum_{\begin{subarray}{c}(i_{1},j_{1})\in I_{j}\\ (i_{2},j_{2})\in I_{j}\end{subarray}}\sigma_{i,j}^{1.2}\,\rho(x)\ell_{sq} \left(\tilde{f}_{i_{1},j_{1}}^{(+)},\tilde{f}_{i_{2},j_{2}}^{(+)},\mathbf{X}, \mathbf{Y}\right)\lesssim\sqrt{c_{j}\cdot m_{j}},\end{split}\] (33)

where we define \(c_{j}\) as \(\ln^{2}\left|\mathcal{D}\right|\cdot A\cdot\mu^{2}\cdot\rho_{\max}^{2}\cdot \log\left(2B\cdot\mu\cdot\rho_{\max}\cdot\left|\mathcal{D}\right|^{2}\cdot k+C\right)\) as \(c_{j}\), and define \(m_{j}\) as \(|I_{j}|\).

Assume that the number of images in the dataset is even, _i.e._, \(\left|\mathcal{D}\right|\equiv 0\pmod{2}\). We have:

\[\hat{\mathfrak{R}}\left(\ell\circ\mathcal{F}\right) =\frac{1}{\left|\mathcal{D}\right|\left(\left|\mathcal{D}\right|- 1\right)}\sum_{j\in\left[J\right]}w_{j}\mathfrak{R}_{j}\] (34) \[\lesssim\frac{1}{\left|\mathcal{D}\right|\left(\left|\mathcal{D} \right|-1\right)}\sum_{j\in\left[J\right]}w_{j}\sqrt{c_{j}\cdot m_{j}}\] \[=\frac{\chi_{f}(G)}{\left|\mathcal{D}\right|\left(\left|\mathcal{ D}\right|-1\right)}\sum_{j\in\left[J\right]}\frac{w_{j}}{\chi_{f}(G)}\sqrt{c_{j}m_{j}}\] \[\leq\frac{\sqrt{\chi_{f}(G)}}{\left|\mathcal{D}\right|\left( \left|\mathcal{D}\right|-1\right)}\sqrt{\sum_{j\in\left[J\right]}w_{j}m_{j}c_ {j}}\] \[=\frac{\sqrt{\chi_{f}(G)}}{\sqrt{\left|\mathcal{D}\right|\left( \left|\mathcal{D}\right|-1\right)}}\sqrt{\frac{\sum\limits_{j\in\left[J \right]}w_{j}m_{j}c_{j}}{\left|\mathcal{D}\right|\left(\left|\mathcal{D}\right| -1\right)}}\] \[\overset{(*)}{\lesssim}\frac{1}{\sqrt{\left|\mathcal{D}\right|/ 2}}\sqrt{\frac{\sum\limits_{j\in\left[J\right]}w_{j}c_{j}}{2(\left|\mathcal{D }\right|-1)}},\]

where \((*)\) follows the Lemma 8.

The second-order inequality is based on the fact that:

\[\mathfrak{R}_{j}\lesssim\sqrt{c_{j}\cdot m_{j}},\] (35)

define \(\rho_{\max}^{j}\) as the largest \(\rho\) in the \(j\)-th cluster, and that:

\[\sqrt{c_{j}} =\rho_{\max}^{j}\cdot\sqrt{A\log(2B\mu\rho_{\max}\left|\mathcal{D }\right|^{2}k+C)}\] (36) \[\leq\frac{\left|\mathcal{D}\right|\left(\left|\mathcal{D}\right| -1\right)n_{\max}^{+}n_{\max}^{-}}{\left|\mathcal{N}_{+}\right|\left| \mathcal{N}_{-}\right|}\sqrt{A\log(2B\mu\rho_{\max}\left|\mathcal{D}\right| ^{2}k+C)}\] \[\approx\frac{n_{\max}^{+}\cdot n_{\max}^{-}}{n_{\text{mean}}^{+} \cdot n_{\max}^{-}}\sqrt{A\log(2B\mu\rho_{\max}\left|\mathcal{D}\right|^{2}k +C)}\] \[\leq\tau\sqrt{A\log(2B\mu\tau\left|\mathcal{D}\right|k+C)}\]

where:

\[n_{\max}^{+} =\max_{\mathbf{X}\in\mathcal{X}}n(\mathbf{X}^{+}),\] \[n_{\max}^{-} =\max_{\mathbf{X}\in\mathcal{X}}n(\mathbf{X}^{-}),\] \[n_{mean}^{+} =\sum_{i=1}^{\left|\mathcal{D}\right|}n(\mathbf{X}_{i}^{+}),\] \[n_{mean}^{-} =\sum_{i=1}^{\left|\mathcal{D}\right|}n(\mathbf{X}_{i}^{-}).\]

and \(\tau=\left(\max_{c\in\left[K\right]}\frac{n_{c}^{(c)}}{n_{\min}^{(c)}}\right)^ {2}\).

Combining Equation (34) and Equation (36), we obtain:

\[\hat{\mathfrak{R}}\left(\ell\circ\mathcal{F}\right)\leq\frac{1}{\sqrt{\left| \mathcal{D}\right|/2}}\cdot\tau\sqrt{A\log(2B\mu\tau\left|\mathcal{D}\right|k +C)}.\] (37)Based on Lemma 7 and Lemma 8, it comes:

\[\begin{split}&\sup_{f\in\mathcal{F}}\left(\ell^{\text{inter}}_{+,-}- \mathbb{E}_{\mathcal{D}}\left[\tilde{\ell}^{\text{inter}}_{+,-}\right]\right)\\ &\leq 2\hat{\mathfrak{R}}\left(\ell\circ\mathcal{F}\right)+3M\sqrt{ \frac{\chi_{f}(G)}{2m}\log\left(\frac{4K(K-1)}{\delta}\right)}\\ &\leq\frac{2\sqrt{2}}{\sqrt{|\mathcal{D}|}}\cdot\tau\sqrt{A\log \left(2B\mu\tau\left|\mathcal{D}\right|k+C\right)}+3K\sqrt{\frac{|\mathcal{D} |-1}{|\mathcal{D}|}\log\left(\frac{4K(K-1)}{\delta}\right)}.\end{split}\] (38)

Therefore, by combining Equation (28),Equation (31) and Equation (38), we can obtain:

\[\begin{split}\left|\hat{\mathcal{L}}_{\mathcal{D}}\left(f \right)-\mathbb{E}_{\mathcal{D}}\left[\hat{\mathcal{L}}_{\mathcal{D}}\left(f \right)\right]\right|&\leq\frac{8}{N}+\frac{\eta_{\text{inner}} +\eta_{\text{inter}}}{\sqrt{N}}\sqrt{A\log\left(2B\mu\tau Nk+C\right)}\\ &\hskip 142.26378pt+3\left(\sqrt{\frac{1}{2N}}+K\sqrt{1-\frac{1}{N} }\right)\sqrt{\log\left(\frac{4K(K-1)}{\delta}\right)},\end{split}\] (39)

where \(\eta_{\text{inner}}=\frac{48\mu\tau\ln N}{N}\), \(\eta_{\text{inter}}=2\sqrt{2}\tau\), \(N=|\mathcal{D}|\) and \(k=H\times W\).

This completed the proof. 

## Appendix C Proof for Propositions of Tail-class Memory Bank

**Restate of Proposition 1.** Consider a dataset \(\mathcal{D}\) that includes images with \(K\) different pixel categories. Let \(p_{i}\) represent the probability of observing a pixel with label \(i\) in a given image. Randomly select \(B\) images from \(\mathcal{D}\) as training data, where

\[B=\Omega\left(\frac{\log(\delta/K)}{\log(1-\min_{i}p_{i})}\right).\]

Then with probability at least \(1-\delta\), for any \(c\in[K]\), there exists \(\mathbf{X}\) in the training data that contains pixels of label \(c\).

Proof.: Define event \(A_{j}\) as the extraction of \(N\) images where the pixels of class \(j\) appear at least once. Let \(p_{m}=\min\{p_{1},p_{2},\ldots,p_{K}\}\), where \(m\in[K]\).

The probability that each category appears at least once when randomly selecting \(N\) images:

\[\begin{split}\mathbb{P}\left(\bigcap_{i=1}^{K}A_{i}\right)& =1-\mathbb{P}\left(\bigcup_{i=1}^{K}\overline{A_{i}}\right)\\ &\geq 1-\sum_{i=1}^{K}\mathbb{P}\left(\overline{A_{i}}\right)\\ &=1-\sum_{i=1}^{K}(1-p_{i})^{B}\\ &\geq 1-K(1-p_{m})^{B}\end{split}\] (40)

When \(\mathbb{P}\left(\bigcap_{i=1}^{K}A_{i}\right)\geq 1-\delta\),

\[\delta\geq K(1-p_{m})^{B}\] (41)

Therefore,

\[B=\Omega\left(\frac{\log(\delta/K)}{\log(1-p_{m})}\right)=\Omega\left(\frac{ \log(\delta/K)}{\log(1-\min_{i}p_{i})}\right),\] (42)

This completed the proof.

Details of T-Memory Bank Algorithm

In this section, Algorithm 2 provides a full version of Algorithm 1.

``` Input: Training data \(\mathcal{D}\), number of tail classes \(n_{t}\), labels of tail classes \(\mathcal{C}_{t}=\{c_{i}\}_{i=1}^{n_{t}}\), Memory Branch \(\mathcal{M}=\{\mathcal{M}_{c_{1}},\ldots\mathcal{M}_{c_{m}}\}\), memory size \(S_{M}\), sample ratio \(R_{S}\), resize ratio \(R_{R}\), max iteration \(T_{max}\), batch size \(N_{b}\) Output: model parameters \(\theta\)
1for\(iter=1\) to\(T_{max}\)do
2\(\mathcal{D}_{B}=\{(\mathbf{X}_{i},\mathbf{Y}_{i})\}_{i=1}^{N_{b}}\gets Sample Batch( \mathcal{D},N_{b})\);
3\(\mathcal{C}_{miss}\subseteq\mathcal{C}_{t}\gets MissingTailClass( \mathcal{D}_{B})\);
4\(\overline{\mathcal{C}_{miss}}=\mathcal{C}_{t}-\mathcal{C}_{miss}\);
5\(\triangleright\) Store Branch
6for\(\overline{c_{m}}\) in \(\overline{\mathcal{C}_{miss}}\)do
7 Divide a picture containing only the \(\overline{c_{m}}\)-th class pixels from \(\mathcal{D}_{B}\) and name it \(\mathcal{P}\);
8if\(|\mathcal{M}_{\overline{c_{m}}}|<S_{M}\)then
9 Add the divided image \(\mathcal{P}\) to \(\mathcal{M}_{\overline{c_{m}}}\);
10
11else
12 Randomly replace an image in \(\mathcal{M}_{\overline{c_{m}}}\) with the divided image \(\mathcal{P}\);
13
14\(\triangleright\) Retrieve Branch
15\(n_{sample}=\lceil|\mathcal{C}_{miss}|\times R_{S}\rceil\);
16for\(i=1\) to \(n_{sample}\)do
17 Randomly choose \(c_{m}\) from \(\mathcal{C}_{miss}\);
18 Remove \(c_{m}\) in \(\mathcal{C}_{miss}\);
19if\(|\mathcal{M}_{c_{m}}|\neq 0\)then
20 Sample from \(\mathcal{M}_{c_{m}}\), scale according to \(R_{R}\), paste randomly into \(\mathcal{D}_{B}\);
21\(\triangleright\) Semantic Segmentation
22\(\hat{\mathbf{Y}}_{i}\gets f_{\theta}(\mathbf{X}_{i})\);
23 Calculate \(\ell=\tilde{\ell}_{auc}+\lambda\ell_{ce}\) with Equation (2) and Equation (8);
24 Backpropagation updates \(\theta\). ```

**Algorithm 2**AUCSeg Algorithm (Full Version)

## Appendix E More Discussions about T-Memory Bank

### Discussion on the Improved Version of Stratified Sampling

In this section, we introduce the definition of the improved version of stratified sampling and explain why it is not applicable to the PLSS task.

**The definition of the improved version of stratified sampling.** The improved version of stratified sampling starts by grouping images according to their categories. Due to the multi-label nature, different categories may contain the same images. After that, stratified sampling is applied to each category, making sure that even if an image is sampled more than once (as both head and tail), each mini-batch still includes at least one sample from every category.

**Reasons for inapplicability to PLSS task.** Conventional stratified sampling can hardly cover all the involved classes with a small batch size. To ensure coverage, one has to employ a much larger batch size, which results in a significantly higher computational burden. Although the improved version of stratified sampling can cover all classes, images from tail classes may appear repeatedly, leading to overfitting and consequently a degradation in performance. In contrast, our Tail-class Memory Bank only involves pasting a portion of one image onto another, effectively functioning as an implicit data augmentation. This approach mitigates the sampling problem without compromising generalization ability. The following empirical results support our assertion.

First, we counted the number of images containing pixels from each class in the Cityscapes, ADE20K, and COCO-Stuff 164K datasets. The results are shown in Table 5, Table 6, and Table 7.

The results indicate that images containing tail class pixels are very limited. Specifically, in the ADE20K dataset, there are only \(41\) images in the training set of \(20210\) images that contain pixels from the tail class with ID \(97\). As a result, tail class images are repeatedly sampled when using stratified sampling, leading to overfitting on such repeated images.

Next, we trained on the ADE20K dataset using the improved version of the stratified sampling method. The results show that, compared to using the Tail-class Memory Bank, the performance on tail classes dropped by over \(3\%\) due to heavy sample repetition in the batch. However, our T-Memory Bank, with its random pasting technique, diversifies the backgrounds of the tail classes, enabling the model to better learn the features of these tail classes.

### Discussion on Why the T-Memory Bank Works

In this section, we discuss why the primary function of the T-Memory Bank is not to enhance the diversity of tail samples.

As shown in Table 3, using a memory bank does indeed increase the diversity of tail classes as an implicit form of augmentation (comparing the rows for SegNeXt and SegNeXt+TMB in the table). However, we cannot rely solely on the bank to fully address the long-tail issue, as the bank's capacity

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c c c c c} \hline \hline Class ID & **5** & **0** & **2** & **8** & **13** & **1** & **7** & **10** & **11** & **6** & **9** & **18** & **4** & **12** & **3** & **17** & **14** & **15** & **16** \\
**Num** & 2949 & 2934 & 2934 & 2891 & 2832 & 2811 & 2808 & 2686 & 2343 & 1658 & 1654 & 1646 & 1296 & 1023 & 970 & 513 & 359 & 274 & 142 \\ \hline \hline \end{tabular}
\end{table}
Table 6: The number of images containing pixels from each class in the **ADE20K**. Class ID represents the class number in the original dataset.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c c c c c} \hline \hline Class ID & **1** & **4** & **3** & **5** & **6** & **2** & **13** & **9** & **16** & **18** & **7** & **23** & **15** & **20** & **21** & **37** & **12** & **11** & **44** \\
**Num** & 11588 & 9314 & 8240 & 6674 & 6579 & 6042 & 5069 & 4687 & 4266 & 3995 & 3990 & 3275 & 3276 & 3258 & 3161 & 3083 & 3061 & 2851 & 2646 \\ \hline Class ID & **83** & **10** & **19** & **88** & **5** & **14** & **70** & **42** & **25** & **33** & **67** & **136** & **28** & **24** & **126** & **48** & **31** & **29** \\
**Num** & 2508 & 2421 & 2148 & 1997 & 1355 & 1791 & 1600 & 1447 & 1437 & 1404 & 1355 & 1308 & 1281 & 1229 & 1191 & 1191 & 1191 & 1117 & 1132 \\ \hline Class ID & **68** & **135** & **94** & **99** & **58** & **54** & **39** & **43** & **65** & **35** & **149** & **22** & **34** & **139** & **26** & **70** & **27** & **113** & **90** \\
**Num** & 1112 & 1020 & 992 & 965 & 930 & 880 & 800 & 799 & 792 & 781 & 773 & 702 & 698 & 671 & 667 & 658 & 650 & 622 & 618 \\ \hline Class ID & **86** & **60** & **53** & **103** & **143** & **45** & **67** & **72** & **116** & **137** & **32** & **148** & **82** & **49** & **50** & **111** & **138** & **96** & **84** \\
**Num** & 583** & 564 & 561 & 566 & 556 & 549 & 549 & 532 & 531 & 530 & 528 & 521 & 504 & 492 & 497 & 468 & 465 & 425 & 451 & 440 \\ \hline Class ID & **150** & **124** & **41** & **38** & **51** & **140** & **466** & **36** & **73** & **46** & **101** & **128** & **109** & **64** & **71** & **76** & **61** & **125** & **47** \\
**Num** & 421 & 417 & 411 & 404 & 402 & 397 & 395 & 378 & 369 & 367 & 354 & 347 & 340 & 335 & 330 & 324 & 303 & 319 & 310 \\ \hline Class ID & **98** & **77** & **49** & **133** & **117** & **63** & **134** & **69** & **122** & **75** & **62** & **81** & **130** & **124** & **144** & **119** & **145** & **132** & **57** \\
**Num** & 307 & 304 & 287 & 284 & 282 & 275 & 268 & 266 & 266 & 265 & 261 & 247 & 246 & 228 & 217 & 213 & 206 & 201 & 198 \\ \hline Class ID & **95** & **93** & **147** & **56** & **78** & **74** & **99** & **120** & **85** & **91** & **52** & **146** & **100** & **121** & **102** & **131** & **105** & **122** & **141** \\
**Num** & 181** & **178** & **178** & **172** & 170 & 144 & 139 & 136 & 136 & 135 & 133 & 126 & 117 & 116 & 108 & 108 & 99 & 97 & 92 \\ \hline Class ID & **55** & **92** & **114** & **108** & **118** & **89** & **79** & **107** & **110** & **80** & **115** & **123** & **106** & **104** & **129** & **112** & **97** & **9** \\
**Num** & 84** & 83 & 80 & 77 & 73 & 71 & 68 & 66 & 66 & 65 & 579 & 58 & 57 & 52 & 52 & 50 & 41 & 41 \\ \hline \hline \end{tabular}
\end{table}
Table 5: The number of images containing pixels from each class in the **Cityscapes**. Class ID represents the class number in the original dataset.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c c c c} \hline \hline Class ID & **0** & **157** & **145** & **160** & **93** & **84** & **112** & **120** & **164** & **128** & **111** & **153** & **137** & **169** & **155** & **56** & **2** & **60** & **118** \\
**Num** & 6395 & 5666 & 31898 & 31481 & 27677 & 2302 & 231575 & 2255 & 19695 & 18311 & 17882 & 16282 & 15420 & 14590 & 13095 & 12757 & 12283 & 16504 & 11772 \\ \hline Class ID & **101** & **131** & **94** & **99** & **94** & **85** & **102** & **17** & **180** & **48** & **103** & **91** & **86** & **48** & **26** & **109** & **146** & **106** & **143** \\
**Num** & 1135 & 1117 & 10546 & 10638 & 58699 & 95222 & 3521 & 9475 & 9108 & 58938 & 2617 & 1716 & 10825 & 6732 & 6947 & 6672 & 6642 & 6618 & 6is always limited. This is why we also need to consider the problem from the perspective of the loss function. We find that the AUC loss focuses only on the ranking loss between positive and negative samples and is not sensitive to data distribution, fundamentally avoiding the risk of underfitting caused by insufficient training samples. **We believe that the use of the T-Memory Bank is intended to both facilitate the effectiveness of the AUC loss and enhance the diversity of tail samples.**

### Discussion on AUC and Contrastive Learning from the Perspective of the Loss Function

In this section, we compare AUC and contrastive learning from the perspective of loss functions.

**Theorem 2** (Comparison between AUC Loss and Contrastive Loss).: _Minimizing the weighted contrastive loss approximately corresponds to minimizing an upper bound of the logistic AUC loss:_

\[\sum_{i}w_{i}\left[-\log\frac{e^{f(x^{i})}}{e^{f(x^{i})}+\sum_{j\neq i}w_{j}e ^{f(x^{i})}}\right]\geq\sum_{i}\sum_{j\neq i}\frac{1}{n_{i}n_{j}}\left[-\log \left(\frac{1}{1+e^{f(x^{j})-f(x^{i})}}\right)\right],\]

_where, \(w_{j}=\frac{1/n_{j}}{\sum_{k\neq i}1/n_{k}}\) and \(w_{i}=\frac{\sum_{k\neq i}1/n_{k}}{n_{i}}\)._

Proof.: For the AUC loss under the logistic surrogate loss function:

\[\ell_{auc}^{logistic} =\ell_{logistic}\left(f(x^{+})-f(x^{-})\right)\] \[=\sum_{i}\sum_{j\neq i}\frac{1}{n_{i}n_{j}}\left[-\log\left( \frac{1}{1+e^{f(x^{j})-f(x^{i})}}\right)\right]\] \[=\sum_{i}\sum_{j\neq i}w_{i}w_{j}\left[-\log\left(\frac{1}{1+e^{f (x^{j})-f(x^{i})}}\right)\right]\] \[=\sum_{i}w_{i}\sum_{j\neq i}w_{j}\left[-\log(e^{f(x^{i})})+\log( e^{f(x^{i})}+e^{f(x^{j})})\right]\] \[=\sum_{i}w_{i}\left[-\log(e^{f(x^{i})})+\sum_{j\neq i}w_{j}\log( e^{f(x^{i})}+e^{f(x^{j})})\right]\] \[\leq\sum_{i}w_{i}\left[-\log(e^{f(x^{i})})+\log\left(\sum_{j\neq i }w_{j}(e^{f(x^{i})}+e^{f(x^{j})})\right)\right]\] \[=\sum_{i}w_{i}\left[-\log(e^{f(x^{i})})+\log\left(e^{f(x^{i})}+ \sum_{j\neq i}w_{j}e^{f(x^{j})}\right)\right]\] \[=\sum_{i}w_{i}\left[-\log\frac{e^{f(x^{i})}}{e^{f(x^{i})}+\sum_{j \neq i}w_{j}e^{f(x^{j})}}\right]\]

where, \(w_{j}=\frac{1/n_{i}n_{j}}{\sum_{k\neq i}1/n_{i}n_{k}}=\frac{1/n_{j}}{\sum_{k \neq i}1/n_{k}}\) and \(w_{i}=\frac{\sum_{k\neq i}1/n_{k}}{n_{i}}\).

This completed the proof. 

The theorem indicates that minimizing a **weighted version** of contrastive loss can implicitly optimize the ovo logistic AUC loss. This paper adopts a more general form of AUC loss, in which various surrogate loss functions are explored.

Additional Experimental Settings

In this section, we make a supplementation to Section 5.1.

### Datasets

We use three datasets in our experiments: Cityscapes, ADE20K, and COCO-Stuff 164K.

**Cityscapes**[19] is a dataset of urban road traffic scenes, with each image sized at \(1024\times 2048\) pixels. It consists of \(5000\) images with pixel-level annotations across \(19\) classes. The training, validation, and testing set numbers are \(2975\), \(500\), and \(1525\), respectively.

**ADE20K**[109] is a benchmark for scene parsing with \(150\) class labels. It includes over \(25000\) images, with \(20210\), \(2000\), and \(3352\) images used for training, validation, and testing, respectively.

**COCO-Stuff 164K**[9] is a large-scale dataset with \(164K\) images. It is finely annotated across \(171\) classes.

We split the three datasets into head class, middle class, and tail class based on the proportion of pixels for each class in the training set. Table 8, Table 9, and Table 10 provide the details of these partitions.

### Implementation Details

**Network Architecture.** We perform all experiments using mmsegmentation [18] on an NVIDIA 3090 GPU. For our model, we use SegNeXt [32] as the backbone and pretrain all encoders on the ImageNet-1K [22] dataset.

**Data Augmentation.** For Cityscapes, we resize the images to \(1024\times 2048\), randomly crop them to \(1024\times 1024\), and then apply random horizontal flips. For ADE20K and COCO-Stuff 164K, the resizing is set to \(512\times 2048\), random cropping is done at \(512\times 512\), and random horizontal flips are applied as well.

**Training Strategy.** We use _Adam with Weight Decay (AdamW)_[59] optimizer with an initial learning rate of \(6e{-5}\) and a weight decay of \(0.01\). We adopt the 'poly' learning rate policy, where the initial learning rate is multiplied by \(1-\frac{iter}{max\_iter}\). Moreover, a 'linear' warmup strategy is employed at the beginning of training, allowing the learning rate to increase from \(1e{-6}\) to the initial learning rate within \(1500\) iterations. The batch size is set to \(2\) for the Cityscapes dataset and \(4\) for all the other datasets. The total number of iterations is \(160000\) on Cityscapes and ADE20K and \(80000\) on COCO-Stuff 164K.

**Evaluation Metrics.** Following the setup outlined by SegNeXt [32], we conduct experiments using the _mean of Intersection over Union (mIoU)_ as the evaluation metric on the validation set.

### Competitors

Here we give a more detailed summary of the competitors mentioned in the experiments.

We compared our method with \(13\) recent advancements and \(6\) long-tail approaches in semantic segmentation. The recent advancements include DeepLabV3+, EncNet, FastFCN, EMANet, DANet, HRNet, OCRNet, DNLNet, PointRend, BiSeNetV2, ISANet, STDC, and SegNeXt. The long-tail methods are VS, LA, LDAM, Focal Loss, DisAlign, and BLV, all based on SegNeXt. **To ensure fairness, we re-implement the listed methods using their publicly shared code and test them on the same hardware.**

For the semantic segmentation methods:

**DeepLabV3+**[14] combines the advantages of the spatial pyramid pooling module and the encoder-decoder structure. It explores the Xception model and applies depthwise separable convolution to both the atrous spatial pyramid pooling and decoder modules, resulting in a faster and more robust encoder-decoder network.

**EncNet**[102] enhances semantic segmentation by utilizing a context encoding module that captures global contextual information to aid in the accurate segmentation of complex scenes.

[MISSING_PAGE_FAIL:31]

**FastFCN**[80] employs a novel joint upsampling module called Joint Pyramid Upsampling, which transforms the task of extracting high-resolution feature maps into a joint upsampling challenge.

**EMANet**[55] enhances semantic segmentation by utilizing an EM-based attention mechanism that iteratively refines feature representations for more accurate segmentation.

**DANet**[27] improves scene segmentation by integrating both spatial and channel-wise attention mechanisms to capture rich contextual relationships across features.

**HRNet**[71] maintains high-resolution representations through the network and progressively adds lower-resolution subnetworks to enhance the learning of spatial hierarchies, significantly improving semantic segmentation.

**OCRNet**[97] enhances semantic segmentation by leveraging object-contextual representations, which aggregates contextual information around each pixel to improve segmentation accuracy.

**DNLNet**[93] improves performance on tasks like image classification by disentangling the traditional non-local operations into two separate streams for capturing spatial and channel dependencies separately.

**PointRend**[50] introduces a novel rendering-style algorithm that selectively refines segmentation predictions at adaptively sampled points, enhancing detail accuracy in image segmentation tasks.

**BiSeNetV2**[95] utilizes an efficient architecture with inverted residuals and linear bottlenecks, enabling high-performance mobile vision applications with significantly reduced computational cost.

**ISANet**[98] improves semantic segmentation by using a novel interlaced sparse self-attention mechanism that efficiently captures long-range dependencies with fewer parameters and computational overhead.

**STDC**[25] addresses real-time semantic segmentation by proposing a novel and efficient structure that removes structural redundancy, reduces dimensions of feature maps gradually, and uses their aggregation for image representation.

**SegNeXt**[32] rethinks convolutional attention design for semantic segmentation by introducing an advanced network architecture, enhancing the model's ability to focus on relevant features for more accurate segmentation.

For the long-tail methods:

**VS**[49] proposes to leverage both multiplicative and additive logit adjustments to address label imbalance problems.

**LA**[61] advances the conventional softmax cross-entropy by ensuring Fisher consistency in minimizing the balanced error.

**LDAM**[10] improves the performance of tail classes by encouraging larger margins for tail classes.

**Focal Loss**[56] is a modified cross-entropy loss designed to address class imbalance by focusing more on hard-to-classify examples, reducing the relative loss for well-classified instances and thus boosting performance on imbalanced datasets.

**DisAlign**[104] introduces a unified framework for long-tail visual recognition by aligning feature distributions across different classes, using a novel distribution alignment technique that adjusts class-specific thresholds to mitigate the bias towards head classes and enhance recognition of tail classes.

**BLV**[77] addresses long-tailed semantic segmentation by dynamically adjusting the learning rates for the logits of different classes based on their frequency, effectively reducing the performance gap between head and tail classes.

Additional Experimental Results

### Per-tail-class Results

In Figure 6, we present the results for each tail class. We select \(7\) classes with the fewest training samples from the Cityscapes dataset as tail classes: truck, bicycle, bus, train, traffic light, rider, and motorcycle. Our method not only achieves the highest overall mIoU but also shows significant improvements in these tail classes. Specifically, it outperforms the current SOTA method by more than \(1\%\) in several tail classes.

### Performance Differences Across Different Datasets

The performance gain depends on the degree of imbalance of the underlying dataset. To see this, we show the pairwise mean imbalance ratio \(r_{m}\) (average the imbalance ratio of each class pair).

\[r_{m}=\frac{1}{|\mathcal{C}_{h}||\overline{\mathcal{C}_{h}}|}\sum_{a\in \mathcal{C}_{h}}\sum_{b\in\overline{\mathcal{C}_{h}}}\left(\frac{a}{b}\right)\] (43)

where \(\mathcal{C}_{h}\) represents the set containing the pixel counts of each head class, and \(\overline{\mathcal{C}_{h}}\) denotes the set containing the pixel counts of each non-head class. The larger the \(r_{m}\) value, the more imbalanced the dataset is.

In Table 11, we compare \(r_{m}\) for ADE20K, Cityscapes, and COCO-Stuff 164K, along with the tail classes performance improvements of AUCSeg compared to the runner-up method.

The results suggest that the larger the imbalance degree the larger the improvements of our method. ADE20K has the largest degree imbalance, therefore gaining the most significant improvement.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Dataset** & **ADE20K** & **Cityscapes** & **COCO-Stuff 164K** \\ \hline \(r_{m}\) & 90.43 & 80.39 & 38.17 \\ Tail Classes Improvement & 1.21\% & 0.75\% & 0.38\% \\ \hline \hline \end{tabular}
\end{table}
Table 11: The comparison of imbalance ratio and tail classes performance improvements on ADE20K, Cityscapes, and COCO-Stuff 164K.

Figure 6: Per-tail-class results on **Cityscapes** val set. The tail class names are listed from left to right according to the ascending number of training samples in the dataset, with ‘motorcycles’ containing the fewest.

### More Qualitative Results

Here we present more qualitative results on the Cityscapes, ADE20K, and COCO-Stuff 164K validation sets.

Figure 7: More qualitative results on the **Cityscapes**, **ADE20K** and **COCO-Stuff 164K** val set. Red rectangles highlight and magnify the details of the image.

### Backbone Extension of Different Model Sizes

In this paper, we use the large version of SegNeXt because of its outstanding performance. We also provide the results for the tiny, small, and base versions of SegNeXt, as shown in Table 12. The experiments indicate that AUCSeg achieves better performance under any model size.

### Backbone Extension of Different Pixel-level Long-tail Problems

Apart from semantic segmentation, salient object detection is also a pixel-level task. In salient object detection, the salient objects often exhibit a long-tailed distribution. We apply AUCSeg to this task, using the latest SOTA method SI-SOD-EDN [53, 81] as the backbone. The results are shown in Table 13.

Our AUCSeg achieves improvements across three commonly used evaluation metrics on three datasets, demonstrating that our method is highly versatile and extensible.

### Spatial Resource Consumption

In this section, we thoroughly explore the spatial resource consumption of the T-Memory Bank.

Table 14 details the ablation experiments on the spatial resource use of the T-Memory Bank. We set the memory size \(S_{M}\) to \(5\) and conduct experiments on the ADE20K dataset. It is no longer necessary for samples of all classes to appear in the mini-batch, but rather only a minimum of \(5\) tail class samples are needed. The results demonstrate that using AUC alone requires a batch size and graphics memory 5.5 times greater than the baseline to satisfy computational demands, which is a significant expense. However, the T-Memory Bank substantially reduces this cost, enabling more efficient training without an intolerant increase in graphics memory.

Moreover, we explore the effect of memory size \(S_{M}\) on spatial resource consumption. The findings in Figure 8 show that the graphics memory occupation increases slightly as \(S_{M}\) rises. However, a smaller memory size generally suffices for effective performance, indicating that AUCSeg can achieve significant improvements with a manageable graphics memory burden. As shown in Figure 4(a), when \(S_{M}=5\), significant performance improvements can be achieved with lower spatial resource consumption.

\begin{table}
\begin{tabular}{c|c c c|c c c|c c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{3}{c|}{ECSSD} & \multicolumn{3}{c|}{HKU-IS} & \multicolumn{3}{c}{PASCAL-S} \\  & MAE \(\downarrow\) & F\({}_{m}^{\beta}\uparrow\) & E\({}_{m}\uparrow\) & MAE \(\downarrow\) & F\({}_{m}^{\beta}\uparrow\) & E\({}_{m}\uparrow\) & MAE \(\downarrow\) & F\({}_{m}^{\beta}\uparrow\) & E\({}_{m}\uparrow\) \\ \hline SI-SOD-EDN & 0.0358 & 0.9084 & 0.9375 & 0.0287 & 0.8986 & 0.9442 & 0.0644 & 0.826 & 0.8859 \\ +AUCSeg & **0.0349** & **0.9087** & **0.9377** & **0.0278** & **0.8992** & **0.9455** & **0.0629** & **0.8281** & **0.8875** \\ \hline \hline \end{tabular}
\end{table}
Table 13: Experimental results of AUCSeg in the salient object detection.

\begin{table}
\begin{tabular}{c|c|c c} \hline \hline
**Backbone** & **AUCSeg** & **Overall** & **Tail** \\ \hline \multirow{2}{*}{Tiny} & \(\bigtimes\) & 38.73 & 33.96 \\  & \(\bigvee\) & **39.00** & **34.52** \\ \hline \multirow{2}{*}{Small} & \(\bigtimes\) & 43.25 & 38.90 \\  & \(\bigvee\) & **43.29** & **39.18** \\ \hline \multirow{2}{*}{Base} & \(\bigtimes\) & 45.45 & 41.33 \\  & \(\bigvee\) & **46.37** & **42.49** \\ \hline \multirow{2}{*}{Large} & \(\bigtimes\) & 47.45 & 43.28 \\  & \(\bigvee\) & **49.20** & **45.52** \\ \hline \hline \end{tabular}
\end{table}
Table 12: Results of AUCSeg on different model sizes of SegNeXt in terms of mIoU (%).

### Results of Different AUC Surrogate Losses and Calculation Methods

AUCSeg can adopt various surrogate losses. In the previous section, we use the square loss. In Table 15, we explore two other popular surrogate losses (hinge loss and exponential loss) for AUCSeg. Additionally, we include results for two AUC loss calculation methods (one-vs-one and one-vs-all) applied to AUCSeg using the square loss. The results are presented in Table 16.

The results indicate that AUCSeg shows improved performance with any of the surrogate functions. Among them, using square loss and the ovo calculation method delivers the best overall performance.

### Results of the Comparison Between PMB and TMB

There are two differences between the Pixel-level Memory Bank (PMB) and our Tail-class Memory Bank (TMB). **First**, the PMB stores pixels from all classes, whereas TMB only stores pixels from tail classes. **Second**, in TMB, the storing and retrieving processes are conducted on an entire object (we ensure that the pasted pixel forms a meaningful object). However, the PMB typically focuses on a fixed number of pixels without structural information (regardless of whether these pixels can form a complete image).

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Dataset** & **AUC Calculation Method** & **Overall** & **Tail** \\ \hline \multirow{3}{*}{ADE20K} & ova & 48.46 & 44.58 \\  & ovo & **49.2** & **45.52** \\ \hline \multirow{3}{*}{Cityscapes} & ova & 82.31 & 80.79 \\  & ovo & **82.71** & **81.67** \\ \hline \multirow{3}{*}{COCO-Stuff 164K} & ova & 42.25 & 40.17 \\  & ovo & **42.73** & **40.72** \\ \hline \hline \end{tabular}
\end{table}
Table 16: Results of different AUC calculation methods in terms of mIoU (%).

\begin{table}
\begin{tabular}{c|c|c c} \hline \hline
**AUC** & **TMB** & **Batch Size** & **Graphic Memory** \\ \hline \(\bigtimes\) & \(\bigtimes\) & 4 & 13.29G \\ \(\bigvee\) & \(\bigtimes\) & 22 & 72.90G \\ \(\bigvee\) & \(\bigvee\) & 4 & 15.45G \\ \hline \hline \end{tabular} 
\begin{tabular}{c|c|c c} \hline \hline
**AUC** & **TMB** & **Batch Size** & **Graphic Memory** \\ \hline \(\bigtimes\) & \(\bigtimes\) & 4 & 13.29G \\ \(\bigvee\) & \(\bigtimes\) & 22 & 72.90G \\ \(\bigvee\) & \(\bigvee\) & 4 & 15.45G \\ \hline \hline \end{tabular} 
\begin{tabular}{c|c c} \hline \hline
**AUC** & **TMB** & **Batch Size** & **Graphic Memory** \\ \hline \(\bigtimes\) & \(\bigtimes\) & 4 & 13.29G \\ \(\bigvee\) & \(\bigtimes\) & 22 & 72.90G \\ \(\bigvee\) & \(\bigvee\) & 4 & 15.45G \\ \hline \hline \end{tabular}
\end{table}
Table 14: Space resource consumption required for training properly. TMB is the abbreviation of T-Memory Bank.

Figure 8: The effect of memory size on spatial resource consumption.

### Why do we only store tail class pixels instead of all pixels?

Table 17 shows the average number of pixels from head and tail classes per image in the ADE20K, Cityscapes, and COCO-Stuff 164K datasets.

It can be observed that the number of head class pixels in each image is \(2.46\) to \(9.45\) times greater than that of the tail classes, meaning that storing head class pixels would require significantly more memory.

Table 18 compares the performance differences between storing all and tail class pixels. It shows that the PMB, which incurs additional memory costs, performs almost the same as the TMB, and even shows a noticeable decline in the Cityscapes dataset. This is because head classes appear in almost every image (for example, in urban road datasets, it is hard to find an image without head class pixels like 'road' or'sky'), so they do not need additional supplementation. Even if some images require supplementation of head classes, their larger pixel counts might cause them to overwrite the original tail class pixels when pasted, leading to a decline in performance. Thus, we only store tail class pixels.

### Why it is not feasible to focus on a fixed number of pixels?

We conduct tests on the ADE20K dataset by supplementing a fixed number of tail class pixels (\(1000\)/\(20000\)/\(30000\)/\(40000\)) in each image and find that compared to AUCSeg, the performance differences are \(-3.00\%\)/\(-1.93\%\)/\(-0.86\%\)/\(-0.73\%\). This is because supplementing a fixed number of pixels can result in incomplete images, such as only adding the front wheel of a bicycle, therefore loss of the structural information. The model is then unable to learn complete and accurate features. Therefore, in TMB, storing and retrieving are conducted on all pixels of an entire image.

### Results of Different Memory Bank Update Strategies

In the previous section, we use the random replacement strategy to update the T-Memory Bank. We experiment with three other selection methods on the ADE20K dataset:

* **First-In-First-Out (FIFO) replacement**: Prioritizes replacing the images that were first stored in the Tail-class Memory Bank.
* **Last-In-First-Out (LIFO) replacement**: Prioritizes replacing the images that were last stored in the Tail-class Memory Bank.
* **Priority Used (PU) replacement**: Prioritizes replacing images that have previously been selected by the retrieval branch.

The results are shown in table 19.

FIFO and PU both show better performance overall and on tail classes compared to random sampling. However, LIFO, by updating only the most recently added images in the T-Memory Bank, causes the earlier images to remain unchanged. This leads to overfitting and, consequently, a decline in performance.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Dataset** & **ADE20K** & **Cityscapes** & **COCO-Stuff 164K** \\ \hline Head & 46685 & 294290 & 60157 \\ Tail & 18977 & 31128 & 22526 \\ \hline \hline \end{tabular}
\end{table}
Table 17: The average number of pixels from head and tail classes per image.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Dataset** & **ADE20K** & **Cityscapes** & **COCO-Stuff 164K** \\ \hline PMB & 49.09 & 82.07 & 42.66 \\ TMB & 49.2 & 82.71 & 42.73 \\ \hline \hline \end{tabular}
\end{table}
Table 18: The performance differences between PMB and TMB in terms of mIoU (%).

[MISSING_PAGE_EMPTY:38]

## Appendix H More Discussions About AUCSeg

**Training and inference efficiency.** During training, since AUCSeg (\(1.62\pm 0.2s\) per iteration) adopts pairwise loss, it will inevitably suffer from extra complexity compared to the standard CE-based SegNeXt (\(0.76\pm 0.15s\) per iteration). Besides, during inference, since AUCSeg does not modify the model's backbone, all algorithms with the same backbone achieve similar inference efficiency (\(60\pm 5ms\) per image). Overall, AUCSeg could perform well with acceptable efficiency.

**Performance trade-off between head and tail classes.** We recognize that AUCSeg might slightly impair the performance of head classes due to an increased focus on tail classes. However, this often results in substantial improvements for tail classes, a trade-off that is generally beneficial since tail classes are typically more critical. For instance, on the Cityscapes dataset, AUCSeg exhibits a marginal decrease of \(0.17\%\) in head classes but gains \(0.75\%\) in tail classes compared to the runner-up, SegNeXt. Furthermore, we note that performance decreases in head classes mainly arise from misclassification at the blurry edges of objects. In contrast, gains in tail classes often stem from either the successful detection of smaller objects or the more complete detection of such objects. To summarize, on the Cityscapes datasets, detecting a new tail object like 'Traffic Lights' is far more significant than precisely detecting the edge pixels of a head class like'sky'. Thus, we consider this trade-off highly beneficial.

## Appendix I Broader Impact

We propose a general semantic segmentation method to deal with the potential bias toward long-tail objects. For fairness-sensitive scenarios, it might be helpful to improve fairness for long-tail groups.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Batch Size** & **Overall** & **Tail** \\ \hline
1 & 34.73 & 29.66 \\ +AUCSeg & **40.14** & **35.74** \\ \hline
2 & 45.5 & 41.34 \\ +AUCSeg & **46.86** & **42.93** \\ \hline
4 & 47.45 & 43.28 \\ +AUCSeg & **49.2** & **45.52** \\ \hline
8 & 49.35 & 45.46 \\ +AUCSeg & **49.36** & **45.53** \\ \hline
16 & 50.07 & 46.32 \\ +AUCSeg & **50.96** & **47.03** \\ \hline \hline \end{tabular}
\end{table}
Table 24: Results of the ablation study on the impact of batch size in terms of mIoU (%).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We briefly summarize it in the abstract and detail the paper's contributions and scope in the introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss this issue in Appendix H. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide complete proofs for each theoretical result in Appendix B and Appendix C. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe our algorithm in Section 4 and fully disclose all the information needed to reproduce the main experimental results of the paper in Section 5.1 and Appendix F. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide a link to the data and code in the abstract. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ( https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ( https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide the completed experimental setting in Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report this issue in Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide sufficient information on the computer resources in Appendix F.2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms in every respect with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss this issue in Appendix I. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not release data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the original paper that produced the code package and dataset. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide the code with documentation in the link within the abstract. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing and research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing and research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.