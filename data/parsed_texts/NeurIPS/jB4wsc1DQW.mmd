# Hierarchical Adaptive Value Estimation for Multi-modal Visual Reinforcement Learning

 Yangru Huang\({}^{1}\), Peixi Peng\({}^{2,3}\)\({}^{*}\), Yifan Zhao\({}^{1}\), Haoran Xu\({}^{3,4}\),

**Mengyue Geng\({}^{1}\), Yonghong Tian\({}^{1,2,3}\)\({}^{*}\)**

\({}^{1}\)School of Computer Science, Peking University

\({}^{2}\)School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University

\({}^{3}\)Peng Cheng Laboratory

\({}^{4}\)School of Intelligent Systems Engineering, Sun Yat-sen University

yrhuang@stu.pku.edu.cn,

{pxpeng, zhaoyf, mygeng, yhtian}@pku.edu.cn,xuhr9@mail2.sysu.edu.cn

###### Abstract

Integrating RGB frames with alternative modality inputs is gaining increasing traction in many vision-based reinforcement learning (RL) applications. Existing multi-modal vision-based RL methods usually follow a Global Value Estimation (GVE) pipeline, which uses a fused modality feature to obtain a unified global environmental description. However, such a _feature-level_ fusion paradigm with a single critic may fall short in policy learning as it tends to overlook the distinct values of each modality. To remedy this, this paper proposes a Local modality-customized Value Estimation (LVE) paradigm, which dynamically estimates the contribution and adjusts the importance weight of each modality from a _value-level_ perspective. Furthermore, a task-contextual re-fusion process is developed to achieve a _task-level_ re-balance of estimations from both feature and value levels. To this end, a **Hierarchical Adaptive Value Estimation (HAVE)** framework is formed, which adaptively coordinates the contributions of individual modalities as well as their collective efficacy. Agents trained by HAVE are able to exploit the unique characteristics of various modalities while capturing their intricate interactions, achieving substantially improved performance. We specifically highlight the potency of our approach within the challenging landscape of autonomous driving, utilizing the CARLA benchmark with neuromorphic event and depth data to demonstrate HAVE's capability and the effectiveness of its distinct components. The code of our paper can be found at https://github.com/Yara-HYR/HAVE.

## 1 Introduction

Recent years have witnessed a renewed interest in multi-modal perception in the computer vision research community [10; 36; 20; 26; 37]. For many visual tasks such as semantic segmentation and object detection, the inclusion of multi-modal data (_e.g._, depth, infrared) is proven to be indisputably beneficial [41; 29; 49; 25]. This trend equally applies to the intelligent agents in vision-based Reinforcement Learning (RL), in which multi-modal inputs can also promote decision robustness [39; 19; 1; 11; 31]. For instance, an autonomous-driving agent taking RGB frames solely as input may frequently suffer from extreme light conditions, as shown in Fig. 1(a). Nevertheless, combining additional sensory inputs such as event signals coming from a neuromorphic event camera [27] can effectively alleviate these problems, enabling a more comprehensive realization of traffic status [5; 32; 12].

Despite the abundance of fruitful studies in the field of multi-modal visual perception, a majority of them focus on traditional static learning tasks. In contrast, the dynamic and unlabeled nature of RL renders the development of multi-modal agents exceedingly difficult [6]. To reconcile multi-sensory data, existing methods [18, 36, 6, 22, 40] train the agents with fused modality features and form a mixed global modality, as shown in Fig.1(c). The policy is then learned via Global Value Estimation (GVE), in which the global modality feature is responsible for environment description and then paired with action to compute a single Q-value. Although being an effective and widely adopted paradigm, utilizing GVE alone may overlook the distinct task-related contributions of different modalities. Specifically, due to the diverse attributes of sensors, single modalities may not contribute equally under multiple environmental conditions (_e.g._, the cases given in Fig. 1 (a) and (b)). The _feature-level_ fusion process in GVE, no matter how sophisticated, inevitably disregards the unique value of each sensory data since only a single critic is shared over multiple modalities. As a result, negative interference between modalities may occur and compromise learning performance.

Based on the above analysis, we propose _Local modality-customized Value Estimation (LVE)_, as illustrated in Fig. 1 (d). Different from feature-level fusion, LVE learns a single policy with distinct, per-modality value calculation, forming a _value-level_ fusion paradigm through a tailor-designed Q-value weighting process. As a result, the modality-specific contributions can be explicitly estimated, which promotes policy flexibility. In addition, while LVE is intrinsically a better alternative than GVE, the two paradigms are not competitive but complementary. Considering this, we further develop a _task-contextual re-fusion_ process, which utilizes an efficient fusing network guided directly by the task reward to reach a _task-level_ balance between LVE and GVE. The collaboration of the above components eventually forms a **Hierarchical Adaptive Value Estimation (HAVE)** framework for multi-modal vision-based RL, yielding a potent policy that can flexibly leverage the unique strengths of each modality while profiting from the comprehensive information from all modalities. We specifically highlight the potency of our approach to the challenging autonomous driving task. In particular, we explicitly consider the neuromorphic event camera [27] signals that capture the motion information of the environment, which well-suites our task but has not yet been explored by existing multi-modal RL algorithms.

In summary, the contributions of our work are three-fold: 1) We design a novel hierarchical adaptive value estimation (HAVE) framework for multi-modal vision-based RL. HAVE distinctively features a local modality-customized value estimation (LVE) paradigm to enable optimized reward allocation based on modality importance. 2) We develop a task-contextual re-fusion process to merge the profi

Figure 1: Top: Desired modality contributions of (a) under-exposed RGB frame vs. high dynamic range (HDR) event signals, and (b) clear RGB frame vs. event signals saturated by background noise. Bottom: Multi-modal fusion at (c) the feature level, where only the fused modality feature is used for value evaluation, and (d) the value level, where individual value estimation is conducted for each modality to identify which one performs better under the current circumstance.

ciencies of LVE and GVE, allowing HAVE to benefit from both particularized and unified modality values. 3) Our approach achieves state-of-the-art performance on challenging autonomous driving tasks. As the first multi-modal RL algorithm considering event camera signals as one of the input modalities, our approach exhibits superior performance under various environmental conditions, demonstrating its potential as an effective multi-modal vision-based RL solution.

## 2 Related Work

**Vision-based RL** aims to train agents that receive raw image-based observations from environments for decision-making. Compared to state-based RL, vision-based RL has found significant use in many practical tasks, from video game playing [34] to robotic manipulation [21]. However, learning policy directly from such high-dimensional input is challenging. To tackle the problem, a considerable amount of works have been developed, including 1) applying data augmentation to increase the diversity of samples [50; 24; 30], 2) introducing auxiliary tasks such as contrastive loss [23; 54; 2], 3) pretraining an encoder to improve the representational ability [51; 28; 42], and 4) modeling environment dynamics in the latent space [17; 16; 38]. Although most vision-based RL methods adopt RGB camera frames as inputs, some recent works have also started to explore new sensors and data formats for RL, such as event cameras [46; 47] which captures fast and asynchronous light changes with high temporal precision.

**Multi-modal Visual Learning** has been extensively studied in the field of computer vision [3]. With the development of sensor technologies, it becomes effortless to acquire sufficient data from complemented visual modalities (_e.g._, RGB, infrared, depth, and event signals). As a result, many multi-modal learning methods are proposed for traditional visual tasks, such as object detection [29] and segmentation [10; 25]. For multi-modal RL, the agent's observation space is modified to include all modalities. Recent works have started to focus on multi-modal vision-based RL due to its improved effectiveness compared with using only single-modality data. Chen _et al._propose a multi-modal state-space model trained with mutual information lower-bound to promote the consistency between the latent codes of each modality [6]. A fusion network is proposed by Khalil _et al._to produce accurate joint multi-modal perception and motion prediction for autonomous driving [22]. Ma _et al._propose a multi-modal RL approach that focuses on modality alignment and importance enhancement [31]. There are also multi-modal RL methods designed for other tasks such as robot control [4] and dialog system [33; 53]. Despite their distinct technical details, most of the existing methods perform a feature-level fusion of modalities and ignore the properties of RL itself. In fact, policy learning depends on the value estimation of the critic. Consequently, we view the multi-modal visual RL problem from a novel adaptive value estimation perspective. Instead of concentrating solely on robust global modality features, our approach synergistically balances individual modalities' contributions, leading to a more equitable and efficient value allocation. Leveraging the task-contextual re-fusion mechanism, our method further capitalizes on both feature-based and value-based fusion paradigms, resulting in a more robust policy.

## 3 Methodology

### Preliminaries

**Problem Definition** We formulate the task of multi-modal vision-based RL as a Markov Decision Process (MDP) [44] with multiple observations, which is defined by a tuple \((\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma)\), where \(\mathcal{S}=\prod_{i=1}^{d}\mathcal{O}^{M_{i}}\) is the joint observation space of \(d\) modalities and \(\mathcal{O}^{M_{i}}\) is the observation space of modality \(i\). \(\mathcal{A}\) is all possible actions the agent can take. \(\mathcal{P}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]\) is the transition probability function, \(\mathcal{P}(o_{t+1}|o_{t},a_{t})\) denotes the probability of transitioning from joint observation \(o_{t}=(o_{t}^{M_{1}},o_{t}^{M_{2}},\ldots,o_{t}^{M_{d}})\) at time step \(t\) to the next joint observation \(o_{t+1}=(o_{t+1}^{M_{1}},o_{t+1}^{M_{2}},\ldots,o_{t+1}^{M_{d}})\) after taking action \(a_{t}\) from a policy function \(\pi\).

**Soft Actor-Critic (SAC)** Our approach is built on SAC [14; 15], which goal is to learn a policy that maximizes the expected cumulative reward while maintaining exploration by encouraging diverse actions. In SAC, the objective function is given by introducing a policy entropy term:

\[\mathcal{L}_{\pi}=\mathbb{E}_{a_{t}\sim\pi}\left[Q(o_{t},a_{t})-\alpha\log\pi (a_{t}|o_{t})\right],\] (1)where \(Q\) is the value function, and \(\alpha\) is a temperature parameter that controls the trade-off between exploration and exploitation. The value function is trained using the Bellman equation and a soft Q-function update:

\[\mathcal{L}_{Q}=\mathbb{E}_{(o_{t},a_{t})\sim\mathcal{D}}\left[\left(Q(o_{t},a_{ t})-\left(\mathcal{R}(o_{t},a_{t})+\gamma\mathbb{E}_{o_{t+1}\sim p}\left[V(o_{t+1}) \right]\right)\right)^{2}\right],\] (2)

where \(o_{t}\) and \(a_{t}\) are sampled from the replay buffer \(\mathcal{D}\) and \(\gamma\) is the discount factor. \(V(o_{t+1})\) is the soft state value function, defined as:

\[V(o_{t+1})=\mathbb{E}_{\tilde{a}_{t+1}\sim\pi}\left[\bar{Q}(o_{t+1},\tilde{a}_ {t+1})-\alpha\log\pi(\tilde{a}_{t+1}|o_{t+1})\right],\] (3)

where \(\bar{Q}\) denotes an exponential moving average of the critic network \(Q\) and \(\tilde{a}_{t+1}\) comes from the current policy.

**Event Cameras and Representations** Despite RGB frames, another major modality that we use to evaluate our approach is the event signals generated by a neuromorphic event camera [27]. Each pixel in the event camera outputs a positive/negative event signal whenever the log light intensity of that pixel has increased/decreased by a constant threshold. The event signals have an extremely high dynamic range (up to 120 dB) and can reach a high temporal resolution in the order of \(\mu\)s. Therefore, they are able to capture the missing motion clues that are missed in the RGB frames for many visual tasks such as autonomous driving and robot navigation. To utilize event signals, we use the stacking based on time (SBT) [48] representation that splits the event sequence into fixed temporal bins, forming an event frame representation with multiple channels similar to RGB frames.

### Global Value Estimation

In Global Value Estimation (GVE), a total of \(d\) observation encoders are used to extract observation features \(f_{t}^{M_{i}}\) for each modality observation \(o_{t}^{M_{i}}\) at time step \(t\). A global modality feature \(f_{t}^{g}\) is then calculated by fusing all modality features. For simplicity, we directly concatenate \(f_{t}^{M_{i}}(i=1,2,\dots,d)\) along the channel dimension. Given \(f_{t}^{g}\), GVE first computes the value estimation \(q_{t}^{g}=Q^{\theta_{g}}(f_{t}^{g},a_{t})\) using a single global critic network \(Q^{\theta_{g}}\), as shown in Fig. 2(a). Following the SAC algorithm, the target value \(y_{t}^{g}\) is then calculated as:

\[y_{t}^{g}=\mathcal{R}(o_{t},a_{t})+\gamma V(f_{t+1}^{g}),\] (4)

where \(\mathcal{R}(o_{t},a_{t})\) is the reward returned by the environment. The soft state value function \(V(f_{t+1}^{g})\) is denoted as:

\[V(f_{t+1}^{g})=\mathbb{E}_{\tilde{a}_{t+1}\sim\pi}\left[\bar{Q}^{\theta_{g}}( f_{t+1}^{g},\tilde{a}_{t+1})-\alpha\log\pi(\tilde{a}_{t+1}|f_{t+1}^{g}) \right].\] (5)

However, since using a single value function \(Q^{\theta_{g}}\) shared over all modalities, it is hard to dynamically balance the relative contribution of each \(f_{t}^{M_{i}}\) under complex environmental conditions.

Figure 2: HAVE framework and its distinct components. (a) Global Value Estimation (GVE), (b) Local modality-customized Value Estimation (LVE), and (c) HAVE with task-level value re-fusion.

### Local Modality Customized Value Estimation

The objective of Local modality-customized Value Estimation (LVE) is to effectively differentiate and quantify the unique contribution of each modality, thereby facilitating a more granular and modality-aware decision-making process. Specifically, in LVE, \(d\) individual value functions \(Q^{\theta_{M_{i}}},Q^{\theta_{M_{2}}},...,Q^{\theta_{M_{d}}}\) are setup instead of global-only functions in GVE. These value functions share the same network architecture but are learned separately. To facilitate value decomposition, an assignment network is designed to assimilate all estimated values and re-calibrate them according to the collective environmental condition portrayed by modality features.

**Value Inference:** We assume that the locally-aggregated total value \(q_{t}^{l}\) can be approximately decomposed into a linear combination of the separate value functions across different modalities under a shared policy:

\[q_{t}^{l}\approx\sum_{i=1}^{d}w_{t}^{M_{i}}Q^{\theta_{M_{i}}}(f_{t}^{M_{i}},a_ {t}),\] (6)

where \(w^{M_{i}}\) denotes the contribution weight of modality \(M_{i}\) to the total value. The value function \(Q^{\theta_{M_{i}}}\) for each modality processes the current individual modal observation feature \(f_{t}^{M_{i}}\) along with the current action \(a_{t}\), generating the estimated action values at each time step. Each \(Q^{\theta_{M_{i}}}\) is learned by backpropagating gradients from the Q-learning rule. Similarly, the total target value \(y_{t}^{l}\) can be defined as:

\[y_{t}^{l}\approx\mathcal{R}(o_{t},a_{t})+\gamma V(f_{t+1}^{M_{1}},...,f_{t+1} ^{M_{d}}).\] (7)

The soft state value function \(V(f_{t+1}^{M_{1}},...,f_{t+1}^{M_{d}})\) is then defined as:

\[V(f_{t+1}^{M_{1}},...,f_{t+1}^{M_{d}})=\mathbb{E}_{\tilde{a}_{t+1}\sim\pi} \left[\sum_{i=1}^{d}v_{t+1}^{M_{i}}\tilde{Q}^{\theta_{M_{i}}}(f_{t+1}^{M_{i}}, \tilde{a}_{t+1})-\alpha\log\pi(\tilde{a}_{t+1}|f_{t+1}^{g})\right],\] (8)

where \(v_{t}^{M_{i}}\) is the contribution weight of modality \(M_{i}\) to the the estimated Q-value from \(\tilde{Q}^{\theta_{M_{i}}}\) at the next time. \(\tilde{a}_{t+1}\sim\pi(\cdot|f_{t+1}^{g})\) comes from the current policy according to the global modality feature \(f_{t+1}^{g}\) since there is only one policy network for the multi-modal decision task.

**Contribution Assignment:** Given value decomposition in Eq. 6 and Eq. 7, a critical next step is to calculate accurate modality weights \(w_{t}^{M_{i}}\) and \(v_{t}^{M_{i}}\) to ensure precise total value estimation. To achieve this, the contribution assignment process involves modality interactions through an assignment network based on the attention mechanism. Taking the calculation of \(w_{t}^{M_{i}}\) as an example, the global modality feature \(f_{t}^{g}\) is used as a bridge to ensure information flow between all individual modalities. As demonstrated in Fig. 2(b), two fully connected (FC) layers with parameters \(W_{q}\) and \(W_{k}\) are used to project both \(f_{t}^{M_{i}}\) and \(f_{t}^{g}\) into a \(p\)-dimensional common latent subspace. Then \(w_{t}^{M_{i}}\) can be obtained through a softmax function as:

\[w_{t}^{M_{i}}=\frac{\text{exp}((W_{q}f_{t}^{g})(W_{k}f_{t}^{M_{i}})^{\top}/ \sqrt{p})}{\sum_{i=1}^{d}\text{exp}((W_{q}f_{t}^{g})(W_{k}f_{t}^{M_{i}})^{\top} /\sqrt{p})},\] (9)

where \(\sqrt{p}\) is used for scaling to prevent vanishing gradients [45]. The computation of \(v_{t+1}^{M_{i}}\) follows a similar derivation, which is omitted here for brevity.

By assigning the value function in a modality-customized manner, LVE adeptly manages multi-modal sensory inputs, enhancing model efficiency and interpretability. Our approach readily adapts to complex environmental scenarios, accommodating varying modality importance levels.

### Task-Contextual Re-fusion

**Feature-based _vs._ Value-based Fusion** From the details of GVE and LVE, we see that the main difference distinguishing them is their fusion principle (feature-based vs. value-based). Intuitively, the two paradigms are not competitive but complementary. Specifically, in the context of RL, value-based fusion offers significant advantages over feature-based fusion since the former is more related to the actual decision. In the meantime, the feature-based fusion method can use its collective modality value to offer a value-based one with a sturdy global reference, mitigating inaccuracies caused by input noise. Therefore, we further develop task-contextual re-fusion for a subsequent re-fuse of GVE with LVE in the reward/task level. The idea is to directly bridge the two fusion mechanismsbased on environmental reward feedback, allowing the task to determine which paradigm better suits the current situation.

**Details of the Re-fusion Process** Given the estimated value \(q_{t}^{g}\) and \(q_{t}^{l}\) obtained from GVE and LVE, respectively, we adopt a dynamic fusion mechanism with a re-fusion network \(\mathcal{H}\) to fuse \(q_{t}^{g}\) and \(q_{t}^{l}\). Specifically, \(\mathcal{H}\) consists of a series of FC layers and hypernetworks. Each hypernetwork consists of a single linear layer, which receives the global modality feature \(f_{t}^{g}\) as input and produces a weight matrix or a bias vector for a single FC layer in \(\mathcal{H}\). To fuse \(q_{t}^{g}\) and \(q_{t}^{l}\), \(f_{t}^{g}\) is first sent into all hypernetworks to obtain the parameters of the FC layers in \(\mathcal{H}\), then \(q_{t}^{g}\) and \(q_{t}^{l}\) are sent into \(\mathcal{H}\) to get the final total value \(\mathcal{H}(q_{t}^{g},q_{t}^{l})\). The final target value can be obtained using a similar way, wherein the estimated Q-value from the target Q-network at the next time is processed by another mixing network \(\mathcal{H}^{\prime}\). For brevity, we denote the this target value as \(\mathcal{H}^{\prime}(y_{t}^{g},y_{t}^{l})\).

### Learning Framework

Coping GVE with LVE and task-contextual re-fusion, our approach forms a unified framework, which we name as **Hierarchical Adaptive Value Estimation (HAVE)** for multi-modal vision-based RL. In this section, we elaborate on the training details of our HAVE framework.

**Policy Evaluation with HAVE** For the training of policy evaluation, we minimize the temporal difference (TD) error between the predicted value \(\mathcal{H}(q_{t}^{g},q_{t}^{l})\), and the target value \(\mathcal{H}^{\prime}(y_{t}^{g},y_{t}^{l})\) following the SAC algorithm:

\[\mathcal{L}_{Q}=\mathbb{E}_{(o_{t},a_{t})\sim\mathcal{D}}\left[\left(\mathcal{ H}(q_{t}^{g},q_{t}^{l})-\mathcal{H}^{\prime}(y_{t}^{g},y_{t}^{l})\right)^{2} \right],\] (10)

where \(\mathcal{D}\) denotes the replay buffer.

**Policy Improvement** The policy evaluation described above can cover reasonable Q-values to help find better policies. In the policy improvement step, we leverage the values obtained from GVE, LVE, and the task-contextual re-fusion process to update the policy \(\pi\). To be specific, the goal is to maximize the expected cumulative reward by selecting actions that have the highest combined value estimates:

\[\mathcal{L}_{\pi}=\mathbb{E}_{a_{t}\sim\pi}\left[\mathcal{H}(q_{t}^{g},q_{t}^ {l})-\alpha\log\pi(a_{t}|f_{t}^{g})\right].\] (11)

After identifying the optimal action under the improved policy, we can update the policy parameters by backpropagating gradients.

**Auxiliary Losses** Besides policy evaluation and improvement, we also learn to predict the rewards and next latent states as in DeepMDP [13] to assist representation learning, providing a latent state space that is consistent with environmental dynamics:

\[\mathcal{L}_{aux}=\|\mathcal{P}^{g}(f_{t}^{g},a_{t})-f_{t+1}^{g}\|^{2}+\sum_{ i=1}^{d}\|\mathcal{P}^{M_{i}}(f_{t}^{M_{i}},a_{t})-f_{t+1}^{M_{i}}\|^{2}+\left[ \mathcal{R}^{g}(f_{t}^{g},a_{t})-\mathcal{R}(o_{t+1},a_{t+1})\right]^{2},\] (12)

where \(\mathcal{P}^{g}\) and \(\mathcal{P}^{M_{i}}(i=1,2,\ldots,d)\) are transition networks for the global and individual modalities, respectively. \(\mathcal{R}^{g}\) is the reward prediction network. All these networks are formed by FC layers and a more detailed architecture description can be found in the supplementary material.

**Overall Training Objective** Given the above training objectives, the overall loss of HAVE is finally defined by:

\[\mathcal{L}=\mathcal{L}_{Q}+\mathcal{L}_{\pi}+\mathcal{L}_{aux}.\] (13)

By iteratively performing policy evaluation and improvement steps, HAVE can effectively exploit the strengths of both global and distinct value estimates, ultimately converging to an improved policy that can handle the challenges posed by a broad range of environmental modalities.

### Further Analyses

Our hierarchical adaptive value estimation framework for multi-modal RL process offers several key benefits, which we list as follows:

**Remark 1** (Prevention of Modality Dominance): _The mechanism of LVE and task-contextual re-fusion prevents one modality from dominating the others, thereby avoiding the issue of modality collapse._To realize this, consider that the mixing network \(\mathcal{H}\) in the task-contextual re-fusion performs fixed non-linear transformations on both \(q_{t}^{g}\) and \(q_{t}^{l}\) given \(f_{t}^{g}\), where \(q_{t}^{l}\) is formed by the convex combination of individual modality values (Eq. 6) as in LVE. Therefore, we have:

\[\text{sign}(\frac{\partial\mathcal{H}(q_{t}^{g},q_{t}^{l})}{\partial q_{t}^{M_ {i}}})=\text{sign}(\frac{\partial\mathcal{H}(q_{t}^{g},q_{t}^{l})}{\partial q_ {t}^{l}}\frac{\partial q_{t}^{l}}{\partial q_{t}^{M_{i}}})=\text{sign}(\frac{ \partial\mathcal{H}(q_{t}^{g},q_{t}^{l})}{\partial q_{t}^{l}}),\forall i\in[1,2,\ldots,d]\,,\] (14)

where \(\text{sign}(\cdot)\) is the real sign function, \(q_{t}^{M_{i}}=Q^{\theta_{M_{i}}}(f_{t}^{M_{i}},a_{t})\) is the individual modality value and \(\frac{\partial q_{t}^{M_{i}}}{\partial q_{t}^{M_{i}}}=w_{t}^{M_{i}}>0\) are the modality weights. Eq. 14 indicates that the contributions of all modalities are either increasing or decreasing together during the learning process, which prevents opposite gradient values between modalities that enhance some of them while suppressing others. In addition, the calculation of modality weights \(w_{t}^{M_{i}}\) based on softmax global-individual feature similarity (Eq. 9) further prevents modality collapse caused by persistent zero weights.

**Remark 2** (Pareto Optimality of Modalities): _Assuming continuous action space, the global optimal action produced by our approach achieves a Pareto optimum across individual modalities._

Suppose the optimal action given some fixed modality features is \(a^{*}\), which implies \(\frac{\partial\mathcal{H}(q_{t}^{g},q_{t}^{l})}{\partial a^{*}}=0\). Combining Eq. 6 with the chain rule, we have:

\[\frac{\partial\mathcal{H}(q_{t}^{g},q_{t}^{l})}{\partial a^{*}}=\frac{ \partial\mathcal{H}(q_{t}^{g},q_{t}^{l})}{\partial q_{t}^{l}}\frac{\partial q _{t}^{l}}{\partial a^{*}}=\frac{\partial\mathcal{H}(q_{t}^{g},q_{t}^{l})}{ \partial q_{t}^{l}}\sum_{i=1}^{d}w_{t}^{M_{i}}\frac{\partial q_{t}^{M_{i}}}{ \partial a^{*}}=0.\] (15)

Considering the non-linear structure of \(\mathcal{H}\), \(\frac{\partial\mathcal{H}(q_{t}^{g},q_{t}^{l})}{\partial q_{t}^{l}}\) is unlikely to be zero for all \(q_{t}^{l}\). This implies that \(\sum_{i=1}^{d}w_{t}^{M_{i}}\frac{\partial q_{t}^{M_{i}}}{\partial a^{*}}=0\) for all \(i=1,2,\ldots,d\), which suggests that at the point \(a^{*}\), the positive weighted sum of the gradients of individual modalities' values is zero. This implies that there is no alternative action assignment in the vicinity of \(a^{*}\) that can increase the value for any individual modality without decreasing the value for some other modality. Such condition forms a Pareto optimum across the modalities, which indicates the resources (modality values) are allocated in the most efficient way possible [35].

## 4 Experiments

### Settings

**Environments** To evaluate our approach under realistic and challenging multi-modal environments, we employ the CARLA simulator [8], which is a widely used open-source platform for autonomous driving research. CARLA provides a rich and realistic urban environment to evaluate autonomous driving agents in various traffic scenarios. Three distinct modalities are adopted: 1) RGB frames, 2) event signals generated by CARLA's synthetic event-camera simulator, and 3) per-pixel depth frames. Eight different weather settings are used to provide thorough coverage of different environmental conditions. The action space consists of continuous control actions, such as steering, acceleration, and braking. Similar to [52; 9], the reward function is designed to encourage the agent to maintain a safe distance from 20 other moving vehicles and obstacles, and drive as far as possible along the highway of CARLA's Town04 map in 1000 time steps. We use the single camera view setting on the vehicle's roof with 60-degree views.

**Implementation Details** Our approach is implemented based on SAC [14; 15] and DeepMDP [13]. The same encoder network architecture and training hyperparameters are adopted for all comparative methods. Following common practices [50; 23], we convert each modality data into its corresponding image-based representations and stack several consecutive images to infer temporal information. The spatial resolution of the input images is \(128\times 128\) and the channel numbers of RGB, event and depth frames are 3, 5 and 1, respectively. All methods are trained for 120k frames across 5 random seeds to report the mean and standard deviation of the rewards. We evaluate the performance of each approach in terms of driving episode reward and distance. Other details are provided in the supplementary material.

### Comparison with State of the Art

We compare our method with a variety of methods, including the SAC [15] baseline, DeepMDP [13], DrQ [50], TransFuser [7], and EFNet [43]. For RL methods SAC, DrQ and DeepMDP, we directly concatenate the features of the different modalities as the input for subsequent value and policy learning. For TransFuser and EFNet which are designed for traditional visual tasks, we only adopt their advanced modality fusion modules and keep DeepMDP as the RL algorithm for a fair comparison.

**Results of Two Modalities** We first evaluate different methods under two modality inputs (RGB frames and event signals) in Table 1. The results show that our method achieves the highest episode reward and driving distance under all eight weather conditions. DrQ and DeepMDP perform better than the SAC baseline with limited improvement, showing that the feature-fusion based GVE paradigm, together with simple feature concatenation, cannot fully extract the expressive power of each modality. The improved performance of TransFuser and EFNet over DeepMDP reflects the importance of the advanced modality feature fusion mechanism. However, they are still inferior to ours-LVE, which uses the proposed LVE paradigm to explicitly assign modality contributions weights with value-based fusion. The results indicate that the key to multi-modal RL is to consider the suitability of individual modalities. Finally, by using the task-contextual re-fusion process to integrate GVE and LVE, our full method achieve superior performance even with simple feature concatenation fusion, proving the effectiveness of our task-driven hierarchical design.

**Results of Multiple Modalities** We then evaluate our method on three modalities (RGB frames, event signals, and depth frames). Although TransFuser and EFNet technically possible to scale both methods to accommodate more than two modalities, such an adaptation would require significant modifications to the implementation, along with a quadratic increase in computational complexity due to cross attention mechanism. Thus we directly compare our method with SAC, DrQ and DeepMDP. The training curves under two weather conditions are demonstrated in Fig. 3, which also show the advantage of our methods. Additional experiment results can be found in the supplementary material.

### Performance Analysis

**Ablation Studies** To systematically evaluate the effectiveness of the proposed HAVE approach and its individual components, we conduct a series of ablation experiments on the CARLA benchmark that trains agents using: 1) single modality data with RGB frames or event signals, 2) GVE with feature concatenation, 3) our proposed LVE paradigm, and 4) our full method. The training curves and testing performance are shown in Fig. 4 and Tab. 2, respectively. First, we observe that the indi

\begin{table}
\begin{tabular}{l|c|c c c c c c c} \hline \hline Weather & Measures & SAC & DrQ & DeepMDP & TransFuser & EFNet & Ours-LVE & Ours-HAVE \\ \hline \hline \multirow{2}{*}{ClearNight} & ER & 186\(\pm\) 65 & 242\(\pm\) 84 & 225\(\pm\) 87 & 260\(\pm\) 95 & 241\(\pm\) 89 & 274\(\pm\) 68 & **319\(\pm\) 71** \\  & DD(m) & 112\(\pm\) 39 & 169\(\pm\) 41 & 161\(\pm\) 51 & 178\(\pm\) 43 & 170\(\pm\) 62 & 192\(\pm\) 50 & **212\(\pm\) 52** \\ \multirow{2}{*}{CloudyNight} & ER & 218\(\pm\) 71 & 248\(\pm\) 69 & 265\(\pm\) 85 & 280\(\pm\) 71 & 289\(\pm\) 64 & 295\(\pm\) 67 & **322\(\pm\) 96** \\  & D(m) & 132\(\pm\) 64 & 167\(\pm\) 36 & 183\(\pm\) 55 & 195\(\pm\) 43 & 197\(\pm\) 41 & 213\(\pm\) 44 & **217\(\pm\) 53** \\ \multirow{2}{*}{HardRainNight} & ER & 170\(\pm\) 90 & 261\(\pm\) 91 & 255\(\pm\) 77 & 287\(\pm\) 65 & 275\(\pm\) 72 & 294\(\pm\) 85 & **327\(\pm\) 87** \\  & DD(m) & 107\(\pm\) 63 & 163\(\pm\) 63 & 160\(\pm\) 36 & 204\(\pm\) 56 & 203\(\pm\) 54 & 209\(\pm\) 54 & **229\(\pm\) 51** \\ \multirow{2}{*}{WetNight} & ER & 189\(\pm\) 79 & 234\(\pm\) 97 & 241\(\pm\) 47 & 274\(\pm\) 76 & 290\(\pm\) 93 & 289\(\pm\) 112 & **304\(\pm\) 102** \\  & D(m) & 127\(\pm\) 57 & 155\(\pm\) 53 & 164\(\pm\) 36 & 171\(\pm\) 58 & 201\(\pm\) 56 & 196\(\pm\) 61 & **222\(\pm\) 69** \\ \hline \multirow{2}{*}{ClearNoon} & ER & 235\(\pm\) 58 & 280\(\pm\) 90 & 269\(\pm\) 64 & 282\(\pm\) 53 & 234\(\pm\) 79 & 294\(\pm\) 82 & **336\(\pm\) 76** \\  & D(m) & 153\(\pm\) 49 & 195\(\pm\) 40 & 187\(\pm\) 36 & 193\(\pm\) 39 & 150\(\pm\) 46 & 186\(\pm\) 61 & **223\(\pm\) 44** \\ \multirow{2}{*}{CloudyNoon} & ER & 201\(\pm\) 87 & 274\(\pm\) 77 & 226\(\pm\) 24 & 277\(\pm\) 67 & 261\(\pm\) 78 & 293\(\pm\) 76 & **315\(\pm\) 82** \\  & DD(m) & 138\(\pm\) 68 & 170\(\pm\) 42 & 136\(\pm\) 16 & 171\(\pm\) 42 & 164\(\pm\) 58 & 186\(\pm\) 44 & **209\(\pm\) 68** \\ \multirow{2}{*}{HardRainNoon} & ER & 189\(\pm\) 74 & 220\(\pm\) 72 & 248\(\pm\) 59 & 264\(\pm\) 99 & 279\(\pm\) 91 & 287\(\pm\) 95 & **316\(\pm\) 88** \\ \multirow{2}{*}{WetNoon} & ER & 209\(\pm\) 81 & 245\(\pm\) 83 & 226\(\pm\) 52 & 304\(\pm\) 81 & 273\(\pm\) 82 & 296\(\pm\) 84 & **341\(\pm\) 78** \\  & DD(m) & 136\(\pm\) 64 & 172\(\pm\) 58 & 169\(\pm\) 38 & 213\(\pm\) 51 & 204\(\pm\) 70 & 215\(\pm\) 54 & **239\(\pm\) 55** \\ \hline \end{tabular}
\end{table}
Table 1: Comparison with state-of-the-art methods on eight different kinds of weather. ER denotes episode return and D is distance in meters. The best results are **bolded** and the second best results are underlined.

Figure 3: Performance with three modalities.

vidual modality is vulnerable to failure under extreme weather conditions. For example, the episode reward trained with RGB frame under ClearNight weather is pretty poor as shown in Fig. 4. Second, directly fusing the two modalities with GVE generally improves performance. However, the performance gain is limited, and in some cases even close to the results of using a single modality (_e.g._, in WetNight and ClearNoon). This might be caused by the heterogeneity of multi-modal data, which leads to difficulty in determining the target reward. Third, in most cases, LVE performs significantly higher than single modality or GVE-based agents, showing better cooperation of different modality data. Finally, our full method with task-contextual re-fusion clearly outperforms all other models thanks to the synergistic interplay of GVE and LVE at the reward/task-level.

**Visualization** To further obtain an intuitive understanding of our approach, we visualize the modality weights obtained by HAVE in Fig. 5 using RGB frames and event signals under two different weather conditions. The following observations can be made: 1) at the beginning, all vehicles have zero initial speed. As a result, no valid event signal is generated and the weights of RGB modality are nearly 1.0. 2) After speeding up, the weights of event signals are continuously increasing, especially when there are many moving vehicles ahead. 3) However, when the agent is facing complex background (such as road fences and clear roads without close vehicles), the event signals mainly consist of background noise, and the weights of RGB frames will raise again. The varying modality weights show that our approach can indeed adjust modality contributions under different situations, which serves as a key advantage over previous multi-modal RL methods.

\begin{table}
\begin{tabular}{l|c|c c c c|c c c c} \hline \multirow{2}{*}{Methods} & \multirow{2}{*}{Measures} & \multicolumn{3}{c}{Night} & \multicolumn{3}{c}{Noon} \\  & & & Clear & Cloudy & HardRain & Wet & Clear & Cloudy & HardRain & Wet \\ \hline \hline RGB & ER & \(75\pm 5\) & \(194\pm 36\) & \(189\pm 17\) & \(88\pm 23\) & \(214\pm 55\) & \(201\pm 37\) & \(223\pm 67\) & \(232\pm 74\) \\  & D/m & \(46\pm 21\) & \(121\pm 24\) & \(136\pm 15\) & \(53\pm 26\) & \(125\pm 42\) & \(121\pm 25\) & \(128\pm 51\) & \(141\pm 54\) \\ Events & ER & \(158\pm 29\) & \(187\pm 32\) & \(164\pm 28\) & \(175\pm 29\) & \(145\pm 37\) & \(151\pm 41\) & \(154\pm 34\) & \(147\pm 40\) \\  & D/m & \(102\pm 23\) & \(129\pm 26\) & \(105\pm 21\) & \(120\pm 18\) & \(86\pm 27\) & \(92\pm 32\) & \(104\pm 21\) & \(86\pm 36\) \\  & ER & \(225\pm 87\) & \(265\pm 85\) & \(255\pm 77\) & \(241\pm 47\) & \(269\pm 46\) & \(262\pm 24\) & \(88\pm 59\) & \(226\pm 52\) \\  & D/m & \(161\pm 51\) & \(183\pm 55\) & \(160\pm 36\) & \(164\pm 36\) & \(187\pm 36\) & \(136\pm 16\) & \(161\pm 43\) & \(169\pm 38\) \\ LVE & ER & \(274\pm 68\) & \(295\pm 67\) & \(294\pm 85\) & \(289\pm 112\) & \(294\pm 82\) & \(293\pm 76\) & \(287\pm 95\) & \(296\pm 84\) \\  & D/m & \(192\pm 50\) & \(213\pm 44\) & \(209\pm 54\) & \(109\pm 61\) & \(186\pm 61\) & \(186\pm 44\) & \(207\pm 69\) & \(215\pm 54\) \\  & ER & \(319\pm 71\) & \(322\pm 96\) & \(327\pm 87\) & \(304\pm 102\) & \(336\pm 76\) & \(315\pm 82\) & \(316\pm 88\) & \(341\pm 78\) \\  & D/m & \(212\pm 52\) & \(217\pm 53\) & \(229\pm 51\) & \(222\pm 69\) & \(223\pm 44\) & \(209\pm 68\) & \(218\pm 63\) & \(239\pm 55\) \\ \hline \end{tabular}
\end{table}
Table 2: Test performance of different models in Fig. 4. Table notations are the same as in Table 1.

Figure 4: Training curves of individual modalities and different multi-modal training paradigms.

Figure 5: Visualization of modality weights during testing. Note that only weights of RGB frames are drawn and the weights of event signals are one minus RGB frame weights at any time step.

Conclusion and Limitation

We have studied the representational capacity of value functions in multi-modal vision-based RL problems. We hypothesize that the limitation of feature-level fusion methods may come from unclear contributions for different modalities under a single value function. To mitigate this, we have presented a novel Hierarchical Adaptive Value Estimation (HAVE) framework to reconcile both feature-level and value-level fusion in a task/reward-driven manner. Our approach represents one of the first explorations of modality-specific and hierarchical value estimation for multi-modal vision-based RL tasks. Extensive experiment results demonstrate the effectiveness of our approach. However, one limitation of our work is that we mainly focus on the value estimation of multiple visual modalities, while the effectiveness of other forms of modalities (_e.g._, audio, text) is not verified. In our future work, we will consider utilizing both visual and other modalities, forming a more generalized multi-modal RL framework.

## 6 Acknowledgement

The study was funded by the Key-Area Research and Development Program of Guangdong Province with contract No. 2021B0101400002, the National Natural Science Foundation of China under contracts No. 62027804, No. 61825101, No. 62088102 and No. 62202010, and the major key project of the Peng Cheng Laboratory (PCL2021A13). Computing support was provided by Pengcheng Cloudbrain.

## References

* [1]M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, et al. (2022) Do as i can, not as i say: grounding language in robotic affordances. arXiv preprint arXiv:2204.01691. Cited by: SS1.
* [2]A. Banino, A. Puidomenech Badia, J. Walker, T. Scholtes, J. Mitrovic, and C. Blundell (2022) Coberl: contrastive bert for reinforcement learning. In Proceedings of the International Conference on Learning Representations, Cited by: SS1.
* [3]K. Bayoudh, R. K. K. Hamdaoui, and A. Mitbaa (2022) A survey on deep multimodal learning for computer vision: advances, trends, applications, and datasets. Visual Computer. Cited by: SS1.
* [4]D. C. Chaplot, L. Lee, R. Salakhutdinov, D. Parikh, and D. Batra (2020) Embodied multimodal multitask learning. In Proceedings of the International Joint Conference on Artificial Intelligence, Cited by: SS1.
* [5]G. Chen, H. Cao, J. Conradt, H. Tang, F. Rohrbein, and A. Knoll (2020) Event-based neuromorphic vision for autonomous driving: a paradigm shift for bio-inspired visual sensing and perception. IEEE Signal Processing Magazine. Cited by: SS1.
* [6]K. Chen, Y. Lee, and H. Soh (2021) Multi-modal mutual information (munmi) training for robust self-supervised deep reinforcement learning. In IEEE Proceedings of the International Conference on Robotics and Automation, Cited by: SS1.
* [7]K. Chitta, A. Prakash, B. Jaeger, Z. Yu, K. Renz, and A. Geiger (2022) Transfuser: imitation with transformer-based sensor fusion for autonomous driving. IEEE Transactions on Pattern Analysis and Machine Intelligence. Cited by: SS1.
* [8]A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun (2017) CARLA: an open urban driving simulator. In Proceedings of the Conference on Robot Learning, Cited by: SS1.
* [9]L. Fan, G. Wang, D. Huang, Z. Yu, L. Fei-Fei, Y. Zhu, and A. Anandkumar (2021) Secant: self-expert cloning for zero-shot generalization of visual policies. In Proceedings of the International Conference on Machine Learning, Cited by: SS1.
* [10]D. Feng, C. Haase-Schutz, L. Rosenbaum, H. Hertlein, C. Glaeser, F. Timm, W. Wiesbeck, and K. Dietmayer (2020) Deep multi-modal object detection and semantic segmentation for autonomous driving: datasets, methods, and challenges. IEEE Transactions on Intelligent Transportation Systems. Cited by: SS1.
** [11] Wen Fu, Yanjie Li, Zhaohui Ye, and Qi Liu. Decision making for autonomous driving via multimodal transformer and deep reinforcement learning. In _IEEE International Conference on Real-time Computing and Robotics_, 2022.
* [12] Mathias Gehrig, Willem Aarents, Daniel Gehrig, and Davide Scaramuzza. Desc: A stereo event camera dataset for driving scenarios. _IEEE Robotics and Automation Letters_, 2021.
* [13] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare. Deepmdp: Learning continuous latent space models for representation learning. In _Proceedings of the International Conference on Machine Learning_, 2019.
* [14] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _Proceedings of the International Conference on Machine Learning_, 2018.
* [15] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. _arXiv preprint arXiv:1812.05905_, 2018.
* [16] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. _arXiv preprint arXiv:1912.01603_, 2019.
* [17] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In _Proceedings of the International Conference on Machine Learning_, 2019.
* [18] Zhiyu Huang, Chen Lv, Yang Xing, and Jingda Wu. Multi-modal sensor fusion-based deep neural network for end-to-end autonomous driving with scene understanding. _IEEE Sensors Journal_, 2020.
* [19] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. _arXiv preprint arXiv:2210.03094_, 2022.
* [20] Yang Jiao, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Xiaolin Wei, Lin Ma, and Yu-Gang Jiang. Msmdfusion: Fusing lidar and camera at multiple scales with multi-depth seeds for 3d object detection. _arXiv preprint arXiv:2209.03102_, 2022.
* [21] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. _arXiv preprint arXiv:1806.10293_, 2018.
* [22] Yasser H Khalil and Hussein T Mouftah. Exploiting multi-modal fusion for urban autonomous driving using latent deep reinforcement learning. _IEEE Transactions on Vehicular Technology_, 2022.
* [23] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations for reinforcement learning. In _Proceedings of the International Conference on Machine Learning_, 2020.
* [24] Misha Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. In _Proceedings of the Advances in Neural Information Processing Systems_, 2020.
* [25] Jiale Li, Hang Dai, Hao Han, and Yong Ding. Mseg3d: Multi-modal 3d semantic segmentation for autonomous driving. _arXiv preprint arXiv:2303.08600_, 2023.
* [26] Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Yifeng Lu, Denny Zhou, Quoc V Le, et al. Deepfusion: Lidar-camera deep fusion for multi-modal 3d object detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* [27] Patrick Lichtsteiner, Christoph Posch, and Tobi Delbruck. A 128\(\times\)128 120 db 15 \(\mu\)s latency asynchronous temporal contrast vision sensor. _IEEE J. Solid-State Circuits_, 2008.
* [28] Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. In _Proceedings of the Advances in Neural Information Processing Systems_, 2021.
* [29] Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, Risheng Liu, Wei Zhong, and Zhongxuan Luo. Target-aware dual adversarial learning and a multi-scenario multi-modality benchmark to fuse infrared and visible for object detection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.

* [30] Guozheng Ma, Zhen Wang, Zhecheng Yuan, Xueqian Wang, Bo Yuan, and Dacheng Tao. A comprehensive survey of data augmentation in visual reinforcement learning. _arXiv preprint arXiv:2210.04561_, 2022.
* [31] Jinming Ma, Feng Wu, Yingfeng Chen, Xianpeng Ji, and Yu Ding. Effective multimodal reinforcement learning with modality alignment and importance enhancement. _arXiv preprint arXiv:2302.09318_, 2023.
* [32] Ana I. Maqueda, Antonio Loquercio, Guillermo Gallego, Narciso Garcia, and Davide Scaramuzza. Event-based vision meets deep learning on steering prediction for self-driving cars. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2018.
* [33] Dipendra Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In _Proceedings of the Conference on Empirical Methods in Natural Language Processing_, 2017.
* [34] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_, 2013.
* [35] Martin J Osborne and Ariel Rubinstein. _A Course in Game Theory_. The MIT Press. MIT Press, London, England, July 1994.
* [36] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-modal fusion transformer for end-to-end autonomous driving. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021.
* [37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [38] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel. Masked world models for visual control. In _Proceedings of the Conference on Robot Learning_. PMLR, 2023.
* [39] Afagh Mehri Shervedani, Siyu Li, Natawut Monaikul, Bahareh Abbasi, Barbara Di Eugenio, and Milos Zefran. Multimodal reinforcement learning for robots collaborating with humans. _arXiv preprint arXiv:2303.07265_, 2023.
* [40] Hailuo Song, Ao Li, Tong Wang, and Minghui Wang. Multimodal deep reinforcement learning with auxiliary task for obstacle avoidance of indoor mobile robot. _Sensors_, 2021.
* [41] Mengke Song, Wenfeng Song, Guowei Yang, and Chenglizhao Chen. Improving rgb-d salient object detection via modality-aware decoder. _IEEE Transactions on Image Processing_, 2022.
* [42] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling representation learning from reinforcement learning. In _Proceedings of the International Conference on Machine Learning_. PMLR, 2021.
* [43] Lei Sun, Christos Sakaridis, Jingyun Liang, Qi Jiang, Kailun Yang, Peng Sun, Yaozu Ye, Kaiwei Wang, and Luc Van Gool. Event-based fusion for motion deblurring with cross-modal attention. In _Proceedings of European Conference on Computer Vision, year=2022_.
* [44] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Proceedings of the International Conference on Neural Information Processing Systems_, 2017.
* [46] Sai Vemprala, Sami Mian, and Ashish Kapoor. Representation learning for event-based visuomotor policies. In _Proceedings of the Advances in Neural Information Processing Systems_, 2021.
* [47] Celyn Walters and Simon Hadfield. Ceril: Continuous event-based reinforcement learning. _arXiv preprint arXiv:2302.07667_, 2023.
* [48] Lin Wang, I S Mohammad Mostafavi, Yo-Sung Ho, and Kuk-Jin Yoon. Event-based high dynamic range image and very high frame rate video generation using conditional generative adversarial networks. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019.
* [49] Zongwei Wu, Guillaume Allibert, Fabrice Meriaudeau, Chao Ma, and Cedric Demonceaux. Hidanet: Rgb-d salient object detection via hierarchical depth awareness. _arXiv preprint arXiv:2301.07405_, 2023.

* [50] Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In _Proceedings of the International Conference on Learning Representations_, 2020.
* [51] Zhecheng Yuan, Zhengrong Xue, Bo Yuan, Xueqian Wang, Yi Wu, Yang Gao, and Huazhe Xu. Pretrained image encoder for generalizable visual reinforcement learning. In _Proceedings of the Advances in Neural Information Processing Systems_, 2022.
* [52] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. _arXiv preprint arXiv:2006.10742_, 2020.
* [53] Jiaping Zhang, Tiancheng Zhao, and Zhou Yu. Multimodal hierarchical reinforcement learning policy for task-oriented visual dialog. In _Proceedings of the Annual SIGdial Meeting on Discourse and Dialogue_, 2018.
* [54] Yue Zhao, Chenzhuang Du, Hang Zhao, and Tiejun Li. Intrinsically motivated self-supervised learning in reinforcement learning. In _Proceedings of the International Conference on Robotics and Automation_. IEEE, 2022.