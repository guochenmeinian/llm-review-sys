# Towards a Unified Framework of Clustering-based Anomaly Detection

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Unsupervised Anomaly Detection (UAD) plays a crucial role in identifying abnormal patterns within data without labeled examples, holding significant practical implications across various domains. Although the individual contributions of representation learning and clustering to anomaly detection are well-established, their interdependencies remain under-explored due to the absence of a unified theoretical framework. Consequently, their collective potential to enhance anomaly detection performance remains largely untapped. To bridge this gap, in this paper, we propose a novel probabilistic mixture model for anomaly detection to establish a theoretical connection among representation learning, clustering, and anomaly detection. By maximizing a novel anomaly-aware data likelihood, representation learning and clustering can effectively reduce the adverse impact of anomalous data and collaboratively benefit anomaly detection. Meanwhile, a theoretically substantiated anomaly score is naturally derived from this framework. Lastly, drawing inspiration from gravitational analysis in physics, we have devised an improved anomaly score that more effectively harnesses the combined power of representation learning and clustering. Extensive experiments, involving 17 baseline methods across 30 diverse datasets, validate the effectiveness and generalization capability of the proposed method, surpassing state-of-the-art methods.

## 1 Introduction

Unsupervised Anomaly Detection (UAD) refers to the task dedicated to identifying abnormal patterns or instances within data in the absence of labeled examples [8]. It has long received extensive attention in the past decades for its wide-ranging applications in numerous practical scenarios, including financial auditing [3], healthcare monitoring [44] and e-commerce sector [23]. Due to the lack of explicit label guidance, the key to UAD is to uncover the dominant patterns that widely exist in the dataset so that samples do not conform to these patterns can be recognized as anomalies. To achieve this, early works [7] have heavily relied on powerful unsupervised _representation learning_ methods to extract the normal patterns from high-dimensional and complex data such as images, text, and graphs. More recent works [45; 2] have utilized _clustering_, a widely observed natural pattern in real-world data, to provide critical global information for anomaly detection and achieved tremendous success.

While the individual contributions of representation learning and clustering to anomaly detection are well-established, their interrelationships remain largely unexplored. Intuitively, _discriminative representation learning_ can leverage accurate clustering results to differentiate samples from distinct clusters in the embedding space (i.e., 1). Similarly, it can utilize accurate anomaly detection to avoid preserving abnormal patterns (i.e., 2). For _accurate clustering_, it can gain advantages from representation learning by operating in the discriminative embedding space (i.e., 3). Meanwhile, itcan potentially benefit from accurate anomaly detection by excluding anomalies when formulating clusters (i.e., ). _Anomaly detection_ can greatly benefit from both discriminative representation learning and accurate clustering (i.e., & & ). However, these benefits hinge on the successful identification of anomalies and the reduction of their detrimental impact on the aforementioned tasks. As depicted in Figure 1, the integration of these three elements exhibits a significant reciprocal nature. In summary, representation learning, clustering, and anomaly detection are interdependent and intricately intertwined. Therefore, it is crucial for anomaly detection to _fully leverage and mutually enhance the relationships among these three components_.

Despite the intuitive significance of the interactions among representation learning, clustering, and anomaly detection, existing methods have only made limited attempts to exploit them and fall short of expectations. On one hand, some methods [58] have acknowledged the interplay among these three factors, but their focus remains primarily on the interactions between two factors at a time, making only targeted improvements. For instance, some strategies include explicitly removing outlier samples during the clustering process [9] or designing robust representation learning methods [10] to mitigate the influence of anomalies. On the other hand, recent methods [45] have begun to explore the simultaneous optimization of these three factors within a single framework. However, these attempts are still in the stage of merely superimposing the objectives of the three factors without a unified theoretical framework. This lack of a guiding framework prevents the adequate modeling of the interdependencies among these factors, thereby limiting their collective contribution to a unified anomaly detection objective. Consequently, we aim to address the following question: _Is it possible to employ a unified theoretical framework to jointly model these three interdependent objectives, thereby leveraging their respective strengths to enhance anomaly detection?_

In this paper, we try to answer this question and propose a novel model named UniCAD for anomaly detection. The proposed UniCAD integrates representation learning, clustering, and anomaly detection into a unified framework, achieved through the theoretical guidance of maximizing the anomaly-aware data likelihood. Specifically, we explicitly model the relationships between samples and multiple clusters in the representation space using the probabilistic mixture models for the likelihood estimation. Moreover, we creatively introduce a learnable indicator function into the objective of maximum likelihood to explicitly attenuate the influence of anomalies on representation learning and clustering. Under this framework, we can theoretically derive an anomaly score that indicates the abnormality of samples, rather than heuristically designing it based on clustering results as existing works do. Furthermore, building upon this theoretically supported anomaly score and inspired by the theory of universal gravitation, we propose a more comprehensive anomaly metric that considers the complex relationships between samples and multiple clusters. This allows us to better utilize the learned representations and clustering results from this framework for anomaly detection.

To sum up, we underline our contributions as follows:

* We propose a unified theoretical framework to jointly optimize representation learning, clustering, and anomaly detection, allowing their mutual enhancement and aid in anomaly detection.
* Based on the proposed framework, we derive a theoretically grounded anomaly score and further introduce a more comprehensive score with the vector summation, which fully releases the power of the framework for effective anomaly detection.
* Extensive experiments have been conducted on 30 datasets to validate the superior unsupervised anomaly detection performance of our approach, which surpassed the state-of-the-art through comparative evaluations with 17 baseline methods.

Figure 1: Interdependent relationships among representation learning, clustering, and anomaly detection.

Related Work

Typical unsupervised anomaly detection (UAD) methods calculate a continuous score for each sample to measure its anomaly degree. Various UAD methods have been proposed based on different assumptions, making them suitable for detecting various types of anomaly patterns, including subspace-based models [24], statistical models [16], linear models [49; 32], density-based models [6; 38], ensemble-based models [39; 29], probability-based models [40; 58; 28; 27], neural network-based models [42; 51], and cluster-based models [18; 9]. Considering the field of anomaly detection has progressed by integrating clustering information to enhance detection accuracy [26; 56], we primarily focus on and analyze anomaly patterns related to clustering, incorporating a global clustering perspective to assess the degree of anomaly. Notable methods in this context include CBLOF [18], which evaluates anomalies based on the size of the nearest cluster and the distance to the nearest large cluster. Similarly, DCFOD [45] introduces innovation by applying the self-training architecture of the deep clustering [50] to outlier detection. Meanwhile, DAGMM [58] combines deep autoencoders with Gaussian mixture models, utilizing sample energy as a metric to quantify the anomaly degree. In contrast, our approach introduces a unified theoretical framework that integrates representation learning, clustering, and anomaly detection, overcoming the limitations of heuristic designs and the overlooked anomaly influence in existing methods.

## 3 Methodology

In this section, we first define the problem we studied and the notations used in this paper. Then we elaborate on the proposed method UniCAD. More specifically, we first introduce a novel learning objective that optimizes representation learning, clustering, and anomaly detection within a unified theoretical framework by maximizing the data likelihood. A novel anomaly score with theoretical support is also naturally derived from this framework. Then, inspired by the concept of universal gravitation, we further propose an enhanced anomaly scoring approach that leverages the intricate relationship between samples and clustering to detect anomalies effectively. Finally, we present an efficient iterative optimization strategy to optimize this model and provide a complexity analysis for the proposed model.

**Definition 1** (Unsupervised Anomaly Detection).: _Given a dataset \(\mathbf{X}\in\mathbb{R}^{N\times D}\) comprising \(N\) instances with \(D\)-dimensional features, unsupervised anomaly detection aims to learn an anomaly score \(o_{i}\) for each instance \(\mathbf{x}_{i}\) in an unsupervised manner so that the abnormal ones have higher scores than the normal ones._

### Maximizing Anomaly-aware Likelihood

Previous research has demonstrated the importance of discriminative representation and accurate clustering in anomaly detection [45]. However, the presence of anomalous samples can significantly disrupt the effectiveness of both representation learning and clustering [12]. While some existing studies have attempted to integrate these three separate learning objectives, the lack of a unified theoretical framework has hindered their mutual enhancement, leading to suboptimal results.

To tackle this issue, in this paper, we propose a unified and coherent approach that considers representation learning, clustering, and anomaly detection by maximizing the likelihood of the observed data. Specifically, we denote the parameters of representation learning as \(\Theta\), the clustering parameter as \(\Phi\), and the dynamic indicator function for anomaly detection as \(\delta(\cdot)\). These parameters are optimized simultaneously by maximizing the likelihood of the observed data \(\mathbf{X}\):

\[\max\log p(\mathbf{X}|\Theta,\Phi)=\max\sum_{i=1}^{N}\delta(\mathbf{x}_{i}) \log p(\mathbf{x}_{i}|\Theta,\Phi)=\max\sum_{i=1}^{N}\delta(\mathbf{x}_{i}) \log\sum_{k=1}^{K}p(\mathbf{x}_{i},c_{i}=k|\Theta,\Phi),\] (1)

where \(c_{i}\) represents the latent cluster variable associated with \(\mathbf{x}_{i}\), and \(c_{i}=k\) denotes the probabilistic event that \(\mathbf{x}_{i}\) belongs to the \(k\)-th cluster. The \(\delta(\mathbf{x}_{i})\) is an indicator function that determines whether a sample \(\mathbf{x}_{i}\) is an anomaly of value 0 or a normal sample of value 1.

#### 3.1.1 Joint Representation Learning and Clustering with \(p(\mathbf{x}_{i}|\Theta,\Phi)\)

Based on the aforementioned advantages of MMs, we estimate the likelihood \(p(\mathbf{x}_{i}|\Theta,\Phi)\) with mixture models defined as:

\[\begin{split} p(\mathbf{x}_{i}|\Theta,\Phi)=\sum_{k=1}^{K}p( \mathbf{x}_{i},c_{i}=k|\Theta,\Phi)&=\sum_{k=1}^{K}p(c_{i}=k) \cdot p(\mathbf{x}_{i}|c_{i}=k,\Theta,\boldsymbol{\mu}_{k},\Sigma_{k})\\ &=\sum_{k=1}^{K}\omega_{k}\cdot p(\mathbf{x}_{i}|c_{i}=k,\Theta, \boldsymbol{\mu}_{k},\Sigma_{k}),\end{split}\] (2)

where \(\Phi=\{\omega_{k},\boldsymbol{\mu}_{k},\Sigma_{k}\}\). The mixture model is parameterized by the prototypes \(\boldsymbol{\mu}_{k}\), covariance matrices \(\Sigma_{k}\), and mixture weights \(\omega_{k}\) from all clusters. \(\sum_{k=1}^{K}\omega_{k}=1\), and \(k=1,2,\cdots,K\).

In practice, the samples are usually attributed to high-dimensional features and it is challenging to detect anomalies from the raw feature space [41]. Therefore, modern anomaly detection methods [42, 58] often map raw data samples \(\mathbf{X}=\{\mathbf{x}_{i}\}\in\mathbb{R}^{N\times D}\) into a low-dimensional representation space \(\mathbf{Z}=\{\mathbf{z}_{i}\}\in\mathbb{R}^{N\times d}\) with a representation learning function \(\mathbf{z}_{i}=f_{\Theta}(\mathbf{x}_{i})\) and detect anomalies within this latent representation space.

Following this widely adopted practice, we model the distribution of samples in the latent representation space with a multivariate Student's-\(t\) distribution giving its cluster \(c_{i}=k\). The Student's-\(t\) distribution is robust against outliers due to its heavy tails. Bayesian robustness theory leverages such distributions to dismiss outlier data, favoring reliable sources, making the Student's-\(t\) process preferable over Gaussian processes for data with atypical information [1]. Thus the probability distribution of generating \(\mathbf{x}_{i}\) with latent representation \(\mathbf{z}_{i}\) given its cluster \(c_{i}=k\) can be expressed as:

\[p(\mathbf{x}_{i}|c_{i}=k,\Theta,\boldsymbol{\mu}_{k},\Sigma_{k})=\frac{\Gamma (\frac{\nu+1}{2})|\Sigma_{k}|^{-1/2}}{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}} \left(1+\frac{1}{\nu}D_{M}(\mathbf{z}_{i},\boldsymbol{\mu}_{k})^{2}\right)^{- \frac{\nu+1}{2}},\] (3)

where \(\mathbf{z}_{i}=f_{\Theta}(\mathbf{x}_{i})\) denotes the representation obtained from the data mapped through the neural network parameterized by \(\Theta\). \(\Gamma\) denotes the gamma function while \(\nu\) is the degree of freedom. \(\Sigma_{k}\) is the scale parameter. \(D_{M}(\mathbf{z}_{i},\boldsymbol{\mu}_{k})=\sqrt{(\mathbf{z}_{i}-\boldsymbol{ \mu}_{k})^{T}\Sigma_{k}^{-1}(\mathbf{z}_{i}-\boldsymbol{\mu}_{k})}\) represents the Mahalanobis distance [33]. In the unsupervised setting, as cross-validing \(\nu\) on a validation set or learning it is unnecessary, \(\nu\) is set as 1 for all experiments [50, 48]. The overall marginal likelihood of the observed data \(\mathbf{x}_{i}\) can be simplified as:

\[p(\mathbf{x}_{i}|\Theta,\Phi)=\sum_{k=1}^{K}\omega_{k}\cdot\frac{\pi^{-1} \cdot|\Sigma_{k}|^{-1/2}}{1+D_{M}(\mathbf{z}_{i},\boldsymbol{\mu}_{k})^{2}}.\] (4)

#### 3.1.2 Anomaly Indicator \(\delta(\mathbf{x}_{i})\) and Score \(o_{i}\)

As we have discussed, the indicator function \(\delta(\mathbf{x}_{i})\) not only benefits both representation and clustering but also directly serves as the output of anomaly detection. Ideally, with the percentage of outliers denoted as \(l\), an optimal solution for \(\delta(\mathbf{x}_{i})\) that maximizes the objective function \(J(\Theta,\Phi)\) entails setting all \(\delta(\mathbf{x}_{i})=0\) for \(\mathbf{x}_{i}\) among the \(l\) percent of outliers with lowest generation possibility \(p(\mathbf{x}_{i}|\Theta,\Phi)\), and otherwise \(\delta(\mathbf{x}_{i})=1\) is set for the remaining normal samples. Therefore, the indicator function is determined as:

\[\delta(\mathbf{x}_{i})=\begin{cases}0,&\text{if }p(\mathbf{x}_{i}|\Theta,\Phi) \text{ is among the }l\text{ lowest},\\ 1,&\text{otherwise.}\end{cases}\] (5)

As this method involves sorting the samples based on the generation probability as being anomalous, the values of \(p(\mathbf{x}_{i}|\Theta,\Phi)\) can serve as a form of anomaly score, a classic approach within the mixture model framework [40, 58]. This suggests that the likelihood of a sample being anomalous is inversely related to its generative probability since a lower generative probability indicates a higher chance of the sample being an outlier. Thus the anomaly score of sample \(\mathbf{x}_{i}\) can be defined as:\[o_{i}=\frac{1}{p(\mathbf{x}_{i}|\Theta,\Phi)}=\frac{1}{\sum_{k=1}^{K}\omega_{k} \cdot\frac{\pi^{-1}\cdot|\Sigma_{k}|^{-1/2}}{1+D_{M}(\mathbf{z}_{i},\bm{\mu}_{k} )^{2}}}.\] (6)

### Gravity-inspired Anomaly Scoring

In practical applications, it is proved that anomaly scores derived from generation probabilities often yield suboptimal performance [17]. This observation prompts a reconsideration of _how to fully leverage the complex relationships among samples or even across multiple clusters for anomaly detection_. In this section, we first provide a brief introduction to the concept of Newton's Law of Universal Gravitation [35] and then demonstrate how the anomaly score is intriguingly similar to this cross-field principle. Finally, we discuss the advantages of introducing the vector sum operation into the anomaly score inspired by the analogy.

#### 3.2.1 Analog Anomaly Scoring and Force Analysis

To begin with, Newton's Law of Universal Gravitation [35] stands as a fundamental framework for describing the interactions among entities in the physical world. According to this law, every object in the universe experiences an attractive force from another object. In classical mechanics, force analysis involves calculating the vector sum of all forces acting on an object, known as the **resultant force**, which is crucial in determining an object's acceleration or change in motion:

\[\mathbf{\widetilde{F}}_{i,\text{total}}=\sum_{k=1}^{K}\mathbf{ \widetilde{F}}_{ik},\text{~{}~{}with~{}}\mathbf{\widetilde{F}}_{ik}=\frac{G \cdot m_{i}m_{k}}{r_{ik}^{2}}\cdot\mathbf{\widetilde{r}}_{ik},\] (7)

where \(\mathbf{\widetilde{F}}_{ik}\) represents the \(k\)-th force acting on the object \(i\). This force is proportional to the product of their masses, (\(m_{i}\) and \(m_{k}\)), and inversely proportional to the square of the distance \(r_{ik}\) between them. \(G\) represents the gravitational constant, and \(\mathbf{\widetilde{r}}_{ij}\) is the unit direction vector.

Similarly, if denoting: \(\mathbf{\widetilde{F}}_{ik}=p(\mathbf{x}_{i},c_{i}=k|\Theta,\Phi)=\omega_{k} \cdot\frac{\pi^{-1}\cdot|\Sigma_{k}|^{-1/2}}{1+D_{M}(\mathbf{z}_{i},\bm{\mu}_{ k})^{2}}\), the score of Equation (6) bears analogies to the summation of the magnitudes of forces as:

\[o_{i}=\frac{1}{\sum_{k=1}^{K}\mathbf{\widetilde{F}}_{ik}},\text{~{}~{}with~{} }\mathbf{\widetilde{F}}_{ik}=\frac{\widetilde{G}\cdot\widetilde{m}_{i} \widetilde{m}_{k}}{\widetilde{r}_{ik}^{2}},\] (8)

where \(\widetilde{G}=\pi^{-1}\), \(\widetilde{m}_{k}=\omega_{k}|\Sigma_{k}|^{-1/2}\), \(\widetilde{m}_{i}=1\), and \(\widetilde{r}_{ik}=\sqrt{1+D_{M}(\mathbf{z}_{i},\bm{\mu}_{k})^{2}}\). Here, \(\widetilde{r}_{ik}\) is taken as the measure of distance within the representation space, modified slightly by an additional term for smoothness. The constant \(\widetilde{G}\) serves a role akin to the gravitational constant in this analogy, whereas \(\widetilde{m}_{k}\) resembles the concept of mass for the cluster. The notation \(\widetilde{m}_{i}\) suggests a standardization where the mass of each data point is considered uniform and not differentiated.

#### 3.2.2 Anomaly Scoring with Vector Sum

Comparing Equation (7) with Equation (8), what still differs is that, unlike a simple sum of the scalar value, the resultant force \(\mathbf{\widetilde{F}}_{i,\text{total}}\) employs the vector sum and incorporates both the magnitude and direction \(\mathbf{\widetilde{r}}_{ik}\) of each force. This distinction is crucial because forces in different directions can neutralize each other with a large angle between them or enhance each other's effects with a small angle. Inspired by this difference, we consider modeling the relationship between samples and clusters as a vector, and aggregating them through vector summation. The vector-formed anomaly score \(o_{i}^{V}\) is defined as:

\[o_{i}^{V}=\frac{1}{\|\sum_{k=1}^{K}\mathbf{\widetilde{F}}_{ik} \cdot\mathbf{\widetilde{r}}_{ik}\|},\] (9)

where \(\mathbf{\widetilde{r}}_{ik}\) represents the unit direction vector in the representation space from the sample \(\mathbf{z}_{i}\) to the cluster prototype \(\bm{\mu}_{k}\), and \(\|\cdot\|\) represents the \(L_{2}\) norm.

### Iterative Optimization

Given the challenge posed by the interdependence of the parameters of the network \(\Theta\) and those of the mixture model \(\{\omega_{k},\bm{\mu}_{k},\Sigma_{k}\}\) in joint optimization, we propose an iterative optimization procedure. The pseudocode for training the model is presented in Algorithm 1 in the appendix.

#### 3.3.1 Update \(\Phi\)

To update the parameters of the mixture model \(\Phi=\{\omega_{k},\bm{\mu}_{k},\Sigma_{k}\}\), we use the Expectation-Maximization (EM) algorithm to maximize equation (1) [36]. The detailed derivation is included in Appendix B.

**E-step.** During the E-step of iteration \((t+1)\), our goal is to compute the posterior probabilities of each data point belonging to the \(k\)-th cluster within the mixture model. Given the observed sample \(\mathbf{x}_{i}\) and the current estimates of the parameters \(\Theta^{(t)}\) and \(\Phi^{(t)}\), the expected value of the likelihood function of latent variable \(c_{k}\), or the posterior possibilities, can be expressed as:

\[\bm{\tau}_{ik}^{(t+1)}=p(c_{i}=k|\mathbf{x}_{i},\Theta,\Phi^{(t)})=\frac{p( \mathbf{x}_{i},c_{i}=k|\Theta,\Phi^{(t)})}{\sum_{j=1}^{K}p(\mathbf{x}_{i},c_{i }=j|\Theta,\Phi^{(t)})}=\frac{\widetilde{\mathbf{F}}_{ik}^{(t)}}{\sum_{j=1}^{K }\widetilde{\mathbf{F}}_{ij}^{(t)}}.\] (10)

The scale factor[36] serving as an intermediate result for subsequent updates in the M-step is :

\[\mathbf{u}_{ik}^{(t+1)}=\frac{2}{1+D_{M}(\mathbf{z}_{i}^{(t)},\bm{\mu}_{k}^{( t)})}.\] (11)

**M-step.** In the M-step of iteration \((t+1)\), given the gradients \(\frac{\partial J(\Theta,\Phi)}{\partial\omega_{k}}=0\), \(\frac{\partial J(\Theta,\Phi)}{\partial\bm{\mu}_{k}}=0\), and \(\frac{\partial J(\Theta,\Phi)}{\partial\Sigma_{k}}=0\), we derive the analytical solutions for the mixture model parameters \(\omega_{k}\), \(\bm{\mu}_{k}\), and \(\Sigma_{k}\). Assume the anomalous ratio is \(l\in[0,1]\), the number of the normal samples is \(n=\text{int}(l*N)\). The updating process for \(\{\omega_{k}^{(t+1)},\bm{\mu}_{k}^{(t+1)},\bm{\Sigma}_{k}^{(t+1)}\}\) is as follows:

* The mixture weights \(\omega_{k}\) are updated by averaging the posterior probabilities over all data points with the number of samples, reflecting the relative presence of each component in the mixture: \[\omega_{k}^{(t+1)}=\sum_{i=1}^{n}\bm{\tau}_{ik}^{(t+1)}/n.\] (12)
* The prototypes \(\bm{\mu}_{k}\) are updated to be the weighted average of the data points, where weights are the posterior probabilities: \[\bm{\mu}_{k}^{(t+1)}=\sum_{i=1}^{n}\left(\bm{\tau}_{ik}^{(t+1)}\mathbf{u}_{ ik}^{(t+1)}\mathbf{z}_{i}\right)/\sum_{i=1}^{n}\left(\bm{\tau}_{ik}^{(t+1)} \mathbf{u}_{ik}^{(t+1)}\right).\] (13)
* The covariance matrices \(\Sigma_{k}\) are updated by considering the dispersion of the data around the newly computed prototypes: \[\bm{\Sigma}_{k}^{(t+1)}=\frac{\sum_{i=1}^{n}\bm{\tau}_{ik}^{(t+1)}\mathbf{u}_{ ik}^{(t+1)}(\mathbf{z}_{i}-\bm{\mu}_{k}^{(t+1)})(\mathbf{z}_{i}-\bm{\mu}_{k}^{(t+1 )})\bm{\tau}}{\sum_{j=1}^{K}\bm{\tau}_{ij}^{(t+1)}}.\] (14)

#### 3.3.2 Update \(\Theta\)

We focus on anomaly-aware representation learning and use stochastic gradient descent to optimize the network parameters \(\Theta\), by minimizing the following joint loss:

\[\mathcal{L}=-J(\Theta,\Phi)+g(\Theta),\] (15)

where \(J(\Theta,\Phi)=\log p(\mathbf{X}|\Theta,\Phi)\). An additional constraint term \(g(\Theta)\) is introduced to prevent shortcut solution [15]. In practice, an autoencoder architecture is implemented, utilizing a reconstruction loss \(g(\Theta)=\|x-\hat{x}\|^{2}\) as the constraint.

These updates are iteratively performed until convergence, resulting in optimized model parameters that best fit the given data according to the mixture model framework.

Experiments

### Datasets & Baselines

We evaluated UniCAD on an extensive collection of datasets, comprising 30 tabular datasets that span 16 diverse fields. We specifically focused on naturally occurring anomaly patterns, rather than synthetically generated or injected anomalies, as this aligns more closely with real-world scenarios. The detailed descriptions are provided in Table 4 of Appendix D.1. Following the setup in ADBench [17], we adopt an inductive setting to predict newly emerging data, a highly beneficial approach for practical applications.

To assess the effectiveness of UniCAD, we compared it with 17 advanced unsupervised anomaly detection methods, including: (1) _traditional methods_: SOD [24] and HBOS [16]; (2) _linear methods_: PCA [49] and OCSVM [32]; (3) _density-based methods_: LOF [6] and KNN [38]; (4) _ensemble-based methods_: LODA [39] and Forest [29]; (5) _probability-based methods_: DAGMM [58], ECOD [28], and COPOD [27]; (6) _cluster-based methods_: DBSCAN [13], CBLOF [18], DCOD [45] and KMeans- [9]; and (7) _neural network-based methods_: DeepSVDD [42] and DIF [51]. These baselines encompass the majority of the latest methods, providing a comprehensive overview of the state-of-the-art. For a detailed description, please refer to Appendix D.2.

### Experiment Settings

In the unsupervised setting, we employ the default hyperparameters from the original papers for all comparison methods. Similarly, the UniCAD also utilizes a fixed set of parameters to ensure a fair comparison. For all datasets, we employ a two-layer MLP with a hidden dimension of \(d=128\) and ReLU activation function as both encoder and decoder. We utilize the Adam optimizer [21] with a learning rate of \(1e^{-4}\) for 100 epochs. For the EM process, we set the maximum iteration number to 100 and a tolerance of \(1e^{-3}\) for stopping training when the objectives converge. The number of components in the mixture model is set as \(k=10\), and the proportion of the outlier is set as \(l=1\%\). We evaluate the methods using Area Under the Receiver Operating Characteristic (AUC-ROC) and Area Under the Precision-Recall Curve (AUC-PR) metrics [17], reporting the average ranking (Avg. Rank) across all datasets. All experiments are run 3 times with different seeds, and the mean results are reported.

### Performance and Analysis

**Performance Comparison**. Table 1 presents a comparison of UniCAD with 10 unsupervised baseline methods across 30 tabular datasets using the AUC-ROC metric. The experimental results, which encompass 17 baselines, are included in Tables 5 and 6 of Appendix D.3, with additional experiments on other data domains presented in Appendix E. Our proposed UniCAD achieves the top average ranking, exhibiting the best or near-best performance on a larger number of datasets and confirming advanced capabilities. It is noteworthy that there is no one-size-fits-all unsupervised anomaly detection method suitable for every type of dataset, as demonstrated by the observation that other methods have also achieved some of the best results on certain datasets. However, our model showcased a remarkable ability to generalize across most datasets featuring natural anomalies, as evidenced by statistical average ranking. As for clustering-based methods such as KMeans-, DCOD, and CBLOF, they mostly rank in the top tier among all baseline methods, supporting the advantage of combining deep clustering with anomaly detection. However, our method significantly outperformed these methods by mitigating their limitations and further providing a unified framework for joint representation learning, clustering, and anomaly detection.

**Effectiveness of Vector Sum in Anomaly Scoring**. As demonstrated in Table 1, we compare the anomaly score \(\mathbf{o}_{i}\) derived directly from the generation possibility with its vector summation form \(\mathbf{o}_{i}^{V}\). According to our statistical findings, we observe that vector scores \(\mathbf{o}_{i}^{V}\) consistently outperform scalar scores \(\mathbf{o}_{i}\). This indicates that the introduction of the vector summation, analogous to the concept of resultant force, makes a substantial difference in anomaly detection scenarios involving multiple clusters. The performance gains of the vector sum scores strongly demonstrate the effectiveness of the UniCAD in capturing the subtle differences in the distinctions among multiple clusters and underscore the utility of this factor in the context of anomaly detection based on clustering.

[MISSING_PAGE_FAIL:8]

to efficiently model complex patterns but also achieves an optimal balance between computational efficiency and modeling capability.

### Ablation Studies

In this section, we examine the contributions of different components in UniCAD. Tables 3 reports the results. We make three major observations. **Firstly**, the anomaly detection performance experiences a significant drop when replacing the Student's t distribution with a Gaussian distribution for the Mixture Model, highlighting the robustness of the Student's t distribution in unsupervised anomaly detection. **Secondly**, omitting the likelihood maximization loss (w/o \(J(\Theta,\Phi)\)) also results in a considerable decrease in overall performance. This observation underscores the importance of deriving both the optimization objectives and anomaly scores from the likelihood generation probability through a theoretical framework, which allows for unified joint optimization of anomaly detection and clustering in the representation space. **Furthermore**, the indicator function \(\delta(\mathbf{x}_{i})\) also contributes to a performance increase. These results further confirm the effectiveness of our UniCAD in mitigating the negative influence of anomalies in the clustering process, as the existence of outliers may significantly degrade the performance of clustering. In summary, all these ablation studies clearly demonstrate the effectiveness of our theoretical framework in simultaneously considering representation learning, clustering, and anomaly detection.

### Sensitivity of Hyperparameters

In this section, we conducted a sensitivity analysis on key hyperparameters of the model applied to the donors dataset, focusing on the number of clusters \(k\) and the proportion of the outlier set \(l\). The results of this analysis are illustrated in Figure 2. Notably, the optimal range for \(l\) tends to be lower than the actual proportion of anomalies in the dataset. Furthermore, a pattern was observed with the number of clusters \(k\), where the model performance initially improved with an increase in \(k\), followed by a subsequent decline. This suggests the existence of an optimal range for the number of clusters, which should be carefully selected based on the specific application context.

## 5 Conclusion

This paper presents UniCAD, a novel model for Unsupervised Anomaly Detection (UAD) that seamlessly integrates representation learning, clustering, and anomaly detection within a unified theoretical framework. Specifically, UniCAD introduces an anomaly-aware data likelihood based on the mixture model with the Student-t distribution to guide the joint optimization process, effectively mitigating the impact of anomalies on representation learning and clustering. This framework enables a theoretically grounded anomaly score inspired by universal gravitation, which considers complex relationships between samples and multiple clusters. Extensive experiments on 30 datasets across various domains demonstrate the effectiveness and generalization capability of UniCAD, surpassing 15 baseline methods and establishing it as a state-of-the-art solution in unsupervised anomaly detection. Despite its potential, the proposed method's applicability to broader fields like time series and multimodal anomaly detection requires further exploration and validation, highlighting a significant area for future work.

\begin{table}
\begin{tabular}{l|r r r r r} \hline \hline
**Phase** & **IForest** & **KMeans-** & **DAGMM** & **DCOD** & **UniCAD** \\ \hline Fit & 0.256 & 103.697 & 795.004 & 4548.634 & 246.113 \\ Infer & 0.0186 & 0.059 & 4.190 & 16.190 & 0.079 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Runtime Comparison. The runtime is reported in seconds (s).

\begin{table}
\begin{tabular}{l|r r r r} \hline \hline
**Metric** & **w/ Gauss.** & **w/o**\(J(\Theta,\Phi)\) & **w/o**\(\delta(\mathbf{x}_{i})\) & **Full Model** \\ \hline Avg. Rank (w/ baselines \& variants) & 6.2 & 6.6 & 5.0 & **4.2** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation study on AUC-ROC scores, calculated across 30 datasets.

## References

* [1] J Ailton A Andrade. On the robustness to outliers of the student-t process. _Scandinavian Journal of Statistics_, 50(2):725-749, 2023.
* [2] Caglar Aytekin, Xingyang Ni, Francesco Cricri, and Emre Aksu. Clustering and unsupervised anomaly detection with l 2 normalized deep auto-encoder representations. In _2018 International Joint Conference on Neural Networks (IJCNN)_, pages 1-6. IEEE, 2018.
* [3] Alexander Bakumenko and Ahmed Elragal. Detecting anomalies in financial data using machine learning algorithms. _Systems_, 10(5):130, 2022.
* [4] Sambaran Bandyopadhyay, Saley Vishal Vivek, and MN Murty. Outlier resistant unsupervised deep architectures for attributed network embedding. In _Proceedings of the 13th international conference on web search and data mining_, pages 25-33, 2020.
* [5] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. _Advances in neural information processing systems_, 26, 2013.
* [6] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and Jorg Sander. Lof: identifying density-based local outliers. In _Proceedings of the 2000 ACM SIGMOD international conference on Management of data_, pages 93-104, 2000.
* [7] Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly detection: A survey. _arXiv preprint arXiv:1901.03407_, 2019.
* [8] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. _ACM computing surveys (CSUR)_, 41(3):1-58, 2009.
* [9] Sanjay Chawla and Aristides Gionis. k-means-: A unified approach to clustering and outlier detection. In _Proceedings of the 2013 SIAM international conference on data mining_, pages 189-197. SIAM, 2013.
* [10] Hyunsoo Cho, Jinseok Seol, and Sang-goo Lee. Masked contrastive learning for anomaly detection. _arXiv preprint arXiv:2105.08793_, 2021.
* [11] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. Deep anomaly detection on attributed networks. In _Proceedings of the 2019 SIAM International Conference on Data Mining_, pages 594-602. SIAM, 2019.
* [12] Lian Duan, Lida Xu, Ying Liu, and Jun Lee. Cluster-based outlier detection. _Annals of Operations Research_, 168:151-168, 2009.
* [13] Martin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering clusters in large spatial databases with noise. In _kdd_, volume 96, pages 226-231, 1996.
* [14] Haoyi Fan, Fengbin Zhang, and Zuoyong Li. Anomalydae: Dual autoencoder for anomaly detection on attributed networks. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 5685-5689. IEEE, 2020.
* [15] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. _Nature Machine Intelligence_, 2(11):665-673, 2020.
* [16] Markus Goldstein and Andreas Dengel. Histogram-based outlier score (hbos): A fast unsupervised anomaly detection algorithm. _KI-2012: poster and demo track_, 1:59-63, 2012.
* [17] Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. Adbench: Anomaly detection benchmark. _Advances in Neural Information Processing Systems_, 35:32142-32159, 2022.
* [18] Zengyou He, Xiaofei Xu, and Shengchun Deng. Discovering cluster-based local outliers. _Pattern recognition letters_, 24(9-10):1641-1650, 2003.

* [19] Meng Jiang. Catching social media advertisers with strategy analysis. In _Proceedings of the First International Workshop on Computational Methods for CyberSafety_, pages 5-10, 2016.
* [20] Ming Jin, Yixin Liu, Yu Zheng, Lianhua Chi, Yuan-Fang Li, and Shirui Pan. Anemone: Graph anomaly detection with multi-scale contrastive learning. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 3122-3126, 2021.
* [21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [22] Thomas N Kipf and Max Welling. Variational graph auto-encoders. _arXiv preprint arXiv:1611.07308_, 2016.
* [23] Yufeng Kou, Chang-Tien Lu, Sirrat Sirwongwattana, and Yo-Ping Huang. Survey of fraud detection techniques. In _IEEE International Conference on Networking, Sensing and Control, 2004_, volume 2, pages 749-754. IEEE, 2004.
* [24] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. Outlier detection in axis-parallel subspaces of high dimensional data. In _Advances in Knowledge Discovery and Data Mining: 13th Pacific-Asia Conference, PAKDD 2009 Bangkok, Thailand, April 27-30, 2009 Proceedings 13_, pages 831-838. Springer, 2009.
* [25] Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in temporal interaction networks. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 1269-1278, 2019.
* [26] Jinbo Li, Hesam Izakian, Witold Pedrycz, and Iqbal Jamal. Clustering-based anomaly detection in multivariate time series data. _Applied Soft Computing_, 100:106919, 2021.
* [27] Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, and Xiyang Hu. Copod: copula-based outlier detection. In _2020 IEEE international conference on data mining (ICDM)_, pages 1118-1123. IEEE, 2020.
* [28] Zheng Li, Yue Zhao, Xiyang Hu, Nicola Botta, Cezar Ionescu, and George Chen. Ecod: Unsupervised outlier detection using empirical cumulative distribution functions. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* [29] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation forest. In _2008 eighth ieee international conference on data mining_, pages 413-422. IEEE, 2008.
* [30] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly detection on attributed networks via contrastive self-supervised learning. _IEEE transactions on neural networks and learning systems_, 33(6):2378-2392, 2021.
* [31] Xuexiong Luo, Jia Wu, Amin Beheshti, Jian Yang, Xiankun Zhang, Yuan Wang, and Shan Xue. Comga: Community-aware attributed graph anomaly detection. In _Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining_, pages 657-665, 2022.
* [32] Larry M Manevitz and Malik Yousef. One-class svms for document classification. _Journal of machine Learning research_, 2(Dec):139-154, 2001.
* [33] Goeffrey J McLachlan. Mahalanobis distance. _Resonance_, 4(6):20-26, 1999.
* [34] Emmanuel Muller, Patricia Iglesias Sanchez, Yvonne Mulle, and Klemens Bohm. Ranking outlier nodes in subspaces of attributed graphs. In _2013 IEEE 29th international conference on data engineering workshops (ICDEW)_, pages 216-222. IEEE, 2013.
* [35] Isaac Newton. _Philosophiae naturalis principia mathematica_, volume 1. G. Brookman, 1833.
* [36] David Peel and Geoffrey J McLachlan. Robust mixture modelling using the t distribution. _Statistics and computing_, 10:339-348, 2000.
* [37] Zhen Peng, Minnan Luo, Jundong Li, Luguo Xue, and Qinghua Zheng. A deep multi-view framework for anomaly detection on attributed networks. _IEEE Transactions on Knowledge and Data Engineering_, 34(6):2539-2552, 2020.

* [38] Leif E Peterson. K-nearest neighbor. _Scholarpedia_, 4(2):1883, 2009.
* [39] Tomas Pevny. Loda: Lightweight on-line detector of anomalies. _Machine Learning_, 102:275-304, 2016.
* [40] Douglas A Reynolds et al. Gaussian mixture models. _Encyclopedia of biometrics_, 741(659-663), 2009.
* [41] Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Gregoire Montavon, Wojciech Samek, Marius Kloft, Thomas G Dietterich, and Klaus-Robert Muller. A unifying review of deep and shallow anomaly detection. _Proceedings of the IEEE_, 109(5):756-795, 2021.
* [42] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In _International conference on machine learning_, pages 4393-4402. PMLR, 2018.
* [43] Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimensionality reduction. In _Proceedings of the MLSDA 2014 2nd workshop on machine learning for sensory data analysis_, pages 4-11, 2014.
* [44] Osman Salem, Yaning Liu, Ahmed Mehaoua, and Raouf Boutaba. Online anomaly detection in wireless body area networks for reliable healthcare monitoring. _IEEE journal of biomedical and health informatics_, 18(5):1541-1551, 2014.
* [45] Hanyu Song, Peizhao Li, and Hongfu Liu. Deep clustering based fair outlier detection. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 1481-1489, 2021.
* [46] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly detection. In _International Conference on Machine Learning_, pages 21076-21089. PMLR, 2022.
* [47] Shantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, Mehdi Azabou, Eva L Dyer, Remi Munos, Petar Velickovic, and Michal Valko. Large-scale representation learning on graphs via bootstrapping. _arXiv preprint arXiv:2102.06514_, 2021.
* [48] Laurens Van Der Maaten. Learning a parametric embedding by preserving local structure. In _Artificial intelligence and statistics_, pages 384-391. PMLR, 2009.
* [49] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. _Chemometrics and intelligent laboratory systems_, 2(1-3):37-52, 1987.
* [50] Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. In _International conference on machine learning_, pages 478-487. PMLR, 2016.
* [51] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for anomaly detection. _IEEE Transactions on Knowledge and Data Engineering_, 2023.
* [52] Xiaowei Xu, Nurcan Yuruk, Zhidan Feng, and Thomas AJ Schweiger. Scan: a structural clustering algorithm for networks. In _Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 824-833, 2007.
* [53] Zhiming Xu, Xiao Huang, Yue Zhao, Yushun Dong, and Jundong Li. Contrastive attributed network anomaly detection with data augmentation. In _Advances in Knowledge Discovery and Data Mining: 26th Pacific-Asia Conference, PAKDD 2022, Chengdu, China, May 16-19, 2022, Proceedings, Part II_, pages 444-457. Springer, 2022.
* [54] Xu Yuan, Na Zhou, Shuo Yu, Huafei Huang, Zhikui Chen, and Feng Xia. Higher-order structure based anomaly detection on attributed networks. In _2021 IEEE International Conference on Big Data (Big Data)_, pages 2691-2700. IEEE, 2021.
* [55] Yu Zheng, Ming Jin, Yixin Liu, Lianhua Chi, Khoa T Phan, and Yi-Ping Phoebe Chen. Generative and contrastive self-supervised learning for graph anomaly detection. _IEEE Transactions on Knowledge and Data Engineering_, 2021.

* Zhou et al. [2022] Shuang Zhou, Xiao Huang, Ninghao Liu, Qiaoyu Tan, and Fu-Lai Chung. Unseen anomaly detection on networks via multi-hypersphere learning. In _Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)_, pages 262-270. SIAM, 2022.
* Zhou et al. [2021] Shuang Zhou, Qiaoyu Tan, Zhiming Xu, Xiao Huang, and Fu-lai Chung. Subtractive aggregation for attributed network anomaly detection. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 3672-3676, 2021.
* Zong et al. [2018] Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In _International conference on learning representations_, 2018.

## Appendix A Iterative Training Algorithm

The pseudocode for training the model is presented in Algorithm 1. Initially, all parameters undergo random initialization. In subsequent iterations, following the initial round, the outlier set \(L\) undergoes updates based on the anomaly score \(o_{i}\). This is succeeded by the adjustment of the network parameters \(\Theta\) based on \(\mathbf{x}_{i}\), further optimizing the performance of \(\Theta\) through the utilization of the estimated parameters \(\boldsymbol{\mu}_{k},\omega_{k},\Sigma_{k}\). The essence of the algorithm is embedded in its alternating optimization strategy, iteratively refining the accuracy of representation learning and mixed model parameter estimation, thereby augmenting the overall training effectiveness of the model.

```
0: data points \(\mathbf{X}\), cluster number \(K\), outlier ratio \(l\), tolerance \(\lambda\), iterations \(t\)
0: network parameters \(\Theta\), mixture parameters \(\{\omega_{k},\boldsymbol{\mu}_{k},\Sigma_{k}\}\)
1: Initialize \(\Theta\) and \(\{\boldsymbol{\mu}_{k},\omega_{k},\Sigma_{k}\}\);
2:for\(i=1\) to \(t\)do
3:if\(i=1\)then
4:\(\mathbf{X}_{i}\leftarrow\mathbf{X}\);
5:else
6: Re-order the point in \(\mathbf{X}\) such that \(o_{1}\geq\cdots\geq o_{n}\);
7:\(L_{i}\leftarrow\{x_{1},\ldots,x_{\lfloor N\rfloor l\rfloor}\}\);
8:\(\mathbf{X}_{i}\leftarrow\mathbf{X}\cup L_{i}\);
9:endif
10: Update \(\Theta\) with Equation (15);
11:while\(|J(\Theta,\Phi)-J^{old}(\Theta,\Phi)|>\lambda\)do
12:\(J^{old}(\Theta,\Phi)=J(\Theta,\Phi)\);
13: Calculate \(\boldsymbol{\tau}\) with Equation (10);
14: Update \(\{\omega_{k},\boldsymbol{\mu}_{k},\Sigma_{k}\}\) with Equation (12), (13) and (14);
15:endwhile
16: Calculate \(o_{i}\) with Equation (9);
17:endfor
18:return\(\Theta\) and \(\{\omega_{k},\boldsymbol{\mu}_{k},\Sigma_{k}\}\) ```

**Algorithm 1** Model training for UniCAD

## Appendix B Derivation of EM Algorithm

This appendix provides the detailed derivation of the Expectation-Maximization (EM) algorithm for optimizing the parameters of a mixture model based on Student's t-distribution. The focus is on deriving analytical solutions for the maximization of the parameters \(\Phi=\{\boldsymbol{\mu}_{k},\Sigma_{k},\omega_{k}\}\) of the mixture components. The EM algorithm alternates between two steps:

**In the E-step**, we calculate the posterior probabilities \(\boldsymbol{\tau}_{ik}\), representing the probability of data point \(i\) belonging to cluster \(k\), given the current parameters. The posterior probabilities for a Student's t-distribution mixture model are formulated as:

\[\boldsymbol{\tau}_{ik}=\frac{\omega_{k}\cdot p(\mathbf{z}_{i}|\boldsymbol{\mu }_{k},\Sigma_{k})}{\sum_{j=1}^{K}\omega_{j}\cdot p(\mathbf{z}_{i}|\boldsymbol{ \mu}_{j},\Sigma_{j})},\] (16)

where \(\boldsymbol{\tau}(\mathbf{z}_{i}|\boldsymbol{\mu}_{k},\Sigma_{k})\) denotes the Student's t-distribution for data point \(i\) with respect to cluster \(k\), and \(K\) is the number of mixture components.

The Student's t-distribution is depicted as a hierarchical conditional probability, resembling a Gaussian distribution with an accuracy scale factor \(\mathbf{u}\), where its latent variable follows a gamma distribution. Adopting a degree of freedom \(\nu=1\), the value of \(\mathbf{u}_{ik}\) is given by:

\[\mathbf{u}_{ik}=\frac{\nu+1}{\nu+D_{M}(z_{i},\boldsymbol{\mu}_{k})}=\frac{2}{ 1+D_{M}(z_{i},\boldsymbol{\mu}_{k})}\] (17)

**In the M-step**, we update the parameters \(\Phi=\{\omega_{k},\boldsymbol{\mu}_{k},\) and \(\Sigma_{k}\}\) using the derivatives obtained in the previous steps. In our model, the likelihood function for a Student's-t Distribution Mixture Model(SMM) is represented as:

\[L(\omega,\bm{\mu},\Sigma)=\sum_{i=1}^{N}\sum_{k=1}^{K}\omega_{k}\cdot\frac{\pi^{- 1}\cdot|\Sigma_{k}|^{-\frac{1}{2}}}{1+(\mathbf{z}_{i}-\bm{\mu}_{k})^{T}\Sigma_{ k}^{-1}(\mathbf{z}_{i}-\bm{\mu}_{k})},\] (18)

where \(\omega_{k}\) are the mixture weights, \(\Sigma_{k}\) the covariance matrices, \(\bm{\mu}_{k}\) the means, and \(\mathbf{z}_{i}\) the data points. The derivative with respect to \(\omega_{k}\) must consider the constraint that the sum of the mixture weights equals 1, i.e., \(\sum_{k}\omega_{k}=1\). Hence, we introduce a Lagrange multiplier \(\lambda\) to address this constraint and construct the Lagrangian \(L^{\prime}\):

\[L^{\prime}(\omega,\bm{\mu},\Sigma,\lambda)=L(\omega,\bm{\mu},\Sigma)+\lambda \left(1-\sum_{k=1}^{K}\omega_{k}\right),\] (19)

The derivative with respect to \(\omega_{k}\) is:

\[\frac{\partial L^{\prime}}{\partial\omega_{k}}=\frac{\partial L}{\partial \omega_{k}}-\lambda,\] (20)

Substituting the definition of \(L(\omega,\bm{\mu},\Sigma)\), we obtain:

\[\frac{\partial L}{\partial\omega_{k}}=\sum_{i}\frac{p(\mathbf{z}_{i}|\bm{\mu} _{k},\Sigma_{k})}{\sum_{j=1}^{K}\omega_{j}\cdot p(\mathbf{z}_{i}|\bm{\mu}_{j}, \Sigma_{j})}=\sum_{i}\frac{\bm{\tau}_{ik}}{\omega_{k}},\] (21)

To solve for \(\omega_{k}\), we first multiply both sides of the equation by \(\omega_{k}\) and apply the constraint condition:

\[\sum_{k}\omega_{k}\left(\sum_{i}\frac{\bm{\tau}_{ik}}{\omega_{k}}-\lambda \right)=0,\] (22)

Upon further organization, we find that the Lagrange multiplier \(\lambda\) actually equals the total number of data points \(N\) (since \(\sum_{i}\bm{\tau}_{ik}=N_{k}\), where \(N_{k}\) is the expected total number of data points belonging to the \(k\)th component, and the sum of all \(N_{k}\) equals the total number of data points \(N\)).

Finally, we can solve for \(\omega_{k}\):

\[\omega_{k}=\frac{\sum_{i}\bm{\tau}_{ik}}{N},\] (23)

This result indicates that the weight \(\omega_{k}\) of each mixture component equals the proportion of the posterior probabilities of the data points it contains relative to all data points.

To update \(\bm{\mu}_{k}\) and \(\Sigma_{k}\), we consider the conditional expectation of the data log-likelihood function:

\[\begin{split} Q(\bm{\mu}_{k},\Sigma_{k})=&\sum_{i= 1}^{N}\bm{\tau}_{ik}\left(-\log(\pi)-\frac{1}{2}\log|\sigma_{k}|+\frac{1}{2} \log u_{ik}\right.\\ &\left.-\frac{1}{2}\mathbf{u}_{ik}(\mathbf{z}_{i}-\bm{\mu}_{k})^ {T}\Sigma_{k}^{-1}(\mathbf{z}_{i}-\bm{\mu}_{k})\right)\end{split}\] (24)

Figure 3: Score comparison with other methods.

Maximizing \(Q(\bm{\mu}_{k},\Sigma_{k})\) with respect to \(\bm{\mu}_{k}\) leads to:

\[\frac{\partial Q}{\partial\bm{\mu}_{k}}=\frac{1}{2}\sum_{i=1}^{N}\bm{\tau}_{ik} \mathbf{u}_{ik}(2\Sigma_{k}^{-1}\bm{\mu}_{k}-2\Sigma_{k}^{-1}\mathbf{z}_{ik})\] (25)

Setting \(\frac{\partial Q}{\partial\bm{\mu}_{k}}=0\) results in the updated mean \(\bm{\mu}_{k}^{(t+1)}\):

\[\bm{\mu}_{k}^{(t+1)}=\sum_{i=1}^{n}\left(\bm{\tau}_{ik}^{(t+1)} \mathbf{u}_{ik}^{(t+1)}\mathbf{z}_{i}\right)/\sum_{i=1}^{n}\left(\bm{\tau}_{ik }^{(t+1)}\mathbf{u}_{ik}^{(t+1)}\right).\] (26)

Considering the derivative of \(Q(\bm{\mu}_{k},\Sigma_{k})\) with respect to \(\Sigma_{k}^{-1}\):

\[\frac{\partial Q}{\partial\Sigma_{k}^{-1}}=\frac{1}{2}\sum_{i=1}^{N}\bm{\tau} _{ik}\left(\Sigma_{k}-\mathbf{u}_{ik}(\mathbf{z}_{i}-\bm{\mu}_{k})\times( \mathbf{z}_{i}-\bm{\mu}_{k})^{T}\right).\] (27)

Setting \(\frac{\partial Q}{\partial\bm{\mu}_{k}}=0\) yields the updated covariance matrix \(\bm{\Sigma}_{k}^{(t+1)}\):

\[\bm{\Sigma}_{k}^{(t+1)}=\frac{\sum_{i=1}^{n}\bm{\tau}_{ik}^{(t+1)} \mathbf{u}_{ik}^{(t+1)}(\mathbf{z}_{i}-\bm{\mu}_{k}^{(t+1)})(\mathbf{z}_{i}- \bm{\mu}_{k}^{(t+1)})^{T}}{\sum_{j=1}^{K}\bm{\tau}_{ij}^{(t+1)}}.\] (28)

## Appendix C Anomaly Score with Vector Sum

### Advantages

Here we discuss the advantages of employing vector sum in anomaly score with a toy example.

The application of the vector sum principle extends beyond physical mechanics and finds relevance in various domains. In relational embedding [5], for example, relationships can be represented as vectors. Aggregating these vectors allows for capturing complexities like transitivity, symmetry, and antisymmetry.

Similarly, in our context, the vector sum can help capture more complex relationships along clusters. Consider Figure 4 as an example, where a sample \(v\) is attracted by two groups of cluster prototypes \((\{\bm{\mu}_{1},\bm{\mu}_{2}\},\ \{\bm{\mu}_{3},\bm{\mu}_{4}\})\) with the same mass and sample-prototype distances (\(\widetilde{m}_{1}=\widetilde{m}_{2}=\widetilde{m}_{3}=\widetilde{m}_{4}\), \(\widetilde{r}_{v1}=\widetilde{r}_{v2}=\widetilde{r}_{v3}=\widetilde{r}_{v4}\)). Without considering the direction of the forces, the two groups of prototypes would attract the sample with equal forces. However, we argue that the two groups of prototypes should exert different influences. A sample close to two clusters with a large difference (\(\{\bm{\mu}_{1},\bm{\mu}_{2}\}\)) is more likely to be an anomaly compared to a sample that is close to two clusters with a smaller difference (\(\{\bm{\mu}_{3},\bm{\mu}_{4}\}\)). For example, in a social network, a user who equally likes two extremely different communities, like money-saving tips and luxury items, is more anomalous than a user who equally likes two similar communities, like private jets and luxury items. Applying the vector sum, the total force of \(\{\bm{\mu}_{1},\bm{\mu}_{2}\}\) is much smaller than that of \(\{\bm{\mu}_{3},\bm{\mu}_{4}\}\). As the anomaly score is inversely related to the total force, it is more anomalous when equally attracted by \(\{\bm{\mu}_{1},\bm{\mu}_{2}\}\) with large difference. This indicates that _the vector sum successfully captures subtle differences in the distinctions among multiple clusters, thereby assisting in the identification of more accurate anomalies_.

Figure 4: Analysis of gravitational force.

### Toy Example

In the appendix, as illustrated in Figure 3, we investigated a toy example. We discussed a specific pattern of anomalies termed _group anomalies_, where a small number of anomalous samples cluster together. It is crucial to note that we do not claim this anomaly pattern is common in real-world data; our goal is merely to point out a specific anomaly pattern that is challenging for traditional cluster-based anomaly detection methods to detect. Specifically, we utilize three Gaussian distributions with high variance (each generating 300 data samples) and one with lower variance (generating 30 data samples). Because the samples from the smaller Gaussian follow a different generative mechanism and represent a minority in the dataset, we consider them anomalies.

We set the cluster number for KMeans-- and GMM at four, indicating that the Gaussian distribution comprising anomalous samples was also recognized as a cluster. KMeans- employs a cluster-based approach, using the distance to the nearest cluster center as the anomaly score, while GMM uses a probability-based approach, considering the samples' likelihood in the mixture model as the anomaly score. However, both approaches are ineffective in this scenario. Rather than identifying the small cluster as anomalous, they tend to misidentify samples on the peripheries of larger clusters as anomalies.

By contrast, our scoring method views the entire small cluster as more likely anomalous, followed by outlier samples on the margins of the larger clusters. This visualization provides a perspective that distinguishes our method from previous efforts.

## Appendix D Experimental Supplementary

### Benchmark Datasets Details

Due to space constraints in the main text, we utilized 30 public datasets from ADBench [17], covering all different types of data. The details of the 30 datasets are presented in Table 4.

### Baselines Details

A comprehensive overview of the unsupervised anomaly detection methods is presented below.

#### d.2.1 Traditional Models

* **Subspace Outlier Detection (SOD) [24]:** Identifies outliers in varying subspaces of a high-dimensional feature space, targeting anomalies that emerge in lower-dimensional projections.
* **Histogram-based Outlier Detection (HBOs) [16]:** Assumes feature independence and calculates outlyingness via histograms, offering scalability and efficiency.

#### d.2.2 Linear Models

* **Principal Component Analysis (PCA) [49]:** Utilizes singular value decomposition for dimensionality reduction, with anomalies indicated by reconstruction errors.
* **One-class SVM (OCSVM) [32]:** Defines a decision boundary to separate normal samples from outliers, maximizing the margin from the data origin.

#### d.2.3 Density-based Models

* **Local Outlier Factor (LOF) [6] :** Measures local density deviation, marking samples as outliers if they lie in less dense regions compared to their neighbors.
* **K-Nearest Neighbors (KNN) [38]:** Anomaly scores are assigned based on the distance to the k-th nearest neighbor, embodying a simple yet effective approach.

#### d.2.4 Ensemble-based Models

* **Lightweight On-line Detector of Anomalies (LODA) [39] :** An ensemble method suitable for real-time processing and adaptable to concept drift through random projections and histograms.
* **Isolation Forest (IForest) [29]:** Isolates anomalies by randomly selecting features and split values, leveraging the ease of isolating anomalies to identify them efficiently.

#### d.2.5 Probability-based Models

* **Deep Autoencoding Gaussian Mixture Model (DAGMM) [58]:** Combines a deep autoencoder with a GMM for anomaly scoring, utilizing both low-dimensional representation and reconstruction error.
* **Empirical-Cumulative-distribution-based Outlier Detection (ECOD) [28]:** Uses ECDFs to estimate feature densities independently, targeting outliers in distribution tails.
* **Copula Based Outlier Detector (COPOD) [27]:** A hyperparameter-free method leveraging empirical copula models for interpretable and efficient outlier detection.

#### d.2.6 Cluster-based Models

* **DBSCAN [13]:** A density-based clustering algorithm that identifies clusters based on the density of data points, effectively separating high-density clusters from low-density noise, and is widely used for anomaly detection in spatial data.
* **Clustering Based Local Outlier Factor (CBLOF) [18]:** Calculates anomaly scores based on cluster distances, using global data distribution.
* **KMeans-[45]:** Extends k-means to include outlier detection in the clustering process, offering an integrated approach to anomaly detection.
* **Deep Clustering-based Fair Outlier Detection (DCFOD) [9]:** Enhances outlier detection with a focus on fairness, combining deep clustering and adversarial training for representation learning.

\begin{table}
\begin{tabular}{r|r r r r r} \hline \hline Data & \# Samples & \# Features & \# Anomaly & \% Anomaly & Category \\ \hline annthyroid & 7200 & 6 & 534 & 7.42 & Healthcare \\ backdoor & 95329 & 196 & 2329 & 2.44 & Network \\ breastw & 683 & 9 & 239 & 34.99 & Healthcare \\ campaign & 41188 & 62 & 4640 & 11.27 & Finance \\ celeba & 202599 & 39 & 4547 & 2.24 & Image \\ census & 299285 & 500 & 18568 & 6.20 & Sociology \\ glass & 214 & 7 & 9 & 4.21 & Forensic \\ Hepatitis & 80 & 19 & 13 & 16.25 & Healthcare \\ http & 567498 & 3 & 2211 & 0.39 & Web \\ Ionosphere & 351 & 33 & 126 & 35.90 & Oryctognosy \\ landsat & 6435 & 36 & 1333 & 20.71 & Astronautics \\ Lymphography & 148 & 18 & 6 & 4.05 & Healthcare \\ magic.gamma & 19020 & 10 & 6688 & 35.16 & Physical \\ mnist & 7603 & 100 & 700 & 9.21 & Image \\ musk & 3062 & 166 & 97 & 3.17 & Chemistry \\ pendigits & 6870 & 16 & 156 & 2.27 & Image \\ Pima & 768 & 8 & 268 & 34.90 & Healthcare \\ satellite & 6435 & 36 & 2036 & 31.64 & Astronautics \\ satimage-2 & 5803 & 36 & 71 & 1.22 & Astronautics \\ shuttle & 49097 & 9 & 3511 & 7.15 & Astronautics \\ skin & 245057 & 3 & 50859 & 20.75 & Image \\ Stamps & 340 & 9 & 31 & 9.12 & Document \\ thyroid & 3772 & 6 & 93 & 2.47 & Healthcare \\ vertebral & 240 & 6 & 30 & 12.50 & Biology \\ vowels & 1456 & 12 & 50 & 3.43 & Linguistics \\ Waveform & 3443 & 21 & 100 & 2.90 & Physics \\ WBC & 223 & 9 & 10 & 4.48 & Healthcare \\ Wilt & 4819 & 5 & 257 & 5.33 & Botany \\ wine & 129 & 13 & 10 & 7.75 & Chemistry \\ WPBC & 198 & 33 & 47 & 23.74 & Healthcare \\ \hline \hline \end{tabular}
\end{table}
Table 4: Statistics of tabular benchmark datasets.

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_FAIL:20]

* **Weibo[19]** is a labeled graph comprising user posts extracted from the social media platform Tencent Weibo. The user-user graph establishes connections between users who exhibit similar topic labels. A user is considered anomalous if they have engaged in a minimum of five suspicious events, whereas normal nodes represent users who have not.
* **Reddit[25]** consists of a user-subreddit graph extracted from the popular social media platform Reddit. This publicly accessible dataset encompasses user posts within various subreddits over a month. Each user is assigned a binary label indicating whether they have been banned on the platform. Our assumption is that banned users exhibit anomalous behavior compared to regular Reddit users.
* **Disney[34]** is a co-purchase network of movies that includes attributes such as price, rating, and the number of reviews. The ground truth labels, indicating whether a movie is considered anomalous or not, were assigned by high school students through majority voting.
* **T-Finance[46]** aims to identify anomalous accounts within a trading network. The nodes in this network represent unique anonymous accounts, each characterized by ten features related to registration duration, recorded activity, and interaction frequency. Graph edges denote transaction records between accounts. If a node is associated with activities such as fraud, money laundering, or online gambling, human experts will designate it as an anomaly.

### Experiment Settings

In this experiment, we compared graph-based methods on relational data. For methods originally designed around feature vectors, including CBLOF, DCFOD, and our approach, we uniformly employed the same graph representation learning technique as described in BGRL [47]. Specifically, we used a two-layer Graph Convolutional Network (GCN) for encoding, which produced output embeddings with a dimensionality of 128. The training epochs were set to 3000, including a warm-up period of 300 epochs. The hidden size of the predictor was set to 512, and the momentum was fixed at 0.99.

\begin{table}
\begin{tabular}{r|r r r r r r} \hline \hline Dataset & \# Nodes & \# Edges & \# Features & \# Anomaly & Category \\ \hline Disney & 124 & 670 & 28 & 6 & co-purchase network \\ Weibo & 8,405 & 407,963 & 400 & 868 & social media network \\ Reddit & 10,984 & 168,016 & 64 & 366 & user-subreddit network \\ T-Finance & 39,357 & 42,445,086 & 10 & 1,803 & trading network \\ \hline \hline \end{tabular}
\end{table}
Table 7: Statistics of graph benchmark datasets.

\begin{table}
\begin{tabular}{r|r|r r r r r r r} \hline \hline \multirow{2}{*}{**Group**} & \multirow{2}{*}{**Method**} & \multicolumn{2}{c}{**Weibo**} & \multicolumn{2}{c}{**Reddit**} & \multicolumn{2}{c}{**Disney**} & \multicolumn{2}{c}{**T-Finance**} \\  & & **AUC-ROC** & **AUC-PR** & **AUC-ROC** & **AUC-PR** & **AUC-ROC** & **AUC-PR** & **AUC-ROC** & **AUC-PR** \\ \hline \multirow{4}{*}{CL-Based} & CoLA & 0.382 & 0.087 & 0.527 & 0.036 & 0.455 & 0.060 & 0.243 & 0.031 \\  & SL-GAD & 0.421 & 0.109 & **0.594** & 0.040 & 0.494 & 0.061 & 0.442 & 0.041 \\  & ANEMONE & 0.320 & 0.082 & 0.536 & 0.036 & 0.454 & 0.068 & 0.226 & 0.030 \\  & CONAD & 0.806 & 0.432 & 0.551 & 0.037 & 0.600 & 0.138 & N/A & N/A \\ \hline \multirow{4}{*}{AE-Based} & MLPAE & 0.880 & 0.629 & 0.501 & 0.035 & 0.563 & 0.064 & 0.299 & 0.030 \\  & GCNAE & 0.847 & 0.567 & 0.526 & 0.033 & 0.517 & 0.059 & 0.295 & 0.030 \\  & GUIDE & 0.897 & 0.692 & 0.566 & 0.040 & 0.521 & 0.060 & N/A & N/A \\  & DOMINANT & 0.927 & 0.797 & 0.561 & 0.037 & 0.590 & 0.077 & N/A & N/A \\  & Comda & 0.925 & 0.809 & 0.568 & 0.037 & 0.494 & 0.058 & N/A & N/A \\  & AnomalyDAE & 0.892 & 0.694 & 0.560 & 0.037 & 0.520 & 0.070 & N/A & N/A \\  & ALARM & 0.952 & 0.843 & 0.559 & 0.037 & 0.595 & 0.123 & N/A & N/A \\  & DONE & 0.856 & 0.579 & 0.551 & 0.037 & 0.517 & 0.061 & 0.550 & 0.046 \\  & AAGNN & 0.804 & 0.530 & 0.564 & **0.045** & 0.479 & 0.059 & N/A & N/A \\ \hline \multirow{4}{*}{Cluster-Based} & SCAN & 0.701 & 0.186 & 0.496 & 0.033 & 0.548 & 0.053 & N/A & N/A \\  & CBLOF* & 0.972 & 0.875 & 0.503 & 0.035 & 0.574 & **0.146** & 0.524 & 0.046 \\ \cline{1-1}  & DCFOD* & 0.684 & 0.196 & 0.552 & 0.038 & 0.675 & 0.119 & 0.521 & 0.066 \\ \cline{1-1}  & UniCAD * & **0.985** & **0.927** & 0.560 & 0.040 & **0.701** & 0.130 & **0.876** & **0.422** \\ \hline \hline \end{tabular}
\end{table}
Table 8: AUC-ROC and AUC-PR of 16 unsupervised algorithms on 4 graph benchmark datasets.

### Performance Analysis

The performance of UniCAD compared to 16 baseline methods on the four datasets are summarized in Table 8. From the results, we have the following observations: Our model consistently outperforms the baseline methods on most datasets, underlining its effectiveness in anomaly detection even within graph data contexts. This highlights the superiority of UniCAD in detecting anomalies in real-world graph data.

When comparing UniCAD with the four contrastive learning-based methods, it exhibits a distinct advantage, outperforming them by a substantial margin across all metrics. Unlike contrastive learning methods that rely on the local neighborhood for anomaly detection, UniCAD leverages the global clustering distribution. This key difference contributes to its consistently superior performance. Although CONAD incorporates human prior knowledge about anomalies, enabling it to outperform other similar methods on the Weibo and Disney datasets, it still falls short compared to our proposed UniCAD.

Compared to the autoencoder-based methods, UniCAD offers the advantage of lower memory requirements along with better performance. Graph autoencoders typically reconstruct the entire adjacency matrix during full graph training, resulting in memory usage of at least \(\mathcal{O}(N^{2})\). In contrast, UniCAD, as a clustering-based method, only requires \(\mathcal{O}(N\times K)\). Among the autoencoder-based methods, GCNAE, DONE, and AdONE can be extended to the T-Finance dataset as they only reconstruct the sampled subgraphs rather than the entire adjacency matrix. However, UniCAD still showcases superior performance while being more memory-efficient.

UniCAD also demonstrates superior performance compared to various other clustering-based methods, including traditional structural clustering (SCAN) methods that treat the embedding from BGRL as tabular data (CBLOF, DCFOD).

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes]" is generally preferable to "[No]", it is perfectly acceptable to answer "[No]" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No]" or "[NA]" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims presented in the abstract and introduction are consistent with the paper's contributions and accurately outline the scope. Guidelines:

* The answer NA means that the abstract and introduction do not include the claims made in the paper.
* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: The limitations of the method's application scope are discussed in Section 5 of the paper, along with considerations for future work. Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The full set of assumptions and complete proofs for each theoretical result are provided and can be found in the appendix, specifically in Section B, ensuring that the theoretical framework is transparent and verifiable. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]Justification: All necessary information for reproducing the main experimental results, including dataset links, baseline comparisons, and the methodologies of our proposed approach, are comprehensively included within the submission files, facilitating transparency and reproducibility of the research findings.

Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper ensures open access to both the data and code necessary for reproducing the main experimental results, complemented by detailed instructions in the supplemental material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.

* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Detailed information regarding the training and test setups, including data splits, hyperparameters and their selection process, the type of optimizer used, and other relevant details, are thoroughly documented in Section 4.2 of the paper. Guidelines:
7. The answer NA means that the paper does not include experiments.
8. The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
9. The full details can be provided either with the code, in appendix, or as supplemental material.
10. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper reports the statistical significance of the experiments by detailing the results of the Friedman test and the Nemenyi test in Appendix D.3. Guidelines:
11. The answer NA means that the paper does not include experiments.
12. The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
13. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
14. The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
15. The assumptions made should be given (e.g., Normally distributed errors).
16. It should be clear whether the error bar is the standard deviation or the standard error of the mean.
17. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
18. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
19. If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
20. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?

Answer: [Yes]

Justification: Detailed information on the compute resources, including the type of compute workers (CPU/GPU), memory, and execution time for each experiment, is provided in the supplementary materials, enabling accurate reproduction of the experiments.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research adheres to the NeurIPS Code of Ethics, including considerations for anonymity, fairness, and transparency, with no deviations reported or necessary under current laws or regulations. Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper thoroughly discusses both the potential positive impacts, such as enhancements in anomaly detection for critical applications. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper appropriately credits the creators of the utilized assets, including code, data, and models, and explicitly mentions the licenses and terms of use, ensuring compliance with the original terms set by the asset owners. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]Justification: The paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.