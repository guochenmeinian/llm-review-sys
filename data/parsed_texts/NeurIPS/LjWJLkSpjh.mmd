# When Can We Track Significant Preference Shifts in Dueling Bandits?

 Joe Suk

Columbia University

joe.suk@columbia.edu

&Arpit Agarwal

Columbia University

aa4931@columbia.edu

###### Abstract

The \(K\)-armed dueling bandits problem, where the feedback is in the form of noisy pairwise preferences, has been widely studied due its applications in information retrieval, recommendation systems, etc. Motivated by concerns that user preferences/tastes can evolve over time, we consider the problem of _dueling bandits with distribution shifts_. Specifically, we study the recent notion of _significant shifts_(Suk and Kpotufe, 2022), and ask whether one can design an _adaptive_ algorithm for the dueling problem with \(O(\sqrt{K\tilde{L}T})\) dynamic regret, where \(\tilde{L}\) is the (unknown) number of significant shifts in preferences. We show that the answer to this question depends on the properties of underlying preference distributions. Firstly, we give an impossibility result that rules out any algorithm with \(O(\sqrt{K\tilde{L}T})\) dynamic regret under the well-studied Condorcet and SST classes of preference distributions. Secondly, we show that SST\(\cap\)STI is the largest amongst popular classes of preference distributions where it is possible to design such an algorithm. Overall, our results provides an almost complete resolution of the above question for the hierarchy of distribution classes.

## 1 Introduction

The \(K\)-armed dueling bandits problem has been well-studied in the multi-armed bandits literature (Yue and Joachims, 2011; Yue et al., 2012; Urvoy et al., 2013; Ailon et al., 2014; Zoghi et al., 2014, 2015a, 2015b; Dudik et al., 2015; Jamieson et al., 2015; Komiyama et al., 2015, 2016; Ramamohan et al., 2016; Chen and Frazier, 2017; Saha and Gaillard, 2022; Agarwal et al., 2022). In this problem, on each trial \(t\in[T]\), the learner pulls a _pair_ of arms and observes _relative feedback_ between these arms indicating which arm was preferred. The feedback is typically stochastic, drawn according to a pairwise preference matrix \(\textbf{P}\in[0,1]^{K\times K}\), and the regret measures the'sub-optimality' of arms with respect to a 'best' arm.

This problem has many applications, e.g. information retrieval, recommendation systems, etc, where relative feedback between arms is easy to elicit, while real-valued feedback is difficult to obtain or interpret. For example, a central task for information retrieval algorithms is to output a ranked list of documents in response to a query. The framework of online learning has been very useful for automatic parameter tuning, i.e. finding the best parameter(s), for such retrieval algorithms based on user feedback (Liu, 2009). However, it is often difficult to get numerical feedback for an individual list of documents. Instead, one can (implicitly) compare two lists of documents by interleaving them and observing the relative number of clicks (Radlinski et al., 2008). The availability of these pairwise comparisons allows one to tune the parameters of retrieval algorithms in real-time using the framework of dueling bandits.

However, in many such applications that rely on user generated preference feedback, there are practical concerns that the tastes/beliefs of users can change over time, resulting in a dynamicallychanging preference distribution. Motivated by these concerns, we consider the problem of _switching dueling bandits_ (or non-stationary dueling bandits), where the pairwise preference matrix \(\textbf{P}_{t}\) changes an unknown number of times over \(T\) rounds. The performance of the learner is evaluated using _dynamic regret_ where sub-optimality of arms is calculated with respect to the current 'best' arm.

Saha and Gupta (2022) first studied this problem and provided an algorithm that achieves a nearly optimal (up to \(\log\) terms) _dynamic regret_ of \(\tilde{O}(\sqrt{KLT})\) where \(L\) is the total number of _shifts_ in the preference matrix, i.e., the number of times \(\textbf{P}_{t}\) differs from \(\textbf{P}_{t+1}\). However, this result requires algorithm knowledge of \(L\). Alternatively, the algorithm of Saha and Gupta (2022) can be tuned to achieve a dynamic regret rate (also nearly optimal) \(\tilde{O}(V_{T}^{1/3}K^{1/3}T^{2/3})\) in terms of the total-variation of change in preferences \(V_{T}\) over \(T\) total rounds. This is similarly limited by requiring knowledge of \(V_{T}\).

On the other hand, recent works on the switching MAB problem show it is not only possible to design _adaptive_ algorithms with \(\tilde{O}(\sqrt{KLT})\) dynamic regret without knowledge of the underlying environment (Auer et al., 2019), but also possible to achieve a much better bound of \(\tilde{O}(\sqrt{KLT})\) where \(\tilde{L}\ll L\) is the number of _significant shifts_(Suk and Kpotufe, 2022). Specifically, a shift is significant when there is no'safe' arm left to play, i.e., every arm has, on some interval \([s_{1},s_{2}]\), regret order \(\Omega(\sqrt{s_{2}-s_{1}})\). Such a weaker measure of non-stationarity is appealing as it captures the changes in best-arm which are most severe, and allows for more optimistic regret rates over the previously known \(\sqrt{KLT}\wedge(V_{T}K)^{1/3}T^{2/3}\).

Very recently, Buening and Saha (2022) considered an analogous notion of significant shifts for switching dueling bandits under the SST\(\cap\)STI1 assumption. They gave an algorithm that achieves a dynamic regret of \(\tilde{O}(K\sqrt{LT})\), where \(\tilde{L}\) is the (unknown) number of _significant shifts_. However, their algorithm estimates \(\Omega(K^{2})\) pairwise preferences, and hence, suffers from a sub-optimal dependence on \(K\).

Footnote 1: SST\(\cap\)STI imposes a linear ordering over arms and two well-known conditions on the preference matrices: strong stochastic transitivity (SST) and stochastic triangle inequality (STI).

In this paper we consider the goal of designing _optimal_ algorithms for switching dueling bandits whose regret depends on the number of _significant_ shifts \(\tilde{L}\). We ask the following question:

**Question**.: _Is it possible to achieve a dynamic regret of \(O(\sqrt{KLT})\) without knowledge of \(\tilde{L}\)?_

We show that the answer to this question depends on conditions on the preference matrices. Specifically, we consider several well-studied conditions from the dueling bandits literature, and give an almost complete resolution of the achievability of \(O(\sqrt{KLT})\) dynamic regret under these conditions.

### Our Contributions

We first consider the classical _Condorcet winner_ (CW) condition where, at each time \(t\in[T]\), there is a 'best' arm under the preference \(\textbf{P}_{t}\) that stochastically beats every other arm. Such a winner arm is a benchmark in defining the aforementioned dueling _dynamic regret_. Our first result shows that, even under the CW condition, it is in general impossible to achieve \(O(\sqrt{KLT})\) dynamic regret.

**Theorem 1**.: _(Informal) There is a family of instances \(\mathcal{F}\) under Condorcet where all shifts are non-significant, i.e. \(\tilde{L}=0\), but no algorithm can achieve \(o(T)\) dynamic regret uniformly over \(\mathcal{F}\)._

Note that in the case when \(\tilde{L}=0\), one would ideally like to achieve a dynamic regret of \(O(\sqrt{KT})\). The above theorem shows that, under the Condorcet condition when \(\tilde{L}=0\), not only is it impossible

Figure 1: The hierarchy of distribution classes. The dark region is where \(O(\sqrt{KLT})\) dynamic regret is not achievable, whereas the light region indicates achievablility (e.g., by our Algorithm 1).

to achieve \(O(\sqrt{KT})\) regret, it is even impossible to achieve \(O(T^{\alpha})\) regret for any \(\alpha<1\). Hence, this rules out the possibility of an algorithm whose regret under this condition is sublinear in \(\tilde{L}\) and \(T\).

The proof of the above theorem relies on a careful construction where, at each time \(t\), the preference \(\mathbf{P}_{t}\) is chosen uniformly at random from two different matrices \(\mathbf{P}^{+}\) and \(\mathbf{P}^{-}\). These matrices have different 'best' arms but there is a unique _safe arm_ in both. However, it is impossible to identify this safe arm as all observed pairwise preferences are \(\mathrm{Ber}(\frac{1}{2})\) over the randomness of the environment. Moreover, the theorem gives two different constructions (one ruling out SST and one STI) which together rule out all preference classes outside of SST\(\cap\)STI. Our second result shows that the desired regret \(\sqrt{K\tilde{L}T}\) is in fact achievable (adaptively) under SST\(\cap\)STI.

**Theorem 2**.: _(Informal) There is an algorithm that achieves a dynamic regret of \(\tilde{O}(\sqrt{K\tilde{L}T})\) under SST\(\cap\)STI without requiring knowledge of \(\tilde{L}\)._

Figure 1 gives a summary of our results. Note that in stationary dueling bandits there is no separation in the regret achievable under the CW vs. SST\(\cap\)STI conditions, i.e. \(O(\sqrt{KT})\) is the minimax optimal regret rate under both conditions (Saha and Gaillard, 2022). However, our results show that in the non-stationary setting with regret in terms of significant shifts, there is a separation in adaptively achievable regret.

Key Challenge and Novelty in Regret Upper Bound:To contrast, the recent work of Buening and Saha (2022) only attains \(\tilde{O}(K\sqrt{\tilde{L}T})\) dynamic regret under SST\(\cap\)STI due to inefficient exploration of arm pairs. Our more challenging goal of obtaining the optimal dependence on \(K\) introduces key difficulties in algorithmic design. In fact, even in the classical stochastic dueling bandit problem with SST\(\cap\)STI, most existing results that achieve \(O(\sqrt{KT})\) regret require identifying a coarse ranking over arms to avoid suboptimal exploration of low ranked arms (Yue et al., 2012; Yue and Joachims, 2011; Yue et al., 2012). However, in the non-stationary setting, ranking the arms meaningfully is difficult as the true ordering of arms may change (insignificantly) at all rounds. Our main algorithmic innovation is to bypass the task of ranking arms and instead directly focus on minimizing the cumulative regret of played arms. This entails a new rule for selecting "candidate" arms based on cumulative regret that may be of independent interest.

### Related Work

Dueling bandits.The stochastic dueling bandits problem and its variants have been studied widely (see Sui et al. (2018) for a comprehensive survey). This problem was first proposed by Yue et al. (2012), who provide an algorithm achieving instance-dependent \(O(K\log T)\) regret under the SST\(\cap\)STI condition. Yue and Joachims (2011) also studied this problem under the SST\(\cap\)STI condition and gave an algorithm that achieves optimal instance-dependent regret. Urvoy et al. (2013) studied this problem under the Condorcet winner condition and achieved an instance-dependent \(O(K^{2}\log T)\) regret bound, which was further improved by Zoghi et al. (2014) and Komiyama et al. (2015) to \(O(K^{2}+K\log T)\). Finally, Saha and Gaillard (2022) showed that it is possible to achieve an optimal instance-dependent bound of \(O(K\log T)\) and instance-independent bound of \(O(\sqrt{KT})\) under the Condorcet condition. More general notions of winners such as Borda winner (Jamieson et al., 2015), Copeland winner (Zoghi et al., 2015; Komiyama et al., 2016; Wu and Liu, 2016), and von Nuemann winner (Dudik et al., 2015) have also been considered. However, these works only consider the stationary setting whereas we consider the non-stationary setting.

There has also been work on adversarial dueling bandits (Saha et al., 2021; Gajane et al., 2015), however, these works only consider static regret against the 'best' arm in hindsight and whereas we consider the harder dynamic regret. Other than the two previously mentioned works (Gupta and Saha, 2022; Buening and Saha, 2022), the only other work on switching dueling bandits is Kolpaczki et al. (2022), whose procedures require knowledge of non-stationarity and only consider the weaker measure of non-stationarity \(L\) counting all changes in the preferences.

Non-stationary multi-armed bandits.Multi-armed bandits with changing rewards was first considered in the adversarial setup by Auer et al. (2002), where a version of EXP3 was shown to attain optimal dynamic regret \(\sqrt{KLT}\) when properly tuned using the number \(L\) of changes in the rewards. Later works established similar (non-adaptive) guarantees in this so-called _switching bandit_ problem via procedures inspired by stochastic bandit algorithms (Garivier and Moulines, 2011; Kocsisand Szepesvari, 2006]. More recent works [Auer et al., 2018, 2019, Chen et al., 2019] established the first adaptive and optimal dynamic regret guarantees, without requiring knowledge of the number of changes. An alternative parametrization of switching bandits, via a total-variation quantity, was introduced in Besbes et al. [2014] with minimax rates quantified therein and adaptive rates attained in Chen et al. [2019]. Yet another characterization, in terms of the number of best arm switches \(S\) was studied in Abbasi-Yadkori et al. [2022], establishing an adaptive regret rate of \(\sqrt{SKT}\). Around the same time, Suk and Kpotufe [2022] introduced the aforementioned notion of _significant shifts_ and adaptively achieved rates of the form \(\sqrt{K\tilde{L}T}\) in terms of \(\tilde{L}\) significant shifts in rewards.

## 2 Problem Formulation

We consider non-stationary dueling bandits with \(K\) arms and time-horizon \(T\). At round \(t\in[T]\), the pairwise preference matrix is denoted by \(\textbf{P}_{t}\in[0,1]^{K\times K}\), where the \((i,j)\)-th entry \(P_{t}(i,j)\) encodes the likelihood of observing a preference for arm \(i\) in a direct comparison with arm \(j\). The preference matrix may change arbitrarily from round to round. At round \(t\), the learner selects a pair of actions \((i_{t},j_{t})\in[K]\times[K]\) and observes the feedback \(O_{t}(i_{t},j_{t})\sim\mathrm{Ber}(P_{t}(i_{t},j_{t}))\) where \(P_{t}(i_{t},j_{t})\) is the underlying preference of arm \(i_{t}\) over \(j_{t}\). We define the pairwise gaps \(\delta_{t}(i,j):=P_{t}(i,j)-1/2\).

**Conditions on Preference Matrix.** We consider two different conditions on preference matrices: (1) the Condorcet winner (CW) condition and (2) the strong stochastic transitivity (SST) and stochastic triangle inequality (STI), formalized below.

**Definition 1**.: _(CW condition) At each round \(t\), there is a_ **Condorcet winner** _arm, denoted by \(a_{t}^{*}\), such that \(\delta_{t}(a_{t}^{*},a)\geq 0\) for all \(a\in[K]\backslash\{a_{t}^{*}\}\). Note that \(a_{t}^{*}\) need not be unique._

**Definition 2**.: _(SST\(\cap\)STI condition) At each round \(t\), there exists a total ordering on arms, denoted by \(\succ_{t}\), and \(\forall i\succeq_{t}j\succeq_{t}k\):_

* \(\delta_{t}(i,k)\geq\max\{\delta_{t}(i,j),\delta_{t}(j,k)\}\) _(SST)._
* \(\delta_{t}(i,k)\leq\delta_{t}(i,j)+\delta_{t}(i,k)\) _(STI)._

It's easy to see that the SST condition implies the CW condition as \(\delta_{t}(i,j)\geq\delta_{t}(i,i)=0\) for any \(i\succ_{t}j\). Hence, the highest ranked item under \(\succ_{t}\) in Definition 2 is the CW \(a_{t}^{*}\). We emphasize here that the CW in Definition 1 and the total ordering on arms in Definition 2 can change at each round, even while such unknown changes in preference may not be counted as significant (see below).

**Regret Notion.** Our benchmark is the _dynamic regret_ to the sequence of Condorcet winner arms:

\[\text{DR}(T):=\sum_{t=1}^{T}\frac{\delta_{t}(a_{t}^{*},i_{t})+\delta_{t}(a_{t }^{*},j_{t})}{2}.\]

Here, the regret of an arm \(i\) is defined in terms of the preference gap \(\delta_{t}(a_{t}^{*},i)\) between the winner arm \(a_{t}^{*}\) and \(i\), and the regret of the pair \((i_{t},j_{t})\) is the average regret of individual arms \(i_{t}\) and \(j_{t}\). Note the this regret is well-defined under both Condorcet and SST\(\cap\)STI conditions due to the existence of a unique 'best' arm \(a_{t}^{*}\), and is non-negative due to the fact that \(\delta_{t}(a_{t}^{*},i)\geq 0\) for all \(i\in[K]\).

**Measure of Non-Stationarity.** We first recall the notion of Significant Condorcet Winner Switches from Buening and Saha [2022], which captures only the switches in \(a_{t}^{*}\) which are severe for regret. Throughout the paper, we'll also refer to these as _significant shifts_ for brevity.

**Definition 3** (Significant CW Switches).: _Define an arm \(a\) as having_ **significant regret** _over \([s_{1},s_{2}]\) if_

\[\sum_{s=s_{1}}^{s_{2}}\delta_{s}(a_{s}^{*},a)\geq\sqrt{K\cdot(s_{2}-s_{1})}.\] (1)

_We then define_ **significant CW switches** _recursively as follows: let \(\tau_{0}=1\) and define the \((i+1)\)-th significant CW switch \(\tau_{i+1}\) as the smallest \(t>\tau_{i}\) such that for each arm \(a\in[K]\), \(\exists[s_{1},s_{2}]\subseteq[\tau_{i},t]\) such that arm \(a\) has significant regret over \([s_{1},s_{2}]\). We refer to the interval of rounds \([\tau_{i},\tau_{i+1})\) as a_ **significant phase**_. Let \(\tilde{L}\) be the number of significant CW switches elapsed in \(T\) rounds._

**Notation.**_To ease notation, we'll conflate the closed, open, and half-closed intervals of real numbers \([a,b]\), \((a,b)\), and \([a,b)\), with the corresponding rounds contained therein, i.e. \([a,b]\equiv[a,b]\cap\mathbb{N}\)._Hardness of Significant Shifts in the Condorcet Winner Setting

We first consider regret minimization in an environment with no significant shift in \(T\) rounds. Such an environment admits a _safe arm_2 which does not incur significant regret throughout play. Our first result shows that, under the Condorcet condition, it is not possible to distinguish the identity of \(a^{\sharp}\) from other unsafe arms, which will in turn make sublinear regret impossible.

Footnote 2: The lower bound construction of Saha and Gupta (2022) in fact uses \(\Omega(L)\) significant shifts so that the \(\sqrt{L\cdot\tilde{L}}\) rate is in fact minimax optimal

**Theorem 3**.: _For each horizon \(T\), there exists a finite family \(\mathcal{F}\) of switching dueling bandit environments with \(K=3\) that satisfies the Condorcet winner condition (Definition 1) with \(\tilde{L}=0\) significant shifts. The worst-case regret of any algorithm on an environment \(\mathcal{E}\) in this family is lower bounded as_

\[\sup_{\mathcal{E}\in\mathcal{F}}\mathbb{E}_{\mathcal{E}}\left[\text{DR}(T) \right]\geq T/8.\]

Proof.: (sketch; details found in Appendix B) Letting \(\epsilon\ll 1/\sqrt{T}\), consider the preference matrices:

\[\textbf{P}^{+}:=\begin{pmatrix}1/2&1/2+\epsilon&1\\ 1/2-\epsilon&1/2&1/2+\epsilon\\ 0&1/2-\epsilon&1/2\end{pmatrix},\textbf{P}^{-}:=\begin{pmatrix}1/2&1/2- \epsilon&0\\ 1/2+\epsilon&1/2&1/2-\epsilon\\ 1&1/2+\epsilon&1/2\end{pmatrix}.\]

In \(\textbf{P}^{+}\), arm \(1\) is the Condorcet winner and \(1\succ 2\succ 3\), whereas in \(\textbf{P}^{-}\), \(3\) is the winner with \(3\succ 2\succ 1\). Let an oblivious adversary set \(\textbf{P}_{t}\) at round \(t\) to one of \(\textbf{P}^{+}\) and \(\textbf{P}^{-}\), uniformly at random, inducing an environment where arm \(2\) remains safe for \(T\) rounds. Then, any algorithm will, over the randomness of the adversary, observe \(O_{t}(i_{t},j_{t})\sim\mathrm{Ber}(1/2)\)_no matter the choice of arms \((i_{t},j_{t})\) played_, by the symmetry of \(\textbf{P}^{+},\textbf{P}^{-}\). Thus, it is impossible to distinguish arms, which implies linear regret by standard Pinsker's inequality arguments. In particular, even a strategy playing arm \(2\) every round fails as arm \(2\) is unsafe in another (indistinguishable) setup with arms \(1\) and \(2\) switched in \(\textbf{P}^{+},\textbf{P}^{-}\). 

SST and STI Both Needed To Learn Significant Shifts.The preferences \(\textbf{P}^{+},\textbf{P}^{-}\) in the above proof violate STI but satisfy SST, whereas another construction using preferences \(\textbf{P}^{+},\textbf{P}^{-}\) which violate SST but satisfy STI also works in the proof (see Remark 2 in Appendix B). This shows that sublinear regret is impossible outside of the class SST\(\cap\)STI (visualized in Figure 1).

**Remark 1**.: _Note the lower bound of Theorem 3 does not violate the established upper bounds \(\sqrt{LT}\) and \(V_{T}^{1/3}T^{2/3}\) scaling with \(L\) changes in the preference matrix or total variation \(V_{T}\)(Gupta and Saha, 2022). Our construction in fact uses \(L=\Omega(T)\) changes in the preference matrix and \(V_{T}=\Omega(T)\) total variation. Furthermore, the regret upper bound \(\sqrt{ST}\), in terms of \(S\) changes in Condorcet winner, of Buening and Saha (2022) is not contradicted either, for \(S=\Omega(T)\)._

## 4 Dynamic Regret Upper Bounds under SST/STI

Acknowledging that significant shifts are hard outside of the class SST\(\cap\)STI, we now turn our attention to the achievability of \(\sqrt{K\tilde{L}T}\) regret3 in the SST\(\cap\)STI setting. Our main result is an optimal dynamic regret upper bound attained _without knowledge of the significant shift times or the number of significant shifts_. Up to log terms, this is the first dynamic regret upper bound with optimal dependence on \(T\), \(\tilde{L}\), and \(K\).

Footnote 3: The lower bound construction of Saha and Gupta (2022) in fact uses \(\Omega(L)\) significant shifts so that the \(\sqrt{L\cdot\tilde{L}}\) rate is in fact minimax optimal

**Theorem 4**.: _Suppose SST and STI hold (see Definition 2). Let \(\{\tau_{i}\}_{i=0}^{\tilde{L}}\) denote the unknown significant shifts of Definition 3. Then, for some constant \(C_{0}>0\), Algorithm 1 has expected dynamic regret_

\[\mathbb{E}[\text{DR}(T)]\leq C_{0}\log^{3}(T)\sum_{i=0}^{\tilde{L}}\sqrt{K \cdot(\tau_{i+1}-\tau_{i})},\]

_and using Jensen's inequality, this implies a regret rate of \(C_{0}\log^{3}(T)\sqrt{K\cdot(\tilde{L}+1)\cdot T}\)._In fact, this regret rate can be transformed to depend on the _Condorcet winner variation_ introduced in Buening and Saha (2022) and the _total variation_ quantities introduced in Gupta and Saha (2022) and inspired by the total-variation quantity from non-stationary MAB (Besbes et al., 2014). The following corollary is shown using just the definition of the non-stationarity measures.

**Corollary 5** (Regret in terms of CW Variation).: _Let \(V_{T}:=\sum_{t=2}^{T}\max_{a\in[K]}|P_{t}(a_{t}^{*},a)-P_{t-1}(a_{t}^{*},a)|\) be the unknown Condorcet winner variation. Using the same notation of Theorem 4: Algorithm 1 has expected dynamic regret_

\[\mathbb{E}[\text{DR}(T)]\leq C_{0}\log^{3}(T)\left(\sqrt{RT}+(KV_{T})^{1/3}T^{ 2/3}\right).\]

## 5 Algorithm

At a high level, the strategy of recent works on non-stationary multi-armed bandits (Chen et al., 2019; Wei and Luo, 2021; Suk and Kpotufe, 2022) is to first design a suitable base algorithm and then use a meta-algorithm to randomly schedule different instances of this base algorithm at variable durations across time. The key idea is that unknown time periods of significant regret can be detected fast enough with the right schedule. In order to accurately identify significant shifts, the base algorithm in question should be robust to all non-significant shifts. In the multi-armed bandit setting, a variant of the classical successive elimination algorithm (Even-Dar et al., 2006) possesses such a guarantee (Allesiardo et al., 2017), and serves as a base algorithm in Suk and Kpotufe (2022).

### Difficulty of Efficient Exploration of Arms.

In the non-stationary dueling problem, a natural analogue of successive elimination is to uniformly explore the arm-pair space \([K]\times[K]\) and eliminate arms based on observed comparisons (Urvoy et al., 2013). The previous work (Theorem 5.1 of Buening and Saha, 2022) employs such a strategy as a base algorithm. However, such a uniform exploration approach incurs a large estimation variance of \(K^{2}\), which enters into the final regret bound of \(K\sqrt{T\cdot\hat{L}}\). Thus, smarter exploration strategies are needed to obtain \(\sqrt{K}\) dependence.

In the stationary dueling bandit problem with SST\(\cap\)STI, such efficient exploration strategies have long been known: namely, the Beat-The-Mean algorithm (Yue and Joachims, 2011) and the Interleaved Filtering (IF) algorithm (Yue et al., 2012). We highlight that these existing algorithms aim to learn the ordering of arms, i.e., arms are ruled out roughly in the same order as their true underlying ordering. This fact is crucial to attaining the optimal dependence in \(K\) in their regret analyses, as the higher ranked arms must be played more often against other arms to avoid the \(K^{2}\) cost of exploration.

However, in our setting, adversarial but non-significant changes in the ordering of arms could force perpetual exploration of lowest-ranked arms. This suggests that learning an ordering should not be a subtask of our desired dueling base algorithm. Rather, the algorithm should prioritize minimizing its own regret over time. Keeping this intuition in mind, we introduce an algorithm called **SW**itching **I**nterleaved **F**il**T**ering (SWIFT) (see Algorithm 2 in Section 5.2) which directly tracks regret and avoids learning a fixed ordering of arms.

A new idea for switching candidate arms.A natural idea that is common to many dueling bandit algorithms (including IF) is to maintain a _candidate arm_\(\hat{a}\) which is always played at each round, and serves as a reference point for partially ordering other arms in contention. If the current candidate is beaten by another arm then a new candidate is chosen, and this process quickly converges to the best arm. Since the ordering of arms may change at each round, any such rule that relies on a fixed ordering is deemed to fail in our setting. Our procedure does not rely on such a fixed ordering over arms, but instead tracks the aggregate regret \(\sum_{t}\delta_{t}(a,\hat{a}_{t})\) of the _changing sequence of candidate arms_\(\{\hat{a}_{t}\}_{t}\) to another fixed arm \(a\). Crucially, the candidate arm \(\hat{a}_{s}\) is always played at round \(s\) and so the history of candidate arms \(\{\hat{a}_{s}\}_{s\leq t}\) is fixed at a round \(t\). This fact allows us to estimate the quantity \(\sum_{s=1}^{t}\delta_{s}(a,\hat{a}_{s})\) using importance-weighting at \(\sqrt{K\cdot t}\) rates via martingale concentration. An algorithmic _switching criterion_ then switches the candidate arm \(\hat{a}_{t}\) to any arm \(a\) dominating the sequence \(\{\hat{a}_{s}\}_{s\leq t}\) over time, i.e., \(\sum_{s=1}^{t}\delta_{s}(a,\hat{a}_{s})\gg\sqrt{K\cdot t}\). This simple, yet powerful, idea immediately gives us control of the regret of the candidate sequence \(\{\hat{a}_{t}\}_{t}\) which allows us to bypass the ranking-based arguments of vanilla IF and Beat-The-Mean. It also allows us to simultaneously bound the regret of a sub-optimal arm \(a\) against the sequence of candidate arms \(\sum_{s=1}^{t}\delta_{s}(\hat{a}_{s},a)\).

### Switching Interleaved Filtering (Swift)

SWIFT at round \(t\) compares a _candidate arm_\(\hat{a}_{t}\) with an arm \(a_{t}\) (chosen uniformly at random) from an _active arm set_\(\mathcal{A}_{t}\). Additionally, SWIFT maintains estimates \(\hat{\delta}_{t}(\hat{a}_{t},a)\) of \(\hat{\delta}_{t}(\hat{a}_{t},a)\) which are used to (1) evict active arms \(a\in\mathcal{A}_{t}\) and (2) _switch_ the candidate arm \(\hat{a}_{t+1}\) for the next round.

Estimators and Eviction/Switching Criteria.Let \(\mathcal{A}_{t}\) be the active arm set at round \(t\). Let

\[\hat{\delta}_{t}(\hat{a}_{t},a):=|\mathcal{A}_{t}|\cdot O_{t}(\hat{a}_{t},a) \cdot\mathbf{1}\{(i_{t},j_{t})=(\hat{a}_{t},a)\}-1/2,\] (2)

which is an unbiased estimator of the gap \(\delta_{t}(\hat{a}_{t},a)\) when \(a\in\mathcal{A}_{t}\). We _evict_ an active arm \(a\) from \(\mathcal{A}_{t}\) at round \(t\) if for some constant \(C>0\)5 and rounds \(s_{1}<s_{2}\leq t\):

Footnote 5: The constant \(C>0\) does not depend on \(T\), \(K\), or \(\tilde{L}\), and a suitable value can be derived from the regret analysis.

\[\sum_{s=s_{1}}^{s_{2}}\hat{\delta}_{s}(\hat{a}_{s},a)\geq C\log(T)\sqrt{K\cdot( s_{2}-s_{1})\lor K^{2}},\] (3)

where \(\hat{\delta}_{s}(a,\hat{a}_{s}):=-\hat{\delta}_{s}(\hat{a}_{s},a)\). Next, we switch the next candidate arm \(\hat{a}_{t+1}\gets a\) to another arm \(a\in\mathcal{A}_{t}\) at round \(t\) if for some round \(s_{1}<t\):

\[\sum_{s=s_{1}}^{t}\hat{\delta}_{s}(a,\hat{a}_{s})\geq C\log(T)\sqrt{K\cdot(t-s _{1})\lor K^{2}}.\] (4)

SWIFT is formally shown in Algorithm 2, defined for generic start time \(t_{\text{start}}\) and duration \(m_{0}\) so as to allow for recursive calls in our meta-algorithm framework.

### Non-Stationary Algorithm (Metaswift)

``` Input: horizon \(T\).
1Initialize: round count \(t\gets 1\).
2Episode Initialization (setting global variables \(t_{\ell},\mathcal{A}_{\text{master}},B_{s,m}\)): // \(t_{\ell}\) indicates start of \(\ell\)-th episode. \(t_{\ell}\leftarrow t\). ; // Master active arm set.  For each \(m=2,4,\dots,2^{\lceil\log(T)\rceil}\) and \(s=t_{\ell}+1,\dots,T\):  Sample and store \(B_{s,m}\sim\text{Bernoulli}\left(\frac{1}{\sqrt{m\cdot(s-t_{\ell})}}\right)\). ; // Set replayschedule.
3Run Base-Alg\((t_{\ell},T+1-t_{\ell})\).
4if\(t<T\)then restart from Line 2 (i.e. start a new episode). ; ```

**Algorithm 1**Meta-Elimination while Tracking Arms in SWIFT (METASWIFT)

For the non-stationary setting with multiple (unknown) significant shifts, we run SWIFT as a base algorithm at randomly scheduled rounds and durations.

Our algorithm, dubbed METASWIFT and found in Algorithm 1, operates in _episodes_, starting each episode by playing a _base algorithm_ instance of SWIFT. A running base algorithm _activates_ its own base algorithms of varying durations (Line 8 of Algorithm 2), called _replays_ according to a random schedule decided by the Bernoulli's \(B_{s,m}\) (see Line 6 of Algorithm 1). We refer to the (unique) base algorithm playing at round \(t\) as the _active base algorithm_.

Global Variables.The _active arm set_\(\mathcal{A}_{t}\) is pruned by the active base algorithm at round \(t\), and globally shared between all running base algorithms. In addition, all other variables, i.e. the \(\ell\)-th episode start time \(t_{\ell}\), round count \(t\), schedule \(\{B_{s,m}\}_{s,m}\), and candidate arm \(\hat{a}_{t}\) (and thus the quantities \(\delta_{t}(\hat{a}_{t},a)\)) are shared between base algorithms. Thus, while active, each Base-Alg can switch the candidate arm (4) and evict arms (3) over all intervals \([s_{1},s_{2}]\) elapsed since it began.

Note that only one base algorithm (the active one) can edit \(\mathcal{A}_{t}\) and set the candidate arm \(\hat{a}_{t}\) at round \(t\), while other base algorithms can access these global variables at later rounds. By sharing these globalvariables, any replay can trigger a new episode: every time an arm is evicted by a replay, it is also evicted from the _master arm set_\(\mathcal{A}_{\text{master}}\), tracking arms' regret throughout the entire episode. A new episode is triggered when \(\mathcal{A}_{\text{master}}\) becomes empty, i.e., there is no _safe_ arm left to play.

## 6 Regret Analysis

### Regret of METASWIFT over Significant Phases

Now, we turn to sketching the proof of Theorem 4. Full details are found in Appendix C.

Decomposing the Regret.Let \(a_{t}^{\sharp}\) denote the _last safe arm_ at round \(t\), or the last arm to incur significant regret in the unique phase \([\tau_{i},\tau_{i+1})\) containing round \(t\). Then, we can decompose the dynamic regret around this safe arm using SST and STI (i.e., using Lemma 8 twice) as:

\[\sum_{t=1}^{T}\delta_{t}(a_{t}^{*},\hat{a}_{t})+\delta_{t}(a_{t}^{*},a_{t})\leq 6 \sum_{t=1}^{T}\delta_{t}(a_{t}^{*},a_{t}^{\sharp})+3\sum_{t=1}^{T}\delta_{t}(a_ {t}^{\sharp},\hat{a}_{t})+\sum_{t=1}^{T}\delta_{t}(\hat{a}_{t},a_{t}),\]

where we recall that \(a_{t}\in\mathcal{A}_{t}\) is the other arm played (Line 3 of Algorithm 2). Next, the first sum on the above RHS is order \(\sum_{i=1}^{L}\sqrt{K\cdot(\tau_{i}-\tau_{i-1})}\) as the last safe arm \(a_{t}^{\sharp}\) does not incur significant regret on \([\tau_{i},\tau_{i+1})\). So, it remains to bound the last two sums on the RHS above.

Episodes Align with Significant Phases.We claim that a new episode is triggered only if there a significant shift occurs (Lemma 11). This follows from our eviction criteria (3) with Freedman's inequality for martingale concentration (Lemma 9). Then, acknowledging episodes roughly align with significant phases, we turn our attention to bounding the remaining regret in each episode.

Bounding Regret of an Episode.Let \(t_{\ell}\) be the start of the \(\ell\)-th episode of METASWIFT. Then, our goal is to show for all \(\ell\in[\hat{L}]\) (where \(\hat{L}\) is the random number of episodes used by the algorithm):

\[\max\left\{\mathbb{E}\left[\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(a_{t}^{ \sharp},\hat{a}_{t})\right],\mathbb{E}\left[\sum_{t=t_{\ell}}^{t_{\ell+1}-1} \delta_{t}(\hat{a}_{t},a_{t})\right]\right\}\lesssim\sum_{i\in[\hat{L}+1];[ \tau_{i-1},\tau_{i})\cap[t_{\ell},t_{\ell+1})\neq\emptyset}\sqrt{K\cdot(\tau_ {i}-\tau_{i-1})},\] (5)where the RHS sum above is over the significant phases \([\tau_{i-1},\tau_{i})\) overlapping episode \([t_{\ell},t_{\ell+1})\). Summing over episodes \(\ell\in[\hat{L}]\) will then yield the desired total regret bound by our earlier observation that the episodes align with significant phases (see Lemma 11).

Bounding Regret of Active Arms to Candidate Arms.Bounding \(\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(\hat{a}_{t},a_{t})\) follows in a similar manner as Appendix B.1 of Suk and Kpotufe (2022). First, observe by concentration (Lemma 9) the eviction criterion (3) bounds the sums \(\sum_{t=s_{1}}^{s_{2}}\delta_{t}(\hat{a}_{t},a)\) over intervals \([s_{1},s_{2}]\) where \(a\) is active. Then, accordingly, we further partition the episode rounds \([t_{\ell},t_{\ell+1})\) into different intervals distinguishing the unique regret contributions of different active arms from varying base algorithms, on each of which we can relate the regret to our eviction criterion. Details can be found in Appendix C.3.

\(\bullet\)Bounding Regret of Candidate Arm to Safe Arm.The first sum on the LHS of (5) will be further decomposed using the _last master arm_\(a_{\ell}\) which is the last arm to be evicted from the master arm set \(\mathcal{A}_{\text{master}}\) in episode \([t_{\ell},t_{\ell+1})\). Carefully using SST and STI (see Lemma 13), we further decompose \(\delta_{t}(a_{\ell}^{\ddagger},\hat{a}_{t})\) as:

\[\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(a_{t}^{\sharp},\hat{a}_{t})\leq 2 \underbrace{\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(a_{t}^{\sharp},a_{\ell} )}_{\text{A}}+\underbrace{\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(a_{\ell}, \hat{a}_{t})}_{\text{B}}+3\underbrace{\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{ t}(a_{t}^{*},a_{t}^{\sharp})}_{\text{C}}\] (6)

The sum C above was already bounded earlier. So, we turn our attention to B and A.

\(\bullet\)Bounding B.Note that the arm \(a_{\ell}\) by definition is never evicted by any base algorithm until the end of the episode \(t_{\ell+1}-1\). This means that at round \(t\in[t_{\ell},t_{\ell+1})\), the quantity \(\sum_{s=t_{\ell}}^{t}\hat{\delta}_{s}(a_{\ell},\hat{a}_{s})\) is always kept small by the candidate arm switching criterion (4). So, by concentration (Proposition 10), we have \(\sum_{s=t_{\ell}}^{t_{\ell+1}-1}\hat{\delta}_{s}(a_{\ell},\hat{a}_{s}) \lesssim\sqrt{K(t_{\ell+1}-t_{\ell})}\).

\(\bullet\)Bounding AThe main intuition here, similar to Appendix B.2 of Suk and Kpotufe (2022), is that well-timed replay are scheduled w.h.p. to ensure fast detection of large regret of the last master arm \(a_{\ell}\). Key in this is the notion of a _bad segment of time_: i.e., an interval \([s_{1},s_{2}]\subseteq[\tau_{i},\tau_{i+1})\) lying inside a significant phase with last safe arm \(a^{\sharp}\) where:

\[\sum_{t=s_{1}}^{s_{2}}\delta_{t}(a^{\sharp},a_{\ell})\gtrsim\sqrt{K\cdot(s_{2 }-s_{1})}.\] (7)

For a fixed bad segment \([s_{1},s_{2}]\), the idea is that a fortuitously timed replay scheduled at round \(s_{1}\) and remaining active till round \(s_{2}\) will evict arm \(a_{\ell}\).

It is not immediately obvious how to carry out this argument in the dueling bandit problem since, to detect large \(\sum_{t}\delta_{t}(a^{\sharp},a_{\ell})\), the pair of arms \(a^{\sharp},a_{\ell}\) need to both be played which, as we discussed in Section 5.1, may not occur often enough to ensure tight estimation of the gaps.

Instead, we carefully make use of SST/STI to relate \(\delta_{t}(a^{\sharp},a_{\ell})\) to \(\delta_{t}(\hat{a}_{t},a_{\ell})\). Note this latter quantity controls both the eviction (3) and \(\hat{a}_{t}\) switching (4) criteria. This allows us to convert bad intervals with large \(\sum_{t}\delta_{t}(a_{t}^{\sharp},a_{\ell})\) to intervals with large \(\sum_{t}\delta_{t}(\hat{a}_{t},a_{\ell})\). Specifically, by Lemma 13, we have that (7) implies

\[2\sum_{t=s_{1}}^{s_{2}}\delta_{t}(a^{\sharp},\hat{a}_{t})+\sum_{t=s_{1}}^{s_{2 }}\delta_{t}(\hat{a}_{t},a_{\ell})+3\sum_{t=s_{1}}^{s_{2}}\delta_{t}(a_{t}^{*},a ^{\sharp})\gtrsim\sqrt{K\cdot(s_{2}-s_{1})}.\] (8)

Then, we claim that, so long as a base algorithm \(\textsf{Base-Alg}(s_{1},m)\) is scheduled from \(s_{1}\) running till \(s_{2}\), we will have \(\sum_{t=s_{1}}^{s_{2}}\delta_{t}(\hat{a}_{t},a_{\ell})\gtrsim\sqrt{K\cdot(s_{2 }-s_{1})}\) which implies \(a_{\ell}\) will be evicted. In other words, the second sum dominates the first and third sums in (8). We repeat earlier arguments to show this:

* By the definition of the last safe arm \(a^{\sharp}\), \(\sum_{t=s_{1}}^{s_{2}}\delta_{t}(a_{t}^{*},a^{\sharp})<\sqrt{K\cdot(s_{2}-s_{1 })}\).
* Meanwhile, \(\sum_{t=s_{1}}^{s_{2}}\delta_{t}(a^{\sharp},\hat{a}_{t})\lesssim\sqrt{K\cdot(s_ {2}-s_{1})}\) by the candidate switching criterion (4) and because \(a^{\sharp}\) will not be evicted before round \(s_{2}\) less it incurs significant regret which cannot happen by definition of \(a^{\sharp}\).

Combining the above two points with (8), we have that \(\sum_{t=s_{i}}^{s_{2}}\delta_{t}(\hat{a}_{t},a_{\ell})\gtrsim\sqrt{K\cdot(s_{2}-s_ {1})}\), which directly aligns with our criterion (3) for evicting \(a_{\ell}\). To summarize, a bad segment \([s_{1},s_{2}]\) in the sense of (7) is detectable using a well-timed instance of \(\mathsf{SWIFT}\), which happens often enough with high probability. Concretely, we argue that not too many bad segments elapse before \(a_{\ell}\) is evicted by a well-timed replay in the above sense and that thus the regret incurred by \(a_{\ell}\) is bounded by the RHS of (5). The details can be found in Appendix C.5.

## 7 Conclusion

We consider the problem of switching dueling bandits where the distribution over preferences can change over time. We study a notion of significant shifts in preferences and ask whether one can achieve adaptive dynamic regret of \(O(\sqrt{K\tilde{L}T})\) where \(\tilde{L}\) is the number of significant shifts. We give a negative result showing that one cannot achieve such a result outside of the SST\(\cap\)STI setting, and answer this question in the affirmative under the SST\(\cap\)STI setting. In the future, it would be interesting to consider other notions of shifts which are weaker than the notion of significant shift, and ask whether adaptive algorithms for the Condorcet setting can be designed with respect to these notions. Buening and Saha (2022) already give a \(O(K\sqrt{ST})\) bound for the Condorcet setting, where \(S\) is the number of changes in 'best' arm. However, their results have a suboptimal dependence on \(K\) due to reduction to "all-pairs" exploration.

## Acknowledgements

We thank Samory Kpotufe for helpful discussions. We also acknowledge computing resources from Columbia University's Shared Research Computing Facility project, which is supported by NIH Research Facility Improvement Grant 1G20RR030893-01, and associated funds from the New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded April 15, 2010.

## References

* A. Agarwal, R. Ghuge, and V. Nagarajan (2022)Batched dueling bandits. In Proceedings of the 39th International Conference on Machine Learning, pp. 89-110. Cited by: SS1.
* N. Ailon, Z. Karnin, and T. Joachims (2014)Reducing dueling bandits to cardinal bandits. In Proceedings of the 31st International Conference on Machine Learning, Cited by: SS1.
* R. Allesiardo, R. Feraud, and O. Maillard (2017)The non-stationary stochastic multi-armed bandit problem. International Journal of Data Science and Analytics3 (4), pp. 267-283. Cited by: SS1.
* P. Auer, N. C. Desa-Bianchi, Y. Freund, and R. E. Schapire (2002)The Nonstochastic multi-armed Bandit Problem. SIAM Journal on Computing32 (1), pp. 48-77. Cited by: SS1.
* P. Auer, P. Gajane, and R. Ortner (2018)Adaptively tracking the best arm with an unknown number of distribution changes. 14th European Workshop on Reinforcement Learning (EWRL). Cited by: SS1.
* P. Auer, P. Gajane, and R. Ortner (2019)Adaptively tracking the best bandit arm with an unknown number of distribution changes. In Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, pp. 138-158. Cited by: SS1.
* O. Besbes, Y. Gur, and A. Zeevi (2014)Stochastic multi-armed-bandit problem with non-stationary rewards. Advances in Neural Information Processing Systems27, pp. 199-207. Cited by: SS1.
* A. Beygelzimer, J. Langford, L. Li, L. Reyzin, and R. Schapire (2011)Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, pp. 19-26. Cited by: SS1.

Thomas Kleine Buening and Aadirup Saha. Anaconda: An improved dynamic regret algorithm for adaptive non-stationary dueling bandits. _arXiv preprint arXiv:2210.14322_, 2022.
* Chen and Frazier (2017) Bangrui Chen and Peter I. Frazier. Dueling Bandits with Weak Regret. In _Proceedings of the 34th International Conference on Machine Learning_, 2017.
* Chen et al. (2019) Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextual bandits: Efficient, optimal, and parameter-free. _In Proceedings of the 32nd Conference on Learning Theory_, 99:1-30, 2019.
* Dudik et al. (2015) Miroslav Dudik, Katja Hofmann, Robert E. Schapire, Aleksandrs Slivkins, and Masrour Zoghi. Contextual Dueling Bandits. In _Proceedings of the 28th Conference on Learning Theory_, 2015.
* Even-Dar et al. (2006) Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems. _Journal of Machine Learning Research_, 7:1079-1105, 2006.
* Gajane et al. (2015) Pratik Gajane, Tanguy Urvoy, and Fabrice Clerot. A relative exponential weighing algorithm for adversarial utility-based dueling bandits. In Francis R. Bach and David M. Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015_, volume 37 of _JMLR Workshop and Conference Proceedings_, pages 218-227. JMLR.org, 2015.
* Garivier and Moulines (2011) Aurelien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. In _International Conference on Algorithmic Learning Theory_, pages 174-188. Springer, 2011.
* Gupta and Saha (2022) Shubham Gupta and Aadirupa Saha. Optimal and efficient dynamic regret algorithms for non-stationary dueling bandits. In _International Conference on Machine Learning_, pages 19027-19049. PMLR, 2022.
* Jamieson et al. (2015) Kevin Jamieson, Sumeet Katariya, Atul Deshpande, and Robert Nowak. Sparse Dueling Bandits. In _Proceedings of the 18th International Conference on Artificial Intelligence and Statistics_, 2015.
* Kocsis and Szepesvari (2006) Levente Kocsis and Csaba Szepesvari. Discounted ucb. _2nd PASCAL Challenges Workshop_, 2006.
* Kolpaczki et al. (2022) Patrick Kolpaczki, Viktor Benggs, and Eyke Hullermeier. Non-stationary dueling bandits. _arXiv preprint arXiv:2202.00935_, 2022.
* Komiyama et al. (2015) Junpei Komiyama, Junya Honda, Hisashi Kashima, and Hiroshi Nakagawa. Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem. In _Proceedings of the 28th Conference on Learning Theory_, 2015.
* Komiyama et al. (2016) Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm. In _Proceedings of the 33rd International Conference on Machine Learning_, 2016.
* Liu (2009) Tie-Yan Liu. Learning to rank for information retrieval. _Found. Trends Inf. Retr._, 3(3):225-331, mar 2009. ISSN 1554-0669. doi: 10.1561/1500000016.
* Radlinski et al. (2008) Filip Radlinski, Madhu Kurup, and Thorsten Joachims. How does clickthrough data reflect retrieval quality? In _Proceedings of the 17th ACM Conference on Information and Knowledge Management_, CIKM '08, page 43-52, New York, NY, USA, 2008. Association for Computing Machinery. ISBN 9781595939913. doi: 10.1145/1458082.1458092.
* Ramamohan et al. (2016) Siddartha Ramamohan, Arun Rajkumar, and Shivani Agarwal. Dueling Bandits : Beyond Condorcet Winners to General Tournament Solutions. In _Advances in Neural Information Processing Systems 29_, 2016.
* Saha and Gaillard (2022) Aadirupa Saha and Pierre Gaillard. Versatile dueling bandits: Best-of-both world analyses for learning from relative preferences. In _International Conference on Machine Learning_, pages 19011-19026. PMLR, 2022.
* Saha et al. (2015)* Saha and Gupta (2022) Aadirupa Saha and Shubham Gupta. Optimal and efficient dynamic regret algorithms for nonstationary dueling bandits. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 19027-19049. PMLR, 2022.
* Saha et al. (2021) Aadirupa Saha, Tomer Koren, and Yishay Mansour. Adversarial dueling bandits. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 9235-9244. PMLR, 2021.
* Sui et al. (2018) Yanan Sui, Masrour Zoghi, Katja Hofmann, and Yisong Yue. Advancements in dueling bandits. In Jerome Lang, editor, _Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden_, pages 5502-5510. ijcai.org, 2018.
* Suk and Kpotufe (2022) Joe Suk and Samory Kpotufe. Tracking most significant arm switches in bandits. In _Conference on Learning Theory_, pages 2160-2182. PMLR, 2022.
* Urvoy et al. (2013) Tanguy Urvoy, Fabrice Clerot, Raphael Feraud, and Sami Naamane. Generic Exploration and K-armed Voting Bandits. In _Proceedings of the 30th International Conference on Machine Learning_, 2013.
* Wei and Luo (2021) Chen-Yu Wei and Haipeng Luo. Non-stationary reinforcement learning without prior knowledge: An optimal black-box approach. _In Proceedings of the 32nd International Conference on Learning Theory_, 2021.
* Wu and Liu (2016) Huasen Wu and Xin Liu. Double thompson sampling for dueling bandits. In _Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain_, pages 649-657, 2016.
* Yue and Joachims (2011) Yisong Yue and Thorsten Joachims. Beat the mean bandit. In _Proceedings of the 28th International Conference on Machine Learning_, 2011.
* Yue et al. (2012a) Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The \(k\)-armed dueling bandits problem. _Journal of Computer and System Sciences_, 78(5):1538-1556, 2012a.
* Yue et al. (2012b) Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem. _Journal of Computer and System Sciences_, 78(5):1538-1556, 2012b. ISSN 0022-0000. doi: https://doi.org/10.1016/j.jcss.2011.12.028. JSCS Special Issue: Cloud Computing 2011.
* Zoghi et al. (2014) Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten de Rijke. Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem. In _Proceedings of the 31st International Conference on Machine Learning_, 2014.
* Zoghi et al. (2015a) Masrour Zoghi, Zohar Karnin, Shimon Whiteson, and Maarten de Rijke. Copeland Dueling Bandits. In _Advances in Neural Information Processing Systems 28_, 2015a.
* Zoghi et al. (2015b) Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. MergeRUCB: A method for large-scale online ranker evaluation. In _Proceedings of the 8th ACM International Conference on Web Search and Data Mining_, 2015b.

## Appendix A Experiments

Code can be found at https://github.com/joesuk/nonstationary-duel.

Synthetic Environments.We used a geometric BTL model where the arms are linearly ordered and the \(i\)-th best arm beats the \(j\)-th best arm with probability \(\mathbb{P}(i\succ j)=\frac{2^{-i}}{2^{-j}+2^{-i}}\). At each changepoint, the ordering of arms was randomly permuted, with a total \(T=50,000\) rounds with \(K=10\) arms. Regret was computed over \(N=50\) trials of each environment with standard confidence bands shown.

Algorithms.We considered four algorithms: (1) METASWIFT (Algorithm 1, (2) the ANACONDA algorithm of Buening and Saha (2022) (3) Interleaved Filtering (which we abbreviate as IF) as specified by Yue et al. (2012), and a baseline (3) RANDDUEL which naively plays a pair of arms selected uniformly at random every round.

Parameters.Parameters associated with each of the algorithms (e.g., the constants in displays (3) and (4), analogous quantities in ANACONDA, and IF's eliminination threshold) were tuned using cross validation on randomly generated geometric BTL environments with number of changepoints varying from \(0\) to \(1000\). For fairness, all algorithms were given the chance to tune parameters on the same environments before testing.

The first graphic in Figure 2 shows the regret curves in a stationary environment with \(S=0\) changes. The second graphic shows the regret curves in a non-stationary environment with \(S=4\) changes. Exact mean and standard deviations on final regret are given in Table 1. These do support the theoretical message that METASWIFT performs better than the existing ANACONDA algorithm in non-stationary environments due to more efficient exploration of arms (demonstrated through \(\sqrt{K}\) versus \(K\) dependence in the theoretical bounds). Moreover, we also observe that the IF algorithm which is designed for stationary environments can have almost linear regret in non-stationary environments.

\begin{table}
\begin{tabular}{|l|c|c|c|} \hline  & Algorithm & Mean Regret & Standard Deviation \\ \hline \(S=0\) Changes & METASWIFT & 3048 & 600 \\  & ANACONDA & 4863 & 707 \\  & IF & 140 & 46 \\  & RANDDUEL & 18688 & 30 \\ \hline \(S=4\) Changes & METASWIFT & 3346 & 531 \\  & ANACONDA & 5331 & 705 \\  & IF & 14142 & 3739 \\  & RANDDUEL & 18684 & 23 \\ \hline \end{tabular}
\end{table}
Table 1: Table of total dynamic regrets.

Figure 2: Plots of dynamic regret curves over time.

Proof of Theorem 3

Consider the following preference matrices for some \(\epsilon>0\) (to be chosen later):

\[\textbf{P}^{+}:=\begin{pmatrix}1/2&1/2+\epsilon&1\\ 1/2-\epsilon&1/2&1/2+\epsilon\\ 0&1/2-\epsilon&1/2\end{pmatrix},\textbf{P}^{-}:=\begin{pmatrix}1/2&1/2- \epsilon&0\\ 1/2+\epsilon&1/2&1/2-\epsilon\\ 1&1/2+\epsilon&1/2\end{pmatrix}.\]

In environment \(\textbf{P}^{+}\), arm \(1\) is the Condorcet winner and we have \(1\succ 2\succ 3\). In environment \(\textbf{P}^{-}\), arm \(3\) is the winner with \(3\succ 2\succ 1\).

Consider a uniform mixture \(\mathcal{U}\) of the preference matrices \(\textbf{P}^{+}\) and \(\textbf{P}^{-}\), Let \(\mathcal{E}\) be a (random) sequence of \(T\) environments sampled i.i.d. from \(\mathcal{U}\), with \(\textbf{P}_{t}:=(\mathcal{E})_{t}\) being the sampled environment at round \(t\).

First, it is straightforward to verify in every such switching dueling bandit \(\mathcal{E}\), arm \(2\) does not incur significant regret over any interval of rounds \([s_{1},s_{2}]\subseteq[1,T]\), for \(\epsilon<1/\sqrt{T}\). Thus, every such \(\mathcal{E}\) exhibits zero significant shifts.

Next, in what follows, we use \(\mathbb{E}_{\mathcal{E}}[\cdot]\) to denote an expectation over both the randomness of \(\mathcal{U}^{\otimes T}\) and the algorithm's feedback and decisions. If there exists a realization of \(\mathcal{E}\) such that the algorithm gets expected regret at least \(T/8\), then we are already done. Otherwise, we have the expected regret over the random environment \(\mathcal{E}\) is bounded above by \(T/8\). Next, define the _arm-pull counts_\(N(T,a):=\sum_{t=1}^{T}\textbf{1}\{i_{t}=a\}+\textbf{1}\{j_{t}=a\}\) for each arm \(a\). Then, we relate these arm-pull counts to the regret:

\[T/8 >\sum_{t=1}^{T}\mathbb{E}_{\mathcal{E}}\left[\delta_{t}(i^{*},i_{ t})+\delta_{t}(i^{*},j_{t})\right]\] \[\geq\frac{1}{2}\sum_{t=1}^{T}\mathbb{E}_{\mathcal{E}}[(\textbf{1} \{i_{t}=3\}+\textbf{1}\{j_{t}=3\})\cdot\textbf{1}\{(\mathcal{E})_{t}=\textbf {P}^{+}\}\] \[\qquad+(\textbf{1}\{i_{t}=1\}+\textbf{1}\{j_{t}=1\})\cdot\textbf {1}\{(\mathcal{E})_{t}=\textbf{P}^{-}\}]\] \[=\frac{1}{2}\sum_{t=1}^{T}\mathbb{E}_{\mathcal{E}}\left[\frac{1} {2}\cdot(\textbf{1}\{i_{t}=3\}+\textbf{1}\{j_{t}=3\}+\textbf{1}\{i_{t}=1\}+ \textbf{1}\{j_{t}=1\})\right]\] \[\geq\frac{1}{4}\cdot\mathbb{E}_{\mathcal{E}}[N(T,3)+N(T,1)],\]

where we use the tower law in the third inequality (note that \(i_{t},j_{t}\) are independent of \((\mathcal{E})_{t}\)). Thus, in expectation over both the model noise and randomness of \(\mathcal{E}\), arms \(3\) and \(1\) cannot be played more than \(T/2\) times without causing linear regret.

Since \(\sum_{a=1}^{3}\mathbb{E}_{\mathcal{E}}[N(T,a)]=2T\), we conclude that \(\mathbb{E}_{\mathcal{E}}[N(T,2)]\geq 3T/2\). We will next show that arm \(2\) is statistically indistinguishable from arm \(3\). To do so, we consider an analogous environment which is identical to \(\mathcal{E}\) except the identities of arms \(2\) and \(3\) are switched. Specifically, let \(\mathcal{E}^{\prime}\) be a random sequence of \(T\) environments sampled i.i.d. from a uniform mixture of \(\textbf{Q}^{+}\) and \(\textbf{Q}^{-}\), which are respectively \(\textbf{P}^{+}\) and \(\textbf{P}^{-}\) with switched entries for arms \(2\) and \(3\).

We next claim \(\mathbb{E}_{\mathcal{E}}[N(T,2)]=\mathbb{E}_{\mathcal{E}^{\prime}}[N(T,2)]\). Admitting this claim, it immediately follows that the algorithm has expected regret (over the randomness of \(\mathcal{E}^{\prime}\)) at least (using an analogous chain of inequalities as above):

\[\mathbb{E}_{\mathcal{E}^{\prime}}[\text{DR}(T)]\geq\frac{1}{4}\cdot\mathbb{E }_{\mathcal{E}^{\prime}}[N(T,2)]\geq 3T/8.\]

In particular, there exists a realization of \(\mathcal{E}^{\prime}\) within the prior on environments on which the regret is at least \(3T/8\).

It remains to show \(\mathbb{E}_{\mathcal{E}}[N(T,2)]=\mathbb{E}_{\mathcal{E}^{\prime}}[N(T,2)]\). This will follow from Pinsker's inequality and showing that the KL divergence between \(\mathcal{E}\) and \(\mathcal{E}^{\prime}\) is zero.

We first observe that the dueling observations \(O_{t}(i,j)\) at each round \(t\in[T]\) are identically a \(\mathrm{Ber}(1/2)\) R.V. for all pairs of arms \(i,j\) in both \(\mathcal{E}\) and \(\mathcal{E}^{\prime}\), since a uniform mixture of a \(\mathrm{Ber}(1/2+\epsilon)\) and a \(\mathrm{Ber}(1/2-\epsilon)\) is a \(\mathrm{Ber}(1/2)\), while so is the uniform mixture of a \(\mathrm{Ber}(1)\) and a \(\mathrm{Ber}(0)\).

Then, since \(N(T,2)\leq 2T\), by Pinsker's inequality (see Gupta and Saha, 2022, proof of Lemma C.1), we have:

\[\mathbb{E}_{\mathcal{E}}[N(T,2)]-\mathbb{E}_{\mathcal{E}^{\prime}}[N(T,2)]\leq 2 T\sqrt{\frac{\mathrm{KL}(\mathcal{P},\mathcal{P}^{\prime})}{2}},\]

where \(\mathcal{P}\) and \(\mathcal{P}^{\prime}\) are the induced distributions over the randomness \(\mathcal{U}^{\otimes T}\), and the history of observations and decisions in \(T\) rounds by \(\mathcal{E}\) and \(\mathcal{E}^{\prime}\). Let \(\mathcal{H}_{t}\) be the history of randomness, observations, and decisions till round \(t\): \(\mathcal{H}_{t}=\{(u_{s},i_{s},j_{s},O_{s}(i_{s},j_{s}))\}_{s\leq t}\) where \(u_{s}\sim\mathrm{Ber}(1/2)\) decides whether \(\textbf{P}^{+}/\textbf{Q}^{+}\) or \(\textbf{P}^{-}/\textbf{Q}^{-}\) is realized at round \(t\). Let \(\mathcal{P}_{t}\) and \(\mathcal{P}_{t}^{\prime}\) denote the respective marginal distributions over the round \(t\) data \((u_{t},i_{t},j_{t},O_{t}(i_{t},j_{t}))\). Then, repeatedly using chain rule for KL and then conditioning on the played arms \((i_{t},j_{t})\) (whose identities are fixed given \(\mathcal{H}_{t-1}\)) at round \(t\), we get:

\[\mathrm{KL}(\mathcal{P},\mathcal{P}^{\prime})=\sum_{t=1}^{T}\mathrm{KL}( \mathcal{P}_{t}|\mathcal{H}_{t-1},\mathcal{P}_{t}^{\prime}|\mathcal{H}_{t-1}) =\sum_{t=1}^{T}\mathbb{E}_{\mathcal{H}_{t-1}}[\mathbb{E}_{i_{t},j_{t}}[ \mathrm{KL}(\mathrm{Ber}(1/2),\mathrm{Ber}(1/2))]]=0.\]

**Remark 2**.: _The constructed environments \(\textbf{P}^{+},\textbf{P}^{-}\) in the proof of Theorem 3 satisfies SST but violates STI. A similar construction which violates SST (but satisfies STI) can also be used in the proof. Let_

\[\textbf{P}^{+}:=\begin{pmatrix}1/2&1/2-\epsilon&1/2-\epsilon\\ 1/2+\epsilon&1/2&0\\ 1/2+\epsilon&1&1/2\end{pmatrix},\]

_and let \(\textbf{P}^{-}\) be the same preference matrix with arms \(2\) and \(3\) switched. Note that \(3\succ 2\succ 1\) in \(\textbf{P}^{+}\) and \(2\succ 3\succ 1\) in \(\textbf{P}^{-}\). Here, arm \(1\) is the "safe" arm as it always has a gap of \(\epsilon\) while arms \(2\) and \(3\) randomly alternate between being the best arm and the worst arm with a gap of \(1/2\). Thus, both the STI and SST assumptions are required to get sublinear regret in mildly adversarial environments._

Due to these observations we have the following corollaries.

**Corollary 6**.: _For each horizon \(T\), there exists a finite family \(\mathcal{F}\) of switching dueling bandit environments with \(K=3\) that satisfies the SST condition with \(\tilde{L}=0\) significant shifts. The worst-case regret of any algorithm on an environment \(\mathcal{E}\) in this family is lower bounded as_

\[\sup_{\mathcal{E}\in\mathcal{F}}\mathbb{E}_{\mathcal{E}}\left[\text{DR}(T) \right]\geq T/8.\]

**Corollary 7**.: _For each horizon \(T\), there exists a finite family \(\mathcal{F}\) of switching dueling bandit environments with \(K=3\) that satisfies the STI condition with \(\tilde{L}=0\) significant shifts. The worst-case regret of any algorithm on an environment \(\mathcal{E}\) in this family is lower bounded as_

\[\sup_{\mathcal{E}\in\mathcal{F}}\mathbb{E}_{\mathcal{E}}\left[\text{DR}(T) \right]\geq T/8.\]

## Appendix C Full Proof of Theorem 4

Throughout the proof \(c_{1},c_{2},\ldots\) will denote positive constants not depending on \(T\) or any distributional parameters. First, we observe the regret bound is vacuous for \(T<K\); so, assume \(T\geq K\). Recall from Line 3 of Algorithm 1 that \(t_{\ell}\) is the first round of the \(\ell\)-th episode. WLOG, there are \(T\) total episodes and, by convention, we let \(t_{\ell}:=T+1\) if only \(\ell-1\) episodes occurred by round \(T\).

Next, we establish an elementary lemma which will help us leverage the STI and SST assumptions.

### Decomposing the Regret

**Lemma 8**.: _For any three arms \(b,c\), under \(\mathrm{SST}\cap\mathrm{STI}\): \(\delta_{t}(a_{t}^{*},c)\leq 2\cdot\delta_{t}(a_{t}^{*},b)+\delta_{t}(b,c)\)._

Proof.: If \(b\succeq_{t}c\), this is true by STI. Otherwise, \(\delta_{t}(a_{t}^{*},c)\leq\delta_{t}(a_{t}^{*},b)\leq\delta_{t}(a_{t}^{*},b)+ \delta_{t}(a_{t}^{*},b)-\delta_{t}(c,b)\) by SST. 

Using Lemma 8 twice, we have the regret can be written as

\[\sum_{t=1}^{T}\delta_{t}(a_{t}^{*},\hat{a}_{t})+\delta_{t}(a_{t}^{*},a_{t})\leq \sum_{t=1}^{T}6\cdot\delta_{t}(a_{t}^{*},a_{t}^{sharp})+3\cdot\delta_{t}(a_{t}^{ *},\hat{a}_{t})+\delta_{t}(\hat{a}_{t},a_{t}).\]Following the discussion of Section 6, it remains to bound \(\sum_{t=1}^{T}\delta_{t}(a_{t}^{t},\hat{a}_{t})\) and \(\sum_{t=1}^{T}\delta_{t}(\hat{a}_{t},a_{t})\) in expectation. For this, we need to relate our estimators \(\hat{\delta}_{t}(\hat{a}_{t},a)\) to the true gaps \(\delta_{t}(\hat{a}_{t},a)\).

### Relating Estimated Gaps to Regret

We first recall a version of Freedman's martingale concentration inequality, identical to the one used in Suk and Kpotufe (2022); Buening and Saha (2022).

**Lemma 9** (Theorem 1 of Beygelzimer et al. (2011)).: _Let \(X_{1},\ldots,X_{n}\in\mathbb{R}\) be a martingale difference sequence with respect to some filtration \(\{\mathcal{F}_{0},\mathcal{F}_{1},\ldots\}\). Assume for all \(t\) that \(X_{t}\leq R\) a.s. and that \(\sum_{i=1}^{n}\mathbb{E}[X_{i}^{2}|\mathcal{F}_{i-1}]\leq V_{n}\) a.s. for some constant \(V_{n}\) only depending on \(n\). Then for any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have:_

\[\sum_{i=1}^{n}X_{i}\leq(e-1)\left(\sqrt{V_{n}\log(1/\delta)}+R\log(1/\delta) \right).\]

We next apply Lemma 9 to bound the estimation error of our estimates \(\hat{\delta}_{t}(\hat{a}_{t},a)\), found in (2).

**Proposition 10**.: _Let \(\mathcal{E}_{1}\) be the event that for all rounds \(s_{1}<s_{2}\) and all arms \(a\in[K]\):_

\[\left|\sum_{t=s_{1}}^{s_{2}}\hat{\delta}_{t}(\hat{a}_{t},a)-\ \sum_{t=s_{1}}^{s_{2}}\mathbb{E}\left[\hat{\delta}_{t}(\hat{a}_{t},a)\mid \mathcal{F}_{t-1}\right]\right|\leq c_{1}\log(T)\left(\sqrt{K(s_{2}-s_{1})}+K \right),\] (9)

_for an appropriately large constant \(c_{1}\), and where \(\mathcal{F}:=\{\mathcal{F}_{t}\}_{t=1}^{T}\) is the canonical filtration generated by observations and randomness of elapsed rounds. Then, \(\mathcal{E}_{1}\) occurs with probability at least \(1-1/T^{2}\)._

Proof.: The random variable \(\hat{\delta}_{t}(\hat{a}_{t},a)-\mathbb{E}[\hat{\delta}_{t}(\hat{a}_{t},a)| \mathcal{F}_{t-1}]\) is a martingale difference bounded above by \(K\) for all rounds \(t\) and all arms \(a,a^{\prime}\). Note here that the identity of the candidate arm \(\hat{a}_{t}\) is fixed conditional on the observations of the previous rounds \(\mathcal{F}_{t-1}\). The variance of this difference is:

\[\sum_{t=s_{1}}^{s_{2}}\mathbb{E}[\hat{\delta}_{t}^{2}(\hat{a}_{t },a)\mid\mathcal{F}_{t-1}] \leq\sum_{t=s_{1}}^{s_{2}}|\mathcal{A}_{t}|^{2}\mathbb{E}[\mathbf{ 1}\{j_{t}=a\}|\mathcal{F}_{t-1}]\] \[\leq\sum_{t=s_{1}}^{s_{2}}|\mathcal{A}_{t}|^{2}\cdot\frac{1}{| \mathcal{A}_{t}|}\] \[\leq K\cdot(s_{2}-s_{1}+1).\] \[\leq 2K\cdot(s_{2}-s_{1})\]

Then, the result follows from Lemma 9 and taking union bounds over arms \(a\) and rounds \(s_{1},s_{2}\). 

Since the contribution to the expected regret is small outside of the high-probability good event \(\mathcal{E}_{1}\), going forward we will assume as necessary that (9) holds for all arms \(a\in[K]\) and rounds \(s_{1},s_{2}\). The next result asserts that episodes roughly correspond to significant shifts in the sense that a restart (Line 8 of Algorithm 1) occurs only if a significant shift has been detected.

**Lemma 11**.: _On event \(\mathcal{E}_{1}\), for each episode \([t_{\ell},t_{\ell+1})\) with \(t_{\ell+1}\leq T\) (i.e., an episode which concludes with a restart), there exists a significant shift \(\tau_{i}\in[t_{\ell},t_{\ell+1})\)._

Proof.: We have that

\[\mathbb{E}[\hat{\delta}_{t}(\hat{a}_{t},a)|\mathcal{F}_{t-1}]=\begin{cases} \delta_{t}(\hat{a}_{t},a)&a\in\mathcal{A}_{t}\\ -1/2&a\not\in\mathcal{A}_{t}\end{cases}.\]

Thus, by concentration (Proposition 10) and the eviction criteria (3) with large enough constant \(C>0\), we have that an arm \(a\) being evicted over interval \([s_{1},s_{2}]\) implies \(\sum_{t=s_{1}}^{s_{2}}\delta_{t}(\hat{a}_{t},a)>\sqrt{K\cdot(s_{2}-s_{1})}\). By the SST condition, this means that

\[\sum_{t=s_{1}}^{s_{2}}\delta_{t}(a_{t}^{*},a)\geq\sum_{t=s_{1}}^{s_{2}}\delta_ {t}(\hat{a}_{t},a)>\sqrt{K\cdot(s_{2}-s_{1})}.\]

This means, over the course of episode \([t_{\ell},t_{\ell+1})\), every arm \(a\in[K]\) incurs significant regret meaning a significant shift must take place between rounds \(t_{\ell}\) and \(t_{\ell+1}-1\).

Following the outline of Section 6, we now turn our attention to bounding the regrets \(\delta_{t}(a_{t}^{\ddagger},\hat{a}_{t})\) and \(\delta_{t}(\hat{a}_{t},a_{t})\) over a single episode \([t_{\ell},t_{\ell+1})\).

Bounding \(\mathbb{E}[\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(\hat{a}_{t},a_{t})]\): Regret of Active Arms to Candidate Arm

We first decompose the total sum of regrets \(\mathbb{E}[\sum_{t=1}^{T}\delta_{t}(\hat{a}_{t},a_{t})]\) based on which arm \(a_{t}\) chooses within the active set \(\mathcal{A}_{t}\). Using tower law, we have

\[\mathbb{E}\left[\sum_{t=1}^{T}\delta_{t}(\hat{a}_{t},a_{t})\right]=\sum_{t=1} ^{T}\mathbb{E}[\mathbb{E}[\delta_{t}(\hat{a}_{t},a_{t})\mid\mathcal{F}_{t-1}]] =\mathbb{E}\left[\sum_{t=1}^{T}\sum_{a\in\mathcal{A}_{t}}\frac{\delta_{t}( \hat{a}_{t},a)}{|\mathcal{A}_{t}|}\right].\]

Splitting the above RHS back along episodes, we obtain the sum \(\mathbb{E}[\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\sum_{a\in\mathcal{A}_{t}}\delta_{ t}(\hat{a}_{t},a)/|\mathcal{A}_{t}|]\).

Next, we condition on the good event \(\mathcal{E}_{1}\) on which the concentration bounds of Proposition 10 hold. Additionally, we divide up the rounds \(t\) into those before arm \(a\) is evicted from \(\mathcal{A}_{\text{master}}\) and those after. Suppose arm \(a\) is evicted from \(\mathcal{A}_{\text{master}}\) at round \(t_{\ell}^{a}\in[t_{\ell},t_{\ell+1})\). In particular, this means arm \(a\in\mathcal{A}_{t}\) for all \(t\in[t_{\ell},t_{\ell}^{a})\). Thus, it suffices to bound:

\[\mathbb{E}\left[\textbf{1}\{\mathcal{E}_{1}\}\cdot\left(\sum_{a=1}^{K}\sum_{t =t_{\ell}}^{t_{\ell}^{a}-1}\frac{\delta_{t}(\hat{a}_{t},a)}{|\mathcal{A}_{t}| }+\sum_{a=1}^{K}\sum_{t=t_{\ell}^{a}}^{t_{\ell+1}-1}\frac{\delta_{t}(\hat{a}_{ t},a)}{|\mathcal{A}_{t}|}\cdot\textbf{1}\{a\in\mathcal{A}_{t}\}\right) \right].\] (10)

Suppose WLOG that \(t_{\ell}^{1}\leq t_{\ell}^{2}\leq\cdots\leq t_{\ell}^{K}\). Then, for each round \(t<t_{\ell}^{a}\) all arms \(a^{\prime}\geq a\) are retained in \(\mathcal{A}_{\text{master}}\) and thus retained in the candidate arm set \(\mathcal{A}_{t}\). Thus, \(|\mathcal{A}_{t}|\geq K+1-a\) for all \(t\leq t_{\ell}^{a}\).

Then, the first double sum in (10) can be bounded by combining our eviction criterion (3) with our concentration bounds Proposition 10. Since arm \(a\) is not evicted from \(\mathcal{A}_{t}\) till round \(t_{\ell}^{a}\), on event \(\mathcal{E}_{1}\) we have for some \(c_{2}>0\):

\[\sum_{t=t_{\ell}}^{t_{\ell}^{a}-1}\delta_{t}(\hat{a}_{t},a)=\sum_{t=t_{\ell}}^ {t_{\ell}^{a}-1}\mathbb{E}[\hat{\delta}_{t}(\hat{a}_{t},a)\mid\mathcal{F}_{t- 1}]\leq c_{2}\log(T)\sqrt{K(t_{\ell}^{a}-t_{\ell})\lor K^{2}}\]

Then, using the fact that \(|\mathcal{A}_{t}|\geq K+1-a\) for all \(t\in[t_{\ell},t_{\ell}^{a})\), we have:

\[\sum_{t=t_{\ell}}^{t_{\ell}^{a}-1}\frac{\delta_{t}(\hat{a}_{t},a)}{|\mathcal{ A}_{t}|}\leq\frac{c_{2}\log(T)\sqrt{K(t_{\ell}^{a}-t_{\ell})\lor K^{2}}}{K+1-a}.\]

Then, summing the above R.H.S. over all arms \(a\), we have on event \(\mathcal{E}_{1}\):

\[\sum_{a=1}^{K}\sum_{t=t_{\ell}}^{t_{\ell}^{a}-1}\frac{\delta_{t}(\hat{a}_{t},a )}{|\mathcal{A}_{t}|}\leq c_{2}\log(K)\log(T)\sqrt{K(t_{\ell+1}-t_{\ell})\lor K ^{2}}.\]

Next, we handle the second double sum in (10). We first observe that if arm \(a\) is played after round \(t_{\ell}^{a}\), then it must due to a scheduled replay. The difficulty here is that replays may interrupt each other and so care must be taken in managing the relative regret contribution \(\sum_{t}\delta_{t}(\hat{a}_{t},a)\) (which may be negative if \(a\prec\hat{a}_{t}\)) of different overlapping replays.

Fixing an arm \(a\), our strategy is to partition the rounds when \(a\) is played by a replay after round \(t_{\ell}^{a}\) according to which replay is active and not accounted for by another replay. This involves carefully designating a subclass of replays whose durations while playing \(a\) span all the rounds where \(a\) is played after \(t_{\ell}^{a}\). Then, we cover the times when \(a\) is played by a collection of intervals corresponding to the schedules of this subclass of replays, on each of which we can employ the eviction criterion (3) and concentration like before.

For this purpose, we define the following terminology (which is all w.r.t. a fixed arm \(a\)):

**Definition 4**.:
1. _For each scheduled and activated_ \(\mathsf{Base\text{-}Alg}(s,m)\)_, let the round_ \(M(s,m)\) _be the minimum of two quantities: (a) the last round in_ \([s,s+m]\) _when arm_ \(a\) _is retained by_ \(\mathsf{Base\text{-}Alg}(s,m)\) _and all of its children, and (b) the last round that_ \(\mathsf{Base\text{-}Alg}(s,m)\) _is active and not permanently interrupted. Call the interval_ \([s,M(s,m)]\) _the_ **active interval** _of_ \(\mathsf{Base\text{-}Alg}(s,m)\)_
2. _Call a replay_ \(\mathsf{Base\mbox{-}Alg}(s,m)\) _proper_ _if there is no other scheduled replay_ \(\mathsf{Base\mbox{-}Alg}(s^{\prime},m^{\prime})\) _such that_ \([s,s+m]\subset(s^{\prime},s^{\prime}+m^{\prime})\) _where_ \(\mathsf{Base\mbox{-}Alg}(s^{\prime},m^{\prime})\) _will become active again after round_ \(s+m\)_. In other words, a proper replay is not scheduled inside the scheduled range of rounds of another replay. Let_ \(\textsc{Proper}(t_{\ell},t_{\ell+1})\) _be the set of proper replays scheduled to start before round_ \(t_{\ell+1}\)_._
3. _Call a scheduled replay_ \(\mathsf{Base\mbox{-}Alg}(s,m)\) _subproper_ _if it is non-proper and if each of its ancestor replays (i.e., previously scheduled replays whose durations have not concluded)_ \(\mathsf{Base\mbox{-}Alg}(s^{\prime},m^{\prime})\) _satisfies_ \(M(s^{\prime},m^{\prime})<s\)_. In other words, a subproper replay either permanently interrupts its parent or does not, but is scheduled after its parent (and all its ancestors) stops playing arm_ \(a\)_. Let_ \(\textsc{SubProper}(t_{\ell},t_{\ell+1})\) _be the set of all subproper replays scheduled before round_ \(t_{\ell+1}\)_._

Equipped with this language, we now show some basic claims which essentially reduce analyzing the complicated hierarchy of replays to analyzing the active intervals of replays in \(\textsc{Proper}(t_{\ell},t_{\ell+1})\cup\textsc{SubProper}(t_{\ell},t_{\ell+1})\).

**Proposition 12**.: _The active intervals_

\[\{[s,M(s,m)]:\mathsf{Base\mbox{-}Alg}(s,m)\in\textsc{Proper}(t_{\ell},t_{\ell +1})\cup\textsc{SubProper}(t_{\ell},t_{\ell+1})\},\]

_are mutually disjoint._

Proof.: Clearly, the classes of replays \(\textsc{Proper}(t_{\ell},t_{\ell+1})\) and \(\textsc{SubProper}(t_{\ell},t_{\ell+1})\) are disjoint. Next, we show the respective active intervals \([s,M(s,m)]\) and \([s^{\prime},M(s^{\prime},m^{\prime})]\) of any two \(\mathsf{Base\mbox{-}Alg}(s,m)\) and \(\mathsf{Base\mbox{-}Alg}(s^{\prime},m^{\prime})\in\textsc{Proper}(t_{\ell},t_ {\ell+1})\cup\textsc{SubProper}(t_{\ell},t_{\ell+1})\) are disjoint.

1. Proper replay vs. subproper replay: a subproper replay can only be scheduled after the round \(M(s,m)\) of the most recent proper replay \(\mathsf{Base\mbox{-}Alg}(s,m)\) (which is necessarily an ancestor). Thus, the active intervals of proper replays and subproper replays are disjoint.
2. Two distinct proper replays: two such replays can only permanently interrupt each other, and since \(M(s,m)\) always occurs before the permanent interruption of \(\mathsf{Base\mbox{-}Alg}(s,m)\), we have the active intervals of two such replays are disjoint.
3. Two distinct subproper replays: consider two non-proper replays \(\mathsf{Base\mbox{-}Alg}(s,m),\mathsf{Base\mbox{-}Alg}(s^{\prime},m^{\prime })\in\textsc{SubProper}(t_{\ell},t_{\ell+1})\) with \(s^{\prime}>s\). The only way their active intervals intersect is if \(\mathsf{Base\mbox{-}Alg}(s,m)\) is an ancestor of \(\mathsf{Base\mbox{-}Alg}(s^{\prime},m^{\prime})\). Then, if \(\mathsf{Base\mbox{-}Alg}(s^{\prime},m^{\prime})\) is subproper, we must have \(s^{\prime}>M(s,m)\), which means that \([s^{\prime},M(s^{\prime},m^{\prime})]\) and \([s,M(s,m)]\) are disjoint.

Next, we claim that the active intervals \([s,M(s,m)]\) for \(\mathsf{Base\mbox{-}Alg}(s,m)\in\textsc{Proper}(t_{\ell},t_{\ell+1})\cup \textsc{SubProper}(t_{\ell},t_{\ell+1})\) contain all the rounds where \(a\) is played after being evicted from \(\mathcal{A}_{\text{master}}\). To show this, we first observe that for each round \(t\) when a replay is active, there is a unique proper

Figure 3: Shown are replay scheduled durations (in gray) with dots marking when arm \(a\) is reintroduced to \(\mathcal{A}_{t}\). Black segments indicate the period \([s,M(s,m)]\) for proper and subproper replays. Note that the rounds where \(a\in\mathcal{A}_{t}\) in the left unlabeled replay’s duration are accounted for by the larger proper replay.

replay associated to \(t\), namely the proper replay scheduled most recently. Next, note that any round \(t>t_{\ell}^{a}\) where arm \(a\in\mathcal{A}_{t}\) must belong to the active interval \([s,M(s,m)]\) of the unique proper replay \(\textsf{Base-Alg}(s,m)\) associated to round \(t\), or else satisfies \(t>M(s,m)\) in which case a unique subproper replay \(\textsf{Base-Alg}(s^{\prime},m)\in\textsc{SubProper}(t_{\ell},t_{\ell+1})\) was active and not yet permanently interrupted by round \(t\). Thus, it must be the case that \(t\in[s^{\prime},M(s^{\prime},m^{\prime})]\).

At the same time, every round \(t\in[s,M(s,m)]\) for a proper or subproper \(\textsf{Base-Alg}(s,m)\) is clearly a round where \(a\in\mathcal{A}_{t}\) and no such round is accounted for twice by Proposition 12. Thus,

\[\{t\in(t_{\ell}^{a},t_{\ell+1}):a\in\mathcal{A}_{t}\}=\bigsqcup_{\textsf{ Base-Alg}(s,m)\in\textsc{Proper}(t_{\ell},t_{\ell+1})\cup\textsc{SubProper}(t_{ \ell},t_{\ell+1})}[s,M(s,m)].\]

Then, we can rewrite the second double sum in (10) as:

\[\sum_{a=1}^{K}\sum_{\textsf{Base-Alg}(s,m)\in\textsc{Proper}(t_{\ell},t_{ \ell+1})\cup\textsc{SubProper}(t_{\ell},t_{\ell+1})}\mathbf{1}\{B_{s,m}=1\} \sum_{t=s\lor t_{\ell}^{a}}^{M(s,m)}\frac{\delta_{t}(\hat{a}_{t},a)}{|\mathcal{ A}_{t}|}.\]

Recall in the above that the Bernoulli \(B_{s,m}\) (see Line 6 of Algorithm 1) decides whether \(\textsf{Base-Alg}(s,m)\) is scheduled.

Further bounding the sum over \(t\) above by its positive part, we can expand the sum over \(\textsf{Base-Alg}(s,m)\in\textsc{Proper}(t_{\ell},t_{\ell+1})\cup\textsc{ SubProper}(t_{\ell},t_{\ell+1})\) to be over all \(\textsf{Base-Alg}(s,m)\), or obtain:

\[\sum_{a=1}^{K}\sum_{\textsf{Base-Alg}(s,m)}\mathbf{1}\{B_{s,m}=1\}\left(\sum _{t=s\lor t_{\ell}^{a}}^{M(s,m)}\frac{\delta_{t}(\hat{a}_{t},a)}{|\mathcal{A}_ {t}|}\cdot\mathbf{1}\{a\in\mathcal{A}_{t}\}\right)_{+},\] (11)

where the sum is over all replay \(\textsf{Base-Alg}(s,m)\), i.e. \(s\in\{t_{\ell}+1,\ldots,t_{\ell+1}-1\}\) and \(m\in\{2,4,\ldots,2^{\lceil\log(T)\rceil}\}\). It then remains to bound the contributed relative regret of each \(\textsf{Base-Alg}(s,m)\) in the interval \([s\lor t_{\ell}^{a},M(s,m)]\), which will follow similarly to the previous steps. Fix \(s,m\) and suppose \(t_{\ell}^{a}+1\leq M(s,m)\) since otherwise \(\textsf{Base-Alg}(s,m)\) contributes no regret in (11).

Then, following similar reasoning as before, i.e. combining our concentration bound (9) with the eviction criterion (3), we have for a fixed arm \(a\):

\[\sum_{t=s\lor t_{\ell}^{a}}^{M(s,m)}\frac{\delta_{t}(\hat{a}_{t},a)}{| \mathcal{A}_{t}|}\leq\frac{c_{2}\log(T)\sqrt{Km\lor K^{2}}}{\min_{t\in[s,M(s,m )]}\,|\mathcal{A}_{t}|},\]

Plugging this into (11) and switching the ordering of the outer double sum, we obtain (now for clarity overloading the notation \(M(s,m,a)\) to also depend on the reference arm \(a\)):

\[\sum_{\textsf{Base-Alg}(s,m)}\mathbf{1}\{B_{s,m}=1\}\cdot c_{2}\log(T)\sqrt {Km\lor K^{2}}\sum_{a=1}^{K}\frac{1}{\min_{t\in[s,M(s,m.a)]}\,|\mathcal{A}_{t} |}.\]

We claim the above innermost sum over \(a\) is at most \(\log(K)\). For a fixed \(\textsf{Base-Alg}(s,m)\), if \(a_{k}\) is the \(k\)-th arm in \([K]\) to be evicted by \(\textsf{Base-Alg}(s,m)\) or any of its children, then \(\min_{t\in[s,M(s,m,a_{k})]}\,|\mathcal{A}_{t}|\geq K+1-k\). Thus, our claim follows follows from \(\sum_{k=1}^{K}\frac{1}{K+1-k}\leq\log(K)\).

Let \(R(m):=c_{2}\log(K)\log(T)\sqrt{Km\lor K^{2}}\) which is the bound we've obtained so far on the relative regret for a single \(\textsf{Base-Alg}(s,m)\). Then, plugging \(R(m)\) into (11) gives:

\[\mathbb{E}\left[\mathbf{1}\{\mathcal{E}_{1}\}\sum_{a=1}^{K}\sum_ {t=t_{\ell}^{a+1}}^{t_{\ell+1}-1}\frac{\delta_{t}(\hat{a}_{t},a)}{|\mathcal{A}_ {t}|}\cdot\mathbf{1}\{a\in\mathcal{A}_{t}\}\right]\leq\mathbb{E}_{t_{\ell}} \left[\mathbb{E}\left[\sum_{\textsf{Base-Alg}(s,m)}\mathbf{1}\{B_{s,m}=1\} \cdot R(m)\mid t_{\ell}\right]\right]\] \[\quad=\mathbb{E}_{t_{\ell}}\left[\sum_{s=t_{\ell}}^{T}\sum_{m} \mathbb{E}[\mathbf{1}\{B_{s,m}=1\}\cdot\mathbf{1}\{s<t_{\ell+1}\}\mid t_{ \ell}]\cdot R(m)\right].\]

Next, we observe that \(B_{s,m}\) and \(\mathbf{1}\{s<t_{\ell+1}\}\) are independent conditional on \(t_{\ell}\) since \(\mathbf{1}\{s<t_{\ell+1}\}\) only depends on the scheduling and observations of base algorithms scheduled before round \(s\). Thus,recalling that \(\mathbb{P}(B_{s,m}=1)=1/\sqrt{m\cdot(s-t_{\ell})}\),

\[\mathbb{E}[\textbf{1}\{B_{s,m}=1\}\cdot\textbf{1}\{s<t_{\ell+1}\} \mid t_{\ell}] =\mathbb{E}[\textbf{1}\{B_{s,m}=1\}\mid t_{\ell}]\cdot\mathbb{E}[ \textbf{1}\{s<t_{\ell+1}\}\mid t_{\ell}]\] \[=\frac{1}{\sqrt{m\cdot(s-t_{\ell})}}\cdot\mathbb{E}[\textbf{1}\{ s<t_{\ell+1}\}\mid t_{\ell}].\]

Plugging this into our expectation from before and unconditioning, we obtain:

\[\mathbb{E}\left[\sum_{s=t_{\ell}+1}^{t_{\ell+1}-1}\sum_{n=1}^{\lceil\log(T) \rceil}\frac{1}{\sqrt{2^{n}\cdot(s-t_{\ell})}}\cdot R(2^{n})\right]\leq c_{3} \log^{3}(T)\mathbb{E}_{t_{\ell},t_{\ell+1}}\left[\sqrt{K(t_{\ell+1}-t_{\ell}) \lor K^{2}}\right].\] (12)

Then, it suffices to bound \(\sqrt{K(t_{\ell+1}-t_{\ell})\lor K^{2}}\). First, we claim that every phase \([\tau_{i},\tau_{i+1})\) is length at least \(K/4\). Observe by our notion of significant regret, that an arm \(a\) incurring significant regret on the interval \([s_{1},s_{2}]\) means

\[\sum_{t=s_{1}}^{s_{2}}\delta_{t}(a_{t}^{*},a)\geq\sqrt{K\cdot(s_{2}-s_{1})} \implies 2\cdot(s_{2}-s_{1})\geq\sqrt{K\cdot(s_{2}-s_{1})}\implies s_{2}-s_{1} \geq K/4.\]

Thus, each significant phase (Definition 3) must be at least \(K/4\) rounds long meaning \(\tau_{i+1}-\tau_{i}=(\tau_{i+1}-\tau_{i})\lor K/4\). This will allow us to remove the "\(\lor K^{2}\)" in (12). In particular, since the episode length \(t_{\ell+1}-t_{\ell}\) in (12) can be upper bounded by the combined length of all significant phases \([\tau_{i},\tau_{i+1})\) intereesecting episode \([t_{\ell},t_{\ell+1})\), (12) gives us the desired bound.

Bounding \(\mathbb{E}[\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(a_{t}^{\sharp},\hat{a} _{t})]\): Regret of Candidate Arm to Safe Arm

We first invoke an elementary lemma based on SST and STI to further help us decompose the regret.

**Lemma 13**.: _For any three arms \(a,b,c\), under SST\(\cap\) STI:_

\[\delta_{t}(a,c)\leq 2\cdot\delta_{t}(a,b)+\delta_{t}(b,c)+3\cdot\delta_{t}(a_{t} ^{*},a),\]

_where \(a_{t}^{*}\) is the winner arm._

Proof.: We handle all the different orderings:

1. \(a\succ_{t}b,c\): this already follows from Lemma 8 since then \(\delta_{t}(a,c)\leq 2\cdot\delta_{t}(a,b)+\delta_{t}(b,c)\).
2. \(c\succ_{t}a\succ_{t}b\): \(\delta_{t}(a,c)\leq 0\leq\delta_{t}(a,b)\) and \(\delta_{t}(a^{*},b)\geq\delta_{t}(c,b)\) by SST. Summing these together gives the result.
3. \(b\succ_{t}a\succ_{t}c\): \(\delta_{t}(a,c)\leq\delta_{t}(b,c)\) and \(\delta_{t}(a_{t}^{*},a)\geq\delta_{t}(b,a)\) by SST. Summing these together gives the result.
4. \(b,c\succ_{t}a\): \(\delta_{t}(a^{*},a)\) dominates the first two terms on the desired inequality's RHS.

Then, using Lemma 13, we further decompose the regret about the _last master arm_\(a_{\ell}\) defined in Section 4, which is the last arm to be evicted from \(\mathcal{A}_{\text{master}}\) in episode \([t_{\ell},t_{\ell+1})\). We have

\[\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(a_{t}^{\sharp},\hat{a}_{t})\leq 2 \sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(a_{t}^{\sharp},a_{\ell})+\sum_{t=t_{ \ell}}^{t_{\ell+1}-1}\delta_{t}(a_{\ell},\hat{a}_{t})+3\sum_{t=t_{\ell}}^{t_{ \ell+1}-1}\delta_{t}(a_{t}^{*},a_{t}^{\sharp}).\] (13)

As said earlier, the sum \(\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(a_{t}^{*},a_{t}^{\sharp})\) is of the right order. Meanwhile, the sum \(\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(a_{\ell},\hat{a}_{t})\) is bounded using our candidate arm switching criterion (4). If \(\hat{a}_{t}=a_{\ell}\) for every round \(t\in[t_{\ell},t_{\ell+1})\) we are already done. Otherwise, let \(m_{\ell}\) be the last round that \(a_{\ell}\) is not the candidate arm \(\hat{a}_{t}\). Then, we must have that since arm \(a_{\ell}\) is not evicted until round \(t_{\ell+1}-1\):

\[\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\hat{\delta}_{t}(a_{\ell},\hat{a}_{t})=\sum_{t =t_{\ell}}^{m_{\ell}-1}\hat{\delta}_{t}(a_{\ell},\hat{a}_{t})\leq C\log(T) \sqrt{K\cdot(m_{\ell}-t_{\ell})\lor K^{2}}\]Then, by concentration (Proposition 10) and the fact from earlier that each phase \([\tau_{i},\tau_{i+1})\) is at least \(K/4\) rounds (so that "\(\lor K^{2}\)" can be removed in the above), we have that \(\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(a_{\ell},\hat{a}_{t})\) is of the right order.

Then, turning back to (13), it remains to bound the regret of \(a_{\ell}\) to \(a_{t}^{\sharp}\) over the episode \([t_{\ell},t_{\ell+1})\).

Bounding \(\mathbb{E}[\sum_{t=t_{\ell}}^{t_{\ell+1}-1}\delta_{t}(a_{t}^{\sharp},a_{\ell})]\): Regret of Last Master Arm to Safe Arm

First, following the outline of Section 4, we recall the definition of the _last safe arm_\(a_{t}^{\sharp}\) at round \(t\) which is the last arm to incur significant regret in the unique phase \([\tau_{i},\tau_{i+1})\) containing round \(t\).

We next formally define a bad segment, alluded to in Section 4. In what follows, bad segments will be defined with respect to a fixed arm \(a\) and conditional on the episode start time \(t_{\ell}\). We will then show that, with respect to any arm \(a\), not too many bad segments will elapse before \(a\) is evicted from \(\mathcal{A}_{\text{master}}\). In particular, this will hold for \(a=a_{\ell}\) which will ultimately be used to bound \(\delta_{t}(a_{t}^{\sharp},a_{\ell})\) across the episode \([t_{\ell},t_{\ell+1})\).

**Definition 5**.: _Fix the episode start time \(t_{\ell}\), and let \([\tau_{i},\tau_{i+1})\) be any phase intersecting \([t_{\ell},T)\). For any arm \(a\), define rounds \(s_{i,0}(a),s_{i,1}(a),s_{i,2}(a)\ldots\in[t_{\ell}\lor\tau_{i},\tau_{i+1})\) recursively as follows: let \(s_{i,0}(a):=t_{\ell}\lor\tau_{i}\) and define \(s_{i,j}(a)\) as the smallest round in \((s_{i,j-1}(a),\tau_{i+1})\) such that arm \(a\) satisfies for some fixed \(c_{4}>0\):_

\[\sum_{t=s_{i,j-1}(a)}^{s_{i,j}(a)}\delta_{t}(a_{t}^{\sharp},a)\geq c_{4}\log( T)\sqrt{K\cdot(s_{i,j}(a)-s_{i,j-1}(a))},\] (14)

_if such a round \(s_{i,j}(a)\) exists. Otherwise, we let the \(s_{i,j}(a):=\tau_{i+1}-1\). We refer to any interval \([s_{i,j-1}(a),s_{i,j}(a))\) as a_ **critical segment**_, and as a_ **bad segment** (w.r.t. arm \(a\)) if (14) above holds._

Note that the above definition only depends on the arm \(a\) and the episode start time \(t_{\ell}\) and that, conditional on these variables, they are fixed in the environment. Observe also that the arm \(a_{t}^{\sharp}\) is fixed within any critical segment \([s_{i,j-1}(a),s_{i,j}(a))\subseteq[\tau_{i},\tau_{i+1})\) since a significant shift does not occur inside \([\tau_{i},\tau_{i+1})\).

Now relating this notion of a bad segment to our goal of bounding regret, a given bad segment \([s_{i,j}(a),s_{i,j}(a))\) only contributes order \(\sqrt{K\cdot(s_{i,j}(a)-s_{i,j-1}(a))}\) to the regret of \(a\) to \(a_{t}^{\sharp}\). At the same time, we claim that a well-timed replay (see Definition 6 below) running from \(s_{i,j-1}(a)\) to \(s_{i,j}(a)\) is capable of evicting arm \(a\). This in turn allows us to reduce the regret bounding to studying the number and lengths of bad segments which elapse before one is detected by such a replay.

We first define such a well-timed and _perfect_ replay.

**Definition 6**.: _Let \(\tilde{s}_{i,j}(a):=[\frac{s_{i,j}(a)+s_{i,j+1}(a)}{2}]\) denote the approximate midpoint of \([s_{i,j}(a),s_{i,j+1}(a))\). Given a bad segment \([s_{i,j}(a),s_{i,j+1}(a))\), define a_ **perfect replay** _w.r.t. \([s_{i,j}(a),s_{i,j+1}(a))\) as a call of \(\mathsf{Base\_Alg}(t_{\text{start}},m)\) where \(t_{\text{start}}\in[s_{i,j}(a),\tilde{s}_{i,j}(a)]\) and \(m\geq s_{i,j+1}(a)-s_{i,j}(a)\)_

Next, we analyze the behavior of a perfect replay on the bad segment \([s_{i,j}(a),s_{i,j+1}(a))\). Going forward, we will use the simpler notation \(a_{i}^{\sharp}\) to denote the last safe arm of a phase \([\tau_{i},\tau_{i+1})\), known in context.

**Proposition 14**.: _Suppose the good event \(\mathcal{E}_{1}\) holds (cf. Proposition 10). Let \([s_{i,j}(a),s_{i,j+1}(a))\) be a bad segment with respect to arm \(a\). Fix an integer \(m\geq s_{i,j+1}(a)-s_{i,j}(a)\). Then, if a perfect replay with respect to \([s_{i,j}(a),s_{i,j+1}(a))\) is scheduled, arm \(a\) will be evicted from \(\mathcal{A}_{\text{master}}\) by round \(s_{i,j+1}(a)\)._

Proof.: Suppose event \(\mathcal{E}_{1}\) (i.e., our concentration bound (9)) holds. We first observe that by elementary calculations and the definition of the rounds \(s_{i,j}(a)\), we have (in an identical fashion to Lemma 4 of Suk and Kpotufe (2022)):

\[\sum_{t=\tilde{s}_{i,j}(a)}^{s_{i,j+1}(a)}\delta_{t}(a_{i}^{\sharp},a)\geq\frac {c_{4}}{4}\log(T)\sqrt{K\,(s_{i,j+1}(a)-\tilde{s}_{i,j}(a))},\] (15)where \(\tilde{s}_{i,j}(a)\) is the midpoint of \([s_{i,j}(a),s_{i,j+1}(a))\) as defined in Definition 6. The above will come in handy in showing arm \(a\) is evicted over the second half of the bad segment \([\tilde{s}_{i,j}(a),s_{i,j+1}(a)]\).

Next, following the intuition given in Section 4, in order to relate \(\delta_{t}(a_{i}^{\sharp},a)\) to \(\delta_{t}(\hat{a}_{t},a)\), we again use SST and STI via Lemma 13 on inequality (15):

\[\sum_{t=\tilde{s}_{i,j}(a)}^{s_{i,j+1}(a)}2\cdot\delta_{t}(a_{i}^{\sharp},\hat {a}_{t})+\delta_{t}(\hat{a}_{t},a)+3\cdot\delta_{t}(a_{t}^{\star},a_{i}^{ \sharp})\geq\frac{c_{4}}{4}\log(T)\sqrt{K\left(s_{i,j+1}(a)-\tilde{s}_{i,j}(a) \right)}.\] (16)

We next show that \(\sum_{t=\tilde{s}_{i,j}(a)}^{s_{i,j+1}(a)}\delta_{t}(a_{i}^{\sharp},\hat{a}_{ t})\) and \(\sum_{t=\tilde{s}_{i,j}(a)}^{s_{i,j+1}(a)}\delta_{t}(a_{t}^{\star},a_{i}^{ \sharp})\) on the above LHS are small.

First, it is clear that any perfect replay \(\textsf{Base-Alg}(t_{\text{start}},m)\) will not evict \(a_{i}^{\sharp}\) since otherwise it incurs significant regret within phase \([\tau_{i},\tau_{i+1})\) (see the earlier Lemma 11). At the same time, by the candidate arm switching criterion (4) and concentration:

\[\sum_{t=\tilde{s}_{i,j}(a)}^{s_{i,j+1}(a)}\delta_{t}(a_{i}^{\sharp},\hat{a}_{ t})\leq c_{5}\log(T)\sqrt{K\left(s_{i,j+1}(a)-\tilde{s}_{i,j}(a)\right)}.\]

Meanwhile, by the definition of significant regret (Definition 3),

\[\sum_{t=\tilde{s}_{i,j}(a)}^{s_{i,j+1}(a)}\delta_{t}(a_{t}^{\star},a_{i}^{ \sharp})\geq\sqrt{K\left(s_{i,j+1}(a)-\tilde{s}_{i,j}(a)\right)}.\]

Thus, for sufficiently large \(c_{4}>0\) in the definition of bad segments (Definition 5), we have that the above two inequalities can be combined with (16) to yield:

\[\sum_{t=\tilde{s}_{i,j}(a)}^{s_{i,j+1}(a)}\delta_{t}(\hat{a}_{t},a)\geq\sqrt{ K\left(s_{i,j+1}(a)-\tilde{s}_{i,j}(a)\right)}.\]

If arm \(a\) is evicted from \(\mathcal{A}_{\text{master}}\) before round \(s_{i,j+1}(a)\), then we are already done. Otherwise, using the fact that \(\mathbb{E}[\hat{\delta}_{t}(\hat{a}_{t},a)|\mathcal{F}_{t-1}]=\delta_{t}(\hat{ a}_{t},a)\) for any round \(t\in[\tilde{s}_{i,j}(a),s_{i,j+1}(a)]\) with \(a\in\mathcal{A}_{t}\), we have that arm \(a\) will be evicted at round \(s_{i,j+1}(a)\) using the above inequality and concentration. 

It remains to show that, for any arm \(a\), a perfect replay is scheduled w.h.p. before too much regret is incurred on the elapsed bad segments w.r.t. \(a\). In particular, this will hold for the last master arm \(a_{\ell}\), allowing us to bound the remaining term \(\mathbb{E}[\sum_{t=\ell_{\ell}}^{t_{t+1}-1}\delta_{t}(a_{t}^{\sharp},a_{\ell})]\). The argument will be identical to that of Appendix B.2 of Suk and Kpotufe (2022).

First, fix an arm \(a\) and an episode start time \(t_{\ell}\). Then, define the _bad round_\(s(a)>t_{\ell}\) as follows:

**Definition 7**.: _(bad round) For a fixed round \(t_{\ell}\) and arm \(a\), the_ **bad round**_\(s(a)>t_{\ell}\) is defined as the smallest round which satisfies, for some fixed \(c_{6}>0\):_

\[\sum_{(i,j)}\sqrt{s_{i,j+1}(a)-s_{i,j}(a)}>c_{6}\log(T)\sqrt{s(a)-t_{\ell}},\] (17)

_where the above sum is over all pairs of indices \((i,j)\in\mathbb{N}\times\mathbb{N}\) such that \([s_{i,j}(a),s_{i,j+1}(a))\) is a bad segment with \(s_{i,j+1}(a)<s(a)\)._

Our goal is then to then to show that arm \(a\) is evicted by some perfect replay scheduled within episode \([t_{\ell},t_{\ell+1})\) with high probability before the bad round \(s(a)\) occurs. Going forward, to simplify notation we will drop the dependence on the fixed arm \(a\) in some variables.

For each bad segment \([s_{i,j}(a),s_{i,j+1}(a))\), recall that \(\tilde{s}_{i,j}(a)\) is the approximate midpoint between \(s_{i,j}(a)\) and \(s_{i,j+1}(a)\) (see Definition 6). Next, let \(m_{i,j}:=2^{n}\) where \(n\in\mathbb{N}\) satisfies:

\[2^{n}\geq s_{i,j+1}(a)-s_{i,j}(a)>2^{n-1}.\]

Plainly, \(m_{i,j}\) is a dyadic approximation of the bad segment length. Next, recall that the Bernoulli \(B_{t,m}\) decides whether \(\textsf{Base-Alg}(t,m)\) is scheduled at round \(t\) (see Line 6 of Algorithm 1). If for 

[MISSING_PAGE_FAIL:23]