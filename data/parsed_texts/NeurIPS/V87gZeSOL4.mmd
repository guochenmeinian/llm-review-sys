# Nonparametric Identifiability of Causal Representations from Unknown Interventions

Julius von Kuglegen\({}^{1,2}\)

Michel Besserve\({}^{1}\)

Liang Wendong\({}^{1}\)

Luigi Gresele\({}^{1}\)

Armin Kekic\({}^{1}\)

Elias Bareinboim\({}^{3}\)

David M. Blei\({}^{3}\)

Bernhard Scholkopf\({}^{1}\)

###### Abstract

We study causal representation learning, the task of inferring latent causal variables and their causal relations from high-dimensional functions ("mixtures") of the variables. Prior work relies on weak supervision, in the form of counterfactual pre- and post-intervention views or temporal structure; places restrictive assumptions, such as linearity, on the mixing function or latent causal model; or requires partial knowledge of the generative process, such as the causal graph or intervention targets. We instead consider the general setting in which both the causal model and the mixing function are _nonparametric_. The learning signal takes the form of multiple datasets, or environments, arising from _unknown interventions_ in the underlying causal model. Our goal is to identify both the ground truth latents and their causal graph up to a set of ambiguities which we show to be irresolvable from interventional data. We study the fundamental setting of two causal variables and prove that the observational distribution and one perfect intervention per node suffice for identifiability, subject to a genericity condition. This condition rules out spurious solutions that involve fine-tuning of the intervened and observational distributions, mirroring similar conditions for nonlinear cause-effect inference. For an arbitrary number of variables, we show that at least one pair of distinct perfect interventional domains per node guarantees identifiability. Further, we demonstrate that the strengths of causal influences among the latent variables are preserved by all equivalent solutions, rendering the inferred representation appropriate for drawing causal conclusions from new data. Our study provides the first identifiability results for the general nonparametric setting with unknown interventions, and elucidates what is possible and impossible for causal representation learning without more direct supervision.

## 1 Introduction

Causal representation learning (CRL) seeks to describe high-dimensional, low-level observations through a small number of interpretable, causally-related latent variables [108, 110]. In doing so, its goal is to combine the strengths of classical causal inference with those of modern machine learning. A causal model represents an entire family of distributions arising from _interventions_ on a system of variables [10, 95, 100]. This provides a principled way for reasoning about distribution shifts, which facilitates out-of-distribution generalization and planning [9, 64, 96, 102]. However, causal models require that most (or at least some) relevant causal variables are directly observed. While reasonable in domains such as economics [7], social [91] or biomedical science [48, 57], this assumption has challenged the application of causal methodology to complex and high-dimensional data [85]. Machine learning, on the other hand, has proven successful at learning useful "representations"--latent vectors generating the observables via some nonlinear map--of high-dimensional data such as images, video, or text [16, 20, 77, 106]. However, most methods rely on independent and identically distributed (i.i.d.) data and only extract _associated_ information. As a consequence, they often fail under distribution shifts and do not generalize beyond the training distribution [107], as exhibited by their reliance on _spurious correlations_[13, 87] or their vulnerability to _adversarial examples_[40, 119].

**Identifiability.** It has been argued that to address these shortcomings, we ought to learn representations endowed with causal model semantics. However, a major challenge to this goal is that _different representations can explain the same data equally well_. Strictly simpler representation learning tasks, such as disentanglement [14] or independent component analysis (ICA) [53], are already _non-identifiable_ in general [54, 83]. Identifiability studies are thus required to characterize additional assumptions under which the desired latent variables can be provably recovered [4, 17, 21, 31, 42, 43, 45, 46, 51, 52, 55, 56, 66, 69, 70, 75, 76, 84, 90, 101, 112, 114, 127, 141]. In CRL, we need not only identify the latents, but also the causal graph encoding their relations. Even in the fully observed case, this task of causal discovery, or structure learning, is very challenging: the graph can only be recovered up to Markov equivalence [115] based only on (observational) i.i.d. data [116], meaning that the direction of some edges cannot be determined. For CRL, the task gets strictly harder. For instance, if the causal latents \(V_{i}\) in Fig. 1 (_Left_) form a valid representation, then replacing them by the _independent_ exogeneous \(U_{i}\) might be considered an equally valid alternative.

**What sets _causal_ representations apart?** A crucial feature of causal variables is that they are the ones on which _interventions_ are defined and whose relations we are interested in [95]. Causal discovery and CRL thus often rely on non-i.i.d. data linked to interventions on the underlying causal variables [59, 72]. Unless all variables are subject to intervention, however, some fundamental differences between the fully observed and representation learning settings in the level of ambiguity in the graph remain [117], as illustrated in Fig. 1 (_Right_). In a sense, the non-identifiabilities of representation and structure learning combine, and both need to be addressed in conjunction.

**Problem Setting.** We study the general _nonparametric_ CRL problem (SS 2.1) in which both the causal mechanisms and the mixing function are completely unconstrained. Our goal (SS 2.2) is to identify the latent causal variables up to element-wise nonlinear rescaling and their graph up to isomorphism (Defn. 2.6). As motivated above, doing so without further supervision requires access to interventional data. To this end, we consider learning from heterogeneous data from multiple related domains, or _environments_, that arise from interventions in a shared underlying causal model (SS 2.3).

**Contributions.** Our main contributions are theoretical in nature (SS 3). First, we establish the minimality of the targeted equivalence class (Prop. 3.1) in the sense that its ambiguities cannot be resolved from interventional data. We then present our main identifiability results. For the case of two latent causal variables, we show that an observational environment and one for each perfect intervention on either variable suffice (Thm. 3.2)--provided that the intervened and unintervened mechanisms are not "fine-tuned" to each other, which we formalize in the form of a _genericity condition_ (3.2). For any number of latent causal variables, we prove that access to pairs of environments corresponding to two distinct perfect interventions on each node guarantees identifiability (Thm. 3.4). We then question how to use or interpret causal representations (SS 4), and show that certain quantities, such as the strengths of causal influences among variables, are preserved by all equivalent solutions (Thm. 4.2). We sketch possible learning objectives (SS 5), and empirically investigate training different generative models (SS 6), finding that only those based on the correct causal structure attain the best fit and identify the ground truth. We conclude by discussing limitations and extensions of our work (SS 7).

Figure 1: _(Left)_ **Data-Generating Process for Causal Representation Learning (CRL).** Observations \(\bm{X}\) are generated by applying a nonlinear mixing function \(f\) to a set of causal latent variables \(\bm{V}=\{V_{1},...,V_{n}\}\), which are related through a structural causal model (SCM) with independent exogenous variables \(\bm{U}=\{U_{1},...,U_{n}\}\); illustration by Ana Martin Larraafaga. _(Right)_**Comparison to Structure Learning.** The causal direction between two unconfounded _observed_ variables (top) is uniquely identified from a single intervention [37]; for CRL (bottom), this is not the case, as the nonlinear mixing introduces additional ambiguity due to spurious representations. Shaded nodes are observed, white ones unobserved, and interventions highlighted as red diamonds.

Related Work on _Multi-Environment_ CRL.Most closely related are recent identifiability studies which also leverage multiple environments arising from single node interventions [5; 22; 117; 122], thus mirroring in different interventional setups the result of Brehmer et al. [18] based on counterfactual, multi-view data, which is harder to obtain. Squires et al. [117] provide results for linear causal models and linear mixing; Ahuja et al. [5] consider nonlinear causal models and polynomial mixings, subject to additional constraints on the latent support [125]; and Varici et al. [122] employ a score-based approach for nonlinear causal models and linear mixing. The concurrent work of Buchholz et al. [22] extends the results of Squires et al. [117] to general nonlinear mixings and linear Gaussian causal models. Liu et al. [82] leverage recent advances in nonlinear ICA [55; 65] to identify a linear Gaussian causal model with context-dependent weights and nonlinear mixing from sufficiently diverse environments. A more extensive discussion of other related works and a detailed comparison of existing results with the present study are provided in Appx. A and Tab. 1.

## 2 Nonparametric Causal Representation Learning

In this section, we describe the considered problem setting and state our main assumptions. First, we specify the assumed data generating process (SS 2.1) and learning task in the form of a target identifiability class (SS 2.2). We then demonstrate the hardness of our task from i.i.d. data or imperfect interventions and use this to motivate a multi-environment approach with perfect interventions (SS 2.3).

**Notation.** We use upper-case \(X\) for random variables and lower-case \(x\) for their realizations. Bold uppercase \(\bm{X}\) denotes random vectors and lowercase \(\bm{x}\) their realizations. We assume throughout that all distributions \(P_{X}\) possess densities \(p(x)\) with respect to (w.r.t.) the Lebesgue measure. We denote the pushforward of \(P_{X}\) through a measurable function \(f\) by \(f_{*}(P_{X})\), and write \([n]=\{1,...,n\}\).

### Data Generating Process

The assumed data generating process consists of a latent causal model and a mixing function, see Fig. 1 (_left_). For the former, we build on the structural causal model (SCM) framework [95; 100].

**Definition 2.1** (Latent SCM).: Let \(\bm{V}=\{V_{1},...,V_{n}\}\) denote a set of causal "endogenous" variables, with each \(V_{i}\) taking values in \(\mathbb{R}\), and let \(\bm{U}=\{U_{1},...,U_{n}\}\) denote a set of mutually independent "exogenous" random variables. The latent SCM consists of a set of structural equations

\[\{V_{i}:=f_{i}(\bm{V}_{\mathrm{pa}(i)},U_{i})\}_{i=1}^{n}.\] (2.1)

where \(\bm{V}_{\mathrm{pa}(i)}\subseteq\bm{V}\setminus\{V_{i}\}\) are the direct causes, or causal parents, of \(V_{i}\), and \(f_{i}\) are deterministic functions; and a fully factorized joint distribution \(P_{U}\) over the exogenous variables. The associated causal diagram \(G\), a directed graph with vertices \(\bm{V}\) and edges \(V_{j}\to V_{i}\) iff. \(V_{j}\in\bm{V}_{\mathrm{pa}(i)}\), is assumed acyclic.

By acyclicity, recursive substitution of the assignments in (2.1) yields the reduced form \(\bm{V}{=}f_{\mathrm{RF}}(\bm{U})\). The SCM thus induces a unique distribution \(P_{\bm{V}}\) over the endogenous variables, given by the pushforward of \(P_{\bm{U}}\) via (2.1), that is, \(P_{\bm{V}}=f_{\mathrm{RF}^{*}}(P_{\bm{U}})\). By construction, \(P_{\bm{V}}\) is Markovian w.r.t. the causal graph \(G\)[95; 100], meaning that its density obeys the causal Markov factorization:

\[p(v_{1},\ldots,v_{n})=\prod_{i=1}^{n}p_{i}(v_{i}\mid\bm{v}_{\mathrm{pa}(i)}).\] (2.2)

We place the following additional assumption on the distribution \(P_{\bm{V}}\) induced by the SCM.

**Assumption 2.2** (Faithfulness).: The _only_ conditional independence relations satisfied by \(P_{\bm{V}}\) are those implied by \(\{V_{i}\perp\bm{V}_{\mathrm{nd}(i)}\mid\bm{V}_{\mathrm{pa}(i)}\}_{i\in[n]}\), where \(\bm{V}_{\mathrm{nd}(i)}\) denotes the non-descendants of \(V_{i}\) in \(G\).

Asm. 2.2 ensures a one-to-one correspondence between (conditional) independence in \(P_{\bm{V}}\) and graphical separation in \(G\) and is a standard assumption in causal discovery [115]. Faithfulness rules out cancellations of influences along different paths, which occurs with probability zero for random path-coefficients [121]. It can thus also be viewed as a minimality or genericity assumption.

In contrast to classical causal inference, we assume that both the exogenous variables \(\bm{U}\) and the endogenous causal variables \(\bm{V}\) are unobserved. Instead, we will only have access to \(d\)-dimensional nonlinear mixtures \(\bm{X}\) of \(\bm{V}\). We therefore make the following additional assumption.

**Assumption 2.3** (Known \(n\)).: The number \(n\) of latent causal variables is known.

Next, we specify the relationship between the unobserved causal variables \(\bm{V}\) and the observed \(\bm{X}\).

**Definition 2.4** (Mixing function).: The observations \(\bm{X}\) are deterministically generated from \(\bm{V}\) by applying a mixing function \(f:\mathbb{R}^{n}\to\mathbb{R}^{d}\) to \(\bm{V}\), that is, \(\bm{X}:=f(\bm{V})\).

The terminology and setting of a deterministic mixing is rooted in the nonlinear ICA literature [56]. In deep generative models, it is commonly relaxed by considering additive noise in Defn. 2.4[65; 90]. For representation learning scenarios, we are particularly interested in the case \(n\ll d\). To allow for recovery of \(\bm{V}\) from \(\bm{X}\), we assume that \(f\) is invertible, which is a standard assumption for identifiability.

**Assumption 2.5** (Diffeomorphic mixing).: \(f\) is a diffeomorphism1 onto its image \(\operatorname{Im}(f)=\mathcal{X}\subseteq\mathbb{R}^{d}\).

Footnote 1: A diffeomorphism is a bijective function \(f\) such that both \(f\) and \(f^{-1}\) are continuously differentiable.

### Learning Target: The CRL Identifiability Class \(\sim_{\textsc{CRL}}\)

Our goal is to infer the underlying latent causal variables \(\bm{V}=f^{-1}(\bm{X})\) and their causal relations. We therefore consider the true unmixing function \(f^{-1}\)_and_ the causal graph \(G\) our joint learning target: \(f^{-1}\) informs us how to map observations \(\bm{X}\) to causal variables \(\bm{V}\), and \(G\) tells us how to factorise the implied joint \(p(\bm{v})\) into the causal mechanisms \(p_{i}(v_{i}\mid\bm{v}_{\operatorname{pa}(i)})\) from (2.2). Given only observations of \(\bm{X}\), this is a challenging task since neither \(\bm{V}\) nor \(G\) are directly observed or known a priori.

When is a candidate solution \((h,G^{\prime})\) that satisfies a given learning objective (such as maximizing the likelihood, possibly subject to additional constraints) guaranteed to match the ground truth \((f^{-1},G)\)? This is the subject of identifiability studies and the main focus of our work. A statistical model \(\mathcal{P}=\{p_{\theta}:\theta\in\Theta\}\) with parameter space \(\Theta\) is identifiable if the mapping \(\theta\mapsto p_{\theta}\) is injective [78]. For representation learning tasks, full identifiability is often not attainable, as there are some fundamental ambiguities that cannot be resolved. One therefore typically instead considers identifiability up to an appropriately chosen equivalence class in the model space [2; 65; 127].

For the assumed data generating process (SS 2.1), the order of the causal variables is arbitrary, since \(\bm{V}\) is unobserved. We can therefore assume without loss of generality (w.l.o.g.) that the \(V_{i}\)'s are partially ordered w.r.t. \(G\), that is, \(V_{i}\to V_{j}\implies i<j\).2 Learning \(G\) thus reduces to inferring whether the edges \(\{V_{1},\ldots,V_{i-1}\}\to V_{i}\) exist for \(i=2,\ldots,n\). The only remaining permutation ambiguity arises from permutations \(\pi\) that preserve the partial order: for example, if \(G\) is given by \(V_{1}\to V_{3}\gets V_{2}\), the order of \(V_{1}\) and \(V_{2}\) cannot be uniquely determined without further assumptions. Moreover, the scaling of the causal variables is also arbitrary: any invertible element-wise transformation can be undone as part of \(f\). We therefore define the desired identifiability class through the following equivalence relation.3

Footnote 2: If they were not in such order to begin with, we could apply an appropriate permutation \(\tilde{\pi}\) to \(\bm{V}\) and incorporate the inverse permutation into the unknown mixing \(f\) without affecting \(\bm{X}=f(\bm{V})=(f\circ\tilde{\pi}^{-1})(\tilde{\pi}(\bm{V}))\)[117].

**Definition 2.6** (\(\sim_{\textsc{CRL}}\)-identifiability).: Let \(\mathcal{H}\) be a space of unmixing functions \(h:\mathcal{X}\to\mathbb{R}^{n}\) and let \(\mathcal{G}\) be the space of DAGs over \(n\) vertices. Let \(\sim_{\textsc{CRL}}\) be the equivalence relation on \(\mathcal{H}\times\mathcal{G}\) defined as

\[(h_{1},G_{1})\sim_{\textsc{CRL}}(h_{2},G_{2})\quad\iff\quad(h_{2},G_{2})=(\bm {P}_{\pi^{-1}}\circ\phi\circ h_{1},\pi(G_{1}))\] (2.3)

for some element-wise diffeomorphism \(\phi(\bm{v})=(\phi_{1}(v_{1}),\ldots,\phi_{n}(v_{n}))\) of \(\mathbb{R}^{n}\) and a permutation \(\pi\) of \([n]\) such that \(\pi:G_{1}\mapsto G_{2}\) is a graph isomorphism and \(\bm{P}_{\pi}\) the corresponding permutation matrix.

_Remark 2.7_.: When \(G\) has no edges, any permutation is admissible and \(\sim_{\textsc{CRL}}\) reduces to the standard notion of identifiability up to permutation and element-wise reparametrisation of nonlinear ICA [56].

The ground truth \((f^{-1},G)\) is identified up to \(\sim_{\textsc{CRL}}\) by a given learning objective if any candidate solution \((h,G^{\prime})\) satisfies \((h,G^{\prime})\sim_{\textsc{CRL}}(f^{-1},G)\). We seek to discover suitable conditions that ensure this.

### Multi-Environment Data

Given only a single dataset of i.i.d. observations from \(P_{\bm{X}}\), there is no hope for \(\sim_{\textsc{CRL}}\)-identifiability. Even for observed \(\bm{V}\) (i.e., with \(n=d\) and known \(f=\operatorname{id}\)), \(G\) can only be identified up to Markov equivalence [115]. With unknown mixing \(f\), the degree of observational non-identifiability gets even worse: for example, by using the reduced form of the SCM (SS 2.1) we can express \(\bm{X}\) in terms of the latent exogenous variables \(\bm{U}\) via \(\bm{X}=f(\bm{V})=f\circ f_{\textsc{RF}}(\bm{U})\)[108]. This gives rise to a "spurious ICA solution" \((f_{\textsc{RF}}^{-1}\circ f^{-1},G_{\textsc{ICA}})\) where \(G_{\textsc{ICA}}\) denotes the empty graph with independent components. Due to the non-identifiability of nonlinear ICA [54; 83], however, we cannot even learn the composition \(f\circ f_{\textsc{RF}}\), let alone separate it into its constituents \(f\) and \(f_{\textsc{RF}}\) to isolate the intermediate causal variables \(\bm{V}\).

Motivated by these challenges to identifiability from i.i.d. data, we instead consider learning from _multiple environments_\(e\in\mathcal{E}\). That is, we assume access to heterogenous data from multiple distinct distributions \(P_{\bm{X}}^{e}\). Environments can arise, for example, from different experimental settings or correspond to broader contexts such as climate or time. Previous work has shown that this setting can, in principle, provide useful causal learning signals [5, 8, 19, 32, 34, 47, 50, 59, 71, 72, 73, 82, 89, 98, 99, 102, 104, 109, 117, 120, 122]. However, multi-environment data is not necessarily useful if the corresponding distributions \(P_{\bm{X}}^{e}\) are allowed to differ in arbitrary ways. What makes this setting interesting is the assumption that certain parts of the causal generative process are shared across environments.

Here, we assume that all environments share the same invariant mixing function and underlying SCM, and that any distribution shifts arise from interventions on some of the causal mechanisms.4 General interventions can be modelled in SCMs by replacing some of the equations in (2.1) by new assignments \(V_{i}:=\tilde{f}_{i}(\bm{V}_{\text{pa}(i)},\tilde{U}_{i})\), resulting in "intervened" mechanisms \(\tilde{p}_{i}(v_{i}\mid\bm{v}_{\text{pa}(i)})\) replacing the corresponding conditionals \(p_{i}(v_{i}\mid\bm{v}_{\text{pa}(i)})\) in (2.2) [29, 36, 95]. We summarise this as follows.

Footnote 4: One could also say that there exists only one environment and each dataset corresponds to an intervention.

**Assumption 2.8** (Shared mixing and mechanisms).: Each environment \(e\) shares the same mixing \(f\),

\[P_{\bm{X}}^{e}=f_{*}(P_{\bm{V}}^{e})\]

and each \(P_{\bm{V}}^{e}\) results from the same SCM through an intervention on a subset of mechanisms \(\mathcal{I}^{e}\subseteq[n]\):

\[p^{e}(\bm{v})=p^{e}(v_{1},...,v_{n})=\prod_{i\in\mathcal{I}^{e}}p_{i}^{e} \left(v_{i}\mid\bm{v}_{\text{pa}(i)}\right)\prod_{j\in[n]\setminus\mathcal{I }^{e}}p_{j}\left(v_{j}\mid\bm{v}_{\text{pa}(j)}\right)\,.\]

Importantly, the intervention targets \(\mathcal{I}^{e}\) are not assumed to be known.

An intervention on some \(V_{i}\) that fully removes the influence from its parents \(\bm{V}_{\text{pa}(i)}\) is referred to as _perfect_, otherwise as _imperfect_. It has been shown that imperfect interventions are generally insufficient for full identifiability [18], even in the linear case [117]. This is intuitive: if arbitrary imperfect interventions were allowed, including ones which preserve \(f_{i}(\bm{V}_{\text{pa}(i)},\cdot)\) and only replace \(U_{i}\) with some new \(\tilde{U}_{i}\), then the spurious ICA solution \((f_{\bm{V}^{e}}^{-1}\circ f^{-1},G_{\text{ICA}})\) should be indistinguishable from the ground truth. In line with prior work [5, 18, 117, 112], we therefore assume perfect interventions.

**Assumption 2.9** (Perfect interventions).: For all \(e\in\mathcal{E}\) and \(i\in\mathcal{I}^{e}\), we have \(p^{e}(v_{i}\mid\bm{v}_{\text{pa}(i)})=p^{e}(v_{i})\).

Based on the principle of independent causal mechanisms [100, 109], the _sparse mechanism shift hypothesis_[98, 110] posits that distribution changes tend to manifest themselves in a sparse or local way in the causal factorization. In this spirit, we will assume single-node ("atomic") interventions, \(|\mathcal{I}^{e}|=1\), for our main results, as also required for existing results [5, 18, 117, 122].

As motivated in SS 2.2, given data from \(\{P_{\bm{X}}^{e}\}_{e\in\mathcal{E}}\) we consider candidate solutions of the form \((h,G^{\prime})\) where \(h\) is an unmixing function, or encoder, which maps observations \(\bm{X}\) to the inferred latents \(\bm{Z}=h(\bm{X})\), and \(G^{\prime}\) is causal graph capturing the relations among the \(Z_{i}\). The corresponding distributions of the inferred latents are thus given by the push-forward

\[Q_{\bm{Z}}^{e}=h_{*}(P_{\bm{X}}^{e})=(h\circ f)_{*}P_{\bm{V}}^{e}\]

The multi-environment setup with unknown single-node perfect interventions is illustrated in Fig. 2.

Figure 2: **Multi-Environment Setup with Single-Node Perfect Interventions and Shared Mixing Function.** Illustration of the considered multi-environment setup for \(n=2\) causal variables \(\bm{V}=\{V_{1},V_{2}\}\) with graph \(G\) given by \(V_{1}\to V_{2}\), shared mixing function \(f\), and environments \(\mathcal{E}=\{e_{0},e_{1},e_{2}\}\), corresponding to the observational setting \((e_{0})\) and perfect stochastic interventions on \(V_{1}\) (in \(e_{1}\)) and \(V_{2}\) (in \(e_{2}\)). The learnt unmixing function, or encoder, is denoted by \(h\), and the inferred latent representation by \(\bm{Z}=h(\bm{X})\). The corresponding inferred latent distributions \(Q_{\bm{Z}}^{e_{\bm{Z}}}=h_{*}(P_{\bm{X}}^{e})\) are Markovian w.r.t. the candidate graph \(G^{\prime}\) (here, equal to \(G\)). Since the intervention targets are not known, they may in principle differ in \(Q_{\bm{Z}}^{e}\) as shown here. However, as we prove in Thm. 3.2, such misalignment is only possible if a certain genericity condition (3.2) is violated.

Identifiability Theory

We start by showing that identifiability up to \(\sim_{\text{\tiny CRL}}\) is, in fact, the best we can hope for when learning from interventional data, without any more direct forms of supervision. To this end, we state the following result, which is presented more formally and proven in Appx. B.

**Proposition 3.1** (Minimality of \(\sim_{\text{\tiny CRL}}\); informal).: _Let \(\bm{Z}\) be any representation that is \(\sim_{\text{\tiny CRL}}\) equivalent to \(\bm{V}\), with \(G^{\prime}=\pi(G)\) the associated DAG. Then for any intervention on \(\bm{V}_{\bm{Z}}\subseteq\bm{V}\) in \(G\), there exists an equally sparse intervention on \(\bm{Z}_{\pi(\mathcal{I})}\subseteq\bm{Z}\) in \(G^{\prime}\) inducing the same observed distribution on \(\bm{X}\)._

We now present our identifiability results. We first study the most fundamental bivariate case with two latent causal variables \(V_{1}\) and \(V_{2}\). This can loosely be seen as the CRL analogue of the widely studied cause-effect problem (\(X{\rightarrow}Y\) or \(Y{\rightarrow}X\)?) in classical structure learning [88, 100].

**Theorem 3.2** (Bivariate identifiability up to \(\sim_{\text{\tiny CRL}}\) from one perfect stochastic intervention per node).: _Suppose that we have access to multiple environments \(\{P_{\bm{X}}^{e}\}_{e\in\mathcal{E}}\) generated as described in SS 2 under Asms. 2.2, 2.5, 2.8 and 2.9 with \(n=2\). Let \((h,G^{\prime})\) be any candidate solution such that the inferred latent distributions \(Q_{\bm{Z}}^{e}=h_{*}(P_{\bm{X}}^{e})\) of \(\bm{Z}=h(\bm{X})\) and the inferred mixing function \(h^{-1}\) satisfy the above assumptions w.r.t. the candidate causal graph \(G^{\prime}\). Assume additionally that_

1. _all densities_ \(p^{e}\) _and_ \(q^{e}\) _are continuously differentiable and fully supported on_ \(\mathbb{R}^{n}\)_;_
2. _we have access to a_ known observational environment__\(e_{0}\) _and one_ single node perfect intervention for each node, _with_ unknown targets_: there exist_ \(n+1\) _environments_ \(\mathcal{E}=\{e_{i}\}_{i=0}^{n}\) _such that_ \(\mathcal{I}^{e_{0}}=\varnothing\) _and for each_ \(i\in[n]\) _we have_ \(\mathcal{I}^{e_{i}}=\{\pi(i)\}\) _for an unknown permutation_ \(\pi\) _of_ \([n]\)_;_
3. _for all_ \(i\in[n]\)_, the intervened mechanisms_ \(\tilde{p}_{i}(v_{i})\) _differ from the corresponding base mechanisms_ \(p_{i}(v_{i}\mid\bm{v}_{\text{pa}(i)})\) _everywhere, in the sense that_ \[\forall\bm{v}:\qquad\frac{\partial}{\partial v_{i}}\frac{\tilde{p}_{i}(v_{i}) }{p_{i}(v_{i}\mid\bm{v}_{\text{pa}(i)})}\neq 0\,;\] (3.1)
4. _(_"genericity"_) the base and intervened mechanisms are not fine-tuned to each other, in the sense that there exists a continuous function_ \(\varphi:\mathbb{R}^{+}\rightarrow\mathbb{R}\) _for which_ \[\mathbb{E}_{\bm{w}\sim P_{\bm{V}}^{e_{0}}}\left[\varphi\left(\frac{\tilde{p}_{ 2}(v_{2})}{p_{2}(v_{2}\mid v_{1})}\right)\right]\neq\mathbb{E}_{\bm{w}\sim P_{ \bm{V}}^{e_{0}}}\left[\varphi\left(\frac{\tilde{p}_{2}(v_{2})}{p_{2}(v_{2} \mid v_{1})}\right)\right]\] (3.2)

_Then the ground truth is identified in the sense of Defn. 2.6, that is, \((f^{-1},G)\sim_{\text{\tiny CRL}}(h,G^{\prime})\)._

Proof sketch (full proof in Appx. C.2).: Consider \(V_{1}\to V_{2}\) (the proof for \(V_{1}\not\to V_{2}\) is similar). Let \(\psi=f^{-1}\circ h^{-1}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\) such that \(\bm{V}=\psi(\bm{Z})\).5 By the change of variable formula, for all \(\bm{z}\)

Footnote 5: By Asm. 2.5, \(f\), \(h\), and thus also \(h\circ f\) are diffeomorphisms. Hence, \(\psi\) is well-defined and also diffeomorphic.

\[q^{e}(\bm{z})=p^{e}(\psi(\bm{z}))\left|\det\bm{J}_{\psi}(\bm{z})\right|\] (3.3)

where \((\bm{J}_{\psi}(\bm{z}))_{ij}=\frac{\partial\psi_{i}}{\partial z_{j}}(\bm{z})\) denotes the Jacobian of \(\psi\). We consider two separate cases, depending on whether the intervention targets in \(q^{e_{i}}\) for \(e_{i}\in\{e_{1},e_{2}\}\) match those in \(p^{e_{i}}\) (Case 1) or not (Case 2).

_Case 1: Aligned Intervention Targets._ According to Asm. 2.8 and (A2), (3.3) applied to the known observational environment \(e_{0}\) and the interventional environments \(e_{1},e_{2}\) leads to the system of equations:

\[q_{1}(z_{1})q_{2}(z_{2}\mid z_{\text{pa}(2;G^{\prime})}) =p_{1}\left(\psi_{1}(\bm{z})\right)p_{2}\left(\psi_{2}(\bm{z}) \mid\psi_{1}(\bm{z})\right)\left|\det\bm{J}_{\psi}(\bm{z})\right|\] (3.4) \[\tilde{q}_{1}(z_{1})q_{2}(z_{2}\mid z_{\text{pa}(2;G^{\prime})}) =\tilde{p}_{1}\left(\psi_{1}(\bm{z})\right)p_{2}\left(\psi_{2}(\bm{z}) \mid\psi_{1}(\bm{z})\right)\left|\det\bm{J}_{\psi}(\bm{z})\right|\] (3.5) \[q_{1}(z_{1})\tilde{q}_{2}(z_{2}) =p_{1}\left(\psi_{1}(\bm{z})\right)\tilde{p}_{2}\left(\psi_{2}(\bm {z})\right)\left|\det\bm{J}_{\psi}(\bm{z})\right|\] (3.6)

where \(z_{\text{pa}(2;G^{\prime})}\) denotes the parents of \(z_{2}\) in \(G^{\prime}\). Taking quotients of (3.5) and (3.4) yields

\[\frac{\tilde{q}_{1}}{q_{1}}(z_{1})=\frac{\tilde{p}_{1}}{p_{1}}(\psi_{1}(\bm{z}) )\quad\stackrel{{\frac{\partial}{\partial z_{2}}}}{{\Longrightarrow}} \quad 0=\left(\frac{\tilde{p}_{1}}{p_{1}}\right)^{\prime}\left(\psi_{1}(\bm{z}) \right)\frac{\partial\psi_{1}}{\partial z_{2}}(\bm{z})\quad\stackrel{{ \eqref{eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: where we have used that, according to (3.7), \(\bm{J}_{\psi}\) is triangular. According to (3.8), for all \(z_{1}\), the mapping \(z_{2}\mapsto\psi_{2}(z_{1},z_{2})\) is measure preserving for \(\tilde{q}_{2}\) and \(\tilde{p}_{2}\). By Lemma C.1[18, SS A.2, Lemma 2], it follows that \(\psi_{2}\) must be constant in \(z_{1}\).6 Hence, \(\psi\) is an element-wise function. Finally, \(G=G^{\prime}\) follows from faithfulness (Asm. 2.2), for otherwise \((V_{1},V_{2})=(\psi_{1}(Z_{1}),\psi_{2}(Z_{2}))\) would be independent.

Footnote 6: This step is where the assumption of perfect interventions (Asm. 2.9) is leveraged: the conclusion would not hold for arbitrary imperfect interventions for which (3.8) would involve \(\tilde{q}_{2}(z_{2}\mid z_{1})\) and \(p_{2}\left(\psi_{2}(z_{1},z_{2})\mid\psi_{1}(z_{1})\right)\).

_Case 2: Misaligned Intervention Targets._ If \(G^{\prime}\neq G\), a similar argument to Case 1 (with the roles of \(z_{1}\) and \(z_{2}\) interchanged) also yields a contradiction to faithfulness. This leaves \(G=G^{\prime}\). Writing down the system of equations similar to (3.4)-(3.6), and then taking ratios of \(e_{1}\) and \(e_{2}\) with \(e_{0}\) yields

\[\frac{\tilde{q}_{1}}{q_{1}}(z_{1})=\frac{\tilde{p}_{2}\left(\psi_{2}(\bm{z}) \right)}{p_{2}\left(\psi_{2}(\bm{z})\mid\psi_{1}(\bm{z})\right)}\qquad\text{ and}\qquad\frac{\tilde{q}_{2}(z_{2})}{q_{2}(z_{2}\mid z_{1})}=\frac{ \tilde{p}_{1}}{p_{1}}\left(\psi_{1}(\bm{z})\right)\,.\] (3.9)

These conditions highlight the misalignment in intervention targets (see Fig. 2). Unlike in Case 1, they do not directly imply that some elements of \(\bm{J}_{\psi}\) need to be zero, that is \(\bm{Z}\) can be arbitrarily mixed w.r.t. \(\bm{V}\). However, (3.9) imposes constraints on the form of \(\psi\) that, by exploiting the invariance of \(q_{1}\) across \(e_{0}\) and \(e_{1}\) while \(p_{1}\) changes to \(\tilde{p}_{1}\), can ultimately be shown to only be satisfied if the two expectations in (3.2) are equal for all continuous \(\varphi\). However, such fine-tuning is ruled out by (A4). 

_Remark 3.3_.: The main difficulty of the proof is that (3.9) may, in principle, hold when \((p,\tilde{p},q,\tilde{q})\) and \(\psi\) are completely unconstrained. This does not arise in prior work if the intervention targets are known [80, 81, 126] (Case 1), or the densities or mixing are parametrically constrained [3, 117, 122].

**On the Genericity Condition (A4).** The condition in (3.2) contrasts expectations of the same quantity w.r.t. the observational distribution \(P_{\bm{V}}^{e_{0}}\) and the interventional distribution \(P_{\bm{V}}^{e_{1}}\) corresponding to an intervention on \(V_{1}\) that turns \(p_{1}\) into \(\tilde{p}_{1}\). The shared argument, on the other hand, is a function of the ratio between the intervened mechanism \(\tilde{p}_{2}\) and its base mechanism \(p_{2}\). While the two expectations are always equal for linear \(\varphi\), other choices imply non-trivial constraints. For instance, \(\varphi(y)=y^{2}\) yields

\[\int\big{(}\tilde{p}_{1}(v_{1})-p_{1}(v_{1})\big{)}\int\frac{\tilde{p}_{2}(v_ {2})^{2}}{p_{2}(v_{2}\mid v_{1})}\,\mathrm{d}v_{2}\,\mathrm{d}v_{1}\neq 0\,.\]

Since \(p_{1}\neq\tilde{p}_{1}\) by assumption (A3), we argue that (A4) should generally hold for randomly chosen \((p_{1},p_{2},\tilde{p}_{1},\tilde{p}_{2})\) and can only be violated if they are fine-tuned to each other. It can thus be viewed as encoding some notion of genericity--in line with the principle of independent causal mechanisms [15, 43, 61, 100, 109], but also involving the intervened mechanisms. Interestingly, related genericity conditions also arise in the study of nonlinear cause-effect inference from observational data, where identifiability is often obtained up to a set of pathological ("fine-tuned") spurious solutions satisfying a partial differential equation involving the original mechanisms [49, 58, 118, 137]. Further, we note that \(\varphi\) in (3.2) can also be thought of as a _witness function of genericity_, similar to witness functions in kernel-based two sample and independence testing [44].

Next, we provide our identifiability result for an arbitrary number of causal variables.

**Theorem 3.4** (Identifiability up to \(\sim_{\text{\tiny{CRL}}}\) from two paired perfect stochastic interventions per node).: _Suppose that we have access to multiple environments \(\{P_{\bm{X}}^{e}\}_{e\in\mathcal{E}}\) generated as described in SS 2 under Asms. 2.2, 2.3, 2.5, 2.8 and 2.9. Let \((h,G^{\prime})\) be any candidate solution such that the inferred latent distributions \(Q_{\bm{Z}}^{e}=h_{*}(P_{\bm{X}}^{e})\) of \(\bm{Z}=h(\bm{X})\) and the inferred mixing function \(h^{-1}\) satisfy the above assumptions w.r.t. the candidate causal graph \(G^{\prime}\). Assume additionally that_

1. _all densities_ \(p^{e}\) _and_ \(q^{e}\) _are continuously differentiable and fully supported on_ \(\mathbb{R}^{n}\)_;_
2. _we have access to at least one_ pair _of single-node perfect interventions per node, with unknown targets: there exist_ \(m\geq n\) _known pairs of environments_ \(\mathcal{E}=\{(e_{j},e_{j}^{\prime})\}_{j=1}^{m}\) _such that for each_ \(i\in[n]\) _there exists some_ unknown__\(j\in[m]\) _for which_ \(\mathcal{I}^{e_{j}}=\mathcal{I}^{e_{j}^{\prime}}=\{i\}\)_;_
3. _for all_ \(i\in[n]\)_, the intervened mechanisms_ \(\tilde{p}_{i}(v_{i})\) _and_ \(\tilde{p}_{i}(v_{i})\) _differ everywhere, in the sense that_ \[\forall v_{i}:\qquad\left(\frac{\tilde{p}_{i}}{\tilde{p}_{i}}\right)^{\prime}(v_ {i})\neq 0\,;\] (3.10)

_Then the ground truth is identified in the sense of Defn. 2.6, that is, \((f^{-1},G)\sim_{\text{\tiny{CRL}}}(h,G^{\prime})\)._Proof sketch (full proof in Appx. C.3).: By considering ratios between \(e_{j}\) and \(e^{\prime}_{j}\), taking partial derivatives w.r.t. \(z_{l}\), and using assumptions (A3'), we can identify a subset \(\mathcal{E}_{n}\subseteq\mathcal{E}\) of exactly \(n\) pairs of environments corresponding to distinct intervention targets in \(p\) (for otherwise \(\psi\) cannot be invertible). For \((e_{i},e^{\prime}_{i})\in\mathcal{E}_{n}\), w.l.o.g. fix the intervention targets in \(p\) to \(\mathcal{T}^{e_{i}}=\mathcal{T}^{e_{i}}=\{i\}\) and let \(\pi\) be a permutation of \([n]\) such that \(\pi(i)\) denotes the inferred intervention target in \(q\) that by (A2') is shared across \((e_{i},e^{\prime}_{i})\). By the same argument as before, we must have \(V_{i}=\psi_{i}(Z_{\pi(i)})\), that is, \(\psi\) is a permutation composed with an element-wise function. It remains to show that \(\pi\) is a graph isomorphism, that is, \(V_{i}\to V_{j}\) in \(G\iff Z_{\pi(i)}\to Z_{\pi(j)}\) in \(G^{\prime}\). We prove \(\implies\); the other direction is analogous. Suppose for a contradiction that there exist \((i,j)\) such that \(V_{i}\to V_{j}\) in \(G\), but \(Z_{\pi(i)}\not\to Z_{\pi(j)}\) in \(G^{\prime}\). Consider \(e_{i}\) in which there are perfect interventions on \(Z_{\pi(i)}\) and \(V_{i}\). For \(Z_{\pi(k)}\in\bm{Z}_{\mathrm{pa}(\pi(j);G^{\prime})}\), let \(\tilde{V}_{k}=\psi_{k}(Z_{\pi(k)})\) and denote \(\tilde{\bm{V}}=\cup_{k}\tilde{V}_{k}\subset\bm{V}\setminus\{V_{i},V_{j}\}\). Since \(Z_{\pi(i)}\) and \(Z_{\pi(j)}\) are d-separated by \(\bm{Z}_{\mathrm{pa}(\pi(j);G^{\prime})}\) in the post-intervention graph \(G^{\prime}_{\bm{Z}_{\pi(i)}}\) with arrows pointing into \(Z_{\pi(i)}\) removed [95], it follows by Markovianity of \(q\) w.r.t. \(G^{\prime}\) that \(Z_{\pi(i)}\perp Z_{\pi(j)}\mid\bm{Z}_{\mathrm{pa}(\pi(j);G^{\prime})}\) in \(Q^{\prime\prime}_{\bm{Z}}\). By applying the corresponding diffeomorphic functions \(\psi_{i}\), it follows from Lemma C.2 in Appx. C.1 that \(V_{i}\perp V_{j}\mid\tilde{\bm{V}}\) in \(P^{e_{i}}_{\bm{V}}\). This violates faithfulness (Asm. 2.2) of \(P_{\bm{V}}\) to \(G\) since \(V_{i}\) and \(V_{j}\) are d-connected in \(G_{\overline{V}_{i}}\). 

(A2') states that we know that a _pair of datasets_ corresponds to two distinct interventions on the same underlying variable, even though we may not know the exact target of the intervention. This situation could arise, for example, if both datasets are collected under the same experimental setup but with varying experimental parameters. We stress that this is different from data consisting of _pairs of views_\((\bm{x},\tilde{\bm{x}})\) sharing the values of some variables, which is _counterfactual_ in nature [123; 4; 18]. One of the main challenges for our analysis (compared to a counterfactual multi-view setting) thus stems from the lack of correspondences across observations from different datasets. We also stress that a purely observational environment is not needed in this case, cf. (A2) in Thm. 3.2.

(A3') states that the intervention mechanisms are distinct in that their ratio is strictly monotonic, similar to (3.1) in (A3). This is a slightly stronger version of the assumption of _interventional discrepancy_ proposed by Wendong et al. [126],7 which has been shown to be necessary for identifiability even if the graph \(G\) is known. For Gaussian \(\tilde{p}_{i},\tilde{\tilde{p}}_{i}\), (A3') is satisfied, for example, by a shift in mean. In the proofs, this is used to show that \(\psi\) must be an element-wise function, see (3.7). Intuitively, if \(\tilde{p}_{i}=\tilde{\tilde{p}}_{i}\) in some open set for more than one \(i\), then the underlying variables can be nonlinearly mixed by a measure-preserving automorphism within this set without affecting the observed distributions [126].

Footnote 7: Cf. the _interventional regularity_ assumption of Varici et al. [122, Asm. 2] which instead considers partial derivatives w.r.t. the parents and is related to c-faithfulness [59, Defn. 7].

## 4 Interpreting Causal Representations

Suppose that we succeed in identifying \(\bm{V}\) and \(G\) up to \(\sim_{\textsc{CRL}}\) (Defn. 2.6). How can we use or interpret such a causal representation? Since the scale of the variables is arbitrary (SS 2.2), we clearly cannot predict the exact outcomes of interventions. We therefore seek causal quantities that are preserved by the irresolvable ambiguities of \(\sim_{\textsc{CRL}}\). A prime candidate for this are interventional causal notions defined in terms of information theoretic quantities [28] and in particular the KL divergence \(D_{\textsc{KL}}\).

**Definition 4.1** (Causal influence; based on Defn. 2 of Janzing et al. [62]).: Let \(P_{\bm{V}}\) be Markovian w.r.t. a DAG \(G\) with vertices \(\bm{V}\). For any \(V_{i}\to V_{j}\) in \(G\), the _causal influence of \(V_{i}\) on \(V_{j}\)_ is given by

\[\mathfrak{C}^{P_{\bm{V}}}_{i\to j}:=D_{\textsc{KL}}\big{(}P_{\bm{V}}\bigm{\|}P_ {\bm{V}}^{i\to j}\big{)},\quad\text{where}\quad p_{j}^{i\mapsto j}\big{(}v_{j} \mid\bm{v}_{\mathrm{pa}(j)\setminus\{i\}}\big{)}=\int p_{j}\big{(}v_{j}\mid \bm{v}_{\mathrm{pa}(j)}\big{)}p_{i}(v_{i})\,\mathrm{d}v_{i}\]

and \(P_{\bm{V}}^{i\to j}\) is the interventional distribution arising from replacing the \(j^{\text{th}}\) mechanism by \(p_{j}^{i\to j}\).8

Footnote 8: Intuitively, this intervention captures the process of removing the arrow \(V_{i}\to V_{j}\) in \(G\) and “feeding” the conditional \(p(v_{j}\mid\bm{v}_{\mathrm{pa}(j)})\) with an independent copy of \(V_{i}\), distributed according to its marginal, see [62] for details.

The following result, proven in Appx. C.4, states that the causal influences are invariant to representation and equivariant to permutations, the two irresolvable ambiguities of the \(\sim_{\textsc{CRL}}\) equivalence class.

**Theorem 4.2** (Preservation of causal influences under \(\sim_{\textsc{CRL}}\)).: _Let \(P_{\bm{V}}\) be Markovian w.r.t. \(G\), let \(\pi\) be a graph isomorphism of \(G\), and let \(\phi\) be an element-wise diffeomorphism. Let \(\bm{Z}=\bm{P}_{\pi^{-1}}\circ\phi(\bm{V})\) and denote its induced distribution by \(Q_{\bm{Z}}\). Then for any \(V_{i}\to V_{j}\) in \(G\) we have \(\mathfrak{C}^{P_{\bm{V}}}_{i\to j}=\mathfrak{C}^{Q_{\bm{Z}}}_{\pi(i)\to\pi(j)}\)._

Thm. 4.2 implies that the strength of causal relations among variables in the inferred graph army meaning. They can thus be used to uncover changes to the latent causal mechanisms underlying different experimental datasets, for example, to gain scientific insights when combined with domain knowledge.

## 5 Learning Objectives

While our main focus is on studying identifiability, our theoretical insights also suggest approaches to learning causal representations from finite interventional datasets sampled from \(\{\mathcal{P}^{c}_{\bm{X}}\}_{e\in\mathcal{E}}\). The main idea is to fit the data in a way that preserves the sparsity of interventions ([98; 110]) by employing the same (un)mixing function and sharing mechanisms across environments (Asm. 2.8). We sketch two approaches, which, according to our theory, should both asymptotically identify the ground truth up to \(\sim_{\mathrm{CRL}}\) if the set of available environments \(\mathcal{E}\) is sufficiently diverse (and the other assumptions hold).

* _Autoencoder Framework_: Jointly learn an encoder \(h\), a graph \(G^{\prime}\), and intervention targets \(\mathcal{I}^{e}\) such that the encoded latents \(\bm{Z}=h(\bm{X})\) can be used to reconstruct the observed \(\bm{X}\) across all environments, while ensuring all but the intervened mechanisms are shared. Using \(E\) as an environment indicator, the latter corresponds to the constraint \(Z_{i}\perp E\mid\bm{Z}_{\text{pa}(i;G^{\prime})}\) for \(i\not\in\mathcal{I}^{e}\), implementable, for example, through a suitable conditional independence test ([39; 94; 138]).
* _Generative Modelling Approach_: Fit a base generative model (\(G^{\prime},p,f\)) and intervention models \((\mathcal{I}^{e},\{p^{c}_{i}\}_{i\in\mathcal{I}^{e}})_{e\in\mathcal{E}}\) by maximizing the likelihood of the multi-environment data. For example, given a candidate graph \(G^{\prime}\) and candidate intervention targets \(\{\mathcal{I}^{e}\}_{e\in\mathcal{E}}\), learn the base and intervened mechanisms and mixing; then pick the graph and intervention targets that achieve the best fit.

## 6 Experiments

**Setup.** We experimentally pursue the second, generative approach. Specifically, we model the mixing function generating \(\bm{X}\) from \(\bm{Z}\) as a _normalizing flow_([30; 93]) with different environment-specific base distributions, determined by the underlying causal graph, intervention targets, and (learnt) base and intervened mechanisms. Here, we focus on the bivariate case with two ground-truth causal latent variables \(V_{1}\to V_{2}\). According to Thm. 3.2, this setting should be identifiable from three environments: an observational one and one perfect intervention on each of \(V_{1}\) and \(V_{2}\). Our goal is to verify this empirically in light of finite data and optimization issues. We fix the observation dimension to \(d=2\) to facilitate exact likelihood training of the normalizing flows, and fit a separate generative model for each choice of graph and intervention targets.9 The base and intervened mechanisms are linear Gaussian and the mixing function a three-layer MLP, see Appx. D.1 for implementation details.

Footnote 9: This can be viewed as a nested approach in which the inner loop corresponds to the method for Causal Component Analysis (CauCA) of Wendong et al. [126], and the outer loop to a search over \(G^{\prime}\) and \(\{\mathcal{I}^{e}\}_{e\in\mathcal{E}}\). Code to reproduce our experiments is available at: https://github.com/akkic/causal-component-analysis.

**Results.** The results are summarized in Fig. 3. Our main findings are two-fold: First, we observe that, in the majority of cases, the well-specified model attains the highest held-out log-likelihood, as shown in Fig. 3_(Right)_. This suggests that the likelihood of otherwise comparable generative models can act as a reliable criterion to select the correct causal graph and intervention targets. Second, we find that the ground truth latent causal variables are approximately identified up to element-wise rescaling (MCC values close to one) by the correctly specified model and not by any other model, as shown in Fig. 3_(Left)_. This indicates that recovering the correct graph and targets is not only sufficient but also necessary for reliable identification of the causal representation. Taken together, these findings are consistent with our notion of \(\sim_{\mathrm{CRL}}\)-identifiability (Defn. 2.6) and Thm. 3.2.

Figure 3: **Empirical Comparison of Correctly and Incorrectly Specified Normalizing Flow-Based Models. For \(n=2\) latent causal variables with graph \(G\) given by \(V_{1}\to V_{2}\), we compare a generative model based on the correct causal graph \(G^{\prime}=G\) and intervention targets (blue) to other generative models assuming the wrong graph \(G^{\prime}\neq G\) or misaligned intervention targets (yellow, red, purple). We show mean correlation coefficients (MCCs) between the learned and ground truth latents (_Left_) and the difference in validation model log-likelihood between the well-specified and misspecified models (_Right_). Each violin plot is based on 50 different ground truth data generating processes; the horizontal lines indicate the minimum, median and maximum values.**In Appx. D.2, we perform an additional experiment with \(n=3\) variables, nonlinear latent SCMs, fixed causal order without graph search, and one interventional environment per node, thus assaying violations of assumption (A2\({}^{*}\)) in the context of Thm. 3.4. Generally, we find that a correct choice of intervention targets can be selected based on model likelihood, leading to approximate identification of the causal variables, even when only the causal order is given, see Fig. 4 and Appx. D.2 for details.

## 7 Conclusions and Discussion

The world is full of domain shifts and different environments. Often, we cannot pin down what exactly differs between two domains, but it may reasonably be modelled as a change in some causal mechanisms. While prior work relied on harder-to-obtain counterfactual data or parametric constraints for identifiability, our study demonstrates that interventional data can be sufficient--even in the nonparametric case where the spaces of mixing functions and mechanisms are infinite dimensional. Our results can be considered a step towards justifying the use of expressive machine learning methods for learning interpretable causal representations from high-dimensional experimental data in situations where parametric assumptions are undesirable, e.g., for complex systems in physics, biology, or medicine.

**Weaker Notions of Identifiability.** In this work, we have focused on the strongest notion of identifiability that is achievable in a nonparametric setting (Defn. 2.6). However, subject to the available data and assumptions, identifiability up to \(\sim_{\text{CRL}}\) is not always possible. In this case, weaker notions of identifiability are of interest. For example, we may not be able to uniquely recover variables that are not targeted by interventions [5, 80, 123], or only recover groups of variables up to (non-)linear mixing [5, 122, 123] and the graph up to transitive closure if interventions are imperfect, or soft [117, 136]. A precise characterization of weaker notions of _nonparametric_ identifiability from different types of _interventions_ (cf. [74] for a temporal, semi-parametric setting) is an interesting direction for future work.

**Known vs. Unknown Intervention Targets.** When intervention targets can be considered known appears to be a more nuanced concept in CRL than in a fully observed setting, see also [126, SSE] for an extended discussion. Recall that we assume w.l.o.g. that \(V_{1}\preceq...\preceq V_{n}\) and only consider graphs respecting this ordering (SS 2.2), see also [117, Remark 1]. The intervention targets are then unknown w.r.t. the pre-imposed causal ordering. This is a key aspect that makes our setting more realistic, but also substantially complicates the analysis (see (3.9) and Remark 3.3). Similar to [122], for Thm. 3.2 we require a set of _exactly_\(n\) environments (one intervention on each node).10 However, we relax this requirement to mere coverage ("at least \(1\)") in Thm. 3.4 as shown for linear causal models in [117, 22].

Footnote 10: By dropping the assumption of a fixed ordering and considering all DAGs, in this case one could also call \(V_{i}\) the variable intervened upon in environment \(i\), and then consider the targets “known” in this sense.

**Identifiability From One Intervention per Node for Any \(n\).** We conjecture that Thm. 3.2 can be generalized to \(n>2\), subject to a suitably adjusted set of genericity conditions involving several intervened and base mechanisms, akin to (3.2) in the bivariate case. The main challenge to such a generalization appears to be combinatorial, as there are \(n!-1\) ways of misaligning intervention targets across \(p\) and \(q\). In Thm. 3.4, we sidestep this issue by assuming pairs of environments. Thus, while two single-node perfect interventions are sufficient, we do not believe this to be necessary.

**On the Assumption of Known \(n\) and Its Relation to Markovianity.** Asm. 2.3 relates to _causal sufficiency_ or _Markovianity_ in classical causal inference, which correspond to the assumption of independent \(U_{i}\) in Defn. 2.1, implying the causal Markov factorization (2.2) [115, 100, 105]. With _unobserved_\(V\) and _unknown_\(n\), the notion of "unobserved confounders" gets blurred, since one can, in principle, always construct a causally sufficient system by increasing \(n\) and adding any causes of two or more endogenous variables to \(V\). Asm. 2.3 then states that the minimum number of variables required to do so is known.11 However, this can lead to very large systems, which may in turn challenge the assumption of an invertible mixing (Asm. 2.5). Extensions of our analysis to unknown \(n\)[63, 68], non-Markovian, or non-invertible [131] models constitute an interesting direction for future investigations.

Footnote 11: Techniques for estimating the intrinsic dimensionality \(n\) of the observation manifold \(\mathcal{X}\subseteq\mathbb{R}^{d}\) or methods rooted in Bayesian nonparametrics could provide a means of relaxing this assumption.

**Practicality.** The method explored in SS 6 requires searching over graphs and intervention targets, which gets intractable even for moderate \(n\). Simultaneously learning an (un)mixing function, causal graph, intervention targets, and mechanisms is challenging. Further work is needed to make methods for nonparametric CRL from multi-environment data more practical, e.g., by exploring the proposed autoencoder framework with a continuous parametrisation of graph [139, 140] and targets [60].

## Acknowledgments and Disclosure of Funding

We thank Simon Buchholz, Jiaqi Zhang, and the anonymous reviewers for helpful discussions, comments, and suggestions.

This work was supported by the Tubingen AI Center and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy, EXC number 2064/1, Project number 390727645. L.G. was supported by the VideoPredict project, FKZ: 01IS21088.

## References

* [1] J. Adams, N. Hansen, and K. Zhang. Identification of partially observed linear causal models: Graphical conditions for the non-gaussian and heterogeneous cases. _Advances in Neural Information Processing Systems_, 34:22822-22833, 2021.
* [2] K. Ahuja, J. Hartford, and Y. Bengio. Properties from mechanisms: an equivariance perspective on identifiable representation learning. In _International Conference on Learning Representations_, 2021.
* [3] K. Ahuja, D. Mahajan, V. Syrgkanis, and I. Mitliagkas. Towards efficient representation identification in supervised learning. In _First Conference on Causal Learning and Reasoning_, 2021.
* [4] K. Ahuja, J. S. Hartford, and Y. Bengio. Weakly supervised representation learning with sparse perturbations. In _Advances in Neural Information Processing Systems_, volume 35, pages 15516-15528, 2022.
* [5] K. Ahuja, D. Mahajan, Y. Wang, and Y. Bengio. Interventional causal representation learning. In _International Conference on Machine Learning_, pages 372-407. PMLR, 2023.
* [6] T. V. Anand, A. H. Ribeiro, J. Tian, and E. Bareinboim. Effect identification in cluster causal diagrams. In _Proceedings of the 37th AAAI Conference on Artificial Intelligence_, 2023.
* [7] J. D. Angrist and J.-S. Pischke. _Mostly harmless econometrics: An empiricist's companion_. Princeton University Press, 2009.
* [8] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. _arXiv preprint 1907.02893_, 2019.
* [9] E. Bareinboim and J. Pearl. Causal inference and the data-fusion problem. _Proceedings of the National Academy of Sciences_, 113(27):7345-7352, 2016.
* [10] E. Bareinboim, J. D. Correa, D. Ibeling, and T. Icard. On Pearl's hierarchy and the foundations of causal inference. In _Probabilistic and Causal Inference: The Works of Judea Pearl_, page 507-556. Association for Computing Machinery, 2022.
* [11] S. Beckers and J. Y. Halpern. Abstracting causal models. In _Proceedings of the AAAI conference on Artificial Intelligence_, volume 33, pages 2678-2685, 2019.
* [12] S. Beckers, F. Eberhardt, and J. Y. Halpern. Approximate causal abstractions. In _Uncertainty in Artificial Intelligence_, pages 606-615. PMLR, 2020.
* [13] S. Beery, G. Van Horn, and P. Perona. Recognition in terra incognita. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 456-473, 2018.
* [14] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 35(8):1798-1828, 2013.
* [15] M. Besserve, N. Shajarisales, B. Scholkopf, and D. Janzing. Group invariance principles for causal generative models. In _International Conference on Artificial Intelligence and Statistics_, pages 557-565. PMLR, 2018.
** [16] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. _Journal of Machine Learning Research_, 3(Jan):993-1022, 2003.
* [17] J. Brady, R. S. Zimmermann, Y. Sharma, B. Scholkopf, J. von Kugelgen, and W. Brendel. Provably learning object-centric representations. In _International Conference on Machine Learning_, 2023.
* [18] J. Brehmer, P. De Haan, P. Lippe, and T. Cohen. Weakly supervised causal representation learning. In _Advances in Neural Information Processing Systems_, volume 35, pages 38319-38331, 2022.
* [19] P. Brouillard, S. Lachapelle, A. Lacoste, S. Lacoste-Julien, and A. Drouin. Differentiable causal discovery from interventional data. In _Advances in Neural Information Processing Systems_, volume 33, pages 21865-21877, 2020.
* [20] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. In _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901, 2020.
* [21] S. Buchholz, M. Besserve, and B. Scholkopf. Function classes for identifiable nonlinear independent component analysis. In _Advances in Neural Information Processing Systems_, 2022.
* [22] S. Buchholz, G. Rajendran, E. Rosenfeld, B. Aragam, B. Scholkopf, and P. Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing. In _Advances in Neural Information Processing Systems_, 2023.
* [23] R. Cai, F. Xie, C. Glymour, Z. Hao, and K. Zhang. Triad constraints for learning causal structure of latent variables. In _Advances in Neural Information Processing Systems_, volume 32, pages 12883-12892, 2019.
* [24] K. Chalupka, P. Perona, and F. Eberhardt. Visual causal feature learning. In _Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence_, pages 181-190. AUAI Press, 2015.
* [25] K. Chalupka, F. Eberhardt, and P. Perona. Multi-level cause-effect systems. In _Artificial Intelligence and Statistics_, pages 361-369. PMLR, 2016.
* [26] K. Chalupka, F. Eberhardt, and P. Perona. Causal feature learning: an overview. _Behaviormetrika_, 44(1):137-164, 2017.
* [27] P. Comon. Independent component analysis, a new concept? _Signal processing_, 36(3):287-314, 1994.
* [28] T. M. Cover. _Elements of information theory_. John Wiley & Sons, 1999.
* [29] A. P. Dawid. Influence diagrams for causal modelling and inference. _International Statistical Review_, 70(2):161-189, 2002.
* [30] C. Durkan, A. Bekasov, I. Murray, and G. Papamakarios. Neural spline flows. In _Advances in Neural Information Processing Systems_, volume 32, pages 7511-7522, 2019.
* [31] C. Eastwood, A. L. Nicolicioiu, J. Von Kugelgen, A. Kekic, F. Trauble, A. Dittadi, and B. Scholkopf. DCI-ES: An extended disentanglement framework with connections to identifiability. In _The Eleventh International Conference on Learning Representations_, 2022.
* [32] C. Eastwood, A. Robey, S. Singh, J. von Kugelgen, H. Hassani, G. J. Pappas, and B. Scholkopf. Probable domain generalization via quantile risk minimization. In _Advances in Neural Information Processing Systems_, 2022.
* [33] C. Eastwood, S. Singh, A. L. Nicolicioiu, M. Vlastelica, J. von Kugelgen, and B. Scholkopf. Spuriosity didn't kill the classifier: Using invariant predictions to harness spurious features. In _Advances in Neural Information Processing Systems_, 2023.
** [34] D. Eaton and K. Murphy. Exact bayesian structure learning from uncertain interventions. In _Artificial Intelligence and Statistics_, pages 107-114. PMLR, 2007.
* [35] F. Eberhardt. Green and grue causal variables. _Synthese_, 193(4):1029-1046, 2016.
* [36] F. Eberhardt and R. Scheines. Interventions and causal inference. _Philosophy of science_, 74(5):981-995, 2007.
* [37] F. Eberhardt, C. Glymour, and R. Scheines. N-1 experiments suffice to determine the causal relations among n variables. _Innovations in Machine Learning: Theory and Applications_, pages 97-112, 2006.
* [38] J. Eriksson and V. Koivunen. Identifiability, separability, and uniqueness of linear ICA models. _IEEE Signal Processing Letters_, 11(7):601-604, 2004.
* [39] K. Fukumizu, A. Gretton, X. Sun, and B. Scholkopf. Kernel measures of conditional dependence. In _Advances in Neural Information Processing Systems_, volume 20, pages 489-496. Curran Associates, Inc., 2007.
* [40] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In _3rd International Conference on Learning Representations_, 2015.
* [41] L. Gresele, G. Fissore, A. Javaloy, B. Scholkopf, and A. Hyvarinen. Relative gradient optimization of the Jacobian term in unsupervised deep learning. In _Advances in Neural Information Processing Systems_, volume 33, pages 16567-16578, 2020.
* [42] L. Gresele, P. K. Rubenstein, A. Mehrjou, F. Locatello, and B. Scholkopf. The incomplete rosetta stone problem: Identifiability results for multi-view nonlinear ICA. In _Uncertainty in Artificial Intelligence_, pages 217-227. PMLR, 2020.
* [43] L. Gresele, J. von Kugelgen, V. Stimper, B. Scholkopf, and M. Besserve. Independent mechanism analysis, a new concept? In _Advances in Neural Information Processing Systems_, volume 34, pages 28233-28248, 2021.
* [44] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Scholkopf, and A. Smola. A kernel two-sample test. _The Journal of Machine Learning Research_, 13(1):723-773, 2012.
* [45] H. Halva and A. Hyvarinen. Hidden markov nonlinear ica: Unsupervised learning from nonstationary time series. In _Conference on Uncertainty in Artificial Intelligence_, pages 939-948. PMLR, 2020.
* [46] H. Halva, S. Le Corff, L. Lehericy, J. So, Y. Zhu, E. Gassiat, and A. Hyvarinen. Disentangling identifiable features from noisy data with structured nonlinear ica. In _Advances in Neural Information Processing Systems_, volume 34, pages 1624-1633, 2021.
* [47] C. Heinze-Deml, J. Peters, and N. Meinshausen. Invariant causal prediction for nonlinear models. _Journal of Causal Inference_, 6(2), 2018.
* [48] M. A. Hernan and J. M. Robins. _Causal inference: What if_. Boca Raton: Chapman & Hall/CRC., 2020.
* [49] P. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Scholkopf. Nonlinear causal discovery with additive noise models. In _Advances in Neural Information Processing Systems_, volume 21, pages 689-696, 2008.
* [50] B. Huang, K. Zhang, J. Zhang, J. D. Ramsey, R. Sanchez-Romero, C. Glymour, and B. Scholkopf. Causal discovery from heterogeneous/nonstationary data. _Journal of Machine Learning Research_, 21(89):1-53, 2020.
* [51] A. Hyvarinen and H. Morioka. Unsupervised feature extraction by time-contrastive learning and nonlinear ICA. In _Advances in Neural Information Processing Systems_, pages 3765-3773, 2016.
* [52] A. Hyvarinen and H. Morioka. Nonlinear ICA of temporally dependent stationary sources. In _Artificial Intelligence and Statistics_, pages 460-469. PMLR, 2017.

* [53] A. Hyvarinen and E. Oja. Independent component analysis: algorithms and applications. _Neural networks_, 13(4-5):411-430, 2000.
* [54] A. Hyvarinen and P. Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. _Neural networks_, 12(3):429-439, 1999.
* [55] A. Hyvarinen, H. Sasaki, and R. Turner. Nonlinear ICA using auxiliary variables and generalized contrastive learning. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 859-868, 2019.
* [56] A. Hyvarinen, I. Khemakhem, and H. Morioka. Nonlinear independent component analysis for principled disentanglement in unsupervised deep learning. _arXiv preprint arXiv:2303.16535_, 2023.
* [57] G. W. Imbens and D. B. Rubin. _Causal inference in statistics, social, and biomedical sciences_. Cambridge University Press, 2015.
* [58] A. Immer, C. Schultheiss, J. E. Vogt, B. Scholkopf, P. Buhlmann, and A. Marx. On the identifiability and estimation of causal location-scale noise models. In _40th International Conference on Machine Learning_, 2023.
* [59] A. Jaber, M. Kocaoglu, K. Shanmugam, and E. Bareinboim. Causal discovery from soft interventions with unknown targets: Characterization and learning. In _Advances in Neural Information Processing Systems_, volume 33, pages 9551-9561, 2020.
* [60] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. In _International Conference on Learning Representations_, 2017.
* [61] D. Janzing, J. Mooij, K. Zhang, J. Lemeire, J. Zscheischler, P. Daniusis, B. Steudel, and B. Scholkopf. Information-geometric approach to inferring causal directions. _Artificial Intelligence_, 182:1-31, 2012.
* 2358, 2013.
* [63] Y. Jiang and B. Aragam. Learning nonparametric latent causal graphs with unknown interventions. In _Advances in Neural Information Processing Systems_, 2023.
* Beyond Explainable AI. Lecture Notes in Computer Science_, volume 13200, pages 139-166. Springer, 2022.
* [65] I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational autoencoders and nonlinear ICA: A unifying framework. In _International Conference on Artificial Intelligence and Statistics_, pages 2207-2217, 2020.
* [66] I. Khemakhem, R. Monti, D. Kingma, and A. Hyvarinen. ICE-BeeM: Identifiable conditional energy-based deep models based on nonlinear ICA. In _Advances in Neural Information Processing Systems_, volume 33, pages 12768-12778, 2020.
* [67] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In _3rd International Conference on Learning Representations_, 2015.
* [68] B. Kivva, G. Rajendran, P. Ravikumar, and B. Aragam. Learning latent causal graphs via mixture oracles. In _Advances in Neural Information Processing Systems_, volume 34, pages 18087-18101, 2021.
* [69] B. Kivva, G. Rajendran, P. Ravikumar, and B. Aragam. Identifiability of deep generative models without auxiliary information. In _Advances in Neural Information Processing Systems_, volume 35, pages 15687-15701, 2022.
* [70] D. A. Klindt, L. Schott, Y. Sharma, I. Ustyuzhaninov, W. Brendel, M. Bethge, and D. Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. In _International Conference on Learning Representations_, 2020.

* [71] M. Kocaoglu, K. Shanmugam, and E. Bareinboim. Experimental design for learning causal graphs with latent variables. In _Advances in Neural Information Processing Systems_, volume 31, pages 7021-7031, 2017.
* [72] M. Kocaoglu, A. Jaber, K. Shanmugam, and E. Bareinboim. Characterization and learning of causal graphs with latent variables from soft interventions. In _Advances in Neural Information Processing Systems_, volume 33, pages 14369-14379, 2019.
* [73] D. Krueger, E. Caballero, J.-H. Jacobsen, A. Zhang, J. Binas, D. Zhang, R. Le Priol, and A. Courville. Out-of-distribution generalization via risk extrapolation (rex). In _International Conference on Machine Learning_, pages 5815-5826. PMLR, 2021.
* [74] S. Lachapelle and S. Lacoste-Julien. Partial disentanglement via mechanism sparsity. _arXiv preprint arXiv:2207.07732_, 2022.
* [75] S. Lachapelle, P. Rodriguez, Y. Sharma, K. E. Everett, R. Le Priol, A. Lacoste, and S. Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In _Conference on Causal Learning and Reasoning_, pages 428-484. PMLR, 2022.
* [76] S. Lachapelle, T. Deleu, D. Mahajan, I. Mitliagkas, Y. Bengio, S. Lacoste-Julien, and Q. Bertrand. Synergies between disentanglement and sparsity: Generalization and identifiability in multi-task learning. In _International Conference on Machine Learning_, pages 18171-18206. PMLR, 2023.
* [77] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. _nature_, 521(7553):436, 2015.
* [78] E. L. Lehmann and G. Casella. _Theory of point estimation_. Springer Science & Business Media, 2006.
* [79] M. S. Lewicki and T. J. Sejnowski. Learning overcomplete representations. _Neural computation_, 12(2):337-365, 2000.
* [80] P. Lippe, S. Magliacane, S. Lowe, Y. M. Asano, T. Cohen, and S. Gavves. Citris: Causal identifiability from temporal intervened sequences. In _International Conference on Machine Learning_, pages 13557-13603. PMLR, 2022.
* [81] P. Lippe, S. Magliacane, S. Lowe, Y. M. Asano, T. Cohen, and E. Gavves. Causal representation learning for instantaneous and temporal effects in interactive systems. In _The Eleventh International Conference on Learning Representations_, 2023.
* [82] Y. Liu, Z. Zhang, D. Gong, M. Gong, B. Huang, A. v. d. Hengel, K. Zhang, and J. Q. Shi. Identifying weight-variant latent causal models. _arXiv preprint arXiv:2208.14153_, 2022.
* [83] F. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Scholkopf, and O. Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In _36th International Conference on Machine Learning_, pages 4114-4124. PMLR, 2019.
* [84] F. Locatello, B. Poole, G. Ratsch, B. Scholkopf, O. Bachem, and M. Tschannen. Weakly-supervised disentanglement without compromises. In _International Conference on Machine Learning_, pages 6348-6359. PMLR, 2020.
* [85] D. Lopez-Paz, R. Nishihara, S. Chintala, B. Scholkopf, and L. Bottou. Discovering causal signals in images. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017)_, pages 58-66, 2017.
* [86] C. Lu, Y. Wu, J. M. Hernandez-Lobato, and B. Scholkopf. Invariant causal representation learning for out-of-distribution generalization. In _International Conference on Learning Representations_, 2022.
* [87] C. Mao, K. Xia, J. Wang, H. Wang, J. Yang, E. Bareinboim, and C. Vondrick. Causal transportability for visual recognition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7521-7531, 2022.
** Mooij et al. [2016] J. M. Mooij, J. Peters, D. Janzing, J. Zscheischler, and B. Scholkopf. Distinguishing cause from effect using observational data: methods and benchmarks. _The Journal of Machine Learning Research_, 17(1):1103-1204, 2016.
* Mooij et al. [2020] J. M. Mooij, S. Magliacane, and T. Claassen. Joint causal inference from multiple contexts. _The Journal of Machine Learning Research_, 21(1):3919-4026, 2020.
* Moran et al. [2022] G. E. Moran, D. Sridhar, Y. Wang, and D. Blei. Identifiable deep generative models via sparse decoding. _Transactions on Machine Learning Research_, 2022. ISSN 2835-8856.
* Morgan and Winship [2014] S. L. Morgan and C. Winship. _Counterfactuals and Causal Inference: Methods and Principles for Social Research_. Cambridge University Press, 2014.
* Muandet et al. [2013] K. Muandet, D. Balduzzi, and B. Scholkopf. Domain generalization via invariant feature representation. In _Proceedings of the 30th International Conference on Machine Learning_, volume 28, pages 10-18, 2013.
* Papamakarios et al. [2021] G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. _The Journal of Machine Learning Research_, 22(1):2617-2680, 2021.
* Park et al. [2021] J. Park, U. Shalit, B. Scholkopf, and K. Muandet. Conditional distributional treatment effect with kernel conditional mean embeddings and u-statistic regression. In _International Conference on Machine Learning_, pages 8401-8412. PMLR, 2021.
* Pearl [2009] J. Pearl. _Causality: Models, Reasoning, and Inference_. Cambridge University Press, New York, NY, 2nd edition, 2009.
* Pearl and Bareinboim [2014] J. Pearl and E. Bareinboim. External validity: From do-calculus to transportability across populations. _Statistical Science_, pages 579-595, 2014.
* Pearl and Mackenzie [2018] J. Pearl and D. Mackenzie. _The book of why: the new science of cause and effect_. Basic books, 2018.
* Perry et al. [2022] R. Perry, J. von Kugelgen, and B. Scholkopf. Causal discovery in heterogeneous environments under the sparse mechanism shift hypothesis. In _Advances in Neural Information Processing Systems_, 2022.
* Peters et al. [2016] J. Peters, P. Buhlmann, and N. Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 78(5):947-1012, 2016.
* Peters et al. [2017] J. Peters, D. Janzing, and B. Scholkopf. _Elements of causal inference: foundations and learning algorithms_. MIT Press, 2017.
* Roeder et al. [2021] G. Roeder, L. Metz, and D. Kingma. On linear identifiability of learned representations. In _International Conference on Machine Learning_, pages 9030-9039, 2021.
* Rojas-Carulla et al. [2018] M. Rojas-Carulla, B. Scholkopf, R. Turner, and J. Peters. Invariant models for causal transfer learning. _Journal of Machine Learning Research_, 19(1):1309-1342, 2018.
* Roth et al. [2023] K. Roth, M. Ibrahim, Z. Akata, P. Vincent, and D. Bouchacourt. Disentanglement of correlated factors via hausdorff factorized support. In _The Eleventh International Conference on Learning Representations_, 2023.
* Rothenhausler et al. [2021] D. Rothenhausler, N. Meinshausen, P. Buhlmann, and J. Peters. Anchor regression: Heterogeneous data meet causality. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 83(2):215-246, 2021.
* Rubenstein et al. [2017] P. Rubenstein, S. Weichwald, S. Bongers, J. Mooij, D. Janzing, M. Grosse-Wentrup, and B. Scholkopf. Causal consistency of structural equation models. In _33rd Conference on Uncertainty in Artificial Intelligence_, pages 808-817. Curran Associates, Inc., 2017.

* [106] J. Schmidhuber. Deep learning in neural networks: An overview. _Neural networks_, 61:85-117, 2015. [Cited on p. 1.]
* [107] B. Scholkopf. Causality for machine learning. In _Probabilistic and Causal Inference: The Works of Judea Pearl_, pages 765-804, 2022. [Cited on p. 1.]
* [108] B. Scholkopf and J. von Kugelgen. From statistical to causal learning. In _Proceedings of the International Congress of Mathematicians_, 2022. [Cited on p. 1 and 4.]
* [109] B. Scholkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. Mooij. On causal and anticausal learning. In _Proceedings of the 29th International Conference on Machine Learning_, pages 459-466, 2012. [Cited on p. 5 and 7.]
* [110] B. Scholkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio. Toward causal representation learning. _Proceedings of the IEEE_, 109(5):612-634, 2021. [Cited on p. 1, 5, 9, and 20.]
* [111] X. Shen, F. Liu, H. Dong, Q. Lian, Z. Chen, and T. Zhang. Weakly supervised disentangled generative causal representation learning. _Journal of Machine Learning Research_, 23:1-55, 2022. [Cited on p. 20.]
* [112] R. Shu, Y. Chen, A. Kumar, S. Ermon, and B. Poole. Weakly supervised disentanglement with guarantees. In _International Conference on Learning Representations_, 2019. [Cited on p. 2.]
* [113] R. Silva, R. Scheines, C. Glymour, P. Spirtes, and D. M. Chickering. Learning the structure of linear latent variable models. _Journal of Machine Learning Research_, 7(2), 2006. [Cited on p. 20.]
* [114] P. Sorrenson, C. Rother, and U. Kothe. Disentanglement by nonlinear ICA with general incompressible-flow networks (gin). In _International Conference on Learning Representations_, 2019. [Cited on p. 2.]
* [115] P. Spirtes, C. Glymour, and R. Scheines. _Causation, Prediction, and Search_. The MIT Press, 2001. [Cited on p. 2, 3, 4, 10, and 20.]
* [116] C. Squires and C. Uhler. Causal structure learning: a combinatorial perspective. _Foundations of Computational Mathematics_, pages 1-35, 2022. [Cited on p. 2.]
* [117] C. Squires, A. Seigal, S. Bhate, and C. Uhler. Linear causal disentanglement via interventions. In _40th International Conference on Machine Learning_, 2023. [Cited on p. 2, 3, 4, 5, 7, 10, 21, and 36.]
* [118] E. V. Strobl and T. A. Lasko. Identifying patient-specific root causes with the heteroscedastic noise model. _arXiv preprint arXiv:2205.13085_, 2022. [Cited on p. 7.]
* [119] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In _2nd International Conference on Learning Representations_, 2014. [Cited on p. 1.]
* [120] J. Tian and J. Pearl. Causal discovery from changes. In _Proceedings of the 17th Annual Conference on Uncertainty in Artificial Intelligence_, pages 512-522, 2001. [Cited on p. 5.]
* [121] C. Uhler, G. Raskutti, P. Buhlmann, and B. Yu. Geometry of the faithfulness assumption in causal inference. _The Annals of Statistics_, pages 436-463, 2013. [Cited on p. 3.]
* [122] B. Varici, E. Acarturk, K. Shanmugam, A. Kumar, and A. Tajer. Score-based causal representation learning with interventions. _arXiv preprint arXiv:2301.08230_, 2023. [Cited on p. 3, 5, 7, 8, 10, and 21.]
* [123] J. von Kugelgen, Y. Sharma, L. Gresele, W. Brendel, B. Scholkopf, M. Besserve, and F. Locatello. Self-supervised learning with data augmentations provably isolates content from style. In _Advances in Neural Information Processing Systems_, volume 34, pages 16451-16467, 2021. [Cited on p. 8, 10, 20, and 21.]
* [124] Y. Wald, A. Feder, D. Greenfeld, and U. Shalit. On calibration and out-of-domain generalization. In _Advances in Neural Information Processing Systems_, volume 34, pages 2215-2227, 2021. [Cited on p. 20.]* [125] Y. Wang and M. I. Jordan. Desiderata for representation learning: A causal perspective. _arXiv preprint arXiv:2109.03795_, 2021.
* [126] L. Wendong, A. Kekic, J. von Kugelgen, S. Buchholz, M. Besserve, L. Gresele, and B. Scholkopf. Causal component analysis. In _Advances in Neural Information Processing Systems_, 2023.
* [127] Q. Xi and B. Bloem-Reddy. Indeterminacy in generative models: Characterization and strong identifiability. In _Proceedings of the 26th International Conference on Artificial Intelligence and Statistics_, volume 206. PMLR, 2023.
* [128] F. Xie, R. Cai, B. Huang, C. Glymour, Z. Hao, and K. Zhang. Generalized independent noise condition for estimating latent variable causal graphs. In _Advances in Neural Information Processing Systems_, volume 33, pages 14891-14902, 2020.
* [129] F. Xie, B. Huang, Z. Chen, Y. He, Z. Geng, and K. Zhang. Identification of linear non-gaussian latent hierarchical structure. In _International Conference on Machine Learning_, pages 24370-24387. PMLR, 2022.
* [130] M. Yang, F. Liu, Z. Chen, X. Shen, J. Hao, and J. Wang. CausalVAE: Disentangled representation learning via neural structural causal models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9593-9602, 2021.
* [131] D. Yao, D. Xu, S. Lachapelle, S. Magliacane, P. Taslakian, G. Martius, J. von Kugelgen, and F. Locatello. Multi-view causal representation learning with partial observability. _arXiv preprint arXiv:2311.04056_, 2023.
* [132] W. Yao, Y. Sun, A. Ho, C. Sun, and K. Zhang. Learning temporally causal latent processes from general temporal data. In _International Conference on Learning Representations_, 2021.
* [133] W. Yao, G. Chen, and K. Zhang. Temporally disentangled representation learning. In _Advances in Neural Information Processing Systems_, 2022.
* [134] F. M. Zennaro. Abstraction between structural causal models: A review of definitions and properties. _arXiv preprint arXiv:2207.08603_, 2022.
* [135] F. M. Zennaro, M. Dravucz, G. Apachitei, W. D. Widanage, and T. Damoulas. Jointly learning consistent causal abstractions over multiple interventional distributions. In _2nd Conference on Causal Learning and Reasoning_, 2023.
* [136] J. Zhang, K. Greenewald, C. Squires, A. Srivastava, K. Shanmugam, and C. Uhler. Identifiability guarantees for causal disentanglement from soft interventions. In _Advances in Neural Information Processing Systems_, 2023.
* [137] K. Zhang and A. Hyvarinen. On the identifiability of the post-nonlinear causal model. In _25th Conference on Uncertainty in Artificial Intelligence_, pages 647-655. AUAI Press, 2009.
* [138] K. Zhang, J. Peters, D. Janzing, and B. Scholkopf. Kernel-based conditional independence test and application in causal discovery. In _27th Conference on Uncertainty in Artificial Intelligence_, pages 804-813. AUAI Press, 2011.
* [139] X. Zheng, B. Aragam, P. Ravikumar, and E. Xing. DAGs with no tears: Continuous optimization for structure learning. In _Advances in Neural Information Processing Systems_, volume 31, 2018.
* [140] X. Zheng, C. Dan, B. Aragam, P. Ravikumar, and E. Xing. Learning sparse nonparametric dags. In _International Conference on Artificial Intelligence and Statistics_, pages 3414-3425. PMLR, 2020.
* [141] R. S. Zimmermann, Y. Sharma, S. Schneider, M. Bethge, and W. Brendel. Contrastive learning inverts the data generating process. In _International Conference on Machine Learning_, pages 12979-12990. PMLR, 2021.
*

## Appendix

### Table of Contents

* A Extended Discussion of Related Work
* A.1 Comparison of Related Identifiability Results
* B Proof of Minimality of the CRL Equivalence Class \(\sim_{\text{CRL}}\).
* C Identifiability Proofs
* C.1 Auxiliary Lemmata
* C.2 Proof of Thm. 3.2
* C.3 Proof of Thm. 3.4
* C.4 Proof of Thm. 4.2
* D Experimental Details and Additional Results
* D.1 Experimental Details for $ 6
* D.2 Additional Results: Learning Nonlinear Latent SCMs from Partial Causal Order
* E Discussion of the Role of Our Assumptions
Extended Discussion of Related Work

Prior work on causal representation learning with general nonlinear relationships (both among latents and between latents and observations) and without an explicit task or label typically relies on some form of _weak supervision_. One example of weak supervision is "multi-view" data consisting of tuples of related observations. von Kugelgen et al. [123] consider counterfactual pairs of observations arising from imperfect interventions through data augmentation, and prove identifiability for the invariant non-descendants of intervened-upon variables. Brehmer et al. [18] also use counterfactual pre- and post-intervention views and show that the latent SCM can be identified given all single-node perfect stochastic interventions. Another type of weak-supervision is temporal structure [2], possibly combined with nonstationarity [132, 133], interventions on known targets [80, 81], or observed actions inducing sparse mechanism shifts [74, 75, 110]. Other works use more explicit supervision in the form of annotations of the ground truth causal variables or a known causal graph [111, 126, 130].

A different line of work instead approaches causal representation learning from the perspective of causal discovery in the presence of latent variables [115]. Doing so from _purely observational i.i.d. data_ requires additional constraints on the generative process, such as restrictions on the graph structure or particular parametric and distributional assumptions, and typically leverages the identifiability of linear ICA [27, 38, 79]. For _linear, non-Gaussian_ models, Silva et al. [113] show that the causal DAG can be recovered up to Markov equivalence if all observed variables are "pure" in that they have a unique latent causal parent. Cai et al. [23] and Xie et al. [128, 129] extend this result to identify the full graph given two pure observed children per latent, and Adams et al. [1] provide sufficient and necessary graphical conditions for full identification. For _discrete_ causal variables, Kivva et al. [68] introduce a similar "no twins" condition to reduce the task of learning the number and cardinality of latents and the Markov-equivalence class of the graph to mixture identifiability.

Other lines of work have investigated the relationship between causal models at different levels of coarse-graining or abstraction [135, 105, 12, 24, 25, 6, 134, 105, 134], or learning invariant representations in a supervised setting [3, 8, 32, 33, 73, 86, 92, 124, 125], often for domain generalization.

Other concurrent works address, e.g., learning from soft interventions with polynomial mixing [136], or inferring both causal graph and the number of latents subject to graphical constraints [63].

### Comparison of Related Identifiability Results

To complement the presentation of related multienvironment CRL works in SS 1, we provide a structured overview of and comparison with existing identifiability results for causal representation learning in Tab. 1. The table categorizes work along different dimensions. First, we make a distinction based on the type of data (observational, interventional, or counterfactual) results rely on (colour coded in green, yellow, and red, respectively). These are also referred to as different rungs in the "ladder of causation" [97] or layers in the Pearl Causal Hierarchy [10]. Second, we categorize work depending on the assumptions placed on the latent causal model and the mixing function. As can be seen, works relying solely on observational data often require restrictive graphical assumptions on the mixing function. On the other hand, access to much more informative counterfactual data has allowed identification even for nonparametric causal models and mixing functions. Our work can be viewed as a step towards addressing the lack of nonparametric identifiability results in the interventional regime.

We emphasize that Tab. 1 is not exhaustive: certain relevant works did not easily fit into our categorization or the causal representation learning framework adopted in the present work. For example, not listed are works that leverage temporal structure [2, 74, 75, 80, 81], rely on heterogenous data and distribution shifts which are not directly expressed in terms of or linked to interventions [82, 132, 133], allow for edges from observed to latent variables [1], or require more direct supervision [111, 130].

\begin{table}
\begin{tabular}{p{56.9pt} p{113.8pt} p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline
**Work** & **Layer** & **Causal Model** & **Mixing Function** & **Main Identifiability Result** \\ \hline Cai et al. [23], Xie et al. [128, 129] & observational & linear, non-Gaussian & linear with non-Gaussian noise s.t. each \(V_{i}\) has 2 pure (obs. or unobs.) children & number of latents + \(G\) \\ \hline Kivva et al. [68] & observational & discrete, nonparametric & indentifiable mixture model s.t. obs. children of \(V_{i}\not\subseteq\) obs. children of \(V_{i}\) & number, cardinality \& dist. of discrete latents + \(G\) up to Markov equivalence \\ \hline Ahuja et al. [5, Thm. 4] & observational & nonlinear w. independent support [103, 125] & finite-degree polynomial & \(\bm{V}\) up to permutation, shift, \& linear scaling \\ \hline Squires et al. [117, Thms. 1 \& 2] & interventional & linear & \(G\) and \(\bm{V}\) up to partial-order preserving permutations from obs. dist. \& all single-node _perfect_ interventions \\ \hline Squires et al. [117, Thm. 1] & interventional & linear & linear & \(G\) up to transitive closure from obs. dist. \& all single-node _imperfect_ interventions \\ \hline Varici et al. [122, Thm. 16] & interventional & nonparametric & linear & \(G\) and \(\bm{V}\) up to partial-order preserving permutations from obs. dist. \& all single-node _perfect_ interventions \\ \hline Ahuja et al. [5, Thm. 2] & interventional & nonparametric & finite-degree polynomial & \(\bm{V}\) up to permutation, shift, and linear scaling from all single-node _perfect_ interventions \\ \hline Buchholz et al. [22] & interventional & linear Gaussian & nonparametric & \(G\) and \(\bm{V}\) up to permutation from obs. dist. \& all single-node _perfect_ interventions \\ \hline
**This Work** (Thm. 3.2) & interventional & nonparametric & nonparametric & for \(n=2\): \(G\) and \(\bm{V}\) up to \(\sim_{\text{crit.}}\) from all single-node _perfect_ interventions, subject to genericity (3.2) \\ \hline
**This Work** (Thm. 3.4) & interventional & nonparametric & nonparametric & \(G\) and \(\bm{V}\) up to \(\sim_{\text{crit.}}\) from two distinct, paired single-node _perfect_ interventions per node \\ \hline von Kügelgen et al. [123] & counterfactual & nonparametric & nonparametric & block of non-descendants \(\bm{V}_{\text{hd}(Z)}\) up to invertible function from fat-hand _imperfect_ interventions on \(\bm{V}_{Z}\) \\ \hline Brehmer et al. [18] & counterfactual & nonparametric & nonparametric & \(G\) and \(\bm{V}\) up to \(\sim_{\text{crit.}}\) from all single-node _perfect_ interventions \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Comparison of Existing Identifiability Results for Causal Representation Learning. All of the listed works assume invertibility (or injectivity) of the mixing function, as well as causal sufficiency (Markovianity) for the causal latent variables. Most or all of the listed results require additional technical assumptions, and may provide additional results, which we omit for sake of readability; see the references for more details.**Proof of Minimality of the CRL Equivalence Class \(\sim_{\text{CRL}}\)

First, let us recall the main statements from SS 2.2.

**Definition 2.6** (\(\sim_{\text{\tiny{CRL}}}\)-identifiability).: Let \(\mathcal{H}\) be a space of unmixing functions \(h:\mathcal{X}\to\mathbb{R}^{n}\) and let \(\mathcal{G}\) be the space of DAGs over \(n\) vertices. Let \(\sim_{\text{\tiny{CRL}}}\) be the equivalence relation on \(\mathcal{H}\times\mathcal{G}\) defined as

\[(h_{1},G_{1})\sim_{\text{\tiny{CRL}}}(h_{2},G_{2})\quad\iff\quad(h_{2},G_{2})= (\bm{P}_{\pi^{-1}}\circ\phi\circ h_{1},\pi(G_{1}))\] (2.3)

for some element-wise diffeomorphism \(\phi(\bm{v})=(\phi_{1}(v_{1}),\dots,\phi_{n}(v_{n}))\) of \(\mathbb{R}^{n}\) and a permutation \(\pi\) of \([n]\) such that \(\pi:G_{1}\mapsto G_{2}\) is a graph isomorphism and \(\bm{P}_{\pi}\) the corresponding permutation matrix.

**Proposition 3.1** (Minimality of \(\sim_{\text{\tiny{CRL}}}\); informal).: _Let \(\bm{Z}\) be any representation that is \(\sim_{\text{\tiny{CRL}}}\) equivalent to \(\bm{V}\), with \(G^{\prime}=\pi(G)\) the associated DAG. Then for any intervention on \(\bm{V}_{\bm{Z}}\subseteq\bm{V}\) in \(G\), there exists an equally sparse intervention on \(\bm{Z}_{\pi(\bm{Z})}\subseteq\bm{Z}\) in \(G^{\prime}\) inducing the same observed distribution on \(\bm{X}\)._

We now restate this result more formally.

**Proposition B.1** (Minimality of \(\sim_{\text{\tiny{CRL}}}\)).: _Let \((h,G^{\prime})\sim_{\text{\tiny{CRL}}}(f^{-1},G)\) with \(\pi\) denoting the graph isomorphism mapping \(G\) to \(G^{\prime}\) (i.e., a permutation that preserves the partial topological order of \(G\)). Let \(\bm{Z}=h(\bm{X})\) be the inferred representation with distribution \(Q_{\bm{Z}}=h_{*}(P_{\bm{X}})\) Markov w.r.t. \(G^{\prime}\) and associated density \(q\). Let \(\mathcal{I}^{e}\subseteq[n]\) denote a set of intervention targets, and consider an intervention that changes \(p_{i}(v_{i}\mid\bm{v}_{\text{\tiny{pa}}(i)})\) to some intervened mechanism \(\tilde{p}_{i}(v_{i}\mid\bm{v}_{\text{\tiny{pa}}(i)})\) for all \(i\in\mathcal{I}^{e}\), giving rise to the interventional distributions \(P^{e}_{\bm{V}}\) and \(P^{e}_{\bm{X}}=f_{*}(P^{e}_{\bm{V}})\). Then there exist appropriately chosen \(\tilde{q}_{\pi(i)}(z_{\pi(i)}\mid\bm{z}_{\text{\tiny{pa}}(\pi(i),G^{\prime})})\) for \(i\in\mathcal{I}^{e}\) such that the resulting interventional distribution \(Q^{e}_{\bm{Z}}\) gives rise to the same observed distributions, that is, \(P^{e}_{\bm{X}}=h_{*}^{-1}(Q^{e}_{\bm{Z}})\)._

Proof.: Since \((h,G^{\prime})\sim_{\text{\tiny{CRL}}}(f^{-1},G)\), by Defn. 2.6 we have

\[\bm{Z}=\bm{P}_{\pi^{-1}}\circ\phi(\bm{V})\] (B.1)

for some element-wise diffeomorphism \(\phi\) with inverse \(\psi=\phi^{-1}\). Then (B.1) implies that for all \(i\in[n]\)

\[V_{i}=\psi_{i}(Z_{\pi(i)})\] (B.2)

According to (B.2), each conditional in the Markov factorization of \(Q_{\bm{Z}}\) is given in terms of \(p\) by

\[q_{\pi(i)}\left(z_{\pi(i)}\mid\bm{z}_{\text{\tiny{pa}}(\pi(i);G^{\prime})} \right)=p_{i}\left(\psi_{i}\left(z_{\pi(i)}\right)\mid\psi_{\text{\tiny{pa}}(i )}\left(\bm{z}_{\text{\tiny{pa}}(\pi(i);G^{\prime})}\right)\right)\left|\frac{ \mathrm{d}\psi_{i}}{\mathrm{d}z_{\pi(i)}}\left(z_{\pi(i)}\right)\right|\] (B.3)

where we have used the change of variables in (B.2) and the fact that \(\pi(\mathrm{pa}(i))=\mathrm{pa}(\pi(i);G^{\prime})\) since \(\pi:G\mapsto G^{\prime}\) is a graph isomorphism.

Consider an intervention that changes \(p_{i}(v_{i}\mid\bm{v}_{\text{\tiny{pa}}(i)})\) to some intervened mechanism \(\tilde{p}_{i}(v_{i}\mid\bm{v}_{\text{\tiny{pa}}(i)})\) for all \(i\in\mathcal{I}^{e}\). Denote the corresponding intervened joint distribution by \(P^{e}_{\bm{V}}\) with joint density \(p^{e}\) given by

\[p^{e}(\bm{v})=\prod_{i\in\mathcal{I}^{e}}\tilde{p}_{i}\left(v_{i}\mid\bm{v}_{ \text{\tiny{pa}}(i)}\right)\prod_{j\in[n]\setminus\mathcal{I}^{e}}p_{j}\left(v _{j}\mid\bm{v}_{\text{\tiny{pa}}(j)}\right)\,.\] (B.4)

Denote by \(Q^{e}_{\bm{Z}}=(\bm{P}_{\pi^{-1}}\circ\phi)_{*}(P^{e}_{\bm{V}})\) the corresponding distribution over \(\bm{Z}\) with joint density given by \(q^{e}\)

\[q^{e}(\bm{z}) =p^{e}(\psi\circ\bm{P}_{\pi}(\bm{z}))\left|\det\bm{J}_{\psi\circ\bm {P}_{\pi}}(\bm{z})\right|\] (B.5) \[=\prod_{i\in\mathcal{I}^{e}}\tilde{p}_{i}\left(\psi_{i}\left(z_{ \pi(i)}\right)\mid\psi_{\text{\tiny{pa}}(i)}\left(\bm{z}_{\text{\tiny{pa}}(\pi(i) ;G^{\prime})}\right)\right)\left|\frac{\mathrm{d}\psi_{i}}{\mathrm{d}z_{\pi(i)} }\left(z_{\pi(i)}\right)\right|\prod_{j\in[n]\setminus\mathcal{I}^{e}}q_{\pi( j)}\left(z_{\pi(j)}\mid\bm{z}_{\text{\tiny{pa}}(\pi(j);G^{\prime})}\right)\,,\] (B.6)

where we have used (B.3), (B.4), and the fact that \(\bm{J}_{\psi}\) is diagonal.

By defining

\[\tilde{q}_{\pi(i)}\left(z_{\pi(i)}\mid\bm{z}_{\text{\tiny{pa}}(\pi(i);G^{\prime })}\right):=\tilde{p}_{i}\left(\psi_{i}\left(z_{\pi(i)}\right)\mid\psi_{\text{ \tiny{pa}}(i)}\left(\bm{z}_{\text{\tiny{pa}}(\pi(i);G^{\prime})}\right)\right) \left|\frac{\mathrm{d}\psi_{i}}{\mathrm{d}z_{\pi(i)}}\left(z_{\pi(i)}\right)\right|\] (B.7)we finally arrive at

\[q^{e}(\bm{z})=\prod_{i\in\mathcal{I}^{e}}\tilde{q}_{\pi(i)}\left(z_{\pi(i)}\mid \bm{z}_{\mathrm{pa}(\pi(i);G^{\prime})}\right)\prod_{j\in[n]\setminus\mathcal{I} ^{e}}q_{\pi(j)}\left(z_{\pi(j)}\mid\bm{z}_{\mathrm{pa}(\pi(j);G^{\prime})} \right)\,.\] (B.8)

This shows that any intervention on \(\{V_{i}\}_{i\in\mathcal{I}^{e}}\subseteq\bm{V}\) which replaces

\[\left\{p_{i}(v_{i}\mid\bm{v}_{\mathrm{pa}(i)})\right\}_{i\in\mathcal{I}^{e}} \mapsto\left\{\tilde{p}_{i}(v_{i}\mid\bm{v}_{\mathrm{pa}(i)})\right\}_{i\in \mathcal{I}^{e}}\,,\] (B.9)

can equivalently be captured by an intervention on \(\{Z_{\pi(i)}\}_{i\in\mathcal{I}^{e}}\subseteq\bm{Z}\) which replaces

\[\left\{q_{\pi(i)}\left(z_{\pi(i)}\mid\bm{z}_{\mathrm{pa}(\pi(i);G^{\prime})} \right)\right\}_{i\in\mathcal{I}^{e}}\mapsto\left\{\tilde{q}_{\pi(i)}\left(z _{\pi(i)}\mid\bm{z}_{\mathrm{pa}(\pi(i);G^{\prime})}\right)\right\}_{i\in \mathcal{I}^{e}}\,.\] (B.10)

with \(\tilde{q}_{i}\) defined according to (B.7). 

## Appendix C Identifiability Proofs

### Auxiliary Lemmata

**Lemma C.1** (Lemma 2 of Brehmer et al. [18]).: _Let \(A=C=\mathbb{R}\) and \(B=\mathbb{R}^{n}\). Let \(f:A\times B\to C\) be differentiable. Define two differentiable measures \(p_{A}\) on \(A\) and \(p_{C}\) on \(C\). Let \(\forall b\in B\), \(f(\cdot,b):A\to C\) be measure-preserving, in the sense that the pushforward of \(p_{A}\) is always \(p_{C}\). Then \(f(\cdot,b)\) is constant in \(b\) on \(B\)._

Proof.: See Appendix A.2 of Brehmer et al. [18]. 

**Lemma C.2** (Preservation of conditional independence under invertible transformation.).: _Let \(X\) and \(Y\) be continuous real-valued random variables, and let \(\bm{Z}\) be a continuous random vector taking values in \(\mathbb{R}^{n}\). Suppose that \((X,Y,\bm{Z})\) have a joint density w.r.t. the Lebesgue measure. Let \(f:\mathbb{R}\to\mathbb{R}\), \(g:\mathbb{R}\to\mathbb{R}\), and \(h:\mathbb{R}^{n}\to\mathbb{R}^{n}\) be diffeomorphisms. Then \(X\perp\!\!\!\perp Y\mid\bm{Z}\implies f(X)\perp\!\!\!\perp g(Y)\mid h(\bm{Z})\)._

Proof.: Denote by \(p(x,y,\bm{z})\) the density of \((X,Y,\bm{Z})\). Then \(X\perp\!\!\!\perp Y\mid\bm{Z}\) implies that for all \((x,y,\bm{z})\), \(p\) can be factorized as follows:

\[p(x,y,\bm{z})=p_{\bm{z}}(\bm{z})p_{x}(x\mid\bm{z})p_{y}(y\mid\bm{z})\,.\] (C.1)

Let \((A,B,\bm{C})=(f(X),g(Y),h(\bm{Z}))\), and write \(\tilde{f}=f^{-1}\), \(\tilde{g}=g^{-1}\), and \(\tilde{h}=h^{-1}\).

Then \((A,B,\bm{C})\) also has a density \(q(a,b,\bm{c})\), which for all \((a,b,\bm{c})\) is given by the change of variable formula:

\[q(a,b,\bm{c}) =p\left(\tilde{f}(a),\tilde{g}(b),\tilde{h}(\bm{c})\right)\left| \frac{\mathrm{d}\tilde{f}}{\mathrm{d}a}(a)\frac{\mathrm{d}\tilde{g}}{ \mathrm{d}b}(b)\det\bm{J}_{\tilde{h}}(\bm{c})\right|\] (C.2) \[=p_{\bm{z}}\left(\tilde{h}(\bm{c})\right)\left|\det\bm{J}_{\tilde {h}}(\bm{c})\right|\,p_{x}\left(\tilde{f}(a)\mid\tilde{h}(\bm{c})\right)\left| \frac{\mathrm{d}\tilde{f}}{\mathrm{d}a}(a)\right|\,p_{y}\left(\tilde{g}(b) \mid\tilde{h}(\bm{c})\right)\left|\frac{\mathrm{d}\tilde{g}}{\mathrm{d}b}(b)\right|\] (C.3)

where in (C.2) we have used the fact that \((X,Y,\bm{Z})\mapsto(f(X),g(Y),h(\bm{Z}))\) has block-diagonal Jacobian, and in (C.3) that \(p\) factorises as in (C.1). Next, define

\[q_{e}(\bm{c}) :=p_{\bm{z}}\left(\tilde{h}(\bm{c})\right)\left|\det\bm{J}_{\tilde {h}}(\bm{c})\right|\,,\] (C.4) \[q_{a}(a\mid\bm{c}) :=p_{x}\left(\tilde{f}(a)\mid\tilde{h}(\bm{c})\right)\left|\frac{ \mathrm{d}\tilde{f}}{\mathrm{d}a}(a)\right|\,,\] (C.5) \[q_{b}(b\mid\bm{c}) :=p_{y}\left(\tilde{g}(b)\mid\tilde{h}(\bm{c})\right)\left|\frac{ \mathrm{d}\tilde{g}}{\mathrm{d}b}(b)\right|\,.\] (C.6)

Since \(p_{\bm{z}}\), \(p_{x}\), and \(p_{y}\) are valid densities (non-negative and integrating to one), so are \(q_{\bm{c}}\), \(q_{a}\), and \(q_{b}\). Substitution into (C.3) yields for all \((a,b,\bm{c})\),

\[q(a,b,\bm{c})=q_{\bm{c}}(\bm{c})q_{a}(a\mid\bm{c})q_{b}(b\mid\bm{c})\,,\] (C.7)

which implies that \(A\perp\!\!\!\perp B\mid\bm{C}\), concluding the proof.

### Proof of Thm. 3.2

**Theorem 3.2** (Bivariate identifiability up to \(\sim_{\text{\tiny{CRL}}}\) from one perfect stochastic intervention per node).: _Suppose that we have access to multiple environments \(\{P_{\bm{\varepsilon}}^{e}\}_{\bm{\varepsilon}}\in\mathcal{E}\) generated as described in SS 2 under Asms. 2.2, 2.5, 2.8 and 2.9 with \(n=2\). Let \((h,G^{\prime})\) be any candidate solution such that the inferred latent distributions \(Q_{\bm{Z}}^{e}=h_{\bm{\cdot}}(P_{\bm{X}}^{e})\) of \(\bm{Z}=h(\bm{X})\) and the inferred mixing function \(h^{-1}\) satisfy the above assumptions w.r.t. the candidate causal graph \(G^{\prime}\). Assume additionally that_

1. _all densities_ \(p^{e}\) _and_ \(q^{e}\) _are continuously differentiable and fully supported on_ \(\mathbb{R}^{n}\)_;_
2. _we have access to a_ known observational environment__\(e_{0}\) _and one_ single node perfect intervention for each node, _with_ unknown targets_: there exist_ \(n+1\) _environments_ \(\mathcal{E}=\{e_{i}\}_{i=0}^{n}\) _such that_ \(\mathcal{I}^{e_{0}}=\varnothing\) _and for each_ \(i\in[n]\) _we have_ \(\mathcal{I}^{e_{i}}=\{\pi(i)\}\) _for an unknown permutation_ \(\pi\) _of_ \([n]\)_;_
3. _for all_ \(i\in[n]\)_, the intervened mechanisms_ \(\tilde{p}_{i}(v_{i})\) _differ from the corresponding base mechanisms_ \(p_{i}(v_{i}\mid\bm{v}_{\mathrm{pa}(i)})\) _everywhere, in the sense that_ \[\forall\bm{v}:\qquad\frac{\partial}{\partial v_{i}}\frac{\tilde{p}_{i}(v_{i}) }{p_{i}(v_{i}\mid\bm{v}_{\mathrm{pa}(i)})}\neq 0\,;\] (3.1)
4. _("genericity") the base and intervened mechanisms are not fine-tuned to each other, in the sense that there exists a continuous function_ \(\varphi:\mathbb{R}^{+}\to\mathbb{R}\) _for which_ \[\mathbb{E}_{\bm{v}\sim P_{\bm{\nu}}^{e_{0}}}\left[\varphi\left(\frac{\tilde{p} _{2}(v_{2})}{p_{2}(v_{2}\mid v_{1})}\right)\right]\neq\mathbb{E}_{\bm{v}\sim P_ {\bm{\nu}}^{e_{0}}}\left[\varphi\left(\frac{\tilde{p}_{2}(v_{2})}{p_{2}(v_{2} \mid v_{1})}\right)\right]\] (3.2)

_Then the ground truth is identified in the sense of Defn. 2.6, that is, \((f^{-1},G)\sim_{\text{\tiny{CRL}}}(h,G^{\prime})\)._

Proof.: From the assumption of a shared mixing \(f\) and shared encoder \(h\) across all environments, we have that

\[\bm{Z}=h(\bm{X})=h(f(\bm{V}))=h\circ f(\bm{V})\,.\] (C.8)

Let \(\psi=f^{-1}\circ h^{-1}:\mathbb{R}^{n}\to\mathbb{R}^{n}\) such that

\[\bm{V}=\psi(\bm{Z})\,.\]

By Asm. 2.5, \(f\), \(h\), and thus also \(h\circ f\) are diffeomorphisms. Hence, \(\psi\) is well-defined and also diffeomorphic.

By the change of variable formula, for all \(e\) and all \(\bm{z}\) the density \(q^{e}(\bm{z})\) is given by

\[q^{e}(\bm{z})=p^{e}(\psi(\bm{z}))\left|\det\bm{J}_{\psi}(\bm{z})\right|\] (C.9)

where \((\bm{J}_{\psi}(\bm{z}))_{ij}=\frac{\partial\psi_{i}}{\partial z_{j}}(\bm{z})\) denotes the Jacobian of \(\psi\).

We now consider two separate cases, depending on whether the intervention targets in \(q^{e_{i}}\) for \(e_{i}\in\{e_{1},e_{2}\}\) match those in \(p^{e_{i}}\) (Case 1) or not (Case 2).

**Case 1: Aligned Intervention Targets.** According to Asm. 2.8 and (A2), (C.9) applied to the known observational environment \(e_{0}\) and the interventional environments \(e_{1},e_{2}\) leads to the system of equations:

\[q_{1}(z_{1})q_{2}(z_{2}\mid z_{\mathrm{pa}(2;G^{\prime})}) =p_{1}\left(\psi_{1}(\bm{z})\right)p_{2}\left(\psi_{2}(\bm{z}) \mid\psi_{\mathrm{pa}(2)}(\bm{z})\right)\left|\det\bm{J}_{\psi}(\bm{z})\right|\] (C.10) \[\tilde{q}_{1}(z_{1})q_{2}(z_{2}\mid z_{\mathrm{pa}(2;G^{\prime})}) =\tilde{p}_{1}\left(\psi_{1}(\bm{z})\right)p_{2}\left(\psi_{2}( \bm{z})\mid\psi_{\mathrm{pa}(2)}(\bm{z})\right)\left|\det\bm{J}_{\psi}(\bm{z})\right|\] (C.11) \[q_{1}(z_{1})\tilde{q}_{2}(z_{2}) =p_{1}\left(\psi_{1}(\bm{z})\right)\tilde{p}_{2}\left(\psi_{2}( \bm{z})\right)\left|\det\bm{J}_{\psi}(\bm{z})\right|\] (C.12)

where \(z_{\mathrm{pa}(2;G^{\prime})}\) denotes the parents of \(z_{2}\) in \(G^{\prime}\), and \(\mathrm{pa}(2)\) denotes the parents of \(V_{2}\) in \(G\).

Note that neither side of the previous equations can be zero due to the full support assumption12 (A1) and \(\psi\) being diffeomorphic implying the determinant is non-zero.

Taking quotients of (C.11) and (C.10), yields

\[\frac{\tilde{q}_{1}}{q_{1}}(z_{1})=\frac{\tilde{p}_{1}}{p_{1}}(\psi_{1}(\bm{z}))\,.\] (C.13)

Next, we take the partial derivative w.r.t. \(z_{2}\) on both sides and use the chain rule to obtain:

\[0=\left(\frac{\tilde{p}_{1}}{p_{1}}\right)^{\prime}\left(\psi_{1}(\bm{z}) \right)\frac{\partial\psi_{1}}{\partial z_{2}}(\bm{z})\,.\] (C.14)

Now, by assumption (A3), the first term on the RHS of (C.14) is non-zero everywhere. Hence,

\[\forall\bm{z}:\qquad\frac{\partial\psi_{1}}{\partial z_{2}}(\bm{z})=0\,.\] (C.15)

It follows that \(\psi_{1}\) is, in fact, a scalar function, and

\[V_{1}=\psi_{1}(Z_{1})\,.\] (C.16)

Since \(\psi\) is a diffeomorphism, \(\psi_{1}\) must also be diffeomorphic. Hence, by the change of variable formula applied to (C.16), the marginal density \(q_{1}(z_{1})\) is given by

\[q_{1}(z_{1})=p_{1}(\psi_{1}(z_{1}))\left|\frac{\partial\psi_{1}}{\partial z_{1 }}(z_{1})\right|\,.\] (C.17)

Further, since \(\bm{J}_{\psi}\) is triangular due to (C.15), its determinant is given by

\[\left|\det\bm{J}_{\psi}(\bm{z})\right|=\left|\frac{\partial\psi_{1}}{\partial z _{1}}(z_{1})\,\frac{\partial\psi_{2}}{\partial z_{2}}(z_{1},z_{2})\right|\,.\] (C.18)

Substituting (C.17) and (C.18) into (C.12) yields (after cancellation of equal terms):

\[\tilde{q}_{2}(z_{2})=\tilde{p}_{2}\left(\psi_{2}(z_{1},z_{2})\right)\left| \frac{\partial\psi_{2}}{\partial z_{2}}(z_{1},z_{2})\right|\,.\] (C.19)

The expression in (C.19) implies that, for all \(z_{1}\), the mapping \(\psi_{2}(z_{1},\cdot):\mathbb{R}\to\mathbb{R}\) is measure preserving for the differentiable \(\tilde{q}_{2}\) and \(\tilde{p}_{2}\). By Lemma C.1 (Lemma 2 of Brehmer et al. [18, SS A.2]), it then follows that \(\psi_{2}\) must, in fact, be constant in \(z_{1}\), that is

\[\forall\bm{z}:\qquad\frac{\partial\psi_{2}}{\partial z_{1}}(\bm{z})=0\,.\] (C.20)

Note that this last step is where the assumption of perfect interventions (Asm. 2.9) is leveraged: the conclusion would not hold for arbitrary imperfect interventions for which (3.8) would involve \(\tilde{q}_{2}(z_{2}\mid z_{1})\) and \(p_{2}\left(\psi_{2}(z_{1},z_{2})\mid\psi_{1}(z_{1})\right)\).

Hence, we have shown that \(\psi\) is an element-wise function:

\[\bm{V}=(V_{1},V_{2})=\psi(\bm{Z})=(\psi_{1}(Z_{1}),\psi_{2}(Z_{2}))\,.\] (C.21)

Finally, since \(\psi\) is a diffeomorphism, (C.21) implies that

\[V_{1}\,\perp\!\!\!\perp\,V_{2}\iff Z_{1}\,\perp\!\!\!\perp\,Z_{2}\,.\] (C.22)

It then follows from the faithfulness assumption (Asm. 2.2) that we also must have \(G=G^{\prime}\).

This concludes the proof of Case 1, as we have shown that \((h,G^{\prime})\sim_{\text{\tiny{CRL}}}(f^{-1},G)\) with \(G^{\prime}=\pi(G)=G\) (\(\pi\) being the identity permutation) and \(h\circ f=\psi^{-1}=:\phi\) an element-wise diffeomorphism.

**Case 2: Misaligned Intervention Targets.** Writing down the system of equations similar to (C.10)-(C.12), but for the case with misaligned intervention targets across \(p\) and \(q\) yields:

\[q_{1}(z_{1})q_{2}(z_{2}\mid z_{\text{\tiny{pa}}(2;G^{\prime})}) =p_{1}\left(\psi_{1}(\bm{z})\right)p_{2}\left(\psi_{2}(\bm{z})\mid \psi_{\text{\tiny{pa}}(2)}(\bm{z})\right)\left|\det\bm{J}_{\psi}(\bm{z})\right|\] (C.23) \[\tilde{q}_{1}(z_{1})q_{2}(z_{2}\mid z_{\text{\tiny{pa}}(2;G^{ \prime})}) =p_{1}\left(\psi_{1}(\bm{z})\right)\tilde{p}_{2}\left(\psi_{2}(\bm{z}) \right)\left|\det\bm{J}_{\psi}(\bm{z})\right|\] (C.24) \[q_{1}(z_{1})\tilde{q}_{2}(z_{2}) =\tilde{p}_{1}\left(\psi_{1}(\bm{z})\right)p_{2}\left(\psi_{2}( \bm{z})\mid\psi_{\text{\tiny{pa}}(2)}(\bm{z})\right)\left|\det\bm{J}_{\psi}(\bm{ z})\right|\,.\] (C.25)Taking ratios of \(e_{1}\) and \(e_{2}\) with \(e_{0}\) yields

\[\frac{\tilde{q}_{1}}{q_{1}}(z_{1}) =\frac{\tilde{p}_{2}\left(\psi_{2}(\bm{z})\right)}{p_{2}\left(\psi_ {2}(\bm{z})\mid\psi_{\mathrm{pa}(2)}(\bm{z})\right)}\] (C.26) \[\frac{\tilde{q}_{2}(z_{2})}{q_{2}(z_{2}\mid z_{\mathrm{pa}(2;G^{ \prime})})} =\frac{\tilde{p}_{1}}{p_{1}}\left(\psi_{1}(\bm{z})\right)\,.\] (C.27)

We separate the remainder of the proof of Case 2 into different subcases depending on \(G\) and \(G^{\prime}\): as we will see, we can use a similar reasoning as in Case 1, except for the case where both \(G\) and \(G^{\prime}\) are missing no edge.

_Case 2a: \(V_{1}\not\to V_{2}\) in \(G\), that is \(\mathrm{pa}(2)=\varnothing\)._ Then we can proceed similarly to Case 1. First, we take the partial derivative of (C.26) w.r.t. \(z_{2}\) to arrive at:

\[0=\left(\frac{\tilde{p}_{2}}{p_{2}}\right)^{\prime}\left(\psi_{2}(\bm{z}) \right)\frac{\partial\psi_{2}}{\partial z_{2}}(\bm{z})\,.\] (C.28)

Using (A3), this implies that \(\psi_{2}\) does not depend on \(Z_{2}\), that is, \(V_{2}=\psi_{2}(Z_{1})\).

Next, we again write \(q(z_{1})\) in terms of \(p_{2}(\psi_{2}(z_{1}))\) using the univariate change of variable formula, substitute into (C.25), cancel the corresponding terms, and arrive at:

\[\tilde{q}_{2}(z_{2})=\tilde{p}_{1}\left(\psi_{1}(z_{1},z_{2})\right)\left| \frac{\partial\psi_{1}}{\partial z_{2}}(z_{1},z_{2})\right|\] (C.29)

Lemma C.1 applied to \(\psi_{1}(z_{1},\cdot)\) which preserves \(\tilde{q}_{2}\) and \(\tilde{p}_{1}\) for all \(z_{1}\) shows that \(\psi_{1}\) is constant in \(Z_{1}\), that is

\[\bm{V}=(V_{1},V_{2})=\psi(\bm{Z})=\left(\psi_{1}(Z_{2}),\psi_{2}(Z_{1})\right).\] (C.30)

Since \(V_{1}\perp V_{2}\) by the assumption of Case 2a, it follows from the invertible element-wise reparametrisation above that \(Z_{1}\perp Z_{2}\) and hence, by faithfulness, \(Z_{1}\not\to Z_{2}\) in \(G^{\prime}\).

Finally, note that there is no partial order on the empty graph and so \(G^{\prime}=\pi(G)=G\) and \(\bm{Z}=\bm{P}_{\pi^{-1}}\cdot\psi^{-1}(\bm{V})\) where \(\pi\) is the nontrivial permutation of \(\{1,2\}\).

_Case 2b: \(V_{1}\to V_{2}\) in \(G\), that is \(\mathrm{pa}(2)=\{1\}\)._ If \(G^{\prime}\neq G\), that is \(Z_{1}\not\to Z_{2}\) in \(G^{\prime}\), then the same argument as in Case 2a, this time starting by taking the partial derivative of (C.27) w.r.t. \(z_{1}\), can be used to reach the same conclusion in (C.30). However, this contradicts faithfulness since \(V_{1}\perp V_{2}\) in \(G\).

Hence, we must have \(G^{\prime}=G\), and the following two equations must hold for all \(\bm{z}\):

\[\frac{\tilde{q}_{1}(z_{1})}{q_{1}(z_{1})} =\frac{\tilde{p}_{2}\left(\psi_{2}(\bm{z})\right)}{p_{2}\left(\psi _{2}(\bm{z})\mid\psi_{1}(\bm{z})\right)}\] (C.31) \[\frac{\tilde{q}_{2}(z_{2})}{q_{2}(z_{2}\mid z_{1})} =\frac{\tilde{p}_{1}\left(\psi_{1}(\bm{z})\right)}{p_{1}\left(\psi _{1}(\bm{z})\right)}\] (C.32)

The remainder of the proof consists of exploring the implications of (C.32) and (C.31), ultimately resulting in a violation of the genericity condition (A4).

To ease notation, define the following auxiliary functions:

\[a(z_{1}) :=\frac{\tilde{q}_{1}(z_{1})}{q_{1}(z_{1})}\,,\] (C.33) \[b(\bm{v}) :=\frac{\tilde{p}_{2}(v_{2})}{p_{2}(v_{2}\mid v_{1})}\,,\] (C.34) \[c(\bm{z}) :=\frac{\tilde{q}_{2}(z_{2})}{q_{2\mid 1}(z_{2}\mid z_{1})}\,,\] (C.35) \[d(v_{1}) :=\frac{\tilde{p}_{1}\left(v_{1}\right)}{p_{1}\left(v_{1}\right)}\,.\] (C.36)With this, (C.31) and (C.32) take the following form:

\[a(z_{1}) =b(\psi(\bm{z}))\,.\] (C.37) \[c(\bm{z}) =d(\psi_{1}(\bm{z}))\,,\] (C.38)

Next, define the following maps:

\[\kappa:\bm{z} \mapsto\begin{bmatrix}a(z_{1})\\ c(\bm{z})\end{bmatrix}\] (C.39) \[\rho:\bm{v} \mapsto\begin{bmatrix}b(\bm{v})\\ d(v_{1})\end{bmatrix}\] (C.40)

Then, (C.37) and (C.38) together imply that

\[\kappa=\rho\circ\psi\,.\] (C.41)

Recalling that by (A1) all densities are continuously differentiable, the Jacobians of \(\kappa\) and \(\rho\) are given by:

\[\bm{J}_{\kappa}(\bm{z}) =\begin{bmatrix}a^{\prime}(z_{1})&0\\ \frac{\partial c}{\partial z_{1}}(\bm{z})&\frac{\partial c}{\partial z_{2}}( \bm{z})\end{bmatrix}\,,\] (C.42) \[\bm{J}_{\rho}(\bm{v}) =\begin{bmatrix}\frac{\partial b}{\partial v_{1}}(\bm{v})&\frac {\partial b}{\partial v_{2}}(\bm{v})\\ d^{\prime}(v_{1})&0\end{bmatrix}\,,\] (C.43)

and the corresponding determinants are given by

\[\left|\det\bm{J}_{\kappa}(\bm{z})\right| =\left|a^{\prime}(z_{1})\frac{\partial c}{\partial z_{2}}(\bm{z} )\right|\neq 0\] (C.44) \[\left|\det\bm{J}_{\rho}(\bm{v})\right| =\left|d^{\prime}(v_{1})\frac{\partial b}{\partial v_{2}}(\bm{v} )\right|\neq 0\] (C.45)

where the inequalities for all \(\bm{z}\) follow since, by assumption (A3), the derivatives of ratios of intervened and original mechanisms are non-vanishing everywhere:

\[a^{\prime}(z_{1})\neq 0\neq\frac{\partial c}{\partial z_{2}}(\bm{z})\qquad \text{and}\qquad d^{\prime}(v_{1})\neq 0\neq\frac{\partial b}{\partial v_{2}}(\bm{v})\,,\] (C.46)

This implies that the following families of maps are continuously differentiable, monotonic, and invertible,

\[a:z_{1} \mapsto a(z_{1})\,,\] (C.47) \[b_{v_{1}}:v_{2} \mapsto b(v_{1},v_{2})\,,\] (C.48) \[c_{z_{1}}:z_{2} \mapsto c(z_{1},z_{2})\,,\] (C.49) \[d:v_{1} \mapsto d(v_{1})\,,\] (C.50)

with continuously differentiable inverses

\[a^{-1}:w_{1} \mapsto a^{-1}(w_{1})\,,\] (C.51) \[b_{v_{1}}^{-1}:w_{1} \mapsto b_{v_{1}}^{-1}(w_{1})\,,\] (C.52) \[c_{z_{1}}^{-1}:w_{2} \mapsto c_{z_{1}}^{-1}(w_{2})\,,\] (C.53) \[d^{-1}:w_{2} \mapsto d^{-1}(w_{2})\,.\] (C.54)

This implies that \(\rho\) and \(\kappa\) are valid diffeomorphisms onto their image and their inverses are given by:

\[\kappa^{-1}:\bm{w} \mapsto\begin{bmatrix}a^{-1}(w_{1})\\ c_{a^{-1}(w_{1})}^{-1}(w_{2})\end{bmatrix}\,,\] (C.55) \[\rho^{-1}:\bm{w} \mapsto\begin{bmatrix}d^{-1}(w_{2})\\ b_{d^{-1}(w_{2})}^{-1}(w_{1})\end{bmatrix}\,.\] (C.56)Since \(\bm{V}=\psi(\bm{Z})\), by (C.41) we have

\[\bm{W}:=\rho(\bm{V})=\rho\circ\psi(\bm{Z})=\kappa(\bm{Z})\,.\] (C.57)

Denote the distributions of \(\bm{W}\) by \(R_{\bm{W}}\) and its density by \(r(\bm{w})\). Since for all \(e\), we have

\[P_{\bm{V}}^{e}=\psi_{*}(Q_{\bm{Z}}^{e})\] (C.58)

it follows from (C.57) that

\[R_{\bm{W}}^{e}=\rho_{*}(P_{\bm{V}}^{e})=\kappa_{*}(Q_{\bm{Z}}^{e})\,.\] (C.59)

This provides two different ways of applying the change of variable formula to compute \(r(\bm{w})\).

First, we consider the pushforward of \(Q_{\bm{Z}}^{e_{0}}\) by \(\kappa\):

\[r(\bm{w}) =q\left(\kappa^{-1}(\bm{w})\right)\left|\det\bm{J}_{\kappa^{-1}} (\bm{w})\right|\] (C.60) \[=q_{1}\left(a^{-1}(w_{1})\right)q_{2}\left(c_{a^{-1}(w_{1})}^{-1 }\left(w_{2}\right)\ \mid\ a^{-1}(w_{1})\right)\left|\frac{\mathrm{d}}{\mathrm{d}w_{1}}a^{-1}(w_{ 1})\frac{\mathrm{d}}{\mathrm{d}w_{2}}c_{a^{-1}(w_{1})}^{-1}(w_{2})\right|\] (C.61)

By integrating this joint density with respect to \(w_{2}\), we obtain the following expression for the marginal \(r_{1}(w_{1})\):

\[r_{1}(w_{1}) =\left|\frac{\mathrm{d}}{\mathrm{d}w_{1}}a^{-1}(w_{1})\right|q_{ 1}\left(a^{-1}(w_{1})\right)\int q_{2}\left(c_{a^{-1}(w_{1})}^{-1}\left(w_{2} \right)\ \mid\ a^{-1}(w_{1})\right)\left|\frac{\mathrm{d}}{\mathrm{d}w_{2}}c_{a^{-1}( w_{1})}^{-1}(w_{2})\right|\mathrm{d}w_{2}\,.\] (C.62)

By the diffeomorphic change of variable \(z_{2}=c_{a^{-1}(w_{1})}^{-1}\left(w_{2}\right)\), 13 this can be written as

Footnote 13: Note that: \(\int q_{2}(z_{2}(w_{2}))\left|\frac{\mathrm{d}z_{2}}{\mathrm{d}w_{2}}\right| \mathrm{d}w_{2}=\int q_{2}(z_{2})\,\mathrm{d}z_{2}\,.\)

\[r_{1}(w_{1}) =\left|\frac{\mathrm{d}}{dw_{1}}a^{-1}(w_{1})\right|q_{1}\left(a ^{-1}(w_{1})\right)\int q_{2}\left(z_{2}\ \mid\ a^{-1}(w_{1})\right)dz_{2}\] (C.63) \[=\left|\frac{\mathrm{d}}{dw_{1}}a^{-1}(w_{1})\right|q_{1}\left(a ^{-1}(w_{1})\right)\] (C.64)

Next, we carry out the same calculation for the pushforward of \(P_{\bm{V}}^{e_{0}}\) by \(\rho\):

\[r(\bm{w}) =p\left(\rho^{-1}(\bm{w})\right)\left|\det\bm{J}_{\rho^{-1}}(\bm{ w})\right|\] (C.65) \[=p_{1}\left(d^{-1}(w_{2})\right)p_{2}\left(b_{d^{-1}(w_{2})}^{-1} (w_{1})\ \mid\ d^{-1}(w_{2})\right)\left|\frac{\mathrm{d}}{\mathrm{d}w_{2}}d^{-1}(w_{ 2})\frac{\mathrm{d}}{\mathrm{d}w_{1}}b_{d^{-1}(w_{2})}^{-1}(w_{1})\right|\,,\] (C.66)

leading to the marginal

\[r_{1}(w_{1}) =\int p_{1}\left(d^{-1}(w_{2})\right)p_{2}\left(b_{d^{-1}(w_{2}) }^{-1}(w_{1})\ \mid\ d^{-1}(w_{2})\right)\left|\frac{\mathrm{d}}{\mathrm{d}w_{1}}b_{d^{-1}(w_ {2})}^{-1}(w_{1})\right|\left|\frac{\mathrm{d}}{\mathrm{d}w_{2}}d^{-1}(w_{2}) \right|\mathrm{d}w_{2}\] (C.67) \[=\int p_{1}(v_{1})p_{2}\left(b_{v_{i}}^{-1}(w_{1})\ \mid\ v_{1}\right)\left|\frac{\mathrm{d}}{\mathrm{d}w_{1}}b_{v_{1}}^{-1}(w_{1} )\right|\mathrm{d}v_{1}\,,\] (C.68)

where the second line is obtained by the diffeomorphic change of variable \(v_{1}=d^{-1}(w_{2})\).

Equating the two expressions for \(r(w_{1})\) in \(e_{0}\) in (C.68) and (C.64), we obtain for all \(w_{1}\):

\[\left|\frac{\mathrm{d}}{\mathrm{d}w_{1}}a^{-1}(w_{1})\right|q_{1}\left(a^{-1}( w_{1})\right)=\int p_{1}(v_{1})p_{2}\left(b_{v_{1}}^{-1}(w_{1})\ \mid\ v_{1}\right)\left|\frac{\mathrm{d}}{\mathrm{d}w_{1}}b_{v_{1}}^{-1}(w_{1 })\right|\mathrm{d}v_{1}\,.\] (C.69)

Applying the same approach to the environment in which \(V_{1}\) is intervened upon changing \(p_{1}\) to \(\tilde{p}_{1}\) while \(Z_{2}\) is intervened upon leaving \(q_{1}\) invariant, yields for all \(w_{1}\):

\[\left|\frac{\mathrm{d}}{\mathrm{d}w_{1}}a^{-1}(w_{1})\right|q_{1}\left(a^{-1}( w_{1})\right)=\int\tilde{p}_{1}(v_{1})p_{2}\left(b_{v_{1}}^{-1}(w_{1})\ \mid\ v_{1}\right)\left|\frac{\mathrm{d}}{\mathrm{d}w_{1}}b_{v_{1}}^{-1}(w_{1} )\right|\mathrm{d}v_{1}\,.\] (C.70)Finally, by equating (C.69) and (C.70), we arrive at the following expression which must hold for all \(w_{1}\):

\[\int p_{1}(v_{1})p_{2}\left(b_{v_{1}}^{-1}(w_{1})\ \mid\ v_{1}\right)\left|\frac{ \mathrm{d}}{\mathrm{d}w_{1}}b_{v_{1}}^{-1}(w_{1})\right|\mathrm{d}v_{1}=\int \tilde{p}_{1}(v_{1})p_{2}\left(b_{v_{1}}^{-1}(w_{1})\ \mid\ v_{1}\right)\left|\frac{\mathrm{d}}{\mathrm{d}w_{1}}b_{v_{1}}^{-1}(w_{1 })\right|\mathrm{d}v_{1}\] (C.71)

which we can rewrite as

\[\int\left(\tilde{p}_{1}(v_{1})-p_{1}(v_{1})\right)p_{2}\left(b_{v_{1}}^{-1}(w_ {1})\ \mid\ v_{1}\right)\left|\frac{\mathrm{d}}{\mathrm{d}w_{1}}b_{v_{1}}^{-1}(w_{1 })\right|\mathrm{d}v_{1}=0\,.\] (C.72)

Multiplying by any continuous function \(\varphi(w_{1})\), integrating w.r.t. \(w_{1}\) and applying the diffeomorphic change of variable \(v_{2}=b_{v_{1}}^{-1}(w_{1})\), this can be expressed as:

\[0 =\int\varphi(w_{1})\int\left(\tilde{p}_{1}(v_{1})-p_{1}(v_{1}) \right)p_{2}\left(b_{v_{1}}^{-1}(w_{1})\ \mid\ v_{1}\right)\left|\frac{\mathrm{d}}{\mathrm{d}w_{1}}b_{v_{1}}^{-1}(w_{1 })\right|\mathrm{d}v_{1}\,\mathrm{d}w_{1}\] (C.73) \[=\int\int\varphi\left(b_{v_{1}}(v_{2})\right)\left(\tilde{p}_{1}( v_{1})-p_{1}(v_{1})\right)p_{2}(v_{2}\ \mid\ v_{1})\,\mathrm{d}v_{2}\,\mathrm{d}v_{1}\] (C.74) \[=\int\int\varphi\left(\frac{\tilde{p}_{2}(v_{2})}{p_{2}(v_{2}\mid v _{1})}\right)\left(\tilde{p}_{1}(v_{1})-p_{1}(v_{1})\right)p_{2}(v_{2}\ \mid\ v_{1})\,\mathrm{d}v_{2}\,\mathrm{d}v_{1}\] (C.75)

where we have resubstituted the expression for \(b_{v_{1}}(v_{2})\) in the last line.

Equivalently, this can be written as: _for any continuous function \(\varphi\)_,_

\[\mathbb{E}_{\boldsymbol{v}\sim P_{\boldsymbol{v}}^{\alpha}}\left[\varphi \left(\frac{\tilde{p}_{2}(v_{2})}{p_{2}(v_{2}\mid v_{1})}\right)\right]= \mathbb{E}_{\boldsymbol{v}\sim P_{\boldsymbol{v}}^{\alpha_{1}}}\left[\varphi \left(\frac{\tilde{p}_{2}(v_{2})}{p_{2}(v_{2}\mid v_{1})}\right)\right]\,.\] (C.76)

However, the genericity condition (A4) precisely rules this out, since the above equality must be violated for at least one \(\varphi\), concluding this last case.

To sum up, all cases either lead to a contradiction, or imply the conclusion that \((f^{-1},G)\sim_{\textsc{CRL}}(h,G^{\prime})\), concluding the proof. 

### Proof of Thm. 3.4

**Theorem 3.4** (Identifiability up to \(\sim_{\textsc{CRL}}\) from two paired perfect stochastic interventions per node).: _Suppose that we have access to multiple environments \(\{P_{\boldsymbol{X}}^{\epsilon}\}_{e\in\mathcal{E}}\) generated as described in SS 2 under Asms. 2.2, 2.3, 2.5, 2.8 and 2.9. Let \((h,G^{\prime})\) be any candidate solution such that the inferred latent distributions \(Q_{\boldsymbol{Z}}^{\epsilon}=h_{*}(P_{\boldsymbol{X}}^{e})\) of \(\boldsymbol{Z}=h(\boldsymbol{X})\) and the inferred mixing function \(h^{-1}\) satisfy the above assumptions w.r.t. the candidate causal graph \(G^{\prime}\). Assume additionally that_

1. _all densities_ \(p^{e}\) _and_ \(q^{e}\) _are continuously differentiable and fully supported on_ \(\mathbb{R}^{n}\)_;_
2. _we have access to at least one_ pair _of single-node perfect interventions per node, with unknown targets: there exist_ \(m\geq n\) _known pairs of environments_ \(\mathcal{E}=\{(e_{j},e_{j}^{\prime})\}_{j=1}^{m}\) _such that for each_ \(i\in[n]\) _there exists some_ unknown__\(j\in[m]\) _for which_ \(\mathcal{I}^{e_{j}}=\mathcal{I}^{e_{j}^{\prime}}=\{i\}\)_;_
3. _for all_ \(i\in[n]\)_, the intervened mechanisms_ \(\tilde{p}_{i}(v_{i})\) _and_ \(\tilde{\tilde{p}}_{i}(v_{i})\) _differ everywhere, in the sense that_ \[\forall v_{i}:\qquad\left(\frac{\tilde{\tilde{p}}_{i}}{\tilde{p}_{i}}\right)^{ \prime}(v_{i})\neq 0\,;\] (3.10)

_Then the ground truth is identified in the sense of Defn. 2.6, that is, \((f^{-1},G)\sim_{\textsc{CRL}}(h,G^{\prime})\)._

Proof.: First, we show that we can extract from the \(m\geq n\) available pairs of environments a suitable subset \(\mathcal{E}_{n}\) of exactly \(n\) pairs, containing one pair of interventional environments for each node.

Let \(\mathcal{E}_{n}\subseteq\mathcal{E}\) be a subset of \(n\) pairs of environments which are assumed to correspond to distinct targets in the model \(q\), and suppose for a contradiction that this is not actually the case for the ground truth \(p\) (i.e., there are duplicate and missing interventions w.r.t. \(p\)). Then there must be two pairs of environments \((e_{a},e_{a}^{\prime}),(e_{b},e_{b}^{\prime})\in\mathcal{E}_{n}\), both corresponding to interventions on some \(V_{i}\) in \(p\), but which are modelled as interventions on distinct nodes \(Z_{j}\) and \(Z_{k}\) with \(j\neq k\) in \(q\). We show that this implies that \(V_{i}\) must simultaneously be a deterministic function of only \(Z_{j}\) and only \(Z_{k}\). Similar to the proof of Thm. 3.2, we obtain the following equations,

\[\frac{\tilde{q}_{j}}{\tilde{q}_{j}}\left(z_{j}\right) =\frac{\tilde{p}_{i}}{\tilde{p}_{i}}\left(\psi_{i}(\bm{z})\right)\,,\] (C.77) \[\frac{\tilde{q}_{k}}{\tilde{q}_{k}}\left(z_{k}\right) =\frac{\hat{\tilde{p}}_{i}}{\tilde{p}_{i}}\left(\psi_{i}(\bm{z}) \right)\,.\] (C.78)

By taking partial derivatives w.r.t. \(z_{l}\) and applying assumption (A3'), we find that

\[\frac{\partial\psi_{i}}{\partial z_{l}} =0\qquad\forall l\neq j\,,\] (C.79) \[\frac{\partial\psi_{i}}{\partial z_{l}} =0\qquad\forall l\neq k\,.\] (C.80)

Since \(j\neq k\), this implies that \(\partial\psi_{i}/\partial z_{l}=0\) for all \(l\) which contradicts invertibility of \(\psi\). Thus, by contradiction, we find that \(\mathcal{E}_{n}\) must contain exactly one pair of intervention per node also w.r.t. \(p\). For the remainder of the proof, we only consider \(\mathcal{E}_{n}\).

W.l.o.g., for any \((e_{i},e^{\prime}_{i})\in\mathcal{E}_{n}\) we now fix the intervention targets in \(p\) to \(\mathcal{I}^{e_{i}}=\mathcal{I}^{e^{\prime}_{i}}=\{i\}\) and let \(\pi\) be a permutation of \([n]\) such that \(\pi(i)\) denotes the inferred intervention target in \(q\) that by (A2') is shared across \((e_{i},e^{\prime}_{i})\). (We will show later that not all permutations are admissible, but only ones that preserve the partial order of \(G\).)

The first part of the proof is similar to Case 1 in the proof of Thm. 3.2. Consider the densities in environments \(e_{i}\) and \(e^{\prime}_{i}\), which are related through the change of variable formula by:

\[\tilde{q}_{\pi(i)}\left(z_{\pi(i)}\right)\prod_{j\in[n]\setminus\{\pi(i)\}}q_ {j}\left(z_{j}\mid\bm{z}_{\mathrm{pa}(j;G^{\prime})}\right) =\tilde{p}_{i}\left(\psi_{i}(\bm{z})\right)\prod_{j\in[n]\setminus\{i\}}p_{j }\left(\psi_{j}(\bm{z})\mid\psi_{\mathrm{pa}(j)}(\bm{z})\right)\left|\det\bm{J }_{\psi}(\bm{z})\right|\,,\] (C.81) \[\tilde{\tilde{q}}_{\pi(i)}\left(z_{\pi(i)}\right)\prod_{j\in[n] \setminus\{\pi(i)\}}q_{j}\left(z_{j}\mid\bm{z}_{\mathrm{pa}(j;G^{\prime})} \right) =\tilde{\tilde{p}}_{i}\left(\psi_{i}(\bm{z})\right)\prod_{j\in[n] \setminus\{i\}}p_{j}\left(\psi_{j}(\bm{z})\mid\psi_{\mathrm{pa}(j)}(\bm{z}) \right)\left|\det\bm{J}_{\psi}(\bm{z})\right|\,,\] (C.82)

where \(\bm{Z}_{\mathrm{pa}(j;G^{\prime})}\subseteq\bm{Z}\setminus\{Z_{j}\}\) denotes the parents of \(Z_{j}\) in \(G^{\prime}\).

Taking the quotient of the two equations yields

\[\frac{\tilde{\tilde{q}}_{\pi(i)}}{\tilde{q}_{\pi(i)}}\left(z_{\pi(i)}\right)= \frac{\tilde{\tilde{p}}_{i}}{\tilde{p}_{i}}\left(\psi_{i}(\bm{z})\right)\,.\] (C.83)

Next, for any \(j\neq\pi(i)\), taking partial derivatives w.r.t. \(z_{j}\) on both sides yields

\[0=\left(\frac{\tilde{\tilde{p}}_{i}}{\tilde{p}_{i}}\right)^{\prime}\left(\psi_ {i}(\bm{z})\right)\frac{\partial\psi_{i}}{\partial z_{j}}(\bm{z})\,.\] (C.84)

By assumption (A3'), the first term on the RHS is non-zero everywhere. Hence, (C.84) implies

\[\forall j\neq\pi(i),\,\forall\bm{z}:\quad\frac{\partial\psi_{i}}{\partial z_{j }}(\bm{z})=0\] (C.85)

from which we can conclude that

\[V_{i}=\psi_{i}\left(Z_{\pi(i)}\right)\] (C.86)

for all \(i\in[n]\). That is, \(\psi\) is the composition of the permutation \(\pi\) with an element-wise reparametrisation.

It remains to show that \(\pi\) must, in fact, be a graph isomorphism, which is equivalent to the statement

\[V_{i}\to V_{j}\quad\text{in}\quad G\quad\iff\quad Z_{\pi(i)}\to Z_{\pi(j)} \quad\text{in}\quad G^{\prime}.\] (C.87)( \(\implies\) ) Suppose for a contradiction that there exist \((i,j)\) such that \(V_{i}\to V_{j}\) in \(G\), but \(Z_{\pi(i)}\not\to Z_{\pi(j)}\) in \(G^{\prime}\).

The main idea is to demonstrate that the lack of such direct arrow implies a certain conditional independence which, by faithfulness, would contradict the unconditional dependence of \(V_{i}\) and \(V_{j}\).

Consider environment \(e_{i}\) in which there are perfect interventions on \(Z_{\pi(i)}\) and \(V_{i}\), which has the effect of removing all incoming arrows to \(Z_{\pi(i)}\) and \(V_{i}\) in the respective post-intervention graphs \(G^{\prime}_{Z_{\pi(i)}}\) and \(G_{\overline{V}_{i}}\).

As a result of this and the lack of direct arrow by assumption, any d-connecting path between \(Z_{\pi(i)}\) and \(Z_{\pi(j)}\) must enter the latter via \(\bm{Z}_{\mathrm{pa}(\pi(j);G^{\prime})}\)[95].

It then follows from Markovianity of \(q\) w.r.t. \(G^{\prime}\) that the following holds in \(Q^{e_{i}}_{\bm{Z}}\):

\[Z_{\pi(i)}\perp\!\!\!\perp Z_{\pi(j)}\mid\bm{Z}_{\mathrm{pa}(\pi(j);G^{\prime} )}\,.\] (C.88)

We now consider the corresponding implication for \(P^{e_{i}}_{\bm{V}}\). Define

\[\tilde{\bm{V}}=\left\{V_{k}=\psi_{k}\left(Z_{\pi(k)}\right):Z_{\pi(k)}\in\bm{Z }_{\mathrm{pa}(\pi(j);G^{\prime})}\right\}\subseteq\bm{V}\setminus\left\{V_{i },V_{j}\right\},\] (C.89)

and note that by assumption, \(Z_{\pi(i)}\not\in\bm{Z}_{\mathrm{pa}(\pi(j);G^{\prime})}\) and hence \(V_{i}\not\in\tilde{\bm{V}}\).

By applying the corresponding diffeomorphic functions \(\psi_{i}\) from (C.86) to (C.88), it follows from Lemma C.2 that

\[V_{i}\perp\!\!\!\perp V_{j}\mid\tilde{\bm{V}}\] (C.90)

in \(P^{e_{i}}_{\bm{V}}\). However, this violates faithfulness (Asm. 2.2) of \(P_{\bm{V}}\) to \(G\) since \(V_{i}\) and \(V_{j}\) are d-connected in \(G_{\overline{V}_{i}}\).

Thus, by contradiction, we must have \(Z_{\pi(i)}\to Z_{\pi(j)}\) in \(G^{\prime}\).

(\(\Longleftarrow\) ) Now, suppose for a contradiction that there exist \((i,j)\) such that \(Z_{\pi(i)}\to Z_{\pi(j)}\) in \(G^{\prime}\), but \(V_{i}\not\to V_{j}\) in \(G\).

By the same argument as before, we find that

\[V_{i}\perp\!\!\!\perp V_{j}\mid\bm{V}_{\mathrm{pa}(j)}\] (C.91)

in \(P^{e_{i}}_{\bm{V}}\), and thus by Lemma C.2

\[Z_{\pi(i)}\perp\!\!\!\perp Z_{\pi(j)}\mid\tilde{\bm{Z}}\] (C.92)

in \(Q^{e_{i}}_{\bm{Z}}\) where

\[\tilde{\bm{Z}}=\left\{Z_{\pi(k)}:V_{k}\in\bm{V}_{\mathrm{pa}(j)}\right\} \subseteq\bm{Z}\setminus\left\{Z_{\pi(i)},Z_{\pi(j)}\right\}.\]

However, this contradicts faithfulness of \(Q_{\bm{Z}}\) to \(G^{\prime}\). Hence, we must have that \(V_{i}\to V_{j}\) in \(G\).

This shows that \(\pi\) must be a graph isomorphism, thus concluding the proof. 

### Proof of Thm. 4.2

**Theorem 4.2** (Preservation of causal influences under \(\sim_{\text{\tiny CRL}}\)).: _Let \(P_{\bm{V}}\) be Markovian w.r.t. \(G\), let \(\pi\) be a graph isomorphism of \(G\), and let \(\phi\) be an element-wise diffeomorphism. Let \(\bm{Z}=\bm{P}_{\pi^{-1}}\circ\phi(\bm{V})\) and denote its induced distribution by \(Q_{\bm{Z}}\). Then for any \(V_{i}\to V_{j}\) in \(G\) we have \(\mathfrak{C}^{P_{\bm{V}}}_{i\to j}=\mathfrak{C}^{Q_{\bm{Z}}}_{\pi(i)\to\pi(j)}\)._

Proof.: First, recall that according to Defn. 4.1,

\[\mathfrak{C}^{P_{\bm{V}}}_{i\to j}:=D_{\text{\tiny KL}}\big{(}P_{\bm{V}}\parallel P ^{i\to j}_{\bm{V}}\big{)},\] (C.93)

where \(P^{i\to j}_{\bm{V}}\) denotes the interventional distribution obtained by replacing \(p_{j}\left(v_{j}\mid\bm{v}_{\mathrm{pa}(j)}\right)\) with

\[p^{i\to j}_{j}\big{(}v_{j}\mid\bm{v}_{\mathrm{pa}(j)\setminus\{i\}}\big{)}= \int_{\mathcal{V}_{i}}p_{j}\big{(}v_{j}\mid\bm{v}_{\mathrm{pa}(j)}\big{)}p_{i} (v_{i})\,\mathrm{d}v_{i}\,.\] (C.94)Writing out the KL divergence and noting that all terms except the interved mechanism \(j\) cancel inside the \(\log\), we obtain

\[\mathfrak{C}^{P_{\bm{\nu}}}_{i\to j}=\int_{\mathcal{V}}\log\left(\frac{p_{j} \left(v_{j}\mid\bm{v}_{\mathrm{pa}(j)}\right)}{\int_{\mathcal{V}_{i}}p_{j} \big{(}v_{j}\mid\bm{v}_{\mathrm{pa}(j)}\big{)}p_{i}(v_{i})\,\mathrm{d}v_{i}} \right)p(\bm{v})\,\mathrm{d}\bm{v}\,.\] (C.95)

and similarly

\[\mathfrak{C}^{Q\bm{z}}_{\pi(i)\to\pi(j)}=\int_{\bm{Z}}\log\left(\frac{q_{\pi(j )}\left(z_{\pi(j)}\mid\bm{z}_{\mathrm{pa}(\pi(j);G^{\prime})}\right)}{\int_{ \mathcal{Z}_{\pi(i)}}q_{\pi(j)}\left(z_{\pi(j)}\mid\bm{z}_{\mathrm{pa}(\pi(j); G^{\prime})}\right)q_{\pi(i)}(z_{\pi(i)})\,\mathrm{d}z_{\pi(i)}}\right)q(\bm{z})\, \mathrm{d}\bm{z}\,.\] (C.96)

Since \(\bm{Z}=\bm{P}_{\pi^{-1}}\circ\phi(\bm{V})\), we have \(V_{i}=\psi_{i}(Z_{\pi(i)})\) for all \(i\in[n]\) where \(\psi=\phi^{-1}\).

Thus, by the change of variable formula, and using the fact that \(\pi(\mathrm{pa}(i))=\mathrm{pa}(\pi(i);G^{\prime})\) since \(\pi:G\mapsto G^{\prime}\) is a graph isomorphism, we have for all \(i\in[n]\):

\[q_{\pi(i)}\left(z_{\pi(i)}\mid\bm{z}_{\mathrm{pa}(\pi(i);G^{\prime})}\right)=p _{i}\left(\psi_{i}\left(z_{\pi(i)}\right)\mid\psi_{\mathrm{pa}(i)}\left(\bm{z} _{\mathrm{pa}(\pi(i);G^{\prime})}\right)\right)\left|\frac{\mathrm{d}\psi_{i}} {\mathrm{d}z_{\pi(i)}}\left(z_{\pi(i)}\right)\right|\,,\] (C.97)

as well as for the marginal density

\[q_{\pi(i)}\left(z_{\pi(i)}\right)=p_{i}\left(\psi_{i}\left(z_{\pi(i)}\right) \right)\left|\frac{\mathrm{d}\psi_{i}}{\mathrm{d}z_{\pi(i)}}\left(z_{\pi(i)} \right)\right|\,,\] (C.98)

and

\[q(\bm{z})=p(\psi\circ\bm{P}_{\pi}(\bm{z}))\left|\det\bm{J}_{\psi}(\bm{z}) \right|\,.\] (C.99)

Substitution into the expression for \(\mathfrak{C}^{Q\bm{z}}_{\pi(i)\to\pi(j)}\) yields:

\[\mathfrak{C}^{Q\bm{z}}_{\pi(i)\to\pi(j)} =\int_{\bm{Z}}\log\left(\frac{p_{j}\left(\psi_{j}(z_{\pi(j)})\mid \psi_{\mathrm{pa}(j)}(\bm{z}_{\mathrm{pa}(\pi(j);G^{\prime})})\right)}{\int_{ \mathcal{Z}_{\pi(i)}}p_{j}\left(\psi_{j}(z_{\pi(j)})\mid\psi_{\mathrm{pa}(j)}( \bm{z}_{\mathrm{pa}(\pi(j);G^{\prime})})\right)p_{i}(\psi_{i}(z_{\pi(i)})) \left|\frac{\mathrm{d}\psi_{i}}{\mathrm{d}z_{\pi(i)}}(z_{\pi(i)})\right|\mathrm{ d}z_{\pi(i)}}\right)\] (C.100) \[\qquad p(\psi\circ\bm{P}_{\pi}(\bm{z}))\left|\det\bm{J}_{\psi}( \bm{z})\right|\mathrm{d}\bm{z}\,.\] (C.101) \[=\int_{\mathcal{V}}\log\left(\frac{p_{j}\left(v_{j}\mid\bm{v}_{ \mathrm{pa}(j)}\right)}{\int_{\mathcal{V}_{i}}p_{j}\big{(}v_{j}\mid\bm{v}_{ \mathrm{pa}(j)}\big{)}p_{i}(v_{i})\,\mathrm{d}v_{i}}\right)p(\bm{v})\,\mathrm{ d}\bm{v}\] (C.102) \[=\mathfrak{C}^{P_{\bm{\nu}}}_{i\to j}\,.\] (C.103)

where the second to last line follows by integration by substitution, applied to both integrals.

Experimental Details and Additional Results

In this appemndix, we describe the experiments presented in SS \(6\) in more details (Appx. D.1), and present additional results (Appx. D.2).

### Experimental Details for SS 6

Synthetic Data Generating Process.We consider linear Gaussian latent SCMs of the form

\[V_{1}:=U_{1},\qquad\qquad V_{2}:=\alpha V_{1}+U_{2},\] (D.1)

with standard normal \(U_{1}\) and \(U_{2}\). As a mixing function, we use a three-layer multilayer perceptron (MLP),

\[f=\sigma\circ\bm{W}_{3}\circ\sigma\circ\bm{W}_{2}\circ\sigma\circ\bm{W}_{1}\] (D.2)

where \(\bm{W}_{1},\bm{W}_{2},\bm{W}_{3}\in\mathbb{R}^{2\times 2}\) are invertible weight matrices, and \(\sigma\) is an element-wise invertible nonlinear leaky-tanh activation function used in [41]:

\[\sigma(x)=\tanh(x)+0.1x\,.\] (D.3)

To compute averages of our results over multiple runs, we construct different ground truth data generating processes as follows. We generate different latent SCMs by drawing \(\alpha\) uniformly from \([-10,-2]\cup[2,10]\). (We exclude \((-2,2)\) to avoid sampling near unfaithful models.) We generate the corresponding mixing functions by uniformly sampling each element of the weight matrices, \((\bm{W}_{k})_{ij}\sim U(0,1)\). (To avoid the sampled weight matrices being too close to singular, we reject and resample if \(|\det\bm{W}_{k}|<0.1\).)

Interventional Environments.In line with Thm. 3.2, for each choice of latent SCM and mixing function, we generate three environments: one observational environment and one interventional environment for each perfect single-node intervention. For \(i=1,2\), we model a perfect intervention on \(V_{i}\) by removing the influence of the parent variables and changing the exogenous noise by shifting its mean up or down. Specifically, we replace the corresponding assignment in (D.1) by

\[V_{i}:=\tilde{U}_{i}\,,\quad\text{where}\quad\tilde{U}_{i}\sim\mathcal{N}(m_{i },1)\] (D.4)

where the mean \(m_{i}\) of the shifted Gaussian noise is fixed per environment and sampled uniformly from \(\{\pm 2\}\).

We label the observational environment as \(e=0\) and the environment arising from intervention on \(V_{i}\) by \(e=i\) for \(i=1,2\). Samples from \(p^{e}\) are then generated by sampling latents \(\bm{v}\) from the respective (un)intervened SCM and then applying the mixing function.

Model Architecture.We use normalizing flows [93] to model observations \(\bm{x}\) as the result of an invertible, differentiable transformation \(g\) of some latent (noise) variable \(\bm{z}\),

\[\bm{x}=g(\bm{z})\,.\] (D.5)

We apply a series of \(L\) such transformations \(g^{l}:\mathbb{R}^{2}\to\mathbb{R}^{2}\) such that \(g=g^{L}\circ\ldots\circ g^{1}\) which we refer to as _flow layers_. We use Neural Spline Flows [30] for the invertible transformation, with a 3-layer feedforward neural network with hidden dimension \(128\) and permutation in each flow layer and \(L=12\) layers. The transformations \(g,g^{1},\ldots,g^{L}\) have learnable parameters (the weights and biases of the neural networks), which we omit to simplify notation.

Typically, simple distributions such as a uniform or isotropic Gaussian are used as base distribution \(q(\bm{z})\) in normalizing flows. Here, we instead choose a base distribution that encodes information about the latent SCM. Specifically, we model the base mechanism as

\[q_{1}(z_{1})=\mathcal{N}\left(\mu_{1},\sigma_{1}^{2}\right)\,,\qquad q_{2}(z_{2 }\mid z_{1})=\mathcal{N}\left(\hat{\alpha}z_{1},\sigma_{2}^{2}\right)\,,\qquad q _{2}(z_{2})=\mathcal{N}\left(\mu_{2},\hat{\sigma}_{2}^{2}\right)\] (D.6)

and the intervened mechanism as

\[\tilde{q}_{1}(z_{1})=\mathcal{N}(\tilde{\mu}_{1},\tilde{\sigma}_{1}^{2})\,, \qquad\tilde{q}_{2}(z_{2})=\mathcal{N}(\tilde{\mu}_{2},\tilde{\sigma}_{2}^{2} )\,.\] (D.7)Candidate Graphs and Intervention Targets.We train a separate normalizing-flow based model for each choice of candidate graph \(G^{\prime}\) and inferred intervention targets. For the bivariate case with \(n=2\), this gives rise to four models, depending on whether \(G^{\prime}\) matches \(G\) or not, and whether the intervention targets are aligned or misaligned w.r.t. the ground truth intervention targets. To model the setting \(G^{\prime}\neq G\) in which \(Z_{1}\) and \(Z_{2}\) are assumed independent, we use \(q_{2}(z_{2})\) in place of \(q_{2}(z_{2}\mid z_{1})\) in (D.6). If the intervention targets are aligned, we use \(\tilde{q}_{i}\) instead of \(q_{i}\) in \(e=i\) for \(i=1,2\). Else, if they are misaligned, we use \(\tilde{q}_{2}\) instead of \(q_{2}\) in \(e=1\) and \(\tilde{q}_{1}\) instead of \(q_{1}\) in \(e=2\). By multiplying the respective mechanisms, we thus obtain three environment-specific joint base distributions \(q^{e}(\bm{z})\) for \(e=0,1,2\).

Learning Objective.Given multi-environment data, the parameters \(\mu_{1}\), \(\sigma_{1}\), \(\hat{\alpha}\), \(\sigma_{2}\), \(\mu_{2}\), \(\tilde{\sigma}_{2}\), \(\tilde{\mu}_{1}\) and \(\tilde{\sigma}_{1}\) are jointly learned with the parameters of the invertible transformations \(g^{l}\) by maximising the log-likelihood of the data under our model, which is given by:

\[\sum_{e\in\mathcal{E}}\mathbb{E}_{\bm{x}\sim p^{e}(\bm{x})}\left[\log p^{e}_{ \text{model}}(\bm{x})\right]=\sum_{e\in\mathcal{E}}\mathbb{E}_{\bm{x}\sim p^{e }(\bm{x})}\left[\log q^{e}(h(\bm{x}))+\log\left|\det\bm{J}_{h}(\bm{x})\right|\right]\] (D.8)

where the encoder \(h:=g^{-1}\) is the inverse of the normalizing flow which is readily available by construction; and where the expectations are empirical averages over the respective datasets in practice.

Training and Model Selection Details.Each environment comprises a total of \(200\)k data points. We use the ADAM optimizer [67] with cosine annealing learning rate scheduling, starting with a learning rate of \(5\times 10^{-3}\) and ending with \(1\times 10^{-7}\). We train the model for \(200\) epochs with a batch size of \(4096\). We split the dataset into \(70\%\) for training, and \(15\%\) for validation and held-out test data, each sampled randomly across all environments. For each drawn data generating process, we train three versions of each model with different random initializations and select the one with the highest validation log likelihood at the end of training for evaluation.

Evaluation Metrics.We evaluate the trained models w.r.t. _mean correlation coefficient_ (MCC) on held-out data and _log-likelihood_ on validation data (for model selection).

* The MCC measures the extent to which there is a one-to-one correspondence between the ground truth latents \(V_{i}\) and (a permuted version of) the inferred latents \(Z_{i}=h_{i}(\bm{X})\). Its maximum value of one indicates a perfect correlation between the two. MCC is thus a proxy measure for the level of identifiability up to permutation and invertible reparametrisation. We report MCC based on Pearson (linear) correlation, though we found the results based on Spearman (nonlinear monotonic) correlation to be almost identical.
* The log-likelihood, on the other hand, measures how well a model explains or fits the data. Since the ground truth is typically unknown, a reasonable procedure when training multiple models is to select the one that attains the highest likelihood. For this reason, we report the difference in log-likelihood between misspecified models (ones assuming a wrong graph or intervention targets) to the correctly specified model. Whenever this difference is larger than zero, the correct model fits the data better and would thus be selected.

### Additional Results: Learning Nonlinear Latent SCMs from Partial Causal Order

In this subsection, we present an additional experiment, in which we extend the setting investigated in SS 6 and Appx. D.1 along the following axes.

* [leftmargin=*,noitemsep,topsep=0pt]
* We fit generative models over three instead of two variables, corresponding to the setting of Thm. 3.4.
* The ground-truth SCM is now given by nonlinear mechanisms with non-additive, non-Gaussian noise.
* The generative model, including the learnt mechanisms, is now fully nonlinear.
* Despite Thm. 3.4 formally requiring two environments per single-node intervention, we only provide one interventional environment per node.
* Rather than searching over candidate graphs, we only fix the causal order and fit the reduced form of the SCM (see SS 2.1) with a second normalizing flow.

Below, we describe these differences in more detail.

Three-Variable Graph.The _unknown_ ground truth graph is given by

\[V_{2}\gets V_{1}\to V_{3}\,.\] (D.9)

This is consistent with the partial ordering \(V_{1}\preceq V_{2}\preceq V_{3}\), which is assumed for all models a priori w.l.o.g., see SS 2.2. Note that, due to the encoding of causal structure in the nonparametric model explained below, we only iterate over different permutations of the intervention targets and not over latent graph configurations. Due to the causal order implied by the graph (D.9), the permutations \((1,2,3)\) (no permutaion) and \((1,3,2)\) (permutation of the two effects) are equivalent since the latter also implies the correct causal ordering.

Nonlinear, Non-Gaussian SCM.The mechanisms in the ground-truth SCM are now given by

\[V_{i}:=\beta f_{i}^{\mathrm{loc}}(\bm{V}_{\mathrm{pa}(i)})+f_{i}^{\mathrm{ scale}}(\bm{V}_{\mathrm{pa}(i)})U_{i}\] (D.10)

for all \(i\), where the location and scale functions \(f_{i}^{\mathrm{loc}}\), \(f_{i}^{\mathrm{scale}}:\mathbb{R}^{|\mathrm{pa}(i)|}\to\mathbb{R}\) are parameterized by random 3-layer neural networks (sampled as the random mixing function in (D.2)) and the noise variables are Gaussian, \(U_{i}\sim\mathcal{N}(0,1)\). The factor \(\beta\) controls the influence of the parent variables relative to the exogenous noise. As \(\beta\) increases, variables tend to become more dependent, as also the mean shifts as a function of the parent variables. We set \(\beta=10\) for the experiments shown in Fig. 4.

Nonparametric Latent SCM.We use a second normalizing flow to learn a _reduced form of the latent SCM_ via the transformation \(g^{\mathrm{SCM}}:\mathbb{R}^{3}\to\mathbb{R}^{3}\) mapping an exogenous noise variable \(\bm{\epsilon}\) to the latent variable \(\bm{z}\),

\[\bm{z}=g^{\mathrm{SCM}}(\bm{\epsilon})\,.\] (D.11)

The distribution of the exogenous noise variable \(\bm{\epsilon}\) as well as the distribution of the intervened mechanisms \(\tilde{q}_{i}(z_{i})\) for \(i=1,2,3\) is fixed and standard (isotropic) Gaussian. The flow layers in \(g^{\mathrm{SCM}}\) have an _upper triangular Jacobian_ and thus allow us to encode assumptions about the causal graph: by passing the variables in topological order, which we can assume w.l.o.g., we ensure that an exogenous noise variable \(\epsilon_{i}\) can only influence endogenous variables in \(\bm{z}\) that are descendants of \(z_{i}\). The learned weights of the flow layers then implicitly encode which endogenous variables are connected. Therefore, only different choices of the permutations of the intervention targets need to be considered as candidate models. We use a similar architecture based on Neural Spline Flows. However, we omit permutation layers, which would violate the topological order of the variables.

Figure 4: **Comparison of Correctly and Incorrectly Specified Models for \(V_{2}\gets V_{1}\to V_{3}\) with Fixed Causal Order and Nonlinear SCM. Each violinplot corresponds to one setting where the intervention target labels are permuted. The blue plot (\(123^{*}\)) is the setting with correct intervention target labels. The yellow plot (\(132^{(*)}\)) has the targets for the two children \(V_{2}\) and \(V_{3}\) permuted, which also corresponds to a correct causal ordering and should thus be considered equivalent. We show mean correlation coefficients (MCCs) between the learned and ground truth latents _(Left)_ and the difference in validation model log-likelihood between the well-specified (blue) and misspecified models _(Right)_. Each violin plot is based on 20 different ground truth data generating processes; the horizontal lines indicate the minimum, median and maximum values.**

Results.In Fig. 4, we present identifiability scores and model fits for both well-specified and misspecified models (corresponding to different intervention target choices). Notably, we observe that the well-specified model (in blue) or its equivalent (in yellow) yield the highest log-likelihood in the majority of cases, as depicted in Fig. 4 (_Right_). This demonstrates that, even in this nonparametric setting without fully specified graph, the log-likelihood remains a reliable criterion for selecting the correct intervention targets. Fig. 4 (_Left_) shows that the selected models (blue or yellow) approximately identify the ground-truth latent variables up to element-wise rescaling, whereas other choices lead to much lower MCCs.

It is worth noting that, compared to the parametric setting investigated in SS 6 and Fig. 3, the nonparametric setting appears to be more challenging (as expected), as there is a less pronounced distinction between well-specified and misspecified models, both in terms of identifiability scores and model fits. Moreover, future work is needed to parse the implicitly learned causal relationships in the transformation \(g^{\mathrm{SCM}}\) in (D.11): since only the (pre-imposed) causal order is specified, in practice, \(g^{\mathrm{SCM}}\) may learn to use additional or fewer edges than in the true graph \(G\).

## Appendix E Discussion of the Role of Our Assumptions

Below, we summarize the rationale and intuition behind each assumption:

* Asm. 2.2 helps rule out degenerate cases (cancellation along different paths) in which variables are (conditionally) independent despite being causally related. It is a standard assumption in classical causal discovery from observational data, and therefore also helps in CRL to recover the true causal graph.
* Asm. 2.3 is required to know how many latent variables we are looking for. It is a standard assumption in identifiable representation learning (that is often made implicitly). However, it may be dropped when suitable techniques for estimating the intrinsic dimensionality of \(\mathcal{X}\) can be employed.
* Asm. 2.5 is needed for the mapping between latents and observations to be invertible in the first place. Without it, full recovery of the causal variables (up to CRL equivalence) is infeasible. This assumption is also standard for the simpler problem of nonlinear ICA.
* Asm. 2.8 is a characterisation of our generative setup. Sharing of some mechanisms and the mixing function is needed for the multi-environment setting to provide useful additional information: if everything may change across environments, the datasets can only be analysed in isolation, running into the non-identifiability of CRL from iid data.
* Asm. 2.9 and (A2) / (A2') are needed since with imperfect interventions or interventions not on all nodes, identifiability is not achievable even in the linear setting as shown by Squires et al. [117].
* Asm. (A1) is a technical assumption needed for our analysis. It is not strictly necessary (it can also be relaxed to fully supported on a Cartesian product of intervals) but substantially eases the readability and accessibility of the proof, without a major impact on the main causal aspects of the problem setup.
* Asm. (A3) / (A3') is needed to avoid spurious solutions based on applying a measure preserving transformation on a part of the domain unaffected by the intervention.
* Asm. (A4) is needed to rule out a fine-tuning of the ground-truth generating process that are possible due to fully non-parametric nature of the setup, see also Remark 4.2 and the following paragraph.