# Credal Marginal MAP

 Radu Marinescu

IBM Research, Ireland

radu.marinescu@ie.ibm.com &Debarun Bhattacharjya

IBM Research, USA

debarunb@us.ibm.com &Junkyu Lee

IBM Research, USA

junkyu.lee@ibm.com &Alexander Gray

IBM Research, USA

alexander.gray@ibm.com &Fabio Cozman

Universidade de Sao Paulo, Brazil

fgcozman@usp.br

###### Abstract

Credal networks extend Bayesian networks to allow for imprecision in probability values. Marginal MAP is a widely applicable mixed inference task that identifies the most likely assignment for a subset of variables (called MAP variables). However, the task is extremely difficult to solve in credal networks particularly because the evaluation of each complete MAP assignment involves exact likelihood computations (combinatorial sums) over the vertices of a complex joint credal set representing the space of all possible marginal distributions of the MAP variables. In this paper, we explore Credal Marginal MAP inference and develop new exact methods based on variable elimination and depth-first search as well as several approximation schemes based on the mini-bucket partitioning and stochastic local search. An extensive empirical evaluation demonstrates the effectiveness of our new methods on random as well as real-world benchmark problems.

## 1 Introduction

Probabilistic graphical models such as Bayesian networks [1] provide a powerful framework for reasoning about conditional (in)dependency structures over many variables. Marginal MAP (MMAP) is the task that identifies the most likely instantiation for a subset of variables given some evidence in a Bayesian network. Since MMAP inference distinguishes between maximization variables (called MAP variables) and summation variables, it is computationally more difficult than either max- or sum-inference tasks alone, primarily because summation and maximization operations do not commute; this forces processing along constrained variable orderings that may have significantly higher induced widths [2, 3, 4]. MMAP is NPPP-complete, but despite its complexity, it is often the appropriate task for problems that involve hidden variables such as conformant planning [5], image segmentation with hidden fields [6], or probabilistic diagnosis in healthcare [7].

In many practical situations it may not always be possible to provide the precise specification of a Bayesian network's parameters (i.e., probability values). Credal networks [8] provide an elegant extension to Bayesian networks that retain the graphical appeal of the latter while allowing for a more flexible quantification of the probability values via credal sets. This ability to represent and reason with imprecision in probability values allows credal networks to often reach more conservative and robust conclusions than Bayesian networks. Over the past three decades, the bulk of research hasfocused on developing marginal inference algorithms that are concerned with computing efficiently the marginal probability of a query variable given some evidence in the credal network [9].

Abductive reasoning tasks such as explaining the evidence in a credal network with or without hidden (unobserved) variables (i.e., Marginal MAP inference) are equally important to consider in practice. For example, in a hypothetical medical diagnosis situation modeled as a credal network, one may be interested in identifying the most likely combination of underlying medical conditions that determine a negative CT scan result and the presence of severe memory loss [10; 11]. Furthermore, since probabilistic structural causal models can be mapped exactly into equivalent credal networks, credal Marginal MAP inference could also be used to enable effective counterfactual analysis [12].

ContributionsIn this paper, we address the Marginal MAP inference task in credal networks. Specifically, we define the credal Marginal MAP (CMMAP) task as finding an assignment to a subset of variables that has maximum _upper_ (respectively, _lower_) _marginal probability_. We focus first on exact inference and propose a variable elimination as well as a depth-first search scheme for CMMAP. The complexity analysis of these exact methods indicates that they are likely to be limited to very easy problems. Therefore, we subsequently propose a mini-bucket partitioning based approximation of variable elimination for CMMAP as well as a family of approximate search based schemes that combine stochastic local search algorithms such as hill climbing, taboo search and simulated annealing with approximate maginal inference for credal networks. We evaluate empirically the new CMMAP inference algorithms on random credal networks with different graph topologies as well as a collection of credal networks derived from real-world applications. Our experimental results show that the exact approaches are limited to solving relatively small scale problems, while the approximation schemes can scale to much larger and practical problems.

The supplementary material includes additional details, experimental results, code and benchmarks.

## 2 Background

### Bayesian Networks

A _Bayesian network_ (BN) [1] is defined by a tuple \(\langle\mathbf{X},\mathbf{D},\mathbf{P},G\rangle\), where \(\mathbf{X}=\{X_{1},\ldots,X_{n}\}\) is a set of variables over multi-valued domains \(\mathbf{D}=\{D_{1},\ldots,D_{n}\}\), \(G\) is a directed acyclic graph (DAG) over \(\mathbf{X}\) as nodes where each \(X_{i}\) has a set of parents \(\Pi_{i}\), and \(\mathbf{P}\) is a set of _conditional probability tables_ (CPTs) where each \(P_{i}=P(X_{i}|\Pi_{i})\). A Bayesian network represents a joint probability distribution over \(\mathbf{X}\), namely \(P(\mathbf{X})=\prod_{i=1}^{n}P(X_{i}|\Pi_{i})\).

Let \(\mathbf{X}_{M}=\{X_{1},\ldots,X_{m}\}\) be a subset of \(\mathbf{X}\) called MAP variables and \(\mathbf{X}_{S}=\mathbf{X}\setminus\mathbf{X}_{M}\) be the complement of \(\mathbf{X}_{M}\), called sum variables. The Marginal MAP (MMAP) task seeks an assignment \(\mathbf{x}_{M}^{*}\) to variables \(\mathbf{X}_{M}\) having maximum probability. This requires access to the marginal distribution over \(\mathbf{X}_{M}\), which is obtained by summing out variables \(\mathbf{X}_{S}\):

\[\mathbf{x}_{M}^{*}=\operatorname*{argmax}_{\mathbf{X}_{M}}\sum_{\mathbf{X}_{ S}}\prod_{i=1}^{n}P(X_{i}|\Pi_{i})\] (1)

MMAP is a mixed inference task (max-sum) and its complexity is known to be \(\mathrm{NP}^{\mathrm{PP}}\)-complete [3]. Over the past decades, several algorithmic schemes have been developed for solving MMAP efficiently. We later overview the most relevant exact and approximate algorithms for MMAP.

### Credal Networks

A set of probability distributions for variable \(X\) is called a _credal set_ and is denoted by \(K(X)\)[13]. Similarly, a _conditional credal set_ is a set of conditional distributions, obtained by applying Bayes rule to each distribution in a credal set of joint distributions [14]. We consider credal sets that are closed and convex with a finite number of vertices. Two credal sets \(K(X|Y=y_{1})\) and \(K(X|Y=y_{2})\), where \(y_{1}\) and \(y_{2}\) are two distinct values of variable \(Y\), are called _separately specified_ if there is no constraint on the first set that is based on the properties of the second set.

A _credal network_ (CN) [8] is defined by a tuple \(\langle\mathbf{X},\mathbf{D},\mathbf{K},G\rangle\), where \(\mathbf{X}=\{X_{1},\ldots,X_{n}\}\) is a set of discrete variables with finite domains \(\mathbf{D}=\{D_{1},\ldots,D_{n}\}\), \(G\) is a directed acyclic graph (DAG) over \(\mathbf{X}\) as nodes, and \(\mathbf{K}=\{K(X_{i}|\Pi_{i}=\pi_{ik})\}\) is a set of separately specified conditional credal sets for each variable \(X_{i}\) and each configuration \(\pi_{ik}\) of its parents \(\Pi_{i}\) in \(G\). The _strong extension_\(K(\mathbf{X})\) of a credal network is the _convex hull_ (denoted CH) of all joint distributions that satisfy the following Markov property: every variable is strongly independent of its non-descendants conditional on its parents [8] (see also [8] for more details on strong conditional independence).

\[K(\mathbf{X})=CH\{P(\mathbf{X})\ :\ P(\mathbf{X})=\prod_{i=1}^{n}P(X_{i}|\Pi_{i}),P(X_{i}|\Pi_{i}=\pi_{ik})\text{ is a vertex of }K(X_{i}|\Pi_{i}=\pi_{ik})\}\] (2)

**Example 1**.: _Figure 0(a) shows a simple Bayesian network with 5 bi-valued variables \(\{A,B,C,D,E\}\). The conditional probability tables are shown next to the nodes. For example, we have that \(P(B=1|A=0)=0.2\) and \(P(B=1|A=1)=0.6\), respectively. In Figure 0(b) we show a credal network defined over the same set of variables. In this case, the conditional credal sets associated with the variables are given by closed probability intervals such as, for example, \(0.1\leq P(B=1|A=0)\leq 0.3\) and \(0.5\leq P(B=1|A=1)\leq 0.7\), respectively._

Unlike in Bayesian networks, a MAP assignment in a credal network may correspond to more than one marginal distribution. Therefore, we define the following two _Credal Marginal MAP_ (CMMAP) tasks:

**Definition 1** (maximin).: _Let \(\mathcal{C}=\langle\mathbf{X},\mathbf{D},\mathbf{K},G\rangle\) be a credal network whose variables are partitioned into MAP variables \(\mathbf{X}_{M}\) and sum variables \(\mathbf{X}_{S}=\mathbf{X}\setminus\mathbf{X}_{M}\). The maximin Credal Marginal MAP task is finding the assignment \(\mathbf{x}_{M}^{*}\) to \(\mathbf{X}_{M}\) with the maximum lower marginal probability, namely:_

\[\mathbf{x}_{M}^{*}=\operatorname*{argmax}_{\mathbf{X}_{M}}\min_{P(\mathbf{X}) \in K(\mathbf{X})}\sum_{\mathbf{X}_{S}}\prod_{i=1}^{n}P(X_{i}|\Pi_{i})\] (3)

**Definition 2** (maximax).: _Let \(\mathcal{C}=\langle\mathbf{X},\mathbf{D},\mathbf{K},G\rangle\) be a credal network whose variables are partitioned into MAP variables \(\mathbf{X}_{M}\) and sum variables \(\mathbf{X}_{S}=\mathbf{X}\setminus\mathbf{X}_{M}\). The maximax Credal Marginal MAP task is finding the assignment \(\mathbf{x}_{M}^{*}\) to \(\mathbf{X}_{M}\) with the maximum upper marginal probability, namely:_

\[\mathbf{x}_{M}^{*}=\operatorname*{argmax}_{\mathbf{X}_{M}}\max_{P(\mathbf{X}) \in K(\mathbf{X})}\sum_{\mathbf{X}_{S}}\prod_{i=1}^{n}P(X_{i}|\Pi_{i})\] (4)

Solving CMMAP can be shown to be \(\text{NP}^{\text{NP}^{\text{NP}^{\text{NP}}}}\)-hard [15; 11]. Despite being much harder than MMAP, CMMAP is applicable for explaining evidence in imprecise probabilistic models [12; 16].

## 3 Related Work

An exact solution to the Bayesian MMAP task given by Equation (1) can be obtained by using the _variable elimination_ (VE) algorithm, a form of dynamic programming which eliminates the variables

Figure 1: Examples of Bayesian and credal networks with bi-valued variables.

along a constrained elimination order such that all sum variables are eliminated before the MAP variables [2]. The optimal configuration \(\mathbf{x}_{M}^{*}\) is obtained by a backward pass that proceeds from the last to the first MAP variable in the constrained elimination ordering and processes each MAP variable by an argmax operator conditioned on the previously instantiated MAP variables.

Alternatively, a depth-first branch and bound search can also be conducted to solve the MMAP exactly, guided by an unconstrained join-tree based upper bound which can be re-evaluated fully [17] or incrementally [18] at each step during search. More recently, a collection of exact schemes sensitive to the problem structure have emerged including depth-first branch and bound search, best-first search, memory efficient recursive best-first search as well as anytime weighted best-first search algorithms that traverse an AND/OR search space associated with the MMAP task [4; 19; 20; 21]. These algorithms are guided by an effective weighted mini-bucket partitioning-based heuristic function and are currently the state-of-the-art for exact MMAP inference.

Several approximation schemes for Bayesian MMAP inference, including mini-bucket partitioning, message-passing and variational methods, have been introduced over the years [22; 23; 24; 25; 26]. These methods, however, do not guarantee eventual optimality of their solutions without significantly increasing their memory requirements.

Stochastic local search algorithms have also been developed to approximate MMAP efficiently [3] while the algorithm introduced by [27] that propagates sets of messages between the clusters of a join-tree and the hybrid schemes combining depth-first and best-first AND/OR search [20] provide lower and upper bounds on the optimal MMAP value in an anytime manner.

## 4 Exact Credal Marginal MAP

In this section, we describe two exact algorithms for solving the Credal Marginal MAP tasks defined by Equations (2) and (3), respectively. Due to space limitation, we discuss only the _maximax_ case (the _maximin_ is analogous). Specifically, we present a variable elimination scheme that eliminates the variables either by summation or by maximization, as well as a depth-first search scheme that finds the optimal MAP assignment by traversing the search space defined by the MAP variables.

### Variable Elimination

Algorithm 1 describes our variable elimination procedure for CMMAP which extends the exact method developed previously for marginal inference tasks [9] and operates on _potentials_.

**Definition 3** (potential).: _Given a set of variables \(\mathbf{Y}\), a potential \(\phi(\mathbf{Y})\) is a set of non-negative real-valued functions \(p(\mathbf{Y})\) on \(\mathbf{Y}\). The product of two potentials \(\phi(\mathbf{Y})\) and \(\psi(\mathbf{Z})\) is defined by \(\phi(\mathbf{Y})\cdot\psi(\mathbf{Z})=\{p\cdot q:p\in\phi,q\in\psi\}\). The sum-marginal \(\sum_{\mathbf{Z}}\phi(\mathbf{Y})\) and the max-marginal \(\max_{\mathbf{Z}}\phi(\mathbf{Y})\) of a potential \(\phi(\mathbf{Y})\) with respect to a subset of variables \(\mathbf{Z}\subseteq\mathbf{Y}\) are defined by \(\sum_{\mathbf{Z}}\phi(\mathbf{Y})=\{\sum_{\mathbf{Z}}p(\mathbf{Y}):p\in\phi\}\) and \(\max_{\mathbf{Z}}\phi(\mathbf{Y})=\{\max_{\mathbf{Z}}p(\mathbf{Y}):p\in\phi\}\), respectively._Since the multiplication operator may grow the size of potentials dramatically, we introduce an additional pruning operation that can reduces the cardinality of a potential. Specifically, the operator \(\max\phi(\mathbf{Y})\) returns the set of non-zero maximal elements of \(\phi(\mathbf{Y})\), under the partial order \(\geq\) defined component-wise as \(p\geq q\) iff \(\forall\mathbf{y}_{k}\in\mathbf{D}_{\mathbf{Y}},p(\mathbf{y}_{k})\geq q(\mathbf{ y}_{k})\), where \(\mathbf{D}_{\mathbf{Y}}\) is the cartesian product of the domains of the variables in \(\mathbf{Y}\): \(\max\phi(\mathbf{Y})=\{p\in\phi(\mathbf{Y}):\nexists q\in\phi,q\geq p\}\).

Given a credal network \(\mathcal{C}=\langle\mathbf{X},\mathbf{D},\mathbf{K},G\rangle\) as input together with a partitioning of its variables into disjoint subsets \(\mathbf{X}_{M}\) (as MAP variables) and \(\mathbf{X}_{S}\) (as sum variables), algorithm CVE transforms each conditional credal set \(K(X_{i}|\Pi_{i})\) into a corresponding potential that contains the set of all conditional probability distributions in the strong extension of \(K(X_{i}|\Pi_{i})\) (lines 3-5). Subsequently, given an ordering \(o\) of the variables in which all the MAP variables come after the sum variables, the potentials are partitioned into buckets. A bucket is associated with a single variable \(X_{i}\) and contains every unallocated potential \(\phi\) that has \(X_{i}\) in its scope \(vars(\phi)\) (lines 6-9). The algorithm then processes each bucket, from first to last in the constrained elimination ordering \(o\), by multiplying all potentials in the current bucket and eliminating the bucket's variable (by summation for sum variables, and by maximization for MAP variables), resulting in a new potential which is first pruned by its non-maximal elements and then placed in a subsequent bucket, depending on its scope (lines 10-16). Following the top-down elimination phase, a bottom-up pass over the MAP buckets, from the last to the first MAP variable in the ordering, assembles the solution \(\mathbf{x}_{M}^{*}\) by selecting the value \(x_{i}^{*}\) of variable \(X_{i}\) that maximizes the combination of potentials in its bucket, conditioned on the already assigned MAP variables in the ordering (lines 18-21). Note that the bucket \(X_{i}\)'s combined potential may contain more than one components. In this case, we choose the value \(x_{i}^{*}\) that maximizes the largest number of components in that potential (breaking ties arbitrarily). Clearly, we have the following complexity result:

**Theorem 1** (complexity).: _Given a credal network \(\mathcal{C}\), the complexity of algorithm CVE is time and space \(O(n\cdot C\cdot k^{w^{*}_{o}})\), where \(n\) is the number of variables, \(k\) bounds the domain sizes, \(w^{*}_{o}\) is the induced width of the constrained elimination order \(o\) and \(C\) bounds the cardinality of the potentials._

### Depth-First Search

An alternative approach to solving CMMAP exactly is to conduct a depth-first search over the space of partial assignments to the MAP variables, and, for each complete MAP assignment \(\mathbf{x}_{M}\) compute its score as the exact upper probability \(\overline{P}(\mathbf{x}_{M})\). This way, the optimal solution \(\mathbf{x}_{M}^{*}\) corresponds to the configuration with the highest score. Evaluating \(\overline{P}(\mathbf{x}_{M})\) can be done by using a simple modification of the CVE algorithm described in the previous section. Specifically, given a complete assignment \(\mathbf{x}_{M}\) to the MAP variables, the modified CVE, denoted by CVE\({}^{+}\), computes an _unconstrained_ elimination ordering of all the variables regardless of whether they are MAP or summation variables. Then, for each MAP variable \(X_{i}\) and corresponding value \(x_{i}\in\mathbf{x}_{M}\), CVE\({}^{+}\) adds to the bucket of \(X_{i}\) a deterministic potential \(\phi(X_{i})=\{\delta_{x_{i}}\}\), where \(\delta_{x_{i}}\) returns one if \(X_{i}=x_{i}\) and zero otherwise. Finally, CVE\({}^{+}\) eliminates all variables by summation and obtains the desired upper probability bound after processing the bucket of the last variable in the ordering.

**Theorem 2** (complexity).: _Given a credal network \(\mathcal{C}\), the complexity of the depth-first search algorithm is time \(O(n\cdot C\cdot k^{m+w^{*}_{o}})\) and space \(O(n\cdot C\cdot k^{w^{*}_{o}})\), where \(n\) is the number of variables, \(m\) is the number of MAP variables, \(k\) is the maximum domain size, \(w^{*}_{n}\) is the induced width of the unconstrained elimination ordering \(u\) and \(C\) bounds the cardinality of the potentials._

## 5 Approximate Credal Marginal MAP

Solving the CMMAP task exactly is computationally hard and does not scale to large problems. Therefore, in this section, we present several approximation schemes using the mini-bucket partitioning as well as stochastic local search combined with approximate credal marginal inference.

### Mini-Buckets Approximation

The first approximation scheme is described by Algorithm 2 and adapts the mini-bucket partitioning scheme developed for graphical models [28] to the CMMAP task. Specifically, algorithm CMBE(\(i\)) is parameterized by an i-bound \(i\) and works by partitioning large buckets into smaller subsets, called _mini-buckets_, each containing at most \(i\) distinct variables (line 5). The mini-buckets are processedseparately, as follows: MAP mini-buckets (in \(\mathbf{X}_{M}\)) are eliminated by maximization, while variables in \(\mathbf{X}_{S}\) are eliminated by summation. In practice, however, for variables in \(\mathbf{X}_{S}\), one (arbitrarily selected) is eliminated by summation, while the rest of the mini-buckets are processed by maximization. Clearly, CMBE(\(i\)) outputs an upper bound on the optimal maximax CMMAP value from Equation 4.

**Theorem 3** (complexity).: _Given a credal network \(\mathcal{C}\), the complexity of algorithm CMBE(\(i\)) is time and space \(O(n\cdot C\cdot k^{i})\), where \(n\) is the number of variables, \(k\) is the maximum domain size, \(i\) is the mini-bucket i-bound and \(C\) bounds the cardinality of the potentials._

### Local Search

The second approximation scheme is described by Algorithm 3 and combines stochastic local search with approximate marginal inference for credal networks [29; 30]. More specifically, the basic idea behind the method is to start from an initial guess \(\mathbf{x}_{M}\) as a solution, and iteratively try to improve it by moving to a better neighbor \(\mathbf{x}^{\prime}_{M}\) that has a higher score. A _neighbor_\(\mathbf{x}^{\prime}_{M}\) of instantiation \(\mathbf{x}_{M}\) is defined as an instantiation \(\mathbf{x}^{\prime}_{M}\) which results from changing the value of a single variable \(X\) in \(\mathbf{x}_{M}\). For example, the neighbors of \(\mathbf{x}_{M}:(B=0,C=1,D=0)\) for the credal network from Figure 0(b) are \((B=1,C=1,D=0)\), \((B=0,C=0,D=0)\) and \((B=0,C=1,D=1)\), respectively. In this case, computing the score \(score(\mathbf{x}^{\prime}_{M})\) of a neighbor \(\mathbf{x}^{\prime}_{M}\) requires estimating the upper probability of the evidence \(\overline{P}(\mathbf{x}^{\prime}_{M})\) represented by the assignment \(\mathbf{x}^{\prime}_{M}\). This can be done efficiently using any of the approximation schemes developed for marginal inference in credal networks such as L2U [29], GL2U [30] or ApproxLP [31]. However, since these schemes were originally designed to compute the lower and upper marginal probabilities of a query variable \(Z=z\) conditioned on evidence \(\mathbf{Y}=\mathbf{y}\), we use a simple transformation of the credal network to evaluate the probability of evidence \(P(\mathbf{Y}=\mathbf{y})\) (see the supplementary material for more details). We present next three strategies for conducting the local search for CMMAP.

Stochastic Hill Climbing.Firstly, procedure SHC in Algorithm 3 describes our Stochastic Hill Climbing based approach for CMMAP. Specifically, SHC proceeds by repeatedly either changing the state of the variable that creates the maximum score change (line 13), or changing a variable at random (lines 9 and 15). The quality of the solution returned by the method depends to a large extent on which part of the search space it is given to explore. Therefore, our scheme restarts the search from a different initial solution which is initialized uniformly at random (lines 3-4).

Taboo Search.Secondly, procedure TS in Algorithm 3 implements the Taboo Search approach for CMMAP. Taboo search is similar to stochastic hill climbing except that the next neighbor of the current solution is chosen as the best neighbor that hasn't been visited recently. A taboo list maintains a portion of the previously visited solutions so that at the next step a unique point is selected. Our TS algorithm implements a random restarts strategy.

Simulated Annealing.Finally, procedure SA in Algorithm 3 describes our Simulated Annealing based scheme for CMMAP. The basic principle behind this approach is to consider some neighboring state \(\mathbf{x}^{\prime}_{M}\) of the current state \(\mathbf{x}_{M}\), and probabilistically decides between moving to state \(\mathbf{x}^{\prime}_{M}\) or staying in the current state. The probability of making the transition from \(\mathbf{x}_{M}\) to \(\mathbf{x}^{\prime}_{M}\) is specified by an acceptance probability function \(P(\mathbf{x}^{\prime}_{M},\mathbf{x}_{M},T)\) that depends on the scores of the two states as well as a global time-varying parameter \(T\) called _temperature_. We chose \(P(\mathbf{x}^{\prime}_{M},\mathbf{x}_{M},T)=e^{\frac{\Theta}{T}}\)where \(\Delta=\log\overline{P}(\mathbf{x}^{\prime}_{M})-\log\overline{P}(\mathbf{x}_{M})\). At each iteration, the temperature is decreased using a cooling schedule \(\sigma<1\). Like SHC and TS, algorithm SA implements a random restarts strategy.

**Theorem 4** (complexity).: _Given a credal network \(\mathcal{C}\), the complexity of algorithms SHC, TS and SA is time \(O(N\cdot M\cdot P)\) and space \(O(n)\), where \(n\) is the number of variables, \(N\) is the number of iterations, \(M\) is the maximum number of flips allowed per iteration, and \(P\) bounds the complexity of approximating the probability of evidence in \(\mathcal{C}\)._

## 6 Experiments

We evaluate the proposed algorithms for CMMAP on random credal networks and credal networks derived from real-world applications. All competing algorithms were implemented in C++ and the experiments were run on a 32-core machine with 128GB of RAM running Ubuntu Linux 20.04.

We consider the two exact algorithms denoted by CVE and DFS, as well as the four approximation schemes denoted by SHC, TS, SA and CMBE(\(i\)), respectively. The local search algorithms used \(N=10\) iterations and \(M=10,000\) maximum flips per iteration, and they all used the approximate L2U algorithm with 10 iterations [29] to evaluate the MAP assignments during search. Furthermore, for SHC we set the flip probability \(p_{flip}\) to \(0.2\), TS used a tabo list of size \(100\), while for SA we set the initial temperature and cooling schedule to \(T_{init}=100\) and \(\sigma=0.9\), respectively. For \(\text{CMBE}(i)\) we set the i-bound \(i\) to 2 and used the same L2U algorithm to evaluate the solution found. All competing algorithms were allocated a 1 hour time limit and 8GB of memory per problem instance.

In all our experiments, we report the CPU time in seconds, the number of problems solved within the time/memory limit and the number of times an algorithm converged to the best possible solution. The latter is called the number of _wins_ and is meant to be a measure of solution quality for the respective algorithm. We also record the number of variables (\(n\)), the number (or percentage) of MAP variables (Q) and the constrained induced widths (\(w^{*}\)). The best performance points are highlighted.

### Random Credal Networks

For our purpose, we generated random credal networks, \(m\)-by-\(m\) grid networks as well as \(k\)-tree networks. Specifically, for the random networks, we varied the number of variables \(n\in\{100,150,200\}\), for grids, we choose \(m\in\{10,14,16\}\), and for \(k\)-trees we selected \(k=2\) and the number of variables \(n\in\{100,150,200\}\), respectively. In all cases, the maximum domain size was set to 2 and the conditional credal sets were generated uniformly at random as probability intervals such that the difference between the lower and upper probability bounds was at most \(0.3\).

First, we note that the exact algorithms CVE and DFS could only solve very small problems with up to 10 variables and 5 MAP variables. The main reason for the poor performance of these algorithms is the extremely large size of the intermediate potentials generated during the variable elimination procedure which causes the algorithms to run out of memory or time on larger problems. Therefore, we omit their evaluation hereafter.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \(n\) & Q & \(w^{*}\) & \begin{tabular}{c} SHC \\ time (\#) \\ \end{tabular} & \begin{tabular}{c} TS \\ W \\ \end{tabular} & \begin{tabular}{c} SA \\ time (\#) \\ \end{tabular} & 
\begin{tabular}{c} CMBE(2) \\ time (\#) \\ \end{tabular} \\ \hline \hline \multicolumn{10}{c}{random} \\ \hline \hline \multirow{3}{*}{100} & 20 & 25 & 32.69 & 100 & 23.08 & 100 & **6.47** & 100 & 225.28 (70) & 1 \\  & 40 & 37 & 163.05 & 100 & 79.11 & 100 & **14.78** & 100 & 327.67 (43) & 0 \\  & 60 & 23 & 421.93 & 100 & 185.41 & 100 & **29.99** & 100 & 224.93 (7) & 0 \\ \hline \multirow{3}{*}{150} & 30 & 39 & 254.32 & 100 & 141.03 & 100 & **24.08** & 100 & 294.48 (43) & 0 \\  & 60 & 57 & 1143.78 & 100 & 531.45 & 100 & **70.06** & 100 & 555.98 (14) & 0 \\  & 90 & 66 & 2811.47 & 100 & 1259.79 & 100 & **139.78** & 75 & 925.41 (2) & 0 \\ \hline \multirow{3}{*}{200} & 50 & 58 & 1044.79 & 100 & 490.38 & 100 & **72.09** & 100 & 276.98 (38) & 0 \\  & 100 & 86 & 3496.77 (32) & 32 & 2143.79 & 100 & **211.47** & 14 & 927.31 (1) & 0 \\  & 150 & 69 & 3601.67 (16) & 16 & 3550.99 (17) & 17 & **339.34** & 72 & - (0) & 0 \\ \hline \hline \multicolumn{10}{c}{grid} \\ \hline \hline \multirow{3}{*}{100} & 20 & 25 & 31.35 & 100 & 22.77 & 100 & 4.66 & 100 & **0.07** & 2 \\  & 40 & 37 & 155.34 & 100 & 79.83 & 100 & 10.51 & 100 & **3.85** & 0 \\  & 60 & 23 & 358.81 & 100 & 168.79 & 100 & 19.18 & 100 & **28.76** & 0 \\ \hline \multirow{3}{*}{144} & 30 & 36 & 219.49 & 100 & 121.86 & 100 & 21.02 & 100 & **0.34** & 0 \\  & 60 & 53 & 878.63 & 100 & 426.70 & 100 & 54.77 & 100 & **1.03** & 0 \\  & 90 & 26 & 2109.47 & 100 & 958.13 & 100 & 102.93 & 73 & **27.79** & 0 \\ \hline \multirow{3}{*}{196} & 50 & 55 & 817.52 & 100 & 382.46 & 100 & 58.13 & 100 & **0.68** & 0 \\  & 100 & 56 & 3045.54 (94) & 94 & 1453.39 & 100 & 147.11 & 13 & **51.01** (98) & 0 \\  & 150 & 22 & 3601.25 (23) & 23 & 3011.47 (93) & 93 & 190.26 & 3 & **41.27** (99) & 2 \\ \hline \hline \multicolumn{10}{c}{\(k\)-tree} \\ \hline \hline \multirow{3}{*}{100} & 20 & 25 & 68.25 & 100 & 44.25 & 100 & **10.48** & 100 & 221.18 (55) & 0 \\  & 40 & 37 & 307.91 & 100 & 151.97 & 100 & **23.19** & 100 & 163.59 (8) & 0 \\  & 60 & 23 & 650.72 & 100 & 306.26 & 100 & **40.19** & 100 & - (0) & 0 \\ \hline \multirow{3}{*}{150} & 30 & 28 & 443.33 & 100 & 245.71 & 100 & **44.58** & 100 & 492.55 (26) & 0 \\  & 60 & 47 & 1647.29 & 100 & 724.01 & 100 & **106.71** & 100 & 14.68 (1) & 0 \\ \cline{1-1}  & 90 & 51 & 2917.01 (84) & 84 & 1541.91 & 100 & **192.76** & 82 & - (0) & 0 \\ \hline \multirow{3}{*}{200} & 50 & 45 & 1306.43 & 100 & 660.59 & 100 & **108.36** & 100 & 1199.83 & 0 \\ \cline{1-1}  & 100 & 64 & 3376.59 (54) & 54 & 1917.95 & 100 & **266.24** & 21 & - (0) & 0 \\ \cline{1-1}  & 1500 & 48 & 3602.98 (4) & 4 & 3334.49 (59) & 59 & **344.96** & 38 & - (0) & 0 \\ \end{tabular}
\end{table}
Table 1: Results on random, grid and \(k\)-tree credal networks. Mean CPU times in seconds, number of instance solved (#) and number of wins (W). Time limit 1 hour, 8GB of RAM.

Table 1 summarizes the results obtained on random, grid and \(k\)-tree networks. Each data point represents an average over 100 random problem instances generated for each problem size (\(n\)) and number of MAP variables (Q), respectively. Next to the running time we show the number of instances solved within the time/memory limit (if the number is omitted then all 100 instances were solved). We can see that in terms of running time, CMBE(2) performs best on the grid networks. This is because the intermediate potentials generated during elimination are relatively small size and therefore are processed quickly. However, the algorithm is not able converge to good quality solutions compared with its competitors. The picture is reversed on the random and \(k\)-tree networks where CMBE(2) is the worst performing algorithm both in terms of running time and solution quality. In this case, the relatively large intermediate potentials cause the algorithm to exceed the time and memory limits on many problem instances and thus impact negatively its performance.

The local search algorithms SHC, TS and SA yield the best performance in terms of solution quality with all three algorithms almost always converging to the best possible solutions on these problem instances. In terms of running time, SA is the fastest algorithm achieving almost one order of magnitude speedup over its competitors, especially for larger numbers of MAP variables (e.g., \(k\)-trees with \(n=100\) variables and \(Q=60\) MAP variables). Algorithms SHC and TS have comparable running times (with SHC being slightly slower than TS) but they are significantly slower than SA. This is due to the significantly larger computational overhead required for evaluating the scores of all the neighbors of the current state, especially when there are many MAP variables.

### Real-World Credal Networks

Table 2 shows the results obtained on a set of credal networks derived from 22 real-world Bayesian networks1 by converting the probability values in the CPTs into probability intervals such that the difference between the corresponding lower and upper probability bounds was at most 0.3. Furthermore, since the local search algorithms rely on the L2U approximation to evaluate the MAP configurations, we restricted the domains of the multi-valued variables to the first two values in the domain while shrinking and re-normalizing the corresponding CPTs. For each network we selected uniformly at random \(\tilde{Q}=50\%\) of the variables to act as MAP variable and generated 10 random instances. As before, we indicate next to the average running times the number of instances solved

\begin{table}
\begin{tabular}{l|r|r r|r r|r r|r r} problem & \(w^{*}\) & \multicolumn{2}{c|}{SHC} & \multicolumn{2}{c|}{TS} & \multicolumn{2}{c}{SA} & \multicolumn{2}{c}{CMBE(2)} \\  & & time (\#) & W & time (\#) & W & time (\#) & W & time (\#) & W \\ \hline \hline alarm & 12 & 27.32 (10) & 10 & 21.31 (10) & 10 & **4.89** (10) & 10 & 324.23 (10) & 0 \\ child & 7 & 3.51 (10) & 10 & 3.69 (10) & 10 & 1.19 (10) & 10 & **0.64** (10) & 0 \\ link & 239 & 3655.67 (2) & 2 & 3628.15 (2) & 1 & **1300.14** (10) & 8 & - (0) & 0 \\ insurance & 12 & 64.22 (10) & 10 & 45.77 (10) & 10 & **17.11** (10) & 10 & 97.16 (9) & 0 \\ hepar2 & 25 & 1734.11 (10) & 10 & 833.82 (10) & 10 & **163.16** (10) & 10 & - (0) & 0 \\ pathfinder & 33 & 2509.89 (10) & 10 & 79.78 (10) & 10 & **93.98** (10) & 10 & - (0) & 0 \\ hailfinder & 14 & 126.60 (10) & 10 & 72.73 (10) & 10 & **12.53** (10) & 10 & 531.52 (10) & 0 \\ largefam & 402 & - (0) & 0 & - (0) & 0 & **2903.55** (10) & 10 & - (0) & 0 \\ mastermind1 & 389 & - (0) & 0 & - (0) & 0 & **3600.85** (10) & 10 & - (0) & 0 \\ mastermind2 & 726 & - (0) & 0 & - (0) & 0 & **3617.17** (5) & 5 & - (0) & 0 \\ mastermind3 & 1193 & - (0) & 0 & - (0) & 0 & **3650.54** (3) & 3 & - (0) & 0 \\ mildew & 12 & 22.49 (10) & 10 & 15.22 (10) & 10 & 3.15 (10) & 10 & **0.16** (10) & 0 \\ munn & 175 & 3615.45 (5) & 4 & 3639.57 (3) & 3 & **652.72** (10) & 5 & - (0) & 0 \\ pedigree1 & 74 & 3603.87 (7) & 7 & 3609.44 (8) & 8 & **410.61** (10) & 0 & 1269.45 (8) & 0 \\ pedigree7 & 147 & 3620.74 (2) & 1 & 3625.51 (2) & 1 & **1689.89** (10) & 9 & - (0) & 0 \\ pedigree9 & 175 & - (0) & 0 & - (0) & 0 & 1719.92 (10) & 6 & **1128.50** (4) & 4 \\ win95pts & 28 & 3612.38 (6) & 6 & 3610.94 (6) & 6 & **821.20** (10) & 10 & - (0) & 0 \\ sandes & 75 & 3619.00 (6) & 6 & 3611.76 (6) & 6 & **3565.62** (10) & 3 & - (0) & 0 \\ xdiabetes & 75 & 3605.08 (9) & 9 & 3603.76 (10) & 10 & **175.76** (10) & 0 & 182.38 (6) & 0 \\ zbarey & 18 & 140.93 (10) & 10 & 82.36 (10) & 10 & 18.47 (10) & 10 & 863.64 (1) & 0 \\ zpigs & 105 & 3606.83 (4) & 4 & 3603.8 (5) & 5 & 234.84 (10) & 5 & **100.47** (2) & 0 \\ zwater & 16 & 207.07 (10) & 10 & 126.87 (10) & 10 & **44.68** (10) & 10 & - (0) & 0 \\ \end{tabular}
\end{table}
Table 2: Results on real-world credal networks with \(Q=50\%\) MAP variables. Mean CPU time in seconds, number of instances solved (#) and number of wins (W). Time limit 1 hour, 8 GB of RAM.

by the respective algorithms within the time and memory limits. We can see again that CMBE(2) is competitive only on the easiest instances (e.g., child, mildev) while SA yields the best performance in terms of both running time and solution quality on the majority of the problem instances. In summary, the relatively large potentials hinder CMBE's performance, while the computational overhead incurred during the evaluation of relatively large neighborhoods of the current state slows down significantly SHC and TS compared with SA.

### Applications

Figure 1(a) shows the credal network for the brain tumour diagnosis use case derived from the Bayesian network described in [11]. The variables are: MC - metastatic cancer, PD - Paget disease, B - brain tumour, ISC - increased serum calcium, H - headaches, M - memory loss, CT - scan result.

Considering the query variables \(B\) and \(ISC\), the exact solution for both maximax and maximin CMMAP is \((B=0,ISC=0)\) (obtained by both the CVE and DFS algorithms). In this case, the maximax and maximin scores are \(0.837\) and \(0.42\), respectively. Algorithms SHC, TS and SA also find the optimal configuration \((B=0,ISC=0)\) which is evaluated by L2U to \(0.8316\) for maximax CMMAP and to \(0.37296\) for maximin CMMAP, respectively.

Figure 1(b) shows the credal network for the intelligence report analysis described in [9]. The variables are: As - assassination, C - coup/revolt, R - regime change, D - decision to invade, At - attack, B - build-up, P - propaganda, I - invasion.

Considering the query variables \(D\), \(At\) and \(I\), the exact solution for both maximax and maximin CMMAP is \((D=0,At=0,I=0)\) and is obtained by both algorithms CVE and DFS. The corresponding scores are in this case \(0.765\) and \(0.458806\), respectively. The approximation schemes SHC, TS and SA also find the same optimal CMMAP configuration \((D=0,At=0,I=0)\) which is evaluated by L2U to \(0.69651\) for maximax CMMAP and to \(0.305486\) for maximin CMMAP, respectively.

We note that in both cases, the constrained induced width is 2 and therefore CMBE(2) coincides with the exact CVE. Therefore, all our approximation schemes found the optimal solutions.

## 7 Conclusions

The paper explores the Marginal MAP inference task in credal networks. We formally define the Credal Marginal MAP task and present new exact algorithms based on variable elimination and depth-first search. Subsequently, we introduce approximate algorithms using the mini-bucket partitioning or a combination of stochastic local search and approximate credal marginal inference. Our experiments on random and real-world credal networks demonstrate the effectiveness of our CMMAP algorithms. A potential direction for future work is to investigate branch-and-bound and best-first search strategies guided by a credal version of weighted mini-buckets [32].

Figure 2: Applications of Credal Marginal MAP inference.

## Acknowledgements

Fabio Cozman was supported in part by C4AI funding (FAPESP and IBM Corporation, grant 2019/07665-4) and CNPq (grant 305753/2022-3).

## References

* [1]J. Pearl (1988) Probabilistic reasoning in intelligent systems. Morgan Kaufmann. Cited by: SS1.
* [2]R. Dechter (1999) Bucket elimination: a unifying framework for reasoning. Artificial Intelligence113, pp. 41-85. Cited by: SS1.
* [3]J. Park and A. Darwiche (2003) Solving MAP exactly using systematic search. In Uncertainty in Artificial Intelligence (UAI), pp. 459-468. Cited by: SS1.
* [4]R. Marinescu, R. Dechter, and A. Ihler (2014) AND/OR search for marginal MAP. In Uncertainty in Artificial Intelligence (UAI), pp. 563-572. Cited by: SS1.
* [5]J. Lee, R. Marinescu, and R. Dechter (2016) Applying search based probabilistic inference algorithms to probabilistic conformant planning: preliminary results. In Proceedings of the International Symposium on Artificial Intelligence and Mathematics (ISAIM), Cited by: SS1.
* [6]J. M. Bioucas-Dias and M. A. T. Figueiredo (2016) Bayesian image segmentation using hidden fields: supervised, unsupervised, and semi-supervised formulations. In Proceedings of the European Signal Processing Conference (EUSIPCO), pp. 523-527. Cited by: SS1.
* [7]E. Kyrimi, S. McLachlan, K. Dube, M. R. Neves, A. Fahmi, and N. Fenton (2021) A comprehensive scoping review of bayesian networks in healthcare: past, present and future. Artificial Intelligence in Medicine117. Cited by: SS1.
* [8]F. Cozman (2000) Generalizing variable-elimination in Bayesian networks. In Workshop on Probabilistic Reasoning in Bayesian Networks at SBIA/Iberamia 2000, pp. 21-26. Cited by: SS1.
* [9]D. Maua and F. Cozman (2020) Thirty years of credal networks: specifications, algorithms and complexity. International Journal of Approximate Reasoning1 (126), pp. 133-137. Cited by: SS1.
* [10]G. Cooper (1984) Nestor: a computer-based medical diagnosis aid that integrates causal and probabilistic knowledge. Technical report Computer Science department, Stanford University, Palo-Alto, California. Cited by: SS1.
* [11]J. Kwisthout (2011) Most probable explanations in Bayesian networks: complexity and tractability. International Journal of Approximate Reasoning52 (1), pp. 1452-1469. Cited by: SS1.
* [12]M. Zaffalon, A. Antonucci, and R. Cabanas (2020) Structural causal models are (solvable by) credal networks. In European Workshop on Probabilistic Graphical Models, Cited by: SS1.
* [13]I. Levi (1980) The enterprise of knowledge. MIT Press. Cited by: SS1.
* [14]P. Walley (1991) Statistical reasoning with imprecise probabilities. Chapman and Hall, London, UK. Cited by: SS1.
* [15]C. Campos and F. Cozman (2005) The inferential complexity of bayesian and credal networks. In International Joint Conference on Artificial Intelligence (IJCAI), pp. 1313-1318. Cited by: SS1.
* [16]E. Hullermeier, S. Destercke, and M. Hossein Shaker (2022) Quantification of credal uncertainty in machine learning: a critical analysis and empirical comparison. In Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, Vol. 180, pp. 548-557. Cited by: SS1.
* [17]J. Park and A. Darwiche (2003) Solving MAP exactly using systematic search. In Uncertainty in Artificial Intelligence (UAI), pp. 459-468. Cited by: SS1.
* [18]J. Park (2005) Probabilistic reasoning in intelligent systems. Morgan Kaufmann. Cited by: SS1.
* [19]J. Park (2002) MAP complexity results and approximation methods. In Uncertainty in Artificial Intelligence (UAI), pp. 388-396. Cited by: SS1.

[MISSING_PAGE_POST]

* [18] Changhe Yuan and Eric Hansen. Efficient computation of jointree bounds for systematic MAP search. In _International Joint Conference on Artificial Intelligence (IJCAI)_, pages 1982-1989, 2009.
* [19] Junkyu Lee, Radu Marinescu, Rina Dechter, and Alexander Ihler. From exact to anytime solutions for marginal MAP. In _30th AAAI Conference on Artificial Intelligence_, pages 1749-1755, 2016.
* [20] Radu Marinescu, Junkyu Lee, Rina Dechter, and Alexander Ihler. Anytime best+depth-first search for bounding marginal MAP. In _31st AAAI Conference on Artificial Intelligence_, pages 1749-1755, 2017.
* [21] Radu Marinescu, Junkyu Lee, Rina Dechter, and Alexander Ihler. AND/OR search for marginal MAP. _Journal of Artificial Intelligence Research_, 63:875-921, 2018.
* [22] Rina Dechter and Irina Rish. Mini-buckets: A general scheme of approximating inference. _Journal of ACM_, 50(2):107-153, 2003.
* [23] Jiarong Jiang, Piyush Rai, and Hal Daume. Message-passing for approximate MAP inference with latent variables. In _Advances in Neural Information Processing Systems (NIPS)_, pages 1197-1205. 2011.
* [24] Qiang Cheng, Feng Chen, Jianwu Dong, Wenli Xu, and Alexander Ihler. Approximating the sum operation for marginal-MAP inference. In _26th AAAI Conference on Artificial Intelligence_, AAAI, pages 1882-1887, 2012.
* [25] Qiang Liu and Alexander Ihler. Variational algorithms for marginal MAP. _Journal of Machine Learning Research_, 14:3165-3200, 2013.
* [26] Wei Ping, Qiang Liu, and Alexander Ihler. Decomposition bounds for marginal MAP. In _Advances in Neural Information Processing Systems 28_, pages 3267-3275, 2015.
* [27] Denis Maua and Cassio Campos. Anytime marginal MAP inference. In _International Conference on Machine Learning (ICML)_, pages 1471-1478, 2012.
* [28] Rina Dechter. Mini-buckets: A general scheme of generating approximations in automated reasoning. In _International Joint Conference on Artificial Intelligence (IJCAI)_, pages 1297-1302, 1997.
* [29] Jaime Shinsuke Ide and Fabio Gagliardi Cozman. Approximate algorithms for credal networks with binary variables. _International Journal of Approximate Reasoning_, 48(1):275-296, 2008.
* [30] Alessandro Antonucci, Yi Sun, Cassio P De Campos, and Marco Zaffalon. Generalized loopy 2u: A new algorithm for approximate inference in credal networks. _International Journal of Approximate Reasoning_, 51(5):474-484, 2010.
* [31] Alessandro Antonucci, Cassio Campos, David Huber, and Marco Zaffalon. Approximate credal network updating by linear programming with applications to decision making. _International Journal of Approximate Reasoning_, 58(3):25-38, 2015.
* [32] R. Marinescu and R. Dechter. AND/OR branch-and-bound search for combinatorial optimization in graphical models. _Artificial Intelligence_, 173(16-17):1457-1491, 2009.