# Riemannian SAM: Sharpness-Aware Minimization on Riemannian Manifolds

Jihun Yun

KAIST

arcprime@kaist.ac.kr

&Eunho Yang

KAIST, AITRICS

eunhoy@kaist.ac.kr

###### Abstract

Contemporary advances in the field of deep learning have embarked upon an exploration of the underlying geometric properties of data, thus encouraging the investigation of techniques that consider general manifolds, for example, hyperbolic or orthogonal neural networks. However, the optimization algorithms for training such geometric deep models still remain highly under-explored. In this paper, we introduce Riemannian SAM by generalizing conventional Euclidean SAM to Riemannian manifolds. We successfully formulate the sharpness-aware minimization on Riemannian manifolds, leading to one of a novel instantiation, Lorentz SAM. In addition, SAM variants proposed in previous studies such as Fisher SAM can be derived as special examples under our Riemannian SAM framework. We provide the convergence analysis of Riemannian SAM under a less aggressively decaying ascent learning rate than Euclidean SAM. Our analysis serves as a theoretically sound contribution encompassing a diverse range of manifolds, also providing the guarantees for SAM variants such as Fisher SAM, whose convergence analyses are absent. Lastly, we illustrate the superiority of Riemannian SAM in terms of generalization over previous Riemannian optimization algorithms through experiments on knowledge graph completion and machine translation tasks.

## 1 Introduction

Deep learning incorporating the underlying geometry of data, referred to as geometric deep learning (GDL), has emerged as a significant research area in recent years due to its remarkable capability to effectively capture intrinsic structural properties. As a significant direction in this line, hyperbolic representation learning has been shown to offer several advantages over conventional Euclidean geometry. For example, the hyperbolic space allows for more efficient representations of high-dimensional data by offering a more flexible and natural way to model hierarchical structures, which are commonly encountered in network embeddings [1; 2; 3], computer visions [4; 5], and natural language processing [6; 7]. Adding to the fascinating array of approaches in geometric deep learning, the orthogonal neural networks enforcing the orthogonality in model parameters emerge as a promising research area. In another dimension, the orthogonal neural networks that constrain the model parameter to satisfy the orthogonality (known as Stiefel manifold) are proposed [8; 9; 10] under the motivation that they prevent the vanishing/exploding gradient problem and theoretically enhance the model generalization [11]. Furthermore, more generally, the Riemannian extension of deep learning technique on Euclidean space continues to be proposed in many fields including Riemannian normalizing flows [12; 13], Riemannian diffusion models [14], Poincare ResNet [15], hyperbolic deep reinforcement learning [16].

Along with the attempts to learn non-Euclidean representations in deep learning, Riemannian optimization has also been greatly studied to train non-Euclidean deep models. As a pioneering example, Riemannian (stochastic) gradient descent (R(S)GD) [17] is an extension of (stochastic) gradient descent to Riemannian manifolds, which updates the gradient computed on (a random subset of) thetraining data at each iteration and then projects the gradient onto the tangent space of the manifold before taking a descent step. Starting with Riemannian gradient descent, several popular optimization algorithms in Euclidean space, such as conjugate gradient and trust region, have been generalized to Riemannian manifolds [18; 19; 20; 21; 22; 23; 24]. In addition to these previous works, there have been many studies to incorporate the momentum and variance reduction technique into the Riemannian manifolds. In this line of work, many optimization algorithms are proposed including the stochastic variance reduction scheme (R-SVRG) [25], stochastic recursive momentum (R-SRG) [26], and stochastic path-integrated differential estimator (R-SPIDER) [27], extended from their Euclidean counterparts. Going beyond the first-order algorithms, Riemannian (quasi)-Newton methods [28; 29] are a family of second-order methods using a (quasi)-Newton approach on Riemannian manifolds. In the context of deep learning, Riemannian extensions of adaptive gradient methods (ex. AdaGrad/Adam/AMSGrad) are proposed [30; 31; 32], which combines the benefits of adaptive learning rate methods with the efficiency of Riemannian optimization techniques.

However, the exploration of optimization algorithms for training deep learning models on non-Euclidean geometry has been considerably limited, highlighting the need to generalize successful optimizers in Euclidean space to Riemannian manifolds. Under this motivation, we introduce a new class of optimization schemes on Riemannian manifolds. Toward this, as our motivating optimization algorithm, we consider the sharpness-aware minimization (SAM) [33] in Euclidean space, which efficiently improves model generalization by considering the underlying geometry of loss landscapes. With the great success of SAM, several SAM variants including adaptive SAM [34], Fisher SAM [35], Efficient SAM [36], and GSAM [37], are proposed in recent years. In this paper, we propose Riemannian SAM, a sharpness-aware minimization on Riemannian manifolds that can be applied to various manifolds. Our Riemannian SAM considers the sharpness of the loss defined on manifolds, thereby effectively improving model generalization. We believe that our framework could further bring out the potential of the Riemannian deep models and enable more accurate evaluation.

Our contributions are summarized as follows:

* We introduce Riemannian SAM, a sharpness-aware minimization scheme on Riemannian manifolds. Under our framework, we present a novel instance, Lorentz SAM, mainly used in our empirical evaluations. Furthermore, one of the SAM variants, Fisher SAM, which considers the underlying distribution space of neural networks can be derived as a special example under our Riemannian SAM framework.
* We provide the convergence analysis of Riemannian SAM. Our convergence analysis achieves the first-order optimal rate of SGD and we highlight the challenges in our analysis. We allow for a less aggressively decaying ascent learning rate than the condition in the convergence of Euclidean SAM. Also, we provide the convergence guarantee for the SAM variants such as Fisher SAM whose convergence proofs are absent.
* We validate the Riemannian SAM on knowledge graph completion and machine translation tasks for hyperbolic neural networks. The state-of-the-art hyperbolic architecture equipped with our Riemannian SAM improves the performance of the baselines trained with Riemannian Adam, which is a conventional optimizer in Riemannian deep learning.

## 2 Preliminaries

Before introducing the sharpness-aware minimization on Riemannian manifolds, we organize the necessary concepts and notations for Riemannian geometry and Riemannian optimization.

### Riemannian Geometry for Optimization

We refer to the definitions in literature [38; 39; 40] where one can find more details.

**Definition 1** (Riemannian manifold).: _For each \(w\in\mathcal{M}\), let \(\mathrm{T}_{w}\mathcal{M}\) denote the **tangent space** at \(w\). An **inner product** on tangent space \(\mathrm{T}_{w}\mathcal{M}\) is a bilinear, symmetric, positive definite function \(g_{w}(\cdot,\cdot)\coloneqq\langle\cdot,\cdot\rangle_{w}:\mathrm{T}_{w} \mathcal{M}\times\mathrm{T}_{w}\mathcal{M}\rightarrow\mathbb{R}\). If a metric \(\langle\cdot,\cdot\rangle_{w}\) smoothly varies with \(w\in\mathcal{M}\), we call \(\langle\cdot,\cdot\rangle_{w}\)**a Riemannian metric**. An induced norm on \(\mathrm{T}_{w}\mathcal{M}\) is \(\|\zeta\|_{w}\coloneqq\sqrt{\langle\zeta,\zeta\rangle_{w}}\). A **Riemannian manifold** is a pair \((\mathcal{M},g)\) of the manifold \(\mathcal{M}\) and the associated Riemannian metric tensor \(g\)._

**Definition 2** (Geodesic).: _A **geodesic** is a curve \(\gamma(\cdot):[0,1]\to\mathcal{M}\) that locally minimizes the distance between two points on a manifold with constant speed, which is the generalization of a straight line in Euclidean space._

**Definition 3** (Exponential maps/Retraction).: _An **exponential map**\(\exp_{w}:\mathrm{T}_{w}\mathcal{M}\to\mathcal{M}\) maps a tangent vector \(\zeta_{w}\in\mathrm{T}_{w}\mathcal{M}\) onto \(\mathcal{M}\) along a geodesic curve such that \(\gamma(0)=w\) and \(\gamma(1)=z\) with \(\dot{\gamma}(0)=\zeta_{w}\). Specifically, \(\gamma(t)\coloneqq\exp_{w}(\zeta_{w}t)\) represents a geodesic. A **retraction**\(\mathrm{R}_{w}(\cdot)\) is a (computationally efficient) generalization of an exponential map satisfying the following properties:_

* \(\mathrm{R}_{w}(0)=w\) _and_ \(DR_{w}(0)=\mathrm{Id}_{\mathrm{T}_{w}\mathcal{M}}\) _where_ \(DR_{w}\) _represents the derivatives of_ \(\mathrm{R}_{w}\) _and_ \(\mathrm{Id}_{\mathrm{T}_{w}\mathcal{M}}\) _denotes an identity map on_ \(\mathrm{T}_{w}\mathcal{M}\)_._

**Definition 4** (Parallel translation/Vector transport).: _A **parallel translation**\(P_{z}^{w}(\cdot):\mathrm{T}_{z}\mathcal{M}\to\mathrm{T}_{w}\mathcal{M}\) transport a tangent vector in \(\mathrm{T}_{z}\mathcal{M}\) to \(\mathrm{T}_{w}\mathcal{M}\) in parallel while preserving norm and direction (i.e., along a geodesic). A **vector transport**\(\mathcal{T}(\gamma)_{z}^{w}(\cdot):\mathrm{T}_{z}\mathcal{M}\to\mathrm{T}_{w} \mathcal{M}\)**with respect to retraction map**\(\mathrm{R}\) maps a vector \(\zeta_{z}\in\mathrm{T}_{z}\mathcal{M}\) to \(\zeta_{w}\in\mathrm{T}_{w}\mathcal{M}\) along a retraction curve \(\gamma(t)=\mathrm{R}_{w}(\zeta_{w}t)\) for some \(\xi_{w}\in\mathrm{T}_{w}\mathcal{M}\), which is computationally efficient approximation of a parallel translation. In this work, we only consider **isometric** vector transport, i.e., \(\langle\mathcal{T}_{z}^{w}\zeta_{z},\mathcal{T}_{z}^{w}\eta_{z}\rangle_{w}= \langle\zeta_{w},\eta_{w}\rangle_{w}\) for all \(\zeta_{w},\eta_{w}\in\mathrm{T}_{w}\mathcal{M}\)._

We introduce important examples of Riemannian manifolds in deep learning. The initial two instances represent dominant manifolds within the realm of hyperbolic deep learning. Hyperbolic space is a natural geometry for capturing underlying tree-like, graph-shaped, or hierarchical structures, which are properties existing in many real datasets. Owing to this characteristic, there have been many approaches to hyperbolic deep learning encompassing network embeddings [1, 2, 3, 41], computer vision [4, 5], and natural language processing [42, 6]. For manifolds in hyperbolic space, we mainly follow the definitions in [1, 2].

**Poincare Ball.** The Poincare ball \(\mathbb{P}^{n}=(\mathbb{B}^{n},g_{p})\) is a Riemannian manifold with

\[\mathbb{B}^{n}=\{x\in\mathbb{R}^{n}:\|x\|_{2}<1\},\quad g_{p}(x)=\Big{(}\frac{ 2}{1-\|x\|_{2}^{2}}\Big{)}^{2}g_{e}(x).\]

where \(g_{e}\) represents an Euclidean metric tensor. The associated distance on Poincare ball is given by

\[d_{p}(x,y)=\mathrm{arcosh}\Big{(}1+2\frac{\|x-y\|_{2}^{2}}{(1-\|x\|_{2}^{2})( 1-\|y\|_{2}^{2})}\Big{)}.\]

We remark on some properties of the Poincare ball. The Poincare ball is a conformal model, meaning that angles between curves are preserved under conformal transformations. This property enables the Poincare ball to accurately represent the local geometry of complex spaces. Additionally, the Poincare ball offers an intuitive visualization of hyperbolic spaces, particularly in two or three dimensions. Despite these advantageous properties, the Poincare ball also presents challenges in computing mathematical concepts such as geodesics and distances.

**Lorentz Model.** The Lorentz model \(\mathbb{L}^{n}=(\mathbb{H}^{n},g_{\ell})\) is a semi-Riemannian manifold, but it is still possible to employ Riemannian optimization. The Lorentz model consist of

\[\mathbb{H}^{n}=\{x\in\mathbb{R}^{n+1}:\langle x,x\rangle_{\mathbb{L}}=-1,x_{0 }>0\},\quad g_{\ell}(x)=\begin{bmatrix}-1&\cdots&0&0\\ 0&1&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&1\end{bmatrix}.\] (1)

where \(\langle x,y\rangle_{\mathbb{L}}=-x_{0}y_{0}+\sum_{j=1}^{n}x_{j}y_{j}\) is known as the _Lorentzian scalar product_. The associated distance function is given by

\[d_{\ell}(x,y)=\mathrm{arcosh}\big{(}-\langle x,y\rangle_{\mathbb{L}}\big{)}\]

Note that an \(n\)-dimensional Lorentz model requires one more redundant dimension in Euclidean space. Importantly, Poincare ball and the Lorentz model are equivalent under the diffeomorphism \(\varphi:\mathbb{H}^{n}\to\mathbb{P}^{n}\) (with the corresponding inverse mapping \(\varphi^{-1}:\mathbb{P}^{n}\to\mathbb{H}^{n}\)) defined as

\[\varphi(x_{0},x_{1},\cdots,x_{n}) =\frac{(x_{1},x_{2},\cdots,x_{n})}{p_{0}+1},\] (2) \[\varphi^{-1}(x_{1},x_{2},\cdots,x_{n}) =\frac{(1+x_{1}^{2}+\cdots+x_{n}^{2},2x_{1},\cdots,2x_{n})}{1-(x _{1}^{2}+\cdots+x_{n}^{2})}.\] (3)We also remark on some characteristics of the Lorentz model. The main advantage is that it allows for more stable Riemannian optimization under relatively simple formulas for mathematical quantities, such as geodesics and distances. However, the visualization is difficult due to less intuitive projection onto a lower-dimensional space. As noted in each manifold, both two manifolds on hyperbolic space are equivalent, but they have different purposes. For this reason, some studies [2] use Lorentz manifolds for their model design and training, then visualize the results on Poincare ball using the diffeomorphism \(\varphi\) in (2) and (3).

**Stiefel manifold.** The orthogonality, i.e., \(W^{\mathsf{T}}W=I\) for model parameter \(W\), plays a role to circumvent the vanishing/exploding gradient problem [8; 10; 43] and delivers theoretically enhanced generalization error bounds [11]. The Stiefel manifold \(\mathrm{St}(n,p)=(\mathbb{V}^{n},g_{W})\) for \(n\geq p\) is prevalent for orthogonal neural networks, which is also a Riemannian manifold defined by

\[\mathbb{V}^{n}=\{X\in\mathbb{R}^{n\times p}:X^{\mathsf{T}}X=I\},\quad g_{W}(Z _{1},Z_{2})=\mathrm{Tr}(Z_{1}^{\mathsf{T}}Z_{2}).\]

for tangent vectors \(Z_{1},Z_{2}\in\mathrm{T}_{W}\mathrm{St}(n,p)\). The tangent space of \(\mathrm{St}(n,p)\) at \(W\) is defined by \(\mathrm{T}_{W}\mathrm{St}(n,p)=\{Z:Z^{\mathsf{T}}W+W^{\mathsf{T}}Z=0\}\).

### Riemannian Optimization

We are interested in the following optimization problem over the Riemannian manifold \((\mathcal{M},g)\)

\[\min_{w\in\mathcal{M}}\mathcal{L}(w).\]

where \(\mathcal{L}:\mathcal{M}\to\mathbb{R}\) is a smooth function defined on manifold \(\mathcal{M}\). Following the work [17], Riemannian stochastic gradient descent (RSGD) updates the model parameter \(w\in\mathcal{M}\) as

\[w_{t+1}=\exp_{w_{t}}\big{(}-\alpha_{t}\mathrm{grad}\mathcal{L}(w_{t})\big{)}.\] (4)

where \(\mathrm{grad}\mathcal{L}(w_{t})\in\mathrm{T}_{w_{t}}\mathcal{M}\) is a Riemannian gradient at \(w_{t}\) and \(\alpha_{t}\) is the learning rate. In some practical cases, the exponential map is computationally inefficient. Hence, it may be replaced with (more computationally efficient) suitable retraction map \(\mathrm{R}_{w_{t}}(\cdot)\), yielding the update rule \(w_{t+1}=\mathrm{R}_{w_{t}}\big{(}-\alpha_{t}\mathrm{grad}\mathcal{L}(w_{t}) \big{)}\). Generally, the Riemannian gradient \(\mathrm{grad}\mathcal{L}(w_{t})\) in (4) is computed with the Riemannian metric tensor \(g\) as

\[\mathrm{grad}\mathcal{L}(w_{t})=g^{-1}(w_{t})\nabla\mathcal{L}(w_{t}).\] (5)

where \(\nabla\mathcal{L}(w_{t})\) denotes the Euclidean gradient. This is also known as natural gradient descent [44]. The quantity on the right-hand side in (5) may not be on \(\mathrm{T}_{w_{t}}\mathcal{M}\). In this case, we should project the gradient onto the tangent space since the exponential map is not defined.

## 3 Sharpness-Aware Minimization on Riemannian Manifolds

In empirical risk minimization (ERM) including deep learning tasks, we generally minimize the finite-sum objective (or equivalently expected objective) for training dataset \(\mathcal{D}=\{(x_{i},y_{i})\}_{i=1}^{n}\) as

\[\min_{w\in\mathcal{M}}\mathcal{L}(w)\coloneqq\frac{1}{n}\sum_{i=1}^{n} \mathcal{L}(w;x_{i}).\] (6)

where the smooth loss function \(\mathcal{L}(\cdot):\mathcal{M}\to\mathbb{R}\) is defined on a Riemannian manifold \(\mathcal{M}\). We formulate the sharpness-aware minimization in terms of loss function values on the manifold \(\mathcal{M}\) as

\[\min_{w\in\mathcal{M}}\max_{\|\delta\|_{w}^{2}\leq\rho^{2}}\underbrace{ \Big{\{}\mathcal{L}(\mathrm{R}_{w}(\delta))-\mathcal{L}(w)\Big{\}}}_{\text{ Sharpness in terms of loss function}}.\] (7)

In contrast to Euclidean space, the Riemannian manifold is not a vector space in general. Hence, the familiar concepts defined in Euclidean space may not be well-defined. Therefore, we restrict the perturbation \(\delta\) in the tangent space \(\mathrm{T}_{w}\mathcal{M}\). To solve the inner subproblem, we resolve the inner optimization problem in a different manner as

\[\max_{\delta\in\mathrm{T}_{w}\mathcal{M}}\mathcal{L}\big{(}\mathrm{R}_{w}( \delta)\big{)}-\mathcal{L}(w)\quad\text{such that}\quad\|\delta\|_{w}^{2}\leq \rho^{2}.\] (8)for a fixed point \(w\in\mathcal{M}\). For ease of computations, we approximate the perturbed loss function \(\mathcal{L}\big{(}\mathrm{R}_{w}(\delta)\big{)}\) via Taylor's expansion as

\[\mathcal{L}\big{(}\mathrm{R}_{w}(\delta)\big{)}\approx\mathcal{L}(w)+\langle \mathrm{grad}\mathcal{L}(w),\delta\rangle_{w}.\] (9)

Then, our inner maximization problem comes in hand:

\[\max_{\delta\in\mathrm{T}_{w}\mathcal{M}}\langle\mathrm{grad}\mathcal{L}(w), \delta\rangle_{w}\quad\text{such that}\quad\|\delta\|_{w}^{2}\leq\rho^{2}.\] (10)

This problem could be easily solved since it finds the steepest direction \(\delta\) on Riemannian manifold \(\mathcal{M}\), whose solution is known to be just Riemannian gradient. Therefore, we have

\[\delta^{*}=\rho\frac{\mathrm{grad}\mathcal{L}(w)}{\|\mathrm{grad}\mathcal{L}(w )\|_{w}}.\] (11)

Under the optimal perturbation \(\delta^{*}\) in (11), we further approximate the gradient of the sharpness-aware minimization in (7) (for outer minimization problem) as

\[\mathrm{grad}\mathcal{L}\big{(}\mathrm{R}_{w}(\delta^{*})\big{)}\approx \mathrm{grad}\mathcal{L}(w)|_{w=\mathrm{R}_{w}(\delta^{*})}.\] (12)

since the left-hand side of (12) requires a higher-order Riemannian gradient, which is not computationally feasible in practice. The remaining is that the approximated Riemannian SAM gradient \(\mathrm{grad}\mathcal{L}(w)|_{w=\mathrm{R}_{w}(\delta^{*})}\) in (12) is not on the tangent space at \(w\), \(\mathrm{T}_{w}\mathcal{M}\). To perform an actual parameter update on \(w\), we should transport the Riemannian SAM gradient to the tangent space \(\mathrm{T}_{w}\mathcal{M}\) via vector transport \(\mathcal{T}_{\mathrm{R}_{w}(\delta^{*})}^{w}\) with respect to the retraction \(\mathrm{R}_{w}\). We summarize the overall optimization procedure in Algorithm 1. Note that, in order to consider the most practical case, we assume that the same minibatch is used for computing SAM perturbation with the ascent step and the actual parameter updates with the descent step (see lines 5, 6, and 8). In fact, one can use different batches for lines 5\(\sim\) 7 and lines 8\(\sim\) 10 respectively or full-batch gradient for both ascent and descent steps.

**Remarks on Algorithm 1.** In fact, it might be most natural to choose a perturbation region at the current point as in the conventional Euclidean SAM, \(\delta\in B_{\rho}(w_{t})\coloneqq\{x\in\mathcal{M}:d_{\mathcal{M}}(w_{t},x) \leq\rho\}\) where \(d_{\mathcal{M}}\) represents the distance on the manifold. However, adopting the constraint in this manner may pose challenges in utilizing the standard assumptions for analyzing non-convex Riemannian optimization, such as geodesic or retraction smoothness (see condition (C-4) in Section 4), which makes it difficult to guarantee convergence. Moreover, the computation of \(d_{\mathcal{M}}\) is often computationally inefficient in practice. Another possible extension is to apply the vector transport operation from line 8 of Algorithm 1 to line 9. The following outlines the modified procedure: (i) \(g_{t}^{adv}=\mathcal{A}\big{(}\mathrm{grad}\mathcal{L}(w;\mathcal{S})|_{w=w_{t }^{adv}}\big{)}\) and (ii) \(\Delta_{t}=\mathcal{T}_{w_{t}^{adv}}^{w_{t}}g_{t}^{adv}\). For base optimizer \(\mathcal{A}\), any optimization algorithm commonly used in Riemannian optimization can be adopted (e.g., Riemannian SGD). In the meanwhile, when the vector transport is applied after constructing \(g_{t}^{adv}\) via the momentum-based optimizer \(\mathcal{A}\), the momentum construction takes place on the tangent space \(\mathrm{T}_{w_{t}^{adv}}\mathcal{M}\) at the perturbed point \(w_{t}^{adv}\), while the parameter update occurs on the different tangent space at the point. As a consequence, this might introduce another challenges in understanding and analyzing the overall optimization process. In this perspective, various alternative extensions can also be possible, but among them, we have carefully designed a _theoretically valid, computationally practical, and non-trivially extended Sharpness-Aware Minimization on general manifolds for Riemannian optimization_. Then, we have successfully demonstrated both convergence analysis (Section 4) and empirical studies (Section 5) to corroborate our Riemannian SAM.

**Existing example of Riemannian SAM framework: Fisher SAM.** Following our Riemannian SAM update rule in Algorithm 1, we can show that Fisher SAM is a special instance of Riemannian SAM. We can view the set of neural networks as a neuromanifold [45, 46] equipped with the KL divergence metric between two points. Hence, let \(w\in\mathcal{M}\) be the point on a neuromanifold (or statistical manifold) \(\mathcal{M}\) which is realized by Euclidean network parameter \(\theta\in\mathbb{R}^{d}\). On distribution space, the corresponding metric tensor \(g\) is known to be Fisher information matrix [44, 45, 46]. According to Algorithm 1, the perturbation at line 6 and 7 could be computed as

\[\mathrm{grad}\mathcal{L}(w) =F(\theta)^{-1}\nabla\mathcal{L}(\theta)\] \[\delta^{*} =\rho\frac{\mathrm{grad}\mathcal{L}(w)}{\|\mathrm{grad}\mathcal{L} (w)\|_{w}}=\rho\frac{F(\theta)^{-1}\nabla\mathcal{L}(\theta)}{\sqrt{\mathrm{ grad}\mathcal{L}(w)^{\mathsf{T}}F(\theta)\mathrm{grad}\mathcal{L}(w)}}=\rho \frac{F(\theta)^{-1}\nabla\mathcal{L}(\theta)}{\sqrt{\nabla\mathcal{L}(\theta )^{\mathsf{T}}F(\theta)^{-1}\nabla\mathcal{L}(\theta)}}.\]which is entirely identical to the perturbation of Fisher SAM [35].

**Novel example: Lorentz SAM on hyperbolic geometry.** We derive the novel instance of Riemannian SAM called Lorentz SAM over the Lorentz model introduced in 2.1. First, we derive the Riemannian gradient on the Lorentz model \(\mathbb{L}^{n}=(\mathbb{H}^{n},g_{\ell})\). As in Section 2.2, the Riemannian gradient could be computed as

\[h=g_{\ell}^{-1}\nabla\mathcal{L}(w).\] (13)

Since \(g_{\ell}\) in (1) is a diagonal matrix, it is easy to compute the vector \(h\) with Euclidean gradient \(\nabla\mathcal{L}(w)\). However, the vector \(h\) is not on the tangent space at \(w\), \(\mathrm{T}_{w}\mathbb{L}^{n}\), thus we should have to project the vector \(h\) onto the tangent space \(\mathrm{T}_{w}\mathbb{L}^{n}\). The projection is easily computed in a closed-form as

\[\mathrm{proj}_{w}(v)=v+\langle w,v\rangle_{\mathbb{L}}w.\] (14)

Hence, the Riemannian gradient on Lorentz model is computed by

\[\mathrm{grad}\mathcal{L}(w)=\mathrm{proj}_{w}\big{(}g_{\ell}^{-1}\nabla \mathcal{L}(w)\big{)}.\] (15)

As a next step, we should normalize the Riemannian gradient as in line 6 in Algorithm 1, and this is easy to compute as \(\|\mathrm{grad}\mathcal{L}(w)\|_{w}=\sqrt{\langle\mathrm{grad}\mathcal{L}(w), \mathrm{grad}\mathcal{L}(w)\rangle_{\mathbb{L}}}\) via Lorentzian scalar product \(\langle\cdot,\cdot\rangle_{\mathbb{L}}\) defined in Section 2.1.

We utilize the Lorentz SAM derived above for our primary empirical studies conducted on hyperbolic space. In a similar way, one can derive the Riemannian SAM on Poincare ball or Stiefel manifold, which we defer to Appendix.

### Riemannian SAM Illustration: Toy 3D Illustration

We illustrate the 3-dimensional toy example on the sphere manifold. Let us define the 3D sphere manifold \(\mathbb{S}^{2}\) with the tangent space at \(w\), \(\mathrm{T}_{w}\mathcal{M}\) as

\[\mathbb{S}^{2} :=\{w\in\mathbb{R}^{3}:\|w\|_{2}=1\},\] \[\mathrm{T}_{w}\mathbb{S}^{2} :=\{v\in\mathbb{R}^{3}:w^{\mathrm{T}}v=0\}\]

We consider the regression problem with the neural-net-like objective function on the randomly generated synthetic dataset. Toy 3D optimization problem with objective function \(f(w)=\frac{1}{2^{\prime\prime}}\|y-\mathrm{ReLU}(Xw)\|_{2}^{2}\) where \(X\in\mathbb{R}^{500\times 3}\) and \(y\in\mathbb{R}^{500}\) are drawn from \(\mathcal{N}(0,1^{2})\) and \(\mathcal{U}(0,1)\) respectively with the model parameter \(w=(x,y,z)\in\mathbb{R}^{3}\) under \(\|w\|_{2}=1\). **(a)** Comparison of converged points for each method. We plot the contour plots with the spherical coordinates under the relation \((x,y,z)\leftrightarrow(r,\theta,\varphi)=(1,\theta,\varphi)\).

Figure 1: Toy 3D illustration

The Figure 1 corresponds to Cartesian coordinates \((x,y,z)\) to spherical coordinates \((r,\theta,\varphi)=(1,\theta,\varphi)\), rendering contour plots. In Figure 1, we showcases the converged points on the objective function under the optimization using Riemannian SAM (in purple color) and the conventional Euclidean SAM (in pink color). Within a maximum iteration budget \(100\), the purple point (Riemannian SAM) attains a loss value of \(0.3800\) while the pink point (conventional Euclidean SAM) converges with a slightly higher loss value of \(0.3808\).

Furthermore, in terms of sharpness measures, we consider the following basic two quantities: (i) the trace of the Hessian (sharpness in the context of Euclidean space), and (ii) manifold-aware sharpness, characterized by the Riemannian gradient norm \(\|\mathrm{grad}\mathcal{L}(w)\|_{w}\). Notably, the manifold-aware sharpness aligns with information-geometric sharpness [47] when dealing with statistical manifolds, where the Riemannian metric is defined by the Fisher information. For the aforementioned problem, we compare two metrics and Figure 2 depicts the results. In both metrics, Riemannian SAM achieves smaller sharpness values than Euclidean SAM, implying convergence toward flatter regions. In other words, since the Euclidean SAM might fail to properly consider the underlying structure of the manifold even for toy examples, this phenomenon is expected to be exacerbated in extremely high-dimensional problems such as deep learning.

## 4 Convergence Analysis

In this section, we present the convergence guarantees for the RiemSAM framework. Our goal is to find a first-order \(\epsilon\)-approximate solution: the output \(\tilde{w}\) such that \(\mathbb{E}\big{[}\|\mathrm{grad}\mathcal{L}(\tilde{w})\|_{\tilde{w}}^{2}\big{]} \leq\epsilon^{2}\), which is a generalized convergence criterion of Euclidean \(\epsilon\)-stationary point. To guarantee the convergence for \(\epsilon\)-approximate solution, we require the following mild assumptions.

**(C-1)**: (Upper-Hessian bounded) The objective function \(\mathcal{L}\) is said to be _upper-Hessian bounded_ in \(\mathcal{U}\subset\mathcal{M}\) with respect to retraction \(\mathrm{R}\) if there exists some positive constant \(C\) such that \(\frac{d^{2}\mathcal{L}(\mathrm{R}_{w}(t\eta))}{dt^{2}}\leq C\), for all \(w\in\mathcal{U}\) and \(\eta\in\mathrm{T}_{w}\mathcal{M}\) with \(\|\eta\|_{w}=1\), and for all \(t\) such that \(\mathrm{R}_{w}(\tau\eta)\) for all \(\tau\in[0,t]\).
**(C-2)**: (Lower-bounded) The objective function \(\mathcal{L}(\cdot)\) is differentiable and has bounded suboptimality.

\[\mathcal{L}(w^{*})>-\infty.\]

for the optimal point \(w^{*}\in\mathcal{M}\).
**(C-3)**: (Unbiasedness and bounded variance) The stochastic Riemannian gradient is unbiased and has a bounded variance:

\[\mathbb{E}_{(x,y)\in\mathcal{D}}[\mathrm{grad}\mathcal{L}(w;x)]= \mathrm{grad}\mathcal{L}(w),\] \[\mathbb{E}_{(x,y)\in\mathcal{D}}[\|\mathrm{grad}\mathcal{L}(w;x)- \mathrm{grad}\mathcal{L}(w)\|_{w}^{2}]\leq\sigma^{2}.\]

where \(\mathrm{grad}\mathcal{L}(w)\) is a true Riemannian gradient evaluated on a full batch of training dataset \(\mathcal{D}\).
**(C-4)**: (Retraction smoothness) We assume that there exists a constant \(L_{S}>0\) such that

\[\mathcal{L}(z)\leq\mathcal{L}(w)+\langle\mathrm{grad}\mathcal{L}(w),\eta \rangle_{w}+\frac{1}{2}L_{S}\|\eta\|_{w}^{2}.\]

Figure 2: Comparison of two sharpness measures.

for all \(w,z\in\mathcal{M}\) and \(\gamma(t)\coloneqq\mathrm{R}_{w}(t\eta)\) represents a retraction curve on \(\mathcal{M}\) for \(\eta\in\mathrm{T}_{w}\mathcal{M}\) with the starting point \(\gamma(0)=w\) and the terminal point \(\gamma(1)=z\).
**(C-5)**: (Individual Retraction Lipschitzness) We assume that there exists \(L_{\mathrm{R}}>0\) such that

\[\|\mathcal{T}(\gamma)_{z}^{w}\mathrm{grad}\mathcal{L}(z;x)-\mathrm{grad} \mathcal{L}(w;x)\|_{w}\leq L_{\mathrm{R}}\|\eta\|_{w}.\]

for all \(w,z\in\mathcal{M}\). As in condition (C-4), \(\gamma(t)\) denotes a retraction curve and \(\mathcal{T}(\gamma)_{z}^{w}\) is a vector transport associated with this retraction curve.

The function class with condition (C-1) corresponds to the continuous function family with Lipschitz continuous gradients in the Euclidean space [26, 32, 48]. The assumptions (C-2) \(\sim\) (C-4) are standard in convergence analysis of Riemannian optimization algorithms, under which Riemannian SGD is known to be first-order optimal [49]. Note that, unlike in Euclidean space, the constant \(L_{S}\) in (C-4) and \(L_{\mathrm{R}}\) in (C-5) may be different. According to [26, 50], the condition (C-5) can be derived under the standard assumption on retraction Lipschitzness with parallel translation and one additional assumption on the bound between the parallel translation and the vector transport, but we assume the retraction Lipschitzness with the vector transport for simplicity. Lastly, in condition (C-5), the retraction Lipschitzness is assumed individually with respect to each sample in order to control the alignment of SAM gradient and the original gradient step as in [51].

Now, we are ready to present our main theorem.

**Theorem 1** (Convergence of Riemannian SAM).: _Let \(\widetilde{w}\) denote an iterate uniformly chosen at random from \(\{w_{1},w_{2},\cdots,w_{T}\}\). Further, we let \(\widetilde{L}=\max\{L_{S},L_{\mathrm{R}}\}\) where the constants \(L_{S}\) and \(L_{\mathrm{R}}\) are defined in condition (C-4) and (C-5) respectively. Under the conditions (C-1) \(\sim\) (C-5) with descent learning rate \(\alpha_{t}=\frac{1}{\sqrt{T_{L}}}\) and ascent learning rate \(\rho_{t}=\frac{1}{T^{1/5}L}\), we have the following complexity for the constant batch size \(b\):_

\[\mathbb{E}\big{[}\|\mathrm{grad}\mathcal{L}(\widetilde{w})\|_{ \widetilde{w}}^{2}\big{]}\leq\frac{Q_{1}\Delta}{\sqrt{T}}+\frac{Q_{2}\sigma^{2 }}{b\sqrt{T}}+\frac{Q_{3}\sigma^{2}}{bT^{5/6}}=\mathcal{O}(1/\sqrt{T}).\] (16)

_where \(\Delta=\mathcal{L}(w_{0})-\mathcal{L}(w^{*})\) and the constants \(\{Q_{i}\}_{i=1}^{3}\) are irrelevant to the total iteration \(T\) or the manifold dimension \(d\)._

We make some remarks on our convergence results and relationship to conventional Euclidean SAM.

**Theoretical implications.** Our key observation of Theorem 1 lies in the _alignment between the Riemannian gradient_\(\mathrm{grad}\mathcal{L}(w_{t})\) _(line 5 in Algorithm 1) and the Riemannian SAM gradient_\(\mathcal{T}_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}(w_{t}^{adv})\) _(line 9 in Algorithm 1)_ for the perturbed point \(w_{t}^{adv}\), \(w_{t}^{adv}\). The previous study [51] on Euclidean SAM says that Euclidean SAM gradient should be well-aligned with the true gradient step for convergence. Unlike the theoretical claim in [51], we stress that for convergence guarantee those gradients should be _well-aligned within the preconditioned space (by inverse Riemannian metric) regardless of alignment in Euclidean space_. To verify this insight, we directly measure the angles between two vectors with a 2D toy example, illustrating how they align in practice. Toward this, we consider two angles: (i) \(\angle(\nabla f(w_{t}^{adv}),\nabla f(w_{t}))\) (Euclidean Alignment) and (ii) \(\angle(\mathcal{T}_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}f(w_{t}^{adv}),\mathrm{ grad}f(w_{t}))\) (Riemannian Alignment, Ours). In this example, we consider the logistic regression where \(200\) data samples are generated with \(100\) of them sampled from \(\mathcal{N}(-1,1^{2})\) and the remaining sampled from \(\mathcal{N}(1,1^{2})\). The labels are assigned such that if a sample was drawn from a Gaussian distribution with a mean of \(-1\), the label was set to \(y=0\), and otherwise, we set \(y=1\). We minimize the cross-entropy loss with our Riemannian SAM with the Fisher information matrix as the Riemannian metric. The Figure 3 depicts the comparison of angles. The loss decreases up to \(10\)-th iteration, after which it remains around the converged point. As evident from the illustration, while the angles between the Euclidean space SAM gradient and the gradient deviate by up to around \(25\) degrees, the angles between the preconditioned SAM gradient and the preconditioned gradient, influenced by the Fisher information, align more closely with deviations

Figure 3: Toy 2D illustration

only up to a maximum of \(10\) degrees. In high-dimensional loss landscapes, we expect that the angles would become significantly larger, corroborating our theoretical insight.

**On upper bound.** Distinct from the convergence of Euclidean SAM [51], our upper bound (16) has the additional term involving \(Q_{3}\), but we still achieve the optimal complexity of SGD, \(\mathcal{O}(1/\epsilon^{4})\) for \(\epsilon\)-approximate solution. Note that the presence of term involving the constant \(Q_{3}\) in our bound comes from the fact that (i) smoothness condition (C-4) and Lipschitzness condition (C-5) are not equivalent on manifolds and (ii) we should handle the vector-transported gradients (see line 8 in Algorithm 1) at each iteration, which are the main challenges in our proof. Our results can also provide the guarantees for SAM variants such as Fisher SAM [35], whose convergence guarantees are missing.

## 5 Experiments

We conduct two sets of experiments; (i) knowledge graph completion, and (ii) machine translation. The first experiment aims to evaluate our Riemannian SAM on shallow networks and the second task is for optimizing large-scale deep neural networks. For all our experiments, we consider the Lorentz manifold introduced in Section 2.1 and employ the recent hyperbolic architecture, HyboNet[52]. The HyboNet is a _fully hyperbolic_ neural network, whose each layer is constructed on the Lorentz manifold including a linear, attention, residual, and positional encoding layer. We implement our Riemannian SAM upon Geoopt framework [53] written in PyTorch library [54]. Regarding hyperparameters, we basically adhere to the same experiment settings in [52] and the details are provided in each section and Appendix.

### Knowledge Graph Completion

A knowledge graph completion aims to predict missing relationships within a knowledge graph, which represents structured information as a collection of entities, their attributes, and the relationships between them. More precisely, knowledge in a graph is of the form of triplets \((h,r,t)\) where \(h\), \(r\), and \(t\) denote the head entity, the relationship or predicate, and the tail entity respectively. In the knowledge graph completion task, given a partially populated knowledge graph, the goal is to predict the missing entity or relationship in a triplet: solving \((h,r,?)\) and \((?,r,t)\).

In our experiments, we use two popular benchmark datasets; WN18RR [41] and Fb15k-237 [55]. We employ the same data preprocessing in [3] and two standard metrics for evaluations: (i) Mean Reciprocal Rank (MRR), the average of the inverse of the true entity ranking, and (ii) Precision at \(K\) (H@K), the proportion of test instances where the correct answer appears in the top-\(K\) ranked predictions. For

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multirow{3}{*}{**Manifold**} & \multirow{3}{*}{**Model**} & \multicolumn{6}{c}{**WN18RR**} & \multicolumn{6}{c}{**FB15k-237**} \\ \cline{3-13}  & & \#Dims & **MRR** & **H@10** & **H@3** & **H@1** & **\#Dims** & **MRR** & **H@10** & **H@3** & **H@1** \\ \hline \multirow{6}{*}{\begin{tabular}{c} HyboNet \\ \end{tabular} } & MuRP & 32 & 46.5 & 54.4 & 48.4 & 42.0 & 32 & 32.3 & 50.1 & 35.3 & 23.5 \\  & RoTH & 32 & 47.2 & 55.3 & 49.0 & 42.8 & 32 & 31.4 & 49.7 & 34.6 & 22.3 \\  & AttH & 32 & 46.6 & 55.1 & 48.4 & 41.9 & 32 & 32.4 & 50.1 & 35.4 & 23.6 \\  & HyboNet & & & & & & & & & & \\  & w/tAdam & & & & & & & & & & \\ \hline \multirow{6}{*}{\begin{tabular}{c} HyboNet \\ w/**SAM** \\ \end{tabular} } & MuRP & 32 & **49.3\({}^{\pm 0.2}\)** & **56.0\({}^{\pm 0.2}\)** & **50.7\({}^{\pm 0.1}\)** & **46.2\({}^{\pm 0.3}\)** & 32 & **34.3\({}^{\pm 0.2}\)** & **52.0\({}^{\pm 0.1}\)** & **37.3\({}^{\pm 0.3}\)** & **25.1\({}^{\pm 0.4}\)** \\ \cline{1-1} \cline{2-13}  & MuRP & \(\beta\) & 48.1 & 56.6 & 49.5 & 44.0 & \(\beta\) & 33.5 & 51.8 & 36.7 & 24.3 \\  & RoTH & \(\beta\) & 49.6 & 58.6 & 51.4 & 44.9 & \(\beta\) & 34.4 & 53.5 & 38.0 & 24.6 \\  & AttH & \(\beta\) & 48.6 & 57.3 & 49.9 & 44.3 & \(\beta\) & 34.8 & 54.0 & 38.4 & 25.2 \\  & HyboNet & & & & & & & & & & \\  & w/tAdam & & & & & & & & & & & \\ \hline \multirow{6}{*}{
\begin{tabular}{c} HyboNet \\ w/**tSAM** \\ \end{tabular} } & MuRP & \(\beta\) & 48.1 & 56.6 & 49.5 & 44.0 & \(\beta\) & 33.5 & 51.8 & 36.7 & 24.3 \\  & RoTH & \(\beta\) & 49.6 & 58.6 & 51.4 & 44.9 & \(\beta\) & 34.4 & 53.5 & 38.0 & 24.6 \\  & AttH & \(\beta\) & 48.6 & 57.3 & 49.9 & 44.3 & \(\beta\) & 34.8 & 54.0 & 38.4 & 25.2 \\  & HyboNet & & & & & & & & & & \\  & w/tAdam & & & & & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Link prediction results (%) in the filtered setting for WN18RR and FB15k-237 datasets. For hyperbolic architectures, \(\beta\in\{200,400,500\}\) and we report the best result. The results of baselines are taken from [52] except for HyboNet whose results are reproduced by ourselves. The best results among hyperbolic architectures with the same dimensions are in boldface. Our Riemannian SAM (denoted by rSAM) shows the superior performance compared to Riemannian Adam (denoted by rAdam).

marginal hyperparameter tuning, we tune the ascent learning rate \(\rho_{t}\in\{10^{-4},10^{-3},10^{-2},10^{-1}\}\) for Riemannian SAM and the other hyperparameters are the same as HyboNet[52] for fair comparisons.

Table 1 illustrates the results on WN18RR and Fb15k-237 datasets. As in previous work [52], we test our Riemannian SAM on two different regimes: (i) small embedding dimension \(32\) and (ii) large dimension \(\beta\in\{200,400,500\}\) for both datasets. Regarding the large dimension, we report the best results among the dimension candidates. In Table 1, Riemannian SAM on the Lorentz model achieves the best performance with great margins for all comparison metrics. In the same way, Riemannian SAM shows the state-of-the-art performance for all metrics considered under both regimes.

### Machine Translation

In this experiment, we evaluate our Riemannian SAM on Lorentz Transformer built with Lorentz components introduced in [52] for IWSLT '14 and WMT '14 benchmark datasets in machine translation. We use the BLEU score as an evaluation metric on the IWSLT '14 test set and the newstest2013 test set of WMT '14 respectively. According to [56], we train hyperbolic models with Riemannian SAM in a low-dimensional setting where the dimension of the word vector is \(d=64\). As in the knowledge graph completion task, we choose the ascent learning rate in \(\rho_{t}\in\{10^{-5},10^{-4},\cdots,10^{-2}\}\) for marginal hyperparameter tuning.

Table 2 demonstrates the results. HyboNet baseline trained with Riemannian Adam already outperforms the Euclidean Transformer in both IWSLT '14 and WMT '14 datasets. Upon this baseline, we only substitute our Riemannian SAM for Riemannian Adam with other hyperparameters unchanged. As seen in Table 2, Riemannian SAM significantly outperforms the Riemannian Adam baseline for both datasets. Note that both HyperNN++ and HAtt are partially hyperbolic networks, so we could not evaluate our Riemannian SAM on these models since it is difficult for a fair evaluation.

**Wall-clock time.** As in Euclidean SAM, Riemannian SAM requires additional forward and backward propagation in a single iteration loop (see Algorithm 1). Thus, we report the wall-clock time comparison for each experiment. For knowledge graph completion (Section 5.1) and machine translation (Section 5.2), Riemannian SAM takes roughly \(1.6\) and \(1.8\) times longer than Riemannian Adam for one epoch, respectively. To alleviate the computational overhead, one can employ the stochastic weight perturbation (SWP) and sharpness-sensitive data selection (SDS) suggested in [36], which do not depend on the manifold structure. Another practical consideration is to use a subset of minibatch in computing perturbation (see line 6 in Algorithm 1) for large-scale models. We leave the study on reducing computational cost as future work.

## 6 Conclusion

In this study, we proposed a sharpness-aware minimization on Riemannian manifolds, called Riemannian SAM. Under our framework, we presented novel examples of Riemannian SAM including a Lorentz SAM. We analyzed the convergence of the Riemannian SAM for general manifolds with a less aggressively decaying ascent learning rate condition. Moreover, we showed that Riemannian SAM can provide the convergence guarantee for SAM variants whose convergence proofs are missing such as Fisher SAM. We also illustrated that Riemannian SAM empirically outperforms ERM-based Riemannian optimization algorithms for popular deep learning tasks with hyperbolic neural networks. As future work, we plan to study the technique to reduce the computations and analyze the generalization error bounds of Riemannian SAM theoretically.

\begin{table}
\begin{tabular}{l l|c|c} \hline \hline \multirow{2}{*}{**Manifold**} & **Model** & **IWSLT ’14** & **WMT ’14** \\ \hline \multirow{2}{*}{**Euclidean**} & ConvSeq2Seq[57] & 23.6 & 14.9 \\  & Transformer[58] & 23.0 & 17.0 \\ \hline \multirow{4}{*}{**Hyperbolic**} & HyperNN++[56] & 22.0 & 17.0 \\  & Hatt[42] & 23.7 & 18.8 \\ \cline{1-1}  & HyboNet (with Riemannian Adam) & 25.5 & 19.3 \\ \cline{1-1} \cline{2-4}  & \(\overline{\text{HyboNet}}\) (with **Riemannian SAM, Ours**) & **26.0** & **20.1** \\ \hline \hline \end{tabular}
\end{table}
Table 2: The BLEU scores on the test set of IWSLT ’14 and WMT ’14 under low-dimensional setting following the hyperbolic study [56] with the word vector dimension \(d=64\). The results on baselines are taken from [52] except for HyboNet whose results are reproduced by ourselves.

## Acknowledgement

This work was supported by the National Research Foundation of Korea (NRF) grants (No.2018R1A5A1059921, RS-2023-00209060), Institute of Information & Communications Technology Planning & Evaluation (IITP) grants (No.2019-0-00075, Artificial Intelligence Graduate School Program(KAIST)) funded by the Korea government (MSIT).

## References

* [1] Maximillian Nickel and Douwe Kiela. Poincare embeddings for learning hierarchical representations. _Advances in neural information processing systems_, 30, 2017.
* [2] Maximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. In _International conference on machine learning_, pages 3779-3788. PMLR, 2018.
* [3] Ivana Balazevic, Carl Allen, and Timothy Hospedales. Multi-relational poincare graph embeddings. _Advances in Neural Information Processing Systems_, 32, 2019.
* [4] Keegan Lensink, Bas Peters, and Eldad Haber. Fully hyperbolic convolutional neural networks. _Research in the Mathematical Sciences_, 9(4):60, 2022.
* [5] Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, and Ivan Oseledets. Hyperbolic vision transformers: Combining improvements in metric learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7409-7419, 2022.
* [6] Zhe Liu and Yibin Xu. Thg: Transformer with hyperbolic geometry. _arXiv preprint arXiv:2106.07350_, 2021.
* [7] Boli Chen, Yao Fu, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen, and Liping Jing. Probing {bert} in hyperbolic spaces. In _International Conference on Learning Representations_, 2021.
* [8] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In _International conference on machine learning_, pages 1120-1128. PMLR, 2016.
* [9] Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled cayley transform. In _International Conference on Machine Learning_, pages 1969-1978. PMLR, 2018.
* [10] Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, and Stella X Yu. Orthogonal convolutional neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 11505-11515, 2020.
* [11] Shuai Li, Kui Jia, Yuxin Wen, Tongliang Liu, and Dacheng Tao. Orthogonal deep neural networks. _IEEE transactions on pattern analysis and machine intelligence_, 43(4):1352-1368, 2019.
* [12] Mevlana C Gemici, Danilo Rezende, and Shakir Mohamed. Normalizing flows on riemannian manifolds. _arXiv preprint arXiv:1611.02304_, 2016.
* [13] Emile Mathieu and Maximilian Nickel. Riemannian continuous normalizing flows. _Advances in Neural Information Processing Systems_, 33:2503-2515, 2020.
* [14] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville. Riemannian diffusion models. _Advances in Neural Information Processing Systems_, 35:2750-2761, 2022.
* [15] Max van Spengler, Erwin Berkhout, and Pascal Mettes. Poincar\(\backslash\)'e resnet. _arXiv preprint arXiv:2303.14027_, 2023.
* [16] Edoardo Cetin, Benjamin Paul Chamberlain, Michael M. Bronstein, and Jonathan J Hunt. Hyperbolic deep reinforcement learning. In _The Eleventh International Conference on Learning Representations_, 2023.
* [17] Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. _IEEE Transactions on Automatic Control_, 58(9):2217-2229, 2013.
* [18] Antti Honkela, Tapani Raiko, Mikael Kuusela, Matti Tornio, and Juha Karhunen. Approximate riemannian conjugate gradient learning for fixed-form variational bayes. _The Journal of Machine Learning Research_, 11:3235-3268, 2010.
* [19] Nicolas Boumal. Riemannian trust regions with finite-difference hessian approximations are globally convergent. In _Geometric Science of Information: Second International Conference, GSI 2015, Palaiseau, France, October 28-30, 2015, Proceedings 2_, pages 467-475. Springer, 2015.

* Absil [2007] P-A Absil. Trust-region methods on riemannian manifolds. _Foundations of Computational Mathematics_, 7:303-330, 2007.
* Tripuraneni et al. [2018] Nilesh Tripuraneni, Nicolas Flammarion, Francis Bach, and Michael I Jordan. Averaging stochastic gradient descent on riemannian manifolds. In _Conference On Learning Theory_, pages 650-687. PMLR, 2018.
* Ferreira et al. [2019] Orizon P Ferreira, Mauricio S Louzeiro, and LF4018420 Prudente. Gradient method for optimization on riemannian manifolds with lower bounded curvature. _SIAM Journal on Optimization_, 29(4):2517-2541, 2019.
* Zhu and Sato [2020] Xiaojing Zhu and Hiroyuki Sato. Riemannian conjugate gradient methods with inverse retraction. _Computational Optimization and Applications_, 77:779-810, 2020.
* Weber and Sra [2022] Melanie Weber and Svarit Sra. Riemannian optimization via frank-wolfe methods. _Mathematical Programming_, pages 1-32, 2022.
* Zhang et al. [2016] Hongyi Zhang, Sashank J Reddi, and Suvrit Sra. Riemannian svrg: Fast stochastic optimization on riemannian manifolds. _Advances in Neural Information Processing Systems_, 29, 2016.
* Kasai et al. [2018] Hiroyuki Kasai, Hiroyuki Sato, and Bamdev Mishra. Riemannian stochastic recursive gradient algorithm. In _International Conference on Machine Learning_, pages 2516-2524. PMLR, 2018.
* Zhang et al. [2018] Jingzhao Zhang, Hongyi Zhang, and Suvrit Sra. R-spider: A fast riemannian stochastic optimization algorithm with curvature independent rate. _arXiv preprint arXiv:1811.04194_, 2018.
* Zhao et al. [2015] Zhi Zhao, Zheng-Jian Bai, and Xiao-Qing Jin. A riemannian newton algorithm for nonlinear eigenvalue problems. _SIAM Journal on Matrix Analysis and Applications_, 36(2):752-774, 2015.
* Kasai et al. [2018] Hiroyuki Kasai, Hiroyuki Sato, and Bamdev Mishra. Riemannian stochastic quasi-newton algorithm with variance reduction and its convergence analysis. In _International Conference on Artificial Intelligence and Statistics_, pages 269-278. PMLR, 2018.
* Roy et al. [2018] Soumava Kumar Roy, Zakaria Mhammedi, and Mehrtash Harandi. Geometry aware constrained optimization techniques for deep learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 4460-4469, 2018.
* Becigneul and Ganea [2019] Gary Becigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. In _International Conference on Learning Representations_, 2019.
* Kasai et al. [2019] Hiroyuki Kasai, Pratik Jawanpuria, and Bamdev Mishra. Riemannian adaptive stochastic gradient algorithms on matrix manifolds. In _International Conference on Machine Learning_, pages 3262-3271. PMLR, 2019.
* Foret et al. [2021] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _International Conference on Learning Representations_, 2021.
* Kwon et al. [2021] Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In _International Conference on Machine Learning_, pages 5905-5914. PMLR, 2021.
* Kim et al. [2022] Minyoung Kim, Da Li, Shell X Hu, and Timothy Hospedales. Fisher sam: Information geometry and sharpness aware minimisation. In _International Conference on Machine Learning_, pages 11148-11161. PMLR, 2022.
* Du et al. [2022] Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent Tan. Efficient sharpness-aware minimization for improved training of neural networks. In _International Conference on Learning Representations_, 2022.
* Zhuang et al. [2022] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha C Dvornek, sekhar tatikonda, James s Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware training. In _International Conference on Learning Representations_, 2022.
* Lee and Lee [2012] John M Lee and John M Lee. _Smooth manifolds_. Springer, 2012.
* Absil et al. [2008] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. _Optimization algorithms on matrix manifolds_. Princeton University Press, 2008.
* Boumal [2023] Nicolas Boumal. _An introduction to optimization on smooth manifolds_. Cambridge University Press, 2023.

* [41] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* [42] Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, and Nando de Freitas. Hyperbolic attention networks. In _International Conference on Learning Representations_, 2019.
* [43] Asher Trockman and J Zico Kolter. Orthogonalizing convolutional layers with the cayley transform. In _International Conference on Learning Representations_, 2021.
* [44] Shun-Ichi Amari. Natural gradient works efficiently in learning. _Neural computation_, 10(2):251-276, 1998.
* [45] Ovidiu Calin. _Neuromanifolds_, pages 465-504. Springer International Publishing, Cham, 2020.
* [46] Shun-ichi Amari, Hyeyoung Park, and Tomoko Ozeki. Geometrical singularities in the neuromanifold of multilayer perceptrons. _Advances in neural information processing systems_, 14, 2001.
* [47] Cheongjae Jang, Sungyoon Lee, Frank Park, and Yung-Kyun Noh. A reparametrization-invariant sharpness measure based on information geometry. _Advances in neural information processing systems_, 35:27893-27905, 2022.
* [48] Andi Han and Junbin Gao. Riemannian stochastic recursive momentum method for non-convex optimization. In Zhi-Hua Zhou, editor, _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21_, pages 2505-2511. International Joint Conferences on Artificial Intelligence Organization, 8 2021. Main Track.
* [49] Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower bounds for non-convex stochastic optimization. _Mathematical Programming_, 199(1-2):165-214, 2023.
* [50] Andi Han and Junbin Gao. Variance reduction for riemannian non-convex optimization with batch size adaptation. _arXiv preprint arXiv:2007.01494_, 2020.
* [51] Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization. In _International Conference on Machine Learning_, pages 639-668. PMLR, 2022.
* [52] Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Fully hyperbolic neural networks. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5672-5686, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* [53] Max Kochurov, Rasul Karimov, and Serge Kozlukov. Geoopt: Riemannian optimization in pytorch. _arXiv preprint arXiv:2005.02819_, 2020.
* [54] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [55] Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In _Proceedings of the 3rd workshop on continuous vector space models and their compositionality_, pages 57-66, 2015.
* [56] Ryohei Shimizu, YUSUKE Mukuta, and Tatsuya Harada. Hyperbolic neural networks++. In _International Conference on Learning Representations_, 2021.
* [57] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In _International conference on machine learning_, pages 1243-1252. PMLR, 2017.
* [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.

## Appendix A Proofs of Theorem 1

We basically follow the arguments in the convergence of Euclidean SAM [51], but the details are totally different.

**Lemma 1** (Properties of Retraction Smoothness).: _Let \(w^{*}=\mathrm{R}_{w}\big{(}\rho\mathrm{grad}\mathcal{L}(w)\big{)}\) and the curve \(\gamma(t)=\mathrm{R}_{w}(t\eta)\) with the endpoints \(\gamma(0)=w\) and \(\gamma(1)=w^{*}\). Then, we have_

\[\langle\mathcal{T}(\gamma)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}(w^{*}), \mathrm{grad}\mathcal{L}(w)\rangle\geq(1-\rho L_{\mathrm{R}})\|\mathrm{grad} \mathcal{L}(w)\|_{w}^{2}\]

Proof.: By the condition (C-5), we have

\[\|\mathcal{T}(\gamma)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}(w^{*})-\mathrm{ grad}\mathcal{L}(w)\|_{w}\leq L_{\mathrm{R}}\|\eta\|_{w}\]

By the Cauchy-Schwarz inequality, we have

\[|\langle\mathcal{T}(\gamma)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}( w^{*})-\mathrm{grad}\mathcal{L}(w),\eta\rangle_{w}| \leq\|\mathcal{T}(\gamma)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}(w^{* })-\mathrm{grad}\mathcal{L}(w)\|_{w}\|\eta\|_{w}\] \[\leq L_{\mathrm{R}}\|\eta\|_{w}^{2}\] \[\leq\rho^{2}L_{\mathrm{R}}\|\mathrm{grad}\mathcal{L}(w)\|_{w}^{2}\]

Therefore, we obtain

\[\langle\mathcal{T}(\gamma)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}(w^{*})-\mathrm{ grad}\mathcal{L}(w),\rho\mathrm{grad}\mathcal{L}(w)\rangle_{w}\geq-\rho^{2}L_{ \mathrm{R}}\|\mathrm{grad}\mathcal{L}(w)\|_{w}^{2}\]

Removing the constant \(\rho\), the above inequality becomes

\[\langle\mathcal{T}(\gamma)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}(w^{*})-\mathrm{ grad}\mathcal{L}(w),\mathrm{grad}\mathcal{L}(w)\rangle_{w}\geq-\rho L_{ \mathrm{R}}\|\mathrm{grad}\mathcal{L}(w)\|_{w}^{2}\]

Lastly, we arrive at the final result as

\[\langle\mathcal{T}(\gamma)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}(w^{ *}),\mathrm{grad}\mathcal{L}(w)\rangle_{w} =\langle\mathcal{T}(\gamma)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}(w ^{*})-\mathrm{grad}\mathcal{L}(w),\mathrm{grad}\mathcal{L}(w)\rangle_{w}+\| \mathrm{grad}\mathcal{L}(w)\|_{w}^{2}\] \[\geq(1-\rho L_{\mathrm{R}})\|\mathrm{grad}\mathcal{L}(w)\|_{w}^{2}\]

In the next lemma, we will show the alignment of the true Riemannian gradient and the true Riemannian SAM gradient.

**Lemma 2** (Alignment of the true Riemannian gradient and the true Riemannian SAM gradient).: _Let us denote the stochastic Riemannian gradient at time \(t\) by \(\mathrm{grad}\mathcal{L}_{t}(w)=\frac{1}{b}\sum_{i\in J_{t}}\mathrm{grad} \mathcal{L}(w;x_{i})\in\mathrm{T}_{w}\mathcal{M}\) and \(w^{adv}=\mathrm{R}_{w}\big{(}\rho\mathrm{grad}\mathcal{L}_{t}(w)\big{)}\). Further, let \(\gamma(t)=\mathrm{R}_{w}(t\eta)\) be a retraction curve with \(\gamma(0)=w\) and \(\gamma(1)=w^{adv}\). Then, we have the following inequality_

\[\mathbb{E}\Big{[}\Big{\langle}\mathcal{T}(\gamma)_{w^{adv}}^{w}\mathrm{grad} \mathcal{L}_{t}(w^{adv}),\mathrm{grad}\mathcal{L}(w)\Big{\rangle}_{w}\Big{]} \geq\Big{(}\frac{1}{2}-\rho L_{\mathrm{R}}-3\rho^{2}L_{\mathrm{R}}^{2}\Big{)} \Big{\|}\mathrm{grad}\mathcal{L}(w)\Big{\|}_{w}^{2}-\frac{2\rho^{2}L_{\mathrm{ R}}^{2}\sigma^{2}}{b}\]

Proof.: Let \(w^{*}=\mathrm{R}_{w}\big{(}\rho\mathrm{grad}\mathcal{L}(w)\big{)}\) evaluated on the loss function. We first add and subtract \(\langle\mathcal{T}(\zeta)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}_{t}(w^{*}), \mathrm{grad}\mathcal{L}(w)\rangle_{w}\) where \(\zeta(t)=\mathrm{R}_{w}(t\xi)\) is a retraction curve where \(\zeta(0)=w\) and \(\zeta(1)=w^{*}\).

\[\langle\mathcal{T}(\gamma)_{w^{adv}}^{w}\mathrm{grad}\mathcal{L}_ {t}(w^{adv}),\mathrm{grad}\mathcal{L}(w)\rangle_{w} =\underbrace{\langle\mathcal{T}(\gamma)_{w^{adv}}^{w}\mathrm{ grad}\mathcal{L}_{t}(w^{adv})-\mathcal{T}(\zeta)_{w^{*}}^{w}\mathrm{grad} \mathcal{L}_{t}(w^{*}),\mathrm{grad}\mathcal{L}(w)\rangle_{w}}_{\mathcal{T}_{1}}\]

We will bound two terms, \(T_{1}\) and \(T_{2}\), separately. Regarding the term \(T_{1}\), we derive

\[-T_{1} =-\langle\mathcal{T}(\gamma)_{w^{adv}}^{w}\mathrm{grad}\mathcal{L} _{t}(w^{adv})-\mathcal{T}(\zeta)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}_{t}(w^{*}), \mathrm{grad}\mathcal{L}(w)\rangle_{w}\] \[\leq\frac{1}{2}\Big{\|}\mathcal{T}(\gamma)_{w^{adv}}^{w}\mathrm{ grad}\mathcal{L}_{t}(w^{adv})-\mathcal{T}(\zeta)_{w^{*}}^{w}\mathrm{grad} \mathcal{L}_{t}(w^{*})\Big{\|}_{w}^{2}+\frac{1}{2}\Big{\|}\mathrm{grad}\mathcal{L }(w)\Big{\|}_{w}^{2}\] \[\leq\Big{\|}\mathcal{T}(\gamma)_{w^{adv}}^{w}\mathrm{grad} \mathcal{L}_{t}(w^{adv})-\mathrm{grad}\mathcal{L}_{t}(w)\Big{\|}_{w}^{2}+\Big{\|} \mathcal{T}(\zeta)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}_{t}(w^{*})-\mathrm{ grad}\mathcal{L}_{t}(w)\Big{\|}_{w}^{2}+\frac{1}{2}\Big{\|}\mathrm{grad} \mathcal{L}(w)\Big{\|}_{w}^{2}\] \[\leq L_{\mathrm{R}}^{2}\|\rho\mathrm{grad}\mathcal{L}_{t}(w)\|_{w}^ {2}+L_{\mathrm{R}}^{2}\|\rho\mathrm{grad}\mathcal{L}(w)\|_{w}^{2}+\frac{1}{2}\| \mathrm{grad}\mathcal{L}(w)\|_{w}^{2}\] \[\leq\rho^{2}L_{\mathrm{R}}^{2}\Big{(}2\|\mathrm{grad}\mathcal{L}_{t }(w)-\mathrm{grad}\mathcal{L}(w)\|_{w}^{2}+2\|\mathrm{grad}\mathcal{L}(w)\|_{w}^ {2}\Big{)}+\Big{(}\frac{1}{2}+\rho^{2}L_{\mathrm{R}}^{2}\Big{)}\|\mathrm{ grad}\mathcal{L}(w)\|_{w}^{2}\] \[\leq\frac{2\rho^{2}L_{\mathrm{R}}^{2}\sigma^{2}}{b}+\Big{(}\frac{1 }{2}+3\rho^{2}L_{\mathrm{R}}^{2}\Big{)}\|\mathrm{grad}\mathcal{L}(w)\|_{w}^{2}\]From the above inequality, we could finally bound the term \(T_{1}\) as

\[T_{1}\geq-\frac{2\rho^{2}L_{\mathrm{R}}^{2}\sigma^{2}}{b}-\Big{(}\frac{1}{2}+3 \rho^{2}L_{\mathrm{R}}^{2}\Big{)}\|\mathrm{grad}\mathcal{L}(w)\|_{w}^{2}\]

Regarding the term \(T_{2}\), we just use the lemma as

\[T_{2}=\langle\mathcal{T}(\zeta)_{w^{*}}^{w}\mathrm{grad}\mathcal{L}_{t}(w^{*}), \mathrm{grad}\mathcal{L}(w)\rangle_{w}\geq(1-\rho L_{\mathrm{R}})\|\mathrm{ grad}\mathcal{L}(w)\|_{w}^{2}\]

Hence, we arrive at

\[\mathbb{E}\Big{[}\Big{\langle}\mathcal{T}(\gamma)_{w^{*de}}^{w}\mathrm{grad} \mathcal{L}_{t}(w^{adv}),\mathrm{grad}\mathcal{L}(w)\Big{\rangle}_{w}\Big{]} \geq\Big{(}\frac{1}{2}-\rho L_{\mathrm{R}}-3\rho^{2}L_{\mathrm{R}}^{2}\Big{)} \Big{\|}\mathrm{grad}\mathcal{L}(w)\Big{\|}_{w}^{2}-\frac{2\rho^{2}L_{\mathrm{ R}}^{2}\sigma^{2}}{b}\]

According to Algorithm 1, we follow the notation as

\[\mathrm{grad}\mathcal{L}_{t}(w) =\frac{1}{b}\sum_{i\in I_{t}}\mathrm{grad}\ell_{i}(w)\] \[w_{t}^{adv} =\mathrm{R}_{w_{t}}\big{(}\rho\mathrm{grad}\mathcal{L}_{t}(w_{t}) \big{)}\]

We assume the stochastic \(m\)-SAM where the same batch is used for both inner and outer updates.

**Lemma 3** (Descent inequality).: _Under the assumptions in Theorem 1, we have_

Proof.: Using the condition (C-4), we have

\[\mathcal{L}(w_{t+1}) =\mathcal{L}\Bigg{(}\mathrm{R}_{w_{t}}\Big{(}-\alpha\mathcal{T}( \gamma)_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w_{t}^{adv})\Big{)} \Bigg{)}\] \[\leq\mathcal{L}(w_{t})-\alpha\Big{\langle}\mathrm{grad}\mathcal{L} (w_{t}),\mathcal{T}(\gamma)_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w _{t}^{adv})\Big{\rangle}_{w_{t}}+\frac{\alpha^{2}L_{S}}{2}\Big{\|}\mathcal{T}( \gamma)_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w_{t}^{adv})\Big{\|} _{w_{t}}^{2}\]

For the last term in RHS, we can bound as

\[\Big{\|}\mathcal{T}(\gamma)_{w_{t}^{adv}}^{w_{t}}\mathrm{grad} \mathcal{L}_{t}(w_{t}^{adv})\Big{\|}_{w_{t}}^{2} =-\|\mathrm{grad}\mathcal{L}(w_{t})\|_{w_{t}}^{2}+\Big{\|}\mathcal{ T}(\gamma)_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w_{t}^{adv})- \mathrm{grad}\mathcal{L}(w_{t})\Big{\|}_{w_{t}}^{2}\] \[\quad+2\Big{\langle}\mathcal{T}(\gamma)_{w_{t}^{adv}}^{w_{t}} \mathrm{grad}\mathcal{L}_{t}(w_{t}^{adv}),\mathrm{grad}\mathcal{L}(w_{t}) \Big{\rangle}_{w_{t}}\]

Again, we have

\[\mathcal{L}(w_{t+1}) \leq\mathcal{L}(w_{t})-\alpha\Big{\langle}\mathrm{grad}\mathcal{L }(w_{t}),\mathcal{T}(\gamma)_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t} (w_{t}^{adv})\Big{\rangle}_{w_{t}}+\frac{\alpha^{2}L_{S}}{2}\Big{\|}\mathcal{ T}(\gamma)_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w_{t}^{adv}) \Big{\|}_{w_{t}}^{2}\] \[=\mathcal{L}(w_{t})-\frac{\alpha^{2}L_{S}}{2}\|\mathrm{grad} \mathcal{L}(w_{t})\|_{w_{t}}^{2}+\frac{\alpha^{2}L_{S}}{2}\Big{\|}\mathcal{T}( \gamma)_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w_{t}^{adv})- \mathrm{grad}\mathcal{L}(w_{t})\Big{\|}_{w_{t}}^{2}\] \[\quad-\alpha(1-\alpha L_{S})\Big{\langle}\mathcal{T}(\gamma)_{w_{t }^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w_{t}^{adv}),\mathrm{grad} \mathcal{L}(w_{t})\Big{\rangle}_{w_{t}}\] \[\leq\mathcal{L}(w_{t})-\frac{\alpha^{2}L_{S}}{2}\|\mathrm{grad} \mathcal{L}(w_{t})\|_{w_{t}}^{2}+\alpha^{2}L_{S}\Big{\|}\mathcal{T}(\gamma)_{ w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w_{t}^{adv})-\mathrm{grad} \mathcal{L}_{t}(w_{t})\Big{\|}_{w_{t}}^{2}\] \[\quad+\alpha^{2}L_{S}\|\mathrm{grad}\mathcal{L}_{t}(w_{t})-\mathrm{ grad}\mathcal{L}(w_{t})\|_{w_{t}}^{2}-\alpha(1-\alpha L_{S})\Big{\langle} \mathcal{T}(\gamma)_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w_{t}^{ adv}),\mathrm{grad}\mathcal{L}(w_{t})\Big{\rangle}_{w_{t}}\] \[\leq\mathcal{L}(w_{t})-\frac{\alpha^{2}L_{S}}{2}\|\mathrm{grad} \mathcal{L}(w_{t})\|_{w_{t}}^{2}+\alpha^{2}L_{S}^{3}\rho^{2}\|\mathrm{grad} \mathcal{L}_{t}(w_{t})\|_{w_{t}}^{2}+2\alpha^{2}L_{S}^{3}\rho^{2}\|\mathrm{ grad}\mathcal{L}_{t}(w_{t})-\mathrm{grad}\mathcal{L}(w_{t})\|_{w_{t}}^{2}\] \[\quad+\alpha^{2}L_{S}\|\mathrm{grad}\mathcal{L}_{t}(w_{t})- \mathrm{grad}\mathcal{L}(w_{t})\|_{w_{t}}^{2}-\alpha(1-\alpha L_{S})\Big{\langle} \mathcal{T}(\gamma)_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w_{t}),\mathrm{grad}\mathcal{L}(w_{t})\Big{\rangle}_{w_{t}}\] \[=\mathcal{L}(w_{t})-\frac{\alpha^{2}L_{S}(1-4L_{S}^{2}\rho^{2})}{2} \|\mathrm{grad}\mathcal{L}(w_{t})\|_{w_{t}}^{2}+\alpha^{2}L_{S}(1+2L_{S}^{2} \rho^{2})\|\mathrm{grad}\mathcal{L}_{t}(w_{t})-\mathrm{grad}\mathcal{L}(w_{t}) \|_{w_{t}}^{2}\] \[\quad-\alpha(1-\alpha L_{S})\Big{\langle}\mathcal{T}(\gamma)_{w_{t }^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w_{t}^{adv}),\mathrm{grad} \mathcal{L}(w_{t})\Big{\rangle}_{w_{t}}\]Taking the expectation on both sides, we have

\[\mathbb{E}\big{[}\mathcal{L}(w_{t+1})\big{]} \leq\mathbb{E}\big{[}\mathcal{L}(w_{t})\big{]}-\frac{\alpha^{2}L_{S} (1-4L_{S}^{2}\rho^{2})}{2}\mathbb{E}\big{[}\|\mathrm{grad}\mathcal{L}(w_{t})\| _{w_{t}}^{2}\big{]}+\frac{\alpha^{2}L_{S}(1+2L_{S}^{2}\rho^{2})\sigma^{2}}{b}\] \[\quad-\alpha(1-\alpha L_{S})\mathbb{E}\Bigg{[}\Big{\langle} \mathcal{T}(\gamma)_{w_{t}^{adv}}^{w_{t}}\mathrm{grad}\mathcal{L}_{t}(w_{t}^{ adv}),\mathrm{grad}\mathcal{L}(w_{t})\Big{\rangle}_{w_{t}}\Bigg{]}\] \[\mathbb{E}\big{[}\mathcal{L}(w_{t})\big{]}-\frac{\alpha^{2}L_{S} (1-4L_{S}^{2}\rho^{2})}{2}\mathbb{E}\big{[}\|\mathrm{grad}\mathcal{L}(w_{t})\| _{w_{t}}^{2}\big{]}+\frac{\alpha^{2}L_{S}(1+2L_{S}^{2}\rho^{2})\sigma^{2}}{b}\] \[\quad-\alpha(1-\alpha L_{S})\Bigg{[}\Big{(}\frac{1}{2}-\rho L_{ \mathrm{R}}-3\rho^{2}L_{\mathrm{R}}^{2}\Big{)}\mathbb{E}\big{[}\|\mathrm{ grad}\mathcal{L}(w_{t})\|_{w_{t}}^{2}\big{]}-\frac{2\rho^{2}L_{\mathrm{R}}^{2} \sigma^{2}}{b}\Bigg{]}\]

For sufficiently large number of total iteration \(T\), the condition \(\rho\leq\frac{1}{4L}\) is easily satisfied where \(\widetilde{L}=\max\{L_{\mathrm{R}},L_{S}\}\) (defined in Theorem 1). Hence, we obtain

\[\frac{3\alpha}{8}\mathbb{E}\big{[}\|\mathrm{grad}\mathcal{L}(w_{t })\|_{w_{t}}^{2}\big{]} \leq\mathbb{E}\big{[}\mathcal{L}(w_{t})\big{]}-\mathbb{E}\big{[} \mathcal{L}(w_{t+1})\big{]}+\frac{\alpha^{2}L_{S}(1+2L_{S}^{2}\rho^{2}) \sigma^{2}}{b}+\frac{2\alpha(1-\alpha L_{S})\rho^{3}L_{\mathrm{R}}^{2}\sigma^ {2}}{b}\] \[\leq\mathbb{E}\big{[}\mathcal{L}(w_{t})\big{]}-\mathbb{E}\big{[} \mathcal{L}(w_{t+1})\big{]}+\frac{\alpha^{2}L_{S}\sigma^{2}}{b}+\frac{2\alpha ^{2}L_{S}^{3}\rho^{2}\sigma^{2}}{b}+\frac{2\alpha\rho^{3}L_{\mathrm{R}}^{2} \sigma^{2}}{b}\]

By telescoping the above inequality from \(t=0\sim T-1\), we arrive at

\[\mathbb{E}\big{[}\|\mathrm{grad}\mathcal{L}(\widetilde{w})\|_{ \widetilde{w}}^{2}\big{]}=\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\big{[}\| \mathrm{grad}\mathcal{L}(w_{t})\|_{w_{t}}^{2}\big{]}\leq\frac{8\Delta}{3\alpha T }+\frac{8\alpha^{2}L_{S}\sigma^{2}}{3b}+\frac{16\alpha^{2}L_{S}^{3}\rho^{2} \sigma^{2}}{3b}+\frac{16\alpha\rho^{3}L_{\mathrm{R}}^{2}\sigma^{2}}{3b}\]

Under the step size condition \(\alpha_{t}=\frac{1}{\sqrt{TL}}\) and \(\rho_{t}=\frac{1}{T^{1/6}L}\), we finally get

\[\mathbb{E}\big{[}\|\mathrm{grad}\mathcal{L}(\widetilde{w})\|_{ \widetilde{w}}^{2}\big{]}\leq\frac{Q_{1}\widetilde{L}\Delta}{\sqrt{T}}+\frac {Q_{2}\sigma^{2}}{b\sqrt{T}}+\frac{Q_{3}\sigma^{2}}{bT^{5/6}}\]

for appropriate constants \(\{Q_{i}\}_{i=1}^{3}\).

[MISSING_PAGE_EMPTY:17]