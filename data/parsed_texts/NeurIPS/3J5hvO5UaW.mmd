# Optimal Classification under Performative Distribution Shift

Edwige Cyffers

Univ. Lille, Inria, CNRS, Centrale Lille,

UMR 9189 - CRIStAL, F-59000 Lille

edwige.cyffers@inria.fr

&Muni Sreenivas Pydi

Universite Paris Dauphine, Universite PSL,

CNRS, LAMSADE, 75016 Paris &Jamal Atif

Universite Paris Dauphine, Universite PSL,

CNRS, LAMSADE, 75016 Paris &Oliver Cappe

Ecole Normale Superieure, Universite PSL,

CNRS, Inria, DI ENS, 75005 Paris

###### Abstract

Performance learning addresses the increasingly pervasive situations in which algorithmic decisions may induce changes in the data distribution as a consequence of their public deployment. We propose a novel view in which these performative effects are modelled as push-forward measures. This general framework encompasses existing models and enables novel performative gradient estimation methods, leading to more efficient and scalable learning strategies. For distribution shifts, unlike previous models which require full specification of the data distribution, we only assume knowledge of the shift operator that represents the performative changes. This approach can also be integrated into various change-of-variable-based models, such as VAEs or normalizing flows. Focusing on classification with a linear-in-parameters performative effect, we prove the convexity of the performative risk under a new set of assumptions. Notably, we do not limit the strength of performative effects but rather their direction, requiring only that classification becomes harder when deploying more accurate models. In this case, we also establish a connection with adversarially robust classification by reformulating the minimization of the performative risk as a min-max variational problem. Finally, we illustrate our approach on synthetic and real datasets.

## 1 Introduction

Machine learning models are increasingly deployed in real-world scenarios where their predictions can influence the users' behaviors, thereby altering the underlying data distribution. This phenomenon, though rooted in long-standing economic theory [22, 14], has recently attracted interest in the machine learning community under the name of _performance prediction_[17, 1]. Consider for instance a social ranking system: if it consistently favors a particular subpopulation of individuals, user behavior might shift towards mimicking the main characteristics of this subgroup or, conversely, some features of this subpopulation can undergo modification as a consequence of the selection by the system, both effects leading to subtle alterations of the original data distribution. More generally, performative learning captures dynamics at stake in strategic classification, where individuals are confronted by algorithmic decisions that impact their life - such as loan acceptance, college admission, probation - and might thus try to overturn predictions by optimizing some of their features.

This feedback loop, where predictions influence future data, poses new challenges and necessitates the development of novel approaches within statistical learning theory and practice [17].

2020, Jagadeesan et al., 2022, Drusvyatskiy and Xiao, 2023, Hardt and Mendler-Dunner, 2023, Zezulka and Genin, 2023]. Perdomo et al. [2020] proposed to formalize performative learning as a generalized risk minimization problem, with the _performance risk_ being defined as

\[\mathrm{PR}(\theta)=\mathbb{E}_{\theta}[\ell(Z;\theta)], \tag{1}\]

where \(\ell\) is a loss function, \(\theta\) a model's parameters, and \(Z\) an observable random variable drawn from a distribution \(\mathbb{P}_{\theta}\) also parametrized by \(\theta\) itself. In light of the difficulty of minimizing \(\mathrm{PR}(\theta)\) directly, one can define a _decoupled performative risk_ as \(\mathrm{DPR}(\theta,\theta^{\prime})=\mathbb{E}_{\theta}[\ell(Z;\theta^{\prime})]\), clarifying the interplay between the model's prediction and the distribution change. This can be seen as a Stackelberg game that stabilizes when neither the modeler (learned parameters) nor the environment (distribution) has incentive to change their states. Solving the performative learning problem consists in minimizing this risk under the constraint that \(\theta=\theta^{\prime}\), because the testing samples will follow the distribution corresponding to the deployed model, and thus \(\mathrm{PR}(\theta)=\mathrm{DPR}(\theta,\theta)\). Minimizing \(\mathrm{DPR}(\theta,\theta^{\prime})\) w.r.t. \(\theta^{\prime}\) for a fixed \(\theta\) corresponds to the classical machine learning setting. In contrast, estimating the performative effect, i.e., knowing how to optimize \(\theta\) for a given \(\theta^{\prime}\) is more challenging as, per definition, one can only perform statistics from samples collected for values of the parameters \(\theta\) for which the model has already been deployed. Hence, performative learning does require some form of counterfactual extrapolation, i.e., what will happen to the data distribution when the parameter \(\theta\) changes from its current setting?

Hence, instead of focusing on methods finding performatively optimal points, \(\theta_{PO}\in\operatorname*{arg\,min}\mathrm{PR}(\theta)\), many previous works, following Perdomo et al. [2020], focus on finding stable points \(\theta_{PS}\in\operatorname*{arg\,min}\mathrm{DPR}(\theta_{PS},\theta)\), through methods that iteratively minimize the empirical risk. This line of research is appropriate in settings where the performative effect can be tamed. If it is sufficiently small, explicitly taking into account the performative changes of distribution is not required and optimal and stable points will be close enough [Perdomo et al., 2020]. However, real use cases do not always satisfy such strong assumptions (see further discussion in Section 4). In general, stable points may not be good proxys for performatively optimal points, particularly in settings where the performative effect cannot be bounded a priori.

Towards this goal, another line of research focuses on finding the optimal points \(\theta_{PO}\). Izzo et al. [2022] propose to use Monte Carlo sample-based approximations of the gradient of the performative risk, \(\nabla_{\theta}\,\mathrm{PR}(\theta)\), based on the score function estimator (see Section 2 below). Miller et al. [2021] use a two-stage approach that deploys random models to estimate the performative effect in the first stage, and then minimizes the estimated performative risk in the second stage. A drawback of both of these approaches is the restrictive set of assumptions needed to show that the algorithms converge. While Izzo et al. [2022] assume the convexity of \(\mathrm{PR}(\theta)\) along with smoothness and boundedness conditions, Miller et al. [2021] assume that the loss function is simultaneously strongly convex and smooth. Moreover, the score function estimator of Izzo et al. [2022] necessitates full knowledge of a parametric form of \(\mathbb{P}_{\theta}\), which is unrealistic in practice. Alternatively, Jagadeesan et al. [2022] resort to derivative-free (or zeroth-order) optimization strategies. However, such an approach is appropriate only when it is possible to sequentially deploy a large number of model instances, and it does not scale with the dimension of the parameter \(\theta\).

The present work is connected to the second line of the research explored above, where the focus is on finding the optimal point \(\theta_{PO}\). Our contributions are as follows.

**(i) Model the performative effect as a push-forward operator** This novel approach provides a _new explicit expression of the performative gradient_. Not only does this approach allow estimation of the performative gradient in settings where previous methods couldn't, but we show that in typical use cases, the variance of this new estimator is significantly smaller.

**(ii) Convexity for Performative Classification** We then focus on the specific task of strategic classification, as this performative learning problem encompasses various real-use cases with important societal impact such as college admission or credit decisions. Our second contribution is to provide new convexity results on the performative risk in this case. Whereas existing results were only proving convexity under assumptions restricting the performative effect to be small compared to the (assumed) strong convexity of the loss function \(\ell(z;\theta)\), our results leverage structural assumptions on the performative effect that ensure that the performative risk is convex _without any restriction on the strength of the performative effect_.

**(iii) Linking Performative and Robust Learning** We establish a connexion between performative learning and adversarially robust learning, paving the way to transferring robustness results to the performative learning field. In particular, this result gives new insights on the empirical evidence in favor of using regularization in the presence of performative effets. Finally, we illustrate our findings on synthetic and real-world datasets.

## 2 Push-forward Model for Performative Effects

In this section, we study the general performative learning setting without yet specializing it to the classification context. In Section 2.1, we introduce the push-forward model of performative learning and derive the expression of the gradient of the performative risk under this model. In Section 2.2, we present a reparameterization-based estimator for the gradient of the performative risk, and compare it to the score function based estimator considered by Izzo et al. (2022).

### The Push-forward Model

We aim to minimize the performative risk defined in eq.1, where the observation \(Z\) is drawn from the distribution \(\mathbb{P}_{\theta}\), which depends on the parameter \(\theta\in\mathbb{R}^{p}\) of the learning model. For this to be tractable, one needs additional hypotheses on the nature of the performative effect. We propose to represent the performative effect through a push-forward measure, which matches the intuition of having an untouched distribution that is steered by the performative effect.

**Assumption 1** (Push-forward Performative Model).: _For a given model parameter \(\theta\in\mathbb{R}^{p}\), the samples' distribution under the performative effect is given by \(\mathbb{P}_{\theta}=\varphi(\cdot;\theta)_{\sharp}\mathbb{P}\), where \(\varphi(\cdot;\theta)\) is a differentiable invertible mapping on \(\mathbb{R}^{d}\), depending on \(\theta\)._

This assumption can be equivalently stated by the probabilistic representation \(Z\stackrel{{ d}}{{=}}\varphi(U;\theta)\), where \(U\sim\mathbb{P}\) (the symbol \(\stackrel{{ d}}{{=}}\) denoting equality in distribution). If \(\mathbb{P}\) admits a density, then so does \(\mathbb{P}_{\theta}\) with density function given by \(p_{\theta}(z)=|J_{z}\psi(z;\theta)|p(\psi(z;\theta))\) where \(\psi(\cdot;\theta)=\varphi^{-1}(\cdot;\theta)\). In this last formula, \(J_{z}\psi(z;\theta)\) refers to the Jacobian matrix where \([J_{z}\psi(z;\theta)]_{ij}=\frac{\partial\psi(z;\theta)_{i}}{\partial z_{j}}\).

From an abstract point of view, such a representation of a parametrized family of distributions exists under very general conditions. However, the above model is more interesting in scenarios where \(\varphi(\cdot;\theta)\) is a simple operator, for instance a linear one, as is the case in most of the examples considered by Perdomo et al. (2020); Miller et al. (2021), and the dependence with respect to \(\theta\) can also be made explicit. This representation is also modular in the sense that \(\varphi\) could be chosen as the composition \(\varphi(u;\theta)=\varphi_{0}(\varphi_{1}(u;\theta))\), where \(\varphi_{0}^{-1}(Z)\) corresponds to a fixed (not depending on \(\theta\)) representation of \(Z\) in a feature space and \(\varphi_{1}(\cdot;\theta)\) models the performative effect _in the representation space_. Although we will not explicitly consider such cases in the rest of the paper, this representation of the performative effect is particularly attractive when using embedding tools based on kernels (Hofmann et al., 2008), neural nets (e.g., VAEs (Kingma and Welling, 2013)) or normalizing flows (Papamakarios et al., 2021; Kobyzev et al., 2021).

This structural assumption on the performative effect yields a new estimator for the performative gradient, which may be seen as an instance of the "reparametrization trick" used in VAEs, normalizing flows or by Kucukelbir et al. (2017). Mohamed et al. (2020) also refer to this approach as "pathwise" gradient estimation.

**Theorem 1** (Performative Risk Gradient).: _Under Assumption 1, the gradient of the performative risk is given by_

\[\nabla_{\theta}\operatorname{PR}(\theta)=\mathbb{E}_{\theta}\left[\nabla_{ \theta}\ell(Z;\theta)\right]+\mathbb{E}_{\theta}\left[\operatorname{J}_{\theta }^{T}\varphi(\psi(Z;\theta);\theta)\nabla_{z}\ell(Z;\theta)\right], \tag{2}\]

_where \(\nabla_{z}\ell(z;\theta)\) and \(\nabla_{\theta}\ell(z;\theta)\) denote respectively the gradient with respect to the first and the second parameter of the loss, and \(\operatorname{J}_{\theta}^{T}\varphi(u;\theta)\) is the transpose of the Jacobian with respect to \(\theta\)._

Proof.: Notice that under assumption 1, we can rewrite the decoupled risk with a change of variable as \(\operatorname{DPR}(\theta,\theta^{\prime})=\mathbb{E}[\ell(\varphi(U;\theta); \theta^{\prime})]\). This expression leads to the following.

\[\nabla_{\theta}\operatorname{PR}(\theta)=\nabla_{\theta}\mathbb{E}[\ell( \varphi(U;\theta);\theta)]=\mathbb{E}\left[\nabla_{\theta}\ell(\varphi(U; \theta);\theta)+\operatorname{J}_{\theta}^{T}\varphi(U;\theta)\nabla_{z}\ell (\varphi(U;\theta));\theta)\right],\]

which gives eq.2 under a change of variable.

### Estimating the Performance Gradient

From Theorem 1, it is clear that the gradient of performative risk in eq. (2) is composed of two terms - the first term corresponds to the classical risk minimization, while the second one - which we will refer to as the performative gradient in the following - captures the performative effect. The first term can be estimated by \(\frac{1}{n}\sum_{i=1}^{n}\nabla\ell(Z_{i};\theta)\) as usual. For the second term, we propose the following estimator.

**Definition 1** (Reparameterization-based Performative Gradient Estimator).: _The performative gradient \(\nabla_{\theta}\operatorname{DPR}(\theta,\theta^{\prime})|_{\theta^{\prime}=\theta}\) admits as unbiased estimator:_

\[\hat{G}^{\text{RP}}_{\theta}=\frac{1}{n}\sum_{i=1}^{n}\operatorname{J}_{\theta }^{T}\varphi(\psi(Z_{i};\theta);\theta)\nabla_{z}\ell(Z_{i};\theta). \tag{3}\]

This estimator allows performing gradient descent to minimize the performative risk and thus, if the performative objective is well behaved, to converge to the performative optimal point. \(\hat{G}^{\text{RP}}_{\theta}\) should be compared to the following estimator used by Izzo et al. (2022), which relies on the well-known score function formula -see (L'Ecuyer, 1991; Kleijnen and Rubinstein, 1996; Mohamed et al., 2020) and references therein.

\[\hat{G}^{\text{SF}}_{\theta}=\frac{1}{n}\sum_{i=1}^{n}\ell(Z_{i};\theta)\nabla _{\theta}\log p_{\theta}(Z_{i}).\]

While both \(\hat{G}^{\text{RP}}_{\theta}\) and \(\hat{G}^{\text{SF}}_{\theta}\) estimate the same quantity \(\nabla_{\theta}\operatorname{DPR}(\theta,\theta^{\prime})|_{\theta^{\prime}= \theta}\), \(\hat{G}^{\text{RP}}_{\theta}\) has two distinct advantages over \(\hat{G}^{\text{SF}}_{\theta}\). First, computing \(\hat{G}^{\text{SF}}_{\theta}\) requires access to the analytical form of \(p_{\theta}\), which is fairly unrealistic in a learning scenario, whereas our estimator \(\hat{G}^{\text{RP}}_{\theta}\) only requires knowledge of \(\varphi\), paving the way for a _semi-parametric approach_ in which the performative effect is modelled explicitly, but not the distribution of the data. For general maps \(\varphi\), \(\hat{G}^{\text{RP}}_{\theta}\) still requires to use the inverse mapping \(\psi\), however this is not required in situations where the Jacobian \(\operatorname{J}_{\theta}\varphi(u;\theta)\) does not depend on \(u\). Specifically, when \(\varphi\) is a shift operator, one obtains a very simple expression for \(\hat{G}^{\text{RP}}_{\theta}\) as shown in the following example.

**Example 1** (Shift Operator).: _If the performative effect can be modelled by a shift operator, i.e., \(\varphi(U;\theta)=U+\Pi(\theta)\), the \(\hat{G}^{\text{RP}}_{\theta}\) estimator is given by:_

\[\hat{G}^{\text{RP}}_{\theta}=\operatorname{J}_{\theta}^{T}\Pi(\theta)\,\frac{1 }{n}\sum_{i=1}^{n}\nabla_{z}\ell(Z_{i};\theta),\]

_where \(\operatorname{J}_{\theta}\Pi(\theta)\) is the Jacobian of the performative shift \(\Pi(\theta)\)._

In addition to removing the need to know \(p_{\theta}\), a second advantage of \(\hat{G}^{\text{RP}}_{\theta}\) is that it can lead to significant decrease of the variance of the estimates, as illustrated by the following example.

**Example 2** (Performative Gaussian Mean estimation).: _Let \(\ell(z;\theta)=\|z-\theta\|^{2}/2\), and \(Z\overset{d}{=}U+\Pi\theta\), that is, \(\Pi(\theta)=\Pi\theta\) is a linear shift operator. We will assume \(U\sim\mathcal{N}(0,\sigma^{2}I_{d})\), so that \(p_{\theta}(z)\propto\exp[-\|z-\Pi\theta\|^{2}/(2\sigma^{2})]\), where \(\Pi\) represents the performative effect. The gradient of \(\operatorname{DPR}(\theta,\theta^{\prime})\) w.r.t. the distributional parameter \(\theta\) is given both by_

\[\nabla_{\theta}\operatorname{DPR}(\theta,\theta^{\prime}) =\mathbb{E}_{\theta}[\Pi^{T}\nabla_{z}\ell(Z;\theta^{\prime})]= \Pi^{T}\mathbb{E}_{\theta}[Z-\theta^{\prime}]=\Pi^{T}\mathbb{E}[U+a]\] _(reparameterization)_ \[=\mathbb{E}_{\theta}[\ell(Z;\theta^{\prime})\nabla_{\theta}\log p _{\theta}(z)]=\Pi^{T}\frac{1}{2\sigma^{2}}\mathbb{E}\left[\|U+a\|^{2}U\right]\] _(score function)_ _where \[a=\Pi\theta-\theta^{\prime}\]. Hence, in this case \[\hat{G}^{\text{RP}}_{\theta}=\Pi^{T}\frac{1}{n}\sum_{i=1}^{n}(U_{i}+a)\], while \[\hat{G}^{\text{SF}}_{\theta}=\Pi^{T}\frac{1}{2n\sigma^{2}}\sum_{i=1}^{n}\|U_ {i}+a\|^{2}U_{i}\]. Both of these expressions have equal expectation \[\Pi^{T}(\Pi\theta-\theta^{\prime})\] which corresponds to the gradient of \[\operatorname{DPR}(\theta,\theta^{\prime})\] w.r.t. \[\theta\]. However, the reparametrization estimator \[\hat{G}^{\text{RP}}_{\theta}\] has covariance \[\sigma^{2}\Pi^{T}\Pi/n\] while the score-based estimator \[\hat{G}^{\text{SF}}_{\theta}\] has covariance:_ \[\frac{1}{n}\Pi^{T}\left(\frac{(d^{2}+6d+8)\sigma^{2}+2(d+4)\|a\|^{2}+\|a\|^{4}/ \sigma^{2}}{4}I_{d}+aa^{T}\right)\Pi.\]The details of this computation can be found in appendix A.1. Both estimators are unbiased but note that while \(\hat{G}^{\text{RP}}_{\theta}\) would always be an unbiased estimator of the performative gradient without any further assumption on the distribution of \(U\), the unbiasedness of \(\hat{G}^{SF}_{\theta}\) relies on the fact that \(U\) is Gaussian. \(\hat{G}^{RP}_{\theta}\) has a covariance that does not depend on \(\theta\), \(\theta^{\prime}\) nor on the dimension \(d\). In contrast, \(\hat{G}^{SF}_{\theta}\)'s covariance includes a factor that increases with \(d^{2}\), making it unreliable in high dimensions. It also includes additional terms that grow with the norm of \(\Pi\theta-\theta^{\prime}\), so the estimator becomes less reliable when the performative effect is strong.

One could argue that the previous result does not provide a fair comparison between both estimators, as \(G^{SF}\)'s variance can be reduced by subtracting a baseline. Indeed, \(\tilde{G}^{\text{SF}}_{\theta}=\frac{1}{n}\sum_{i=1}^{n}(\ell(Z_{i};\theta)-m) \nabla_{\theta}\log p_{\theta}(Z_{i})\) is also an unbiased estimator of the gradient of the performative effect (for any choice of the baseline \(m\)), as the score function has, by definition, zero expectation. Tuning \(m\) properly may reduce the variance by creating a so-called _control variate_ --see, e.g., Greensmith et al. (2004) for the use of this principle in policy gradient methods. However, similar calculations (detailed in appendix A.1) show that the minimum covariance that can be achieved by subtracting a baseline is

\[\frac{1}{n}\Pi^{T}\left(\left((1+d/2)\sigma^{2}+\|a\|^{2}\right)I_{d}+aa^{T} \right)\Pi,\]

which is still larger than the covariance of \(\hat{G}^{\text{RP}}_{\theta}\) by a factor that grows with the model dimension \(d\). The fact that the reparameterization-based estimator is preferable when considering the Gaussian distribution with quadratic loss function was observed before by Mohamed et al. (2020) in the scalar case. The above computations however show that the difference between the two approaches gets more and more significant as the dimension increases. The case of other distribution/loss function combination still needs to be investigated.

## 3 Classification under Performative Shift

In this section, we specialize to the setting of binary classification which encompasses various machine learning applications where performative effects are expected. Usually, this setting involves a desirable class and an undesirable one. For example, the desirable class might represent college admission, loan acceptance, no-spam email, or probation. In this setting, one can also expect that individuals belonging to the favored class - we designate this as class \(1\) - do not need to alter their features, or only with small changes. On the contrary, individuals with negative predictions - in class \(0\) - have an incentive to modify their features, resulting in a significant performative effect.

We particularize the arguments introduced in section 2 to the setting of binary classification, by fixing \(z=(x,y)\), with a covariate vector \(x\in\mathbb{R}^{d}\) and a label \(y\in\{0,1\}\). As is done classically --see, e.g., (Bach, 2024), we further assume that the classifier \(f_{\theta}(x)\) is a real valued function that depends on a parameter \(\theta\in\mathbb{R}^{p}\) and that a convex loss surrogate \(\Phi\) is used, such that the loss function \(\ell(z;\theta)\) is equal to \(\Phi((-1)^{y}f_{\theta}(x))\). We model the performative effect as label-dependent push forward models, i.e., that, under \(\mathbb{F}_{\theta}\),

\[X|_{Y=1}\stackrel{{\text{d}}}{{=}}\varphi_{1}(U_{1};\theta)\text{ and }X|_{Y=0}\stackrel{{\text{d}}}{{=}}\varphi_{0}(U_{0};\theta),\]

where \(\varphi_{1}\) and \(\varphi_{0}\) represent the performative changes affecting class-conditional distributions of classes \(0\) and \(1\) respectively. If the classifier \(f_{\theta}(x)\) is sufficiently expressive, changes such that \(\varphi_{1}=\varphi_{0}\) will not create performative effects. We thus focus on scenarios where the _performative changes affect each class-dependent distribution differently_. For concreteness, we will assume the following.

**Assumption 2**.: \(\mathbb{P}_{\theta}(Y=1)=\rho\) _is fixed and not subject to performative effects._

**Assumption 3**.: \(\varphi_{1}\) _does not depend on \(\theta\), and for simplicity, we assume it is the identity function._

**Assumption 4**.: \(\varphi_{0}(u_{0};\theta)=u_{0}+\Pi(\theta)\) _is a shift operator._

Assumption 2 is a consequence of the intuitive property that even if the distribution is modified, the ground truth labels are not impacted by the performative effect. Assumption 3 allows to focus on the performative effect on the unfavored class and simplifies the presentation but could be easily relaxed. Finally, assumption 4 is restricting the performative change to a shift, which does simplify the problem but still corresponds to a realistic model for feature alteration.

It is important to stress that, despite the fact that this performative effect is modelled as a shift, the joint distribution of \(Z=(X,Y)\) does not belong to the location-scale family discussed by Miller et al. (2021). Under these assumptions, the decoupled performative risk takes the following form:

\[\mathrm{DPR}(\theta,\theta^{\prime})=\mathbb{E}_{\theta}\left[\Phi\left((-1)^{Y} f_{\theta^{\prime}}(X)\right)\right]=\rho\mathbb{E}\left[\Phi(f_{\theta^{\prime}}(U_ {1}))\right]+(1-\rho)\mathbb{E}\left[\Phi(-f_{\theta^{\prime}}(U_{0}+\Pi( \theta)))\right], \tag{4}\]

where the performative effect is only manifested in the second term, which corresponds to class 0.

**Remark 1** (Localization of the Performative Shift).: _In eq. (4), we refer to \(U_{0}\) which corresponds to the covariates of the second class in the absence of performative effect, i.e., when \(\theta=0\). However, for a shift operator, for any value of \(\bar{\theta}\), one may equivalently write that, under \(\mathbb{P}_{\theta}\), \(X\overset{d}{=}\varphi(U_{\bar{\theta}};\theta)\), where \(\varphi(u;\theta)=u+\Pi(\theta)-\Pi(\bar{\theta})\) and \(U_{\bar{\theta}}\) is distributed under \(\mathbb{P}_{\bar{\theta}}\). Thus eq. (4) can equivalently be rewritten by taking expectation under an arbitrary parameter value \(\bar{\theta}\), upon defining the performative effect as \(U_{\bar{\theta}}+\Pi(\theta)-\Pi(\bar{\theta})\) and the linear model as \(f_{\theta}(x)=x^{T}(\theta-\bar{\theta})\)._

## 4 Convexity of Performative Risk

Identification of the cases in which the performative risk \(\mathrm{PR}(\theta)\) is convex is an important step towards generalizing results obtained in the context of traditional (ie., non performative) learning theory. Existing results mainly exploit the fact that if the loss function is strongly convex, a sufficiently small performative effect cannot break this convexity. For this reason, it is often assumed (Miller et al., 2021; Hardt and Mendler-Dunner, 2023) that the change in the distributions has a bounded sensitivity with respect to the parameters, using the 1-Wasserstein distance:

\[W_{1}\left(P_{\theta},P_{\theta^{\prime}}\right)\leqslant\varepsilon\left\| \theta-\theta^{\prime}\right\|_{2}.\]

Note that in our setting, such \(\epsilon\) exists and corresponds to the operator norm of \(\Pi(\theta)\). In order to preserve convexity, it is then needed that \(\epsilon\leqslant\mu/2L\) when the risk is \(L\)-smooth and \(\mu\)-strongly convex. The pricing model, considered by Izzo et al. (2022), is a very simple example showing that the performative risk can be convex while not fulfilling this criterion.

**Example 3** (Pricing Model).: _Given a fixed set of \(d\) resources, the pricing model aims at finding the prices \(\theta\in\mathbb{R}^{d}\) for the \(d\) resources that maximize the overall profit given the elasticity of the demand level:_

\[\ell(z;\theta)=-z^{T}\theta\text{ with }Z\overset{d}{=}\varphi(U;\theta)=U- \Pi\theta, \tag{5}\]

_where \(\Pi\) is a diagonal matrix with positive elements, encoding the elasticity of the demand level to a raise in price of each resource, and \(\mu=\mathbb{E}[U]\) contains the baseline demand levels for each resource._

In this example, the performative risk \(\mathrm{PR}(\theta)=-\sum_{i=1}^{d}(\mu_{i}-\Pi_{ii}\theta_{i})\theta_{i}\) is a strongly convex quadratic function minimized at \(\theta_{i}^{*}=\mu_{i}/(2\Pi_{ii})\). In contrast, the decoupled performative risk \(\mathrm{DPR}(\theta,\theta^{\prime})=-(\alpha-\Pi\theta)^{T}\theta^{\prime}\) is still convex, but not strongly convex in \(\theta\) and is always minimized in \(\theta^{\prime}\) at infinity. Despite its simplicity, this example is thus not covered by existing theorems, and retraining procedures considered by Perdomo et al. (2020) fail by diverging.

Moreover, this example highlights that requiring a small sensitivity for the performative effect does not match the true convexity conditions. The performative risk is indeed strongly convex as long as the \(\Pi_{ii}\) are positive, irrespectively of their magnitude. In contrast, in this example, the performative risk would become non-convex if one of the \(\Pi_{ii}\) were negative, even with a small magnitude.

This motivates the search for related phenomenons in the classification context, by looking at conditions for ensuring the convexity of eq. (4). Indeed, we show that in the classification setting, one can also observe convexity without restriction on the magnitude of the performative effect. For this, we consider the case of linearly parameterized models, which we denote for simplicity by \(f_{\theta}(x)=x^{T}\theta\). Note that, as discussed in Section 2, our model of performative effect is composable because, we could also consider the more general linearly parameterized model in which \(f_{\theta}(x)=\psi(x)^{T}\theta\), with a linear-in-the-parameter performative effect in the feature space such that \(\psi(X)=U+\Pi(\theta)\). For ease of notation, we stick to the case where \(\psi\) is the identity function in the following. Using standard arguments, the choice of a convex loss surrogate \(\Phi\) then entails that \(\mathrm{DPR}(\theta,\theta^{\prime})\) is a convex function of \(\theta^{\prime}\). For the same reason, if \(\Pi(\theta)=\Pi\theta\) is a linear-in-parameters shift operator, \(\mathrm{DPR}(\theta,\theta^{\prime})\) is also convex in \(\theta\). Note however that, unless there is no performative effect (i.e., if \(\Pi=0\)), eq. (4) is not jointly convex in \((\theta,\theta^{\prime})\). The following result show that it is nonetheless the case that the performative risk \(\mathrm{PR}(\theta)=\mathrm{DPR}(\theta,\theta)\) is convex under the condition that \(\Pi\) is a positive semidefinite matrix (see proof in appendix A.2).

**Theorem 2** (Convexity of Classification Performative Risk).: _Under assumptions 2 to 4, for linearly parameterized classifier \(f_{\theta}(x)=x^{T}\theta\) and linear shift operator \(\Pi(\theta)=\Pi\theta\), the performative risk \(\mathrm{PR}(\theta)\) is convex when \(\Pi\) is a positive semidefinite matrix and one of the following conditions holds._

* \(\Phi\) _is the quadratic loss function;_
* \(\Phi\) _is a convex non increasing function (such as hinge, logistic or exp loss)._

This theorem allows us to extend the known convexity results to losses that are not strongly convex, and to performative effects with arbitrary magnitude.

**Remark 2** (Generalization to Performative Effect Affecting Both Classes).: _One could remove assumption 3 to allow class \(1\) to change under performative effect. The convexity of \(\mathrm{PR}(\theta)\) remains if \(\varphi_{1}(u;\theta)=u-\Pi_{1}\theta\), where \(\Pi_{1}\) is a positive semidefinite matrix. Similarly, the classification task becomes harder with performative effects and the performative risk is convex._

## 5 Connection with Robustness and Regularization

In order to enforce strong convexity of the loss function \(\ell(\cdot;\theta)\), previous works on performative prediction have considered the use of an additional regularization term --see, e.g., Section 5.2 of [20] where logistic regression is used with a ridge regularizer. When doing so, it has been observed empirically that the retraining method performs quite well. To build on this observation, we show below that for linear-in-the-parameter performative effects that tends to make the classification task harder, the performative optimum may indeed be interpreted as a regularized version of the base classification problem. This regularization does not take the form of and additive penalty but can be interpreted as the solution of a specific adversarially robust classification objective. In this section, we use the slightly stronger assumption that \(\Pi\) is a symmetric positive definite matrix, in order to ensure that both \(\|v\|_{\Pi}=(v^{T}\Pi v)^{1/2}\) and \(\|v\|_{\Pi^{-1}}=(v^{T}\Pi^{-1}v)^{1/2}\) are norms on \(\mathbb{R}^{d}\).

**Theorem 3** (Variational Formulation of the performative Risk).: _Under assumptions 2 to 4, for linearly parameterized classifiers \(f_{\theta}(x)=x^{T}\theta\) and linear shift operators \(\Pi(\theta)=\Pi\theta\), and assuming that \(\Phi\) is a convex non increasing function and that \(\Pi\) is symmetric positive definite, the performative risk may be rewritten as_

\[\mathrm{PR}(\theta)=\rho\mathbb{E}[\Phi(U_{1}^{T}\theta)]+(1-\rho)\mathbb{E} \left[\max_{\{\Delta U_{0}:\|\Delta U_{0}\|_{\Pi^{-1}}\leq\|\theta\|\ln\}}\Phi (-(U_{0}+\Delta U_{0})^{T}\theta)\right]. \tag{6}\]

Intuitively, for a classification-calibrated loss function [1, 24] and classes with identical covariances, we expect \(\theta\) to align with the direction of \(\mu_{1}(\theta)-\mu_{0}(\theta)\), so that, when \(\Pi\) is positive definite, the performative shift \(\Pi\theta\) has itself a positive dot product with \(\mu_{1}(\theta)-\mu_{0}(\theta)\). The reformulation of the performative risk in eq. (6) formalizes this intuition by showing that the performative optimum is associated to an adversarially robust classification task [1, 24, 25] in which the points of class \(0\) are allowed to shift towards those of class \(1\), so as to increase the overall loss. Compared to objectives found in the robust

Figure 1: Profile risk for classifying two Gaussian centered in \(\mu_{0}=(0,0)\) and \(\mu_{1}=(-1,1)\) with quadratic loss and various values of \(\lambda\) for the diagonal coefficients of \(\Pi\). The performative risk remains convex as long as \(\Pi\) is positive semidefinite i.e. \(\lambda\geq 0\), and becomes non-convex whenever some of the \(\lambda_{i}\) are negative.

classification literature, the specificity of eq. (6) lies in the fact that the tolerance (or budget) on the adversarial displacement \(\Delta U_{0}\) depends on both \(\Pi\) and \(\theta\).

To understand the role played by the \(\|\cdot\|_{\Pi}\) and \(\|\cdot\|_{\Pi-1}\) norms, consider the particular case where only a subset of the variables have a performative effect, i.e., if we let \(\theta\) and \(U_{0}\) be partitioned into

\[\theta=\begin{pmatrix}\theta_{p}\\ \ldots\\ \theta_{s}\end{pmatrix}\text{ and }U_{0}=\begin{pmatrix}U_{0,p}\\ \ldots\\ U_{0,s}\end{pmatrix},\]

with \(\Pi_{p}=\gamma I\) and \(\Pi_{s}=\epsilon I\), one obtains, letting \(\epsilon\) tend to zero, that the performative risk is equal to

\[\operatorname{PR}(\theta)=\rho\mathbb{E}[\Phi(U_{1}^{T}\theta)]+(1-\rho) \mathbb{E}\left[\max_{\{\Delta U_{0,p}:\|\Delta U_{0,p}\|\leq\gamma\|\theta_{p }\|\}}\Phi\left(-(U_{0,s}^{T}\theta_{s}+(U_{0,p}+\Delta U_{0,p})^{T}\theta_{p })\right)\right].\]

The above expression shows that in this case, only the coordinates subject to the performative effect appear in the adversarial reformulation.

In the proof of Theorem 3 (see appendix A.3), we observe that the second term of eq. (6) may also be rewritten as \((1-\rho)\mathbb{E}[\Phi(-U_{0}^{T}\theta-\|\theta\|_{\Pi}^{2})]\). Similarly to the case studied by Ribeiro et al. (2023), the term \(\|\theta\|_{\Pi}^{2}\) that appears inside the surrogate loss function \(\Phi\) has a regularization effect. Note however that it is not equivalent to the use of a standard ridge regression penalty on \(\theta\). The following theorem provides a bound on the performative optimum that highlights the role played by \(\Pi\) on the significance of this regularization effect.

**Theorem 4** (Regularization Bound).: _Define \(\mu_{i}=\mathbb{E}[U_{i}]\). Under assumptions 2 to 4, for linearly parameterized classifiers \(f_{\theta}(x)=x^{T}\theta\) and linear drift operators \(\Pi(\theta)=\Pi\theta\), when \(\Phi\) is a convex non increasing function and \(\Pi\) a symmetric positive definite matrix, the minimizer \(\theta^{*}\) of \(\operatorname{PR}(\theta)\) satisfies the following condition._

\[\|\theta^{*}\|_{\Pi}\leq\frac{\|\Pi^{-\frac{1}{2}}(\rho\mu_{1}-(1-\rho)\mu_{0} )\|}{1-\rho}. \tag{7}\]

Theorem 4 shows that the performative optimum has a smaller value in \(\|\cdot\|_{\Pi}\) norm when the performative effect is stronger, that is, when \(\Pi\) gets larger. In the particular case where \(\Pi=\gamma I\), eq. (7) rewrites as \(\gamma^{1/2}\|\theta^{*}\|\leq\gamma^{-1/2}\|\rho\mu_{1}-(1-\rho)\mu_{0}\|/( 1-\rho)\) and thus larger values of \(\gamma\) decrease the r.h.s. while the l.h.s. increases for identical values of \(\|\theta^{*}\|\), showing that \(\|\theta^{*}\|\) has to decrease to zero.

## 6 Experiments

In this section1, we test the performance of our algorithm Reparametrization-based Performative Gradient (RPPerfGD) with respect to existing algorithms. Three baselines were introduced in Perdomo et al. (2020). First, _Repeated Risk Minimization (RRM)_ computes at each step the next \(\theta\) to minimize the non-performative risk, leading to the update rule \(\theta^{t+1}=\arg\min_{\theta^{\prime}}\operatorname{DPR}(\theta^{t},\theta^{ \prime})\). In practice, we found that this algorithm is unstable as soon as performative effects become significant. We thus report separately the results obtained with this algorithm in appendix B.2. A second baseline is _Repeated Gradient Descent (RGD)_, which ignores the performative effect but limits itself to a gradient step towards this minimization \(\theta^{t+1}=\theta^{t}-\eta\;\nabla_{\theta^{\prime}}\operatorname{DPR}{( \theta,\theta^{\prime})}_{|\theta^{\prime}=\theta}\). In numerical experiments, it is often chosen to add a regularizer to the objective function, which is particularly interesting in the context of performative learning, as discussed in section 5. Hence, we report _Regularized Repeated Gradient Descent (RRGD)_, that corresponds to the repeated gradient with a loss function including an additional ridge penalty on \(\theta\), leading to a more conservative behavior that is also more robust to performative effects. Finally, we also compare to the _Score Function Performative Gradient Descent (SFPerfGD)_ which estimates the performative part of the gradient using the \(\hat{G}_{\theta}^{\text{RP}}\) estimator based on the score-function approach (see section 2). This was previously used in small dimensions in Miller et al. (2021).

Footnote 1: The code is available at [https://github.com/totilas/PerfOpti](https://github.com/totilas/PerfOpti)Influence of the performative effectIn fig. 1(b), we illustrate how taking into account the performative effect allows to mitigate regimes with strong distribution shift. We generate two Gaussian distributions, one fixed for class \(1\) and one moving with mean \(\mu(\theta)^{T}=-(1,1)+\gamma\theta^{T}\operatorname{diag}(0.1,0.9)\), where \(\gamma\) is the magnitude parameter effect. We learn a logistic regression and report the accuracy of the predictions as well as the trajectory of the parameter in \((\theta_{1},\theta_{2})\)-space. As expected, when there is no performative effect (\(\gamma=0\)), all methods are equivalent. As soon as there are performative effects, Performative gradient takes advantage over other methods. RRGD proposes an interesting tradeoff in terms of performance: agnostic to the performative effect, it still moderates its value and thus the magnitude of the performative effect.

Stability of the estimatorIn fig. 1(c), we illustrate the result of example 2, by training a classifier with the square loss, showing that the score-based estimator used in SFPerfGD becomes unstable in high dimensions. We use Gaussian distributions of dimension \(7\) with two dimensions subject to performative effects. We vary the variance of the distributions. When the scale \(\sigma\) is small, the variance of the estimator increases to the point of making learning impossible with unstable trajectories of the parameter \(\theta\). Even when the scale is small enough to ensure convergence, RPPerfGD provides faster convergence illustrating its better scalability for high dimensions.

Estimation of \(\Pi\)We estimate \(\Pi\) in fig. 1(d) and fig. 1(e) by running a ridge regression along the successive deployments of the model as described in Algorithm 1: the ridge penalty ensures that initially the estimate of \(\Pi\) is close to zero making the RPPerfGD updates very similar to those of

Figure 2: **(a) Logistic regression to classify two Gaussian distributions centered in \((0,0)\) and \((-1,-1)\) and different magnitudes of performative effects \(\gamma\). We report the accuracy for three different magnitudes of the performative effects, from no performative effect (\(\gamma=0\)) to a strong one (\(\gamma=1\)). (b) we report the position of the parameter \(\theta\) in its 2D-space, starting from \((0,0)\) and following different paths depending on the algorithm. (c) Accuracy of a classification with quadratic loss on two Gaussian distributions of dimension \(7\) with various levels of variance \(\sigma\) of the distributions. (d) Same experiments but using the learnt \(\Pi\) for RPPerfGD. (e) In this case, distance between the true matrix \(\Pi\) and the estimated version. Note that in RGD and RRGD the estimation of \(\Pi\) is not used in the algorithm. (f) Logistic regression for the Housing dataset with various magnitude of performative shift \(\lambda\) on the coordinates \(0\), \(4\) and \(6\). Accuracy is averaged over \(20\) runs.**

RGD and it is easy to check that order \(d\) deployments are enough to obtain a non void estimate of \(\Pi\). While this plug-in approach is not guaranteed to converge from a theoretical standpoint, we observe results that very similar to the case where \(\Pi\) is fully known.

Houses price predictionTo simulative performative effects from a dataset, we follow the methodology of Perdomo et al. (2020), by shifting the coordinate \(i\) of a factor \(\lambda\theta_{i}\) if the \(i\)-th coordinate could be easily modified, and keeping its real value intact otherwise. We use the binarized version of the Housing dataset2, where the outcome is whether the price is high or not. Assuming that a seller wants to obtain a high price, the high price is the favored class. Some characteristics are harder to tamper with such as the location or the income, whereas other can be slightly adjusted such as the household and the number of bedrooms (a room could be promoted bedroom). Coordinates \(0\), \(4\) and \(6\) are thus shifted while other remains identical. We see that when the magnitude of the shift increases, RPPerfGD outperforms RGD. In particular, it seems that RPPerfGD succeeds in converging faster than the non performative approach.

Footnote 2: [https://www.openml.org/d/823](https://www.openml.org/d/823)

```
Input : Stepsize \(\eta\), regularizer \(\lambda\), starting \(\theta_{0}\), Loss \(\ell\) Output :Parameters \(\theta_{K}\) and diagonal matrix \(\Pi_{K}\)
1\(\Pi_{0}\gets 0_{d\times d}\)// initialize \(\Pi\) as a zero matrix of size \(d\times d\)
2for\(k\in\{0,\ldots,K-1\}\)do
3 Receive \(n\) samples \(\{x^{i}_{k}\}_{i=1}^{n}\sim D(\theta_{k})\) with \(n_{0}\) samples of label \(-1\) denoted \((x_{0,k})_{k}\)
4 Compute \(\nabla_{1}\leftarrow\frac{1}{n}\sum_{i=1}^{n}\nabla_{\theta}\ell(x^{i}_{k}, \theta_{k})\)// Non performative part of the gradient
5 Compute \(\nabla_{2}\leftarrow\frac{1}{n}\Pi_{k}^{\top}\sum_{i=1}^{n_{0}}\nabla_{x}\ell (x^{i}_{0,k},\theta_{k})\)// Performative part of the gradient over negative samples
6\(\theta_{k+1}\leftarrow\theta_{k}-\eta(\nabla_{1}+\nabla_{2})\)// Gradient Descent step
7\(\Pi_{k+1}\leftarrow\operatorname*{argmin}\sum_{j=1}^{k}\sum_{l=1}^{n_{0}}\|x^ {l}_{0,j}-\hat{\mu}-\Pi\theta_{j}\|^{2}+\lambda\|\Pi\|^{2}\)// \(\hat{\mu}\) is the estimated mean of the class
```

**Algorithm 1**RPPerfGD with \(\Pi\) learning

## 7 Conclusion

In this work, we have investigated the consequences of assuming a novel, more explicit, model for performative effects under the form of a push-forward shift of distribution. We have demonstrated that it comes with practically important consequences, such as enabling more reliable performative gradient estimation in large dimensional models.

In the classification case, we observed that when the change of distribution is given by a linear-in-parameters shift, the performative risk is convex under relatively general assumptions. It would be interesting to study how these results may extend to non-linear models for the performative effect.

Finally, we have shown that certain kinds of performative effect induce implicit regularization of the risk minimization problem. Moreover, this regularization effect can alternatively be viewed through the lens of adversarial robustness. It would be useful to explore whether this reformulation can be used to optimize the performative risk without an explicit model for the performative effect.

## 8 Acknowledgments

This work was supported by grants ANR-20-THIA-0014 program "AI PhD@Lille" and ANR-22-PESN-0014 under the France 2030 program. The authors thank Francis Bach, Bruno Loureiro and Kamelia Daudel for their helpful insights.

## References

* Bach (2024) Francis Bach. _Learning Theory from First Principles_. MIT Press, 2024.
* Bach et al. (2018)Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk bounds. _Journal of the American Statistical Association_, 473(101), 2006.
* Drusvyatskiy and Xiao (2023) Dmitriy Drusvyatskiy and Lin Xiao. Stochastic optimization with decision-dependent distributions. _Mathematics of Operations Research_, 48(2), 2023.
* Goodfellow et al. (2015) Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In _International Conference on Learning Representations_, 2015.
* Greensmith et al. (2004) Evan Greensmith, Peter L. Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. _J. Mach. Learn. Res._, 5, 2004.
* Hardt and Mendler-Dunner (2023) Moritz Hardt and Celestine Mendler-Dunner. Performative prediction: Past and future, 2023. arXiv:2310.16608.
* Hofmann et al. (2008) Thomas Hofmann, Bernhard Scholkopf, and Alexander J Smola. Kernel methods in machine learning. _Annals of Statistics_, 36(3), 2008.
* Izzo et al. (2022) Zachary Izzo, James Zou, and Lexing Ying. How to learn when data gradually reacts to your model. In _International Conference on Artificial Intelligence and Statistics_, 2022.
* Jagadeesan et al. (2022) Meena Jagadeesan, Tijana Zrnic, and Celestine Mendler-Dunner. Regret minimization with performative feedback. In _International Conference on Machine Learning_, 2022.
* Kingma and Welling (2013) Diederik P. Kingma and Max Welling. Auto-encoding variational bayes, 2013. arXiv:1312.6114.
* Kleijnen and Rubinstein (1996) Jack P.C. Kleijnen and Reuven Y. Rubinstein. Optimization and sensitivity analysis of computer simulation models by the score function method. _European Journal of Operational Research_, 88(3), 1996.
* Kobyzev et al. (2021) Ivan Kobyzev, Simon J. D. Prince, and Marcus A. Brubaker. Normalizing flows: An introduction and review of current methods. _IEEE Trans. PAMI_, 43(11), 2021.
* Kucukelbir et al. (2017) Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Automatic differentiation variational inference. _Journal of Machine Learning Research_, 18(14), 2017.
* L'Ecuyer (1991) Pierre L'Ecuyer. An overview of derivative estimation. In _Proceedings of the 23rd conference on Winter simulation_, 1991.
* Madry et al. (2018) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.
* Miller et al. (2021) John Miller, Juan C. Perdomo, and Tijana Zrnic. Outside the echo chamber: Optimizing the performative risk. In _International Conference on Machine Learning_, 2021.
* Mohamed et al. (2020) Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine learning. _Journal of Machine Learning Research_, 21, 2020.
* Morgenstern (1928) Oskar Morgenstern. _Wirtschaftsprognose: Eine Untersuchung ihrer Voraussetzungen und Moglichkeiten_. Springer Vienna, 1928.
* Muth (1961) John F. Muth. Rational expectations and the theory of price movements. _Econometrica: journal of the Econometric Society_, 1961.
* Papamakarios et al. (2021) George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. _J. Mach. Learn. Res._, 22(1), 2021.
* Perdomo et al. (2020) Juan Perdomo, Tijana Zrnic, Celestine Mendler-Dunner, and Moritz Hardt. Performative prediction. In _International Conference on Machine Learning_, 2020.
* Ribeiro et al. (2023) Antonio H. Ribeiro, Dave Zachariah, Francis Bach, and Thomas B. Schon. Regularization properties of adversarially-trained linear regression. In _Conference on Neural Information Processing Systems_, 2023.
* Zezulka and Genin (2023) Sebastian Zezulka and Konstantin Genin. Performativity and Prospective Fairness, October 2023. arXiv:2310.08349.

Supplementary Material

### Proofs for Example 2

Using the notations of Example 2 the score function estimator may we written as \(\Pi^{T}\frac{1}{2\sigma^{2}}G\), where \(G=\|U+a\|^{2}U\) with \(U\sim\mathcal{N}(0,\sigma^{2}I_{d})\) and \(a\in\mathbb{R}^{d}\) is a deterministic vector depending on the parameters \(\theta,\theta^{\prime}\) and the performative effect. To compute the expectation and covariance matrix of \(G\) we will use Isserlis' (or Wick's probability) theorem which state that

1. \(\mathbb{E}[U_{i_{1}}\ldots U_{i_{2m+1}}]=0\), for any \(\{i_{1},\ldots,i_{2m+1}\}\in\{1,\ldots,d\}^{2m+1}\) 2. \[\mathbb{E}[U_{i_{1}}\ldots U_{i_{2m}}]=\sigma^{2m}\sum_{\{j_{1},k_{1}\},\ldots, \{j_{m},k_{m}\}\in\mathcal{P}(\{i_{1},\ldots,i_{2m}\})}\delta_{j_{1}k_{1}} \ldots\delta_{j_{m}k_{m}}\] where \(\mathcal{P}(\{i_{1},\ldots,i_{2m}\})\) denotes all the distinct ways of partitioning \(\{i_{1},\ldots,i_{2m}\}\) into non-overlapping (unordered) pairs and \(\delta\) is the Kronecker delta. It is easily checked that the number of partitions in \(\mathcal{P}(\{i_{1},\ldots,i_{2m}\})\) is equal to \(\binom{2m}{m}\frac{m!}{2^{m}}\) which is also equal to the product of all odd numbers between 1 and \(2m-1\).

For the expectation, \(\mathbb{E}[G]=\mathbb{E}[(\|U\|^{2}+\|a\|^{2}+2a^{T}U)U]=2\mathbb{E}(UU^{T})a =2\sigma^{2}a\), as the expansion of all other terms would involve and odd number of coordinates of \(U\).

Let \(M=\mathbb{E}[GG^{T}]\), we have

\[M_{ij}=\mathbb{E}\left[\sum_{k=1}^{d}(U_{k}+a_{k})^{2}\sum_{l=1} ^{d}(U_{l}+a_{l})^{2}U_{i}U_{j}\right]\\ =\mathbb{E}\left[\sum_{k=1}^{d}\sum_{l=1}^{d}\left(U_{k}^{2}U_{l} ^{2}+U_{k}^{2}a_{l}^{2}+U_{l}^{2}a_{k}^{2}+4a_{l}a_{k}U_{k}U_{l}+a_{k}^{2}a_{l }^{2}\right)U_{i}U_{j}\right]\]

omitting terms in the expansion that involve and odd number of coordinates of \(U\) (which have 0 expectation). Now, we apply Isserlis' theorem to each term in this decomposition, starting with the lowest order (rightmost) ones:

\[\mathbb{E}\left[\sum_{k=1}^{d}\sum_{l=1}^{d}a_{k}^{2}a_{l}^{2}U_{ i}U_{j}\right]=\|a\|^{4}\sigma^{2}\delta_{ij}\] \[\mathbb{E}\left[\sum_{k=1}^{d}\sum_{l=1}^{d}4a_{l}a_{k}U_{k}U_{l} U_{i}U_{j}\right]=4\sigma^{4}\sum_{k=1}^{d}\sum_{l=1}^{d}a_{l}a_{k}(\delta_{ij} \delta_{kl}+\delta_{ik}\delta_{jl}+\delta_{il}\delta_{jk})=4\sigma^{4}(\|a\|^ {2}+2a_{i}a_{j})\delta_{ij}\] \[\mathbb{E}\left[\sum_{k=1}^{d}\sum_{l=1}^{d}U_{k}^{2}U_{i}U_{j}a _{l}^{2}\right]=\sigma^{4}\|a\|^{2}\left(\sum_{k=1}^{d}(\delta_{ij}+2\delta_ {ki}\delta_{kj})\right)=\sigma^{4}\|a\|^{2}(d+2)\delta_{ij}\] \[\mathbb{E}\left[\sum_{k=1}^{d}\sum_{l=1}^{d}U_{k}^{2}U_{l}^{2}U_{ i}U_{j}\right]=\sigma^{6}\left(\delta_{ij}+2\delta_{il}\delta_{jl}+2\delta_{ ik}\delta_{jk}+2\delta_{kl}\delta_{ij}+8\delta_{kl}\delta_{ki}\delta_{ij} \right)=\sigma^{6}(d^{2}+6d+8)\delta_{ij}\]

where the last decomposition is obtained by examination of the \(\binom{6}{3}\frac{3!}{2^{3}}=15\) possible partitions of \(\{k,k,l,l,i,j\}\) in 3 pairs of indices. Putting all together, one obtains

\[M=\left((d^{2}+6d+8)\sigma^{6}+2(d+4)\|a\|^{2}\sigma^{4}+\|a\|^{4}\sigma^{2} \right)I_{d}+8\sigma^{4}aa^{T}\]

which yields

\[\mathrm{Cov}(G)=\left((d^{2}+6d+8)\sigma^{6}+2(d+4)\|a\|^{2}\sigma^{4}+\|a\|^ {4}\sigma^{2}\right)I_{d}+4\sigma^{4}aa^{T} \tag{8}\]

Subtracting a scalar baseline \(m\) yields the estimator \(\tilde{G}=(\|U+a\|^{2}-m)U\) which has the same expectation as \(G\). In terms of covariances, one has

\[\mathrm{Cov}(\tilde{G})=\mathrm{Cov}(G)+m^{2}I_{d}-2m\mathbb{E}\left(\|U+a\|^ {2}UU^{T}\right)\]The rightmost expression, when expanded, features terms have already been met in the computation above and it is easy to check that

\[\mathbb{E}\left(\|U+a\|^{2}UU^{T}\right)=((d+2)\sigma^{4}+\|a\|^{2}\sigma^{2})I_{d}\]

Hence,

\[\operatorname{Cov}(\tilde{G})=\operatorname{Cov}(G)+\left(m^{2}-2m((d+2)\sigma ^{4}+\|a\|^{2}\sigma^{2})\right)I_{d}\]

In the above equation, the scalar term \(m^{2}-2m((d+2)\sigma^{4}+\|a\|^{2}\sigma^{2})\) is minimized by choosing \(m=(d+2)\sigma^{2}+\|a\|^{2}\) and is equal to \(-\left((d+2)\sigma^{2}+\|a\|^{2}\right)^{2}\sigma^{2}\), which, combined with eq. (8) yields

\[\operatorname{Cov}(\tilde{G})\geq\left(2(d+2)\sigma^{6}+4\|a\|^{2}\sigma^{4} \right)I_{d}+4\sigma^{4}aa^{T} \tag{9}\]

### Proof of Theorem 2

Proof.: For (a), eq. (4) may be rewritten as

\[\operatorname{PR}(\theta)=\rho\mathbb{E}[(U_{1}^{T}\theta-1)^{2}]+(1-\rho) \mathbb{E}[((U_{0}+\Pi\theta)^{T}\theta+1)^{2}]\]

Denoting \(\mathbb{E}(U_{i})=\mu_{i}\) and \(\operatorname{Cov}_{\theta}(U_{i})=\Sigma_{i}\), for \(i\in\{0,1\}\), one has

\[\operatorname{PR}(\theta)=\rho\mathbb{E}[((U_{1}-\mu_{1})^{T} \theta-(1-\mu_{1}^{T}\theta))^{2}]+(1-\rho)\mathbb{E}[((U_{0}-\mu_{0})^{T} \theta+(\mu_{0}+\Pi\theta)^{T}\theta+1)^{2}]\] \[=\rho[\|\theta\|_{\Sigma_{1}}^{2}+(1-\mu_{1}^{T}\theta)^{2}]+(1- \rho)[\|\theta\|_{\Sigma_{0}}^{2}+((\mu_{0}+\Pi\theta)^{T}\theta+1)^{2}]\]

Both squared norms are convex as well as the squares of, respectively, an affine function and a convex second order polynomial (as \(\Pi\) is positive semidefinite).

For (b), examining

\[\operatorname{PR}(\theta)=\rho\mathbb{E}[\Phi(U_{1}^{T}\theta)]+(1-\rho) \mathbb{E}[\Phi(-(U_{0}+\Pi\theta)^{T}\theta)] \tag{10}\]

one observes that

* \(\Phi(u_{1}^{T}\theta)\) is convex by our assumption on \(\Phi\) (for any value of \(u_{1}\));
* \((u_{0}+\Pi\theta)^{T}\theta\) is a convex second order (multivariate) polynomial in \(\theta\) when \(\Pi\) is positive semidefinite and \(v\mapsto\Phi(-v)\) is convex non decreasing, hence \(\Phi(-(u_{0}+\Pi\theta)^{T}\theta)\) is also convex.

Thus \(\operatorname{PR}(\theta)\) is also convex in \(\theta\) as the expectation of convex functions. 

### Proof of Theorem 3

Proof.: To obtain eq. (6), as \(v\mapsto\Phi(-v)\) is non decreasing, one has

\[\max_{\{\Delta u_{0}:\|\Delta u_{0}\|_{\Pi^{-1}}\leq\|\theta\|_{\Pi}\}}\Phi \left(-(u_{0}+\Delta u_{0})^{T}\theta\right)=\Phi\left(-u_{0}^{T}\theta-\max _{\{\Delta u_{0}:\|\Delta u_{0}\|_{\Pi^{-1}}\leq\|\theta\|_{\Pi}\}}(\Delta u_ {0})^{T}\theta\right)\]

for any outcome \(u_{0}\) of the random variable \(U_{0}\). The maximization occurs for \(\Delta u_{0}=\Pi\theta\), which does not depend on \(u_{0}\), leading to \((\Delta u_{0})^{T}\theta=\|\theta\|_{\Pi}^{2}\) and thus

\[\max_{\{\Delta U_{0}:\|\Delta U_{0}\|_{\Pi^{-1}}\leq\|\theta\|_{\Pi}\}}\Phi \left(-(U_{0}+\Delta U_{0})^{T}\theta\right)=\Phi\left(-U_{0}^{T}\theta-\| \theta\|_{\Pi}^{2}\right)\]

whose expectation is recognized as the second term of eq. (10). 

### Proof of Theorem 4

Proof.: Recall from the proof of Theorem 2 that the performative risk can be rewritten as follows.

\[\operatorname{PR}(\theta) =\rho\mathbb{E}[\Phi(U_{1}^{T}\theta)]+(1-\rho)\mathbb{E}[\Phi(- (U_{0}+\Pi\theta)^{T}\theta)]\] \[=\rho\mathbb{E}[\Phi(U_{1}^{T}\theta)]+(1-\rho)\mathbb{E}[\Phi(- U_{0}^{T}\theta-\|\theta\|_{\Pi}^{2})]\]Let \(\mu_{\rho}=\rho\mu_{1}-(1-\rho)\mu_{o}\). We have,

\[\Phi(0)=\text{PR}(0) \geq\text{PR}(\theta^{*})\] \[=\rho\mathbb{E}[\Phi(U_{1}^{T}\theta^{*})]+(1-\rho)\mathbb{E}[\Phi (-U_{0}^{T}\theta^{*}-\|\theta^{*}\|_{\Pi}^{2})]\] \[\geq\rho\Phi(\mathbb{E}[U_{1}^{T}\theta^{*}])+(1-\rho)\Phi( \mathbb{E}[-U_{0}^{T}\theta^{*}-\|\theta^{*}\|_{\Pi}^{2}])\] \[=\rho\Phi(\mu_{1}^{T}\theta^{*})+(1-\rho)\Phi(-\mu_{0}^{T}\theta^ {*}-\|\theta^{*}\|_{\Pi}^{2})\] \[\geq\Phi\left(\rho\mu_{1}^{T}\theta^{*}-(1-\rho)(\mu_{0}^{T} \theta^{*}+\|\theta^{*}\|_{\Pi}^{2})\right)\] \[=\Phi\left(\mu_{\rho}^{T}\theta^{*}-(1-\rho)\|\theta^{*}\|_{\Pi}^ {2}\right)\]

where we have successively used Jensen's inequality and the convexity of \(\Phi\). Since \(\Phi\) is non-increasing, we must have \(0\leq\mu_{\rho}^{T}\theta^{*}-(1-\rho)\|\theta^{*}\|_{\Pi}^{2}\). Denoting \(\beta=\Pi^{\frac{1}{2}}\theta^{*}\), it holds that

\[(1-\rho)\|\beta\|^{2}\leq\mu_{\rho}^{T}\Pi^{-\frac{1}{2}}\beta\leq\|\Pi^{- \frac{1}{2}}\mu_{\rho}\|\|\beta\|\]

where that last inequality is obtained using Cauchy-Schwarz. Hence, \(\|\theta^{*}\|_{\Pi}=\|\beta\|\leq\frac{\|\mu_{\rho}^{T}\Pi^{-\frac{1}{2}}\|}{ 1-\rho}\). 

## Appendix B Numerical Experiments

### Full parameters list

In this section, we report all the parameters needed to reproduce the figures in the paper. Note that we always use the same step size for all the methods. This choice stems from the fact that the methods are equivalent (up to the regularization parameter) when there is no performative effect.

### Repeated Risk Minimization

In this section, we report the same learning tasks as those reported in the main text, but for Repeated Risk Minimization (RRM). In every setting, as soon as the performative effect is not negligible, the technique diverges. To ensure the readability of the figure and avoid shrinking the differences between the other algorithms, we report it separately.

\begin{table}
\begin{tabular}{l l} \hline \hline
**Parameter** & **Value** \\ \hline Number of iterations (num\_iter) & 100 \\ Sample size (\(n\)) & 1000 \\ Scale (\(\sigma\)) & 0.5 \\ Average number of iterations (num\_iter\_average) & 100 \\ Step size (step\_size) & 0.1 \\ Regularization parameter (\(\lambda\)) & \(3\times 10^{-2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Parameters used for figure fig. 1(b)

\begin{table}
\begin{tabular}{l l} \hline \hline
**Parameter** & **Value** \\ \hline Number of iterations (num\_iter) & 25 \\ Sample size (\(n\)) & 1000 \\ Initial scale (scale\({}_{0}\)) & 0.5 \\ Transition probability matrix (\(\Pi\)) & \(\text{diag}([0.1,3,0,0,0,0,0])\) \\ Mean of class \(0\) (\(\mu\)) & \([1\ 2\ \ \ 0.5\ \ \ 0.5\ \ 0\ \ 0\ \ 0]\) \\ Average number of iterations (num\_iter\_average) & 100 \\ Step size (step\_size) & 0.1 \\ Regularization parameter (\(\lambda\)) & \(10^{-1}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Parameters used for figure fig. 1(c)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We define our approach with pushforward measure in section 2, the application to classification inspection 3, the new theorems on convexity in section 4 and the relation with robustness in section 5
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss our model as pushforward measures in section 2 and we discuss our theorems to ensure convexity in section 4. For the experiments, we acknowledge the lack of true performative datasets and use the same simulations approximation as in previous work, as described in section 6.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All theorems are defined with their full set of assumptions, with proofs just after them or in the supplementary material (appendix A).
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]

\begin{table}
\begin{tabular}{l l} \hline \hline
**Parameter** & **Value** \\ \hline Number of iterations (num\_iter) & 15 \\ Sample size (\(n\)) & 18000 \\ Number of runs (n\_runs) & 20 \\ Step size (step\_size) & 0.2 \\ Regularization parameter (\(\lambda\)) & \(5\times 10^{-3}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Parameters used for fig. 2f

Figure 3: (a) Learning a logistic regression between two Gaussian distributions centered in \((0,0)\) and \((-1,1)\) and different magnitude of performative effects \(\gamma\). (b) Accuracy of a classification with quadratic loss on two Gaussian of dimension \(7\) with various level of noise \(\sigma\)Justification: The experiments are described in section 6. Additional choice of parameters are reported in appendix A.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: the code will be publicly released after publication
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The detailed parameters are in Appendix (appendix B). The choice of parameters are quite limited as the studied models have very simple architecture
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All figures report standard deviation over the runs, as stated in legend.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The experiments are only using small datasets so all runs were done on a single laptop where the time of execution is very small (few seconds per run).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The paper is conform to the NeurIPS code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper is theoretical and should not have directly any societal impacts. In fact, raising the attention on the feedback loop that might occurs in performative learning could lead to a positive societal impact, if better understood.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes]

Justification: We only reuse one dataset (Housing) and we cite it.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.