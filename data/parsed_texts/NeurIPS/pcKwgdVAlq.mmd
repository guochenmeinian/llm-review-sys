# Binarized Spectral Compressive Imaging

 Yuanhao Cai \({}^{1}\), Yuxin Zheng \({}^{1}\), Jing Lin \({}^{1}\),

**Xin Yuan**\({}^{2}\), **Yulun Zhang**\({}^{3,}\)*, **Haoqian Wang**\({}^{1,}\)*

\({}^{1}\) Tsinghua University, \({}^{2}\) Westlake University, \({}^{3}\) ETH Zurich

Yulun Zhang and Haoqian Wang are the corresponding authors.

###### Abstract

Existing deep learning models for hyperspectral image (HSI) reconstruction achieve good performance but require powerful hardwares with enormous memory and computational resources. Consequently, these methods can hardly be deployed on resource-limited mobile devices. In this paper, we propose a novel method, Binarized Spectral-Redistribution Network (BiSRNet), for efficient and practical HSI restoration from compressed measurement in snapshot compressive imaging (SCI) systems. Firstly, we redesign a compact and easy-to-deploy base model to be binarized. Then we present the basic unit, Binarized Spectral-Redistribution Convolution (BiSR-Conv). BiSR-Conv can adaptively redistribute the HSI representations before binarizing activation and uses a scalable hyperbolic tangent function to closer approximate the Sign function in backpropagation. Based on our BiSR-Conv, we customize four binarized convolutional modules to address the dimension mismatch and propagate full-precision information throughout the whole network. Finally, our BiSRNet is derived by using the proposed techniques to binarize the base model. Comprehensive quantitative and qualitative experiments manifest that our proposed BiSRNet outperforms state-of-the-art binarization algorithms. Code and models are publicly available at https://github.com/caiyuanhao1998/BiSCI

## 1 Introduction

Compared to normal RGB images, hyperspectral images (HSIs) have more spectral bands to capture richer information of the desired scenes. Thus, HSIs have wide applications in agriculture [1; 2; 3], medical image analysis [4; 5; 6], object tracking [7; 8; 9], remote sensing [10; 11; 12], _etc._

To capture HSIs, conventional imaging systems leverage 1D or 2D spectrometers to scan the desired scenes along the spatial or spectral dimension. Yet, this process is very time-consuming and thus fails in measuring dynamic scenes. In recent years, snapshot compressive imaging (SCI) systems [13; 14; 15; 16; 17] have been developed to capture HSI cubes in real time. Among these SCI systems, the coded aperture snapshot spectral imaging (CASSI) [14; 18; 19] demonstrates its outstanding effectiveness and efficiency. The CASSI systems firstly employ a coded aperture (physical mask) to modulate the 3D HSI cube, then use a disperser to shift spectral information of different wavelengths, and finally integrate these HSI signals on a detector array to capture a 2D compressed measurement. We study the inverse problem, _i.e._, restoring the original 3D HSI cube from the 2D measurement.

Existing state-of-the-art (SOTA) SCI reconstruction methods are based on deep learning. Convolutional neural network (CNN) [18; 20; 21; 22; 23; 24; 25] and Transformer [26; 27; 28; 29] have been used to implicitly learn the mapping from compressed measurements to HSIs. Although superior performance is achieved, these CNN-/Transformer-based methods require powerful hardwares with abundant computing and memory resources, such as high-end graphics processing units (GPUs). However, edge devices (_e.g.,_ mobile phones, hand-held cameras, small drones, _etc._) evidently cannot meet the requirements of these expensive algorithms because edge devices have very limited memory, computational power, and battery. As mobile devices are more and more widely used, the demands of running and storing HSI restoration models on edge devices grow significantly. This motivates us to reduce the memory and computational burden of HSI reconstruction methods while preserving the performance as much as possible so that the algorithms can be deployed on resource-limited devices.

The studies on neural network compression and acceleration [37; 38] can be divided into four categories: quantization [31; 32; 33; 34; 35; 39], pruning [40; 41; 42], knowledge distillation [43; 44], and compact network design [45; 46; 47; 48]. Among these methods, binarized neural network (BNN) belonging to quantization stands out because it can extremely compress the memory and computational costs by quantizing the weights and activations of CNN to only 1 bit. In particular, BNN could achieve 32\(\times\) memory compression ratio and up to 58\(\times\) practical computational reduction on central processing units (CPUs) [49]. In addition, the pure logical computation (_i.e._, XNOR and bit-count operations) of BNN is highly energy-efficient for embedded devices [50; 51]. However, directly applying model binarization for HSI reconstruction algorithms may encounter three issues. **(i)** The HSI representations have different density and distribution in different spectral bands. Equally binarizing the activations of different spectral channels may lead to the collapse of HSI features. **(ii)** Previous model binarization methods mainly adopt a piecewise linear [33; 35; 36] or quadratic [31; 32; 34] function to approximate the non-differentiable Sign function. Nonetheless, there still remain large approximation errors between them and Sign. **(iii)** How to tackle the dimension mismatch problem during feature reshaping while allowing full-precision information propagation in BNN has not been fully explored. Previous model binarization methods [32; 33; 34; 35; 36] mainly consider the feature downsampling situation in a backbone network for high-level vision tasks.

Bearing the above considerations in mind, we propose a novel BNN-based method, namely Binarized Spectral-Redistribution Network (BiSRNet) for efficient and practical HSI reconstruction. **Firstly**, we redesign a compact and easy-to-deploy base model to be binarized. Different from previous CNN-/Transformer-based methods, this base model does not include complex computations like unfolding inference and non-local self-attention that are difficult to implement on edge devices. Instead, our base model only uses convolutional units that can be easily replaced by XNOR and bit-count logical operations on resource-limited devices. **Secondly**, we develop the basic unit, Binarized Spectral-Redistribution Convolution (BiSR-Conv), used in model binarization. Specifically, BiSR-Conv can adapt the density and distribution of HSI representations in spectral dimension before binarizing the activation. Besides, BiSR-Conv employs a scalable hyperbolic tangent function to closer approximate the non-differentiable Sign function by arbitrarily reducing the approximation error. **Thirdly**, as the full-precision information is very critical in BNN and the input HSI is the only full-precision source, we use BiSR-Conv to build up four binarized convolutional modules that can handle the dimension mismatch issue during feature reshaping and simultaneously propagate full-precision information through all layers. **Finally**, we derive our BiSRNet by using the proposed techniques to binarize the base model. As shown in Fig. 1, BiSRNet outperforms SOTA BNNs by large margins, **over 2.5 dB**.

In a nutshell, our contributions can be summarized as follows:

**(i)** We propose a novel BNN-based algorithm BiSRNet for HSI reconstruction. To the best of our knowledge, this is the first work to study the binarized spectral compressive imaging problem.

**(ii)** We customize a new binarized convolution unit BiSR-Conv that can adapt the density and distribution of HSI representations and approximate the Sign function better in backpropagation.

**(iii)** We design four binarized convolutional modules to address the dimension mismatch issue during feature reshaping and propagate full-precision information through all convolutional layers.

**(iv)** Our BiSRNet dramatically surpasses SOTA BNNs and even achieves comparable performance with full-precision CNNs while requiring extremely lower memory and computational costs.

Figure 1: Comparison between our BiSRNet (in red color) and state-of-the-art BNNs (in blue color). BiSRNet significantly advances BTM [30], BBCU [31], ReActNet [32], IRNet [33], Bi-Real [34], BNN [35], and BiConnect [36] by 2.55, 3.25, 3.39, 3.46, 3.50, 5.88, and 7.57 dB on the simulation HSI reconstruction task.

## 2 Related Work

### Hyperspectral Image Reconstruction

Traditional HSI reconstruction methods [14; 52; 53; 54; 55; 56; 57; 58; 59; 60; 61] are mainly based on hand-crafted image priors. Yet, these traditional methods achieve unsatisfactory performance and generality due to their poor representing capacity. Recently, deep CNN [18; 20; 21; 22; 23; 24; 25; 62] and Transformer [26; 27; 28; 29] have been employed as powerful models to learn the underlying mapping from compressed measurements to HSI data cubes. For example, TSA-Net [18] employs three spatial-spectral self-attention layers at the decoder of a U-shaped CNN. Cai _et al._ propose a series of Transformer-based algorithms (MST [26], MST++ [27], CST [28], and DAUHST [29]), pushing the performance boundary from 32 dB to 38 dB. Although impressive results are achieved, these CNN-/Transformer-based methods rely on powerful hardwares with enormous computational and memory resources, which are unaffordable for mobile devices. How to develop HSI restoration algorithms toward resource-limited platforms is under-explored. Our goal is to fill this research gap.

### Binarized Neural Network

BNN [35] is the extreme case of model quantization as it quantizes the weights and activations into only 1 bit. Due to its impressive effectiveness in memory and computation compression, BNN has been widely applied in high-level vision [32; 33; 34; 35; 49] and low-level vision [30; 31; 39]. For example, Jiang _et al._[30] train a BNN without batch normalization for image super-resolution. Xia _et al._[31] design a binarized convolution unit BBCU for image super-resolution, denoising, and JPEG compression artifact reduction. Yet, the potential of BNN for SCI reconstruction has not been studied.

## 3 Method

### Base Model

The full-precision model to be binarized should be compact and its computation should be easy to deploy on edge devices. However, previous CNN-/Transformer-based algorithms are computationally expensive or have large model sizes. Some of them exploit complex operations like unfolding inference [21; 22; 23; 29; 63] and non-local self-attention computation [18; 20; 26; 27; 28] that are challenging to binarize and difficult to implement on mobile devices. Hence, we redesign a simple, compact, and easy-to-deploy base model without using complex computation operations.

Inspired by the success of MST [26] and CST [28], we adopt a U-shaped structure for the base model as shown in Fig. 2. It consists of an encoder \(\mathcal{E}\), a bottleneck \(\mathcal{B}\), and a decoder \(\mathcal{D}\). Please refer to the supplementary for the CASSI mathematical model. Firstly, we reverse the dispersion of CASSI by shifting back the measurement \(\mathbf{Y}\in\mathbb{R}^{H\times(W+d(N_{\lambda}-1))\times N_{\lambda}}\) to derive the input \(\mathbf{H}\in\mathbb{R}^{H\times W\times N_{\lambda}}\) as

\[\mathbf{H}(x,y,n_{\lambda})=\mathbf{Y}(x,y-d(\lambda_{n}-\lambda_{c})),\] (1)

where \(H\), \(W\), and \(N_{\lambda}\) denote the HSI's height, width, and number of wavelengths. \(d\) represents the shifting step. The concatenation of \(\mathbf{H}\) and the 3D mask \(\mathbf{M}\in\mathbb{R}^{H\times W\times N_{\lambda}}\) is fed into a feature embedding module to produce the shallow feature \(\mathbf{X}_{s}\in\mathbb{R}^{H\times W\times N_{\lambda}}\). The feature embedding module is a \(conv1\times 1\) (convolutional layer with kernel size = 1\(\times\)1). Subsequently, \(\mathbf{X}_{s}\) undergoes the encoder \(\mathcal{E}\), bottleneck \(\mathcal{B}\), and decoder \(\mathcal{D}\) to generate the deep feature \(\mathbf{X}_{d}\in\mathbb{R}^{H\times W\times N_{\lambda}}\). \(\mathcal{E}\) consists of two convolutional blocks and two downsample modules. The details of the convolutional block are depicted in Fig. 2 (b). The fusion up and down modules are both \(conv1\times 1\) to aggregate the feature maps and modify the channels. The downsample module is a strided \(conv4\times 4\) layer that downscales the feature maps and doubles the channels. \(\mathcal{B}\) is a convolutional block. \(\mathcal{D}\) consists of two convolutional blocks and two upsample modules. The upsample module is a bilinear interpolation followed by a \(conv3\times 3\) to upscale the feature maps and halve the channels. Skip connections between \(\mathcal{E}\) and \(\mathcal{D}\) are employed to alleviate the information loss during rescaling. Finally, the sum of \(\mathbf{X}_{s}\) and \(\mathbf{X}_{d}\) is fed into the feature mapping module (\(conv1\times 1\)) to produce the reconstructed HSI \(\mathbf{H}^{\prime}\in\mathbb{R}^{H\times W\times N_{\lambda}}\).

### Binarized Spectral-Redistribution Convolution

The details of BiSR-Conv are illustrated in Fig. 2 (c). We define the input full-precision activation as \(\mathbf{X}_{f}\in\mathbb{R}^{H\times W\times C}\). We notice that HSI signals have different density and distribution along the spectral dimension due to the constraints of specific wavelengths. To adaptively fit this HSI nature, we propose to redistribute the HSI representations in channel wise before binarizing the activation as

\[\mathbf{X}_{r}=\bm{k}\cdot\mathbf{X}_{f}+\bm{b},\] (2)where \(\mathbf{X}_{r}\in\mathbb{R}^{H\times W\times C}\) denotes the redistributed activation of \(\mathbf{X}_{f}\). \(\bm{k}\) and \(\bm{b}\in\mathbb{R}^{C}\) are learnable parameters. \(\bm{k}\) rescales the density of HSIs while \(\bm{b}\) shifts the bias. Then \(\mathbf{X}_{r}\) undergoes a Sign function to be binarized into 1-bit activation \(\mathbf{X}_{b}\in\mathbb{R}^{H\times W\times C}\), where \(x_{b}=+1\) or \(-1\) for \(\forall\;x_{b}\in\mathbf{X}_{b}\) as

\[x_{b}=\text{Sign}(x_{r})=\left\{\begin{aligned} &+1,\quad x_{r}>0\\ &-1,\quad x_{r}\leq 0\end{aligned}\right.\] (3)

where \(x_{r}\in\mathbf{X}_{r}\). As shown in Fig. 3 (b) and (c), since the Sign function is non-differentiable, previous methods either adopt a piecewise linear function Clip(\(x\)) [33, 35, 36, 49, 64] or a piecewise quadratic function Quad(\(x\)) [31, 32, 34] to approximate the Sign function during the backpropagation as

\[\text{Clip}(x)=\left\{\begin{aligned} &+1,\quad\quad\quad\quad x\geq 1\\ & x,\quad\quad-1<x<1\\ &-1,\quad\quad\quad x\leq-1\end{aligned}\right.\quad\text{Quad}(x)=\left\{ \begin{aligned} &+1,\quad\quad\quad\quad x\geq 1\\ & 2x+x^{2},\quad\quad\quad 0<x<1\\ & 2x-x^{2},\quad\quad-1<x\leq 0\\ &-1,\quad\quad\quad\quad x\leq-1\end{aligned}\right.\] (4)

Nonetheless, the Clip function is a rough estimation and there is a large approximation error between Clip and Sign. The shaded areas in Fig. 3 reflect the differences between the Sign function and its approximations. The shaded area corresponding to the Clip function is 1. Besides, once the absolute values of weights or activations are outside the range of \([-1,1]\), they are no longer updated. Although the piecewise quadratic function is a closer approximation (the shaded area is 2/3) than Clip, the above two problems have not been fundamentally resolved. To address the two issues, we redesign a scalable hyperbolic tangent function to approximate the Sign function in the backpropagation as

\[x_{b}=\text{Tanh}(\alpha x_{r})=\frac{e^{\alpha x_{r}}-e^{-\alpha x_{r}}}{e^{ \alpha x_{r}}+e^{-\alpha x_{r}}},\] (5)

where \(\alpha\in\mathbb{R}^{+}\) is a learnable parameter adaptively adjusting the distance between Tanh(\(\alpha x\)) and Sign(\(x\)). \(e\) denotes the natural constant. We prove that when \(\alpha\rightarrow+\infty\), Tanh(\(\alpha x\)) \(\rightarrow\) Sign(\(x\)) as

\[\underset{\alpha\rightarrow+\infty}{lim}\text{Tanh}(\alpha x)=\left\{ \begin{aligned} &\underset{\alpha\rightarrow+\infty}{lim}\;\frac{e^{\alpha x}-0}{e^{ \alpha x}+0}&=+1,\quad x>0\\ &\underset{\alpha\rightarrow+\infty}{lim}\;\frac{e^{0}-e^{0}}{e^{ 0}+e^{0}}&=&\quad 0,\quad x=0\\ &\underset{\alpha\rightarrow+\infty}{lim}\;\frac{0-e^{-\alpha x}}{0 +e^{-\alpha x}}&=-1,\quad x<0\end{aligned}\right.\] (6)

Figure 2: The overall diagram of our method. (a) The proposed base model to be binarized adopts a U-shaped architecture. (b) The components of the convolutional block. (c) The details of our Binarized Spectral-Redistribution Convolution (BiSR-Conv). (d) The structure of our binarized downsample module. The binarized fusion up module is derived by removing the average pooling operation. (e) The architecture of our binarized upsample module, which includes one more bilinear upscaling operation than the binarized fusion down module.

If strictly following the mathematical definition, \(\text{Sign}(0)=0\neq\pm 1\). However, in BNN, the weights and activations are binarized into 1-bit, _i.e._, only two values (\(\pm 1\)). Hence, \(\text{Sign}(0)\) is usually set to \(-1\). Similar to this common setting, we also define \(\underset{\alpha\rightarrow+\infty}{lim}\text{Tanh}(\alpha\cdot 0)=-1\) in BNN. Then we have

\[\underset{\alpha\rightarrow+\infty}{lim}\text{Tanh}(\alpha x)=\text{Sign}(x).\] (7)

We compute the area of the shaded region between our \(\text{Tanh}(\alpha x)\) and \(\text{Sign}(x)\) in Fig. 3 (d) as

\[\begin{split}\int_{-\infty}^{+\infty}\left|\text{Sign}(x)-\text{ Tanh}(\alpha x)\right|\text{d}x&=2\int_{0}^{+\infty}(1-\text{Tanh}( \alpha x))\text{ d}x\\ &=2(x-x+\frac{1}{\alpha}\text{log}(\text{Tanh}(\alpha x)+1)) \right|_{x=0}^{x=+\infty}\\ &=\frac{2}{\alpha}(\text{log}(2)-\text{log}(1))=\frac{2\text{ log}(2)}{\alpha}.\end{split}\] (8)

Different from previous \(\text{Clip}(x)\) and \(\text{Quad}(x)\), our \(\text{Tanh}(\alpha x)\) can arbitrarily reduce the approximation error with \(\text{Sign}(x)\) when \(\alpha\) in Eq. (8) is large enough. Besides, our \(\text{Tanh}(\alpha x)\) is neither piecewise nor unchanged when \(x\) is outside the range of \([-1,1]\). On the contrary, the weights and activations can still be updated when their absolute values are larger than 1. In addition, as depicted in the lower line of Fig. 3, the value ranges \([0,1]\) and fixed shapes of \(\frac{\partial\text{Clip}(x)}{\partial x}\) and \(\frac{\partial\text{Quad}(x)}{\partial x}\) are fundamentally different from those of \(\frac{\partial\text{Sign}(x)}{\partial x}\in[0,+\infty)\). In contrast, our \(\frac{\partial\text{Tanh}(\alpha x)}{\partial x}\) can change its value range \((0,\alpha)\) and shape by adapting the parameter \(\alpha\). It is more flexible and can approximate \(\frac{\partial\text{Sign}(x)}{\partial x}\) better.

In the binarized convolutional layer, the 32-bit weight \(\mathbf{W}_{f}\) is also binarized into 1-bit weight \(\mathbf{W}_{b}\) as

\[w_{b}=\mathbb{E}_{w_{f}\in\mathbf{W}_{f}}(|w_{f}|)\cdot\text{Sign}(w_{f}),\] (9)

where \(\mathbb{E}\) represents computing the mean value. Multiplying the mean absolute value of 32-bit weight value \(w_{f}\in\mathbf{W}_{f}\) can narrow down the difference between binarized and full-precision weights. Subsequently, the computationally heavy operations of floating-point matrix multiplication in full-precision convolution can be replaced by pure logical XNOR and bit-count operations [49] as

\[\mathbf{Y}_{b}=\mathbf{X}_{b}*\mathbf{W}_{b}=\text{bit-count}(\text{XNOR}( \mathbf{X}_{b},\mathbf{W}_{b})),\] (10)

where \(\mathbf{Y}_{b}\) represents the output and \(*\) denotes the convolution operation. Since the value range of full-precision activation \(\mathbf{X}_{f}\) largely varies from that of 1-bit convolution output \(\mathbf{Y}_{b}\), directly employing an identity mapping to aggregate them may cover up the information of \(\mathbf{Y}_{b}\). To cope with this problem, we first fed \(\mathbf{Y}_{b}\) into a RPReLU [32] activation function to change its value range and then add it with \(\mathbf{X}_{f}\) by a residual connection to propagate full-precision information as

\[\mathbf{X}_{o}=\mathbf{X}_{f}+\text{RPReLU}(\mathbf{Y}_{b}),\] (11)

where \(\mathbf{X}_{o}\) denotes the output feature and RPReLU is formulated for the \(i\)-th channel of \(\mathbf{Y}_{b}\) as

\[\text{RPReLU}(y_{i})=\begin{cases}y_{i}-\gamma_{i}+\zeta_{i},&y_{i}>\gamma_{i} \\ \beta_{i}\cdot(y_{i}-\gamma_{i})+\zeta_{i},&y_{i}\leq\gamma_{i}\end{cases}\] (12)

Figure 3: The upper line shows (a) \(\text{Sign}(x)\) and its three approximation functions including (b) piecewise linear function \(\text{Clip}(x)\), (c) piecewise quadratic function \(\text{Quad}(x)\), and (d) our scalable hyperbolic tangent function \(\text{Tanh}(\alpha x)\). The area of the shaded region reflects the approximation error. The lower line depicts the derivatives.

where \(y_{i}\in\mathbb{R}\) indicates single pixel values belonging to the \(i\)-th channel of \(\mathbf{Y}_{b}\). \(\beta_{i},\gamma_{i},\) and \(\zeta_{i}\in\mathbb{R}\) represents learnable parameters. Please note that the full-precision information is not blocked by the binarized convolutional layer in the proposed BiSR-Conv. Instead, it is propagated by the bypass identity mapping, as shown in the red arrow \(\rightarrow\) in Fig. 2 (c). Based on this important property of BiSR-Conv, we design the four binarized convolutional modules, as illustrated in Fig. 2 (d) and (e).

### Binarized Convolutional Modules

In model binarization, the identity mapping is critical to propagate full-precision information and ease the training procedure. The only source of full-precision information is the input end (\(\mathbf{X}_{s}\) in Fig. 2) of the binarized part. However, the dimension mismatch during the feature downsampling, upsampling, and aggregation processes blocks the residual connections, which degrades the HSI reconstruction performance. To tackle this problem, we use BiSR-Conv to build up four binarized convolutional modules including downsample, upsample, fusion up, and fusion down with unblocked identity mappings to make sure the full-precision information can flow through all binarized layers.

Specifically, Fig. 4 compares the normal and our binarized modules. The red arrow \(\downarrow\) indicates the full-precision information flow, while the blue arrow \(\downarrow\) denotes the binarized signal flow. The downsample modules in Fig. 4 (a) downscale the input feature maps and double the channels. The upsample modules in Fig. 4 (b) upscale the input spatial dimension and half the channels. The fusion down modules in Fig. 4 (c) maintain the spatial size of the input feature and half the channels. The fusion up modules in Fig. 4 (d) keep the spatial dimension of the input feature maps while doubling the channels. In the normal modules, the full-precision information is blocked by the Sign function and binarized into 1-bit signal. Meanwhile, the intermediate feature maps are directly reshaped by the binarized convolutional layers. For example, in the normal binarized downsample module, the intermediate feature is directly reshaped from \(\mathbb{R}^{H\times W\times C}\) to \(\mathbb{R}^{\frac{H}{2}\times\frac{W}{2}\times 2C}\) by a strided 1-bit \(conv4\times\)4. The spatial and channel dimension mismatch of the input and output feature maps impede the identity mapping to propagate full-precision information from previous layers. In contrast, our binarized modules rely on the proposed BiSR-Conv that has a bypass identity connection for full-precision information flow, as shown in Fig. 2 (c). By using channel-wise concatenating and splitting operations, the intermediate feature maps at the input and output ends of BiSR-Conv are free from being reshaped. Therefore, the full-precision information can flow through our binarized modules. Finally, we derive our BiSRNet by using BiSR-Conv and the four modules to binarize \(\mathcal{E}\), \(\mathcal{B}\), and \(\mathcal{D}\) of the base model.

Figure 4: Comparison between normal and our binarized convolutional modules, including (a) downsample, (b) upsample, (c) fusion down to half the channels, and (d) fusion up to double the channels. The red arrow \(\downarrow\) indicates the full-precision information flow, while the blue arrow \(\downarrow\) denotes the binarized signal flow.

[MISSING_PAGE_FAIL:7]

Bi-Real, BNN, and BiConnect by 2.55, 3.25, 3.39, 3.46, 3.50, 5.88, and 7.57 dB. This evidence suggests the significant effectiveness advantage of our BiSRNet in HSI restoration.

The proposed BiSRNet with extremely lower memory and computational complexity yields comparable results with 32-bit full-precision CNN-based methods. Surprisingly, our BiSRNet outperforms \(\lambda\)-Net by 1.23 dB while only costing 0.06 % (36/62640) Params and 1.0% (1.18/117.98) OPs. In contrast, the previous best BNN-based method BTM is still 1.33 dB lower than \(\lambda\)-Net. When compared with TSA-Net, our BiSRNet only uses 0.08% Params and 1.1% OPs but achieves 94.6% (29.76/31.46) performance. These results demonstrate the efficiency superiority of the proposed method.

The three model-based algorithms are implemented by MATLAB, where the default type of variable is double-precision floating-point number. Although they use more accurate 64-bit data type, our 1-bit BiSRNet dramatically outperforms DeSCI, GAP-TV, and TwIST by 4.06, 5.40, and 6.64 dB.

### Qualitative Results

**Simulation HSI Restoration.** Fig. 5 depicts the simulation HSIs on \(Scene\) 1 with 4 out of 28 spectral channels reconstructed by the 7 SOTA BNN-based algorithms and BiSRNet. Previous BNNs are less favorable to restore HSI details. They generate blurry HSIs while introducing undesirable artifacts. In contrast, BiSRNet reconstructs more visually pleasing HSIs with more structural contents and sharper edges. Additionally, we plot the spectral density curves (bottom-left) corresponding to the selected regions of the green box in the RGB image (Top-left). BiSRNet achieves the highest correlation score with the ground truth, suggesting the advantage of BiSRNet in spectral-wise consistency restoration.

**Real HSI Restoration.** Fig. 6 visualizes the reconstructed HSIs of the seven SOTA BNN-based algorithms and our BiSRNet. We follow the setting of [18; 26; 28; 29; 63] to re-train the models with all samples of the CAVE and KAIST datasets. To simulate the noise disturbance in real imaging scenes, we inject 11-bit shot noise into measurements during training. It can be observed that our BiSRNet is more effective in detailed content reconstruction and real imaging noise suppression.

### Ablation Study

**Break-down Ablation.** We adopt baseline-1 to conduct a break-down ablation towards higher performance. Baseline-1 is derived by using vanilla 1-bit convolution to replace BiSR-Conv and normal binarized convolutional modules (see Fig. 4) to replace our binarized convolutional modules. As shown in Tab. 1(a), baseline-1 yields 23.90 dB in PSNR and 0.594 in SSIM. When we apply BiSR-Conv, the model achieves 3.90 dB improvement. Then we successively use our binarized downsample (BiDS), upsample (BiUS), fusion down (BiFD), and fusion up (BiFU) modules, the model gains by 1.96 dB in total. These results verify the effectiveness of the proposed techniques.

**BiSR-Conv.** To study the effects of BiSR-Conv components, we adopt baseline-2 to conduct an ablation. Baseline-2 is obtained by removing Spectral-Redistribution (SR) operation and Sign approximation Tanh(\(\alpha x\)) from BiSRNet. As reported in Tab. 1(b), baseline-2 achieves 27.68 dB in PSNR and 0.723 in SSIM. When we respectively apply SR and Tanh(\(\alpha x\)), baseline-2 gains by 1.29 and 1.06 dB. When we exploit SR and Tanh(\(\alpha x\)) jointly, the model achieves 2.08 dB improvement.

Figure 5: Reconstructed simulation HSIs of \(Scene\) 1 with 4 out of 28 spectral channels. Seven SOTA BNN-based algorithms and our proposed BiSRNet are compared. The spectral density curves (bottom-left) are corresponding to the selected region of the green box in the RGB image (Top-left). Please zoom in for a better view.

**Sign Approximation.** We compare our scalable hyperbolic tangent function with previous Sign approximation functions. The experimental results are listed in Tab. 1(c). Our Tanh(\(\alpha x\)) dramatically surpasses the piecewise linear function Clip(\(x\)) and quadratic function Quad(\(x\)) by 0.79 and 0.74 dB, suggesting the superiority of the proposed Tanh(\(\alpha x\)). This advantage can be explained by the analysis in Sec. 3.2 that our Tanh(\(\alpha x\)) is more flexible and can adaptively reduce the difference with Sign(\(x\)).

**Binarizing Different Parts.** We binarize one part of the base model while keeping the other parts full-precision to study the binarization benefit. The experimental results are reported in Tab. 1(d). The base model yields 34.11 dB in PSNR and 0.936 in SSIM while costing 10.52 G OPs and 634 K Params. It can be observed from Tab. 1(d): **(i)** Binarizing the bottleneck \(\mathcal{B}\) reduces the Params the most (270174) with the smallest performance drop (only 0.31 dB). **(ii)** Binarizing the decoder \(\mathcal{D}\) achieves the largest OPs reduction (4927 M) while the performance degrades by a moderate margin (1.08 dB).

## 5 Conclusion

In this paper, we propose a novel BNN-based method BiSRNet for binarized HSI restoration. To the best of our knowledge, this is the first work to study the binarized spectral compressive imaging reconstruction problem. We first redesign a compact and easy-to-deploy base model with simple computation operations. Then we customize the basic unit BiSR-Conv for model binarization. BiSR-Conv can adaptively adjust the density and distribution of HSI representations before binarizing the activation. Besides, BiSR-Conv employs a scalable hyperbolic tangent function to closer approach Sign by arbitrarily reducing the approximation error. Subsequently, we use BiSR-Conv to build up four binarized convolutional modules that can handle the dimension mismatch issue during feature reshaping and propagate full-precision information through all layers. Comprehensive quantitative and qualitative experiments demonstrate that our BiSRNet significantly outperforms SOTA BNNs and even achieves comparable performance with full-precision CNN-based HSI reconstruction algorithms.

\begin{table}

\end{table}
Table 2: Ablations on the simulation datasets. In table (a), BiUS, BiDS, BiFU, and BiFD denote the binarized upsample, downsample, fusion up, and fusion down modules of Fig. 4. In table (b), SR refers to the Spectral-Redistribution of Eq. (11). In table (d), the full-precision model yields 34.11 dB in PSNR and 0.936 in SSIM.

Figure 6: Reconstructed real HSIs of seven SOTA BNN-based algorithms and our BiSRNet on four scenes with 4 out of 28 wavelengths. BiSRNet is more effective in reconstructing detailed contents and suppressing noise.

## Acknowledgement

This research was funded through National Key Research and Development Program of China (Project No. 2022YFB36066), in part by the Shenzhen Science and Technology Project under Grant (JCYJ20220818101001004, JSGG20210802153150005), National Natural Science Foundation of China (62271414), Science Fund for Distinguished Young Scholars of Zhejiang Province (LR23F010001), and Research Center for Industries of the Future at Westlake University.

## References

* [1]A. A. Gowen, C. P. O'Donnell, P. J. Cullen, G. Downey, and J. M. Frias (2007) Hyperspectral imaging-an emerging process analytical tool for food quality and safety control. Trends in food science & technology. Cited by: SS1.
* [2]D. Lorente, N. Aleixos, J. Gomez-Sanchis, S. Cubero, O. L. Garcia-Navarrete, and J. Blasco (2012) Recent advances and applications of hyperspectral imaging for fruit and vegetable quality assessment. Food and Bioprocess Technology. Cited by: SS1.
* [3]R. Lu and Y. Chen (1999) Hyperspectral imaging for safety inspection of food and agricultural products. In Pathogen Detection and Remediation for Safe Eating, Cited by: SS1.
* [4]V. Backman, M. B. Wallace, L. Perelman, J. Arendt, R. Gurjar, M. Muller, Q. Zhang, G. Zonios, E. Kline, and T. McGillican (2000) Detection of preinvasive cancer cells. Nature. Cited by: SS1.
* [5]G. Lu and B. Fei (2014) Medical hyperspectral imaging: a review. Journal of Biomedical Optics. Cited by: SS1.
* [6]Z. Meng, M. Qiao, J. Ma, Z. Yu, K. Xu, and X. Yuan (2020) Snapshot multispectral endomicroscopy. Optics Letters. Cited by: SS1.
* [7]M. H. Kim, T. A. Harvey, D. S. Kittle, H. Rushmeier, J. Dorsey, R. O. Prum, and D. J. Brady (2012) 3D imaging spectroscopy for measuring hyperspectral patterns on solid objects. TOG. Cited by: SS1.
* [8]M. Borengasser, W. S. Hungate, and R. Watkins (2007) Hyperspectral remote sensing: principles and applications. CRC press. Cited by: SS1.
* [9]M. B. Wallace, M. J. Hoffman, and A. Vodacek (2016) Real-time vehicle tracking in aerial video using hyperspectral features. In CVPRW, Cited by: SS1.
* [10]M. B. W. H. Kim, M. J. Hoffman, and A. Vodacek (2016) Real-time vehicle tracking in aerial video using hyperspectral features. In CVPRW, Cited by: SS1.
* [11]A. Wagadarikar, R. John, R. Willett, and D. Brady (2008) Single disperser design for coded aperture snapshot spectral imaging. Applied Optics. Cited by: SS1.
* [12]A. Wagadarikar, N. P. Pitsianis, X. Sun, and D. J. Brady (2009) Video rate spectral imaging using a coded aperture snapshot spectral imager. Optics Express. Cited by: SS1.
* [13]A. Wagadarikar, R. John, R. Willett, and D. Brady (2008) Single disperser design for coded aperture snapshot spectral imaging. Applied Optics. Cited by: SS1.

[MISSING_PAGE_POST]

is, and X. Sun (2009) End-to-end low cost compressive spectral imaging with spatial-spectral self-attention.

* [21] L. Wang, C. Sun, M. Zhang, Y. Fu, and H. Huang, "Dnu: Deep non-local unrolling for computational spectral imaging," in _CVPR_, 2020.
* [22] L. Wang, C. Sun, Y. Fu, M. H. Kim, and H. Huang, "Hyperspectral image reconstruction using a deep spatial-spectral prior," in _CVPR_, 2019.
* [23] J. Ma, X.-Y. Liu, Z. Shou, and X. Yuan, "Deep tensor admm-net for snapshot compressive imaging," in _ICCV_, 2019.
* [24] X. Hu, Y. Cai, J. Lin, H. Wang, X. Yuan, Y. Zhang, R. Timofte, and L. V. Gool, "Hdnet: High-resolution dual-domain learning for spectral compressive imaging," in _CVPR_, 2022.
* [25] Z. Meng, Z. Yu, K. Xu, and X. Yuan, "Self-supervised neural networks for spectral snapshot compressive imaging," in _ICCV_, 2021.
* [26] Y. Cai, J. Lin, X. Hu, H. Wang, X. Yuan, Y. Zhang, R. Timofte, and L. V. Gool, "Mask-guided spectral-wise transformer for efficient hyperspectral image reconstruction," in _CVPR_, 2022.
* [27] Y. Cai, J. Lin, Z. Lin, H. Wang, Y. Zhang, H. Pfister, R. Timofte, and L. V. Gool, "Mst++: Multi-stage spectral-wise transformer for efficient spectral reconstruction," in _CVPRW_, 2022.
* [28] Y. Cai, J. Lin, X. Hu, H. Wang, X. Yuan, Y. Zhang, R. Timofte, and L. V. Gool, "Coarse-to-fine sparse transformer for hyperspectral image reconstruction," in _ECCV_, 2022.
* [29] Y. Cai, J. Lin, H. Wang, X. Yuan, H. Ding, Y. Zhang, R. Timofte, and L. V. Gool, "Degradation-aware unfolding half-shuffle transformer for spectral compressive imaging," in _NeurIPS_, 2022.
* [30] X. Jiang, N. Wang, J. Xin, K. Li, X. Yang, and X. Gao, "Training binary neural network without batch normalization for image super-resolution," in _AAAI_, 2021.
* [31] B. Xia, Y. Zhang, Y. Wang, Y. Tian, W. Yang, R. Timofte, and L. V. Gool, "Basic binary convolution unit for binarized image restoration network," in _ICLR_, 2023.
* [32] Z. Liu, Z. Shen, M. Savvides, and K.-T. Cheng, "Reactnet: Towards precise binary neural network with generalized activation functions," in _ECCV_, 2020.
* [33] H. Qin, R. Gong, X. Liu, M. Shen, Z. Wei, F. Yu, and J. Song, "Forward and backward information retention for accurate binary neural networks," in _CVPR_, 2020.
* [34] Z. Liu, B. Wu, W. Luo, X. Yang, W. Liu, and K.-T. Cheng, "Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm," in _ECCV_, 2018.
* [35] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, "Binarized neural networks," in _NeurIPS_, 2016.
* [36] M. Courbariaux, Y. Bengio, and J.-P. David, "Binaryconnect: Training deep neural networks with binary weights during propagations," in _NeurIPS_, 2015.
* [37] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer, "Efficient processing of deep neural networks: A tutorial and survey," _Proceedings of the IEEE_, 2017.
* [38] H. Qin, R. Gong, X. Liu, X. Bai, J. Song, and N. Sebe, "Binary neural networks: A survey," _Pattern Recognition_, 2020.
* [39] J. Xin, N. Wang, X. Jiang, J. Li, H. Huang, and X. Gao, "Binarized neural network for single image super resolution," in _ECCV_, 2020.
* [40] H. Wang and Y. Fu, "Trainability preserving neural structured pruning," in _ICLR_, 2023.
* [41] Y. Zhang, H. Wang, C. Qin, and Y. Fu, "Learning efficient image super-resolution networks via structure-regularized pruning," in _ICLR_, 2022.
* [42] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, "Pruning filters for efficient convnets," in _ICLR_, 2017.
* a statistical perspective," in _NeurIPS_, 2022.
* [44] G. Hinton, O. Vinyals, and J. Dean, "Distilling the knowledge in a neural network," in _NeurIPSW_, 2014.
* [45] X. Zhang, X. Zhou, M. Lin, and J. Sun, "Shufflenet: An extremely efficient convolutional neural network for mobile devices," in _CVPR_, 2018.

* [46] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, "Shufflenet v2: Practical guidelines for efficient cnn architecture design," in _ECCV_, 2018.
* [47] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, "Mobilenets: Efficient convolutional neural networks for mobile vision applications," _arXiv preprint arXiv:1704.04861_, 2017.
* [48] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, "Mobilenetv2: Inverted residuals and linear bottlenecks," in _CVPR_, 2018.
* [49] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, "Xnor-net: Imagenet classification using binary convolutional neural networks," in _ECCV_, 2016.
* [50] R. Ding, T.-W. Chin, Z. Liu, and D. Marculescu, "Regularizing activation distribution for training binarized deep networks," in _CVPR_, 2019.
* [51] J. Zhang, Y. Pan, T. Yao, H. Zhao, and T. Mei, "dabnn: A super fast inference framework for binary neural networks on arm devices," in _ACM MM_, 2019.
* [52] D. Kittle, K. Choi, A. Wagadarikar, and D. J. Brady, "Multiframe image estimation for coded aperture snapshot spectral imagers," _Applied optics_, 2010.
* [53] Y. Liu, X. Yuan, J. Suo, D. Brady, and Q. Dai, "Rank minimization for snapshot compressive imaging," _TPAMI_, 2019.
* [54] L. Wang, Z. Xiong, G. Shi, F. Wu, and W. Zeng, "Adaptive nonlocal sparse representation for dual-camera compressive hyperspectral imaging," _TPAMI_, 2016.
* [55] S. Zhang, L. Wang, Y. Fu, X. Zhong, and H. Huang, "Computational hyperspectral imaging based on dimension-discriminative low-rank tensor recovery," in _ICCV_, 2019.
* [56] X. Yuan, "Generalized alternating projection based total variation minimization for compressive sensing," in _ICIP_, 2016.
* [57] J. Tan, Y. Ma, H. Rueda, D. Baron, and G. R. Arce, "Compressive hyperspectral imaging via approximate message passing," _IEEE Journal of Selected Topics in Signal Processing_, 2016.
* [58] M. A. Figueiredo, R. D. Nowak, and S. J. Wright, "Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems," _IEEE Journal of selected topics in signal processing_, 2007.
* [59] J. Bioucas-Dias and M. Figueiredo., "A new twist: Two-step iterative shrinkage/thresholding algorithms for image restoration.," _TIP_, 2007.
* [60] J. Yang, X. Liao, X. Yuan, P. Llull, D. J. Brady, G. Sapiro, and L. Carin, "Compressive sensing by learning a gaussian mixture model from measurements," _TIP_, 2014.
* [61] J. Yang, X. Yuan, X. Liao, P. Llull, D. J. Brady, G. Sapiro, and L. Carin, "Video compressive sensing using gaussian mixture models," _TIP_, 2014.
* [62] X. Zhang, Y. Zhang, R. Xiong, Q. Sun, and J. Zhang, "Herosnet: Hyperspectral explicable reconstruction and optimal sampling deep network for snapshot compressive imaging," in _CVPR_, 2022.
* [63] T. Huang, W. Dong, X. Yuan, J. Wu, and G. Shi, "Deep gaussian scale mixture prior for spectral compressive imaging," in _CVPR_, 2021.
* [64] X. Lin, C. Zhao, and W. Pan, "Towards accurate binary convolutional neural network," in _NeurIPS_, 2017.
* [65] I. Choi, D. S. Jeon, G. Nam, D. Gutierrez, and M. H. Kim, "High-quality hyperspectral reconstruction using a spectral prior," _TOG_, 2017.
* [66] J.-I. Park, M.-H. Lee, M. D. Grossberg, and S. K. Nayar, "Multispectral imaging using multiplexed illumination," in _ICCV_, 2007.
* [67] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, "Pytorch: An imperative style, high-performance deep learning library," in _NeurIPS_, 2019.
* [68] D. P. Kingma and J. L. Ba, "Adam: A method for stochastic optimization," in _ICLR_, 2015.
* [69] I. Loshchilov and F. Hutter, "Sgdr: Stochastic gradient descent with warm restarts," in _ICLR_, 2017.