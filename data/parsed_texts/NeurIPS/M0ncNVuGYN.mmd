# Are High-Degree Representations Really Unnecessary

in Equivariant Graph Neural Networks?

 Jiacheng Cen\({}^{1}\)\({}^{2}\), Anyi Li\({}^{1}\)\({}^{2}\), Ning Lin\({}^{1}\)\({}^{2}\), Yuxiang Ren\({}^{3}\), Zihe Wang\({}^{1}\)\({}^{2}\), Wenbing Huang\({}^{1}\)\({}^{2}\)

\({}^{1}\) Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\) Beijing Key Laboratory of Big Data Management and Analysis Methods

\({}^{3}\) 2012 Laboratories, Huawei Technologies, Shanghai

{jiacc.cn, li_anyi, ninglin00}@outlook.com; renyxiang1@huawei.com; wang.zihe@ruc.edu.cn; hwenbing@126.com

Wenbing Huang is the corresponding author.

###### Abstract

Equivariant Graph Neural Networks (GNNs) that incorporate the E(3) symmetry have achieved significant success in various scientific applications. As one of the most successful models, EGNN [1] leverages a simple scalarization technique to perform equivariant message passing over only Cartesian vectors (i.e., 1st-degree steerable vectors), enjoying greater efficiency and efficacy compared to equivariant GNNs using higher-degree steerable vectors. This success suggests that higher-degree representations might be unnecessary. In this paper, we disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric structures, including \(k\)-fold rotations and regular polyhedra. We theoretically demonstrate that equivariant GNNs will always degenerate to a zero function if the degree of the output representations is fixed to 1 or other specific values. Based on this theoretical insight, we propose HEGNN, a high-degree version of EGNN to increase the expressivity by incorporating high-degree steerable vectors while still maintaining EGNN's advantage through the scalarization trick. Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on a toy dataset consisting of symmetric structures, but also shows substantial improvements on other complicated datasets without obvious symmetry, including \(N\)-body and MD17. Our study potentially showcase an effective way of modeling high-degree representations in equivariant GNNs.

## 1 Introduction

Molecules, proteins, crystals, and many other scientific data can be effectively modeled and represented through _geometric graphs_[2, 3, 4, 5, 6, 7, 8]. This type of data structure encapsulates not only node characteristics and edge information but also a 3D vector (such as position, velocity, etc.) for each node. To process geometric graphs, equivariant Graph Neural Networks (GNNs) have been developed, which undergo equivariant message passing over nodes, conforming to the \(\mathrm{E}(3)\) or \(\mathrm{SE}(3)\) symmetry of physical laws. These models have achieved remarkable successes in a lot of scientific tasks, such as physical dynamics simulation [9, 10, 11], molecular generation [12, 13, 14, 15] and protein design [16, 17, 18].

Pioneer equivariant GNNs [19, 20, 21, 22] derive high-degree steerable representations beyond scalars and 3D coordinates with the help of spherical harmonics and conduct equivariant message passing between representations of different degrees through the Clebsch-Gordan (CG) tensor product. While these high-degree models are able to approximate any function of fully connected geometric graphs in theory [23], they usually suffer from expensive computational costs in practice. In contrast,EGNN [1] leverages a simple scalarization technique to allow equivariant message passing over only 3D vectors (_i.e._ the 1st-degree steerable features). Specifically, the scalarization technique first encodes 3D vectors into scalars as invariant messages, which are passed as geometric messages after the multiplication with the 3D vectors to recover the orientation information. Despite its simplicity, EGNN achieves remarkably better efficacy and efficiency against conventional high-degree models for a broad range of applications [24; 25]. Such successes suggest that higher-degree representations might be unnecessary.

In this paper, we challenge and disprove this hypothesis by exploring the expressivity of equivariant GNNs on symmetric graphs. Fig. 1 illustrates the examples of \(k\)-fold rotations and regular polyhedra, which are invariant to rotations up to certain rotating angles. Taking the cube for example, conducting \(90^{\circ}\) rotation around the axes crossing the center of the two opposite faces keeps its shape and orientation unchanged. Interestingly, by making use of group theory, we theoretically prove that any equivariant GNN (after translating the coordinate center to the origin and conducting graph-level readout) on these symmetric graphs will degenerate to a zero function if the degree of their representations is fixed to be 1. The direct deduction of this theorem is that EGNN can only output a zero 3D vector no matter how we rotate the input graph, indicating that EGNN totally loses the recognition ability of orientation. Additionally, this statement points out the limitation of the methods that rely on constructing global features for symmetric graphs 2 (_e.g._ frames in frame averaging [26; 27], virtual nodes in FastEGNN [28]), equivariant pooling in EGHN [29], and meshes in Neural P\({}^{3}\)M [30]), since it is impossible to output another non-collinear 3D vector except the center coordinate.

Footnote 2: See Appendix A.3 for further discussion.

Based on the above theoretical insights, we propose a novel equivariant GNN model termed HEGNN3, which enhances EGNN by incorporating high-degree steerable vectors while inheriting the desired benefit from EGNN through the scalarization trick. In summary, our contributions are as follows:

Footnote 3: Code is available at https://github.com/GLAD-RUC/HEGNN.

* We theoretically investigate the expressivity reduction issue of equivariant GNNs on symmetric graphs.
* We propose HEGNN, to further incorporate high-degree steerable representations into EGNN. Moreover, since the equivariant message passing process between different-degree representations is conducted via inner products, it shares the same benefit as EGNN, compared to traditional high-degree models.
* Our extensive experiments demonstrate that HEGNN not only aligns with our theoretical analyses on toy datasets consisting of symmetric graphs, but also shows substantial improvements on more complicated datasets without explicit symmetry, such as \(N\)-body and MD17.

## 2 Related Works

**Equivariant GNNs.** Equivariant GNNs can be divided into two classes: scalarization-based models and high-degree steerable models [31]. Scalarization-based models adopt norms or inner products to convert equivariant 3D vectors into invariant scalars, which are considered as coefficients to linearly combine 3D vectors for node update. EGNN [1] is the first work falling into this category. Concurrently, PAINN [32] further enhances the expressive ability of the model by introducing multi-channel equivariant features. On the contrary, high-degree steerable models (_e.g._ TFN [19],

Figure 1: Common symmetric graphs. Equivariant GNNs on symmetric graphs will degenerate to a zero function if the degree of their representations is fixed as 1.

SEGNN [33] and SE(3)-Transformer) use spherical harmonics to ensure the equivariance of message passing, and realize interaction between steerable features of different degrees through CG tensor products. Our HEGNN also uses high-degree steerable features, but it leverages the scalarization trick for the interaction between steerable features of different degrees, thus leading to more expressivity than EGNN and less computational cost than other high-degree models.

**Expressivity of Equivariant GNNs.** The theoretical expressivity of equivariant GNNs is initially explored by [23], which proves the university of the high-degree steerable model, _i.e._, TFN [19], over fully-connected geometric graphs. GemNet [34] further demonstrates that the universality holds with just spherical representations other than the full \(\mathrm{SO}(3)\) representations that are required in the proof of [23]. More recently, the GWL framework [35] extends the Weisfeiler-Lehman (WL) test into a geometric version [36] to study the expressive power of geometric GNNs operating on sparse graphs from the perspective of discriminating geometric graphs. Different from all the above works, our paper investigates the expressivity of equivariant GNNs on symmetric graphs, and demonstrates the necessity of involving high-degree representations. Although the GWL test paper [35] has experimentally compared different models on \(k\)-fold structures that are allowed to rotate only in the 2D space, the conclusions of this paper are proved both theoretically and experimentally. Moreover, our discussions cover a full range of examples including \(k\)-folds (rotation in 3D space) and regular polyhedra.

## 3 Theoretical Analyses

In this section, we first present the necessary preliminaries related to geometric graphs and group representation. Then, we define and illustrate typical examples of symmetric graphs. Finally, we will discuss when equivariant GNNs will degenerate to a zero function on symmetric graphs.

### Preliminaries

**Geometric graph.** A geometric graph of \(N\) nodes is defined as \(\mathcal{G}\coloneqq\left(\bm{H},\vec{\bm{X}};\bm{A}\right)\), where \(\bm{H}\coloneqq\{\bm{h}_{i}\in\mathbb{R}^{C_{H}}\}_{i=1}^{N}\) and \(\vec{\bm{X}}\coloneqq\{\vec{\bm{x}}_{i}\in\mathbb{R}^{3}\}_{i=1}^{N}\) are node features and 3D coordinates, respectively; \(\bm{A}\in\mathbb{R}^{N\times N}\) represents the adjacency matrix and can be assigned with edge features \(\bm{e}_{ij}\) if necessary. Throughout our theoretical analyses in this section, we assume the node features and edge features to be identical for all.

**Transformation of geometric graph.** We are interested in the transformations of a geometric graph \(\mathcal{G}\) with respect to a group \(\mathfrak{G}\), which is defined as \(\mathfrak{g}\cdot\mathcal{G}\), for \(\mathfrak{g}\in\mathfrak{G}\) and \(\cdot\) denoting the group action. For instance, \(\mathfrak{g}\cdot\mathcal{G}\) can be explained as translation, rotation, or reflection of the coordinates \(\vec{\bm{X}}\). These transformations form a 3D Euclidean group denoted as \(\mathrm{E}(3)\), and its subgroup without translation is called the orthogonality group \(\mathrm{O}(3)\). With the aid of group representation \(\rho(\mathfrak{g})\), the transformation of a coordinate \(\vec{\bm{x}}\) is represented as \(\rho(\mathfrak{g})\vec{\bm{x}}\). For example, orthogonal matrices are the trivial representations of \(\mathrm{O}(3)\), that is, the orthogonal transformation of a vector \(\vec{\bm{x}}\) is represented by \(\bm{O}\vec{\bm{x}}\) with \(\bm{O}\in\mathbb{R}^{3\times 3}\) being an orthogonal matrix. Besides, there are other representations of \(\mathrm{O}(3)\), such as the irreducible representations which will be detailed below.

**Equivariance.** Let \(\mathcal{X}\) and \(\mathcal{Y}\) be the input and output vector spaces, respectively. A function \(f:\mathcal{X}\rightarrow\mathcal{Y}\) is called _equivariant_ with respect to group \(\mathfrak{G}\) if

\[\forall\mathfrak{g}\in\mathfrak{G},f(\rho_{\mathcal{X}}(\mathfrak{g})\vec{ \bm{x}})=\rho_{\mathcal{Y}}(\mathfrak{g})f(\vec{\bm{x}}),\] (1)

where \(\rho_{\mathcal{X}}\) and \(\rho_{\mathcal{Y}}\) are the group representations in the input and output spaces, respectively. Since we can eliminate the translation effect by simply translating the center of all coordinates to the origin, we only discuss equivariance with respect to \(\mathrm{O}(3)\) in this section. In other words, we default that the center of \(\vec{\bm{X}}\) is at the origin.

**Irreducible representations and steerable features.**\(\mathrm{O}(3)\) consists of rotation and inversion, implying \(\mathrm{O}(3)=\mathrm{SO}(3)\times C_{i}\), where \(\mathrm{SO}(3)\) is the rotation group and \(C_{i}=\{\mathfrak{e},\mathfrak{i}\}\) denotes the inverse group. We first discuss the irreducible representations of \(\mathrm{SO}(3)\). For each rotation \(\mathfrak{r}\in\mathrm{SO}(3)\), its irreducible representations are Wigner-D matrices \(\bm{D}^{(l)}(\mathfrak{r})\in\mathbb{R}^{(2l+1)\times(2l+1)}\) of different degree \(l\in\mathbb{N}\)[21; 37]. When \(l=1\), it becomes the common rotation matrix \(\bm{R_{\mathfrak{r}}}\) acting on the 3D coordinate space. Under the irreducible representations, the equivariant constraint in Eq. (1) turns into \(f^{(l)}(\bm{R}_{\bm{x}}\vec{\bm{x}})=\bm{D}^{(l)}(\mathfrak{r})f^{(l)}(\vec{\bm{x}})\), if the output degree is \(l\). According to [38], spherical harmonics \(Y^{(l)}=[Y^{(l)}_{m}(\vec{\bm{x}})]^{l}_{m=-l}\) offer a _unique_ and _complete_ set of function bases satisfying the equivariant constraint. We further define a modulated spherical harmonics as \(f^{(l)}(\vec{\bm{x}})=\varphi(\|\vec{\bm{x}}\|)\cdot Y^{(l)}(\vec{\bm{x}}/\| \vec{\bm{x}}\|)\) by adding a continuous radial function \(\varphi:\mathbb{R}^{+}\to\mathbb{R}\) of vector norm \(\|\cdot\|\) for re-scaling. Such a function \(f^{l}\) and its output \(f^{l}(\vec{\bm{x}})\) are called type-\(l\)_steerable function_ and _steerable feature_, respectively. We now deduce the irreducible representations from \(\mathrm{SO}(3)\) to \(\mathrm{O}(3)\). Note that spherical harmonics satisfy \(Y^{(l)}(-\vec{\bm{x}})=(-1)^{l}Y^{(l)}(\vec{\bm{x}})\); in other words, they are inverse-equivariant when \(l\) is odd, but inverse-invariant when \(l\) is even. We thus specify the group representation of \(\mathrm{O}(3)\) as

\[\rho^{(l)}(\mathfrak{m})\coloneqq\sigma^{(l)}(\mathfrak{m})\bm{D}^{(l)}( \mathfrak{r}),\] (2)

where \(\sigma^{(l)}(\mathfrak{m})=1\) for \(\mathfrak{m}=\mathfrak{e}\) (the identity) and \(\sigma^{(l)}(\mathfrak{m})=(-1)^{l}\) if \(\mathfrak{m}=\mathfrak{i}\) (the inverse). Readers can refer to the discussion in e3nn [39] with another representation method by using the concept of _parity_ and construct this through methods such as Clebsch-Gordan (CG) tensor product [40]. For concision, the type-\(l\) steerable feature is denoted as \(\tilde{\bm{v}}^{(l)}\) with a tilde notation.

### Symmetric Graph

In SS 1, we present that \(k\)-fold rotations and regular polyhedra exhibit certain symmetries. In this subsection, we formally describe them via the notion of the symmetric graph.

**Definition 3.1** (Symmetric Graph).: A geometric graph \(\mathcal{G}\) is called a symmetric graph, if there exists a finite and nontrivial subgroup \(\mathfrak{H}\leq\mathrm{O}(3),\mathfrak{H}\neq\{\mathfrak{e}\}\), satisfying that \(\forall\mathfrak{h}\in\mathfrak{H},\mathfrak{h}\cdot\mathcal{G}=\mathcal{G}\). All subgroups making \(\mathcal{G}\) symmetric yields a set \(\mathbb{H}(\mathcal{G})\), and all geometric graphs that are symmetric _w.r.t._\(\mathfrak{H}\)) constitute a set denoted as \(\mathbb{G}(\mathfrak{H})\).

Here \(\mathfrak{h}\cdot\mathcal{G}=\mathcal{G}\) is defined in the graph level. Particularly for the coordinates \(\vec{\bm{X}}\in\mathbb{R}^{3\times N}\), it implies that \(\forall\mathcal{O}\in\mathfrak{H}\), \(\exists\bm{P}\in S_{N}\), \(\bm{O}\vec{\bm{X}}=\vec{\bm{X}}\bm{P}\) and \(\bm{P}\bm{A}=\bm{A}\bm{P}\), where \(S_{N}\) is the permutation group of order \(N\). Essentially, rotating the coordinates of a symmetric graph leads to a copy of this graph up to a different permutation of the nodes.

Without considering inversion, the finite subgroups of \(\mathrm{SO}(3)\) are only cyclic group \(C_{n}\), dihedral group \(D_{n}\), tetrahedral group \(T\), octahedral group \(O\), and Icosahedral group \(I\)[41]. We provide several examples of symmetric graphs as follows.

**Example 3.2** (\(k\)-folds).: On the one hand, for a geometric graph \(\mathcal{G}\) corresponding to a \(2k\)-fold with nodes \(\{(\cos(i\cdot\pi/k),\sin(i\cdot\pi/k),0)\}_{i=0}^{2k-1}\), the inverse group \(C_{i}\) and the dihedral group \(D_{2k}\) (rotation around \(z\)-axis with angle \(\pi/k\), and reflection around the axis connecting the midpoints of opposite sides or the axis connecting opposite vertices), are symmetric groups on \(\mathcal{G}\), namely, \(C_{i},D_{2k}\in\mathbb{H}(\mathcal{G})\). On the other hand, for a geometric graph \(\mathcal{G}\) corresponding to a \((2k+1)\)-fold with nodes \(\{(\cos(i\cdot 2\pi/(2k+1),\sin(i\cdot 2\pi/(2k+1)),0))\}_{i=0}^{2k}\), \(\mathbb{H}(\mathcal{G})\) includes the dihedral group \(D_{2k+1}\) but without the inverse group \(C_{i}\).

**Example 3.3** (Regular Polygons).: The symmetric groups of regular polygons in the plane and regular prisms in space include the dihedral group \(D_{n}\). Regular tetrahedra are symmetric with respect to three rotation axes of the second order and four axes of the third order, corresponding to 12 group elements. Regular hexahedra (cubes) and the regular octahedra (which are dual to each other and share the same symmetric groups) are symmetric about six axes of the second order, four axes of the third order, and three axes of the fourth order, corresponding to 24 group elements. Regular dodecahedra and regular icosahedra (which are also dual to each other) are symmetric about six axes of the fifth order, ten axes of the third order, and fifteen axes of the second order, corresponding to 60 group elements. Additionally, except tetrahedra, all other four regular polygons are central symmetric, indicating that \(C_{i}\) is their common symmetric group.

### Equivariant GNNs on symmetric graphs

We now demonstrate that equivariant GNNs on symmetric graphs will encounter the issue of expressivity degeneration. Here, we assume that the graph functions we explore are invariant to the permutation of the nodes. This fits the case when we add a readout layer to all nodes globally or just focus on the message passing process for each node individually.

We first derive a crucial theorem that greatly facilitates our analyses.

**Theorem 3.4**.: _Suppose that \(f^{(l)}\) is an \(\mathrm{O}(3)\)-equivariant function on geometric graphs, regarding the group representation \(\rho^{(l)}\) defined in Eq. (2). Then, for any symmetric graph \(\mathcal{G}\) induced by the group \(\mathfrak{H}\leq\mathrm{O}(3)\), namely, \(\forall\mathcal{G}\in\mathbb{G}(\mathfrak{H})\), we always have_

\[f^{(l)}(\mathcal{G})=\rho^{(l)}(\mathfrak{H})f^{(l)}(\mathcal{G}).\] (3)

_Here we have defined group average as \(\rho^{(l)}(\mathfrak{H})\coloneqq\frac{1}{|\mathfrak{H}|}\sum_{\mathfrak{h} \in\mathfrak{H}}\rho^{(l)}(\mathfrak{h})\)._

Eq. (3) is interesting and it shows that the function \(f^{(l)}\) is symmetric with respect to the group average \(\rho^{(l)}(\mathfrak{H})\). More importantly, it indicates an linear equation \(\left(\bm{I}_{2l+1}-\rho^{(l)}(\mathfrak{H})\right)f^{(l)}(\mathcal{G})=0\), where \(\bm{I}_{2l+1}\in\mathbb{R}^{(2l+1)\times(2l+1)}\) is the identity matrix. We can immediately attain the following conclusion.

**Theorem 3.5**.: _If and only if the matrix \(\bm{I}_{2l+1}-\rho^{(l)}(\mathfrak{H})\) is non-singular, the \(\mathrm{O}(3)\)-equivariant function \(f^{(l)}\) is always a zero function on \(\mathcal{G}\), namely,_

\[f^{(l)}(\mathcal{G})\equiv\bm{0},\quad\forall\mathcal{G}\in\mathbb{G}( \mathfrak{H}).\] (4)

A more general version of Theorem 3.5 is that the output space of \(f^{(l)}\) corresponds to the null space of the matrix \(\bm{I}_{2l+1}-\rho^{(l)}(\mathfrak{H})\), indicating that \(\dim(f^{(l)})=(2l+1)-\mathrm{rank}\left(\bm{I}_{2l+1}-\rho^{(l)}(\mathfrak{H })\right)\). Therefore, even the function \(f^{(l)})\) will not exactly reduce to a zero function when \(\bm{I}_{2l+1}-\rho^{(l)}(\mathfrak{H})\) is singular, its output space is still limited to a subspace and suffers from diminished expressivity owing to the symmetry of the input geometric graph.

In practice, it is difficult to determine if the matrix \(\bm{I}_{2l+1}-\rho^{(l)}(\mathfrak{H})\) is singular. This determination becomes easier if we can show that the group average \(\rho^{(l)}(\mathfrak{H})\) is equal to the zero matrix. Fortunately, we have the following property.

**Theorem 3.6**.: _For a finite group \(\mathfrak{H}\) with its representation \(\rho^{(l)}\), \(\rho^{(l)}(\mathfrak{H})\) is a zero matrix (i.e., \(\rho^{(l)}(\mathfrak{H})=\bm{0}\)) if and only if \(\mathrm{tr}(\rho^{(l)}(\mathfrak{H}))=0\). In this case, \(f^{(l)}(\mathcal{G})\equiv\bm{0},\forall\mathcal{G}\in\mathbb{G}(\mathfrak{H})\)._

According to Theorem 3.6, we calculate the trace of the group average for each symmetric graph of interest and check if the trace is equal to zero. We summarize the conclusions for \(k\)-fold structures and regular polyhedra in Table 1. We find that when \(l=1\), \(f^{(1)}\equiv\bm{0}\) for all cases. In addition, the function degenerates when \(l\) is odd, if the symmetric graph is induced by the inverse group \(C_{i}\). We defer more details of the calculations in the Appendix. Compared to the conclusions drawn by the GWL paper [35] which only experimentally discusses the \(k\)-fold structures under 2D rotations, here we apply rigorous theoretical derivations to analyze more cases besides \(k\)-folds, regarding more symmetric subgroups of \(\mathrm{O}(3)\).

## 4 The Proposed HEGNN

The analyses in the last section imply the necessity of involving the representations with more and higher degrees in equivariant GNNs. Therefore, we propose HEGNN by further conducting the update of high-degree steerable features upon EGNN [1]. As illustrated in Fig. 2, HEGNN is composed of the three key components: initialization of high-degree steerable features, calculation of cross-degree invariant messages, and aggregation of neighbor messages, the latter two of which are conducted over multiple layers. We depict each component separately in what follows.

**Initialization of high-degree steerable features.** Given a geometric graph \(\mathcal{G}\left(\bm{H},\bm{\vec{X}};\bm{A}\right)\) where each node contains only type-0 feature \(\bm{h}_{i}\) and type-1 feature \(\vec{\bm{x}}_{i}\), we first obtain the initialization

\begin{table}
\begin{tabular}{l l l} \hline \hline Symmetric Graph \(\mathcal{G}\) & Symmetry Group \(\mathfrak{H}\in\mathbb{H}(\mathcal{G})\) & \(l\) leading to \(f^{(l)}(\mathcal{G})\equiv\bm{0}\) \\ \hline \(2k\)-fold & \(C_{i},D_{2k}\) & \(l\) is odd \\ \((2k+1)\)-fold & \(D_{2k+1}\) & \(l<2k+1\) and \(l\) is odd \\ Tetrahedron & \(T\) & \(l\in\{1,2,5\}\) \\ Cube/Octahedron & \(C_{i},O\) & \(l=2\) or \(l\) is odd \\ Dodecahedron/Icosahedron & \(C_{i},I\) & \(l\in\{2,4,8,14\}\) or \(l\) is odd \\ \hline \hline \end{tabular}
\end{table}
Table 1: Expressivity degeneration of equivariant GNNs on symmetric graphs.

of high-degree steerable features \(\{\tilde{\bm{v}}_{i}^{(l)}\}_{l=0}^{L}\) by using spherical harmonics on normalized relative coordinates. In detail, we aggregate spherical harmonics from all neighbors as

\[\tilde{\bm{v}}_{i,\texttt{init}}^{(l)}=\frac{1}{|\mathcal{N}(i)|}\sum_{j\in \mathcal{N}(i)}\varphi_{\tilde{\bm{v}},\texttt{init}}^{(l)}(\bm{m}_{ij, \texttt{init}})\cdot Y^{(l)}\left(\frac{\bm{\vec{x}}_{i}-\bm{\vec{x}}_{j}}{ \|\bm{\vec{x}}_{i}-\bm{\vec{x}}_{j}\|}\right),\] (5)

where \(\bm{m}_{ij,\texttt{init}}=\varphi_{\bm{m},\texttt{init}}\left(\bm{h}_{i},\bm{h }_{j},\bm{e}_{ij},d_{ij}^{2}\right)\) is an invariant scalar with \(\varphi_{\bm{m},\texttt{init}}\) being an arbitrary MultiLayer Perceptron (MLP), and \(\mathcal{N}(i)\) denotes the neighbors of \(i\)4.

Footnote 4: Eq. (5) is unable to derive pseudo-vectors such as torque or angular momentum, which are type-1 steerable features but invariant to reflection. To address this issue, we can further conduct CG tensor product between \(\tilde{\bm{v}}_{i,\texttt{init}}^{(l)}\) and \(\tilde{\bm{x}}_{i}-\tilde{\bm{x}}_{j}\) to yield the steerable feature of desired symmetry.

**Calculation of cross-degree invariant messages.** EGNN [1] employs a scalarization trick by transforming the relative coordinate \(\bm{\vec{x}}_{i}-\bm{\vec{x}}_{j}\) (the usage of relative coordinates is for translation invariance) into an invariant scalar via the vector norm, which will be used to compute invariant message for both node features and coordinates. We generalize this scalarization trick to the case of high-degree steerable features. To be specific, we carry out the inner product between \(\tilde{\bm{v}}_{i}^{(l)}\) and \(\tilde{\bm{v}}_{j}^{(l)}\) for each degree \(l\) individually, resulting in an invariant scalar \(z_{ij}^{(l)}\). Then, we get the invariant message between node \(i\) and \(j\), namely \(\bm{m}_{ij}\) after undergoing an MLP of all invariant quantities. The above processes are summarized as follows:

\[d_{ij}=\|\bm{\vec{x}}_{i}-\bm{\vec{x}}_{j}\|,\quad z_{ij}^{(l)}=\left\langle \tilde{\bm{v}}_{i}^{(l)},\tilde{\bm{v}}_{j}^{(l)}\right\rangle,\quad\bm{m}_{ij }=\varphi_{\bm{m}}\left(\bm{h}_{i},\bm{h}_{j},\bm{e}_{ij},d_{ij}^{2},\bigoplus _{l=0}^{L}z_{ij}^{(l)}\right),\] (6)

where \(\bigoplus\) refers to concatenation. It should be noted that the form of SO3KRATES introduced in the concurrent work [42] is equivalent to the expression for \(z_{ij}^{(l)}\) in Eq. (6). Furthermore, our scalarization trick simplifies the formulation by bypassing the Clebsch-Gordan coefficients, making it more straightforward and easier to understand.

**Aggregation of neighbor messages.** With the invariant message \(\bm{m}_{ij}\) at hand, we then update \(\bm{h}_{i},\bm{\vec{x}}_{i},\tilde{\bm{v}}_{i}^{(l)}\) via message aggregation over all neighbors. We define \(\Delta\bm{h}_{i},\Delta\bm{\vec{x}}_{i},\Delta\tilde{\bm{v}}_{i}^{(l)}\) as the residues, which are calculated by:

\[\Delta\bm{h}_{i}=\varphi_{\bm{h}}\left(\bm{h}_{i},\frac{1}{| \mathcal{N}(i)|}\sum_{j\in\mathcal{N}(i)}\bm{m}_{ij}\right),\;\Delta\bm{\vec{x} }_{i}=\frac{1}{|\mathcal{N}(i)|}\sum_{j\in\mathcal{N}(i)}\varphi_{\bm{\vec{x} }}(\bm{m}_{ij})\cdot(\bm{\vec{x}}_{i}-\bm{\vec{x}}_{j}),\] (7) \[\Delta\tilde{\bm{v}}_{i}^{(l)}=\frac{1}{|\mathcal{N}(i)|}\sum_{j \in\mathcal{N}(i)}\varphi_{\tilde{\bm{v}}}^{(l)}(\bm{m}_{ij})\cdot\left(\tilde {\bm{v}}_{i}^{(l)}-\tilde{\bm{v}}_{j}^{(l)}\right),\] (8)

Figure 2: The different architectures of our HEGNN, EGNN [1] and TFN [19]. HEGNN exploits the scalarization trick inspired by EGNN to enable steerable features to interact between different degrees, avoiding the high computational cost of using CG tensor products in TFN.

where \(\varphi_{\bm{h}},\varphi_{\bm{\tilde{v}}},\varphi_{\bm{\tilde{v}}}^{(l)}\) are different MLPs, and \(\varphi_{\bm{\tilde{x}}},\varphi_{\bm{\tilde{v}}}^{(l)}\) both output a 1D scalar. Note that the application of Eq. (8) for all degrees can be compactly rewritten as \(\bigoplus_{l=0}^{L}\Delta\bm{\tilde{v}}_{i}^{(l)}=\frac{1}{|\mathcal{N}(i)|} \sum_{j\in\mathcal{N}(i)}1\otimes_{\mathsf{cg}}^{\varphi_{\bm{(}}\bm{m}_{ij} \big{)}}\Big{(}\bigoplus_{l=0}^{L}\left(\widetilde{\bm{v}}_{i}^{(l)}-\tilde{ \bm{v}}_{j}^{(l)}\right)\Big{)}\) in the form of CG tensor product with the weights \(\varphi_{\bm{\tilde{v}}}(\bm{m}_{ij}):=\bigoplus_{l=0}^{L}\varphi_{\bm{\tilde{v }}}^{(l)}\). This form can be easily implemented using existing libraries such as e3nn.o3.FullyConnectedTensorProduct[39]. The resulting residues are used for the update:

\[\bm{h}_{i}=\bm{h}_{i}+\Delta\bm{h}_{i},\quad\bm{\vec{x}}_{i}=\bm{\vec{x}}_{i}+ \Delta\bm{\vec{x}}_{i},\quad\bm{\tilde{v}}_{i}^{(l)}=\tilde{v}_{i}^{(l)}+ \Delta\bm{\tilde{v}}_{i}^{(l)}.\] (9)

In addition, we can augment the update of \(\bm{\vec{x}}_{i}\) with 1st-degree feature \(\tilde{\bm{v}}_{i}^{(1)}\), leading to \(\bm{\vec{x}}_{i}=\bm{\vec{x}}_{i}+\Delta\bm{\vec{x}}_{i}+\phi_{\bm{\tilde{v}}} ^{(1)}(\bm{h}_{i})\tilde{\bm{v}}_{i}^{(1)}\), which yet is not explored in our experiments for the sake of simplicity. The final output of \(\bm{h}_{i}\) and \(\bm{\vec{x}}_{i}\) can be used for the node-level invariant prediction and equivariant prediction, respectively. We can also obtain a graph-level prediction by further adding a readout layer of all nodes.

We now analyze the expressivity of HEGNN. Apparently, by including high-degree features, HEGNN is able to avoid the loss of expressive ability even on symmetric graphs. Moreover, when tackling general geometric graphs, HEGNN is capable of characterizing the complete angle information of the input graph, if its maximal degree \(L\) is sufficiently large. For concision and without losing the generality, we assume the steerable features \(\tilde{\bm{v}}_{i}^{(1)}\) are initialized with only spherical harmonics without the weights \(\varphi_{\tilde{\bm{v}},\texttt{init}}^{(l)}\) in Eq. (5). Let \(\bm{\vec{x}}_{is}=(\bm{\vec{x}}_{i}-\bm{\vec{x}}_{s})/\|\bm{\vec{x}}_{i}-\bm{ \vec{x}}_{s}\|\), the inner product \(z_{ij}^{(l)}\) can be expanded as follows

\[\left\langle\sum_{s\in\mathcal{N}(i)}Y^{(l)}\left(\bm{\vec{x}}_{is}\right), \sum_{t\in\mathcal{N}(j)}Y^{(l)}\left(\bm{\vec{x}}_{jt}\right)\right\rangle= \frac{4\pi}{2l+1}\sum_{s\in\mathcal{N}(i)}\sum_{t\in\mathcal{N}(j)}P^{(l)} \left(\langle\bm{\vec{x}}_{is},\bm{\vec{x}}_{jt}\rangle\right),\] (10)

where \(P^{(l)}:\mathbb{R}\rightarrow\mathbb{R}\) is Legendre polynomial of degree \(l\), and Eq. (10) is based on the properties of spherical harmonics that \(\langle Y^{(l)}(\bm{\vec{x}}),Y^{(l)}(\bm{\vec{y}})\rangle=4\pi/(2l+1)\cdot P ^{(l)}(\langle\bm{\vec{x}},\bm{\vec{y}}\rangle),\|\bm{\vec{x}}\|=\|\bm{\vec{y}} \|=1\). We have the following result.

**Theorem 4.1**.: _For any geometric graph, there exists a bijection between the set of inner products \(\{z_{ij}^{(l)}\}_{l=1}^{|\mathbb{A}_{ij}|}\) given by Eq. (10) and the set of edge angles \(\mathbb{A}_{ij}=\{\theta_{is,jt}\coloneqq\arccos(\bm{\vec{x}}_{is},\bm{\vec{x}} _{jt})\}_{s\in\mathcal{N}(i),t\in\mathcal{N}(j)}\)._

Theorem 4.1 states that the inner products of full degrees can recover the information of all angles between each pair of edges, affirming the enhanced expressivity of our HEGNN. The proof is derived mainly based on the fact that Legendre polynomials are orthogonal polynomial bases which can injectivly represent the set \(\mathbb{A}_{ij}\) thanks to Newton's identities. The details are deferred to the appendix. Although the upper-bound of the degree in Theorem 4.1 grows rapidly with the graph size, it will be shown in our experiments that HEGNN with only \(L\leq 6\) is sufficient to outperform traditional models like EGNN [1] and TFN [19] in practice.

## 5 Experiment

### Expressivity on Symmetric Graphs

**Design of experiments:** To experimentally verify the conclusion we proved above, we design a more comprehensive experiment based on code5 in [35]. This experiment uses four \(k\)-fold structures (\(k\in\{2,3,5,10\}\)) and five convex regular polyhedra shown in Fig. 1 as test objects, and the center of each is at the origin. In detail, an arbitrary rotation in \(3D\) is acted on such symmetric structures called \(\mathcal{G}_{0}\) which ensures the geometric graph after rotation called \(\mathcal{G}_{1}\) does not coincide with the original one. The goal of our experiments is to check whether different equivariant neural networks can distinguish \(\mathcal{G}_{0}\) and \(\mathcal{G}_{1}\).

Footnote 5: https://github.com/chaitjo/geometric-gnn-dojo.

The models we select include two models that only use Cartesian coordinates: EGNN and GVP-GNN; and two models that use high-degree steerable features: TFN and MACE. However, TFN and MACE (denoted as TFN/MACE\({}_{l\leq L}\)) always use all degrees \(l\in\{0,\ldots,L\}\), so it is not clear which degree(s) of steerable features distinguish the two geometric graphs. In our HEGNN, all steerable features corresponding to unwanted degrees could be masked during initialization in Eq. (5), and we let HEGNN\({}_{l=L}\) be a HEGNN with only \(l\)th-degree steerable features. Additionally, to align with TFN/MACE, we also test the performance of HEGNN with all \(l\in\{0,1,\dots,L\}\) donated as HEGNN\({}_{l\leq L}\). Following the settings6 in [35], the output of each graph is the concatenation of invariant scalars, coordinates, and high-degree steerable features pooling among all nodes. We then map this spliced vector to a two-dimensional vector and input it into a simple classifier to determine whether the equivariant graph neural network can distinguish \(\mathcal{G}_{0}\) from \(\mathcal{G}_{1}\).

Footnote 6: It should be noted here that because only \(0\sim 11\)-th-degree spherical harmonics can be used in e3nn[39], we only measure the models with up to 11th-degree here, and in Appendix B.3, We have given a new verification method.

**Results:** The results on \(k\)-fold are deferred to Appendix for saving space, and the results on regular polyhedra are shown in Table 2. From Table 1, we can know steerable features in which degree could not distinguish specific symmetry structure, and both results on \(k\)-fold and regular polyhedra are also in perfect agreement with our conclusions. Models (EGNN and GVP-GNN) only with Cartesian vectors cannot distinguish any symmetric graph at all. Taking HEGNN\({}_{l=5}\) as an example, since \(\bm{D}^{(5)}(\bm{\hat{y}})=0,\forall\bm{\hat{y}}\in\{T,O,I\}\), no matter which kind of regular polyhedron, \(f^{(5)}\) could only output 0 thus failing to distinguish the structures.

### Physical Dynamics Simulation

**Datasets:** We benchmark our HEGNN in two scenarios, including: \(N\)**-body system**[43] is a dataset generated from simulations. In our simulations, each system contains \(N\) charged particles with random charge \(c_{i}\in\{0,1\}\), whose movements are driven by Coulomb forces. To verify the efficiency and effectiveness of our HEGNN on datasets of different sizes, we select \(N\) from \(\{5,20,50,100\}\). We use \(5000\) samples for training, \(2000\) for validation, and \(2000\) for testing. The task is to estimate the positions of the \(N\) particles after 1,000 timesteps. **MD17**[44] dataset contains trajectory data for eight molecules generated through molecular dynamics simulations. The goal of this experiment is to predict the future positions of the atoms based on their current state. We follow the dataset partitioning scheme from [45], splitting the dataset into 500/2000/2000 frame pairs for training, validation and testing, respectively. All experiments are run on a single NVIDIA A100-80G GPU.

**Baselines:** To demonstrate the advantages of our HEGNN over both models with scalarization techniques and models with high-degree steerable vectors at the same time, our baseline needs to consider the selection issues of both models simultaneously. Therefore, we select some representative models as baselines, including the invariant RF [46], the equivariant EGNN [1], TFN [19] and SE(3)-Tr. [20]. In addition, we select classical models such as Linear dynamics [1], the non-equivariant

\begin{table}
\begin{tabular}{c l c c c c c} \hline \hline  & \multicolumn{4}{c}{**Rotational symmetry**} \\  & **GNN Layer** & Tetrahedron & Cube & Octahedron & Dodecahedron & Icosahedron \\ \hline \multirow{4}{*}{\(N\)-body system} & E-GNN\({}_{l=1}\) & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & GVP-GNN\({}_{l=1}\) & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\ \hline \multirow{4}{*}{\(N\)-body system} & HEGNN\({}_{l=1}\) & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & HEGNN\({}_{l=2}\) & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & HEGNN\({}_{l=3}\) & **100.0 \(\pm\) 0.0** & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & HEGNN\({}_{l=4}\) & **100.0 \(\pm\) 0.0** & 90.0 \(\pm\) 0.0 & 90.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & HEGNN\({}_{l=5}\) & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & HEGNN\({}_{l=6}\) & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** \\  & HEGNN\({}_{l=7}\) & **100.0 \(\pm\) 0.0** & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & HEGNN\({}_{l=8}\) & **100.0 \(\pm\) 0.0** & 90.0 \(\pm\) 0.0 & 90.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & HEGNN\({}_{l=9}\) & **100.0 \(\pm\) 0.0** & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & HEGNN\({}_{l=10}\) & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & 95.0 \(\pm\) 0.0 & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** \\ \hline \multirow{4}{*}{\(N\)-body system} & HEGNN/TFN/MACE\({}_{l\leq 2}\) & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & HEGNN/TFN/MACE\({}_{l\leq 3}\) & **100.0 \(\pm\) 0.0** & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & HEGNN/TFN/MACE\({}_{l\leq 4}\) & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\  & HEGNN/TFN/MACE\({}_{l\leq 6}\) & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & 50.0 \(\pm\) 0.0 & 50.0 \(\pm\) 0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 2: _Regular polyhedra._Message Passing Neural Network (MPNN) [47], the invariant SchNet [48], and the equivariant GVP-GNN [49] for the \(N\)-body experiments. For MD17 experiments, we also select GMN [45].

**Metrics: 1. Loss function:** We use Mean Squared Error (MSE) to measure the accuracy of the prediction results in both experiments. **2. Inference time:** Given that the \(N\)-body system we use contains data of varying sizes, we test the inference time of each model on this dataset. The inference time for each model is calculated relative to the benchmark, which is the inference time of EGNN at the corresponding scale.

**Results on \(N\)-Body systems:** The main results of \(N\)-body system simulation are presented in Table 3. From these results, we observe the following: **1. Overall performance**: Our HEGNN significantly outperforms other models across datasets of all sizes. Although EGNN [1] performs better than high-degree steerable models like TFN or \(\mathrm{SE}(3)\)-Transformer in this task, our HEGNN is still better than EGNN, which show that the method of HEGNN introducing high-degree steerable features is more effective. **2. Stability**: Although the performance of the model (\(\mathrm{HEGNN}_{l\leqslant 6}\)) using high-degree steerable features declines slightly when the geometric graph is small, overall, HEGNN performs better than other models. **3. Inference time**: Our model's inference time is significantly faster than that of high-degree steerable models like TFN, reflecting the simplicity and efficiency of our HEGNN.

**Results on MD17:** The main results of MD17 experiment are shown in Table 4, with some data sourced from [45]. From these results, we draw the following insights: **1. Overall performance:** Our HEGNN outperforms other models on six out of eight molecules. The effect on the remaining two molecules is only not as good as GMN [45] and this is because GMN introduces additional knowledge such as chemical bonds. **2. Advantage of high-degree vectors:** Most of the best results are obtained on \(\mathrm{HEGNN}_{l\leqslant 6}\), indicating that the use of high-degree steerable features can enhance model expression capabilities.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & Aspirin & Benzene & Ethanol & Malonaldehyde & Naphthalene & Salicylic & Toluene & Uracil \\ \hline RF & 10.94\(\pm\)0.01 & 103.72\(\pm\)1.29 & 4.64\(\pm\)0.01 & 13.93\(\pm\)0.03 & 0.50\(\pm\)0.01 & 1.23\(\pm\)0.01 & 10.93\(\pm\)0.04 & 0.64\(\pm\)0.01 \\ EGNN & 14.41\(\pm\)0.15 & 62.40\(\pm\)0.53 & 4.64\(\pm\)0.01 & 13.64\(\pm\)0.01 & 0.47\(\pm\)0.02 & 1.02\(\pm\)0.02 & 11.78\(\pm\)0.07 & 0.64\(\pm\)0.01 \\ EGNNReg & 13.82\(\pm\)0.19 & 61.68\(\pm\)0.37 & 6.06\(\pm\)0.01 & 13.49\(\pm\)0.06 & 0.63\(\pm\)0.01 & 1.68\(\pm\)0.01 & 11.05\(\pm\)0.01 & 0.66\(\pm\)0.01 \\ GMN & 10.14\(\pm\)0.03 & **48.12\(\pm\)**0.40 & 4.83\(\pm\)0.01 & 13.11\(\pm\)0.03 & 0.40\(\pm\)0.01 & 0.91\(\pm\)0.01 & **10.22\(\pm\)**0.08 & 0.59\(\pm\)0.01 \\ \hline \(\mathrm{TFN}_{l\leqslant 2}\) & 12.37\(\pm\)0.18 & 58.48\(\pm\)1.98 & 4.81\(\pm\)0.04 & 13.62\(\pm\)0.08 & 0.49\(\pm\)0.01 & 1.03\(\pm\)0.02 & 10.89\(\pm\)0.01 & 0.84\(\pm\)0.02 \\ SE(3)-\(\mathrm{Tr}_{l\leqslant 2}\) & 11.12\(\pm\)0.06 & 68.11\(\pm\)0.67 & 4.74\(\pm\)0.13 & 13.89\(\pm\)0.02 & 0.52\(\pm\)0.01 & 1.13\(\pm\)0.02 & 10.88\(\pm\)0.06 & 0.79\(\pm\)0.02 \\ \hline \(\mathrm{HEGNN}_{l\leqslant 1}\) & 10.32\(\pm\)0.58 & 62.53\(\pm\)7.62 & 4.63\(\pm\)0.01 & **12.85\(\pm\)**0.01 & 0.38\(\pm\)0.01 & 0.90\(\pm\)0.05 & 10.56\(\pm\)0.10 & 0.56\(\pm\)0.02 \\ \(\mathrm{HEGNN}_{l\leqslant 2}\) & 10.04\(\pm\)0.45 & 61.80\(\pm\)5.92 & 4.63\(\pm\)0.01 & **12.85\(\pm\)**0.01 & 0.39\(\pm\)0.01 & 0.91\(\pm\)0.06 & 10.56\(\pm\)0.05 & 0.55\(\pm\)0.01 \\ \(\mathrm{HEGNN}_{l\leqslant 3}\) & 10.20\(\pm\)0.23 & 62.82\(\pm\)2.45 & 4.63\(\pm\)0.01 & **12.85\(\pm\)**0.02 & **0.37\(\pm\)**0.01 & 0.94\(\pm\)0.10 & 10.55\(\pm\)0.16 & **0.52\(\pm\)**0.01 \\ \(\mathrm{HEGNN}_{l\leqslant 6}\) & **9.94\(\pm\)**0.07 & 59.93\(\pm\)5.21 & **4.62\(\pm\)**0.01 & **12.85\(\pm\)**0.01 & **0.37\(\pm\)**0.02 & **0.88\(\pm\)**0.02 & 10.56\(\pm\)0.33 & 0.54\(\pm\)0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Prediction error (\(\times 10^{-2}\)) on MD17 dataset. Results averaged across 3 runs.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{\(5\)-body} & \multicolumn{2}{c}{\(20\)-body} & \multicolumn{2}{c}{\(50\)-body} & \multicolumn{2}{c}{\(100\)-body} \\  & MSE & Relative & MSE & Relative & MSE & Relative & MSE & Relative & MSE & Relative \\  & (\(\times 10^{-2}\)) & Time & (\(\times 10^{-2}\)) & Time & (\(\times 10^{-2}\)) & Time & (\(\times 10^{-2}\)) & Time \\ \hline Linear & \(7.72\) & \(0.01\) & \(10.12\) & \(0.02\) & \(11.81\) & \(0.02\) & \(12.69\) & \(0.01\) \\ MPNN [47] & \(1.80\) & \(0.49\) & \(2.50\) & \(0.51\) & \(2.96\) & \(0.50\) & \(3.55\) & \(0.45\) \\ SchNet [48] & \(11.31\) & \(2.93\) & \(17.72\) & \(6.24\) & \(22.14\) & \(31.63\) & \(22.14\) & \(27.04\) \\ RF [46] & \(1.51\) & \(0.54\) & \(3.41\) & \(0.65\) & \(4.75\) & \(0.67\) & \(5.72\) & \(0.49\) \\ GVP-GNN [49] & \(7.26\) & \(2.36\) & \(5.76\) & \(2.38\) & \(7.07\) & \(2.42\) & \(7.55\) & \(2.33\) \\ EGNN [1] & \(0.65\) & \(1.00\) & \(1.01\) & \(1.00\) & \(1.00\) & \(1.00\) & \(1.36\) & \(1.00\) \\ \hline \(\mathrm{TFN}_{l\leqslant 2}\) & \(1.49\) & \(2.69\) & \(1.86\) & \(3.19\) & \(2.20\) & \(2.87\) & \(3.42\) & \(6.58\) \\ \(\mathrm{TFN}_{l\leqslant 3}\) & \(1.76\) & \(3.91\) & \(1.87\) & \(4.54\) & \(1.94\) & \(4.89\) & **OOM** & - \\ \(\mathrm{SE}(3)\)-\(\mathrm{Tr}_{l\leqslant 2}\) & \(3.24\) & \(4.94\) & \(3.19\) & \(5.88\) & \(2.54\) & \(5.97\) & \(2.33\) & \(5.15\) \\ \hline \(\mathrm{HEGNN}_{l\leqslant 1}\) & \(0.52\) & \(1.77\) & \(0.79\) & \(1.84\) & \(0.88\) & \(1.60\) & \(1.13\) & \(1.45\) \\ \(\mathrm{HEGNN}_{l\leqslant 2}\)

### Perturbation Experiment

In practical scenarios, slight perturbations (such as molecular vibrations) can disrupt strict symmetry, potentially mitigating the conclusions outlined in Theorems 3.5 and 3.6. We therefore designed this perturbation experiment for a simple study and were surprised to find that HEGNN can still bring better robustness through the introduction of high-degree steerable features.

**Design of experiments:** We take the tetrahedron as an example and compare the cases of EGNN, \(\text{HEGNN}_{l=3}\), and \(\text{HEGNN}_{l\leq 3}\) when adding noise perturbations with results in Table 5. Here, \(\varepsilon\) represents the ratio of noise, and the modulus of the noise obeys \(\mathcal{N}(0,\varepsilon\cdot\mathbb{E}[\|\vec{x}-\vec{x}_{c}\|]\cdot I)\).

**Results:** It can be observed that the performance of EGNN is slightly improved in the presence of noise (from 50% when \(\varepsilon=0.01\) to 60% when \(\varepsilon=0.5\)), while HEGNN demonstrates better robustness. Even though symmetry-breaking factors will make the geometric graph deviate from the symmetric state, the deviated graph is still roughly symmetric. In other words, the outputs of equivariant GNNs on the derivated graphs keep close to zero if the degree value is chosen to be those in Table 1, which will still lead to defective performance.

## 6 Conclusion

In this paper, we challenged the prevailing notion that higher-degree steerable vectors are unnecessary for achieving expressivity in equivariant Graph Neural Networks (GNNs). Through rigorous theoretical analysis, we demonstrated that equivariant GNNs constrained to 1st-degree representations inevitably degenerate to zero functions when applied to symmetric structures, such as \(k\)-fold rotations and regular polyhedra. To address this limitation, we introduced HEGNN, a high-degree extension of the EGNN model. HEGNN enhances expressivity by integrating higher-degree steerable vectors while retaining the efficiency of the original model through a scalarization technique. Our extensive empirical evaluations on various datasets, including the symmetric toy dataset, \(N\)-body, and MD17, validate our theoretical predictions. HEGNN not only adheres to our theoretical insights but also exhibits significant performance improvements over existing models. These findings underscore the critical role of higher-degree representations in fully leveraging the potential of equivariant GNNs.

## 7 Acknowledgment

This work was jointly supported by the following projects: the National Science and Technology Major Project under Grant 2020AAA0107300, the National Natural Science Foundation of China (No. 62376276, No. 62172422); Beijing Nova Program (No. 20230484278); Beijing Outstanding Young Scientist Program (No. BJJWZYJH012019100020098), the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China (23XNKJ19); Public Computing Cloud, Renmin University of China.

## References

* [1] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. In _International Conference on Machine Learning_, pages 9323-9332. PMLR, 2021.
* [2] Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges, 2021.
* [3] Rui Jiao, Xiangzhe Kong, Ziyang Yu, Wenbing Huang, and Yang Liu. Equivariant pretrained transformer for unified geometric learning on multi-domain 3d molecules. In _ICLR 2024 Workshop on Generative and Experimental Perspectives for Biomolecular Design_, 2024.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \(\varepsilon=0.01\) & \(\varepsilon=0.05\) & \(\varepsilon=0.10\) & \(\varepsilon=0.50\) \\ \hline EGNN & 50.0 \(\pm\)0.0 & 45.0 \(\pm\) 15.0 & 65.0 \(\pm\) 22.9 & 60.0 \(\pm\) 20.0 \\ \(\text{HEGNN}_{l=3}\) & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** \\ \(\text{HEGNN}_{l\leq 3}\) & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** & **100.0 \(\pm\) 0.0** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results for perturbation experiment.

* [4] Jun Wu, Xiangzhe Kong, Ningguan Sun, Jing Wei, Sisi Shan, Fuli Feng, Feng Wu, Jian Peng, Linqi Zhang, Yang Liu, et al. Better prior distribution for antibody design. _Available at SSRN 4909414_, 2024.
* [5] Thorben Frank, Oliver Unke, and Klaus-Robert Muller. So3krates: Equivariant attention for interactions on arbitrary length-scales in molecular systems. _Advances in Neural Information Processing Systems_, 35:29400-29413, 2022.
* [6] Yusong Wang, Tong Wang, Shaoning Li, Xinheng He, Mingyu Li, Zun Wang, Nanning Zheng, Bin Shao, and Tie-Yan Liu. Enhancing geometric representations for molecules with equivariant vector-scalar interactive message passing. _Nature Communications_, 15(1):313, 2024.
* [7] Runfa Chen, Jiaqi Han, Fuchun Sun, and Wenbing Huang. Subequivariant graph reinforcement learning in 3d environments. In _International Conference on Machine Learning_, pages 4545-4565. PMLR, 2023.
* [8] Runfa Chen, Ling Wang, Yu Du, Tianrui Xue, Fuchun Sun, Jianwei Zhang, and Wenbing Huang. Subequivariant reinforcement learning in 3d multi-entity physical environments. In _Forty-first International Conference on Machine Learning_, 2024.
* [9] Jiaqi Han, Wenbing Huang, Hengbo Ma, Jiachen Li, Josh Tenenbaum, and Chuang Gan. Learning physical dynamics with subequivariant graph neural networks. _Advances in Neural Information Processing Systems_, 35:26256-26268, 2022.
* [10] Liming Wu, Zhichao Hou, Jirui Yuan, Yu Rong, and Wenbing Huang. Equivariant spatio-temporal attentive graph networks to simulate physical dynamics. _Advances in Neural Information Processing Systems_, 36, 2024.
* [11] Jiaqi Han, Minkai Xu, Aaron Lou, Haotian Ye, and Stefano Ermon. Geometric trajectory diffusion models. _arXiv preprint arXiv:2410.13027_, 2024.
* [12] Yuxuan Song, Jingjing Gong, Hao Zhou, Mingyue Zheng, Jingjing Liu, and Wei-Ying Ma. Unified generative modeling of 3d molecules with bayesian flow networks. In _The Twelfth International Conference on Learning Representations_, 2023.
* [13] Minkai Xu, Alexander S Powers, Ron O Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In _International Conference on Machine Learning_, pages 38592-38610. PMLR, 2023.
* [14] Yanru Qu, Keyue Qiu, Yuxuan Song, Jingjing Gong, Jiawei Han, Mingyue Zheng, Hao Zhou, and Wei-Ying Ma. Molcraft: Structure-based drug design in continuous parameter space. In _Forty-first International Conference on Machine Learning_, 2024.
* [15] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In _International Conference on Learning Representations_, 2022.
* [16] Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, et al. De novo design of protein structure and function with rfdiffusion. _Nature_, 620(7976):1089-1100, 2023.
* [17] John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana M Lord, Christopher Ng-Thow-Hing, Erik R Van Vlack, et al. Illuminating protein space with a programmable generative model. _Nature_, pages 1-9, 2023.
* [18] Ziyang Yu, Wenbing Huang, and Yang Liu. Rigid protein-protein docking via equivariant elliptic-paraboloid interface prediction. In _The Twelfth International Conference on Learning Representations_, 2024.
* [19] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds. _arXiv preprint arXiv:1802.08219_, 2018.

* Fuchs et al. [2020] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d roto-translation equivariant attention networks. _Advances in Neural Information Processing Systems_, 33:1970-1981, 2020.
* Batzner et al. [2022] Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. _Nature communications_, 13(1):2453, 2022.
* Musaelian et al. [2023] Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. _Nature Communications_, 14(1):579, 2023.
* Dym and Maron [2020] Nadav Dym and Haggai Maron. On the universality of rotation equivariant point cloud networks. _arXiv preprint arXiv:2010.02449_, 2020.
* Hoogeboom et al. [2022] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In _International Conference on Machine Learning_, pages 8867-8887. PMLR, 2022.
* Xu et al. [2024] Minkai Xu, Jiaqi Han, Aaron Lou, Jean Kossaifi, Arvind Ramanathan, Kamyar Azizzadenesheli, Jure Leskovec, Stefano Ermon, and Anima Anandkumar. Equivariant graph neural operator for modeling 3d dynamics. _arXiv preprint arXiv:2401.11037_, 2024.
* Puny et al. [2021] Omri Puny, Matan Atzmon, Edward J Smith, Ishan Misra, Aditya Grover, Heli Ben-Hamu, and Yaron Lipman. Frame averaging for invariant and equivariant network design. In _International Conference on Learning Representations_, 2021.
* Duval et al. [2023] Alexandre Agm Duval, Victor Schmidt, Alex Hernandez-Garcia, Santiago Miret, Fragkiskos D Malliaros, Yoshua Bengio, and David Rolnick. Faenet: Frame averaging equivariant gnn for materials modeling. In _International Conference on Machine Learning_, pages 9013-9033. PMLR, 2023.
* Zhang et al. [2024] Yuelin Zhang, Jiacheng Cen, Jiaqi Han, Zhiqiang Zhang, Jun Zhou, and Wenbing Huang. Improving equivariant graph neural networks on large geometric graphs via virtual nodes learning. In _Forty-first International Conference on Machine Learning_, 2024.
* Han et al. [2022] Jiaqi Han, Wenbing Huang, Tingyang Xu, and Yu Rong. Equivariant graph hierarchy-based neural networks. _Advances in Neural Information Processing Systems_, 35:9176-9187, 2022.
* Wang et al. [2024] Yusong Wang, Chaoran Cheng, Shaoning Li, Yuxuan Ren, Bin Shao, Ge Liu, Pheng-Ann Heng, and Nanning Zheng. Neural p\({}^{3}\) m: A long-range interaction modeling enhancer for geometric gnns. _Advances in Neural Information Processing Systems_, 2024.
* Han et al. [2024] Jiaqi Han, Jiacheng Cen, Liming Wu, Zongzhao Li, Xiangzhe Kong, Rui Jiao, Ziyang Yu, Tingyang Xu, Fandi Wu, Zihe Wang, et al. A survey of geometric graph neural networks: Data structures, models and applications. _arXiv preprint arXiv:2403.00485_, 2024.
* Schutt et al. [2021] Kristof Schutt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In _International Conference on Machine Learning_, pages 9377-9388. PMLR, 2021.
* Brandstetter et al. [2021] Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. Geometric and physical quantities improve e (3) equivariant message passing. In _International Conference on Learning Representations_, 2021.
* Gasteiger et al. [2021] Johannes Gasteiger, Florian Becker, and Stephan Gunnemann. Gemnet: Universal directional graph neural networks for molecules. _Advances in Neural Information Processing Systems_, 34:6790-6802, 2021.
* Joshi et al. [2023] Chaitanya K Joshi, Cristian Bodnar, Simon V Mathis, Taco Cohen, and Pietro Lio. On the expressive power of geometric graph neural networks. In _International Conference on Machine Learning_, pages 15330-15355. PMLR, 2023.

* [36] Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which appears therein. _nti, Series_, 2(9):12-16, 1968.
* [37] Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and Gabor Csanyi. Mace: Higher order equivariant message passing neural networks for fast and accurate force fields. _Advances in Neural Information Processing Systems_, 35:11423-11436, 2022.
* [38] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3d steerable cnns: Learning rotationally equivariant features in volumetric data. _Advances in Neural Information Processing Systems_, 31, 2018.
* [39] Mario Geiger and Tess Smidt. e3nn: Euclidean neural networks. _arXiv preprint arXiv:2207.09453_, 2022.
* [40] David J Griffiths and Darrell F Schroeter. _Introduction to quantum mechanics_. Cambridge university press, 2018.
* [41] Lev Davidovich Landau and Evgenii Mikhailovich Lifshitz. _Quantum mechanics: non-relativistic theory_, volume 3. Elsevier, 2013.
* [42] J Thorben Frank, Oliver T Unke, Klaus-Robert Muller, and Stefan Chmiela. A euclidean transformer for fast and stable machine learned force fields. _Nature Communications_, 15(1):6539, 2024.
* [43] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In _International Conference on Machine Learning_, pages 2688-2697. PMLR, 2018.
* [44] Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Schutt, and Klaus-Robert Muller. Machine learning of accurate energy-conserving molecular force fields. _Science advances_, 3(5):e1603015, 2017.
* [45] Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Equivariant graph mechanics networks with constraints. In _International Conference on Learning Representations_, 2022.
* [46] Jonas Kohler, Leon Klein, and Frank Noe. Equivariant flows: sampling configurations for multi-body systems with symmetric energies. _arXiv preprint arXiv:1910.00753_, 2019.
* [47] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International Conference on Machine Learning_, pages 1263-1272. PMLR, 2017.
* [48] Kristof T Schutt, Huziel E Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Muller. Schnet-a deep learning architecture for molecules and materials. _The Journal of Chemical Physics_, 148(24), 2018.
* [49] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. In _International Conference on Learning Representations_, 2020.
* [50] Weitao Du, He Zhang, Yuanqi Du, Qi Meng, Wei Chen, Nanning Zheng, Bin Shao, and Tie-Yan Liu. Se (3) equivariant graph neural networks with complete local frames. In _International Conference on Machine Learning_, pages 5583-5608. PMLR, 2022.
* [51] James F Epperson. _An introduction to numerical methods and analysis_. John Wiley & Sons, 2013.
* [52] Michael Engel. Point group analysis in particle simulation data. _arXiv preprint arXiv:2106.14846_, 2021.
* [53] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. _Advances in Neural Information Processing Systems_, 30, 2017.

* [54] Hannah Lawrence, Vasco Portilleiro, Yan Zhang, and Sekou-Oumar Kaba. Improving equivariant networks with probabilistic symmetry breaking. In _ICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling_, 2024.
* [55] Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. Comenet: Towards complete and efficient message passing for 3d molecular graphs. _Advances in Neural Information Processing Systems_, 35:650-664, 2022.
* [56] Kelin Xia and Guo-Wei Wei. Persistent homology analysis of protein structure, flexibility, and folding. _International journal for numerical methods in biomedical engineering_, 30(8):814-844, 2014.
* [57] Floor Eijkelboom, Rob Hesselink, and Erik J Bekkers. E(n) equivariant message passing simplicial networks. In _International Conference on Machine Learning_, pages 9071-9081. PMLR, 2023.
* [58] Claudio Battiloro, Ege Karaismailoglu, Mauricio Tec, George Dasoulas, Michelle Audirac, and Francesca Dominici. E (n) equivariant topological neural networks. _arXiv preprint arXiv:2405.15429_, 2024.
* [59] Ralf Drautz. Atomic cluster expansion for accurate and transferable interatomic potentials. _Physical Review B_, 99(1):014104, 2019.
* [60] Genevieve Dusson, Markus Bachmayr, Gabor Csanyi, Ralf Drautz, Simon Etter, Cas van Der Oord, and Christoph Ortner. Atomic cluster expansion: Completeness, efficiency and stability. _Journal of Computational Physics_, 454:110946, 2022.
* [61] Xiangzhe Kong, Wenbing Huang, Zhixing Tan, and Yang Liu. Molecule generation by principal subgraph mining and assembling. _Advances in Neural Information Processing Systems_, 35:2550-2563, 2022.
* [62] Xiangzhe Kong, Wenbing Huang, and Yang Liu. Generalist equivariant transformer towards 3d molecular interaction learning. In _Forty-first International Conference on Machine Learning_, 2023.
* [63] Xiangzhe Kong, Wenbing Huang, and Yang Liu. Full-atom peptide design with geometric latent diffusion. _Advances in Neural Information Processing Systems_, 2024.
* [64] Militadis Kofinas, Naveen Nagaraja, and Efstratios Gavves. Roto-translated local coordinate frames for interacting dynamical systems. _Advances in Neural Information Processing Systems_, 34:6417-6429, 2021.
* [65] Yi-Lun Liao, Brandon M Wood, Abhishek Das, and Tess Smidt. Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations. In _The Twelfth International Conference on Learning Representations_, 2024.
* [66] Ziqiao Meng, Liang Zeng, Zixing Song, Tingyang Xu, Peilin Zhao, and Irwin King. Towards geometric normalization techniques in se(3) equivariant graph neural networks for physical dynamics simulations. In _Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence (IJCAI)_, pages 5981-5989, 2024.
* [67] Zian Li, Xiyuan Wang, Yinan Huang, and Muhan Zhang. Is distance matrix enough for geometric deep learning? _Advances in Neural Information Processing Systems_, 36, 2024.
* [68] Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: A universal 3d molecular representation learning framework. In _The Eleventh International Conference on Learning Representations_, 2023.
* [69] Weiliang Luo, Gengmo Zhou, Zhengdan Zhu, Yannan Yuan, Guolin Ke, Zhewei Wei, Zhifeng Gao, and Hang Zheng. Bridging machine learning and thermodynamics for accurate p k a prediction. _JACS Au_, 2024.
* [70] Gengmo Zhou, Zhen Wang, Feng Yu, Guolin Ke, Zhewei Wei, and Zhifeng Gao. S-molsearch: 3d semi-supervised contrastive learning for bioactive molecule search. In _The Thirty-eighth Annual Conference on Neural Information Processing Systems_, 2024.

* [71] Fanmeng Wang, Hongteng Xu, Xi Chen, Shuqi Lu, Yuqing Deng, and Wenbing Huang. Mperformer: An se (3) transformer-based molecular perceptron. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, pages 2512-2522, 2023.
* [72] Fanmeng Wang, Wentao Guo, Minjie Cheng, Shen Yuan, Hongteng Xu, and Zhifeng Gao. Mmpolymer: A multimodal multitask pretraining framework for polymer property prediction. In _Proceedings of the 33rd ACM International Conference on Information and Knowledge Management_, CIKM '24, 2024.
* [73] Johannes Klicpera, Janek Gross, and Stephan Gunnemann. Directional message passing for molecular graphs. In _International Conference on Learning Representations_, 2020.
* [74] Angxiao Yue, Dixin Luo, and Hongteng Xu. A plug-and-play quaternion message-passing module for molecular conformation representation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 16633-16641, 2024.
* [75] Yang Zhang, Wenbing Huang, Zhewei Wei, Ye Yuan, and Zhaohan Ding. Equipocket: an e (3)-equivariant geometric graph neural network for ligand binding site prediction. In _Forty-first International Conference on Machine Learning_, 2024.
* [76] Rui Jiao, Jiaqi Han, Wenbing Huang, Yu Rong, and Yang Liu. Energy-motivated equivariant pretraining for 3d molecular graphs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 8096-8104, 2023.
* [77] Ziyang Yu, Wenbing Huang, and Yang Liu. Force-guided bridge matching for full-atom time-coarsened dynamics of peptides. _arXiv preprint arXiv:2408.15126_, 2024.
* [78] Qi Li, Rui Jiao, Liming Wu, Tiannan Zhu, Wenbing Huang, Shifeng Jin, Yang Liu, Hongming Weng, and Xiaolong Chen. Powder diffraction crystal structure determination using generative models. _arXiv preprint arXiv:2409.04727_, 2024.
* [79] Rui Jiao, Wenbing Huang, Peijia Lin, Jiaqi Han, Pin Chen, Yutong Lu, and Yang Liu. Crystal structure prediction by joint equivariant diffusion. _Advances in Neural Information Processing Systems_, 36, 2024.
* [80] Rui Jiao, Xiangzhe Kong, Wenbing Huang, and Yang Liu. 3d structure prediction of atomic systems with flow-based direct preference optimization. In _The Thirty-eighth Annual Conference on Neural Information Processing Systems_, 2024.
* [81] Xiangzhe Kong, Wenbing Huang, and Yang Liu. Conditional antibody design as 3d equivariant graph translation. In _The Eleventh International Conference on Learning Representations_, 2022.
* [82] Xiangzhe Kong, Wenbing Huang, and Yang Liu. End-to-end full-atom antibody design. In _Proceedings of the 40th International Conference on Machine Learning_, pages 17409-17429, 2023.
* [83] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. _Nature Methods_, 17:261-272, 2020.

Theoretical Details

### Equivariance/Invariance of HEGNN

In this section, we demonstrate the equivariance of our HEGNN. In order to further illustrate the connection between our HEGNN, EGNN, and TFN, a general proof is given here.

**Theorem A.1** (Equivariance/Invariance of HEGNN).: \(\bm{h}_{i},\tilde{\bm{v}}_{i}^{(0)}\) _in HEGNN is \(\mathrm{E}(3)\) invariant, \(\bm{\vec{x}}_{i}\) is \(\mathrm{E}(3)\) equivariant. In addition, all \(\bm{\tilde{v}}_{i}^{(l)}\) are \(O(3)\) equivariant and translation invariant when \(l\) is odd; all \(\bm{\tilde{v}}_{i}^{(l)}\) is \(SO(3)\) equivariant and inversion/translation invariant to when \(l\) even._

Proof.: Consider a sequence composed of functions \(\{\varphi_{i}:\mathcal{X}^{(i-1)}\rightarrow\mathcal{X}^{(i)}\}_{i=1}^{N}\) equivariant to a same group \(\mathfrak{H}\), the equivariance lead to an interesting property that

\[\varphi_{N}\circ\cdots\circ\varphi_{i+1}\circ\rho_{\mathcal{X}^{(i)}}( \mathfrak{h})\varphi_{i}\circ\cdots\circ\varphi_{1}=\varphi_{N}\circ\cdots \circ\varphi_{j+1}\circ\rho_{\mathcal{X}^{(j)}}(\mathfrak{h})\varphi_{j}\circ \cdots\circ\varphi_{1},\]

holds for all \(i,j\in\{1,2,\ldots,N\}\) and \(\mathfrak{h}\in\mathfrak{H}\), which means that the group elements \(\mathfrak{h}\) can be freely exchanged in the composite sequence of equivariant functions. In particular, if one of the equivariant functions (_e.g._\(\varphi_{k}\)) is replaced by an invariant function, the group element \(\mathfrak{h}\) will be absorbed, which means

\[\varphi_{N}\circ\cdots\circ\varphi_{k}\circ\cdots\circ\varphi_{i+1}\circ\rho_ {\mathcal{X}^{(i)}}(\mathfrak{h})\varphi_{i}\circ\cdots\circ\varphi_{1}= \varphi_{N}\circ\cdots\circ\varphi_{1}.\]

holds for all \(\mathfrak{h}\in\mathfrak{H}\) but only \(i\in\{1,2,\ldots,k\}\). Although \(\varphi_{N}\circ\cdots\circ\varphi_{k}\) is still equivariant, because the group elements must be input starting from \(\varphi_{1}\), the overall \(\varphi_{N}\circ\cdots\circ\varphi_{1}\) is still an invariant function. That is to say, to conclude that the entire HEGNN is equivariant, we only need to prove that HEGNN is equivariant in initialization and each layer.

The initialization of HEGNN is based on spherical harmonics, which is similar to TFN. Spherical harmonics are inherently equivariant, that is,

\[Y^{(l)}(\bm{R}_{\bm{\mathrm{r}}}\bm{\vec{x}})=\bm{D}^{(l)}(\mathfrak{r})Y^{( l)}(\bm{\vec{x}}).\]

Note that variables participating in the coefficient in Eq. (5) are all invariant scalars, so the initialization of HEGNN is consistent with the spherical harmonic function. Note that for Cartesian vectors, they can be aligned by arranging the spherical harmonics of 1st degree [19], that is,

\[\bm{\vec{x}}\propto Y^{(1)}(\bm{\vec{x}}).\]

From this perspective, EGNN can also be considered to be initialized using spherical harmonics, but this step is omitted because the value is proportional to the input Cartesian vector.

It is worth explaining that spherical harmonics are inversion invariant when \(l\) is even, that is,

\[Y^{(l)}(\mathfrak{m}\bm{\vec{x}})=Y^{(l)}(\bm{\vec{x}}).\]

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & \multicolumn{1}{c}{EGNN [1]} & \multicolumn{1}{c}{TFN [19]} & \multicolumn{1}{c}{HEGNN (Ours)} \\ \hline \multirow{3}{*}{Msg} & \(\bm{m}_{ij}=\phi_{\bm{m}}(\bm{h}_{i},\bm{h}_{j},\bm{e}_{ij},d_{ij}^{2})\) & \(\bm{m}_{ij}=\tilde{\bm{v}}_{i}^{(\mathbb{L})}\otimes_{\mathbb{G}}^{\bm{W}(d_{ ij})}Y^{(\mathbb{L})}\left(\frac{\bm{\vec{x}}_{i}}{\|\bm{\vec{x}}_{ij}\|}\right)\) & \(\bm{m}_{ij}=\varphi_{\bm{m}}(\bm{h}_{i},\bm{h}_{j},\bm{e}_{ij},d_{ij}^{2}, \bm{\hat{\mathfrak{D}}}_{\mathrm{f}_{\mathrm{f}_{\mathrm{f}_{\mathrm{f}_{ \mathrm{f}_{\mathrm{f}_{\mathrm{f}_{\mathrm{f}_{\mathrm{f}_{\mathrm{f}_{ \mathrm{f}_{\mathrm{f_{f}}}}}}}}}}}}}}\bm{\hat{v}}_{ij}^{(l)}=\varphi_{\bm{\hat{x}} }(\bm{m}_{ij})\cdot(\bm{\vec{x}}_{i}-\bm{\vec{x}}_{j})\) \\  & \(\bm{\hat{m}}_{ij}=\varphi_{\bm{\hat{x}}}(\bm{m}_{ij})\cdot(\bm{\vec{x}}_{i}- \bm{\vec{x}}_{j})\) & \(\bm{\hat{v}}_{ij}^{(l)}=\varphi_{\bm{\hat{v}}}^{(l)}(\bm{m}_{ij})\cdot(\bm{ \vec{v}}_{i}^{(l)}-\bm{\hat{v}}_{j}^{(l)})\) \\  & & & \(\bm{m}_{i}=\alpha_{i}\sum_{j\in\mathcal{N}^{(i)}}\bm{m}_{ij}\) \\ \multirow{3}{*}{Agg} & \(\bm{m}_{i}=\alpha_{i}\sum_{j\in\mathcal{N}^{(i)}}\bm{m}_{ij}\) & \(\bm{\hat{m}}_{i}^{(\mathbb{L})}=\alpha_{i}\sum_{j\in\mathcal{N}^{(i)}}\bm{\hat{ m}}_{ij}^{(\mathbb{L})}\) & \(\bm{\hat{m}}_{i}=\alpha_{i}\sum_{j\in\mathcal{N}^{(i)}}\bm{\hat{m}}_{ij}\) \\  & \(\bm{\hat{m}}_{i}=Equivariant ones are not necessarily better than invariant ones. When we need to predict pseudovectors (such as moments), we need to inversion invariant 1st-degree steerable features, because pseudovectors are inversion invariant. This is why introducing the cross product \(\vec{\bm{x}}\times\vec{\bm{y}}\) (the result is inversion invariant) into a linear combination can only build a \(\mathrm{SE}(3)\) equivariant network, but not a \(\mathrm{E}(3)\) network [50].

In fact, inversion equivariant/invariant high degree steerable features can be obtained by calculating the CG tensor product of spherical harmonics and inversion equivariant Cartesian vectors like \(\vec{\bm{x}}_{i}-\vec{\bm{x}}_{c}\)[39]. Moreover, the equivariance at each layer is also easy to prove and the internal Wigner-D matrix can be extracted through the CG tensor product [31].

\[\bm{D}^{(l)}(\mathfrak{h})\tilde{\bm{v}}^{(l)}=\left[\left(\bm{D}^{(l_{1})}( \mathfrak{h})\tilde{\bm{v}}^{(l_{1})}\right)\otimes_{\mathrm{cg}}^{\bm{W}} \left(\bm{D}^{(l_{2})}(\mathfrak{h})\tilde{\bm{v}}^{(l_{2})}\right)\right]^{( l)}.\]

When the output result is an invariant scalar, the Wigner-D matrix degenerates into a trivial representation \(1\). Norm and inner product are all special cases of this type, so equivariance is established. From this perspective, EGNN and HEGNN are equivalent to using only the weight coefficients of \((l,\,l)\to 0\) and \((0,\,l)\to l\). Similar ideas include the steerable MLPs in [33]. 

### Other Proofs

**Theorem A.2** (Theorem 3.4).: _Suppose that \(f^{(l)}\) is an \(\mathrm{O}(3)\)-equivariant function on geometric graphs, regarding the group representation \(\rho^{(l)}\) defined in Eq. (2). Then, for any symmetric graph \(\mathcal{G}\) induced by the group \(\mathfrak{H}\leq\mathrm{O}(3)\), namely, \(\forall\mathcal{G}\in\mathbb{G}(\mathfrak{H})\), we always have_

\[f^{(l)}(\mathcal{G})=\rho^{(l)}(\mathfrak{H})f^{(l)}(\mathcal{G}).\] (11)

_Here we have defined group average as \(\rho^{(l)}(\mathfrak{H})\coloneqq\frac{1}{|\mathfrak{H}|}\sum_{\mathfrak{h}\in \mathfrak{H}}\rho^{(l)}(\mathfrak{h})\)._

Proof.: If \(f^{(l)}\) is \(\mathrm{O}(3)\)-equivariant, then it is also a \(\mathfrak{H}\)-equivariant, thus

\[f^{(l)}(\mathcal{G})=\frac{1}{|\mathfrak{H}|}\sum_{\mathfrak{h}\in\mathfrak{ H}}f^{(l)}(\mathfrak{h}\cdot\mathcal{G})=\frac{1}{|\mathfrak{H}|}\sum_{ \mathfrak{h}\in\mathfrak{H}}\rho(\mathfrak{h})f^{(l)}(\mathcal{G})=\left( \frac{1}{|\mathfrak{H}|}\sum_{\mathfrak{h}\in\mathfrak{H}}\rho(\mathfrak{h}) \right)f^{(l)}(\mathcal{G})=\rho(\mathfrak{H})f^{(l)}(\mathcal{G}).\]

**Theorem A.3** (Theorem 3.5).: _If and only if the matrix \(\bm{I}_{2l+1}-\rho^{(l)}(\mathfrak{H})\) is non-singular, the \(\mathrm{O}(3)\)-equivariant function \(f^{(l)}\) is always a zero function on \(\mathcal{G}\), namely,_

\[f^{(l)}(\mathcal{G})\equiv\bm{0},\quad\forall\mathcal{G}\in\mathbb{G}( \mathfrak{H}).\] (12)

Proof.: From Theorem 3.4, we know that

\[f^{(l)}(\mathcal{G})=\rho(\mathfrak{H})f^{(l)}(\mathcal{G})\iff\left(\bm{I}_{ 2l+1}-\rho^{(l)}(\mathfrak{H})\right)f^{(l)}(\mathcal{G})=0,\]

and the theorem holds for basic knowledge of linear algebra. 

**Theorem A.4** (Theorem 3.6).: _For a finite group \(\mathfrak{H}\) with its representation \(\rho^{(l)}\), \(\rho^{(l)}(\mathfrak{H})\) is a zero matrix (i.e., \(\rho^{(l)}(\mathfrak{H})=\bm{0}\)) if and only if \(\mathrm{tr}(\rho^{(l)}(\mathfrak{H}))=0\). In this case, \(f^{(l)}(\mathcal{G})\equiv\bm{0},\forall\mathcal{G}\in\mathbb{G}(\mathfrak{H })\)._

Proof.: It is obvious that \(\rho^{(l)}(\mathfrak{H})=\bm{0}\implies\mathrm{tr}(\rho^{(l)}(\mathfrak{H}))=0\) since all elements are zero not to mention the main diagonal, and we only to prove \(\mathrm{tr}(\rho^{(l)}(\mathfrak{H}))=0\implies\rho^{(l)}(\mathfrak{H})=\bm{0}\).

A basic fact is \(\mathfrak{h}\cdot\mathfrak{H}=\mathfrak{H}\), thereby

\[\rho^{(l)}(\mathfrak{h})\rho^{(l)}(\mathfrak{H})=\rho^{(l)}(\mathfrak{H}).\]

Now we can use group average and get

\[\left(\rho^{(l)}(\mathfrak{H})\right)^{2}=\rho^{(l)}(\mathfrak{H})\]Such operation can be repeated many times, so that

\[\left(\rho^{(l)}(\mathfrak{H})\right)^{k}=\rho^{(l)}(\mathfrak{H}),\qquad\forall k \in\mathbb{N}_{+}.\]

Now we calculate the trace for each matrix and find

\[\operatorname{tr}\left(\left(\rho^{(l)}(\mathfrak{H})\right)^{k}\right)= \operatorname{tr}\left(\rho^{(l)}(\mathfrak{H})\right)=0,\qquad\forall k\in \mathbb{N}_{+}.\]

By Newton's identity [51], all eigenvalues of the matrix \(\rho(\mathfrak{H})\) are 0, that is, the matrix is a zero matrix. 

**Theorem A.5** (Theorem 4.1).: _For any geometric graph, there exists a bijection between the set of inner products \(\{z_{ij}^{(l)}\}_{l=1}^{|\mathbb{A}_{ij}|}\) given by Eq. (10) and the set of edge angles \(\mathbb{A}_{ij}=\{\theta_{is,jt}:=\langle\vec{\boldsymbol{x}}_{is},\vec{ \boldsymbol{x}}_{jt}\rangle\}_{s\in\mathcal{N}(i),t\in\mathcal{N}(j)}\)._

Proof.: Note that the Legendre polynomial is a set of orthogonal polynomial bases, and there is a bijection to the power function polynomial space, that is

\[\operatorname{span}\left\{\sum_{n=1}^{|\mathbb{A}_{ij}|}P^{(l)}\left(\cos \theta_{n}\right)\right\}_{l=0}^{M}=\operatorname{span}\left\{\sum_{n=1}^{| \mathbb{A}_{ij}|}\cos^{\alpha}\theta_{n}\right\}_{\alpha=0}^{M},\]

where \(M\) is any non-negative integer represents the degree of the polynomial space. Moreover, from the knowledge of Newton's identities, the space of power sums can be converted to space of elementary symmetric polynomials as

\[\operatorname{span}\left\{\sum_{n=1}^{|\mathbb{A}_{ij}|}\cos^{\alpha}\theta_{ n}\right\}_{\alpha=0}^{M}=\operatorname{span}\left\{\sum_{1\leq n_{1}<n_{2}< \dots n_{m}\leq|\mathbb{A}_{ij}|}\left(\prod_{\nu=1}^{k}\cos\theta_{n_{k}} \right)\right\}_{\alpha=0}^{M}\]

From Vieta's formulas, when \(M=|\mathbb{A}_{ij}|\), with the \(M+1\) polynomial in the space of elementary symmetric polynomials being coefficients, we can build a \(|\mathbb{A}_{ij}|\)-degree polynomial with \(\{\cos\theta\mid\theta\in\mathbb{A}_{ij}\}\) as its all roots7. Since all angles are in \([0,\pi)\), the cosine uniquely determines the angle value, and the proposition is established. 

Footnote 7: The trick comes from Lemma 6 in DeepSet [53].

### Further Discussion

Our theory in fact shows that the degeneration of global features of a certain degree (in Table 1) are _inevitable_ on symmetric geometric graphs. This raises two points worth discussing:

1. The degree not indicated to degenerate not necessarily produce a non-zero representation, which may still be affected by the model form and the edge situation.
2. There are some tricks to get around this degeneration: for example, making the output a set or relaxing equivariance constraints (_e.g._ probabilistic symmetry breaking [54]).

\begin{table}
\begin{tabular}{l c c} \hline \hline Group & Notation & Data for Wigner-D matrix traces \(\boldsymbol{D}^{(l)}(H)\) \\ \hline Reflection group & \(C_{i}\) & \((2l+1)\cdot\delta_{l\,\mathrm{mod}\,2,0}\) \\ Cyclic group & \(C_{n}\) & \(2\lfloor l/n\rfloor+1\) \\ Dihedral group & \(D_{n}\) & \(\lfloor l/n\rfloor+\delta_{l\,\mathrm{mod}\,2,0}\) \\ Tetrahedral group & \(T\) & \(r=6\) & \(b=100110\) \\ Octahedral group & \(O\) & \(r=12\) & \(b=100010101110\) \\ Icosahedral group & \(I\) & \(r=30\) & \(b=100000100100100110101110111110\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: The traces of symmetric groups based on [52]. Trace for polyhedral groups can be calculated by \(\boldsymbol{D}^{(l)}(H)=\lfloor l/r\rfloor+b[l\,\mathrm{mod}\,r]\) with repeat length \(r\), where \(b\) is a string only with 0 or 1. For example, for Tetrahedral group \(T\), \(\boldsymbol{D}^{(5)}(T)=\lfloor 5/6\rfloor+b[5\,\mathrm{mod}\,6]=0+0=0\).

It is worth mentioning that outputting a set can solve most problems, although such operators may be quite intractable to implement in computer systems. The failure cases of frame averaging [26; 27] and Neural P\({}^{3}\)M [30] which depend on singular value decomposition or eigenvalue decomposition, is caused by the non-unique matrix decomposition. Some other examples include ComENet [55], which uses the scatter_min() operator in PyTorch to extract the nearest neighbors, making it intractable to handle the situation where multiple neighbors are simultaneously closest, which is quite common in chemical molecules (_e.g._ -CH\({}_{3}\), -NH\({}_{2}\)).

Moreover, from Theorem 4.1, we show the expressivity of our HEGNN, which is able to recover the information of all angles between each pair of edge. However, it should be noted that \(|\mathbb{A}_{ij}|\) may be an extremely large number, which is unacceptable in practical applications to achieve completeness. The same problem also arises in discussions based on CG tensor product models (_e.g._ TFN [19]), such as discussions based on \(D\)-spanning [23], because a sufficiently high-degree \(D\) is unacceptable. However, in terms of actual results, the performance of both our HEGNN and TFN is remarkable. From this perspective, how to bridge the gap between completeness and actual performance with features of limited channels and degrees is a question worth considering.

To get the ball rolling, we raise an interesting question here. Is there a performance gap between this type of purely mathematical representation and other features based on physical and biochemical prior knowledge?

* Purely mathematical representation: topological characteristics [56; 57; 58], cluster expansion basis [59; 60], subgraph blocks [61; 62; 63], frames [26; 27; 64], normalization operators [65; 66];
* Features based on physical and biochemical prior knowledge: distance matrix [67; 68; 69; 70; 71; 72], chemical bond length, angle and dihedral angle [73; 74; 75; 55], force [76; 77], fractional coordinates [78; 79; 80], canonical ordering [81; 82].

In fact, some of these features are directly related (_e.g._ frames and fractional coordinates). How to construct effective and interpretable pure mathematical features based on those with prior knowledge will become a key point in network design, and we will consider further exploration in future work.

## Appendix B More Experimental Details and Results

### Comparison of parameters between models

Like EGNN [1], different features use different numbers of channels, so the inference time does not obviously reflect the time complexity of \(\mathcal{O}(L^{2})\). We list the details of our HEGNN of different degree in Table 8. Intuitively, we add one of each steerable feature on the basis of EGNN (using 64 invariant scalars and 2 Cartesian vectors, _i.e._ coordinate and velocity).

Table 9 shows the number of parameters and inference time of different models.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & HEGNN\({}_{l\leq 1}\) & HEGNN\({}_{l\leq 2}\) & HEGNN\({}_{l\leq 3}\) & HEGNN\({}_{l\leq 6}\) \\ \hline Channel for \(\tilde{\bm{v}}^{(0)}\) & 65 & 65 & 65 & 65 \\ Channel for \(\tilde{\bm{v}}^{(1)}\) & 3 & 3 & 3 & 3 \\ Channel for \(\tilde{\bm{v}}^{(2)}\) & – & 1 & 1 & 1 \\ Channel for \(\tilde{\bm{v}}^{(3)}\) & – & – & 1 & 1 \\ Channel for \(\tilde{\bm{v}}^{(4)}\) & – & – & – & 1 \\ Channel for \(\tilde{\bm{v}}^{(5)}\) & – & – & – & 1 \\ Channel for \(\tilde{\bm{v}}^{(6)}\) & – & – & – & 1 \\ \hline Total dimensions & 74 & 79 & 86 & 119 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Channels for steerable features of different degrees and total dimensions of HEGNN of different degrees.

[MISSING_PAGE_FAIL:20]

covers the maximum repeat length \(r\) in Table 7. We implemented the calculation of the high-degree features based on scipy[83] library.

**Results:** Our calculation results are shown in Table 11 which is completely consistent with the theoretical results in Table 7. The simple experiment shows that the sum of spherical harmonics \(\sum_{i}Y^{(l)}(\vec{\bm{x}}_{i}-\vec{\bm{x}}_{c})\), as a function on graph, will actually vanish on regular polyhedra for some integer degree \(l\).

### Results on other dataset settings

The \(N\)-body dataset setting of this paper refers to FastEGNN [28]8, that is, 5,000 samples are used as the training set (instead of 3,000). We tested the situation of using different data segmentation in Table 12. We also add ClofNet [50], a local frame based scalarization method and SEGNN [33] (select \(l=1\) according to the optimal situation in the paper) and MACE [37] (\(l=2\)), two classic high-degree steerable models for comparison.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & & \multicolumn{4}{c}{**Rotational symmetry**} \\ \(L\) & Tetrahedron & Cube & Octahedron & Dodecahedron & Icosahedron \\ \hline
1 & False & False & False & False & False \\
2 & False & False & False & False & False \\
3 & True & False & False & False & False \\
4 & True & True & True & False & False \\
5 & False & False & False & False & False \\
6 & True & True & True & True & True \\
7 & True & False & False & False & False \\
8 & True & True & True & False & False \\
9 & True & False & False & False & False \\
10 & True & True & True & True & True \\
11 & True & False & False & False & False \\
12 & True & True & True & False & False \\
13 & True & False & False & False & False \\
14 & True & True & True & False & False \\
15 & True & False & False & False & False \\
16 & True & True & True & True & True \\
17 & True & False & False & False & False \\
18 & True & True & True & True & True \\
19 & True & False & False & False & False \\
20 & True & True & True & True & True \\
21 & True & False & False & False & False \\
22 & True & True & True & True & True \\
23 & True & False & False & False & False \\
24 & True & True & True & True & True \\
25 & True & False & False & False & False \\
26 & True & True & True & True & True \\
27 & True & False & False & False & False \\
28 & True & True & True & True & True \\
29 & True & False & False & False & False \\
30 & True & True & True & True & True \\ \hline \hline \end{tabular}
\end{table}
Table 11: _Expressivity analysis of HEGNN using sums of spherical harmonics._ Here, “True” indicates that our HEGNN can distinguish the orientations, meaning the norm of the sum of spherical harmonics is greater than 1. "False" in the table means that no distinction can be made, with the norm of the corresponding spherical harmonic being less than \(10^{-3}\).

Note that SEGNN in Table 12 does not show the effect in the original paper. We also tried to reproduce the dataset in the original paper [33], and the effect of SEGNN on 5-body dataset can be reproduced. However, see Table 13, it is also shown that SEGNN performs poorly on larger datasets.

We found that the GMN-L method proposed in [29] generally performs the best on MD17 dataset, but our HEGNN-6 also achieves comparable performance to GMN-L in most cases. Given that GMN-L requires careful handcrafting of constraints for chemical bonds into the model design, our model's ability to derive promising results without such enhancements supports its competitive performance.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \(N\)-body(\(\times 10^{-2}\)) & 5-body & 20-body & 50-body & 100-body \\ \hline EGNN & 0.71 & 1.04 & 1.15 & 1.31 \\ SEGNN & **0.50** & 6.61 & 9.34 & 13.46 \\ HEGNN\({}_{l\leq 1}\) & 0.71 & 0.97 & **0.93** & 1.22 \\ HEGNN\({}_{l\leq 2}\) & 0.65 & **0.91** & 1.05 & 1.14 \\ HEGNN\({}_{l\leq 3}\) & 0.63 & 0.99 & 1.05 & 1.27 \\ HEGNN\({}_{l\leq 6}\) & 0.72 & 1.05 & 1.11 & 1.28 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Comparison between EGNN, SEGNN and HEGNN on \(N\)-body from [33]

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \(N\)-body(\(\times 10^{-2}\)) & 5-body & 20-body & 50-body & 100-body \\ \hline \multicolumn{5}{c}{train/valid/test=3k/2k/2k} \\ \hline EGNN & 0.71 & 1.08 & 1.16 & 1.29 \\ ClofNet & 0.89 & 1.79 & 2.40 & 2.94 \\ ClofNet-vel & 0.84 & 1.50 & 2.28 & 2.67 \\ GMN & 0.67 & 1.21 & 1.18 & 2.55 \\ MACE & 1.43 & 1.93 & 2.20 & 2.51 \\ SEGNN & 1.81 & 2.67 & 3.44 & NaN \\ HEGNN\({}_{l\leq 1}\) & 0.64 & **0.84** & 0.92 & 1.04 \\ HEGNN\({}_{l\leq 2}\) & 0.69 & 0.89 & 1.13 & **0.94** \\ HEGNN\({}_{l\leq 3}\) & **0.58** & 1.04 & **0.92** & 1.04 \\ HEGNN\({}_{l\leq 6}\) & 0.77 & 1.06 & 1.02 & 1.18 \\ \hline \multicolumn{5}{c}{train/valid/test=5k/2k/2k} \\ \hline ClofNet & 0.80 & 1.49 & 2.28 & 2.77 \\ ClofNet-vel & 0.78 & 1.45 & 2.22 & 2.77 \\ GMN & 0.52 & 0.98 & 1.04 & 1.21 \\ MACE & 1.13 & 1.60 & 2.41 & 3.38 \\ SEGNN & 1.68 & 2.63 & 3.30 & NaN \\ HEGNN\({}_{l\leq 1}\) & 0.52 & 0.79 & 0.88 & 1.13 \\ HEGNN\({}_{l\leq 2}\) & **0.47** & **0.78** & 0.90 & 0.97 \\ HEGNN\({}_{l\leq 3}\) & 0.48 & 0.80 & **0.84** & 0.94 \\ HEGNN\({}_{l\leq 6}\) & 0.69 & 0.86 & 0.96 & **0.86** \\ \hline \hline \end{tabular}
\end{table}
Table 12: Results of \(N\)-body dataset under two partitions.

### Expressiveness of initialization layer

Note that both EGNN [1] and \(\text{HEGNN}_{l\leq 1}\) only use Cartesian vectors. However, in Table 3, the effect of the latter is greatly improved. We speculate that there are two possible factors for this improvement: 1) the extra layer of message passing brought in by Eq. (5); 2) the multi-channel of Cartesian vectors (see Table 8). Therefore, we tested the effect of \(\text{HEGNN}_{l\leq 1}\)-3layers, and the results are shown in Table 15. From the improvement, the first factor contributes more.

## Appendix C Limitation

Our current experiments are mainly limited to testing on small molecules and have not been verified on large-scale molecules or large-scale physical systems. Whether our HEGNN is effective on large-scale geometric graph data sets remains to be verified.

## Appendix D Broader Impact

Our research belongs to the field of AI for Science. The HEGNN proposed in this article is expected to better model scientific problems, thereby promoting the development of higher-precision and efficient AI scientific models.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & Aspirin & Benzene & Ethanol & Malonaldehyde & Naphthalene & Salicylic & Toluene & Uracil \\ \hline RF & 10.94\(\pm\)0.01 & 103.72\(\pm\)1.29 & 4.64\(\pm\)0.01 & 13.93\(\pm\)0.03 & 0.50\(\pm\)0.01 & 1.23\(\pm\)0.01 & 10.93\(\pm\)0.04 & 0.64\(\pm\)0.01 \\ EGNN & 14.41\(\pm\)0.15 & 62.40\(\pm\)0.53 & 4.64\(\pm\)0.01 & 13.64\(\pm\)0.01 & 0.47\(\pm\)0.02 & 1.02\(\pm\)0.02 & 11.78\(\pm\)0.07 & 0.64\(\pm\)0.01 \\ EGNNReg & 13.82\(\pm\)0.19 & 61.68\(\pm\)0.37 & 6.06\(\pm\)0.01 & 13.49\(\pm\)0.06 & 0.63\(\pm\)0.01 & 1.68\(\pm\)0.01 & 11.05\(\pm\)0.01 & 0.66\(\pm\)0.01 \\ GMN & 10.14\(\pm\)0.43 & **48.12\(\pm\)**0.40 & 4.83\(\pm\)0.10 & 13.11\(\pm\)0.03 & 0.40\(\pm\)0.01 & 0.91\(\pm\)0.01 & **10.22\(\pm\)**0.08 & 0.59\(\pm\)0.01 \\ GMN-L & **9.76\(\pm\)**0.11 & 54.17\(\pm\)0.69 & 6.43\(\pm\)0.01 & **12.82\(\pm\)**0.03 & 0.41\(\pm\)0.01 & **0.88\(\pm\)**0.01 & 10.45\(\pm\)0.04 & 0.59\(\pm\)0.01 \\ \hline \(\text{TFN}_{l\leq 2}\) & 12.37\(\pm\)0.18 & 58.48\(\pm\)1.98 & 4.81\(\pm\)0.04 & 13.62\(\pm\)0.08 & 0.49\(\pm\)0.01 & 1.03\(\pm\)0.02 & 10.89\(\pm\)0.01 & 0.84\(\pm\)0.02 \\ SE(3)-\(\text{Tr}_{l\leq 2}\) & 11.12\(\pm\)0.06 & 68.11\(\pm\)0.67 & 4.74\(\pm\)0.13 & 13.89\(\pm\)0.02 & 0.52\(\pm\)0.01 & 1.13\(\pm\)0.02 & 10.88\(\pm\)0.06 & 0.79\(\pm\)0.02 \\ \hline \(\text{HEGNN}_{l\leq 1}\) & 10.32\(\pm\)0.58 & 62.53\(\pm\)7.62 & 4.63\(\pm\)0.01 & 12.85\(\pm\)0.01 & 0.38\(\pm\)0.01 & 0.90\(\pm\)0.05 & 10.56\(\pm\)0.10 & 0.56\(\pm\)0.02 \\ \(\text{HEGNN}_{l\leq 2}\) & 10.04\(\pm\)0.45 & 61.80\(\pm\)5.92 & 4.63\(\pm\)0.01 & 12.85\(\pm\)0.01 & 0.39\(\pm\)0.01 & 0.91\(\pm\)0.06 & 10.56\(\pm\)0.05 & 0.55\(\pm\)0.01 \\ \(\text{HEGNN}_{l\leq 3}\) & 10.20\(\pm\)0.23 & 62.82\(\pm\)4.25 & 4.63\(\pm\)0.01 & 12.85\(\pm\)0.02 & **0.37\(\pm\)**0.01 & 0.94\(\pm\)0.10 & 10.55\(\pm\)0.16 & **0.52\(\pm\)**0.01 \\ \(\text{HEGNN}_{l\leq 6}\) & 9.94\(\pm\)0.07 & 59.93\(\pm\)5.21 & **4.62\(\pm\)**0.01 & 12.85\(\pm\)0.01 & **0.37\(\pm\)**0.02 & **0.88\(\pm\)**0.02 & 10.56\(\pm\)0.33 & 0.54\(\pm\)0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Prediction error (\(\times 10^{-2}\)) on MD17 dataset. Results averaged across 3 runs.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \(N\)-body(\(\times 10^{-2}\)) & 5-body & 20-body & 50-body & 100-body \\ \hline EGNN-4layers & 0.65 & 1.01 & 1.00 & 1.36 \\ \(\text{HEGNN}_{l\leq 1}\)-3layers & 0.63 & 0.98 & 0.96 & 1.31 \\ \(\text{HEGNN}_{l\leq 1}\)-4layers & 0.52 & 0.79 & 0.88 & 1.13 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Comparison between EGNN and HEGNN on \(N\)-body.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract and introduction nicely summarize the theoretical contributions mentioned in SS 3, the model design mentioned in SS 4, and the experimental results mentioned in SS 5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Our current experiments are mainly limited to testing on small molecules and have not been verified on large-scale molecules or large-scale physical systems. Whether our HEGNN is effective on large-scale geometric graph data sets remains to be verified. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Our main theoretical results are given in SS 3 and the proofs are given in Appendix A. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The details of our experimental results are in "Design of experiments" in SS 5.1 and "Datasets" in SS 5.2. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: https://github.com/GLAD-RUC/HEGNN. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the training and test details have listed in "Design of experiments" in SS 5.1 and "Datasets" in SS 5.2. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the variance of the model runs in Table 4. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We use NVIDIA A100-80G, which has written in SS 5.2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our research belongs to the field of AI for Science. The HEGNN proposed in this article is expected to better model scientific problems, thereby promoting the development of higher-precision and efficient AI scientific models. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper poses no such risk. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All existing assets used in our paper have used with citation. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: https://github.com/GLAD-RUC/HEGNN. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.