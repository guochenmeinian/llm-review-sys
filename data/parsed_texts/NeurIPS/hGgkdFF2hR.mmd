# Low-Rank Optimal Transport through Factor

Relaxation with Latent Coupling

Peter Halmos

Xinhao Liu

Julian Gold

Benjamin J. Raphael

Department of Computer Science, Princeton University

###### Abstract

Optimal transport (OT) is a general framework for finding a minimum-cost transport plan, or coupling, between probability distributions, and has many applications in machine learning. A key challenge in applying OT to massive datasets is the quadratic scaling of the coupling matrix with the size of the dataset. Forrow et al. (2019) introduced a factored coupling for the \(k\)-Wasserstein barycenter problem, which Setbon et al. (2021) adapted to solve the primal low-rank OT problem. We derive an alternative parameterization of the low-rank problem based on the _latent coupling_ (LC) factorization previously introduced by Lin et al. (2021) generalizing Forrow et al. (2019). The LC factorization has multiple advantages for low-rank OT including decoupling the problem into three OT problems and greater flexibility and interpretability. We leverage these advantages to derive a new algorithm _Factor Relaxation with Latent Coupling_ (FRLC), which uses _coordinate_ mirror descent to compute the LC factorization. FRLC handles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein), and marginal constraints (balanced, unbalanced, and semi-relaxed) with linear space complexity. We provide theoretical results on FRLC, and demonstrate superior performance on diverse applications - including graph clustering and spatial transcriptomics - while demonstrating its interpretability.

## 1 Introduction

Optimal transport (OT) is a powerful geometric framework for comparing probability distributions. OT problems seek a transport plan \(\bm{P}\) efficiently transforming one distribution (\(\bm{a}\)) into another (\(\bm{b}\)), subject to a ground cost \(\bm{C}\). The minimum cost yields a distance between \(\bm{a}\) and \(\bm{b}\), while the optimal transport plan reveals key structural similarities between the distributions. Owing to its versatility - different ground costs result in different ways to compare data - OT has found many applications in machine learning and beyond: from self-attention Tay et al. (2020); Sander et al. (2022); Geshkovski et al. (2023) and domain adaptation Courty et al. (2014); Solomon et al. (2015) to computational biology Schiebinger et al. (2019); Yang et al. (2020); Bunne et al. (2023); Liu et al. (2023).

This versatility is compounded by several variants using different forms of the objective function and/or constraints on the transport plan \(\bm{P}\). Wasserstein (W) OT Kantorovich (1942) compares distributions over the same space through the expected work of \(\bm{P}\), while Gromov-Wasserstein (GW) OT Memoli (2011) compares distributions supported on distinct geometries through the expected metric distortion of \(\bm{P}\). Fused Gromov-Wasserstein (FGW) Vayer et al. (2020) OT is suited to structured data, taking a convex combination of the former two objectives. Independently, one can relax constraints on the _marginals_ of \(\bm{P}\): in computational applications, \(\bm{P}\) is a matrix whose row-sum \(\bm{P}\bm{1}_{m}\) and column-sum \(\bm{P}^{\mathrm{T}}\bm{1}_{n}\), are called its left and right marginals. _Balanced_ OT requires \(\bm{P}\bm{1}_{m}=\bm{a}\) and \(\bm{P}^{\mathrm{T}}\bm{1}_{n}=\bm{b}\). _Unbalanced_ OT Frogner et al. (2015) replaces these constraints with penalties in thetransport cost, and is more robust to outliers. _Semi-relaxed_ OT can be used to understand how one dataset embeds into another by imposing one hard constraint on either the left or right marginal, used for feature transfer Dong et al. (2023), and alignment of spatiotemporal data Halmos et al. (2024).

An important consideration in applying OT is the quadratic space of the transport plan. To address both the quadratic complexity and to provide robustness under sampling noise, Forrow et al. (2019) introduced another variant of OT, optimizing a \(k\)-Wasserstein Barycenter proxy for the rank-constrained Wasserstein objective. Their approach factors the transport plan through a small set of anchor points called hubs. Generalizing this approach, Scetbon et al. (2021) introduce the factorization \(\bm{P}=\bm{Q}\operatorname{diag}(1/\bm{g})\bm{R}^{\mathrm{T}}\) comprised of _sub-coupling_ matrices \(\bm{Q}\) and \(\bm{R}\) sharing an _inner marginal_\(\bm{g}\), meaning \(\bm{Q}^{\mathrm{T}}\bm{1}_{n}=\bm{R}^{\mathrm{T}}\bm{1}_{m}=\bm{g}\). Building on this, Scetbon et al. (2021, 2022, 2023) derived algorithms to compute low-rank optimal transport plans for the primal OT problem with general costs, extending low-rank OT to GW and unbalanced problems using factored couplings.

Interestingly, a different factorization of \(\bm{P}\) was proposed by Lin et al. (2021) in the context of \(k\)-Wasserstein barycenters. We call their factorization a _latent coupling_ (LC) factorization, given by \(\bm{P}=\bm{Q}\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\mathrm{diag}(1/\bm{g}_{R})\bm{R }^{\mathrm{T}}\), with _two_ inner marginals \(\bm{g}_{Q}=\bm{Q}^{\mathrm{T}}\bm{1}_{n}\) and \(\bm{g}_{R}=\bm{R}^{\mathrm{T}}\bm{1}_{n}\) and a general coupling \(\bm{T}\). Lin et al. (2021) constrain the transport between \(\bm{a}\) and \(\bm{b}\) through two sets of learned anchor points, where the factorization is defined by three transport plans computed from three cost matrices between the points and their anchors. This objective differs from that of Forrow et al. (2019); Scetbon et al. (2021), who seek a minimal rank coupling with respect to a single, fixed cost \(\bm{C}\). We observe that factored couplings of Forrow et al. (2019) correspond to LC factorizations with diagonal \(\bm{T}\), suggesting the LC factorization of Lin et al. (2021) may provide an alternative parameterization of transport plans for the low-rank OT problem considered in Forrow et al. (2019); Scetbon et al. (2021). To our knowledge, this idea has not yet been explored.

Contributions.We present a new algorithm, Factor Relaxation with Latent Coupling (FRLC, with the informal mnemonic "frolic"), to compute a minimum cost low-rank transport plan using the LC factorization. Parameterizing low-rank transport plans with the LC factorization has a number of advantages. First, optimization of the low-rank OT objective decouples into three OT sub-problems on the LC factors \(\bm{Q},\bm{R},\bm{T}\), leading to a simpler optimization algorithm. Second, this decoupling provides straightforward extensions of FRLC to low-rank unbalanced and semi-relaxed OT; similar extensions for factored couplings required additional work Scetbon et al. (2023) beyond the balanced case. Third, the latent coupling \(\bm{T}\) in the LC factorization provides additional flexibility to model transport between datasets with different numbers of clusters, and to model mass-splitting between these clusters, providing a high-level and interpretable description of \(\bm{P}\) that differs from the factored couplings of Forrow et al. (2019). FRLC computes the LC factorization using a novel _coordinate_ mirror descent scheme, alternating descent steps on variables \((\bm{Q},\bm{R})\) and \(\bm{T}\), inspired by the mirror descent approach of Scetbon et al. (2021). We call the descent step on \((\bm{Q},\bm{R})\)_factor relaxation_, as the factors \(\bm{Q}\) and \(\bm{R}\) have relaxed inner marginals, allowing FRLC to be solved by OT sub-problems. FRLC handles multiple OT objectives (Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein), and marginal constraints (balanced, unbalanced, and semi-relaxed). We show FRLC performs better than existing state-of-the-art low-rank methods on a range of synthetic and real datasets, retaining the interpretability of Lin et al. (2021), and inheriting the broad applicability of Scetbon et al. (2021); Scetbon and Cuturi (2022); Scetbon et al. (2022, 2023).

## 2 Background

Wasserstein OT.Let \(\{x_{1},\ldots,x_{n}\}\) and \(\{y_{1},\ldots y_{m}\}\) be datasets in a metric space \(\mathcal{X}\), and let \(\Delta_{d}\) be the probability simplex of size \(d\). Through probability vectors \(\bm{a}\in\Delta_{n}\) and \(\bm{b}\in\Delta_{m}\), each dataset is encoded as a probability measure: \(\mu=\sum_{i=1}^{n}\bm{a}_{i}\delta_{x_{i}}\) and \(\nu=\sum_{j=1}^{m}\bm{b}_{j}\delta_{y_{j}}\). Let

\[\Pi_{\bm{a},\cdot}:=\{\bm{P}\in\mathbb{R}_{+}^{n\times m}:\bm{P}\bm{1}_{m}=\bm {a}\},\ \Pi\,._{,\bm{b}}:=\{\bm{P}\in\mathbb{R}_{+}^{n\times m}:\bm{P}^{\mathrm{T}}\bm{1 }_{n}=\bm{b}\},\ \Pi_{\bm{a},\bm{b}}:=\Pi_{\bm{a},\cdot}\cap\Pi\,._{,\bm{b}}.\]

Thus, \(\Pi_{\bm{a},\bm{b}}\) is the set of transport plans (probabilistic coupling matrices) with marginals \(\bm{a}\) and \(\bm{b}\). Given a cost function \(c:\mathcal{X}\times\mathcal{X}\to\mathbb{R}_{+}\), define the cost matrix \(\bm{C}\in\mathbb{R}_{+}^{n\times m}\) via \(\bm{C}_{ij}=c(x_{i},y_{j})\). The Kantorovich formulation Kantorovich (1942) of discrete OT, also called the Wasserstein problem, seeks a transport plan \(\bm{P}\) of minimal cost :

\[\mathrm{W}(\mu,\nu):=\min_{\bm{P}\in\Pi_{\bm{a},\bm{b}}}\langle\bm{C},\bm{P} \rangle_{F}.\] (1)Gromov-Wasserstein Ot.In many applications, one wishes to compare datasets \(\{x_{1},\ldots,x_{n}\}\subset\mathcal{X}\) and \(\{y_{1},\ldots,y_{m}\}\subset\mathcal{Y}\) across distinct metric spaces \(\mathcal{X}\) and \(\mathcal{Y}\). The Gromov-Wasserstein (GW) objective Memoli (2007, 2011) addresses the absence of a common metric or coordinate system through intra-domain cost functions \(c_{1}:\mathcal{X}\times\mathcal{X}\to\mathbb{R}_{+}\) and \(c_{2}:\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}_{+}\), leading to intra-domain cost matrices \(\bm{A}_{ik}=c_{1}(\bm{x}_{i},x_{k})\) and \(\bm{B}_{jl}=c_{2}(y_{j},y_{l})\). The GW objective function \(\mathcal{Q}_{\bm{A},\bm{B}}(\bm{P}):=\sum_{i,j,k,l}(\bm{A}_{ik}-\bm{B}_{jl})^{ 2}\bm{P}_{ij}\bm{P}_{kl}\) quantifies the expected metric distortion under \(\bm{P}\), leading to the optimization problem:

\[\mathrm{GW}(\mu,\nu):=\min_{\bm{P}\in\Pi_{\bm{a},\bm{b}}}\mathcal{Q}_{\bm{A}, \bm{B}}(\bm{P}).\] (2)

The Fused Gromov-Wasserstein (FGW) objective function Vayer et al. (2020) is a convex combination of the W and GW objectives, given as \(\alpha\langle\bm{C},\bm{P}\rangle_{F}+(1-\alpha)\mathcal{Q}_{\bm{A},\bm{B}}( \bm{P})\), for hyperparameter \(\alpha\in(0,1)\).

Relaxed marginal constraints._Balanced_ OT (1) constrains \(\bm{P}\) to lie in \(\Pi_{\bm{a},\bm{b}}\). _Unbalanced_ OT relaxes constraints \(\bm{P}\bm{1}_{m}=\bm{a}\) and \(\bm{P}^{\mathrm{T}}\bm{1}_{n}=\bm{b}\), replacing them with penalties in the form of KL divergences (or other divergences, see Chizat et al. (2018)):

\[\text{U-W}(\mu,\nu):=\min_{\bm{P}\in\mathbb{R}_{+}^{n\times m}}\langle\bm{C}, \bm{P}\rangle_{F}+\tau_{L}\mathrm{KL}(\bm{P}\bm{1}_{m}\|\bm{a})+\tau_{R} \mathrm{KL}(\bm{P}^{\mathrm{T}}\bm{1}_{n}\|\bm{b}),\] (3)

where \(\tau_{L},\tau_{R}>0\) control the strength of each penalty. _Semi-relaxed_ optimal transport relaxes exactly one of the hard constraints \(\bm{P}\bm{1}_{m}=\bm{a}\) and \(\bm{P}^{\mathrm{T}}\bm{1}_{n}=\bm{b}\) in the same manner. The semi-relaxed version of (1) obtained by relaxing only the "right" marginal constraint on \(\bm{b}\) is:

\[\text{SR}^{\mathrm{R}}\text{-W}(\mu,\nu):=\min_{\bm{P}\in\Pi_{\bm{a}_{\cdot}}} \langle\bm{C},\bm{P}\rangle+\tau\mathrm{KL}(\bm{P}^{\mathrm{T}}\bm{1}_{n}\| \bm{b}),\] (4)

while it's "left" marginal counterpart \(\text{SR}^{\mathrm{L}}\text{-W}(\mu,\nu)\) is defined analogously over \(\bm{P}\in\Pi_{\cdot,\bm{b}}\), using penalty \(\tau\mathrm{KL}(\bm{P}\bm{1}_{m}\|\bm{a})\). Likewise, one can form semi-relaxed or unbalanced GW and FGW problems.

Entropy regularization.The seminal work Cuturi (2013) introduced the Sinkhorn algorithm to solve an entropy regularized version of (1), \(\mathrm{W}_{\epsilon}(\mu,\nu):=\min_{\bm{P}\in\Pi_{\bm{a}_{\cdot},\bm{b}}} \langle\bm{C},\bm{P}\rangle_{F}-\epsilon H(\bm{P})\), massively improving the \(O(n^{3}\log n)\) time complexity of classical techniques Orlin (1997); Tarjan (1997). Above, \(H\) is the entropy, \(H(\bm{P})=-\sum_{ij}\bm{P}_{ij}(\log\bm{P}_{ij}-1)\), and \(\epsilon>0\) is the regularization strength.

Low-rank regularization.The nonnegative rank \(\mathrm{rk}_{+}(\bm{M})\) of matrix \(\bm{M}\) is the least number of nonnegative rank-one matrices summing to \(\bm{M}\). For \(r\geq 1\), define

\[\Pi_{\bm{a}_{\cdot}}(r)=\{\bm{P}\in\Pi_{\bm{a}_{\cdot}}:\mathrm{rk}_{+}(\bm{P} )\leq r\},\quad\Pi_{\cdot,\bm{b}}(r)=\{\bm{P}\in\Pi_{\cdot,\bm{b}}:\mathrm{rk} _{+}(\bm{P})\leq r\},\] (5)

and let \(\Pi_{\bm{a},\bm{b}}(r)=\Pi_{\bm{a}_{\cdot}}(r)\cap\Pi_{\cdot,\bm{b}}(r)\). To estimate Wasserstein distances with greater stability and accuracy under sampling noise, Forrow et al. (2019) proposed a low-rank regularization on the coupling matrix, factoring the transport through a small set of anchor points. More explicitly, Svetbon et al. (2021) parameterized the set as \(\Pi_{\bm{a},\bm{b}}(r)\) through the set \(\mathsf{FC}_{\bm{a},\bm{b}}(r)\) of _factored couplings_,

\[\mathsf{FC}_{\bm{a},\bm{b}}(r):=\{(\bm{Q},\bm{R},\bm{g})\in\mathbb{R}_{+}^{n \times r}\times\mathbb{R}_{+}^{m\times r}\times(\mathbb{R}_{+}^{*})^{r}:\bm{ Q}\in\Pi_{\bm{a},\bm{g}},\bm{R}\in\Pi_{\bm{b},\bm{g}}\}.\]

The set \(\mathsf{FC}_{\bm{a},\bm{b}}(r)\) parameterizes \(\Pi_{\bm{a},\bm{b}}(r)\) through \((\bm{Q},\bm{R},\bm{g})\mapsto\bm{Q}\mathrm{diag}(1/\bm{g})\bm{R}^{\mathrm{T}}\), as shown by Cohen & Rothblum (1993).

Scetbon et al. (2021) apply this factorization to solve the Wasserstein problem subject to \(\bm{P}\in\Pi_{\bm{a},\bm{b}}(r)\) for general cost matrices:

\[\mathrm{W}_{r}(\mu,\nu):=\min_{\bm{P}\in\Pi_{\bm{a},\bm{b}}(r)}\langle\bm{C}, \bm{P}\rangle_{F}\] (6)

GW, unbalanced and semi-relaxed low-rank OT problems are defined as in (2), (3) and (4), replacing \(\mathbb{R}_{+}^{n\times m}\), \(\Pi_{\bm{a}_{\cdot},\cdot}\), or \(\Pi_{\cdot,\bm{b}}\) with rank-constrained counterparts (5). Svetbon & Cuturi (2022); Svetbon et al. (2022, 2023) developed a robust framework for solving all of these problems.

## 3 Factor Relaxation with Latent Coupling (FRLC) algorithm

### Latent Coupling Factorization

We parameterize low-rank coupling matrices \(\bm{P}\in\Pi_{\bm{a},\bm{b}}(r)\) using a factorization introduced in Lin et al. (2021), which we call the _latent coupling (LC) factorization_ (Fig. 1). The key property of this factorization is the presence of a coupling matrix \(\bm{T}\) linking two distinct inner marginals. For simplicity we describe this factorization using an \(r\)-dimensional latent space, but we also extend to non-square matrices linking two latent spaces of different dimensions, as demonstrated in the results.

**Definition 3.1** (Inner marginals).: Given a factorization \(\bm{P}=\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\) of a coupling matrix \(\bm{P}\in\Pi_{\bm{a},\bm{b}}(r)\), the _inner marginals_ of \(\bm{Q}\) and \(\bm{R}\) are \(\bm{g}_{Q}:=\bm{Q}^{\mathrm{T}}\bm{1}_{n}\) and \(\bm{g}_{R}:=\bm{R}^{\mathrm{T}}\bm{1}_{m}\), respectively, where \(\bm{g}_{Q},\bm{g}_{R}\in\Delta_{r}\).

To distinguish the different marginals, we refer to \(\bm{a}\) and \(\bm{b}\) as _outer marginals_.

**Definition 3.2** (LC factorization).: Given a coupling matrix \(\bm{P}\in\Pi_{\bm{a},\bm{b}}(r)\), a _latent coupling (LC) factorization of \(\bm{P}\)_ is \(\bm{P}=\bm{Q}\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\mathrm{diag}(1/\bm{g}_{R})\bm{ R}^{\mathrm{T}}\), where \(\bm{g}_{Q}\) and \(\bm{g}_{R}\) are the inner marginals of \(\bm{Q}\) and \(\bm{R}\), \(\bm{Q}\in\Pi_{\bm{a},\cdot}\), \(\bm{R}\in\Pi_{\bm{b},\cdot}\), and \(\bm{T}\in\Pi_{\bm{g}_{Q},\bm{g}_{R}}\).

We call the factors \(\bm{Q},\bm{R},\bm{T}\) in an LC factorization _sub-couplings_. Let \(\mathcal{R}_{+}:=\mathbb{R}_{+}^{n\times r}\times\mathbb{R}_{+}^{m\times r} \times\mathbb{R}_{+}^{r\times r}\). Given probability vectors \(\bm{a}\in\Delta_{n}\), \(\bm{b}\in\Delta_{m}\) and a positive integer rank \(r\), let

\[\mathsf{LC}_{\bm{a},\bm{b}}(r):=\{(\bm{Q},\bm{R},\bm{T})\in\mathcal{R}_{+}:\bm {Q}\in\Pi_{\bm{a},\cdot},\bm{R}\in\Pi_{\bm{b},\cdot},\bm{T}\in\Pi_{\bm{g}_{Q },\bm{g}_{R}}\},\]

be the set of admissible sub-couplings for the LC factorization. Definition 3.2 gives the following map from \(\mathsf{LC}_{\bm{a},\bm{b}}(r)\) to \(\Pi_{\bm{a},\bm{b}}(r)\):

\[(\bm{Q},\bm{R},\bm{T})\mapsto\bm{Q}\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\mathrm{ diag}(1/\bm{g}_{R})\bm{R}^{\mathrm{T}}=:\bm{P}_{(\bm{Q},\bm{R},\bm{T})}.\] (7)

Since this map is surjective, the set \(\mathsf{LC}_{\bm{a},\bm{b}}(r)\) parameterizes \(\Pi_{\bm{a},\bm{b}}(r)\). Surjectivity follows from the fact that \(\mathsf{FC}_{\bm{a},\bm{b}}(r)\) maps injectively into \(\mathsf{LC}_{\bm{a},\bm{b}}(r)\), through \((\bm{Q},\bm{R},\bm{g})\mapsto(\bm{Q},\bm{R},\mathrm{diag}(\bm{g}))\), and \(\mathsf{FC}_{\bm{a},\bm{b}}(r)\) maps surjectively onto \(\Pi_{\bm{a},\bm{b}}(r)\) via \((\bm{Q},\bm{R},\bm{g})\mapsto\bm{Q}\mathrm{diag}(1/\bm{g})\bm{R}^{\mathrm{T}}\). Definition 3.1 and Definition 3.2 are readily extended in two directions: the case when the outer marginal constraints are relaxed such that \(\bm{Q}\in\mathbb{R}_{+}^{n\times m}\) or \(\bm{R}\in\mathbb{R}_{+}^{n\times m}\), while maintaining the constraint that \(\bm{T}\in\Pi_{\bm{g}_{Q},\bm{g}_{R}}\); as well as the case of non-square \(\bm{T}\).

### The Balanced FRLC Algorithm

We introduce an algorithm Factor Relaxation with Latent Coupling (FRLC), to compute a LC factorization of minimum cost. We first describe the FRLC algorithm for the balanced Wasserstein problem. Extensions to other and marginal constraints are discussed later. The FRLC objective function, for low-rank, balanced Wasserstein OT, is

\[\mathcal{L}_{\mathrm{LC}}(\bm{Q},\bm{R},\bm{T}):=\langle\bm{C},\bm{P}_{(\bm{Q},\bm{R},\bm{T})}\rangle_{F},\] (8)

where \(P_{(\bm{Q},\bm{R},\bm{T})}\) is defined by (7). Since \(\mathsf{LC}_{\bm{a},\bm{b}}(r)\) parameterizes \(\Pi_{\bm{a},\bm{b}}(r)\), problem (8) is equivalent to low rank problem (6). The FRLC algorithm is built from projections onto convex sets, described by constraints on the outer marginals alone for \((\bm{Q},\bm{R})\) and by the inner marginals alone for \(\bm{T}\). Given \((\bm{Q},\bm{R},\bm{T})\in\mathsf{LC}_{\bm{a},\bm{b}}(r)\), sub-couplings \(\bm{Q}\) and \(\dot{\bm{R}}\) are constrained by:

\[\mathcal{C}_{1}(\bm{a}):=\{(\bm{Q},\bm{R},\bm{T})\in\mathcal{R}_{+}:\bm{Q} \bm{1}_{r}=\bm{a}\},\quad\mathcal{C}_{1}(\bm{b}):=\{(\bm{Q},\bm{R},\bm{T})\in \mathcal{R}_{+}:\bm{R}\bm{1}_{r}=\bm{b}\}.\]The convex sets constraining the latent coupling matrix \(\bm{T}\) are

\[\mathcal{C}_{2}(\bm{g}_{Q}):=\{(\bm{Q},\bm{R},\bm{T})\in\mathcal{R}_{+}:\bm{T} \mathbf{1}_{r}=\bm{g}_{Q}\},\quad\mathcal{C}_{2}(\bm{g}_{R}):=\{(\bm{Q},\bm{R}, \bm{T})\in\mathcal{R}_{+}:\bm{T}^{\mathrm{T}}\mathbf{1}_{r}=\bm{g}_{R}\},\]

where \(\bm{g}_{Q}=\bm{Q}^{\mathrm{T}}\mathbf{1}_{n}\) and \(\bm{g}_{R}=\bm{R}^{\mathrm{T}}\mathbf{1}_{m}\) as per Definition 3.1. Writing \(\mathcal{C}_{1}=\mathcal{C}_{1}(\bm{a})\cap\mathcal{C}_{1}(\bm{b})\) and \(\mathcal{C}_{2}=\mathcal{C}_{2}(\bm{g}_{Q})\cap\mathcal{C}_{2}(\bm{g}_{R})\), one has \(\mathrm{LC}_{\bm{a},\bm{b}}(r)=\mathcal{C}_{1}\cap\mathcal{C}_{2}\).

We use _coordinate_ mirror descent to optimize (8), building on the mirror descent (MD) approach of Svetbon et al. (2021); Svetbon and Cuturi (2022); Svetbon et al. (2022, 2023) for the low-rank problem. First we take a descent step in the variables \((\bm{Q},\bm{R})\) for a fixed \(\bm{T}\), using KL penalties on their inner marginals. These "soft" constraints allow the joint optimization in \((\bm{Q},\bm{R})\) to decouple into two semi-relaxed OT problems, one for each variable. We call this step _factor relaxation_ as this allows \((\bm{Q},\bm{R})\) to have relaxed inner marginals \(\bm{g}_{Q}\) and \(\bm{g}_{R}\). Next we take a descent step in the latent coupling variable \(\bm{T}\), fixing the \(\bm{Q}\) and \(\bm{R}\), equivalent to solving a balanced OT problem. Thus, solving both coordinate descent steps corresponds to solving three OT problems.

We now provide further details on these coordinate descent steps, with the full algorithm given in Algorithm 1. Let \((\gamma_{k})_{k=1}^{N}\) be a sequence of step sizes. As in Svetbon and Cuturi (2022), we choose \(\ell^{\infty}\)-normalization for the step-sizes. Our coordinate mirror descent in the factor relaxation step is:

\[(\bm{Q}_{k+1},\bm{R}_{k+1})\leftarrow\operatorname*{arg\,min}_{( \bm{Q},\bm{R})\,:\,(\bm{Q},\bm{R},\bm{T}_{k})\in\mathcal{C}_{1}} \langle(\bm{Q},\bm{R}),\nabla_{(\bm{Q},\bm{R})}\mathcal{L}_{\mathrm{ LC}}\rangle+\frac{1}{\gamma_{k}}\mathrm{KL}((\bm{Q},\bm{R})\|(\bm{Q}_{k},\bm{R}_{k}))\] \[+\tau\mathrm{KL}((\bm{Q}^{\mathrm{T}}\mathbf{1}_{n},\bm{R}^{ \mathrm{T}}\mathbf{1}_{m})\|(\bm{Q}_{k}^{\mathrm{T}}\mathbf{1}_{n},\bm{R}_{k}^ {\mathrm{T}}\mathbf{1}_{m}))\]

The Sinkhorn kernels for the semi-relaxed OT problems arising from the factor relaxation step are:

\[\bm{K}_{\bm{Q}}^{(k)} :=\bm{Q}_{k}\odot\exp(-\gamma_{k}(\bm{C}\bm{R}_{k}\bm{X}_{k}^{ \mathrm{T}}-\mathbf{1}_{n}\mathrm{diag}^{-1}((\bm{C}\bm{R}_{k}\bm{X}_{k}^{ \mathrm{T}})^{\mathrm{T}}\bm{Q}_{k}\mathrm{diag}(1/\bm{g}_{Q_{k}}))^{\mathrm{ T}})\,)\] \[\bm{K}_{\bm{R}}^{(k)} :=\bm{R}_{k}\odot\exp(-\gamma_{k}(\bm{C}^{\mathrm{T}}\bm{Q}_{k} \bm{X}_{k}-\mathbf{1}_{m}\mathrm{diag}^{-1}(\mathrm{diag}(1/\bm{g}_{R_{k}}) \bm{R}_{k}^{\mathrm{T}}\bm{C}^{\mathrm{T}}\bm{Q}_{k}\bm{X}_{k})^{\mathrm{T}}) \,),\]

introducing the shorthand \(\bm{X}=\mathrm{diag}\left(1/\bm{g}_{Q}\right)\bm{T}\,\mathrm{diag}\left(1/\bm {g}_{R}\right)\) and where \(\mathrm{diag}^{-1}(\cdot):\mathbb{R}^{r\times r}\rightarrow\mathbb{R}^{r}\) denotes the matrix-to-vector extraction of the diagonal. This \(\tau\)-dependent regularization also allows us to show smoothness of the objective in Proposition E.5, from which the convergence guarantee Proposition 3.3 follows. We derive the semi-relaxed projection Algorithm 2 of the sub-couplings \(\bm{Q}\) and \(\bm{R}\) in Appendix G for completeness. We also show in Lemma A.1 that \(\bm{g}_{Q}\) and \(\bm{g}_{R}\) induced by the semi-relaxed projection are both feasible and locally optimal, not requiring separate optimization.

As \((\bm{Q}_{k+1},\bm{R}_{k+1},\bm{T})\in\mathcal{C}_{2}\) if and only if \(\bm{T}\in\Pi_{\bm{g}_{Q_{k+1}},\bm{g}_{R_{k+1}}}\), after the factor relaxation step, we next take a coordinate MD step on the latent coupling \(\bm{T}\):

\[\bm{T}_{k+1}\leftarrow\operatorname*{arg\,min}_{\bm{T}\,:\,(\bm{Q}_{k+1},\bm{R }_{k+1},\bm{T})\in\mathcal{C}_{2}}\langle\bm{T},\nabla_{\bm{T}}\mathcal{L}_{ \mathrm{LC}}\rangle+\frac{1}{\gamma_{k}}\mathrm{KL}(\bm{T}\|\bm{T}_{k}).\] (9)

This is equivalent to applying Sinkhorn (Algorithm 5) to \(\bm{T}\) given \(\bm{g}_{Q}\) and \(\bm{g}_{R}\) with the kernel:

\[\bm{K}_{\bm{T}}^{(k)}:=\mathbf{T}_{k}\odot\exp(-\gamma_{k}\mathrm{diag}(1/\bm{g }_{Q_{k+1}})\bm{Q}_{k+1}^{\mathrm{T}}\bm{C}\bm{R}_{k+1}\mathrm{diag}(1/\bm{g }_{R_{k+1}})\,).\]

After the final iteration of the coordinate-MD scheme, \(\bm{X}=\mathrm{diag}\left(1/\bm{g}_{Q}\right)\bm{T}\,\mathrm{diag}\left(1/\bm{g }_{R}\right)\) satisfies \(\bm{X}\bm{g}_{R}=\mathbf{1}_{r}\) and \(\bm{X}^{\mathrm{T}}\bm{g}_{Q}=\mathbf{1}_{r}\) as \(\bm{T}\) is a coupling between \(\bm{g}_{Q}\) and \(\bm{g}_{R}\). Thus \(\bm{P}_{r}=\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\in\Pi_{\bm{a},\bm{b}}\) and the iterates \((\bm{Q}_{k},\bm{T}_{k},\bm{R}_{k})\) remain in the intersection of the constraint sets. Thus, in contrast to other approaches Svetbon et al. (2021); Lin et al. (2021); Forrow et al. (2019), we do not require Dykstra projections back into the intersection to maintain feasability. We note that our implementation of FRLC allows for a non-square latent coupling \(\bm{T}\), providing greater interpretability in problem-specific applications. Above, we presented FRLC in the simplest case that \(\bm{T}\) is square.

### Initialization, convergence, and FRLC extensions

Full-rank random initializations of the sub-coupling matrices.We propose a new initialization of the sub-couplings \((\bm{Q},\bm{R},\bm{T})\) for the LC-factorization in Algorithm 6.This generates a full-rank initialization (Proposition F.1) in the set of rank-\(r\) couplings \(\Pi_{\bm{a},\bm{b}}(r)\) and is accomplished by applying Sinkhorn to random matrices. Our approach differs from Svetbon et al. (2021); Svetbon and Cuturi (2022) who use initializations for the diagonal factorization of Forrow et al. (2019), and are not applicable to a latent coupling that is non-diagonal, non-square, or with two distinct inner marginals.

Convergence analysis of FRLC.As objective (8) is non-convex, it is important to have convergence guarantees. Our convergence criterion \(\Delta(\cdot,\cdot)\) is defined in (10). To prove convergence we require a lower bound on the entries of \(\bm{g}_{Q}\) and \(\bm{g}_{R}\). Previous works introduce a lower-bound vector \(\bm{\alpha}\leq\bm{g}\) enforced element-wise for stability and smoothness Section et al. (2021). In FRLC the use of semi-relaxed projections naturally enforces a lower-bound. In Appendix E.5, we show that for any \(\delta\in(0,\frac{1}{r})\), the FRLC algorithm's \(\tau\)-weighted regularization on the inner marginals can guarantee a uniform lower-bound of \(\delta\) on the entries: for sufficiently large \(\tau\) and \(\tilde{O}(m^{2}/\epsilon)\) iterations for the sub-coupling Pham et al. (2020), one guarantees a lower bound of \(\delta\) on \(\bm{g}_{R}\) and \(\bm{g}_{Q}\). This allows us to show objective smoothness in Proposition E.5. Previous work on low-rank optimal transport Section et al. (2021) use the non-asymptotic convergence criterion of Ghadimi et al. (2014). Following existing works Dang and Lan (2015) establishing convergence rates of coordinate mirror-descent for smooth objectives, we show in Proposition 3.3 this criterion may be extended to coordinate-MD by adapting the block-descent lemma of Beck and Tetruashvili (2013).

**Proposition 3.3**.: _Suppose one has \(f\in C^{1}(\mathcal{X},\mathbb{R})\) with block-coordinate Lipschitz gradient and block smoothness constants \((L_{i})_{i=1}^{p}\), and a function \(h\in C(\mathcal{X},\mathbb{R})\) which is \(\alpha\)-strongly convex. For \(\Phi=f+h\), suppose one performs coordinate mirror descent on \(\Phi\) minimized over a product of closed convex sets \(\mathcal{X}=\prod_{i=1}^{p}\mathcal{X}_{i}\). Let the sub-iterates with respect to the \(i\)-th block update be \(\{\mathbf{x}_{k}^{i}\}_{i=0}^{p}\) where \(\mathbf{x}_{k}:=\mathbf{x}_{k}^{0}\) for \(k\in[N]\) outer iterations. Then one has:_

\[\min_{k}\Delta(\mathbf{x}_{k},\mathbf{x}_{k-1})\leq\frac{D^{2}L}{N(\alpha^{2 }/2L)}=\frac{2D^{2}L^{2}}{N\alpha^{2}},\]

_where \(D\) is (36), \(L\) is the global smoothness constant, stepsizes \(\gamma_{k,i}:=\alpha/L\), and convergence criterion \(\Delta(\mathbf{x}_{k},\mathbf{x}_{k-1})\) is given in (35)._

Specialized to the LC-parametrization, the criterion \(\Delta_{k}(\bm{x}_{k},\bm{x}_{k+1})\) is:

\[\Delta_{k}(\bm{x}_{k},\bm{x}_{k+1}):=\frac{1}{\gamma_{k}^{2}}\left[\|\bm{Q}_{ k+1}-\bm{Q}_{k}\|_{F}^{2}+\|\bm{R}_{k+1}-\bm{R}_{k}\|_{F}^{2}+\|\bm{T}_{k+1}-\bm{T} _{k}\|_{F}^{2}\right]\] (10)

for \(\bm{x}_{k}=(\bm{Q}_{k},\bm{R}_{k},\bm{T}_{k})\). We show through Propositions 3.3, E.5 the following result:

**Proposition 3.4**.: _The FRLC algorithm with step-sizes \(\gamma_{k}=\alpha/L\) and iterates \(\bm{x}_{k}=(\bm{Q}_{k},\bm{R}_{k},\bm{T}_{k})\) has non-asymptotic stationary convergence in the criterion \(\Delta(\cdot,\cdot)\) with:_

\[\min_{k\in 1,..,N-1}\Delta_{k}(\bm{x}_{k},\bm{x}_{k+1})\leq 2D^{2}L^{2}/N \alpha^{2}\]_Where \(N\) is the number of iterations, \(D\) the optimality-gap as in (36), and \(L=\max_{i\in\{1,..,3\}}(L_{i})\) the global smoothness for \(L_{i}=\mathrm{poly}(\|\bm{C}\|_{F},n,m,r,\delta)\) the block-wise smoothness constants._

The proof of Proposition 3.4 follows directly from our extension of the non-asymptotic criterion with the block-descent result Proposition 3.3 and the proof that this lemma holds in FRLC Proposition E.5. We also mention two improvements to other low rank approximation results in literature. In Proposition F.2 we show that one can _analytically_ solve for the block-optimal \(\bm{g}\) for the factorization of Svetbon et al. (2021), and we improve the bound on the low-rank approximation error in Proposition E.7.

FRLC for other marginal constraints and objectives.The balanced FRLC algorithm can be extended simply to other marginal constraints owing to the decoupling of the coordinate MD scheme. In particular, by using either the semi-relaxed projections (Algorithm 2) or fully-relaxed (unbalanced) projections (Algorithm 3) on sub-couplings \(\bm{Q}\) and \(\bm{R}\), one can solve the balanced problem, the problem with the left or right marginal relaxed, or the unbalanced problem. As such, _all_ variants of marginal constraints can be handled by a single algorithm, given in Algorithm 4.

We also extend the FRLC algorithm to the Gromov-Wasserstein problem. This consists of computing a GW-specific gradient with the appropriate marginal constraints applied to simplify their form, and re-computing Sinkhorn kernels as exponentiations of these gradients. The matrix form of the quadratic GW objective is \(\mathbf{1}_{m}^{\mathrm{T}}\bm{P}^{\mathrm{T}}\bm{A}^{\odot 2}\bm{P} \mathbf{1}_{m}+\mathbf{1}_{n}^{\mathrm{T}}\bm{P}\bm{B}^{\odot 2}\bm{P} \mathbf{1}_{n}-2\langle\bm{APB},\bm{P}\rangle\), where \(\odot\) denotes the Hadamard (entrywise) product. Then the GW-specific Sinkhorn kernels are

\[\bm{K}_{\bm{Q}}^{(k)} \leftarrow\exp\left(2\gamma_{k}(2\bm{AQXR}^{\mathrm{T}}\bm{BRX}^{ \mathrm{T}}-\bm{A}^{\odot 2}\bm{Q}\mathbf{1}_{r}\mathbf{1}_{r}^{\mathrm{T}}) \right),\] \[\bm{K}_{\bm{R}}^{(k)} \leftarrow\exp\left(2\gamma_{k}(2\bm{BRX}^{\mathrm{T}}\bm{Q}^{ \mathrm{T}}\bm{AQX}-\bm{B}^{\odot 2}\bm{R}\mathbf{1}_{r}\mathbf{1}_{r}^{\mathrm{T}}) \right),\] \[\bm{K}_{\bm{T}}^{(k)} \leftarrow\exp(4\gamma_{k}\operatorname{diag}(\bm{g}_{Q}^{-1}) \bm{Q}^{\mathrm{T}}\bm{AQXR}^{\mathrm{T}}\bm{BR}\operatorname{diag}(\bm{g}_{ R}^{-1})).\]

In Algorithm 4, one can solve the GW-problem by using the kernels above. Here, we present the kernels omitting a rank-1 perturbation, which is given in Appendix D. From the Wasserstein and GW gradients, the FGW gradient is easily taken as a convex combination of the two. In this work, we primarily focus on the LC-factorization for the rank \(r\) Wasserstein problem (6).

## 4 Experimental Results

We compare FRLC to existing low-rank and full-rank optimal transport algorithms on several datasets: simulated datasets previously used in Tong et al. (2023) and Svetbon et al. (2021); a massive spatial-transcriptomics dataset Chen et al. (2022); and a graph partitioning task Chowdhury and Needham (2021). Further details of each experiment (e.g. pre-processing, validation) are in Appendices K, L, and M. In the section below, LOT refers to the works of Svetbon et al. (2021, 2023, 2022) and Latent OT refers to Lin et al. (2021).

Figure 2: (a) Simulated dataset containing points from two moons (orange) and eight Gaussians (blue). (b) Transport cost \(\langle\bm{C},\bm{P}\rangle_{F}\) achieved by FRLC and LOT Svetbon et al. (2021) for the balanced Wasserstein problem on the dataset in (a) for different ranks and initializations. FLRC full rank (blue curve) is average over 10 random initializations. (c) Results on the 10D mixture of Gaussians dataset.

### Evaluation of Low-rank Approximations for Balanced OT on Synthetic Data

We first compare the balanced OT version of FRLC with the the low-rank balanced OT algorithm LOT of Scethon et al. (2021) on a synthetic dataset following Tong et al. (2023). The dataset consists of \(m=1000\) points from two mooms and \(n=1000\) points sampled from eight 2D Gaussian densities (Fig. 2a). We solve the Wasserstein problem (\(1\)) with cost matrix \(\bm{C}\) computed using the Euclidean distance. The full-rank coupling matrix \(\bm{P}\) has rank 1000, and we compute both FRLC and LOT solutions with rank between 20 and 200. For each rank, we initialize FRLC adapting the deterministic rank-2 initialization proposed in Scethon et al. (2021) and the random initialization of Alg. 6. We initialize LOT using the rank-2 initialization and two other options in \(\tttt\)-\(\tt\)jax Cuturi et al. (2022).

We find that FRLC obtains lower transport cost \(\langle\bm{C},\bm{P}\rangle_{F}\) with increasing rank (Fig. 2b) and consistently achieves lower transport cost than LOT across all ranks and all initializations. Specifically, starting both methods at the same rank-2 initialization, FRLC consistently achieves a lower cost than LOT for all ranks. Additionally, we observe smooth convergence of FRLC for both rank-2 initialization and the full-rank random initialization of Alg. 6 (Fig. 5).

We also evaluate FRLC and LOT on two datasets of Gaussian mixtures, one in 2-dimensions and one in 10-dimensions, each with \(n=m=5,000\) points from two mixtures of Gaussians, following Scethon et al. (2021), with further details in Appendix K. We observe the same trend as the previous simulation for both datasets (Fig. 2c, Fig. 7), with FRLC achieving lower transport costs than LOT across all ranks and all initializations. In addition FRLC has half the runtime of LOT (CPU) - including the setup time of FRLC but excluding the setup time of LOT in \(\tttt\)-\(\tt\)jax - on datasets of \(n=m=1000\) points from all three datasets with rank \(r=100\) (Table 2). At the same time FRLC achieves lower primal cost \(\langle\bm{C},\bm{P}\rangle_{F}\) with tighter marginals \(\|\bm{P}\bm{1}_{n}-\bm{a}\|_{2}\) and \(\|\bm{P}^{T}\bm{1}_{m}-\bm{b}\|_{2}\). Lin et al. (2021) only solves a proxy for the rank-constrained Wasserstein problem, and thus is not the focus of our comparisons. Nevertheless, we verify that on all synthetic experiments that FRLC achieves significantly lower primal OT cost than Latent OT (Table 5).

### Interpretation of the Latent Coupling and LC-Projection

We demonstrate the intepretability of the latent coupling \(\bm{T}\) in the LC factorization. In both the LC factorization and factored couplings, the sub-couplings \(\bm{Q}\) and \(\bm{R}\) each have associated barycentric projection operators which coarse-grain input datasets \(\mathbf{Z}^{(1)},\mathbf{Z}^{(2)}\). In particular, the LC projection is defined from the LC factorization as follows.

**Definition 4.1** (LC-Projection).: Let \(\bm{Q}\operatorname{diag}(1/\bm{g}_{Q})\bm{T}\operatorname{diag}(1/\bm{g}_{ R})\bm{R}^{\mathrm{T}}\) be an LC factorization of of a coupling matrix \(\bm{P}\in\Pi_{\bm{a},\bm{b}}(r)\) computed from datasets \(\mathbf{Z}^{(1)}\in\mathbb{R}^{n\times d},\mathbf{Z}^{(2)}\in\mathbb{R}^{m \times d}\), with \(\bm{T}\in\mathbb{R}^{+\times r_{2}}_{t}\). The _LC-projections_\(\mathbf{Y}^{(1)}\) and \(\mathbf{Y}^{(2)}\) of \(\mathbf{Z}^{(1)}\) and \(\mathbf{Z}^{(2)}\) are \(\mathbf{Y}^{(1)}:=\operatorname{diag}(1/\bm{g}_{Q})\bm{Q}^{\mathrm{T}}\mathbf{ Z}^{(1)}\), and \(\mathbf{Y}^{(2)}:=\operatorname{diag}(1/\bm{g}_{R})\bm{R}^{\mathrm{T}}\mathbf{ Z}^{(2)}\).

By interpreting any factored coupling \((\bm{Q},\bm{R},\bm{g})\) as an LC factorization \((\bm{Q},\bm{R},\operatorname{diag}(\bm{g}))\), Definition 4.1 describes the barycentric projections for both factorizations. We compare the projections of the coupling computed by FRLC to those of LOT Scethon et al. (2021) on a dataset containing 1000 samples from 2D-Gaussians centered at the 5th-roots of unity and 1000 samples from 2D Gaussians

Figure 3: LC-projections of couplings of Gaussians centered on the 5th-roots of unity (green) and 10th roots of unity (yellow). (a) Ground-truth full-rank coupling. (b) Non-square rank-5 latent-coupling of FRLC (c) LC-projection barycenters aligned with rank-5 diagonal coupling of LOT Scethon et al. (2021). (d) Square rank-10 latent coupling of FRLC. (e) Rank-10 diagonal coupling of LOT.

centered at the 10th-roots of unity (the latter scaled by a factor of two, Fig. 2(a)). In both cases, the latent coupling \(\bm{T}\) or \(\mathrm{diag}(\bm{g})\) is visualized as a transport between barycenters. We run FRLC and LOT with ranks \(r=5\) and \(r=10\) to match the number of target and source clusters. In the rank-5 case, FRLC uses a _non-square_ latent coupling \(\bm{T}\in\mathbb{R}_{+}^{10\times 5}\) which correctly captures the coupling between clusters (Fig. 2(b)), while the LOT rank-5 projection computes barycenters that are outside of the clusters (Fig. 2(c)). A similar result is observed for square rank-10 latent couplings computed by FRLC (Fig. 2(d)) and LOT (Fig. 2(e)) demonstrating that the LOT barycenters in Fig. 2(b) are not an artifact of using the lowest rank. We observe similar results on other simulated datasets (Fig. 11).

### Evaluation on Spatial Transcriptomics Alignment

We compare FRLC and the algorithm (LOT-U) of Scetbon et al. (2023) (which solves unbalanced low-rank Wasserstein, GW, and FGW problems) on the task of computing an alignment between cells from different time points during mouse embryonic development. Specifically, we compute an alignment between a spatial transcriptomics (ST) dataset of an E11.5 stage mouse embryo and an E12.5 stage mouse embryo Chen et al. (2022). Optimal transport is a popular approach to align single-cell Schiebinger et al. (2019) and spatial transcriptomics datasets Zeira et al. (2022); Liu et al. (2023); Klein et al. (2023). In single-cell transcriptomics, one measures a gene expression vector for each cell, and in spatial transcriptomics one additionally measures the 2D location of each cell. The cost matrix \(\bm{C}\) describes the difference between gene expression vectors and intra-domain cost matrices \(\bm{A}\) and \(\bm{B}\) are derived from the 2D coordinates within each slice. Therefore, OT problems of W, GW, and FGW objectives can be solved and the coupling matrix represents the cell-cell alignment (Appendix M). However, computation of a full-rank OT solution is not feasible in our large-scale dataset: the E11.5 slice has about 30,000 cells while the E12.5 slice has about 50,000 cells.

We evaluate the alignments by assessing performance on two prediction tasks from Scetbon et al. (2023): (1) a _gene expression prediction_ task where we predict the expression of a gene in E12.5 from expression of the gene in E11.5 using the alignment; (2) a _cell type prediction task_ where we predict the cell types of E12.5 from the cell type clustering of E11.5 (Appendix M). We evaluate the accuracy of the gene expression prediction task through the Spearman correlation \(\rho\) between the predicted expression and the ground truth expression of 10 test marker genes. We evaluate the accuracy of the cell type prediction task by computing the Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) between the predicted cell types and the cell types derived in the original publication Chen et al. (2022). Being a comparison between different objectives, this relies on downstream metrics. For completeness, we validate the efficacy of FRLC on directly minimizing the balanced Wasserstein cost \(\langle\bm{C},\bm{P}\rangle_{F}\) against Scetbon et al. (2021) in Figure 8.

For a direct comparison, we use FRLC to solve the same unbalanced problems (denoted FRLC-U). We perform an extensive grid search (Appendix M.3) to pick the best hyperparameters (including \(\mathrm{rank}\ll 30,000\)) for all algorithms. Scetbon et al. (2023) previously showed that unbalanced FGW algorithm has the best performance on ST alignment. We find that unbalanced FRLC achieves comparable or better results than the previous state-of-the-art unbalanced low-rank method on all three objectives (Table 4). We also solve a semi-relaxed version of each problem motivated by the

Figure 4: (a) Brain marker gene _Tubb2b_ expression and FRLC prediction. (b) Comparison of the low-rank unbalanced (LOT-U) algorithm of Scetbon et al. (2023) and FRLC on aligning spatial transcriptomics data. Bold indicates top performing method for each metric on each objective.

observation that all cells from E12.5 have an ancestor, but not all cells from E11.5 have the same number of descendants due to cell growth and death. Thus the former marginal is tight, and the latter relaxed Halmos et al. (2024). We run both semi-relaxed FRLC (FRLC-SR) and a setting of LOT-U that recovers the semi-relaxed problem (LOT-SR). Semi-relaxed FRLC achieves the best results on all three metrics by a large margin (Table 4). As one example, the expression of _Tubb2b_, a mouse brain marker gene, agreeing with the expression predicted from the semi-relaxed alignment of FRLC (Fig. 4a).

### Additional Experiments

We evaluate FRLC on an unsupervised graph partitioning problem Chowdhury and Needham (2021) on four real-world graph datasets Yang and Leskovec (2012); Yin et al. (2017); Banerjee et al. (2013). We benchmark the performance of the semi-relaxed and GW settings of FRLC against (1) GWL Xu et al. (2019), solving a balanced GW problem; (2) SpecGWL Chowdhury and Needham (2021) using the heat kernel on the graph Laplacian as the cost matrix. We find FRLC achieves the better clustering performance than GWL and SpecGWL on 9/12 and 11/12 of the datasets (Table 3 and Appendix L).

## 5 Discussion

We provide comparison of existing low-rank solvers in Table 1. The FRLC algorithm has a number of advantages, including (1) coarsening a full-rank plan \(\bm{P}\) to non-diagonal latent coupling \(\bm{T}\); (2) minimizing the primal OT problem for general cost \(\bm{C}\) rather than a barycenteric problem; (3) optimizing only sub-couplings; and (4) using Sinkhorn alone as the sub-routine for low-rank OT. While we argue these are substantial advantages, FRLC has limitations which warrant follow-up work. In particular, three key limitations of our work, common to the existing low-rank OT algorithms, are: (1) selecting values of the latent coupling ranks; (2) strengthening the convergence criterion; (3) addressing sensitivity to the initialization from non-convexity of the objective. A limitation specific to our work is the selection of the \(\tau\) hyperparameter controlling the smoothness of the trajectory. These and other limitations are discussed in Section N of the Appendix. Another direction for further investigation is to better understand what structure LC factorizations capture when the optimal plan is known to have full rank, e.g. when the Monge map exists, as has been explored by Liu et al. (2021).

## 6 Conclusion

We introduce FRLC, an algorithm to compute low-rank optimal transport plan from the latent coupling (LC) factorization. FRLC handles different OT objective costs and relaxations of the marginal constraints. Moreover, the LC factorization provides an interpretable coarse-graining of the full transport plan and its marginals through the mapping \((\bm{P},\bm{a},\bm{b})\rightarrow(\bm{T},\bm{g}_{Q},\bm{g}_{R})\). We demonstrate the superior performance of FRLC compared to state-of-the-art low-rank methods on real and synthetic datasets.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline
**Method** & **Factorization** & **Cost** & **Variables** & **Algorithm** & \begin{tabular}{c} **Sub-routine** \\ **for coupling** \\ \end{tabular} \\ \hline \hline \begin{tabular}{c} Factored Coupling \\ Forrow et al. (2019) \\ \end{tabular} & Factored coupling & \begin{tabular}{c} Factored coupling \\ barycenter \\ \end{tabular} & \begin{tabular}{c} Anchors \& \\ sub-couplings \\ \end{tabular} & Lloyd-type & Dykstra’s \\ \hline \begin{tabular}{c} Latent OT \\ Lin et al. (2021) \\ \end{tabular} & Latent coupling & \begin{tabular}{c} Extension of \\ \(k\)-Wasserstein \\ barycenter \\ \end{tabular} & \begin{tabular}{c} Anchors \& \\ sub-couplings \\ \end{tabular} & Lloyd-type & Dykstra’s \\ \hline \begin{tabular}{c} LOT \\ Scebon et al. (2021) \\ \end{tabular} & Factored coupling & Primal OT cost & \begin{tabular}{c} Sub-couplings \& \\ inner marginal \\ \end{tabular} & Mirror-descent & Dykstra’s \\ \hline \begin{tabular}{c} FRLC (this work) \\ \end{tabular} & Latent coupling & Primal OT cost & Sub-couplings & 
\begin{tabular}{c} Coordinate \\ mirror-descent \\ \end{tabular} & OT \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparing aspects of low-rank OT methods. Factorization indicates the structure of the inner matrix.

## Acknowledgments and Disclosure of Funding

This work is supported by NCI grant U24CA248453 to B.J.R. J.G. gratefully acknowledges support from the Schmidt DataX Fund at Princeton University made possible through a major gift from the Schmidt Futures Foundation.

## References

* Bakshi & Woodruff (2018) Bakshi, A. and Woodruff, D. Sublinear Time Low-Rank Approximation of Distance Matrices. _Advances in Neural Information Processing Systems_, 31, 2018.
* Banerjee et al. (2013) Banerjee, A., Chandrasekhar, A. G., Duflo, E., and Jackson, M. O. The Diffusion of Microfinance. _Science_, 341(6144):1236498, 2013.
* Bauschke & Lewis (2000) Bauschke, H. H. and Lewis, A. S. Dykstra's algorithm with Bregman projections: A convergence proof. _Optimization_, 48(4):409-427, January 2000. ISSN 1029-4945. doi: 10.1080/02331930008844513. URL http://dx.doi.org/10.1080/02331930008844513.
* Beck & Tetrushvili (2013) Beck, A. and Tetrushvili, L. On the Convergence of Block Coordinate Descent Type Methods. _SIAM J. Optim._, 23:2037-2060, 2013. URL https://api.semanticscholar.org/CorpusID:6866704.
* Benamou et al. (2015) Benamou, J.-D., Carlier, G., Cuturi, M., Nenna, L., and Peyre, G. Iterative Bregman Projections for Regularized Transportation Problems. _SIAM Journal on Scientific Computing_, 37(2):A1111-A1138, January 2015. ISSN 1095-7197. doi: 10.1137/141000439. URL http://dx.doi.org/10.1137/141000439.
* Bregman (1967) Bregman, L. M. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. _USSR computational mathematics and mathematical physics_, 7(3):200-217, 1967.
* Bunne et al. (2023) Bunne, C., Stark, S. G., Gut, G., del Castillo, J. S., Levesque, M., Lehmann, K.-V., Pelkmans, L., Krause, A., and Ratsch, G. Learning single-cell perturbation responses using neural optimal transport. _Nature Methods_, 20(11):1759-1768, September 2023. ISSN 1548-7105. doi: 10.1038/s41592-023-01969-x. URL http://dx.doi.org/10.1038/s41592-023-01969-x.
* Charikar et al. (2023) Charikar, M., Chen, B., Re, C., and Waingarten, E. Fast Algorithms for a New Relaxation of Optimal Transport. In Neu, G. and Rosasco, L. (eds.), _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pp. 4831-4862. PMLR, 12-15 Jul 2023. URL https://proceedings.mlr.press/v195/charikar23a.html.
* Chen et al. (2022) Chen, A., Liao, S., Cheng, M., Ma, K., Wu, L., Lai, Y., Qiu, X., Yang, J., Xu, J., Hao, S., et al. Spatiotemporal transcriptomic atlas of mouse organogenesis using DNA nanoball-patterned arrays. _Cell_, 185(10):1777-1792, 2022.
* Chen & Price (2017) Chen, X. and Price, E. Condition number-free query and active learning of linear families. 2017.
* Chizat et al. (2018) Chizat, L., Peyre, G., Schmitzer, B., and Vialard, F.-X. Unbalanced Optimal Transport: Dynamic and Kantorovich Formulations. _Journal of Functional Analysis_, 274(11):3090-3123, June 2018. ISSN 0022-1236. doi: 10.1016/j.jfa.2018.03.008. URL http://dx.doi.org/10.1016/j.jfa.2018.03.008.
* Chowdhury & Needham (2021) Chowdhury, S. and Needham, T. Generalized Spectral Clustering via Gromov-Wasserstein Learning. In _International Conference on Artificial Intelligence and Statistics_, pp. 712-720. PMLR, 2021.
* Chung (2005) Chung, F. Laplacians and the Cheeger Inequality for Directed Graphs. _Annals of Combinatorics_, 9:1-19, 2005.
* Cohen & Rothblum (1993) Cohen, J. E. and Rothblum, U. G. Nonnegative Ranks, Decompositions, and Factorizations of Nonnegative Matrices. _Linear Algebra and its Applications_, 190:149-168, 1993.
* Courty et al. (2014) Courty, N., Flamary, R., and Tuia, D. Domain adaptation with regularized optimal transport. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I 14_, pp. 274-289. Springer, 2014.
* Courty et al. (2015)Cuturi, M. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. _Advances in neural information processing systems_, 26, 2013a.
* Cuturi (2013) Cuturi, M. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in Neural Information Processing Systems_, pp. 2292-2300, 2013b. URL https://proceedings.neurips.cc/paper/2013/hash/af21ddc97db2e27e13572cbf59eb343d-Abstract.html.
* Cuturi et al. (2022) Cuturi, M., Meng-Papaxanthos, L., Tian, Y., Bunne, C., Davis, G., and Teboul, O. Optimal Transport Tools (OTT): A JAX Toolbox for all things Wasserstein. _arXiv preprint arXiv:2201.12324_, 2022.
* Dang & Lan (2015) Dang, C. D. and Lan, G. Stochastic Block Mirror Descent Methods for Nonsmooth and Stochastic Optimization. _SIAM J. Optim._, 25(2):856-881, January 2015.
* Dong et al. (2023) Dong, S., Pan, Z., Fu, Y., Xu, D., Shi, K., Yang, Q., Shi, Y., and Zhuo, C. Partial Unbalanced Feature Transport for Cross-Modality Cardiac Image Segmentation. _IEEE Transactions on Medical Imaging_, 2023.
* Dykstra (1983) Dykstra, R. L. An Algorithm for Restricted Least Squares Regression. _Journal of the American Statistical Association_, 78(384):837-842, December 1983. ISSN 1537-274X. doi: 10.1080/01621459.1983.10477029. URL http://dx.doi.org/10.1080/01621459.1983.10477029.
* Forrow et al. (2019) Forrow, A., Hutter, J.-C., Nitzan, M., Rigollet, P., Schiebinger, G., and Weed, J. Statistical Optimal Transport via Factored Couplings. In Chaudhuri, K. and Sugiyama, M. (eds.), _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, volume 89 of _Proceedings of Machine Learning Research_, pp. 2454-2465. PMLR, 16-18 Apr 2019. URL https://proceedings.mlr.press/v89/forrow19a.html.
* Frieze et al. (2004) Frieze, A., Kannan, R., and Vempala, S. Fast Monte-Carlo Algorithms for Finding Low-rank Approximations. _J. ACM_, 51(6):1025-1041, nov 2004. ISSN 0004-5411. doi: 10.1145/1039488.1039494. URL https://doi.org/10.1145/1039488.1039494.
* Frogner et al. (2015) Frogner, C., Zhang, C., Mobahi, H., Araya, M., and Poggio, T. A. Learning with a Wasserstein Loss. _Advances in neural information processing systems_, 28, 2015.
* Geshkovski et al. (2023) Geshkovski, B., Letrouit, C., Polyanskiy, Y., and Rigollet, P. A mathematical perspective on Transformers. _arXiv preprint arXiv:2312.10794_, 2023.
* Ghadimi et al. (2014) Ghadimi, S., Lan, G., and Zhang, H. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. _Mathematical Programming_, 155(1-2):267-305, December 2014. ISSN 1436-4646. doi: 10.1007/s10107-014-0846-1. URL http://dx.doi.org/10.1007/s10107-014-0846-1.
* Halmos et al. (2024) Halmos, P., Liu, X., Gold, J., Chen, F., Ding, L., and Raphael, B. J. DeST-OT: Alignment of Spatiotemporal Transcriptomics Data. In _International Conference on Research in Computational Molecular Biology_, pp. 434-437. Springer, 2024.
* Indyk et al. (2019) Indyk, P., Vakilian, A., Wagner, T., and Woodruff, D. P. Sample-optimal low-rank approximation of distance matrices. In Beygelzimer, A. and Hsu, D. (eds.), _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pp. 1723-1751. PMLR, 25-28 Jun 2019. URL https://proceedings.mlr.press/v99/indyk19a.html.
* Kantorovich (1942) Kantorovich, L. On the Translocation of Masses: Doklady akademii nauk ussr. 1942.
* Klein et al. (2023) Klein, D., Palla, G., Lange, M., Klein, M., Piran, Z., Gander, M., Meng-Papaxanthos, L., Sterr, M., Bastidas-Ponce, A., Tarquis-Medina, M., et al. Mapping cells through time and space with moscot. _bioRxiv_, pp. 2023-05, 2023.
* Lin et al. (2021) Lin, C.-H., Azabou, M., and Dyer, E. L. Making transport more robust and interpretable by moving data through a small number of anchor points. _Proceedings of machine learning research_, 139:6631, 2021.
* Liu et al. (2021) Liu, W., Zhang, C., Zheng, N., and Qian, H. Approximating optimal transport via low-rank and sparse factorization. _CoRR_, abs/2111.06546, 2021. URL https://arxiv.org/abs/2111.06546.

Liu, X., Zeira, R., and Raphael, B. J. Partial alignment of multislice spatially resolved transcriptomics data. _Genome Research_, 33(7):1124-1132, 2023.
* Memoli (2007) Memoli, F. On the use of Gromov-Hausdorff Distances for Shape Comparison. 2007.
* Memoli (2011) Memoli, F. Gromov-Wasserstein Distances and the Metric Approach to Object Matching. _Foundations of computational mathematics_, 11:417-487, 2011.
* Nesterov (2012) Nesterov, Y. Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems. _SIAM Journal on Optimization_, 22(2):341-362, 2012.
* Orlin (1997) Orlin, J. B. A polynomial time primal network simplex algorithm for minimum cost flows. _Mathematical Programming_, 78(2):109-129, Aug 1997. ISSN 1436-4646. doi: 10.1007/BF02614365. URL https://link.springer.com/content/pdf/10.1007/BF02614365.pdf.
* Pham et al. (2020) Pham, K., Le, K., Ho, N., Pham, T., and Bui, H. H. On Unbalanced Optimal Transport: An Analysis of Sinkhorn Algorithm. In _International Conference on Machine Learning_, 2020. URL https://api.semanticscholar.org/CorpusID:211068892.
* Sander et al. (2022) Sander, M. E., Ablin, P., Blondel, M., and Peyre, G. Sinkformers: Transformers with Doubly Stochastic Attention. In Camps-Valls, G., Ruiz, F. J. R., and Valera, I. (eds.), _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151 of _Proceedings of Machine Learning Research_, pp. 3515-3530. PMLR, 28-30 Mar 2022. URL https://proceedings.mlr.press/v151/sander22a.html.
* Scethon and Cuturi (2022) Scethon, M. and Cuturi, M. Low-rank Optimal Transport: Approximation, Statistics and Debiasing. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=4btNeXXFFAQ.
* Scethon et al. (2021) Scethon, M., Cuturi, M., and Peyre, G. Low-Rank Sinkhorn Factorization. In _International Conference on Machine Learning_, 2021. URL https://api.semanticscholar.org/CorpusID:232147563.
* Scethon et al. (2022) Scethon, M., Peyre, G., and Cuturi, M. Linear-time Gromov Wasserstein Distances using Low Rank Couplings and Costs. In _International Conference on Machine Learning_, pp. 19347-19365. PMLR, 2022.
* Scethon et al. (2023) Scethon, M., Klein, M., Palla, G., and Cuturi, M. Unbalanced Low-rank Optimal Transport Solvers, 2023.
* Schiebinger et al. (2019) Schiebinger, G., Shu, J., Tabaka, M., Cleary, B., Subramanian, V., Solomon, A., Gould, J., Liu, S., Lin, S., Berube, P., et al. Optimal-Transport Analysis of Single-Cell Gene Expression Identifies Developmental Trajectories in Reprogramming. _Cell_, 176(4):928-943, 2019.
* Solomon et al. (2015) Solomon, J., De Goes, F., Peyre, G., Cuturi, M., Butscher, A., Nguyen, A., Du, T., and Guibas, L. Convolutional Wasserstein Distances: Efficient Optimal Transportation on Geometric Domains. _ACM Transactions on Graphics (ToG)_, 34(4):1-11, 2015.
* Stahl et al. (2016) Stahl, P. L., Salmen, F., Vickovic, S., Lundmark, A., Navarro, J. F., Magnusson, J., Giacomello, S., Asp, M., Westholm, J. O., Huss, M., et al. Visualization and analysis of gene expression in tissue sections by spatial transcriptomics. _Science_, 353(6294):78-82, 2016.
* Tarjan (1997) Tarjan, R. E. Dynamic trees as search trees via Euler tours, applied to the network simplex algorithm. _Mathematical Programming_, 78(2):169-177, Aug 1997. ISSN 1436-4646. doi: 10.1007/BF02614369. URL https://link.springer.com/content/pdf/10.1007/BF02614369.pdf.
* Tay et al. (2020) Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C. Sparse Sinkhorn Attention. In III, H. D. and Singh, A. (eds.), _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pp. 9438-9447. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/tay20a.html.
* Tong et al. (2023) Tong, A., Malkin, N., Huguet, G., Zhang, Y., Rector-Brooks, J., Fatras, K., Wolf, G., and Bengio, Y. Improving and generalizing flow-based generative models with minibatch optimal transport. In _ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems_, 2023.
* Tsoe et al. (2019)Vayer, T., Chapel, L., Flamary, R., Tavenard, R., and Courty, N. Fused Gromov-Wasserstein distance for structured objects. _Algorithms_, 13(9):212, August 2020. ISSN 1999-4893. doi: 10.3390/a13090212. URL http://dx.doi.org/10.3390/a13090212.
* Vincent-Cuez et al. (2022) Vincent-Cuez, C., Flamary, R., Corneli, M., Vayer, T., and Courty, N. Semi-relaxed Gromov-Wasserstein divergence and applications on graphs. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=RShaMexjc-x.
* Wolf et al. (2018) Wolf, F. A., Angerer, P., and Theis, F. J. SCANPY: large-scale single-cell gene expression data analysis. _Genome biology_, 19:1-5, 2018.
* Xu et al. (2019) Xu, H., Luo, D., Zha, H., and Duke, L. C. Gromov-Wasserstein Learning for Graph Matching and Node Embedding. In _International conference on machine learning_, pp. 6932-6941. PMLR, 2019.
* Yang & Leskovec (2012) Yang, J. and Leskovec, J. Defining and Evaluating Network Communities based on Ground-truth. In _Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics_, pp. 1-8, 2012.
* Yang et al. (2020) Yang, K. D., Damodaran, K., Venkatachalapathy, S., Soylemezoglu, A. C., Shivashankar, G., and Uhler, C. Predicting cell lineages using autoencoders and optimal transport. _PLoS computational biology_, 16(4):e1007828, 2020.
* Yin et al. (2017) Yin, H., Benson, A. R., Leskovec, J., and Gleich, D. F. Local Higher-Order Graph Clustering. In _Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining_, pp. 555-564, 2017.
* Zeira et al. (2022) Zeira, R., Land, M., Strzalkowski, A., and Raphael, B. J. Alignment and integration of spatial transcriptomics data. _Nature Methods_, 19(5):567-575, 2022.

Low-rank optimal transport

### Low-rank factorizations

The set of low-rank couplings.Given \(\bm{M}\in\mathbb{R}_{+}^{n\times m}\), the _nonnegative rank_ of \(\bm{M}\) is the least number of nonnegative, rank-1 matrices that sum to \(\bm{M}\):

\[\operatorname{rk}_{+}(\bm{M})=\min_{r\geq 1}\left\{\bm{M}=\sum_{i=1}^{r}\bm{M}_{ i},\ \ \text{such that}\ \operatorname{rk}(\bm{M}_{i})=1\ \text{and}\ \bm{M}_{i}\geq 0\ \text{for all}\ i\right\}.\]

Let \(\bm{a}\in\Delta_{n},\bm{b}\in\Delta_{m}\) be probability vectors, and let \(\Pi_{\bm{a},\bm{b}}(r)\) denote the set of rank-\(r\) coupling matrices with marginals \(\bm{a}\) and \(\bm{b}\):

\[\Pi_{\bm{a},\bm{b}}(r)=\{\bm{P}\in\mathbb{R}_{+}^{n\times m}:\bm{P}^{\rm T} \bm{1}_{m}=\bm{a},\,\bm{P}\bm{1}_{n}=\bm{b},\,\operatorname{rk}_{+}(\bm{P}) \leq r\}.\]

To optimize any cost over \(\Pi_{\bm{a},\bm{b}}(r)\), one requires a parameterization of this set.

Factored couplings.The _factored coupling_ parameterization of \(\Pi_{\bm{a},\bm{b}}(r)\) introduced in Forrow et al. (2019), and used by Scethon et al. (2021); Scethon and Cuturi (2022); Scethon et al. (2022, 2023) is

\[\mathsf{FC}_{\bm{a},\bm{b}}(r):=\{(\bm{Q},\bm{R},\bm{g})\in\mathbb{R}_{+}^{n \times r}\times\mathbb{R}_{+}^{m\times r}\times(\mathbb{R}_{+}^{*})^{r}:\bm{Q }\in\Pi_{\bm{a},\bm{g}},\bm{R}\in\Pi_{\bm{b},\bm{g}}\}.\]

Cohen and Rothblum (1993) show that any \(\bm{P}\in\Pi_{\bm{a},\bm{b}}(r)\) may be decomposed as \(\bm{P}=\bm{Q}\mathrm{diag}(1/\bm{g})\bm{R}^{\rm T}\) for some triple \((\bm{Q},\bm{R},\bm{g})\in\mathsf{FC}\). Thus, for cost matrix \(\bm{C}\in\mathbb{R}^{n\times m}\), the general low-rank optimal transport problem is equivalent to an optimization over factored couplings:

\[\min_{\bm{P}\in\Pi_{\bm{a},\bm{b}}(r)}\langle\bm{C},\bm{P}\rangle_{F}=\min_{( \bm{Q},\bm{R},\bm{g})\in\mathsf{FC}_{\bm{a},\bm{b}}(r)}\langle\bm{C},\bm{Q} \mathrm{diag}(1/\bm{g})\bm{R}^{\rm T}\rangle_{F}.\] (11)

Latent coupling factorization.The _latent coupling_ parameterization of \(\Pi_{\bm{a},\bm{b}}(r)\) introduced in Lin et al. (2021), and used in the present work is

\[\mathsf{LC}_{\bm{a},\bm{b}}(r):=\{(\bm{Q},\bm{R},\bm{T})\in\mathbb{R}_{+}^{n \times r}\times\mathbb{R}_{+}^{m\times r}\times\mathbb{R}_{+}^{r\times r}:\bm {Q}\in\Pi_{\bm{a},\cdot},\bm{R}\in\Pi_{\bm{b},\cdot},\bm{T}\in\Pi_{\bm{g}_{Q},\bm{g}_{R}}\},\]

where \(\bm{g}_{Q},\bm{g}_{R}\) are the inner marginals of \(\bm{Q}\) and \(\bm{R}\).

Latent coupling diagonalization.The LC-factorization recovers the factorization of Forrow et al. (2019) as a sub-case. While the diagonal factorization of previous works cannot be directly converted to the LC-factorization, the LC-factorization can easily recover the diagonal factorization. In particular, taking \(\bm{Q}^{\prime}\leftarrow\bm{Q}\,\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\) one can refactor

\[\bm{P}_{r}=\bm{Q}\,\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\,\mathrm{diag}(1/\bm{g}_ {R})\bm{R}^{\rm T}=\bm{Q}^{\prime}\,\mathrm{diag}(1/\bm{g}_{R})\bm{R}^{\rm T}\]

or alternatively taking \(\bm{R}^{\prime}=\bm{R}\,\mathrm{diag}(1/\bm{g}_{R})\bm{T}^{\rm T}\) may refactor as

\[\bm{P}_{r}=\bm{Q}\,\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\,\mathrm{diag}(1/\bm{g}_ {R})\bm{R}^{\rm T}=\bm{Q}\,\mathrm{diag}(1/\bm{g}_{Q})(\bm{R}^{\prime})^{\rm T}\]

So that instead of returning \((\bm{Q},\bm{R},\bm{T})\) one may alternatively return \((\bm{Q},\bm{R},\bm{T})\to(\bm{Q}^{\prime},\bm{R},\mathrm{diag}(\bm{g}_{R}))\) or \((\bm{Q},\bm{R},\bm{T})\to(\bm{Q},\bm{R}^{\prime},\mathrm{diag}(\bm{g}_{Q}))\) to recover the Forrow et al. (2019) factorization. An example of this diagonal-conversion is offered in Figure 12.

### Balanced low-rank optimal transport

The FRLC optimization problemOur optimization problem is over the variables \((\bm{Q},\bm{R},\bm{T})\) and defined as follows:

\[\min_{(\bm{Q},\bm{R},\bm{T})\in\mathsf{LC}_{\bm{a},\bm{b}}(r)}\mathcal{L}_{ \mathrm{LC}}(\bm{Q},\bm{R},\bm{T}),\] (12)

where our objective function \(\mathcal{L}_{\mathrm{LC}}\) is

\[\mathcal{L}_{\mathrm{LC}}(\bm{Q},\bm{R},\bm{T})=\langle\bm{C},\bm{Q}(\mathrm{ diag}(1/\bm{Q}^{\rm T}\bm{1}_{n}))\bm{T}(\mathrm{diag}(1/\bm{R}^{\rm T}\bm{1}_{m})) \bm{R}^{\rm T}\rangle\] (13)

Given \((\bm{Q},\bm{R},\bm{T})\in\mathsf{LC}_{\bm{a},\bm{b}}(r)\), sub-couplings \(\bm{Q}\) and \(\bm{R}\) are constrained by:

\[\mathcal{C}_{1}(\bm{a}):=\{(\bm{Q},\bm{R},\bm{T})\in\mathcal{R}_{+}:\bm{Q} \bm{1}_{r}=\bm{a}\},\quad\mathcal{C}_{1}(\bm{b}):=\{(\bm{Q},\bm{R},\bm{T})\in \mathcal{R}_{+}:\bm{R}\bm{1}_{r}=\bm{b}\},\]while the convex sets constraining the latent coupling matrix \(\bm{T}\) are

\[\mathcal{C}_{2}(\bm{g}_{Q}):=\{(\bm{Q},\bm{R},\bm{T})\in\mathcal{R}_{+}:\bm{T} \bm{1}_{r}=\bm{g}_{Q}\},\quad\mathcal{C}_{2}(\bm{g}_{R}):=\{(\bm{Q},\bm{R},\bm{ T})\in\mathcal{R}_{+}:\bm{T}^{T}\bm{1}_{r}=\bm{g}_{R}\},\]

where \(\bm{g}_{Q}=\bm{Q}^{\mathrm{T}}\bm{1}_{n}\) and \(\bm{g}_{R}=\bm{R}^{\mathrm{T}}\bm{1}_{m}\) as per Definition 3.1. Under these definitions, \(\mathsf{LC}_{\bm{a},\bm{b}}(r)=\mathcal{C}_{1}\cap\mathcal{C}_{2}\), where

\[\mathcal{C}_{1}=\mathcal{C}_{1}(\bm{a})\cap\mathcal{C}_{1}(\bm{b})\quad\text{ and}\quad\mathcal{C}_{2}=\mathcal{C}_{2}(\bm{g}_{Q})\cap\mathcal{C}_{2}(\bm{g}_{R})\] (14)

To solve (12), we separate the variables into two "blocks" of variables, \((\bm{Q},\bm{R})\) and \(\bm{T}\), and perform two block updates per iteration, as follows. Let \((\gamma_{k})_{k\geq 0}\) be a positive sequence of stepsizes. Suppose we have \((\bm{Q}_{k},\bm{R}_{k},\bm{T}_{k})\in\mathcal{C}\). We update the first variable block \((\bm{Q},\bm{R})\) by taking a locally optimal (mirror descent) update step, while the second variable block \(\bm{T}\) is held fixed:

\[(\bm{Q}_{k+1},\bm{R}_{k+1})\leftarrow\operatorname*{arg\,min}_{(\bm{Q},\bm{R} ):(\bm{Q},\bm{R},\bm{T}_{k})\in\mathcal{C}_{1}}\langle(\bm{Q},\bm{R}),\nabla_{ (\bm{Q},\bm{R})}\mathcal{L}_{\mathrm{LC}}\rangle+\frac{1}{\gamma_{k}}\mathrm{ KL}((\bm{Q},\bm{R})\|(\bm{Q}_{k},\bm{R}_{k}))\] (15)

Here, we slightly abused the notation by putting \((\bm{Q},\bm{R})\) inside an inner product. The triple \((\bm{Q}_{k+1},\bm{R}_{k+1},\bm{T}_{k})\) produced by this update lies in \(\mathcal{C}_{1}\). Next, we update \(\bm{T}\), the second variable block, by taking another locally optimal (mirror descent) step, while the first variable block is held fixed.

\[\bm{T}_{k+1}\leftarrow\operatorname*{arg\,min}_{\bm{T}:(\bm{Q}_{k+1},\bm{R}_{ k+1},\bm{T})\in\mathcal{C}_{2}}\langle\bm{T},\nabla_{\bm{T}}\mathcal{L}_{ \mathrm{LC}}\rangle+\frac{1}{\gamma_{k}}\mathrm{KL}(\bm{T}\|\bm{T}_{k}).\] (16)

By construction, the triple \((\bm{Q}_{k+1},\bm{R}_{k+1},\bm{T}_{k+1})\in\mathcal{C}_{2}\). However, because the set \(\mathcal{C}_{1}\) only constrains the first variable block \(\bm{Q}_{k+1},\bm{R}_{k+1}\), we have that \((\bm{Q}_{k+1},\bm{R}_{k+1},\bm{T}_{k+1})\in\mathcal{C}_{2}\), and hence \((\bm{Q}_{k+1},\bm{R}_{k+1},\bm{T}_{k+1})\in\mathcal{C}\). Thus, each iteration produces a feasible triple \((\bm{Q}_{k+1},\bm{R}_{k+1},\bm{T}_{k+1})\) through a pair of locally optimal block updates.

The LOT optimization problem [1]For comparison, recall the optimization problem that is solved in the LOT framework:

\[\min_{(\bm{Q},\bm{R},\bm{g})\in\widetilde{\mathcal{C}}}\mathcal{L}_{\mathrm{ LOT}},\] (17)

where the objective function \(\mathcal{L}_{\mathrm{LOT}}\) is

\[\mathcal{L}_{\mathrm{LOT}}:=\langle\bm{C},\bm{Q}\mathrm{diag}(1/\bm{g})\bm{R} ^{\mathrm{T}}\rangle\] (18)

and where \(\widetilde{\mathcal{C}}=\widetilde{\mathcal{C}}_{1}\cap\widetilde{\mathcal{C} }_{2}\) with:

\[\widetilde{\mathcal{C}}_{1} :=\{(\bm{Q},\bm{R},\bm{g})\in\mathbb{R}_{+}^{n\times r}\times \mathbb{R}_{+}^{m\times r}\times(\mathbb{R}_{+}^{n})^{r}:\bm{Q}\bm{1}_{r}=\bm{a },\bm{R}\bm{1}_{r}=\bm{b}\}\] \[\widetilde{\mathcal{C}}_{2} :=\{(\bm{Q},\bm{R},\bm{g})\in\mathbb{R}_{+}^{n\times r}\times \mathbb{R}_{+}^{m\times r}\times\mathbb{R}_{+}^{r}:\bm{Q}^{\mathrm{T}}\bm{1}_{n} =\bm{g}=\bm{R}^{\mathrm{T}}\bm{1}_{m}\}.\]

Here, there are also three optimization variables \((\bm{Q},\bm{R},\bm{g})\), but they are updated _together_ in each iteration of LOT. That is, given feasible \((\bm{Q}_{k},\bm{R}_{k},\bm{g}_{k})\in\mathcal{C}\), an iteration of LOT updates this triple via

\[(\bm{Q}_{k+1},\bm{R}_{k+1},\bm{g}_{k+1}):=\operatorname*{arg\,min}_{(\bm{Q},\bm {R},\bm{g})\in\widetilde{\mathcal{C}}}\langle(\bm{Q},\bm{R},\bm{g}),\nabla_{( \bm{Q},\bm{R},\bm{g})}\mathcal{L}_{\mathrm{LOT}}\rangle+\frac{1}{\gamma_{k}} \mathrm{KL}((\bm{Q},\bm{R},\bm{g})\|(\bm{Q}_{k},\bm{R}_{k},\bm{g}_{k})),\]

where \((\gamma_{k})_{k\geq 0}\) is again a positive sequence of stepsizes. Scethon et al. (2021) then compute the unconstrained argmin across all variables to yield a set of unconstrained kernels \((\bm{K}_{Q},\bm{K}_{R},\bm{k}_{g})\), using Dykstra to jointly project the unconstrained update onto the intersection \(\widetilde{\mathcal{C}}\) of the constraint sets.

OT subroutine in FRLCTo see why we do not need Dykstra in the FRLC scheme, observe that the update (15) of variables \((\bm{Q},\bm{R})\) can be equivalently expressed as

\[(\bm{Q}_{k+1},\bm{R}_{k+1})\leftarrow\operatorname*{arg\,min}_{\bm{Q},\bm{R}:( \bm{Q},\bm{R},\bm{T}_{k})\in\mathcal{C}_{1}}\langle(\bm{Q},\bm{R}),\nabla_{( \bm{Q},\bm{R})}\mathcal{L}_{\mathrm{LC}}\rangle+\frac{1}{\gamma_{k}}\mathrm{ KL}((\bm{Q},\bm{R})\|(\bm{Q}_{k},\bm{R}_{k})),\]

Thus, even though the pair \((\bm{Q},\bm{R})\) is being updated at this step, solving for \((\bm{Q}_{k+1},\bm{R}_{k+1})\) above is equivalent to updating each individually because \(\bm{Q}\) and \(\bm{R}\) do not share an inner marginal:

\[\bm{Q}_{k+1}\leftarrow\operatorname*{arg\,min}_{\bm{Q}:\bm{Q}1_{r}=\bm{a}} \langle\bm{Q},\nabla_{\bm{Q}}\mathcal{L}_{\mathrm{LC}}\rangle+\frac{1}{\gamma_{k}} \mathrm{KL}(\bm{Q}\|\bm{Q}_{k})\] (19)\[\bm{R}_{k+1}\leftarrow\operatorname*{arg\,min}_{\bm{R}\,:\,\bm{R}\bm{1}_{r}=\bm{b}} \langle\bm{R},\nabla_{\bm{R}}\mathcal{L}_{\mathrm{LC}}\rangle+\frac{1}{\gamma_{ k}}\mathrm{KL}(\bm{R}\|\bm{R}_{k})\] (20)

We choose to add the regularization \(\tau\mathrm{KL}((\bm{Q}^{\mathrm{T}}\bm{1}_{n},\bm{R}^{\mathrm{T}}\bm{1}_{m}) \|(\bm{Q}_{k}^{\mathrm{T}}\bm{1}_{n},\bm{R}_{k}^{\mathrm{T}}\bm{1}_{m}))\) to turn each update here into an entropy-regularized semi-relaxed optimal transport problem, and to ensure \(\beta\)-smoothness:

\[\bm{Q}_{k+1}\leftarrow\operatorname*{arg\,min}_{\bm{Q}\,:\,\bm{Q}\bm{1}_{r}= \bm{a}}\langle\bm{Q},\nabla_{\bm{Q}}\mathcal{L}_{\mathrm{LC}}\rangle+\frac{1} {\gamma_{k}}\mathrm{KL}(\bm{Q}\|\bm{Q}_{k})+\tau\mathrm{KL}(\bm{Q}^{\mathrm{T }}\bm{1}_{n}\|\bm{Q}_{k}^{\mathrm{T}}\bm{1}_{n})\] (21)

\[\bm{R}_{k+1}\leftarrow\operatorname*{arg\,min}_{\bm{R}\,:\,\bm{R}\bm{1}_{r}= \bm{b}}\langle\bm{R},\nabla_{\bm{R}}\mathcal{L}_{\mathrm{LC}}\rangle+\frac{1} {\gamma_{k}}\mathrm{KL}(\bm{R}\|\bm{R}_{k})+\tau\mathrm{KL}(\bm{R}^{\mathrm{T }}\bm{1}_{m}\|\bm{R}_{k}^{\mathrm{T}}\bm{1}_{m})\] (22)

After updating \(\bm{Q}\) and \(\bm{R}\), the update on \(\bm{T}\) then follows a similar form:

\[\bm{T}_{k+1}\leftarrow\operatorname*{arg\,min}_{\bm{T}\,:\,(\bm{Q}_{k+1},\bm{ R}_{k+1},\bm{T})\in\mathcal{C}_{2}}\langle\bm{T},\nabla_{\bm{T}}\mathcal{L}_{ \mathrm{LC}}\rangle+\frac{1}{\gamma_{k}}\mathrm{KL}(\bm{T}\|\bm{T}_{k}),\] (23)

leading to balanced constraints on \(\bm{T}\) of the form \(\bm{T}^{\mathrm{T}}\bm{1}_{r}=\bm{Q}_{k+1}^{\mathrm{T}}\bm{1}_{n}\) and \(\bm{T}\bm{1}_{r}=\bm{R}_{k+1}^{\mathrm{T}}\bm{1}_{m}\), allowing the problem to be solved by Sinkhorn.

Importantly, there are no constraints in \(\mathcal{C}_{1}\) involving both \(\bm{Q}\) and \(\bm{R}\), which is what allows the optimization to split in this way. If there were such constraints, we would have needed to use Dykstra to update the pair \((\bm{Q},\bm{R})\). Because our update scheme is equivalent to the three updates of individual variables given in (21), (22), (23), we can solve for each update using optimal transport. As \(\bm{Q}\) and \(\bm{R}\) are not required to match the inner marginals exactly, the OT problems associated to \(\bm{Q}\) and \(\bm{R}\) are semi-relaxed by construction.

The separation of our block updates into a step where \((\bm{Q},\bm{R},\bm{T}_{k})\in\mathcal{C}_{1}\) and \((\bm{Q}_{k+1},\bm{R}_{k+1},\bm{T})\in\mathcal{C}_{2}\) allows us to entirely remove the optimization over inner marginals \(\bm{g}_{Q}\) and \(\bm{g}_{R}\), as done in all previous works on low-rank optimal transport which optimize \(\bm{g}\) explicitly as both a variable and a constraint of the optimization Scetbon et al. (2021, 2023); Scetbon and Cuturi (2022). If one were to introduce an extended loss in the style of previous works which adds \(\bm{g}_{Q}\) and \(\bm{g}_{R}\) as variables in the form \(\mathcal{H}(\bm{Q},\bm{R},\bm{T},\bm{g}_{Q},\bm{g}_{R})=\langle\bm{Q}\mathrm{ diag}(1/\bm{g}_{Q})\bm{T}\mathrm{diag}(1/\bm{g}_{R})\bm{R}^{\mathrm{T}},\bm{C} \rangle_{F}\), one observes an equivalence to simply taking a semi-relaxed projection.

**Lemma A.1**.: _Define the function \(\mathcal{H}(\bm{Q},\bm{R},\bm{T},\bm{g}_{Q},\bm{g}_{R})=\langle\bm{C},\bm{Q} \mathrm{diag}(1/\bm{g}_{Q})\bm{T}\mathrm{diag}(1/\bm{g}_{R})\bm{R}^{\mathrm{T}} \rangle_{F}\) and let \(\mathcal{L}_{\mathrm{LC}}(\bm{Q},\bm{R},\bm{T})\) be as in (13). One has the following equivalence:_

\[\min_{\bm{g}_{R}\in\Delta_{r},\bm{g}_{Q}\in\Delta_{r},\bm{Q}\in\Pi_{\bm{a}_{Q},\bm{R}\in\Pi_{\bm{b}_{Q}},\bm{R}\in\Pi_{\bm{b}_{Q}},\bm{R}}}\mathcal{H}(\bm{ Q},\bm{R},\bm{T}_{k},\bm{g}_{Q},\bm{g}_{R})=\min_{(\bm{Q},\bm{R},\bm{T}_{k})\in \mathcal{C}_{1}}\mathcal{L}_{\mathrm{LC}}(\bm{Q},\bm{R},\bm{T}_{k})\] (24)

_Thus the semi-relaxed projections yield locally optimal inner marginals._

Proof.: To see why this is true, notice that, so long as the outer marginals are tightly satisfied \(\bm{Q}\bm{1}_{r}=\bm{a}\) and \(\bm{R}\bm{1}_{r}=\bm{b}\) for \(\bm{Q}\geq\bm{0}_{n\times r},\bm{R}\geq\bm{0}_{m\times r}\), one has

\[\sum_{i}\bm{g}_{R,i}=\sum_{i}\langle\bm{R}_{i,.}^{\mathrm{T}},\bm{1}_{m} \rangle=\sum_{i}\langle\bm{R}_{.,i},\bm{1}_{m}\rangle=\sum_{\ell}\sum_{i}\bm{ R}_{\ell,i}=\sum_{\ell}\bm{b}_{\ell}=1\]

and

\[\sum_{i}\bm{g}_{Q,i}=\sum_{i}\langle\bm{Q}_{i,.}^{\mathrm{T}},\bm{1}_{n} \rangle=\sum_{i}\langle\bm{Q}_{.,i},\bm{1}_{m}\rangle=\sum_{\ell}\sum_{i}\bm{ Q}_{\ell,i}=\sum_{\ell}\bm{a}_{\ell}=1\]

Therefore, the inner marginals \(\bm{g}_{Q}^{*}=(\bm{Q}^{*})^{\mathrm{T}}\bm{1}_{n}\) and \(\bm{g}_{R}^{*}=(\bm{R}^{*})^{\mathrm{T}}\bm{1}_{m}\) induced by the optimal \(\bm{Q}^{*},\bm{R}^{*}\) of the optimization problem on the right hand side satisfy the constraints \(\bm{g}_{R}^{*}\in\Delta_{r},\bm{g}_{Q}^{*}\in\Delta_{r}\) on the left hand side, so the two minimums coincide. 

This implies that an extra optimization for \(\bm{g}_{Q}\) and \(\bm{g}_{R}\) is unnecessary in a coordinate update which alternates \((\bm{Q},\bm{R})\) and \(\bm{T}\). If we did not perform a block-update, in the form of standard MD in the objective described above, we would encounter some difficulty. In particular \(\bm{g}_{Q}\) and \(\bm{g}_{R}\) would optimized with the constraint \(\bm{g}_{Q}\in\Delta_{r}\) and \(\bm{g}_{R}\in\Delta_{r}\), and would concurrently constrain all of the other variables as \(\bm{Q}^{\mathrm{T}}\bm{1}_{n}=\bm{g}_{Q}\), \(\bm{T}\bm{1}_{r}=\bm{g}_{Q}\), and \(\bm{R}^{\mathrm{T}}\bm{1}_{m}=\bm{g}_{R}\), \(\bm{T}^{\mathrm{T}}\bm{1}_{r}=\bm{g}_{R}\).

Block-Coordinate steps for the OT sub-problems

We use a latent non-diagonal coupling instead of an inner diagonal coupling \(\mathrm{diag}(\bm{g})\) of the form of Forrow et al. (2019). This allows us to loosen the constraint that the inner marginals have to be joined by a common coupling \(\bm{Q}^{\mathrm{T}}\bm{1}_{n}=\bm{R}^{\mathrm{T}}\bm{1}_{m}=\bm{g}\). The fundamental advantage of this choice is that we can decouple the convex-optimization problem for \((\bm{Q},\bm{R},\bm{T})\) entirely. One can simply solve for the optimal \(\bm{Q}\) and \(\bm{R}\)_independently_, yield the associated inner marginals for each \(\bm{Q}^{\mathrm{T}}\bm{1}_{n}=\bm{g}_{\bm{Q}}\) and \(\bm{R}^{\mathrm{T}}\bm{1}_{m}=\bm{g}_{\bm{R}}\), and then find the optimal \(\bm{T}\) which links the two. This link is provided by the aforementioned form of the problem, where:

\[\bm{P}=\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\]

For \(\bm{Q},\bm{R}\) in either the appropriate set of couplings or a relaxation thereof (which we will describe shortly). \(\bm{X}\) is related to \(\bm{T}\) by:

\[\bm{X}=\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\,\mathrm{diag}(1/\bm{g}_{R})\]

And, \(\bm{T}\in\Pi_{\bm{g}_{\bm{Q}},\bm{g}_{\bm{R}}}\) consistently for all cases. As the semi-relaxed case is intermediate between fully-relaxed and balanced, it has ideas which generalize to both directly. As such, we use it as the leading example again. As in Sectbon et al. (2021), we take proximal-steps of the form:

\[\min_{\bm{\zeta}}\langle\nabla\mathcal{L}(\bm{\zeta})\mid_{\bm{\zeta}_{k}}, \bm{\zeta}\rangle_{F}+\frac{1}{\gamma_{k}}\mathrm{KL}(\bm{\zeta}\|\bm{K}^{(k )})\]

Where these steps are now in block-wise fashion on \((\bm{Q},\bm{R})\) and \(\bm{T}\), rather than joint. One may identify for each sub-factor in \((\bm{Q},\bm{R})\) and \(\bm{T}\) a linearized gradient as before, which yields a set of objectives which each solve an independent optimal-transport for the sub-factors. In particular, we have that:

\[\langle\bm{Q}\bm{X}\bm{R}^{\mathrm{T}},\bm{C}\rangle_{F} =\mathrm{Tr}\left[\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\bm{C}^{\mathrm{ T}}\right]=\langle\bm{C}\bm{R}\bm{X}^{\mathrm{T}},\bm{Q}\rangle_{F}\] \[\langle\bm{Q}\bm{X}\bm{R}^{\mathrm{T}},\bm{C}\rangle_{F} =\mathrm{Tr}\left[\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\bm{C}^{\mathrm{ T}}\right]=\langle\bm{C}^{\mathrm{T}}\bm{Q}\bm{X},\bm{R}\rangle_{F}\] \[\langle\bm{Q}\bm{X}\bm{R}^{\mathrm{T}},\bm{C}\rangle_{F} =\mathrm{Tr}\left[\bm{R}^{\mathrm{T}}\bm{C}^{\mathrm{T}}\bm{Q}\bm{ X}\right]=\langle\bm{Q}^{\mathrm{T}}\bm{C}\bm{R},\bm{X}\rangle_{F}\]

A linearization in the left-slot of the inner product as \(\langle\bm{Q},\bm{C}\bm{R}\bm{X}(\bm{Q}_{k})^{\mathrm{T}}\rangle:=\langle\bm {Q},\bm{C}\bm{R}\bm{X}^{\mathrm{T}}\rangle\) or \(\langle\bm{C}^{\mathrm{T}}\bm{Q}\bm{X}(\bm{R}_{k}),\bm{R}\rangle_{F}:=\langle \bm{C}^{\mathrm{T}}\bm{Q}\bm{X},\bm{R}\rangle_{F}\) is common practice for quadratic problems. In this case, the directional derivative of \(\bm{Q}\) and \(\bm{R}\) in the matrix-direction \(\bm{V}\) are respectively:

\[D\langle\bm{C}\bm{R}\bm{X}^{\mathrm{T}},\bm{Q}\rangle_{F}\circ( \bm{V}) =\langle\bm{C}\bm{R}\bm{X}^{\mathrm{T}},\bm{V}\rangle_{F}\implies \nabla_{\bm{Q}}\mathcal{L}=\bm{C}\bm{R}\bm{X}^{\mathrm{T}}\] \[D\langle\bm{C}^{\mathrm{T}}\bm{Q}\bm{X},\bm{R}\rangle_{F}\circ( \bm{V}) =\langle\bm{C}^{\mathrm{T}}\bm{Q}\bm{X},\bm{V}\rangle_{F}\implies \nabla_{\bm{R}}\mathcal{L}=\bm{C}^{\mathrm{T}}\bm{Q}\bm{X}\]

Without this linearization assumption on \(\bm{X}\), the full gradient may be evaluated as well. In particular, we note that for \(\mathrm{diag}^{-1}(\cdot)\) the matrix-to-vector extraction of the diagonal, the directional derivative on \(\bm{Q}\) is:

\[D\langle\bm{C},\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\rangle_{F}\circ \bm{V} =\langle\bm{C}\bm{R}\bm{X}^{\mathrm{T}},\bm{V}\rangle_{F}+\langle \bm{C},\bm{Q}\bm{D}\bm{X}^{\mathrm{T}}\circ(\bm{V})\bm{R}^{\mathrm{T}}\rangle_{F}\] \[=\langle\bm{C}\bm{R}\bm{X}^{\mathrm{T}}-\bm{1}_{n}\mathrm{diag}^{- 1}((\bm{C}\bm{R}\bm{X}^{\mathrm{T}})^{\mathrm{T}}\bm{Q}\mathrm{diag}(1/\bm{g}_{ Q}))^{\mathrm{T}},\bm{V}\rangle_{F}\]

Thus, without the linearization assumption one may use product rule on \(\bm{X}\) as an implicit function of \(\bm{Q}\) (resp. \(\bm{R}\)) to take the total derivative:

\[\nabla_{\bm{Q}}\mathcal{L}_{\mathrm{FRLC}} =\bm{C}\bm{R}\bm{X}^{\mathrm{T}}-\bm{1}_{n}\mathrm{diag}^{-1}(( \bm{C}\bm{R}\bm{X}^{\mathrm{T}})^{\mathrm{T}}\bm{Q}\mathrm{diag}(1/\bm{g}_{ Q}))^{\mathrm{T}}\]

Likewise for \(\bm{R}\),

\[D\langle\bm{C},\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\rangle_{F}\circ\bm {V} =\langle\bm{C}^{\mathrm{T}}\bm{Q}\bm{X},\bm{V}\rangle_{F}+\langle \bm{C},\bm{Q}\bm{D}\bm{X}\circ(\bm{V})\bm{R}^{\mathrm{T}}\rangle_{F}\] \[=\langle\bm{C}^{\mathrm{T}}\bm{Q}\bm{X}-\bm{1}_{m}\mathrm{diag}^{- 1}(\mathrm{diag}(1/\bm{g}_{R})\bm{R}^{\mathrm{T}}\bm{C}^{\mathrm{T}}\bm{Q}\bm{X} )^{\mathrm{T}},\bm{V}\rangle_{F},\]

and so

\[\nabla_{\bm{R}}\mathcal{L}_{\mathrm{FRLC}} =\bm{C}^{\mathrm{T}}\bm{Q}\bm{X}-\bm{1}_{m}\mathrm{diag}^{-1}( \mathrm{diag}(1/\bm{g}_{R})\bm{R}^{\mathrm{T}}\bm{C}^{\mathrm{T}}\bm{Q}\bm{X} )^{\mathrm{T}}.\]

Both the \(\mathrm{W}\) and \(\mathrm{GW}\) problem have these rank-one perturbations of the gradient from the derivative with respect to \(\bm{X}\).

Lastly, owing to the block-coordinate updates which fix \((\bm{Q},\bm{R})\) preceding the update on \(\bm{T}\), the derivative on \(\bm{T}\) follows directly by chain rule on \(\bm{X}\):

\[D\langle\bm{Q}^{\mathrm{T}}\bm{C}\bm{R},\bm{X}\rangle_{F}\circ( \bm{V})=\langle\bm{Q}^{\mathrm{T}}\bm{C}\bm{R},\bm{V}\rangle_{F}\implies\nabla_ {\bm{X}}\mathcal{L} =\bm{Q}^{\mathrm{T}}\bm{C}\bm{R}\] \[\nabla_{\bm{T}}\mathcal{L} =\mathrm{diag}(1/\bm{g}_{\bm{Q}})\bm{Q}^{\mathrm{T}}\bm{C}\bm{R} \,\mathrm{diag}(1/\bm{g}_{\bm{R}})\]

Let \((\gamma_{k})\) be a sequence of step sizes and consider the first-order conditions required for the proximal step as before. From these we have the updated proximal-step updates:

\[\bm{K}_{\bm{Q}}^{(k)} \leftarrow\min_{\bm{Q}}\langle\bm{C}\bm{R}\bm{X}^{\mathrm{T}}, \bm{Q}\rangle_{F}+\frac{1}{\gamma_{k}}\mathrm{KL}(\bm{Q}_{k}\|\bm{K}_{\bm{Q}}^ {(k)})\] \[\bm{K}_{\bm{R}}^{(k)} \leftarrow\min_{\bm{R}}\langle\bm{C}^{\mathrm{T}}\bm{Q}\bm{X},\bm {R}\rangle_{F}+\frac{1}{\gamma_{k}}\mathrm{KL}(\bm{R}_{k}\|\bm{K}_{\bm{R}}^{( k)})\] \[\bm{K}_{\bm{T}}^{(k)} \leftarrow\min_{\bm{T}}\langle\bm{Q}^{\mathrm{T}}\bm{C}\bm{R},\bm {X}\rangle_{F}+\frac{1}{\gamma_{k}}\mathrm{KL}(\bm{T}_{k}\|\bm{K}_{\bm{T}}^{( k)}),\]

with kernels \(\bm{K}_{\bm{\zeta}_{j}}^{(k)}\), for \(j=1,2,3\) given by

\[\bm{K}_{\bm{Q}}^{(k)} :=\bm{Q}_{k}\odot\exp(\,-\gamma_{k}\bm{C}\bm{R}_{k}\bm{X}_{k}^{ \mathrm{T}}\,)\] \[\bm{K}_{\bm{R}}^{(k)} :=\bm{R}_{k}\odot\exp(\,-\gamma_{k}\bm{C}^{\mathrm{T}}\bm{Q}_{k} \bm{X}_{k}\,)\] \[\bm{K}_{\bm{T}}^{(k)} :=\bm{T}_{k}\odot\exp(\,-\gamma_{k}\mathrm{diag}(\bm{g}_{Q}^{-1}) \bm{Q}^{\mathrm{T}}\bm{C}\bm{R}\mathrm{diag}(\bm{g}_{R}^{-1})\,)\]

Or, dropping the linearization assumption on \((\bm{Q},\bm{R})\) the updates are: \(\bm{K}_{\bm{\zeta}_{j}}^{(k)}\), for \(j=1,2\) given by

\[\bm{K}_{\bm{Q}}^{(k)} :=\bm{Q}_{k}\odot\exp(\,-\gamma_{k}(\bm{C}\bm{R}_{k}\bm{X}_{k}^{ \mathrm{T}}-\bm{1}_{n}\mathrm{diag}^{-1}((\bm{C}\bm{R}_{k}\bm{X}_{k}^{ \mathrm{T}})^{\mathrm{T}}\bm{Q}_{k}\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k}}))^{ \mathrm{T}})\,)\] \[\bm{K}_{\bm{R}}^{(k)} :=\bm{R}_{k}\odot\exp(\,-\gamma_{k}(\bm{C}^{\mathrm{T}}\bm{Q}_{k} \bm{X}_{k}-\bm{1}_{m}\mathrm{diag}^{-1}(\mathrm{diag}(1/\bm{g}_{R})\bm{R}_{k} ^{\mathrm{T}}\bm{C}^{\mathrm{T}}\bm{Q}_{k}\bm{X}_{k})^{\mathrm{T}})\,)\]

The first projection is onto the set that satisfies the marginal constraint \(\bm{R}\bm{1}_{r}=\bm{b}\). In particular, following the discussion above, one has the coordinate-MD step:

\[\min_{(\bm{Q},\bm{R},\bm{T})} \frac{1}{\gamma_{k}}\mathrm{KL}\left(\langle\bm{Q},\bm{R},\bm{T} \rangle\,\|\,(\bm{K}_{\bm{Q}},\bm{K}_{\bm{R}},\bm{K}_{\bm{T}})\right)+\tau \mathrm{KL}(\bm{Q}\bm{1}_{r}\|\bm{a})\] (25) \[s.t. \bm{R}\bm{1}_{r}=\bm{b}\]

As before, there is no difference from the previous case, where one takes the unconstrained projection \(\bm{R}=\mathrm{diag}(\bm{b}/\bm{K}_{\bm{R}}\bm{1}_{r})\bm{K}_{\bm{R}}\). To generalize this, we also consider adding a soft-constraint on the inner marginal of \(\bm{R}\) to be near that of the previous iteration. In particular, we consider the problem:

\[\min_{(\bm{Q},\bm{R},\bm{T})} \frac{1}{\gamma_{k}}\mathrm{KL}((\bm{Q},\bm{R},\bm{T})\|(\bm{K}_{ \bm{Q}},\bm{K}_{\bm{R}},\bm{K}_{\bm{T}}))\] (26) \[\quad+\tau\mathrm{KL}(\bm{Q}\bm{1}_{r}\|\bm{a})+\tau\mathrm{KL}( \bm{R}^{\mathrm{T}}\bm{1}_{m}\|\bm{g}_{R}^{(k-1)}\!\!\equiv R_{k-1}^{\mathrm{ T}}\!1_{m})\] \[s.t. \bm{R}\bm{1}_{r}=\bm{b}\]

Which yields the relaxed solution of \(\bm{R}=\mathrm{SR}^{\mathrm{R}}\)-_projection_\((\bm{K}_{\bm{R}},\gamma_{k},\tau,\bm{b},\bm{g}_{R}^{(k-1)})\) which generalizes the original projection and is equivalent to it for \(\tau=0\). This regularization is essential, as it ensures \(\beta\)-smoothness of the objective. For \(\bm{Q}\), as the constraint on \(\bm{g}\) is fully relaxed, the Lagrange multiplier \(\bm{\lambda}_{1}=\bm{0}\) entirely, such that the problem 40 now becomes fully-unconstrained:

\[\inf_{\bm{Q}}\left(\frac{1}{\gamma_{k}}\mathrm{KL}(\bm{Q}\|\bm{K}_{\bm{Q}})+ \tau\mathrm{KL}(\bm{Q}\bm{1}_{r}\|\bm{a})\right)\] (27)

To generalize this solution, we again consider adding a soft-regularization on the inner marginal of \(\bm{Q}\), where we consider the alternate problem:

\[\inf_{\bm{Q}}\left(\frac{1}{\gamma_{k}}\mathrm{KL}(\bm{Q}\|\bm{K}_{\bm{Q}})+ \tau\mathrm{KL}(\bm{Q}\bm{1}_{r}\|\bm{a})+\tau\mathrm{KL}(\bm{Q}^{\mathrm{T}} \bm{1}_{n}\|\bm{g}_{\bm{Q}}^{(k-1)}\!\!\equiv Q_{k-1}^{\mathrm{T}}\!1_{n})\right)\] (28)Which trivially recovers the original for \(\tau=0\). This form has a solution given by an unbalanced optimal transport with kernel \(\bm{K_{Q}}\). We see these two, convex problems in sequence give _independent_ optimal solutions for \(\bm{Q}\) and \(\bm{R}\) as the two matrices are not required to share an inner marginal. The last step is to link them via \(\bm{T}\), corresponding to the projection of \((\bm{Q}_{k},\bm{R}_{k},\bm{T})\) onto the set of valid rank-\(r\) couplings \(\min_{(\bm{Q}_{k},\bm{R}_{k},\bm{T})\in C}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k}, \bm{R}_{k},\bm{T}):=\min_{\bm{T}\in\Pi(g_{Q},g_{R})}\mathcal{L}_{\mathrm{LC}}( \bm{Q}_{k},\bm{R}_{k},\bm{T})\). We verify in C that if \(\bm{T}\in\Pi_{\bm{g}_{Q}=\bm{Q}^{\mathrm{T}}\bm{1}_{n},\bm{g}_{R}=\bm{R}^{ \mathrm{T}}\bm{1}_{m}}\) then \(\bm{P}\in\Pi_{\bm{a},\bm{b}}\). As such, one does not require any projection onto the intersection of convex sets, as done in Sectionb et al. (2021, 2023) via the Dykstra projection algorithm Dykstra (1983). Alternating a coordinate-MD step in \((\bm{Q},\bm{R})\) and a step on \(\bm{T}\), one not only minimizes the objective in an alternating fashion but remains in the feasible set without the need for projection algorithms beyond Sinkhorn. As such, the final linking step is done via:

\[\begin{split}\min_{\bm{T}}&\frac{1}{\gamma_{k}} \mathrm{KL}(\bm{T}\|\bm{K_{T}})\\ s.t.&\bm{T}\bm{1}_{r}=\bm{g}_{\bm{Q}},\bm{T}^{ \mathrm{T}}\bm{1}_{r}=\bm{g}_{\bm{R}}\end{split}\] (29)

This formulation amounts to solving a _balanced_ Sinkhorn problem on \(\bm{T}\) with respect to the proximal step kernel matrix.

``` Input \(\bm{K},\gamma,\tau,\bm{a},\bm{b},\delta\)\(\bm{u}\leftarrow\bm{1}_{n}\)\(\bm{v}\leftarrow\bm{1}_{r}\) repeat\(\tilde{\bm{u}}\leftarrow\bm{u}\)\(\tilde{\bm{v}}\leftarrow\bm{v}\)\(\bm{u}\leftarrow(\bm{a}/\bm{K}\bm{v})\)\(\bm{v}\leftarrow\left(\bm{b}/\bm{K}^{\mathrm{T}}\bm{u}\right)^{\tau/(\tau+\gamma^{-1})}\) until\(\gamma^{-1}\max\{\|\!\log\tilde{\bm{u}}/\bm{u}\|_{\infty},\|\log\tilde{\bm{v}}/\bm{v}\|_{ \infty}\}<\delta\) return \(\mathrm{diag}(\bm{u})\bm{K}\:\mathrm{diag}(\bm{v})\) ```

**Algorithm 2**\(\mathrm{SR}^{\mathrm{R}}\)-projection _(semi-relaxed OT, right marginal relaxed)_

``` Input \(\bm{K},\gamma,\tau,\bm{a},\bm{b},\delta\)\(\bm{u}\leftarrow\bm{1}_{n}\)\(\bm{v}\leftarrow\bm{1}_{r}\)repeat\(\tilde{\bm{u}}\leftarrow\bm{u}\)\(\tilde{\bm{v}}\leftarrow\bm{v}\)\(\bm{u}\leftarrow\left(\bm{a}/\bm{K}\bm{v}\right)^{\tau/(\tau+\gamma^{-1})}\)\(\bm{v}\leftarrow\left(\bm{b}/\bm{K}^{\mathrm{T}}\bm{u}\right)^{\tau/(\tau+\gamma^{-1})}\) until\(\gamma^{-1}\max\{\|\!\log\tilde{\bm{u}}/\bm{u}\|_{\infty},\|\!\log\tilde{\bm{v}}/\bm{v}\|_{ \infty}\}<\delta\) return \(\mathrm{diag}(\bm{u})\bm{K}\:\mathrm{diag}(\bm{v})\) ```

**Algorithm 3** U-projection _(unbalanced OT)_

## Appendix C The Latent Coupling Matrix

To solve the balanced form (and generalize the principle to the relaxed problems), we consider an alternative parametrization of the inner matrix. In particular, previous works Scetbon et al. (2021, 2023) consider \(\mathrm{diag}(1/\bm{g})\) to be the inner matrix, with marginals \(\bm{Q}^{\mathrm{T}}\bm{1}_{n}=\bm{R}^{\mathrm{T}}\bm{1}_{m}=\bm{g}\) to ensure that the outer conditions \(\bm{Q}\bm{1}_{r}=\bm{a}\) and \(\bm{R}\bm{1}_{r}=\bm{b}\) hold. We instead relax the constraint that \(\bm{Q}^{\mathrm{T}}\bm{1}_{n}\) and \(\bm{R}^{\mathrm{T}}\bm{1}_{m}\) are equal, by allowing \(\bm{Q}^{\mathrm{T}}\bm{1}_{n}=\bm{g}_{Q}\) and \(\bm{R}^{\mathrm{T}}\bm{1}_{m}=\bm{g}_{R}\) to vary arbitrarily, and considering a non-diagonal inner matrix \(\bm{X}\in\mathbb{R}^{r\times r}\) in the place of \(\mathrm{diag}(1/\bm{g})\) where we have the conditions:

\[\bm{X}\bm{g}_{R}=\bm{X}\bm{R}^{\mathrm{T}}\bm{1}_{m}=\bm{1}_{r}\]

And

\[\bm{X}^{\mathrm{T}}\bm{g}_{Q}=\bm{X}\bm{Q}^{\mathrm{T}}\bm{1}_{n}=\bm{1}_{r}\]Thus, if one considers the coupling formed as \(\bm{P}_{r}=\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\), one maintains that \(\bm{P}_{r}\in\Pi_{\bm{a},\bm{b}}\) from the condition of \(\mathcal{C}_{1}\), defined in the balanced case as in (14). We clearly have that:

\[\bm{P}_{r}\bm{1}_{m}=\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\bm{1}_{m}=\bm{Q}\bm{1}_{r}= \bm{a}\]

And:

\[\bm{P}_{r}^{\mathrm{T}}\bm{1}_{n}=\bm{R}\bm{X}^{\mathrm{T}}\bm{Q}\bm{1}_{n}= \bm{R}\bm{1}_{r}=\bm{b}\]

We first consider two approaches to optimizing for such a \(\bm{X}\), given that it is not a coupling. First, we consider the appropriate proximal step for \(\bm{X}\)

\[\min_{\bm{\zeta}}\langle\nabla\mathcal{L}(\bm{\zeta})\mid_{\xi_{k}},\bm{ \zeta}\rangle_{F}+\frac{1}{\gamma_{k}}\mathrm{KL}(\bm{\zeta}\|\bm{K}^{(k)})\]

We first note that the gradient of our loss with respect to \(\bm{X}\) is given as \(\bm{Q}^{\mathrm{T}}\bm{C}\bm{R}\). If one supposes that \(\bm{X}\) is invertible with \(\bm{X}^{-1}=\bm{T}\), we have that for any such \(\bm{T}\):

\[\bm{X}^{-1}\bm{1}_{r}=\bm{T}\bm{1}_{r}=\bm{g}_{R}\]

And

\[\bm{X}^{-T}\bm{1}_{r}=\bm{T}^{\mathrm{T}}\bm{1}_{r}=\bm{g}_{Q}\]

This implies that this inverse matrix \(\bm{T}\) is a coupling such that \(\bm{T}\in\Pi_{\bm{g}_{R},\bm{g}_{Q}}\), which also suggests one might be able to update it using Sinkhorn. In fact, being a density in \(\mathbb{R}_{+}^{r\times r}\) it represents a transition matrix between the latent \(r\)-dimensional variables. In particular, writing the proximal step in full, we have:

\[\min_{\bm{T}}\langle\bm{Q}^{\mathrm{T}}\bm{C}\bm{R},\bm{T}^{-1}\rangle_{F}+ \frac{1}{\gamma_{k}}\mathrm{KL}(\bm{T}_{k}\|\bm{K}_{\bm{T}}^{(k)})\]

Noting the derivative \(D(\bm{X}^{-1})\circ\bm{V}=-\bm{X}^{-1}\bm{V}\bm{X}^{-1}\), we have from the first-order condition that:

\[-\bm{T}^{-T}\bm{Q}^{\mathrm{T}}\bm{C}\bm{R}\bm{T}^{-T}+\frac{1}{\gamma_{k}} \log\left[\frac{\bm{T}_{k}}{\bm{K}_{\bm{T}}^{(k)}}\right]=\bm{0}\]

This implies the kernel matrix update:

\[\bm{K}_{\bm{T}}^{(k)}=\bm{T}_{k}\odot\exp\{+\gamma_{k}\bm{T}^{-T}\bm{Q}^{ \mathrm{T}}\bm{C}\bm{R}\bm{T}^{-T}\}\]

Where one then takes the Sinkhorn projection J onto the set \(\Pi_{\bm{g}_{R},\bm{g}_{Q}}\) as \(\mathcal{P}_{\Pi_{\bm{g}_{R},\bm{g}_{Q}}}(\bm{K}_{\bm{T}}^{(k)})\) using the Sinkhorn algorithm Cuturi (2013b). However, a more stable and inversion-free update exists which ensures \(\bm{X}\) remains positive by a diagonal re-scaling in the form introduced by Lin et al. (2021). In particular, if one takes \(\bm{X}=\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\,\mathrm{diag}(1/\bm{g}_{R})\), then

\[\bm{X}\bm{g}_{R}=\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\,\mathrm{diag}(1/\bm{g}_{R })\bm{g}_{R}=\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\bm{1}_{r}=\bm{1}_{r}\]

and likewise

\[\bm{X}^{\mathrm{T}}\bm{g}_{Q}=\mathrm{diag}(1/\bm{g}_{R})\bm{T}^{\mathrm{T}} \,\mathrm{diag}(1/\bm{g}_{Q})\bm{g}_{Q}=\mathrm{diag}(1/\bm{g}_{R})\bm{T}^{ \mathrm{T}}\bm{1}_{r}=\bm{1}_{r}\]

so that \(\bm{X}\) necessarily satisfies \(\bm{X}\bm{g}_{R}=\bm{1}_{r}\) and \(\bm{X}^{\mathrm{T}}\bm{g}_{Q}=\bm{1}_{r}\). Thus \(\bm{T}\bm{1}_{r}=\bm{g}_{Q}\) and \(\bm{T}^{\mathrm{T}}\bm{1}_{r}=\bm{g}_{R}\) and \(\bm{T}\in\Pi_{\bm{g}_{Q},\bm{g}_{R}}\). With analogous reasoning to before, one has a step for the coupling \(\bm{T}\) in the form:

\[\min_{\bm{T}}\langle\bm{Q}^{\mathrm{T}}\bm{C}\bm{R},\mathrm{diag}(1/\bm{g}_{Q} )\bm{T}\,\mathrm{diag}(1/\bm{g}_{R})\rangle_{F}+\frac{1}{\gamma_{k}}\mathrm{ KL}(\bm{T}_{k}\|\bm{K}_{\bm{T}}^{(k)})\]

Which yields the kernel matrix:

\[\bm{K}_{\bm{T}}^{(k)}=\bm{T}_{k}\odot\exp\{-\gamma_{k}\,\mathrm{diag}(\bm{g}_{ Q})^{-1}\bm{Q}^{\mathrm{T}}\bm{C}\bm{R}\,\mathrm{diag}(\bm{g}_{R})^{-1}\}\]

Which is likewise projected onto \(\Pi_{\bm{g}_{Q},\bm{g}_{R}}\) using the Sinkhorn algorithm. From this \(\bm{T}\in\Pi_{\bm{g}_{Q},\bm{g}_{R}}\), one takes \(\bm{X}=\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\,\mathrm{diag}(1/\bm{g}_{R})\) as the inner matrix that corresponds the unequal marginals \(\bm{g}_{Q}\) and \(\bm{g}_{R}\) which ensuring \(\bm{P}_{r}\in\Pi_{\bm{a},\bm{b}}\).

``` Input \(\bm{C},r,r_{2},\bm{a},\bm{b},\tau,\tau_{2},\gamma,\delta,\varepsilon\)  Initialize \(\bm{g}_{Q},\bm{g}_{R}=\frac{1}{r}\bm{r}_{r},\frac{1}{r_{2}}\bm{1}_{r_{2}}\) \(\bm{Q}_{0},\bm{R}_{0},\bm{T}_{0}\leftarrow\) Initialize-Couplings(\(\bm{a},\bm{b},\bm{g}_{Q},\bm{g}_{R}\)) if\(r=r_{2}\)then \(\bm{X}_{0}\leftarrow\bm{T}_{0}^{-1}\quad\#\) Invertible case else \(\bm{X}_{0}\leftarrow\mathrm{diag}(1/\bm{Q}_{0}^{\mathrm{T}}\bm{1}_{n})\bm{T}_{0 }\,\mathrm{diag}(1/\bm{R}_{0}^{\mathrm{T}}\bm{1}_{m})\quad\#\) General case endif while\(\Delta((\bm{Q}_{k},\bm{R}_{k},\bm{T}_{k}),(\bm{Q}_{k-1},\bm{R}_{k-1},\bm{T}_{k-1}))>\varepsilon\)do \(\nabla_{\bm{Q}}\leftarrow\bm{C}\bm{R}_{k}\bm{X}_{k}^{\mathrm{T}}-\bm{1}_{n} \mathrm{diag}^{-1}((\bm{C}\bm{R}_{k}\bm{X}_{k}^{\mathrm{T}})^{\mathrm{T}}\bm{Q }_{k}\mathrm{diag}(1/\bm{g}_{Q}))^{\mathrm{T}}\) \(\nabla_{\bm{R}}\leftarrow\bm{C}^{\mathrm{T}}\bm{Q}_{k}\bm{X}_{k}-\bm{1}_{m} \mathrm{diag}^{-1}(\mathrm{diag}(1/\bm{g}_{R})\bm{R}_{k}^{\mathrm{T}}\bm{C}^{ \mathrm{T}}\bm{Q}_{k}\bm{X}_{k})^{\mathrm{T}}\) \(\gamma_{k}\leftarrow\gamma/\max\{\|\nabla_{\bm{Q}}\|_{\infty},\|\nabla_{\bm{R} }\|_{\infty}\}\quad\#\)\(\ell^{\infty}\)-normalization of Section & Cuturi (2022) \(\bm{K}_{Q}^{(k)},\bm{K}_{R}^{(k)}\leftarrow\bm{Q}_{k}\odot\exp(-\gamma_{k} \nabla_{\bm{Q}}\,),\bm{R}_{k}\odot\exp(-\gamma_{k}\nabla_{\bm{R}}\,)\) if Balancedthen \(\bm{Q}_{k}\leftarrow\) SR\({}^{\mathrm{R}}\)-projection(\(\bm{K}_{Q}^{(k)},\gamma_{k},\tau,\bm{a},\bm{Q}_{k-1}^{\mathrm{T}}\bm{1}_{n},\delta\)) \(\#\) Semi-relaxed OT \(\bm{R}_{k}\leftarrow\) SR\({}^{\mathrm{R}}\)-projection(\(\bm{K}_{R}^{(k)},\gamma_{k},\tau,\bm{b},\bm{R}_{k-1}^{\mathrm{T}}\bm{1}_{m},\delta\)) \(\#\) Semi-relaxed OT elseif Unbalancedthen \(\bm{Q}_{k}\leftarrow\) U-projection(\(\bm{K}_{Q}^{(k)},\gamma_{k},\tau,\bm{a},\bm{Q}_{k-1}^{\mathrm{T}}\bm{1}_{n},\delta\)) \(\#\) Unbalanced OT elseif Semi-Relaxed Leftthen \(\bm{Q}_{k}\leftarrow\) U-projection(\(\bm{K}_{Q}^{(k)},\gamma_{k},\tau,\bm{a},\bm{Q}_{k-1}^{\mathrm{T}}\bm{1}_{n},\delta\)) \(\#\) Unbalanced OT \(\bm{R}_{k}\leftarrow\) SR\({}^{\mathrm{R}}\)-projection(\(\bm{K}_{R}^{(k)},\gamma_{k},\tau,\bm{b},\bm{R}_{k-1}^{\mathrm{T}}\bm{1}_{m},\delta\)) \(\#\) Semi-relaxed OT elseif Semi-Relaxed Rightthen \(\bm{Q}_{k}\leftarrow\) SR\({}^{\mathrm{R}}\)-projection(\(\bm{K}_{Q}^{(k)},\gamma_{k},\tau,\bm{a},\bm{Q}_{k-1}^{\mathrm{T}}\bm{1}_{n},\delta\)) \(\#\) Semi-relaxed OT \(\bm{R}_{k}\leftarrow\) U-projection(\(\bm{K}_{R}^{(k)},\gamma_{k},\tau,\bm{b},\bm{R}_{k-1}^{\mathrm{T}}\bm{1}_{m},\delta\)) \(\#\) Unbalanced OT endif \(\bm{g}_{Q},\bm{g}_{R}\leftarrow\bm{Q}_{k}^{\mathrm{T}}\bm{1}_{n},\bm{R}_{k}^{ \mathrm{T}}\bm{1}_{m}\) \(\nabla_{\bm{T}}=\mathrm{diag}(\bm{g}_{Q})^{-1}\bm{Q}_{k}^{\mathrm{T}}\bm{C} \bm{R}_{k}\,\mathrm{diag}(\bm{g}_{R})^{-1}\) \(\gamma_{\bm{T}}=\gamma/\|\nabla_{\|\infty}\quad\#\)\(\ell^{\infty}\)-normalization \(\bm{K}_{\bm{T}}^{(k)}\leftarrow\bm{T}_{k}\odot\exp\{-\gamma_{\bm{T}}\nabla_{\bm{T}}\}\) \(\bm{T}_{k}\leftarrow\) Sinkhorn(\(\bm{K}_{\bm{T}}^{(k)},\bm{g}_{R},\bm{g}_{Q},\delta\)) \(\#\) Balanced OT \(\bm{X}_{k}\leftarrow\mathrm{diag}(1/\bm{g}_{Q})\bm{T}\,\mathrm{diag}(1/\bm{g}_ {R})\) endwhile  Return \(\bm{P}_{r}=\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\) ```

**Algorithm 5** (Sinkhorn Algorithm _(Cuturi (2013b), balanced OT)_

## Appendix D Gromov-Wasserstein (GW)

As defined in, the general Gromov-Wasserstein problem concerns a minimization of the energy:

\[\mathcal{Q}_{\bm{A},\bm{B}}(\bm{P})=\sum_{i,j,k,l}(\bm{A}_{ik}-\bm{B}_{jl})^{2 }\bm{P}_{ij}\bm{P}_{kl}\] (30)Where the minimization is over the set of all couplings \(\bm{\Pi}_{\bm{a},\bm{b}}\):

\[\mathrm{GW}(\mu,\nu):=\min_{\bm{P}\in\Pi_{\bm{a},\bm{b}}}\mathcal{Q}_{\bm{A},\bm{ B}}(\bm{P})\] (31)

We consider extending the semi-relaxed framework to the GW problem under the low-rank restriction on \(\bm{P}\). This extension has thus far been considered for the balanced and unbalanced case in two previous works Scetbon et al. (2023, 2022). In both of these works, the algorithms for the Wasserstein case extend trivially to the Gromov-Wasserstein (GW) problem. In particular, each kernel with variable \(\bm{\zeta}\) has an update of the form \(\bm{K}\leftarrow\bm{\zeta}\odot\exp\left(-\gamma_{k}\nabla_{\bm{\zeta}}\bm{ \mathcal{L}}(\bm{\zeta})\right)\) for \(\mathcal{L}(\bm{\zeta})\) heretofore taken to be the Wasserstein loss \(\langle\bm{P}(\bm{\zeta}),\bm{C}\rangle_{F}\) where the coupling \(\bm{P}=\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\) is interpreted as a function of the low-rank sub-factor variables \(\bm{\zeta}\in\{\bm{Q},\bm{R},\bm{X}\}\). Taking \(\mathcal{L}:=\mathcal{Q}_{\bm{A},\bm{B}}(\bm{P}(\bm{\zeta}))\) one can simply extend the gradient through the GW-loss and directly use it in place of the Wasserstein gradient in the update \(\bm{K}\leftarrow\bm{\zeta}\odot\exp\left(-\gamma_{k}\nabla_{\bm{\zeta}}\bm{ \mathcal{L}}(\bm{\zeta})\right)\) for \(\mathcal{L}(\bm{\zeta})\) of each algorithm. The matrix-form of the GW-cost is expressed as:

\[\mathcal{Q}_{\bm{A},\bm{B}}(\bm{P})=\bm{1}_{m}^{\mathrm{T}}\bm{P}^{\mathrm{T} }\bm{A}^{\odot 2}\bm{P}\bm{1}_{m}+\bm{1}_{n}^{\mathrm{T}}\bm{P}\bm{B}^{\odot 2}\bm{P}^{ \mathrm{T}}\bm{1}_{n}-2\langle\bm{APB},\bm{P}\rangle\]

Which, using the constraints of \(\mathcal{C}_{2}\) reduces the cost as a function of \(\bm{Q}\), \(\bm{R}\), \(\bm{X}\) to:

\[\mathcal{Q}_{\bm{A},\bm{B}}(\bm{Q},\bm{R},\bm{X})=\bm{1}_{r}^{\mathrm{T}}\bm{Q }^{\mathrm{T}}\bm{A}^{\odot 2}\bm{Q}\bm{1}_{r}+\bm{1}_{r}^{\mathrm{T}}\bm{R}^{ \mathrm{T}}\bm{B}^{\odot 2}\bm{R}\bm{1}_{r}-2\langle\bm{Q}\bm{X}\bm{R}^{ \mathrm{T}},\bm{A}\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\bm{B}\rangle_{F}\]

\[\nabla_{\bm{Q}}\mathcal{Q}_{\bm{A},\bm{B}}(\bm{Q},\bm{R},\bm{X})=2\bm{A}^{ \odot 2}\bm{Q}\bm{1}_{r}\bm{1}_{r}^{\mathrm{T}}-4\bm{A}\bm{Q}\bm{X}\bm{R}^{ \mathrm{T}}\bm{B}\bm{R}\bm{X}^{\mathrm{T}}\]

Which is proportional to \(\nabla_{\bm{Q}}\mathcal{Q}_{\bm{A},\bm{B}}(\bm{Q},\bm{R},\bm{X})\propto-4\bm{ A}\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\bm{B}\bm{R}\bm{X}^{\mathrm{T}}\) for the balanced and right-marginal semi-relaxed case. And:

\[\nabla_{\bm{R}}\mathcal{Q}_{\bm{A},\bm{B}}(\bm{Q},\bm{R},\bm{X})=2\bm{B}^{ \odot 2}\bm{R}\bm{1}_{r}\bm{1}_{r}^{\mathrm{T}}-4\bm{B}\bm{R}\bm{X}^{\mathrm{T}}\bm {Q}^{\mathrm{T}}\bm{A}\bm{Q}\bm{X}\]

Which likewise can be reduced in proportionality to \(\nabla_{\bm{R}}\mathcal{Q}_{\bm{A},\bm{B}}(\bm{Q},\bm{R},\bm{g})\propto-4\bm{ B}\bm{R}\bm{X}^{\mathrm{T}}\bm{Q}^{\mathrm{T}}\bm{A}\bm{Q}\bm{X}\) in the balanced and left-marginal semi-relaxed case. The gradients, as presented above, assume a linearization in \(\bm{X}\leftarrow\bm{X}_{k}\). If one does not make this assumption and takes \(\bm{X}(\bm{Q},\bm{R})=\operatorname{diag}(1/\bm{Q}^{T}\bm{1}_{n})\bm{T} \operatorname{diag}(1/\bm{R}^{T}\bm{1}_{m})\), a rank-one perturbation must be added to the \(\bm{Q}\) and \(\bm{R}\) gradient of the form:

\[\nabla_{\bm{Q}}^{(2)}=4\bm{1}_{n}\operatorname{diag}^{-1}(\bm{X}\bm{R}^{ \mathrm{T}}\bm{B}(\bm{Q}\bm{X}\bm{R}^{\mathrm{T}})^{\mathrm{T}}\bm{A}\bm{Q} \operatorname{diag}(1/\bm{g}_{\bm{Q}}))^{\mathrm{T}}\]

\[\nabla_{\bm{R}}^{(2)}=4\bm{1}_{m}\operatorname{diag}^{-1}(\bm{X}^{T}\bm{Q}^{T }\bm{A}\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\bm{B}\bm{R}\operatorname{diag}(1/\bm{g }_{R}))^{\mathrm{T}}\]

Analogously, for the gradient on \(\bm{T}\) one can simply take the gradient with respect to \(\bm{X}\), and subsequently \(\bm{T}\) as \(\bm{X}(\bm{T})=\operatorname{diag}(\bm{g}_{Q})^{-1}\bm{T}\operatorname{diag}( \bm{g}_{R})^{-1}\). The gradient with respect to \(\bm{X}\) is given as:

\[\nabla_{\bm{X}}\mathcal{Q}_{\bm{A},\bm{B}}(\bm{Q},\bm{R},\bm{X})=-4\bm{Q}^{ \mathrm{T}}\bm{A}\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\bm{B}\bm{R}\]

And thus, the directional derivative with respect to \(\bm{X}\) in the direction \(\bm{V}_{\bm{X}}=\operatorname{diag}(\bm{g}_{Q})^{-1}\bm{V}_{\bm{T}} \operatorname{diag}(\bm{g}_{R})^{-1}\) and thus by the chain rule \(\bm{V}_{\bm{T}}\) is:

\[D\mathcal{Q}_{\bm{A},\bm{B}}(\bm{Q},\bm{R},\bm{X})\circ(\bm{V}_{ \bm{X}}) =-4\langle\bm{Q}^{\mathrm{T}}\bm{A}\bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\bm {B}\bm{R},\bm{V}_{\bm{X}}\rangle_{F}\] \[=-4\langle\bm{Q}^{\mathrm{T}}\bm{A}\bm{Q}\bm{X}\bm{R}^{\mathrm{T}} \bm{B}\bm{R},\operatorname{diag}(1/\bm{g}_{Q})\bm{V}_{\bm{T}}\operatorname{diag} (1/\bm{g}_{R})\rangle_{F}\]

So that the gradient with respect to the coupling matrix \(\bm{T}\) is given as:

\[\nabla_{\bm{T}}\mathcal{Q}_{\bm{A},\bm{B}}(\bm{Q},\bm{R},\bm{T}) =-4\operatorname{diag}(1/\bm{g}_{Q})\bm{Q}^{\mathrm{T}}\bm{A} \bm{Q}\bm{X}\bm{R}^{\mathrm{T}}\bm{B}\bm{R}\operatorname{diag}(1/\bm{g}_{R})\]

## Appendix E Convergence Analysis and Other Proofs

### Convergence and Smoothness of the Objective

We show in Proposition 3.3 that directly applying the block-descent lemma of Beck & Tetruashvili (2013) to the template of Ghadimi et al.'s proof Ghadimi et al. (2014) is sufficient to show the non-asymptotic stationary convergence of a coordinate mirror descent procedure in Ghadimi's criterion. The non-asymptotic guarantee of Ghadimi et al. (2014) follows directly in the case of coordinate mirror descent using Lemma E.1 and Lemma E.2 below. For completeness, we define all notation used, describe the coordinate mirror descent algorithm in general, and discuss a few relevant preliminaries.

Suppose that the vector of \(n\) variables \(\mathbf{x}\in\mathbb{R}^{n}\) is partitioned into \(p\) blocks, \(\mathbf{x}=(\mathbf{x}(1),\ldots,\mathbf{x}(p))\), where \(\mathbf{x}(i)\in\mathbb{R}^{n_{i}}\). Here, \(n_{1},\ldots,n_{p}\) are positive integers summing to \(n\). Following the notation of Beck & Tetruashvili (2013); Nesterov (2012), we define matrices \(\mathbf{U}_{i}\in\mathbb{R}^{n\times n_{i}}\) such that \(\mathbf{x}(i)=\mathbf{U}_{i}^{\mathrm{T}}\mathbf{x}\) for all \(i=1,\ldots,p\). This also implies \(\mathbf{x}=\sum_{i=1}^{p}\mathbf{U}_{i}\mathbf{x}(i)\). This allows us to define the vector of partial derivatives corresponding to each block of variables \(\mathbf{x}(i)\):

\[\nabla_{i}f(\mathbf{x}):=\mathbf{U}_{i}^{\mathrm{T}}\nabla f(\mathbf{x}).\]

In Beck & Tetruashvili (2013), the gradient of \(f\) is assumed to be block-coordinate-wise Lipschitz, with \(L_{i}\) the smoothness constant associated to the \(i\)-th block of variables: for all \(\mathbf{h}_{i}\in\mathbb{R}^{n_{i}}\), one has

\[\|\nabla_{i}f(\mathbf{x}+\mathbf{U}_{i}\mathbf{h}_{i})-\nabla_{i}f(\mathbf{x })\|\leq L_{i}\|\mathbf{h}_{i}\|,\] (32)

and for such functions we denote by \(L:=\max_{i}L_{i}\) the (global) smoothness constant of \(\nabla f\). To be clear, a _smoothness constant_ associated to \(f\) is a Lipschitz constant of its gradient.

**Lemma E.1** (Block descent lemma, Beck & Tetruashvili (2013), Lemma 3.2.).: _Suppose \(f\in C^{1}(\mathbb{R}^{n},\mathbb{R})\) is a continuously differentiable function over \(\mathbb{R}^{n}\) whose gradient is block-coordinatewise Lipschitz (32) for \(L_{i}\) the smoothness constant associated to the \(i\)-th block of variables \(\mathbf{x}(i)\). Let \(\mathbf{u},\mathbf{v}\) be two vectors differing only in the \(i\)-th block: there exists \(\mathbf{h}_{i}\in\mathbb{R}^{n_{i}}\) such that \(\mathbf{v}-\mathbf{u}=\mathbf{U}_{i}\mathbf{h}_{i}\). Then,_

\[f(\mathbf{v})\leq f(\mathbf{u})+\langle\nabla f(\mathbf{u}),\mathbf{v}- \mathbf{u}\rangle+\frac{L_{i}}{2}\|\mathbf{u}-\mathbf{v}\|^{2}.\] (33)

Lemma E.1 is central to adapting the proof of Theorem 1 of Ghadimi et al. (2014) to our case. Their Theorem 1 concerns the non-asymptotic convergence of mirror descent for objectives of the form

\[\min_{\mathbf{x}\in\mathcal{X}}f(\mathbf{x})+h(\mathbf{x}),\]

where \(\mathcal{X}\) is a closed, convex subset of \(\mathbb{R}^{n}\), \(f\in C^{1}(\mathcal{X},\mathbb{R})\) is a possibly non-convex objective, and where \(h\) is an \(\alpha\)-strongly convex function. Using notation similar to Ghadimi et al. (2014), we write \(\Phi(\mathbf{x})=f(\mathbf{x})+h(\mathbf{x})\). Additionally, Ghadimi et al. (2014) assume \(\nabla f\) is \(L\)-Lipschitz for some \(L>0\).

To unify our assumptions on \(f\), we suppose that \(\nabla f\in C^{1}(\mathcal{X},\mathbb{R})\) is block-coordinate Lipschitz (32) with block Lipschitz constants \((L_{i})_{i=1}^{p}\), and that \(\mathcal{X}\) itself decomposes as a product \(\mathcal{X}=\prod_{i=1}^{p}\mathcal{X}_{i}\), where each \(\mathcal{X}_{i}\) is a closed convex set constraining the block variables \(\mathbf{x}(i)\).

The proof of Ghadimi relies on \(\beta\)-smoothness of the objective in all variables. We show that component-wise smoothness in each block is sufficient to achieve an analogous convergence result for a coordinate mirror descent. To provide context for Proposition 3.3, we now describe in general (1) mirror descent, and (2) block-coordinate mirror descent.

We again follow the notation of Ghadimi et al. (2014). A function \(\omega:\mathcal{X}\to\mathbb{R}\) is a _distance generating function_ with modulus \(\alpha>0\), with respect to the Euclidean norm \(\|\cdot\|\), if \(\omega\) is continuously differentiable and strongly convex, so that

\[\langle\mathbf{x}-\mathbf{z},\nabla\omega(\mathbf{x})-\nabla\omega(\mathbf{z} )\rangle\geq\alpha\|\mathbf{x}-\mathbf{z}\|^{2},\quad\text{ for all }\mathbf{x},\mathbf{z}\in\mathcal{X}.\]

The _prox-function_ (or Bregman divergence) associated with \(\omega\) is then

\[V(\mathbf{x},\mathbf{z})=\omega(\mathbf{x})-\omega(\mathbf{z})-\langle\nabla \omega(\mathbf{z}),\mathbf{x}-\mathbf{z}\rangle,\]

and from this prox-function, \(\gamma>0\), and some \(\mathbf{g}\in\mathbb{R}^{n}\), we define the _generalized projection_

\[\mathbf{x}^{+}:=\operatorname*{arg\,min}_{\mathbf{u}\in\mathcal{X}}\left( \langle\mathbf{g},\mathbf{u}\rangle+\frac{1}{\gamma}V(\mathbf{u},\mathbf{x})+h( \mathbf{u})\right).\] (34)

To describe the projected gradient descent algorithm (which coincides with mirror descent in our case of interest), we first define the _generalized projected gradient_ of \(\Phi\) at \(\mathbf{x}\):

\[P_{\mathcal{X}}(\mathbf{x},\mathbf{g},\gamma):=\frac{1}{\gamma}(\mathbf{x}- \mathbf{x}^{+}).\]

The _mirror descent (MD) algorithm_ is as follows: given initial point \(\mathbf{x}_{0}\in\mathcal{X}\), a total number of iterations \(N\), and positive stepsizes \((\gamma_{k})_{k=1}^{N}\), at step \(k\), the \((k+1)\)-st iterate is computed via

\[\mathbf{x}_{k+1}\leftarrow\operatorname*{arg\,min}_{\mathbf{u}\in\mathcal{X}} \left(\langle\nabla f(\mathbf{x}_{k}),\mathbf{u}\rangle+\frac{1}{\gamma_{k}}V( \mathbf{u},\mathbf{x}_{k})+h(\mathbf{u})\right).\]Among all iterates \(\mathbf{x}_{k}\), the MD algorithm outputs the one at which the generalized projected gradient is of least norm. Concretely, \(\mathbf{x}_{R}\) is the output of the MD algorithm, where

\[R:=\operatorname*{arg\,min}_{k=0,\ldots,N}\|\mathbf{g}_{\mathcal{X},k}\|^{2},\]

and where \(\mathbf{g}_{\mathcal{X},k}\) is

\[\mathbf{g}_{\mathcal{X},k}:=P_{\mathcal{X}}(\mathbf{x}_{k},\nabla f(\mathbf{ x}_{k}),\gamma_{k}).\]

Having described the MD algorithm, let us consider a block-coordinate variant; we assume \(\mathbf{x}\) admits the block-coordinate structure described above. To simplify the presentation, we suppose that in a given iteration \(k\), the block variables are updated sequentially from \(i=1,\ldots,p\). This leads to doubly-indexed iterates \((\mathbf{x}_{k}^{i})\) with \(k=1,\ldots,N\) indexing each full iteration through all variables, and \(i=0,1,\ldots,p\) indexing the sub-iterations which update one block of variables at a time.

The _coordinate mirror descent (CMD) algorithm_ takes as input an initial point \(\mathbf{x}_{0}\in\mathcal{X}\), a number of iterations \(N\), and a sequence of positive stepsizes \((\gamma_{k,i})_{k=1,i=1}^{N,p}\). We set \(\mathbf{x}_{0}^{0}=\mathbf{x}_{0}\), and for \(k=0,\ldots,N-1\) and \(i=1,\ldots,p\), we compute \(\mathbf{x}_{k}^{i}\) from \(\mathbf{x}_{k}^{i-1}\) as follows:

\[\mathbf{x}_{k}^{i}(i) \leftarrow\operatorname*{arg\,min}_{\mathbf{u}_{i}\in\mathcal{X }_{i}}\left(\langle\nabla_{i}f(\mathbf{x}_{k}^{i-1}),\mathbf{u}_{i}\rangle+ \frac{1}{\gamma_{k,i}}V_{i}(\mathbf{u}_{i},\mathbf{U}_{i}^{\mathrm{T}} \mathbf{x}_{k}^{i-1})+h_{i}(\mathbf{u}_{i})\right)\] \[\mathbf{x}_{k}^{i}(j) \leftarrow\mathbf{x}_{k}^{i-1}(j)\quad\text{ for }j\neq i\]

Here, we have assumed that \(V\) can be written as a composite function of the block variables,

\[V(\mathbf{x},\mathbf{z})=\sum_{i=1}^{p}V_{i}(\mathbf{x}(i),\mathbf{z}(i)),\]

as is the case for the KL divergence. We also have assumed that \(h\) has this composite structure (as with entropy):

\[h(\mathbf{x})=\sum_{i=1}^{p}h_{i}(\mathbf{x}(i)).\]

Lastly, for \(k=0,\ldots,N-1\), we set \(\mathbf{x}_{k+1}^{0}:=\mathbf{x}_{k}^{p}\). We define \(\mathbf{g}_{\mathcal{X},k}:=(\mathbf{g}_{\mathcal{X},k,1},\ldots,\mathbf{g}_{ \mathcal{X},k,p})\) to be the collection of block-wise differences, where by definition

\[\mathbf{g}_{\mathcal{X},k,i}=P_{\mathcal{X}_{i}}(\mathbf{x}_{k}^{i-1},\nabla_ {i}f(\mathbf{x}_{k}^{i-1}),\gamma_{k,i})=\frac{1}{\gamma_{k,i}}\left(\mathbf{ x}_{k}^{i-1}-\mathbf{x}_{k}^{i}\right)=\mathbf{U}_{i}^{\mathrm{T}}\mathbf{g}_{ \mathcal{X},k}.\]

The convergence criterion \(\Delta\) we use in Proposition 3.3 is the one used by Ghadimi et al. (2014) summed across blocks. In particular, the CMD algorithm returns iterate \(\mathbf{x}_{R}\), where

\[\begin{split} R:=\operatorname*{arg\,min}_{k=0,\ldots,N}\Delta( \mathbf{x}_{k},\mathbf{x}_{k-1}),\\ \Delta(\mathbf{x}_{k},\mathbf{x}_{k-1}):=\|\mathbf{g}_{\mathcal{X },k}\|^{2}=\sum_{i=1}^{p}\|\mathbf{g}_{\mathcal{X},k,i}\|^{2}.\end{split}\] (35)

For \(\Phi=f+h\) as above (\(f\) has global smoothness constant \(L\)), let \(\Phi^{*}=\operatorname*{arg\,min}_{\mathbf{x}\in\mathcal{X}}\Phi(\mathbf{x})\), and define

\[D:=\left(\frac{\Phi(x_{0})-\Phi^{*}}{L}\right)^{1/2}.\] (36)

The other lemma used in the proof of Proposition 3.3 is as follows.

**Lemma E.2** (Ghadimi et al. (2014), Lemma 1).: _Let \(\mathbf{x}^{+}=\operatorname*{arg\,min}_{\mathbf{u}\in\mathcal{X}}\{\langle \mathbf{g},\mathbf{u}\rangle+\frac{1}{\gamma}V(\mathbf{u},\mathbf{x})+h( \mathbf{u})\}\) and \(P_{\mathcal{X}}(\mathbf{x},\mathbf{g},\gamma)=\frac{1}{\gamma}(\mathbf{x}- \mathbf{x}^{+})\). Then for all \(\mathbf{x}\in\mathbb{R}^{n}\), all \(\mathbf{g}\in\mathbb{R}^{n}\), and \(\gamma>0\), one has:_

\[\langle\mathbf{g},P_{\mathcal{X}}(\mathbf{x},\mathbf{g},\gamma)\rangle\geq \alpha\|P_{\mathcal{X}}(\mathbf{x},\mathbf{g},\gamma)\|^{2}+\frac{1}{\gamma}(h( \mathbf{x}^{+})-h(\mathbf{x})).\]

**Proposition E.3** (Proposition 3.3).: _Suppose one has \(f\in C^{1}(\mathcal{X},\mathbb{R})\) whose gradient is block-coordinate Lipschitz, with block smoothness constants \((L_{i})_{i=1}^{p}\), and a function \(h\in C(\mathcal{X},\mathbb{R})\) which is \(\alpha\)-strongly convex. For \(\Phi=f+h\), suppose one performs a coordinate mirror descent on \(\Phi\) minimized over a product of closed convex sets \(\mathcal{X}=\prod_{i=1}^{p}\mathcal{X}_{i}\). Let the sub-iterates with respect to the \(i\)-th block update be \(\{\mathbf{x}_{k}^{i}\}_{i=0}^{p}\) where \(\mathbf{x}_{k}:=\mathbf{x}_{k}^{0}\) for \(k\in[N]\) outer iterations. Then one has:_

\[\min_{k}\Delta(\mathbf{x}_{k},\mathbf{x}_{k-1})\leq\frac{D^{2}L}{N(\alpha^{2} /2L)}=\frac{2D^{2}L^{2}}{N\alpha^{2}},\]

_where \(D\) is (36), \(L\) is the global smoothness constant of \(f\), and convergence criterion \(\Delta(\mathbf{x}_{k},\mathbf{x}_{k-1})\) is given in (35). Above, the stepsizes \(\gamma_{k,i}\) in the coordinate mirror descent are \(\gamma_{k,i}:=\alpha/L\)._

Proof.: As \(f\) satisfies the hypotheses of the block descent lemma, Lemma E.1, we apply (33) to obtain:

\[f(\mathbf{x}_{k}^{i})\leq f(\mathbf{x}_{k}^{i-1})+\langle\nabla_{i}f(\mathbf{ x}_{k}^{i-1}),\mathbf{x}_{k}^{i}-\mathbf{x}_{k}^{i-1}\rangle+\frac{L_{i}}{2}\| \mathbf{x}_{k}^{i}-\mathbf{x}_{k}^{i-1}\|^{2}.\]

Noting the definition \(\mathbf{g}_{\mathcal{X},k,i}=\frac{1}{\gamma_{k,i}}\left(\mathbf{x}_{k}^{i-1}- \mathbf{x}_{k}^{i}\right)\), one has

\[f(\mathbf{x}_{k}^{i})\leq f(\mathbf{x}_{k}^{i-1})-\gamma_{k,i}\langle\nabla_{ i}f(\mathbf{x}_{k}^{i-1}),\mathbf{g}_{\mathcal{X},k,i}\rangle+\frac{L_{i}}{2} \gamma_{k,i}^{2}\|\mathbf{g}_{\mathcal{X},k,i}\|^{2}.\]

Lemma 1 of Ghadimi et al. (2014) (stated as Lemma E.2 above) applies identically through block-wise optimality on \(\mathcal{X}_{i}\) because \(\mathbf{g}_{\mathcal{X},k,i}=P_{\mathcal{X}_{i}}(\mathbf{x}_{k}^{i-1},\nabla_ {i}f(\mathbf{x}_{k}^{i-1}),\gamma_{k,i})\). Thus for any value \(\nabla_{i}f(\mathbf{x}_{k}^{i-1})\) takes,

\[f(\mathbf{x}_{k}^{i})\leq f(\mathbf{x}_{k}^{i-1})-\left[\alpha\gamma_{k,i}\| \mathbf{g}_{\mathcal{X}^{\prime},k,i}\|^{2}+h(\mathbf{x}_{k}^{i})-h(\mathbf{x }_{k}^{i-1})\right]+\frac{L_{i}}{2}\gamma_{k,i}^{2}\|\mathbf{g}_{\mathcal{X},k, i}\|^{2},\]

and thus,

\[f(\mathbf{x}_{k}^{i})+h(\mathbf{x}_{k}^{i})\leq f(\mathbf{x}_{k}^{i-1})+h( \mathbf{x}_{k}^{i-1})-\left[\alpha\gamma_{k,i}-\frac{L_{i}}{2}\gamma_{k,i}^{2} \right]\|g_{\mathcal{X},k,i}\|^{2}.\]

The right-hand side above only becomes larger, taking \(L_{i}=L\) to be the global smoothness constant. Introducing a sum over sub-iterates and total iterates, one has

\[\sum_{k,i}^{N,p}f(\mathbf{x}_{k}^{i})+h(\mathbf{x}_{k}^{i}) \leq\sum_{k,i}^{N,p}f(\mathbf{x}_{k}^{i-1})+h(\mathbf{x}_{k}^{i-1 })-\sum_{k,i}^{N,p}\left[\alpha\gamma_{k,i}-\frac{L}{2}\gamma_{k,i}^{2} \right]\|\mathbf{g}_{\mathcal{X}^{\prime},k,i}\|^{2},\] \[\sum_{k,i}^{N,p}\Phi(\mathbf{x}_{k}^{i}) \leq\sum_{k,i}^{N,p}\Phi(\mathbf{x}_{k}^{i-1})-\sum_{k,i}^{N,p} \left[\alpha\gamma_{k,i}-\frac{L}{2}\gamma_{k,i}^{2}\right]\|\mathbf{g}_{ \mathcal{X}^{\prime},k,i}\|^{2}.\]

Noting the end-point condition \(\Phi(\mathbf{x}_{k}^{p})=\Phi(\mathbf{x}_{k+1}^{0})\), one may cancel all intermediate terms:

\[\Phi^{*}\leq\Phi(\mathbf{x}_{N})\leq\Phi(\mathbf{x}_{0})-\sum_{k,i}^{N,p} \left[\alpha\gamma_{k,i}-\frac{L}{2}\gamma_{k,i}^{2}\right]\|\mathbf{g}_{ \mathcal{X},k,i}\|^{2}\]

Thus one finds the upper bound in terms of \(\Phi(\mathbf{x}_{0})\) and the minimum value \(\Phi^{*}=\min_{\mathbf{x}\in\mathcal{X}}\Phi(\mathbf{x})\) of \(\Phi\):

\[\sum_{k}^{N}\sum_{i}^{p}\left[\alpha\gamma_{k,i}-\frac{L}{2}\gamma_{k,i}^{2} \right]\|\mathbf{g}_{\mathcal{X}^{\prime},k,i}\|^{2}\leq\Phi(\mathbf{x}_{0})- \Phi^{*}.\] (37)

Taking \(\gamma_{k,i}=\alpha/L\) as in Ghadimi et al. (2014), the bracketed term directly above becomes \(\alpha^{2}/2L\), and one has:

\[\sum_{k}^{N}\left[\frac{\alpha^{2}}{2L}\right]\left(\min_{k}\Delta (\mathbf{x}_{k},\mathbf{x}_{k-1})\right) =\sum_{k}^{N}\left[\frac{\alpha^{2}}{2L}\right]\left(\min_{k}\sum _{i}^{p}\|\mathbf{g}_{\mathcal{X}^{\prime},k,i}\|^{2}\right)\] \[\leq\sum_{k}^{N}\left[\frac{\alpha^{2}}{2L}\right]\sum_{i}^{p}\| \mathbf{g}_{\mathcal{X}^{\prime},k,i}\|^{2}\] \[\leq\Phi(\mathbf{x}_{0})-\Phi^{*}=D^{2}L,\]where \(D\) is as in (36), and where we used (37) to obtain the last line. Thus,

\[\min_{k}\Delta(\mathbf{x}_{k},\mathbf{x}_{k-1})\leq\frac{D^{2}L}{N(\alpha^{2}/2L )}=\frac{2D^{2}L^{2}}{N\alpha^{2}},\]

completing the proof. 

**Definition E.4** (Relative smoothness).: Let \(\beta>0\) and let \(g\in\mathcal{C}^{1}(\mathbb{R}^{n},\mathbb{R})\) be continuously differentiable. Additionally, let \(\omega\) be a distance generating function with associated prox-function \(V\). The function \(g\) is \(\beta\)-_smooth relative to \(\omega\)_ if the following holds:

\[g(\bm{y})\leq g(\bm{x})+\langle\nabla\omega(\bm{x}),\bm{x}-\bm{y}\rangle+ \beta V(\bm{y},\bm{x})\]

**Proposition E.5**.: _Let \(\epsilon>0\) be a predefined error tolerance, \(r>0\) a small rank parameter, \(\delta\in(0,\frac{1}{r})\) a lower-bound parameter, and \(N\) the number of inner iterations required for the semi-relaxed projection to converge for each iteration \(k\) as \(\|\bm{Q}^{\mathrm{T}}\mathbf{1}_{n}-\bm{g}_{\bm{Q}}^{(k-1)}\|_{2}<\epsilon= \frac{1}{N}\left(\frac{1}{r}-\delta\right)\) for \(\bm{g}_{\bm{Q}}^{(0)}=\frac{1}{r}\mathbf{1}_{r}\). Then, the FRLC objective_

\[\mathcal{L}_{\mathrm{LC}}(\bm{Q},\bm{R},\bm{T})=\langle\bm{Q}\operatorname{ diag}(1/\bm{Q}^{\mathrm{T}}\mathbf{1}_{n})\bm{T}\operatorname{diag}(1/\bm{R}^{ \mathrm{T}}\mathbf{1}_{m})\bm{R}^{\mathrm{T}},\bm{C}\rangle_{F}\]

_is component-wise smooth with respect to the variables \(\bm{Q},\bm{R},\bm{T}\) with smoothness constants \(\{\beta_{i}\}_{i=1}^{3}\) where \(\beta_{i}=\operatorname{poly}(\|\bm{C}\|_{F},n,m,r,\delta)\)._

Proof.: The addition of a pair of regularizations on the inner marginals \(\tau\mathrm{KL}(\bm{Q}^{\mathrm{T}}\mathbf{1}_{n}\|\bm{Q}_{k}^{\mathrm{T}} \mathbf{1}_{n})\) and \(\tau\mathrm{KL}(\bm{R}^{\mathrm{T}}\mathbf{1}_{m}\|\bm{R}_{k}^{\mathrm{T}} \mathbf{1}_{m})\) in 21 and 22 ensures that we may use standard results on relaxed optimal-transport which bound how far the marginal \(\bm{Q}_{k}^{\mathrm{T}}\mathbf{1}_{n}\) deviates across iterations.

In particular, for all \(\epsilon>0\) there exists \(\tau\) and \(N\) (number of iterations) sufficiently large, so that \(\|\bm{R}^{\mathrm{T}}\mathbf{1}_{m}-\bm{g}_{\bm{R}}\|_{2}^{2}<\epsilon\) and \(\|\bm{Q}^{\mathrm{T}}\mathbf{1}_{n}-\bm{g}_{\bm{Q}}\|_{2}^{2}<\epsilon\). In particular, Pham et al. (2020) shows that one can attain convergence to any \(\epsilon\) for the unbalanced problem in \(N=\tilde{O}(m^{2}/\epsilon)\) iterations (hiding logarithmic and \(\tau\)-factors) for each sub-problem solving for \(\bm{R}_{k}\) in Algorithm 4 and analogously \(N=\tilde{O}(n^{2}/\epsilon)\) for \(\bm{Q}\). Under the uniform initialization of \(\bm{g}_{\bm{R}}\), and after \(N\) iterations, we have:

\[\|\bm{g}_{\bm{R}}^{(0)}-\bm{g}_{\bm{R}}^{(N)}\|_{2}=\left\|\frac{1}{r} \mathbf{1}_{r}-\bm{g}_{\bm{R}}^{(N)}\right\|_{2}\leq\sum_{k=1}^{N}\|\bm{g}_{ \bm{R}}^{(k)}-\bm{g}_{\bm{R}}^{(k-1)}\|_{2}.\]

With sufficiently large \(\tau\) and \(N=\tilde{O}(m^{2}/\epsilon)\) sub-iterations, one can guarantee that:

\[\|\bm{g}_{\bm{R}}^{(k)}-\bm{g}_{\bm{R}}^{(k-1)}\|_{2}<\epsilon=\frac{1}{N} \left(\frac{1}{r}-\delta\right).\]

This implies, for all iterations of the algorithm and all indices \(i\), that

\[(\bm{g}_{R_{k}})_{i}>\delta,\quad\text{ and analogously},\quad(\bm{g}_{Q_{k}})_{i }>\delta.\] (38)

Thus, by adding the regularization on the inner marginal, one may guarantee a lower-bound on the entries of \(\bm{g}_{R}\) and \(\bm{g}_{Q}\). This is essential for demonstrating smoothness of the objective.

First, we consider smoothness in \(\bm{Q}\). We note that the gradient in \(\bm{Q}\) splits into two terms:

\[\nabla_{\bm{Q}}\mathcal{L}_{\mathrm{LC}}(\bm{Q},\bm{R},\bm{T}) =\nabla_{\bm{Q}}^{(A)}\mathcal{L}_{\mathrm{LC}}+\nabla_{\bm{Q}} ^{(B)}\mathcal{L}_{\mathrm{LC}}\] (39) \[=\bm{C}\bm{R}\bm{X}^{\mathrm{T}}-\mathbf{1}_{n}\mathrm{diag}^{-1 }((\bm{C}\bm{R}\bm{X}^{\mathrm{T}})^{\mathrm{T}}\bm{Q}\mathrm{diag}(1/\bm{g}_{ Q}))^{\mathrm{T}}\] \[=\nabla_{\bm{Q}}^{(A)}\mathcal{L}_{\mathrm{LC}}-\mathbf{1}_{n} \mathrm{diag}^{-1}((\nabla_{\bm{Q}}^{(A)}\mathcal{L}_{\mathrm{LC}})^{\mathrm{T}} \bm{Q}\mathrm{diag}(1/\bm{g}_{Q}))^{\mathrm{T}}\]

Where

\[\begin{split}\|\nabla_{\bm{Q}}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k+1},\bm{R}_{k},\bm{T}_{k})-\nabla_{\bm{Q}}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k},\bm{R }_{k},\bm{T}_{k})\|_{F}\\ \leq\|\nabla_{\bm{Q}}^{(A)}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k+1},\bm{R}_{k},\bm{T}_{k})-\nabla_{\bm{Q}}^{(A)}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k}, \bm{R}_{k},\bm{T}_{k})\|_{F}\\ \qquad\qquad+\|\nabla_{\bm{Q}}^{(B)}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_ {k+1},\bm{R}_{k},\bm{T}_{k})-\nabla_{\bm{Q}}^{(B)}\mathcal{L}_{\mathrm{LC}}( \bm{Q}_{k},\bm{R}_{k},\bm{T}_{k})\|_{F}.\end{split}\]Starting with the first term on the right side of (39), one has:

\[\|\nabla^{(A)}_{\bm{Q}}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k+1},\bm{R} _{k},\bm{T}_{k})-\nabla^{(A)}_{\bm{Q}}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k},\bm{R }_{k},\bm{T}_{k})\|_{F}\] \[\quad=\|\bm{C}\bm{R}_{k}(\mathrm{diag}(1/\bm{g}_{Q_{k}})\bm{T}_{k} \mathrm{diag}(1/\bm{g}_{R_{k}}))^{\mathrm{T}}-\bm{C}\bm{R}_{k}(\mathrm{diag}( 1/\bm{g}_{Q_{k-1}})\bm{T}_{k}\mathrm{diag}(1/\bm{g}_{R_{k}}))^{\mathrm{T}}\|_{F}\] \[\quad\leq\|\mathrm{diag}(1/\bm{g}_{R_{k}})\|_{F}\|\bm{C}\|_{F}\| \bm{R}_{k}\|_{F}\|\bm{T}_{k}\|\mathrm{diag}(1/\bm{g}_{Q_{k}})-\mathrm{diag}(1/ \bm{g}_{Q_{k-1}})\|_{F}.\]

Note that \(\|\bm{R}_{k}\|_{F}^{2}=\sum_{i,j}\left(\bm{R}_{k}\right)_{i,j}^{2}<\sum_{i,j} \left(\bm{R}_{k}\right)_{i,j}=1\), as \(\bm{R}_{k}\) has marginals which sum to one. The same bound holds for \(\|\bm{T}_{k}\|_{F}^{2}\), which is also a coupling. Invoking the lower-bound (38) of \(\delta\) on the entries of the inner marginals, and continuing from the above display,

\[\|\nabla^{(A)}_{\bm{Q}}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k+1},\bm {R}_{k},\bm{T}_{k})-\nabla^{(A)}_{\bm{Q}}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k}, \bm{R}_{k},\bm{T}_{k})\|_{F}\] \[\quad\leq\frac{\|\bm{C}\|_{F}}{\delta}\|\mathrm{diag}(1/\bm{g}_{Q _{k}})-\mathrm{diag}(1/\bm{g}_{Q_{k-1}})\|_{F}\] \[\quad=\frac{\|\bm{C}\|_{F}}{\delta}\|\mathrm{diag}(1/\bm{g}_{Q_{k -1}})\mathrm{diag}(1/\bm{g}_{Q_{k}})(\mathrm{diag}(\bm{g}_{Q_{k-1}})-\mathrm{ diag}(\bm{g}_{Q_{k}}))\|_{F}\] \[\quad\leq\frac{\|\bm{C}\|_{F}}{\delta^{3}}\|\mathrm{diag}(\bm{g}_ {Q_{k-1}})-\mathrm{diag}(\bm{g}_{Q_{k}})\|_{F}.\]

To further bound the right-hand side above, consider:

\[\|\mathrm{diag}(\bm{g}_{\bm{Q}_{k}})-\mathrm{diag}(\bm{g}_{\bm{Q}_{k-1}})\|_{ F}^{2}=\|\bm{Q}_{k}^{\mathrm{T}}\bm{1}_{n}-\bm{Q}_{k-1}^{\mathrm{T}}\bm{1}_{n}\|_{ 2}^{2}=\sum_{i=1}^{r}\left(\sum_{j=1}^{r}\left(\bm{Q}_{k}\right)_{i,j}-\left( \bm{Q}_{k-1}\right)_{i,j}\right)^{2}\]

While can easily be upper-bounded by an application of Jensen's inequality as

\[=\sum_{i=1}^{r}r^{2}\left(\sum_{j=1}^{r}\frac{1}{r}\left(\left( \bm{Q}_{k}\right)_{i,j}-\left(\bm{Q}_{k-1}\right)_{i,j}\right)\right)^{2}\] \[\leq\sum_{i=1}^{r}r^{2}\left(\sum_{j=1}^{r}\frac{1}{r}\left( \left(\bm{Q}_{k}\right)_{i,j}-\left(\bm{Q}_{k-1}\right)_{i,j}\right)^{2}\right)\] \[=r\sum_{i=1}^{r}\sum_{j=1}^{r}\left(\left(\bm{Q}_{k}\right)_{i,j} -\left(\bm{Q}_{k-1}\right)_{i,j}\right)^{2}\] \[=r\|\bm{Q}_{k}-\bm{Q}_{k-1}\|_{F}^{2}.\]

Likewise, we have that \(\|\mathrm{diag}(\bm{g}_{\bm{R}_{k}})-\mathrm{diag}(\bm{g}_{\bm{R}_{k-1}})\|_{F} ^{2}\leq r\|\bm{R}_{k}-\bm{R}_{k-1}\|_{F}^{2}\). Thus, it holds that

\[\frac{\|\bm{C}\|_{F}}{\delta^{3}}\|\bm{g}_{\bm{Q}_{k}}-\bm{g}_{\bm{Q}_{k-1}}\|_ {2}\leq\frac{\|\bm{C}\|_{F}\sqrt{r}}{\delta^{3}}\|\bm{Q}_{k}-\bm{Q}_{k-1}\|_{F}.\]

Next, we focus on the \(\nabla^{(B)}_{\bm{Q}}\) term. Observe that

\[\|\bm{1}_{n}\mathrm{diag}^{-1}\bm{X}\|_{F}^{2} =\mathrm{Tr}(\bm{1}_{n}\mathrm{diag}^{-1}\bm{X})^{\mathrm{T}}( \bm{1}_{n}\mathrm{diag}^{-1}\bm{X})\] \[=n\|\mathrm{diag}^{-1}\bm{X}\|_{2}^{2}\leq n\|\bm{X}\|_{F}^{2}.\]

Thus:

\[\|\nabla^{(B)}_{\bm{Q}_{k+1}}-\nabla^{(B)}_{\bm{Q}_{k}}\|_{F}\leq\sqrt{n}\|( \nabla^{(A)}_{\bm{Q}_{k+1}})^{\mathrm{T}}\bm{Q}_{k+1}\mathrm{diag}(1/\bm{g}_{ \bm{Q}_{k+1}})-(\nabla^{(A)}_{\bm{Q}_{k}})^{\mathrm{T}}\bm{Q}_{k}\mathrm{diag}(1/ \bm{g}_{\bm{Q}_{k}})\|_{F}\]

Adding and subtracting terms in the norm and applying triangle inequality:

\[\sqrt{n}\|(\nabla^{(A)}_{\bm{Q}_{k+1}})^{\mathrm{T}}\bm{Q}_{k+1} \mathrm{diag}(1/\bm{g}_{\bm{Q}_{k+1}})-(\nabla^{(A)}_{\bm{Q}_{k}})^{\mathrm{T}} \bm{Q}_{k+1}\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k+1}})\] \[\quad+(\nabla^{(A)}_{\bm{Q}_{k}})^{\mathrm{T}}\bm{Q}_{k+1} \mathrm{diag}(1/\bm{g}_{\bm{Q}_{k+1}})-(\nabla^{(A)}_{\bm{Q}_{k}})^{\mathrm{T}} \bm{Q}_{k}\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k}})\|_{F}\] \[\qquad\leq\sqrt{n}\|\nabla^{(A)}_{\bm{Q}_{k+1}}-\nabla^{(A)}_{\bm {Q}_{k}}\|_{F}\|\bm{Q}_{k+1}\|_{F}\|\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k+1}})\|_{F}\] \[\qquad\qquad+\sqrt{n}\|\nabla^{(A)}_{\bm{Q}_{k}}\|_{F}\|(\bm{Q}_{k+ 1}\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k+1}})-\bm{Q}_{k}\mathrm{diag}(1/\bm{g}_{\bm{Q} _{k}})\|_{F}\]Invoking the lower-bound on the marginal, continuing from the above display,

\[\leq\frac{\sqrt{n}}{\delta}\|\nabla^{(A)}_{\bm{Q}_{k+1}}-\nabla^{(A) }_{\bm{Q}_{k}}\|_{F}\] \[+\sqrt{n}\|\nabla^{(A)}_{\bm{Q}_{k}}\|_{F}\|(\bm{Q}_{k+1}\mathrm{ diag}(1/\bm{g}_{\bm{Q}_{k+1}})-\bm{Q}_{k}\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k}})\|_{F}\]

Let us consider \(\|\nabla^{(A)}_{\bm{Q}_{k}}\|_{F}\leq\|\bm{X}_{k}\|_{F}\|\bm{C}\|_{F}\|\bm{R}_{ k}\|_{F}\leq\|\bm{X}_{k}\|_{F}\|\bm{C}\|_{F}\). We have that:

\[\|\bm{X}_{k}\|_{F}^{2}=\sum_{i,j}\frac{1}{\bm{g}_{(\bm{Q}_{k})_{i}}^{2}}\bm{T} _{ij}^{2}\frac{1}{\bm{g}_{(\bm{R}_{k})_{j}}^{2}}\]

Where we always have that \(\bm{T}_{ij}\leq\bm{g}_{\bm{Q}i}\) and \(\bm{T}_{ij}\leq\bm{g}_{\bm{R}j}\) by definition of \(\bm{T}\) as a coupling. As such:

\[\leq\sum_{ij}\frac{1}{\bm{g}_{\bm{Q}i}^{2}}\left(\bm{g}_{\bm{Q}i}\bm{g}_{\bm{ R}j}\right)\frac{1}{\bm{g}_{\bm{R}j}^{2}}=\sum_{ij}\frac{1}{\bm{g}_{\bm{Q}i} }\frac{1}{\bm{g}_{\bm{R}j}}=\left\langle\bm{g}_{\bm{Q}}^{-1}\bm{g}_{\bm{R}}^{ -T},\bm{1}_{m}\bm{1}_{r}^{\mathrm{T}}\right\rangle_{F}\leq\frac{mr}{\delta^{2}}\]

Thus \(\|\nabla^{(A)}_{\bm{Q}_{k}}\|_{F}\leq\frac{\sqrt{mr}}{\delta}\|\bm{C}\|_{F}\), and the bound above reduces to

\[\leq\frac{\sqrt{n}}{\delta}\|\nabla^{(A)}_{\bm{Q}_{k+1}}-\nabla^{ (A)}_{\bm{Q}_{k}}\|_{F}\] \[+\frac{\sqrt{nmr}}{\delta}\|\bm{C}\|_{F}\|(\bm{Q}_{k+1}\mathrm{ diag}(1/\bm{g}_{\bm{Q}_{k+1}})-\bm{Q}_{k}\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k}})\|_{F}\]

Further bounding the last term in the norm

\[\frac{\sqrt{nmr}}{\delta}\|\bm{C}\|_{F}\|(\bm{Q}_{k+1}\mathrm{ diag}(1/\bm{g}_{\bm{Q}_{k+1}})-\bm{Q}_{k}\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k+1}})\] \[\qquad\qquad\qquad\qquad+\bm{Q}_{k}\mathrm{diag}(1/\bm{g}_{\bm{Q }_{k+1}})-\bm{Q}_{k}\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k}})\|_{F}\] \[\leq\frac{\sqrt{nmr}}{\delta}\|\bm{C}\|_{F}(\|\mathrm{diag}(1/\bm {g}_{\bm{Q}_{k+1}})\|_{F}\|\bm{Q}_{k+1}-\bm{Q}_{k}\|_{F}\] \[\qquad\qquad\qquad\qquad+\|\bm{Q}_{k}\|_{F}\|\mathrm{diag}(1/\bm {g}_{\bm{Q}_{k+1}})-\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k}})\|_{F})\] \[\leq\frac{\sqrt{nmr}}{\delta}\|\bm{C}\|_{F}\left(\frac{1}{\delta }+\frac{\sqrt{r}}{\delta^{2}}\right)\|\bm{Q}_{k+1}-\bm{Q}_{k}\|_{F}\]

Thus, the final bound on the \(\nabla^{(B)}_{\bm{Q}}\) term is:

\[\leq\frac{\sqrt{n}}{\delta}\|\nabla^{(A)}_{\bm{Q}_{k+1}}-\nabla^{ (A)}_{\bm{Q}_{k}}\|_{F}+\frac{\sqrt{nmr}}{\delta}\|\bm{C}\|_{F}\|(\bm{Q}_{k+1} \mathrm{diag}(1/\bm{g}_{\bm{Q}_{k+1}})-\bm{Q}_{k}\mathrm{diag}(1/\bm{g}_{\bm{ Q}_{k}})\|_{F}\] \[\leq\left(\frac{\|\bm{C}\|_{F}\sqrt{nr}}{\delta^{4}}+\frac{\sqrt{ nmr}}{\delta^{2}}\|\bm{C}\|_{F}\left(1+\frac{\sqrt{r}}{\delta}\right)\right)\|\bm{Q}_{k+1}- \bm{Q}_{k}\|_{F}\]

The total component-wise smoothness bound on \(\bm{Q}\) is then

\[\|\nabla_{\bm{Q}}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k+1},\bm{R}_{k},\bm{T}_{k})-\nabla_{\bm{Q}}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k},\bm{R}_{k},\bm {T}_{k})\|_{F}\] \[\leq\frac{\|\bm{C}\|_{F}}{\delta^{2}}\left(\left(\frac{\sqrt{nr} }{\delta^{2}}+\sqrt{nmr}\left(1+\frac{\sqrt{r}}{\delta}\right)\right)+\frac{ \sqrt{r}}{\delta}\right)\|\bm{Q}_{k+1}-\bm{Q}_{k}\|_{F}\]

Identical reasoning applies for \(\nabla_{\bm{R}}\mathcal{L}_{\mathrm{LC}}\), where we similarly have the gradient split into two terms:

\[\nabla_{\bm{R}}\mathcal{L}_{\mathrm{LC}}(\bm{Q},\bm{R},\bm{T})= \nabla^{(A)}_{\bm{R}}\mathcal{L}_{\mathrm{LC}}\nabla^{(B)}_{\bm{R}}\mathcal{L}_{ \mathrm{LC}}\] \[=\bm{C}^{\mathrm{T}}\bm{Q}\bm{X}-\bm{1}_{m}\mathrm{diag}^{-1}( \mathrm{diag}(1/\bm{g}_{\bm{R}})\bm{R}^{\mathrm{T}}\bm{C}^{\mathrm{T}}\bm{Q} \bm{X})^{\mathrm{T}}\] \[=\nabla^{(A)}_{\bm{R}}\mathcal{L}_{\mathrm{LC}}-\bm{1}_{m} \mathrm{diag}^{-1}(\mathrm{diag}(1/\bm{g}_{R})\bm{R}^{\mathrm{T}}\nabla^{(A)}_{ \bm{R}}\mathcal{L}_{\mathrm{LC}})^{\mathrm{T}}\]As before, we may first show smoothness in \(\nabla^{(A)}_{\bm{R}}\) using the same steps as for \(\bm{Q}\)

\[\|\nabla^{(A)}_{\bm{R}} \mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k+1},\bm{R}_{k+1},\bm{T}_{k})- \nabla^{(A)}_{\bm{R}}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k+1},\bm{R}_{k},\bm{T}_{ k})\|_{F}\] \[=\|\bm{C}^{\mathrm{T}}\bm{Q}_{k+1}\mathrm{diag}(1/\bm{g}_{\bm{Q}_ {k+1}})\bm{T}_{k}\mathrm{diag}(1/\bm{g}_{\bm{R}_{k+1}})-\bm{C}^{\mathrm{T}}\bm{ Q}_{k+1}\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k+1}})\bm{T}_{k}\mathrm{diag}(1/\bm{g}_{ \bm{R}_{k}})\|_{F}\] \[\leq\|\bm{C}\|_{F}\|\mathrm{diag}(1/\bm{g}_{\bm{Q}_{k+1}})\|_{F}\| \bm{Q}_{k+1}\|_{F}\|\bm{T}_{k}\|_{F}\|\mathrm{diag}(1/\bm{g}_{\bm{R}_{k+1}})- \mathrm{diag}(1/\bm{g}_{\bm{R}_{k}})\|_{F}\] \[\leq\frac{\|\bm{C}\|_{F}}{\delta}\|\mathrm{diag}(1/\bm{g}_{\bm{R} _{k+1}})-\mathrm{diag}(1/\bm{g}_{\bm{R}_{k}})\|_{F}\] \[\leq\frac{\|\bm{C}\|_{F}}{\delta^{3}}\|\bm{g}_{\bm{R}_{k+1}}-\bm{ g}_{\bm{R}_{k}}\|_{2}\] \[\leq\frac{\|\bm{C}\|_{F}\sqrt{r}}{\delta^{3}}\|\bm{R}_{k+1}-\bm{R} _{k}\|_{F}.\]

For \(\nabla^{(B)}_{\bm{R}}\), one may use the same reasoning as before to find:

\[\|\nabla^{(B)}_{\bm{R}}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k+1},\bm {R}_{k+1},\bm{T}_{k})-\nabla^{(B)}_{\bm{R}}\mathcal{L}_{\mathrm{LC}}(\bm{Q}_{k +1},\bm{R}_{k},\bm{T}_{k})\|_{F}\] \[\leq\|\bm{1}_{m}\left[\mathrm{diag}^{-1}(\mathrm{diag}(1/\bm{g}_{ \bm{R}_{k+1}})\bm{R}_{k+1}^{\mathrm{T}}\nabla^{(A)}_{\bm{R}_{k+1}}\mathcal{L}_ {\mathrm{LC}})-\mathrm{diag}^{-1}(\mathrm{diag}(1/\bm{g}_{\bm{R}_{k}})\bm{R}_ {k}^{\mathrm{T}}\nabla^{(A)}_{\bm{R}_{k}}\mathcal{L}_{\mathrm{LC}})\right]^{ \mathrm{T}}\|_{F}\] \[\leq\sqrt{m}\|\mathrm{diag}(1/\bm{g}_{\bm{R}_{k+1}})\bm{R}_{k+1}^ {\mathrm{T}}\nabla^{(A)}_{\bm{R}_{k+1}}\mathcal{L}_{\mathrm{LC}}-\mathrm{diag }(1/\bm{g}_{\bm{R}_{k}})\bm{R}_{k}^{\mathrm{T}}\nabla^{(A)}_{\bm{R}_{k}} \mathcal{L}_{\mathrm{LC}}\|_{F}\]

As before, one may apply three rounds of triangle inequality inside the norm to bound this directly in terms of \(\|\nabla^{(A)}_{\bm{R}_{k+1}}-\nabla^{(A)}_{\bm{R}_{k}}\|_{F}\), \(\|\mathrm{diag}(1/\bm{R}_{k+1})-\mathrm{diag}(1/\bm{R}_{k})\|_{F}\), and \(\|\bm{R}_{k+1}-\bm{R}_{k}\|_{F}\). Each of these terms is smooth in \(\bm{R}\) by the lower-bound argument, so that smoothness in \(\bm{R}\) holds analogously to \(\bm{Q}\). The remainder of the proof for smoothness in \(\bm{R}\) thus follows identically to that of \(\bm{Q}\) above.

For \(\bm{T}\), the component-wise bound of

\[\|\nabla_{\bm{T}_{k+1}}\mathcal{L}_{\mathrm{LC}}-\nabla_{\bm{T}_{k}}\mathcal{ L}_{\mathrm{LC}}\|_{F}=\|\bm{Q}_{k+1}\bm{C}\bm{R}_{k+1}^{\mathrm{T}}-\bm{Q}_{k+1} \bm{C}\bm{R}_{k+1}^{\mathrm{T}}\|_{F}^{2}\leq L_{T}\|\bm{T}_{k+1}-\bm{T}_{k}\| _{F}\]

holds trivially for any \(L_{T}>0\) as the gradient is uniquely determined by \(\bm{Q}\) and \(\bm{R}\) alone. Thus there exist \(L_{\bm{Q}},L_{\bm{R}},L_{\bm{T}}>0\) as component-wise smoothness constants for \(\mathcal{L}_{\mathrm{LC}}(\bm{Q},\bm{R},\bm{T})\). 

We next prove Proposition 3.4, restated just below for convenience.

**Proposition E.6** (Proposition 3.4).: _Consider the FRLC objective (8). The FRLC algorithm, Algorithm 4, yields \(\beta\)-smooth iterates for \(\beta=\mathrm{poly}(\|\bm{C}\|_{F},m,r,\delta)\), where \(\delta\) denotes the lower-bound on the entries of \(\bm{g}_{\bm{Q}},\bm{g}_{\bm{R}}\). Consider the convergence metric of 3.3 adapted from Ghadimi et al. (2014), given as:_

\[\Delta_{k}(\bm{x}_{k},\bm{x}_{k+1})=\sum_{i=1}^{p}\|\mathbf{g}_{\mathcal{X},k, i}\|^{2}=\frac{1}{\gamma_{k}^{2}}\left[\|\bm{Q}_{k}-\bm{Q}_{k-1}\|_{F}^{2}+\|\bm{R}_{k}- \bm{R}_{k-1}\|_{F}^{2}+\|\bm{T}_{k}-\bm{T}_{k-1}\|_{F}^{2}\right]\]

_for \(\bm{x}_{k}=(\bm{Q}_{k},\bm{T}_{k},\bm{R}_{k})\). Define the gap to the optimal solution \(D\) as in (36), and let \(L=\sup_{i}(L_{i})\) to be the global smoothness constant across all components. Then for \(\gamma_{k}=\alpha/L\) as defined in 3.3 the FRLC algorithm has the non-asymptotic stationary convergence guarantee that:_

\[\min_{k\in 1,..,N-1}\Delta_{k}\leq\frac{2D^{2}L^{2}}{N\alpha^{2}}\]

Proof.: The proof of the non-asymptotic stationary convergence of mirror descent of Ghadimi et al. (2014), adapted for coordinate mirror descent using the block-descent lemma in 3.3, only requires component-wise smoothness in \((\bm{Q},\bm{R},\bm{T})\). The proof of this for FRLC is given in [11], and the guarantee follows directly for this value of \(L=\max(L_{\bm{Q}},L_{\bm{R}},L_{\bm{T}})=\mathrm{poly}(\|\bm{C}\|_{F},m,r,\delta)\). 

**Proposition E.7**.: _Low-rank Approximation Error. Let \(\mathrm{SR}\text{-}\mathrm{W}^{*}_{r}\) denote the optimal rank-\(r\) approximation for the semi-relaxed low-rank optimal transport problem, and let \(\mathrm{SR}\text{-}\mathrm{W}^{*}\) denote the optimal solution for the full-rank semi-relaxed optimal transport problem. Additionally, suppose \(c_{\bm{b}}=\sum_{j=1}^{m}\bm{b}_{j}\) denotes the sum of the entries of the second marginal (\(c_{\bm{b}}=1\) if a probability measure). Then we have the following upper-bound on the objective error:_

\[|\mathrm{SR}\text{-}\mathrm{W}^{*}_{r}(\mu_{\bm{b}})-\mathrm{SR}\text{-}\mathrm{W}^ {*}(\mu_{\bm{b}})|\leq c_{\bm{b}}\left(\max_{p,q}\{\bm{C}_{pq}\}-\min_{p,q}\{\bm{C}_ {pq}\}\right)\ln\left(\min\{n,m\}/(r-1)\right)\]_We note that this bound also applies for the standard balanced optimal transport case, giving:_

\[|\mathrm{W}_{r}^{\star}(\mu_{\bm{a}},\mu_{\bm{b}})-\mathrm{W}^{\star}(\mu_{\bm{a }},\mu_{\bm{b}})|\leq\left(\max_{p,q}\{\bm{C}_{pq}\}-\min_{p,q}\{\bm{C}_{pq}\} \right)\ln\left(\min\{n,m\}/(r-1)\right)\]

_and improves the previous bound of \(|\mathrm{W}_{r}^{\star}(\mu_{\bm{a}},\mu_{\bm{b}})-\mathrm{W}^{\star}(\mu_{\bm {a}},\mu_{\bm{b}})|\leq\|\bm{C}\|_{\infty}\ln\left(\min\{n,m\}/(r-1)\right)\) as the distance matrix \(\bm{C}\) contains only non-negative entries._

Proof.: We adapt the proof from Scethon and Cuturi (2022) for the balanced case, which previously gave the bound:

\[|\mathrm{W}_{r}^{\star}(\mu_{\bm{a}},\mu_{\bm{b}})-\mathrm{W}^{\star}(\mu_{\bm {a}},\mu_{\bm{b}})|\leq\|\bm{C}\|_{\infty}\ln\left(\min\{n,m\}/(r-1)\right)\]

In particular, for \(z=\min\{m,n\}\), there exists an optimal \(\mathrm{rank}_{+}(\bm{P}^{\ast})\leq z\) where one may express the optimal solution for the non-negative coupling matrix \(\bm{P}\) as a sum of \(z\) rank-one, non-negative outer products \(\tilde{\bm{q}}_{k}\tilde{\bm{r}}_{k}^{\mathrm{T}}\succcurlyeq 0\):

\[\bm{P}^{\ast}=\sum_{k=1}^{z}\tilde{\bm{q}}_{k}\tilde{\bm{r}}_{k}^{\mathrm{T}} =\sum_{k=1}^{z}\lambda_{k}\bm{q}_{k}\bm{r}_{k}^{\mathrm{T}}\]

Where we write this sum in terms of normalized vectors \(\bm{q}_{k}=\tilde{\bm{q}}_{k}/\|\tilde{\bm{q}}_{k}\|_{1}\), \(\bm{r}_{k}=\tilde{\bm{r}}_{k}/\|\tilde{\bm{r}}_{k}\|_{1}\), and \(\lambda_{k}=\|\tilde{\bm{r}}_{k}\|_{1}\|\tilde{\bm{q}}_{k}\|_{1}\). Without any loss of generality, \((\lambda_{k})_{k=1}^{z}\) is ordered in terms of decreasing value such that \(\lambda_{1}\geq\lambda_{2}\geq...\geq\lambda_{z}\). Note that we have a fixed constraint for the sum of the entries of \(\bm{P}\) for the semi-relaxed case, assuming \(\bm{b}\) is a general positive measure, as \(\mathbf{1}_{n}^{\mathrm{T}}\bm{P}\mathbf{1}_{m}=\left(\bm{P}^{\mathrm{T}} \mathbf{1}_{n}\right)^{\mathrm{T}}\mathbf{1}_{m}=\bm{b}^{\mathrm{T}}\mathbf{1} _{m}=\sum_{j=1}^{m}\bm{b}_{j}:=c_{\bm{b}}\) (where \(c_{\bm{b}}=1\) if \(\bm{b}\) is chosen to be a probability measure, i.e. in the balanced case). Moreover, it is simple to observe for these ordered values that \(\lambda_{k}\leq(c_{\bm{b}}/k)\). As in Scethon and Cuturi (2022), define the weighted average of the bottom \(z-r+1\) vectors of the decomposition to be:

\[\bm{\alpha}_{r} =\frac{\sum_{i=r}^{z}\lambda_{i}\bm{q}_{i}}{\sum_{i=r}^{z} \lambda_{i}}\] \[\bm{\beta}_{r} =\frac{\sum_{i=r}^{z}\lambda_{i}\bm{r}_{i}}{\sum_{i=r}^{z} \lambda_{i}}\]

And take the rank-\(r\) approximation using the optimal \(r-1\) vectors of \(\mathrm{OPT}\) and this weighted average of the bottom to be:

\[\tilde{\bm{P}}_{r}=\sum_{i=1}^{r-1}\lambda_{i}\bm{q}_{i}\bm{r}_{i}^{\mathrm{T} }+\left(\sum_{i=r}^{z}\lambda_{i}\right)\bm{\alpha}_{r}\bm{\beta}_{r}^{\mathrm{ T}}\]

Where, by the assumption that \(\bm{P}^{\ast}\in\Pi_{\bm{b}}\) is feasible:

\[\tilde{\bm{P}}_{r}^{\mathrm{T}}\mathbf{1}_{n}=\sum_{i=1}^{r-1}\lambda_{i}\bm{r }_{i}\bm{q}_{i}^{\mathrm{T}}\mathbf{1}_{n}+\left(\sum_{i=r}^{z}\lambda_{i} \right)\bm{\beta}_{r}\bm{\alpha}_{r}^{\mathrm{T}}\mathbf{1}_{n}=\sum_{i=1}^{r -1}\lambda_{i}\bm{r}_{i}+\left(\sum_{i=r}^{z}\lambda_{i}\right)\bm{\beta}_{r}= \sum_{i=1}^{z}\lambda_{i}\bm{r}_{i}=\bm{b}\]

Thus \(\tilde{\bm{P}}_{r}\in\Pi_{\bm{b},r}\) is a feasible rank-\(r\) solution by feasibility of \(\bm{P}^{\ast}\). One can verify that if \(\bm{P}^{\ast}\in\Pi_{\bm{a},\bm{b}}\), then \(\tilde{\bm{P}}_{r}\mathbf{1}_{m}=\bm{a}\) and the solution is again feasible. From this, we observe that the difference between this solution and \(\bm{P}^{\ast}\) is an upper-bound to the difference between \(\bm{P}^{\ast}\) and the optimal rank-\(r\) solution:

\[|\mathrm{SR-W}_{r}^{\star}(\mu_{\bm{a}},\mu_{\bm{b}})-\mathrm{SR-W} ^{\star}(\mu_{\bm{a}},\mu_{\bm{b}})| =|\langle\bm{P}_{r}^{\ast},\bm{C}\rangle_{F}-\langle\bm{P}^{\ast},\bm{C}\rangle_{F}|\leq\langle\tilde{\bm{P}}_{r},\bm{C}\rangle_{F}-\langle\bm{P} ^{\ast},\bm{C}\rangle_{F}\] \[=\left\langle\left(\sum_{i=r}^{z}\lambda_{i}\right)\bm{\alpha}_{r }\bm{\beta}_{r}^{\mathrm{T}}-\sum_{i=r}^{z}\lambda_{i}\bm{q}_{i}\bm{r}_{i}^{ \mathrm{T}},\bm{C}\right\rangle_{F}\]Noting that \(\bm{\alpha}_{r_{i}}\bm{\beta}_{r}\) and \(\bm{q}_{i},\bm{r}_{i}\) are unit normalized positive vectors, the sum of the entries of the outer product \(\mathbf{1}_{n}^{\mathrm{T}}\bm{q}_{i}\bm{r}_{i}^{\mathrm{T}}\mathbf{1}_{m}=1\), and likewise for \(\bm{\alpha}_{r}\bm{\beta}_{r}^{\mathrm{T}}\). Thus, continuing from the above display:

\[=(\sum_{i=r}^{z}\lambda_{i})\langle\bm{\alpha}_{r}\bm{\beta}_{r}^ {\mathrm{T}},\bm{C}\rangle_{F}-\sum_{i=r}^{z}\lambda_{i}\langle\bm{q}_{i}\bm{r }_{i}^{\mathrm{T}},\bm{C}\rangle_{F}\] \[\leq(\sum_{i=r}^{z}\lambda_{i})\langle\bm{\alpha}_{r}\bm{\beta}_ {r}^{\mathrm{T}},\bm{C}\rangle_{F}-(\sum_{i=r}^{z}\lambda_{i})\min_{p,q}\{\bm{ C}_{pq}\}\] \[\leq\left(\max_{p,q}\{\bm{C}_{pq}\}-\min_{p,q}\{\bm{C}_{pq}\} \right)\sum_{i=r}^{z}\lambda_{i}\] \[\leq c_{\bm{b}}\left(\max_{p,q}\{\bm{C}_{pq}\}-\min_{p,q}\{\bm{C} _{pq}\}\right)\sum_{i=r}^{z}\frac{1}{i}\] \[\leq c_{\bm{b}}\left(\max_{p,q}\{\bm{C}_{pq}\}-\min_{p,q}\{\bm{C} _{pq}\}\right)\ln\left(z/(r-1)\right)\]

Concluding the proof. As discussed, this directly applies to the balanced case (for \(c_{\bm{b}}=1\)). 

## Appendix F Initialization

We propose a new initialization of the sub-couplings \(\bm{Q},\bm{R},\bm{T}\) for the LC-factorization. Algorithm 6 generates a random full-rank initial condition in the set of couplings \(\Pi_{\bm{a},\bm{b}}\) which still satisfies the marginal constraints. It accomplishes this by sampling random matrices which are full-rank and applying the Sinkhorn algorithm to each of them. Ssetbon et al. (2021) proposed an initialization which represents an improvement over the rank-1 product measure which is rank-2. Follow-up work proposed initialization using k-means Ssetbon & Cuturi (2022). However, this assumes the previous diagonal factorization and is thus not application for generating a latent coupling which may be non-diagonal, non-square, and with two distinct inner marginals. Our initialization is tailored to the LC-factorization, is effective, and has a full-rank guarantee. In particular, higher-rank initializations may exhibit better convergence properties by allowing the gradient to explore a larger set of directions immediately in the optimization. This initialization is given in Algorithm 6.

``` Input \(\bm{a}\in\Delta_{n},\bm{b}\in\Delta_{m}\), \(\bm{g}_{Q}\in\Delta_{r}\), \(\bm{g}_{R}\in\Delta_{r}\)\(C_{\bm{Q}}\sim[0,1]^{n\times r},\bm{C}_{\bm{R}}\sim[0,1]^{m\times r},\bm{C}_{\bm{T}} \sim[0,1]^{r\times r}\)\(\bm{K}_{\bm{Q}}\leftarrow\bm{e}^{\bm{C}_{\bm{Q}}},\bm{K}_{\bm{R}}\gets\bm{e}^{ \bm{C}_{\bm{R}}},\bm{K}_{\bm{T}}\leftarrow\bm{e}^{\bm{C}_{\bm{T}}}\)\(\bm{Q}\leftarrow\text{Sinkhorn}(\bm{K}_{\bm{Q}},\bm{a},\bm{g}_{Q})\)\(\bm{R}\leftarrow\text{Sinkhorn}(\bm{K}_{\bm{R}},\bm{b},\bm{g}_{R})\)\(\bm{T}\leftarrow\text{Sinkhorn}(\bm{K}_{\bm{T}},\bm{g}_{Q}=\bm{Q}^{\mathrm{T}} \mathbf{1}_{n},\bm{g}_{R}=\bm{R}^{\mathrm{T}}\mathbf{1}_{m})\) Return \((\bm{Q},\bm{R},\bm{T})\) ```

**Algorithm 6** Initialize-Couplings

**Proposition F.1**.: _Suppose one samples an initial condition on the optimal transport coupling using Algorithm 6, where we assume \(\bm{C}_{ij}\sim\mathrm{Unif}(0,1)\) such that \(\mathbb{P}(\mathrm{rank}(\bm{C})<\min\{n,m\})=0\). Additionally, suppose that \(\bm{a},\bm{b}>\bm{0}\) holds elementwise for both marginals \(\bm{a}\in\Delta_{n},\bm{b}\in\Delta_{r}\). Then the elementwise exponential \(\exp\{\bm{C}\}\) (or \(\exp\{-\bm{C}\}\)) has full-rank and the return Sinkhorn\((\bm{e}^{-\bm{C}},\bm{a},\bm{b})\) has full-rank._

Proof.: It is established that a random matrix \(\bm{C}\sim[0,1]^{n\times m}\) has full-rank with probability one. For \(\bm{K}=\exp\{\bm{C}\}\), it holds that the matrix must be entry-wise positive with \(\bm{K}_{ij}\geq 0\). If columns \(\bm{C}_{\cdot,i}\neq\bm{C}_{\cdot,j}\) then clearly \(\bm{C}_{\cdot,i}^{\otimes k}\neq\bm{C}_{\cdot,i}^{\otimes k}\), and if \(\bm{C}_{\cdot,i},\bm{C}_{\cdot,j}\succeq\bm{0}\) and are independent remain so under element-wise powers. One may easily show this by contrapositive. Suppose there exist constants \(c_{1},c_{2}\) such that:

\[c_{1}\bm{C}_{\cdot,i}^{\otimes k}+c_{2}\bm{C}_{\cdot,j}^{\odot k}=0\]As \(\bm{C}_{\cdot,i},\bm{C}_{\cdot,j}\succeq\bm{0}\), without loss of generality one may assume \(c_{1}>0\) and \(c_{2}<0\). Then:

\[c_{1}\bm{C}_{\cdot,i}^{\otimes k}=c_{1}\bm{1}\odot\bm{C}_{\cdot,i}^{\otimes k}=- c_{2}\bm{1}\odot\bm{C}_{\cdot,j}^{\otimes k}=-c_{2}\bm{C}_{\cdot,j}^{\otimes k} \implies\left(-\frac{c_{1}}{c_{2}}\right)^{1/k}\bm{1}=c\bm{1}=\frac{\bm{C}_{ \cdot,j}}{\bm{C}_{\cdot,i}}\]

So clearly one has that \(\bm{C}_{\cdot,j}-c\bm{C}_{\cdot,i}=\bm{0}\) for \(c>0\). This implies the columns \(\bm{C}_{\cdot,j}\) and \(\bm{C}_{\cdot,i}\) are dependent. Thus it is clear that elementwise powers of entrywise positive independent vectors preserve independence. The same principle extends trivially to exponentiation of the columns, where if one assumes by contradiction that \(c_{1}e^{\bm{C}_{\cdot,i}}+c_{2}e^{\bm{C}_{\cdot,j}}=\bm{0}\) for \(c_{1}>0\), \(c_{2}<0\), one finds \(\log\left(-\frac{c_{1}}{c_{2}}\right)\bm{1}=\bm{C}_{\cdot,j}-\bm{C}_{\cdot,i}\). Without loss of generality, assume \(0<-c_{2}\leq c_{1}\) and \(\bm{C}_{\cdot,j}>\bm{C}_{\cdot,i}>\bm{0}\), so that \(\delta=\log\left(-\frac{c_{1}}{c_{2}}\right)\geq 0\). Then, considering constants \(q_{1},q_{2}\):

\[q_{2}\bm{C}_{\cdot,j}+q_{1}\bm{C}_{\cdot,i}=(q_{1}+q_{2})\bm{C}_{\cdot,i}+ \delta q_{2}\bm{1}=\bm{0}\]

Assuming \(\bm{C}_{\cdot,i}\) has greater than one unique entry, which we assume as the entries are sampled densely in \(\mathbb{R}\), the two vectors are dependent if and only if \(q_{1}=-q_{2}\) and \(\delta=0\), implying \(\bm{C}_{\cdot,i}=\bm{C}_{\cdot,j}\). Thus, for the set of independent column vectors of \(\bm{C}\), given as \(\{\bm{C}_{\cdot,i}\}_{i=1}^{m}\), the set \(\{e^{\bm{C}_{\cdot,i}}\}_{i=1}^{m}\), is also linearly independent. This holds analogously for the row vectors. As \(\bm{C}\) is full-rank and \(\operatorname{span}(\{\bm{C}_{\cdot,i}\})=\mathbb{R}^{\min\{m,n\}}\), we have that \(\operatorname{span}(\bm{K})=\mathbb{R}^{\min\{m,n\}}\) as \(\bm{K}=e^{\bm{C}}=\sum_{k=0}^{\infty}\frac{\bm{C}^{\otimes k}}{k!}\) (analogously \(e^{-\bm{C}}\)) and remains full-rank.

Sinkhorn expresses each variable as

\[\bm{X}=\operatorname{diag}(\bm{u})\bm{K}\operatorname{diag}(\bm{v})\]

where \(\operatorname{rank}(\bm{X})=\operatorname{rank}(\operatorname{diag}(\bm{u}) \bm{K}\operatorname{diag}(\bm{v}))\). As Cuturi (2013b) updates the vectors \(\bm{u}\leftarrow\bm{a}/\bm{K}\bm{v}\) and \(\bm{v}\leftarrow\bm{b}/\bm{K}^{\mathrm{T}}\bm{u}\) from \(\bm{u}_{0}=\bm{1}_{n}\) and \(\bm{v}_{0}=\bm{1}_{m}\), if \(\bm{a},\bm{b}>\bm{0}\) holds element-wise, one has that \(\bm{u},\bm{v}>\bm{0}\) elementwise as well. Then, one has that \(\operatorname{null}\operatorname{diag}(\bm{v})=\{\bm{0}\}\) and \(\operatorname{null}\operatorname{diag}(\bm{u})=\{\bm{0}\}\), implying that \(\operatorname{rank}(\operatorname{diag}(\bm{u})\bm{K}\operatorname{diag}(\bm{ v}))=\operatorname{rank}(\bm{K})=\min\{n,m\}\).

Thus, our initialization returns a random coupling matrix \(\bm{X}\in\Pi_{\bm{a},\bm{b}}\) of full-rank. 

In the next proposition, we show that one can _analytically_ solve for the block-optimal weights \(\bm{g}\) for the factorization of the coupling matrix \(\bm{P}\) as \(\bm{P}=\bm{Q}\operatorname{diag}(1/\bm{g})\bm{R}^{\mathrm{T}}\)Forrow et al. (2019); Scethon et al. (2021).

**Proposition F.2**.: _For the minimization problem expressed as_

\[\min_{\bm{g}\in\Delta_{r}}\langle\bm{Q}\operatorname{diag}(1/\bm{g})\bm{R}^{ \mathrm{T}},\bm{C}\rangle_{F}\]

_One has the closed-form minimizer of \(\bm{g}^{*}\) defined entrywise as:_

\[\bm{g}_{i}^{*}=\frac{\sqrt{\bm{\omega}_{i}}}{\sum_{j=1}^{r}\sqrt{\bm{\omega}_{ j}}}\]

_For \(\bm{\omega}=\operatorname{diag}^{-1}(\bm{Q}^{\mathrm{T}}\bm{C}\bm{R})\), when \(\bm{\omega}\geq\bm{0}\) holds entrywise._

Proof.: As \(\bm{\omega}\geq\bm{0}\), we consider the simplex condition \(\sum_{j=1}^{r}\bm{g}_{j}=1\) alone. Writing out the Lagrangian associated to our objective, with \(\lambda\in\mathbb{R}\) our equality-condition dual variable, we have:

\[\mathcal{L}(\bm{g},\lambda)=\langle\bm{Q}\operatorname{diag}(1/\bm{g})\bm{R}^{ \mathrm{T}},\bm{C}\rangle_{F}+\lambda(1-\sum_{j}\bm{g}_{j})\]

Let us consider a rewriting of the inner product term:

\[\langle\bm{Q}\operatorname{diag}(1/\bm{g})\bm{R}^{\mathrm{T}}, \bm{C}\rangle_{F} =\sum_{i=1}^{n}\sum_{j=1}^{m}\bm{C}_{ij}\sum_{k=1}^{r}\bm{Q}_{ik} \left(\frac{1}{\bm{g}_{k}}\right)\bm{R}_{kj}^{\mathrm{T}}\] \[=\sum_{k=1}^{r}\left(\frac{1}{\bm{g}_{k}}\right)\sum_{i=1}^{n}\sum_ {j=1}^{m}\bm{Q}_{ki}^{\mathrm{T}}C_{ij}\bm{R}_{jk}\] \[=\sum_{k=1}^{r}\left(\frac{1}{\bm{g}_{k}}\right)\left(\bm{Q}^{ \mathrm{T}}\bm{C}\bm{R}\right)_{k,k}\] \[=\sum_{k=1}^{r}\frac{\bm{\omega}_{k}}{\bm{g}_{k}}=\bm{\omega}^{ \mathrm{T}}\left(1/\bm{g}\right)\]Where division is interpreted element-wise in the last line. Thus, one can interpret the problem as minimizing the weighted sum of reciprocals of a density. As a result, we can simplify our Lagrangian's Froebenius inner product to a vector dot-product as:

\[\mathcal{L}(\bm{g},\lambda)=\bm{\omega}^{\mathrm{T}}\left(1/\bm{g}\right)- \lambda(1-\sum_{j=1}^{r}\bm{g}_{j})\]

Thus, the first order condition tells us that the value of the coupling weight \(\bm{g}_{j}\) is related to \(\lambda\) as:

\[\partial_{\bm{g}_{j}}\mathcal{L}(\bm{g},\lambda)=-\frac{\bm{\omega}_{j}}{\bm {g}_{j}^{2}}+\lambda=0\implies\bm{g}_{j}=\sqrt{\frac{\bm{\omega}_{j}}{\lambda}}\]

And by relying on the summation condition on the probability density \(\bm{g}\), yields the Langrange multiplier as

\[\sum_{j=1}^{r}\bm{g}_{j}=1=\sum_{j=1}^{r}\sqrt{\frac{\bm{\omega}_{j}}{\lambda}}\]

so that one finds

\[1=\frac{1}{\sqrt{\lambda}}\sum_{j=1}^{r}\sqrt{\bm{\omega}_{j}}\implies\lambda =\left(\sum_{j=1}^{r}\sqrt{\bm{\omega}_{j}}\right)^{2}\]

Plugging our Lagrange-multiplier into the above expression yields:

\[\bm{g}_{j}=\sqrt{\frac{\bm{\omega}_{j}}{\lambda}}=\sqrt{\frac{\bm{\omega}_{j} }{\left(\sum_{i=1}^{r}\sqrt{\bm{\omega}_{i}}\right)^{2}}}=\frac{\sqrt{\bm{ \omega}_{j}}}{\sum_{i=1}^{r}\sqrt{\bm{\omega}_{i}}}\]

As the Hessian \(\nabla_{\bm{g}}^{2}\mathcal{L}=\mathrm{diag}(\frac{\bm{\omega}}{\bm{g}^{3}}) \succcurlyeq\bm{0}\), we conclude that this value of \(\bm{g}\) indeed minimizes the loss over \(\Delta_{r}\). 

## Appendix G Alternating updates on the dual variables

For the problem:

\[\inf_{(\bm{Q},\bm{R}_{k},\bm{g}_{k})\in\mathcal{C}_{1}\cap\mathcal{C}_{2}} \left(\frac{1}{\gamma_{k}}\mathrm{KL}(\bm{Q}\|\bm{K}_{\bm{Q}})+\tau\mathrm{KL} (\bm{Q}\bm{1}_{r}\|\bm{a})-\bm{\lambda}_{1}^{\mathrm{T}}\bm{Q}^{\mathrm{T}} \bm{1}_{n}\right)\] (40)

one can find a simple set of semi-relaxed updates for the coupling matrix. We note the primal-dual relationship of Sinkhorn, \(\bm{Q}=\mathrm{diag}(e^{\gamma_{k}\bm{f}_{1}})\bm{K}_{\bm{Q}}\,\mathrm{diag}(e ^{\gamma_{k}\bm{h}_{1}})\), and consider the entry-wise first-order condition required for the sub-coupling \(\bm{Q}\):

\[\begin{split} 0=\gamma_{k}^{-1}\log\left(\frac{\bm{Q}_{ij}}{\left( \bm{K}_{\bm{Q}}\right)_{ij}}\right)+\tau\log\left(\frac{\left\langle\bm{Q}_{i, },\bm{1}_{r}\right\rangle}{\bm{a}_{i}}\right)-\bm{\lambda}_{1,i}\\ \implies\log\bm{Q}_{ij}=\tau\gamma_{k}\log\left(\frac{\bm{a}_{i }}{\left\langle\bm{Q}_{i,},\bm{1}_{r}\right\rangle}\right)+\log\left(\bm{K}_{ \bm{Q}}\right)_{ij}-\bm{\lambda}_{1j}\gamma_{k}\end{split}\] (41)

Thus:

\[\bm{Q}_{ij}=\left(\frac{\bm{a}_{i}}{\bm{Q}_{i,}^{\mathrm{T}}\bm{1}_{r}}\right) ^{\tau\gamma_{k}}\left(\bm{K}_{\bm{Q}}\right)_{ij}e^{-\bm{\lambda}_{1j}\gamma _{k}}\]

And in matrix-form, this yields:

\[\begin{split}\bm{Q}&=\mathrm{diag}\left(\frac{\bm{a }}{\bm{Q}\bm{1}_{r}}\right)^{\tau\gamma_{k}}\bm{K}_{\bm{Q}}\,\mathrm{diag}(e ^{-\gamma_{k}\bm{\lambda}_{1}})\\ &=\mathrm{diag}(e^{\gamma_{k}\bm{f}_{1}})\bm{K}_{\bm{Q}}\,\mathrm{ diag}(e^{\gamma_{k}\bm{h}_{1}})\end{split}\]And expanding the \(\bm{Q}\bm{1}_{r}\) term explicitly, noting that \(\bm{X}\operatorname{diag}(\bm{v})\bm{1}=\bm{X}\bm{v}\), we have:

\[\operatorname{diag} \left(\frac{\bm{a}}{\bm{Q}\bm{1}_{r}}\right)^{\tau\gamma_{k}}\bm{K }_{\bm{Q}}\operatorname{diag}(e^{-\gamma_{k}\bm{\lambda}_{1}})\] \[=\operatorname{diag}\left(\frac{\bm{a}}{\operatorname{diag}(e^{ \gamma_{k}\bm{f}_{1}})\bm{K}_{\bm{Q}}\operatorname{diag}(e^{\gamma_{k}\bm{h}_{ 1}})\bm{1}_{r}}\right)^{\tau\gamma_{k}}\bm{K}_{\bm{Q}}\operatorname{diag}(e^{- \gamma_{k}\bm{\lambda}_{1}})\] \[=\operatorname{diag}\left(\frac{\bm{a}}{e^{\gamma_{k}\bm{f}_{1}} \odot\bm{K}_{\bm{Q}}e^{\gamma_{k}\bm{h}_{1}}}\right)^{\tau\gamma_{k}}\bm{K}_{ \bm{Q}}\operatorname{diag}(e^{-\gamma_{k}\bm{\lambda}_{1}})\]

Thus, we identify \(e^{\gamma_{k}\bm{h}_{1}}=e^{-\gamma_{k}\bm{\lambda}_{1}}\) as the right dual vector, and identify the following relationship in terms of the left dual vector:

\[\left(\frac{\bm{a}}{e^{\gamma_{k}\bm{f}_{1}}\odot\bm{K}_{\bm{Q}}e^{\gamma_{k} \bm{h}_{1}}}\right)^{\tau\gamma_{k}}=e^{\gamma_{k}\bm{f}_{1}}\implies e^{ \gamma_{k}\bm{f}_{1}}=\left(\frac{\bm{a}}{\bm{K}_{\bm{Q}}e^{\gamma_{k}\bm{h}_{ 1}}}\right)^{\frac{\tau}{\tau+1/\gamma_{k}}}\]

From 40, the condition that \((\bm{Q},\bm{R}_{k},\bm{g}_{k})\in\mathcal{C}_{1}\cap\mathcal{C}_{2}\) implies that \(\bm{Q}^{\mathrm{T}}\bm{1}_{m}=\bm{g}_{k}:=\bm{g}\). As such, we find that:

\[\bm{Q}^{\mathrm{T}}\bm{1}_{m}=\operatorname{diag}(e^{\gamma_{k}\bm{h}_{1}}) \bm{K}_{\bm{Q}}^{\mathrm{T}}\operatorname{diag}(e^{\gamma_{k}\bm{f}_{1}})\bm {1}_{m}=\operatorname{diag}(e^{\gamma_{k}\bm{h}_{1}})\bm{K}_{\bm{Q}}^{\mathrm{ T}}e^{\gamma_{k}\bm{f}_{1}}=\bm{g}\]

Implying an update for \(e^{\gamma_{k}\bm{h}_{1}}\) in the form:

\[e^{\gamma_{k}\bm{h}_{1}}=\left(\frac{\bm{g}}{\bm{K}_{\bm{Q}}^{\mathrm{T}}e^{ \gamma_{k}\bm{f}_{1}}}\right)\]

Analogous reasoning applies for a relaxation of the other marginal, yielding the \(\operatorname{SR}^{\mathrm{R}}\)-projection and \(\operatorname{SR}^{\mathrm{L}}\)-projection (i.e. semi-relaxed OT).

``` Input \(\bm{K},\gamma,\tau,\bm{a},\bm{b},\delta\)\(\bm{u}\leftarrow\bm{1}_{n}\)\(\bm{v}\leftarrow\bm{1}_{r}\)repeat\(\tilde{\bm{u}}\leftarrow\bm{u}\)\(\tilde{\bm{v}}\leftarrow\bm{v}\)\(\bm{u}\leftarrow(\bm{a}/\bm{K}\bm{v})^{\tau/(\tau+\gamma^{-1})}\)\(\bm{v}\leftarrow\left(\bm{b}/\bm{K}^{\mathrm{T}}\bm{u}\right)\)until\(\gamma^{-1}\max\{\|\log\tilde{\bm{u}}/\bm{u}\|_{\infty},\|\log\tilde{\bm{v}}/\bm{v}\|_{ \infty}\}<\delta\) return\(\operatorname{diag}(\bm{u})\bm{K}\operatorname{diag}(\bm{v})\) ```

**Algorithm 7**\(\operatorname{SR}^{\mathrm{L}}\)-projection _(semi-relaxed OT, left marginal relaxed)_

## Appendix H Discussion of Complexity

For \((\bm{Q},\bm{R},\bm{T})\in\mathbb{R}_{+}^{n\times r_{1}}\times\mathbb{R}_{+}^{m \times r_{2}}\times\mathbb{R}_{+}^{r_{1}\times r_{2}}\), the space complexity \(O(nr_{1}+r_{1}r_{2}+mr_{2})\) is linear if the ranks \(r_{1},r_{2}=o(1)\) are taken to be small constants. The time-complexity of Algorithm 4 is \(O(BLr^{2}(n+m))\) for \(B\) the number of inner Sinkhorn iterations, \(L\) the number of mirror-descent steps, \(n,m\) the number of samples in the first and second dataset, and \(r=\max\{r_{1},r_{2},d\}\) for \(r_{1},r_{2}\) the ranks of the latent coupling and \(d\) the rank of the factorized distance matrix \(\bm{C}\) (generally chosen to be a constant near \(r_{1},r_{2}\)). Each matrix-multiplication is of max order \((n+m)r^{2}\), which happens a constant number of times in the computation of each gradient \(\nabla_{i}\), and for the respective Sinkhorn matrix-vector multiplications \(\bm{K}\bm{v}\) and \(\bm{K}^{\mathrm{T}}\bm{u}\). The \(L\) outer steps follow from the mirror-descent convergence rate and the number of iterations \(B\) required for each projection follow from the convergence of Sinkhorn. In particular, for \(\varepsilon\) a fixed error tolerance and \(\eta\) the entropy constant, one finds a \(\pm\varepsilon D\) approximation for \(D\) the diameter of the data in \(B=\operatorname{poly}(1/\eta\varepsilon)\) iterations using the Sinkhorn algorithm Charikar et al. (2023); Cuturi (2013a).

Review of Background Material

### Low-Rank Approximation of Pairwise Distance Matrices

As mentioned previously, works such as Charikar et al. (2023) have developed algorithms with linear \(O((n+m)^{1+o(1)}{\rm poly}(1/\epsilon))\) time-complexity and \(O((n+m)d)\) space-complexity for sketching the optimal transport cost _value_. Recent works on low-rank factorization of the optimal transport coupling matrix \(\bm{P}\) (the matrix associated to the coupling \(\gamma\in\Pi(\mu,\nu)\)) Svetbon & Cuturi (2022); Svetbon et al. (2023, 2021) have achieved per-iteration time-complexities of \(O(T(n+m)dr)\) for some constant non-negative rank \(r\geq 1\), \(d\) the dimension of the metric space, and \(T\) the number of iterations. By the JL-lemma one can simply embed the points in dimension \(d=O(\log{(nm)}/\epsilon^{2})\) while preserving pairwise distances, however, currently no proofs exist which offer the number of iterations \(T\) until convergence to some tolerance \(\varepsilon\). This is partially due to how recent these works are, and also to the non-convexity of the objective which is sensitive to initial conditions Svetbon et al. (2021). However, the space complexity of the algorithm is \(O((n+m)dr)\), which is noteworthy for being _linear_ in the number of points and avoids storing the potentially intractable \(O(nm)\) coupling matrix \(\bm{P}\). To accomplish this, however, these works rely on a low-rank approximation of the pairwise distance matrix. A number of works by Indyk and Woodruff have concerned algorithms for finding low-rank approximations for such distance matrices. A seminal work Bakshi & Woodruff (2018) developed an algorithm which, given two point sets \(\{\bm{x}_{i}\}_{i=1}^{n}\) and \(\{\bm{y}_{j}\}_{j=1}^{m}\) in some metric space \(\mathcal{X}\), finds a rank \(r\) approximation in \(O((n+m)^{1+\gamma}{\rm poly}(r,1/\varepsilon))\) for \(\gamma>0\) an arbitrarily small constant and \(\varepsilon>0\) an error parameter. A more recent work Indyk et al. (2019) improves on this one, by reading a sample-optimal \(O((n+m)r/\varepsilon)\) entries of the input matrix with a run-time which removes dependence on \(\gamma\) that is merely \(O((n+m){\rm poly}(r,1/\varepsilon))\). This algorithm is used by all of the low-rank optimal transport works. These works, by finding a low-rank approximation to the coupling matrix \(\bm{P}\approx\bm{A}\bm{B}^{\rm T}\in\mathbb{R}_{+}^{n\times m}\) due to space limitations on the coupling, necessarily cannot store the full distance matrix \(\bm{C}\in\mathbb{R}_{+}^{n\times m}\) of the same size in memory either. As such, it must also be approximated as \(\bm{C}\approx\bm{V}\bm{U}\) before input to the algorithm, where we necessarily require very effective approximations of \(\bm{U}\) and \(\bm{V}\) to tolerate the additional source of error from coarse-graining the distance matrix to be low-rank. As such, we present some of the details and algorithm of Indyk et al. (2019) as an essential component of the existing low-rank optimal transport solvers. We begin by summarizing the main theorems in Indyk et al. (2019), which provide an algorithm (upper-bound) on the low-rank distance-matrix approximation problem and a lower-bound on the number of entries which must be read.

**Theorem I.1**.: _Indyk et al. (2019) There is a randomized algorithm that, given a distance matrix \(\bm{C}\in\mathbb{R}^{n\times m}\), reads \(O((n+m)r/\varepsilon)\) entries of \(\bm{C}\), runs in time \(\tilde{O}(n+m)\cdot{\rm poly}(r,1/\varepsilon)\)1 and computes low-rank factors \(\bm{V}\in\mathbb{R}^{n\times r}\), \(\bm{U}\in\mathbb{R}^{r\times m}\) that with probability \(0.99\) satisfy:_

Footnote 1: Where \(\tilde{O}(\cdot)\) hides poly-log factors.

\[\|\bm{C}-\bm{V}\bm{U}\|_{F}^{2}\leq\|\bm{C}-\bm{C}_{r}\|_{F}^{2}+\varepsilon\| \bm{C}\|_{F}^{2}\] (42)

_For \(\bm{C}_{r}\) the optimal rank-\(r\) approximation of \(\bm{C}\)._

This is a remarkable result, especially in light of the next theorem.

**Theorem I.2**.: _Indyk et al. (2019) Let \(r\leq m\leq n\) and \(\varepsilon>0\) such that \(r/\varepsilon=O(\min\{m,n^{1/3}\})\). Any randomized and possibly adaptive algorithm that given a distance matrix \(\bm{C}\in\mathbb{R}^{n\times m}\) computes \(\bm{V}\in\mathbb{R}^{n\times r}\), \(\bm{U}\in\mathbb{R}^{r\times m}\) satisfying \(\|\bm{C}-\bm{V}\bm{U}\|_{F}^{2}\leq\|\bm{C}-\bm{C}_{r}\|_{F}^{2}+\varepsilon\| \bm{C}\|_{F}^{2}\) must read \(\Omega((n+m)r/\varepsilon)\) entries of \(\bm{C}\) in expectation. This lower bound also holds for symmetric distance matrices \(\bm{C}\in\mathcal{S}_{n}\)._

The lower-bound follows from a difficult argument which involves constructing a hard distribution over distance matrices, involving the use of random matrix theory. The upper-bound, however, follows relatively straightforwardly from a number of previous algorithms and their associated guarantees, along with the algorithm presented below. We introduce the algorithm and also offer a proof of I.1 for completeness.

This algorithm relies on two previous works, whose main results we summarize here.

**Theorem I.3**.: _Frieze et al. (2004) Let \(\bm{C}\in\mathbb{R}^{n\times m}\). For a sample of \(O(r/\varepsilon)\) rows according to a distribution \(\bm{p}\in\Delta_{n}\) which satisfies \(p_{i}\geq\Omega(1)\|\bm{C}_{i,\cdot}\|_{2}^{2}/\|\bm{C}\|_{F}^{2}\) for \(i\in[n]\). Then in \(O(mr/\epsilon+poly(r,1/\varepsilon))\) time one may compute a matrix \(\bm{U}\in\mathbb{R}^{r\times m}\) from this sample which satisfies:_

\[\|\bm{C}-\bm{C}\bm{U}^{\mathrm{T}}\bm{U}\|_{F}^{2}\leq\|\bm{C}-\bm{C}_{k}\|_{F }^{2}+\varepsilon\|\bm{C}\|_{F}^{2}\]

_With probability 0.99._

Thus, to compute the first low-rank factor \(\bm{U}\), we need to ensure the \(p_{i}\) generated from the algorithm satisfies this requirement and offer the (short) proof below.

Proof.: First, it is helpful to note \(d(x,y)^{2}\leq(d(x,z)+d(z,y))^{2}=d(x,z)^{2}+2d(x,z)d(y,z)+d(y,z)^{2}\leq 2(d(x,z)^{2}+d (y,z)^{2})\). Where in the last step one uses AM-GM where \(\prod_{i}a_{i}^{1/n}\leq\frac{\sum_{i}a_{i}}{n}\). Rewriting the norm of row \(i\) we have:

\[\|\bm{C}_{i,\cdot}\|_{2}^{2} =\sum_{j=1}^{m}d(\bm{x}_{i},\bm{y}_{j})^{2}\leq 2\sum_{j=1}^{m}d( \bm{x}_{i},\bm{y}_{j^{*}})^{2}+d(\bm{y}_{j^{*}},\bm{y}_{j})^{2}\] \[=2md(\bm{x}_{i},\bm{y}_{j^{*}})^{2}+2\sum_{j=1}^{m}d(\bm{y}_{j^{*} },\bm{y}_{j})^{2}\] \[\leq 2md(\bm{x}_{i},\bm{y}_{j^{*}})^{2}+4\sum_{j=1}^{m}d(\bm{y}_{j ^{*}},\bm{x}_{i^{*}})^{2}+d(\bm{y}_{j},\bm{x}_{i^{*}})^{2}\] \[=2md(\bm{x}_{i},\bm{y}_{j^{*}})^{2}+4md(\bm{y}_{j^{*}},\bm{x}_{i^ {*}})^{2}+4\sum_{j=1}^{m}d(\bm{y}_{j},\bm{x}_{i^{*}})^{2}\] \[=4m\left(\frac{1}{2}d(\bm{x}_{i},\bm{y}_{j^{*}})^{2}+d(\bm{y}_{j ^{*}},\bm{x}_{i^{*}})^{2}+\frac{1}{m}\sum_{j=1}^{m}d(\bm{y}_{j},\bm{x}_{i^{*} })^{2}\right)\leq 4mp_{i}\]As we have the re-normalization \(p_{i}\leftarrow\frac{p_{i}}{\sum_{i}p_{i}}\) before sampling, we need to consider the value of the expectation \(\mathbb{E}[\sum_{i}p_{i}]\) to conclude.

\[\mathbb{E}\left[\sum_{i=1}^{n}p_{i}\right] =\sum_{i=1}^{n}\mathbb{E}\left[d(\bm{x}_{i},\bm{y}_{j^{*}})^{2}+d (\bm{x}_{i^{*}},\bm{y}_{j^{*}})^{2}+\frac{1}{m}\sum_{j=1}^{m}d(\bm{x}_{i^{*}}, \bm{y}_{j})^{2}\right]\] \[=\sum_{i=1}^{n}\mathbb{E}_{j^{*}\sim[m]}\left[d(\bm{x}_{i},\bm{y} _{j^{*}})^{2}\right]+\mathbb{E}_{i^{*}\sim[n],j^{*}\sim[m]}\left[d(\bm{x}_{i^{ *}},\bm{y}_{j^{*}})^{2}\right]\] \[\qquad\qquad+\frac{1}{m}\sum_{j=1}^{m}\mathbb{E}_{i^{*}\sim[n]} \left[d(\bm{x}_{i^{*}},\bm{y}_{j})^{2}\right]\] \[=\sum_{i=1}^{n}\frac{1}{m}\sum_{j=1}^{m}d(\bm{x}_{i},\bm{y}_{j})+ n\sum_{i=1}^{n}\sum_{j=1}^{m}\frac{1}{nm}d(\bm{x}_{i},\bm{y}_{j})+\frac{n}{m} \sum_{j=1}^{m}\frac{1}{n}\sum_{i=1}^{n}d(\bm{x}_{i},\bm{y}_{j})\] \[=\frac{3}{m}\sum_{i=1}^{n}\sum_{j=1}^{m}d(\bm{x}_{i},\bm{y}_{j})= \frac{3}{m}\|\bm{C}\|_{F}^{2}\]

By an application of Markov's inequality we have that:

\[P\left[\left(\sum_{i=1}^{n}p_{i}\right)^{-1}\geq\left(\frac{3\|\bm{C}\|_{F}^{ 2}}{m\delta}\right)^{-1}\right]=P\left[\sum_{i=1}^{n}p_{i}\leq\frac{3\|\bm{C} \|_{F}^{2}}{m\delta}\right]\geq 1-\frac{\mathbb{E}[\sum_{i=1}^{n}p_{i}]}{\frac{3\|\bm{C}\|_{F}^{ 2}}{m\delta}}=1-\delta\]

Thus with probability \(1-\delta\) we have:

\[\frac{p_{i}}{\sum_{i^{\prime}=1}^{n}p_{i^{\prime}}}\geq\frac{mp_{i}\delta}{3 \|\bm{C}\|_{F}^{2}}=\frac{4mp_{i}\delta}{12\|\bm{C}\|_{F}^{2}}\geq\frac{\|\bm{ C}_{i..}\|_{2}^{2}\delta}{12\|\bm{C}\|_{F}^{2}}=\Omega(\delta)\frac{\|\bm{C}_{i..}\|_{ 2}^{2}}{\|\bm{C}\|_{F}^{2}}\]

This indicates the algorithm presented has probabilities with an appropriate bound for using the algorithm of Frieze et al. (2004) to sample the \(O(r/\varepsilon)\) rows of \(\bm{C}\) and generate a rank-r factor \(\bm{U}\). 

To conclude the result of Indyk et al. (2019) requires reference to an additional work which solves a regression problem for \(\bm{V}\) given \(\bm{C}\) and \(\bm{U}\).

**Theorem 1.4**.: _Chen & Price (2017) There is a randomized algorithm \(\mathcal{A}\), given matrices \(\bm{C}\in\mathbb{R}^{n\times m}\), \(\bm{U}\in\mathbb{R}^{r\times m}\) reads only \(O(r/\varepsilon)\) columns of \(\bm{C}\) with time-complexity \(O(mr)+poly(r/\varepsilon)\) and returns \(\bm{V}\in\mathbb{R}^{n\times r}\) which satisfies_

\[\|\bm{C}-\bm{V}\bm{U}\|_{F}^{2}\leq(1+\varepsilon)\min_{\bm{Z}\in\mathbb{R}^{ n\times r}}\|\bm{C}-\bm{Z}\bm{U}\|_{F}^{2}\]

_with probability \(0.99\)._

Thus, using the result of Chen and Price Chen & Price (2017), one may find a satisfying \(\bm{V}\) easily for a fixed \(\bm{U}\), \(\bm{C}\). In particular, Indyk et al. (2019) concludes by tying together the low-rank distance matrix algorithm and the guarantees of the algorithms from Frieze et al. (2004) Chen & Price (2017) as follows

\[\|\bm{C}-\bm{V}\bm{U}\|_{F}^{2} \leq(1+\varepsilon)\min_{\bm{Z}}\|\bm{C}-\bm{Z}\bm{U}\|_{F}^{2}\] \[\leq(1+\varepsilon)\|\bm{C}-\bm{C}\bm{U}^{\mathrm{T}}\bm{U}\|_{F}^ {2}\] \[\leq(1+\varepsilon)(\|\bm{C}-\bm{C}_{r}\|_{F}^{2}+\varepsilon\| \bm{C}\|_{F}^{2})\] \[=\|\bm{C}-\bm{C}_{r}\|_{F}^{2}+\varepsilon\|\bm{C}-\bm{C}_{r}\|_{ F}^{2}+(1+\varepsilon)\varepsilon\|\bm{C}\|_{F}^{2}\] \[\leq\|\bm{C}-\bm{C}_{r}\|_{F}^{2}+(1+2\varepsilon)\varepsilon\| \bm{C}\|_{F}^{2}\leq\|\bm{C}-\bm{C}_{r}\|_{F}^{2}+\tilde{\varepsilon}\|\bm{C}\| _{F}^{2}\]

Which achieves the bound up to a constant scaling of \(\varepsilon\) and shows the result of Indyk et al. (2019). We next investigate low-rank optimal transport solvers, which assume the result and algorithm of Indyk et al. (2019) to tractably scale to massive datasets.

Connection of Optimal Transport to Projection Problems

Before discussing works which have address the problem of finding low-rank decompositions of the coupling matrix \(\bm{P}\), we discuss a few relevant preliminaries. The Bregman-divergence of some function \(F(\bm{x})\), defined by the first-order Taylor expansion of \(F\):

\[D_{F}(\bm{x},\bm{y})=F(\bm{x})-F(\bm{y})+\nabla F(\bm{y})^{\mathrm{T}}\left(\bm {x}-\bm{y}\right)\]

For the negative entropy function \(-\mathrm{H}(\bm{\xi})\), this corresponds to the KL-divergence \(\mathrm{KL}(\bm{\zeta}\mid\bm{\xi})\). We introduce the iterative-Bregman projection algorithm in the context of the KL-divergence owing to the direct connection with entropically-regularized optimal transport.

**Definition J.1**.: Iterative Bregman ProjectionsBregman (1967)

Suppose \(\mathcal{C}=\cap_{i=1}^{L}\mathcal{C}_{l}\) is an intersection of closed convex sets \(\{\mathcal{C}_{l}\}_{l=1}^{L}\). For \(n>L\), let the indexing be \(L\)-periodic as \(\mathcal{C}_{n}:=\mathcal{C}_{n\mod L}\). Suppose we want to find a minimizer \(\bm{\zeta}\) of the KL-divergence with some positive vector \(\bm{\xi}\in\mathbb{R}_{+}^{n\times m}\) such that \(\bm{\zeta}\in\mathcal{C}\). This is to say, we hope to solve the problem of minimizing a distance subject to the condition that one remains in this intersection of convex sets:

\[\min_{\bm{\zeta}\in\mathcal{C}}\mathrm{KL}(\bm{\zeta}\|\bm{\xi})\]

Where the projection of \(\bm{\xi}\) onto the set \(\mathcal{C}\) is denoted by

\[\bm{\zeta}^{*}=\operatorname*{arg\,min}_{\bm{\zeta}\in\mathcal{C}}\mathrm{KL }(\bm{\zeta}\|\bm{\xi}):=\mathcal{P}_{\mathcal{C}}^{KL}(\bm{\xi})\]

Supposing that each \(\mathcal{C}_{l}\) forms a quotient space \(\mathcal{C}_{l}=V/U\) for a subspace \(U\subset V\), defined as \(V/U=\{\bm{v}+U\mid\bm{v}\in V\}\)2, the iterative Bregman projection algorithm alternates projections onto each set \(\mathcal{C}_{n}\) as

Footnote 2: This is to say, \(\bm{x},\bm{y}\in\mathcal{C}_{l}\implies c_{1}\bm{x}+c_{2}\bm{y}\in V/U\) for any \(c_{1},c_{2}\in\mathbb{R}\).

\[\bm{\zeta}^{(n)}\leftarrow\mathcal{P}_{C_{n}}^{KL}(\bm{\zeta}^{(n-1)})\] (43)

starting from \(\bm{\zeta}^{(0)}=\bm{\xi}\).

One may show Bregman (1967) the convergence of Bregman projections to the unique minimizer in \(\mathcal{C}\), \(\bm{\zeta}^{*}\), where we have the guarantee that \(\bm{\zeta}^{(n)}\rightarrow\bm{\zeta}^{*}\) as \(n\rightarrow\infty\). These iterative projections only have convergence guarantees when the constraint sets are quotient spaces-this is clearly not the case for the constraints of the optimal transport LP, and a few more notions are required.

**Definition J.2**.: Dykstra's AlgorithmDykstra (1983)

Given a point \(\bm{x}_{0}\in E\) for \(E\) a Euclidean space 3, to find the unique point in \(\mathcal{C}=\cap_{i=1}^{L}\mathcal{C}_{l}\) for closed, convex sets \(\mathcal{C}_{l}\) that minimize the distance to \(\bm{x}_{0}\) as

Footnote 3: e.g. \(\mathbb{R},\mathbb{R}^{d}\), \(\mathbb{R}^{n\times m}\), etc.

\[\bm{x}^{*}=\operatorname*{arg\,min}_{\bm{x}\in\mathcal{C}}\lVert\bm{x}-\bm{x}_ {0}\rVert_{2}\]

One may initialize the residuals \(\bm{q}_{-(L-1)}=...=\bm{q}_{0}=0\) and apply the algorithm:

\[\bm{x}_{n}=\mathcal{P}_{\mathcal{C}_{n}}(\bm{x}_{n-1}+\bm{q}_{n-1})\] \[\bm{q}_{n}=(\bm{x}_{n-1}-\bm{x}_{n})+\bm{q}_{n-L}\]

Where \(\mathcal{C}_{n}:=\mathcal{C}_{n\mod L}\) as before and \(\mathcal{P}_{\mathcal{C}_{n}}\) denotes the projection operator onto the convex set \(\mathcal{C}_{n}\).

One can note that Dykstra's algorithm for projections onto intersections of convex sets no longer relies on the assumption that the set is a quotient space, and applies in the case that it is merely closed under convex-combinations. To generalize Dykstra's for more general functions that than the \(\ell_{2}\)-norm, one may define the projection with respect to the Bregman divergence of a cost function \(F\) and define the projection by the minimization: \(\mathcal{P}_{\mathcal{C}_{n}}:=\operatorname*{arg\,min}_{\bm{x}}D_{F}(\bm{x}, \bm{y})\). It was proven in Bauschke & Lewis (2000) that the generalized form of Dykstra's iterations are given as:

\[\bm{x}_{n} =\mathcal{P}_{\mathcal{C}_{n}}\left(\nabla F^{*}(\nabla F(\bm{x}_ {n-1})+\bm{q}_{n-1})\right)\] \[\bm{q}_{n} =(\nabla F(\bm{x}_{n-1})-\nabla F(\bm{x}_{n}))+\bm{q}_{n-L}\]

For \(F^{*}\) denoting the Fenchel-conjugate of \(F\), which we define later in connection to the low-rank dual problem. Notably, Bauschke & Lewis (2000) also provided guarantees of convergence to the optimal solution which extend to the case that \(F\) is the negative entropy and \(D_{F}\) the KL-divergence. These constitute Dykstra's algorithm with cyclic Bregman projections, and project a point to the closest point in the intersection of convex sets \(\mathcal{C}=\cap_{i=1}^{L}\mathcal{C}_{l}\) for an arbitrary cost function \(F\) and its associated Bregman-divergence \(D_{F}\).

### Dykstra's algorithm with cyclic Bregman projections

As such, we can see the updates for \(F\) being the negative entropy and \(D_{F}\) the KL-divergence. Without proof, the conjugate of the negative entropy is simply given as \(F^{*}(\bm{\zeta})=\exp\{\bm{\zeta}-\bm{1}\}\). Thus, for the minimization problem:

\[\bm{\zeta}^{*}=\operatorname*{arg\,min}_{\bm{\zeta}\in\mathcal{C}}\mathrm{KL} (\bm{\zeta}\|\bm{\xi}):=\mathcal{P}_{\mathcal{C}}^{\mathrm{KL}}(\bm{\xi})\]

Letting \(\log\bm{q}_{-(L-1)}=...=\log\bm{q}_{0}=0\), one may combine the two algorithms above to solve this minimization using generalized Dykstra's iterations. In particular, we have:

\[\bm{\zeta}^{(n)} =\mathcal{P}_{\mathcal{C}_{n}}\left(\nabla F^{*}(\nabla F(\bm{ \zeta}^{(n-1)})+\log\bm{q}_{n-1})\right)\] \[=\mathcal{P}_{\mathcal{C}_{n}}\left(\exp\left(\nabla F(\bm{ \zeta}^{(n-1)})+\log\bm{q}_{n-1}-\bm{1}\right)\right)\] \[=\mathcal{P}_{\mathcal{C}_{n}}\left(\bm{\zeta}^{(n-1)}\odot\bm{ q}_{n-1}\right)\]

And:

\[\log\bm{q}_{n} =(\nabla F(\bm{\zeta}^{(n-1)})-\nabla F(\bm{\zeta}^{(n)}))+\log \bm{q}_{n-N}\] \[=\left(\log\bm{\zeta}^{(n-1)}+\bm{1}-(\log\bm{\zeta}^{(n)}+\bm{1} )+\log\bm{q}_{n-L}\right)\] \[=\log\left(\bm{q}_{n-L}\odot\frac{\bm{\zeta}^{(n-1)}}{\bm{\zeta}^ {(n)}}\right)\]

So that:

\[\bm{\zeta}^{(n)}\leftarrow\mathcal{P}_{\mathcal{C}_{n}}\left(\bm{\zeta}^{(n-1 )}\odot\bm{q}_{n-1}\right)\] (44)

\[\bm{q}_{n}\leftarrow\bm{q}_{n-L}\odot\frac{\bm{\zeta}^{(n-1)}}{\bm{\zeta}^{(n )}}\] (45)

Where division is interpreted to be element-wise, \(\odot\) refers to the Hadamard product, and the logarithm is applied elementwise.

### Connection to Sinkhorn distances

Interestingly, the Sinkhorn algorithm described in Algorithm 5 can be alternatively derived in the context of Bregman iterations Benamou et al. (2015) as a minimization of the form:

\[\mathrm{W}_{\epsilon}(\mu,\nu)=\epsilon\min_{\bm{P}\in\Pi(\mu,\nu)}\mathrm{KL} (\bm{P}\|\bm{\xi})\]

Where \(\bm{\xi}\) is the kernel \(\bm{\xi}=e^{-\bm{C}/\epsilon}\) and \(\Pi(\mu,\nu)=\mathcal{C}_{1}\cap\mathcal{C}_{2}\) for the convex constraint sets \(\mathcal{C}_{1}=\{\bm{P}:\bm{P}\bm{1}_{m}=\bm{a}\}\) and \(\mathcal{C}_{2}=\{\bm{P}:\bm{P}^{\mathrm{T}}\bm{1}_{n}=\bm{b}\}\). To cast this into the Bregman-projection framework, one alternates between the two updates:

\[\bm{P}^{(l)} =\mathcal{P}_{\mathcal{C}_{1}}(\bm{P}^{(l-1)})\] \[\bm{P}^{(l+1)} =\mathcal{P}_{\mathcal{C}_{2}}(\bm{P}^{(l)})\]

Where for the first projection one has the following first-order KKT condition:

\[\nabla\left(\epsilon\mathrm{KL}(\bm{P}\|\bm{P}^{(l-1)})+\bm{\lambda}^{ \mathrm{T}}(\bm{P}\bm{1}_{m}-\bm{a})\right) =\epsilon\log\left(\frac{\bm{P}}{\bm{P}^{(l-1)}}\right)+\bm{ \lambda}\bm{1}_{m}^{\mathrm{T}}=\bm{0}\] \[\implies\bm{P}=\mathrm{diag}(e^{-\bm{\lambda}/\epsilon})\bm{P}^{( l-1)}\]

With the constraint of \(\mathcal{C}_{1}\) that \(\bm{P}\bm{1}_{m}=\bm{a}\), this implies \(\mathrm{diag}(\bm{a}/\bm{P}^{(l-1)}\bm{1}_{m})=\mathrm{diag}(e^{-\bm{\lambda}/ \epsilon})\) and recovers the first update of Sinkhorn \(\bm{P}^{(l)}\leftarrow\mathrm{diag}(\bm{a}/\bm{P}^{(l-1)}\bm{1}_{m})\bm{P}^{( l-1)}\). An analogous argument gives the second, where all iterates satisfy \(\bm{P}^{(l)}=\mathrm{diag}(\bm{u}^{(l)})e^{-\bm{C}/\epsilon}\mathrm{diag}(\bm{v}^ {(l)})\) for \(\bm{u}\), \(\bm{v}\) as defined in Algorithm 5.

## Appendix K Additional Simulations

We tested on two additional synthetic datasets, both used as benchmarking datasets in Svetbon et al. (2021). We follow exactly the parameters provided in Svetbon et al. (2021) to simulate these datasets. For the first one, we simulated \(n=m=10,000\) points from two Gaussian mixtures in 2D (Fig. 6). The first Gaussian mixture is a mixture of three Gaussian distributions with means \((0,0),(0,1),(1,1)\) respectively. The mixture proportion is \(\frac{1}{3}\) and the covariance is 0.05 \(\times\) identity for each Gaussian. The second Gaussian mixture is a mixture of two Gaussian distributions with means \((0.5,0.5),(-0.5,0.5)\) respectively. The mixture proportions is \(\frac{1}{2}\) and the covariance is 0.05 \(\times\) identity for each Gaussian.

For the second dataset, we simulated \(n=m=5,000\) points from two Gaussian mixtures in 10D. The first Gaussian mixture is a mixture of three Gaussian distributions with means \((0,0,0,\cdots,0),(0,1,0,\cdots,0),(1,1,0,\cdots,0)\) respectively. The mixture proportions is \(\frac{1}{3}\) and the covariance is 0.05 \(\times\) identity for each Gaussian. The second Gaussian mixture is a mixture of two Gaussian distributions with means \((0.5,0.5,0,\cdots,0),(-0.5,0.5,0,\cdots,0)\) respectively. The mixture proportions is \(\frac{1}{2}\) and the covariance is 0.05 \(\times\) identity for each Gaussian.

For each dataset, we repeat the same procedure as in SS 4.1, running FRLC and LOT with Euclidean distance as cost to find a low-rank coupling matrix between the two Gaussian mixtures with rank between 50 and 200. We observe the same pattern as Fig. 2b. FRLC obtains lower transport cost with increasing rank, and achieves lower cost for each rank than LOT under all initializations (Fig. 23c, Fig. 7).

## Appendix L Graph Partitioning

### Evaluation on a Graph Partitioning Task

We next evaluate FRLC on an unsupervised graph partitioning (node clustering) problem described by Chowdhury and Needham (2021). Specifically, given a graph \(G=(V,E)\) of \(n\) nodes, we represent the graph as \(G=(\bm{A},\bm{h})\), where \(\bm{A}\in\mathbb{R}^{n\times n}\) encodes the intra-graph node relationship (e.g. adjacency matrix) and \(\bm{h}\in\Delta_{n}\) is a uniform measure. We cluster the nodes of \(G\) by estimating a GW coupling

Figure 5: Transport cost \(\langle\bm{C},\bm{P}\rangle_{F}\) against number of iterations for FRLC with rank 200 on the synthetic dataset of two moons and eight Gaussians. Smooth convergence is observed for both rank-2 and full-rank random initialization.

Figure 6: Plot of the two simulated mixtures of Gaussians in 2D, following the same parameters as Scetbon et al. (2021).

Figure 7: Transport cost \(\langle\bm{C},\bm{P}\rangle_{F}\) achieved by LOT Scetbon et al. (2021) and FRLC across different ranks and different initializations on the Wasserstein problem on the synthetic dataset of two mixtures of Gaussians in 2D.

between \(G\) and a smaller graph \(\overline{G}=(\overline{\bm{B}},\overline{\bm{h}})\), where \(\overline{\bm{B}}\in\mathbb{R}^{m\times m}\) represents the relationship between each of the \(m\) clusters and \(\overline{\bm{h}}\in\Delta_{m}\) is the proportion of nodes of \(G\) in each cluster. Without a priori knowledge, \(\overline{\bm{h}}\) is set to be uniform and \(\overline{\bm{B}}\) is set as the identity matrix. Vincent-Couaz et al. (2022) notes that instead of solving a balanced GW problem, semi-relaxed GW with the right marginal \(\overline{\bm{h}}\) relaxed learns \(\overline{\bm{h}}\) from data and leads to more accurate node clustering.

We benchmark FRLC for semi-relaxed GW on four real-world graphs: a Wikipedia hyperlink network with 15 webpage categories Yang and Leskovec (2012); an email interaction network within a European institute with 42 departments Yin et al. (2017); an Amazon product network with 12 product categories Yang and Leskovec (2012); and a network of interactions between 12 Indian villages Banerjee et al. (2013). We also test on the symmetric and noisy versions of each graph provided by Chowdhury and Needham (2021). We compare with two OT-based methods: (1) GWL Xu et al. (2019), which solves a balanced GW problem between \(G\) and \(\overline{G}\) with the adjacency matrix of \(G\) as the intra-domain cost matrix \(\bm{A}\); (2) SpecGWL Chowdhury and Needham (2021) which uses the heat kernel on the graph Laplacian as \(\bm{A}\). We similarly run our FRLC algorithm using with both the adjacency matrix (denoted FRLC-SR-GW) and heat kernel (denoted SpecFRLC-SR-GW). Since the number of clusters in each dataset is not large, we compute the full-rank coupling matrix in each case.

Overall, FRLC achieves the best clustering performance on 9 out of 12 datasets (Table 3). When using the adjacency matrix, our semi-relaxed algorithm achieves better clustering result than GWL on 9 out of 12 datasets. When using the heat kernel, our semi-relaxed algorithm achieves better

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Method & Dataset & Time in seconds & Cost Value \(\langle\bm{C},\bm{P}\rangle_{F}\) & \(\|\bm{P}\bm{1}_{n}-\bm{a}\|_{2}\) & \(\|\bm{P}^{T}\bm{1}_{m}-\bm{b}\|_{2}\) \\ \hline FRLC & 2-moons, & **0.379** & **0.207** & 1.56E-5 & 6.1E-18 \\ LOT & 8-Gaussians & 0.751 & 0.210 & 1.90E-5 & 1.89E-5 \\ \hline FRLC & Gaussian mixture & **0.354** & **0.178** & 1.05E-5 & 7.0E-18 \\ LOT & (2D) & 0.735 & 0.181 & 1.65E-5 & 1.70E-5 \\ \hline FRLC & Gaussian mixture & **0.323** & **0.294** & 2.48E-6 & 8.9E-18 \\ LOT & (10D) & 0.677 & 0.307 & 1.39E-5 & 1.46E-5 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Runtime of FRLC and LOT on the synthetic datasets of 1000 samples, as well as cost value \(\langle\bm{C},\bm{P}\rangle_{F}\) and marginal tightness for context. This was done with the FRLC setting max_inneriters_balanced=1000, max_inneriters_relaxed=50, min_iter=7 and rank \(r=100\). This time excludes the extra time incurred for out-jax problem setup and includes the setup time for FRLC.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Dataset & & GWL & FRLC & SpecGWL & SpecFRLC \\  & & & SR-GW & & SR-GW \\ \hline Wikipedia & sym, raw & 0.314 & 0.387 & 0.372 & **0.444** \\ (1998 nodes, 2700 edges) & sym, noisy & 0.250 & 0.361 & 0.293 & **0.400** \\  & asym, raw & 0.263 & 0.276 & 0.194 & **0.304** \\  & asym, noisy & **0.208** & 0.201 & 0.141 & 0.177 \\ \hline EU-email & sym, raw & 0.434 & **0.464** & 0.009 & 0.040 \\ (1005 nodes, 25571 edges) & sym, noisy & 0.392 & **0.422** & 0.009 & 0.014 \\  & asym, raw & 0.388 & **0.398** & 0.012 & 0.028 \\  & asym, noisy & **0.385** & 0.348 & 0.008 & 0.012 \\ \hline Amazon & raw & 0.322 & 0.338 & **0.505** & 0.479 \\ (1501 nodes, 4626 edges) & noisy & 0.274 & 0.257 & 0.438 & **0.453** \\ \hline Village & raw & 0.531 & **0.710** & 0.553 & 0.579 \\ (1991 nodes, 8423 edges) & noisy & 0.413 & 0.536 & 0.397 & **0.829** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Performance (measured using Adjusted Mutual Information (AMI)) in graph partitioning for _full-rank_ OT algorithms GWL, SpecGWL and full-rank semi-relaxed FRLC. The top performing method for each dataset is highlighted in bold.

clustering result than SpecGWL on 11 out of 12 datasets. These results show the importance of semi-relaxed OT on real-world problems, as well as the accuracy of FRLC.

### Problem Statement

As discussed in Vincent-Cuaz et al. (2022); Chowdhury & Needham (2021), it is possible to achieve unsupervised graph partitioning (node clustering) through Gromov-Wasserstein (GW) OT. Given a graph \((V,E)\) of \(n\) nodes, we encode it as \(G=(\bm{A},\bm{h})\), where \(\bm{A}\in\mathbb{R}^{n\times n}\) encodes the intra-graph node relationship (e.g. adjacency matrix, Laplacian) and \(\bm{h}\in\Delta n\) is a uniform measure over the nodes. If we want to cluster the nodes of \(G\) into \(m\) clusters, we can define a new graph \(\overline{G}=(\overline{\bm{B}},\overline{\bm{h}})\), where \(\overline{\bm{B}}\in\mathbb{R}^{m\times m}\) is a diagonal matrix representing the cluster's connections and \(\overline{\bm{h}}\) is a distribution over clusters estimating the proportion of nodes in \(G\) in each cluster. Since we don't know the density of clusters a priori, we can set \(\overline{\bm{h}}\) to be uniform. Usually \(\overline{\bm{B}}\) is set as the identity matrix.

To cluster the nodes in \(G\), we can solve a GW problem matching nodes in \(G\) with nodes in \(\overline{G}\), with intra-domain cost matrices \(\bm{A}\) and \(\overline{\bm{B}}\):

\[\begin{split}\min&\sum_{ij^{\prime}k!^{\prime}}(\bm{A} _{ik}-\overline{\bm{B}}_{j^{\prime}l^{\prime}})^{2}\bm{P}_{ij^{\prime}}\bm{P}_ {kl^{\prime}}\\ s.t.&\bm{P}\in\Pi_{\bm{h},\overline{\bm{h}}}\end{split}\] (46)

The cluster assignment of each node in \(G\) can then be recovered from \(\bm{P}\) by finding the node in \(\overline{G}\) mapped to it with the maximum weight.

However, since the proportion of nodes in \(G\) in each cluster is not known a priori, solving a balanced GW problem fixing the marginal of \(\bm{P}\) on \(\overline{G}\) to be a uniform \(\overline{\bm{h}}\) significantly constrains the expressivity of the algorithm. Therefore, as proposed in Vincent-Cuaz et al. (2022), we can instead solve a semi-relaxed GW problem with the right marginal relaxed, fixing the marginal on \(G\) to be \(\bm{h}\) but allowing the marginal on \(\overline{G}\) to deviate from \(\overline{\bm{h}}\):

\[\begin{split}\min&\sum_{ij^{\prime}k!^{\prime}}(\bm{A} _{ik}-\overline{\bm{B}}_{j^{\prime}l^{\prime}})^{2}\bm{P}_{ij^{\prime}}\bm{P}_ {k^{\prime}}+\tau\mathrm{KL}(\bm{P}^{\mathrm{T}}\bm{1}_{n}\mid\overline{\bm{ h}})\\ s.t.&\bm{P}\in\Pi_{\bm{h}_{n}}.\end{split}\] (47)

The learned \(\overline{\bm{h}}\) from semi-relaxed GW estimates the posterior proportion of nodes in \(G\) in each of the \(m\) clusters.

### Dataset and Preprocessing

We run our semi-relaxed FRLC algorithm on four real-world graph datasets: a Wikipedia hyperlink network with 1998 nodes and 15 clusters Yang & Leskovec (2012), a directed graph of email interactions in a European research institute with 1005 nodes and 42 clusters Yin et al. (2017), an Amazon product network with 1501 nodes and 12 clusters Yang & Leskovec (2012), and a network of interactions between Indian villages with 1991 nodes and 12 clusters Banerjee et al. (2013). The Wikipedia and EU-email graphs are directed, so we also use undirected versions of them. We also use noisy version of each graph by adding up to 10% additional edges following Chowdhury & Needham (2021), leading to a total of 12 different graphs to cluster.

### Experiment Settings

We compare our algorithm with two baseline methods, GWL and SpecGWL. GWL Xu et al. (2019) solves the entropy-regularized version of the balanced GW problem of (46) with the adjacency matrix of \(G\) as \(\bm{A}\). We set \(\bm{h}\) such that the density of each node is proportional to its degree. We set \(\overline{\bm{h}}\) to be a distribution estimated by sorting the weights of \(\bm{h}\) and sampling \(m\) values via linear interpolation, following Chowdhury & Needham (2021). We set \(\overline{\bm{B}}=diag(\overline{\bm{h}})\). We set the entropy regularization \(\eta=10^{-6}\). SpecGWL Chowdhury & Needham (2021) solves the same problem as GWL, but instead of using the adjacency matrix as \(\bm{A}\), it uses the heat kernel on the normalized graph Laplacians Chung (2005). We set the heat parameter \(t=10\).

For our method, we solve the semi-relaxed GW problem of (47) with full rank solution. We use both adjacency matrix and heat kernel as \(\bm{C}\) and label the result of the two representations as FRLC-SR-GW and SpecFRLC-SR-GW. We set \(\tau=0.01\) as to minimize the conformation to the right marginal. Since our method depends on random initialization, we run our method 10 times on each dataset and report the mean performance. We evaluate the resulting clusterings of all methods by computing the Adjusted Mutual Information (AMI) between the computed clustering and the ground truth clustering. This experiment, and the experiments on mouse embryo spatial transcriptomics, were conducted on cluster GPUs.

## Appendix M Spatial Transcriptomics Alignment

### Problem Statement

In this problem, we use FRLC to find a low-rank alignment matrix between cells from two spatial transcriptomics (ST) Stahl et al. (2016) slices, collected at two timepoints, then use the computed alignment matrix for two downstream prediction tasks. An ST experiment on a 2D tissue slice yields a pair \((X,Z)\). \(X\in\mathbb{R}^{n\times p}\) is the gene expression matrix, where \(n\) is the number of cells on the slice and \(p\) is the number of genes measured. \(X_{ij}\in\mathbb{R}\) is the gene expression level of gene \(j\) in cell \(i\), where a higher number indicates stronger expression. \(Z\in\mathbb{R}^{n\times 2}\) is the spatial coordinate matrix, where each row \(i\) stores the x-y coordinate of cell \(i\) on the slice. Therefore, each cell on the slice has a gene expression vector of length \(p\), which encodes the feature of the cell, as well as a coordinate vector of length two, which encodes the geometrical information of the cell on the slice.

Our input data is a pair of ST slices \(\mathcal{S}_{0}=(X^{0},Z^{0}),\mathcal{S}_{1}=(X^{1},Z^{1})\), with \(n\) and \(m\) cells, of the same tissue region. We assume \(\mathcal{S}_{0}\) is collected at timepoint \(t=0\) and \(\mathcal{S}_{1}\) is collected at timepoint \(t=1\), hence the transition from \(\mathcal{S}_{0}\) to \(\mathcal{S}_{1}\) reflects the biological development of the tissue during the time period. We would like to find the ancestor-descendant relationship between cells from \(\mathcal{S}_{0}\) and \(\mathcal{S}_{1}\) by computing an optimal transport coupling matrix between cells from \(\mathcal{S}_{0}\) and cells from \(\mathcal{S}_{1}\). The state-of-the-art spatial transcriptomics alignment method moscot Klein et al. (2023) claims that

Figure 8: FRLC achieves lower primal cost \(\langle\bm{C},\bm{P}\rangle_{F}\) for \(\bm{P}\in\Pi_{\bm{a},\bm{b}}\) than Scetbon et al. (2021) on a spatial-transcriptomics dataset of mouse embryonic development Chen et al. (2022). FRLC demonstrates a more robust trend of improved performance with higher rank.

unbalanced OT is the most appropriate setup for this problem. Specifically, given discrete uniform measure \(\bm{a}\) and \(\bm{b}\) over cells from \(\mathcal{S}_{0}\) and \(\mathcal{S}_{1}\), we solve the unbalanced Wasserstein problem

\[\min_{\bm{P}\in\mathbb{R}_{+}^{n\times m}}\langle\bm{C},\bm{P}\rangle+\tau \mathrm{KL}(\bm{P}\bm{1}_{m}\|\bm{a})+\tau\mathrm{KL}(\bm{P}^{\mathrm{T}}\bm{1} _{n}\|\bm{b})\] (48)

\(\bm{C}\in\mathbb{R}_{+}^{n\times m}\) has entries \(\bm{C}_{ij}=c(X_{i}^{0},X_{j}^{1})\), where \(c\) is the Euclidean distance between the features of cell \(i\) from \(\mathcal{S}_{0}\) and cell \(j\) from \(\mathcal{S}_{1}\).

The above formulation only considers the feature information of the two slices, but not the geometrical information. Therefore, we can also solve the unbalanced GW problem

\[\min_{\bm{P}\in\mathbb{R}_{+}^{n\times m}}\sum_{i,j^{\prime},k,l^{\prime}}(\bm {A}_{ik}-\bm{B}_{j^{\prime}l^{\prime}})^{2}\bm{P}_{ij^{\prime}}\bm{P}_{kl^{ \prime}}+\tau\mathrm{KL}(\bm{P}\bm{1}_{m}\|\bm{a})+\tau\mathrm{KL}(\bm{P}^{ \mathrm{T}}\bm{1}_{n}\|\bm{b})\] (49)

where \(\bm{A}\in\mathbb{R}^{n\times n}\) is the Euclidean distance matrix between the spatial location of cells within \(\mathcal{S}_{0}\), and \(\bm{B}\in\mathbb{R}^{m\times m}\) is the Euclidean distance matrix between the spatial location of cells within \(\mathcal{S}_{1}\). Similarly, we can combine the above two formulations to solve the unbalanced FGW problem.

Notice the inherent asymmetry stemming from the temporal nature of the problem: all cells on \(\mathcal{S}_{1}\) should have an ancestor from \(\mathcal{S}_{1}\), but not all cells from \(\mathcal{S}_{1}\) need to have a descendent in \(\mathcal{S}_{0}\) because of cell death. Therefore, the most natural OT task for this problem is semi-relaxed OT with the left marginal (the marginal on the first/ancestor slice) relaxed. Specifically, we can also solve the semi-relaxed Wasserstein problem

\[\min_{\bm{P}\in\Pi\,\cdot,b}\langle\bm{C},\bm{P}\rangle+\tau\mathrm{KL}(\bm{P} \bm{1}_{m}\|\bm{a})\] (50)

as well as semi-relaxed GW and FGW problem.

Gene expression prediction taskGiven the alignment matrix \(\bm{P}\) linking cells from \(\mathcal{S}_{0}\) to \(\mathcal{S}_{1}\), we can predict properties of cells in \(\mathcal{S}_{1}\) from properties of cells in \(\mathcal{S}_{0}\). Let the expression of a gene \(j\) in \(\mathcal{S}_{0}\) be a vector \(\bm{f}_{j}\in\mathbb{R}^{n}\), such that \(\bm{f}_{ji}\) is the expression level of gene \(j\) in cell \(i\), we can predict the expression of gene \(j\) in \(\mathcal{S}_{1}\) as \(\widehat{\bm{f}}_{j}=m\times\bm{P}^{\mathrm{T}}\times\bm{f}_{j}\in\mathbb{R}^ {m}\). The accuracy of the prediction can be measured by the Spearman correlation between the predicted expression and the ground truth expression \(\overline{\bm{f}}_{j}\) of gene \(j\) in \(\mathcal{S}_{1}\): \(\rho(\widehat{\bm{f}}_{j},\overline{\bm{f}}_{j})\). In this work, we test the prediction accuracy on 10 test genes: Tubb2b, Pantr1, Actc1, Tnn11, Afp, Hbb-bh1, Fez1, Crabp1, Crabp2, Col3a1, which are markers genes for various cell types in mouse embryo.

Cell type prediction taskWe can also use the cell type labels of cells in \(\mathcal{S}_{0}\) to predict the cell type labels of cells in \(\mathcal{S}_{1}\). Specifically, for each cell \(j\) in \(\mathcal{S}_{1}\), we can assign it the type of the cell \(\mathrm{argmax}_{i}\bm{P}_{ij}\) in \(\mathcal{S}_{0}\). We can measure the accuracy of the cell type prediction by computing the Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) between the predicted clustering of cells in \(\mathcal{S}_{1}\) and the ground truth clustering.

### Dataset and Preprocessing

We use the large-scale real-world dataset of mouse embryo development Chen et al. (2022), consisting of eight timepoints of ST slices during the whole process of mouse embryo development. In this work, we align the pair of adjacent timepoints of E11.5 and E12.5 embryos, consisting of 30,124 cells and 51,365 cells, respectively. We preprocess the dataset using the standard SCANPY Wolf et al. (2018) pipeline. We first filter the two slices to have the same set of genes, resulting in 26,436 genes for all cells from both slices. We then log-normalize the gene expression of all cells from the two slices, and apply Principle Component Analysis (PCA) to reduce the dimensionality of gene expressions to 30. We take the Euclidean distance between the gene expression in the PCA space as the cost matrix \(\bm{C}\). We take the Euclidean distance between the 2D coordinate of each cell within each slice as the intra-domain cost matrices \(\bm{A}\) and \(\bm{B}\).

Fig. 9 visualizes the two slices in this dataset, with each cell annotated with a cell type from the original publication.

### Experiment Settings

We compare with the unbalanced low-rank optimal transport algorithm of Svetbon et al. (2023), the backbone of the spatial transcriptomics alignment method moscot Klein et al. (2023), which was shown by Svetbon et al. (2023); Klein et al. (2023) to achieve state-of-the art performance on spatial transcriptomics alignment. We use Svetbon et al. (2023) to solve three unbalanced problems for spatial transcriptomics alignment: the unbalanced Wasserstein problem of (48) (result denoted as LOT-U-W), the unbalanced GW problem of (49) (LOT-U-GW), and the unbalanced FGW problem with a convex combination of the previous two costs (LOT-U-FGW). We use FRLC to solve the same three unbalanced problems (results denoted as FRLC-U-W, FRLC-U-GW, FRLC-U-FGW). Since the two slices contain \(>\) 30,000 cells and \(>\) 50,000 cells respectively, a full-rank solution is not feasible, hence we solve for low-rank solutions with the rank validated. We also solve semi-relaxed versions of Wasserstein (result denoted as FRLC-SR-W), GW (FRLC-SR-GW), FGW (FRLC-SR-FGW) problems using our FRLC algorithm, as well as using a particular setting of LOT-U that is equivalent to semi-relaxed solver (results denoted as LOT-SR-W, LOT-SR-GW, LOT-SR-FGW).

We perform extensive grid search to find the best hyperparameter combinations for each method and each problem. The grid of hyperparameters searched for each method is reported in Table. 6. The best performing hyperparameter combination for each method is reported in Table. 7 along with the performance on the validation genes. We pick the best hyperparameters using the Spearman correlation on the gene expression prediction task for 10 validation genes: Ckb, Fabp7, My14, Tnnt2, Apoa2, Hba-x, Tubb3, Epha7, Ldha, Col1a2, which are marker genes of various cell types as well. We report the performance of the alignment computed by each method using the Spearman correlation on the gene expression prediction task for 10 test genes, as well as the ARI and AMI on the cell type prediction task. Fig. 10 visualizes the ground truth cell type classification versus the classification predicted by our method FRLC-SR-W.

### Runtime

We report the runtime and OT cost of FRLC and LOT Svetbon et al. (2021) on this dataset (mouse embryo E11.5-12.5) as well as two other datasets (E9.5-10.5, E10.5-11.5) from Chen et al. (2022) in Table 4. For all three datasets, FRLC achieves a better OT cost in a shorter time.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Dataset** & **LOT (seconds)** & **FRLC (seconds)** & **LOT (OT Cost)** & **FRLC (OT Cost)** \\ \hline Mouse embryo (E9.5–10.5) & 2.545 & 1.112 & 0.440 & 0.385 \\ Mouse embryo (E10.5–11.5) & 4.209 & 1.190 & 0.371 & 0.344 \\ Mouse embryo (E11.5–12.5) & 8.667 & 1.889 & 0.478 & 0.439 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of methods on Stereo-Seq mouse embryo spatial transcriptomics datasets using GPU and default settings: min_iter=10, max_iter=100, rank \(r=50\).

Figure 9: Visualization of the E11.5 and E12.5 mouse embryos, with each cell colored by the cell type annotated by Chen et al. (2022).

\(n^{\rm th}\)-roots of unity

### Problem Statement

To test if FRLC can effectively coarse-grain transport between two datasets with obvious cluster structure, we generate a pair of two-dimensional datasets \(\mathbf{Z}^{(1)}\) and \(\mathbf{Z}^{(2)}\), with ten and five clusters respectively. We run FRLC with \(\bm{T}\in\mathbb{R}_{+}^{10\times 5}\) to see if the barycentric projections for the LC factorization (Definition 4.1) can recover the cluster structure. These projections induce 10 and 5 barycenters for the first and second dataset, and are defined by

\[\mathbf{Y}^{(1)}:=\mathrm{diag}(1/\bm{g}_{\partial})\bm{Q}^{\rm T }\mathbf{Z}^{(1)},\quad\mathbf{Y}^{(2)}:=\mathrm{diag}(1/\bm{g}_{R})\bm{R}^{ \rm T}\mathbf{Z}^{(2)}.\]

We examine, visually, whether these barycenters are good representatives of the clusters in each dataset, and whether the latent coupling depicts a reasonable transfer of mass between barycenters. We also run FRLC with \(\bm{T}\in\mathbb{R}_{+}^{10\times 10}\) and plot the barycenters from the resulting factorization for comparison. As discussed in SS 4.2, the barycentric projections defined above, and in Definition 4.1 can be applied to factored couplings Forrow et al. (2019); Scethon et al. (2021), yielding projections of the form:

\[\mathbf{Y}^{(1)}:=\mathrm{diag}(1/\bm{g})\bm{Q}^{\rm T}\mathbf{Z} ^{(1)},\quad\mathbf{Y}^{(2)}:=\mathrm{diag}(1/\bm{g})\bm{R}^{\rm T}\mathbf{Z} ^{(2)}.\]

Thus, we also ran the method of Scethon et al. (2021) on this data, called LOT throughout the experiment, with \(\bm{g}\in(\mathbb{R}_{+}^{\star})^{5}\) and \(\bm{g}\in(\mathbb{R}_{+}^{\ast})^{10}\), plotting its barycenters in each case along with the diagonal latent coupling \(\mathrm{diag}(\bm{g})\).

### Dataset and Preprocessing

We instantiate a pair of two-dimensional datasets \(\mathbf{Z}^{(1)}\) and \(\mathbf{Z}^{(2)}\) as follows. Let \(\mathcal{U}_{n}\) denote the \(n^{\rm th}\)-roots of unity:

\[\mathcal{U}_{n}:=\{e^{2\pi ik/n}:k=0,\dots,n\}.\]

These complex numbers can be equivalently expressed as ordered pairs on the unit circle:

\[\mathsf{c}_{k}=\begin{pmatrix}\mathrm{Re}(e^{2\pi ik/n})\\ \mathrm{Im}(e^{2\pi ik/n})\end{pmatrix}=\begin{pmatrix}\cos(2\pi k/n))\\ \sin(2\pi k/n)\end{pmatrix}.\]

We consider \(n\) uniformly weighted mixtures of Gaussians, where for each sample \(X\), we first sample a root of unity uniformly,

\[k\sim\mathrm{Uniform}(n),\]

and conditionally on \(k\), we sample \(X\) from an isotropic Gaussian centered at this root of unity

\[X\sim\mathcal{N}(\mathsf{c}_{k},\sigma^{2}\mathrm{Id}_{2}),\]

using a standard deviation \(\sigma=0.1\). Samples are generated with the make_blobs function from sklearn.datasets. We generate two datasets in this way, \(\mathbf{Z}^{(1)}\) for \(n=10\), and \(\mathbf{Z}^{(2)}\) for \(n=5\). To generate dataset \(\mathbf{Z}^{(1)}\), we first homogeneously scale the roots of unity to lie on a circle of radius \(3\) before sampling, so that the two datasets do not overlap but still use the same standard deviation. We do not scale the centers used for \(\bm{Z}^{(2)}\), so they all lie on the unit circle. We generate \(\mathbf{Z}^{(1)}=\{\mathbf{z}_{1}^{(1)},\dots,\mathbf{z}_{1000}^{(1)}\}\) and \(\mathbf{Z}^{(2)}=\{\mathbf{z}_{1}^{(2)},\dots,\mathbf{z}_{1000}^{(2)}\}\) using \(1000\) samples each and form the empirical measures

\[\mu:=\sum_{i=1}^{1000}\frac{1}{1000}\delta_{\mathbf{z}_{i}^{(1)} },\quad\nu:=\sum_{j=1}^{100}\frac{1}{1000}\delta_{\mathbf{z}_{j}^{(2)}}.\]

supported on \(\mathbf{Z}^{(1)}\) and \(\mathbf{Z}^{(2)}\), corresponding to uniform probability vectors \(\bm{a},\bm{b}\in\Delta_{1000}\).

### Experiment Settings

We used default hyperparameter settings for both methods. In particular, FRLC sets \(\tau=75\) and \(\gamma=90\), with a maximum of 200 iterations and a minimum of 25 iterations, subject to the stopping criterion \(\Delta\) given in (10). The LOT default settings are \(\gamma=10\) (there is no hyperparameter analogous to \(\tau\) in LOT). Both methods use a random initialization and were run on CPU.

Discussion of differences between FRLC and existing low-rank optimal transport algorithms

We provide an extensive comparison of FRLC against the parametrization and objective of Sectbon et al. (2021) in Appendix A. As Latent-OT of Lin et al. (2021) is an extension of the \(k\)-Wasserstein barycenter problem, it has a distinct objective and thus performs worse on primal OT cost (Table 5), making a direct experimental comparison on primal cost minimization only appropriate relative to the works of Sectbon et al. (2021, 2023, 2022). This stated, we still list a number of distinctions between FRLC and Latent-OT - noting that most differences between FRLC and Forrow et al. (2019) transfer from this discussion since Lin et al. (2021) extends the \(k\)-Barycenter problem of Forrow et al. (2019). (1) Lin et al. optimize two sets of variables: sub-couplings \((\bm{Q},\bm{R},\bm{T})\) and anchor points \((z^{x},\,z^{y})\) on which the sub-couplings depend. FRLC only has \((\bm{Q},\bm{R},\bm{T})\) as variables of the optimization. (2) Cost matrices used in Lin et al. (2021) are built from distances between each dataset and its representative anchor points for \(\bm{Q},\bm{R}\), or the distances between the two sets of anchor points for \(\bm{T}\). In contrast, ground costs used in FRLC to update \((\bm{Q},\bm{R},\bm{T})\) are always derived from the distance matrix \(\bm{C}\) in the Wasserstein objective \(\langle\bm{C},\bm{P}\rangle_{F}\). Specifically, the cost matrices used by Lin et al. are:

\[[\bm{C_{Q}}]_{ik}=\|x_{i}-z_{k}^{x}\|_{2}^{2},\quad[\bm{C_{R}}]_{j\ell}=\|y_{ j}-z_{\ell}^{y}\|_{2}^{2},\quad[\bm{C_{T}}]_{k\ell}=\|z_{k}^{x}-z_{\ell}^{y}\|_{2}^ {2},\]

optionally using a Wasserstein distance for the entries of \(\bm{C_{T}}\). (3) FRLC costs are given in the exponents of the Gibbs kernels written above and below Equation 9. These are derived directly from the rank-\(r\) Wasserstein problem \(\min_{\bm{P}\in\Pi_{r}(a,b)}\langle\bm{C},\bm{P}\rangle_{F}\) and differ substantially from those of the proxy objective in Lin et al. (2021); Forrow et al. (2019). (4) The different objectives and variables lead to very different algorithms: Lin et al. alternate updates to the sub-couplings \((\bm{Q},\bm{R},\bm{T})\) using Dykstra, with updates to the latent anchor points \((z^{x},z^{y})\) using first-order conditions. In contrast, FRLC alternates semi-relaxed OT to update \((\bm{Q},\bm{R})\) and balanced OT to update \(\bm{T}\). (5) Because FRLC does not require anchor points to define costs, FRLC can handle cost matrices which are not simple functions of distance. For example, if \(\bm{C}_{ij}\) is the price of transporting good \(i\) to warehouse \(j\) one may not be able to re-evaluate a price \(c(x_{i},z_{k}^{x})\) between \(x_{i}\) and latent anchor \(z_{k}^{x}\). In such situations, while finding a low-rank plan may make sense (e.g. to approximate an assignment for a massive dataset), an "anchor" may not have clear definition in the setting of general cost matrices. (6) The Lin et al. objective is only a proxy for a Wasserstein-type loss, and Lin et al. do not explore extensions to Gromov-Wasserstein (GW), or Fused GW, which FRLC readily generalizes to. A summary of the existing low-rank OT algorithms and key distinctions between them is given in Table 1.

For completeness, we offer a compare against the work Latent OT Lin et al. (2021), which solves a variation of the \(k\)-Wasserstein barycenter problem. As discussed, while their factorization is similar, their problem is distinct from FRLC  as it does not solve the primal OT problem for general cost. We report the cost obtained by FRLC and by Latent OT on various simulated datasets in Table 5.

## Appendix P Limitations

Our method introduces an additional hyperparameter \(\tau\) relative to previous approaches Sectbon et al. (2021), controlling the strength of the KL penalty on the inner marginals when updating \(\bm{Q}\) and \(\bm{R}\).

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Dataset** & **OT-cost (FRLC)** & **OT-cost (Lin et al.)** \\ \hline \(\bm{5}^{\text{th}}\) and \(\bm{10}^{\text{th}}\) roots of unity & **1.174** & 2.124 \\ (rank \(r_{1},r_{2}=5,10\)) & **1.174** & 2.124 \\ \hline Two-moons and 8-Gaussians (rank \(r=20\)) & **2.716** & 4.291 \\ \hline
2D Gaussian mixture (rank \(r=20\)) & **0.552** & 0.922 \\ \hline
10D Gaussian mixture (rank \(r=20\)) & **1.038** & 1.298 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison against Lin et al. (2021) in primal OT-cost \(\langle\bm{C},\bm{P}\rangle_{F}\).

Empirically, we found FRLC to be robust to different choices of \(\tau\), but applying the method optimally requires this additional hyperparamter in any grid-search.

We also note that the non-asymptotic criterion \(\Delta(\cdot,\cdot)\) is weak relative to stronger notions of convergence, and that often users might prefer to simply run the method up to some number of maximal iterations by setting the parameter for whether \(\Delta(\cdot,\cdot)\) is used to False. The \(\mathrm{W}\) optimization empirically converges to minima smoothly, so for the most part there is not much of a need for \(\Delta\) except for early stopping. We recommend that users plot the loss over iterations and use it to set the tolerance parameter tol, and the minimum and maximum iteration parameters for the time-being. The needs for these parameters might vary widely-the minimum number of iterations should be very low (around 5) for simple datasets and substantially higher for high-dimensional, structured ones.

Although we demonstrate strong performance already, there is massive room for improvement as our implementation is preliminary and not at the level of a high-performance library like ott-jax. We use lightweight vanilla implementations of Sinkhorn as a sub-routine, not taking advantage of the momentum-based techniques which could accelerate it massively. Thus, one can imagine that the potential scalability and speed of this method could be much higher than reported in this document.

## Appendix Q Broader impacts

FRLC is general enough to be used modularly within any ML algorithm using OT as a subroutine to help with scalability. We also note that the LC factorization is similar to a PCA in the context of OT, yielding an optimal low-rank coupling with an interpretable latent coupling factor.

\begin{table}
\begin{tabular}{c c} \hline \hline Hyperparameter & Values \\ \hline rank (Both) & 50, 100, 200 \\ \(\tau\) (Ours) & 30, 50, 100 \\ \(\tau\) (LOT-U) & 0.99, 0.9, 0.7 \\ \(\epsilon\) (LOT-U) & 0.001, 0.01, 0.1 \\ \end{tabular}
\end{table}
Table 6: Hyperparameter grid considered in hyperparameter search for validation. Scetbon et al. (2023); Klein et al. (2023) scales \(\tau^{\prime}=\frac{\tau}{1-\tau}\) and their \(\tau^{\prime}\) are in the same range.

Figure 10: Ground truth and the predicted cell type classification of the E12.5 embryo.

Figure 11: (a) LC-projection barycenters aligned with FRLC latent-coupling \(\bm{T}\) on (a) two moons and eight Gaussians (\(r=30\)), (b) LC-projection barycenters aligned with \(\mathrm{diag}(\bm{g})\) from Svetbon et al. (2021) (\(r=30\)), (c) the checkerboard dataset with FRLC latent coupling aligned barycenters (\(r=12\)), and (d) with diagonal alignment Svetbon et al. (2021) (\(r=12\)). We show in A.1 that the output of FRLC can be diagonalized to the factorization of Forrow et al. (2019) (Figure 12).

Figure 12: As discussed in A.1, one may recover the factorization of Forrow et al. (2019) as a sub-case of the LC-factorization. Shown is the factorization found by diagonalizing the output of FRLC from \((\bm{Q},\bm{R},\bm{T})\mapsto(\bm{Q}^{\prime},\bm{R},\mathrm{diag}(\bm{g}_{R}))\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction exactly describe the background preceding this paper and placing it into context, and exactly describe the contribution and scope of the paper to the field. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We describe the limitations of our method in Section P. These include an additional hyperparameter that FRLC introduces relative to previous work, and the substantial room for code optimization. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All theoretical results state their assumptions clearly and all propositions are followed by thorough line-by-line proofs with careful justifications made for each step. They have checked over by all of the authors. All proofs are provided in the supplement with detail and are referenced in the main body where appropriate. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all code used for generating the results of our paper. This not only includes the source code for the optimization, but the experimental code used for benchmarking. The algorithm is implemented exactly as described in the paper, and reviewers can freely consult the code we provide to them. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

* We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
* **Open access to data and code*
* Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We will upload all code, and all code required to generate the synthetic data experiments used. Any real data experiments using especially large-scale data have publicly available and easily accessible datasets which we provide references for. We provide comprehensive descriptions of how the data was pre-processed, and provide experimental code which can be followed easily. Guidelines:
* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
* **Experimental Setting/Details*
* Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We introduce all hyper-parameters with rigorous justification of their utility. The values of the default hyper-parameters are accessible in the code we upload and any experiment which does not use the default hyperparameter (e.g. for a validation search) has the full table provided and the code used for the experiment. Guidelines:
* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
* **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [Yes] Justification: The optimization is deterministic, so generally there's no need for reporting statistical significance. One small exception is that one of our proposed initializations is randomized, and for experiments which use it we do include \(\pm 1\sigma\) error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We indicate when experiments are run on CPU versus GPU in the experimental details. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This research was conducted ethnically and conforms to the guidelines of the code of ethics. We do not anticipate any major negative societal impact to result from this work on optimal transport. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We address broader impacts in Section Q: FRLC can be used for scaling an interpretability in any ML method using OT as a subroutine. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not generate any new datasets other than a toy dataset of Gaussians centered at nth roots of unity, which has no societal impact. We do not believe our optimal transport work has significant risk of misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes] Justification: All data that we use and all work that we build on are cited and credited heavily. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We provide no new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper involves no research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.