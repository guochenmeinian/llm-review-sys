# Factorized Contrastive Learning:

Going Beyond Multi-view Redundancy

 Paul Pu Liang\({}^{1}\)1, Zihao Deng\({}^{2}\)2, Martin Q. Ma\({}^{1}\)1

**James Zou\({}^{3}\), Louis-Philippe Morency\({}^{1}\), Ruslan Salakhutdinov\({}^{1}\)**

\({}^{1}\)Carnegie Mellon University, \({}^{2}\)University of Pennsylvania, \({}^{3}\)Stanford University

pliang@cs.cmu.edu, zihaoden@cs.cmu.edu, qianlim@cs.cmu.edu

Footnote 1: First three authors contributed equally.

###### Abstract

In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of _multi-view redundancy_ - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks.

## 1 Introduction

Learning representations from different modalities is a central paradigm in machine learning [48]. Today, a popular learning method is to first pre-train general representations on unlabeled multimodal data before fine-tuning on task-specific labels [10, 40, 47, 48, 50]. These current multimodal pre-training approaches have largely been inherited from prior work in multi-view learning [13, 58] that exploit a critical assumption of _multi-view redundancy_: the property that shared information between modalities is almost exactly what is relevant for downstream tasks [70, 73, 76]. When this assumption holds, approaches based on contrastive pre-training to capture shared information [13, 41, 61, 72], followed by fine-tuning to keep task-relevant shared information [76], have seen successful applications in learning from images and captions [61], video and audio [3], speech and transcribed text [58], and instructions and actions [21]. However, our paper studies two fundamental limitations in the application of contrastive learning (CL) to broader real-world multimodal settings (see Figure 1 for a visual depiction and experimental results showing the performance drop of CL):

1. **Low _shared_ information** relevant to tasks: There exists a wide range of multimodal tasks involving small amounts of shared information, such as between cartoon images and figurative captions (i.e., not literal but metaphoric or idiomatic descriptions of the images [52, 88]). In these situations, standard multimodal CL will only receive a small percentage of information from the learned representations and struggle to learn the desired task-relevant information.
2. **High _unique_ information relevant to tasks: Many real-world modalities can provide unique information not present in other modalities. Examples include healthcare with medical sensors or robotics with force sensors [45, 49]. Standard CL will discard task-relevant unique information, leading to poor downstream performance.

In light of these limitations, how can we design suitable multimodal learning objectives that work beyond multi-view redundancy? In this paper, starting from the first principles in information theory, we provide formal definitions of shared and unique information via conditional mutual information and propose an approach, Factorized Contrastive Learning (FactorCL for short), to learn these multimodal representations beyond multi-view redundancy using three key ideas. The first idea is to explicitly _factorize_ shared and unique representations. The second idea is to _capture task-relevant_ information via maximizing lower bounds on MI and _remove task-irrelevant_ information via minimizing upper bounds on MI, resulting in representations with sufficient and necessary information content. Finally, a notion of task relevance without explicit labels in the self-supervised setting is achieved by leveraging _multimodal augmentations_. Experimentally, we evaluate the effectiveness of FactorCL on a suite of synthetic datasets and large-scale real-world multimodal benchmarks involving images and figurative language [88], prediction of human sentiment [91], emotions [93], humor [27], and sarcasm [12], as well as patient disease and mortality prediction from health indicators and sensor readings [38], achieving new state-of-the-art performance on six datasets. Overall, we summarize our key technical contributions here:

1. A new analysis of contrastive learning performance showing that standard multimodal CL fails to capture task-relevant unique information under low shared or high unique information cases.
2. A new contrastive learning algorithm called FactorCL: 1. FactorCL factorizes task-relevant information into shared and unique information, expanding contrastive learning to better handle low shared or high unique information. 2. FactorCL optimizes shared and unique information separately, by removing task-irrelevant information via MI upper bounds and capturing task-relevant information via lower bounds, yielding optimal task-relevant representations. 3. FactorCL leverages multimodal augmentations to approximate task-relevant information, enabling self-supervised learning from our proposed FactorCL.

## 2 Analysis of Multi-view Contrastive Learning

We begin by formalizing definitions of four types of information: shared, unique, task-relevant, and task-irrelevant information in multimodal data. To formalize the learning setting, we assume there exist two modalities expressed as random variables \(X_{1}\) and \(X_{2}\) with outcomes \(x_{1}\) and \(x_{2}\), and a task with the random variable \(Y\) and outcome \(y\). We denote \(X_{-i}\) as the other modality where appropriate.

**Shared and unique information**: We formalize shared and unique information by decomposing the total multimodal information \(I(X_{1},X_{2};Y)\) into three conditional mutual information (MI) terms:

\[I(X_{1},X_{2};Y)=\underbrace{I(X_{1};X_{2};Y)}_{S=\text{shared}}+\underbrace{I (X_{1};Y|X_{2})}_{U_{1}=\text{unigeness in }X_{1}}+\underbrace{I(X_{2};Y|X_{1})}_{U_{2}=\text{unigeness in }X_{2}},\] (1)

where \(I(X_{1},X_{2};Y)=\int p(x_{1},x_{2},y)\log\frac{p(x_{1},x_{2},y)}{p(x_{1},x_{ 2}p(y))}dx_{1}dx_{2}dy\) is the total MI between the joint random variable \(X_{1},X_{2}\) and the task \(Y\), \(S=I(X_{1};X_{2};Y)=I(X_{1};X_{2})-I(X_{1};X_{2}|Y)=\int p(x_{1},x_{2})\log\frac {p(x_{1},x_{2})}{p(x_{1})p(x_{2})}dx_{1}dx_{2}-I(X_{1};X_{2}|Y)\) is the task-relevant shared in

Figure 1: **Left**: We define \(S=I(X_{1};X_{2};Y)\) as task-relevant shared information and \(U_{1}=I(X_{1};Y|X_{2})\), \(U_{2}=I(X_{2};Y|X_{1})\) as task-relevant unique information. **Right**: On controllable datasets with varying ratios of \(S\), \(U_{1}\), and \(U_{2}\), standard CL captures \(S\) but struggles when there is more \(U_{1}\) and \(U_{2}\). Our FactorCL approach maintains best performance, whereas SimCLR [13] and SupCon [41] see performance drops as unique information increases, and Cross+Self [33; 36; 44; 89] recovers in fully unique settings but suffers at other ratios.

formation, \(I(X_{1};X_{2}|Y)=\int p(x_{1},x_{2},y)\log\frac{p(x_{1},x_{2}|y)}{p(x_{1}|y)p(x_{ 2}|y)}dx_{1}dx_{2}dy\) is the task-irrelevant shared information, and \(U_{1}=I(X_{1};Y|X_{2})\), \(U_{2}=I(X_{2};Y|X_{1})\) denote unique task-relevant information.

**Limitations of CL**: Current approaches for CL maximize mutual information \(I(X_{1};X_{2})\) (and subsequently task-relevant shared information \(I(X_{1};X_{2};Y)\) during supervised fine-tuning), without modeling unique information. These methods generally learn a pair of representations [73, 76],

\[Z_{1}=\operatorname*{arg\,max}_{Z_{1}=f_{\theta}(X_{1})}I(Z_{1};X_{2}),\quad Z _{2}=\operatorname*{arg\,max}_{Z_{2}:=f_{\theta}(X_{2})}I(X_{1};Z_{2}).\] (2)

For example, \(Z_{1}\) could encode images \(X_{1}\) and \(Z_{2}\) encodes text \(X_{2}\) via maximizing a lower bound on \(I(X_{1};X_{2})\) using the NCE objective [58]. The NCE objective falls into a broader class of contrastive learning methods [13, 15, 28, 41, 61] that model the ratio between joint densities \(p(x_{1},x_{2})\) and product of marginal densities \(p(x_{1})p(x_{2})\) using positive and negative samples [57, 59, 60, 79, 84] or probabilistic classifiers [55, 77]. It has been shown that contrastive learning works well under the assumption of multi-view redundancy [4, 31, 70, 71, 76]:

**Definition 1**.: _(Multi-view redundancy) \(\exists\epsilon>0\) such that \(I(X_{1};Y|X_{2})\leq\epsilon\) and \(I(X_{2};Y|X_{1})\leq\epsilon\)._

In other words, the task-relevant information in data is mostly shared across both views and the unique information is at most a small \(\epsilon\). From a representation perspective, Tian et al. [72] further introduces the assumption that the optimal representation is minimal and sufficient, where all learned task-relevant information is shared information: \(I(Z_{1};Y|X_{2})=I(Z_{2};Y|X_{1})=0\). While the multi-view redundancy is certainly true for particular types of multimodal distributions, it crucially ignores settings that display _multi-view non-redundancy_ and unique information can be important, such as when health indicators, medical sensors, and robotic visual or force sensors each provide unique information not present in other modalities [45, 49].

**Definition 2**.: _(Multi-view non-redundancy) \(\exists\epsilon>0\) such that \(I(X_{1};Y|X_{2})>\epsilon\) or \(I(X_{2};Y|X_{1})>\epsilon\)._

Under multi-view non-redundancy, we show that standard CL only receives a weak training signal since it can only maximize a lower bound on shared information \(I(X_{1};X_{2})\), and struggles to learn task-relevant unique information. We formalize this intuition with the following statement:

**Theorem 1**.: _(Suboptimality of standard CL) When there is multi-view non-redundancy as in Definition 2, given optimal representations \(\{Z_{1},Z_{2}\}\) that satisfy Eq.(2 and \(I(Z_{1};Y|X_{2})=I(Z_{2};Y|X_{1})=0\)[72], we have that_

\[I(Z_{1},Z_{2};Y)=I(X_{1},X_{2};Y)-I(X_{1};Y|X_{2})-I(X_{2};Y|X_{1})=I(X_{1};X_{ 2})-I(X_{1};X_{2}|Y)<I(X_{1},X_{2};Y).\] (3)

_Correspondingly, the Bayes error rate \(P_{e}(Z_{1},Z_{2}):=1-\mathbb{E}_{p(z_{1},z_{2})}\left[\max_{y\in Y}P\left( \hat{Y}=y\mid z_{1},z_{2}\right)\right]\) of contrastive representations \(\{Z_{1},Z_{2}\}\) for a downstream task \(Y\) is given by:_

\[P_{e} \leq 1-\exp\left[I(X_{1},X_{2};Y)-I(X_{1};Y|X_{2})-I(X_{2};Y|X_{1}) -H(Y)\right]\] (4) \[=1-\exp\left[I(X_{1};X_{2};Y)-H(Y)\right]\] (5)

We include proofs and a detailed discussion of the assumptions in Appendix B. Based on Eq.(3), \(I(Z_{1},Z_{2};Y)\) decreases with higher task-relevant unique information \(I(X_{1};Y|X_{2})\) and \(I(X_{2};Y|X_{1})\); we call this the difference \(I(X_{1},X_{2};Y)-I(Z_{1},Z_{2};Y)\) the _uniqueness gap_. The uniqueness gap measures the loss in task-relevant information between the input and encoded representation: as task-relevant unique information grows, the uniqueness gap increases. In addition, \(I(Z_{1},Z_{2};Y)\) also drops with lower \(I(X_{1};X_{2})\) (i.e., two modalities sharing little information to begin with), or with higher \(I(X_{1};X_{2}|Y)\) (i.e., when the shared information is mostly task-irrelevant). Similarly, in Eq.(5), the Bayes error rate of using \(\{Z_{1},Z_{2}\}\) for prediction is directly related to the task-relevant information in \(\{Z_{1},Z_{2}\}\): error on the downstream task increases with higher unique information and lower shared information.

## 3 Factorized Contrastive Learning

We now present a suite of new CL objectives that alleviate the challenges above and work at all ranges of shared and unique information. At a high level, we aim to learn a set of factorized representations \(Z_{S_{1}},Z_{S_{2}},Z_{U_{1}},Z_{U_{2}}\) representing task-relevant information in \(X_{1}\) shared with \(X_{2}\), in \(X_{2}\) shared with \(X_{1}\), unique to \(X_{1}\), and unique to \(X_{2}\) respectively. As common in practice [61, 72], we define neural networks \(f_{\theta}\) with trainable parameters \(\theta\) to extract representations from inputs \(X_{1}\) and \(X_{2}\). Learning these parameters requires optimizing differentiable and scalable training objectives to capture task-relevant shared and unique information (see overview in Figure 2):

\[Z_{S_{1}} =\operatorname*{arg\,max}_{Z_{1}\in f_{\theta}(X_{1})}I(Z_{1};X_{2 };Y), Z_{S_{2}} =\operatorname*{arg\,max}_{Z_{2}\in f_{\theta}(X_{2})}I(Z_{2};X_{1} ;Y),\] (6) \[Z_{U_{1}} =\operatorname*{arg\,max}_{Z_{1}\in f_{\theta}(X_{1})}I(Z_{1};Y|X _{2}), Z_{U_{2}} =\operatorname*{arg\,max}_{Z_{2}\in f_{\theta}(X_{2})}I(Z_{2};Y|X_ {1}).\] (7)

where \(I(Z_{1};X_{2};Y)=I(Z_{1};X_{2})-I(Z_{1};X_{2}|Y)\) is the shared information and \(I(Z_{2};X_{1};Y)=I(Z_{2};X_{2})-I(Z_{2};X_{1}|Y)\) is the unique information. One important characteristic of our framework is that when unique information is zero: \(I(X_{1};Y|X_{2})=0\) and \(I(X_{2};Y|X_{1})=0\), or all shared information is task-relevant: \(I(X_{1};X_{2};Y)=I(X_{1};X_{2})\), our framework recovers standard CL as in Eq.(2). However, as we have previously indicated and will show empirically, these assumptions can easily be violated, and our framework enlarges Eq.(2) to cases where unique information is present.

The learned \(Z\)s can then be used as input to a linear classifier and fine-tuned to predict the label for multimodal classification or retrieval tasks. However, the shared and unique MI terms above are often intractable in practice. In the next section, we will build up our method step by step, eventually showing that each term in Eqs.(6- 7) can be approximated as follows:

\[S =I(X_{1};X_{2};Y)\geq I_{\text{NCE}}(X_{1};X_{2})-I_{\text{NCE- CLUB}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime})\] (8) \[U_{i} =I(X_{i};Y|X_{-i})\geq I_{\text{NCE}}(X_{i};X_{i}^{\prime})-I_{ \text{NCE-CLUB}}(X_{1};X_{2})+I_{\text{NCE}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^ {\prime})\] (9)

where \(I_{\text{NCE}}\) and \(I_{\text{NCE-CLUB}}\) are scalable contrastive estimators (Section 3.1) and \(X_{1}^{\prime},X_{2}^{\prime}\) are suitable data augmentations (Section 3.2) on each modality. Overall, these equations can be interpreted as both positive and negative signals to learn representations for \(S\) and \(U\). For shared information \(S\), the estimator maximizes task-relevant shared information via \(I_{\text{NCE}}(X_{1};X_{2})\) while removing task-irrelevant shared information via a novel upper bound \(-I_{\text{NCE-CLUB}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime})\). For unique information \(U_{i}\), we capture task-relevant uniqueness via \(+I_{\text{NCE}}(X_{i};X_{i}^{\prime})\) while non-unique information is removed via \(-(I_{\text{NCE-CLUB}}(X_{1};X_{2})-I_{\text{NCE}}(X_{1};X_{2}|X_{1}^{\prime},X _{2}^{\prime}))\). In the following sections, we derive this final objective step-by-step: (1) approximating the MI objectives in \(S\) and \(U\) with CL estimators, (2) relaxing the dependence on labels \(Y\) with self-supervised data augmentations, finally (3) discussing overall training and implementation details of end-to-end self-supervised learning.

### Supervised FactorCL with shared and unique information

To capture shared and unique information via an objective function, we will need to maximize lower bounds for all terms with a positive sign in Eq.(8) and (9) (\(I\left(X_{1};X_{2}\right),I\left(X_{i};Y\right),I\left(X_{1};X_{2}|Y\right)\)) and minimize upper bounds for all terms with a negative sign (\(I\left(X_{1};X_{2}\right),I\left(X_{1};X_{2}|Y\right)\)). Our first theorem derives general lower and upper bounds for MI terms as variants of contrastive estimation:

Figure 2: FactorCL: We propose a self-supervised CL method to learn _factorized_ representations \(Z_{S_{1}}\), \(Z_{S_{2}}\), \(Z_{U_{1}}\), and \(Z_{U_{2}}\) to capture task-relevant information shared in both \(X_{1}\) and \(X_{2}\), unique to \(X_{1}\), and unique to \(X_{2}\). By starting with information-theoretic first principles of shared and unique information, we design contrastive estimators to both _capture task-relevant_ and _remove task-irrelevant_ information, where a notion of task-relevance without explicit labels is afforded by a new definition of _multimodal augmentations_\(X_{1}^{\prime},X_{2}^{\prime}\). Lower bounds are in green and upper bounds are in red.

**Theorem 2**.: _(Contrastive estimators for \(I(X_{1};X_{2})\)) Defining the NCE and NCE-CLUB estimators,_

\[I_{\text{NCE}}(X_{1};X_{2}) =\mathbb{E}_{\begin{subarray}{c}x_{1},x_{2}^{+}\text{-}p(x_{1},x_{ 2})\\ x_{2}^{-}\text{-}p(x_{2})\end{subarray}}\left[\log\frac{\exp f(x_{1},x_{2}^{+} )}{\sum_{k}\exp f(x_{1},x_{2}^{-})}\right]\] (10) \[I_{\text{NCE-CLUB}}(X_{1};X_{2}) =\mathbb{E}_{\begin{subarray}{c}x_{1},x_{2}^{+}\text{-}p(x_{1},x_{ 2})\end{subarray}}\left[f^{*}(x_{1},x_{2}^{+})\right]-\mathbb{E}_{\begin{subarray} {c}x_{1}\text{-}p(x_{1})\\ x_{2}^{-}\text{-}p(x_{2})\end{subarray}}\left[f^{*}(x_{1},x_{2}^{-})\right]\] (11)

_where \(f^{*}(x_{1},x_{2})\) is the optimal critic from \(I_{\text{NCE}}\) plugged into the \(I_{\text{CLUB}}\) objective [16]. We call the proposed plug-in objective Eq.(11) \(I_{\text{NCE-CLUB}}\), and obtain lower and upper bounds on \(I(X_{1};X_{2})\):_

\[I_{\text{NCE}}(X_{1};X_{2})\leq I(X_{1};X_{2})\leq I_{\text{NCE-CLUB}}(X_{1}; X_{2}).\] (12)

Proof.: The lower bound \(I_{\text{NCE}}(X_{1};X_{2})\leq I(X_{1};X_{2})\) follows from Oord et al. [58]: optimizing the objective leads to an optimal critic [60]\(f^{*}=\log p(x_{1}|x_{2})+c(x_{1})\), with a deterministic function \(c(\cdot)\). Plugging optimal critic \(f^{*}\) into \(I_{\text{NCE-CLUB}}(X_{1};X_{2})\) cancels out the \(c(x_{1})\) term and yields \(I_{\text{NCE-CLUB}}(X_{1};X_{2})\) and \(I(X_{1};X_{2})\leq I_{\text{NCE-CLUB}}\). We include a detailed proof in Appendix C.1. 

\(I_{\text{NCE-CLUB}}(X_{1};X_{2})\) gives a desired upper bound of \(I(X_{1};X_{2})\) "for free" while avoiding separately optimizing lower bound and upper bounds. In Figure 3, we show these two bounds in practice across two Gaussian distributions \(X_{1}\) and \(X_{2}\) with varying amounts of MI \(I(X_{1};X_{2})\). We use the second formulation of \(I_{\text{CLUB}}\)[16], which assumes \(p(x_{1}|x_{2})\) to be unknown. Our upper bound is empirically tighter (see Figure 3) and comes for "free" via jointly maximizing the lower bound \(I_{\text{NCE}}\). These lower and upper bounds can be seen as new contrastive objectives over positive and negative \((x_{1},x_{2})\) pairs, enabling a close integration with existing pre-training paradigms. Finally, we can similarly obtain bounds for the conditional MI \(I_{\text{NCE}}(X_{1};X_{2}|Y)\leq I(X_{1};X_{2}|Y)\leq I_{\text{NCE-CLUB}}(X_{1 };X_{2}|Y)\):

\[I_{\text{NCE}}(X_{1};X_{2}|Y) =\mathbb{E}_{p(y)}\left[\mathbb{E}_{\begin{subarray}{c}x_{1},x_{ 2}^{+}\text{-}p(x_{1},x_{2}|y)\\ x_{2}^{-}\text{-}p(x_{2}|y)\end{subarray}}\left[\log\frac{\exp f(x_{1},x_{2}^{ +},y)}{\sum_{k}\exp f(x_{1},x_{2}^{-},y)}\right]\right]\] (13) \[I_{\text{NCE-CLUB}}(X_{1};X_{2}|Y) =\mathbb{E}_{p(y)}\left[\mathbb{E}_{\begin{subarray}{c}x_{1},x_{ 2}^{+}\text{-}p(x_{1},x_{2}|y)\end{subarray}}\left[f^{*}(x_{1},x_{2}^{+},y) \right]-\mathbb{E}_{\begin{subarray}{c}x_{1}\text{-}p(x_{1}|y)\\ x_{2}^{-}\text{-}p(x_{2}|y)\end{subarray}}\left[f^{*}(x_{1},x_{2}^{-},y)\right]\right]\] (14)

These two bounds result in _conditional CL_ objectives [51; 74; 78] - they differ critically from standard CL methods since they capture task-irrelevant shared information that remains between \(X_{1}\) and \(X_{2}\) after observing \(Y\). This task-irrelevant shared information is removed by minimizing its upper bound. Note that \(f(x_{1},x_{2},y)\) here denotes a different function from \(f(x_{1},x_{2})\) in Eq.(10), as the general forms are different (taking in \(x_{1},x_{2}\) versus \(x_{1},x_{2},y\)). \(f(x_{1},x_{2},y)\) can be implemented in different ways, e.g., \(g([x_{1},y])^{T}h(x_{2})\) where \(g(),h()\) are trainable encoders and \([x_{1},y]\) denotes concatenation [69].

### Self-supervised FactorCL via multimodal augmentations

The derivations above bring about supervised CL objectives with access to \(Y\)[41]. For unsupervised CL [58; 72], we derive similar objectives without access to \(Y\) by leveraging semantic augmentations on each modality. Denote \(X^{\prime}\) as some augmentation of \(X\) (e.g., rotating, shifting, or cropping). Under

Figure 3: Estimated \(I_{\text{NCE}}\) lower bound [58] and our proposed upper bound \(I_{\text{NCE-CLUB}}\) on sample distributions with changing mutual information: our upper bound is tighter, more accurate, and more stable than \(I_{\text{CLUB}}\) upper bound [16], and also comes for ‘free’ via jointly estimating both lower and upper bounds simultaneously. We find that as dimension increases, the \(I_{\text{CLUB}}\) estimator collapses to zero and no longer tracks true MI.

[MISSING_PAGE_FAIL:6]

## 4 Experiments

We run comprehensive experiments on a suite of synthetic and large-scale real-world datasets with varying requirements of shared and unique task-relevant information, comparing our FactorCL method to key baselines:

1. SimCLR [(13)]: the straightforward method of cross-modal \((X_{1},X_{2})\) contrastive learning.
2. Cross+Self [(33; 36; 44; 64; 85; 89)]: captures a range of methods combining cross-modal \((X_{1},X_{2})\) CL with additional unimodal \((X_{i},X_{i}^{\prime})\) CL objectives. This category also includes other ways of preserving unique information, such as through (variational) autoencoder reconstructions [(81)].
3. Cross+Self+Fact [(86; 89)]: A factorized extension of Cross+Self, which is approximately done in prior work that adds separate (typically pre-trained) unimodal encoders for each modality.
4. SupCon [(41)], which learns \(I(X_{1};X_{2}|Y)\) using CL conditioned on \(Y\) from labeled data.

We also carefully ablate each component of our method and investigate factors, including training data size and choice of augmentations. The intermediate ablations that emerge include:

1. FactorCL-SUP: The supervised CL version which uses labels \(Y\) in Eqs.(13) and (14).

Figure 4: Standard vs. unique augmentations for the figurative language [(88)] dataset. After augmenting text modality \(X_{1}\) independently (same for both augmentation types), we illustrate their differences for image augmentation: unique augmentation on images should avoid removing information referred to by \(X_{1}\) (the text). The text mentions that the car is fast so unique augmentation for images should _not_ remove the highway pixels of the image which can suggest the car is fast.

2. FactorCL-SSL: The fully self-supervised version of our approach replacing \(Y\) with multimodal augmentations \(X_{1}^{\prime}\) and \(X_{2}^{\prime}\) to approximate the task.
3. OurCL-SUP: FactorCL-SUP but removing the factorization so only two features \(Z_{1}\) is optimized for both \(I(X_{1};X_{2};Y)\) and \(I(X_{1};Y|X_{2})\), \(Z_{2}\) optimized for both \(I(X_{1};X_{2};Y)\) and \(I(X_{2};Y|X_{1})\).
4. OurCL-SSL: FactorCL-SSL but also removing the factorization in the self-supervised setting.

The formulation of each ablation and implementation can be found in Appendix D.1.

### Controlled experiments on synthetic datasets

**Synthetic data generation**: We begin by generating data with controllable ratios of task-relevant shared and unique information. Starting with a set of latent vectors \(w_{1},w_{2},w_{s}\sim\mathcal{N}(0_{d},\Sigma_{d}^{2}),d=50\) representing information unique to \(X_{1},X_{2}\) and common to both respectively, the concatenated vector \(\left[w_{1},w_{s}\right]\) is transformed into high-dimensional \(x_{1}\) using a fixed transformation \(T_{1}\) and likewise \(\left[w_{2},w_{s}\right]\) to \(x_{2}\) via \(T_{2}\). The label \(y\) is generated as a function (with nonlinearity and noise) of varying ratios of \(w_{s}\), \(w_{1}\), and \(w_{2}\) to represent shared and unique task-relevant information.

**Results**: In Figure 1, we show our main result on synthetic data comparing FactorCL with existing CL baselines. FactorCL consistently maintains the best performance, whereas SimCLR [13] and SupCon [41] see performance drops as unique information increases. Cross+Self [33; 36; 44; 89] recovers in fully unique settings (x-axis\(=1.0\)) but suffers at other ratios.

**Representation probing information**: We run a probing experiment to compute how well different contrastive representations capture shared and unique information. In Table 1, for the \(Z_{i}\)'s learned by each method, we approximately compute \(I(Z_{i};w_{1})\), \(I(Z_{i};w_{2})\), and \(I(Z_{i};w_{s})\) with respect to ground truth generative variables \(w_{s}\), \(w_{1}\), and \(w_{2}\). As expected, existing methods such as SimCLR capture smaller amounts of unique information (roughly \(4\) bits in \(I(Z_{i};w_{1})\) and \(I(Z_{i};w_{2})\)), focusing instead on learning \(I(Z_{i};w_{s})\) (12 bits). Cross+self captures slightly larger \(I(Z_{i};w_{2})=4.26\), and SupCon with labeled data captures up to \(5\) bits of unique information. Our FactorCL approach captures \(7\) bits of unique information and maintains \(10\) bits of shared information, with total information captured higher than the other approaches. Furthermore, \(\{Z_{S_{1}},Z_{S_{2}}\}\) capture more information about \(w_{s}\), \(Z_{U_{1}}\) about \(w_{1}\), and \(Z_{U_{2}}\) about \(w_{2}\), indicating that factorization in our approach is successful.

### Self-supervised multimodal learning with low redundancy and high uniqueness

**Multimodal fusion datasets**: We use a large collection of real-world datasets provided in MultiBench [45], where we expect varying ratios of shared and unique information important for the task, to compare FactorCL with other CL baselines:

1. MIMIC [38]: mortality and disease prediction from \(36,212\) medical records (tabular patient data and medical time-series sensors from ICU).
2. MOSEI [93]: multimodal sentiment and emotion benchmark with \(23,000\) monologue videos.
3. MOSI [91]: multimodal sentiment analysis from \(2,199\) YouTube videos.
4. UR-FUNNY [27]: a dataset of humor detection from more than \(16,000\) TED talk videos.
5. MUSTARD [12]: a corpus of \(690\) videos for research in sarcasm detection from TV shows.
6. IRFL [88]: \(6,697\) matching images and figurative captions (rather than literal captions).

Together, these datasets cover seven different modalities from the healthcare, affective computing, and multimedia research areas and total more than \(84,000\) data points. For MIMIC with tabular and medical sensor inputs, we train self-supervised CL models on top of raw modality inputs. For IRFL with image and caption inputs, we start with a pretrained CLIP model [61] and perform continued pre-training to update CLIP weights with our FactorCL objectives, before linear classifier testing. For the remaining four video datasets, we train self-supervised CL models starting from standard pre-extracted text, video, and audio features [45]. Please refer to Appendix D.2 for experimental details. We release our code and models at https://github.com/pliang279/FactorCL.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c c c} \hline \hline Model & \multicolumn{2}{c|}{SimCLR} & \multicolumn{2}{c|}{Cross+self} & \multicolumn{2}{c|}{SupCon} & \multicolumn{4}{c}{FactorCL} \\ Representations & \(Z_{1}\) & \(Z_{2}\) & \(Z_{1}\) & \(Z_{2}\) & \(Z_{1}\) & \(Z_{2}\) & \(Z_{U_{1}}\) & \(Z_{U_{2}}\) & \(Z_{S_{1}}\) & \(Z_{S_{2}}\) \\ \hline \(I(Z;w_{1})\) & 4.45 & 0.16 & 4.39 & 0.14 & 5.17 & 0.19 & **7.83** & 0.03 & 6.25 & 0.04 \\ \(I(Z;w_{2})\) & 0.17 & 3.92 & 0.13 & 4.26 & 0.23 & 5.17 & 0.06 & **7.17** & 0.05 & 5.79 \\ \(I(Z;w_{s})\) & 12.61 & 12.06 & 11.30 & 11.47 & 7.48 & 7.17 & 9.47 & 9.89 & 10.13 & 9.40 \\ \hline \hline \end{tabular}
\end{table}
Table 1: We probe whether contrastive representations learned by classic CL methods and FactorCL contain shared \(w_{s}\) or unique \(w_{1},w_{2}\) information. FactorCL captures the most unique information.

**Multimodal fusion results**: From Table 2, FactorCL significantly outperforms the baselines that do not capture both shared and unique information in both supervised and self-supervised settings, particularly on MuStARD (where unique information expresses sarcasm, such as sardonic facial expressions or ironic tone of voice), and on MIMIC (with unique health indicators and sensor readings). In Table 3, we also show that FactorCL substantially improves the state-of-the-art in classifying images and figurative captions which are not literally descriptive of the image on IRFL, outperforming zero-shot and fine-tuned CLIP [61] as well as continued pre-training baselines on top of CLIP.

**Modeling ablations**: In Table 2, we also carefully ablate each component in our method and indicate either existing baselines or newly-run ablation models.

1. **Factorized representations**: In comparing FactorCL-SSL with OurCL-SSL, and also FactorCL-SUP with OurCL-SUP, we find that factorization is critical: without it, performance drops on average \(6.1\%\), with performance drop as high as \(8.6\%\) for MIMIC.
2. **Information removal via upper bound**: By comparing FactorCL with SimCLR, Cross+Self, and Cross+Self+Fact, and SupCon that only seek to capture task-relevant information via contrastive lower bounds on MI, we find that separately modeling the task-relevant information (to be captured) and task-irrelevant information (to be removed) is helpful. Without removing task-irrelevant information via the upper-bound objective, performance drops on average \(13.6\%\), with performance drops as high as \(23.5\%\) for the MOSI dataset. We also found that training was more difficult without this objective, which is expected due to overwhelming superfluous information from the dataset [93].
3. **Multimodal augmentations**: Finally, we investigate the differences between separate unimodal augmentations (FactorCL-IndAug in Table 3) versus a joint multimodal augmentation (FactorCL-SSL) on the IRFL dataset. We choose this dataset since its images and captions are the easiest to visualize (see Figure 4 for augmentations from both strategies). In the self-supervised setting, we find that multimodal augmentations achieve \(95\%\) performance, higher than the \(92\%\) for separate unimodal augmentations, and both outperform baselines SimCLR and Cross+Self.

**Ablations on \(S,U_{1}\) and \(U_{2}\)**: In Table 4, we also test FactorCL when training linear classifiers on top of only shared \(\{Z_{S_{1}},Z_{S_{2}}\}\) and unique \(Z_{U_{1}}\), \(Z_{U_{2}}\) separately. We call these models FactorCL-\(S\), FactorCL-\(U_{1}\), and FactorCL-\(U_{2}\). Immediately, we observe that performance drops as compared to the full FactorCL model, indicating that both shared and unique information are critical in real-world multimodal tasks. As expected, the best-performing submodel is the one that captures the region with the largest amount of task-relevant information: MOSEI and MOSI are known to include a lot of redundancy and unique information since language is very important for detecting sentiment [93], so FactorCL-\(S\) and FactorCL-\(U_{2}\) perform best. For sarcasm detection on MuStARD, video information is most important with FactorCL-\(U_{1}\) performing best (\(59.4\%\)), and ablation models are also the furthest away from full multimodal performance (\(69.9\%\)). This is aligned with intuition where sarcasm is expressed through tone of voice and visual gestures (high \(U_{1}\)), as well as from contradictions between language and video (higher multimodal performance).

\begin{table}
\begin{tabular}{l|c c c c c|c c c c} \hline \hline Model & \((X_{1};X_{2})\) & \((X_{i};X_{j}^{\prime})\) & \((X_{1};X_{2}|Y)\) & \((X_{2}^{\prime\prime})\) & Fact & MIMIC & MOSEI & MOSI & UR-FUNNY & MUStARD \\ \hline SimCLR [13] & ✓ & ✗ & ✗ & ✗ & ✗ & 66.67\% & 71.03\% & 46.21\% & 50.09\% & 53.48\% \\ Cross+Self [81] & ✓ & ✓ & ✗ & ✗ & ✗ & 65.20\% & 71.04\% & 46.92\% & 56.52\% & 53.91\% \\ Cross+Self+Fact [89] & ✓ & ✓ & ✗ & ✗ & ✓ & 65.49\% & 71.07\% & 52.37\% & 59.91\% & 53.91\% \\ OurCL-SSL & ✓ & ✓ & ✓ & ✓ & ✗ & 65.22\% & 71.16\% & 48.98\% & 58.79\% & 53.98\% \\ FactorCL-SSL & ✓ & ✓ & ✓ & ✓ & ✓ & **67.34\%** & **74.88\%** & **52.91\%** & **60.50\%** & **55.80\%** \\ \hline SupCon [41] & ✗ & ✗ & ✓ & ✗ & ✗ & 67.37\% & 72.71\% & 47.23\% & 50.98\% & 52.75\% \\ OurCL-SUP & ✓ & ✓ & ✓ & ✗ & ✗ & 68.16\% & 71.15\% & 65.32\% & 58.32\% & 65.05\% \\ FactorCL-SUP & ✓ & ✓ & ✓ & ✗ & ✓ & **76.79\%** & **77.34\%** & **70.69\%** & **63.52\%** & **69.86**\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on MultiBench [45] datasets with varying shared and unique information: FactorCL achieves strong results vs self-supervised (top \(5\) rows) and supervised (bottom \(3\) rows) baselines that do not have unique representations, factorization, upper-bounds to remove irrelevant information, and multimodal augmentations.

\begin{table}
\begin{tabular}{l|l} \hline \hline Task & IRFL \\ \hline Zero-shot CLIP [61] & 89.15\% \\ SimCLR [13] & 91.57\% \\ Cross+Self [81, 89] & 95.18\% \\ FactorCL-IndAug & 92.77\% \\ FactorCL-SSL & **95.18\%** \\ \hline Fine-tuned CLIP [61] & 96.39\% \\ SupCon [41] & 89.16\% \\ FactorCL-SUP & **98.80\%** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Continued pre-training on CLIP with our FactorCL objectives on classifying images and figurative language.

**Additional results**: In Appendix D.3, we also verify FactorCL in settings with abundant shared information, where we expect to recover the same performance as standard CL [13; 58; 72].

## 5 Related Work

**Contrastive learning** is a successful self-supervised learning paradigm for computer vision [11; 13; 14; 25; 28; 58], natural language [24; 54; 56], speech [5; 58; 63], and multimodal tasks [1; 37; 61]. Its foundational underpinnings are inspired by work in multiview information theory [23; 41; 70; 72; 76] studying the shared information between two views and whether they are necessary or sufficient in predicting the label. Recently, Wang et al. [81] and Kahana and Hoshen [39] discuss the limitations of assuming multiview redundancy and propose autoencoder reconstruction or unimodal contrastive learning to retain unique information, which resembles the Cross+self baselines in our experiments. We refer the reader to Shwartz-Ziv and LeCun [67] for a comprehensive review on multiview and contrastive learning. Our work also relates to conditional contrastive learning [17; 51; 78; 87], where positive or negative pairs are supposed to sample from conditional distributions.

**Multimodal contrastive learning** aims to align related data from different modalities, typically provided as positive pairs. This could be done via optimizing a contrastive objective for inter-modality pairs [1; 2; 37; 61], or both intra- and inter-modality data pairs [33; 36; 42; 44; 89]. Our work also relates to factorized representation learning, which primarily studies how to capture modality-specific information primarily in each modality and multimodal information redundant in both modalities [32; 75]. Prior work has used disentangled latent variable models [8; 30; 32; 75], mixture-of-experts [66], or product-of-experts [83] layer to explain factors in multimodal data.

**Information theory**[18; 65] has been used to study several phenomena in multimodal learning, including co-learning [62; 92] and multi-view learning [34; 76]. Due to its theoretical importance, several lower and upper bounds have been proposed for practical estimation [58; 59; 60; 84]. We build on the CLUB upper bound [16] to create a more accurate and stable bound. Our characterizations of shared and unique information are also related to partial information decomposition [82], co-information [7; 80], interaction information [53], and cross-domain disentanglement [35] research.

## 6 Conclusion

This paper studied how standard CL methods suffer when task-relevant information lies in regions unique to each modality, which is extremely common in real-world applications such as sensor placement, medical testing, and multimodal interaction. In response, we proposed FactorCL, a new method expanding CL techniques through the use of factorized representations, removing task-irrelevant information via upper bounds on MI, and multimodal data augmentations suitable for approximating the unobserved task. Based on FactorCL's strong performance, there are several exciting directions in extending these ideas for masked and non-contrastive pre-training; we further discuss broader impacts and limitations of this line of work in Appendix A.

### Acknowledgements

This material is based upon work partially supported by Meta, National Science Foundation awards 1722822 and 1750439, and National Institutes of Health awards R01MH125740, R01MH132225, R01MH096951 and R21MH130767. PPL is supported in part by a Siebel Scholarship and a Waibel Presidential Fellowship. RS is supported in part by ONR grant N000142312368 and DARPA FA87502321015. One of the aims of this project is to understand the comfort zone of people for better privacy and integrity. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the sponsors, and no official endorsement should be inferred. Finally, we would also like to acknowledge feedback from anonymous reviewers who significantly improved the paper and NVIDIA's GPU support.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Model & MIMIC & MOSEI & MOSI & UR-FUNNY & MUSTARD \\ \hline FactorCL-\(S\) & 63.77\% & 77.17\% & 70.12\% & 63.42\% & 57.25\% \\ FactorCL-\(U_{1}\) & 55.90\% & 77.06\% & 70.11\% & 62.00\% & 59.42\% \\ FactorCL-\(U_{2}\) & 69.08\% & 71.01\% & 52.33\% & 54.35\% & 53.62\% \\ \hline FactorCL-SUP & **76.79\%** & **77.34\%** & **70.69\%** & **63.52\%** & **69.86\%** \\ \hline \hline \end{tabular}
\end{table}
Table 4: We ablate using only shared representations \(\{Z_{S_{1}},Z_{S_{2}}\}\), unique representation \(Z_{U_{1}}\), and \(Z_{U_{2}}\) separately for prediction. Both shared and unique information are critical in real-world multimodal tasks.

## References

* [1] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. _Advances in Neural Information Processing Systems_, 34:24206-24221, 2021.
* [2] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelovic, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Self-supervised multimodal versatile networks. _Advances in Neural Information Processing Systems_, 33:25-37, 2020.
* [3] Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In _Proceedings of the IEEE international conference on computer vision_, pages 609-617, 2017.
* [4] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. _Advances in neural information processing systems_, 32, 2019.
* [5] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. _Advances in neural information processing systems_, 33:12449-12460, 2020.
* [6] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. In _International Conference on Learning Representations_, 2021.
* [7] Anthony J Bell. The co-information lattice. In _Proceedings of the fifth international workshop on independent component analysis and blind signal separation: ICA_, volume 2003, 2003.
* [8] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. _TPAMI_, 35(8), August 2013.
* [9] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. _Advances in neural information processing systems_, 29, 2016.
* [10] Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, and Desmond Elliott. Multimodal pretraining unmasked: A meta-analysis and a unified framework of vision-and-language bertes. _Transactions of the Association for Computational Linguistics_, 9:978-994, 2021.
* [11] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. _Advances in neural information processing systems_, 33:9912-9924, 2020.
* [12] Santiago Castro, Devamanyu Hazarika, Veronica Perez-Rosas, Roger Zimmermann, Rada Mihalcea, and Soujanya Poria. Towards multimodal sarcasm detection (an _obviously_ perfect paper). _arXiv preprint arXiv:1906.01815_, 2019.
* [13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [14] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 15750-15758, 2021.
* [15] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9640-9649, 2021.
* [16] Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. Club: A contrastive log-ratio upper bound of mutual information. In _International conference on machine learning_, pages 1779-1788. PMLR, 2020.
* [17] Jianfeng Chi, William Shand, Yaodong Yu, Kai-Wei Chang, Han Zhao, and Yuan Tian. Conditional supervised contrastive learning for fair text classification. _arXiv preprint arXiv:2205.11485_, 2022.
* [18] Thomas M Cover and Joy A Thomas. Information theory and statistics. _Elements of information theory_, 1 (1):279-335, 1991.
* [19] Li Deng. The mnist database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.

* Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Eysenbach et al. [2022] Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Russ R Salakhutdinov. Contrastive learning as goal-conditioned reinforcement learning. _Advances in Neural Information Processing Systems_, 35:35603-35620, 2022.
* Feder and Merhav [1994] Meir Feder and Neri Merhav. Relations between entropy and error probability. _IEEE Transactions on Information theory_, 40(1):259-266, 1994.
* Federici et al. [2020] Marco Federici, Anjan Dutta, Patrick Forre, Nate Kushman, and Zeynep Akata. Learning robust representations via multi-view information bottleneck. _arXiv preprint arXiv:2002.07017_, 2020.
* Gao et al. [2021] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simce: Simple contrastive learning of sentence embeddings. _arXiv preprint arXiv:2104.08821_, 2021.
* Grill et al. [2020] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.
* Guo et al. [2022] Qing Guo, Junya Chen, Dong Wang, Yuewei Yang, Xinwei Deng, Jing Huang, Larry Carin, Fan Li, and Chenyang Tao. Tight mutual information estimation with contrastive fenchel-legendre optimization. _Advances in Neural Information Processing Systems_, 35:28319-28334, 2022.
* Hasan et al. [2019] Md Kamrul Hasan, Wasifur Rahman, AmirAli Bagher Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer, Louis-Philippe Morency, and Mohammed Ehsan Hoque. Ur-funny: A multimodal language dataset for understanding humor. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 2046-2056, 2019.
* He et al. [2020] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9729-9738, 2020.
* He et al. [2022] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16000-16009, 2022.
* Higgins et al. [2016] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. 2016.
* Hjelm et al. [2018] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In _International Conference on Learning Representations_, 2018.
* Hsu and Glass [2018] Wei-Ning Hsu and James Glass. Disentangling by partitioning: A representation learning framework for multimodal sensory data. _arXiv preprint arXiv:1805.11264_, 2018.
* Huang et al. [2021] Po-Yao Huang, Mandela Patrick, Junjie Hu, Graham Neubig, Florian Metze, and Alexander Hauptmann. Multilingual multimodal pre-training for zero-shot cross-lingual transfer of vision-language models. _arXiv preprint arXiv:2103.08849_, 2021.
* Huang et al. [2021] Yu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo Huang. What makes multi-modal learning better than single (provably). _Advances in Neural Information Processing Systems_, 34:10944-10956, 2021.
* Hwang et al. [2020] HyeongJoo Hwang, Geon-Hyeong Kim, Seunghoon Hong, and Kee-Eung Kim. Variational interaction information maximization for cross-domain disentanglement. _Advances in Neural Information Processing Systems_, 33:22479-22491, 2020.
* Jain et al. [2021] Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, Chao Jia, Yinfei Yang, and Jason Baldridge. Mural: multimodal, multitask retrieval across languages. _arXiv preprint arXiv:2109.05125_, 2021.
* Jia et al. [2021] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In _International Conference on Machine Learning_, pages 4904-4916. PMLR, 2021.

* [38] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. _Scientific data_, 3(1):1-9, 2016.
* [39] Jonathan Kahana and Yedid Hoshen. A contrastive objective for learning disentangled representations. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXVI_, pages 579-595. Springer, 2022.
* [40] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of NAACL-HLT_, pages 4171-4186, 2019.
* [41] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. _Advances in neural information processing systems_, 33:18661-18673, 2020.
* [42] Byoungjijp Kim, Sungik Choi, Dasol Hwang, Moontae Lee, and Honglak Lee. Transferring pre-trained multimodal representations with cross-modal similarity matching. _Advances in Neural Information Processing Systems_, 35:30826-30839, 2022.
* [43] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). URL http://www.cs.toronto.edu/~kriz/cifar.html.
* [44] Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas Breuel, Jan Kautz, and Yale Song. Parameter efficient multimodal transformers for video representation learning. _arXiv preprint arXiv:2012.04124_, 2020.
* [45] Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle A Lee, Yuke Zhu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Multibench: Multiscale benchmarks for multimodal representation learning. _NeurIPS Datasets and Benchmarks Track_, 2021.
* [46] Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. Towards understanding and mitigating social biases in language models. In _International Conference on Machine Learning_, pages 6565-6576. PMLR, 2021.
* [47] Paul Pu Liang, Yiwei Lyu, Xiang Fan, Shengtong Mo, Dani Yogatama, et al. Highmmt: Towards modality and task generalization for high-modality representation learning. _arXiv preprint arXiv:2203.01311_, 2022.
* [48] Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. Foundations and recent trends in multimodal machine learning: Principles, challenges, and open questions. _arXiv preprint arXiv:2209.03430_, 2022.
* [49] Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Faisal Mahmood, Ruslan Salakhutdinov, and Louis-Philippe Morency. Quantifying & modeling feature interactions: An information decomposition framework. _arXiv preprint arXiv:2302.12247_, 2023.
* [50] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In _Proceedings of the 33rd International Conference on Neural Information Processing Systems_, pages 13-23, 2019.
* [51] Martin Q Ma, Yao-Hung Hubert Tsai, Paul Pu Liang, Han Zhao, Kun Zhang, Ruslan Salakhutdinov, and Louis-Philippe Morency. Conditional contrastive learning for improving fairness in self-supervised learning. _arXiv preprint arXiv:2106.02866_, 2021.
* [52] Emily E Marsh and Marilyn Domas White. A taxonomy of relationships between images and text. _Journal of documentation_, 2003.
* [53] William McGill. Multivariate information transmission. _Transactions of the IRE Professional Group on Information Theory_, 4(4):93-111, 1954.
* [54] Yu Meng, Chenyan Xiong, Payal Bajaj, Paul Bennett, Jiawei Han, Xia Song, et al. Coco-lm: Correcting and contrasting text sequences for language model pretraining. _Advances in Neural Information Processing Systems_, 34:23102-23114, 2021.
* [55] Sudipto Mukherjee, Himanshu Asnani, and Sreeram Kannan. Ccmi: Classifier based conditional mutual information estimation. In _Uncertainty in artificial intelligence_, pages 1083-1093. PMLR, 2020.
* [56] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. Text and code embeddings by contrastive pre-training. _arXiv preprint arXiv:2201.10005_, 2022.

* Nguyen et al. [2010] XuanLong Nguyen, Martin J Wainwright, and Michael T Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. _IEEE Transactions on Information Theory_, 56(11):5847-5861, 2010.
* van den Oord et al. [2018] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* Ozair et al. [2019] Sherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron Van den Oord, Sergey Levine, and Pierre Sermanet. Wasserstein dependency measure for representation learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* Poole et al. [2019] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In _International Conference on Machine Learning_, pages 5171-5180. PMLR, 2019.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* Rahate et al. [2022] Anil Rahate, Rahee Walambe, Sheela Ramanna, and Ketan Kotecha. Multimodal co-learning: challenges, applications with datasets, recent advances and future directions. _Information Fusion_, 81:203-239, 2022.
* Schneider et al. [2019] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-training for speech recognition. _arXiv preprint arXiv:1904.05862_, 2019.
* Shan et al. [2022] Bin Shan, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil 2.0: Multi-view contrastive learning for image-text pre-training, 2022.
* Shannon [1948] Claude Elwood Shannon. A mathematical theory of communication. _The Bell system technical journal_, 27(3):379-423, 1948.
* Shi et al. [2019] Yuge Shi, Brooks Paige, Philip Torr, et al. Variational mixture-of-experts autoencoders for multi-modal deep generative models. _Advances in Neural Information Processing Systems_, 32, 2019.
* Shwartz-Ziv and LeCun [2023] Ravid Shwartz-Ziv and Yann LeCun. To compress or not to compress-self-supervised learning and information theory: A review. _arXiv preprint arXiv:2304.09355_, 2023.
* Song and Ermon [2019] Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information estimators. _CoRR_, abs/1910.06222, 2019. URL http://arxiv.org/abs/1910.06222.
* Sordoni et al. [2021] Alessandro Sordoni, Nouha Dziri, Hannes Schulz, Geoff Gordon, Philip Bachman, and Remi Tachet Des Combes. Decomposed mutual information estimation for contrastive representation learning. In _International Conference on Machine Learning_, pages 9859-9869. PMLR, 2021.
* Sridharan and Kakade [2008] Karthik Sridharan and Sham M Kakade. An information theoretic framework for multi-view learning. In _Conference on Learning Theory_, 2008.
* Tian et al. [2020] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. _ECCV_, 2020.
* Tian et al. [2020] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? _Advances in Neural Information Processing Systems_, 33:6827-6839, 2020.
* Tosh et al. [2021] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy, and linear models. In _Algorithmic Learning Theory_, pages 1179-1206. PMLR, 2021.
* Tsai et al. [2019] Yao-Hung Hubert Tsai, Tianqin Li, Weixin Liu, Peiyuan Liao, Ruslan Salakhutdinov, and Louis-Philippe Morency. Learning weakly-supervised contrastive representations. In _International Conference on Learning Representations_.
* Tsai et al. [2019] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhutdinov. Learning factorized multimodal representations. _ICLR_, 2019.
* Tsai et al. [2020] Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-supervised learning from a multi-view perspective. In _International Conference on Learning Representations_, 2020.
* Tsai et al. [2020] Yao-Hung Hubert Tsai, Han Zhao, Makoto Yamada, Louis-Philippe Morency, and Russ R Salakhutdinov. Neural methods for point-wise dependency estimation. _Advances in Neural Information Processing Systems_, 33:62-72, 2020.

* [78] Yao-Hung Hubert Tsai, Tianqin Li, Martin Q Ma, Han Zhao, Kun Zhang, Louis-Philippe Morency, and Ruslan Salakhutdinov. Conditional contrastive learning with kernel. _arXiv preprint arXiv:2202.05458_, 2022.
* [79] Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual information maximization for representation learning. In _International Conference on Learning Representations_, 2019.
* [80] Jorge R Vergara and Pablo A Estevez. A review of feature selection methods based on mutual information. _Neural computing and applications_, 24:175-186, 2014.
* [81] Haoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. Rethinking minimal sufficient representation in contrastive learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16041-16050, 2022.
* [82] Paul L Williams and Randall D Beer. Nonnegative decomposition of multivariate information. _arXiv preprint arXiv:1004.2515_, 2010.
* [83] Mike Wu and Noah Goodman. Multimodal generative models for scalable weakly-supervised learning. _Advances in Neural Information Processing Systems_, 31, 2018.
* [84] Mike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, and Noah Goodman. On mutual information in contrastive learning for visual representations. _arXiv preprint arXiv:2005.13149_, 2020.
* [85] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao. Unified contrastive learning in image-text-label space, 2022.
* [86] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang. Vision-language pre-training with triple contrastive learning, 2022.
* [87] Zesheng Ye and Lina Yao. Contrastive conditional neural processes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9687-9696, 2022.
* [88] Ron Yosef, Yonatan Bitton, and Dafna Shahaf. Irfl: Image recognition of figurative language. _arXiv preprint arXiv:2303.15445_, 2023.
* [89] Xin Yuan, Zhe Lin, Jason Kuen, Jianming Zhang, Yilin Wang, Michael Maire, Ajinkya Kale, and Baldo Faieta. Multimodal contrastive training for visual representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6995-7004, 2021.
* [90] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6023-6032, 2019.
* [91] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos. _arXiv preprint arXiv:1606.06259_, 2016.
* [92] Amir Zadeh, Paul Pu Liang, and Louis-Philippe Morency. Foundations of multimodal co-learning. _Information Fusion_, 64:188-193, 2020.
* [93] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2236-2246, 2018.
* [94] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In _International Conference on Machine Learning_, pages 12310-12320. PMLR, 2021.

## Appendix A Broader Impact

Multimodal data and self-supervised models are ubiquitous in a range of real-world applications. This paper is our attempt at broadening the applicability of self-supervised contrastive methods to a wider range of multimodal tasks beyond those that exhibit multi-view redundancy. We believe that special care must be taken to ensure that these models are safely deployed for real-world benefit:

**Time and space complexity**: Modern multimodal models are large and take up a significant amount of carbon footprint during training and testing. As compared to heuristic combinations of cross-modal and single-modality CL [33, 36, 44, 64, 81, 85, 89], we believe that FactorCL does not significantly increase complexity: (1) upper bounds on MI can be estimated "for free" by directly plugging in the optimal critic from \(I_{\text{NCE}}\), and (2) removal of task-irrelevant information via \(I(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime})\) shares encoders with \(I_{\text{NCE}}\), and (3) separate unimodal augmentations perform well enough in practice. We also release our code and models so that they can be evaluated quickly on new tasks, which can amortize complexity costs.

**Privacy and security**: There may be privacy risks associated with making predictions from multimodal data of recorded human behaviors and medical data (i.e., the datasets used in our experiments for analysis of sentiment, emotions, personality, sarcasm, and humor, as well as disease prediction from medical data). We have followed best practices in maintaining the privacy and safety of these datasets: (1) the creators of these video datasets have taken the appropriate steps to only access public data that participants or content creators have consented for public release (creative commons license and following fair use guidelines of YouTube) [12, 27, 93], (2) MIMIC has been rigorously de-identified in accordance with Health Insurance Portability and Accountability Act (HIPAA) such that all possible personal information has been removed from the dataset [38], (3) all video data was also anonymized and stripped of all personal (e.g., personally identifiable information) and protected attributes (e.g., race, gender), (4) all models trained on affect recognition datasets use only pre-extracted non-invertible features that rely on general visual or audio features such as the presence of a smile or magnitude of voice which cannot be used to identify the speaker [91, 93], and (5) we studied the videos collected in these affective computing datasets and found no offensive words used or personal attacks recorded in the video. Finally, we only use these datasets for research purposes and emphasize that any multimodal models trained to perform prediction should only be used for scientific study and should not in any way be used for real-world harm.

**Social biases**: We acknowledge risks of social bias due to imbalanced datasets, resulting in potential biases surrounding gender, race, and ethnicity, among others [9, 46]. We note that our FactorCL approach has a close link with conditional CL [51], which can also be adapted to condition on sensitive attributes and therefore reduce bias. Studying these research questions is an important direction for future work.

**Future work**: We discuss more limitations and potential future work in this direction. Firstly, optimizing our objectives using better MI lower and upper bounds such as in Guo et al. [26] and Sordoni et al. [69], could improve the performance for inputs of higher dimension and complex modality. Next, the current data augmentation method requires one to pick augmentations to approximately satisfy Definition 4; future work could extend InfoMin [72] to automatically generate data augmentations to satisfy Definition 4, or leverage future progress in multimodal generative models for data augmentation. Lastly, future work could quantify whether shared or unique information is more important for different tasks and reweight the terms in the FactorCL objective to suit the tasks.

## Appendix B Analysis of Multi-view Contrastive Learning

**Multi-view shared information** describes the extent and dimensions in which information can be shared across different views. The presence of shared information is often in contrast to unique information that exists solely in a single modality, and can be formalized via information theory:

**Definition 5**.: _(Shared information) Given \(X_{1}\) and \(X_{2}\), \(I(X_{1};X_{2})=\int p(x_{1},x_{2})\log\frac{p(x_{1},x_{2})}{p(x_{1})p(x_{2})}\) measures the degree of information-theoretic shared information between \(X_{1}\) and \(X_{2}\)._

**Definition 6**.: _(Task-relevant shared information) Given \(X_{1}\), \(X_{2}\), and a target \(Y\), \(I(X_{1};X_{2};Y)=I(X_{1};X_{2})-I(X_{1};X_{2}|Y)=\int p(x_{1},x_{2})\log\frac{ p(x_{1},x_{2})}{p(x_{1})p(x_{2})}-\int p(x_{1},x_{2}|y)\log\frac{p(x_{1},x_{2}|y)}{p(x_{1 }|y)p(x_{2}|y)}\) measuresthe amount of task-relevant shared information between \(X_{1}\) and \(X_{2}\) for predicting \(Y\). \(I(X_{1};X_{2}|Y)\) represents the task-irrelevant shared information._

**Learning shared information via contrastive learning**: Current approaches for multi-view contrastive learning model shared information \(I(X_{1};X_{2})\) (and subsequently task-relevant shared information \(I(X_{1};X_{2};Y)\) during downstream task fine-tuning), without modeling unique information.

\[Z_{1}=\operatorname*{arg\,max}_{Z_{1}=f_{\theta}(X_{1})}I(Z_{1};X_{2}),Z_{2}= \operatorname*{arg\,max}_{Z_{2}=f_{\theta}(X_{2})}I(X_{1};Z_{2}).\] (19)

Optimizing for \(I(X_{1};X_{2})\) is performed via a surrogate loss during self-supervised pre-training (where we do not have access to the label \(Y\)) by maximizing the InfoNCE objective:

\[\text{\sc InfoNCE}=\sup_{f}\mathbb{E}_{\begin{subarray}{c}x_{1},x_{2}^{*}\text {-}p(x_{1},x_{2})\\ x_{2}^{*}\text{-}p(x_{2})\end{subarray}}\left[\log\frac{\exp f(x_{1},x_{2}^{* })}{\sum_{k}\exp f(x_{1},x_{2}^{*})}\right],\] (20)

Oord et al. [58] show that \(I(X_{1};X_{2})\geq\log k-\mathcal{L}_{\text{NCE}}(X_{1};X_{2})\) where \(\mathcal{L}_{\text{NCE}}(X_{1};X_{2})\) is negative of InfoNCE and is the loss to minimize (rather than maximize) in training. NCE falls into a broader class of contrastive learning methods [13, 15, 28, 41, 61] that model the ratio between joint densities \(p(x_{1},x_{2})\) and product of marginal densities \(p(x_{1})p(x_{2})\) using positive and negative samples [57, 59, 60, 79, 84] or probabilistic classifiers [55, 77], all of which can also be used to capture shared information.

Tian et al. [71] argues that the optimal view of contrastive learning is also minimal: the minimal representations only extract relevant information of the contrastive task (maximizing the shared part) and throw away other information. Therefore, from this minimal assumption, we have \(I(Z_{1};Y|X_{2})=0\) and \(I(Z_{2};Y|X_{1})=0\) as minimal \(Z_{1}\) and \(Z_{2}\) only captures task-relevant information from the shared part. By conditioning on \(X_{1}\) or \(X_{2}\), the shared part is removed, and \(Z_{1}\) and \(Y\) (or \(Z_{2}\) and \(Y\)) do not share information.

Lastly, we restate the multi-view non-redundancy from Definition 2:

**Definition 7**.: _(Multi-view non-redundancy) \(\exists\epsilon>0\) such that \(I(X_{1};Y|X_{2})>\epsilon\) or \(I(X_{2};Y|X_{1})>\epsilon\)._

We would like to compare and clarify the differences between the multiview redundancy assumption in Eq.(1) and the multi-view nonredundancy in Def. 7. The multiview redundancy assumption in Eq.(1) states that the task-relevant information from the unique part is minimal (\(\leq\epsilon\)). The multiview non-redundancy states the opposite: the task-relevant information from the unique part is nonzero and nonminimal, as it is not bounded by \(\epsilon\). Next we briefly clarify the relationship between these two assumptions and the InfoMin assumption: \(I(Z_{1};Y|X_{2})=I(Z_{2};Y|X_{1})=0\). InfoMin is about representation \(Z\) while the redundancy assumptions are only about data \(X\). InfoMin states that the optimal (sufficient and minimal) representation learns task-relevant information only from the shared part, as we discussed in the paragraph above. We empirically checked the two assumptions: Tables 1 and 4 in the main text show that the multiview non-redundancy assumption holds empirically, and Table 11 shows that the InfoMin assumption holds empirically.

We now show the limitations of CL methods, first restating the Theorem here:

**Theorem 4**.: _(Suboptimality of standard CL) When there is multi-view non-redundancy as in Definition 7, given optimal representations \(\{Z_{1},Z_{2}\}\) that satisfy Eq.(19 and \(I(Z_{1};Y|X_{2})=I(Z_{2};Y|X_{1})=0\)[72], we have that_

\[I(Z_{1},Z_{2};Y)=I(X_{1},X_{2};Y)-I(X_{1};Y|X_{2})-I(X_{2};Y|X_{1})=I(X_{1};X_{ 2})-I(X_{1};X_{2}|Y)<I(X_{1},X_{2};Y).\] (21)

Proof.: Since \(Z_{1}\) and \(Z_{2}\) maximize \(I(X_{1};X_{2})\) we have that \(I(Z_{1};X_{2})=I(X_{1};Z_{2})=I(X_{1};X_{2})\) so \(I(Z_{1};X_{2};Y)=I(X_{1};Z_{2};Y)=I(X_{1};X_{2};Y)\) and \(I(Z_{1};X_{2}|Y)=I(X_{1};Z_{2}|Y)=I(X_{1};X_{2}|Y)\).

We now show the relationship between \(I(X_{1},X_{2};Y)\), which is the total information \(X_{1},X_{2}\) contributes towards predicting \(Y\) in classic supervised learning, with \(I(Z_{1},Z_{2};Y)\), which is the information that our learned self-supervised representations can contribute towards \(Y\) during supervised fine-tuning. We first derive the relationship between \(I(Z_{1};Y)\) and \(I(X_{1};Y)\):

\[I(Z_{1};Y) =I(Z_{1};X_{2};Y)+I(Z_{1};Y|X_{2})\] (22) \[=I(X_{1};X_{2};Y)+I(Z_{1};Y|X_{2})\] (23) \[=I(X_{1};Y)-I(X_{1};Y|X_{2})+I(Z_{1};Y|X_{2})\] (24) \[=I(X_{1};Y)-I(X_{1};Y|X_{2})\] (25)Given \(X_{1}\), we further derive a relationship between \(I(Z_{2};Y|Z_{1})\) and \(I(X_{2};Y|X_{1})\):

\[I(Z_{2};Y|Z_{1}) =I(Z_{2};X_{1};Y|Z_{1})+I(Z_{2};Y|Z_{1},X_{1})\] (26) \[=I(X_{1};X_{2};Y|Z_{1})+I(Z_{2};Y|Z_{1},X_{1})\] (27) \[=I(X_{1};X_{2};Y|Z_{1})+I(Z_{2};Y|X_{1})\] (28) \[=I(X_{2};Y|Z_{1})-I(X_{2};Y|X_{1},Z_{1})+I(Z_{2};Y|X_{1})\] (29) \[=I(X_{2};Y|Z_{1})-I(X_{2};Y|X_{1})+I(Z_{2};Y|X_{1})\] (30) \[=I(X_{2};Y)-I(Z_{1};X_{2};Y)-I(X_{2};Y|X_{1})+I(Z_{2};Y|X_{1})\] (31) \[=I(X_{2};Y)-I(X_{1};X_{2};Y)-I(X_{2};Y|X_{1})+I(Z_{2};Y|X_{1})\] (32) \[=I(X_{2};Y|X_{1})-I(X_{2};Y|X_{1})+I(Z_{2};Y|X_{1})=0\] (33)

In Eqs.(28) and (30), we use the fact that conditioning on \(Z_{1}\) and \(X_{1}\) jointly reduces to conditioning on \(X_{1}\) since \(Z_{1}\) is deterministically obtained from \(X_{1}\), and in Eq.(32) we use the definition of learning \(Z_{s}\) to maximize \(I(X_{1};X_{2})\) so \(I(Z_{1};X_{2};Y)=I(X_{1};Z_{2};Y)\). Finally, adding both terms up,

\[I(Z_{1},Z_{2};Y) =I(Z_{1};Y)+I(Z_{2};Y|Z_{1})\] (34) \[=I(X_{1};Y)-I(X_{1};Y|X_{2})\] (35) \[=I(X_{1};X_{2};Y)\] (36) \[=I(X_{1},X_{2};Y)-I(X_{1};Y|X_{2})-I(X_{2};Y|X_{1})\] (37) \[=I(X_{1};X_{2})-I(X_{1};X_{2}|Y)\] (38)

gives the desired result. 

**Bayes error rate.** The Bayes error rate \(P_{e}(Z_{1},Z_{2}):=1-\mathbb{E}_{P_{Z_{1},Z_{2}}}\left[\max_{y\in Y}P\left( \hat{Y}=y\mid z_{1},z_{2}\right)\right]\) of contrastive representations \(\{Z_{1},Z_{2}\}\) is given by:

\[P_{e} \leq 1-\exp\left[I(X_{1},X_{2};Y)-I(X_{1};Y|X_{2})-I(X_{2};Y|X_{1} )-H(Y)\right]\] (39) \[=1-\exp\left[I(X_{1};X_{2};Y)-H(Y)\right]\] (40)

Proof.: We use the inequality between \(P_{e}\) and \(H(Y|Z)\)[22, 76, 81]:

\[-\ln(1-P_{e})\leq H(Y|Z),\text{ or equivalently, }P_{e}\leq 1-\exp[-H(Y|Z)]\] (41)

If we regard \(Z\) as the joint of \(Z_{1}\) and \(Z_{2}\), then we have

\[P_{e}\leq 1-\exp[-H(Y|Z_{1},Z_{2})]\] (42)

We further expand \(H(Y|Z_{1},Z_{2})\) by definition of mutual information, \(I(X;Y)=H(X)-H(X|Y)\), Theorem 4, and the \(I(X_{1};X_{2};Y)=I(X_{1};X_{2})-I(X_{1};X_{2}|Y)\):

\[H(Y|Z_{1},Z_{2}) =H(Y)-I(Z_{1},Z_{2};Y)\] (43) \[=H(Y)-I(X_{1},X_{2};Y)+I(X_{1};Y|X_{2})+I(X_{2};Y|X_{1})\] (44) \[=H(Y)-I(X_{1};X_{2})+I(X_{1};X_{2}|Y)\] (45) \[=H(Y)-I(X_{1};X_{2};Y)\] (46)

Plugging in Eq.(42), we have

\[P_{e} \leq 1-\exp[-H(Y|Z_{1},Z_{2})]\] (47) \[=1-\exp[-(H(Y)-I(X_{1},X_{2};Y)+I(X_{1};Y|X_{2})+I(X_{2};Y|X_{1} ))]\] (48) \[=1-\exp[-H(Y)+I(X_{1},X_{2};Y)-I(X_{1};Y|X_{2})-I(X_{2};Y|X_{1})]\] (49)

and

\[P_{e} \leq 1-\exp[-H(Y|Z_{1},Z_{2})]\] (50) \[=1-\exp[-(H(Y)-I(X_{1};X_{2};Y))]\] (51) \[=1-\exp[-H(Y)+I(X_{1};X_{2};Y)]\] (52)

resulting in the Bayes error rate as desired.

Factorized Contrastive Learning

### Contrastive estimators

**Theorem 5**.: _(Contrastive estimators for \(I(X_{1};X_{2})\)) Defining the NCE estimator and NCE-CLUB estimator as follows,_

\[I_{\text{NCE}}(X_{1};X_{2}) =\mathbb{E}_{x_{1},x_{2}^{+}\text{-}p(x_{1},x_{2})}\left[\log\frac {\exp f(x_{1},x_{2}^{+})}{\sum_{k}\exp f(x_{1},x_{2}^{-})}\right]\] (53) \[I_{\text{NCE-CLUB}}(X_{1};X_{2}) =\mathbb{E}_{x_{1},x_{2}^{+}\text{-}p(x_{1},x_{2})}\left[f^{*}(x_ {1},x_{2}^{+})\right]-\mathbb{E}_{x_{2}\text{-}p(x_{2})}\left[f^{*}(x_{1},x_{2 }^{-})\right]\] (54)

_where \(f^{*}(x_{1},x_{2})\) is the optimal critic from \(I_{\text{NCE}}\) plugged into the \(I_{\text{CLUB}}\) objective [16]. We call the proposed plug-in objective Eq.(11) \(I_{\text{NCE-CLUB}}\), and obtain lower and upper bounds on \(I(X_{1};X_{2})\):_

\[I_{\text{NCE}}(X_{1};X_{2})\leq I(X_{1};X_{2})\leq I_{\text{NCE-CLUB}}(X_{1};X _{2}).\] (55)

Proof.: The lower bound \(I_{\text{NCE}}(X_{1};X_{2})\leq I(X_{1};X_{2})\) follows from Oord et al. [58]: optimizing the objective leads to an optimal critic \(f^{*}=\log p(x_{2}|x_{1})+c(x_{2})\)[60] or without loss of generality \(f^{*}=\log p(x_{1}|x_{2})+c(x_{1})\), where \(c(\cdot)\) is an arbitrary deterministic function. Plugging the optimal critic into the \(I_{\text{NCE}}(X_{1};X_{2})\) gives the result: \(I_{\text{NCE}}(X_{1};X_{2})+\log N\leq I(X_{1};X_{2})\)[58, 60].

Next, the original \(I_{\text{CLUB}}(X_{1};X_{2})\)[16] is defined as:

\[I_{\text{CLUB}}(X_{1};X_{2}):=\mathbb{E}_{p(x_{1},x_{2})}\left[\log p(x_{2}|x _{1})\right]-\mathbb{E}_{p(x_{1})p(x_{2})}\left[\log p(x_{2}|x_{1})\right].\] (56)

As mutual information is symmetric w.r.t \(x_{1}\) and \(x_{2}\): \(I(X_{1};X_{2})=I(X_{2};X_{1})\), without loss of generality, we have:

\[I_{\text{CLUB}}(X_{1};X_{2})=I_{\text{CLUB}}(X_{2};X_{1})=\mathbb{E}_{p(x_{1},x_{2})}\left[\log p(x_{1}|x_{2})\right]-\mathbb{E}_{p(x_{1})p(x_{2})}\left[ \log p(x_{1}|x_{2})\right]\] (57)

The formulation above assumes \(p(x_{1}|x_{2})\) is known, which is satisfied when we use the optimal critic \(f^{*}=\log p(x_{1}|x_{2})+c(x_{1})\) from \(I_{\text{NCE}}(X_{1};X_{2})\). Plugging the optimal critic \(f^{*}\) into \(I_{\text{CLUB}}(X_{1};X_{2})\), we obtain a desired upper bound \(I_{\text{NCE-CLUB}}(X_{1};X_{2})\) of \(I(X_{1};X_{2})\):

\[I_{\text{NCE-CLUB}}(X_{1};X_{2})=\mathbb{E}_{p(x_{1},x_{2})}\left[ \log p(x_{1}|x_{2})+c(x_{1})\right]-\mathbb{E}_{p(x_{1})p(x_{2})}\left[\log p( x_{1}|x_{2})+c(x_{1})\right]\] (58) \[=\mathbb{E}_{p(x_{1},x_{2})}\left[\log p(x_{1}|x_{2})\right]+ \mathbb{E}_{p(x_{1},x_{2})}\left[c(x_{1})\right]-\mathbb{E}_{p(x_{1})p(x_{2})} \left[\log p(x_{1}|x_{2})\right]-\mathbb{E}_{p(x_{1})p(x_{2})}\left[c(x_{1})\right]\] (59) \[=\mathbb{E}_{p(x_{1},x_{2})}\left[\log p(x_{1}|x_{2})\right]- \mathbb{E}_{p(x_{1})p(x_{2})}\left[\log p(x_{1}|x_{2})\right]\] (60) \[\geq I(X_{1};X_{2}).\] (61)

Eq.(59) is from the linearity of expectation, Eq.(60) is from the fact that \(c(x_{1})\) is not a function of \(x_{2}\) and therefore \(\mathbb{E}_{p(x_{1},x_{2})}\left[c(x_{1})\right]=\mathbb{E}_{p(x_{1})p(x_{2})} \left[c(x_{1})\right]=\mathbb{E}_{p(x_{1})}\left[c(x_{1})\right]\), and Eq.(61) is directly from the original \(I_{\text{CLUB}}(X_{1};X_{2})\) paper [16]. 

### Unimodal and multimodal augmentations

We first restate the definitions of optimal single-view and multi-view augmentation:

**Definition 8**.: _(Optimal unimodal augmentation) \(X_{1}^{\prime}\) is an optimal unimodal augmentation for \(X_{1}\) when \(I(X;X^{\prime})\) = \(I(X;Y)\), which implies that the only information shared between \(X\) and \(X^{\prime}\) is task-relevant with no irrelevant noise._

**Definition 9**.: _(Optimal multimodal augmentation) \(X_{1}^{\prime}\) and \(X_{2}^{\prime}\) are optimal multimodal augmentation for \(X_{1}\) and \(X_{2}\) when \(I(X_{1},X_{2};X_{1}^{\prime},X_{2}^{\prime})\) = \(I(X_{1},X_{2};Y)\), which implies that the only information shared between \(X_{1},X_{2}\) and \(X_{1}^{\prime},X_{2}^{\prime}\) is task-relevant with no irrelevant noise._

When are these assumptions satisfied? \(I(X;X^{\prime})\) = \(I(X;Y)\) holds when all information shared between \(X\) and \(X^{\prime}\) is task-relevant, which implies that the augmentation keeps task-relevant information constant while changing task-irrelevant information. In the case of image classification, task-relevant information is the object in the picture, while task-irrelevant information is the background. To satisfy \(I(X_{1},X_{2};X_{1}^{\prime},X_{2}^{\prime})\) = \(I(X_{1},X_{2};Y)\), by the chain rule of MI, we augment in two steps:

_Unimodal aug: \[X_{1}^{\prime}\]_ s.t. \[I(X_{1};X_{1}^{\prime})=I(X_{1};Y),\] (62) _Unique aug: \[X_{2}^{\prime}\]_ s.t. \[I(X_{2};X_{2}^{\prime}|X_{1})=I(X_{2};Y|X_{1}).\] (63)the second step is the _unique augmentation_: after observing \(X_{1}\), we create augmented \(X_{2}^{\prime}\) from \(X_{2}\) to keep the task-relevant information but meanwhile do not affect any information from \(X_{1}\). In Table 5, we include some more examples of how unique augmentations could be designed across different datasets.

**Final objectives**: If Definitions 8 and 9 are both satisfied, we can substitute contrastive estimators in the following equations:

\[I_{\text{NCE}}(X_{1};X_{2}|Y) =\mathbb{E}_{p(y)}\Bigg{[}\mathbb{E}_{\begin{subarray}{c}x_{1},x_ {2}^{*}-p(x_{1},x_{2}|y)\\ x_{2}^{*}-p(x_{2}|y)\end{subarray}}\Bigg{[}\log\frac{\exp f(x_{1},x_{2}^{*},y) }{\sum_{k}\exp f(x_{1},x_{2}^{*},y)}\Bigg{]}\Bigg{]}\] (64) \[I_{\text{NCE-CLUB}}(X_{1};X_{2}|Y) =\mathbb{E}_{p(y)}\Bigg{[}\mathbb{E}_{\begin{subarray}{c}x_{1},x_ {2}^{*}-p(x_{1},x_{2}|y)\end{subarray}}\big{[}f^{*}(x_{1},x_{2}^{*},y) \big{]}-\mathbb{E}_{\begin{subarray}{c}x_{1}-p(x_{1}|y)\\ x_{2}^{*}-p(x_{2}|y)\end{subarray}}\big{[}f^{*}(x_{1},x_{2}^{*},y)\big{]} \Bigg{]}\] (65)

by replacing \(I(X_{i};Y)\) terms with \(I(X_{i};X_{i}^{\prime})\) and replacing \(I(X_{1};X_{2}|Y)\) terms with \(I(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime})\):

\[I_{\text{NCE}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime}) =\mathbb{E}_{p(x_{1}^{\prime},x_{2}^{\prime})}\Bigg{[}\mathbb{E}_ {\begin{subarray}{c}x_{1},x_{2}^{*}-p(x_{1},x_{2}|x_{1}^{\prime},x_{2}^{ \prime})\\ x_{2}-p(x_{2}|x_{1}^{\prime},x_{2}^{\prime})\end{subarray}}\Bigg{[}\log\frac{ \exp f(x_{1},x_{2}^{*},x_{1}^{\prime},x_{2}^{\prime})}{\sum_{k}\exp f(x_{1},x_ {2}^{*},x_{1}^{\prime},x_{2}^{\prime})}\Bigg{]}\Bigg{]}\] (66) \[I_{\text{NCE-CLUB}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime}) =\mathbb{E}_{p(x_{1}^{\prime},x_{2}^{\prime})}\Big{[}\mathbb{E}_ {\begin{subarray}{c}x_{1},x_{2}^{*}-p(x_{1},x_{2}|x_{1}^{\prime},x_{2}^{ \prime})\\ x_{2}^{*}-p(x_{2}|x_{1}^{\prime},x_{2}^{\prime})\end{subarray}}[f^{*}(x_{1},x_{2}^{*},x_{1}^{\prime},x_{2}^{\prime})]\] \[-\mathbb{E}_{\begin{subarray}{c}x_{1}-p(x_{1}|x_{1}^{\prime},x_{2 }^{\prime})\\ x_{2}^{*}-p(x_{2}|x_{1}^{\prime},x_{2}^{\prime})\end{subarray}}[f^{*}(x_{1},x_{2}^{*},x_{1}^{\prime},x_{2}^{\prime})]\Big{]}\] (67)

#### c.2.1 Implementing conditional CL via kernel

We restate our objectives below:

\[I_{\text{NCE}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime}) =\mathbb{E}_{p(x_{1}^{\prime},x_{2}^{\prime})}\Bigg{[}\mathbb{E} _{\begin{subarray}{c}x_{1},x_{2}^{*}-p(x_{1},x_{2}|x_{1}^{\prime},x_{2}^{ \prime})\\ x_{2}^{*}-p(x_{2}|x_{1}^{\prime},x_{2}^{\prime})\end{subarray}}\Bigg{[}\log \frac{\exp f(x_{1},x_{2}^{*},x_{1}^{\prime},x_{2}^{\prime})}{\sum_{k}\exp f(x_ {1},x_{2}^{*},x_{1}^{\prime},x_{2}^{\prime})}\Bigg{]}\Bigg{]}\] (68) \[I_{\text{NCE-CLUB}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime}) =\mathbb{E}_{p(x_{1}^{\prime},x_{2}^{\prime})}\Big{[}\mathbb{E} _{\begin{subarray}{c}x_{1},x_{2}^{*}-p(x_{1},x_{2}|x_{1}^{\prime},x_{2}^{ \prime})\end{subarray}}[f^{*}(x_{1},x_{2}^{*},x_{1}^{\prime},x_{2}^{\prime})]\] \[-\mathbb{E}_{\begin{subarray}{c}x_{1}-p(x_{1}|x_{1}^{\prime},x_{2 }^{\prime})\\ x_{2}^{*}-p(x_{2}|x_{1}^{\prime},x_{2}^{\prime})\end{subarray}}[f^{*}(x_{1},x_{2}^{-},x_{1}^{\prime},x_{2}^{\prime})]\Big{]}\] (69)

However, sampling from \(p(\cdot|x_{1}^{\prime},x_{2}^{\prime})\) is hard. Since \(X_{1}^{\prime},X_{2}^{\prime}\) are continuous variables, directly sampling from the conditional distributions \(p(\cdot|x_{1}^{\prime},x_{2}^{\prime})\) may be difficult; training a generative model \(p_{\theta}(x_{1},x_{2}|x_{1}^{\prime},x_{2}^{\prime})\) from augmented data \(x_{1}^{\prime},x_{2}^{\prime}\) to original data \(x_{1},x_{2}\) can be expensive and nontrivial in a multimodal setup. In this work, we implement the conditioning in \(p(x_{1},x_{2}|x_{1}^{\prime},x_{2}^{\prime})\) through concatenation and the details are in Appendix D.1. Here we discuss an alternative solution to this problem introduced by Tsai et al. [78]. It leverages kernel methods for conditional sampling in contrastive learning by assigning weights to each sampled data given the kernel similarity between conditioned variables, avoiding directly sampling from the conditional distributions or training generative models. In our formulation, given multimodal input \((x_{1},x_{2})\) with their augmentation \((x_{1}^{\prime},x_{2}^{\prime})\), we can simply use the technique from [78] to estimate \(I_{\text{NCE}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime})\) and \(I_{\text{NCE-CLUB}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime})\), where the kernel measures the similarity between different pairs \((x_{1}^{\prime},x_{2}^{\prime})\) of the conditional variable

\begin{table}
\begin{tabular}{l c c c c c} \hline  & & & & Standard Aug & **Unique Aug** \\ Dataset & \(X_{1}\) & \(X_{2}\) & \(X_{1}^{\prime}\) & \(X_{2}^{\prime}\) & \(X_{2}^{\prime\prime}\) \\ \hline Cartoon & Caption & Image & Word Masking & Crop + Flip + Resize & Flip + Resize \\ MIMIC & Signals & Tables & Time Warping & CutMix [90] on All Features & CutMix on Nonclinical Features \\ MOSEI & Transcripts & Video+Audio & Word Masking & Noise Injection on Any Frames & Noise Injection on Silent Frames \\ UR-FUNNY & Transcripts & Video+Audio & Word Masking & Noise Injection on Any Frames & Noise Injection on Silent Frames \\ MUsTARD & Transcripts & Video+Audio & Word Masking & Noise Injection on Any Frames & Noise Injection on Silent Frames \\ \hline \hline \end{tabular}
\end{table}
Table 5: More examples of optimal single-view and multi-view augmentations.

\(X_{1},X_{2}\). Specifically,

\[I_{\text{NCE}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime}) =\mathbb{E}_{p(x_{1},x_{2},x_{1}^{\prime},x_{2}^{\prime})}\Bigg{[} \log\frac{\exp f(x_{1},x_{2}^{+})}{\exp f(x_{1},x_{2})+n*\big{[}K_{X_{1}X_{2}| X_{1}^{\prime},X_{2}^{\prime}}\big{]}_{ii}}\Bigg{]}\] (70) \[I_{\text{NCE-CLUB}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime}) =\mathbb{E}_{p(x_{1},x_{2},x_{1}^{\prime},x_{2}^{\prime})}\left[ f^{*}(x_{1},x_{2})-\log\left[K_{X_{1}\pm X_{2}|X_{1}^{\prime},X_{2}^{ \prime}}\right]_{ii}\right]\] (71)

where \(K_{X_{1}\pm X_{2}|X_{1}^{\prime},X_{2}^{\prime}}=K_{X_{1}X_{2}}(K_{X_{1}^{ \prime}X_{2}^{\prime}}+\lambda\mathbf{I})^{-1}K_{X_{1}^{\prime}X_{2}^{\prime}}\) and \(\left[K_{X_{1}X_{2}|X_{1}^{\prime},X_{2}^{\prime}}\right]_{ii}\) is the \(i\)th row and \(i\)th column of \(K_{X_{1}\pm X_{2}|X_{1}^{\prime},X_{2}^{\prime}}\). \(K_{X_{1}X_{2}}\) is a kernel similarity matrix between \(X_{1}\) and \(X_{2}\), and \(K_{X_{1}^{\prime}X_{2}^{\prime}}\) is a separate kernel similarity matrix between \(X_{1}^{\prime}\) and \(X_{2}^{\prime}\). \(f^{*}\) is the optimal solution of Eq.(70). By leveraging the similarity \(K_{X_{1}^{\prime}X_{2}^{\prime}}\) between conditional variables \(X_{1}^{\prime}\) and \(X_{2}^{\prime}\), \(K_{X_{1}AX_{2}|X_{1}^{\prime},X_{2}^{\prime}}\) transforms the similarity scores between \(X_{1}\) and \(X_{2}\) under unconditional distributions into similarity scores under conditional distributions. Note that the expectations in Eqs.(70) and (71) are taken over the joint distribution \(p(x_{1},x_{2},x_{1}^{\prime},x_{2}^{\prime})\), which comes naturally after augmenting both modalities \(X_{1}\) and \(X_{2}\). This method could effectively alleviate the problem of sampling from conditional distributions in our formulation. We refer the reader to Tsai et al. [78] for more details.

### Final estimators in FactorCL

**Theorem 6**.: _(Contrastive estimators for shared and unique information). Under assumptions on single-view augmentations \(I(X_{1};Y)=I(X_{1},X_{1}^{\prime})\) (Definition 8) and optimal multi-view augmentation \(X_{2}^{\prime}\) such that \(I(X_{1},X_{2};X_{1}^{\prime},X_{2}^{\prime})=I(X_{1},X_{2};Y)\) (Definition 9), we can define contrastive objectives for task-relevant shared and unique information with:_

\[S =I(X_{1};X_{2};Y)\geq I_{\text{NCE}}(X_{1};X_{2})-I_{\text{NCE-CLUB }}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime})\] (72) \[U_{i} =I(X_{i};Y|X_{-i})\geq I_{\text{NCE}}(X_{i};X_{i}^{\prime})-I_{ \text{NCE-CLUB}}(X_{1};X_{2})+I_{\text{NCE}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^ {\prime})\] (73)

Proof.: The objectives follow from the fact that \(I_{\text{NCE}}(X_{1};X_{2})\) and \(I_{\text{NCE}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime})\) are lower bounds of \(I(X_{1};X_{2})\) and \(I(X_{1};X_{2}|Y)\) respectively, and \(I_{\text{NCE-CLUB}}(X_{1};X_{2})\) and \(I_{\text{NCE-CLUB}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime})\) are upper bounds of \(I(X_{1};X_{2})\) and \(I(X_{1};X_{2}|Y)\) respectively:

\[S =I(X_{1};X_{2};Y)=I(X_{1};X_{2})-I(X_{1};X_{2}|Y)\] (74) \[\geq I_{\text{NCE}}(X_{1};X_{2})-I_{\text{NCE-CLUB}}(X_{1};X_{2}| X_{1}^{\prime},X_{2}^{\prime})\] (75) \[U_{i} =I(X_{i};Y|X_{-i})=I(X_{i};Y)-(I(X_{1};X_{2})-I(X_{1};X_{2}|Y))\] (76) \[\geq I_{\text{NCE}}(X_{i};X_{i}^{\prime})-(I_{\text{NCE-CLUB}}(X_ {1};X_{2})-I_{\text{NCE}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime}))\] (77)

and symmetrically for \(U_{2}\). 

Now we show that FactorCL learns both shared and unique task-relevant information. First, we restate the definition of the factorized representations:

\[Z_{S_{1}} =\operatorname*{arg\,max}_{Z_{1}=f_{\theta}(X_{1})}I(Z_{1};X_{2};Y), Z_{S_{2}} =\operatorname*{arg\,max}_{Z_{2}=f_{\theta}(X_{2})}I(Z_{2};X_{1};Y),\] (78) \[Z_{U_{1}} =\operatorname*{arg\,max}_{Z_{1}=f_{\theta}(X_{1})}I(Z_{1};Y|X_{2}), Z_{U_{2}} =\operatorname*{arg\,max}_{Z_{2}=f_{\theta}(X_{2})}I(Z_{2};Y|X_{1}).\] (79)

where \(I(Z_{1};X_{2};Y)=I(Z_{1};X_{2})-I(Z_{1};X_{2}|Y)\) is the shared information and \(I(Z_{2};X_{1};Y)=I(Z_{2};X_{2})-I(Z_{2};X_{1}|Y)\) is the unique information.

**Theorem 7**.: _(Optimality of FactorCL) If \(Z_{S_{1}},Z_{S_{2}},Z_{U_{1}},Z_{U_{2}}\) perfectly maximize Eqs.(78-79) and the estimations in Eqs.(13-67) are tight, we obtain \(I(X_{1},X_{2};Y)=I(Z_{S_{1}};Z_{S_{2}};Y)+I(Z_{U_{1}};Y|Z_{S_{2}})+I(Z_{U_{2}};Y| Z_{S_{1}})\), suggesting that FactorCL learns both shared and unique task-relevant information._

Proof.: Because \(I(X_{1},X_{2};Y)=I(X_{1};X_{2};Y)+I(X_{1};Y|X_{2})+I(X_{2};Y|X_{1})\), it is sufficient to show that \(I(X_{1};X_{2};Y)=I(Z_{S_{1}};Z_{S_{2}};Y),I(X_{1};Y|X_{2})=I(Z_{U_{1}};Y|Z_{S_{2}})\) and \(I(X_{2};Y|X_{1})=I(Z_{U_{2}};Y|Z_{S_{1}})\).

First we show \(I(X_{1};X_{2};Y)=I(Z_{S_{1}};Z_{S_{2}};Y)\). Crucially, by definition of how \(Z_{S_{1}}\) and \(Z_{S_{2}}\) are optimized to maximize \(I(X_{1};X_{2};Y)\), we have that:

\[I(X_{1};X_{2};Y)=I(Z_{S_{1}};X_{2};Y)=I(Z_{S_{2}};X_{1};Y).\] (80)

We can then obtain

\[I(X_{1};X_{2};Y) =I(X_{1};Z_{S_{2}};Y)\] (81) \[=I(X_{1};Z_{S_{2}};Y|Z_{S_{1}})+I(Z_{S_{1}};Z_{S_{2}};X_{1};Y)\] (82) \[=I(Z_{S_{2}};Y|Z_{S_{1}})-I(Z_{S_{2}};Y|Z_{S_{1}},X_{1})+I(Z_{S_{1 }};Z_{S_{2}};X_{1};Y)\] (83) \[=I(Z_{S_{2}};Y|Z_{S_{1}})-I(Z_{S_{2}};Y|X_{1})+I(Z_{S_{1}};Z_{S_{2 }};X_{1};Y)\] (84) \[=I(Z_{S_{2}};Y|Z_{S_{1}})-I(Z_{S_{2}};Y|X_{1})+I(Z_{S_{1}};Z_{S_{2 }};Y)\] (85) \[=I(Z_{S_{2}};Y|Z_{S_{1}})+I(Z_{S_{1}};Z_{S_{2}};Y)\] (86) \[=I(Z_{S_{1}};Z_{S_{2}};Y)\] (87)

where Eq.(84) is because \(Z_{S_{1}}\) are deterministically obtained from \(S_{1}\) and Eq.(85) is because \(Z_{S_{1}}\) maximizes the shared information. Finally, we go to Eq.(87) \(I(Z_{S_{2}};Y|Z_{S_{1}})=0\) as shown in Eqs.(26-33) using the fact that \(Z_{S_{1}}\) is learned to maximize \(I(X_{1};X_{2};Y)\) and \(I(Z_{S_{1}};X_{2};Y)=I(X_{1};Z_{S_{2}};Y)\).

Next, we show \(I(X_{1};Y|X_{2})=I(Z_{U_{1}};Y|Z_{S_{2}})\):

\[I(Z_{U_{1}};Y|Z_{S_{2}})=I(Z_{U_{1}};Y|Z_{S_{2}},Z_{U_{2}})+I(Z_{U_{1}};Y;Z_{U _{2}}|Z_{S_{2}}),\] (88)

which is by the chain rule of conditional mutual information. Then we show \(I(Z_{U_{1}};Y;Z_{U_{2}}|Z_{S_{2}})=0\):

\[I(Z_{U_{1}};Y;Z_{U_{2}}|Z_{S_{2}})=I(Z_{U_{1}};Z_{U_{2}}|Z_{S_{2}})-I(Z_{U_{1} };Z_{U_{2}}|Y;Z_{S_{2}})=0-0=0\] (89)

This is because Eq.(79) leads to \(I(Z_{U_{1}};Y|X_{2})=I(X_{1};Y|X_{2})\) and \(I(Z_{U_{2}};Y|X_{1})=I(X_{2};Y|X_{1})\). If the estimations in Eqs.(13-67) are tight, by conditioning and by the previously stated \(I(Z_{U_{1}};Y|X_{2})=I(X_{1};Y|X_{2})\), \(Z_{U_{1}}\) tightly captures information from only \(X_{1}\) and not in \(X_{2}\). The same applies to \(Z_{U_{2}}\). We have \(I(Z_{U_{1}};X_{2})=I(Z_{U_{2}};X_{1})=I(Z_{U_{1}};Z_{U_{2}})=I(Z_{U_{1}};Z_{U_ {2}}|T)=0\) with \(T\) being an arbitrary random variable because no shared information exists between \(Z_{U_{1}}\) and \(Z_{U_{2}}\). Then we obtain:

\[I(Z_{U_{1}};Y|Z_{S_{2}},Z_{U_{2}}) =I(Z_{U_{1}};Y|Z_{S_{2}},Z_{U_{2}},X_{2})+I(Z_{U_{1}};Y;X_{2}|Z_{S _{2}},Z_{U_{2}})\] (90) \[=I(Z_{U_{1}};Y|X_{2})\] (91)

We use the fact that conditioning on \(Z_{S_{2}},Z_{U_{2}}\) and \(X_{2}\) jointly reduces to conditioning on \(X_{2}\) since \(Z_{S_{2}}\) and \(Z_{U_{2}}\) are deterministically obtained from \(X_{2}\). Lastly, since Eqs.(78-79) are satisfied, \(Z_{U_{1}}=\arg\max_{Z_{1}=f_{0}(X_{1})}I(Z_{1};Y|X_{2})\) therefore \(I(Z_{U_{1}};Y|X_{2})=I(X_{1};Y|X_{2})\). We have:

\[I(Z_{U_{1}};Y|Z_{S_{2}})=I(Z_{U_{1}};Y|X_{2})=I(X_{1};Y|X_{2}).\] (92)

The proof for \(I(X_{2};Y|X_{1})=I(Z_{U_{2}};Y|Z_{S_{1}})\) is similar. We now have shown that \(I(X_{1};X_{2};Y)=I(Z_{S_{1}};Z_{S_{2}};Y)\), \(I(X_{1};Y|X_{2})=I(Z_{U_{1}};Y|Z_{S_{2}})\) and \(I(X_{2};Y|X_{1})=I(Z_{U_{2}};Y|Z_{S_{1}})\), adding up all LHS and RHS we have the theorem. 

### Extensions to masking and non-contrastive learning

We now show how similar ideas can be extended to other popular self-supervised learning objectives, such as non-contrastive learning [94; 6] and masked pre-training [20; 29]. Importantly, this paper provides a new principle for multimodal self-supervised learning: (1) learning task-relevant information and (2) removing task-irrelevant information from both shared and unique parts across modalities. Our paper focuses on realizing this principle via multi-view information theory and contrastive learning. Below we provide two potential alternatives to realize this principle on non-contrastive and masking methods, respectively:

**Non-contrastive learning:** Methods such as Barlow Twins [94] and VICReg [6] use invariance and covariance regularizations to maximally preserve shared information in the embeddings across two modalities. However, the embeddings learned may contain only contain task-relevant information from the shared part and not unique parts. To use the principle in this paper to capture more task-relevant information from unique parts, one should perform VIC-regularization on \(X_{1}\) features, on \(X_{2}\) features, and on \(X_{1},X_{2}\) cross-modal features. When performing VICReg on unimodal features, one should condition on the other modality when performing augmentation. Specifically, similar to the idea of multimodal augmentation in this paper, the augmentation of the second modality should not interfere with the shared part (i.e., do not augment regions referred to by the first modality), making the invariance and covariance regularization of the second modality focus on the augmented modality-unique features. This makes the model learn unique modality features that are not captured by the joint embedding from standard independent augmentations.

**Masking:** Conceptually, masking [20; 29] can be interpreted as leveraging unmasked regions in the same modality to predict masked regions or leveraging the other modality to predict the masked region. However, the learned representation may not be all task-relevant. To use the principle in this paper to exclude task-irrelevant information and capture more task-relevant information from unique parts, we can perform conditional masking, where masking is conditioned on augmented views (similar to the multimodal augmentation in the paper, where the conditioned views are approximating the labels). As a result, only unique regions in the second modality can be masked out, making the model capture more unique information from the second modality by masked prediction. Here we have only provided high-level intuitions of extensions to these methods, and future work should explore these ideas in more detail.

## Appendix D Experimental Details

### Implementation details

**Objective Formulation and Architecture**

In Algorithm 2 in the main text, we see the sketch for doing contrastive learning with our proposed objectives. To implement all algorithms used in our ablation experiments, we start with two encoders \(e_{1}(\cdot)\) and \(e_{2}(\cdot)\), which takes samples \(x_{1}\) and \(x_{2}\) from the modalities \(X_{1}\) and \(X_{2}\), and outputs corresponding representations \(z_{1}\) and \(z_{2}\). We also have a critic function \(f_{\theta}(\cdot,\cdot)\) parametrized by \(\theta\) which takes \(z_{1}\) and \(z_{2}\) as inputs and returns a scalar. A popular way to perform contrastive learning aims to maximize \(I_{\text{NCE}}(X_{1};X_{2})\), where

\[I_{\text{NCE}}(X_{1};X_{2})=\mathbb{E}_{\begin{subarray}{c}x_{1},x_{2}^{+}p( x_{1},x_{2})\\ x_{2}^{-}p(x_{2})\end{subarray}}\left[\log\frac{\exp f_{\theta}(e_{1}(x_{1}),e _{2}(x_{2}^{+}))}{\sum_{k}\exp f_{\theta}(e_{1}(x_{1}),e_{2}(x_{2}^{-}))} \right].\] (93)

In our algorithms, we follow the derivations in Eqs.(8-9) to maximize each \(I_{\text{NCE}}\) objective and minimize each \(I_{\text{NCE-CLUB}}\) objective. Therefore, for each objective, we add two additional MLP heads on top of the two encoders and create a separate critic which takes in the outputs of the MLP heads instead of the encoders. In all the experiments, we adopt the concat critic design [58; 60; 68], where \(f_{\theta}(x,y)=h_{\theta}([x,y])\) with \(h_{\theta}\) being an MLP.

**FactorCL-SUP**: In the supervised version of CL which uses label \(Y\), the objective we aim to maximize is formulated as

\[\mathcal{L}_{\text{FACTORCL-SUP}} =I_{\text{NCE}}(X_{1};X_{2})-I_{\text{NCE-CLUB}}(X_{1};X_{2}|Y)\] (94) \[+I_{\text{NCE}}(X_{1};Y)+I_{\text{NCE}}(X_{2};Y)\] (95) \[-I_{\text{NCE-CLUB}}(X_{1};X_{2})+I_{\text{NCE}}(X_{1};X_{2}|Y).\] (96)

Each \(I_{\text{NCE}}\) and \(I_{\text{NCE-CLUB}}\) term in this objective is calculated using its own critic as discussed above. The conditional terms involving the label \(Y\) are implicitly modeled by directly concatenating \(Y\) to the outputs of both heads before feeding into the critic. To obtain the learned representations \(Z_{S_{1}}\), we concatenate the outputs of the heads on top of the encoder \(e_{1}\) that correspond to the terms \(I_{\text{NCE}}(X_{1};X_{2})\) and \(I_{\text{NCE-CLUB}}(X_{1};X_{2}|Y)\). To obtain \(Z_{U_{1}}\), we concatenate \(e_{1}\)'s head outputs from the terms \(I_{\text{NCE}}(X_{1};Y)\), \(I_{\text{NCE-CLUB}}(X_{1};X_{2})\), and \(I_{\text{NCE}}(X_{1};X_{2}|Y)\). \(Z_{S_{2}}\) and \(Z_{U_{2}}\) are obtained in a similar fashion, except we use the outputs from \(e_{2}\)'s heads instead of \(e_{1}\).

**FactorCL-SSL**: In the self-supervised version of CL which uses augmentations \(X_{1}^{\prime}\) and \(X_{2}^{\prime}\) of the input modalities, the objective we aim to maximize is formulated as

\[\mathcal{L}_{\text{FACTORCL-SSL}} =I_{\text{NCE}}(X_{1};X_{2})-I_{\text{NCE-CLUB}}(X_{1};X_{2}|X_{1 }^{\prime},X_{2}^{\prime})\] (97) \[+I_{\text{NCE}}(X_{1};X_{1}^{\prime})+I_{\text{NCE}}(X_{2};X_{2}^ {\prime})\] (98) \[-I_{\text{NCE-CLUB}}(X_{1};X_{2})+I_{\text{NCE}}(X_{1};X_{2}|X_{1 }^{\prime},X_{2}^{\prime}).\] (99)Here the conditional terms are conditioned on the augmentations \(X_{1}^{\prime}\) and \(X_{2}^{\prime}\), and we can similarly model it by concatenating the head outputs of \(X_{1}^{\prime}\) to \(X_{1}\) and the head outputs of \(X_{2}^{\prime}\) to \(X_{2}\) before feeding into the critic. We use Figure 5 to illustrate this. The way to obtain the learned representations is the same as described in FactorCL-SUP.

**Estimation of CMI**: To estimate the conditional mutual information (CMI) \(I(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime})\), we can estimate the lower or upper bounds of true CMI [51, 55, 69]. However, direct sampling from the conditional distribution \(p(x_{1},x_{2}|x_{1}^{\prime},x_{2}^{\prime})\) can be expensive because we should consider a different conditional distribution \(p(x_{1},x_{2}|x_{1}^{\prime},x_{2}^{\prime})\) for each data pair \(x_{1}^{\prime},x_{2}^{\prime}\). Sordoni et al. [69] address this by concatenating the conditioning variable with the input in the critic: \(\phi(x_{1},x_{2},c)\), and showing that Conditional InfoNCE (Eq.(15) is a lower bound and estimator of CMI. This estimator can be made more exact by further importance sampling [69]. However, adding importance sampling [69] or using more accurate estimators [26] comes with a trade-off in complexity. Since we focus on capturing unique information to learn a scalable multimodal representation instead of accurately estimating the CMI, we leveraged a simpler version of the estimator from Sordoni et al. [69]: generating multiple augmented pairs from \(x_{1},x_{2}\), and concatenating the input \(x_{1},x_{2}\) and each augmented pair \(x_{1}^{\prime},x_{2}^{\prime}\) to define samples from the conditional distribution \(p(x_{1},x_{2}|x_{1}^{\prime},x_{2}^{\prime})\). We argue that since augmentations do not significantly change the semantics of images, \(p(x_{1},x_{2}|x_{1}^{\prime},x_{2}^{\prime})\) could be approximated by \(p(x_{1}^{\prime\prime},x_{2}^{\prime\prime}|x_{1}^{\prime},x_{2}^{\prime})\) where \(x_{1}^{\prime\prime},x_{2}^{\prime\prime}\) are other augmented pairs in addition to \(x_{1}^{\prime},x_{2}^{\prime}\). In this submission, we use one pair of augmented samples for consistency, but our code easily supports increasing the number of augmented pairs that can improve the accuracy of CMI estimation.

Regardless, our existing one-pair implementation can already show that our estimators are empirically comparable to CMI estimators with guarantees such as Mukherjee et al. [55] (Table 9), and our estimators empirically satisfy that the lower bound is smaller than the true CMI, and the true CMI smaller than the upper bound, i.e., Conditional InfoNCE \(\leq\) true CMI \(\leq\) Conditional InfoNCE-CLUB (also in Table 9). We refer the reader to Sordoni et al. [69] for tighter bounds for CMI.

**OurCL-SUP**: For this ablation, we remove the factorization and only learn \(Z_{1}\) for \(X_{1}\) and \(Z_{2}\) for \(X_{2}\). The objective we use is the same as that of FactorCL-SUP. The only difference is that we now take \(e_{1}(x_{1})\) and \(e_{2}(x_{2})\) as the learned representations for inputs \(x_{1}\) and \(x_{2}\).

**OurCL-SSL**: This is a similar ablation for FactorCL-SSL where we remove the factorization. The objective is the same as that of FactorCL-SSL and we use \(e_{1}(x_{1})\) and \(e_{2}(x_{2})\) as the learned representations for inputs \(x_{1}\) and \(x_{2}\).

**Training Strategy**: In regular contrastive learning using \(I_{\text{NCE}}\) as the only objective, we can simply perform gradient descent to minimize \(I_{\text{NCE}}\), updating all parameters in the encoders, MLP heads, and critics. However, training any of the four methods above also involves the minimization of the \(I_{\text{NCE-CLUB}}\) objectives, which require the optimal critic \(f^{*}\) from \(I_{\text{NCE}}\), as stated in Eq.(11). Therefore, within each iteration during our training, we need to first obtain the optimal critics for the \(I_{\text{NCE-CLUB}}\) terms using the \(I_{\text{NCE}}\) objective. We outline the training strategy using a sampling method in Algorithm 3. In this algorithm, \(\mathcal{L}_{\text{FactorCL}}\) can be either \(\mathcal{L}_{\text{FactorCL-}SUP}\) or \(\mathcal{L}_{\text{FACTORCL-}SSL}\), and \(\mathcal{L}_{\text{NCE}}\) is the summation of \(I_{\text{NCE}}\) objectives for the \(I_{\text{NCE-CLUB}}\) terms. In particular, we have

\[\mathcal{L}_{\text{NCE}}=\begin{cases}I_{\text{NCE}}(X_{1};X_{2}|Y)+I_{\text{ NCE}}(X_{1};X_{2}),&\text{if }\mathcal{L}=\mathcal{L}_{\text{FACTORCL-}SUP}\ ;\\ I_{\text{NCE}}(X_{1};X_{2}|X_{1}^{\prime},X_{2}^{\prime})+I_{\text{NCE}}(X_{1}; X_{2}),&\text{if }\mathcal{L}=\mathcal{L}_{\text{FACTORCL-}SSL}\.\end{cases}\] (100)

Figure 5: An illustration of conditioning by concatenation in the implementation of FactorCL. Conditioning is done by concatenating \(Z_{1}\), the encoded representation of \(X_{1}\), and \(Z_{1}^{\prime}\), the encoded representation of \(X_{1}^{\prime}\). A similar operation is performed for \(X_{2}\) and \(X_{2}^{\prime}\). The concatenated vectors are then fed to MI estimators, such as \(I_{\text{NCE}}\) and \(I_{\text{NCE-CLUB}}\) (the figure illustrates \(I_{\text{NCE}}\)).

We define \(\phi\) to be the parameters of critics for the \(I_{\text{NCE-CLUB}}\) terms, and \(\theta\) corresponds to all the rest parameters in the network (parameters of encoders, heads, and critics for \(I_{\text{NCE}}\) terms). In the outer loop, we update \(\theta\) using the main learning objective \(\mathcal{L}\). In the inner loop, we update \(\phi\) using the \(\mathcal{L}_{\text{NCE}}\) objective, which learns the optimal critics \(f^{*}\) needed to compute the \(I_{\text{NCE-CLUB}}\) terms. Ideally in the inner loop we would update \(\phi\) until convergence so we get a good approximation to the optimal critic. In practice we found sampling just one batch by setting \(k=1\) in Algorithm 3 works pretty well. Using only one iteration does not have a big impact on the convergence and still produces promising results. More importantly, it significantly reduces the time required for training, and allows our algorithms to have comparable running time to existing contrastive learning methods.

### Datasets

**Gaussian datasets for MI estimation**: As shown in Figure 3 in the main text, we first demonstrate the quality of our proposed upper bounds \(I_{\text{NCE-CLUB}}(X_{1};X_{2})\) on a toy Gaussian dataset. We obtain the samples \(\{(x_{i},y_{i})\}\) from 4 multivariate Gaussian distribution with dimensions {20, 50, 100, 200}. In each dataset, we set the ground truth MI values to be {2, 4, 6, 8, 10}, and so we can compute the correlation \(\rho\) needed for achieving these MI values using the ground truth MI formula for Multivariate Gaussian: \(I(X,Y)=-\frac{d}{2}\log(1-\rho^{2})\). At each true MI value we sample 4000 times using a batch size of 64.

**Synthetic dataset with controlled generation**: We generate data with controllable ratios of task-relevant shared and unique information to analyze the behavior of each contrastive learning objective in Figure 1 in the main text. Starting with a set of latent vectors \(w_{1},w_{2},w_{s}\sim\mathcal{N}(0_{d},\Sigma_{d}^{2}),d=50\) representing information unique to \(X_{1},X_{2}\) and common to both respectively, the concatenated vector \([w_{1},w_{s}]\) is transformed into high-dimensional \(x_{1}\) using a fixed full-rank transformation \(T_{1}\) and likewise \([w_{2},w_{s}]\) to \(x_{2}\) via \(T_{2}\). The label \(y\) is generated as a function (with nonlinearity and noise) of varying ratios of \(w_{s}\), \(w_{1}\), and \(w_{2}\) to represent shared and unique task-relevant information. For experiments, we used 1-layer MLPs with 512 hidden size as encoders, and the embedding dimensions are 128 for both modalities. The heads on top of encoders are also 1-layer MLPs with the same hidden and output dimension as the input, and all critics are 1-layer MLPs with 512 hidden size.

**Multimodal fusion datasets**: We use a collection of 5 real-world datasets provided in MultiBench [45] and the IRFL dataset to test our method in the context of varying ratios of shared and unique information that is important for the task. In all the datasets below, the heads added on top of the encoders are 1-Layer MLPs with ReLU activations that map the encoder outputs to the same dimensions. All critics are also MLPs with 1 hidden layer of size 512 and ReLU activation.

1. **MIMIC-III**[38] (Medical Information Mart for Intensive Care III) is a large-scale dataset for healthcare which contains records of over 40,000 ICU patients in both forms of times-series data measured by hours and static data (age, gender, ethnicity) in the tabular numerical form. We use the preprocessed data provided in MultiBench [45], where the time-series data is measured every 1 hour in a 24-hour period and consists of vectors of size 12, and the tabular data consists of vectors of size 5. The task we use in the experiment is a binary classification on whether the patient fits any ICD-9 code in group 7 (460-519).

In the experiments, we use a 2-layer MLP with 10 hidden layer size for the tabular data modality, and map it to a vector of size 10. The time-series modality is encoded using a GRU with hidden size 30 and followed by a linear layer which projects the output to embeddings of size 15. We train the model for 100 epochs using the Adam optimizer with a 1e-4 learning rate.
2. **CMU-MOSEI**[93] is the largest sentence-level multimodal sentiment and emotion benchmark with \(23,000\) monologue videos. It contains more than 65 hours of annotated video from more than 1,000 speakers and 250 topics. Each video is labeled with a sentiment intensity ranging from -3 to 3. In our experiments, we cast the intensity values to a binary classification on whether the sentiment is positive or negative. MultiBench [45] provides access to the extracted features of the vision, text, and audio modalities, and in our experiments, we use the vision and text features for doing contrastive learning. In our experiments, we encode both the vision and text modalities using Transformer encoders with 5 heads and 5 layers, and map them to 40-dimensional embeddings. We train the model for 100 epochs using the Adam optimizer with a 1e-4 learning rate.
3. **CMU-MOSEI**[91] is a similar dataset for multimodal sentiment analysis created from \(2,199\) YouTube videos clips. The data focuses on videos that reflect the real-world distribution of speakers expressing their opinions in the form of monologues. The sentiment intensities are labeled continuously from -3 to 3. Again we cast the label into a binary classification on whether the sentiment is positive or negative, and we used the extracted vision and text features for contrastive learning. In our experiments we encode both the vision and text modalities using Transformer encoders with 5 heads and 5 layers, and map them to 40-dimensional embeddings. We train the model for 100 epochs using the Adam optimizer with a 1e-4 learning rate.
4. **UR-FUNNY**[27] is the first large-scale dataset for humor detection in human speech. The dataset consists of samples from more than \(16,000\) TED talk videos with speakers from diverse backgrounds sharing their ideas. The laughter markup is used to filter out 8,257 humorous punchlines from the transcripts. The context is extracted from the prior sentences to the punchline. Using a similar approach, 8,257 negative samples are chosen at random intervals where the last sentence is not immediately followed by a laughter marker. The task is to classify whether there is humor or not using the vision and text modalities.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Model & MIMIC & MOSEI & MOSI & UR-FUNNY & MUSTARD \\ \hline SimCLR [13] & 66.7 \(\pm\) 0.0\% & 71.9 \(\pm\) 0.3\% & 47.8 \(\pm\) 1.8\% & 50.1 \(\pm\) 1.9 \% & 53.5 \(\pm\) 2.9\% \\ Cross+Self [81] & 65.2 \(\pm\) 0.0\% & 71.1 \(\pm\) 0.2\% & 48.6 \(\pm\) 1.2\% & 56.5 \(\pm\) 0.7\% & 53.9 \(\pm\) 4.5\% \\ Cross+Self+Fact [89] & 65.5 \(\pm\) 0.0\% & 71.9 \(\pm\) 0.2\% & 49.0 \(\pm\) 1.1\% & 59.9 \(\pm\) 0.9\% & 53.9 \(\pm\) 4.0\% \\ OurCL-SSL & 65.2 \(\pm\) 0.0\% & 71.2 \(\pm\) 0.2\% & 49.0 \(\pm\) 0.8\% & 58.8 \(\pm\) 1.3\% & 54.0 \(\pm\) 2.5\% \\ FactorCL-SSL & **67.3 \(\pm\) 0.0\%** & **74.5 \(\pm\) 0.1\%** & **51.2 \(\pm\) 1.6\%** & **60.5 \(\pm\) 0.8\%** & **55.8 \(\pm\) 0.9\%** \\ \hline SupCon [41] & 67.4 \(\pm\) 0.0\% & 71.0 \(\pm\) 0.1\% & 47.2 \(\pm\) 1.2\% & 50.1 \(\pm\) 2.0\% & 52.7 \(\pm\) 2.2\% \\ OurCL-SUP & 68.2 \(\pm\) 0.0\% & 71.1 \(\pm\) 0.2\% & 65.3 \(\pm\) 0.8\% & 58.3 \(\pm\) 1.1\% & 65.1 \(\pm\) 1.8\% \\ FactorCL-SUP & **76.8 \(\pm\) 0.0\%** & **77.8 \(\pm\) 0.3\%** & **69.1 \(\pm\) 0.6\%** & **63.5 \(\pm\) 0.8\%** & **69.9 \(\pm\) 1.9\%** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results on MultiBench [45] datasets with varying shared and unique information: FactorCL achieves strong results vs self-supervised (top \(5\) rows) and supervised (bottom \(3\) rows) baselines that do not have unique representations, factorization, upper-bounds to remove irrelevant information, and multimodal augmentations.

\begin{table}
\begin{tabular}{l|c} \hline \hline Task & IRFL \\ \hline Zero-shot CLIP [61] & 89.2 \(\pm\) 0.0\% \\ SimCLR [13] & 91.6 \(\pm\) 0.0\% \\ Cross+Self [81, 89] & 91.1 \(\pm\) 1.2\% \\ FactorCL-IndAug & 91.6 \(\pm\) 1.3\% \\ FactorCL-SSL & **93.8 \(\pm\) 1.4\%** \\ \hline Fine-tuned CLIP [61] & 96.4 \(\pm\) 0.0\% \\ SupCon [41] & 87.7 \(\pm\) 4.7\% \\ FactorCL-SUP & **98.3 \(\pm\) 1.2\%** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Continued pre-training on CLIP with our FactorCL objectives on classifying images and figurative language. Our approach shows strong results as compared to standard fine-tuning and continued pre-training.

[MISSING_PAGE_FAIL:27]

augmentations, we use cropping, flipping, and color jittering. The unique augmentation simply removes the cropping operation, as illustrated in Figure 4 in the main text.

**Additional experiments on high shared information and low unique information**: In Table 8 we include additional results using our method on the CIFAR10 [43] and MNIST [19] datasets. Our method outperforms the self-supervised contrastive learning on both datasets as expected, and roughly maintains the same performance as supervised contrastive learning. Therefore, in cases with abundant shared information (two modalities with high shared information or two different views generated from augmentations), our method recovers the performance of existing methods that do not capture unique information.

**Experiments on CMI estimator verification**: In Table 9 and Table 10, we include experiment results which verify that computing the conditional MI lower and upper bounds via concatenation indeed yields reliable estimates. In particular, we aim to verify that the the Conditional InfoNCE objective gives a lower bound of the CMI, and the Conditional InfoNCE-CLUB objective gives an upper bound of the CMI. We follow the experiment setups in [55], presenting the true CMI and results of our estimators on synthetic data with fixing dimension of representation \(Z\) and varying samples \(n\), and fixing samples \(n\) and varying \(d_{z}\). The specific implementations used for conditional InfoNCE and conditional InfoNCE-CLUB can be found in Equation 13 and Equation 14, respectively. The results indicate that our Conditional InfoNCE gives estimations smaller than the true CMI, and Conditional InfoNCE-CLUB gives estimations greater than the true CMI. The performances are comparable to estimators in [55], suggesting that our method yields valid and competitive lower and upper bounds for CMI.

**Empirical verification on InfoMin assumption**: To verify the InfoMin assumption [72] (\(I(Z_{1};Y|X_{2})=I(Z_{2};Y|X_{1})=0\)), we use the same synthetic dataset as in Table 1 and measure \(I(Z_{1};Y|X_{2})\). The results are shown in Table 11: we get \(I(X_{1};X_{2})=12.29\) and \(I(Z_{1};Y|X_{2})=0.4\). \(I(Z_{1};Y|X_{2})\) is much smaller and closer to zero than \(I(Z_{1};Y|X_{2})\), indicating that the InfoMin assumption holds in practice.

**Compute resources**: All experiments in this paper are run on a single NVIDIA A100 GPU. It takes about 10 to 12 GPU hours to train the model on the CIFAR10 [43] for 300 epochs, and all the other experiments can be finished within 1 GPU hour using our specified hyperparameters.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Dimension \(d_{z}\), \(n=2\times 10^{4}\) & 1 & 10 & 20 & 50 & 100 \\ \hline CCMI (MI-Diff + Classifier) & 2.30 & 2.18 & 2.15 & 1.98 & 1.67 \\ Conditional InfoNCE & 2.18 & 2.20 & 2.20 & 2.26 & 2.30 \\ Conditional InfoNCE-CLUB & 3.70 & 2.95 & 2.98 & 2.79 & 2.86 \\ True CMI & 2.32 & 2.32 & 2.32 & 2.32 & 2.32 \\ \hline \hline \end{tabular}
\end{table}
Table 10: We verify our conditional lower and upper bound estimators on a synthetic dataset with varying dimensions of representation \(d_{z}\) and fixed number of samples \(n\).

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline \(I(X_{1},Y;X_{2})\) & \(I(X_{1};X_{2})\) & \(I(Z_{1};Y|X_{2})\) & \(I(X_{2},Y;X_{1})\) & \(I(X_{2};X_{1})\) & \(I(Z_{2};Y|X_{1})\) \\ \hline
12.69 & 12.29 & 0.40 & 11.31 & 10.92 & 0.38 \\ \hline \hline \end{tabular}
\end{table}
Table 11: We probe whether the InfoMin assumption from Tian et al., \(I(Z_{1};Y|X_{2})=0\) and \(I(Z_{2};Y|X_{1})=0\), is reasonable for Theorem 1. Compared to the shared information \(I(X_{1};X_{2})\), \(I(Z_{1};Y|X_{2})\) is much smaller and closer to zero, indicating that the InfoMin assumption is reasonable, and Theorem 1 holds in practice.