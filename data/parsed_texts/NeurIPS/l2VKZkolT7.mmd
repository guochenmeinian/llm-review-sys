# Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples

 Marco Jiralerspong

Universite de Montreal and Mila

&Avishek (Joey) Bose

McGill University and Mila

&Ian Gemp

Google Deepmind

&Chongli Qin

Google Deepmind

&Yoram Bachrach

Google Deepmind

&Gauthier Gidel

Universite de Montreal and Mila

e-mail correspondence to marco.jiralerspong@mila.quebec

###### Abstract

The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Divergence (FLD), a parametric sample-based metric that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLD to identify overfitting problem cases, even when previously proposed metrics fail. We also extensively evaluate FLD on various image datasets and model classes, demonstrating its ability to match intuitions of previous metrics like FID while offering a more comprehensive evaluation of generative models. Code is available at https://github.com/marcojira/fld.

## 1 Introduction

Generative modeling is one of the fastest-growing areas of deep learning, with success stories spanning the artificial intelligence spectrum (Karras et al., 2020; Brown et al., 2020; Wu et al., 2021; Rombach et al., 2022). Despite the growth of applications--and unlike supervised or reinforcement learning--there is a lack of a clear consensus on an evaluation protocol in high-dimensional data regimes in which these models excel. In particular, the standard metric of evaluating log-likelihood of held-out test data (Bishop and Nasrabadi, 2006; Goodfellow et al., 2016; Murphy, 2022) fails to provide a meaningful evaluation signal due to its large variability between repetitions (Nowozin et al., 2016) and lack of direct correlation with sample fidelity (Theis et al., 2015).

Departing from pure likelihood-based evaluation, sample-based metrics offer appealing benefits such as being able to evaluate any generative model family via their generated samples. Furthermore, sample-based metrics such as Inception score (IS) (Salimans et al., 2016), Frechet Inception distance (FID) (Heusel et al., 2017), precision, and recall (Lucic et al., 2018; Sajjadi et al., 2018) have been shown to correlate with sample quality, i.e. the perceptual visual quality of a generated sample, and the perceptual sample diversity. Despite being the current defacto gold standard, sample-basedmetrics miss important facets of evaluation (Xu et al., 2018; Esteban et al., 2017; Meehan et al., 2020). For example, on CIFAR10, the current standard FID computation uses 50k generated samples and 50k training samples from the dataset, a practice that does not take into account overfitting. Consider the worst-case scenario of a model called _copycat_ which simply outputs copies of the training samples. Using the standard evaluation protocol, such a model would obtain a _FID of 0_ (as we compare distances of two identical Gaussians)--a perfect score for a useless model. Instead, we could try looking at the FID of the copycat relative to the test set. While this improves the situation somewhat (see Fig. 2), copycat still obtains _better than SOTA test FID_, demonstrating that aiming for the lowest FID is highly vulnerable to overfitting.

While old generative models struggled to produce good quality samples, recent models have demonstrated the ability to memorize (Somepalli et al., 2022) and overfit (Yazici et al., 2020). With the widespread adoption of deep generative models in high-stakes and industrial production environments, important concerns regarding data privacy (Carlini et al., 2023; Arora et al., 2018; Hitaj et al., 2017) should be raised. For instance, in safety-critical application domains such as precision medicine, data leakage in the form of memorization is unacceptable and severely limits the adoption of generative models--a few of which have been empirically shown to be guilty of "digital forgery" (Somepalli et al., 2022). These concerns highlight the limitations with current evaluation metrics:

There are currently no _sample-based evaluation metrics_ accounting for the trichotomy

between sample _fidelity, diversity, and novelty_ (Figure 2).

We believe this trichotomy encompasses what is required for a generative model to have the desired generalization properties, i.e., a "good" generative model should generate samples that are diverse and perceptually indistinguishable from the training data distribution, but _at the same time_, different from that training data. In other words, the more generated data looks like _unseen_ test samples, the better. Additionally, by assessing the novelty of generated samples in relation to the training set, we can better identify potential privacy and copyright risks.

**Main Contribution**. We propose the feature likelihood divergence (FLD): a novel sample-based metric that captures sample fidelity, diversity, and novelty. FLD enjoys the same scalability as popular sample-based metrics such as FID and IS but crucially also assesses sample novelty, overfitting, and memorization. Evaluation using FLD has many consequential benefits:

1. **Explainability:** Samples that contribute the most (and the least) to the performance are identified.
2. **Diagnosing Overfitting:** As overfitting begins (i.e., copying of the training set) FLD identifies the copies and reports an inferior value _despite_ no drop in sample fidelity and diversity.
3. **Holistic Evaluation:** FLD simultaneously is the only metric proposed in the literature that simultaneously evaluates the fidelity, diversity, and novelty of the samples (Fig. 2).
4. **Universal Applicability:** FLD applies to all generative models, including VAEs, Normalizing Flows, GANs, and Diffusion models with minimal overhead as it is computed only using samples.
5. **Flexibility:** Because of its connection with likelihood, FLD can be naturally extended to conditional and multi-modal generative modeling.

Intuitively, FLD achieves these goals by first mapping samples to a perceptually meaningful feature space such as a pre-trained Inception-v3 (Szegedy et al., 2016) or DINOV2 (Oquab et al., 2023). Then, FLD is derived from the likelihood evaluation protocol that assesses the generalization performance of generative models in a similar manner to supervised learning setups. As most models lack explicit densities, we model the density of the generative model in our chosen feature space by using a mixture of isotropic Gaussians (MoG), whose means are the mapped features of the generated samples. We then fit the variances of the Gaussians to the train set in such a way that memorized samples obtain vanishingly small variances and thus worsen the density estimation of the MoG. Finally, we use the MoG and estimate the perceptual likelihood of some held-out test set.

## 2 Background and Related Work

Given a training dataset \(\mathcal{D}_{\text{train}}=\{\mathbf{x}_{i}\}_{i=1}^{n}\) drawn from a distribution \(p_{\text{data}}\), one of the key objectives of generative modeling is to train a parametric model \(g\) that is able to generate novel synthetic yet high-quality samples--i.e., the distribution \(p_{g}\) induced by the generator is close to \(p_{\text{data}}\).3

Footnote 3: By close we mean either a divergence between distributions (e.g. KL, JSD) or a distance like Wasserstein.

**Likelihood Evaluation**. The most common metric, and perhaps most natural, is the negative log-likelihood (NLL) of the test set, whenever it is easily computable. While appealing theoretically, generative models typically do not provide a density (e.g. GANs) or it is only possible to compute a lower bound of the test NLL (e.g. VAEs, continuous diffusion models, etc.) (Burda et al., 2015; Song et al., 2021; Huang et al., 2021). Even when possible, NLL-based evaluation suffers from a variety of pitfalls in high dimensions (Theis et al., 2015; Nowozin et al., 2016) and may often not correlate with higher sample quality (Nalisnick et al., 2018; Le Lan and Dinh, 2021). Indeed many practitioners have empirically witnessed phenomena such as mode-dropping, mode-collapse, and overfitting (Yazici et al., 2020), all of which are not easily captured simply through the NLL.

**Sample-based Metrics**. As all deep generative models are capable of producing samples, an effective way to evaluate these models is via their samples. Such a strategy has the benefit of bypassing the need to compute the exact model density of a sample point--allowing for a unified evaluation setting. More precisely, given \(\mathcal{D}_{\text{gen}}=\{\mathbf{x}^{\text{gen}}\}_{i=1}^{m}\) generated samples, where each \(\mathbf{x}^{\text{gen}}\sim p_{g}\) and \(\mathcal{D}_{\text{test}}=\{\mathbf{x}^{\text{test}}\}_{i=1}^{n}\) drawn from \(p_{\text{data}}\), the goal is to evaluate how "good" the generated samples are with respect to the real data distribution. Historically, sample-based metrics for evaluating deep generative models have been based on two ideas: 1. using an Inception network (Szegedy et al., 2016) backbone \(\varphi\) as a feature extractor to 2. compute a notion of distance (or similarity) between the generated and the real distribution. The Inception Score (IS) and the Frechet Inception Distance (FID) are the two most popular examples and can be computed as follows:

IS: \(e^{\frac{1}{m}\sum_{i=1}^{m}\mathrm{KL}(p_{\varphi}(y|\mathbf{x}_{i}^{\text{ gen}})||p_{\mathcal{U}}(y))}\), FID: \(\|\mu_{g}-\mu_{p}\|^{2}+\mathrm{Tr}(\Sigma_{g}+\Sigma_{p}-2(\Sigma_{g}\Sigma_ {p})^{1/2})\)

where \(p_{\varphi}(y|x)\) is the probability of each class given by the Inception network \(\varphi\), \(p_{d}(y)\) is the ratio of each class in the real data, \(\mu_{g}:=\frac{1}{m}\sum_{i=1}^{m}\varphi(\mathbf{x}_{i}^{\text{gen}}),\mu_{p }:=\frac{1}{n}\sum_{i=1}^{n}\varphi(\mathbf{x}_{i}^{\text{test}})\) are the empirical means of each distribution, and \(\Sigma_{g}:=\frac{1}{m}\sum_{i=1}^{m}(\mathbf{x}_{i}^{\text{gen}}-\mu_{g})( \mathbf{x}_{i}^{\text{test}}-\mu_{g})^{\top}\), \(\Sigma_{p}:=\frac{1}{n}\sum_{i=1}^{n}(\mathbf{x}_{i}^{\text{test}}-\mu_{p})( \mathbf{x}_{i}^{\text{test}}-\mu_{p})^{\top}\) are the empirical covariances.

The popularity of IS and FID as metrics for generative models is motivated by their correlation with perceptual quality, diversity, and ease of use. More recently, other metrics such as KID (Binkowski et al., 2018) (an unbiased version of FID) and precision/recall (which disentangles sample quality and distribution coverage) (Sajjadi et al., 2018) have added nuance to generative model evaluation.

**Overfitting Evaluation**. Several approaches seek to provide metrics to detect overfitting and can be categorized based on whether one can extract an exact likelihood (van den Burg and Williams, 2021) or a lower bound to it via annealed importance sampling (Wu et al., 2016). For GANs, popular approaches include training an additional discriminator in a Wasserstein GAN (Adlam et al., 2019) and adding a memorization score to the FID (Bai et al., 2021). Alternate approaches include finding real data samples that are closest to generated samples via membership attacks (Liu et al., 2018; Webster et al., 2019). Non-parametric tests have also been employed to detect memorization or exact data copying in generative models (Xu et al., 2018; Esteban et al., 2017; Meehan et al., 2020). Parametric approaches to detect data copying have also been explored such as using neural network divergences (Gulrajani et al., 2020) or using latent recovery (Webster et al., 2019). Finally, the [Meehan et al., 2020] test statistic and Alaa et al. [2022] proposes a multi-faceted metric with a binary sample-wise test to determine whether a sample is authentic (i.e., overfit).

## 3 Feature Likelihood Divergence

We now introduce our Feature Likelihood Score (FLD) which is predicated on the belief that a proper evaluation measure for generative models should go beyond sample quality and also inform practitioners of the generalization capabilities of their trained models. While previous sample-based methods have foregone density estimation in favor of computing distances between sample statistics, we seek to bring back a likelihood-based approach to evaluating generative models. To do so, we first propose our method for fitting a mixture of Gaussians (MoGs) to estimate the _perceptual_ density of high-dimensional samples in a way that accounts for _overfitting_. Specifically, our method aims at attributing 1) a good NLL to high-quality, non-overfit images and 2) a poor NLL in cases of overfitting.

Intuitively, we say a generative model is overfitting if on average the distribution of generated samples is closer to the training set than the test set in feature space. We seek to characterize this exact behavior with FLD by a MoG where the variance parameter \(\sigma^{2}\) of each Gaussian approaches zero around generated samples that are responsible for overfitting. In section 3.1 we deepen this intuition by outlining how the MoG used in FLD can be tuned to assess the perceptual likelihood of samples while punishing memorized samples, before giving a precise definition for memorization and overfitting under FLD in section 3.2.

### Overfitting Mixtures of Gaussians

Our method consists of a simple sample-based density estimator amenable to a variety of data domains inspired by a traditional mixture of Gaussians (MoG) density estimator with a few key distinctions. Figure 3 summarizes the \(4\) key steps in computing FLD using our MoG density estimator (also detailed in Algorithm 1 in Appendix SSD.1) which we now describe below. While it is indeed possible to train any other density model, a MoG offers a favorable tradeoff in being simple to use--while still being a universal density estimator [Nguyen et al., 2020]--and enjoying efficient scalability to large datasets.

**Step 1: Map to the feature space**. The first change we make is to use some map \(\varphi\) to map inputs to some perceptually meaningful feature space. Natural choices for this include the representation space of Inception-v3 and DINOv2. While still high-dimensional, we ensure that a larger proportion of dimensions are useful and that the resulting \(\ell_{2}\) distances between images are more meaningful.

**Step 2: Model the density using a MoG.** As in kernel density estimation (KDE), to estimate a density from some set of points \(\mathcal{D}_{\text{gen}}=\{\mathbf{x}_{j}^{\text{gen}}\}_{j=1}^{m}\) we center an isotropic Gaussian around each point--i.e., the mean of the Gaussian is the coordinates of the point. This means that \(j\)-th data point has a Gaussian \(\mathcal{N}(\varphi(\mathbf{x}_{j}^{\text{gen}}),\,\sigma_{j}^{2}I_{d})\). Then, to compute the likelihood of a new point \(\mathbf{x}\), we simply calculate the mean likelihood assigned to that point by all Gaussians in the mixture:

\[p_{\sigma}(\mathbf{x}|\mathcal{D}_{\text{gen}}):=\frac{1}{m}\sum_{j=1}^{m} \mathcal{N}(\varphi(\mathbf{x})|\varphi(\mathbf{x}_{j}^{\text{gen}}),\,\sigma _{j}^{2}I_{d})=\frac{1}{m}\sum_{j=1}^{m}\frac{1}{(\sqrt{2\pi}\sigma_{j})^{d}} \exp\left(\frac{-\|\varphi(\mathbf{x}_{j}^{\text{gen}})-\varphi(\mathbf{x})\| ^{2}}{2\sigma_{j}^{2}}\right)\] (1)

with the convention that \(\mathcal{N}(\varphi(\mathbf{x})|\varphi(\mathbf{x}^{\text{gen}}),\,0_{d})\) is a dirac at \(\varphi(\mathbf{x}^{\text{gen}})\). Henceforth, we denote this MoG estimator which has fixed centers initialized to a dataset (e.g. train set, generated set) as \(\mathcal{N}(\varphi(\mathcal{D});\Sigma)\), where \(\Sigma\) is a diagonal matrix of bandwidths parameters--i.e. \(\sigma^{2}I\), where \(\sigma^{2}\) is a vector.

**Step 3: Use the train set to select \(\sigma_{j}^{2}\)**. An important question in kernel density estimation is selecting an appropriate bandwidth \(\sigma_{j}^{2}\). Overwhelmingly, a single bandwidth is selected which can either be derived statistically or by minimizing some loss through cross validation [Murphy, 2012]. We depart from this single bandwidth philosophy in favor of separate \(\sigma_{j}^{2}\) values for each Gaussian. To select \(\sigma_{j}^{2}\), instead of performing standard cross-validation on samples from \(p_{g}\), we fit the bandwidths using

Figure 3: Steps involved in our overfit mixture of Gaussians illustrated on a 2D example

a subset of training examples \(\{\varphi(\mathbf{x}_{i}^{\text{train}})\}_{i=1}^{n}\) by minimizing their negative log-likelihood (NLL). Specifically, we solve the following optimization problem:

\[\hat{\sigma}^{2}\in\arg\max_{\sigma^{2}}\frac{1}{n}\sum_{i=1}^{n}\log\left( \frac{1}{m}\sum_{j=1}^{m}\frac{1}{(\sqrt{2\pi}\sigma_{j})^{d}}\exp\left(\frac{- ||\varphi(\mathbf{x}_{i}^{\text{train}})-\varphi(\mathbf{x}_{i}^{\text{train} })||^{2}}{2\sigma_{j}^{2}}\right)+\mathcal{L}_{i}\right)\] (2)

where \(\mathcal{L}_{i}\) is a base likelihood given to each sample (see Appendix SSD for details). In particular, by centering each Gaussian on each \(\mathbf{x}_{j}^{\text{gen}}\in\mathcal{D}_{\text{gen}}\) and fitting each \(\sigma_{j}^{2}\) to the train set, we aim to have memorized samples obtain an overly small \(\sigma_{j}^{2}\), worsening the quality of the MoG estimator (and thus penalizing memorization). The following proposition (proof in SSF) formalizes this intuition.

**Proposition 1**.: _Let \(D_{ij}:=||\varphi(\mathbf{x}_{j}^{\text{gen}})-\varphi(\mathbf{x}_{i}^{\text{ train}})||^{2}\) be the distance between a generated sample and a train sample. Assume \(\forall i,j:D_{ij}\leq\hat{D}\) with \(\delta_{j}:=\min_{i}D_{ij}\). Then, for any \(l\in\{1,\dots,m\}\), we have that \(\hat{\sigma}_{l}=O(\delta_{l})\) where \(\hat{\sigma}^{2}\) is a solution of Eq. 2._

Proposition 1 implies that each element of the training set that has been memorized induces a Dirac in the MoG density Eq. 1. Thus, one can identify copies of training samples with the learned density. More generally, if one of the generated samples is unreasonably close to a training sample, its associated \(\sigma^{2}\) will be very small as this maximizes the likelihood of the training sample. We illustrate this phenomenon with the Two-Moons dataset (Pedregosa et al., 2011) in Figure 4. Note that since this dataset is low-dimensional, we do not need to use a feature extractor (Step 1). In Figure 4 we can see that the more approximate copies of the training set appear in the generated set, the more the estimated density (using Eq. 2) contains high values around approximate copies of the training set. As such, overfitted generated samples yield an overfitted MoG that does not model the distribution of real data \(p_{\text{data}}\) and will yield poor (i.e., low) log-likelihood on the test set \(\mathcal{D}_{\text{test}}\).

**Step 4: Evaluate MoG density**. A foundational concept used by FLD is to evaluate the perceptual negative log-likelihood of a held-out test set using an MoG density estimator. To quantitatively evaluate the density obtained in Step 3, we evaluate the negative log-likelihood of \(\mathcal{D}_{\text{test}}\) under \(p_{\hat{\sigma}}(\mathbf{x})\). As demonstrated in Figure 4, in settings with \(k>0\), the generated samples are too close to the training set, meaning that all test samples will have a high negative log-likelihood (as they are far from the center of Gaussians with low variances). Evaluation of the test set provides a succinct way of measuring the generalization performance of our generative model, which is a key aspect that is lost in metrics such as IS and FID. Our final FLD score is thus given by the following expression:

\[\text{FLD}(\mathcal{D}_{\text{test}},\mathcal{D}_{\text{gen}}):=-\tfrac{100}{d} \log p_{\hat{\sigma}}(\mathcal{D}_{\text{test}}|\mathcal{D}_{\text{gen}})-C,\] (3)

where \(d\) is the dimension of the feature space \(\varphi\) and is equivalent to looking at the \(d^{th}\) root of the likelihood, and \(C\) is a dataset dependant constant.4 As a result of this adjustment by a constant, FLD is essentially estimating the forward Kullback-Leibler (KL) divergence (up to a constant factor) between the learned MoG distributions of the true data and the generated data. Higher FLD score values are indicative of problems in some of the three areas evaluated by FLD. Poor sample fidelity leads to Gaussian centers that are far from the test set and thus a higher NLL. Similarly, a failure to sufficiently cover the data manifold will lead to some test samples yielding very high NLL. Finally, overfitting to the training set will yield a MoG density estimator that overfits and yield a poor NLL value on the test set.

**Computational Complexity of FLD**. To quantify the computational complexity of FLD we plot, in Figure 12 in SSA.2, the computation time of various metrics with the number of samples. As depicted, we observe a linear scaling in the number of train samples for FLD which is in line with the computational cost of popular metrics like FID. Finally, the cost of computing FLD--and other metrics--is dwarfed by the cost of generating samples and then mapping them to an appropriate feature space which is a one-time cost and can be done prior to any metric computation.

### Detecting Memorization and Overfitting

**Memorization**. One of the key advantages of FLD over other metrics like FID is that it can be used to precisely characterize memorization at the sample level. Intuitively, memorization occurs when a generated sample \(\mathbf{x}_{j}^{\text{gen}}\) is an approximate copy of a training sample \(\mathbf{x}_{i}^{\text{train}}\). By Proposition 1, such a phenomenon encourages the optimization of Eq. 2 to select a \(\hat{\sigma}_{j}^{2}\ll 1\) to achieve a high training NLL. As a result, such samples will assign a disproportionately high likelihood to that \(\mathbf{x}_{i}^{\text{train}}\). To quantify this phenomenon, we compute the train likelihood assigned by each fitted Gaussian.

**Definition 3.1**.: _Let \(\delta>0\). The sample \(x_{j}^{\text{gen}}\) is said to be \(\delta\)-memorized if_

\[\mathcal{O}_{j}:=\max_{i}\mathcal{N}(\varphi(x_{i}^{\text{train}})|\varphi(x_ {j}^{\text{gen}});\hat{\sigma_{j}}^{2}I)>\delta\,.\] (4)

The quantity \(\mathcal{O}_{j}\) is appealing because it is efficient to compute, and the distribution of \(\{\mathcal{O}_{j}\}\) allows us to quantify what is a "large" value for \(\delta\) and identify the generated samples that are the most-likely copies of the training set. In SS4.1.3, we explore this method to assess sample novelty.

**Overfitting**. Intuitively, a generative model is overfitting when it is more likely to generate samples closer to the train set than to the (unseen) test set. Thus, overfitting for deep generative models can be precisely defined using the standard tools for likelihood estimation.

**Definition 3.2**.: _Given samples \(\mathcal{D}_{\text{gen}}=\{\mathbf{x}^{\text{gen}}\}_{i=1}^{n}\) from a generative model \(G\) trained on \(\mathcal{D}_{\text{train}}\) and unseen test samples \(\mathcal{D}_{\text{test}}\). We say that \(G\) is overfitting if \(\log p_{\hat{\sigma}}(\mathcal{D}_{\text{test}}|\mathcal{D}_{\text{gen}})< \log p_{\hat{\sigma}}(\mathcal{D}_{\text{train}}|\mathcal{D}_{\text{gen}})\), i.e if:_

\[\text{Generalization Gap }\text{FLD}:=\text{FLD}(\mathcal{D}_{\text{train}}, \mathcal{D}_{\text{gen}})-\text{FLD}(\mathcal{D}_{\text{test}},\mathcal{D}_ {\text{gen}})<0\] (5)

For FLD, this effect is particularly noticeable due to the MoG being fit to the training set. In fact, samples that are too close to the train set relative to the test set have two effects: they worsen the density estimation of the MoG (increasing \(\text{FLD}(\mathcal{D}_{\text{test}},\mathcal{D}_{\text{gen}})\) and assign higher likelihood to the train set (lowering \(\text{FLD}(\mathcal{D}_{\text{train}},\mathcal{D}_{\text{gen}})\)). In section 4.1.3 and Tab. 1 experiments we empirically validate the overfitting behavior of popular generative models using our above definition and also visualize samples that are most overfit.

**Evaluating individual sample fidelity**. While FLD focuses on estimating the density learned by the generative model, it is also possible to estimate the density of the data and use that to evaluate the likelihood of the generated samples5. In particular, instead of centering the Gaussians at the generated samples, we can instead **center them at the test set**. Then, after fitting this MoG to the train set, we compute the likelihood it assigns to generated samples and use that as a measure of sample quality. More formally:

Footnote 5: This is somewhat analoguous to the difference between Recall and Precision (Sajjadi et al., 2018; Kynkanniemi et al., 2019) where FLD can be seen as a smoother version of these metrics.

\[\mathcal{Q}_{j}:=\mathcal{N}(\varphi(\mathbf{x}_{j}^{\text{gen}})|\varphi( \mathcal{D}_{\text{test}});\Sigma).\] (6)

As is the case for \(\mathcal{O}_{j}\), \(\mathcal{Q}_{j}\) is easy to compute once the Mog is fit and can be used to rank and potentially filter out poor fidelity samples.

## 4 Experiments

We investigate the application of FLD on generative models that span a broad category of model families, including popular GAN and diffusion models. For datasets, we evaluate a variety of popular natural image benchmarks in CIFAR10 (Krizhevsky et al., 2014), FFHQ (Karras et al., 2019) and ImageNet (Deng et al., 2009). Through our experiments, we seek to validate the correlation between FLD and sample fidelity, diversity, and novelty.

Stein et al. (2023) find that the DINOV2 feature space allows for a more comprehensive evaluation of generative models (relative to Inception-V3) and correlates better with human judgement. As such, for our experiments, unless indicated otherwise, we map samples to the DINOV2 feature space (Oquab et al., 2023). We do so even for other metrics (e.g. FID, Precision, Recall, etc.) which have typically used other feature spaces (comparisons with vanilla FID are provided in Appendix SSC).

### Trichotomic evaluation of FLD

We now experimentally validate FLD's ability to evaluate the samples of generative models along the three axes of fidelity, diversity and novelty.

#### 4.1.1 Sample Fidelity

We evaluate the effect of 2 types of transformations on FLD and plot the results in Fig. 5. The first consists of minor, almost imperceptible transformations (very minor gaussian blur, posterizing and converting to a high quality JPG) that are problematic for FID. As described in (Parmar et al., 2022) small changes to images such as compression or the wrong form of anti-aliasing increase FID substantially despite yielding essentially indistinguishable samples. These transformations affect FLD but _noticeably less than FID_. This phenomenon occurs even when we use DINOV2 for both FLD and FID (though the effect is more drastic using the original Inception-V3 feature space, see Appendix SSC).

Specifically, when all the samples are transformed, FID rates the imperceptibly transformed samples PFGM++ samples as worse than those produced by StyleGAN-XL (or even worse than StyleGAN2-ada). On the other hand, while the imperceptible transforms yield slightly worse FLD values (in part due to the feature space being sensitive to them), the FLD values are barely changed for the "Posterize" and "Light Blur" transforms and only somewhat worse for "JPG 90". The second type of transformations are larger transformations that affect the structure of the image (e.g. cropping, rotation, etc.) and have a significant negative impact on both FLD and FID.

#### 4.1.2 Sample Diversity

We consider two experiments on CIFAR10 to evaluate the effect of mode coverage and diversity on FLD. For the first, we vary the number of classes included in the set of generated samples for various conditional generative models. For the second, instead of the original \(n\) generated samples, we look at a set comprised of \(\frac{n}{k}\) samples from the original set combined with \((k-1)\) approximate copies of each of those samples (i.e. for a total of \(n\) samples). In both cases, we find a strong and consistent relationship (across all models) indicating that both sample diversity and mode coverage are important to get good FLD values and report our findings in Fig. 6.

#### 4.1.3 Sample Novelty

We now study the effect of data copying on CIFAR10 on evaluation metrics for generative models. As overfitting can be more subtle than direct copying of the training set, we also consider transformed

Figure 5: Starting from a set of SOTA samples produced by PFGM++, we replace each sample with a transformed copy. **Left:** Effect of nearly imperceptible transformations on FLD and FID (with corresponding values for various models as reference). **Right:** Effect of large transformations on FLD and FID.

copies of the train set. For each transform in Fig. 7, we start with PFGM++ samples that have had the transform applied and gradually replace them with transformed copies of the training set. As such, sample fidelity/diversity in this "pseudo-generated" set remains roughly constant while overfitting increases.

We then evaluate this set of samples using a variety of metrics designed to detect overfitting. For FLD, we look at the generalization gap \(\text{FLD}(\mathcal{D}_{\text{train}},\mathcal{D}_{\text{gen}})-\text{FLD}( \mathcal{D}_{\text{test}},\mathcal{D}_{\text{gen}}))\). For FID, we consider the difference \(\text{FID}_{\text{train}}-\text{FID}_{\text{test}}\) (where \(\text{FID}_{\text{test}}\) is FID using the test set). We use the same amount of samples (10k) for both as FID is biased. \(C_{T}\)[Moehan et al., 2020] is the result of a Mann-Whitney test on the distribution of distances between generated and train samples compared to the distribution of distances between train samples and test samples (negative implies overfit, positive implies underfit). The \(\text{Aut}\text{Pct}\in[0,100]\) is derived from authenticity described in [Alaa et al., 2022] and is simply the percentage of generated samples deemed authentic by their metric.

We find that FLD is the only metric consistently capable of detecting overfitting for all transforms (though the effect is less pronounced for large transforms). The generalization gap of FID oscillates though generally trends in the right direction. However, the magnitude of the gap is small (considering that the FID values using DINOv2 are considerably larger, e.g. the difference between StyleGAN2-ADA and StyleGAN-XL is >60). AuthPct trends in the right direction but seems to only be able to detect a subset of the memorized samples (as it detects none of the sample sets as overfit). Finally, \(C_{T}\) is capable of detecting overfitting for all but the largest transforms (though it does require a significant proportion of copied samples).

#### 4.1.4 Interaction effects

We now examine the relative effect of these three axes of evaluation on FLD. For fidelity, we use an increasing blur intensity. For diversity, we take a subset of the generated samples and replace the rest of the sample with copies of this subset. For novelty, we replace generated samples with copies of the training set. Then, in Fig. 8, starting from high quality generated samples, we vary each combination of the 2 axes and observe the effect on FLD through a heatmap.

In summary, Fig. 8 indicates that for FLD, fidelity matters more than diversity which matters more than novelty (with the notable exception of all samples being copied resulting in very high FLD). We argue this ordering is very much aligned with the potential usefulness of a generative model. If samples have poor fidelity, then regardless of their diversity and novelty, they will not be useful. With poor diversity but good fidelity and novelty, removing duplicates yields a useful generative model.

Figure 6: **Left:** FLD for various models as we increase the number of classes included in the generated samples. **Right:** FLD as the set of generated samples includes increasing amounts of copies of itself.

Figure 7: Comparison of various overfitting metrics when \(\mathcal{D}_{\text{gen}}\) consists of a combination of transformed generated samples and transformed copies of the train set.

### Comparison of Evaluation of State-of-the-Art Models

Using samples provided by (Stein et al., 2023), we perform a large-scale evaluation of various generative models in Tab. 1 using different metrics on CIFAR10, FFHQ and ImageNet. We find that FLD yields a similar model ranking as FID with some exceptions (for example MHGAN vs BigGAN-Deep on CIFAR10). In addition, we observe that modern generative models are overfitting in a benign way on CIFAR, i.e., there is a noticeable gap between train and test FLD even though the test FLD is reasonably low (indicating good generalization).

### Applications of FLD

**Identifying memorized samples**. As discussed in 3.2, we sort the samples generated by various models trained on CIFAR10 according to their respective \(\mathcal{O}_{j}\) and plot samples for different percentiles. As demonstrated in Fig. 9, all models produce a non-negligible amount of very near copies (especially a specific car of which there are multiple copies in the train set). For PFGM++ and StyleGAN-XL near copies exist even at the 1st percentile of \(\mathcal{O}_{j}\) (roughly in line with the findings of (Carlini et al., 2023; Stein et al., 2023)).

\begin{table}
\begin{tabular}{c c c c c c c c c} Dataset & Model & FLD & FID & Gen Gap FLD & \(C_{T}\) & AuthPct & Precision & Recall \\ \hline \multirow{6}{*}{**Dataset**} & \multirow{6}{*}{\begin{tabular}{c} CGAN-Mod \\ LOGAN \\ BigGAN-Deep \\ MHGANGAN \\ \end{tabular} } & 24.22 & 1143.07 & 0.10 & 26.11 & 72.09 & 0.76 & 0.00 \\  & & 18.94 & 753.34 & 0.18 & 55.66 & 84.10 & 0.63 & 0.13 \\  & & BigGAN-Deep & 9.28 & 203.90 & -0.05 & 55.70 & 88.10 & 0.50 & 0.26 \\  & & MHGAN & 8.84 & 231.38 & -0.01 & 47.87 & 86.69 & 0.55 & 0.26 \\  & & StyleGAN2-ada & 6.86 & 178.64 & -0.09 & 45.31 & 86.40 & 0.54 & 0.33 \\  & & iDDPM-DDIM & 5.63 & 128.57 & -0.33 & 39.65 & 84.60 & 0.57 & 0.55 \\  & & StyleGAN-XL & 5.58 & 109.42 & -0.23 & 36.79 & 85.29 & 0.57 & 0.13 \\  & & PFGMPP & 4.58 & 80.47 & -0.35 & 32.79 & 83.54 & 0.60 & 0.62 \\ \hline \multirow{6}{*}{**Dataset**} & \multirow{6}{*}{\begin{tabular}{c} Efficient-VDVAE \\ Projected-GAN \\ StyleGAN-ADA \\ \end{tabular} } & 9.40 & 465.34 & -0.10 & 32.22 & 86.48 & 0.66 & 0.07 \\  & & Projected-GAN & 8.61 & 339.72 & 0.05 & 36.77 & 93.46 & 0.40 & 0.11 \\  & & StyleGAN2-ADA & 7.24 & 296.93 & -0.08 & 27.47 & 90.95 & 0.49 & 0.06 \\  & & Unleashing & 7.23 & 287.38 & -0.06 & 38.98 & 90.01 & 0.57 & 0.19 \\  & & InSGen & 6.20 & 249.91 & -0.08 & 26.06 & 89.74 & 0.52 & 0.12 \\  & & StyleGAN-XL & 5.94 & 155.88 & -0.04 & 39.41 & 88.17 & 0.56 & 0.30 \\  & & StyleSwin & 5.77 & 200.80 & -0.29 & 32.19 & 87.68 & 0.60 & 0.20 \\  & & StyleNAT & 4.69 & 156.38 & -0.12 & 29.38 & 86.35 & 0.62 & 0.30 \\  & & LDM & 4.63 & 162.45 & -0.23 & 28.28 & 84.84 & 0.66 & 0.34 \\ \hline \multirow{6}{*}{**Dataset**} & \multirow{6}{*}{
\begin{tabular}{c} RQ-Transformer \\ StyleGAN-XL \\ GigagGAN \\ Mask-GIT \\ LDM \\ DiT-XL-2 \\ \end{tabular} } & 11.55 & 212.99 & -0.53 & 125.48 & 86.10 & 0.39 & 0.55 \\  & & StyleGAN-XL & 8.46 & 150.27 & -0.40 & 98.69 & 84.10 & 0.43 & 0.26 \\  & & GigaGAN & 8.34 & 156.40 & -0.42 & 98.78 & 82.48 & 0.47 & 0.32 \\  & & Mask-GIT & 6.74 & 144.23 & -0.63 & 78.97 & 80.02 & 0.48 & 0.44 \\  & & LDM & 3.41 & 82.42 & -0.74 & 33.63 & 69.23 & 0.63 & 0.45 \\  & & DiT-XL-2 & 1.98 & 62.42 & -0.99 & 22.57 & 65.79 & 0.69 & 0.55 \\ \hline \end{tabular}
\end{table}
Table 1: Summary of performance metrics for various generative models.

Figure 8: Heatmap of FLD values for all pairs of the three axes of generalization.

**Evaluating individual sample fidelity**. We repeat the process looking instead at the \(\mathcal{Q}_{j}\) of the models. From Fig. 10, the progression in fidelity of generative models is quite striking with older models such as LOGAN and ACGAN-Mod producing implausible images for all but their best samples. For recent, SOTA generative models, the top half of samples in terms of \(\mathcal{Q}_{j}\) are generally of high quality. However, the bottom half has many examples of poor quality samples that would easily be identified by most humans as being fake. Consequently, even if the ranking is not perfect, filtering generated samples using \(\mathcal{Q}_{j}\) could potentially be beneficial for downstream applications.

## 5 Conclusion

We introduce FLD, a new holistic evaluation metric for deep generative models. FLD is easy to compute, broadly applicable to all generative models, and evaluates generation quality, diversity, and generalization. Moreover, we show that, unlike previous approaches, FLD provides more explainable insights into the overfitting and memorization behavior of trained generative models. We empirically demonstrate both on synthetic and real-world datasets that FLD can diagnose important failure modes such as memorization/overfitting, informing practitioners on the potential limitations of generative models that generate photo-realistic images. While we focused on the domain of natural images, a fertile direction for future work is to extend FLD to other data modalities such as text, audio, or time series and also evaluate conditional generative models.

Figure 10: Ranked generated samples for different models from CIFAR10 according to \(\mathcal{Q}_{j}\).

Figure 9: Ranked generated samples for different models from CIFAR10 according to \(\mathcal{O}_{j}\). The left sample is generated, the right one is the nearest sample in the train set (using distances in DINOV2 feature space).

## Acknowledgements

The authors acknowledge the material support of NVIDIA in the form of computational resources. AJB was generously supported by an IVADO Ph.D. fellowship. Finally, the authors would like to thank Quentin Bertrand, David Dobre, and Alexia Jolicoeur-Martineau for helpful feedback on drafts of this work.

## References

* Adlam et al. (2019) B. Adlam, C. Weill, and A. Kapoor. Investigating under and overfitting in wasserstein generative adversarial networks. _arXiv preprint arXiv:1910.14137_, 2019.
* Alaa et al. (2022) A. Alaa, B. Van Breugel, E. S. Saveliev, and M. van der Schaar. How faithful is your synthetic data? sample-level metrics for evaluating and auditing generative models. _ICML_, 2022.
* Arora et al. (2018) S. Arora, A. Risteski, and Y. Zhang. Do GANs learn the distribution? some theory and empirics. _ICLR_, 2018.
* Bai et al. (2021) C.-Y. Bai, H.-T. Lin, C. Raffel, and W. C.-w. Kan. On training sample memorization: Lessons from benchmarking generative modeling with a large-scale competition. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, 2021.
* Binkowski et al. (2018) M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton. Demystifying MMD GANs. _ICLR_, 2018.
* Bishop and Nasrabadi (2006) C. M. Bishop and N. M. Nasrabadi. _Pattern recognition and machine learning_. Springer, 2006.
* Brown et al. (2020) T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020.
* Burda et al. (2015) Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. _arXiv preprint arXiv:1509.00519_, 2015.
* Carlini et al. (2023) N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramer, B. Balle, D. Ippolito, and E. Wallace. Extracting Training Data from Diffusion Models. _arXiv preprint arXiv:2301.13188_, 2023.
* Deng et al. (2009) J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_. Ieee, 2009.
* Esteban et al. (2017) C. Esteban, S. L. Hyland, and G. Ratsch. Real-valued (medical) time series generation with recurrent conditional GANs. _arXiv preprint arXiv:1706.02633_, 2017.
* Goodfellow et al. (2016) I. Goodfellow, Y. Bengio, and A. Courville. _Deep learning_. MIT press, 2016.
* Gulrajani et al. (2020) I. Gulrajani, C. Raffel, and L. Metz. Towards GAN benchmarks which require generalization. _arXiv preprint arXiv:2001.03653_, 2020.
* Heusel et al. (2017) M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two time-scale update rule converge to a local nash equilibrium. _NeurIPS_, 30, 2017.
* Hitaj et al. (2017) B. Hitaj, G. Ateniese, and F. Perez-Cruz. Deep models under the GAN: information leakage from collaborative deep learning. In _Proceedings of the 2017 ACM SIGSAC conference on computer and communications security_, 2017.
* Huang et al. (2021) C.-W. Huang, J. H. Lim, and A. C. Courville. A variational perspective on diffusion-based generative models and score matching. _NeurIPS_, 34, 2021.
* Karras et al. (2019) T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2019.
* Karras et al. (2020) T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image quality of styleGAN. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020.
* Krizhevsky et al. (2014) A. Krizhevsky, V. Nair, and G. Hinton. The CIFAR-10 dataset. _online: http://www. cs. toronto. edu/kriz/cifar. html_, 2014.
* Kynkaanniemi et al. (2019) T. Kynkaanniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila. Improved precision and recall metric for assessing generative models. _NeurIPS_, 32, 2019.
* Krizhevsky et al. (2015)C. Le Lan and L. Dinh. Perfect density models cannot guarantee anomaly detection. _Entropy_, 23(12), 2021.
* Liu et al. (2018) K. S. Liu, B. Li, and J. Gao. Generative model: Membership attack, generalization and diversity. _CoRR, abs/1805.09898_, 3, 2018.
* Lucic et al. (2018) M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bousquet. Are gans created equal? a large-scale study. _NeurIPS_, 2018.
* maintainers and contributors (2016) T. maintainers and contributors. TorchVision: PyTorch's Computer Vision library. https://github.com/pytorch/vision, 2016.
* Meehan et al. (2020) C. Meehan, K. Chaudhuri, and S. Dasgupta. A non-parametric test to detect data-copying in generative models. In _International Conference on Artificial Intelligence and Statistics_, 2020.
* Murphy (2012) K. P. Murphy. _Machine learning: a probabilistic perspective_. MIT press, 2012.
* Murphy (2022) K. P. Murphy. _Probabilistic Machine Learning: An introduction_. MIT Press, 2022. URL.probml.ai.
* Nalisnick et al. (2018) E. Nalisnick, A. Matsukawa, Y. W. Teh, D. Gorur, and B. Lakshminarayanan. Do deep generative models know what they don't know? _ICLR_, 2018.
* Nguyen et al. (2020) T. T. Nguyen, H. D. Nguyen, F. Chamroukhi, and G. J. McLachlan. Approximation by finite mixtures of continuous density functions that vanish at infinity. _Cogent Mathematics & Statistics_, 7(1), 2020.
* Nowozin et al. (2016) S. Nowozin, B. Cseke, and R. Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. _NeurIPS_, 2016.
* Oquab et al. (2023) M. Oquab, T. Darceet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. _arXiv preprint arXiv:2304.07193_, 2023.
* Parmar et al. (2022) G. Parmar, R. Zhang, and J.-Y. Zhu. On aliased resizing and surprising subtleties in gan evaluation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11410-11420, 2022.
* Pedregosa et al. (2011) F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in Python. _the Journal of machine Learning research_, 12, 2011.
* Rombach et al. (2022) R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022.
* Sajjadi et al. (2018) M. S. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models via precision and recall. _NeurIPS_, 31, 2018.
* Salimans et al. (2016) T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training GANs. _NeurIPS_, 29, 2016.
* Sompalli et al. (2022) G. Sompalli, V. Singla, M. Goldblum, J. Geiping, and T. Goldstein. Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models. _arXiv preprint arXiv:2212.03860_, 2022.
* Song et al. (2021) Y. Song, C. Durkan, I. Murray, and S. Ermon. Maximum likelihood training of score-based diffusion models. _NeurIPS_, 34, 2021.
* Stein et al. (2023) G. Stein, J. C. Cresswell, R. Hosseinzadeh, Y. Sui, B. L. Ross, V. Villecroze, Z. Liu, A. L. Caterini, J. E. T. Taylor, and G. Loaiza-Ganem. Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. _arXiv preprint arXiv:2306.04675_, 2023.
* Szegedy et al. (2016) C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2016.
* Szegedy et al. (2015)L. Theis, A. v. d. Oord, and M. Bethge. A note on the evaluation of generative models. _arXiv preprint arXiv:1511.01844_, 2015.
* van den Burg and Williams [2021] G. van den Burg and C. Williams. On memorization in probabilistic deep generative models. _NeurIPS_, 34, 2021.
* Webster et al. [2019] R. Webster, J. Rabin, L. Simon, and F. Jurie. Detecting overfitting of deep generative networks via latent recovery. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2019.
* Wu et al. [2016] Y. Wu, Y. Burda, R. Salakhutdinov, and R. Grosse. On the quantitative analysis of decoder-based generative models. _arXiv preprint arXiv:1611.04273_, 2016.
* Wu et al. [2021] Z. Wu, K. E. Johnston, F. H. Arnold, and K. K. Yang. Protein sequence design with deep generative models. _Current Opinion in Chemical Biology_, 65, 2021.
* Xu et al. [2018] Q. Xu, G. Huang, Y. Yuan, C. Guo, Y. Sun, F. Wu, and K. Weinberger. An empirical study on evaluation metrics of generative adversarial networks. _arXiv preprint arXiv:1806.07755_, 2018.
* Yazici et al. [2020] Y. Yazici, C.-S. Foo, S. Winkler, K.-H. Yap, and V. Chandrasekhar. Empirical analysis of overfitting and mode drop in gan training. In _2020 IEEE International Conference on Image Processing (ICIP)_. IEEE, 2020.

Additional Results

### Number of samples

From Fig. 11, FLD appears to be reliable even when using less reference set samples (similarly to KID Binkowski et al. (2018)). This is of particular importance for applications in low data regimes or even conditional generation (e.g. the test set of CIFAR10 only contains 1k images for each class). On the other hand, FID highly recommends a minimum of 10k Heusel et al. (2017). Generally however, the whole training set is used as the value is still biased at 10k and a noticeably lower score can be obtained by using more samples.

As for generated samples, for FID, most papers use 50k as it gives a lower FID than with 10k. We find that using 10k samples for FLD is sufficient for robust evaluation. This is benefit is particularly important given the computational complexity of sampling from modern diffusion models (for example, generating 50k samples often takes hours).

### Computational Complexity of FLD

We include the plot of computation time of our score relative to other metrics and find that while it scales linearly with the number of train samples at a faster rate than many metrics, it is still roughly within the same order of magnitude to other metrics and insignificant compared to sample generation time.

### Synthetic Experiments

We include a synthetic 2D experiment analyzing how FLD/FID varies as the generated samples go from underfit to overfit. Specifically, we generate \(3000\) points from the Two Moons dataset using scikit-learn (Pedregosa et al., 2011) with a noise value of \(0.1\). The first \(2000\) points are used as

Figure 11: FLD, FID and KID for various sizes of the test set (here, for FID/KID, we use the test set as reference set). Both FLD and KID appear to be relatively consistent, even for smaller test set sizes, while FID is clearly biased.

Figure 12: Time taken (on 1xRTX8000) for different metrics as we vary the number of train samples. We solely include the time to compute the metric value once the samples have been generated and mapped to the appropriate feature space (as these two steps are necessary for each metric and are significantly more computationally expensive than the metric computation).

the training set and the last \(1000\) as test set. We fit a KDE to the train set using bandwidth values varying from \(10^{-4}\) to \(10\) and sample \(1000\) points as generated samples before computing our score. As expected, FLD decreases initially as the samples begin to match the distribution. However, unlike Test FID, as the samples begin getting too close to the train samples, FLD increases, punishing the model for overfitting.

Figure 13: FLD vs. FID for samples from a KDE centered at the train points. As we decrease the bandwidth of the KDE, the generated samples go from A) (underfit e.g. low fidelity resulting in high FID/FLD) to B) (just right, the samples match the distribution but are not overfit to the train samples, yielding low FID/FLD) to C) (overfit, the samples are almost identical to the training samples, yielding low FID but **high FLD**).

[MISSING_PAGE_EMPTY:17]

Figure 16: Ranked generated samples for different models on ImageNet according to \(\mathcal{Q}_{j}\).

[MISSING_PAGE_EMPTY:19]

Method Details

### Algorithm for MoG

```
0:\(\mathcal{D}_{\mathrm{train}},\mathcal{D}_{\mathrm{gen}},\varphi,\alpha\)
0: Fit MoG on \(\mathcal{D}_{\mathrm{train}}\) with \(\mu=\varphi(\mathcal{D}_{\mathrm{gen}})\)
1:\(\sigma^{2}=\mathbf{1}\) {// Initialize all bandwidths \(\sigma_{j}^{2}=1\)}
2:\(\mathbb{H}=d(\varphi(\mathcal{D}_{\mathrm{train}}),\varphi(\mathcal{D}_{ \mathrm{gen}}))\) {// Pre-compute distance matrix.}
3:while\(\sigma^{2}\) not converged do
4:\(\mathcal{L}=-\log p_{\sigma}(\varphi(\mathcal{D}_{\mathrm{train}})|\varphi( \mathcal{D}_{\mathrm{gen}}))\)
5:\(\sigma^{2}\leftarrow\sigma^{2}-\alpha\nabla_{\sigma^{2}}\mathcal{L}\) {// Gradient descent on NLL }
6:endwhile
7:return\(\sigma\) {// Return Trained MoG} ```

**Algorithm 1** Fitting MoGs for FLD

### Optimization

Even with the simplifying assumption of a diagonal covariance matrix, to solve Eq. 2 we still need to learn one parameter per Gaussian (of which there are 10000). As the likelihood of a train sample is the result of the \(\log\) of the sum of individual Gaussians, the \(\sigma_{j}\) cannot be optimized independently. Thus, we turn to full batch gradient descent using the Adam optimizer with the following hyperparameters:

* 10000 generated samples
* 50 epochs
* \(lr=0.5\)
* Initial value for the variance vector: 0

We modified the original version of Eq. 2 to include a per sample term \(\mathcal{L}_{i}\) intended to provided a minimum amount of likelihood to each train sample. In practice, we set \(\mathcal{L}_{i}\) to be equivalent to the likelihood of the point \(\varphi(x^{i}_{\mathrm{train}})\) relative to a Gaussian with identity covariance existing at distance \(0.9||\varphi(x^{i}_{\mathrm{train}})||_{2}^{2}\) from \(\varphi(x^{i}_{\mathrm{train}})\). We find this helps the optimization process recover low variances in the edge cases where most of the generated samples are copied. Otherwise, non-copied train samples have an exponentially small likelihood assigned to them.

In addition, getting the likelihood assigned by each Gaussian to each point requires computing the distance between each pair \((\mathbf{x}^{\mathrm{gen}}_{i},\mathbf{x}^{\mathrm{train}}_{i})\) which is \(O(n^{2})\) and time-consuming. Also, as the distances and dimensions are large (at least for our high-dimensional experiments), the exponentiation and summations were often numerically unstable. To address this, we:

* Compute the \(O(n^{2})\) distance matrix once and store it so as not to recompute it for each step of the optimization procedure.
* Optimize the log variances instead of the variances themselves
* Convert \(\sigma_{j}^{-d}\) to \(\exp(-d\log(\sigma_{j}))\) to be able to take advantage of a numerically stable logsumexp.

From plotting the loss, the variances almost always converged in short order (usually less than 10 steps). While we have no guarantee that these were global minima, when there were exact copies or close to exact copies, 1 would recover very low log variances, as shown in 1.

Experimental Setup

### Transforms

To assess both quality and overfitting, we use standard torchvision [maintainers and contributors, 2016] image transforms described below:

* **Posterize**: Posterize the image to 5 bits (we found 5 was the lowest we could go while the difference still being mostly imperceptible).
* **Resize**: Resize the image to the size accepted by InceptionV3 using bicubic interpolation which has been shown to be problematic [Parmar et al., 2022].
* **Color Distort**: Apply the color jitter transform described in [maintainers and contributors, 2016] with default parameters.
* **Elastic transform**: Apply the elastic transform described in [maintainers and contributors, 2016] with default parameters.
* **Blur**: Gaussian blur with a \((5,5)\) kernel. For the light blur, we use a \(\sigma\) of \(0.5\). For the heavy blur we use \(\sigma=1.4\). For the heatmap, the blur intensity corresponds to the value of \(\sigma\)
* **Center crop x**: Center crop the image to \((x,x)\) and fill the rest with black.
* **JPG x**: Convert image to a JPG of quality x.

### Generated samples

For all experiments, we have updated our paper to use the samples provided by Stein et al. [2023] who have generously open-sourced an excellent set of samples from a wide variety of SOTA models on various datasets.

Proof of Proposition 1

We now prove our Proposition 1 from the main paper.

**Proposition 1**.: _Let \(D_{ij}:=||\varphi(\mathbf{x}_{j}^{\text{ren}})-\varphi(\mathbf{x}_{i}^{\text{ train}})||^{2}\) be the distance between a generated sample and a train sample. Assume \(\forall i,j:D_{ij}\leq\hat{D}\) with \(\delta_{j}:=\min_{i}D_{ij}\). Then, for any \(l\in\{1,\ldots,m\}\), we have that \(\hat{\sigma}_{l}=O(\delta_{l})\) where \(\hat{\sigma}^{2}\) is a solution of Eq. 2._

Proof.: We begin by showing that \(\hat{\sigma}_{l}^{2}=O(\delta_{l}^{2})\) for \(\delta_{l}\) small enough.

We define the variance vector \(\hat{\sigma}\) to be the maximizer of the following equation

\[\hat{\sigma}^{2}\in\arg\max_{\sigma^{2}}\frac{1}{n}\sum_{i=1}^{n}\log\Bigg{(} \frac{1}{m}\sum_{j=1}^{m}\frac{1}{(\sqrt{2\pi}\sigma_{j})^{d}}\exp\Big{(} \frac{-\|\varphi(\mathbf{x}_{j}^{\text{ren}})-\varphi(\mathbf{x}_{i}^{\text{ train}})\|^{2}}{2\sigma_{j}^{2}}\Big{)}\Bigg{)}.\] (7)

Without loss of generality, we consider the following shifted and rescaled objective function with the same argmax as Eq. 7

\[f(\sigma):=\sum_{i=1}^{n}\log\left(\frac{1}{m}\sum_{j=1}^{m}\frac{1}{\sigma_{ j}^{d}}\exp\Big{(}\frac{-D_{ij}^{2}}{2\sigma_{j}^{2}}\Big{)}\right).\] (8)

Then, for any \(i=1,\ldots,n\), we have

\[\max_{j}\sigma_{j}^{-d}\exp\Big{(}\tfrac{-D_{ij}^{2}}{2\sigma_{j}^{2}}\Big{)} \leq\sum_{j=1}^{m}\sigma_{j}^{-d}\exp\Big{(}\tfrac{-D_{ij}^{2}}{2\sigma_{j}^{2} }\Big{)}\leq m\max_{j}\sigma_{j}^{-d}\exp\Big{(}\tfrac{-D_{ij}^{2}}{2\sigma_{j }^{2}}\Big{)}\] (9)

and using the fact that \(\log\) is an increasing function, we have that for any \(\sigma\)

\[g(\sigma):=\sum_{i=1}^{n}\max_{j}\left(-d\log\sigma_{j}-\frac{D_{ij}^{2}}{2 \sigma_{j}^{2}}\right)\leq f(\sigma)\leq g(\sigma)+n\log(m)\] (10)

Now, let \(\sigma^{*}\in\arg\max g(\sigma)\) and take \(l\in\{1,\ldots,m\}\). We now show that there exists a \(\bar{\delta}\) such that for \(\delta_{l}<\bar{\delta}\), \(\sigma_{l}^{*}=\frac{\delta_{l}}{\sqrt{d}}\) and appears exactly once in the sum \(g(\sigma^{*})\).

Suppose \(\sigma_{l}^{*}\) appears in \(k\) terms (WLOG we can assume they are the first \(k\) terms). We can then break down \(g(\sigma^{*})\) as follows,

\[g_{k}(\sigma^{*})=\sum_{i=1}^{k}\left(-d\log\sigma_{l}^{*}-\frac{D_{il}^{2}}{2 \sigma_{l}^{*2}}\right)+\sum_{i=k+1}^{n}\max_{j\neq l}\left(-d\log\sigma_{j}^ {*}-\frac{D_{ij}^{2}}{2\sigma_{j}^{*2}}\right)\] (11)

and define

\[C:=\sup_{\sigma}\sum_{i=k+1}^{n}\max_{j\neq l}\Bigg{|}-d\log\sigma_{j}-\frac{ D_{ij}^{2}}{2\sigma_{j}^{*2}}\Bigg{|}<+\infty\,.\] (12)

We then examine the following three cases:

* \(k=0\): The sum in \(g(\sigma^{*})\) only consists of \(j\neq l\). \[g_{0}(\sigma^{*})=\sum_{i=1}^{n}\max_{j\neq l}\left(-d\log\sigma_{j}^{*}-\frac{ D_{ij}^{2}}{2\sigma_{j}^{*2}}\right)\] (13) which is bounded by Eq. 12.
* \(k=1\): Eq. 11 is maximized with respect to \(\sigma_{l}^{*}\) for \(\sigma_{l}^{*}=\frac{D_{il}}{\sqrt{d}}\). Plugging in to Eq. 11

\[g_{1}(\sigma^{*})=-d\log(D_{1l})-\frac{d}{2}(\log(d)+1)+\sum_{i=2}^{n}\max_{j} \left(-d\log\sigma_{j}^{*}-\frac{D_{ij}^{2}}{2\sigma_{j}^{*2}}\right)\] (14)Specifically, if the only term \(\sigma_{l}\) appears in is the one with \(D_{1l}=\delta_{l}\) \[\hat{g}_{1}(\sigma^{*})=-d\log(\delta_{l})-\frac{d}{2}(\log(d)+1)+\sum_{i=2}^{n} \max_{j}\left(-d\log\sigma_{j}^{*}-\frac{D_{ij}^{2}}{2\sigma_{j}^{*2}}\right)\] (15) where the term \(-d\log(\delta_{l})\) and thus \(\hat{g}_{k=1}(\sigma^{*})\) can be made arbitrarily large as \(\delta_{l}\to 0\). In contrast, if \(\sigma_{l}^{*}\) appears for \(D_{1l}>\delta_{l}\) we get a bounded \(g_{1}\) (independently of \(\delta_{l}\)).
* \(k>1\): Eq. 11 is maximized with respect to \(\sigma_{l}^{*}\) for \(\sigma_{l}^{*}=\sqrt{\frac{\sum_{i=1}^{k}D_{il}^{2}}{kd}}\). Plugging in to Eq. 11

\[g_{k}(\sigma^{*}) =\sum_{i=1}^{k}\left(-d\log\left(\sqrt{\frac{\sum_{i=1}^{k}D_{il}^ {2}}{kd}}\right)-\frac{D_{il}^{2}}{2\frac{\sum_{i=1}^{k}D_{il}^{2}}{kd}}\right)\] (16) \[=-kd\log\left(\sqrt{\frac{\sum_{i=1}^{k}D_{il}^{2}}{kd}}\right)- \frac{kd}{2}\] (17) \[\leq-kd\log\left(\sqrt{\frac{(k-1)}{kd}}\tilde{\delta}_{l}\right) -\frac{kd}{2}\] (18) and is thus bounded independently of \(\delta_{l}\).

Thus, we conclude that, for \(\delta_{l}\) small enough, \(\sigma_{l}^{*}=\frac{\delta_{l}}{\sqrt{d}}\) since \(\hat{g}_{1}(\sigma^{*})>\{g_{0}(\sigma^{*}),g_{1}(\sigma^{*}),g_{k}(\sigma^{*})\}\).

Now, consider \(\tilde{\sigma}\) such that \(\tilde{\sigma}_{l}=\sigma_{l}^{*}/\sqrt{1-c}\) and \(\tilde{\sigma}_{j}=\sigma_{j}^{*}\) otherwise. We now look at \(g(\tilde{\sigma})\). From the above

\[g(\tilde{\sigma})=g(\sigma^{*})+\frac{d}{2}(\log(1-c)+c)\] (19)

as long as \(\tilde{\sigma}_{l}\) is found in a single term of the summation in \(g(\tilde{\sigma})\) (i.e. \(k=1\)). If not, with the same argument as above, \(g(\tilde{\sigma})\) is bounded above by a constant \(U\) independent of \(\delta_{l}\) and \(c\). Thus \(\exists 0<\tilde{\delta}<\bar{\delta}\) such that \(\forall\delta_{l}<\tilde{\delta}\), we have \(U+n\log m\leq g(\sigma^{*})\). Thus, in both cases \(\forall\delta_{l}<\tilde{\delta}\) and for all \(c\) such that \(\frac{d}{2}(\log(1-c)+c)<-n\log m\), we have

\[f(\tilde{\sigma}) \leq g(\tilde{\sigma})+n\log m\] (20) \[\leq\max[U,g(\sigma^{*})+\frac{d}{2}(\log(1-c)+c)]+n\log m\] (21) \[<\max[U,g(\sigma^{*})-n\log m]+n\log m\] (22) \[\leq g(\sigma^{*})\] (23) \[\leq f(\hat{\sigma}).\] (24)

Since \(\frac{d}{2}(\log(1-c)+c)\) is decreasing in \(c\) (for \(c>0\)) and \(\tilde{\sigma}_{l}\) is increasing in \(c\), we must have \(\hat{\sigma}<\tilde{\sigma}_{j}=\frac{\delta_{j}}{d\sqrt{1-c}}\) where \(1-c=\exp(-\frac{2n+1}{d}\log(m))\). Thus, \(\hat{\sigma}=O(\delta_{j})\) for the case \(\delta_{l}<\bar{\delta}\).

For the case \(\delta_{l}\geq\bar{\delta}\), we first show that \(\hat{\sigma}_{l}\) is bounded (\(\hat{\sigma}_{l}\leq M\)). Denote the individual likelihood terms of \(f(\sigma)\) as follows

\[L_{ij}(\sigma_{j}):=\frac{1}{\sigma_{j}^{d}}\exp\left(\frac{-D_{ij}^{2}}{2 \sigma_{j}^{2}}\right).\] (25)

Then \(\forall i\), we have that

\[L_{il}(\sigma_{l})\leq\frac{1}{\sigma_{l}^{d}}\underset{\sigma_{l}\to\infty}{ \rightarrow}0\] (26)

Thus, since \(f(\sigma)\) is increasing in the \(L_{ij}(\sigma)\), it suffices to show there is \(\sigma_{l}\leq M\) such that all \(L_{il}(\sigma_{l})\) are positive. Taking \(\sigma_{l}=1\), we have that \(\forall i:L_{il}(1)\geq e^{-D^{2}/2}>0\). Thus we can take \(M=e^{\frac{D^{2}}{2d}}\). Hence, for the case \(\delta_{l}\geq\tilde{\delta}\)\[\hat{\sigma_{l}} \leq\frac{\delta_{l}}{\delta}\hat{\sigma_{l}}\] \[\leq\frac{M}{\delta}\delta_{l}.\]

Finally \(\hat{\sigma_{l}}\) is \(O(\delta_{l})\) by taking \(C=\max\{\frac{1}{d\sqrt{1-c}},\frac{M}{\delta}\}\).

Feature spaces

Feature spaces play a very important role in our metric and similar metrics (FID, Precision/Recall, etc.). While those produced by InceptionV3 have proved useful, there is work illustrating some potential issues, motivating our switch to DINOv2. As such, while the resulting feature pspace has proved adequate for our experiments, it is likely that FLD could provide an even better assessment of generated samples with better feature embeddings. Additionally, while this work has solely focused on the evaluation of generative models in the image domain, there is nothing about FLD indicating it could not be applied to other domains (using appropriate feature embeddings) and we leave this as a fruitful future area of research.

## Appendix H Broader Impact

Generative modeling has the potential to create copies of people's identities or artistic styles without their consent, which can have negative impacts on individuals and communities. While evaluation metrics can ensure the generalizability of generative models, they should not be employed to justify copying an artist's style without their permission. The concept of copyrighting an artistic style is intricate, and while some may argue its challenges and ambiguities, it is vital to acknowledge the impact on the artist whose style is being reproduced. Respecting the rights of individuals and communities whose data is used in creating generative models is crucial to avoid harm or exploitation. As the field of generative modeling progresses, it is imperative to engage in ongoing discussions regarding the ethical implications of these technologies and establish guidelines for their responsible application. Additionally, if evaluation metrics fail to consider the generation of harmful or unethical content, negative societal consequences can emerge. Therefore, it is crucial to consider a diverse range of evaluation metrics that encompass various aspects of model performance while aligning with ethical and societal considerations.