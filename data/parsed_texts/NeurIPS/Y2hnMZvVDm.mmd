Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time

Arvind Mahankali

Stanford University

amahanka@stanford.edu

&Jeff Z. HaoChen

Stanford University

jhaochen@stanford.edu

&Kefan Dong

Stanford University

kefandong@stanford.edu

&Margalit Glasgow

Stanford University

mglasgow@stanford.edu

&Tengyu Ma

Stanford University

tengyuma@stanford.edu

Equal ContributionEqual Contribution

###### Abstract

Despite recent theoretical progress on the non-convex optimization of two-layer neural networks, it is still an open question whether gradient descent on neural networks without unnatural modifications can achieve better sample complexity than kernel methods. This paper provides a clean mean-field analysis of projected gradient flow on polynomial-width two-layer neural networks. Different from prior works, our analysis does not require unnatural modifications of the optimization algorithm. We prove that with sample size \(n=O(d^{3.1})\) where \(d\) is the dimension of the inputs, the network trained with projected gradient flow converges in \(\text{poly}(d)\) time to a non-trivial error that is not achievable by kernel methods using \(n\ll d^{4}\) samples, hence demonstrating a clear separation between unmodified gradient descent and NTK. As a corollary, we show that projected gradient descent with a positive learning rate and a polynomial number of iterations converges to low error with the same sample complexity.

## 1 Introduction

Training neural networks requires optimizing non-convex losses, which is often practically feasible but still not theoretically understood. The lack of understanding of non-convex optimization limits the design of new principled optimizers for training neural networks that use theoretical insights.

Early analysis on optimizing neural networks with linear or quadratic activations [29; 36; 51; 35; 65; 38; 48; 39; 60] relies on linear algebraic tools that do not extend to nonlinear and non-quadratic activations. The neural tangent kernel (NTK) approach analyzes nonconvex optimization under certain hyperparameter settings, e.g., when the initialization scale is large and the learning rate is small (see, e.g., Du et al. [30], Jacot et al. [43], Li and Liang [50], Arora et al. [10], Daniely et al. [26]). However, subsequent research shows that neural networks trained with practical hyperparameter settings typically outperform their corresponding NTK kernels [9]. Furthermore, the initialization and learning rate under the NTK regime does not yield optimal generalization guarantees [76; 22; 77; 34].

Many recent works study modified versions of stochastic gradient descent (SGD) and prove sample complexity and runtime guarantees beyond NTK [23; 2; 58; 52; 1; 7; 5; 79; 78; 25; 6; 67; 70; 20; 59]. These modified algorithms often contain multiple stages that optimize different blocks of parametersand/or use different learning rates or regularization strengths. For example, the work of Li et al. [52] uses a non-standard parameterization for two-layer neural networks and runs a two-stage algorithm with sharply changing gradient clipping strength; Damian et al. [23] use one step of gradient descent with a large learning rate and then optimize only the last layer of the neural net. Nichani et al. [59] construct non-standard regularizers based on the NTK feature covariance matrix, in order to prevent the movement of weights in certain directions which hinder generalization. Additionally, Abbe et al. [2] only train the hidden layer for \(O(1)\) time in the first stage of their algorithm, and then train the second layer to convergence in the second stage. They do not study how the population loss decreases during the first stage. However, oftentimes vanilla (stochastic) gradient descent with a constant learning rate empirically converges to a minimum with good generalization error. Thus, the modifications are arguably artifacts tailored to the analysis, and to some extent, over-using the modification may obscure the true power of gradient descent.

Another technique to analyze optimization dynamics is the mean-field approach [21, 56, 57, 61, 28, 2, 44, 62, 76], which views the collection of weight vectors as a (discrete) distribution over \(\mathbb{R}^{d}\) (where \(d\) is the input dimension) and approximates its evolution by an infinite-width neural network, where the weight vector distribution is continuous and evolves according to a partial differential equation. However, these works do not provide an end-to-end polynomial runtime bound on the convergence to a global minimum in the concrete setting of two-layer neural networks (without modifying gradient descent). For example, the works of Chizat and Bach [21] and Mei et al. [56] do not provide a concrete bound on the number of iterations needed for convergence. Mei et al. [57] provide a coupling between the trajectories of finite and infinite-width networks with exponential growth of coupling error but do not apply it to a concrete setting to obtain a global convergence result with sample complexity guarantees. (See more discussion below and in Section 3.) Wei et al. [76] achieve a polynomial number of iterations but require exponential width and an artificially added noise. In other words, these works, without modifying gradient descent, cannot prove convergence to a global minimizer with polynomial width and iterations. We also discuss additional related works in Appendix A.

In this paper, we provide a mean-field analysis of projected gradient flow on two-layer neural networks with _polynomial width_ and quartic activations. Under a simple data distribution, we demonstrate that the network converges to a non-trivial error in _polynomial time_ with _polynomial samples_. Notably, our results show a sample complexity that is superior to the NTK approach. As a corollary, we show that projected gradient descent with polynomially small step size and polynomially many iterations can converge to a low error, with sample complexity that is superior to NTK. Our proof is similar to the standard bound on the error of Euler's method.

Concretely, the neural network is assumed to have unit-norm weight vectors and no bias, and the second-layer weights are all \(\frac{1}{m}\) where \(m\) is the width of the neural network. The data distribution is uniform over the sphere. The target function is of the form \(y(x)=h(q_{\star}^{\top}x)\) where \(h\) is an _unknown_ quartic link function and \(q_{\star}\) is an unknown unit vector. Our main result (Theorem 3.4) states that with \(n=O(d^{3.1})\) samples, a polynomial-width neural network with random initialization converges in polynomial time to a non-trivial error, which is statistically not achievable by any kernel method with an inner product kernel using \(n\ll d^{4}\) samples. To the best of our knowledge, our result is the first to demonstrate the advantage of _ummodified_ gradient descent on neural networks over kernel methods.

The rank-one structure in the target function, also known as the single-index model, has been well-studied in the context of neural networks as a simplified case to demonstrate that neural networks can learn a latent feature direction \(q_{\star}\) better than kernel methods [58, 2, 23, 16]. Many works on single-index models study (stochastic) gradient descent in the setting where only a single vector (or "neuron") in \(\mathbb{R}^{d}\) is trained. This includes earlier works where the link function is monotonic, or the convergence is analyzed with quasi-convexity [46, 40, 55, 64], along with more recent work [69, 72, 11] for more general link functions. Since these works only train a single neuron, they have limited expressivity in comparison to a neural network, and can only achieve zero loss when the link function equals the activation. We stress that in our setting, the link function \(h\) is unknown, not necessarily monotonic, and does not need to be equal to the activation function. We show that in this setting, the first layer weights will converge to a _distribution_ of neurons that are correlated with but not exactly equal to \(q_{\star}\), so that even without bias terms, their mixture can represent the link function \(h\). Our analysis demonstrates that gradient flow, and gradient descent with a consistent inverse-polynomial learning rate, can _simultaneously_ learn the feature \(q_{\star}\), and the link function \(h\), which is a key challenge that is side-stepped in previous works on neural-networks which use two-stage algorithms [58, 1, 2, 23, 14].

The main novelty of our population dynamics analysis is designing a potential function that shows that the iterate stays away from the saddle points.

Our sample complexity results leverage a coupling between the dynamics on the empirical loss for a finite-width neural network and the dynamics on the population loss for an infinite-width neural network. The main challenge stems from the fact that some exponential coupling error growth is inevitable over a certain period of time when the dynamics resemble a power method update. Heavily inspired by Li et al. [52], we address this challenge by using a direct and sharp comparison between the growth of the coupling error and the growth of the signal. In contrast, a simple exponential growth bound on the coupling error similar to the bound of Mei and Montanari [54] would result in a \(d^{O(1)}\) sample complexity, which is not sufficient to outperform NTK.

## 2 Preliminaries and Notations

We use \(O(\cdot),\lesssim,\gtrsim\) to hide only absolute constants. Formally, every occurrence of \(O(x)\) in this paper can be simultaneously replaced by a function \(f(x)\) where \(|f(x)|\leq C|x|,\forall x\in\mathbb{R}\) for some universal constant \(C>0\) (each occurrence can have a different universal constant \(C\) and \(f\)). We use \(a\lesssim b\) as a shorthand for \(a\leq O(b)\). Similarly, \(\Omega(x)\) is a placeholder for some \(g(x)\) where \(|g(x)|\geq|x|/C,\forall x\in\mathbb{R}\) for some universal constant \(C>0\). We use \(a\gtrsim b\) as a shorthand for \(a\geq\Omega(b)\) and \(a\asymp b\) as a shorthand to indicate that \(a\gtrsim b\) and \(a\lesssim b\) simultaneously hold.

**Legendre Polynomials.** We summarize the necessary facts about Legendre polynomials below, and present related background more comprehensively in Appendix C. Let \(P_{k,d}:[-1,1]\to\mathbb{R}\) be the degree-\(k\)_un-normalized_ Legendre polynomial [12], and \(\overline{P}_{k,d}(t)=\sqrt{N_{k,d}}P_{k,d}(t)\) be the _normalized_ Legendre polynomial, where \(N_{k,d}\triangleq\binom{d+k-1}{d-1}-\binom{d+k-3}{d-1}\) is the normalizing factor. The polynomials \(\overline{P}_{k,d}(t)\) form an orthonormal basis for the set of square-integrable functions over \([-1,1]\) with respect to the measure \(\mu_{d}(t)\triangleq(1-t^{2})^{\frac{d-3}{2}}\frac{\Gamma(d/2)}{\Gamma((d-1)/ 2)}\frac{1}{\sqrt{\pi}}\), i.e., the density of \(u_{1}\) when \(u=(u_{1},\cdots,u_{d})\) is uniformly drawn from sphere \(\mathbb{S}^{d-1}\). Hence, for every function \(h:[-1,1]\to\mathbb{R}\) such that \(\mathbb{E}_{t\sim\mu_{d}}[h(t)^{2}]<\infty\), we can define \(\hat{h}_{k,d}\triangleq\mathbb{E}_{t\sim\mu_{d}}[h(t)\overline{P}_{k,d}(t)]\) and consequently, we have \(h(t)=\sum_{k=0}^{\infty}\hat{h}_{k,d}\overline{P}_{k,d}(t)\).

## 3 Main Results

We will formally define the data distribution, neural networks, projected gradient flow, and assumptions on the problem-dependent quantities and then state our main theorems.

_Target function._ The ground-truth function \(y(x):\mathbb{R}^{d}\to\mathbb{R}\) that we aim to learn has the form \(y(x)=h(q_{\star}^{\top}x)\), where \(h:\mathbb{R}\to\mathbb{R}\) is an _unknown_ one-dimensional _even_ quartic polynomial (which is called a link function), and \(q_{\star}\) is an _unknown_ unit vector in \(\mathbb{R}^{d}\). Note that \(h(s)\) has the Legendre expansion \(h(s)=\hat{h}_{0,d}+\hat{h}_{2,d}\overline{P}_{2,d}(s)+\hat{h}_{4,d}\overline{ P}_{4,d}(s)\).

_Two-layer neural networks._ We consider a two-layer neural network where the first-layer weights are all unit vectors and the second-layer weights are fixed and all the same. Let \(\sigma(\cdot)\) be the activation function, which can be different from \(h(\cdot)\). Using the mean-field formulation, we describe the neural network using the distribution of first-layer weight vectors, denoted by \(\rho\):

\[f_{\rho}(x)\triangleq\mathbb{E}_{u\sim\rho}[\sigma(u^{\top}x)]\,.\] (3.1)

For example, when \(\rho=\text{unif}(\{u_{1},\ldots,u_{m}\})\), is a discrete, uniform distribution supported on \(m\) vectors \(\{u_{1},\ldots,u_{m}\}\), then \(f_{\rho}(x)=\frac{1}{m}\sum_{i=1}^{m}\sigma(u_{i}^{\top}x)\), i.e. \(f_{\rho}\) corresponds to a finite-width neural network whose first-layer weights are \(u_{1},\ldots,u_{m}\). For a continuous distribution \(\rho\), the function \(f_{\rho}(\cdot)\) can be viewed as an infinite-width neural network where the weight vectors are distributed according to \(\rho\) (and can be viewed as taking the limit as \(m\to\infty\) of the finite-width neural network). We assume that the weight vectors have unit norms, i.e. the support of \(\rho\) is contained in \(\mathbb{S}^{d-1}\). The activation \(\sigma:\mathbb{R}\to\mathbb{R}\) is assumed to be a fourth-degree polynomial with Legendre expansion \(\sigma(s)=\sum_{k=0}^{4}\hat{\sigma}_{k,d}\overline{P}_{k,d}(s)\).

The simplified neural network defined in Eq. (3.1), even with infinite width (corresponding to a continuous distribution \(\rho\)), has a limited expressivity due to the lack of biases and trainable second-layer weights. We characterize the expressivity by the following lemma:

**Lemma 3.1** (Expressivity).: _Let \(\gamma_{2}=\hat{h}_{2,d}/\hat{\sigma}_{2,d}\) and \(\gamma_{4}=\hat{h}_{4,d}/\hat{\sigma}_{4,d}\). Suppose for some \(d\) and \(q_{\star}\), there exists a network \(\rho\) such that \(f_{\rho}(x)=h(q_{\star}^{\top}x)\) on \(\mathbb{S}^{d-1}\). Then we have \(\hat{\sigma}_{0,d}=\hat{h}_{0,d}\), and \(0\leq\gamma_{2}^{2}\leq\gamma_{4}\leq\gamma_{2}\leq 1\). Moreover, if this condition holds with strict inequalities, then for sufficiently large \(d\), there exists a network \(\rho\) such that \(f_{\rho}(x)=h(q_{\star}^{\top}x)\) on \(\mathbb{S}^{d-1}\). (A more explicit version is stated in Appendix G.1.)_

Informally, an almost sufficient and necessary condition to have \(f_{\rho}(x)=h(q_{\star}^{\top}x)\) for some \(\rho\) is that there exists a random variable \(w\) supported on \([0,1]\) such that \(\mathbb{E}[w^{2}]\approx\gamma_{2}\) and \(\mathbb{E}[w^{4}]\approx\gamma_{4}\), which is equivalent to \(0\leq\gamma_{2}^{2}\leq\gamma_{4}\leq\gamma_{2}\leq 1\). In particular, assuming the existence of such a random variable \(w\), the perfectly-fit network \(\rho\) that fits the target function has the form

\[q_{\star}^{\top}u\stackrel{{ d}}{{=}}w,\text{ and }u-q_{\star} ^{\top}u\mid q_{\star}^{\top}u\text{ is uniformly distributed in the subspace orthogonal to }q_{\star}\.\] (3.2)

Motivated by this lemma, we will assume that \(\gamma_{2},\gamma_{4}\) are universal constants that satisfy \(0\leq\gamma_{2}^{2}\leq\gamma_{4}\leq\gamma_{2}\leq 1\), and \(d\) is chosen to be sufficiently large (depending on the choice of \(\gamma_{2}\) and \(\gamma_{4}\)). In addition, we also assume that \(\gamma_{4}\leq O(\gamma_{2}^{2})\), that is, the equality \(\gamma_{2}^{2}\leq\gamma_{4}\) is somewhat tight -- this ensures that the distribution of \(w=q_{\star}^{\top}u\) under the perfectly-fitted neural network is not too spread-out. We also assume that \(\gamma_{2}\) is smaller than a sufficiently small universal constant. This ensures that the distribution of \(q_{\star}^{\top}u\) under the perfectly-fitted network does not concentrate on 1, i.e. the distribution of \(u\) is not merely a point mass around \(q_{\star}\). In other words, this assumption restricts our setting to the most interesting case where the landscape has bad saddle points (and thus is fundamentally more challenging to analyze). We also assume for simplicity that \(\hat{\sigma}_{0,d}=\hat{h}_{0,d}\) (because otherwise, the activation introduces a constant bias that prohibits perfect fitting), even though adding a trainable scalar to the neural network formulation can remove the assumption. If \(\hat{\sigma}_{0,d}=\hat{h}_{0,d}\), then we can assume without loss of generality that \(\hat{\sigma}_{0,d}=\hat{h}_{0,d}=0\), since \(\hat{\sigma}_{0,d}\) and \(\hat{h}_{0,d}\) will cancel with each other in the population and empirical mean-squared losses (defined below). In summary, we make the following formal assumptions on the Legendre coefficients of the link function and the activation function.

**Assumption 3.2**.: _Let \(\gamma_{2}=\hat{h}_{2,d}/\hat{\sigma}_{2,d}\) and \(\gamma_{4}=\hat{h}_{4,d}/\hat{\sigma}_{4,d}\). We first assume \(\gamma_{4}\geq 1.1\cdot\gamma_{2}^{2}\). For any universal constant \(c_{1}>1\), we assume that \(\hat{\sigma}_{2,d}^{2}/c_{1}\leq\hat{\sigma}_{4,d}^{2}\leq c_{1}\cdot\hat{ \sigma}_{2,d}^{2}\), and \(\gamma_{4}\leq c_{1}\gamma_{2}^{2}\). For a sufficiently small universal constant \(c_{2}>0\) (which is chosen after \(c_{1}\) is determined), we assume \(0\leq\gamma_{2}\leq c_{2}\). We also assume that \(d\) is larger than a sufficiently large constant \(c_{3}\) (which is chosen after \(c_{1}\) and \(c_{2}\).) We also assume \(\hat{h}_{0,d}=\hat{\sigma}_{0,d}=0\), and \(\hat{h}_{1,d}=\hat{h}_{3,d}=0\)._

Our intention is to replicate the ReLU activation as well as possible with quartic polynomials; our assumption that \(\hat{\sigma}_{2,d}^{2}\asymp\hat{\sigma}_{2,d}^{2}\) is indeed satisfied by the quartic expansion of ReLU because \(\widehat{\text{relu}_{2,d}}\asymp d^{-1/2}\) and \(\widehat{\text{relu}_{2,d}}\asymp d^{-1/2}\) (see Proposition C.3). Following the convention defined in Section 2, we will simply write \(\hat{\sigma}_{4,d}^{2}\asymp\hat{\sigma}_{2,d}^{2}\), \(\gamma_{4}\geq 1.1\cdot\gamma_{2}^{2}\), and \(\gamma_{4}\lesssim\gamma_{2}^{2}\).

Our assumptions rule out the case that \(\gamma_{4}=\gamma_{2}^{2}\), for the sake of simplicity. We believe that our analysis could be extended to this case with some modifications. Our analysis also rules out the case where \(\gamma_{2}=0\) and \(\gamma_{4}\neq 0\), due to the restriction that \(\gamma_{4}\leq c_{1}\gamma_{2}^{2}\). The case \(\gamma_{2}=0\) and \(\gamma_{4}\neq 0\) would have a significantly different analysis, and potentially a different sample complexity, since our analysis in Section 4.2 and Section 5 makes use of the fact that the initial phase of the population and empirical dynamics behaves similarly to a power method, which follows from \(\gamma_{2}\) being nonzero.

_Data distribution, losses, and projected gradient flow._ The population data distribution is assumed to be \(\mathbb{S}^{d-1}\). We draw \(n\) training examples \(x_{1},\dots,x_{n}\stackrel{{\text{i.i.d}}}{{\sim}}\mathbb{S}^{d-1}\). Thus, the population and empirical mean-squared losses are:

\[L(\rho)=\frac{1}{2}\cdot\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\left( f_{\rho}(x)-y(x)\right)^{2},\qquad\text{ and }\quad\widehat{L}(\rho)=\frac{1}{2n}\sum_{i=1}^{n}\left(f_{\rho}(x_{i})-y(x_{i}) \right)^{2}\,.\] (3.3)

To ensure that the weight vectors remain on \(\mathbb{S}^{d-1}\), we perform projected gradient flow on the empirical loss. We start by defining the gradient of the population loss \(L\) with respect to a particle \(u\) at \(\rho\) and the corresponding Riemannian gradient (which is simply the projection of the gradient to the tangent space of \(\mathbb{S}^{d-1}\)):

\[\nabla_{u}L(\rho)=\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\left[(f_{ \rho}(x)-y(x))\sigma^{\prime}(u^{\top}x)x\right]\,,\quad\text{and}\quad\text{ grad}_{u}L(\rho)=(I-uu^{\top})\nabla_{u}L(\rho)\,.\] (3.4)

Here we interpret \(L(\rho)\) as a function of a collection of particles (denoted by \(\rho\)) and \(\nabla_{u}L(\rho)\) as the partial derivative with respect to a single particle \(u\) evaluated at \(\rho\). Similarly, the (Riemannian) gradient of the empirical loss \(\widehat{L}\) with respect to the particle \(u\) is defined as

\[\nabla_{u}\widehat{L}(\rho)=\frac{1}{n}\sum_{i=1}^{n}(f_{\rho}(x_{i})-y(x_{i} ))\sigma^{\prime}(u^{\top}x_{i})x_{i}\,,\quad\text{and}\quad\text{grad}_{u} \widehat{L}(\rho)=(I-uu^{\top})\nabla_{u}\widehat{L}(\rho)\,.\] (3.5)

**Population, Infinite-Width Dynamics.** Let the initial distribution \(\rho_{0}\) of the infinite-width neural network be the uniform distribution over \(\mathbb{S}^{d-1}\). We use \(\chi\) to denote a particle sampled uniformly at random from the initial distribution \(\rho_{0}\). A particle initialized at \(\chi\) follows a deterministic trajectory afterwards -- we use \(u_{t}(\chi)\) to denote the location, at time \(t\), of the particle that was initialized at \(\chi\). Because \(u_{t}(\cdot)\) is a deterministic function, we can use \(\chi\) to index the particles at any time based on their initialization. The projected gradient flow on an infinite-width neural network and using population loss \(L\) can be described as

\[\forall\chi\in\mathbb{S}^{d-1},u_{0}(\chi) =\chi\,,\] (3.6) \[\frac{du_{t}(\chi)}{dt} =-\text{grad}_{u_{t}}L(\rho_{t})\,,\] (3.7) \[\text{and}\ \rho_{t} =\text{distribution of}\ u_{t}(\chi)\ \text{(where}\ \chi\sim\rho_{0})\,.\] (3.8)

**Empirical, Finite-Width Dynamics.** The training dynamics of a neural network with width \(m\) can be described in this language by setting the initial distribution to be a discrete distribution uniformly supported on \(m\) initial weight vectors. The update rule will maintain that at any time, the distribution of neurons is uniformly supported over \(m\) items and thus still corresponds to a width-\(m\) neural network. Let \(\chi_{1},\dots,\chi_{m}\overset{\text{i.i.d.}}{\sim}\mathbb{S}^{d-1}\) be the initial weight vectors of the width-\(m\) neural network, and let \(\hat{\rho}_{0}=\text{unif}(\{\chi_{1},\dots,\chi_{m}\})\) be the uniform distribution over these initial neurons. We use \(\chi\in\{\chi_{1},\dots,\chi_{m}\}\) to index neurons and denote a single initial neuron as \(\hat{u}_{0}(\chi)=\chi\). Then, we can describe the projected gradient flow on the empirical loss \(\widehat{L}\) with initialization \(\{\chi_{1},\dots,\chi_{m}\}\) by:

\[\frac{d\hat{u}_{t}(\chi)}{dt} =-\text{grad}_{\hat{u}_{t}(\chi)}\widehat{L}(\hat{\rho}_{t})\,,\] (3.9) \[\text{and}\ \hat{\rho}_{t} =\text{distribution of}\ \hat{u}_{t}(\chi)\ \text{(where}\ \chi\sim\hat{\rho}_{0})\,.\] (3.10)

We first state our result on the population, infinite-width dynamics.

**Theorem 3.3** (Population, infinite-width dynamics).: _Suppose Assumption 3.2 holds, and let \(\epsilon\in(0,1)\) be the target error. Let \(\rho_{t}\) be the result of projected gradient flow on the population loss, initialized with the uniform distribution on \(\mathbb{S}^{d-1}\), as defined in Eq. (3.7). Let \(T_{*,\epsilon}=\inf\{t>0\ |\ L(\rho_{t})\leq\frac{1}{2}(\hat{\sigma}_{2,d}^{2}+ \hat{\sigma}_{4,d}^{2})\epsilon^{2}\}\) be the earliest time \(t\) such that a loss of at most \(\frac{1}{2}(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\) is reached. Then, we have_

\[T_{*,\epsilon}\lesssim\frac{1}{\hat{\sigma}_{2,d}^{2}\gamma_{2}}\log d+\frac{( \log\log d)^{20}}{\hat{\sigma}_{2,d}^{2}\epsilon\gamma_{2}^{8}}\log\left( \frac{\gamma_{2}}{\epsilon}\right).\] (3.11)

The proof is given in Appendix D. The first term on the right-hand side of Eq. (3.11) corresponds to the burn-in time for the network to reach a region around where the Polyak-Lojasiewicz condition holds. We divide our analysis of this burn-in phase into two phases, Phase 1 and Phase 2, and we obtain tight control on the factor by which the signal component, \(q_{*}^{\top}\,u\), grows during Phase 1, while Phase 2 takes place for a comparatively short period of time. (This tight control is critical for our sample complexity bounds where we must show that the coupling error does not blow up too much -- see more discussion below Lemma 5.4.) Phases 1 and 2 are mostly governed by the quadratic components in the activation and target functions, and the dynamics behave similarly to a power method update. After the burn-in phase, the dynamics operate for a short period of time (Phase 3) in a regime where the Polyak-Lojasiewicz condition holds. We explicitly prove the dynamics stay away from saddle points during this phase, as further discussed in Section 4.2.

We note that Theorem 3.3 provides a concrete polynomial runtime bound for projected gradient flow which is not achievable by prior mean-field analyses [21, 56, 2] using Wasserstein gradient flow techniques. For instance, while the population dynamics of Abbe et al. [2] only trains the hidden layer for \(O(1)\) time, our projected gradient flow updates the hidden layer for \(O(\log d)\) time. The main challenge in the proof is to deal with the saddle points that are not strict-saddle [32] in the loss landscape which cannot be escaped simply by adding noise in the parameter space [45, 49, 24].3 Our analysis develops a fine-grained analysis of the dynamics that shows the iterates stay away from saddle points, which allows us to obtain the running time bound in Theorem 3.3 which can be translated to a polynomial-width guarantee in Theorem 3.4. In contrast, Wei et al. [76] escape the saddles by randomly replacing an exponentially small fraction of neurons, which makes the network require exponential width.

Footnote 3: There are a long list of prior works on the loss landscape of neural networks [55, 64, 72, 46, 16, 15, 11, 66, 71, 33, 80, 18].

Next, we state the main theorem on projected gradient flow on empirical, finite-width dynamics.

**Theorem 3.4** (Empirical, finite-width dynamics).: _Suppose Assumption 3.2 holds. Suppose \(\epsilon=\frac{1}{\log\log d}\) is the target error and \(T_{*,\epsilon}\) is the running time defined in Theorem 3.3. Suppose \(n\geq d^{\mu}(\log d)^{\Omega(1)}\) for any constant \(\mu>3\), and the network width \(m\) satisfies \(m\gtrsim d^{2.5+\mu/2}(\log d)^{\Omega(1)}\), and \(m\leq d^{C}\) for some sufficiently large universal constant \(C\). Let \(\hat{\rho}_{t}\) be the projected gradient flow on the empirical loss, initialized with \(m\) uniformly sampled weights, defined in Eq. (3.10). Then, with probability at least \(1-\frac{1}{d^{\Omega(\log d)}}\) over the randomness of the data and the initialization of the finite-width network, we have that \(L(\hat{\rho}_{T_{*,\epsilon}})\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4, d}^{2})\epsilon^{2}\)._

Plugging in \(\mu=3.1\), Theorem 3.4 suggests that, when the network width is at least \(d^{4.05}\) (up to logarithmic factors), the empirical dynamics of gradient descent could achieve \(\hat{\sigma}_{\text{max}}^{2}(1/\log\log d)^{2}\) population loss with \(d^{3.1}\) samples (up to logarithmic factors).

In our analysis, we will establish a coupling between neurons in the empirical dynamics and neurons in the population dynamics. The main challenge is to bound the coupling error during Phase 1 where the population dynamics are similar to the power method. During this phase, we show that the coupling error (i.e., the distance between coupled neurons) remains small by showing that it can be upper bounded in terms of the growth of the signal \(q_{\tau}^{T}u\) in the population dynamics. Such a delicate relationship between the growth of the error and that of the signal is the main challenge to proving a sample complexity bound better than NTK. Even an additional constant factor in the growth rate of the coupling error would lead to an additional \(\text{poly}(d)\) factor in the sample complexity. Prior work (e.g. Mei et al. [57], Abbe et al. [2]) also establishes a coupling between the population and empirical dynamics. However, these works obtain bounds on the coupling error which are exponential in the running time -- specifically, their bounds have a factor of \(e^{KT}\) where \(K\) is a universal constant and \(T\) is the time. In many settings, including ours, \(\Omega(\log d)\) time is necessary to achieve a non-trivial error, (as we further discuss in Section 4.2) and thus this would lead to a \(d^{O(1)}\) bound on the sample complexity, where \(O(1)\) is a unspecified and likely loose constant, which cannot outperform NTK. Our work addresses this challenge by comparing the growth of the coupling error with the growth of the signal, which enables us to control the constant factor in the growth rate of the coupling error.

**Projected Gradient Descent with \(1/\text{poly}(d)\) Step Size.** As a corollary of Theorem 3.4, we show that projected gradient descent with a small learning rate can also achieve low population loss in \(\text{poly}(d)\) iterations. We first define the dynamics of projected gradient descent as follows. As before, we let \(\chi_{1},\ldots,\chi_{m}\overset{\text{i.i.d.}}{\sim}\mathbb{S}^{d-1}\) be the initial weight vectors, and we let \(\tilde{\rho}=\text{unif}(\{\chi_{1},\ldots,\chi_{m}\})\) denote the uniform distribution over these vectors. Thus, we can write a single neuron at initialization as \(\tilde{u}_{0}(\chi)=\chi\) for \(\chi\in\{\chi_{1},\ldots,\chi_{m}\}\). The dynamics of projected gradient descent, on a finite-width neural network and using the empirical loss \(\widehat{L}\), can be then described as

\[\tilde{u}_{t+1}(\chi) =\frac{\tilde{u}_{t}(\chi)-\eta\cdot\text{grad}_{\tilde{u}_{t}( \chi)}\widehat{L}(\tilde{\rho}_{t})}{\|\tilde{u}_{t}(\chi)-\eta\cdot\text{grad} _{\tilde{u}_{t}(\chi)}\widehat{L}(\tilde{\rho}_{t})\|_{2}}\,,\] (3.12) \[\text{and }\tilde{\rho}_{t} =\text{distribution of }\tilde{u}_{t}(\chi)\text{ (where }\chi\sim\tilde{\rho}_{0})\,.\] (3.13)

where \(\eta>0\) is the learning rate.4 We show that projected gradient descent with a \(1/\text{poly}(d)\) learning rate can achieve a low population loss in \(\text{poly}(d)\) iterations:

Footnote 4: See Chapter 3, page 20 of Boumal [17], accessed on August 21, 2023.

**Theorem 3.5** (Projected Gradient Descent).: _Suppose we are in the setting of Theorem 3.4. Let \(\hat{\rho}_{t}\) be the discrete-time projected gradient descent on the empirical loss, initialized with \(m\) weight vectors sampled uniformly from \(\mathbb{S}^{d-1}\), defined in Eq. (3.13). Finally, assume that \(\eta\leq\frac{1}{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{2}}\) for a sufficiently large universal constant \(B\). Then, \(L(\tilde{\rho}_{T_{*,\epsilon}/\eta})\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2})\epsilon^{2}\)._

Theorem 3.5 follows from Theorem 3.4 together with a standard inductive argument to bound the discretization error. The full proof is in Appendix F.

The following theorem states that in the setting of Theorem 3.4, kernel methods with any inner product kernel require \(\Omega(d^{4}(\ln d)^{-6})\) samples to achieve a non-trivial population loss. (Note that the zero function has loss \(\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[y(x)^{2}]\geq(\hat{h}_{4,d})^{2}\).)

**Theorem 3.6** (Sample complexity lower bound for kernel methods).: _Let \(K\) be an inner product kernel. Suppose \(d\) is larger than a universal constant and \(n\lesssim d^{4}(\ln d)^{-6}\). Then, with probability at least \(1/2\) over the randomness of \(n\) i.i.d data points \(\{x_{i}\}_{i=1}^{n}\) drawn from \(\mathbb{S}^{d-1}\), any estimator of \(y\) of the form \(f(x)=\sum_{i=1}^{n}\beta_{i}K(x_{i},x)\) of \(\{x_{i}\}_{i=1}^{n}\) must have a large error: \(\mathbb{E}_{x\sim\mathbb{S}^{d-1}}(y(x)-f(x))^{2}\geq\frac{3}{4}(\hat{h}_{4,d} )^{2}\)._

Theorem 3.4 and Theorem 3.6 together prove a clear sample complexity separation between gradient flow and NTK. When \(\hat{h}_{4,d}\asymp\hat{\sigma}_{\text{max}}\) (i.e., \(\gamma_{2},\gamma_{4}\asymp 1\)), gradient flow with finite-width neural networks can achieve \((\hat{h}_{4,d})^{2}(\log\log d)^{-2}\) population error with \(d^{3.1}\) samples, while kernel methods with any inner product kernel (including NTK) must have an error at least \((\hat{h}_{4,d})^{2}/2\) with \(d^{3.9}\) samples.

On a high level, Abbe et al. [2; 3] prove similar lower bounds in a different setting where the target function is drawn randomly and the data points can be arbitrary (also see Kamath et al. [47], Hsu et al. [42], Hsu [41]). In comparison, our lower bound works for a fixed target function by exploiting the randomness of the data points. In fact, we can strengthen Theorem 3.6 by proving a \(\Omega(\hat{h}_{k,d}^{2})\) loss lower bound for any universal constant \(k\geq 0\) (Theorem H.2). We defer the proof of Theorem 3.6 to Appendix H.

## 4 Analysis of Population Dynamics

### Symmetry of the Population Dynamics

A key observation is that due to the symmetry in the data, the population dynamics \(\rho_{t}\) has a symmetric distribution in the subspace orthogonal to the vector \(q_{\star}\). As a result, the dynamics \(\rho_{t}\) can be precisely characterized by the dynamics in the direction of \(q_{\star}\).

Recall that \(w=q_{\star}^{\top}u\) denotes the projection of a weight vector \(u\) in the direction of \(q_{\star}\). Let \(z=(I-q_{\star}\vec{q_{\star}}^{\top})u\) be the remaining component. For notational convenience, without loss of generality, we can assume that \(q_{\star}=\bm{e_{1}}\) and write \(u=(w,z)\), where \(w\in[-1,1]\) and \(z\in\sqrt{1-w^{2}}\cdot\mathbb{S}^{d-2}\). _We will use this convention throughout the rest of the paper_. We will use \(w_{t}(\chi)\) to refer to the first coordinate of the particle \(u_{t}(\chi)\), and \(z_{t}(\chi)\) to refer to the last \((d-1)\) coordinates.

**Definition 4.1** (Rotational invariance and symmetry).: _We say a neural network \(\rho\) is rotationally invariant if for \(u=(w,z)\sim\rho\), the distribution of \(z\mid w\) is uniform over \(\sqrt{1-w^{2}}\mathbb{S}^{d-2}\) almost surely. We also say \(\rho\) is symmetric w.r.t a variable \(w\) if the density of \(w\) is an even function._

We note that any polynomial-width neural network (e.g., \(\hat{\rho}\)) is very far from rotationally invariant, and therefore the definition is specifically used for population dynamics. If \(\rho\) is rotationally invariant and symmetric, then \(L(\rho)\) has a simpler form that only depends on the marginal distribution of \(w\).

**Lemma 4.2**.: _Let \(\rho\) be a rotationally invariant neural network. Then, for any target function \(h\) and any activation \(\sigma\),_

\[L(\rho)=\tfrac{1}{2}\sum_{k=0}^{\infty}\left(\hat{\sigma}_{k,d}\,\mathbb{E}_{u \sim\rho}[P_{k,d}(w)]-\hat{h}_{k,d}\right)^{2}.\] (4.1)

_In addition, suppose \(h\) and \(\sigma\) satisfy Assumption 3.2 and \(\rho\) is symmetric. Then_

\[L(\rho)=\tfrac{\hat{\sigma}_{2,d}^{2}}{2}\Big{(}\,\mathbb{E}_{u\sim\rho}[P_{2,d}(w)]-\gamma_{2}\Big{)}^{2}+\tfrac{\hat{\sigma}_{2,d}^{2}}{2}\Big{(}\, \mathbb{E}_{u\sim\rho}[P_{4,d}(w)]-\gamma_{4}\Big{)}^{2}.\] (4.2)The proof of Lemma 4.2 is deferred to Appendix D.1. Eq. (4.1) says that if \(\rho\) is rotationally invariant, then \(L(\rho)\) only depends on the marginal distribution of \(w\). Eq. (4.2) says that if the distribution of \(w\) is additionally symmetric and \(\sigma\) and \(h\) are quartic, then the terms corresponding to odd \(k\) and the higher order terms for \(k>4\) in Eq. (4.1) vanish. Note that \(P_{2,d}(s)\approx s^{2}\) and \(P_{4,d}(s)\approx s^{4}\) by Eq. (C.4). Thus, \(L(\rho)\) essentially corresponds to matching the second and fourth moments of \(w\) to some desired values \(\gamma_{2}\) and \(\gamma_{4}\). Inspired by this lemma, we define the following key quantities: for any time \(t\geq 0\), we define \(D_{2,t}=\mathbb{E}_{u\sim\rho_{t}}[P_{2,d}(w)]-\gamma_{2}\) and \(D_{4,t}=\mathbb{E}_{u\sim\rho_{t}}[P_{4,d}(w)]-\gamma_{4}\), where \(\rho_{t}\) is defined according to the population, infinite-width dynamics (Eq. (3.6), Eq. (3.7) and Eq. (3.8)).

We next show that the rotational invariance and symmetry properties of \(\rho_{t}\) are indeed maintained:

**Lemma 4.3**.: _Suppose we are in the setting of Theorem 3.3. At any time \(t\in[0,\infty)\), \(\rho_{t}\) is symmetric and rotationally invariant._

Rotational invariance follows from the rotational invariance of the data. To show symmetry, we use Eq. (4.1), and the facts that \(P_{k,d}\) is an odd polynomial for odd \(k\) and \(\rho_{t}\) is symmetric at initialization. Using Lemma 4.2 and Lemma 4.3, we obtain a simple formula for the dynamics of \(w_{t}\):

**Lemma 4.4** (1-dimensional dynamics).: _Suppose we are in the setting of Theorem 3.3. Then, for any \(\chi\in\mathbb{S}^{d-1}\), writing \(w_{t}:=w_{t}(\chi)\), we have_

\[\frac{dw_{t}}{dt}=\underbrace{-(1-w_{t}^{2})\cdot(P_{t}(w_{t})+Q_{t}(w_{t}))}_ {\triangleq v(w_{t})},\] (4.3)

_where for any \(w\in[-1,1]\), we have \(P_{t}(w)=2\delta_{2,d}^{2}D_{2,t}w+4\delta_{4,d}^{2}D_{4,t}w^{3}\), and \(Q_{t}(w)={\lambda_{d}}^{(1)}w+{\lambda_{d}}^{(3)}w^{3}\), where \(|{\lambda_{d}}^{(1)}|,|{\lambda_{d}}^{(3)}|\lesssim\frac{\delta_{2,d}^{2}|D_{2,t}|+\delta_{4,d}^{2}|D_{4,t}|}{\delta}\). More specifically, \({\lambda_{d}}^{(1)}=2\delta_{2,d}^{2}D_{2,t}\cdot\frac{1}{d-1}-2\delta_{4,d}^ {2}D_{4,t}\cdot\frac{6d+12}{d^{2}-1}\) and \({\lambda_{d}}^{(3)}=4\delta_{4,d}^{2}D_{4,t}\cdot\frac{6d+9}{d^{2}-1}\)._

Eq. (4.3) is a properly defined dynamics for \(w\) because the update rule for \(w_{t}\) only depends on \(w_{t}\) and the quantities \(D_{2,t}\) and \(D_{4,t}\) -- additionally, \(D_{2,t}\) and \(D_{4,t}\) only depend on the distribution of \(w\). We henceforth refer to the \(w_{t}(\chi)\) as particles -- this is well-defined by Lemma 4.4.

### Analysis of One-Dimensional Population Dynamics

We also use \(\iota\in[-1,1]\) to refer to the first coordinate of \(\chi\), the initialization of a particle under the population dynamics. We note that \(\iota=\langle\chi,e_{1}\rangle\) and therefore, the distribution of \(\iota\) is \(\mu_{d}\). For any time \(t\geq 0\), we use \(w_{t}(\iota)\) to refer to \(w_{t}(\chi)\) for any \(\chi\in\mathbb{S}^{d-1}\). This notation is also well-defined by Lemma 4.4. We divide our proof of Theorem 3.3 into three phases, which are defined as follows. For ease of presentation, we will only consider particles \(w_{t}(\iota)\) for \(\iota>0\) in the following discussion. Our argument also applies for \(\iota<0\) by the symmetry of \(\rho_{t}\) (Lemma 4.3).

**Definition 4.5** (Phase 1).: _Let \(w_{\text{max}}=\frac{1}{\log d}\) and \(\iota_{\text{U}}=\frac{\log d}{\sqrt{d}}\). Let \(T_{1}>0\) be the minimum time such that \(w_{T_{1}}(\iota_{\text{U}})=w_{\text{max}}:=\frac{1}{\log d}\). We refer to the time interval \([0,T_{1}]\) as Phase 1._

Note that essentially all of the particles are less than \(\iota_{\text{U}}=\frac{\log d}{\sqrt{d}}\) at initialization by tail bounds for \(\mu_{d}\). During Phase 1, the term corresponding to \(D_{2,t}\) in the velocity dominates, and all particles grow by a \(\frac{\sqrt{d}}{(\log d)^{2}}\) factor. During Phase 1, the loss does not decrease much, but a large portion of the particles have grown by a large factor. During Phase 2, the particles and their velocity will become large.

**Definition 4.6** (Phase 2).: _Let \(T_{2}>0\) be the minimum time such that either \(D_{2,T_{2}}=0\) or \(D_{4,T_{2}}=0\). We refer to the time interval \([T_{1},T_{2}]\) as Phase 2. Note that \(T_{2}>T_{1}\) by Lemma D.8._

**Definition 4.7** (Phase 3).: _Phase 3 is defined as the time interval \([T_{2},\infty)\)._

We divide this phase into two cases, which are both more challenging to analyze than Phases 1 and 2: (i) \(D_{2,T_{2}}=0\) and \(D_{4,T_{2}}<0\), and (ii) \(D_{2,T_{2}}<0\) and \(D_{4,T_{2}}=0\). Here we discuss Case 1 -- the analyses of Cases 1 and 2 are in Appendix D.5 and Appendix D.6 respectively. Suppose Case 1 holds, i.e. \(D_{2,T_{2}}=0\) and \(D_{4,T_{2}}<0\). Then, for all \(t\geq T_{2}\), we will have \(D_{2,t}\geq 0\) and \(D_{4,t}\leq 0\) (Lemma D.19 and Lemma D.20), meaning that for \(t\geq T_{2}\) a \(\text{poly}(\gamma_{2})\) fraction of particles will be far from \(0\) and \(1\) (Lemma D.17). However, this does not guarantee a large average velocity -- unlike in Phases 1 and 2, the velocity \(v(w)=P_{t}(w)+Q_{t}(w)\) defined in Lemma 4.4 may have a positive root \(r\in(0,1)\), and the root \(r\) can give rise to bad stationary points because for certain values of \(r\), if \(\rho\) degenerates to the singleton distribution on the root \(r\), the velocity at \(r\) would be exactly \(0\). Indeed, we can construct such an \(r\), intuitively because \(D_{2,t}\) and \(D_{4,t}\) have different signs in \(P_{t}(w)\) and \(Q_{t}(w)\approx O(1/d)\) is a lower order term (Lemma D.35).

Thus, we must leverage some information about the trajectory to show that the average velocity is large when \(D_{2,t},D_{4,t}\) have different signs. To this end, we prove that \(\mathbb{E}_{u\sim\rho_{t}}[(w-r)^{2}]\gtrsim\mathbb{E}_{w,w^{\prime}\sim\rho_ {t}}(w-w^{\prime})^{2}\) is large by designing a novel potential function \(\Phi(w):=\log(\frac{w}{\sqrt{1-w^{2}}})\). We show that \(|\Phi(w)-\Phi(w^{\prime})|\) is always increasing for any two particles \(w,w^{\prime}\) (Lemma D.13). Because \(\Phi\) is Lipschitz on an interval bounded away from \(0\) and \(1\), a lower bound on \(|\Phi(w)-\Phi(w^{\prime})|\) also leads to a lower bound on \((w-w^{\prime})^{2}\), and thus the particles away from \(0\) and \(1\) will have a large variance. Recall that when \(D_{2,t}>0\) and \(D_{4,t}<0\), a large portion of particles are away from \(0\) and \(1\), and hence, the average velocity is large. In Appendix B, we include simulations to illustrate the effects of Phases 1, 2 and 3.

## 5 Analysis of Empirical, Finite-Width Dynamics

**Coupling Between Empirical and Population Dynamics.** To analyze the difference between the empirical and population dynamics, we define an intermediate process \(\bar{\rho}_{t}\), where the initial particles are from \(\hat{\rho}_{0}\), but the dynamics of the particles then follow the population trajectories:

\[\bar{\rho}_{0} =\hat{\rho}_{0}=\text{unif}(\{\chi_{1},\ldots,\chi_{m}\})\,,\] \[\text{and }\bar{\rho}_{t} =\text{distribution of }u_{t}(\chi)\text{ (where }\chi\sim\bar{\rho}_{0})\,.\] (5.1)

For \(\chi\sim\text{unif}(\{\chi_{1},\ldots,\chi_{m}\})\), let \(\bar{u}_{t}(\chi)=u_{t}(\chi)\). Let \(\Gamma_{t}\) be the joint distribution of \((\bar{u}_{t}(\chi),\hat{u}_{t}(\chi))\). Then, \(\Gamma_{t}\) forms a natural coupling between \(\bar{\rho}_{t}\) and \(\hat{\rho}_{t}\). We will use \(\bar{u}_{t}\) and \(\hat{u}_{t}\) as shorthands for the random variables \(\bar{u}_{t}(\chi),\hat{u}_{t}(\chi)\) respectively in the rest of this section. We define the average distance \(\overline{\Delta}_{t}^{2}:=\mathbb{E}_{(\hat{u}_{t},\hat{u}_{t})\sim\Gamma_{t }}[\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}]\). Intuitively, \(f_{\rho_{t}}(x)\) and \(f_{\hat{\rho}_{t}}(x)\) are close when \(\hat{u}_{t}\) and \(\bar{u}_{t}\) are close, which is formalized by the following lemma:

**Lemma 5.1**.: _In the setting of Theorem 3.4, let \(T\leq\frac{d^{O(1)}}{\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}}\). Then, with probability at least \(1-\exp(-d^{2})\) over the initialization, we have for all \(t\in[0,T]\) that \(\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho_{t}}(x)-f_{\bar{\rho}_{t}}(x))^{2}] \lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{2}(\log d)^{O (1)}}{m}\) and \(\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}_{t}}(x)-f_{\bar{\rho}_{t}}(x ))^{2}]\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\overline{ \Delta}_{t}^{2}\)._

The proof of Lemma 5.1 is deferred to Appendix E. As a simple corollary of Lemma 5.1, we can upper bound \(\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho_{t}}(x)-f_{\hat{\rho}_{t}}(x))^{2}]\) by the triangle inequality. Thus, so long as \(\overline{\Delta}_{T_{*,\epsilon}}\) is small, we can show that the empirical and population dynamics achieve similar test error at time \(T_{*,\epsilon}\).

**Upper Bound for \(\overline{\Delta}_{T_{*,\epsilon}}\).** The following lemma gives our upper bound on \(\overline{\Delta}_{T_{*,\epsilon}}\):

**Lemma 5.2** (Final Bound on \(\overline{\Delta}_{T_{*,\epsilon}}\)).: _In the setting of Theorem 3.4, we have \(\overline{\Delta}_{T_{*,\epsilon}}\leq d^{-\frac{\mu-1}{4}}\)._

We prove Lemma 5.2 by induction over the time \(t\leq T_{*,\epsilon}\). Let us define the maximal distance \(\Delta_{\text{max},\,t}:=\max_{(\hat{u}_{t},\bar{u}_{t})\in\text{supp}( \Gamma_{t})}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}\) where the max is over the _finite_ support of the coupling \(\Gamma_{t}\). We will control \(\overline{\Delta}_{t}\) and \(\Delta_{\text{max},\,t}\) simultaneously using induction. The inductive hypothesis is:

**Assumption 5.3** (Inductive Hypothesis).: _For some universal constant \(1>\phi>1/2\) and \(\psi\in(0,\phi-\frac{1}{2})\), we say that the inductive hypothesis holds at time \(T\) if, for all \(0\leq t\leq T\), we have \(\overline{\Delta}_{t}\leq d^{-\phi}\) and \(\Delta_{\text{max},\,t}\leq d^{-\psi}\)._

Showing that the inductive hypothesis holds for all \(t\) requires studying the growth of the error \(\|\hat{u}_{t}-\bar{u}_{t}\|\). We analyze it using the decomposition5\(\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}=A_{t}+B_{t}+C_{t}\,,\) where \(A_{t}:=-2(\text{grad}_{\hat{u}}L(\rho_{t})-\text{grad}_{\hat{u}}L(\rho_{t}), \hat{u}_{t}-\bar{u}_{t})\), \(B_{t}:=-2\langle\text{grad}_{\hat{u}}L(\hat{\rho}_{t})-\text{grad}_{\hat{u}}L( \rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle\), and \(C_{t}:=-2\langle\text{grad}_{\hat{u}}\widehat{L}(\hat{\rho}_{t})-\text{grad}_{ \hat{u}}L(\hat{\rho}_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle\). Intuitively, \(A_{t}\) captures the growth of the coupling error due to the population dynamics, \(B_{t}\) the growth due to the discrepancy between the gradients from the finite-width and infinite-width networks, and \(C_{t}\) the growth due to the discrepancy between finite samples and infinite samples. The following lemma gives upper bounds on these terms:

**Lemma 5.4** (Bounds on \(A_{t},B_{t},C_{t}\)).: _In the setting of Theorem 3.4, suppose the inductive hypothesis (Assumption 5.3) holds up to time \(t\), for some \(t\leq T_{*,\epsilon}\). Let \(\delta_{t}:=\hat{u}_{t}-\bar{u}_{t}\). Let \(T_{1}\) be the runtime of Phase 1 (where Phase 1 is defined in Definition 4.5). Then, we have_

\[A_{t}\leq\begin{cases}4\hat{\sigma}_{2,d}^{2}|D_{2,t}|\left\|\delta_{t}\right\| ^{2}+O\big{(}\frac{\hat{\sigma}_{2,d}^{2}}{\log d}|D_{2,t}|\left\|\delta_{t} \right\|^{2}\big{)}&\text{if }0\leq t\leq T_{1}\\ O(1)\cdot(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\gamma_{2}\left\| \delta_{t}\right\|^{2}&\text{if }T_{1}\leq t\leq T_{*,\epsilon}\end{cases}.\] (5.2)

_Additionally, with probability at least \(1-1\) over the initialization and dataset, we have \(B_{t}\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\cdot\Big{(}\frac {d(\log d)^{O(1)}}{\sqrt{m}}+\overline{\Delta}_{t}\Big{)}\|\delta_{t}\|_{2}\), \(\mathbb{E}[B_{t}]\lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2} )d(\log d)^{O(1)}}{\sqrt{m}}\overline{\Delta}_{t}+(\hat{\sigma}_{2,d}^{2}+ \hat{\sigma}_{4,d}^{2})\overline{\Delta}_{t}^{3}\) and \(C_{t}\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)} \cdot\Big{(}\sqrt{\frac{d}{n}}\|\delta_{t}\|_{2}+\frac{d^{2-2\cdot w}}{n} \overline{\Delta}\|\delta_{t}\|_{2}^{2}+\frac{d^{2\cdot 5-2\cdot w}}{n}\overline{ \Delta}^{2}\|\delta_{t}\|_{2}+\frac{d^{4-w}}{n}\overline{\Delta}^{2}\|\delta_ {t}\|_{2}^{2}\Big{)}\)._

The proof is in Appendix E. We now give an explanation for each of these bounds. For \(A_{t}\), we establish two separate bounds, one which holds during Phase 1, and one which holds during Phases 2 and 3. Intuitively, the signal part in a neuron (i.e. \(w^{2}\)) grows at a rate \(4\hat{\sigma}_{2,d}^{2}|D_{2,t}|\) during Phase 1 (which follows from Lemma 4.4) and in Eq. (5.2) we show that the growth rate of the coupling error due to the growth in the signal is also at most \(4\hat{\sigma}_{2,d}^{2}|D_{2,t}|\). Intuitively, the growth rates match because the signal parts of the neurons follow similar dynamics to a power method during Phase 1. For example, in the worst case when \(\delta_{t}\) is mostly in the direction of \(e_{1}\), the factor by which \(\delta_{t}\) grows is the same as that of \(w\). More mathematically, this is due to the fact that the first-order term in Lemma 4.4 is the dominant term in \(v(w)-v(\hat{w})\) for any two particles \(w,\hat{w}\) (where \(v(w)\) is the one-dimensional velocity defined in the previous section). To get a sample complexity better than NTK, the precise constant factor \(4\) in the growth rate \(4\hat{\sigma}_{2,d}^{2}|D_{2,t}|w\) is important -- a larger constant would lead to \(\delta_{T_{1}}\) being larger by a poly\((d)\) factor, thus increasing the sample complexity by poly\((d)\).

The same bound for \(A_{t}\) no longer applies during Phases 2 and 3. This is because the dynamics of \(w\) are no longer similar to a power method, and the higher-order terms may make a larger contribution to \(v(w)-v(\hat{w})\) than to the growth of \(w\), or vice versa. Thus we use a looser bound \(O(1)\cdot(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\gamma_{2}\left\| \delta_{t}\right\|^{2}\) in Eq. (5.2). However, since the remaining running time after Phase 1, \(T_{*,\epsilon}-T_{1}\), is very short (only poly\((\log\log d)\) -- this corresponds to the second term of the running time in Theorem 3.3), the total growth of the coupling error is at most \(\exp(\text{poly}(\log\log d))\), which is sub-polynomial and does not contribute to the sample complexity. Note that if the running time during Phases 2 and 3 is longer (e.g. \(O(\log d)\)) then the coupling error would grow by an additional \(d^{O(1)}\) factor during Phases 2 and 3, which would make our sample complexity worse than NTK.

Our upper bounds on \(B_{t}\) and \(C_{t}\) capture the growth of coupling error due to the finite width and samples. We establish a stronger bound for \(\mathbb{E}[B_{t}]\) than for \(B_{t}\). In our proof, we use the bounds for \(B_{t}\) and \(\mathbb{E}[B_{t}]\) to prove the inductive hypothesis for \(\overline{\Delta}_{t}\) and \(\Delta_{\text{max},t}\) respectively. We prove the bound for \(C_{t}\) by expanding the error due to finite samples into second-order and fourth-order polynomials and applying concentration inequalities for higher moments (Lemma I.7).

All of these bounds together control the growth of \(\|\hat{u}_{t}-\bar{u}_{t}\|\) at time \(t\). Using Lemma 5.4 we can show that for some properly chosen \(\phi\) and \(\psi\), the inductive hypothesis in Assumption 5.3 holds until \(T_{*,\epsilon}\) (see Lemma E.3 for the statement), which naturally leads to the bound in Lemma 5.2. The full proof can be found in Appendix E.

## 6 Conclusion

In this paper, we prove a clear sample complexity separation between vanilla gradient flow and kernel methods with any inner product kernel, including NTK. Our work leads to several directions for future research. The first question is to generalize our results to the ReLU activation. Another question is whether gradient descent can achieve less than \(d^{3}\) sample complexity in our setting. The work of Arous et al. [11] shows that if the population loss has information exponent \(2\), then a sample complexity of \(d\) (up to logarithmic factors) can be achieved with a single-neuron student network -- it is an open question if a similar sample complexity could be obtained in our setting. A final open question is whether two-layer neural networks can be shown to attain arbitrarily small generalization error \(\epsilon\) -- one limitation of our analysis is that we require \(\epsilon\geq\text{poly}(1/\log\log d)\).

## Acknowledgments

The authors would like to thank Zhiyuan Li for helpful discussions. The authors would like to thank the support of NSF IIS 2045685, NSF Grant CCF-1844628 and a Sloan Research Fellowship.

## References

* Abbe et al. [2021] Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning. _Advances in Neural Information Processing Systems_, 34:26989-27002, 2021.
* Abbe et al. [2022] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In _Conference on Learning Theory_, pages 4782-4887. PMLR, 2022.
* Abbe et al. [2023] Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. _arXiv preprint arXiv:2302.11055_, 2023.
* Adamczak et al. [2009] Radoslaw Adamczak, Alexander E. Litvak, Alain Pajor, and Nicole Tomczak-Jaegermann. Quantitative estimates of the convergence of the empirical covariance matrix in log-concave ensembles. _Journal of the American Mathematical Society_, 23(2):535-561, oct 2009. doi: 10.1090/s0894-0347-09-00650-x. URL https://doi.org/10.1090%2Fs0894-0347-09-00650-x.
* Allen-Zhu and Li [2019] Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? _arXiv preprint arXiv:1905.10337_, 2019.
* Allen-Zhu and Li [2020] Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep learning. _arXiv preprint arXiv:2001.04413_, 2020.
* Allen-Zhu et al. [2019] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. _Advances in neural information processing systems_, 32, 2019.
* Arnaboldi et al. [2023] Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro. From high-dimensional & mean-field dynamics to dimensionless odes: A unifying approach to sgd in two-layers networks, 2023.
* Arora et al. [2019] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. _arXiv preprint arXiv:1904.11955_, 32, 2019. URL https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf.
* Arora et al. [2019] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. _arXiv preprint arXiv:1901.08584_, pages 322-332, 2019.
* Arous et al. [2021] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent on non-convex losses from high-dimensional inference. _The Journal of Machine Learning Research_, 22(1):4788-4838, 2021.
* Atkinson and Han [2012] Kendall Atkinson and Weimin Han. _Spherical harmonics and approximations on the unit sphere: an introduction_, volume 2044. Springer Science & Business Media, 2012.
* Bainov and Simeonov [1992] Drumi D Bainov and Pavel S Simeonov. _Integral inequalities and applications_, volume 57. Springer Science & Business Media, 1992.
* Barak et al. [2022] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. _Advances in Neural Information Processing Systems_, 35:21750-21764, 2022.
* Arous et al. [2020] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Algorithmic thresholds for tensor pca. 2020.

* Bietti et al. [2022] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. _arXiv preprint arXiv:2210.15651_, 2022.
* Boumal [2023] Nicolas Boumal. _An introduction to optimization on smooth manifolds_. Cambridge University Press, 2023. doi: 10.1017/9781009166164. URL https://www.nicolasboumal.net/book.
* Brutzkus and Globerson [2017] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. _arXiv preprint arXiv:1702.07966_, 2017.
* Chen et al. [2023] Fan Chen, Zhenjie Ren, and Songbo Wang. Uniform-in-time propagation of chaos for mean field langevin dynamics, 2023.
* Chen et al. [2020] Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard Socher. Towards understanding hierarchical learning: Benefits of neural representations. _Neural Information Processing Systems (NeurIPS)_, 2020.
* Chizat and Bach [2018] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. _Advances in neural information processing systems_, 31, 2018.
* Chizat and Bach [2018] Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming. _arXiv preprint arXiv:1812.07956_, 8, 2018.
* Damian et al. [2022] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In _Conference on Learning Theory_, pages 5413-5452. PMLR, 2022.
* Daneshmand et al. [2018] Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with stochastic gradients. In _International Conference on Machine Learning_, pages 1155-1164. PMLR, 2018.
* Daniely and Malach [2020] Amit Daniely and Eran Malach. Learning parities with neural networks. _Advances in Neural Information Processing Systems_, 33:20356-20365, 2020.
* Daniely et al. [2016] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In _Advances In Neural Information Processing Systems_, pages 2253-2261, 2016.
* Dong and Ma [2023] Kefan Dong and Tengyu Ma. Toward \(l_{\infty}\)-recovery of nonlinear functions: A polynomial sample complexity bound for gaussian random fields. _arXiv preprint arXiv:2305.00322_, 2023.
* Dou and Liang [2021] Xialiang Dou and Tengyuan Liang. Training neural networks as learning data-adaptive kernels: Provable representation and approximation benefits. _Journal of the American Statistical Association_, 116(535):1507-1520, 2021.
* Du and Lee [2018] Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic activation. _International Conference on Machine Learning (ICML)_, 2018.
* Du et al. [2018] Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. _arXiv preprint arXiv:1811.03804_, pages 1675-1685, November 2018.
* Gabcke [1979] Wolfgang Gabcke. _New derivation and explicit remainder estimation of the Riemann-Siegel formula_. PhD thesis, Georg-August-Universityat Gottingen, 1979.
* Ge et al. [2015] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points--online stochastic gradient for tensor decomposition. In _Conference on Learning Theory_, COLT 2015, pages 797-842, 2015.
* Ge et al. [2017] Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape design. _arXiv preprint arXiv:1711.00501_, 2017.
* Ghorbani et al. [2020] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks outperform kernel methods? _Advances in Neural Information Processing Systems_, 33:14820-14830, 2020.

* [35] Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in terms of optimization geometry. _arXiv preprint arXiv:1802.08246_, 2018.
* [36] Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. In _Advances in Neural Information Processing Systems_, pages 9461-9471, 2018.
* [37] Karl Hajjar and Lenaic Chizat. On the symmetries in the dynamics of wide two-layer neural networks, 2023.
* [38] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. _arXiv preprint arXiv:1611.04231_, 2016.
* [39] Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. _Journal of Machine Learning Research_, 19:1-44, 2018.
* [40] Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. In _Advances in Neural Information Processing Systems_, pages 1594-1602, July 2015.
* [41] Daniel Hsu. Dimension lower bounds for linear approaches to function approximation. _Daneil Hsu's homepage_, 2021.
* [42] Daniel Hsu, Clayton H Sanford, Rocco Servedio, and Emmanouil Vasileios Vlatakis-Gkaragkounis. On the approximation power of two-layer networks of random relus. In _Conference on Learning Theory_, pages 2423-2461. PMLR, 2021.
* [43] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In _Advances in neural information processing systems_, pages 8571-8580, 2018.
* [44] Adel Javanmard, Marco Mondelli, and Andrea Montanari. Analysis of a two-layer neural network via displacement convexity. 2020.
* [45] Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points. _Journal of the ACM (JACM)_, 68(2):1-29, 2021.
* [46] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. In _Advances in Neural Information Processing Systems_, pages 927-935, 2011.
* [47] Pritish Kamath, Omar Montasser, and Nathan Srebro. Approximate is good enough: Probabilistic variants of dimensional and margin complexity. In _Conference on Learning Theory_, pages 2236-2262. PMLR, 2020.
* [48] Kenji Kawaguchi. Deep learning without poor local minima. _Advances in neural information processing systems_, 29, 2016.
* [49] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges to minimizers. _arXiv preprint arXiv:1602.04915_, 1050:16, 2016.
* [50] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. In _Advances in Neural Information Processing Systems_, pages 8157-8166, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16067.
* [51] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In _Conference On Learning Theory_, pages 2-47. PMLR, 2018.
* [52] Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural networks beyond ntk. In _Conference on Learning Theory_, pages 2613-2682. PMLR, 2020.
* [53] Tengyu Ma. Lecture notes for machine learning theory (cs229m/stats214), June 2022.

* [54] Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and double descent curve. _arXiv preprint arXiv:1908.05355_, 2019.
* [55] Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for nonconvex losses. _The Annals of Statistics_, 46(6A):2747-2774, 2018.
* [56] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layers neural networks. _Proceedings of the National Academy of Sciences_, pages E7665-E7671, 2018.
* [57] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In _Conference on Learning Theory_, pages 2388-2464. PMLR, 2019.
* [58] Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Erdogdu. Neural networks efficiently learn low-dimensional representations with sgd. _arXiv preprint arXiv:2209.14863_, 2022.
* [59] Eshaan Nichani, Yu Bai, and Jason D. Lee. Identifying good directions to escape the NTK regime and efficiently learn low-degree plus sparse polynomials. In _NeurIPS_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/5d6ae8ba43ecb378030753c4408ef9bd-Abstract-Conference.html.
* [60] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. _IEEE Journal on Selected Areas in Information Theory_, 1(1):84-105, 2020.
* [61] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks. _arXiv preprint arXiv:1805.01053_, 2018.
* [62] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central limit theorem. _Stochastic Processes and their Applications_, 130(3):1820-1852, 2020.
* [63] Maciej Skorski. Bernstein-type bounds for beta distribution. _Modern Stochastics: Theory and Applications_, 10(2):211-228, 2023.
* [64] Mahdi Soltanolkotabi. Learning ReLUs via gradient descent. In _Advances in Neural Information Processing Systems_, pages 2007-2017, 2017.
* [65] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. _IEEE Transactions on Information Theory_, 65(2):742-769, 2018.
* [66] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. _arXiv preprint arXiv:1605.08361_, 2016.
* [67] Taiji Suzuki and Shunta Akiyama. Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods. In _International Conference on Learning Representations_.
* [68] Taiji Suzuki, Denny Wu, and Atsushi Nitanda. Convergence of mean-field langevin dynamics: Time and space discretization, stochastic gradient, and variance reduction, 2023.
* [69] Yan Shuo Tan and Roman Vershynin. Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval. _arXiv preprint arXiv:1910.12837_, 2019.
* [70] Matus Telgarsky. Feature selection with gradient descent on two-layer networks in low-rotation regimes. _arXiv preprint arXiv:2208.02789_, 2022.
* [71] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. _arXiv preprint arXiv:1703.00560_, 2017.

* Vardi et al. [2021] Gal Vardi, Gilad Yehudai, and Ohad Shamir. Learning a single neuron with bias using gradient descent. _Advances in Neural Information Processing Systems_, 34:28690-28700, 2021.
* Vershynin [2010] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. _arXiv preprint arXiv:1011.3027_, 2010.
* Vershynin [2018] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Wei et al. [2019] Colin Wei, Jason D. Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and optimization of neural nets v.s. their induced kernel. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 9709-9721, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/8744cf92c88433f8cb04a02e6db69a0d-Abstract.html.
* Wei et al. [2019] Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and optimization of neural nets vs their induced kernel. In _Advances in Neural Information Processing Systems_, pages 9709-9721, 2019.
* Woodworth et al. [2020] Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. _arXiv preprint arXiv:2002.09277_, 2020.
* Wu et al. [2023] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Learning high-dimensional single-neuron relu networks with finite samples. _arXiv preprint arXiv:2303.02255_, 2023.
* Xu and Du [2023] Weihang Xu and Simon S Du. Over-parameterization exponentially slows down gradient descent for learning a single neuron. _arXiv preprint arXiv:2302.10034_, 2023.
* Zhang et al. [2019] Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu networks via gradient descent. In _The 22nd international conference on artificial intelligence and statistics_, pages 1524-1534. PMLR, 2019.

List of Appendices
* A Additional Related Works
* B Simulations for Population Dynamics
* C Background on Spherical Harmonics
* D Proofs for Population Dynamics
* D.1 Proof of Lemma 4.2
* D.2 Proof of Lemma 4.3 and Lemma 4.4
* D.3 Analysis of Phase 1
* D.3.1 Summary of Phase 1
* D.4 Analysis of Phase 2
* D.4.1 Summary of Phase 2
* D.5 Phase 3, Case 1
* D.6 Phase 3, Case 2
* D.7 Summary of Phase 3
* D.8 Proof of Main Theorem for Population Case
* D.9 Additional Lemmas
* D.10 Helper Lemmas
* E Proofs for Finite-Samples and Finite-Width Dynamics
* E.1 Upper bound on \(A_{t}\)
* E.2 Upper Bound on \(B_{t}\)
* E.3 Upper Bound on \(C_{t}\)
* E.4 Additional Lemmas
* F Proof of Theorem 3.5
* G Missing Proofs in Section 3
* G.1 Proof of Lemma 3.1
* H Proofs for Sample Complexity Lower Bounds for Kernel Method
* H.1 Proof of Theorem 3.6
* H.2 Proof of Lemma H.1
* I Toolbox
	* I.1 Rademacher Complexity and Generalization Bounds
	* I.2 Concentration Lemmas
	* I.3 Helper Lemmas
Additional Related Works

In addition to the related works on the NTK approach, mean-field analyses, and other works analyzing the training dynamics of neural networks that we discussed in Section 1, we also discuss the following additional related works.

There are several works which study mean-field Langevin dynamics [19, 68]. These works show "uniform-in-time propagation of chaos" estimates, i.e. their results imply bounds on the coupling error between finite-width and infinite-width trajectories which do not grow exponentially in the time \(T\). However, it is likely non-trivial to apply these analyses to directly obtain good test error and sample complexity in the setting of two-layer neural networks, since mean-field Langevin dynamics may require many iterations to obtain good population loss. More concretely, Theorem 4 of Mei et al. [56] suggests that the inverse temperature \(\lambda\) for mean-field Langevin dynamics has to be at least proportional to the dimension \(d\) in order for Langevin dynamics to achieve low population loss. This would cause the log-Sobolev constant in Suzuki et al. [68] to be on the order of \(e^{-d}\), leading to a running time of \(e^{-d}\). In comparison, we show in Theorem 3.5 that projected gradient descent with \(\text{poly}(d)\) iterations can achieve a low population loss. We also note that Chen et al. [19] do not study the impact of finite samples on the coupling error.

We also note that there are several works which reduce the population dynamics to a lower-dimensional dynamics [2, 37, 8], as we do in Section 4.1. The focus of our work is on the analysis of the population dynamics (Section 4.2) and coupling error between the empirical and population dynamics (Section 5), rather than the dimension-free dynamics.

Finally, we note that our setting goes beyond that of Abbe et al. [2]: our target function does not satisfy the merged-staircase property required by Abbe et al. [2], since the lowest-degree term in our target function has degree \(2\). The work of Abbe et al. [3] studies functions which have higher "leap complexity," meaning that terms of the target function can introduce more than one new coordinate at once, or introduce a new coordinate using a Hermite polynomial with degree greater than \(1\) (in the case where the inputs are Gaussian). However, Abbe et al. [3] use a non-standard algorithm which separates the coordinates based on whether they are large or small, and applying different projections to each subset of coordinates.

## Appendix B Simulations for Population Dynamics

Here we include some simulations in a setting which is a simplified version of our 1-dimensional dynamics, to illustrate the different phases. Here, the goal is to find a weight vector \(w\in\mathbb{R}^{d}\), where \(d=1000\), that minimizes the following loss:

\[L(w)=\Big{(}\frac{1}{d}\sum_{i=1}^{d}w_{i}^{2}-\gamma_{2}\Big{)}^{2}+\Big{(} \frac{1}{d}\sum_{i=1}^{d}w_{i}^{4}-\gamma_{4}\Big{)}^{2}\] (B.1)

where \(\gamma_{2}=0.8\) and \(\gamma_{4}=0.7\). Intuitively, the \(w_{i}\) play the role of the particles \(w\) in the \(1\)-dimensional dynamics of Section 4.2. We initialize \(w\) as follows -- each coordinate is initialized according to a standard Gaussian distribution with mean \(0\) and standard deviation \(0.0001\). We then train \(w\) using gradient descent on \(L(w)\) with learning rate \(0.1\), for \(100,000\) iterations. We define \(D_{2}=\frac{1}{d}\sum_{i=1}^{d}w_{i}^{2}-\gamma_{2}\) and \(D_{4}=\frac{1}{d}\sum_{i=1}^{d}w_{i}^{4}-\gamma_{4}\).

The dynamics are illustrated by Figure 1, Figure 2, Figure 3 and Figure 4. At initialization, since the standard deviation is \(0.0001\), all of the \(w_{i}\) are very small, as shown in Figure 1. Now, as shown in Figure 4, while both \(D_{2}\) and \(D_{4}\) grow rapidly after a certain point, \(D_{4}\) reaches \(0\) first. At the point where \(D_{4}\) reaches \(0\), the distribution of the coordinates of \(w\) is shown in Figure 2. At this point, the coordinates \(w_{i}\) have grown substantially, with a substantial fraction of coordinates around \(0.5\) or larger. This illustrates the intuition that all of the coordinates grow by a large factor during Phases 1 and 2. The weight histogram after many iterations have passed during Phase 3 is shown in Figure 3. In our infinite-width 1-dimensional population dynamics, when \(D_{4}>0\) and \(D_{2}<0\), the particles \(w\) which are larger than the root \(r\) of the velocity will move towards \(0\), while the particles \(w\) which are smaller than the root \(r\) of the velocity will move away from \(0\). This intuition is reflected in Figure 3, where not as many particles are concentrated around \(0\) compared to at the end of Phase 2, but the particles have not grown larger than \(1\) or \(-1\) in absolute value.

Figure 3: Distribution of coordinates of \(w\) after time passes in Phase 3

Figure 2: Distribution of coordinates of \(w\) after Phases 1 and 2

Figure 1: Distribution of coordinates of \(w\) at initialization

## Appendix C Background on Spherical Harmonics

In the following, we summarize the background needed for this paper based on Atkinson and Han [12, Section 2].

**Spherical Harmonics.** Spherical harmonics are the eigenfunctions of any inner product kernels on the unit sphere \(\mathbb{S}^{d-1}\). The eigenfunctions corresponding to the \(k\)-th eigenvalue are degree-\(k\) polynomials, and form a Hilbert space \(\mathbb{Y}_{k}^{d}\) with inner product \(\langle f,g\rangle=\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[f(x)g(x)]\). The dimension of \(\mathbb{Y}_{k}^{d}\) is \(N_{k,d}=\binom{d+k-1}{d-1}-\binom{d+k-3}{d-1}\). For any universal constant \(k\), \(N_{k,d}\asymp d^{k}\). Spherical harmonics with different degrees are orthogonal to each other, and their linear combinations can represent all square-integrable functions over \(\mathbb{S}^{d-1}\).

The following theorem states that \(\mathbb{Y}_{k}^{d}\) is the eigenspace corresponding to the \(k\)-th eigenvalue for any inner product kernel on the sphere.

**Theorem C.1** (Funk-Hecke formula [12]).: _Let \(h:[-1,1]\to\mathbb{R}\) be any one-dimensional function with \(\int_{-1}^{1}|h(t)|\mu_{d}(t)\mathrm{d}t<\infty\), and \(\lambda_{k}=N_{k,d}^{-1/2}\,\mathbb{E}_{t\sim\mu_{d}}[h(t)\overline{P}_{k,d}( t)]\). Then for any function \(Y_{k}\in\mathbb{Y}_{k,d}\),_

\[\forall x\in\mathbb{S}^{d-1},\quad\mathop{\mathbb{E}}_{z\sim\mathbb{S}^{d-1}} [h(\langle x,z\rangle)Y_{k}(z)]=\lambda_{k}Y_{k}(x).\] (C.1)

**Legendre Polynomials.** The un-normalized Legendre polynomial is defined recursively by

\[P_{0,d}(t) =1,\quad P_{1,d}(t)=t,\] (C.2) \[P_{k,d}(t) =\frac{2k+d-4}{k+d-3}tP_{k-1,d}(t)-\frac{k-1}{k+d-3}P_{k-2,d}(t), \quad\forall k\geq 2.\] (C.3)

In particular, we have

\[P_{2,d}(t)=\frac{d}{d-1}t^{2}-\frac{1}{d-1},\quad P_{4,d}(t)=\frac{(d+2)(d+4) }{d^{2}-1}t^{4}-\frac{6d+12}{d^{2}-1}t^{2}+\frac{3}{d^{2}-1}.\] (C.4)

For any \(u\in\mathbb{S}^{d-1}\), \(P_{k,d}(\langle u,x\rangle)\) is a spherical harmonic of degree \(k\) and, for every \(k,k^{\prime}\geq 0\) and \(u_{1},u_{2}\in\mathbb{S}^{d-1}\), \(\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[\overline{P}_{k,d}(\langle u_{1},x\rangle )\overline{P}_{k^{\prime},d}(\langle u_{2},x\rangle)]=\mathbf{1}\left[k=k^{ \prime}\right]P_{k,d}(\langle u_{1},u_{2}\rangle)\) (Lemma C.5).

**Projection Operator.** Let \(\Pi_{k}\) be the projection operator to \(\mathbb{Y}_{k}^{d}\). For any function \(f:\mathbb{S}^{d-1}\to\mathbb{R}\), the projection operator \(\Pi_{k}\) is given by

\[(\Pi_{k}f)(x)=\sqrt{N_{k,d}}\mathop{\mathbb{E}}_{\xi\sim\mathbb{S}^{d-1}}[ \overline{P}_{k,d}(\langle x,\xi\rangle)f(\xi)].\] (C.5)

In addition, if \(f\) is a function of the form \(f(x)=h(\langle u,x\rangle)\) for some fixed vector \(u\in\mathbb{S}^{d-1}\) and one-dimensional function \(h:[-1,1]\to\mathbb{R}\), the Funk-Hecke formula (Theorem C.1) implies

\[(\Pi_{k}f)(x)=\hat{h}_{k,d}\overline{P}_{k,d}(\langle u,x\rangle),\quad\forall x \in\mathbb{S}^{d-1},\] (C.6)

Figure 4: Evolution of \(D_{2}\) and \(D_{4}\)where \(\hat{h}_{k,d}=\mathbb{E}_{t\sim\mu_{d}}[h(t)\overline{P}_{k,d}(t)]\) is the \(k\)-th coefficient in the Legendre decomposition of \(h\).

Note that by the orthogonality of Legendre polynomials, we have

\[\forall x\in\mathbb{S}^{d-1},\quad\mathop{\mathbb{E}}_{u\sim\mathbb{S}^{d-1}}[ \overline{P}_{k,d}(\langle u,x\rangle)]=\mathop{\mathbb{E}}_{t\sim\mu_{d}}[ \overline{P}_{k,d}(t)]=\mathop{\mathbb{E}}_{t\sim\mu_{d}}[\overline{P}_{k,d}( t)\overline{P}_{0,d}(t)]=\mathbf{1}\left[k=0\right].\] (C.7)

The following lemma computes the expectation of \(\overline{P}_{k,d}(\langle u,x\rangle)\) with respect to a distribution on \(u\) which is rotationally invariant on the last \((d-1)\) coordinates.

**Lemma C.2**.: _Let \(\rho\) be a distribution on \(\mathbb{S}^{d-1}\) that is rotationally invariant as in Definition 4.1. Then, for any \(x\in\mathbb{S}^{d-1}\) we have \(\mathbb{E}_{u\sim\rho}[P_{k,d}(\langle u,x\rangle)]=\mathbb{E}_{u\sim\rho}[P_ {k,d}(\langle e_{1},u\rangle)]P_{k,d}(\langle e_{1},x\rangle)\)._

Proof.: We prove this lemma by invoking Eq. (2.167) of Atkinson and Han [12], which states that for every \(s,t\in[-1,1]\) and \(k\geq 0,d\geq 3\),

\[\int_{-1}^{1}P_{k,d}(st+(1-s^{2})^{1/2}(1-t^{2})^{1/2}\xi)\mu_{d-1}(\xi) \mathrm{d}\xi=P_{k,d}(s)P_{k,d}(t).\] (C.8)

In the following, for \(x\in\mathbb{S}^{d-1}\), we write \(x=(\langle e_{1},x\rangle,\zeta)\), and for \(u\sim\rho\) we write \(u=(w,z)\). Note that when \(\rho\) is symmetric and \(u\sim\rho\), conditioned on \(u_{1}\) we have \(\langle z,\zeta\rangle\sim(1-u_{1}^{2})^{1/2}(1-x_{1}^{2})^{1/2}\mu_{d-1}\) for any fixed \(x\). As a result, plugging in \(t=w,s=\langle e_{1},x\rangle\) and \(\xi=(1-u_{1}^{2})^{-1/2}(1-x_{1}^{2})^{-1/2}\left\langle z,\zeta\right\rangle\) we get

\[\mathop{\mathbb{E}}_{u\sim\rho}[P_{k,d}(\langle u,x\rangle)\mid u_{1}]=P_{k,d }(w)P_{k,d}(\langle e_{1},x\rangle).\] (C.9)

Finally, taking expectation over \(w\) we prove the desired result. 

Additional Useful Lemmas.In the following, we present some useful lemmas about spherical harmonics and Legendre polynomials.

The following proposition computes the coefficient in the Legendre polynomial decomposition of ReLU activation.

**Proposition C.3** (Lemma C.2 of Dong and Ma [27]).: _Let \(\sigma(t)=\max\{t,0\}\) be the ReLU activation. For every \(d\geq 3\) and even \(k\), we have_

\[\Big{|}\mathop{\mathbb{E}}_{t\sim\mu_{d}}[\sigma(t)\overline{P}_{k,d}(t)] \Big{|}\asymp d^{1/4}k^{-5/4}(d+k)^{-3/4}.\] (C.10)

_As a corollary, when \(k\) is an absolute constant we have_

\[\Big{|}\mathop{\mathbb{E}}_{t\sim\mu_{d}}[\sigma(t)\overline{P}_{k,d}(t)] \Big{|}\asymp d^{-1/2}.\] (C.11)

**Theorem C.4** (Addition Theorem [12]).: _Let \(\{Y_{k,j}:1\leq j\leq N_{k,d}\}\) be an orthonormal basis of \(\mathbb{Y}_{k,d}\), that is,_

\[\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[Y_{k,j}(x)Y_{k,j^{\prime}}(x)]= \mathbf{1}\left[j=j^{\prime}\right].\] (C.12)

_Then for all \(x,z\in\mathbb{S}^{d-1}\)_

\[\sum_{j=1}^{N_{k,d}}Y_{k,j}(x)Y_{k,j}(z)=N_{k,d}P_{k,d}(\langle x,z\rangle).\] (C.13)

**Lemma C.5**.: _For every \(k,k^{\prime}\geq 0,d\geq 3\) and \(u,v\in\mathbb{S}^{d-1}\), we have_

\[\mathop{\mathbb{E}}_{\xi\sim\mathbb{S}^{d-1}}[\overline{P}_{k,d}(\langle u, \xi\rangle)\overline{P}_{k^{\prime},d}(\langle v,\xi\rangle)]=\mathbf{1} \left[k=k^{\prime}\right]P_{k,d}(\langle u,v\rangle).\] (C.14)

Proof.: Let \(\{Y_{k,j}:1\leq j\leq N_{k,d}\}\) be an orthonormal basis of \(\mathbb{Y}_{k,d}\) with

\[\mathop{\mathbb{E}}_{\xi\sim\mathbb{S}^{d-1}}[Y_{k,j}(\xi)Y_{k,j^{\prime}}( \xi)]=\mathbf{1}\left[j=j^{\prime}\right].\] (C.15)Similarly define \(\{Y_{k^{\prime},j}:1\leq j\leq N_{k^{\prime},d}\}\) for \(\mathbb{Y}_{k^{\prime},d}\). The addition theorem (Theorem C.4) implies that

\[P_{k,d}(\langle u,\xi\rangle) =\frac{1}{N_{k,d}}\sum_{j=1}^{N_{k,d}}Y_{k,j}(u)Y_{k,j}(\xi).\] (C.16) \[P_{k^{\prime},d}(\langle u,\xi\rangle) =\frac{1}{N_{k^{\prime},d}}\sum_{j=1}^{N_{k^{\prime},d}}Y_{k^{ \prime},j}(u)Y_{k^{\prime},j}(\xi).\] (C.17)

Recall that for \(k\neq k^{\prime}\), \(\mathbb{Y}_{k,d}\) and \(\mathbb{Y}_{k^{\prime},d}\) are orthononal subspaces. Consequently,

\[\operatorname*{\mathbb{E}}_{\xi\sim\mathbb{S}^{d-1}}[\overline{P} _{k,d}(\langle u,\xi\rangle)\overline{P}_{k^{\prime},d}(\langle v,\xi\rangle)] =\frac{1}{\sqrt{N_{k,d}N_{k^{\prime},d}}}\sum_{i=1}^{N_{k,d}} \sum_{j=1}^{N_{k^{\prime},d}}\operatorname*{\mathbb{E}}_{\xi\sim\mathbb{S}^{ d-1}}[Y_{k,i}(u)Y_{k,i}(\xi)Y_{k^{\prime},j}(v)Y_{k^{\prime},j}(\xi)]\] (C.18) \[=\frac{\mathbf{1}\left[k=k^{\prime}\right]}{N_{k,d}}\sum_{j=1}^{ N_{k,d}}\operatorname*{\mathbb{E}}_{\xi\sim\mathbb{S}^{d-1}}[Y_{k,j}(u)Y_{k,j}(v)]\] (C.19) \[=\mathbf{1}\left[k=k^{\prime}\right]P_{k,d}(\langle u,v\rangle).\] (C.20)

**Lemma C.6**.: _For any \(k\geq 0,d\geq 3\), there exists a feature mapping \(\phi:\mathbb{S}^{d-1}\to\mathbb{R}^{N_{k,d}}\) such that for every \(u,v\in\mathbb{S}^{d-1}\),_

\[\langle\phi(u),\phi(v)\rangle =N_{k,d}P_{k,d}(\langle u,v\rangle),\] (C.21) \[\|\phi(u)\|_{2}^{2} =N_{k,d},\] (C.22) \[\operatorname*{\mathbb{E}}_{u\sim\mathbb{S}^{d-1}}[\phi(u)\phi(u) ^{\top}] =I.\] (C.23)

Proof.: Let \(Y_{1},\cdots,Y_{N_{k,d}}:\mathbb{S}^{d-1}\to\mathbb{R}\) be an orthonormal basis for the degree-\(k\) spherical harmonics \(\mathbb{Y}_{k,d}\). We let \(\phi(x)=(Y_{l}(x))_{l\in[N_{k,d}]}\) be the feature mapping, and in the following we verify Eqs. (C.21)-(C.23). By the addition theorem (Theorem C.4), for every \(u,v\in\mathbb{S}^{d-1}\) we have

\[\langle\phi(u),\phi(v)\rangle=\sum_{j=1}^{N_{k,d}}Y_{j}(u)Y_{j}(v)= N_{k,d}P_{k,d}(\langle u,v\rangle),\] (C.24)

which proves Eq. (C.21). Note that for every \(u\in\mathbb{S}^{d-1}\), \(P_{k,d}(\langle u,u\rangle)=P_{k,d}(1)=1\). Hence Eq. (C.22) follows directly. To prove Eq. (C.23), note that \(Y_{1},\cdots,Y_{N_{k,d}}\) are orthonormal. Therefore for every \(i,j\in[N_{k,d}]\)

\[\operatorname*{\mathbb{E}}_{u\sim\mathbb{S}^{d-1}}[Y_{i}(u)Y_{j} (u)]=\mathbf{1}\left[i=j\right].\] (C.25)

Hence, Eq. (C.23) follows directly. 

**Lemma C.7**.: _Let \(\mu_{d}\) be the distribution of \(\langle u,v\rangle\) when \(v\) is drawn uniformly at random from the unit sphere \(\mathbb{S}^{d-1}\) and \(u\in\mathbb{S}^{d-1}\) is a fixed vector. For fixed \(n\geq 1\), let \(x_{1},\cdots,x_{n}\in\mathbb{S}^{d-1}\) be (not necessarily independent) random variables drawn from the distribution \(\mu_{d}\). Then there exists a universal constant \(c>0\) such that for any integer \(k\geq 1\),_

\[\forall t\geq 4^{1+k}(k\ln d)^{k},\quad\Pr\left(\max_{i\leq n}d^{k}P_{k,d}(x_{i} )^{2}\geq t\right)\leq 2n\exp\left(-\frac{t^{1/k}}{32}\right).\] (C.26)

_In addition, we also have_

\[\operatorname*{\mathbb{E}}\left[\max_{i\leq n}d^{k}P_{k,d}(x_{i} )^{2}\right]\lesssim(32k\ln(dn))^{k}.\] (C.27)Proof.: To prove the first part of this lemma, we first invoke Proposition C.8, which states that

\[\forall x\in\left[\sqrt{\frac{k\ln d}{d}},1\right],\quad|P_{k,d}(x)|\leq 2^{1+k}x^{k}.\] (C.28)

When \(x\sim\mu_{d}\), Proposition I.5 states that

\[\forall t>0,\quad\Pr(|x|\geq t)\leq 2\exp\left(-\frac{t^{2}d}{2}\right).\] (C.29)

Therefore for every \(t\geq 4^{1+k}(k\ln d)^{k}\) we have

\[\Pr\left(P_{k,d}(x)^{2}d^{k}\geq t\right) \leq\Pr\left(|x|\geq t^{1/2k}d^{-1/2}2^{-1-1/k}\right)\] (C.30) \[\leq\ 2\exp\left(-\frac{1}{2}t^{1/k}4^{-1-1/k}\right)\] (C.31) \[\leq 2\exp(-t^{1/k}/32).\] (C.32)

By union bound we get

\[\forall t\geq 4^{1+k}(k\ln d)^{k},\quad\Pr\left(\max_{i\leq n}d^{k}P_{k,d}(x _{i})^{2}\geq t\right)\leq 2n\exp\left(-\frac{t^{1/k}}{32}\right).\] (C.33)

which proves the first part of this lemma.

As a corollary, for any fixed \(\epsilon>4^{1+k}(k\ln d)^{k}\) we have

\[\mathbb{E}\left[\max_{i\leq n}d^{k}P_{k,d}(x_{i})^{2}\right] \leq\epsilon+\int_{\epsilon}^{\infty}\Pr\left(\max_{i\leq n}d^{k} P_{k,d}(x_{i})^{2}\geq t\right)\mathrm{d}t\] (C.34) \[\leq\epsilon+n\int_{\epsilon}^{\infty}\Pr\left(d^{k}P_{k,d}(t)^{ 2}\geq t\right)\mathrm{d}t\] (C.35) \[\leq\epsilon+2n\int_{\epsilon}^{\infty}\exp\left(-\frac{t^{1/k}}{ 32}\right)\mathrm{d}t.\] (C.36)

Now we upper bound the integral by changing variables. Letting \(u=t^{1/k}/32\), we get

\[\int_{\epsilon}^{\infty}\exp\left(-\frac{t^{1/k}}{32}\right) \mathrm{d}t \leq\int_{\frac{1/k}{32}}^{\infty}32^{k}ku^{k-1}\exp\left(-u \right)\mathrm{d}u\] (C.37) \[\leq k^{2}\exp\left(-\frac{\epsilon^{1/k}}{32}\right)32^{k}\left( \frac{\epsilon^{1/k}}{32}\right)^{k-1}\] (C.38) \[\leq 32k^{2}\exp\left(-\frac{\epsilon^{1/k}}{32}\right)\epsilon\] (C.39)

where the last inequality comes from an upper bound for the incomplete gamma functions [31, Theorem 4.4.3]. Therefore we get

\[\mathbb{E}\left[\max_{i\leq n}d^{k}P_{k,d}(x_{i})^{2}\right]\leq\epsilon+64n \epsilon k^{2}\exp\left(-\frac{\epsilon^{1/k}}{32}\right).\] (C.41)

Finally taking \(\epsilon=(32k\ln(dn))^{k}\), we prove the desired result. 

**Proposition C.8**.: _For any \(k\geq 0,d\geq 2\), we have_

\[\forall t\in\left[\sqrt{\frac{k\ln d}{d}},1\right],\quad|P_{k,d}(t)|\leq 2^{1 +k}t^{k}.\] (C.42)Proof.: Let \(\mu_{d}(t)\triangleq(1-t^{2})^{\frac{d-3}{2}}\frac{\Gamma(d/2)}{\Gamma((d-1)/2)} \frac{1}{\sqrt{\pi}}\) be the density of \(u_{1}\) when \(u=(u_{1},\cdots,u_{d})\) is drawn uniformly from \(\mathbb{S}^{d-1}\). By Atkinson and Han [12, Theorem 2.24] we have

\[P_{k,d}(t)=\operatorname*{\mathbb{E}}_{s\sim\mu_{d-1}}\left[(t+i(1-t^{2})^{1/2} s)^{k}\right].\] (C.43)

As a result,

\[|P_{k,d}(t)|\leq\operatorname*{\mathbb{E}}_{s\sim\mu_{d-1}}\left[[t+i(1-t^{2})^ {1/2}s]^{k}\right]\leq\operatorname*{\mathbb{E}}_{s\sim\mu_{d-1}}\left[(t^{2} +(1-t^{2})s^{2})^{k/2}\right].\] (C.44)

By Proposition I.5 we have,

\[\Pr(\sqrt{d-1}|s|\leq 2\sqrt{k\ln d})\leq 2\exp(-2k\ln d)\leq d^{-k}.\] (C.45)

Consequently,

\[\operatorname*{\mathbb{E}}_{s\sim\mu_{d-1}}\left[(t^{2}+(1-t^{2}) s^{2})^{k/2}\right] \leq d^{-k}+\operatorname*{\mathbb{E}}_{s\sim\mu_{d-1}}\left[ \mathbf{1}\left[s\leq 2\sqrt{k\ln d}/\sqrt{d-1}\right](t^{2}+s^{2})^{k/2}\right]\] (C.46) \[\leq d^{-k}+\left(t^{2}+\frac{2k\ln d}{d-1}\right)^{k/2}.\] (C.47)

Hence, when \(t\geq\sqrt{\frac{k\ln d}{d}}\) we get

\[|P_{k,d}(t)|\leq d^{-k}+\left(t^{2}+\frac{2k\ln d}{d-1}\right)^{k/2}\leq 2^{1+k} t^{k}.\] (C.48)

## Appendix D Proofs for Population Dynamics

### Proof of Lemma 4.2

In the following, we prove Lemma 4.2, which provides a way to simplify \(L(\rho)\) when \(\rho\) is rotationally invariant and symmetric.

Proof of Lemma 4.2.: Recall that the population loss is

\[L(\rho)=\frac{1}{2}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho }(x)-h(\langle x,e_{1}\rangle))^{2}],\] (D.1)

and the activation function \(\sigma:\mathbb{R}\to\mathbb{R}\) and target function \(h:\mathbb{R}\to\mathbb{R}\) has the following decomposition:

\[\sigma(t) =\sum_{k=0}^{\infty}\hat{\sigma}_{k,d}\overline{P}_{k,d}(t),\] (D.2) \[h(t) =\sum_{k=0}^{\infty}\hat{h}_{k,d}\overline{P}_{k,d}(t).\] (D.3)

Therefore we have

\[2L(\rho) =\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x) -h(\langle x,e_{1}\rangle))^{2}]\] (D.4) \[=\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(} \operatorname*{\mathbb{E}}_{u\sim\rho}[\sigma(\langle u,x\rangle)]-h(\langle x,e_{1}\rangle)\Big{)}^{2}\] (D.5) \[=\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(} \operatorname*{\mathbb{E}}_{u\sim\rho}\Big{[}\sum_{k=0}^{\infty}\hat{\sigma}_ {k,d}\overline{P_{k,d}}(\langle u,x\rangle)\Big{]}-\sum_{k=0}^{\infty}\hat{h} _{k,d}\overline{P}_{k,d}(\langle x,e_{1}\rangle)\Big{)}^{2}\] (D.6) \[=\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(}\sum_{ k=0}^{\infty}\Big{(}\hat{\sigma}_{k,d}\operatorname*{\mathbb{E}}_{u\sim\rho}[ \overline{P}_{k,d}(\langle u,x\rangle)]-\hat{h}_{k,d}\overline{P}_{k,d}( \langle x,e_{1}\rangle)\Big{)}\Big{)}^{2}.\] (D.7)When the neural network \(\rho\) is rotationally invariant, we can simplify the above equation by invoking Lemma C.2, which states that

\[\mathop{\mathbb{E}}_{u\sim\rho}[\overline{P}_{k,d}(\langle u,x\rangle)]=\mathop{ \mathbb{E}}_{u\sim\rho}[P_{k,d}(\langle e_{1},u\rangle)]\overline{P}_{k,d}( \langle e_{1},x\rangle).\] (D.8)

Continuing Eq. (D.4) above we have,

\[2L(\rho) =\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(}\sum_{k=0}^{ \infty}\hat{\sigma}_{k,d}\mathop{\mathbb{E}}_{u\sim\rho}[P_{k,d}(\langle e_{1},u\rangle)]\overline{P}_{k,d}(\langle e_{1},x\rangle)-\hat{h}_{k,d}\overline{P} _{k,d}(\langle e_{1},x\rangle)\Big{)}^{2}\] (D.9) \[=\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(}\sum_{k=0}^{ \infty}(\hat{\sigma}_{k,d}\mathop{\mathbb{E}}_{u\sim\rho}[P_{k,d}(\langle e_{1 },u\rangle)]-\hat{h}_{k,d})\overline{P}_{k,d}(\langle e_{1},x\rangle)\Big{)}^{2}\] (D.10)

Now we expand the square by invoking Lemma C.5. In particular, Lemma C.5 states that

\[\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[\overline{P}_{k,d}(\langle e_{1},x\rangle)\overline{P}_{k^{\prime},d}(\langle e_{1},x\rangle)]=\mathbf{1} \left[k=k^{\prime}\right]P_{k,d}(\langle e_{1},e_{1}\rangle)=\mathbf{1}\left[k =k^{\prime}\right].\] (D.11)

Consequently,

\[2L(\rho)=\sum_{k=0}^{\infty}\Big{(}\hat{\sigma}_{k,d}\mathop{\mathbb{E}}_{u \sim\rho}[P_{k,d}(\langle e_{1},u\rangle)]-\hat{h}_{k,d}\Big{)}^{2},\] (D.12)

as desired.

To prove the second part of this lemma, in the following, we assume that (1) \(\hat{h}_{k,d}=0,\forall k\not\in\{0,2,4\}\), (2) \(\hat{h}_{0,d}=\hat{\sigma}_{0,d}\), and (3) that \(\sigma\) is a degree-4 polynomial. Then we get

\[L(\rho)=\frac{\hat{\sigma}_{2,d}^{2}}{2}\Big{(}\mathop{\mathbb{E}}_{u\sim\rho }[P_{2,d}(w)]-\gamma_{2}\Big{)}^{2}+\frac{\hat{\sigma}_{4,d}^{2}}{2}\Big{(} \mathop{\mathbb{E}}_{u\sim\rho}[P_{4,d}(w)]-\gamma_{4}\Big{)}^{2}+\sum_{k\in \{1,3\}}\frac{\hat{\sigma}_{k,d}^{2}}{2}\left(\mathop{\mathbb{E}}_{u\sim\rho }[P_{k,d}(w)]\right)^{2}.\] (D.13)

Since \(\rho\) is symmetric and \(P_{k,d}(w)\) is an odd function when \(k\) is odd, we get

\[\forall k\in\{1,3\},\quad\mathop{\mathbb{E}}_{u\sim\rho}[P_{k,d}(w)]=0.\] (D.14)

It follows directly that

\[L(\rho)=\frac{\hat{\sigma}_{2,d}^{2}}{2}\Big{(}\mathop{\mathbb{E}}_{u\sim\rho }[P_{2,d}(w)]-\gamma_{2}\Big{)}^{2}+\frac{\hat{\sigma}_{4,d}^{2}}{2}\Big{(} \mathop{\mathbb{E}}_{u\sim\rho}[P_{4,d}(w)]-\gamma_{4}\Big{)}^{2}\] (D.15)

### Proof of Lemma 4.3 and Lemma 4.4

The goal of this subsection is to show Lemma 4.3, which states that \(\rho_{t}\) remains symmetric and rotationally invariant under the infinite-width population dynamics, and to show Lemma 4.4 which describes the dynamics of the first coordinates of the particles in \(\rho_{t}\).

The following lemma is a first step. Recall that when \(\rho\) is rotationally invariant, then Lemma 4.2 allows us to rewrite \(L(\rho)=\frac{1}{2}\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(}f_{\rho}( x)-y\Big{)}^{2}\) as \(\frac{1}{2}\sum_{k=0}^{\infty}\Big{(}\hat{\sigma}_{k,d}\mathop{\mathbb{E}}_{u \sim\rho}[P_{k,d}(w)]-\hat{h}_{k,d}\Big{)}^{2}\). The following lemma states that the gradient from the two formulas for \(L(\rho)\) are the same when \(\rho\) is rotationally invariant.

**Lemma D.1**.: _Let \(\rho\) be a distribution on \(\mathbb{S}^{d-1}\) and let \(\nu\) be a distribution on \([-1,1]\). Define_

\[F(\nu)=\frac{1}{2}\sum_{k=0}^{\infty}\Big{(}\hat{\sigma}_{k,d}\mathop{\mathbb{E}} _{w\sim\nu}[P_{k,d}(w)]-\hat{h}_{k,d}\Big{)}^{2}\,.\] (D.16)

_If \(\rho\) is rotationally invariant as in Definition 4.1 and \(\nu\) is the marginal distribution of \(w\) for \(u=(w,z)\sim\rho\), then for any particle \(u=(w,z)\), we have_

\[(1-w^{2})\nabla_{w}F(\nu)=\text{grad}_{w}L(\rho)\] (D.17)

_where \(\text{grad}_{w}L(\rho)\) denotes the first coordinate of \(\text{grad}_{u}L(\rho)\)._Proof of Lemma D.1.: For any particle \(u=(w,z)\), we write the first coordinate of \(\nabla_{u}L(\rho)\) as \(\nabla_{w}L(\rho)\), and the vector consisting of the last \((d-1)\) coordinates as \(\nabla_{z}L(\rho)\). Observe that

\[\text{grad}_{w}L(\rho) =\left\langle e_{1},\text{grad}_{u}L(\rho)\right\rangle\] (D.18) \[=\left\langle e_{1},(I-uu^{\top})\nabla_{u}L(\rho)\right\rangle\] (D.19) \[=\nabla_{w}L(\rho)-w\left\langle u,\nabla_{u}L(\rho)\right\rangle\] (D.20) \[=(1-w^{2})\nabla_{w}L(\rho)-w\left\langle z,\nabla_{z}L(\rho)\right\rangle\] (D.21)

On the other hand, suppose \(\rho\) satisfies the rotational invariance property, and that \(\nu\) is the marginal distribution of the first coordinate under \(\rho\). Then, we can write \(\rho\) in terms of \(\nu\) -- for any \(w\in[-1,1]\), the distribution of \(z\) conditioned on \(w\) is \(\sqrt{1-w^{2}}\mathbb{S}^{d-1}\). Thus, since \(L(\rho)=F(\nu)\) by Lemma 4.2, we have by the chain rule that

\[\nabla_{w}F(\nu) =\operatorname*{\mathbb{E}}_{\begin{subarray}{c}\xi\sim\delta^{d -2}\\ u=wc_{1}+\sqrt{1-w^{2}}\xi\end{subarray}}\left[\left\langle\nabla_{u}L(\rho), \frac{\partial u}{\partial w}\right\rangle\right]\] (D.22) \[=\nabla_{w}L(\rho)+\operatorname*{\mathbb{E}}_{\xi\sim\mathbb{S} ^{d-2}}\left[\left\langle\nabla_{z}L(\rho)\right|_{z=\sqrt{1-w^{2}}\xi},\frac {\partial z}{\partial w}\right\rangle\right]\] (D.23) \[=\nabla_{w}L(\rho)+\operatorname*{\mathbb{E}}_{\xi\sim\mathbb{S} ^{d-2}}\left[\left\langle\nabla_{z}L(\rho)\right|_{z=\sqrt{1-w^{2}}\xi},-\frac {w}{1-w^{2}}\xi\right\rangle\right]\] (D.24) \[=\nabla_{w}L(\rho)+\operatorname*{\mathbb{E}}_{\xi\sim\mathbb{S} ^{d-2}}\left[\left\langle\nabla_{z}L(\rho)\right|_{z=\sqrt{1-w^{2}}\xi},-\frac {w}{1-w^{2}}z\right\rangle\right]\] (D.25) \[=\nabla_{w}L(\rho)-\frac{w}{1-w^{2}}\operatorname*{\mathbb{E}}_{ \xi\sim\mathbb{S}^{d-2}}[\left\langle\nabla_{z}L(\rho),z\right\rangle]\] (D.26)

Moreover, for any fixed \(w\in[-1,1]\), the quantity \(\left\langle\nabla_{z}L(\rho),z\right\rangle\) does not depend on \(z\), since by Lemma D.2, for any rotation matrix \(A\) and \(z^{\prime}=Az\), we have \(\left\langle\nabla_{z^{\prime}}L(\rho),z^{\prime}\right\rangle=\left\langle A \cdot\nabla_{z}L(\rho),Az\right\rangle=\left\langle\nabla_{z}L(\rho),z\right\rangle\). Thus, for any \(w\in[-1,1]\) and \(z\in\sqrt{1-w^{2}}\mathbb{S}^{d-2}\), we can write

\[(1-w^{2})\nabla_{w}F(\nu)=(1-w^{2})\nabla_{w}L(\rho)-w\left\langle z,\nabla_{ z}L(\rho)\right\rangle=\text{grad}_{w}L(\rho)\] (D.27)

as desired. 

In the proof of the above lemma, we used the following fact which we now prove: when \(\rho\) is rotationally invariant, \(\nabla_{u}L(\rho)\) is rotationally equivariant as a function of \(u\), i.e. if \(u\) is rotated by a rotation matrix \(A\) which only modifies the last \((d-1)\) coordinates, then \(\nabla_{u}L(\rho)\) is rotated by \(A\) as well.

**Lemma D.2** (Gradient is Rotationally Equivariant).: _Let \(\rho\) be a rotationally invariant distribution as in Definition 4.1. Let \(A\in\mathbb{R}^{d\times d}\) be a rotation matrix with \(Ae_{1}=e_{1}\), i.e. \(A\) only modifies the last \((d-1)\) coordinates. Let \(u\in\mathbb{S}^{d-1}\) and \(u^{\prime}=Au\). Then, \(\nabla_{u^{\prime}}L(\rho)=A\cdot\nabla_{u}L(\rho)\), and \(\text{grad}_{u^{\prime}}L(\rho)=A\cdot\text{grad}_{u}L(\rho)\)._

Proof.: The proof is by a straightforward calculation:

\[\nabla_{u^{\prime}}L(\rho) =\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)- y(x))\sigma^{\prime}(\langle u^{\prime},x\rangle)x]\] (D.28) \[=\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x) -h(\langle e_{1},x\rangle))\sigma^{\prime}(\langle u^{\prime},x\rangle)x]\] (D.29) \[=\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(Ax) -h(\langle e_{1},Ax\rangle))\sigma^{\prime}(\langle u^{\prime},Ax\rangle)Ax]\] (By rotational invariance of uniform distribution on \[\mathbb{S}^{d-1}\] ) \[=A\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(Ax) -h(\langle e_{1},Ax\rangle))\sigma^{\prime}(\langle Au,Ax\rangle)x]\] (By definition of \[u^{\prime}\] ) \[=A\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(Ax) -h(\langle e_{1},Ax\rangle))\sigma^{\prime}(\langle u,x\rangle)x]\] (B.c. \[A\] is a rotation matrix so \[A^{\top}A=I_{d\times d}\]By the rotational invariance of \(\rho\), we have \(f_{\rho}(Ax)=f_{\rho}(x)\), meaning

\[\nabla_{u^{\prime}}L(\rho) =A\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-h( \langle\epsilon_{1},Ax\rangle))\sigma^{\prime}(\langle u,x\rangle)x]\] (B.c. \[f_{\rho}(Ax)=f_{\rho}(x)\] by rotational invariance of \[\rho\] ) \[=A\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-h( \langle\epsilon_{1},x\rangle))\sigma^{\prime}(\langle u,x\rangle)x]\qquad\text{ (B.c. $Ae_{1}=e_{1}$ and $A^{\top}A=I_{d\times d}$)}\] \[=A\cdot\nabla_{u}L(\rho)\] (D.31)

This proves the first statement of the lemma. The second statement also follows from a similar calculation:

\[\text{grad}_{w}L(\rho) =(I-u^{\prime}(u^{\prime})^{\top})\cdot\nabla_{u^{\prime}}L(\rho)\] (D.32) \[=(I-Auu^{\top}A^{\top})\cdot A\cdot\nabla_{u}L(\rho)\] (D.33) \[=A\cdot\nabla_{u}L(\rho)-Auu^{\top}A^{\top}A\cdot\nabla_{u}L(\rho)\] (D.34) \[=A\cdot\nabla_{u}L(\rho)-Auu^{\top}\cdot\nabla_{u}L(\rho)\] (B.c. \[A^{\top}A=I_{d\times d}\] ) \[=A\cdot(I-uu^{\top})\cdot\nabla_{u}L(\rho)\] (D.35) \[=A\cdot\text{grad}_{u}L(\rho)\] (D.36)

as desired. 

As a corollary, we obtain our first formula for the dynamics of the 1-dimensional particles:

**Lemma D.3** (One-Dimensional Velocity).: _Suppose we are in the setting of Theorem 3.3 and that \(\rho_{t}\) is rotationally invariant. Then, for any particle \(u_{t}\) (where we omit the initialization \(\chi\) for convenience), if \(w_{t}\) is the first coordinate of \(u_{t}\), then_

\[\frac{dw_{t}}{dt}=-(1-w_{t}^{2})\sum_{k=0}^{4}\left(\hat{\sigma}_{k,d} \mathop{\mathbb{E}}_{u\sim\rho_{t}}[P_{k,d}(w)]-\hat{h}_{k,d}\right)\cdot\hat {\sigma}_{k,d}{P_{k,d}}^{\prime}(w)\,.\] (D.37)

Proof of Lemma d.3.: This follows directly from Lemma D.1, and because the activation \(\sigma\) and the target \(h\) only have nonzero coefficients \(\hat{\sigma}_{k,d}\) and \(\hat{h}_{k,d}\) for \(k=0,\dots,4\). 

We now complete the proof of Lemma 4.3, i.e. that \(\rho_{t}\) remains rotationally invariant and symmetric if it is defined according to the population projected gradient flow as in Eq. (3.6) and Eq. (3.7). To prove that \(\rho_{t}\) remains symmetric, we make use of the above formula for the 1-dimensional dynamics.

Proof of Lemma 4.3.: First, we show the rotational invariance property. Assume inductively that at a time \(t\), \(\rho_{t}\) satisfies the desired rotational invariance property -- it holds at initialization since \(\rho_{0}\) is the uniform distribution on \(\mathbb{S}^{d-1}\). Consider a particle \(u_{t}\) and let \(A\in\mathbb{R}^{d\times d}\) be a rotation matrix which only modifies the last \((d-1)\) coordinates. Let \(u^{\prime}_{t}=Au_{t}\) be another particle. Then, it follows from Lemma D.2 that

\[\frac{du^{\prime}_{t}}{dt}=A\frac{du_{t}}{dt}\] (D.38)

Thus, for a fixed \(w\in[-1,1]\), if \(z\sim\sqrt{1-w^{2}\mathbb{S}^{d-2}}\), then the distribution of \(\frac{dz}{dt}\) is uniform on \(c\mathbb{S}^{d-2}\) for a fixed scalar \(c\) that depends on \(w\). Therefore, \(\rho\) will remain rotationally invariant.

Next, we prove that \(\rho_{t}\) is symmetric. First observe that \(\rho_{t}\) is symmetric at initialization, since it is the uniform distribution on \(\mathbb{S}^{d-1}\). Now, suppose that \(\rho_{t}\) is symmetric at time \(t\). The polynomial \(P_{k,d}\) is odd if \(k\) is odd and even if \(k\) is even -- this can be seen from induction on Eq. (C.2) and Eq. (C.3). Thus, \(\mathbb{E}_{u\sim\rho_{t}}[P_{k,d}(w)]=0\) for odd \(k\) by our assumption that \(\rho_{t}\) is symmetric. Additionally, recall that we defined \(h\) so that \(\hat{\sigma}_{0,d}=\hat{h}_{0,d}\) and \(\hat{h}_{k,d}=0\) for odd \(k\). Thus, by our definition of \(D_{2,t}\) and \(D_{4,t}\) in Section 4,

\[\frac{dw_{t}}{dt}=-(1-w_{t}^{2})(\hat{\sigma}_{2,d}^{2}D_{2,t}{P_{2,d}}^{\prime }(w_{t})+\hat{\sigma}_{4,d}^{2}D_{4,t}{P_{4,d}}^{\prime}(w_{t}))\] (D.39)

by Lemma D.3. Since \(P_{2,d}\) and \(P_{4,d}\) are even polynomials, we know \({P_{2,d}}^{\prime}\) and \({P_{4,d}}^{\prime}\) are odd polynomials, and thus \(\frac{dw_{t}}{dt}\) is an odd polynomial in \(w_{t}\). In other words, if \(w_{t}\) and \(w^{\prime}_{t}\) are the first coordinates of two particles \(u_{t}\) and \(u_{t}^{\prime}\) respectively, such that \(w_{t}^{\prime}=-w_{t}\), then \(\frac{d{w_{t}^{\prime}}}{dt}=-\frac{d{w_{t}}}{dt}\). Thus, the distribution of \(\frac{d{w_{t}}}{dt}\) is symmetric, meaning that \(\rho_{t}\) will remain symmetric. 

In the rest of the proofs for the infinite-width population dynamics, we make use of the symmetry property. In particular, many of the lemmas are stated for \(w>0\) or \(\iota>0\) -- however, the generalizations to \(w<0\) or \(\iota<0\) also hold. We use this symmetry implicitly throughout the proofs in this section.

In Lemma 4.4, we are simply rewriting our formula for the 1-dimensional dynamics in a more convenient form. The following proof shows the detailed calculation.

Proof of Lemma 4.4.: By Lemma D.3 and the fact that only the second and fourth order terms are nonzero (which follows from Lemma 4.3, the fact that \(P_{k,d}\) is an odd polynomial for odd \(k\) (Eq. (C.2) and Eq. (C.3)) and because \(\hat{h}_{1,d}=\hat{h}_{3,d}=0\)), the velocity of \(w\) is equal to the following:

\[\text{grad}_{w}L(\rho)=-(1-w^{2})\cdot\left(\hat{\sigma}_{2,d}^{2}D_{2}(t)P_{2,d}{}^{\prime}(w)+\hat{\sigma}_{4,d}^{2}D_{4}(t)P_{4,d}{}^{\prime}(w)\right)\] (D.40)

where we use \(\text{grad}_{w}L(\rho)\) to denote the first coordinate of \(\text{grad}_{w}L(\rho)\). For convenience, during the rest of the proof of this lemma, let \(\eta_{d}{}^{(1)}=\frac{d}{d-1}\), \(\eta_{d}{}^{(2)}=\frac{d^{2}+6d+8}{d^{2}-1}\) and \(\eta_{d}{}^{(3)}=\frac{6d+12}{d^{2}-1}\). Then, by Eq. (C.4),

\[\text{grad}_{w}L(\rho) =-(1-w^{2})\cdot\left(\hat{\sigma}_{2,d}^{2}D_{2}(t)P_{2,d}^{ \prime}(w)+\hat{\sigma}_{4,d}^{2}D_{4}(t)P_{4,d}^{\prime}(w)\right)\] (D.41) \[=-(1-w^{2})\cdot\left(\hat{\sigma}_{2,d}^{2}D_{2}(t)\cdot 2\eta_{d}{}^{(1)}w+ \hat{\sigma}_{4,d}^{2}D_{4}(t)\cdot(4\eta_{d}{}^{(2)}w^{3}-2\eta_{d}{}^{(3)}w)\right)\] (D.42) \[=-(1-w^{2})\cdot\left(P_{t}(w)+2\hat{\sigma}_{2,d}^{2}D_{2}(t) \cdot(\eta_{d}{}^{(1)}-1)w\right.\] (D.43) \[\left.\hskip 28.452756pt+4\hat{\sigma}_{4,d}^{2}D_{4}(t)\cdot( \eta_{d}{}^{(2)}-1)w^{3}-2\hat{\sigma}_{4,d}^{2}D_{4}(t)\eta_{d}{}^{(3)}w\right)\] (D.44) \[=-(1-w^{2})\cdot\left(P_{t}(w)+Q_{t}(w)\right)\] (D.45)

Here we have chosen \(\lambda_{d}{}^{(1)}=2\hat{\sigma}_{2,d}^{2}(\eta_{d}{}^{(1)}-1)D_{2}(t)-2\hat {\sigma}_{4,d}^{2}D_{4}(t)\eta_{d}{}^{(3)}\) and \(\lambda_{d}{}^{(3)}=4\hat{\sigma}_{4,d}^{2}D_{4}(t)(\eta_{d}{}^{(2)}-1)\). Observe that \(|\lambda_{d}{}^{(1)}|,|\lambda_{d}{}^{(3)}|\leq O(\frac{\hat{\sigma}_{2,d}^{2} |D_{2,d}|+\hat{\sigma}_{2,d}^{2}|D_{4,t}|}{d})\), as desired. 

### Analysis of Phase 1

Recall the formal definition of Phase 1 in Definition 4.5. Intuitively, during Phase 1, the particles are all growing at a similar rate -- because the particles \(w\) are all less than \(w_{\text{max}}=\frac{1}{\log d}\) in magnitude, the cubic term involving \(w^{3}\) in the velocity of \(w\) does not have a significant effect on the growth of \(w\). We formalize this in the following lemma.

**Lemma D.4** (Uniform Growth Lemma).: _Suppose we are in the setting of Theorem 3.3. Consider a time interval \([t_{0},t_{1}]\in[0,T_{2}]\) in Phase 1 or 2. We are interested in quantifying the (relative) growth rate of the neurons indexed (or initialized) at \(\iota_{1}\) and \(\iota_{2}\) (where \(0<\iota_{1}<\iota_{2}\)), defined as_

\[F_{1} \triangleq\frac{w_{t_{1}}(\iota_{1})}{w_{t_{0}}(\iota_{1})}\] \[F_{2} \triangleq\frac{w_{t_{1}}(\iota_{2})}{w_{t_{0}}(\iota_{2})}\]

_Let \(\delta\in(0,1)\) such that \(\frac{1}{\sqrt{d}}\lesssim\delta\leq\frac{1}{\log F_{2}}\) -- here, we implicitly assume that \(F_{2}\leq e^{d}\). We also assume that \(\delta\leq\frac{\gamma_{2}^{2}}{16}\). Further assume that \(w_{t_{1}}(\iota_{2})=\delta\). Finally assume that \(\mathbb{P}(|\iota|\geq|\iota_{2}|)\leq\frac{\gamma_{2}^{2}}{16}\). Then,_

\[F_{1}\asymp F_{2}\] (D.46)

_and additionally,_

\[F_{1}\asymp\exp\Big{(}\int_{t_{0}}^{t_{1}}2\hat{\sigma}_{2,d}^{2}|D_{2,t}|dt \Big{)}\] (D.47)_Finally,_

\[t_{1}-t_{0}\lesssim\frac{1}{\hat{\sigma}_{2,d}^{2}\gamma_{2}}\log F_{2}\] (D.48)

_We note that \(F_{1}\) and \(F_{2}\) are both at least \(1\)._

Note that this lemma also holds for \(\iota_{2}<\iota_{1}<0\), and with \(w_{t_{1}}(\iota_{2})=-\delta\), by the symmetry of \(\rho_{t}\).

Proof of Lemma D.4.: In this proof, we define \(\tau=\sqrt{\gamma_{2}}\) -- then, by Assumption 3.2, we can write \(\gamma_{4}=\beta\cdot\tau^{4}\) where \(1.1\leq\beta\) and \(\beta\) is less than some universal constant \(c_{1}\). Suppose at time \(t_{1}\), \(w_{t_{1}}(\iota_{2})=\delta\). One useful observation, which we use in the rest of this proof, is that for all \(t\leq t_{1}\), \(w_{t}(\iota_{2})\leq\delta\), since for \(w\gtrsim\frac{1}{\sqrt{d}}\), we have \(\frac{dw}{dt}\geq 0\) by Lemma 4.4 (here we used the assumption that \(\delta\gtrsim\frac{1}{\sqrt{d}}\)). Thus,

\[\begin{split}\mathop{\mathbb{E}}_{w\sim\rho}[P_{2,d}(w)]& =\mathbb{P}(|\iota|>|\iota_{2}|)\cdot\mathop{\mathbb{E}}_{w\sim\rho}[P _{2,d}(w)\mid|\iota|>|\iota_{2}|]+\mathbb{P}(|\iota|<|\iota_{2}|)\cdot\mathop {\mathbb{E}}_{w\sim\rho}[P_{2,d}(w)\mid|\iota|<|\iota_{2}|]\\ &\leq\frac{\gamma_{2}^{2}}{16}+\mathop{\mathbb{E}}_{w\sim\rho}[P _{2,d}(w)\mid|\iota|<|\iota_{2}|]\\ &\text{(B.c. $|P_{2,d}(w)|\leq 1$ by Eq. (\ref{eq:11}) of Atkinson and Han [12] and $\mathbb{P}(|\iota|>|\iota_{2}|)\leq\delta$)}\\ &\leq\frac{\gamma_{2}^{2}}{16}+\delta^{2}+O\Big{(}\frac{1}{d} \Big{)}\\ &\text{(By Eq. (\ref{eq:11}) and b.c. $|w_{t}(\iota)|\leq\delta$ if $|\iota|\leq|\iota_{2}|$ by Proposition D.37)}\\ &\leq\frac{\tau^{4}}{8}+O\Big{(}\frac{1}{d}\Big{)}\text{ (B.c. $\delta\leq\frac{\gamma_{2}^{2}}{16}$ and $\gamma_{2}=\tau^{2}$)}\end{split}\] (D.49)

Thus, writing \(\gamma_{2}=\tau^{2}\), we have

\[|D_{2,t}|=|\mathop{\mathbb{E}}_{w\sim\rho}[P_{2,d}(w)]-\tau^{2}|\geq\Big{|} \frac{\tau^{4}}{8}+O\Big{(}\frac{1}{d}\Big{)}-\tau^{2}\Big{|}\geq\frac{\tau^{ 2}}{2}\] (D.50)

where the last inequality is by Assumption 3.2 b.c. \(d\geq c_{3}\) and \(\gamma_{4}\lesssim\gamma_{2}\). By a similar argument,

\[|D_{4,t}| =|\mathop{\mathbb{E}}_{w\sim\rho}[P_{4,d}(w)]-\beta\tau^{4}|\] (D.51) \[\leq\beta\tau^{4}+\frac{\gamma_{2}^{2}}{16}+\delta^{4}+O\Big{(} \frac{1}{d}\Big{)}\] \[\qquad\qquad\text{(Using $|P_{4,d}(w)|\leq 1$, $\mathbb{P}(|\iota|>|\iota_{2}|)\leq\frac{\gamma_{2}^{2}}{16}$, $\delta\leq\frac{\tau^{4}}{16}$ and Eq. (\ref{eq:11}))}\] \[\leq(\beta+1/8)\tau^{4}\] (B.c. \[\delta\leq\frac{\gamma_{2}^{2}}{16}\leq\frac{\beta\tau^{4}}{16}\] ) \[\leq|D_{2,t}|\] (D.52)

Thus, for any time \(t\in[t_{0},t_{1}]\), for any \(\iota\in[t_{1},\iota_{2}]\), by Lemma 4.4 (and because \(w_{t}(\iota)\leq\delta\) for any \(\iota\in[\iota_{1},\iota_{2}]\) by Proposition D.37), we have

\[v_{t}(w_{t}(\iota)) =-(1-w_{t}(\iota)^{2})\cdot\Big{(}2\hat{\sigma}_{2,d}^{2}D_{2,t}w_ {t}(\iota)\cdot(1\pm O(\delta))+Q_{t}(w_{t}(\iota))\Big{)}\] \[\qquad\qquad\text{(B.c. $\hat{\sigma}_{4,d}^{2}\lesssim\hat{\sigma}_{2,d}^{2}$ by Assumption 3.2, $|D_{4,t}|\leq|D_{2,t}|$, and $w_{t}(\iota)\leq\delta$)}\] \[=2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w_{t}(\iota)\cdot(1\pm O(\delta) )-O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d}\Big{)}|w_{t}(\iota)|\] \[\qquad\qquad\text{(B.c. $|w_{t}(\iota)|\leq\delta$ and by bound on coefficients of $Q_{t}$ from Lemma \ref{eq:11})}\] \[=2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w_{t}(\iota)\cdot(1\pm O(\delta))\] (B.c. \[\delta\geq\frac{1}{d}\] )

In summary,

\[v_{t}(w_{t}(\iota))=2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w_{t}(\iota)\cdot(1\pm O( \delta))\] (D.53)

Thus,

\[\frac{v(w_{t}(\iota_{2}))}{w_{t}(\iota_{2})}\leq\frac{1+O(\delta)}{1-O(\delta)} \frac{v(w_{t}(\iota_{1}))}{w_{t}(\iota_{1})}\leq(1+O(\delta))\cdot\frac{v(w_{t }(\iota_{1}))}{w_{t}(\iota_{1})}\] (D.54)and \(w_{t}(\iota_{2})\) grows by a factor

\[F_{2}=\exp\Big{(}\int_{t_{0}}^{t_{1}}\frac{v(w_{t}(\iota_{2}))}{w_{t}(\iota_{2})} dt\Big{)}\leq\exp\Big{(}(1+O(\delta))\int_{t_{0}}^{t_{1}}\frac{v(w_{t}(\iota_{1}))}{w _{t}(\iota_{1})}dt\Big{)}=F_{1}^{1+O(\delta)}\] (D.55)

Rearranging gives

\[F_{1}\geq F_{2}^{1-O(\delta)}=\frac{F_{2}}{F_{2}^{O(\delta)}}\] (D.56)

Finally, observe that \(F_{2}^{O(\delta)}\leq O(1)\), since by the assumption that \(\delta\leq\frac{1}{\log F_{2}}\),

\[F_{2}^{\delta}\leq F_{2}^{\frac{1}{\log F_{2}}}\] (D.57)

and for any \(x>0\), \(x^{\frac{1}{\log x}}=e\), meaning that \(F_{2}^{\delta}\leq O(1)\). This implies that \(F_{1}\gtrsim F_{2}\). From the above calculations, we can also obtain

\[\exp\Big{(}\int_{t_{0}}^{t_{1}}2\hat{\sigma}_{2,d}^{2}|D_{2,t}|dt \Big{)} \gtrsim\exp\Big{(}\int_{t_{0}}^{t_{1}}\frac{1}{1+O(\delta)}\cdot \frac{v_{t}(w_{t}(\iota_{2}))}{w_{t}(\iota_{2})}dt\Big{)}\] (By Eq. ( D.53)) \[\gtrsim\exp\Big{(}(1-O(\delta))\cdot\int_{t_{0}}^{t_{1}}\frac{v_{t }(w_{t}(\iota_{2}))}{w_{t}(\iota_{2})}dt\Big{)}\] (D.58) \[\gtrsim F_{2}^{1-O(\delta)}\] (D.59) \[\gtrsim F_{2}\] (By Eq. ( D.57))

as desired. Similarly, we can obtain

\[\exp\Big{(}\int_{t_{0}}^{t_{1}}2\hat{\sigma}_{2,d}^{2}|D_{2,t}|dt \Big{)} \lesssim\exp\Big{(}\int_{t_{0}}^{t_{1}}(1+O(\delta))\cdot\frac{v_ {t}(w_{t}(\iota_{2}))}{w_{t}(\iota_{2})}dt\Big{)}\] (D.60) \[\lesssim F_{2}^{1+O(\delta)}\] (D.61) \[\lesssim F_{2}\] (B.c. \[\delta\leq\frac{1}{\log F_{2}}\] )

Finally,

\[F_{1} \lesssim\exp\Big{(}\int_{t_{0}}^{t_{1}}(1+O(\delta))\cdot\frac{v_ {t}(w_{t}(\iota_{2}))}{w_{t}(\iota_{2})}dt\Big{)}\] (D.62) \[\lesssim F_{2}^{1+O(\delta)}\] (D.63) \[\lesssim F_{2}\] (B.c. \[\delta\leq\frac{1}{\log F_{2}}\] )

as desired. Finally, we show the running time bound. Since \(\delta\leq\frac{\gamma^{2}}{16}\) and \(\gamma_{2}\) is less than a sufficiently small universal constant by Assumption 3.2, for any time \(t\) during this interval, \(v(w_{t}(\iota_{2}))\gtrsim\hat{\sigma}_{2,d}^{2}D_{2}(t)w_{t}(\iota_{2}) \gtrsim\hat{\sigma}_{2,d}^{2}\tau^{2}w_{t}(\iota_{2})\) (where the second inequality is by Eq. ( D.50)). Thus, by Gronwall's inequality (Fact I.13), the time elapsed when \(w_{t}(\iota_{2})\) grows by a factor of \(F_{2}\) is at most \(\frac{1}{\hat{\sigma}_{2,d}^{2}\tau^{2}}\log F_{2}\). 

#### d.3.1 Summary of Phase 1

As a direct consequence of the above lemma, we can show Lemma D.5, which characterizes how fast the particles \(w_{t}\) grow during Phase 1.

**Lemma D.5** (Phase 1).: _Suppose we are in the setting of Theorem 3.3. At the end of phase 1, i.e. at time \(T_{1}\), for all \(\iota\in(0,\iota_{\text{U}})\), we have \(w_{T_{1}}(\iota)\asymp\frac{\sqrt{d}}{(\log d)^{2}}\iota\). Furthermore, \(T_{1}\lesssim\frac{1}{\hat{\sigma}_{2,d}^{2}\tau^{2}}\log d\). Additionally, we have \(\exp\Big{(}\int_{0}^{T_{1}}2\hat{\sigma}_{2,d}^{2}|D_{2,t}|dt\Big{)}\asymp \frac{\sqrt{d}}{(\log d)^{2}}\)._

Proof of Lemma D.5.: To show the lower bound on \(w_{T_{1}}(\iota)\), we apply Lemma D.4, with \(t_{0}=0\) and \(t_{1}=T_{1}\), with \(\delta=\frac{1}{\log d}\), and with \(\iota_{1}=\iota\) and \(\iota_{2}=\iota_{\text{U}}\). Let us verify that the assumptions of Lemma D.4 hold:* Observe that \(\frac{w_{T_{1}}(\iota_{\mathrm{U}})}{\iota_{\mathrm{U}}}=\frac{1/\log d}{\log d/ \sqrt{d}}=\frac{\sqrt{d}}{(\log d)^{2}}\), meaning that \(\log F_{2}\leq\log d\) and \(\frac{1}{\log F_{2}}\geq\delta\).
* By Assumption 3.2, \(\delta=\frac{1}{\log d}\leq\frac{\gamma_{2}^{2}}{16}\), since \(d\geq c_{3}\).
* The assumption that \(\mathbb{P}(|\iota|\geq\iota_{\mathrm{U}})\leq\frac{\gamma_{2}^{2}}{16}\) holds by a standard bound on the tail of \(\mu_{d}\).

Using the notation of Lemma D.4,

\[F_{2}=\frac{w_{T_{1}}(\iota_{\mathrm{U}})}{\iota_{\mathrm{U}}}=\frac{w_{\text{ max}}}{(\log d)/\sqrt{d}}=\frac{\sqrt{d}}{(\log d)^{2}}\] (D.64)

where \(w_{\text{max}}\) is as defined in Definition 4.5. By Eq. (D.46), we therefore know that

\[\frac{\sqrt{d}}{(\log d)^{2}}\gtrsim\frac{w_{T_{1}}(\iota)}{\iota}\gtrsim\frac {\sqrt{d}}{(\log d)^{2}}\] (D.65)

For the bound on the running time, observe that by Eq. (D.48), \(T_{1}\lesssim\frac{1}{\hat{\sigma}_{2,d^{\prime}\gamma^{2}}^{2}}\log\frac{ \sqrt{d}}{(\log d)^{2}}\). The final statement holds because \(F_{2}=\frac{\sqrt{d}}{(\log d)^{2}}\) and from the second statement of Lemma D.4. This completes the proof. 

### Analysis of Phase 2

Recall the formal definition of Phase 2 from Definition 4.6. Our analysis in this section has two main goals: (1) to show that the running time of Phase 2 is small, and (2) to show that a majority of the particles become at least \(\frac{1}{\log\log d}\) in magnitude, up to a constant factor. We will achieve goal (2) using Lemma D.4 -- then, we show that goal (1) holds using goal (2). Additionally, goal (2) will be useful for the subsequent phases as well.

First, the following proposition defines a range of particles which will be relevant during Phase 2 and Phase 3. This range of particles is a strict subset of the interval \((0,\iota_{\mathrm{U}})\) which we considered during Phase 1 -- we will show that this new range of particles grows uniformly during the beginning of Phase 2, until the largest of them becomes about \(\frac{1}{\log\log d}\) in magnitude. Specifically, we define \(\iota_{\mathrm{L}}\triangleq\frac{\kappa}{\sqrt{d}}\) and \(\iota_{\mathrm{R}}\triangleq\frac{1}{\kappa\sqrt{d}}\) where \(\kappa\asymp\frac{1}{\log\log d}\). The following proposition states that at initialization, a very large fraction of the \(\iota\)'s have magnitude between \(\iota_{\mathrm{L}}\) and \(\iota_{\mathrm{R}}\).

**Proposition D.6**.: _Let \(\kappa\in(0,1)\), and suppose \(\iota\) is drawn uniformly at random from \(\mu_{d}\). Define \(\iota_{\mathrm{L}}=\frac{\kappa}{\sqrt{d}}\) and \(\iota_{\mathrm{R}}=\frac{1}{\kappa\sqrt{d}}\). Then, \(\mathbb{P}(|\iota|\not\in[\iota_{\mathrm{L}},\iota_{\mathrm{R}}])\lesssim\kappa\)._

Proof of Proposition D.6.: First, we upper bound the probability that \(|\iota|\geq\iota_{\mathrm{R}}\). By Markov's inequality,

\[\mathbb{P}(|\iota|\geq\iota_{\mathrm{R}})\leq\frac{\mathbb{E}[\iota^{2}]}{ \iota_{\mathrm{R}}^{2}}=\frac{1/d}{1/(\kappa^{2}d)}=\kappa^{2}\] (D.66)

Now, we upper bound the probability that \(|\iota|\leq\iota_{\mathrm{L}}\). By Eqs. (1.16) and (1.18) of Atkinson and Han [12], the probability density function of \(\iota\) is \(p(\iota)=\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/2)}(1-\iota^{2})^{\frac{d- 2}{2}}\). Thus, we can simply upper bound the density by its maximum value:

\[\mathbb{P}(|\iota|\leq\iota_{\mathrm{L}}) \lesssim\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/2)}\int_{-\iota_ {\mathrm{L}}}^{\iota_{\mathrm{L}}}(1-t^{2})^{\frac{d-3}{2}}dt\] (D.67) \[\lesssim\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/2)}\cdot 2\iota_{ \mathrm{L}}\] (D.68) \[\lesssim\sqrt{d}\cdot\iota_{\mathrm{L}}\] (By Stirling's Formula) \[\lesssim\sqrt{d}\cdot\frac{\kappa}{\sqrt{d}}\] (D.69) \[\lesssim\kappa\] (D.70)

as desired.

Next, as a corollary of Lemma D.4, we show that for a portion of Phase 2, the particles initialized at \(\iota\in[t_{\iota},\iota_{\mathbf{R}}]\) grow by a similar factor. Specifically, this is true until the largest of these particles is \(\xi=\frac{1}{2\log\log d}\) in magnitude.

**Lemma D.7** (Corollary of Lemma D.4).: _Suppose we are in the setting of Theorem 3.3. Let \(T_{\mathbb{R}}>T_{1}\) be the first time such that \(w_{T_{\mathbb{R}}}(\iota_{\mathbf{R}})=\xi=\frac{1}{2\log\log d}\). Then, for all \(\iota\) with \(|\iota|\in[\iota_{\iota},\iota_{\mathbf{R}}]\), we have \(|w_{T_{\mathbb{R}}}(\iota)|\gtrsim\xi\kappa^{2}\), and additionally, \(\frac{w_{T_{\mathbb{R}}}(\iota)}{w_{T_{\mathbb{1}}}(\iota)}\asymp\xi\kappa( \log d)^{2}\). Furthermore, \(T_{\mathbb{R}}-T_{1}\lesssim\frac{1}{\delta_{2,d}^{2}\gamma_{2}}\log\log d\)._

Proof of Lemma D.7.: By Lemma D.5, \(w_{T_{1}}(\iota_{\mathbf{R}})\asymp\frac{\sqrt{d}}{(\log d)^{2}}\iota_{ \mathbf{R}}=\frac{1}{\kappa(\log d)^{2}}\). Let \(\iota_{\mathbf{L}}=\frac{\kappa}{\sqrt{d}}\) and \(\iota_{\mathbf{R}}=\frac{1}{\kappa\sqrt{d}}\), where \(\kappa=\frac{1}{\log\log d}\). Now, we apply Lemma D.4, with \(t_{0}=T_{1}\) and \(t_{1}=T_{\mathbb{R}}\), and with \(\iota_{1}=\iota\) for some \(\iota\in[\iota_{\iota},\iota_{\mathbf{R}}]\) and \(\iota_{2}=\iota_{\mathbf{R}}\), and with \(\delta=\frac{1}{2\log\log d}\). Note that we assume without loss of generality that \(\iota>0\) -- the statement for \(\iota<0\) follows by the symmetry of \(\rho_{t}\). Let us verify that the assumptions hold:

* First, \(\delta=\frac{1}{2\log\log d}\gtrsim\frac{1}{\sqrt{d}}\). Additionally, observe that by Lemma D.5, \(\frac{w_{T_{1}}(\iota_{\mathbf{R}})}{\iota_{\mathbf{R}}}\asymp\frac{\sqrt{d}} {(\log d)^{2}}\), meaning that \(w_{T_{1}}(\iota_{\mathbf{R}})\asymp\frac{1}{\kappa(\log d)^{2}}\). Thus, using the notation of Lemma D.4, we have \(F_{2}\lesssim\frac{1}{1/(\kappa(\log d)^{2})}=\frac{\kappa(\log d)^{2}}{\log \log d}\), and \(\log F_{2}\leq 2\log\log d-2\log\log\log d+C<\frac{1}{\delta}\), as desired. (Here \(C\) is a universal constant, and the last inequality holds by Assumption 3.2 since \(d\geq c_{3}\).)
* By Assumption 3.2, \(\delta=\frac{1}{2\log\log d}\lesssim\frac{\gamma_{2}^{2}}{16}\) since \(d\geq c_{3}\).
* Additionally, the probability that \(|\iota|\geq|\iota_{\mathbf{R}}|\) is at most \(\kappa=\frac{1}{\log\log d}\) up to constant factors by Proposition D.6, and this is clearly at most \(\frac{\gamma_{2}^{2}}{16}\) by Assumption 3.2.

Using Lemma D.4, we obtain

\[\frac{w_{T_{\mathbb{R}}}(\iota)}{w_{T_{1}}(\iota)}\asymp\frac{w_{T_{\mathbb{R }}}(\iota_{\mathbf{R}})}{w_{T_{1}}(\iota_{\mathbf{R}})}\asymp\frac{\xi}{w_{T_ {1}}(\iota_{\mathbf{R}})}\asymp\frac{\xi}{\left(\frac{1}{\kappa(\log d)^{2}} \right)}\asymp\xi\kappa(\log d)^{2}\] (D.71)

Thus, using the conclusion of Lemma D.5 that \(\frac{w_{T_{\mathbb{R}}}(\iota)}{\iota}\gtrsim\frac{\sqrt{d}}{(\log d)^{2}}\), we obtain

\[w_{T_{\mathbb{R}}}(\iota)\gtrsim\frac{w_{T_{\mathbb{R}}}(\iota)}{w_{T_{1}}( \iota)}\cdot\frac{w_{T_{1}}(\iota)}{\iota}\cdot\iota\gtrsim\xi\kappa(\log d)^{ 2}\cdot\frac{\sqrt{d}}{(\log d)^{2}}\cdot\frac{\kappa}{\sqrt{d}}\gtrsim\xi \kappa^{2}\] (D.72)

as desired. Thus, we obtain the first statement of this lemma. In addition, by Eq. (D.48), and since \(w_{T_{\mathbb{R}}}(\iota_{\mathbf{R}})=\xi\) and \(w_{T_{1}}(\iota_{\mathbf{R}})\gtrsim\frac{1}{\kappa(\log d)^{2}}\), the running time bound we obtain for this part of Phase 2 is \(T_{\mathbb{R}}-T_{1}\lesssim\frac{1}{\delta_{2,d}^{2}\gamma^{2}}\log\frac{\xi} {1/(\kappa(\log d)^{2})}\lesssim\frac{1}{\delta_{2,d}^{2}\gamma^{2}}(\log\log d)\), as desired. 

We now prove some auxiliary lemmas. First, we show an upper bound on \(L(\rho_{0})\), which is needed in order to later show an upper bound on the time required to have \(L(\rho_{t})\leq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\). In the process, we also show a reasonably tight bound on the magnitude of \(D_{2,t}\) and \(D_{4,t}\) during the first part of Phase 2.

**Lemma D.8**.: _Suppose we are in the setting of Theorem 3.3 and Lemma D.7, and suppose \(0\leq t\leq T_{\mathbb{R}}\). Then, \(-\frac{3\gamma_{2}}{2}\leq D_{2,t}\leq-\frac{\gamma_{2}}{2}\) and \(-\frac{3\gamma_{4}}{2}\leq D_{4,t}\leq-\frac{\gamma_{4}}{2}\). As a corollary, for all \(t\geq 0\), \(L(\rho_{t})\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\gamma_{2}^{2}\)._

Proof.: Since \(t\leq T_{\mathbb{R}}\), \(w_{t}(\iota_{\mathbf{R}})\leq\xi=\frac{1}{2\log\log d}\) (because for \(w\gtrsim\frac{1}{\sqrt{d}}\), we have \(\frac{dw}{dt}\geq 0\) by Lemma 4.4). By Proposition D.37, for all \(\iota\) with \(|\iota|\leq\iota_{\mathbf{R}}\), \(|w_{t}(\iota)|\leq\xi\). Thus, by Proposition D.6,

\[\begin{split}\operatorname*{\mathbb{E}}_{w\sim\rho_{t}}[w^{2}]& \leq\mathbb{P}_{\iota\sim\rho_{0}}(|\iota|\leq\iota_{\mathbf{R}})\cdot \operatorname*{\mathbb{E}}[w^{2}\ |\ |\iota|\leq\iota_{\mathbf{R}}]+\mathbb{P}_{\iota\sim\rho_{0}}(|\iota|\geq\iota_{ \mathbf{R}})\cdot\operatorname*{\mathbb{E}}[w^{2}\ |\ |\iota|\geq\iota_{\mathbf{R}}]\\ &\leq\xi^{2}+O(\kappa)\end{split}\] (By Proposition D.6)

Thus, \(D_{2,t}=O(\xi^{2}+\kappa)-\gamma_{2}\), meaning that \(-\frac{3\gamma_{2}}{2}\leq D_{2}\leq-\frac{\gamma_{2}}{2}\), and similarly, \(-\frac{3\gamma_{4}}{2}\leq D_{4}\leq-\frac{\gamma_{4}}{2}\). The corollary follows from the fact that \(\gamma_{4}\lesssim\gamma_{2}^{2}\) by Assumption 3.2.

Next, we show that the particles are in fact increasing in magnitude during Phases 1 and 2. This makes use of our bound on the magnitudes of \(D_{2,t}\) and \(D_{4,t}\) up to time \(T_{\text{R}}\).

**Lemma D.9**.: _Suppose we are in the setting of Theorem 3.3, and suppose \([t_{0},t_{1}]\) is a time interval with \(0\leq t_{0}<t_{1}\leq T_{2}\). Then, for all \(\iota\) with \(|\iota|\in[\iota_{\text{L}},\iota_{\text{R}}]\), \(|w_{\iota}(\iota)|\) is increasing on \([t_{0},t_{1}]\)._

Proof.: Recall that by Lemma 4.4,

\[v_{\iota}(w)=-(1-w^{2})(P_{t}(w)+Q_{t}(w))\] (D.74)

and for \(w\geq 0\), it suffices to show that \(P_{t}(w)+Q_{t}(w)\leq 0\). For \(t\leq T_{\text{R}}\),

\[P_{t}(w)+Q_{t}(w) \leq 2\hat{\sigma}_{2,d}^{2}D_{2,t}w+4\hat{\sigma}_{4,d}^{2}D_{4,t }w^{3}+O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D _{4,t}|}{d}\Big{)}|w|\] (D.75) \[\leq-\hat{\sigma}_{2,d}^{2}\gamma_{2}w+O\Big{(}\frac{\hat{\sigma} _{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}|w|\] (By Lemma D.8, b.c. \[D_{4,t}\leq 0\] )

For \(t\geq T_{\text{R}}\), suppose \(w\gtrsim\xi\kappa^{2}\) (which holds at time \(T_{\text{R}}\) for \(\iota\in[\iota_{\text{L}},\iota_{\text{R}}]\) by Lemma D.7). Then,

\[P_{t}(w)+Q_{t}(w) \leq 2\hat{\sigma}_{2,d}^{2}D_{2,t}w+4\hat{\sigma}_{4,d}^{2}D_{4,t }w^{3}+O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D _{4,t}|}{d}\Big{)}|w|\] (By Lemma 4.4) \[\leq\hat{\sigma}_{2,d}^{2}D_{2,t}\Big{(}2w-\frac{Cw}{d}\Big{)}+ \hat{\sigma}_{4,d}^{2}D_{4,t}\Big{(}4w^{3}-\frac{Cw}{d}\Big{)}\] (D.76)

for some universal constant \(C>0\). The final expression on the right-hand side is negative -- to see this, note that \(2w-\frac{w}{d}>0\) if \(w\gtrsim\xi\kappa^{2}\), and \(4w^{3}-\frac{w}{d}>0\) if \(w\gtrsim\xi\kappa^{2}\) since \(d\geq c_{3}\) by Assumption 3.2. This completes the proof of the lemma when \(\iota\geq 0\). For \(\iota<0\), the lemma holds by the symmetry of \(\rho_{t}\). 

#### d.4.1 Summary of Phase 2

Finally, we show Lemma D.10 which concludes the analysis of Phase 2. For times \(t\geq T_{\text{R}}\) within Phase 2, the particles with \(|\iota|\in[\iota_{\text{L}},\iota_{\text{R}}]\) satisfy \(|w_{\iota}(\iota)|\gtrsim\xi\kappa^{2}\). Using this observation, and the fact that \(D_{2,t}\) and \(D_{4,t}\) have the same sign (meaning that the two terms of \(P_{t}(w)=2\hat{\sigma}_{2,d}^{2}D_{2,t}w+4\hat{\sigma}_{4,d}^{2}D_{4,t}w^{3}\) do not cancel) we show that the velocity \(v(w)\) is large on average. By Lemma D.39, this allows us to show that the loss decreases quickly at any time \(t\geq T_{\text{R}}\) in Phase 2, meaning that either the loss goes below \((\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\) quickly, or we exit Phase 2 quickly.

**Lemma D.10** (Phase 2 Summary).: _Suppose we are in the setting of Theorem 3.3. Let \(T_{2}\) be the minimum time such that either \(D_{2,T_{2}}=0\) or \(D_{4,T_{2}}=0\), i.e. \(T_{2}\) is the end of phase 2. Then, either \(T_{2}-T_{1}\lesssim\frac{(\log\log d)^{18}}{(\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2})}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\) or \(T_{*,\epsilon}-T_{1}\lesssim\frac{(\log\log d)^{18}}{(\hat{\sigma}_{2,d}^{2}+ \hat{\sigma}_{4,d}^{2})}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\)._

Proof of Lemma D.10.: First, let \(T_{\text{R}}\) be as defined in Lemma D.7. By Lemma D.7, \(T_{\text{R}}-T_{1}\lesssim\frac{1}{\hat{\sigma}_{2,d}^{2}\gamma_{2}}\log\log d\). Furthermore, at time \(T_{\text{R}}\), for all \(\iota\) such that \(|\iota|\in[\iota_{\text{L}},\iota_{\text{R}}]\), \(|w_{\iota}(\iota)|\gtrsim\xi\kappa^{2}\).

Our proof strategy is now to show that \(v(w)\) is large on average, and then apply Lemma D.39. At any time \(t\in[T_{\text{R}},T_{2}]\), since \(D_{2,t}\leq 0\), Proposition D.38 implies that \(\mathbb{E}_{w\sim\rho_{t}}[w^{2}]\leq\gamma_{2}+O\Big{(}\frac{1}{d}\Big{)}\). Thus, by Markov's inequality, and because \(d\geq c_{3}\) (where \(c_{3}\) is defined in Assumption 3.2),

\[\mathbb{P}_{w\sim\rho_{t}}(|w|\geq 1/2)\leq 4\cdot\Big{(}\gamma_{2}+\frac{1}{d} \Big{)}\leq 5\tau^{2}\] (D.77)

For all \(\iota\) with \(|\iota|\in[\iota_{\text{L}},\iota_{\text{R}}]\), \(|w_{\iota}(\iota)|\) is increasing as a function of \(t\) on \([0,T_{2}]\), by Lemma D.9. Thus, by Proposition D.6 and a union bound, \(\mathbb{P}_{w\sim\rho_{t}}(\xi\kappa^{2}\leq|w_{\iota}(\iota)|\leq 1/2)\geq 1-O( \kappa)-5\gamma_{2}\geq\frac{1}{2}\) for all \(t\geq T_{\text{R}}\) and all \(\iota\) with \(|\iota|\in[\iota_{\text{L}},\iota_{\text{R}}]\). (Note that \(1-O(\kappa)-5\gamma_{2}\geq\frac{1}{2}\) holds because \(\gamma_{2}\) is less than a sufficiently small constant by Assumption 3.2.) Finally, for all \(w\) satisfying \(\xi\kappa^{2}\leq|w|\leq\frac{1}{2}\), using Lemma 4.4 we can compute that

\[|v_{t}(w)| \geq(1-w^{2})\cdot(|P_{t}(w)|-|Q_{t}(w)|)\] (By Lemma 4.4 ) \[\geq(1-w^{2})\cdot\Big{|}2\hat{\sigma}_{2,d}^{2}|D_{2,t}||w|+4\hat {\sigma}_{4,d}^{2}|D_{4,t}||w^{3}|\Big{|}-O\Big{(}\frac{\hat{\sigma}_{2,d}^{2} |D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}|w|\] (By Lemma 4.4 ) \[\geq\frac{3}{4}\cdot(2\hat{\sigma}_{2,d}^{2}|D_{2,t}|\xi\kappa^{2 }+4\hat{\sigma}_{4,d}^{2}|D_{4,t}|\cdot\xi^{3}\kappa^{6})-O\Big{(}\frac{\hat{ \sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}|w|\] (B.c. \[\xi\kappa^{2}\leq|w|\leq\frac{1}{2}\] ) \[\geq\frac{1}{2}\cdot(2\hat{\sigma}_{2,d}^{2}|D_{2,t}|\xi\kappa^{2 }+4\hat{\sigma}_{4,d}^{2}|D_{4,t}|\cdot\xi^{3}\kappa^{6})\] (B.c. \[d\] is sufficiently large (Assumption 3.2 ) )

Thus, by Lemma D.39,

\[\frac{dL(\rho_{t})}{dt} \lesssim-(\hat{\sigma}_{2,d}^{2}|D_{2,t}|\xi\kappa^{2}+\hat{ \sigma}_{4,d}^{2}|D_{4,t}|\xi^{3}\kappa^{6})^{2}\] (D.78) \[\lesssim-\xi^{6}\kappa^{12}(\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat {\sigma}_{4,d}^{2}|D_{4,t}|)^{2}\] (D.79) \[\lesssim-\xi^{6}\kappa^{12}(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{ 4,d}^{2})(\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D_{4,t}|) \cdot(|D_{2,t}|+|D_{4,t}|)\] (D.80) \[\lesssim-\xi^{6}\kappa^{12}(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{ 4,d}^{2})(\hat{\sigma}_{2,d}^{2}|D_{2,t}|^{2}+\hat{\sigma}_{4,d}^{2}|D_{4,t}|^ {2})\] (D.81) \[\lesssim-\xi^{6}\kappa^{12}(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{ 4,d}^{2})L\] (By Lemma 4.2 )

Thus, since \(L(\rho_{\mathcal{R}_{\text{k}}})\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{ 4,d}^{2})\gamma_{2}\) (by Lemma D.8), by Gronwall's inequality (Fact I.13), we know that for \(t\in[T_{\text{R}},T_{2}]\),

\[L(\rho_{t}) \leq L(\rho_{\mathcal{T}_{\text{k}}})\cdot\exp\Big{(}\int_{T_{ \text{k}}}^{t}-\xi^{6}\kappa^{12}(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{ 2})ds\Big{)}\] (D.82) \[=L(\rho_{\mathcal{T}_{\text{k}}})e^{-(t-T_{\text{k}})\xi^{6} \kappa^{12}(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})}\] (D.83)

Thus, if \(L(\rho_{t})\gtrsim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\), this implies that

\[e^{-(t-T_{\text{k}})\xi^{6}\kappa^{12}(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4, d}^{2})}\gtrsim\frac{\epsilon^{2}}{\gamma_{2}}\] (D.84)

and rearranging gives \(t-T_{\text{R}}\leq\frac{1}{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) \xi^{6}\kappa^{12}}\log(\frac{\gamma_{2}}{\epsilon^{2}})\). Thus, either \(T_{2}-T_{\text{R}}\leq\frac{1}{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2} )\xi^{6}\kappa^{12}}\log(\frac{\gamma_{2}}{\epsilon^{2}})\), or there exists a time \(t\in[T_{\text{R}},T_{2}]\) such that \(L(\rho_{t})\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\), and such that \(t-T_{\text{R}}\leq\frac{1}{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) \xi^{6}\kappa^{12}}\log(\frac{\gamma_{2}}{\epsilon})\). This completes the proof of the lemma. 

### Phase 3, Case 1

Phase 3, Case 1 is the case where \(D_{2,T_{2}}=0\) and \(D_{4,T_{2}}<0\), i.e. the case where \(D_{2,t}\) reaches \(0\) first. As we later show in Lemma D.19 and Lemma D.20, if this occurs, then for all \(t\geq T_{2}\), it will be the case that \(D_{2,t}\geq 0\) and \(D_{4,t}\leq 0\). We now outline our analysis of this case -- the goal of our analysis is to show that either \(|D_{2,t}|\leq\epsilon\) or \(|D_{4,t}|\leq\epsilon\) holds within \(\frac{1}{\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}}\cdot\text{poly}\Big{(} \frac{\log\log d}{\epsilon}\Big{)}\) time after the start of Phase 3, Case 1.

**Proof Strategy for Phase 3, Case 1** Our overall proof strategy in this section is to show that \(|v(w)|\) is large for a large portion of particles \(w\), and then to apply Lemma D.39 to show that the loss decreases quickly. Recall that

\[v_{t}(w)=-(1-w^{2})(P_{t}(w)+Q_{t}(w))\] (D.85)

and intuitively, we can ignore \(Q_{t}(w)\) due to the factor of \(\frac{1}{d}\) in its coefficients. Thus, we can write

\[v_{t}(w) \approx-(1-w^{2})P_{t}(w)\] (D.86) \[\approx-(1-w^{2})\cdot w\cdot(2\hat{\sigma}_{2,d}^{2}D_{2,t}+4 \hat{\sigma}_{4,d}^{2}D_{4,t}w^{2})\,.\] (D.87)Since \(D_{4,t}<0\) and \(D_{2,t}>0\), we can further rewrite the above as

\[v_{t}(w)\approx-(1-w^{2})\cdot w\cdot 4\hat{\sigma}_{4,d}^{2}|D_{4,t}|(w-r)(w+r)\] (D.88)

where \(r=\sqrt{-\frac{\hat{\sigma}_{2,d}^{2}D_{2,t}}{2\hat{\sigma}_{4,d}^{2}D_{4,t}}}\) is the positive root of the last factor \((2\hat{\sigma}_{2,d}^{2}D_{2,t}+4\hat{\sigma}_{4,d}^{2}D_{4,t}w^{2})\). Thus, to show that the velocity \(v_{T}(w)\) is large, we want to show that there is a large fraction of particles \(w\in[0,1]\) simultaneously satisfying the following: (1) \(w\) is far from \(1\), (2) \(w\) is far from \(0\), and (3) \(w\) is far from \(r\). Indeed, in Lemma D.17, we show that out of the particles \(w\in[0,1]\), at least a \(\frac{\gamma_{2}}{2}\) fraction of these particles is in \([\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\), enabling us to bound the distance of these particles \(w\) from \(0\) and \(1\).

Thus, to show that \(v_{t}(w)\) is large for these particles, it suffices to show that \(|w-r|\) is large on average for the particles \(w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\), i.e. the conditional expectation \(\mathbb{E}_{w\sim\rho_{t}}[(w-r)^{2}\mid w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1 /2}]]\) is large. In turn, this is at least the conditional variance of \(w\) conditioned on \(w\) being in \([\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\), so it suffices to show that \(\mathrm{Var}(w\mid w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])\) is large, and by Proposition D.40 it suffices to show that \(\mathbb{E}[(w-w^{\prime})^{2}\mid w,w^{\prime}\in[\gamma_{2}^{3/4},1-\gamma_{ 2}^{1/2}]]\) is large.

To show that the quantity \(\mathbb{E}[(w-w^{\prime})^{2}\mid w,w^{\prime}\in[\gamma_{2}^{3/4},1-\gamma_{ 2}^{1/2}]]\) is large, intuitively we would like to show that during Phase 3, Case 1, \(w\) and \(w^{\prime}\) are getting farther apart. However, this is not true. If we write the velocity as \(v_{t}(w)=(1-w^{2})\cdot w\cdot(4\hat{\sigma}_{4,d}^{2}|D_{4,t}|w^{2}-2\hat{ \sigma}_{2,d}^{2}|D_{2,t}|)\), then the last factor is indeed increasing as a function of \(w\in[0,1]\), and thus "pushes" the different particles \(w,w^{\prime}\) further apart. This does not work as a formal argument, because as two particles \(w,w^{\prime}\) become closer to \(0\) or \(1\), their distance \(|w-w^{\prime}|\) will decrease again due to the factors \(w\) and \((1-w^{2})\) in the velocity. However, we are only concerned with particles \(w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\) -- we must make precise the intuition that the behavior of the particles very close to \(0\) or \(1\) does not matter.

To make this proof strategy precise, we define the potential function \(\Phi(w)=\log\left(\frac{w}{\sqrt{1-w^{2}}}\right)\) in Definition D.11 and show that \(|\Phi(w)-\Phi(w^{\prime})|\) is increasing in Phase 3, Case 1. As shown in Lemma D.12, we have \(\frac{d}{dt}\Phi(w)\approx-2\hat{\sigma}_{2,d}^{2}D_{2,t}-4\hat{\sigma}_{4,d }^{2}D_{4,t}w^{2}\) -- this is simply because \(\frac{d\Phi(w)}{dw}=\frac{1}{w(1-w^{2})}\). In other words, \(\frac{d}{dt}\Phi(w)\) is exactly \(v(w)\), but without the factors \(w\) and \((1-w^{2})\) -- we have selected this potential function \(\Phi(w)\) specifically to eliminate these factors. Using this observation, it is easy to show that \(|\Phi(w)-\Phi(w^{\prime})|\) is increasing and thus obtain a lower bound on \(|\Phi(w)-\Phi(w^{\prime})|\) for a large fraction of pairs \(w,w^{\prime}\). Finally, to convert this to a lower bound on \(|w-w^{\prime}|\), we use the fact that \(\Phi\) is Lipschitz on the interval \([\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\) (Lemma D.14), which gives us \(|w-w^{\prime}|\geq\frac{1}{\gamma_{2}^{(1)}}|\Phi(w)-\Phi(w^{\prime})|\).

We now repeat the definition of the potential function:

**Definition D.11** (Potential Function \(\Phi\)).: _We define \(\Phi:[0,1]\to(-\infty,\infty)\) by \(\Phi(w)=\log\left(\frac{w}{\sqrt{1-w^{2}}}\right)\)._

We note that \(\frac{d\Phi(w)}{dw}=\frac{1}{w(1-w^{2})}\). Next, we note the following useful fact about the potential function (we note that this does not require any assumption that we are in Phase 3, Case 1, and just follows from algebraic manipulations):

**Lemma D.12**.: _In the setting of Theorem 3.3,_

\[\frac{d}{dt}\Phi(w_{t})=-2\hat{\sigma}_{2,d}^{2}D_{2,t}-4\hat{\sigma}_{4,d}^{2} D_{4,t}w^{2}-{\lambda_{d}}^{(1)}-{\lambda_{d}}^{(3)}w^{2}\] (D.89)

_where \({\lambda_{d}}^{(1)}\) and \({\lambda_{d}}^{(3)}\) are as in the statement of Lemma 4.4._

Proof of Lemma D.12.: For any \(w_{t}\),

\[\frac{d}{dt}\Phi(w_{t})=\frac{d\Phi}{dw}\cdot\frac{dw_{t}}{dt}=\frac{1}{w(1-w ^{2})}\cdot\frac{dw_{t}}{dt}\] (D.90)Thus, writing \(\frac{d\nu_{t}}{dt}=-(1-w^{2})\cdot(P_{t}(w)+Q_{t}(w))\) where \(P_{t}\) and \(Q_{t}\) are defined in Lemma 4.4, we obtain

\[\frac{d}{dt}\Phi(w_{t}) =\frac{1}{w(1-w^{2})}\cdot\frac{dw_{t}}{dt}\] (D.91) \[=-\frac{1}{w}\cdot(P_{t}(w)+Q_{t}(w))\] (D.92) \[=-\frac{P_{t}(w)}{w}-\frac{Q_{t}(w)}{w}\] (D.93) \[=-2\hat{\sigma}_{2,d}^{2}D_{2,t}-4\hat{\sigma}_{4,d}^{2}D_{4,t}w^ {2}-\frac{Q_{t}(w)}{w}\] (D.94) \[=-2\hat{\sigma}_{2,d}^{2}D_{2,t}-4\hat{\sigma}_{4,d}^{2}D_{4,t}w^ {2}-{\lambda_{d}}^{(1)}-{\lambda_{d}}^{(3)}w^{2}\] (D.95)

where \({\lambda_{d}}^{(1)}\) and \({\lambda_{d}}^{(3)}\) are as in the statement of Lemma 4.4. 

Intuitively, when \(D_{2}>0\) and \(D_{4}<0\), the main part of the velocity is \(P_{t}(w)=-2\hat{\sigma}_{2,d}^{2}D_{2,t}w-4\hat{\sigma}_{4,d}^{2}D_{4,t}w^{3}\) which is an upward sloping cubic polynomial -- thus, the particles \(w\) move away from the positive root of this polynomial, and the distance between any pair \(w,w^{\prime}\) of particles is increasing provided that \(w\) and \(w^{\prime}\) are not too close to \(0\) or \(1\). In the following few lemmas, we make this intuition formal using the potential function \(\Phi\). We first show that for two particles \(w,w^{\prime}\), the distance between \(\Phi(w)\) and \(\Phi(w^{\prime})\) is increasing as long as \(D_{4,t}\leq 0\). Note that this condition holds not only during Phase 3, Case 1, but also during Phases 1 and 2.

**Lemma D.13** (\(\Phi(w)\) and \(\Phi(w^{\prime})\) Moving Apart).: _In the setting of Theorem 3.3, suppose \(D_{4,t}\leq 0\) for all \(t\) in some time interval \([t_{0},t_{1}]\), and let \(\iota_{1},\iota_{2}>0\). Then,_

\[\left|\Phi(w_{t}(\iota_{1}))-\Phi(w_{t}(\iota_{2}))\right|\] (D.96)

_is increasing on the interval \([t_{0},t_{1}]\)._

Proof of Lemma D.13.: Suppose \(\iota_{1},\iota_{2}\in[0,1]\) such that \(\iota_{1}<\iota_{2}\) -- by Proposition D.37, we will always have \(w_{t}(\iota_{1})\leq w_{t}(\iota_{2})\). For convenience, in the rest of this proof, we will write \(w_{1,t}=w_{t}(\iota_{1})\) and \(w_{2,t}=w_{t}(\iota_{2})\). Since \(\Phi\) is an increasing function of \(w\), it will also always be the case that \(\Phi(w_{2,t})\geq\Phi(w_{1,t})\). Thus, to show that \(|\Phi(w_{2,t})-\Phi(w_{1,t})|\) is increasing, it suffices to show that \(\Phi(w_{2,t})-\Phi(w_{1,t})\) is increasing. By Lemma D.12,

\[\frac{d}{dt}\Big{(}\Phi(w_{2,t})-\Phi(w_{1,t})\Big{)} =\Big{(}-2\hat{\sigma}_{2,d}^{2}D_{2,t}-4\hat{\sigma}_{4,d}^{2}D_ {4,t}w_{2,t}^{2}-{\lambda_{d}}^{(1)}-{\lambda_{d}}^{(3)}w_{2,t}^{2}\Big{)}\] (D.97) \[\qquad\quad-\Big{(}-2\hat{\sigma}_{2,d}^{2}D_{2,t}-4\hat{\sigma}_ {4,d}^{2}D_{4,t}w_{1,t}^{2}-{\lambda_{d}}^{(1)}-{\lambda_{d}}^{(3)}w_{1,t}^{2 }\Big{)}\] (D.98) \[=-4\hat{\sigma}_{4,d}^{2}D_{4,t}(w_{2,t}^{2}-w_{1,t}^{2})-{ \lambda_{d}}^{(3)}(w_{2,t}^{2}-w_{1,t}^{2})\] (D.99) \[=4\hat{\sigma}_{4,d}^{2}|D_{4,t}|(w_{2,t}^{2}-w_{1,t}^{2})-{ \lambda_{d}}^{(3)}(w_{2,t}^{2}-w_{1,t}^{2})\] (D.100)

where \({\lambda_{d}}^{(1)}\) and \({\lambda_{d}}^{(3)}\) are as in Lemma 4.4. Recall from the statement of Lemma 4.4 that \(|{\lambda_{d}}^{(3)}|\lesssim\hat{\sigma}_{4,d}^{2}|D_{4,t}|\cdot\frac{1}{d}\). Therefore,

\[\frac{d}{dt}\Big{(}\Phi(w_{2,t})-\Phi(w_{1,t})\Big{)} =4\hat{\sigma}_{4,d}^{2}|D_{4,t}|(w_{2,t}^{2}-w_{1,t}^{2})-{ \lambda_{d}}^{(3)}(w_{2,t}^{2}-w_{1,t}^{2})\] (D.101) \[\geq 3\hat{\sigma}_{4,d}^{2}|D_{4,t}|(w_{2,t}^{2}-w_{1,t}^{2})\] (D.102)

and the right hand side is nonnegative since \(w_{2,t}\geq w_{1,t}\geq 0\). Thus, \(\Phi(w_{2,t})-\Phi(w_{1,t})\) is increasing, as desired. 

Next, we prove a helpful lemma showing that the potential function is bi-Lipschitz. This is useful both when we show that \(|\Phi(w)-\Phi(w^{\prime})|\) is initially large (as it allows us to leverage an initial lower bound on \(|w-w^{\prime}|\)) and when we show that \(|w-w^{\prime}|\) is large later during Phase 3, Case 1 (as it allows us to leverage the lower bound we obtain on \(|\Phi(w)-\Phi(w^{\prime})|\)).

**Lemma D.14**.: _For all \(w,w^{\prime}\in[0,1]\), \(|\Phi(w)-\Phi(w^{\prime})|\geq|w-w^{\prime}|\). Additionally, for \(\eta<\frac{1}{2}\) and \(w,w^{\prime}\in[\eta,1-\eta]\), \(|\Phi(w)-\Phi(w^{\prime})|\leq\frac{1}{\eta^{2}}|w-w^{\prime}|\)._

Proof of Lemma D.14.: First, for all \(w\in[0,1]\), \(\frac{d\Phi(w)}{dw}=\frac{1}{w(1-w^{2})}>1\), and the first statement of the lemma follows. For the second statement of the lemma, observe that for \(w\in[\eta,1-\eta]\),

\[\Big{|}\frac{d\Phi(w)}{dw}\Big{|}\leq\frac{1}{w(1-w^{2})}\leq \frac{1}{\eta(1-(1-\eta)^{2})}=\frac{1}{\eta(1-(1-2\eta+\eta^{2}))}=\frac{1} {\eta(2\eta-\eta^{2})}<\frac{1}{\eta^{2}}\] (D.103)

where the last equality is because \(\eta>\eta^{2}\) (so \(2\eta-\eta^{2}>\eta\)). Thus, \(\Phi\) is \(\frac{1}{\eta^{2}}\)-Lipschitz on the interval \([\eta,1-\eta]\). 

We next need to show that the potential initially has a large value, at the beginning of Phase 3, Case 1. We show this in the next two lemmas, by showing that for any two particles \(w,w^{\prime}\), the distance between them grows by a large factor during Phase 2 (specifically, by time \(T_{\text{R}}\)) and then using this to obtain a lower bound on \(|\Phi(w)-\Phi(w^{\prime})|\) by the bi-Lipschitzness of \(\Phi\).

**Lemma D.15**.: _In the setting of Theorem 3.3, let \(T_{\text{R}}\) be as defined in Lemma D.7. Then, for \(\iota,\iota^{\prime}\in[\iota_{\text{L}},\iota_{\text{R}}]\),_

\[\frac{w_{T_{\text{R}}}(\iota^{\prime})-w_{T_{\text{R}}}(\iota)} {\iota^{\prime}-\iota}\gtrsim\xi\kappa\sqrt{d}\] (D.104)

Proof of Lemma D.15.: Let \(\iota,\iota^{\prime}\in[\iota_{\text{L}},\iota_{\text{R}}]\), with \(\iota<\iota^{\prime}\). For convenience, we will write \(w_{t}=w_{t}(\iota)\) and \(w^{\prime}_{t}=w_{t}(\iota^{\prime})\). Our goal is to obtain a lower bound on the factor by which \(w^{\prime}_{t}-w_{t}\) grows by time \(T_{\text{R}}\). First, define \(P_{t}\) and \(Q_{t}\) as in Lemma 4.4, and observe that

\[\frac{d}{dt}(w^{\prime}_{t}-w_{t}) =-(1-(w^{\prime}_{t})^{2})(P_{t}(w^{\prime}_{t})+Q_{t}(w^{\prime} _{t}))-(1-w^{2}_{t})(P_{t}(w_{t})+Q_{t}(w_{t}))\] (D.105) \[=-\Big{(}P_{t}(w^{\prime}_{t})-P_{t}(w_{t})\Big{)}+\Big{(}(w^{ \prime}_{t})^{2}P_{t}(w^{\prime}_{t})-w^{2}_{t}P_{t}(w_{t})\Big{)}-\Big{(}Q_{t} (w^{\prime}_{t})-Q_{t}(w_{t})\Big{)}\] (D.106) \[\qquad+\Big{(}(w^{\prime}_{t})^{2}Q_{t}(w^{\prime}_{t})-w^{2}_{t }Q_{t}(w_{t})\Big{)}\] (D.107)

We first obtain a lower bound on the first term \(-\Big{(}P_{t}(w^{\prime}_{t})-P_{t}(w_{t})\Big{)}\), and then show that the other terms are lower-order terms as long as \(t\leq T_{\text{R}}\). For \(t\in[0,T_{\text{R}}]\),

\[-(P_{t}(w^{\prime}_{t})-P_{t}(w_{t})) =-\Big{(}2\hat{\sigma}^{2}_{2,d}D_{2,t}w^{\prime}_{t}+4\hat{ \sigma}^{2}_{4,d}D_{4,t}(w^{\prime}_{t})^{3}\Big{)}+\Big{(}2\hat{\sigma}^{2}_ {2,d}D_{2,t}w_{t}+4\hat{\sigma}^{2}_{4,d}D_{4,t}w^{3}_{t}\Big{)}\] (Lemma 4.4 ) \[=-2\hat{\sigma}^{2}_{2,d}D_{2,t}(w^{\prime}_{t}-w_{t})-4\hat{ \sigma}^{2}_{4,d}D_{4,t}((w^{\prime}_{t})^{3}-w^{3}_{t})\] (D.108) \[=2\hat{\sigma}^{2}_{2,d}|D_{2,t}|(w^{\prime}_{t}-w_{t})+4\hat{ \sigma}^{2}_{4,d}|D_{4,t}|((w^{\prime}_{t})^{3}-w^{3}_{t})\] (B.c. \[t\leq T_{\text{R}},D_{2,t},D_{4,t}<0\] by Lemma D.8 ) \[=2\hat{\sigma}^{2}_{2,d}|D_{2,t}|(w^{\prime}_{t}-w_{t})+4\hat{ \sigma}^{2}_{4,d}|D_{4,t}|((w^{\prime}_{t})^{2}+w_{t}w^{\prime}_{t}+w^{2}_{t })(w^{\prime}_{t}-w_{t})\] (D.109) \[\geq 2\hat{\sigma}^{2}_{2,d}|D_{2,t}|(w^{\prime}_{t}-w_{t})\] (B.c. omitted term is nonnegative -- \[w^{\prime}_{t}>w_{t}\] by Proposition D.37 )

Next, let us show that the remaining terms in Eq. (D.107) are lower-order terms which will not decrease the growth rate of \(w^{\prime}_{t}-w_{t}\) too much. First let us deal with the absolute value of the secondterm in Eq. (D.107). For convenience, let \(w_{R,t}=w_{t}(t_{\text{R}})\). Then,

\[\Big{|}(w_{t}^{\prime})^{2}P_{t}(w_{t}^{\prime})-w_{t}^{2}P_{t}(w_{t })\Big{|} \lesssim\Big{|}(2\hat{\sigma}_{2,d}^{2}D_{2,t}(w_{t}^{\prime})^{3}+4 \hat{\sigma}_{4,d}^{2}D_{4,t}(w_{t}^{\prime})^{5})\] (D.110) \[\qquad\qquad-(2\hat{\sigma}_{4,d}^{2}D_{2,t}(w_{t})^{3}+4\hat{ \sigma}_{4,d}^{2}D_{4,t}(w_{t})^{5})\Big{|}\] (D.111) \[\lesssim 2\hat{\sigma}_{2,d}^{2}|D_{2,t}||(w_{t}^{\prime})^{3}-w_{t }^{3}|+4\hat{\sigma}_{4,d}^{2}|D_{4,t}||(w_{t}^{\prime})^{5}-w_{t}^{5}|\] (By Lemma 4.4) \[\lesssim 2\hat{\sigma}_{2,d}^{2}|D_{2,t}||(w_{t}^{\prime})^{2}+w_{t }^{\prime}w_{t}+w_{t}^{2}||w_{t}^{\prime}-w_{t}|\] (D.112) \[\qquad+4\hat{\sigma}_{4,d}^{2}|D_{4,t}||(w_{t}^{\prime})^{4}+(w_ {t}^{\prime})^{3}w_{t}+(w_{t}^{\prime})^{2}w_{t}^{2}+w_{t}^{\prime}w_{t}^{3}+w _{t}^{4}||w_{t}^{\prime}-w_{t}|\] (D.113) \[\lesssim\hat{\sigma}_{2,d}^{2}|D_{2,t}|w_{R,t}^{2}|w_{t}^{\prime} -w_{t}|\] (B.c. \[|w_{t}|,|w_{t}^{\prime}|\leq w_{R,t}\] by Proposition D.37)

where the last inequality is also because \(\hat{\sigma}_{4,d}^{2}\lesssim\hat{\sigma}_{2,d}^{2}\), and \(\gamma_{4}\lesssim\gamma_{2}^{2}\) (Assumption 3.2) and \(|D_{2}|\geq\frac{\gamma_{2}}{2}\) and \(|D_{4}|\leq\frac{3\gamma_{4}}{2}\) for \(t\leq T_{\text{R}}\) (Lemma D.8), meaning that the first term in Eq. (D.113) dominates up to universal constant factors. Next, let us deal with the absolute value of the third term in Eq. (D.107). By Lemma I.16,

\[|Q_{t}(w_{t})^{\prime}-Q_{t}(w_{t})| \lesssim\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d} ^{2}|D_{4,t}|}{d}\cdot|w_{t}^{\prime}-w_{t}|\] (By definition of \[\lambda_{d}{}^{(1)},\lambda_{d}{}^{(3)}\], Lemma 4.4) \[\lesssim\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d}|w_{t}^{\prime}- w_{t}|\] (B.c. \[\hat{\sigma}_{4,d}^{2}\lesssim\hat{\sigma}_{2,d}^{2}\] and \[|D_{4}|\lesssim\gamma_{4}\lesssim\gamma_{2}^{2}\lesssim|D_{2}|\] for \[t\leq T_{\text{R}}\] (Lemma D.8)

Using the same argument with Lemma I.16, we can bound the fourth term in Eq. (D.107):

\[|(w_{t}^{\prime})^{2}Q_{t}(w_{t}^{\prime})-w_{t}^{2}Q_{t}(w_{t})|\lesssim \frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d}|w_{t}^{\prime}-w_{t}|\] (D.114)

Combining the bounds we obtained for all the terms of Eq. (D.107), we obtain

\[\frac{d}{dt}(w_{t}^{\prime}-w_{t}) \geq 2\hat{\sigma}_{2,d}^{2}|D_{2,t}|(w_{t}^{\prime}-w_{t})-O \Big{(}\hat{\sigma}_{2,d}^{2}|D_{2,t}|\Big{(}w_{R,t}^{2}+1/d\Big{)}(w_{t}^{ \prime}-w_{t})\Big{)}\] (D.115) \[=\Big{(}2\hat{\sigma}_{2,d}^{2}|D_{2,t}|-O\Big{(}\hat{\sigma}_{2, d}^{2}|D_{2,t}|(w_{R,t}^{2}+1/d)\Big{)}\Big{)}(w_{t}^{\prime}-w_{t})\] (D.116)

and therefore,

\[\frac{\frac{d}{dt}(w_{t}^{\prime}-w_{t})}{w_{t}^{\prime}-w_{t}}\geq 2\hat{ \sigma}_{2,d}^{2}|D_{2,t}|-O\Big{(}\hat{\sigma}_{2,d}^{2}|D_{2,t}|(w_{R,t}^{2 }+1/d)\Big{)}\] (D.117)

First, let us consider how much \(w_{t}^{\prime}-w_{t}\) grows during Phase 1. At this point, we apply Lemma D.4 with \(\iota_{2}=\iota_{\text{U}}\) and \(0<\iota_{1}<\iota_{\text{U}}\), with \(\delta=\frac{1}{\log d}\). Let us first verify that the assumptions of Lemma D.4 are satisfied. By Lemma D.5, \(F_{2}\asymp\frac{\sqrt{d}}{(\log d)^{2}}\), using the notation of Lemma D.4. Thus, \(\log F_{2}\leq\log d\), meaning \(\delta\leq\frac{1}{\log F_{2}}\). By Assumption 3.2, \(\delta\leq\frac{\gamma_{2}^{2}}{16}\), and by Proposition I.5, \(\mathbb{P}(|\iota|>\iota_{\text{U}})\leq\frac{1}{\log d}\leq\frac{\gamma_{2}^ {2}}{16}\) clearly holds. Thus, the assumptions of Lemma D.4 are satisfied. As a consequence of Lemma D.4 (specifically Eq. (D.47)) we obtain

\[\exp\Big{(}\int_{0}^{T_{1}}2\hat{\sigma}_{2,d}^{2}|D_{2,t}|dt\Big{)}\gtrsim F_{ 2}\gtrsim\frac{\sqrt{d}}{(\log d)^{2}}\] (D.118)For \(t\leq T_{1}\), we have \(w_{R,t}^{2}\leq\delta^{2}\leq O(\frac{1}{\log d})\) by Definition 4.5 (and because \(w_{t}(\iota_{\text{R}})\leq w_{t}(\iota_{\text{U}})\) by Proposition D.37). Thus,

\[\frac{w_{T_{1}}^{\prime}-w_{T_{1}}}{w_{0}^{\prime}-w_{0}} \gtrsim\exp\Big{(}\int_{0}^{T_{1}}2\hat{\sigma}_{2,d}^{2}|D_{2,t}| \cdot\Big{(}1-O\Big{(}\frac{1}{\log d}\Big{)}\Big{)}dt\Big{)}\] (By Eq. (D.117)) \[\gtrsim\Big{(}\frac{\sqrt{d}}{(\log d)^{2}}\Big{)}^{1-O(1/\log d)}\] (By Eq. (D.118)) \[\gtrsim\frac{\sqrt{d}}{(\log d)^{2}}\] (B.c. \[d^{1/\log d}\leq O(1)\] )

Additionally, we apply Lemma D.4 with \(\iota_{2}=\iota_{\text{R}}\) (and an arbitrary \(\iota_{1}\in[\iota_{\text{L}},\iota_{\text{R}}]\)) with \(\delta=\frac{1}{2\log\log d}\), and \(t_{0}=T_{1}\) and \(t_{1}\) being the time \(T_{\text{R}}\) such that \(w_{T_{\text{R}}}(\iota_{\text{R}})=\xi\). We verify that the conditions of Lemma D.4 hold. Using the notation of Lemma D.4, \(F_{2}=\frac{w_{T_{\text{R}}}(\iota_{\text{R}})}{w_{T_{1}}(\iota_{\text{R}})}=\xi \kappa(\log d)^{2}\) by Lemma D.7, and thus \(\log F_{2}\leq\frac{1}{\delta}\). Again by Assumption 3.2, \(\delta\leq\frac{\gamma_{2}^{2}}{16}\), and by Proposition D.6, \(\mathbb{P}(|\iota|>\iota_{\text{R}})\leq O(\kappa)\leq\frac{\gamma_{2}^{2}}{16}\). Thus, we apply Lemma D.4 to obtain

\[\exp\Big{(}\int_{T_{1}}^{T}2\hat{\sigma}_{2,d}^{2}|D_{2,t}|dt\Big{)}\gtrsim F_ {2}\gtrsim\xi\kappa(\log d)^{2}\] (D.119)

For \(t\leq T_{\text{R}}\), \(w_{R,t}^{2}\leq\xi^{2}\leq\delta^{2}\), meaning that

\[\frac{w_{T_{\text{R}}}^{\prime}-w_{T_{\text{R}}}}{w_{T_{1}}^{\prime }-w_{T_{1}}} \gtrsim\exp\Big{(}\int_{T_{1}}^{T_{\text{R}}}2\hat{\sigma}_{2,d}^ {2}|D_{2,t}|\cdot(1-O(\delta^{2}))dt\Big{)}\] (By Eq. (D.117)) \[\gtrsim(\xi\kappa(\log d)^{2})^{1-O(\delta^{2})}\] (By Eq. (D.119)) \[\gtrsim\xi\kappa(\log d)^{2}\] (B.c. \[(\log d)^{O(\delta)}=(\log d)^{O(\frac{1}{\log\log d})}\leq O(1)\] )

In summary, since \(w_{t}^{\prime}-w_{t}\) grows by \(\frac{\sqrt{d}}{(\log d)^{2}}\) (up to a constant factor) from time \(0\) to time \(T_{1}\), and by \(\xi\kappa(\log d)^{2}\) (up to a constant factor) from time \(T_{1}\) to time \(T_{\text{R}}\), this means

\[\frac{w_{T_{\text{R}}}^{\prime}-w_{T_{\text{R}}}}{w_{0}^{\prime}-w_{0}} \gtrsim\xi\kappa\sqrt{d}\] (D.120)

from time \(0\) to time \(T_{\text{R}}\), as desired. 

We now use the previous lemma to obtain a lower bound on \(|\Phi(w)-\Phi(w^{\prime})|\) by using the bi-Lipschitzness of \(\Phi\).

**Lemma D.16** (Initial Large Distance Between \(w\) and \(w^{\prime}\)).: _Suppose we are in the setting of Theorem 3.3. Let \(T_{\text{R}}\) be as in the statement of Lemma D.7. If \(\iota,\iota^{\prime}\) are sampled independently from \(\rho_{0}\), then with probability at least \(1-O(\epsilon)\),_

\[|\Phi(w_{T_{\text{R}}}(\iota))-\Phi(w_{T_{\text{R}}}(\iota^{\prime}))|\gtrsim \xi\kappa^{3}\] (D.121)

Proof of Lemma D.16.: By Eqs. (1.16) and (1.18) of Atkinson and Han [12], the probability density function of \(\iota\) is \(p(\iota)=\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/2)}(1-\iota^{2})^{\frac{d-3 }{2}}\). Thus, for \(\iota\in[\iota_{\text{L}},\iota_{\text{R}}]\), the conditional density function \(p(\iota\mid\iota\in[\iota_{\text{L}},\iota_{\text{R}}])\) can be upper bounded as

\[p(\iota\mid\iota\in[\iota_{\text{L}},\iota_{\text{R}}]) \lesssim\frac{p(\iota)}{p(\iota\in[\iota_{\text{L}},\iota_{\text{R }}])}\] (D.122) \[\lesssim\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/2)}\cdot\frac{(1- \iota^{2})^{\frac{d-3}{2}}}{p(\iota\in[\iota_{\text{L}},\iota_{\text{R}}])}\] (D.123) \[\lesssim\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/2)}\cdot\frac{(1- \iota^{2})^{\frac{d-3}{2}}}{1-O(\kappa)}\] (By Proposition D.6) \[\lesssim\sqrt{d}\] (By Stirling's Formula)In particular, for any interval \(I\) of length at most \(\frac{\kappa^{2}}{\sqrt{d}}\),

\[\mathbb{P}(\iota\in I\mid\iota\in[\iota_{\mathrm{L}},\iota_{\mathrm{R}}]) \lesssim\sqrt{d}\cdot|I|\lesssim\kappa^{2}\] (D.124)

and from this it follows that

\[\mathbb{P}_{\iota,\iota^{\prime}\sim\rho_{0}}\Big{(}|\iota-\iota^{\prime}| \leq\frac{\kappa^{2}}{\sqrt{d}}\mid\iota,\iota^{\prime}\in[\iota_{\mathrm{L}}, \iota_{\mathrm{R}}]\Big{)}\lesssim\kappa^{2}\] (D.125)

Next, suppose \(\iota,\iota^{\prime}\in[\iota_{\mathrm{L}},\iota_{\mathrm{R}}]\) with \(\iota^{\prime}>\iota\), and \(|\iota-\iota^{\prime}|\geq\frac{\kappa^{2}}{\sqrt{d}}\). Then, by Lemma D.15, if \(T_{\mathrm{R}}\) is the time \(T\) mentioned in the statement of Lemma D.7, then

\[w_{T_{\mathrm{R}}}(\iota^{\prime})-w_{T_{\mathrm{R}}}(\iota)\gtrsim\xi\kappa^{3}\] (D.126)

Suppose we sample \(\iota,\iota^{\prime}\) independently from \(\rho_{0}\). By Proposition D.6, with probability at least \(1-O(\kappa)\), \(\iota,\iota^{\prime}\in[\iota_{\mathrm{L}},\iota_{\mathrm{R}}]\). Thus, by Eq. (D.125), the probability that \(\iota,\iota^{\prime}\in[\iota_{\mathrm{L}},\iota_{\mathrm{R}}]\) and \(|\iota-\iota^{\prime}|\geq\frac{\kappa^{2}}{\sqrt{d}}\) is at least \(1-O(\kappa)\). In summary, if we sample \(\iota,\iota^{\prime}\) independently from \(\rho_{0}\), then with probability \(1-O(\kappa)\),

\[|\Phi(w_{T_{\mathrm{R}}}(\iota))-\Phi(w_{T_{\mathrm{R}}}(\iota^{\prime}))| \gtrsim\xi\kappa^{3}\] (D.127)

by Lemma D.14, as desired. 

Next, we show that during Phase 3, Case 1, at any time, there is a significant fraction of particles whose distance from \(0\) or \(1\) can be bounded below.

**Lemma D.17**.: _In the setting of Theorem 3.3, suppose \(D_{4,t}\leq 0\) and \(D_{2,t}\geq 0\). Then,_

\[\mathbb{P}_{w\sim\rho_{t}}(|w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])\geq \frac{\gamma_{2}}{2}\] (D.128)

Proof of Lemma D.17.: For convenience, we define \(\tau=\sqrt{\gamma_{2}}\) and \(\beta\geq 1.1\) such that \(\gamma_{4}=\beta\tau^{4}\). (Here, \(\beta\) and \(\tau\) are well-defined by Assumption 3.2.) First, since \(D_{2,t}\geq 0\) during Phase 3 Case 1, we know that

\[\underset{w_{t}\sim\rho_{t}}{\mathbb{E}}[P_{2,d}(w)]\geq\tau^{2}\] (By Definition of \[D_{2,t}\] )

and since \(D_{4,t}\leq 0\) during Phase 3 Case 1, we know that

\[\underset{w_{t}\sim\rho_{t}}{\mathbb{E}}[P_{4,d}(w)]\leq\beta\tau^{4}\] (By Definition of \[D_{4,t}\] )

Rearranging using \(P_{2,d}(t)=\frac{d}{d-1}t^{2}-\frac{1}{d-1}\), we obtain

\[\frac{d}{d-1}\underset{w_{t}\sim\rho_{t}}{\mathbb{E}}[w^{2}]-\frac{1}{d-1} \geq\tau^{2}\] (D.129)

or

\[\underset{w_{t}\sim\rho_{t}}{\mathbb{E}}[w^{2}]\geq\frac{d-1}{d}\tau^{2}+ \frac{1}{d}\] (D.130)

Similarly, rearranging using \(P_{4,d}(t)=\frac{d^{2}+6d+8}{d^{2}-1}t^{4}-\frac{6d+12}{d^{2}-1}t^{2}+\frac{3 }{d^{2}-1}\), we obtain

\[\frac{d^{2}+6d+8}{d^{2}-1}\underset{w_{t}\sim\rho_{t}}{\mathbb{E}}[w^{4}]- \frac{6d+12}{d^{2}-1}\underset{w_{t}\sim\rho_{t}}{\mathbb{E}}[w^{2}]+\frac{3 }{d^{2}-1}\leq\beta\tau^{4}\] (D.131)

and rearranging gives

\[\underset{w_{t}\sim\rho_{t}}{\mathbb{E}}[w^{4}]\leq\beta\tau^{4}+O\Big{(} \frac{1}{d}\Big{)}\] (D.132)because \(w\leq 1\) and we can assume \(\beta\tau^{4}\leq 1\) due to Assumption 3.2. Suppose that with probability more than \(1-\frac{\tau^{2}}{2}\) under \(\rho_{t}\), \(w\not\in[\tau^{3/2},1-\tau]\). Then,

\[\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{2}] =\mathbb{P}_{w\sim\rho_{t}}(w\geq 1-\tau)\cdot\mathop{\mathbb{E}}_ {w\sim\rho_{t}}[w^{2}\mid w\geq 1-\tau]\] (D.133) \[\qquad\qquad\qquad+\mathbb{P}_{w\sim\rho_{t}}(w\in[\tau^{3/2},1- \tau])\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{2}\mid w\in[\tau^{3/2},1-\tau]]\] (D.134) \[\qquad\qquad\qquad+\mathbb{P}_{w\sim\rho_{t}}(w\leq\tau^{3/2}) \cdot\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{2}\mid w\leq\tau^{3/2}]\] (D.135) \[\leq\mathbb{P}_{w\sim\rho_{t}}(w\geq 1-\tau)+\frac{\tau^{2}}{2}+ \tau^{3}\] (By assumption that \[\mathbb{P}_{w\sim\rho_{t}}(w\in[\tau^{3/2},1-\tau])\leq\frac{\tau^{2}}{2}\] ) \[\leq\mathbb{P}_{w\sim\rho_{t}}(w\geq 1-\tau)+\frac{3\tau^{2}}{4}\] (B.c. \[\tau\leq 1/4\] (by Assumption 3.2, \[\tau\] is sufficiently small))

which by Eq. (D.130) implies that

\[\mathbb{P}_{w\sim\rho_{t}}(w\geq 1-\tau)\geq\mathop{\mathbb{E}}_{w\sim\rho_{t}}[ w^{2}]-\frac{3\tau^{2}}{4}\geq\frac{d-1}{d}\tau^{2}+\frac{1}{d}-\frac{3\tau^{2}}{4}= \frac{\tau^{2}}{4}-\frac{\tau^{2}}{d}+\frac{1}{d}\geq\frac{\tau^{2}}{4}\] (D.136)

where the last inequality is because \(\tau\leq 1\). On the other hand,

\[\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{4}] =\mathbb{P}_{w\sim\rho_{t}}(w\geq 1-\tau)\cdot\mathop{\mathbb{E}}_ {w\sim\rho_{t}}[w^{4}\mid w\geq 1-\tau]\] (D.137) \[\qquad\qquad\qquad+\mathbb{P}_{w\sim\rho_{t}}(w\in[\tau^{3/2},1- \tau])\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{4}\mid w\in[\tau^{3/2},1-\tau]]\] (D.138) \[\qquad\qquad\qquad+\mathbb{P}_{w\sim\rho_{t}}(w\leq\tau^{3/2}) \cdot\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{4}\mid w\leq\tau^{3/2}]\] (D.139) \[\geq\mathbb{P}_{w\sim\rho_{t}}(w\geq 1-\tau)\cdot\mathop{ \mathbb{E}}_{w\sim\rho_{t}}[w^{4}\mid w\geq 1-\tau]\] (D.140) \[\geq(1-\tau)^{4}\cdot\frac{\tau^{2}}{4}\] (By Eq. (D.136)) \[\geq\frac{\tau^{2}}{64}\] (B.c. \[\tau\leq 1/2\] (see Assumption 3.2))

By Eq. (D.132), this is a contradiction, since it implies that

\[\frac{\tau^{2}}{64}\leq\beta\tau^{4}+O\Big{(}\frac{1}{d}\Big{)}\leq 2\beta \tau^{4}\] (D.142)

where the second inequality is because \(\beta\geq 1.1\) and \(d\) is sufficiently large (see Assumption 3.2). This implies that \(128\beta\tau^{2}\geq 1\), which contradicts Assumption 3.2 (since \(\gamma_{2}\) is chosen to be sufficiently small). Thus, our original assumption that \(\mathbb{P}_{w\sim\rho_{t}}(w\in[\tau^{3/2},1-\tau])\leq\frac{\tau^{2}}{2}\) is incorrect. 

The purpose of the next three lemmas is to show that if Phase 3, Case 1 occurs, i.e. if \(D_{2,T_{2}}=0\) and \(D_{4,T_{2}}<0\), then for all \(t\geq T_{2}\), we have \(D_{2,t}\geq 0\) and \(D_{4,t}\leq 0\). First, the following lemma states that if \(D_{4,t}\) is much smaller than \(D_{2,t}\) in absolute value, and \(D_{4,t}<0\) and \(D_{2,t}>0\), then the velocity of the particles \(w\geq 0\) will be significantly negative.

**Lemma D.18**.: _Suppose we are in the setting of Theorem 3.3, and suppose \(D_{4,T_{2}}=0\) and \(t\geq T_{2}\) such that \(D_{4,t}\leq 0\) and \(D_{2,t}\geq 0\). If \(|D_{4,t}|\leq\xi|D_{2,t}|\) where \(\xi=\frac{1}{\log\log d}\), then for all \(w\in[0,1]\),_

\[v_{t}(w)=-(1-w^{2})\cdot(1\pm O(\xi))\cdot 2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w\] (D.143)

_In particular, for \(w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\),_

\[v_{t}(w)\lesssim-\gamma_{2}^{5/4}\hat{\sigma}_{2,d}^{2}|D_{2,t}|\] (D.144)Proof.: Suppose \(|D_{4,t}|\leq\xi|D_{2,t}|\) where \(\xi=\frac{1}{\log\log d}\). Let \(P_{t}\) and \(Q_{t}\) be as in Lemma 4.4. Then, for any \(w\in[0,1]\),

\[P_{t}(w) =(2\hat{\sigma}_{2,d}^{2}D_{2,t}w+4\hat{\sigma}_{4,d}^{2}D_{4,t}w^ {3})\] (D.145) \[=(2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w-4\hat{\sigma}_{4,d}^{2}|D_{4,t }|w^{3})\] (D.146) \[=\Big{(}2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w\pm O(\xi\hat{\sigma}_{ 4,d}^{2}|D_{2,t}|w^{3})\Big{)}\] (D.147) \[=(1\pm O(\xi))\cdot 2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w\] (By Assumption 3.2, \[\hat{\sigma}_{4,d}^{2}\lesssim\hat{\sigma}_{2,d}^{2}\] )

Thus, for \(w\in[0,1]\),

\[v_{t}(w) =-(1-w^{2})(P_{t}(w)+Q_{t}(w))\] (By Lemma 4.4 ) \[=-(1-w^{2})\Big{(}|P_{t}(w)|\pm O\Big{(}\frac{\hat{\sigma}_{2,d}^ {2}|D_{2,t}|}{d}w\Big{)}\Big{)}\] (B.c. \[|D_{2,t}|\geq\xi|D_{4,t}|\] and \[\hat{\sigma}_{4,d}^{2}\lesssim\hat{\sigma}_{2,d}^{2}\] by Assumption 3.2 ) \[=-(1-w^{2})\Big{(}(1\pm O(\xi))\cdot 2\hat{\sigma}_{2,d}^{2}|D_{2,t }|w\pm O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d}w\Big{)}\Big{)}\] (By Eq. (D.145)) \[=-(1-w^{2})\cdot(1\pm O(\xi))\cdot 2\hat{\sigma}_{2,d}^{2}|D_{2,t }|w\] (B.c. \[\xi\geq\tfrac{1}{d}\] )

In particular, for \(w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\),

\[v_{t}(w) \lesssim-(1-(1-\gamma_{2}^{1/2})^{2})\cdot(1-O(\xi))\cdot 2\hat{ \sigma}_{2,d}^{2}|D_{2,t}|\gamma_{2}^{3/4}\] (D.148) \[\lesssim-(1-(1-\gamma_{2}^{1/2})^{2})\cdot\hat{\sigma}_{2,d}^{2} |D_{2,t}|\gamma_{2}^{3/4}\] (B.c. \[\xi=\tfrac{1}{\log\log d}\] ) \[\lesssim-(2\gamma_{2}^{1/2}-\gamma_{2})\cdot\hat{\sigma}_{2,d}^{2 }|D_{2,t}|\gamma_{2}^{3/4}\] (D.149) \[\lesssim-\gamma_{2}^{5/4}\hat{\sigma}_{2,d}^{2}|D_{2,t}|\] (B.c. \[\gamma_{2}^{1/2}\geq\gamma_{2}\] since \[\gamma_{2}\leq 1\] )

as desired. 

The next lemma essentially implies that if at some point \(D_{4,t}\leq 0\) and \(D_{2,t}>0\), then it will never be the case that \(D_{4,t}>0\) and \(D_{2,t}>0\).

**Lemma D.19**.: _Suppose we are in the setting of Theorem 3.3, and suppose \(D_{4,t}=0\) and \(D_{2,t}\geq 0\) for some \(t\geq 0\). Then, \(\frac{d}{ds}D_{4,s}\Big{|}_{s=t}\leq 0\)._

Proof.: We can calculate that

\[\frac{d}{dt}D_{4,t}=\frac{d}{dt}\operatorname*{\mathbb{E}}_{w\sim\rho_{t}}[P_{ 4,d}(w)]=\operatorname*{\mathbb{E}}_{w\sim\rho_{t}}[P_{4,d}{}^{\prime}(w)\cdot v _{t}(w)]\] (D.150)

Recall from Eq. (C.4) that

\[P_{4,d}(t)=\frac{d^{2}+6d+8}{d^{2}-1}t^{4}-\frac{6d+12}{d^{2}-1}t^{2}+\frac{3} {d^{2}-1}\] (D.151)

meaning that

\[P_{4,d}{}^{\prime}(t)=4t^{3}\pm O\Big{(}\frac{1}{d}\Big{)}\cdot(t^{3}|+|t|)=4t^ {3}\pm O\Big{(}\frac{1}{d}\Big{)}|t|\] (D.152)Observe that for \(t\gtrsim\frac{1}{\sqrt{d}}\), \(P_{4,d}{}^{\prime}(t)>0\) and for \(t\lesssim\frac{1}{\sqrt{d}}\), it may be the case that \(P_{4,d}{}^{\prime}(t)<0\). In particular,

\[\operatorname*{\mathbb{E}}_{w\sim\rho_{t}}[P_{4,d}{}^{\prime}(w) \cdot v_{t}(w)] =\mathbb{P}(|w|\lesssim 1/\sqrt{d})\cdot\operatorname*{\mathbb{E}}_{w \sim\rho_{t}}[P_{4,d}{}^{\prime}(w)\cdot v_{t}(w)\mid|w|\lesssim 1/\sqrt{d}]\] (D.153) \[\qquad\qquad+\mathbb{P}(|w|\gtrsim 1/\sqrt{d})\operatorname*{ \mathbb{E}}_{w\sim\rho_{t}}[P_{4,d}{}^{\prime}(w)\cdot v_{t}(w)\mid|w|\gtrsim 1 /\sqrt{d}]\] (D.154) \[\leq O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d^{2}}\Big{)} +\mathbb{P}(|w|\gtrsim 1/\sqrt{d})\operatorname*{\mathbb{E}}_{w \sim\rho_{t}}[P_{4,d}{}^{\prime}(w)\cdot v_{t}(w)\mid|w|\gtrsim 1/\sqrt{d}]\] (B.c. \[|w|\lesssim\frac{1}{\sqrt{d}}\] and by Lemma D.18 ) \[\leq O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d^{2}}\Big{)} +\mathbb{P}(|w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])\] (D.155) \[\operatorname*{\mathbb{E}}_{w\sim\rho_{t}}[P_{4,d}{}^{\prime}(w) \cdot v_{t}(w)\mid|w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]]\] (D.156) \[\leq O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d^{2}}\Big{)} -\mathbb{P}(|w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])\gamma_{2}^{9/4}\cdot \gamma_{2}^{5/4}\hat{\sigma}_{2,d}^{2}|D_{2,t}|\] (B.c. \[P_{4,d}{}^{\prime}(w)\gtrsim\gamma_{2}^{9/4}\], and by Lemma D.18 ) \[\leq O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d^{2}}\Big{)} -\frac{\gamma_{2}}{2}\cdot\gamma_{2}^{14/4}\hat{\sigma}_{2,d}^{2}|D_{2,t}|\] (By Lemma D.17 ) \[<0\] (By Assumption 3.2 since \[d\] is sufficiently large) meaning that \[\frac{d}{dt}D_{4,t}<0\], as desired. 

The next lemma essentially implies that if at some point \(D_{4,t}\leq 0\) and \(D_{2,t}>0\), it will never be the case that \(D_{4,t}\leq 0\) and \(D_{2,t}<0\).

**Lemma D.20**.: _Suppose we are in the setting of Theorem 3.3, and suppose \(D_{4,t}\leq 0\) and \(D_{2,t}=0\), for some \(t\geq 0\). Then, \(\frac{d}{ds}D_{2,s}\Big{|}_{s=t}\geq 0\)._

Proof.: Recall from Eq. (C.4) that \(P_{2,d}(t)=\frac{d}{d-1}t^{2}-\frac{1}{d-1}\) meaning that \(P_{2,d}{}^{\prime}(t)=\frac{2d}{d-1}t\). Thus,

\[\frac{d}{dt}D_{2,t}=\frac{d}{dt}\operatorname*{\mathbb{E}}_{w\sim\rho_{t}}[w^{2 }]=\frac{d}{d-1}\operatorname*{\mathbb{E}}_{w\sim\rho_{t}}[2w\cdot v_{t}(w)]\] (D.157)

If \(D_{4,t}\leq 0\) and \(D_{2,t}=0\), then for any particle \(w\in[0,1]\), we can write

\[v_{t}(w) =-(1-w^{2})(P_{t}(w)+Q_{t}(w))\] (By Lemma 4.4 ) \[=-(1-w^{2})\Big{(}4\hat{\sigma}_{4,d}^{2}D_{4,t}w^{3}\pm O\Big{(} \frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}|w|\Big{)}\] (By Lemma 4.4 ) \[=(1-w^{2})\Big{(}4\hat{\sigma}_{4,d}^{2}|D_{4,t}|w^{3}\pm O\Big{(} \frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}|w|\Big{)}\] (D.158)where the last equality is because \(D_{4,t}\leq 0\). Observe that \(v_{t}(w)\geq 0\) for \(w\gtrsim\frac{1}{\sqrt{d}}\), while it may be the case that \(v_{t}(w)\leq 0\) for \(w\lesssim\frac{1}{\sqrt{d}}\). Thus,

\[\operatorname*{\mathbb{E}}_{w\sim_{\rho_{t}}}[2w\cdot v_{t}(w)] =\mathbb{P}(|w|\lesssim 1/\sqrt{d})\cdot\operatorname*{\mathbb{E}}_{w \sim_{\rho_{t}}}[2w\cdot v_{t}(w)\mid|w|\lesssim 1/\sqrt{d}]\] (D.159) \[\qquad\qquad+\mathbb{P}(|w|\gtrsim 1/\sqrt{d})\operatorname*{ \mathbb{E}}_{w\sim_{\rho_{t}}}[2w\cdot v_{t}(w)\mid|w|\lesssim 1/\sqrt{d}]\] (D.160) \[\geq-O\Big{(}\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d^{2}}\Big{)} +\mathbb{P}(|w|\gtrsim 1/\sqrt{d})\operatorname*{\mathbb{E}}_{w\sim_{ \rho_{t}}}[2w\cdot v_{t}(w)\mid|w|\gtrsim 1/\sqrt{d}]\] (By bounding Eq. (D.158) when \[|w|\lesssim\frac{1}{\sqrt{d}}\] ) \[\geq-O\Big{(}\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d^{2}}\Big{)} +\mathbb{P}(|w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])\] (D.161) \[\operatorname*{\mathbb{E}}_{w\sim_{\rho_{t}}}[2w\cdot v_{t}(w) \mid|w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]]\] (D.162) \[\gtrsim-O\Big{(}\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d^{2}} \Big{)}+\mathbb{P}(|w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])\cdot\gamma_{2} ^{9/4}\hat{\sigma}_{4,d}^{2}|D_{4,t}|\] (By Eq. (D.158)) \[\gtrsim-O\Big{(}\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d^{2}} \Big{)}-\frac{\gamma_{2}}{2}\cdot\gamma_{2}^{9/4}\hat{\sigma}_{4,d}^{2}|D_{4,t}| \text{(By Lemma D.\ref{lem:2.1.1})}\] \[\geq 0\qquad\qquad\qquad\qquad\qquad\text{(By Assumption \ref{lem:2.1.1} since $d$ is sufficiently large)}\]

as desired. 

Putting all of the previous lemmas together, we obtain the following invariant: a lower bound for \(|\Phi(w)-\Phi(w^{\prime})|\), for a large fraction of \(w,w^{\prime}\), which holds throughout Phase 3, Case 1. Note that in the proof of this invariant, we need to make use of the fact that once we enter the case where \(D_{2,t}>0\) and \(D_{4,t}<0\), we cannot leave this case. If this is not true, then we cannot use the fact that \(|\Phi(w_{t})-\Phi(w^{\prime}_{t})|\) is increasing as a function of \(t\) (Lemma D.13) since if \(D_{4,t}>0\) at any point, then \(|\Phi(w_{t})-\Phi(w^{\prime}_{t})|\) could decrease, and we would not have control over this decrease.

**Lemma D.21** (Phase 3, Case 1 Invariant).: _Suppose we are in the setting of Theorem 3.3. Let \(T_{\text{\rm R}}\) be as defined in the statement of Lemma D.7, and assume that \(D_{2,T_{2}}=0\) and \(D_{4,T_{2}}<0\). Then, for all \(t\geq T_{\text{\rm R}}\) (in particular, for all times \(t\) during Phase 3 Case 1), if \(t,\iota^{\prime}\) are sampled independently from \(\rho_{0}\),_

\[|\Phi(w_{t}(\iota))-\Phi(w_{t}(\iota^{\prime}))|\gtrsim\xi\kappa^{3}\] (D.163)

_with probability at least \(1-O(\kappa)\)._

Proof of Lemma D.21.: By Lemma D.19 and Lemma D.20, if \(D_{2,T_{2}}=0\) and \(D_{4,T_{2}}<0\), then for all \(t\geq T_{2}\), we will have \(D_{2,t}\geq 0\) and \(D_{4,t}\leq 0\). Thus, the lemma follows from Lemma D.13 and Lemma D.16, as well as the fact that \(D_{4,t}\leq 0\) for \(t\in[T_{\text{\rm R}},T_{2}]\) (meaning that the potential difference is increasing in the time interval \([T_{\text{\rm R}},T_{2}]\)). 

Finally, we show Lemma D.22 which gives a running time bound for Phase 3, Case 1.

**Lemma D.22** (Phase 3, Case 1 Summary).: _In the setting of Theorem 3.3, suppose that \(D_{2,T_{2}}=0\) and \(D_{4,T_{2}}<0\) (i.e. Phase 3, Case 1 holds). Then, the total amount of time \(t\geq T_{2}\) such that \(L(\rho_{t})\geq\frac{1}{2}(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) \epsilon^{2}\) is at most \(O(\frac{1}{\hat{\sigma}_{4,d}^{2}\gamma_{2}^{5}\xi\kappa^{6}}\log(\frac{\gamma_ {2}}{\epsilon}))\)._

Proof of Lemma D.22.: Suppose \(t\geq T_{2}\) and assume that \(D_{2,T_{2}}=0\) and \(D_{4,t}<0\) (i.e. Phase 3, Case 1 occurs). First, suppose \(|D_{4,t}|\leq\xi|D_{2,t}|\) where \(\xi=\frac{1}{\log\log d}\). Then, by Lemma D.18, if \(|w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\), we have

\[|v_{t}(w)|\gtrsim\gamma_{2}^{5/4}\hat{\sigma}_{2,d}^{2}|D_{2,t}|\] (D.164)(note that the lemma is stated for \(w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/4}]\), but holds for \(|w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/4}]\) since \(v_{t}(w)\) is an odd function). Therefore,

\[\mathop{\mathbb{E}}_{w\sim\rho_{t}}|v_{t}(w)|^{2} \gtrsim\mathop{\mathbb{P}}_{w\sim\rho_{t}}(|w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])\cdot\mathop{\mathbb{E}}_{w\sim\rho_{t}}[|v_{t}(w)|^{2}\ |\ |w|\in[ \gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]]\] (D.165) \[\gtrsim\frac{\gamma_{2}}{2}\cdot\mathop{\mathbb{E}}_{w\sim\rho_{ t}}[|v_{t}(w)|^{2}\ |\ |w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]]\] (By Lemma D.17) \[\gtrsim\frac{\gamma_{2}}{2}\cdot\gamma_{2}^{5/2}\hat{\sigma}_{2,d }^{4}|D_{2,t}|^{2}\] (By Eq. (D.164)) \[\gtrsim\gamma_{2}^{7/2}\hat{\sigma}_{2,d}^{4}|D_{2,t}|^{2}\] (D.166) \[\gtrsim\gamma_{2}^{7/2}\hat{\sigma}_{2,d}^{2}\cdot(\hat{\sigma}_ {2,d}^{2}|D_{2,t}|^{2}+\hat{\sigma}_{4,d}^{2}|D_{4,t}|^{2})\] (B.c. \[\hat{\sigma}_{4,d}^{2}\lesssim\hat{\sigma}_{2,d}^{2}\] (Assumption 3.2) and \[|D_{4,t}|\leq\xi|D_{2,t}|\]) \[\gtrsim\gamma_{2}^{7/2}\hat{\sigma}_{2,d}^{2}L(\rho_{t})\] (D.167)

Thus, by Lemma D.39, for any time \(t\) during Phase 3, Case 1 such that \(|D_{4,t}|\leq\xi|D_{2,t}|\), we have

\[\frac{dL(\rho_{t})}{dt}\lesssim-\gamma_{2}^{7/2}\hat{\sigma}_{2,d }^{2}L(\rho_{t})\] (D.168)

Let \(T_{*,\epsilon}\) be as in Theorem 3.3, and define

\[I_{|D_{4}|\leq\xi|D_{2}|}=\{t\leq T_{*,\epsilon}\text{ and }t\text{ in Phase 3, Case 1 }|\ |D_{4,t}|\leq\xi|D_{2,t}|\}\] (D.169)

Then, by Eq. (D.168),

\[\frac{dL(\rho_{t})}{dt}\lesssim-\gamma_{2}^{7/2}\hat{\sigma}_{2,d }^{2}L(\rho_{t})1(t\in I_{|D_{4}|\leq\xi|D_{2}|})\] (D.170)

Thus, by Gronwall's inequality (Fact I.13),

\[\frac{L(\rho_{T_{*,\epsilon}})}{L(\rho_{0})} \leq\exp\Big{(}-C\int_{T_{2}}^{T_{*,\epsilon}}\gamma_{2}^{7/2} \hat{\sigma}_{2,d}^{2}1(t\in I_{|D_{4}|\leq\xi|D_{2}|})dt\Big{)}\] (D.171) \[=\exp\Big{(}-C\gamma_{2}^{7/2}\hat{\sigma}_{2,d}^{2}\int_{I_{|D_ {4}|\leq\xi|D_{2}|}}1dt\Big{)}\] (D.172)

Since \(L(\rho_{0})\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\gamma_{2}^{2}\) by Lemma D.8 and \(L(\rho_{T_{*,\epsilon}})\gtrsim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2} )\epsilon^{2}\), rearranging gives

\[\int_{I_{|D_{4}|\leq\xi|D_{2}|}}1dt\lesssim\frac{1}{\hat{\sigma}_{2,d}^{2} \gamma_{2}^{7/2}}\log\Big{(}\frac{\gamma_{2}^{2}}{\epsilon^{2}}\Big{)}\] (D.173)

Now, suppose \(t\) is such that \(|D_{4,t}|\geq\xi|D_{2,t}|\). Let us lower bound the average velocity for \(w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/4}]\) first (then, we can conclude the average velocity is large by Lemma D.17). By Lemma 4.4,

\[|v_{t}(w)|=(1-w^{2})|P_{t}(w)+Q_{t}(w)|\] (D.174)

and we can write

\[P_{t}(w)=2\hat{\sigma}_{2,d}^{2}D_{2,t}w+4\hat{\sigma}_{4,d}^{2}D _{4,t}w^{3}=4\hat{\sigma}_{4,d}^{2}D_{4,t}w(w-r)(w+r)\] (D.175)

where \(r=\sqrt{-\frac{\hat{\sigma}_{2,d}^{2}D_{2,t}}{2\hat{\sigma}_{4,d}^{2}D_{4,t}}}\) is the positive root of \(P_{t}\). For \(|w|\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\), we have

\[1-w^{2}\geq 1-(1-\gamma_{2}^{1/2})^{2}=2\gamma_{2}^{1/2}-\gamma_{2} \geq\gamma_{2}^{1/2}.\] (D.176)Additionally, \(|w|\geq\gamma_{2}^{3/4}\) and \(|w+r|\geq\gamma_{2}^{3/4}\). Thus, for \(w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\), we have

\[|v_{t}(w)|^{2} \gtrsim(1-w^{2})^{2}\Big{(}P_{t}(w)+Q_{t}(w)\Big{)}^{2}\] (D.177) \[\gtrsim\gamma_{2}\Big{(}P_{t}(w)+Q_{t}(w)\Big{)}^{2}\] (B.c. \[1-w^{2}\geq\gamma_{2}^{1/2}\] ) \[\gtrsim\gamma_{2}\cdot\Big{(}P_{t}(w)^{2}-2Q_{t}(w)^{2}\Big{)} \qquad\text{(B.c. $(a+b)^{2}\leq 2a^{2}+2b^{2}$, so $a^{2}-2b^{2}\leq 2(a+b)^{2}$)}\] \[\gtrsim\gamma_{2}\cdot\Big{(}P_{t}(w)^{2}-O\Big{(}\frac{\hat{ \sigma}_{2,d}^{4}D_{2,t}^{2}+\hat{\sigma}_{4,d}^{4}D_{4,t}^{2}}{d^{2}}\Big{)}w^ {2}\Big{)}\] (By Lemma 4.4 ) \[\gtrsim\gamma_{2}\cdot\Big{(}P_{t}(w)^{2}-O\Big{(}\frac{(\hat{ \sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})L(\rho_{t})}{d^{2}}\Big{)}w^{2}\Big{)}\] (By Lemma 4.2 ) \[\gtrsim\gamma_{2}\cdot\Big{(}\hat{\sigma}_{4,d}^{4}D_{4,t}^{2}w^{2 }(w+r)^{2}(w-r)^{2}-O\Big{(}\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{ 2})L(\rho_{t})}{d^{2}}\Big{)}w^{2}\Big{)}\] (By factorization of \[P_{t}(w)\] above) \[\gtrsim\gamma_{2}\cdot\Big{(}\hat{\sigma}_{4,d}^{4}D_{4,t}^{2} \gamma_{2}^{3}(w-r)^{2}-O\Big{(}\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d }^{2})L(\rho_{t})}{d^{2}}\Big{)}w^{2}\Big{)}\] (D.178)

where the last inequality is by the discussion above Eq. (D.177). Now, in order to take the expectation over \(w\sim\rho_{t}\) (conditioned on \(w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\)), we first compute \(\mathbb{E}_{w\sim\rho_{t}}[(w-r)^{2}\mid w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{ 1/2}]]\):

\[\mathbb{E}_{w\sim\rho_{t}}[(w-r)^{2}\mid w\sim[\gamma_{2}^{3/4},1 -\gamma_{2}^{1/2}]] \geq\operatorname*{Var}_{w\sim\rho_{t}}(w\mid w\sim[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])\] (D.179) \[\gtrsim\mathbb{E}_{w,w^{\prime}\sim\rho_{t}}[(w-w^{\prime})^{2} \mid w,w^{\prime}\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]]\] (D.180)

where the last inequality is by Proposition D.40. By Lemma D.21,

\[\mathbb{P}_{w,w^{\prime}\sim\rho_{t}}(|\Phi(w)-\Phi(w^{\prime})|\gtrsim\xi \kappa^{3})\geq 1-O(\kappa)\] (D.181)

By Lemma D.14, if \(w,w^{\prime}\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\), then

\[|\Phi(w)-\Phi(w^{\prime})|\leq\frac{1}{\gamma_{2}^{3/2}}|w-w^{\prime}|\] (D.182)

Thus, combining the previous two equations gives

\[\mathbb{P}_{w,w^{\prime}\sim\rho_{t}}(|w-w^{\prime}|<\gamma_{2}^{ 3/2}\xi\kappa^{3} \mid w,w^{\prime}\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])\] (D.183) \[\lesssim\mathbb{P}_{w,w^{\prime}\sim\rho_{t}}(|\Phi(w)-\Phi(w^{ \prime})|<\xi\kappa^{3}\mid w,w^{\prime}\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2 }])\] (D.184) \[\lesssim\frac{\mathbb{P}_{w,w^{\prime}\sim\rho_{t}}(|\Phi(w)-\Phi (w^{\prime})|<\xi\kappa^{3}\text{ and }w,w^{\prime}\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])}{ \mathbb{P}_{w,w^{\prime}\sim\rho_{t}}(w,w^{\prime}\in[\gamma_{2}^{3/4},1- \gamma_{2}^{1/2}])}\] (D.185) \[\lesssim\frac{\kappa}{\mathbb{P}_{w,w^{\prime}\sim\rho_{t}}(w,w^ {\prime}\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])}\] (By Lemma D.21) \[\lesssim\frac{\kappa}{\gamma_{2}}\] (By Lemma D.17)

and therefore, by Assumption 3.2, since \(\kappa/\gamma_{2}\lesssim\frac{1}{\gamma_{2}\log\log d}\), this is at most \(\frac{1}{2}\), since \(d\) is sufficiently large. Thus,

\[\mathbb{E}_{w,w^{\prime}\sim\rho_{t}}[(w-w^{\prime})^{2} \mid w,w^{\prime}\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]]\] (D.186) \[\gtrsim\mathbb{P}_{w,w^{\prime}\sim\rho_{t}}(|w-w^{\prime}|\geq \gamma_{2}^{3/2}\xi\kappa^{3}\mid w,w^{\prime}\in[\gamma_{2}^{3/4},1-\gamma_{2}^{ 1/2}])\cdot\gamma_{2}^{3}\xi^{2}\kappa^{6}\] (D.187) \[\gtrsim\gamma_{2}^{3}\xi^{2}\kappa^{6}\] (D.188)where the last inequality is because \(\mathbb{P}_{w,w^{\prime}\sim\rho_{t}}(|w-w^{\prime}|<\gamma_{2}^{3/2}\xi\kappa^{3} \mid w,w^{\prime}\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])\leq\frac{1}{2}\). By Eq. (D.178), we have

\[\mathbb{E}_{w\sim\rho_{t}}[|v_{t}(w)|^{2}\mid w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]] \gtrsim\gamma_{2}\cdot\Big{(}\hat{\sigma}_{4,d}^{4}D_{4,t}^{2} \gamma_{2}^{3}\operatorname*{\mathbb{E}}_{w\sim\rho_{t}}\Big{[}(w-r)^{2}\mid w \in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}]\Big{]}\] (D.189) \[\qquad\qquad\qquad-O\Big{(}\frac{(\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2})L(\rho_{t})}{d^{2}}\Big{)}\Big{)}\] (D.190) \[\gtrsim\gamma_{2}\cdot\Big{(}\hat{\sigma}_{4,d}^{4}D_{4,t}^{2} \gamma_{2}^{3}\cdot\gamma_{2}^{3}\xi^{2}\kappa^{6}-O\Big{(}\frac{(\hat{\sigma }_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})L(\rho_{t})}{d}\Big{)}\Big{)}\] (By Eq. (D.180) and Eq. (D.188))

Since \(\mathbb{P}_{w\sim\rho_{t}}(w\in[\gamma_{2}^{3/4},1-\gamma_{2}^{1/2}])\geq\frac{ \gamma_{2}}{4}\) by Lemma D.17 and the symmetry of \(\rho_{t}\), this implies that

\[\operatorname*{\mathbb{E}}_{w\sim\rho_{t}}|v_{t}(w)|^{2} \gtrsim\gamma_{2}^{2}\cdot\Big{(}\hat{\sigma}_{4,d}^{4}D_{4,t}^{2 }\gamma_{2}^{6}\xi^{2}\kappa^{6}-O\Big{(}\frac{(\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2})L(\rho_{t})}{d}\Big{)}\Big{)}\] (D.191) \[\gtrsim\gamma_{2}^{2}\cdot\Big{(}\hat{\sigma}_{4,d}^{2}\xi^{2}L( \rho_{t})\gamma_{2}^{6}\xi^{2}\kappa^{6}-O\Big{(}\frac{(\hat{\sigma}_{2,d}^{2} +\hat{\sigma}_{4,d}^{2})L(\rho_{t})}{d}\Big{)}\Big{)}\] (B.c. \[\hat{\sigma}_{4,d}^{2} \gtrsim\hat{\sigma}_{2,d}^{2}\] (Assumption 3.2) \[\geq\xi|D_{2,t}|\] implies \[\hat{\sigma}_{4,d}^{2}D_{4,t}^{2}\gtrsim\xi^{2}L(\rho_{t})\] (D.192) \[\gtrsim\hat{\sigma}_{4,d}^{2}\gamma_{2}^{8}\xi^{4}\kappa^{6}L( \rho_{t})\] (Because \[d\geq c_{3}\] by Assumption 3.2 )

Thus, by Lemma D.39, \(\frac{dL(\rho_{t})}{dt}\lesssim-\hat{\sigma}_{4,d}^{2}\gamma_{2}^{8}\xi^{4} \kappa^{6}L(\rho_{t})\). By a similar argument as for the subcase where \(|D_{4,t}|\leq\xi|D_{2,t}|\), since \(L(\rho_{0})\lesssim\hat{\sigma}_{2,d}^{2}\gamma_{2}^{2}\) (by Lemma D.8), if we define

\[I_{|D_{4}|\geq\xi|D_{2}|}=\{t\leq T_{*,\epsilon}\text{ and }t\text{ in Phase \ref{phase:2}, Case 1 }|\ |D_{4,t}|\geq\xi|D_{2,t}|\}\] (D.193)

then

\[\int_{I_{|D_{4}|\geq\xi|D_{2}|}}1dt\lesssim\frac{1}{\hat{\sigma}_{4,d}^{2} \gamma_{2}^{8}\xi^{4}\kappa^{6}}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\] (D.194)

since \(L(\rho_{T_{*,\epsilon}})\gtrsim\hat{\sigma}_{2,d}^{2}\epsilon^{2}\). Combining Eq. (D.173) and Eq. (D.194), we find that the total time spent in Phase 3, Case 1 is at most \(O(\frac{1}{\hat{\sigma}_{4,d}^{2}\gamma_{2}^{8}\xi^{4}\kappa^{6}}\log(\frac{ \gamma_{2}}{\epsilon}))\), as desired. 

### Phase 3, Case 2

Next, we analyze Phase 3, Case 2, where \(D_{4,T_{2}}=0\) and \(D_{2,T_{2}}<0\). In this case, as we will show, for all \(t\geq T_{2}\), we will have \(D_{4,t}\geq 0\) and \(D_{2,t}\leq 0\) (Lemma D.28). We will first give an outline of our analysis in this case -- our goal is again to show that either \(|D_{2,t}|\leq\epsilon\) or \(|D_{4,t}|\leq\epsilon\) within \(\frac{1}{\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}}\cdot\text{poly}(\frac{ \log\log d}{\epsilon})\) time after the start of Phase 3, Case 2.

**Proof Strategy for Phase 3, Case 2** We again show that \(|v(w)\) is large on average and then use Lemma D.39 to show that the loss decreases quickly. However, our argument to show that \(|v(w)\) is large on average is significantly different from Phase 3, Case 1. We can approximately write

\[v_{t}(w) =-(1-w^{2})(P_{t}(w)+Q_{t}(w))\] (D.195) \[\approx-(1-w^{2})P_{t}(w)\] (D.196) \[\approx(1-w^{2})w(2\hat{\sigma}_{2,d}^{2}D_{2,t}+4\hat{\sigma}_{4,d}^{2}D_{4,t}w^{2})\] (D.197) \[\approx(1-w^{2})w(2\hat{\sigma}_{2,d}^{2}|D_{2,t}|-4\hat{\sigma}_ {4,d}^{2}|D_{4,t}|w^{2})\] (B.c. \[D_{2,t}\leq 0\text{ and }D_{4,t}\geq 0\] ) \[\approx-(1-w^{2})w(4\hat{\sigma}_{4,d}^{2}|D_{4,t}|w^{2}-2\hat{ \sigma}_{2,d}^{2}|D_{2,t}|)\] (D.198)

Letting \(r=\sqrt{\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{2\hat{\sigma}_{4,d}^{2}|D_{4,t}|}}\) be the positive root of \(P_{t}(w)\), we obtain

\[v_{t}(w)\approx-4\hat{\sigma}_{4,d}^{2}|D_{4,t}|(w-r)(w+r)w(1-w^{2})\] (D.199)This time, \(v_{t}(w)\) has a negative slope at \(r\), meaning that the particles are attracted towards \(r\), and we cannot use the same argument as in Phase 3, Case 1.

Instead, the key point of our argument in this case is the following. Recall that \(D_{4,t}\geq 0\) and \(D_{2,t}\leq 0\) for all \(t\geq T_{2}\) -- these roughly imply that \(\mathbb{E}_{w\sim\rho_{t}}[w^{4}]\geq\gamma_{4}\) and \(\mathbb{E}_{w\sim\rho_{t}}[w^{2}]\leq\gamma_{2}\). By Assumption 3.2, we have \(\gamma_{4}\geq 1.1\gamma_{2}^{2}\) -- in other words, \(\mathbb{E}_{w\sim\rho_{t}}[w^{4}]\geq 1.1(\mathbb{E}_{w\sim\rho_{t}}[w^{2}])^{2}\). From this observation, we know that a significant fraction of the particles must be far from \(r\) -- otherwise, if all of the particles are concentrated at \(r\), then we would have \(\mathbb{E}_{w\sim\rho_{t}}[w^{4}]\approx(\mathbb{E}_{w\sim\rho_{t}}[w^{2}])^{2}\), which would give a contradiction.

One complication in showing that the velocity is large is that, a priori, nearly all of the particles could be close to \(0\) or \(1\) -- their velocities could be small even if they are far from \(r\). We make use of the potential function \(\Phi\) defined in the analysis of Phase 3, Case 1 to avoid this issue. In this case, it turns out that for any two particles \(w,w^{\prime}\), their potential difference \(|\Phi(w)-\Phi(w^{\prime})|\) is decreasing (Lemma D.23). We can use this to show that a \(1-O(\gamma_{4})\) fraction of particles \(w\) with initializations in \([t_{\text{L}},t_{\text{M}}]\) (where \(t_{\text{M}}\) is defined later) cannot become too close to \(0\) or \(1\) in Lemma D.24 and Lemma D.25. Intuitively, this is because if \(|\Phi(w)-\Phi(w^{\prime})|\) is bounded by \(\log B\) for two particles \(w,w^{\prime}>0\), and \(C\) is a sufficiently large constant, then \(w\leq(\frac{1}{\log\log d})^{C}\) roughly implies that \(w^{\prime}\lesssim B(\frac{1}{\log\log d})^{C}\), simply by the definition \(\Phi(w)=\log(\frac{w}{\sqrt{1-w^{2}}})\) together with some algebraic manipulations. Moreover, if all of the particles \(w^{\prime}\) in this group are at most \(B(\frac{1}{\log\log d})^{C}\), then this implies that \(D_{2}\) has a very large negative value, and this implies that all particles \(w\) have a positive velocity (as argued in Lemma D.25), meaning that the particles \(w\) in this group cannot become smaller than \(B(\frac{1}{\log\log d})^{C}\). We can show by a similar argument that the particles having initializations in \([t_{\text{L}},t_{\text{M}}]\) cannot become too close to \(1\).

As long as the positive root \(r\) of \(P_{t}(w)\) is at least \(\frac{1}{2}\), this group of particles will have a large velocity as argued in Lemma D.29, since a significant fraction of these particles must be at most \(\frac{1}{3}\). We encounter a potential issue when the positive root \(r\) is at most \(\frac{1}{2}\): it is possible that the particles initialized in \([t_{\text{L}},t_{\text{M}}]\) get stuck at \(r\). We mentioned above that by the constraints \(D_{2,t}<0\) and \(D_{4,t}>0\), there must always be some mass away from \(r\). However, this mass may only be a \(\gamma_{4}^{5/4}\) fraction of the total mass, while the particles initialized at \([t_{\text{L}},t_{\text{M}}]\) only have a \(1-O(\gamma_{4})\) fraction of the total mass. Thus, once \(r\leq\frac{1}{2}\), it is possible for the mass located far away from \(r\) to be disjoint from the set of particles initialized at \([t_{\text{L}},t_{\text{M}}]\). To resolve this, we argue in Lemma D.31 that the particles larger than \(w_{t}(t_{\text{M}})\) (which may be close to \(1\)) will move away from \(1\) at an exponentially fast rate while \(r\leq\frac{1}{2}\). Once these particles, which are larger than \(w_{t}(t_{\text{M}})\), are sufficiently far away from \(1\), they will also contribute a significant amount to the velocity. Thus, we can argue as discussed above that there must be a significant fraction of particles away from \(r\) due to the constraints \(D_{2,t}\leq 0\) and \(D_{4,t}\geq 0\) (Lemma D.32).

We now begin the formal proof. We first show that as long as \(D_{4,t}\geq 0\), the potential difference between any two particles \(w,w^{\prime}\) is increasing. In particular, this holds during Phase 3, Case 2 (but note that this lemma is no longer applicable during Phase 1 or Phase 2).

**Lemma D.23** (\(\Phi(w)\) and \(\Phi(w^{\prime})\) Move Closer in Phase 3 Case 2).: _Suppose we are in the setting of Theorem 3.3, and suppose \(D_{4,t}\geq 0\) for all \(t\) in some time interval \([t_{0},t_{1}]\). Then, for all \(\iota,\iota^{\prime}\),_

\[\left|\Phi(w_{t}(\iota))-\Phi(w_{t}(\iota^{\prime}))\right|\] (D.200)

_is decreasing as a function of \(t\) on the interval \([t_{0},t_{1}]\)._

Proof of Lemma D.23.: We mostly follow the proof of Lemma D.13. Suppose \(\iota,\iota^{\prime}\in[0,1]\) such that \(\iota\leq\iota^{\prime}\). Then, \(w_{t}(\iota^{\prime})\geq w_{t}(\iota)\) for all times \(t\) (by Proposition D.37), and since \(\Phi\) is an increasing function, \(\Phi(w_{t}(\iota^{\prime}))\geq\Phi(w_{t}(\iota))\) for all times \(t\). Thus, it suffices to show that \(\Phi(w_{t}(\iota^{\prime}))-\Phi(w_{t}(\iota))\) is decreasing. For convenience, we write \(w_{t}^{\prime}=w_{t}(\iota^{\prime})\) and \(w_{t}=w_{t}(\iota)\). We do a computation similar to that in the proof of Lemma D.13:

\[\frac{d}{dt}(\Phi(w^{\prime}_{t})-\Phi(w_{t})) =\Big{(}-2\hat{\sigma}_{2,d}^{2}D_{2,t}-4\hat{\sigma}_{4,d}^{2}D_{4, t}(w^{\prime}_{t})^{2}-{\lambda_{d}}^{(1)}-{\lambda_{d}}^{(3)}(w^{\prime}_{t})^{2} \Big{)}\] (D.201) \[\qquad-\Big{(}-2\hat{\sigma}_{2,d}^{2}D_{2,t}-4\hat{\sigma}_{4,d}^ {2}D_{4,t}w_{t}^{2}-{\lambda_{d}}^{(1)}-{\lambda_{d}}^{(3)}w_{t}^{2}\Big{)}\] (By Lemma D.12) \[=-4\hat{\sigma}_{4,d}^{2}D_{4,t}((w^{\prime}_{t})^{2}-w_{t}^{2})-{ \lambda_{d}}^{(3)}((w^{\prime}_{t})^{2}-w_{t}^{2})\] (D.202)

According to the statement of Lemma 4.4, \({\lambda_{d}}^{(3)}=4\hat{\sigma}_{4,d}^{2}D_{4,t}\cdot\frac{6d+9}{d^{2}-1}\), meaning that

\[\frac{d}{dt}(\Phi(w^{\prime}_{t})-\Phi(w_{t})) =-4\hat{\sigma}_{4,d}^{2}D_{4,t}((w^{\prime}_{t})^{2}-w_{t}^{2})- 4\hat{\sigma}_{4,d}^{2}D_{4,t}\cdot\frac{6d+9}{d^{2}-1}((w^{\prime}_{t})^{2}- w_{t}^{2})\] (D.203) \[=-4\hat{\sigma}_{4,d}^{2}D_{4,t}\cdot\Big{(}1-\frac{6d+9}{d^{2}- 1}\Big{)}\cdot((w^{\prime}_{t})^{2}-w_{t}^{2})\] (D.204) \[<0\qquad\quad\text{(B.c. $D_{4,t}\geq 0$ and $w^{\prime}_{t}>w_{t}>0$, and $d\geq c_{3}$ (Assumption \ref{eq:2}))}\]

Thus, on any interval \([t_{0},t_{1}]\) where \(D_{4,t}\geq 0\), \(\Phi(w^{\prime}_{t})-\Phi(w_{t})\) is decreasing, as desired. 

In the next lemma, we show an upper bound on \(|\Phi(w)-\Phi(w^{\prime})|\), at the beginning of Phase 3, Case 2, for particles \(w,w^{\prime}\) which are respectively initialized at \(\iota,\iota^{\prime}\in[t_{\mathsf{L}},t_{\mathsf{M}}]\). Here \(t_{\mathsf{M}}\) is defined in the statement/proof of the following lemma.

**Lemma D.24**.: _Suppose we are in the setting of Theorem 3.3. Assume that \(D_{4,T_{2}}=0\) and \(D_{2,T_{2}}<0\), i.e. we are in Phase 3, Case 2. Then, there exists \(\iota_{\mathsf{M}}\in[t_{\mathsf{L}},\iota_{\mathsf{R}}]\) such that \(\mathbb{P}_{\iota\sim\rho_{0}}(|\iota|>\iota_{\mathsf{M}})\lesssim\gamma_{4}\) and for all \(\iota,\iota^{\prime}\in[t_{\mathsf{L}},\iota_{\mathsf{M}}]\),_

\[\Big{|}\Phi(w_{T_{2}}(\iota))-\Phi(w_{T_{2}}(\iota^{\prime}))\Big{|}\leq 2 \log\frac{1}{\kappa}+\log\frac{1}{\xi}+C\] (D.205)

_for some universal constant \(C\)._

Proof of Lemma D.24.: Since \(D_{4,T_{2}}=0\), we have \(\mathbb{E}_{w\sim\rho_{T_{2}}}[P_{4,d}(w)]=\gamma_{4}\). Thus, by Eq. (C.4),

\[\frac{d^{2}+6d+8}{d^{2}-1}\mathop{\mathbb{E}}_{w\sim\rho_{T_{2}}}[w^{4}]-\frac {6d+12}{d^{2}-1}\mathop{\mathbb{E}}_{w\sim\rho_{T_{2}}}[w^{2}]+\frac{3}{d^{2}- 1}=\gamma_{4}\] (D.206)

Since \(|w_{t}(\iota)|\leq 1\) for all \(t\geq 0\) and all \(\iota\in[-1,1]\), the left-hand side of the above equation is \(\mathbb{E}_{w\sim\rho_{t}}[w^{4}]\pm O(\frac{1}{d})\), and rearranging gives

\[\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{4}]\leq\gamma_{4}+O\Big{(}\frac{1}{d} \Big{)}\] (D.207)

By Markov's inequality,

\[\mathbb{P}_{w\sim\rho_{T_{2}}}(|w|\geq 1/2)\lesssim\gamma_{4}+O\Big{(}\frac{1}{d} \Big{)}\lesssim\gamma_{4}\] (D.208)

where the second inequality is because \(d\geq c_{3}\) by Assumption 3.2. Let \(\iota_{\mathsf{0}}\) be such that \(w_{T_{2}}(\iota_{0})=\frac{1}{2}\). Let \(\iota_{\mathsf{M}}=\min(\iota_{0},\iota_{\mathsf{R}})\). Then, \(\iota_{\mathsf{M}}\in[t_{\mathsf{L}},\iota_{\mathsf{R}}]\), and \(\mathbb{P}_{\iota\sim\rho_{0}}(|\iota|>\iota_{\mathsf{M}})\lesssim\gamma_{4}\), because \(\mathbb{P}_{\iota\sim\rho_{0}}(|\iota|>\iota_{\mathsf{0}})\lesssim\gamma_{4}\) and \(\mathbb{P}_{\iota\sim\rho_{0}}(|\iota|>\iota_{\mathsf{R}})\lesssim O(\kappa)\) by Proposition D.6, meaning that \(\mathbb{P}_{\iota\sim\rho_{0}}(|\iota|>\iota_{\mathsf{M}})\lesssim\max(\gamma_{ 4},\kappa)\lesssim\gamma_{4}\) (where the last inequality is because \(d\geq c_{3}\) by Assumption 3.2).

Now, let \(\iota\in[t_{\mathsf{L}},\iota_{\mathsf{M}}]\). Observe that on the interval \([0,T_{2}]\), for all \(\iota\), \(w_{t}(\iota)\) is strictly increasing as a function of \(t\), for all \(\iota\) (see Lemma D.9). Thus, if \(T_{\mathsf{R}}\) is as defined in Lemma D.7, then for all \(\iota\in[t_{\mathsf{L}},\iota_{\mathsf{R}}]\), \(|w_{T_{\mathsf{R}}}(\iota)|\gtrsim\xi\kappa^{2}\). In particular, for all \(\iota\in[t_{\mathsf{L}},\iota_{\mathsf{M}}]\), we can write \(w_{T_{2}}(\iota)\geq C\xi\kappa^{2}\) for some universal constant \(C\). Additionally, for \(\iota\leq\iota_{\mathsf{M}}\), we have \(w_{T_{2}}(\iota)\leq\frac{1}{2}\) by Proposition D.37. Thus, since \(\Phi(w)\) is increasing in \(w\), for \(\iota\in[t_{\mathsf{L}},\iota_{\mathsf{M}}]\) we have

\[\Phi(w_{T_{2}}(\iota))\geq\Phi(C\xi\kappa^{2})=\log\Big{(}\frac{C\xi\kappa^{2}}{ \sqrt{1-C^{2}\xi^{2}\kappa^{4}}}\Big{)}\geq\log(C\xi\kappa^{2})=\log C+\log \xi+2\log\kappa\] (D.209)

[MISSING_PAGE_FAIL:49]

\[D_{4,t} =\mathop{\mathbb{E}}_{w\sim\rho_{t}}\Big{[}\frac{d^{2}+6d+8}{d^{2}-1 }w^{4}-\frac{6d+12}{d^{2}-1}w^{2}+\frac{3}{d^{2}-1}\Big{]}-\gamma_{4}\] (By definition of

\[P_{4,d}\]

, Eq. (  C 4 ) \[=\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{4}]-\gamma_{4}\pm O\Big{(} \frac{1}{d}\Big{)}\] (D.216) \[=\mathop{\mathbb{P}}_{\iota\sim\rho_{0}}(|\iota|\leq\iota_{ \text{M}})\mathop{\mathbb{E}}[w_{t}(\iota)^{4}\mid|\iota|\leq\iota_{\text{M}}] +\mathop{\mathbb{P}}_{\iota\sim\rho_{0}}(|\iota|>\iota_{\text{M}})\mathop{ \mathbb{E}}[w_{t}(\iota)^{4}\mid|\iota|>\iota_{\text{M}}]-\gamma_{4}\pm O\Big{(} \frac{1}{d}\Big{)}\] (D.217) \[\leq O(\xi^{4})+O(\gamma_{4})-\gamma_{4}\pm O\Big{(}\frac{1}{d} \Big{)}\] (B.c. \[w_{t}(\iota)\lesssim\xi\] for

\[\iota\in[\iota_{\text{L}},\iota_{\text{M}}]\]

 and

\[\mathop{\mathbb{P}}(\iota>\iota_{\text{M}})\lesssim\gamma_{4}\]

) \[\leq O(\xi^{4}+\gamma_{4})\] (D.218) \[\lesssim\gamma_{4}\] (B.c. \[d\geq c_{3}\]

, by Assumption 3.2 )

To complete the proof, we will show that if these bounds on \(D_{2,t}\) and \(D_{4,t}\) hold, then for any \(w\), \(v_{t}(w)\geq 0\). By Lemma 4.4, it suffices to show that \(P_{t}(w)+Q_{t}(w)\leq 0\) for \(w\geq 0\), where \(P_{t}\) and \(Q_{t}\) are as defined in Lemma 4.4. If \(C_{1}\) and \(C_{2}\) are two appropriately chosen positive universal constants, then by the above upper bounds on \(D_{2,t}\) and \(D_{4,t}\),

\[P_{t}(w)+Q_{t}(w) =2\hat{\sigma}_{2,d}^{2}D_{2,t}w+4\hat{\sigma}_{4,d}^{2}D_{4,t}w^ {3}+{\lambda_{d}}^{(1)}w+{\lambda_{d}}^{(3)}w^{3}\] (D.219) \[\leq-C_{1}\hat{\sigma}_{2,d}^{2}\gamma_{2}w+C_{2}\hat{\sigma}_{4, d}^{2}\cdot\gamma_{4}w^{3}+O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{ \sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}w\] (D.220) \[\qquad+O\Big{(}\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}w^ {3}\] (By definition of

\[\lambda_{d}\]

\[{}^{(1)}\]

,

\[\lambda_{d}\]

\[{}^{(3)}\]

 in Lemma 4.4 and above bounds on

\[D_{2,t},D_{4,t}\]

) \[\leq-C_{1}^{\prime}\hat{\sigma}_{2,d}^{2}\gamma_{2}w+O\Big{(} \frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d} \Big{)}|w|\] (B.c. \[\gamma_{4}\lesssim\gamma_{2}^{2}\leq\gamma_{2}\]

 and

\[\hat{\sigma}_{4,d}^{2}\lesssim\hat{\sigma}_{2,d}^{2}\]

, with new constant

\[C_{1}^{\prime}\]

) \[\lesssim-\hat{\sigma}_{2,d}^{2}\gamma_{2}w\] (B.c. \[d\geq c_{3}\]

, Assumption 3.2 )

In summary, if \(w_{t}(\iota)=\xi^{2}\kappa^{2}\) for some \(\iota\in[\iota_{\text{L}},\iota_{\text{M}}]\), then \(v_{t}(w)>0\) for all \(w\in[0,1]\). In addition, at time \(T_{2}\), we have \(w_{t}(\iota)\gtrsim\xi\kappa^{2}\) for all \(\iota>\iota_{\text{L}}\) by Lemma D.9 and Lemma D.7. Thus, for all \(\iota\geq\iota_{\text{L}}\), \(w_{t}(\iota)\) will never go below \(\xi^{2}\kappa^{2}\) for \(t\in[T_{2},s]\), as desired.

To finish the proof, we will show the upper bound on \(w_{t}(\iota_{\text{M}})\). Suppose \(1-w_{t}(\iota_{\text{M}})\leq\kappa^{4}\xi^{3}\). Then,

\[\Phi(w_{t}(\iota_{\text{M}})) =\log\Big{(}\frac{w_{t}(\iota_{\text{M}})}{\sqrt{1-w_{t}(\iota_{ \text{M}})^{2}}}\Big{)}\] (D.221) \[\geq\log\frac{1}{2}-\frac{1}{2}\log(1-w_{t}(\iota_{\text{M}})^{2})\] (B.c. \[w_{t}(\iota_{\text{M}})\geq\frac{1}{2}\] ) \[\geq\log\frac{1}{2}-\frac{1}{2}\log(1-(1-\kappa^{4}\xi^{3})^{2})\] (D.222) \[=\log\frac{1}{2}-\frac{1}{2}\log(2\kappa^{4}\xi^{3}-\kappa^{8}\xi ^{6})\] (D.223) \[\geq\log\frac{1}{2}-\frac{1}{2}\log(\kappa^{4}\xi^{3})\] (B.c. \[\kappa,\xi\leq 1\] and

\[\kappa^{4}\xi^{3}-(\kappa^{4}\xi^{3})^{2}\geq 0\] ) \[=\log\frac{1}{2}+2\log\frac{1}{\kappa}+\frac{3}{2}\log\frac{1}{\xi}\] (D.224)

Thus, for any \(\iota\in[\iota_{\text{L}},\iota_{\text{M}}]\),

\[\Phi(w_{t}(\iota))\geq C+\frac{1}{2}\log\frac{1}{\xi}\] (D.225)for some universal constant \(C\). In other words, letting \(w_{t}:=w_{t}(\iota)\) and rearranging using the definition of \(\Phi\), we obtain

\[\frac{w_{t}}{\sqrt{1-w_{t}^{2}}}\gtrsim\frac{1}{\xi^{1/2}}\] (D.226)

or

\[\sqrt{1-w_{t}^{2}}\lesssim\xi^{1/2}w_{t}\lesssim\xi^{1/2}\] (D.227)

which finally gives \(1-w_{t}^{2}\lesssim\xi\), or \(w_{t}^{2}\geq 1-O(\xi)\). In other words, for all \(\iota\geq\iota_{\mathsf{L}}\), we have \(w_{t}(\iota)\geq 1-O(\xi)\), which implies that

\[D_{2,t} =\mathop{\mathbb{E}}_{w\sim\rho_{t}}[P_{2,d}(w)]-\gamma_{2}\] (D.228) \[\gtrsim\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{2}]-\gamma_{2}-O \Big{(}\frac{1}{d}\Big{)}\] (D.229) \[\geq\mathbb{P}_{\iota\sim\rho_{0}}(|\iota|\geq\iota_{\mathsf{L} })\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{2}\mid|\iota|\geq\iota_{\mathsf{L}}] -\gamma_{2}-O\Big{(}\frac{1}{d}\Big{)}\] (D.230) \[\geq(1-O(\kappa))\cdot\big{(}\mathop{\mathbb{E}}_{w\sim\rho_{t}} [w^{2}\mid|\iota|\geq\iota_{\mathsf{L}}]\big{)}-\gamma_{2}-O\Big{(}\frac{1}{d }\Big{)}\] (By Proposition D.6) \[\geq(1-O(\kappa))(1-O(\xi))-\gamma_{2}-O\Big{(}\frac{1}{d}\Big{)}\] (B.c. if \[|\iota|>\iota_{\mathsf{L}}\], then \[w_{t}(\iota)^{2}\geq 1-O(\xi)\] ) \[\geq 1-O(\xi+\kappa)-\gamma_{2}\] (B.c. \[1/d\] is small by Assumption 3.2 ) \[>0\] (By Assumption 3.2  b.c. \[\gamma_{2}\] small and \[d\] large) and this contradicts the assumption that for all \[t\in[T_{2},s]\], \[D_{4,t}\geq 0\] and \[D_{2,t}\leq 0\]. Thus, for all \[t\in[T_{2},s]\], \[1-w_{t}(\iota_{\mathsf{M}})\geq\kappa^{4}\xi^{3}\], i.e. \[w_{t}(\iota_{\mathsf{M}})\leq 1-\kappa^{4}\xi^{3}\]. 

The purpose of the next two lemmas is to show that if Phase 3, Case 2 occurs, then for all \(t\geq T_{2}\), we will have \(D_{2,t}\leq 0\) and \(D_{4,t}\geq 0\), assuming the conclusion of Lemma D.25 holds. We will later put these next two lemmas with the above lemma, Lemma D.25, in order to inductively show that the hypothesis of Lemma D.25 holds for \(t\geq T_{2}\) (note that Lemma D.25 assumes that the upper bound on the potential difference holds on an interval \([T_{2},s]\), and more importantly assumes that \(D_{2,t}\leq 0\) and \(D_{4,t}\geq 0\) for all \(t\in[T_{2},s]\)).

**Lemma D.26**.: _Suppose we are in the setting of Theorem 3.3, and suppose \(D_{4,t}=0\) and \(D_{2,t}<0\) for some \(t\geq 0\). Additionally, suppose that the conclusion of Lemma D.25 holds at time \(t\), i.e. for all \(\iota\geq\iota_{\mathsf{L}}\), \(w_{t}(\iota)\geq\xi^{2}\kappa^{2}\), and \(w_{t}(\iota_{\mathsf{M}})\leq 1-\kappa^{4}\xi^{3}\). Then, \(\frac{d}{ds}D_{4,s}\big{|}_{s=t}>0\)._

Proof.: Recall from Eq. (C.4) that

\[P_{4,d}(t)=\frac{d^{2}+6d+8}{d^{2}-1}t^{4}-\frac{6d+12}{d^{2}-1}t^{2}+\frac{3} {d^{2}-1}\] (D.231)

meaning that

\[P_{4,d}{}^{\prime}(t)=4t^{3}\pm O\Big{(}\frac{1}{d}\Big{)}|t|\] (D.232)

Next we compute the derivative of \(D_{4,t}\):

\[\frac{d}{dt}D_{4,t}=\frac{d}{dt}\mathop{\mathbb{E}}_{w\sim\rho_{t}}[P_{4,d}(w )]=\mathop{\mathbb{E}}_{w\sim\rho_{t}}[P_{4,d}{}^{\prime}(w)\cdot v_{t}(w)]\] (D.233)

For positive particles \(w\gtrsim\frac{1}{\sqrt{d}}\), we have \(P_{4,d}{}^{\prime}(w)\geq 0\), and for \(w\lesssim\frac{1}{\sqrt{d}}\), it may be the case that \(P_{4,d}{}^{\prime}(w)\leq 0\). Furthermore, when \(D_{4,t}=0\) and \(D_{2,t}\leq 0\), we have

\[v_{t}(w) =-(1-w^{2})\Big{(}2\hat{\sigma}_{2,d}^{2}D_{2,t}w+O\Big{(}\frac{ \hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d}\Big{)}|w|\Big{)}\] (By Lemma 4.4 and because \[D_{4,t}=0\] ) \[=(1-w^{2})\cdot\Big{(}2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w\pm O\Big{(} \frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d}\Big{)}|w|\Big{)}\] (B.c. \[D_{2,t}\leq 0\] ) \[=(1-w^{2})\cdot 2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w\cdot(1\pm O(1/d))\] (D.234)Finally, to lower bound \(\frac{d}{dt}D_{4,t}\), we must show that there is a significant fraction of particles for which \(w\gtrsim\frac{1}{\sqrt{d}}\) and \(1-w\) is large. Because we have \(w_{t}(\iota)\geq\xi^{2}\kappa^{2}\) for all \(\iota\geq\iota_{\text{L}}\), and \(\mathbb{P}(|\iota|<\iota_{\text{L}})\lesssim\kappa\) by Proposition D.6, using a union bound we therefore know that

\[\mathbb{P}_{w\sim\rho_{t}}(|w|\in[\xi^{2}\kappa^{2},1-\kappa^{4} \xi^{3}])\geq 1-O(\gamma_{4})-O(\kappa)\geq 1-O(\gamma_{4})\] (D.235)

where the second inequality is by Assumption 3.2 since \(d\) is sufficiently large. Thus,

\[\frac{d}{dt}D_{4,t} =\underset{w\sim\rho_{t}}{\mathbb{E}}[P_{4,d}{}^{\prime}(w)\cdot v _{t}(w)]\] (D.236) \[=\mathbb{P}_{w\sim\rho_{t}}(|w|\lesssim 1/\sqrt{d})\underset{w\sim \rho_{t}}{\mathbb{E}}[P_{4,d}{}^{\prime}(w)\cdot v_{t}(w)\mid|w|\lesssim 1/ \sqrt{d}]\] (D.237) \[\qquad\qquad+\mathbb{P}_{w\sim\rho_{t}}(|w|\gtrsim 1/\sqrt{d}) \underset{w\sim\rho_{t}}{\mathbb{E}}[P_{4,d}{}^{\prime}(w)\cdot v_{t}(w)\mid|w |\gtrsim 1/\sqrt{d}]\] (D.238) \[\geq-O\Big{(}\frac{1}{d^{3/2}}\cdot\frac{\hat{\sigma}_{2,d}^{2}|D _{2,t}|}{d^{1/2}}\Big{)}+\mathbb{P}_{w\sim\rho_{t}}(|w|\gtrsim 1/\sqrt{d}) \underset{w\sim\rho_{t}}{\mathbb{E}}[P_{4,d}{}^{\prime}(w)\cdot v_{t}(w)\mid| w|\gtrsim 1/\sqrt{d}]\] (By bounding \[|P_{4,d}{}^{\prime}(w)\cdot v_{t}(w)|\] when \[|w|\lesssim 1/\sqrt{d}\] ) \[\geq-O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d^{2}}\Big{)}+ \mathbb{P}_{w\sim\rho_{t}}(|w|\in[\xi^{2}\kappa^{2},1-\kappa^{4}\xi^{3}])\] (D.239) \[\underset{w\sim\rho_{t}}{\mathbb{E}}[P_{4,d}{}^{\prime}(w)\cdot v _{t}(w)\mid|w|\in[\xi^{2}\kappa^{2},1-\kappa^{4}\xi^{3}]]\] (D.240) \[\geq-O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d^{2}}\Big{)} +(1-O(\gamma_{4}))\underset{w\sim\rho_{t}}{\mathbb{E}}[P_{4,d}{}^{\prime}(w) \cdot v_{t}(w)\mid|w|\in[\xi^{2}\kappa^{2},1-\kappa^{4}\xi^{3}]]\] (By Eq. (D.235)) \[\gtrsim-O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d^{2}}\Big{)} +(\xi^{2}\kappa^{2})^{3}\cdot(1-(1-\kappa^{4}\xi^{3})^{2})\hat{\sigma}_{2,d}^ {2}|D_{2,t}|(\xi^{2}\kappa^{2})\] (D.241)

where the last inequality is by Eq. (D.232) and applying the above bound on \(v_{t}(w)\) from Eq. (D.234) for \(|w|\in[\xi^{2}\kappa^{2},1-\kappa^{4}\xi^{3}]\), and because \(d\geq c_{3}\) by Assumption 3.2. Expanding the last line gives

\[\frac{d}{dt}D_{4,t} \gtrsim-O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d^{2}}\Big{)} +\hat{\sigma}_{2,d}^{2}|D_{2,t}|\xi^{6}\kappa^{6}\kappa^{4}\xi^{3}\xi^{2}\kappa ^{2}\] (D.242) \[\gtrsim-O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d^{2}}\Big{)} +\hat{\sigma}_{2,d}^{2}|D_{2,t}|\xi^{11}\kappa^{12}\] (D.243)

and the right hand side is strictly positive since \(d\) is sufficiently large by Assumption 3.2. 

**Lemma D.27**.: _Suppose we are in the setting of Theorem 3.3, and suppose \(D_{4,t}>0\) and \(D_{2,t}=0\) for some \(t\geq 0\). Additionally, suppose that the conclusion of Lemma D.25 holds at time \(t\), i.e. for all \(\iota\geq\iota_{\text{L}}\), \(w_{t}(\iota)\geq\xi^{2}\kappa^{2}\), and \(w_{t}(\iota_{\text{M}})\leq 1-\kappa^{4}\xi^{3}\). Then, \(\frac{d}{ds}D_{2,s}\big{|}_{s=t}<0\)._

Proof.: Recall from Eq. (C.4) that

\[P_{2,d}(t)=\frac{d}{d-1}t^{2}-\frac{1}{d-1}\] (D.244)

meaning that

\[\frac{d}{dt}D_{2,t}=\frac{d}{dt}\underset{w\sim\rho_{t}}{\mathbb{E}}[P_{2,d}(t) ]=\frac{2d}{d-1}\underset{w\sim\rho_{t}}{\mathbb{E}}[w\cdot v_{t}(w)]\] (D.245)

Suppose \(D_{2,t}=0\) and \(D_{4,t}>0\). Then, the velocity is

\[v_{t}(w) =-(1-w^{2})(P_{t}(w)+Q_{t}(w))\] (By Lemma 4.4 ) \[=-(1-w^{2})\Big{(}4\hat{\sigma}_{4,d}^{2}D_{4,t}w^{3}\pm O\Big{(} \frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}|w|\Big{)}\] (By Lemma 4.4 ) \[=-(1-w^{2})\cdot 4\hat{\sigma}_{4,d}^{2}D_{4,t}\Big{(}w^{3}\pm\frac{|w|}{ d}\Big{)}\] (D.246)Thus, for positive particles \(w\), if \(w\gtrsim\frac{1}{\sqrt{d}}\), \(v_{t}(w)\leq 0\), and for \(w\lesssim\frac{1}{\sqrt{d}}\), the velocity is potentially positive. Using this, we can upper bound \(\frac{d}{dt}D_{2,t}\) by upper bounding \(\mathbb{E}_{w\sim\rho_{t}}[w\cdot v_{t}(w)]\):

\[\mathbb{E}_{w\sim\rho_{t}}[w\cdot v_{t}(w)] =\mathbb{P}_{w\sim\rho_{t}}\Big{(}|w|\lesssim\frac{1}{\sqrt{d}} \Big{)}\mathbb{E}_{w\sim\rho_{t}}\Big{[}w\cdot v_{t}(w)\mid|w|\lesssim\frac{1} {\sqrt{d}}\Big{]}\] (D.247) \[\qquad\qquad+\mathbb{P}_{w\sim\rho_{t}}\Big{(}|w|\gtrsim\frac{1} {\sqrt{d}}\Big{)}\mathbb{E}_{w\sim\rho_{t}}\Big{[}w\cdot v_{t}(w)\mid|w| \gtrsim\frac{1}{\sqrt{d}}\Big{]}\] (D.248) \[\leq O\Big{(}\frac{1}{d^{1/2}}\cdot\frac{\hat{\sigma}_{4,d}^{2}|D _{4,t}|}{d^{3/2}}\Big{)}+\mathbb{P}_{w\sim\rho_{t}}\Big{(}|w|\gtrsim\frac{1} {\sqrt{d}}\Big{)}\mathbb{E}_{w\sim\rho_{t}}\Big{[}w\cdot v_{t}(w)\mid|w| \gtrsim\frac{1}{\sqrt{d}}\Big{]}\] (By bounding \[w\cdot v_{t}(w)\] for \[0\leq w\lesssim\frac{1}{\sqrt{d}}\] ) \[\leq O\Big{(}\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d^{2}}\Big{)}+ \mathbb{P}_{w\sim\rho_{t}}\Big{(}|w|\in[\xi^{2}\kappa^{2},1-\kappa^{4}\xi^{3}] \Big{)}\] (D.249) \[\mathbb{E}_{w\sim\rho_{t}}\Big{[}w\cdot v_{t}(w)\mid|w|\in[\xi^{2 }\kappa^{2},1-\kappa^{4}\xi^{3}]\Big{]}\] (B.c. for \[w\gtrsim\frac{1}{\sqrt{d}}\], \[v_{t}(w)<0\].) \[\leq O\Big{(}\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d^{2}}\Big{)}+ \left(1-O(\kappa)-O(\gamma_{4})\right)\mathbb{E}_{w\sim\rho_{t}}\Big{[}w\cdot v _{t}(w)\mid|w|\in[\xi^{2}\kappa^{2},1-\kappa^{4}\xi^{3}]\Big{]}\] (D.250)

where the last inequality is because \(\mathbb{P}(|\iota|>\iota_{\text{M}})\leq O(\gamma_{4})\) (by the definition of \(\iota_{\text{M}}\), see statement of Lemma D.25) and \(\mathbb{P}(|\iota|<\iota_{\text{L}})\leq O(\kappa)\) by Proposition D.6. Simplifying further, we obtain,

\[\mathbb{E}_{w\sim\rho_{t}}[w\cdot v_{t}(w)] \leq O\Big{(}\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d^{2}}\Big{)} -(1-O(\kappa)-O(\gamma_{4}))(\xi^{2}\kappa^{2})\cdot(1-(1-\kappa^{4}\xi^{3})^ {2})\hat{\sigma}_{4,d}^{2}D_{4,t}(\xi^{2}\kappa^{2})^{3}\] (By Eq. (D.246) and b.c. \[\frac{1}{d}\] is small (Assumption 3.2)) \[\lesssim O\Big{(}\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)} -\hat{\sigma}_{4,d}^{2}D_{4,t}\cdot\xi^{2}\kappa^{2}\cdot\kappa^{4}\xi^{3}\xi ^{6}\kappa^{6}\] (By Assumption 3.2, \[1-O(\kappa)-O(\gamma_{4})\gtrsim 1\] ) \[\lesssim O\Big{(}\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)} -\hat{\sigma}_{4,d}^{2}D_{4,t}\xi^{11}\kappa^{12}\] (D.251)

and the last expression is strictly negative, since \(d\geq c_{3}\) by Assumption 3.2 and \(D_{4,t}\geq 0\). This proves the lemma. 

Now, we combine Lemma D.25, Lemma D.26 and Lemma D.27 to inductively show that for all \(t\geq T_{2}\), we have \(D_{2,t}\leq 0\), \(D_{4,t}\geq 0\) and most importantly, \(w_{t}(\iota)\) can be bounded away from \(0\) and \(1\) for \(\iota\in[\iota_{\text{L}},\iota_{\text{M}}]\).

**Lemma D.28** (Phase 3 Case 2 Invariants).: _Suppose we are in the setting of Theorem 3.3. Assume \(D_{2,T_{2}}<0\) and \(D_{4,T_{2}}=0\), i.e. Phase 3 Case 2 holds. Then, for all \(t\geq T_{2}\), the following will hold:_

1. \(D_{2,t}\leq 0\) _and_ \(D_{4,t}\geq 0\)__
2. _Let_ \(\iota_{\text{M}}\) _be defined as in Lemma D.25. Then,_ \(w_{t}(\iota)\geq\xi^{2}\kappa^{2}\) _for all_ \(\iota\in[\iota_{\text{L}},\iota_{\text{M}}]\) _and_ \(w_{t}(\iota_{\text{M}})\leq 1-\kappa^{4}\xi^{3}\)_._

Proof of Lemma D.28.: For the purpose of this proof, define

\[I=\{s\geq T_{2}\mid D_{2,s}\leq 0\text{ and }D_{4,s}\geq 0\}\] (D.252)

and let \(t_{0}=\sup I\), meaning that for \(t\in[T_{2},t_{0}]\), \(D_{2,t}\leq 0\) and \(D_{4,t}\geq 0\). Assume for the sake of contradiction that \(t_{0}<\infty\). Then, by the continuity of \(D_{2,t}\) and \(D_{4,t}\) as functions of \(t\), \(D_{2,t}\leq 0\) and \(D_{4,t}\geq 0\). Since \(t_{0}=\sup I\), either \(D_{2,t_{0}}=0\) or \(D_{4,t_{0}}=0\) (and one of these has to be nonzero since otherwise \(t_{0}=\infty\), which is a contradiction). By Lemma D.24 and Lemma D.23, for all \(t\in[T_{2},t_{0}]\), for all \(\iota,\iota^{\prime}\in[\iota_{\text{L}},\iota_{\text{M}}]\),

\[\Big{|}\Phi(w_{t}(\iota))-\Phi(w_{t}(\iota^{\prime}))\Big{|}\leq 2\log\frac{1}{ \kappa}+\log\frac{1}{\xi}+C\] (D.253)where \(C\) is a universal constant. Thus, applying Lemma D.25 with \(s=t_{0}\), we find that for all \(t\in[T_{2},t_{0}]\), for all \(t\in[T_{2},t_{0}]\), for all \(\iota\geq\iota_{\text{L}}\), \(w_{t}(\iota)\geq\xi^{2}\kappa^{2}\), and furthermore, for all \(t\in[T_{2},t_{0}]\), \(w_{t}(\iota_{\text{M}})\leq 1-\kappa^{4}\xi^{3}\).

To finish the proof, there are two cases to address: either \(D_{2,t_{0}}<0\) and \(D_{4,t_{0}}=0\), or \(D_{2,t_{0}}=0\) and \(D_{4,t_{0}}>0\). In the first case, by Lemma D.26, \(\frac{d}{dt}D_{4,t}\Big{|}_{t=t_{0}}>0\), meaning that for a sufficiently small open interval of time \(J\) containing \(t_{0}\), \(D_{4,t}\) is increasing on this interval. This gives us a contradiction, since it implies that for some \(t_{0}^{\prime}>t_{0}\) (with \(t_{0}^{\prime}-t_{0}\) sufficiently small) \(D_{4,t}\) is nonnegative on \([t_{0},t_{0}^{\prime}]\), and since \(D_{2,t_{0}}<0\), by continuity, \(D_{2,t}<0\) for \(t\in[t_{0},t_{0}^{\prime}]\) as long as \(t_{0}^{\prime}-t_{0}>0\) is sufficiently small. Thus, \(\sup I>t_{0}\), which is a contradiction. In the second case, we can apply the same argument using Lemma D.27 in place of Lemma D.26.

In summary, if \(D_{4,T_{2}}=0\) and \(D_{2,T_{2}}<0\), then for all \(t\geq T_{2}\), \(D_{2,t}\leq 0\) and \(D_{4,t}\geq 0\). As a consequence of Lemma D.23, Lemma D.24, and Lemma D.25, for all \(\iota\in[\iota_{\text{L}},\iota_{\text{M}}]\), \(w_{t}(\iota)\geq\xi^{2}\kappa^{2}\), and furthermore, \(w_{t}(\iota_{\text{M}})\leq 1-\kappa^{4}\xi^{3}\), for all \(t\geq T_{2}\). 

We now proceed to bound the running time of Phase 3, Case 2, by dividing it into a few subcases, and showing that the running time that each of these subcases contribute is within our desired bound. The first lemma below bounds the running time spent in the subcase where the positive root \(r\) of \(P_{t}\) is larger than \(\frac{1}{2}\) (and where \(D_{4,t}\) is large relative to \(D_{2,t}\)).

**Lemma D.29**.: _Suppose we are in the setting of Theorem 3.3, and assume that \(D_{4,T_{2}}=0\) and \(D_{2,T_{2}}<0\), i.e. we are in Phase 3, Case 2. Then, at most \(\frac{1}{\hat{\sigma}_{2,d}^{2}\xi^{4}\kappa^{6}}\log\left(\frac{\gamma_{2}}{ \epsilon}\right)\) time \(t\geq T_{2}\) elapses such that the following hold:_

1. _If_ \(r\) _is the positive root of_ \(P_{t}\) _(as defined in Lemma_ 4.4_) then_ \(r\geq\frac{1}{2}\)_._
2. \(|D_{4,t}|\geq\xi|D_{2,t}|\)__
3. \(L(\rho_{t})\geq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\)__

Proof of Lemma D.29.: Let \(t\geq T_{2}\) such that \(|D_{4,t}|\geq\xi|D_{2,t}|.\) We can write

\[P_{t}(w)=2\hat{\sigma}_{2,d}^{2}D_{2,t}w+4\hat{\sigma}_{4,d}^{2}D_{4,t}w^{3}= 4\hat{\sigma}_{4,d}^{2}D_{4,t}w(w-r)(w+r)\] (D.254)

where \(r\) is the positive root of \(P_{t}(w)\). We are now going to lower bound the expected velocity while \(r\geq\frac{1}{2}\) by lower bounding \(P_{t}(w)\) for a large fraction of \(w\).

For all \(\iota\geq\iota_{\text{L}}\), and \(t\geq T_{2}\), we have \(w_{t}(\iota)\geq\xi^{2}\kappa^{2}\), by Lemma D.28. Thus, \(\mathbb{P}_{w\sim\rho_{t}}(w\geq\xi^{2}\kappa^{2})\leq 1-O(\kappa)\) by Proposition D.6. Additionally, because \(D_{2,t}\leq 0\), we have \(\mathbb{E}_{w\sim\rho_{t}}[w^{2}]\leq\gamma_{2}+O(1/d)\) by Proposition D.38, meaning that by Markov's inequality, \(\mathbb{P}_{w\sim\rho_{t}}(|w|\geq\frac{1}{3})\lesssim\gamma_{2}+1/d\leq \gamma_{2}\), where the last inequality is because \(d\geq c_{3}\) by Assumption 3.2. By a union bound,

\[\mathbb{P}_{w\sim\rho_{t}}(|w|\in[\xi^{2}\kappa^{2},1/3])\geq 1-O(\gamma_{2}+\kappa)\] (D.255)

For particles \(w\) such that \(|w|\in[\xi^{2}\kappa^{2},1/3]\), we can lower bound the velocity if \(r\geq\frac{1}{2}\):

\[|v_{t}(w)| \gtrsim(1-w^{2})|P_{t}(w)+Q_{t}(w)|\] (By Lemma 4.4 ) \[\gtrsim|P_{t}(w)+Q_{t}(w)|\] (B.c. \[w\leq 1/3\] ) \[\gtrsim|P_{t}(w)|-|Q_{t}(w)|\] (D.256) \[\gtrsim|2\hat{\sigma}_{2,d}^{2}D_{2,t}w+4\hat{\sigma}_{4,d}^{2}D_{ 4,t}w^{3}|-|\lambda_{d}{}^{(1)}w+\lambda_{d}{}^{(3)}w^{3}|\] (By Lemma 4.4 ) \[\gtrsim|2\hat{\sigma}_{2,d}^{2}D_{2,t}w+4\hat{\sigma}_{4,d}^{2}D_{ 4,t}w^{3}|-O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2} |D_{4,t}|}{d}\Big{)}|w|\] (By Lemma 4.4 ) \[\gtrsim\hat{\sigma}_{4,d}^{2}D_{4,t}|w||w-r||w+r|-O\Big{(}\frac{ \hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}\] (By definition of \[r\] ) \[\gtrsim\hat{\sigma}_{4,d}^{2}D_{4,t}\cdot\xi^{2}\kappa^{2}\cdot \frac{1}{6}\cdot\frac{1}{2}-O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{ \sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}\] (B.c. \[w\in[\xi^{2}\kappa^{2},1/3]\] and \[r\geq 1/2\] ) \[\gtrsim\hat{\sigma}_{4,d}^{2}D_{4,t}\xi^{2}\kappa^{2}\] (D.257)where the last inequality is because \(|D_{4,t}|\geq\xi|D_{2,t}|\) and \(d\) is sufficiently large by Assumption 3.2. Thus,

\[\underset{w\sim\rho_{t}}{\mathbb{E}}|v_{t}(w)|^{2} \gtrsim\mathbb{P}_{w\sim\rho_{t}}(|w|\in[\xi^{2}\kappa^{2},1/3]) \cdot\underset{w\sim\rho_{t}}{\mathbb{E}}[|v_{t}(w)|^{2}\mid|w|\in[\xi^{2} \kappa^{2},1/3]]\] (D.258) \[\gtrsim(1-O(\gamma_{2}+\kappa))\cdot\hat{\sigma}_{4,d}^{4}D_{4,t}^ {2}\xi^{4}\kappa^{4}\] (By Eq. (D.255)) \[\gtrsim\hat{\sigma}_{4,d}^{4}D_{4,t}^{2}\xi^{4}\kappa^{4}\] (By Assumption 3.2 since \[\gamma_{2},d\] sufficiently small, large resp.)

and

\[\frac{dL(\rho_{t})}{dt} \lesssim-\hat{\sigma}_{4,d}^{4}D_{4,t}^{2}\xi^{4}\kappa^{4}\] (By Lemma D.39) \[\lesssim-\hat{\sigma}_{4,d}^{4}(D_{4,t}^{2}+\xi^{2}D_{2,t}^{2}) \xi^{4}\kappa^{4}\] (B.c. \[|D_{4,t}|\geq\xi|D_{2,t}|\] ) \[\lesssim-\hat{\sigma}_{4,d}^{4}(D_{4,t}^{2}+D_{2,t}^{2})\xi^{4} \kappa^{6}\] (D.259) \[\lesssim-\hat{\sigma}_{4,d}^{2}\xi^{4}\kappa^{6}L(\rho_{t})\] (B.c. \[\hat{\sigma}_{2,d}^{2}\times\hat{\sigma}_{4,d}^{2}\] by Assumption 3.2 )

Since \(L(\rho_{0})\asymp\hat{\sigma}_{4,d}^{2}\gamma_{2}^{2}\) by Lemma D.8 and Assumption 3.2, the amount of time spent in the case where \(r(t)\geq\frac{1}{2},|D_{4,t}|\geq\xi|D_{4,t}|\) and \(L(\rho_{t})\geq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\) after \(t\geq T_{2}\) is at most \(\frac{1}{\hat{\sigma}_{2,d}^{2}\xi^{4}\kappa^{6}}\log\left(\frac{\gamma_{2}}{ \epsilon}\right)\) by Gronwall's inequality (Fact I.13). 

Next, we deal with the subcase where \(D_{4,t}\) is small relative to \(D_{2,t}\).

**Lemma D.30**.: _Suppose we are in the setting of Theorem 3.3, and \(D_{4,T_{2}}=0\) and \(D_{2,T_{2}}<0\), i.e. we are in Phase 3, Case 2. Then, at most \(\frac{1}{\hat{\sigma}_{2,d}^{2}\xi^{4}\kappa^{4}}\log\left(\frac{\gamma_{2}}{ \epsilon}\right)\) time \(t\geq T_{2}\) elapses such that the following hold:_

1. \(|D_{4,t}|\leq\xi|D_{2,t}|\)__
2. \(L(\rho_{t})\geq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\)__

Proof of Lemma D.30.: Suppose \(|D_{4,t}|\leq\xi|D_{2,t}|\) at some time \(t\geq T_{2}\). Then, we can lower bound the velocity for a large fraction of \(w\)'s, as follows. First, we provide a bound on \(P_{t}(w)\) (defined in Lemma 4.4):

\[P_{t}(w) =2\hat{\sigma}_{2,d}^{2}D_{2,t}w+4\hat{\sigma}_{4,d}^{2}D_{4,t}w^ {3}\] (By Lemma 4.4 ) \[=-2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w+4\hat{\sigma}_{4,d}^{2}|D_{4, t}|w^{3}\] (B.c. \[D_{2,t}<0\] and \[D_{4,t}>0\] by Lemma D.28 ) \[=-2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w+O(\xi\hat{\sigma}_{4,d}^{2}|D _{2,t}|w^{3})\] (B.c. \[|D_{4,t}|\leq\xi|D_{2,t}|\] ) \[=-2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w+O(\xi\hat{\sigma}_{2,d}^{2}|D _{2,t}|w^{3})\] (B.c. \[\hat{\sigma}_{2,d}^{2}\lesssim\hat{\sigma}_{4,d}^{2}\] by Assumption 3.2 ) \[=-2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w\cdot(1-O(\xi))\] (D.260)

Next, we provide a bound on \(Q_{t}(w)\):

\[|Q_{t}(w)| =|{\lambda_{d}}^{(1)}w+{\lambda_{d}}^{(3)}w|\] (By Lemma 4.4 ) \[=O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d}\Big{)}|w|\] (By Lemma 4.4 ) \[=O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|}{d}\Big{)}|w|\] (B.c. \[|D_{4,t}|\leq\xi|D_{2,t}|\] and \[\hat{\sigma}_{4,d}^{2}\lesssim\hat{\sigma}_{2,d}^{2}\] ) \[=O(\xi\hat{\sigma}_{2,d}^{2}|D_{2,t}|)|w|\] (B.c. \[1/d\leq\frac{1}{\log\log d}=\xi\] by Assumption 3.2 )

Thus,

\[|P_{t}(w)+Q_{t}(w)|\geq 2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w\cdot(1-O( \xi))\] (D.261)

By Lemma D.28, we have \(D_{2,t}\leq 0\), meaning that by Proposition D.38,

\[\underset{w\sim\rho_{t}}{\mathbb{E}}[w^{2}]\leq\gamma_{2}+O(1/d)\] (D.262)Thus, by Markov's inequality and Assumption 3.2,

\[\mathbb{P}_{w\sim\rho_{t}}(|w|\geq 1/2)\lesssim\gamma_{2}\] (D.263)

Furthermore, by Lemma D.28, for all \(\iota\geq\iota_{\mathrm{L}}\), \(w_{t}(\iota)\geq\xi^{2}\kappa^{2}\) for all times \(t\geq T_{2}\), meaning that \(w_{t}\geq\xi^{2}\kappa^{2}\) with probability at least \(1-O(\kappa)\) under \(\rho_{t}\), by Proposition D.6. Thus, by a union bound,

\[\mathbb{P}_{w\sim\rho_{t}}(|w|\in[\xi^{2}\kappa^{2},1/2])\geq 1-O(\gamma_{2})-O( \kappa)\geq\frac{1}{2}\] (D.264)

where the last inequality is by Assumption 3.2 since \(\gamma_{2}\) and \(d\) are sufficiently small and large respectively. Thus the average velocity is at least

\[\mathop{\mathbb{E}}_{w\sim\rho_{t}}|v_{t}(w)|^{2} \gtrsim\mathbb{P}_{w\sim\rho_{t}}(\xi^{2}\kappa^{2}\leq|w|\leq 1/2) \cdot\mathop{\mathbb{E}}_{w\sim\rho_{t}}[|v_{t}(w)|^{2}\mid\xi^{2}\kappa^{2} \leq|w|\leq 1/2]\] (D.265) \[\gtrsim\frac{1}{2}\cdot\mathop{\mathbb{E}}_{w\sim\rho_{t}}[|v_{t }(w)|^{2}\mid\xi^{2}\kappa^{2}\leq|w|\leq 1/2]\] (D.266) \[\gtrsim\mathop{\mathbb{E}}_{w\sim\rho_{t}}[|v_{t}(w)|^{2}\mid\xi^{ 2}\kappa^{2}\leq|w|\leq 1/2]\] (D.267) \[\gtrsim\mathop{\mathbb{E}}_{w\sim\rho_{t}}[|(1-w^{2})^{2}|P_{t}(w )+Q_{t}(w)|^{2}\mid\xi^{2}\kappa^{2}\leq|w|\leq 1/2]\] (B.c. \[|w|\leq 1/2\] ) \[\gtrsim\mathop{\mathbb{E}}_{w\sim\rho_{t}}[\hat{\sigma}_{2,d}^{4}| D_{2,t}|^{2}w^{2}\mid\xi^{2}\kappa^{2}\leq|w|\leq 1/2]\] (By Eq. (D.261)) \[\gtrsim\hat{\sigma}_{2,d}^{4}|D_{2,t}|^{2}\xi^{4}\kappa^{4}\] (D.269)

Thus, at any time \(t\) where \(|D_{4,t}|\leq\xi|D_{2,t}|\), we have

\[\frac{dL(\rho_{t})}{dt} \lesssim-\hat{\sigma}_{2,d}^{4}|D_{2,t}|^{2}\xi^{4}\kappa^{4}\] (D.270) \[\lesssim-\hat{\sigma}_{2,d}^{2}(\hat{\sigma}_{2,d}^{2}|D_{2,t}|^{ 2}+\hat{\sigma}_{4,d}^{2}|D_{4,t}|^{2})\xi^{4}\kappa^{4}\] (B.c. \[|D_{4,t}|\leq\xi|D_{2,t}|\] and \[\hat{\sigma}_{4,d}^{2}\lesssim\hat{\sigma}_{2,d}^{2}\] by Assumption 3.2 ) \[\lesssim-\hat{\sigma}_{2,d}^{2}\xi^{4}\kappa^{4}L(\rho_{t})\] (D.271)

Since the initial loss is \(L(\rho_{0})\lesssim\hat{\sigma}_{2,d}^{2}\gamma_{2}^{2}\) by Lemma D.8, by an argument similar to that used in Lemma D.29, using Gronwall's inequality (Fact I.13), we find that

\[\int_{T_{2}}^{\infty}1(|D_{4,t}|\leq\xi|D_{2,t}|)1(L(\rho_{t})\geq(\hat{ \sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2})dt\lesssim\frac{1}{ \hat{\sigma}_{2,d}^{2}\xi^{4}\kappa^{4}}\log\left(\frac{\gamma_{2}}{\epsilon}\right)\] (D.272)

i.e. the time spent in the subcase in the statement of this lemma is at most \(\frac{1}{\hat{\sigma}_{2,d}^{2}\xi^{4}\kappa^{4}}\log\left(\frac{\gamma_{2}}{ \epsilon}\right)\), as desired. 

We have already considered the case where \(|D_{4,t}|\geq\xi|D_{2,t}|\) and \(r\geq\frac{1}{2}\). Next, we must deal with the case where \(|D_{4,t}|\geq\xi|D_{2,t}|\), but where \(r\leq\frac{1}{2}\). We further split this case into two subcases. In the next lemma, we deal with the subcase where \(w_{t}(\iota_{\mathrm{R}})\geq 1-\xi\), meaning a non-negligible portion of the particles are close to \(1\).

**Lemma D.31**.: _Suppose we are in the setting of Theorem 3.3, and \(D_{4,T_{2}}=0\) and \(D_{2,T_{2}}<0\), i.e. we are in Phase 3, Case 2. Then, the amount of time \(t\geq T_{2}\) that elapses while the following conditions simultaneously hold:_

1. \(|D_{4,t}|\geq\xi|D_{2,t}|\)__
2. \(r\leq\frac{1}{2}\) _where_ \(r\) _is the positive root of_ \(P_{t}\)__
3. \(w_{t}(\iota_{\mathrm{R}})\geq 1-\xi\)__
4. \(L(\rho_{t})\geq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\)__

_is at most \(\frac{1}{\hat{\sigma}_{4,d}^{2}\epsilon^{6}\kappa^{2}}\log\left(\frac{\gamma_{2} }{\epsilon}\right)\)._Proof of Lemma D.31.: If \(w(\iota_{\text{R}})\geq 1-\xi\) at any time, then the subsequent times when it could possibly be moving to towards \(1\) are:

1. During phase 2 -- note that before phase 2, \(w_{t}(\iota_{\text{R}})\leq\frac{1}{\log d}\), so it is not possible for \(w_{t}(\iota_{\text{R}})\) to be larger than \(1-\xi\) since, by Lemma D.9, \(w_{t}(\iota_{\text{R}})\) is increasing during Phase 1.
2. The times \(t\geq T_{2}\) when \(|D_{4,t}|\leq\xi|D_{2,t}|\)
3. The times \(t\geq T_{2}\) when \(|D_{4,t}|\geq\xi|D_{2,t}|\) and \(r\geq\frac{1}{2}\), where \(r\) is the positive root of \(P_{t}(w)\)

The only case not mentioned above consists of the times \(t\geq T_{2}\) when \(|D_{4,t}|\geq\xi|D_{2,t}|\) and the positive root \(r\) of \(P_{t}(w)\) is less than \(\frac{1}{2}\). In this case, \(w_{t}(\iota_{\text{R}})\) will be moving away from \(1\). For this reason, we also do not need to consider the case discussed in Lemma D.32.

Thus, we will use upper bounds on the velocity and time during the first 3 cases listed above, and use a lower bound on the velocity in the last. First note that the velocity can always be upper bounded as follows:

\[v_{t}(w) \lesssim(1-w)(1+w)\cdot|P_{t}(w)+Q_{t}(w)|\] (By Lemma 4.4 ) \[\lesssim(1-w)(1+w)\cdot\Big{|}2\hat{\sigma}_{2,d}^{2}|D_{2,t}|w+4 \hat{\sigma}_{4,d}^{2}|D_{4,t}|w^{3}+O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{ 2,t}|+\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}|w|\Big{|}\] (By Lemma 4.4 ) \[\lesssim(1-w)\cdot(\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D _{4,t}|)\] (B.c. \[|w|\leq 1\] )

Thus,

\[\frac{d}{dt}(1-w)\gtrsim-(\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{ 2}|D_{4,t}|)(1-w)\gtrsim-(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(1-w)\] (D.273)

During Phase 2.: By Lemma D.10, the running time during Phase 2 is \(T_{2}-T_{1}\lesssim\frac{1}{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) \hat{\epsilon}^{6}\kappa^{12}}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\). At the beginning of Phase 2, \(w_{t}(\iota_{\text{R}})\leq\frac{1}{\log d}\), by the definition of Phase 1. Thus, by Gronwall's inequality (Fact I.13), at time \(T_{2}\),

\[1-w_{T_{2}}(\iota_{\text{R}}) \geq\Big{(}1-\frac{1}{\log d}\Big{)}\cdot\exp\Big{(}-(\hat{ \sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\cdot\frac{1}{(\hat{\sigma}_{2,d}^{2} +\hat{\sigma}_{4,d}^{2})\hat{\epsilon}^{6}\kappa^{12}}\log\Big{(}\frac{\gamma_ {2}}{\epsilon}\Big{)}\Big{)}\] (By Fact I.13, Eq. (D.273) and bound on \[T_{2}-T_{1}\] ) \[\geq\Big{(}1-\frac{1}{\log d}\Big{)}\cdot\exp\Big{(}-\frac{1}{ \hat{\epsilon}^{6}\kappa^{12}}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)} \Big{)}\] (D.274)

Times \(t\geq T_{2}\) when \(|D_{4,t}|\leq\xi|D_{2,t}|\) and \(L(\rho_{t})\geq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\). By Lemma D.30, this case contributes at most \(\frac{1}{\hat{\sigma}_{2,d}^{2}\hat{\epsilon}^{6}\kappa^{4}}\log\Big{(}\frac{ \gamma_{2}}{\epsilon}\Big{)}\) time to the overall running time. Thus, by Gronwall's inequality (Fact I.13), the additional factor that this case contributes to \(1-w_{t}(\iota_{\text{R}})\) is larger than or equal to

\[\exp\Big{(}-(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\cdot\frac{1}{\hat{ \sigma}_{2,d}^{2}\hat{\epsilon}^{4}\kappa^{4}}\log\Big{(}\frac{\gamma_{2}}{ \epsilon}\Big{)}\Big{)}\geq\exp\Big{(}-\frac{C}{\hat{\epsilon}^{4}\kappa^{4}} \log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\Big{)}\] (D.275)

for some universal constant \(C\).

Times \(t\geq T_{2}\) when \(|D_{4,t}|\geq\xi|D_{2,t}|\), \(r(t)\geq\frac{1}{2}\) and \(L(\rho_{t})\geq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\). By Lemma D.29, this case contributes at most \(\frac{1}{\hat{\sigma}_{2,d}^{2}\hat{\epsilon}^{4}\kappa^{6}}\log\Big{(}\frac{ \gamma_{2}}{\epsilon}\Big{)}\) to the overall running time. Thus, by Gronwall's inequality (Fact I.13), the additional factor that this case contributes to \(1-w_{t}(\iota_{\text{R}})\) is larger than or equal to

\[\exp\Big{(}-(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\cdot\frac{1}{\hat{ \sigma}_{2,d}^{2}\hat{\epsilon}^{4}\kappa^{6}}\log\Big{(}\frac{\gamma_{2}}{ \epsilon}\Big{)}\Big{)}\geq\exp\Big{(}-\frac{C}{\hat{\epsilon}^{4}\kappa^{6}} \log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\Big{)}\] (D.276)

for some universal constant \(C\).

**Times \(t\geq T_{2}\) when \(|D_{4,t}|\geq\xi|D_{2,t}|\), \(r(t)\leq\frac{1}{2}\) and \(w_{\text{R}}\geq 1-\xi\).** In this case, we obtain a lower bound on \(\frac{d}{dt}(1-w_{\text{R}})\), where we have written \(w_{\text{R}}=w_{t}(\iota_{\text{R}})\) for convenience. First let us obtain a lower bound on \(P_{t}(w_{\text{R}})\):

\[P_{t}(w_{\text{R}}) \gtrsim 2\hat{\sigma}_{2,d}^{2}D_{2,t}w_{\text{R}}+4\hat{\sigma}_{4,d }^{2}D_{4,t}w_{\text{R}}{}^{3}\] (D.277) \[\gtrsim 4\hat{\sigma}_{4,d}^{2}D_{4,t}w_{\text{R}}(w_{\text{R}}-r)(w_{ \text{R}}+r)\] (B.c. \[r\] is the positive root of \[P_{t}\] ) \[\gtrsim\hat{\sigma}_{4,d}^{2}D_{4,t}(1-\xi-r)\] (B.c. \[D_{4,t}\geq 0\] and \[w_{\text{R}}\geq 1-\xi\] ) \[\gtrsim\hat{\sigma}_{4,d}^{2}D_{4,t}\] (B.c. \[r\leq\frac{1}{2}\] and \[\xi\leq\frac{1}{\log\log d}\] )

Next let us obtain an upper bound on \(Q_{t}(w_{\text{R}})\):

\[|Q_{t}(w_{\text{R}})| \lesssim|\lambda_{d}{}^{(1)}|+|\lambda_{d}{}^{(3)}|\] (By Lemma 4.4 ) \[\lesssim\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^ {2}|D_{4,t}|}{d}\] (By Lemma 4.4 ) \[\lesssim\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{\xi d}\] (B.c. \[|D_{2,t}|\leq|D_{4,t}|/\xi\] and \[\hat{\sigma}_{2,d}^{2}\lesssim\hat{\sigma}_{4,d}^{2}\] by Assumption 3.2 )

Thus, because \(\frac{1}{\xi d}\asymp\frac{\log\log d}{d}\),

\[P_{t}(w_{\text{R}})+Q_{t}(w_{\text{R}})\gtrsim\hat{\sigma}_{4,d}^{2}D_{4,t}\] (D.278)

and when \(w_{\text{R}}\geq 1-\xi\),

\[\frac{d}{dt}(1-w_{\text{R}}) \gtrsim-v_{t}(w_{\text{R}})\] (D.279) \[\gtrsim(1-w_{\text{R}})(1+w_{\text{R}})(P_{t}(w_{\text{R}})+Q_{t }(w_{\text{R}}))\] (By Lemma 4.4 ) \[\gtrsim(1-w_{\text{R}})\cdot\hat{\sigma}_{4,d}^{2}D_{4,t}\] (By Eq. (D.278) and b.c. \[w_{\text{R}}\geq 1-\xi\] )

Furthermore, since \(|D_{4,t}|\geq\xi|D_{2,t}|\), this means that \(|D_{4,t}|\gtrsim\xi\epsilon\), since otherwise, \(|D_{2,t}|\lesssim\epsilon\) and \(|D_{4,t}|\lesssim\xi\epsilon\), and the loss would be less than \((\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\). Thus, if time \(T_{\text{esc}}\) is spent in this case, then by Gronwall's inequality (Fact I.13), this case contributes a factor of at least

\[\exp(T_{\text{esc}}\cdot\hat{\sigma}_{4,d}^{2}D_{4,t})\geq\exp(T_{\text{esc }}\cdot\hat{\sigma}_{4,d}^{2}\xi\epsilon)\] (D.280)

to \(1-w_{\text{R}}\).

**Summary of 4 Cases.** In summary, by Eq. (D.274), Eq. (D.275), Eq. (D.276) and Eq. (D.280),

\[1-w_{\text{R}}\geq\Big{(}1-\frac{1}{\log d}\Big{)}\cdot\exp\Big{(} -\frac{1}{\xi^{6}\kappa^{12}}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)} \Big{)}\exp\Big{(}-\frac{C}{\xi^{4}\kappa^{4}}\log\Big{(}\frac{\gamma_{2}}{ \epsilon}\Big{)}\Big{)}\] (D.281) \[\exp\Big{(}-\frac{C}{\xi^{4}\kappa^{6}}\log\Big{(}\frac{\gamma_{ 2}}{\epsilon}\Big{)}\Big{)}\exp(T_{\text{esc}}\cdot\hat{\sigma}_{4,d}^{2}\xi\epsilon)\] (D.282)

where \(T_{\text{esc}}\) is the amount of time spent so far in the case where \(|D_{4,t}|\geq\xi|D_{2,t}|\), \(r(t)\geq\frac{1}{2}\) and \(w_{\text{R}}\geq 1-\xi\). This is in fact a lower bound on \(1-w_{\text{R}}\) when \(L(\rho_{t})\geq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\), since at all other times when \(L(\rho_{t})\geq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\), we have that \(1-w_{\text{R}}\) is increasing (i.e. \(w_{\text{R}}\) is moving away from \(1\)). In particular, if

\[T_{\text{esc}}\gtrsim\frac{1}{\hat{\sigma}_{4,d}^{2}\xi\epsilon} \Big{(}\frac{1}{\xi^{6}\kappa^{12}}\log\Big{(}\frac{\gamma_{2}}{\epsilon} \Big{)}+\frac{C}{\xi^{4}\kappa^{4}}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)} +\frac{C}{\xi^{4}\kappa^{6}}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)} \Big{)}\gtrsim\frac{1}{\hat{\sigma}_{4,d}^{2}\epsilon\xi^{7}\kappa^{12}}\log \Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\] (D.283)

then \(1-w_{\text{R}}\) will be at least \(\xi\). 

Finally, the following lemma deals with the last case, where the positive root \(r\) of \(P_{t}\) is at most \(\frac{1}{2}\), and additionally, \(w_{t}(\iota_{\text{R}})\leq 1-\xi\).

**Lemma D.32**.: _Suppose we are in the setting of Theorem 3.3. Assume \(D_{4,T_{2}}=0\) and \(D_{2,T_{2}}<0\), i.e. Phase 3 Case 2 holds. Then, the amount of time \(t\geq T_{2}\) that elapses while the following conditions simultaneously hold:_1. \(|D_{4,t}|\geq\xi|D_{2,t}|\)__
2. \(r\leq\frac{1}{2}\)__
3. \(w_{t}({}_{\rm IR})\leq 1-\xi\)__
4. \(L(\rho_{t})\geq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\)__

_is at most \(\frac{1}{\hat{\sigma}_{4,d}^{2}\gamma_{2}^{1/2}\xi^{8}\kappa^{4}}\log\left(\frac {\gamma_{2}}{\epsilon}\right)\)._

Proof of Lemma D.32.: For convenience and ease of presentation, in this proof we define \(\tau:=\sqrt{\gamma_{2}}\), and \(\beta\geq 1.1\) such that \(\gamma_{4}=\beta\tau^{4}\). (Note that by Assumption 3.2, \(\beta\) is at most a universal constant.) We first show that there is always a large fraction of particles which are far away from the positive root \(r\) of \(P_{t}(w)\). We consider two cases: (i) \(r\leq\tau^{3/2}\) and (ii) \(r\geq\tau^{3/2}\).

**Case 1:**\(r\leq\tau^{3/2}\), Assume for the sake of contradiction that

\[\mathbb{P}_{w\sim\rho_{t}}(|w|\geq r+\tau^{3/2})\leq\tau^{4}\] (D.284)

Then,

\[\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{4}] \leq\mathbb{P}_{w\sim\rho_{t}}(|w|\geq r+\tau^{3/2})+\mathbb{P}_ {w\sim\rho_{t}}(|w|\leq r+\tau^{3/2})\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{4} \mid|w|\leq r+\tau^{3/2}]\] (D.285) \[=\tau^{4}+(r+\tau^{3/2})^{4}\] (D.286) \[\leq\tau^{4}+(2\tau^{3/2})^{4}\] (By assumption that \[r\leq\tau^{3/2}\] ) \[=\tau^{4}+16\tau^{6}\] (D.287)

Thus,

\[D_{4,t} =\mathop{\mathbb{E}}_{w\sim\rho_{t}}[P_{4,d}(w)]-\beta\tau^{4}\] (D.288) \[\leq\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{4}]+O\Big{(}\frac{1}{ d}\Big{)}-\beta\tau^{4}\] (By Eq. (C.4)) \[\leq\tau^{4}+16\tau^{6}+O\Big{(}\frac{1}{d}\Big{)}-\beta\tau^{4}\] (D.289) \[=16\tau^{6}+O\Big{(}\frac{1}{d}\Big{)}-(\beta-1)\tau^{4}\] (D.290) \[\leq 17\tau^{6}-(\beta-1)\tau^{4}\] (B.c. \[d\] is sufficiently large by Assumption 3.2 ) \[<0\] (B.c. \[(\beta-1)>17\tau^{2}\] by Assumption 3.2 )

which contradicts the assumption that we are in Phase 3, Case 2 since \(D_{4,t}>0\) by Lemma D.28. Thus, if \(r\leq\tau^{3/2}\), then

\[\mathbb{P}_{w\sim\rho_{t}}(|w|\geq r+\tau^{3/2})\geq\tau^{4}\] (D.291)

**Case 2:**\(r\geq\tau^{3/2}\). Suppose that

\[\mathbb{P}_{w\sim\rho_{t}}(r-\tau^{3/2}\leq|w|\leq r+\tau^{3/2}) \geq 1-\tau^{5}\] (D.292)

Then,

\[\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{2}] \geq\mathbb{P}_{w\sim\rho_{t}}(r-\tau^{3/2}\leq|w|\leq r+\tau^{3/ 2})\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{2}\mid|w|\in[r-\tau^{3/2},r+\tau^{3 /2}]]\] (D.293) \[\geq(1-\tau^{5})\cdot(r-\tau^{3/2})^{2}\] (D.294)

Since \(D_{2,t}<0\) by Lemma D.28, this implies that \(\mathbb{E}_{w\sim\rho_{t}}[w^{2}]\leq\tau^{2}+O(1/d)\) by Proposition D.38. Thus,

\[(1-\tau^{5})(r-\tau^{3/2})^{2}\leq\tau^{2}+O(1/d)\] (D.295)and taking square roots gives

\[(1-\tau^{5})^{1/2}(r-\tau^{3/2})\leq\tau+O(1/\sqrt{d})\] (D.296)

Thus,

\[r\leq\tau^{3/2}+\frac{1}{(1-\tau^{5})^{1/2}}\cdot\Big{(}\tau+O\Big{(}\frac{1}{ \sqrt{d}}\Big{)}\Big{)}\] (D.297)

On the other hand,

\[\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{4}] \leq\mathbb{P}_{w\sim\rho_{t}}(|w|\in[r-\tau^{3/2},r+\tau^{3/2}]) \mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{4}\mid|w|\in[r-\tau^{3/2},r+\tau^{3/2}]]\] (D.298) \[\qquad\qquad+\mathbb{P}_{w\sim\rho_{t}}(|w|\not\in[r-\tau^{3/2},r+ \tau^{3/2}])\] (D.299) \[\leq(r+\tau^{3/2})^{4}+\tau^{5}\] (By the assumption in Eq. (D.292))

and since \(D_{4,t}\geq 0\) by Lemma D.28, \(\mathop{\mathbb{E}}_{w\sim\rho_{t}}[w^{4}]\geq\beta\tau^{4}-O(1/d)\) by Eq. (C.4) and because \(w\sim\rho_{t}\) are bounded by \(1\) in absolute value. Thus, by the above equation,

\[(r+\tau^{3/2})^{4}+\tau^{5}\geq\beta\tau^{4}-O\Big{(}\frac{1}{d}\Big{)}\] (D.300)

Rearranging gives

\[(r+\tau^{3/2})^{4}+\tau^{5}+O\Big{(}\frac{1}{d}\Big{)}\geq\beta\tau^{4}\] (D.301)

and taking fourth roots gives

\[r+\tau^{3/2}+\tau^{5/4}+O\Big{(}\frac{1}{d^{1/4}}\Big{)}\geq\beta^{1/4}\tau\] (D.302)

(by the inequality \(\sqrt{x+y}\leq\sqrt{x}+\sqrt{y}\)). Therefore,

\[r\geq\beta^{1/4}\tau-\tau^{3/2}-\tau^{5/4}-O\Big{(}\frac{1}{d^{1/4}}\Big{)}\] (D.303)

Combining Eq. (D.297) and Eq. (D.303) gives

\[\beta^{1/4}\tau-\tau^{3/2}-\tau^{5/4}-O\Big{(}\frac{1}{d^{1/4}}\Big{)}\leq\tau ^{3/2}+\frac{1}{(1-\tau^{5})^{1/2}}\cdot\Big{(}\tau+O\Big{(}\frac{1}{\sqrt{d} }\Big{)}\Big{)}\] (D.304)

Rearranging this equation gives

\[\Big{(}\beta^{1/4}-\frac{1}{\sqrt{1-\tau^{5}}}\Big{)}\tau \leq 2\tau^{3/2}+\tau^{5/4}+O\Big{(}\frac{1}{d^{1/4}}\Big{)}+ \frac{1}{\sqrt{1-\tau^{5}}}\cdot O\Big{(}\frac{1}{\sqrt{d}}\Big{)}\] (D.305) \[\leq 3\tau^{5/4}+O\Big{(}\frac{1}{d^{1/4}}\Big{)}+\frac{1}{\sqrt{1- \tau^{5}}}\cdot O\Big{(}\frac{1}{\sqrt{d}}\Big{)}\] (B.c. \[\tau\leq 1\], we have \[\tau^{3/2}\leq\tau^{5/4}\] ) \[\leq 4\tau^{5/4}+\frac{1}{\sqrt{1-\tau^{5}}}\cdot O\Big{(}\frac{1}{ \sqrt{d}}\Big{)}\] (B.c. \[d\] is sufficiently large by Assumption 3.2 ) \[\leq 4\tau^{5/4}+O\Big{(}\frac{1}{\sqrt{d}}\Big{)}\] (B.c. \[\tau\leq\tfrac{1}{2}\] by Assumption 3.2 ) \[\leq 5\tau^{5/4}\] (B.c. \[d\] is sufficiently large by Assumption 3.2 )

Further rearranging gives

\[\beta^{1/4}-\frac{1}{\sqrt{1-\tau^{5}}}\leq 5\tau^{1/4}\] (D.306)

This contradicts Assumption 3.2 -- since \(\beta\geq 1.1\), the left-hand side is larger than an absolute constant, while the right-hand side can be made sufficiently small (since \(\gamma_{2}\leq c_{2}\) and \(c_{2}\) is chosen to be sufficiently small). Thus, in the case that \(r\geq\tau^{3/2}\), we obtain

\[\mathbb{P}_{w\sim\rho_{t}}(|w|\not\in[r-\tau^{3/2},r+\tau^{3/2}])\geq\tau^{5}\] (D.307)In summary, whether \(r\geq\tau^{3/2}\) or \(r\leq\tau^{3/2}\), we can conclude that

\[\mathbb{P}_{w\sim\rho_{t}}(|w|\not\in[r-\tau^{3/2},r+\tau^{3/2}])\geq\tau^{5}\] (D.308)

Further assume that \(w_{t}(\iota_{\mathfrak{h}})\leq 1-\xi\), and note that by Lemma D.28, for all \(\iota>\iota_{\mathfrak{h}}\), we have \(w_{t}(\iota)\geq\xi^{2}\kappa^{2}\). Thus, by Proposition D.6, with probability \(1-O(\kappa)\) under \(\rho_{t}\), we have \(\xi^{2}\kappa^{2}\leq|w|\leq 1-\xi\). By a union bound,

\[\mathbb{P}_{w\sim\rho_{t}}(||w|-r|\geq\tau^{3/2}\text{ and }\xi^{2}\kappa^{2}\leq|w| \leq 1-\xi)\geq\tau^{5}-O(\kappa)\geq\frac{\tau^{5}}{2}\] (D.309)

Suppose \(w\) satisfies \(||w|-r|\geq\tau^{3/2}\) and \(\xi^{2}\kappa^{2}\leq|w|\leq 1-\xi\), i.e. the conditions mentioned in Eq. (D.309), and write

\[P_{t}(w)=2\hat{\sigma}_{2,d}^{2}D_{2,t}w+4\hat{\sigma}_{4,d}^{2}D_{4,t}w^{3}= 4\hat{\sigma}_{4,d}^{2}D_{4,t}w(w-r)(w+r)\] (D.310)

Then,

\[|v_{t}(w)| \gtrsim|1-w^{2}|\cdot|P_{t}(w)+Q_{t}(w)|\] (By Lemma 4.4) \[\gtrsim|1+w|\cdot|1-w|\cdot|P_{t}(w)+Q_{t}(w)|\] (D.311) \[\gtrsim\xi|P_{t}(w)+Q_{t}(w)|\] (B.c. \[|w|\leq 1-\xi\] ) \[\gtrsim\xi\cdot\Big{|}4\hat{\sigma}_{4,d}^{2}D_{4,t}w(w-r)(w+r)- O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d} \Big{)}|w|\Big{|}\] (By Lemma 4.4) \[\gtrsim\xi\Big{(}4\hat{\sigma}_{4,d}^{2}|D_{4,t}|\xi^{2}\kappa^{2 }\cdot\tau^{3/2}\cdot\tau^{3/2}-O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+ \hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}\Big{)}\] (By triangle inequality and b.c. \[|w|\geq\xi^{2}\kappa^{2}\) and \(||w|-r|\geq\tau^{3/2}\)) \[\gtrsim\xi\Big{(}4\hat{\sigma}_{4,d}^{2}|D_{4,t}|\tau^{3}\xi^{2} \kappa^{2}-O\Big{(}\frac{\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{\xi d}\Big{)}\Big{)}\] (B.c. \[|D_{2,t}|\leq|D_{4,t}|/\xi\] and \[\hat{\sigma}_{2,d}^{2}\lesssim\hat{\sigma}_{4,d}^{2}\] by Assumption 3.2) \[\gtrsim\xi\cdot 4\hat{\sigma}_{4,d}^{2}|D_{4,t}|\tau^{3}\xi^{2}\kappa^{2}\] (B.c. \[1/(\xi d)=\frac{\log\log d}{d}\] and \[d\] is sufficiently large (Assumption 3.2)) \[\gtrsim\hat{\sigma}_{4,d}^{2}\tau^{3}\xi^{3}\kappa^{2}|D_{4,t}|\] (D.312)

and the average velocity is at least

\[\mathbb{E}_{w\sim\rho_{t}}|v_{t}(w)|^{2} \gtrsim\mathbb{P}_{w\sim\rho_{t}}(||w|-r|\geq\tau^{3/2}\text{ and }\xi^{2}\kappa^{2}\leq|w|\leq 1-\xi)\cdot\hat{\sigma}_{4,d}^{4}\tau^{6}\xi^{ 6}\kappa^{4}|D_{4,t}|^{2}\] (By Eq. (D.312)) \[\gtrsim\tau^{5}\cdot\hat{\sigma}_{4,d}^{4}\tau^{6}\xi^{6}\kappa^{4 }|D_{4,t}|^{2}\] (By Eq. (D.309)) \[\gtrsim\hat{\sigma}_{4,d}^{4}\tau^{11}\xi^{6}\kappa^{4}|D_{4,t}|^{2}\] (D.313) \[\gtrsim\hat{\sigma}_{4,d}^{2}\tau^{11}\xi^{6}\kappa^{4}(\xi^{2}L( \rho_{t}))\] (B.c. \[|D_{4,t}|\geq\xi|D_{2,t}|\] and \[\hat{\sigma}_{4,d}^{2}\gtrsim\hat{\sigma}_{2,d}^{2}\] by Assumption 3.2) \[\gtrsim\hat{\sigma}_{4,d}^{2}\tau^{11}\xi^{8}\kappa^{4}L(\rho_{t})\] (D.314)

Thus, by Lemma D.39,

\[\frac{dL(\rho_{t})}{dt}\lesssim-\hat{\sigma}_{4,d}^{2}\tau^{11}\xi^{8}\kappa^{ 4}L(\rho_{t})\] (D.315)

Since the initial loss is \(L(\rho_{0})\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\gamma_{2}^{2}\) by Lemma D.8, we can calculate using Gronwall's inequality (Fact I.13) that the time spent when the conditions in the statement of this lemma simultaneously hold is at most \(\frac{1}{\hat{\sigma}_{4,d}^{2}\tau^{11}\xi^{8}\kappa^{4}}\log\left(\frac{ \gamma_{2}}{\epsilon}\right)\), as desired. 

To conclude, we show Lemma D.33 which gives a bound on the running time during Phase 3, Case 2.

**Lemma D.33** (Phase 3, Case 2 Summary).: _Suppose we are in the setting of Theorem 3.3. Assume \(D_{4,T_{2}}=0\) and \(D_{2,T_{2}}<0\), i.e. Phase 3 Case 2 holds. Then, at most \(O\Big{(}\frac{1}{\hat{\sigma}_{2,d}^{2}\epsilon\gamma_{2}^{11/2}\xi^{8}\kappa^{ 12}}\log\left(\frac{\gamma_{2}}{\epsilon}\right)\Big{)}\) time \(t\geq T_{2}\) elapses while \(L(\rho_{t})\gtrsim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\)._Proof of Lemma D.33.: By combining Lemma D.29, Lemma D.30, Lemma D.31, and Lemma D.32, we find that the overall running time during Phase 3 Case 2 is at most

\[O\Big{(}\frac{1}{\hat{\sigma}_{2,d}^{2}\xi^{4}\kappa^{6}}\log\Big{(}\frac{\gamma_ {2}}{\epsilon}\Big{)}+\frac{1}{\hat{\sigma}_{2,d}^{2}\xi^{4}\kappa^{4}}\log \Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}+\frac{1}{\hat{\sigma}_{4,d}^{2} \epsilon\xi^{7}\kappa^{12}}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}+\frac {1}{\hat{\sigma}_{4,d}^{2}\gamma_{2}^{11/2}\xi^{8}\kappa^{4}}\log\Big{(}\frac{ \gamma_{2}}{\epsilon}\Big{)}\Big{)}\] (D.316)

and this can be upper bounded by \(O\Big{(}\frac{1}{\hat{\sigma}_{2,d}^{2}\epsilon\gamma_{2}^{11/2}\xi^{8}\kappa ^{12}}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\Big{)}\), where we have used the fact that \(\hat{\sigma}_{2,d}^{2}\asymp\hat{\sigma}_{4,d}^{2}\) (see Assumption 3.2). 

### Summary of Phase 3

The following lemma provides a bound on the running time after \(T_{2}\) that is required for \(L(\rho_{t})\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\epsilon^{2}\).

**Lemma D.34** (Phase 3).: _Suppose we are in the setting of Theorem 3.3. Then, \(T_{*,\epsilon}-T_{2}\lesssim\frac{(\log\log d)^{20}}{\hat{\sigma}_{2,d}^{2} \epsilon\gamma_{2}^{2}}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\)._

Proof of Lemma D.34.: By combining Lemma D.22 and Lemma D.33, we find that the overall running time during Phase 3 is at most

\[\max\Big{(}O\Big{(}\frac{1}{\hat{\sigma}_{4,d}^{2}\gamma_{2}^{8}\xi^{4}\kappa ^{6}}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\Big{)},O\Big{(}\frac{1}{ \hat{\sigma}_{2,d}^{2}\epsilon\gamma_{2}^{11/2}\xi^{8}\kappa^{12}}\log\Big{(} \frac{\gamma_{2}}{\epsilon}\Big{)}\Big{)}\Big{)}\] (D.317)

and this is at most \(O\Big{(}\frac{1}{\hat{\sigma}_{2,d}^{2}\epsilon\gamma_{2}^{2}\xi^{8}\kappa^{12 }}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\Big{)}\), where we have used the fact that \(\hat{\sigma}_{2,d}^{2}\asymp\hat{\sigma}_{4,d}^{2}\) (see Assumption 3.2) 

### Proof of Main Theorem for Population Case

Finally, we prove Theorem 3.3:

Proof of Theorem 3.3.: We combine Lemma D.5, Lemma D.10 and Lemma D.34 to find that the total runtime is at most

\[T_{*,\epsilon}\lesssim\frac{1}{\hat{\sigma}_{2,d}^{2}\gamma_{2}}\log d+\frac{ (\log\log d)^{18}}{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})}\log\Big{(} \frac{\gamma_{2}}{\epsilon}\Big{)}+\frac{(\log\log d)^{20}}{\hat{\sigma}_{2,d}^ {2}\epsilon\gamma_{2}^{8}}\log\Big{(}\frac{\gamma_{2}}{\epsilon}\Big{)}\] (D.318)

and we can further simplify the above bound to obtain:

\[T_{*,\epsilon}\lesssim\frac{1}{\hat{\sigma}_{2,d}^{2}\gamma_{2}}\log d+\frac{ (\log\log d)^{20}}{\hat{\sigma}_{2,d}^{2}\epsilon\gamma_{2}^{8}}\log\Big{(} \frac{\gamma_{2}}{\epsilon}\Big{)}\] (D.319)

where we have used the fact that \(\hat{\sigma}_{2,d}^{2}\asymp\hat{\sigma}_{4,d}^{2}\) by Assumption 3.2. 

### Additional Lemmas

**Lemma D.35**.: _Suppose Assumption 3.2 holds. There exists some \(r\), with \(r\gtrsim\frac{1}{\log\log d}\) and \(r\leq 1\), such that if \(\rho\) is the singleton distribution under which \(r\) has probability \(1\), then the velocity at \(r\) is \(0\)._

Proof.: Let \(\rho\) be such that \(r\) has probability \(1\) under \(\rho\), for some \(r\in(0,1)\). Applying the definitions of \(D_{2}\) and \(D_{4}\) to \(\rho\), we have \(D_{2}=P_{2,d}(r)-\gamma_{2}\) and \(D_{4}=P_{4,d}(r)-\gamma_{4}\). By Lemma D.3, the velocity of \(r\) is

\[v(r)=-(1-r^{2})(\hat{\sigma}_{2,d}^{2}D_{2}P_{2,d}{}^{\prime}(r)+\hat{\sigma}_{ 4,d}^{2}D_{4}P_{4,d}{}^{\prime}(r))\] (D.320)

The second factor is equal to

\[\hat{\sigma}_{2,d}^{2}(P_{2,d}(r)-\gamma_{2})P_{2,d}{}^{\prime}(r)+\hat{\sigma}_ {4,d}^{2}(P_{4,d}(r)-\gamma_{4})P_{4,d}{}^{\prime}(r)\] (D.321)and we denote this polynomial of \(r\) by \(F(r)\). Note that \(P_{2,d}(1)=P_{4,d}(1)=1\) (see Equation (2.20) of Atkinson and Han [12]). Thus, \(F(1)>0\) by Assumption 3.2, since \(P_{2,d^{\prime}}(1)>0\) and \(P_{4,d^{\prime}}(1)>0\) by Eq. (C.4). On the other hand, suppose \(r\asymp\frac{1}{\log\log d}\). Then,

\[|P_{2,d}(r)| =\Big{|}\frac{d}{d-1}r^{2}-\frac{1}{d-1}\Big{|}\] (By Eq. (C.4)) \[\lesssim\frac{1}{(\log\log d)^{2}}\] (D.322)

Similarly, \(|P_{4,d}(r)|\lesssim\frac{1}{(\log\log d)^{4}}\). Thus, \(P_{2,d}(r)-\gamma_{2}<0\) and \(P_{4,d}(r)-\gamma_{4}<0\) since \(d\) can be chosen to be sufficiently large according to Assumption 3.2. However, by Eq. (C.4), one can show that \(P_{2,d^{\prime}}(r)\gtrsim\frac{1}{\log\log d}\) and \(P_{4,d^{\prime}}(r)\gtrsim\frac{1}{(\log\log d)^{3}}\) as long as \(d\) is sufficiently large. Thus, \(F(r)<0\) for \(r\asymp\frac{1}{\log\log d}\). In summary, \(F\) has a root between \(\frac{C}{\log\log d}\) and \(1\) for some universal constant \(C\), and for this value of \(r\), the velocity of \(r\) is \(0\). 

**Lemma D.36**.: _In the setting of Theorem 3.3, \(T_{*,\epsilon}-T_{1}\lesssim\frac{(\log\log d)^{20}}{\hat{\sigma}_{2,d}^{2} \epsilon\gamma_{2}^{2}}\log\left(\frac{\gamma_{2}}{\epsilon}\right)\). Thus, \((\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\gamma_{2}(T_{*,\epsilon}-T_{ 1})\lesssim\frac{(\log\log d)^{20}}{\epsilon\gamma_{2}^{2}}\log\left(\frac{ \gamma_{2}}{\epsilon}\right)\)._

Proof.: This follows from Lemma D.10 and Lemma D.34. 

### Helper Lemmas

In this subsection, we give some convenient facts that we use in the following subsections.

**Proposition D.37**.: _If \(0\leq\iota_{1}<\iota_{2}\), then for all \(t\geq 0\), \(w_{t}(\iota_{1})<w_{t}(\iota_{2})\)._

Proof of Proposition d.37.: Assume this is not the case, and let \(t\) be the infimum of all times \(s\) such that \(w_{s}(\iota_{1})\geq w_{s}(\iota_{2})\). Then, \(w_{t}(\iota_{1})=w_{t}(\iota_{2})\), meaning that for all \(t^{\prime}\geq t\), \(w_{t^{\prime}}(\iota_{1})=w_{t^{\prime}}(\iota_{2})\). For convenience, let \(w_{0}=w_{t}(\iota_{1})=w_{t}(\iota_{2})\). Note that \(t>0\) since \(\iota_{1}\neq\iota_{2}\). However, this implies that for \(t^{\prime}\in(0,t)\), \(w_{t^{\prime}}(\iota_{1})<w_{t^{\prime}}(\iota_{2})\), meaning that for any \(\delta>0\), \(w_{s}(\iota_{1})\) and \(w_{s}(\iota_{2})\) are non-identical solutions to the initial value problem on the time interval \((t-\delta,t+\delta)\) given by the conditions \(f(t)=w_{0}\) and \(\frac{df}{dt}=v_{t}(f(t))\). This contradicts the Picard-Lindelof theorem, and thus there cannot exist a time \(t\) such that \(w_{t}(\iota_{1})\geq w_{t}(\iota_{2})\). 

**Proposition D.38**.: _Suppose we are in the setting of Theorem 3.3, and let \(t\geq 0\) such that \(D_{2,t}<0\). Then, \(\mathbb{E}_{w\sim\rho_{t}}[w^{2}]\leq\gamma_{2}+O\Big{(}\frac{1}{d}\Big{)}\)._

Proof of Proposition d.37.: Suppose \(D_{2,t}\leq 0\). Then, \(\mathbb{E}_{w\sim\rho_{t}}[P_{2,d}(w)]\leq\gamma_{2}\), which implies that

\[\frac{d}{d-1}\operatorname*{\mathbb{E}}_{w\sim\rho_{t}}[w^{2}]-\frac{1}{d-1} \leq\gamma_{2}\] (D.323)

and rearranging gives

\[\operatorname*{\mathbb{E}}_{w\sim\rho_{t}}[w^{2}]\leq\frac{d-1}{d}\gamma_{2}+ \frac{1}{d}\leq\gamma_{2}+O\Big{(}\frac{1}{d}\Big{)}\] (D.324)

as desired. 

**Lemma D.39** (Decrease in Loss and Average Velocity).: _Suppose we are in the setting of Theorem 3.3. Then, \(\frac{dL}{dt}=-\operatorname*{\mathbb{E}}_{u\sim\rho_{t}}[\|\text{grad}_{u}L( \rho_{t})\|_{2}^{2}]\)._

We note that a similar result was shown in Lemma E.11 of Wei et al. [75], but for gradient flow rather than projected gradient flow.

Proof of Lemma d.39.: First, we can calculate

\[\frac{d}{dt}L(\rho_{t})=\frac{1}{2}\frac{d}{dt}\operatorname*{\mathbb{E}}_{x \sim\mathbb{S}^{d-1}}[(f_{\rho_{t}}(x)-y(x))^{2}]=\operatorname*{\mathbb{E}}_ {x\sim\mathbb{S}^{d-1}}\left[(f_{\rho_{t}}(x)-y(x))\frac{d}{dt}f_{\rho_{t}}(x) \right].\] (D.325)

[MISSING_PAGE_EMPTY:64]

Proof of Lemma 5.1.: For the first statement, for any fixed \(t\leq T\), by Lemma E.13, we have

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho_{t}}(x)-f_{\bar{\rho} _{t}}(x))^{2}]\lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(d^{2 }+\log\frac{1}{\delta})(\log d)^{C}}{m}+\frac{\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2}}{d^{\Omega(\log d)}}\] (E.5)

with probability \(1-\delta\) -- we will choose \(\delta\) shortly. In addition, for any \(t,t^{\prime}\in[0,T]\), by Lemma E.14, we have that \(\|u_{t}(\chi)-u_{t^{\prime}}(\chi)\|_{2}\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2})d^{4}|t-t^{\prime}|\), which by Lemma E.12 implies that for any \(x\in\mathbb{S}^{d-1}\),

\[|f_{\rho_{t}}(x)-f_{\rho_{t^{\prime}}}(x)| \leq\operatorname*{\mathbb{E}}_{\chi\sim\rho_{0}}|\sigma(u_{t}( \chi)\cdot x)-\sigma(u_{t^{\prime}}(\chi)\cdot x)|\] (E.6) \[\lesssim(|\hat{\sigma}_{2,d}|\sqrt{N_{2,d}}+|\hat{\sigma}_{4,d}| \sqrt{N_{4,d}})\cdot(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{4}|t-t^{ \prime}|\] (E.7)

In particular, if \(|t-t^{\prime}|\leq\frac{1}{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{ \Omega(\log d)}}\), then for all \(x\in\mathbb{S}^{d-1}\) we have

\[|f_{\rho_{t}}(x)-f_{\rho_{t^{\prime}}}(x)|\lesssim\frac{|\hat{\sigma}_{2,d}|+| \hat{\sigma}_{4,d}|}{d^{\Omega(\log d)}}\] (E.8)

meaning that

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho_{t}}(x)-f_{\rho_{ t^{\prime}}}(x))^{2}]\lesssim\frac{\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2} }{d^{\Omega(\log d)}}\] (E.9)

By the same argument, we have that

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\bar{\rho}_{t}}(x)-f_{ \bar{\rho}_{t^{\prime}}}(x))^{2}]\lesssim\frac{\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2}}{d^{\Omega(\log d)}}\] (E.10)

Now, we perform a union bound over all times \(t\) such that \(t\leq T\) and \(t\) is an integer multiple of \(\frac{1}{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{\Omega\log d}}\) for some sufficiently large absolute constant \(H>0\). Since \(T\leq\frac{d^{C}}{\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}}\), the number of such times is at most \(d^{C+H\log d}\). Thus, with probability at least \(1-\delta\cdot d^{C+H\log d}\), we have

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho_{t}}(x)-f_{\bar{ \rho}_{t}}(x))^{2}]\lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{ 2})(d^{2}+\log\frac{1}{\delta})(\log d)^{O(1)}}{m}+\frac{\hat{\sigma}_{2,d}^{2} +\hat{\sigma}_{4,d}^{2}}{d^{\Omega(\log d)}}\] (E.11)

for all \(t\in[0,T]\). In particular, in order to have exponentially small failure probability, we set \(\delta=\frac{1}{e^{d^{2}}d^{C+H\log d}}\), and by our assumption that \(m\leq d^{O(\log d)}\), we obtain

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho_{t}}(x)-f_{\bar{ \rho}_{t}}(x))^{2}]\lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2} )d^{2}(\log d)^{O(1)}}{m}\] (E.12)

for all \(t\leq T\), with probability at least \(1-e^{-d^{2}}\), since \(\log\frac{1}{\delta}=d^{2}+O((\log d)^{2})\). This completes the proof of the first statement of the lemma. Additionally, by Lemma E.16, we have

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}_{t}}(x)-f_{ \bar{\rho}_{t}}(x))^{2}]\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2 })\operatorname*{\mathbb{E}}_{(\bar{u},\bar{u})\sim\Gamma}[\|\hat{u}_{t}-\bar{u }_{t}\|_{2}^{2}]\] (E.13)

which proves the second statement of the lemma. 

The rest of the section is dedicated to prove Lemma 5.2. To begin with, we first introduce the following lemma which formalizes the decomposition of \(\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\) discussed in Section 5.

**Lemma E.1** (Decomposition of \(\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\)).: _Suppose we are in the setting of Theorem 3.4. Then, \(\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}=A_{t}+B_{t}+C_{t}\), where \(A_{t}=-2\langle\text{\rm grad}_{\hat{u}}L(\rho_{t})-\text{\rm grad}_{\hat{u}}L( \rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle\), \(B_{t}=-2\langle\text{\rm grad}_{\hat{u}}L(\hat{\rho}_{t})-\text{\rm grad}_{\hat{u}}L (\rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle\), and \(C_{t}=-2\langle\text{\rm grad}_{\hat{u}}L(\hat{\rho}_{t})-\text{\rm grad}_{\hat{u}}L (\hat{\rho}_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle\)._

Proof of Lemma e.1.: By the chain rule,

\[\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}=2\Big{\langle}\frac{d}{dt}\hat{u }_{t}-\frac{d}{dt}\bar{u}_{t},\hat{u}_{t}-\bar{u}_{t}\Big{\rangle}=-2\langle \text{\rm grad}_{\hat{\rho}}\hat{L}(\hat{\rho}_{t})-\text{\rm grad}_{\hat{u}}L( \rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle\] (E.14)We can expand the difference in gradients as

\[\text{grad}_{\hat{\rho}}\widehat{L}(\hat{\rho}_{t})-\text{grad}_{ \bar{u}}L(\rho_{t}) =\Big{(}\text{grad}_{\bar{u}}\widehat{L}(\hat{\rho}_{t})-\text{grad}_{ \bar{u}}L(\hat{\rho}_{t})\Big{)}+\Big{(}\text{grad}L_{\bar{u}}(\hat{\rho}_{t})- \text{grad}L_{\bar{u}}(\rho_{t})\Big{)}\] (E.15) \[\qquad\qquad\qquad+\Big{(}\text{grad}_{\bar{u}}L(\rho_{t})-\text{ grad}_{\bar{u}}L(\rho_{t})\Big{)}\] (E.16)

Thus,

\[\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2} =-2\langle\text{grad}_{\bar{u}}\widehat{L}(\hat{\rho}_{t})-\text{ grad}_{\bar{u}}L(\hat{\rho}_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle-2\langle\text{ grad}L_{\bar{u}}(\hat{\rho}_{t})-\text{grad}L_{\bar{u}}(\rho_{t}),\hat{u}_{t}- \bar{u}_{t}\rangle\] (E.17) \[\qquad-2\langle\text{grad}_{\bar{u}}L(\rho_{t})-\text{grad}_{\bar {u}}L(\rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle\] (E.18) \[=C_{t}+B_{t}+A_{t}\] (E.19)

as desired. 

Next, we give a proof of Lemma 5.4, which gives an upper bound for each of \(A_{t}\), \(B_{t}\) and \(C_{t}\).

Proof of Lemma 5.4.: The proof directly follows from Lemma E.8, Lemma E.7, Lemma E.9, Lemma E.10 and Lemma E.11. 

Combining the upper bounds in Lemma 5.4 and the decomposition in Lemma E.1, we have the following Lemma E.2 which upper bounds the growth of \(\|\hat{u}_{t}-\bar{u}_{t}\|\). Recall that \(T_{*,\epsilon}\) is the total runtime for the algorithm (see Theorem 3.3), and \(T_{1}\) is the time such that \(w_{T_{1}}(u_{\text{U}})=\frac{1}{\log d}\) (see Definition 4.5).

**Lemma E.2** (Growth Rate of \(\|\delta_{t}\|_{2}^{2}\) With Dependency on \(n\)).: _In the setting of Theorem 3.4, suppose that the inductive hypothesis in Assumption 5.3 holds for some \(\phi\) and \(\psi\) up until \(t\) for some \(t\leq T_{*,\epsilon}\). Further assume that \(\psi<\frac{1}{2}\) and \(\phi>\frac{1}{2}\). Then, when \(t\leq T_{1}\) we have_

\[\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2} \leq\Big{(}4\hat{\sigma}_{2,d}^{2}|D_{2,t}|\Big{(}1+O\Big{(}\frac {1}{\log d}\Big{)}\Big{)}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\] (E.20) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\cdot\frac{d(\log d )^{O(1)}}{\sqrt{m}}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}\] (E.21) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)} \Big{(}\frac{1}{d^{\phi}}+\sqrt{\frac{d}{n}}+\frac{d^{2.5-2\psi-2\phi}}{n} \Big{)}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}\] (E.22) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)} \Big{(}\frac{d^{2-2\psi-\phi}}{n}+\frac{d^{4-4\psi-2\phi}}{n}\Big{)}\|\hat{u}_ {t}-\bar{u}_{t}\|_{2}^{2}\] (E.23)

_and_

\[\frac{d}{dt}\overline{\Delta}_{t}^{2} \leq\Big{(}4\hat{\sigma}_{2,d}^{2}|D_{2,t}|\Big{(}1+O\Big{(}\frac {1}{\log d}\Big{)}\Big{)}\overline{\Delta}_{t}^{2}\] (E.24) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\cdot\frac{d(\log d )^{O(1)}}{\sqrt{m}}\overline{\Delta}_{t}\] (E.25) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)} \Big{(}\sqrt{\frac{d}{n}}+\frac{d^{2.5-2\psi-2\phi}}{n}\Big{)}\overline{ \Delta}_{t}\] (E.26) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)} \Big{(}\frac{1}{d^{\phi}}+\frac{d^{2-2\psi-\phi}}{n}+\frac{d^{4-4\psi-2\phi}}{ n}\Big{)}\overline{\Delta}_{t}^{2}\,.\] (E.27)

_When \(t\in[T_{1},T_{*,\epsilon}]\), we have_

\[\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2} \lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\gamma_{2} \|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\] (E.28) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\cdot\frac{d(\log d )^{O(1)}}{\sqrt{m}}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}\] (E.29) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)} \Big{(}\frac{1}{d^{\phi}}+\sqrt{\frac{d}{n}}+\frac{d^{2.5-2\psi-2\phi}}{n} \Big{)}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}\] (E.30) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)} \Big{(}\frac{d^{2-2\psi-\phi}}{n}+\frac{d^{4-4\psi-2\phi}}{n}\Big{)}\|\hat{u}_ {t}-\bar{u}_{t}\|_{2}^{2}\] (E.31)\[\frac{d}{dt}\overline{\Delta}_{t}^{2} \leq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\gamma_{2} \overline{\Delta}_{t}^{2}\] (E.32) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\cdot\frac{d(\log d )^{O(1)}}{\sqrt{m}}\overline{\Delta}_{t}\] (E.33) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)} \Big{(}\sqrt{\frac{d}{n}}+\frac{d^{2.5-2\psi-2\phi}}{n}\Big{)}\overline{\Delta }_{t}\] (E.34) \[+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)} \Big{(}\frac{1}{d^{\phi}}+\frac{d^{2-2\psi-\phi}}{n}+\frac{d^{4-4\psi-2\phi}}{ n}\Big{)}\overline{\Delta}_{t}^{2}\,.\] (E.35)

Proof of Lemma e.2.: This result follows directly from Lemma 5.4 and the inductive hypothesis. By the inductive hypothesis, we have \(\overline{\Delta}_{t}\leq d^{-\phi}\). We apply this to the bound for \(C_{t}\) in Lemma 5.4 to obtain the desired bounds above. Note that in our bounds for \(\frac{d}{dt}\overline{\Delta}_{t}^{2}\), it is important that we use the upper bound for \(\mathbb{E}[B_{t}]\) in Lemma 5.4 instead of the upper bound for \(B_{t}\). 

Using this bound on the growth rate, we can show that the inductive hypothesis is maintained.

**Lemma E.3** (IH is Maintained).: _Suppose we are in the setting of Theorem 3.4. Suppose \(\psi>0\) and \(1>\phi>\frac{1}{2}+\psi\). Let \(n=d^{\mu}(\log d)^{C}\) where \(\mu\geq\max\{2+2\phi,4-2\phi-4\psi\}\) and \(C\) is a sufficiently large universal constant. Suppose the inductive hypothesis in Assumption 5.3 holds for all \(t\leq T\) where \(T<T_{*,e}\). Then, when \(m\gtrsim d^{3+2\phi}(\log d)^{\Omega(1)}\) and \(m\leq d^{C}\) for a sufficiently large universal constant \(C\), we have that the inductive hypothesis holds for all \(t\leq T+c_{\text{\rm H}}\) where \(c_{\text{\rm H}}>0\) is a constant that only depends on \(d\), \(\phi\), \(\psi\), \(\hat{\sigma}_{2,d}\), \(\hat{\sigma}_{4,d}\)._

Proof.: Suppose the inductive hypothesis holds for all \(t\leq T\) where \(T\leq T_{1}\). Then, by Lemma E.2 we have

\[\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2} \leq\Big{(}4\hat{\sigma}_{2,d}^{2}|D_{2,t}|\Big{(}1+O\Big{(} \frac{1}{\log d}\Big{)}\Big{)}\Big{)}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\] (E.36) \[\qquad\qquad+\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2 })d(\log d)^{O(1)}}{\sqrt{m}}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}\] (E.37) \[\qquad\qquad+O\Big{(}\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_ {4,d}^{2})(\log d)^{O(1)}}{d^{\phi}}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}\Big{)}\] (E.38) \[\leq\Big{(}4\hat{\sigma}_{2,d}^{2}|D_{2,t}|\Big{(}1+O\Big{(} \frac{1}{\log d}\Big{)}\Big{)}\Big{)}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\] (E.39) \[\qquad\qquad+O\Big{(}\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_ {4,d}^{2})(\log d)^{O(1)}}{d^{\phi}}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}\Big{)}\,.\] (E.40)

Here we have used the fact that \(n=d^{\mu}(\log d)^{O(1)}\) where \(\mu\geq\max(4-4\psi-2\phi,2+2\phi)\) as well as the fact that \(\phi>\frac{1}{2}\), as follows:

* In the third term in the bound for \(\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\) from Lemma E.2, only the \(\frac{1}{d^{\phi}}\) term is important -- this term dominates the \(\sqrt{\frac{d}{n}}\) and \(\frac{d^{2.5-2\psi-2\phi}}{n}\) terms, using the fact that \(\mu>2+2\phi\).
* In the fourth term in the bound for \(\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\), the \(\frac{d^{2-2\psi-\phi}}{n}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\) term is dominated by the \(\frac{1}{d^{\phi}}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}\) term -- this is because \(\mu\geq 2+2\phi\).
* Since \(\mu\geq 4-4\psi-2\phi\), if we increase \(n\) by a \((\log d)^{O(1)}\) factor, then the \(\frac{d^{4-4\psi-2\phi}}{n}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\) term can be absorbed into the first term, as it is dominated by the \(4\hat{\sigma}_{2,d}^{2}|D_{2,t}|O\Big{(}\frac{1}{\log d}\Big{)}\|\hat{u}_{t}- \bar{u}_{t}\|_{2}^{2}\) term. (Recall that by Lemma D.8, \(|D_{2,t}|\gtrsim\gamma_{2}\) during Phase 1).

In the second inequality above, we have used the fact that \(m\gtrsim d^{3+2\phi}(\log d)^{\Omega(1)}\) -- this implies that the term involving \(m\) is dominated by the \(\frac{(\log d)^{O(1)}}{d^{\phi}}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}\) term.

Additionally, by Lemma E.2, we have

\[\frac{d}{dt}\overline{\Delta}_{t}^{2}\leq\Big{(}4\hat{\sigma}_{2,d}^{2}|D_{2,t}|\Big{(}1+O\Big{(}\frac{1}{\log d}\Big{)}\Big{)}\Big{)} \overline{\Delta}_{t}^{2}+\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2} )\cdot d(\log d)^{O(1)}}{\sqrt{m}}\overline{\Delta}_{t}\] (E.41) \[\qquad\qquad\qquad+O\Big{(}\frac{(\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2})}{d^{0.5+\phi}(\log d)^{O(1)}}\overline{\Delta}_{t}\Big{)}\] (E.42) \[\leq\Big{(}4\hat{\sigma}_{2,d}^{2}|D_{2,t}|\Big{(}1+O\Big{(} \frac{1}{\log d}\Big{)}\Big{)}\Big{)}\overline{\Delta}_{t}^{2}+O\Big{(}\frac {(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})}{d^{0.5+\phi}(\log d)^{O(1)} }\overline{\Delta}_{t}\Big{)}\,.\] (E.43)

Here, we apply the bound for \(\frac{d}{dt}\overline{\Delta}_{t}^{2}\) from Lemma E.2. In the third term in this bound, the \(\sqrt{\frac{d}{n}}\) term dominates the \(\frac{d^{2.5-2\phi-2\phi}}{n}\) term, using the fact that \(\phi>\frac{1}{2}\) and \(\mu\geq 2+2\phi>3\), meaning \(\frac{d^{2.5-2\phi-2\phi}}{n}\leq\frac{d^{1.5}}{n}\leq\frac{1}{\sqrt{n}}\). In the fourth term, we deal with the \(\frac{d^{4-\phi-2\phi}}{n}\) term similarly to our calculations for the bound for \(\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\). We can also deal with the \(\frac{d^{2-2\psi-\phi}}{n}\) term similarly, since by our definition of \(\mu\), this will be at most \(\frac{1}{d^{3\phi}}\leq\frac{1}{(\log d)^{O(1)}}\), and will thus be absorbed into the first term involving \(|D_{2,t}|\). Using \(\mu\geq 2+2\phi\), we then obtain \(\sqrt{\frac{d}{n}}\leq\frac{1}{d^{0.5+\phi}}\) as the coefficient of the \(\overline{\Delta}_{t}\) term (i.e. the first-order term involving \(\overline{\Delta}_{t}\)). Note that here we need to have \(m\gtrsim d^{3+2\phi}(\log d)^{\Omega(1)}\) in order to absorb the term involving \(m\) into the \(\frac{1}{d^{0.5+\phi}}\overline{\Delta}_{t}\) term.

Now using Lemma I.12 we have

\[\overline{\Delta}_{t}^{2}\lesssim\frac{1}{(\log d)^{O(1)}}\Big{(} \frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})O(\frac{1}{d^{0.5+\phi}} )}{4\hat{\sigma}_{2,d}^{2}\min_{t}|D_{2,t}|}\Big{)}^{2}\exp\Big{(}\int_{0}^{T} 4\hat{\sigma}_{2,d}^{2}|D_{2,t}|\Big{(}1+O\Big{(}\frac{1}{\log d}\Big{)}\Big{)} dt\Big{)}\] (E.44)

and

\[\Delta_{\text{max},T}^{2}\lesssim(\log d)^{O(1)}\Big{(}\frac{( \hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})O(\frac{1}{d^{0}})}{4\hat{ \sigma}_{2,d}^{2}\min_{t}|D_{2,t}|}\Big{)}^{2}\exp\Big{(}\int_{t}^{T}4\hat{ \sigma}_{2,d}^{2}|D_{2,t}|\Big{(}1+O\Big{(}\frac{1}{\log d}\Big{)}\Big{)}dt\Big{)}\] (E.45)

By Lemma D.5 we have

\[\exp\Big{(}\int_{t=0}^{T}4\hat{\sigma}_{2,d}^{2}|D_{2,t}|dt\Big{)}\lesssim d\] (E.46)

and by Lemma D.8 we know that \(|D_{2,t}|\gtrsim\gamma_{2}\) during Phase 1. Thus,

\[\Delta_{\text{max},T}^{2}\leq O\Big{(}\frac{(\log d)^{O(1)}}{ \gamma_{2}^{2}d^{2\phi-1}}\Big{)}.\] (E.47)

Since \(2\phi-1>2\psi\) and \(d\) is chosen to be sufficiently large after \(\gamma_{2}\) is chosen (Assumption 3.2), we have that \(\Delta_{\text{max},T}<\sqrt{\frac{1}{d^{\psi+\phi-1/2}}}=\frac{1}{d^{\frac{1}{2 }\phi+\frac{1}{2}\psi-\frac{1}{4}}}<\frac{1}{d^{\psi}}\). Note that \(\Delta_{\text{max},t}\) is continuous in \(t\) and we have

\[\frac{d}{dt}\Delta_{\text{max},t} \leq\max_{\chi}\frac{d}{dt}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}\] (E.48) \[=\max_{\chi}\frac{1}{\|\hat{u}_{t}-\bar{u}_{t}\|_{2}}\frac{d}{dt} \|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\] (E.49) \[\leq\max_{\chi}\frac{1}{\|\hat{u}_{t}-\bar{u}_{t}\|_{2}}|\langle\hat {u}_{t}-\bar{u}_{t},\text{grad}_{\hat{u}_{t}}\widehat{L}(\hat{u}_{t})-\text{ grad}_{\hat{u}_{t}}L(\bar{u}_{t})\rangle|\] (E.50) \[\leq\max_{\chi}\|\text{grad}_{\hat{u}_{t}}\widehat{L}(\hat{u}_{t} )-\text{grad}_{\hat{u}_{t}}L(\bar{u}_{t})\|_{2}\] (By Cauchy-Schwarz Inequality) \[\leq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{4}\,.\] (By Lemma E.14)Thus, for \(\mu_{1}=\left(\frac{1}{d^{\psi}}-\frac{1}{d^{\phi}(\log d)^{O(1)}}\right)\cdot \frac{1}{(\widehat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{4}}\) we still have the inductive hypothesis for \(\Delta_{\max}\) until time \(T+\mu_{1}\), that is, \(\Delta_{\max,T+\mu_{1}}\leq\frac{1}{d^{\psi}}\). Similarly, our calculation above using Lemma I.12 gives us

\[\overline{\Delta}_{T}^{2}\leq O\Big{(}\frac{1}{\gamma_{2}^{2}d^{2\phi}(\log d) ^{O(1)}}\Big{)}\] (E.51)

Thus, we have \(\overline{\Delta}_{T}<\frac{1}{d^{\phi}(\log d)^{O(1)}}<\frac{1}{d^{\phi}}\) (where in the middle expression, we choose a smaller power of \((\log d)^{O(1)}\) so that the inequality holds). Observe that \(\overline{\Delta}_{t}\) is continuous in \(t\), and

\[\frac{d}{dt}\overline{\Delta}_{t} =\frac{1}{\overline{\Delta}_{t}}\frac{d}{dt}\overline{\Delta}_{t }^{2}\] (E.52) \[=\frac{1}{\overline{\Delta}_{t}}\operatorname*{\mathbb{E}}[( \hat{u}_{t}-\bar{u}_{t},\hat{u}_{t}^{\prime}-\bar{u}_{t}^{\prime})]\] (E.53) \[\leq\frac{1}{\overline{\Delta}_{t}}\operatorname*{\mathbb{E}}[ \|\hat{u}_{t}-\bar{u}_{t}\|_{2}\|\hat{u}_{t}^{\prime}-\bar{u}_{t}^{\prime}\|_ {2}]\] (By Cauchy-Schwarz Inequality) \[\leq\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{4}}{ \overline{\Delta}_{t}}\operatorname*{\mathbb{E}}[\|\hat{u}_{t}-\bar{u}_{t}\|_ {2}]\] (By Lemma E.14) \[\leq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{4}\;.\] (E.54)

Thus, for \(\mu_{2}=\left(\frac{1}{d^{\phi}}-\frac{1}{d^{\phi}(\log d)^{O(1)}}\right)\cdot \frac{1}{(\widehat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{4}}\) we still have the inductive hypothesis for \(\overline{\Delta}\) until time \(T+\mu_{2}\), i.e. \(\overline{\Delta}_{T+\mu_{2}}\leq\frac{1}{d^{\psi}}\). As a result, assuming the inductive hypothesis holds up to time \(T\), it holds up to time \(T+\min(\mu_{1},\mu_{2})\).

Our above proof strategy applies to all times \(T\leq T_{1}\). For the induction when \(T\in[T_{1},T_{*,\epsilon}]\) we follow a similar argument but instead use the bound on \(A_{t}\) obtained from Lemma E.8. Since we know from Lemma D.36 and the assumption that \(\epsilon=O(\frac{1}{\log\log d})\) that

\[\int_{T_{1}}^{T_{*,\epsilon}}(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) \gamma_{2}dt\leq(T_{*,\epsilon}-T_{1})(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\gamma_{2}\leq\frac{\text{poly}(\log\log d)}{\gamma_{2}^{O(1)}},\] (E.55)

we know that this additional phase will at most add another \(\exp(\text{poly}(\log\log d))\) factor to the growth of \(\Delta_{\max,t}\) which is smaller than any polynomial in \(d\). Thus, Eq. (E.47) and Eq. (E.51) still hold for this period, which finishes the proof. 

Finally, we prove Lemma 5.2:

Proof of Lemma 5.2.: The proof directly follows Lemma E.3 with \(\phi=\frac{\mu-1}{4}\) and \(\psi=\frac{\mu-3}{8}\). 

### Upper bound on \(A_{t}\)

The goal of this section is to obtain an upper bound on

\[A_{t}(\chi):=-2\langle\text{grad}_{\hat{u}}L(\rho_{t})-\text{grad}_{\hat{u}}L( \rho_{t}),\hat{u}_{t}(\chi)-\bar{u}_{t}(\chi)\rangle\,.\] (E.56)

When there is no risk of confusion, we simply use \(A_{t}\), \(\hat{u}_{t}\), and \(\bar{u}_{t}\), omitting the dependence on \(\chi\).

We first obtain a strong upper bound on this inner product during Phase 1 of the population projected gradient flow. To obtain an upper bound on this inner product, we separately consider the contributions from the first coordinates of the vectors, and the remaining \((d-1)\) coordinates -- in other words, the gradients with respect to \(w\) and the gradients with respect to \(z\). For convenience, let \(\text{grad}_{w}L(\rho)\) denote the gradient with respect to \(w\) and \(\text{grad}_{z}L(\rho)\) denote the gradient with respect to \(z\). The following lemma is about the contribution of the first coordinates of the particles to \(A_{t}\), and is useful during Phase 1.

**Lemma E.4** (Contribution of \(w\)'s).: _Suppose we are in the setting of Theorem 3.4, and let \(\rho_{t}\) be the projected gradient flow on population loss, initialized with the uniform distribution on \(\mathbb{S}^{d-1}\), defined _in Eq. (3.7). Suppose \(D_{2,t}<0\) and \(D_{4,t}<0\), where \(D_{2,t}\) and \(D_{4,t}\) are as defined in Section 4.1. Then, the contribution of the first coordinates to \(A_{t}\) can be bounded by_

\[-2\Big{(}\text{grad}_{\hat{w}}L(\rho_{t}) -\text{grad}_{\bar{w}}L(\rho_{t})\Big{)}\cdot(\hat{w}_{t}-\bar{w}_{t})\] (E.57) \[\leq 4\hat{\sigma}_{2,d}^{2}|D_{2,t}|(\hat{w}_{t}-\bar{w}_{t})^{2} +O(\hat{\sigma}_{4,d}^{2}|D_{4,t}|w_{\text{max}}(\hat{w}_{t}-\bar{w}_{t})^{2})\] (E.58) \[\qquad+O\Big{(}\hat{\sigma}_{4,d}^{2}|D_{4,t}|\Delta_{\text{max}} (\hat{w}_{t}-\bar{w}_{t})^{2}\Big{)}\] (E.59) \[\qquad+O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma} _{4,d}^{2}|D_{4,t}|}{d}\cdot(\hat{w}_{t}-\bar{w}_{t})^{2}\Big{)}\,.\] (E.60)

_where \(w_{\text{max}}\) is as defined in Definition 4.5._

The constant \(4\) at the beginning of the first term is particularly important since it determines the growth rate of \(\|\delta_{t}\|_{2}^{2}\), and in turn affects the number of samples we will need. At least during Phase 1, the contribution of the first coordinates will be the dominant term in \(A_{t}\), as we will see in the next few lemmas.

Proof of Lemma e.4.: The contribution of the first coordinate to the dot product defining \(A_{t}\) is

\[-2\Big{(}\text{grad}_{\hat{w}}L(\rho_{t}) -\text{grad}_{\bar{w}}L(\rho_{t})\Big{)}\cdot(\hat{w}_{t}-\bar{w}_{t})\] (E.61) \[=2(v(\hat{w}_{t})-v(\bar{w}_{t}))\cdot(\hat{w}_{t}-\bar{w}_{t})\] (E.62)

where \(v(w)\) is as defined in Lemma 4.4. We can further simplify \(v(\hat{w}_{t})-v(\bar{w}_{t})\) using Lemma 4.4. Let \(P_{t}\) and \(Q_{t}\) be defined as in Lemma 4.4. Then,

\[v(\hat{w}_{t})-v(\bar{w}_{t})=-(1-\hat{w}_{t}^{2})(P_{t}(\hat{w}_{t})+Q_{t}( \hat{w}_{t}))+(1-\bar{w}_{t}^{2})(P_{t}(\bar{w}_{t})+Q_{t}(\bar{w}_{t}))\,.\] (E.63)

Let us upper bound the term corresponding to \(Q_{t}\) first. Since each coefficient of \(Q_{t}\) is bounded above by \(O(\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d})\) (by the conclusion of Lemma 4.4) and \(Q_{t}\) only has linear and cubic terms, we have

\[\Big{|}-(1-\hat{w}_{t}^{2})Q_{t}(\hat{w}_{t})+(1-\bar{w}_{t}^{2})Q_{t}(\bar{w }_{t})\Big{|}\cdot|\hat{w}_{t}-\bar{w}_{t}|\lesssim\frac{\hat{\sigma}_{2,d}^{ 2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D_{4,t}|}{d}|\hat{w}_{t}-\bar{w}_{t}|^{2}\] (E.64)

by Lemma I.16. The terms corresponding to \(P_{t}(\bar{w}_{t})\) in \((v(\hat{w}_{t})-v(\bar{w}_{t}))(\hat{w}_{t}-\bar{w}_{t})\) can be bounded as follows:

\[(-(1-\hat{w}_{t}^{2})P_{t}(\hat{w}_{t})+(1-\bar{w}_{t}^{2})P_{t}( \bar{w}_{t}))\cdot(\hat{w}_{t}-\bar{w}_{t})\] (E.65) \[=2\hat{\sigma}_{2,d}^{2}|D_{2,t}|(\hat{w}_{t}-\bar{w}_{t})^{2}+4 \hat{\sigma}_{4,d}^{2}|D_{4,t}|(\hat{w}_{t}-\bar{w}_{t})^{2}(\hat{w}_{t}^{2}+ \hat{w}_{t}\bar{w}_{t}+\bar{w}_{t}^{2})\] (E.66) \[\qquad-2\hat{\sigma}_{2,d}^{2}|D_{2,t}|(\hat{w}_{t}-\bar{w}_{t})^ {2}(\hat{w}_{t}^{2}+\hat{w}_{t}\bar{w}_{t}+\bar{w}_{t}^{2})-4\hat{\sigma}_{4,d }^{2}|D_{4,t}|(\hat{w}_{t}-\bar{w}_{t})^{2}(\hat{w}_{t}^{2}+\hat{w}_{t}\bar{w}_ {t}+\bar{w}_{t}^{2})\] (E.67) \[\leq 2\hat{\sigma}_{2,d}^{2}|D_{2,t}|(\hat{w}_{t}-\bar{w}_{t})^{2} +4\hat{\sigma}_{4,d}^{2}|D_{4,t}|(\hat{w}_{t}-\bar{w}_{t})^{2}(\hat{w}_{t}^{2}+ \hat{w}_{t}\bar{w}_{t}+\bar{w}_{t}^{2})\] (Since last two terms are nonnegative) \[\leq 2\hat{\sigma}_{2,d}^{2}|D_{2,t}|(\hat{w}_{t}-\bar{w}_{t})^{2} +O(\hat{\sigma}_{4,d}^{2}|D_{4,t}||\hat{w}_{t}+\bar{w}_{t}|(\hat{w}_{t}-\bar{w}_ {t})^{2})\] (Since \[\bar{w}_{t},\hat{w}_{t}\leq 1\] )

Thus, the overall bound on the contribution of the first coordinates to \(A_{t}\) is

\[2(v(\hat{w}_{t})-v(\bar{w}_{t}))\cdot(\hat{w}_{t}-\bar{w}_{t}) \leq 2\cdot(-(1-\hat{w}_{t}^{2})P_{t}(\hat{w}_{t})+(1-\bar{w}_{t}^{ 2})P_{t}(\bar{w}_{t}))\cdot(\hat{w}_{t}-\bar{w}_{t})\] (E.68) \[\qquad+\Big{|}-(1-\hat{w}_{t}^{2})Q_{t}(\hat{w}_{t})+(1-\bar{w}_{t }^{2})Q_{t}(\bar{w}_{t})\Big{|}\cdot|\hat{w}_{t}-\bar{w}_{t}|\] (E.69) \[\leq 4\hat{\sigma}_{2,d}^{2}|D_{2,t}|(\hat{w}_{t}-\bar{w}_{t})^{2} +O(\hat{\sigma}_{4,d}^{2}|D_{4,t}||\hat{w}_{t}+\bar{w}_{t}|(\hat{w}_{t}-\bar{w}_ {t})^{2})\] (E.70) \[\qquad+O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4, d}^{2}|D_{4,t}|}{d}(\hat{w}_{t}-\bar{w}_{t})^{2}\Big{)}\] (E.71)We can simplify this bound further in terms of \(w_{\text{max}}\) and \(\Delta_{\text{max},t}\). First observe that

\[|\hat{w}_{t}-\bar{w}_{t}|\leq\|\delta_{t}\|_{2}\leq\Delta_{\text{ max},t}\] (E.72)

Thus, by the triangle inequality, we have \(|\bar{w}_{t}|\leq w_{\text{max}}\) and \(|\hat{w}_{t}|\leq w_{\text{max}}+\Delta_{\text{max},t}\) meaning

\[2(v(\hat{w}_{t})-v(\bar{w}_{t}))\cdot(\hat{w}_{t}-\bar{w}_{t}) \leq 4\hat{\sigma}_{2,d}^{2}|D_{2,t}|(\hat{w}_{t}-\bar{w}_{t})^{2}+O( \hat{\sigma}_{4,d}^{2}|D_{4,t}||\hat{w}_{t}+\bar{w}_{t}|(\hat{w}_{t}-\bar{w}_{ t})^{2})\] (E.73) \[\qquad+O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma }_{4,d}^{2}|D_{4,t}|}{d}(\hat{w}_{t}-\bar{w}_{t})^{2}\Big{)}\] (E.74) \[\leq 4\hat{\sigma}_{2,d}^{2}|D_{2,t}|(\hat{w}_{t}-\bar{w}_{t})^{2}+O (\hat{\sigma}_{4,d}^{2}|D_{4,t}|w_{\text{max}}(\hat{w}_{t}-\bar{w}_{t})^{2})\] (E.75) \[\qquad+O\Big{(}\hat{\sigma}_{4,d}^{2}|D_{4,t}|\Delta_{\text{max}, t}(\hat{w}_{t}-\bar{w}_{t})^{2}\Big{)}\] (E.76) \[\qquad+O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma }_{4,d}^{2}|D_{4,t}|}{d}(\hat{w}_{t}-\bar{w}_{t})^{2}\Big{)}\] (E.77)

as desired. 

Next, we study the contribution of the last \((d-1)\) coordinates of the particles to \(A_{t}(\chi)\) during Phase 1. We first introduce the following formula for the projected gradient of the population loss with respect to the last \((d-1)\) coordinates of a particle.

**Lemma E.5**.: _Let \(\rho\) be a distribution which is rotationally invariant, as in Definition 4.1. Let \(\operatorname{grad}_{u}L(\rho)\) be defined as in Eq. (3.4). In addition, let \(\operatorname{grad}_{z}L(\rho)\) denote the last \((d-1)\) coordinates of \(\operatorname{grad}_{u}L(\rho)\), for any particle \(u=(w,z)\). Then, for any particle \(u=(w,z)\), we have \(\operatorname{grad}_{z}L(\rho)=-\Big{(}\frac{w}{1-w^{2}}\cdot\operatorname{ grad}_{w}L(\rho)\Big{)}z\)._

Proof of Lemma e.5.: Notice that \(w^{2}+z^{2}=1\) for any \(u=(w,z)\in\mathbb{S}^{d-1}\), and this equality is maintained by projected gradient descent on the population loss, as in Eq. (3.7). Also, by Lemma I.15, we have \(\operatorname{grad}_{z}L(\rho)=cz\) for some scalar \(c\), since \(\rho\) is rotationally invariant. Thus,

\[0 =\frac{d}{dt}(w^{2}+z^{2})\] (E.79) \[=-2w\operatorname{grad}_{w}L(\rho)-2\langle z,\operatorname{grad} _{z}L(\rho)\rangle\] (E.80) \[=-2w\operatorname{grad}_{w}L(\rho)-2c\left\|z\right\|^{2}\] (E.81) \[=-2w\operatorname{grad}_{w}L(\rho)-2c(1-w^{2}).\] (E.82)

Rearranging gives us

\[c=-\frac{w}{1-w^{2}}\operatorname{grad}_{w}L(\rho).\] (E.83)

By directly applying this formula for the last \((d-1)\) coordinates of the projected gradient, we have the following upper bound on the contribution of the last \((d-1)\) coordinates to \(A_{t}\) during Phase 1.

**Lemma E.6** (Contribution of \(z\)'s During Phase 1.).: _Suppose we are in the setting of Theorem 3.4. Let \(t\leq T_{1}\) (where \(T_{1}\) is as defined in Definition 4.5). Let \(\rho_{t}\) be defined according to the population projected gradient flow as in Eq. (3.7). Let \(\operatorname{grad}_{u}L(\rho)\) be as in Eq. (3.4), and let \(\operatorname{grad}_{z}L(\rho)\) denote the last \((d-1)\) coordinates of \(\operatorname{grad}_{u}L(\rho)\) for any particle \(u=(w,z)\). Then, we have the following upper bound:_

\[-2\langle\operatorname{grad}_{z}L(\rho_{t})- \operatorname{grad}_{z}L(\rho_{t}),\hat{z}_{t}-\bar{z}_{t}\rangle\] (E.84) \[\lesssim\hat{\sigma}_{2,d}^{2}|D_{2,t}|(|\bar{w}_{t}|+|\hat{w}_{t }|)\left\|\bar{u}_{t}-\hat{u}_{t}\right\|^{2}+\hat{\sigma}_{4,d}^{2}|D_{4,t}|(| \bar{w}_{t}^{3}|+|\bar{w}_{t}^{3}|)\left\|\bar{u}_{t}-\hat{u}_{t}\right\|^{2}\] (E.85) \[\qquad+\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{ 2}|D_{4,t}|}{d}\|\bar{u}_{t}-\hat{u}_{t}\|_{2}^{2}\,.\] (E.86)Proof of Lemma e.6.: Using the formula for projected gradient in Lemma e.5, we have that

\[-\langle\text{grad}_{\hat{z}}L(\rho_{t})-\text{grad}_{\bar{z}}L( \rho_{t}),\hat{z}_{t}-\bar{z}_{t}\rangle\] (E.87) \[\qquad\qquad=\Big{\langle}\frac{\hat{w}_{t}}{1-\bar{w}_{t}^{2}} \cdot\text{grad}_{\bar{w}}L(\rho_{t})\cdot\hat{z}_{t}-\frac{\bar{w}_{t}}{1- \bar{w}_{t}^{2}}\cdot\text{grad}_{\bar{w}}L(\rho_{t})\cdot\bar{z}_{t},\hat{z} _{t}-\bar{z}_{t}\Big{\rangle}\] (E.88) \[\qquad\qquad=\Big{(}\frac{\hat{w}_{t}\text{grad}_{\bar{w}}L(\rho _{t})}{1-\bar{w}_{t}^{2}}-\frac{\bar{w}_{t}\text{grad}_{\bar{w}}L(\rho_{t})}{ 1-\bar{w}_{t}^{2}}\Big{)}\hat{z}^{\top}(\hat{z}_{t}-\bar{z}_{t})+\frac{\bar{w}_ {t}\text{grad}_{\bar{w}}L(\rho_{t})}{1-\bar{w}_{t}^{2}}\|\hat{z}_{t}-\bar{z}_ {t}\|_{2}^{2}\] (E.89)

The second term is always negative for \(t\leq T_{1}\), because Lemma D.9 implies that \(v(\bar{w}_{t})\) has the same sign as \(\bar{w}_{t}\), meaning that \(\text{grad}_{\bar{w}}L(\rho_{t})\) has the opposite sign. We bound the first term as

\[\Big{(}\frac{\hat{w}_{t}\text{grad}_{\bar{w}}L(\rho_{t})}{1-\hat{ w}_{t}^{2}}-\frac{\bar{w}_{t}\text{grad}_{\bar{w}}L(\rho_{t})}{1-\bar{w}_{t}^{2}} \Big{)}\hat{z}_{t}^{\top}(\hat{z}_{t}-\hat{z}_{t})\] (E.90) \[\qquad\qquad\leq\Big{|}\frac{\hat{w}_{t}\text{grad}_{\bar{w}}L( \rho_{t})}{1-\hat{w}_{t}^{2}}-\frac{\bar{w}_{t}\text{grad}_{\bar{w}}L(\rho_{t} )}{1-\bar{w}_{t}^{2}}\Big{|}\cdot\|\bar{z}_{t}-\hat{z}_{t}\|\qquad\qquad\text{ (B.c. $|\hat{z}_{t}\|_{2}\leq\|\hat{u}_{t}\|_{2}=1$)}\] \[\qquad\qquad=|\hat{w}_{t}(P_{t}(\hat{w}_{t})+Q_{t}(\hat{w}_{t}))- \bar{w}_{t}(P_{t}(\bar{w}_{t})+Q_{t}(\bar{w}_{t}))|\cdot\|\bar{z}_{t}-\hat{z}_ {t}\|_{2}\] (By Lemma 4.4) \[\leq 2\hat{\sigma}_{2,d}^{2}|D_{2,t}||\hat{w}_{t}^{2}-\bar{w}_{t}^ {2}||\bar{z}_{t}-\hat{z}_{t}||_{2}+4\hat{\sigma}_{4,d}^{2}|D_{4,t}||\hat{w}_{t }^{4}-\bar{w}_{t}^{4}||\bar{z}_{t}-\hat{z}_{t}||_{2}\] (E.91) \[\qquad\qquad+|Q_{t}(\bar{w}_{t})-Q_{t}(\hat{w}_{t})|\cdot\|\bar{ z}_{t}-\hat{z}_{t}\|_{2}\qquad\qquad\qquad\text{ (By Lemma 4.4 and Def. of $P_{t}$)}\] \[\leq 2\hat{\sigma}_{2,d}^{2}|D_{2,t}||\hat{w}_{t}^{2}-\bar{w}_{t}^ {2}||\bar{z}_{t}-\hat{z}_{t}||_{2}+4\hat{\sigma}_{4,d}^{2}|D_{4,t}||\hat{w}_{t }^{4}-\bar{w}_{t}^{4}||\bar{z}_{t}-\hat{z}_{t}||_{2}\] (E.92) \[\qquad\qquad+O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{ \sigma}_{4,d}^{2}|D_{4,t}|}{d}\Big{)}|\bar{w}_{t}-\hat{w}_{t}||\bar{z}_{t}- \hat{z}_{t}||_{2}\] (By Lemma I.16 and Def. of \[Q_{t}\] from Lemma 4.4 ) \[\lesssim\hat{\sigma}_{2,d}^{2}|D_{2,t}|(|\bar{w}_{t}|+|\hat{w}_{t }|)||\hat{u}_{t}-\bar{u}_{t}||_{2}^{2}+\hat{\sigma}_{4,d}^{2}|D_{4,t}|(|\bar{w }_{t}|^{3}+|\hat{w}_{t}|^{3})\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\] (E.93) \[\qquad\qquad+\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_ {4,d}^{2}|D_{4,t}|}{d}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\] (B.c. \[|\bar{w}_{t}-\hat{w}_{t}|\leq\|\bar{u}_{t}-\hat{u}_{t}\|_{2}\text{ and }|\bar{z}_{t}-\hat{z}_{t}\|_{2}\leq\|\bar{u}_{t}-\hat{u}_{t}\|_{2}\] )

as desired. 

We can combine Lemma e.4 and Lemma E.6 and get the following bound on \(A_{t}\) during Phase 1.

**Lemma e.7**.: _Suppose we are in the setting of Theorem 3.4. Let \(t\leq T_{1}\) (where \(T_{1}\) is as defined in Definition 4.5). Then,_

\[A_{t} \leq 4\hat{\sigma}_{2,d}^{2}|D_{2,t}|(\hat{w}_{t}-\bar{w}_{t})^{2}+O ((\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d}^{2}|D_{4,t}|)w_{\text{max}}) \|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\] (E.94) \[\qquad+O\Big{(}(\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_{4,d} ^{2}|D_{4,t}|)\Delta_{\text{max}}\Big{)}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\] (E.95) \[\qquad+O\Big{(}\frac{\hat{\sigma}_{2,d}^{2}|D_{2,t}|+\hat{\sigma}_ {4,d}^{2}|D_{4,t}|}{d}\Big{)}\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}\,.\] (E.96)

Proof of Lemma e.7.: This directly follows from Lemma E.4 and Lemma E.6. 

Next, we give a potentially looser bound for \(A_{t}\) which holds even after Phase 1. The running time after Phase 1 is at most \((\log\log d)^{O(1)}\) (aside from the dependency on \(\hat{\sigma}_{2,d},\hat{\sigma}_{4,d}\), etc.), meaning that even the following growth rate is enough to ensure that the coupling error grows by only a sub-polynomial factor after Phase 1.

**Lemma E.8**.: _Suppose we are in the setting of Theorem 3.4. Then, for all \(t\geq 0\), we have_

\[|A_{t}|\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\gamma_{2}\left\|\bar{u }_{t}-\hat{u}_{t}\right\|^{2}.\] (E.97)

Proof of Lemma e.8.: We first consider the part of \(A_{t}\) that originates from the non-projected gradients, which is defined as

\[A_{t}^{\prime} :=-2\langle\nabla_{\bar{u}}L(\rho_{t})-\nabla_{\bar{u}}L(\rho_{t}), \bar{u}_{t}-\hat{u}_{t}\rangle\] (E.98) \[=-2\mathbb{E}_{x\sim\delta^{q-1}}\big{[}(f_{\rho}(x)-y(x))(\sigma^{ \prime}(\bar{u}_{t}^{\top}x)-\sigma^{\prime}(\hat{u}_{t}^{\top}x))(\bar{u}_{t}^{ \top}x-\hat{u}_{t}^{\top}x)\big{]}.\] (E.99)By the Cauchy-Schwarz inequality, we have

\[|A^{\prime}_{t}|\lesssim\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-y(x))^{2}] ^{\frac{1}{2}}\cdot\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[(\sigma^{\prime}(\bar{u}_{ t}^{\top}x)-\sigma^{\prime}(\hat{u}_{t}^{\top}x))^{2}(\bar{u}_{t}^{\top}x-\hat{u}_{t}^{ \top}x)^{2}]^{\frac{1}{2}}.\] (E.100)

We bound the first factor in the equation above by the loss at initialization, which is

\[\frac{1}{2}\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho_{0}}(x)-y)^{2}] \lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\gamma_{2}^{2}.\] (E.101)

by Lemma D.8. Thus, plugging Eq. (E.101) and Eq. (E.210) (from the statement of Lemma E.15) into Eq. (E.100) gives us

\[|A^{\prime}_{t}|\lesssim(\hat{\sigma}_{4,d}^{2}+\hat{\sigma}_{2,d}^{2})\gamma _{2}\left\|\delta_{t}\right\|^{2}.\] (E.102)

Next, we bound the difference between \(A_{t}\) and \(A^{\prime}_{t}\). We have

\[A_{t}-A^{\prime}_{t} =-\langle\text{grad}_{\hat{u}}L(\rho_{t})-\text{grad}_{\hat{u}}L( \rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle+\langle\nabla_{\hat{u}}L(\rho_{t})- \nabla_{\hat{u}}L(\rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle\] (E.103) \[=\langle\nabla_{\hat{u}}L(\rho_{t})-\text{grad}_{\hat{u}}L(\rho_ {t}),\hat{u}_{t}-\bar{u}_{t}\rangle-\langle\nabla_{\hat{u}}L(\rho_{t})-\text{ grad}_{\hat{u}}L(\rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle\] (E.104)

For the first term of Eq. (E.104), we have

\[\langle\nabla_{\hat{u}}L(\rho_{t})-\text{grad}_{\hat{u}}L(\rho_{t} ),\hat{u}_{t}-\bar{u}_{t}\rangle =\langle\hat{u}_{t}\hat{u}_{t}^{\top}\nabla_{\hat{u}}L(\rho_{t}), \hat{u}_{t}-\bar{u}_{t}\rangle\] (E.105) \[=\langle\nabla_{\hat{u}}L(\rho_{t}),(\hat{u}_{t}\hat{u}_{t}^{ \top})\hat{u}_{t}-(\hat{u}_{t}\hat{u}_{t}^{\top})\bar{u}_{t}\rangle\] (E.106) \[=\langle\nabla_{\hat{u}}L(\rho_{t}),\hat{u}_{t}\rangle\cdot(1- \langle\hat{u}_{t},\bar{u}_{t}\rangle)\] (E.107) \[=\langle\nabla_{\hat{u}}L(\rho_{t}),\hat{u}_{t}\rangle\cdot\frac {\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}}{2}\] (B.c. \[\|\hat{u}_{t}\|_{2}=\|\bar{u}_{t}\|_{2}=1\] (B.108) \[=\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho_{t}}(x)-y(x))\sigma^ {\prime}(\hat{u}_{t}^{\top}x)\hat{u}_{t}^{\top}x]\cdot\frac{\|\hat{u}_{t}-\bar {u}_{t}\|_{2}^{2}}{2}\] (E.109) \[=\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho_{t}}(x)-y(x))\sigma^ {\prime}(\hat{u}_{t}^{\top}x)\hat{u}_{t}^{\top}x]\cdot\frac{\|\delta_{t}\|_{2} ^{2}}{2}\] (E.110)

and by an argument similar to the one used to bound \(A^{\prime}_{t}\) (here using Eq. (E.211) from Lemma E.15 instead of Eq. (E.210)), we can thus show that

\[|\langle\nabla_{\hat{u}}L(\rho_{t})-\text{grad}_{\hat{u}}L(\rho_{t}),\bar{u}_ {t}-\hat{u}_{t}\rangle|\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2} )\gamma_{2}\left\|\delta_{t}\right\|^{2}.\] (E.110)

Similarly, for the second term we have

\[|\langle\nabla_{\hat{u}}L(\rho_{t})-\text{grad}_{\hat{u}}L(\rho_{t}),\bar{u}_ {t}-\hat{u}_{t}\rangle|\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2} )\gamma_{2}\left\|\delta_{t}\right\|^{2}.\] (E.111)

Combining Eq. (E.110), Eq. (E.111) and Eq. (E.102) finishes the proof. 

### Upper Bound on \(B_{t}\)

The goal of this section is to obtain an upper bound on

\[B_{t}(\chi):=-2\langle\text{grad}_{\hat{u}}L(\hat{\rho}_{t})-\text{grad}_{\hat {u}}L(\rho_{t}),\hat{u}_{t}(\chi)-\bar{u}_{t}(\chi)\rangle.\] (E.112)

When there is no risk of confusion, we simply use \(B_{t}\), \(\hat{u}_{t}\), and \(\bar{u}_{t}\), omitting the \(\chi\) dependency. In the next lemma, we first obtain a bound on \(B_{t}(\chi)\), and after that, we obtain a bound on \(\mathbb{E}_{\chi}[B_{t}(\chi)]\). The proof of the following lemma is mostly a straightforward calculation, and follows mostly from Lemma 5.1 and the definition of \(\overline{\Delta}\).

**Lemma E.9**.: _Suppose we are in the setting of Theorem 3.4. Suppose \(T\lesssim\frac{d^{C}}{\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}}\) for any absolute constant \(C\). Assume the width \(m\) of \(\bar{\rho},\hat{\rho}\) is at most \(d^{O(\log d)}\). Then, with probability at least \(1-e^{-d^{2}}\), we have_

\[\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho_{t}}(x)-f_{\hat{\rho}_{t}}(x))^{2}] \lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{2}(\log d)^{O(1 )}}{m}\] (E.113)

_for all times \(t\in[0,T]\). Additionally, with probability at least \(1-e^{-d^{2}}\), for all \(t\in[0,T]\) and \(\chi\in\hat{\rho}_{0}\), we have_

\[B_{t}(\chi)\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\cdot\Big{(} \frac{d(\log d)^{O(1)}}{\sqrt{m}}+\overline{\Delta}_{t}\Big{)}\|\delta_{t}\|_{2}\,.\] (E.114)Proof of Lemma e.9.: We first decompose \(B_{t}\) into two terms:

\[B_{t} =-2\langle\text{grad}_{\hat{u}}L(\hat{\rho}_{t})-\text{grad}_{\hat{u }}L(\rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle\] (E.115) \[=-2\langle(I-\hat{u}_{t}\hat{u}_{t}^{\top})\nabla_{\hat{u}}L(\hat {\rho}_{t})-(I-\hat{u}_{t}\hat{u}_{t}^{\top})\nabla_{\hat{u}}L(\rho_{t}),\hat{u }_{t}-\bar{u}_{t}\rangle\] (E.116) \[=-2\langle\nabla_{\hat{u}}L(\hat{\rho}_{t})-\nabla_{\hat{u}}L(\rho _{t}),(I-\hat{u}_{t}\hat{u}_{t}^{\top})(\hat{u}_{t}-\bar{u}_{t})\rangle\] (E.117) \[=-2\langle\nabla_{\hat{u}}L(\hat{\rho}_{t})-\nabla_{\hat{u}}L( \rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle+2\langle\nabla_{\hat{u}}L(\hat{\rho}_ {t})-\nabla_{\hat{u}}L(\rho_{t}),\hat{u}_{t}\hat{u}_{t}^{\top}(\hat{u}_{t}- \bar{u}_{t})\rangle\] (E.118) \[=-2\langle\nabla_{\hat{u}}L(\hat{\rho}_{t})-\nabla_{\hat{u}}L( \rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle+2\langle\nabla_{\hat{u}}L(\hat{\rho}_ {t})-\nabla_{\hat{u}}L(\rho_{t}),\hat{u}_{t}\rangle\cdot(1-\langle\hat{u}_{t}, \bar{u}_{t}\rangle)\] (E.119) \[=-2\langle\nabla_{\hat{u}}L(\hat{\rho}_{t})-\nabla_{\hat{u}}L( \rho_{t}),\hat{u}_{t}-\bar{u}_{t}\rangle+2\langle\nabla_{\hat{u}}L(\hat{\rho}_ {t})-\nabla_{\hat{u}}L(\rho_{t}),\hat{u}_{t}\rangle\cdot\frac{\|\hat{u}_{t}- \bar{u}_{t}\|_{2}^{2}}{2}\] (E.120)

where the last equality is because \(\frac{\|\hat{u}_{t}-\bar{u}_{t}\|_{2}^{2}}{2}=1-\langle\hat{u}_{t},\bar{u}_{t}\rangle\) since \(\|\hat{u}_{t}\|_{2}=\|\bar{u}_{t}\|_{2}=1\). For convenience, define \(\delta_{t}:=\hat{u}_{t}-\bar{u}_{t}\). Then, we have

\[B_{t}=-2\langle\nabla_{\hat{u}}L(\hat{\rho}_{t})-\nabla_{\hat{u}}L(\rho_{t}), \delta_{t}\rangle+\langle\nabla_{\hat{u}}L(\hat{\rho}_{t})-\nabla_{\hat{u}}L( \rho_{t}),\hat{u}_{t}\rangle\cdot\|\delta_{t}\|_{2}^{2}\] (E.121)

We first focus on the first term in Eq. (E.121), which is the main contributor to \(B_{t}\).

**Analysis of First Term in Eq. (E.121).** For convenience, we refer to the first term in Eq. (E.121) as \(B_{t}^{\prime}\). Observe that we can expand \(B_{t}^{\prime}\) as

\[B_{t}^{\prime} =-2\langle\nabla_{\hat{u}}L(\hat{\rho}_{t})-\nabla_{\hat{u}}L( \rho_{t}),\delta_{t}\rangle\] (E.122) \[=-2\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}(x) -f_{\rho}(x))\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t},x\rangle]\] (By Eq. ( 3.4 )) \[=-2\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}(x) -f_{\rho}(x))\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t},x \rangle]-2\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}(x)-f_{ \hat{\rho}}(x))\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t}, x\rangle]\] (E.123)

Let us bound the first term of Eq. (E.123). By Lemma 5.1, with probability \(e^{-d^{2}}\), for all \(t\leq T\) we have

\[\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho_{t}}(x)-f_{\hat{\rho}_{t} }(x))^{2}]\lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{2}( \log d)^{O(1)}}{m}\,.\] (E.124)

Thus, we can bound the first term of Eq. (E.123) by

\[\Big{|}\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho }}(x)-f_{\rho}(x))\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t}, x\rangle]\Big{|}\] (E.125) \[\qquad\qquad\qquad\qquad\leq\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{ d-1}}[(f_{\hat{\rho}}(x)-f_{\rho}(x))^{2}]^{1/2}\mathop{\mathbb{E}}_{x\sim \mathbb{S}^{d-1}}[\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)^{2}\langle \delta_{t},x\rangle^{2}]^{1/2}\] (By Cauchy-Schwarz inequality) \[\lesssim\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho }}(x)-f_{\rho}(x))^{2}]^{1/2}(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})^{1/ 2}\|\delta_{t}\|_{2}\] (By Lemma E.15, Eq. ( 2.211 )) \[\lesssim\frac{\sqrt{\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}} \cdot d(\log d)^{O(1)}}{\sqrt{m}}\cdot(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^ {2})^{1/2}\|\delta_{t}\|_{2}\] (By Eq. ( E.124 )) \[\lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\cdot d (\log d)^{O(1)}}{\sqrt{m}}\|\delta_{t}\|_{2}\,.\] (E.126)

Next, we bound the second term in Eq. (E.123). By applying the Cauchy-Schwarz inequality, we have

\[\Big{|}\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho} }(x)-f_{\hat{\rho}}(x))\sigma^{\prime}(\langle\hat{u},x\rangle)\langle\delta_{t}, x\rangle]\Big{|}\] (E.127) \[\qquad\qquad\qquad\qquad\leq\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{ d-1}}[(f_{\hat{\rho}}(x)-f_{\hat{\rho}}(x))^{2}]^{1/2}\mathop{\mathbb{E}}_{x\sim \mathbb{S}^{d-1}}[\sigma^{\prime}(\langle\hat{u},x\rangle)^{2}\langle\delta_{t}, x\rangle^{2}]^{1/2}\] (E.128) \[\qquad\qquad\qquad\qquad\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{ 4,d}^{2})^{1/2}\overline{\Delta}_{t}\cdot\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[ \sigma^{\prime}(\langle\hat{u}_{t},x\rangle)^{2}\langle\delta_{t},x\rangle^{2}]^{1/2}\] (By Lemma 5.1 ) \[\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})^{1/2}\overline {\Delta}_{t}\cdot(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})^{1/2}\| \delta_{t}\|_{2}\qquad\text{(By Lemma E.15 (Eq. (E.211)))}\] (E.129)

[MISSING_PAGE_FAIL:75]

together with a third-order term which is at most \((\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\overline{\Delta}_{t}^{3}\), which is a significantly better bound than \((\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\overline{\Delta}_{t}\|\delta_{t }\|_{2}\), which we obtained in the previous lemma. This is because to simplify Eq. (E.138), we can use

\[\sigma(\langle\bar{u}_{t},x\rangle)=\sigma(\langle\hat{u}_{t},x\rangle)+ \sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\bar{u}_{t}-\hat{u}_{t},x \rangle+O(\sigma^{\prime\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t}, x\rangle^{2})\] (E.140)

meaning that we can essentially substitute \(\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t},x\rangle\approx \sigma(\langle\hat{u}_{t},x\rangle)-\sigma(\langle\bar{u}_{t},x\rangle)\) in Eq. (E.138). Note that we need this better bound on the growth rate of \(\overline{\Delta}_{t}\) since our inductive hypothesis imposes a stricter condition on \(\overline{\Delta}_{t}\).

**Lemma E.10**.: _Suppose we are in the setting of Lemma E.9, and in particular, suppose \(T\) satisfies the same assumptions as in the statement of Lemma E.9. Then, for all \(t\leq T\), we have_

\[\mathop{\mathbb{E}}_{\chi\sim\hat{\rho}_{0}}[B_{t}(\chi)]\lesssim\frac{( \hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d(\log d)^{O(1)}}{\sqrt{m}} \overline{\Delta}_{t}+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) \overline{\Delta}_{t}^{3}\,.\] (E.141)

Proof of Lemma E.10.: We again make use of Eq. (E.121), and we recall from the proof of the previous lemma that \(B_{t}=B_{t}^{\prime}+B_{t}^{\prime\prime}\), where

\[B_{t}^{\prime}=-2\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}( x)-f_{\rho}(x))\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t},x \rangle]-2\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}(x)-f_{ \hat{\rho}}(x))\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t}, x\rangle]\] (E.142)

and

\[B_{t}^{\prime\prime}=\langle\nabla_{\hat{u}}L(\hat{\rho}_{t})-\nabla_{\hat{u}} L(\rho_{t}),\hat{u}_{t}\rangle\cdot\|\delta_{t}\|_{2}^{2}\,.\] (E.143)

The main difference between Lemma E.9 is that we now bound the second term of \(B_{t}^{\prime}\) in expectation. By Taylor's theorem, we can write

\[\sigma(\langle\bar{u}_{t},x\rangle)=\sigma(\langle\hat{u}_{t},x\rangle)+ \sigma^{\prime}(\langle\hat{u}_{t},x\rangle)(\langle\bar{u}_{t},x\rangle- \langle\hat{u}_{t},x\rangle)+\frac{\sigma^{\prime\prime}(\lambda)}{2}(\langle \bar{u}_{t},x\rangle-\langle\hat{u}_{t},x\rangle)^{2}\] (E.144)

for some \(\lambda\) which is a convex combination of \(\langle\hat{u},x\rangle\) and \(\langle\bar{u},x\rangle\). Rearranging gives

\[\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t},x\rangle= \sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\hat{u}_{t}-\bar{u}_{t},x \rangle=\sigma(\langle\hat{u}_{t},x\rangle)-\sigma(\langle\bar{u}_{t},x \rangle)+\frac{\sigma^{\prime\prime}(\lambda)}{2}\langle\delta_{t},x\rangle^{2}\] (E.145)

Thus, taking the expectation of the second term of \(B_{t}^{\prime}\) over \((\hat{u}_{t},\bar{u}_{t})\sim\Gamma_{t}\) gives

\[-2\mathop{\mathbb{E}}_{(\hat{u},\hat{u})\sim\Gamma_{t}}\mathop{ \mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}(x)-f_{\hat{\rho}}(x)) \sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t},x\rangle]\] (E.146) \[\qquad\qquad\qquad-\mathop{\mathbb{E}}_{(\hat{u},\hat{u})\sim \Gamma_{t}}\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}(x)-f_{ \hat{\rho}}(x))\sigma(\langle\hat{u}_{t},x\rangle)-\sigma(\langle\bar{u}_{t}, x\rangle))]\] (E.147) \[\qquad\qquad\qquad-\mathop{\mathbb{E}}_{(\hat{u},\hat{u})\sim \Gamma_{t}}\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}(x)-f_{ \hat{\rho}}(x))\sigma^{\prime\prime}(\lambda)\langle\delta_{t},x\rangle^{2}] \qquad\qquad\qquad\text{(By Eq. \eqref{E.145})}\] \[\qquad\qquad\qquad=-2\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[( f_{\hat{\rho}}(x)-f_{\hat{\rho}}(x))^{2}]\] (E.148) \[\qquad\qquad\qquad-\mathop{\mathbb{E}}_{(\hat{u},\hat{u})\sim \Gamma_{t}}\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}(x)-f_{ \hat{\rho}}(x))\sigma^{\prime\prime}(\lambda)\langle\delta_{t},x\rangle^{2}] \qquad\qquad\qquad\text{(By Eq. \eqref{E.146})}\] \[\qquad\qquad\qquad\leq-\mathop{\mathbb{E}}_{(\hat{u},\hat{u})\sim \Gamma_{t}}\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}(x)-f_{ \hat{\rho}}(x))\sigma^{\prime\prime}(\lambda)\langle\delta_{t},x\rangle^{2}]\] (E.149)

Next, let us bound the inner expectation in Eq. (E.149). By the Cauchy-Schwarz inequality, we have

\[\Big{|}\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}} (x)-f_{\hat{\rho}}(x))\sigma^{\prime\prime}(\lambda)\langle\delta_{t},x\rangle^{2}] \Big{|}\] (E.150) \[\qquad\qquad\leq\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{ \hat{\rho}}(x)-f_{\hat{\rho}}(x))^{2}]^{1/2}\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1 }}[\sigma^{\prime\prime}(\lambda)^{2}\langle\delta_{t},x\rangle^{4}]^{1/2}\] (E.151) \[\qquad\qquad\qquad\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d }^{2})^{1/2}\overline{\Delta}_{t}\cdot\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[ \sigma^{\prime\prime}(\lambda)^{2}\langle\delta_{t},x\rangle^{4}]^{1/2}\] (By Lemma E.16) \[\qquad\qquad\qquad\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d }^{2})^{1/2}\overline{\Delta}_{t}\cdot\Big{(}\mathop{\mathbb{E}}_{x\sim \mathbb{S}^{d-1}}[\sigma^{\prime\prime}(\langle\hat{u}_{t},x\rangle)^{2} \langle\delta_{t},x\rangle^{4}]^{1/2}+\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1 }}[\sigma^{\prime\prime}(\langle\bar{u}_{t},x\rangle)^{2}\langle\delta_{t},x \rangle^{4}]^{1/2}\Big{)}\] (E.152)Here to obtain the last inequality, we observe that \(\sigma^{\prime\prime}\) is convex because \(P_{2,d}{}^{\prime\prime}\) is a constant function and \(P_{4,d}{}^{\prime\prime}\) is a quadratic function by Eq. (C.4) -- additionally, we used the inequality \((a+b)^{2}\lesssim a^{2}+b^{2}\) and the fact that \(\lambda\) is a convex combination of \(\langle\hat{u}_{t},x\rangle\) and \(\langle\bar{u}_{t},x\rangle\).

By an argument similar to the proof of Lemma E.15, we can show that \(\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[\sigma^{\prime\prime}(\langle\hat{u}_{t},x \rangle)^{2}\langle\delta_{t},x\rangle^{4}]^{1/2}\lesssim(\hat{\sigma}_{2,d}^{ 2}+\hat{\sigma}_{4,d}^{2})^{1/2}\|\delta_{t}\|_{2}^{2}\) and \(\mathbb{E}_{x\sim\mathbb{S}^{d-1}}[\sigma^{\prime\prime}(\langle\bar{u}_{t},x \rangle)^{2}\langle\delta_{t},x\rangle^{4}]^{1/2}\lesssim(\hat{\sigma}_{2,d}^{ 2}+\hat{\sigma}_{4,d}^{2})^{1/2}\|\delta_{t}\|_{2}^{2}\). Combining this with Eq. (E.149), we have that

\[\Big{|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}(x)- f_{\hat{\rho}}(x))\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t},x \rangle]\Big{|}\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\overline {\Delta}_{t}\|\delta_{t}\|_{2}^{2}\] (E.153)

and thus,

\[-2\operatorname*{\mathbb{E}}_{(\bar{u},\bar{u})\sim\Gamma_{t}} \operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\hat{\rho}}(x)-f_{\hat{ \rho}}(x))\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\langle\delta_{t},x \rangle]\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\overline{ \Delta}_{t}^{3}\] (E.154)

by the definition of \(\overline{\Delta}_{t}^{3}\). This gives a bound on the second term of \(B_{t}^{\prime}\). Combining this with Eq. (E.126) to bound the first term of \(B_{t}^{\prime}\), we have

\[\operatorname*{\mathbb{E}}_{(\bar{u},\bar{u})\sim\Gamma_{t}}[B_{ t}^{\prime}] \lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d( \log d)^{O(1)}}{\sqrt{m}}\operatorname*{\mathbb{E}}_{(\bar{u},\bar{u})\sim \Gamma_{t}}\|\delta_{t}\|_{2}+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) \overline{\Delta}_{t}^{3}\] (E.155) \[\lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d( \log d)^{O(1)}}{\sqrt{m}}\overline{\Delta}_{t}+(\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2})\overline{\Delta}_{t}^{3}\] (By Cauchy-Schwarz)

Combining this with Eq. (E.136) (for which we also take the expectation over \((\hat{u}_{t},\bar{u}_{t})\sim\Gamma_{t}\)), we have that

\[\operatorname*{\mathbb{E}}_{(\bar{u},\bar{u})\sim\Gamma_{t}}|B_{t}^{\prime \prime}|\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\cdot\Big{(} \frac{d(\log d)^{O(1)}}{\sqrt{m}}+\overline{\Delta}_{t}\Big{)}\cdot\overline {\Delta}_{t}^{2}\] (E.156)

which is at most the bound that we have for \(B_{t}^{\prime}\), and thus,

\[\operatorname*{\mathbb{E}}_{(\bar{u},\bar{u})\sim\Gamma_{t}}[B_{ t}]\lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d(\log d)^{O(1)}}{ \sqrt{m}}\overline{\Delta}_{t}+(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) \overline{\Delta}_{t}^{3}\] (E.157)

as desired. 

### Upper Bound on \(C_{t}\)

The goal of this section is to obtain an upper bound on

\[C_{t}(\chi):=-2\langle\text{grad}_{\hat{u}}\widehat{L}(\hat{\rho}_{t})-\text{ grad}_{\hat{u}}L(\hat{\rho}_{t}),\hat{u}_{t}(\chi)-\bar{u}_{t}(\chi)\rangle\,.\] (E.158)

This quantity is essentially the part of the growth of \(\|\delta_{t}\|_{2}^{2}\) which is due to the difference between the gradient using finite samples and the gradient using infinitely many samples. When there is no risk of confusion, we simply use \(C_{t}\), \(\hat{u}_{t}\), and \(\bar{u}_{t}\), omitting the \(\chi\) dependency. We bound this term with the following lemma:

**Lemma E.11**.: _Suppose we are in the setting of Theorem 3.4. Assume that the number of samples \(x_{i}\) is \(n\leq d^{C}\), for any universal constant \(C>0\). Assume that the width \(m\leq d^{C}\) for any universal constant \(C>0\). Let \(T>0\). Assume for all \(t\in[0,T]\) we have a bound \(B_{1}\) on \(\overline{\Delta}_{t}\) with \(B_{1}\leq\frac{1}{\sqrt{d}}\). Additionally, assume for all \(t\in[0,T]\) that we have a bound \(B_{2}\) on \(\Delta_{\text{max},t}\) such that \(B_{2}\geq\frac{1}{\sqrt{d}}\). Then, with probability \(1-\frac{1}{d^{k(\log d)}}\), for all \(t\in[0,T]\) and \(\chi\in\mathbb{S}^{d-1}\), we have_

\[|C_{t}(\chi)|\lesssim\hat{\sigma}_{\text{max}}^{2}(\log d)^{O(1)} \cdot\left(\sqrt{\frac{d}{n}}\|\delta_{t}\|_{2}+\frac{d^{2}}{n}B_{2}^{2} \overline{\Delta}\|\delta_{t}\|_{2}^{2}+\frac{d^{2.5}}{n}\overline{\Delta}^{2 }B_{2}^{2}\|\delta_{t}\|_{2}+\frac{d^{4}}{n}B_{2}^{4}\overline{\Delta}^{2}\| \delta_{t}\|_{2}^{2}\right).\] (E.159)

_where \(\delta_{t}=\hat{u}_{t}(\chi)-\bar{u}_{t}(\chi)\)._

Proof.: We bound \(C_{t}\) by expanding \(\text{grad}_{\hat{u}}L(\hat{\rho}_{t})\) as the sum of several monomials, as well as expanding \(\text{grad}_{\hat{u}}\widehat{L}(\hat{\rho}_{t})\), and then applying Lemma I.7 to each term from \(\text{grad}_{\hat{u}}L(\hat{\rho}_{t})\) and the corresponding term from \(\text{grad}_{\hat{u}}\hat{L}(\hat{\rho}_{t})\). For convenience, we use \(\ell^{(x)}(\hat{\rho})\) to denote the loss of \(f_{\hat{\rho}}\) on a single data point \(x\in\mathbb{S}^{d-1}\), and \(\text{grad}_{\hat{u}}\hat{\ell}^{(x)}(\hat{\rho})\) to denote the projected gradient of \(\ell^{(x)}(\hat{\rho})\) with respect to \(\hat{u}\). We have

\[\langle\text{grad}_{\hat{u}}\hat{\ell}^{(x)}(\hat{\rho}_{t}),\hat{u }_{t}-\bar{u}_{t}\rangle =\langle(f_{\hat{\rho}}(x)-y(x))\sigma^{\prime}(\langle\hat{u}_{t },x\rangle)x,(I-\hat{u}_{t}\hat{u}_{t}^{\top})(\hat{u}_{t}-\bar{u}_{t})\rangle\] (E.160) \[=\langle(f_{\hat{\rho}}(x)-y(x))\sigma^{\prime}(\langle\hat{u}_{ t},x\rangle)x,\hat{u}_{t}-\bar{u}_{t}\rangle\] (E.161) \[\qquad\qquad\qquad-\langle(f_{\hat{\rho}}(x)-y(x))\sigma^{\prime }(\langle\hat{u}_{t},x\rangle)x,\hat{u}_{t}\rangle\cdot(1-\langle\hat{u}_{t}, \bar{u}_{t}\rangle)\] (E.162) \[=\langle(f_{\hat{\rho}}(x)-y(x))\sigma^{\prime}(\langle\hat{u}_{ t},x\rangle)x,\hat{u}_{t}-\bar{u}_{t}\rangle\] (E.163) \[\qquad\qquad\qquad-\langle(f_{\hat{\rho}}(x)-y(x))\sigma^{\prime }(\langle\hat{u}_{t},x\rangle)x,\hat{u}_{t}\rangle\cdot\frac{\|\hat{u}_{t}- \bar{u}_{t}\|_{2}^{2}}{2}\] (E.164)

For convenience, define \(\delta_{t}:=\hat{u}_{t}-\bar{u}_{t}\). Then we can rewrite the above as

\[\langle\text{grad}_{\hat{u}}\hat{\ell}^{(x)}(\hat{\rho}_{t}), \delta_{t}\rangle =\langle(f_{\hat{\rho}}(x)-y(x))\sigma^{\prime}(\langle\hat{u}_{ t},x\rangle)\langle x,\delta_{t}\rangle\] (E.165) \[\qquad\qquad\qquad-\langle(f_{\hat{\rho}}(x)-y(x))\sigma^{\prime }(\langle\hat{u}_{t},x\rangle)x,\hat{u}_{t}\rangle\cdot\frac{\|\delta_{t}\|_{2 }^{2}}{2}\] (E.166)

For illustration, define \(\mathcal{C}(g)\) as

\[\mathcal{C}(g):=\Big{|}\frac{1}{n}\sum_{i=1}^{n}g(x_{i})-\mathop{{\mathbb{E} }}_{x\sim\mathbb{S}^{d-1}}[g(x)]\Big{|}\] (E.167)

i.e. how close the empirical mean of the \(g(x_{i})\) is to its expected value. We will show that \(\mathcal{C}(g)\) is small when \(g\) corresponds to each of the terms on the right-hand side of Eq. (E.165), even when we take the supremum over the \(\hat{u}_{t}\), which may depend on the \(x_{i}\).

**Concentration for Mean of First Term in Eq. (E.165).** As a first step, we list all of the terms we obtain when expanding the first term on the right-hand side of Eq. (E.165). The \(\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\) factor consists of the following terms: (1) a \(\hat{\sigma}_{2,d}\sqrt{N_{2,d}}\langle\hat{u}_{t},x\rangle\) term, (2) a \(\hat{\sigma}_{4,d}\sqrt{N_{4,d}}\langle\hat{u}_{t},x\rangle^{3}\) term, and (3) a \(\frac{\hat{\sigma}_{4,d}\sqrt{N_{4,d}}}{d}\langle\hat{u}_{t},x\rangle\) term -- here, we have stated these terms up to absolute constant factors for convenience. In the \(f_{\hat{\rho}}(x)-y(x)\) factor, \(f_{\hat{\rho}}(x)\) contributes (1) a \(\hat{\sigma}_{2,d}\sqrt{N_{2,d}}\langle\hat{u}_{t}^{\prime},x\rangle^{2}\) term, (2) a \(\hat{\sigma}_{4,d}\sqrt{N_{4,d}}\langle\hat{u}_{t}^{\prime},x\rangle^{4}\) term, and (3) a \(\frac{\hat{\sigma}_{4,d}\sqrt{N_{4,d}}}{d}\langle\hat{u}_{t}^{\prime},x\rangle^ {2}\) term. Here note that \(\hat{u}_{t}^{\prime}\) is not a single vector, but ranges over the entire support of \(\hat{\rho}_{t}\). Thus we implicitly perform a union bound over the \(m\) vectors in the support of \(\hat{\rho}_{t}\) -- this does not affect the proof since the failure probability of Lemma I.7 is \(\frac{1}{d!^{(\log d)}}\), while we assume the width \(m\) is at most \(d^{C}\) where \(C\) can be any absolute constant. Additionally, \(y(x)\) consists of the following terms: (1) a \(\hat{\sigma}_{2,d}\gamma_{2}\sqrt{N_{2,d}}\langle e_{1},x\rangle^{2}\) term, (2) a \(\hat{\sigma}_{4,d}\gamma_{4}\sqrt{N_{4,d}}\langle e_{1},x\rangle^{4}\) term, and (3) a \(\frac{\hat{\sigma}_{4,d}\gamma\sqrt{N_{4,d}}}{d}\langle e_{1},x\rangle^{2}\) term. Finally, there is a factor of \(\langle\delta_{t},x\rangle\).

For convenience, throughout this proof, let \(\bar{\delta}_{t}=\frac{\delta_{t}}{\|\delta_{t}\|_{2}}\). In each of the above factors, for every occurrence of \(\langle\hat{u}_{t},x\rangle\), we substitute \(\hat{u}_{t}=\bar{u}_{t}+\delta_{t}\) and further expand. This is useful since originally \(\hat{u}_{t}\) depends on the \(x_{i}\) and thus, when we applied Lemma I.7 we would have to take the supremum over \(\hat{u}_{t}\) -- however, now \(\bar{u}_{t}\) can be considered a fixed vector with respect to the \(x_{i}\), and while we need to take the supremum over \(\delta_{t}\), in each term where \(\delta_{t}\) occurs, we obtain an additional factor of \(\|\delta_{t}\|_{2}\), which reduces the error from uniform convergence. We now completely expand the first term on the right-hand side of Eq. (E.165). First we consider the contribution to \(\mathcal{C}(g)\) of \(f_{\hat{\rho}}(x)\) and later study \(y(x)\). Up to constant factors, it suffices to obtain an upper bound on the concentration of

\[\sum_{\begin{subarray}{c}p+\tau=1,3\\ q+s=2,4\end{subarray}}\hat{\sigma}_{\text{max}}^{2}\text{d}^{\frac{p+1+q+\tau+s }{2}}\langle\delta_{t},x\rangle^{p+1}\langle\delta_{t}^{\prime},x\rangle^{q} \langle\bar{u}_{t},x\rangle^{r}\langle\bar{u}_{t}^{\prime},x\rangle^{s}\] (E.168)

where we have let \(\hat{\sigma}_{\text{max}}=\max(|\hat{\sigma}_{2,d}|,|\hat{\sigma}_{4,d}|)\). Here, the sum ranges over \(p+q=1,3\) and \(q+s=2,4\) since the \(\delta_{t}\) and \(\bar{u}_{t}\) terms are due to the \(\sigma^{\prime}(\langle\hat{u}_{t},x\rangle)\) factor, while the \(\delta_{t}^{\prime}\) and \(\bar{u}_{t}^{\prime}\) terms are due to the \(f_{\hat{\rho}}(x)\) factors.

We now obtain concentration bounds for each of the terms in Eq. (E.168). First, we consider the case where \(p+q\geq 1\), as in this case, Lemma I.7 can be applied (since we can choose in Lemma I.7 so that \(u_{1}=\delta_{t}\) and \(p_{1}=p+1\), and \(u_{2}=\delta^{\prime}_{t}\) and \(p_{2}=q\), and \(p_{3}=p_{4}=0\) -- the hypotheses are then satisfied since \(p_{1}+p_{2}+p_{3}+p_{4}\geq 2\)). From Lemma I.7, we obtain the bound

\[\hat{\sigma}_{\text{max}}^{2}d^{\frac{p+3+r+s+1}{2}}\Big{|}\frac{1}{n}\sum_{i=1 }^{n}\langle\delta_{t},x\rangle^{p+1}\langle\delta^{\prime}_{t},x\rangle^{q} \langle\bar{u}_{t},x\rangle^{r}\langle\bar{u}^{\prime}_{t},x\rangle^{s}-\mathop {\mathbb{E}}_{x\sim\widehat{\mathbb{S}}^{d-1}}[\langle\delta_{t},x\rangle^{p+ 1}\langle\delta^{\prime}_{t},x\rangle^{q}\langle\bar{u}_{t},x\rangle^{r} \langle\bar{u}^{\prime}_{t},x\rangle^{s}]\] (E.169)

\[\lesssim\hat{\sigma}_{\text{max}}^{2}(\log d)^{O(1)}\Big{(}\sqrt{\frac{d}{n}}+ \frac{d^{\frac{p+1+q}{2}}}{n}\Big{)}\|\delta_{t}\|_{2}^{p+1}\|\delta^{\prime} _{t}\|_{2}^{q}\] (E.170)

Here in applying Lemma I.7, we have assumed that the number of samples \(n\) is at most \(d^{C}\) for some absolute constant \(C\), meaning that the \(\frac{1}{d^{D(\log d)}}\) term in the conclusion is a lower-order term. Lastly, we consider the case where \(p+q=0\). To apply Lemma I.7, we can let \(u_{1}=\bar{u}_{t}\), \(p_{1}=1\), \(u_{2}=\frac{\delta_{t}}{\|\delta_{t}\|_{2}}\), \(p_{2}=1\), and let \(p_{3}=p_{4}=0\), with \(w_{1}=\bar{u}_{t}\), \(q_{1}=r-1\) and \(w_{2}=\bar{u}^{\prime}_{t}\), \(q_{2}=s\) -- since \(p_{1}+p_{2}=2\), the hypotheses of Lemma I.7 are still satisfied. This leads to us obtaining only one power of \(\|\delta_{t}\|_{2}\) in the bound for this term: if \(p=q=0\), then we have the bound

\[\hat{\sigma}_{\text{max}}^{2}d^{\frac{p+3+r+s+1}{2}}\Big{|}\frac{1}{n}\sum_{i= 1}^{n}\langle\delta_{t},x\rangle^{p+1}\langle\delta^{\prime}_{t},x\rangle^{q} \langle\bar{u}_{t},x\rangle^{r}\langle\bar{u}^{\prime}_{t},x\rangle^{s}- \mathop{\mathbb{E}}_{x\sim\widehat{\mathbb{S}}^{d-1}}[\langle\delta_{t},x \rangle^{p+1}\langle\delta^{\prime}_{t},x\rangle^{q}\langle\bar{u}_{t},x \rangle^{r}\langle\bar{u}^{\prime}_{t},x\rangle^{s}]\] (E.171)

\[\lesssim\hat{\sigma}_{\text{max}}^{2}(\log d)^{O(1)}\Big{(}\sqrt{\frac{d}{n}} +\frac{d}{n}\Big{)}\|\delta_{t}\|_{2}\] (E.172)

Summing over the possible values of \(p,q,r,s\), we find that the overall contribution to the concentration error of the term with \(f_{\hat{\rho}}\) is, up to logarithmic factors and a \(\hat{\sigma}_{\text{max}}^{2}\) factor,

\[\mathop{\mathbb{E}}_{\delta^{\prime}\sim\hat{\rho}}\sum_{p,q,r,s} \Big{(}\sqrt{\frac{d}{n}}\|\delta_{t}\|_{2}^{p+1}\|\delta^{\prime} _{t}\|_{2}^{q}+\frac{d^{\frac{p+1+q}{2}}}{n}\|\delta_{t}\|_{2}^{p+1}\|\delta^{ \prime}_{t}\|_{2}^{q}\Big{)}\] (E.173) \[\lesssim\mathop{\mathbb{E}}_{\delta^{\prime}\sim\hat{\rho}}\sum_{ p\leq 3,q\leq 4}\Big{(}\sqrt{\frac{d}{n}}\|\delta_{t}\|_{2}^{p+1}\|\delta^{ \prime}_{t}\|_{2}^{q}+\frac{d^{\frac{p+1+q}{2}}}{n}\|\delta_{t}\|_{2}^{p+1}\| \delta^{\prime}_{t}\|_{2}^{q}\Big{)}\] (E.174) \[\lesssim\sqrt{\frac{d}{n}}\|\delta_{t}\|_{2}+\mathop{\mathbb{E}}_{ \delta^{\prime}\sim\hat{\rho}}\sum_{p\leq 3,q\leq 4}\frac{d^{\frac{p+q+1}{2}}}{n}\| \delta_{t}\|_{2}^{p+1}\|\delta^{\prime}_{t}\|_{2}^{q}\] (E.175) \[\lesssim\sqrt{\frac{d}{n}}\|\delta_{t}\|_{2}+\mathop{\mathbb{E}}_{ \delta^{\prime}\sim\hat{\rho}}\sum_{p\leq 3,q\leq 4}\frac{d^{\frac{p+q+1}{2}}}{n}\| \delta_{t}\|_{2}^{p+1}\|\delta^{\prime}_{t}\|_{2}^{q}\] (E.176) \[\lesssim\sqrt{\frac{d}{n}}\|\delta_{t}\|_{2}+\sum_{p\leq 3,q\leq 4} \frac{d^{\frac{p+q+1}{2}}}{n}\|\delta_{t}\|_{2}^{p+1}\mathop{\mathbb{E}}_{ \delta^{\prime}\sim\hat{\rho}}\|\delta^{\prime}_{t}\|_{2}^{q}\] (E.177)

Finally, in order to simplify this bound, we isolate the dominant terms among the terms of the form \(\frac{d^{\frac{p+q+1}{2}}}{n}\|\delta_{t}\|_{2}^{p+1}\mathop{\mathbb{E}}_{ \delta^{\prime}\sim\hat{\rho}}\|\delta^{\prime}_{t}\|_{2}^{q}\), with the following cases. First, consider all the terms where \(p=0\). For \(q\leq 2\) we can bound these terms by \(\frac{d^{\frac{p+q+1}{2}}}{n}\overline{\Delta}^{q}\|\delta_{t}\|_{2}\), and for \(q>2\) we can bound these terms by \(\frac{d^{\frac{p+q+1}{2}}}{n}\cdot\overline{\Delta}^{2}\cdot B_{2}^{q-2}\| \delta_{t}\|_{2}\). Thus, out of these terms, the term with \(q=4\) is dominant, and it contributes

\[\frac{d^{2.5}}{n}\cdot\overline{\Delta}^{2}\cdot B_{2}^{2}\|\delta_{t}\|_{2}\] (E.178)

Next, consider the case where \(p\geq 1\). In this case, since the bound \(B_{2}\) on \(\Delta_{\text{max},t}\) is greater than \(\frac{1}{d^{1/2}}\), the term with \(p=3\) and \(q=4\) dominates, and it contributes

\[\frac{d^{4}}{n}\cdot B_{2}^{2}\overline{\Delta}^{2}\cdot B_{2}^{2}\|\delta_{t}\|_ {2}^{2}=\frac{d^{4}}{n}B_{2}^{4}\overline{\Delta}^{2}\|\delta_{t}\|_{2}^{2}\] (E.179)

However, we must lastly consider the case where \(p\geq 1\) and \(q<2\) -- the bound on the concentration error may be larger for \(q=1\) than \(q=2\) since \(\overline{\Delta}<\frac{1}{\sqrt{d}}\). In this case, since \(B_{2}>\frac{1}{\sqrt{d}}\), we must consider the \(p=3,q=1\) and \(p=3,q=0\) terms, which respectively contribute

\[\frac{d^{2.5}}{n}\overline{\Delta}B_{2}^{2}\|\delta_{t}\|_{2}^{2}\] (E.180)

and

\[\frac{d^{2}}{n}B_{2}^{2}\|\delta_{t}\|_{2}^{2}\] (E.181)

Note that the \(p=3,q=0\) term dominates since our bound on \(\overline{\Delta}\) is less than \(\frac{1}{\sqrt{d}}\), meaning we can ignore the \(p=3,q=1\) term. Thus our overall bound on the concentration error originating from the first term in Eq. (E.165) is

\[\hat{\sigma}_{\text{max}}^{2}(\log d)^{O(1)}\cdot\Big{(}\sqrt{\frac{d}{n}}\| \delta_{t}\|_{2}+\frac{d^{2}}{n}B_{2}^{2}\overline{\Delta}\|\delta_{t}\|_{2}^{ 2}+\frac{d^{2.5}}{n}\overline{\Delta}^{2}B_{2}^{2}\|\delta_{t}\|_{2}+\frac{d^ {4}}{n}B_{2}^{4}\overline{\Delta}^{2}\|\delta_{t}\|_{2}^{2}\Big{)}\] (E.182)

To deal with the terms originating from \(y(x)\), we observe that we can simply treat \(\langle e_{1},x\rangle\) in the same way as we deal with \(\langle\bar{u}_{t},x\rangle\), and thus the terms originating from \(y(x)\) do not make any new contributions to the concentration error.

**Concentration for Mean of Second Term in Eq. (E.165).** Finally, to deal with the second term on the right-hand side of Eq. (E.165), we note that we can follow the same proof, but instead obtaining an upper bound on the concentration of

\[\|\delta_{t}\|_{2}^{2}\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\! \!\!

**Lemma E.13** (Error from Finite Width).: _Let \(\rho\) be a distribution on \(\mathbb{S}^{d-1}\), and let \(\bar{\rho}\) be a distribution on \(\mathbb{S}^{d-1}\) of support at most \(m\), where the elements of the support of \(\bar{\rho}\) are sampled i.i.d. from \(\rho\). Additionally, let \(\delta>0\). Then, for some absolute constant \(C>0\),_

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-f_{\bar{\rho} }(x))^{2}]\lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(d^{2}+ \log\frac{1}{\delta})(\log d)^{C}}{m}+\frac{\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2}}{d^{\Omega(\log d)}}\] (E.187)

_with probability at least \(1-\delta\), where \(f_{\rho}\) and \(f_{\bar{\rho}}\) are defined as in Eq. (3.1)._

Proof of Lemma e.13.: We can write \(f_{\rho}(x)=\operatorname*{\mathbb{E}}_{u\sim\rho}[\sigma(\langle u,x\rangle)]\), and \(f_{\bar{\rho}}(x)=\frac{1}{m}\sum_{i=1}^{m}\sigma(\langle u_{i},x\rangle)\). Now, let \(B\in(0,1)\), which we will choose appropriately later, and define the function \(\varphi_{B}:[-1,1]\to[-1,1]\) by

\[\varphi_{B}(t)=\begin{cases}t&t\in[-B,B]\\ B&t>B\\ -B&t<-B\end{cases}\] (E.188)

Additionally, define \(\tilde{f}_{\rho}(x)=\operatorname*{\mathbb{E}}_{u\sim\rho}[\sigma(\varphi_{B }(\langle u,x\rangle))]\), and define \(\tilde{f}_{\rho}(x)=\frac{1}{m}\sum_{i=1}^{m}\sigma(\varphi_{B}(\langle u_{i},x\rangle))\). Let us analyze the error incurred from replacing \(f_{\rho}\) be \(\tilde{f}_{\rho}\) and \(f_{\bar{\rho}}\) by \(\tilde{f}_{\bar{\rho}}\). Observe that

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-\tilde{f}_{ \rho}(x))^{2}]=\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}(f_{\rho}(x)) ^{2}-2\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}f_{\rho}(x)\tilde{f}_ {\rho}(x)+\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}(\tilde{f}_{\rho} (x))^{2}\] (E.189)

We can simplify the second term on the right-hand side by expanding \(f_{\rho}\) and \(\tilde{f}_{\rho}\):

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}f_{\rho}(x) \tilde{f}_{\rho}(x) =\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\operatorname*{ \mathbb{E}}_{u_{1},x\sim\rho}\sigma(\langle u_{1},x\rangle)\sigma(\varphi_{B} (\langle u_{2},x\rangle))\] (E.190) \[=\operatorname*{\mathbb{E}}_{u_{1},u_{2}\sim\rho}\operatorname*{ \mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\sigma(\langle u_{1},x\rangle)\sigma( \varphi_{B}(\langle u_{2},x\rangle))\] (E.191)

To simplify the inner expectation, we use Proposition I.5 to find that \(\varphi_{B}(\langle u_{2},x\rangle)=\langle u_{2},x\rangle\) with probability at least \(1-2\exp(-\frac{B^{2}d}{2})\). Thus,

\[\Big{|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}(f_{ \rho}(x))^{2}-\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}f_{\rho}(x) \tilde{f}_{\rho}(x)\Big{|}\] (E.192) \[\leq\operatorname*{\mathbb{E}}_{u_{1},u_{2}\sim\rho}\operatorname* {\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}|\sigma(\langle u_{1},x\rangle)|\Big{|} \sigma(\varphi_{B}(\langle u_{2},x\rangle))-\sigma(\langle u_{2},x\rangle) \Big{|}\] (E.193) \[\leq(|\hat{\sigma}_{2,d}|\sqrt{N_{2,d}}+|\hat{\sigma}_{4,d}|\sqrt {N_{4,d}})\operatorname*{\mathbb{E}}_{u_{2}\sim\rho}\operatorname*{\mathbb{E} }_{x\sim\mathbb{S}^{d-1}}\Big{|}\sigma(\varphi_{B}(\langle u_{2},x\rangle))- \sigma(\langle u_{2},x\rangle)\Big{|}\] (By Lemma E.12) \[\lesssim\exp\Big{(}-\frac{B^{2}d}{2}\Big{)}\cdot(\hat{\sigma}_{2,d}^{2}N_{2,d}+\hat{\sigma}_{4,d}^{2}N_{4,d})\quad\text{ \quad(By Proposition I.5 and Lemma E.12)}\]

Similarly, we can show that

\[\Big{|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}(f_{\rho}(x))^{2}- \operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}(\tilde{f}_{\rho}(x))^{2} \Big{|}\lesssim\exp\Big{(}-\frac{B^{2}d}{2}\Big{)}\cdot(\hat{\sigma}_{2,d}^{2} N_{2,d}+\hat{\sigma}_{4,d}^{2}N_{4,d})\] (E.194)

and therefore,

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-\tilde{f}_{ \rho}(x))^{2}]\lesssim\exp\Big{(}-\frac{B^{2}d}{2}\Big{)}\cdot(\hat{\sigma}_{2,d }^{2}N_{2,d}+\hat{\sigma}_{4,d}^{2}N_{4,d})\] (E.195)

By the same argument, we can show that

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\bar{\rho}}(x)-\tilde{f}_ {\bar{\rho}}(x))^{2}]\lesssim\exp\Big{(}-\frac{B^{2}d}{2}\Big{)}\cdot(\hat{ \sigma}_{2,d}^{2}N_{2,d}+\hat{\sigma}_{4,d}^{2}N_{4,d})\] (E.196)

Therefore, it suffices to show that \(\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(\tilde{f}_{\rho}(x)-\tilde{f}_ {\bar{\rho}}(x))^{2}]\) is small with high probability over \(u_{1},\dots,u_{m}\). Observe that for \(|t|\leq B\), we have \(|P_{2,d}(t)|\leq B^{2}+O(1/d)\), and \(|P_{4,d}(t)|\leq B^{4}+O(B^{2}/d)+O(1/d^{2})\). Thus, if we choose \(B\geq 1/\sqrt{d}\), then for \(|t|\leq B\) we have \(|P_{2,d}(t)|\lesssim B^{2}\) and \(|P_{4,d}(t)|\lesssim B^{4}\). In particular, we have

\[|\sigma(t)|\lesssim|\hat{\sigma}_{2,d}|\sqrt{N_{2,d}}B^{2}+|\hat{\sigma}_{4,d}| \sqrt{N_{4,d}}B^{4}\,.\] (E.197)Now, letting \(x\in\mathbb{S}^{d-1}\), and using Hoeffding's inequality, we find that if \(u_{1},\ldots,u_{m}\) are i.i.d. samples from \(\rho\), then

\[\Big{|}\frac{1}{m}\sum_{i=1}^{m}\sigma(\varphi_{B}(\langle u_{1},x \rangle)-\mathop{\mathbb{E}}_{u\sim\rho}[\sigma(\varphi_{B}(\langle u,x\rangle) )]\Big{|}\leq t\] (E.198)

with probability at least \(1-2\exp\Big{(}-\frac{Cmt^{2}}{\hat{\sigma}_{2,d}^{2}N_{2,d}B^{4}+\hat{\sigma}_{ 4,d}^{2}N_{4,d}B^{8}}\Big{)}\) for some universal constant \(C\). Now, by Corollary 4.2.13 of Vershynin [74], for any \(\gamma>0\), there exists \(\mathcal{N}\subset\mathbb{S}^{d-1}\) of size at most \((\frac{C}{\gamma})^{d}\) such that for any \(x\in\mathbb{S}^{d-1}\), there exists \(y\in\mathbb{S}^{d-1}\) such that \(\|x-y\|_{2}\leq\gamma\). Thus, with probability at least \(1-2\Big{(}\frac{C}{\gamma}\Big{)}^{d}\exp\Big{(}-\frac{Cmt^{2}}{\hat{\sigma}_{ 2,d}^{2}N_{2,d}B^{4}+\hat{\sigma}_{4,d}^{2}N_{4,d}B^{8}}\Big{)}\), for all \(y\in\mathcal{N}\), we have

\[\Big{|}\frac{1}{m}\sum_{i=1}^{m}\sigma(\varphi_{B}(\langle u_{1}, y\rangle)-\mathop{\mathbb{E}}_{u\sim\rho}[\sigma(\varphi_{B}(\langle u,y \rangle))]\Big{|}\leq t\,.\] (E.199)

If Eq. (E.199) holds for any \(y\in\mathcal{N}\), then for any \(x\in\mathbb{S}^{d-1}\), since \(|\sigma^{\prime}(t)|\lesssim|\hat{\sigma}_{2,d}|\sqrt{N_{2,d}}+|\hat{\sigma}_{ 4,d}|\sqrt{N_{4,d}}\) for any \(t\in[-1,1]\) (see Lemma E.12), if we let \(y\in\mathcal{N}\) such that \(\|x-y\|_{2}\leq\gamma\), then we have

\[\Big{|}\frac{1}{m}\sum_{i=1}^{m}\sigma(\varphi_{B}(\langle u_{1}, x\rangle))-\mathop{\mathbb{E}}_{u\sim\rho}[\sigma(\varphi_{B}(\langle u,x \rangle))]\Big{|}\leq t+(|\hat{\sigma}_{2,d}|\sqrt{N_{2,d}}+|\hat{\sigma}_{4,d }|\sqrt{N_{4,d}})\gamma\,.\] (E.200)

In summary, with probability at least \(1-2\Big{(}\frac{C}{\gamma}\Big{)}^{d}\exp\Big{(}-\frac{Cmt^{2}}{\hat{\sigma}_{ 2,d}^{2}N_{2,d}B^{4}+\hat{\sigma}_{4,d}^{2}N_{4,d}B^{8}}\Big{)}\), we have

\[\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(\tilde{f}_{\rho}( x)-\tilde{f}_{\tilde{\rho}}(x))^{2}]\leq t^{2}+(\hat{\sigma}_{2,d}^{2}N_{2,d}+ \hat{\sigma}_{4,d}^{2}N_{4,d})\gamma^{2}\,.\] (E.201)

Combining Eq. (E.201), Eq. (E.195) and Eq. (E.196), we have that

\[\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-f_{ \tilde{\rho}}(x))^{2}]\lesssim t^{2}+(\hat{\sigma}_{2,d}^{2}N_{2,d}+\hat{ \sigma}_{4,d}^{2}N_{4,d})\gamma^{2}\] (E.202) \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\exp \Big{(}-\frac{B^{2}d}{\hat{\sigma}_{2,d}^{2}N_{2,d}+\hat{\sigma}_{4,d}^{2}N_{ 4,d}}\Big{)}\,.\] (E.203)

with probability at least \(1-2\Big{(}\frac{C}{\gamma}\Big{)}^{d}\exp\Big{(}-\frac{Cmt^{2}}{\hat{\sigma}_{ 2,d}^{2}N_{2,d}B^{4}+\hat{\sigma}_{4,d}^{2}N_{4,d}B^{8}}\Big{)}\). Choosing \(B=\frac{C^{\prime}(\log d)^{2}}{\sqrt{d}}\) for a sufficiently large universal constant \(C^{\prime}\), and using the fact that \(N_{2,d}\lesssim d^{2}\) and \(N_{4,d}\lesssim d^{4}\) (by Eq. (2.10) of Atkinson and Han [12]) we have that

\[\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-f_{ \tilde{\rho}}(x))^{2}]\lesssim t^{2}+(\hat{\sigma}_{2,d}^{2}d^{2}+\hat{\sigma} _{4,d}^{2}d^{4})\gamma^{2}+\frac{\hat{\sigma}_{2,d}^{2}d^{2}+\hat{\sigma}_{4,d }^{2}d^{4}}{d^{\Omega(\log d)}}\] (E.204)

with probability at least \(1-2\Big{(}\frac{C}{\gamma}\Big{)}^{d}\exp\Big{(}-\Omega\Big{(}\frac{mt^{2}}{( \hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{16}}\Big{)}\Big{)}\). We can rearrange the failure probability to find that, up to constant factors, it is at most \(\exp\Big{(}-\Omega\Big{(}\frac{mt^{2}}{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{ 4,d}^{2})(\log d)^{16}}\Big{)}+d\log\frac{C}{\gamma}\Big{)}\). Finally, choosing \(\gamma=\frac{1}{d^{\Omega(\log d)}}\), and choosing \(t\) so that the failure probability is at most \(\delta\), we obtain

\[\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-f_{ \tilde{\rho}}(x))^{2}] \lesssim\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d )^{16}(d\log\frac{C}{\gamma}+\log\frac{1}{\delta})}{m}+\frac{\hat{\sigma}_{2,d} ^{2}d^{2}+\hat{\sigma}_{4,d}^{2}d^{4}}{d^{\Omega(\log d)}}\] (E.205) \[=\frac{(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{16 }(d^{2}\log d+\log\frac{1}{\delta})}{m}+\frac{\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2}}{d^{\Omega(\log d)}}\] (B.c. \[\gamma=\frac{1}{d^{\Omega(\log d)}}\] )

with probability at least \(1-\delta\), as desired. 

**Lemma E.14** (Bound on Gradient).: _Let \(u\in\mathbb{S}^{d-1}\) and \(\rho\) be a distribution with support on \(\mathbb{S}^{d-1}\). In addition, let \(\nabla_{u}L(\rho)\) and \(\mathop{\mathrm{grad}}_{u}L(\rho)\) be defined as in Eq. (3.4). Then, we have \(\|\nabla_{u}L(\rho)\|_{2}\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d ^{4}\) and \(\|\mathop{\mathrm{grad}}_{u}L(\rho)\|_{2}\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2})d^{4}\)._

_Similarly, let \(\nabla_{u}\widehat{L}(\rho)\) and \(\mathop{\mathrm{grad}}_{u}\widehat{L}(\rho)\) be defined as in Eq. (3.5). Then, we have \(\|\nabla_{u}\widehat{L}(\rho)\|_{2}\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{ 4,d}^{2})d^{4}\) and \(\|\mathop{\mathrm{grad}}_{u}\widehat{L}(\rho)\|_{2}\lesssim(\hat{\sigma}_{2,d}^{2}+ \hat{\sigma}_{4,d}^{2})d^{4}\)._Proof of Lemma e.14.: First, we will bound \(\nabla_{u}L(\rho)\) for any \(u\in\mathbb{S}^{d-1}\) and distribution \(\rho\) with support in \(\mathbb{S}^{d-1}\):

\[\|\nabla_{u}L(\rho)\|_{2} =\Big{\|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{ \rho}(x)-y(x))\sigma^{\prime}(\langle u,x\rangle)x]\Big{\|}_{2}\] (E.206) \[\leq\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}|f_{\rho}(x )-y(x)||\sigma^{\prime}(\langle u,x\rangle)|\] (E.207)

By Lemma E.12, for any \(u,x\in\mathbb{S}^{d-1}\), we have \(|\sigma^{\prime}(\langle u,x\rangle)|\lesssim|\hat{\sigma}_{2,d}|\sqrt{N_{2,d} }+|\hat{\sigma}_{4,d}|\sqrt{N_{4,d}}\). Additionally, by Eq. (3.1) and by the definition of \(y(x)\), we have \(|f_{\rho}(x)-y(x)|\leq|\hat{\sigma}_{2,d}|\sqrt{N_{2,d}}+|\hat{\sigma}_{4,d}| \sqrt{N_{4,d}}\) by Lemma E.12. Thus,

\[\|\nabla_{u}L(\rho)\|_{2}\lesssim\hat{\sigma}_{2,d}^{2}N_{2,d}+\hat{\sigma}_{ 4,d}^{2}N_{4,d}\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{4}\] (E.208)

because \(N_{2,d}\lesssim d^{2}\) and \(N_{4,d}\lesssim d^{4}\) by Eq. (2.10) of Atkinson and Han [12]. By the same argument, for any \(u\in\mathbb{S}^{d-1}\),

\[\|(I-uu^{\top})\nabla_{u}L(\rho)\|_{2}\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{ \sigma}_{4,d}^{2})d^{4}\] (E.209)

because \(I-uu^{\top}\) is an orthogonal projection matrix. This completes the proof -- the proof of the analogous statements for \(\nabla_{u}\widetilde{L}(\rho)\) and \(\operatorname{grad}_{u}\widetilde{L}(\rho)\) follow from the same arguments. 

**Lemma E.15**.: _Let \(u,u^{\prime}\in\mathbb{S}^{d-1}\). Then, we have_

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(\sigma^{\prime}(u^{\top} x)-\sigma^{\prime}(u^{\prime\top}x))^{2}(u^{\top}x-u^{\prime\top}x)^{2}]\lesssim( \hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\|u-u^{\prime}\|_{2}^{4}\] (E.210)

_and_

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[\sigma^{\prime}(u^{\top} x)^{2}(u^{\prime\top}x)^{2}]\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\,.\] (E.211)

Proof of Lemma e.15.: For convenience, let \(\delta:=u^{\prime}-u\). Then, we have

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(\sigma^{\prime}(u^{\top} x)-\sigma^{\prime}(u^{\prime\top}x))^{2}(u^{\top}x-u^{\prime\top}x)^{2}]\] (E.212)

\[\lesssim\hat{\sigma}_{2,d}^{2}N_{2,d}\operatorname*{\mathbb{E}}_{x\sim \mathbb{S}^{d-1}}[(P_{2,d}{}^{\prime}(u^{\top}x)-P_{2,d}{}^{\prime}(u^{\prime \top}x))^{2}(u^{\top}x-u^{\prime\top}x)^{2}]\] (E.213)

\[+\hat{\sigma}_{4,d}^{2}N_{4,d}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1 }}[(P_{4,d}{}^{\prime}(u^{\top}x)-P_{4,d}{}^{\prime}(u^{\prime\top}x))^{2}(u^ {\top}x-u^{\prime\top}x)^{2}]\]

(By definition of \(\sigma\) and \(\overline{P}_{k,d}\) and b.c. \((a+b)^{2}\lesssim a^{2}+b^{2}\))

\[\lesssim\hat{\sigma}_{2,d}^{2}N_{2,d}\operatorname*{\mathbb{E}}_{x\sim \mathbb{S}^{d-1}}[(\delta^{\top}x)^{4}]+\hat{\sigma}_{4,d}^{2}N_{4,d} \operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[((u^{\top}x)^{3}-(u^{\prime \top}x)^{3})^{2}(u^{\top}x-u^{\prime\top}x)^{2}]\] (E.214)

\[+\frac{\hat{\sigma}_{4,d}^{2}N_{4,d}}{d^{2}}\operatorname*{\mathbb{E}}_{x\sim \mathbb{S}^{d-1}}[(u^{\top}x-u^{\prime\top}x)^{4}]\] (By Eq. (C.4) and expanding the term with \(P_{4,d}{}^{\prime}\))

\[\lesssim\hat{\sigma}_{2,d}^{2}N_{2,d}\operatorname*{\mathbb{E}}_{x\sim \mathbb{S}^{d-1}}[(\delta^{\top}x)^{4}]+\hat{\sigma}_{4,d}^{2}N_{4,d} \sum_{p+q=2}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(u^{\top}x)^{2 p}(u^{\prime\top}x)^{2q}(\delta^{\top}x)^{4}]\] (E.215)

\[+\frac{\hat{\sigma}_{4,d}^{2}N_{4,d}}{d^{2}}\operatorname*{\mathbb{E}}_{x\sim \mathbb{S}^{d-1}}[(\delta^{\top}x)^{4}]\] (E.216)

Here the last inequality is due to the identity \(a^{3}-b^{3}=(a-b)(a^{2}+ab+b^{2})\), which gives \(((u^{\prime\top}x)^{3}-(u^{\top}x)^{3})^{2}=((u^{\prime\top}x)^{2}+(u^{\prime \top}x)(u^{\top}x)+(u^{\top}x)^{2})^{2}(u^{\prime\top}x-u^{\top}x)^{2}\) -- expanding the first factor fully gives terms of the form \((u^{\prime\top}x)^{p}(u^{\top}x)^{q}\) where \(p\) and \(q\) are both even and \(p+q=4\). Thus, applying Lemma I.17 to the final expression above gives

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(\sigma^{\prime}(u^{\top}x)- \sigma^{\prime}(u^{\prime\top}x))^{2}(u^{\top}x-u^{\prime\top}x)^{2}]\] (E.217)

\[\lesssim\hat{\sigma}_{2,d}^{2}N_{2,d}\cdot\frac{1}{d^{2}}\|\delta \|_{2}^{4}+\hat{\sigma}_{4,d}^{2}N_{4,d}\sum_{p+q=2}\frac{1}{d^{p+q+2}}\|\delta \|_{2}^{4}+\frac{\hat{\sigma}_{4,d}^{2}N_{4,d}}{d^{2}}\cdot\frac{1}{d^{2}}\| \delta\|_{2}^{4}\] (By Lemma I.17)

\[\lesssim\hat{\sigma}_{2,d}^{2}\|\delta\|_{2}^{4}+\hat{\sigma}_{4,d}^{2}\|\delta \|_{2}^{4}\] (B.c. \[N_{2,d}\lesssim d^{2}\] and \[N_{4,d}\lesssim d^{4}\] (see Equation 2.10 of Atkinson and Han [12]))which completes the proof of the first statement. The proof of the second statement is similar -- we expand using the definition of \(\sigma\) and then apply Lemma I.17 to each of the terms. We omit the details of the calculation. 

**Lemma E.16**.: _Let \(\rho\) and \(\rho^{\prime}\) be two distributions on \(\mathbb{S}^{d-1}\) which both have support size \(m<\infty\), and consider a coupling \((u,u^{\prime})\sim(\rho,\rho^{\prime})\) of the two distributions. Additionally, let \(f_{\rho}\) and \(f_{\rho^{\prime}}\) be defined as in Eq. (3.1). Then,_

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-f_{\rho^{ \prime}}(x))^{2}]\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) \operatorname*{\mathbb{E}}_{(u,u^{\prime})\sim(\rho,\rho^{\prime})}\|u-u^{ \prime}\|_{2}^{2}\] (E.218)

Proof of Lemma e.16.: We can write the left-hand side as

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)- f_{\rho^{\prime}}(x))^{2}] =\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{[}\Big{(} \operatorname*{\mathbb{E}}_{(u,u^{\prime})\sim(\rho,\rho^{\prime})}[\sigma( \langle u,x\rangle)-\sigma(\langle u^{\prime},x\rangle)]\Big{)}^{2}\Big{]}\] (E.219) \[\leq\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\operatorname *{\mathbb{E}}_{(u,u^{\prime})\sim(\rho,\rho^{\prime})}\Big{(}\sigma(\langle u,x\rangle)-\sigma(\langle u^{\prime},x\rangle)\Big{)}^{2}\] (E.220) \[=\operatorname*{\mathbb{E}}_{(u,u^{\prime})\sim(\rho,\rho^{\prime })}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(}\sigma(\langle u,x\rangle)-\sigma(\langle u^{\prime},x\rangle)\Big{)}^{2}\] (E.221)

For any \(u,u^{\prime}\in\mathbb{S}^{d-1}\) and \(x\in\mathbb{S}^{d-1}\), by the mean-value theorem, there exists \(\lambda\) which is a convex combination of \(\langle u,x\rangle\) and \(\langle u^{\prime},x\rangle\) such that

\[\sigma(\langle u,x\rangle)-\sigma(\langle u^{\prime},x\rangle)=\sigma^{\prime} (\lambda)(\langle u,x\rangle-\langle u^{\prime},x\rangle)\,.\] (E.222)

Using a similar argument as in the proof of Lemma E.15, we can show that

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{[}\sigma^{\prime}( \lambda)^{2}(\langle u,x\rangle-\langle u^{\prime},x\rangle)^{2}\Big{]} \lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\|u-u^{\prime}\|_{2}^{2}\,.\] (E.223)

The proof proceeds by expanding \(\sigma^{\prime}(\lambda)\) into each of its terms and using the inequality \(|a+b|^{3}\lesssim|a|^{3}+|b|^{3}\) (which holds for any two real numbers \(a,b\)) in order to consider only terms which involve one of \(u\) or \(u^{\prime}\) -- the resulting terms are of the same form as those which appear in Eq. (E.216). We omit the details of the calculation. Thus,

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)- f_{\rho^{\prime}}(x))^{2}] \leq\operatorname*{\mathbb{E}}_{(u,u^{\prime})\sim(\rho,\rho^{ \prime})}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(}\sigma( \langle u,x\rangle)-\sigma(\langle u^{\prime},x\rangle)\Big{)}^{2}\] (E.224) \[\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) \operatorname*{\mathbb{E}}_{(u,u^{\prime})\sim(\rho,\rho^{\prime})}\|u-u^{ \prime}\|_{2}^{2}\] (E.225)

as desired. 

## Appendix F Proof of Theorem 3.5

In this section, we build on our analysis from Appendix E to show that projected gradient descent with \(\frac{1}{\operatorname{poly}(d)}\) learning rate can achieve low population loss. In the rest of the section, for convenience, we define \(\hat{\delta}_{t}=\hat{u}_{t}-\hat{u}_{\eta t}\), and omit \(\chi\) when it is clear from context.

**Lemma F.1**.: _Let \(\rho,\rho^{\prime}\) be two distributions on \(\mathbb{S}^{d-1}\), and let \(\Gamma\) be a coupling between \(\rho\) and \(\rho^{\prime}\). Additionally, let \(u_{1},u_{2},v\in\mathbb{S}^{d-1}\). Then, we have_

\[|\nabla_{u_{1}}L(\rho)\cdot v-\nabla_{u_{2}}L(\rho^{\prime})\cdot v|\lesssim( \hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\Big{(}\operatorname*{\mathbb{E}} _{u,u^{\prime}\sim\Gamma}\left[\|u-u^{\prime}\|_{2}^{2}\right]\Big{)}^{1/2}+\|u _{1}-u_{2}\|_{2}\Big{)}\,.\] (F.1)

Proof.: We can expand the left-hand side as

\[\Big{|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\left[(f_{\rho}(x)- y(x))\sigma^{\prime}(u_{1}\cdot x)(v\cdot x)-(f_{\rho^{\prime}}(x)-y(x))\sigma^{ \prime}(u_{2}\cdot x)(v\cdot x)\right]\Big{|}\,.\] (F.2)

We first obtain a bound on

\[\Big{|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\left[f_{\rho}(x) \sigma^{\prime}(u_{1}\cdot x)(v\cdot x)-f_{\rho^{\prime}}(x)\sigma^{\prime}(u_{2 }\cdot x)(v\cdot x)\right]\Big{|}\,.\] (F.3)By the definition of \(f_{\rho}\), we can rewrite Eq. (F.3) as

\[\Big{|}\operatorname*{\mathbb{E}}_{u^{\prime}_{1},u^{\prime}_{2}\sim\Gamma} \operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{[}\sigma(u^{\prime}_{1} \cdot x)\sigma^{\prime}(u_{1}\cdot x)(v\cdot x)-\sigma(u^{\prime}_{2}\cdot x) \sigma^{\prime}(u_{2}\cdot x)(v\cdot x)\Big{]}\Big{|}\] (F.4)

By Eq. (C.4), \(\sigma\) is an even fourth-degree polynomial and \(\sigma^{\prime}\) is an odd third-degree polynomial. Thus, the inner expectation can be written as a linear combination of a constant number of terms of the form

\[\Big{|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{[}(u^{\prime}_{1 }\cdot x)^{i}(u_{1}\cdot x)^{j}(v\cdot x)-(u^{\prime}_{2}\cdot x)^{i}(u_{2} \cdot x)^{j}(v\cdot x)\Big{]}\Big{|}\] (F.5)

where \(i=0,2,4\) and \(j=1,3\). We can also estimate the coefficient which accompanies each individual term Eq. (F.5). We first recall the facts that \(\sigma(t)=\hat{\sigma}_{2,d}\bar{P}_{2,d}(t)+\hat{\sigma}_{4,d}\bar{P}_{4,d}(t)\), that \(N_{2,d}=O(d^{2})\) and \(N_{4,d}=O(d^{4})\) by Equation (2.10) of Atkinson and Han [12], and that the \(i^{\text{th}}\) degree term of \(P_{2,d}\) has a coefficient which is \(O(d^{i/2-1})\) and the \(i^{\text{th}}\) degree term of \(P_{4,d}\) has a coefficient which is \(O(d^{i/2-2})\). From these facts it follows that the \(i^{th}\) degree term of \(\bar{P}_{2,d}=\sqrt{N_{2,d}}P_{2,d}\) has a coefficient which is \(O(d^{i/2})\) and the \(i^{th}\) degree term of \(\bar{P}_{4,d}=\sqrt{N_{4,d}}P_{4,d}\) has a coefficient which is \(O(d^{i/2})\). Thus, each term of the form given in Eq. (F.5) has a coefficient which is at most \((\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{(i+j+1)/2}\) in absolute value. We can now upper bound each of the terms of the form given in Eq. (F.5) as follows:

\[\Big{|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}} \Big{[}(u^{\prime}_{1}\cdot x)^{i}(u_{1}\cdot x)^{j}(v\cdot x)-(u^ {\prime}_{2}\cdot x)^{i}(u_{2}\cdot x)^{j}(v\cdot x)\Big{]}\Big{|}\] (F.6) \[\leq\Big{|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}} \Big{[}\Big{(}(u^{\prime}_{1}\cdot x)^{i}-(u^{\prime}_{2}\cdot x)^{i}\Big{)} (u_{1}\cdot x)^{j}(v\cdot x)\Big{]}\Big{|}\] (F.7) \[+\Big{|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}} \Big{[}(u^{\prime}_{2}\cdot x)^{i}\Big{(}(u_{1}\cdot x)^{j}-(u_{2}\cdot x)^{j }\Big{)}(v\cdot x)\Big{]}\Big{|}\] (F.8) \[\leq\sum_{k=0}^{i-1}\Big{|}\operatorname*{\mathbb{E}}_{x\sim \mathbb{S}^{d-1}}\Big{[}\Big{(}(u^{\prime}_{1}-u^{\prime}_{2})\cdot x\Big{)}( u^{\prime}_{1}\cdot x)^{k}(u^{\prime}_{2}\cdot x)^{i-1-k}(u_{1}\cdot x)^{j}(v \cdot x)\Big{]}\Big{|}\] (F.9) \[+\sum_{k=0}^{j-1}\Big{|}\operatorname*{\mathbb{E}}_{x\sim \mathbb{S}^{d-1}}\Big{[}(u^{\prime}_{2}\cdot x)^{i}\Big{(}(u_{1}-u_{2})\cdot x \Big{)}(u_{1}\cdot x)^{k}(u_{2}\cdot x)^{j-1-k}(v\cdot x)\Big{]}\Big{|}\] (F.10) \[\lesssim\sum_{k=0}^{i-1}\|u^{\prime}_{1}-u^{\prime}_{2}\|_{2}\cdot \frac{1}{d^{(1+k+i-1-k+j+1)/2}}+\sum_{k=0}^{j-1}\|u_{1}-u_{2}\|_{2}\cdot\frac{1 }{d^{(i+1+k+j-1-k+1)/2}}\] (By Lemma I.17) \[\lesssim\|u^{\prime}_{1}-u^{\prime}_{2}\|_{2}\frac{1}{d^{(i+j+1)/2 }}+\|u_{1}-u_{2}\|_{2}\frac{1}{d^{(i+j+1)/2}}\,.\] (F.11)

Since the coefficient of this term is at most \((\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{(i+j+1)/2}\) in absolute value, this term contributes at most

\[(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\Big{(}\|u^{\prime}_{1}-u^{ \prime}_{2}\|_{2}+\|u_{1}-u_{2}\|_{2}\Big{)}\,.\] (F.12)

Thus, taking the expectation over \(u^{\prime}_{1},u^{\prime}_{2}\sim\Gamma\), we have

\[\Big{|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{[} f_{\rho}(x)\sigma^{\prime}(u_{1}\cdot x)(v\cdot x)- f_{\rho^{\prime}}(x)\sigma^{\prime}(u_{2}\cdot x)(v\cdot x) \Big{]}\Big{|}\] (F.13) \[\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\Big{(} \operatorname*{\mathbb{E}}_{u,u^{\prime}\sim\Gamma}\Big{[}\|u-u^{\prime}\|_{2} \Big{]}+\|u_{1}-u_{2}\|_{2}\Big{)}\] (F.14) \[\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\Big{(} \operatorname*{\mathbb{E}}_{u,u^{\prime}\sim\Gamma}\Big{[}\|u-u^{\prime}\|_{2}^{2 }\Big{]}^{1/2}+\|u_{1}-u_{2}\|_{2}\Big{)}\,.\] (F.15)

A similar but simpler argument can be used to bound

\[\Big{|}\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{[}y(x)\sigma^{ \prime}(u_{1}\cdot x)(v\cdot x)-y(x)\sigma^{\prime}(u_{2}\cdot x)(v\cdot x) \Big{]}\Big{|}\] (F.16)by expanding \(y(x)\) and \(\sigma^{\prime}(x)\). Thus, we obtain

\[|\nabla_{u_{1}}L(\rho)\cdot v-\nabla_{u_{2}}L(\rho)\cdot v|\leq( \hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\Big{(}\mathop{\mathbb{E}}_{u,u^ {\prime}\sim\Gamma}\Big{[}\|u-u^{\prime}\|_{2}^{2}\Big{]}\Big{]}^{1/2}+\|u_{1}-u_ {2}\|_{2}\Big{)}\] (F.17)

as desired. 

**Lemma F.2**.: _Suppose we are in the setting of Theorem 3.5. Assume that \(n\gtrsim d^{3}(\log d)^{C}\) for a sufficiently large universal constant \(C>0\). Additionally, assume that \(\max_{\chi}\|\tilde{\delta}_{t}(\chi)\|_{2}\leq\frac{1}{\sqrt{d}}\). Then, we have_

\[|(\text{\rm grad}_{\tilde{u}_{t}}\widehat{L}(\tilde{\rho}_{t})- \text{\rm grad}_{\tilde{a}_{\eta t}}\widehat{L}(\hat{\rho}_{\eta t}))\cdot \tilde{\delta}_{t}|\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) \max_{\chi}\|\tilde{\delta}_{t}(\chi)\|_{2}^{2}\] (F.18)

Proof.: We can write the left-hand side of Eq. (F.18) as

\[(\text{\rm grad}_{\tilde{u}_{t}}\widehat{L}(\tilde{\rho}_{t})- \text{\rm grad}_{\tilde{a}_{\eta t}}\widehat{L}(\hat{\rho}_{t}))\cdot\tilde{ \delta}_{t}\] (F.19) \[=\frac{1}{n}\sum_{i=1}^{n}\Big{(}(f_{\tilde{\rho}_{t}}(x_{i})-y( x_{i}))\sigma^{\prime}(\tilde{u}_{t}\cdot x_{i})-(f_{\hat{\rho}_{\eta t}}(x_{i})-y( x_{i}))\sigma^{\prime}(\hat{u}_{\eta t}\cdot x_{i})\Big{)}x_{i}\cdot\tilde{ \delta}_{t}\,.\] (F.20)

We proceed by first showing using Lemma I.7 that we do not incur much error when we replace the empirical average over the \(x_{i}\) by an expectation over \(x\sim\mathbb{S}^{d-1}\). We can separate the right hand side of Eq. (F.19) into two parts:

\[S_{1}=\frac{1}{n}\sum_{i=1}^{n}\Big{(}f_{\tilde{\rho}_{t}}(x_{i })\sigma^{\prime}(\tilde{u}_{t}\cdot x_{i})-f_{\hat{\rho}_{\eta t}}(x_{i}) \sigma^{\prime}(\hat{u}_{\eta t}\cdot x_{i})\Big{)}(\tilde{\delta}_{t}\cdot x _{i})\] (F.21)

and

\[S_{2}=\frac{1}{n}\sum_{i=1}^{n}\Big{(}y(x_{i})\sigma^{\prime}( \tilde{u}_{t}\cdot x_{i})-y(x_{i})\sigma^{\prime}(\hat{u}_{\eta t}\cdot x_{i} )\Big{)}(\tilde{\delta}_{t}\cdot x_{i})\,.\] (F.22)

We wish to show that \(S_{1}\) and \(S_{2}\) are very close to

\[M_{1}=\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{[}\Big{(} f_{\tilde{\rho}_{t}}(x)\sigma^{\prime}(\tilde{u}_{t}\cdot x)-f_{\hat{\rho}_{ \eta t}}(x)\sigma^{\prime}(\hat{u}_{\eta t}\cdot x)\Big{)}(\tilde{\delta}_{t} \cdot x)\Big{]}\] (F.23)

and

\[M_{2}=\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{[}\Big{(} y(x_{i})\sigma^{\prime}(\tilde{u}_{t}\cdot x_{i})-y(x_{i})\sigma^{\prime}( \hat{u}_{\eta t}\cdot x_{i})\Big{)}(\tilde{\delta}_{t}\cdot x_{i})\Big{]}\] (F.24)

respectively, with high probability -- then, we will obtain bounds on \(M_{1}\) and \(M_{2}\). We will show that \(S_{1}\) and \(M_{1}\) are close -- the proof that \(S_{2}\) and \(M_{2}\) are close is similar.

**Expanding into Monomials** Our proof strategy is to expand the following quantity, for \(x\in\mathbb{S}^{d-1}\):

\[G(x)=\Big{(}f_{\tilde{\rho}_{t}}(x)\sigma^{\prime}(\tilde{u}_{t} \cdot x)-f_{\hat{\rho}_{\eta t}}(x)\sigma^{\prime}(\hat{u}_{\eta t}\cdot x) \Big{)}(\tilde{\delta}_{t}\cdot x)\] (F.25)

and obtain concentration bounds for each of the terms. In this proof, we let \(\Gamma\) denote the natural coupling between \(\tilde{\rho},\tilde{\rho},\tilde{\rho}\) which is the distribution over triples \((\tilde{u}_{\eta t}(\chi),\tilde{u}_{\eta t}(\chi),\tilde{u}_{\eta t}(\chi))\), where \(\chi\) is sampled from \(\{\chi_{1},\dots,\chi_{m}\}\). Then, we can write \(G(x)\) as

\[G(x)=\mathop{\mathbb{E}}_{\tilde{u}^{\prime}_{\eta t},\tilde{u}^{ \prime}_{\eta t},\tilde{u}^{\prime}_{\eta t}\sim\Gamma}\Big{[}\Big{(}\sigma( \tilde{u}^{\prime}_{t}\cdot x)\sigma^{\prime}(\tilde{u}_{t}\cdot x)-\sigma( \tilde{u}^{\prime}_{\eta t}\cdot x)\sigma^{\prime}(\hat{u}_{\eta t}\cdot x) \Big{)}\cdot(\tilde{\delta}_{t}\cdot x)\Big{]}\,.\] (F.26)

For convenience, let us define

\[g(x)=\Big{(}\sigma(\tilde{u}^{\prime}_{t}\cdot x)\sigma^{\prime}(\tilde{u}_{t }\cdot x)-\sigma(\hat{u}^{\prime}_{\eta t}\cdot x)\sigma^{\prime}(\hat{u}_{\eta t }\cdot x)\Big{)}\cdot(\tilde{\delta}_{t}\cdot x)\,.\] (F.27)

Clearly we have \(G(x)=\mathop{\mathbb{E}}_{\tilde{u}^{\prime}_{\eta t},\tilde{u}^{\prime}_{\eta t },\tilde{u}^{\prime}_{t}\sim\Gamma}[g(x)]\). Now, we consider the effect of expanding \(g(x)\) by expanding \(\sigma\) and \(\sigma^{\prime}\) into monomials that involve \((\tilde{u}^{\prime}_{t}\cdot x)\), \((\tilde{u}_{t}\cdot x)\), \((\tilde{u}^{\prime}_{\eta t}\cdot x)\) and \((\hat{u}_{\eta t}\cdot x)\). Since \(\sigma\)is an even polynomial of degree \(4\) and \(\sigma^{\prime}\) is an odd polynomial of degree \(3\), we can write \(g(x)\) as a linear combination of terms of the following form:

\[\Big{(}(\tilde{u}^{\prime}_{t}\cdot x)^{i_{1}}(\tilde{u}_{t}\cdot x)^{i_{2}}-( \tilde{u}^{\prime}_{\eta t}\cdot x)^{i_{1}}(\hat{u}_{\eta t}\cdot x)^{i_{2}} \Big{)}(\tilde{\delta}_{t}\cdot x)\] (F.28)

where \(i_{1}=0,2,4\) and \(i_{2}=1,3\) -- each such term also has a constant factor of \(O(\hat{\sigma}_{i_{1},d}\hat{\sigma}_{i_{2},d}\sqrt{N_{i_{1},d}}\sqrt{N_{i_{2 },d}})\). We now consider the contribution of Eq. (F.28) for each value of \(i_{1},i_{2}\). In the case where \(i_{1}=0\), we can rewrite Eq. (F.28) as \((\tilde{\delta}_{t}\cdot x)^{2}\) when \(i_{2}=1\), and

\[\Big{(}(\tilde{u}_{t}\cdot x)^{2}+(\tilde{u}_{t}\cdot x)(\hat{u}_{\eta t} \cdot x)+(\hat{u}_{\eta t}\cdot x)^{2}\Big{)}(\tilde{\delta}_{t}\cdot x)^{2}\] (F.29)

when \(i_{2}=3\). In the case where \(i_{1}\) and \(i_{2}\) are both nonzero, letting \(\tilde{\delta}^{\prime}_{t}=\tilde{u}^{\prime}_{t}-\hat{u}^{\prime}_{\eta t}\) we can write Eq. (F.28) as

\[\Big{(}(\tilde{u}^{\prime}_{t}\cdot x)^{i_{1}}(\tilde{u}_{t} \cdot x)^{i_{2}}-(\hat{u}^{\prime}_{\eta t}\cdot x)^{i_{1}}(\hat{u}_{\eta t} \cdot x)^{i_{2}}\Big{)}(\tilde{\delta}_{t}\cdot x)\] (F.30) \[=\Big{(}(\tilde{u}^{\prime}_{t}\cdot x)^{i_{1}}-(\hat{u}^{\prime }_{\eta t}\cdot x)^{i_{1}}\Big{)}(\tilde{u}_{t}\cdot x)^{i_{2}}(\tilde{\delta }_{t}\cdot x)\] (F.31) \[\qquad+(\hat{u}^{\prime}_{\eta t}\cdot x)^{i_{1}}\Big{(}(\tilde{ u}_{t}\cdot x)^{i_{2}}-(\hat{u}_{\eta t}\cdot x)^{i_{2}}\Big{)}(\tilde{\delta}_{t} \cdot x)\] (F.32) \[=\sum_{\begin{subarray}{c}i_{3}+i_{4}=i_{1}-1\\ i_{3},i_{4}\geq 0\end{subarray}}(\tilde{u}^{\prime}_{t}\cdot x)^{i_{3}}(\hat{u}^{ \prime}_{\eta t}\cdot x)^{i_{4}}(\tilde{u}^{\prime}_{t}\cdot x-\hat{u}^{ \prime}_{\eta t}\cdot x)(\tilde{u}_{t}\cdot x)^{i_{2}}(\tilde{\delta}_{t} \cdot x)\] (F.33) \[\qquad+\sum_{\begin{subarray}{c}i_{3}+i_{4}=i_{2}-1\\ i_{3},i_{4}\geq 0\end{subarray}}(\hat{u}^{\prime}_{\eta t}\cdot x)^{i_{1}}(\tilde{u} _{t}\cdot x)^{i_{3}}(\hat{u}_{\eta t}\cdot x)^{i_{4}}(\tilde{u}_{t}\cdot x- \hat{u}_{\eta t}\cdot x)(\tilde{\delta}_{t}\cdot x)\] (F.34) \[=\sum_{\begin{subarray}{c}i_{3}+i_{4}=i_{1}-1\\ i_{3},i_{4}\geq 0\end{subarray}}(\hat{u}^{\prime}_{t}\cdot x)^{i_{3}}(\hat{u}^{ \prime}_{\eta t}\cdot x)^{i_{4}}(\tilde{u}_{t}\cdot x)^{i_{2}}(\tilde{\delta }_{t}\cdot x)(\tilde{\delta}^{\prime}_{t}\cdot x)\] (F.35) \[\qquad+\sum_{\begin{subarray}{c}i_{3}+i_{4}=i_{2}-1\\ i_{3},i_{4}\geq 0\end{subarray}}(\hat{u}^{\prime}_{\eta t}\cdot x)^{i_{1}}(\tilde{u} _{t}\cdot x)^{i_{3}}(\hat{u}_{\eta t}\cdot x)^{i_{4}}(\tilde{\delta}_{t}\cdot x )^{2}\,.\] (F.36)

In summary, Eq. (F.28) can be written as a linear combination of terms of the form

\[(\tilde{u}^{\prime}_{t}\cdot x)^{i_{3}}(\hat{u}_{\eta t}\cdot x)^{i_{4}}(\tilde {u}_{t}\cdot x)^{i_{2}}(\tilde{\delta}_{t}\cdot x)(\tilde{\delta}^{\prime}_{t} \cdot x)\] (F.37)

where \(i_{3}+i_{4}=1,3\) and \(i_{2}=1,3\), and terms of the form

\[(\hat{u}^{\prime}_{\eta t}\cdot x)^{i_{1}}(\tilde{u}_{t}\cdot x)^{i_{3}}(\hat {u}_{\eta t}\cdot x)^{i_{4}}(\tilde{\delta}_{t}\cdot x)^{2}\] (F.38)

where \(i_{1}=0,2,4\) and \(i_{3}+i_{4}=0,2\).

**Contribution of Eq. (F.38) to Concentration Error** In Eq. (F.38), we can write \(\tilde{u}_{t}=\hat{u}_{\eta t}+\tilde{\delta}_{t}\), and thus write Eq. (F.38) as a linear combination of terms of the form

\[(\hat{u}^{\prime}_{\eta t}\cdot x)^{i_{1}}(\hat{u}_{\eta t}\cdot x)^{i_{2}}( \tilde{\delta}_{t}\cdot x)^{2+i_{3}}\] (F.39)

where \(i_{1}=0,2,4\) and \(i_{2}+i_{3}=0,2\). Next, in order to use Lemma I.7, we write \(\hat{u}_{\eta t}=\bar{u}_{\eta t}+\delta_{t}\) and expand (recalling the definition of \(\delta_{t}\) from Lemma 5.4). Thus, Eq. (F.38) is a linear combination of terms of the form

\[(\bar{u}^{\prime}_{\eta t}\cdot x)^{i_{1}}(\delta^{\prime}_{\eta t}\cdot x)^{i_{2} }(\bar{u}_{\eta t}\cdot x)^{i_{3}}(\delta_{\eta t}\cdot x)^{i_{4}}(\tilde{ \delta}_{t}\cdot x)^{2+i_{5}}\] (F.40)

where \(i_{1}+i_{2}=0,2,4\), and \(i_{3}+i_{4}+i_{5}=0,2\). Finally, we obtain a concentration bound for Eq. (F.40) using Lemma I.7:

\[\Big{|}\frac{1}{n}\sum_{i=1}^{n}(\bar{u}^{\prime}_{\eta t}\cdot x )^{i_{1}}(\delta^{\prime}_{\eta t}\cdot x_{i})^{i_{2}}(\bar{u}_{\eta t}\cdot x )^{i_{3}}(\delta_{\eta t}\cdot x_{i})^{i_{4}}(\tilde{\delta}_{t}\cdot x)^{2+i_{5}}\] (F.41) \[\qquad\qquad-\operatorname*{\mathbb{E}}_{x\sim\delta^{d-1}}\Big{[}( \bar{u}^{\prime}_{\eta t}\cdot x)^{i_{1}}(\delta^{\prime}_{\eta t}\cdot x)^{i_{2}}( \bar{u}_{\eta t}\cdot x)^{i_{3}}(\delta_{\eta t}\cdot x)^{i_{4}}(\tilde{\delta}_{t} \cdot x)^{2+i_{5}}\Big{]}\Big{|}\] (F.42) \[\lesssim(\log d)^{O(1)}\frac{\|\delta^{\prime}_{\eta t}\|_{2}^{i_{2}} \|\tilde{\delta}_{\eta t}\|_{2}^{i_{4}}\|\tilde{\delta}_{t}\|_{2}^{2+i_{5}}}{d^{(i_ {1}+i_{2}+i_{3}+i_{4}+i_{5})/2+1}}\Big{(}\sqrt{\frac{d}{n}}+\frac{d^{(i_{2}+i_{4}+ i_{5})/2+1}}{n}+\frac{1}{d^{\Omega(\log d)}}\Big{)}\,.\] (F.43)Here, when applying Lemma I.7, \(\bar{u}_{\eta t}\) and \(\bar{u}^{\prime}_{\eta t}\) take the role of \(w_{1}\) and \(w_{2}\), while \(\delta^{\prime}_{\eta t}/\|\delta^{\prime}_{\eta t}\|_{2}\), \(\delta_{\eta t}/\|\delta_{\eta t}\|_{2}\) and \(\tilde{\delta}_{t}/\|\tilde{\delta}_{t}\|_{2}\) take the role of \(u_{1}\), \(u_{2}\), \(u_{3}\) and \(u_{4}\) (we let \(u_{3}=u_{4}=\tilde{\delta}_{t}/\|\tilde{\delta}_{t}\|_{2}\) and choose \(p_{3}\) and \(p_{4}\) so that \(p_{3}+p_{4}=2+i_{5}\)). Note that we implicitly perform a union bound over \(\bar{u}^{\prime}_{\eta t}\) and \(\delta^{\prime}_{\eta t}\) drawn from the support of \(f_{\hat{\rho}_{\eta t}}\) -- this does not affect the failure probability, since the failure probability for the result of Lemma I.7 is \(\frac{1}{4!(\log d)}\), and it is assumed in Theorem 3.5 that \(m\leq d^{C}\) for some universal constant \(C\). When applying Lemma I.7 at later points in this proof, we implicitly perform this union bound.

In order to compute the final contribution of each such term to the concentration error between \(S_{1}\) and \(M_{1}\), we must first (1) incorporate the coefficients of \(\sigma\) and \(\sigma^{\prime}\) with respect to the Legendre polynomials \(P_{2,d}\) and \(P_{4,d}\), (2) take the average of \(\|\delta^{\prime}_{\eta t}\|_{2}^{b}\) with respect to \(\chi\) (which is implicit throughout this proof), and (3) use the fact that \(n\gtrsim d^{3}(\log d)^{O(1)}\).

Recall that \(\sigma=\hat{\sigma}_{2,d}\sqrt{N_{2,d}}P_{2,d}+\hat{\sigma}_{4,d}\sqrt{N_{4,d }}P_{4,d}\). Using Eq. (C.4) and the fact that \(N_{2,d}\asymp d^{2}\) and \(N_{4,d}\asymp d^{4}\) (by Equation (2.10) of Atkinson and Han [12]), we find that, up to constant factors, the coefficient of the \(0^{\text{th}}\)-order term in \(\sigma(t)\) can be bounded above by \(\max(\hat{\sigma}_{2,d},\hat{\sigma}_{4,d})\), the coefficient of the \(2^{\text{nd}}\)-order term in \(\sigma(t)\) can be bounded above by \(\max(\hat{\sigma}_{2,d},\hat{\sigma}_{4,d})d\), and the coefficient of the \(4^{\text{th}}\) order term can be bounded above by \(\hat{\sigma}_{4,d}d^{2}\). Similarly, up to constant factors, the coefficient of the \(1^{\text{st}}\)-order term in \(\sigma^{\prime}(t)\) can be bounded above by \(\max(\hat{\sigma}_{2,d},\hat{\sigma}_{4,d})d\) and the coefficient of the \(3^{\text{nd}}\)-order term in \(\sigma^{\prime}(t)\) can be bounded above by \(\max(\hat{\sigma}_{2,d},\hat{\sigma}_{4,d})d^{2}\). Thus, each term of degree \(k\) originating from \(\sigma\) contributes a factor of \(\max(\hat{\sigma}_{2,d},\hat{\sigma}_{4,d})d^{k/2}\) to the coefficient of Eq. (F.38), and each term of degree \(k-1\) originating from \(\sigma^{\prime}\) contributes a factor of \(\max(\hat{\sigma}_{2,d},\hat{\sigma}_{4,d})d^{k/2}\) to the coefficient of Eq. (F.38). Finally, for each term in Eq. (F.40) of total degree \(i_{1}+i_{2}+i_{3}+i_{4}+i_{5}+2\), note that \(\sigma\) and \(\sigma^{\prime}\) together contribute a factor of degree \(i_{1}+i_{2}+i_{3}+i_{4}+i_{5}+1\), since in Eq. (F.27), aside from the factors contributed by \(\sigma\) and \(\sigma^{\prime}\), only a single factor of \(\tilde{\delta}_{t}\cdot x\) is present.

Thus, to obtain the final contribution of Eq. (F.41) to the overall error between \(M_{1}\) and \(S_{1}\), we multiply by an additional coefficient of \(\max(\hat{\sigma}_{2,d}^{2},\hat{\sigma}_{4,d}^{2})d^{(i_{1}+i_{2}+i_{3}+i_{4} +i_{5})/2+1}\). Taking this into account, the contribution of Eq. (F.41) is

\[(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)}\|\delta^{\prime }_{\eta t}\|_{2}^{i_{2}}\|\delta_{\eta t}\|_{2}^{i_{4}}\|\tilde{\delta}_{t}\|_ {2}^{2+i_{5}}\Big{(}\sqrt{\frac{d}{n}}+\frac{d^{(i_{2}+i_{4}+i_{5})/2+1}}{n}+ \frac{1}{d^{\Omega(\log d)}}\Big{)}\,.\] (F.44)

Next, we simplify this using casework on \(i_{2}\). For the case \(i_{2}\leq 1\), since \(i_{4}+i_{5}\leq 2\), the bound is at most

\[(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\|\delta^{\prime}_{\eta t}\|_{2} ^{i_{2}}\|\delta_{\eta t}\|_{2}^{i_{4}}\|\tilde{\delta}_{t}\|_{2}^{2+i_{5}} \lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\|\tilde{\delta}_{t}\|_ {2}^{2}\] (F.45)

since \(d^{(i_{2}+i_{4}+i_{5})/2+1}\leq d^{2.5}\), and because \(n\gtrsim d^{3}(\log d)^{O(1)}\), and we eliminate a factor of \(\|\delta^{\prime}_{\eta t}\|_{2}^{i_{2}}\|\delta_{\eta t}\|_{2}^{i_{4}}\|\tilde {\delta}_{t}\|_{2}^{i_{5}}\) because all of the particles have \(\ell_{2}\) norm at most \(1\). Additionally, for the case \(i_{2}\geq 2\), after taking the average of \(\|\delta^{\prime}_{\eta t}\|_{2}^{i_{2}}\lesssim\|\delta^{\prime}_{\eta t}\|_{2}^ {2}\) across all \(\chi\), the bound is at most

\[(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2}) (\log d)^{O(1)}\overline{\Delta}^{2}_{\eta t}\|\tilde{\delta}_{t}\|_ {2}^{2}\Big{(}\sqrt{\frac{d}{n}}+\frac{d^{(i_{2}+i_{4}+i_{5})/2+1}}{n}+\frac{1} {d^{\Omega(\log d)}}\Big{)}\] (F.46) \[\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\overline{ \Delta}^{2}_{\eta t}\|\tilde{\delta}_{t}\|_{2}^{2}\Big{(}d^{i_{2}/2-1}+O\Big{(} \frac{1}{d}\Big{)}\Big{)}\] (B.c. \[n\gtrsim d^{3}(\log d)^{O(1)}\] and \[i_{4}+i_{5}\leq 2\] ) \[\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\|\tilde{ \delta}_{t}\|_{2}^{2}\] (B.c. \[\overline{\Delta}_{\eta t}\leq\frac{1}{\sqrt{d}}\] and \[i_{2}\leq 4\] )

where we have \(\overline{\Delta}_{\eta t}\leq\frac{1}{\sqrt{d}}\) because by Lemma E.3, we have that Assumption 5.3 holds as long as \(\eta t\leq T_{*,\epsilon}\). In summary, we have shown that terms of the form Eq. (F.38) contribute at most \((\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\|\tilde{\delta}_{t}\|_{2}^{2}\), up to constant factors, to the concentration error between \(S_{1}\) and \(M_{2}\).

**Contribution of Eq. (F.37) to Concentration Error** We expand Eq. (F.37) by using \(\tilde{u}_{t}=\tilde{\delta}_{t}+\delta_{\eta t}+\bar{u}_{\eta t}\), \(\hat{u}_{\eta t}=\delta_{\eta t}+\bar{u}_{\eta t}\) and \(\tilde{u}^{\prime}_{t}=\tilde{\delta}^{\prime}_{t}+\delta^{\prime}_{\eta t}+ \bar{u}^{\prime}_{\eta t}\). Thus, we can write Eq. (F.37) as a linear combination of terms of the form

\[(\tilde{\delta}_{t}\cdot x)^{1+i_{1}}(\tilde{\delta}^{\prime}_{t}\cdot x)^{1+i_{2} }(\delta_{\eta t}\cdot x)^{i_{3}}(\delta^{\prime}_{\eta t}\cdot x)^{i_{4}}( \bar{u}_{\eta t}\cdot x)^{i_{5}}(\bar{u}^{\prime}_{\eta t}\cdot x)^{i_{6}}\] (F.47)where \(i_{1}+i_{2}+i_{3}+i_{4}+i_{5}+i_{6}=6\) (since the degrees of each of the monomials must be \(8\)) and \(i_{3}\leq 3\) (since each of these terms originates from \((\tilde{u}^{\prime}_{t}\cdot x)^{i_{3}}(\hat{u}^{\prime}_{\eta t}\cdot x)^{i_{4} }(\tilde{u}_{t}\cdot x)^{i_{2}}(\tilde{\delta}_{t}\cdot x)(\tilde{\delta}^{ \prime}_{t}\cdot x)\) where \(i_{2}\leq 3\)). Using Lemma I.7, we obtain a concentration bound for Eq. (F.47):

\[\Big{|}\frac{1}{n}\sum_{i=1}^{n}(\tilde{\delta}_{t}\cdot x_{i})^{1+i_{1}}( \tilde{\delta}^{\prime}_{t}\cdot x_{i})^{1+i_{2}}(\delta_{\eta t}\cdot x_{i} )^{i_{3}}(\delta^{\prime}_{\eta t}\cdot x_{i})^{i_{4}}(\bar{u}_{\eta t}\cdot x )^{i_{5}}(\bar{u}^{\prime}_{\eta t}\cdot x_{i})^{i_{6}}\] (F.48)

\[-\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\left[(\tilde{\delta}_{t} \cdot x)^{1+i_{1}}(\tilde{\delta}^{\prime}_{t}\cdot x)^{1+i_{2}}(\delta_{\eta t }\cdot x)^{i_{3}}(\delta^{\prime}_{\eta t}\cdot x)^{i_{4}}(\bar{u}_{\eta t} \cdot x)^{i_{5}}(\bar{u}^{\prime}_{\eta t}\cdot x)^{i_{6}}\right]\Big{|}\] (F.49)

\[\lesssim(\log d)^{O(1)}\frac{\|\tilde{\delta}_{t}\|_{2}^{1+i_{1}}\|\tilde{ \delta}^{\prime}_{t}\|_{2}^{1+i_{2}}\|\delta_{\eta t}\|_{2}^{i_{3}}\|\delta^{ \prime}_{\eta t}\|_{2}^{i_{4}}}{d^{1+(i_{1}+i_{2}+i_{3}+i_{4}+i_{6}+i_{6})/2}} \Big{(}\sqrt{\frac{d}{n}}+\frac{d^{1+(i_{1}+i_{2}+i_{3}+i_{4})/2}}{n}+\frac{1 }{d^{2(\log d)}}\Big{)}\,.\] (F.50)

As discussed in the previous case, after we consider the coefficients of \(\sigma(t)\) and \(\sigma^{\prime}(t)\), we find that the term in Eq. (F.47) contributes

\[(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)}\|\tilde{\delta} _{t}\|_{2}^{1+i_{1}}\|\tilde{\delta}^{\prime}_{t}\|_{2}^{1+i_{2}}\|\delta_{ \eta t}\|_{2}^{i_{3}}\|\delta^{\prime}_{\eta t}\|_{2}^{i_{4}}\Big{(}\sqrt{ \frac{d}{n}}+\frac{d^{1+(i_{1}+i_{2}+i_{3}+i_{4})/2}}{n}+\frac{1}{d^{\Omega( \log d)}}\Big{)}\,.\] (F.51)

Now, suppose \(i_{5}+i_{6}\geq 2\). Then, \(i_{1}+i_{2}+i_{3}+i_{4}\leq 4\), meaning the contribution to the concentration error is at most

\[(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\|\tilde{\delta}_{t}\|_{2}\| \tilde{\delta}^{\prime}_{t}\|_{2}\leq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d }^{2})\max_{\chi}\|\tilde{\delta}_{t}(\chi)\|_{2}^{2}\] (F.52)

using the fact that \(n\gtrsim d^{3}(\log d)^{C}\) for a sufficiently large constant \(C\), and \(\|\tilde{\delta}_{t}\|_{2}\lesssim 1\) and \(\|\delta_{\eta t}\|_{2}\lesssim 1\). On the other hand, if \(i_{5}+i_{6}\leq 1\), then \(i_{1}+i_{2}+i_{3}+i_{4}\geq 5\), and since \(i_{3}\leq 3\), this implies that \(i_{1}+i_{2}+i_{4}\geq 2\). In this case, the bound is at most

\[(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})(\log d)^{O(1)}\| \tilde{\delta}_{t}\|_{2}^{1+i_{1}}\|\tilde{\delta}^{\prime}_{t}\|_{2}^{1+i_{2}} \|\delta_{\eta t}\|_{2}^{i_{3}}\|\delta^{\prime}_{\eta t}\|_{2}^{i_{4}}\Big{(} \sqrt{\frac{d}{n}}+\frac{d^{1+(i_{1}+i_{2}+i_{3}+i_{4})/2}}{n}+\frac{1}{d^{ \Omega(\log d)}}\Big{)}\] (F.53) \[\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\|\tilde{ \delta}_{t}\|_{2}^{1+i_{1}}\|\tilde{\delta}^{\prime}_{t}\|_{2}^{1+i_{2}}\| \delta_{\eta t}\|_{2}^{i_{2}}\|\delta^{\prime}_{\eta t}\|_{2}^{i_{4}}d\] \[\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\max_{\chi }\|\tilde{\delta}_{t}(\chi)\|_{2}^{2}\cdot\frac{1}{d^{(i_{1}+i_{2})/2}}\|\delta^{ \prime}_{\eta t}\|_{2}^{i_{4}}d\,.\] (B.c. \[\max_{\chi}\|\tilde{\delta}_{t}(\chi)\|_{2}\leq\frac{1}{\sqrt{d}}\] and \[\|\delta_{\eta t}\|_{2}\lesssim 1\] )

Thus, if \(i_{1}+i_{2}\geq 2\), then the contribution of this term to the overall concentration error is at most \((\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\max_{\chi}\|\tilde{\delta}_{t}( \chi)\|_{2}^{2}\). On the other hand, if \(i_{4}=1\), then we can take the average of \(\|\delta^{\prime}_{\eta t}\|_{2}\) across \(\chi\), and by Lemma E.3, we have that Assumption 5.3 holds for \(\eta t\leq T_{*,\epsilon}\), meaning that \(\operatorname*{\mathbb{E}}_{\chi}\|\delta^{\prime}_{\eta t}\|_{2}\leq \overline{\Delta}_{\eta t}\leq\frac{1}{\sqrt{d}}\). In this case, the contribution of this term to the overall concentration error is at most \((\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\max_{\chi}\|\tilde{\delta}_{t}( \chi)\|_{2}^{2}\cdot\frac{1}{d^{(i_{1}+i_{2}+i_{4})/2}}d=(\hat{\sigma}_{2,d}^{2}+ \hat{\sigma}_{4,d}^{2})\max_{\chi}\|\tilde{\delta}_{t}(\chi)\|_{2}^{2}\). Finally, when \(i_{4}\geq 2\), we can take the average of \(\|\delta^{\prime}_{\eta t}\|_{2}^{2}\) over \(\chi\) -- we have \(\operatorname*{\mathbb{E}}_{\chi}\|\delta^{\prime}_{\eta t}\|_{2}^{2}\leq\frac{1}{d}\), and therefore the contribution of this term to the overall concentration error is at most

\[(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\max_{\chi}\|\tilde{\delta}_{t}( \chi)\|_{2}^{2}\cdot\frac{1}{d^{1+(i_{1}+i_{2})/2}}d\leq(\hat{\sigma}_{2,d}^{2}+ \hat{\sigma}_{4,d}^{2})\max_{\chi}\|\tilde{\delta}_{t}(\chi)\|_{2}^{2}\,.\] (F.54)

In all cases, the contribution of Eq. (F.37) to the overall concentration error between \(S_{1}\) and \(M_{1}\) is at most

\[(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\max_{\chi}\|\tilde{\delta}_{t}( \chi)\|_{2}^{2}\,.\] (F.55)

#### Overall Concentration Error

In summary, the overall concentration error between \(S_{1}\) and \(M_{1}\) is that of \(G(x)\). We have expanded \(G(x)\) into various monomials of dot products involving \(x\), and shown that each monomial contributes at most \((\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\max_{\chi}\|\tilde{\delta}_{t}( \chi)\|_{2}^{2}\) to the concentration error. Thus, with high probability,

\[|M_{1}-S_{1}|\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\max_{\chi} \|\tilde{\delta}_{t}(\chi)\|_{2}^{2}\,.\] (F.56)In order to bound \(|M_{2}-S_{2}|\), an almost identical, but simpler, argument can be used, with \(y(x)\) in the place of \(f_{\hat{\rho}_{t}}(x)\) or \(f_{\hat{\rho}_{\eta t}}(x)\). In the modified argument, \(\hat{u}^{\prime}_{t}\), \(\hat{u}^{\prime}_{\eta t}\) and \(\hat{u}^{\prime}_{\eta t}\) will all be replaced with \(e_{1}\). Thus, in the analysis of Eq. (F.40), we can replace \(\hat{u}^{\prime}_{\eta t}\) with \(e_{1}\) and assume that \(i_{2}=0\) (since otherwise this term becomes \(0\) due to the \(\delta^{\prime}_{\eta t}\cdot x\) factor) -- we can then proceed similarly to the subcase where \(i_{2}\leq 1\) in the analysis of Eq. (F.40). Additionally, the term analogous to Eq. (F.47) does not arise in the case where \(f_{\hat{\rho}_{t}}(x)\) and \(f_{\hat{\rho}_{\eta t}}(x)\) are replaced by \(y(x)\), since \(\hat{u}^{\prime}_{t}\) and \(\hat{u}^{\prime}_{\eta t}\) are both replaced by \(e_{1}\), so the corresponding term is \(0\). We also note that the Legendre coefficients of \(y(x)\) have absolute values which are less than those of \(\sigma\). Thus, we have

\[|M_{1}-S_{1}|+|M_{2}-S_{2}|\leq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2} )\max_{\chi}\|\tilde{\delta}_{t}(\chi)\|_{2}^{2}\,.\] (F.57)

Bounding the Expectation To complete the proof, it suffices to bound \(|M_{1}-M_{2}|\). We have that

\[M_{1}-M_{2}=\nabla_{\hat{u}_{t}}L(\tilde{\rho}_{t})\cdot\tilde{ \delta}_{t}-\nabla_{\hat{u}_{\eta t}}L(\hat{\rho}_{\eta t})\cdot\tilde{\delta }_{t}\] (F.58)

and by Lemma F.1, we have

\[|M_{1}-M_{2}| \lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\Big{(} \operatorname*{\mathbb{E}}_{(\hat{u}^{\prime}_{t},\hat{u}^{\prime}_{\eta t}) \sim(\hat{\rho}_{t},\hat{\rho}_{\eta t})}\Big{[}\|\tilde{u}^{\prime}_{t}-\hat {u}^{\prime}_{\eta t}\|_{2}^{2}\Big{]}^{1/2}+\|\tilde{u}_{t}-\hat{u}_{\eta t} \|_{2}\Big{)}\|\tilde{\delta}_{t}\|_{2}\] (F.59) \[\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\max_{ \chi}\|\tilde{\delta}_{t}(\chi)\|_{2}^{2}\] (F.60)

Combining this with our bound on \(|M_{1}-S_{1}|+|M_{2}-S_{2}|\), we find that

\[|(\text{grad}_{\hat{u}_{t}}\widehat{L}(\tilde{\rho}_{t})-\text{grad}_{\hat{u}_ {\eta t}}\widehat{L}(\hat{\rho}_{\eta t}))\cdot\tilde{\delta}_{t}|\lesssim( \hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})\max_{\chi}\|\tilde{\delta}_{t} (\chi)\|_{2}^{2}\] (F.61)

as desired. 

**Lemma F.3**.: _Suppose we are in the setting of Theorem 3.5. Suppose \(s,t\geq 0\). Then,_

\[\|\text{grad}_{\hat{u}_{t}}\widehat{L}(\hat{\rho}_{t})-\text{grad}_{\hat{u}_{s }}\widehat{L}(\hat{\rho}_{s})\|_{2}\leq(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})d^{4}\cdot\max_{i\in[m]}\|\hat{u}_{t}(\chi_{i})-\hat{u}_{s}(\chi_{i})\| _{2}\] (F.62)

Proof.: We have

\[\|\text{grad}_{\hat{u}_{t}}\widehat{L}(\hat{\rho}_{t}) -\text{grad}_{\hat{u}_{s}}\widehat{L}(\hat{\rho}_{s})\|_{2}\] (F.63) \[=\|(I-\hat{u}_{t}\hat{u}_{t}^{\top})\nabla_{\hat{u}_{t}}\widehat{ L}(\hat{\rho}_{t})-(I-\hat{u}_{s}\hat{u}_{s}^{\top})\nabla_{\hat{u}_{s}}\widehat{L}( \hat{\rho}_{s})\|_{2}\] (F.64) \[=\Big{\|}(I-\hat{u}_{t}\hat{u}_{t}^{\top})\cdot\frac{1}{n}\sum_{ i=1}^{n}(f_{\hat{\rho}_{t}}(x_{i})-y(x_{i}))\sigma^{\prime}(\hat{u}_{t}^{\top}x_{i})x_ {i}\] (F.65) \[\qquad\qquad\qquad-(I-\hat{u}_{s}\hat{u}_{s}^{\top})\cdot\frac{1} {n}\sum_{i=1}^{n}(f_{\hat{\rho}_{s}}(x_{i})-y(x_{i}))\sigma^{\prime}(\hat{u}_{s }^{\top}x_{i})x_{i}\Big{\|}_{2}\] (F.66) \[\leq\frac{1}{n}\sum_{i=1}^{n}\|(f_{\hat{\rho}_{t}}(x_{i})-y(x_{i} ))\sigma^{\prime}(\hat{u}_{t}^{\top}x_{i})(I-\hat{u}_{t}\hat{u}_{t}^{\top})x_ {i}\] (F.67) \[\qquad\qquad\qquad-(f_{\hat{\rho}_{s}}(x_{i})-y(x_{i}))\sigma^{ \prime}(\hat{u}_{s}^{\top}x_{i})(I-\hat{u}_{s}\hat{u}_{s}^{\top})x_{i}\|_{2}\,.\] (F.68)

We bound the right-hand side by several applications of the triangle inequality. First, observe that for any \(x\in\mathbb{S}^{d-1}\), we have

\[|f_{\hat{\rho}_{t}}(x)-f_{\hat{\rho}_{s}}(x)| \leq\frac{1}{m}\sum_{j=1}^{m}|\sigma(\hat{u}_{t}(\chi_{i})\cdot x) -\sigma(\hat{u}_{s}(\chi_{i})\cdot x)|\] (F.69) \[\lesssim\frac{1}{m}\sum_{j=1}^{m}(|\hat{\sigma}_{2,d}|\sqrt{N_{2,d }}+|\hat{\sigma}_{4,d}|\sqrt{N_{4,d}})|\hat{u}_{t}(\chi_{i})\cdot x-\hat{u}_{s}( \chi_{i})\cdot x|\] (By Lemma E.12) \[\lesssim\frac{(|\hat{\sigma}_{2,d}|\sqrt{N_{2,d}}+|\hat{\sigma}_{4,d}|\sqrt{N_{4,d}})}{m}\sum_{j=1}^{m}\|\hat{u}_{t}(\chi_{i})-\hat{u}_{s}(\chi_{i} )\|_{2}\] (F.70) \[\lesssim(|\hat{\sigma}_{2,d}|\sqrt{N_{2,d}}+|\hat{\sigma}_{4,d}| \sqrt{N_{4,d}})\max_{i\in[m]}\|\hat{u}_{t}(\chi_{i})-\hat{u}_{s}(\chi_{i})\|_{2}\,.\] (F.71)

[MISSING_PAGE_FAIL:91]

Proof of Theorem 3.5.: To show that projected gradient descent achieves low population loss, we show that it does not diverge far from projected gradient descent. Suppose \(\tilde{\rho}_{0}\) and \(\tilde{\rho}_{0}\) are both equal to \(\text{unif}(\chi_{1},\ldots,\chi_{m}))\). Then, by induction over \(t\), we bound the difference between \(\tilde{u}_{t}(\chi)\) and \(\hat{u}_{\eta t}(\chi)\) for all \(\chi\in\{\chi_{1},\ldots,\chi_{m}\}\). At \(t=0\), we have \(\tilde{u}_{0}(\chi)=\hat{u}_{0}(\chi)\). Let \(t\) be a nonnegative integer, and assume that for all \(s\leq t\) we have \(\max_{\chi}\|\tilde{\delta}_{t}(\chi)\|_{2}\leq\frac{1}{\sqrt{d}}\).

By the definition of projected gradient descent, we have

\[\tilde{u}_{t+1}(\chi)=\frac{\tilde{u}_{t}(\chi)-\eta\cdot\text{ grad}_{\tilde{u}_{t}(\chi)}\widehat{L}(\tilde{\rho}_{t})}{\|\tilde{u}_{t}(\chi)- \eta\cdot\text{grad}_{\tilde{u}_{t}(\chi)}\widehat{L}(\tilde{\rho}_{t})\|_{2}}\,.\] (F.90)

Note that \(\|\tilde{u}_{t}-\eta\cdot\text{grad}_{\tilde{u}_{t}}\widehat{L}(\tilde{\rho} _{t})\|_{2}\geq\|\tilde{u}_{t}\|_{2}\), since \(\text{grad}_{\tilde{u}_{t}}\widehat{L}(\tilde{\rho}_{t})\) is orthogonal to \(\tilde{u}_{t}\). Thus, letting \(\Pi\) denote the projection onto the \(\ell_{2}\) unit ball, we have

\[\|\tilde{u}_{t+1}-\hat{u}_{\eta(t+1)}\|_{2} =\|\Pi(\tilde{u}_{t}-\eta\cdot\text{grad}_{\tilde{u}_{t}}\widehat {L}(\tilde{\rho}_{t}))-\Pi(\hat{u}_{\eta(t+1)})\|_{2}\] (F.91) \[\leq\|\tilde{u}_{t}-\eta\cdot\text{grad}_{\tilde{u}_{t}}\widehat {L}(\tilde{\rho}_{t})-\hat{u}_{\eta(t+1)}\|_{2}\,.\] (F.92)

Also, we have

\[\hat{u}_{\eta(t+1)}(\chi)=\hat{u}_{\eta t}(\chi)-\int_{\eta t}^{\eta(t+1)} \text{grad}_{\tilde{u}_{s}(\chi)}\widehat{L}(\hat{\rho}_{s})ds\,.\] (F.93)

Thus,

\[\|\tilde{u}_{t}-\eta\cdot\text{grad}_{\tilde{u}_{t}}\widehat{L} (\tilde{\rho}_{t})-\hat{u}_{\eta(t+1)}\|_{2}^{2} =\left\|\tilde{u}_{t}-\eta\cdot\text{grad}_{\tilde{u}_{t}} \widehat{L}(\tilde{\rho}_{t})-\hat{u}_{\eta t}+\int_{\eta t}^{\eta(t+1)}\text {grad}_{\tilde{u}_{s}}\widehat{L}(\hat{\rho}_{s})ds\right\|_{2}^{2}\] (F.94) \[=\|\tilde{u}_{t}-\hat{u}_{\eta t}\|_{2}^{2}+\left\|\eta\cdot \text{grad}_{\tilde{u}_{t}}\widehat{L}(\tilde{\rho}_{t})-\int_{\eta t}^{\eta(t +1)}\text{grad}_{\tilde{u}_{s}}\widehat{L}(\hat{\rho}_{s})ds\right\|_{2}^{2}\] (F.95) \[\qquad+2(\tilde{u}_{t}-\hat{u}_{\eta t})\cdot\int_{\eta t}^{\eta(t +1)}(\text{grad}_{\tilde{u}_{t}}\widehat{L}(\tilde{\rho}_{t})-\text{grad}_{ \tilde{u}_{s}}\widehat{L}(\hat{\rho}_{s}))ds\] (F.96) \[=\|\tilde{\delta}_{t}\|_{2}^{2}+O\Big{(}\eta^{2}(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2})^{2}d^{8}\Big{)}\] (F.97) \[\qquad+2\tilde{\delta}_{t}\cdot\int_{\eta t}^{\eta(t+1)}(\text{ grad}_{\tilde{u}_{\eta t}}\widehat{L}(\hat{\rho}_{\eta t})-\text{grad}_{\tilde{u}_{s}} \widehat{L}(\hat{\rho}_{s})ds\] (By Lemma E.14) \[\leq\|\tilde{\delta}_{t}\|_{2}^{2}+O\Big{(}\eta^{2}(\hat{\sigma}_ {2,d}^{2}+\hat{\sigma}_{4,d}^{2})^{2}d^{8}\Big{)}\] (F.98) \[\qquad+O\Big{(}\eta(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{ 2})\max_{\chi}\|\tilde{\delta}_{t}(\chi)\|_{2}^{2}\Big{)}\] (F.99) \[\qquad+2\tilde{\delta}_{t}\cdot\int_{\eta t}^{\eta(t+1)}(\text{ grad}_{\tilde{u}_{\eta t}}\widehat{L}(\hat{\rho}_{\eta t})-\text{grad}_{\tilde{u}_{s}} \widehat{L}(\hat{\rho}_{s})ds\,.\] (By Lemma F.2)

[MISSING_PAGE_EMPTY:93]

This completes the induction, and therefore for all \(t\leq\frac{T_{*,t}}{\eta}\), we have \(\max_{\chi}\|\tilde{\delta}_{t}(\chi)\|_{2}\leq\frac{1}{\sqrt{d}}\). Additionally, by Lemma E.16, and using the coupling between \(\hat{\rho}_{t}\) and \(\hat{\rho}_{\eta t}\) which is the joint distribution of \((\tilde{u}_{t}(\chi),\hat{u}_{\eta t}(\chi))\), we have

\[\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\tilde{\rho}_{t}}(x)-f_{\hat{ \rho}_{\eta t}}(x))^{2}]\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d}^{2} )\max_{\chi}\|\tilde{\delta}_{t}(\chi)\|_{2}^{2}\lesssim\frac{\hat{\sigma}_{2, d}^{2}+\hat{\sigma}_{4,d}^{2}}{d}\lesssim(\hat{\sigma}_{2,d}^{2}+\hat{\sigma}_{4,d} ^{2})\epsilon\] (F.120)

which completes the proof. 

## Appendix G Missing Proofs in Section 3

### Proof of Lemma 3.1

In this section, we prove sufficient and necessary conditions for \(\gamma_{2}\) and \(\gamma_{4}\) when the population loss is 0.

**Proposition G.1** (Necessary condition).: _Let \(f_{\rho}(x)=\mathop{\mathbb{E}}_{u\sim\rho}[\sigma(u^{\top}x)]\) be a (not necessarily symmetric nor rotational invariant) two-layer neural network and \(y(x)=h(e_{1}^{\top}x)\) the target function. Define \(\gamma_{k}=\frac{\tilde{h}_{k,d}}{\hat{\sigma}_{k,d}},\forall k\geq 0\). Suppose \(\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}(f_{\rho}(x)-y(x))^{2}=0\) and \(\hat{\sigma}_{2,d}\neq 0,\hat{\sigma}_{4,d}\neq 0\), we have_

\[\gamma_{2}^{2} \leq\gamma_{4}+O\left(\frac{1}{\sqrt{d}}\right),\] (G.1) \[\gamma_{4} \leq\gamma_{2}+O\left(\frac{1}{\sqrt{d}}\right),\] (G.2) \[\gamma_{2} \leq 1.\] (G.3)

Proof.: Similar to the proof of Lemma 4.2, we have

\[\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-h(x^{ \top}e_{1}))^{2}]=\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(}\mathop{ \mathbb{E}}_{u\sim\rho}[\sigma(u^{\top}x)]-h(x^{\top}e_{1})\Big{)}^{2}\] (G.4) \[=\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(}\mathop{ \mathbb{E}}_{u\sim\rho}\Big{[}\sum_{k=0}^{\infty}\hat{\sigma}_{k,d}\overline{ P_{k,d}}(u^{\top}x)\Big{]}-\sum_{k=0}^{\infty}\hat{h}_{k,d}\overline{P}_{k,d}(x^{ \top}e_{1})\Big{)}^{2}\] (G.5) \[=\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(}\sum_{k=0}^{ \infty}\Big{(}\hat{\sigma}_{k,d}\mathop{\mathbb{E}}_{u\sim\rho}[\overline{P}_ {k,d}(u^{\top}x)]-\hat{h}_{k,d}\overline{P}_{k,d}(x^{\top}e_{1})\Big{)}\Big{)} ^{2}.\] (G.6)

Continuing the equation by invoking Lemma C.5, we get

\[\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}\Big{(}\sum_{k=0}^{ \infty}\Big{(}\hat{\sigma}_{k,d}\mathop{\mathbb{E}}_{u\sim\rho}[\overline{P}_ {k,d}(u^{\top}x)]-\hat{h}_{k,d}\overline{P}_{k,d}(x^{\top}e_{1})\Big{)}\Big{)}^ {2}\] (G.7) \[=\sum_{k=0}^{\infty}\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}} \Big{(}\hat{\sigma}_{k,d}\mathop{\mathbb{E}}_{u\sim\rho}[\overline{P}_{k,d}(u^ {\top}x)]-\hat{h}_{k,d}\overline{P}_{k,d}(x^{\top}e_{1})\Big{)}^{2}\] (G.8) \[=\sum_{k=0}^{\infty}\hat{\sigma}_{k,d}^{2}\Big{(}\mathop{\mathbb{ E}}_{u,u^{\top}\sim\rho}[P_{k,d}(u^{\top}u^{\prime})-2\gamma_{k}P_{k,d}(u^{\top}e_{1} )+\gamma_{k}^{2}P_{k,d}(e_{1}^{\top}e_{1})]\Big{)}^{2}.\] (G.9)

Therefore, \(\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-h(x^{\top}e_{1}))^{2} ]=0\) implies that

\[\mathop{\mathbb{E}}_{u,u^{\prime}\sim\rho}[P_{2,d}(u^{\top}u^{ \prime})-2\gamma_{2}P_{2,d}(u^{\top}e_{1})+\gamma_{2}^{2}P_{2,d}(e_{1}^{\top}e_ {1})]=0,\] (G.10) \[\mathop{\mathbb{E}}_{u,u^{\prime}\sim\rho}[P_{4,d}(u^{\top}u^{ \prime})-2\gamma_{4}P_{4,d}(u^{\top}e_{1})+\gamma_{4}^{2}P_{4,d}(e_{1}^{\top}e_ {1})]=0.\] (G.11)

First we prove Eq. (G.3). By Lemma C.6, there is a feature mapping \(\phi:\mathbb{S}^{d-1}\to\mathbb{R}^{N_{2,d}}\) such that for every \(u,v\in\mathbb{S}^{d-1}\),

\[\langle\phi(u),\phi(v)\rangle=N_{2,d}P_{2,d}(u^{\top}v),\] (G.12) \[\|\phi_{u}\|_{2}^{2}=N_{2,d}.\] (G.13)Rewrite Eq. (G.10) using \(\phi_{2}\) we get

\[\|\mathop{\mathbb{E}}_{u\sim\rho}[\phi_{2}(u)]-\gamma_{2}\phi_{2}(e_{1})\|_{2}^{2 }=0.\] (G.14)

Or equivalently,

\[\gamma_{2}\phi_{2}(e_{1})=\mathop{\mathbb{E}}_{u\sim\rho}[\phi_{2}(u)]\] (G.15)

As a result,

\[\gamma_{2}=\left\|\frac{\phi_{2}(e_{1})}{\sqrt{N_{k,d}}}\right\|_{2}=\left\| \frac{\mathop{\mathbb{E}}_{u\sim\rho}[\phi_{2}(u)]}{\sqrt{N_{k,d}}}\right\|_{2} \leq\frac{\mathop{\mathbb{E}}_{u\sim\rho}[\|\phi_{2}(u)]\|_{2}}{\sqrt{N_{2,d}} }=1.\] (G.16)

Similarly, we also have \(\gamma_{4}\leq 1\).

Now we prove Eq. (G.1). Recall that

\[P_{2,d}(t) =\frac{d}{d-1}t^{2}-\frac{1}{d-1},\] (G.17) \[P_{4,d}(t) =\frac{(d+2)(d+4)}{d^{2}-1}t^{4}-\frac{6d+12}{d^{2}-1}t^{2}+\frac {3}{d^{2}-1}.\] (G.18)

Combining Eq. (G.10) and Eq. (G.17) we have

\[0=\frac{d-1}{d}\mathop{\mathbb{E}}_{u,u^{\prime}\sim\rho}[P_{2,d }(u^{\top}u^{\prime})-2\gamma_{2}P_{2,d}(u^{\top}e_{1})+\gamma_{2}^{2}P_{2,d}( e_{1}^{\top}e_{1})]\] (G.19) \[=\mathop{\mathbb{E}}_{u,u^{\prime}\sim\rho}[(u^{\top}u^{\prime}) ^{2}-2\gamma_{2}(u^{\top}e_{1})^{2}+\gamma_{2}^{2}(e_{1}^{\top}e_{1})^{2}]- \frac{1}{d}(1-2\gamma_{2}+\gamma_{2}^{2})\] (G.20) \[=\|\mathop{\mathbb{E}}_{u\sim\rho}[u^{\otimes 2}-\gamma_{2}e_{1}^{ \otimes 2}]\|_{F}^{2}-\frac{1}{d}(1-\gamma_{2})^{2}.\] (G.21)

Consequently,

\[\|\mathop{\mathbb{E}}_{u\sim\rho}[u^{\otimes 2}-\gamma_{2}e_{1}^{ \otimes 2}]\|_{F}^{2}=\frac{1}{d}(1-\gamma_{2})^{2}.\] (G.22)

Similarly, combining Eq. (G.11) and Eq. (G.18) we get

\[0=\frac{d^{2}-1}{(d+2)(d+4)}\mathop{\mathbb{E}}_{u,u^{\prime} \sim\rho}[P_{4,d}(u^{\top}u^{\prime})-4\gamma_{4}P_{4,d}(u^{\top}e_{1})+\gamma _{4}^{2}P_{4,d}(e_{1}^{\top}e_{1})]\] (G.23) \[=\mathop{\mathbb{E}}_{u,u^{\prime}\sim\rho}[(u^{\top}u^{\prime}) ^{4}-2\gamma_{4}(u^{\top}e_{1})^{4}+\gamma_{4}^{2}(e_{1}^{\top}e_{1})^{4}]+ \frac{3}{(d+2)(d+4)}(1-2\gamma_{4}+\gamma_{4}^{2})\] (G.24) \[-\frac{6d+12}{(d+2)(d+4)}\mathop{\mathbb{E}}_{u,u^{\prime}\sim \rho}[(u^{\top}u^{\prime})^{2}-2\gamma_{4}(u^{\top}e_{1})^{2}+\gamma_{4}^{2}( e_{1}^{\top}e_{1})^{2}].\] (G.25)

Consequently,

\[\|\mathop{\mathbb{E}}_{u\sim\rho}[u^{\otimes 4}-\gamma_{4}e_{1}^{ \otimes 4}]\|_{F}^{2}\leq\frac{3}{(d+2)(d+4)}(1-\gamma_{4})^{2}+\frac{6d+12}{( d+2)(d+4)}(\gamma_{4}+1)^{2}.\] (G.26)

Now for any fixed \(\xi\in\mathbb{S}^{d-1}\), our key observation is that by Jensen's inequality,

\[\mathop{\mathbb{E}}_{u\sim\rho}[(u^{\top}\xi)^{2}]^{2}\leq\mathop{\mathbb{E}} _{u\sim\rho}[(u^{\top}\xi)^{4}].\] (G.27)

On the one hand, by Eq. (G.22) we have

\[\left|\mathop{\mathbb{E}}_{u\sim\rho}[(u^{\top}\xi)^{2}]-\gamma_ {2}(e_{1}^{\top}\xi)^{2}\right|=\left|\mathop{\mathbb{E}}_{u\sim\rho}[\langle u ^{\otimes 2}-\gamma_{2}e_{1}^{\otimes 2},\xi^{\otimes 2}\rangle]\right|\] (G.28) \[\leq\|\mathop{\mathbb{E}}_{u\sim\rho}[u^{\otimes 2}-\gamma_{2}e_{1}^{ \otimes 2}]\|_{F}\|\xi^{\otimes 2}\|_{F}=\sqrt{\frac{(1-\gamma_{2})^{2}}{d}}.\] (G.29)On the other hand, by Eq. (G.26) we have

\[\left|\mathop{\mathbb{E}}_{u\sim\rho}[(u^{\top}\xi)^{4}]-\gamma_{4}( e_{1}^{\top}\xi)^{4}\right|=\left|\mathop{\mathbb{E}}_{u\sim\rho}[\langle u^{ \otimes 4}-\gamma_{4}e_{1}^{\otimes 4},\xi^{\otimes 4}\rangle]\right|\] (G.30) \[\leq\|\mathop{\mathbb{E}}_{u\sim\rho}[u^{\otimes 4}-\gamma_{4}e_{1}^{ \otimes 4}]\|_{F}\|\xi^{\otimes 4}\|_{F}\leq 2\sqrt{\frac{6d+12}{(d+2)(d+4)}}(| \gamma_{4}+1|+|\gamma_{4}-1|).\] (G.31)

Finally, combining Eqs (G.27)-(G.31) and set \(\xi=e_{1}\) we get

\[\left(\gamma_{2}-\sqrt{\frac{(1-\gamma_{2})^{2}}{d}}\right)^{2} \leq\gamma_{4}+2\sqrt{\frac{6d+12}{(d+2)(d+4)}}(|\gamma_{4}+1|+|\gamma_{4}-1|).\] (G.32)

It follows directly that

\[\gamma_{2}^{2}\leq\gamma_{4}+O\left(\frac{1}{\sqrt{d}}\right).\] (G.33)

Finally we prove Eq. (G.2). For any \(\xi,u\in\mathbb{S}^{d-1}\) we have \(|u^{\top}\xi|\leq 1\). As a result,

\[\mathop{\mathbb{E}}_{u\sim\rho}[(u^{\top}\xi)^{4}]\leq\mathop{ \mathbb{E}}_{u\sim\rho}[(u^{\top}\xi)^{2}].\] (G.34)

Combining with Eq. (G.29) and Eq. (G.31) and setting \(\xi=e_{1}\) we get

\[\gamma_{4}\leq O(1/\sqrt{d})+\mathop{\mathbb{E}}_{u\sim\rho}[(u^{\top}\xi)^{4 }]\leq O(1/\sqrt{d})+\mathop{\mathbb{E}}_{u\sim\rho}[(u^{\top}\xi)^{2}]\leq O( 1/\sqrt{d})+\gamma_{2}.\] (G.35)

**Proposition G.2** (Sufficient condition).: _Let \(f_{\rho}(x)=\mathop{\mathbb{E}}_{u\sim\rho}[\sigma(u^{\top}x)]\) be a two-layer neural network and \(y(x)=h(e_{1}^{\top}x)\) the target function. Let \(\gamma_{k}=\frac{\hat{h}_{k,d}}{\hat{g}_{k,d}},\forall k\geq 0\). Suppose \(\hat{h}_{k,d}=0,\forall k\not\in\{0,2,4\}\), \(\hat{h}_{0,d}=\hat{\sigma}_{0,d}\), \(\sigma\) is a degree-4 polynomial, and_

\[\gamma_{2}^{2} \leq\gamma_{4}-O\left(1/d\right),\] (G.36) \[\gamma_{4} \leq\gamma_{2}-O\left(1/d\right),\] (G.37) \[\gamma_{2} \leq 1-O\left(1/d\right).\] (G.38)

_Then there exists a symmetric and rotational invariant neural network \(\rho\) such that \(\mathop{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}(f_{\rho}(x)-y(x))^{2}=0\)._

Proof.: Recall that by Lemma 4.2, for every symmetric and rotational invariant neural network \(\rho\) we have

\[L(\rho)=\frac{\hat{\sigma}_{2,d}^{2}}{2}\Big{(}\mathop{\mathbb{E }}_{u\sim\rho}[P_{2,d}(w)]-\gamma_{2}\Big{)}^{2}+\frac{\hat{\sigma}_{4,d}^{2}} {2}\Big{(}\mathop{\mathbb{E}}_{u\sim\rho}[P_{4,d}(w)]-\gamma_{4}\Big{)}^{2}\] (G.39)

where \(u=(w,z)\), \(w\in[-1,1]\), and \(z\in\sqrt{1-w^{2}}\mathbb{S}^{d-2}\). Therefore, we only need to show that there exists a one-dimensional distribution \(\mu\) such that

\[\mathop{\mathbb{E}}_{w\sim\mu}[P_{2,d}(w)] =\gamma_{2},\] (G.40) \[\mathop{\mathbb{E}}_{w\sim\mu}[P_{4,d}(w)] =\gamma_{4}.\] (G.41)

Recall that

\[P_{2,d}(t) =\frac{d}{d-1}t^{2}-\frac{1}{d-1},\] (G.42) \[P_{4,d}(t) =\frac{(d+2)(d+4)}{d^{2}-1}t^{4}-\frac{6d+12}{d^{2}-1}t^{2}+\frac{ 3}{d^{2}-1}.\] (G.43)Consequently, Eq. (G.40) and Eq. (G.41) is equivalent to

\[\operatorname*{\mathbb{E}}_{w\sim\mu}[w^{2}] =\frac{d-1}{d}\gamma_{2}+\frac{1}{d},\] (G.44) \[\operatorname*{\mathbb{E}}_{w\sim\mu}[w^{4}] =\frac{d^{2}-1}{(d+2)(d+4)}\gamma_{4}+\frac{6d+12}{(d+2)(d+4)} \left(\frac{d-1}{d}\gamma_{2}+\frac{1}{d}\right)-\frac{3}{(d+2)(d+4)}.\] (G.45)

Let \(\beta_{2}=\frac{d-1}{d}\gamma_{2}+\frac{1}{d}\) and \(\beta_{4}=\frac{d^{2}-1}{(d+2)(d+4)}\gamma_{4}+\frac{6d+12}{(d+2)(d+4)}\left( \frac{d-1}{d}\gamma_{2}+\frac{1}{d}\right)-\frac{3}{(d+2)(d+4)}\). Then we have \(|\beta_{2}-\gamma_{2}|\leq O(1/d)\) and \(|\beta_{4}-\gamma_{4}|\leq O(1/d)\). Rewriting Eqs. (G.36)-(G.38) using \(\beta_{2}\) and \(\beta_{4}\), we have

\[\beta_{2}^{2} \leq\beta_{4},\] (G.46) \[\beta_{4} \leq\beta_{2},\] (G.47) \[\beta_{2} \leq 1.\] (G.48)

By Proposition I.14, there exists a distribution \(\mu^{\star}\) such that

\[\operatorname*{\mathbb{E}}_{w\sim\mu^{\star}}[w^{2}] =\beta_{2},\] (G.49) \[\operatorname*{\mathbb{E}}_{w\sim\mu^{\star}}[w^{4}] =\beta_{4}.\] (G.50)

Hence, by setting the marginal distribution of \(w\) equal to \(\mu^{\star}\), we have

\[\operatorname*{\mathbb{E}}_{w\sim\mu^{\star}}[P_{2,d}(w)]=\gamma_{2},\quad \operatorname*{\mathbb{E}}_{w\sim\mu^{\star}}[P_{4,d}(w)]=\gamma_{4},\] (G.51)

which proves the desired result. 

## Appendix H Proofs for Sample Complexity Lower Bounds for Kernel Method

In this section, we present a sample complexity lower bound for kernel methods with any inner product kernel. That is, the kernel \(K:\mathbb{S}^{d-1}\times\mathbb{S}^{d-1}\to\mathbb{R}\) can be written as \(K(x,z)=\kappa(x^{\top}z)\) for some one-dimensional function \(\kappa:[-1,1]\to\mathbb{R}\). Inner product kernels are rotational invariant and do not specialize to any coordinate. In particular, NTK of two-layer neural networks is an inner product kernel (e.g., Du et al. [30], Wei et al. [76]).

To prove Theorem 3.6, we only need to work on the degree-\(k\) spherical harmonics component of the target function and its estimation because, by the orthogonality of spherical harmonics, any lower bound for the degree-\(k\) components directly translates to the original setting. The following lemma states the lower bound for general \(k\geq 0\), while Theorem 3.6 only requires the case \(k=4\).

**Lemma H.1**.: _Fixed any \(k\geq 1\), let \(g^{\star}(x)=\overline{P}_{k,d}(u^{\top}x)\) for some fixed \(u\in\mathbb{S}^{d-1}\). When \(d\) is larger than a sufficiently large universal constant and \(n<d^{k}(8k)^{-(3k+1)}(\ln d)^{-(k+2)}\), with probability at least \(1/2\) over the randomness of \(n\) i.i.d. data \(\{x_{1},\cdots,x_{n}\}\) drawn uniformly from \(\mathbb{S}^{d-1}\), any estimation \(g\) of the form \(g(x)=\sum_{i=1}^{n}\beta_{i}\overline{P}_{k,d}(x^{\top}x_{i})\) must have a constant error:_

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}(g(x)-g(x)^{\star})^{2}\geq 3 /4.\] (H.1)

Proof of Lemma H.1 is deferred to Appendix H.2, where our key observation is that \(\overline{P}_{k,d}(\langle x_{i},\cdot\rangle)\) cannot correlate too much with \(g^{\star}\) when \(x_{i}\) is uniformly randomly drawn from the unit sphere. Indeed, \(\langle\overline{P}_{k,d}(\langle x_{i},\cdot\rangle),g^{\star}\rangle=P_{k,d} (x_{i}^{\top}u)\) concentrates within \(\pm 1/N_{k,d}\) (Lemma C.7). Hence, we can upperbound \(\langle g,g^{\star}\rangle=\sum_{i=1}^{n}\beta_{i}P_{k,d}(x_{i}^{\top}u)\lesssim \|\beta\|_{2}\sqrt{n/N_{k,d}}\). However, we also have \(\|g\|_{2}^{2}=\sum_{i,j\in[n]}\beta_{i}\beta_{j}P_{k,d}(x_{i}^{\top}x_{j}) \gtrsim\|\beta\|_{2}^{2}\) because the matrix \((P_{k,d}(x_{i}^{\top}x_{j}))_{i,j\in[n]}\) concentrates around \(I\) (Lemma H.3). Since \(\|g-g^{\star}\|_{2}^{2}<3/4\) requires \(\langle g,g^{\star}\rangle>\|g\|_{2}/2\), we must have \(n\gtrsim N_{k,d}\approx d^{-k}\).

### Proof of Theorem 3.6

The following theorem states that for every \(k\geq 0\), to achieve a constant population error, we need at least \(C_{k}d^{-k}(\ln d)^{-(k+2)}\) samples where \(C_{k}=(8k)^{-(3k+1)}\) is a constant that only depends on \(k\). Hence, Theorem 3.6 is a direct corollary of Theorem H.2 by taking \(k=4\).

**Theorem H.2**.: _Let \(y:\mathbb{S}^{d-1}\to\mathbb{R}\) be a function of the form \(y(x)=h(q_{\star}^{\top}x)\) where \(q_{\star}\in\mathbb{S}^{d-1}\) is a fixed vector and \(h:[-1,1]\to\mathbb{R}\) is a one-dimensional function. Let \(K:\mathbb{S}^{d-1}\times\mathbb{S}^{d-1}\to\mathbb{R}\) be any inner product kernel. When \(d\) is larger than a universal constant and \(n<d^{k}(8k)^{-(3k+1)}(\ln d)^{-(k+2)}\), with probability at least \(1/2\) over the randomness of \(n\) i.i.d. data points \(\{x_{i}\}_{i=1}^{n}\), any estimation \(f\) of the form \(f(x)=\sum_{i=1}^{n}\beta_{i}K(x_{i},x)\) must have a significant error:_

\[\|y-f\|_{2}^{2}\geq\frac{3}{4}(\hat{h}_{k,d})^{2}.\]

Proof of Theorem H.2.: In the following we first fix a degree \(k\geq 1\). For every \(l>0\), let \(\mathbb{Y}_{l,d}\) be the space of degree-\(l\) spherical harmonics and \(\Pi_{l}\) the projection to \(\mathbb{Y}_{l,d}\). By the orthogonality of \(\mathbb{Y}_{l,d}\), we have

\[\|y-f\|_{2}^{2}=\sum_{l=0}^{\infty}\|\Pi_{l}(y-f)\|_{2}^{2}\geq\| \Pi_{k}(y-f)\|_{2}^{2}.\] (H.2)

By Atkinson and Han [12, Section 2.3], the projection operator \(\Pi_{k}\) is given by

\[(\Pi_{k}f)(x)=\sqrt{N_{k,d}}\operatorname*{\mathbb{E}}_{\xi\sim \mathbb{S}^{d-1}}[\overline{P}_{k,d}(x^{\top}\xi)f(\xi)].\] (H.3)

Consequently, we have \((\Pi_{k}y)(x)=\sqrt{N_{k,d}}\operatorname*{\mathbb{E}}_{\xi\sim\mathbb{S}^{d-1 }}[\overline{P}_{k,d}(x^{\top}\xi)h(q_{\star}^{\top}\xi)]\). Recall that

\[\mu_{d}(t)\triangleq(1-t^{2})^{\frac{d-3}{2}}\frac{\Gamma(d/2)}{ \Gamma((d-1)/2)}\frac{1}{\sqrt{\pi}}\] (H.4)

is the density of \(u_{1}\) when \(u=(u_{1},\cdots,u_{d})\) is drawn uniformly from \(\mathbb{S}^{d-1}\), and \(\hat{h}_{k,d}=\operatorname*{\mathbb{E}}_{t\sim\mu_{d}}[\overline{P}_{k,d}(t) h(t)]\). By Funk-Hecke formula (Theorem C.1) we get

\[(\Pi_{k}y)(x)=\hat{h}_{k,d}\overline{P}_{k,d}(x^{\top}q_{\star} ),\quad\forall x\in\mathbb{S}^{d-1}.\] (H.5)

Similarly, we can write the inner product kernel \(K\) as \(K(x,z)=\kappa(x^{\top}z)\). Define \(\hat{\kappa}_{k,d}=\operatorname*{\mathbb{E}}_{t\sim\mu_{d}}[\overline{P}_{k,d} (t)\kappa(t)]\). Then we have

\[(\Pi_{k}f)(x)=\hat{\kappa}_{k,d}\sum_{i=1}^{n}\beta_{i}\overline {P}_{k,d}(x^{\top}x_{i}),\quad\forall x\in\mathbb{S}^{d-1}.\] (H.6)

Now we apply Lemma H.1 to the function \((\hat{h}_{k,d})^{-1}(\Pi_{k}y)=\overline{P}_{k,d}(\langle q_{\star},\cdot\rangle)\). As a result, with probability at least \(1/2\) over the randomness of \(\{x_{i}\}_{i=1}^{n}\), for any function \(f\) of the form \(f(\cdot)=\sum_{i=1}^{n}\beta_{i}K(x_{i},\cdot)\), we have \(\|(\hat{h}_{k,d})^{-1}\Pi_{4}(y-f)\|_{2}^{2}\geq 3/4\), which implies \(\|y-f\|_{2}^{2}\geq\frac{3}{4}(\hat{h}_{k,d})^{2}\). 

### Proof of Lemma H.1

Proof of Lemma H.1.: First we prove that when \(\|g^{\star}\|_{2}=1\), \(\|g-g^{\star}\|_{2}^{2}<3/4\) implies \(\langle g,g^{\star}\rangle\geq\|g\|_{2}/2\). To this end, by basic algebra we get

\[3/4>\|g-g^{\star}\|_{2}^{2}=\|g\|_{2}^{2}+\|g^{\star}\|_{2}^{2}-2 \,\langle g,g^{\star}\rangle\,.\] (H.7)

Consequently,

\[\langle g,g^{\star}\rangle\geq\frac{1}{2}\left(\|g\|_{2}^{2}+ \frac{1}{4}\right)\geq\frac{1}{2}\|g\|_{2},\] (H.8)

where the last inequality follows from AM-GM. In the following, we prove that with probability at least \(1/2\), Eq. (H.8) is impossible when \(n<d^{k}(8k)^{-(3k+1)}(\ln d)^{-(k+2)}\).

Recall that Lemma C.5 states that for every \(x,z\in\mathbb{S}^{d-1}\) and \(k\geq 0\),

\[\langle\overline{P}_{k,d}(\langle x,\cdot\rangle),\overline{P}_{k,d}(\langle z,\cdot\rangle)\rangle=P_{k,d}(\langle x,z\rangle).\] (H.9)

As a result,

\[\|g\|_{2}^{2}=\sum_{i,j=1}^{n}\beta_{i}\beta_{j}\,\langle\overline {P}_{k,d}(\langle x_{i},\cdot\rangle),\overline{P}_{k,d}(\langle x_{j},\cdot \rangle)\rangle=\sum_{i,j=1}^{n}\beta_{i}\beta_{j}P_{k,d}(\langle x_{i},x_{j} \rangle).\] (H.10)Invoking Lemma H.3, with probability at least \(3/4\) over the randomness of \(\{x_{i}\}_{i\in[n]}\), we have

\[\|g\|_{2}^{2}=\sum_{i,j=1}^{n}\beta_{i}\beta_{j}P_{k,d}(\langle x_{i},x_{j} \rangle)\geq\frac{1}{2}\|\beta\|_{2}^{2}.\] (H.11)

For the LHS of Eq. (H.8),

\[\langle g,g^{\star}\rangle =\sum_{i=1}^{n}\beta_{i}\left\langle\overline{P}_{k,d}(\langle x_ {i},\cdot\rangle),\overline{P}_{k,d}(\langle u,\cdot\rangle)\right\rangle=\sum _{i=1}^{n}\beta_{i}P_{k,d}(\langle x_{i},u\rangle)\] (H.12) \[\leq\left(\sum_{i=1}^{n}\beta_{i}^{2}\right)^{1/2}\left(\sum_{i=1 }^{n}P_{k,d}(\langle x_{i},u\rangle)^{2}\right)^{1/2}=\|\beta\|_{2}\left(\sum _{i=1}^{n}P_{k,d}(\langle x_{i},u\rangle)^{2}\right)^{1/2}.\] (H.13)

By Lemma C.7, when \(n<d^{k}(8k)^{-(3k+1)}(\ln d)^{-(k+2)}\), with probability at least \(3/4\) we have

\[\langle g,g^{\star}\rangle\leq\|\beta\|_{2}\left(\sum_{i=1}^{n}P_ {k,d}(\langle x_{i},u\rangle)^{2}\right)^{1/2}\leq\|\beta\|_{2}\left(n\max_{i \in[n]}P_{k,d}(\langle x_{i},u\rangle)^{2}\right)^{1/2}\] (H.14) \[\leq\|\beta\|_{2}\sqrt{\frac{n(32k\ln(dn))^{k}}{d^{k}}}\leq\| \beta\|_{2}\sqrt{\frac{1}{\ln d}}.\] (H.15)

Combining Eq. (H.11) and Eq. (H.15), there exists a universal constant \(d_{0}>0\) such that when \(d>d_{0}\), with probability at least \(1/2\) over the randomness of \(\{x_{1},\cdots,x_{n}\}\),

\[\langle g,g^{\star}\rangle<\|\beta\|_{2}/4\leq\|g\|_{2}/2.\] (H.16)

Recall that \(\|g-g^{\star}\|_{2}^{2}<3/4\) implies \(\langle g,g^{\star}\rangle\geq\|g\|_{2}/2\). Consequently, with probability at least \(1/2\), when \(n<d^{k}(8k)^{-(3k+1)}(\ln d)^{-(k+2)}\) and \(d>d_{0}\) we have \(\|g-g^{\star}\|_{2}^{2}\geq 3/4\). 

**Lemma H.3**.: _Let \(x_{1},\cdots,x_{n}\) be i.i.d. random variables drawn uniformly from \(\mathbb{S}^{d-1}\), and \(P_{k,d}\) the degree-\(k\) Legendre polynomial. For any fixed \(k\geq 1\), define the matrix \(M_{k}\in\mathbb{R}^{n\times n}\) where \([M_{k}]_{i,j}=P_{k,d}(\langle x_{i},x_{j}\rangle)\). There exists universal constants \(d_{0}\) such that, when \(n<d^{k}(8k)^{-(3k+1)}(\ln d)^{-(k+2)}\) and \(d>d_{0}\), with probability at least \(3/4\),_

\[\lambda_{\min}(M_{k})\geq 1/2.\] (H.17)

Proof.: In the following, we fix \(k\geq 1\). First, we prove that

\[\mathbb{E}[\lambda_{\min}(M_{k})]\geq 7/8.\] (H.18)

To this end, we invoke Theorem I.4 by constructing a matrix \(A\) such that \(\frac{1}{N_{k,d}}A^{\top}A=M_{k}\).

By Lemma C.6, there exists a feature mapping \(\phi:\mathbb{S}^{d-1}\to\mathbb{R}^{N_{k,d}}\) such that for every \(u,v\in\mathbb{S}^{d-1}\)

\[\langle\phi(u),\phi(v)\rangle=N_{k,d}P_{k,d}(u^{\top}v),\] (H.19) \[\|\phi(u)\|_{2}^{2}=N_{k,d},\] (H.20) \[\mathbb{E}[A_{i}A_{i}^{\top}]=I.\] (H.21)

We set \(A_{i}=\phi(x_{i}),\forall 1\leq i\leq n\). Consequently,

\[\frac{1}{N_{k,d}}A^{\top}A=M_{k},\] (H.22) \[\|A_{i}\|_{2}^{2}=N_{k,d},\] (H.23) \[\mathbb{E}[A_{i}A_{i}^{\top}]=I.\] (H.24)

In the following, we upper bound the incoherence parameter \(p\) defined in Theorem I.4:

\[p=\frac{1}{N_{k,d}}\,\mathbb{E}\left[\max_{i\leq n}\sum_{j\in[n],j\neq i} \langle A_{i},A_{j}\rangle^{2}\right].\] (H.25)By Eq. (H.19) we get

\[\frac{1}{N_{k,d}}\operatorname{\mathbb{E}}\left[\max_{i\leq n}\sum_{ j\in[n],j\neq i}{\langle A_{i},A_{j}\rangle}^{2}\right]=N_{k,d}\operatorname{ \mathbb{E}}\left[\max_{i\leq n}\sum_{j\in[n],j\neq i}P_{k,d}(\langle x_{i},x_{j }\rangle)^{2}\right]\] (H.26) \[\leq nN_{k,d}\operatorname{\mathbb{E}}\left[\max_{i\leq n}\max_{ j\neq i}P_{k,d}(\langle x_{i},x_{j}\rangle)^{2}\right].\] (H.27)

Invoking Lemma C.7 we get

\[N_{k,d}\operatorname{\mathbb{E}}\left[\max_{i\leq n}\max_{j\neq i}P_{k,d}( \langle x_{i},x_{j}\rangle)^{2}\right]\lesssim(32k\ln(dn))^{k}.\] (H.28)

It follows that \(p\lesssim n(32k\ln(dn))^{k}\).

Now that we have established the conditions, by Theorem I.4 we have

\[\operatorname{\mathbb{E}}[\|M-I\|]\lesssim\sqrt{\frac{n(\ln n)(32k\ln(dn))^{k} }{N_{k,d}}}.\] (H.29)

Note that \(N_{k,d}\geq\binom{k+d-2}{k}\geq(d/k)^{k}\) when \(k\geq 2\), and clearly \(N_{1,d}\geq d\). Hence, there exists a universal constant \(d_{0}\) such that, when \(n<d^{k}(8k)^{-(3k+1)}(\ln d)^{-(k+2)}\).

\[\frac{n(\ln n)(32k\ln(dn))^{k}}{N_{k,d}}\leq\frac{n(8k)^{2k}\ln(dn)^{k+1}}{d^{ k}}\leq\frac{n(8k)^{2k}\ln(d^{k+1})^{k+1}}{d^{k}}\leq\frac{1}{\ln d}.\] (H.30)

Hence, there exists a universal constant \(d_{0}\) such that when \(d\geq d_{0}\),

\[\operatorname{\mathbb{E}}[\lambda_{\min}(M)]\geq 1-O(1)\sqrt{\frac{n(\ln n)(32k \ln(dn))^{k}}{N_{k,d}}}\geq 1-O(1)\sqrt{\frac{1}{\ln d}}\geq 7/8.\] (H.31)

Now we prove Eq. (H.17). Let \(t=1-\lambda_{\min}(M)\) be a random variable. Note that by Eq. (H.18) we have \(\operatorname{\mathbb{E}}[t]\leq 1/8\). By basic algebra, we also have

\[\lambda_{\min}(M)\leq M_{1,1}=P_{k,d}(\langle x_{1},x_{1}\rangle)=P_{k,d}(1)=1,\] (H.32)

which implies \(t\geq 0\). Therefore, by Markov inequality we get

\[\Pr(\lambda_{\min}(M)\leq 1/2)=\Pr(t\geq 1/2)\leq 2\operatorname{\mathbb{E}}[t] \leq 1/4.\] (H.33)

## Appendix I Toolbox

### Rademacher Complexity and Generalization Bounds

**Lemma I.1** (Special Case of Contraction Principle (Lemma 4.6 of Adamczak et al. [4])).: _Let \(\mathcal{F}\) be a family of functions from \(D\) to \(\mathbb{R}\), and let \(\varphi\) be an \(L\)-Lipschitz function with \(\varphi(0)=0\). Then, for any \(S=\{x_{1},\dots,x_{N}\}\subset D\),_

\[\operatorname{\mathbb{E}}\sup_{\epsilon_{i}}\frac{1}{N}\sum_{i=1}^{N} \epsilon_{i}\varphi(f(x_{i}))\leq L\cdot\operatorname{\mathbb{E}}\sup_{ \epsilon_{i}}\frac{1}{N}\sum_{i=1}^{N}\epsilon_{i}f(x_{i})\] (I.1)

_where the expectations are taken over i.i.d. Rademacher random variables \(\epsilon_{1},\dots,\epsilon_{N}\)._

In the following, for a function family \(\mathcal{H}\), we let \(R_{N}(\mathcal{H})\) denote its Rademacher complexity and \(R_{S}(\mathcal{H})\) denote its empirical Rademacher complexity (given a fixed dataset \(S\)) -- see Section 4.4.2 of Ma [53] for a definition of these concepts.

**Lemma I.2** (Empirical Rademacher Complexity of Products).: _Suppose \(\mathcal{F}\) and \(\mathcal{G}\) are two families of functions from \(D\) to \(\mathbb{R}\), with \(D\subset\mathbb{R}^{d}\), which are uniformly bounded by \(B>0\). Then, for any set \(\hat{S}=\{x_{1},\dots,x_{N}\}\subset D\),_

\[R_{S}(\mathcal{F}\cdot\mathcal{G})\leq B\cdot\left(R_{S}(\mathcal{F})+R_{S}( \mathcal{G})\right),\] (I.2)

_where \(\mathcal{F}\cdot\mathcal{G}=\{fg\mid f\in\mathcal{F},g\in\mathcal{G}\}\). As a corollary,_

\[R_{N}(\mathcal{F}\cdot\mathcal{G})\leq B\cdot\left(R_{N}(\mathcal{F})+R_{N}( \mathcal{G})\right).\] (I.3)Proof of Lemma I.2.: We can write

\[R_{S}(\mathcal{F}\cdot\mathcal{G}) =\operatorname*{\mathbb{E}}_{\epsilon_{i}}\sup_{f\in\mathcal{F},g\in \mathcal{G}}\frac{1}{N}\sum_{i=1}^{N}\epsilon_{i}f(x_{i})g(x_{i})\] (I.4) \[=\operatorname*{\mathbb{E}}_{\epsilon_{i}}\sup_{f\in\mathcal{F},g \in\mathcal{G}}\frac{1}{N}\sum_{i=1}^{N}\epsilon_{i}\cdot\frac{(f(x_{i})+g(x_{ i}))^{2}-(f(x_{i})-g(x_{i}))^{2}}{4}\] (I.5) \[\leq\operatorname*{\mathbb{E}}_{\epsilon_{i}}\sup_{f\in\mathcal{F },g\in\mathcal{G}}\frac{1}{N}\sum_{i=1}^{N}\epsilon_{i}\cdot\frac{(f(x_{i})+g( x_{i}))^{2}}{4}+\operatorname*{\mathbb{E}}_{\epsilon_{i}}\sup_{f\in\mathcal{F},g\in \mathcal{G}}\frac{1}{N}\sum_{i=1}^{N}\epsilon_{i}\frac{(f(x_{i})-g(x_{i}))^{2} }{4}\,.\] (I.6)

Using Lemma I.1 and the fact that the function \(x\mapsto x^{2}\) is \(2B\) Lipschitz on the interval \([-B,B]\), we have

\[R_{S}(\mathcal{F}\cdot\mathcal{G}) \leq 2B\cdot\operatorname*{\mathbb{E}}_{\epsilon_{i}}\sup_{f\in \mathcal{F},g\in\mathcal{G}}\frac{1}{N}\sum_{i=1}^{N}\epsilon_{i}\cdot\frac{f( x)+g(x)}{4}+2B\cdot\operatorname*{\mathbb{E}}_{\epsilon_{i}}\sup_{f\in \mathcal{F},g\in\mathcal{G}}\frac{1}{N}\sum_{i=1}^{N}\epsilon_{i}\frac{f(x_{i} )-g(x_{i})}{4}\] (I.7) \[\leq\frac{B}{2}\cdot(R_{S}(\mathcal{F})+R_{S}(\mathcal{G}))+\frac {B}{2}\cdot(R_{S}(\mathcal{F})+R_{S}(\mathcal{G}))\] (I.8) \[=B\cdot(R_{S}(\mathcal{F})+R_{S}(\mathcal{G}))\,,\] (I.9)

as desired. 

**Lemma I.3** (Symmetrization -- Analogous to Corollary 4.7 of Adamczak et al. [4]).: _Let \(X_{1},\ldots,X_{N}\) be independent random variables. Let \(\mathcal{F}\) be a family of functions from \(D\subset\mathbb{R}^{d}\) to \(\mathbb{R}\) that is uniformly bounded by \(B_{1}\geq 1\), and let \(\mathcal{G}\) be a family of functions from \(D\subset\mathbb{R}^{d}\) to \(\mathbb{R}\). Then, for nonnegative integers \(p_{1},p_{2},p_{3},p_{4}\) and a fixed \(g\in\mathcal{G}\) that is bounded by \(B_{2}\geq 1\), we have_

\[\operatorname*{\mathbb{E}}_{X_{i}}\sup_{f_{1},f_{2},f_{3},f_{4} \in\mathcal{F},g\in\mathcal{G}}\Big{|}\frac{1}{N}\sum_{i=1}^{N}\Big{(}|f_{1}(X _{i})|^{p_{1}}|f_{2}(X_{i})|^{p_{2}}|f_{3}(X_{i})|^{p_{3}}|f_{4}(X_{i})|^{p_{4 }}|g(X_{i})|\] (I.10) \[\qquad\qquad\qquad\qquad\qquad-\operatorname*{\mathbb{E}}_{X_{i} }\Big{[}|f_{1}(X_{i})|^{p_{1}}|f_{2}(X_{i})|^{p_{2}}|f_{3}(X_{i})|^{p_{3}}|f_{ 4}(X_{i})|^{p_{4}}|g(X_{i})|\Big{]}\Big{)}\Big{|}\] (I.11) \[\leq\text{poly}(p_{1},p_{2},p_{3},p_{4},B_{1}^{p_{1}+p_{2}+p_{3} +p_{4}},B_{2})\cdot R_{N}(\mathcal{F})\] (I.12)

Proof of Lemma I.3.: By Theorem 4.13 of Ma [53],

\[\operatorname*{\mathbb{E}}_{X_{i}}\sup_{f_{1},f_{2},f_{3},f_{4} \in\mathcal{F},g\in\mathcal{G}}\Big{|}\frac{1}{N}\sum_{i=1}^{N}\Big{(}|f_{1}(X _{i})|^{p_{1}}|f_{2}(X_{i})|^{p_{2}}|f_{3}(X_{i})|^{p_{3}}|f_{4}(X_{i})|^{p_{4 }}|g(X_{i})|\] (I.13) \[\qquad\qquad\qquad\qquad-\operatorname*{\mathbb{E}}_{X_{i}}\Big{[} |f_{1}(X_{i})|^{p_{1}}|f_{2}(X_{i})|^{p_{2}}|f_{3}(X_{i})|^{p_{3}}|f_{4}(X_{i} )|^{p_{4}}|g(X_{i})|\Big{]}\Big{)}\Big{|}\] (I.14) \[\lesssim R_{N}(\mathcal{F}^{p_{1}}\cdot\mathcal{F}^{p_{2}}\cdot \mathcal{F}^{p_{3}}\cdot\mathcal{F}^{p_{4}}\cdot g)\qquad\qquad\qquad\text{ (By Theorem \ref{thm:13} of Ma \@@cite[cite]{[\@@bibref{}{g:13}{}{}]})}\] \[\lesssim\max(B_{1}^{p_{1}+p_{2}+p_{3}+p_{4}},B_{2})\cdot\Big{(} R_{N}(\mathcal{F}^{p_{1}}\cdot\mathcal{F}^{p_{2}}\cdot\mathcal{F}^{p_{3}}\cdot \mathcal{F}^{p_{4}})+R_{N}(\{g\})\Big{)}\] (By Lemma I.2) \[\lesssim\max(B_{1}^{p_{1}+p_{2}+p_{3}+p_{4}},B_{2})\cdot R_{N}( \mathcal{F}^{p_{1}}\cdot\mathcal{F}^{p_{2}}\cdot\mathcal{F}^{p_{3}}\cdot \mathcal{F}^{p_{4}})\] (I.15) \[\lesssim\max(B_{1}^{p_{1}+p_{2}+p_{3}+p_{4}},B_{2})\cdot\max(B_{1} ^{p_{1}},B_{1}^{p_{2}+p_{3}+p_{4}})\cdot\max(B_{1}^{p_{2}},B_{1}^{p_{3}+p_{4}})\] (I.16) \[\qquad\qquad\qquad\cdot\max(B_{1}^{p_{3}},B_{1}^{p_{4}})\cdot(R_{ N}(\mathcal{F}^{p_{1}})+R_{N}(\mathcal{F}^{p_{2}})+R_{N}(\mathcal{F}^{p_{3}})+R_{N}( \mathcal{F}^{p_{4}}))\] (By repeated applications of Lemma I.2) \[\lesssim\max(B_{1}^{O(p_{1}+p_{2}+p_{3}+p_{4})},B_{2})\cdot(R_{N}(\mathcal{F}^{p_{1}}) +R_{N}(\mathcal{F}^{p_{2}})+R_{N}(\mathcal{F}^{p_{3}})+R_{N}(\mathcal{F}^{p_{4}}))\] (I.17)Finally, we simplify \(R_{N}(\mathcal{F}^{p_{i}})\) for each \(i\). For nonnegative integers \(p\), since \(\mathcal{F}\) is uniformly bounded by \(B_{1}\), the function \(x\mapsto x^{p}\) is \(pB_{1}^{p-1}\)-Lipschitz on \([-B_{1},B_{1}]\) for \(p\geq 1\) and is \(1\)-Lipschitz for \(p=0\). Thus, by Lemma I.1, we have \(R_{N}(\mathcal{F}^{p})\leq\min(1,pB_{1}^{p-1})R_{N}(\mathcal{F})\), and

\[\operatorname*{\mathbb{E}}_{X_{i}}\sup_{f_{1},f_{2},f_{3},f_{4} \in\mathcal{F},g\in\mathcal{G}}\Big{|}\frac{1}{N}\sum_{i=1}^{N}\Big{(}|f_{1}(X _{i})|^{p_{1}}|f_{2}(X_{i})|^{p_{2}}|f_{3}(X_{i})|^{p_{3}}|f_{4}(X_{i})|^{p_{4} }|g(X_{i})|\] (I.18) \[-\operatorname*{\mathbb{E}}_{X_{i}}\Big{[}|f_{1}(X_{i})|^{p_{1}} |f_{2}(X_{i})|^{p_{2}}|f_{3}(X_{i})|^{p_{3}}|f_{4}(X_{i})|^{p_{4}}|g(X_{i})| \Big{]}\Big{)}\Big{|}\] (I.19) \[\lesssim\max(B_{1}^{O(p_{1}+p_{2}+p_{3}+p_{4})},B_{2})\cdot\max(p _{1}B_{1}^{p_{1}},p_{2}B_{1}^{p_{2}},p_{3}B_{1}^{p_{3}},p_{4}B_{1}^{p_{4}}) \cdot R_{N}(\mathcal{F})\] (I.20) \[\lesssim\text{poly}(p_{1},p_{2},p_{3},p_{4},B_{1}^{p_{1}+p_{2}+p_ {3}+p_{4}},B_{2})\cdot R_{N}(\mathcal{F})\] (I.21)

as desired. 

### Concentration Lemmas

**Theorem I.4** (Theorem 5.62 of Vershynin [73]).: _Let \(A\) be an \(m\times n\) matrix with independent columns \(\{A_{j}\}_{j=1}^{n}\) and \(m\geq n\). Suppose \(A_{j}\) is isotropic, i.e., \(\operatorname*{\mathbb{E}}[A_{j}A_{j}^{\top}]=I\) and \(\|A_{j}\|_{2}=\sqrt{m}\) for every \(j\in[n]\). Let \(p\) be the incoherence parameter defined by_

\[p\triangleq\frac{1}{m}\operatorname*{\mathbb{E}}\left[\max_{i\leq n}\sum_{j \in[n],j\neq i}{\langle A_{i},A_{j}\rangle}^{2}\right].\] (I.22)

_Then there is a universal constant \(C_{0}\) such that_

\[\operatorname*{\mathbb{E}}\left[\left\|\frac{1}{m}A^{\top}A-I\right\|\right] \leq C_{0}\sqrt{\frac{p\ln n}{m}}.\] (I.23)

The following proposition gives the tail bound for inner products of random vectors in \(\mathbb{S}^{d-1}\) (c.f. Theorem 3.4.6 of Vershynin [74]).

**Proposition I.5**.: _Let \(u\) be a vector drawn uniformly at random from the unit sphere \(\mathbb{S}^{d-1}\). For any fixed vector \(v\in\mathbb{S}^{d-1}\), we have_

\[\forall t>0,\quad\Pr(|u^{\top}v|\geq t)\leq 2\exp\left(-\frac{t^{2}d}{2} \right).\] (I.24)

Proof of Proposition I.5.: By the symmetricity of \(u\) and \(v\), we can assume that \(v=(1,0,\cdots,0)\) without loss of generality. Let \(x=\frac{1}{2}(u^{\top}v+1)\). Then we have \(x\sim\text{Beta}(\frac{d-1}{2},\frac{d-1}{2}).\) Hence, the desired result follows directly from Skorski [63, Theorem 1]. 

**Lemma I.6** (Modification of Proposition 4.4 of Adamczak et al. [4]).: _Let \(X_{1},\dots,X_{N}\) be i.i.d. random vectors drawn uniformly from \(\sqrt{d}\mathbb{S}^{d-1}\), and let \(p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}\) be nonnegative integers. Suppose \(p_{1}+p_{2}+p_{3}+p_{4}\geq 2\). Then, for fixed \(w_{1},w_{2}\in\mathbb{S}^{d-1}\), as long as \(N\leq d^{C}\) for any universal constant \(C>0\), we have_

\[\sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}}\Bigl{|}\frac{1}{N}\sum_{i=1}^ {N}|{\langle X_{i},u_{1}\rangle}|^{p_{1}}|{\langle X_{i},u_{2}\rangle}|^{p_{2} }|{\langle X_{i},u_{3}\rangle}|^{p_{3}}|{\langle X_{i},u_{4}\rangle}|^{p_{4} }|{\langle X_{i},w_{1}\rangle}|^{q_{1}}|{\langle X_{i},w_{2}\rangle}|^{q_{2}}\] (I.25) \[-\operatorname*{\mathbb{E}}_{X\sim\sqrt{d}\mathbb{S}^{d-1}}\left[ {\langle X,u_{1}\rangle}|^{p_{1}}|{\langle X,u_{2}\rangle}|^{p_{2}}|{\langle X,u_{3}\rangle}|^{p_{3}}|{\langle X,u_{4}\rangle}|^{p_{4}}|{\langle X,w_{1} \rangle}|^{q_{1}}|{\langle X,w_{2}\rangle}|^{q_{2}}\right]\] (I.26) \[\leq C_{p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}}(\log d)^{O(p_{1}+p_ {2}+p_{3}+p_{4}+q_{1}+q_{2})}\cdot\sqrt{\frac{d}{N}}\] (I.27) \[+C_{p_{1},p_{2},p_{3},p_{4}}(\log d)^{O(q_{1}+q_{2})}\cdot\frac{d^ {\frac{p_{1}+p_{2}+p_{3}+p_{4}}{2}}}{N}+\frac{C_{p_{1},p_{2},p_{3},p_{4},q_{1},q _{2}}}{d^{O(\log d)}}\] (I.28)_with probability at least \(1-\frac{1}{d^{\Omega(\log d)}}\). Here, \(C_{p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}}\) is a sufficiently large constant which depends on \(p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}\) and \(C_{p_{1},p_{2},p_{3},p_{4}}\) is a constant which depends on \(p_{1},p_{2},p_{3},p_{4}\)._

Proof of Lemma I.6.: We largely follow the proof of Proposition 4.4 of Adamczak et al. [4], with some minor modifications. First, let \(E_{1}\) be the event that \(|\langle X_{i},w_{1}\rangle|\leq(\log d)^{2}\) for all \(i=1,\ldots,N\), and define \(E_{2}\) analogously for \(w_{2}\). By Lemma I.10, the sub-exponential norm of \(\langle X_{i},w_{1}\rangle\) is at most an absolute constant \(K>0\). Thus, by Proposition 2.7.1 of Vershynin [74], we have

\[\mathbb{P}\Big{(}|\langle X_{i},w_{1}\rangle|\geq(\log d)^{2}\Big{)} \leq 2\exp\Big{(}-\Omega\Big{(}\frac{(\log d)^{2}}{\|\langle X_{i},w_{1}\rangle\|_{\psi_{i}}}\Big{)}\Big{)}\] (I.29) \[\lesssim\exp\Big{(}-\Omega(\log d)^{2}\Big{)}\] (I.30) \[\lesssim\frac{1}{d^{\Omega(\log d)}}\,.\] (I.31)

Thus, by a union bound, with probability at least \(1-\frac{N}{d^{\Omega(\log d)}}\), for all \(i=1,\ldots,N\), we have \(|\langle X_{i},w_{1}\rangle|\leq(\log d)^{2}\), and \(|\langle X_{i},w_{2}\rangle|\leq(\log d)^{2}\) (by performing a similar union bound using the sub-exponential norm of \(\langle X_{i},w_{2}\rangle\)). In summary, \(E_{1}\) and \(E_{2}\) simultaneously hold with probability at least \(1-\frac{1}{d^{\Omega(\log d)}}\) since \(N\leq d^{C}\) for some universal constant \(C>0\).

Now, as in Adamczak et al. [4], define \(B>1\), which we will specify later -- this is the amount by which we will truncate \(\langle X_{i},u_{1}\rangle\), \(\langle X_{i},u_{2}\rangle\), \(\langle X_{i},u_{3}\rangle\) and \(\langle X_{i},u_{4}\rangle\). For convenience, define the function \(\varphi_{B}:\mathbb{R}\to\mathbb{R}\) given by

\[\varphi_{B}(t)=\begin{cases}t&t\in[-B,B]\\ B&t>B\\ -B&t<B\end{cases}\] (I.32)

Define \(B^{\prime}=(\log d)^{2}\), and define \(\varphi_{B^{\prime}}\) similarly to \(\varphi_{B}\), but with \(B\) replaced by \(B^{\prime}\). If \(E_{1}\) and \(E_{2}\) hold, then for all \(u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}\), we have

\[\Big{|}\frac{1}{N}\sum_{i=1}^{N}|\langle X_{i},u_{1}\rangle|^{p_{1}}|\langle X _{i},u_{2}\rangle|^{p_{2}}|\langle X_{i},u_{3}\rangle|^{p_{3}}|\langle X_{i}, u_{4}\rangle|^{p_{4}}|\varphi_{B^{\prime}}(\langle X_{i},w_{1}\rangle)|^{q_{1}}| \varphi_{B^{\prime}}(\langle X_{i},w_{2}\rangle)|^{q_{2}}\] (I.33)

\[-\operatorname*{\mathbb{E}}_{X\sim\sqrt{\operatorname*{d}}\mathbb{S}^{d-1}} \Big{[}|\langle X,u_{1}\rangle|^{p_{1}}|\langle X,u_{2}\rangle|^{p_{2}}| \langle X,u_{3}\rangle|^{p_{3}}|\langle X,u_{4}\rangle|^{p_{4}}|\varphi_{B^{ \prime}}(\langle X,w_{1}\rangle)|^{q_{1}}|\varphi_{B^{\prime}}(\langle X,w_{2} \rangle)|^{q_{2}}\Big{]}\] (I.34)

\[=\Big{|}\frac{1}{N}\sum_{i=1}^{N}|\langle X_{i},u_{1}\rangle|^{p_{1}}|\langle X _{i},u_{2}\rangle|^{p_{2}}|\langle X_{i},u_{3}\rangle|^{p_{3}}|\langle X_{i}, u_{4}\rangle|^{p_{4}}|\langle X_{i},w_{1}\rangle|^{q_{1}}|\langle X_{i},w_{2} \rangle|^{q_{2}}\] (I.35)

\[-\operatorname*{\mathbb{E}}_{X\sim\sqrt{\operatorname*{d}}\mathbb{S}^{d-1}} \Big{[}|\langle X,u_{1}\rangle|^{p_{1}}|\langle X,u_{2}\rangle|^{p_{2}}| \langle X,u_{3}\rangle|^{p_{3}}|\langle X,u_{4}\rangle|^{p_{4}}|\varphi_{B^{ \prime}}(\langle X,w_{1}\rangle)|^{q_{1}}|\varphi_{B^{\prime}}(\langle X,w_{2} \rangle)|^{q_{2}}\Big{]}\] (B.c. \[E_{1}\] and \[E_{2}\] hold)

\[=\Big{|}\frac{1}{N}\sum_{i=1}^{N}|\langle X_{i},u_{1}\rangle|^{p_{1}}|\langle X _{i},u_{2}\rangle|^{p_{2}}|\langle X_{i},u_{3}\rangle|^{p_{3}}|\langle X_{i}, u_{4}\rangle|^{p_{4}}|\langle X_{i},w_{1}\rangle|^{q_{1}}|\langle X_{i},w_{2} \rangle|^{q_{2}}\] (I.36)

\[-\operatorname*{\mathbb{E}}_{X\sim\sqrt{\operatorname*{d}}\mathbb{S}^{d-1}} \Big{[}|\langle X,u_{1}\rangle|^{p_{1}}|\langle X,u_{2}\rangle|^{p_{2}}| \langle X,u_{3}\rangle|^{p_{3}}|\langle X,u_{4}\rangle|^{p_{4}}|\langle X,w_{1} \rangle|^{q_{1}}|\langle X,w_{2}\rangle|^{q_{2}}\Big{]}\Big{|}\] (I.37)

\[\pm C_{p_{1},\ldots,p_{4},q_{1},q_{2}}e^{-\Omega(B^{\prime})}\,.\] (By Lemma I.8)

Thus, the rest of the proof will focus on obtaining an upper bound on

\[U:=\sup_{u_{1},u_{2},u_{3},u_{4}}\Big{|}\frac{1}{N}\sum_{i=1}^{N}|\langle X_{i}, u_{1}\rangle|^{p_{1}}|\langle X_{i},u_{2}\rangle|^{p_{2}}|\langle X_{i},u_{3} \rangle|^{p_{3}}|\langle X_{i},u_{4}\rangle|^{p_{4}}|\varphi_{B^{\prime}}( \langle X_{i},w_{1}\rangle)|^{q_{1}}|\varphi_{B^{\prime}}(\langle X_{i},w_{2} \rangle)|^{q_{2}}\] (I.38)

\[-\operatorname*{\mathbb{E}}_{X\sim\sqrt{\operatorname*{d}}\mathbb{S}^{d-1}} \Big{[}|\langle X,u_{1}\rangle|^{p_{1}}|\langle X,u_{2}\rangle|^{p_{2}}|\langle X,u_{3}\rangle|^{p_{3}}|\langle X,u_{4}\rangle|^{p_{4}}|\varphi_{B^{\prime}}( \langle X,w_{1}\rangle)|^{q_{1}}|\varphi_{B^{\prime}}(\langle X,w_{2}\rangle)|^{q _{2}}\Big{]}\Big{|}\] (I.39)First define

\[U^{\prime}:=\sup_{u_{1},u_{2},u_{3},u_{4}}\Big{|}\frac{1}{N}\sum_{i=1}^{N}| \varphi_{B}(\langle X_{i},u_{1}\rangle)|^{p_{1}}|\varphi_{B}(\langle X_{i},u_{2} \rangle)|^{p_{2}}|\varphi_{B}(\langle X_{i},u_{3}\rangle)|^{p_{3}}|\varphi_{B}( \langle X_{i},u_{4}\rangle)|^{p_{4}}\] (I.40)

\[|\varphi_{B^{\prime}}(\langle X_{i},w_{1}\rangle)|^{q_{1}}|\varphi_{B^{\prime} }(\langle X_{i},w_{2}\rangle)|^{q_{2}}\] (I.41)

\[-\operatorname*{\mathbb{E}}_{X\sim\sqrt{d}\mathbb{S}^{d-1}}\Big{[}|\varphi_{B }(\langle X,u_{1}\rangle)|^{p_{1}}|\varphi_{B}(\langle X,u_{2}\rangle)|^{p_{2} }|\varphi_{B}(\langle X,u_{3}\rangle)|^{p_{3}}|\varphi_{B}(\langle X,u_{4} \rangle)|^{p_{4}}\] (I.42)

\[|\varphi_{B^{\prime}}(\langle X,w_{1}\rangle)|^{q_{1}}|\varphi_{B^{\prime}}( \langle X,w_{2}\rangle)|^{q_{2}}\Big{]}\Big{|}\] (I.43)

We will first obtain an upper bound on \(U^{\prime}\), then show that \(U^{\prime}\) and \(U\) are close. We apply Lemma I.3 with \(\mathcal{F}=\{x\mapsto\varphi_{B}(\langle x,u\rangle)\mid u\in\mathbb{S}^{d-1}\}\), and with \(f_{i}\) corresponding to \(\varphi_{B}(\langle x,u_{i}\rangle)\), and finally with \(g(x)=|\varphi_{B^{\prime}}(\langle x,w_{1}\rangle)|^{q_{1}}|\varphi_{B^{\prime }}(\langle x,w_{2}\rangle)|^{q_{2}}\), to find that

\[\operatorname*{\mathbb{E}}_{X_{i}}U^{\prime} \leq\text{poly}(p_{1},p_{2},p_{3},p_{4},B^{p_{1}+p_{2}+p_{3}+p_{4 }},(B^{\prime})^{q_{1}+q_{2}})\cdot R_{N}(\mathcal{F})\] (By Lemma I.3 ) \[\leq\text{poly}(p_{1},p_{2},p_{3},p_{4},B^{p_{1}+p_{2}+p_{3}+p_{4 }},(B^{\prime})^{q_{1}+q_{2}})\cdot\operatorname*{\mathbb{E}}_{X_{i},e_{i}} \sup_{u\in\mathbb{S}^{d-1}}\Big{|}\frac{1}{N}\sum_{i=1}^{N}\epsilon_{i}\varphi _{B}(\langle X_{i},u\rangle)\Big{|}\] (I.44) \[\leq\text{poly}(p_{1},p_{2},p_{3},p_{4},B^{p_{1}+p_{2}+p_{3}+p_{4 }},(B^{\prime})^{q_{1}+q_{2}})\cdot\operatorname*{\mathbb{E}}_{X_{i},e_{i}} \sup_{u\in\mathbb{S}^{d-1}}\Big{|}\frac{1}{N}\sum_{i=1}^{N}\epsilon\langle X_{ i},u\rangle\Big{|}\] (By Lemma I.1 since

\[\leq\text{poly}(p_{1},p_{2},p_{3},p_{4},B^{p_{1}+p_{2}+p_{3}+p_{4 }},(B^{\prime})^{q_{1}+q_{2}})\cdot\operatorname*{\mathbb{E}}_{X_{i},e_{i}} \Big{\|}\frac{1}{N}\sum_{i=1}^{N}\epsilon_{i}X_{i}\Big{\|}_{2}\] (I.45) \[\leq\text{poly}(p_{1},p_{2},p_{3},p_{4},B^{p_{1}+p_{2}+p_{3}+p_{4 }},(B^{\prime})^{q_{1}+q_{2}})\cdot\sqrt{\frac{d}{N}}\,.\] (By Lemma I.9 )

Now, to show a high-probability upper bound on \(U^{\prime}\), we apply Lemma 4.8 of Adamczak et al. [4] (Talagrand's concentration inequality), and we ensure that all of the conditions are satisfied:

* We set \(X_{1},\ldots,X_{N}\) in Lemma 4.8 of Adamczak et al. [4] to be as defined in the statement of Lemma I.6.
* For convenience, define \[g_{B,B^{\prime},u_{1},u_{2},u_{3},u_{4},w_{1},w_{2}}(x)=|\varphi_{B}(\langle x,u_{1}\rangle)|^{p_{1}}|\varphi_{B}(\langle x,u_{2}\rangle)|^{p_{2}}|\varphi_{B }(\langle x,u_{3}\rangle)|^{p_{3}}|\varphi_{B}(\langle x,u_{4}\rangle)|^{p_{4}}\] (I.46) \[|\varphi_{B^{\prime}}(\langle x,w_{1}\rangle)|^{q_{1}}|\varphi_{B^{ \prime}}(\langle x,w_{2}\rangle)|^{q_{2}}\,.\] (I.47) Then, we let \[\mathcal{F}=\{x\mapsto g_{B,B^{\prime},u_{1},u_{2},u_{3},u_{4},w_{1},w_{2}}(x) -\operatorname*{\mathbb{E}}_{X\sim\sqrt{d}\mathbb{S}^{d-1}}[g_{B,B^{\prime},u_ {1},u_{2},u_{3},u_{4},w_{1},w_{2}}(X)]\mid u_{1},u_{2},u_{3},u_{4}\in\mathbb{S} ^{d-1}\}\] (I.48) where \(w_{1}\) and \(w_{2}\) are fixed (i.e. as defined in the statement of Lemma I.6).
* We let \(a=2B^{p_{1}+p_{2}+p_{3}+p_{4}}(B^{\prime})^{q_{1}+q_{2}}\), which is a uniform bound on all the functions in \(\mathcal{F}\).
* Observe that for all \(f\in\mathcal{F}\), \(\operatorname*{\mathbb{E}}_{X\sim\sqrt{d}\mathbb{S}^{d-1}}f(X)=0\).
* We let \(Z=\sup_{f\in\mathcal{F}}\sum_{i=1}^{N}f(X_{i})\) and \(\sigma^{2}=\sup_{f\in\mathcal{F}}\sum_{i=1}^{N}\operatorname*{\mathbb{E}}f(X_{ i})^{2}\). Observe that \(\sigma^{2}\leq a^{2}N\).

Thus, we apply Lemma 4.8 of Adamczak et al. [4] with

\[t=a\text{poly}(p_{1},p_{2},p_{3},p_{4},B^{p_{1}+p_{2}+p_{3}+p_{4}},(B^{\prime})^{ q_{1}+q_{2}})\cdot\sqrt{\frac{d}{N}}\cdot(\log d)^{2}\geq a(\log d)^{2} \operatorname*{\mathbb{E}}_{X_{i}}U^{\prime}\] (I.49)to find that

\[U^{\prime}\leq\mathop{\mathbb{E}}_{X_{i}}U^{\prime}+a\text{poly}(p_{1},p_{2},p_{3 },p_{4},B^{p_{1}+p_{2}+p_{3}+p_{4}},(B^{\prime})^{q_{1}+q_{2}})\cdot\sqrt{\frac{ d}{N}}\] (I.50)

with failure probability at most

\[\exp\Big{(}-\frac{t^{2}N^{2}}{2(\sigma^{2}+2aN\mathop{\mathbb{E} }U^{\prime})+3atN}\Big{)} \leq\exp\Big{(}-\frac{t^{2}N^{2}}{12\max(\sigma^{2}+2a\cdot N \mathop{\mathbb{E}}U^{\prime},atN)}\Big{)}\] (I.51) \[=\exp\Big{(}-\frac{1}{12}\min\Big{(}\frac{t^{2}N^{2}}{\sigma^{2}+ 2aN\mathop{\mathbb{E}}U^{\prime}},\frac{tN}{a}\Big{)}\Big{)}\] (I.52) \[\leq\exp\Big{(}-\frac{1}{12}\min\Big{(}\frac{t^{2}N^{2}}{a^{2}N+2 aN\mathop{\mathbb{E}}U^{\prime}},\frac{tN}{a}\Big{)}\Big{)}\] (I.53) \[\leq\exp\Big{(}-\frac{1}{12}\min\Big{(}\frac{t^{2}N^{2}}{a^{2}N+2 aN\mathop{\mathbb{E}}U^{\prime}},\sqrt{dN}\Big{)}\Big{)}\] (I.54) \[\leq\exp\Big{(}-\frac{1}{12}\min\Big{(}\frac{t^{2}N}{a^{2}+2a \mathop{\mathbb{E}}U^{\prime}},\sqrt{dN}\Big{)}\Big{)}\] (I.55) \[\leq\exp\Big{(}-\frac{1}{24}\min\Big{(}\frac{t^{2}N}{a^{2}},\frac {t^{2}N}{2a\mathop{\mathbb{E}}U^{\prime}},\sqrt{dN}\Big{)}\Big{)}\] (I.56) \[\leq\exp\Big{(}-\frac{1}{24}\min\Big{(}d,\frac{t^{2}N}{2a\mathop{ \mathbb{E}}U^{\prime}},\sqrt{dN}\Big{)}\Big{)}\] (I.57) \[\leq\exp\Big{(}-\frac{1}{24}\min\Big{(}d,tN,\sqrt{dN}\Big{)}\Big{)}\] (I.58) \[\leq\exp\Big{(}-\frac{1}{24}\min\Big{(}d,\sqrt{dN}\Big{)}\Big{)}\] (I.59) \[\leq\exp(-\Omega(\sqrt{d}))\,.\] (I.60)

Observe that Talagrand's concentration inequality only gives us a bound on \(\sup_{f\in\mathcal{F}}\sum_{i=1}^{N}f(X_{i})\), while we wish to obtain a high-probability bound on \(\sup_{f\in\mathcal{F}}|\sum_{i=1}^{N}f(X_{i})|\). However, using the same argument, we can obtain a similar high-probability bound on \(\sup_{f\in\mathcal{F}}\sum_{i=1}^{N}-f(X_{i})=-\inf_{f\in\mathcal{F}}\sum_{i=1 }^{N}f(X_{i})\). Thus, we obtain a lower bound on \(\inf_{f\in\mathcal{F}}\sum_{i=1}^{N}f(X_{i})\) which is the negative of the upper bound that we obtained for \(\sup_{f\in\mathcal{F}}\sum_{i=1}^{N}f(X_{i})\), meaning that the upper bound we obtained for \(\sup_{f\in\mathcal{F}}\sum_{i=1}^{N}f(X_{i})\) also holds for \(\sup_{f\in\mathcal{F}}|\sum_{i=1}^{N}f(X_{i})|\). In summary,

\[U^{\prime}\leq\text{poly}(p_{1},p_{2},p_{3},p_{4},B^{p_{1}+p_{2}+p_{3}+p_{4}}, (B^{\prime})^{q_{1}+q_{2}})\cdot(\log d)^{2}\cdot\sqrt{\frac{d}{N}}\] (I.61)

with probability at least \(1-\exp(-\Omega(\sqrt{d}))\).

Now that we have obtained a high-probability bound on \(U^{\prime}\), let us analyze the difference between \(U^{\prime}\) and \(U\). Recall the definition of \(g_{B,B^{\prime},u_{1},u_{2},u_{3},u_{4},w_{1},w_{2}}\) from above. Additionally, for convenience, let \(g_{B^{\prime},u_{1},u_{2},u_{3},u_{4},w_{1},w_{2}}(X)=|\langle x,u_{1}\rangle|^{ p_{1}}|\langle x,u_{2}\rangle|^{p_{2}}|\langle x,u_{3}\rangle|^{p_{3}}| \langle x,u_{4}\rangle|^{p_{4}}|\varphi_{B^{\prime}}(\langle x,w_{1}\rangle)|^ {q_{1}}|\varphi_{B^{\prime}}(\langle x,w_{2}\rangle)|^{q_{2}}\), i.e. we are simply removing the truncation by \(B\). Then, following the proof of Proposition 4.4 in Adamczak et al. [4], we have

\[U \leq U^{\prime}\] (I.62) \[+\sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}}\frac{1}{N} \sum_{i=1}^{N}|g_{B^{\prime},u_{1},u_{2},u_{3},u_{4},w_{1},w_{2}}(X_{i})-g_{B, B^{\prime},u_{1},u_{2},u_{3},u_{4},w_{1},w_{2}}(X_{i})|1\Big{(}\bigcup_{j=1}^{4} \{|\langle X_{i},u_{j}\rangle|\geq B\}\Big{)}\] (I.63) \[+\sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}}\frac{1}{N} \sum_{i=1}^{N}\mathop{\mathbb{E}}_{X_{i}}|g_{B^{\prime},u_{1},u_{2},u_{3},u_{4},w_{1},w_{2}}(X_{i})-g_{B,B^{\prime},u_{1},u_{2},u_{3},u_{4},w_{1},w_{2}}(X_{i})| 1\Big{(}\bigcup_{j=1}^{4}\{|\langle X_{i},u_{j}\rangle|\geq B\}\Big{)}\,,\] (I.64)

[MISSING_PAGE_FAIL:106]

most \(\sqrt{d}+\sqrt{m}\log\left(\frac{2N}{m}\right)\) up to a constant factor. Thus, for any set \(S\subset[N]\), if \(E_{\text{sparse}}\) holds,

\[\sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}} \Big{(}\sum_{i\in S}|\langle X_{i},u_{1}\rangle|^{p_{1}}|\langle X _{i},u_{2}\rangle|^{p_{2}}|\langle X_{i},u_{3}\rangle|^{p_{3}}|\langle X_{i},u_ {4}\rangle|^{p_{4}}|\varphi_{B^{\prime}}(\langle X_{i},u_{1}\rangle)|^{q_{1}}| \varphi_{B^{\prime}}(\langle X_{i},u_{2}\rangle)|^{q_{2}}\Big{)}^{\frac{1}{p_{1 }+p_{2}+p_{3}+p_{4}}}\] (I.73) \[\leq(B^{\prime})^{\frac{q_{1}+q_{2}}{p_{1}+p_{2}+p_{3}+p_{4}}} \sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}}\Big{(}\sum_{i\in S}|\langle X _{i},u_{1}\rangle|^{p_{1}}|\langle X_{i},u_{2}\rangle|^{p_{2}}|\langle X_{i}, u_{3}\rangle|^{p_{3}}|\langle X_{i},u_{4}\rangle|^{p_{4}}\Big{)}^{\frac{1}{p_{1 }+p_{2}+p_{3}+p_{4}}}\] (I.74) \[\leq(B^{\prime})^{\frac{q_{1}+q_{2}}{p_{1}+p_{2}+p_{3}+p_{4}}} \sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}}\Big{(}\sum_{i\in S}|\langle X _{i},u_{1}\rangle|^{\frac{2p_{1}}{p_{1}+p_{2}+p_{3}+p_{4}}}|\langle X_{i},u_{ 2}\rangle|^{\frac{2p_{2}}{p_{1}+p_{2}+p_{3}+p_{4}}}\] (I.75) \[|\langle X_{i},u_{3}\rangle|^{\frac{2p_{3}}{p_{1}+p_{2}+p_{3}+p_{4 }}}|\langle X_{i},u_{4}\rangle|^{\frac{2p_{4}}{p_{1}+p_{2}+p_{3}+p_{4}}}\Big{)} ^{\frac{1}{2}}\] (B.c. \[\|x\|_{2}\geq\|x\|_{p}\] for \[p\geq 2\] and \[x\in\mathbb{R}^{d}\], and \[p_{1}+p_{2}+p_{3}+p_{4}\geq 2\] ) \[\leq C_{p_{1},p_{2},p_{3},p_{4}}(B^{\prime})^{\frac{q_{1}+q_{2}} {p_{1}+p_{2}+p_{3}+p_{4}}}\sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}} \Big{(}\sum_{i\in S}|\langle X_{i},u_{1}\rangle|^{2}+|\langle X_{i},u_{2}\rangle |^{2}\] (I.76) \[+|\langle X_{i},u_{3}\rangle|^{2}+|\langle X_{i},u_{4}\rangle|^{2 }\Big{)}^{\frac{1}{2}}\] (By Weighted AM-GM Inequality, with \[C_{p_{1},p_{2},p_{3},p_{4}}\] a constant depending on \[p_{1},p_{2},p_{3},p_{4}\] ) \[\lesssim C_{p_{1},p_{2},p_{3},p_{4}}(B^{\prime})^{\frac{q_{1}+q_{ 2}}{p_{1}+p_{2}+p_{3}+p_{4}}}\cdot\left(\sqrt{d}+\sqrt{|S|}\log\left(\frac{2N }{|S|}\right)\right).\] (I.77)

where the last inequality is because \(E_{\text{sparse}}\) holds.

Following the proof of Proposition 4.4 of Adamczak et al. [4], we now define \(S_{B}=\{i\in[N]\mid|\langle X_{i},u_{j}\rangle|\geq B\text{ for some }j\in[4]\}\). Note that \(S_{B}\) depends on \(u_{1},u_{2},u_{3},u_{4}\), and we will now show that if we select \(B\) to be a certain value which is not too large, this will still ensure that \(S_{B}\) is small. Observe that

\[\Big{(}\sum_{i\in S_{B}}|\langle X_{i},u_{1}\rangle|^{2}+|\langle X _{i},u_{2}\rangle|^{2}+|\langle X_{i},u_{3}\rangle|^{2}+|\langle X_{i},u_{4} \rangle|^{2}\Big{)}^{1/2}\geq\Big{(}\sum_{i\in S_{B}}B^{2}\Big{)}^{1/2}\geq B \sqrt{|S_{B}|}\] (I.78)

and

\[\Big{(}\sum_{i\in S_{B}}|\langle X_{i},u_{1}\rangle|^{2}+|\langle X _{i},u_{2}\rangle|^{2}+|\langle X_{i},u_{3}\rangle|^{2}+|\langle X_{i},u_{4} \rangle|^{2}\Big{)}^{1/2}\lesssim\sqrt{d}+\sqrt{|S_{B}|}\log\left(\frac{N}{|S_ {B}|}\right).\] (B.c. \[E_{\text{sparse}}\] holds)

Combining these we obtain

\[B\sqrt{|S_{B}|}\lesssim\sqrt{d}+\sqrt{|S_{B}|}\log\left(\frac{N}{|S_{B}|} \right).\] (I.79)

Therefore, if we select \(B\gtrsim\log\left(\frac{N}{|S_{B}|}\right)\), we can eliminate the second term on the right-hand side of Eq. (I.79), finding that \(|S_{B}|\lesssim\frac{d}{B^{2}}\). Combining this with Eq. (I.73), if \(B\gtrsim\log N\) and \(E_{\text{sparse}}\) holds we obtain

\[\sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}} \Big{(}\sum_{i\in S_{B}}|\langle X_{i},u_{1}\rangle|^{p_{1}}| \langle X_{i},u_{2}\rangle|^{p_{2}}|\langle X_{i},u_{3}\rangle|^{p_{3}}|\langle X _{i},u_{4}\rangle|^{p_{4}}|\varphi_{B^{\prime}}(\langle X_{i},w_{1}\rangle)|^{q_ {1}}|\varphi_{B^{\prime}}(\langle X_{i},w_{2}\rangle)|^{q_{2}}\Big{)}^{\frac{1}{p_ {1}+p_{2}+p_{3}+p_{4}}}\] (I.80) \[\lesssim C_{p_{1},p_{2},p_{3},p_{4}}(B^{\prime})^{\frac{q_{1}+q_{ 2}}{p_{1}+p_{2}+p_{3}+p_{4}}}\cdot\left(\sqrt{d}+\sqrt{|S_{B}|}\log N\right)\] (I.81) \[\lesssim C_{p_{1},p_{2},p_{3},p_{4}}(B^{\prime})^{\frac{q_{1}+q_{ 2}}{p_{1}+p_{2}+p_{3}+p_{4}}}\cdot\left(\sqrt{d}+\frac{\sqrt{d}}{B}\log N\right)\] (B.c. \[|S_{B}|\lesssim\frac{d}{B^{2}}\] ) \[\lesssim C_{p_{1},p_{2},p_{3},p_{4}}(B^{\prime})^{\frac{q_{1}+q_{ 2}}{p_{1}+p_{2}+p_{3}+p_{4}}}\sqrt{d}\,.\] (B.c. \[B\gtrsim\log N\]Raising both sides to the \((p_{1}+p_{2}+p_{3}+p_{4})^{\text{th}}\) power, we finally obtain

\[\sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}} \sum_{i\in S_{B}}|\langle X_{i},u_{1}\rangle|^{p_{1}}|\langle X_{i},u_{2}\rangle|^{p_{2}}|\langle X_{i},u_{3}\rangle|^{p_{3}}|\langle X_{i},u_{4} \rangle|^{p_{4}}|\varphi_{B^{\prime}}(\langle X_{i},w_{1}\rangle)|^{q_{1}}| \varphi_{B^{\prime}}(\langle X_{i},w_{2}\rangle)|^{q_{2}}\] (I.82) \[\lesssim C^{\prime}_{p_{1},p_{2},p_{3},p_{4}}(B^{\prime})^{q_{1}+ q_{2}}\cdot d^{\frac{p_{1}+p_{2}+p_{3}+p_{4}}{2}}\] (I.83)

with probability \(1-e^{-\Omega(\sqrt{d})}\), where \(C^{\prime}_{p_{1},p_{2},p_{3},p_{4}}\) is a constant which depends on \(p_{1},p_{2},p_{3},p_{4}\).

In summary, assuming \(E_{\text{sparse}}\) holds and the high-probability bound on \(U^{\prime}\) holds (and these both hold simultaneously with probability at least \(1-\exp(-\Omega(\sqrt{d}))\)), we obtain

\[U\lesssim\text{poly}(p_{1},p_{2},p_{3},p_{4},B^{p_{1}+p_{2}+p_{ 3}+p_{4}},(B^{\prime})^{q_{1}+q_{2}})\cdot(\log d)^{2}\cdot\sqrt{\frac{d}{N}} +C^{\prime}_{p_{1},p_{2},p_{3},p_{4}}(B^{\prime})^{q_{1}+q_{2}}\cdot\frac{d^{ \frac{p_{1}+p_{2}+p_{3}+p_{4}}{2}}}{N}\] (I.84) \[+C_{p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}}e^{-\Omega(B)}\] (I.85)

by Eq. (I.62). Finally, by Eq. (I.33), we have

\[\sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}}\Big{|}\frac{1} {N}\sum_{i=1}^{N} |\langle X_{i},u_{1}\rangle|^{p_{1}}|\langle X_{i},u_{2}\rangle|^{p_{2}}| \langle X_{i},u_{3}\rangle|^{p_{3}}|\langle X_{i},u_{4}\rangle|^{p_{4}}| \langle X_{i},w_{1}\rangle|^{q_{1}}|\langle X_{i},w_{2}\rangle|^{q_{2}}\] (I.86) \[-\operatorname*{\mathbb{E}}_{X\sim\sqrt{d}\mathbb{S}^{d-1}}\Big{[} |\langle X,u_{1}\rangle|^{p_{1}}|\langle X,u_{2}\rangle|^{p_{2}}|\langle X,u_ {3}\rangle|^{p_{3}}|\langle X,u_{4}\rangle|^{p_{4}}|\langle X,w_{1}\rangle|^{ q_{1}}|\langle X,w_{2}\rangle|^{q_{2}}\Big{]}\Big{|}\] (I.87) \[=U\pm C_{p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}}e^{-\Omega(B^{ \prime})}\] (I.88)

as long as the events \(E_{1}\) and \(E_{2}\) hold, and these events simultaneously hold with probability at least \(1-\frac{1}{d^{\Omega(\log d)}}\). Thus, by our above bound on \(U\) we have

\[\sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}}\Big{|}\frac{1} {N}\sum_{i=1}^{N} |\langle X_{i},u_{1}\rangle|^{p_{1}}|\langle X_{i},u_{2}\rangle|^{p_{2}}| \langle X_{i},u_{3}\rangle|^{p_{3}}|\langle X_{i},u_{4}\rangle|^{p_{4}}| \langle X_{i},w_{1}\rangle|^{q_{1}}|\langle X_{i},w_{2}\rangle|^{q_{2}}\] (I.89) \[-\operatorname*{\mathbb{E}}_{X\sim\sqrt{d}\mathbb{S}^{d-1}}\Big{[} |\langle X,u_{1}\rangle|^{p_{1}}|\langle X,u_{2}\rangle|^{p_{2}}|\langle X,u_ {3}\rangle|^{p_{3}}|\langle X,u_{4}\rangle|^{p_{4}}|\langle X,w_{1}\rangle|^{ q_{1}}|\langle X,w_{2}\rangle|^{q_{2}}\Big{]}\] (I.90) \[\lesssim\text{poly}(p_{1},p_{2},p_{3},p_{4},B^{p_{1}+p_{2}+p_{3} +p_{4}},(B^{\prime})^{q_{1}+q_{2}})\cdot(\log d)^{2}\cdot\sqrt{\frac{d}{N}}\] (I.91) \[+C^{\prime}_{p_{1},p_{2},p_{3},p_{4}}(B^{\prime})^{q_{1}+q_{2}} \cdot\frac{d^{\frac{p_{1}+p_{2}+p_{3}+p_{4}}{2}}}{N}+C_{p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}}e^{-\Omega(B)}\] (I.92) \[+C_{p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}}e^{-\Omega(B^{\prime})}\,.\] (I.93)Setting \(B^{\prime}=(\log d)^{2}\) and \(B\gtrsim\max(\log N,(\log d)^{2})\asymp(\log d)^{2}\) (recall that \(N\leq d^{C}\) for a universal constant \(C\)), we have

\[\sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}}\Big{|}\frac{1}{N} \sum_{i=1}^{N}|\langle X_{i},u_{1}\rangle|^{p_{1}}|\langle X_{i},u_{2}\rangle| ^{p_{2}}|\langle X_{i},u_{3}\rangle|^{p_{3}}|\langle X_{i},u_{4}\rangle|^{p_{4} }|\langle X_{i},w_{1}\rangle|^{q_{1}}|\langle X_{i},w_{2}\rangle|^{q_{2}}\] (I.94) \[\qquad\qquad-\operatorname*{\mathbb{E}}_{X\sim\sqrt{\mathbb{d}} \mathbb{S}^{d-1}}\Big{[}|\langle X,u_{1}\rangle|^{p_{1}}|\langle X,u_{2} \rangle|^{p_{2}}|\langle X,u_{3}\rangle|^{p_{3}}|\langle X,u_{4}\rangle|^{p_{4 }}|\langle X,w_{1}\rangle|^{q_{1}}|\langle X,w_{2}\rangle|^{q_{2}}\Big{]}\Big{]}\] (I.95) \[\leq C_{p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}}(\log d)^{O(p_{1}+p_ {2}+p_{3}+p_{4}+q_{1}+q_{2})}\cdot\sqrt{\frac{d}{N}}\] (I.96) \[\qquad\qquad\qquad+C_{p_{1},p_{2},p_{3},p_{4}}(\log d)^{O(q_{1}+q _{2})}\cdot\frac{d^{\frac{p_{1}+p_{2}+p_{3}+p_{4}}{2}}}{N}+\frac{C_{p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}}}{d^{\Omega(\log d)}}\] (I.97)

with probability at least \(1-\frac{1}{d^{\Omega(\log d)}}\), as desired. 

**Lemma I.7** (Corollary for Uniform Samples from \(\mathbb{S}^{d-1}\)).: _Let \(x_{1},\ldots,x_{N}\) be i.i.d. random vectors sampled uniformly from \(\mathbb{S}^{d-1}\), and let \(p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}\) be nonnegative integers. Suppose \(p_{1}+p_{2}+p_{3}+p_{4}\geq 2\). Then, for fixed \(w_{1},w_{2}\in\mathbb{S}^{d-1}\), as long as \(N\leq d^{C}\) for any universal constant \(C>0\), we have_

\[\sup_{u_{1},u_{2},u_{3},u_{4}\in\mathbb{S}^{d-1}}\Big{|}\frac{1} {N}\sum_{i=1}^{N}|\langle x_{i},u_{1}\rangle|^{p_{1}}|\langle x_{i},u_{2} \rangle|^{p_{2}}|\langle x_{i},u_{3}\rangle|^{p_{3}}|\langle x_{i},u_{4}\rangle |^{p_{4}}|\langle x_{i},w_{1}\rangle|^{q_{1}}|\langle x_{i},w_{2}\rangle|^{q_{2}}\] (I.98) \[\qquad\qquad-\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}} \Big{[}|\langle x,u_{1}\rangle|^{p_{1}}|\langle x,u_{2}\rangle|^{p_{2}}| \langle x,u_{3}\rangle|^{p_{3}}|\langle x,u_{4}\rangle|^{p_{4}}|\langle x,w_{1 }\rangle|^{q_{1}}|\langle x,w_{2}\rangle|^{q_{2}}\Big{]}\] (I.109) \[\leq\frac{1}{d^{\frac{p_{1}+p_{2}+p_{3}+p_{4}+q_{1}+q_{2}}{2}}} \Big{(}C_{p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}}(\log d)^{O(p_{1}+p_{2}+p_{3}+p _{4}+q_{1}+q_{2})}\cdot\sqrt{\frac{d}{N}}\] (I.100) \[\qquad\qquad\qquad+C_{p_{1},p_{2},p_{3},p_{4}}(\log d)^{O(q_{1}+q _{2})}\cdot\frac{d^{\frac{p_{1}+p_{2}+p_{3}+p_{4}}{2}}}{N}+\frac{C_{p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}}}{d^{\Omega(\log d)}}\Big{)}\] (I.101)

_with probability at least \(1-\frac{1}{d^{\Omega(\log d)}}\). Here, \(C_{p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}}\) is a constant which depends on \(p_{1},p_{2},p_{3},p_{4},q_{1},q_{2}\), and \(C_{p_{1},p_{2},p_{3},p_{4}}\) is a constant which depends on \(p_{1},p_{2},p_{3},p_{4}\)._

Proof of Lemma I.7.: This is a direct corollary of Lemma I.6, obtained by dividing both sides of Lemma I.6 by \(\frac{1}{d^{(p_{1}+p_{2}+p_{3}+p_{4}+q_{1}+q_{2})/2}}\). 

**Lemma I.8** (Moments for Truncated Dot Products with Random Spherical Vectors).: _Let \(P\) be a positive integer, and for \(i=1,\ldots,P\), let \(u_{i}\in\mathbb{S}^{d-1}\) and \(p_{i}\) be a nonnegative integer. Additionally, let \(B\geq 1\). Finally, let \(X\) be a random vector drawn uniformly from \(\sqrt{d}\mathbb{S}^{d-1}\). Then, for some absolute constant \(c>0\) we have_

\[\Big{|}\operatorname*{\mathbb{E}}_{X}\Big{[}\prod_{i=1}^{P}|\langle X,u_{i} \rangle|^{p_{i}}\Big{]}-\operatorname*{\mathbb{E}}_{X}\Big{[}\prod_{i=1}^{P}| \varphi_{B}(\langle X,u_{i}\rangle)|^{p_{i}}\Big{]}\Big{|}\leq C_{p_{1},\ldots,p_ {P}}e^{-cB}\] (I.102)

_where \(\varphi_{B}\) is defined as_

\[\varphi_{B}(t)=\begin{cases}t&t\in[-B,B]\\ B&t>B\\ -B&t<B\end{cases}\] (I.103)

_and \(C_{p_{1},\ldots,p_{P}}\) is a constant which depends only on \(p_{1},\ldots,p_{P}\)._Proof of Lemma I.8.: We use an argument similar to one used in part of the proof of Proposition 4.4 in Adamczak et al. [4]. First, for \(i\in[P]\), we can write

\[|\langle X,u_{i}\rangle|^{p_{i}}=|\varphi_{B}(\langle X,u_{i}\rangle)|^{p_{i}}+( |\langle X,u_{i}\rangle|^{p_{i}}-B^{p_{i}})1_{|\langle X,u_{i}\rangle\geq B}\,.\] (I.104)

Thus, in the expression

\[\prod_{i=1}^{P}|\langle X,u_{i}\rangle|^{p_{i}}-\prod_{i=1}^{P}|\varphi_{B}( \langle X,u_{i}\rangle)|^{p_{i}}\,,\] (I.105)

if we expand the first product using Eq. (I.104), then we obtain terms of the form \(\prod_{i=1}^{P}F_{i}\), where \(F_{i}\) is either \(|\varphi_{B}(\langle X,u_{i}\rangle)|^{p_{i}}\) or \((|\langle X,u_{i}\rangle|^{p_{i}}-B^{p_{i}})1(|\langle X,u_{i}\rangle|\geq B)\). Observe that the term where \(F_{i}=|\varphi_{B}(\langle X,u_{i}\rangle)|^{p_{i}}\) for all \(i\in[P]\) cancels with the second term in Eq. (I.105). Thus, in order to obtain an upper bound on

\[\operatorname*{\mathbb{E}}\Big{[}\prod_{i=1}^{P}|\langle X,u_{i}\rangle|^{p_ {i}}-\prod_{i=1}^{P}|\varphi_{B}(\langle X,u_{i}\rangle)|^{p_{i}}\Big{]}\,,\] (I.106)

it suffices to upper bound the terms \(\prod_{i=1}^{P}F_{i}\) where at least one of the \(F_{i}\) is \((|\langle X,u_{i}\rangle|^{p_{i}}-B^{p_{i}})1(|\langle X,u_{i}\rangle|\geq B)\). Observe however that

\[(|\langle X,u_{i}\rangle|^{p_{i}}-B^{p_{i}})1(|\langle X,u_{i}\rangle|\geq B )\leq|\langle X,u_{i}\rangle|^{p_{i}}1(|\langle X,u_{i}\rangle|\geq B)\,.\] (I.107)

Thus, because \(|\varphi_{B}(\langle X,u_{i}\rangle)|\leq|\langle X,u_{i}\rangle|\), in order to upper bound the terms \(\prod_{i=1}^{P}F_{i}\) where at least one of the \(F_{i}\) is \((|\langle X,u_{i}\rangle|^{p_{i}}-B^{p_{i}})1(|\langle X,u_{i}\rangle\geq B)\), it suffices to obtain an upper bound on the expected value of

\[1(|\langle X,u_{i}\rangle|\geq B\text{ for some }i\in[P])\prod_{i=1}^{P}| \langle X,u_{i}\rangle|^{p_{i}}\leq\sum_{i=1}^{P}1(|\langle X,u_{i}\rangle|\geq B )\prod_{j=1}^{P}|\langle X,u_{j}\rangle|^{p_{i}}\,.\] (I.108)

Without loss of generality, let us bound the expected value of the first term in the sum above:

\[\operatorname*{\mathbb{E}}_{X}\Big{[}1(|\langle X,u_{1}\rangle| \geq B)\prod_{i=1}^{P}|\langle X,u_{j}\rangle|^{p_{i}}\Big{]} \leq\operatorname*{\mathbb{E}}_{X}[1(|\langle X,u_{1}\rangle| \geq B)]^{1/2}\operatorname*{\mathbb{E}}_{X}\Big{[}\prod_{i=1}^{P}|\langle X,u_{j}\rangle|^{2p_{i}}\Big{]}^{1/2}\] (By the Cauchy-Schwarz Inequality) \[\leq\operatorname*{\mathbb{E}}_{X}[1(|\langle X,u_{1}\rangle| \geq B)]^{1/2}\cdot\prod_{i=1}^{P}\operatorname*{\mathbb{E}}_{X}[|\langle X,u _{j}\rangle|^{2^{i+1}p_{i}}]^{\frac{1}{2^{i+1}}}\] (By applying the Cauchy-Schwarz inequality \[P-1\] times) \[\leq C_{p_{1},\dots,p_{P}}\operatorname*{\mathbb{E}}_{X}[1(| \langle X,u_{1}\rangle|\geq B)]^{1/2}\,,\] (I.109)

where the last inequality is by Lemma I.10 and Proposition 2.7.1 of Vershynin [74]. Finally, since the sub-exponential norm of \(\langle X,u_{1}\rangle\) is at most an absolute constant by Lemma I.10, we have that

\[\operatorname*{\mathbb{E}}_{X}[1(|\langle X,u_{1}\rangle|\geq B)=\mathbb{P}(| \langle X,u_{1}\rangle|\geq B)\lesssim e^{-cB}\] (I.110)

for some absolute constant \(c>0\). Thus, by Eq. (I.109), we obtain

\[\operatorname*{\mathbb{E}}_{X}\Big{[}1(|\langle X,u_{1}\rangle|\geq B)\prod_{i =1}^{P}|\langle X,u_{j}\rangle|^{p_{j}}\Big{]}\lesssim C_{p_{1},\dots,p_{P}}e ^{-cB}\] (I.111)

for some absolute constant \(c>0\). This completes the proof. 

**Lemma I.9**.: _Let \(N,d\in\mathbb{N}\). Let \(X_{1},\dots,X_{N}\) i.i.d. random vectors drawn uniformly from \(\sqrt{d}\mathbb{S}^{d-1}\). Additionally, let \(\epsilon_{1},\dots,\epsilon_{N}\) be i.i.d. Rademacher random variables. Then,_

\[\operatorname*{\mathbb{E}}_{X,\epsilon_{i}}\Big{\|}\frac{1}{N}\sum_{i=1}^{N} \epsilon_{i}X_{i}\Big{\|}_{2}\leq\sqrt{\frac{d}{N}}\,.\] (I.112)Proof of Lemma I.9.: We first bound the second moment of the norm:

\[\mathop{\mathbb{E}}\limits_{X,\epsilon_{i}}\Big{\|}\frac{1}{N}\sum \limits_{i=1}^{N}\epsilon_{i}X_{i}\Big{\|}_{2}^{2} =\frac{1}{N^{2}}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N}\mathop {\mathbb{E}}\limits_{\epsilon_{i},X_{i}}\epsilon_{i}\epsilon_{j}\langle X_{i},X_ {j}\rangle\] (I.113) \[=\frac{1}{N^{2}}\sum\limits_{i=1}^{N}\mathop{\mathbb{E}}\limits_ {X_{i}}\|X_{i}\|_{2}^{2}\] (B.c. \[\mathbb{E}[\epsilon_{i}\epsilon_{j}]=0\] for \[i\neq j\] ) \[=\frac{1}{N^{2}}\sum\limits_{i=1}^{N}d\] (B.c. \[\|X_{i}\|_{2}=\sqrt{d}\] with probability \[1\] ) \[=\frac{Nd}{N^{2}}\] (I.114) \[=\frac{d}{N}\,.\] (I.115)

Thus, \(\mathop{\mathbb{E}}\nolimits_{X,\epsilon_{i}}\Big{\|}\frac{1}{N}\sum\limits_ {i=1}^{N}\epsilon_{i}X_{i}\Big{\|}_{2}\leq\Big{(}\mathop{\mathbb{E}}\nolimits _{X,\epsilon_{i}}\Big{\|}\frac{1}{N}\sum\nolimits_{i=1}^{N}\epsilon_{i}X_{i} \Big{\|}_{2}^{2}\Big{)}^{1/2}\leq\sqrt{\frac{d}{N}}\), as desired. 

**Lemma I.10** (Sub-Exponential Norm of Uniform Distribution on \(\sqrt{d}\mathbb{S}^{d-1}\)).: _Assume \(d\geq C\) for some sufficiently large universal constant \(C\). Then there exists an absolute constant \(K>0\) such that, if \(X\) is a random vector drawn uniformly from \(\sqrt{d}\mathbb{S}^{d-1}\) and \(v\in\mathbb{S}^{d-1}\) is a fixed vector, then \(\|\langle X,v\rangle\|_{\psi_{1}}\leq K\), where \(\|\cdot\|_{\psi_{1}}\) denotes the sub-exponential norm of a random variable. (See Definition 2.7.5 of Vershynin [74].) As a corollary, for any \(p\geq 1\), there exists a constant \(C_{p}>0\) depending on \(p\) such that for any \(v\in\mathbb{S}^{d-1}\), \(\mathop{\mathbb{E}}\nolimits_{X\sim\sqrt{d}\mathbb{S}^{d-1}}[|\langle X,v \rangle|^{p}]\leq C_{p}\)._

Proof of Lemma I.10.: Let \(X\) be drawn uniformly at random from the uniform distribution on \(\sqrt{d}\mathbb{S}^{d-1}\), and let \(v\in\mathbb{S}^{d-1}\). Observe that the distribution of \(\frac{1}{\sqrt{d}}\langle X,v\rangle\) is \(\mu_{d}\). Thus, the density of \(\frac{1}{\sqrt{d}}\langle X,v\rangle\) is \(p(t)=\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/2)}(1-t^{2})^{\frac{d-3}{2}}\) by Eqs. (1.16) and (1.18) of Atkinson and Han [12]. By a change of variables, the density of \(\langle X,v\rangle\) is

\[p(t)=\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/2)}\Big{(}1-\frac{t^{2}}{d} \Big{)}^{\frac{d-3}{2}}\cdot\frac{1}{\sqrt{d}}.\] (I.116)

Thus, for any \(t\geq 0\),

\[\mathbb{P}(|\langle X,v\rangle|\geq t) \lesssim 2\int_{t}^{\infty}p(s)ds\] (I.117) \[\lesssim 2\int_{t}^{\infty}\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma((d-1)/ 2)}\Big{(}1-\frac{s^{2}}{d}\Big{)}^{\frac{d-3}{2}}\cdot\frac{1}{\sqrt{d}}ds\] (I.118) \[\lesssim\int_{t}^{\infty}\Big{(}1-\frac{s^{2}}{d}\Big{)}^{\frac{d -3}{2}}ds\] (By Stirling's formula) \[\lesssim\int_{t}^{\infty}e^{-\frac{d-3}{2}\cdot\frac{s^{2}}{d}}ds\] (B.c. \[1+x\leq e^{x}\] ) \[\lesssim\int_{t}^{\infty}e^{-\frac{s^{2}}{3}}ds\] (B.c. \[\frac{d-3}{2d}\geq\frac{1}{3}\] for \[d\] sufficiently large)

Thus, for some absolute constant \(C>0\) and all \(t\geq 0\),

\[\mathbb{P}(|\langle X,v\rangle|\geq t)\leq C\int_{t}^{\infty}e^{-\frac{s^{2}}{ 3}}ds\leq C\int_{t}^{\infty}e^{-\frac{st}{3}}ds=C\Big{(}-\frac{3}{t}e^{-\frac{ st}{3}}\Big{)}\Big{|}_{t}^{\infty}=\frac{3C}{t}e^{-\frac{t^{2}}{3}}\leq e^{-\frac{t^{2}}{3} +\log\frac{3C}{t}}\] (I.119)

Suppose \(t\geq(18C)^{1/3}\). Then, \(\frac{t^{2}}{6}\geq\log\frac{3C}{t}\), meaning that \(-\frac{t^{2}}{3}+\log\frac{3C}{t}\leq-\frac{t^{2}}{6}\), and therefore

\[\mathbb{P}(|\langle X,v\rangle|\geq t)\leq e^{-\frac{t^{2}}{6}}\] (I.120)Thus, by the definition of sub-Gaussian norm (Definition 2.5.6 and Proposition 2.5.2 of Vershynin [74]), the sub-Gaussian norm of \(\langle X,v\rangle\) is bounded by a constant, i.e. for any \(v\in\mathbb{S}^{d-1}\), we have \(\|\langle X,v\rangle\|_{\psi_{2}}\leq K\) for some absolute constant \(K>0\) (here \(\|\cdot\|_{\psi_{2}}\) denotes the sub-Gaussian norm of a random variable). By Propositions 2.5.2 and 2.7.1, and Definition 2.7.5, of Vershynin [74], this implies that \(\|\langle X,v\rangle\|_{\psi_{1}}\lesssim 1\), since sub-Gaussian random variables are always sub-exponential (specifically, using item (ii) of Proposition 2.5.2 and item (b) of Proposition 2.7.1 of Vershynin [74]). The last statement of the lemma follows directly from Proposition 2.7.1, item (b) of Vershynin [74]. 

**Lemma I.11** (Modification of Theorem 3.6 of Adamczak et al. [4]).: _Let \(d\geq C\) for some absolute constant \(C\), and \(1\leq N\leq e^{\sqrt{d}}\). Let \(X_{1},\ldots,X_{N}\) be i.i.d. random vectors sampled from the uniform distribution on \(\sqrt{d}\mathbb{S}^{d-1}\). Finally, let \(t\geq 1\). Then, for some absolute constant \(c>0\), with probability at least \(1-e^{-ct\sqrt{d}}\), for all \(m\in[N]\),_

\[\sup_{\begin{subarray}{c}z\in\mathbb{S}^{d-1}\\ |\text{supp}\,z|\leq m\end{subarray}}\Big{\|}\sum_{i=1}^{N}z_{i}X_{i}\Big{\|} _{2}\lesssim t\Big{(}\sqrt{d}+\sqrt{m}\log\Big{(}\frac{2N}{m}\Big{)}\Big{)}\] (I.121)

_Here, we use \(\text{supp}\,z\) to denote the set of nonzero coordinates of \(z\)._

Proof of Lemma I.11.: The proof is the same as that of Theorem 3.6 of Adamczak et al. [4] -- while Theorem 3.6 of Adamczak et al. [4] is shown for the case where \(X_{1},\ldots,X_{N}\) are sampled from an isotropic, log-concave probability distribution, the proof can be generalized to the case where the \(X_{i}\) are sampled from \(\sqrt{d}\mathbb{S}^{d-1}\). To see why, observe that the proof only uses the fact that the distribution of the \(X_{i}\) is log-concave in the following places: (i) when applying Lemma 3.1 (also of Adamczak et al. [4]), of which the conclusion is that \(\max_{i\leq N}\|X_{i}\|_{2}\leq C_{0}K\sqrt{d}\) with probability at least \(1-e^{-K\sqrt{d}}\), for some absolute constant \(C_{0}\), (ii) when applying Lemmas 3.3 and 3.4 (also of Adamczak et al. [4]) the proof requires that \(\|\langle X_{i},y\rangle\|_{\psi_{1}}\leq\psi\) for some absolute constant \(\psi>0\) and any \(y\in\mathbb{S}^{d-1}\). However, property (i) trivially holds when the \(X_{i}\) are on \(\sqrt{d}\mathbb{S}^{d-1}\) since \(\|X_{i}\|_{2}=\sqrt{d}\), and property (ii) holds by Lemma I.10. 

### Helper Lemmas

**Lemma I.12**.: _Let \(f:[0,\infty)\to\mathbb{R}_{\geq 0}\) such that_

\[\frac{d}{dt}f(t)^{2}\leq p_{t}f(t)+q_{t}f(t)^{2}\] (I.122)

_where \(p_{t},q_{t}>0\) for \(t\in[0,\infty)\). Then, for \(T>0\) we have_

\[f(T)^{2}\leq\Big{(}f(0)+\frac{\max_{t}p_{t}}{\min_{t}q_{t}}\Big{)}^{2}\exp \Big{(}\int_{0}^{T}q_{t}dt\Big{)}.\] (I.123)

Proof.: \[\frac{df(t)}{dt}=\frac{1}{2f(t)}\frac{df(t)^{2}}{dt}\leq\frac{p_{t}}{2}+\frac{ q_{t}}{2}f(t)\leq\frac{\max_{t}p_{t}}{2}+\frac{q_{t}}{2}f(t).\] (I.124)

Thus, we have

\[\frac{d}{dt}\Big{(}f(t)+\frac{\max_{t}p_{t}}{\min_{t}q_{t}}\Big{)}\leq\frac{ q_{t}}{2}\Big{(}f(t)+\frac{\max_{t}p_{t}}{\min_{t}q_{t}}\Big{)}.\] (I.125)

As a result, by Fact I.13,

\[f(T)\leq\Big{(}f(0)+\frac{\max_{t}p_{t}}{\min_{t}q_{t}}\Big{)}\exp\Big{(}\int_ {0}^{T}\frac{q_{t}}{2}dt\Big{)}.\] (I.126)

Taking the square of both sides finishes the proof.

**Fact I.13** (Gronwall's Inequality [13]).: _Let \(a<b\), and let \(f:[a,b]\to\mathbb{R}_{\geq 0}\) and \(g:[a,b]\to\mathbb{R}\) be differentiable functions, such that_

\[f^{\prime}(t)\leq g(t)f(t)\] (I.127)

_for all \(t\in[a,b]\). Then, for all \(t\in[a,b]\), \(f(t)\leq f(a)\exp(\int_{a}^{s}g(s)ds)\). Additionally, suppose \(f,g\) satisfy_

\[f^{\prime}(t)\geq g(t)f(t)\] (I.128)

_for all \(t\in[a,b]\). Then, for all \(t\in[a,b]\), \(f(t)\geq f(a)\exp(\int_{a}^{s}g(s)ds)\)._

**Proposition I.14**.: _For any \(\beta_{2},\beta_{4}\) such that \(0\leq\beta_{2}^{2}\leq\beta_{4}\leq\beta_{2}\leq 1\), there exists a one-dimensional distribution \(\mu\) that satisfies_

\[\operatorname*{\mathbb{E}}_{w\sim\mu}[w^{2}] =\beta_{2},\] (I.129) \[\operatorname*{\mathbb{E}}_{w\sim\mu}[w^{4}] =\beta_{4},\] (I.130) \[\operatorname{supp}(\mu) \subseteq[-1,1].\] (I.131)

Proof.: Let \(p=\beta_{2}^{2}/\beta_{4}\). By the assumptions we have \(0\leq p\leq 1\). Consider the distribution

\[\mu(x)=\frac{\beta_{2}^{2}}{\beta_{4}}\delta_{x=\beta_{4}^{1/2}\beta_{2}^{-1/ 2}}+\left(1-\frac{\beta_{2}^{2}}{\beta_{4}}\right)\delta_{x=0},\] (I.132)

where \(\delta_{x=t}\) denotes the Dirac measure at \(x=t\). First of all, since \(\beta_{2}^{2}\leq\beta_{4}\), the distribution \(\mu(x)\) is well-defined. And we can verify that

\[\operatorname*{\mathbb{E}}_{w\sim\mu}[w^{2}] =\frac{\beta_{2}^{2}}{\beta_{4}}(\beta_{4}^{1/2}\beta_{2}^{-1/2}) ^{2}=\beta_{2},\] (I.133) \[\operatorname*{\mathbb{E}}_{w\sim\mu}[w^{4}] =\frac{\beta_{2}^{2}}{\beta_{4}}(\beta_{4}^{1/2}\beta_{2}^{-1/2} )^{4}=\beta_{4}.\] (I.134)

Note that \(0\leq\beta_{4}\leq\beta_{2}\) implies \(\beta_{4}^{1/2}\beta_{2}^{-1/2}\in[0,1]\). Consequently,

\[\operatorname{supp}(\mu)\subseteq[-1,1].\] (I.135)

**Lemma I.15**.: _Let \(\rho\) be a distribution which is rotationally invariant as in Definition 4.1. Let \(\nabla_{u}L(\rho)\) be defined as in Eq. (3.4), and let \(\nabla_{z}L(\rho)\) denote the last \((d-1)\) coordinates of \(\nabla_{u}L(\rho)\). Then, for any particle \(u=(w,z)\), we have that \(\nabla_{z}L(\rho)\) is a scalar multiple of \(z\). In particular, if \(\operatorname{grad}_{u}L(\rho)\) is defined as in Eq. (3.4) and \(\operatorname{grad}_{z}L(\rho)\) denotes the last \((d-1)\) coordinates of \(\operatorname{grad}_{u}L(\rho)\), then \(\operatorname{grad}_{z}L(\rho)\) is a scalar multiple of \(z\)._

Proof.: From Eq. (3.4), we have

\[\nabla_{u}L(\rho)=\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho} (x)-y(x))\sigma^{\prime}(u^{\top}x)x]\,.\] (I.136)

Since \(\rho\) is rotationally invariant, if \(x,x^{\prime}\in\mathbb{S}^{d-1}\) such that \(\langle e_{1},x\rangle=\langle e_{1},x^{\prime}\rangle\), then \(f_{\rho}(x)-y(x)=f_{\rho}(x^{\prime})-y(x^{\prime})\). Thus, \((f_{\rho}(x)-y(x))\sigma^{\prime}(u^{\top}x)\) only depends on \(\langle e_{1},x\rangle\) and \(\langle u,x\rangle\).

Let \(P_{e_{1},u}\in\mathbb{R}^{d\times d}\) denote the projection matrix into the subspace spanned by \(e_{1}\) and \(u\). Then,

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho}(x)-y(x))\sigma^{ \prime}(u^{\top}x)(x-P_{e_{1},u}x)]=0\] (I.137)

because if \(v\in\mathbb{S}^{d-1}\) is orthogonal to \(e_{1}\) and \(u\), then the distribution of \(\langle v,x\rangle\) conditioned on \(\langle e_{1},x\rangle\) and \(\langle u,x\rangle\) is symmetric around \(0\). Thus,

\[\nabla_{u}L(\rho)=\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[(f_{\rho} (x)-y(x))\sigma^{\prime}(u^{\top}x)P_{e_{1},u}x]=P_{e_{1},u}(\nabla_{u}L(\rho))\] (I.138)

and we have that \(\nabla_{u}L(\rho)\) is in the subspace spanned by \(e_{1}\) and \(u\). In other words, there are scalars \(c_{1},c_{2}\in\mathbb{R}\) such that

\[\nabla_{u}L(\rho)=c_{1}e_{1}+c_{2}u\] (I.139)

and taking the last \((d-1)\) coordinates of both sides of this equation gives \(\nabla_{u}L(\rho)=c_{2}z\), as desired. The last statement of the lemma holds because \(\operatorname{grad}_{u}L(\rho)=(I-uu^{\top})\nabla_{u}L(\rho)\), which is also a linear combination of \(e_{1}\) and \(u\)

**Lemma I.16**.: _Let \(P\) be a polynomial of degree at most \(k>0\), where \(k\) is bounded by a universal constant, and let \(w_{1},w_{2}\in[-1,1]\). Suppose the coefficients of \(P\) are bounded by \(B\). Then, \(|P(w_{1})-P(w_{2})|\lesssim B|w_{1}-w_{2}|\)._

Proof.: Let \(c_{i}\) be the coefficient of the \(i^{th}\) degree term in \(P\). Then, the lemma follows from expanding \(P(w_{1})-P(w_{2})\) as

\[|P(w_{1})-P(w_{2})| \leq\sum_{i=0}^{k}|c_{i}||w_{1}^{i}-w_{2}^{i}|\] (I.140) \[\leq\sum_{i=0}^{k}|c_{i}||w_{1}-w_{2}|\cdot\sum_{j=0}^{i}|w_{1}|^{ j}|w_{2}|^{i-j}\] (I.141) \[\leq\sum_{i=0}^{k}(i+1)\cdot|c_{i}||w_{1}-w_{2}|\] (B.c. \[|w_{1}|,|w_{2}|\leq 1\] ) \[\leq B(k+1)\sum_{i=0}^{k}|w_{1}-w_{2}|\] (I.142) \[=B(k+1)^{2}|w_{1}-w_{2}|\] (I.143) \[\lesssim B|w_{1}-w_{2}|\] (B.c. \[k\] is at most an absolute constant)

as desired. 

**Lemma I.17** (Powers of Dot Products on \(\mathbb{S}^{d-1}\)).: _Let \(u_{1},u_{2},u_{3},u_{4},u_{5}\in\mathbb{S}^{d-1}\), and \(p,q,r,s,t\geq 1\). Then,_

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[|\langle u_{1},x\rangle|^{ p}|\langle u_{2},x\rangle|^{q}|\langle u_{3},x\rangle|^{r}|\langle u_{4},x \rangle|^{s}|\langle u_{5},x\rangle|^{t}]\leq C_{p,q,r,s,t}\frac{1}{d^{(p+q+r+ s+t)/2}}\,.\] (I.144)

_Here, \(C_{p,q,r,s,t}\) is a constant depending on \(p,q,r,s,t\)._

Proof of Lemma I.17.: By repeated applications of the Cauchy-Schwarz inequality, we have

\[\operatorname*{\mathbb{E}}_{X\sim\sqrt{\mathbb{S}^{d-1}}} [|\langle u_{1},X\rangle|^{p}|\langle u_{2},X\rangle|^{q}|\langle u _{3},X\rangle|^{r}|\langle u_{4},X\rangle|^{s}|\langle u_{5},X\rangle|^{t}]\] (I.145) \[\leq\operatorname*{\mathbb{E}}_{X\sim\sqrt{\mathbb{S}^{d-1}}}[ \langle u_{1},X\rangle|^{2p}]^{1/2}\operatorname*{\mathbb{E}}_{X\sim\sqrt{ \mathbb{S}^{d-1}}}[|\langle u_{2},X\rangle|^{2q}|\langle u_{3},X\rangle|^{2r}| \langle u_{4},X\rangle|^{2s}|\langle u_{5},X\rangle|^{2t}]^{1/2}\] (I.146) \[\leq\operatorname*{\mathbb{E}}_{X\sim\sqrt{\mathbb{S}^{d-1}}}[ \langle u_{1},X\rangle|^{2p}]^{1/2}\operatorname*{\mathbb{E}}_{X\sim\sqrt{ \mathbb{S}^{d-1}}}[|\langle u_{2},X\rangle|^{4q}]^{1/4}\operatorname*{\mathbb{E }}_{X\sim\sqrt{\mathbb{S}^{d-1}}}[|\langle u_{3},X\rangle|^{4r}|\langle u_{ 4},X\rangle|^{4s}|\langle u_{5},X\rangle|^{4t}]^{1/4}\] (I.147) \[\leq\operatorname*{\mathbb{E}}_{X\sim\sqrt{\mathbb{S}^{d-1}}}[ \langle u_{1},X\rangle|^{2p}]^{1/2}\operatorname*{\mathbb{E}}_{X\sim\sqrt{ \mathbb{S}^{d-1}}}[|\langle u_{2},X\rangle|^{4q}]^{1/4}\operatorname*{\mathbb{E }}_{X\sim\sqrt{\mathbb{S}^{d-1}}}[|\langle u_{3},X\rangle|^{8r}]^{1/8}\] (I.148) \[\operatorname*{\mathbb{E}}_{X\sim\sqrt{\mathbb{S}^{d-1}}}[|\langle u _{4},X\rangle|^{8s}|\langle u_{5},X\rangle|^{8t}]^{1/8}\] (I.149) \[\leq\operatorname*{\mathbb{E}}_{X\sim\sqrt{\mathbb{S}^{d-1}}}[ \langle u_{1},X\rangle|^{2p}]^{1/2}\operatorname*{\mathbb{E}}_{X\sim\sqrt{ \mathbb{S}^{d-1}}}[|\langle u_{2},X\rangle|^{4q}]^{1/4}\operatorname*{\mathbb{E }}_{X\sim\sqrt{\mathbb{S}^{d-1}}}[|\langle u_{3},X\rangle|^{8r}]^{1/8}\] (I.150) \[\operatorname*{\mathbb{E}}_{X\sim\sqrt{\mathbb{S}^{d-1}}}[|\langle u _{4},X\rangle|^{16s}]^{1/16}\operatorname*{\mathbb{E}}_{X\sim\sqrt{\mathbb{S} ^{d-1}}}[|\langle u_{5},X\rangle|^{16t}]^{1/16}\] (I.151)

where each line results from a single application of the Cauchy-Schwarz inequality. Thus, by Lemma I.10, we have

\[\operatorname*{\mathbb{E}}_{X\sim\sqrt{\mathbb{S}^{d-1}}}[|\langle u_{1},X \rangle|^{p}|\langle u_{2},X\rangle|^{q}|\langle u_{3},X\rangle|^{r}|\langle u _{4},X\rangle|^{s}|\langle u_{5},X\rangle|^{t}]\leq C_{p,q,r,s,t}\,.\] (I.152)

Dividing both sides by \(d^{(p+q+r+s+t)/2}\), we obtain

\[\operatorname*{\mathbb{E}}_{x\sim\mathbb{S}^{d-1}}[|\langle u_{1},x\rangle|^{ p}|\langle u_{2},x\rangle|^{q}|\langle u_{3},x\rangle|^{r}|\langle u_{4},x \rangle|^{s}|\langle u_{5},x\rangle|^{t}]\leq C_{p,q,r,s,t}\frac{1}{d^{(p+q+r+ s+t)/2}}\] (I.153)

as desired.