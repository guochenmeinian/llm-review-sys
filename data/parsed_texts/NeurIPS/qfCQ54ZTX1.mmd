# Entity Alignment with Noisy Annotations from Large Language Models

 Shengyuan Chen

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

shengyuan.chen@connect.polyu.hk

&Qinggang Zhang

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

qinggangg.zhang@connect.polyu.hk

&Junnan Dong

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

hanson.dong@connect.polyu.hk

&Wen Hua

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

wency.hua@polyu.edu.hk

&Qing Li

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

csqli@comp.polyu.edu.hk

&Xiao Huang

Department of Computing

The Hong Kong Polytechnic University

Hung Hom, Hong Kong SAR

xiaohuang@comp.polyu.edu.hk

###### Abstract

Entity alignment (EA) aims to merge two knowledge graphs (KGs) by identifying equivalent entity pairs. While existing methods heavily rely on human-generated labels, it is prohibitively expensive to incorporate cross-domain experts for annotation in real-world scenarios. The advent of Large Language Models (LLMs) presents new avenues for automating EA with annotations, inspired by their comprehensive capability to process semantic information. However, it is nontrivial to directly apply LLMs for EA since the annotation space in real-world KGs is large. LLMs could also generate noisy labels that may mislead the alignment. To this end, we propose a unified framework, LLM4EA, to effectively leverage LLMs for EA. Specifically, we design a novel active learning policy to significantly reduce the annotation space by prioritizing the most valuable entities based on the entire inter-KG and intra-KG structure. Moreover, we introduce an unsupervised label refiner to continuously enhance label accuracy through in-depth probabilistic reasoning. We iteratively optimize the policy based on the feedback from a base EA model. Extensive experiments demonstrate the advantages of LLM4EA on four benchmark datasets in terms of effectiveness, robustness, and efficiency.

## 1 Introduction

Knowledge graphs (KGs) serve as a foundational structure for storing and organizing structured knowledge about entities and their relationships, which facilitates effective and efficient search capabilities across various applications. They have been widely applied in question-answering systems (Dong et al., 2023, 2024c), recommendation systems (Catherine and Cohen, 2016; Chen et al., 2024a), social network analysis (Tang et al., 2008), Natural Language Processing (Weikum and Theobald, 2010), etc. Despite their extensive utility, real-world KGs often suffer from issuessuch as incompleteness, domain specificity, or language constraints, which limit their effectiveness in cross-disciplinary or multilingual contexts. To address these challenges, entity alignment (EA) aims to merge disparate KGs into a unified, comprehensive knowledge base by identifying and linking equivalent entities across different KGs. For instance, by aligning entities between a financial KG and a legal KG, EA facilitates the understanding of complex relationships, such as identifying the same corporations across the two KGs to assess how legal regulations impact their financial performance. This alignment enables a more nuanced exploration and interrogation of interconnected data, providing richer insights into how entities operate across multiple domains.

Entity alignment models predict the equivalence of two entities by measuring their alignment probability. Specifically, rule-based methods (Suchanek et al., 2012; Jimenez-Ruiz and Cuenca Grau, 2011; Qi et al., 2021) utilize predefined rules or heuristics to update alignment probabilities and propagate alignment labels. Conversely, embedding-based models seek to exploit advanced techniques in graph learning (Li et al., 2024; Liu et al., 2024, 2024, 2023), parameterizing these probabilities using similarity scores between entity representations learned through knowledge graph embedding algorithms such as translation models (Chen et al., 2017; Sun et al., 2018) or Graph Convolutional Networks (GCNs) (Wu et al., 2019; Mao et al., 2021; Wang et al., 2018; Huang et al., 2023). However, these methods heavily rely on extensive and accurate seed alignments for training--a requirement that poses significant challenges. The need for substantial, cross-domain knowledge to annotate such alignments often makes their acquisition prohibitively expensive.

Recently, Large Language Models (LLMs) have showcased their superior capability in processing semantic information Dong et al. (2024), which has significantly advanced various graph learning tasks such as node classification (Chen et al., 2024), graph reasoning (Zhao et al., 2023; Chai et al., 2023), recommender systems (Zhou et al., 2022; Wu et al., 2023), SQL query generation (Zhang et al., 2024), and knowledge graph-based question answering (Wang et al., 2024; Zhang et al., 2024; Dong et al., 2024). Their capacity to extract meaningful insights from graph data opens up new possibilities for automating EA. Notably, recent studies (Zhong et al., 2022; Zhao et al., 2023; Jiang et al., 2024) have explored the use of LLMs in EA, primarily focusing on finetuning a pretrained LLM such as Bert to learn semantic-aware representations, relying on accurate seed alignments as training labels. Yet, the potential of LLMs for label-free EA via in-context learning remains unexplored.

However, directly applying LLMs to automate EA poses significant challenges. Firstly, conventional EA models presume that all annotations are correct; yet, LLMs can generate false labels due to LLMs' inherent randomness and the potential incompleteness or ambiguity in the semantic information of entities. Training an EA model directly on these noisy labels can severely impair the final alignment performance. Secondly, given the vast number of entity pairs, annotation with LLMs would be prohibitively expensive. Maximizing the utility of a limited LLM query budget is essential. Existing solutions such as active learning cannot be directly applied since the annotations are noisy.

In response to the outlined challenges, we introduce LLM4EA, a unified framework designed to effectively learn from noisy pseudo-labels generated by LLMs while dynamically optimizing the utility of a constrained query budget. LLM4EA actively selects source entities based on feedback from a base EA model, focusing on those that significantly reduce uncertainty for both the entities themselves and their neighbors. This approach allocates the query budget to important entities, guided by the intra-KG and inter-KG structure. To manage the noisy pseudo-labels effectively, LLM4EA incorporates an unsupervised label refiner that enhances label accuracy by selecting a subset of confident pseudo-labels through probabilistic reasoning. These refined labels are then utilized to train the base EA model for entity alignment. The confident alignment results inferred by the EA model inform active selection in subsequent iterations, thereby progressively improving the framework's effectiveness in a coherent and integrated manner. Contributions are summarized as follows:

* **Novel LLM-based framework for entity alignment:** We propose LLM4EA, an in-context learning framework that uses an LLM to annotate entity pairs. Leveraging the LLMs' zero-shot learning capability, this framework generates pseudo-labels, providing a foundation for entity alignment without ground truth labels.
* **Unsupervised label refinement:** Our framework introduces an unsupervised label refiner informed by probabilistic reasoning. This component significantly improves the accuracy of LLM-derived pseudo-labels, enabling effective training of entity alignment models.
* **Active sampling module:** We propose an active selection algorithm that dynamically searches entities in the huge annotation space. Guided by feedback from the EA model,this algorithm adjusts its policy based on inter-KG and intra-KG structures, optimizing the utility of LLM queries and ensuring efficient use of resources.
* **Empirical validation and superior performance:** We rigorously evaluate our framework through extensive experiments and ablation studies. The results demonstrate that LLM4EA not only outperforms baselines by a large margin but also shows robustness and efficiency. Each component of the framework is shown to contribute meaningfully to the overall performance, with clear synergistic effects observed among the components.

## 2 Problem definition

A knowledge graph \(\mathcal{G}\) comprises a set of entities \(\mathcal{E}\), a set of relations \(\mathcal{R}\), and a set of relation triples \(\mathcal{T}\) where each triple \((e_{h},r,e_{t})\in\mathcal{T}\) represents a directional relationship between its head entity and tail entity. Given two KGs \(\mathcal{G}=\{\mathcal{E},\mathcal{R},\mathcal{T}\}\), \(\mathcal{G}^{\prime}=\{\mathcal{E}^{\prime},\mathcal{R}^{\prime},\mathcal{T}^ {\prime}\}\) and a fixed query budget \(\mathcal{B}\) to a Large Language Model, we aim to train an entity alignment model \(\theta\) based on the LLM's annotations to infer the matching score \(m_{\theta}(e,e^{\prime})\) for all entity pairs \(\{(e,e^{\prime}),e\in\mathcal{E},e^{\prime}\in\mathcal{E}^{\prime}\}\). The evaluation process utilizes a ground truth alignment set \(\mathcal{A}\) to assess the prediction accuracy for target entities in both directions, i.e.,\((e,?)\) and \((?,e^{\prime})\) for each true pair \((e,e^{\prime})\in\mathcal{A}\), based on the ranked matching scores \(m_{\theta}\). Evaluation metrics are hit@k (where \(k\in\{1,10\}\)) and mean reciprocal rank (MRR).

## 3 Entity alignment with noisy annotations from LLMs

We aim to design a framework to perform entity alignment with LLMs. Our design is motivated by the following insights. Firstly, we have a huge search space (the overall annotation space is \(O(|\mathcal{E}||\mathcal{E}^{\prime}|)\)) to identify the core entity pairs to annotate. Secondly, we don't know whether the annotated labels are correct or not, because we have no prior knowledge or heuristic of the label distribution. Finally, we perform annotations iteratively, requiring the model to adjust its search policy based on annotation effectiveness, while we have no verifiable feedback of this annotation accuracy.

Based on these insights, we propose LLM4EA--an iterative framework that consists of four interconnected steps in each cycle, as illustrated in Figure 1. Initially, **an active selection** technique optimizes the use of resources by choosing critical source entities that significantly reduce uncertainty for themselves and their neighbors. Subsequently, **an LLM-based annotator** identifies the counterparts for the selected source entities, generating a set of pseudo-labels. Next, **a label refiner** improves label accuracy by eliminating structurally incompatible labels. This process involves formulating a combinatorial optimization problem and utilizing a probabilistic-reasoning-based greedy search algorithm to efficiently find a local-optimal solution. Finally, these refined labels are used to train **a base EA model** for the entity alignment task. The outcomes of the alignment then serve as feedback to inform subsequent rounds of the active selection policy. Further details are provided below.

### Active selection of source entity

We aim to maximize the utility of the budget by actively allocating the budget to those beneficial entities. To do this, we sample source entities that reduce the most uncertainty of both themselves and their neighboring entities, by a dynamically adjusted policy. The measurement of uncertainty

Figure 1: Overview of the LLM4EA framework. LLM4EA utilizes active sampling to select important entities based on feedback from an EA model. It also includes a label refiner to effectively train the base EA model using noisy pseudo-labels. Feedback from the EA model updates the selection policy.

reduction is based on two assumptions: 1) an entity's own uncertainty is inversely proportional to its alignment probability with its most probable counterpart; 2) the amount of uncertainty an entity eliminates for its neighbors is closely linked to the relational ties between them. To systematically assess this, we introduce the concept of _relational uncertainty_, quantified as follows:

\[U_{r}(e_{h})=(1-P(e_{h}))+\sum_{(e_{h},r,e_{t})\in\mathcal{T}}w_{r}\left(1-P(e_{t })\right).\] (1)

Here, \(w_{r}\) is a weight coefficient reflecting the significance of relation \(r\) and signifies how much \(e_{h}\) contributes to reducing the uncertainty of \(e_{t}\) through the relation \(r\). For this purpose, we employ functionality \(\mathcal{F}(r)\) (formally defined in Eq. (4)) as the weight \(w_{r}\), as it quantifies the uniqueness of the tail entity for a given specified head entity. \(P(e)\coloneqq\max_{e^{\prime}\in\mathcal{E}^{\prime}}P(e\equiv e^{\prime})\) represents the alignment probability of the top-match entity for \(e\). These alignment probabilities \(P(e\equiv e^{\prime})\) are obtained through probabilistic reasoning during label refinement (Section 3.3.2) and are augmented by the inferred alignments from the base EA model (Section 3.4). In the initial iteration, all alignment probabilities are set to 0.

It's important to note that some source entities are linked to a large number of uncertain neighbors (those with low \(P(e)\)). These source entities are crucial but may be overlooked if their connected relations have low functionality. Hence, we introduce _neighbor uncertainty_ as another metric to assess an entity's importance, by removing the functionality-based weight coefficient:

\[U_{n}(e_{h})=(1-P(e_{h}))+\sum_{(e_{h},r,e_{t})\in\mathcal{T}}\left(1-P(e_{t}) \right).\] (2)

To integrate these two metrics, we employ rank aggregation by mean reciprocal rank:

\[U(e_{h})=2\times\left(\frac{1}{r_{ur}(e_{h})}+\frac{1}{r_{un}(e_{h})}\right).\] (3)

Here, \(r_{ur}(e_{h})\) and \(r_{un}(e_{h})\) denote the ranking of \(e_{h}\) when using \(U_{r}\) and \(U_{n}\) as metric, respectively. This simple-effective aggregation technique is advantageous for our task since it's scale invariant and requires no validation set for tuning hyperparameters, making it more practical in this task.

### LLM as annotator

**Counterpart filtering.** With the selected source entities, we employ an LLM as an annotator to identify the counterpart from \(\mathcal{E}^{\prime}\) for each source entity, generating a set of pseudo-labels \(\mathcal{L}=\{(e,e^{\prime})|e\in\mathcal{E},e^{\prime}\in\mathcal{E}^{ \prime}\}\). To narrow down the search space, we first filter out the less likely counterparts before querying the LLM, selecting only the top-\(k\) most similar counterparts from \(\mathcal{E}^{\prime}\). The similarity metric is flexible: we use a string matching score based on word edit distance, but other methods are also viable, such as semantic embedding distances derived from word embedding models. By adjusting \(k\), we can trade-off between the recall rate of counterparts and the query cost.

**Prompt design.** There are primarily two methods for retrieving context information to construct textual prompts: randomly generated prompts and dynamically tuned prompts. The former involves randomly selecting neighbors to construct contexts for the entity, while the latter dynamically selects neighbors based on feedback from the EA model. For a fair comparison, we use randomly generated prompts across all baselines and the proposed LLM4EA. These prompts include the name of each entity and a set of relation triples to three randomly selected neighbors. For the baseline models, pseudo-labels are generated at once and used for training. For LLM4EA, we evenly divide the budget \(\mathcal{B}\) into \(n\) iterations and generate pseudo-labels at each iteration using the allocated \(\mathcal{B}/n\) budget.

### Probabilistic reasoning for label refinement

The pseudo-labels generated by the LLM can be noisy, and directly using these labels to train an entity alignment (EA) model could undermine the final performance. Although estimating the label distribution by asking the LLM for confidence scores or querying multiple times to measure consistency are potential solutions, these approaches can be vulnerable or introduce additional costs.

In light of this, we propose a label refiner that leverages the structure of knowledge graphs. The refinement process is framed as a combinatorial optimization problem aimed at minimizing overall structural incompatibility among labels. Utilizing a probabilistic reasoning technique, we progressively update our confidence estimation for each label and select those that are mutually compatible, ultimately producing a set of accurate pseudo-labels. Detailed explanations follow below.

#### 3.3.1 Functionality and probabilistic reasoning

**Functionality.** The functionality of a relation quantifies the uniqueness of tail entities for a specified head entity, calculated as the ratio of unique head entities to total head-tail pairs linked by the relation. Conversely, inverse functionality quantifies the tail entity uniqueness for a specified head entity. Formally, these are defined as:

\[\mathcal{F}(r)\coloneqq\frac{|\{e_{h}|(e_{h},r,e_{t})\in\mathcal{T}\}}{|\{(e _{h},e_{t})|(e_{h},r,e_{t})\in\mathcal{T})\}|},\quad\mathcal{F}^{-1}(r) \coloneqq\frac{|\{e_{t}|(e_{h},r,e_{t})\in\mathcal{T}\}}{|\{(e_{h},e_{t})|(e_{h },r,e_{t})\in\mathcal{T})\}|}.\] (4)

For instance, suppose a KG contains two triples for the relation \(locate\_in\): \((Hawaii,locate\_in,US)\) and \((Miami,locate\_in,US)\). Then \(\mathcal{F}(locate\_in)=1.0\) and \(\mathcal{F}^{-1}(locate\_in)=0.5\). In other words, given \((Miami,locate\_in,?)\), the answer for the missing tail entity is unique; while given \((?,locate\_in,US)\), there are multiple answers for the missing head entity. Such relational patterns are useful for identifying an entity based on its connections within the intra-graph structure.

**Probabilistic reasoning.** If two entities are each connected to entities that are aligned across KGs, this increases the likelihood that they should be aligned as well. Based on this heuristic, an entity pair's alignment probability \(P(e_{h}\equiv e^{\prime}_{h})\) can be inferred by aggregating its neighbors' alignment probability via relation functionality:

\[1-\prod_{\begin{subarray}{c}(e_{h},r,e_{t})\in\mathcal{T},\\ (e_{h},r^{\prime},e^{\prime}_{t})\in\mathcal{T}^{\prime}\end{subarray}}\left(1 -\mathcal{F}^{-1}(r)P(r\subseteq r^{\prime})P(e_{t}\equiv e^{\prime}_{t})\right) \times\left(1-\mathcal{F}^{-1}(r^{\prime})P(r^{\prime}\subseteq r)P(e_{t} \equiv e^{\prime}_{t})\right).\] (5)

Here, \(P(r\subseteq r^{\prime})\) denotes the probability of \(r\) being a subrelation of \(r^{\prime}\), estimated by alignment probabilities of connected entities:

\[\frac{\sum\left(1-\prod_{(e^{\prime}_{h},r^{\prime},e^{\prime}_{t})\in \mathcal{T}^{\prime}}\left(1-P(e^{\prime}_{h}\equiv e_{h})P(e^{\prime}_{t} \equiv e_{t})\right)\right)}{\sum\left(1-\prod_{e^{\prime}_{h},e^{\prime}_{t} \in\mathcal{E}^{\prime}}\left(1-P(e^{\prime}_{h}\equiv e_{h})P(e^{\prime}_{t} \equiv e_{t})\right)\right)}.\] (6)

These formulations allow for the propagation and updating of alignment probabilities in a manner that is cognizant of relational structures. We employ this technique to design a label refiner below.

#### 3.3.2 Label refiner

**Label incompatibility.** We exploit the "incompatibility" of labels for label refinement, based on the assumption that correct labels can infer each other, while a false label could be incompatible with its correctly aligned neighbors. We define the _overall incompatibility_ on a label set \(\mathcal{L}\) as:

\[\Phi(\mathcal{L})\coloneqq\sum_{(e_{h},e^{\prime}_{h})\in\mathcal{L}}\left( \mathbf{1}_{P(e_{h}\equiv e^{\prime}_{h})<\max_{e\in\mathcal{E}}P(e,e^{\prime} _{h})}+\mathbf{1}_{P(e_{h}\equiv e^{\prime}_{h})<\max_{e^{\prime}\in\mathcal{ E}^{\prime}}P(e_{h},e^{\prime})}\right).\] (7)

Here, \(\mathbf{1}_{P(e_{h}\equiv e^{\prime}_{h})<\max_{e\in\mathcal{E}}P(e,e^{\prime} _{h})}=1\) if \(e_{h}\) is not the top-match for \(e^{\prime}_{h}\), otherwise \(0\). It's important to note that a detected incompatibility doesn't necessarily indicate the false alignment of \((e_{h},e^{\prime}_{h})\): it may suggest a misalignment of their neighbors. Given this, the key to label refinement is to jointly optimize the label's overall incompatibility while avoiding accidentally filtering out correct labels.

**Objective.** To enhance label quality, we propose to refine the pseudo-label set \(\mathcal{L}\) by finding a subset \(\mathcal{L}^{*}\subset\mathcal{L}\) that minimizes its overall incompatibility: \(\mathcal{L}^{*}=\operatorname*{argmin}_{\mathcal{E}^{\prime}\subset\mathcal{ L}}\Phi(\mathcal{L}^{\prime})\). Noteworthy that a trivial solution for this optimization problem is only preserving a set of isolated labels, such that \(\max_{e\in\mathcal{E}}P(e\equiv e^{\prime}_{h})=0\) and \(\max_{e^{\prime}\in\mathcal{E}^{\prime}}P(e_{h}\equiv e^{\prime})=0\) for all \((e_{h},e^{\prime}_{h})\in\mathcal{L}^{\prime}\). This trivial solution would lead to the exclusion of most accurate labels, an outcome we aim to avoid. Considering this, we introduce an \(1\) penalty term to penalize the removal of labels, leading to our overall objective:

\[\mathcal{L}^{*}=\operatorname*{argmin}_{\mathcal{L}^{\prime}\subset\mathcal{L} }\left(\Phi(\mathcal{L}^{\prime})+\lambda|\mathcal{L}-\mathcal{L}^{\prime}| \right).\] (8)Here \(\lambda>0\) is a weight coefficient. Solving the above combinatorial problem is intractable as it requires computing \(\Phi(\mathcal{L}^{\prime})\) for each possible set \(\mathcal{L}^{\prime}\subset\mathcal{L}\), which is NP-hard. Below we propose to search for a local-optimal solution by a greedy algorithm powered by probabilistic reasoning.

**Greedy search.** The algorithm begins by initializing the alignment probability \(P(e\equiv e^{\prime})=\delta_{0}\) for every pair \((e,e^{\prime})\) within the set \(\mathcal{L}\), where \(\delta_{0}\) is a constant within the range (0,1). It then iteratively performs a search for an optimal label set \(\mathcal{L}^{\prime}\) through a series of voting steps. Each iteration is comprised of two main steps: probabilistic reasoning and label adjustment.

During the probabilistic reasoning step, the alignment probabilities and subrelation probabilities are updated according to Eq. (5) and Eq. (6), respectively. This update process refines our estimates of label confidence based on the latest information. During the label adjustment step, the label set \(\mathcal{L}^{\prime}\) is updated based on these updated probabilities. Labels are appended to \(\mathcal{L}^{\prime}\) if their updated alignment probabilities exceed \(\delta_{0}\), indicative of high confidence in their alignment, supported by their neighbors:

\[\mathcal{L}^{\prime}\leftarrow\mathcal{L}^{\prime}\cup\{(e_{h},e^{\prime}_{h} )\in\mathcal{L}|P(e_{h}\equiv e^{\prime}_{h})>\delta_{0}\}\,.\] (9)

Conversely, labels demonstrating structural incompatibilities are excluded from \(\mathcal{L}^{\prime}\):

\[\mathcal{L}^{\prime}\leftarrow\mathcal{L}^{\prime}\setminus\left\{(e_{h},e^{ \prime}_{h})\in\mathcal{L}\mid P(e_{h}\equiv e^{\prime}_{h})<\max\left(\max_{e \in\mathcal{E}}P(e,e^{\prime}_{h}),\max_{e^{\prime}\in\mathcal{E}^{\prime}}P( e_{h},e^{\prime})\right)\right\}.\] (10)

In this manner, labels are removed if they are incompatible with updated aligned neighbors, ensuring the preservation of only the most confident pairs within \(\mathcal{L}^{\prime}\). To further refine the search process in subsequent iterations, we augment all entity alignment probabilities within \(\mathcal{L}^{\prime}\) to a superior score:

\[P(e\equiv e^{\prime})\leftarrow\max\left(P(e\equiv e^{\prime}),\delta_{1} \right)\quad\text{ for each }(e,e^{\prime})\in\mathcal{L}^{\prime}.\] (11)

Here \(\delta_{1}\in(\delta_{0},1)\) serves as a new threshold, elevating the alignment probabilities of confident pairs to foster a more directed and effective search. After \(n_{lr}\) iterations, we get a set of confidently selected labels \(\mathcal{L}^{\star}\) that have high compatibility. The detailed algorithm is presented in Appendix A.2, and analyses of parameter efficiency and computational efficiency are provided in Appendix A.3.

### Entity alignment

With the refined labels, we train an embedding-based EA model to learn structure-aware representations for each entity. After training, the EA model computes a matching score \(m_{\theta}(e,e^{\prime})\) for each entity pair \((e,e^{\prime})\) for evaluation. The selection of the base EA model is flexible, tailored to the task requirements. We chose a recently proposed GCN-based model, Dual-AMN (Mao et al., 2021), for its effectiveness and efficiency.

Feedback from the base EA model is crucial for dynamic update of the active selection policy. To generate effective feedback, we infer high-confidence pairs \((e,e^{\prime})\) with the trained EA model, by selecting the pairs that both entities rank top for each other. These pairs are injected into the probabilistic reasoning system. Similar to the label refinement process, this system initializes with an alignment probability of \(\delta_{0}\) for these pairs and updates the estimation of alignment and subrelation probabilities using Eq. (5) and Eq. (6). The updated probabilities are used to construct the uncertainty terms (i.e., \(U_{r}\) and \(U_{n}\)) to inform the active selection policy in subsequent iterations, thereby optimizing the budget utility and improving final performance continuously.

## 4 Experiments

In this section, we conduct experiments to evaluate the effectiveness of our framework. We begin by introducing the experimental settings. Then, we present experiments to answer the following research questions: **RQ1.** How effective is the overall framework? **RQ2.** What is the impact of the choice of LLM on the cost and performance of LLM4EA? **RQ3.** What is the effect of the label refiner? **RQ4.** What is the impact of active selection?

### Experimental setting

**Datasets and LLM.** In this study, we use the widely-adopted OpenEA dataset (Sun et al., 2020), including two monolingual datasets (D-W-15K and D-Y-15K) and two cross-lingual datasets (EN-DE-15K and EN-FR-15K). OpenEA comes in two versions: "V1" the normal version, and "V2"the dense version. We employ "V2" in the experiments in the main text. The LLM version in this experiment is GPT-3.5 (gpt-3.5-turbo-0125) and GPT-4 (gpt-4-turbo-2024-04-09). By default, the overall query budget is \(\mathcal{B}=0.1|\mathcal{E}|\).

**Baselines.** Baseline models include three GCN-based models -- GCNAlign (Wang et al., 2018), RDGCN (Wu et al., 2019), Dual-AMN (Mao et al., 2021), and three translation-based models -- IMUSE (He et al., 2019), AlignE, BootEA (Sun et al., 2018), Here, BootEA is a variant of AlignE that adopts a bootstrapping strategy, equipped with a label calibration component for improving the accuracy of bootstrapped labels. Baseline models are directly trained on the pseudo-labels generated by the LLM annotator, without label refinement or active selection. Every experiment is repeated three times to report statistics.

**Setup of LLM4EA.** We employ GPT-3.5 as the default LLM due to its cost efficiency. Other parameters are \(n=3\), \(n_{lr}\), \(k=20\), \(\delta_{0}=0.5\), \(\delta_{1}=0.9\).

### Results

#### 4.2.1 Comprehensive evaluation of entity alignment performance

To answer **RQ1** and **RQ2**, we conducted two groups of experiments on OpenEA datasets, using GPT-3.5 and GPT-4 as the annotator, respectively. Results are presented in Table 1. We also investigated the performance-cost comparison between the GPT-3.5 annotator and the GPT-4 annotator, illustrated in Figure 2. To control the randomness introduced by the LLMs, each experiment was repeated three times to report mean and standard deviation. These results lead to several key observations:

**First, LLM4EA surpasses all baseline EA models, which are directly trained on the pseudo-labels, by a large margin.** This can be attributed to 1) our label refiner's capability in filtering out false labels, reducing noise during training and enabling more accurate optimization towards the ground true objective; 2) our active selection component's ability to smartly identify important entities to annotate, which takes full advantage of the fixed query budget.

**Second, using the GPT-4 results in higher performance than using the GPT-3.5 as the annotator.** This observation conforms to the fact that GPT-4 is a more advanced LLM with higher reasoning capacity and stronger semantic analysis, resulting in more precise annotation results and higher recall, thus providing more labels of high quality. We also observe that translation-based models (e.g., AlignE) are sensitive to noisy labels under GPT-3.5, while state-of-the-art GCN-based models (e.g., RDGCN and Dual-AMN) are more robust. BootEA also demonstrates superior performance and robustness, attributed to its bootstrapping technique and enhanced by its capability in calibrating bootstrapped labels. However, its label calibration is only applied to the bootstrapped labels, so it still suffers from the false training labels. Our proposed LLM4EA, on the other hand, refines the label accuracy before training the EA model, thus ensuring more accurate training.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**EN-FR-15K**} & \multicolumn{3}{c}{**EN-DE-15K**} & \multicolumn{3}{c}{**D-W-15K**} & \multicolumn{3}{c}{**D-Y-15K**} \\  & Hit@1 & Hit@10 & MRR & Hit@1 & Hit@10 & MRR & Hit@10 & MRR & Hit@1 & Hit@10 & MRR \\ \hline \multicolumn{10}{c}{**Group: Entity Alignment with GPT-3.5**} \\ \hline IMUSE & 50.04.0 & 72.640.8 & 57.540.4 & 51.644.7 & 75.943.9 & 60.548.5 & 60.02.1 & 41.642.5 & 9.01.0 & 54.42.2 & 78.941.1 & 63.22.2 \\ AlignE & 6.640.3 & 24.540.5 & 12.640.5 & 6.240.3 & 18.410.1 & 10.400.5 & 8.0 & 9.2402.7 & 13.331.4 & 50.182.0 & 76.641.4 & 59.241.8 \\ BootEA & 44.81.1 & 71.941.2 & 54.21.2 & 68.140.2 & 58.440.3 & 74.33.0 & 62.048.0 & 79.340.1 & 67.440.2 & 87.880.1 & 96.740.1 & 91.240.1 \\ GCNAlign & 17.440.3 & 14.324.0 & 25.940.3 & 22.240.2 & 46.241.1 & 30.30.3 & 16.940.1 & 39.340.3 & 23.430.1 & 45.340.8 & 63.330.5 & 53.340.5 \\ RDGCN & 69.340.3 & 82.580.3 & 74.340.3 & 73.340.4 & 84.642.6 & 74.373.9 & 72.940.7 & 87.940.5 & 83.240.6 & 86.237.1 & 94.191.3 & 86.122.7 \\ Dual-AMN & 51.94.3 & 79.640.9 & 61.640.5 & 70.540.7 & 79.110.3 & 78.990.6 & 62.040.1 & 86.840.1 & 71.940.1 & 85.80.3 & 98.490.0 & 91.440.1 \\ LLM4EA & **74.240.3** & **92.940.4** & **81.040.3** & **89.120.5** & **97.840.1** & **92.640.3** & **87.440.3** & **97.401.0** & **99.402.0** & **97.780.0** & **99.540.0** & **98.320.0** \\ \hline \multicolumn{10}{c}{**Group: Entity Alignment with GPT-4.1**} \\ \hline IMUSE & 52.740.9 & 74.941.0 & 59.840.9 & 59.642.6 & 81.841.5 & 57.942.1 & 21.646.1 & 50.040.10 & 31.147.4 & 86.640.5 & 94.240.1 & 89.240.4 \\ AlignE & 30.824.2 & 69.142.5 & 43.12.5 & 46.445.2 & 76.543.8 & 56.648.8 & 36.143.7 & 67.843.6 & 46.743.7 & 86.440.9 & 97.040.3 & 90.240.6 \\ BooFiE & 58.240.3 & 83.740.3 & 67.040.3 & 80.540.4 & 92.620.4 & 84.80.3 & 71.640.2 & 88.340.2 & 77.640.2 & 95.050.01 & 98.640.0 & 96.340.1 \\ GCNAlign & 30.640.0 & 65.340.3 & 42.120.2 & 41.940.4 & 68.60.5 & 51.514.0 & 33.410.3 & 61.640.1 & 41.440.2 & 82.640.2 & 9.4940.2 & 87.240.1 \\ RDGCN & 72.140.2 & 84.540.1 & 76.740.2 & 74.141.1 & 85.140.7 & 78.010.0 & 82.541.1 & 91.440.7 & 85.941.0 & 85.440.9 & 93.240.4 & 88.340.8 \\ Dual-AMN & 76.740.1 & 94.940.3 & 83.640.2 & 90.780.1 & 97.940.2 & 93.680.1 & 81.540.1 & 94.940.2 & 86.740.1 & 97.540.0 & 99.340.1 & 98.18.0 \\ LLM4EA & **80.240.3** & **96.040.2** & **86.040.2** & **93.140.5** & **97.840.2** & **95.130.3** & **89.840.3** & **97.940.2** & **92.940.3** & **97.940.1** & **99.960.0** & **95.540.1** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Evaluation of entity alignment performance, measured by Hit@K for \(K\in\{1,10\}\), and Mean Reciprocal Rank (MRR), presented in %. Experiment statistics are computed over three trials.

**Finally, LLM4EA is noise adaptive, enabling cost-efficient entity alignment.** To further investigate the effect of the choice of LLM, we examined the performance-cost comparison between GPT-3.5 and GPT-4 as the annotator. We illustrate MRR in Figure 2 (detailed results are available in Appendix B.3). The results show that, by increasing the query budgets (measured by the number of tokens) for GPT-3.5, the performance gradually increases. When the budget is \(2\times\) that of GPT-4, the performance is comparable to or exceeds the performance of using GPT-4 as the annotator. According to the pricing scheme of OpenAI, the input/output cost for 1 million tokens for GPT-3.5 and GPT-4 is \(80.50\)/\(\$1.5\) and \(\$10\)/\(\$30\), respectively. This means that our noise-adaptive framework enables cost-efficient entity alignment with less advanced LLMs at \(10\times\) less actual cost than using more advanced LLMs, simply by increasing the token budget for the less advanced LLMs.

#### 4.2.2 Effect of the label refiner

To answer **RQ3**, we first analyze the evolution of the True Positive Rate (TPR) and the recall rate of the refined labels. Specifically, at each label refinement iteration, the TPR is calculated as \(\frac{|\mathcal{A}\cap\mathcal{L}^{\prime}|}{|\mathcal{L}^{\prime}|}\), and the recall is calculated as \(\frac{|\mathcal{A}\cap\mathcal{L}^{\prime}|}{|\mathcal{A}\cap\mathcal{L}|}\). The left and middle subfigures of Figure 3 demonstrate how **our label refiner progressively discourse accurate labels and optimizes the TPR.** Initially, the TPR of the refined label set is high (approximately 1.0), then it decreases by a certain percentage, and eventually increases again to a high TPR. We attribute this pattern to: 1) the most confident labels being discovered in the earliest iterations, which are obvious alignments with many connected alignments; 2) as the algorithm progresses, some false pseudo-labels being erroneously added to the label set \(\mathcal{L}^{\prime}\); 3) as the label refinement continues, \(\mathcal{L}^{\prime}\) is adjusted and the false pseudo-labels are replaced with the correct labels inferred by the updated probability as in Eq. (10).

Furthermore, we assess the robustness of our label refiner, as depicted in the right subfigure of Figure 3. We synthesize noisy labels and evaluate the output TPR in relation to varying input TPR levels, using two experimental schemes: _fixed budget_, where the budget remains constant at \(0.1|\mathcal{E}|\) while the TPR changes, and _fixed TP_, where the number of true positives is fixed but the TPR and corresponding budgets are adjusted. The results demonstrate that **the label refiner consistently elevates the TPR to over \(0.9\), even when the initial TPR is around \(0.5\), showcasing its high

Figure 3: Analysis of the Label Refinement. We illustrate the evolution of the true positive rate (TPR) (left) and recall (middle) for refined labels across four datasets. Furthermore, we assess the robustness of the label refinement process by examining the TPR of refined labels against varying initial TPRs within the D-W-15K dataset (right), with initial pseudo-labels synthesized at different TPR levels.

Figure 2: Performance-cost comparison between GPT-3.5 and GPT-4 as the annotator, evaluated by MRR. We increase the budget for GPT-3.5 to evaluate its performance. [\(n\times\)] denotes using \(n\times\) of the default query budget. Each experiment is repeated three times to show mean and standard deviation.

robustness to noisy pseudo-labels.** This result also reveals why our framework demonstrates robust performance with the less advanced GPT-3.5 annotator.

#### 4.2.3 Ablation study

Ablation studies detailed in Table 2 answer **RQ4** and reveal several key insights: **1) Necessity of the Label Refiner for Effective Active Selection:** The performance of "w/o LR", which lacks a label refiner, is inferior not only to other model variants but also to the base model, Dual-AMN. This underscores that active selection depends crucially on reliable feedback, which is compromised when the label refiner is absent; **2) Contribution of Relation and Neighbor Uncertainty in Active Selection:** Incorporating both relation and neighbor uncertainties significantly enhances the utility of the budget. Methods like "Ours-degree" and "Ours-funcSum" focus only on their connections to neighbors and ignore the uncertainty of neighbors. In contrast, "Ours-ru" and "Ours-nu", which take these uncertainties into account, exhibit superior performance. This underscores the importance of considering neighbor uncertainty for effective active selection; **3) Robust Active Selection through Combined Metrics:** Our active selection approach integrates both relation uncertainty and neighbor uncertainty to enable robust active selection. By employing rank aggregation, it prioritizes entities that are deemed significant by both metrics, ensuring a more effective and nuanced selection process.

#### 4.2.4 Pareto frontier of runtime overhead against performance.

The runtime overhead is directly proportional to the number of active selection iterations, \(n\), since each iteration involves a subsequent label refinement process. To explore the runtime-performance trade-off in the LLM4EA approach, we examine the Pareto frontier of runtime versus performance. We conduct entity alignment experiments with a fixed query budget, varying the number of active selection iterations. The results of these experiments are illustrated in Figure 4.

The results indicate that performance initially increases as the number of iterations rises from 1 to around 3, but further increases beyond this point lead to a decline. This pattern can be attributed to two main factors: (1) More iterations allow for extensive learning from feedback during the active selection phase. (2) However, when iterations are excessively high, the number of generated pseudo-labels per iteration becomes small, leading to isolated pseudo-labels that undermine the label refinement process. These findings suggest that **an optimal balance between runtime efficiency and performance can be achieved without excessive trade-offs**, indicating a specific threshold for iterations beyond which no further performance gains are observed.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**EN-FR-15K**} & \multicolumn{3}{c}{**EN-DE-15K**} & \multicolumn{3}{c}{**D-W-15K**} & \multicolumn{3}{c}{**D-Y-15K**} \\  & Hit@1 & Hit@10 & MRR & Hit@1 & Hit@10 & MRR & Hit@1 & Hit@10 & MRR & Hit@1 & Hit@10 & MRR \\ \hline Ours & 74.24.03 & 92.940.4 & 81.040.3 & **89.140.5** & **97.840.1** & **97.840.1** & **97.266.03** & 87.540.3 & 96.740.1 & 90.980.2 & **97.70.09** & **95.540.0** & **98.340.0** \\ w/o LR & 51.64.10 & 80.240.7 & 61.940.8 & 74.441.7 & 94.240.6 & 82.61.61 & 39.214.4 & 75.770.7 & 52.912.4 & 85.310.9 & 99.240.1 & 91.540.5 \\ w/o Act & 68.12.12 & 88.441.7 & 75.442.0 & 78.440.8 & 93.940.8 & 84.60.6 & 82.840.7 & 92.540.8 & 86.340.6 & 97.540.1 & 99.240.3 & 98.140.1 \\ \hline Ours-ru & 70.82.01 & 91.240.8 & 78.208.3 & 89.340.3 & 97.720.2 & 89.610.2 & **88.740.6** & **97.440.3** & **93.210.6** & **97.70.09** & 99.40.1 & **98.40.0** \\ Ours-un & **74.540.70** & **93.140.8** & **82.406.8** & 88.620.2 & 97.40.3 & 88.40.3 & 85.180.5 & 95.240.5 & 88.905.7 & 97.640.1 & 99.400.0 & 95.240.0 \\ Ours-degree & 73.62.62 & 92.540.8 & 80.442.0 & 88.440.1 & 96.640.2 & 91.540.2 & 80.143.7 & 90.992.2 & 84.083.2 & 97.240.2 & 99.040.1 & 97.940.1 \\ Ours-funcSum & 59.540.6 & 78.840.6 & 66.340.6 & 81.240.5 & 96.040.3 & 87.140.0 & 83.940.9 & 93.141.1 & 87.341.0 & 97.540.1 & 99.440.1 & 98.140.1 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ablation study overview. The table presents the performance of the LLM4EA (Ours) with various modifications. **Group 1**: removing the label refiner (w/o LR) and the active selection component (w/o Act); **Group 2**: replacing the active selection technique with relational uncertainty (-ru), neighbor uncertainty (-nu), degree (-degree), and functionality sum (-funcSum).

Figure 4: Performance of entity alignment across four datasets with varying active sampling iterations, under a fixed query budget.

Related work

LLMs for Entity Alignment.Recent approaches have sought to leverage LLMs for entity alignment in knowledge graphs, primarily focusing on integrating structural and semantic information for improved alignment performance. BERT-INT (Tang et al., 2020) fine-tunes a pretrained BERT model to capture interactions and attribute information between entities. Similarly, SDEA (Zhong et al., 2022) employs a pretrained BERT to encode attribute data, while integrating neighbor information via a GRU to enhance structural representation. TEA (Zhao et al., 2023) reconceptualizes entity alignment as a bidirectional textual entailment task, utilizing pretrained language models to estimate entailment probabilities between unified textual sequences representing entities. A novel approach, ChatEA (Jiang et al., 2024), introduces a KG-code translation module that converts knowledge graph structures into a format comprehensible to LLMs, enabling these models to apply their extensive background knowledge to boost the accuracy of entity alignment. Notably, these models primarily focus on fine-tuning pretrained language models using a set of training labels and do not exploit the zero-shot capabilities of LLMs. In contrast, our proposed model, LLM4EA, leverages the zero-shot potential of LLMs, enabling it to generalize to new datasets without the need for labeled data.

**Probabilistic Reasoning.** In the literature on knowledge graph reasoning, probabilistic reasoning has been effectively applied to infer new soft labels and their associated probabilities from existing labels. It has been utilized in domains such as knowledge graph completion (Qu and Tang, 2019; Zhang et al., 2020; Fang et al., 2023; Chen et al., 2024) and entity alignment (Suchanek et al., 2012; Qi et al., 2021; Liu et al., 2022; Chen et al., 2024), where it naturally represents complex relational patterns with simple rules and performs precise inferences. In this work, however, due to the potential inaccuracies in the pseudo-labels generated by LLMs, the newly inferred alignments may be incorrect. Consequently, we opt not to employ probabilistic reasoning directly for the entity alignment task. Instead, we emphasize its use primarily for filtering false pseudo-labels that demonstrate structural incompatibilities within our framework. For completeness, we include the results of comparison with a probabilistic reasoning model - PARIS (Suchanek et al., 2012) in Appendix B.4.

## 6 Limitations

Firstly, during active selection, we distribute the query budget evenly for each selection rather than dynamically customizing the budget allocation for each iteration. This allocation approach could be improved by developing a more adaptive budget allocation strategy. Secondly, the framework currently does not provide direct support for temporal KGs. Although the probabilistic reasoning and active selection components inherently support entity alignment on evolving KGs, the base EA model is transductive. This necessitates retraining the model whenever new entities and relation triples are introduced into the KGs. However, this can be complemented by the research line of inductive entity alignment, such as path-based embedding models or logic-based models, which can generalize to previously unseen entities without the need for retraining.

## 7 Conclusions

In this paper, we address the challenge of automating entity alignment with Large Language Models (LLMs) under budget constraints and noisy annotations. We introduce LLM4EA, a framework that maximizes the utility of a fixed query budget using active sampling and mitigates erroneous labels with a label refiner employing probabilistic reasoning. Empirical results show that LLM4EA's noise-adaptive capabilities reduce costs without sacrificing performance, achieving comparable or superior results with less advanced LLMs at up to 10 times lower cost. This highlights the potential of LLM-based models in merging cross-domain and cross-lingual KGs. Future work will explore incorporating real-time learning capabilities to dynamically adjust to evolving knowledge bases.

## Acknowledgement

The work was fully supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. PolyU 15200023).

## References

* Catherine and Cohen (2016) Rose Catherine and William Cohen. Personalized recommendations using knowledge graphs: A probabilistic logic programming approach. In _Proceedings of the 10th ACM conference on recommender systems_, pp. 325-332, 2016.
* Chai et al. (2023) Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, and Yang Yang. Graphllm: Boosting graph reasoning ability of large language model. _arXiv preprint arXiv:2310.05845_, 2023.
* Chen et al. (2024a) Hao Chen, Yuanchen Bei, Qijie Shen, Yue Xu, Sheng Zhou, Wenbing Huang, Feiran Huang, Senzhang Wang, and Xiao Huang. Macro graph neural networks for online billion-scale recommender systems. In _Proceedings of the ACM on Web Conference 2024_, pp. 3598-3608, 2024a.
* Chen et al. (2017) Muhao Chen, Yingtao Tian, Mohan Yang, and Carlo Zaniolo. Multilingual knowledge graph embeddings for cross-lingual knowledge alignment. In _Proceedings of the 26th International Joint Conference on Artificial Intelligence_, pp. 1511-1517, 2017.
* Chen et al. (2024b) Shengyuan Chen, Yunfeng Cai, Huang Fang, Xiao Huang, and Mingming Sun. Differentiable neuro-symbolic reasoning on large-scale knowledge graphs. _Advances in Neural Information Processing Systems_, 36, 2024b.
* Chen et al. (2024c) Shengyuan Chen, Qinggang Zhang, Junnan Dong, Wen Hua, Jiannong Cao, and Xiao Huang. Neuro-symbolic entity alignment via variational inference. _arXiv preprint arXiv:2410.04153_, 2024c.
* Chen et al. (2024d) Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, and Jiliang Tang. Label-free node classification on graphs with large language models (LLMs). In _The Twelfth International Conference on Learning Representations_, 2024d.
* Dong et al. (2023) Junnan Dong, Qinggang Zhang, Xiao Huang, Keyu Duan, Qiaoyu Tan, and Zhimeng Jiang. Hierarchy-aware multi-hop question answering over knowledge graphs. In _Proceedings of the ACM Web Conference 2023_, pp. 2519-2527, 2023.
* Dong et al. (2024a) Junnan Dong, Zijin Hong, Yuanchen Bei, Feiran Huang, Xinrun Wang, and Xiao Huang. Clr-bench: Evaluating large language models in college-level reasoning, 2024a. URL https://arxiv.org/abs/2410.17558.
* Dong et al. (2024b) Junnan Dong, Qinggang Zhang, Chuang Zhou, Hao Chen, Daochen Zha, and Xiao Huang. Cost-efficient knowledge-based question answering with large language models. _arXiv preprint arXiv:2405.17337_, 2024b.
* Dong et al. (2024c) Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, and Xiao Huang. Modality-aware integration with large language models for knowledge-based visual question answering, 2024c.
* Fang et al. (2023) Huang Fang, Yang Liu, Yunfeng Cai, and Mingming Sun. Mln4kb: an efficient markov logic network engine for large-scale knowledge bases and structured logic rules. In _Proceedings of the ACM Web Conference 2023_, pp. 2423-2432, 2023.
* He et al. (2019) Fuzhen He, Zhixu Li, Yang Qiang, An Liu, Guanfeng Liu, Pengpeng Zhao, Lei Zhao, Min Zhang, and Zhigang Chen. Unsupervised entity alignment using attribute triples and relation triples. In _Database Systems for Advanced Applications: 24th International Conference, DASFAA 2019, Chiang Mai, Thailand, April 22-25, 2019, Proceedings, Part I 24_, pp. 367-382, 2019.
* Huang et al. (2023) Feiran Huang, Zefan Wang, Xiao Huang, Yufeng Qian, Zhetao Li, and Hao Chen. Aligning distillation for cold-start item recommendation. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pp. 1147-1157, 2023.
* Jiang et al. (2024) Xuhui Jiang, Yinghan Shen, Zhichao Shi, Chengjin Xu, Wei Li, Zixuan Li, Jian Guo, Huawei Shen, and Yuanzhuo Wang. Unlocking the power of large language models for entity alignment. _arXiv preprint arXiv:2402.15048_, 2024.
* Jimenez-Ruiz and Grau (2011) Ernesto Jimenez-Ruiz and Bernardo Cuenca Grau. Logmap: Logic-based and scalable ontology matching. In _The Semantic Web-ISWC 2011: 10th International Semantic Web Conference, Bonn, Germany, October 23-27, 2011, Proceedings, Part I 10_, pp. 273-288, 2011.
* Jiang et al. (2015)Zhixun Li, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao Luo, Xiangxin Zhou, Qiang Liu, Shu Wu, Liang Wang, et al. Gslb: the graph structure learning benchmark. _Advances in Neural Information Processing Systems_, 36, 2024.
* Liu et al. (2022) Bing Liu, Harrisen Scells, Wen Hua, Guido Zuccon, Genghong Zhao, and Xia Zhang. Guiding neural entity alignment with compatibility. _arXiv preprint arXiv:2211.15833_, 2022.
* Liu et al. (2024) Yang Liu, Huang Fang, Yunfeng Cai, and Mingming Sun. Mquine: a cure for" z-paradox"in knowledge graph embedding models. _arXiv preprint arXiv:2402.03583_, 2024a.
* Liu et al. (2024b) Yang Liu, Chuan Zhou, Peng Zhang, Yanan Cao, Yongchao Liu, Zhao Li, and Hongyang Chen. Cl4kge: A curriculum learning method for knowledge graph embedding. _arXiv preprint arXiv:2408.14840_, 2024b.
* Liu et al. (2023) Zirui Liu, Chen Shengyuan, Kaixiong Zhou, Daochen Zha, Xiao Huang, and Xia Hu. Rsc: accelerate graph neural networks training via randomized sparse computations. In _International Conference on Machine Learning_, pp. 21951-21968. PMLR, 2023.
* Mao et al. (2021) Xin Mao, Wenting Wang, Yuanbin Wu, and Man Lan. Boosting the speed of entity alignment 10\(\times\): Dual attention matching network with normalized hard sample mining. In _Proceedings of the Web Conference 2021_, pp. 821-832, 2021.
* Qi et al. (2021) Zhiyuan Qi, Ziheng Zhang, Jiaoyan Chen, Xi Chen, Yuejia Xiang, Ningyu Zhang, and Yefeng Zheng. Unsupervised knowledge graph alignment by probabilistic reasoning and semantic embedding. In _Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21_, pp. 2019-2025, 2021.
* Qu and Tang (2019) Meng Qu and Jian Tang. Probabilistic logic neural networks for reasoning. _Advances in neural information processing systems_, 32, 2019.
* Suchanek et al. (2012) Fabian M. Suchanek, Serge Abiteboul, and Pierre Senellart. Paris: Probabilistic alignment of relations, instances, and schema. In _Proceedings of the 38th International Conference on Very Large Databases_, pp. 157-168, 2012.
* Sun et al. (2018) Zequn Sun, Wei Hu, Qingheng Zhang, and Yuzhong Qu. Bootstrapping entity alignment with knowledge graph embedding. In _IJCAI_, number 2018, 2018.
* Sun et al. (2020) Zequn Sun, Qingheng Zhang, Wei Hu, Chengming Wang, Muhao Chen, Farahnaz Akrami, and Chengkai Li. A benchmarking study of embedding-based entity alignment for knowledge graphs. _Proceedings of the VLDB Endowment_, 13(12):2326-2340, 2020.
* Tang et al. (2008) Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and mining of academic social networks. In _Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining_, pp. 990-998, 2008.
* Tang et al. (2020) Xiaobin Tang, Jing Zhang, Bo Chen, Yang Yang, Hong Chen, and Cuiping Li. Bert-int:a bert-based interaction model for knowledge graph alignment. In _Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20_, pp. 3174-3180, 2020.
* Wang et al. (2024) Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. Knowledge graph prompting for multi-document question answering. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pp. 19206-19214, 2024.
* Wang et al. (2018) Zhichun Wang, Qingsong Lv, Xiaohan Lan, and Yu Zhang. Cross-lingual knowledge graph alignment via graph convolutional networks. In _Proceedings of the 2018 conference on empirical methods in natural language processing_, pp. 349-357, 2018.
* Weikum and Theobald (2010) Gerhard Weikum and Martin Theobald. From information to knowledge: harvesting entities and relationships from web sources. In _Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems_, pp. 65-76, 2010.
* Wu et al. (2023) Xuansheng Wu, Huachi Zhou, Wenlin Yao, Xiao Huang, and Ninghao Liu. Towards personalized cold-start recommendation with prompts. _arXiv preprint arXiv:2306.17256_, 2023.
* Wu et al. (2018)Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Rui Yan, and Dongyan Zhao. Relation-aware entity alignment for heterogeneous knowledge graphs. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence_, 2019.
* Zhang et al. (2024a) Qinggang Zhang, Junnan Dong, Hao Chen, Wentao Li, Feiran Huang, and Xiao Huang. Structure guided large language model for sql generation. _arXiv preprint arXiv:2402.13284_, 2024a.
* Zhang et al. (2024b) Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, and Xiao Huang. Knowgpt: Knowledge injection for large language models, 2024b.
* Zhang et al. (2020) Yuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi, and Le Song. Efficient probabilistic logic reasoning with graph neural networks. In _International Conference on Learning Representations_, 2020.
* Zhao et al. (2023a) Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, and Jian Tang. Graphtext: Graph reasoning in text space. _arXiv preprint arXiv:2310.01089_, 2023a.
* Zhao et al. (2023b) Yu Zhao, Yike Wu, Xiangrui Cai, Ying Zhang, Haiwei Zhang, and Xiaojie Yuan. From alignment to entailment: A unified textual entailment framework for entity alignment. In _Findings of the Association for Computational Linguistics: ACL 2023_, pp. 8795-8806, 2023b.
* Zhong et al. (2022) Ziyue Zhong, Meihui Zhang, Ju Fan, and Chenxiao Dou. Semantics driven embedding learning for effective entity alignment. In _2022 IEEE 38th International Conference on Data Engineering (ICDE)_, pp. 2127-2140, 2022.
* Zhou et al. (2022) Huachi Zhou, Jiaqi Fan, Xiao Huang, Ka Ho Li, Zhenyu Tang, and Dahai Yu. Multi-interest refinement by collaborative attributes modeling for click-through rate prediction. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, pp. 4732-4736, 2022.

## Appendix A Notations and algorithms

### Notations

### Pseudo-code of the greedy algorithm

Below we present the pseudo-code of the greedy algorithm, that incorporates probabilistic reasoning to refine the label set.

``` Inputs: The pseudo-label set \(\mathcal{L}\) Parameters: The initialization probability \(\delta_{0}\in(0,1)\), the threshold \(\delta_{1}\in(\delta_{0},1)\), probabilistic reasoning iterations \(n_{lr}\) Outputs: The refined pseudo-label set \(\mathcal{L}^{*}\subset\mathcal{L}\) \(\mathcal{L}^{\prime}\leftarrow\varnothing\). \(\forall e\in\mathcal{E},\forall e^{\prime}\in\mathcal{E}^{\prime}\), \(P(e\equiv e^{\prime})\gets 0\) \(\forall(e,e^{\prime})\in\mathcal{L}\), \(P(e\equiv e^{\prime})\leftarrow\delta_{0}\) \(i\gets 0\) while\(i<n_{lr}\)do \(\forall e_{h}\in\mathcal{E},\forall e^{\prime}_{h}\in\mathcal{E}^{\prime}\), \(P(e_{h}\equiv e^{\prime}_{h})\gets 1-\prod\limits_{\begin{subarray}{c}(e_{h},r^{ \prime},e_{t})\in\mathcal{T},\\ (e^{\prime}_{h},r^{\prime},e^{\prime}_{t})\in\mathcal{T}^{\prime}\end{subarray}} \left(1-\mathcal{F}^{-1}(r)P(r\subseteq r^{\prime})P(e_{t}\equiv e^{\prime}_{t })\right)\times\left(1-\mathcal{F}^{-1}(r^{\prime})P(r^{\prime}\subseteq r)P(e_ {t}\equiv e^{\prime}_{t})\right)\). /* Update entity alignment probabilities.*/ \(\forall r\in\mathcal{R},\forall r^{\prime}\in\mathcal{R}^{\prime}\), \(P(r\subseteq r^{\prime})\leftarrow\frac{\sum\left(1-\prod_{\begin{subarray}{c }(e^{\prime}_{h},r^{\prime},e^{\prime}_{t})\in\mathcal{T}^{\prime}\end{subarray}} \left(1-P(e^{\prime}_{h}\equiv e_{h})P(e^{\prime}_{t}\equiv e_{t})\right) \right)}{\sum\left(1-\prod_{\begin{subarray}{c}(e^{\prime}_{h},e^{\prime}_{t }\in\mathcal{E}^{\prime}\end{subarray}}\left(1-P(e^{\prime}_{h}\equiv e_{h}) P(e^{\prime}_{t}\equiv e_{t})\right)\right)}\) /* Update subrelation probabilities.*/ \(\mathcal{L}^{\prime}\leftarrow\mathcal{L}^{\prime}\cup\{(e_{h},e^{\prime}_{h}) \in\mathcal{L}|P(e_{h}\equiv e^{\prime}_{h})>\delta_{0}\}\) /* Label adjustment, add confident pairs to the label set.*/ \(\mathcal{L}^{\prime}\leftarrow\mathcal{L}^{\prime}\setminus\{(e_{h},e^{ \prime}_{h})\in\mathcal{L}\mid P(e_{h}\equiv e^{\prime}_{h})<\max\left(\max_{ e\in\mathcal{E}}P(e,e^{\prime}_{h}),\max_{e^{\prime}\in\mathcal{E}^{\prime}}P(e_{h},e^{ \prime})\right)\}\) /* Label adjustment, remove less confident pairs from the label set.*/ \(P(e\equiv e^{\prime})\leftarrow\max\left(P(e\equiv e^{\prime}),\delta_{1} \right)\) for each \((e,e^{\prime})\in\mathcal{L}^{\prime}\) /* Elevate alignment probability of confident pairs.*/ endwhile \(\mathcal{L}^{*}\leftarrow\mathcal{L}^{\prime}\cup\{(e,e^{\prime})|P(e\equiv e ^{\prime})>\delta_{1}\}\) /* Augment the refined label set with confident pairs.*/ Return\(\mathcal{L}^{*}\) ```

**Algorithm 1** The greedy label refinement algorithm

### Efficient implementation of label refiner

**Parameter-efficient probabilistic reasoning.** The total number of alignment probabilities for all entity pairs is \(|\mathcal{E}||\mathcal{E}^{\prime}|\), resulting in a large parameter size when the KGs involved are extensive. We enhance memory efficiency by adopting a lazy inference strategy in probabilistic reasoning. This

\begin{table}
\begin{tabular}{c l} \hline \hline Notation & Description \\ \hline \(\mathcal{G},\mathcal{G}^{\prime}\) & The source and target knowledge graphs, respectively \\ \(\mathcal{E},\mathcal{E}^{\prime}\) & The sets of entities in \(\mathcal{G}\) and \(\mathcal{G}^{\prime}\), respectively \\ \(\mathcal{R},\mathcal{R}^{\prime}\) & The sets of relations in \(\mathcal{G}\) and \(\mathcal{G}^{\prime}\), respectively \\ \(\mathcal{T},\mathcal{T}^{\prime}\) & The sets of relational triplets in \(\mathcal{G}\) and \(\mathcal{G}^{\prime}\), respectively \\ \(\mathcal{A}\) & The ground truth set of aligned entity pairs \\ \(\mathcal{B}\) & The query budget for the LLM \\ \(\mathcal{F}(r)\) & The functionality of relation \(r\) \\ \(P(e\equiv e^{\prime})\) & The alignment probability of entity pair \((e,e^{\prime})\) \\ \(P(e)=\max_{e^{\prime}\in\mathcal{E}^{\prime}}P(e\equiv e^{\prime})\) & The alignment probability of the top-match entity for \(e\) \\ \(\mathcal{L},\mathcal{L}^{*}\) & The annotated pseudo-label set and the refined pseudo-label set \\ \(\Phi(\mathcal{L})\) & The overall incompatibility of the pseudo-label set \(\mathcal{L}\) \\ \(n\) & The number of iterations of active selection \\ \(n_{lr}\) & The number of iterations of label refinement \\ \hline \hline \end{tabular}
\end{table}
Table 3: Notations.

strategy involves only saving the alignment probabilities of the most probable alignments:

\[\left\{P(e_{h},e^{\prime}_{h}),|,e_{h}\in\mathcal{E},e^{\prime}_{h}\in\mathcal{E} ^{\prime},P(e_{h}\equiv e^{\prime}_{h})=\max\left(\max_{e\in\mathcal{E}}P(e,e^{ \prime}_{h}),\max_{e^{\prime}\in\mathcal{E}^{\prime}}P(e_{h},e^{\prime})\right) \right\},\] (12)

Probabilities of other entity pairs can be inferred from these saved alignment probabilities using Eq. (5) when necessary. In this way, parameter complexity is reduced to \(O(\max\left(|\mathcal{E}|+|\mathcal{E}^{\prime}|\right))\).

**Computation efficiency of probabilistic reasoning.** Probabilistic reasoning is executed iteratively, with each iteration updating the alignment probabilities of all entities and relations following Eq. (5) and Eq. (6). We detail the analysis of these two update phases in the following separately.

Since we adopt a lazy inference strategy, the update of entity alignment probabilities involves updating the set of most probable alignments and associated probabilities as shown in Eq. (12). This update requires the estimation of all \(P(e,e^{\prime}_{h}),e\in\mathcal{E}\) and all \(P(e_{h},e^{\prime}),e^{\prime}\in\mathcal{E}^{\prime}\), for each current pair \((e_{h},e^{\prime}_{h})\) to determine if this pair needs an update. Consequently, the computational complexity of this process is proportional to the size of this set, which is \(O(|\mathcal{E}|)\). The estimated scores \(P(e,e^{\prime}_{h}),e\in\mathcal{E}\) and \(P(e_{h},e^{\prime}),e^{\prime}\in\mathcal{E}^{\prime}\) can be precomputed in advance and reused for all pairs \((e_{h},e^{\prime}_{h})\), leading to a computation complexity of \(O(|\mathcal{E}||\mathcal{E}^{\prime}|)\). Thus, the overall computational complexity for updating entity alignment probabilities is \(O(|\mathcal{E}||\mathcal{E}^{\prime}|+|\mathcal{E}|)=O(|\mathcal{E}||\mathcal{ E}^{\prime}|)\).

The update process for sub-relation probabilities involves updating all \(P(r\subset r^{\prime})\) for \(r\in\mathcal{R}\) and \(r^{\prime}\in\mathcal{R}^{\prime}\), resulting in a complexity of \(O(|\mathcal{R}||\mathcal{R}^{\prime}|)\). The estimation of Eq. (6) utilizes the probabilities of the most probable alignments from Eq. (12). Notably, most relation pairs \((r,r^{\prime})\) do not have aligned head entities \((e_{h},e^{\prime}_{h})\) or aligned tail entities \((e_{t},e^{\prime}_{t})\), thus most relation pairs can be excluded for efficient computation by exploiting this sparsity heuristic, reducing the computations by orders.

It is worth noting that these computations can be further accelerated through parallelization, as their execution solely depends on the results from the previous iteration.

## Appendix B Experimental details

### Hardware and software configurations

Our experiments were conducted on a server equipped with six NVIDIA GeForce RTX 3090 GPUs, 48 Intel(R) Xeon(R) Silver 4214R CPUs, and 376GB of host memory. The models were implemented using TensorFlow, NumPy, and SciPy. It was observed that the software version significantly affects hardware-software compatibility. Specifically, the original implementation of Dual-AMN was based on TensorFlow 1.14.0, which is not compatible with newer GPUs such as the NVIDIA GeForce RTX 3090. Therefore, we updated the code to be compatible with TensorFlow 2.7.0, enabling the model to leverage GPU acceleration effectively. The details of the software packages used in our experiments are listed in Table 4.

### Dataset statistics and preprocessing

In our experiments, we utilized the OpenEA dataset version 1.1 (V2), specifically the 15K set. The statistics are detailed in Table 5. It's important to highlight that the destination dataset for D-W-15K, originating from Wikidata, contains only entity IDs, lacking explicit names. These IDs, devoid of semantic content, are not inherently meaningful to a language model. To rectify this, we processed the dataset using the 'wikidatawiki-20160801-abstract.xml' dump file from Wikidata. This file provided the raw data necessary for constructing the D-W-15K dataset, enabling us to extract meaningful entity names.

We shown the quantity of entities with names (after name extraction) for each KG in the 'Named entities' column in Table 5.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Package & tqdm & numpy & scipy & tensorflow & keras & openai \\ \hline Version & 4.66.2 & 1.24.4 & 1.10.1 & 2.7.0 & 2.7.0 & 1.30.1 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Package configurations of our experiments.

In the counterpart filtering phase, we selected the top-\(k\) (where \(k=20\) for our experiments) most similar candidates. The 'Target in top-\(k\)' column of Table 5 shows the number of target entities included in this selection.

### Performance-cost comparison between GPT-3.5 and GPT-4

Performance comparison against rule-based models, evaluated by precision, recall, and f1-score in %.

In this section, we compare the LLM4EA model with several rule-based models, including two lexical matching-based approaches: Emb-Match and Str-Match. Emb-Match uses cosine similarities between word embeddings to identify aligned pairs, employing a pretrained language model. Str-Match utilizes the Levenshtein Distance to calculate similarity scores. Additionally, the probabilistic reasoning model PARIS performs entity alignment by relying on probabilistic methods.

For the lexical matching-based models, we compute the similarity and evaluate the confident entity pairs, specifically targeting those whose normalized similarity scores exceed 0.8. The pretrained language model used for Emb-Match is bert-base-uncased for its ability to process unseen words as a subword model. To reduce false positives, we implementing a filtering process that only considers 1-1 matching for embedding-based matching. For PARIS, we assess all inferred aligned pairs by setting the threshold to zero. The entity alignment (EA) model of LLM4EA generates a ranked score list, which is not directly comparable with these rule-based models. To facilitate comparison, we use the trained EA model to generate confident pairs, ensuring that each entity is the top-ranked candidate for its counterpart. These pairs are then processed through our label refiner, and the refined pairs are evaluated. Experiments for PARIS and LLM4EA are repeated three times to ensure statistical reliability; for Emb-Match and Str-Match, experiments are performed once as these algorithms are deterministic. The results are presented in Table 7.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{} & \multicolumn{3}{c}{**EN-FR-15K**} & \multicolumn{3}{c}{**EN-DE-15K**} & \multicolumn{3}{c}{**D-W-15K**} \\  & Precision & Recall & F1-score & Precision & Recall & F1-score & Precision & Recall & F1-score & Precision & Recall & F1-score \\ \hline Emb-Match & **88.9** & **73.6** & **80.5** & 89.7 & 69.5 & 78.3 & 91.8 & 62.6 & 74.3 & **100** & **100** & **100** \\ Str-Match & 84.8 & 69.8 & 76.6 & **92.3** & 71.4 & 80.5 & **96.2** & 57.9 & 72.3 & 76.9 & **100** & **86.9** \\ \hline PARIS & 58.340.5 & 26.540.3 & 36.540.4 & 90.840.3 & 50.740.3 & 65.040.3 & 92.440.4 & 70.240.2 & 79.840.2 & 99.140.1 & 96.740.1 & 97.940.1 \\ \hline LLM4EA & 68.640.3 & 53.140.2 & 59.840.2 & 90.540.3 & **82.440.4** & **86.240.4** & 90.740.4 & **81.620.5** & **85.940.5** & 98.940.0 & 97.680.1 & 98.340.0 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Performance comparison against rule-based models, evaluated by precision, recall, and f1-score in %.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Datasets & KG & Relations & Relation triplets & Attributes & Attribute triples & Named Entities & Targets in top-\(k\) \\ \hline \multirow{2}{*}{EN-FR} & English (EN) & 193 & 96,318 & 189 & 66,899 & 15,000 & 13,550 \\  & French (FR) & 166 & 80,112 & 221 & 68,779 & 15,000 & 13,550 \\ \hline \multirow{2}{*}{EN-DE} & English (EN) & 169 & 84,867 & 171 & 81,988 & 15,000 & 13,330 \\  & German (DE) & 96 & 92,632 & 116 & 186,335 & 15,000 & 13,330 \\ \hline \multirow{2}{*}{D-W} & DBpedia (DB) & 167 & 73,983 & 175 & 66,813 & 15,000 & 12,910 \\  & Wikidata (WD) & 121 & 83,365 & 457 & 175,686 & 13,458 & 12,910 \\ \hline \multirow{2}{*}{D-Y} & DBpedia (DB) & 72 & 68,063 & 90 & 65,100 & 15,000 & 15,000 \\  & Yago (YG) & 21 & 60,970 & 20 & 131,151 & 15,000 & 15,000 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Data statistics of used OpenEA dataset.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{} & \multicolumn{3}{c}{**EN-FR-15K**} & \multicolumn{3}{c}{**EN-DE-15K**} & \multicolumn{3}{c}{**D-W-15K**} \\  & Precision & Recall & F1-score & Precision & Recall & F1-score & Precision & Recall & F1-score & Precision & Recall & F1-score \\ \hline Emb-Match & **88.9** & **73.6** & **80.5** & 89.7 & 69.5 & 78.3 & 91.8 & 62.6 & 74.3 & **100** & **100** & **100** \\ Str-Match & 84.8 & 69.8 & 76.6 & **92.3** & 71.4 & 80.5 & **96.2** & 57.9 & 72.3 & 76.9 & **100** & **86.9** \\ \hline PARIS & 58.340.5 & 26.540.3 & 36.540.4 & 90.840.3 & 50.740.3 & 65.040.3 & 92.440.4 & 70.240.2 & 79.840.2 & 99.140.1 & 96.740.1 & 97.940.1 \\ \hline LLM4EA & 68.640.3 & 53.140.2 & 59.840.2 & 90.540.3 & **82.440.4** & **86.240.4** & 90.740.4 & **81.620.5** & **85.940.5** & 98.940.0 & 97.680.1 & 98.340.0 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Performance comparison against rule-based models, evaluated by precision, recall, and f1-score in %.

The findings highlight several key observations: 1) Lexical matching-based methods show a degree of alignment ability by leveraging name similarity, particularly on the D-Y-15K dataset, where many aligned entities have identical names. 2) These methods, however, encounter scalability challenges due to their reliance on name similarity. As the size of the KG grows, the number of similar entities increases, making it difficult to maintain precision. This is evident from our experiments on EN-FR-100K and EN-DE-100K, where precision dropped to 49% and recall to 65%, a significant decrease compared to the 15K-size datasets, supporting our analysis. 3) PARIS achieves precise results by handling noisy data through probabilistic reasoning, although its recall is lower than LLM4EA's. 4) LLM4EA, with its active selection and label refinement techniques, consistently delivers robust and accurate performance across all datasets.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have clearly made the main claims in both the abstract and introduction. Specifically, we have summarized and itemized the contributions at the end of the introduction section. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please see page 9, Section 6 for the **Limitations** section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]  Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided the code for the framework, accessible via this URL: https://github.com/chensyCN/llm4ea_official. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code and datasets are accessible through this anonymous URL: https://anonymous.4open.science/r/llm4ea_neurips2024-E763/. In this repository, we included detailed instructions for running the code in the readme.md file. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We present the experimental setting details at the beginning of the experiment section. We also provide the dataset statistics and the preprocessing details in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: To control for the randomness introduced by the Large Language Models, we repeat each experiment three times and report the mean and standard deviation in tables such as Table 1, Table 2, Table 6, and Table 7. For visualized figures such as Figure 2 and Figure 4, we show the error bars representing the standard deviation. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided the computation resources required to reproduce our experiment in Appendix B.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We checked and ensured that our paper conforms with the NeurIPS Code of Ethics in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper focuses on technical improvements in entity alignment using large language models and does not introduce direct societal impacts. Guidelines:* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks, as the work involves only querying the API of an LLM to get pseudo-labels, without releasing a generative model or datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have cited the original papers of the datasets and EA models used in our experiments, ensuring proper credit and respect for their licenses and terms of use. For the open-source code and datasets, we have also stated the licenses they use in the readme.txt file in our code repository. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Code of the proposed framework is released and accessible via this anonymous URL: https://anonymous.4open.science/r/llm4ea_neurips2024-E763/. It contains a readme.md file and a GPLv3 license. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.