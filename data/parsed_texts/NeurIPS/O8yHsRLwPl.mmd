Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity

Alexander Tyurin

KAUST, AIRI, Skoltech

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia AIRI, Moscow, Russia Skoltech

AUST, University of Pavia

KAUST, University of Pavia

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia AIRI, Moscow, Russia Skolkovo Institute of Science and Technology, Moscow, Russia University of Pavia, Pavia, Italy

Ivan Ilin

KAUST1

KAUST1

KAUST1

Footnote 1: footnotemark:

Footnote 2: footnotemark:

###### Abstract

We consider _nonconvex stochastic_ optimization problems in the _asynchronous centralized distributed_ setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique, we develop a new method--Shadowheart SGD--that provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method.

## 1 Introduction

We consider the nonconvex smooth optimization problem

\[\min_{x\in\mathbb{R}^{d}}\Big{\{}f(x):=\mathbb{E}_{\xi\sim\mathcal{D}_{\xi}} \left[f(x;\xi)\right]\Big{\}},\] (1)

where \(f(\cdot;\cdot):\,\mathbb{R}^{d}\times\mathbb{S}_{\xi}\to\mathbb{R}\), and \(\mathcal{D}_{\xi}\) is a distribution on \(\mathbb{S}_{\xi}\neq\emptyset\). Given \(\varepsilon>0\), we seek to find a possibility random point \(\hat{x}\) such that \(\mathbb{E}[\|\nabla f(\hat{x})\|^{2}]\leq\varepsilon\). Such a point \(\hat{x}\) is called an \(\varepsilon\)-stationary point. We focus on solving the problem in the following setup:

(a) \(n\)_workers/nodes_ are able to compute _stochastic_ gradients \(\nabla f(x;\xi)\) of \(f\), _in parallel_ and _asynchronously_, and it takes (at most) \(h_{i}\) seconds for worker \(i\) to compute a single stochastic gradient;

(b) the workers are connected to a _server_ which acts as a communication hub;

(c) the workers can communicate with the server _in parallel_ and _asynchronously_; it takes (at most) \(\tau_{i}\) seconds for worker \(i\) to send a _compressed_ message to the server; compression is performed via applying lossy communication compression to the communicated message (a vector from \(\mathbb{R}^{d}\)); see Def. 2.1;

(d) the server can broadcast compressed vectors to the workers in (at most) \(\tau_{\rm serv}\) seconds; compression is performed via applying a lossy communication compression operator to the communicated message (a vector from \(\mathbb{R}^{d}\)); see Def. A.1.

The main goal of this work is to find an _optimal_ optimization strategy/method that would work uniformly well in all scenarios characterized by the values of the computation times \(h_{1},\ldots,h_{n}\) and 

[MISSING_PAGE_FAIL:2]

but fixed. It is well-known that under Assumptions 1.1, 1.2, and 1.3, the vanilla SGD method \(x^{k+1}=x^{k}-\gamma\nabla f(x^{k};\xi^{k}),\) where \(x^{0}\in\mathbb{R}^{d}\) is a starting point, and \(\gamma>0\) is the step size, solves (1) using _the optimal number of stochastic gradients_(Ghadimi and Lan, 2013; Arjevani et al., 2022). Since the \(\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{   }}}}}}}}}}}}\) of iterations of SGD to get an \(\varepsilon\)-stationary point is \(\mathrm{O}\left(\nicefrac{{L\Delta}}{{\varepsilon}}+\nicefrac{{\sigma^{2}L \Delta}}{{\varepsilon}^{2}}\right),\) SGD run on a _single worker_ whose computation time is \(h_{1}\) seconds would have _time complexity_\(\mathrm{O}\left(h_{1}\times\left(\nicefrac{{L\Delta}}{{\varepsilon}}+ \nicefrac{{\sigma^{2}L\Delta}}{{\varepsilon}^{2}}\right)\right)\) seconds. The time complexity of Minibatch SGD with \(n\) workers, i.e.,

\[x^{k+1}=x^{k}-\tfrac{\gamma}{n}\sum_{i=1}^{n}\nabla f(x^{k};\xi^{k}_{i}),\] (2)

can be shown (Gower et al., 2019) to be

\[\mathrm{O}\left(h_{\max}\times\left(\tfrac{L\Delta}{\varepsilon}+\tfrac{ \sigma^{2}L\Delta}{n\varepsilon^{2}}\right)\right),\] (3)

where \(h_{\max}:=\max_{i\in[n]}h_{i},\) where \([n]\) denotes \(\{1,\ldots,n\}\). The dependence on \(h_{\max}\) is due to Minibatch SGD employing _synchronous_ parallelism which forces it to wait for the slowest worker. While the stochastic part of (3) can be \(n\) times smaller than in the single worker case, (3) does not guarantee an improvement since \(h_{\max}\) can be arbitrarily large. In real systems, computation times can be very heterogeneous and vary in time in chaotic ways (Dutta et al., 2018; Chen et al., 2016).

Recently, Cohen et al. (2021); Mishchenko et al. (2022) and Koloskova et al. (2022) showed that it is possible to improve upon (3) using the celebrated Asynchronous SGD method (Recht et al., 2011; Feyzmahdavian et al., 2016; Nguyen et al., 2018) and get the time complexity \(\mathrm{O}((\nicefrac{{1}}{{n}}\sum_{i=1}^{n}\frac{1}{h_{i}})^{-1}\times \left(\nicefrac{{L\Delta}}{{\varepsilon}}+\nicefrac{{\sigma^{2}L\Delta}}{{n \varepsilon}^{2}}\right)),\) which improves the dependence from \(h_{\max}\) to the harmonic mean of the computation times. Subsequently, Tyurin and Richtarik (2023c) developed the Rennala SGD method whose time complexity is

\[\mathrm{O}\left(\min_{m\in[n]}\left(\tfrac{1}{m}\sum_{i=1}^{m}\tfrac{1}{h_{ \pi_{i}}}\right)^{-1}\times\left(\tfrac{L\Delta}{\varepsilon}+\tfrac{\sigma^ {2}L\Delta}{m\varepsilon^{2}}\right)\right),\] (4)

where \(\pi\) is a permutation for which \(h_{\pi_{1}}\leq\cdots\leq h_{\pi_{n}}\). They also showed that the time complexity (4) is _optimal_ by providing a matching lower bound.

### Communication time is a factor

In many practical scenarios, communication times can be the main bottleneck, and can not be ignored, e.g., in distributed/federated training of machine learning models (Ramesh et al., 2021; Kairouz et al., 2021; Wang et al., 2023). There are two main techniques for reducing the communication bottleneck: local training steps (McMahan et al., 2017) and compressed communication (Seide et al., 2014; Alistarh et al., 2017). In our work, we investigate the latter technique. In particular, efficient methods with compressed communication such as DIANA (Mishchenko et al., 2019), Accelerated DIANA (Li et al., 2020), MARINA (Gorbunov et al., 2021) and DASHA (Tyurin and Richtarik, 2023b) employ _unbiased compressors_, defined next. Assume that \(\mathbb{S}_{\nu}\) is a nonempty arbitrary set of samples, and \(\mathcal{D}_{\nu}\) is a distribution on \(\mathbb{S}_{\nu}\).

**Definition 2.1**.: A mapping \(\mathcal{C}\,:\,\mathbb{R}^{d}\times\mathbb{S}_{\nu}\to\mathbb{R}^{d}\) is an _unbiased compressor_ if there exists \(\omega\geq 0\) such that \(\mathbb{E}_{\nu}[\mathcal{C}(x;\nu)]=x,\ \mathbb{E}_{\nu}[\left\|\mathcal{C}(x;\nu)-x \right\|^{2}]\leq\omega\left\|x\right\|^{2}\) for all \(x\). Let \(\mathbb{U}(\omega)\) denote the family of such compressors5.

Footnote 5: For convenience, following the previous literature, we use the shortcuts \(\mathcal{C}(x;\nu)\equiv\mathcal{C}(x)\) and \(\mathcal{C}(x;\nu_{ij})\equiv\mathcal{C}_{ij}(x)\) assuming that \(\nu\) and \(\nu_{ij}\) are mutually independent.

**Assumption 2.2**.: Samples from \(\mathcal{D}_{\xi}\) and \(\mathcal{D}_{\nu}\) are mutually independent.

The canonical example of an unbiased compressor is the Rand\(K\) compressor (see Def. D.1) that scales \(K\) random entries of the input vector \(x\) by \(\nicefrac{{d}}{{K}}\) and zeros out the rest. Many more examples of unbiased compressors are considered in the literature (Seznosikov et al., 2020; Xu et al., 2021; Horvath et al., 2022). One of the most straightforward methods which use compression is QSGD6(Alistarh et al., 2017):

Footnote 6: It is also called the distributed compressed stochastic gradient descent method (DCGD/DCSGD) (Khaled and RichtÃ¡rik, 2020).

\[x^{k+1}=x^{k}-\tfrac{\gamma}{n}\sum_{i=1}^{n}\mathcal{C}_{i}\left(\nabla f(x^{k };\xi^{k}_{i})\right),\] (5)where each worker calculates one stochastic gradient, compresses it using \(\mathcal{C}_{i}\in\mathbb{U}(\omega)\) drawn independently, and sends it to the server. The server aggregates the compressed vectors and performs step (5). With a proper stepsize choice \(\gamma\), QSGD converges after \(\mathrm{O}\left((\nicefrac{{\omega}}{{n}}+1)\times L\nicefrac{{\Delta}}{{ \varepsilon}}+(\omega+1)\times s^{2}L\nicefrac{{\Delta}}{{n\varepsilon^{2}}}\right)\) iterations7(Khaled and Richtarik, 2020). Let's assume it takes \(\tau_{i}\) seconds for worker \(i\) to send one compressed vector to the server. Since the workers act in parallel, the time complexity of QSGD is

Footnote 7: For \(\omega=0,\) the rate reduces to the rate of Minibatch SGD.

\[\max_{i\in[n]}\left(h_{i}+\tau_{i}\right)\times\left((\tfrac{\omega}{n}+1) \tfrac{L\Delta}{\varepsilon}+(\omega+1)\tfrac{\sigma^{2}L\Delta}{n\varepsilon^ {2}}\right).\] (6)

We can go through a similar exercise with any other method that uses compressed communication (e.g., (Tyurin and Richtarik, 2023; Gauthier et al., 2023; Jia et al., 2023)). Nevertheless, as far as we know, _the optimal time complexities for asynchronous centralized distributed optimization with communication compression are not known_.

## 3 Summary of Contributions

In the regime in which the communication time can be ignored (see Sec. 2.1), Tyurin and Richtarik (2023c) showed that (4) is the optimal time complexity. In this work we endeavor to take the next step: we wish to understand the fundamental limits of the regime in which communication time is a factor. Our main contributions are:

\(\blacklozenge\) We develop a new method--Shadowheart SGD (Algorithm 1)--that guarantees to find an \(\varepsilon\)-stationary point of problem (1) with time complexity \(T_{*}\) given in (10). While the general expression we give for \(T_{*}\) is hard to parse since it involves the equilibrium time \(t_{*}(\cdot)\) whose definition is implicit (see Def. 3.1), we show (see Sec. 7) that \(T_{*}\) is not worse than the time complexity of known centralized8 methods, and also who that it can be _strictly_ better in many regimes, even by _many degrees of magnitude_ (see Sec. 7.1 and Table 1).

Footnote 8: We say that (10) is _centralized_ if the workers calculate stochastic gradients only at points calculated by the server.

\(\blacklozenge\) In Sec. 5 we show that (10) is the _optimal time complexity_ in the family of centralized methods with compression. This is the first such result in the literature.

\(\blacklozenge\) We also developed Adaptive Shadowheart SGD (Sec. 4.2 and M), which does not require the knowledge of the computation and communication times and can work with arbitrary changing times. Moreover, we designed Bidirectional Shadowheart SGD (Sec. A), which works in the regime when broadcast cost not negligible as well.

\(\blacklozenge\) Our theoretical study of Shadowheart SGD is supported by judiciously designed synthetic experiments and machine-learning experiments with logistic regression; see Sec. Q.

## 4 Development of Shadowheart SGD

Our method bears some resemblance to Rennala SGD(Tyurin and Richtarik, 2023c) and QSGD(Alistarh et al., 2017), and involves some additional algorithmic elements which play a key role. First, we adopted the main suggestion of Tyurin and Richtarik (2023c)[Sec.7] behind the design of Rennala SGD that an optimal method should calculate stochastic gradients at the last iterate. Second, QSGD served as an inspiration for how to perform gradient compression. In particular, Shadowheart SGD has the form \(x^{k+1}=x^{k}-\gamma g^{k}\), where

\[g^{k}=\sum\limits_{i=1}^{n}w_{i}\sum\limits_{j=1}^{m_{i}}\mathcal{C}_{ij}\left( \sum\limits_{l=1}^{b_{i}}\nabla f(x^{k};\xi_{il}^{k})\right)/\sum\limits_{i=1} ^{n}w_{i}m_{i}b_{i}.\] (9)

In Shadowheart SGD, worker \(i\) calculates \(b_{i}\) stochastic gradients, adds them up to form \(\sum\nolimits_{l=1}^{b_{i}}\nabla f(x^{k};\xi_{il}^{k})\), and compresses the result \(m_{i}\) times using independently drawn compressors. The compressed messages are sent to the server. The first non-trivial step in the design of our method is the presence of weights \(w_{i}\): the server aggregates the \(\sum_{i=1}^{n}m_{i}\) compressed messages across all workers by performing a conic combination with coefficient \(\frac{w_{i}}{\sum_{i=1}^{n}w_{i}m_{i}b_{i}}\) for messages coming from worker \(i\). One can easily show that (9) is equivalent to Alg. 1. Note that we recover QSGD (see (5)) as a special (suboptimal) case with \(w_{i}=b_{i}=m_{i}=1\) for all \(i\in[n]\).

The weights \(\{w_{i}\}\) are chosen so as to minimize the variance in the proof of Lemma H.1. However, we still need to find the right values for \(b_{i}\) and \(m_{i}\). Since the computation and communication times of worker \(i\) are \(h_{i}\) and \(\tau_{i}\), respectively, the following strategy makes intuitive sense: the server sets some time budget \(t\) for all workers, and each worker then calculates \(\lfloor t/h_{i}\rfloor\) stochastic gradients and sends \(\lfloor t/\tau_{i}\rfloor\) compressed vectors to the server. But what is the right way to choose \(t\)? If \(t\) is too small, then, intuitively, some workers may not have time to calculate "enough" gradients, or may even not have time to send any messages to the server. On the other hand, if \(t\) is too large, then the workers will eventually send information of diminishing utility which will not be worth the extra time this takes.

We find out that, and this one of the key insights of our work, that there exists an optimal time budget \(t^{*}\) which depends on the quantities \(\omega\), \(\nicefrac{{\sigma^{2}}}{{\varepsilon}}\), \(h_{1}\), \(\tau_{1}\),..., \(h_{n}\), \(\tau_{n}\), for which we coin the name _equilibrium time_; see Def. 3.1. Admittedly, the definition of the equilibrium time is implicit; we do not know if it is possible to give a more explicit formula in general. To provide for some peace of mind, we prove the following property:

**Property 4.1**.: _If all inputs of the equilibrium time are non-negative, then the equilibrium time is well defined._

More importantly, in Sec. 5 we provide a _lower bound_ that involves the same mapping. Thus, the equilibrium time is not an "artifact" of our method, but is of a fundamental nature. We use the equilibrium time \(t^{*}\) in Shadowheart SGD when we choose \(b_{i}\) and \(m_{i}\).

Our first main result provides _iteration complexity:_

**Theorem 4.2**.: _Let Assumptions 1.1, 1.2, 1.3, 2.2 hold. Let us take \(\gamma=1/2L\) in Shadowheart SGD (Alg. 1). Then as long as \(K\geq 16L\Delta/\varepsilon,\) we have the guarantee \(\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2} \right]\leq\varepsilon.\)_

This result guarantees that Shadowheart SGD will converge after \(\mathcal{O}\left({}^{L\Delta/\varepsilon}\right)\) iterations. Our second main result provides a much more relevant complexity measure: _time complexity_.

**Corollary 4.3**.: Shadowheart SGD _(Alg. 1) converges after at most \(T_{*}\) seconds, where_

\[T_{*}:=\tfrac{32L\Delta}{\varepsilon}\times t^{*}(\omega,\nicefrac{{\sigma^{ 2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}).\] (10)

Surprisingly, we show in Sec. 5 that our time complexity guarantee (10) is _optimal_ for the family of centralized methods with compressed communication. Moreover, in Sec. 7 and 7.1, we show that (10) is no worse and can be significantly better than the time complexities of previous centralized methods (see also Table 1 for a summary).

### Tighter result with per-iteration times \(h_{i}^{k}\) and \(\tau_{i}^{k}\)

A slight modification of Alg. 1 leads to Alg. 4, which can work with iteration-dependent computation and communication times \(h_{i}^{k}\) and \(\tau_{i}^{k}\). Our main result in this setup is Theorem H.3; here we present its corollary.

**Theorem 4.4**.: _Alg. 4 converges after_

\[\sum_{k=0}^{\left\lceil\frac{16L\Delta}{\varepsilon}\right\rceil}2t^{*}( \omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1}^{k},\tau_{1}^{k},\ldots, h_{n}^{k},\tau_{n}^{k})\] (11)

_seconds, where \(h_{i}^{k}>0\) and \(\tau_{i}^{k}>0\) are computation and communication times for worker \(i\) in iteration \(k\)._

For presentation simplicity sake, in the main part we continue to work with static \(\{h_{i}\}\) and \(\{\tau_{i}\}\).

### On the problem of estimating the times in Algorithms 1 and 4

One of the main features of asynchronous methods (e.g., Rennala SGD, Asynchronous SGD) is their adaptivity to and independence from processing times. In Sec. M, we design Adaptive Shadowheart SGD (Alg. 7) with this feature. Unlike Alg. 1, it does not require the knowledge of \(\{h_{i}\}\) and \(\{\tau_{i}\}\) (or \(\{h_{i}^{k}\}\) and \(\{\tau_{i}^{k}\}\) in the case of Alg. 4), and does not calculate the equilibrium time \(t^{*}\). However, as a byproduct of this flexibility, this method has a slightly worse time complexity guarantee. In order to present our result, we need to define an auxiliary sequence.

**Definition 4.5**.: Assume that the workers have computation and communication times less or equal to \(\{h_{i}\}\) and \(\{\tau_{i}\}\). Assume that \(\bar{h}_{ij}\) is the actual time required to calculate the \(j^{\text{th}}\) stochastic gradient by worker \(i\), \(h_{\min}>0\) is the smallest possible computation time. Then

\[r_{i}:=\sup_{k\geq 0}\tfrac{\sup_{1\leq j\leq\max}\tfrac{\bar{h}_{i,(k+j)}}{ \inf_{1\leq j\leq\max}\tfrac{\bar{h}_{i,(k+j)}}{h_{i,(k+j)}}},\;l_{\max}:=\left \lceil\tfrac{t_{\max}}{h_{\min}}\right\rceil,\]

\(t_{\max}:=128\times t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[ \max\{h_{i},\tau_{i}\},\max\{h_{i},\tau_{i}\}]_{1}^{n}).\)

That is, \(r_{i}\in[1,\infty]\) is the largest ratio between the fastest and the slowest computation of stochastic gradients in local time windows. \(r_{i}\) defines a degree of fluctuations in computation times. Note that \(r_{i}\) describes _local_ fluctuations; it is true that \(r_{i}\leq\nicefrac{{\sup_{i\geq 1}\bar{h}_{i,j}}}{{\inf_{1\leq j\leq\max} \bar{h}_{i,j}}}\) for all \(i\in[n]\) and \(r_{i}\) can be arbitrarily smaller.

A corollary of our main result in this part (Theorem M.1) is presented next.

**Corollary 4.6**.: _If the computation and communication times are positive, the time complexity of Alg. 7 is \(\frac{L\Delta}{\varepsilon}\times t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{ \varepsilon}},[\max\{h_{i},\tau_{i}\},\min\left\{\tau_{i}r_{i},\max\{h_{i},\tau_ {i}\}\right\}]_{1}^{n})\) up to a constant factor, where \(r_{i}\) is defined in Def. 4.5._

Unlike Alg. 1 and Alg. 4, Alg. 7 is more "greedy"; it calculates stochastic gradients and sends compressed vectors in parallel, and it does not know the times \(h_{i}\) and \(\tau_{i}\) (or \(h_{i}^{k}\) and \(\tau_{i}^{k}\)). That is why this method gets a suboptimal complexity and depends on \(r_{i}\). Nevertheless, if we assume that i) the computation times do not fluctuate significantly, i.e., \(r_{i}=\Theta(1)\), and ii) \(\tau_{i}\leq h_{i}\) for all \(i\in[n]\), then this complexity reduces to the optimal complexity \(\nicefrac{{L\Delta}}{{\varepsilon}}\times t^{*}(\omega,\nicefrac{{\sigma^{2}}} {{\varepsilon}},[h_{i},\tau_{1}^{n}]_{1}^{n}).\)

## 5 Lower Bound

```
1:Init \(S=\emptyset\) on the server (all available information)
2:while True do
3:Server calculates a new point \(\bar{x}\) using \(S\) and broadcasts \(\bar{x}\) and \(S\) to any worker (broadcasting does not take time)
4:endwhile\(i^{\text{th}}\) Worker (in parallel):
5:while True do
6: Receives \(\bar{x}\) and \(S\), calculates as many stochastic gradients as it want at the point \(\bar{x}\) (each calculation takes \(h_{i}\) seconds), aggregates all available information, and sends compressed vectors (each dispatch takes \(\tau_{i}\) seconds), which will be added to the set \(S\)
7:endwhile ```

**Protocol 3** Simplified Representation of Protocol 9

In Sec. 4, we stated that Shadowheart SGD converges after \(T_{*}\) seconds; with \(T_{*}\) given in (10). Our next step is to understand if it might be possible to improve this complexity. In Sec. O, we formalize our setup and show in Theorem O.5 that up to a constant factor, the result (10) is _optimal_. Here we present a simplified illustration of our approach.

Protocol 3 can describe all centralized methods (the server updates the iterates, and the workers calculate stochastic gradients at these points), including Minibatch SGD, Asynchronous SGD, Rennala SGD, and Shadowheart SGD. In Theorem O.5, we show that up to a constant factor, no method described by Protocol 3 can converge faster than (10) seconds. In order to use our lower bound, the workers must calculate stochastic gradients at a point that was calculated by the server.

Let us briefly explain the proof's idea. The general approach is the same as in (Nesterov, 2003; Arjevani et al., 2022; Huang et al., 2022): we take the "difficult" function (Sec. P.1), which has large gradients while the last coordinate equals to zero. Every algorithm starts with the point \(x^{0}=0\), and the only way to discover the next coordinate is to calculate a stochastic gradient. Oracles associated with the workers return the next non-zero coordinate with the probability \(p_{\sigma}\approx\nicefrac{{c}}{{\sigma^{2}}}\). Even if the stochastic oracle returns a non-zero coordinate for some worker, the corresponding communication oracle on this worker also has to return a non-zero coordinate, which happens with probability \(p_{\omega}\approx\nicefrac{{1}}{{\omega}}+1\). We fix the Rand\(K\) compressor in the lower bound theorem with \(K\approx\nicefrac{{L\Delta}}{{\varepsilon}}\), and the number of coordinates \(\approx\nicefrac{{L\Delta}}{{\varepsilon}}\); thus, indeed, \(p_{\omega}\approx\nicefrac{{1}}{{\omega}}+1\). Since all \(n\) workers work in parallel, they can discover and send to the server the next non-zero coordinate not earlier than after \(\min_{m\in[n]}\left\{h_{m}\eta_{m}+\tau_{m}\mu_{m}\right\}\) seconds, where \(\eta_{m}\) and \(\mu_{m}\) are i.i.d. _geometric_ random variables with \(p_{\sigma}\) and \(p_{\omega}\). With a high probability, we show that this quantity is \(\Omega(t^{*}(\nicefrac{{1}}{{p_{\omega}}},\nicefrac{{1}}{{p_{\sigma}}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}))\). The number of coordinates is \(\approx\nicefrac{{L\Delta}}{{\varepsilon}}\). Therefore, the lower bound is (10) seconds up to a constant factor.

## 6 Equilibrium Time

Since the time complexity (10) of Shadowheart SGD is optimal, we believe that the equilibrium time is a fundamental mapping that should be investigated more deeply.

### Calculation strategy

The calculation of \(t^{*}\) requires us to sort \(\max\{h_{i},\tau_{i}\}\). Next, it is sufficient to solve \(n\) equations from (7). In Property 4.1, we prove that (7) has one unique solution that can be easily found, for instance, using the bisection method. Then, is it left to find the minimum in (8).

### Intuition behind the equilibrium time \(t^{*}\)

Assuming we found an optimal \(j^{*}\) in (8), we have \(t^{*}=\max\{\max\{h_{\pi_{j^{*}}},\tau_{\pi_{j^{*}}}\},s^{*}(j^{*})\}\). The first observation is that \(t^{*}\) does not depend on the workers that correspond to \(\max\{h_{\pi_{j^{*}+1}},\tau_{\pi_{j^{*}+1}}\},\ldots,\max\{h_{\pi_{n}},\tau_ {\pi_{n}}\}\). Since these values are greater or equal to \(\max\{h_{\pi_{j^{*}}},\tau_{\pi_{j^{*}}}\}\), the mapping "decides" to ignore them because they are too slow. The followingderivations are not rigorous and are merely supposed to offer some intuition. We define \(\alpha_{i}:=\tau_{n_{i}}\omega\) and \(\beta_{i}:=\nicefrac{{h_{s}}}{{\sigma^{2}}}\). Next, using (7), we have

\[1=\sum_{i=1}^{j^{*}}\frac{s^{*}\langle j^{*}\rangle}{2\alpha_{i}+\frac{4\alpha _{i}\beta_{i}}{z^{*}\langle j^{*}\rangle}+2\beta_{i}}\approx s^{*}\langle j^{* }\rangle\sum_{i\in\mathcal{M}}\frac{1}{\max\{\alpha_{i},\beta_{i}\}}+s^{*} \langle j^{*}\rangle^{2}\sum_{i\not\in\mathcal{M}}\frac{1}{\alpha_{i}\beta_{i}},\] (12)

where \(\mathcal{M}:=\{i\in[j^{*}]\,:\,\max\{\alpha_{i},\beta_{i}\}\geq\nicefrac{{ \alpha_{i}\beta_{i}}}{{s^{*}\langle j^{*}\rangle}}\}\). Solving this, one can get that

\[s^{*}\langle j^{*}\rangle\approx\left(\sum_{i\in\mathcal{M}}\frac{1}{\max\{ \alpha_{i},\beta_{i}\}}+\sqrt{\sum_{i\not\in\mathcal{M}}\frac{1}{\alpha_{i} \beta_{i}}}\right)^{-1}.\] (13)

Thus, \(s^{*}\langle j^{*}\rangle\) divides the active workers into two groups \(\mathcal{M}\) and \([j^{*}]\setminus\mathcal{M}\). Both groups contribute to (13) with a _harmonic mean_-like and a _quadratic harmonic mean_-like dependences, correspondingly. The transition between two groups is decided by the rule \(\max\{\alpha_{i},\beta_{i}\}\geq\nicefrac{{\alpha_{i}\beta_{i}}}{{s^{*} \langle j^{*}\rangle}}\Leftrightarrow s^{*}\langle j^{*}\rangle\geq\min\{ \alpha_{i},\beta_{i}\}.\) Intuitively, the last inequality means that if \(\tau_{i}\) or \(h_{i}\) is small (a worker can quickly compute a gradient or send a compressed vector), it belongs to \(\mathcal{M}\). Otherwise, if a worker's computation and communication performance are balanced, it belongs to \([j^{*}]\setminus\mathcal{M}\).

### Properties of the equilibrium time \(t^{*}\)

We now provide some properties and particular cases to understand \(t^{*}\) better. One can find the proofs and more properties in Sec. E. The first result says that \(t^{*}\) is monotonic.

**Property 6.1**.: _If \(\bar{\omega}\geq\omega\geq 0,\nicefrac{{\sigma^{2}}}{{\varepsilon}}\geq \nicefrac{{\sigma^{2}}}{{\varepsilon}}\geq 0,\bar{h}_{1}\geq h_{1}\geq 0,\bar{ \tau}_{1}\geq\tau_{1}\geq 0,\ldots,\bar{h}_{n}\geq h_{n}\geq 0,\) and \(\bar{\tau}_{n}\geq\tau_{n}\geq 0,\) then \(t^{*}\langle\bar{\omega},\nicefrac{{\sigma^{2}}}{{\varepsilon}},[\bar{h}_{i}, \bar{\tau}_{i}]_{1}^{n}\rangle\geq t^{*}\langle\omega,\nicefrac{{\sigma^{2}}} {{\varepsilon}},[h_{i},\tau_{i}]_{1}^{n}\rangle.\)_

Consider the Rand\(K\) compressor. If it takes \(\dot{\tau}_{i}\) sec to send _one coordinate_ by worker \(i,\) then, up to a constant factor, Property 6.2 ensures that an optimal choice of \(K\) is \(1.\)

**Property 6.2**.: _For all \(K\in[1,d],\nicefrac{{\sigma^{2}}}{{\varepsilon}},\)\(h_{1},\)\(\dot{\tau}_{1},\ldots,h_{n},\dot{\tau}_{n}\geq 0,\) we have \(24\times t^{*}\left(\nicefrac{{d}}{{K}}-1,\nicefrac{{\sigma^{2}}}{{\varepsilon }},h_{1},K\dot{\tau}_{1},\ldots,h_{n},K\dot{\tau}_{n}\right)\geq t^{*}\left(d-1, \nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\dot{\tau}_{1},\ldots,h_{n}, \dot{\tau}_{n}\right).\)_

### Examples

We now list several examples, starting with simple corner/extreme cases. One can find the derivations in Sec. F. For brevity, we will sometimes write \(t^{*}\) instead of \(t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_{ n},\tau_{n}).\)

_Example 6.3_.: **[Infinitely Fast Worker]** If exists \(j\in[n]\) such that \(\tau_{j}=0\) and \(h_{j}=0,\) then \(t^{*}=0.\)

_Example 6.4_.: **[Infinitely Slow Workers]** If \(\tau_{i}=\infty\) and \(h_{i}=\infty\) for all \(i\in[n],\) then \(t^{*}=\infty.\)

_Example 6.5_.: **[Equal Performance]** If \(\tau_{i}=\tau\) and \(h_{i}=h\) for all \(i\in[n],\) then9

Footnote 9: From the proof, it is clear that the result is tight up to a constant factor.

\[t^{*}\leq 6\max\left\{h,\tau,\frac{\tau_{W}}{n},\frac{h\sigma^{2}}{n\varepsilon}, \sqrt{\frac{\tau_{W}\sigma^{2}\omega}{n\varepsilon}}\right\}.\] (14)

In the next example, we consider the setting from Sec. 2.1. Example 6.6 and Corollary 4.3 restore the optimal rate (4) of Rennala SGD.

_Example 6.6_.: **[Infinitely Fast Communication]** If \(\tau_{i}=0\) for all \(i\in[n],\) then

\[t^{*}\leq 2\min_{m\in[n]}\max\left\{h_{\pi_{m}},\frac{\sigma^{2}}{\varepsilon} \left(\sum_{i=1}^{m}\frac{1}{h_{\pi_{i}}}\right)^{-1}\right\}=\Theta\left( \min_{m\in[n]}\left(\tfrac{1}{m}\sum_{i=1}^{m}\tfrac{1}{h_{\pi_{i}}}\right)^ {-1}\left(1+\tfrac{\sigma^{2}}{m\varepsilon}\right)\right),\] (15)

where \(\pi\) is a permutation that sorts \(\{h_{i}\}_{i=1}^{n}.\)

The following two examples show that \(t^{*}\) is robust to slow workers or workers that do not participate.

_Example 6.7_.: **[Inoring Slow Workers]** If \(h_{i}\) and \(\tau_{i}\) are fixed and finite for all \(i\leq p,\) and \(\max\{h_{i},\tau_{i}\}=m\in\mathbb{R}\) for all \(i>p,\) then, for \(m\) large enough, we have \(t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i},\tau_{i}]_{1}^{n})=t^{*} (\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i},\tau_{i}]_{1}^{p}).\)

_Example 6.8_.: **[Partial Participation]** If \(\max\{h_{i},\tau_{i}\}=\infty\) for all \(i>p\geq 1,\) then \(t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i},\tau_{i}]_{1}^{n})=t^{*} (\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i},\tau_{i}]_{1}^{p}).\)Comparison with Baselines

In the previous sections, we did not invoke any assumptions about the compressors except for Def. 2.1. Inspired by Property 6.2, to make the comparisons with the baselines easier, we consider the \(\text{Rand}K\) compressor with \(K=1\). Using Theorem D.2, we have \(\omega=d-1\). We also assume that worker \(i\) takes \(\hat{\tau}_{i}\) seconds to send _one coordinate_ to the server; thus \(\tau_{i}=\dot{\tau}_{i},\) since we use \(\text{Rand}1\). Also, it takes \(d\hat{\tau}_{i}\) to send a non-compressed vector for all \(i\in[n]\).

**Minibatch SGD and QSGD.** It is well known (Lan, 2020) that the number of iterations of Minibatch SGD required to find an \(\varepsilon\)-solution is \(\text{O}\left(\nicefrac{{L\Delta}}{{\varepsilon}}+\nicefrac{{\sigma^{2}L \Delta}}{{\varepsilon}}\right).\) In Minibatch SGD, each worker calculates one stochastic gradient and sends a non-compressed vector. Since the server waits for the slowest worker, the time complexity of such method (up to a constant factor) is

\[T_{\text{MB}}:=\max_{i\in[n]}\left(h_{i}+d\hat{\tau}_{i}\right)\left(\tfrac{L \Delta}{\varepsilon}+\tfrac{\sigma^{2}L\Delta}{n\varepsilon^{2}}\right).\] (16)

In Sec. J, we compare (16) with (10) and show

_Comparison 7.1_.: \(T_{*}=\text{O}(T_{\text{MB}})\).

However, there are many regimes when \(T_{*}\ll T_{\text{MB}}\). For instance, if \(\max\{h_{i},\hat{\tau}_{i}\}=\infty\) for some worker (Example 6.8), then \(T_{\text{MB}}=\infty\) and \(T_{*}<\infty\). Also, under the conditions of Example 6.7, if \(m\to\infty\), we get \(T_{\text{MB}}\to\infty\) whereas \(T_{*}\) is bounded. The same reasoning applies to QSGD because its time complexity (6) depends on \(\max_{i\in[n]}\left(h_{i}+\dot{\tau}_{i}\right).\) Due to Theorem O.5, up to a constant factor, \(T_{*}\) is less or equal to (6); see also Table 1.

**Rennala SGD and Asynchronous SGD.** When the communication time is negligible, Tyurin and Richtarik (2023c) proved that the optimal time complexity is attained by Rennala SGD. When \(\hat{\tau}_{i}\to 0\) for all \(i\in[n]\), we show in Example 6.6 that (10) is the same as the time complexity of Rennala SGD obtained by Tyurin and Richtarik (2023c). Assume \(\hat{\tau}_{i}>0\) for all \(i\in[n]\). We can apply the result from Theorem O.5 to Rennala SGD, thus the time complexity of Rennala SGD is not better than

\[T_{\text{R}}:=\tfrac{L\Delta}{\varepsilon}\times t^{*}(0,\nicefrac{{\sigma^ {2}}}{{\varepsilon}},h_{1},d\hat{\tau}_{1},\dots,h_{n},d\hat{\tau}_{n}).\] (17)

Note that Asynchronous SGD also has the same lower bound. In Sec. J, we compare (17) with (10) and show

_Comparison 7.2_.: \(T_{*}=\text{O}(T_{\text{R}})\).

### Shadowheart SGD is strictly better in many regimes.

Due to the non-explicit nature, it is not transparent that the time complexity of Shadowheart SGD is universally strictly better than in all baselines in many practical regimes. Let us prove it. Take

\[h_{i}=h\text{ for all }i<n,\,h_{n}=\infty,\text{ and }\hat{\tau}_{i}=\dot{\tau}\text{ for all }i\in[n].\] (18)

Due to (16) and (17), the time complexities of Minibatch SGD and QSGD are \(\infty,\) and the time complexities of Asynchronous SGD and Rennala SGD are not smaller than

\[\Omega\left(\max\left\{h,d\dot{\tau},\tfrac{h\sigma^{2}}{(n-1)\varepsilon} \right\}\tfrac{L\Delta}{\varepsilon}\right),\text{ which }\stackrel{{ n\to\infty}}{{\to}}\Omega\left(\max\left\{h,d\dot{ \tau}\right\}\tfrac{L\Delta}{\varepsilon}\right).\]

From (14), the time complexity of Shadowheart SGD is at most

\[\text{O}\left(\max\left\{h,\dot{\tau},\tfrac{d\dot{\tau}}{n-1},\tfrac{h \sigma^{2}}{(n-1)\varepsilon},\sqrt{\tfrac{\dot{\tau}h\sigma^{2}d}{(n-1) \varepsilon}}\tfrac{L\Delta}{\varepsilon}}\right),\text{ which }\stackrel{{ n\to\infty}}{{\to}}\text{O}\left(\max \left\{h,\dot{\tau}\right\}\tfrac{L\Delta}{\varepsilon}\right).\]

Thus, Shadowheart SGD can be \(d\) times faster than all previous methods if the number of workers \(n\) is large. Using the same reasoning, Shadowheart SGD can be \(n-1\) times faster if the dimension \(d\) or \(\dot{\tau}\) is large. Due to the continuity of the complexities, such huge differences hold even if we take \(n<\infty,\) and start considering more heterogenous times \(h_{i}\) and \(\tau_{i}\) by perturbating (18).

### The fastest worker works locally

Another important baseline is the vanilla SGD method, which works on the fastest worker, does not communicate with the server, and performs local steps (non-centralized method). For simplicity, assume that \(\nicefrac{{\sigma^{2}}}{{\varepsilon}}\geq 1\). Then, the time complexity of such an algorithm (Lan, 2020) is \(\min_{i\in[n]}h_{i}\times\nicefrac{{\sigma^{2}L\Delta}}{{\varepsilon^{2}}}\). Clearly, comparing \(T_{\text{SGD}}\) and (10), if \(\hat{\tau}_{i}\) are large enough, then \(T_{\text{SGD}}\) can be smaller than \(T_{*}\). However, this does not contradict our lower bounds because this method does not satisfy the conditions of Theorem O.5: it does not communicate with the server. In other words, if the communication channel is too slow, it does not make sense to communicate. One may now ask: "Under which conditions is it beneficial to communicate?" Comparing \(T_{\text{SGD}}\) and (10), one can see that (10) is better when \(t^{*}(d-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\hat{\tau}_{1},\ldots,h _{n},\hat{\tau}_{n})\leq\min_{i\in[n]}h_{i}\times\nicefrac{{\sigma^{2}}}{{ \varepsilon}}\). It is sufficient to substitute the initial parameters to this inequality and decide which method to use. For instance, in the view of Example 6.5, one should compare \(\max\{h,\dot{\tau},\nicefrac{{\tau(d-1)}}{{n}},\nicefrac{{h\sigma^{2}}}{{ \varepsilon}},\sqrt{\nicefrac{{h\sigma^{2}(d-1)}}{{n}}e}\}\) vs. \(\nicefrac{{h\sigma^{2}}}{{\varepsilon}}\). In the regime when \(n\) is large enough or \(\varepsilon\) is small enough, we have \(t^{*}<\nicefrac{{h\sigma^{2}}}{{\varepsilon}}\), and Alg. 7 has better convergence guarantees. On the other hand, if \(\dot{\tau}\) is large enough, then it is possible that \(t^{*}>\nicefrac{{h\sigma^{2}}}{{\varepsilon}}\).

## Acknowledgments and Disclosure of Funding

The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, ii) Center of Excellence for Generative AI, under award number 5940, iii) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science. The work of A.T. was partially supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021).

## References

* Alistarh et al. (2017) Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M. (2017). QSGD: Communication-efficient SGD via gradient quantization and encoding. In _Advances in Neural Information Processing Systems (NIPS)_, pages 1709-1720.
* Arjevani et al. (2022) Arjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J., Srebro, N., and Woodworth, B. (2022). Lower bounds for non-convex stochastic optimization. _Mathematical Programming_, pages 1-50.
* Beznosikov et al. (2020) Beznosikov, A., Horvath, S., Richtarik, P., and Safaryan, M. (2020). On biased compression for distributed learning. _arXiv preprint arXiv:2002.12410_.
* Carmon et al. (2020) Carmon, Y., Duchi, J. C., Hinder, O., and Sidford, A. (2020). Lower bounds for finding stationary points i. _Mathematical Programming_, 184(1):71-120.
* Chen et al. (2016) Chen, J., Pan, X., Monga, R., Bengio, S., and Jozefowicz, R. (2016). Revisiting distributed synchronous sgd. _arXiv preprint arXiv:1604.00981_.
* Cohen et al. (2021) Cohen, A., Daniely, A., Drori, Y., Koren, T., and Schain, M. (2021). Asynchronous stochastic optimization robust to arbitrary delays. _Advances in Neural Information Processing Systems_, 34:9024-9035.
* Dutta et al. (2018) Dutta, S., Joshi, G., Ghosh, S., Dube, P., and Nagpurkar, P. (2018). Slow and stale gradients can win the race: Error-runtime trade-offs in distributed SGD. In _International Conference on Artificial Intelligence and Statistics_, pages 803-812. PMLR.
* Feyzmahdavian et al. (2016) Feyzmahdavian, H. R., Aytekin, A., and Johansson, M. (2016). An asynchronous mini-batch algorithm for regularized stochastic optimization. _IEEE Transactions on Automatic Control_, 61(12):3740-3754.
* Gauthier et al. (2023) Gauthier, F., Gogineni, V. C., Werner, S., Huang, Y.-F., and Kuh, A. (2023). Asynchronous online federated learning with reduced communication requirements. _arXiv preprint arXiv:2303.15226_.
* Ghadimi and Lan (2013) Ghadimi, S. and Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368.
* Gorbunov et al. (2021) Gorbunov, E., Burlachenko, K., Li, Z., and Richtarik, P. (2021). MARINA: Faster non-convex distributed learning with compression. In _38th International Conference on Machine Learning_.
* Gower et al. (2019) Gower, R. M., Loizou, N., Qian, X., Sailanbayev, A., Shulgin, E., and Richtarik, P. (2019). SGD: General analysis and improved rates. In _International Conference on Machine Learning_, pages 5200-5209. PMLR.
* Goyal et al. (2019)Gruntkowska, K., Tyurin, A., and Richtarik, P. (2023). EF21-P and friends: Improved theoretical communication complexity for distributed optimization with bidirectional compression. In _International Conference on Machine Learning_, pages 11761-11807. PMLR.
* Horvath et al. (2022) Horvath, S., Ho, C.-Y., Horvath, v., Sahu, A. N., Canini, M., and Richtarik, P. (2022). Natural compression for distributed deep learning. In _Mathematical and Scientific Machine Learning_, pages 129-141. PMLR.
* Huang et al. (2022) Huang, X., Chen, Y., Yin, W., and Yuan, K. (2022). Lower bounds and nearly optimal algorithms in distributed learning with communication compression. _arXiv preprint arXiv:2206.03665_.
* Jia et al. (2023) Jia, J., Liu, J., Zhou, C., Tian, H., Dong, M., and Dou, D. (2023). Efficient asynchronous federated learning with sparsification and quantization. _arXiv preprint arXiv:2312.15186_.
* Kairouz et al. (2021) Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al. (2021). Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210.
* Khaled and Richtarik (2020) Khaled, A. and Richtarik, P. (2020). Better theory for SGD in the nonconvex world. _arXiv preprint arXiv:2002.03329_.
* Koloskova et al. (2022) Koloskova, A., Stich, S. U., and Jaggi, M. (2022). Sharper convergence guarantees for asynchronous SGD for distributed and federated learning. _arXiv preprint arXiv:2206.08307_.
* Konecny et al. (2016) Konecny, J., McMahan, H. B., Yu, F. X., Richtarik, P., Suresh, A. T., and Bacon, D. (2016). Federated learning: Strategies for improving communication efficiency. _arXiv preprint arXiv:1610.05492_.
* Lan (2020) Lan, G. (2020). _First-order and stochastic optimization methods for machine learning_. Springer.
* LeCun et al. (2010) LeCun, Y., Cortes, C., and Burges, C. (2010). Mnist handwritten digit database. _ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist_, 2.
* Li et al. (2020) Li, Z., Kovalev, D., Qian, X., and Richtarik, P. (2020). Acceleration for compressed gradient descent in distributed and federated optimization. In _International Conference on Machine Learning_.
* McMahan et al. (2017) McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. (2017). Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR.
* Mishchenko et al. (2022) Mishchenko, K., Bach, F., Even, M., and Woodworth, B. (2022). Asynchronous SGD beats minibatch SGD under arbitrary delays. _arXiv preprint arXiv:2206.07638_.
* Mishchenko et al. (2019) Mishchenko, K., Gorbunov, E., Takac, M., and Richtarik, P. (2019). Distributed learning with compressed gradient differences. _arXiv preprint arXiv:1901.09269_.
* Nemirovskij and Yudin (1983) Nemirovskij, A. S. and Yudin, D. B. (1983). Problem complexity and method efficiency in optimization.
* Nesterov (2003) Nesterov, Y. (2003). _Introductory lectures on convex optimization: A basic course_, volume 87. Springer Science & Business Media.
* Nesterov (2018) Nesterov, Y. (2018). _Lectures on convex optimization_, volume 137. Springer.
* Nguyen et al. (2018) Nguyen, L., Nguyen, P. H., Dijk, M., Richtarik, P., Scheinberg, K., and Takac, M. (2018). SGD and hogwild! convergence without the bounded gradients assumption. In _International Conference on Machine Learning_, pages 3750-3758. PMLR.
* Ramesh et al. (2021) Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. (2021). Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR.
* Recht et al. (2011) Recht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. _Advances in Neural Information Processing Systems_, 24.
* Recht et al. (2016)Richtarik, P., Sokolov, I., and Fatkhullin, I. (2021). EF21: A new, simpler, theoretically better, and practically faster error feedback. _arXiv preprint arXiv:2106.05203_.
* Seide et al. (2014) Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D. (2014). 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs. In _Fifteenth Annual Conference of the International Speech Communication Association_.
* Tyurin and Richtarik (2023a) Tyurin, A. and Richtarik, P. (2023a). A computation and communication efficient method for distributed nonconvex problems in the partial participation setting. _Advances in Neural Information Processing Systems (NeurIPS)_.
* Tyurin and Richtarik (2023b) Tyurin, A. and Richtarik, P. (2023b). DASHA: Distributed nonconvex optimization with communication compression, optimal oracle complexity, and no client synchronization. _11th International Conference on Learning Representations (ICLR)_.
* Tyurin and Richtarik (2023c) Tyurin, A. and Richtarik, P. (2023c). Optimal time complexities of parallel stochastic optimization methods under a fixed computation model. _Advances in Neural Information Processing Systems (NeurIPS)_.
* Vogels et al. (2019) Vogels, T., Karimireddy, S. P., and Jaggi, M. (2019). PowerSGD: Practical low-rank gradient compression for distributed optimization. In _Neural Information Processing Systems_.
* Wang et al. (2023) Wang, J., Lu, Y., Yuan, B., Chen, B., Liang, P., De Sa, C., Re, C., and Zhang, C. (2023). Cocktailsgd: Fine-tuning foundation models over 500mbps networks. In _International Conference on Machine Learning_, pages 36058-36076. PMLR.
* Xu et al. (2021a) Xu, H., Ho, C.-Y., Abdelmoniem, A. M., Dutta, A., Bergou, E. H., Karatsenidis, K., Canini, M., and Kalnis, P. (2021a). Grace: A compressed communication framework for distributed machine learning. In _2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)_, pages 561-572. IEEE.
* Xu et al. (2021b) Xu, H., Kostopoulou, K., Dutta, A., Li, X., Ntoulas, A., and Kalnis, P. (2021b). Deepreduce: A sparse-tensor communication framework for federated deep learning. _Advances in Neural Information Processing Systems_, 34:21150-21163.

###### Contents

* 1 Introduction
* 2 Related Work
	* 2.1 Communication time can be ignored
	* 2.2 Communication time is a factor
* 3 Summary of Contributions
* 4 Development of Shadowheart SGD
	* 4.1 Tighter result with per-iteration times \(h_{i}^{k}\) and \(\tau_{i}^{k}\)
	* 4.2 On the problem of estimating the times in Algorithms 1 and 4
* 5 Lower Bound
* 6 Equilibrium Time
	* 6.1 Calculation strategy
	* 6.2 Intuition behind the equilibrium time \(t^{*}\)
	* 6.3 Properties of the equilibrium time \(t^{*}\)
	* 6.4 Examples
* 7 Comparison with Baselines
	* 7.1 Shadowheart SGD is strictly better in many regimes.
	* 7.2 The fastest worker works locally
* A Bidirectional Compression
* B Frequently Used Notation
* C Basic Facts
* D Rand\(K\) Compressor
* E Proofs of the Properties of the Equilibrium Time
* F Derivations of the Examples for the Equilibrium Time
* G Generic Lemma For Unbiased Gradient Estimators
* H Proofs for Algorithms 1 and 4
* I The Classical SGD Theorem
* J Comparison with Baselines

[MISSING_PAGE_EMPTY:14]

Bidirectional Compression

In this section, we discuss a simple way to use the Shadowheart SGD techniques in the setup when broadcasting is expensive (Line 7 in Alg. 1); i.e., when \(\tau_{\mathrm{serv}}\gg 0\). We will employ the following family of compressors.

**Definition A.1**.: A mapping \(\mathcal{C}\,:\,\mathbb{R}^{d}\times\mathbb{S}_{\nu}\to\mathbb{R}^{d}\) is a _biased compressor_ if there exists \(\alpha\in(0,1]\) such that

\[\mathbb{E}_{\nu}\left[\left\|\mathcal{C}(x;\nu)-x\right\|^{2}\right]\leq(1- \alpha)\left\|x\right\|^{2},\ \forall x\in\mathbb{R}^{d}.\] (19)

We shall use the shortcut \(\mathcal{C}(x;\nu)\equiv\mathcal{C}(x),\) and denote the family of such biased compressors as \(\mathbb{B}(\alpha).\)

The family \(\mathbb{B}(\alpha)\) is more general than \(\mathbb{U}(\omega)\) in the sense that if \(\mathcal{C}\in\mathbb{U}(\omega),\) then \((\omega+1)^{-1}\mathcal{C}\in\mathbb{B}((\omega+1)^{-1}).\) It includes the Top\(K\) and Rank\(K\) compressors (Vogels et al., 2019; Beznosikov et al., 2020), among many others.

Let \(\mathcal{C}_{\mathrm{serv}}\in\mathbb{B}(\alpha)\) be the compressor used by the server. We use the primal error-feedback mechanism EF21-P (Gruntkowska et al., 2023) which requires us to add the following changes to Alg. 1 and Alg. 2. We add the steps

\[p^{k+1}=\mathcal{C}_{\mathrm{serv}}(x^{k+1}-w^{k}),\quad w^{k+1}=w^{k}+p^{k+1}\]

to Alg. 1 and broadcast \(p^{k+1}\) instead of \(x^{k}\). This change leads to Bidirectional Shadowheart SGD (Alg. 5). In Alg. 2, the workers should receive \(p^{k+1}\), calculate \(w^{k+1},\) and use \(w^{k}\) instead of \(x^{k}\) in the calculations of stochastic gradients. We provide the pseudo-codes of these algorithms in Sec. K.

Our main results are:

**Theorem A.2**.: _Let Assumptions 1.1, 1.2, 1.3, 2.2 hold. Choose \(\gamma=\frac{\alpha}{16L}\). Then as long as \(K\geq\frac{76L\Delta}{\alpha\varepsilon},\) Bidirectional Shadowheart SGD (Alg. 5) guarantees to find an \(\varepsilon\)-stationary point._

**Corollary A.3**.: _If the broadcast time of \(\mathcal{C}_{\mathrm{serv}}\) is not greater than \(\tau_{\mathrm{serv}}\), then Bidirectional Shadowheart SGD (Alg. 5) converges after at most_

\[T_{*,\mathrm{serv}}:=\frac{768L\Delta}{\alpha\varepsilon}\times\left(\tau_{ \mathrm{serv}}+2t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i},\tau_{i}]_{1}^{n})\right)\] (20)

_seconds._

_Remark A.4_.: If the broadcast cost can't be ignored, the time complexity of Alg. 1 changes from (10) to

\[T_{*}:=\tfrac{16L\Delta}{\varepsilon}\times(\tau_{\mathrm{serv}}^{\mathrm{full }}+2t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i},\tau_{i}]_{1 }^{n})),\] (21)

where \(\tau_{\mathrm{serv}}^{\mathrm{full}}\) is the time required to broadcast a _full/ non-compressed_ vector.

We should compare (21) obtained by the unidirectional algorithm and (20) obtained by the bidirectional algorithm. Consider that \(\mathcal{C}_{\mathrm{serv}}=\text{Top}K\) with \(K\leq d.\) We can see (20) that depends on \(\tau_{\mathrm{serv}},\) that is much less than \(\tau_{\mathrm{serv}}^{\mathrm{full}}\) because \(K\ll d.\) At the same time, (20) is \(\nicefrac{{1}}{{\alpha}}\) times larger than (21). This is a standard price for the fact that we use a biased compressor (e.g. (Richtarik et al., 2021; Gruntkowska et al., 2023)). However, \(\alpha\) is very close \(1\) in practice (Beznosikov et al., 2020; Vogels et al., 2019; Xu et al., 2021b). It turns out that we can always choose \(K\) in \(\text{Top}K\) (we take this compressor as an example) in such a way that Alg. 5 is never worse than Alg. 1.

_Comparison A.5_.: Assume that it takes \(\hat{\tau}_{\mathrm{serv}}\) seconds to send _one coordinate_ from the server to the workers. If we take \(K\geq\min\left\{d,t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i},\tau_{i}]_{1}^{n})/\hat{\tau}_{\mathrm{serv}}\right\}\) in \(\text{Top}K\), then \(T_{*,\mathrm{serv}}=\text{O}\left(T_{*}\right).\)

If \(\tau_{\mathrm{serv}}^{\mathrm{full}}=d\hat{\tau}_{\mathrm{serv}}\) is the bottleneck in (21) with Alg. 1, i.e., \(d\hat{\tau}_{\mathrm{serv}}\gg t^{*}\), then one can take \(K=\nicefrac{{t^{*}}}{{\hat{\tau}_{\mathrm{serv}}}}\ll d\) in \(\text{Top}K\) with Alg. 5 and improve the time complexity.

## Appendix B Frequently Used Notation

We thought a table of frequently used notation could be useful. Here it is:

\begin{tabular}{c c} \hline
**Notation** & **Meaning** \\ \hline \(\varepsilon\) & error tolerance \\ \(f\) & Function \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) whose \(\varepsilon\)-stationary point we want to find (see (1)) \\ \(L\) & Smoothness parameter of \(f\) (see Assumption 1.1) \\ \(f^{*}\) & Lower bound on \(f\) (see (1.2)) \\ \(\sigma^{2}\) & Stochastic gradients \(\nabla f(x;\xi)\) have variance bounded by \(\sigma^{2}\) (see Assumption 1.3) \\ \(x^{0}\) & Starting point of all algorithms; a vector in \(\mathbb{R}^{d}\) \\ \(\gamma\) & Positive stepsize used by all algorithms \\ \(\Delta\) & \(\Delta:=f(x^{0})-f^{*}\) \\ \hline \(n\) & number of workers \\ \(h_{i}\) & Maximal time it takes for worker \(i\) to compute one stochastic gradient of \(\nabla f(\cdot;\xi)\) \\ \(b_{i}\) & Minibatch size associated with worker \(i\) (worker \(i\) compresses minibatch gradients) \\ \(m_{i}\) & Number of compressed messages sent to the server by worker \(i\) in a single iteration \\ \(\mathbb{U}(\omega)\) & Set of unbiased compressors with variance parameter \(\omega\geq 0\) (see Definition 2.1) \\ \(c_{ij}\) & Compressors used by worker \(i\); \(C_{ij}\in\mathbb{U}(\omega),j\in\{1,\ldots,m_{i}\}\) \\ \(\tau_{i}\) & Maximal time it takes for worker \(i\) to communicate vector \(\mathcal{C}_{ij}(\cdot)\), where \(\mathcal{C}_{ij}\in\mathbb{U}(\omega)\), to the server \\ \(\tau_{i}\) & Time it takes to send to worker \(i\) one float to the server (equal to \(\tau_{i}\) of the Brandl compressor is used) \\ \(\mathbb{B}(\alpha)\) & Set of biased compressors with contraction parameter \(0<\alpha\leq 1\) (see Definition A.1) \\ \(C_{\text{exper}}\) & Compressor used by the server; \(\mathcal{C}_{\text{exper}}\in\mathbb{B}(\alpha)\) \\ \(\tau_{\text{exper}}^{\text{full}}\) & Maximal time it takes for server to broadcast a non-compressed vector from \(\mathbb{R}^{d}\) to the workers \\ \(\tau_{\text{exper}}\) & Maximal time it takes for server to broadcast a vector \(C_{\text{exper}}(\cdot)\), where \(\mathcal{C}_{\text{exper}}\in\mathbb{B}(\alpha)\), to the workers \\ \(\hat{\tau}_{\text{exper}}\) & Time it takes for server to broadcast one float to the workers \\ \hline \(t^{*}\) & Equilibrium time; a function of \(\omega,\nicefrac{{\sigma^{2}}}{{c}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}\) (see Definition 3.1) \\ \(T_{\text{t}}\) & Time complexity of ShadowWeather SGD (see Corollary 4.3) \\ \(T_{\text{tIB}}\) & Time complexity of Minibatch SGD (see (16)) \\ \(T_{\text{R}}\) & Time complexity of Rennala SGD (see (17)) \\ \hline \(g=\text{O}(f)\) & Exist \(C>0\) such that \(g(z)\leq C\times f(z)\) for all \(z\in\mathcal{Z}\) \\ \(g=\Omega(f)\) & Exist \(C>0\) such that \(g(z)\geq C\times f(z)\) for all \(z\in\mathcal{Z}\) \\ \(g=\Theta(f)\) & \(g=\text{O}(f)\) and \(g=\Omega(f)\) \\ \(\{a,\ldots,b\}\) & Set \(\{i\in\mathcal{Z},2\leq i\leq b\}\) \\ \([n]\) & \(\{1,\ldots,n\}\) \\ \hline \end{tabular}

## Appendix C Basic Facts

Here we collect some basic facts which are used repeatedly in the proofs.

**Variance decomposition.** Let \(x\in\mathbb{R}^{d}\) be a random vector with finite mean and finite variance. Then for any deterministic vector \(c\in\mathbb{R}^{d}\), we have the identity

\[\mathbb{E}\left[\left\|x-\mathbb{E}\left[x\right]\right\|^{2}\right]=\mathbb{ E}\left[\left\|x-c\right\|^{2}\right]-\left\|\mathbb{E}\left[x\right]-c \right\|^{2}.\] (22)

**Lemma C.1**.: _Consider a sequence \(q_{1},\ldots,q_{n}\in[0,1],\) then_

\[1-\sum_{m=1}^{n}q_{m}\leq\prod_{m=1}^{n}\left(1-q_{m}\right).\]

Proof.: We prove by induction. For \(n=1,\) is it true: \(1-\sum_{m=1}^{1}q_{m}=\prod_{m=1}^{1}\left(1-q_{m}\right).\) Assume that that it is true for \(n-1.\) Then

\[1-\sum_{m=1}^{n-1}q_{m}\leq\prod_{m=1}^{n-1}\left(1-q_{m}\right).\]

Multiply both parts by \(1-q_{n}\in[0,1]\) to obtain

\[\prod_{m=1}^{n}\left(1-q_{m}\right)\geq\left(1-q_{n}\right)\left(1-\sum_{m=1}^{ n-1}q_{m}\right)=1-\sum_{m=1}^{n-1}q_{m}-q_{n}+q_{n}\left(\sum_{m=1}^{n-1}q_{m} \right)\geq 1-\sum_{m=1}^{n}q_{m}\]

since \(q_{m}\in[0,1]\) for all \(m\in[n].\)

## Appendix D Rand\(K\) Compressor

**Definition D.1**.: Assume that \(S\) is a random subset from \([d],|S|=K,\)\(K\in[d].\) A stochastic mapping \(\mathcal{C}\,:\,\mathbb{R}^{d}\times\mathbb{S}_{\nu}\rightarrow\mathbb{R}^{d}\) is Rand\(K\) if

\[\mathcal{C}(x;S)=\frac{d}{K}\sum_{j\in S}x_{j}e_{j},\]

where \(\{e_{i}\}_{i=1}^{d}\) is the standard unit basis.

**Theorem D.2**.: _If \(\mathcal{C}\) is Rand\(K\), then \(\mathcal{C}\in\mathbb{U}\left(\frac{d}{k}-1\right).\)_

One can find the proof in (Beznosikov et al., 2020).

## Appendix E Proofs of the Properties of the Equilibrium Time

**Property 4.1**.: _If all inputs of the equilibrium time are non-negative, then the equilibrium time is well defined._

Proof.:

_(Part 1: \(s^{*}(j)\) is well-defined)_

First, we show that \(s^{*}(j)\) is well-defined for all \(j\in[n].\) We fix \(j\in[n]\) and consider the equation from Def. 3.1:

\[\underbrace{\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{ \pi_{i}}h_{\pi_{i}}\sigma^{2}\omega}{s\times\varepsilon}+\frac{2h_{\pi_{i}} \sigma^{2}}{\varepsilon}}\right)^{-1}}_{\phi(s)}=\underbrace{s}_{\psi(s)}\] (23)

w.r.t \(s\). The function \(\phi(s)\) is a **non-increasing** function for all \(s\geq 0,\) and the function \(\psi(s)\) is an **increasing** function for all \(s\geq 0.\) Let us consider two cases.

1) Exists \(p\leq j\) such that \(\tau_{\pi_{p}}\omega=0\) and \(\frac{h_{\pi_{p}}\sigma^{2}}{\varepsilon}=0,\) then

\[\phi(s)=\left(\sum_{i\neq p}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{i }}h_{\pi_{i}}\sigma^{2}\omega}{s\times\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{ 2}}{\varepsilon}}+\frac{1}{0}\right)^{-1}=(\infty)^{-1}=0.\]

for all \(s\geq 0,\) then the only solution to the equation is \(s=0.\)

2) Otherwise, we have

\[\phi(s)=\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{i }}h_{\pi_{i}}\sigma^{2}\omega}{s\times\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{ 2}}{\varepsilon}}\right)^{-1}\geq\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}} \omega+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}>0\]

for all \(s\geq 0.\) Then \(\phi(0)>0\) (can be equal to \(\infty\)). Using \(\psi(0)=0\) and the monotonicity of the functions, one can show the unique solution (greater zero) exists.

If a permutation \(\pi\) is unique, then the formula \(\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\}\) is well-defined and we can finish the proof.

_(Part 2: non-unique permutation)_

We assume that there exists \(i\in[n]\) such that \(\max\{h_{i},\tau_{i}\}<\infty.\) Otherwise, \(\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\}=\infty\) for any permutation. Next, note that \(s^{*}(j+1)\leq s^{*}(j)\) for all \(j<n\) because

\[\left(\sum_{i=1}^{j+1}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{i}}h_{ \pi_{i}}\sigma^{2}\omega}{s\times\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{ \varepsilon}}\right)^{-1}\leq\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}} \omega+\frac{4\tau_{\pi_{i}}h_{\pi_{i}}\sigma^{2}\omega}{s\times\varepsilon} +\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}\]

for all \(s\geq 0.\) We will use this property later.

Consider that there are two non-equal permutations \(\pi\) and \(\bar{\pi}\) that sort the pairs \((h_{i},\tau_{i})\) by \(\max\{h_{i},\tau_{i}\}\), and there are two corresponding solutions \(s^{*}(j)\) and \(\bar{s}^{*}(j)\). Each permutation divides the pairs \((h_{i},\tau_{i})\) into the same equivalence classes:

\[\underbrace{\max\{h_{\pi_{1}},\tau_{\pi_{1}}\}=\cdots=\max\{h_{\pi_{j_{1}}}, \tau_{\pi_{j_{1}}}\}}_{C_{1}}<\underbrace{\max\{h_{\pi_{j_{1}+1}},\tau_{\pi_{j_ {1}+1}}\}=\cdots=\max\{h_{\pi_{j_{2}}},\tau_{\pi_{j_{2}}}\}}_{C_{2}}<\ldots,\]

\[\underbrace{\max\{h_{\bar{\pi}_{1}},\bar{\tau}_{\bar{\pi}_{1}}\}=\cdots=\max \{h_{\bar{\pi}_{j_{1}}},\bar{\tau}_{\bar{\pi}_{j_{1}}}\}}_{C_{1}}<\underbrace {\max\{h_{\bar{\pi}_{j_{1}+1}},\bar{\tau}_{\bar{\pi}_{j_{1}+1}}\}=\cdots=\max \{h_{\bar{\pi}_{j_{2}}},\bar{\tau}_{\bar{\pi}_{j_{2}}}\}}_{C_{2}}<\ldots.\]

The order within each class can be different, but the elements are the same. Next, since \(s^{*}(j+1)\leq s^{*}(j)\) for all \(j<n,\) we can conclude that the minimum in

\[\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\}\]

is attained for \(j^{*}\) such that \(\max\{h_{\pi_{j^{*}}},\tau_{\pi_{j^{*}}}\}<\max\{h_{\pi_{j^{*}+1}},\tau_{\pi_{ j^{*}+1}}\}\) (\(\max\{h_{\pi_{n+1}},\tau_{\pi_{n+1}}\}\equiv\infty\)). Since \(\max\{h_{\pi_{j^{*}}},\tau_{\pi_{j^{*}}}\}<\max\{h_{\pi_{j^{*}+1}},\tau_{\pi_{ j^{*}+1}}\}\), we have \(\max\{h_{\pi_{j^{*}}},\tau_{\pi_{j^{*}}}\}=\max\{h_{\bar{\pi}_{j^{*}}},\tau_{ \bar{\pi}_{j^{*}}}\}\). Therefore, we obtain

\[\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\}=\max\{\max\{h _{\pi_{j^{*}}},\tau_{\pi_{j^{*}}}\},s^{*}(j^{*})\}=\max\{\max\{h_{\bar{\pi}_{ j^{*}}},\tau_{\bar{\pi}_{j^{*}}}\},s^{*}(j^{*})\}.\]

Also, for all \(j\in[n]\) such that \(\max\{h_{\pi_{j}},\tau_{\pi_{j}}\}<\max\{h_{\pi_{j+1}},\tau_{\pi_{j+1}}\}\), we have

\[\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{j}}h_{\pi_ {j}}\sigma^{2}\omega}{s\times\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{ \varepsilon}}\right)^{-1}=\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\bar{\pi}_{i}} \omega+\frac{4\tau_{\pi_{j}}h_{\pi_{i}}\sigma^{2}\omega}{s\times\varepsilon} +\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}\]

Therefore, we get \(s^{*}(j^{*})=\bar{s}^{*}(j^{*})\) and

\[\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\}=\max\{\max\{h _{\bar{\pi}_{j^{*}}},\tau_{\bar{\pi}_{j^{*}}}\},\bar{s}^{*}(j^{*})\}\geq\min_{ j\in[n]}\max\{\max\{h_{\bar{\pi}_{j}},\tau_{\bar{\pi}_{j}}\},\bar{s}^{*}(j)\}.\]

Using the same reasoning, we can show that

\[\min_{j\in[n]}\max\{\max\{h_{\bar{\pi}_{j}},\tau_{\bar{\pi}_{j}}\},\bar{s}^{*}( j)\}\geq\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\}.\]

It means that \(\min_{j\in[n]}\max\{\max\{h_{\bar{\pi}_{j}},\tau_{\bar{\pi}_{j}}\},\bar{s}^{*}( j)\}=\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\}\), thus the final result of the mapping does not depend on a chosen permutation. 

**Property 6.1**.: _If \(\bar{\omega}\geq\omega\geq 0,\nicefrac{{\bar{s}^{2}}}{{\varepsilon}}\geq \nicefrac{{\bar{s}^{2}}}{{\varepsilon}}\geq 0,\bar{h}_{1}\geq h_{1}\geq 0,\bar{ \tau}_{1}\geq\tau_{1}\geq 0,\ldots,\bar{h}_{n}\geq h_{n}\geq 0,\) and \(\bar{\tau}_{n}\geq\tau_{n}\geq 0,\) then \(t^{*}(\bar{\omega},\nicefrac{{\bar{s}^{2}}}{{\varepsilon}}/\bar{\varepsilon}, \bar{h}_{1},\bar{\tau}_{1}]^{n})\geq t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{ \varepsilon}},[h_{i},\tau_{i}]_{1}^{n})._

Proof.: Assume that \(\pi\) is a permutation that sorts the pairs \((h_{i},\tau_{i})\) by \(\max\{h_{i},\tau_{i}\}\), and \(\bar{\pi}\) is a permutation that sorts the pairs \((\bar{h}_{i},\bar{\tau}_{i})\) by \(\max\{\bar{h}_{i},\bar{\tau}_{i}\}\), then

\[\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{i}}h_{\pi_{i }}\sigma^{2}\omega}{s\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}} \right)^{-1}\leq\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\bar{\pi}_{i}}\bar{\omega}+ \frac{4\bar{\pi}_{j_{i}}h_{\pi_{i}}\sigma^{2}\bar{\omega}}{s\varepsilon}+\frac{2 \bar{h}_{\pi_{i}}\bar{\sigma}^{2}}{\varepsilon}}\right)^{-1}\]

for all \(j\in[n].\) It means \(\bar{s}^{*}(j)\geq s^{*}(j),\) where \(s^{*}(j)\) and \(\bar{s}^{*}(j)\) are the solutions of the equation (7) with the pairs \((h_{i},\tau_{i})\) and \((\bar{h}_{i},\bar{\tau}_{i})\) and corresponding permutations \(\pi\) and \(\bar{\pi}\). Also, we have \(\max\{h_{\pi_{j}},\tau_{\pi_{j}}\}\leq\max\{\bar{h}_{\bar{\pi}_{j}},\bar{\tau}_{ \bar{\pi}_{j}}\}\) for all \(j\in[n].\) Therefore, we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n },\tau_{n}) =\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\} \leq\min_{j\in[n]}\max\{\max\{\bar{h}_{\bar{\pi}_{j}},\bar{\tau}_{\bar{\pi}_{j}}\}, \bar{s}^{*}(j)\}\] \[=t^{*}(\bar{\omega},\nicefrac{{\bar{s}^{2}}}{{\varepsilon}},\bar{h }_{1},\bar{\tau}_{1},\ldots,\bar{h}_{n},\bar{\tau}_{n}).\]

**Property E.1**.: _For all \(c\in(0,1]\) and \(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}\geq 0,\) we have_

\[t^{*}(c\times\omega,c\times\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1}, \tau_{1},\ldots,h_{n},\tau_{n})\geq c\times t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{ \varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}).\]Proof.: Using the definition of the equilibrium time, we have

\[t^{*}(\omega,{{}^{\sigma^{2}}\!/\varepsilon},h_{1},\tau_{1},\ldots,h_{n},\tau_{n} )=\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\},\]

where \(s^{*}(j)\) is the solution of

\[\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{i}}h_{ \pi_{i}}\sigma^{2}\omega}{s\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{ \varepsilon}}\right)^{-1}=s,\] (24)

and

\[t^{*}(c\times\omega,c\times{{}^{\sigma^{2}}\!/\varepsilon},h_{1},\tau_{1}, \ldots,h_{n},\tau_{n})=\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s ^{*}_{c}(j)\},\]

where \(s^{*}_{c}(j)\) is the solution of

\[\left(\sum_{i=1}^{j}\frac{1}{2c\tau_{\pi_{i}}\omega+\frac{4c^{2}\tau_{\pi_{i}} h_{\pi_{i}}\sigma^{2}\omega}{s\varepsilon}+\frac{2ch_{\pi_{i}}\sigma^{2}}{ \varepsilon}}\right)^{-1}=s.\]

Using simple algebra, we obtain

\[\left(\sum_{i=1}^{j}\frac{1}{2c\tau_{\pi_{i}}\omega+\frac{4c^{2}\tau_{\pi_{i}} h_{\pi_{i}}\sigma^{2}\omega}{s\varepsilon}+\frac{2ch_{\pi_{i}}\sigma^{2}}{ \varepsilon}}\right)^{-1}=c\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}}\omega +\frac{4\tau_{\pi_{i}}h_{\pi_{i}}\sigma^{2}\omega}{\frac{\varepsilon}{ \varepsilon}\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^ {-1}.\]

Thus, \(s^{*}_{c}(j)\) is the solution of

\[\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{i}}h_{ \pi_{i}}\sigma^{2}\omega}{\frac{\varepsilon}{\varepsilon}\varepsilon}+\frac{2 h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}=\frac{s}{c}\] (25)

Comparing (24) and (25), one can see that \(s^{*}_{c}(j)=c\times s^{*}(j)\) for all \(j\in[n]\). Using this and \(c\in(0,1]\), we get

\[t^{*}(c\times\omega,c\times{{}^{\sigma^{2}}\!/\varepsilon},h_{1},\tau_{1}, \ldots,h_{n},\tau_{n})=\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},c \times s^{*}(j)\}\]

\[\geq c\times\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\}= c\times t^{*}(\omega,{{}^{\sigma^{2}}\!/\varepsilon},h_{1},\tau_{1},\ldots,h_{n}, \tau_{n}).\]

**Property E.2**.: _For all \(c\geq 1\) and \(\omega,{{}^{\sigma^{2}}\!/\varepsilon},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}\geq 0,\) we have_

\[t^{*}(c\times\omega,c\times{{}^{\sigma^{2}}\!/\varepsilon},h_{1},\tau_{1}, \ldots,h_{n},\tau_{n})\leq c\times t^{*}(\omega,{{}^{\sigma^{2}}\!/\varepsilon},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}).\]

Proof.: The proof of this property repeats the proof of Property E.1 up to the last inequality. Using \(c\geq 1\), we get

\[t^{*}(c\times\omega,c\times{{}^{\sigma^{2}}\!/\varepsilon},h_{1}, \tau_{1},\ldots,h_{n},\tau_{n})=\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{ \pi_{j}}\},c\times s^{*}(j)\}\] \[\leq c\times\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}} \},s^{*}(j)\}=c\times t^{*}(\omega,{{}^{\sigma^{2}}\!/\varepsilon},h_{1},\tau_{ 1},\ldots,h_{n},\tau_{n}).\]

**Property E.3**.: _For all \(c\in(0,1]\) and \(\omega,{{}^{\sigma^{2}}\!/\varepsilon},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}\geq 0,\) we have \(t^{*}(c\times\omega,{{}^{\sigma^{2}}\!/\varepsilon},[h_{i},\tau_{i}]_{1}^{n}) \geq c\times t^{*}(\omega,{{}^{\sigma^{2}}\!/\varepsilon},[h_{i},\tau_{i}]_{1}^ {n})\geq c\times t^{*}(\omega,{{}^{\sigma^{2}}\!/\varepsilon},[h_{i},\tau_{i}]_{1 }^{n})\geq c\times t^{*}(\omega,{{}^{\sigma^{2}}\!/\varepsilon},[h_{i},\tau_{i} ]_{1}^{n}).\) For all \(c\geq 1\) and \(\omega,{{}^{\sigma^{2}}\!/\varepsilon},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}\geq 0,\) we have \(t^{*}(c\times\omega,{{}^{\sigma^{2}}\!/\varepsilon},[h_{i},\tau_{i}]_{1}^{n}) \leq c\times t^{*}(\omega,{{}^{\sigma^{2}}\!/\varepsilon},[h_{i},\tau_{i}]_{1 }^{n})\) and \(t^{*}(\omega,c\times{{}^{\sigma^{2}}\!/\varepsilon},[h_{i},\tau_{i}]_{1}^{n}) \leq c\times t^{*}(\omega,{{}^{\sigma^{2}}\!/\varepsilon},[h_{i},\tau_{i}]_{1}^ {n})\leq c\times t^{*}(\omega,{{}^{\sigma^{2}}\!/\varepsilon},[h_{i},\tau_{i}]_{1 }^{n})\)

_Remark E.4_.: We can obtain stronger inequalities. See Properties E.1 and E.2.

Proof.: For all \(c\in(0,1]\), using Properties 6.1 and E.1, we have

\[t^{*}(c\times\omega,\sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1}, \ldots,h_{n},\tau_{n})\geq t^{*}(c\times\omega,\sideset{}{{}^{\sigma^{2}}}{/{ \varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n})\geq c\times t^{*}(\omega, \sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{ n})\]

and

\[t^{*}(\omega,\sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1}, \ldots,h_{n},\tau_{n})\geq t^{*}(c\times\omega,\sideset{}{{}^{\sigma^{2}}}{/{ \varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n})\geq c\times t^{*}(\omega, \sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{ n}).\]

For all \(c\geq 1\), using Properties 6.1 and E.2, we have

\[t^{*}(c\times\omega,\sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1 },\ldots,h_{n},\tau_{n})\leq t^{*}(c\times\omega,\sideset{}{{}^{\sigma^{2}}}{/{ \varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n})\leq c\times t^{*}(\omega, \sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{ n})\]

and

\[t^{*}(\omega,\sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1}, \ldots,h_{n},\tau_{n})\leq t^{*}(c\times\omega,\sideset{}{{}^{\sigma^{2}}}{/{ \varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n})\leq c\times t^{*}(\omega, \sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{ n}).\]

**Property E.5**.: _For all \(c\geq 0\) and \(\omega,\sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n}, \tau_{n}\geq 0,\) we have_

\[t^{*}(\omega,\sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},c\times h_{1},c \times\tau_{1},\ldots,c\times h_{n},c\times\tau_{n})=c\times t^{*}(\omega, \sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{ n}).\]

Proof.: For \(c=0\), it it clear. Assume that \(c>0\). Using the definition of the equilibrium time, we have

\[t^{*}(\omega,\sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1}, \ldots,h_{n},\tau_{n})=\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\}, s^{*}(j)\},\]

where \(s^{*}(j)\) is the solution of

\[\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{i}}h_{ \pi_{j}}\sigma^{2}\omega}{s\varepsilon}+\frac{2h_{\pi_{j}}\sigma^{2}}{ \varepsilon}}\right)^{-1}=s,\] (26)

and

\[t^{*}(\omega,\sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},c\times h_{1},c \times\tau_{1},\ldots,c\times h_{n},c\times\tau_{n})=\min_{j\in[n]}\max\{\max \{c\times h_{\pi_{j}},c\times\tau_{\pi_{j}}\},s^{*}_{c}(j)\},\]

where \(s^{*}_{c}(j)\) is the solution of

\[\left(\sum_{i=1}^{j}\frac{1}{2c\tau_{\pi_{i}}\omega+\frac{4c^{2}\tau_{\pi_{i}} h_{\pi_{j}}\sigma^{2}\omega}{s\varepsilon}+\frac{2ch_{\pi_{j}}\sigma^{2}}{ \varepsilon}}\right)^{-1}=s.\]

For both cases, we can take the same permutation \(\pi\). Using simple algebra, we obtain

\[\left(\sum_{i=1}^{j}\frac{1}{2c\tau_{\pi_{i}}\omega+\frac{4c^{2}\tau_{\pi_{i}} h_{\pi_{i}}\sigma^{2}\omega}{s\varepsilon}+\frac{2ch_{\pi_{i}}\sigma^{2}}{ \varepsilon}}\right)^{-1}=c\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}}\omega+ \frac{4\tau_{\pi_{i}}h_{\pi_{i}}\sigma^{2}\omega}{\frac{1}{\varepsilon} \varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}.\]

Thus, \(s^{*}_{c}(j)\) is the solution of

\[\left(\sum_{i=1}^{j}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{i}}h_{\pi_{ i}}\sigma^{2}\omega}{\frac{1}{\varepsilon}\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{ \varepsilon}}\right)^{-1}=\frac{s}{c}\] (27)

Comparing (26) and (27), one can see that \(s^{*}_{c}(j)=c\times s^{*}(j)\) for all \(j\in[n]\). Using this, we get

\[t^{*}(\omega,\sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},c\times h_{1},c \times\tau_{1},\ldots,c\times h_{n},c\times\tau_{n})=\min_{j\in[n]}\max\{\max\{c \times h_{\pi_{j}},c\times\tau_{\pi_{j}}\},c\times s^{*}(j)\}\]

**Property E.6**.: _We fix a nonempty subset \(S=\{k_{1},\ldots,k_{m}\}\) from the set \([n]\) with a size \(m\geq 1\). For all \(\omega,\sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n}, \tau_{n}\geq 0,\) we have_

\[t^{*}(\omega,\sideset{}{{}^{\sigma^{2}}}{/{\varepsilon}},h_{1},\tau_{1}, \ldots,h_{n},\tau_{n})\leq t^{*}(\omega,\sideset{}{{}^{\sigma^{2}}}{/{ \varepsilon}},h_{k_{1}},\tau_{k_{1}},\ldots,h_{k_{m}},\tau_{k_{m}}).\]Proof.: Using Property 6.1 with \(\bar{\tau}_{i}=\infty\) and \(\bar{h}_{i}=\infty\) for all \(i\not\in S\) and \(\bar{\tau}_{i}=\tau_{i}\) and \(\bar{h}_{i}=h_{i}\) for all \(i\in S,\) we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\dots,h_{n },\tau_{n})\leq t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},\bar{h}_{1 },\bar{\tau}_{1},\dots,\bar{h}_{n},\bar{\tau}_{n}).\]

Next, using Def. 3.1, we obtain

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},\bar{h}_{1},\bar{\tau}_{ 1},\dots,\bar{h}_{n},\bar{\tau}_{n})=\min_{j\in[n]}\max\{\max\{\bar{h}_{\pi_{j} },\bar{\tau}_{\pi_{j}}\},s^{*}(j)\},\]

where \(s^{*}(j)\) is the solution of

\[\left(\sum_{i=1}^{j}\frac{1}{2\bar{\tau}_{\pi_{i}}\omega+\frac{4\bar{\tau}_{ \pi_{i}}\bar{h}_{\pi_{i}}\nicefrac{{\sigma^{2}\omega}}{{\varepsilon}}}+\frac{ 2\bar{h}_{\pi_{i}}\nicefrac{{\sigma^{2}}}{{\varepsilon}}}{{\varepsilon}}} \right)^{-1}=s\] (28)

w.r.t \(s\) for all \(j\in[n],\) and \(\pi\) is a permutation that sorts \(\max\{\bar{h}_{i},\bar{\tau}_{i}\}\) in such a way that the set \(\{\pi_{1},\dots,\pi_{m}\}\) equals to the set \(\{k_{1},\dots,k_{m}\}\) (the order of elements can be different). Such permutation exists because \(\max\{\bar{h}_{i},\bar{\tau}_{i}\}=\infty\) for all \(i\not\in S.\) Using \(\max\{\bar{h}_{\pi_{i}},\bar{\tau}_{\pi_{i}}\}=\infty\) for all \(i>m,\) we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},\bar{h}_{1},\bar{\tau}_{ 1},\dots,\bar{h}_{n},\bar{\tau}_{n})=\min_{j\in[m]}\max\{\max\{\bar{h}_{\pi_{ j}},\bar{\tau}_{\pi_{j}}\},s^{*}(j)\}.\] (29)

By the construction of \(\pi,\) (28) and (29) depend only on the elements from \(S.\) Thus, we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_ {1},\dots,h_{n},\tau_{n}) \leq t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},\bar{h}_{ 1},\bar{\tau}_{1},\dots,\bar{h}_{n},\bar{\tau}_{n})=t^{*}(\omega,\nicefrac{{ \sigma^{2}}}{{\varepsilon}},\bar{h}_{k_{1}},\bar{\tau}_{k_{1}},\dots,\bar{h}_{k _{m}},\bar{\tau}_{k_{m}})\] \[=t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{k_{1}}, \tau_{k_{1}},\dots,h_{k_{m}},\tau_{k_{m}}).\]

**Property E.7**.: _For all \(\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\dot{\tau}_{1},\dots,h_{n},\dot {\tau}_{n}\geq 0,\) we have_

\[12t^{*}\left(0,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},d \dot{\tau}_{1},\dots,h_{n},d\dot{\tau}_{n}\right)\] \[\geq t^{*}\left(d-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1}, \dot{\tau}_{1},\dots,h_{n},\dot{\tau}_{n}\right).\]

Proof.: For \(d=1,\) it is clear. Assume that \(d>1.\) Using the definition of \(t^{*},\) we have

\[t^{*}\left(0,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},d\dot{\tau}_{1}, \dots,h_{n},d\dot{\tau}_{n}\right)\geq\min_{j\in[n]}\max\left\{\max\{h_{\bar{ \pi}_{j}},d\dot{\tau}_{\bar{\pi}_{j}}\},\frac{\sigma^{2}}{{\varepsilon}}\left( \sum_{i=1}^{j}\frac{1}{h_{\bar{\pi}_{i}}}\right)^{-1}\right\}\] (30)

where \(\bar{\pi}\) is a permutation that sorts \(\max\{h_{i},d\dot{\tau}_{i}\}.\) Assume that \(j^{*}\) is the minimal index that minimizes (30). Then

\[t^{*}\left(0,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},d\dot{\tau}_{1}, \dots,h_{n},d\dot{\tau}_{n}\right)\geq\max\left\{\max\{h_{\bar{\pi}_{j^{*}}},d \dot{\tau}_{\bar{\pi}_{j^{*}}}\},\frac{\sigma^{2}}{{\varepsilon}}\left(\sum_{i=1 }^{j^{*}}\frac{1}{h_{\bar{\pi}_{i}}}\right)^{-1}\right\}.\] (31)

Let us define

\[I_{*}:=t^{*}(d-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\dot{\tau}_{1}, \dots,h_{n},\dot{\tau}_{n}).\]

Using Property E.6, we have

\[I_{*}\leq t^{*}(d-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{\bar{\pi}_{1}}, \dot{\tau}_{\bar{\pi}_{1}},\dots,h_{\bar{\pi}_{j^{*}}},\dot{\tau}_{\bar{\pi}_{ j^{*}}}).\]

Using Def. 3.1 of \(t^{*},\) we get

\[I_{*}\leq\max\{\max_{j\in[j^{*}]}\max\{h_{\bar{\pi}_{j}},\dot{\tau}_{\bar{\pi}_{ j}}\},s^{*}\},\] (32)

where \(s^{*}\) is the solution of

\[\left(\sum_{i=1}^{j^{*}}\frac{1}{2\dot{\tau}_{\bar{\pi}_{i}}(d-1)+\frac{4\bar{ \tau}_{\pi_{i}}h_{\bar{\pi}_{j}}\sigma^{2}(d-1)}{s\times\varepsilon}+\frac{2h_{ \bar{\pi}_{i}}\sigma^{2}}{{\varepsilon}}}\right)^{-1}=s.\] (33)Let us take \(s^{\prime}=12\max\left\{(d-1)\max_{j\in[j^{*}]}\dot{\tau}_{\hat{\pi}_{j}},\frac{ \sigma^{2}}{\varepsilon}\left(\sum_{i=1}^{j^{*}}\frac{1}{h_{\pi_{i}}}\right)^{- 1}\right\}.\) Since \(s^{\prime}\geq(d-1)\max_{j\in[j^{*}]}\dot{\tau}_{\hat{\pi}_{j}}\), we have

\[\left(\sum_{i=1}^{j^{*}}\frac{1}{2\dot{\tau}_{\hat{\pi}_{i}}(d-1) +\frac{4\hat{\tau}_{\hat{\pi}_{j}}h_{\hat{\pi}_{j}}\sigma^{2}(d-1)}{s^{\prime} \times\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1} \leq\left(\sum_{i=1}^{j^{*}}\frac{1}{2\dot{\tau}_{\hat{\pi}_{i}}( d-1)+\frac{4h_{\pi_{i}}\sigma^{2}}{\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{ \varepsilon}}\right)^{-1}\] \[\leq\left(\sum_{i=1}^{j^{*}}\frac{1}{2\dot{\tau}_{\hat{\pi}_{i}} (d-1)+\frac{6h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}\] \[\leq 12\left(\sum_{i=1}^{j^{*}}\min\left\{\frac{1}{\dot{\tau}_{ \hat{\pi}_{i}}(d-1)},\frac{1}{\frac{h_{\pi_{i}}\sigma^{2}}{\varepsilon}} \right\}\right)^{-1}.\]

If there exists \(p\in[j^{*}]\) such that \(\frac{1}{\dot{\tau}_{\hat{\pi}_{p}}(d-1)}<\frac{1}{\frac{h_{\pi_{p}}\sigma^{2} }{\varepsilon}},\) then

\[\sum_{i=1}^{j^{*}}\min\left\{\frac{1}{\dot{\tau}_{\hat{\pi}_{i}}(d-1)},\frac{1 }{\frac{h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right\}\geq\frac{1}{\dot{\tau}_{ \hat{\pi}_{p}}(d-1)}\]

and

\[\left(\sum_{i=1}^{j^{*}}\frac{1}{2\dot{\tau}_{\hat{\pi}_{i}}(d-1)+\frac{4\dot{ \tau}_{\hat{\pi}_{i}}h_{\hat{\pi}_{i}}\sigma^{2}(d-1)}{s^{\prime}\times \varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}\leq 12(d-1) \dot{\tau}_{\hat{\pi}_{p}}\leq 12(d-1)\max_{j\in[j^{*}]}\dot{\tau}_{\hat{\pi}_{j}}.\]

Otherwise, we have

\[\sum_{i=1}^{j^{*}}\min\left\{\frac{1}{\dot{\tau}_{\hat{\pi}_{i}}(d-1)},\frac{ 1}{\frac{h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right\}=\sum_{i=1}^{j^{*}}\frac {1}{\frac{h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\]

and

\[\left(\sum_{i=1}^{j^{*}}\frac{1}{2\dot{\tau}_{\hat{\pi}_{i}}(d-1)+\frac{4\dot{ \tau}_{\hat{\pi}_{i}}h_{\hat{\pi}_{i}}\sigma^{2}(d-1)}{s^{\prime}\times \varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}\leq 12\left( \sum_{i=1}^{j^{*}}\frac{1}{\frac{h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^ {-1}=12\frac{\sigma^{2}}{\varepsilon}\left(\sum_{i=1}^{j^{*}}\frac{1}{h_{\hat{ \pi}_{i}}}\right)^{-1}.\]

Considering both cases, we have

\[\left(\sum_{i=1}^{j^{*}}\frac{1}{2\dot{\tau}_{\hat{\pi}_{i}}(d-1)+\frac{4\dot{ \tau}_{\hat{\pi}_{i}}h_{\hat{\pi}_{i}}\sigma^{2}(d-1)}{s^{\prime}\times \varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}\leq 12\max \left\{(d-1)\max_{j\in[j^{*}]}\dot{\tau}_{\hat{\pi}_{j}},\frac{\sigma^{2}}{ \varepsilon}\left(\sum_{i=1}^{j^{*}}\frac{1}{h_{\hat{\pi}_{i}}}\right)^{-1} \right\}=s^{\prime}.\]

It means that \(s^{*}\leq s^{\prime}\) because \(s^{*}\) is the solution of (33). Using (32), we get

\[I_{*}\leq 12\max\left\{\max_{j\in[j^{*}]}\max\{h_{\hat{\pi}_{j}},\dot{\tau}_{ \hat{\pi}_{j}}\},\max\left\{(d-1)\max_{j\in[j^{*}]}\dot{\tau}_{\hat{\pi}_{j}}, \frac{\sigma^{2}}{\varepsilon}\left(\sum_{i=1}^{j^{*}}\frac{1}{h_{\hat{\pi}_{i} }}\right)^{-1}\right\}\right\}.\]

Using \(d\geq 1\) and \(d\dot{\tau}_{\hat{\pi}_{j}}\leq\max\{h_{\hat{\pi}_{j}},d\dot{\tau}_{\hat{\pi}_{j }}\},\) we get

\[I_{*}\leq 12\max\left\{\max_{j\in[j^{*}]}\max\{h_{\hat{\pi}_{j}},d \dot{\tau}_{\hat{\pi}_{j}}\},\max\left\{\max_{j\in[j^{*}]}\max\{h_{\hat{\pi}_{j }},d\dot{\tau}_{\hat{\pi}_{j}}\},\frac{\sigma^{2}}{\varepsilon}\left(\sum_{i=1}^ {j^{*}}\frac{1}{h_{\hat{\pi}_{i}}}\right)^{-1}\right\}\right\}\] \[\leq 12\max\left\{\max_{j\in[j^{*}]}\max\{h_{\hat{\pi}_{j}},d\dot{ \tau}_{\hat{\pi}_{j}}\},\frac{\sigma^{2}}{\varepsilon}\left(\sum_{i=1}^{j^{*}} \frac{1}{h_{\hat{\pi}_{i}}}\right)^{-1}\right\}.\]Due to \(\max_{j\in[j^{*}]}\max\{h_{\bar{\tau}_{j}},d\hat{\tau}_{\bar{\tau}_{j^{*}}}\}= \max\{h_{\bar{\tau}_{j^{*}}},d\hat{\tau}_{\bar{\tau}_{j^{*}}}\}\) and (31), we obtain

\[I_{*} =t^{*}(d-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\hat{\tau }_{1},\ldots,h_{n},\hat{\tau}_{n})\leq 12\max\left\{\max\{h_{\bar{\tau}_{j^{*}}},d \hat{\tau}_{\bar{\tau}_{j^{*}}}\},\frac{\sigma^{2}}{\varepsilon}\left(\sum_{i=1 }^{j^{*}}\frac{1}{h_{\bar{\tau}_{i}}}\right)^{-1}\right\}\] \[\leq 12t^{*}\left(0,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1}, d\hat{\tau}_{1},\ldots,h_{n},d\hat{\tau}_{n}\right).\]

**Property 6.2**.: _For all \(K\in[1,d],\nicefrac{{\sigma^{2}}}{{\varepsilon}}\), \(h_{1}\), \(\hat{\tau}_{1},\ldots,h_{n},\hat{\tau}_{n}\geq 0,\) we have \(24\times t^{*}\left(\nicefrac{{d}}{{K}}-1,\nicefrac{{\sigma^{2}}}{{ \varepsilon}},h_{1},K\hat{\tau}_{1},\ldots,h_{n},K\hat{\tau}_{n}\right)\geq t ^{*}\left(d-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\hat{\tau}_{1}, \ldots,h_{n},\hat{\tau}_{n}\right).\)_

Proof.: (Part 1: \(K\leq\frac{d+1}{2}\))

For all \(K\leq\frac{d+1}{2},\) we have

\[t^{*}\left(\frac{d}{K}-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},K\hat{ \tau}_{1},\ldots,h_{n},K\hat{\tau}_{n}\right)=\min_{j\in[n]}\max\{\max\{h_{\pi_ {j}},K\hat{\tau}_{\pi_{j}}\},s^{*}(j)\},\] (34)

where \(s^{*}(j)\) is the solution of

\[\left(\sum_{i=1}^{j}\frac{1}{2K\hat{\tau}_{\pi_{i}}\left(\frac{d}{K}-1\right) +\frac{4K\hat{\tau}_{\pi_{i}}h_{\pi_{i}}\sigma^{2}\left(\frac{d}{K}-1\right) }{s\times\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}=s,\]

and \(\pi\) is a permutation that sorts \(\max\{h_{j},K\hat{\tau}_{j}\}.\) Also, assume that \(j^{*}\) is a minimizer in (34). For all \(j\in[n],\) we get

\[s^{*}(j)=\left(\sum_{i=1}^{j}\frac{1}{2\hat{\tau}_{\pi_{i}}\left( d-K\right)+\frac{4K\hat{\tau}_{\pi_{i}}h_{\pi_{i}}\sigma^{2}(d-K)}{s^{*}(j) \times\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}.\]

Since \(K\leq\frac{d+1}{2},\) we have

\[s^{*}(j)\geq\frac{1}{2}\left(\sum_{i=1}^{j}\frac{1}{2\hat{\tau}_{\pi_{i}}(d-1) +\frac{4\hat{\tau}_{\pi_{i}}h_{\pi_{i}}\sigma^{2}(d-1)}{s^{*}(j)\times \varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}\]

and

\[2\times s^{*}(j)\geq\left(\sum_{i=1}^{j}\frac{1}{2\hat{\tau}_{\pi_{i}}(d-1)+ \frac{4\hat{\tau}_{\pi_{i}}h_{\pi_{i}}\sigma^{2}(d-1)}{2\times s^{*}(j)\times \varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}.\] (35)

At the same time, using Property E.6, we have

\[t^{*}\left(d-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1}, \hat{\tau}_{1},\ldots,h_{n},\hat{\tau}_{n}\right)\leq t^{*}\left(d-1,\nicefrac{{ \sigma^{2}}}{{\varepsilon}},h_{\pi_{1}},\hat{\tau}_{\pi_{1}},\ldots,h_{\pi_{j ^{*}}},\hat{\tau}_{\pi_{j^{*}}}\right)\leq\max\{\max_{j\in[j^{*}]}\max\{h_{ \hat{\tau}_{j}},\hat{\tau}_{\bar{\tau}_{j}}\},s^{\prime}(j^{*})\},\] (36)

where \(s^{\prime}(j^{*})\) is the solution of

\[\left(\sum_{i=1}^{j^{*}}\frac{1}{2\hat{\tau}_{\pi_{i}}\left(d-1 \right)+\frac{4\hat{\tau}_{\pi_{i}}h_{\pi_{i}}\sigma^{2}(d-1)}{s\times \varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}}\right)^{-1}=s.\]

From (35), we can conclude that \(2\times s^{*}(j^{*})\geq s^{\prime}(j^{*}).\) Using this and (36), we obtain

\[t^{*}\left(d-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1}, \hat{\tau}_{1},\ldots,h_{n},\hat{\tau}_{n}\right)\leq\max\{\max_{j\in[j^{*}]} \max\{h_{\bar{\tau}_{j}},\hat{\tau}_{\bar{\tau}_{j}}\},2s^{*}(j^{*})\}\]\[\leq 2\max\{\max_{j\in[^{*}]}\max\{h_{\bar{\tau}_{j}},\hat{\tau}_{\bar{ \tau}_{j}}\},s^{*}(j^{*})\}\] \[\leq 2\max\{\max_{j\in[^{*}]}\max\{h_{\bar{\tau}_{j}},K\hat{\tau}_{ \bar{\tau}_{j}}\},s^{*}(j^{*})\}.\]

Note that \(\max_{j\in[^{*}]}\max\{h_{\bar{\tau}_{j}},K\hat{\tau}_{\bar{\tau}_{j}}\}=\max\{h _{\bar{\tau}_{j^{*}}},K\hat{\tau}_{\bar{\tau}_{j^{*}}}\},\) thus

\[t^{*}\left(d-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\hat{\tau}_{1}, \ldots,h_{n},\hat{\tau}_{n}\right)\leq 2\max\{\max\{\max\{h_{\bar{\tau}_{j^{*}}},K \hat{\tau}_{\bar{\tau}_{j^{*}}}\},s^{*}(j^{*})\}=2t^{*}\left(\frac{d}{K}-1, \nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},K\hat{\tau}_{1},\ldots,h_{n},K \hat{\tau}_{n}\right).\]

(Part 2: \(K>\frac{d+1}{2}\))

For all \(K>\frac{d+1}{2}\), using Property 6.1, we get

\[t^{*}\left(\frac{d}{K}-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},K\hat{ \tau}_{1},\ldots,h_{n},K\hat{\tau}_{n}\right)\geq t^{*}\left(0,\nicefrac{{ \sigma^{2}}}{{\varepsilon}},\frac{1}{2}h_{1},\frac{d}{2}\hat{\tau}_{1},\ldots, \frac{1}{2}h_{n},\frac{d}{2}\hat{\tau}_{n}\right).\]

Next, using Property E.5, we have

\[t^{*}\left(\frac{d}{K}-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},K\hat{ \tau}_{1},\ldots,h_{n},K\hat{\tau}_{n}\right)\geq\frac{1}{2}t^{*}\left(0, \nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},d\hat{\tau}_{1},\ldots,h_{n},d \hat{\tau}_{n}\right).\]

It is left to use Property E.7 to get

\[t^{*}\left(\frac{d}{K}-1,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},K\hat{ \tau}_{1},\ldots,h_{n},K\hat{\tau}_{n}\right)\geq\frac{1}{24}t^{*}\left(d-1, \nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\hat{\tau}_{1},\ldots,h_{n},\hat {\tau}_{n}\right).\]

## Appendix F Derivations of the Examples for the Equilibrium Time

_Example 6.3_.: **[Infinitely Fast Worker]** If exists \(j\in[n]\) such that \(\tau_{j}=0\) and \(h_{j}=0,\) then \(t^{*}=0\).

Proof.: Let us take a permutation \(\pi\) where \(\pi_{1}=j.\) Such a permutation exists because \(\max\{h_{j},\tau_{j}\}=0.\) By the definition of \(t^{*}\), we have

\[\begin{split} t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon }},h_{1},\tau_{1},\ldots,h_{n},\tau_{n})&=\min_{j\in[n]}\max\{ \max\{h_{\tau_{j}},\tau_{\bar{\tau}_{j}}\},s^{*}(j)\}\\ &\leq\max\{\max\{h_{\pi_{1}},\tau_{\pi_{1}}\},s^{*}(1)\}\\ &=\max\{0,s^{*}(1)\},\end{split}\] (37)

where \(s^{*}(1)\) is the solution of

\[\left(\sum_{i=1}^{1}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{i}}h_{\pi _{i}}\sigma^{2}\omega}{s\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{ \varepsilon}}\right)^{-1}=s.\]

Since

\[\left(\sum_{i=1}^{1}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{i}}h_{\pi _{i}}\sigma^{2}\omega}{s\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{ \varepsilon}}\right)^{-1}=\left(\sum_{i=1}^{1}\frac{1}{0}\right)^{-1}=\left( \infty\right)^{-1}=0,\]

we obtain \(s^{*}(1)=0.\) We substitute it to (37) to get \(t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n },\tau_{n})=0.\) 

_Example 6.4_.: **[Infinitely Slow Workers]** If \(\tau_{i}=\infty\) and \(h_{i}=\infty\) for all \(i\in[n],\) then \(t^{*}=\infty.\)

Proof.: By the definition of \(t^{*}\), we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_{ n},\tau_{n})=\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\}=\min_{j\in[n]} \max\{\infty,s^{*}(j)\}=\infty.\]_Example 6.5_.: **[Equal Performance]** If \(\tau_{i}=\tau\) and \(h_{i}=h\) for all \(i\in[n]\), then10

Footnote 10: From the proof, it is clear that the result is tight up to a constant factor.

\[t^{*}\leq 6\max\left\{h,\tau,\frac{\tau\omega}{n},\frac{h\sigma^{2}}{\varepsilon}, \sqrt{\frac{\tau h\sigma^{2}\omega}{n\varepsilon}}\right\}.\] (14)

Proof.: By the definition, we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_ {n},\tau_{n})=\min_{j\in[n]}\max\{\max\{h,\tau\},s^{*}(j)\}=\max\{\max\{h,\tau \},\min_{j\in[n]}s^{*}(j)\}=\max\{\max\{h,\tau\},s^{*}\}\] (38)

where \(s^{*}\) is the solution of

\[\left(\sum_{i=1}^{n}\frac{1}{2\tau\omega+\frac{4\tau h\sigma^{2}\omega}{s \varepsilon}+\frac{2h\sigma^{2}}{\varepsilon}}\right)^{-1}=s.\]

Since

\[\left(\sum_{i=1}^{n}\frac{1}{2\tau\omega+\frac{4\tau h\sigma^{2}\omega}{s \varepsilon}+\frac{2h\sigma^{2}}{\varepsilon}}\right)^{-1}=\left(\frac{n}{2 \tau\omega+\frac{4\tau h\sigma^{2}\omega}{s\varepsilon}+\frac{2h\sigma^{2}}{ \varepsilon}}\right)^{-1}=\frac{2\tau\omega}{n}+\frac{4\tau h\sigma^{2} \omega}{sn\varepsilon}+\frac{2h\sigma^{2}}{n\varepsilon},\]

we have to solve and find the non-negative solution of the quadratic equation

\[s^{2}-s\left(\frac{2\tau\omega}{n}+\frac{2h\sigma^{2}}{n\varepsilon}\right)- \frac{4\tau h\sigma^{2}\omega}{n\varepsilon}=0.\]

The solution is

\[s^{*}=\left(\frac{\tau\omega}{n}+\frac{h\sigma^{2}}{n\varepsilon}\right)+\sqrt {\left(\frac{\tau\omega}{n}+\frac{h\sigma^{2}}{n\varepsilon}\right)^{2}+\frac {4\tau h\sigma^{2}\omega}{n\varepsilon}}\leq 2\left(\frac{\tau\omega}{n}+\frac{h \sigma^{2}}{n\varepsilon}+\sqrt{\frac{\tau h\sigma^{2}\omega}{n\varepsilon}} \right).\]

Therefore, we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_ {1},\ldots,h_{n},\tau_{n}) \leq\max\left\{\max\{h,\tau\},2\left(\frac{\tau\omega}{n}+\frac{ h\sigma^{2}}{n\varepsilon}+\sqrt{\frac{\tau h\sigma^{2}\omega}{n\varepsilon}} \right)\right\}\] \[\leq 6\max\left\{h,\tau,\frac{\tau\omega}{n},\frac{h\sigma^{2}}{n \varepsilon},\sqrt{\frac{\tau h\sigma^{2}\omega}{n\varepsilon}}\right\}.\]

_Example 6.6_.: **[Infinitely Fast Communication]** If \(\tau_{i}=0\) for all \(i\in[n]\), then

\[t^{*}\leq 2\min_{m\in[n]}\max\left\{h_{\pi_{m}},\frac{\sigma^{2}}{ \varepsilon}\left(\sum_{i=1}^{m}\frac{1}{h_{\pi_{i}}}\right)^{-1}\right\}= \Theta\left(\min_{m\in[n]}\left(\frac{1}{m}\sum_{i=1}^{m}\frac{1}{h_{\pi_{i}}} \right)^{-1}\left(1+\frac{\sigma^{2}}{m\varepsilon}\right)\right),\] (15)

where \(\pi\) is a permutation that sorts \(\{h_{i}\}_{i=1}^{n}\).

Proof.: By the definition, we have \(t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_ {n},\tau_{n})=\min_{j\in[n]}\max\{h_{\pi_{j}},s^{*}(j)\}\), where \(s^{*}(j)\) is the solution of

\[\left(\sum_{i=1}^{j}\frac{1}{\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon}} \right)^{-1}=s.\]

Therefore, we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_ {n},\tau_{n}) \leq 2\min_{j\in[n]}\max\left\{h_{\pi_{j}},\frac{\sigma^{2}}{ \varepsilon}\left(\sum_{i=1}^{j}\frac{1}{h_{\pi_{i}}}\right)^{-1}\right\}= \Theta\left(\min_{j\in[n]}\left(h_{\pi_{j}}+\frac{\sigma^{2}}{\varepsilon} \left(\sum_{i=1}^{j}\frac{1}{h_{\pi_{i}}}\right)^{-1}\right)\right).\]

Using Lemma F.1, we obtain

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_ {n},\tau_{n}) \leq 2\min_{j\in[n]}\max\left\{h_{\pi_{j}},\frac{\sigma^{2}}{ \varepsilon}\left(\sum_{i=1}^{j}\frac{1}{h_{\pi_{i}}}\right)^{-1}\right\}= \Theta\left(\min_{j\in[n]}\left(j+\frac{\sigma^{2}}{\varepsilon}\right)\left( \sum_{i=1}^{j}\frac{1}{h_{\pi_{i}}}\right)^{-1}\right).\]

**Lemma F.1**.: _Let us consider the two functions_

\[g(j):=h_{j}+a\left(\sum_{i=1}^{j}\frac{1}{h_{i}}\right)^{-1},\qquad p(j):=(j+a) \left(\sum_{i=1}^{j}\frac{1}{h_{i}}\right)^{-1}\]

_for all \(j\in[n],\) where \(h_{i}\geq 0\) for all \(i\in[n],\)\(a\geq 0,\) and \(h_{1}\leq\cdots\leq h_{n}.\) Then_

\[\frac{1}{2}\min_{j\in[n]}g(j)\leq\min_{j\in[n]}p(j)\leq\min_{j\in[n]}g(j)\]

Proof.: If \(h_{1}=0,\) then \(p(1)=h(1)=0\) and \(\min_{i\in[n]}p(i)=\min_{i\in[n]}h(i)=0.\) Assume that \(h_{1}>0.\) Using the fact that a harmonic mean is less or equal to the maximum, we have

\[\min_{j\in[n]}p(j)=\min_{j\in[n]}\left(j+a\right)\left(\sum_{i=1}^{j}\frac{1}{ h_{i}}\right)^{-1}\leq\min_{j\in[n]}\left(h_{j}+a\left(\sum_{i=1}^{j}\frac{1}{h_ {i}}\right)^{-1}\right)=\min_{j\in[n]}g(j).\]

Thus, we proved the upper bound. Next, assume that \(j^{*}\) is the smallest minimizer of \(p(j).\) If \(j^{*}=1,\) then

\[\min_{j\in[n]}p(j)=p(1)=\left(1+a\right)h_{1}=h_{1}+ah_{1}=g(1)\geq\min_{j\in[ n]}g(j).\]

Otherwise, if \(j^{*}>1,\) then \(p(j^{*})\leq p(j^{*}-1).\) Using simple algebra, we obtain

\[(j^{*}+a)\left(\sum_{i=1}^{j^{*}}\frac{1}{h_{i}}\right)^{-1}\leq (j^{*}-1+a)\left(\sum_{i=1}^{j^{*}-1}\frac{1}{h_{i}}\right)^{-1}\] \[\Leftrightarrow(j^{*}+a)\left(\sum_{i=1}^{j^{*}-1}\frac{1}{h_{i} }\right)\leq(j^{*}-1+a)\left(\sum_{i=1}^{j^{*}}\frac{1}{h_{i}}\right)\] \[\Leftrightarrow\left(\sum_{i=1}^{j^{*}}\frac{1}{h_{i}}\right) \leq(j^{*}+a)\left(\frac{1}{h_{j^{*}}}\right)\] \[\Leftrightarrow h_{j^{*}}\leq(j^{*}+a)\left(\sum_{i=1}^{j^{*}}\frac{1}{h_{i} }\right)^{-1}.\]

Using the last inequality, we get

\[\min_{j\in[n]}p(j) =(j^{*}+a)\left(\sum_{i=1}^{j^{*}}\frac{1}{h_{i}}\right)^{-1}= \frac{1}{2}\left(j^{*}+a\right)\left(\sum_{i=1}^{j^{*}}\frac{1}{h_{i}}\right)^ {-1}+\frac{1}{2}\left(j^{*}+a\right)\left(\sum_{i=1}^{j^{*}}\frac{1}{h_{i}} \right)^{-1}\] \[\geq\frac{1}{2}h_{j^{*}}+\frac{1}{2}a\left(\sum_{i=1}^{j^{*}} \frac{1}{h_{i}}\right)^{-1}=\frac{1}{2}g(j^{*})\geq\frac{1}{2}\min_{j\in[n]}g( j).\]

_Example 6.7_.: [**Ignoring Slow Workers**] If \(h_{i}\) and \(\tau_{i}\) are fixed and finite for all \(i\leq p,\) and \(\max\{h_{i},\tau_{i}\}=m\in\mathbb{R}\) for all \(i>p,\) then, for \(m\) large enough, we have \(t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i},\tau_{i}]_{1}^{n})= t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i},\tau_{i}]_{1}^{p}).\)

Proof.: By the definition, we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_ {n},\tau_{n})=\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\}.\]For \(m>\max_{i\in[p]}\max\left\{h_{i},\tau_{i}\right\}\), we have \(\max\left\{h_{i},\tau_{i}\right\}<m\) for all \(i\leq p\) and the set \(\{1,\ldots,p\}\) equals to \(\{\pi_{1},\ldots,\pi_{p}\}\). Thus, we get

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{ 1},\ldots,h_{n},\tau_{n}) =\min\left\{\min_{j\in[p]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}} \},s^{*}(j)\},\min_{j\in\{p+1,\ldots,n\}}\max\{m,s^{*}(j)\}\right\}\] \[=\min\left\{t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h _{1},\tau_{1},\ldots,h_{p},\tau_{p}),\min_{j\in\{p+1,\ldots,n\}}\max\{m,s^{*}( j)\}\right\}\]

By taking \(m>t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots, h_{p},\tau_{p})\), we obtain

\[\min_{j\in\{p+1,\ldots,n\}}\max\{m,s^{*}(j)\}\geq m>t^{*}(\omega,\nicefrac{{ \sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_{p},\tau_{p}).\]

Therefore, we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_ {n},\tau_{n})=t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau _{1},\ldots,h_{p},\tau_{p}).\]

_Example 6.8_.: [Partial Participation] If \(\max\{h_{i},\tau_{i}\}=\infty\) for all \(i>p\geq 1,\) then \(t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i},\tau_{i}]_{1}^{n} )=t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i},\tau_{i}]_{1}^{p})\).

Proof.: By the definition, we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_ {n},\tau_{n})=\min_{j\in[n]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\},\]

where \(\pi\) is a permutation such that the set \(\{\pi_{p+1},\ldots,\pi_{n}\}\) equals to the set \(\{p+1,\ldots,n\}\). Such a permutation exists because \(\max\{h_{i},\tau_{i}\}=\infty\) for all \(i>p\). Using this, we have

\[t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau_ {1},\ldots,h_{n},\tau_{n}) =\min\left\{\min_{j\in[p]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\},\min_{j\in\{p+1,\ldots,n\}}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}} \},s^{*}(j)\}\right\}\] \[=\min\left\{\min_{j\in[p]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}} \},s^{*}(j)\},\infty\right\}\] \[=\min_{j\in[p]}\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\}\] \[=t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\tau _{1},\ldots,h_{p},\tau_{p}).\]

## Appendix G Generic Lemma For Unbiased Gradient Estimators

We prove the following generic lemma that estimates the variance of the general family of unbiased gradient estimators.

**Lemma G.1**.: _Consider that Assumptions 1.3 and 2.2 hold. Let us consider the gradient estimator_

\[g^{k}=\frac{1}{\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij}}\sum_{i=1}^{n}\sum_ {j=1}^{m_{i}}w_{ij}\mathcal{C}_{ij}\left(\sum_{l=1}^{b_{ij}}\nabla f(x^{k}; \xi_{il}^{k})\right),\]

_where \(m_{i}\geq 0\) for all \(i\in[n]\), \(b_{ij}\geq 0\) for all \(i\in[n]\) and \(j\in[m_{i}]\) are ordered batch sizes (\(b_{i_{1}}\leq\cdots\leq b_{i,m_{i}}\) for all \(i\in[n]\)), \(w_{ij}\geq 0\) are weights for all \(i\in[n]\) and \(j\in[m_{i}]\), and \(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij}>0,\mathcal{C}_{ij}\in\mathbb{U} \left(\omega_{ij}\right)\) are mutually independent compressors from Def. 2.1 for all \(i\in[n]\) and \(j\in[m_{i}]\), and \(x^{k}\in\mathbb{R}^{d}\) is an arbitrary point. Then \(\mathbb{E}\left[g^{k}\right]=\nabla f(x^{k})\) and_

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right] \leq\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij} \right)^{2}}\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}^{2}b_{ij}^{2}\omega_{ij} \left\|\nabla f(x^{k})\right\|^{2}\] \[\quad+\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij} \right)^{2}}\sum_{i=1}^{n}\left(\sum_{j=1}^{m_{i}}w_{ij}^{2}b_{ij}\omega_{ij} \sigma^{2}+\sum_{j=1}^{m_{i}}\sum_{p=1}^{m_{i}}\min\{b_{ij},b_{ip}\}w_{ij}w_{ip} \sigma^{2}\right).\] (39)Proof.: First, we show the gradient estimator is unbiased:

\[\mathbb{E}\left[g^{k}\right] =\mathbb{E}\left[\frac{1}{\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij }}\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}\mathcal{C}_{ij}\left(\sum_{l=1}^{b_{ij }}\nabla f(x^{k};\xi_{il}^{k})\right)\right]\] \[=\frac{1}{\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij}}\sum_{i=1} ^{n}\sum_{j=1}^{m_{i}}w_{ij}\mathbb{E}\left[\mathcal{C}_{ij}\left(\sum_{l=1}^{b _{ij}}\nabla f(x^{k};\xi_{il}^{k})\right)\right].\]

Using Def. 2.1 and Assumption 1.3, we have

\[\mathbb{E}\left[g^{k}\right]=\frac{1}{\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_ {ij}}\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij}\nabla f(x^{k})=\nabla f(x^{ k}).\]

Next, we estimate the variance

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right]\] \[=\mathbb{E}\left[\left\|\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_ {ij}\right\|_{i}^{2}\sum_{j=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}\mathcal{C}_{ij} \left(\sum_{l=1}^{b_{ij}}\nabla f(x^{k};\xi_{il}^{k})\right)-\sum_{i=1}^{n} \sum_{j=1}^{m_{i}}w_{ij}b_{ij}\nabla f(x^{k})\right\|^{2}\right].\]

Using the independence and (22), we have

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right]\] \[=\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij} \right)^{2}}\sum_{i=1}^{n}\mathbb{E}\left[\left\|\sum_{j=1}^{m_{i}}w_{ij} \mathcal{C}_{ij}\left(\sum_{l=1}^{b_{ij}}\nabla f(x^{k};\xi_{il}^{k})\right)- \sum_{j=1}^{m_{i}}w_{ij}\sum_{l=1}^{b_{ij}}\nabla f(x^{k};\xi_{il}^{k})\right\| ^{2}\right]\] \[=\underbrace{\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij }b_{ij}\right)^{2}}\sum_{i=1}^{n}\mathbb{E}\left[\left\|\sum_{j=1}^{m_{i}}w_{ ij}\mathcal{C}_{ij}\left(\sum_{l=1}^{b_{ij}}\nabla f(x^{k};\xi_{il}^{k})\right)- \sum_{j=1}^{m_{i}}w_{ij}\sum_{l=1}^{b_{ij}}\nabla f(x^{k};\xi_{il}^{k})\right\| ^{2}\right]}_{I_{1}}\] \[+\underbrace{\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij }b_{ij}\right)^{2}}\sum_{i=1}^{n}\mathbb{E}\left[\left\|\sum_{j=1}^{m_{i}}w_{ ij}\sum_{l=1}^{b_{ij}}\nabla f(x^{k};\xi_{il}^{k})-\sum_{j=1}^{m_{i}}w_{ij}b_{ ij}\nabla f(x^{k})\right\|^{2}\right]}_{I_{2}}.\] (40)

Using the independence of the compressors and Def. 2.1, we get

\[I_{1} =\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij} \right)^{2}}\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}^{2}\mathbb{E}\left[\left\| \mathcal{C}_{ij}\left(\sum_{l=1}^{b_{ij}}\nabla f(x^{k};\xi_{il}^{k})\right)- \sum_{l=1}^{b_{ij}}\nabla f(x^{k};\xi_{il}^{k})\right\|^{2}\right]\] \[\leq\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij} \right)^{2}}\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}^{2}\omega_{ij}\mathbb{E} \left[\left\|\sum_{l=1}^{b_{ij}}\nabla f(x^{k};\xi_{il}^{k})\right\|^{2} \right].\]

In the view of (22), the independence of the stochastic gradients, and Assumption 1.3, we obtain

\[I_{1}\leq\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij}\right)^{2 }}\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}^{2}\omega_{ij}\mathbb{E}\left[\left\| \sum_{l=1}^{b_{ij}}\nabla f(x^{k};\xi_{il}^{k})-\sum_{l=1}^{b_{ij}}\nabla f(x^{ k})\right\|^{2}\right]\]\[\leq\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij}\right)^{2}} \sum_{i=1}^{n}\sum_{l=1}^{b_{i,m_{i}}}\left(\sum_{j\in S_{il}}w_{ij}\right)^{2} \sigma^{2}.\]

Note that

\[\sum_{l=1}^{b_{i,m_{i}}}\left(\sum_{j\in S_{il}}w_{ij}\right)^{2}=\sum_{l=1}^{b _{i,m_{i}}}\sum_{j\in S_{il}}\sum_{p\in S_{il}}w_{ij}w_{ip}.\]

The number of appearances of the term \(w_{ij}w_{ip}\) in the sum equals to \(\min\{b_{ij},b_{ip}\}\). Thus

\[I_{2}\leq\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij}\right)^{2 }}\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}\sum_{p=1}^{m_{i}}\min\{b_{ij},b_{ip}\}w_{ij }w_{ip}\sigma^{2}.\]

We now substitute the bounds on \(I_{1}\) and \(I_{2}\) to (40), and get (39).

## Appendix H Proofs for Algorithms 1 and 4

In the appendix, we work with Alg. 4 instead of Alg. 1. Alg. 4 is more general and estimates all the parameters based on local per-iteration times \(h_{i}^{k}\) and \(\tau_{i}^{k}\) instead of \(h_{i}\) and \(\tau_{i}\). All results for Alg. 1 can be easily obtained by taking \(h_{i}^{k}=h_{i}\) and \(\tau_{i}^{k}=\tau_{i}\).

**Lemma H.1**.: _Consider that Assumptions 1.3 and 2.2 hold. Then the gradient estimator (9) with the weights \(w_{i}\) from Alg. 4 is unbiased and_

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right]\leq\left(\sum _{i\,:\,b_{i}\wedge m_{i}>0}\frac{b_{i}m_{i}}{b_{i}\omega+\omega\frac{\sigma^ {2}}{\varepsilon}+m_{i}\frac{\sigma^{2}}{\varepsilon}}\right)^{-1}\left( \left\|\nabla f(x^{k})\right\|^{2}+\varepsilon\right).\] (41)

Proof.: Alg. 4 implements the gradient estimator (9). We can use Lemma G.1 with \(b_{ij}=b_{i},w_{ij}=w_{i}\) and \(\omega_{ij}=\omega\) for all \(i\in[n]\) and \(j\in[m_{i}]\). Using (39), we have

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right] \leq\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij} \right)^{2}}\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}^{2}b_{ij}^{2}\omega_{ij} \left\|\nabla f(x^{k})\right\|^{2}+\] \[\quad\frac{1}{\left(\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}w_{ij}b_{ij} \right)^{2}}\sum_{i=1}^{n}\left(\sum_{j=1}^{m_{i}}w_{ij}^{2}b_{ij}\omega_{ij} \sigma^{2}+\sum_{j=1}^{m_{i}}\sum_{p=1}^{m_{i}}\min\{b_{ij},b_{ip}\}w_{ij}w_{ip }\sigma^{2}\right)\] \[=\frac{1}{\left(\sum_{i=1}^{n}m_{i}w_{i}b_{i}\right)^{2}}\sum_{i= 1}^{n}w_{i}^{2}\left(m_{i}b_{i}^{2}\omega\right)\left\|\nabla f(x^{k})\right\| ^{2}+\] \[\quad\frac{1}{\left(\sum_{i=1}^{n}m_{i}w_{i}b_{i}\right)^{2}}\sum _{i=1}^{n}w_{i}^{2}\left(m_{i}b_{i}\omega\sigma^{2}+b_{i}m_{i}^{2}\sigma^{2} \right).\]

We add nonnegative terms to the last inequality to obtain

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right]\leq\frac{1}{ \left(\sum_{i=1}^{n}m_{i}w_{i}b_{i}\right)^{2}}\sum_{i=1}^{n}w_{i}^{2}\left(m_ {i}b_{i}^{2}\omega+m_{i}b_{i}\omega\frac{\sigma^{2}}{\varepsilon}+b_{i}m_{i}^ {2}\frac{\sigma^{2}}{\varepsilon}\right)\left\|\nabla f(x^{k})\right\|^{2}+\]\[\frac{1}{\left(\sum_{i=1}^{n}m_{i}w_{i}b_{i}\right)^{2}}\sum_{i=1}^{n}w_{i}^{2} \left(m_{i}b_{i}^{2}\omega\varepsilon+m_{i}b_{i}\omega\sigma^{2}+b_{i}m_{i}^{2} \sigma^{2}\right)\]

Using the choice of the weights \(w_{i},\) we get (41). 

**Lemma H.2**.: _Consider two quantities \(\omega\geq 0\) and \(\nicefrac{{\sigma^{2}}}{{\varepsilon}}\geq 0,\) and \(n\in\mathbb{N}.\) Also, consider a sequence of positive pairs \(\{(h_{i},\tau_{i})\}_{i=1}^{n}.\) We take \(b_{i}=\left\lfloor\frac{t^{*}}{h_{i}}\right\rfloor\) and \(m_{i}=\left\lfloor\frac{t^{*}}{\tau_{i}}\right\rfloor\) for all \(i\in[n],\) where \(t^{*}\equiv t^{*}\left(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1}, \tau_{1},\ldots,h_{n},\tau_{n}\right)\) is the equilibrium time from Def. 3.1. Then_

\[\left(\sum_{i\,:\,b_{i}\wedge m_{i}>0}\frac{b_{i}m_{i}}{b_{i}\omega+\omega \frac{\sigma^{2}}{\varepsilon}+m_{i}\frac{\sigma^{2}}{\varepsilon}}\right)^{ -1}\leq 1.\]

Proof.: Assume that \(j^{*}\in[n]\) is the smallest index that minimizes \(\max\{\max\{h_{\pi_{j}},\tau_{\pi_{j}}\},s^{*}(j)\},\) then

\[t^{*}=\max\{\max\{h_{\pi_{j^{*}}},\tau_{\pi_{j^{*}}}\},s^{*}(j^{*})\}.\] (42)

Therefore, we have \(t^{*}\geq\max\{h_{\pi_{j^{*}}},\tau_{\pi_{j^{*}}}\}\geq\max\{h_{\pi_{j}},\tau_ {\pi_{j}}\}\) for all \(j\leq j^{*}\) since \(\max\{h_{\pi_{j}},\tau_{\pi_{j}}\}\) are sorted. It means that \(b_{\pi_{j}}=\left\lfloor\frac{t^{*}}{h_{\pi_{j}}}\right\rfloor\geq 1\) and \(m_{\pi_{j}}=\left\lfloor\frac{t^{*}}{\tau_{\pi_{j}}}\right\rfloor\geq 1\) for all \(j\leq j^{*}.\) Using this, we have

\[I:=\left(\sum_{i\,:\,b_{i}\wedge m_{i}>0}\frac{b_{i}m_{i}}{b_{i}\omega+\omega \frac{\sigma^{2}}{\varepsilon}+m_{i}\frac{\sigma^{2}}{\varepsilon}}\right)^{ -1}\leq\left(\sum_{i=1}^{j^{*}}\frac{b_{\pi_{i}}m_{\pi_{i}}}{b_{\pi_{i}} \omega+\omega\frac{\sigma^{2}}{\varepsilon}+m_{\pi_{i}}\frac{\sigma^{2}}{ \varepsilon}}\right)^{-1}=\left(\sum_{i=1}^{j^{*}}\frac{1}{\frac{\omega}{m_{ \pi_{i}}}+\frac{\omega\sigma^{2}}{b_{\pi_{i}}\varepsilon}+\frac{\sigma^{2}}{b _{\pi_{i}}\varepsilon}}\right)^{-1}.\]

Since \(b_{\pi_{j}}=\left\lfloor\frac{t^{*}}{h_{\pi_{j}}}\right\rfloor\geq 1\) and \(m_{\pi_{j}}=\left\lfloor\frac{t^{*}}{\tau_{\pi_{j}}}\right\rfloor\geq 1,\) we can also conclude that \(b_{\pi_{j}}\geq\frac{t^{*}}{2h_{\pi_{j}}}\) and \(m_{\pi_{j}}\geq\frac{t^{*}}{2\tau_{\pi_{j}}}\) for all \(j\leq j^{*}.\) Therefore, we obtain

\[I \leq\left(\sum_{i=1}^{j^{*}}\frac{1}{\frac{2\tau_{\pi_{i}}\omega} +\frac{4\tau_{\pi_{i}}h_{\pi_{i}}\omega\sigma^{2}}{\left(t^{*}\right)^{2} \varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{t^{*}\varepsilon}}\right)^{-1}\leq \left(\sum_{i=1}^{j^{*}}\frac{1}{\frac{2\tau_{\pi_{i}}\omega}{\frac{2\tau_{ \pi_{i}}\omega}{\frac{4\tau_{\pi_{i}}h_{\pi_{i}}\omega\sigma^{2}}{\varepsilon} +\frac{4\tau_{\pi_{i}}h_{\pi_{i}}\omega\sigma^{2}}{\varepsilon}+\frac{2h_{\pi _{i}}\sigma^{2}}{\varepsilon}}}}\right)^{-1}\] \[=\frac{1}{s^{*}(j^{*})}\left(\sum_{i=1}^{j^{*}}\frac{1}{2\tau_{ \pi_{i}}\omega+\frac{4\tau_{\pi_{i}}h_{\pi_{i}}\omega\sigma^{2}}{\frac{s^{*}(j ^{*})\times\varepsilon}{\varepsilon}+\frac{2h_{\pi_{i}}\sigma^{2}}{\varepsilon }}}\right)^{-1}.\]

where the last inequality follows from (42). Recall that \(s^{*}(j^{*})\) is the solution of the equation (7). Thus

\[\left(\sum_{i=1}^{j^{*}}\frac{1}{2\tau_{\pi_{i}}\omega+\frac{4\tau_{\pi_{i}}h_{ \pi_{i}}\omega\sigma^{2}}{s^{*}(j^{*})\times\varepsilon}}\right)^{-1}=s^{*}(j^{*})\]

and

\[I\leq\frac{1}{s^{*}(j^{*})}\times s^{*}(j^{*})=1.\]

**Theorem H.3**.: _Assume that Assumptions 1.1, 1.2, 1.3, 2.2 hold. Let us take \(\gamma=\frac{1}{2L}\) in Alg. 4. Then for all iterations_

\[K\geq\frac{16L\Delta}{\varepsilon},\] (43)

_Alg. 4 guarantees that \(\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2} \right]\leq\varepsilon.\)_Proof.: Let us fix any iteration \(k\in\mathbb{N}\). Consider that \(\mathcal{G}_{k}\) is a \(\sigma\)-algebra generated by \(g^{0},\ldots,g^{k-1}\). Then, given \(\mathcal{G}_{k},\)\(x^{k}\) is a deterministic vector. Using Lemma H.1, we have

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right| \mathcal{G}_{k}\right]\leq\left(\sum_{i\,:\,b_{i}\wedge m_{i}>0}\frac{b_{i}m_{ i}}{b_{i}\omega+\omega^{\frac{\sigma^{2}}{e}}+m_{i}\frac{\sigma^{2}}{e}} \right)^{-1}\left(\left\|\nabla f(x^{k})\right\|^{2}+\varepsilon\right).\]

Note that the choice of the parameters in Alg. 4 satisfy the conditions of Lemma H.2. Thus, we have

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right| \mathcal{G}_{k}\right]\leq\left\|\nabla f(x^{k})\right\|^{2}+\varepsilon\]

for all \(k\geq 0.\) It is left to use the standard SGD analysis from Theorem I.1 with \(B=1\) and \(C=\varepsilon\) to finish the proof. 

**Theorem 4.2**.: _Let Assumptions 1.1, 1.2, 1.3, 2.2 hold. Let us take \(\gamma=1/2L\) in Shadowheart SGD (Alg. 1). Then as long as \(K\geq 16L\Delta/\varepsilon,\) we have the guarantee \(\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2} \right]\leq\varepsilon.\)_

Proof.: It immediately follows from Theorem H.3 for \(h_{i}=h_{i}^{k}\) and \(\tau_{i}=\tau_{i}^{k}.\) 

**Theorem 4.4**.: _Alg. 4 converges after_

\[\sum_{k=0}^{\left\lceil\frac{16L\Delta}{\varepsilon}\right\rceil }2t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1}^{k},\tau_{1}^{k}, \ldots,h_{n}^{k},\tau_{n}^{k})\] (11)

_seconds, where \(h_{i}^{k}>0\) and \(\tau_{i}^{k}>0\) are computation and communication times for worker \(i\) in iteration \(k\)._

Proof.: Let us fix an iteration index \(k\in[n].\) In every iteration, every worker calculates \(b_{i}\) stochastic gradients and sends \(m_{i}\) compressed vectors. Thus, the processing time of each iteration is not greater than

\[\max_{i\in[n]}\left\{h_{i}^{k}b_{i}+\tau_{i}^{k}m_{i}\right\}= \max_{i\in[n]}\left\{h_{i}^{k}\left\lfloor\frac{t^{*}}{h_{i}^{k}}\right\rfloor+ \tau_{i}^{k}\left\lfloor\frac{t^{*}}{\tau_{i}^{k}}\right\rfloor\right\}\] (44) \[\leq 2t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1}^{k },\tau_{1}^{k},\ldots,h_{n}^{k},\tau_{n}^{k}).\]

Using the fact that the number of iterations equals to (43), we finally get (11). 

**Corollary 4.3**.: Shadowheart SGD _(Alg. 1) converges after at most \(T_{*}\) seconds, where_

\[T_{*}:=\tfrac{32L\Delta}{\varepsilon}\times t^{*}(\omega,\nicefrac{{\sigma^{2 }}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}).\] (10)

Proof.: It immediately follows from Theorem 4.4 for \(h_{i}=h_{i}^{k}\) and \(\tau_{i}=\tau_{i}^{k}.\) 

## Appendix I The Classical SGD Theorem

Let us consider a slightly modified classical SGD result from (Ghadimi and Lan, 2013; Khaled and Richtarik, 2020).

**Theorem I.1**.: _Assume that Assumptions 1.1 and 1.2 hold. We consider the SGD method:_

\[x^{k+1}=x^{k}-\gamma g(x^{k}),\]

_where_

\[\gamma=\min\left\{\frac{1}{L(1+B)},\frac{\varepsilon}{2LC}\right\}\]

_For all \(k\geq 0,\) the vector \(g(x)\) is a random vector such that \(\mathbb{E}\left[\left.g(x^{k})\right|\mathcal{G}_{k}\right]=\nabla f(x^{k}),\)_

\[\mathbb{E}\left[\left.\left\|g(x^{k})-\nabla f(x^{k})\right\|^{2} \right|\mathcal{G}_{k}\right]\leq B\left\|\nabla f(x^{k})\right\|^{2}+C,\] (45)_where \(\mathcal{G}_{k}\) is a \(\sigma\)-algebra generated by \(g(x^{0}),\ldots,g(x^{k-1})\). The quantities \(B\) and \(C\) are arbitrary nonnegative constants. Then_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k}) \right\|^{2}\right]\leq\varepsilon\]

_for_

\[K\geq\frac{4L\Delta(1+B)}{\varepsilon}+\frac{8L\Delta C}{ \varepsilon^{2}}.\]

Proof.: From Assumption 1.1, we have

\[f(x^{k+1}) \leq f(x^{k})+\left\langle\nabla f(x^{k}),x^{k+1}-x^{k}\right\rangle +\frac{L}{2}\left\|x^{k+1}-x^{k}\right\|^{2}\] \[=f(x^{k})-\gamma\left\langle\nabla f(x^{k}),g(x^{k})\right\rangle +\frac{L\gamma^{2}}{2}\left\|g(x^{k})\right\|^{2}.\]

We denote \(\mathcal{G}^{k}\) as a sigma-algebra generated by \(g(x^{0}),\ldots,g(x^{k-1})\). Using unbiasedness and (45), we obtain

\[\mathbb{E}\left[\left.f(x^{k+1})\right|\mathcal{G}^{k}\right] \leq f(x^{k})-\gamma\left(1-\frac{L\gamma}{2}\right)\left\| \nabla f(x^{k})\right\|^{2}+\frac{L\gamma^{2}C}{2}\mathbb{E}\left[\left. \left\|g(x^{k})-\nabla f(x^{k})\right\|^{2}\right|\mathcal{G}^{k}\right]\] \[\leq f(x^{k})-\gamma\left(1-\frac{L\gamma(1+B)}{2}\right)\left\| \nabla f(x^{k})\right\|^{2}+\frac{L\gamma^{2}C}{2}.\]

Since \(\gamma\leq\frac{1}{L(1+B)}\), we get

\[\mathbb{E}\left[\left.f(x^{k+1})\right|\mathcal{G}^{k}\right] \leq f(x^{k})-\frac{\gamma}{2}\left\|\nabla f(x^{k})\right\|^{2}+\frac{L \gamma^{2}C}{2}.\]

We subtract \(f^{*}\) and take the full expectation to obtain

\[\mathbb{E}\left[f(x^{k+1})-f^{*}\right] \leq\mathbb{E}\left[f(x^{k})-f^{*}\right]-\frac{\gamma}{2}\mathbb{ E}\left[\left\|\nabla f(x^{k})\right\|^{2}\right]+\frac{L\gamma^{2}C}{2}.\]

Next, we sum the inequality for \(k\in\{0,\ldots,K-1\}\):

\[\mathbb{E}\left[f(x^{K})-f^{*}\right] \leq f(x^{0})-f^{*}-\sum_{k=0}^{K-1}\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{k})\right\|^{2}\right]+\frac{KL\gamma^{2}C}{2}\] \[=\Delta-\sum_{k=0}^{K-1}\frac{\gamma}{2}\mathbb{E}\left[\left\| \nabla f(x^{k})\right\|^{2}\right]+\frac{KL\gamma^{2}C}{2}.\]

Finally, we rearrange the terms and use that \(\mathbb{E}\left[f(x^{K})-f^{*}\right]\geq 0\):

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k}) \right\|^{2}\right]\leq\frac{2\Delta}{\gamma K}+L\gamma C.\]

The choice of \(\gamma\) and \(K\) ensures that

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k}) \right\|^{2}\right]\leq\varepsilon.\]

## Appendix J Comparison with Baselines

In the following proofs, we use assumptions and definitions from Sec. 7.

_Comparison 7.1_.: \(T_{*}=\mathrm{O}(T_{\text{MB}})\).

Proof.: Without loss of generality, we assume that all workers are sorted by \(\max\{h_{i},\dot{\tau}_{i}\}\). For the Rand\(1\) compressor, we have \(\omega=d-1\). From Corollary 4.3, we know that the time complexity of Alg. 1 is

\[T_{*}:=\frac{L\Delta}{\varepsilon}\times t^{*}(\omega,\nicefrac{{ \sigma^{2}}}{{\varepsilon}},h_{1},\dot{\tau}_{1},\ldots,h_{n},\dot{\tau}_{n})\]

up to a constant factor. Using Def. 3.1 of \(t^{*}\), we have

\[T_{*}\leq\max\{\max\{h_{n},\dot{\tau}_{n}\},s^{*}(n)\}\times\frac {L\Delta}{\varepsilon},\] (46)

where \(s^{*}(n)\) is the solution of

\[\left(\sum_{i=1}^{n}\frac{1}{2\dot{\tau}_{i}\omega+\frac{4\dot{ \tau}_{i}h_{i}\sigma^{2}\omega}{s\times\varepsilon}+\frac{2h_{i}\sigma^{2}}{ \varepsilon}}\right)^{-1}=s.\] (47)

Let us take \(s^{\prime}=12\max_{i\in[n]}\max\left\{\frac{h_{i}\sigma^{2}}{n\varepsilon}, \omega\dot{\tau}_{i}\right\}.\) Using simple bounds, we have

\[\left(\sum_{i=1}^{n}\frac{1}{2\dot{\tau}_{i}\omega+\frac{4\dot{ \tau}_{i}h_{i}\sigma^{2}\omega}{s^{\prime}\times\varepsilon}+\frac{2h_{i} \sigma^{2}}{\varepsilon}}\right)^{-1} \leq\left(\frac{n}{\max_{i\in[n]}\left(2\dot{\tau}_{i}\omega+ \frac{4\dot{\tau}_{i}h_{i}\sigma^{2}\omega}{s^{\prime}\times\varepsilon}+ \frac{2h_{i}\sigma^{2}}{\varepsilon}\right)}\right)^{-1}\] \[=\frac{\max_{i\in[n]}\left(2\dot{\tau}_{i}\omega+\frac{4\dot{ \tau}_{i}h_{i}\sigma^{2}\omega}{s^{\prime}\times\varepsilon}+\frac{2h_{i} \sigma^{2}}{\varepsilon}\right)}{n}.\]

Since \(s^{\prime}\geq\omega\max_{i\in[n]}\dot{\tau}_{i}\), we get

\[\left(\sum_{i=1}^{n}\frac{1}{2\dot{\tau}_{i}\omega+\frac{4\dot{ \tau}_{i}h_{i}\sigma^{2}\omega}{s^{\prime}\times\varepsilon}+\frac{2h_{i} \sigma^{2}}{\varepsilon}}\right)^{-1} \leq\frac{\max_{i\in[n]}\left(2\dot{\tau}_{i}\omega+\frac{4h_{i} \sigma^{2}}{\varepsilon}+\frac{2h_{i}\sigma^{2}}{\varepsilon}\right)}{n}\] \[\leq\frac{12\max_{i\in[n]}\max\left\{\frac{h_{i}\sigma^{2}}{ \varepsilon},\omega\dot{\tau}_{i}\right\}}{n}\leq s^{\prime}.\]

It means that \(s^{*}(n)\leq s^{\prime}\) since \(s^{*}(n)\) is the solution of (47). Using the properties of \(\max\) and (46), we get

\[T_{*} =\mathrm{O}\left(\max\left\{\max\{h_{n},\dot{\tau}_{n}\},\max_{i \in[n]}\max\left\{\frac{h_{i}\sigma^{2}}{n\varepsilon},\omega\dot{\tau}_{i} \right\}\right\}\times\frac{L\Delta}{\varepsilon}\right)\] \[=\mathrm{O}\left(\max\left\{\max_{i\in[n]}\left(h_{i}+(\omega+1) \dot{\tau}_{i}\right),\left(\max_{i\in[n]}\frac{h_{i}\sigma^{2}}{n\varepsilon} +\max_{i\in[n]}\omega\dot{\tau}_{i}\right)\right\}\times\frac{L\Delta}{ \varepsilon}\right)\] \[=\mathrm{O}\left(\max\left\{\max_{i\in[n]}\left(h_{i}+(\omega+1) \dot{\tau}_{i}\right)\times\frac{L\Delta}{\varepsilon},\max_{i\in[n]}h_{i} \times\frac{\sigma^{2}L\Delta}{n\varepsilon^{2}},\max_{i\in[n]}(\omega+1)\dot{ \tau}_{i}\times\frac{L\Delta}{\varepsilon}\right\}\right)\] \[=\mathrm{O}\left(\max_{i\in[n]}\left(h_{i}+(\omega+1)\dot{\tau}_ {i}\right)\left(\frac{L\Delta}{\varepsilon}+\frac{\sigma^{2}L\Delta}{n \varepsilon^{2}}\right)\right)\] \[=\mathrm{O}\left(\max_{i\in[n]}\left(h_{i}+d\dot{\tau}_{i}\right) \left(\frac{L\Delta}{\varepsilon}+\frac{\sigma^{2}L\Delta}{n\varepsilon^{2}} \right)\right),\]

where we use \(\omega+1=d\). 

_Comparison 7.2_.: \(T_{*}=\mathrm{O}(T_{\mathbb{R}})\).

Proof.: From Sec. 7, we know that

\[T_{*}:=\frac{L\Delta}{\varepsilon}\times t^{*}(\omega,\nicefrac{{ \sigma^{2}}}{{\varepsilon}},h_{1},\dot{\tau}_{1},\ldots,h_{n},\dot{\tau}_{n})\]

and

\[T_{\mathbb{R}}:=\frac{L\Delta}{\varepsilon}\times t^{*}(0,\nicefrac{{ \sigma^{2}}}{{\varepsilon}},h_{1},d\dot{\tau}_{1},\ldots,h_{n},d\dot{\tau}_{n}).\]

Using Property E.7, we get \(T_{*}=\mathrm{O}(T_{\mathbb{R}})\) since \(\omega=d-1\) for Rand\(1\).

Description of Alg. 5 in the Bidirectional Setting

In this section, we provide the modification of Alg. 4 with the EF21-P mechanism (Gruntkowska et al., 2023). Almost all steps are the same as in Alg. 4 except for the EF21-P mechanism (we mark the main changes with the color).

```
1:Input: starting point \(x^{0}\), stepsize \(\gamma\), the ratio \(\nicefrac{{\sigma^{2}}}{{\varepsilon}}\)
2:for\(k=0,1,\ldots,K-1\)do
3: Find the current computation speeds \(h_{i}^{k}>0\) and communication speeds \(\tau_{i}^{k}>0\) of the workers
4: Find the equilibrium time \(t^{*}\) using Def. 3.1
5: Set \(b_{i}=\left\lfloor\frac{t^{*}}{k_{i}^{k}}\right\rfloor\) and \(m_{i}=\left\lfloor\frac{t^{*}}{\tau_{i}^{k}}\right\rfloor\) for all \(i\in[n]\)
6: Find active workers \(S_{\text{A}}=\left\{i\in[n]\,:\,b_{i}\wedge m_{i}>0\right\}\)
7: Run Alg. 6 in all workers
8: Broadcast \(b_{i},\) and \(m_{i}\) to all workers
9: Init \(g^{k}=0\)
10:for\(i\in S_{\text{A}}\) in paralleldo
11:\(w_{i}\left(\frac{a}{b}\omega+\omega\frac{\sigma^{2}}{\varepsilon}+m_{i}\frac {\sigma^{2}}{\varepsilon}\right)^{-1}\)
12:for\(j=1,\ldots,m_{i}\)do
13: Receive \(\mathcal{C}_{ij}\left(g_{i}^{k}\right)\) from the \(i^{\text{th}}\) worker
14:\(g^{k}=g^{k}+w_{i}\mathcal{C}_{ij}\left(g_{i}^{k}\right)\)
15:endfor
16:endfor
17:\(g^{k}=g^{k}/\left(\sum_{i=1}^{n}w_{i}m_{i}b_{i}\right)\)
18:\(x^{k+1}=x^{k}-\gamma g^{k}\)
19:\(p^{k+1}=\mathcal{C}_{\text{serv}}(x^{k+1}-w^{k})\)
20:\(w^{k+1}=w^{k}+p^{k+1}\)
21: Broadcast \(p^{k+1}\) to all workers
22:endfor\((a):\) If \(\omega=0\) and \(\frac{\sigma^{2}}{\varepsilon}=0,\) then \(w_{i}=1\) ```

**Algorithm 5** Bidirectional Shadowheart SGD

## Appendix L Proofs for Alg. 5

**Theorem A.2**.: _Let Assumptions 1.1, 1.2, 1.3, 2.2 hold. Choose \(\gamma=\frac{\alpha}{16L}\). Then as long as \(K\geq\frac{768L\Delta}{\alpha\varepsilon},\) Bidirectional Shadowheart SGD (Alg. 5) guarantees to find an \(\varepsilon\)-stationary point._

[MISSING_PAGE_FAIL:36]

## Appendix M Development of Adaptive Shadowheart SGD

```
1:Input: starting point \(x^{0}\), stepsize \(\gamma\), the ratio \(\sigma^{2}/_{\varepsilon}\)
2:for\(k=0,1,\ldots,K-1\)do
3: Run Alg. 8 in all workers
4: Broadcast \(x^{k}\) to the workers
5: Init \(l_{i}=0\) for all \(i\in[n]\)
6:while\(\left(\sum\limits_{i:\,l_{i}>0}\left(\sum\limits_{j=1}^{l_{i}}\left(\frac{ \omega}{l_{i}^{2}m_{ij}}+\frac{\omega\sigma^{2}}{l_{i}^{3}m_{ij}\varepsilon} \right)+\frac{\sigma^{2}}{l_{i}\varepsilon}\right)^{-1}\right)^{-1}>\frac{1}{4}\)do
7: Receive \(\mathcal{C}_{i,l_{i},m_{i,l_{i}}}\left(g_{i}\right),l_{i}\) and \(m_{i,l_{i}}\) from some worker (we indicate this worker with \(i\))
8:if\(m_{i,l_{i}}=1\) and \(l_{i}>1\)then
9:\(\bar{g}_{i}=\bar{g}_{i}+\frac{1}{m_{i,(l_{i}-i)}}\hat{g}_{i}\) and \(\hat{g}_{i}=0\)
10:endif
11:\(\hat{g}_{i}=\hat{g}_{i}+\mathcal{C}_{i,l_{i},m_{i,l_{i}}}\left(g_{i}\right)\)
12:endwhile
13: Init \(g^{k}=0\)
14:for\(i\in[n]\,:\,l_{i}>0\)do
15:\(\bar{g}_{i}=\bar{g}_{i}+\frac{1}{m_{i,l_{i}}}\hat{g}_{i}\)
16:\(w_{i}\left(\frac{a}{\varepsilon}\right)=\left(\sum_{j=1}^{l_{i}}\frac{\omega} {m_{ij}}+\sum_{j=1}^{l_{i}}\frac{\omega\sigma^{2}}{l_{i}m_{ij}\varepsilon}+ \frac{l_{i}\sigma^{2}}{\varepsilon}\right)^{-1}\)
17:\(g^{k}=g^{k}+w_{i}\bar{g}_{i}\)
18:endfor
19:\(g^{k}=g^{k}/\left(\sum_{i\,:\,l_{i}>0}w_{i}\sum_{j=1}^{l_{i}}j\right)\)
20:\(x^{k+1}=x^{k}-\gamma g^{k}\)
21:endfor\((a):\) If \(\omega=0\) and \(\frac{\sigma^{2}}{\varepsilon}=0,\) then \(w_{i}=1\) ```

**Algorithm 7**Adaptive Shadowheart SGD

In this section, we design a new method that, unlike Alg. 1 and 4, does not require the bounds on computations times. It automatically understands when to stop the collection of compressed vectors in \(g^{k}\).

Let us consider Alg. 7, which we call Adaptive Shadowheart SGD. It implements the following gradient estimator:

\[g^{k}=\frac{1}{\sum_{i=1}^{n}w_{i}\sum_{j=1}^{l_{i}}j}\sum_{i=1}^{n}w_{i}\sum_ {j=1}^{l_{i}}\frac{1}{m_{ij}}\sum_{p=1}^{m_{ij}}\mathcal{C}_{ijp}\left(\sum_{ r=1}^{j}\nabla f(x^{k};\xi_{ir}^{k})\right).\] (48)

The idea is that each worker calculate and send compressed vectors _in parallel:_ while the next stochastic gradients \(\nabla f(x^{k};\xi_{i,j+1}^{k})\) are calculating, the workers are sending \(\mathcal{C}_{ij}.\left(\sum_{r=1}^{j}\nabla f(x^{k};\xi_{ir}^{k})\right)\) to server. The main difficulty is to understand when to stop. It turns out that it is sufficient to wait for the moment when the condition in Line 6 of Alg. 7 does not hold. For this method, we can prove the following guarantees.

**Theorem M.1**.: _Let Assumptions 1.1, 1.2, 1.3, 2.2 hold. Let us take \(\gamma=\frac{1}{2L}\) in Alg. 7. Then for all iterations \(K\geq\frac{16L\Delta}{\varepsilon},\) Alg. 7 guarantees that \(\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2} \right]\leq\varepsilon.\)_

**Corollary 4.6**.: _If the computation and communication times are positive, the time complexity of Alg. 7 is \(\frac{L\Delta}{\varepsilon}\times t^{*}(\omega,\sigma^{2}/_{\varepsilon},[\max\{h _{i},\tau_{i}\},\min\left\{\tau_{i}r_{i},\max\{h_{i},\tau_{i}\}\right\}]_{1}^{n})\) up to a constant factor, where \(r_{i}\) is defined in Def. 4.5._

## Appendix N Proofs for Alg. 7

**Lemma N.1**.: _Consider that Assumptions 1.3 and 2.2 hold. Then the gradient estimator (48) with the parameters from Alg. 7 is unbiased and_

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right]\leq 4 \left(\sum_{i\in[n]\,:\,l_{i}>0}\left(\sum_{j=1}^{l_{i}}\frac{\omega}{l_{i}^{2}m _{ij}}+\sum_{j=1}^{l_{i}}\frac{\omega\sigma^{2}}{l_{i}^{3}m_{ij}\varepsilon}+ \frac{\sigma^{2}}{l_{i}\varepsilon}\right)^{-1}\right)^{-1}\left(\left\|\nabla f (x^{k})\right\|^{2}+\varepsilon\right).\] (49)

Proof.: Alg. 7 implements the gradient estimator (48). Note that since \(\mathcal{C}_{ijp}\in\mathbb{U}(\omega),\) then \(\frac{1}{m_{ij}}\sum_{p=1}^{m_{ij}}\mathcal{C}_{ijp}\in\mathbb{U}(\nicefrac{{ \omega}}{{m_{ij}}}).\) Therefore, we can use Lemma G.1 with \(\omega_{ij}=\nicefrac{{\omega}}{{m_{ij}}},b_{ij}=j,w_{ij}=w_{i},\) and \(m_{i}=l_{i},\) and get

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right] \leq\frac{1}{\left(\sum_{i=1}^{n}w_{i}\sum_{j=1}^{l_{i}}j\right) ^{2}}\sum_{i=1}^{n}w_{i}^{2}\sum_{j=1}^{l_{i}}\frac{j^{2}\omega}{m_{ij}}\left\| \nabla f(x^{k})\right\|^{2}\] \[\quad+\frac{1}{\left(\sum_{i=1}^{n}w_{i}\sum_{j=1}^{l_{i}}j \right)^{2}}\sum_{i=1}^{n}w_{i}^{2}\left(\sum_{j=1}^{l_{i}}\frac{j\omega\sigma ^{2}}{m_{ij}}+\sum_{j=1}^{l_{i}}\sum_{p=1}^{l_{i}}\min\{j,p\}\sigma^{2}\right).\]

Since \(\sum_{j=1}^{l_{i}}\sum_{p=1}^{l_{i}}\min\{j,p\}\leq l_{i}^{3},\) we have

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right] \leq\frac{1}{\left(\sum_{i=1}^{n}w_{i}\sum_{j=1}^{l_{i}}j\right) ^{2}}\sum_{i=1}^{n}w_{i}^{2}\sum_{j=1}^{l_{i}}\frac{j^{2}\omega}{m_{ij}}\left\| \nabla f(x^{k})\right\|^{2}\] \[\quad+\frac{1}{\left(\sum_{i=1}^{n}w_{i}\sum_{j=1}^{l_{i}}j \right)^{2}}\sum_{i=1}^{n}w_{i}^{2}\left(\sum_{j=1}^{l_{i}}\frac{j\omega\sigma ^{2}}{m_{ij}}+l_{i}^{3}\sigma^{2}\right).\]We add nonnegative terms to the last inequality to obtain

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right] \leq\frac{1}{\left(\sum_{i=1}^{n}w_{i}\sum_{j=1}^{l_{i}}j\right)^{ 2}}\sum_{i=1}^{n}w_{i}^{2}\left(\sum_{j=1}^{l_{i}}\frac{j^{2}\omega}{m_{ij}}+ \sum_{j=1}^{l_{i}}\frac{j\omega}{m_{ij}}\frac{\sigma^{2}}{\varepsilon}+l_{i}^{3 }\frac{\sigma^{2}}{\varepsilon}\right)\left\|\nabla f(x^{k})\right\|^{2}\] \[\quad+\frac{1}{\left(\sum_{i=1}^{n}w_{i}\sum_{j=1}^{l_{i}}j\right) ^{2}}\sum_{i=1}^{n}w_{i}^{2}\left(\sum_{j=1}^{l_{i}}\frac{j^{2}\omega\varepsilon }{m_{ij}}+\sum_{j=1}^{l_{i}}\frac{j\omega\sigma^{2}}{m_{ij}}+l_{i}^{3}\sigma^ {2}\right)\] \[=\frac{1}{\left(\sum_{i=1}^{n}w_{i}\sum_{j=1}^{l_{i}}j\right)^{ 2}}\sum_{i=1}^{n}w_{i}^{2}\left(\sum_{j=1}^{l_{i}}\frac{j^{2}\omega}{m_{ij}}+ \sum_{j=1}^{l_{i}}\frac{j\omega}{m_{ij}}\frac{\sigma^{2}}{\varepsilon}+l_{i}^{ 3}\frac{\sigma^{2}}{\varepsilon}\right)\left(\left\|\nabla f(x^{k})\right\|^ {2}+\varepsilon\right).\]

Using \(\sum_{j=1}^{l_{i}}j\geq\frac{l_{i}^{2}}{2},\) we obtain

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right] \leq\frac{4}{\left(\sum_{i=1}^{n}w_{i}l_{i}^{2}\right)^{2}}\sum_{i=1}^{n}w_{i }^{2}\left(\sum_{j=1}^{l_{i}}\frac{j^{2}\omega}{m_{ij}}+\sum_{j=1}^{l_{i}} \frac{j\omega}{m_{ij}}\frac{\sigma^{2}}{\varepsilon}+l_{i}^{3}\frac{\sigma^{2} }{\varepsilon}\right)\left(\left\|\nabla f(x^{k})\right\|^{2}+\varepsilon \right).\]

In the last two sums, we bound the terms \(j\) with \(l_{i}\) to get

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right] \leq\frac{4}{\left(\sum_{i=1}^{n}w_{i}l_{i}^{2}\right)^{2}}\sum_{i=1}^{n}w_{i }^{2}\left(\sum_{j=1}^{l_{i}}\frac{l_{i}^{2}\omega}{m_{ij}}+\sum_{j=1}^{l_{i}} \frac{l_{i}\omega}{m_{ij}}\frac{\sigma^{2}}{\varepsilon}+l_{i}^{3}\frac{\sigma ^{2}}{\varepsilon}\right)\left(\left\|\nabla f(x^{k})\right\|^{2}+\varepsilon \right).\]

It is left to use the choice of the weights \(w_{i}\) to obtain (49). 

**Theorem M.1**.: _Let Assumptions 1.1, 1.2, 1.3, 2.2 hold. Let us take \(\gamma=\frac{1}{2L}\) in Alg. 7. Then for all iterations \(K\geq\frac{16L\Delta}{\varepsilon},\) Alg. 7 guarantees that \(\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2} \right]\leq\varepsilon.\)_

Proof.: The proof of this theorem is very close to the proof of Theorem H.3. Let us fix any iteration \(k\in\mathbb{N}\). Consider that \(\mathcal{G}_{k}\) is a \(\sigma\)-algebra generated by \(g^{0},\ldots,g^{k-1}\). Then, given \(\mathcal{G}_{k},\)\(x^{k}\) is a deterministic vector. Using Lemma N.1, we have

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right| \mathcal{G}_{k}\right] \leq 4\left(\sum_{i\in[n]:\,l_{i}>0}\left(\sum_{j=1}^{l_{i}}\frac{ \omega}{l_{i}^{2}m_{ij}}+\sum_{j=1}^{l_{i}}\frac{\omega\sigma^{2}}{l_{i}^{3}m_{ ij}\varepsilon}+\frac{\sigma^{2}}{l_{i}\varepsilon}\right)^{-1}\right)^{-1} \left(\left\|\nabla f(x^{k})\right\|^{2}+\varepsilon\right).\]

The algorithm is constructed in such a way that the first bracket in the last inequality is less or equal to \(1\) (see Line 6 in Alg. 7). Thus

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right| \mathcal{G}_{k}\right]\leq\left\|\nabla f(x^{k})\right\|^{2}+\varepsilon\]

for all \(k\geq 0\). It is left to use the standard SGD analysis from Theorem I.1 with \(B=1\) and \(C=\varepsilon\) to ensure that the algorithm converges after \(\frac{16L\Delta}{\varepsilon}\) iterations. 

**Corollary 4.6**.: _If the computation and communication times are positive, the time complexity of Alg. 7 is \(\frac{L\Delta}{\varepsilon}\times t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{ \varepsilon}},[\max\{h_{i},\tau_{i}\},\min\left\{\tau_{i}r_{i},\max\{h_{i},\tau_ {i}\}\right\}]_{1}^{n})\) up to a constant factor, where \(r_{i}\) is defined in Def. 4.5._

Proof.: Let us fix an iteration and take \(k\in[K]\). It is sufficient to find a time required to send enough compressed vectors such that the inequality

\[4\left(\sum_{i\in[n]:\,l_{i}>0}\left(\sum_{j=1}^{l_{i}}\frac{ \omega}{l_{i}^{2}m_{ij}}+\sum_{j=1}^{l_{i}}\frac{\omega\sigma^{2}}{l_{i}^{3}m_{ ij}\varepsilon}+\frac{\sigma^{2}}{l_{i}\varepsilon}\right)^{-1}\right)^{-1}\leq 1\]holds. As soon as this inequality holds, the algorithm stops the loop in Line 6 from Alg. 7. Then the upper bound on the time complexity equals to the number of iterations \(\times\) the upper bound on the time of each iteration. The previous inequality is equivalent to

\[\mathcal{H}:=\sum_{i\in[n]:\,l_{i}>0}\frac{1}{\sum_{j=1}^{l_{i}}\frac{4\omega}{l_ {i}^{2}m_{ij}}+\sum_{j=1}^{l_{i}}\frac{4\omega\sigma^{2}}{l_{i}^{2}m_{ij} \varepsilon}+\frac{4\sigma^{2}}{l_{i}\varepsilon}}\geq 1.\] (50)

Let us show that

\(t^{\prime}:=128\times t^{*}(\omega,\nicefrac{{\sigma^{2}}}{{\varepsilon}}, \max\{h_{1},\tau_{1}\},\min\left\{\tau_{1}r_{1},\max\{h_{1},\tau_{1}\}\right\}, \ldots,\max\{h_{n},\tau_{n}\},\min\left\{\tau_{n}r_{n},\max\{h_{n},\tau_{n}\}\right\})\)

is a sufficient time such that (50) holds.

By the definition of the equilibrium time \(t^{*}\), in order to apply this mapping, we first have to find a permutation \(\pi\) that sorts the input pairs \((\max\{h_{i},\tau_{i}\},\min\left\{\tau_{i}r_{i},\max\{h_{i},\tau_{i}\}\right\})\) by

\[\max\left\{\max\{h_{i},\tau_{i}\},\min\left\{\tau_{i}r_{i},\max\{h_{i},\tau_{i }\}\right\}\right\}.\]

This term equals to \(\max\{h_{i},\tau_{i}\}\). Without loss of generality, we assume that the sequence \(\max\{h_{i},\tau_{i}\}\) is sorted, thus \(\pi_{i}=i\) for all \(i\in[n]\). Therefore, we have

\[t^{\prime}=128\min_{j\in[n]}\max\{\max\{h_{j},\tau_{j}\},s^{*}(j)\},\] (51)

where \(s^{*}(j)\) is the solution of the equation

\[\left(\sum_{i=1}^{j}\left(2\min\left\{\tau_{i}r_{i},\max\{h_{i},\tau_{i}\} \right\}\omega+\frac{4\min\left\{\tau_{i}r_{i},\max\{h_{i},\tau_{i}\}\right\} \max\{h_{i},\tau_{i}\}\sigma^{2}\omega}{s\varepsilon}+\frac{2\max\{h_{i},\tau_ {i}\}\sigma^{2}}{\varepsilon}\right)^{-1}\right)^{-1}=s\] (52)

for all \(j\in[n]\).

Let us define \(j^{*}\) as the smallest by index minimizer in (51). Then

\[t^{\prime}=128\max\{\max\{h_{j^{*}},\tau_{j^{*}}\},s^{*}(j^{*})\}.\]

Assume that \(l_{i}\) is the number of iterations (the number of calculated stochastic gradients) that the \(i^{\text{th}}\) worker does by the time \(t^{\prime}\). Since \(t^{\prime}\geq 4\max\{h_{j^{*}},\tau_{j^{*}}\}\) and the workers are sorted by \(\max\{h_{i},\tau_{i}\}\), we have \(t^{\prime}\geq 2\left(h_{i}+\tau_{i}\right)\) for all \(i\leq j^{*}\). Therefore, for all \(i\leq j^{*}\), the \(i^{\text{th}}\) worker will have time to calculate and send at least one compressed vector, i.e., \(l_{i}\geq 1\) for \(i\leq j^{*}\).

Next, the \(i^{\text{th}}\) worker requires at most \(\tau_{i}\) seconds to send a compressed vector and it waits for at least one calculated gradient. Consider that the computation time of the \(j^{\text{th}}\) stochastic gradient in the \(k^{\text{th}}\) iteration equals to \(h_{ij}^{k}\). Thus \(\frac{t^{\prime}}{2}\leq\sum_{j=1}^{l_{i}}\left(h_{ij}^{k}+\tau_{i}\right).\) Indeed, if \(\frac{t^{\prime}}{2}>\sum_{j=1}^{l_{i}}\left(h_{ij}^{k}+\tau_{i}\right),\) then the \(i^{\text{th}}\) worker will have time to calculate and send at least one more compressed vector because \(\frac{t^{\prime}}{2}\geq 2\max\{h_{i},\tau_{i}\}\geq h_{i}+\tau_{i}\) for all \(i\leq j^{*}\). It would contradict the definition of \(l_{i}\). Therefore, we have

\[\frac{t^{\prime}}{2}\leq\sum_{j=1}^{l_{i}}\left(h_{ij}^{k}+\tau_{i}\right)\leq l _{i}\max_{j\in[l_{i}]}h_{ij}^{k}+l_{i}\tau_{i}\]

and

\[l_{i}\geq\frac{t^{\prime}}{2\left(\max_{j\in[l_{i}]}h_{ij}^{k}+\tau_{i}\right)}.\] (53)

At the same time, by the definition of \(l_{i},\) we have

\[\frac{t^{\prime}}{h_{\min}}\geq l_{i},\]

because \(h_{\min}>0\) is the smallest possible calculating time. Therefore, we have

\[l_{i}\leq l_{\max}:=\left\lceil\frac{t_{\max}}{h_{\min}}\right\rceil,\]where \(t_{\max}\) is defined in Def. 4.5 (\(t_{\max}\geq t^{\prime}\)). Since \(l_{i}\geq 1\) for all \(i\leq j^{*}\), we get

\[\begin{split}\mathcal{H}&:=\sum_{i\in[n]:\,l_{i}>0} \left(\sum_{j=1}^{l_{i}}\frac{4\omega}{l_{i}^{2}m_{ij}}+\sum_{j=1}^{l_{i}} \frac{4\omega\sigma^{2}}{l_{i}^{3}m_{ij}\varepsilon}+\frac{4\sigma^{2}}{l_{i} \varepsilon}\right)^{-1}\\ &\geq\sum_{i=1}^{j^{*}}\left(\underbrace{\sum_{j=1}^{l_{i}} \frac{4\omega}{l_{i}^{2}m_{ij}}+\sum_{j=1}^{l_{i}}\frac{4\omega\sigma^{2}}{l_{ i}^{3}m_{ij}\varepsilon}}_{A}+\underbrace{\frac{4\sigma^{2}}{l_{i}\varepsilon}}_{B} \right)^{-1}.\end{split}\] (54)

Using (53), we obtain

\[B:=\frac{4\sigma^{2}}{\varepsilon l_{i}}\leq\frac{8\sigma^{2} \left(\max_{j\in[l_{i}]}h_{ij}^{k}+\tau_{i}\right)}{\varepsilon t^{\prime}} \leq\frac{8\sigma^{2}\left(h_{i}+\tau_{i}\right)}{\varepsilon t^{\prime}} \leq\frac{16\sigma^{2}\max\{h_{i},\tau_{i}\}}{\varepsilon t^{\prime}}.\]

For every \(j^{\text{th}}\) stochastic gradient, the \(i^{\text{th}}\) worker sends at least one compressed vector or \(\left\lfloor\frac{h_{ij}^{k}}{\tau_{i}}\right\rfloor\) compressed vectors because it is possible that \(\tau_{i}\leq h_{ij}^{k}\), then the worker will have time to send more than one compressed vector. Therefore, we have

\[m_{ij}\geq\max\left\{\left\lfloor\frac{h_{ij}^{k}}{\tau_{i}} \right\rfloor,1\right\}\geq\max\left\{\frac{h_{ij}^{k}}{2\tau_{i}},1\right\} \geq\max\left\{\frac{\min\limits_{j\in[l_{i}]}h_{ij}^{k}}{2\tau_{i}},1\right\}.\]

and

\[\frac{1}{l_{i}}\sum_{j=1}^{l_{i}}\frac{1}{m_{ij}}\leq\frac{2}{l_ {i}}\sum_{j=1}^{l_{i}}\min\left\{\frac{\tau_{i}}{\min\limits_{j\in[l_{i}]}h_{ ij}^{k}},1\right\}=2\min\left\{\frac{\tau_{i}}{\min\limits_{j\in[l_{i}]}h_{ ij}^{k}},1\right\}.\]

Using the last inequality and (53), we get

\[\frac{1}{l_{i}^{2}}\sum_{j=1}^{l_{i}}\frac{1}{m_{ij}} \leq\frac{4\left(\max_{j\in[l_{i}]}h_{ij}^{k}+\tau_{i}\right)}{t^ {\prime}}\min\left\{\frac{\tau_{i}}{\min\limits_{j\in[l_{i}]}h_{ij}^{k}},1\right\}\] \[\leq\frac{8\max\{\max\{\max\limits_{j\in[l_{i}]}h_{ij}^{k},\tau_{ i}\}}}{t^{\prime}}\min\left\{\frac{\tau_{i}}{\min\limits_{j\in[l_{i}]}h_{ ij}^{k}},1\right\}\] \[=\underbrace{\min\left\{\frac{8\tau_{i}}{t^{\prime}}\times\frac{ \max\{\max\limits_{j\in[l_{i}]}h_{ij}^{k},\tau_{i}\}}{\min\limits_{j\in[l_{i}] }h_{ij}^{k}},\frac{8\max\{\max\limits_{j\in[l_{i}]}h_{ij}^{k},\tau_{i}\}}{t^{ \prime}}\right\}}_{T}.\]

It is clear that \(T\leq\frac{8\max\{\max\limits_{j\in[l_{i}]}h_{ij}^{k},\tau_{i}\}}{t^{\prime}}.\) If \(\tau_{i}<\min\limits_{j\in[l_{i}]}h_{ij}^{k}\), then \(T=\frac{8\tau_{i}}{t^{\prime}}\times\frac{\max\limits_{j\in[l_{i}]}h_{ij}^{k}}{ \min\limits_{j\in[l_{i}]}h_{ij}^{k}}.\) If \(\tau_{i}\geq\min\limits_{j\in[l_{i}]}h_{ij}^{k}\) and \(\tau_{i}<\max\limits_{j\in[l_{i}]}h_{ij}^{k},\) then \(T=\frac{\frac{8\tau_{i}}{t^{\prime}}\times\frac{\max\limits_{j\in[l_{i}]}h_{ ij}^{k}}{t^{\prime}}\leq\frac{8\tau_{i}}{t^{\prime}}\times\frac{\max \limits_{j\in[l_{i}]}h_{ij}^{k}}{\min\limits_{j\in[l_{i}]}h_{ij}^{k}}.}\) If \(\tau_{i}\geq\max\limits_{j\in[l_{i}]}h_{ij}^{k},\) then \(T=\frac{8\tau_{i}}{t^{\prime}}\leq\frac{8\tau_{i}}{t^{\prime}}\times\frac{ \max\limits_{j\in[l_{i}]}h_{ij}^{k}}{\min\limits_{j\in[l_{i}]}h_{ij}^{k}}.\)

Thus, we have

\[\frac{1}{l_{i}^{2}}\sum_{j=1}^{l_{i}}\frac{1}{m_{ij}}\leq\min\left\{\frac{8 \tau_{i}}{t^{\prime}}\times\frac{\max\limits_{j\in[l_{i}]}h_{ij}^{k}}{\min \limits_{j\in[l_{i}]}h_{ij}^{k}},\frac{8\max\{\max\limits_{j\in[l_{i}]}h_{ij}^{ k},\tau_{i}\}}{t^{\prime}}\right\}\]\[\leq\min\left\{\frac{8\pi_{i}r_{i}}{t^{\prime}},\frac{8\max\{h_{i}, \tau_{i}\}}{t^{\prime}}\right\},\]

where we use the definition of \(r_{i}\). Using the last inequality and \(l_{i}\geq\frac{t^{\prime}}{4\max\{h_{i},\tau_{i}\}}\), we have

\[A :=4\sum_{j=1}^{l_{i}}\frac{\omega}{m_{ij}l_{i}^{2}}+4\sum_{j=1}^{l _{i}}\frac{\omega\sigma^{2}}{m_{ij}\varepsilon l_{i}^{3}}\] \[\leq\frac{32\omega\min\left\{\tau_{i}r_{i},\max\{h_{i},\tau_{i}\} \right\}}{t^{\prime}}+\frac{32\omega\sigma^{2}\min\left\{\tau_{i}r_{i},\max\{h _{i},\tau_{i}\}\right\}}{\varepsilon t^{\prime}l_{i}}\] \[\leq\frac{32\omega\min\left\{\tau_{i}r_{i},\max\{h_{i},\tau_{i}\} \right\}}{t^{\prime}}+\frac{128\omega\sigma^{2}\min\left\{\tau_{i}r_{i},\max\{ h_{i},\tau_{i}\}\right\}\max\{h_{i},\tau_{i}\}}{\varepsilon\left(t^{\prime} \right)^{2}},\]

where we use (53) and \(\max\limits_{j\in[l_{i}]}h_{ij}^{k}\leq h_{i}\). One can substitute the bounds on \(A\) and \(B\) to (54) and obtain

\[\mathcal{H}\geq\frac{1}{128}\sum_{i=1}^{j^{*}}\left(\frac{\omega\min\left\{ \tau_{i}r_{i},\max\{h_{i},\tau_{i}\}\right\}}{t^{\prime}}+\frac{\omega\sigma^ {2}\min\left\{\tau_{i}r_{i},\max\{h_{i},\tau_{i}\}\right\}\max\{h_{i},\tau_{i }\}}{\varepsilon\left(t^{\prime}\right)^{2}}+\frac{\sigma^{2}\max\{h_{i},\tau_ {i}\}}{\varepsilon t^{\prime}}\right)^{-1}.\]

Note that \(t^{\prime}\geq 128s^{*}(j^{*})\), thus

\[\mathcal{H} \geq\sum_{i=1}^{j^{*}}\left(\frac{\omega\min\left\{\tau_{i}r_{i}, \max\{h_{i},\tau_{i}\}\right\}}{s^{*}(j^{*})}+\frac{\omega\sigma^{2}\min\left\{ \tau_{i}r_{i},\max\{h_{i},\tau_{i}\}\right\}\max\{h_{i},\tau_{i}\}}{\varepsilon \left(s^{*}(j^{*})\right)^{2}}+\frac{\sigma^{2}\max\{h_{i},\tau_{i}\}}{ \varepsilon s^{*}(j^{*})}\right)^{-1}\] \[\geq\sum_{i=1}^{j^{*}}\left(\frac{2\omega\min\left\{\tau_{i}r_{i},\max\{h_{i},\tau_{i}\}\right\}}{s^{*}(j^{*})}+\frac{4\omega\sigma^{2}\min\left\{ \tau_{i}r_{i},\max\{h_{i},\tau_{i}\}\right\}\max\{h_{i},\tau_{i}\}}{\varepsilon \left(s^{*}(j^{*})\right)^{2}}+\frac{2\sigma^{2}\max\{h_{i},\tau_{i}\}}{ \varepsilon s^{*}(j^{*})}\right)^{-1}\] \[=s^{*}(j^{*})\times\sum_{i=1}^{j^{*}}\left(2\omega\min\left\{\tau _{i}r_{i},\max\{h_{i},\tau_{i}\}\right\}+\frac{4\omega\sigma^{2}\min\left\{ \tau_{i}r_{i},\max\{h_{i},\tau_{i}\}\right\}\max\{h_{i},\tau_{i}\}}{\varepsilon s ^{*}(j^{*})}+\frac{2\sigma^{2}\max\{h_{i},\tau_{i}\}}{\varepsilon}\right)^{-1}.\]

It is left to use the definition of \(s^{*}(j^{*})\) (see (52)) to obtain that \(\mathcal{H}\geq s^{*}(j^{*})\times\frac{1}{s^{*}(j^{*})}=1\).

It means that after at most \(t^{\prime}\) seconds, we can ensure that the algorithm will finish the loop in Line 6 from Alg. 7. In the view of Theorem M.1, the time complexity is less or equal to \(K\times t^{\prime}\). 

## Appendix O Construction of the Lower Bound

We prove the lower bound by generalizing the _time multiple oracles protocol_ from (Tyurin and Richtarik, 2023c). Note that in the classical approaches (Nemirovskij and Yudin, 1983; Carmon et al., 2020; Arjevani et al., 2022; Nesterov, 2018), the researchers bound the _number of oracle calls_ required to find an \(\varepsilon\)-solution. Our approach is based on the idea from (Tyurin and Richtarik, 2023c), where the authors propose to bound the _time_ required to find an \(\varepsilon\)-solution. We refer to a detailed explanation to (Tyurin and Richtarik, 2023c)[Sections 3-6].

First, we define an oracle that emulates the process of computing stochastic gradients or the process of sending a compressed vector (Tyurin and Richtarik, 2023c)[Section 4]:

\[O_{\tau}^{g,\mathcal{D}}:\underbrace{\mathbb{R}_{\geq 0}}_{\text{time}}\times \underbrace{\mathbb{R}^{d}}_{\text{point}}\times\underbrace{\{0,1\}}_{\text{ control}}\times\underbrace{(\mathbb{R}_{\geq 0}\times\mathbb{R}^{d}\times\{0,1\})}_{\text{input state}}\rightarrow\underbrace{(\mathbb{R}_{\geq 0} \times\mathbb{R}^{d}\times\{0,1\})}_{\text{output state}}\times\mathbb{R}^{d}\]

\[\text{such that}\qquad O_{\tau}^{g,\mathcal{D}}(t,x,c,(s_{t},s_{x},s_{q}))= \begin{cases}((t,x,1),&0),&c=1,s_{q}=0,\\ ((s_{t},s_{x},1),&0),&c=1,s_{q}=1,t<s_{t}+\tau,\\ ((0,0,0),&g(s_{x};\xi)),&c=1,s_{q}=1,t\geq s_{t}+\tau,\\ ((0,0,0),&0),&c=0,\end{cases}\] (55)where \(\xi\sim\mathcal{D},\,g\) is an arbitrary mapping such that \(g\,:\,\mathbb{R}^{d}\times\mathbb{S}\rightarrow\mathbb{R}^{d},\) and \(\mathbb{S}\) is the sample space of a distribution \(\mathcal{D}.\) Next, we define the _time multiple oracles protocol with compression_:

```
1:Input: function(s) \(f\in\mathcal{F},\) computation oracles \((O_{1},...,O_{n})\in\mathcal{O}(f),\) communication oracles \((\hat{\mathcal{C}}_{1},...,\hat{\mathcal{C}}_{n})\in\mathcal{U},\) algorithm \(A=\{(B^{k},N_{1}^{k},\ldots,N_{n}^{k})\}_{k=0}^{\infty}\,\,\in\,\,\mathcal{A}\)
2:\(s_{i}^{\nabla f,0}=s_{i}^{\mathcal{C},0}=0\) for all \(i\in[n]\)
3:for\(k=0,\ldots,\infty\)do
4:\((t^{k+1},\,i^{k+1},c^{\nabla f,k+1},c^{\nabla f,k+1},x^{k})=B^{k}(g^{1}, \ldots,g^{k}),\)\(\triangleright\)\(t^{k+1}\geq t^{k}\)
5:\((s_{i^{k+1}}^{\nabla f,k+1},\,g_{i^{k+1}}^{k+1})=O_{i^{k+1}}(t^{k+1}+x^{k},c^{ \nabla f,k+1},s_{i^{k+1}}^{\nabla f,k})\)\(\forall j\neq i^{k+1}\cdot s_{j}^{\nabla f,k}=s_{j}^{\nabla f,k},\quad g_{j}^{k+1}=0\)
6:\(g_{\text{pre}}^{k+1}=N_{i^{k+1}}^{k}(g^{1},\ldots,g^{k},g_{i^{k+1}}^{k},\ldots,g_{i^{k+1}}^{k+1}),\)
7:\((s_{i^{k+1}}^{\nabla,k+1},g^{k+1})=\hat{\mathcal{C}}_{i^{k+1}}(t^{k+1},g_{ \text{pre}}^{k+1},c^{\nabla k+1},s_{i^{k+1}}^{\nabla,k})\)
8:endfor ```

**Protocol 9** Time Multiple Oracles Protocol with Compression

In this protocol, the server via \(B^{k}\) returns a new point \(x^{k},\) and broadcasts it to the \(i^{k+1\text{th}}\) worker. Then, the worker calls the oracle \(O_{i^{k+1}}\) that calculates stochastic gradients. Next, the oracle returns the vector \(g_{i^{k+1}}^{k+1},\) and the worker processes it with \(N_{i^{k+1}}^{k}.\) Finally, the worker sends \(g_{\text{pre}}^{k+1}\) to the oracle \(\hat{\mathcal{C}}_{i^{k+1}}\) that sends compressed vectors to the server. Using the parameters \(c^{\nabla f,k+1}\) and \(c^{\mathcal{C},k+1},\) it can decide if it wants to start/stop the process of a gradient calculation and the process of communicating a compressed vector (See Sec. F in (Tyurin and Richtarik, 2023c)). As far as we know, all centralized distributed optimization methods can be described by Protocol 9, including Minibatch SGD, QSGD, Asynchronous SGD, Rennala SGD, and Shadowheart SGD.

We consider the standard function class from the optimization literature (Nesterov, 2018; Arjevani et al., 2022; Carmon et al., 2020):

**Definition O.1** (Function Class \(\mathcal{F}_{\Delta,L}\)).: We assume that a function \(f\,:\,\mathbb{R}^{d}\rightarrow\mathbb{R}\) is differentiable, \(\|\nabla f(x)-\nabla f(y)\|\,\leq L\,\|x-y\|\quad\forall x,y\in\mathbb{R}^{d},\) (\(L\)-smooth) and \(f(0)-\inf_{x\in\mathbb{R}^{d}}f(x)\leq\Delta\) (\(\Delta\)-bounded). The set of all functions with such properties we denote by \(\mathcal{F}_{\Delta,L}.\)

Next, we define the class of algorithms that we analyze.

**Definition O.2** (Algorithm Class \(\mathcal{A}_{\text{ar}}\)).: Let us consider Protocol 9. We say that the sequence of tuples of mappings \(A=\{(B^{k},N_{1}^{k},\ldots,N_{n}^{k})\}_{k=0}^{\infty}\) is a zero-respecting algorithm, if

1. \(B^{k}\,:\,\underbrace{\mathbb{R}^{d}\times\cdots\times\mathbb{R}^{d}}_{k\text { times}}\rightarrow\mathbb{R}_{\geq 0}\times\mathbb{N}\times\mathbb{N}\times \mathbb{N}\times\mathbb{R}^{d}\) for all \(k\geq 1,\) and \(B^{0}\in\mathbb{R}_{\geq 0}\times\mathbb{N}\times\mathbb{N}\times\mathbb{N} \times\mathbb{N}\times\mathbb{R}^{d}.\)
2. For all \(k\geq 1\) and and \(g^{1},\ldots,g^{k}\in\mathbb{R}^{d},\,t^{k+1}\geq t^{k},\) where \(t^{k+1}\) and \(t^{k}\) are defined as \((t^{k+1},\ldots)=B^{k}(g^{1},\ldots,g^{k})\) and \((t^{k},\ldots)=B^{k-1}(g^{1},\ldots,g^{k-1}).\)
3. \(N_{i}^{k}\,:\,\underbrace{\mathbb{R}^{d}\times\cdots\times\mathbb{R}^{d}}_{k \text{ times}}\times\underbrace{\mathbb{R}^{d}\times\cdots\times\mathbb{R}^{d}}_{k +1\text{ times}}\rightarrow\mathbb{R}^{d}\) for all \(k\geq 0\) and for all \(i\in[n].\)
4. \(\text{supp}\left(x^{k}\right)\,\subseteq\,\bigcup_{j=1}^{k}\text{supp}\left(g^{ j}\right),\) and \(\text{supp}\left(g_{\text{pre}}^{k+1}\right)\,\subseteq\,\bigcup_{j=1}^{k}\text{supp} \left(g^{j}\right)\bigcup_{j=1}^{k+1}\text{supp}\left(g_{i^{k+1}}^{j}\right),\) for all \(k\in\mathbb{N}_{0},\) where \(\text{supp}(x):=\{i\in[d]\,|\,x_{i}\neq 0\}.\)

The set of all algorithms with this properties we define as \(\mathcal{A}_{\text{ar}}.\)

The properties 1 and 3 are only required to define the domains of the mappings. The property 4 ensures that these mappings are _zero-respecting_(Arjevani et al., 2022). The property 2 is explained in (Tyurin and Richtarik, 2023c)[Section 4, Definition 4.1]. It ensures that our algorithm does not "travel into the past".

The following oracle class is the same as in (Tyurin and Richtarik, 2023c). For any \(f\in\mathcal{F}_{\Delta,L},\) it returns \(n\) oracles that require \(h_{1},\ldots,h_{n}\) seconds to calculate a stochastic gradient. These oracles emulate the real behavior where the workers have different processing times.

**Definition O.3** (Computation Oracle Class \(\mathcal{O}_{h_{1},\ldots,h_{n}}^{\sigma^{2}_{1},\ldots,h_{n}}\)).: Let us consider an oracle class such that, for any \(f\in\mathcal{F}_{\Delta,L},\) it returns oracles \(O_{i}=O_{h_{i}}^{\nabla f,\mathcal{D}_{i}^{\nabla f}}\) for all \(i\in[n],\) where \(\nabla f(x;\xi)\) is an unbiased \(\sigma^{2}\)-variance-bounded mapping (see Assumption 1.3). The oracles \(O_{h_{i}}^{\nabla f,\mathcal{D}_{i}^{\nabla f}}\) are defined in (55). We define such oracle class as \(\mathcal{O}_{h_{1},\ldots,h_{n}}^{\sigma^{2}}\).

The following oracle class emulates the behavior of compressors. It returns \(n\) oracles that require \(\tau_{1},\ldots,\tau_{n}\) seconds to send a compressed vector to the server.

**Definition O.4** (Communication Oracle Class \(\mathcal{U}_{\tau_{1},\ldots,\tau_{n}}^{\omega}\)).: Let us consider an oracle class such that, it returns oracles \(\hat{\mathcal{C}}_{i}=O_{\tau_{i}}^{\mathcal{C},\mathcal{D}_{i}^{\mathcal{C}}}\) for all \(i\in[n],\) where \(\mathcal{C}\) is an unbiased compressor with a parameter \(\omega,\) i.e., \(\mathcal{C}\in\mathbb{U}(\omega)\) (see Def. 2.1). The oracles \(O_{\tau_{i}}^{\mathcal{C},\mathcal{D}_{i}^{\mathcal{C}}}\) are defined in (55). We define such oracle class as \(\mathcal{U}_{\tau_{1},\ldots,\tau_{n}}^{\omega}\).

Finally, we present our lower bound theorem:

**Theorem O.5**.: _Let us consider Protocol 9. We take any \(h_{i}>0,\)\(\tau_{i}>0\) for all \(i\in[n]\), \(\omega\geq 0,L,\Delta,\varepsilon,\sigma^{2}>0\) such that \(\varepsilon<c_{1}L\Delta\) and \(\omega+1\leq T,\)11 where \(T=\left\lfloor\frac{\Delta L}{c_{2}\varepsilon}\right\rfloor\) is the dimension of the construction. For any algorithm \(A\in\mathcal{A}_{\mathrm{zr}},\) there exists a function \(f\in\mathcal{F}_{\Delta,L},\) computation oracles \((O_{1},\ldots,O_{n})\in\mathcal{O}_{h_{1},\ldots,h_{n}}^{\sigma^{2}}(f),\) and communication oracles \((\hat{\mathcal{C}}_{1},\ldots,\hat{\mathcal{C}}_{n})\in\mathcal{U}_{\tau_{1},\ldots,\tau_{n}}^{\omega},\)12 such that \(\mathbb{E}\left[\inf_{k\in S_{t}}\left\|\nabla f(x^{k})\right\|^{2}\right]> \varepsilon,\) where \(S_{t}:=\left\{k\in\mathbb{N}_{0}\,|\,t^{k}\leq t\right\}\) and_

Footnote 11: We can avoid this constraint using a slightly different construction of a compressor. However, the number of non-zero returned values by the new construction is random. See Sec. P.3.

Footnote 12: The function \(f\) defined on \(\mathbb{R}^{T}\) with \(T=\Theta\left(\nicefrac{{L\Delta}}{{\varepsilon}}\right),\) and the constructed compressor \(\mathcal{C}\) preserves only \(K=\left\lceil\nicefrac{{7}}{{\omega+1}}\right\rceil\) non-zero coordinates.

\[t=c_{3}\times\frac{L\Delta}{\varepsilon}\times t^{*}(\omega,\nicefrac{{\sigma ^{2}}}{{\varepsilon}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}).\]

_The quantities \(c_{1},\)\(c_{2},\) and \(c_{3}\) are universal constants. The sequences \(x^{k}\) and \(t^{k}\) are defined in Protocol 9._

## Appendix P Proof of Theorem 0.5

### The "Worst Case" Function

Let us consider the "worst case" function, which is a standard function to obtain lower bounds in the nonconvex world. We define

\[\text{prog}(x):=\max\{i\geq 0\,|\,x_{i}\neq 0\}\quad(x_{0}\equiv 1).\]

In our proofs, we use the construction from (Carmon et al., 2020; Arjevani et al., 2022). For any \(T\in\mathbb{N},\) the authors define

\[F_{T}(x):=-\Psi(1)\Phi(x_{1})+\sum_{i=2}^{T}\left[\Psi(-x_{i-1})\Phi(-x_{i})- \Psi(x_{i-1})\Phi(x_{i})\right],\] (56)

where

\[\Psi(x)=\begin{cases}0,&x\leq 1/2,\\ \exp\left(1-\frac{1}{(2x-1)^{2}}\right),&x\geq 1/2,\end{cases}\quad\text{and} \quad\Phi(x)=\sqrt{e}\int_{-\infty}^{x}e^{-\frac{1}{2}t^{2}}dt.\]

The main property of the function \(F_{T}(x)\) is that its gradients are large unless \(\text{prog}(x)\geq T.\)

**Lemma P.1** (Carmon et al. (2020); Arjevani et al. (2022)).: _The function \(F_{T}\) satisfies:_

1. \(F_{T}(0)-\inf_{x\in\mathbb{R}^{T}}F_{T}(x)\leq\Delta^{0}T,\) _where_ \(\Delta^{0}=12.\)__2. _The function_ \(F_{T}\) _is_ \(l_{1}\)_-smooth, where_ \(l_{1}=152.\)__
3. _For all_ \(x\in\mathbb{R}^{T},\)__\(\left\|\nabla F_{T}(x)\right\|_{\infty}\leq\gamma_{\infty},\) _where_ \(\gamma_{\infty}=23.\)__
4. _For all_ \(x\in\mathbb{R}^{T},\)__\(\operatorname{prog}(\nabla F_{T}(x))\leq\operatorname{prog}(x)+1.\)__
5. _For all_ \(x\in\mathbb{R}^{T},\) _if_ \(\operatorname{prog}(x)<T,\) _then_ \(\left\|\nabla F_{T}(x)\right\|>1.\)__

**Theorem 0.5**.: _Let us consider Protocol 9. We take any \(h_{i}>0,\)\(\tau_{i}>0\) for all \(i\in[n]\), \(\omega\geq 0,L,\Delta,\varepsilon,\sigma^{2}>0\) such that \(\varepsilon<c_{1}L\Delta\) and \(\omega+1\leq T,\)13 where \(T=\left|\frac{\Delta L}{\varepsilon\varepsilon}\right|\) is the dimension of the construction. For any algorithm \(A\in\mathcal{A}_{\mathbb{z}},\) there exists a function \(f\in\mathcal{F}_{\Delta,L},\) computation oracles \((O_{1},\ldots,O_{n})\in\mathcal{O}_{h_{1},\ldots,h_{n}}^{\sigma^{2}}(f),\) and communication oracles \((\hat{\mathcal{C}}_{1},\ldots,\hat{\mathcal{C}}_{n})\in\mathcal{U}_{\tau_{1}, \ldots,\tau_{n}}^{\omega},\)14 such that \(\mathbb{E}\left[\inf_{k\in S_{t}}\left\|\nabla f(x^{k})\right\|^{2}\right]>\varepsilon,\) where \(S_{t}:=\left\{k\in\mathbb{N}_{0}\,|\,t^{k}\leq t\right\}\) and_

Footnote 13: We can avoid this constraint using a slightly different construction of a compressor. However, the number of non-zero returned values by the new construction is random. See Sec. P.3.

Footnote 14: The function \(f\) defined on \(\mathbb{R}^{T}\) with \(T=\Theta\left(\nicefrac{{L\Delta}}{{\varepsilon}}\right),\) and the constructed compressor \(\mathcal{C}\) preserves only \(K=\left[\nicefrac{{T}}{{\omega+1}}\right]\) non-zero coordinates.

\[t=c_{3}\times\frac{L\Delta}{\varepsilon}\times t^{*}(\omega, \nicefrac{{\sigma^{2}}}{{\varepsilon}},h_{1},\ldots,h_{n},\tau_{n}).\]

_The quantities \(c_{1},\)\(c_{2},\) and \(c_{3}\) are universal constants. The sequences \(x^{k}\) and \(t^{k}\) are defined in Protocol 9._

Proof.: Without loss of generality, we assume that the workers are sorted by \(\max\{h_{i},\tau_{i}\}:\max\{h_{1},\tau_{1}\}\leq\cdots\leq\max\{h_{n},\tau_{ n}\}.\)

(**Step 1**: \(f\in\mathcal{F}_{\Delta,L}\))

Let us fix \(\lambda>0\) and take the function \(f(x):=\frac{L\lambda^{2}}{l_{1}}F_{T}\left(\frac{x}{\lambda}\right),\) where the function \(F_{T}\) is defined in Sec. P.1. Tyurin and Richtarik (2023c)[Sec. D.2, Proof of Thm. 6.4] show that the function \(f\) is \(L\)-smooth and \(f(0)-\inf_{x\in\mathbb{R}^{T}}f(x)\leq\Delta\) if

\[T=\left\lfloor\frac{\Delta l_{1}}{L\lambda^{2}\Delta^{0}}\right\rfloor.\]

Thus, we have \(f\in\mathcal{F}_{\Delta,L}.\)

(**Step 2**: Oracle Class) Let us construct a stochastic gradient mapping. For our lower bound, we take

\[[\nabla f(x;\xi)]_{j}:=\nabla_{j}f(x)\left(1+1\left[j>\operatorname{prog}(x) \right]\left(\frac{\xi}{p}-1\right)\right)\quad\forall x\in\mathbb{R}^{T},\]

and \(\mathcal{D}_{i}^{\forall f}=\text{\emph{Bernoulli}}(p)\) for all \(i\in[n],\) where \(p\in(0,1].\) We denote \([x]_{j}\) as the \(j^{\text{th}}\) index of a vector \(x\in\mathbb{R}^{T}.\) Let us take

\[p=\min\left\{\frac{L^{2}\lambda^{2}\gamma_{\infty}^{2}}{\sigma^{2}l_{1}^{2}}, 1\right\}.\]

Then Tyurin and Richtarik (2023c)[Sec. D.2, Proof of Thm. 6.4] show that this mapping is unbiased and \(\sigma^{2}\)-variance-bounded.

(**Step 3**: Compression Operator) In our construction, we take the Rand\(K\) compressor (outputs \(K\) random values of an input vector _without replacement_, scaled by \(\nicefrac{{T}}{{K}}\) (Def. D.1)). From Theorem D.2, we know that \(\mathcal{C}\) is unbiased and \(\frac{T}{K}-1\)-variance bounded, i.e.,

\[\mathbb{E}_{S}\left[\mathcal{C}(x;S)\right]=x,\qquad\mathbb{E}_{S}\left[ \left\|\mathcal{C}(x;S)-x\right\|^{2}\right]\leq\left(\frac{T}{K}-1\right) \left\|x\right\|^{2},\qquad\forall x\in\mathbb{R}^{T},\]

where

\[[\mathcal{C}(x;S)]_{j}:=\begin{cases}\frac{T}{K}x_{j},&j\in S,\\ 0,&j\not\in S,\end{cases}\quad\forall j\in[T].\]and \(S\) is an uniformly random subset of \([T]\) without replacement. It is sufficient to take \(K=\left\lceil\frac{T}{\omega+1}\right\rceil\) to ensure that \(\mathcal{C}\in\mathbb{U}(\omega)\). Let us define \(p_{\omega}:=\frac{K}{T}\). We take mutually independent distributions \(\mathcal{D}_{t}^{\mathcal{C}}\) that generate random subsets \(S\) described above.

(**Step 4**: Analysis of Protocol)

Let us take

\[\lambda=\frac{\sqrt{2\varepsilon}l_{1}}{L}\]

to ensure that \(\left\|\nabla f(x)\right\|^{2}=\frac{L^{2}\lambda^{2}}{l_{1}^{2}}\left\| \nabla F_{T}(\frac{x}{\lambda})\right\|^{2}>2\varepsilon 1\left[\text{prog}(x)<T\right]\) for all \(x\in\mathbb{R}^{T}\), where we use Lemma P.1. Thus

\[T=\left\lfloor\frac{\Delta L}{2\varepsilon l_{1}\Delta^{0}}\right\rfloor\] (57)

and

\[p=\min\left\{\frac{2\varepsilon\gamma_{\infty}^{2}}{\sigma^{2}},1\right\}.\]

Protocol 9 generates the sequence \(\{x^{k}\}_{k=0}^{\infty}\). We have

\[\inf_{k\in S_{t}}\left\|\nabla f(x^{k})\right\|^{2}>2\varepsilon\inf_{k\in S _{t}}\mathbbm{1}\left[\text{prog}(x^{k})<T\right].\] (58)

Using Lemma P.2 with \(\delta=1/2\) and (58), we obtain

\[\mathbb{E}\left[\inf_{k\in S_{t}}\left\|\nabla f(x^{k})\right\|^{2}\right]\geq 2 \varepsilon\mathbb{P}\left(\inf_{k\in S_{t}}\mathbbm{1}\left[\text{prog}(x^{k })<T\right]\geq 1\right)>\varepsilon\]

for

\[t\leq\frac{1}{48}t^{*}\left(\frac{T}{K},\max\left\{\frac{\sigma^{2}}{2 \varepsilon\gamma_{\infty}^{2}},1\right\},h_{1},\tau_{1},\ldots,h_{n},\tau_{ n}\right)\left(\frac{\Delta L}{8\varepsilon l_{1}\Delta^{0}}-1\right).\]

By the assumption of the theorem, we have \(\omega+1\leq T\). Therefore, we get the series of inequalities:

\[\frac{T}{K}=\frac{T}{\left\lceil\frac{T}{\omega+1}\right\rceil}\geq\frac{ \omega+1}{2}\]

and, using Properties 6.1 and E.1, we have

\[t^{*}\left(\frac{T}{K},\max\left\{\frac{\sigma^{2}}{2\varepsilon \gamma_{\infty}^{2}},1\right\},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}\right)\] \[\geq t^{*}\left(\frac{\omega+1}{2},\max\left\{\frac{\sigma^{2}}{ 2\varepsilon\gamma_{\infty}^{2}},1\right\},h_{1},\tau_{1},\ldots,h_{n},\tau_{ n}\right)\] \[\geq t^{*}\left(\frac{1}{2\gamma_{\infty}^{2}}\times\omega,\frac{ 1}{2\gamma_{\infty}^{2}}\times\frac{\sigma^{2}}{\varepsilon},h_{1},\tau_{1}, \ldots,h_{n},\tau_{n}\right)\] \[\geq\frac{1}{2\gamma_{\infty}^{2}}\times t^{*}\left(\omega,{ \nicefrac{{\sigma^{2}}}{{\varepsilon}}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n} \right),\]

Thus, we can take

\[t=\frac{1}{48}\times\frac{1}{2\gamma_{\infty}^{2}}\times t^{*}\left(\omega,{ \nicefrac{{\sigma^{2}}}{{\varepsilon}}},h_{1},\tau_{1},\ldots,h_{n},\tau_{ n}\right)\left(\frac{\Delta L}{8\varepsilon l_{1}\Delta^{0}}-1\right).\]

#### p.1.1 Proof of Lemma P.2

**Lemma P.2**.: _Let us fix \(T,T^{\prime}\in\mathbb{N}\) such that \(T\leq T^{\prime}\), consider Protocol 9 with an algorithm \(A\in\mathcal{A}_{\text{ar}}\), a differentiable function \(f\::\mathbb{R}^{T^{\prime}}\to\mathbb{R}\) such that \(\text{prog}(\nabla f(x))\leq\text{prog}(x)+1\) for all \(x\in\text{domain}(f)\)._1. _We take stochastic oracles_ \(O_{i}=O_{h_{i}}^{\mathcal{C},\mathcal{D}_{i}^{\mathcal{C}}}\) _with the distributions_ \(\mathcal{D}_{i}^{\mathcal{C}}=\text{{Bernoulli}}(P_{\sigma}),\)__ \(p_{\sigma}\in(0,1],\)__\(h_{i}>0,\) _and the mappings_ \[[\nabla f(x;\xi)]_{j}=\nabla_{j}f(x)\left(1+\mathbb{1}\left[j>\text{prog}(x) \right]\left(\frac{\xi}{p_{\sigma}}-1\right)\right)\quad\forall x\in\mathbb{R }^{T^{\prime}},\forall\xi\in\{0,1\},\forall j\in[T].\] (59)
2. _We take compression oracles_ \(\hat{\mathcal{C}}_{i}=O_{\tau_{i}}^{\mathcal{C},\mathcal{D}_{i}^{\mathcal{C} }}\) _with the distributions_ \(\mathcal{D}_{i}^{\mathcal{C}}=\text{uniform}(K,T^{\prime})\) _(= "uniformly random subset of_ \([T^{\prime}]\) _of the size_ \(K\) _without replacement") and the mappings_ \[[\mathcal{C}(x;S)]_{j}:=\begin{cases}\frac{T^{\prime}}{K}x_{j},&j\in S,\\ 0,&j\not\in S,\end{cases}\quad\forall x\in\mathbb{R}^{T^{\prime}},\forall S \subseteq[n],\forall j\in[T^{\prime}],\] (60)

\(\tau_{i}>0.\) _We define \(p_{\omega}:=\frac{K}{T^{\prime}}.\) We assume that the workers are sorted by \(\max\{h_{m},\tau_{m}\}:\max\{h_{1},\tau_{1}\}\leq\cdots\leq\max\{h_{n},\tau_{n}\}.\) With probability not less than \(1-\delta,\) the following inequality holds:_

\[\inf_{k\in S_{t}}\mathbb{1}\left[\text{prog}(x^{k})<T\right]\geq 1\]

_for_

\[t\leq\frac{1}{48}t^{*}(\nicefrac{{1}}{{p_{\omega}}},\nicefrac{{1}}{{p_{ \sigma}}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n})\left(\frac{T}{2}+\log\delta \right),\]

_where \(S_{t}:=\left\{k\in\mathbb{N}_{0}\,|\,t^{k}\leq t\right\},\) the iterates \(t^{k}\) and \(x^{k}\) are defined in Protocol 9, and \(t^{*}\) is the equilibrium time from Def. 3.1._

Proof.: **(Part 1): The Construction of Random Variables.**

Let us fix \(t\geq 0\) and define the smallest index \(k(i)\) of the sequence when the progress \(\text{prog}(x^{k(i)})\) equals \(i:\)

\[k(i):=\inf\left\{k\in\mathbb{N}_{0}\,|\,i=\text{prog}(x^{k})\right\}\in \mathbb{N}_{0}\cup\{\infty\}.\]

If \(\inf_{k\in S_{t}}\mathbb{1}\left[\text{prog}(x^{k})<1\right]<1\) holds, then exists \(k\in S_{t}\) such that \(\text{prog}(x^{k})=1,\) thus, by the definition of \(k(1),\)\(t^{k(1)}\leq t^{k}\leq t,\) and \(k(1)<\infty.\) Note that \(t^{k(1)}\) is the smallest time when we make progress to the \(1^{\text{th}}\) (first) coordinate.

Since \(x^{0}=0\) and \(A\) is a zero-respecting algorithm, the algorithm can return a vector \(x^{k}\) with a non-zero first coordinate only if some of returned by the stochastic gradients oracles and compression oracles have the first coordinate not equal to zero. The oracles \(O_{i}\) and \(\hat{\mathcal{C}}_{i}\) are constructed in such a way (see (59) and (60)) that they zero out a coordinate based on i.i.d. _bernoulli_ and _uniform_ trials. According to Protocol 9, even if a stochastic oracle returns a non-zero coordinate, it would not mean that the server will get a non-zero coordinate because a subsequent compression oracle also has to return a non-zero coordinate.

Every time when the oracle (55) evaluates \(g(s_{x};\xi),\) it draws i.i.d. random variables \(\xi\sim\mathcal{D}.\) Let us enumerate them:

1. For the stochastic/computation oracles \(O_{i}=O_{h_{i}}^{\forall f,\mathcal{D}_{i}^{\nabla f}},\) we consider the sequence \(\{\xi^{m,j}\}_{j=1}^{\infty},\) where \(\xi^{m,j}\) is a _bernoulli_ random variable drawn in \(j^{\text{th}}\) call of \(g(s_{x};\xi)\) in the \(m^{\text{th}}\) worker in Line 5 of Protocol 9.
2. For the compression oracles \(\hat{\mathcal{C}}_{i}=O_{\tau_{i}}^{\mathcal{C},\mathcal{D}_{i}^{\mathcal{C} }},\) we consider the sequence \(\{S^{m,j}\}_{j=1}^{\infty},\) where \(S^{m,j}\) is a _uniform_ random variable drawn in \(j^{\text{th}}\) call of \(g(s_{x};\xi)\) in the \(m^{\text{th}}\) worker in Line 7 of Protocol 9.

Let us define the following useful random variables based on previous definitions. We define

\[\eta_{m,j}:=\begin{cases}\inf\{i\,|\,\xi^{m,(i+b_{m,j}^{\eta}-1)}=1\text{ and }i\in\mathbb{N}\}\in\mathbb{N}\cup\{\infty\},&b_{m,j}^{\eta}<\infty,\\ \infty,&b_{m,j}^{\eta}=\infty,\end{cases}\quad\forall j\in\{1,\ldots,T\},\] (61)\[\mu_{m,j}:=\begin{cases}\inf\{i\,|\,j\in S^{m,(i+e^{\mu}_{m,j}-1)}\text{ and }i\in\mathbb{N}\}\in\mathbb{N}\cup\{\infty\},&e^{\mu}_{m,j}<\infty,\\ \infty,&e^{\mu}_{m,j}=\infty,\end{cases}\quad\forall j\in\{1,\ldots,T\},\] (62)

where

1. For all \(m\in[n],\,j\geq 1,\,b^{\eta}_{m,j}\in\mathbb{N}\cup\{\infty\}\) is the first index of the sequence \(\{\xi^{m,j}\}_{j=1}^{\infty}\) that started calculating in (55) in the stochastic oracle \(O_{m}\) in or after the iteration \(k(j-1)\).
2. For all \(m\in[n],\,j\geq 1,\,b^{\mu}_{m,j}\in\mathbb{N}\cup\{\infty\}\) is the first index of the sequence \(\{S^{m,j}\}_{j=1}^{\infty}\) that started calculating in (55) in the compression oracle \(\hat{\mathcal{C}}_{m}\) in or after the iteration \(k(j-1)\).
3. For all \(m\in[n],\,j\geq 1,\,\text{if}\,b^{\eta}_{m,j}=\infty,\) then \(e^{\eta}_{m,j}=\infty.\) For all \(m\in[n],\,j\geq 1,\,\text{if}\,b^{\eta}_{m,j}<\infty,\) then \(e^{\eta}_{m,j}\in\mathbb{N}\cup\{\infty\}\) is the first index of the sequence \(\{\xi^{m,j}\}_{j=1}^{\infty}\) that started calculating in (55) in the stochastic oracle \(O_{m}\) in or after the iteration \(k(j-1)\) and the first moment when \(\xi^{m,(i+b^{\eta}_{m,j}-1)}=1\) for some \(i\geq 1\).
4. For all \(m\in[n],\,j\geq 1,\,\text{if}\,b^{\eta}_{m,j}=\infty,\) then \(e^{\mu}_{m,j}=\infty.\) For all \(m\in[n],\,j\geq 1,\,\text{if}\,b^{\eta}_{m,j}<\infty,\) then \(e^{\mu}_{m,j}\in\mathbb{N}\cup\{\infty\}\) is the first index of the sequence \(\{S^{m,j}\}_{j=1}^{\infty}\) that started calculating in (55) in the compression oracle \(\hat{\mathcal{C}}_{m}\) in or after the iteration \(k(j-1)\) and the first moment when \(\xi^{m,(i+b^{\eta}_{m,j}-1)}=1\) for some \(i\geq 1\).

It possible that such indexes do not exist, then we take \(b^{\eta}_{m,j}=\infty,\,b^{\mu}_{m,j}=\infty,\,e^{\eta}_{m,j}=\infty,\) or \(e^{\mu}_{m,j}=\infty,\) accordingly. By the construction, \(e^{\eta}_{m,j}\geq b^{\eta}_{m,j}\) and \(e^{\mu}_{m,j}\geq b^{\mu}_{m,j}.\)

Let us clarify the definitions. At the beginning \(x_{0}=0,\) thus \(k(0)=0.\) It would mean that the first index of the sequence \(\{\xi^{m,j}\}_{j=1}^{\infty},\) when the worker evaluates \(g(s_{x};\xi)\) in (55), simply equals \(b^{\eta}_{m,1}=1\) or \(b^{\eta}_{m,1}=\infty\) (by the definition, it equals to \(\infty\) if the oracle was never called). Assume that \(b^{\eta}_{m,1}=1,\) then \(\eta_{m,1}=\inf\{i\,|\,\xi^{m,i}=1\text{ and }i\in\mathbb{N}\}\) is the first time when the oracle draws a "successful" random _bernoulli_ trial. This random variable is distributed according to the _geometric_ distribution. Then, \(e^{\eta}_{m,1}\) can be equal to \(\eta_{m,1}+1\) or \(\infty.\) At some (random) iteration \(k(1),\) the algorithm \(A\) can get the first non-zero coordinate through \(g^{k},\) then \(b^{\mu}_{m,2}\) is the next index of the sequence \(\{\xi^{m,j}\}_{j=1}^{\infty}\) that started calculating in (55).15

Footnote 15: Let us consider an example with the \(m^{\text{th}}\) worker. Assume that it starts calculations of stochastic gradients and with \(\eta_{m,1}=5\) (as an example) it gets a âsuccessfulâ trial: \(\xi^{m,\eta_{m,\eta_{m,1}}}=\xi^{m,5}=1\) (\(\xi^{m,1}=\cdots=\xi^{m,A}=0\)). And only then, starting with \(e^{\mu}_{m,1},\) the compression oracle can get a vector with a non-zero coordinate. Even if there was a previous âsuccessfulâ trial: \(1\in S^{m,i}\) for some \(i<e^{\mu}_{m,1}.\) This trial did not return a vector with a non-zero coordinate because the stochastic oracle did not return a vector with a non-zero coordinate by that time. Assume that \(e^{\mu}_{m,1}=10,\) then the server waits for \(\mu_{m,1}=\inf\{i\,|\,1\in S^{m,(i+e^{\mu}_{m,1}-1)}\}.\) Assume that \(\mu_{m,1}=7,\) then the time moment, when \(1\in S^{m,(\mu_{m,1}+e^{\mu}_{m,1}-1)}=1\in S^{m,(7+10-1)},\) is the first possible moment when the \(m^{\text{th}}\) worker can send a vector with a non-zero coordinate to the server.

The server gets a non-zero coordinate if at least one worker draws a successful _bernoulli_ trial, and this coordinate belongs to a set generated by the _uniform_ distribution. It takes \(h_{i}\) seconds to generate one _bernoulli_ trial and \(\tau_{i}\) seconds to generate one _uniform_ trial.

Then, if \(:=\inf_{k\in S_{t}}\mathbbm{1}\left[\text{prog}(x^{k})<1\right]<1\) holds, then

\[\hat{t}_{1}:=\min_{m\in[n]}\left\{h_{m}\eta_{m,1}+\tau_{m}\mu_{m,1}\right\} \leq t^{k(1)}.\]

because \(h_{m}\eta_{m,1}+\tau_{m}\mu_{m,1}\) is the time required to generate \(\eta_{m,1}\)_bernoulli_ and \(\mu_{m,1}\)_uniform_ trials. In other words, the algorithm can not progress to the next coordinate before the moment when at least one worker generates "successful" _bernoulli_ and _uniform_ trials.

Using the same reasoning, \(t^{k(j)}\geq t^{k(j-1)}+\hat{t}_{j},\) where

\[\hat{t}_{j}:=\min_{m\in[n]}\left\{h_{m}\eta_{m,j}+\tau_{m}\mu_{m,j}\right\}.\]Combining the observations, if \(\inf_{k\in S_{i}}\mathbbm{1}\left[\text{prog}(x^{k})<T\right]<1\) holds, then \(\sum_{j=1}^{T}\min_{j\in[n]}\left(h_{i}\eta_{i,j}+\tau_{i}\mu_{i,j}\right)\leq t ^{k(T)}\leq t\). Thus

\[\mathbb{P}\left(\inf_{k\in S_{t}}\mathbbm{1}\left[\text{prog}(x^{k})<T\right]< 1\right)\leq\mathbb{P}\left(\sum_{i=1}^{T}\hat{t}_{i}\leq t\right)=\mathbb{P} \left(\sum_{i=1}^{T}\min_{j\in[n]}\left(h_{i}\eta_{i,j}+\tau_{i}\mu_{i,j}\right) \leq t\right)\quad\forall t\geq 0.\]

**(Part 2): The Chernoff Method**

Let us fix \(s\geq 0\) and \(\hat{t}\geq 0\). Using the Chernoff method, we have

\[\mathbb{P}\left(\sum_{i=1}^{T}\hat{t}_{i}\leq\hat{t}\right) =\mathbb{P}\left(-s\left(\sum_{i=1}^{T}\hat{t}_{i}\right)\geq-s \hat{t}\right)=\mathbb{P}\left(\exp\left(-s\sum_{i=1}^{T}\hat{t}_{i}\right) \geq\exp\left(-s\hat{t}\right)\right)\] (63) \[\leq e^{s\hat{t}}\mathbb{E}\left[\exp\left(-s\sum_{i=1}^{T}\hat{t }_{i}\right)\right].\]

Let us bound the expected value separately. For all \(j\in[T]\), let us define \(\mathcal{G}_{j}\) as the \(\sigma\)-algebra generated by random variables

\[b_{1,j}^{\eta},\ldots,b_{n,j}^{\eta}\] (64) \[\xi^{1,1},\xi^{1,2},\ldots,\xi^{1,b_{1,j}^{\eta}-1},\] \[\ldots\] \[\xi^{n,1},\xi^{n,2},\ldots,\xi^{1,b_{n,j}^{\eta}-1},\] \[b_{1,j}^{\mu},\ldots,b_{n,j}^{\mu},\] \[S^{1,1},S^{1,2},\ldots,S^{1,b_{1,j}^{\mu}-1},\] \[\ldots\] \[S^{n,1},S^{n,2},\ldots,S^{n,b_{n,j}^{\mu}-1}.\]

The \(\sigma\)-algebra \(\mathcal{G}_{T}\) contains all information about the random variables before the moment when \(\text{prog}(x^{k})=T-1\). Then, we have

\[\mathbb{E}\left[\exp\left(-s\sum_{i=1}^{T}\hat{t}_{i}\right)\right]=\mathbb{E }\left[\left.\mathbb{E}\left[\left.\exp\left(-s\sum_{i=1}^{T-1}\hat{t}_{i}-s \hat{t}_{T}\right)\right|\mathcal{G}_{T}\right]\right].\]

Note that if the random variables from (64) are "fixed," then \(\hat{t}_{i}\) is deterministic for all \(i\in[T-1]\) because \(\hat{t}_{i}\) is a deterministic function of (64), and does not depend on other subsequent random variables.

Let us show it using a contradiction proof. Without the loss of generality, assume that \(\hat{t}_{T-1}\) depends on \(\xi^{1,b_{1,T}^{\eta}}\not\in\eqref{eq:c1}\). By the definition of \(b_{1,T}^{\eta}\), it would mean that the first time when the server can get a vector \(g^{k}\) with a non-zero coordinate in the index \(T-1\) is _after_ the moment \(t^{k(T-1)}\). We get a contradiction since \(t^{k(T-1)}\) is the first time when the algorithm return an iterate with a non-zero coordinate in the index \(T-1\).

Thus, \(\hat{t}_{i}\) is \(\mathcal{G}_{T}\)-measurable for all \(i\in[T-1]\) and

\[\mathbb{E}\left[\exp\left(-s\sum_{i=1}^{T}\hat{t}_{i}\right)\right]=\mathbb{E }\left[\exp\left(-s\sum_{i=1}^{T-1}\hat{t}_{i}\right)\mathbb{E}\left[\left. \exp\left(-s\hat{t}_{T}\right)\right|\mathcal{G}_{T}\right]\right].\] (65)

Let us fix \(t^{\prime}\geq 0\), then, since \(\hat{t}_{T}\geq 0\), we have

\[\mathbb{E}\left[\left.e^{-s\hat{t}_{T}}\right|\mathcal{G}_{T}\right] =\mathbb{E}\left[\left.e^{-s\hat{t}_{T}}\right|\hat{t}_{T}\leq t ^{\prime},\mathcal{G}_{T}\right]\mathbb{P}\left(\hat{t}_{T}\leq t^{\prime} \big{|}\mathcal{G}_{T}\right)+\mathbb{E}\left[\left.e^{-s\hat{t}_{T}}\right| \hat{t}_{T}>t^{\prime},\mathcal{G}_{T}\right]\left(1-\mathbb{P}\left(\hat{t}_ {T}\leq t^{\prime}\big{|}\mathcal{G}_{T}\right)\right)\] (66)

We now use the result of the following lemma that we prove separately.

**Lemma P.3**.: _Using the notations from the proof of Lemma P.2, we have_

\[\mathbb{P}\left(\hat{t}_{j}\leq t^{\prime}\big{|}\mathcal{G}_{j} \right)\leq 1-\prod_{m=1}^{n}\left(1-\left(1-(1-p_{\omega}\right)^{\left\lfloor \frac{t^{\prime}}{t_{m}}\right\rfloor}\right)\left(1-(1-p_{\sigma})^{\left\lfloor \frac{t^{\prime}}{t_{m}}\right\rfloor}\right)\right)\] (67)

_for all \(j\in[T]\)._

Let us temporarily define

\[p^{\prime}:=1-\prod_{m=1}^{n}\left(1-\left(1-(1-p_{\omega})^{ \left\lfloor\frac{t^{\prime}}{t_{m}}\right\rfloor}\right)\left(1-(1-p_{\sigma })^{\left\lfloor\frac{t^{\prime}}{t_{m}}\right\rfloor}\right)\right)\]

We substitute (67) to (66) and (65) to obtain

\[\mathbb{E}\left[\exp\left(-s\sum_{i=1}^{T}\hat{t}_{i}\right) \right]\leq\left(p^{\prime}+e^{-st^{\prime}}\left(1-p^{\prime}\right)\right) \mathbb{E}\left[\exp\left(-s\sum_{i=1}^{T-1}\hat{t}_{i}\right)\right]\leq \left(p^{\prime}+e^{-st^{\prime}}\left(1-p^{\prime}\right)\right)^{T}.\]

Next, using (63), we get

\[\mathbb{P}\left(\sum_{i=1}^{T}\hat{t}_{i}\leq\hat{t}\right)\leq e ^{s\hat{t}}\left(p^{\prime}+e^{-st^{\prime}}\left(1-p^{\prime}\right)\right)^ {T}=e^{s\hat{t}-st^{\prime}T}\left(1+\left(e^{st^{\prime}}-1\right)p^{\prime} \right)^{T}.\]

Let us take \(s=\nicefrac{{1}}{{t^{\prime}}}\), and get

\[\mathbb{P}\left(\sum_{i=1}^{T}\hat{t}_{i}\leq\hat{t}\right)\leq e ^{\hat{t}/t^{\prime}-T}\left(1+\left(e-1\right)p^{\prime}\right)^{T}\leq e^{ \hat{t}/t^{\prime}-T+2p^{\prime}T}.\] (68)

Let us recall the definition of \(p^{\prime}:\)

\[p^{\prime}:=1-\prod_{m=1}^{n}\left(1-\left(1-(1-p_{\omega})^{ \left\lfloor\frac{t^{\prime}}{t_{m}}\right\rfloor}\right)\left(1-(1-p_{\sigma })^{\left\lfloor\frac{t^{\prime}}{t_{m}}\right\rfloor}\right)\right)=1-\prod_{ m=1}^{n}\left(1-q_{m}\right)\]

where we define \(q_{m}:=\left(1-(1-p_{\omega})^{\left\lfloor\frac{t^{\prime}}{t_{m}}\right\rfloor }\right)\left(1-(1-p_{\sigma})^{\left\lfloor\frac{t^{\prime}}{t_{m}}\right\rfloor }\right)\in[0,1].\) Using Lemma C.1, we have

\[p^{\prime}\leq\sum_{m=1}^{n}q_{m}.\]

Using the inequality16\(1-(1-p)^{m}\leq pm\) for all \(p\in[0,1]\) and \(m\in\mathbb{N}_{0},\) we can get the following three inequalities:

Footnote 16: We implicitly assume that \(1-(1-p)^{m}=0\) if \(p=1\) and \(m=0.\) See footnote 17 for the details.

\[q_{m}\leq 1-(1-p_{\sigma})^{\left\lfloor\frac{t^{\prime}}{t_{m}} \right\rfloor}\leq p_{\sigma}\left\lfloor\frac{t^{\prime}}{t_{m}}\right\rfloor,\]

\[q_{m}\leq 1-(1-p_{\omega})^{\left\lfloor\frac{t^{\prime}}{t_{m}} \right\rfloor}\leq p_{\omega}\left\lfloor\frac{t^{\prime}}{t_{m}}\right\rfloor,\]

and

\[q_{m}\leq\left(1-(1-p_{\omega})^{\left\lfloor\frac{t^{\prime}}{t _{m}}\right\rfloor}\right)\left(1-(1-p_{\sigma})^{\left\lfloor\frac{t^{\prime} }{t_{m}}\right\rfloor}\right)\leq p_{\omega}\left\lfloor\frac{t^{\prime}}{t_ {m}}\right\rfloor p_{\sigma}\left\lfloor\frac{t^{\prime}}{h_{m}}\right\rfloor.\]

Therefore, we get

\[q_{m}\leq\min\left\{p_{\sigma}\left\lfloor\frac{t^{\prime}}{h_{m}} \right\rfloor,p_{\omega}\left\lfloor\frac{t^{\prime}}{t_{m}}\right\rfloor,p_{ \omega}p_{\sigma}\left\lfloor\frac{t^{\prime}}{t_{m}}\right\rfloor\left\lfloor \frac{t^{\prime}}{h_{m}}\right\rfloor\right\}\]\[p^{\prime}\leq\sum_{m=1}^{n}\min\left\{p_{\sigma}\left\lfloor\frac{t^{\prime}}{h_{m }}\right\rfloor,p_{\omega}\left\lfloor\frac{t^{\prime}}{\tau_{m}}\right\rfloor,p _{\omega}p_{\sigma}\left\lfloor\frac{t^{\prime}}{\tau_{m}}\right\rfloor\left \lfloor\frac{t^{\prime}}{h_{m}}\right\rfloor\right\}.\] (69)

Now, we have to take the right \(t^{\prime}\). Assume that \(s^{*}(j)\) is the solution of

\[\left(\sum_{m=1}^{j}\frac{1}{\frac{2\tau_{m}}{p_{\omega}}+\frac{4\tau_{m}h_{m }}{p_{\omega}p_{\sigma}s}+\frac{2h_{m}}{p_{\sigma}}}\right)^{-1}=s\] (70)

and

\[j^{*}=\inf\left\{j\in\left[n\right]|s^{*}(j)<\max\{h_{j+1},\tau_{j+1}\}\right\} \in[n],\qquad\max\{h_{n+1},\tau_{n+1}\}\equiv\infty.\]

Then we take \(t^{\prime}=\frac{1}{24}s^{*}(j^{*})\). If \(j^{*}=1\), then

\[s^{*}(j^{*})=\left(\frac{1}{\frac{2\tau_{1}}{p_{\omega}}+\frac{4\tau_{m}h_{1 }}{p_{\omega}p_{\sigma}s^{*}(j^{*})}+\frac{2h_{1}}{p_{\sigma}}}\right)^{-1} \geq\left(\frac{1}{\frac{2\tau_{1}}{p_{\omega}}+\frac{2h_{1}}{p_{\sigma}}} \right)^{-1}\geq\left(\frac{1}{2\tau_{1}+2h_{1}}\right)^{-1}\geq\frac{1}{2} \max\{h_{1},\tau_{1}\}.\]

Otherwise, if \(j^{*}>1\), since \(s^{*}(j^{*})\leq s^{*}(j^{*}-1)\), we have

\[s^{*}(j^{*}) =\left(\sum_{m=1}^{j^{*}}\frac{1}{\frac{2\tau_{m}}{p_{\omega}}+ \frac{4\tau_{m}h_{m}}{p_{\omega}p_{\sigma}s^{*}(j^{*})}+\frac{2h_{m}}{p_{\sigma }}}\right)^{-1}\] \[\geq\left(\sum_{m=1}^{j^{*}-1}\frac{1}{\frac{2\tau_{m}}{p_{\omega }}+\frac{4\tau_{m}h_{m}}{p_{\omega}p_{\sigma}s^{*}(j^{*}-1)}+\frac{2h_{m}}{p_{ \sigma}}}+\frac{1}{\frac{2\tau_{j^{*}}}{p_{\omega}}+\frac{2h_{j^{*}}}{p_{ \sigma}}}\right)^{-1}\] \[\geq\left(\sum_{m=1}^{j^{*}-1}\frac{1}{\frac{2\tau_{m}}{p_{\omega }}+\frac{4\tau_{m}h_{m}}{p_{\omega}p_{\sigma}s^{*}(j^{*}-1)}+\frac{2h_{m}}{p_{ \sigma}}}+\frac{1}{\max\{h_{j^{*}},\tau_{j^{*}}\}}\right)^{-1}.\]

By the definitions of \(s^{*}(j^{*}-1)\) and \(j^{*}\), we have

\[\left(\sum_{m=1}^{j^{*}-1}\frac{1}{\frac{2\tau_{m}}{p_{\omega}}+ \frac{4\tau_{m}h_{m}}{p_{\omega}p_{\sigma}s^{*}(j^{*}-1)}+\frac{2h_{m}}{p_{ \sigma}}}\right)^{-1}=s^{*}(j^{*}-1)\geq\max\{h_{j^{*}},\tau_{j^{*}}\}.\]

Therefore, we get

\[s^{*}(j^{*})\geq\left(\frac{1}{\max\{h_{j^{*}},\tau_{j^{*}}\}}+ \frac{1}{\max\{h_{j^{*}},\tau_{j^{*}}\}}\right)^{-1}=\frac{1}{2}\max\{h_{j^{*} },\tau_{j^{*}}\}.\]

Using \(s^{*}(j^{*})\geq\frac{1}{2}\max\{h_{j^{*}},\tau_{j^{*}}\}\), we obtain

\[t^{\prime}=\frac{1}{24}\max\left\{\frac{1}{2}\max\{h_{j^{*}}, \tau_{j^{*}}\},s^{*}(j^{*})\right\}\geq\frac{1}{48}\min_{j\in[n]}\max\{\max\{h_ {j},\tau_{j}\},s^{*}(j)\}.\] (71)

We use the last inequality later. Let us return to the inequality (69). Using the definition of \(j^{*}\), we obtain \(\left\lfloor\frac{t^{\prime}}{\tau_{m}}\right\rfloor=0\) or \(\left\lfloor\frac{t^{\prime}}{h_{m}}\right\rfloor=0\) for all \(m>j^{*}\) and

\[p^{\prime} \leq\sum_{m=1}^{n}\min\left\{p_{\sigma}\left\lfloor\frac{t^{ \prime}}{h_{m}}\right\rfloor,p_{\omega}\left\lfloor\frac{t^{\prime}}{\tau_{m}} \right\rfloor,p_{\omega}p_{\sigma}\left\lfloor\frac{t^{\prime}}{\tau_{m}} \right\rfloor\left\lfloor\frac{t^{\prime}}{h_{m}}\right\rfloor\right\}\] \[=\sum_{m=1}^{j^{*}}\min\left\{p_{\sigma}\left\lfloor\frac{t^{ \prime}}{h_{m}}\right\rfloor,p_{\omega}\left\lfloor\frac{t^{\prime}}{\tau_{m}} \right\rfloor,p_{\omega}p_{\sigma}\left\lfloor\frac{t^{\prime}}{\tau_{m}} \right\rfloor\left\lfloor\frac{t^{\prime}}{h_{m}}\right\rfloor\right\}\] \[\leq\sum_{m=1}^{j^{*}}\min\left\{\frac{p_{\sigma}t^{\prime}}{h_{m}},\frac{p_{\omega}t^{\prime}}{\tau_{m}},\frac{p_{\omega}p_{\sigma}t^{\prime 2}}{\tau_{m}h_{m}} \right\},\]where we use \(\lfloor x\rfloor\leq x\) for all \(x\geq 0.\) Using \(\max\{x,y,z\}\geq\frac{1}{3}\left(x+y+z\right)\) for all \(x,y,z\geq 0,\) we have

\[p^{\prime}\leq\sum_{m=1}^{j^{*}}\left(\max\left\{\frac{h_{m}}{p_{\sigma}t^{ \prime}},\frac{\tau_{m}}{p_{\omega}t^{\prime}},\frac{\tau_{m}h_{m}}{p_{\omega}p _{\sigma}t^{\prime 2}}\right\}\right)^{-1}\leq\sum_{m=1}^{j^{*}}\frac{3}{\frac{ \tau_{m}}{p_{\omega}t^{\prime}}+\frac{\tau_{m}h_{m}}{p_{\omega}p_{\sigma}t^{ \prime 2}}+\frac{h_{m}}{p_{\sigma}t^{\prime}}}.\]

Since \(t^{\prime}=\frac{1}{24}s^{*}(j^{*}),\) we obtain

\[p^{\prime}\leq\sum_{m=1}^{j^{*}}\frac{3}{\frac{24\tau_{m}}{p_{\omega}s^{*}(j^{ *})}+\frac{24\tau_{m}h_{m}}{p_{\omega}p_{\sigma}(s^{*}(j^{*}))^{2}}+\frac{24h _{m}}{p_{\sigma}s^{*}(j^{*})}}\leq\frac{1}{4}\sum_{m=1}^{j^{*}}\frac{1}{\frac {2\tau_{m}}{p_{\omega}s^{*}(j^{*})}+\frac{4\tau_{m}h_{m}}{p_{\omega}p_{\sigma} (s^{*}(j^{*}))^{2}}+\frac{2h_{m}}{p_{\sigma}s^{*}(j^{*})}}.\]

Note that \(s^{*}(j^{*})\) is the solution of (70), thus, we get

\[p^{\prime}\leq\frac{1}{4}.\]

Substituting this inequality to (68), we obtain

\[\mathbb{P}\left(\sum_{i=1}^{T}\hat{t}_{i}\leq\hat{t}\right)\leq e^{\hat{t}/t^ {\prime}-\frac{T}{2}}.\]

For \(\hat{t}\leq t^{\prime}\left(\frac{T}{2}+\log\delta\right),\) we have

\[\mathbb{P}\left(\sum_{i=1}^{T}\hat{t}_{i}\leq\hat{t}\right)\leq\delta.\]

Recall that the definition of \(t^{\prime}.\) Using (71), we have

\[t^{\prime}\geq\frac{1}{48}\min_{j\in[n]}\max\{\max\{h_{j},\tau_{j}\},s^{*}(j)\}.\]

The last term equals to the _equilibrium time_\(t^{*}(\nicefrac{{1}}{{p_{\omega}}},\nicefrac{{1}}{{p_{\sigma}}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n})\) from Def. 3.1 since the pairs \((h_{j},\tau_{j})\) are sorted by \(\max\{h_{j},\tau_{j}\}.\) Thus, we obtain

\[t^{\prime}\geq\frac{1}{48}t^{*}(\nicefrac{{1}}{{p_{\omega}}},\nicefrac{{1}}{ {p_{\sigma}}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n}).\]

Finally, we obtain

\[\mathbb{P}\left(\inf_{k\in S_{t}}\mathbb{1}\left[\text{prog}(x^{k})<T\right]< 1\right)\leq\mathbb{P}\left(\sum_{i=1}^{T}\hat{t}_{i}\leq t\right)\leq\delta\] (72)

for

\[t\leq\frac{1}{48}t^{*}(\nicefrac{{1}}{{p_{\omega}}},\nicefrac{{1}}{{p_{ \sigma}}},h_{1},\tau_{1},\ldots,h_{n},\tau_{n})\left(\frac{T}{2}+\log\delta \right).\]

### Proof of Lemma p.3

**Lemma p.3**.: _Using the notations from the proof of Lemma p.2, we have_

\[\mathbb{P}\left(\hat{t}_{j}\leq t^{\prime}\big{|}\mathcal{G}_{j}\right)\leq 1 -\prod_{m=1}^{n}\left(1-\left(1-(1-p_{\omega})^{\left\lfloor\frac{t^{\prime}}{ \tau_{m}}\right\rfloor}\right)\left(1-(1-p_{\sigma})^{\left\lfloor\frac{t^{ \prime}}{\tau_{m}}\right\rfloor}\right)\right)\] (67)

_for all \(j\in[T].\)_

Proof.: We prove the result for \(j=T.\) The proofs for the cases \(1\leq j<T\) are the same. We consider the conditional probability

\[\mathbb{P}\left(\hat{t}_{T}\leq t^{\prime}\big{|}\mathcal{G}_{T}\right)= \mathbb{P}\left(\min_{m\in[n]}\left\{h_{m}\eta_{m,T}+\tau_{m}\mu_{m,T}\right\} \leq t^{\prime}\bigg{|}\mathcal{G}_{T}\right).\]Let us consider the \(\sigma\)-algebra \(\mathcal{H}_{T}\) generated by (64) with \(j=T\) and

\[e_{1,T}^{\eta},\ldots,e_{n,T}^{\eta}\] (73) \[\xi^{1,1},\xi^{1,2},\ldots,\xi^{1,e_{1,T}^{\eta}-1},\] \[\ldots\] \[\xi^{n,1},\xi^{n,2},\ldots,\xi^{1,e_{n,T}^{\eta}-1},\] \[e_{1,T}^{\mu},\ldots,e_{n,T}^{\mu},\] \[S^{1,1},S^{1,2},\ldots,S^{1,e_{1,T}^{\mu}-1},\] \[\ldots\] \[S^{n,1},S^{n,2},\ldots,S^{n,e_{n,T}^{\mu}-1}.\]

By the construction, \(e_{m,T}^{\eta}\geq b_{m,T}^{\eta}\) and \(e_{m,T}^{\mu}\geq b_{m,T}^{\mu}\). Since \(\mathcal{G}_{T}\subseteq\mathcal{H}_{T}\), we have

\[\mathbb{P}\left(\hat{t}_{T}\leq t^{\prime}\big{|}\mathcal{G}_{T}\right)= \mathbb{E}\left[\left.\mathbb{P}\left(\min_{m\in[n]}\left\{h_{m}\eta_{m,T}+ \tau_{m}\mu_{m,T}\right\}\leq t^{\prime}\right|\mathcal{H}_{T}\right)\right| \mathcal{G}_{T}\right].\]

Since \(\mathcal{G}_{T}\subseteq\mathcal{H}_{T}\), then \(b_{m,T}^{\mu}\) is \(\mathcal{H}_{T}\)-measurable. By the definition of \(e_{m,T}^{\eta}\), \(\eta_{m,T}\) are \(\mathcal{H}_{T}\)-measurable. Let us show it using a contradiction proof. Without the loss of generality, assume that \(\eta_{m,T}\) depends on \(\xi^{1,e_{m,T}^{\eta}}\not\in\eqref{eq:1}\). It would mean that the first time, when \(\xi^{m,i}=1\) after the iteration \(k(T-1)\), happens with \(i\geq e_{m,T}^{\eta}\). At the same time, by the definition of \(e_{m,T}^{\eta}\), there exists \(i<e_{m,T}^{\eta}\) such that \(\xi^{m,i}=1\) calculated after the iteration \(k(T-1)\). We get a contradiction. Given \(\mathcal{H}_{T}\), \(\mu_{m,T}\) are mutually independent since \(\{S^{m,j}\}_{j=1}^{\infty}\) are mutually independent and \(e_{m,T}^{\mu}\) are \(\mathcal{H}_{T}\)-measurable. Therefore, we have

\[\mathbb{P}\left(\hat{t}_{T}\leq t^{\prime}\big{|}\mathcal{G}_{T}\right) =\mathbb{E}\left[\left.\mathbb{P}\left(\min_{m\in[n]}\left\{h_{m} \eta_{m,T}+\tau_{m}\mu_{m,T}\right\}\leq t^{\prime}\right|\mathcal{H}_{T} \right)\right|\mathcal{G}_{T}\right]\] (74) \[=1-\mathbb{E}\left[\left.\mathbb{P}\left(\bigcap_{m=1}^{n}\left\{ h_{m}\eta_{m,T}+\tau_{m}\mu_{m,T}>t^{\prime}\right\}\right|\mathcal{H}_{T} \right)\right|\mathcal{G}_{T}\right]\] \[=1-\mathbb{E}\left[\left.\prod_{m=1}^{n}\mathbb{P}\left(h_{m} \eta_{m,T}+\tau_{m}\mu_{m,T}>t^{\prime}\right|\mathcal{H}_{T}\right)\right| \mathcal{G}_{T}\right].\]

Let us consider the probability \(\mathbb{P}\left(h_{m}\eta_{m,T}+\tau_{m}\mu_{m,T}>t^{\prime}|\mathcal{H}_{T}\right)\) :

\[\mathbb{P}\left(h_{m}\eta_{m,T}+\tau_{m}\mu_{m,T}>t^{\prime}| \mathcal{H}_{T}\right) =1-\mathbb{P}\left(h_{m}\eta_{m,T}+\tau_{m}\mu_{m,T}\leq t^{ \prime}|\mathcal{H}_{T}\right)\] (75) \[\geq 1-\mathbb{P}\left(h_{m}\eta_{m,T}\leq t^{\prime},\tau_{m}\mu _{m,T}\leq t^{\prime}|\mathcal{H}_{T}\right)\] \[=1-\mathbb{E}\left[\left.\mathbbm{1}\left[h_{m}\eta_{m,T}\leq t ^{\prime}\right]\mathbbm{1}\left[\tau_{m}\mu_{m,T}\leq t^{\prime}\right]\right| \mathcal{H}_{T}\right]\]

because the event \(\{h_{m}\eta_{m,T}\leq t^{\prime}\}\bigcap\{\tau_{m}\mu_{m,T}\leq t^{\prime}\}\) follows from \(\{h_{m}\eta_{m,T}+\tau_{m}\mu_{m,T}\leq t^{\prime}\}\). Since \(\eta_{m,T}\) is \(\mathcal{H}_{T}\)-measurable, we have

\[\mathbb{P}\left(h_{m}\eta_{m,T}+\tau_{m}\mu_{m,T}>t^{\prime}| \mathcal{H}_{T}\right) \geq 1-\mathbb{E}\left[\left.\mathbbm{1}\left[\tau_{m}\mu_{m,T} \leq t^{\prime}\right]\right|\mathcal{H}_{T}\right]\mathbbm{1}\left[h_{m}\eta _{m,T}\leq t^{\prime}\right]\] \[=1-\mathbb{P}\left(\tau_{m}\mu_{m,T}\leq t^{\prime}|\mathcal{H}_{T }\right)\mathbbm{1}\left[h_{m}\eta_{m,T}\leq t^{\prime}\right].\]

Let us consider the probability \(\mathbb{P}\left(\tau_{m}\mu_{m,T}\leq t^{\prime}|\mathcal{H}_{T}\right).\) Given \(\mathcal{H}_{T},\) if \(e_{m,T}^{\mu}=\infty,\) then \(\mu_{m,T}=\infty\) and \(\mathbb{P}\left(\tau_{m}\mu_{m,T}\leq t^{\prime}|\mathcal{H}_{T}\right)=0.\) Otherwise, if \(e_{m,T}^{\mu}=e<\infty,\) then

\[\mu_{m,T}=\inf\{i\,|\,j\in S^{m,(i+e-1)}\text{ and }i\in\mathbb{N}\}.\]

and it is distributed with the _geometric_ distribution with \(p_{\omega}\). Thus, we have17

Footnote 17: We implicitly assume that \(1-(1-p_{\omega})\left\lfloor\frac{t^{\prime}}{r_{m}}\rfloor=0\) if \(p_{\omega}=1\) and \(\left\lfloor\frac{t^{\prime}}{r_{m}}\right\rfloor=0,\) because if \(t^{\prime}<\tau_{m},\) then \(\mathbb{P}\left(\tau_{m}\mu_{m,T}\leq t^{\prime}\right)=0\) for the r.v. \(\mu_{m,T}\) from the _geometric_ distribution for all \(p_{\omega}\in(0,1]\).

\[\mathbb{P}\left(\tau_{m}\mu_{m,T}\leq t^{\prime}|\mathcal{H}_{T}\right)=\begin{cases} 1-(1-p_{\omega})\left\lfloor\frac{t^{\prime}}{r_{m}}\right\rfloor,&e_{m,T}^{\mu} \leq 1-(1-p_{\omega})\left\lfloor\frac{t^{\prime}}{r_{m}}\right\rfloor,\\ 0,&e_{m,T}^{\mu}=\infty\end{cases}\]because the probability that \(j^{\text{th}}\) coordinate belongs to \(S^{m,(i+e-1)}\) equals to \(p_{\omega}\). We substitute this inequality to (75) and get

\[\mathbb{P}\left(h_{m}\eta_{m,T}+\tau_{m}\mu_{m,T}>t^{\prime}|\mathcal{H}_{T} \right)\geq 1-\left(1-(1-p_{\omega})^{\left\lfloor\frac{t^{\prime}}{\tau_{m}} \right\rfloor}\right)\mathbbm{1}\left[h_{m}\eta_{m,T}\leq t^{\prime}\right].\]

Next, we substitute this inequality to (74) and obtain

\[\mathbb{P}\left(\hat{t}_{T}\leq t^{\prime}\big{|}\mathcal{G}_{T}\right)\leq 1 -\mathbb{E}\left[\left.\prod_{m=1}^{n}\left(1-\left(1-(1-p_{\omega})^{\left\lfloor \frac{t^{\prime}}{\tau_{m}}\right\rfloor}\right)\right)\mathbbm{1}\left[h_{m} \eta_{m,T}\leq t^{\prime}\right]\right)\right|\mathcal{G}_{T}\right].\]

Given \(\mathcal{G}_{T},\,\eta_{m,T}\) are independent because \(b_{m,T}^{\eta}\) are \(\mathcal{G}_{T}\)-measurable. Thus

\[\mathbb{P}\left(\hat{t}_{T}\leq t^{\prime}\big{|}\mathcal{G}_{T}\right) \leq 1-\prod_{m=1}^{n}\left(1-\left(1-(1-p_{\omega})^{\left\lfloor \frac{t^{\prime}}{\tau_{m}}\right\rfloor}\right)\mathbb{E}\left[\left. \mathbbm{1}\left[h_{m}\eta_{m,T}\leq t^{\prime}\right]\right|\mathcal{G}_{T} \right]\right)\] \[=1-\prod_{m=1}^{n}\left(1-\left(1-(1-p_{\omega})^{\left\lfloor \frac{t^{\prime}}{\tau_{m}}\right\rfloor}\right)\mathbb{P}\left(h_{m}\eta_{m,T }\leq t^{\prime}|\mathcal{G}_{T}\right)\right).\]

Using the same reasoning as with \(\mu_{m,T},\) we get

\[\mathbb{P}\left(h_{m}\eta_{m,T}\leq t^{\prime}|\mathcal{G}_{T}\right)=\begin{cases} 1-(1-p_{\sigma})^{\left\lfloor\frac{t^{\prime}}{\tau_{m}}\right\rfloor},&b_{m, T}^{\eta}<\infty\\ 0,&b_{m,T}^{\eta}=\infty\end{cases}\leq 1-(1-p_{\sigma})^{\left\lfloor\frac{t^{ \prime}}{\tau_{m}}\right\rfloor}.\]

because, given \(\mathcal{G}_{T},\,\eta_{m,T}\) equals \(\infty\) or a random variable distributed according to the _geometric_ distribution with \(p_{\sigma}\). Therefore, we obtain

\[\mathbb{P}\left(\hat{t}_{T}\leq t^{\prime}\big{|}\mathcal{G}_{T}\right)\leq 1- \prod_{m=1}^{n}\left(1-\left(1-(1-p_{\omega})^{\left\lfloor\frac{t^{\prime}}{ \tau_{m}}\right\rfloor}\right)\left(1-(1-p_{\sigma})^{\left\lfloor\frac{t^{ \prime}}{\tau_{m}}\right\rfloor}\right)\right).\]

### Another Construction

In the proof of Theorem 0.5, we could use the following construction:

(**Step 3**: Compression Operator) Let us define \(p_{\omega}:=\frac{1}{\omega+1}\). In our construction, we take a compressor that outputs random coordinates of an input vector, scaled by \(\nicefrac{{1}}{{p_{\omega}}}\), where each coordinate is taken with the probability \(p_{\omega}\). Each worker has access to the independent compressed realizations of a such compressor. More formally, we assume that

\[[\mathcal{C}(x;S)]_{j}:=\begin{cases}\frac{1}{p_{\omega}}x_{j},&j\in S,\\ 0,&j\not\in S,\end{cases}\quad\forall j\in[T],\]

where \(S\) is a random subset of \([T],\) where each element from \([T]\) appears with the probability \(p_{\omega}\)_independently_. Then

\[\mathbb{E}_{S}\left[[\mathcal{C}(x;S)]_{j}\right]=x_{j}\]

and

\[\mathbb{E}_{S}\left[\left\|\mathcal{C}(x;S)\right\|^{2}\right]=\mathbb{E}_{S} \left[\sum_{j=1}^{T}\mathbbm{1}\left[j\in S\right]\frac{1}{p_{\omega}^{2}}x_{ j}^{2}\right]=\sum_{j=1}^{T}\mathbb{P}\left(j\in S\right)\frac{1}{p_{\omega}^{2}}x_{ j}^{2}=\sum_{j=1}^{T}\frac{1}{p_{\omega}}x_{j}^{2}=\left(\omega+1\right) \left\|x\right\|^{2}.\]

Thus, we have \(\mathcal{C}\in\mathbb{U}(\omega).\) We take mutually independent distributions \(\mathcal{D}_{i}^{\mathcal{C}}\) that generate random subsets \(S\) described above.

This construction is also valid and does not require the assumption \(\omega+1\lesssim\nicefrac{{L\Delta}}{{\varepsilon}}\). However, unlike the construction from Theorem 0.5, this construction can return a random number of non-zero coordinates.

Experiments

The experiments were prepared in Python. The distributed environment was emulated on machines with Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz and 64 cores.

### Experiments with Logistic Regression

We start our experiments with a practical setup: a logistic regression problem with the _MNIST_ dataset (LeCun et al., 2010). The optimization steps of algorithms are emulated in Python, where we fix the number of workers to \(n=100\), each worker has access to the _MNIST_ dataset and sample \(4\) samples when calculating a stochastic gradient. We compare Shadowheart SGD with QSGD, Asynchronous SGD (we implement the version from (Koloskova et al., 2022)), Minibatch SGD, and \(\mathsf{SGD}_{\mathsf{one}}\). \(\mathsf{SGD}_{\mathsf{one}}\) is the method described in Sec. 7, where SGD is run on the fastest worker locally. In Shadowheart SGD, we fine-tune the parameter \(\nicefrac{{\sigma^{2}}}{{\epsilon}}\in\{1,5,10,20,30,40,80,120,150,200\}\). In all the methods, we also finetune the step sizes. The dimension of the problem in the logistic regression problem is \(d=7850\). In Shadowheart SGD and QSGD, we take \(\text{Rand}K\) with \(K=700\).

We assume that the computations time \(h_{i}\) of a stochastic gradient equals to \(\sqrt{i}\) seconds in the \(i^{\text{th}}\) worker. We consider three communication time setups, where it takes \(\hat{\tau}_{i}\) seconds to send _one coordinate_ from the \(i^{\text{th}}\) worker to the server and

1. \(\hat{\tau}_{i}=\sqrt{i}/d\) (High-speed communications),
2. \(\hat{\tau}_{i}=\sqrt{i}/d^{3/4}\) (Medium-speed communications),
3. \(\hat{\tau}_{i}=\sqrt{i}/d^{1/2}\) (Low-speed communications).

In the high-speed regime, the communication between the server and the worker is relatively fast. At the same time, in low-speed regimes, communication is expensive. We are ready to present the results of our experiments in Fig. 0(a), 0(c), and 0(b).

In Figure 0(a), one can see that Shadowheart SGD, Asynchronous SGD, and Minibatch SGD are the fastest because it is not expensive to send a non-compressed vector in the "high-speed communications" regime. \(\mathsf{SGD}_{\mathsf{one}}\) is the slowest since it utilizes only one worker.

Next, we analyze Figure 0(b), where Shadowheart SGD and SGD\({}_{\mathsf{one}}\) have the best performance. SGD\({}_{\mathsf{one}}\) improves the convergence relative to other methods because the communication speed is much slower than in Figure 0(a), and it is expensive to send a non-compressed vector.

One can see that Shadowheart SGD is very robust to all regimes and has one of the best convergence rates in all experiments. Notably, in the "medium-speed communications" regime, where it is still expensive to send a non-compressed vector, our new method converges faster than other baseline methods.

### Experiments with quadratic optimization tasks and multiplicative noise

In real machine learning tasks, it is not easy to control noise. Thus, we generated synthetic quadratic optimization tasks where we can control the noise of stochastic gradients. In particular, we consider

\[f(x)=\frac{1}{2}x^{\top}\mathbf{A}x-b^{\top}x\]

for all \(x\in\mathbb{R}^{d}\) and take \(d=1000,\)

\[\mathbf{A}=\frac{1}{4}\left(\begin{array}{cccc}2&-1&&0\\ -1&\ddots&\ddots&\\ &\ddots&\ddots&-1\\ 0&&-1&2\end{array}\right)\in\mathbb{R}^{d\times d}\quad\text{ and }\quad b=\frac{1}{4}\left[ \begin{array}{c}-1\\ 0\\ \vdots\\ 0\end{array}\right]\in\mathbb{R}^{d}.\]

We consider the following stochastic gradients:

\[[\nabla f(x;\xi)]_{j}:=\nabla_{j}f(x)\left(1+1\left[j>\text{prog}(x)\right] \left(\frac{\xi}{p}-1\right)\right)\quad\forall x\in\mathbb{R}^{d},\] (76)

where \(\xi\sim\text{Bernoulli}(p)\) for all \(i\in[n],\) and \(p\in(0,1].\) We denote \([x]_{j}\) as the \(j^{\text{th}}\) index of a vector \(x\in\mathbb{R}^{d}.\) In our experiments, we take the starting point \(x^{0}=[\sqrt{d},0,\ldots,0]^{\top}\) and \(p\in\{10^{-3},10^{-4}\}\); the smaller \(p\) the larger the noise of stochastic gradients.

#### q.2.1 Discussion of the experiments from Sec. Q.2.2

Using this setup, in Figures 2, 3, 4, we fix all parameters except one that we vary to understand the dependencies. In all experiments, we observe that Shadowheart SGD is the most robust to input changes among other _centralized_ methods (QSGD, Asynchronous SGD, Minibatch SGD) and can converge significantly faster. At the same time, we observe that SGD\({}_{\mathsf{one}}\) can be faster than our method in some setups. It happens in the regimes when communication is expensive (see Figure 1(a)), which is expected and discussed in Sec. 7. Even if communication is expensive, SGD\({}_{\mathsf{one}}\) starts to slow down relative to other methods when we increase the noise (compare Figures 1(b) and 1(a)). The following experiments agree with our theoretical discussion in Sec. 7.

#### q.2.2 Plots

In these experiments, we take \(n=10000,\)\(p=10^{-3},\)\(h_{i}=\sqrt{i},\)\(\dot{\tau}_{i}=\sqrt{i}/d^{3/4}\) as base parameters; in each plot, we vary one parameter.

Figure 2: SGD\({}_{\mathsf{one}}\) starts to slow down relative to Shadowheart SGD and other methods when we increase the noise.

### Experiments with quadratic optimization tasks and additive noise

In this section, we consider the same problem as in Sec. Q.2. However, unlike the _multiplicative_ noise, we consider the following _additive_ noise:

\[[\nabla f(x;\xi)]_{j}:=\nabla_{j}f(x)+\zeta\quad\forall x\in\mathbb{R}^{d},\]

where \(\zeta\sim\mathcal{N}(0,\sigma^{2})\) is a sample from the normal distribution. Be default, we take \(n=100\) workers, the dimension \(d=100\), and use the Rand\(1\) compressor (\(\omega=d/1-1=99\)), \(x_{0}=[1,\cdots,1]^{\top}\), \(\sigma=10^{-1}\), \(\varepsilon=10^{-4}\); thus, the ratio \(\nicefrac{{\sigma^{2}}}{{\varepsilon}}=10^{2}\). For all methods, we choose the step sizes in such a way that they converge to the same neighborhood of the stationary point.

In Figure 5, we sample \(h_{i}^{k}\) and \(\dot{\tau}_{i}^{k}\) from the uniform distribution \(U(0.1,1)\), hence the communication and computation time vary on each iteration for each client. If we increase the number of clients \(n\), Shadowheart SGD improves (Fig. 5) compared to other methods, confirming our theory.

In Figure 6, we can see the similar results with different ratios \(\nicefrac{{\sigma^{2}}}{{\varepsilon}}\): Shadowheart SGD is much better when the ratio is large (Fig. 6). On the other hand, when \(\nicefrac{{\sigma^{2}}}{{\varepsilon}}\) is small (Fig. 5(a)) SGD\({}_{\textsf{one}}\) can be better because, intuitively, we only need a few workers to find the minimum with a small noise (see also Sec. 7).

Next, we perform a series of experiments with different computation time and communication times ratios. We take \(\nicefrac{{i^{k}}}{{h_{i}^{k}}}=c\) for all \(i\in[n]\), where \(c>0\).

In Figure 7, we take \(h_{i}^{k}\sim U(0.1,1)\) and \(\dot{\tau}_{i}^{k}\sim c\cdot U(0.1,1)\). Shadowheart SGD is better in the high and medium communication speed regimes (Fig. 6(b)), when the communication times are not too large. On the other hand, with large \(c=10^{2}\), Shadowheart SGD spends much time on sending gradients to the server, whereas SGD\({}_{\textsf{one}}\) does not spend time on communication and does not compress (Fig. 6(c)). Similar to Figure 7, we obtain the results with \(h_{i}^{k}=\sqrt{i}\) and \(\dot{\tau}_{i}^{k}=c\cdot\sqrt{i}\) in Figure 8.

Figure 4: Shadowheart SGD improves when we decrease the computation times from \(\sqrt{i}\) to \(1\).

Figure 3: The non-compressed methods Asynchronous SGD and Minibatch SGD slow down relative to Shadowheart SGD when we increase the communication times.

#### 0.3.1 Plots

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Section 3 and Table 1 Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Sections 4.1, 4.2, and 7.2 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Section 1 and Appendix * Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section Q Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification: In the supplementary material Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Section Q Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: While each particular experiment in every plot from Section Q was run with one seed, the amount of provided experiments and considered settings can give a high confidence in our judgments that the experimental part supports the theoretical part, which is our main contribution. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Section Q Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our work considers a mathematical problem from the machine learning domain. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work considers a mathematical problem from the machine learning domain. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work considers a mathematical problem from the machine learning domain. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Section Q Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code for the experiments is in the supplementary materials. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.