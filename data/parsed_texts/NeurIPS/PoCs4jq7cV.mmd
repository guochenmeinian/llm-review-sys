# Inference via Interpolation:

Contrastive Representations Provably Enable Planning and Inference

 Benjamin Eysenbach

Princeton University

eysenbach@princeton.edu

&Vivek Myers

UC Berkeley

vmyers@berkeley.edu

&Ruslan Salakhutdinov

Carnegie Mellon University

rsalakhu@cs.cmu.edu

&Sergey Levine

UC Berkeley

svelvine@eecs.berkeley.edu

Equal contribution.

###### Abstract

Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-dimensional matrix. In one special case, inferring intermediate representations will be equivalent to interpolating between the learned representations. We validate our theory using numerical simulations on tasks up to 46-dimensions.1

Footnote 1: Code: https://github.com/vivekmyers/contrastive_planning

## 1 Introduction

Probabilistic modeling of time-series data has applications ranging from robotic control [1] to material science [2], from cell biology [3] to astrophysics [4]. These applications are often concerned with two questions: _predicting_ future states (e.g., what will this cell look like in an hour), and _inferring_ trajectories between two given states. However, answering these questions often requires reasoning over high-dimensional data, which can be challenging as most tools in the standard probabilistic toolkit require generation. Might it be possible to use discriminative methods (e.g., contrastive learning) to perform such inferences?

Many prior works aim to learn representations that are easy to predict while retaining salient bits of information. For time-series data, we want the representation to remain a sufficient statistic for distributions related to time -- for example, they should retain bits required to predict future states (or representations thereof). While generative methods [5, 6, 7, 8] have this property, they tend to be computationally expensive (see, e.g., [9]) and can be challenging to scale to high-dimensional observations.

[MISSING_PAGE_EMPTY:2]

Ideally, these representations should retain information required to predict future observations and infer likely paths between pairs of observations. Many approaches use an autoencoder, learning representations that retain the bits necessary to reconstruct the input observation, while also regularizing the representations to compressed or predictable [6; 27; 28; 29; 30; 31]. A prototypical method is the sequential VAE [5], which is computationally expensive to train because of the reconstruction loss, but is easy to use for inference. Our work shares the aims of prior prior methods that attempt to linearize the dynamics of nonlinear systems [32; 33; 34; 35; 36; 37], including videos [38; 39]. Our work aims to retain uncertainty estimates over predictions (like the sequential VAE) without requiring reconstruction. Avoiding reconstruction is appealing _practically_ because it decreases the computational requirements and number of hyperparameters; and _theoretically_ because it means that representations only need to retain bits about temporal relationships and not about the bits required to reconstruct the original observation.

**Contrastive Learning.** Contrastive learning methods circumvent reconstruction by learning representations that merely classify if two events were sampled from the same joint distribution [40; 17; 41]. When applied to representing states along trajectories, contrastive representations learn to classify whether two points lie on the same trajectory or not [42; 43; 44; 10; 25]. Empirically, prior work in computer vision and NLP has observed that contrastive learning acquires representations where interpolation between representations corresponds to changing the images in semantically meaningful ways [45; 46; 47; 48; 49; 16].

Our analysis will be structurally similar to prior theoretical analysis on explaining why word embeddings can solve analogies [50; 51; 52]. Our work will make a Gaussianity assumption similar to Arora et al. [51] and our Markov assumption is similar to the random walks analyzed in Arora et al. [51], Hashimoto et al. [53]. Our paper builds upon and extends these results to answer questions such as: "what is the distribution over future observations representations?" and "what is the distribution over state (representations) that would occur on the path between one observation and another?" While prior work is primarily aimed at explaining the good performance of contrastive word embeddings (see, e.g., [51]), we are primarily interested in showing how similar contrastive methods are an effective tool for inference over high-dimensional time series data. Our analysis will show how representations learned via temporal contrastive learning (i.e., without reconstruction) are sufficient statistics for inferring future outcomes and can be used for performing inference on a graphical model (a problem typically associated with generative methods).

**Goal-oriented decision making.** Much work on time series representations is done in service of learning goal-reaching behavior, an old problem [54; 55] that has received renewed attention in recent years [56; 57; 58; 59; 60; 61; 62; 63]. Some of the excitement in goal-conditioned RL is a reflection of the recent success of self-supervised methods in computer vision [64] and NLP [65]. Our analysis will study a variant of contrastive representation learning proposed in prior work for goal-conditioned RL [42; 43]. These methods are widespread, appearing as learning objectives for learning value functions [66; 67; 68; 69; 70; 71; 72; 73], as auxiliary objectives [74; 75; 76; 77; 71; 78; 79], in objectives for model-based RL [80; 81; 82; 32], and in exploration methods [83; 84]. Our analysis will highlight connections between these prior methods, the classic successor representation [85; 86], and probabilistic inference.

**Planning.** Planning lies at the core of many RL and control methods, allowing methods to infer the sequence of states and actions that would occur if the agent navigated from one state to a goal state. While common methods such as PRM [87] and RRT [88] focus on building random graphs, there is a strong community focusing on planning methods based on probabilistic inference [19; 89; 90]. The key challenge is scaling to high-dimensional settings. While semi-parametric methods make progress on this problem this limitation through semi-parametric planning [91; 92; 93], it remains unclear how to scale any of these methods to high-dimensional settings when states do not lie on a low-dimensional manifold. Our analysis will show how contrastive representations may lift this limitation, with experiments validating this theory on 39-dimensional and 46-dimensional tasks.

## 3 Preliminaries

Our aim is to learn representations of time series data such that the spatial arrangement of representations corresponds to the temporal arrangement of the underlying data: if one example occurs shortly after another, then they should be mapped to similar representations. This problem setting arises in many areas, including video understanding and reinforcement learning. To de fine this problem formally, we will define a Markov process with states \(x_{t}\) indexed by time \(t\):4\(p(x_{1:T}\mid x_{0})=\prod_{t=0}^{T}p(x_{t+1}\mid x_{t})\). The dynamics \(p(x_{t+1}\mid x_{t})\) tell us the immediate next state, and we can define the distribution over states \(t\) steps in the future by marginalizing over the intermediate states, \(p_{t}(x_{t}\mid x_{0})=\int p(x_{1:t}\mid x_{0})\,\mathrm{d}x_{1:t-1}\). A key quantity of interest will be the \(\gamma\)-discounted state occupancy measure, which corresponds to a time-averaged distribution over future states:

Footnote 4: This can be extended to _controlled_ Markov processes appending the previous action to the observations.

\[p_{t+}(x_{t+}=x)=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}p_{t}(x_{t}=x).\] (1)

Contrastive learning.Our analysis will focus on applying contrastive learning to a particular data distribution. Contrastive learning [10, 94, 40] acquires representations using "positive" pairs \((x,x^{+})\) and "negative" pairs \((x,x^{-})\). While contrastive learning typically learns just one representation, we will use two different representation for the two elements of the pair; that is, our analysis will use terms like \(\phi(x)\), \(\psi(x^{+})\) and \(\psi(x^{-})\). We assume all representations lie in \(\mathbb{R}^{k}\).

The aim of contrastive learning is to learn representations such that positive pairs have similar representations (\(\phi(x)\approx\psi(x^{+})\)) while negative pairs have dissimilar representations (\(\phi(x)\neq\psi(x^{-})\)). Let \(p(x,x^{+})\) be the joint distribution over positive pairs (i.e., \((x,x^{+})\sim p(x,x^{+})\)). We will use the product of the marginal distributions to sample negative pairs (\((x,x^{-})\sim p(x)p(x)\)). Let \(B\) be the batch size, and note that the positive samples \(x_{j}^{+}\) at index \(j\) in the batch serve as _negatives_ for \(x_{i}\) for any \(i\neq j\). Our analysis is based on the infoNCE objective without resubstitution [10, 11]:

\[\max_{\phi(\cdot),\psi(\cdot)}\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B}\sim p (x,x^{+})}\!\left[\sum_{i=1}^{B}\log\frac{e^{-\frac{1}{2}\|\phi(x_{i})-\psi(x_{ i}^{+})\|_{2}^{2}}}{\sum_{j\neq i}e^{-\frac{1}{2}\|\phi(x_{i})-\psi(x_{j}^{+})\|_{2}^{2} }}+\log\frac{e^{-\frac{1}{2}\|\phi(x_{i})-\psi(x_{j}^{+})\|_{2}^{2}}}{\sum_{j \neq i}e^{-\frac{1}{2}\|\phi(x_{j})-\psi(x_{i}^{+})\|_{2}^{2}}}\right]\] (2)

We will use the symmetrized version of this objective [17], where the denominator is the sum across rows of a logits matrix and once where it is a sum across the.

While contrastive learning is typically applied to an example \(x\) and an augmentation \(x^{+}\sim p(x\mid x)\) of that same example (e.g., a random crop), we will follow prior work [10, 42] in using the time series _dynamics_ to generate the positive pairs, so \(x^{+}\) will be an observation that occurs temporally after \(x\). While our experiments will sample positive examples from the discounted state occupancy measure (\(x^{+}\sim p_{t+}(x_{t+}\mid x)\)) in line with prior work [43], our analysis will also apply to different distributions (e.g., always sampling a state \(k\) steps ahead).

While prior work typically constrains the representations to have a constant norm (i.e., to lie on the unit hypersphere) [10], we will instead constrain the _expected_ norm of the representations is bounded, a difference that will be important for our analysis:

\[\tfrac{1}{k}\,\mathbb{E}_{p(x)}\left[\|\psi(x)\|_{2}^{2}\right]\leq c.\] (3)

Because the norm scales with the dimension of the representation, we have scaled down the left side by the representation dimension, \(k\). In practice, we will impose this constraint by adding a regularization term \(\lambda\,\mathbb{E}_{p(x)}\left[\|\psi(x)\|_{2}^{2}\right]\) to the infoNCE objective (Eq. 2) and dynamically tuning the weight \(\lambda\) via dual gradient descent.

### Key assumptions

This section outlines the two key assumptions behind our analysis, both of which have some theoretical justification. Our main assumption examines the distribution over representations:

**Assumption 1**.: _Regularized, temporal contrastive learning acquires representations whose marginal distribution representations \(p(\psi)\triangleq\int p(x)\,\mathds{1}(\psi(x)=\psi)\,\mathrm{d}x\) is an isotropic Gaussian distribution:_

\[p(\psi)=\mathcal{N}(\psi;\mu=0,\sigma=c\cdot I).\] (4)

In Appendix A.1 we extend prior work [20] provide some theoretical intuition for why this assumption should hold: namely, that the isotropic Gaussian is the distribution that maximizes entropy subject to an expected L2 norm constraint (Eq. 3) [95, 96, 97]. Our analysis also assumes that the learned representations converge to the theoretical minimizer of the infoNCE objective:

**Assumption 2**.: _Applying contrastive learning to the symmetrized infoNCE objective results in representations that encode a probability ratio:_

\[e^{-\frac{1}{2}\|\phi(x_{0})-\psi(x)\|_{2}^{2}}=\frac{p_{t+}(x_{t+}=x\mid x_{0})} {p(x)C}.\] (5)

This assumption holds under ideal conditions [98; 99] (see Appendix A.5),5 but we nonetheless call this an "assumption" because it may not hold in practice due to sampling and function approximation error. This assumption means the learned representations are sufficient statistics for predicting the probability (ratio) of future states: these representations must retain all the information pertinent to reasoning about _temporal_ relationships, but need not retain information about the precise contents of the observations. As such, they may be much more compressed than representations learned via reconstruction.

Footnote 5: While the result of Ma and Collins [98] has \(C(x)\) depending on \(x\), the symmetrized version [17] removes the dependence on \(x\).

Combined, these assumptions will allow us to express the distribution over sequences of representations as a Gauss-Markov chain. The denominator in Assumption 2, \(p(x)\), may have a complex distribution, but Assumption 1 tells us that the distribution over _representations_ has a simpler form. This will allow us to rearrange Assumption 2 to express the conditional distribution over representations as the product of two Gaussian likelihoods. Note that the left hand side of Assumption 2 already looks like a Gaussian likelihood.

## 4 Contrastive Representations Make Inference Easy

In this section, our main result will be to show how representations learned by (regularized) contrastive learning are distributed according to a Gauss-Markov chain, making it straightforward to perform inference (e.g., planning, prediction) over these representations. Our proof technique will combine (known) results about Gaussian distributions with (known) results about contrastive learning. We start by discussing an important choice of parametrization (Section 4.1) that facilitates prediction (Section 4.2) before presenting the main result in Section 4.3.

### A Parametrization for Shared Encoders

This section describes the two encoders (\(\psi(\cdot),\phi(\cdot)\)) to compute representations of \(x\) and \(x^{+}\). While prior work in computer vision and NLP literature use the same encoder for both \(x\) and \(x^{+}\), this decision does not make sense for many time-series data as it would imply that our prediction for \(p(x_{t}\mid x_{0})\) is the same as our prediction for \(p(x_{0}\mid x_{t})\). However, the difficulty of transiting from \(x_{0}\) to \(x_{t}\) (e.g., climbing to the peak of a mountain) might be more difficult than the reverse (e.g., shedding down a mountain). Our proposed parametrization will handle this asymmetry.

We will treat the encoder \(\psi(\cdot)\) as encoding the contents of the state. We will additionally learn a matrix \(A\) so that the function \(\psi\mapsto A\psi\) corresponds to a (multi-step) prediction of the future representation. To map this onto contrastive learning, we will use \(\phi(x)\triangleq A\psi(x)\) as the encoder for the initial state. One way of interpreting this encoder is as an additional linear projection applied on top of \(\psi(\cdot)\), a design similar to those used in other areas of contrastive learning [41]. Once learned, we can use these encoders to answer questions about prediction (Section 4.2) and planning (Section 4.3).

### Representations Encode a Predictive Model

Given an initial state \(x_{0}\), what states are likely to occur in the future? Answering this question directly in terms of high-dimensional states is challenging, but our learned representations provide a straightforward answer. Let \(\psi_{0}=\psi(x_{0})\) and \(\psi_{t+}=\psi(x_{t+})\) be random variables representing the representations of the initial state and a future state. Our aim is to estimate the distribution over these future representations, \(p(\psi_{t+}\mid\psi_{0})\). We will show that the learned representations encode this distribution.

Figure 2: A parametrization for temporal contrastive learning.

**Lemma 1**.: _Under the assumptions from Section3, the distribution over representations of future states follows a Gaussian distribution with mean parameter given by the initial state representation:_

\[p(\psi_{t+}=\psi\mid\psi_{0})=\mathcal{N}\Big{(}\mu=\frac{c}{c+1}A\psi_{0}, \Sigma=\frac{c}{c+1}I\Big{)}.\] (6)

The main takeaway here is that the distribution over future representations has a convenient, closed form solution. The representation norm constraint, \(c\), determines the shrinkage factor \(\frac{c}{c+1}\in[0,1)\); highly regularized settings (small \(c\)) move the mean closer towards the origin and decrease the variance, as visualized in \(\mathrm{Fig.~{}\ref{eq:1}}\). Regardless of the constraint \(c\), the predicted mean is a linear function \(\psi\mapsto\frac{c}{c+1}A\psi\). The proof is in AppendixA.2. The proof technique is similar to that of the law of the unconscious statistician.

### Planning over One Intermediate State

We now show how these representations can be used for a specific type of planning: given an initial state \(x_{0}\) and a future state \(x_{t+}\), infer the representation of an intermediate "waypoint" state \(x_{w}\). The next section will extend this analysis to inferring the entire sequence of intermediate states. We assume \(x_{0}\to x_{w}\to x_{t+}\) form a Markov chain where \(x_{w}\sim p(x_{t+}\mid x_{0}=x_{0})\) and \(x_{t+}\sim p(x_{t+}\mid x_{0}=x_{w})\) are both drawn from the discounted state occupancy measure (Eq.1). Let random variable \(\psi_{w}=\psi(x_{w})\) be the representation of this intermediate state. Our main result is that the posterior distribution over waypoint _representations_ has a closed form solution in terms of the initial state representation and future state representation:

**Theorem 2**.: _Under Assumptions1 and 2, the posterior distribution over waypoint representations is a Gaussian whose mean and covariance are linear functions of the initial and final state representations:_

\[p(\psi_{w}\mid\psi_{0},\psi_{t+})=\mathcal{N}\Big{(}\psi_{w};\mu=\Sigma(A^{T} \psi_{t+}+A\psi_{0}),\Sigma^{-1}=\frac{c}{c+1}A^{T}A+\frac{c+1}{c}I\Big{)}.\]

The proof (AppendixA.3) uses the Markov property together with Lemma1. The main takeaway from this lemma is that the posterior distribution takes the form of a simple probability distribution (a Gaussian) with parameters that are linear functions of the initial and final representations.

We give three examples to build intuition:

**Example 1:**\(A=I\) and the \(c\) is very large (little regularization). Then, the covariance is \(\Sigma^{-1}\approx 2I\) and the mean is the simple average of the initial and final representations \(\mu\approx\frac{1}{2}(\psi_{0}+\psi_{t+})\). In other words, the waypoint representation is the midpoint of the line \(\psi_{0}\to\psi_{t+}\).

**Example 2:**\(A\) is a rotation matrix and \(c\) is very large. Rotation matrices satisfy \(A^{T}=A^{-1}\) so the covariance is again \(\Sigma^{-1}\approx 2I\). As noted in Section4.2, we can interpret \(A\psi_{0}\) as a _prediction_ of which representations will occur after \(\psi_{0}\). Similarly, \(A^{-1}\psi_{t+}=A^{T}\psi_{t+}\) is a prediction of which representations will occur before \(\psi_{t+}\). Theorem2 tells us that the mean of the waypoint distribution is the simple average of these two predictions, \(\mu\approx\frac{1}{2}(A^{T}\psi_{t+}+A\psi_{0})\).

**Example 3:**\(A\) is a rotation matrix and \(c=0.01\) (very strong regularization). In this case \(\Sigma^{-1}=\frac{0.01}{0.01+1}A^{T}A+\frac{0.01+1}{0.01}I\approx 100I\), so \(\mu\approx\frac{1}{100}(\psi_{0}+\psi_{t+})\approx 0\). Thus, in the case of strong regularization, the posterior concentrates around the origin.

### Planning over Many Intermediate States

This section extends the analysis to multiple intermediate states. Again, we will infer the posterior distribution of the representations of these intermediate states, \(\psi_{w_{1}},\psi_{w_{2}},\cdots\). We assume that these states form a Markov chain.

**Theorem 3**.: _Given observations from a Markov chain \(x_{0}\to x_{1}\cdots x_{t+}\), the joint distribution over representations is a Gaussian distribution. Using \(\psi_{1:n}=(\psi_{w_{1}},\cdots,\psi_{w_{n}})\) to denote the concatenated representations of each observation, we can write this distribution as_

\[p(\psi_{1:n})\propto\exp\bigl{(}-\tfrac{1}{2}\psi_{1:n}^{T}\Sigma^{-1}\psi_{1: n}+\eta^{T}\psi_{1:n}\bigr{)},\]

Figure 3: Predicting representations of future states.

_where \(\Sigma^{-1}\) is a tridiagonal matrix_

\[\Sigma^{-1}=\begin{pmatrix}\frac{c}{c+1}A^{T}A+\frac{c+1}{c}I&-A^{T}&\\ -A&\frac{c}{c+1}A^{T}A+\frac{c+1}{c}I&-A^{T}&\ddots\end{pmatrix}\quad\text{and }\eta= \begin{pmatrix}A\psi_{0}\\ 0\\ \vdots\\ A^{T}\psi_{t+}\end{pmatrix}.\]

This distribution can be written in the canonical parametrization as \(\Sigma=\Lambda^{-1}\) and \(\mu=\Sigma\eta\). Recall that Gaussian distributions are closed under marginalization. Thus, once in this canonical parametrization, the marginal distributions can be obtained by reading off individual entries of these parameters:

\[p(\psi_{i}\mid\psi_{0},\psi_{t+})=\mathcal{N}\left(\psi_{i};\mu_{i}=(\Sigma \eta)^{(i)},\Sigma_{i}=(\Lambda^{-1})^{(i,i)}\right).\]

The key takeaway here is that this posterior distribution over waypoints is Gaussian, and it has a closed form expression in terms of the initial and final representations (as well as regularization parameter \(c\) and the learned matrix \(A\)).

In the general case of \(n\) intermediate states, the posterior distribution is

\[p(\psi_{w_{1}}\cdots\psi_{w_{n}}\mid\psi_{0},\psi_{t+})\propto e^{-\frac{1+ \frac{1}{2}}{\sum_{i=1}^{n}\|\frac{c}{c+1}A\psi_{w_{i}}-\psi_{w_{i+1}}\|_{2}^{ 2}}},\]

where \(\psi_{w_{0}}=\psi_{0}\) and \(\psi_{w_{n+1}}=\psi_{t+}\). This corresponds to a chain graphical model with edge potentials \(f(\psi,\psi^{\prime})=e^{-\frac{1+\frac{1}{2}}{\|\frac{c}{c+1}A\psi-\psi^{ \prime}\|_{2}^{2}}}\).

**Special case.** To build intuition, consider the special case where \(A\) is a rotation matrix and \(c\) is very large, so \(\frac{c}{c+1}A^{T}A+\frac{c+1}{c}\approx 2I\). In this case, \(\Sigma^{-1}\) is a (block) second difference matrix [100]:

\[\Sigma^{-1}=\begin{pmatrix}\begin{smallmatrix}2I&-I\\ -I&2I&-I\\ -I&\ddots\end{smallmatrix}\end{pmatrix}.\]

The inverse of this matrix has a closed form solution [101, Pg. 471], allowing us to obtain the mean of each waypoint in closed form:

\[\mu_{i}=(1-\lambda(i))A\psi_{0}+\lambda(i)A^{T}\psi_{t+},\] (7)

where \(\lambda(i)=\frac{i}{n+1}\). Thus, each posterior mean is a convex combination of the (forward prediction from the) initial representation and the (backwards prediction from the) final representation. When \(A\) is the identity matrix, the posterior mean is simple linear interpolation between the initial and final representations!

## 5 Numerical Simulation

We include several didactic experiments to illustrate our results. All results and figures can be reproduced by running make in the source code: https://github.com/vivekmyers/contrastive_planning. The expected compute time is a few hours on a A6000 GPU. Figures in this section show error across different training and dataset split seeds.

### Synthetic Dataset

To validate our analysis, we design a time series task with 2D points where inference over intermediate points (i.e., in-filling) requires nonlinear interpolation. Fig. 4_(Top Left)_ shows the dataset of time series data, starting at the origin and spiraling outwards, with each trajectory using a randomly-chosen initial angle. We applied contrastive learning with the parametrization in Section 4.1 to these data and used the learned representations to solve prediction and planning problems (see Fig. 4 for details). Note that these predictions correctly handle the nonlinear structure of these data -- states nearby the initial state in Euclidean space that are not temporally adjacent are assigned low likelihood.

### Solving Mazes with Inferred Representations

Our next experiment studies whether the inferred representations are useful for solving a control task. We took a 2d maze environment and dataset from prior work (Fig. 5, _Left_) [102] and learned encodersfrom this dataset. To solve the maze, we take the observation of the starting state and goal state, compute the representations of these states, and use the analysis in Section 4.3 to infer the sequence of intermediate representations. We visualize the results using a nearest neighbor retrieval (Fig. 5, _Left_). Figure 7 contains additional examples.

Finally, we studied whether these representations are useful for control. We implemented a simple proportional controller for this maze. As expected, this proportional controller can successfully navigate to close goals, but fails to reach distant goals (Fig. 5, _Right_). However, if we use the proportional controller to track a series of waypoints planned using our representations (i.e., the orange dots shown in Fig. 5 (_Left_)), the success rate increases by up to \(4.5\times\). To test the importance of _nonlinear_ representations, we compare with a "PCA" baseline that predicts waypoints by interpolating between the principal components of the initial state and goal state. The better performance of our method indicates the importance of doing the interpolation using representations that are _nonlinear_ functions of the input observations. While prior methods learn representations to encode temporal distances, it

Figure 4: **Numerical simulation of our analysis.**_(Top Left)_ _Toy dataset of time-series data consisting of many outwardly-spiraling trajectories. We apply temporal contrastive learning to these data._(Top Right)_ _For three initial observations (_\(\blacksquare\)_), we use the learned representations to predict the distribution over future observations. Note that these distributions correctly capture the spiral structure._(Bottom Left)_ _For three observations (_\(\cdot\)_), we use the learned representations to predict the distribution over preceding observations._(Bottom Right)_ _Given an initial and final observation, we plot the inferred posterior distribution over the waypoint (Section 4.3). The representations capture the shape of the distribution._

Figure 5: Using inferred paths over our contrastive representations for control boosts success rates by \(4.5\times\) on the most difficult goals (\(18\%\to 84\%\)). Alternative representation learning techniques fail to improve performance when used for planning.

is unclear whether these methods support inference via interpolation. To test this hypothesis, we use one of these methods ("VIP" [69]) as a baseline. While the VIP representations likely encode similar bits as our representations, the better performance of the contrastive representations indicates that the VIP representations do not expose those bits in a way that makes planning easy.

### Higher dimensional tasks

In this section we provide preliminary experiments showing the planning approach in Section4 scales to higher dimensional tasks. We used two datasets from prior work [102]: door-human-v0 (39-dimensional observations) and hammer-human-v0 (46-dimensional observations). After learning encoders on these tasks, we evaluated the inference capabilities of the learned representations. Given the first and last observation from a trajectory in a validation set, we use linear interpolation (see Eq.7) to infer the representation of five intermediate waypoint representations.

We evaluate performance in two ways. **Quantitatively**, we measure the mean squared error between each of the true waypoint observations and those inferred by our method. Since our method infers representations, rather than observations, we use a nearest-neighbor retrieval on a validation set so that we can measure errors in the space of observations. **Qualitatively**, we visualize the high-dimensional observations from the validation trajectory using a 2-dimensional TSNE [103] embedding, overlying the infer waypoints from our method; as before, we convert the representations inferred by our method to observations using nearest neighbors.

We compare with three alternative methods in Fig.6. To test the importance of representation learning, we first naively interpolate between the initial and final observations ("no planning"). The poor performance of this baseline indicates that the input time series are highly nonlinear. Similarly, interpolating the principle components of the initial and final observations ("PCA") performs poorly, again highlighting that the input time series is highly nonlinear and that our representations are doing more than denoising (i.e., discarding directions of small variation). The third baseline, "VIP" [69],

Figure 6: Planning for 39-dimensional robotic door opening. _(Top Left)_ We use a dataset of trajectories demonstrating door opening from prior work [102] to learn representations. _(Top Right)_ We use our method and three baselines to infer one intermediate waypoint between the first and last observation in a trajectory from a held-out validation set. Errors are measured using the mean squared error with the true waypoint observation; predicted representations are converted to observations using nearest neighbors on a validation set. _(Bottom)_ We visualize a TSNE [103] of the states along the sampled trajectory as blue circles, with the transparency indicating the index along the trajectory. The inferred plan is shown as red circles connected by arrows. Our method generates better plans than alternative representation learning methods (PCA, VIP).

learns representations to encode temporal distances using approximate dynamic programming. Like our method, VIP avoids reconstruction and learns nonlinear representations of the observations. However, the results in Fig. 6 highlight that VIP's representations do not allow users to plan by interpolation. The error bars shown in Fig. 6 (_Top Right_) show the standard deviation over 500 trajectories sampled from the validation set. For reproducibility, we repeated this entire experiment on another task, the 46-dimensional hammer-human-v0 from D4RL. The results, shown in Appendix Fig. 8, support the conclusions above. Taken together, these results show that our procedure for interpolating contrastive representations continues to be effective on tasks where observations have dozens of dimensions.

## 6 Discussion

Representation learning is at the core of many high-dimensional time-series modeling questions, yet how those representations are learned is often disconnected with the inferential task. The main contribution of this paper is to show how _discriminative_ techniques can be used to acquire compact representations that make it easy to answer inferential questions about time. The precise objective and parametrization we studied is not much different from that used in practice, suggesting that either our theoretical results might be adapted to the existing methods, or that practitioners might adopt these details so they can use the closed-form solutions to inference questions. Our work may also have implications for studying the structure of learned representations. While prior work often studies the geometry of representations as a post-hoc check, our analysis provides tools for studying _when_ interpolation properties are guaranteed to emerge, as well as _how_ to learn representations with certain desired geometric properties.

**Limitations.** Our analysis hinges on the two assumptions mentioned in Section 3.1, and it remains open how errors in those approximations translate into errors in our analysis. One important open question is whether it is always possible to satisfy these assumptions using sufficiently-expressive representations.

## Acknowledgments

We thank Seohong Park, Gautam Reddy, Chongyi Zheng, and anonymous reviewers for feedback and discussions that shaped this project. This work was supported by Princeton Research Computing resources at Princeton University. This work was partially supported by AFOSR FA9550-22-1-0273.

## References

* [1] Evangelos Theodorou, Jonas Buchli, and Stefan Schaal. A Generalized Path Integral Control Approach to Reinforcement Learning. _Journal of Machine Learning Research_, 11:3137-3181, 2010.
* [2] Hannes Jonsson, Greg Mills, and Karsten W Jacobsen. Nudged Elastic Band Method for Finding Minimum Energy Paths of Transitions. In _Classical and Quantum Dynamics in Condensed Phase Simulations_, pp. 385-404. World Scientific, 1998.
* [3] Wouter Saelens, Robrecht Cannoodt, Helena Todorov, and Yvan Saeys. A Comparison of Single-Cell Trajectory Inference Methods. _Nature Biotechnology_, 37(5):547-554, 2019.
* [4] Steven R Majewski, Ricardo P Schiavon, Peter M Frinchaboy, Carlos Allende Prieto, Robert Barkhouser, Dmitry Bizyaev, Basil Blank, Sophia Brunner, Adam Burton, Ricardo Carrera, et al. The Apache Point Observatory Galactic Evolution Experiment (APOGEE). _Astronomical Journal_, 154(3):94, 2017.
* [5] Shengjia Zhao, Jiaming Song, and Stefano Ermon. Towards Deeper Understanding of Variational Autoencoding Models. arXiv:1702.08658, 2017.
* [6] Yizhe Zhu, Martin Renqiang Min, Asim Kadav, and Hans Peter Graf. S3vae: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 6538-6547. 2020.
* [7] Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial Autoencoders. arXiv:1511.05644, 2015.

* [8] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially Learned Inference. In _International Conference on Learning Representations_. 2016.
* [9] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating Diverse High-Fidelity Images With Vq-VAE-2. _Neural Information Processing Systems_, 32, 2019.
* [10] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning With Contrastive Predictive Coding. arXiv:1807.03748, 2018.
* [11] Kihyuk Sohn. Improved Deep Metric Learning With Multi-Class N-Pair Loss Objective. _Neural Information Processing Systems_, 29, 2016.
* [12] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved Baselines With Momentum Contrastive Learning. arXiv:2003.04297, 2020.
* [13] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive Multiview Coding. In _Computer Vision-ECCV 2020: 16th European Conference_, pp. 776-794. 2020.
* [14] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised Feature Learning via Non-Parametric Instance Discrimination. In _IEEE Conference on Computer Vision and Pattern Recognition_, pp. 3733-3742. 2018.
* [15] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for Contrastive Learning of Visual Representations. In _International Conference on Machine Learning_, pp. 1597-1607. 2020.
* [16] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. arXiv:1301.3781, 2013.
* [17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In _International Conference on Machine Learning_, pp. 8748-8763. 2021.
* [18] Matthew Botvinick and Marc Toussaint. Planning as Inference. _Trends in Cognitive Sciences_, 16(10):485-488, 2012.
* [19] Hagai Attias. Planning by Probabilistic Inference. In _International Workshop on Artificial Intelligence and Statistics_, pp. 9-16. 2003.
* [20] Tongzhou Wang and Phillip Isola. Understanding Contrastive Representation Learning Through Alignment and Uniformity on the Hypersphere. In _International Conference on Machine Learning_, pp. 9929-9939. 2020.
* [21] George E Uhlenbeck and Leonard S Ornstein. On the Theory of the Brownian Motion. _Physical Review_, 36(5):823, 1930.
* [22] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. _Time Series Analysis: Forecasting and Control_. John Wiley & Sons, 2015.
* [23] Dmitry M Malioutov, Jason K Johnson, and Alan S Willsky. Walk-Sums and Belief Propagation in Gaussian Graphical Models. _Journal of Machine Learning Research_, 7:2031-2064, 2006.
* [24] Yair Weiss and William Freeman. Correctness of Belief Propagation in Gaussian Graphical Models of Arbitrary Topology. _Neural Information Processing Systems_, 12, 1999.
* [25] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal Contrastive Video Representation Learning. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 6964-6974. 2021.
* [26] Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. C-Learning: Learning to Achieve Goals via Recursive Classification. In _International Conference on Learning Representations_. 2020.
* [27] Siddharth Karamcheti, Suraj Nair, Annie S. Chen, Thomas Kollar, Chelsea Finn, Dorsa Sadigh, and Percy Liang. Language-Driven Representation Learning for Robotics. arXiv:2302.12766, 2023.
* [28] Seongmin Park and Jihwa Lee. Finetuning Pretrained Transformers Into Variational Autoencoders. arXiv:2108.02446, 2021.
* [29] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding. In _NAACL-Hlt_, pp. 4171-4186.

2019.
* [30] Micah Carroll, Orr Paradise, Jessy Lin, Raluca Georgescu, Mingfei Sun, David Bignell, Stephanie Milani, Katja Hofmann, Matthew Hausknecht, Anca Dragan, et al. UniMASK: Unified Inference in Sequential Decision Problems. arXiv:2211.10869, 2022.
* [31] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A Recurrent Latent Variable Model for Sequential Data. In _Neural Information Processing Systems_, volume 28. 2015.
* [32] Rui Shu, Tung Nguyen, Yinlam Chow, Tuan Pham, Khoat Than, Mohammad Ghavamzadeh, Stefano Ermon, and Hung Bui. Predictive Coding for Locally-Linear Control. In _International Conference on Machine Learning_, pp. 8862-8871. 2020.
* [33] Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to Control: A Locally Linear Latent Dynamics Model for Control From Raw Images. _Neural Information Processing Systems_, 28, 2015.
* [34] Ershad Banijamali, Rui Shu, Hung Bui, Ali Ghodsi, et al. Robust Locally-Linear Controllable Embedding. In _International Conference on Artificial Intelligence and Statistics_, pp. 1751-1759. 2018.
* [35] Brandon Cui, Yinlam Chow, and Mohammad Ghavamzadeh. Control-Aware Representations for Model-Based Reinforcement Learning. In _International Conference on Learning Representations_. 2020.
* [36] Tung D Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Temporal Predictive Coding for Model-Based Planning in Latent Space. In _International Conference on Machine Learning_, pp. 8130-8139. 2021.
* [37] Tung Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon. Non-Markovian Predictive Coding for Planning in Latent Space. 2020.
* [38] Ross Goroshin, Michael F Mathieu, and Yann LeCun. Learning to Linearize Under Uncertainty. _Neural Information Processing Systems_, 28, 2015.
* [39] Dinesh Jayaraman and Kristen Grauman. Slow and Steady Feature Analysis: Higher Order Temporal Coherence in Video. In _IEEE Conference on Computer Vision and Pattern Recognition_, pp. 3852-3861. 2016.
* [40] Michael Gutmann and Aapo Hyvarinen. Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models. In _International Conference on Artificial Intelligence and Statistics_, pp. 297-304. 2010.
* [41] Xinlei Chen and Kaiming He. Exploring Simple Siamese Representation Learning. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 15750-15758. 2021.
* [42] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. Time-Contrastive Networks: Self-Supervised Learning From Video. In _IEEE International Conference on Robotics and Automation_, pp. 1134-1141. 2018.
* [43] Benjamin Eysenbach, Tianjun Zhang, Sergey Levine, and Russ R Salakhutdinov. Contrastive Learning as Goal-Conditioned Reinforcement Learning. _Neural Information Processing Systems_, 35:35603-35620, 2022.
* [44] Mengda Xu, Zhenjia Xu, Cheng Chi, Manuela Veloso, and Shuran Song. Xskill: Cross Embodiment Skill Discovery. In _Conference on Robot Learning_, pp. 3536-3555. 2023.
* [45] Laurenz Wiskott and Terrence J. Sejnowski. Slow Feature Analysis: Unsupervised Learning of Invariances. _Neural Computation_, 14(4):715-770, 2002.
* [46] Jia-Wei Yan, Ci-Siang Lin, Fu-En Yang, Yu-Jhe Li, and Yu-Chiang Frank Wang. Semantics-Guided Representation Learning With Applications to Visual Synthesis. In _International Conference on Pattern Recognition_, pp. 7181-7187. 2021.
* [47] Alon Oring, Zohar Yakhini, and Yacov Hel-Or. Autoencoder Image Interpolation by Shaping the Latent Space. In _International Conference on Machine Learning_, pp. 8281-8290. 2021.
* [48] Ying-Cong Chen, Xiaogang Xu, Zhuotao Tian, and Jiaya Jia. Homomorphic Latent Space Interpolation for Unpaired Image-to-Image Translation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2403-2411. 2019.

* Liu et al. [2018] Xiaofeng Liu, Yang Zou, Lingsheng Kong, Zhihui Diao, Junliang Yan, Jun Wang, Site Li, Ping Jia, and Jane You. Data Augmentation via Latent Space Interpolation for Image Classification. In _International Conference on Pattern Recognition_, pp. 728-733. 2018.
* Levy and Goldberg [2014] Omer Levy and Yoav Goldberg. Linguistic Regularities in Sparse and Explicit Word Representations. In _Computational Natural Language Learning_, pp. 171-180. 2014.
* Arora et al. [2016] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A Latent Variable Model Approach to Pmi-Based Word Embeddings. _Transactions of the Association for Computational Linguistics_, 4:385-399, 2016.
* Allen and Hospedales [2019] Carl Allen and Timothy Hospedales. Analogies Explained: Towards Understanding Word Embeddings. In _International Conference on Machine Learning_, pp. 223-231. 2019.
* Hashimoto et al. [2016] Tatsunori B Hashimoto, David Alvarez-Melis, and Tommi S Jaakkola. Word Embeddings as Metric Recovery in Semantic Spaces. _Transactions of the Association for Computational Linguistics_, 4:273-286, 2016.
* Newell et al. [1959] Allen Newell, John C Shaw, and Herbert A Simon. Report on a General Problem Solving Program. In _IFIP Congress_, volume 256, p. 64. 1959.
* Laird et al. [1987] John E Laird, Allen Newell, and Paul S Rosenbloom. Soar: An Architecture for General Intelligence. _Artificial Intelligence_, 33(1):1-64, 1987.
* Chen et al. [2021] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision Transformer: Reinforcement Learning via Sequence Modeling. _Neural Information Processing Systems_, 34:15084-15097, 2021.
* Chane-Sane et al. [2021] Elliot Chane-Sane, Cordelia Schmid, and Ivan Laptev. Goal-Conditioned Reinforcement Learning With Imagined Subgoals. In _International Conference on Machine Learning_, pp. 1430-1440. 2021.
* Colas et al. [2021] Cedric Colas, Tristan Karch, Olivier Sigaud, and Pierre-Yves Oudeyer. Intrinsically Motivated Goal-Conditioned Reinforcement Learning: A Short Survey. _Preprint_, 2021.
* Yang et al. [2021] Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie Zhang. Rethinking Goal-Conditioned Supervised Learning and Its Connection to Offline RL. In _International Conference on Learning Representations_. 2021.
* Ma et al. [2022] Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How Far I'll Go: Offline Goal-Conditioned Reinforcement Learning via \(f\)-Advantage Regression. arXiv:2206.03023, 2022.
* Schroecker and Isbell [2020] Yannick Schroecker and Charles Isbell. Universal Value Density Estimation for Imitation Learning and Goal-Conditioned Reinforcement Learning. arXiv:2002.06473, 2020.
* Janner et al. [2021] Michael Janner, Qiyang Li, and Sergey Levine. Offline Reinforcement Learning as One Big Sequence Modeling Problem. _Neural Information Processing Systems_, 34:1273-1286, 2021.
* Hejna et al. [2023] Joey Hejna, Jensen Gao, and Dorsa Sadigh. Distance Weighted Supervised Learning for Offline Interaction Data. arXiv:2304.13774, 2023.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis With Latent Diffusion Models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 10684-10695. 2022.
* OpenAI [2023] OpenAI. GPT-4 Technical Report. arXiv:2303.08774, 2023.
* Zheng et al. [2024] Chongyi Zheng, Benjamin Eysenbach, Homer Rich Walke, Patrick Yin, Kuan Fang, Ruslan Salakhutdinov, and Sergey Levine. Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching From Offline Data. In _International Conference on Learning Representations_. 2024.
* Tian et al. [2020] Stephen Tian, Suraj Nair, Frederik Ebert, Sudeep Dasari, Benjamin Eysenbach, Chelsea Finn, and Sergey Levine. Model-Based Visual Planning With Self-Supervised Functional Distances. In _International Conference on Learning Representations_. 2020.
* Agarwal et al. [2019] Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement Learning: Theory and Algorithms. _CS Dept_, pp. 10-4, 2019.
* Ma et al. [2022] Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang. VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training. In _International Conference on Learning Representations_. 2022.

* [70] Yecheng Jason Ma, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman. LIV: Language-Image Representations and Rewards for Robotic Control. _International Conference on Machine Learning_, 2023.
* [71] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3M: A Universal Visual Representation for Robot Manipulation. In _Conference on Robot Learning_, pp. 892-909. 2023.
* [72] Bo Liu, Yihao Feng, Qiang Liu, and Peter Stone. Metric Residual Network for Sample Efficient Goal-Conditioned Reinforcement Learning. In _AAAI Conference on Artificial Intelligence_, volume 37, pp. 8799-8806. 2023.
* [73] Tongzhou Wang, Antonio Torralba, Phillip Isola, and Amy Zhang. Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. In _International Conference on Machine Learning_. 2023.
* [74] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-Efficient Reinforcement Learning With Self-Predictive Representations. In _International Conference on Learning Representations_. 2020.
* [75] Yunhao Tang, Zhaohan Daniel Guo, Pierre Harvey Richemond, Bernardo Avila Pires, Yash Chandak, Remi Munos, Mark Rowland, Mohammad Gheshlaghi Azar, Charline Le Lan, Clare Lyle, et al. Understanding Self-Predictive Learning for Reinforcement Learning. In _International Conference on Machine Learning_, pp. 33632-33656. 2023.
* [76] Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin. Decoupling Representation Learning From Reinforcement Learning. In _International Conference on Machine Learning_, pp. 9870-9879. 2021.
* [77] Homanga Bharadhwaj, Mohammad Babaeizadeh, Dumitru Erhan, and Sergey Levine. Information Prioritization Through Empowerment in Visual Model-Based RL. In _International Conference on Learning Representations_. 2021.
* [78] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre Cote, and R Devon Hjelm. Unsupervised State Representation Learning in Atari. _Neural Information Processing Systems_, 32, 2019.
* [79] Pablo Samuel Castro, Tyler Kastner, P. Panangaden, and Mark Rowland. MICo: Improved Representations via Sampling-Based State Similarity for Markov Decision Processes. In _Neural Information Processing Systems_. 2021.
* [80] Raj Ghugare, Homanga Bharadhwaj, Benjamin Eysenbach, Sergey Levine, and Russ Salakhutdinov. Simplifying Model-Based RL: Learning Representations, Latent-Space Models, and Policies With One Objective. In _International Conference on Learning Representations_. 2022.
* [81] Cameron S. Allen. Learning Markov State Abstractions for Deep Reinforcement Learning. In _Neural Information Processing Systems_. 2021.
* [82] Bogdan Mazoure, Benjamin Eysenbach, Ofir Nachum, and Jonathan Tompson. Contrastive Value Learning: Implicit Models for Simple Offline RL. In _Conference on Robot Learning_, pp. 1257-1267. 2023.
* [83] Zhaohan Guo, Shantanu Thakoor, Miruna Pislar, Bernardo Avila Pires, Florent Altche, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, et al. Byol-Explore: Exploration by Bootstrapped Prediction. _Neural Information Processing Systems_, 35:31855-31870, 2022.
* [84] Yilun Du, Chuang Gan, and Phillip Isola. Curious Representation Learning for Embodied Intelligence. _IEEE/CVF International Conference on Computer Vision_, pp. 10388-10397, 2021.
* [85] Peter Dayan. Improving Generalization for Temporal Difference Learning: The Successor Representation. _Neural Computation_, 5:613-624, 1993.
* [86] Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor Features for Transfer in Reinforcement Learning. _Neural Information Processing Systems_, 30, 2017.
* [87] Lydia E Kavraki, Petr Svestka, J-C Latombe, and Mark H Overmars. Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces. _IEEE Transactions on Robotics and Automation_, 12(4):566-580, 1996.

* [88] Steven M LaValle, James J Kuffner, BR Donald, et al. Rapidly-Exploring Random Trees: Progress and Prospects. _Algorithmic and Computational Robotics: New Directions_, 5:293-308, 2001.
* [89] Sep Thijssen and H. J. Kappen. Path Integral Control and State-Dependent Feedback. _Physical Review E_, 91(3):032104, 2015.
* [90] Grady Williams, Andrew Aldrich, and Evangelos Theodorou. Model Predictive Path Integral Control Using Covariance Variable Importance Sampling. arXiv:1509.01149, 2015.
* [91] Kuan Fang, Patrick Yin, Ashvin Nair, Homer Rich Walke, Gengchen Yan, and Sergey Levine. Generalization With Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks. In _Conference on Robot Learning_, pp. 106-117. 2023.
* [92] Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine. Search on the Replay Buffer: Bridging Planning and Reinforcement Learning. In _Neural Information Processing Systems_, volume 32. 2019.
* [93] Tianjun Zhang, Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine, and Joseph E Gonzalez. C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks. In _International Conference on Learning Representations_. 2021.
* [94] Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A Theoretical Analysis of Contrastive Unsupervised Representation Learning. In _International Conference on Machine Learning_, pp. 5628-5637. 2019.
* [95] Claude Elwood Shannon. A Mathematical Theory of Communication. _Bell System Technical Journal_, 27(3):379-423, 1948.
* [96] E. T. Jaynes. Information Theory and Statistical Mechanics. _Physical Review_, 106(4):620-630, 1957.
* [97] Keith Conrad. Probability Distributions and Maximum Likelihood. 2010.
* [98] Zhuang Ma and Michael Collins. Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency. In _Empirical Methods in Natural Language Processing_. 2018.
* [99] Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On Variational Bounds of Mutual Information. In _International Conference on Machine Learning_, pp. 5171-5180. 2019.
* [100] Nick Higham. What Is the Second Difference Matrix? https://nhigham.com/2022/01/31/what-is-the-second-difference-matrix/, 2022.
* [101] Morris Newman and John Todd. The Evaluation of Matrix Inversion Programs. _Journal of the Society for Industrial and Applied Mathematics_, 6(4):466-476, 1958.
* [102] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for Deep Data-Driven Reinforcement Learning. arXiv:2004.07219, 2020.
* [103] Laurens Van der Maaten and Geoffrey Hinton. Visualizing Data Using T-SNE. _Journal of Machine Learning Research_, 9(11), 2008.

Proofs

This section contains the proofs omitted from the main text.

### Marginal Distribution over Representations is Gaussian

Recall Assumption1, which states that the marginal distribution over representations is Gaussian.

**Assumption 1**.: _Regularized, temporal contrastive learning acquires representations whose marginal distribution representations \(p(\psi)\triangleq\int p(x)\,\mathds{1}(\psi(x)=\psi)\,\mathrm{d}x\) is an isotropic Gaussian distribution:_

\[p(\psi)=\mathcal{N}(\psi;\mu=0,\sigma=c\cdot I).\] (4)

We will motivate this statement by connecting the optimal contrastive infoNCE objective to the maximum entropy marginal distribution over the representations.

The infoNCE objective (Eq.2) can be decomposed into an alignment term and a uniformity term [20], where the uniformity term can be simplified as follows:

\[\mathbb{E}_{x\sim p(x)}\left[\log\mathbb{E}_{x^{-}\sim p(x)} \left[e^{-\frac{1}{2}\|A\psi(x^{-})-\psi(x)\|_{2}^{2}}\right]\right]\] \[=\frac{1}{N}\sum_{i=1}^{N}\log\left(\frac{1}{N-1}\sum_{j=1\cdots N,j\neq i}e^{-\frac{1}{2}\|A\psi(x_{i})-\psi(x_{j})\|_{2}^{2}}\right)\] \[=\frac{1}{N}\sum_{i=1}^{N}\log\left(\frac{1}{N-1}\sum_{j=1\cdots N,j\neq i}\underbrace{\frac{1}{(2\pi)^{k/2}}e^{-\frac{1}{2}\|A\psi(x_{i})-\psi( x_{j})\|_{2}^{2}}}_{\mathcal{N}(\mu=\psi(x_{j});\Sigma=I)}\right)+\frac{k}{2}\log(2\pi)\] \[=\frac{1}{N}\sum_{i=1}^{N}\log\hat{p}_{\text{GMM}}(\psi(x_{i}))+ \frac{k}{2}\log(2\pi)\] \[=-\hat{\mathcal{H}}[\psi(x)]+\frac{k}{2}\log(2\pi).\]

The derivation above extends that in Wang and Isola [20] by considering a Gaussian distribution rather than a von Mises Fisher distribution. We are implicitly making the assumption that the marginal distributions satisfy \(p(x)=p(x^{-})\). This difference corresponds to our choice of using a negative squared L2 distance in the infoNCE loss rather than an inner product, a difference that will be important later in our analysis. A second difference is that we do not use the resubstitution estimator (i.e., we exclude data point \(x_{i}\) from our estimate of \(\hat{p}_{\text{GMM}}\) when evaluating the likelihood of \(x_{i}\)), which we found hurt performance empirically. The takeaway from this identity is that maximizing the uniformity term corresponds to maximizing (an estimate of) the entropy of the representations.

We next prove that the maximum entropy distribution with an expected L2 norm constraint is a Gaussian distribution. Variants of this result are well known [95, 96, 97], but we include a full proof here for transparency.

**Lemma 4**.: _The maximum entropy distribution satisfying the expected L2 norm constraint in Eq.3 is a multivariate Gaussian distribution with mean \(\mu=0\) and covariance \(\Sigma=c\cdot I\)_

Proof.: We start by defining the corresponding Lagrangian, with the second constraint saying that \(p(x)\) must be a valid probability distribution.

\[\mathcal{L}(p)=\mathcal{H}_{p}[x]+\lambda_{1}\left(\mathbb{E}_{p(x)}\left[\|x \|_{2}^{2}\right]-c\cdot k\right)+\lambda_{2}\left(\int p(x)\,\mathrm{d}x-1\right)\]

We next take the derivative w.r.t. \(p(x)\):

\[\frac{\partial\mathcal{L}}{\partial p(x)}=-p(x)/p(x)-\log p(x)+\lambda_{1}\|x \|_{2}^{2}+\lambda_{2}\]

Setting this derivative equal to 0 and solving for \(p(x)\), we get

\[p(x)=e^{-1+\lambda_{2}+\lambda_{1}\|x\|_{2}^{2}}.\]We next solve for \(\lambda_{1}\) and \(\lambda_{2}\) to satisfy the constraints in the Lagrangian. Note that \(x\sim\mathcal{N}(\mu=0,\Sigma=c\cdot I)\) has an expected norm \(\mathbb{E}[\|x\|_{2}^{2}]=c\cdot k\), so we must have \(\lambda_{1}=-\frac{1}{2c}\). We determine \(\lambda_{1}\) as the normalizing constant for a Gaussian, finally giving us:

\[p(x)=\frac{1}{(2c\pi)^{k/2}}e^{\frac{-1}{2c}\|x\|_{2}^{2}}\]

corresponding to an isotropic Gaussian distribution with mean \(\mu=0\) and covariance \(\Sigma=c\cdot I\). 

### Proof of Lemma 1

Below we present the proof of Lemma 1.

**Lemma 1**.: _Under the assumptions from Section 3, the distribution over representations of future states follows a Gaussian distribution with mean parameter given by the initial state representation:_

\[p(\psi_{t+}=\psi\mid\psi_{0})=\mathcal{N}\Big{(}\mu=\frac{c}{c+1}A\psi_{0}, \Sigma=\frac{c}{c+1}I\Big{)}.\] (6)

Proof.: Our proof technique will be similar to that of the law of the unconscious statistician:

\[p(\psi_{t+}\mid\psi_{0})\stackrel{{\rm(a)}}{{=}} \frac{p(\psi_{t+},\psi_{0})}{{p\!\!\left(\!\varphi\!\!\left(\!\varphi\!\! \right)\!\right)}}\propto\iint p(\psi_{t+},x_{t+},\psi_{0},x_{0})\,\mathrm{d}x _{t+}\,\mathrm{d}x_{0}\] \[\stackrel{{\rm(b)}}{{=}}\iint p(\psi_{t+}\mid x_{t +})p(\psi_{0}\mid x_{0})p(x_{t+}\mid x_{0})p(x_{0})\,\mathrm{d}x_{t+}\, \mathrm{d}x_{0}\] \[\stackrel{{\rm(c)}}{{\propto}}\iint\mathds{1}(\psi( x_{t+})=\psi_{t+})\,\mathds{1}(\psi(x_{0})=\psi_{0})p(x_{t+})e^{-\frac{1}{2}\|A\psi(x_{0})- \psi(x_{t+})\|_{2}^{2}}p(x_{0})\,\mathrm{d}x_{t+}\,\mathrm{d}x_{0}\] \[\stackrel{{\rm(d)}}{{=}}e^{-\frac{1}{2}\|A\psi_{0}- \psi_{t+}\|_{2}^{2}}\iint\mathds{1}(\psi(x_{t+})=\psi_{t+})\,\mathds{1}(\psi(x _{0})=\psi_{0})p(x_{t+})p(x_{0})\,\mathrm{d}x_{t+}\,\mathrm{d}x_{0}\] \[\stackrel{{\rm(e)}}{{=}}e^{-\frac{1}{2}\|A\psi_{0}- \psi_{t+}\|_{2}^{2}}\underbrace{\left(\int p(x_{t+})\,\mathds{1}(\psi(x_{t+}) )\,\mathrm{d}x_{t+}\right)}_{p(\psi_{t+})}\underbrace{\left(\int p(x_{0})\, \mathds{1}(\psi(x_{0})\,\mathrm{d}x_{0}\right)}_{p(\psi_{0})}\] \[\stackrel{{\rm(f)}}{{\propto}}e^{-\frac{1}{2}\|A \psi_{0}-\psi_{t+}\|_{2}^{2}}e^{-\frac{1}{2c}\|\psi_{0}\|_{2}^{2}}\] \[\stackrel{{\rm(g)}}{{\propto}}e^{-\frac{1+\frac{1}{2 }}{2c}\|\frac{1}{1+\frac{1}{c}}A\psi_{0}-\psi_{t+}\|_{2}^{2}}\] \[\propto\mathcal{N}\left(\psi_{t+};\mu=\frac{c}{c+1}A\psi_{0}, \Sigma=\frac{c}{c+1}I\right).\]

In _(a)_ we applied Bayes' Rule and removed the denominator, which is a constant w.r.t. \(\psi_{t+}\). In _(b)_ we factored the joint distribution, noting that \(\psi_{t+}\) and \(\psi_{0}\) are deterministic functions of \(x_{t+}\) and \(x_{0}\) respectively, so they are conditionally independent from the other random variables. In _(c)_ we used Assumption 2 after solving for \(p(x_{t+}\mid x_{0})=p(x_{t+})e^{-\frac{1}{2}\|A\psi(x_{0})-\psi(x)\|_{2}^{2}}\). In _(d)_ we noted that when the integrand is nonzero, it takes on a constant value of \(e^{-\frac{1}{2}\|A\psi_{0}-\psi_{t+}\|_{2}^{2}}\), so we can move that constant outside the integral. In _(e)_ we used the definition of the marginal representation distribution (Eq. 6). In _(f)_ we used Assumption 1 to write the marginal distributions \(p(\psi_{t+})\) and \(p(\psi_{0})\) as Gaussian distributions. We removed the normalizing constants, which are independent of \(\psi_{t+}\). In _(g)_ we completed the square and then recognized the expression as the density of a multivariate Gaussian distribution. 

### Proof of Theorem 2: Waypoint Distribution

**Theorem 2**.: _Under Assumptions 1 and 2, the posterior distribution over waypoint representations is a Gaussian whose mean and covariance are linear functions of the initial and final state representations:_

\[p(\psi_{w}\mid\psi_{0},\psi_{t+})=\mathcal{N}\Big{(}\psi_{w};\mu=\Sigma(A^{T} \psi_{t+}+A\psi_{0}),\Sigma^{-1}=\tfrac{c}{c+1}A^{T}A+\tfrac{c+1}{c}I\Big{)}.\]Proof.: \[p(\psi_{w}\mid\psi_{0},\psi_{t+}) \stackrel{{(a)}}{{=}}\frac{p(\psi_{t+}\mid\psi_{w})p( \psi_{w}\mid\psi_{0})}{p(\psi_{w}\vdash+\overleftarrow{\psi_{0}})}\] \[\stackrel{{(b)}}{{\propto}}e^{-\frac{1+\frac{1}{2}}{ 2}\|\,\varepsilon_{\Gamma}^{c}A\psi_{w}-\psi_{t+}\|_{2}^{2}}e^{-\frac{1+\frac{ 1}{2}}{2}\|\,\varepsilon_{\Gamma}^{c}A\psi_{0}-\psi_{w}\|_{2}^{2}}\] \[\stackrel{{(c)}}{{\propto}}e^{-\frac{1}{2}(\psi_{w} -\mu)^{T}\Sigma^{-1}(\psi_{w}-\mu)}=\mathcal{N}(\psi_{w};\mu,\Sigma)\]

where \(\Sigma^{-1}=\frac{c}{c+1}A^{T}A+\frac{c+1}{c}I\) and \(\mu=\Sigma(A^{T}\psi_{t+}+A\psi_{0})\). 

In line _(a)_ we used the definition of the conditional distribution and then simplified the numerator using the Markov property. Line _(b)_ uses the Lemma 1. Line _(c)_ completes the square, the details of which are below:

\[\frac{1}{2}\cdot\frac{c+1}{c}\bigg{(}\Big{\|}\frac{c}{c+1}A\psi_ {w}-\psi_{t+}\Big{\|}_{2}^{2}+\Big{\|}\frac{c}{c+1}A\psi_{0}-\psi_{w}\Big{\|}_ {2}^{2}\bigg{)}\] \[\qquad=\frac{1}{2}\cdot\frac{c+1}{c}\bigg{(}\psi_{w}^{T}\bigg{(} \frac{c}{c+1}A\bigg{)}^{T}\bigg{(}\frac{c}{c+1}A\bigg{)}\psi_{w}-2\psi_{t+}^{T }\Big{(}\frac{c}{c+1}A\bigg{)}\psi_{w}+\psi_{t+}^{T}\psi_{t+}\] \[\qquad\qquad+\psi_{0}^{T}\bigg{(}\frac{c}{c+1}A\bigg{)}^{T}\bigg{(} \frac{c}{c+1}A\bigg{)}\psi_{0}-2\psi_{0}^{T}\Big{(}\frac{c}{c+1}A\bigg{)}^{T} \psi_{w}+\psi_{w}^{T}\psi_{w}\bigg{)}\] \[\stackrel{{\text{const.}}}{{=}}\frac{1}{2}\cdot \frac{c+1}{c}\Bigg{(}\psi_{w}^{T}\bigg{(}\Big{(}\frac{c}{c+1}\Big{)}^{2}A^{T} A+I\bigg{)}\psi_{w}-2\cdot\frac{c}{c+1}\big{(}A^{T}\psi_{t+}+A\psi_{0} \big{)}^{T}\psi_{w}\Bigg{)}\] \[\qquad=\frac{1}{2}\psi_{w}^{T}\bigg{(}\underbrace{\frac{c}{c+1}A ^{T}A+\frac{c+1}{c}I}_{\Sigma^{-1}}\bigg{)}\psi_{w}-\big{(}A^{T}\psi_{t+}+A \psi_{0}\big{)}^{T}\psi_{w}\] \[\stackrel{{\text{const.}}}{{=}}(\psi_{w}-\mu)^{T} \Sigma^{-1}(\psi_{w}-\mu),\]

where \(\Sigma^{-1}=\frac{c}{c+1}A^{T}A+\frac{c+1}{c}I\) and \(\mu=\Sigma(A^{T}\psi_{t+}+A\psi_{0})\). Above, we have used \(\stackrel{{\text{const.}}}{{=}}\) to denote equality up to an additive constant that is independent of \(\psi_{w}\).

### Proof of Theorem 3: Planning over Many Intermediate States

**Theorem 3**.: _Given observations from a Markov chain \(x_{0}\to x_{1}\cdots x_{t+}\), the joint distribution over representations is a Gaussian distribution. Using \(\psi_{1:n}=(\psi_{w_{1}},\cdots,\psi_{w_{n}})\) to denote the concatenated representations of each observation, we can write this distribution as_

\[p(\psi_{1:n})\propto\exp\bigl{(}-\tfrac{1}{2}\psi_{1:n}^{T}\Sigma^{-1}\psi_{1:n }+\eta^{T}\psi_{1:n}\bigr{)},\]

_where \(\Sigma^{-1}\) is a tridiagonal matrix_

\[\Sigma^{-1}=\begin{pmatrix}\frac{c}{c+1}A^{T}A+\frac{c+1}{c}I&-A^{T}\\ -A&\frac{c}{c+1}A^{T}A+\frac{c+1}{c}I&-A^{T}&\ddots\end{pmatrix}\quad\text{and }\eta= \begin{pmatrix}A\psi_{0}\\ 0\\ \vdots\\ A^{T}\psi_{t+}\end{pmatrix}.\]

Proof.: We start by recalling that the waypoints form a Markov chain, so we can express their joint density as a product of conditional densities:

\[p(\psi_{1:n})=p(\psi_{0})p(\psi_{1}\mid\psi_{0})p(\psi_{2}\mid\psi_{1})\cdots p (\psi_{n}\mid\psi_{n-1}).\]

The aim of this lemma is to express the joint distribution over multiple waypoints, given an initial and final state representation:

\[p(\psi_{1:n}\mid\psi_{0},\psi_{t+})\stackrel{{(a)}}{{=}}\frac{p( \psi_{1:n}\mid\psi_{0})p(\psi_{t+}\mid\psi_{n})}{p(\psi_{t+}\vdash+\overleftarrow {\psi_{0}})}\]\[\begin{split}&\overset{(b)}{\propto}p(\psi_{1}\mid\psi_{0})p(\psi_{2} \mid\psi_{1})\cdots p(\psi_{t+}\mid\psi_{n})\\ &\overset{(c)}{\propto}\exp\Bigl{(}-\tfrac{1}{2}\tfrac{c+1}{c} \|\tfrac{c}{c+1}A\psi_{0}-\psi_{1}\|_{2}^{2}-\tfrac{1}{2}\tfrac{c+1}{c}\| \tfrac{c}{c+1}A\psi_{1}\\ &\qquad\qquad-\psi_{2}\|_{2}^{2}-\cdots-\tfrac{1}{2}\tfrac{c+1}{c +1}\|\tfrac{c}{c+1}A\psi_{n}-\psi_{t+}\|_{2}^{2}\Bigr{)}\\ &\overset{(d)}{=}\exp\Bigl{(}-\tfrac{1}{2}\tfrac{c}{c+1}\psi_{0}^ {T}A^{T}\overline{A}\overline{\psi_{0}}+\psi_{0}^{T}A^{T}\psi_{1}-\tfrac{1}{2} \tfrac{c+1}{c}\psi_{1}^{T}\psi_{1}\\ &\qquad\qquad-\tfrac{1}{2}\tfrac{c}{c+1}\psi_{1}^{T}A^{T}A\psi_{ 1}+\psi_{1}^{T}A^{T}\psi_{2}-\tfrac{1}{2}\tfrac{c+1}{c}\psi_{2}^{T}\psi_{2}\\ &\qquad\qquad-\tfrac{1}{2}\tfrac{c}{c+1}\psi_{2}^{T}A^{T}A\psi_{ 2}+\psi_{2}^{T}A^{T}\psi_{3}-\tfrac{1}{2}\tfrac{c+1}{c}\psi_{3}^{T}\psi_{3}\\ &\qquad\qquad\qquad\qquad\vdots\\ &\qquad\qquad-\tfrac{1}{2}\tfrac{c}{c+1}\psi_{n}^{T}A^{T}A\psi_{ n}+\psi_{n}^{T}A^{T}\psi_{t+}-\tfrac{1}{2}\tfrac{c+1}{c}\psi_{t+}^{T}\overline{ \psi_{t+}}\end{split}\]

\[=\exp\bigl{(}-\tfrac{1}{2}\psi_{1:n}^{T}\Sigma^{-1}\psi_{1:n}+\eta^{T}\psi_{1: n}\bigr{)},\]

where

\[\Sigma^{-1}=\begin{pmatrix}\tfrac{c}{c+1}A^{T}A+\tfrac{c+1}{c}I&-A^{T}&\\ -A&\tfrac{c}{c+1}A^{T}A+\tfrac{c+1}{c}I&-A^{T}&\\ &-A&\tfrac{c}{c+1}A^{T}A+\tfrac{c+1}{c}I&\\ &&&\ddots\end{pmatrix}\text{ and }\eta=\begin{pmatrix}A\psi_{0}\\ 0\\ \vdots\\ A^{T}\psi_{t+}\end{pmatrix}.\]

In _(a)_ we applied Bayes' rule and removed the denominator because it is a constant with respect to \(\psi_{1:n}\). In _(b)_ we applied the Markov assumption. In _(c)_ we used Lemma1 to express the conditional probabilities as Gaussians, ignoring the proportionality constants (which are independent of \(\psi\). In _(d)_ we simplified the exponents, removing terms that do not depend on \(\psi_{1:n}\). 

### Formalizing Assumption2

Assumption2 relates the learned contrastive critic to a log-likelihood ratio between the positive and negative data distribution.

**Assumption 2**.: _Applying contrastive learning to the symmetrized infoNCE objective results in representations that encode a probability ratio:_

\[e^{-\frac{1}{2}\|\phi(x_{0})-\psi(x)\|_{2}^{2}}=\frac{p_{t+}(x_{t+}=x\mid x_{0 })}{p(x)C}.\] (5)

We can justify this assumption by analyzing the general solution to the symmetrized version of the Oord et al. [10] infoNCE objective, which we do in Lemma5. Applying this lemma to our representation learning objective (2) for sufficiently large batch size \(B\) then yields Eq.5, with the function approximator \(\|\phi(x)-\psi(x^{+})\|^{2}\approx f(x,x^{+})\).

**Lemma 5**.: _The solution to the optimization problem_

\[\max_{f(x,x^{+})}\lim_{B\to\infty}\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B} \sim p(x,x^{+})}\bigg{[}\tfrac{1}{B}\sum_{i=1}^{B}\log\frac{e^{f(x_{i},x_{i}^ {+})}}{\sum_{j\neq i}e^{f(x_{i},x_{j}^{+})}}+\log\frac{e^{f(x_{i},x_{i}^{+})} }{\sum_{j\neq i}e^{f(x_{j},x_{i}^{+})}}\bigg{]}\] (8)

_satisfies_

\[f(x,x^{+})=\log\biggl{(}\frac{p(x^{+}\mid x)}{p(x^{+})C}\biggr{)}\] (9)

_for some \(C\)._

Proof of Lemma5.: We first break down the LHS and RHS of Eq.2:

\[\max_{f}\lim_{B\to\infty}\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B}\sim p(x, x^{+})}\Bigg{[}\frac{1}{B}\sum_{i=1}^{B}\underbrace{\frac{\log e^{f(x_{i},x_{i}^{+})} }{\sum_{j\neq i}e^{f(x_{i},x_{j}^{+})}}}_{\mathcal{I}_{1}}+\underbrace{\log \frac{e^{f(x_{i},x_{i}^{+})}}{\sum_{j\neq i}e^{f(x_{j},x_{i}^{+})}}}_{ \mathcal{J}_{2}}\Bigg{]}\]\[\mathcal{J}_{1}(f) =\lim_{B\to\infty}\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B}\sim p( x,x^{+})}\bigg{[}\frac{1}{B}\sum_{i=1}^{B}\log\frac{e^{f(x_{i},x_{i}^{+})}}{\sum_{j \neq i}e^{f(x_{i},x_{j}^{+})}}\bigg{]}\] \[\mathcal{J}_{2}(f) =\lim_{B\to\infty}\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B}\sim p (x,x^{+})}\bigg{[}\frac{1}{B}\sum_{i=1}^{B}\log\frac{e^{f(x_{i},x_{i}^{+})}}{ \sum_{j\neq i}e^{f(x_{j},x_{i}^{+})}}\bigg{]}\]

We now use the following result from Ma and Collins [98]:

**Lemma 6**.: _The optimal solutions \(f_{1}\) and \(f_{2}\) for \(\mathcal{J}_{1}\) and \(\mathcal{J}_{2}\) satisfy_

\[f_{1}(x,x^{+}) =\log p(x\mid x^{+})-\log c_{1}(x)\] (10) \[f_{2}(x,x^{+}) =\log p(x^{+}\mid x)-\log c_{2}(x^{+})\] (11)

_for arbitrary \(c_{1}(x),c_{2}(x^{+})\)._

For any \(C\), when \(c_{1}(x)=Cp(x)\) and \(c_{2}(x^{+})=Cp(x^{+})\),

\[f_{1}(x,x^{+})=\log\biggl{(}\frac{p(x\mid x^{+})}{p(x)C}\biggr{)}=\log\biggl{(} \frac{p(x^{+}\mid x)}{p(x^{+})C}\biggr{)}=f_{2}(x,x^{+}).\] (12)

It follows that 12 maximizes both \(\mathcal{J}_{1}\) and \(\mathcal{J}_{2}\), and is precisely the optimal solution 9 for 8. 

What does \(C\) represent?From 9, we can connect \(C\) to the mutual information \(I(x,x^{+})\):

\[C=\frac{\mathbb{E}_{(x,x^{+})\sim p(x,x^{+})}\bigl{[}f(x,x^{+})\bigr{]}}{I(x, x^{+})}.\] (13)

Proof of Lemma 6.: We can first consider \(\mathcal{J}_{1}\) without loss of generality. Denoting

\[g(x,x^{+})=e^{f(x,x^{+})},\]

we take the functional derivative:

\[\delta\mathcal{J}_{1}(\log g) =\lim_{B\to\infty}\delta\,\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1} ^{B}\sim p(x,x^{+})}\bigg{[}\frac{1}{B}\sum_{i=1}^{B}\log\frac{g(x_{i},x_{i}^ {+})}{\sum_{j\neq i}g(x_{i},x_{j}^{+})}\bigg{]}\] \[=\lim_{B\to\infty}\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B}\sim p (x,x^{+})}\bigg{[}\frac{1}{B}\sum_{i=1}^{B}\frac{(\sum_{j\neq i}g(x_{i},x_{j}^ {+}))\delta g(x_{i},x_{i}^{+})-g(x_{i},x_{i}^{+})\delta(\sum_{j\neq i}g(x_{i},x _{j}^{+}))}{g(x_{i},x_{i}^{+})(\sum_{j\neq i}g(x_{i},x_{j}^{+}))}\bigg{]}\] \[=\lim_{B\to\infty}\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B}\sim p (x,x^{+})}\bigg{[}\frac{1}{B}\sum_{i=1}^{B}\int\Bigl{(}\big{(}\frac{\delta g( x_{i},x^{+})}{g(x_{i},x^{+})}\big{)}p(x^{+}\mid x_{i})\] \[\qquad\qquad\qquad\qquad\qquad-\sum_{k\neq i}\bigl{(}\frac{\delta g (x_{i},x^{+})}{g(x_{i},x^{+})-g(x_{i},x_{k}^{+})+\sum_{j\neq i}g(x_{i},x_{j}^{+ })}\bigr{)}p(x^{+})\Bigr{)}\,\mathrm{d}x^{+}\bigg{]}\] \[=\lim_{B\to\infty}\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B}\sim p (x,x^{+})}\bigg{[}\frac{1}{B}\sum_{i=1}^{B}\int\delta g(x_{i},x^{+})\Bigl{(} \frac{p(x^{+}\mid x_{i})}{g(x_{i},x^{+})}\] \[\qquad\qquad\qquad\qquad\qquad\qquad-\underbrace{\mathbb{E}_{\{(x_ {i},x_{i}^{+})\}_{i=1}^{B}}\Bigl{[}\frac{1}{\sum_{j\neq i}g(x_{i},x_{j}^{+})} \Bigr{]}}_{\text{as }B\to\infty}p(x^{+})\Bigr{)}\,\mathrm{d}x^{+}\bigg{]}\]\[=\lim_{B\to\infty}\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B}\sim p (x,x^{+})}\bigg{[}\tfrac{1}{B}\sum_{i=1}^{B}\int\delta g(x_{i},x^{+})\Big{(} \tfrac{p(x^{+}|x_{i})}{g(x_{i},x^{+})}\] \[-\underbrace{\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B}}\Big{[} \tfrac{1}{\sum_{j\neq i}g(x_{i},x_{j}^{+})}\Big{]}}_{\triangleq k(x_{i})\text{ indep. of }x^{+}}\] \[=\lim_{B\to\infty}\mathbb{E}_{\{(x_{i},x_{i}^{+})\}_{i=1}^{B}\sim p (x,x^{+})}\bigg{[}\tfrac{1}{B}\sum_{i=1}^{B}\int\delta g(x_{i},x^{+})\Big{(} \tfrac{p(x^{+}|x_{i})}{g(x_{i},x^{+})}-k(x_{i})p(x^{+})\Big{)}\,\mathrm{d}x^{+} \bigg{]}\] \[=\int\delta g(x,x^{+})\big{(}\tfrac{p(x^{+}|x)}{g(x,x^{+})}-k(x)p (x^{+})\big{)}\,\mathrm{d}x^{+}.\]

This is zero when

\[g(x,x^{+})=\frac{p(x\mid x^{+})}{k(x)p(x)},\]

i.e.,

\[f(x,x^{+})=\log p(x\mid x^{+})-\log\underbrace{c_{1}(x)}_{k(x)p(x)}\]

as in Eq.10, and Eq.11 follows similarly, exchanging \(x\) and \(x^{+}\). 

## Appendix B Additional Experiments

Fig. 7 visualizes the inferred waypoints from the task in Fig. 5. Fig. 8 visualizes the representations learned on a 46-dimensional robotic hammering task (see Section5.3).

Figure 8: Planning for 46-dimensional robotic hammering. _(Left)_ A dataset of trajectories demonstrating a hammer knocking a nail into a board [102]. _(Center)_ We visualize the learned representations as blue circles, with the transparency indicating the index of that observation along the trajectory. We also visualize the inferred plan (Section4.3) as red circles connected by arrows. _(Right)_ Representations learned by PCA on the same trajectory as _(a, left)_.

Figure 7: Our approach enables a goal-conditioned policy to reach farther targets (red) from the start (green) by planning over intermediate waypoints (orange).

### Stock Prediction

We show results on a stock opening price task in Fig. 9.

Figure 9: **Stock Prediction.** We apply temporal contrastive learning to time series data of the stock market. Data are the opening prices for the 500 stocks in the S&P 500, over a four year window. We remove 30 stocks that are missing data. For evaluation, we choose a 100 day window from a validation set, and use Theorem 2 to perform “inpainting,” predicting the intermediate stock prices _jointly_ for all stocks (orange), given the first and last stock price. The true stock prices are shown in blue. While we do not claim that this is a state-of-the-art model for stock prediction, this experiment demonstrates another potential application of our theoretical results.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims in the abstract and introduction are supposed by the Lemmas in the main text. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See "Limitations" in Sec. 6, and discussion following the Assumptions in the main text. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Proofs for each Lemma are provided in either the main text or the appendices. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All results and figures in paper can be reproduced by running make in the linked code. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code has been released. The datasets used have been released by prior work. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Details are in the appendix and/or the supplemental code. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Sec. 5. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Sec. 5 Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We are unaware of any violations. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper provides theoretical analysis of representations learned by contrastive learning. This analysis suggests new ways of using these representations, for both beneficial and harmful purposes. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: N/A Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See citations to the D4RL dataset. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [No] Justification: N/A Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: N/A Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: N/A Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.