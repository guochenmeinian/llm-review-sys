# Differentially Private Reinforcement Learning with Self-Play

 Dan Qiao

Department of Computer Science \(\&\) Engineering

University of California, San Diego

San Diego, CA 92093

d2qiao@ucsd.edu

&Yu-Xiang Wang

Halciooglu Data Science Institute

University of California, San Diego

San Diego, CA 92093

yuxiangw@ucsd.edu

###### Abstract

We study the problem of multi-agent reinforcement learning (multi-agent RL) with differential privacy (DP) constraints. This is well-motivated by various real-world applications involving sensitive data, where it is critical to protect users' private information. We first extend the definitions of Joint DP (JDP) and Local DP (LDP) to two-player zero-sum episodic Markov Games, where both definitions ensure trajectory-wise privacy protection. Then we design a provably efficient algorithm based on optimistic Nash value iteration and privatization of Bernstein-type bonuses. The algorithm is able to satisfy JDP and LDP requirements when instantiated with appropriate privacy mechanisms. Furthermore, for both notions of DP, our regret bound generalizes the best known result under the single-agent RL case, while our regret could also reduce to the best known result for multi-agent RL without privacy constraints. To the best of our knowledge, these are the first results towards understanding trajectory-wise privacy protection in multi-agent RL.

## 1 Introduction

This paper considers the problem of multi-agent reinforcement learning (multi-agent RL), wherein several agents simultaneously make decisions in an unfamiliar environment with the goal of maximizing their individual cumulative rewards. Multi-agent RL has been deployed not only in large-scale strategy games like Go (Silver et al., 2017), Poker (Brown and Sandholm, 2019) and MOBA games (Ye et al., 2020), but also in various real-world applications such as autonomous driving (Shalev-Shwartz et al., 2016), negotiation (Bachrach et al., 2020), and trading in financial markets (Shavandi and Khedmati, 2022). In these applications, the learning agent analyzes users' private feedback in order to refine its performance, where the data from users usually contain sensitive information. Take autonomous driving as an instance, here a trajectory describes the interaction between the cars in a neighborhood during a fixed time window. At each timestamp, given the current situation of each car, the system (central agent) will send a command for each car to take (_e.g._ speed up, pull over), and finally the system gathers the feedback from each car (_e.g._ whether the driving is safe, whether the customer feels comfortable) and enhances its policy. Here, (situation, command, feedback) corresponds to (state, action, reward) in a Markov Game where the state and reward of each user are considered as sensitive information. Therefore, leakage of such information is not acceptable. Regrettably, it has been demonstrated that without the implementation of privacy safeguards, learning agents tend to maladvertently memorize details from individual training data points (Carlini et al., 2019), regardless of their relevance to the learning process (Brown et al., 2021). This susceptibility exposes multi-agent RL agents to potential privacy threats.

To handle the above privacy issue, Differential privacy (DP) (Dwork et al., 2006) has been widely considered. The output of a differentially private reinforcement learning algorithm cannot be discernedfrom its output in an alternative reality where any specific user is substituted, which effectively mitigates the privacy risks mentioned earlier. However, it is shown [Shariff and Sheffet, 2018] that standard DP will lead to linear regret even under contextual bandits. Therefore, Vietri et al. [2020] considered a relaxed surrogate of DP: _Joint Differential Privacy_ (JDP) [Kearns et al., 2014] for RL. Briefly speaking, JDP protects the information about any specific user even given the output of all other users. Meanwhile, another variant of DP: _Local Differential Privacy_ (LDP) [Duchi et al., 2013] has also been extended to RL by Garcelon et al. [2021] due to its stronger privacy protection. LDP requires that the raw data of each user is privatized before being sent to the agent. Although following works [Chowdhury and Zhou, 2022, Qiao and Wang, 2023] established near optimal results under these two notions of DP, all of the previous works focused on the single-agent RL setting while the solution to multi-agent RL with differential privacy is still unknown. Therefore we question:

**Question 1.1**.: _Is it possible to design a provably efficient self-play algorithm to solve Markov games while satisfying the constraints of differential privacy?_

**Our contributions.** In this paper, we answer the above question affirmatively by proposing a general algorithm for DP multi-agent RL: DP-Nash-VI (Algorithm 1). Our contributions are threefold.

* We first extend the definitions of Joint DP (Definition 2.2) and Local DP (Definition 2.3) to the multi-agent RL setting. Both notions of DP focus on protecting the sensitive information of each trajectory, which is consistent with the counterparts under single-agent RL.
* We design a new algorithm DP-Nash-VI (Algorithm 1) based on optimistic Nash value iteration and privatization of Bernstein-type bonuses. The algorithm can be combined with any Privatizer (for JDP or LDP) that possesses a corresponding regret bound (Theorem 4.1). Moreover, when there is no privacy constraint (_i.e._ the privacy budget is infinity), our regret reduces to the best known regret for non-private multi-agent RL.
* Under the constraint of \(\epsilon\)-JDP, DP-Nash-VI achieves a regret of \(\widetilde{O}(\sqrt{H^{2}SABT}+H^{3}S^{2}AB/\epsilon)\) (Theorem 5.2). Compared to the regret lower bound (Theorem 5.3), the main term is nearly optimal while the additional cost due to JDP has optimal dependence on \(\epsilon\). Under the \(\epsilon\)-LDP constraint, DP-Nash-VI achieves a regret of \(\widetilde{O}(\sqrt{H^{2}SABT}+S^{2}AB\sqrt{H^{5}T}/\epsilon)\) (Theorem 5.5), where the dependence on \(K,\epsilon\) is optimal according to the lower bound (Theorem 5.6). The pair of results strictly generalizes the best known results for single-agent RL with DP [Qiao and Wang, 2023].

### Related work

We compare our results with existing works on differentially private reinforcement learning [Vietri et al., 2020, Garcelon et al., 2021, Chowdhury and Zhou, 2022, Qiao and Wang, 2023] and regret minimization under Markov Games [Liu et al., 2021] in Table 1, while more discussions about differentially private learning algorithms are deferred to Appendix A. Notably, all existing DP RL

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Algorithms for Markov Games & Regret without privacy & Regret after \(\epsilon\)-JDP & Regret under \(\epsilon\)-JDP \\ \hline DP-Nash-VI (Algorithm 1) & \(\widetilde{O}(\sqrt{H^{2}SABT}+T^{2}SABT)\) & \(\widetilde{O}(\sqrt{H^{2}SABT}+S^{2}AB/\epsilon)\) & \(\widetilde{O}(\sqrt{H^{2}SABT}+S^{2}AB/\epsilon)\) (\(\sqrt{H^{2}SABT}+S^{2}AB/\epsilon)\) (\(\sqrt{H^{2}SABT}+S^{2}AB/\epsilon)\) \\ \hline Nash VI (Liu et al., 2021) & \(\widetilde{O}(\sqrt{H^{2}SABT})\): & NSA & NSA \\ \hline Lower bounds & \(\ln(\sqrt{H^{2}SA}(A+BT)\) (Bai and Jin, 2020) & \(\widetilde{\ln(\sqrt{H^{2}SA}(A+BT)+(\frac{\epsilon^{2}(A+BT)+(\frac{\epsilon^{2 }(A+BT)}{\epsilon^{2}(A+BT)}}))}\) & \(\widetilde{\ln(\sqrt{H^{2}SA}(A+BT)+(\frac{\epsilon^{2}(A+BT)}{\epsilon^{2}(A+ BT)})}\) \\ \hline Algorithms for MDPs (\(1-1\)) & Regret without privacy & Regret after \(\epsilon\)-JDP & Regret after \(\epsilon\)-JDP \\ \hline PCQ [Vier et al., 2020] & \(\widetilde{O}(\sqrt{H^{2}SA}AT)\) & \(\widetilde{O}(\sqrt{H^{2}SA}AT+\sqrt{H^{2}SA}A/\epsilon)^{2}\) & NSA \\ \hline LDP-QVI [Garcelon et al., 2021] & \(\widetilde{O}(\sqrt{H^{2}SA}AT)\) & NSA & \(\widetilde{O}(\sqrt{H^{2}SA}AT+S^{2}AT/\epsilon)^{2}\) \\ Private-UCBVI [Garcelon et al., 2021] & \(\widetilde{O}(\sqrt{H^{2}SA}AT)\) & \(\widetilde{O}(\sqrt{H^{2}SA}AT+S^{2}BA/\epsilon)\) & \(\widetilde{O}(\sqrt{H^{2}SA}AT+S^{2}AA/\epsilon)\) (\(\widetilde{O}(\sqrt{H^{2}SA}AT+S^{2}AA/\epsilon)\) \\ DP-UCWVI [Garcelon et al., 2022] & \(\widetilde{O}(\sqrt{H^{2}SA}AT)\) & \(\widetilde{O}(\sqrt{H^{2}SA}AT+S^{2}BA/\epsilon)\) & \(\widetilde{O}(\sqrt{H^{2}SA}AT+S^{2}AA/\epsilon)\) \\ \hline DP-UCWVI [Garcelon et al., 2021] & \(\widetilde{O}(\sqrt{H^{2}SA}AT)\) & \(\widetilde{O}(\sqrt{H^{2}SA}AT+S^{2}BA/\epsilon)\) & \(\widetilde{O}(\sqrt{H^{2}SA}AT+S^{2}AA/\epsilon)\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of our results (in blue) to existing work regarding regret without privacy (_i.e._ the privacy budget is infinity), regret under \(\epsilon\)-Joint DP and regret under \(\epsilon\)-Local DP. In the above, \(S\) is the number of states, \(A,B\) are the number of actions for both players, \(H\) is the planning horizon and \(K\) is the number of episodes (\(T=HK\) is the number of steps). Markov decision processes (MDPs) is a special case of Markov Games where \(B=1\). \(*\): This result is the best known regret bound when there is no privacy concern. \(\star\): More discussions about this bound can be found in Chowdhury and Zhou [2022]. \(\dagger\): The original regret bound in Garcelon et al. [2021] is derived under the setting of stationary MDP, and can be directly transferred to the bound here by adding \(\sqrt{H}\) to the first term. \(\ddagger\): This algorithm achieved the best known results under single-agent MDPs, and our Algorithm 1 can obtain the same regret bounds under this setting.

algorithms focus on the single-agent case. In comparison, our algorithm works for the more general two-player setting and our results directly match the best known regret bounds (Qiao and Wang, 2023) when applied to the single-agent setting.

Recently, several works provide non-asymptotic theoretical guarantees for learning Markov Games. Bai and Jin (2020) developed the first provably-efficient algorithms in MGs based on optimistic value iteration, and the result is improved by Liu et al. (2021) using model-based approach. Meanwhile, model-free approaches are shown to break the curse of multiagency and improve the dependence on action space (Bai et al., 2020; Jin et al., 2021; Mao et al., 2022; Wang et al., 2023; Cui et al., 2023). However, all these algorithms base on the original data from users, and thus are vulnerable to various privacy attacks. While several works (Hossain and Lee, 2023; Hossain et al., 2023; Zhao et al., 2023; Gohari et al., 2023) study the privatization of communications between multiple agents, none of them provide regret guarantees. In comparison, we design algorithms that provably protect the sensitive information in each trajectory, while achieving near-optimal regret bounds simultaneously.

Technically speaking, we follow the idea of optimistic Nash value iteration and privatization of Bernstein-type bonuses. Optimistic Nash value iteration aims to construct both upper bounds and lower bounds for value functions, which could guide the exploration. Such idea has been applied by previous model-based approaches (Bai and Jin, 2020; Liu et al., 2021) to derive tight regret bounds. To satisfy the privacy guarantees, we are required to construct the UCB and LCB privately. In this work, we privatize the transition kernel estimate and construct a private bonus function for our purpose. Among different bonuses, we generalize the approach in Qiao and Wang (2023) and directly operate on the Bernstein-type bonus, which could enable tight regret analysis while the privatization is more technically demanding due to the variance term. To handle this, we first privatize the visitation counts such that they satisfy several nice properties, then we use these counts to construct private transition estimates and private bonuses. Lastly, we manage to prove UCB and LCB, and bound the private terms by their non-private counterparts to complete the regret analysis.

## 2 Problem Setup

We consider reinforcement learning under Markov Games (MGs) (Shapley, 1953) with Differential Privacy (DP) (Dwork et al., 2006). Below we introduce MGs and define DP under multi-agent RL.

### Markov Games and Regret

Markov Games (MGs) are the generalization of Markov Decision Processes (MDPs) to the multi-player setting, where each player aims to maximize her own reward. We consider _two-player zero-sum_ episodic MGs, denoted by a tuple \(\mathcal{MG}=(\mathcal{S},\mathcal{A},\mathcal{B},H,\{P_{h}\}_{h=1}^{H},\{r_{ h}\}_{h=1}^{H},s_{1})\), where \(\mathcal{S}\) is the state space with \(S=|\mathcal{S}|\), \(\mathcal{A}\) and \(\mathcal{B}\) are the action space for the max-player (who aims to maximize the total reward) and the min-player (who aims to minimize the total reward) respectively with \(A=|\mathcal{A}|,B=|\mathcal{B}|\). Besides, \(H\) is the horizon while the non-stationary transition kernel \(P_{h}(\cdot|s,a,b)\) gives the distribution of the next state if action \((a,b)\) is taken at state \(s\) and time step \(h\). In addition, we assume that the reward function \(r_{h}(s,a,b)\in[0,1]\) is deterministic and known1. For simplicity, we assume each episode starts from a fixed initial state \(s_{1}\). Then at each time step \(h\in[H]\), two players observe \(s_{h}\) and choose their actions \(a_{h}\in\mathcal{A}\) and \(b_{h}\in\mathcal{B}\) simultaneously, after which both players observe the action of their opponent and receive reward \(r_{h}(s_{h},a_{h},b_{h})\), the environment will transit to \(s_{h+1}\sim P_{h}(\cdot|s_{h},a_{h},b_{h})\).

Footnote 1: This assumption is wlog since the uncertainty of reward is dominated by that of transition kernel.

**Markov policy, value function.** A Markov policy \(\mu\) of the max-player can be seen as a series of mappings \(\mu=\{\mu_{h}\}_{h=1}^{H}\), where each \(\mu_{h}\) maps each state \(s\in\mathcal{S}\) to a probability distribution over actions \(\mathcal{A}\), _i.e._\(\mu_{h}:\mathcal{S}\rightarrow\Delta(\mathcal{A})\). A Markov policy \(\nu\) for the min-player is defined similarly. Given a pair of policies \((\mu,\nu)\) and time step \(h\in[H]\), the value function \(V_{h}^{\mu,\nu}(\cdot)\) is defined as \(V_{h}^{\mu,\nu}(s)=\mathbb{E}_{\mu,\nu}[\sum_{t=h}^{H}r_{t}|s_{h}=s]\) while the Q-value function \(Q_{h}^{\mu,\nu}(\cdot,\cdot,\cdot)\) is defined as \(Q_{h}^{\mu,\nu}(s,a,b)=\mathbb{E}_{\mu,\nu}[\sum_{t=h}^{H}r_{t}|s_{h},a_{h},b _{h}=s,a,b]\) for all \(s,a,b\). According to the definitions, the following Bellman equation holds:

\[Q_{h}^{\mu,\nu}(s,a,b)=[r_{h}+P_{h}V_{h+1}^{\mu,\nu}](s,a,b),\;\;V_{h}^{\mu, \nu}(s)=[\mathbb{E}_{\mu,\nu}Q_{h}^{\mu,\nu}](s),\;\;\forall\;(h,s,a,b).\]

**Best responses, Nash equilibrium.** For any policy \(\mu\) of the max-player, there exists a best response policy \(\nu^{\dagger}(\mu)\) of the min-player such that \(V_{h}^{\mu,\nu^{\dagger}(\mu)}(s)=\inf_{\nu}V_{h}^{\mu,\nu}(s)\) for all \((s,h)\). For simplicity, we denote \(V_{h}^{\mu,\dagger}:=V_{h}^{\mu,\nu^{\dagger}(\mu)}\). Also, \(\mu^{\dagger}(\nu)\) and \(V_{h}^{\dagger,\nu}\) can be defined by symmetry. It is shown [15] that there exists a pair of policies \((\mu^{\star},\nu^{\star})\) that are best responses against each other, _i.e._, \(V_{h}^{\mu^{\star},\dagger}(s)=V_{h}^{\mu^{\star},\nu^{\star}}(s)=V_{h}^{\dagger,\nu^{\star}}(s),\ \forall\ (s,h)\in\mathcal{S}\times[H]\). The pair of policies \((\mu^{\star},\nu^{\star})\) is called the Nash equilibrium of the Markov game, which further satisfies the following minimax property: for all \((s,h)\in\mathcal{S}\times[H],\ \sup_{\mu}\inf_{\nu}V_{h}^{\mu,\nu}(s)=V_{h}^{\mu^{ \star},\nu^{\star}}(s)=\inf_{\nu}\sup_{\mu}V_{h}^{\mu,\nu}(s)\). The value functions of \((\mu^{\star},\nu^{\star})\) are called Nash value functions and we denote \(V_{h}^{\star}=V_{h}^{\mu^{\star},\nu^{\star}},Q_{h}^{\star}=Q_{h}^{\mu^{\star},\nu^{\star}}\) for simplicity. Nash equilibrium means that no player could gain more from updating her own policy.

**Learning objective: regret.** Following previous works [16, 15], we aim to minimize the regret, which is defined as below:

\[\text{Regret}(K)=\sum_{k=1}^{K}\left[V_{1}^{1,\nu^{k}}(s_{1})-V_{1}^{\mu^{k}, \dagger}(s_{1})\right],\]

where \(K\) is the number of episodes the agent interacts with the environment and \((\mu^{k},\nu^{k})\) are the policies executed by the agent in the \(k\)-th episode. Note that any sub-linear regret bound can be transferred to a PAC guarantee according to the standard online-to-batch conversion [15].

### Differential Privacy in Multi-agent RL

For RL with self-play, each trajectory corresponds to the interaction between a pair of users and the environment. The interaction generally follows the protocol below. At time step \(h\) of the \(k\)-th episode, the users send their state \(s_{h}^{k}\) to a central agent \(\mathcal{M}\), then \(\mathcal{M}\) sends back a pair of actions \((a_{h}^{k},b_{h}^{k})\) for the users to take, and finally the users send their reward \(r_{h}^{k}\) to \(\mathcal{M}\). Following previous works [21, 14], here we let \(\mathcal{U}=(u_{1},\cdots,u_{K})\) denote the sequence of \(K\) unique 2 pairs of users who participate in the above RL protocol. Besides, each pair of users \(u_{k}\) is characterized by the \(\{s_{h}^{k},r_{h}^{k}\}_{h=1}^{H}\) information they would respond to all \((AB)^{H3}\) possible sequences of actions from the agent. Let \(\mathcal{M}(\mathcal{U})=\{(a_{h}^{k},b_{h}^{k})\}_{h,k=1,1}^{H,K}\) denote the whole sequence of actions suggested by the agent \(\mathcal{M}\). Then a direct adaptation of differential privacy [14] is defined below, which says that \(\mathcal{M}(\mathcal{U})\) and all other pairs excluding \(u_{k}\) together will not disclose much information about user \(u_{k}\).

Footnote 2: Uniqueness is assumed wlog, as for a returning user pair one can group them with their previous occurrences.

**Definition 2.1** (Differential Privacy (DP)).: _For any \(\epsilon>0\) and \(\delta\in[0,1]\), a mechanism \(\mathcal{M}:\mathcal{U}\rightarrow(\mathcal{A}\times\mathcal{B})^{KH}\) is \((\epsilon,\delta)\)-differentially private if for any possible user sequences \(\mathcal{U}\) and \(\mathcal{U}^{\prime}\) that is different on one pair of users and any subset \(E\) of \((\mathcal{A}\times\mathcal{B})^{KH}\),_

\[\mathbb{P}[\mathcal{M}(\mathcal{U})\in E]\leq e^{\epsilon}\cdot\mathbb{P}[ \mathcal{M}(\mathcal{U}^{\prime})\in E]+\delta.\]

_If \(\delta=0\), we say that \(\mathcal{M}\) is \(\epsilon\)-differentially private (\(\epsilon\)-DP)._

Unfortunately, privately recommending actions to the pair of users \(u_{k}\), while protecting their own state and reward information is shown to be impractical even for the single-player setting. Therefore, we consider a relaxed version of DP, known as _Joint Differential Privacy_ (JDP) [11]. JDP says that for all pairs of users \(u_{k}\), the recommendation to all other pairs excluding \(u_{k}\) will not disclose the sensitive information about \(u_{k}\). Although being weaker than DP, JDP could still provide meaningful privacy protection by ensuring that even if an adversary can observe the interactions between all other users and the environment, it is statistically hard to reconstruct the interaction between \(u_{k}\) and the environment. JDP is first studied by Vietri et al. [16] under single-agent reinforcement learning, and we extend the definition to the two-player setting.

**Definition 2.2** (Joint Differential Privacy (JDP)).: _For any \(\epsilon>0\), a mechanism \(\mathcal{M}:\mathcal{U}\rightarrow(\mathcal{A}\times\mathcal{B})^{KH}\) is \(\epsilon\)-joint differentially private if for any \(k\in[K]\), any user sequences \(\mathcal{U}\) and \(\mathcal{U}^{\prime}\) that is different on the \(k\)-th pair of users and any subset \(E\) of \((\mathcal{A}\times\mathcal{B})^{(K-1)H}\),_

\[\mathbb{P}[\mathcal{M}_{-k}(\mathcal{U})\in E]\leq e^{\epsilon}\cdot\mathbb{P}[ \mathcal{M}_{-k}(\mathcal{U}^{\prime})\in E],\]_where \(\mathcal{M}_{-k}(\mathcal{U})\in E\) means the sequence of actions sent to all pairs of users excluding \(u_{k}\) belongs to set \(E\)._

In the example of autonomous driving, JDP ensures that even if an adversary observes the interactions between cars within all time windows except one, it is hard to know what happens during the specific time window. While providing strong privacy protection, JDP requires the central agent \(\mathcal{M}\) to have access to the real trajectories from users. However, in various scenarios the users are not even willing to directly share their data with the agent. To address such circumstances, Duchi et al. (2013) developed a stronger notion of privacy named _Local Differential Privacy_ (LDP). Now that when considering LDP, the agent can not observe the state of users, we consider the following protocol specific for LDP: at the beginning of the \(k\)-th episode, the agent \(\mathcal{M}\) first sends a policy pair \(\pi_{k}=(\mu_{k},\nu_{k})\) to the pair of users \(u_{k}\), after running \(\pi_{k}\) and getting a trajectory \(X_{k}\), \(u_{k}\) privatizes their trajectory to \(X^{\prime}_{k}\) and sends it back to \(\mathcal{M}\). We present the definition of Local DP below, which generalizes the LDP under single-agent reinforcement learning by Garcelon et al. (2021). Briefly speaking, Local DP ensures that it is impractical for an adversary to reconstruct the whole trajectory of \(u_{k}\) even if observing their whole response.

**Definition 2.3** (Local Differential Privacy (LDP)).: _For any \(\epsilon>0\), a mechanism \(\widetilde{\mathcal{M}}\) is \(\epsilon\)-local differentially private if for any possible trajectories \(X,X^{\prime}\) and any possible set \(E\subseteq\{\widetilde{\mathcal{M}}(X)|X\text{ is any possible trajectory}\}\),_

\[\mathbb{P}[\widetilde{\mathcal{M}}(X)\in E]\leq e^{\epsilon}\cdot\mathbb{P}[ \widetilde{\mathcal{M}}(X^{\prime})\in E].\]

In the example of autonomous driving, LDP ensures that the system can only observe a private version of the interactions between cars instead of the raw data.

**Remark 2.4**.: _Note that here our definitions of JDP and LDP both provide trajectory-wise privacy protection, which is consistent with previous works (Chowdhury and Zhou, 2022, Qiao and Wang, 2023). Moreover, under the special case where the min-player plays a fixed and known deterministic policy (or equivalently, \(\mathcal{B}\) only contains a single action and \(B=1\)), the Markov Game setting reduces to a single-agent Markov decision process while our JDP and LDP directly matches previous definitions for the MDP setting. Therefore, our setting strictly generalizes previous works and requires novel techniques to handle the min-player._

**Remark 2.5**.: _In the following sections we will show that LDP is consistent with sub-linear regret bounds, while it is known that we can not derive sub-linear regret bounds under the constraint of DP. We remark that there is no contradictory since here the RL protocols for DP and LDP are different. As a result, here a guarantee of LDP does not directly imply a guarantee of DP and the two notions are indeed not directly comparable._

## 3 Algorithm

In this part, we introduce DP-Nash-VI (Algorithm 1). Note that the algorithm takes Privatizer as an input. We analyze the regret of Algorithm 1 for all Privatizers satisfying the Assumption 3.1 below, which includes the cases where the Privatizer is chosen as Central (for JDP) or Local (for LDP).

We first introduce the definition of visitation counts, where \(N^{k}_{h}(s,a,b)=\sum_{i=1}^{k-1}\mathds{1}(s^{i}_{h},a^{i}_{h},b^{i}_{h}=s,a,b)\) denotes the visitation count of \((s,a,b)\) at time step \(h\) until the beginning of the \(k\)-th episode. Similarly, we let \(N^{k}_{h}(s,a,b,s^{\prime})=\sum_{i=1}^{k-1}\mathds{1}(s^{i}_{h},a^{i}_{h},b^{ i}_{h},s^{i+1}_{h+1}=s,a,b,s^{\prime})\) be the visitation count of \((h,s,a,b,s^{\prime})\) before the \(k\)-th episode. In multi-agent RL without privacy constraints, such visitation counts are sufficient for estimating the transition kernel \(\{P_{h}\}_{h=1}^{H}\) and updating the exploration policy, as in previous model-based approaches (Liu et al., 2021). However, these counts base on the original trajectories from the users, which could reveal sensitive information. Therefore, with the concern of privacy, we can only incorporate these counts after a privacy-preserving step. In other words, we use a Privatizer to transfer the original counts to the private version \(\widetilde{N}^{k}_{h}(s,a,b),\widetilde{N}^{k}_{h}(s,a,b,s^{\prime})\). We make the following Assumption 3.1 for Privatizer, which says that the private counts are close to real ones. Privatizers for JDP and LDP that satisfy Assumption 3.1 will be proposed in Section 5.

**Assumption 3.1** (Private counts).: _For any privacy budget \(\epsilon>0\) and failure probability \(\beta\in[0,1]\), there exists some \(E_{\epsilon,\beta}>0\) such that with probability at least \(1-\beta/3\), for all \((h,s,a,b,s^{\prime},k)\in[H]\times\mathcal{S}\times\mathcal{A}\times\mathcal{ B}\times\mathcal{S}\times[K]\), the \(\widetilde{N}^{k}_{h}(s,a,b,s^{\prime})\) and \(\widetilde{N}^{k}_{h}(s,a,b)\) from Privatizer satisfies:_(1) \(|\widetilde{N}_{h}^{k}(s,a,b,s^{\prime})-N_{h}^{k}(s,a,b,s^{\prime})|\leq E_{ \epsilon,\beta}\), \(|\widetilde{N}_{h}^{k}(s,a,b)-N_{h}^{k}(s,a,b)|\leq E_{\epsilon,\beta}\). \(\widetilde{N}_{h}^{k}(s,a,b,s^{\prime})>0\). (2) \(\widetilde{N}_{h}^{k}(s,a,b)=\sum_{s^{\prime}\in\mathcal{S}}\widetilde{N}_{h}^ {k}(s,a,b,s^{\prime})\geq N_{h}^{k}(s,a,b)\).

Given the private counts satisfying Assumption 3.1, the private estimate of transition kernel is defined as below.

\[\widetilde{P}_{h}^{k}(s^{\prime}|s,a,b)=\frac{\widetilde{N}_{h}^{k}(s,a,b,s^{ \prime})}{\widetilde{N}_{h}^{k}(s,a,b)},\ \ \forall\ (h,s,a,b,s^{\prime},k).\] (1)

**Remark 3.2**.: _Assumption 3.1 is a generalization of Assumption 3.1 of Qiao and Wang (2023) to the two-player setting. The assumption (2) guarantees that the private transition kernel \(\widetilde{P}_{h}^{k}(\cdot|s,a,b)\) is a valid probability distribution, which enables our usage of Bernstein-type bonus. Besides, \(\widetilde{P}\) is close to the empirical transition kernel based on original visitation counts according to Assumption (1)._

**Algorithmic design.** Following previous non-private approaches (Liu et al., 2021), DP-Nash-VI (Algorithm 1) maintains a pair of value functions \(\overline{Q}\) and \(\underline{Q}\) which are the upper bound and lower bound of the Q value of the current policy when facing best responses (with high probability). More specifically, we use private visitation counts \(\widetilde{N}_{h}^{k}\) to construct a private estimate of transition kernel \(\widetilde{P}_{h}^{k}\) (line 7) and a pair of private bonus \(\gamma_{h}^{k}\) (line 8) and \(\Gamma_{h}^{k}\) (line 9). Intuitively, the first term of \(\Gamma_{h}^{k}\) is derived from Bernstein's inequality while the second term is the additional bonus due to differential privacy. Next we do value iteration with bonuses to construct the UCB function \(\overline{Q}_{h}^{k}\) (line 10) and the LCB function \(\underline{Q}_{h}^{k}\) (line 11). The policy \(\pi^{k}\) for the \(k\)-th episode is calculated using the CCE function (discussed below) and we run \(\pi^{k}\) to collect a trajectory (line 14,18). Finally, the Privatizer transfers the non-private counts to private ones for the next episode (line 19). The output policy \(\pi^{\text{out}}\) is chosen as the policy \(\pi^{k}\) with minimal gap \((\overline{V}_{1}^{k}-\underline{V}_{1}^{k})(s_{1})\) (line 21). Decomposing the output policy, the output policy \((\mu^{\text{out}},\nu^{\text{out}})\) for both players are the marginal policies of \(\pi^{\text{out}}\), _i.e._\(\mu^{\text{out}}_{h}(\cdot|s)=\sum_{b\in\mathcal{B}}\pi^{\text{out}}_{h}(\cdot,b|s)\) and \(\nu^{\text{out}}_{h}(\cdot|s)=\sum_{a\in\mathcal{A}}\pi^{\text{out}}_{h}(a, \cdot|s)\) for all \((h,s)\in[H]\times\mathcal{S}\).

**Coarse Correlated Equilibrium (CCE).** Intuitively speaking, CCE of a Markov Game is a potentially correlated policy where no player could benefit from unilateral unconditional deviation. As a computationally friendly relaxation of Nash Equilibrium, CCE has been applied by previous works [Xie et al., 2020, Liu et al., 2021] to design efficient algorithms. Formally, for any two functions \(\overline{Q}(\cdot,\cdot),\underline{Q}(\cdot,\cdot):\mathcal{A}\times \mathcal{B}\rightarrow[0,H]\), \(\text{CCE}(\overline{Q},\underline{Q})\) returns a policy \(\pi\in\Delta(\mathcal{A}\times\mathcal{B})\) such that

\[\mathbb{E}_{(a,b)\sim\pi}\overline{Q}(a,b)\geq\max_{a^{\prime}}\mathbb{E}_{(a,b)\sim\pi}\overline{Q}(a^{\prime},b),\ \ \mathbb{E}_{(a,b)\sim\pi}\underline{Q}(a,b)\leq\min_{b^{\prime}}\mathbb{E}_{(a,b)\sim\pi}\underline{Q}(a,b^{\prime}).\]

Since Nash Equilibrium (NE) is a special case of CCE and a NE always exists. Moreover, a CCE can be derived in polynomial time via linear programming. Note that the policies given by CCE can be correlated for the two players, therefore deploying such policy requires the cooperation of both players (line 18).

## 4 Main results

We first state the regret analysis of DP-Nash-VI (Algorithm 1) based on Assumption 3.1, which can be combined with any Privatizers. The proof of Theorem 4.1 is sketched in Appendix B with details in the Appendix. Note that \((\mu^{k},\nu^{k})\) denote the marginal policies of \(\pi^{k}\) for both players.

**Theorem 4.1**.: _For any privacy budget \(\epsilon>0\), failure probability \(\beta\in[0,1]\) and any Privatizer satisfying Assumption 3.1, with probability at least \(1-\beta\), the regret of DP-Nash-VI (Algorithm 1) is_

\[\mathrm{Regret}(K)=\sum_{k=1}^{K}\left[V_{1}^{\dagger,\nu^{k}}(s_{1})-V_{1}^{ \mu^{k},\dagger}(s_{1})\right]\leq\widetilde{O}\left(\sqrt{H^{2}SABT}+H^{2}S^ {2}ABE_{\epsilon,\beta}\right),\] (2)

_where \(K\) is the number of episodes and \(T=HK\)._

Under the special case where the privacy budget \(\epsilon\rightarrow\infty\) (_i.e._ there is no privacy concern), plugging \(E_{\epsilon,\beta}=0\) in Theorem 4.1 will imply a regret bound of \(\widetilde{O}(\sqrt{H^{2}SABT})\). Such result directly matches the best known result for regret minimization without privacy constraints [Liu et al., 2021] and nearly matches the lower bound of \(\Omega(\sqrt{H^{2}S(A+B)T})\)[Bai and Jin, 2020]. Furthermore, under the special case of single-agent MDP (where \(B=1\)), our result reduces to \(\mathrm{Regret}(K)\leq\widetilde{O}(\sqrt{H^{2}SAT}+H^{2}S^{2}AE_{\epsilon, \beta})\). Such result matches the best known result under the same set of conditions (Theorem 4.1 of Qiao and Wang [2023]). Therefore, Theorem 4.1 is a generalization of the best known results under MARL [Liu et al., 2021] and Differentially Private (single-agent) RL [Qiao and Wang, 2023] simultaneously.

**PAC guarantee.** Recall that we output a policy \(\pi^{\text{out}}\) whose marginal policies are \((\mu^{\text{out}},\nu^{\text{out}})\). We highlight that the output policy for each player is a single Markov policy that is convenient to store and deploy. Moreover, as a corollary of the regret bound, we give a PAC bound for the output policy.

**Theorem 4.2**.: _For any privacy budget \(\epsilon>0\), failure probability \(\beta\in[0,1]\) and any Privatizer that satisfies Assumption 3.1, if the number of episodes satisfies that \(K\geq\widetilde{\Omega}\left(\frac{H^{3}SAB}{\alpha^{2}}+\min\left\{K^{\prime} |\frac{H^{2}S^{2}ABE_{\epsilon,\beta}}{K^{\prime}}\leq\alpha\right\}\right)\), with probability \(1-\beta\), \((\mu^{\text{out}},\nu^{\text{out}})\) is \(\alpha\)-approximate Nash, i.e., \(V_{1}^{\dagger,\nu^{\text{out}}}(s_{1})-V_{1}^{\mu^{\text{out}},\dagger}(s_{1})\leq\alpha\)._

The proof is deferred to Appendix C.4. Here the second term of the sample complexity bound4 ensures that the additional cost due to DP is bounded by \(O(\alpha)\). The detailed PAC guarantees under the special cases where the Privatizer is either Central or Local will be provided in Section 5.

Footnote 4: The presentation here is because the term \(E_{\epsilon,\beta}\) is indeed dependent of the number of episodes \(K\).

## 5 Privatizers for JDP and LDP

In this section, we propose Privatizers that provide DP guarantees (JDP or LDP) while satisfying Assumption 3.1. The proofs for this section can be found in Appendix D.

### Central Privatizer for Joint DP

Given the number of episodes \(K\), the Central Privatizer applies \(K\)-bounded Binary Mechanism (Chan et al., 2011) to privatize all the visitation counter streams \(N_{h}^{k}(s,a,b)\), \(N_{h}^{k}(s,a,b,s^{\prime})\), thus protecting the information of all single users. Briefly speaking, Binary mechanism takes a stream of partial sums as input and outputs a surrogate stream satisfying differential privacy, while the error for each item scales only logarithmically on the length of the stream5. Here in multi-agent RL, for each \((h,s,a,b)\), the stream \(\{N_{h}^{k}(s,a,b)=\sum_{i=1}^{k-1}\mathds{1}(s_{h}^{i},a_{h}^{i},b_{h}^{i}=s, a,b)\}_{k\in[K]}\) can be considered as the partial sums of \(\{\mathds{1}(s_{h}^{i},a_{h}^{i},b_{h}^{i}=s,a,b)\}\). Therefore, after observing \(\mathds{1}(s_{h}^{k},a_{h}^{k},b_{h}^{k}=s,a,b)\) at the end of episode \(k\), the Binary Mechanism will output a private version of \(\sum_{i=1}^{k}\mathds{1}(s_{h}^{i},a_{h}^{i},b_{h}^{i}=s,a,b)\). However, Binary Mechanism alone does not satisfy (2) of Assumption 3.1, and a post-processing step is required. To sum up, we let the Central Privatizer follow the workflow below:

Footnote 5: More details in Chan et al. (2011) and Kairouz et al. (2021).

Given the privacy budget for JDP \(\epsilon>0\),

(1) For all \((h,s,a,b,s^{\prime})\), we apply Binary Mechanism (Algorithm 2 in Chan et al. (2011)) with input parameter \(\epsilon^{\prime}=\frac{\epsilon}{2H\log K}\) to privatize all the visitation counter streams \(\{N_{h}^{k}(s,a,b)\}_{k\in[K]}\) and \(\{N_{h}^{k}(s,a,b,s^{\prime})\}_{k\in[K]}\). We denote the output of Binary Mechanism by \(\widehat{N}_{h}^{k}\).

(2) The private counts \(\widehat{N}_{h}^{k}\) are derived through Section 5.3 with \(E_{\epsilon,\beta}=O(\frac{H}{\epsilon}\log(HSABK/\beta)^{2})\).

Our Central Privatizer satisfies the privacy guarantee below.

**Lemma 5.1**.: _For any possible \(\epsilon,\beta\), the Central Privatizer satisfies \(\epsilon\)-JDP and Assumption 3.1 with \(E_{\epsilon,\beta}=\widetilde{O}(\frac{H}{\epsilon})\)._

Combining Lemma 5.1 with Theorem 4.1 and Theorem 4.2, we have the following regret \(\&\) PAC guarantee under \(\epsilon\)-JDP.

**Theorem 5.2** (Results under JDP).: _For any possible \(\epsilon,\beta\), with probability \(1-\beta\), the regret from running DP-Nash-VI (Algorithm 1) instantiated with Central Privatizer satisfies:_

\[\mathrm{Regret}(K)\leq\widetilde{O}(\sqrt{H^{2}SABT}+H^{3}S^{2}AB/\epsilon).\] (3)

_Moreover, if the number of episodes \(K\) is larger than \(\widetilde{\Omega}(\frac{H^{3}SAB}{\alpha^{2}}+\frac{H^{3}S^{2}AB}{\epsilon \alpha})\), with probability \(1-\beta\), the output policy \((\mu^{\mathrm{out}},\nu^{\mathrm{out}})\) is \(\alpha\)-approximate Nash._

Similar to the single-agent (MDP) setting (\(B=1\)), the additional cost due to JDP is a lower order term under the most prevalent regime where the privacy budget \(\epsilon\) is a constant. When applied to the single-agent case, our regret matches the best known regret \(\widetilde{O}(\sqrt{H^{2}SAT}+H^{3}S^{2}A/\epsilon)\)(Qiao and Wang, 2023). Moreover, when compared to the regret lower bound below, our main term is nearly optimal while the lower order term has optimal dependence on \(\epsilon\).

**Theorem 5.3**.: _For any algorithm \(\mathrm{Alg}\) satisfying \(\epsilon\)-JDP, there exists a Markov Game such that the expected regret from running \(\mathrm{Alg}\) for \(K\) episodes (\(T=HK\) steps) satisfies:_

\[\mathbb{E}\left[\mathrm{Regret}(K)\right]\geq\widetilde{\Omega}(\sqrt{H^{2}S(A +B)T}+\frac{HS(A+B)}{\epsilon}).\]

The regret lower bound results from the lower bound for the non-private learning (Bai and Jin, 2020) and an adaptation of the lower bound under JDP guarantees (Vietri et al., 2020) to the multi-player setting. Details are deferred to the appendix.

### Local Privatizer for Local DP

At the end of episode \(k\), the Local Privatizer perturbs the statistics calculated from the new trajectory before sending it to the agent. Since the set of original visitation counts \(\{\sigma_{h}^{k}(s,a,b)=\mathds{1}(s_{h}^{k},a_{h}^{k},b_{h}^{k}=s,a,b)\}_{(h,s,a,b)}\) has \(\ell_{1}\) sensitivity \(H\), we can achieve \(\frac{\epsilon}{2}\)-LDP by directly adding Laplace noise, _i.e._, \(\widetilde{\sigma}_{h}^{k}(s,a,b)=\sigma_{h}^{k}(s,a,b)+\text{Lap}(\frac{2H}{ \epsilon})\). Similarly, repeating the above perturbation to \(\{\mathds{1}(s_{h}^{k},a_{h}^{k},b_{h}^{k},s_{h+1}^{k}=s,a,b,s^{\prime})\}_{(h,s,a,b,s^{\prime})}\) will lead to identical results. Therefore, the Local Privatizer with budget \(\epsilon\) is as below:(1) We perturb \(\sigma^{k}_{h}(s,a,b)=\mathds{1}(s^{k}_{h},a^{k}_{h},b^{k}_{h}=s,a,b)\) and \(\sigma^{k}_{h}(s,a,b,s^{\prime})=\mathds{1}(s^{k}_{h},a^{k}_{h},b^{k}_{h},s^{k}_ {h+1}=s,a,b,s^{\prime})\) by adding independent Laplace noises: for all \((h,s,a,b,s^{\prime},k)\),

\[\widetilde{\sigma}^{k}_{h}(s,a,b)=\sigma^{k}_{h}(s,a,b)+\text{\sf Lap}\left( \frac{2H}{\epsilon}\right),\ \ \widetilde{\sigma}^{k}_{h}(s,a,b,s^{\prime})=\sigma^{k}_{h}(s,a,b,s^{\prime})+ \text{\sf Lap}\left(\frac{2H}{\epsilon}\right).\] (4)

(2) Then the noisy counts are derived according to

\[\widehat{N}^{k}_{h}(s,a,b)=\sum_{i=1}^{k-1}\widetilde{\sigma}^{i}_{h}(s,a,b), \ \ \widehat{N}^{k}_{h}(s,a,b,s^{\prime})=\sum_{i=1}^{k-1}\widetilde{\sigma}^{i}_{ h}(s,a,b,s^{\prime}),\] (5)

and the private counts \(\widetilde{N}^{k}_{h}\) are solved through Section 5.3 with \(E_{\epsilon,\beta}=O(\frac{H}{\epsilon}\sqrt{K\log(HSABK/\beta)})\).

Our Local Privatizer satisfies the privacy guarantee below.

**Lemma 5.4**.: _For any possible \(\epsilon,\beta\), the Local Privatizer satisfies \(\epsilon\)-LDP and Assumption 3.1 with \(E_{\epsilon,\beta}=\widetilde{O}(\frac{H}{\epsilon}\sqrt{K})\)._

Combining Lemma 5.4 with Theorem 4.1 and Theorem 4.2, we have the following regret \(\&\) PAC guarantee under \(\epsilon\)-LDP.

**Theorem 5.5** (Results under LDP).: _For any possible \(\epsilon,\beta\), with probability \(1-\beta\), the regret from running DP-Nash-VI (Algorithm 1) instantiated with Local Privatizer satisfies:_

\[\mathrm{Regret}(K)\leq\widetilde{O}\left(\sqrt{H^{2}SABT}+S^{2}AB\sqrt{H^{5}T }/\epsilon\right).\] (6)

_Moreover, if the number of episodes \(K\) is larger than \(\widetilde{\Omega}\left(\frac{H^{3}SAB}{\alpha^{2}}+\frac{H^{6}S^{4}A^{2}B^{2 }}{\epsilon^{2}\alpha^{2}}\right)\), with probability \(1-\beta\), the output policy \((\mu^{\mathrm{out}},\nu^{\mathrm{out}})\) is \(\alpha\)-approximate Nash._

Similar to the single-agent case, the additional cost due to LDP is a multiplicative factor to the regret bound. When applied to the single-agent case, our regret matches the best known regret \(\widetilde{O}\left(\sqrt{H^{2}SAT}+S^{2}A\sqrt{H^{5}T}/\epsilon\right)\)(Qiao and Wang, 2023). Moreover, we state the lower bound.

**Theorem 5.6**.: _For any algorithm \(\mathrm{Alg}\) satisfying \(\epsilon\)-LDP, there exists a Markov Game such that the expected regret from running \(\mathrm{Alg}\) for \(K\) episodes (\(T=HK\) steps) satisfies:_

\[\mathbb{E}\left[\mathrm{Regret}(K)\right]\geq\widetilde{\Omega}\left(\sqrt{H^ {2}S(A+B)T}+\frac{\sqrt{HS(A+B)T}}{\epsilon}\right).\]

The lower bound is adapted from Garcelon et al. (2021). While our regret has optimal dependence on \(\epsilon\) and \(K\), the optimal dependence on \(H,S,A,B\) remains open.

### The post-processing step

Now we introduce the post-processing step. At the end of episode \(k\), given the noisy counts \(\widehat{N}^{k}_{h}(s,a,b)\) and \(\widehat{N}^{k}_{h}(s,a,b,s^{\prime})\) for all \((h,s,a,b,s^{\prime})\), the private visitation counts are constructed as following: for all \((h,s,a,b)\),

\[\left\{\widetilde{N}^{k}_{h}(s,a,b,s^{\prime})\right\}_{s^{\prime}\in\mathcal{S }}=\operatorname*{argmin}_{\{x_{s^{\prime}}\}_{s^{\prime}\in\mathcal{S}}} \ \max_{s^{\prime}\in\mathcal{S}}\left|x_{s^{\prime}}-\widehat{N}^{k}_{h}(s,a,b,s^{ \prime})\right|\]

such that \(\left|\sum_{s^{\prime}\in\mathcal{S}}x_{s^{\prime}}-\widehat{N}^{k}_{h}(s,a,b) \right|\leq\frac{E_{\epsilon,\beta}}{4}\) and \(x_{s^{\prime}}\geq 0,\ \forall\,s^{\prime}\). \(\widetilde{N}^{k}_{h}(s,a,b)=\sum_{s^{\prime}\in\mathcal{S}}\widetilde{N}^{k} _{h}(s,a,b,s^{\prime})\).

Lastly, we add a constant term to each count to ensure no underestimation (with high probability).

\[\widetilde{N}^{k}_{h}(s,a,b,s^{\prime})=\widetilde{N}^{k}_{h}(s,a,b,s^{\prime} )+\frac{E_{\epsilon,\beta}}{2S},\ \ \ \widetilde{N}^{k}_{h}(s,a,b)=\widetilde{N}^{k}_{h}(s,a,b)+\frac{E_{ \epsilon,\beta}}{2}.\] (8)

**Remark 5.7**.: _Solving problem (7) is equivalent to solving:_

\[\min\;t,\;\text{s.t.}\;\left|x_{s^{\prime}}-\widehat{N}_{h}^{k}(s,a,b,s^{\prime} )\right|\leq t,\;x_{s^{\prime}}\geq 0,\;\forall\,s^{\prime}\in\mathcal{S},\; \;\left|\sum_{s^{\prime}\in\mathcal{S}}x_{s^{\prime}}-\widehat{N}_{h}^{k}(s,a, b)\right|\leq\frac{E_{\epsilon,\beta}}{4},\]

_which is a **Linear Programming** problem with \(O(S)\) variables and \(O(S)\) linear constraints. This can be solved in polynomial time (Nemhauser and Wolsey, 1988). Note that the computation of CCE (line 14 in Algorithm 1) is also a LP problem, therefore the computational complexity of DP-Nash-VI is dominated by \(O(HSABK)\) Linear Programming problems, which is computationally friendly._

We summarize the properties of private counts \(\widetilde{N}_{h}^{k}\) below, which says that the post-processing step ensures that our private transition kernel estimate is a valid probability distribution while only enlarging the error by a constant factor.

**Lemma 5.8**.: _Suppose \(\widehat{N}_{h}^{k}\) satisfies that with probability \(1-\frac{\beta}{3}\), uniformly over all \((h,s,a,b,s^{\prime},k)\),_

\[\left|\widehat{N}_{h}^{k}(s,a,b,s^{\prime})-N_{h}^{k}(s,a,b,s^{\prime})\right| \leq\frac{E_{\epsilon,\beta}}{4},\;\;\;\left|\widehat{N}_{h}^{k}(s,a,b)-N_{h} ^{k}(s,a,b)\right|\leq\frac{E_{\epsilon,\beta}}{4},\]

_then the \(\widetilde{N}_{h}^{k}\) derived above satisfies Assumption 3.1._

### Some discussions

In this part, we generalize the Privatizers in Qiao and Wang (2023) (for single-agent case) to the two-player setting, which enables our usage of Bernstein-type bonuses. Such techniques lead to a tight regret analysis and a near-optimal "non-private part" of the regret bound eventually.

Meanwhile, the additional cost due to DP has sub-optimal dependence on parameters regarding the Markov Game. The issue appears even in the single-agent case and is considered to be inherent to model-based algorithms due to the explicit estimation of private transitions (Garcelon et al., 2021). The improvement requires new algorithmic designs (_e.g._, private Q-learning) and we leave those as future works.

Lastly, the Laplace Mechanism can be replaced with other mechanisms, such as Gaussian Mechanism (Dwork et al., 2014) with approximate DP guarantee (or zCDP). The regret and PAC guarantees are readily derived by plugging in the corresponding \(E_{\epsilon,\beta}\) to Theorem 4.1 and Theorem 4.2.

## 6 Conclusion

We take the initial steps to study trajectory-wise privacy protection in multi-agent RL. We extend the definitions of Joint DP and Local DP to multi-player RL. In addition, we design a provably-efficient algorithm: DP-Nash-VI (Algorithm 1) that could satisfy either of the two DP constraints with corresponding regret guarantee. Moreover, our regret bounds strictly generalize the best known results under DP single-agent RL. There are various interesting future directions, such as improving the additional cost due to DP via model-free approaches and considering Markov Games with function approximations. We believe the techniques in this paper could serve as basic building blocks.

## Acknowledgments

The research is partially supported by NSF Awards \(\#\)2007117 and \(\#\)2048091. The work was done while DQ and YW were with the Department of Computer Science at UCSB.

## References

* Ayoub et al. (2020) Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_, pages 463-474. PMLR, 2020.
* Azar et al. (2017) Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _Proceedings of the 34th International Conference on Machine Learning-Volume 70_, pages 263-272. JMLR. org, 2017.
* Azar et al. (2018)Yoram Bachrach, Richard Everett, Edward Hughes, Angeliki Lazaridou, Joel Z Leibo, Marc Lanctot, Michael Johanson, Wojciech M Czarnecki, and Thore Graepel. Negotiating team formation using deep reinforcement learning. _Artificial Intelligence_, 288:103356, 2020.
* Bai and Jin (2020) Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In _International Conference on Machine Learning_, pages 551-560. PMLR, 2020.
* Bai et al. (2020) Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. _Advances in neural information processing systems_, 33:2159-2170, 2020.
* Balle et al. (2016) Borja Balle, Maziar Gomrokchi, and Doina Precup. Differentially private policy evaluation. In _International Conference on Machine Learning_, pages 2130-2138. PMLR, 2016.
* Brown et al. (2021) Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. When is memorization of irrelevant training data necessary for high-accuracy learning? In _ACM SIGACT Symposium on Theory of Computing_, pages 123-132, 2021.
* Brown and Sandholm (2019) Noam Brown and Tuomas Sandholm. Superhuman ai for multiplayer poker. _Science_, 365(6456):885-890, 2019.
* Bun and Steinke (2016) Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and lower bounds. In _Theory of Cryptography Conference_, pages 635-658. Springer, 2016.
* Carlini et al. (2019) Nicholas Carlini, Chang Liu, Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In _USENIX Security Symposium (USENIX Security 19)_, pages 267-284, 2019.
* Chan et al. (2011) T-H Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. _ACM Transactions on Information and System Security (TISSEC)_, 14(3):1-24, 2011.
* Chowdhury and Zhou (2022) Sayak Ray Chowdhury and Xingyu Zhou. Differentially private regret minimization in episodic markov decision processes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2022.
* Chowdhury et al. (2021) Sayak Ray Chowdhury, Xingyu Zhou, and Ness Shroff. Adaptive control of differentially private linear quadratic systems. In _2021 IEEE International Symposium on Information Theory (ISIT)_, pages 485-490. IEEE, 2021.
* Chowdhury et al. (2023) Sayak Ray Chowdhury, Xingyu Zhou, and Nagarajan Natarajan. Differentially private reward estimation with preference feedback. _arXiv preprint arXiv:2310.19733_, 2023.
* Cui et al. (2023) Qiwen Cui, Kaiqing Zhang, and Simon Du. Breaking the curse of multiagents in a large state space: Rl in markov games with independent linear function approximation. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2651-2652. PMLR, 2023.
* Cundy and Ermon (2020) Chris Cundy and Stefano Ermon. Privacy-constrained policies via mutual information regularized policy gradients. _arXiv preprint arXiv:2012.15019_, 2020.
* Dann et al. (2017) Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning. In _Advances in Neural Information Processing Systems_, pages 5713-5723, 2017.
* Duchi et al. (2013) John C Duchi, Michael I Jordan, and Martin J Wainwright. Local privacy and statistical minimax rates. In _2013 IEEE 54th Annual Symposium on Foundations of Computer Science_, pages 429-438. IEEE, 2013.
* Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of cryptography conference_, pages 265-284. Springer, 2006.
* Dwork et al. (2014) Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Found. Trends Theor. Comput. Sci._, 9(3-4):211-407, 2014.
* Filar and Vrieze (2012) Jerzy Filar and Koos Vrieze. _Competitive Markov decision processes_. Springer Science & Business Media, 2012.
* Filar et al. (2017)Evrard Garcelon, Vianney Perchet, Ciara Pike-Burke, and Matteo Pirotta. Local differential privacy for regret minimization in reinforcement learning. _Advances in Neural Information Processing Systems_, 34, 2021.
* Gohari et al. (2023) Parham Gohari, Matthew Hale, and Ufuk Topcu. Privacy-engineered value decomposition networks for cooperative multi-agent reinforcement learning. In _2023 62nd IEEE Conference on Decision and Control (CDC)_, pages 8038-8044. IEEE, 2023.
* Hossain and Lee (2023) Md Tamjid Hossain and John WT Lee. Hiding in plain sight: Differential privacy noise exploitation for evasion-resilient localized poisoning attacks in multiagent reinforcement learning. In _2023 International Conference on Machine Learning and Cybernetics (ICMLC)_, pages 209-216. IEEE, 2023.
* Hossain et al. (2023) Md Tamjid Hossain, Hung Manh La, Shahriar Badsha, and Anton Netchaev. Brnes: Enabling security and privacy-aware experience sharing in multiagent robotic and autonomous systems. In _2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 9269-9276. IEEE, 2023.
* Hsu et al. (2014) Justin Hsu, Zhiyi Huang, Aaron Roth, Tim Roughgarden, and Zhiwei Steven Wu. Private matchings and allocations. In _Proceedings of the forty-sixth annual ACM symposium on Theory of computing_, pages 21-30, 2014.
* Jaksch et al. (2010) Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. _Journal of Machine Learning Research_, 11(4), 2010.
* Jin et al. (2018) Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient? In _Advances in Neural Information Processing Systems_, pages 4863-4873, 2018.
* Jin et al. (2020) Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* Jin et al. (2021) Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning-a simple, efficient, decentralized algorithm for multiagent rl. _arXiv preprint arXiv:2110.14555_, 2021.
* Kairouz et al. (2021) Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, and Zheng Xu. Practical and private (deep) learning without sampling or shuffling. In _International Conference on Machine Learning_, pages 5213-5225. PMLR, 2021.
* Kearns et al. (2014) Michael Kearns, Mallesh Pai, Aaron Roth, and Jonathan Ullman. Mechanism design in large games: Incentives and privacy. In _Proceedings of the 5th conference on Innovations in theoretical computer science_, pages 403-410, 2014.
* Lebensold et al. (2019) Jonathan Lebensold, William Hamilton, Borja Balle, and Doina Precup. Actor critic with differentially private critic. _arXiv preprint arXiv:1910.05876_, 2019.
* Liao et al. (2023) Chonghua Liao, Jiafan He, and Quanquan Gu. Locally differentially private reinforcement learning for linear mixture markov decision processes. In _Asian Conference on Machine Learning_, pages 627-642. PMLR, 2023.
* Liu et al. (2021) Qinghua Liu, Tiancheng Yu, Yu Bai, and Chi Jin. A sharp analysis of model-based reinforcement learning with self-play. In _International Conference on Machine Learning_, pages 7001-7010. PMLR, 2021.
* Luyo et al. (2021) Paul Luyo, Evrard Garcelon, Alessandro Lazaric, and Matteo Pirotta. Differentially private exploration in reinforcement learning with linear representation. _arXiv preprint arXiv:2112.01585_, 2021.
* Mao et al. (2022) Weichao Mao, Lin Yang, Kaiqing Zhang, and Tamer Basar. On improving model-free algorithms for decentralized multi-agent reinforcement learning. In _International Conference on Machine Learning_, pages 15007-15049. PMLR, 2022.
* Nemhauser and Wolsey (1988) George Nemhauser and Laurence Wolsey. Polynomial-time algorithms for linear programming. _Integer and Combinatorial Optimization_, pages 146-181, 1988.
* Neves and Vanhoucke (2015)Dung Daniel T Ngo, Giuseppe Vietri, and Steven Wu. Improved regret for differentially private exploration in linear mdp. In _International Conference on Machine Learning_, pages 16529-16552. PMLR, 2022.
* Ono and Takahashi [2020] Hajime Ono and Tsubasa Takahashi. Locally private distributed reinforcement learning. _arXiv preprint arXiv:2001.11718_, 2020.
* Qiao and Wang [2022a] Dan Qiao and Yu-Xiang Wang. Near-optimal deployment efficiency in reward-free reinforcement learning with linear function approximation. _arXiv preprint arXiv:2210.00701_, 2022a.
* Qiao and Wang [2022b] Dan Qiao and Yu-Xiang Wang. Offline reinforcement learning with differential privacy. _arXiv preprint arXiv:2206.00810_, 2022b.
* Qiao and Wang [2023] Dan Qiao and Yu-Xiang Wang. Near-optimal differentially private reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 9914-9940. PMLR, 2023.
* Qiao and Wang [2024] Dan Qiao and Yu-Xiang Wang. Near-optimal reinforcement learning with self-play under adaptivity constraints. _arXiv preprint arXiv:2402.01111_, 2024.
* Qiao et al. [2022] Dan Qiao, Ming Yin, Ming Min, and Yu-Xiang Wang. Sample-efficient reinforcement learning with loglog(T) switching cost. In _International Conference on Machine Learning_, pages 18031-18061. PMLR, 2022.
* Qiao et al. [2023] Dan Qiao, Ming Yin, and Yu-Xiang Wang. Logarithmic switching cost in reinforcement learning beyond linear mdps. _arXiv preprint arXiv:2302.12456_, 2023.
* Shalev-Shwartz et al. [2016] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning for autonomous driving. _arXiv preprint arXiv:1610.03295_, 2016.
* Shapley [1953] Lloyd S Shapley. Stochastic games. _Proceedings of the national academy of sciences_, 39(10):1095-1100, 1953.
* Shariff and Sheffet [2018] Roshan Shariff and Or Sheffet. Differentially private contextual linear bandits. _Advances in Neural Information Processing Systems_, 31, 2018.
* Shavandi and Khedmati [2022] Ali Shavandi and Majid Khedmati. A multi-agent deep reinforcement learning framework for algorithmic trading in financial markets. _Expert Systems with Applications_, 208:118124, 2022.
* Silver et al. [2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. _nature_, 550(7676):354-359, 2017.
* Ullah et al. [2023] Imdad Ullah, Najm Hassan, Sukhpal Singh Gill, Basem Suleiman, Tariq Ahmed Ahanger, Zawar Shah, Junaid Qadir, and Salil S Kanhere. Privacy preserving large language models: Chatgpt case study based vision and framework. _arXiv preprint arXiv:2310.12523_, 2023.
* Vietri et al. [2020] Giuseppe Vietri, Borja Balle, Akshay Krishnamurthy, and Steven Wu. Private reinforcement learning with pac and regret guarantees. In _International Conference on Machine Learning_, pages 9754-9764. PMLR, 2020.
* Wang and Hegde [2019] Baoxiang Wang and Nidhi Hegde. Privacy-preserving q-learning with functional noise in continuous spaces. _Advances in Neural Information Processing Systems_, 32, 2019.
* Wang et al. [2023] Yuanhao Wang, Qinghua Liu, Yu Bai, and Chi Jin. Breaking the curse of multiagency: Provably efficient decentralized multi-agent rl with function approximation. _arXiv preprint arXiv:2302.06606_, 2023.
* Wu et al. [2023a] Fan Wu, Huseyin A Inan, Arturs Backurs, Varun Chandrasekaran, Janardhan Kulkarni, and Robert Sim. Privately aligning language models with reinforcement learning. _arXiv preprint arXiv:2310.16960_, 2023a.
* Wu et al. [2023b] Yulian Wu, Xingyu Zhou, Sayak Ray Chowdhury, and Di Wang. Differentially private episodic reinforcement learning with heavy-tailed rewards. _arXiv preprint arXiv:2306.01121_, 2023b.
* Wu et al. [2023c]* Xie et al. [2020] Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium. In _Conference on learning theory_, pages 3674-3682. PMLR, 2020.
* Xie et al. [2019] Tengyang Xie, Philip S Thomas, and Gerome Miklau. Privacy preserving off-policy evaluation. _arXiv preprint arXiv:1902.00174_, 2019.
* Ye et al. [2020] Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, et al. Towards playing full moba games with deep reinforcement learning. _Advances in Neural Information Processing Systems_, 33:621-632, 2020.
* Zhao et al. [2023a] Canzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, and Shuai Li. Differentially private temporal difference learning with stochastic nonconvex-strongly-concave optimization. In _Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining_, pages 985-993, 2023a.
* Zhao et al. [2023b] Canzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, and Shuai Li. Dpmac: differentially private communication for cooperative multi-agent reinforcement learning. _arXiv preprint arXiv:2308.09902_, 2023b.
* Zhao et al. [2022] Fuheng Zhao, Dan Qiao, Rachel Redberg, Divyakant Agrawal, Amr El Abbadi, and Yu-Xiang Wang. Differentially private linear sketches: Efficient implementations and applications. _arXiv preprint arXiv:2205.09873_, 2022.
* Zhou [2022] Xingyu Zhou. Differentially private reinforcement learning with linear function approximation. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 6(1):1-27, 2022.

Extended related works

**Differentially private reinforcement learning.** The stream of research on DP RL started from the offline setting. Balle et al. (2016) first studied privately evaluating the value of a fixed policy from running it for several episodes (the on policy setting). Later, Xie et al. (2019) considered a more general setting of DP off policy evaluation. Recently, Qiao and Wang (2022b) provided the first results for offline reinforcement learning with DP guarantees.

More efforts focused on solving regret minimization. Under the setting of tabular MDP, Vietri et al. (2020) designed PUCB by privatizing UBEV (Dann et al., 2017) to satisfy Joint DP. Besides, under the constraints of Local DP, Garcelon et al. (2021) designed LDP-OBI based on UCRL2 (Jaksch et al., 2010). Chowdhury and Zhou (2022) designed a general framework for both JDP and LDP based on UCBVI (Azar et al., 2017), and improved upon previous results. Finally, the best known results are obtained by Qiao and Wang (2023) via incorporating Bernstein-type bonuses. Meanwhile, Wu et al. (2023b) studied the case with heavy-tailed rewards. Under linear MDP, the only algorithm with JDP guarantee: Private LSVI-UCB (Ngo et al., 2022) is a private and low switching 6 version of LSVI-UCB (Jin et al., 2020), while LDP under linear MDP still remains open. Under linear mixture MDP, LinOpt-VI-Reg (Zhou, 2022) generalized UCRL-VTR (Ayoub et al., 2020) to guarantee JDP, while Liao et al. (2023) also privatized UCRL-VTR for LDP guarantee. In addition, Luyo et al. (2021) provided a unified framework for analyzing joint and local DP exploration.

Footnote 6: For low switching RL, please refer to Qiao et al. (2022), Qiao and Wang (2022a), Qiao et al. (2023), Qiao and Wang (2024).

There are several other works regarding DP RL. Wang and Hegde (2019) proposed privacy-preserving Q-learning to protect the reward information. Ono and Takahashi (2020) studied the problem of distributed reinforcement learning under LDP. Lebensold et al. (2019) presented an actor critic algorithm with differentially private critic. Cundy and Ermon (2020) tackled DP-RL under the policy gradient framework. Chowdhury et al. (2021) considered the adaptive control of differentially private linear quadratic (LQ) systems. Zhao et al. (2023a) studied differentially private temporal difference (TD) learning. Chowdhury et al. (2023) analyzed reward estimation with preference feedback under the constraints of DP. Hossain and Lee (2023), Hossain et al. (2023), Zhao et al. (2023b), Gohari et al. (2023) focused on the privatization of communications between multiple agents in multi-agent RL. For applications, DP RL was applied to protect sensitive information in natural language processing and large language models (LLM) (Ullah et al., 2023; Wu et al., 2023a). Meanwhile, Zhao et al. (2022) considered linear sketches with DP.

## Appendix B Proof overview

In this section, we provide a proof sketch of Theorem 4.1, which can further imply the PAC guarantee (Theorem 4.2) and the regret bounds under JDP (Theorem 5.2) or LDP (Theorem 5.5). The proof consists of the following steps:

(1) Bound the difference between the private statistics and their non-private counterparts.

(2) Prove that UCB and LCB hold with high probability.

(3) Bound the regret via telescoping over time steps and replace the private terms by non-private ones.

Below we explain the key steps in detail. Recall that \(N_{h}^{k}\) denotes the real visitation counts, while \(\tilde{N}_{h}^{k},\tilde{P}_{h}^{k}\) are the private visitation counts and private transition kernel respectively.

**Step (1).** According to Assumption 3.1 and standard concentration inequalities, we provide high probability upper bounds for \(\|\widetilde{P}_{h}^{k}(\cdot|s,a,b)-P_{h}(\cdot|s,a,b)\|_{1}\) and \(|\widetilde{P}_{h}^{k}(s^{\prime}|s,a,b)-P_{h}(s^{\prime}|s,a,b)|\). Besides, we upper bound the following key term \(|(\tilde{P}_{h}^{k}-P_{h})\cdot V_{h+1}^{*}(s,a,b)|\) by \(\widetilde{O}\left(\sqrt{\operatorname{Var}_{\tilde{P}_{h}^{k}(\cdot|s,a,b)} \tilde{V}_{h+1}^{*}(\cdot)/\widetilde{N}_{h}^{k}(s,a,b)}+HSE_{e,\beta}/ \widetilde{N}_{h}^{k}(s,a,b)\right)\). Details are deferred to Appendix C.1.

**Step (2).** Then we prove that UCB and LCB hold with high probability via backward induction over timesteps (Appendix C.2). More specifically, the variance term of \(\Gamma_{h}^{k}\) is the private Bernstein-typebonus, while the difference between the private variance and its non-private counterpart can be bounded by \(\gamma_{h}^{k}\) and the lower order terms in \(\Gamma_{h}^{k}\).

**Step (3).** Lastly, the regret can be bounded by telescoping:

\[\mathrm{Regret}(K)\leq\underbrace{O\left(\sum_{k=1}^{K}\sum_{h=1}^ {H}\Gamma_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h}^{k})\right)}_{\text{bound by non-private terms}}\] \[\leq \widetilde{O}\left(\sum_{\begin{subarray}{c}k=1\\ k\end{subarray}}^{K}\sum_{h=1}^{H}\sqrt{\frac{\mathrm{Var}_{P_{h}(\cdot|s_{h}^ {k},a_{h}^{k},b_{h}^{k})}V_{h+1}^{\pi^{k}}}{N_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h }^{k})}}+\underbrace{\sum_{\begin{subarray}{c}k=1\\ k\end{subarray}}^{K}\sum_{h=1}^{H}\frac{HSE_{\epsilon,\beta}}{N_{h}^{k}(s_{h} ^{k},a_{h}^{k},b_{h}^{k})}}_{\leq H^{2}S^{2}ABE_{\epsilon,\beta t}}\right)\] \[\leq \widetilde{O}(\sqrt{H^{2}SABT}+H^{2}S^{2}ABE_{\epsilon,\beta}).\]

The details about each inequality above and the lower order terms we ignore are deferred to Appendix C.3.

## Appendix C Proof of main theorems

In this section, we prove Theorem 4.1 and Theorem 4.2.

### Properties of private estimations

We begin with some concentration results about our private transition kernel estimate \(\widetilde{P}\) that will be useful for the proof. Throughout the paper, let the non-private empirical transition kernel be:

\[\widehat{P}_{h}^{k}(s^{\prime}|s,a,b)=\frac{N_{h}^{k}(s,a,b,s^{\prime})}{N_{h }^{k}(s,a,b)},\ \ \forall\ (h,s,a,b,s^{\prime},k).\] (9)

In addition, recall that our private transition kernel estimate is defined as below.

\[\widetilde{P}_{h}^{k}(s^{\prime}|s,a,b)=\frac{\widetilde{N}_{h}^{k}(s,a,b,s^ {\prime})}{\widetilde{N}_{h}^{k}(s,a,b)},\ \ \forall\ (h,s,a,b,s^{\prime},k).\] (10)

Now we are ready to list the properties below. Note that \(\iota=\log(30HSABK/\beta)\) throughout the paper.

**Lemma C.1**.: _With probability \(1-\frac{\beta}{15}\), for all \((h,s,a,b,k)\in[H]\times\mathcal{S}\times\mathcal{A}\times\mathcal{B}\times[K]\), it holds that:_

\[\left\|\widetilde{P}_{h}^{k}(\cdot|s,a,b)-P_{h}(\cdot|s,a,b)\right\|_{1}\leq 2 \sqrt{\frac{S\iota}{\widetilde{N}_{h}^{k}(s,a,b)}}+\frac{2SE_{\epsilon,\beta} }{\widetilde{N}_{h}^{k}(s,a,b)},\] (11)

\[\left\|\widetilde{P}_{h}^{k}(\cdot|s,a,b)-\widetilde{P}_{h}^{k}(\cdot|s,a,b) \right\|_{1}\leq\frac{2SE_{\epsilon,\beta}}{\widetilde{N}_{h}^{k}(s,a,b)}.\] (12)

Proof of Lemma c.1.: The proof is a direct generalization of Lemma B.2 and Remark B.3 in Qiao and Wang [2023] to the two-player setting. 

**Lemma C.2**.: _With probability \(1-\frac{2\beta}{15}\), for all \((h,s,a,b,s^{\prime},k)\in[H]\times\mathcal{S}\times\mathcal{A}\times\mathcal{ B}\times\mathcal{S}\times[K]\), it holds that:_

\[\left|\widetilde{P}_{h}^{k}(s^{\prime}|s,a,b)-P_{h}(s^{\prime}|s,a,b)\right| \leq 2\sqrt{\frac{\min\{P_{h}(s^{\prime}|s,a,b),\widetilde{P}_{h}^{k}(s^{ \prime}|s,a,b)\iota}}{\widetilde{N}_{h}^{k}(s,a,b)}}+\frac{2E_{\epsilon,\beta }\iota}{\widetilde{N}_{h}^{k}(s,a,b)},\] (13)

\[\left|\widetilde{P}_{h}^{k}(s^{\prime}|s,a,b)-\widehat{P}_{h}^{k}(s^{\prime}| s,a,b)\right|\leq\frac{2E_{\epsilon,\beta}}{\widetilde{N}_{h}^{k}(s,a,b)}.\] (14)Proof of Lemma c.2.: The proof is a direct generalization of Lemma B.4 and Remark B.5 in Qiao and Wang [2023] to the two-player setting. 

**Lemma C.3**.: _With probability \(1-\frac{2\beta}{15}\), for all \((h,s,a,b,k)\in[H]\times\mathcal{S}\times\mathcal{A}\times\mathcal{B}\times[K]\), it holds that:_

\[\Big{|}\Big{(}\widetilde{P}_{h}^{k}-P_{h}\Big{)}\cdot V_{h+1}^{ \star}(s,a,b)\Big{|}\leq\min\Bigg{\{}\sqrt{\frac{2\mathrm{Var}_{P_{h}(\cdot|s, a,b)}V_{h+1}^{\star}(\cdot)\cdot\iota}{\widetilde{N}_{h}^{k}(s,a,b)}},\sqrt{ \frac{2\mathrm{Var}_{\widehat{P}_{h}^{k}(\cdot|s,a,b)}V_{h+1}^{\star}(\cdot) \cdot\iota}{\widetilde{N}_{h}^{k}(s,a,b)}}\Bigg{\}}+\frac{2HSE_{\epsilon, \beta\iota}}{\widetilde{N}_{h}^{k}(s,a,b)},\] (15) \[\Big{|}\Big{(}\widetilde{P}_{h}^{k}-\widehat{P}_{h}^{k}\Big{)} \cdot V_{h+1}^{\star}(s,a,b)\Big{|}\leq\frac{2HSE_{\epsilon,\beta}}{ \widetilde{N}_{h}^{k}(s,a,b)}.\] (16)

Proof of Lemma c.3.: The proof is a direct generalization of Lemma B.6 and Remark B.7 in Qiao and Wang [2023] to the two-player setting. 

According to a union bound, the following lemma holds.

**Lemma C.4**.: _Under the high probability event that Assumption 3.1 holds, with probability at least \(1-\frac{\beta}{3}\), the conclusions in Lemma C.1, Lemma C.2, Lemma C.3 hold simultaneously._

Throughout the proof, we will assume that Assumption 3.1 and Lemma C.4 hold, which will happen with high probability. Before we prove the main theorems, we present the following lemma which bounds the two variances.

**Lemma C.5** (Lemma C.5 of Qiao and Wang [2022b]).: _For any function \(V\in\mathbb{R}^{S}\) such that \(\|V\|_{\infty}\leq H\), it holds that_

\[\Big{|}\sqrt{\mathrm{Var}_{\widetilde{P}_{h}^{k}(\cdot|s,a,b)}(V)}-\sqrt{ \mathrm{Var}_{\widehat{P}_{h}^{k}(\cdot|s,a,b)}(V)}\Big{|}\leq\sqrt{3}H\cdot \sqrt{\Big{\|}\widetilde{P}_{h}^{k}(\cdot|s,a,b)-\widehat{P}_{h}^{k}(\cdot|s,a,b)\Big{\|}_{1}}.\] (17)

_In addition, according to Lemma C.1, the left hand side can be further bounded by_

\[\Big{|}\sqrt{\mathrm{Var}_{\widetilde{P}_{h}^{k}(\cdot|s,a,b)}(V)}-\sqrt{ \mathrm{Var}_{\widehat{P}_{h}^{k}(\cdot|s,a,b)}(V)}\Big{|}\leq 3H\sqrt{\frac{SE_{ \epsilon,\beta}}{\widetilde{N}_{h}^{k}(s,a,b)}}.\] (18)

### Proof of UCB and LCB

For notational simplicity, for \(V\in\mathbb{R}^{S}\) such that \(\|V\|_{\infty}\leq H\), we define

\[\widetilde{V}_{h}^{k}V(s,a,b)=\mathrm{Var}_{\widetilde{P}_{h}^{k}(\cdot|s,a,b) }V(\cdot),\quad V_{h}V(s,a,b)=\mathrm{Var}_{P_{h}(\cdot|s,a,b)}V(\cdot).\] (19)

Then the bonus term \(\Gamma\) can be represented as below (\(C_{2}\) is the universal constant in Algorithm 1).

\[\Gamma_{h}^{k}(s,a,b)=C_{2}\sqrt{\frac{\widetilde{V}_{h}^{k}\left(\frac{ \overline{V}_{h+1}^{k}+\underline{V}_{h+1}^{k}}{2}\right)(s,a,b)\cdot\iota}{ \widetilde{N}_{h}^{k}(s,a,b)}}+\frac{C_{2}HSE_{\epsilon,\beta}\cdot\iota}{ \widetilde{N}_{h}^{k}(s,a,b)}+\frac{C_{2}H^{2}S_{\iota}}{\widetilde{N}_{h}^{k} (s,a,b)}.\] (20)

We state the following lemma that can bound the lower order term, which is helpful for proving UCB and LCB.

**Lemma C.6**.: _Suppose Assumption 3.1 and Lemma C.4 hold, then there exists a universal constant \(c_{1}>0\) such that: if function \(g(s)\) satisfies \(|g|(s)\leq(\overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k})(s)\), then it holds that:_

\[\begin{split}\Big{|}(\widetilde{P}_{h}^{k}-P_{h})g(s,a,b)\Big{|} \leq&\frac{c_{1}}{H}\min\Big{\{}P_{h}(\overline{V}_{h+1}^{k}- \underline{V}_{h+1}^{k})(s,a,b),\widetilde{P}_{h}^{k}(\overline{V}_{h+1}^{k} -\underline{V}_{h+1}^{k})(s,a,b)\Big{\}}\\ &+\frac{c_{1}H^{2}S_{\iota}}{\widetilde{N}_{h}^{k}(s,a,b)}+\frac{ c_{1}HSE_{\epsilon,\beta\iota}}{\widetilde{N}_{h}^{k}(s,a,b)}.\end{split}\] (21)Proof of Lemma c.6.: If \(|g|(s)\leq(\overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k})(s)\), it holds that:

\[\begin{split}&\left|(\widetilde{P}_{h}^{k}-P_{h})g(s,a,b)\right| \leq\sum_{s^{\prime}}\left|\left(\widetilde{P}_{h}^{k}-P_{h}\right)(s^{\prime} |s,a,b)\right|\cdot|g|(s^{\prime})\\ \leq&\sum_{s^{\prime}}\left|\left(\widetilde{P}_{h}^ {k}-P_{h}\right)(s^{\prime}|s,a,b)\right|\cdot\left(\overline{V}_{h+1}^{k}- \underline{V}_{h+1}^{k}\right)(s^{\prime})\\ \leq&\sum_{s^{\prime}}\left(2\sqrt{\frac{P_{h}(s^{ \prime}|s,a,b)_{\iota}}{\widetilde{N}_{h}^{k}(s,a,b)}}+\frac{2E_{\epsilon, \beta^{\iota}}}{\widetilde{N}_{h}^{k}(s,a,b)}\right)\cdot\left(\overline{V}_ {h+1}^{k}-\underline{V}_{h+1}^{k}\right)(s^{\prime})\\ \leq&\sum_{s^{\prime}}\left(\frac{P_{h}(s^{\prime}|s,a,b)}{H}+\frac{H\iota}{\widetilde{N}_{h}^{k}(s,a,b)}+\frac{2E_{\epsilon, \beta^{\iota}}}{\widetilde{N}_{h}^{k}(s,a,b)}\right)\cdot\left(\overline{V}_ {h+1}^{k}-\underline{V}_{h+1}^{k}\right)(s^{\prime})\\ \leq&\frac{c_{1}}{H}P_{h}(\overline{V}_{h+1}^{k}- \underline{V}_{h+1}^{k})(s,a,b)+\frac{c_{1}H^{2}S_{\epsilon}}{\widetilde{N}_ {h}^{k}(s,a,b)}+\frac{c_{1}HSE_{\epsilon,\beta^{\iota}}}{\widetilde{N}_{h}^{ k}(s,a,b)},\end{split}\] (22)

where the third inequality is because of Lemma c.2. The forth inequality results from AM-GM inequality. The last inequality holds for some universal constant \(c_{1}\).

The empirical part with the R.H.S to be \(\widetilde{P}_{h}^{k}\) can be proven using identical proof according to (13). 

Then we prove that the UCB and LCB functions are actually upper and lower bounds of the best responses. Recall that \(\pi^{k}\) is the (correlated) policy executed in the \(k\)-th episode and \((\mu^{k},\nu^{k})\) for both players are the marginal policies of \(\pi^{k}\). In other words, \(\mu^{k}_{h}(\cdot|s)=\sum_{b\in\mathcal{B}}\pi^{k}_{h}(\cdot,b|s)\) and \(\nu^{k}_{h}(\cdot|s)=\sum_{a\in\mathcal{A}}\pi^{k}_{h}(a,\cdot|s)\) for all \((h,s)\in[H]\times\mathcal{S}\).

**Lemma C.7**.: _Suppose Assumption 3.1 and Lemma C.4 hold, then there exist universal constants \(C_{1},C_{2}>0\) (in Algorithm 1) such that for all \((h,s,a,b,k)\in[H]\times\mathcal{S}\times\mathcal{A}\times\mathcal{B}\times[K]\), it holds that:_

\[\begin{cases}\overline{Q}_{h}^{k}(s,a,b)\geq Q_{h}^{\dagger,\nu^{k}}(s,a,b) \geq Q_{h}^{\mu^{k},\dagger}(s,a,b)\geq\underline{Q}_{h}^{k}(s,a,b),\\ \overline{V}_{h}^{k}(s)\geq{V}_{h}^{\dagger,\nu^{k}}(s)\geq{V}_{h}^{\mu^{k}, \dagger}(s)\geq\underline{V}_{h}^{k}(s).\end{cases}\] (23)

Proof of Lemma c.7.: We prove by backward induction. For each \(k\in[K]\), the conclusion is obvious for \(h=H+1\). Suppose UCB and LCB hold for Q value functions in the \((h+1)\)-th time step, we first prove the bounds for V functions in the \((h+1)\)-th step and then prove the bounds for Q functions in the \(h\)-th step. For all \(s\in\mathcal{S}\), it holds that

\[\begin{split}\overline{V}_{h+1}^{k}(s)=&\mathbb{E}_{\pi^ {k}_{h+1}}\overline{Q}_{h+1}^{k}(s)\\ \geq&\sup_{\mu}\mathbb{E}_{\mu,\nu^{k}_{h+1}}\overline{Q} _{h+1}^{k}(s)\\ \geq&\sup_{\mu}\mathbb{E}_{\mu,\nu^{k}_{h+1}}Q_{h+1}^{ \dagger,\nu^{k}}(s)\\ =&{V}_{h+1}^{\dagger,\nu^{k}}(s).\end{split}\] (24)

The conclusion \(\underline{V}_{h+1}^{k}(s)\leq{V}_{h+1}^{\mu^{k},\dagger}(s)\) can be proven by symmetry. Therefore, it holds that

\[\overline{V}_{h+1}^{k}(s)\geq{V}_{h+1}^{\dagger,\nu^{k}}(s)\geq{V}_{h+1}^{ \star}(s)\geq{V}_{h+1}^{\mu^{k},\dagger}(s)\geq\underline{V}_{h+1}^{k}(s).\] (25)

Next we prove the bounds for Q value functions at the \(h\)-th step. For all \((s,a,b)\), it holds that

\[\begin{split}&\left(\overline{Q}_{h}^{k}-Q_{h}^{\dagger,\nu^{k}} \right)(s,a,b)\geq\min\left\{\left(\widetilde{P}_{h}^{k}\overline{V}_{h+1}^{k}- P_{h}{V}_{h+1}^{\dagger,\nu^{k}}+\gamma_{h}^{k}+\Gamma_{h}^{k}\right)(s,a,b),0 \right\}\\ \geq&\min\left\{\left(\widetilde{P}_{h}^{k}{V}_{h+1} ^{\dagger,\nu^{k}}-P_{h}{V}_{h+1}^{\dagger,\nu^{k}}+\gamma_{h}^{k}+\Gamma_{h}^{k} \right)(s,a,b),0\right\}\\ =&\min\left\{\underbrace{\left(\widetilde{P}_{h}^{k}-P_ {h}\right)\left({V}_{h+1}^{\dagger,\nu^{k}}-{V}_{h+1}^{\star}\right)(s,a,b)}_ {\text{(i)}}+\underbrace{\left(\widetilde{P}_{h}^{k}-P_{h}\right){V}_{h+1}^{ \star}(s,a,b)}_{\text{(ii)}}+\gamma_{h}^{k}(s,a,b)+\Gamma_{h}^{k}(s,a,b),0 \right\}.\end{split}\] (26)The absolute value of term (i) can be bounded as below.

\[|(\mathrm{i})|\leq\frac{c_{1}}{H}\widetilde{P}_{h}^{k}(\overline{V}_{h+1}^{k}- \underline{V}_{h+1}^{k})(s,a,b)+\frac{c_{1}H^{2}S_{\epsilon}}{\widetilde{N}_{h} ^{k}(s,a,b)}+\frac{c_{1}HSE_{\epsilon,\beta}\iota}{\widetilde{N}_{h}^{k}(s,a,b )},\] (27)

for some universal constant \(c_{1}\) according to Lemma C.6.

The absolute value of term (ii) can be bounded as below.

\[|(\mathrm{ii})|\leq\sqrt{\frac{2\mathrm{Var}_{\widetilde{P}_{h}^{k}(\cdot|s,a,b)}V_{h+1}^{\star}(\cdot)\cdot\iota}{\widetilde{N}_{h}^{k}(s,a,b)}}+\frac{2 HSE_{\epsilon,\beta}\iota}{\widetilde{N}_{h}^{k}(s,a,b)}\leq\sqrt{\frac{2 \mathrm{Var}_{\widetilde{P}_{h}^{k}(\cdot|s,a,b)}V_{h+1}^{\star}(\cdot)\cdot \iota}{\widetilde{N}_{h}^{k}(s,a,b)}}+\frac{8HSE_{\epsilon,\beta}\iota}{ \widetilde{N}_{h}^{k}(s,a,b)},\] (28)

where the first inequality is because of Lemma C.3 while the second inequality holds due to Lemma C.5.

We further bound the term \(\mathrm{Var}_{\widetilde{P}_{h}^{k}(\cdot|s,a,b)}V_{h+1}^{\star}(\cdot)\) as below.

\[\begin{split}&\left|\widetilde{V}_{h}^{k}\left(\frac{\overline {V}_{h+1}^{k}+\underline{V}_{h+1}^{k}}{2}\right)-\widetilde{V}_{h}^{k}V_{h+1}^ {\star}(\cdot)\right|(s,a,b)\\ \leq&\left|\widetilde{P}_{h}^{k}\cdot\left(\overline {\underline{V}_{h+1}^{k}+\underline{V}_{h+1}^{k}}{2}\right)^{2}-\widetilde{P} _{h}^{k}\cdot\left(V_{h+1}^{\star}\right)^{2}\right|(s,a,b)+\left|\left| \widetilde{P}_{h}^{k}\cdot\left(\overline{\underline{V}_{h+1}^{k}+ \underline{V}_{h+1}^{k}}{2}\right)(s,a,b)\right|^{2}-\left[\widetilde{P}_{h}^ {k}V_{h+1}^{\star}(s,a,b)\right]^{2}\right|\\ \leq& 4H\widetilde{P}_{h}^{k}\cdot\left(\overline{V}_{h+1}^ {k}-\underline{V}_{h+1}^{k}\right)(s,a,b).\end{split}\] (29)

Therefore, the term (ii) can be further bounded as below.

\[\begin{split}&|(\mathrm{ii})|\leq\sqrt{\frac{2\mathrm{Var}_{ \widetilde{P}_{h}^{k}(\cdot|s,a,b)}V_{h+1}^{\star}(\cdot)\cdot\iota}{ \widetilde{N}_{h}^{k}(s,a,b)}}+\frac{8HSE_{\epsilon,\beta}\iota}{\widetilde{N} _{h}^{k}(s,a,b)}\\ \leq&\sqrt{\frac{2\iota\cdot\widetilde{V}_{h}^{k} \left(\overline{\underline{V}_{h+1}^{k}+\underline{V}_{h+1}^{k}}{2}\right)(s, a,b)+2\iota\cdot 4H\widetilde{P}_{h}^{k}\cdot\left(\overline{\underline{V}_{h+1}^{k}- \underline{V}_{h+1}^{k}}\right)(s,a,b)}{\widetilde{N}_{h}^{k}(s,a,b)}}+\frac{8 HSE_{\epsilon,\beta}\iota}{\widetilde{N}_{h}^{k}(s,a,b)}\\ \leq&\sqrt{\frac{2\widetilde{V}_{h}^{k}\left(\frac{ \overline{V}_{h+1}^{k}+\underline{V}_{h+1}^{k}}{2}\right)(s,a,b)\iota}{ \widetilde{N}_{h}^{k}(s,a,b)}}+\frac{\widetilde{P}_{h}^{k}\cdot\left( \overline{\underline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}}\right)(s,a,b)}{H} +\frac{2H^{2}\iota}{\widetilde{N}_{h}^{k}(s,a,b)}+\frac{8HSE_{\epsilon,\beta} \iota}{\widetilde{N}_{h}^{k}(s,a,b)},\end{split}\] (30)

where the second inequality results from (29) and the third inequality is due to AM-GM inequality. Combining the upper bounds of \(|(\mathrm{i})|\) and \(|(\mathrm{ii})|\), there exist universal constants \(C_{1},C_{2}>0\) such that

\[(\mathrm{i})+(\mathrm{ii})+\gamma_{h}^{k}(s,a,b)+\Gamma_{h}^{k}(s,a,b)\geq 0.\] (31)

The inequality implies that \(\left(\overline{Q}_{h}^{k}-Q_{h}^{\dagger,\nu^{k}}\right)(s,a,b)\geq 0\). By symmetry, we have \(\left(\underline{Q}_{h}^{k}-Q_{h}^{\mu^{k},\dagger}\right)(s,a,b)\leq 0\). As a result, it holds that \(\overline{Q}_{h}^{k}(s,a,b)\geq Q_{h}^{\dagger,\nu^{k}}(s,a,b)\geq Q_{h}^{ \star}(s,a,b)\geq Q_{h}^{\mu^{k},\dagger}(s,a,b)\geq\underline{Q}_{h}^{k}(s,a,b)\).

According to backward induction, the conclusion holds for all \((h,s,a,b,k)\). 

### Proof of Theorem 4.1

Given the UCB and LCB property, we are now ready to prove our main results. We first state the following lemma that controls the error of the empirical variance estimator.

**Lemma C.8**.: _Suppose Assumption 3.1 and Lemma C.4 hold, then there exists a universal constant \(c_{2}>0\) such that for all \((h,s,a,b,k)\in[H]\times\mathcal{S}\times\mathcal{A}\times\mathcal{B}\times[K]\), it holds that_

\[\begin{split}&\left|\widetilde{V}_{h}^{k}\left(\frac{\overline{V}_{h+1 }^{k}+\underline{V}_{h+1}^{k}}{2}\right)-V_{h}V_{h+1}^{\pi^{k}}\right|(s,a,b) \\ \leq& 4HP_{h}\left(\overline{V}_{h+1}^{k}-\underline{V}_{h +1}^{k}\right)(s,a,b)+\frac{c_{2}H^{2}SE_{\epsilon,\beta}}{\widetilde{N}_{h}^ {k}(s,a,b)}+c_{2}H^{2}\sqrt{\frac{S_{\mathcal{L}}}{\widetilde{N}_{h}^{k}(s,a, b)}}.\end{split}\] (32)

Proof of Lemma C.8.: According to Lemma C.7, \(\overline{V}_{h}^{k}(s)\geq V_{h}^{\pi^{k}}(s)\geq\underline{V}_{h}^{k}(s)\) always holds. Then it holds that

\[\begin{split}&\left|\widetilde{V}_{h}^{k}\left(\frac{\overline{V}_{h+1 }^{k}+\underline{V}_{h+1}^{k}}{2}\right)-V_{h}V_{h+1}^{\pi^{k}}\right|(s,a,b) \\ \leq&\left|\widetilde{P}_{h}^{k}\left(\frac{\overline {V}_{h+1}^{k}+\underline{V}_{h+1}^{k}}{2}\right)^{2}-P_{h}\left(V_{h+1}^{\pi^ {k}}\right)^{2}-\left[\widetilde{P}_{h}^{k}\left(\frac{\overline{V}_{h+1}^{k} +\underline{V}_{h+1}^{k}}{2}\right)\right]^{2}+\left(P_{h}V_{h+1}^{\pi^{k}} \right)^{2}\right|(s,a,b)\\ \leq&\left|\widetilde{P}_{h}^{k}\left(\overline{V}_{ h+1}^{k}\right)^{2}-P_{h}\left(\underline{V}_{h+1}^{k}\right)^{2}-\left( \widetilde{P}_{h}^{k}\underline{V}_{h+1}^{k}\right)^{2}+\left(P_{h}\overline{ V}_{h+1}^{k}\right)^{2}\right|(s,a,b)\\ \leq&\underbrace{\left|\left(\widetilde{P}_{h}^{k}- P_{h}\right)\left(\overline{V}_{h+1}^{k}\right)^{2}\right|(s,a,b)}_{(\text{i})}+ \underbrace{\left|P_{h}\left[\left(\overline{V}_{h+1}^{k}\right)^{2}-\left( \underline{V}_{h+1}^{k}\right)^{2}\right]\right|(s,a,b)}_{(\text{ii})}\\ &+\underbrace{\left|\left(\widetilde{P}_{h}^{k}\underline{V}_{h+ 1}^{k}\right)^{2}-\left(P_{h}\underline{V}_{h+1}^{k}\right)^{2}\right|(s,a,b) }_{(\text{iii})}+\underbrace{\left|\left(P_{h}\underline{V}_{h+1}^{k}\right)^{ 2}-\left(P_{h}\overline{V}_{h+1}^{k}\right)^{2}\right|(s,a,b)}_{(\text{iv})}. \end{split}\] (33)

The term (i) can be bounded as below due to Lemma C.1.

\[(\mathrm{i})\leq 2H^{2}\sqrt{\frac{S_{\mathcal{L}}}{\widetilde{N}_{h}^{k}(s,a, b)}}+\frac{2H^{2}SE_{\epsilon,\beta}}{\widetilde{N}_{h}^{k}(s,a,b)}.\] (34)

The term (ii) can be directly bounded as below.

\[(\mathrm{ii})\leq 2HP_{h}\left(\overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k} \right)(s,a,b).\] (35)

The term (iii) can be bounded as below due to Lemma C.1.

\[(\mathrm{iii})\leq 2H\left|\left(\widetilde{P}_{h}^{k}-P_{h}\right)\underline{V}_ {h+1}^{k}\right|(s,a,b)\leq 4H^{2}\sqrt{\frac{S_{\mathcal{L}}}{\widetilde{N}_{h}^{k}(s,a,b)}}+\frac{4H^{2}SE_{\epsilon,\beta}}{\widetilde{N}_{h}^{k}(s,a,b)}.\] (36)

The term (iv) can be directly bounded as below.

\[(\mathrm{iv})\leq 2HP_{h}\left(\overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k} \right)(s,a,b).\] (37)

The conclusion holds according the upper bounds of term (i), (ii), (iii) and (iv). 

Finally we prove the regret bound of Algorithm 1.

Proof of Theorem 4.1.: Our proof base on Assumption 3.1 and Lemma C.4. We define the following notations.

\[\begin{cases}\Delta_{h}^{k}=\left(\overline{V}_{h}^{k}-\underline{V}_{h}^{k} \right)(s_{h}^{k}),\\ \zeta_{h}^{k}=\Delta_{h}^{k}-\left(\overline{Q}_{h}^{k}-\underline{Q}_{h}^{k} \right)(s_{h}^{k},a_{h}^{k},b_{h}^{k}),\\ \xi_{h}^{k}=P_{h}\left(\overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\right)(s _{h}^{k},a_{h}^{k},b_{h}^{k})-\Delta_{h+1}^{k}.\end{cases}\] (38)Then it holds that \(\zeta_{h}^{k}\) and \(\xi_{h}^{k}\) are martingale differences bounded by \(H\). In addition, we use the following abbreviations for notational simplicity:

\[\left\{\begin{array}{l}\gamma_{h}^{k}=\gamma_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h} ^{k}),\\ \Gamma_{h}^{k}=\Gamma_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h}^{k}),\\ N_{h}^{k}=N_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h}^{k}),\\ \widetilde{N}_{h}^{k}=\widetilde{N}_{h}^{k}(s_{h}^{k},a_{h}^{k},b_{h}^{k}). \end{array}\right.\] (39)

Then we have the following analysis about \(\Delta_{h}^{k}\).

\[\Delta_{h}^{k}=\zeta_{h}^{k}+\left(\overline{Q}_{h}^{k}-\underline {Q}_{h}^{k}\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})\] \[\leq \zeta_{h}^{k}+2\gamma_{h}^{k}+2\Gamma_{h}^{k}+\widetilde{P}_{h} ^{k}\left(\overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\right)(s_{h}^{k},a_ {h}^{k},b_{h}^{k})\] \[\leq \zeta_{h}^{k}+2\Gamma_{h}^{k}+\left(1+\frac{2C_{1}}{H}\right) \cdot\left[\left(1+\frac{c_{1}}{H}\right)\cdot P_{h}\left(\overline{V}_{h+1}^ {k}-\underline{V}_{h+1}^{k}\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})+\frac{c_{1} H^{2}S_{t}}{\widetilde{N}_{h}^{k}}+\frac{c_{1}HSE_{\epsilon,\beta}t}{ \widetilde{N}_{h}^{k}}\right]\] \[\leq \zeta_{h}^{k}+\left(1+\frac{c_{3}}{H}\right)\cdot P_{h}\left( \overline{V}_{h+1}^{k}-\underline{V}_{h+1}^{k}\right)(s_{h}^{k},a_{h}^{k},b_{ h}^{k})+\frac{c_{3}H^{2}S_{t}}{\widetilde{N}_{h}^{k}}+\frac{c_{3}HSE_{\epsilon, \beta}t}{\widetilde{N}_{h}^{k}}\] \[+c_{3}\underbrace{\sqrt{\frac{\widetilde{V}_{h}^{k}\left( \frac{\overline{V}_{h+1}^{k}+\underline{V}_{h+1}^{k}}{2}\right)(s_{h}^{k},a_ {h}^{k},b_{h}^{k})_{\iota}}{\widetilde{N}_{h}^{k}}}_{(\text{i})}},\] (40)

where the first inequality holds because of the definition of \(\overline{Q}\) and \(\underline{Q}\). The second inequality holds due to the definition of \(\gamma_{h}^{k}\) and Lemma C.6. The last inequality holds for some universal constant \(c_{3}>0\). The term (i) can be further bounded as below according to Lemma C.8 and AM-GM inequality.

\[\text{(i)}\leq \sqrt{\frac{V_{h}V_{h+1}^{\pi}(s_{h}^{k},a_{h}^{k},b_{h}^{k})_{ \iota}}{\widetilde{N}_{h}^{k}}}+\sqrt{\frac{4HP_{h}\left(\overline{V}_{h+1}^{k }-\underline{V}_{h+1}^{k}\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})_{\iota}}{ \widetilde{N}_{h}^{k}}}+\frac{H\sqrt{c_{2}SE_{\epsilon,\beta}t}}{\widetilde{N} _{h}^{k}}+c_{2}\sqrt{\frac{\iota}{\widetilde{N}_{h}^{k}}}+\frac{H^{2}\iota \sqrt{c_{2}}\underline{S}}{\widetilde{N}_{h}^{k}}\] \[\leq \sqrt{\frac{V_{h}V_{h+1}^{\pi^{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k})_ {\iota}}{\widetilde{N}_{h}^{k}}}+\frac{c_{4}P_{h}\left(\overline{V}_{h+1}^{k}- \underline{V}_{h+1}^{k}\right)(s_{h}^{k},a_{h}^{k},b_{h}^{k})}{H}+\frac{c_{4} H^{2}\sqrt{S}_{t}}{\widetilde{N}_{h}^{k}}+\frac{c_{4}H\sqrt{SE_{\epsilon, \beta}t}}{\widetilde{N}_{h}^{k}}+c_{4}\sqrt{\frac{\iota}{\widetilde{N}_{h}^{k }}},\] (41)

where the first inequality results from Lemma C.8 and AM-GM inequality on the last term of (32).

The second inequality holds for some universal constant \(c_{4}>0\) according to AM-GM inequality.

Plugging in the upper bound of term (i), for some universal constant \(c_{5}>0\), it holds that:

\[\Delta_{h}^{k}\leq\zeta_{h}^{k}+\left(1+\frac{c_{5}}{H}\right)\xi_{h}^{k}+ \left(1+\frac{c_{5}}{H}\right)\Delta_{h+1}^{k}+c_{5}\sqrt{\frac{V_{h}V_{h+1}^ {\pi^{k}}(s_{h}^{k},a_{h}^{k},b_{h}^{k})_{\iota}}{\widetilde{N}_{h}^{k}}}+c_ {5}\sqrt{\frac{\iota}{\widetilde{N}_{h}^{k}}}+\frac{c_{5}H^{2}S_{t}}{\widetilde {N}_{h}^{k}}+\frac{c_{5}HSE_{\epsilon,\beta}t}{\widetilde{N}_{h}^{k}}.\] (42)

Summing \(\Delta_{1}^{k}\) over \(k\in[K]\), we have for some universal constant \(c_{6}>0\), it holds that:

\[\sum_{k=1}^{K}\Delta_{1}^{k}\leq \underbrace{\sum_{k=1}^{K}\sum_{h=1}^{H}\left(1+\frac{c_{5}}{H} \right)^{h-1}\zeta_{h}^{k}}_{(\text{ii})}+\underbrace{\sum_{k=1}^{K}\sum_{h=1} ^{H}\left(1+\frac{c_{5}}{H}\right)^{h}\xi_{h}^{k}}_{(\text{iii})}+c_{6} \underbrace{\sum_{k=1}^{K}\sum_{h=1}^{H}\sqrt{\frac{V_{h}V_{h+1}^{\pi^{k}}(s_{h}^ {k},a_{h}^{k},b_{h}^{k})_{\iota}}{\widetilde{N}_{h}^{k}}}}_{(\text{iv})}\] \[+c_{6}\underbrace{\sum_{k=1}^{K}\sum_{h=1}^{H}\sqrt{\frac{\iota}{ \widetilde{N}_{h}^{k}}}}_{(\text{v})}+c_{6}\underbrace{\sum_{k=1}^{K}\sum_{h=1} ^{H}\frac{H^{2}S_{t}+HSE_{\epsilon,\beta}t}{\widetilde{N}_{h}^{k}}}_{(\text{vi})}.\] (43)The term (ii) and term (iii) can be bounded by Azuma-Hoeffding inequality. With probability \(1-\frac{2\beta}{9}\), it holds that

\[|(\mathrm{ii})|\leq O\left(\sqrt{H^{3}K\iota}\right),\quad|(\mathrm{iii})|\leq O \left(\sqrt{H^{3}K\iota}\right).\] (44)

The main term (iv) is bounded as below.

\[(\mathrm{iv})\leq \sum_{k=1}^{K}\sum_{h=1}^{H}\sqrt{\frac{V_{h}V_{h+1}^{\pi^{k}}(s_{ h}^{k},a_{h}^{k},b_{h}^{k})_{\iota}}{N_{h}^{k}}}\] (45) \[\leq \sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}V_{h+1}(s_{h}^{k},a_{h}^{k},b_{h }^{k})_{\iota}\cdot\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{N_{h}^{k}}}\] \[\leq \sqrt{O\left(H^{2}K+H^{3}\iota\right)\iota\cdot O(HSAB\iota)}\] \[= \widetilde{O}\left(\sqrt{H^{3}SABK}+H^{2}\sqrt{SAB}\right).\]

The first inequality is because \(\widetilde{N}_{h}^{k}\geq N_{h}^{k}\) (Assumption 3.1). The second inequality holds due to Cauchy-Schwarz inequality. The third inequality holds with probability \(1-\frac{\beta}{9}\) because of Law of total variance and standard concentration inequalities (for details please refer to Lemma 8 of Azar et al. (2017)).

The term (v) is bounded as below due to pigeon-hole principle.

\[(\mathrm{v})\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\sqrt{\frac{\iota}{N_{h}^{k}}}\leq O (\sqrt{H^{2}SABK\iota}),\] (46)

where the first inequality is because \(\widetilde{N}_{h}^{k}\geq N_{h}^{k}\) (Assumption 3.1). The last one results from pigeon-hole principle.

The term (vi) can be bounded as below.

\[(\mathrm{vi})\leq\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{H^{2}S\iota+HSE\iota}{N_{h} ^{k}}\leq O(H^{3}S^{2}AB\iota^{2})+O(H^{2}S^{2}AB\iota^{2}).\] (47)

Combining the upper bounds for term \(|(\mathrm{ii})|\), \(|(\mathrm{iii})|\), (iv), (v) and (vi). The regret of Algorithm 1 can be bounded as below.

\[\begin{split}\mathrm{Regret}(K)=&\sum_{k=1}^{K} \left[V_{1}^{\dagger,\nu^{k}}(s_{1})-V_{1}^{\mu^{k},\dagger}(s_{1})\right] \leq\sum_{k=1}^{K}\left[\overline{V}_{1}^{k}(s_{1})-\underline{V}_{1}^{k}(s_ {1})\right]\\ =&\sum_{k=1}^{K}\Delta_{1}^{k}\leq\widetilde{O} \left(\sqrt{H^{2}SABT}+H^{3}S^{2}AB+H^{2}S^{2}ABE_{\epsilon,\beta}\right), \end{split}\] (48)

where \(T=HK\) is the number of steps.

The failure probability is bounded by \(\beta\) (\(\frac{\beta}{3}\) for Assumption 3.1, \(\frac{\beta}{3}\) for Lemma C.4, \(\frac{\beta}{3}\) for terms (ii), (iii) and (iv)). The proof of Theorem 4.1 is complete. 

### Proof of Theorem 4.2

In this part, we provide a proof of the PAC guarantee: Theorem 4.2. The proof directly follows from the proof of the regret bound (Theorem 4.1).

Proof of Theorem 4.2.: Recall that we choose \(\pi^{\text{out}}=\pi^{\overline{k}}\) such that \(\overline{k}=\operatorname*{argmin}_{k}\left(\overline{V}_{1}^{k}-\underline{ V}_{1}^{k}\right)(s_{1})\). Therefore, we have

\[V_{1}^{\dagger,\nu^{\text{out}}}(s_{1})-V_{1}^{\mu^{\text{out}},\dagger}(s_{1} )\leq\overline{V}_{1}^{\overline{k}}(s_{1})-\underline{V}_{1}^{\overline{k}}(s _{1})\leq\frac{1}{K}\widetilde{O}\left(\sqrt{H^{3}SABK}+H^{2}S^{2}ABE_{\epsilon, \beta}\right),\] (49)

if ignoring the lower order term of the regret bound.

Therefore, choosing \(K\geq\widetilde{O}\left(\frac{H^{3}SAB}{\alpha^{2}}+\min\left\{K^{\prime}| \frac{H^{2}S^{2}ABE_{\epsilon,\beta}}{K^{\prime}}\leq\alpha\right\}\right)\) bounds the R.H.S by \(\alpha\).

Missing proof in Section 5

In this section, we provide the missing proof for results in Section 5. Recall that \(N_{h}^{k}\) is the real visitation count, \(\widehat{N}_{h}^{k}\) is the intermediate noisy count calculated by both Privatizers and \(\widehat{N}_{h}^{k}\) is the final private count after the post-processing step. Note that most of the proof here are generalizations of Appendix D in Qiao and Wang (2023) to the multi-player setting, and here we state the proof for completeness.

Proof of Lemma 5.1.: Due to Theorem 3.5 of Chan et al. (2011) and Lemma 34 of Hsu et al. (2014), the release of \(\{\widehat{N}_{h}^{k}(s,a,b)\}_{(h,s,a,b,k)}\) satisfies \(\frac{\epsilon}{2}\)-DP. Similarly, the release of \(\{\widehat{N}_{h}^{k}(s,a,b,s^{\prime})\}_{(h,s,a,b,s^{\prime})}\) also satisfies \(\frac{\epsilon}{2}\)-DP. Therefore, the release of the following private counters \(\{\widehat{N}_{h}^{k}(s,a,b)\}_{(h,s,a,b,k)}\), \(\{\widehat{N}_{h}^{k}(s,a,b,s^{\prime})\}_{(h,s,a,b,s^{\prime},k)}\) satisfy \(\epsilon\)-DP. Due to post-processing (Lemma 2.3 of Bun and Steinke (2016)), the release of both private counts \(\{\widetilde{N}_{h}^{k}(s,a,b)\}_{(h,s,a,b,k)}\) and \(\{\widetilde{N}_{h}^{k}(s,a,b,s^{\prime})\}_{(h,s,a,b,s^{\prime},k)}\) also satisfies \(\epsilon\)-DP. Then it holds that the release of all \(\pi^{k}\) is \(\epsilon\)-DP according to post-processing. Finally, the guarantee of \(\epsilon\)-JDP results from Billboard Lemma (Lemma 9 of Hsu et al. (2014)).

For utility analysis, because of Theorem 3.6 of Chan et al. (2011), our choice \(\epsilon^{\prime}=\frac{\epsilon}{2H\log K}\) in Binary Mechanism and a union bound, with probability \(1-\frac{\beta}{3}\), for all \((h,s,a,b,s^{\prime},k)\),

\[\begin{split}\left|\widehat{N}_{h}^{k}(s,a,b,s^{\prime})-N_{h}^{ k}(s,a,b,s^{\prime})\right|&\leq O\left(\frac{H}{\epsilon}\log(HSABK/\beta)^{2}\right),\\ \left|\widehat{N}_{h}^{k}(s,a,b)-N_{h}^{k}(s,a,b)\right|& \leq O\left(\frac{H}{\epsilon}\log(HSABK/\beta)^{2}\right).\end{split}\] (50)

Together with Lemma 5.8, the Central Privatizer satisfies Assumption 3.1 with \(E_{\epsilon,\beta}=\widetilde{O}\left(\frac{H}{\epsilon}\right)\). 

Proof of Theorem 5.2.: The proof directly results from plugging \(E_{\epsilon,\beta}=\widetilde{O}\left(\frac{H}{\epsilon}\right)\) into Theorem 4.1 and Theorem 4.2. 

Proof of Theorem 5.3.: The first term results from the non-private regret lower bound \(\Omega(\sqrt{H^{2}S(A+B)T})\)(Bai and Jin, 2020). The second term is a direct adaptation of the \(\Omega(HSA/\epsilon)\) lower bound for any algorithms with \(\epsilon\)-JDP guarantee under single-agent MDP (Vietri et al., 2020). 

Proof of Lemma 5.4.: The privacy guarantee directly results from properties of Laplace Mechanism and composition of DP (Dwork et al., 2014).

For utility analysis, because of Corollary 12.4 of Dwork et al. (2014) and a union bound, with probability \(1-\frac{\beta}{3}\), for all possible \((h,s,a,b,s^{\prime},k)\),

\[\begin{split}\left|\widehat{N}_{h}^{k}(s,a,b,s^{\prime})-N_{h}^{ k}(s,a,b,s^{\prime})\right|&\leq O\left(\frac{H}{\epsilon}\sqrt{K\log(HSABK/\beta)}\right),\\ \left|\widehat{N}_{h}^{k}(s,a,b)-N_{h}^{k}(s,a,b)\right|& \leq O\left(\frac{H}{\epsilon}\sqrt{K\log(HSABK/\beta)}\right).\end{split}\] (51)

Together with Lemma 5.8, the Local Privatizer satisfies Assumption 3.1 with \(E_{\epsilon,\beta}=\widetilde{O}\left(\frac{H}{\epsilon}\sqrt{K}\right)\). 

Proof of Theorem 5.5.: The proof directly results from plugging \(E_{\epsilon,\beta}=\widetilde{O}\left(\frac{H}{\epsilon}\sqrt{K}\right)\) into Theorem 4.1 and Theorem 4.2. 

Proof of Theorem 5.6.: The first term results from the non-private regret lower bound \(\Omega(\sqrt{H^{2}S(A+B)T})\)(Bai and Jin, 2020). The second term is a direct adaptation of the \(\Omega(\sqrt{HSAT}/\epsilon)\) lower bound for any algorithms with \(\epsilon\)-LDP guarantee under single-agent MDP (Garcelon et al., 2021).

Proof of Lemma 5.8.: For clarity, we denote the solution of (7) by \(\bar{N}_{h}^{k}\) and therefore \(\widetilde{N}_{h}^{k}(s,a,b,s^{\prime})=\bar{N}_{h}^{k}(s,a,b,s^{\prime})+\frac {E_{e,\beta}}{2S},\widetilde{N}_{h}^{k}(s,a,b)=\bar{N}_{h}^{k}(s,a,b)+\frac{E_{e,\beta}}{2}\).

When the condition (two inequalities) in Lemma 5.8 holds, the original counts \(\{N_{h}^{k}(s,a,b,s^{\prime})\}_{s^{\prime}\in\mathcal{S}}\) is a feasible solution to the optimization problem, which means that

\[\max_{s^{\prime}}\left|\bar{N}_{h}^{k}(s,a,b,s^{\prime})-\widehat{N}_{h}^{k}( s,a,b,s^{\prime})\right|\leq\max_{s^{\prime}}\left|N_{h}^{k}(s,a,b,s^{\prime})- \widehat{N}_{h}^{k}(s,a,b,s^{\prime})\right|\leq\frac{E_{e,\beta}}{4}.\]

Combining with the condition in Lemma 5.8 with respect to \(\widehat{N}_{h}^{k}(s,a,b,s^{\prime})\), it holds that

\[\left|\bar{N}_{h}^{k}(s,a,b,s^{\prime})-N_{h}^{k}(s,a,b,s^{\prime})\right| \leq\left|\bar{N}_{h}^{k}(s,a,b,s^{\prime})-\widehat{N}_{h}^{k}(s,a,b,s^{ \prime})\right|+\left|\widehat{N}_{h}^{k}(s,a,b,s^{\prime})-N_{h}^{k}(s,a,b,s ^{\prime})\right|\leq\frac{E_{e,\beta}}{2}.\]

Since \(\widetilde{N}_{h}^{k}(s,a,b,s^{\prime})=\bar{N}_{h}^{k}(s,a,b,s^{\prime})+ \frac{E_{e,\beta}}{2S}\) and \(\bar{N}_{h}^{k}(s,a,b,s^{\prime})\geq 0\), we have

\[\widetilde{N}_{h}^{k}(s,a,b,s^{\prime})>0,\quad\left|\widehat{N}_{h}^{k}(s,a, b,s^{\prime})-N_{h}^{k}(s,a,b,s^{\prime})\right|\leq E_{e,\beta}.\] (52)

For \(\bar{N}_{h}^{k}(s,a,b)\), according to the constraints in the optimization problem (7), it holds that

\[\left|\bar{N}_{h}^{k}(s,a,b)-\widehat{N}_{h}^{k}(s,a,b)\right|\leq\frac{E_{e,\beta}}{4}.\]

Combining with the condition in Lemma 5.8 with respect to \(\widehat{N}_{h}^{k}(s,a,b)\), it holds that

\[\left|\bar{N}_{h}^{k}(s,a,b)-N_{h}^{k}(s,a,b)\right|\leq\left|\bar{N}_{h}^{k} (s,a,b)-\widehat{N}_{h}^{k}(s,a,b)\right|+\left|\widehat{N}_{h}^{k}(s,a,b)-N_ {h}^{k}(s,a,b)\right|\leq\frac{E_{e,\beta}}{2}.\]

Since \(\widetilde{N}_{h}^{k}(s,a,b)=\bar{N}_{h}^{k}(s,a,b)+\frac{E_{e,\beta}}{2}\), we have

\[N_{h}^{k}(s,a,b)\leq\widetilde{N}_{h}^{k}(s,a,b)\leq N_{h}^{k}(s,a,b)+E_{e, \beta}.\] (53)

According to the last line of the optimization problem (7), we have \(\bar{N}_{h}^{k}(s,a,b)=\sum_{s^{\prime}\in\mathcal{S}}\bar{N}_{h}^{k}(s,a,b,s ^{\prime})\) and therefore,

\[\widetilde{N}_{h}^{k}(s,a,b)=\sum_{s^{\prime}\in\mathcal{S}}\widetilde{N}_{h} ^{k}(s,a,b,s^{\prime}).\] (54)

The proof is complete by combining (52), (53) and (54).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract claims that this paper is about differentially private reinforcement learning with self-play. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss in Section 5 that the additional cost due to DP does not have optimal dependence on \(H,S,A,B\). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The paper provides the full set of assumptions and a complete proof. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: This is a theory paper and we do not conduct experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This is a theory paper and we do not conduct experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This is a theory paper and we do not conduct experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This is a theory paper and we do not conduct experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This is a theory paper and we do not conduct experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: This is a theory paper regarding privacy protection, which does not have negative societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: There is no risk of misuse of the algorithm in this paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This is a theory paper and we do not use other assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not introduce any new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This is a theory paper and we do not conduct any experiments. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.