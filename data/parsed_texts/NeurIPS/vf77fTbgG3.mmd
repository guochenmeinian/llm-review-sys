# Structured Voronoi Sampling

Afra Amini\({}^{1}\) Li Du\({}^{2}\) Ryan Cotterell\({}^{1}\)

\({}^{1}\)ETH Zurich \({}^{2}\)Johns Hopkins University

{afra.amini, ryan.cotterell}@inf.ethz.ch

leodu@cs.jhu.edu

###### Abstract

Gradient-based sampling algorithms have demonstrated their effectiveness in text generation, especially in the context of controlled text generation. However, there exists a lack of theoretically grounded and principled approaches for this task. In this paper, we take an important step toward building a principled approach for sampling from language models with gradient-based methods. We use discrete distributions given by language models to define densities and develop an algorithm based on Hamiltonian Monte Carlo to sample from them. We name our gradient-based technique Structured Voronoi Sampling (svs). In an experimental setup where the reference distribution is known, we show that the empirical distribution of svs samples is closer to the reference distribution compared to alternative sampling schemes. Furthermore, in a controlled generation task, svs is able to generate fluent and diverse samples while following the control targets significantly better than other methods.

https://github.com/AfraAmini/svs

## 1 Introduction

Gradient-based sampling algorithms such as Hamiltonian Monte Carlo [hmc; 32] and Langevin dynamics [46] are widely used in Bayesian inference due to their efficiency in drawing samples from high-dimensional space [4]. Such algorithms construct a Markov Chain that has the desired distribution as its stationary distribution and use the gradient information of this distribution to efficiently navigate the state space. Additionally, gradient-based sampling schemes have recently been deployed in computer vision to generate high-quality images from state-of-the-art models [13; 43], and are a popular choice for tasks such as image synthesis [5] and image-to-image translation [40].

In natural language processing, there have been several attempts to apply gradient-based sampling techniques to sampling text from neural language models [21; 23; 38]. The motivation behind this approach to text generation is to sample from energy-based probabilistic models, where the normalization factor is not tractable. One such case is in controlled text generation, where energy functions are usually defined as a linear combination of LM probabilities and the probability of satisfying a set of predefined constraints [21; 23; 38]. In contrast to computer vision, however, applying gradient-based sampling schemes to text generation is nuanced as text, in contrast to images, is discrete.

Upon closer inspection, none of the proposed algorithms actually defines a valid Markov Chain Monte Carlo [MCMC; 12; 29] scheme that will draw samples from the model in the limit. For instance, Qin et al. [38] relax the language model from a distribution over strings to a distribution over logits. While the relaxation does transform the language model into a continuous distribution, it introduces bias. Kumar et al. [MuCoLA; 21] take a different approach. They derive a constrained gradient-basedsampler where the constraint is enforced through a projection. However, the projection invalidates the MCMC procedures, leading to an algorithm without guarantees.1

Footnote 1: In fact, a similar projection step is often applied in computer vision applications, which motivated Lou and Ermon [26] to develop a principled approach that avoids projection.

We derive a principled Hamiltonian Monte Carlo scheme for generating text from language models, which we call a structured Voronoi sampler. Our scheme consists of two steps. First, we give a recipe for encoding discrete distributions as densities over \(\mathbb{R}^{d}\); we term the resulting encoding Voronoi measures.2 Second, we derive a refractive Hamiltonian Monte Carlo algorithm [30] for sampling from an arbitrary Voronoi measure. In our theoretical analysis, we show that, despite the presence of discontinuities, we are able to give proof that our sampler satisfies the detailed balance condition and, thus, is a correct MCMC scheme.

Footnote 2: We call the measures structured Voronoi samplers when the state face is larger and factored.

To empirically evaluate the performance of structured Voronoi sampling, we begin by applying it to a toy example where the exact reference probability distribution is known. We compare the empirical distribution of drawn samples with the reference distribution and show Voronoi sampler's distribution is closer to the reference distribution than MuCoLa or unconstrained hmc. Furthermore, we use our sampling scheme for controlled generation, where the goal is to use GPT-2 to generate restaurant reviews for a target type of food, e.g., Italian, Fast food, or Japanese, and separately to generate text with a positive sentiment. We find that structured Voronoi sampling outperforms fudge[48], MuCoLa, and Langevin Dynamics algorithms in terms of adhering to the control target. Additionally, the samples generated by structured Voronoi sampling are comparably fluent and diverse to those produced by the other methods.

## 2 Language Models

Let \(\Sigma\) be an alphabet, a finite, non-empty set. By \(\Sigma^{*}\stackrel{{\text{\tiny def}}}{{=}}\bigcup_{n=0}^{ \infty}\Sigma^{n}\), we denote the Kleene closure of \(\Sigma\).3 A probability distribution over \(\Sigma^{*}\) is called a **language model** (**LM**). The elements of \(\Sigma\) may be characters, subword pieces, or words; the choice lies with the modeler. Language models can be factored autoregressively by means of the chain rule of probability, i.e., for any string \(\bm{w}=w_{1}\cdots w_{N}\in\Sigma^{*}\), we can write

Footnote 3: We define \(\Sigma^{0}\stackrel{{\text{\tiny def}}}{{=}}\{\varepsilon\}\).

\[p(\bm{w})=p(\text{\sc eos}\mid\bm{w})\prod_{n=1}^{N}p(w_{n}\mid\bm{w}_{<n}),\] (1)

where \(\text{\sc eos}\not\in\Sigma\) is a distinguished end-of-sequence token and \(\bm{w}_{<n}\) is the prefix of length \((n-1)\) of the string \(\bm{w}\). We define \(\overline{\Sigma}\stackrel{{\text{\tiny def}}}{{=}}\Sigma \cup\{\text{\sc eos}\}\), and require the conditional distributions \(p(\cdot\mid\bm{w}_{<n})\) to be defined over \(\overline{\Sigma}\). While all language models can be factored autoregressively, not all conditionals \(p(\cdot\mid\bm{w}_{<n})\) can be assembled into a language model. In some cases, probability mass must be placed on infinite sequences [6]. In this work, we assume working with **tight** language models, i.e., that they indeed define valid probability distributions over \(\Sigma^{*}\).

### Language Modeling with Embeddings

Most neural language models make use of embeddings. Moreover, in most language models, the weights are shared between the language model head and the embedding layer. Such an embedding-based language model is defined as follows

\[p(w_{n}\mid\bm{w}_{<n})\stackrel{{\text{\tiny def}}}{{=}}\frac{ \exp\bm{v}_{w_{n}}\cdot\text{\sf enc}(\bm{w}_{<n})}{\sum_{w\in\Sigma}\exp\bm{ v}_{w}\cdot\text{\sf enc}(\bm{w}_{<n})},\] (2)

where \(\bm{v}_{w}\in\mathbb{R}^{d}\) is the embedding of \(w\) and \(\text{\sf enc}:\Sigma^{*}\to\mathbb{R}^{d}\) is a real-valued encoding of the context. Notably, the context embedding \(\text{\sf enc}(\bm{w}_{<n})\in\mathbb{R}^{d}\) is obtained by inputting the context \(\bm{w}_{<n}\) into the language model, converting it to embeddings \(\bm{v}_{\bm{w}_{<n}}\), passing it through neural network layers, and extracting the encoding from the model at position \(n-1\).

Interestingly, one can lift an embedding-based language model to be a distribution over the set of **base embeddings**: \(\mathcal{B}=\{\mathbf{v}_{w}:w\in\overline{\Sigma}\}\). Specifically, we can associate a sequence of embeddings \(\mathbf{V}=[\mathbf{v}_{1},\dots,\mathbf{v}_{N}]\) to any string \(\bm{w}=w_{1}\cdots w_{N}\). Substituting each \(w\) with the corresponding \(\mathbf{v}_{w}\), we can rewrite Eq. (1) as

\[p(\mathbf{V})=p(\mathbf{v}_{\textsc{eos}}\mid\mathbf{V})\prod_{n=1}^{N}p( \mathbf{v}_{n}\mid\mathbf{V}_{<n}).\] (3)

This transformation encodes a language model as a distribution over real-valued vectors. However, \(p(\mathbf{V})\) only places a positive probability on a countable set, and is zero everywhere else.

### Controlled Language Modeling with Embeddings

In a controlled generation task, we are interested in a subset of strings \(\bm{w}\) that have a target property \(t\). Therefore, we want to model and sample from a conditional distribution \(p(\bm{w}\mid t)\), e.g., sample a sentence given a topic. Following Bayes' rule, one can write \(p(\bm{w}\mid t)\propto p(t\mid\bm{w})\,p(\bm{w})\). Previous papers model \(p(t\mid\bm{w})\) with an embedding-augmented classifier. Such a classifier receives embeddings \(\mathbf{V}\) associated with \(\bm{w}\) and predicts the probability of the target \(t\). Notably, if the classifier and the LM share the same base embeddings \(\mathcal{B}\), the controlled LM can also be lifted as a distribution over the base embeddings

\[p(\mathbf{V}\mid t)=\frac{1}{Z_{t}}\,p(t\mid\mathbf{V})\,p(\mathbf{V}),\] (4)

where \(Z_{t}=\sum_{\mathbf{V}}p(t\mid\mathbf{V})\,p(\mathbf{V})\) is an intractable normalization factor, that sums over the embeddings of all possible strings.4 Identically to \(p(\mathbf{V})\), \(p(\mathbf{V}\mid t)\) only places a positive probability on a countable set.

Footnote 4: We will discuss in ยง5.2 how gradient-based sampling can help to sample from this distribution without the need to compute \(Z_{t}\).

## 3 Voronoi Measures

In this section, we demonstrate how to encode an embedding-based language model as a density that places positive probability on a set with a measure greater than zero. Such encoding allows us to derive a principled gradient-based sampling approach to generate samples in SS6. We start with some definitions.

**Definition 1**.: _An **embedding-augmented** probability distribution over the first \(M\) positive integers \([M]\) is an array \(\bm{p}=[p_{1},\dots,p_{M}]\) such that \(p_{m}\geq 0\) and \(\sum_{m=1}^{M}p_{m}=1\) where we assume that there is a real-valued embedding \(\{\mathbf{v}_{m}\}_{m=1}^{M}\subset\mathbb{R}^{d}\) associated with each \(m\in[M]\)._

Embedding-augmented distributions can be viewed as densities over \(\mathbb{R}^{d}\) using the following simple encoding

\[p(\mathbf{x})=\begin{cases}p_{m},&\textbf{if }\mathbf{x}=\mathbf{v}_{m}\\ 0,&\textbf{otherwise}.\end{cases}\] (5)

Eq. (5), however, yields a density that is \(0\) almost everywhere (with respect to the standard Lebesgue measure) and its gradient with respect to \(p_{m}\) is also zero almost everywhere. Thus, Eq. (5) is not amenable to gradient-based sampling, and to derive a meaningful gradient-based sampling we require a more nuanced encoding.

To provide such an encoding, we introduce the **Voronoi measure**. Given an embedding-augmented distribution \(\bm{p}=[p_{1},\dots,p_{M}]\) with embeddings \(\{\mathbf{v}_{m}\}_{m=1}^{M}\), and a compact set \(\mathcal{K}\subset\mathbb{R}^{d}\) that covers the embeddings, i.e., \(\{\mathbf{v}_{m}\}_{m=1}^{M}\subset\mathcal{K}\), we define the **Voronoi cell** for the \(m^{\text{th}}\) item with respect to the compact set \(\mathcal{K}\) as follows

\[C_{m}=\Big{\{}\mathbf{x}:\mathbf{x}\in\mathcal{K},||\mathbf{x}-\mathbf{v}_{m} ||_{2}^{2}\leq||\mathbf{x}-\mathbf{v}_{m^{\prime}}||_{2}^{2},\forall m^{ \prime}\neq m\Big{\}}.\] (6)

Now, using the definition of a Voronoi cell \(C_{m}\) given in Eq. (6), we can define a density that is _not_ zero almost everywhere as follows. The strategy is to spread out the probability mass \(p_{m}\) over the entirety of the set \(C_{m}\). To do so, we assume access to a set of **base measures**\(\{\mu_{m}\}_{m=1}^{M}\) that give us a reference for how to judge the probability mass in each \(C_{m}\). We make this encoding formal in the following definition.

**Definition 2**.: _Let \(\bm{p}=[p_{1},\ldots,p_{M}]\) be an embedding-augmented distribution with embeddings \(\{\mathbf{v}_{m}\}_{m=1}^{M}\subset\mathbb{R}^{d}\), and let \(\mathcal{K}\) be a compact set such that \(\{\mathbf{v}_{m}\}_{m=1}^{M}\subset\mathcal{K}\). Furthermore, let \(\{\mu_{m}\}_{m=1}^{M}\) be a set of base measures over \(\mathbb{R}^{d}\) that are absolutely continuous with respect to the standard Lebesgue measure \(\lambda\) over \(\mathbb{R}^{d}\), i.e., \(\mu_{m}\ll\lambda\). Define the \((\mathcal{K},\mu)\)-**Voronoi measure** as follows_

\[p_{\mathrm{V}}(\mathbf{x})\stackrel{{\mathrm{def}}}{{=}}\left\{ \frac{p_{m^{*}(\mathbf{x})}}{\mu_{m}(C_{m^{*}(\mathbf{x})})}\frac{\mathrm{d} \mu_{m}}{\mathrm{d}\lambda}(\mathbf{x}),\ \ \textbf{if}\ \mathbf{x}\in\mathcal{K}\right.\] (7)

_where we define projection_

\[m^{*}(\mathbf{x})\stackrel{{\mathrm{def}}}{{=}}\operatorname*{ argmin}_{m\in[M]}||\mathbf{x}-\mathbf{v}_{m}||_{2}^{2}.\] (8)

In the following proposition, we make precise the sense in which a Voronoi measure encodes the original embedding-augmented distribution.

**Proposition 1**.: _Let \(\bm{p}=[p_{1},\ldots,p_{M}]\) be an embedding-augmented distribution with embeddings \(\{\mathbf{v}_{m}\}_{m=1}^{M}\subset\mathbb{R}^{d}\), and let \(p_{\mathrm{V}}\) be the corresponding Voronoi measure Eq. (7). Then, \(p_{\mathrm{V}}(C_{m})=p_{m}\) where \(C_{m}\) is defined as in Eq. (6). See App. C.1 for proof._

**Example 1**.: _Suppose \(\bm{p}=[p_{1},\ldots,p_{4}]\) is a categorical distribution, and there are 4 embeddings in \(\mathbb{R}^{2}\) associated with each \(p_{i}\), namely: \(\mathbf{v}_{1}=[1,1],\mathbf{v}_{2}=[-1,1],\mathbf{v}_{3}=[-1,-1],\mathbf{v}_ {4}=[1,-1]\). Given the \(\mathcal{K}=[-2,2]\times[-2,2]\) and the embedding-augmented probability distribution \(\bm{p}\), Eq. (7) defines a Voronoi measure over this space, where the Voronoi cells are visualized in Fig. 1. We will discuss in SS6 how Voronoi sampling navigates this space._

### Structured Voronoi Measures

To encode language models as densities more naturally, we introduce a generalization of the Voronoi measure, which we term a structured Voronoi measure. Now, rather than a distribution over \(M\) elements, we assume to have a sequence of length \(N\). Each token in the sequence takes value in \([M]\). Let \(\bm{m}=[m_{1},\ldots,m_{N}]\in[M]^{N}\). We define a **structured Voronoi cell** as \(C_{\bm{m}}=\prod_{n=1}^{N}C_{m_{n}}\), where \(\prod\) denotes the Cartesian product and we define the individual Voronoi cell as

\[C_{m_{n}}=\Big{\{}\mathbf{x}:\mathbf{x}\in\mathcal{K},||\mathbf{x}-\mathbf{v} _{m_{n}}||_{2}^{2}\leq||\mathbf{x}-\mathbf{v}_{m^{\prime}}||_{2}^{2},\forall m ^{\prime}\neq m_{n}\Big{\}}.\] (9)

**Proposition 2**.: _Let \(\mu\) be a measure on \(\mathbb{R}^{d}\). Then, we have the product measure space as \(\mu(C_{\bm{m}})=\prod_{n=1}^{N}\mu(C_{m_{n}})\). See App. C.2 for proof._

**Definition 3**.: _Let \(\bm{p}\) be an embedding-augmented distribution over \([M]^{N}\). For \(\bm{m}\in[M]^{N}\), we denote \(\bm{m}\)'s probability as \(p_{\bm{m}}\), and \(\bm{m}\)'s embedding as \(\mathbf{V}_{\bm{m}}\in\mathbb{R}^{N\times d}\). Let \(\mathcal{K}\) be a compact set that covers the embeddings \(\mathbf{V}_{\bm{m}}\) and let \(\mu\ll\lambda\) be a base measure absolutely continuous with respect to the Lebesgue measure \(\lambda\). We define the \((\mathcal{K},\mu)\)-**Voronoi measure** as follows_

\[p_{\mathrm{V}}(\mathbf{x})\stackrel{{\mathrm{def}}}{{=}}\left\{ \begin{subarray}{c}\frac{p_{\bm{m}^{*}(\mathbf{x})}}{\mu(C_{\bm{m}^{*}(\mathbf{ x})})}\frac{\mathrm{d}\mu}{\mathrm{d}\lambda}(\mathbf{x}),\ \ \textbf{if}\ \mathbf{x}\in\mathcal{K}\\ 0,\ \Application to Text Generation

Def. 3 gives us the flexibility to use any probability distribution over sequences of embeddings to define a structured Voronoi measure. For example, one can substitute \(p_{\bm{m}^{*}(\mathbf{x})}\) with an embedding-augmented LM, i.e., Eq. (3), to encode a language model as a structured Voronoi measure. Another example is to encode a controlled LM as a structured Voronoi measure by substituting \(p_{\bm{m}^{*}(\mathbf{x})}\) with Eq. (4).

Base Measure.We have explained how to encode our desired distribution as a structured Voronoi measure. However, in order to actually implement a gradient-based sampler, we need to specify the base probability measures \(\{\mu_{m}\}_{m=1}^{M}\). Given an embedding-augmented probability \(p(\mathbf{V})\) it is natural to follow the gradient of \(\log p(\mathbf{V})\) with respect to the word embedding, i.e., \(\mathbf{g}_{\bm{m}}=\nabla_{\mathbf{V}}\log p(\mathbf{V})\). Thus, if we want to follow a direction similar to \(\mathbf{g}_{\bm{m}}\), one natural choice for \(\mu\) is a Gaussian measure centered at the gradient \(\mathbf{g}_{\bm{m}}\)_restricted_ to Voronoi cell \(C_{\bm{m}}\),6 which we define:

Footnote 6: Please refer to App. B for a discussion on the reasoning behind choosing the Gaussian measure.

\[\mu(A)=\frac{1}{\mu(C_{\bm{m}})}\int_{A\cap C_{\bm{m}}}\exp\left(-\frac{1}{2} \|\mathbf{g}_{\bm{m}}-\mathbf{x}\|_{2}^{2}\right)\mathrm{d}\lambda(\mathbf{x}).\] (12)

The normalizer ensures the measure is a probability measure. Furthermore, we have that \(\mu\)'s Radon-Nikodym derivative with respect to the Lebesgue measure \(\lambda\) is given by

\[\frac{\mathrm{d}\mu}{\mathrm{d}\lambda}(\mathbf{x})\stackrel{{ \text{\tiny def}}}{{=}}\begin{cases}\frac{1}{\mu(C_{\bm{m}})}\exp \left(-\frac{1}{2}||\mathbf{g}_{\bm{m}}-\mathbf{x}||_{2}^{2}\right),&\text{if } \mathbf{x}\in C_{\bm{m}}\\ 0,&\text{otherwise}\end{cases}\] (13)

Eq. (13) should be recognizable as the standard Gaussian density, albeit one that is truncated to the Voronoi cell \(C_{\bm{m}}\)[27].

**Proposition 3**.: _Eq. (13) is absolutely continuous with respect to the Lebesgue measure \(\lambda\). See App. C.3 for proof._

**Proposition 4**.: _The gradient of the log of the Voronoi measure \(p_{\mathbf{V}}\) is given by_

\[\nabla_{\mathbf{x}}\log p_{\mathbf{V}}(\mathbf{x})=\begin{cases}\mathbf{g}_{ \bm{m}}-\mathbf{x},&\text{if }\mathbf{x}\in\mathrm{int}(C_{\bm{m}})\\ \text{undefined},&\text{if }\mathbf{x}\in\partial C_{\bm{m}}\\ \mathbf{0},&\text{otherwise}\end{cases}\] (14)

_where the first two blocks in the case statement apply if there exists some \(\bm{m}\) such that \(\mathbf{x}\in\mathrm{int}(C_{\bm{m}})\) or \(\mathbf{x}\in\partial C_{\bm{m}}\). See App. C.4 for proof._

## 5 Gradient-Based Sampling

In this section, we first review gradient-based sampling methods, namely hmc and Langevin dynamics. Then we discuss how they have been used in previous papers on text generation. This will set the stage for our algorithm in SS6, which is based on hmc.

### Hamiltonian Monte Carlo

The goal of hmc, originally proposed by Duane et al. [7], is to design a better proposal distribution in the standard Metropolis-Hastings MCMC by taking advantage of the gradient information in a principled way. Concretely, to sample from a given distribution \(p(\mathbf{x})\) where \(\mathbf{x}\in\mathbb{R}^{d}\), hmc treats \(\mathbf{x}\) as the coordinates of the particles in some fictitious physical system. It then introduces an auxiliary momentum variable \(\mathbf{r}\in\mathbb{R}^{d}\) associated with each coordinate and defines a Hamiltonian function \(H(\mathbf{x},\mathbf{r})\). Here, the Hamiltonian \(H\) has the intuitive physical interpretation of the total energy of some conservative system, and, in classical mechanics, decomposes into the potential energy \(U(\mathbf{x})\) and the kinetic energy \(K(\mathbf{r})\), i.e., \(H(\mathbf{x},\mathbf{r})=U(\mathbf{x})+K(\mathbf{r})\). This formulation is convenient in part because if we define the joint distribution \(p(\mathbf{x},\mathbf{r})\propto e^{-H(\mathbf{x},\mathbf{r})}\) as in energy-based models, then

\[p(\mathbf{x},\mathbf{r})\propto e^{-U(\mathbf{x})}\cdot e^{-K(\mathbf{r})},\] (15)which means we can treat \(\mathbf{x}\) and \(\mathbf{r}\) as independent variables. Naturally, we can let \(U(\mathbf{x})=-\log p(\mathbf{x})\) so that \(\mathbf{x}\) has the marginal of the target distribution. It is also common practice to set \(K(\mathbf{r})=\mathbf{r}^{\top}M^{-1}\mathbf{r}/2\) so that the momentum variable has a Gaussian distribution. Here, \(M\in\mathbb{R}^{d\times d}\) is called the **mass matrix** and commonly set to identity.7

Footnote 7: There exist sophisticated methods [15] to tune the mass matrix, but given that LM gradients are expensive to compute, we will not attempt such methods in this paper.

The Hamiltonian \(H\) determines the equations of motion in a physical system, given by the Hamiltonian equations, which is also known as Hamiltonian dynamics,

\[\frac{\mathrm{d}\mathbf{x}}{\mathrm{d}t}=\frac{\partial H}{\partial\mathbf{r }},\qquad\frac{\mathrm{d}\mathbf{r}}{\mathrm{d}t}=-\frac{\partial H}{\partial \mathbf{x}}.\] (16)

We are now ready to give a high-level description of how hmc generates a single sample (see Algorithm 1): First sample a momentum \(\mathbf{r}\) from a Gaussian (line 1), then evolve the system using the Hamiltonian equations Eq. (16) for some predetermined amount of time (lines 4 to 7), and finally accept the new state with the Metropolis-Hastings acceptance criterion (lines 8 to 13). Note that the Hamiltonian equations often admit no closed-form solution in practice, and hence one needs to use numerical integrators for approximation. In particular, the leapfrog integrator, corresponding to lines 5 to 7, is almost always used in hmc.

Ingeniously designed by Duane et al. [7], the efficiency of this elegant procedure is a result of several favorable properties of Hamiltonian mechanics--namely volume preservation, reversibility, and conservation. Upon exact simulation of the Hamiltonian dynamics, these properties will lead to the acceptance probability of one, i.e., every proposed move will be accepted. In App. D, we will give intuition about these properties. Since the method developed later in SS6 will subsume hmc as a special case, we delay the proof of correctness until then.

Langevin Dynamics.Langevin dynamics is a simplification of hmc, where only one leapfrog step is performed. Furthermore, what is usually referred to as Langevin dynamics is in fact the _uncorrected_ Langevin dynamics, where the acceptance criterion is ignored, i.e., every proposed sample is accepted; see Algorithm 2. While uncorrected Langevin dynamics is guaranteed to converge when the energy function is smooth [31, SS5.3], there is no such guarantee with the presence of a discontinuity in the energy function. Nevertheless, due to its simplicity, Langevin dynamics is the only gradient-based sampling method that has been applied for text generation applications.

```
0:\(\mathbf{x}^{t}\): current sample, \(U\): potential energy function, \(\varepsilon\): step size, \(L\): number of leapfrog steps
0: next sample: \(\mathbf{x}^{t+1}\)
1:\(\mathbf{r}\sim\mathcal{N}(\mathbf{0},\mathbb{I})\)
2:\(H^{t}\gets U(\mathbf{x}^{t})+K(\mathbf{r})\)
3:\(\mathbf{x}^{t+1}\leftarrow\mathbf{x}^{t}\)
4:for\(l=1,\dots,L\):
5:\(\mathbf{r}\leftarrow\mathbf{r}-\frac{\varepsilon}{2}\nabla U(\mathbf{x}^{t+1})\)
6:\(\mathbf{x}^{t+1}\leftarrow\mathbf{x}^{t+1}+\varepsilon\mathbf{r}\)
7:\(\mathbf{r}\leftarrow\mathbf{r}-\frac{\varepsilon}{2}\nabla U(\mathbf{x}^{t+1})\)
8:\(H^{t+1}\gets U(\mathbf{x}^{t+1})+K(\mathbf{r})\)
9:\(\Delta H\gets H^{t+1}-H^{t}\)
10:if\(\mathbf{s}\sim\mathcal{U}(0,1)<e^{-\Delta H}:\)
11:return\(\mathbf{x}^{t+1}\)
12:else
13:return\(\mathbf{x}^{t+1}\leftarrow\mathbf{x}^{t}\) ```

**Algorithm 1** HMC

### Applications to Controlled Generation

One of the primary advantages of using gradient-based techniques for controlled generation is that it provides a means to sample from the conditional distribution Eq. (4) without having to calculate the normalization factor \(Z_{t}\). For example in hmc algorithm (Algorithm 1), all we need to calculate regarding the potential energy \(U(\mathbf{V})=-\log p(\mathbf{V}\mid t)\) is two terms: (1) the gradient of \(U\) with respect to \(\mathbf{V}\): \(\nabla U\), and (2) the difference between the potential energy of two points \(\Delta U\) for the Metropolis criterion. Fortunately, both terms are independent of \(Z_{t}\), and we can sample from the conditional distribution without the need to compute \(Z_{t}\).

MuCoLa.As defined in Eq. (4), \(p(\mathbf{V}\mid t)\) is \(\mathbb{R}^{d}\)-valued, so it is tempting to apply a gradient-based sampling technique to sample from it. And, indeed, Kumar et al. [21] have proposed such a scheme based on Langevin dynamics. They define the potential energy of the embeddings as \(U(\mathbf{V})\stackrel{{\text{def}}}{{=}}-\log p(\mathbf{V}\mid t)\) and apply Langevin dynamics out of the box; see Algorithm 3. However, since \(U(\mathbf{V})\) is zero for any vector other than vectors in \(\mathcal{B}\) (defined in SS2.1), they modify the sampling process and project the proposed sample to the base embeddings set at each step of the sampling process (line 3 of the algorithm). The added projection step, however, is neither volume-preserving nor time-reversible. Hence, this sampling procedure does not sample from the intended distribution and is not a valid MCMC algorithm.

cold and other schemes.Alternative approaches have been proposed in the literature to reformulate a language model as potential energy over the logit [cold; 38] or simplex space [14; 20]. However, these formulations are not suitable for principled gradient-based sampling. cold only employs a heuristic energy function to select among the candidate generations obtained via top-\(k\) sampling, and the simplex-based approach requires an extra constraint to ensure the sample stays on the simplex.

## 6 Structured Voronoi Sampling

Given our structured Voronoi measure \(p_{\mathrm{V}}\), one can apply hmc to sample from it. In this section, we take one step further and propose a variation of hmc that is more suitable to sample from \(p_{\mathrm{V}}\). Importantly, \(p_{\mathrm{V}}\) contains discontinuities whereas the generic leapfrog integrator does not account for such sudden jumps in the potential function. In other words, even if the leapfrog integrator itself is volume preserving and time-reversible, the sudden jumps in potential can lead to large deviations in the Hamiltonian value, causing a low acceptance rate. We therefore would like to find an alternative to leapfrog in such situations.

A Physical Analogy.In classical mechanics, discontinuity with smooth boundary in the potential function occurs naturally, e.g., in collision dynamics or a slab magnetic field, and is referred to as a potential barrier (or interface). Upon encountering a potential barrier, a particle will either be reflected from or transmitted through the barrier surface, depending on whether it has enough kinetic energy to overcome the potential jump (e.g., [8; SS4.6.2]). Such behavior is similar to reflection-refraction phenomenon in optics. The key insight here is that, in both cases, the Hamiltonian is conserved.

Reflection and Refraction.To give a precise mechanical description of this behavior, suppose a particle encounters a potential barrier at position \(\mathbf{x}\) with momentum \(\mathbf{r}\). We can decompose momentum as \(\mathbf{r}=\mathbf{r}_{\perp}+\mathbf{r}_{\parallel}\) where \(\mathbf{r}_{\perp}\) is normal to the barrier and \(\mathbf{r}_{\parallel}\) parallel to it. Let \(\Delta U\) be the signed potential energy difference between the two sides of the barrier. If \(\|\mathbf{r}_{\perp}\|_{2}^{2}>2\Delta U\), then the particle has enough kinetic energy to overcome the barrier, and its momentum's normal component will instantaneously become \(\mathbf{r}_{\perp}^{\prime}=\sqrt{\|\mathbf{r}_{\perp}\|_{2}^{2}-2\Delta U} \cdot\frac{\mathbf{r}_{\perp}}{\|\mathbf{r}_{\perp}\|_{2}^{2}}\) after being transmitted through the barrier (refraction). Otherwise, if \(\|\mathbf{r}_{\perp}\|_{2}^{2}\leq 2\Delta U\), the particle will be reflected from the barrier and the normal component will instantaneously become \(\mathbf{r}_{\perp}^{\prime}=-\mathbf{r}_{\perp}\). We show in App. E.1 that Hamiltonian is conserved in either case. The reflect-refract process is summarized in Algorithm 5.

### A Sampling Algorithm

Noting that the discontinuity surfaces of \(p_{\mathrm{V}}\) are all piecewise smooth, we can build on the above and develop a sampling algorithm for \(p_{\mathrm{V}}\) to handle discontinuity in a principled and effective way. In fact, we only need to make one change to the generic hmc, which is updates \((\mathbf{x},\mathbf{r})\) according to the mechanical description given above. Concretely, we need to replace step 2 in the hmc outline in SS5.1: When a single step of leapfrog encounters no discontinuity, we may advance to the next point as in hmc; however, when there is discontinuity, if a full leapfrog step is taken, we need to proceed by repeatedly computing where the discontinuity is encountered, taking a smaller step up to the discontinuity and refracting-reflecting based on the potential energy difference. This process is continued until we have exhausted the step size. Since refracting-reflecting conserves Hamiltonian (App. E.1), this process yields a better acceptance rate in the presence of a discontinuity. See Algorithm 4 for the details of this sampling procedure, and App. F.1 for how to efficiently find discontinuities. We will supply proof of correctness in App. E.2.

A note on calculating the base measure.To adjust the momentum, one needs to compute \(\Delta U\), which implies computing the difference between two base measures, as defined in Eq. (12). However, computing such an integral in a high-dimensional space is not practical. Therefore, make an assumption that the base measures of Voronoi cells are equal, and thus do not have an effect on \(\Delta U\). However, such an assumption might not hold. See App. A for limitations.

## 7 Experiments

We empirically assess the performance of Voronoi sampling in a series of experiments. In each experiment, we perform a grid search to find the best set of hyperparameters, these experimental details can be found in App. H. Our open-source implementation will be available upon publication.

### Toy Example

We first apply our Voronoi sampling8 method on the toy example discussed earlier in Example 1, where the reference probability distribution is tractable and known \(p(\mathbf{x})\). The potential energy is then set to \(U(\mathbf{x})=-\log p_{\mathrm{V}}(\mathbf{x})\). Importantly, the toy experiment is intentionally designed such that the base measure of all of the Voronoi cells is equal, therefore, we can safely ignore calculating the base measure and arrive at exact sampling methods.

Footnote 8: Note that in the case of the toy experiment, we are not sampling a _sequence_ of text, but rather a single embedding. To highlight this point, we use Voronoi sampling instead of _structured_ Voronoi sampling to refer to our algorithm in this section.

We compare Voronoi sampling to MuCoLa and hmc. To make a fair comparison, we add the Metropolis criterion9 to the MuCoLa algorithm and only do one leapfrog step in all algorithms. Furthermore, to see the effect of the reference distribution on the performance of the sampling algorithm, we anneal this distribution with 6 temperatures, where the lower temperatures lead to peaky distributions and the higher temperatures to uniform-like distributions.

Footnote 9: We accept transitioning from \(\mathbf{x}^{t}\) to \(\mathbf{x}^{t+1}\) with probability \(e^{H^{t}-H^{t+1}}\).

We take \(200\) samples after \(500\) burn-in iterations, and compare the empirical distributions of samples to the reference by measuring the Jensen-Shannon (JS) divergence between the two. As results in Fig. 2 show, the JS divergence between the reference distribution and the empirical distribution of Voronoi samples is the smallest. The difference between the methods is more pronounced at lower temperatures, as the change in potential energy is greater, resulting in more errors in the leapfrog integrator. In App. I we provide more empirical support that Voronoi sampling converges faster to the reference distribution, especially when we increase the dimensionality of the sampling space.

### Sampling from Language Models

Next, we apply our method to sample from a language model. The underlying LM is a finetuned GPT-20 on E2E dataset [34]; see App. G for dataset statistics. As opposed to the previous experiment, the reference distribution of the LM is not tractable. Therefore, we use the empirical distribution of ancestral samples as an unbiased estimate of the reference distribution. Note that ancestral sampling incrementally draws samples from a language model, where at each step of the generation a token \(w_{n}\) is sampled with the probability given by the LM: \(p(w_{n}\mid\bm{w}_{<n})\). Therefore, the process can give unbiased estimates of the reference distribution.

We follow SS4 to define a structured Voronoi measure \(p_{\mathrm{V}}\) using the LM probability. We then implement svs, where the potential energy is set to \(-\log p_{\mathrm{V}}\). To empirically measure the benefit of reflection-refraction step in svs, we compare it to applying Langevin dynamics directly to \(p_{\mathrm{V}}\). We implement MuCoLa as a baseline, which writes the potential energy using an embedding-augmented LM, i.e., Eq. (3).

We show the distribution of samples' perplexity in Fig. 3. The green trace is the empirical distribution of \(1000\) ancestral samples. While all the sampling methods result in distributions comparably close to ancestral samples' distribution, we observe that svs manages to model the tail of the distribution better. On the other hand, MuCoLa and langevin tend to take samples from the mode of the distribution more often.

### Controlled Generation

Finally, we apply our structured Voronoi sampling to \(2\) controlled generation task. The goal of the first task is to generate restaurant reviews for a target food type \(t\), e.g., Italian, Fast food, Japanese, etc. The goal of the second task is to control the sentiment of the generations to enforce a positive sentiment. We train classifiers to predict the target \(t\) (food type or positive sentiment) from the input sequence \(p(t\mid\bm{w})\).11 We implement two baselines:

Footnote 11: See App. H for more experimental details about classifiers.

fudge.Yang and Klein [48] offer a heuristic approach to sample from the conditional distribution. They incrementally sample tokens under the language model. At each sampling step, they adjust the probabilities given by the LM, by feeding each candidate prefix to a classifier and obtain the probability of that prefix following the control target.

MuCoLa.Kumar et al. [21] treat \(p(\mathbf{V}\mid t)\), Eq. (4), as a distribution in \(\mathbb{R}^{d}\) and apply Langevin dynamics directly to sample a sequence of embeddings \(\mathbf{V}\). The potential energy is defined as \(-\log p(\mathbf{V}\mid t)\). When rewriting this potential energy with Bayes' rule, it has been shown empirically, that adding a hyperparameter \(\gamma\) is helpful to keep the balance between the classifier and LM. Therefore, the final potential energy is defined as:

\[U(\mathbf{V})\stackrel{{\text{def}}}{{=}}-\log p(\mathbf{V})- \gamma\log p(t\mid\mathbf{V}).\] (17)

As mentioned earlier, \(p(\mathbf{V}\mid t)\) only places a positive probability on a countable set. We, therefore, use Def. 3 to define structured Voronoi measures and set the potential energy to

\[U(\mathbf{x})\stackrel{{\text{def}}}{{=}}-\log p_{\mathrm{V}}( \mathbf{x})-\gamma\log p_{\mathrm{V}}(t\mid\mathbf{x}).\] (18)

We then apply Langevin dynamics and svs to sample according to this potential energy.

Evaluation.We sample \(120\) sentences of length \(20\)12 and evaluate the generations on three metrics:

Footnote 12: We sample \(20\) sentences per control target.

* **Success:** is defined as the percentage of generations that adhere to the control target. To determine whether a generation conforms to the specified target we use an _evaluator classifier_.
* **Fluency:** is measured by the mean and standard deviation of perplexity under the language model.
* **Diversity:** is measured by the mean number of distinct \(n\)-grams (\(n=1,2,3\)) in a set of samples, normalized by the length of the sequence.

As results in Table 1 show,13 fudge tends to achieve the highest diversity, however, it fails to follow the control target. MuCoLa either generates fluent results without paying enough attention to the control, or sacrifices fluency in favor of following the control target; thus, the high variance in success rates. Both langevin and svs result in a high success rate and maintain fluency and diversity, and svs is effective in maintaining a balance between various metrics and producing fluent sentences that adhere to control targets.

Footnote 13: Please refer to Fig. 7 to see a visualization of the topic control results, and to Table 6 for results per target type.

## 8 Related Work

Controlled Generation.Numerous approaches have been proposed to enforce controls during the text generation process [19; 24; 41], _inter alia_]. For example, weighted decoding [10; 17] scores each candidate token with a weighted sum of its score under the language model and its adherence to control targets, subsequently selecting candidates with the highest scores. fudge method adopts a similar scoring function, resembling a Bayesian formulation for \(p(\bm{w}\mid t)\). After making simplifying assumptions and factorizing \(p(\bm{w}\mid t)\), fudge samples tokens autoregressively based on their scores. More recently, a line of research attempts to directly sample from \(p(\bm{w}\mid t)\) by reformulating it as an energy-based model and sampling from it using efficient gradient-based sampling algorithms. As discussed in SS5.2 cold reformulates \(p(\bm{w}\mid t)\) as an energy-based model on the logit space and uses that to select samples with high energy from a number of candidate generations. MuCoLa offers a sampling algorithm motivated by Langevin Dynamics that operates in the embedding space.

Gradient-based Sampling.Our work is closely related to the line of research that makes use of gradient information to sample from complex distributions [7; 31; 46]. Gradient-based samplers [15; 32] are shown to be highly effective when sampling from continuous distributions [3; 4; 37]. However, it is a difficult problem to adapt gradient-based samplers to discrete settings [36; 50]. More recently, several papers proposed promising gradient-based MCMC for discrete distribution that are reversible chains [11; 39; 49]. Our work instead formulates an irreversible Markov chain based on HMC. We leave it to future work to explore the utility of these recent papers on text generation.

## 9 Conclusion

In this work, we propose structured Voronoi sampling, a principled gradient-based sampling method for text generation. To formulate the energy function used in svs, we define structured Voronoi measures on the embedding space and show how such measures can encode language models. In a controlled generation task, svs outperformed other sampling methods in following the control target while producing comparably fluent and diverse samples.

## Broader Impacts

It has been repeatedly shown that LMs can generate harmful, toxic, or non-factual content [9; 35; 42]. In fact, an application of the controlled generation scheme discussed in this paper could be to mitigate such issues. However, the same method could be used to generate misinformation, or toxic content intentionally.

## Acknowledgements

We thank Tim Vieira, Tiago Pimentel, and Clara Meister for their feedback on this work. We also thank the anonymous reviewers for their constructive feedback during the review process. Afra Amini is supported by ETH AI Center doctoral fellowship.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{Topic Control} & \multicolumn{4}{c}{Sentiment Control} \\ \cline{2-10}  & Success(\(\uparrow\)) & P(\(\downarrow\)) & Dist-(\(\uparrow\)) & Dist-(\(\uparrow\)) & Dist-(\(\uparrow\)) & Success(\(\uparrow\)) & P(\(\downarrow\)) & Dist-(\(\uparrow\)) & Dist-2(\(\uparrow\)) & Dist-3(\(\uparrow\)) \\ \hline GPT & \(0.12\pm 0.10\) & \(5.10\pm 2.06\) & \(0.40\) & \(0.56\) & \(0.67\) & \(0.55\pm 0.49\) & \(21.33\pm 27.17\) & \(0.40\) & \(0.60\) & \(0.71\) \\ fudge & \(0.30\pm 0.12\) & \(5.59\pm 0.60\) & \(0.39\) & \(0.55\) & \(0.65\) & \(0.57\pm 0.49\) & \(24.27\pm 12.46\) & \(0.40\) & \(0.60\) & \(0.70\) \\ MuCoLa & \(0.58\pm 0.23\) & \(33.09\pm 36.32\) & \(0.26\) & \(0.40\) & \(0.51\) & \(0.66\pm 0.47\) & \(85.74\pm 152.18\) & \(0.28\) & \(0.42\) & \(0.53\) \\ \hline Langevin & \(0.91\pm 0.12\) & \(14.26\pm 2.55\) & \(0.24\) & \(0.39\) & \(0.51\) & \(0.82\pm 0.38\) & \(26.76\pm 14.42\) & \(0.16\) & \(0.30\) & \(0.41\) \\ svs & \(0.92\pm 0.05\) & \(13.9\pm 2.04\) & \(0.22\) & \(0.37\) & \(0.49\) & \(0.84\pm 0.36\) & \(32.73\pm 16.70\) & \(0.14\) & \(0.28\) & \(0.41\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Evaluation of different sampling methods on controlled generation, using three criteria: success in following the control target (measured by the evaluator classifier), fluency (measured by perplexity), and diversity.

## References

* [1]L. Ambrosio (2008) Transport equation and cauchy problem for non-smooth vector fields. Vol. 1, Springer Berlin Heidelberg. External Links: ISBN 978-3-642-2211-1, Link, Document Cited by: SS1.
* [2]V. I. Arnold (1989) Mathematical methods of classical mechanics. Vol. 3, Springer, New York, NY. External Links: ISBN 978-3-642-211-1, Link, Document Cited by: SS1.
* [3]E. Bingham, J. P. Chen, M. Jankowiak, F. Obermeyer, N. Pradhan, T. Karaletsos, R. Singh, P. Szerlip, P. Horsfall, and N. D. Goodman (2018) Pyro: deep universal probabilistic programming. Journal of Machine Learning Research. External Links: Document, Link Cited by: SS1.
* [4]B. Carpenter, A. Gelman, M. D. Hoffman, D. Lee, B. Goodrich, M. Betancourt, M. Brubaker, J. Guo, P. Li, and A. Riddell (2017) Stan: a probabilistic programming language. Journal of statistical software76 (1). External Links: Document, Link Cited by: SS1.
* [5]P. Dhariwal and A. Nichol (2021) Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, Vol. 34, pp. 8780-8794. External Links: Document, Link Cited by: SS1.
* [6]L. Du, L. Torroba Hennigen, T. Pimentel, C. Meister, J. Eisner, and R. Coterell (2022) A measure-theoretic characterization of tight language models. External Links: 2202.0222 Cited by: SS1.
* [7]S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth (1987) Hybrid Monte Carlo. Physics Letters B195 (2), pp. 216-222. External Links: Document, Link Cited by: SS1.
* [8]J. Michael Finn (2010) Jones and Bartlett Publishers. Vol., MA. External Links: ISBN 978-3-642-211-1, Link, Document Cited by: SS1.
* [9]S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith (2020) RealToxicityPrompts: evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online, pp. 3356-3369. External Links: Document, Link Cited by: SS1.
* [10]M. Ghazvininejad, X. Shi, J. Priyadarshi, and K. Knight (2017) Hafez: an interactive poetry generation system. In Proceedings of ACL 2017, System Demonstrations, Vancouver, Canada, pp. 43-48. External Links: ISBN 978-3-642-211-1, Link, Document Cited by: SS1.
* [11]W. Grathwohl, K. Swersky, M. Hashemi, D. Duvenaud, and C. Maddison (2021) Oops I took a gradient: scalable sampling for discrete distributions. In Proceedings of the 38th International Conference on Machine Learning, Vol. 139 of _Proceedings of Machine Learning Research_, pp. 3831-3841. External Links: Document, Link Cited by: SS1.
* [12]W. K. Hastings (1970) Monte Carlo sampling methods using Markov chains and their applications. Biometrika57 (1), pp. 97-109. External Links: Document, Link Cited by: SS1.
* [13]J. Ho, A. Jain, and P. Abbeel (2020) Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, Vol. 33, pp. 6840-6851. External Links: Document, Link Cited by: SS1.
* [14]C. Duy V. Hoang, G. Haffari, and T. Cohn (2017) Towards decoding as continuous optimisation in neural machine translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Copenhagen, Denmark, pp. 146-156. External Links: Document, Link Cited by: SS1.
* [15]M. D. Hoffman and A. Gelman (2014) The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research15 (47), pp. 1593-1623. External Links: Document, Link Cited by: SS1.
* [16]A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi (2020) The curious case of neural text degeneration. In Proceedings of the 8th International Conference on Learning Representations,* Holtzman et al. [2018] Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. 2018. Learning to write with cooperative discriminators. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1638-1649, Melbourne, Australia. Association for Computational Linguistics.

* Krause et al. [2021] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2021. GeDi: Generative discriminator guided sequence generation. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 4929-4952, Punta Cana, Dominican Republic. Association for Computational Linguistics.
* Kumar et al. [2021] Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia Tsvetkov. 2021. Controlled text generation as continuous optimization with multiple constraints. In _Advances in Neural Information Processing Systems_.
* Kumar et al. [2022] Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. 2022. Constrained sampling from language models via langevin dynamics in embedding spaces. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics.
* Lanczos [1949] Cornelius Lanczos. 1949. _The Variational Principles of Mechanics_. Mathematical Expositions, No. 4. University of Toronto Press, Toronto.
* Li et al. [2022] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. 2022. Diffusion-LM improves controllable text generation. In _Advances in Neural Information Processing Systems_.
* Liu et al. [2021] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. 2021. DE experts: Decoding-time controlled text generation with experts and anti-experts. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 6691-6706, Online. Association for Computational Linguistics.
* Liu et al. [2020] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. RoBERTa: A robustly optimized BERT pretraining approach.
* Lou and Ermon [2023] Aaron Lou and Stefano Ermon. 2023. Reflected diffusion models. In _Proceedings of the 40th International Conference on International Conference on Machine Learning_.
* Maatouk and Bay [2016] Hassan Maatouk and Xavier Bay. 2016. A new rejection sampling method for truncated multivariate Gaussian random variables restricted to convex sets. In _Monte Carlo and Quasi-Monte Carlo Methods_, pages 521-530, Cham. Springer International Publishing.
* Meister et al. [2022] Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. 2022. Locally typical sampling. _Transactions of the Association for Computational Linguistics_.
* Metropolis et al. [1953] Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller. 1953. Equation of state calculations by fast computing machines. _The Journal of Chemical Physics_, 21(6):1087-1092.
* Afshar and Domke [2015] Hadi Mohasel Afshar and Justin Domke. 2015. Reflection, refraction, and Hamiltonian monte carlo. In _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc.
* Neal [1993] Radford M. Neal. 1993. _Probabilistic inference using Markov chain Monte Carlo methods_. Department of Computer Science, University of Toronto Toronto, ON, Canada.

* Neal (2011) Radford M. Neal. 2011. MCMC using Hamiltonian dynamics. In Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng, editors, _Handbook of Markov Chain Monte Carlo_, chapter 5. Chapman and Hall/CRC.
* Nishimura et al. (2020) Akihiko Nishimura, David B. Dunson, and Jianfeng Lu. 2020. Discontinuous Hamiltonian Monte Carlo for discrete parameters and discontinuous likelihoods. _Biometrika_, 107(2):365-380.
* Novikova et al. (2017) Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. 2017. The E2E dataset: New challenges for end-to-end generation. In _Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue_, pages 201-206, Saarbrucken, Germany. Association for Computational Linguistics.
* Pagnoni et al. (2021) Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4812-4829, Online. Association for Computational Linguistics.
* Pakman and Paninski (2013) Ari Pakman and Liam Paninski. 2013. Auxiliary-variable exact hamiltonian monte carlo samplers for binary distributions. In _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc.
* Phan et al. (2019) Du Phan, Neeraj Pradhan, and Martin Jankowiak. 2019. Composable effects for flexible and accelerated probabilistic programming in numpyro. _arXiv preprint arXiv:1912.11554_.
* Qin et al. (2022) Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. 2022. COLD decoding: Energy-based constrained text generation with Langevin dynamics. In _Advances in Neural Information Processing Systems_.
* Rhodes and Gutmann (2022) Benjamin Rhodes and Michael U. Gutmann. 2022. Enhanced gradient-based MCMC in discrete spaces. _Transactions on Machine Learning Research_.
* Saharia et al. (2022) Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. 2022. Palette: Image-to-image diffusion models. In _ACM SIGGRAPH 2022 Conference Proceedings_, New York, NY, USA. Association for Computing Machinery.
* Schick et al. (2021) Timo Schick, Sahana Udupa, and Hinrich Schutze. 2021. Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP. _Transactions of the Association for Computational Linguistics_, 9:1408-1424.
* Sheng et al. (2021) Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2021. Societal biases in language generation: Progress and challenges. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 4275-4293, Online. Association for Computational Linguistics.
* Song and Ermon (2020) Yang Song and Stefano Ermon. 2020. Improved techniques for training score-based generative models. In _Advances in Neural Information Processing Systems_, volume 33, pages 12438-12448. Curran Associates, Inc.
* Tao and Jin (2022) Molei Tao and Shi Jin. 2022. Accurate and efficient simulations of hamiltonian mechanical systems with discontinuous potentials. _Journal of Computational Physics_, 450:110846.
* Welleck et al. (2020) Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2020. Neural text generation with unlikelihood training. In _International Conference on Learning Representations_.
* Welling and Teh (2011) Max Welling and Yee Whye Teh. 2011. Bayesian learning via stochastic gradient Langevin dynamics. In _Proceedings of the 28th International Conference on International Conference on Machine Learning_, page 681-688, Madison, WI, USA. Omnipress.

* [47] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online. Association for Computational Linguistics.
* [48] Kevin Yang and Dan Klein. 2021. FUDGE: Controlled text generation with future discriminators. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 3511-3535, Online. Association for Computational Linguistics.
* [49] Ruqi Zhang, Xingchao Liu, and Qiang Liu. 2022. A Langevin-like sampler for discrete distributions. In _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 26375-26396. PMLR.
* [50] Yichuan Zhang, Zoubin Ghahramani, Amos J. Storkey, and Charles Sutton. 2012. Continuous relaxations for discrete hamiltonian monte carlo. In _Advances in Neural Information Processing Systems_, volume 25.

Limitations

Approximating the base measure.In this work, we go one step closer to implementing a principled approach for text generation. However, in our text generation experiments, we follow the previous work in using uncorrected Langevin dynamics. Moreover, when implementing svs for text generation, we assume all Voronoi cells to have an equal base measure, which might not hold.

Efficiency.As reported in App. H, all gradient-based samplers are considerably slower than ancestral sampling and heuristics such as fudge. Further efforts are needed to make gradient-based methods faster.

Text Quality.As shown in previous work, sequences with high probability under the LM can be repetitive, dull, or degenerate [16]. Therefore, as with any other sampling method, svs might sample degenerate sentences, and the quality of the samples depends on the LM probability distribution. This is an active area of research with various proposals from changing the loss function during training [45], to modifying the decoding objective [28].

## Appendix B A Note on the Choice of the Gaussian Measure

We thank the reviewers and the meta reviewer for their careful assessment of this work and their thoughtful feedback. As a response to an important point raised by the meta reviewer, here we explain the reasoning behind using a Gaussian measure in Eq. (12). The gradient \(\mathbf{g}_{\bm{m}}\) is only defined at Voronoi centers and the goal is to follow a direction similar to \(\mathbf{g}_{\bm{m}}\) when we are in the vicinity of the Voronoi center, i.e., in the Voronoi cell. We note that this direction needs to be adjusted depending on the position in the Voronoi cell \(\mathbf{x}\). A natural choice for this would be to use a truncated Gaussian that is centered at \(\mathbf{g}_{\bm{m}}\), which implies that the gradient in \(\mathbf{x}\) should be \(\mathbf{g}_{\bm{m}}-\mathbf{x}\), to follow a similar direction \(\mathbf{g}_{\bm{m}}\) at the center of the Voronoi.

## Appendix C Proofs

### Proof of Proposition 1

**Proposition 1**.: _Let \(\bm{p}=[p_{1},\dots,p_{M}]\) be an embedding-augmented distribution with embeddings \(\{\mathbf{v}_{m}\}_{m=1}^{M}\subset\mathbb{R}^{d}\), and let \(p_{\mathrm{V}}\) be the corresponding Voronoi measure Eq. (7). Then, \(p_{\mathrm{V}}(C_{m})=p_{m}\) where \(C_{m}\) is defined as in Eq. (6). See App. C.1 for proof._

Proof.: \[p_{\mathrm{V}}(C_{m}) =\int_{C_{m}}p_{\mathrm{V}}(\mathbf{x})\mathrm{d}\lambda\] (19a) \[=\int_{C_{m}}\frac{p_{m}}{\mu(C_{m})}\frac{\mathrm{d}\mu}{ \mathrm{d}\lambda}(\mathbf{x})\mathrm{d}\lambda\] (19b) \[=\frac{p_{m}}{\mu(C_{m})}\int_{C_{m}}\frac{\mathrm{d}\mu}{ \mathrm{d}\lambda}(\mathbf{x})\mathrm{d}\lambda\] (19c) \[=\frac{p_{m}}{\mu(C_{m})}\mu(C_{m})=p_{m}\] (19d)

### Proof of Proposition 2

**Proposition 2**.: _Let \(\mu\) be a measure on \(\mathbb{R}^{d}\). Then, we have the product measure space as \(\mu(C_{\bm{m}})=\prod_{n=1}^{N}\mu(C_{m_{n}})\). See App. C.2 for proof._Proof.: Let \(\bm{m}=[m_{1},\dots,m_{N}]\in[M]^{N}\). We have

\[\mu(C_{\bm{m}}) =\mu\left(\prod_{n=1}^{N}C_{m_{n}}\right)\] (20a) \[=\prod_{n=1}^{N}\mu(C_{m_{n}})\] (20b)

### Proof of Proposition 3

**Proposition 3**.: _Eq. (13) is absolutely continuous with respect to the Lebesgue measure \(\lambda\). See App. C.3 for proof._

Proof.: Choose \(E\in\mathscr{B}(\mathbb{R}^{d})\) such that \(\lambda(E)=0\).14 Note that \(E=(E\setminus\mathcal{K})\cup\bigcup_{m=1}^{M}(E\cap C_{m})\) where \(\mu(E\setminus\mathcal{K})=0\) by Eq. (13). Since the Gaussian measure over \(\mathbb{R}^{d}\) itself is absolutely continuous with respect to \(\lambda\), we can also conclude that \(\mu(E\cap C_{m})=0\) for any \(m\). Hence, \(\mu(E)\leq\mu(E\setminus\mathcal{K})+\sum_{m}\mu(E\cap C_{m})=0\) which means \(\mu(E)=0\). So \(\mu\ll\lambda\). 

Footnote 14: We use \(\mathscr{B}(\cdot)\) to denote the standard Borel \(\sigma\)-algebra.

### Proof of Proposition 4

**Proposition 4**.: _The gradient of the log of the Voronoi measure \(p_{\mathrm{V}}\) is given by_

\[\nabla_{\mathbf{x}}\log p_{\mathrm{V}}(\mathbf{x})=\begin{cases}\mathbf{g}_{ \bm{m}}-\mathbf{x},&\textbf{if}\ \mathbf{x}\in\mathrm{int}(C_{\bm{m}})\\ \text{undefined},&\textbf{if}\ \mathbf{x}\in\partial C_{\bm{m}}\\ \mathbf{0},&\textbf{otherwise}\end{cases}\] (14)

_where the first two blocks in the case statement apply if there exists some \(\bm{m}\) such that \(\mathbf{x}\in\mathrm{int}(C_{\bm{m}})\) or \(\mathbf{x}\in\partial C_{\bm{m}}\). See App. C.4 for proof._

Proof.: We have three cases. Suppose \(\mathbf{x}\in\mathrm{int}(C_{m})\) for some \(m\in[M]\). Then, by Observation 1, we have that \(p_{\mathrm{V}}(\mathbf{x})\), and, thus, \(\log p_{\mathrm{V}}(\mathbf{x})\) is differentiable. Direct computation reveals:

\[\nabla_{\mathbf{x}}\log p_{\mathrm{V}}(\mathbf{x})=\underbrace{ \nabla_{\mathbf{x}}\log\frac{p_{m}}{\mu(C_{m})}}_{=0}+\nabla_{\mathbf{x}}\log \frac{\mathrm{d}\mu}{\mathrm{d}\lambda}\] (21a) \[=\nabla_{\mathbf{x}}\log\exp\left(-\frac{1}{2}||\mathbf{g}_{m}- \mathbf{x}||_{2}^{2}\right)-\underbrace{\nabla_{\mathbf{x}}\log\mu(C_{m})}_{=0}\] \[=-\frac{1}{2}\nabla_{\mathbf{x}}||\mathbf{g}_{m}-\mathbf{x}||_{2 }^{2}\] (21b) \[=\mathbf{g}_{m}-\mathbf{x}\] (21c)

Next, suppose \(\mathbf{x}\notin\bigcup_{m=1}^{M}C_{m}\). Then, the measure is zero, so the gradient is as well. Finally, we have that \(\mathbf{x}\in\partial C_{m}\) for some \(m\). Then, by the next Observation (Observation 1), we have that \(p_{\mathrm{V}}\) is discontinuous, so the derivative is not defined. 

**Observation 1**.: _A \((\mathcal{K},\mu)\)-Voronoi measure \(p_{\mathrm{V}}\) is differentiable with respect to \(p_{m}\) on the set \(\cup_{m=1}^{M}\mathrm{int}\left(C_{m}\right)\), and discontinuous on the set \(\mathcal{K}\setminus\cup_{m=1}^{M}\mathrm{int}\left(C_{m}\right)\), i.e., the union of the Voronoi cells' boundaries \(\cup_{m=1}^{M}\partial C_{m}\)._Properties of Hamiltonian Dynamics

Preservation of Measure.First of all, the Hamiltonian equations are volume- or measure-preserving. Intuitively, this means that if we move a region in the phase space along the dynamics for an arbitrary amount of time, the volume of the region would stay unchanged.15 Concretely, we can define a function

Footnote 15: This result is known as Liouvilleโs theorem in classical mechanics [2, ยง16]. Upon noticing that the Hamiltonian dynamics is divergenceless, one can interpret it as a simple application of the (higher-dimensional) divergence theorem, which itself is a consequence of the generalized Stokesโ theorem.

\[g^{t}:(\mathbf{x}(0),\mathbf{r}(0))\mapsto(\mathbf{x}(t),\mathbf{r}(t))\] (22)

as moving every point in the phase space along the Hamiltonian equations for some time \(t\).16 Then, using \(g^{t}\) as a change of variable would leave the underlying probability measure unchanged.

Footnote 17: We say a transformation taking \((\mathbf{x},\mathbf{r})\) to \((\mathbf{x}^{\prime},\mathbf{r}^{\prime})\) **conserves** Hamiltonian if \(H(\mathbf{x},\mathbf{r})=H(\mathbf{x}^{\prime},\mathbf{r}^{\prime})\).

Time Reversibility.Next, the Hamiltonian equations are time reversible, meaning that if the equations can move \((\mathbf{x},\mathbf{r})\) to \((\mathbf{x}^{\prime},\mathbf{r}^{\prime})\), then it would also take \((\mathbf{x}^{\prime},-\mathbf{r}^{\prime})\) to \((\mathbf{x},-\mathbf{r})\). Due to our choice of distribution of \(\mathbf{r}\) is always symmetric, these properties simplify the acceptance probability to be only the energy difference, i.e., we can ignore the conditional probabilities in the standard Metropolis-Hastings acceptance probability.

Conservation of Hamiltonian.Finally, we can quickly verify that the Hamiltonian \(H\) is conserved by the Hamiltonian dynamics:17

Footnote 17: This function is called the Hamiltonian phase flow, or simply Hamiltonian flow [2, ยง16]. Flow is a general and important notion in the study of differential equations and related subjects, in which some additive group acts on \((\mathbb{R},+)\). In the case of a smooth Hamiltonian \(H\), \(\{g^{t}\}_{t\in\mathbb{R}}\) can be seen a one-parameter group of diffeomorphisms over \(\mathbb{R}^{2d}\).

\[\frac{\mathrm{d}H}{\mathrm{d}t} =\sum_{i}\frac{\partial H}{\partial\mathbf{x}_{i}}\frac{\mathrm{ d}\mathbf{x}_{i}}{\mathrm{d}t}+\sum_{i}\frac{\partial H}{\partial \mathbf{r}_{i}}\frac{\mathrm{d}\mathbf{r}_{i}}{\mathrm{d}t}\] (23) \[=\sum_{i}\frac{\partial H}{\partial\mathbf{x}_{i}}\frac{\partial H }{\partial\mathbf{r}_{i}}-\sum_{i}\frac{\partial H}{\partial\mathbf{r}_{i}} \frac{\partial H}{\partial\mathbf{x}_{i}}\] (definition in Eq. ( 16 ) ) (24) \[=\sum_{i}\frac{\partial H}{\partial\mathbf{x}_{i}}\frac{\partial H }{\partial\mathbf{r}_{i}}-\sum_{i}\frac{\partial H}{\partial\mathbf{x}_{i}} \frac{\partial H}{\partial\mathbf{r}_{i}}\] (symmetry) (25) \[=0\] (26)

This shows that the acceptance probability will be exactly 1 if the Hamiltonian equations are integrated exactly. In practice, numerical errors will lead to fluctuations in \(H\). The Metropolis-Hastings acceptance probability ensures detailed balance.

## Appendix E Details of Voronoi Sampling

### Conservation of Hamiltonian in Refraction-Reflection

**Proposition 5**.: _A single step of refraction-reflection conserves the Hamiltonian._

Proof.: Suppose an instantaneous refraction-reflection takes \((\mathbf{x},\mathbf{r})\) to \((\mathbf{x}^{\prime},\mathbf{r}^{\prime})\). In other words, \((\mathbf{x},\mathbf{r})\) is the particle's coordinate and momentum at the boundary prior to the refraction-reflection, and \((\mathbf{x}^{\prime},\mathbf{r}^{\prime})\) is the particle's coordinate and momentum after the refraction-reflection, which includes instantaneous changes in its potential energy and momentum. Recall the refraction-reflection equation is

\[\mathbf{r}^{\prime}_{\perp}=\begin{cases}\sqrt{\|\mathbf{r}_{ \perp}\|_{2}^{2}-2\Delta U}\cdot\frac{\mathbf{r}_{\perp}}{\|\mathbf{r}_{\perp }\|_{2}^{2}}&\text{(refraction when $\|\mathbf{r}_{\perp}\|_{2}^{2}>2\Delta U $ )}\\ -\mathbf{r}_{\perp}&\text{(reflection when $\|\mathbf{r}_{\perp}\|_{2}^{2} \leq 2\Delta U$ )}\end{cases}\] (27a)

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_EMPTY:19]

\[\frac{\partial\mathbf{r}_{1}^{\prime}}{\partial\mathbf{x}_{1}} =\frac{\partial\sqrt{\mathbf{r}_{1}^{2}-2\Delta U(\mathbf{y})}}{ \partial\mathbf{x}_{1}}\] (by Eq. ( 35 )) **(41a)** \[=\frac{1}{2\sqrt{\mathbf{r}_{1}^{2}-2\Delta U(\mathbf{y})}}\frac{ \partial(\mathbf{r}_{1}^{2}-2\Delta U(\mathbf{y}))}{\partial\mathbf{x}_{1}}\] (chain rule) **(41b)** \[=-\frac{1}{\mathbf{r}_{1}^{\prime}}\frac{\partial\Delta U(\mathbf{y})}{ \partial\mathbf{x}_{1}},\] (41c)

and

\[\frac{\partial\mathbf{r}_{1}^{\prime}}{\partial\mathbf{r}_{1}} =\frac{\partial\sqrt{\mathbf{r}_{1}^{2}-2\Delta U(\mathbf{y})}}{ \partial\mathbf{r}_{1}}\] (by Eq. ( 35 )) **(42a)** \[=\frac{1}{2\sqrt{\mathbf{r}_{1}^{2}-2\Delta U(\mathbf{y})}}\frac{ \partial(\mathbf{r}_{1}^{2}-2\Delta U(\mathbf{y}))}{\partial\mathbf{r}_{1}}\] (chain rule) **(42b)** \[=\frac{\mathbf{r}_{1}}{\mathbf{r}_{1}^{\prime}}-\frac{1}{\mathbf{r }_{1}^{\prime}}\frac{\partial\Delta U(\mathbf{y})}{\partial\mathbf{r}_{1}}.\] (42c)

Then,

\[\det\nabla\gamma =\frac{\partial\mathbf{x}_{1}^{\prime}}{\partial\mathbf{x}_{1}} \frac{\partial\mathbf{r}_{1}^{\prime}}{\partial\mathbf{r}_{1}}-\frac{\partial \mathbf{x}_{1}^{\prime}}{\partial\mathbf{r}_{1}}\frac{\partial\mathbf{r}_{1}^ {\prime}}{\partial\mathbf{x}_{1}}\] (Eq. ( 34 )) \[=\left(\left(\underline{\varepsilon}+\frac{\mathbf{x}_{1}}{ \mathbf{r}_{1}}\right)\underline{\partial\mathbf{r}_{1}^{\prime}}+\frac{ \mathbf{r}_{1}^{\prime}}{\mathbf{r}_{1}}\right)\frac{\partial\mathbf{r}_{1}^ {\prime}}{\partial\mathbf{r}_{1}}-\left(\frac{\mathbf{x}_{1}\mathbf{r}_{1}^{ \prime}}{\mathbf{r}_{1}^{2}}+\left(\underline{\varepsilon}+\underline{ \mathbf{x}_{1}}\right)\underline{\partial\mathbf{r}_{1}^{\prime}}\right) \frac{\partial\mathbf{r}_{1}^{\prime}}{\partial\mathbf{x}_{1}}\] (Eq. ( 39 ) and ( 40 )) \[=\frac{\mathbf{r}_{1}^{\prime}}{\mathbf{r}_{1}}\left(\frac{ \partial\mathbf{r}_{1}^{\prime}}{\partial\mathbf{r}_{1}}+\frac{\mathbf{x}_{1}} {\mathbf{r}_{1}}\frac{\partial\mathbf{r}_{1}^{\prime}}{\partial\mathbf{x}_{1}}\right)\] (43c) \[=\frac{\mathbf{r}_{1}^{\prime}}{\mathbf{r}_{1}}\left(\frac{ \mathbf{r}_{1}}{\mathbf{r}_{1}^{\prime}}-\frac{1}{\mathbf{r}_{1}^{\prime}} \frac{\partial\Delta U(\mathbf{y})}{\partial\mathbf{r}_{1}}-\frac{\mathbf{x}_ {1}}{\mathbf{r}_{1}}\frac{1}{\mathbf{r}_{1}^{\prime}}\frac{\partial\Delta U( \mathbf{y})}{\partial\mathbf{x}_{1}}\right)\] (Eq. ( 41 ) and ( 42 )) \[=1-\frac{1}{\mathbf{r}_{1}}\left(\frac{\partial\Delta U(\mathbf{y})}{ \partial\mathbf{r}_{1}}+\frac{\mathbf{x}_{1}}{\mathbf{r}_{1}}\frac{\partial \Delta U(\mathbf{y})}{\partial\mathbf{x}_{1}}\right)\] (43e)

where

\[\frac{\partial\Delta U(\mathbf{y})}{\partial\mathbf{r}_{1}}+ \frac{\mathbf{x}_{1}}{\mathbf{r}_{1}}\frac{\partial\Delta U(\mathbf{y})}{ \partial\mathbf{x}_{1}}\] (44a) \[=\sum_{i}\frac{\partial\Delta U(\mathbf{y})}{\partial\mathbf{y}_ {i}}\frac{\partial\mathbf{y}_{i}}{\partial\mathbf{r}_{1}}+\frac{\mathbf{x}_{1}} {\mathbf{r}_{1}}\sum_{i}\frac{\partial\Delta U(\mathbf{y})}{\partial\mathbf{y}_ {i}}\frac{\partial\mathbf{y}_{i}}{\partial\mathbf{x}_{1}}\] (chain rule) (44b) \[=\sum_{i}\frac{\partial\Delta U(\mathbf{y})}{\partial\mathbf{y}_ {i}}\frac{\mathbf{x}_{1}\mathbf{r}_{i}}{\mathbf{r}_{1}^{2}}-\frac{\mathbf{x}_{1}} {\mathbf{r}_{1}}\sum_{i}\frac{\partial\Delta U(\mathbf{y})}{\partial\mathbf{y}_ {i}}\frac{\mathbf{r}_{i}}{\mathbf{r}_{1}}\] (Eq. ( 38 )) \[=0.\] (44d)

Hence, Eq. (43) and Eq. (44) together imply that

\[|\det\nabla\gamma|=1.\] (45)

So \(\gamma\) is measure-preserving in the case of refraction.

Reflection.In the case of reflection, \(\mathbf{r}_{1}^{2}\leq 2\Delta U(\mathbf{y})\) and

\[\mathbf{r}_{1}^{\prime}=-\mathbf{r}_{1}.\] (46)

As in the case of refraction, let \(\delta\) be the step size it takes to reach \(\mathbf{y}\). Then

\[\delta =-\frac{\mathbf{x}_{1}}{\mathbf{r}_{1}},\] (47) \[\mathbf{x}_{1}^{\prime} =(\varepsilon-\delta)\mathbf{r}_{1}^{\prime}=\left(\varepsilon+ \frac{\mathbf{x}_{1}}{\mathbf{r}_{1}}\right)(-\mathbf{r}_{1}^{\prime})=- \varepsilon\mathbf{r}_{1}^{\prime}-\mathbf{x}_{1}.\] (48)We can then directly calculate

\[\frac{\partial\mathbf{x}_{1}^{\prime}}{\partial\mathbf{x}_{1}} =-1,\;\frac{\partial\mathbf{x}_{1}^{\prime}}{\partial\mathbf{x}_{1}} =-\varepsilon,\] ( 49a ) \[\frac{\partial\mathbf{x}_{1}^{\prime}}{\partial\mathbf{x}_{1}} =0,\quad\frac{\partial\mathbf{x}_{1}^{\prime}}{\partial\mathbf{x}_ {1}} =-1.\] ( 49b )

Hence,

\[|\det\nabla\gamma|=|(-1)(-1)-0\cdot(-\varepsilon)|=1.\] (50)

So \(\gamma\) is measure preserving in the case of reflection as well.

#### e.2.3 Time Reversibility

**Proposition 8**.: _The refraction-reflection step is time-reversible._

Proof.: A refraction-reflection step is time-reversible means that, if a single step of refraction-reflection procedure takes \((\mathbf{x},\mathbf{r})\) to \((\mathbf{x}^{\prime},\mathbf{r}^{\prime})\), then it would also take \((\mathbf{x}^{\prime},-\mathbf{r}^{\prime})\) to \((\mathbf{x},-\mathbf{r})\). We can show this by considering each cases:

* Reflection: When reflection happens, \(\|\mathbf{r}_{\perp}\|_{2}^{2}\leq 2\Delta U\), and hence \(\|-\mathbf{r}_{\perp}^{\prime}\|_{2}^{2}=\|-\mathbf{r}_{\perp}\|_{2}^{2}=\| \mathbf{r}_{\perp}\|_{2}^{2}\leq 2\Delta U\), meaning that the reverse trajectory would be reflected as well;
* Refraction: If \(\Delta U>0\), then the reverse trajectory is crossing the boundary from the other side, seeing a sudden decrease in potential, and hence would be refracted and regain the magnitude of momentum lost in the forward step. If \(\Delta U<0\), then \(\|-\mathbf{r}_{\perp}^{\prime}\|_{2}^{2}>2\Delta U\) and hence the reverse trajectory would also be refracted, ending in the original momentum with the sign reversed.

Proposition 8, combined with time reversibility of leapfrog step as in basic hmc, shows that Voronoi sampling is time reversible. Hence we only need to use the Hamiltonian difference in the Metropolis-Hastings acceptance probability.

#### e.2.4 Detailed Balance

**Theorem 1**.: _A step of Structured Voronoi Sampling (Algorithm 4) satisfies the detailed balance._

Proof Sketch.: From measure preservation (App. E.2.1 and App. E.2.2), the change of variable as introduced by integrating Hamiltonian equations have Jacobian with absolute determinant 1 and hence can be omitted. From time reversibility (App. E.2.3), we only need to use the Hamiltonian difference in the Metropolis-Hastings acceptance probability. And, finally, by using a Metropolis-Hastings accepting step (lines 8 to 13) we ensure the detailed balance. 

### Related Work

Such an integrator scheme appears to be well-known in the computational physics community, dating back to as early as Jin and Wen [18], and quite possibly even earlier. Integration algorithms based on this idea continue to receive active research [44].

The first usage of such integrator with discrete stepin the context of MCMC appeared in Mohasel Afshar and Domke [30].18 Mohasel Afshar and Domke [30] primarily based their motivation on the optical reflection-refraction analogy. We note that, in fact, Hamiltonian mechanics originated from Hamilton's formulation of geometrical optics (or Hamiltonian optics) [22, SSVIII.7]. This connection, important to modern physics, should be interpreted with care in our context since momentum isn't a natural quantity in optical rays.

[MISSING_PAGE_FAIL:22]

### Efficiently Finding Discontinuities

As explained in SS6, the key insight in svs is to adjust the momentum when facing discontinuity in the potential function. Therefore, we first need to find discontinuities efficiently and then reflect/refrac on the boundary of the discontinuity. Remember that at each leapfrog step, we move from \(\mathbf{x}^{t}\) to \(\mathbf{x}^{t+1}=\mathbf{x}^{t}+\varepsilon\mathbf{r}\). To find discontinuities along this trajectory, we divide each leapfrog step into fractions of a step and check for discontinuity. Concretely, we only advance the embedding by a fraction \(\alpha\), i.e. \(\mathbf{x}^{\prime}=\mathbf{x}+\alpha\varepsilon\mathbf{r}\) (line 4 in Algorithm 6). Then, we check for discontinuity by looking at the difference between the potential energies. If we don't observe any changes in the potential function, we take the next fraction of a step. Otherwise, we find the boundary and adjust the momentum (line 10 in Algorithm 6). Note that in svs, finding the boundary is straightforward, and it is a hyperplane characterized by the normal vector, as the line goes through the two Voronoi centers on each side of the hyperplane.

```
0:\(\mathbf{x}^{t}\): current embeddings, \(\mathbf{r}\): current momentum, \(U\): potential function, \(\varepsilon\): step size, \(\alpha\): discontinuity step size
0: next sample: \(\mathbf{x}^{t+1}\)
1:\(\tau\gets 0\)
2:\(\mathbf{x}\leftarrow\mathbf{x}^{t}\)
3:while\(\tau<1\) :
4:\(\mathbf{x}^{\prime}\leftarrow\mathbf{x}+\alpha\varepsilon\mathbf{r}\)
5:\(\Delta U\gets U(\mathbf{V}_{\boldsymbol{m}^{*}(\mathbf{x}^{\prime})})-U( \mathbf{V}_{\boldsymbol{m}^{*}(\mathbf{x})})\)
6:if\(\Delta U=0\) : \(\triangleright\) there is no discontinuity
7:\(\mathbf{x}\leftarrow\mathbf{x}^{\prime}\)
8:\(\tau\leftarrow\tau+\alpha\)
9:else
10:\(\boldsymbol{b},\alpha^{\prime}\leftarrow\textsc{FindB}(\mathbf{x},\mathbf{x}^ {\prime})\)\(\triangleright\) Returns the intersection \(\alpha^{\prime}\) and the normal vector of the boundary \(\boldsymbol{b}\)
11:\(\mathbf{x}\leftarrow\mathbf{x}+\alpha^{\prime}\varepsilon\mathbf{r}\)
12:\(\tau\leftarrow\tau+\alpha^{\prime}\)
13:\(\mathbf{r}\leftarrow\textsc{RefractReject}(\mathbf{r},\boldsymbol{b},\Delta U)\)
14:return\(\mathbf{x}^{t+1}\leftarrow\mathbf{x}\) ```

**Algorithm 6** Find Discontinuity

## Appendix G Dataset Statistics

This dataset is made available under the CC BY-SA 4.0 license. Our use of this dataset is for i) fine-tuning GPT-2, and ii) training classifiers for topic control, which clearly matches the intended use of this dataset mentioned by the creators as end-to-end training and generating text with content selection. A summary of dataset statistics can be found in Table 2.

## Appendix H Experimental Details

Sampling algorithms.Hyperparameters for each experiment are reported in Table 4. Following prior work [46], in algorithms based on Langevin dynamics, we apply an exponential decay to the step size by decreasing it to 0.05 after 500 steps. In all settings, we take 500 burn-in steps.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline Split & Chinese & English & Fast food & French & Indian & Italian & Japanese & Total \\ \hline train & \(2929\) & \(4100\) & \(5891\) & \(5889\) & \(4412\) & \(5909\) & \(5996\) & \(42061\) \\ valid & \(1489\) & \(1780\) & - & - & - & - & - & \(4672\) \\ test & \(492\) & \(613\) & \(632\) & \(639\) & \(497\) & \(608\) & \(638\) & \(4693\) \\ \hline \end{tabular}
\end{table}
Table 2: Number of restaurant reviews in each split and food type. Note that some reviews do not represent any specific food type, therefore, the total count is bigger than the sum count of reviews in each food category.

Food Classifiers.We train 3 classifiers. First, for experiments with fudge, we follow the experimental detail as in the original paper [48] and train a \(3\)-layered BiLSTM classifier with \(0.5\) dropout. The hidden dimension is set to \(300\), and the embedding layer is trained from scratch. Second, in experiments with MuCoLa, langevin, and svs, to make the setup as close as to fudge, we train a \(3\)-layered BiLSTM classifier with \(0.5\) dropout. However, this time the BiLSTM is trained on top of _frozen_ GPT-2 representations, thus sharing the embedding layer that is necessary for these methods to work. Finally, to evaluate the success of the methods in following the control targets, we finetune a RoBERTa [25] base model. The accuracy of all the classifiers and the number of trained parameters are reported in Table 5. We train all the models on a single gtx_1080_ti GPU with approximately 2 hours of total computational budget.

Inference times.We report inference times based on sec/batch. The number of decoded sentences per batch depends on the GPU memory and the size of the model. As depicted in table 3, using Voronoi measures does _not_ increase the inference time (compare MuCoLa and langevin). We observe that svs inference time is longer, because of the extra momentum adjustment steps. However, one can reduce the inference time by increasing \(\alpha\).

## Appendix I More Experiments on The Toy Model

To better understand the advantages of Voronoi sampling over hmc or MuCoLa, we further look at the JS divergence between the reference distribution and the distribution of samples when increasing the number of iterations. As depicted in Fig. 4, while the divergence decreases with more iterations across all sampling methods, Voronoi sampling converges to the reference distribution with fewer iterations. We then look at the distribution of sampled elements in Fig. 5. We observe that with \(100\) iterations, MuCoLa undersamples the element with the maximum probability while oversampling other elements. Finally, we extend the toy model from \(4\) squared cells in \(\mathbb{R}^{2}\) to \(2^{k}\) hypercube cells in \(\mathbb{R}^{k}\) (Fig. 6). As the dimensionality increases, the divergence between the samples' distribution and the reference distribution also increases in all sampling methods. Importantly, Voronoi sampling consistently converges faster across different values for \(k\).

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{2}{c}{Toy Example} & \multicolumn{2}{c}{LM Sampling} & \multicolumn{3}{c}{Controlled Generation} \\ \cline{2-7}  & \(\varepsilon\) & \(\alpha\) & \(\varepsilon\) & \(\alpha\) & \(\varepsilon\) & \(\alpha\) & \(\gamma\) \\ \hline vs & \(0.1\) & \(0.1\) & - & - & - & - & - \\ svs & \(0.1\) & \(0.1\) & \(1.\) & \(0.4\) & \(1.5\) & \(0.3\) & \(1.5\) \\ hmc & \(0.1\) & \(0.1\) & - & - & - & - & - \\ langevin & - & - & \(1.\) & - & \(1.5\) & - & \(1.5\) \\ MuCoLa & \(0.1\) & \(0.1\) & \(1.\) & - & \(1.\) & - & \(2.\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyperparameters used in sampling algorithms.

\begin{table}
\begin{tabular}{l c c} \hline \hline method & sec/batch \\ \hline fudge & \(10\) \\ MuCoLa & \(30\) \\ Langevin (ours) & \(31\) \\ svs (ours) & \(84\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Inference time for different methods on the controlled generation experiments. All experiments are done on a single A100-40GB GPU.

\begin{table}
\begin{tabular}{l c c c} \hline \hline model & f1-score & precision & recall & \# params \\ \hline BiLSTM & \(0.87\) & \(0.87\) & \(0.87\) & \(17\)M \\ BiLSTMProbe & \(0.84\) & \(0.84\) & \(0.84\) & \(37\)M \\ RoBERTa & \(0.90\) & \(0.91\) & \(0.90\) & \(124\)M \\ \hline BiLSTMProbe & \(0.90\) & \(0.90\) & \(0.90\) & \(878\)M \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance of food classifiers, and their number of learnable parameters, used in controlled generation experiment. All classifiers are trained and tested on E2E dataset.

[MISSING_PAGE_EMPTY:25]

Figure 5: Comparing the distribution of sampled elements at temperature \(0.25\). With \(100\) iterations, MuCoLa undersamples the element with the highest probability while oversampling other elements.

Figure 6: Comparing the distribution of sampled elements with the true distribution after \(100\) iterations, at temperature \(0.5\). There are \(2^{k}\) Voronoi cells with equal base measures in \(\mathbb{R}^{k}\), where elements to sample from are located at the center of each Voronoi cell. Voronoi sampling converges faster to the true distribution across all \(k\) values. As the dimensionality of the sample space increases, the divergence of all methods increases.

## 6 Conclusion

Figure 7: Evaluation of different sampling methods on restaurant review generation, along 6 axes: mean and standard deviation of negative perplexity\({}^{19}\)(-ppl-mean \(\uparrow\), -ppl-std \(\uparrow\)), the percentage of generated sentences adhering to the control target (success \(\uparrow\)), and diversity metrics (dist-1 \(\uparrow\), dist-2 \(\uparrow\), dist-3 \(\uparrow\)). For most control targets, svs achieves the highest success rate, with relatively low perplexity.

**Chinese**

\begin{table}
\begin{tabular}{l l} \multicolumn{2}{c}{**Chinese**} \\ \multicolumn{2}{c}{fudge} & In the city centre near Yippee Noodle Bar Chinese, is Aluminum. It has moderate prices and \\ MuCoLa & and has a 1 out of 5. It has food and high customer rating. The Rice Boat is \\ Langevin & It serves Chinese food with a low customer rating. The fast food and restaurant The Golden Curry is a \\ svs & It has a low customer rating and a price. The highly rated Chinese restaurant The Phoenix has a high \\ \hline \multicolumn{2}{c}{**English**} \\ \multicolumn{2}{c}{fudge} & It has an average customer Rating. Bibimbap House has English food in the riverside area near \\ MuCoLa & and has a low customer rating. The Golden Curry is a children friendly, serving English food, with \\ Langevin & It has low rating and is located near the to the city centre. The Phoenix is a English food \\ svs & Aluminum in the city centre near the a moderate price range. It serves English food, is \\ \hline \multicolumn{2}{c}{**Fast food**} \\ \multicolumn{2}{c}{fudge} & A fast food, coffee shop, Strada has a low customer rating, has a price range of over \(\xi 30\). It is \\ MuCoLa & and is family friendly and serves fast food. The Wrestlers is a fast food coffee shop in the \\ langevin & It is located near the riverside, is a cheap family friendly fast food restaurant, and is called \\ svs & It is located near the river. The Mill is a cheap, fast food and coffee shop near the \\ \hline \multicolumn{2}{c}{**French**} \\ \multicolumn{2}{c}{fudge} & It has a low-priced Inn French food. It is near Cafe Rouge.The Aluminum is a kid friendly fast food \\ MuCoLa & The French restaurant The Waterman is located in the city centre. The price range is less than \\ langevin & It is a restaurant located in the riverside, the restaurant, offers French food with a price \\ svs & It is a family restaurant that serves French food with a price \\ \hline \multicolumn{2}{c}{**Indian**} \\ \multicolumn{2}{c}{fudge} & The Phoenix Indian restaurant has moderate prices with a 3 out of 5 rating. Located on the \\ MuCoLa & It is in the city and has a low customer rating. The Waterman is a low priced \\ langevin & It is not child friendly and it is near the river. It serves Indian food and a customer rating \\ svs & It is located in the city centre near The Portland Arms Indian food and has a low customer rating. \\ \hline \multicolumn{2}{c}{**Italian**} \\ \multicolumn{2}{c}{fudge} & It has family Italian food and has a low a moderate price range. The Rice Boat has an average \\ MuCoLa & is a high priced Italian food restaurant with a customer rating of average. The Phoenix is a high \\ langevin & It is located in the city centre, it is not family friendly and is a coffee shop serving Italian \\ svs & It is located in the the city centre near The Portland Arms.The Eagle is an Italian restaurant. \\ \hline \multicolumn{2}{c}{**Japanese**} \\ \multicolumn{2}{c}{fudge} & Japanese food. Its customer rating is 3 out of 5.The Phoenix is Japanese in the city centre \\ MuCoLa & for Japanese food is located in the city centre. It has a low customer rating. The Golden \\ langevin & It is located in the riverside. It is located in the riverside. It is a Japanese food. It is a pub restaurant \\ svs & It is located in the riverside. It is a low rated Japanese restaurant, and coffee shop. \\ \hline \end{tabular}
\end{table}
Table 7: Examples of sampled sentences from different control food targets.