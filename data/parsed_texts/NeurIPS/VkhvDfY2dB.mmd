# Efficient Exploration in Continuous-time

Model-based Reinforcement Learning

 Lenart Treven

ETH Zurich

trevenl@ethz.ch &Jonas Hubotter

ETH Zurich

jhuebotter@student.ethz.ch &Bhavya Sukhija

ETH Zurich

sukhijab@ethz.ch &Florian Dorfler

ETH Zurich

dorfler@ethz.ch &Andreas Krause

ETH Zurich

krausea@ethz.ch

###### Abstract

Reinforcement learning algorithms typically consider discrete-time dynamics, even though the underlying systems are often continuous in time. In this paper, we introduce a model-based reinforcement learning algorithm that represents continuous-time dynamics using nonlinear ordinary differential equations (ODEs). We capture epistemic uncertainty using well-calibrated probabilistic models, and use the optimistic principle for exploration. Our regret bounds surface the importance of the _measurement selection strategy_ (MSS), since in continuous time we not only must decide how to explore, but also _when_ to observe the underlying system. Our analysis demonstrates that the regret is sublinear when modeling ODEs with Gaussian Processes (GP) for common choices of MSS, such as equidistant sampling. Additionally, we propose an _adaptive_, data-dependent, practical MSS that, when combined with GP dynamics, also achieves sublinear regret with significantly fewer samples. We showcase the benefits of continuous-time modeling over its discrete-time counterpart, as well as our proposed _adaptive_ MSS over standard baselines, on several applications.

## 1 Introduction

Real-world systems encountered in natural sciences and engineering applications, such as robotics (Spong et al., 2006), biology (Lenhart and Workman, 2007; Jones et al., 2009), medicine (Panetta and Fister, 2003), etc., are fundamentally continuous in time. Therefore, ordinary differential equations (ODEs) are the natural modeling language. However, the reinforcement learning (RL) community predominantly models problems in discrete time, with a few notable exceptions (Doya, 2000; Yildiz et al., 2021; Lutter et al., 2021). The discretization of continuous-time systems imposes limitations on the application of state-of-the-art RL algorithms, as they are tied to specific discretization schemes.

Discretization of continuous-time systems sacrifices several essential properties that could be preserved in a continuous-time framework (Nesic and Postoyan, 2021). For instance, exact discretization is only feasible for linear systems, leading to an inherent discrepancy when using standard discretization techniques for nonlinear systems. Additionally, discretization obscures the inter-sample behavior of the system, changes stability properties, may result in uncontrollable systems, or requires an excessively high sampling rate. Multi-time-scale systems are particularly vulnerable to these issues (Engquist et al., 2007). In many practical scenarios, the constraints imposed by discrete-time modeling are undesirable. Discrete-time models do not allow for the independent adjustment of measurement and control frequencies, which is crucial for real-world systems that operate in different regimes. For example, in autonomous driving, low-frequency sensor samplingand control actuation may suffice at slow speeds, while high-speed driving demands faster control. How to choose aperiodic measurements and control is studied in the literature on the event and self-triggered control (Astrom and Bernhardsson, 2002; Anta and Tabuada, 2010; Heemels et al., 2012). They show that the number of control inputs can be significantly reduced by using aperiodic control. Moreover, data from multiple sources is often collected at varying frequencies (Ghysels et al., 2006) and is often not even equidistant in time. Discrete-time models struggle to exploit this data for learning, while continuous-time models can naturally accommodate it.

Continuous-time modeling also offers the flexibility to determine optimal measurement times based on need, in contrast to the fixed measurement frequency in discrete-time settings, which can easily miss informative samples. This advantage is particularly relevant in fields like medicine, where patient monitoring often requires higher frequency measurements during the onset of illness and lower frequency measurements as the patient recovers (Kaandorp and Koole, 2007). At the same time, in fields such as medicine, measurements are often costly, and hence, it is important that the most informative measurements are selected. Discrete-time models, limited to equidistant measurements, therefore often result in suboptimal decision-making and their sample efficiency is fundamentally limited. In summary, continuous-time modeling is agnostic to the choice of measurement selection strategy (MSS), whereas discrete-time modeling often only works for an equidistant MSS.

ContributionsGiven the advantages of continuous-time modeling, in this work, we propose an optimistic continuous-time model-based RL algorithm - OCoRL. Moreover, we theoretically analyze OCoRL and show a general regret bound that holds for any MSS. We further show that for common choices of MSSs, such as equidistant sampling, the regret is sublinear when we model the dynamics with GPs (Williams and Rasmussen, 2006). To our knowledge, we are the first to give a no-regret algorithm for a rich class of nonlinear dynamical systems in the continuous-time RL setting. We further propose an _adaptive_ MSS that is practical, data-dependent, and requires considerably fewer measurements compared to the equidistant MSS while still ensuring the sublinear regret for the GP case. Crucial to OCoRL is the exploration induced by the _optimism in the face of uncertainty_ principle for model-based RL, which is commonly employed in the discrete-time realm (Auer et al., 2008; Curi et al., 2020). We validate the performance of OCoRL on several robotic tasks, where we clearly showcase the advantages of continuous-time modeling over its discrete-time counterpart. Finally, we provide an efficient implementation1 of OCoRL in JAX (Bradbury et al., 2018).

Footnote 1: https://github.com/lenarttreven/ocorl

## 2 Problem setting

In this work, we study a continuous-time deterministic dynamical system \(\bm{f}^{*}\) with initial state \(\bm{x}_{0}\in\mathcal{X}\subset\mathbb{R}^{d_{x}}\), i.e.,

\[\bm{x}(t)=\bm{x}_{0}+\int_{0}^{t}\bm{f}^{*}\left(\bm{x}(s),\bm{u}(s)\right)\, ds.\]

Here \(\bm{u}:[0,\infty)\rightarrow\mathbb{R}^{d_{u}}\) is the input we apply to the system. Moreover, we consider state feedback controllers represented through a policy \(\bm{\pi}:\mathcal{X}\rightarrow\mathcal{U}\subset\mathbb{R}^{d_{u}}\), that is, \(\bm{u}(s)=\bm{\pi}(\bm{x}(s))\). Our objective is to find the optimal policy with respect to a given running cost function \(c:\mathbb{R}^{d_{x}}\times\mathbb{R}^{d_{u}}\rightarrow\mathbb{R}\). Specifically, we are interested in solving the following optimal control (OC) problem over the policy space \(\Pi\):

\[\begin{split}\bm{\pi}^{*}\stackrel{{\text{def}}}{{=} }\operatorname*{argmin}_{\bm{\pi}\in\Pi}C(\bm{\pi},\bm{f}^{*})=\operatorname* {argmin}_{\bm{\pi}\in\Pi}\int_{0}^{T}c(\bm{x},\bm{\pi}(\bm{x}))\,dt\\ \text{s.t.}\quad\dot{\bm{x}}=\bm{f}^{*}(\bm{x},\bm{\pi}(\bm{x})), \quad\bm{x}(0)=\bm{x}_{0}.\end{split}\] (1)

The function \(\bm{f}^{*}\) is _unknown_, but we can collect data over episodes \(n=1\ldots,N\) and learn about the system by deploying a policy \(\bm{\pi}_{n}\in\Pi\) for the horizon of \(T\) in episode \(n\). In an (overly) idealized continuous time setting, one can measure the system at any time step. However, we consider a more practical setting, where we assume taking measurements is costly, and therefore want as few measurements as necessary. To this end, we formally define a measurement selection strategy below.

**Definition 1** (Measurement selection strategy).: _A measurement selection strategy \(S\) is a sequence of sets \((S_{n})_{n\geq 1}\), such that \(S_{n}\) contains \(m_{n}\) points at which we take measurements, i.e., \(S_{n}\subset[0,T],|S_{n}|=m_{n}\).2_

Footnote 2: Here, the set \(S_{n}\) may depend on observations prior to episode \(n\) or is even constructed while we execute the trajectory. For ease of notation, we do not make this dependence explicit.

During episode \(n\), given a policy \(\bm{\pi}_{n}\) and a MSS \(S_{n}\), we collect a dataset \(\mathcal{D}_{n}\sim(\bm{\pi}_{n},S_{n})\). The dataset is defined as

\[\mathcal{D}_{n} \stackrel{{\text{def}}}{{=}}\{(\bm{z}_{n}(t_{n,i}), \dot{\bm{y}}_{n}(t_{n,i}))\mid t_{n,i}\in S_{n},i\in\{1\dots,m_{n}\}\}\qquad \text{where}\] \[\bm{z}_{n}(t_{n,i}) \stackrel{{\text{def}}}{{=}}(\bm{x}_{n}(t_{n,i}), \bm{\pi}_{n}(\bm{x}_{n}(t_{n,i}))),\quad\dot{\bm{y}}_{n}(t_{n,i})\stackrel{{ \text{def}}}{{=}}\dot{\bm{x}}_{n}(t_{n,i})+\bm{\epsilon}_{n,i}.\]

Here \(\bm{x}_{n}(t),\dot{\bm{x}}_{n}(t)\) are state and state derivative in episode \(n\), and \(\bm{\epsilon}_{n,i}\) is i.i.d, \(\sigma\)-sub-Gaussian noise of the state derivative observations. Note, even though in practice only the state \(\bm{x}(t)\) might be observable, one can estimate its derivative \(\dot{\bm{x}}(t)\) (e.g., using finite differences, interpolation methods, etc. Cullum (1971); Knowles and Wallace (1995); Chartrand (2011); Knowles and Renka (2014); Wagner et al. (2018); Treven et al. (2021)). We capture the noise in our measurements and/or estimation of \(\dot{\bm{x}}(t)\) with \(\bm{\epsilon}_{i,n}\).

In summary, at each episode \(n\), we deploy a policy \(\bm{\pi}_{n}\) for a horizon of \(T\), observe the system according to a proposed MSS \(S_{n}\), and learn the dynamics \(\bm{f}^{*}\). By deploying \(\bm{\pi}_{n}\) instead of the optimal policy \(\bm{\pi}^{*}\), we incur a regret,

\[r_{n}(S)\stackrel{{\text{def}}}{{=}}C(\bm{\pi}_{n},\bm{f}^{*})- C(\bm{\pi}^{*},\bm{f}^{*}).\]

Note that the policy \(\bm{\pi}_{n}\) depends on the data \(\mathcal{D}_{1:n-1}=\cup_{i<n}\mathcal{D}_{i}\) and hence implicitly on the MSS \(S\).

Performance measureWe analyze OCoRL by comparing it with the performance of the best policy \(\bm{\pi}^{*}\) from the class \(\Pi\). We evaluate the _cumulative regret_\(R_{N}(S)\stackrel{{\text{def}}}{{=}}\sum_{n=1}^{N}r_{n}(S)\) that sums the gaps between the performance of the policy \(\bm{\pi}_{n}\) and the optimal policy \(\bm{\pi}^{*}\) over all the episodes. If the regret \(R_{N}(S)\) is sublinear in \(N\), then the average cost of the policy \(C(\bm{\pi}_{n},\bm{f}^{*})\) converges to the optimal cost \(C(\bm{\pi}^{*},\bm{f}^{*})\).

### Assumptions

Any meaningful analysis of cumulative regret for continuous time, state, and action spaces requires some assumptions on the system and the policy class. We make some continuity assumptions, similar to the discrete-time case (Khalil, 2015; Curi et al., 2020; Sussex et al., 2023), on the dynamics, policy, and cost.

**Assumption 1** (Lipschitz continuity).: _Given any norm \(\left\lVert\cdot\right\rVert\), we assume that the system dynamics \(\bm{f}^{*}\) and cost \(c\) are \(L_{\bm{f}}\) and \(L_{c}\)-Lipschitz continuous, respectively, with respect to the induced metric. Moreover, we define \(\Pi\) to be the policy class of \(L_{\bm{\pi}}\)-Lipschitz continuous policy functions and \(\mathcal{F}\) a class of \(L_{\bm{f}}\) Lipschitz continuous dynamics functions with respect to the induced metric._

We learn a model of \(\bm{f}^{*}\) using data collected from the episodes. For a given state-action pair \(\bm{z}=(\bm{x},\bm{u})\), our learned model predicts a mean estimate \(\bm{\mu}_{n}(\bm{z})\) and quantifies our epistemic uncertainty \(\bm{\sigma}_{n}(\bm{z})\) about the function \(\bm{f}^{*}\).

**Definition 2** (Well-calibrated statistical model of \(\bm{f}^{*}\), Rothfuss et al. (2023)).: _Let \(\mathcal{Z}\stackrel{{\text{def}}}{{=}}\mathcal{X}\times\mathcal{U}\). An all-time well-calibrated statistical model of the function \(\bm{f}^{*}\) is a sequence \(\{\mathcal{M}_{n}(\delta)\}_{n\geq 0}\), where_

\[\mathcal{M}_{n}(\delta)\stackrel{{\text{def}}}{{=}}\left\{\bm{f}: \mathcal{Z}\rightarrow\mathbb{R}^{d_{x}}\mid\forall\bm{z}\in\mathcal{Z}, \forall j\in\{1,\dots,d_{x}\}:|\mu_{n,j}(\bm{z})-f_{j}(\bm{z})|\leq\beta_{n}( \delta)\sigma_{n,j}(\bm{z})\right\},\]

_if, with probability at least \(1-\delta\), we have \(\bm{f}^{*}\in\bigcap_{n\geq 0}\mathcal{M}_{n}(\delta)\). Here, \(\mu_{n,j}\) and \(\sigma_{n,j}\) denote the \(j\)-th element in the vector-valued mean and standard deviation functions \(\bm{\mu}_{n}\) and \(\bm{\sigma}_{n}\) respectively, and \(\beta_{n}(\delta)\in\mathbb{R}_{\geq 0}\) is a scalar function that depends on the confidence level \(\delta\in(0,1]\) and which is monotonically increasing in \(n\)._

**Assumption 2** (Well-calibration).: _We assume that our learned model is an all-time well-calibrated statistical model of \(\bm{f}^{*}\). We further assume that the standard deviation functions \((\bm{\sigma}_{n}(\cdot))_{n\geq 0}\) are \(L_{\bm{\sigma}}\)-Lipschitz continuous._This is a natural assumption, which states that we are with high probability able to capture the dynamics within a confidence set spanned by our predicted mean and epistemic uncertainty. For example, GP models are all-time well-calibrated for a rich class of functions (c.f., Section 3.2) and also satisfy the Lipschitz continuity assumption on \((\bm{\sigma}_{n}(\cdot))_{n\geq 0}\)(Rothfuss et al., 2023). For Bayesian neural networks, obtaining accurate uncertainty estimates is still an open and active research problem. However, in practice, re-calibration techniques (Kuleshov et al., 2018) can be used.

By leveraging these assumptions, in the next section, we propose our algorithm OCoRL and derive a generic bound on its cumulative regret. Furthermore, we show that OCoRL provides sublinear cumulative regret for the case when GPs are used to learn \(\bm{f}^{*}\).

## 3 Optimistic Continuous-time RL Algorithm

``` **OnCoRL:** Optimistic Continuous-time RL

``` **Init:** Statistical model \(\mathcal{M}_{0}\), Simulator Sim, MSS \(S\), Probability \(\delta\) for episode \(n=1,\dots,N\)do \[\bm{\pi}_{n}=\operatorname*{argmin}_{\bm{\pi}\in\Pi}\min_{\bm{f} \in\mathcal{M}_{n-1}\cap\mathcal{F}}C(\bm{\pi},\bm{f})\] /* Select optimistic policy /* \[\mathcal{D}_{n}=\{(\bm{z}_{n}(t_{n,i}),\dot{\bm{y}}_{n}(t_{n,i})) \mid t_{n,i}\in S_{n}\}\leftarrow\textsc{Sim}(\bm{\pi}_{n},S_{n})\] /* Measurement collection /* \[\mathcal{M}_{n}\leftarrow(\bm{\mu}_{n},\bm{\sigma}_{n},\beta_{n}( \delta))\leftarrow\mathcal{D}_{1:n}\] /* Update statistical model/* ```

**Optimistic policy selection** There are several strategies we can deploy to trade-off exploration and exploitation, e.g., dithering (\(\varepsilon\)-greedy, Boltzmann exploration (Sutton and Barto, 2018)), Thompson sampling (Osband et al., 2013), upper-confidence RL (UCRL) (Auer et al., 2008), etc. OCoRL is a continuous-time variant of the Hallucinated UCRL strategy introduced by Chowdhury and Gopalan (2017); Curi et al. (2020). In episode \(n\), the optimistic policy is obtained by solving the optimal control problem:

\[(\bm{\pi}_{n},\bm{f}_{n})\stackrel{{\text{def}}}{{=}}\operatorname* {argmin}_{\bm{\pi}\in\Pi,\ \bm{f}\in\mathcal{M}_{n-1}\cap\mathcal{F}}C(\bm{\pi},\bm{f})\] (2)

Here, \(\bm{f}_{n}\) is a dynamical system such that the cost by controlling \(\bm{f}_{n}\) with its optimal policy \(\bm{\pi}_{n}\) is the lowest among all the plausible systems from \(\mathcal{M}_{n-1}\cap\mathcal{F}\). The optimal control problem (2) is infinite-dimensional, in general nonlinear, and thus hard to solve. For the analysis, we assume we can perfectly solve it. In Appendix B, we present details on how we solve it in practice. Specifically, in Appendix B.1, we show that our results seamlessly extend to the setting where the optimal control problem of Equation (1) is discretized w.r.t. an _arbitrary_ choice of discretization. Moreover, in this setting, we show that theoretical guarantees can be derived without restricting the models to \(\bm{f}\in\mathcal{M}_{n-1}\cap\mathcal{F}\). Instead, a practical optimization over all models in \(\mathcal{M}_{n-1}\) can be performed as in Curi et al. (2020); Pasztor et al. (2021).

Model complexityWe expect that the regret of any model-based continuous-time RL algorithm depends both on the hardness of learning the underlying true dynamics model \(\bm{f}^{*}\) and the MSS. To capture both, we define the following model complexity:

\[\mathcal{I}_{N}(\bm{f}^{*},S)\stackrel{{\text{def}}}{{=}} \max_{\begin{subarray}{c}\bm{\pi}_{1},\dots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}\sum_{n=1}^{N}\int_{0}^{T}\left\lVert\bm{ \sigma}_{n-1}\left(\bm{z}_{n}(t)\right)\right\rVert^{2}\,dt.\] (3)

The model complexity measure captures the hardness of learning the dynamics \(\bm{f}^{*}\). Intuitively, for a given \(N\), the more complicated the dynamics \(\bm{f}^{*}\), the larger the epistemic uncertainty and thereby the model complexity. For the discrete-time setting, the integral is replaced by the sum over the uncertainties on the trajectories (c.f., Equation (8) of Curi et al. (2020)). In the continuous-time setting, we do not observe the state at every time step, but only at a finite number of times wherever the MSS \(S\) proposes to measure the system. Accordingly, \(S\) influences how we collect data and update our calibrated model. Therefore, the model complexity depends on \(S\). Next, we first present the regret bound for general MSSs, then we look at particular strategies for which we can show convergence of the regret.

**Proposition 1**.: _Let \(S\) be any MSS. If we run OCoRL, we have with probability at least \(1-\delta\),_

\[R_{N}(S)\leq 2\beta_{N}L_{c}(1+L_{\bm{\pi}})T^{\frac{3}{2}}e^{L_{\bm{f}}(1+L_{ \bm{\pi}})T}\sqrt{N\mathcal{I}_{N}(\bm{f}^{*},S)}.\] (4)

We provide the proof of Proposition 1 in Appendix A. Because we have access only to the statistical model and any errors in dynamics compound (continuously in time) over the episode, the regret \(R_{N}(S)\) depends exponentially on the horizon \(T\). This is in line with the prior work in the discrete-time setting (Curi et al., 2020). If the model complexity term \(\mathcal{I}_{N}(\bm{f}^{*},S)\) and \(\beta_{N}\) grow at a rate slower than \(N\), the regret is sublinear and the average performance of OCoRL converges to \(C(\bm{\pi}^{*},\bm{f}^{*})\). In our analysis, the key step to show sublinear regret for the GP dynamics model is to upper bound the integral of uncertainty in the model complexity with the sum of uncertainties at the points where we collect the measurements. In the next section, we show how this can be done for different measurement selection strategies.

### Measurement selection strategies (MSS)

In the following, we present different natural MSSs and compare the number of measurements they propose per episode. Formal derivations are included in Appendix A.2.

OracleIntuitively, if we take measurements at the times when we are the most uncertain about dynamics on the executed trajectory, i.e., when \(\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t)\right)\right\|\) is largest, we gain the most knowledge about the true function \(\bm{f}^{*}\). Indeed, when the statistical model is a GP and noise is homoscedastic and Gaussian, observing the most uncertain point on the trajectory leads to the maximal reduction in entropy of \(\bm{f}^{*}(\bm{z}_{n}(t))\) (c.f., Lemma 5.3 of Srinivas et al. (2009)). In the ideal case, we can define an MSS that collects only the point with the highest uncertainty in every episode, i.e., \(S_{n}^{\text{ORA def}}\overset{\text{def}}{=}\{t_{n,1}\}\) where \(t_{n,1}\overset{\text{def}}{=}\operatorname*{argmax}_{0\leq t\leq T}\left\| \bm{\sigma}_{n-1}\left(\bm{z}_{n}(t)\right)\right\|^{2}\). For this MSS we bound the integral over the horizon \(T\) with the maximal value of the integrand times the horizon \(T\):

\[\mathcal{I}_{N}(\bm{f}^{*},S_{n}^{\text{ORA}})\leq\max_{\begin{subarray}{c} \bm{\pi}_{1},\dots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}T\sum_{n=1}^{N}\left\|\bm{\sigma}_{n-1} \left(\bm{z}_{n}(t_{n,1})\right)\right\|^{2}\] (5)

The _oracle_ MSS collects only one measurement per episode, however, it is impractical since it requires knowing the most uncertain point on the true trajectory a priori, i.e., before executing the policy.

EquidistantAnother natural MSS is the equidistant MSS. We collect \(m_{n}\) equidistant measurements in episode \(n\) and upper bound the integral with the upper Darboux integral.

\[\mathcal{I}_{N}(\bm{f}^{*},S_{n}^{\text{ROI}})\leq\max_{ \begin{subarray}{c}\bm{\pi}_{1},\dots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}\sum_{n=1}^{N}\frac{T}{m_{n}}\sum_{i=1}^{m_{n }}\left(\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t_{n,i})\right)\right\|^{2}+ \frac{TL_{\bm{\sigma}^{2}}}{m_{n}}\right).\] (6)

Here \(L_{\bm{\sigma}^{2}}\) is the Lipschitz constant of \(\left\|\bm{\sigma}_{n-1}(\cdot)\right\|^{2}\). To achieve sublinear regret, we require that \(\sum_{n=1}^{N}\frac{1}{m_{n}}\in o(N)\). Therefore, for a fixed equidistant MSS, our analysis does not ensure a sublinear regret. This is because we consider a continuous-time regret which is integrated (c.f., Equation (1)) while in the discrete-time setting, the regret is defined for the equidistant MSS only (Curi et al., 2020). Accordingly, we study a strictly harder problem. Nonetheless, by linearly increasing the number of observations per episode and setting \(m_{n}=n\), we get \(\sum_{n=1}^{N}\frac{1}{m_{n}}\in\mathcal{O}(\log(N))\) and sublinear regret. The equidistant MSS is easy to implement, however, the required number of samples is increasing linearly with the number of episodes.

AdaptiveFinally, we present an MSS that is practical, i.e., easy to implement and at the same time requires only a few measurements per episode. The core idea of receding horizon adaptive MSS is simple: simulate (hallucinate) the system \(\bm{f}_{n}\) with the policy \(\bm{\pi}_{n}\), and find the time \(t\) such that \(\left\|\bm{\sigma}_{n-1}(\bm{\tilde{z}}_{n}(t))\right\|\) is largest. Here, \(\bm{\tilde{z}}_{n}(t)\) is the state-action pair at time \(t\) in episode \(n\) for the hallucinated trajectory.

However, the hallucinated trajectory can deviate from the true trajectory exponentially fast in time, and the time of the largest variance on the hallucinated trajectory can be far away from the time of the largest variance on the true trajectory. To remedy this technicality, we utilize a receding horizon MSS. We split, in episode \(n\), the time horizon \(T\) into uniform-length intervals of \(\Delta_{n}\) time, where \(\Delta_{n}\in\Omega(\frac{1}{\beta_{n}})\), c.f., Appendix A. At the beginning of every time interval, we measure the true state-action pair \(\bm{z}\), hallucinate with policy \(\bm{\pi}_{n}\) starting from \(\bm{z}\) for \(\Delta_{n}\) time on the system \(\bm{f}_{n}\), and calculate the time when the hallucinated trajectory has the highest variance. Over the next time horizon \(\Delta_{n}\), we collect a measurement only at that time. Formally, let \(m_{n}=\lceil T/\Delta_{n}\rceil\) be the number of measurements in episode \(n\) and denote for every \(i\in\{1,\ldots,m_{n}\}\) the time with the highest variance on the hallucinated trajectory in the time interval \([(i-1)\Delta_{n},i\Delta_{n}]\) by \(t_{n,i}\):

\[t_{n,i} \stackrel{{\text{def}}}{{=}}(i-1)\Delta_{n}+\underset{ 0\leq t\leq\Delta_{n}}{\operatorname{argmax}}\|\bm{\sigma}_{n-1}(\widehat{ \bm{z}}_{n}(t))\|\] s.t. \[\hat{\bm{x}}_{n}=\bm{f}_{n}(\widehat{\bm{x}}_{n},\bm{\pi}_{n}( \widehat{\bm{x}}_{n}))\] \[\widehat{\bm{x}}_{n}(0)=\bm{x}_{n}((i-1)\Delta_{n})\]

For this MSS, we show in Appendix A the upper bound

\[\mathcal{I}_{N}(\bm{f}^{*},S_{n}^{\text{ADP}})\leq\underset{ \begin{subarray}{c}\bm{\pi}_{1},\ldots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}{\max}9\sum_{n=1}^{N}\frac{T}{m_{n}}\sum_{i=1} ^{m_{n}}\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t_{n,i})\right)\|^{2}.\] (7)

The model complexity upper bound for the adaptive MSS in Equation (7) does not have the \(L_{\bm{\sigma}^{2}}\)-dependent term from the equidistant MSS (6) and only depends on the sum of uncertainties at the collected points. Further, the number of points we collect in episode \(n\) is \(\mathcal{O}\left(\beta_{n}\right)\), and \(\beta_{n}\), e.g., if we model the dynamics with GPs with radial basis function (RBF) kernel, is of order \(\text{polylog}(n)\) (c.f., Lemma 2).

### Modeling dynamics with a Gaussian Process

We can prove sublinear regret for the proposed three MSSs coupled with optimistic exploration when we model the dynamics using GPs with standard kernels such as RBF, linear, etc. We consider the vector-valued dynamics model \(\bm{f}^{*}(\bm{z})=(f_{1}^{*}(\bm{z}),\ldots,f_{d_{x}}^{*}(\bm{z}))\), where scalar-valued functions \(f_{j}^{*}\in\mathcal{H}_{k}\) are elements of a Reproducing Kernel Hilbert Space (RKHS) \(\mathcal{H}_{k}\) with kernel function \(k\), and their norm is bounded \(\left\|f_{j}^{*}\right\|_{k}\leq B\). We write \(\bm{f}^{*}\in\mathcal{H}_{k,B}^{d_{x}}\stackrel{{\text{def}}}{{= }}\left\{(f_{1},\ldots,f_{d_{x}})\mid\left\|f_{j}\right\|_{k}\leq B\right\}\).

To learn \(\bm{f}^{*}\) we fit a GP model with a zero mean and kernel \(k\) to the collected data \(\mathcal{D}_{1:n}\). For ease of notation, we denote by \(\dot{\bm{y}}_{1:n}^{j}\) the concatenated \(j\)-th dimension of the state derivative observations from \(\mathcal{D}_{1:n}\). The posterior means and variances of the GP model have a closed form (Williams and Rasmussen (2006)):

\[\mu_{n,j}(\bm{z}) =\bm{k}_{n}^{\top}(\bm{z})\left(\bm{K}_{n}+\sigma^{2}\bm{I} \right)^{-1}\dot{\bm{y}}_{1:n}^{j}\] \[\sigma_{n,j}^{2}(\bm{z}) =k(\bm{z},\bm{z})-\bm{k}_{n}^{\top}(\bm{z})\left(\bm{K}_{n}+\sigma ^{2}\bm{I}\right)^{-1}\bm{k}_{n}(\bm{z}),\]

where we write \(\bm{K}_{n}=[k(\bm{z}_{l},\bm{z}_{m})]_{\bm{z}_{l},\bm{z}_{m}\in\mathcal{D}_{1:n}}\), and \(\bm{k}_{n}(\bm{z})=[k(\bm{z}_{l},\bm{z})]_{\bm{z}_{l}\in\mathcal{D}_{1:n}}\). The posterior means and variances with the right scaling factor \(\beta_{n}(\delta)\) satisfy the all-time well-calibrated Assumption 2.

**Lemma 2** (Lemma 3.6 from Rothfuss et al. (2023)).: _Let \(\bm{f}^{*}\in\mathcal{H}_{k,B}^{d_{x}}\) and \(\delta\in(0,1)\). Then there exist \(\beta_{n}(\delta)\in\mathcal{O}(\sqrt{\gamma_{n}+\log(1/\delta)})\) such that the confidence sets \(\mathcal{M}_{n}\) built from the triplets \((\bm{\mu}_{n},\bm{\sigma}_{n},\beta_{n}(\delta))\) form an all-time well-calibrated statistical model._

Figure 1: In episode \(n\) we split the horizon \(T\) into intervals of \(\Delta_{n}\) time. We hallucinate the trajectory in every interval and select time \(t_{n,i}\) in the interval \(i\) where the uncertainty on the hallucinated trajectory is the highest.

Here, \(\gamma_{n}\) is the _maximum information gain_ after observing \(n\) points (Srinivas et al., 2009), as defined in Appendix A, where we also provide the rates for common kernels. For example, for the RBF kernel, \(\gamma_{n}=\mathcal{O}\left(\log(n)^{d_{x}+d_{u}+1}\right)\).

Finally, we show sublinear regret for the proposed MSSs for the case when we model dynamics with GPs. The proof of Theorem 3 is provided in Appendix A.

**Theorem 3**.: _Assume that \(\bm{f}^{*}\in\mathcal{H}_{k,B}^{d_{x}}\), the observation noise is i.i.d. \(\mathcal{N}(\bm{0};\sigma^{2}\bm{I})\), and \(\|\cdot\|\) is the Euclidean norm. We model \(\bm{f}^{*}\) with the GP model. The regret for different MSSs is with probability at least \(1-\delta\) bounded by_

\[R_{N}(S^{\text{ORA}}) \leq\mathcal{O}\left(\beta_{N}T^{2}e^{L_{\bm{f}}(1+L_{\bm{\pi}}) T}\sqrt{N\gamma_{N}}\right), m_{n}^{\text{ORA}}=1\] \[R_{N}(S^{\text{EQ0}}) \leq\mathcal{O}\left(\beta_{N}T^{2}e^{L_{\bm{f}}(1+L_{\bm{\pi}}) T}\sqrt{N\left(\gamma_{N}+\log(N)\right)}\right), m_{n}^{\text{EQ0}}=n\] \[R_{N}(S^{\text{ADP}}) \leq\mathcal{O}\left(\beta_{N}T^{2}e^{L_{\bm{f}}(1+L_{\bm{\pi}}) T}\sqrt{N\gamma_{N}}\right), m_{n}^{\text{ADP}}=\mathcal{O}(\beta_{n})\]

The optimistic exploration coupled with any of the proposed MSSs achieves regret that depends on the maximum information gain. All regret bounds from Theorem 3 are sublinear for common kernels, like linear and RBF. The bound for the adaptive MSS with RBF kernel is \(\mathcal{O}\left(\sqrt{N\log(N)^{2(d_{x}+d_{u}+1)}}\right)\), where we hide the dependence on \(T\) in the \(\mathcal{O}\) notation. We reiterate that while the number of observations per episode of the oracle MSS is optimal, we cannot implement the oracle MSS in practice. In contrast, the equidistant MSS is easy to implement, but the number of measurements grows linearly with episodes. Finally, adaptive MSS is practical, i.e., easy to implement, and requires only a few measurements per episode, i.e., \(\text{polylog}(n)\) in episode \(n\) for RBF.

SummaryOCoRL consists of two key and orthogonal components; _(i)_ optimistic policy selection and _(ii)_ measurement selection strategies (MSSs). In optimistic policy selection, we optimistically, w.r.t. plausible dynamics, plan a trajectory and rollout the resulting policy. We study different MSSs, such as the typical equidistant MSS, for data collection within the framework of continuous time modeling. Furthermore, we propose an adaptive MSS that measures data where we have the highest uncertainty on the planned (hallucinated) trajectory. We show that OCoRL suffers no regret for the equidistant, adaptive, and oracle MSSs.

## 4 Related work

Model-based Reinforcement LearningModel-based reinforcement learning (MBRL) has been an active area of research in recent years, addressing the challenges of learning and exploiting environment dynamics for improved decision-making. Among the seminal contributions, Deisenroth and Rasmussen (2011) proposed the PILCO algorithm which uses Gaussian processes for learning the system dynamics and policy optimization. Chua et al. (2018) used deep ensembles as dynamics models. They coupled MPC (Morari and Lee, 1999) to efficiently solve high dimensional tasks with considerably better sample complexity compared to the state-of-the-art model-free methods SAC (Haarnoja et al., 2018), PPO (Schulman et al., 2017), and DDPG (Lillicrap et al., 2015). The aforementioned model-based methods use more or less greedy exploitation that is provably optimal only in the case of the linear dynamics (Simchowitz and Foster, 2020). Exploration methods based on Thompson sampling (Dearden et al., 2013; Chowdhury and Gopalan, 2019) and Optimism (Auer et al., 2008; Abbasi-Yadkori and Szepesvari, 2011; Luo et al., 2018; Curi et al., 2020), however, provably converge to the optimal policy also for a large class of nonlinear dynamics if modeled with GPs. Among the discrete-time RL algorithms, our work is most similar to the work of Curi et al. (2020) where they use the reparametrization trick to explore optimistically among all statistically plausible discrete dynamical models.

Continuous-time Reinforcement LearningReinforcement learning in continuous-time has been around for several decades (Doya, 2000; Vrabie and Lewis, 2008, 2009; Vamvoudakis et al., 2009). Recently, the field has gained more traction following the work on Neural ODEs by Chen et al. (2018). While physics biases such as Lagrangian (Cranmer et al., 2020) or Hamiltonian (Greydanus et al., 2019) mechanics can be enforced in continuous-time modeling, different challenges such as vanishing \(Q\)-function (Tallec et al., 2019) need to be addressed in the continuous-time setting. Yildiz et al. (2021) introduce a practical episodic model-based algorithm in continuous time. In each episode, they use the learned mean estimate of the ODE model to solve the optimal control task with a variant of a continuous-time actor-critic algorithm. Compared to Yildiz et al. (2021) we solve the optimal control task using the optimistic principle. Moreover, we thoroughly motivate optimistic planning from a theoretical standpoint. Lutter et al. (2021) introduce a continuous fitted value iteration and further show a successful hardware application of their continuous-time algorithm. A vast literature exists for continuous-time RL with linear systems (Modares and Lewis, 2014; Wang et al., 2020), but few, only for linear systems, provide theoretical guarantees (Mohammadi et al., 2021; Basei et al., 2022). To the best of our knowledge, we are the first to provide theoretical guarantees of _convergence to the optimal cost in continuous time_ for a large class of RKHS functions.

Aperiodic strategiesAperiodic MSSs and controls (event and self-triggered control) are mostly neglected in RL since most RL works predominantly focus on discrete-time modeling. There exists a considerable amount of literature on the event and self-triggered control (Astrom and Bernhardsson, 2002; Anta and Tabuada, 2010; Heemels et al., 2012). However, compared to periodic control, its theory is far from being mature (Heemels et al., 2021). In our work, we assume that we can control the system continuously, and rather focus on when to measure the system instead. The closest to our adaptive MSS is the work of Du et al. (2020), where they empirically show that by optimizing the number of interaction times, they can achieve similar performance (in terms of cost) but with fewer interactions. Compared to us, they do not provide any theoretical guarantees. Umlauft and Hirche (2019); Lederer et al. (2021) consider the non-episodic setting where they can continuously monitor the system. They suggest taking measurements only when the uncertainty of the learned model on the monitored trajectory surpasses the boundary ensuring stability. They empirically show, for feedback linearizable systems, that by applying their strategy the number of measurements reduces drastically and the tracking error remains bounded. Compared to them, we consider general dynamical systems and also don't assume continuous system monitoring.

## 5 Experiments

We now empirically evaluate the performance of OCoRL on several environments. We test OCoRL on _Cancer Treatment and Glucose in blood systems_ from Howe et al. (2022), _Pendulum_, _Mountain Car_ and _Cart Pole_ from Brockman et al. (2016), _Bicycle_ from Polack et al. (2017), _Furuta Pendulum_ from Lutter et al. (2021) and _Quadrotor in 2D and 3D_ from Nonami et al. (2010). The details of the systems' dynamics and tasks are provided in Appendix C.

Comparison methodsTo make the comparison fair, we adjust methods so that they all collect the same number of measurements per episode. For the equidistant setting, we collect \(M\) points per episode (we provide values of \(M\) for different systems in Appendix C). For the adaptive MSS, we assume \(\Delta_{n}\geq T\), and instead of one measurement per episode we collect a batch of \(M\) measurements such that they (as a batch) maximize the variance on the hallucinated trajectory. To this end, we consider the _Greedy Max Determinant_ and _Greedy Max Kernel Distance_ strategies of Holzmuller et al. (2022). We provide details of the adaptive strategies in Appendix C. We compare OCoRL with the described MSSs to the optimal discrete-time zero hold control, where we assume the access to the true discretized dynamics \(\bm{f}_{d}^{*}(\bm{x},\bm{u})=\bm{x}+\int_{0}^{T/(M-1)}\bm{f}^{*}(\bm{x}(t), \bm{u})\,dt\). We further also compare with the best continuous-time control policy, i.e., the solution of Equation (1).

Does the continuous-time control policy perform better than the discrete-time control policy?In the first experiment, we test whether learning a continuous-time model from the finite data coupled with a continuous-time control policy on the learned model can outperform the discrete-time zero-order hold control on the true system. We conduct the experiment on all environments and report the cost after running OCoRL for a few tens of episodes (the exact experimental details are provided in Appendix C). From Table 1, we conclude that the OCoRL outperforms the discrete-time zero-order hold control on the true model on every system if we use the adaptive MSS, while achieving lower cost on 7 out of 9 systems if we measure the system equidistantly.

Does the adaptive MSS perform better than equidistant MSS?We compare the adaptive and equidistant MSSs on all systems and observe that the adaptive MSSs consistently perform better than the equidistant MSS. To better illustrate the difference between the adaptive and equidistant MSSs we study a 2-dimensional Pendulum system (c.f., Figure 2). First, we see that if we use adaptive MSSs we consistently achieve lower per-episode costs during the training. Second, we observe that while equidistant MSS spreads observations equidistantly in time and collects lots of measurements with almost the same state-action input of the dynamical system, the adaptive MSS spreads the measurements to have diverse state-action input pairs for the dynamical system on the executed trajectory and collects higher quality data.

Does optimism help?For the final experiment, we examine whether planning optimistically helps. In particular, we compare our planning strategy to the mean planner that is also used by Yildiz et al. (2021). The mean planner solves the optimal control problem greedily with the learned mean model \(\bm{\mu}_{n}\) in every episode \(n\). We evaluate the performance of this model on the Pendulum system for all MSSs. We observe that planning optimistically results in reducing the cost faster and achieves better performance for all MSSs.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{2}{c}{Known True Model} & \multicolumn{3}{c}{Optimistic Exploration with different MSS} \\ \cline{2-6} System & \begin{tabular}{c} Continuous \\ time OC \\ \end{tabular} & \begin{tabular}{c} Discrete zero- \\ order hold OC \\ \end{tabular} & \begin{tabular}{c} Max Kernel \\ Distance \\ \end{tabular} & 
\begin{tabular}{c} Max \\ Determinant \\ \end{tabular} & Equidistant \\ \hline Cancer Treatment & 20.57 & 21.05 & \(20.70\pm 0.06\) & \(\bm{20.68\pm 0.05}\) & \(21.05\pm 1.60\) \\ Glucose in Blood & 15.23 & 15.30 & \(\bm{15.23\pm 0.01}\) & \(15.24\pm 0.01\) & \(15.25\pm 0.01\) \\ Pendulum & 20.16 & 20.59 & \(\bm{20.20\pm 0.02}\) & \(\bm{20.20\pm 0.02}\) & \(20.29\pm 0.03\) \\ Mountain Car & 34.63 & 35.04 & \(\bm{34.63\pm 0.01}\) & \(\bm{34.63\pm 0.01}\) & \(34.64\pm 0.01\) \\ Cart Pole & 17.49 & 19.96 & \(\bm{17.52\pm 0.04}\) & \(17.53\pm 0.04\) & \(17.63\pm 0.05\) \\ Bicycle & 9.45 & 10.24 & \(\bm{9.53\pm 0.02}\) & \(9.53\pm 0.03\) & \(9.67\pm 0.05\) \\ Furuta Pendulum & 23.31 & 25.11 & \(23.64\pm 0.22\) & \(\bm{23.52\pm 0.18}\) & \(314.77\pm 411.01\) \\ Quadrotor 2D & 3.54 & 4.01 & \(\bm{3.54\pm 0.01}\) & \(\bm{3.54\pm 0.01}\) & \(3.57\pm 0.01\) \\ Quadrotor 3D & 7.38 & 7.84 & \(\bm{7.51\pm 0.21}\) & \(7.54\pm 0.28\) & \(9.41\pm 1.43\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: OCoRL with adaptive MSSs achieves lower final cost \(C(\bm{\pi}_{N},\bm{f}^{*})\) compared to the discrete-time control on the true system on all tested environments while converging towards the best continuous-time control policy. While equidistant MSS achieves higher cost compared to the adaptive MSS, it still outperforms the discrete-time zero-order hold control on the true model for most systems.

Figure 2: All MSSs coupled with continuous-time control achieve lower cost than the optimal discrete zero-order hold control on the true model. Adaptive MSSs (Greedy Max Kernel Distance and Greedy Max Determinant) reduce the suffered cost considerably faster than equidistant MSS and are converging towards the best possible continuous-time control. Whereas equidistant MSS spreads the measurements uniformly over time, the adaptive MSSs spread the data over the state-action space (dynamical systemâ€™s input) and collect higher quality data.

## 6 Conclusion

We introduce a model-based continuous-time RL algorithm OCoRL that uses the _optimistic paradigm_ to provably achieve sublinear regret, with respect to the best possible continuous-time performance, for several MSSs if modeled with GPs. Further, we develop a practical _adaptive_ MSS that, compared to the standard equidistant MSS, drastically reduces the number of measurements per episode while retaining the regret guarantees. Finally, we showcase the benefits of continuous-time compared to discrete-time modeling in several environments (c.f. Figure 1, Table 1), and demonstrate the benefits of planning with _optimism_ compared to greedy planning.

In this work, we considered the setting with deterministic dynamics where we obtain a noisy measurement of the state's derivative. We leave the more practical setting, where only noisy measurements of the state, instead of its derivative, are available as well as stochastic, delayed differential equations, and partially observable systems to future work.

Our aim with this work is to catalyze further research within the RL community on continuous-time modeling. We believe that this shift in perspective could lead to significant advancements in the field and look forward to future contributions.

## Acknowledgments and Disclosure of Funding

We would like to thank Yarden As, Scott Sussex and Dominik Baumann for their feedback. This project has received funding from the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, and the Microsoft Swiss Joint Research Center.

Figure 3: Both greedy and optimistic continuous-time control strategies outperform the discrete-time zero-order hold control on the true model. However, the optimistic strategy is particularly notable as it expedites cost reduction across all MSSs.

## References

* Abbasi-Yadkori and Szepesvari (2011) Abbasi-Yadkori, Y. and Szepesvari, C. (2011). Regret bounds for the adaptive control of linear quadratic systems. In _Proceedings of the 24th Annual Conference on Learning Theory_, pages 1-26. JMLR Workshop and Conference Proceedings.
* Anta and Tabuada (2010) Anta, A. and Tabuada, P. (2010). To sample or not to sample: Self-triggered control for nonlinear systems. _IEEE Transactions on automatic control_, 55(9):2030-2042.
* Astrom and Bernhardsson (2002) Astrom, K. J. and Bernhardsson, B. M. (2002). Comparison of riemann and lebesgue sampling for first order stochastic systems. In _Proceedings of the 41st IEEE Conference on Decision and Control, 2002._, volume 2, pages 2011-2016. IEEE.
* Auer et al. (2008) Auer, P., Jaksch, T., and Ortner, R. (2008). Near-optimal regret bounds for reinforcement learning. _Advances in neural information processing systems_, 21.
* Basei et al. (2022) Basei, M., Guo, X., Hu, A., and Zhang, Y. (2022). Logarithmic regret for episodic continuous-time linear-quadratic reinforcement learning over a finite-time horizon. _Journal of Machine Learning Research_, 23(178):1-34.
* Bradbury et al. (2018) Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. (2018). JAX: composable transformations of Python+NumPy programs.
* Brockman et al. (2016) Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. (2016). Openai gym. _arXiv preprint arXiv:1606.01540_.
* Chartrand (2011) Chartrand, R. (2011). Numerical differentiation of noisy, nonsmooth data. _International Scholarly Research Notices_, 2011.
* Chen et al. (2018) Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. (2018). Neural ordinary differential equations. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc.
* Chowdhury and Gopalan (2017) Chowdhury, S. R. and Gopalan, A. (2017). On kernelized multi-armed bandits. In _International Conference on Machine Learning_, pages 844-853. PMLR.
* Chowdhury and Gopalan (2019) Chowdhury, S. R. and Gopalan, A. (2019). Online learning in kernelized markov decision processes. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 3197-3205. PMLR.
* Chua et al. (2018) Chua, K., Calandra, R., McAllister, R., and Levine, S. (2018). Deep reinforcement learning in a handful of trials using probabilistic dynamics models. _Advances in neural information processing systems_, 31.
* Cover (1999) Cover, T. M. (1999). _Elements of information theory_. John Wiley & Sons.
* Cranmer et al. (2020) Cranmer, M., Greydanus, S., Hoyer, S., Battaglia, P., Spergel, D., and Ho, S. (2020). Lagrangian neural networks. _arXiv preprint arXiv:2003.04630_.
* Cullum (1971) Cullum, J. (1971). Numerical differentiation and regularization. _SIAM Journal on numerical analysis_, 8(2):254-265.
* Curi et al. (2020) Curi, S., Berkenkamp, F., and Krause, A. (2020). Efficient model-based reinforcement learning through optimistic policy search and planning. _Advances in Neural Information Processing Systems_, 33:14156-14170.
* Dearden et al. (2013) Dearden, R., Friedman, N., and Andre, D. (2013). Model-based bayesian exploration. _arXiv preprint arXiv:1301.6690_.
* Deisenroth and Rasmussen (2011) Deisenroth, M. and Rasmussen, C. E. (2011). Pilco: A model-based and data-efficient approach to policy search. In _Proceedings of the 28th International Conference on machine learning (ICML-11)_, pages 465-472.
* Deisenroth et al. (2018)* Doya (2000) Doya, K. (2000). Reinforcement learning in continuous time and space. _Neural computation_, 12(1):219-245.
* Du et al. (2020) Du, J., Futoma, J., and Doshi-Velez, F. (2020). Model-based reinforcement learning for semi-markov decision processes with neural odes. _Advances in Neural Information Processing Systems_, 33:19805-19816.
* Engquist et al. (2007) Engquist, B., Li, X., Ren, W., Vanden-Eijnden, E., et al. (2007). Heterogeneous multiscale methods: a review. _Communications in Computational Physics_, 2(3):367-450.
* Gavert (2016) Gavert, M. (2016). _Modelling the firuta pendulum_. Department of Automatic Control, Lund Institute of Technology (LTH).
* Ghysels et al. (2006) Ghysels, E., Santa-Clara, P., and Valkanov, R. (2006). Predicting volatility: getting the most out of return data sampled at different frequencies. _Journal of Econometrics_, 131(1-2):59-95.
* Greydanus et al. (2019) Greydanus, S., Dzamba, M., and Yosinski, J. (2019). Hamiltonian neural networks. _Advances in neural information processing systems_, 32.
* Haarnoja et al. (2018) Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P., et al. (2018). Soft actor-critic algorithms and applications. _arXiv preprint arXiv:1812.05905_.
* Hartman (2002) Hartman, P. (2002). _Ordinary differential equations_. SIAM.
* Heemels et al. (2021) Heemels, W., Johansson, K. H., and Tabuada, P. (2021). Event-triggered and self-triggered control. In _Encyclopedia of Systems and Control_, pages 724-730. Springer.
* Heemels et al. (2012) Heemels, W. P., Johansson, K. H., and Tabuada, P. (2012). An introduction to event-triggered and self-triggered control. In _2012 ieee 51st ieee conference on decision and control (cdc)_, pages 3270-3285. IEEE.
* Holzmuller et al. (2022) Holzmuller, D., Zaverkin, V., Kastner, J., and Steinwart, I. (2022). A framework and benchmark for deep batch active learning for regression. _arXiv preprint arXiv:2203.09410_.
* Howe et al. (2022) Howe, N., Dufort-Labbe, S., Rajkumar, N., and Bacon, P.-L. (2022). Myriad: a real-world testbed to bridge trajectory optimization and deep learning. _Advances in Neural Information Processing Systems_, 35:29801-29815.
* Jacobson and Mayne (1970) Jacobson, D. H. and Mayne, D. Q. (1970). _Differential dynamic programming_. Number 24. Elsevier Publishing Company.
* Jones et al. (2009) Jones, D. S., Plank, M., and Sleeman, B. D. (2009). _Differential equations and mathematical biology_. CRC press.
* Kaandorp and Koole (2007) Kaandorp, G. C. and Koole, G. (2007). Optimal outpatient appointment scheduling. _Health care management science_, 10:217-229.
* Kakade et al. (2020) Kakade, S., Krishnamurthy, A., Lowrey, K., Ohnishi, M., and Sun, W. (2020). Information theoretic regret bounds for online nonlinear control. _Advances in Neural Information Processing Systems_, 33:15312-15325.
* Kelly (2017) Kelly, M. (2017). An introduction to trajectory optimization: How to do your own direct collocation. _SIAM Review_, 59(4):849-904.
* Khalil (2015) Khalil, H. K. (2015). _Nonlinear control_, volume 406. Pearson New York.
* Kirschner and Krause (2018) Kirschner, J. and Krause, A. (2018). Information directed sampling and bandits with heteroscedastic noise. In _Conference On Learning Theory_, pages 358-384. PMLR.
* Knowles and Renka (2014) Knowles, I. and Renka, R. J. (2014). Methods for numerical differentiation of noisy data. _Electron. J. Differ. Equ_, 21:235-246.
* Knowles and Wallace (1995) Knowles, I. and Wallace, R. (1995). A variational method for numerical differentiation. _Numerische Mathematik_, 70:91-110.
* Kastner et al. (2018)Kuleshov, V., Fenner, N., and Ermon, S. (2018). Accurate uncertainties for deep learning using calibrated regression. In _International conference on machine learning_, pages 2796-2804. PMLR. Lederer et al. (2021) Lederer, A., Conejo, A. J. O., Maier, K. A., Xiao, W., Umlauft, J., and Hirche, S. (2021). Gaussian process-based real-time learning for safety critical applications. In _International Conference on Machine Learning_, pages 6055-6064. PMLR. Lenhart and Workman (2007) Lenhart, S. and Workman, J. T. (2007). _Optimal control applied to biological models_. CRC press. Li and Todorov (2004) Li, W. and Todorov, E. (2004). Iterative linear quadratic regulator design for nonlinear biological movement systems. In _ICINCO (1)_, pages 222-229. Citeseer. Lillicrap et al. (2015) Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2015). Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_. Luo et al. (2018) Luo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. (2018). Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. _arXiv preprint arXiv:1807.03858_. Lutter et al. (2021) Lutter, M., Mannor, S., Peters, J., Fox, D., and Garg, A. (2021). Value iteration in continuous actions, states and time. _arXiv preprint arXiv:2105.04682_. Modares and Lewis (2014) Modares, H. and Lewis, F. L. (2014). Linear quadratic tracking control of partially-unknown continuous-time systems using reinforcement learning. _IEEE Transactions on Automatic control_, 59(11):3051-3056. Mohammadi et al. (2021) Mohammadi, H., Zare, A., Soltanolkotabi, M., and Jovanovic, M. R. (2021). Convergence and sample complexity of gradient methods for the model-free linear-quadratic regulator problem. _IEEE Transactions on Automatic Control_, 67(5):2435-2450. Morari and Lee (1999) Morari, M. and Lee, J. H. (1999). Model predictive control: past, present and future. _Computers & Chemical Engineering_, 23(4-5):667-682. Mordatch et al. (2012) Mordatch, I., Todorov, E., and Popovic, Z. (2012). Discovery of complex behaviors through contact-invariant optimization. _ACM Transactions on Graphics (ToG)_, 31(4):1-8. Nesic and Postoyan (2021) Nesic, D. and Postoyan, R. (2021). Nonlinear sampled-data systems. In _Encyclopedia of Systems and Control_, pages 1477-1483. Springer. Nonami et al. (2010) Nonami, K., Kendoul, F., Suzuki, S., Wang, W., and Nakazawa, D. (2010). _Autonomous flying robots: unmanned aerial vehicles and micro aerial vehicles_. Springer Science & Business Media. Osband et al. (2013) Osband, I., Russo, D., and Van Roy, B. (2013). (more) efficient reinforcement learning via posterior sampling. _Advances in Neural Information Processing Systems_, 26. Osband and Van Roy (2014) Osband, I. and Van Roy, B. (2014). Model-based reinforcement learning and the eluder dimension. _Advances in Neural Information Processing Systems_, 27. Paing et al. (2020) Paing, H. S., Schagin, A. V., Win, K. S., and Linn, Y. H. (2020). New designing approaches for quadcopter using 2d model modelling a cascaded pid controller. In _2020 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)_, pages 2370-2373. IEEE. Panetta and Fister (2003) Panetta, J. C. and Fister, K. R. (2003). Optimal control applied to competing chemotherapeutic cell-kill strategies. _SIAM Journal on Applied Mathematics_, 63(6):1954-1971. Pasztor et al. (2021) Pasztor, B., Bogunovic, I., and Krause, A. (2021). Efficient model-based multi-agent mean-field reinforcement learning. _arXiv preprint arXiv:2107.04050_. Polack et al. (2017) Polack, P., Altche, F., d'Andrea Novel, B., and de La Fortelle, A. (2017). The kinematic bicycle model: A consistent model for planning feasible trajectories for autonomous vehicles? In _2017 IEEE intelligent vehicles symposium (IV)_, pages 812-818. IEEE. Rothfuss et al. (2023) Rothfuss, J., Sukhija, B., Birchler, T., Kassraie, P., and Krause, A. (2023). Hallucinated adversarial control for conservative offline policy evaluation. _arXiv preprint arXiv:2303.01076_.

Russo, D. and Van Roy, B. (2014). Learning to optimize via posterior sampling. _Mathematics of Operations Research_, 39(4):1221-1243.
* Russo and Van Roy (2016) Russo, D. and Van Roy, B. (2016). An information-theoretic analysis of thompson sampling. _The Journal of Machine Learning Research_, 17(1):2442-2471.
* Schulman et al. (2017) Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_.
* Simchowitz and Foster (2020) Simchowitz, M. and Foster, D. (2020). Naive exploration is optimal for online lqr. In _International Conference on Machine Learning_, pages 8937-8948. PMLR.
* Singh and Theers (2021) Singh, M. and Theers, M. (2021). Kinematic bicycle model. https://thomasfermi.github.io/Algorithms-for-Automated-Driving/Control/BicycleModel.html. Accessed: 2023-10-09.
* Spong et al. (2006) Spong, M. W., Hutchinson, S., Vidyasagar, M., et al. (2006). _Robot modeling and control_, volume 3. Wiley New York.
* Srinivas et al. (2009) Srinivas, N., Krause, A., Kakade, S. M., and Seeger, M. (2009). Gaussian process optimization in the bandit setting: No regret and experimental design. _arXiv preprint arXiv:0912.3995_.
* Sussex et al. (2023) Sussex, S., Makarova, A., and Krause, A. (2023). Model-based causal bayesian optimization. In _International Conference on Learning Representations (ICLR)_.
* Sutton and Barto (2018) Sutton, R. S. and Barto, A. G. (2018). _Reinforcement learning: An introduction_. MIT press.
* Tallec et al. (2019) Tallec, C., Blier, L., and Ollivier, Y. (2019). Making deep q-learning methods robust to time discretization. In _International Conference on Machine Learning_, pages 6096-6104. PMLR.
* Thomas et al. (2017) Thomas, J., Welde, J., Loianno, G., Daniilidis, K., and Kumar, V. (2017). Autonomous flight for detection, localization, and tracking of moving targets with a small quadrotor. _IEEE Robotics and Automation Letters_, 2(3):1762-1769.
* Thompson (1933) Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 25(3-4):285-294.
* Todorov and Li (2005) Todorov, E. and Li, W. (2005). A generalized iterative lqg method for locally-optimal feedback control of constrained nonlinear stochastic systems. In _Proceedings of the 2005, American Control Conference, 2005._, pages 300-306. IEEE.
* Treven et al. (2021) Treven, L., Wenk, P., Dorfler, F., and Krause, A. (2021). Distributional gradient matching for learning uncertain neural dynamics models. _Advances in Neural Information Processing Systems_, 34:29780-29793.
* Umlauft and Hirche (2019) Umlauft, J. and Hirche, S. (2019). Feedback linearization based on gaussian processes with event-triggered online learning. _IEEE Transactions on Automatic Control_, 65(10):4154-4169.
* Vakili et al. (2021) Vakili, S., Khezeli, K., and Picheny, V. (2021). On information gain and regret bounds in gaussian process bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 82-90. PMLR.
* Vamvoudakis et al. (2009) Vamvoudakis, K., Vrabie, D., and Lewis, F. (2009). Online policy iteration based algorithms to solve the continuous-time infinite horizon optimal control problem. In _2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning_, pages 36-41. IEEE.
* Vrabie and Lewis (2009) Vrabie, D. and Lewis, F. (2009). Neural network approach to continuous-time direct adaptive optimal control for partially unknown nonlinear systems. _Neural Networks_, 22(3):237-246.
* Vrabie and Lewis (2008) Vrabie, D. and Lewis, F. L. (2008). Adaptive optimal control algorithm for continuous-time nonlinear systems based on policy iteration. In _2008 47th IEEE Conference on Decision and Control_, pages 73-79. IEEE.
* Wagener et al. (2019) Wagener, N., Cheng, C.-A., Sacks, J., and Boots, B. (2019). An online learning approach to model predictive control. _arXiv preprint arXiv:1902.08967_.
* Wolski et al. (2019)Wagner, J., Mazurek, P., Miekina, A., and Morawski, R. Z. (2018). Regularised differentiation of measurement data in systems for monitoring of human movements. _Biomedical Signal Processing and Control_, 43:265-277.
* Wang et al. (2020) Wang, H., Zariphopoulou, T., and Zhou, X. Y. (2020). Reinforcement learning in continuous time and space: A stochastic control approach. _The Journal of Machine Learning Research_, 21(1):8145-8178.
* Williams and Rasmussen (2006) Williams, C. K. and Rasmussen, C. E. (2006). _Gaussian processes for machine learning_, volume 2. MIT press Cambridge, MA.
* Williams et al. (2017) Williams, G., Aldrich, A., and Theodorou, E. A. (2017). Model predictive path integral control: From theory to parallel computation. _Journal of Guidance, Control, and Dynamics_, 40(2):344-357.
* Yildiz et al. (2021) Yildiz, C., Heinonen, M., and Lahdesmaki, H. (2021). Continuous-time model-based reinforcement learning. In _International Conference on Machine Learning_, pages 12009-12018. PMLR.

###### Contents of Appendix

* A Theory
* A.1 Bounding Regret with the Model Complexity
* A.2 Bounding Model Complexities
* A.3 Bounding Measurement Uncertainty of GPs
* A.4 Useful Facts and Inequalities
* B Solving Optimistic Optimal Control Problem
* B.1 Application to Arbitrary Discretizations
* C Experimental Setup
* C.1 System's dynamics
* C.2 System's tasks
* C.3 Adaptive MSSs
* C.4 Episodes Hyperparameters
* C.5 Practical Implementation
* D Additional Experiments on Number of Measurements

[MISSING_PAGE_FAIL:17]

By Lemma 4 (with high probability), we further have that

\[r_{n}(S) \leq 2\beta_{n}L_{c}(1+L_{\bm{\pi}})e^{L_{\bm{f}}(1+L_{\bm{\pi}})T} \int_{0}^{T}\int_{0}^{t}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(s)\right) \right\|\,ds\,dt\] \[\leq 2\beta_{n}L_{c}(1+L_{\bm{\pi}})e^{L_{\bm{f}}(1+L_{\bm{\pi}})T }\int_{0}^{T}\int_{0}^{T}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(s)\right) \right\|\,ds\,dt\] \[=2\beta_{n}L_{c}(1+L_{\bm{\pi}})Te^{L_{\bm{f}}(1+L_{\bm{\pi}})T} \int_{0}^{T}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t)\right)\right\|\,dt.\]

Now we are ready to prove Proposition 1.

Proof of Proposition 1.: Let us first bound \(R_{N}^{2}(S)\). By the Cauchy-Schwarz inequality,

\[R_{N}^{2}(S) \leq N\sum_{n=1}^{N}r_{n}^{2}(S).\]

By Lemma 5 (with high probability), we have that

\[R_{N}^{2}(S) \leq N4L_{c}^{2}(1+L_{\bm{\pi}})^{2}T^{2}e^{2L_{\bm{f}}(1+L_{\bm {\pi}})T}\sum_{n=1}^{N}\beta_{n}^{2}\left(\int_{0}^{T}\left\|\bm{\sigma}_{n-1} \left(\bm{z}_{n}(t)\right)\right\|\,dt\right)^{2}\] \[\leq N4\beta_{N}^{2}L_{c}^{2}(1+L_{\bm{\pi}})^{2}T^{3}e^{2L_{\bm{ f}}(1+L_{\bm{\pi}})T}\sum_{n=1}^{N}\int_{0}^{T}\left\|\bm{\sigma}_{n-1}\left( \bm{z}_{n}(t)\right)\right\|^{2}\,dt.\]

Taking the square root, we obtain

\[R_{N}(S) \leq 2\beta_{N}L_{c}(1+L_{\bm{\pi}})T^{\frac{3}{2}}e^{L_{\bm{f}}(1+L _{\bm{\pi}})T}\sqrt{N\sum_{n=1}^{N}\int_{0}^{T}\left\|\bm{\sigma}_{n-1}\left( \bm{z}_{n}(t)\right)\right\|^{2}\,dt}.\]

The result follows by noting that

\[\sum_{n=1}^{N}\int_{0}^{T}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t)\right) \right\|^{2}\,dt\leq\mathcal{I}_{N}(\bm{f}^{*},S).\]

### Bounding Model Complexities

In this section, we derive the model complexity bounds for the presented MSSs. Our bounds are based on the approximation of integrals using an upper Darboux sum.

**Fact 6** (Upper Darboux approximation).: _Given any \(a\leq b\), a function \(f:\mathbb{R}\rightarrow\mathbb{R}\), and a partition \(P=(a,\ldots,b)\) of \([a,b]\) into \(k\) sub-intervals,_

\[\int_{a}^{b}f(x)\,dx\leq\sum_{i=1}^{k}\Delta_{i}\max_{x\in P[i]}f(x)\] (11)

_where \(P[i]\) denotes the \(i\)-th sub-interval and \(\Delta_{i}\) denotes the length of \(P[i]\)._

We are now ready to bound the model complexities.

**Lemma 7** (Oracle model complexity).: _For any \(N\geq 1\),_

\[\mathcal{I}_{N}(\bm{f}^{*},S^{0\text{RA}})\leq\max_{\begin{subarray}{c}\bm{ \pi}_{1},\ldots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}T\sum_{n=1}^{N}\left\|\bm{\sigma}_{n-1} \left(\bm{z}_{n}(t_{n,1})\right)\right\|^{2}.\] (12)Proof.: \[\mathcal{I}_{N}(\bm{f}^{*},S^{\text{ORA}})=\max_{\begin{subarray}{c}\bm{\pi}_{1}, \ldots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}\sum_{n=1}^{N}\int_{0}^{T}\left\|\bm{\sigma}_{ n-1}\left(\bm{z}_{n}(t)\right)\right\|^{2}\,dt.\]

For each episode \(n\), use an upper Darboux approximation (Fact 6) with \(k=1\) to obtain

\[\leq\max_{\begin{subarray}{c}\bm{\pi}_{1},\ldots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}T\sum_{n=1}^{N}\max_{0\leq t\leq T}\left\| \bm{\sigma}_{n-1}\left(\bm{z}_{n}(t)\right)\right\|^{2}\] \[=\max_{\begin{subarray}{c}\bm{\pi}_{1},\ldots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}T\sum_{n=1}^{N}\left\|\bm{\sigma}_{n-1} \left(\bm{z}_{n}(t_{n,1})\right)\right\|^{2}.\]

**Lemma 8** (Equidistant model complexity).: _For any \(N\geq 1\),_

\[\mathcal{I}_{N}(\bm{f}^{*},S^{\text{EQI}})\leq\max_{\begin{subarray}{c}\bm{ \pi}_{1},\ldots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}\sum_{n=1}^{N}\frac{T}{m_{n}}\sum_{i=1}^{m_{ n}}\biggl{(}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t_{n,i})\right)\right\|^{2}+ \frac{TL_{\sigma^{2}}}{m_{n}}\biggr{)}.\] (13)

Proof.: \[\mathcal{I}_{N}(\bm{f}^{*},S^{\text{EQI}})=\max_{\begin{subarray}{c}\bm{\pi} _{1},\ldots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}\sum_{n=1}^{N}\int_{0}^{T}\left\|\bm{\sigma} _{n-1}\left(\bm{z}_{n}(t)\right)\right\|^{2}\,dt.\]

For each episode \(n\), let \(P_{n}\stackrel{{\text{def}}}{{=}}(0,\Delta_{n},2\Delta_{n}, \ldots,T)\) with \(\Delta_{n}\stackrel{{\text{def}}}{{=}}\frac{T}{m_{n}}\) be a partition of \([0,T]\) into \(m_{n}\) sub-intervals, each of length \(\Delta_{n}\). Using the upper Darboux approximations (Fact 6) of \(\left\|\bm{\sigma}_{n-1}(\cdot)\right\|^{2}\) with respect to \(P_{n}\), we have

\[\leq\max_{\begin{subarray}{c}\bm{\pi}_{1},\ldots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}\sum_{n=1}^{N}\Delta_{n}\sum_{i=1}^{m_{n}} \max_{t\in[(i-1)\Delta_{n},i\Delta_{n}]}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_ {n}(t)\right)\right\|^{2}.\]

Observe that \(t_{n,i}\in[(i-1)\Delta_{n},i\Delta_{n}]\), and hence, using that \(\left\|\bm{\sigma}_{n-1}(\cdot)\right\|^{2}\) is \(L_{\sigma^{2}}\)-Lipschitz continuous,

\[\leq\max_{\begin{subarray}{c}\bm{\pi}_{1},\ldots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}\sum_{n=1}^{N}\Delta_{n}\sum_{i=1}^{m_{n}} \Bigl{(}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t_{n,i})\right)\right\|^{2}+ L_{\sigma^{2}}\Delta_{n}\Bigr{)}.\]

In the remainder of this section, we study the model complexity of the adaptive MSS. In each episode \(n\), we partition \([0,T]\) into equally sized sub-intervals (called "buckets") of length \(\Delta_{n}\), where \(\Delta_{n}\) is the solution to

\[\Delta_{n}=\frac{1}{2\Gamma_{n}(\Delta_{n})}\] (14)

where \(\Gamma_{n}(\Delta_{n})\stackrel{{\text{def}}}{{=}}2\beta_{n}L_{ \bm{\sigma}}(1+L_{\bm{\pi}})e^{L_{\bm{f}}(1+L_{\bm{\pi}})\Delta_{n}}\). As \(\Gamma_{n}\) is a monotonically increasing function of \(\Delta_{n}\) it is clear that a suitable \(\Delta_{n}\) exists. Moreover, it follows from \(\Delta_{n}\leq T\) that \(\Gamma_{n}\in\mathcal{O}(\beta_{n})\), and hence that \(m_{n}=T/\Delta_{n}\in\mathcal{O}(\beta_{n})\). Without loss of generality, we assume \(m_{n}\in\mathbb{N}\).

The definition of the adaptive MSS can be interpreted as follows: In every bucket, we take two measurements. One measurement is saved and used later for updating the statistical model. The other measurement is taken at the beginning of every bucket, and used to inform OCoRL of the current state position. For the purposes of our theoretical analysis, these initial measurements are not saved and not used for the construction of the statistical model.

**Lemma 9**.: _For any episode \(n\), bucket \(i\in\{1,\ldots,m_{n}\}\), and \(t\in[(i-1)\Delta_{n},i\Delta_{n}]\) where the real and hallucinated trajectories are "synced" initially, i.e., \(\widehat{\bm{z}}_{n}((i-1)\Delta_{n})=\bm{z}_{n}((i-1)\Delta_{n})\), we have with high probability that_

\[\|\bm{\sigma}_{n-1}(\widehat{\bm{z}}_{n}(t))-\bm{\sigma}_{n-1}( \bm{z}_{n}(t))\|\leq\Gamma_{n}\int_{(i-1)\Delta_{n}}^{t}\|\bm{\sigma}_{n-1}( \bm{z}_{n}(s))\|\ ds\qquad\text{and}\] (15) \[\|\bm{\sigma}_{n-1}(\widehat{\bm{z}}_{n}(t))-\bm{\sigma}_{n-1}( \bm{z}_{n}(t))\|\leq\Gamma_{n}\int_{(i-1)\Delta_{n}}^{t}\|\bm{\sigma}_{n-1}( \widehat{\bm{z}}_{n}(s))\|\ ds\] (16)

Proof.: Using that \(\bm{\sigma}_{n-1}\) is \(L_{\bm{\sigma}}\)-Lipschitz continuous and applying Lemma 15,

\[\|\bm{\sigma}_{n-1}(\widehat{\bm{z}}_{n}(t))-\bm{\sigma}_{n-1}( \bm{z}_{n}(t))\|\leq L_{\bm{\sigma}}(1+L_{\bm{\pi}})\left\|\widehat{\bm{x}}_{ n}(t)-\bm{x}_{n}(t)\right\|.\]

The result then follows from Lemma 4, where the lower integral bounds can be tightened to \((i-1)\Delta_{n}\) as the real and hallucinated trajectories are "synced" in the beginning of the bucket. 

**Lemma 10**.: _For any episode \(n\) and \(i\in\{1,\ldots,m_{n}\}\), if \(t_{n,i}\) is selected according to the adaptive MSS then with high probability,_

\[\max_{t\in[(i-1)\Delta_{n},i\Delta_{n}]}\left\|\bm{\sigma}_{n-1} (\bm{z}_{n}(t))\right\|^{2}\leq 9\left\|\bm{\sigma}_{n-1}(\bm{z}_{n}(t_{n,i})) \right\|^{2}.\] (17)

Proof.: By applying Lemma 9 we have (with high probability) for every \(t\in[(i-1)\Delta_{n},\Delta_{n}]\),

\[\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t)\right)\|\leq\|\bm{\sigma}_ {n-1}\left(\widehat{\bm{z}}_{n}(t)\right)\|+\Gamma_{n}\int_{(i-1)\Delta_{n}}^{ t}\|\bm{\sigma}_{n-1}\left(\widehat{\bm{z}}_{n}(s)\right)\|\ ds.\]

Note that \(\|\bm{\sigma}_{n-1}\left(\widehat{\bm{z}}_{n}(t)\right)\|\leq\|\bm{\sigma}_{n- 1}\left(\widehat{\bm{z}}_{n}(t_{n,i})\right)\|\) by the definition of \(t_{n,i}\), and hence,

\[\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t)\right)\| \leq\|\bm{\sigma}_{n-1}\left(\widehat{\bm{z}}_{n}(t_{n,i})\right) \|+\Gamma_{n}\Delta_{n}\left\|\bm{\sigma}_{n-1}\left(\widehat{\bm{z}}_{n}(t_{n,i})\right)\right\|\] \[=\left(1+\frac{1}{2}\right)\left\|\bm{\sigma}_{n-1}\left(\widehat {\bm{z}}_{n}(t_{n,i})\right)\right\|.\] (18)

Similarly, by applying Lemma 9, we have (with high probability) that

\[\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t_{n,i})\right)\| \geq\|\bm{\sigma}_{n-1}\left(\widehat{\bm{z}}_{n}(t_{n,i})\right) \|-\Gamma_{n}\int_{(i-1)\Delta_{n}}^{t_{n,i}}\left\|\bm{\sigma}_{n-1}\left( \widehat{\bm{z}}_{n}(s)\right)\right\|\ ds\] \[\geq\|\bm{\sigma}_{n-1}\left(\widehat{\bm{z}}_{n}(t_{n,i})\right) \|-\Gamma_{n}\Delta_{n}\left\|\bm{\sigma}_{n-1}\left(\widehat{\bm{z}}_{n}(t_{n,i})\right)\right\|\] \[=\left(1-\frac{1}{2}\right)\left\|\bm{\sigma}_{n-1}\left(\widehat{ \bm{z}}_{n}(t_{n,i})\right)\right\|.\] (19)

Combining Equations (18) and (19), we obtain

\[\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t)\right)\|^{2}\leq\left( \frac{1+\frac{1}{2}}{1-\frac{1}{2}}\right)^{2}\left\|\bm{\sigma}_{n-1}\left( \bm{z}_{n}(t_{n,i})\right)\right\|^{2}=9\left\|\bm{\sigma}_{n-1}\left(\bm{z}_ {n}(t_{n,i})\right)\right\|^{2}.\]

**Corollary 11** (Adaptive model complexity).: _For any \(N\geq 1\),_

\[\mathcal{I}_{N}(\bm{f}^{*},S^{\text{ADF}})\leq\max_{\begin{subarray}{c}\bm{ \pi}_{1},\ldots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}9\sum_{n=1}^{N}\frac{T}{m_{n}}\sum_{i=1}^{m_{ n}}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t_{n,i})\right)\right\|^{2}.\] (20)

Proof.: The result follows analogously to the proof of Lemma 8, using Lemma 10 in the last step.

### Bounding Measurement Uncertainty of GPs

Our goal in this section is to prove Theorem 3.

The informativeness of a set of sampling points \(A\subset\mathcal{Z}\) about a function \(f\sim\mathcal{GP}(0,k)\) with the noisy observations \(\boldsymbol{y}_{A}=\boldsymbol{f}_{A}+\boldsymbol{\epsilon}_{A}\) can be measured by the _information gain_ (c.f., Cover (1999); Srinivas et al. (2009)),

\[\mathrm{I}(\boldsymbol{y}_{A};f)\stackrel{{\text{def}}}{{=}} \mathrm{I}(\boldsymbol{y}_{A};\boldsymbol{f}_{A})=\mathrm{H}(\boldsymbol{y}_{ A})-\mathrm{H}(\boldsymbol{y}_{A}\mid\boldsymbol{f}_{A}),\] (21)

which quantifies the reduction in entropy of \(\boldsymbol{f}_{A}\) when observing \(\boldsymbol{y}_{A}\). Here, we write \(\boldsymbol{f}_{A}=[f(\boldsymbol{x})]_{\boldsymbol{x}\in A}\) and \(\boldsymbol{\epsilon}_{A}\sim\mathcal{N}\left(\boldsymbol{0},\sigma^{2} \boldsymbol{I}\right)\). For a Gaussian, \(\mathrm{H}(\mathcal{N}\left(\boldsymbol{\mu},\boldsymbol{\Sigma}\right))= \frac{1}{2}\log\det(2\pi e\boldsymbol{\Sigma})\), so that when \(f\) is a Gaussian process, \(\mathrm{I}(\boldsymbol{y}_{A};f)=\frac{1}{2}\log\det(\boldsymbol{I}+\sigma^{ -2}\boldsymbol{K}_{AA})\) where \(\boldsymbol{K}_{AA}=[k(\boldsymbol{x},\boldsymbol{x}^{\prime})]_{\boldsymbol{ x},\boldsymbol{x}^{\prime}\in A}\). We denote the maximal information gain by observing \(n\) points by

\[\gamma_{n}\stackrel{{\text{def}}}{{=}}\max_{\begin{subarray}{c}A \subset\mathcal{Z}\\ |A|=n\end{subarray}}\mathrm{I}(\boldsymbol{y}_{A};f).\] (22)

Clearly, \(\gamma_{n}\) depends on the kernel \(k\). In Table 2, we state the magnitude of \(\gamma_{n}\) for different kernels. We take the magnitudes from Theorem 5 of Srinivas et al. (2009) and Remark 2 of Vakili et al. (2021).

The maximum information gain can be interpreted analogously in the setting where \(f\in\mathcal{H}_{k,B}\), see Srinivas et al. (2009) and section 5.2 of Kirschner and Krause (2018). The following result relates mutual information and epistemic uncertainty.

**Lemma 12** (Lemma 5.3 from Srinivas et al. (2009)).: _For any \(A\subset\mathcal{Z}\), if \(f\sim\mathcal{GP}(0,k)\) or \(f\in\mathcal{H}_{k,B}\), and if the observation noise is i.i.d. \(\mathcal{N}(0;\sigma^{2})\) then_

\[\mathrm{I}(\boldsymbol{y}_{A};f)=\frac{1}{2}\sum_{i=1}^{n}\log\biggl{(}1+\frac {\mathrm{Var}[f(\boldsymbol{x}_{i})\mid\boldsymbol{y}(\boldsymbol{x}_{1:i-1}) ]}{\sigma^{2}}\biggr{)}\] (23)

_where \(\{\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{n}\}\stackrel{{\text{ def}}}{{=}}A\)._

In our setting, denote by \(A_{N}\stackrel{{\text{def}}}{{=}}(S_{1},\ldots,S_{N})\) the set of times of observations \(\mathcal{D}_{1:N}\) until episode \(N\). We define for every episode \(n\),

\[i_{n}^{*}\stackrel{{\text{def}}}{{=}}\operatorname*{ argmax}_{i\in\{1,\ldots,m_{n}\}}\left\|\boldsymbol{\sigma}_{n-1}(\boldsymbol{z}_{n}(t_{n,i}))\right\|_{2}^{2}.\] (24)

We write \(t_{n}^{*}\stackrel{{\text{def}}}{{=}}t_{n,i_{n}^{*}}\) and \(\tilde{A}_{N}\stackrel{{\text{def}}}{{=}}(\{t_{1}^{*}\},\ldots, \{t_{N}^{*}\})\). Note that \(\tilde{A}_{N}[n]\subseteq A_{N}[n]\)\((\forall n)\) and \(\tilde{A}_{N}\) comprises exactly \(N\) observations.

**Lemma 13**.: _If \(f_{j}\sim\mathcal{GP}(0,k)\) for all \(j\in\{1,\ldots,d_{x}\}\) or if \(\boldsymbol{f}\in\mathcal{H}_{k,B}^{d_{x}}\), and if the observation noise is i.i.d. \(\mathcal{N}(\boldsymbol{0};\sigma^{2}\boldsymbol{I})\) then_

\[\sum_{n=1}^{N}\frac{T}{m_{n}}\sum_{i=1}^{m_{n}}\left\|\boldsymbol{\sigma}_{n-1 }(\boldsymbol{z}_{n}(t_{n,i}))\right\|_{2}^{2}\leq 2\bar{\sigma}Td_{x}\gamma_{N}\] (25)

_where \(\bar{\sigma}=\frac{\sigma_{\max}^{2}}{\log(1+\sigma^{-2}\sigma_{\max}^{2})}\), \(\sigma_{\max}^{2}=\max_{\boldsymbol{z}\in\mathcal{Z},j\in\{1,\ldots,d_{x}\}} \sigma_{0,j}^{2}(\boldsymbol{z})\), and \(\gamma_{N}\) is with respect to kernel \(k\)._

\begin{table}
\begin{tabular}{l l l} \hline \hline Kernel & \(k(x,x^{\prime})\) & \(\gamma_{n}\) \\ \hline Linear & \(x^{\top}x^{\prime}\) & \(\mathcal{O}\left(d\log(n)\right)\) \\ RBF & \(e^{-\frac{\left\|x-x^{\prime}\right\|^{2}}{2l^{2}}}\) & \(\mathcal{O}\left(\log^{d+1}(n)\right)\) \\ MatÃ©rn & \(\frac{1}{\Gamma(\nu)2^{\nu-1}}\left(\frac{\sqrt{2\nu}\left\|x-x^{\prime} \right\|}{l}\right)^{\nu}B_{\nu}\left(\frac{\sqrt{2\nu}\left\|x-x^{\prime} \right\|}{l}\right)\) & \(\mathcal{O}\left(n^{\frac{d}{2\nu+d}}\log^{\frac{2\nu}{2\nu+d}}(n)\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Here we present different magnitudes of \(\gamma_{n}\). The magnitudes hold under the assumption that \(\mathcal{Z}\) is compact. Here, \(B_{\nu}\) is the modified Bessel function.

Proof.: We have

\[\sum_{n=1}^{N}\frac{T}{m_{n}}\sum_{i=1}^{m_{n}}\left\|\bm{\sigma}_{n-1}(\bm{z}_{n} (t_{n,i}))\right\|_{2}^{2}\leq T\sum_{n=1}^{N}\left\|\bm{\sigma}_{n-1}(\bm{z}_{n }(t_{n}^{*}))\right\|_{2}^{2}.\]

This inequality is tight when \(m_{n}=1\) (\(\forall n\)). When more than one measurement is obtained per episode then the regret may be smaller. Expanding the squared Euclidean norm, yields

\[=T\sum_{n=1}^{N}\sum_{j=1}^{d_{x}}\sigma_{n-1,j}^{2}(\bm{z}_{n}(t_{n}^{*})).\]

Write \(\tilde{\sigma}_{n,j}^{2}(\bm{z})\stackrel{{\text{def}}}{{=}} \operatorname{Var}[f_{j}(\bm{z})\mid\dot{\bm{y}}_{\tilde{A}_{N}}]\) whereas \(\sigma_{n,j}^{2}(\bm{z})=\operatorname{Var}[f_{j}(\bm{z})\mid\dot{\bm{y}}_{ \tilde{A}_{N}}]\). The variance is monotonically decreasing as one conditions on more observations, \(\tilde{\sigma}_{n,j}^{2}(\bm{z})\geq\sigma_{n,j}^{2}(\bm{z})\) (\(\forall n,j,\bm{z}\)), and hence,

\[\leq T\sum_{n=1}^{N}\sum_{j=1}^{d_{x}}\tilde{\sigma}_{n-1,j}^{2}(\bm{z}_{n}(t_ {n}^{*})).\]

Again, this bound is tight when only one measurement is obtained per episode, but may be loose otherwise. Lemma 15 of Curi et al. (2020) shows that \(\tilde{\sigma}_{n,j}^{2}(\bm{z})\leq\bar{\sigma}\log(1+\sigma^{-2}\tilde{ \sigma}_{n,j}^{2}(\bm{z}))\) for any \(n,j,\bm{z}\). Applying this inequality, we obtain

\[\leq\bar{\sigma}T\sum_{j=1}^{d_{x}}\sum_{n=1}^{N}\log\Biggl{(}1+\frac{\tilde{ \sigma}_{n-1,j}^{2}(\bm{z}_{n}(t_{n}^{*}))}{\sigma^{2}}\Biggr{)}.\]

By Lemma 12,

\[=2\bar{\sigma}T\sum_{j=1}^{d_{x}}\operatorname{I}\bigl{(}\dot{\bm {y}}_{\tilde{A}_{N}};f_{j}\bigr{)}\] \[\leq 2\bar{\sigma}Td_{x}\gamma_{N}.\]

Proof of Theorem 3.: We derive the regret bound for the adaptive MSS. The regret bounds for the other MSSs follow analogously.

From Corollary 11, recall the model complexity bound

\[\mathcal{I}_{N}(\bm{f}^{*},S^{\text{ADP}})\leq\max_{\begin{subarray}{c}\bm{ \pi}_{1},\ldots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}9\sum_{n=1}^{N}\frac{T}{m_{n}}\sum_{i=1}^{m_{ n}}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n}(t_{n,i})\right)\right\|^{2}.\]

By Lemma 13, we can further bound this by

\[\leq 18\bar{\sigma}Td_{x}\gamma_{N}.\]

Combining this bound with Proposition 1, with high probability the regret is bounded by

\[R_{N}(S^{\text{ADP}})\leq\mathcal{O}\Bigl{(}\beta_{N}T^{2}e^{L_{I}(1+L_{\bm{ \pi}})T}\sqrt{N\gamma_{N}}\Bigr{)}.\]

### Useful Facts and Inequalities

**Fact 14** (Gronwall's inequality, theorem 1.1 in chapter 3 of Hartman (2002)).: _Let \(g(t)\) be a non-negative continuous function on the interval \([a,b]\subset\mathbb{R}\), and let \(C,K\) be a pair of non-negative constants. If the function \(g\) satisfies_

\[g(t)\leq C+K\int_{a}^{t}g(s)\,ds,\quad\forall t\in[a,b],\] (26)

_then we have_

\[g(t)\leq Ce^{K(t-a)},\quad\forall t\in[a,b].\] (27)

**Lemma 15**.: _For any two metric spaces \((\mathcal{X},d_{\mathcal{X}})\) and \((\mathcal{Y},d_{\mathcal{Y}})\), and any two Lipschitz continuous functions \(f:\mathcal{X}\times\mathcal{X}\rightarrow\mathcal{Y}\) and \(g:\mathcal{X}\rightarrow\mathcal{X}\) with Lipschitz constants \(L_{f}\) and \(L_{g}\) respectively, we have that \(f(\cdot,g(\cdot))\) is \(L\)-Lipschitz continuous with respect to \(d_{\mathcal{Y}}\) with \(L\stackrel{{\text{def}}}{{=}}L_{f}(1+L_{g})\)._

Proof.: Fix any \(\bm{x},\bm{x^{\prime}}\in\mathcal{X}\). By the triangle inequality,

\[d_{\mathcal{Y}}(f(\bm{x},g(\bm{x})),f(\bm{x^{\prime}},g(\bm{x^{ \prime}}))) \leq d_{\mathcal{Y}}(f(\bm{x^{\prime}},g(\bm{x})),f(\bm{x},g(\bm{x })))+d_{\mathcal{Y}}(f(\bm{x^{\prime}},g(\bm{x})),f(\bm{x^{\prime}},g(\bm{x^{ \prime}})))\] \[\leq L_{f}d_{\mathcal{X}}(\bm{x},\bm{x^{\prime}})+L_{f}d_{ \mathcal{X}}(g(\bm{x}),g(\bm{x^{\prime}}))\] \[\leq L_{f}d_{\mathcal{X}}(\bm{x},\bm{x^{\prime}})+L_{f}L_{g}d_{ \mathcal{X}}(\bm{x},\bm{x^{\prime}})\] \[=L_{f}(1+L_{g})d_{\mathcal{X}}(\bm{x},\bm{x^{\prime}}).\]

## Appendix B Solving Optimistic Optimal Control Problem

The optimal control problem solved by OCoRL is constrained to \(L_{f}\)-Lipschitz continuous dynamics. When the dynamics are modeled with a GP using a linear kernel, the control problem can easily be solved directly in weight-space. Beyond this special case, it is commonly assumed that there exists an oracle which solves this optimization problem exactly (see, e.g., assumption 1 of Kakade et al. (2020)).

In this work, we do not explicitly address the problem of approximating the optimal control problem reasonably well. Kakade et al. (2020) give a brief overview of gradient- and sampling-based methods which may be useful (Jacobson and Mayne, 1970; Todorov and Li, 2005; Mordatch et al., 2012; Williams et al., 2017; Wagener et al., 2019). They alternatively suggest to use Thompson sampling (Thompson, 1933; Osband and Van Roy, 2014), that is, to sample \(\bm{f}_{n-1}\) from \(\mathcal{M}_{n-1}\) (which is straightforward when the dynamics are modeled by a GP, see Williams and Rasmussen (2006)) and then to compute and execute the optimal policy \(\bm{\pi}_{n}=\operatorname*{argmin}_{\bm{\pi}\in\Pi}C(\bm{\pi},\bm{f}_{n-1})\) using a planning oracle. Kakade et al. (2020) conjecture that corresponding regret bounds can be derived using standard techniques for analyzing Bayesian regret of Thompson sampling (Russo and Van Roy, 2014, 2016).

### Application to Arbitrary Discretizations

In this section, we briefly show that our guarantees carry over to discretized dynamics and costs for _arbitrary_ discretizations. Importantly, in the discrete-time setting the lemma mirroring Lemma 4 does not require the assumption that the models \(\bm{f}_{n}\) are \(L_{\bm{f}}\)-Lipschitz continuous, hence, simplifying the optimistic optimal control problem solved by OCoRL. In the discrete-time setting, it is sufficient to assume that \(\bm{f}^{*}\) is \(L_{\bm{f}}\)-Lipschitz continuous.

Consider the dynamical system

\[\bm{x}_{k+1}=\bm{h}^{*}(\bm{z}_{k},\tau_{k}),\quad\text{where}\quad\bm{z}_{k} \stackrel{{\text{def}}}{{=}}(\bm{x}_{k},\bm{\pi}(\bm{x}_{k}))\]

with initial state \(\bm{x}_{0}\in\mathcal{X}\) where \(\tau_{k}\in\mathbb{R}_{>0}\) measures the "time" between control points \(\bm{x}_{k}\) and \(\bm{x}_{k+1}\). We make the assumption that \(\bm{h}^{*}\) is \(L_{\bm{h}}\)-Lipschitz continuous in all arguments. The (discrete-time) control problem is

\[\bm{\pi}^{*}\stackrel{{\text{def}}}{{=}}\operatorname*{argmin}_{ \bm{\pi}\in\Pi}C(\bm{\pi},\bm{h}^{*})=\operatorname*{argmin}_{\bm{\pi}\in\Pi} \sum_{k=1}^{T}c(\bm{z}_{k}).\] (28)

In many applications, the cost to be minimized is defined at discrete control points to begin with. In this case, the continuous-time control of Equation (1) can be approximated arbitrarily well with Equation (28) by choosing a sufficiently dense discretization. Commonly, the discretization is taken to be equidistant, i.e., \(\tau_{k}\equiv\tau\ (\forall k)\), but we remark that our results hold more generally for any discretization.

#### b.1.1 Bounding Regret with the Model Complexity

In the following, we denote by \(\bm{x}_{n,k}\) the \(k\)-th state visited during episode \(n\).

**Lemma 16**.: _Assuming \(\bm{x}_{n,0}=\widehat{\bm{x}}_{n,0}\), the distance between the true and the hallucinated trajectory at any iteration \(k\geq 0\) is bounded with high probability by_

\[\left\|\widehat{\bm{x}}_{n,k}-\bm{x}_{n,k}\right\|\leq 2\beta_{n}(1+L_{ \bm{h}}^{\prime}+2\beta_{n}L_{\bm{\sigma}}^{\prime})^{T-1}\sum_{l=0}^{k-1} \left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,l}\right)\right\|\] (29)

_where we write \(L_{\bm{h}}^{\prime}\stackrel{{\text{def}}}{{=}}L_{\bm{h}}(1+L_ {\bm{\pi}})\) and \(L_{\bm{\sigma}}^{\prime}\stackrel{{\text{def}}}{{=}}L_{\bm{ \sigma}}(1+L_{\bm{\pi}})\)_

Proof (based on Lemma 4 of _Curi et al._ (2020)).: We begin by showing by induction that for any \(k\geq 0\),

\[\left\|\widehat{\bm{x}}_{n,k}-\bm{x}_{n,k}\right\|\leq 2\beta_{n} \sum_{l=0}^{k-1}(L_{\bm{h}}^{\prime}+2\beta_{n}L_{\bm{\sigma}}^{\prime})^{k-1 -l}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,l}\right)\right\|.\] (30)

The base case is implied trivially. For the induction step, assume that Equation (30) holds at iteration \(k\). We have

\[\left\|\widehat{\bm{x}}_{n,k+1}-\bm{x}_{n,k+1}\right\| =\left\|\bm{h}_{n-1}(\widehat{\bm{z}}_{n,k},\tau_{k})-\bm{h}^{*}( \bm{z}_{n,k},\tau_{k})\right\|\] \[\leq\left\|\bm{h}_{n-1}(\widehat{\bm{z}}_{n,k},\tau_{k})-\bm{h}^{* }(\bm{z}_{n,k},\tau_{k})\right\|\] \[\leq\left\|\bm{h}_{n-1}(\widehat{\bm{z}}_{n,k},\tau_{k})-\bm{h}^{* }(\widehat{\bm{z}}_{n,k},\tau_{k})\right\|+\left\|\bm{h}^{*}(\widehat{\bm{z}}_ {n,k},\tau_{k})-\bm{h}^{*}(\bm{z}_{n,k},\tau_{k})\right\|\] \[\leq 2\beta_{n}\left\|\bm{\sigma}_{n-1}\left(\widehat{\bm{z}}_{n, k}\right)\right\|+L_{\bm{h}}(1+L_{\bm{\pi}})\left\|\bm{x}_{n,k}-\widehat{\bm{x}}_ {n,k}\right\|\]

where the final inequality follows from Definition 2 (with high probability) and Lemma 15.

\[=2\beta_{n}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,k}\right)+ \bm{\sigma}_{n-1}\left(\widehat{\bm{z}}_{n,k}\right)-\bm{\sigma}_{n-1}\left( \bm{z}_{n,k}\right)\right\|\] \[\qquad\qquad\qquad\qquad\qquad+L_{\bm{h}}(1+L_{\bm{\pi}})\left\| \bm{x}_{n,k}-\widehat{\bm{x}}_{n,k}\right\|\]

Using that \(\bm{\sigma}_{n-1}\) is \(L_{\bm{\sigma}}\)-Lipschitz continuous and applying Lemma 15,

\[\leq 2\beta_{n}(\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,k}\right) \right\|+L_{\bm{\sigma}}(1+L_{\bm{\pi}})\left\|\widehat{\bm{x}}_{n,k}-\bm{x}_ {n,k}\right\|)\] \[\qquad\qquad\qquad\qquad\qquad+L_{\bm{h}}(1+L_{\bm{\pi}})\left\| \bm{x}_{n,k}-\widehat{\bm{x}}_{n,k}\right\|\] \[=2\beta_{n}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,k}\right)\right\| +\left(L_{\bm{h}}(1+L_{\bm{\pi}})+2\beta_{n}L_{\bm{\sigma}}(1+L_{\bm{\pi}}) \right)\left\|\bm{x}_{n,k}-\widehat{\bm{x}}_{n,k}\right\|\]

By the induction hypothesis,

\[\leq 2\beta_{n}\sum_{l=0}^{k}(L_{\bm{h}}^{\prime}+2\beta_{n}L_{ \bm{\sigma}}^{\prime})^{k-l}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,l}\right) \right\|.\]

Thus, Equation (30) holds. Since \(k\leq T\), we have

\[\left\|\widehat{\bm{x}}_{n,k}-\bm{x}_{n,k}\right\| \leq 2\beta_{n}\sum_{l=0}^{k-1}(L_{\bm{h}}^{\prime}+2\beta_{n}L_{ \bm{\sigma}}^{\prime})^{k-1-l}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,l} \right)\right\|\] \[\leq 2\beta_{n}\sum_{l=0}^{k-1}(1+L_{\bm{h}}^{\prime}+2\beta_{n}L_ {\bm{\sigma}}^{\prime})^{k-1-l}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,l} \right)\right\|\] \[\leq 2\beta_{n}(1+L_{\bm{h}}^{\prime}+2\beta_{n}L_{\bm{\sigma}}^{ \prime})^{T-1}\sum_{l=0}^{k-1}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,l}\right) \right\|.\]

**Lemma 17**.: _For any MSS \(S\), the regret of any episode \(n\) is bounded with high probability by_

\[r_{n}(S)\leq 2\beta_{n}T(1+L_{\bm{h}}^{\prime}+2\beta_{n}L_{\bm{\sigma}}^{ \prime})^{T-1}L_{c}^{\prime}\sum_{k=1}^{T}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_ {n,k}\right)\right\|\] (31)

_where \(L_{c}^{\prime}\stackrel{{\text{def}}}{{=}}L_{c}(1+L_{\bm{\pi}})\)._Proof.: By definition of \(\bm{\pi}_{n}\) in OCoRL we have with high probability, \(C(\bm{f}^{*},\bm{\pi}^{*})\geq C(\bm{f}_{n-1},\bm{\pi}_{n})\). Therefore, with high probability,

\[r_{n}(S) =C(\bm{f}^{*},\bm{\pi}_{n})-C(\bm{f}^{*},\bm{\pi}^{*})\] \[\leq C(\bm{f}^{*},\bm{\pi}_{n})-C(\bm{f}_{n-1},\bm{\pi}_{n})\] \[=\sum_{k=0}^{T}c(\bm{z}_{n,k})-c(\widehat{\bm{z}}_{n,k}).\]

By Lemma 15, we further have that

\[r_{n}(S) \leq L_{c}^{\prime}\sum_{k=1}^{T}\left\|\bm{x}_{n,k}-\widehat{\bm{ x}}_{n,k}\right\|.\]

By Lemma 16 (with high probability), we further have that

\[r_{n}(S) \leq 2\beta_{n}(1+L_{\bm{h}}^{\prime}+2\beta_{n}L_{\bm{\sigma}}^{ \prime})^{T-1}L_{c}^{\prime}\sum_{k=1}^{T}\sum_{l=0}^{k-1}\left\|\bm{\sigma}_{ n-1}\left(\bm{z}_{n,l}\right)\right\|\] \[=2\beta_{n}T(1+L_{\bm{h}}^{\prime}+2\beta_{n}L_{\bm{\sigma}}^{ \prime})^{T-1}L_{c}^{\prime}\sum_{k=1}^{T}\left\|\bm{\sigma}_{n-1}\left(\bm{z} _{n,k}\right)\right\|.\]

The above results allow us to prove a general regret bound which is analogous to Proposition 1.

**Lemma 18**.: _Let \(S\) be any MSS. If we run OCoRL with arbitrary discretization \(\bm{h}^{*}\), we have with high probability,_

\[R_{N}(S)\leq 2\beta_{N}T^{\frac{3}{2}}(1+L_{\bm{h}}^{\prime}+2\beta_{N}L_{ \bm{\sigma}}^{\prime})^{T-1}L_{c}^{\prime}\sqrt{N\tilde{\mathcal{I}}_{N}(\bm{ h}^{*},S)}\] (32)

_where we define the discrete-time model complexity,_

\[\tilde{\mathcal{I}}_{N}(\bm{h}^{*},S)\stackrel{{\text{def}}}{{= }}\max_{\begin{subarray}{c}\bm{\pi}_{1},\cdots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}\sum_{n=1}^{N}\sum_{k=1}^{T}\left\|\bm{ \sigma}_{n-1}\left(\bm{z}_{n,k}\right)\right\|^{2}.\] (33)

Proof.: Let us first bound \(R_{N}^{2}(S)\). By the Cauchy-Schwarz inequality,

\[R_{N}^{2}(S) \leq N\sum_{n=1}^{N}r_{n}^{2}(S).\]

By Lemma 5 (with high probability), we have that

\[R_{N}^{2}(S) \leq N4T^{2}L_{c}^{\prime 2}\sum_{n=1}^{N}\beta_{n}^{2}(1+L_{\bm{h} }^{\prime}+2\beta_{n}L_{\bm{\sigma}}^{\prime})^{2(T-1)}\left(\sum_{k=1}^{T} \left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,k}\right)\right\|\right)^{2}\] \[\leq N4\beta_{N}^{2}T^{3}(1+L_{\bm{h}}^{\prime}+2\beta_{N}L_{\bm{ \sigma}}^{\prime})^{2(T-1)}L_{c}^{\prime 2}\sum_{n=1}^{N}\sum_{k=1}^{T}\left\|\bm{ \sigma}_{n-1}\left(\bm{z}_{n,k}\right)\right\|^{2}.\]

Taking the square root, we obtain

\[R_{N}(S) \leq 2\beta_{N}T^{\frac{3}{2}}(1+L_{\bm{h}}^{\prime}+2\beta_{N}L_{ \bm{\sigma}}^{\prime})^{T-1}L_{c}^{\prime}\sqrt{N\sum_{n=1}^{N}\sum_{k=1}^{T} \left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,k}\right)\right\|^{2}}.\]

The result follows by noting that

\[\sum_{n=1}^{N}\sum_{k=1}^{T}\left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,k}\right) \right\|^{2}\leq\tilde{\mathcal{I}}_{N}(\bm{h}^{*},S).\]

#### b.1.2 Bounding Model Complexities

The model complexity bounds from the continuous-time setting (c.f., Appendix A.2) extend seamlessly to the discrete-time setting. In the following we briefly sketch the bound for the oracle MSS.

**Lemma 19** (Discrete-time oracle model complexity).: _For any \(N\geq 1\),_

\[\tilde{\mathcal{I}}_{N}(\bm{h}^{*},S^{\text{QRA}})\leq\max_{ \begin{subarray}{c}\bm{\pi}_{1},\dots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}T\sum_{n=1}^{N}\left\|\bm{\sigma}_{n-1} \left(\bm{z}_{n}(t_{n,1})\right)\right\|^{2}.\] (34)

Proof.: \[\tilde{\mathcal{I}}_{N}(\bm{h}^{*},S^{\text{ORA}}) =\max_{\begin{subarray}{c}\bm{\pi}_{1},\dots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}\sum_{n=1}^{N}\sum_{k=1}^{T}\left\|\bm{ \sigma}_{n-1}\left(\bm{z}_{n,k}\right)\right\|^{2}\] \[\leq\max_{\begin{subarray}{c}\bm{\pi}_{1},\dots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}T\sum_{n=1}^{N}\max_{k\in\{1,\dots,T\}} \left\|\bm{\sigma}_{n-1}\left(\bm{z}_{n,k}\right)\right\|^{2}\] \[\leq\max_{\begin{subarray}{c}\bm{\pi}_{1},\dots,\bm{\pi}_{N}\\ \bm{\pi}_{n}\in\Pi\end{subarray}}T\sum_{n=1}^{N}\left\|\bm{\sigma}_{n-1} \left(\bm{z}_{n}(t_{n,1})\right)\right\|^{2}.\]

Note that the MSS \(S\) can (but not has to) be restricted to the discretization of \([0,T]\) used for the dynamics and costs. The derivations for the equidistant and adaptive MSSs follow analogously, where we note that the bucket length \(\Delta_{n}\) is to be defined with respect to a modified constant \(\Gamma_{n}\).

Finally, applying the measurement uncertainty bound presented in Appendix A.3, we obtain the following theorem.

**Theorem 20**.: _Fix an arbitrary discretization \(\bm{h}^{*}\) and assume that \(\bm{h}^{*}\in\mathcal{H}^{d_{x}}_{n,B}\), the observation noise is i.i.d. \(\mathcal{N}(\bm{0};\sigma^{2}\bm{I})\), and let \(\left\|\cdot\right\|\) be the Euclidean norm. We model \(\bm{h}^{*}\) with the GP model. The regret for the adaptive MSS is with probability at least \(1-\delta\) bounded by_

\[R_{N}(S^{\text{ADP}})\leq\mathcal{O}\left(\beta_{N}T^{2}(1+L^{ \prime}_{\bm{h}}+2\beta_{N}L^{\prime}_{\bm{\sigma}})^{T-1}\sqrt{N\gamma_{N}} \right),\qquad\qquad m^{\text{ADP}}_{n}=\mathcal{O}(\beta_{n}).\]

## Appendix C Experimental Setup

### System's dynamics

Here we either give reference to the equations of the dynamical system or provide their equations:

* Cancer Treatment: \[\dot{x}(t)=rx(t)\log\left(\frac{1}{x(t)}\right)-\delta u(t)x(t)\] \[x(0)=0.975,r=0.3,\delta=0.45\]
* Glucose in Blood: \[\dot{x}_{0}(t)=-ax_{0}(t)-bx_{1}(t)\] \[\dot{x}_{1}(t)=-cx_{1}(t)+u(t),\] \[a=1,b=1,c=1,x(0)=(0.75,0)\]
* Pendulum: \[\dot{x}_{0}(t)=x_{1}(t)\] \[\dot{x}_{1}(t)=\frac{g}{l}\sin(x_{0}(t))+u(t)\] \[g=9.81,l=5.0\]

[MISSING_PAGE_FAIL:27]

Greedy Max Kernel DistanceThe second adaptive MSS we consider in Section 5 is the _Greedy Max Kernel Distance_. Here we greedily solve the metric \(M\)-center problem in space \([0,T]\) with the kernel metric: \(d_{n}(t,t^{\prime})^{2}\stackrel{{\text{def}}}{{=}}k_{n}(\widehat{ \bm{z}}_{n}(t),\widehat{\bm{z}}_{n}(t))+k_{n}(\widehat{\bm{z}}_{n}(t^{\prime}), \widehat{\bm{z}}_{n}(t^{\prime}))-2k_{n}(\widehat{\bm{z}}_{n}(t),\widehat{\bm {z}}_{n}(t^{\prime}))\) on the hallucinated trajectory.

``` \(\bm{Init}\):\(S_{n}=\{\operatorname*{argmax}_{t\in[0,T]}k_{n}(\widehat{\bm{z}}_{n}(t),\widehat{\bm{z}}_{n}(t))\}\) for\(k=2,\dots,M\)do \[S_{n}=S_{n}\cup\left\{\operatorname*{argmax}_{t\in[0,T]}\min_{t^{\prime}\in S _{n}}d_{n}(t,t^{\prime})\right\}\] ```

**Algorithm 2**Greedy Max Kernel Distance

### Episodes Hyperparameters

### Practical Implementation

In practice, we approach solving the optimal control problem (2) with _offline planning_ and _online tracking_ strategy. Before the episode starts we obtain an open loop trajectory \(\tilde{\bm{x}}_{n}(t),\tilde{\bm{u}}_{n}(t)\) by offline planning:

\[\begin{split}\tilde{\bm{x}}_{n}(t),\tilde{\bm{u}}_{n}(t)& =\operatorname*{argmin}_{\bm{x}(t),\bm{u}(t)}\min_{\eta(t)}\int_{0 }^{T}c(\bm{x}(t),\bm{u}(t))\,dt\\ \text{s.t.}&\dot{\bm{x}}(t)=\bm{\mu}_{n}(\bm{x}(t), \bm{u}(t))+\beta_{n}\bm{\sigma}_{n}(\bm{x}(t),\bm{u}(t))\bm{\eta}(t)\\ &\bm{\eta}(t)\in[-1,1]^{d_{x}},\quad\forall t\in[0,T].\end{split}\] (36)

We solve the optimization problem (36) with Iterative Linear Quadratic Regulator (ILQR) (Li and Todorov, 2004). Then we track the open loop trajectory \(\tilde{\bm{x}}_{n}(t)\) online with MPC:

\[\begin{split}\bm{u}(t)&=\operatorname*{argmin}_{\bm {u}(t),\bm{x}(t)}\int_{s}^{s+T_{MPC}}\left(\left\|\bm{x}(t)-\tilde{\bm{x}}(t) \right\|_{2}^{2}+\left\|\bm{u}(t)-\tilde{\bm{u}}(t)\right\|_{2}^{2}\right)\,dt \\ \text{s.t.}&\dot{\bm{x}}(t)=\bm{\mu}_{n}(\bm{x}(t), \bm{u}(t))\end{split}\] (37)

Here, horizon \(T_{MPC}\) depends on the system.

\begin{table}
\begin{tabular}{l c c c} \hline \hline System & Number of & Number of Measurements & Time \\  & episodes \(N\) & per episodes \(M\) & horizon \(T\) \\ \hline Cancer Treatment & 20 & 10 & 20 \\ Glucose in Blood & 20 & 10 & 0.45 \\ Pendulum & 20 & 10 & 10 \\ Mountain Car & 40 & 10 & 1 \\ Cart Pole & 40 & 10 & 10 \\ Furuta Pendulum & 40 & 10 & 10 \\ Bicycle & 20 & 10 & 10 \\ Quadrotor 2D & 20 & 10 & 8 \\ Quadrotor 3D & 25 & 20 & 15 \\ \hline \hline \end{tabular}
\end{table}
Table 4: We take a few tens of measurements per episode in each environment. We run the experiments for at most 40 episodes in each environment.

[MISSING_PAGE_EMPTY:29]