# PowerPM: Foundation Model for Power Systems

 Shihao Tu

Zhejiang University

shihao.tu@zju.edu.cn

&Yupeng Zhang

Zhejiang University

yuppzhang@zju.edu.cn

&Jing Zhang

Renmin University of China

zhang-jing@ruc.edu.cn

&Zhendong Fu

Zhejiang University

zhendongfu@zju.edu.cn

&Yin Zhang

Zhejiang University

yinzh@zju.edu.cn

&Yang Yang

Zhejiang University

yangya@zju.edu.cn

These authors contributed equally to this work.Corresponding authors.

###### Abstract

The proliferation of abundant electricity time series (ETS) data presents numerous opportunities for various applications within power systems, including demand-side management, grid stability, and consumer behavior analysis. Deep learning models have advanced ETS modeling by effectively capturing sequence dependence. However, learning a generic representation of ETS data for various applications is challenging due to the inherently complex hierarchical structure of ETS data. Moreover, ETS data exhibits intricate temporal dependencies and is susceptible to the influence of exogenous variables. Furthermore, different instances exhibit diverse electricity consumption behavior. In this paper, we propose a foundation model PowerPM for ETS data, providing a large-scale, off-the-shelf model for power systems. PowerPM consists of a temporal encoder and a hierarchical encoder. The _temporal encoder_ captures temporal dependencies within ETS data, taking into account exogenous variables. The _hierarchical encoder_ models correlations between different levels of hierarchy. Furthermore, PowerPM leverages a novel self-supervised pre-training framework consisting of _masked ETS modeling_ and _dual-view contrastive learning_. This framework enables PowerPM to capture temporal dependency within ETS windows and aware the discrepancy across ETS windows, providing two different perspectives to learn generic representation. Our experiments span five real-world scenario datasets, including both private and public data. Through pre-training on massive ETS data, PowerPM achieves SOTA performance on diverse downstream tasks within the private dataset. Notably, when transferred to public datasets, PowerPM retains its edge, showcasing its remarkable generalization ability across various tasks and domains. Moreover, ablation studies and few-shot experiments further substantiate the effectiveness of our model.

## 1 Introduction

The volume of Electricity Time Series (ETS) data has recently increased rapidly due to the emergence of advanced power systems known as smart grids [10]. This abundance of data has paved the way for diverse applications in power systems, including demand-side management [22], grid stability [2] and consumer behavior analysis [49], etc. Meanwhile, these applications have spawned various tasks, as shown in Fig. 1(d). These include load forecasting [27, 4], clock anomaly detection [46], electricity theft [15] and and the detection of elderly individuals living alone [45].

As society progresses towards modernization, electricity consumption is rapidly increasing, presenting opportunities and challenges for the development and application of smart grids. On one hand, the substantial economic benefits that accompany this significant electricity usage are considerable. On the other hand, unreasonable electricity planning can have a detrimental impact on the environment[30]. Therefore, given the large volume of data and the variety of tasks, there is an urgent need to study effective ETS data modeling methods for these tasks, so as to improve economic efficiency while adhering to low-carbon principles.

Recently, numerous research studies on pre-training approaches for ETS data have emerged. These approaches adopt the "pre-training then fine-tuning" paradigm to deal with the dilemma of limited annotation data, and the pre-trained model to easily adapt to new tasks, such as PatchTST [21], TS2Vec [42], CoST [37], etc. However, these pre-training methods only utilize small-scale of data with a small number of instances (e.g. users), resulting in poor performance on downstream tasks. As the same time, many researcher begin to apply Large Language Models (LLMs) to assist time series modeling by using pre-trained LLM to encode time series [51] or incorporating additional descriptions related to the time series [17; 20]. Nevertheless, these models have limited ability in the power system scenario due to insufficient pre-training data of power systems and the lack of sufficient domain-specific knowledge. Additionally, none of these models are tailored for the scenario of power systems, so they neglect the unique characteristics of ETS data. Consequently, there remains a significant research gap in existing power systems literature regarding the modeling of ETS data using a foundation model.

In our scenario, the ETS data contains numerous instances and naturally exhibits a complex hierarchy [41; 23]. As depicted in Fig. 1(a), a city ETS can be disaggregated into district ETS according to the administrative divisions, which can further be disaggregated into user ETS in this district. For the complex hierarchy of ETS data, modeling ETS data entails the consideration of several challenges:

**(1) Hierarchical Dependency Modeling.** The hierarchy of ETS data facilitates information interaction across different granularities. Fine-grained ETS provides detailed insights into individual electricity usage, while coarse-grained ETS for districts and cities captures broader factors and indicates overall trends. For example, user-level data reflects user-specific behaviors and city-level data encompasses demographics and policy effects [29; 35]. Integrating these levels of granularity to provide both macro and micro perspectives is a complex task that requires sophisticated modeling.

**(2) Temporal Dependencies within ETS Window.** An ETS window refers to a piece of electricity time series over a period of time. The temporal dependencies within an ETS window refer to the correlations and dependencies between observations at different timestamps. As shown in Fig. 1(b), the city-level ETS exhibits daily and weekly dependency. Moreover, the temporal dependencies are often influenced by exogenous variables, such as weather, temperature, and seasonal effects. Integrating these factors into the model is challenging because their impact may interact with the temporal dynamics in complex ways. Accurately capturing the temporal dependencies with the impact of exogenous variables is a key challenge in modeling ETS data.

**(3) Discrepancy across ETS Windows.** The patterns observed in ETS windows can vary significantly across different instances and different timestamps. For instance, as shown in Fig. 1(c), residential electricity consumption (_User A_) reaches its peak in the mornings and evenings, used for lighting, appliances, and heating. However, electricity usage typically declines during the day because residents

Figure 1: (a) The hierarchical structure of ETS data. (b) The temporal dependency within ETS data and the influence of exogenous variables. (c) Different electricity consumption behaviors exist across time and instances. (d) Various tasks in power systems.

are generally absent, being engaged in work or education activities outside the home. Moreover, industries (_User B_) have high power demand during specific daytime periods for machinery and production lines, with lower load requirements during nighttime and weekends. These variations in behavior highlight the challenge of achieving consistency across ETS windows in personalized modeling.

To address these challenges, we propose a foundation model for power systems named **P**ower **P**re-trained **M**odel (PowerPM), as illustrated in Figure 3. PowerPM contains about \(250\)M parameters and is pre-trained on large-scale hierarchical ETS data with \(987.42\)GB. Specifically, we employ the "pre-training then fine-tuning" paradigm to learn generic representations by pre-training on hierarchical ETS data and to unify various tasks by fine-tuning on downstream data. During pre-training stage, we propose a novel self-supervised pre-training framework consisting of _masked ETS modeling_ and _dual-view contrastive learning_, which enables PowerPM to capture temporal dependency within ETS windows and aware the discrepancy across ETS windows, so as to provide two different perspectives to learn universal representations. PowerPM mainly consists of two modules, namely, _temporal encoder_ and _hierarchical encoder_. The _temporal encoder_ employs Transformer encoders to capture the temporal dependency in ETS data, and incorporates exogenous variables to make the modeling process more robust. Moreover, to model hierarchical dependency, _hierarchical encoder_ utilizes R-GCN [25] to propagate information about the correlation between hierarchy. According to the message that passes through the hierarchies, the micro and macro information can effectively assist in modeling the ETS data. In summary, the main contributions of our work include:

1. We propose a foundation model for power systems named PowerPM, which is pre-trained on large-scale ETS data and provide an off-the-shelf model for power systems.
2. To the best of our knowledge, PowerPM is the first to date that considers temporal dependency and hierarchical dependency simultaneously. In addition, we present a novel self-supervised pre-training framework that combines masked ETS modeling and dual-view contrastive learning, enhancing the model's ability to learn temporal dependencies within ETS windows and aware the discrepancy across ETS windows.
3. Extensive experiments show that PowerPM generalizes well to \(44\) downstream tasks. Fig. 2 summarizes the results of all the downstream tasks, showing its great potential in ETS data modeling. Moreover, when transferred to the public dataset, PowerPM maintains its superiority, showcasing its remarkable generalization ability across various tasks and domains. Further analysis illustrates the effectiveness of PowerPM as well.

## 2 Methodology

**Overview.** As shown in the middle part of Fig. 3: Firstly, the hierarchical graph \(\mathcal{G}\) is constructed according to the naturally existing hierarchical relationship of ETS data. The ETS windows in \(\mathcal{G}\) and its corresponding exogenous variables are denoted as \(\{\bm{x}_{i}\}_{i=1}^{N}\) and \(\{\bm{o}_{i}\}_{i=1}^{N}\), where \(N\) is the number of instances, \(\bm{x}_{i}\in\mathbb{R}^{T_{w}}\), \(\bm{o}_{i}\in\mathbb{R}^{T_{w}\times K}\), and each instance ETS window spans \(T_{w}\) time points starting at \(T_{a}\) and ending at \(T_{b}\). Each time point has \(K\) kinds of exogenous variables. Our objective is to perform

Figure 2: Performance comparison of our model and other baseline models on all downstream tasks in our scenario. Model performances are plotted on \(3\) radar subfigures for clarity with the same coordinate range.

pre-training on an encoder \(f(\cdot)\) to encode each window into a latent representation \(\mathbf{z}_{i}\in\mathbb{R}^{N\times d}\), where \(d\) indicates the dimension of the latent representation. More specific, PowerPM consists of an exogenous variable enhanced temporal encoder \(f_{T}(\cdot)\) and a hierarchical encoder \(f_{H}(\cdot)\), with the process: \(\mathbf{z}_{i}=f(\bm{x}_{i},\bm{o}_{i},\mathcal{G})=f_{H}(f_{T}(\bm{x}_{i}, \bm{o}_{i}),\mathcal{G})\). In addition, a novel self-supervised strategy which combines masked ETS modeling and dual-view contrastive learning is used for pre-training PowerPM. Next, we will detail the techniques in both model architecture and pre-training strategy.

### Hierarchical Graph Construction

The data of cities, districts, and users in ETS data naturally form a hierarchical relationship, based on which we can construct a hierarchical graph. However, the imbalance in the number of users and districts means there will be multitude of edges between user nodes and district nodes, which significantly increases the complexity of graph modeling. To address this, we employ a clustering strategy to create intermediary nodes, which is a common approach to implement graph sparsification [13] and a user group policy in the power systems [36; 44; 12]. As depicted in Fig. 3 (c), we use clustering method to categorize users into several clusters, the detailed process can be found in App. B.1. The cities are bidirectionally connected to districts, and these user clusters are also bidirectionally connected to districts but are unidirectionally connected to districts. By sparsifying the edges, we enhance the efficiency of graph modeling. Mathematically, we represent the hierarchy as a directed graph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathcal{R})\), where \(\mathcal{V}\) is the set of nodes, each node corresponds to an instance, \(\mathcal{E}\) is the set of directed edges, and \(\mathcal{R}\) is the set of type of edges (e.g. user cluster \(\rightarrow\) district, district \(\rightarrow\) user, etc.).

### Temporal Encoder with Exogenous Variables

**Patching.** In the \(\mathcal{G}\), each node's feature \(\bm{x}_{i}\) is a window of ETS data corresponding to instance \(i\). Due to the semantic sparsity of time series, we patch each window \(\bm{x}_{i}\) into \(N_{p}\) segments, each of length \(P\), resulting in \(\mathbf{p}_{i}\in\mathbb{R}^{N_{p}\times P}\), where \(N_{p}=\lceil\frac{T_{w}-P}{S}\rceil+1\), and this method proved its validity in many works [21; 17; 20]. Subsequently, a linear projection is applied to each segment to obtain the window representation \(\mathbf{h}_{i}\in\mathbb{R}^{N_{p}\times d}\).

**Exogenous Variables Encoding.** To efficiently interact with exogenous variables, we model these variables using learnable embeddings \(\mathbf{E}\in\mathbb{R}^{(\sum_{k=0}^{K-1}M_{k})\times d}\), where \(K\) indicates the number of exogenous variables (e.g. weather type and temperature), \(M_{k}\) represents the number of value types of the \(k\)-th exogenous variable (e.g. sunny and rainy in weather type variable). The exogenous variables \(\bm{o}_{i}^{(k)}\in\mathbb{R}^{N_{p}\times P}\) corresponding to \(\mathbf{p}_{i}\) of the \(k\)-th exogenous variable are used to obtain representations of the exogenous variables from \(\mathbf{E}\), indexing out \(\mathbf{e}_{i}^{(k)}\in\mathbb{R}^{N_{p}\times d}\), as illustrated in

Figure 3: The pre-training framework of PowerPM. For simplicity, we take the windows of each instance in the same time range for illustration, and the window process at other times is the same.

Fig. 3 (b). Subsequently, we derive a representation \(\mathbf{u}_{i}\in\mathbb{R}^{N_{p}\times d}\) that considers the window's exogenous variable influence: \(\mathbf{u}_{i}=\mathbf{h}_{i}+\sum_{k=0}^{K-1}\mathbf{e}_{i}^{(k)}\).

**Temporal Encoder.** To model the complex temporal dependency and interaction with exogenous variables, we use the vanilla Transformer encoder [34] to encode \(\mathbf{u}_{i}\), resulting in an augmented temporal representation \(\hat{\mathbf{z}}_{i}\in\mathbb{R}^{N_{p}\times d}\).

### Hierarchical Encoder

To model the complex correlation across different hierarchies, we employ Graph Neural Networks (GNNs). GNNs have recently become increasingly popular for modeling relationships within time series data, which enhances temporal representation [7; 26; 40]. In addition, considering that the correlation relationships of different edges are distinct, we adopt R-GCN [25] to integrate information across various hierarchies and instances, as depicted in Fig 3 (a). Specifically, we use R-GCN to update the representation \(\hat{\mathbf{z}}\) by considering its neighboring nodes in \(\mathcal{G}\), with the final node representation denoted as \(\mathbf{z}_{i}\in\mathbb{R}^{N_{p}\times d}\). Moreover, we use \(\mathbf{z}_{i}\) to perform self-supervised pre-training.

### Self-supervised Pre-training

#### 2.4.1 Masked ETS Modeling

To model temporal dependency within an ETS window, we have adopted the widely utilized masked reconstruction strategy. Nevertheless, existing random masking methods may face a significant challenge: they reconstruct the missing part based on the known surrounding part [21; 8], without considering the prediction of future parts relying solely on the past part. This approach not only diminishes the difficulty of the pre-training stage but also lacks consistency across pre-training task and forecasting task.

To address this issue, we propose a novel masking approach that combines random and casual masking, as shown in Fig. 3 (d) (_left_). Specifically, we randomly select one of the masking approaches for a given patched window \(\mathbf{p}_{i}\), resulting in **masked \(\mathbf{p}_{i}\)**. This approach not only retains the benefits of the random masking strategy but also ensures that the model learns to predict future parts based solely on past information, thereby it can more comprehensively capture the temporal dependencies within a window. Mathematically, this can be formulated as: **masked \(\mathbf{p}_{i}=\begin{cases}\text{Mask}_{r}(\mathbf{p}_{i})&\text{if }\alpha<0.5\\ \text{Mask}_{c}(\mathbf{p}_{i})&\text{otherwise }\end{cases}\)**, where \(\text{Mask}_{r}\) and \(\text{Mask}_{c}\) denote the random and causal masking, respectively, and \(\alpha\in[0,1]\) is a uniformly distributed variable. Specifically, after the \(\bm{x}_{i}\) is inputted into PowerPM for masked ETS modeling, we will obtain a reconstructed \(\hat{\bm{x}}_{i}\). The corresponding reconstruction loss is: \(\mathcal{L}_{MSE}=\frac{1}{N}\sum_{i=1}^{N}(\bm{x}_{i}-\hat{\bm{x}}_{i})^{2}\).

#### 2.4.2 Dual-view Contrastive Learning

The objective of contrastive learning is to learn representations by bringing positive pairs closer and pushing negative pairs farther apart in the latent space [5; 6]. Motivated by this, to make PowerPM aware of the discrepancy across ETS windows, we employ dual-view contrastive learning (DVCL) to discern subtle differences in electricity usage behavior.

**Positive and Negative Sample Pairs.** These pairs are determined from two views: one is _temporal view_, which is based on the time difference between the two windows. Another is the _instance view_, which depends on whether two windows belong to the same instance. For the same instance, the closer the time difference between two windows, the closer their representations are likely to be. This idea is also presented in [31; 42]. Conversely, windows from different instances or the same instance with a larger time difference are likely to have more distinct representations. Overall, we consider adjacent windows from the same instance as positive samples, while windows from different instances or non-adjacent windows from the same instance are negative samples. As depicted in Fig. 3 (d) (_right_), for the district node \(V\) in \(\mathcal{G}\), the original start timestamp about this window is \(T_{a}\). After shifting several time steps \(\delta\) on, we obtain another window \(V^{+}\) starting at \(T_{a}+\delta\), which serves as a positive sample. Meanwhile, we select windows from other nodes in \(\mathcal{G}\), such as city \(P\), starting at \(T_{a}\), as well as windows from the same node \(V\) but starting at \(T_{c}\), where \(|T_{c}-T_{a}|\gg\delta\). These windows serve as instance and temporal negative samples, respectively, and are denoted as \(P^{-}\) and \(V^{-}\).

Mathematically, given an ETS window \(\bm{x}_{i}\), we obtain a positive sample \(\bm{x}_{i}^{+}\) by shifting it by \(\delta\) time steps. The other samples in this batch serve as negative samples, totaling \(B-1\) negative samples, where \(B\) is the batch size during pre-training. The DVCL loss is: \(\mathcal{L}_{DVCL}=-\sum_{i=1}^{N}\log\frac{\exp\left(\text{sim}\left(f(\bm{x}_ {i}),f(\bm{x}_{i}^{+})\right)/\tau\right)}{\sum_{m=1}^{B}\text{exp}\left(\text {sim}\left(f(\bm{x}_{i}),f(\bm{x}_{m})\right)/\tau\right)}\), where \(\mathbf{I}\) is the boolean vector to select the negative pairs and \(\text{sim}(\cdot)\) is cosine similarity function.

## 3 Experiments

### Experiment Setup

**Pre-training Dataset.** PowerPM is pre-trained on a mount of ETS data, a private dataset from the real scenario1. This pre-training dataset encompasses ETS data of cities, districts, and users, covering over \(3\) years records. The ETS data is collected at a frequency of one data point every \(15\) minutes. More details are in App. A

Footnote 1: Due to privacy concerns of the dataset and the company, we mask the specific information.

Downstream Dataset.To evaluate the performance of PowerPM, we conduct comprehensive experiments on eleven downstream private and public datasets. And seven private datasets are also collected from real scenario. These datasets have different labels for different tasks. Among them, the solar generation dataset does not have a hierarchical structure due to its particularity. Four public datasets are obtained from CSISO 2, ISONE3, NYISO 4, and PJM 5, and they all exhibit a hierarchical structure. Further details can be found in Appendix A.

Footnote 2: http://www.energyonline.com/Data/

Footnote 3: https://www.iso-ne.com/isoexpress/web/reports/load-and-demand/

Footnote 4: https://www.nyiso.com/load-data

Footnote 5: https://dataminer2.pjm.com/list

Settings.For the model configurations, the temporal encoder contains a \(26\)-layer Transformer encoder with model dimension \(1024\), inner dimension (FFN) \(2048\) and \(16\) attention heads, and the hierarchical encoder contains \(2\)-layer R-GCN. PowerPM contains about 250M parameters. During pre-training, the \(40\%\) segments in each input window are masked in the form of random mask and casual mask, the user cluster numbers is set to \(12\). See further details in App. B.1

Baselines.We compare with \(8\) state-of-the-art methods: Large Language Model (LLM) enhanced models: GPT4TS [51], Time-LLM [17], UniTime [20]; pre-train models: PatchTST [21], CoST [37], TS2Vec [42]; supervised models: DLinear [43], TimesNet [38]. More implementation details are provided in App. B.2.

Evaluation Metrics.For forecasting and imputation tasks, we use mean squared error (MSE): \(\frac{1}{n}\sum_{i=1}^{n}\left(\bm{y}-\hat{\bm{y}}\right)^{2}\) and mean absolute error (MAE): \(\frac{1}{n}\sum_{i=1}^{n}\left|\bm{y}-\hat{\bm{y}}\right|\) as the evaluation metric. For classification tasks, we use accuracy as the metric. The metric of the anomaly detection task includes precision, recall, \(F0.5\), and \(F1\) scores. The \(F\beta\) is a metric defined as the weighted harmonic mean of precision and recall, with the following equation: \(F\beta=\frac{(1+\beta^{2})\times precision\times recall}{\beta^{2}\times precision +recall}\). We use \(F0.5\) for anomaly detection, since precision is more important than recall in power systems scenario [15].

### Downstream Tasks

Demand-side Management.Demand-side management aims to optimize and balance the power system by managing and adjusting the electricity demand of end-users. We develop tasks to predict load at different levels (such as cities and users) and tasks to forecast solar generation. With demand-side management, we can better plan and schedule power resources, improve energy efficiency, promote the development of renewable energy, and achieve sustainable energy management.

Grid Stability.To ensure the stability of the power grid, we have implemented a series of tasks, including electricity theft detection, load imputation, and clock anomaly detection, to address the impact of potential appliance failures within the grid and external electricity theft on the quality of power data and grid operations. Internal appliance malfunctions within the grid such as clock anomalies or the inability to record electricity usage accurately decrease the accuracy of power data, making it challenging for power dispatch and management. Additionally, external electricity theft 

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

Therefore, the utilization of a larger model with higher capacity and large ETS data enables better generalization across a wide range of downstream tasks.

## 4 Related Work

Self-supervised Pre-training Model.Large-scale model based on self-supervised pre-training has become more significant in both industrial and academic domains due to the versatility and impressive performance. It initially developed in the fields of computer vision [14] and natural language processing [8; 11]. Self-supervised pre-training in time series is typically classified into two paradigms: contrastive learning and mask modeling. The objective of contrastive learning is to learn representation by pushing positive pairs closer and pull negative pairs away in the embedding space [16]. TS2Vec [42] proposes contextual consistency for positive pair selection. Then, CoST [37] extracts the trend and seasonal feature representations, and takes advantage of both time and frequency domain contrastive loss to encourage discriminative seasonal representation. And TF-C [47] applies time-frequency consistency for embedding time-based and frequency-based neighbors. In mask modeling, to extract the contextual semantic information, PatchTST [21] masks at the series-level.

Supervised Learning Model.Since the self-attention mechanism in Transformer [33] showed the great ability to seize global dependencies between input and output, recently many variants have been proposed to tackle power system tasks. LogTrans [19], Informer [48] reduce the complexity by optimizing the vanilla self-attention mechanism. Autoformer [39] leverages auto-correlation mechanism to achieve series-wise representation aggregation. FEDformer [50] incorporates frequency-domain information to enhances prediction performance while reducing complexity to linear levels. Besides, DLinear [43] questions the effectiveness of transformers as it outperforms most Transformer-based SOTAs, with a simple linear model. TimesNet [38] has treated time series as a \(2D\) signal and utilized a convolution-based inception net backbone to function as a comprehensive time series model.

Large Language Models Enhanced Model.Recently, the advancement of Large Language Models (LLMs) has opened up new horizons in time series modeling. Many LLMs, such as llama [32], GPT-3 [11], GPT-4 [1], ChatGLM [9] have the capability to capture complex dependencies and understand varied textual data, yielding sensible reasonable generation results. Therefore, many researchers begin to apply LLMs to assist time series modeling. Time-LLM [17] and TEXT [28] employ reprogrammed input time series with text prototype embedding and incorporate textual prompts for time series. GPT4TS [51] and UniTime [20] apply fine-tuning to selected components of LLMs to improve performance in time series analysis tasks. TEMPO [3] incorporates the decomposition of time series and retrieval-based prompt design for non-stationary time series data.

However, despite numerous methods for self-supervised and supervised time series, the research on foundation models specifically designed for power systems remains relatively sparse. And LLMs are limited in power systems scenario, lacking enough textual descriptions for domain knowledge.

## 5 Conclusion

This paper introduces the PowerPM, a foundational model designed to model ETS data within power systems. PowerPM consists of a _temporal encoder_ and a _hierarchical encoder_. Furthermore, PowerPM leverages a novel self-supervised pre-training framework consisting of _masked ETS modeling_ and _dual-view contrastive learning_. Our experiments involve two real-world scenario datasets, comprising private and public data. Through pre-training on massive ETS data, PowerPM achieves SOTA performance on diverse downstream tasks within the private dataset. Moreover, when transferred to the public dataset, PowerPM maintains its superiority, showcasing its remarkable generalization ability across various tasks and domains. Further analysis shows the effectiveness of a foundation model in the field of power system. Also, PowerPM is an off-the-shelf model with its code and weights. This feature greatly mitigates the challenges associated with sample and label efficiency, allowing it to be directly integrated into various power system applications.

## Acknowledgments

This work was partially supported by National Natural Science Foundation of China (No. 62322606, No. 62441605).

## References

* [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [2] Vadim Arzamasov, Klemens Bohm, and Patrick Jochem. Towards concise models of grid stability. In _2018 IEEE international conference on communications, control, and computing technologies for smart grids (SmartGridComm)_, pages 1-6. IEEE, 2018.
* [3] Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: Prompt-based generative pre-trained transformer for time series forecasting. _arXiv preprint arXiv:2310.04948_, 2023.
* [4] Widyaning Chandramitasari, Bobby Kurniawan, and Shigeru Fujimura. Building deep neural network model for short term electricity consumption forecasting. In _2018 International Symposium on Advanced Intelligent Informatics (SAIN)_, pages 43-48. IEEE, 2018.
* [5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _ICML_, 2020.
* [6] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 15750-15758, 2021.
* [7] Yue Cui, Kai Zheng, Dingshan Cui, Jiandong Xie, Liwei Deng, Feiteng Huang, and Xiaofang Zhou. Metro: A generic graph neural network framework for multivariate time series forecasting. _Proc. VLDB Endow._, 2021.
* [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. _NAACL_, 2018.
* [9] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GIm: General language model pretraining with autoregressive blank infilling. _arXiv preprint arXiv:2103.10360_, 2021.
* [10] Xi Fang, Satyajayant Misra, Guoliang Xue, and Dejun Yang. Smart grid--the new and improved power grid: A survey. _IEEE communications surveys & tutorials_, 14(4):944-980, 2011.
* [11] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. _IJCNLP_, 2020.
* [12] Benjamin Goehry, Yannig Goude, Pascal Massart, and Jean-Michel Poggi. Aggregation of multi-scale experts for bottom-up load forecasting. _IEEE Transactions on Smart Grid_, 2020.
* [13] Mohammad Hashemi, Shengbo Gong, Juntong Ni, Wenqi Fan, B. Aditya Prakash, and Wei Jin. A comprehensive survey on graph reduction: Sparsification, coarsening, and condensation, 2024.
* [14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. _CVPR_, 2022.
* [15] Wenjie Hu, Yang Yang, Jianbo Wang, Xuanwen Huang, and Ziqiang Cheng. Understanding electricity-theft behavior via multi-source data. In _Proceedings of The Web Conference 2020_, pages 2264-2274, 2020.
* [16] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia Makedon. A survey on contrastive self-supervised learning. _Technologies_, 9(1):2, 2020.

* [17] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. _arXiv preprint arXiv:2310.01728_, 2023.
* [18] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR (Poster)_, 2015.
* [19] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. _Advances in neural information processing systems_, 32, 2019.
* [20] Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, and Roger Zimmermann. Unitime: A language-empowered unified model for cross-domain time series forecasting. In _Proceedings of the ACM Web Conference 2024_, 2024.
* [21] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. _ICLR_, 2023.
* [22] Peter Palensky and Dietmar Dietrich. Demand side management: Demand response, intelligent energy systems, and smart loads. _IEEE transactions on industrial informatics_, 7(3):381-388, 2011.
* [23] Yue Pang, Bo Yao, Xiangdong Zhou, Yong Zhang, Yiming Xu, and Zijing Tan. Hierarchical electricity time series forecasting for integrating consumption patterns analysis and aggregation consistency. In _IJCAI_, pages 3506-3512, 2018.
* [24] Adam Paszke, S. Gross, Francisco Massa, A. Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Z. Lin, N. Gimelshein, L. Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In _NeurIPS_, 2019.
* [25] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In _The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3-7, 2018, Proceedings 15_, pages 593-607. Springer, 2018.
* [26] Chao Shang, Jie Chen, and Jinbo Bi. Discrete graph structure learning for forecasting multiple time series. In _International Conference on Learning Representations_, 2021.
* [27] Arunesh Kumar Singh, S Khatoon, Md Muazzam, DK Chaturvedi, et al. Load forecasting techniques and methodologies: A review. In _2012 2nd International Conference on Power, Control and Embedded Systems_, pages 1-10. IEEE, 2012.
* [28] Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test: Text prototype aligned embedding to activate llm's ability for time series. _arXiv preprint arXiv:2308.08241_, 2023.
* [29] Xiaorong Sun, Peter B. Luh, Kwok W. Cheung, Wei Guan, Laurent D. Michel, S. S. Venkata, and Melanie T. Miller. An efficient approach to short-term load forecasting at the distribution level. _IEEE Transactions on Power Systems_, 2016.
* [30] Yuechuan Tao, Jing Qiu, Shuying Lai, Junhua Zhao, and Yusheng Xue. Carbon-oriented electricity network planning and transformation. _IEEE Transactions on Power Systems_, 36(2):1034-1048, 2020.
* [31] Sana Tonekaboni, Danny Eytan, and Anna Goldenberg. Unsupervised representation learning for time series with temporal neighborhood coding. In _International Conference on Learning Representations_, 2021.
* [32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.

* [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In _Advances in Neural Information Processing Systems_, 2017.
* [35] Hong Wang, Khalid A. Alattas, Ardashir Mohammadzadeh, Mohammad Hosein Sabzalian, Ayman A. Aly, and Amir Mosavi. Comprehensive review of load forecasting with emphasis on intelligent computing approaches. _Energy Reports_, 8, 2022.
* [36] Yi Wang, Qixin Chen, Mingyang Sun, Chongqing Kang, and Qing Xia. An ensemble forecasting method for the aggregated load with subprofiles. _IEEE Transactions on Smart Grid_, 2018.
* [37] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Cost: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. _ICLR_, 2022.
* [38] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In _The Eleventh International Conference on Learning Representations_, 2022.
* [39] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Auttoformer: Decomposition transformers with auto-correlation for long-term series forecasting. _NeurIPS_, 2021.
* [40] Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Connecting the dots: Multivariate time series forecasting with graph neural networks. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 753-763, 2020.
* [41] Dazhi Yang, Gary SW Goh, Siwei Jiang, Allan N Zhang, and Orkan Akcan. Forecast upc-level fmcg demand, part ii: Hierarchical reconciliation. In _2015 ieee international conference on big data (big data)_, pages 2113-2121. IEEE, 2015.
* [42] Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. TS2Vec: Towards Universal Representation of Time Series. _AAAI_, 2022.
* [43] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In _Proceedings of the AAAI conference on artificial intelligence_, volume 37, pages 11121-11128, 2023.
* [44] Chi Zhang and Ran Li. A novel closed-loop clustering algorithm for hierarchical load forecasting. _IEEE Transactions on Smart Grid_, 2021.
* [45] Hao Zhang, Fan Zhang, Yu Zhang, Hui Cheng, Ruotian Gao, Zongpeng Li, Jiakui Zhao, and Mingzhu Zhang. An elderly living-alone guardianship model based on wavelet transform. In _2022 4th International Conference on Power and Energy Technology (ICPET)_, pages 1249-1253. IEEE, 2022.
* [46] Huaying Zhang, Qing Wang, Yan Li, Jingwen Ai, Xunyong Hu, Wenhai Zhang, and Dehai Zhang. Clock anomaly detection method of power quality monitoring device based on voltage sag. In _2021 IEEE 2nd China International Youth Conference on Electrical Engineering (CIVCEE)_, pages 1-6. IEEE, 2021.
* [47] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. _NeurIPS_, 2022.
* [48] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 11106-11115, 2021.
* [49] Kaile Zhou and Shanlin Yang. Understanding household energy consumption behavior: The contribution of energy big data analytics. _Renewable and Sustainable Energy Reviews_, 56:810-819, 2016.

* [50] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _ICML_, 2022.
* [51] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series analysis by pretrained lm. _Advances in neural information processing systems_, 36, 2024.

## Appendix A Dataset Description

We conduct experiments on \(5\) real-world hierarchical electricity time series datasets, one of which was collected from the real scenario. The other four are collected from CSISO 6, ISONE7, NYISO 8, and PJM 9. Our experiments include four typical time series analysis tasks on these datasets to evaluate the effect of our approach in both in-domain and cross-domain settings: prediction, missing value imputation, anomaly detection, and classification, which include different sampling frequencies (\(5\) minutes, \(15\) minutes, \(1\) hour, \(1\) day). Moreover, it covers a variety of application scenarios in power systems (load forecasting, solar generation forecasting, electricity theft detection and consumer analysis, etc.). Tab. 3 and Tab. 4 summarize the detailed descriptions of these datasets.

Footnote 6: http://www.energyonline.com/Data/

Footnote 7: https://www.iso-ne.com/isoexpress/web/reports/load-and-demand/

Footnote 8: https://www.nyiso.com/load-data

Footnote 9: https://dataminer2.pjm.com/list

### Private Dataset

Private dataset is collected from the load data in the real scenario, covering the period about \(6\) years. Following data preprocessing, we extract a subset of the data. In order to effectively support our research objectives, we divide the dataset into \(9\) distinct sub-datasets. One biggest of these sub-datasets is served as the pre-training dataset, while the remaining \(7\) sub-datasets are utilized as downstream datasets for downstream tasks. These downstream datasets are partitioned into train, validation, and test sets according to a \(6:2:2\) ratio, ensuring that the training set contain data from the earlier time period. Further details are provided below:

\begin{table}
\begin{tabular}{c|l|c|c c c c} \hline \hline Dataset & \multicolumn{2}{c|}{Instance} & \multicolumn{2}{c}{Samples} & Output Length & Frequency & Classes \\ \hline \multirow{2}{*}{Pre-training} & \#city & 11 & \multirow{2}{*}{268373267040} & \multirow{2}{*}{-} & \multirow{2}{*}{15 minutes} & \multirow{2}{*}{-} \\  & \#district & 90 & & & & \\  & \#user & 1530826 & & & & \\ \hline \multirow{2}{*}{\begin{tabular}{c} Load \\ forecasting \\ \end{tabular} } & \#city & 11 & \multirow{2}{*}{109596429408} & \multirow{2}{*}{15 minutes} & \multirow{2}{*}{-} \\  & \#district & 90 & & & & \\  & \#user & 1563730 & & & & \\ \hline \multirow{2}{*}{\begin{tabular}{c} Load \\ imputation \\ \end{tabular} } & \#city & 11 & \multirow{2}{*}{109596429408} & \multirow{2}{*}{672} & \multirow{2}{*}{15 minutes} & \multirow{2}{*}{-} \\  & \#district & 90 & & & & \\  & \#user & 1563730 & & & & \\ \hline \multirow{2}{*}{\begin{tabular}{c} Solar generation \\ forecasting \\ \end{tabular} } & \#city & - & \multirow{2}{*}{3458400} & \multirow{2}{*}{\{4, 96, 288, 672}} & \multirow{2}{*}{15 minutes} & \multirow{2}{*}{-} \\  & \#district & 90 & & & & \\  & \#user & 44077 & & & \\ \hline \multirow{2}{*}{\begin{tabular}{c} Electricity theft detection \\ \end{tabular} } & \#city & 11 & \multirow{2}{*}{279478936} & \multirow{2}{*}{1} & \multirow{2}{*}{1day} & \multirow{2}{*}{2} \\  & \#district & 90 & & & & \\  & \#user & 44077 & & & & \\ \hline \multirow{2}{*}{\begin{tabular}{c} Clock error \\ detection \\ \end{tabular} } & \#city & 11 & \multirow{2}{*}{1070142528} & \multirow{2}{*}{1} & \multirow{2}{*}{15 minutes} & \multirow{2}{*}{2} \\  & \#district & 90 & & & & \\  & \#user & 35145 & & & & \\ \hline \multirow{2}{*}{\begin{tabular}{c} Elderly alone \\ detection \\ \end{tabular} } & \#city & 11 & \multirow{2}{*}{25762488} & \multirow{2}{*}{1} & \multirow{2}{*}{1day} & \multirow{2}{*}{2} \\  & \#district & 90 & & & & \\  & \#user & 35145 & & & & \\ \hline \multirow{2}{*}{\begin{tabular}{c} High-power \\ appliance detection \\ \end{tabular} } & \#city & 11 & \multirow{2}{*}{33402144} & \multirow{2}{*}{1} & \multirow{2}{*}{1day} & \multirow{2}{*}{2} \\  & \#district & 90 & & & & \\ \hline \multirow{2}{*}{
\begin{tabular}{c} Consumer analysis \\ \end{tabular} } & \#city & 11 & \multirow{2}{*}{18661860} & \multirow{2}{*}{1} & \multirow{2}{*}{1day} & \multirow{2}{*}{\{2, 4, 4} \\  & \#district & 90 & & & & \\ \#user & 29476 & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 3: Private dataset descriptionPre-training Dataset.The pre-training dataset is derived from a subset of the private dataset, encompassing the period about \(4\) years.. It consists of unlabeled data recorded at a frequency of one data point every \(15\) minutes. The dataset is structured hierarchically, including information at the user, district, and city levels.

Load Forecasting and Missing Value Imputation Dataset.This dataset is extracted from a portion of the private dataset about \(1\) years. The dataset includes hierarchical information at the user, district, and city levels, with data points recorded every \(15\) minutes. For the missing value imputation task, the dataset is structured to output \(672\) data points. As for the forecasting task, there are four different prediction horizons: one hour (\(4\) data points), one day (\(96\) data points), three days (\(288\) data points), and seven days (\(672\) data points).

Solar Generation Forecasting Dataset.The dataset is collected from many distributed photovoltaic power stations. The dataset has not a hierarchical structure, and data points are recorded at a frequency of one point every \(15\) minutes. It includes four different prediction horizons: one hour, one day, three days, and seven days.

Electricity Theft Detection Dataset.This dataset comprises the daily electricity consumption records (in K-Wh) of users in \(1\) year. For each user, the dataset includes the daily aggregate electricity usage. Within the dataset, certain users (referred to as electricity thieves) engage in unauthorized activities involving the electricity meter in order to reduce costs.

Clock Anomaly Dataset.This dataset comprises millions of clock error series, each representing the time deviation, compared to the standard time, and communication delay of various watt-hour meters on a weekly basis. The dataset covers the period about \(8\) months.

Elderly Living Alone Dataset.This dataset includes the daily electricity consumption records (in K-Wh) of village users. Additionally, employees conduct extensive on-site investigations specifically targeting these users, from which we obtain labels indicating whether each user is an elderly individual living alone or not.

High-power Appliance Detection Dataset.This dataset consists of the daily electricity consumption records (in K-Wh) of village users. Similar to the previous dataset, on-site investigations are conducted by same method, enabling us to collect labels indicating whether each user possesses high-power appliances.

Consumer Analysis Dataset.This dataset contains the daily electricity consumption records (in K-Wh) of village users. Additionally, employees conducted extensive on-site investigations targeting these users, collecting statistics related to the gender of the gender of user who lives alone, the age of the resident elderly, and family structure. The gender labels of user who lives alone are: male and female, totaling two classes; the age labels for residents are: \(60\sim 70\) years old, \(70\sim 80\) years old, \(80\sim 90\) years old, and over \(90\) years old, totaling four classes; the family structure labels are: \(1\) people, \(2\sim 3\) people, \(4\sim 5\) people, and more than \(6\) people, totaling four classes.

\begin{table}
\begin{tabular}{l|c|c|c c c c} \hline \hline Dataset & Instance & Samples & Output Length & Frequency & Time Span \\ \hline \multirow{2}{*}{CAISO} & \#state & 1 & \multirow{2}{*}{305018} & \multirow{2}{*}{\{12, 24, 168}} & \multirow{2}{*}{1 hour} & \multirow{2}{*}{2023-04-25\(\sim\)2024-04-23} \\  & \#area & 34 & & & & \\ \hline \multirow{2}{*}{ISONE} & \#region & 1 & \multirow{2}{*}{25904} & \multirow{2}{*}{\{12, 24, 168}} & \multirow{2}{*}{1 hour} & \multirow{2}{*}{2023-10-01\(\sim\)2024-04-01} \\  & \#state & 6 & & & & \\ \hline \multirow{2}{*}{NYISO} & \#state & 1 & \multirow{2}{*}{1396992} & \multirow{2}{*}{\{12, 24, 168}} & \multirow{2}{*}{5 minutes} & \multirow{2}{*}{2023-03-01\(\sim\)2024-03-31} \\  & \#area & 11 & & & & \\ \hline \multirow{2}{*}{PJM} & \#state & 3 & \multirow{2}{*}{212369} & \multirow{2}{*}{\{12, 144, 288}} & \multirow{2}{*}{5 minutes} & \multirow{2}{*}{2024-03-28\(\sim\)2024-04-26} \\  & \#city & 22 & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 4: Public dataset description 

### Public Datasets

Four public datasets as cross-domain datasets are selected to validate the generalization ability of our model. These four datasets are named CSISO, ISONE, NYISO, and PJM, which cover 3 types different hierarchical relationships: state-area, region-state, state-city.

**CAISO.** It is sampled from California, including 34 areas loads and an aggregated load for the state, recorded every hour from April \(25\), \(2023\), to April \(23\), \(2024\). The prediction horizons include half a day (\(12\) points), one day (\(24\) points), and seven days (\(168\) points).

**ISONE.** It is sampled from New England, consisting of \(6\) states loads and an aggregated load for the region, recorded every hour from October \(1\), \(2023\), to April \(1\), \(2024\). The prediction horizons include half a day (\(12\) points), one day (\(24\) points), and seven days (\(168\) points).

**NYISO.** It is sampled from California, containing \(11\) areas loads and an aggregated load for the state, recorded every \(5\) minutes from March \(1\), \(2023\), to March \(31\), \(2024\). The prediction horizons include one hour (\(12\) points), half a day (\(144\) points), and one day (\(288\) points).

**PJM.** It is sampled from \(3\) states: Florida, Ohio, Washington, which includes \(22\) cities loads and there \(3\) state loads, recorded every hour from March \(28\), \(2023\), to April \(26\), \(2024\). The prediction horizons include one hour (\(12\) points), half a day (\(144\) points), and one day (\(288\) points).

### Exogenous Variables

We obtained weather and temperature records for all area levels in both the private and public datasets. The weather information from the private dataset is obtained from the Weather Radar10. Additionally, the weather information from the public datasets is obtained from the NSF NCAR Research Data Archive11. Both sources cover the same timespan as mentioned above, respectively. These records include the maximum and minimum temperatures (in "C for private dataset and "F for public datasets) for each hour in each city.

Footnote 10: http://en.weather.com.cn/

Footnote 11: https://rda.ucar.edu/

## Appendix B PowerPM and Baseline Implementation Details

### PowerPM Implementation

The pre-training stage of the experiment is implemented in PyTorch [24] and conducted on a Linux system with \(2\) CPUs (AMD EPYC \(9654\)\(96\)-Core Processor) and \(8\) GPUs (NVIDIA Tesla \(A800\)\(80G\)) for about \(8\) days. And the downstream task experiment is repeated five times. We select \(512\) samples as a batch, and every batch contains about \(174k\) patches, which we set patch len to \(48\), stride to \(24\). To speed up the model training, we stop the gradient update of the background nodes in the hierarchical graph. We optimize with Adam [18], updating the model parameters every \(4\) steps, and the model trains for \(1310k\) updates in total. A reduce learning rate on plateau scheduler is utilized to adjust learning rate during pre-training. Specifically, we set the basic learning rate as \(1e-6\) and the maximum learning rate as \(2e-5\), and the learning rate updates for every \(10k\) updates. In addition, we trained three additional variants of PowerPM with different parameter counts to meet the needs of different users or situations. Detailed model hyperparameters can be found in Tab. 5.

**Full Fine-tuning.** In the F-FT (Full Fine-tuning) setup, for different tasks, we introduce different head \(H\) on the top of pre-trained encoder \(f(.)\), where both the parameters of the encoder \(f(.)\) and the head \(H\) are trainable. For forecasting and imputation tasks, we use a prediction \(H_{l}\) head to map prediction points or reconstruction points from \(\mathbf{z}_{i}\). In this setup, we fine-tune both the head \(H\) and the encoder \(f(.)\). We utilize \(100\)%, \(60\)%, \(30\)% and \(10\)% training data for fine-tuning. we utilize a one-layer fully connected network to implement prediction \(H_{l}\) and logistic regression from the Sklearn library to implement the classifier \(H_{c}\). The learning rates are specifically set to \(4e-4\) and \(3e-5\) for public and private datasets.

**Partial Fine-tuning.** In the P-FT (Partial Fine-tuning) setup, for different tasks, we also introduce different head \(H\) on the top of pre-trained encoder \(f(.)\). For forecasting and imputation tasks, we use a prediction \(H_{l}\) head to map prediction points or reconstruction points from \(\mathbf{z}_{i}\). And for anomalydetection and classfication tasks, a classifier \(H_{c}\) on top of the pre-trained encoder \(f(.)\). During the whole finetune process, we keep the parameters of \(f(.)\) fixed. Only the head is fine-tuned in this setup. we utilize a one-layer fully connected network to implement prediction \(H_{l}\) and logistic regression from the Sklearn library to implement the classifier \(H_{c}\). The learning rates are specifically set to \(4e-4\) and \(3e-5\) for public and private datasets.

### Baselines Implementation

We compare with \(8\) state-of-the-art methods: including Large Language Model (LLM) enhanced models: GPT4TS [51], Time-LLM [17], UniTime [20]; pre-train models: PatchTST [21], CoST [37], TS2Vec [42]; supervised models: DLinear [43], TimesNet [38]. To make a fair and comprehensive comparison, we reproduce all models with official implementation, and use different output head for different downstream tasks. Due to the large scale of the ETS dataset, we increase the number of training epoch and reduce the learning rate in order to make the parameters of the model fully learned.

**GPT4TS**[51] combines the LLM with Transformer, which use frozen pre-trained GPT-2 for general time series analysis. To implement GPT4TS, we utilized their open-source code, available at https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All. We use the 6 layers of GPT-2, which is proved to have the optimal performance in original paper and the total size of GPT4TS is about \(105.15\)M, and the trainable parameters are \(24.04M\) (GPT-2 is frozen). We set the number of train epochs to \(50\), the learning rate to \(0.0005\), and the batch size to \(256\).

**Time-LLM**[17] ferezes the LLM as the backbone, and align time series to text with patch reprogramming. It also designs Prompt-as-Prefix including dataset context, task instruction and input statistics to enrich the input context to direct the transformation of reprogrammed input. We utilized their open-source code, available at https://github.com/KimMeen/Time-LLM to implement Time-LLM. We set the llama-7b with \(32\) layers as the backbone, which is the most effective recorded in [17] and the total size of Time-LLM is about\(7.28B\), and the trainable parameters are \(58.55M\) (llama-7b is frozen). To align the dataset context input to our datasets, we constuct different natural language prompt summarized in App. A for private and public datasets, and we set the number of train epochs to \(50\), the learning rate to \(0.005\), and the batch size to \(256\).

**UniTime**[20] leverages LLM to handle time series forecasting across time series domains, which exhibit significant differences in temporal patterns and distribution. The same as dataset context in Time-LLM, UniTime also designs human-crafted instructions to furnish the model with explicit domain identification information. To implement UniTime, we utilized their open-source code, available at https://github.com/liuxu77/UniTime. We implement the backbone LLM with GPT2-small like original paper, and the total size of UniTime is about \(108.54M\) without freeze any parameters. We use the same natural language prompt in Time-LLM as the human-crafted instructions for different datasets, and we set the number of train epochs to \(50\), the learning rate to \(0.0005\), the weight decay to \(0.0001\), and the batch size to \(256\).

**TS2Vec**[42] performs contextual consistency using overlapping subseries and a hierarchical loss function to capture data consistency at the observation and sample levels. We utilize the open-source code available at https://github.com/zhihanyue/ts2vec. Specifically, we set the number of epochs for pre-training to \(100\), the learning rate to \(0.0005\), and the batch size to \(512\). Due to the large scale and complex semantics of the pre-trained ETS data, we adjust the representation dimension to \(640\), matching the ETS data characteristics. We adopt the default settings provided by the TS2Vec implementation for other settings during pre-training.

**CoST**[37] comprises both time domain and frequency domain contrastive losses to learn discriminative trend and seasonal representations. We utilize the open-source code available at https://github.com/salesforce/CoST to implement CoST. Specifically, we set the number of epochs for pre-training to 100, the learning rate to \(0.0005\), representation dimension to \(640\), and the batch size to \(256\). We adopt the default settings provided by the CoST implementation for other settings during pre-training.

**PatchTST**[21] changes the input sequence as a series of patch windows, focus the subseries-level attention to capture local semantic information while minimizing memory consumption. We utilize the open-source code available at https://github.com/yuqinie98/PatchTST. For hyperparameters of PatchTST, We set the patch len to \(32\) and stride to \(16\), the number of epochs for pre-training to \(100\)

[MISSING_PAGE_FAIL:19]

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline \multirow{2}{*}{Tanks} & \multicolumn{2}{c}{PowerPM} & \multicolumn{2}{c}{PowerPM-Horstens} & \multicolumn{2}{c}{PowerPM-Medium} & \multicolumn{2}{c}{PowerPM-Small} & \multicolumn{2}{c}{PowerPM-Tiny} \\ \hline Model Scale & 256.0\(M\) & 120.1\(M\) & 68.6\(M\) & 35.5\(M\) \\ Temporal Encoder & 26 & 18 & 12 & 4 \\ Model Dimention & 1024 & 768 & 768 & 768 & 768 \\ Inner Dimension & 2048 & 2048 & 1024 & 768 \\ Hierarchical Encoder Layer & 2 & 2 & 2 & 2 \\ Heads & 16 & 16 & 16 & 16 \\ Mask Ratio & 0.4 & 0.4 & 0.4 & 0.4 \\ Time Shift \(\delta\) & 96 & 96 & 96 & 96 \\ Number of Clusters \(K\) & 12 & 12 & 12 & 12 \\ Batch Size & 512 & 256 & 256 & 128 \\ Learning Rate & 1e-6 & 1e-6 & 2e-6 & 2e-6 \\ Optimizer & Adam & Adam & Adam & Adam \\ Scheduler & Plateau & Plateau & Plateau & Plateau & Plateau \\ \hline \hline \end{tabular}
\end{table}
Table 5: The model hyperparameters of PowerPM with different model size.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multirow{2}{*}{Tanks} & \multicolumn{2}{c}{PowerPM} & \multicolumn{2}{c}{PowerPM-Horstens} & \multicolumn{2}{c}{PowerPM-Medium} & \multicolumn{2}{c}{PowerPM-Small} & \multicolumn{2}{c}{PowerPM-Tiny} \\ \hline Model Scale & 256.0\(M\) & 120.1\(M\) & 68.6\(M\) & 35.5\(M\) \\ Temporal Encoder & 26 & 18 & 12 & 4 \\ Model Dimention & 1024 & 768 & 768 & 768 & 768 \\ Inner Dimension & 2048 & 2048 & 1024 & 768 \\ Hierarchical Encoder Layer & 2 & 2 & 2 & 2 \\ Heads & 16 & 16 & 16 & 16 \\ Mask Ratio & 0.4 & 0.4 & 0.4 & 0.4 \\ Time Shift \(\delta\) & 96 & 96 & 96 & 96 \\ Number of Clusters \(K\) & 12 & 12 & 12 & 12 \\ Batch Size & 512 & 256 & 256 & 128 \\ Learning Rate & 1e-6 & 1e-6 & 2e-6 & 2e-6 \\ Optimizer & Adam & Adam & Adam & Adam \\ Scheduler & Plateau & Plateau & Plateau & Plateau \\ \hline \hline \end{tabular}
\end{table}
Table 5: The model hyperparameters of PowerPM with different model size.

\begin{table}
\begin{tabular}{c|c|c|c c c c c c c c c c} \hline \hline \multicolumn{2}{c}{\multirow{2}{*}{Tasks}} & \multicolumn{2}{c}{PowerPM} & \multicolumn{2}{c}{PowerPM-H} & \multicolumn{2}{c}{PowerPM-M} & \multicolumn{2}{c}{PowerPM-C} & \multicolumn{2}{c}{PowerPM-C} & \multicolumn{2}{c}{PowerPM-E} \\ \cline{3-14}  & & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\ \hline \multirow{11}{*}{**Forecasting**} & \multirow{4}{*}{Exclusive User} & 4 & **0.3378** & **0.3638** & 0.3505 & 0.3808 & 0.3777 & 0.3859 & 0.3672 & 0.3776 & *0.3531 & *0.3788 \\  & & 96 & **0.4183** & **0.4496** & 0.4389 & *0.4642 & *0.4343 & 0.4770 & 0.4253 & 0.4546 & 0.4496 & 0.4650 \\  & & 288 & **0.4770** & **0.4653** & 0.5061 & *0.4879 & 0.4957 & 0.4906 & *0.4894 & 0.4885 & 0.4853 & 0.4718 \\  & & 672 & **0.5476** & **0.5222** & **0.5765** & 0.5494 & 0.5772 & 0.5502 & 0.5957 & 0.5562 & 0.5668 & 0.5731 \\  & & Avg. & **0.4452** & **0.4502** & **0.4680** & 0.4706 & 0.4712 & 0.4759 & 0.4694 & *0.4642 & 0.4637 & 0.4632 \\ \hline \multirow{11}{*}{**Forecasting**} & \multirow{4}{*}{Public User} & 4 & **0.2353** & **0.2951** & 0.2428 & 0.3041 & 0.2793 & *0.3024 & 0.2519 & 0.3329 & 0.2448 & 0.2977 \\  & & 96 & **0.2604** & **0.3190** & 0.3102 & 0.3293 & 0.3029 & 0.3473 & *0.2973 & 0.3339 & 0.2966 & 0.3325 \\  & & 288 & **0.3216** & **0.3875** & *0.3455 & 0.4103 & 0.3480 & *0.4047 & 0.3460 & 0.3938 & 0.3334 & 0.4096 \\  & & 672 & **0.3818** & **0.4241** & 0.4330 & 0.4683 & *0.4003 & 0.4595 & 0.3946 & *0.4431 & 0.4031 & 0.4349 \\  & & & **0.3000** & **0.3564** & 0.3335 & 0.3780 & 0.3326 & 0.3785 & *0.3225 & *0.3737 & 0.3195 & 0.3687 \\ \cline{1-1} \cline{2-14}  & \multirow{4}{*}{Discrict} & 4 & **0.2382** & **0.3090** & *0.2643 & 0.3394 & 0.2739 & *0.3222 & 0.2418 & 0.3165 & 0.2714 & 0.3322 \\  & & 96 & **0.2296** & **0.3419** & 0.3454 & 0.3913 & *0.3371 & 0.3654 & 0.3278 & *0.3699 & 0.3388 & 0.3796 \\  & & 288 & **0.3300** & **0.3874** & 0.3767 & 0.4338 & 0.3986 & *0.4015 & 0.3417 & *0.4188 & *0.3659 & 0.4190 \\  & & 672 & **0.3710** & **0.2441** & 0.4105 & 0.4757 & *0.3924 & 0.4682 & 0.3809 & 0.4485 & 0.4038 & *0.4583 \\  & & Avg. & **0.3080** & **0.3656** & 0.3492 & 0.4100 & 0.3483 & *0.3893 & 0.3231 & 0.3884 & *0.3449 & 0.3950 \\ \cline{1-1} \cline{2-14}  & \multirow{4}{*}{City} & 4 & **0.7725** & **0.1639** & *0.2054 & 0.1710 & 0.2340 & 0.1934 & *0.2123 & *0.1770 & 0.1941 & 0.1812 \\  & & 96 & **0.2272** & **0.1311** & 0.2669 & 0.2570 & 0.2462 & 0.2313 & 0.2336 & *0.2403 & 0.2478 & 0.2415 \\  & & 96 & **0.2284** & **0.2471** & 0.3187 & 0.3114 & 0.3119 & *0.2950 & 0.2670 & 0.2929 & *0.2713 & 0.3054 \\  & & 672 & **0.2311** & **0.3191** & 0.3646 & 0.3820 & 0.3415 & *0.3498 & *0.3486 & *0.3426 & *0.3563 & 0.3622 \\  & & Avg. & **0.42423** & **0.3258** & 0.2889 & 0.2804 & 0.2834 & *0.2674 & 0.2654 & *0.2632 & *0.2674 & 0.2726 \\ \hline \multirow{11}{*}{**Forecasting**} & \multirow{4}{*}{Solar Generation**} & 4 & **0.0993** & **0.1541** & - & - & *0.1115 & 0.1827 & 0.1117 & 0.1691 & 0.1109 & *0.1732 \\  & & 96 & **0.1223** & **0.2002** & - & - & *0.1603 & 0.22770 & 0.1412 & 0.2097 & 0.1694 & 0.2310 \\  & & 288 & **0.3317** & **0.2526** & - & - & *0.2637 & 0.2859 & 0.2548 & *0.3113 & *0.2713 & 0.3138 \\  & & 672 & **0.3076** & **0.3165** & - & - & *0.3616 & 0.3332 & 0.3213 & *0.3373 & *0.3562 & 0.3686 \\  & & Avg. & **0.1907** & **0.2309** & - & - & *0.2243 & 0.2572 & 0.2073 & 0.2569 & 0.2270 & 0.2717 \\ \hline \multirow{11}{*}{**Forecasting**} & \multirow{4}{*}{Exclusive User**} & \multirow{4}{*}{Exclusive User} & 0.125 & **0.2459** & **0.2654** & 0.2665 & 0.2999 & 0.2738 & *0.28454 & *0.2633 & 0.2717 & 0.2508 & 0.2865 \\  & & 0.25 & **0.2621** & **0.2849** & 0.3160 & 0.3165 & 0.3055 & 0.3210 & *0.3025 & 0.3117 & 0.2957 & *0.3146 \\  & & 0.375 & **0.3288** & **0.3017** & 0.3586 & 0.3555 & 0.3729 & 0.3892 & *0.3594 & 0.3359 & 0.3783 & *0.3434 \\  & & **0.3616** & **0.3528** & 0.4426 & 0.4095 & 0.4141 & 0.4185 & 0.4421 & 0.3840 & *0.4209 & 0.3723 \\  & & Av

\begin{table}
\begin{tabular}{l|l|c|c c|c c} \hline \hline Model & Tasks & 60\% & 30\% & Decrease & 10\% & Decrease \\ \hline \multirow{4}{*}{TS2vec} & Forecasting(MSE) & 0.4723 & 0.5553 & 17.58\% & 0.6275 & 32.87\% \\  & Imputation(MSE) & 0.4021 & 0.4884 & 21.46\% & 0.5739 & 42.72\% \\  & Anomaly Detection(F0.5) & 0.4027 & 0.3454 & 14.24\% & 0.3173 & 21.20\% \\  & Classification(Acc.) & 0.5234 & 0.4197 & 19.82\% & 0.4335 & 17.17\% \\ \hline \multirow{4}{*}{CoST} & Forecasting(MSE) & 0.4711 & 0.5589 & 18.64\% & 0.6349 & 34.78\% \\  & Imputation(MSE) & 0.3825 & 0.4704 & 22.97\% & 0.5059 & 32.26\% \\  & Anomaly Detection(F0.5) & 0.4221 & 0.3785 & *10.34\% & 0.3156 & 25.23\% \\  & Classification(Acc.) & 0.5534 & 0.4806 & 13.15\% & 0.4363 & 21.15\% \\ \hline \multirow{4}{*}{PatchTST} & Forecasting(MSE) & 0.4456 & 0.5105 & 14.56\% & 0.5716 & 28.29\% \\  & Imputation(MSE) & 0.3623 & 0.4346 & 19.95\% & 0.4592 & 26.76\% \\  & Anomaly Detection(F0.5) & 0.3452 & 0.2657 & 23.03\% & 0.2283 & 33.87\% \\  & Classification(Acc.) & 0.4526 & 0.3341 & 26.18\% & 0.2808 & 37.95\% \\ \hline \multirow{4}{*}{UniTime} & Forecasting(MSE) & 0.3904 & *0.4220 & 8.10\% & 0.4528 & 15.98\% \\  & Imputation(MSE) & 0.3375 & 0.3722 & 10.29\% & 0.3895 & 15.41\% \\  & Anomaly Detection(F0.5) & 0.4102 & 0.3640 & 11.26\% & 0.3391 & 17.34\% \\  & Classification(Acc.) & 0.5439 & 0.4740 & 12.85\% & 0.4551 & 16.33\% \\ \hline \multirow{4}{*}{TimeLLM} & Forecasting(MSE) & 0.3713 & 0.4034 & *8.64\% & 0.4180 & 12.58\% \\  & Imputation(MSE) & 0.2815 & 0.3072 & **9.13\%** & **0.3104** & **10.27\%** \\  & Anomaly Detection(F0.5) & 0.4024 & 0.3655 & 9.16\% & *0.3534 & **12.17\%** \\  & Classification(Acc.) & 0.5417 & 0.4958 & **8.48\%** & *0.4637 & *14.39\% \\ \hline \multirow{4}{*}{GPT4TS} & Forecasting(MSE) & *0.3838 & 0.4343 & 13.15\% & *0.4447 & *15.86\% \\  & Imputation(MSE) & *0.3212 & *0.3614 & 12.53\% & *0.3846 & 19.75\% \\  & Anomaly Detection(F0.5) & *0.4196 & *0.3718 & 11.39\% & 0.3587 & *14.52\% \\  & Classification(Acc.) & *0.5483 & *0.4902 & *10.60\% & 0.4737 & 13.61\% \\ \hline \multirow{4}{*}{PowerPM} & Forecasting(MSE) & **0.3343** & **0.3551** & **6.22\%** & **0.3652** & **9.25\%** \\  & Imputation(MSE) & **0.2717** & **0.2998** & *10.34\% & 0.3167 & *16.57\% \\ \cline{1-1}  & Anomaly Detection(F0.5) & **0.4822** & **0.4459** & **7.53\%** & **0.4166** & 13.60\% \\ \cline{1-1}  & Classification(Acc.) & **0.6594** & **0.5943** & 9.88\% & **0.5735** & **13.02\%** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Complete results of few-shot learning performance comparison. Models are fine-tuned on \(\{10\%\), \(30\%\) and \(60\%\}\) of the downstream dataset. Forecasting tasks involve varying forecasting lengths of \(\{4,96,288,672\}\) time points and imputation tasks involve varying mask ratio \(\{0.125,0.25,0.375,0.5\}\). The length of the input window is \(672\). We average the result for each task.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In this paper, abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of the work are discussed in Appendix D Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The private dataset so one may cannot reproduce the experiments on the private dataset. However, we also use another two public datasets, and the code of our work is fully provided too, which could help you understand our work. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The private dataset so one may cannot reproduce the experiments on the private dataset. However, we also use another four public datasets, and the code of our work is fully provided too, which could help you understand our work. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We give a detailed description of our experiment setting in B.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The overall standard derivation is shown in the figure 4. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide information about Experiments Compute Resources such as CPU, GPU, etc. in the B. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: All authors reviewed and conducted the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our potential positive societal impacts is discussed in Appendix E. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All the existing assets are properly referenced. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Our code is provided as a supplement. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [No] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.