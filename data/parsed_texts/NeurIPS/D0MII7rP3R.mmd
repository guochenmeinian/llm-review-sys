# Selectivity Drives Productivity: Efficient Dataset

Pruning for Enhanced Transfer Learning

Yihua Zhang\({}^{1,*}\)   Yimeng Zhang\({}^{1,*}\)   Aochuan Chen\({}^{1,*}\)   Jinghan Jia\({}^{1}\)   Jiancheng Liu\({}^{1}\)

**Gaowen Liu\({}^{2}\)   Mingyi Hong\({}^{3}\)   Shiyu Chang\({}^{4}\)   Sijia Liu\({}^{1}\)**

\({}^{1}\) Michigan State University, \({}^{2}\) Cisco Research,

\({}^{3}\) University of Minnesota, Twin City, \({}^{4}\) UC Santa Barbara

\({}^{*}\) Equal Contribution

###### Abstract

Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, _i.e._, _how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks_. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings respectively, by revisiting the DP problem through the lens of source-target domain mapping. Furthermore, we demonstrate the effectiveness of our approach on numerous transfer learning tasks. We show that source data classes can be pruned by up to \(40\%\sim 80\%\) without sacrificing downstream performance, resulting in a significant \(2\sim 5\times\) speed-up during the pretraining stage. Besides, our proposal exhibits broad applicability and can improve other computationally intensive transfer learning techniques, such as adversarial pretraining. Codes are available at [https://github.com/OPTML-Group/DP4TL](https://github.com/OPTML-Group/DP4TL).

## 1 Introduction

The abundance of the training data has long been regarded as the key driver of the contemporary machine learning (ML) algorithms [1, 2, 3]. However, the untrimmed, ever-growing training dataset could not only introduce training biases that compromise the model performance [4, 5, 6], but also poses an almost insurmountable obstacle to high training efficiency [1, 2]. Therefore, understanding the impact of the training data and selecting the most critical samples has emerged as an important goal, collectively referred to as the problem of _dataset pruning_ (**DP**).

Although the feasibility and the promise of DP have been unveiled in numerous applications, such as noisy data cleansing [7, 8, 9], continue learning [10, 11, 12, 13] and active learning [14], greater emphasis is placed on the _in-domain_ training setting, _i.e._, the pruned training set share the similar distribution as the evaluation set. Examples of methods to condense the training dataset with lossless generalization (on the in-distribution testing dataset) include data influence functions [15, 16, 17, 18, 19, 20, 21, 22, 23], model training dynamics [11, 12, 13, 14, 24, 25, 26, 27, 28], and coreset selection [29, 30, 31, 32, 33, 34, 35, 36]. In contrast, our work investigates theproblem of DP in the **transfer learning** paradigm [37, 38, 39], which has emerged as a popular way to leverage the knowledge of a _foundation model_ learned on a _source_ dataset (referring to '_pretraining_' stage) to further enhance the performance on a cross-domain _target_ dataset (referring to '_finetuning_' stage). Recent evidence [40, 41, 42] has shown that some source data points could make a _harmful influence_ in the downstream performance. In particular, the previous study [41] showed that removing specific source data classes can improve the transfer learning accuracy of a pretrained model.

Yet, _efficiently_ identifying those _harmful_ source data classes for improved transfer learning is a highly non-trivial task. Firstly, the unique pretrain-finetune paradigm complicates the analysis of the influence of source data on downstream performance, making it an indirect and very challenging process. Consequently, existing gradient and data influence-based in-domain DP methods [10, 19, 20, 21, 22, 23] cannot be naively adapted to the transfer setting. Secondly, transfer learning encompasses a wide range of source training methods other than supervised learning, such as self-supervised learning (**SSL**) [43, 44, 45, 46, 47, 48, 49, 50, 51, 52]. Therefore, a generic DP framework is desired given various pretraining scenarios. Thirdly, from an efficiency standpoint, the design of efficient DP algorithms is non-trivial. Even in the non-transfer learning paradigm, a majority of current in-domain DP methods [11, 25, 26] introduce substantial computational overheads. For instance, influence function-based DP methods [10, 15, 16, 17, 18, 19, 20, 21, 22] necessitate the calculation of the inverse Hessian matrix of model parameters, which is a highly demanding and computationally intensive process. Moreover, training dynamics-based methods [11, 12, 13, 14, 24, 25, 26, 27, 53], such as GraNd-Score [26] and Forgetting-Score [11], necessitate model training on the entire dataset multiple times. As a result, the development of an efficient and effective DP framework tailored for transfer learning remains a significant challenge in the field.

The most relevant work to ours is [41], which proposed a brute-force method to evaluate the influence of every source class in the downstream task following a leave-one-out principle. While this method effectively selects the most influential source classes, it is prohibitively slow, as the data selection process demands significantly higher computational resources than performing transfer learning itself. This brings us to the central question addressed in this paper:

_(Q) How can we extend DP to transfer learning with minimal computation overhead, broad applicability, and improved target performance?_

To address **(Q)**, we formally define the task of DP (dataset pruning) for transfer learning. We start by uncovering the limitations of conventional in-domain DP methods when applied to transfer learning tasks, and establish an intrinsic connection between source dataset pruning for transfer learning and source-target domain mapping. In the supervised pretraining paradigm, we propose an effective and scalable label mapping (**LM**)-based framework capable of pinpointing the most beneficial source

Figure 1: **Left: An illustration of the proposed dataset pruning methods (LM and FM) and their performance overview. Large scale source dataset is pruned by LM and FM through a small surrogate model (ResNet-18). Large foundation models can achieve up to \(5\times\) speed-up on pretraining without no downstream performance drop. Right: Downstream performance overview at different pruning ratios when ResNet-101 [54] is used as the source model (trained on the pruned source dataset) with ImageNet [55] as the source dataset transferring to OxfordPets [56] and StanfordCars [57]. The feature extractor is fixed during finetuning.**

data labels for a cross-domain target dataset. Furthermore, we extend the idea of LM to feature mapping (**FM**) for the SSL pretraining protocol, where data labels are unavailable. To achieve a greater practical impact, we demonstrate that our proposed methods (LM and FM) can facilitate effective DP over a small, simple surrogate source model, and further translate the positive impact of the pruned source dataset to other larger, more complex source models. We provide a schematic overview and result highlights in **Fig. 1**. Our contributions can be summarized as follows:

We connect the concept of DP (dataset pruning) to transfer learning for the first time and formalize the problem of DP for transfer learning.

We develop a highly efficient and effective framework for DP in transfer learning, leveraging the source-target domain mapping with customized LM (label mapping) and FM (feature mapping) for both supervised and SSL (self-supervised learning) settings. Our approach is principled and achieves significant speedups compared to the state-of-the-art methods.

We empirically show the effectiveness of our proposals (LP and FP) on 8 downstream tasks. We find that the source dataset (ImageNet) can be pruned up to \(40\%\sim 80\%\) without sacrificing the downstream performance, together with a \(2\times\sim 5\times\) speed-up in the pretraining stage. Our proposal also unlocks a door to prune a dataset using a simple surrogate source model (_e.g._, ResNet-18) and then reuse the pruned dataset to improve transfer learning on a larger source model (_e.g._, ResNet-101).

Lastly, we show that our proposed DP framework can benefit other computationally-intensive transfer learning techniques. For example, DP for adversarial pretraining [58] leads to a \(1\%\sim 2\%\) improvement in downstream performance with time consumption similar to standard pretraining.

## 2 Related Work

Dataset pruning.DP (dataset pruning) is an emerging technique to improve the data efficiency of model training by selecting the most representative training samples or removing the less influential ones [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]. Thus, the problem of coreset selection [29, 30, 31, 32, 33, 34, 35, 36] can also be considered a form of DP. Prior studies on DP range from clustering-based methods [59, 60, 61] to the more recent score-based pruning methods [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]. In the latter approach, an importance score is assigned to each training data point to quantify its influence on a particular permanence metric during model learning. Specifically, score-based DP methods can be broadly categorized into two main groups: influence function-based approaches [10, 15, 16, 17, 18, 19, 20, 21, 22, 23] and training dynamics-based approaches [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28]. The first category measures data influence by examining the effect of data removal on the learning algorithm used during training [15, 16, 17, 18] and the model's prediction [10, 19, 20, 21, 22, 23]. However, influence function-based approaches typically require high computational costs due to the need for high-order derivatives and complex optimization methods, such as bi-level optimization, as used in [22]. Although an approximate influence score can be obtained efficiently, it may result in a large estimation error [62]. In the second category, training dynamics-based approaches find statistical indicators for pruned data from the training trajectory. Examples of DP metrics include data loss/error [25, 26, 27], prediction confidence [24], model gradient norm [26], forgetting event [11], and compactness [12, 13, 14, 28]. However, these methods typically require repeated training to ensure the representativeness of the collected statistics [11, 25, 26]. In addition to improving data efficiency, DP has been applied in a variety of contexts, such as noisy label cleansing [7, 8, 9], continue learning [10, 11, 12, 13, 63], active learning [14], and reducing annotation cost of a finetuner [64].

Data valuation & attribution.Data valuation [19, 41, 65, 66, 67, 68, 69, 70] and attribution [71, 72, 73] are research streams related to dataset pruning that aim to quantify the influence of training data points on model's performance. Unlike dataset pruning, these approaches do not focus primarily on training efficiency but instead aim to enhance interpretability [74], adversarial robustness [68, 70], generative model design [75], and data acquisition quality [72]. Representative methods include Shapley value-based methods [65, 66], datamodels [73], sampling-based methods [65, 68], and proxy-based methods [67, 72]. Recently, Kim et al. [35] proposed to select data samples from a large public dataset (Open-Set) for self-supervised learning given a specific target dataset through distribution mismatch. However, it fails to make a general framework in both supervised and self-supervised scenarios, leaving the former under-explored. The most relevant work to ours is [41], which leverages a leave-one-out analysis to quantify the influence of source data on downstream tasks in the context of transfer learning. With this inspiration, we propose to connect DP to transfer learning in this work.

Recent advancements in transfer learning.Transfer learning has been a prominent area over the past decade [37, 38, 39]. Significant strides have been made in understanding, analyzing, and improving various aspects of this technique [58, 76, 77, 78, 79, 80, 81, 82, 83, 84]. Recent studies [77, 78] have rigorously analyzed the relationship between source model performance and downstream task effectiveness, arguing that a narrow focus on minimizing source training loss may not lead to improved transfer learning results. To improve transfer learning, adversarial training has been shown to benefit the transferability of source pretraining on target downstream tasks [58, 76]. There also exist studies to identify and understand the failure cases in transfer learning [79, 80, 81, 82, 83, 84]. Other research [77, 78] has examined model transferability from the perspective of model sharpness and argues that a good pretrained model should be situated in a flat basin in the downstream loss landscape. Last but not the least, transfer learning has progressed in several other directions, such as self-supervised learning [43, 44, 45, 46, 47, 48, 49, 50, 51, 52], model weight pruning [85, 86, 87, 88, 89], and visual prompting [90, 91, 92, 93, 94, 95, 96, 97, 98, 99].

## 3 Problem Formulation

In this section, we introduce some essential preliminaries on transfer learning and DP (dataset pruning), and elucidate the design challenge of DP for transfer learning.

Preliminaries on transfer learning and connection to DP.Let \(g\circ f\) denote a deep neural network that consists of a feature extractor \(f\) and a classification head \(g\), where \(\circ\) denotes the function composition. Given source and target datasets \(\mathcal{D}_{\mathcal{S}}\) and \(\mathcal{D}_{\mathcal{T}}\), we study transfer learning in the "_pretrain-finetune_" paradigm. The primary goal of _pretraining_ is to obtain a high-quality feature extractor \(f_{\mathcal{S}}:\mathcal{X}\rightarrow\mathcal{Z}\), which draws a mapping from the input space (\(\mathcal{X}\)) to the deep representation space (\(\mathcal{Z}\)) in a data-rich source domain (\(\mathcal{D}_{\mathcal{S}}\)). Popular pertaining recipes include supervised learning (**SL**) [84, 100] and self-supervised learning (**SSL**) [43, 44, 45, 46] depending on whether the source labels (\(y_{\mathcal{S}}\)) are available or not in \(\mathcal{D}_{\mathcal{S}}\). In the _finetuning_ stage, the pretrained model is further trained on a specific downstream task under the target dataset \(\mathcal{D}_{\mathcal{T}}\). Transfer learning expects improved downstream performance over training on \(\mathcal{D}_{\mathcal{T}}\) from scratch. In this work, we consider two finetuning protocols, linear probe (**LP**) and full-finetune (**FF**), with \(\mathcal{D}_{\mathcal{T}}\) being a labeled dataset. LP finetunes the linear classification head \(g\) with a fixed feature extractor \(f_{\mathcal{S}}\), acquired from pretraining. In contrast, FF finetunes the entire model \(g\circ f\) from the initialization \(f_{\mathcal{S}}\). FF typically yields a better transfer learning accuracy than LP, but the former takes a higher computation cost.

Our motivation for connecting transfer learning with DP comes from a recent data-based perspective on transfer learning [41]. The study shows that removing certain _source data classes_ from \(\mathcal{D}_{\mathcal{S}}\) can potentially improve the accuracy of a finetuned model on \(\mathcal{D}_{\mathcal{T}}\). However, the task of evaluating the transfer effects of source data and removing their influence from a pre-trained source model has not been addressed efficiently. The approach developed in [41] involves a leave-one-out analysis to estimate the influence of a source class \(c\) on a target example \(t\), which is computed as the prediction discrepancy of the finetuned source model at \(t\) when the class \(c\) is either included or excluded from \(\mathcal{D}_{\mathcal{S}}\). During this process, one must train multiple source models (over \(7000\) models on ImageNet in [41]) from scratch over different subsets of \(\mathcal{D}_{\mathcal{S}}\) for a given target task. This approach becomes computationally unaffordable when dealing with large source datasets like ImageNet given limited computing resources. To address this challenge, we propose a DP perspective on transfer learning.

Problem of interest: DP for transfer learning.Next, we introduce the background of DP and the problem we focus on. Let \(\mathcal{D}=\{\mathbf{z}_{1},\mathbf{z}_{2},\ldots,\mathbf{z}_{N}\}\) denote a dataset consisting of \(n\) samples, where each sample \(z_{i}\) is represented as a pair \((\mathbf{x}_{i},y_{i})\), with \(\mathbf{x}_{i}\) denoting the input feature vector and \(y_{i}\) denoting the corresponding label. DP aims to generate a pruned dataset \(\hat{\mathcal{D}}=\{\hat{\mathbf{z}}_{1},\hat{\mathbf{z}}_{2},\ldots,\hat{ \mathbf{z}}_{M}\}\subset\mathcal{D}\) with \(M<N\), which can reduce the training cost without a significant decrease in model generalization performance when trained on \(\hat{\mathcal{D}}\). In the context of [41], instead of individual source data sample \(\{\hat{\mathbf{z}}_{i}\}\), the entire source classes are evaluated and selected in terms of transfer influences. Based on the above, we define the problem of our interest below.

**(DP for transfer learning)** How to _prune_ source data classes to obtain \(\hat{\mathcal{D}}_{\mathcal{S}}\) (a subset of \(\mathcal{D}_{\mathcal{S}}\)), with lossless or improved transfer learning accuracy of the source model (\(f_{\mathcal{S}}\)) on a target task \(\mathcal{D}_{\mathcal{T}}\)?

DP for transfer learning has two key distinctions (\(\boldsymbol{\mathsf{\Theta}}\)-\(\boldsymbol{\mathsf{\Theta}}\)) from the vanilla DP setup. First (\(\boldsymbol{\mathsf{\Theta}}\)), DP must be performed in the source domain (\(\mathcal{D}_{\mathcal{S}}\)), while its effectiveness is evaluated based on the target domain (\(\mathcal{D_{T}}\)). This 'cross-domain' challenge makes the design of efficient and effective DP highly non-trivial. For example, prior work [41] utilizes a computationally-intensive leave-one-out analysis. Classical influence function-based methods [10; 19; 20; 21; 22; 23; 101], which trace the eventual model's prediction through the learning algorithm and back to its training data, are also computationally infeasible due to the complex bi-level optimizer and the calculation of high-order derivatives. Second (), the pre-trained source model (\(f_{\mathcal{S}}\)) in today's transfer learning regime is typically of a large scale. This motivates us to develop a DP method that can be independent of the source model, while the pruned source dataset \(\hat{\mathcal{D}}_{\mathcal{S}}\) remains effective in the original transfer learning setup. Given this challenge, we will design DP methods for transfer learning using a _simple surrogate source model_ to avoid the computation on the large source model \(f_{\mathcal{S}}\) (see **Fig. 1** for an illustration).

**Conventional DP methods lack effectiveness on transfer learning.** Given the challenges () posed by DP for transfer learning, we further conduct a preliminary study to investigate the effectiveness of existing 8 DP methods, including SP [20], SSP [20], GraNd [26], EL2N [26], Moderate[3], Forget[53], InfMax[53], and InfSum[53]. Our results show that these methods are _unable_ to yield significant improvements over _random_ source data pruning. **Fig. 2** shows the transfer learning performance of the ResNet-101 [54] model trained on different pruned versions of the ImageNet dataset when LP-based finetuning is conducted on the downstream Flowers102 [102] and OxfordPets [56] datasets. As we can see, in transfer learning, random pruning is a solid baseline for various state-of-the-art DP methods, which have demonstrated superior performance to the former in the non-transfer learning regime. Therefore, it is crucial to develop an efficient and effective DP method specifically tailored for transfer learning.

## 4 Label/Feature Mapping-based DP for Transfer Learning

In this section, we first introduce a simple yet powerful DP method called label mapping (**LM**) by leveraging the class-discriminative capability of a supervised source model. We then extend the concept of LM to feature mapping (**FM**), suitable for self-supervised pretraining.

**LM-based DP for supervised pretraining.** Following the notations used in Sec. 3, we represent the model obtained through supervised learning on the source dataset (\(\mathcal{D_{S}}\)) as \(g_{\mathcal{S}}\circ f_{\mathcal{S}}\). This model predicts a source label given an input example (\(\mathbf{x}\)). In DP for transfer learning, the source data classes serve as the variables to be pruned. Thus, we express \(\mathcal{D_{S}}\) as \(\mathcal{D_{S}}=\{\mathcal{C}_{\mathcal{S}}^{(1)},\dots,\mathcal{C}_{\mathcal{ S}}^{(N)}\}\) for \(N\) source classes, \(\mathcal{C}_{\mathcal{S}}^{(i)}\) denotes the set of data points belonging to the source class \(i\). Additionally, the pruner has access to the target dataset \(\mathcal{D_{T}}=\{\mathbf{t}_{1},\dots,\mathbf{t}_{n}\}\), which consists of \(n\) target data points. Our objective is to utilize the information provided by \(g_{\mathcal{S}}\circ f_{\mathcal{S}}\), \(\mathcal{D_{S}}\), and \(\mathcal{D_{T}}\) to devise a computationally efficient DP criterion. Importantly, our criterion should not involve model training, distinguishing it from the non-scalable approach in [41]. An important observation by [78] in transfer learning is that transferred knowledge improves transferability. This insight is supported by the loss landscape analysis in transfer learning: The finetuned weights may remain within the flat basin of the pretrained weights for enhanced transfer learning performance.

We next propose to extract the 'transferred knowledge' by leveraging the class-discriminative capability of the source model \(g_{\mathcal{S}}\circ f_{\mathcal{S}}\) on the target data samples in \(\mathcal{D_{T}}\). Specifically, we focus on monitoring the responsiveness of the source label predictions made by \(g_{\mathcal{S}}\circ f_{\mathcal{S}}\) when using target samples \(\{\mathbf{t}_{i}\in\mathcal{D_{T}}\}_{i=1}^{n}\) as input data. Here we resize these target samples to ensure their resolution alignment with source data. For the \(i\)th source data class \(\mathcal{C}_{\mathcal{S}}^{(i)}\), we then define its pruning score below:

\[\mathbf{s}_{\mathrm{LM}}(\mathcal{C}_{\mathcal{S}}^{(i)})=\sum_{j=1}^{n} \mathbb{1}\left(g_{\mathcal{S}}\circ f_{\mathcal{S}}(\mathbf{t}_{j})=i\right),\ \ \text{for}\ i=1,2,\dots,N,\] (LM)

Figure 2: Transfer learning accuracy of existing DP methods on ImageNet at different pruning ratios, where ResNet-101 is the source model, and linear probing (LP) is used for downstream finetuning on the target datasets Flowers102 (**Left**) and OxfordPets (**Right**).

where \(1(\cdot)\) represents an indicator function that evaluates to \(1\) when the condition \(\cdot\) is satisfied, and \(0\) otherwise. We refer to the aforementioned formula as LM (Label Mapping) since the condition \(g_{\mathcal{S}}\circ f_{\mathcal{S}}(\mathbf{t}_{j})=i\) establishes a mapping between the predicted labels of the target samples \(\{\mathbf{t}_{j}\}_{j=1}^{n}\) by the source model and the corresponding source labels \(i\). The larger \(\mathfrak{s}_{\mathrm{LM}}\) in (LM) signifies that a specific source class is more frequently utilized to interpret the target data. Consequently, this indicates a more tightly connected relationship between the source class and the transferred knowledge. Therefore, we prune (or select) source data classes with low (or high) \(\mathfrak{s}_{\mathrm{LM}}\) values.

Although LM is simple, it offers several advantages. _Firstly_, it is highly efficient in computation. Given a pre-trained source model \(g_{\mathcal{S}}\circ f_{\mathcal{S}}\), the calculation of LM only requires forward passes through the model. In cases where obtaining the pretrained model in the source domain is challenging, our proposal supports an alternative approach: training a _smaller and simpler surrogate model_ to carry out LM. This surrogate model can effectively replace the complex pretrained model \(g_{\mathcal{S}}\circ f_{\mathcal{S}}\) and facilitate the efficient execution of the pruning process. As we show in **Fig. 2(a)**, employing ResNet-18 [54] is sufficient to successfully prune the source dataset (ImageNet [55]). The resulting DP scheme remains effective to improve transfer learning utilizing other larger source models, such as ResNet-101 [54]. _In addition_, **Fig. 2(b)** shows that the computational overhead incurred by training the small surrogate model (ResNet-18) for DP is insignificant, compared to the time saved during the pretraining phase of a larger source model (ResNet-101) on the pruned dataset for transfer learning. _Lastly_, pretraining on the subset found by LM can guide the source model towards a flatter region in the downstream loss landscape, (see results in **Fig. A4)**. The source model trained on the LM-pruned dataset achieves a higher flatness score than baselines, which aligns with the understanding of transfer learning in [78].

**FM-based DP framework for self-supervised pretraining.** The requirement for _labeled_ source data in LM may pose limitations on the application of DP methods, particularly in the context of self-supervised learning (**SSL**)-based pertaining. To address this limitation, we introduce a new method called FM (Feature Mapping). Unlike LM, FM determines the DP scheme using only the feature extractor network \(f_{\mathcal{S}}\), the unlabeled source dataset \(\mathcal{D}_{\mathcal{S}}\), and the target dataset \(\mathcal{D}_{\mathcal{T}}\). This allows us to overcome the dependence on labeled source data, making FM applicable in SSL scenarios. The inspiration for FM is derived from the deep clustering technique [103, 104, 105] operating in the representation space, which can generate _pseudo source labels_ using _cluster indices_ provided by _e.g._, \(K\)-means. With the assistance of deep clustering, we can represent the unlabeled source dataset \(\mathcal{D}_{\mathcal{S}}\) as \(\mathcal{D}_{\mathcal{S}}=\{\mathcal{C}_{\mathcal{S}}^{(1)},\mathcal{C}_{ \mathcal{S}}^{(2)},\ldots,\mathcal{C}_{\mathcal{S}}^{(K)}\}\), where \(\mathcal{C}_{\mathcal{S}}^{(k)}\) denotes the set of source data samples within cluster \(k\). Building upon the similar spirit as LM, we propose ranking the importance of pseudo source classes for DP by evaluating the source feature extractor's responsiveness to target data samples. To achieve this objective, we quantify the responsiveness of \(f_{\mathcal{S}}\) for a target sample \(\mathbf{t}\) as follows:

\[r(\mathbf{t})=\operatorname*{arg\,min}_{k\in[K]}\ \left\|f_{\mathcal{S}}( \mathbf{t})-\mathbb{E}_{\mathbf{x}\in\mathcal{C}_{\mathcal{S}}^{(k)}}[f_{ \mathcal{S}}(\mathbf{x})]\right\|_{2}, \tag{1}\]

where \(\mathbb{E}_{\mathbf{x}\in\mathcal{C}_{\mathcal{S}}^{(k)}}[f_{\mathcal{S}}( \mathbf{x})]\) is the centroid of source data within the cluster \(k\), and \(r(\mathbf{t})\) is the nearest pseudo label as the responsiveness of \(f_{\mathcal{S}}\) against \(\mathbf{t}\). The FM score then integrates (1) with (LM):

\[\mathfrak{s}_{\mathrm{FM}}(\mathcal{C}_{\mathcal{S}}^{(i)})=\sum_{j=1}^{n} \mathbb{1}(r(\mathbf{t}_{j})=i),\ \ \text{for}\ i=1,2,\ldots,K,\] (FM)

where different from (LM), \(K\) represents the number of pseudo source classes produced by deep clustering, and \(r(\mathbf{t}_{j})\) corresponds to the source feature extractor's prediction on \(\mathbf{t}_{j}\). It is important to note that the value of \(K\) is a free parameter of the deep clustering process. Our empirical study in Fig. A2 shows that FM is quite robust to the choice of \(K\) without sacrificing the benefit of DP for

Figure 3: Preliminary studies on the usage of surrogate models for LM. **(a)** The downstream performance (on Flowers102) of using the source model ResNet-101 trained on the pruned ImageNet delivered by LM at different pruning ratios. Here LM is conducted using either RN-101 or a smaller surrogate model ResNet-18. **(b)** Computation time decomposition analysis for obtaining the pretrained model using ResNet-18 as surrogate model with different pruning ratios.

transfer learning compared to using the unpruned source dataset. Lastly, it is worth mentioning that FM can also be applied in the context of supervised pretraining by specifying data labels as clusters.

## 5 Experiments

In this section, we provide a comprehensive set of experiments and analyses to showcase the effectiveness of our proposed methods (LM and FM) in diverse transfer learning scenarios.

### Experiment setup

**Datasets and models.** In line with existing transfer learning benchmarks [41; 106], we utilize ImageNet-1K [55] for pretraining and **8** datasets as downstream tasks. These datasets include DTD [107], Flowers102 [102], UCF101 [108], Food101 [109], SUN397 [110], OxfordPets [56], StanfordCars [57], and CIFAR10 [111]. Please refer to Tab. A1 for more details about the datasets. As discussed in Sec. 4, we utilize ResNet-18 (RN-18) [54] as the surrogate source model for pruning source classes. This method significantly reduces the computational cost associated with DP, making the process more efficient. Subsequently, a range of larger models, _e.g.,_ ResNet-101 (RN-101) and ViT-B/16 [112], are trained on the (pruned) ImageNet and then finetuned on downstream tasks.

**Baselines, training, and evaluation.** By examining the performance of the existing **8** DP baselines as shown in Fig. 2, our experiments focus on two of the most effective methods: 1@ GraNd[26] and 2@ Moderate[3], together with 3@ Random (the random pruning strategy). In Fig. A1, we show more results of the rest DP baselines. Unfortunately, we are unable to include the existing data attribution method [41] as our baseline, as it does not release its pruning results, and we are unable to repeat its experiments due to the need for intensive computations. Unless specified otherwise, we focus on the supervised pretraining setting in the experiments. For self-supervised pretraining, we follow the implementation of MoCov2 [45]. The finetuning strategies employed include LP (linear probing), which finetunes the classification head with fixed feature extractor, and FF (full finetuning), which finetunes the entire source model. For FM-based DP method, we utilize K-means clustering to group the ImageNet training data points into \(K=2000\) clusters for the computation of (1).

In accordance with the terminology convention in model pruning [87; 113; 114; 115], we refer the term '**winning subsets'** to the obtained source subsets that do _not_ compromise downstream performance. Among these winning subsets, we identify the one with the highest pruning ratio as the '**best winning subset'**. We then evaluate the performance of DP methods from the following aspects: the downstream performance of the model pretrained on the pruned source dataset obtained by various DP methods, and the pruning ratio of the best winning subset achieved by DP methods, accompanied by the corresponding time saved in the pretraining phase.

### Experiment results

**LM/FM improves transfer learning accuracy by identifying 'winning subsets'.** We first showcase the significant improvements achieved by our proposed DP methods (LM and FM) compared to baselines. Our methods successfully identify winning subsets of the ImageNet, yielding transfer accuracy on par or even better than scenarios without pruning.

**Fig. 4** presents the downstream accuracy of transfer learning vs. different pruning ratios. Here DP is performed using the surrogate model (RN-18) on ImageNet for \(8\) downstream tasks. The source model (RN-101) is then trained on the pruned ImageNet and the transfer learning accuracy is assessed through LP (linear probing) and FF (full finetuning). We also present the downstream performance without pruning the source dataset (No Prune) as a reference for winning subsets. As we can see, both LM and FM significantly outperform the baselines by a substantial margin. Notably, LM and FM consistently identify winning subsets with significantly larger pruning ratios in all settings. This highlights the effectiveness of our proposed methods in achieving substantial dataset pruning without hurting downstream performance. Furthermore, we observe that the downstream performance of LM and FM initially improves and then declines as the pruning ratio increases. This is not surprising, since the initial increase in performance corresponds to the scenario where harmful source classes are removed, consistent with [41]. When the source dataset continues to shrink, the performance inevitably decreases, as a natural consequence of reducing the size of the source dataset. Moreover, in some datasets of small sizes (_e.g.,_ OxfordPets), LP achieves performance comparable to FF. Thisis attributed to the fact that the performance gap between LP and FF using the large-scale RN-101 model on small-scale datasets tends to diminish.

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c c c c c} \hline \hline Dataset & \multicolumn{4}{c|}{Ordion/Pets} & \multicolumn{4}{c|}{SUN/97} & \multicolumn{4}{c|}{SUN/97} & \multicolumn{4}{c}{Flows102} \\ Panning Ratio & 0\% & 50\% & 60\% & 70\% & 80\% & 0\% & 0\% & 50\% & 60\% & 70\% & 80\% & 0\% & 50\% & 60\% & 70\% & 80\% \\ \hline Random & & 63.32 & 61.27 & 59.09 & 53.75 & & 45.63 & 45.08 & 45.54 & 39.81 & & 82.23 & 82.60 & 81.03 & 80.02 \\ Mogenic+ & 69.26 & 63.37 & 62.45 & 63.31 & 57.42 & 47.24 & 45.73 & 45.54 & 44.23 & 40.82 & 85.17 & 82.45 & 81.45 & 81.69 & 81.32 \\ GAND & 69.26 & 64.23 & 63.41 & 54.62 & 74.36 & 45.72 & 45.85 & 45.24 & 41.72 & 85.17 & 82.85 & 82.44 & 82.14 & 81.73 \\ FM (ours) & **69.92** & **60.99** & **70.29** & **70.21** & **48.46** & **48.58** & **47.90** & **46.00** & & **85.22** & **85.42** & **84.37** & **84.61** \\ \hline \hline \end{tabular}
\end{table}
Table 2: The downstream performance with different source data pruning ratios in the SSL pretraining setting. A randomly initialized RN-101 is self-supervised pretrained using MoCo v2 on each full/pruned source dataset and finetuned on the downstream task through LP. The best result in each pruning ratio is marked in **bold** and the performance surpassing the unpruned setting (pruning ratio \(0\%\)) is highlighted in \(\text{cyan}\).

Figure 4: Source dataset pruning trajectory given by downstream testing accuracy (%) vs. source dataset pruning ratio (%) in the supervised pretraining setting. Here the source model RN-101 is trained on each full/pruned source dataset (ImageNet) and finetuned on different downstream tasks through LP and FF. The downstream performance without pruning (No Prune) is marked with the black dashed line. Results are averaged over three independent trials (see exact numbers and variances in Tab. A2).

\begin{table}
\begin{tabular}{c|c c c c c|c c c c c c c c c} \hline \hline Dataset & \multicolumn{4}{c|}{Ordion/Pets} & \multicolumn{4}{c|}{SUN/97} & \multicolumn{4}{c|}{SUN/97} & \multicolumn{4}{c}{Flows102} \\ Panning Ratio & 0\% & 50\% & 60\% & 70\% & 80\% & 0\% & 50\% & 60\% & 70\% & 80\% & 0\% & 50\% & 60\% & 70\% & 80\% \\ \hline Random & & 63.32 & 61.27 & 59.09 & 53.75 & & 45.63 & 45.08 & 43.54 & 39.81 & & 82.23 & 82.60 & 81.03

**Tab. 2** highlights the effectiveness of FM-based DP in the self-supervised pretraining setup for three representative downstream tasks. As we can see, the transfer learning accuracy achieved by using FM consistently outperforms baselines in the self-supervised pretraining paradigm. FM can identify winning subsets for transfer learning even in the challenging regime of large pruning ratios, ranging from \(50\%\) to \(80\%\). For instance, in the case of SUN397, FM-based winning subsets achieves a pruning ratio up to \(70\%\), and for Flowers102 the maximum pruning ratio is \(60\%\). These pruning merits align with the findings for FM in the supervised pretraining setup, as illustrated in Tab. 1.

**DP enhances the efficiency of source pretraining. Tab. 3** displays the computation time required to obtain the pretrained source model using LM at different pruning ratios. The reported time consumption includes the entire pipeline, encompassing surrogate model training (RN-18), DP process, and source model training (RN-101) on the pruned ImageNet dataset. The runtime cost of the conventional transfer learning on the full ImageNet dataset for RN-101 is also listed as a reference. As we can see, DP enjoys high efficiency merit of source training. Taking the \(5.4\) hours required for source training on the full ImageNet dataset as a reference, LM-enabled \(20\%\) pruned ImageNet achieves a \(15\%\) reduction in training time. Moreover, the efficiency advantage increases to \(76\%\) when the pruning ratio reaches \(80\%\) and these computational benefits do not sacrifice transfer learning accuracy at all.

Next, we compare the efficiency of our methods with the state-of-the-art dataset pruning baselines in Tab. 4, including GraNd, Moderate, and the brute-force method proposed in [41]. We showcase the efficiency comparison with a pruning ratio of \(60\%\). As we can see, our method (even using a surrogate model) achieves a substantial computation efficiency improvement over other methods. In particular, [41] involves training thousands of source models. Thus, its computational cost is typically unaffortable in experiments.

**DP enables efficient adversarial pretraining. Tab. 5** showcases the improvement in transfer learning accuracy and efficiency achieved by our proposed LP-based DP method when incorporating _adversarial training_ (**AT**) [116] on either the full or pruned ImageNet dataset. This transfer learning setup is motivated by the findings in [58], showing that enhancing the robustness of the source model against adversarial attacks through AT can improve transfer learning accuracy in both LP and FF-based finetuning scenarios. We then report downstream accuracies using LP and FF on two specific downstream datasets chosen due to the large room for improvement in transfer learning accuracy, as shown in Fig. 4. We also determine the pruning ratios for LM by selecting those that led to the best winning subsets. Our experimental results demonstrate that employing AT on both the unpruned and pruned source datasets can improve transfer learning accuracy. Specifically, we refer to AT on the original unpruned ImageNet dataset as Dense-AT, and AT on the LM-pruned ImageNet dataset as LM-AT. One notable advantage of integrating LM into AT is the significant improvement in computation efficiency. A key highlight of this approach is that LM-AT achieves a similar computation time as the standard source training on ImageNet (Dense), while exhibiting almost no accuracy drop compared to Dense-AT. This observation demonstrates the potential to accelerate AT through DP.

**Robustness against the surrogate model size.** To explore the sensitivity of our proposed method to the surrogate model's size and accuracy, we show the transfer learning performance on the task Oxford-Pets against different surrogate model sizes in **Fig. 5**. It is evident that even though the performance of the surrogate model on the source dataset (ImageNet) decreases, the downstream performance of RN-101 pretrained on the LM-based pruned source subsets remains relatively stable. Further, **Tab. A6** compares the class indices selected by the corresponding surrogate models. Interestingly, the most relevant source class selections exhibit a high level of agreement across surrogate models of differing sizes.

\begin{table}
\begin{tabular}{c|c c c c c} \hline Method & Moderate & GraNd & Brutie-Force[41] & ours \\ \hline Time & \multirow{2}{*}{\(7.6\)} & \multirow{2}{*}{\(18.4\)} & \multirow{2}{*}{\(>500\)} & \multirow{2}{*}{\(2.4\)} \\ Consumption (h) & & & & & \\ \hline \end{tabular}
\end{table}
Table 4: Time consumption comparison with a pruning ratio of \(60\%\) of different dataset pruning methods. Other settings follow Fig. 4.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline Pruning Ratio & 0\% & 20\% & 40\% & 60\% & 80\% \\ \hline Time & \multirow{2}{*}{\(5.4\)} & \multirow{2}{*}{\(4.6\)} & \multirow{2}{*}{\(3.5\)} & \multirow{2}{*}{\(2.4\)} & \multirow{2}{*}{\(1.3\)} \\ Consumption (h) & & & & & \\ \hline \multirow{2}{*}{Consumption (h)} & \multirow{2}{*}{\(5.4\)} & \multirow{2}{*}{\((15\%\downarrow)\)} & \multirow{2}{*}{\((35\%\downarrow)\)} & \multirow{2}{*}{\((56\%\downarrow)\)} & \multirow{2}{*}{\((76\%\downarrow)\)} \\ Consumption (h) & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 3: Time consumption of LM/FM in Fig.4 to obtain the pretrained model. The report time consumption covers surrogate model (RN-18) training, LM/FM dataset pruning, and source model pretraining (RN-101).

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Method & \multicolumn{3}{c|}{SUN397} & \multicolumn{3}{c|}{DTD} & \multicolumn{3}{c}{ImageNet} & \multicolumn{3}{c}{ImageNet} & \multicolumn{3}{c}{ImageNet} \\ \cline{2-9}  & \begin{tabular}{c} Ratio \\ \end{tabular} & \begin{tabular}{c} \(\frac{\text{Area}}{\text{Area}}\) \\ \end{tabular} & \begin{tabular}{c} \(\frac{\text{Area}}{\text{Area}}\) \\ \end{tabular} & \begin{tabular}{c} \(\frac{\text{Area}}{\text{Area}}\) \\ \end{tabular} & \begin{tabular}{c} \(\frac{\text{Area}}{\text{Area}}\) \\ \end{tabular} & \begin{tabular}{c} \(\frac{\text{Area}}{\text{Area}}\) \\ \end{tabular} & \begin{tabular}{c} \(\frac{\text{Area}}{\text{Area}}\) \\ \end{tabular} & \begin{tabular}{c} \(\frac{\text{Area}}{\text{Area}}\) \\ \end{tabular} & \begin{tabular}{c} \(\frac{\text{Area}}{\text{Area}}\) \\ \end{tabular} & \begin{tabular}{c} \(\frac{\text{Area}}{\text{Area}}\) \\ \end{tabular} \\ \hline Dense & \multirow{2}{*}{\(5.4\)} & \multirow{2}{*}{\(5.4\)} & \multirow{2}{*}{\(5.4\)} & \multirow{2}{*}{\(4.5\)} & \multirow{2}{*}{\(1.3\)} & \multirow{2}{*}{\(1.3\)} & \multirow{2}{*}{\(5.5\)} & \multirow{2}{*}{\(2.4\)} \\ Loss & & & & & & & \\ \hline \multirow{2}{*}{\begin{tabular}{c} Loss \\ LM \\ \end{tabular} } & \multirow{2}{*}{\(7.6\)} & \multirow{2}{*}{\(18.4\)} & \multirow{2}{*}{\(>500\)} & \multirow{2}{*}{\(2.4\)} & \multirow{2}{*}{\(>500\)} & \multirow{2}{*}{\(6.2\)} & \multirow{2}{*}{\(6.8\)} & \multirow{2}{*}{\(6.7\)} & \multirow{2}{*}{\(2.9\)} \\ Loss & & & & & & & \\ \hline \multirow{2}{*}{\begin{tabular}{c} Loss \\ LM \\ \end{tabular} } & \multirow{2}{*}{\(7.0\)} & \multirow{2}{*}{\(35.0\)} & \multirow{2}{*}{\(35.4\)} & \multirow{2}{*}{\(4.2\)} & \multirow{2}{*}{\(1.9\)} & \multirow{2}{*}{\(50\)} & \multirow{2}{*}{\(6.2\)} & \multirow{2}{*}{\(6.8\)} & \multirow{2}{*}{\(6.7\)} & \multirow{2}{*}{\(6.8\)} \\ Loss & & & & & & & \\ \hline \multirow{2}{*}{\begin{tabular}{c} Loss \\ LM \\ \end{tabular} } & \multirow{2}{*}{\(7.0\)} & \multirow{2}{*}{\(35.0\)} & \multirow{2}{*}{\(35.4\)} & \multirow{2}{*}{\(4.2\)} & \multirow{2}{*}{\(4.2\)} & \multirow{2}{*}{\(50\)} & \multirow{2}{*}{\(6.2\)} & \multirow{2}{*}{\(6.8\)} & \multirow{2}{*}{\(6.7\)} \\ Loss & & & & & & & \\ \hline \multirow{2}{*}{
\begin{tabular}{c} Loss \\ LM \\ \end{tabular} } & \multirow{2}{*}{\(7.5\)} & \multirow{2}{*}{\(35.0\)} & \multirow{2}{*}{\(35.4\)} & \multirow{2}{*}{\(4.2\)} & \multirow{2}{*}{\(4.2\)} & \multirow{2}{*}{\(50\)} & \multirow{2}{*}{\(6.2\)} & \multirow{2}{*}{\(6.8\)} & \multirow{2}{*}{\(6.7\)} \\ Loss & & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 5: Downstream performance of models pretrained on full/pruned source dataset (Dense/LM) w/wo adversarial pretraining (Adv). For DenseAdv and LM-Adv, \(3\)-step adversarial training [116] is used for pretraining. Pruning ratio, downstream test accuracy (Acc.), and time consumptions for obtaining pretrained models are reported.

[MISSING_PAGE_FAIL:10]

## References

* [1] B. Zhao, K. R. Mopuri, and H. Bilen, "Dataset condensation with gradient matching," _arXiv preprint arXiv:2006.05929_, 2020.
* [2] B. Zhao and H. Bilen, "Dataset condensation with differentiable siamese augmentation," in _International Conference on Machine Learning_. PMLR, 2021, pp. 12 674-12 685.
* [3] X. Xia, J. Liu, J. Yu, X. Shen, B. Han, and T. Liu, "Moderate coreset: A universal method of data selection for real-world data-efficient deep learning," in _The Eleventh International Conference on Learning Representations_, 2023.
* [4] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, and W. Brendel, "Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness," _arXiv preprint arXiv:1811.12231_, 2018.
* [5] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry, "Adversarial examples are not bugs, they are features," _Advances in neural information processing systems_, vol. 32, 2019.
* [6] T. Gu, B. Dolan-Gavitt, and S. Garg, "Badnets: Identifying vulnerabilities in the machine learning model supply chain," _arXiv preprint arXiv:1708.06733_, 2017.
* [7] H. Wei, L. Feng, X. Chen, and B. An, "Combating noisy labels by agreement: A joint training method with co-regularization," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2020, pp. 13 726-13 735.
* [8] L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei, "Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels," in _International conference on machine learning_. PMLR, 2018, pp. 2304-2313.
* [9] C. Guo, B. Zhao, and Y. Bai, "Deepcore: A comprehensive library for coreset selection in deep learning," in _Database and Expert Systems Applications: 33rd International Conference, DEXA 2022, Vienna, Austria, August 22-24, 2022, Proceedings, Part I_. Springer, 2022, pp. 181-195.
* [10] Z. Borsos, M. Mutny, and A. Krause, "Coresets via bilevel optimization for continual learning and streaming," _Advances in Neural Information Processing Systems_, vol. 33, pp. 14 879-14 890, 2020.
* [11] M. Toneva, A. Sordoni, R. T. d. Combes, A. Trischler, Y. Bengio, and G. J. Gordon, "An empirical study of example forgetting during deep neural network learning," _arXiv preprint arXiv:1812.05159_, 2018.
* [12] F. M. Castro, M. J. Marin-Jimenez, N. Guil, C. Schmid, and K. Alahari, "End-to-end incremental learning," in _Proceedings of the European conference on computer vision (ECCV)_, 2018, pp. 233-248.
* [13] R. Aljundi, M. Lin, B. Goujaud, and Y. Bengio, "Gradient based sample selection for online continual learning," _Advances in neural information processing systems_, vol. 32, 2019.
* [14] O. Sener and S. Savarese, "Active learning for convolutional neural networks: A core-set approach," _arXiv preprint arXiv:1708.00489_, 2017.
* [15] S. Chatterjee and A. S. Hadi, "Influential observations, high leverage points, and outliers in linear regression," _Statistical science_, pp. 379-393, 1986.
* [16] R. D. Cook, "Assessment of local influence," _Journal of the Royal Statistical Society: Series B (Methodological)_, vol. 48, no. 2, pp. 133-155, 1986.
* [17] W. Thomas and R. D. Cook, "Assessing influence on predictions from generalized linear models," _Technometrics_, vol. 32, no. 1, pp. 59-65, 1990.
* [18] B.-C. Wei, Y.-Q. Hu, and W.-K. Fung, "Generalized leverage and its applications," _Scandinavian Journal of statistics_, vol. 25, no. 1, pp. 25-37, 1998.

* [19] P. W. Koh and P. Liang, "Understanding black-box predictions via influence functions," in _International conference on machine learning_. PMLR, 2017, pp. 1885-1894.
* [20] A. Schioppa, P. Zablotskaia, D. Vilar, and A. Sokolov, "Scaling up influence functions," in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 36, 2022, pp. 8179-8186.
* [21] H. Guo, N. F. Rajani, P. Hase, M. Bansal, and C. Xiong, "Fastif: Scalable influence functions for efficient model interpretation and debugging," _arXiv preprint arXiv:2012.15781_, 2020.
* [22] S. Yang, Z. Xie, H. Peng, M. Xu, M. Sun, and P. Li, "Dataset pruning: Reducing training data by examining generalization influence," _arXiv preprint arXiv:2205.09329_, 2022.
* [23] S. Kong, Y. Shen, and L. Huang, "Resolving training biases via influence-based data relabeling," in _International Conference on Learning Representations_, 2022. [Online]. Available: [https://openreview.net/forum?id=EskfH0bwNVn](https://openreview.net/forum?id=EskfH0bwNVn)
* [24] G. Pleiss, T. Zhang, E. Elenberg, and K. Q. Weinberger, "Identifying mislabeled data using the area under the margin ranking," _Advances in Neural Information Processing Systems_, vol. 33, pp. 17 044-17 056, 2020.
* [25] M. Welling, "Herding dynamical weights to learn," in _Proceedings of the 26th Annual International Conference on Machine Learning_, 2009, pp. 1121-1128.
* [26] M. Paul, S. Ganguli, and G. K. Dziugaite, "Deep learning on a data diet: Finding important examples early in training," _Advances in Neural Information Processing Systems_, vol. 34, pp. 20 596-20 607, 2021.
* [27] G. Pruthi, F. Liu, S. Kale, and M. Sundararajan, "Estimating training data influence by tracing gradient descent," _Advances in Neural Information Processing Systems_, vol. 33, pp. 19 920-19 930, 2020.
* [28] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, "icarl: Incremental classifier and representation learning," in _Proceedings of the IEEE conference on Computer Vision and Pattern Recognition_, 2017, pp. 2001-2010.
* [29] D. Feldman and M. Langberg, "A unified framework for approximating and clustering data," in _Proceedings of the forty-third annual ACM symposium on Theory of computing_, 2011, pp. 569-578.
* [30] J. Ju, H. Jung, Y. Oh, and J. Kim, "Extending contrastive learning to unsupervised coreset selection," _IEEE Access_, vol. 10, pp. 7704-7715, 2022.
* [31] J. Huggins, T. Campbell, and T. Broderick, "Coresets for scalable bayesian logistic regression," _Advances in Neural Information Processing Systems_, vol. 29, 2016.
* [32] T. Campbell and T. Broderick, "Automated scalable bayesian inference via hilbert coresets," _The Journal of Machine Learning Research_, vol. 20, no. 1, pp. 551-588, 2019.
* [33] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner, "Variational continual learning," _arXiv preprint arXiv:1710.10628_, 2017.
* [34] S. Farquhar and Y. Gal, "Towards robust evaluations of continual learning," _arXiv preprint arXiv:1805.09733_, 2018.
* [35] S. Kim, S. Bae, and S.-Y. Yun, "Coreset sampling from open-set for fine-grained self-supervised learning," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023, pp. 7537-7547.
* [36] Y. Yang, H. Kang, and B. Mirzasoleiman, "Towards sustainable learning: Coresets for data-efficient deep learning," _arXiv preprint arXiv:2306.01244_, 2023.
* [37] S. J. Pan and Q. Yang, "A survey on transfer learning," _IEEE Transactions on knowledge and data engineering_, vol. 22, no. 10, pp. 1345-1359, 2010.

* [38] L. Torrey and J. Shavlik, "Transfer learning," in _Handbook of research on machine learning applications and trends: algorithms, methods, and techniques_. IGI global, 2010, pp. 242-264.
* [39] L. Yang, S. Hanneke, and J. Carbonell, "A theory of transfer learning with applications to active learning," _Machine learning_, vol. 90, pp. 161-189, 2013.
* [40] B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli, and A. S. Morcos, "Beyond neural scaling laws: beating power law scaling via data pruning," in _Advances in Neural Information Processing Systems_, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022. [Online]. Available: [https://openreview.net/forum?id=UmvSIP_PyV](https://openreview.net/forum?id=UmvSIP_PyV)
* [41] S. Jain, H. Salman, A. Khaddaj, E. Wong, S. M. Park, and A. Madry, "A data-based perspective on transfer learning," _arXiv preprint arXiv:2207.05739_, 2022.
* [42] P. Sattigeri, S. Ghosh, I. Padhi, P. Dognin, and K. R. Varshney, "Fair infinitesimal jackknife: Mitigating the influence of biased training data points without refitting," _arXiv preprint arXiv:2212.06803_, 2022.
* [43] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, "A simple framework for contrastive learning of visual representations," in _International conference on machine learning_. PMLR, 2020, pp. 1597-1607.
* [44] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, "Momentum contrast for unsupervised visual representation learning," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2020, pp. 9729-9738.
* [45] X. Chen, H. Fan, R. Girshick, and K. He, "Improved baselines with momentum contrastive learning," _arXiv preprint arXiv:2003.04297_, 2020.
* [46] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton, "Big self-supervised models are strong semi-supervised learners," _Advances in neural information processing systems_, vol. 33, pp. 22 243-22 255, 2020.
* [47] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, "Unsupervised feature learning via non-parametric instance discrimination," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018, pp. 3733-3742.
* [48] Y. Tian, C. Sun, B. Poole, D. Krishnan, C. Schmid, and P. Isola, "What makes for good views for contrastive learning?" _Advances in neural information processing systems_, vol. 33, pp. 6827-6839, 2020.
* [49] J.-B. Grill, F. Strub, F. Altche, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar _et al._, "Bootstrap your own latent-a new approach to self-supervised learning," _Advances in neural information processing systems_, vol. 33, pp. 21 271-21 284, 2020.
* [50] Y. M. Asano, C. Rupprecht, and A. Vedaldi, "Self-labelling via simultaneous clustering and representation learning," _arXiv preprint arXiv:1911.05371_, 2019.
* [51] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, "Unsupervised learning of visual features by contrasting cluster assignments," _Advances in neural information processing systems_, vol. 33, pp. 9912-9924, 2020.
* [52] J. Li, P. Zhou, C. Xiong, and S. C. Hoi, "Prototypical contrastive learning of unsupervised representations," _arXiv preprint arXiv:2005.04966_, 2020.
* [53] V. Feldman and C. Zhang, "What neural networks memorize and why: Discovering the long tail via influence estimation," _Advances in Neural Information Processing Systems_, vol. 33, pp. 2881-2891, 2020.
* [54] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2016, pp. 770-778.

* [55] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein _et al._, "Imagenet large scale visual recognition challenge," _International journal of computer vision_, vol. 115, pp. 211-252, 2015.
* [56] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar, "Cats and dogs," in _2012 IEEE conference on computer vision and pattern recognition_. IEEE, 2012, pp. 3498-3505.
* [57] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, "3d object representations for fine-grained categorization," in _Proceedings of the IEEE international conference on computer vision workshops_, 2013, pp. 554-561.
* [58] H. Salman, A. Ilyas, L. Engstrom, A. Kapoor, and A. Madry, "Do adversarially robust imagenet models transfer better?" _Advances in Neural Information Processing Systems_, vol. 33, pp. 3533-3545, 2020.
* [59] P. K. Agarwal, S. Har-Peled, and K. R. Varadarajan, "Approximating extent measures of points," _Journal of the ACM (JACM)_, vol. 51, no. 4, pp. 606-635, 2004.
* [60] S. Har-Peled and S. Mazumdar, "On coresets for k-means and k-median clustering," in _Proceedings of the thirty-sixth annual ACM symposium on Theory of computing_, 2004, pp. 291-300.
* [61] D. Feldman, M. Schmidt, and C. Sohler, "Turning big data into tiny data: Constant-size coresets for k-means, pca, and projective clustering," _SIAM Journal on Computing_, vol. 49, no. 3, pp. 601-657, 2020.
* [62] S. Basu, P. Pope, and S. Feizi, "Influence functions in deep learning are fragile," _arXiv preprint arXiv:2006.14651_, 2020.
* [63] J. Jia, Y. Zhang, D. Song, S. Liu, and A. Hero, "Robustness-preserving lifelong learning via dataset condensation," in _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE, 2023, pp. 1-5.
* [64] Y. Xie, H. Lu, J. Yan, X. Yang, M. Tomizuka, and W. Zhan, "Active finetuning: Exploiting annotation budget in the pretraining-finetuning paradigm," _arXiv preprint arXiv:2303.14382_, 2023.
* [65] A. Ghorbani and J. Zou, "Data shapley: Equitable valuation of data for machine learning," in _International Conference on Machine Learning_. PMLR, 2019, pp. 2242-2251.
* [66] R. Jia, D. Dao, B. Wang, F. A. Hubis, N. Hynes, N. M. Gurel, B. Li, C. Zhang, D. Song, and C. J. Spanos, "Towards efficient data valuation based on the shapley value," in _The 22nd International Conference on Artificial Intelligence and Statistics_. PMLR, 2019, pp. 1167-1176.
* [67] R. Jia, D. Dao, B. Wang, F. A. Hubis, N. M. Gurel, B. Li, C. Zhang, C. J. Spanos, and D. Song, "Efficient task-specific data valuation for nearest neighbor algorithms," _arXiv preprint arXiv:1908.08619_, 2019.
* [68] J. Lin, A. Zhang, M. Lecuyer, J. Li, A. Panda, and S. Sen, "Measuring the effect of training data on deep learning predictions via randomized experiments," in _International Conference on Machine Learning_. PMLR, 2022, pp. 13 468-13 504.
* [69] K. Guu, A. Webson, E. Pavlick, L. Dixon, I. Tenney, and T. Bolukbasi, "Simfluence: Modeling the influence of individual training examples by simulating training runs," _arXiv preprint arXiv:2303.08114_, 2023.
* [70] Z. Hammoudeh and D. Lowd, "Identifying a training-set attack's target using renormalized influence estimation," in _Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security_, 2022, pp. 1367-1381.
* [71] S. M. Park, K. Georgiev, A. Ilyas, G. Leclerc, and A. Madry, "Trak: Attributing model behavior at scale," _arXiv preprint arXiv:2303.14186_, 2023.

* [72] H. A. Just, F. Kang, J. T. Wang, Y. Zeng, M. Ko, M. Jin, and R. Jia, "Lava: Data valuation without pre-specified learning algorithms," _arXiv preprint arXiv:2305.00054_, 2023.
* [73] A. Ilyas, S. M. Park, L. Engstrom, G. Leclerc, and A. Madry, "Datamodels: Predicting predictions from training data," _arXiv preprint arXiv:2202.00622_, 2022.
* [74] C.-K. Yeh, J. Kim, I. E.-H. Yen, and P. K. Ravikumar, "Representer point selection for explaining deep neural networks," _Advances in neural information processing systems_, vol. 31, 2018.
* [75] Y. Song and S. Ermon, "Generative modeling by estimating gradients of the data distribution," _Advances in neural information processing systems_, vol. 32, 2019.
* [76] Z. Deng, L. Zhang, K. Vodrahalli, K. Kawaguchi, and J. Y. Zou, "Adversarial training helps transfer learning via better representations," _Advances in Neural Information Processing Systems_, vol. 34, pp. 25 179-25 191, 2021.
* [77] H. Liu, S. M. Xie, Z. Li, and T. Ma, "Same pre-training loss, better downstream: Implicit bias matters for language models," _arXiv preprint arXiv:2210.14199_, 2022.
* [78] H. Liu, M. Long, J. Wang, and M. I. Jordan, "Towards understanding the transferability of deep representations," _arXiv preprint arXiv:1909.12031_, 2019.
* [79] M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G. Dietterich, "To transfer or not to transfer," in _NIPS 2005 workshop on transfer learning_, vol. 898, no. 3, 2005.
* [80] L. Duan, D. Xu, and S.-F. Chang, "Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach," in _2012 IEEE Conference on computer vision and pattern recognition_. IEEE, 2012, pp. 1338-1345.
* [81] L. Ge, J. Gao, H. Ngo, K. Li, and A. Zhang, "On handling negative transfer and imbalanced distributions in multiple source transfer learning," _Statistical Analysis and Data Mining: The ASA Data Science Journal_, vol. 7, no. 4, pp. 254-271, 2014.
* [82] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, "How transferable are features in deep neural networks?" _Advances in neural information processing systems_, vol. 27, 2014.
* [83] Z. Cao, M. Long, J. Wang, and M. I. Jordan, "Partial transfer learning with selective adversarial networks," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2018, pp. 2724-2732.
* [84] Z. Wang, Z. Dai, B. Poczos, and J. Carbonell, "Characterizing and avoiding negative transfer," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 2019, pp. 11 293-11 302.
* [85] D. Guo, A. M. Rush, and Y. Kim, "Parameter-efficient transfer learning with diff pruning," _arXiv preprint arXiv:2012.07463_, 2020.
* [86] B. Liu, Y. Cai, Y. Guo, and X. Chen, "Transtailor: Pruning the pre-trained model for improved transfer learning," in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 35, no. 10, 2021, pp. 8627-8634.
* [87] T. Chen, J. Frankle, S. Chang, S. Liu, Y. Zhang, M. Carbin, and Z. Wang, "The lottery tickets hypothesis for supervised and self-supervised pre-training in computer vision models," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2021, pp. 16 306-16 316.
* [88] T. Chen, Y. Cheng, Z. Gan, L. Yuan, L. Zhang, and Z. Wang, "Chasing sparsity in vision transformers: An end-to-end exploration," _Advances in Neural Information Processing Systems_, vol. 34, pp. 19 974-19 988, 2021.
* [89] S. Myung, I. Huh, W. Jang, J. M. Choe, J. Ryu, D. Kim, K.-E. Kim, and C. Jeong, "Pac-net: A model pruning approach to inductive transfer learning," in _International Conference on Machine Learning_. PMLR, 2022, pp. 16 240-16 252.

* [90] H. Bahng, A. Jahanian, S. Sankaranarayanan, and P. Isola, "Exploring visual prompts for adapting large-scale models," _arXiv preprint arXiv:2203.17274_, vol. 1, no. 3, p. 4, 2022.
* [91] G. F. Elsayed, I. Goodfellow, and J. Sohl-Dickstein, "Adversarial reprogramming of neural networks," _arXiv preprint arXiv:1806.11146_, 2018.
* [92] P.-Y. Chen, "Model reprogramming: Resource-efficient cross-domain machine learning," _arXiv preprint arXiv:2202.10629_, 2022.
* [93] Y. Zheng, X. Feng, Z. Xia, X. Jiang, A. Demontis, M. Pintor, B. Biggio, and F. Roli, "Why adversarial reprogramming works, when it fails, and how to tell the difference," _arXiv preprint arXiv:2108.11673_, 2021.
* [94] P. Neekhara, S. Hussain, S. Dubnov, and F. Koushanfar, "Adversarial reprogramming of text classification neural networks," _arXiv preprint arXiv:1809.01829_, 2018.
* [95] P. Neekhara, S. Hussain, J. Du, S. Dubnov, F. Koushanfar, and J. McAuley, "Cross-modal adversarial reprogramming," in _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, 2022, pp. 2427-2435.
* [96] L. Chen, Y. Fan, and Y. Ye, "Adversarial reprogramming of pretrained neural networks for fraud detection," in _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, 2021, pp. 2935-2939.
* [97] A. Chen, Y. Yao, P.-Y. Chen, Y. Zhang, and S. Liu, "Understanding and improving visual prompting: A label-mapping perspective," _arXiv preprint arXiv:2211.11635_, 2022.
* [98] A. Chen, P. Lorenz, Y. Yao, P.-Y. Chen, and S. Liu, "Visual prompting for adversarial robustness," in _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE, 2023, pp. 1-5.
* [99] G. Zhang, Y. Zhang, Y. Zhang, W. Fan, Q. Li, S. Liu, and S. Chang, "Fairness reprogramming," _arXiv preprint arXiv:2209.10222_, 2022.
* [100] G. Zhang, H. Zhao, Y. Yu, and P. Poupart, "Quantifying and improving transferability in domain generalization," _Advances in Neural Information Processing Systems_, vol. 34, pp. 10 957-10 970, 2021.
* [101] Y. Zhang, P. Khanduri, I. Tsaknakis, Y. Yao, M. Hong, and S. Liu, "An introduction to bi-level optimization: Foundations and applications in signal processing and machine learning," _arXiv preprint arXiv:2308.00788_, 2023.
* [102] M.-E. Nilsback and A. Zisserman, "Automated flower classification over a large number of classes," in _2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing_. IEEE, 2008, pp. 722-729.
* [103] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, "Deep clustering for unsupervised learning of visual features," in _Proceedings of the European conference on computer vision (ECCV)_, 2018, pp. 132-149.
* [104] X. Yan, I. Misra, A. Gupta, D. Ghadiyaram, and D. Mahajan, "Clusterfit: Improving generalization of visual representations," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, pp. 6509-6518.
* [105] L. Fan, S. Liu, P.-Y. Chen, G. Zhang, and C. Gan, "When does contrastive learning preserve adversarial robustness from pretraining to finetuning?" _Advances in Neural Information Processing Systems_, vol. 34, pp. 21 480-21 492, 2021.
* [106] U. Evci, V. Dumoulin, H. Larochelle, and M. C. Mozer, "Head2toe: Utilizing intermediate representations for better transfer learning," in _International Conference on Machine Learning_. PMLR, 2022, pp. 6009-6033.
* [107] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi, "Describing textures in the wild," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2014, pp. 3606-3613.

* [108] K. Soomro, A. R. Zamir, and M. Shah, "Ucf101: A dataset of 101 human actions classes from videos in the wild," _arXiv preprint arXiv:1212.0402_, 2012.
* [109] L. Bossard, M. Guillaumin, and L. V. Gool, "Food-101-mining discriminative components with random forests," in _European conference on computer vision_. Springer, 2014, pp. 446-461.
* [110] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, "Sun database: Large-scale scene recognition from abbey to zoo," in _2010 IEEE computer society conference on computer vision and pattern recognition_. IEEE, 2010, pp. 3485-3492.
* [111] A. Krizhevsky, G. Hinton _et al._, "Learning multiple layers of features from tiny images," _cs.utoronto.ca_, 2009.
* [112] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly _et al._, "An image is worth 16x16 words: Transformers for image recognition at scale," _arXiv preprint arXiv:2010.11929_, 2020.
* [113] J. Frankle and M. Carbin, "The lottery ticket hypothesis: Finding sparse, trainable neural networks," _arXiv preprint arXiv:1803.03635_, 2018.
* [114] T. Chen, J. Frankle, S. Chang, S. Liu, Y. Zhang, Z. Wang, and M. Carbin, "The lottery ticket hypothesis for pre-trained bert networks," _Advances in neural information processing systems_, vol. 33, pp. 15 834-15 846, 2020.
* [115] Y. Zhang, Y. Yao, P. Ram, P. Zhao, T. Chen, M. Hong, Y. Wang, and S. Liu, "Advancing model pruning via bi-level optimization," _arXiv preprint arXiv:2210.04092_, 2022.
* [116] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, "Towards deep learning models resistant to adversarial attacks," _arXiv preprint arXiv:1706.06083_, 2017.
* [117] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer, "Multiscale vision transformers," in _Proceedings of the IEEE/CVF international conference on computer vision_, 2021, pp. 6824-6835.
* [118] D. Hendrycks and T. Dietterich, "Benchmarking neural network robustness to common corruptions and perturbations," _arXiv preprint arXiv:1903.12261_, 2019.
* [119] X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen, C. Riquelme, M. Lucic, J. Djolonga, A. S. Pinto, M. Neumann, A. Dosovitskiy _et al._, "A large-scale study of representation learning with the visual task adaptation benchmark," _arXiv preprint arXiv:1910.04867_, 2019.
* [120] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang, "On large-batch training for deep learning: Generalization gap and sharp minima," _arXiv preprint arXiv:1609.04836_, 2016.
* [121] D. Bisla, J. Wang, and A. Choromanska, "Low-pass filtering sgd for recovering flat optima in the deep learning optimization landscape," in _International Conference on Artificial Intelligence and Statistics_. PMLR, 2022, pp. 8299-8339.
* [122] W. J. Maddox, G. Benton, and A. G. Wilson, "Rethinking parameter counting in deep models: Effective dimensionality revisited," _arXiv preprint arXiv:2003.02139_, 2020.
* [123] T. Liang, T. Poggio, A. Rakhlin, and J. Stokes, "Fisher-rao metric, geometry, and complexity of neural networks," in _The 22nd international conference on artificial intelligence and statistics_. PMLR, 2019, pp. 888-896.
* [124] L. Van der Maaten and G. Hinton, "Visualizing data using t-sne." _Journal of machine learning research_, vol. 9, no. 11, 2008.

[MISSING_PAGE_FAIL:18]

GraNd are also demonstrating strong baselines, motivating us to choose them as the default DP baselines in Section 5.

The main results reported in Fig. 4 are stable: analysis with detailed numerical results and standard deviations.In Tab. A2, we provide the exact numerical results used to generate Fig. 4. These numbers give a more granular view of the performance comparisons. The results, calculated over three independent trials, show that the magnitude of the standard deviations is quite small relative to the mean values. This indicates that the trends and conclusions drawn from Fig. 4 are generally valid and not significantly affected by trial variations. To maintain readability and clarity, we choose not to include these minor standard deviations in Fig. 4.

FM performance is robust to the choice of cluster number.The results presented in Fig. A2 provide insights into the sensitivity of FM performance to variations of the cluster number (\(K\)). This extended study from Tab. 2 shows that FM performance remains robust as the cluster number varies from \(500\) to \(2000\), preserving the benefits of DP for transfer learning without much performance degradation. This implies that FM can provide reliable and stable results even when the hyperparameter settings are not optimal.

LM identifies the most influential source classes.To validate that the classes with the highest LM scores are the most influential, we present the pruning trajectory in Fig. A3, where pruning is executed in the reverse order of the class scores, different from the proposed DP implementation. That is, classes with the smallest scores are retained, while the ones with the highest scores are pruned. Remarkably, even a slight pruning ratio (such as \(10\%\)) leads to a significant degradation in downstream performance. This evidence underscores the benefits of source classes with high LM scores in promoting the downstream performance.

FM can be smoothly applied to MoCov3.We conducted experiments to illustrate the effectiveness of our proposed method (FM) when applied to the more recent SSL framework MoCov3 [117] usingthe ViT structure. In line with our SSL experiment plan detailed in Tab. 2 of the paper, **Tab. A3** tested FM on three downstream datasets, specifically OxfordPets, Flowers102, and SUN397, with a source data class pruning ratio ranging from \(50\%\) to \(80\%\). The results affirm that the principal conclusions drawn from MoCov2 remain consistent with MoCov3 on ViT. Our method, FM, successfully identifies data subsets within the source dataset that can be pruned at high ratios without compromising downstream performance (termed as "winning subsets"). For instance, in one particular case, the FM-based winning subsets achieve a pruning ratio of up to \(70\%\) on OxfordPets.

Explore LM in the multi-task setting.A limitation of our method is its task-specific nature in data influence analysis. As our methods assess source data influence for specific downstream tasks, developing a universal DP solution for multiple tasks simultaneously is challenging. As a preliminary study, we examine the performance of source dataset pruning for multiple downstream tasks simultaneously in **Fig. A5**. While LM can still identify winning subsets, the maximum pruning ratio diminishes as more tasks are considered. We will include this limitation in the Conclusion section.

LM helps remove data biases through DP for a downstream task.To investigate the scenarios with data biases, we conducted experiments on CIFAR-10C (the out-of-distribution scenario) [118]. We first pruned ImageNet given CIFAR-10 as a downstream task and evaluated the model on CIFAR-10C with different corruption types. **Tab. A4** shows LM results with different pruning ratios for 5 strong perturbation types in CIFAR-10C. Impressively, LM can achieve winning subsets with pruning up to \(80\%\), even better than on CIFAR-10, confirming our method's effectiveness to filter biased data points to some degree.

[MISSING_PAGE_FAIL:22]

the downstream loss landscape, potentially contributing to the superior transferability of LM when compared to the baseline methods.

Feature distribution analysis.Fig. A6 provides visual explanations of DP at both deep representation (feature) and data levels. Here we focus on the LM method with a pruning ratio of \(90\%\) given the downstream dataset OxfordPets. The other settings are consistent with Fig. 4. In **Fig. A6 (right)**, we visualize the source ImageNet classes (including pruned ones and retrained ones) and target OxfordPets classes in terms of their class-wise feature centroids in a 2D space achieved by t-SNE[124]. The class feature centroid is obtained by averaging the features of data points within a class, extracted by the pretrained source model on the full ImageNet dataset. As we can see, all retained source classes are grouped together and are close to the target classes. This indicates that the source classes that the pruning process share the most resemblance with the target data. In contrast, the pruned source classes are more dispersed and located further away from the target data classes. Furthermore, **Fig. A6 (left)** exhibits image examples of target classes as well as pruned and retrained source classes. We observe that image examples in the retained source classes (_e.g._, relating to animals) are semantically closer to the target data points (relating to pets) than the pruned ones. This highlights the ability of LM to effectively identify and retain the most relevant classes for the downstream tasks. We provide more examples for FM in Fig. A7.

Examining the top selected classes by FM and their image examples.In Fig. A7, we showcase the top-10 source classes chosen by FM, as determined by the endowed scores. These selected classes closely correspond to the downstream datasets' subjects, demonstrating FM's effectiveness in identifying relevant classes for transfer learning. This finding also aligns with our observations in Fig. A6, showing that FM identifies source classes resembling downstream data.

\begin{table}
\begin{tabular}{c|c c c c c c c|c} \hline \hline Surrogate Model & RN-20s & VGG-2 & RN-32s & VGG-4 & RN-44s & RN-56s & VGG-8 & \begin{tabular}{c} RN-18 \\ (Default) \\ \end{tabular} \\ \hline Param. \# (M) & 0.236 & 0.417 & 0.516 & 0.698 & 0.706 & 0.896 & 5.53 & 11.69 \\ \hline Source Acc. (\%) & 36.25 & 22.56 & 40.77 & 29.44 & 43.74 & 45.72 & 58.45 & 68.73 \\ \hline \begin{tabular}{c} Largest Pruning Ratio \\ of Winning Subsets (\%) \\ \end{tabular} & 60 & 50 & 80 & 70 & 80 & 80 & 80 & 80 \\ \hline 
\begin{tabular}{c} Source Class \\ Selection Overlap (\%) \\ \end{tabular} & 89.3 & 84.4 & 90.7 & 87.2 & 93.5 & 94.8 & 97.7 & 100 \\ \hline \hline \end{tabular}
\end{table}
Table A6: Performance on surrogate model size. Experiments follow the setting of Fig. 4: RN-101 is first pretrained on the pruned source dataset (ImageNet) based on the surrogate model, and then finetuned on the downstream task OxfordPets. Under different surrogate models, the source class selection overlapping ratio with the used surrogate model RN-18 in the submission is reported under \(50\%\) pruning ratio.

Figure A7: The source classes with top-10 scores selected by FM for the 8 downstream tasks studied in this work. For each class, the class label (name) as well as an representative image example is presented.