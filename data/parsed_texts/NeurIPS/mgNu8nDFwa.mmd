# Beyond Average Return in Markov Decision Processes

 Alexandre Marthe

UMPA

ENS de Lyon

Lyon, France

alexandre.marthe@ens-lyon.fr &Aurelien Garivier

UMPA UMR 5669 and LIP UMR 5668

Univ. Lyon, ENS de Lyon

46 allee d'Italie F-69364 Lyon cedex 07, France

aurelien.garivier@ens-lyon.fr &Claire Vernade

University of Tuebingen

Tuebingen, Germany

claire.vernade@uni-tuebingen.de

###### Abstract

What are the functionals of the reward that can be computed and optimized exactly in Markov Decision Processes? In the finite-horizon, undiscounted setting, Dynamic Programming (DP) can only handle these operations efficiently for certain classes of statistics. We summarize the characterization of these classes for policy evaluation, and give a new answer for the planning problem. Interestingly, we prove that only generalized means can be optimized exactly, even in the more general framework of Distributional Reinforcement Learning (DistRL). DistRL permits, however, to evaluate other functionals approximately. We provide error bounds on the resulting estimators, and discuss the potential of this approach as well as its limitations. These results contribute to advancing the theory of Markov Decision Processes by examining overall characteristics of the return, and particularly risk-conscious strategies.

## 1 Introduction

Reinforcement Learning (RL) has emerged as a flourishing field of study, delivering significant practical applications ranging from robot control and game solving to drug discovery or hardware design (Lazic et al., 2018; Popova et al., 2018; Volk et al., 2023; Mirhoseini et al., 2020). The cornerstone of RL is the "return" value, a sum of successive rewards. Conventionally, the focus is on computing and optimizing its expected value on Markov Decision Process (MDP). The remarkable efficiency of MDPs comes from their ability to be solved through dynamic programming with the Bellman equations (Sutton and Barto, 2018; Szepesvari, 2010). RL theory has seen considerable expansion, with a renewed interest for the consideration of more rich descriptions of a policy's behavior than the sole average return. At the other end of the spectrum, the so-called _Distributional Reinforcement Learning_ (DistRL) approach aims at studying and optimizing the entire return distribution, leading to impressive practical results (Bellemare et al., 2017; Hessel et al., 2018; Wurman et al., 2022; Fawzi et al., 2022). Between the expectation and the entire distribution, the efficient handling of other statistical functionals of the reward appears also particularly relevant for risk-sensitive contexts (Bernhard et al., 2019; Mowbray et al., 2022).

Despite recent progress, the full understanding of the abilities and limitations of DistRL to compute other functionals remains incomplete, with the underlying theory yet to be fully understood. Historically, the theory of RL has been established for discounted MDPs, see e.g. (Sutton and Barto, 2018; Watkins and Dayan, 1992; Szepesvari, 2010; Bellemare et al., 2023) for modern reference textbooks. Recently more attention was drawn to the undiscounted, finite-horizon setting (Auer, 2002,Osband et al., 2013; Jin et al., 2018; Ghavamzadeh et al., 2020), for which fundamental questions remain open. In this paper, we explore policy evaluation, planning and exact learning algorithms for undiscounted MDPs for the optimization problem of general functionals of the reward. We explicitly delimit the possibilities offered by dynamic programming as well as DistRL.

Our paper specifically addresses two questions:

1. How accurately can we evaluate statistical functionals by using DistRL?
2. Which functionals can be exactly optimized through dynamic programming?

We first recall the fundamental results in dynamic programming and Distributional RL. Addressing question (i), we refer to Rowland et al. (2019)'s results on Bellman closedness and provide their adaptation to undiscounted MDPs. We then prove upper bounds on the approximation error of Policy Evaluation with DistRL and corroborate these bounds with practical experiments. For question (ii), we draw a connection between Bellman closedness and planning. We then utilize the DistRL framework to identify two key properties held by _optimizable_ functionals.

Our main contribution is a characterization of the families of utilities that verify these two properties (Theorem 1). This result gives a comprehensive answer to question (ii) and closes an important open issue in the theory of MDP. It shows in particular that DistRL does not extend the class of functionals for which planing is possible beyond what is already allowed by classical dynamic programming.

## 2 Background

We introduce the classical RL framework in finite-horizon tabular Markov Decisions Processes (MDPs). We write \(\mathscr{P}(\mathbb{R})\) the space of probability distributions on \(\mathbb{R}\). A finite-horizon tabular MDP is a tuple \(\mathcal{M}=(\mathcal{X},\mathcal{A},p,R,H)\), where \(\mathcal{X}\) is a finite state space, \(\mathcal{A}\) is a finite action space, \(H\) is the horizon, for each \(h\in[H]\), \(p_{h}(x,a,\cdot)\) is a transition probability law and \(R_{h}(x,a)\) is a reward random variable with distribution \(\varrho_{h}\). The parameters \((p_{h})\) and \((R_{h})\) define the _model_ of dynamics. A deterministic policy on \(\mathcal{M}\) is a sequence \(\pi=(\pi_{1},\dots,\pi_{H})\) of functions \(\pi_{h}:\mathcal{X}\to\mathcal{A}\).

Reinforcement Learning traditionally focuses on learning policies optimizing the expected return. For a given policy \(\pi\), the \(Q\)-function maps a state-action pair to its expected return under \(\pi\):

\[Q_{h}^{\pi}(x,a)=\mathbb{E}_{\varrho_{h}}\left[R_{h}(x,a)\right]+\sum_{s^{ \prime}}p_{h}(x,a,x^{\prime})Q_{h+1}^{\pi}(x^{\prime},\pi_{h+1}(x^{\prime})), \qquad Q_{H+1}^{\pi}(x,a)=0\.\] (1)

When the model is known, the \(Q\)-function of a policy \(\pi\) can be computed by doing a backward recursion, also called dynamic programming. This is referred to as _Policy Evaluation_. Similarly, an optimal policy can be found by solving the optimal Bellman equation:

\[Q_{h}^{*}(x,a)=\mathbb{E}_{\varrho_{h}}\left[R_{h}(x,a)\right]+\sum_{x^{\prime }}p_{h}(x,a,x^{\prime})\max_{a^{\prime}}Q_{h+1}^{*}(x^{\prime},a^{\prime}), \qquad Q_{H+1}^{*}(x,a)=0\.\] (2)

Solving this equation when the model is known is also called _Planning_. When it is unknown, _reinforcement learning_ aims at finding the optimal policy from sample runs of the MDP. But evaluating and optimizing the _expectation_ of the return in the definition of the Q-function above is just one choice of statistical functional. We now introduce Distributional RL and then discuss other statistical functionals that generalize the expected setting discussed so far.

### Distributional RL

Distributional RL (DistRL) refers to the approach that tracks not just a statistic of the return for each state but its _entire distribution_. We introduce here the most important basic concepts and refer the reader to the recent comprehensive survey by Bellemare et al. (2023) for more details. The main idea is to use the full distributions to estimate and optimize various metrics over the returns ranging from the mere expectation (Bellemare et al., 2017) to more complex metrics (Rowland et al., 2019; Dabney et al., 2018; Liang and Luo, 2022).

At state-action \((x,a)\), let \(Z_{h}^{\pi}(x,a)\) denote the future sum of rewards when following policy \(\pi\) and starting at step \(h\), also called _return_. It verifies the simple recursive formula \(Z_{h}^{\pi}(x,a)=R_{h}(x,a)+Z_{h+1}^{\pi}(X^{\prime},\pi_{h+1}(X^{\prime}))\) where \(X^{\prime}\sim p_{h+1}(x,a,\cdot)\). Its distribution is \(\eta=(\eta_{h}^{\pi}(x,a))_{(x,a,h)\in\mathcal{X}\times\mathcal{A}\times[H]}\) and is often referred to as the _Q-value distribution_. One can easily derive the recursive law of the return as a convolution: for any two measures \(\nu_{1},\nu_{2}\in\mathscr{P}(\mathbb{R})\), we denote their convolution by \(\nu_{1}*\nu_{2}(t)=\int_{\mathbb{R}}\nu_{1}(\tau)\nu_{2}(t-\tau)d\tau\). For any two independent random variables \(X\) and \(Y\), the distribution of the sum \(Z=X+Y\) is the convolution of their distributions: \(\nu_{Z}=\nu_{X}*\nu_{Y}\). Thus, the law of \(Z_{h}^{\pi}(x,a)\) is

\[\forall x,a,h,\quad\eta_{h}^{\pi}(x,a)=\varrho_{h}(x,a)*\sum_{x^{\prime}}p_{h} (x,a,x^{\prime})\eta_{h}^{\pi}(x^{\prime},\pi_{h+1}(x^{\prime}))\.\] (3)

This equation is a distributional equivalent to Eq. (1) and thus defines a _distributional Bellman operator_\(\eta_{h}^{\pi}=\mathcal{T}_{h}^{\pi}\eta_{h+1}^{\pi}\).

Obviously, from a practical point of view, distributions form a non-parametric family that is not computationally tractable. It is necessary to choose a parametric (thus incomplete) family to represent them. Even the restriction to discrete reward distributions is not tractable, since the number of atoms in the distributions may grow exponentially with the number of steps1[Achab and Neu, 2021]: approximations are unavoidable. The most natural solution is to use projections of the obtained distribution on the parametric family, at each step of the Bellman operator. This process is called _parameterization_. The practical equivalent to Eq. (1) in DistRL hence writes

Footnote 1: The support of the return (sum of the rewards) is incremented at each step by a number of atoms that depend on the current support.

\[\forall x,a,h,\quad\hat{\eta}_{h}^{\pi}(x,a)=\Pi\left(\varrho_{h}(x,a)*\sum_{ x^{\prime}}p_{h}(x,a,x^{\prime})\hat{\eta}_{h+1}^{\pi}(x^{\prime},\pi_{h+1}(x^{ \prime}))\right)\,\] (4)

where \(\Pi\) is the projector operator on the parametric family. The full policy evaluation algorithm in DistRL is summarized in Alg.1.

``` Input: model \(p\), reward distributions \(\varrho_{h}\), policy \(\pi\) to evaluated, \(\Pi\) projection. Data:\(\eta\in\mathbb{R}^{H|\mathcal{X}||\mathcal{A}|N}\) \(\forall x,a\in\mathcal{X}\times\mathcal{A},\quad\eta_{H}(x,a)=\delta_{0}\) for\(h=H-1\to 0\)do \(\eta_{h}(x,a)=\varrho_{h}(x,a)*\sum_{x^{\prime}}p_{h}(x,a,x^{\prime})\eta_{h+1}( x^{\prime},\pi_{h+1}(x^{\prime}))\quad\forall x,a\in\mathcal{X}\times\mathcal{A}\) \(\eta_{h}(x,a)=\Pi\left(\eta_{h}(x,a)\right)\quad\forall x,a\in\mathcal{X}\times \mathcal{A}\) endfor Output:\(\eta_{h}(x,a)\forall x,a,h\) ```

**Algorithm 1** Policy Evaluation (Dynamic Programming) for Distributional RL

Distribution ParametrizationThe most commonly used parametrization is the so-called _quantile projection_. It put Diracs (atoms) with fixed weights at locations that correspond to the quantiles of the source distribution. One main benefit is that it does not require a previous knowledge of the support of the distribution, and allows for unbounded distributions.

The quantile projection is defined as

\[\Pi_{Q}\mu=\frac{1}{N}\sum_{i=0}^{N-1}\delta_{z_{i}},\quad\text{with}\ (z_{i})_{i}\ \text{chosen such as}\ F_{\mu}(z_{i})=\frac{2i+1}{2N}\,\] (5)

which corresponds to a minimal \(W_{1}\) distance: \(\Pi_{Q}\mu\in\text{arg min}_{\hat{\mu}=\sum_{i}\delta_{i}/N}\,W_{1}(\mu,\hat{ \mu})\), where \(W_{1}(.,.)\) is the Wasserstein distance defined for any distributions \(\nu_{1},\nu_{2}\) as \(W_{1}(\nu_{1},\nu_{2})=\int_{0}^{1}\left|F_{\nu_{1}}^{-1}(u)-F_{\nu_{2}}^{-1}( u)\right|\,\mathrm{d}u\). Note that this parametrization might admit several solutions and thus the projection may not be unique. For simplicity, we overload the notation to \(\Pi_{Q}\eta=(\Pi_{Q}\eta(x,a))_{(x,a)\in\mathcal{X}\times\mathcal{A}}\)For a Q-value distribution \(\eta\) with support of length \(\Delta_{\eta}\), and parametrization of resolution \(N\), Rowland et al. (2019) prove that the projection error is bounded by

\[\sup_{(x,a)\in\mathcal{X}\times\mathcal{A}}\;W_{1}(\Pi_{Q}\eta(x,a),\eta(x,a)) \leq\frac{\Delta_{\eta}}{2N}\;.\] (6)

In Section 3, we extend this result to the full iterative Policy Evaluation process and bound the error on the returned statistical functional in the finite-horizon setting. Note that other studied parametrizations exist but are less practical. For completeness, we discuss the Categorical Projection (Bellemare et al., 2017)(Rowland et al., 2018)(Bellemare et al., 2023) in Appendix B.

### Beyond expected returns

The expected value is an important functional of a probability distribution, but it is not the only one of interest in decision theory - especially when a control of the risk is important. We discuss two concepts that have received considerable attention: _utilities_, defined as expected values of functions of the return, and _distorted means_ which place emphasis on certain quantiles.

**Expected Utilities** are of the form \(\mathbb{E}[f(Z)]\), or \(\int f\;\mathrm{d}\nu\), where \(Z\) is the return of distribution \(\nu\) and \(f\) is an increasing function. For instance, when \(f\) is a power function, we obtain the different moments of the return. The case of exponential functions plays a particularly important role: the resulting utility is referred to as _exponential utility_, _exponential risk measure_, or _generalized mean_ according to the context:

\[U_{\mathrm{exp}}(\nu)=\frac{1}{\lambda}\log\mathbb{E}\big{[}\exp(\lambda X) \big{]}\quad X\sim\nu\text{ and }\lambda\in\mathbb{R}\;.\] (7)

This family of utilities has a variety of applications in finance, economics, and decision making under uncertainty (Follmer and Schied, 2016). They can be considered as a risk-aware generalization of the expectation, with benefits such as accommodating a wide range of behaviors (Shen et al., 2014) from risk-seeking when \(\lambda>0\), to risk-averse when \(\lambda<0\) (the limit \(\lambda\to 0\) is exactly the expectation). To fix ideas, \(U_{\mathrm{exp}}\big{(}\mathcal{N}(\mu,\sigma^{2})\big{)}=\mu+\lambda\sigma^{2}\): each \(\lambda\) captures a certain quantile of the Gaussian distribution.

**Distorted means**, on the other hand, involve taking the mean of a random variable, but with a different weighting scheme (Dabney et al., 2018). The goal is to place more emphasis on certain quantiles, which can be achieved by considering the quantile function \(F^{-1}\) of the random variable and a continuous increasing function \(\beta:[0,1]\to[0,1]\). By applying \(\beta\) to a uniform variable \(\tau\) on \([0,1]\) and evaluating \(F^{-1}\) at the resulting value \(\beta(\tau)\), we obtain a new random variable that takes the same values as the original variable, but with different probabilities. The distorted mean is then calculated as the mean of this new random variable, given by the formula \(\int\beta^{\prime}(\tau)F^{-1}(\tau)d\tau\). If \(\beta\) is the identity function, the result is the classical mean. When \(\beta\) is \(\tau\mapsto\min(\tau/\alpha,1)\), we get the \(\alpha\)-Conditional Value at Risk (CVaR\((\alpha)\)) of the return, a risk measure widely used in risk evaluation (Rockafellar et al., 2000).

## 3 Policy Evaluation

The theory of MDPs is particularly developed for estimating and optimizing the mean of the return of a policy. But other values associated to the return can be computed the same way, by dynamic programming. This includes for instance the variance of the return, or more generally, any moment of order \(p\geq 2\), as was already noticed in the 1980's (Sobel, 1982). Recently, Rowland et al. (2019) showed that for utilities in discounted MDPs, this is essentially all that can be done. More precisely, they introduce the notion of _Bellman closedness_ (recalled below for completeness) that characterizes a finite set of statistics that can efficiently be computed by dynamic programming.

**Definition 1** (Bellman closedness (Rowland et al., 2019)).: _A set of statistical functionals \(\{s_{1},\ldots s_{K}\}\) is said to be Bellman closed if for each \((x,a)\in\mathcal{X}\times\mathcal{A}\), the statistics \(s_{1:K}(\eta_{n}^{\pi}(x,a))\) can be expressed in closed form in terms of the random variables \(R_{h}(x,a)\) and \((s_{1:K}(\eta_{h+1}^{\pi}(X^{\prime},A^{\prime})),\;A^{\prime}\sim\pi(x),\;X ^{\prime}\sim p_{h}(x,A^{\prime},\cdot)\), independently of the MDP._

Importantly, in the undiscounted setting, Rowland et al. (2019)(Appendix B, Theorem 4.3) show that the only families of utilities that are Bellman closed are of the form \(\{x\mapsto x^{\ell}\exp(\lambda x)|0\leq\ell\leq L\}\)for some \(L<\infty\). Thus, all utilities and statistics of the form of (or linear combinations of) moments and exponential utilities can easily be computed by classic linear dynamic programming and do not require distributional RL (see Appendix A.3).

Some important metrics such as the CVaR or the quantiles are not known to belong to any Bellman-closed set and hence cannot be easily computed. For this kind of function of the return, the knowledge of the transitions and the values in following steps is insufficient to compute the value on a specific step. In general, it requires the knowledge of the whole distribution of each reward in each state. Hence, techniques developed in distributional RL come in handy: for a choice of parametrization, one can use the projected dynamic programming step Eq. (4) to propagate a finite set of values along the MDP and approximate the distribution of the return. In the episodic setting, following the line of Rowland et al. (2019) (see Eq.(6)), we prove that the Wasserstein distance error between the exact and approximate distribution of the Q-values of a policy is bounded.

**Proposition 1**.: _Let \(\pi\) be a policy and \(\eta^{\pi}\) the associated Q-value distributions. Assume the return is bounded on a interval of length \(\Delta_{\eta}\leq H\Delta_{R}\), where \(\Delta_{R}\) is the support size of the reward distribution. Let \(\tilde{\eta}^{\pi}\) be the Q-value distributions obtained by dynamic programming (Algorithm 1) using the quantile projection \(\Pi_{Q}\) with resolution \(N\). Then,_

\[\sup_{(x,a,h)\in(\mathcal{X},\mathcal{A},[H])}W_{1}(\hat{\eta}^{\pi}_{h}(x,a),\eta^{\pi}_{h}(x,a))\leq H\frac{\Delta_{\eta}}{2N}\leq H^{2}\frac{\Delta_{R }}{2N}\;.\]

This result shows that the loss of information due to the parametrization may only grow quadraticly with the horizon. The proof consists of summing the projection bound in (6) at each projection step, and using the non-expansion property of the Bellman operator (Bellemare et al., 2017). The details can be found in Appendix C

The key question is then to understand how such error translates into our estimation problem when we apply the function of interest to the approximate distribution. We provide a first bound on this error for the family of statistics that are either utilities or distorted means.

First, we prove that the utility is Lipschitz on the set of return distributions.

**Lemma 1**.: _Let \(s\) be either an utility or a distorted mean and let \(L\) be the Lipschitz coefficient of its characteristic function. Let \(\nu_{1},\nu_{2}\) be return distributions. Then:_

\[|s(\nu_{1})-s(\nu_{2})|\leq LW_{1}(\nu_{1},\nu_{2})\;.\]

Both family of functionals are treated separately, but lays a similar bound. The utility bound is the direct application of the Kantorovitch-Rubenstein duality, while the distorted mean one is a direct majoration in the integral. Again, the details are provided in the Appendix.

This property allows us to prove a maximal upper bound on the estimation error for those two families.

**Theorem 1**.: _Let \(\pi\) be a policy. Let \(\eta^{\pi}\) be the Q-value return distribution associated to \(\pi\) with the return bounded on a interval of length \(\Delta_{\eta}\leq H\Delta_{R}\) where \(\Delta_{R}\) is the support size of the reward distribution. Let \(\hat{\eta}^{\pi}\) be the approximated return distribution computed with Algorithm 1, for the projection \(\Pi_{Q}\) with resolution \(N\). Let \(s\) be either an expected utility or a distorted mean, and \(L\) the Lipschitz coefficient of its characteristic function. Then:_

\[\sup_{x,a,h}|s(\hat{\eta}^{\pi}_{h}(x,a))-s(\hat{\eta}^{\pi}_{h}(x,a))|\leq LH \frac{\Delta_{\eta}}{2N}\leq LH^{2}\frac{\Delta_{R}}{2N}\;.\]

Note that depending on the choice of utilities, the Lipschitz coefficient \(L\) may also depend on \(H\) and \(\Delta R\). For instance, in a stationary MDP, the Lipschitz constant of the exponential utility depends exponentially on \(\Delta_{\eta}\). For the CVaR\((\alpha)\), however, \(L\) is constant and only depends on \(\alpha\in(0,1)\).

Experiment: empirical validation of the bounds on a simple MDPWe consider a simple Chain MDP environment of length \(H=70\) equal to the horizon (see Figure 1 (right)) (Rowland et al., 2019), with a single action leading to the same discrete reward distribution for every step. We consider a Bernouilli reward distribution \(\mathcal{B}(0.5)\) for each state so that the number of atoms for the return only grows linearly2 with the number of steps, which allows to compute the exact distribution easily.

Footnote 2: At round \(h\in[H]\), the support of the return is \(\{0,1,...,h\}\), so \(h\) atoms.

We compare the distributions obtained with exact dynamic programming and the approximate distribution obtained by Alg 1, with a quantile projection with resolution \(N=1000\). Note that even at early stages, when the true distribution has less atoms than the resolution, the exact and approximate distributions differ due to the weights of the atoms in the quantile projection. Figure 2 (Right) reports the Wasserstein distance between the two distributions: the cumulative projection approximation error (dashed blue), the true error between the current exact and approximate distributions (solid blue) and the theoretical bound (red). Fundamentally, the proof of Prop. 1 upper bounds the distance between distributions by the cumulative projection error so we plot this quantity to help validating it.

We also empirically validate Theorem 1 by computing the CVaR(\(\alpha\)) for \(\alpha\in\{0.1,0.25\},\) corresponding respectively to distorted means with Lipschitz constants \(L=\{10,4\}.\) We compute these statistics for both distributions and report the maximal error together with the theoretical bound, re-scaled3 by a factor 5. Figure 2 (Left) shows an impressive correspondence of the theory and the empirical results despite a constant multiplicative gap.

Footnote 3: Scaling by a constant factor allows us to show the corresponding quadratic trends.

## 4 Planning

Planning refers to the problem of returning a policy that optimizes our objective for a given model and reward function (or distribution in DistRL). It shares with policy evaluation the property to be grounded on a Bellman equation: see Eq. (2) for the classical expected return, which leads to efficient computations by dynamic programming.

For other statistical functionals of the cumulated reward, however, can the optimal policy be computed efficiently? The main result of this section characterizes the family of functionals that can be exactly and efficiently optimized by Dynamic Programming.

In the previous section, we recalled that Bellman-closed sets of utilities can be efficiently computed by DP as long as all the values of the utilities in the Bellman-closed family are computed together. For the planning problem, however, we only want to optimize one utility so we cannot consider families as previously. Under this constraint, only exponential and linear expected utilities are Bellman closed and thus can verify a Bellman Equation. In fact, for the exponential utilities, such Bellman Equation

Figure 1: A Chain MDP of length \(H\) with deterministic transition and identical reward distribution for each state.

Figure 2: Left: Validation of Theorem 1 on CVaR(\(\alpha\)) together with the scaled upper bound (see main text for discussion): the quadratic dependence in \(H\) is verified. Right: Validation of Proposition 1: The cumulative projection error (dashed blue) is the sum of the projection errors at every time step, and matches the true approximation error (solid blue). The theoretical upper bound (dashed red) differs only by a factor 2.

exists and allows for the planning problem to be solved efficiently [Howard and Matheson, 1972]:

\[Q_{h}^{\lambda}(x,a)=U_{\exp}^{\lambda}(R_{h}(x,a))+\frac{1}{ \lambda}\log\left[\sum_{x^{\prime}}p_{h}(x,a,x^{\prime})\exp\left(\lambda\max_{a ^{\prime}}Q_{h+1}^{\lambda}(x^{\prime},a^{\prime})\right)\right]\] (8) \[\text{with }Q_{H+1}^{\lambda}(x,a)=0\.\]

However, the question of the existence of Optimal Bellman Equations for non-utility functionals remains open (e.g. quantiles). More generally, an efficient planning strategy is not known. To address these questions, we consider the most general framework, DistRL, and recall the theoretical DP equations for any statistical functional \(s\) in the Pseudo4-Algorithm 2. DistRL offers the most comprehensive, or 'lossless', approach, so if a statistical functional cannot be optimized with Alg. 2, then there cannot exist a Bellman Operator to perform exact planning.

Footnote 4: This theoretical algorithm handles the full distribution of the return at each step, which cannot be done in practice.

```
1:Input: model \(p\), reward \(R\), statistical functional \(s\)
2:Data:\(\eta\in\mathbb{R}^{H|\mathcal{X}||\mathcal{A}|\mathcal{N}},\nu\in\mathbb{R}^{ H|\mathcal{X}|N}\)
3:\(\forall x\in\mathcal{X},\quad\nu_{H+1}^{x}=\delta_{0}\)
4:for\(h=H\to 1\)do
5:\(\eta_{h}(x,a)=\varrho_{h}^{(x,a)}*\sum_{x^{\prime}}p_{h}^{a}(x,x^{\prime}) \nu_{h+1}^{x^{\prime}}\quad\forall x,a\in\mathcal{X}\times\mathcal{A}\)
6:\(\nu_{h}^{x}=\eta_{h}(x,a^{*})\,\quad a^{*}\in\text{arg max}_{a}s(\eta_{h}(x,a)) \quad\forall x\in\mathcal{X}\)
7:endfor
8:Output:\(\eta_{h}(x,a)\ \forall x,a,h\) ```

**Algorithm 2** Pseudo-Algorithm: Exact Planning with Distributional RL

We formalize this idea with the new concept of _Bellman Optimizable_ statistical functionals:

**Definition 2** (Bellman Optimizable statistical functional).: _A statistical functional \(s\) is called Bellman optimizable if, for any MDP \(\mathcal{M}\), the Pseudo-Algorithm 2 outputs an optimal return distribution \(\eta=\eta^{*}\) that verifies:_

\[\forall x,a,h,\quad s(\eta_{h}^{*}(x,a))=\sup_{\pi}s(\eta_{h}^{ \pi}(x,a))\.\] (9)

**Remark**.: _This definition is equivalent to the satisfaction of an Optimal Distributionnal Bellman equation. Indeed, a statistical functional \(s\) is Bellman Optimizable if and only if, for any \((\mathcal{X},\mathcal{A},\varrho,p,\eta)\), \(s\) verifies_

\[\sup_{a_{x^{\prime}}}s\left(\varrho*\sum_{x^{\prime}}p(x^{\prime} )\eta(x^{\prime},a_{x^{\prime}})\right)=s\left(\varrho*\sum_{x^{\prime}}p(x^{ \prime})\eta(x^{\prime},a_{x^{\prime}}^{*})\right)\]

_with \(a_{x}^{*}\in\arg\max_{a}s(\eta(x,a))\)_

We can now state our main results that characterizes all the _Bellman optimizable_ statistical functionals. First, we prove that such a functional must satisfy two important properties.

**Lemma 2**.: _A Bellman optimizable functional \(s\) satifies the two following properties:_

* _Independence Property: If_ \(\nu_{1},\nu_{2}\in\mathscr{P}(\mathbb{R})\) _are such that_ \(s(\nu_{1})\geq s(\nu_{2})\)_, then_ \[\forall\nu_{3}\in\mathscr{P}(\mathbb{R}),\forall\lambda\in[0,1],\quad s( \lambda\nu_{1}+(1-\lambda)\nu_{3})\geq s(\lambda\nu_{2}+(1-\lambda)\nu_{3}))\.\]
* _Translation Property: Let_ \(\tau_{c}\) _denote the translation on the set of distributions:_ \(\tau_{c}\delta_{x}=\delta_{x+c}\)_. If_ \(\nu_{1},\nu_{2}\in\mathscr{P}(\mathbb{R})\) _are such that_ \(s(\nu_{1})\geq s(\nu_{2})\)_, then_ \[\forall c\in\mathbb{R},\quad s(\tau_{c}\nu_{1})\geq s(\tau_{c} \nu_{2})\.\]

Indeed, the expectation and the exponential utility both satisfy these properties (see Appendix A.2). Each property is implied by an aspect of the Distributional Bellman Equation (Alg. 2, line 5) and the proof (in Appendix E) unveils these important consequences of the recursion identities.

Fundamentally, they follow from the Markovian nature of policies optimized this way: the choice of the action in each state should be independent of other states and rely only on the knowledge of the next-state value distribution.

The Independence property states that, for Bellman optimizable functionals, the value of each next state should not depend on that of any other value in the convex combination in the rightmost term of the convolution. In turn, the Translation property is associated to the leftmost term, the reward, and it imposes that, for Bellman optimizable functionals, the decision on the best action is independent of the previously accumulated reward.

The Independence property is related to expected utilities (von Neumann et al., 1944). Any expected utility verifies this property (Appendix A.2) but, most importantly, the Expected Utility Theorem (also known as the Von Neumann Morgenstein theorem) implies that any continuous statistical functional \(s\) verifying the Independence property can be reduced to an expected utility. This means that for any such statistical functional \(s\), there exists \(f\) continuous such that \(\forall\nu_{1},\nu_{2}\in\mathscr{P}(\mathbb{R})\), we have \(s(\nu_{1})>s(\nu_{2})\Longleftrightarrow U_{f}(\nu_{1})>U_{f}(\nu_{2})\)(von Neumann et al., 1944, Grandmont, 1972).

This result directly narrows down the family of Bellman optimizable functionals to utilities. Indeed, although other functionals might potentially be optimized using the Bellman equation, addressing the problem on utilities is adequate to characterize all possible behaviors. For instance, moment-optimal policies that can be found through dynamic programming, can also be found by optimizing an exponential utility function. The next task is therefore to identify all the utilities that satisfy the second property. We demonstrate that, apart from the mean and exponential utilities, no other \(W_{1}\)-continuous functional satisfies this property.

**Theorem 2**.: _Let \(\varrho\) be a return distribution. The only \(W_{1}\)-continuous Bellman Optimizable statistical functionals of the cumulated return are exponential utilities \(U_{\exp}(\varrho)=\frac{1}{\lambda}\log\mathbb{E}_{\varrho}\left[\exp(\lambda R )\right]\) for \(\lambda\in\mathbb{R}\), with the special case of the expectation \(\mathbb{E}_{\varrho}\left[R\right]\) when \(\lambda=0\)._

Of course, if \(U(\varrho)\) is a utility and \(\psi\) is a monotonous scalar mapping, \(\psi(U(\varrho))\) is an equivalent utility: one should understand in the previous theorem that a _\(W_{1}\)-continuous_ Bellman Optimizable statistical functional is equivalent to \(U_{\exp}(\varrho)\) for some \(\lambda\in\mathbb{R}\). We chose here to define \(U_{\exp}(\varrho)=\frac{1}{\lambda}\log\mathbb{E}_{\varrho}\left[\exp(\lambda R )\right]\) with the \(\log\) and the factor \(1/\lambda\) since it results in a normalized utility that tends to the expectation when \(\lambda\) goes to \(0\). The full proof of Theorem 2 is provided in Appendix E.

We make a few important observations. First, this result shows that algorithms using Bellman updates to optimize any continuous functionals other than the exponential utility cannot guarantee optimality. The theorem does not apply to non-continuous functionals, but Lemma 2 still does. For instance, the quantiles are not \(W_{1}\)-continuous so Theorem 2 does not apply, but it is easy to prove that they do not verify the Independence Property and thus are not Bellman Optimizable. Also, there might also exist other optimizable functionals, like moments, but they must first be reduced to exponential or linear utilities.

Most importantly, while in theory, DistRL provides the most general framework for optimizing policies via dynamic programming, our result shows that in fact, the only utilities that can be exactly and efficiently optimized do not require to resort to DistRL. This certainly does not question the very purpose of DistRL, which has been shown to play important roles in practice to regularize or stabilize policies and to perform deeper exploration (Bellemare et al., 2017; Hessel et al., 2018). Some advantages of learning the distribution lies in the enhanced _robustness_ offered in the richer information learned (Rowland et al., 2023), particularly when utilizing neural networks for function approximation (Dabney et al., 2018; Barth-Maron et al., 2018; Lyle et al., 2019).

## 5 Q-Learning Exponential Utilities

The previous sections consider the the model, i.e. the reward and transition functions, be known. Yet in most practical situations, those are either approximated or learnt5. After addressing policy evaluation (Section 3) and planning (Section 4), we conclude here the argument of this paper by addressing the question of learning the statistical functionals of Theorem 2. In fact, we simply highlight a lesser known version of Q-Learning Watkins and Dayan (1992) that extend this celebrated algorithm to exponential utilities. We provide the pseudo-code for the algorithm proposed by Borkar[2002, 2010] with the relevant utility-based updates in Alg. 3. We refer to these seminal works for convergence proofs. Linear utility updates (line 8) differ only slightly from classical ones for expected return optimization, which have been shown to lead to the optimal value asymptotically [Watkins and Dayan, 1992].

```
1:Input:\((\alpha_{t})_{t\in\mathbb{N}}\), transition and reward generator. \(Q_{h}(x,a)\gets H\),\(\forall(x,a,h)\in\mathcal{X}\times\mathcal{A}\times[H]\)
2:Utilities: Linear (\(Z\mapsto\lambda Z+b\)) or Exponential (\(Z\mapsto\log(\mathbb{E}\exp(\lambda Z))/\lambda\))
3:for episode \(K=1,\ldots,K\)do
4: Observe \(x_{1}\in\mathcal{X}\)
5:for step \(h=1,\ldots,H\)do
6: Choose action \(a_{h}\in\text{arg max}_{a\in\mathcal{A}}\,Q_{h}(x_{h},a)\)
7: Observe reward \(r_{h}\) and transition \(x_{h+1}\) and update for chosen objective:
8: Linear Util.: \(Q_{h}(x_{h},a_{h})\leftarrow(1-\alpha_{k})Q_{h}(x_{h},a_{h})+\alpha_{k}[ \lambda(r_{h}+\text{max}_{a^{\prime}}\,Q_{h+1}(x_{h+1},a^{\prime}))+b]\)
9: Exponential Util.: \(Q_{h}(x_{h},a_{h})\leftarrow\frac{1}{\lambda}\log\left[(1-\alpha_{k})e^{ \lambda\cdot Q_{h}(x_{h},a_{h})}+\alpha_{k}e^{\lambda\left\lvert r_{h}+\text{ max}_{a^{\prime}}\,Q_{h}(x_{h+1},a^{\prime})\right\rvert}\right]\)
10:endfor
11:endfor
12:Output:\(Q_{h}(x,a)\forall x,a\) ```

**Algorithm 3** Q-Learning for Linear and Exponential Utilities

## 6 Discussions and Related Work

The Discounted FrameworkWe focused in this article on undiscounted MDPs, and it is important to note that the results differ for discounted scenarios. The crucial difference is that the family of exponential utilities no longer retains Bellman Closed or Bellman Optimizable properties due to the introduction of the discount factor \(\gamma\)[Rowland et al., 2019]. When it comes to Bellman Optimization, the necessary translation property becomes an affine property : \(\forall c,\gamma,\ s(\tau_{c}^{\gamma}\nu_{1})>s(\tau_{c}^{\gamma}\nu_{2})\) where \(\tau_{c}^{\gamma}\) is the affine operator such that \(\tau_{c}^{\gamma}\delta_{x}=\delta_{\gamma x+c}\). This property is not upheld by the exponential utility. Nonetheless, there exists a method to optimize the exponential utility through dynamic programming in discounted MDPs [Chung and Sobel, 1987]. This approach requires modifying the functional to optimize at each step (the step \(h\) is optimized with the utility \(x\mapsto\exp(\gamma^{-h}\lambda\ x)\)), but it also implies a loss of policy stationarity, property usually obtained in dynamic programming for discounted finite-horizon MDPs [Sutton and Barto, 2018].

Utilizing functionals to optimize expected return.DistRL has also been used in Deep Reinforcement learning to optimize non-Bellman-optimizable functionals such as distorted means[Ma et al., 2020, Dabney et al., 2018a]. While, as we proved so, such algorithms cannot lead to optimal policies in terms of these functionals, experiments show that in some contexts they can lead to better expected return and faster convergence in practice. The change of functional can be interpreted as a change in the exploration process, and the resulting risk-sensitive behaviors seem to be relevant in adequate environments.

Dynamic programming for the optimization of other functionalsTo optimize other statistical functionals such as CVaR and other utilities such as moments with Dynamic Programming, Bauerle and Ott [2011] and Bauerle and Rieder [2014] propose to extend the state space of the original MDP to \(\mathcal{X}^{\prime}=\mathcal{X}\times\mathbb{R}\) by theoretically adding a continuous dimension to store the current cumulative rewards. This idea does not contradict our results, and the resulting algorithms remain empirically much more expensive.

Another recent thread of ideas to optimize functionals of the reward revolve around the dual formulation of RL through the empirical state distribution [Hazan et al., 2019]. Algorithms can be derived by noticing that utilities like the CVar are equivalent to solving a convex RL problem [Mutti et al., 2023].

## 7 Conclusion

Our work closes an important open problem in the theory of MDPs: we exactly characterize the families of statistical functionals that can be evaluated and optimized by dynamic programming.

We also put into perspective the DistRL framework: the only functionals of the return that can be optimized with DistRL can actually be handled exactly by dynamic programming. Its benefit lies elsewhere, and notably in the improved stability of behavioral properties it allows. We believe that, by narrowing down the avenues to explain its empirical successes, our work can contribute to clarify the further research to conduct on the theory of DistRL.

## Acknowledgements

Alexandre Marthe and Aurelien Garivier acknowledge the support of the Project IDEXLYON of the University of Lyon, in the framework of the Programme Investissements d'Avenir (ANR-16-IDEX-0005), Chaire SeqALO (ANR-20-CHIA-0020-01), and Project FOUNDRY in PEPR-IA.

Claire Vernade is funded by the Deutsche Forschungsgemeinschaft (DFG) under both the project 468806714 of the Emmy Noether Programme and under Germany's Excellence Strategy - EXC number 2064/1 - Project number 390727645. Claire Vernade also thanks the international Max Planck Research School for Intelligent Systems (IMPRS-IS) and Seek.AI for their support.

## References

* Achab and Neu (2021) M. Achab and G. Neu. Robustness and risk management via distributional dynamic programming, Dec. 2021. arXiv:2112.15430 [cs, math].
* Auer (2002) P. Auer. Using confidence bounds for exploitation-exploration trade-offs. _Journal of Machine Learning Research_, 3(Nov):397-422, 2002.
* Barth-Maron et al. (2018) G. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan, D. Tb, A. Muldal, N. Heess, and T. Lillicrap. Distributed distributional deterministic policy gradients. _arXiv preprint arXiv:1804.08617_, 2018.
* Bellemare et al. (2017) M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning. In _International conference on machine learning_, pages 449-458. PMLR, 2017.
* Bellemare et al. (2023) M. G. Bellemare, W. Dabney, and M. Rowland. _Distributional Reinforcement Learning_. MIT Press, 2023. http://www.distributional-rl.org.
* Bernhard et al. (2019) J. Bernhard, S. Pollok, and A. Knoll. Addressing inherent uncertainty: Risk-sensitive behavior generation for automated driving using distributional reinforcement learning. In _2019 IEEE Intelligent Vehicles Symposium (IV)_, pages 2148-2155, 2019. doi: 10.1109/IVS.2019.8813791.
* Borkar (2002) V. S. Borkar. Q-learning for risk-sensitive control. _Mathematics of operations research_, 27(2):294-311, 2002.
* Borkar (2010) V. S. Borkar. Learning algorithms for risk-sensitive control. In _Proceedings of the 19th International Symposium on Mathematical Theory of Networks and Systems-MTNS_, 2010.
* Bauerle and Ott (2011) N. Bauerle and J. Ott. Markov Decision Processes with Average-Value-at-Risk criteria. _Mathematical Methods of Operations Research_, 74(3):361-379, Dec. 2011. ISSN 1432-2994, 1432-5217. doi: 10.1007/s00186-011-0367-0.
* Bauerle and Rieder (2014) N. Bauerle and U. Rieder. More Risk-Sensitive Markov Decision Processes. _Mathematics of Operations Research_, 39(1):105-120, Feb. 2014. ISSN 0364-765X. doi: 10.1287/moor.2013.0601. Publisher: INFORMS.
* Chung and Sobel (1987) K.-J. Chung and M. J. Sobel. Discounted MDP's: Distribution Functions and Exponential Utility Maximization. _SIAM Journal on Control and Optimization_, 25(1):49-62, Jan. 1987. ISSN 0363-0129, 1095-7138. doi: 10.1137/0325004.
* Dabney et al. (2018a) W. Dabney, G. Ostrovski, D. Silver, and R. Munos. Implicit Quantile Networks for Distributional Reinforcement Learning. In _Proceedings of the 35th International Conference on Machine Learning_, pages 1096-1105. PMLR, July 2018a. ISSN: 2640-3498.
* Dabney et al. (2018b) W. Dabney, M. Rowland, M. Bellemare, and R. Munos. Distributional Reinforcement Learning With Quantile Regression. _Proceedings of the AAAI Conference on Artificial Intelligence_, 32(1), Apr. 2018b. ISSN 2374-3468. Number: 1.
* D'Auria et al. (2019)A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain, A. Novikov, F. J. R. Ruiz, J. Schrittwieser, G. Swirszcz, D. Silver, D. Hassabis, and P. Kohli. Discovering faster matrix multiplication algorithms with reinforcement learning. _Nature_, 610(7930):47-53, Oct. 2022. ISSN 1476-4687. doi: 10.1038/s41586-022-05172-4. Number: 7930 Publisher: Nature Publishing Group.
* Follmer and Schied (2016) H. Follmer and A. Schied. _Stochastic Finance: An Introduction in Discrete Time_. De Gruyter, Berlin, Boston, 2016. ISBN 9783110463453. doi: doi: 10.1515/9783110463453.
* Ghavamzadeh et al. (2020) M. Ghavamzadeh, A. Lazaric, and M. Pirotta. Exploration in reinforcement learning. Tutorial at AAAI'20, 2020. URL https://rlgammazero.github.io/.
* Grandmont (1972) J.-M. Grandmont. Continuity properties of a von neumann-morgenstern utility. _Journal of Economic Theory_, 4(1):45-57, 1972. ISSN 0022-0531. doi: https://doi.org/10.1016/0022-0531(72)90161-5.
* Hazan et al. (2019) E. Hazan, S. Kakade, K. Singh, and A. Van Soest. Provably efficient maximum entropy exploration. In K. Chaudhuri and R. Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 2681-2691. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/hazan19a.html.
* Hessel et al. (2018) M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver. Rainbow: Combining Improvements in Deep Reinforcement Learning. _Proceedings of the AAAI Conference on Artificial Intelligence_, 32(1), Apr. 2018. doi: 10.1609/aaai.v32i1.11796.
* Howard and Matheson (1972) R. A. Howard and J. E. Matheson. Risk-Sensitive Markov Decision Processes. _Management Science_, 18(7):356-369, Mar. 1972.
* Jin et al. (2018) C. Jin, Z. Allen-Zhu, S. Bubeck, and M. I. Jordan. Is q-learning provably efficient? _Advances in neural information processing systems_, 31, 2018.
* Lazic et al. (2018) N. Lazic, C. Boutilier, T. Lu, E. Wong, B. Roy, M. Ryu, and G. Imwalle. Data center cooling using model-predictive control. _Advances in Neural Information Processing Systems_, 31, 2018.
* Liang and Luo (2022) H. Liang and Z.-Q. Luo. Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds, Oct. 2022. arXiv:2210.14051 [cs, stat] version: 1.
* Lyle et al. (2019) C. Lyle, M. G. Bellemare, and P. S. Castro. A Comparative Analysis of Expected and Distributional Reinforcement Learning. _Proceedings of the AAAI Conference on Artificial Intelligence_, 33(01):4504-4511, July 2019. ISSN 2374-3468. doi: 10.1609/aaai.v33i01.33014504. Number: 01.
* Ma et al. (2020) X. Ma, L. Xia, Z. Zhou, J. Yang, and Q. Zhao. DSAC: Distributional Soft Actor Critic for Risk-Sensitive Reinforcement Learning, June 2020. arXiv:2004.14547 [cs].
* Mirhoseini et al. (2020) A. Mirhoseini, A. Goldie, M. Yazgan, J. Jiang, E. Songhori, S. Wang, Y.-J. Lee, E. Johnson, O. Pathak, S. Bae, et al. Chip placement with deep reinforcement learning. _arXiv preprint arXiv:2004.10746_, 2020.
* Mowbray et al. (2022) M. Mowbray, D. Zhang, and E. A. D. R. Chanona. Distributional reinforcement learning for scheduling of chemical production processes, 2022.
* Mutti et al. (2023) M. Mutti, R. De Santi, P. De Bartolomeis, and M. Restelli. Convex reinforcement learning in finite trials. _Journal of Machine Learning Research_, 24(250):1-42, 2023.
* Osband et al. (2013) I. Osband, D. Russo, and B. Van Roy. (more) efficient reinforcement learning via posterior sampling. _Advances in Neural Information Processing Systems_, 26, 2013.
* Popova et al. (2018) M. Popova, O. Isayev, and A. Tropsha. Deep reinforcement learning for de novo drug design. _Science advances_, 4(7):eaap7885, 2018.
* Rockafellar et al. (2000) R. T. Rockafellar, S. Uryasev, et al. Optimization of conditional value-at-risk. _Journal of risk_, 2:21-42, 2000.
* Ritter et al. (2018)M. Rowland, M. Bellemare, W. Dabney, R. Munos, and Y. W. Teh. An Analysis of Categorical Distributional Reinforcement Learning. In _Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics_, pages 29-37. PMLR, Mar. 2018. ISSN: 2640-3498.
* Rowland et al. (2019) M. Rowland, R. Dadashi, S. Kumar, R. Munos, M. G. Bellemare, and W. Dabney. Statistics and Samples in Distributional Reinforcement Learning. In _Proceedings of the 36th International Conference on Machine Learning_, pages 5528-5536. PMLR, May 2019. ISSN: 2640-3498.
* Rowland et al. (2023) M. Rowland, Y. Tang, C. Lyle, R. Munos, M. G. Bellemare, and W. Dabney. The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation. 2023.
* Shen et al. (2014) Y. Shen, M. J. Tobia, T. Sommer, and K. Obermayer. Risk-sensitive reinforcement learning. _Neural Computation_, 26(7):1298-1328, 2014. doi: 10.1162/NECO_a_00600.
* Sobel (1982) M. J. Sobel. The variance of discounted Markov decision processes. _Journal of Applied Probability_, 19(4):794-802, Dec. 1982. ISSN 0021-9002, 1475-6072. doi: 10.2307/3213832.
* Sutton and Barto (2018) R. S. Sutton and A. G. Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Szepesvari (2010) C. Szepesvari. Algorithms for Reinforcement Learning. _Synthesis Lectures on Artificial Intelligence and Machine Learning_, 4(1):1-103, Jan. 2010. ISSN 1939-4608, 1939-4616. doi: 10.2200/S00268ED1V01Y201005AIM009.
* Villani (2003) C. Villani. _Topics in Optimal Transportation_. Graduate studies in mathematics. American Mathematical Society, 2003. ISBN 9780821833124. URL https://books.google.fr/books?id=idyFAwAAQBAJ.
* Volk et al. (2023) A. A. Volk, R. W. Epps, D. T. Yonemoto, B. S. Masters, F. N. Castellano, K. G. Reyes, and M. Abolhasani. Alphaflow: autonomous discovery and optimization of multi-step chemistry using a self-driven fluidic lab guided by reinforcement learning. _Nature Communications_, 14(1):1403, 2023.
* von Neumann et al. (1944) J. von Neumann, O. Morgenstern, and A. Rubinstein. _Theory of Games and Economic Behavior (60th Anniversary Commemorative Edition)_. Princeton University Press, 1944. ISBN 978-0-691-13061-3.
* Watkins and Dayan (1992) C. J. Watkins and P. Dayan. Q-learning. _Machine learning_, 8:279-292, 1992.
* Wurman et al. (2022) P. R. Wurman, S. Barrett, K. Kawamoto, J. MacGlashan, K. Subramanian, T. J. Walsh, R. Capobianco, A. Devlic, F. Eckert, F. Fuchs, L. Gilpin, P. Khandelwal, V. Kompella, H. Lin, P. MacAlpine, D. Oller, T. Seno, C. Sherstan, M. D. Thomure, H. Aghabozorgi, L. Barrett, R. Douglas, D. Whitehead, P. Durr, P. Stone, M. Spranger, and H. Kitano. Outracing champion Gran Turismo drivers with deep reinforcement learning. _Nature_, 602(7896):223-228, Feb. 2022. ISSN 1476-4687. doi: 10.1038/s41586-021-04357-7. Number: 7896 Publisher: Nature Publishing Group.

## Appendix A Additional remarks

The Wasserstein metric is defined as \(W_{1}(\nu_{1},\nu_{2})=\int_{0}^{1}\left|F_{\nu_{1}}^{-1}(u)-F_{\nu_{2}}^{-1}(u) \right|\ \mathrm{d}u\) and the Cramer metric as \(\ell_{2}(\nu_{1},\nu_{2})=\left(\int_{-\infty}^{+\infty}\left|F_{\nu_{1}}(u)-F _{\nu_{2}}(u)\right|^{2}\ \mathrm{d}u\right)^{\frac{1}{2}}\). For both metrics, we define their supremum \(\overline{\ell_{2}}(\eta_{1},\eta_{2})=\sup_{(x,a)\in\mathcal{X}\times \mathcal{A}}\ell_{2}(\eta_{1}(x,a),\eta_{2}(x,a))\) and \(\overline{W}_{1}(\eta_{1},\eta_{2})=\sup_{(x,a)\in\mathcal{X}\times\mathcal{A }}W_{1}(\eta_{1}(x,a),\eta_{2}(x,a))\).

### Remarks on the recursive definition of the Q-value distribution

The notation of the Q-value distribution \(\eta\) is often deceivingly complex compared to the actual object it means to represent. While the 'usual' expected Q-function \(Q(x,a)\) is simply understood as the expected return of a policy at state-action pair \((x,a)\), DistRL requires us to keep a notation for the complete distribution of the return. In other words, the Q-value distribution \(\eta_{\pi,h}^{(x,a)}\) should be understood as the distribution of the _random variable_\(Z=R+Z(S^{\prime})\), which is the convolution of the individual distributions of these two independent random variable. It can also be written:

\[\forall x,a,h,\quad\eta_{h}^{\pi}(x,a)=\sum_{x^{\prime},a^{\prime}}\int p_{h} (x,a,x^{\prime})\pi_{h+1}^{x^{\prime}}(a^{\prime})\eta_{h+1}^{x}(x^{\prime},a^ {\prime})(\cdot-r)R_{h}^{(x,a)}(dr)\.\] (10)

### Linear and Exponential Utilities satisfy the properties of Lemma 2

Independence PropertyAny utility \(U_{f}\) verifies the independence property. Let \(\nu_{1},\nu_{2},\nu_{3}\in\mathscr{P}(\mathbb{R}),\ \lambda\in[0,1]\). Assume \(U_{f}(\nu_{1})\geq U_{f}(\nu_{2})\). Then,

\[U_{f}(\lambda\nu_{1}+(1-\lambda)\nu_{3}) =\int fd(\lambda\nu_{1}+(1-\lambda)\nu_{3})\] \[=\lambda\underbrace{\int fd\nu_{1}}_{U_{f}(\nu_{1})}+(1-\lambda) \int f\mathrm{d}\nu_{3}\] \[\geq\lambda\underbrace{\int fd\nu_{2}}_{U_{f}(\nu_{2})}+(1-\lambda )\int f\mathrm{d}\nu_{3}\] \[=\int fd(\lambda\nu_{2}+(1-\lambda)\nu_{3})\] \[=U_{f}(\lambda\nu_{2}+(1-\lambda)\nu_{3})\]

In particular, the mean and the exponential utility do.

Translation PropertyThis property comes from the linearity of the mean and the multiplicative morphism of the exponential. Let \(\nu_{1},\nu_{2}\in\mathscr{P}(\mathbb{R}),\ c\in\mathbb{R}\). Assume that \(U_{\exp}\left(\nu_{1}\right)\geq U_{\exp}\left(\nu_{2}\right)\) and \(U_{\text{mean}}(\nu_{1})\geq U_{\text{mean}}(\nu_{2})\). Then,

\[U_{\exp}(\tau_{c}\nu_{1}) =\int\exp(r)\mathrm{d}\tau_{c}\nu_{1}(r)\] \[=\int\exp(r+c)\mathrm{d}\nu_{1}(r)\] \[=\exp(c)\int\exp(r)\mathrm{d}\nu_{1}(r)\] \[=\exp(c)U_{\exp}\left(\nu_{1}\right)\] \[\geq\exp(c)U_{\exp}\left(\nu_{2}\right)\] \[=\ldots\] \[=U_{\exp}(\tau_{c}\nu_{2})\]\[U_{\text{mean}}(\tau_{c}\nu_{1}) =\int\lambda\tau+b+c\mathrm{d}\tau\] \[=c+U_{\text{mean}}(\nu_{1})\] \[\geq c+U_{\text{mean}}(\nu_{2})\] \[=\dots\] \[=U_{\text{mean}}(\tau_{c}\nu_{2})\]

### Policy Evaluation for linear combinations of moments

Rowland et al. (2019) prove a necessary condition on Bellman-closed utilities, namely that they should be a family of the form \(\{x\mapsto x^{\ell}\exp(\lambda x)|0\leq\ell\leq L\}\) for the undiscounted, finite-horizon setting. In the discounted case, the necessary condition is only valid for \(\lambda=0\), that is, without the exponential. They also prove that moments (without the exponential) also verify the sufficient condition such that in that setting they are the only Bellman-closed families of utilities.

In the undiscounted setting, to the best of our knowledge, a similar result has not yet been proved. We provide here the sufficient condition for families of the form \(\{x\mapsto x^{\ell}\exp(\lambda x)|0\leq\ell\leq L\}\). We show that they are Bellman-closed and that this implies that they can be computed by DP.

Let's consider the family \(s_{k}(\nu)=\int r^{k}\exp(\lambda r)\mathrm{d}\nu(r)\) for \(k\in[n]\) and some fixed \(\lambda\in\mathbb{R}\).

\[s_{n}(\eta_{h}^{\pi}(x,a))\] \[=\mathbb{E}\left[Z_{h}^{\pi}(x,a)^{n}\exp(\lambda Z_{h}^{\pi}(x,a ))\right]\] \[=\mathbb{E}\left[\left(R_{h}(x,a)+Z_{h+1}^{\pi}(X^{\prime},A^{ \prime})\right)^{n}\exp(\lambda(R_{h}(x,a)+Z_{h+1}^{\pi}(X^{\prime},A^{\prime }))\right]\right.\] \[=\sum_{x^{\prime},a^{\prime}}p_{h}(x,a,x^{\prime})\pi_{h}^{x^{ \prime}}(a^{\prime})\mathbb{E}_{R_{h},Z_{h+1}}\left[\left(R_{h}(x,a)+Z_{h+1}^{ \pi}(x^{\prime},a^{\prime})\right)^{n}\exp(\lambda(R_{h}(x,a)+Z_{h+1}^{\pi}(x ^{\prime},a^{\prime}))\right]\] \[=\sum_{x^{\prime},a^{\prime}}p_{h}(x,a,x^{\prime})\pi_{h}^{x^{ \prime}}(a^{\prime})\mathbb{E}_{R_{h},Z_{h+1}}\left[\sum_{k=0}^{n}\binom{n}{k }R_{h}(x,a)^{n-k}\exp(\lambda R_{h}(x,a))Z_{h+1}^{\pi}(x^{\prime},a^{\prime}) ^{k}\exp(\lambda Z_{h+1}^{\pi}(x^{\prime},a^{\prime})\right]\] \[=\sum_{x^{\prime},a^{\prime}}p_{h}(x,a,x^{\prime})\pi_{h}^{x^{ \prime}}(a^{\prime})\sum_{k=0}^{n}\binom{n}{k}\mathbb{E}_{R_{h}}\left[R_{h}(x,a )^{n-k}\exp(\lambda R_{h}(x,a))\right]\mathbb{E}_{Z_{h+1}}\left[Z_{h+1}^{\pi} (x^{\prime},a^{\prime})^{k}\exp(\lambda Z_{h+1}^{\pi}(x^{\prime},a^{\prime}) \right]\] \[=\sum_{x^{\prime},a^{\prime}}p_{h}(x,a,x^{\prime})\pi_{h}^{x^{ \prime}}(a^{\prime})\sum_{k=0}^{n}\binom{n}{k}\mathbb{E}_{R_{h}}\left[R_{h}(x, a)^{n-k}\exp(\lambda R_{h}(x,a))\right]s_{k}(\eta_{h+1}^{\pi}(x^{\prime},a^{ \prime}))\]

This first proves that this family of statistical functional is Bellman closed: they can be expressed as a linear combination of the others. Moreover, on the right-hand side, the expression only depends on the distributions and functionals at the current step \(h\) and at the next step \(h+1\). Thus, it provides a natural way to evaluate these functionals by DP.

## Appendix B Categorical Projection: an alternative parametrization

The categorical projection was proposed and studied in Bellemare et al. (2017); Rowland et al. (2018); Bellemare et al. (2023). For a bounded return distribution, it spreads a fixed number \(N\) of Diracs evenly over the support and used weight parameters to represent the true distribution. The parameter \(N\) is often referred to as the _resolution_ of the projection. More precisely, on a support \([V_{\text{min}},V_{\text{max}}]\), we write \(\Delta=\frac{V_{\text{min}}-V_{\text{min}}}{N-1}\) the step between atoms \(z_{i}=V_{\text{min}}+i\Delta,\quad i\in\llbracket 0,N-1\rrbracket\). We define the projection of a given Dirac distribution \(\delta_{y}\) on the parametric space \(\mathscr{P}_{C}(\mathbb{R})=\{\sum_{i}p_{i}\delta_{z_{i}}|\ 0\leq p_{i}\leq 1,\ \sum_{i}p_{i}=1\}\) by

\[\Pi_{C}(\delta_{y})=\begin{cases}\delta_{z_{0}}&y\leq z_{0}\\ \frac{z_{i+1}-y}{z_{i+1}-z_{i}}\delta_{z_{i}}+\frac{y-z_{i}}{z_{i+1}-z_{i}} \delta_{i+1}&z_{i}<y<z_{i+1}\\ \delta_{z_{N-1}}&y\geq z_{N-1}\end{cases}\] (11)This definition can naturally be extended to any bounded distribution \(\nu\), and by extension, \(\Pi_{C}\eta=(\Pi_{C}\eta(s,a))_{(s,a)\in\mathcal{X}\times\mathcal{A}}\). This linear operator minimizes the Cramer distance of the parametrization to the parametric space (Rowland et al., 2018).

This projection verifies this approximation bound, analog to the quantile projection,

\[\sup_{(x,a)\in\mathcal{X}\times\mathcal{A}}\ell_{2}(\Pi_{C}\eta(x,a),\eta(x,a)) \leq\frac{\Delta_{\eta}}{N}\.\] (12)

Using the property that \(W_{1}(\nu_{1},\nu_{2})\leq\sqrt{\Delta}_{\eta}\ell_{2}(\nu_{1},\nu_{2})\), the results with the quantile projection can be adapted for the categorical projection, adding \(\sqrt{\Delta}_{\eta}\) factors to the bounds.

This parametrization has the nice property of preserving the mean of the distribution. Yet, even for risk-neutral RL, Quantile-based DistRL algorithm seem to work better and display better properties(Dabney et al., 2018, 2018).

## Appendix C Proofs for Policy Evaluation with parameterized distributions

### Proof of Proposition 1

We recall the statement of Proposition 1: Let \(\pi\) be a policy and \(\eta^{\pi}\) the associated Q-value distributions. Assume the return is bounded on a interval of length \(\Delta_{\eta}\leq H\Delta_{R}\), where \(\Delta_{R}\) is the support size of the reward distribution. Let \(\hat{\eta}^{\pi}\) be the Q-value distributions obtained by dynamic programming (Algorithm 1) using the quantile projection \(\Pi_{Q}\) with resolution \(N\). Then,

\[\sup_{(x,a,h)\in(\mathcal{X},\mathcal{A},[H])}W_{1}(\hat{\eta}^{\pi}_{h}(x,a), \eta^{\pi}_{h}(x,a))\leq H\frac{\Delta_{\eta}}{2N}\leq H^{2}\frac{\Delta_{R}} {2N}\.\]

To avoid clutter of notation, we denote \(\overline{W}_{1}(\hat{\eta},\eta):=\sup_{(x,a)\in\mathcal{X}\times\mathcal{A} }W_{1}(\hat{\eta}(x,a),\eta(x,a))\).

Proof.: First recall that for any Q-value distribution \((\eta_{h})_{h\in[H]}\), with the return bounded on an interval of length \(\Delta_{\eta}\leq H\Delta_{R}\), and \(\Pi\) one of the projection operator of interest with resolution \(n\), we have the following bound on the projection estimation error due to Rowland et al. (2019) (Eq (6)):

\[\overline{W}_{1}(\Pi\eta,\eta)\leq\frac{\Delta_{\eta}}{2N}\.\] (13)

At a fixed step \(h\in[H]\), we have the following inequality:

\[\overline{W}_{1}(\hat{\eta}^{\pi}_{h},\eta^{\pi}_{h}) =\overline{W}_{1}(\Pi\mathcal{T}^{\pi}_{h}\hat{\eta}^{\pi}_{h+1},\mathcal{T}^{\pi}_{h}\eta^{\pi}_{h+1})\] \[\leq\overline{W}_{1}(\Pi\mathcal{T}^{\pi}_{h}\hat{\eta}^{\pi}_{h +1},\mathcal{T}^{\pi}_{h}\hat{\eta}^{\pi}_{h+1})+\overline{W}_{1}(\mathcal{T}^ {\pi}_{h}\hat{\eta}^{\pi}_{h+1},\mathcal{T}^{\pi}_{h}\eta^{\pi}_{h+1})\] (14) \[\leq H\frac{\Delta_{R}}{2N}+\overline{W}_{1}(\hat{\eta}^{\pi}_{h+ 1},\eta^{\pi}_{h+1})\.\] (15)

Where (14) is due to the triangular inequality with \(\mathcal{T}^{\pi}_{h}\hat{\eta}^{\pi}_{h+1}\) as the middle term. In (15), the first term comes from applying (13) to the first term of the previous line. The second term is a consequence of the non-expansive property of the Bellman operator (Bellemare et al., 2017):

\[\overline{W}_{1}(\mathcal{T}\eta_{1},\mathcal{T}\eta_{2})\leq\overline{W}_{1}( \eta_{1},\eta_{2})\.\]

Using it recursively starting from \(h=1\), and using the fact that \(\hat{\eta}^{\pi}_{H}=\eta^{\pi}_{H}\) we get:

\[\overline{W}_{1}(\hat{\eta}^{\pi}_{1},\eta^{\pi}_{1})\leq H\frac{\Delta_{R}}{2N }+\overline{W}_{1}(\hat{\eta}^{\pi}_{2},\eta^{\pi}_{2})\leq 2H\frac{\Delta_{R}}{2N} +\overline{W}_{1}(\hat{\eta}^{\pi}_{3},\eta^{\pi}_{3})\leq\cdots\leq H^{2} \frac{\Delta_{R}}{2N}\.\]

### Proof of Lemma 1

We recall the statement of Lemma 1: Let \(s\) be either a utility or a distorted mean and let \(L\) be the Lipschitz coefficient of its characteristic function. Let \(\nu_{1},\nu_{2}\) be return distributions. Then:

\[|s(\nu_{1})-s(\nu_{2})|\leq LW_{1}(\nu_{1},\nu_{2})\.\]

Proof.: We prove the property for each family of utilities separately:

**Case 1:**\(s\) is a utility. There exists \(f\) such that \(s(\nu)=\int fd\nu\). Let \(L_{f}\) be its Lipsichtz constant. The Kantorovitch-Rubenstein duality [23] states that:

\[W_{1}(\nu_{1},\nu_{2})=\frac{1}{L_{f}}\sup_{||g||_{L}\leq L_{f}}\left(\int g\; \mathrm{d}\nu_{1}-\int g\;\mathrm{d}\nu_{2}\right)\,,\] (16)

where \(||\cdot||_{L}\) is the Lipschitz norm. We then immediatly get:

\[L_{f}W_{1}(\nu_{1},\nu_{2})\geq\Big{|}\int f\;\mathrm{d}\nu_{1}-\int f\; \mathrm{d}\nu_{2}\Big{|}=|s(\nu_{1})-s(\nu_{2})|\.\] (17)

**Case 2:**\(s\) is a distorted mean. There exists \(g\) such that \(s(\nu)=\int_{0}^{1}g^{\prime}(\tau)F_{\nu}^{-1}(\tau)\mathrm{d}\tau\). Let \(L_{g}\) be its Lipschitz coefficent. Thus:

\[|s(\nu_{1})-s(\nu_{2})| =\Big{|}\int_{0}^{1}g^{\prime}(\tau)\left(F_{\nu_{1}}^{-1}-F_{\nu _{2}}^{-1}(\tau)\right)\mathrm{d}\tau\Big{|}\] \[\leq||g^{\prime}||_{\infty}\int_{0}^{1}\Big{|}F_{\nu_{1}}^{-1}( \tau)-F_{\nu_{2}}^{-1}(\tau)\Big{|}\mathrm{d}\tau\] \[\leq L_{g}W_{1}(\nu_{1},\nu_{2})\.\]

## Appendix D About the tightness of Theorem 1

The upper bound provided by Theorem 1 is mainly based on Proposition 1. The latter is obtained by summing, for every step, the Projection Bound by Rowland et al. [2019] (Eq. 6). Thus, achieving the bound would requires first to find a problem instance for which, at every step, the projection bound is tight. Then it would require to verify that the total error is the sum of those projection errors.

The experiment in Figure 2 already shows that summing the total error is very close to the sum of the projection errors. However, in that example, the projection error bound is not reached after the first step. In the following, we exhibit an MDP for which the projection bound is tight at every timestep.

First, let us consider a family of distributions for which the projection error is tight:

**Proposition 2**.: _Let \(N\in\mathbb{N},\;\Delta\in\mathbb{R}^{+}\). Consider \(z_{i}=\frac{i\Delta}{N}\). The distribution_

\[\nu_{N,\Delta}=\frac{1}{2N}\sum_{i=0}^{N-1}(\delta_{z_{i}}+\delta_{z_{i+1}})\]

_has a support of length \(\Delta\) and verifies \(W_{1}(\nu_{N,\Delta},\Pi_{Q}\nu_{N,\Delta})=\frac{\Delta}{2N}\.\)_

Proof.: The cumulative distribution function (CDF) the distribution \(\nu_{N,\Delta}\) is

\[F_{\nu_{N,\Delta}}(x)=\left\{\begin{array}{ll}0&x<0=z_{0}\,\\ \frac{2i+1}{2N}=\tau_{i}&z_{i}\leq x<z_{i+1}\,\\ 1&x\geq 1=z_{N}\.\end{array}\right.\]

We write \(\tau_{i}=\frac{2i+1}{2N}\), so that \(\forall i\in[0,N-1],F_{\nu_{N,\Delta}}(z_{i})=\tau_{i}\). We now explicit the projection of \(\nu_{N,\Delta}\) and the Wasserstein distance relative to it. A possible Quantile projection is \(\Pi_{Q}\nu_{N,\Delta}=\frac{1}{N}\sum_{i=0}^{N-1}\delta_{z_{i}}\)\[W_{1}(\nu_{N,\Delta},\Pi_{Q}\nu_{N,\Delta}) =\int_{0}^{1}|F_{\nu}^{-1}(w)-F_{\Pi_{Q}\nu}^{-1}(w)|\mathrm{d}w\] \[=\sum_{i=0}^{N-1}\int_{\frac{i}{N}}^{\frac{i+1}{N}}|F_{\nu}^{-1}(w )-\underbrace{F_{\Pi_{Q}\nu}^{-1}(w)}_{z_{i}}|\mathrm{d}w\] \[=\sum_{i=0}^{N-1}\int_{\frac{i}{N}}^{\tau_{i}}|\underbrace{F_{\nu }^{-1}(w)}_{z_{i}}-z_{i}|\mathrm{d}w+\int_{\tau_{i}}^{\frac{i+1}{N}}| \underbrace{F_{\nu}^{-1}(w)}_{z_{i+1}}-z_{i}|\mathrm{d}w\] \[=\sum_{i=0}^{N-1}\frac{1}{2N}\underbrace{(z_{i+1}-z_{i})}_{\frac {\Delta}{N}}\] \[=\sum_{i=0}^{N-1}\frac{\Delta}{2N^{2}}\] \[=\frac{\Delta}{2N}\]

The value distribution of the following time steps is obtained by applying two operators: the Bellman operator and the projection operator. Here will consider a MDP with only one state. We need to find such operators so that \(\Pi_{Q}\mathcal{T}\nu_{N_{1}\Delta_{1}}=\nu_{N_{2}\Delta_{2}}\), where the Bellman operator simply consists in the added reward distribution.

**Proposition 3**.: _Let \(N\in\mathbb{N},\ \Delta\in\mathbb{R}^{+}\). Consider \(\varrho=\frac{1}{2}(\delta_{0}+\delta_{\frac{\Delta}{N-1}})\). Then there exists a Quantile projection operator \(\Pi_{Q}\) such that_

\[\varrho*\Pi_{Q}(\nu_{N,\Delta})=\nu_{N,(\Delta+\frac{\Delta}{N-1})}\]

Proof.: We consider \(\Pi_{Q}(\nu_{N,\Delta})=\frac{1}{N}\sum_{i=0}^{N-1}\delta_{\frac{i}{N-1}}.\) Since \(\forall i,\frac{i\Delta}{N-1}\leq z_{i}<\frac{(i+1)\Delta}{N-1}\), we have \(F(z_{i})=\tau_{i}\), verifying that it is indeed a valid Quantile Projection.
_Consider the MDP with only one state \(x\) and action \(a\), reward distribution \(\varrho_{h}\), horizon \(H\). Consider \(\hat{\eta}_{h}\) the value distributions obtained through dynamic programming with quantile projection. Then, at any timestep \(h\), the error induced by the projection operator matches the bound in Eq. 6:_

\[W_{1}(\Pi_{Q}\eta_{h},\eta_{h})=\frac{\Delta_{h}}{2N}\]

While the projection error is maximal in such instance, it was verified experimentally that the bound in Prop. 1 was still not tight. This comes from the fact that the Bellman operator is not a non-contraction is this case, and that the triangular inequality used to sum the projection error is not tight either.

We hence found that every inequality used in proving Theorem 1 can be tight, but there does not seem to exist an instance for which all the inequalities are tight at the same time, meaning that this bound would be never reached exactly.

## Appendix E Proof of the main results

The proof of the result is divided in parts. First we show that Bellman optimizable functionals verify the two properties of Lemma 2 (Independence and Translation). Then, using those properties, we prove that Bellman optimizable functionals can only be exponential utilities (Theorem 2). Using the known fact that exponential utilities are bellman optimizable, we obtain the full characterization.

### Proof of Lemma 2

Proof.: **To prove that each property is necessary**, we use a proof by contradiction, and exhibit MDPs where the algorithm is not optimal when the property is not verified.

Independence PropertyLet \(s\) be a Bellman optimizable statistical functional that does not satisfy the Independence property. That is, there exists \(\nu_{1},\nu_{2},\nu_{3}\in\mathscr{P}(\mathbb{R})\) and \(\lambda\in[0,1]\) such that \(s(\nu_{1})\geq s(\nu_{2})\) but \(s(\lambda\nu_{1}+(1-\lambda)\nu_{3})<s(\lambda\nu_{2}+(1-\lambda)\nu_{3})\).

Then consider the MDP in Fig.4 (left) with horizon \(H=2\) corresponding to the depth of the tree: The agent starts in Start and must take 2 actions, a unique but random and non-rewarding one (\(a_{0}\)) and a final deterministic step (\(a_{1}\) or \(a_{2}\)) to a rewarding state. Thus, by construction, the optimal strategy is \((a_{0},a_{2})\) that leads to End 2 with probability \(\lambda\) (and End 3 with probability \(1-\lambda\)). The true optimal distribution at Start state is \(\eta_{0}^{*}=\lambda\nu_{2}+(1-\lambda)\nu_{3}\). We compute the distributions output by the algorithm:

\[H=2: \quad\eta_{2}(\text{End 1},a^{*})=\delta_{0},\quad\eta_{2}(\text{End 2},a^{*})=\delta_{0},\quad\eta_{2}(\text{End 3},a^{*})=\delta_{0}\] \[H=1: \quad\eta_{1}(\text{Left},a^{*}=\operatorname*{arg\,max}_{a}s( \nu_{a}))=\nu_{1},\quad\eta_{2}(\text{Right},a^{*}=a_{1},a_{2})=\nu_{3}\] \[H=0: \quad\eta_{0}(\text{Start},a^{*}=a_{0})=\lambda\nu_{1}+(1-\lambda )\nu_{3}\]

Figure 3: Evaluation of the Wasserstein Distance between the true value distribution and the approximated one, in the MDP described in Corollary 1

The output return distribution \(\eta_{0}\) is not the true optimal \(\eta_{0}^{*}\) for \(s\) so the algorithm is incorrect which is a contradiction as \(s\) is assumed to be Bellman optimizable. Hence the property is needed.

Translation PropertyLet \(s\) be a Bellman optimizable statistical functional that does not verify the Translation Property, _i.e._ there exists \(\nu_{1},\nu_{2}\in\mathscr{P}(\mathbb{R})\), \(c\in\mathbb{R}\) such that \(s(\nu_{1})\geq s(\nu_{2})\) but \(s(\tau_{c}\nu_{1})<s(\tau_{c}\nu_{2})\). Then consider MDP in Fig.4 (right). The optimal strategy is again \((a_{0},a_{2})\) by construction. The algorithm output the following distribution:

\[H=2: \eta_{2}(\text{Left},a^{*})=\delta_{0},\quad\eta_{2}(\text{Right},a^{*})=\delta_{0}\] \[H=1: \eta_{1}(\text{Step},a^{*})=\nu_{1}\] \[H=0: \eta_{0}(\text{Start},a^{*})=\tau_{c}\nu_{1}\]

So here again, the algorithm does not output an optimal distribution for \(s\), hence the necessity of the property.

This proof shows that both properties are necessary, but not that they are sufficient. The other implication could be proven, but the proof would be unnecessary as those properties are enough to restrict to only 1 class of function for which we already know is bellman optimizable.

### Proof of Theorem 2

Proof.: For any return distribution \(\nu\), let \(s(\nu)\) be the considered functional. Since we assume that \(s\) is \(W_{1}\)-continuous Bellman optimizable, the by the Independence Property (Lemma 2) we know from the Expected Utility Theorem that we can assume: \(s(\nu)=s_{f}(\nu)=\int_{\mathbb{R}}f(x)d\nu(x)\) for some continuous, monotonous mapping \(f\). Without loss of generality, we may assume by a density argument that \(f\) is twice continuously differentiable.

By the Intermediate Value Theorem, we can then define \(\phi(h)=f^{-1}(\frac{1}{2}(f(0)+f(h)))\), so that \(\frac{1}{2}(f(0)+f(h))=f(\phi(h))\) and in particular \(\phi(0)=f^{-1}(f(0))=0\).

A very special case is when \(f\) is constant, which satisfies the theorem. We now assume that there exists point \(x_{0}\) such that \(f^{\prime}\) does not vanish on a neighborhood of \(x_{0}\). On this neighborhood, using the inverse function theorem, \(\phi\) is also twice differentiable. Without loss of generality, we assume that \(x_{0}=0\).

For any fixed \(h>0\), we consider the probability distributions \(\nu_{1}=\frac{1}{2}(\delta_{0}+\delta_{h})\) and \(\nu_{2}=\delta_{\phi(h)}\). Remark that \(s_{f}(\nu_{1})=\int f\ \mathrm{d}\nu_{1}=\frac{1}{2}(f(0)+f(h))\) and \(s_{f}(\nu_{2})=f(\phi(h))=\frac{1}{2}(f(0)+f(h))\) by definition of \(\phi\), so \(s_{f}(\nu_{1})=s_{f}(\nu_{2})\).

Figure 4: Left: Independence Property Counter Example, Right: Translation Property Counter Example. Each arrow represents a state transition, which is characterized by the action leading to the transition, the probability of such transition, and the reward distribution of the transition.

The Translation property implies that for all \(x\in\mathbb{R}\), \(s_{f}(\nu_{1})\leq s_{f}(\nu_{2})\implies s_{f}\big{(}\nu_{1}(\cdot+x)\big{)} \leq s_{f}\big{(}\nu_{2}(\cdot+x)\big{)}\) and \(s_{f}(\nu_{1})\geq s_{f}(\nu_{2})\implies s_{f}\big{(}\nu_{1}(\cdot+x)\big{)} \geq s_{f}\big{(}\nu_{2}(\cdot+x)\big{)}\). Hence, \(\forall x\in\mathbb{R}\), \(\frac{1}{2}\big{(}f(x)+f(x+h)\big{)}=f(x+\phi(h))\).

This equation can be differentiated twice with respect to \(h\). For any value of \(x\), we obtain:

\[\frac{1}{2}f^{\prime}(x+h) =\phi^{\prime}(h)f^{\prime}(x+\phi(h))\quad\text{ and }\] (18) \[\frac{1}{2}f^{\prime\prime}(x+h) =\phi^{\prime\prime}(h)f^{\prime}(x+\phi(h))+\phi^{\prime}(h)^{2 }f^{\prime\prime}(x+\phi(h))\;.\] (19)

Recall that by definition, \(\phi(0)=0\). Now, for \(x=0\), Eq. (18) yields \(\frac{1}{2}f^{\prime}(0)=\phi^{\prime}(0)f^{\prime}(\phi(0))=\phi^{\prime}(0 )f^{\prime}(0)\) and, since \(f^{\prime}(0)\neq 0\), \(\phi^{\prime}(0)=\frac{1}{2}\).

Now, choosing \(h=0\) in (19) and plugging in the values of \(\phi(0)\) and \(\phi^{\prime}(0)\), we obtain for all \(x\in\mathbb{R}\):

\[\frac{1}{4}f^{\prime\prime}(x)=\phi^{\prime\prime}(0)f^{\prime}(x)\;.\]

We then consider two cases, depending on whether \(\phi^{\prime\prime}(0)\) is null or not.

**Case 1:**\(\phi^{\prime\prime}(0)=0\). The equation simply becomes \(f^{\prime\prime}(x)=0\), hence \(f\) is affine: \(\exists\;a,b\in\mathbb{R},\;f(x)=ax+b\).

**Case 2:**\(\phi^{\prime\prime}(0)\neq 0\). We write \(\beta=4\phi^{\prime\prime}(0)\). The differential equation becomes \(f^{\prime\prime}(x)=\beta f^{\prime}(x)\), whose solutions are of the form

\[\exists\;c_{1},c_{2},\beta,\quad f(x)=c_{1}\exp(\beta x)+c_{2}\;.\]

Hence, \(f\) can only be the identity or the exponential, up to an affine transformation.