# Promises and Pitfalls of Threshold-based Auto-labeling

 Harit Vishwakarma

hvishwakarma@cs.wisc.edu

University of Wisconsin-Madison

&Heguang Lin

hglin@seas.upenn.edu

University of Pennsylvania

&Frederic Sala

fredsala@cs.wisc.edu

University of Wisconsin-Madison

&Ramya Korlakai Vinayak

ramya@ece.wisc.edu

University of Wisconsin-Madison

###### Abstract

Creating large-scale high-quality labeled datasets is a major bottleneck in supervised machine learning workflows. Threshold-based auto-labeling (TBAL), where validation data obtained from humans is used to find a confidence threshold above which the data is machine-labeled, reduces reliance on manual annotation. TBAL is emerging as a widely-used solution in practice. Given the long shelf-life and diverse usage of the resulting datasets, understanding when the data obtained by such auto-labeling systems can be relied on is crucial. This is the first work to analyze TBAL systems and derive sample complexity bounds on the amount of human-labeled validation data required for guaranteeing the quality of machine-labeled data. Our results provide two crucial insights. First, reasonable chunks of unlabeled data can be automatically and accurately labeled by seemingly bad models. Second, a hidden downside of TBAL systems is potentially prohibitive validation data usage. Together, these insights describe the promise and pitfalls of using such systems. We validate our theoretical guarantees with extensive experiments on synthetic and real datasets1.

Footnote 1: https://github.com/harit7/TBAL

## 1 Introduction

Machine learning (ML) models with millions or even billions of parameters are used to obtain state-of-the-art performance in many applications, e.g., object identification [53], machine translation [63], and fraud detection [71]. Such large-scale models require training on large-scale labeled datasets. As an outcome, the typical supervised ML workflow begins with the construction of a large-scale high quality dataset. Datasets with up to millions of labeled data points have played a pivotal role in the advancement of computer vision. However, collecting labeled data is an expensive and time consuming process. A common approach is to rely on the services of crowdsourcing platforms such as Amazon Mechanical Turk (AMT) to get groundtruth labels.

Even with crowdsourcing, obtaining labels for the entire dataset is expensive. To reduce costs, data labeling systems that partially rely on using a model's predictions as labels have been developed. Such systems date back to teacher-less training [21]. Modern examples include Amazon Sagemaker Ground Truth [58] and others [61, 55, 2, 64]. These approaches can be broadly termed _auto-labeling_.

Auto-labeling systems aim to label unlabeled data using predictions from ML models that are often trained on small amounts of human labeled data which can produce incorrect labels. The shelf life of datasets is longer than those of models, e.g., ImageNet continues to be a benchmark for many computer vision tasks [14] fifteen years after its initial development. As a result, to reliably train new models on auto-labeled datasets and deploy them, we need a thorough understanding of how reliable the datasets output by these auto-labeling systems are. Unfortunately, many widelyused commercial auto-labeling systems [58, 55] are largely opaque with limited public information on their functionality. It is therefore unclear whether the quality of the datasets obtained can be trusted. To address this, we study the high level workflow of a popular _threshold-based_ auto-labeling (TBAL) system (see Figure 1). We emphasize that our goal is to understand such systems and their performance--not to promote them as a superior alternative to other approaches. Our goal is:

**Goal.** Develop a fundamental understanding of TBAL systems. This is crucial since there is a lack of theoretical understanding of the reliability of these systems despite their wide adoption.

The TBAL systems we study (Figure 1) work iteratively. At a high level, in each iteration, the system trains a model on currently available human labeled data and decides to label certain parts of unlabeled data using the trained model by finding high-accuracy regions using validation data. It then collects human labels on a small portion of unlabeled data that is deemed helpful for training the current model in the next iteration. The validation data is created by sampling i.i.d. points from the unlabeled pool and querying human labels for them. In addition to training data, the validation data is a major driver of the cost and accuracy of auto-labeling and will be a key component in our study.

**Our Contributions.** We study TBAL systems (Figure 1) and make the following contributions:

* Provide the **first theoretical characterization of TBAL systems**, developing tradeoffs between the quantity of manually labeled data and the quantity and quality of auto-labeled data (Section 3).
* Empirical results validating our theoretical understanding on real and synthetic data (Section 4).

Our results reveal **two important insights**. Promisingly, even poor quality models are capable of reliably labeling at least some data when we have access to sufficient validation data and a good confidence function that can accurately quantify the confidence of a given model on any data point. On the downside, in certain scenarios, the quantity of the validation data required to reach a certain quantity and quality of auto-labeled data can be high.

## 2 Threshold-Based Auto-Labeling Algorithm

We begin with the problem setup and describe the TBAL algorithm that is inspired by the commercial systems [58]. Then we provide experiments and theoretical analysis shedding light on the pros and cons of TBAL. We emphasize that TBAL is not our proposal and our goal is to _understand_ the effectiveness of such an auto-labeling system.

### Problem Setup

Notation.Let the instance and label spaces be \(\mathcal{X}\) and \(\mathcal{Y}=\{1,\dots,k\}\). We assume that there is some _deterministic_ but unknown function \(f^{*}:\mathcal{X}\mapsto\mathcal{Y}\) that assigns true label \(y=f^{*}(\mathbf{x})\) to any \(\mathbf{x}\in\mathcal{X}\). We also assume that there is a _noiseless oracle_\(\mathcal{O}\) that can provide the true label \(y\in\mathcal{Y}\) for any given \(\mathbf{x}\in\mathcal{X}\). Let \(X_{pool}\subseteq\mathcal{X}\) denote a sufficiently large pool of unlabeled data to be labeled.

The goal of an auto-labeling algorithm is to produce accurate labels \(\tilde{y}_{i}\in\mathcal{Y}\) for points \(\mathbf{x}_{i}\in X_{pool}\) while minimizing the number of queries to the oracle. Let \([m]:=\{1,2,\dots,m\}\), \(A\subseteq[N]\) be the set of indices of auto-labeled points, and \(X_{pool}(A)\) be these points. The _auto-labeling error_\(\widehat{\mathcal{E}}(X_{pool}(A))\)

Figure 1: High-level workflow threshold-based auto-labeling (TBAL). Box (B) shows the key component estimating the auto-labeling region using validation data and auto-labeling points in it.

and the _coverage_\(\widehat{\mathcal{P}}(X_{pool}(A))\) are defined as

\[\widehat{\mathcal{E}}(X_{pool}(A)):=\frac{1}{N_{a}}\sum_{i\in A}\mathds{1}(\tilde {y}_{i}\neq f^{*}(\mathbf{x}_{i}))\;\;\text{and}\;\;\widehat{\mathcal{P}}(X_{ pool}(A)):=\frac{|A|}{N}=\frac{N_{a}}{N},\] (1)

where \(N_{a}\) denotes the size of auto-labeled set \(A\). TBAL algorithm aims to auto-label the dataset so that \(\widehat{\mathcal{E}}(X_{pool}(A))\leq\epsilon_{a}\) while maximizing coverage \(\widehat{\mathcal{P}}(X_{pool}(A))\) for any given \(\epsilon_{a}\in(0,1)\).

**Hypothesis Class and Confidence Function.** A TBAL algorithm is given a fixed _hypothesis space_\(\mathcal{H}\) and a _confidence function_\(g:\mathcal{H}\times\mathcal{X}\mapsto T\subseteq\mathbb{R}^{+}\) that quantifies the confidence of \(h\in\mathcal{H}\) on any data point \(\mathbf{x}\in\mathcal{X}\). Confidence functions include prediction probabilities and margin scores. For example, when \(\mathcal{H}\) is a set of unit-norm homogeneous linear classifiers, i.e. \(h_{\mathbf{w}}(\mathbf{x})=\text{sign}(\mathbf{w}^{T}\mathbf{x})\) with \(\mathbf{w}\in\{\mathbf{w}\in\mathbb{R}^{d}:||\mathbf{w}||_{2}=1\}\), a reasonable confidence function is \(g(h_{\mathbf{w}},\mathbf{x})=|\mathbf{w}^{T}\mathbf{x}|\).

Note that the target \(f^{*}\) might not be in the hypothesis space \(\mathcal{H}\). Our analysis (Section 3) shows that the TBAL algorithm can work well, i.e., accurately label a reasonable fraction of unlabeled data with simpler hypothesis classes that do not contain the target hypothesis \(f^{*}\). We illustrate this with a simple example in Section 2.3 and Figure 2. Note as well that the features \(\mathbf{x}\) could be raw features or representations from self-supervised techniques, pre-trained models, etc. We analyze TBAL in settings (i) with no assumptions on the features and (ii) when the features are linearly separable.

### Description of the algorithm

The TBAL algorithm is given in Algorithm 1. It starts with an unlabeled pool \(X_{pool}\) and an auto-labeling error threshold \(\epsilon\). For ease of exposition, the algorithm is given the labeled validation set \(D_{val}\) of size \(N_{v}\) separately. In practice, it is created by selecting points at random from \(X_{pool}\). The algorithm starts with an initial batch of \(n_{s}\) random data points and obtains oracle labels for these. The algorithm works in an iterative manner using the following steps.

1. Oracle labeled data obtained in each iteration \(i\) is added to the training pool \(D_{train}^{(i)}\). It is used to train a model \(\hat{h}_{i}\) by performing empirical risk minimization (ERM).
2. Find the region where \(\hat{h}_{i}\) can auto-label accurately. The algorithm estimates a threshold \(\hat{t}_{i}\) on the confidence score above which it can auto-label with the desired auto-labeling accuracy on the validation data (see Algorithm 2). Thresholds that have too little validation data are discarded, since their estimates are unreliable. The minimum threshold is found such that the sum of the estimated error \(\widehat{\mathcal{E}}_{a}(\hat{h}_{i},t|X_{v}^{(i)})\) (see eq. (2)) and an upper confidence bound using the standard deviation of the estimated error, is at most the given auto-labeling error threshold.
3. Auto-label the points in the pool, \(X_{u}^{(i)}\), which have confidence \(g(\hat{h}_{i},\mathbf{x})>\hat{t}_{i}\). These are added to the set \(D_{out}\) and removed from the unlabeled pool. The validation points that fall in the auto-labeled region are also removed from the validation set so that in the next round the validation set and the unlabeled pool are from the same region and the same distribution. Removing the auto-labeled points from \(X_{\text{pool}}\) is a crucial step in the TBAL algorithm that enables it to focus only on the remaining unlabeled regions in the next iteration.
4. If there are points left in \(X_{\text{pool}}\), the algorithm selects points using some active querying strategy [57], obtains human labels for them, and adds them to the training pool. Note that the auto-labeled data is not added into the training set.

This process continues until there are no data points left to be labeled. The algorithm then outputs the labeled dataset, which is a mixture of human- and machine-labeled points.

### Comparison between Auto-Labeling, Active Learning and Selective Classification

What is the difference between TBAL and methods such as active learning and selective classification?

_Active learning._ The goal of active learning [57] (AL) is to find the best model in hypothesis class \(\mathcal{H}\) by training with less labeled data compared to passive learning. This is usually achieved by obtaining labels for the most informative points. Note that the _end goal is to output a model_ from the function class whose predictions on new data as good as the best model in the function class could.

_Selective Classification._ The goal of selective classification (SC) [17] is to find the best combination of the hypothesis and selection functions to minimize error and maximize coverage of selection regions.

_Auto-Labeling._ The output of an auto-labeling procedure is a labeled dataset (not a model). When the hypothesis class is of lower complexity, it is often not possible to find a good classifier. The goal of an auto-labeling system is to label as much of the unlabeled data as accurately as possible with a given function class and with limited labeled data from humans.

**Is active learning alone enough to auto-label data?** AL has been found to be effective in reducing the number of labels needed to learn versus passive learning, particularly in low-noise cases [28]. Doing auto-labeling using AL followed by SC may be effective in such settings. However, in real-world scenarios, noise levels may be higher and the hypothesis class could be misspecified. In these instances, using the model learned through active learning to automatically label all data may result in a high number of errors.

We illustrate this difference between AL, SC, and auto-labeling through an example. Suppose the data consists of two concentric circles, one for each class, with the same number of points per class (Figure 2(a)). This setting is not linearly separable. We run TBAL, AL, and AL followed by SC with an error tolerance of \(\epsilon_{a}=1\%\) and linear classifiers and confidence functions. The results are shown in Figure 2. Note that the multiple optimal linear classifiers will all incur an error of \(50\%\). AL algorithms can only output models that make at least \(50\%\) error. If we naively use the output model for auto-labeling, we can obtain near full coverage but incur around \(50\%\) auto-labeling error. If we use the model output by AL with threshold-based SC, labeling error is reduced. However, it can only label \(\approx 25\%\) of the unlabeled data. In contrast, TBAL can label almost all of the data accurately (close to \(100\%\) coverage) with less than \(1\%\) auto-labeling error.

Figure 2: Comparison of TBAL, active learning (AL) followed by selective classification (AL+SC) and passive learning (PL) followed b selective classification (PL+SC) on the Circles dataset (Sec. 2.3) using linear classifiers and confidence functions. (a) Samples auto-labeled, queried, and left unlabeled. (b) The auto-labeling error and coverage achieved by the algorithms. (50 trials.)

Theoretical Analysis

The performance of the TBAL (Algorithm 1) depends on many factors including the hypothesis class, the accuracy of the confidence function, the data sampling strategy, and the size of the training and validation data. In particular, the amount of validation data plays a critical role in determining the accuracy of the confidence function, which in turn affects the accuracy and coverage.

We derive bounds on the auto-labeling error and the coverage for Algorithm 1 in terms of the size of the validation data, the number of auto-labeled points \(N_{a}^{(k)}\), and the Rademacher complexity of the extended hypothesis class \(\mathcal{H}^{T,g}\) induced by the confidence function \(g\). Our first result, Theorem (3.2), applies to general settings and makes no assumptions on the particular form of the hypothesis class, the data distribution, and the confidence function. We then instantiate and specialize the results for a specific setting in Section 3.1. We introduce some notation to aid in stating our results,

**Definition 3.1**.: _(Hypothesis Class with Abstain) The function \(g\), set \(T\) and \(\mathcal{H}\) induce an extended hypothesis class \(\mathcal{H}^{T,g}:=\mathcal{H}\times T\). Any function \((h,t)\in\mathcal{H}^{T,g}\) is defined as \((h,t)(\mathbf{x})=h(\mathbf{x})\) if \(g(h,\mathbf{x})\geq t\) and \(\bot\) otherwise. Here \((h,t)(\mathbf{x})=\bot\) means \((h,t)\) abstains in classifying the point \(\mathbf{x}\)._

**Error Definitions.** Let \(\mathcal{S}\subseteq\mathcal{X}\) denote a non-empty sub-region of \(\mathcal{X}\) and \(S\subseteq\mathcal{S}\) be a finite set of i.i.d. samples from \(\mathcal{S}\). The subset \(\mathcal{S}(h,t)\subseteq\mathcal{S}\) denotes the regions where \((h,t)\) does not abstain i.e. \(\mathcal{S}(h,t):=\{\mathbf{x}\in\mathcal{S}:(h,t)(\mathbf{x})\neq\bot\}\), and the conditional probability mass associated with it is \(\mathbb{P}(h,t|\mathcal{S}):=\mathbb{P}(\mathcal{S}(h,t)|\mathcal{S}),\) and its empirical counterpart \(\widehat{\mathbb{P}}(h,t|S):=|S(h,t)|/|S|.\) We use \(\mathbb{P}(\mathcal{S})\) to denote the probability mass of set \(\mathcal{S}\) and \(\mathbb{P}(\mathcal{S}^{\prime}|\mathcal{S})\) for the conditional probability of subset \(\mathcal{S}^{\prime}\subseteq\mathcal{S}\) given \(\mathcal{S}\). The population level and empirical auto-labeling errors are defined as follows:

\[\mathcal{E}_{a}(h,t|\mathcal{S}) :=\mathbb{E}_{\mathbf{x}|\mathcal{S}}[\ell_{0-1}^{\bot}(h,t, \mathbf{x},y)]/\mathbb{P}(h,t|\mathcal{S}),\] (2) \[\widehat{\mathcal{E}}_{a}(h,t|S) :=(\sum_{\mathbf{x}_{i}\in S(h,t)}\ell_{0-1}^{\bot}(h,t,\mathbf{x }_{i},y_{i}))/|S(h,t)|\]

Here \(\ell_{0-1}^{\bot}(h,t,\mathbf{x},y):=\ell_{0-1}(h,\mathbf{x},y)\cdot\ell_{ \bot}(h,t,\mathbf{x})\) with \(\ell_{0-1}(h,\mathbf{x},y):=\mathds{1}(h(\mathbf{x})\neq y)\), and \(\ell_{\bot}(h,t,\mathbf{x}):=\mathds{1}(g(h,\mathbf{x})\geq t)\).

**Rademacher Complexity.** The Rademacher complexities for the function classes induced by the \(\mathcal{H},T,g\) and the loss functions are defined as \(\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g}\big{)}:=\mathfrak{R}_{n}\big{(} \mathcal{H},\ell_{0-1}\big{)}+\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g},\ell_{ \bot}\big{)}\). Let \(\hat{h}_{i}\) and \(\hat{t}_{i}\) be the ERM solution and the auto-labeling threshold at epoch \(i\). Let \(\eta_{i}\in(0,1)\) be a constant such that \(\mathbb{P}(\hat{h}_{i},\hat{t}_{i}|\mathcal{X}^{(i)})\geq P_{0}\) for all \(i\). Let \(X_{v}^{(i)}\) denote the validation set, and \(n_{v}^{(i)}\) and \(n_{a}^{(i)}\) the number of validation and auto-labeled points at epoch \(i\). Let \(\widehat{\mathcal{E}}_{a}(\hat{h}_{i},\hat{t}_{i}|X_{v}^{(i)})\) be the empirical conditional risk of \(\hat{h}_{i}\) in the region where \(g(\hat{h}_{i},\mathbf{x})\geq\hat{t}_{i}\) evaluated on the validation data \(X_{v}^{(i)}\).

We provide the following guarantees on the auto-labeling error and the coverage achieved by TBAL.

**Theorem 3.2**.: _(Overall Auto-Labeling Error and Coverage) Let \(k\) denote the number of rounds of the TBAL Algorithm 1. Let \(n_{v}^{(i)},n_{a}^{(i)}\) denote the number of validation and auto-labeled points at epoch \(i\) and \(n^{(i)}=|X^{(i)}|\). Let \(X_{pool}(A_{k})\) be the set of auto-labeled points at the end of round \(k\). \(N_{a}^{(k)}=\sum_{i=1}^{k}n_{a}^{(i)}\) be the total number of auto-labeled points. Then, for any \(\delta\in(0,1)\), with probability at least \(1-\delta\),_

\[\widehat{\mathcal{E}}\big{(}X_{pool}(A_{k})\big{)}\leq\sum_{i=1}^{k} \frac{n_{a}^{(i)}}{N_{a}^{(k)}} \Big{(} \underbrace{\widehat{\mathcal{E}}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|X_{v }^{(i)}\big{)}}_{(a)}+\underbrace{\frac{4}{P_{0}}\big{(}\mathfrak{R}_{n_{v}^{(i )}}\big{(}\mathcal{H}^{T,g}\big{)}+\frac{2}{P_{0}}\sqrt{\frac{1}{n_{v}^{(i)}} \log(\frac{8k}{\delta})}\big{)}}_{(b)}\Big{)}\] \[+\underbrace{\frac{4}{P_{0}}\Big{(}\sum_{i=1}^{k}\frac{n_{a}^{(i )}}{N_{a}^{(k)}}\mathfrak{R}_{n_{a}^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)}+ \sqrt{\frac{k}{N_{a}^{(k)}}\log(\frac{8k}{\delta})}\Big{)}}_{(c)},\quad and\]

\[\widehat{\mathcal{P}}\big{(}X_{pool}(A_{k})\big{)}\geq\sum_{i=1}^{k}\mathbb{P} \big{(}\mathcal{X}^{(i)}(\hat{h}_{i},\hat{t}_{i})\big{)}-2\mathfrak{R}_{n^{(i)}} \big{(}\mathcal{H}^{T,g}\big{)}-\sqrt{\frac{2k^{2}}{N}\log\big{(}\frac{8k}{ \delta}\big{)}}.\]

**Discussion.** We interpret this result, starting with the auto-labeling error term \(\widehat{\mathcal{E}}(X_{pool}(A_{k})).\) The term (a) \(\widehat{\mathcal{E}}_{a}(\hat{h}_{i},\hat{t}_{i}|X_{v}^{(i)})\) is the empirical conditional error in the auto-labeled region computed on the validation data in \(i\)-th round, which is at most \(\epsilon_{a}\). Thus, summing term (a) over all the rounds is at most \(\epsilon_{a}\). The term (b) provides an upper bound on the excess error over the empirical estimate term (a) as a function of the Rademacher complexity of \(\mathcal{H}^{T,g}\) and the validation data used in each round. The last term (c) captures the variance in the overall estimate as a function of the total number of auto-labeled points and the Rademacher complexity of \(\mathcal{H}^{T,g}\). If we let \(n_{v}^{(i)}\geq n_{v}\) i.e. the minimum validation points ensured in each round, then we can see the second term is \(\widehat{\mathcal{O}}(\mathfrak{R}_{n_{v}}(\mathcal{H}^{T,g}))\) and the third term is \(\mathcal{O}(\sqrt{1/n_{v}})\).

Therefore, validation data of size \(\mathcal{O}\left(1/\epsilon_{a}^{2}\right)\) in each round is sufficient to get a \(\mathcal{O}(\epsilon_{a})\) bound on the excess auto-labeling error. The terms with Rademacher complexities suggest that it is better to use a hypothesis class and confidence function such that the induced hypothesis class has low Rademacher complexity. While such a hypothesis class might not be rich enough to include the target function, it would still be helpful for efficient and accurate auto-labeling of the dataset which can then be used for training richer models in the downstream task. The coverage term provides a lower bound on the empirical coverage \(\widehat{\mathcal{P}}(X_{pool}(A_{k}))\) in terms of the true coverage of the sequence of estimated hypotheses \(\hat{h}_{i}\) and threshold \(\hat{t}_{i}\).

We note that the size of the validation data needed to guarantee the auto-labeling error in each round by Algorithm 1 is optimal up to \(\log\) factors. This follows by applying a result on the tail probability of the sum of independent random variables due to Feller [20]:

**Lemma 3.3**.: _Let \(c_{1},c_{2}\) and \(\sigma>0\). Let \(\mathbf{x}_{i}\in X\) be a set of \(n\) i.i.d. points from \(\mathcal{X}\) with corresponding true labels \(y_{i}\). Given \((h,t)\in\mathcal{H}^{T,g}\), let \(\mathbb{E}\big{[}\big{(}\ell_{0-1}^{\frac{1}{2}}(h,t,\mathbf{x}_{i},y_{i})- \mathcal{E}(h,t|\mathcal{X})\big{)}^{2}\big{]}=\sigma_{i}^{2}>\sigma^{2}\) for every \(\mathbf{x}_{i}\) for \(\sigma_{i}>0\) and let \(\sum_{i}^{n}\sigma_{i}^{2}\geq c_{1}\) then for every \(\epsilon\in[0,(\sum_{i=1}^{n}\sigma_{i}^{2})/\sqrt{c_{1}}]\) with \(n_{v}<12\sigma^{2}\log(4c_{2})/\epsilon^{2}\), the following holds w.p. at least \(1/4\), \(\mathcal{E}_{a}(h,t|\mathcal{X})>\widehat{\mathcal{E}}_{a}(h,t|X)+\epsilon\)._

Therefore, if a sufficiently large validation set is not used in each round, there is a constant probability of erroneously deciding on a threshold for auto-labeling. Such a requirement on validation data also applies to active learning if we seek to validate the output model. Bypassing this requirement demands the use of approaches that are different from threshold-based auto-labeling and traditional validation techniques. We note the possibility of using recently proposed _active testing_ techniques [33], a nascent approach to reducing validation data usage.

### Linear Classifier Setting

Next, we consider a simple setting where active learning is known to be optimal to see if TBAL can offer similar performance guarantees. To do so, we instantiate results from 3.2 to homogeneous linear separators under the uniform distribution in the realizable setting. Let \(P_{\mathbf{x}}\) be supported on the unit ball in \(\mathbb{R}^{d}\), \(\mathcal{X}=\{\mathbf{x}\in\mathbb{R}^{d}:||\mathbf{x}||\leq 1\}\). Let \(\mathcal{W}=\{\mathbf{w}\in\mathbb{R}^{d}:||\mathbf{w}||_{2}=1\}=\mathbb{S}_{d}\), \(\mathcal{H}=\{\mathbf{x}\mapsto\text{sign}(\langle\mathbf{w},\mathbf{x}\rangle) \,\forall\mathbf{w}\in\mathcal{W}\}\), the score function be given by \(g(h,\mathbf{x})=g(\mathbf{w},\mathbf{x})=|\langle\mathbf{w},\mathbf{x}\rangle|\), and set \(T=[0,1]\). For simplicity, we will use \(\mathcal{W}\) in place of \(\mathcal{H}\).

**Corollary 3.4**.: _(Overall Auto-Labeling Error and Coverage) Let \(\hat{\mathbf{w}}_{i},\hat{t}_{i}\) be the ERM solution and the auto-labeling threshold respectively at epoch \(i\). Let \(n_{v}^{(i)},n_{a}^{(i)}\) denote the number of validation and auto-labeled points at epoch \(i\). Let the TBAL algorithm run for \(k\)-epochs. Then, for any \(\delta\in(0,1)\), w.p. at least \(1-\delta\),_

\[\widehat{\mathcal{E}}\big{(}X_{pool}(A_{k})\big{)}\leq\sum_{i=1}^{ k}\frac{n_{b}^{(i)}}{N_{a}^{k}}\bigg{(}\underbrace{\widehat{\mathcal{E}}_{a} \big{(}\hat{\mathbf{w}}_{i},\hat{t}_{i}|X_{v}^{(i)}\big{)}}_{(a)}+\underbrace{ \frac{4}{p_{0}}\sqrt{\frac{2}{n_{v}^{(i)}}\Big{(}2d\log\big{(}\frac{\epsilon n _{v}^{(i)}}{d}\big{)}+\log\big{(}\frac{8k}{\delta}\big{)}\Big{)}}}_{(b)}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\frac{4}{\frac{ 4}{p_{0}}\left(\sqrt{\frac{2k}{N_{a}^{(k)}}\Big{(}2d\log\big{(}\frac{\epsilon N _{b}^{(k)}}{d}\big{)}+\log\big{(}\frac{8k}{\delta}\big{)}\Big{)}}\right)}, \quad and\] \[\widehat{\mathcal{P}}\big{(}X_{pool}(A_{k})\big{)}\geq 1-\min_{i}\hat{t}_{i} \sqrt{4d/\pi}-2k\sqrt{\frac{2}{N}\Big{(}2d\log\big{(}\frac{\epsilon N}{d} \big{)}+\log\big{(}\frac{8k}{\delta}\big{)}\Big{)}}.\]

These results imply that by ensuring the sum of the empirical validation error term (a) and the upper confidence interval to be less than \(\epsilon_{a}\) in each round of the algorithm we can ensure that the overall auto-labeling error remains below \(\epsilon_{a}\). Furthermore, by applying standard VC theory to the first round, we obtain that \(\hat{t}_{1}\leq 1/2\). Therefore, right after the first round, we are guaranteed to label at least half of the unlabeled pool. We empirically observe that TBAL has coverage at par with active learning while respecting the auto-labeling error constraint (See Figure 4(a)).

Tightness of the Bounds.We study this in the setting of the Unit-Ball experiment. The upper bound on excess risk in this setting is given in Corollary 3.4 which is an instantiation of our general results to this specific setting. We consider a simplified form of the upper bound by ignoring the constants to get a sense of the rate in terms of the validation data size. We compute this simplified upper bound for different amounts of validation data. We compare these with the maximum auto-labeling error observed over 25 runs of auto-labeling in the Unit-Ball setting with different random seeds for each validation data size. The results are in Figure 3. As expected, we see that the worst-case error rate follows a similar rate as our upper bound but the upper bound is conservative. Next, we explain why this is the case.

Our upper bound is slightly conservative, as it is based on a uniform bound over all hypotheses in a given hypothesis class. Since the individual hypotheses whose excess auto-labeling error we need to bound are not known a priori we need to derive a bound on the number of validation samples using which we can guarantee that the excess auto-labeling error of any hypothesis (model) is small with high probability. Note that this is a conservative (worst-case) analysis to get an upper bound on the validation sample complexity. The upper bound has two parts: a) the Rademacher complexity of the hypothesis class and b) a term with the number of validation samples. We note that on the validation samples, it matches lower bounds order-wise (see Lemma 3.3). This is the first analysis to provide these bounds based on uniform convergence without making any assumptions about the data distributions or hypothesis class. We provide further discussion on the role of Rademacher complexity in the Appendix C.1.

## 4 Experiments

We study the effectiveness of TBAL on synthetic and real datasets. We validate our theoretical results and aim to understand the amount of labeled validation and training data required to achieve a certain auto-labeling error and coverage. We also seek to understand whether our findings apply to real data--where labels may be noisy--along with how TBAL performs compared to common baselines.

**Baselines.** We compare TBAL to the following methods:,

1. [leftmargin=0pt,itemsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,parsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsep=0pt,topsep=0pt,topsep=0pt,topsepsep=0pt,topsepsep=0pt,topsepsep=0pt,topsepsep=0pt,topsepsep=0pt,topsepsepsep=0pt,topsepsep=0pt,topsep

**Datasets.** We use the following synthetic and real datasets. We also provide empirical results on MNIST and another synthetic dataset in the Appendix. For each dataset, we split the data into two sufficiently large pools. One is used as \(X_{pool}\) on which auto-labeling algorithms are run and the other is used as \(X_{val}\) from which the algorithms subsample validation data.

1. _Unit-Ball_ is a synthetic dataset of uniformly sampled points from the \(d\)-dimensional unit ball. The true labels are generated using a homogeneous linear separator with \(\mathbf{w}=[1/\sqrt{d},\ldots,1/\sqrt{d}]\). We use \(d=30\) and generate \(N=20K\) samples, out of which \(16K\) are in \(X_{pool}\) and \(4K\) are in \(X_{val}\). The dataset has just two classes but there is no margin between them.
2. _Tiny-ImageNet_[1] is a subset of the larger ImageNet [14] dataset, designed for image classification tasks. It consists of _200 classes_, each with \(500\) training images and 50 validation and test images. With a total of \(100K\) images, Tiny ImageNet provides a diverse and challenging dataset. We use pre-computed embeddings of the images using CLIP [50].
3. _IMDB Reviews_[41] is a comprehensive collection of movie reviews, consisting of \(50K\) individual reviews. It is a balanced dataset of positive and negative labels. We use the standard train set of size \(25K\) and split it into \(X_{pool}\) and \(X_{val}\) of sizes \(20K\) and \(5K\) respectively. We compute embeddings of reviews using a pre-trained model bge-large-en[70] from the Massive Text Embedding Benchmark (MTEB) [44, 19].
4. _CIFAR-10_[36] is an image dataset with \(10\) classes. We randomly split the standard training set into \(X_{pool}\) of size \(40K\) and the validation pool of size \(10K\). We use the raw features for training.

**Models and Training.** For the linear models, we use SVM with the usual hinge loss and train it to loss tolerance \(10^{-5}\). To train a multi-layer perceptron (MLP) on the pre-computed embeddings of IMDB and Tiny-ImageNet we use SGD with a learning rate of \(0.05,0.1\) respectively, and batch size of 64. To train the medium CNN we use SGD with a learning rate of \(10^{-2}\), batch size 256, and momentum of 0.9. More details on model training are in the Appendix.

**The score function \(g\).** For SVMs we use the standard implementations of [69, 46] in sklearn to get the prediction probabilities and use them as the score function. Neural networks use softmax output.

### Role of Validation Data

The TBAL algorithm uses validation data to estimate the auto-labeling errors at various thresholds to determine the threshold for automatically labeling points accurately. Thus, it is crucial to have

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline \multirow{2}{*}{\(\mathbf{N_{v}}\)} & \multicolumn{2}{c}{**Error (\%)**} & \multicolumn{2}{c}{**Coverage (\%)**} \\ \cline{2-5}  & **TBAL** & **AL+SC** & **TBAL** & **AL+SC** \\ \hline
100 & 3.10 \(\pm\)1.80 & 0.68 \(\pm\)2.01 & 71.43 \(\pm\)5.84 & 96.95 \(\pm\)1.01 \\ \hline
400 & 1.97 \(\pm\)0.78 & 0.59 \(\pm\)0.18 & 93.99 \(\pm\)1.29 & 97.89 \(\pm\)0.50 \\ \hline
800 & 1.64 \(\pm\)0.98 & 0.66 \(\pm\)2.01 & 96.26 \(\pm\)1.33 & 98.06 \(\pm\)0.93 \\ \hline
1200 & 1.39 \(\pm\)0.98 & 0.67 \(\pm\)2.01 & 96.67 \(\pm\)0.94 & 98.10 \(\pm\)0.94 \\ \hline
1600 & 1.33 \(\pm\)0.98 & 0.70 \(\pm\)0.91 & 97.13 \(\pm\)0.95 & 98.16 \(\pm\)0.94 \\ \hline
2000 & 1.28 \(\pm\)0.34 & 0.71 \(\pm\)0.21 & 97.15 \(\pm\)0.94 & 98.20 \(\pm\)0.94 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Unit-Ball.** Effect of variation of validation data size (\(N_{v}\)) with and without using a UCB on error estimates. We keep training data size \(N_{q}\) fixed at \(500\) and use error threshold \(\epsilon_{a}=1\%\). We report the mean and std. deviation over 10 runs with different random seeds. **Left:** with \(C_{1}=0\). **Right:** with \(C_{1}=0.25\).

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline \multirow{2}{*}{\(\mathbf{N_{v}}\)} & \multicolumn{2}{c}{**Error (\%)**} & \multicolumn{2}{c}{**Coverage (\%)**} \\ \cline{2-5}  & **TBAL** & **AL+SC** & **TBAL** & **AL+SC** \\ \hline
200 & 2.28 \(\pm\)0.21 & 3.11 \(\pm\)0.68 & 68.24 \(\pm\)5.38 & 57.77 \(\pm\)1.50 \\ \hline
400 & 1.29 \(\pm\)0.10 & 1.98 \(\pm\)0.69 & 63.81 \(\pm\)4.86 & 63.06 \(\pm\)1.70 \\ \hline
600 & 1.41 \(\pm\)0.92 & 1.81 \(\pm\)0.22 & 69.64 \(\pm\)1.36 & 62.92 \(\pm\)2.39 \\ \hline
800 & 1.62 \(\pm\)0.30 & 2.04 \(\pm\)0.35 & 67.45 \(\pm\)3.72 & 63.22 \(\pm\)7.89 \\ \hline
1000 & 1.64 \(\pm\)0.23 & 1.97 \(\pm\)0.58 & 70.28 \(\pm\)2.52 & 66.11 \(\pm\)8.00 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **IMDB.** Effect of variation of validation data size (\(N_{v}\)) with and without using a UCB on error estimates. We keep training data size \(N_{q}\) fixed at \(500\) and use error threshold \(\epsilon_{a}=5\%\). We report the mean and std. deviation over 10 runs with different random seeds. **Left:** with \(C_{1}=0\). **Right:** with \(C_{1}=0.25\).

[MISSING_PAGE_FAIL:9]

**Setup.** We limit the amount of training data the algorithm can use and record the resulting auto-labeling error and coverage. We ensure all algorithms have sufficiently large but equal amounts of validation data. We run on Unit-Ball, IMDB, Tiny-Imagenet, and CIFAR-10 datasets with the same values of \(n_{s}\), \(n_{b}\), and \(C\) as in previous experiments.

**Results.** Figures 4(a), 4(b), and 2(b) indicate that TBAL and methods utilizing selective classification (AL+SC, PL+SC) maintain a high level of accuracy, even in scenarios where minimal training samples are used. This is expected as the threshold estimation method (when used with sufficient validation data) will find auto-labeling thresholds such that the auto-labeling error does not exceed \(\epsilon_{a}\). The impact of training data size can be seen clearly in the coverage achieved by the algorithms. As expected, with fewer training samples the model has low accuracy leading to low coverage. However, as more samples are acquired, a more accurate model within the function class is learned, resulting in increased coverage. The Appendix D has additional discussion and results.

## 5 Related Work

We briefly review related work, deferring a more detailed discussion to the Appendix A.

There is a rich body of work on active learning (AL) [57; 12; 30; 28; 8] focused on learning the best model in a function class with less labeled data than passive learning. Various AL algorithms have been developed and analyzed, e.g., uncertainty sampling [62; 45], disagreement region based [9; 27], margin based [4] and abstention based methods that minimize the Chow's excess risk [72].

Selective classification (SC) equips a given classifier with the option to abstain from prediction in order to guarantee prediction quality. The foundations for SC are laid down in [17; 67; 18; 68] where results on the error rate in the prediction region and the coverage of the given classifier are provided. However, these works lack practical algorithms to find the prediction region. A recent work [26] gives a disagreement-based active learning strategy to learn a selective classifier.

A recent paper [49] studies a TBAL-like algorithm for auto-labeling. It focuses on the cost of training incurred when these systems use large-scale model classes for auto-labeling. It proposes an algorithm to predict the training set size that minimizes the overall cost and provides an empirical evaluation.

Weak supervision is another line of work aimed at auto-labeling that does not rely on obtaining human labels but instead uses potentially noisy but cheaply available sources to infer labels [51; 22]. In contrast, we are focused specifically on analyzing the performance of TBAL algorithms [58].

## 6 Conclusion and Future Work

In this work, we analyzed threshold-based auto-labeling systems and derived sample complexity bounds on the amount of human-labeled validation data required to guarantee the quality of machine-labeled data. Our study shows that these methods can accurately label a reasonable size of data using seemingly bad models when good confidence functions are available. Our analysis points to the hidden downside of these systems in terms of a large amount of validation data usage and calls for more sample-efficient methods including active testing.

## 7 Acknowledgments

This work was partly supported by funding from the American Family Data Science Institute. We thank Stephen Mussmann, Changho Shin, Albert Ge, Yi Chen, Kendall Park, and Nick Roberts for their valuable inputs. We thank the anonymous reviewers for their valuable comments and constructive feedback on our work.

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline \multirow{2}{*}{\(\mathbf{N_{q}}\)} & \multicolumn{2}{c}{**Error (\%)**} & \multicolumn{2}{c}{**Coverage (\%)**} \\ \cline{2-5}  & **TBAL** & **AL+SC** & **TBAL** & **AL+SC** \\ \hline
200 & 1.67 \(\pm\)0.23 & 2.15 \(\pm\)0.04 & 73.30 \(\pm\)0.04 & 57.17 \(\pm\)11.00 \\ \hline
400 & 1.63 \(\pm\)0.18 & 1.61 \(\pm\)0.23 & 72.59 \(\pm\)3.16 & 64.53 \(\pm\)0.04 \\ \hline
600 & 1.67 \(\pm\)0.21 & 1.83 \(\pm\)0.20 & 71.38 \(\pm\)2.19 & 70.50 \(\pm\)5.08 \\ \hline
800 & 1.67 \(\pm\)0.27 & 1.90 \(\pm\)0.13 & 69.10 \(\pm\)4.51 & 65.74 \(\pm\)0.14 \\ \hline
1000 & 1.62 \(\pm\)0.22 & 1.97 \(\pm\)0.35 & 73.42 \(\pm\)2.84 & 68.05 \(\pm\)5.56 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results for varying \(N_{q}\), the maximum number of samples algorithm can use for training. **Left: IMDB** with \(N_{v}=1000,C_{1}=0.25,\epsilon_{a}=5\%\). **Right: Tiny-ImageNet** with \(N_{v}=10K,C_{1}=0.25,\epsilon_{a}=10\%\). Mean and std. deviations are reported.

## References

* [1] Tiny imagenet dataset. http://cs231n.stanford.edu/tiny-imagenet-200.zip. Accessed: May 16, 2023.
* [2] Airbus. Airbus active labeling blog. https://acubed.airbus.com/blog/wayfinder/automatic-data-labeling-strategies-for-vision-based-machine-learning-and-ai/, 2022. Accessed: 2022-11-18.
* [3] Maria-Florina Balcan, Andrei Z. Broder, and Tong Zhang. Margin based active learning. In _COLT_, 2007.
* [4] Maria-Florina Balcan and Phil Long. Active and passive learning of linear separators under log-concave distributions. In _Conference on Learning Theory_, pages 288-316. PMLR, 2013.
* [5] Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In _Proceedings of the 26th annual international conference on machine learning_, pages 49-56, 2009.
* [6] Kamalika Chaudhuri, Sham M. Kakade, Praneeth Netrapalli, and Sujay Sanghavi. Convergence rates of active learning for maximum likelihood estimation. In _Proceedings of the 28th International Conference on Neural Information Processing Systems_, 2015.
* [7] C Chow. On optimum recognition error and reject tradeoff. _IEEE Transactions on information theory_, 16(1):41-46, 1970.
* [8] Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin Rostamizadeh, and Sanjiv Kumar. Batch active learning at scale. In _Advances in Neural Information Processing Systems_, volume 34, pages 11933-11944, 2021.
* [9] David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. _Machine Learning_, 15(2):201-221, 1994.
* [10] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In _International Conference on Algorithmic Learning Theory_, pages 67-82. Springer, 2016.
* [11] Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. In Y. Weiss, B. Scholkopf, and J. Platt, editors, _Advances in Neural Information Processing Systems_, volume 18. MIT Press, 2006.
* [12] Sanjoy Dasgupta. Two faces of active learning. _Theoretical Computer Science_, 412(19):1767-1781, 2011. Algorithmic Learning Theory (ALT 2009).
* [13] Sanjoy Dasgupta, Adam Tauman Kalai, and Claire Monteleoni. Analysis of perceptron-based active learning. In _International conference on computational learning theory_, pages 249-263. Springer, 2005.
* [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pages 248-255, 2009.
* [15] Li Deng. The mnist database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.
* [16] Giulia DeSalvo, Mehryar Mohri, and Umar Syed. Learning with deep cascades. In _Proceedings of the Twenty-Sixth International Conference on Algorithmic Learning Theory (ALT 2015)_, 2015.
* [17] Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. _JMLR_, 11:1605-1641, aug 2010.
* [18] Ran El-Yaniv and Yair Wiener. Active learning via perfect selective classification. _Journal of Machine Learning Research_, 13(2), 2012.
* [19] Hugging Face. Hugging face mteb leaderboard. https://huggingface.co/spaces/mteb/leaderboard, 2023. Accessed: 2023-08-07.

* [20] William Feller. Generalization of a probability limit theorem of cramer. In _Transactions of the American Mathematical Society_, pages 361-372,, 1943.
* [21] S. Fralick. Learning to recognize patterns without a teacher. _IEEE Transactions on Information Theory_, 13(1):57-64, 1967.
* [22] Daniel Y. Fu, Mayee F. Chen, Frederic Sala, Sarah M. Hooper, Kayvon Fatahalian, and Christopher Re. Fast and three-rious: Speeding up weak supervision with triplet methods. In _Proceedings of the 37th International Conference on Machine Learning (ICML 2020)_, 2020.
* [23] Olivier Gascuel and Gilles Caraux. Distribution-free performance bounds with the resubstitution error estimate. _Pattern Recognition Letters_, 13(11):757-764, 1992.
* [24] Jakob Gawlikowski, Cedrique Rovile Nijieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. _arXiv preprint arXiv:2107.03342_, 2021.
* [25] Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* [26] Roei Gelbhart and Ran El-Yaniv. The relationship between agnostic selective classification, active learning and the disagreement coefficient. _The Journal of Machine Learning Research_, 20(1):1136-1173, 2019.
* [27] Steve Hanneke. A bound on the label complexity of agnostic active learning. ICML, 2007.
* [28] Steve Hanneke. Theory of disagreement-based active learning. _Found. Trends Mach. Learn._, 7(2-3):131-309, jun 2014.
* [29] Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-confidence predictions far away from the training data and how to mitigate the problem. 2018.
* [30] Daniel Joseph Hsu. _Algorithms for active learning_. PhD thesis, UC San Diego, 2010.
* [31] Matti Kaariainen. Active learning in the non-realizable case. In _International Conference on Algorithmic Learning Theory_, pages 63-77. Springer, 2006.
* [32] Julian Katz-Samuels, Jifan Zhang, Lalit Jain, and Kevin Jamieson. Improved algorithms for agnostic pool-based active classification. In _International Conference on Machine Learning_, pages 5334-5344. PMLR, 2021.
* [33] Jannik Kossen, Sebastian Farquhar, Yarin Gal, and Tom Rainforth. Active testing: Sample-efficient model evaluation. _International Conference on Machine Learning_, 2021.
* [34] Akshay Krishnamurthy, Alekh Agarwal, Tzu-Kuo Huang, Hal Daume III, and John Langford. Active learning for cost-sensitive classification. In _International Conference on Machine Learning_, pages 1915-1924. PMLR, 2017.
* [35] Ranganath Krishnan and Omesh Tickoo. Improving model calibration with accuracy versus uncertainty optimization. _Advances in Neural Information Processing Systems_, 33:18237-18248, 2020.
* [36] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [37] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [38] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. _Predicting structured data_, 1(0), 2006.
* [39] Michel Ledoux and Michel Talagrand. Probability in banach spaces. 1991.
* [40] Chunwei Ma, Ziyun Huang, Jiayi Xian, Mingchen Gao, and Jinhui Xu. Improving uncertainty calibration of deep neural networks via truth discovery and geometric optimization. In _Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence_, volume 161 of _Proceedings of Machine Learning Research_, pages 75-85. PMLR, 27-30 Jul 2021.

* [41] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, pages 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
* [42] Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. In _Advances in Neural Information Processing Systems_, volume 34, pages 15682-15694, 2021.
* [43] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of Machine Learning_. The MIT Press, 2012.
* [44] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. _arXiv preprint arXiv:2210.07316_, 2022.
* [45] Stephen Mussmann and Percy Liang. On the relationship between data efficiency and error for uncertainty sampling. In _Proceedings of the 35th International Conference on Machine Learning, ICML_, 2018.
* [46] John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. _Advances in large margin classifiers_, 10(3):61-74, 1999.
* [47] Nikita Puchkin and Nikita Zhivotovskiy. Exponential savings in agnostic active learning through abstention. Proceedings of Machine Learning Research, pages 3806-3832. PMLR, 2021.
* [48] PyTorch. Cifar-10 pytorch tutorial. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html, 2022. Accessed: 2022-11-18.
* [49] Hang Qiu, Krishna Chintalapudi, and Ramesh Govindan. Minimum cost active labeling. _arXiv preprint arXiv:2006.13999_, 2020.
* [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.
* [51] A. J. Ratner, Christopher M. De Sa, Sen Wu, Daniel Selsam, and C. Re. Data programming: Creating large training sets, quickly. In _Proceedings of the 29th Conference on Neural Information Processing Systems (NIPS 2016)_, Barcelona, Spain, 2016.
* [52] Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Re. Snorkel: Rapid training data creation with weak supervision. In _Proceedings of the 44th International Conference on Very Large Data Bases (VLDB)_, Rio de Janeiro, Brazil, 2018.
* [53] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster, stronger. In _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6517-6525, 2017.
* [54] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B. Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning, 2020.
* [55] Samsung-SDS. Samsung sds auto-labeling service. https://www.samsungsds.com/en/insights/\(\backslash\)TechToolkit_201_Auto_Labeling.html, 2022. Accessed: 2022-11-18.
* [56] Nabeel Seedat and Christopher Kanan. Towards calibrated and scalable uncertainty representations for neural networks. _arXiv preprint arXiv:1911.00104_, 2019.
* [57] Burr Settles. Active learning literature survey. 2009.
* [58] SGT. Aws sagemaker ground truth. https://aws.amazon.com/sagemaker/data-labeling/, 2022. Accessed: 2022-11-18.
* [59] Kulin Shah and Naresh Manwani. Online active learning of reject option classifiers. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(04):5652-5659, 2020.
* [60] Shubhanshu Shekhar, Mohammad Ghavamzadeh, and Tara Javidi. Active learning for classification with abstention. _IEEE Journal on Selected Areas in Information Theory_, 2(2):705-719, 2021.

* [61] Superb-AI. Superb ai automated data labeling service. https://www.superb-ai.com/product/automate, 2022. Accessed: 2022-11-18.
* [62] Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classification. _Journal of machine learning research_, 2(Nov):45-66, 2001.
* [63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS 2017)_, 2017.
* [64] Rudy Venguswamy, Mike Levy, Anirudh Koul, Satyarth Praveen, Tarun Narayanan, Ajay Krishnan, Jenessa Peterson, Siddha Ganju, and Meher Kasam. Curator: A No-Code Self-Supervised Learning and Active Labeling Tool to Create Labeled Image Datasets from Petabyte-Scale Imagery. In _EGU General Assembly Conference Abstracts_, pages EGU21-6853, April 2021.
* [65] Deng-Bao Wang, Lei Feng, and Min-Ling Zhang. Rethinking calibration of deep neural networks: Do not be afraid of overconfidence. In _Advances in Neural Information Processing Systems_, volume 34, pages 11809-11820, 2021.
* [66] Yingfan Wang, Haiyang Huang, Cynthia Rudin, and Yaron Shaposhnik. Understanding how dimension reduction tools work: An empirical approach to deciphering t-sne, umap, trimap, and pacmap for data visualization. _Journal of Machine Learning Research_, 22(201):1-73, 2021.
* [67] Yair Wiener and Ran El-Yaniv. Agnostic selective classification. _Advances in neural information processing systems_, 24, 2011.
* [68] Yair Wiener and Ran El-Yaniv. Agnostic pointwise-competitive selective classification. _Journal of Artificial Intelligence Research_, 52:171-201, 2015.
* [69] Ting-Fan Wu, Chih-Jen Lin, and Ruby Weng. Probability estimates for multi-class classification by pairwise coupling. _Advances in Neural Information Processing Systems_, 16, 2003.
* [70] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources to advance general chinese embedding, 2023.
* [71] Yufan Zeng and Jiashan Tang. Rlc-gnn: An improved deep architecture for spatial-based graph neural network with application to fraud detection. _Applied Sciences_, 11, 06 2021.
* [72] Yinglun Zhu and Robert Nowak. Efficient active learning with abstention. _arXiv preprint arXiv:2204.00043_, 2022.

## Supplementary Material

The supplementary material is organized as follows. We provide a detailed discussion on related works in Section A. We give details of the definitions and notation in Section B.1. The notations are also summarized in the Table 5 in section B. Then we give the proof of the main theorem (Theorem 3.2) followed by proofs of supporting lemmas in section C. We provide details of its instantiation for finite VC-dimension hypothesis classes and the homogeneous linear separators case in Section C.3. Then, we provide the technical details of the lower bound (Lemma 3.3). Then we provide details of additional experiments in Section D. In Section D.4 we provide insights into auto-labeling using PaCMAP [66] visualizations of auto-labeled regions in each round.

## Appendix A Extended Related Work

There is a rich body of work on active learning on empirical and theoretical fronts [57, 12, 30, 28, 8, 54]. In active learning, the goal is to learn the best model in the given function class with fewer labeled data than in classical passive learning. To this end, various active learning algorithms have been developed and analyzed, e.g., uncertainty sampling [62, 45], disagreement region based [9, 27], margin based [3, 4], importance sampling based [5] and others [6]. Active learning has been shown to achieve exponentially smaller label complexity than passive learning in noiseless and low-noise settings [13, 3, 27, 28, 4, 11, 30, 6, 34, 32]. This suggests, in these settings auto-labeling using active learning followed by selective classification is expected to work well. However, in practice we do not have favorable noise conditions and the hypothesis class could be misspecified i.e. it may not contain the Bayes optimal classifier. In such cases, [31] proved lower bounds on the label complexity of active learning that are order wise same as passive learning. These findings have motivated more refined goals for active learning - abstain on hard to classify points and do well on the rest of the points. This idea is captured by the Chow's excess risk [7] and some of the recent works [60, 59, 47, 72] have proved exponential savings in label complexity for active learning when the goal is to minimize Chow's excess risk. The classifier learned by these methods is equipped with the abstain option and hence it can be readily applied for auto-labeling. However, the problem of misspecification of the hypothesis class still remains. Nevertheless, it would be interesting future work to explore the connections between auto-labeling and active learning with abstention. We also note that similar works on learning with abstention are done in the context of passive learning [10].

Another closely related line of work is the selective classification where the goal is to equip a given classifier with the option to abstain from the prediction in order to guarantee prediction quality. The foundations for selective classification are laid down in [17, 67, 18, 68] where they give results on the error rate in the prediction region and the coverage of a given classifier. However, they lack practical algorithms to find the prediction region. A recent work [26] proposes a new disagreement-based active learning strategy to learn a selective classifier.

Recent work studies a practical algorithm for threshold-based selective classification on deep neural networks [25]. The algorithm estimates the prediction threshold using training samples and they bound the error rate of the selective classifier using [23]. We note that their result is applicable to a specific setting of a given classifier. In contrast, in the TBAL algorithm analyzed in this paper, selective classification is done in each round and the classifiers are not given a priori but instead learned via ERM on training data which is adaptively sampled in each round.

Another related work [49] studies an algorithm similar to TBAL for auto-labeling. Their emphasis is on the cost of training incurred when these systems use large-scale model classes for auto-labeling. They propose an algorithm to predict the training set size that minimizes the overall cost and provides an empirical evaluation.

Well-calibrated uncertainty scores are essential to the success of threshold-based auto-labeling. However, in practice, such scores are often hard to get. Moreover, neural networks can produce overconfident ( unreliable) scores [29]. Fortunately, there are plenty of methods in the literature to deal with this problem [46, 69]. More recently, various approaches have been proposed for uncertainty calibration for neural networks [24, 42, 65, 35, 40, 56]. A detailed study of calibration methods and their impact on auto-labeling is beyond the scope of this work and left as future work.

There is another line of work emerging towards auto-labeling that does not rely on getting human labels but instead uses potentially noisy but cheaply available sources to infer labels [51, 52, 22]. The focus of this paper, however, is on analyzing the performance of TBAL algorithms [58, 2] that have emerged recently as auto-labeling solutions in systems.

Definitions and Notation

### Basic Definitions

Let \(\mathcal{X}\) be the instance space and \(p(\mathbf{x})\) be a density function supported on \(\mathcal{X}\). For any \(\mathbf{x}_{i}\in\mathcal{X}\) let \(y_{i}\) be its true label. Let \(X=\{\mathbf{x}_{1},\ldots,\mathbf{x}_{N}\}\) be a set of \(N\) i.i.d samples drawn from \(\mathcal{X}\). Let set \(\mathcal{S}\subseteq\mathcal{X}\) denote a non-empty sub-region of \(\mathcal{X}\) and \(S\subseteq X\cap\mathcal{S}\) be a set of \(n>0\) i.i.d. samples.

**Definition B.1**.: (Hypothesis Class with Abstain) We can think of the function \(g\) along with set \(T\) as inducing an extended hypothesis class \(\mathcal{H}^{(T,g)}\). Let \(\mathcal{H}^{T,g}=\mathcal{H}\times T\). For any function \((h,t)\in\mathcal{H}^{(T,g)}\) is defined as

\[(h,t)(\mathbf{x}):=\begin{cases}h(\mathbf{x})&\quad\text{if }g(h,\mathbf{x}) \geq t\\ \bot&\quad\text{o.w.}\end{cases}\] (3)

Here \((h,t)(\mathbf{x})=\perp\) means the hypothesis \((h,t)\) abstains in classifying the point \(\mathbf{x}\). Otherwise, it is equal to \(h(\mathbf{x})\).

The subset \(\mathcal{S}(h,t)\subseteq\mathcal{S}\) where \((h,t)\) does not abstain and its complement \(\bar{\mathcal{S}}(h,t)\) where \((h,t)\) abstains, are defined as follows,

\[\mathcal{S}(h,t):=\{\mathbf{x}\in\mathcal{S}:(h,t)(\mathbf{x})\neq\perp\}, \qquad\bar{\mathcal{S}}(h,t):=\{\mathbf{x}\in\mathcal{S}:(h,t)(\mathbf{x})=\perp\}\]

Probability Definitions.The probability \(\mathbb{P}(\mathcal{S})\) of subset \(\mathcal{S}\subseteq\mathcal{X}\) and the conditional probability of any subset \(\mathcal{S}^{\prime}\subseteq\mathcal{S}\) are given as follows,

\[\mathbb{P}(\mathcal{S}):=\mathbb{P}(\mathcal{S}|\mathcal{X}):=\int_{\mathbf{ x}\in\mathcal{S}}p(\mathbf{x})d\mathbf{x},\qquad\mathbb{P}(\mathcal{S}^{\prime}| \mathcal{S}):=\frac{\mathbb{P}(\mathcal{S}^{\prime}|\mathcal{X})}{\mathbb{P}( \mathcal{S}|\mathcal{X})},\qquad\mathbb{P}(h,t|\mathcal{S}):=\mathbb{P}( \mathcal{S}(h,t)|\mathcal{S})\]

The empirical probabilities of \(S\) and \(S^{\prime}\subseteq S\) are defined as follows,

\[\widehat{\mathbb{P}}(S):=\frac{|S|}{|X|},\qquad\widehat{\mathbb{P}}(S^{\prime }|S):=\frac{|S^{\prime}|}{|S|},\qquad\widehat{\mathbb{P}}(h,t|S):=\frac{|S(h, t)|}{|S|}\]

Loss Functions.The loss functions are defined as follows,

\[\ell_{0-1}(h,\mathbf{x},y) :=\mathds{1}\,(h(\mathbf{x})\neq y),\] \[\ell_{\perp}(h,t,\mathbf{x}) :=\mathds{1}\,(g(h,\mathbf{x})\geq t),\] \[\ell_{0-1}^{\perp}(h,t,\mathbf{x},y) :=\ell_{0-1}(h,\mathbf{x},y)\cdot\ell_{\perp}(h,t,\mathbf{x}).\]

Error Definitions.Define the conditional error in set \(\mathcal{S}\subseteq\mathcal{X}\) as follows,

\[\mathcal{E}(h,t|\mathcal{S}):=\mathbb{E}_{\mathbf{x}|\mathcal{S}}[\ell_{0-1}^ {\perp}(h,t,\mathbf{x},y)]=\int_{\mathbf{x}\in\mathcal{S}}\ell_{0-1}^{\perp} (h,t,\mathbf{x},y)\cdot\frac{p(\mathbf{x})}{\mathbb{P}(\mathcal{S})}d\mathbf{x}\]

Then, the conditional error in set \(\mathcal{S}(h,t)\) i.e. the subset of \(\mathcal{S}\) on which \((h,t)\) does not abstain,

\[\mathcal{E}_{a}(h,t|\mathcal{S}):=\mathcal{E}(h,t|\mathcal{S}(h,t)):=\mathbb{ E}_{\mathbf{x}|\mathcal{S}(h,t)}[\ell_{0-1}^{\perp}(h,t,\mathbf{x},y)]=\frac{ \mathcal{E}(h,t|\mathcal{S})}{\mathbb{P}(h,t|\mathcal{S})}\]

Similarly, define their empirical counterparts as follows,

\[\widehat{\mathcal{E}}(h,t|S) :=\frac{1}{|S|}\sum_{\mathbf{x}_{i}\in S}\ell_{0-1}^{\perp}(h,t, \mathbf{x}_{i},y_{i}),\] \[\widehat{\mathcal{E}}_{a}(h,t|S):=\widehat{\mathcal{E}}(h,t|S(h, t)) :=\frac{1}{|S(h,t)|}\sum_{\mathbf{x}_{i}\in S(h,t)}\ell_{0-1}^{\perp}(h,t,\mathbf{x} _{i},y_{i}),\]

Note that,

\[\sum_{\mathbf{x}_{i}\in S}\ell_{0-1}^{\perp}(h,t,\mathbf{x}_{i},y_{i})=\sum_{ \mathbf{x}_{i}\in S(h,t)}\ell_{0-1}^{\perp}(h,t,\mathbf{x}_{i},y_{i})=\sum_{ \mathbf{x}_{i}\in S(h,t)}\ell_{0-1}(h,\mathbf{x}_{i},y_{i})\]

**Rademacher Complexity.** The Rademacher complexities for the function classes induced by the \(\mathcal{H},T,g\) and the loss functions are defined as follows,

\[\mathfrak{R}_{n}\big{(}\mathcal{H},\ell_{0-1}\big{)} :=\mathbb{E}_{\sigma,S}\Big{[}\sup_{h\in\mathcal{H}}\frac{1}{n} \sum_{i=1}^{n}\sigma_{i}\ell_{0-1}(h,\mathbf{x}_{i},y_{i})\Big{]}.\] \[\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g},\ell_{\perp}\big{)} :=\mathbb{E}_{\sigma,S}\Big{[}\sup_{(h,t)\in\mathcal{H}^{T,g}} \frac{1}{n}\sum_{i=1}^{n}\sigma_{i}\ell_{\perp}(h,t,\mathbf{x}_{i})\Big{]}.\] \[\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g},\ell_{0-1}^{\perp}\big{)} :=\mathbb{E}_{\sigma,S}\Big{[}\sup_{(h,t)\in\mathcal{H}^{T,g}} \frac{1}{n}\sum_{i=1}^{n}\sigma_{i}\ell_{0-1}^{\perp}(h,t,\mathbf{x}_{i},y_{i} )\Big{]}.\] \[\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g}\big{)} :=\mathfrak{R}_{n}\big{(}\mathcal{H},\ell_{0-1}\big{)}+\mathfrak{ R}_{n}\big{(}\mathcal{H}^{T,g},\ell_{\perp}\big{)}\]

### Glossary

The notation is summarized in Table 5 below. More detailed notation is in section B.1.

\begin{table}
\begin{tabular}{l l} \hline \hline Symbol & Definition \\ \hline \(\mathcal{X}\) & feature space. \\ \(\mathcal{Y}\) & label space. \\ \(\mathcal{H}\) & hypothesis space. \\ \(h\) & a hypothesis in \(\mathcal{H}\). \\ \(\mathbf{x},y\) & \(\mathbf{x}\) is an element in \(\mathcal{X}\) and \(y\) is its true label. \\ \(\mathcal{S},S\) & \(\mathcal{S}\subseteq\mathcal{X}\) is a sub-region in \(\mathcal{X}\), \(S=\{\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\}\) i.i.d. samples in \(\mathcal{S}\). \\ \(X_{pool}\) & unlabeled pool of data points. \\ \(X_{v}^{(i)},n_{v}^{(i)}\) & set of validation points at the beginning of \(i\)th round and \(n_{v}^{(i)}=|X_{v}^{(i)}|\). \\ \(X_{a}^{(i)},n_{a}^{(i)}\) & set of auto-labeled points in \(i\)th round and \(n_{a}^{(i)}=|X_{a}^{(i)}|\). \\ \(\hat{h}_{i},\hat{t}_{i}\) & ERM solution and auto-labeling thresholds respectively in \(i\)th round. \\ \(\mathcal{X}^{(i)}\) & unlabeled region left at the beginning of \(i\)th round. \\ \(X^{(i)}\) & unlabeled pool left at the beginning of \(i\)th round. \\ \(m_{a}^{(i)}\) & number of auto-labeling mistakes in \(i\)th round. \\ \(k\) & number of rounds of the TBAL algorithm. \\ \(X_{pool}(A_{k})\) & set of all auto-labeled points till the end of round \(k\). \\ \(g\) & confidence function \(g:\mathcal{H}\times\mathcal{X}\mapsto T\). Where \(T\subseteq\mathbb{R}^{+}\), usually \(T=[0,1]\) \\ \(\mathcal{H}^{T,g}\) & Cartesian product of \(\mathcal{H}\) and \(T\) the range of \(g\). \\ \(N_{a}^{(k)}\) & \(\sum_{i=1}^{k}n_{a}^{(i)}\). \\ \(\ell_{0-1}(h,\mathbf{x},y)\) & \(\mathbbm{I}(h(\mathbf{x})\neq y)\). \\ \(\ell_{\perp}(h,t,\mathbf{x})\) & \(\mathbbm{I}(g(h,\mathbf{x})\geq t)\). \\ \(\ell_{0-1}^{\perp}(h,t,\mathbf{x},y)\) & \(\ell_{0-1}(h,\mathbf{x},y)\cdot\ell_{\perp}((h,t),\mathbf{x})\). \\ \(\mathfrak{R}_{n}(\mathcal{H},\ell_{0-1})\) & \(\mathbb{E}_{\sigma,S}\Big{[}\sup_{h\in\mathcal{H}}\frac{1}{n}\sum_{i=1}^{n} \sigma_{i}\ell_{0-1}(h,\mathbf{x}_{i},y_{i})\Big{]}\). \\ \(\mathfrak{R}_{n}(\mathcal{H}^{T,g},\ell_{\perp})\) & \(\mathbb{E}_{\sigma,S}\Big{[}\sup_{(h,t)\in\mathcal{H}^{T,g}}\frac{1}{n}\sum_{i=1 }^{n}\sigma_{i}\ell_{\perp}(h,t,\mathbf{x}_{i})\Big{]}\). \\ \(\mathfrak{R}_{n}(\mathcal{H}^{T,g},\ell_{0-1}^{\perp})\) & \(\mathbb{E}_{\sigma,S}\Big{[}\sup_{(h,t)\in\mathcal{H}^{T,g}}\frac{1}{n}\sum_{i=1 }^{n}\sigma_{i}\ell_{0-1}^{\perp}(h,t,\mathbf{x}_{i},y_{i})\Big{]}\). \\ \(\mathfrak{R}_{n}(\mathcal{H}^{T,g},\ell_{0-1}^{\perp})\) & \(\mathbb{E}_{\sigma,S}\Big{[}\sup_{(h,t)\in\mathcal{H}^{T,g}}\frac{1}{n}\sum_{i=1 }^{n}\sigma_{i}\ell_{0-1}^{\perp}(h,t,\mathbf{x}_{i},y_{i})\Big{]}\). \\ \(\mathfrak{R}_{n}(\mathcal{H}^{T,g},\ell_{0-1}^{\perp})\) & \(\mathfrak{R}_{n}(\mathcal{H},\ell_{0-1})+\mathfrak{R}_{n}(\mathcal{H}^{T,g},\ell_ {\perp})\). \\ \(\mathcal{E}(h,t|\mathcal{S})\) & \(\mathbb{E}_{\mathbf{x}|S}[\ell_{\perp}^{\perp}(h,t,\mathbf{x},y)]\). \\ \(\widehat{\mathcal{E}}(h,t|\mathcal{S})\) & \(\frac{1}{|S|}\sum_{i=1}^{|S|}\ell_{0-1}^{\perp}(h,t,\mathbf{x}_{i},y_{i})\). \\ \(\mathbb{Proofs

### Proofs for the General Setup

We begin by restating the theorem here and then give the proof.

**Theorem 3.2**.: _(Overall Auto-Labeling Error and Coverage) Let \(k\) denote the number of rounds of the TBAL Algorithm 1. Let \(n_{v}^{(i)},n_{a}^{(i)}\) denote the number of validation and auto-labeled points at epoch \(i\) and \(n^{(i)}=|X^{(i)}|\). Let \(X_{pool}(A_{k})\) be the set of auto-labeled points at the end of round \(k\). \(N_{a}^{(k)}=\sum_{i=1}^{k}n_{a}^{(i)}\) denote the total number of auto-labeled points. Then, for any \(\delta\in(0,1)\), with probability at least \(1-\delta\),_

\[\widehat{\mathcal{E}}\Big{(}X_{pool}(A_{k})\Big{)}\leq\sum_{i=1} ^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)}}\Bigg{(}\underbrace{\widehat{\mathcal{E}}_{ a}\Big{(}\hat{h}_{i},\hat{t}_{i}|X_{v}^{(i)}\Big{)}}_{(a)}+\frac{4}{R_{b}} \underbrace{\big{(}\mathfrak{R}_{n_{v}^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)} +\frac{2}{R_{b}}\sqrt{\frac{1}{n_{v}^{(i)}}\log(\frac{8k}{\delta})}\big{)}}_{(b )}\Bigg{)}\] \[+\frac{4}{R_{b}}\Bigg{(}\underbrace{\sum_{i=1}^{k}\frac{n_{a}^{(i )}}{N_{a}^{(k)}}\mathfrak{R}_{n_{a}^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)}+ \sqrt{\frac{k}{N_{a}^{(k)}}\log(\frac{8k}{\delta})}\Bigg{)}}_{(c)}\Bigg{)}, \quad and\]

\[\widehat{\mathcal{P}}(X_{pool}(A_{k}))\geq\sum_{i=1}^{k}\mathbb{P}\big{(} \mathcal{X}^{(i)}(\hat{h}_{i},\hat{t}_{i})\big{)}-2\mathfrak{R}_{n^{(i)}} \big{(}\mathcal{H}^{T,g}\big{)}-\sqrt{\frac{2k^{2}}{N}\log\Big{(}\frac{8k}{ \delta}\Big{)}}.\]

Proof.: Recall the definition of auto-labeling error,

\[\widehat{\mathcal{E}}\Big{(}X_{pool}(A_{k})\Big{)}=\sum_{i=1}^{k}\frac{m_{a}^ {(i)}}{N_{a}^{(k)}},\qquad m_{a}^{(i)}=n_{a}^{(i)}\cdot\widehat{\mathcal{E}}_ {a}\big{(}\hat{h}_{i},\hat{t}_{i}|X^{(i)}\big{)}.\]

Here, \(m_{a}^{(i)}\) is the number of auto-labeling mistakes made by the Algorithm in the \(i\)th round and \(\widehat{\mathcal{E}}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|X^{(i)}\big{)}\) is the auto-labeling error in that round. Note that we cannot observe these quantities since the true labels for the auto-labeled points are not available. To estimate the auto-labeling error of each round we make use of validation data. Using the validation data we first get an upper bound on the true error rate of the auto-labeling region i.e. \(\mathcal{E}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|\mathcal{X}^{(i)}\big{)}\) in terms of the auto-labeling error on the validation data \(\widehat{\mathcal{E}}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|X_{v}^{(i)}\big{)}\) and then get an upper bound on empirical auto-labeling error rate \(\widehat{\mathcal{E}}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|X^{(i)}\big{)}\) using the true error rate of the auto-labeling region.

We get these bounds by application of Lemma C.1 with \(\delta_{3}=\delta/4k\) for each round and then apply union bound over all \(k\) epochs. Note that we have to apply the lemma twice, first to get the concentration bound w.r.t the validation data and second to get the concentration w.r.t to the auto-labeled points.

\[\mathcal{E}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|\mathcal{X}^{(i)}\big{)}\leq \widehat{\mathcal{E}}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|X_{v}^{(i)}\big{)}+ \frac{4}{R_{b}}\mathfrak{R}_{n_{v}^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)}+\frac {2}{R_{b}}\sqrt{\frac{1}{n_{v}^{(i)}}\log\Big{(}\frac{8k}{\delta}\Big{)}}.\]

Substituting \(\mathcal{E}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|\mathcal{X}^{(i)}\big{)}\) by its upper confidence bound on the validation data.

\[\widehat{\mathcal{E}}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|X^{(i)} \big{)}\leq\widehat{\mathcal{E}}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|X_{v}^{(i)} \big{)}+\frac{2}{R_{b}}\mathfrak{R}_{n_{v}^{(i)}}\big{(}\mathcal{H}^{T,g} \big{)}+\frac{2}{R_{b}}\mathfrak{R}_{n_{a}^{(i)}}\big{(}\mathcal{H}^{T,g} \big{)}\] \[+\frac{2}{R_{b}}\sqrt{\frac{1}{n_{v}^{(i)}}\log\Big{(}\frac{8k}{ \delta}\Big{)}}+\frac{2}{R_{b}}\sqrt{\frac{1}{n_{a}^{(i)}}\log\Big{(}\frac{8k} {\delta}\Big{)}}.\]Having an upper bound on the empirical auto-labeling error for \(i^{th}\) round gives us an upper bound on the number of auto-labeling mistakes \(m_{a}^{(i)}\) made in that round. It allows us to upper bound the total auto-labeling mistakes in all \(k\) rounds and thus the overall auto-labeling error as detailed below,

\[\widehat{\mathcal{E}}\Big{(}X_{pool}(A_{k})\Big{)}=\sum_{i=1}^{k}\frac{m_{a}^{( i)}}{N_{a}^{(k)}},\qquad m_{a}^{(i)}=n_{a}^{(i)}\cdot\widehat{\mathcal{E}}_{a} \big{(}\hat{h}_{i},\hat{t}_{i}|X^{(i)}\big{)}.\]

Since we have an upper bound on the empirical auto-labeling error in each round, we have an upper bound for each \(m_{a}^{(i)}\), which are used as follows to get the bound on the auto-labeling error,

\[\widehat{\mathcal{E}}(X_{pool}(A_{k})) =\sum_{i=1}^{k}\frac{m_{a}^{(i)}}{N_{a}^{(k)}}\] \[=\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)}}\cdot\frac{m_{a}^{ (i)}}{n_{a}^{(i)}}\] \[=\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)}}\cdot\widehat{ \mathcal{E}}_{a}\Big{(}\hat{h}_{i},\hat{t}_{i}\big{|}X^{(i)}\Big{)}\] \[\leq\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)}}\cdot\bigg{(} \widehat{\mathcal{E}}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|\mathcal{X}^{(i)} \big{)}+\frac{4}{R}\mathfrak{R}_{n_{a}^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)} +\frac{2}{R}\sqrt{\frac{1}{n_{a}^{(i)}}\log\Big{(}\frac{8k}{\delta}\Big{)}} \bigg{)}\] \[\leq\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)}}\cdot\bigg{(} \widehat{\mathcal{E}}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|X^{(i)}_{v}\big{)}+ \frac{4}{R}\mathfrak{R}_{n_{v}^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)}+\frac{2}{ R}\sqrt{\frac{1}{n_{v}^{(i)}}\log\Big{(}\frac{8k}{\delta}\Big{)}}\] \[\qquad\qquad\qquad\qquad+\frac{4}{R}\mathfrak{R}_{n_{a}^{(i)}} \big{(}\mathcal{H}^{T,g}\big{)}+\frac{2}{R}\sqrt{\frac{1}{n_{a}^{(i)}}\log \Big{(}\frac{8k}{\delta}\Big{)}}\bigg{)}\] \[\leq\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)}}\cdot\bigg{(} \widehat{\mathcal{E}}_{a}\big{(}\hat{h}_{i},\hat{t}_{i}|X^{(i)}_{v}\big{)}+ \frac{4}{R}\mathfrak{R}_{n_{v}^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)}+\frac{4}{ R}\mathfrak{R}_{n_{a}^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)}+\] \[\qquad\qquad\qquad\qquad\qquad\frac{2}{R}\sqrt{\frac{1}{n_{v}^{( i)}}\log\Big{(}\frac{8k}{\delta}\Big{)}}\bigg{)}+\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a} ^{(k)}}\cdot\bigg{(}\frac{2}{R}\sqrt{\frac{1}{n_{a}^{(i)}}\log\Big{(}\frac{8k} {\delta}\Big{)}}\bigg{)}\]

The last term is simplified as follows,

\[\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)}}\cdot\bigg{(}\frac{2 }{R_{0}}\sqrt{\frac{1}{n_{a}^{(i)}}\log\Big{(}\frac{8k}{\delta}\Big{)}}\bigg{)} =\frac{2}{R_{0}}\cdot\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)} }\sqrt{\frac{1}{n_{a}^{(i)}}\log\Big{(}\frac{8k}{\delta}\Big{)}}\] \[=\frac{2}{R_{0}}\cdot\frac{1}{N_{a}^{(k)}}\sum_{i=1}^{k}\sqrt{n_{a }^{(i)}\log\Big{(}\frac{8k}{\delta}\Big{)}}\] \[=\frac{2}{R_{0}}\cdot\sqrt{\log\Big{(}\frac{8k}{\delta}\Big{)}} \cdot\sum_{i=1}^{k}\frac{\sqrt{n_{a}^{(i)}}}{N_{a}^{(k)}}\] \[\leq\frac{2}{R_{0}}\cdot\sqrt{\log\Big{(}\frac{8k}{\delta}\Big{)} }\cdot\sqrt{\frac{k}{N_{a}^{(k)}}}\] \[=\frac{2}{R_{0}}\cdot\sqrt{\frac{k}{N_{a}^{(k)}}\log\Big{(}\frac{8 k}{\delta}\Big{)}}\]

The last inequality follows from the application of the inequality \(||\mathbf{u}||_{1}\leq\sqrt{k}||\mathbf{u}||_{2}\) for any vector \(\mathbf{u}\in\mathbb{R}^{k}\). Here we let \(\mathbf{u}=[\sqrt{n}_{a}^{(1)},\ldots,\sqrt{n}_{a}^{(k)}]\), and since \(\forall i\ \sqrt{n_{a}^{(i)}}>0\) so, \(\sum_{i=1}^{k}\sqrt{n_{a}^{(i)}}=||\mathbf{u}||_{1}\)and \(N_{a}^{(k)}=\|\mathbf{u}\|_{2}^{2}\).

\[\frac{\sum_{i=1}^{k}\sqrt{n_{a}^{(i)}}}{N_{a}^{(k)}}=\frac{||\mathbf{u}| |_{1}}{||\mathbf{u}||_{2}^{2}}\leq\frac{\sqrt{k}||\mathbf{u}||_{2}}{||\mathbf{u }||_{2}^{2}}=\frac{\sqrt{k}}{||\mathbf{u}||_{2}}=\sqrt{\frac{k}{N_{a}^{(k)}}}\]

To get the bound on coverage we follow the same steps except that we can use all the unlabeled pool of size \(n^{(i)}\) to estimate the coverage in each round which gives us the bound in terms of \(n^{(i)}\) and \(N\) as follows,

\[\widehat{\mathcal{P}}(X_{pool}(A_{k})) =\frac{1}{N}\sum_{i=1}^{k}n_{a}^{(i)}\] \[=\frac{1}{N}\sum_{i=1}^{k}n^{(i)}\cdot\frac{n_{a}^{(i)}}{n^{(i)}}\] \[=\frac{1}{N}\sum_{i=1}^{k}n^{(i)}\cdot\widehat{\mathbb{P}}(X_{a}^ {(i)}\big{|}X^{(i)})\] \[=\frac{1}{N}\sum_{i=1}^{k}n^{(i)}\cdot\widehat{\mathbb{P}}(\hat{h }_{i},\hat{t}_{i}\big{|}X^{(i)})\] \[\geq\sum_{i=1}^{k}\frac{n^{(i)}}{N}\bigg{(}\mathbb{P}\big{(}\hat{ h}_{i},\hat{t}_{i}\big{|}\mathcal{X}^{(i)}\big{)}-2\mathfrak{R}_{n^{(i)}} \big{(}\mathcal{H}^{T,g}\big{)}-\sqrt{\frac{1}{n^{(i)}}\log\Big{(}\frac{8k}{ \delta}\Big{)}}\bigg{)}\] \[\geq\sum_{i=1}^{k}\frac{n^{(i)}}{N}\bigg{(}\mathbb{P}\big{(}\hat{ h}_{i},\hat{t}_{i}\big{|}\mathcal{X}^{(i)}\big{)}-2\mathfrak{R}_{n^{(i)}} \big{(}\mathcal{H}^{T,g}\big{)}\bigg{)}-\sqrt{\frac{k}{N}\log\Big{(}\frac{8k}{ \delta}\Big{)}}\]

We bound the first term as follows,

\[\sum_{i=1}^{k}\frac{n^{(i)}}{N}\mathbb{P}\big{(}\hat{h}_{i},\hat {t}_{i}\big{|}\mathcal{X}^{(i)}\big{)} =\sum_{i=1}^{k}\frac{n^{(i)}}{N}\cdot\frac{\mathbb{P}\big{(} \mathcal{X}^{(i)}(\hat{h}_{i},\hat{t}_{i})\big{)}}{\mathbb{P}\big{(} \mathcal{X}^{(i)}\big{)}}\] \[\geq\sum_{i=1}^{k}\Big{(}\mathbb{P}\big{(}\mathcal{X}^{(i)}(\hat{ h}_{i},\hat{t}_{i})\big{)}-\sqrt{\frac{1}{N}\log\Big{(}\frac{8k}{\delta} \Big{)}}\]

Substituting it back we get,

\[\widehat{\mathcal{P}}\big{(}X_{pool}(A_{k})\big{)} \geq\sum_{i=1}^{k}\mathbb{P}\big{(}\mathcal{X}^{(i)}(\hat{h}_{i}, \hat{t}_{i})\big{)}-2\mathfrak{R}_{n^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)}-k \sqrt{\frac{1}{N}\log\Big{(}\frac{8k}{\delta}\Big{)}}-\sqrt{\frac{k}{N}\log \Big{(}\frac{8k}{\delta}\Big{)}}\] \[\geq\sum_{i=1}^{k}\mathbb{P}\big{(}\mathcal{X}^{(i)}(\hat{h}_{i}, \hat{t}_{i})\big{)}-2\mathfrak{R}_{n^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)}- \sqrt{\frac{4k^{2}}{N}\log\Big{(}\frac{8k}{\delta}\Big{)}}\]

For the last step we use the inequality \(\sqrt{a}+\sqrt{b}\leq\sqrt{2(a+b)}\) for any \(a,b\in\mathbb{R}^{+}\). 

**Rademacher complexity and validation error trade-off.** The bound contains validation errors at different thresholds. We revisit the definition of validation error appearing in the bound. Let \(X_{v}=\{x_{1},\dots x_{n_{v}}\}\) be the validation samples and \(y_{i}\) be the label corresponding to \(x_{i}\). Given any \(h\in\mathcal{H}\) and the confidence function \(g\), we have different subsets \(X_{v}(h,t)\) of the validation points for which the model's confidence is higher than \(t\). More precisely, \(X_{v}(h,t)=\{x_{i}\in X_{v}:g(h,x_{i})\geq t\}\) and the validation error \(\hat{\mathcal{E}}_{a}(h,t|X_{v})\) is computed on each of these subsets as follows, \(\hat{\mathcal{E}}_{a}(h,t|X_{v})=\frac{1}{|X_{v}(h,t)|}\sum_{x_{i}\in X_{v}(h,t )}\mathbf{1}(h(x_{i})\neq y_{i})\) this error is different from the overall validation error which is computed over the entire set of validation points \(X_{v}\):

\[\hat{\mathcal{E}}(h|X_{v})=\frac{1}{|X_{v}|}\sum_{x_{i}\in X_{v}}\mathbf{1}(h( x_{i})\neq y_{i}).\]

The TBAL method computes \(\hat{\mathcal{E}}_{a}(h,t|X_{v})\) at different thresholds (\(t\)) and selects the threshold at which it is at most \(\epsilon\). Thus even if the overall validation error \(\hat{\mathcal{E}}(h|X_{v})\) is bad, there could still be regions in the space where the conditional validation error \(\hat{\mathcal{E}}_{a}(h,t|X_{v})\) is small. This can be easily seen in Figure 2 in the paper. Here we are doing auto-labeling using a linear function class that has low Rademacher complexity. All the models in this class have high overall validation error \(\hat{\mathcal{E}}(h|X_{v})\) but there are subsets where the conditional validation error \(\hat{\mathcal{E}}_{a}(h,t|X_{v})\) is small and TBAL is able to find those subsets. Thus we see that working with low Rademacher complexity classes might lead to high overall validation error. However, it does not affect TBAL as long as there are regions of low conditional validation error \(\hat{\mathcal{E}}_{a}(h,t|X_{v})\). Furthermore, our upper bound on excess auto-labeling error depends on \(\hat{\mathcal{E}}_{a}(h,t|X_{v})\) which due to the TBAL procedure is at most \(\epsilon\) and hence it is not in conflict with the Rademacher complexity term.

Next, we state the result for uniform convergence between \(\mathcal{E}_{a}\big{(}h,t|\mathcal{S}\big{)},\widehat{\mathcal{E}}_{a}\big{(} h,t|S\big{)}\) and give its proof.

**Lemma C.1**.: _For any \(\delta_{3},p_{\textsc{t}}\in(0,1)\), let \(\mathcal{S}\) and \(S\) be defined as above. Let \(\mathbb{P}(h,t|\mathcal{S})\geq p_{\textsc{t}}\) and \(\hat{\mathbb{P}}(h,t|S)\geq p_{\textsc{t}}\forall(h,t)\in\ \mathcal{H}^{T,g}\), the following holds w.p. at least \(1-\delta_{3}/2\)_

\[\big{|}\mathcal{E}_{a}\big{(}h,t\big{|}\mathcal{S}\big{)}-\widehat{\mathcal{ E}}_{a}\big{(}h,t\big{|}S\big{)}\big{|}\leq\frac{4}{p_{\textsc{t}}}\mathfrak{ N}_{n}\big{(}\mathcal{H}^{T,g}\big{)}+\frac{2}{p_{\textsc{t}}}\sqrt{\frac{1}{n} \log(\frac{2}{\delta_{3}})}\qquad\forall(h,t)\in\mathcal{H}^{T,g}.\] (4)

Proof.: We begin with proving one side of the inequality and the other side is shown by following the same steps. The proof is based on applying the uniform convergence results for \(\widehat{\mathcal{E}}(h,t|S)\) and \(\widehat{\mathbb{P}}(h,t|S)\) from Lemma C.2. The main difficulty here is that \(\mathbb{E}_{S}[\widehat{\mathcal{E}}_{a}\big{(}h,t\big{|}S\big{)}]\neq \mathcal{E}_{a}\big{(}h,t\big{|}\mathcal{S}\big{)}\), so we cannot directly get the above result from standard uniform convergence bounds.

We prove it, by using the results from the Lemma C.2 and restricting the region \(\mathcal{S}\) such that it has probability mass at least \(p_{\textsc{t}}\).

By definitions of \(\mathcal{E}_{a}\big{(}h,t\big{|}\mathcal{S}\big{)}\) and \(\widehat{\mathcal{E}}_{a}\big{(}h,t\big{|}S\big{)}\) we have,

\[\mathcal{E}(h,t|\mathcal{S})=\mathbb{P}(h,t|\mathcal{S})\cdot\mathcal{E}_{a} \big{(}h,t\big{|}\mathcal{S}\big{)}\qquad\text{ and }\qquad\widehat{\mathcal{E}}(h,t|S)=\hat{ \mathbb{P}}(h,t|S)\cdot\widehat{\mathcal{E}}_{a}\big{(}h,t\big{|}S\big{)}.\]

Let \(\xi_{1}=\sqrt{(1/n)\log(2/\delta_{1})},\xi_{2}=\sqrt{(1/n)\log(2/\delta_{2})}\). From lemma C.2 we have,

\[\mathcal{E}(h,t|\mathcal{S})\leq\widehat{\mathcal{E}}(h,t|S)+2\mathfrak{R}_{n} \big{(}\mathcal{H}^{T,g}\big{)}+\xi_{1}\quad\forall(h,t)\in\mathcal{H}^{T,g} \quad\text{ w.p. }1-\delta_{1}/2.\] (5)

\[\hat{\mathbb{P}}(h,t|S)\leq\mathbb{P}(h,t|\mathcal{S})+2\mathfrak{R}_{n} \big{(}\mathcal{H}^{T,g}\big{)}+\xi_{2}\quad\forall(h,t)\in\mathcal{H}^{T,g} \quad\text{ w.p. }1-\delta_{2}/2.\] (6)

Plugging in the above definitions of errors in equation (5) we get,

\[\mathbb{P}(h,t|\mathcal{S})\cdot\mathcal{E}_{a}\big{(}h,t\big{|} \mathcal{S}\big{)} \leq\widehat{\mathbb{P}}(h,t|S)\cdot\widehat{\mathcal{E}}_{a} \big{(}h,t\big{|}S\big{)}+2\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g}\big{)}+ \xi_{1}.\] (7) \[\mathcal{E}_{a}\big{(}h,t\big{|}\mathcal{S}\big{)} \leq\frac{\widehat{\mathbb{P}}(h,t|S)}{\mathbb{P}(h,t|\mathcal{S} )}\widehat{\mathcal{E}}_{a}\big{(}h,t\big{|}S\big{)}+2\frac{\mathfrak{R}_{n} \big{(}\mathcal{H}^{T,g}\big{)}}{\mathbb{P}(h,t|\mathcal{S})}+\frac{\xi_{1}}{ \mathbb{P}(h,t|\mathcal{S})}.\] (8)

Substituting \(\widehat{\mathbb{P}}(h,t|S)\) from equation 6 in the above equation, we get the following w.p. \((1-\delta_{1}/2)(1-\delta_{2}/2)\), \(\forall(h,t)\in\mathcal{H}^{T,g}\),

\[\mathcal{E}_{a}\big{(}h,t\big{|}\mathcal{S}\big{)}\leq\Big{(}\frac{\mathbb{P}(h, t|\mathcal{S})+2\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g}\big{)}+\xi_{2}}{ \mathbb{P}(h,t|\mathcal{S})}\Big{)}\widehat{\mathcal{E}}_{a}\big{(}h,t\big{|}S \big{)}+\frac{2\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g}\big{)}}{\mathbb{P}(h,t| \mathcal{S})}+\frac{\xi_{1}}{\mathbb{P}(h,t|\mathcal{S})}\quad.\]

[MISSING_PAGE_EMPTY:22]

**Lemma C.3**.: _Let \(\mathcal{S}\subseteq\mathcal{X}\) be a sub-region of \(\mathcal{X}\) and \(S=\{\mathbf{x}_{1},\ldots,\mathbf{x}_{n}\}\) be a set of \(n\) i.i.d samples in \(\mathcal{S}\) drawn from distribution \(P_{\mathbf{x}}\). Let \(\{y_{1},\ldots y_{n}\}\) be the corresponding true labels and let \(\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g}\big{)}\) be the Rademacher complexity of the function class \(\mathcal{H}^{T,g}\) defined over \(n\) i.i.d. samples. Then we have,_

\[\mathbb{E}_{S}\Big{[}\sup_{(h,t)\in\mathcal{H}^{T,g}}\mathcal{E}(h,t|\mathcal{ S})-\widehat{\mathcal{E}}(h,t|S)\Big{]}\leq 2\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g} \big{)}.\] (11)

Proof.: Let \(\tilde{S}=\{\tilde{\mathbf{x}}_{1},\tilde{\mathbf{x}}_{2},\ldots\tilde{ \mathbf{x}}_{n}\}\) be another set of independent draws from the same distribution as of \(S\) and let the corresponding labels be \(\{\tilde{y}_{1},\ldots\tilde{y}_{n}\}\). These samples are usually termed as _ghost samples_ and do not need to be counted in the sample complexity.

\[\mathbb{E}_{S}\Big{[}\sup_{(h,t)\in\mathcal{H}^{T,g}}\mathcal{E} (h,t|\mathcal{S})-\widehat{\mathcal{E}}(h,t|S)\Big{]}=\mathbb{E}_{S}\Big{[} \sup_{(h,t)\in\mathcal{H}^{T,g}}\mathbb{E}_{S}\big{[}\widehat{\mathcal{E}}(h,t |\tilde{S})\big{]}-\widehat{\mathcal{E}}(h,t|S)\Big{]}.\] \[=\mathbb{E}_{S}\Big{[}\sup_{(h,t)\in\mathcal{H}^{T,g}}\mathbb{E} _{S}\big{[}\widehat{\mathcal{E}}(h,t|\tilde{S})-\widehat{\mathcal{E}}(h,t|S) \big{]}\Big{]}.\] \[=\mathbb{E}_{S,\tilde{S}}\Big{[}\sup_{h\in\mathcal{H}^{T,g}} \big{[}\widehat{\mathcal{E}}(h,t|\tilde{S})-\widehat{\mathcal{E}}(h,t|S) \big{]}\Big{]}.\] \[=\mathbb{E}_{S,\tilde{S}}\Big{[}\sup_{(h,t)\in\mathcal{H}^{T,g}} \Big{[}\frac{1}{n}\sum_{i=1}^{n}\ell_{0-1}^{\perp}(h,t,\tilde{\mathbf{x}}_{i}, \tilde{y}_{i})-\frac{1}{n}\sum_{i=1}^{n}\ell_{0-1}^{\perp}(h,t,\mathbf{x}_{i}, y_{i})\Big{]}\Big{]}.\] \[=\mathbb{E}_{\sigma,S,\tilde{S}}\Big{[}\sup_{h\in\mathcal{H}^{T,g }}\Big{[}\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}\ell_{0-1}^{\perp}(h,t,\tilde{ \mathbf{x}}_{i},\tilde{y}_{i})-\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}\ell_{0-1}^{ \perp}(h,t,\mathbf{x}_{i},y_{i})\Big{]}\Big{]}.\] \[\leq\mathbb{E}_{\sigma,\tilde{S}}\Big{[}\sup_{(h,t)\in\mathcal{H }^{T,g}}\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}\ell_{0-1}^{\perp}(h,t,\tilde{ \mathbf{x}}_{i},\tilde{y}_{i})\Big{]}+\] \[=2\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g},\ell_{0-1}^{\perp} \big{)}.\] \[\leq 2\mathfrak{R}_{n}\big{(}\mathcal{H}^{T,g}\big{)}.\]

In the last step, we used the upper bound on the Rademacher complexity from Lemma C.5. 

**Lemma C.4**.: _(Bounded Difference) Let \(S\) be a set of i.i.d samples from \(P_{\mathbf{x}}\) then for \(\phi(S):=\sup_{(h,t)\in\mathcal{H}^{T,g}}\mathcal{E}(h,t|\mathcal{S})-\widehat {\mathcal{E}}(h,t|S)\), with probability at least \(1-\delta\),_

\[\phi(S)\leq\mathbb{E}_{S}[\phi(S)]+\sqrt{\frac{1}{|S|}\log(\frac{1}{\delta})}\] (12)

Proof.: It is proved by showing that \(\phi(S)\) satisfies the conditions (in particular the bounded difference assumption) needed for the application of McDiarmid Inequality. To see this, Let \(S=\{\mathbf{x}_{1},\mathbf{x}_{2},\ldots\mathbf{x}_{i},\ldots,\mathbf{x}_{n}\}\) and let \(S^{\prime}=\{\mathbf{x}_{1},\mathbf{x}_{2},\ldots\mathbf{x}_{i}^{\prime}, \ldots,\mathbf{x}_{n}\}\), i.e. \(S\) and \(S^{\prime}\) may differ only on the \(i^{th}\) sample.

\[|\phi(S)-\phi(S^{\prime})| =\Big{|}\sup_{(h,t)\in\mathcal{H}^{T,g}}\mathcal{E}(h,t|\mathcal{ S})-\widehat{\mathcal{E}}(h,t|S)-\sup_{(h,t)\in\mathcal{H}^{T,g}}\mathcal{E}(h,t| \mathcal{S})-\widehat{\mathcal{E}}(h,t|S^{\prime})\Big{|}.\] \[\leq\Big{|}\sup_{(h,t)\in\mathcal{H}^{T,g}}\Big{(}\mathcal{E}(h,t |\mathcal{S})-\widehat{\mathcal{E}}(h,t|S)-\mathcal{E}(h,t|\mathcal{S})+ \widehat{\mathcal{E}}(h,t|S^{\prime})\Big{)}\Big{|}.\] \[=\Big{|}\sup_{(h,t)\in\mathcal{H}^{T,g}}\Big{(}\widehat{\mathcal{ E}}(h,t|S)-\widehat{\mathcal{E}}(h,t|S^{\prime})\Big{)}\Big{|}.\] \[=\Big{|}\sup_{(h,t)\in\mathcal{H}^{T,g}}\Big{(}\frac{1}{|S|}\sum_ {\mathbf{z}_{j}\in S}\ell_{0-1}^{\perp}(h,t,\mathbf{x}_{j},y_{j})-\frac{1}{|S^{ \prime}|}\sum_{\mathbf{z}_{j}\in S^{\prime}}\ell_{0-1}^{\perp}(h,t,\mathbf{x}_ {j},y_{j})\Big{)}\Big{|}.\]

[MISSING_PAGE_FAIL:24]

Proof.: The proof follows by substituting the Rademacher complexity bounds for finite VC dimension function classes from Lemma C.6 in the general result from Theorem 3.2.

\[\widehat{\mathcal{E}}(X_{pool}(A_{k})) \leq\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)}}\Bigg{(}\widehat{ \mathcal{E}}_{\hat{a}}(\hat{h}_{i},\hat{t}_{i}|X_{v}^{(i)})+\frac{4}{R_{b}} \big{(}\underbrace{\mathfrak{R}_{n_{v}^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)}+ \mathfrak{R}_{n_{a}^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)}\big{)}}_{(b)}\] \[+\frac{4}{R_{b}}\underbrace{\sqrt{\frac{1}{n_{v}^{(i)}}\log\Big{(} \frac{8k}{\delta}\Big{)}}}_{(c)}\Bigg{)}+\underbrace{\frac{4}{R_{b}}\sqrt{ \frac{k}{N_{a}^{(k)}}\log\Big{(}\frac{8k}{\delta}\Big{)}}}_{(d)}\]

We first simplify the terms dependent on \(n_{v}^{(i)}\) as follows. Here we use the inequality \(\sqrt{a}+\sqrt{b}\leq\sqrt{2(a+b)}\) for any \(a,b\in\mathbb{R}^{+}\).

\[\mathfrak{R}_{n_{v}^{(i)}}\big{(}\mathcal{H}^{T,g}\big{)}+\sqrt{ \frac{1}{n_{v}^{(i)}}\log\Big{(}\frac{8k}{\delta}\Big{)}} \leq\sqrt{\frac{2d}{n_{v}^{(i)}}\log\Big{(}\frac{en_{v}^{(i)}}{d} \Big{)}}+\sqrt{\frac{1}{n_{v}^{(i)}}\log\Big{(}\frac{4k}{\delta}\Big{)}},\] \[\leq\sqrt{\frac{2}{n_{v}^{(i)}}\bigg{(}2d\log\Big{(}\frac{en_{v}^ {(i)}}{d}\Big{)}+\log\Big{(}\frac{8k}{\delta}\Big{)}\bigg{)}}.\]

Next, we simplify the terms dependent on \(n_{a}^{(i)}\) as follows. First, we substitute the Rademacher complexity using the bound in Lemma C.6 and then apply the same steps as in the proof of Theorem 3.2 to bound \(\sum_{i=1}^{k}\sqrt{n_{a}^{(i)}}/N_{a}^{(k)}\) by \(\sqrt{k/N_{a}^{(k)}}\) followed by the application of \(\sqrt{a}+\sqrt{b}\leq\sqrt{2(a+b)}\) to get the final term.

\[\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)}}\mathfrak{R}_{n_{a}^ {(i)}}\big{(}\mathcal{H}^{T,g}\big{)}+\sqrt{\frac{k}{N_{a}^{(k)}}\log(\frac{8k }{\delta})} \leq\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)}}\sqrt{\frac{2d}{ n_{a}^{(i)}}\log\Big{(}\frac{en_{a}^{(i)}}{d}\Big{)}}+\sqrt{\frac{k}{N_{a}^{(k)}} \log\Big{(}\frac{8k}{\delta}\Big{)}}\] \[=\sum_{i=1}^{k}\frac{\sqrt{n_{a}^{(i)}}}{N_{a}^{(k)}}\sqrt{2d\log \Big{(}\frac{en_{a}^{(i)}}{d}\Big{)}}+\sqrt{\frac{k}{N_{a}^{(k)}}\log\Big{(} \frac{8k}{\delta}\Big{)}}\] \[\leq\sum_{i=1}^{k}\frac{\sqrt{n_{a}^{(i)}}}{N_{a}^{(k)}}\sqrt{2d \log\Big{(}\frac{eN_{a}^{(k)}}{d}\Big{)}}+\sqrt{\frac{k}{N_{a}^{(k)}}\log\Big{(} \frac{8k}{\delta}\Big{)}}\] \[\leq\sqrt{\frac{2dk}{N_{a}^{(k)}}\log\Big{(}\frac{eN_{a}^{(k)}}{ d}\Big{)}}+\sqrt{\frac{k}{N_{a}^{(k)}}\log\Big{(}\frac{8k}{\delta}\Big{)}}\] \[\leq\sqrt{\frac{2k}{N_{a}^{(k)}}\bigg{(}2d\log\Big{(}\frac{eN_{a}^ {(k)}}{d}\Big{)}+\log\Big{(}\frac{8k}{\delta}\Big{)}\bigg{)}}.\]

### Homogeneous Linear Classifiers with Uniform Distribution

Here we instantiate Theorem 3.2 for the case of homogeneous linear separators under the uniform distribution in the realizable setting. Formally, let \(P_{\mathbf{x}}\) be a uniform distribution supported on the unit ball in \(\mathbb{R}^{d}\), \(\mathcal{X}=\{\mathbf{x}\in\mathbb{R}^{d}:||\mathbf{x}||\leq 1\}\). Let \(\mathcal{W}=\{\mathbf{w}\in\mathbb{R}^{d}:||\mathbf{w}||_{2}=1\}=\mathbb{S}_{d}\) and \(\mathcal{H}=\{\mathbf{x}\mapsto\text{sign}(\langle\mathbf{w},\mathbf{x} \rangle)\,\forall\mathbf{w}\in\mathcal{W}\}\), the score function is given by \(g(h,\mathbf{x})=g(\mathbf{w},\mathbf{x})=|\langle\mathbf{w},\mathbf{x}\rangle|\) and set \(T=[0,1]\). For simplicity, we will use \(\mathcal{W}\) in place of \(\mathcal{H}\).

**Corollary 3.4**.: _(Overall Auto-Labeling Error and Coverage) Let \(\hat{\mathbf{w}}_{i},\hat{t}_{i}\) be the ERM solution and the auto-labeling margin threshold respectively at epoch \(i\). Let \(n_{v}^{(i)},n_{a}^{(i)}\) denote the number of validation and auto-labeled points at epoch \(i\). Let the auto-labeling algorithm run for \(k\)-epochs. Then, for any \(\delta\in(0,1)\), w.p. at least \(1-\delta/2\),_

\[\widehat{\mathcal{E}}(X_{pool}(A_{k})) \leq\sum_{i=1}^{k}\frac{n_{a}^{(i)}}{N_{a}^{(k)}}\bigg{(}\underbrace {\widehat{\mathcal{E}}_{a}(\hat{\mathbf{w}}_{i},\hat{t}_{i}|X_{v}^{(i)})}_{(a)} +\underbrace{\frac{4}{p_{0}}\sqrt{\frac{2}{n_{v}^{(i)}}\bigg{(}2d\log\Big{(} \frac{en_{v}^{(i)}}{d}\Big{)}+\log\Big{(}\frac{8k}{\delta}\Big{)}\bigg{)}}}_{(b)}\] \[+\underbrace{\frac{4}{p_{0}}\bigg{(}\sqrt{\frac{2k}{N_{a}^{(k)}} \bigg{(}2d\log\Big{(}\frac{eN_{a}^{(k)}}{d}\Big{)}+\log\Big{(}\frac{8k}{\delta} \Big{)}\bigg{)}\bigg{)}}}_{c}\]

_and w.p. at least \(1-\delta/2\)_

\[\widehat{\mathcal{P}}(X_{pool}(A_{k}))\geq 1-\min_{i}\hat{t}_{i}\sqrt{4d/ \pi}-2k\sqrt{\frac{2}{N}\bigg{(}2d\log\Big{(}\frac{eN}{d}\Big{)}+\log\Big{(} \frac{8k}{\delta}\Big{)}\bigg{)}}.\]

Proof.: The bound on auto-labeling error follows directly from Theorem C.7 as the VC dimension for this setting is \(d\). For the coverage bound, we utilize the fact that the distribution \(P_{\mathbf{x}}\) is the uniform distribution over the unit ball. This enables us to obtain explicit lower bounds on the coverage. The details are given in Lemma C.8 and Lemma C.9. 

**Lemma C.8**.: _Let the auto-labeling algorithm run for \(k\)-epochs and let \(\hat{\mathbf{w}}_{i},\hat{t}_{i}\) be the ERM solution and the auto-labeling margin threshold respectively at epoch \(i\). Let \(\mathcal{X}^{(i)}\) be the unlabeled region at the beginning of epoch \(i\), then we have,_

\[\sum_{i=1}^{k}\mathbb{P}\big{(}\mathcal{X}^{(i)}(\hat{\mathbf{w}}_{i},\hat{t }_{i})\big{)}\geq 1-\min_{i}\hat{t}_{i}\sqrt{4d/\pi}.\] (15)

Proof.: Let \(\mathcal{X}(\hat{\mathbf{w}}_{i},t_{i})=\{\mathbf{x}\in\mathcal{X}:|\langle \hat{\mathbf{w}}_{i},\mathbf{x}\rangle|\geq\hat{t}_{i}\}\) denote the region that can be auto-labeled by \(\hat{\mathbf{w}}_{i},\hat{t}_{i}\). However, since in each round the remaining region is \(\mathcal{X}^{(i)}\) the actual auto-labeled region of epoch \(i\) is \(\mathcal{X}^{(i)}_{a}=\{\mathbf{x}\in\mathcal{X}^{(i)}:|\langle\hat{\mathbf{ w}}_{i},\mathbf{x}\rangle|\geq\hat{t}_{i}\}\). Let \(\bar{\mathcal{X}}(\hat{\mathbf{w}}_{i},t_{i})\) denote the complement of set \(\mathcal{X}(\hat{\mathbf{w}}_{i},t_{i})\).

Now observe that \(\mathcal{X}_{a}=\cup_{i=1}^{k}\mathcal{X}^{(i)}_{a}\) and \(\mathcal{X}(\hat{\mathbf{w}}_{k},\hat{t}_{k})\subseteq\mathcal{X}_{a}\) because any \(\mathbf{x}\in\mathcal{X}(\hat{\mathbf{w}}_{k},\hat{t}_{k})\) is either auto-labeled in previous rounds \(i<k\) or if not then it will be auto-labeled in the \(k^{th}\) round. More specifically, any \(\mathbf{x}\in\mathcal{X}(\hat{\mathbf{w}}_{k},\hat{t}_{k})\) is either in \(\cup_{i=1}^{k-1}\mathcal{X}^{(i)}_{a}\) and if not then it must be in \(\mathcal{X}^{(k)}_{a}\). Thus the sum of probabilities,

\[\sum_{i=1}^{k}\mathbb{P}\big{(}\mathcal{X}^{(i)}(\hat{\mathbf{w }}_{i},\hat{t}_{i})\big{)} =\sum_{i=1}^{k}\mathbb{P}(\mathcal{X}^{(i)}_{a})\] \[=\mathbb{P}(\mathcal{X}_{a})\] \[\geq\min_{i}\mathbb{P}\big{(}\mathcal{X}(\hat{\mathbf{w}}_{i}, \hat{t}_{i})\big{)}\] \[=1-\max_{i}\mathbb{P}\big{(}\bar{\mathcal{X}}(\hat{\mathbf{w}}_{i},\hat{t}_{i})\big{)}\] \[\geq 1-\min_{i}\hat{t}_{i}\sqrt{4d/\pi}\]

The last step used Lemma 4 from [3]) with \(\gamma_{1}=\hat{t}_{i}\) and \(\gamma_{2}=0\) to upper bound \(\mathbb{P}(\bar{\mathcal{X}}(\hat{\mathbf{w}}_{i},\hat{t}_{i}))\) by \(\hat{t}_{i}\sqrt{4d/\pi}\). The lemma is stated as follows in Lemma C.9. 

**Lemma C.9**.: _([3] (Lemma 4)) Let \(d\geq 2\) and let \(\mathbf{x}=[x_{1},\ldots x_{d}]\) be uniformly distributed in the \(d\)-dimensional unit ball. Given \(\gamma_{1}\in[0,1],\gamma_{2}\in[0,1]\), we have:_

\[\mathbb{P}\big{(}(x_{1},x_{2})\in[0,\gamma_{1}]\times[\gamma_{2},1]\big{)}\leq \frac{\gamma_{1}\sqrt{d}}{2\sqrt{\pi}}\exp\Big{(}-\frac{(d-2)\gamma_{2}^{2}}{2} \Big{)}\]

### Lower Bound

**Lemma 3.3**.: _Let \(c_{1},c_{2}\) and \(\sigma>0\). Let \(\mathbf{x}_{i}\in X\) be a set of \(n\) i.i.d. points from \(\mathcal{X}\) with corresponding true labels \(y_{i}\). Given \((h,t)\in\mathcal{H}^{T,g}\), let \(\mathbb{E}\big{[}\big{(}\ell_{0-1}^{\lambda}(h,t,\mathbf{x}_{i},y_{i})- \mathcal{E}(h,t|\mathcal{X})\big{)}^{2}\big{]}=\sigma_{i}^{2}>\sigma^{2}\) for every \(\mathbf{x}_{i}\) for \(\sigma_{i}>0\) and let \(\sum_{i}^{n}\sigma_{i}^{2}\geq c_{1}\) then for every \(\epsilon\in[0,\frac{\sum_{i=1}^{n}\sigma_{i}^{2}}{\sqrt{c_{1}}}]\) with \(n_{v}<\frac{12\sigma^{2}}{\epsilon^{2}}\log(4c_{2})\) the following holds w.p. at least \(1/4\), \(\mathcal{E}_{a}(h,t|\mathcal{X})>\widehat{\mathcal{E}}_{a}(h,t|X)+\epsilon\)._

Proof.: It follows by application of Feller's result stated in lemma C.10. 

**Lemma C.10**.: _(Feller, Lower Bound on Tail Probability of Sum of Independent Random Variables) There exists positive universal constants \(c_{1}\) and \(c_{2}\) such that for any set of independent random variables \(X_{1},\dots,X_{m}\) satisfying \(E[X_{i}]=0\) and \(|X_{i}|\leq M\), for every \(i\in\{1,\dots,m\}\), if \(\sum_{i=1}^{m}\mathbb{E}[(X)_{i}^{2}]\geq c_{1}\), then for every \(\epsilon\in[0,\frac{\sum_{i=1}^{m}\mathbb{E}[(X_{i})^{2}]}{M\sqrt{c_{1}}}]\)_

\[\mathbb{P}(\sum_{i=1}^{m}X_{i}>\epsilon)\geq c_{2}\exp\Big{(}\frac{-\epsilon^ {2}}{12\sum_{i=1}^{m}\mathbb{E}[(X_{i})^{2}]}\Big{)}.\] (16)Additional Experiments

In this section, we discuss additional experiments on the role of hypothesis class in auto-labeling datasets and experiments for studying the role of confidence function in auto-labeling. Finally, we visualize PaCMAP embeddings of the CIFAR-10 and MNIST data points to get a sense of auto-labeling regions in various rounds of the algorithm.

### Additional Experiments on Role of the Hypothesis Class

First, we provide details of the datasets,

**XOR** is a synthetic dataset. Recall that it is created by uniformly drawing points from 4 circles, each centered at the corners of a square of with side length 4 centered at the origin. Points in the diagonally opposite balls belong to the same class. We generate a total of \(N=10,000\) samples, out of which we keep \(8,000\) in \(X_{pool}\) and \(2,000\) in the validation pool \(X_{val}\).

**MNIST**[15] is a standard image dataset of hand-written digits. We randomly split the standard training set into \(X_{pool}\) and the validation pool \(X_{val}\) of sizes 48,000 and 12,000 respectively. While training a linear classifier on this dataset we flatten the \(28\times 28\) images to vectors of size 784.

**XOR Experiment.** We run the TBAL algorithm 1 with an error tolerance of \(\epsilon_{a}=1\%\). we use 20% of \(N_{q}\) as seed training data and keep query size \(n_{b}\) as \(5\%\) of \(N_{q}\). We compare it with active learning and active learning followed by selective classification. The given function class and selective classifier are both linear for all the algorithms. The results are shown in Figure 5. Clearly, there is no linear classifier that can correctly classify this data. We note that there are multiple optimal classifiers in the function class of linear classifiers and they will all incur an error of \(25\%\). So, active learning algorithms can only output models that make at least \(25\%\) error. If we naively use the output model for auto-labeling, we can obtain near full coverage but incur \(25\%\) auto-labeling error. If we use the model output by active learning with threshold-based selective classification, then it can attain lower error in labeling. However, it can only label \(\approx 25\%\) of the unlabeled data. In contrast, the TBAL algorithm can label almost all of the data accurately, i.e., attain close to \(100\%\) coverage, with an error close to \(1\%\) auto-labeling error.

**MNIST Experiment.** For training LeNet [37] we use SGD with a learning rate of 0.1, batch size of 32, and train for 20 epochs. We use auto-labeling error threshold \(\epsilon_{a}=5\%\). We use 20% of \(N_{q}\) as seed training data and keep query size \(n_{b}\) as \(5\%\) of \(N_{q}\). The results are presented in Figure 6 we observe that TBAL using less powerful models can still yield highly accurate datasets with a significant fraction of points labeled by the models. This confirms the notion that bad models can still provide good datasets.

### Role of Confidence Function

The confidence function \(g\) is used to obtain uncertainty scores is an important factor in auto-labeling. In particular, for threshold-based auto-labeling we expect the scores of correctly classified and incorrectly classified points to be reasonably well separated and if this is not the case then the

Figure 5: Comparison of Threshold-Based Auto-Labeling (TBAL) and Active-Learning followed by Selective Classification (AL+SC) on XOR-dataset. Left figure (a) shows samples that were auto-labeled, queried, and left unlabeled by these methods. Right figure (b) shows the auto-labeling error and coverage achieved. The lines show the mean and the shaded region shows 1-standard deviation estimated over 10 trials with different random seeds.

algorithm will struggle to find a good threshold even if the given classifier has good accuracy in certain regions.

**Setup.** We perform auto-labeling on the CIFAR-10 dataset using a small CNN network with 2 convolution layers followed by 3 fully connected layers [48]. We use two different scores for auto-labeling, a) Usual softmax output b) Energy score with temperature = 1 [38]. We vary the maximum number of training samples \(N_{q}\) and keep 20% of \(N_{q}\) as seed samples and query points in the batches of \(10\%\) of \(N_{q}\). The model is trained for 50 epochs, using SGD with a learning rate of 0.05, batch size = 256, weight decay = \(5e^{-4}\), and momentum=0.9, and use \(\epsilon_{a}=10\%\).

**Results.** The results with softmax scores and energy scores used as confidence functions can be seen in Figures 8(a) and 8(b) respectively. We see that for both of these cases, TBAL does not obtain a coverage of more than \(\approx 6\%\). We observe that using the energy score as the confidence function performed marginally better than using the softmax scores. We note that this is the case even though the test accuracies of the trained models were around \(50\%\) for most of the rounds. Note that CIFAR-10 has 10 classes, so an accuracy of \(50\%\) is much better than random guessing and one would expect to be able to auto-label a significant chunk of the data with such a model. However, the softmax scores and energy scores are not well calibrated, and therefore, when used as confidence functions, they result in a poor separation between correct and incorrect predictions by the model. This can be seen in Figure 8 where neither of the softmax and energy scores provides a good separation between the correct and incorrect predictions. We can also see that the energy score is marginally better in terms of the separation, which allows it to achieve slightly better auto-labeling coverage in comparison to using softmax scores. This suggests that more investigation is needed to understand the properties of good confidence functions for auto-labeling which is left to future work.

Figure 6: Auto-labeling performance on MNIST data using different models (hypothesis classes) as a function of samples available for training. The left figure (a) shows the results with the linear classifier and the right figure (b) shows the results with the LeNet classifier. The auto-labeling error threshold \(\epsilon_{a}=5\%\) in both experiments and the algorithms are given the same amount of validation data. The lines show the mean and the shaded region shows 1-standard deviation estimated over 5 trials with different random seeds.

Figure 7: Auto-labeling performance on CIFAR-10 data using a small network and different scoring functions. The left figure (a) shows the results with softmax scores and the right figure (b) shows the results with the energy score. The auto-labeling error threshold \(\epsilon_{a}=10\%\) in both experiments and methods are given the same amount of validation data. The lines show the mean and the shaded region shows 1-standard deviation estimated over 5 trials with different random seeds.

[MISSING_PAGE_EMPTY:30]

### Auto Labeling Visualization

In this section, we visualize the process of TBAL. We use the dimensionality reduction method, PaCMAP [66], to visualize the features of the samples. For neural network models, we visualize the PaCMAP embeddings of the penultimate layer's output and for linear models, we use PaCMAP on the raw features. In these figures, each row corresponds to one TBAL round. Each figure shows a few selected rounds of auto-labeling. Each figure has four columns (left to right), which show: **a)** The samples that are labeled by TBAL in the round are shown in that row. **b)** The embeddings for training samples in that round. **c)** The embeddings for validation data points in that round. **d)** The score distribution for the validation dataset in that round.

In Figure 9 we see visualizations for auto-labeling on the MNIST data using linear models. In this setting the data exhibits clustering structure in the PaCMAP embeddings learned on the raw features and the confidence (probability) scores produced are also reasonably well calibrated which leads to good auto-labeling performance.

The visualizations for the process of TBAL on CIFAR-10 using the small network (a small CNN network with 2 convolution layers followed by 3 fully connected layers [48]) with energy scores and soft-max scores for confidence functions are shown in Figures 10 and 11 respectively. We note that both the energy scores and soft-max scores do not seem to be calibrated to the correctness of the predicted labels which makes it difficult to identify subsets of unlabeled data where the current hypothesis in each round could have potentially auto-labeled. We also note that the test accuracies of the trained models were around \(50\%\) for most of the rounds of TBAL even though the small network model is not a powerful enough model class for this dataset. Note that CIFAR-10 has 10 classes, so the accuracy of \(50\%\) is much better than random guessing and one would expect to be able to auto-label a reasonably large chunk of the data with such a model if accompanied by a good confidence function. This highlights the important role that the confidence function plays in a TBAL system and more investigation is needed which is left to future work.

Note that, in our auto-labeling implementation we find class specific thresholds. In these figures, we show the histograms of scores for all classes for simplicity. We want to emphasize that the visualization figures in this section are 2D representations (approximation) of the high-dimensional features (either of the penultimate layer or the raw features).

\begin{table}
\begin{tabular}{c|c c c c c c|c c c c c c} \hline \hline \multirow{2}{*}{\(\mathbf{N_{q}}\)} & \multicolumn{8}{c}{Error (\%)} & \multicolumn{8}{c}{**Coverage (\%)**} \\ \cline{2-13}  & **TBAL** & **AL+SC** & **PL+SC** & **AL** & **PL** & **TBAL** & **AL+SC** & **PL+SC** & **AL** & **PL** \\ \hline
200 & 4.57 \(\pm\) 4.1 & 4.30 \(\pm\) 4.0 & 3.97 \(\pm\) 4.0 & 6.44 \(\pm\) 0.3 & 6.20 \(\pm\) 4.0 & 93.46 \(\pm\) 1.01 & 90.60 \(\pm\) 2.91 & 91.97 \(\pm\) 4.23 & 99.50 \(\pm\) 4.08 & 99.00 \(\pm\) 4.09 \\ \hline
400 & 4.60 \(\pm\) 4.0 & 3.75 \(\pm\) 4.3 & 3.92 \(\pm\) 1.06 & 11.86 \(\pm\) 0.34 & 6.81 \(\pm\) 1.25 & 92.55 \(\pm\) 0.06 & 84.27 \(\pm\) 7.95 & 89.95 \(\pm\) 1.25 & 99.00 \(\pm\) 4.08 & 98.00 \(\pm\) 4.08 \\ \hline
600 & 4.93 \(\pm\) 4.0 & 3.99 \(\pm\) 5.4 & 4.69 \(\pm\) 4.0 & 6.31 \(\pm\) 2.0 & 6.33 \(\pm\) 0.0 & 92.45 \(\pm\) 0.04 & 91.69 \(\pm\) 3.99 & 91.20 \(\pm\) 0.24 & 98.50 \(\pm\) 0.04 & 97.00 \(\pm\) 4.08 \\ \hline
800 & 4.76 \(\pm\) 4.1 & 3.55 \(\pm\) 4.0 & 4.37 \(\pm\) 0.14 & 6.91 \(\pm\) 4.0 & 6.12 \(\pm\) 0.0 & 92.15 \(\pm\) 1.08 & 89.98 \(\pm\) 3.38 & 89.97 \(\pm\) 4.08 & 98.00 \(\pm\) 4.08 & 96.00 \(\pm\) 4.08 \\ \hline
1000 & 4.49 \(\pm\) 4.08 & 4.19 \(\pm\) 4.1 & 4.25 \(\pm\) 5.0 & 5.65 \(\pm\) 0.5 & 6.14 \(\pm\) 0.11 & 92.25 \(\pm\) 5.06 & 92.28 \(\pm\) 2.13 & 89.47 \(\pm\) 3.0 & 97.50 \(\pm\) 0.08 & 95.00 \(\pm\) 4.08 \\ \hline \hline \end{tabular}
\end{table}
Table 8: **IMDB**. Effect of variation of \(N_{q}\), the maximum number of samples the algorithm can use for training, without using a UCB (i.e., \(C_{1}=0\)) on error estimates. We keep validation data size \(N_{v}\) fixed at \(1000\) and use error threshold \(\epsilon_{a}=5\%\). We report the mean and std. deviation over 10 runs with different random seeds.

\begin{table}
\begin{tabular}{c|c c c c c|c c c c c c} \hline \hline \multirow{2}{*}{\(\mathbf{N_{q}}\)} & \multicolumn{8}{c}{**Error (\%)**} & \multicolumn{8}{c}{**Coverage (\%)**} \\ \cline{2-13}  & **TBAL** & **AL+SC** & **PL+SC** & **AL** & **PL** & **TBAL** & **AL+SC** & **PL+SC** & **AL** & **PL** \\ \hline
200 & 1.67 \(\pm\) 2.38 & 2.15 \(\pm\) 4.07 & 1.59 \(\pm\) 0.10 & 6.44 \(\pm\) 0.03 & 6.20 \(\pm\) 0.00 & 73.30 \(\pm\) 1.40 & 57.17 \(\pm\) 0.10 & 57.39 \(\pm\) 1.43 & 99.50 \(\pm\) 0.08 & 99.00 \(\pm\) 4.08 \\ \hline
400 & 1.63 \(\pm\) 4.19 & 1.61 \(\pm\) 4.29 & 1.76 \(\pm\) 0.13 & 11.86 \(\pm\) 0.34 & 6.81 \(\pm\) 1.25 & 72.59 \(\pm\) 1.36 & 64.53 \(\pm\) 5.51 & 58.48 \(\pm\) 1.79 & 90.00 \(\pm\) 4.08 & 98.00 \(\pm\) 4.09 \\ \hline
600 & 1.67 \(\pm\) 4.21 & 1.83 \(\pm\) 4.30 & 1.67 \(\pm\) 0.08 & 6.31 \(\pm\) 1.25 & 6.33 \(\pm\) 0.03 & 71.38 \(\pm\) 0.15 & 70.50 \(\pm\) 5.58 & 65.71 \(\pm\) 2.14 & 98.50 \(\pm\) 0.08 & 97.00 \(\pm\) 4.08 \\ \hline
800 & 1.67 \(\pm\) 4.27 & 1.90 \(\pm\) 5.33 & 1.79 \(\pm\) 0.09 & 6.91 \(\pm\) 4.0 & 6.12 \(\pm\) 0.01 & 69.10 \(\pm\) 5.41 & 65.74 \(\pm\) 1.46 & 73.21 \(\pm\) 2.57 & 98.00 \(\pm\) 4.08 & 96.00 \(\pm\) 4.08 \\ \hline
1000 & 1.62 \(\pm\) 4.22 & 1.97 \(\pm\) 0.53 & 1.70 \(\pm\) 1.22 & 5.65 \(\pm\) 0.52 & 6.14 \(\pm\) 0.11 & 73.42 \(\pm\) 2.54 & 68.05 \(\pm\) 5.56 & 64.18 \(\pm\) 1.21 & 97.50 \(\pm\) 0.08 & 95.00 \(\pm\) 4.08 \\ \hline \hline \end{tabular}
\end{table}
Table 9: **IMDB**. Effect of variation of \(N_{q}\), the maximum number of samples the algorithm can use for training, using a UCB (i.e., \(C_{1}=0.25\)) on error estimates. We keep validation data size \(N_{v}\) fixed at \(1000\) and use error threshold \(\epsilon_{a}=5\%\). We report the mean and std. deviation over 10 runs with different random seeds.

\begin{table}
\begin{tabular}{c|c c c c c c|c c c c c c} \hline \hline \multirow{2}{*}{\(\mathbf{N_{v}}\)} & \multicolumn{8}{c}{Error (\%)} & \multicolumn{8}{c}{**Coverage (\%)**} \\ \cline{2-13}  & **TBAL** & **AL+SC** & **PL+SC** & **AL** & **PL** & **TBAL** & **AL+SC** & **PL+SC** & **AL** & **PL** \\ \hline
100 & 3.10 \(\pm\)1.03 & 0.68 \(\pm\)0.81 & 1.45 \(\pm\)0.73 & 1.23 \(\pm\)0.69 & 2.87 \(\pm\)0.57 & 71.43 \(\pm\)0.58 & 96.95 \(\pm\)1.01 & 92.29 \(\pm\)3.72 & 98.52 \(\pm\)0.16 & 96.88 \(\pm\)0.00 \\ \hline
400 & 1.65 \(\pm\)0.63 & 0.32 \(\pm\)0.53 & 0.52 \(\pm\)0.32 & 0.81 \(\pm\)0.40 & 2.87 \(\pm\)0.57 & 93.27 \(\pm\)2.50 & 96.91 \(\pm\)0.69 & 87.86 \(\pm\)0.73 & 98.44 \(\pm\)0.00 & 96.88 \(\pm\)0.00 \\ \hline
800 & 1.08 \(\pm\)0.40 & 0.24 \(\pm\)0.36 & 0.31 \(\pm\)0.77 & 0.81 \(\pm\)0.52 & 2.87 \(\pm\)0.57 & 96.01 \(\pm\)1.18 & 96.31 \(\pm\)1.18 & 86.21 \(\pm\)0.58 & 98.44 \(\pm\)0.00 & 96.88 \(\pm\)0.00 \\ \hline
1200 & 0.78 \(\pm\)0.27 & 0.17 \(\pm\)0.51 & 0.18 \(\pm\)0.41 & 0.81 \(\pm\)0.20 & 2.87 \(\pm\)0.57 & 96.82 \(\pm\)0.54 & 95.96 \(\pm\)1.01 & 84.65 \(\pm\)1.14 & 98.84 \(\pm\)0.00 & 96.88 \(\pm\)0.00 \\ \hline
1600 & 0.65 \(\pm\)0.33 & 0.13 \(\pm\)0.66 & 0.12 \(\pm\)0.69 & 0.81 \(\pm\)0.32 & 2.87 \(\pm\)0.57 & 96.93 \(\pm\)0.57 & 95.70 \(\pm\)1.38 & 83.76 \(\pm\)0.53 & 98.44 \(\pm\)0.00 & 96.88 \(\pm\)0.00 \\ \hline
2000 & 0.54 \(\pm\)0.38 & 0.21 \(\pm\)0.51 & 0.21 \(\pm\)0.50 & 0.81 \(\pm\)0.32 & 2.87 \(\pm\)0.57 & 97.23 \(\pm\)0.42 & 96.36 \(\pm\)1.13 & 85.72 \(\pm\)0.57 & 98.44 \(\pm\)0.00 & 96.88 \(\pm\)0.00 \\ \hline \hline \end{tabular}
\end{table}
Table 11: **Unit-Ball. Effect of variation of validation data size (\(N_{v}\)), without using a UCB (i.e., \(C_{1}=0.25\)) on error estimates. We keep training data size \(N_{q}\) fixed at \(500\) and use error threshold \(\epsilon_{a}=1\%\). We report the mean and std. deviation over 10 runs with different random seeds.**

\begin{table}
\begin{tabular}{c|c c c c c c|c c c c c c} \hline \hline \multirow{2}{*}{\(\mathbf{N_{q}}\)} & \multicolumn{8}{c}{**Error (\%)**} & \multicolumn{8}{c}{**Coverage (\%)**} \\ \cline{2-13}  & **TBAL** & **AL+SC** & **PL+SC** & **AL** & **PL** & **TBAL** & **AL+SC** & **PL+SC** & **AL** & **PL** \\ \hline
100 & 3.10 \(\pm\)1.03 & 0.68 \(\pm\)0.81 & 1.45 \(\pm\)0.73 & 1.23 \(\pm\)0.69 & 2.87 \(\pm\)0.57 & 71.43 \(\pm\)0.58 & 96.95 \(\pm\)1.01 & 92.29 \(\pm\)3.72 & 98.52 \(\pm\)0.16 & 96.88 \(\pm\)0.00 \\ \hline
400 & 1.65 \(\pm\)0.63 & 0.32 \(\pm\)0.53 & 0.52 \(\pm\)0.32 & 0.81 \(\pm\)0.40 & 2.87 \(\pm\)0.57 & 93.27 \(\pm\)2.50 & 96.91 \(\pm\)0.69 & 87.86 \(\pm\)0.73 & 98.44 \(\pm\)0.00 & 96.88 \(\pm\)0.00 \\ \hline
800 & 1.08 \(\pm\)0.40 & 0.24 \(\pm\)0.36 & 0.31 \(\pm\)0.77 & 0.81 \(\pm\)0.52 & 2.87 \(\pm\)0.57 & 96.01 \(\pm\)1.18 & 96.31 \(\pm\)1.18 & 86.21 \(\pm\)0.58 & 98.44 \(\pm\)0.00 & 96.88 \(\pm\)0.00 \\ \hline
1200 & 0.78 \(\pm\)0.27 & 0.17 \(\pm\)0.51 & 0.18 \(\pm\)0.41 & 0.81 \(\pm\)0.20 & 2.87 \(\pm\)0.57 & 96.82 \(\pm\)0.54 & 95.96 \(\pm\)1.01 & 84.65 \(\pm\)1.14 & 98.84 \(\pm\)0.00 & 96.88 \(\pm\)0.00 \\ \hline
1600 & 0.65 \(\pm\)0.33 & 0.13 \(\pm\)0.66 & 0.12 \(\pm\)0.69 & 0.81 \(\pm\)0.32 & 2.87 \(\pm\)0.57 & 96.93 \(\pm\)0.57 & 95.70 \(\pm\)1.38 & 83.76 \(\pm\)0.53 & 98.44 \(\pm\)0.00 & 96.88 \(\pm\)0.00 \\ \hline
1600 & 1.08 \(\pm\)0.47 & 0.94 \(\pm\)0.02 & 1.00 \(\pm\)0.20 & 0.81 \(\pm\)0.32 & 2.87 \(\pm\)0.57 & 97.23 \(\pm\)0.42 & 96.36 \(\pm\)1.13 & 85.72 \(\pm\)0.57 & 98.44 \(\pm\)0.00 & 96.88 \(\pm\)0.00 \\ \hline \hline \end{tabular}
\end{table}
Table 12: **Unit-Ball. Effect of variation of \(N_{q}\), the maximum number of samples the algorithm can use for training, without using a UCB (i.e., \(C_{1}=0\)) on error estimates. We keep validation data size \(N_{v}\) fixed at \(4000\) and use error threshold \(\epsilon_{a}=1\%\). We report the mean and std. deviation over 10 runs with different random seeds.**

\begin{table}
\begin{tabular}{c|c c c c c c|c c c c c} \hline \hline \multirow{2}{*}{\(\mathbf{N_{v}}\)} & \multicolumn{8}{c}{**Error (\%)**} & \multicolumn{8}{c}{**Coverage (\%)**} \\ \cline{2-13}  & **TBAL** & **AL+SC** & **PL+SC** & **AL** & **PL** & **TBAL** & **AL+SC** & **PL+SC** & **AL** & **PL** \\ \hline
100 & 3.10 \(\pm\)1.03 & 0.68 \(\pm\)0.81 & 1.45 \(\pm\)0.73 & 1.23 \(\pm\)0.69 & 2.87 \(\pm\)0.57 & 71.43 \(\pm\)0.58 & 96.95 \(\pm\)1.01 & 92.29 \(\pm\)3.72 & 98.52 \(\pm\)0.16 & 96.88 \(\pm\)0.00 \\ \hline
400 & 1.97 \(\pm\)0.53 & 0.59 \(\pm\)0.31 & 1.19 \(\pm\)0.53 & 0.81 \(\pm\)0.50 & 2.87 \(\pm\)0.57 & 93.99 \(\pm\)2.50 & 97.89 \(\pm\)0.50 & 91.73 \(\pm\)2.50 & 98.44 \(\pm\)0.0

[MISSING_PAGE_FAIL:33]

Figure 9: Auto-labeling MNIST data using linear classifiers. Validation size = 12k. Maximum training samples = 1600. Each round algorithm queries 160 samples. Coverage of auto-labeling is 62.9% with 98.0% accuracy. For the rounds we show, the test error rates are 21.4%, 13.9%, 12.5%, 10.2%, and 9.8%, respectively. For four columns (left to right), we show: **a)** The samples that are labeled by TBAL in this round. **b)** The embeddings for training samples. **c)** The embeddings for validation data points. **d)** The score distribution for the validation dataset.

Figure 10: Auto-labeling CIFAR-10 data using a small network and energy scores. Validation size = 10k. Maximum training samples = 25k. Each round algorithm queries 2500 samples. Coverage of auto-labeling is 5.3% with 90.0% accuracy. For the rounds we show, the test error rates are 56.6%, 55.2%, 55.6%, 53.0%, and 49.3% respectively. For four columns (left to right), we show: **a**) The samples that are labeled by TBAL in this round. **b**) The embeddings for training samples. **c**) The embeddings for validation data points. **d**) The score distribution for the validation dataset.

Figure 11: Auto-labeling CIFAR-10 data using a small network and softmax scores. Validation size = 10k. Maximum training samples = 25k. Each round algorithm queries 2500 samples. Coverage of auto-labeling is 2.3% with 91.0% accuracy. For the rounds visualized here in each row, the test error rates of the trained classifiers are 56.6%, 59.1%, 52.8%, 50.5%, and 51.7% respectively. For four columns (left to right), we show: **a**) The samples that are labeled by TBAL in this round. **b**) The embeddings for training samples. **c**) The embeddings for validation data points. **d**) The score distribution for the validation dataset.