# An effective framework for estimating individualized treatment rules

Joowon Lee\({}^{1}\), Jared D. Huling\({}^{2}\), Guanhua Chen\({}^{1}\)

\({}^{1}\) University of Wisconsin-Madison, \({}^{2}\) University of Minnesota

joowon.lee@wisc.edu, huling@umn.edu, gchen25@wisc.edu

###### Abstract

Estimating individualized treatment rules (ITRs) is fundamental in causal inference, particularly for precision medicine applications. Traditional ITR estimation methods rely on inverse probability weighting (IPW) to address confounding factors and \(L_{1}\)-penalization for simplicity and interpretability. However, IPW can introduce statistical bias without precise propensity score modeling, while \(L_{1}\)-penalization makes the objective non-smooth, leading to computational bias and requiring sub-gradient methods. In this paper, we propose a unified ITR estimation framework formulated as a constrained, weighted, and smooth convex optimization problem. The optimal ITR can be robustly and effectively computed by projected gradient descent. Our comprehensive theoretical analysis reveals that weights that balance the spectrum of a 'weighted design matrix' improve both the optimization and likelihood landscapes, yielding improved computational and statistical estimation guarantees. In particular, this is achieved by distributional covariate balancing weights, which are model-free alternatives to IPW. Extensive simulations and applications demonstrate that our framework achieves significant gains in both robustness and effectiveness for ITR learning against existing methods.

## 1 Introduction

Traditional medicine often uses a "one-size-fits-all" approach where the same treatment is applied to all patients regardless of their unique attributes, aiming to find a single optimal treatment that may be most effective for a broad population. However, since not everyone fits the mold, precision medicine has been popular in medical research, emphasizing personalized treatment based on a patient's unique characteristics [11].

One key element of precision medicine is the estimation of _individualized treatment regime_ (ITR), also known as treatment policy or policy. ITRs consider various factors like demographics and socio-psychological aspects to optimize treatment decisions, thereby maximizing individual outcomes. Many frameworks have been proposed for learning ITRs [41, 54, 57, 7, 58, 60, 56] and the related _conditional average treatment effect_ (CATE) estimation problem [3, 53, 46, 13, 36]. For model interpretation and other practical concerns, researchers often focus on pre-specified rule classes. For example, the method proposed in [2] focuses on estimating ITRs using shallow-depth decision trees.

The central task in the ITR estimation is controlling confounding factors to isolate the causal effect of treatment from other factors that may influence the outcome. A standard approach is by weighting each sample using their _inverse probability weights_ (IPWs), which requires specifying a propensity score model. However, propensity score methods have long been known to be highly sensitive to model misspecification, which yields biased estimates of causal effects [25, 30]. _Distributional covariate balancing weights_ (DCBWs) [24, 27] are modern alternatives to IPWs, directly minimizing the distance between empirical covariate distributions induced by the weights.

ITR estimation can be framed as an optimization or _maximum likelihood estimation_ (MLE) problem, where the choice of weights directly impacts both the optimization and likelihood landscapes. This, inturn, affects the computational and statistical errors in the ITR estimation, as well as the performance of the optimization algorithm. The key contribution of this work is to bridge the gap between causal inference and optimization perspectives in ITR estimation. Our main finding is both simple and surprising: _Covariate balancing weights not only address confounding but also yield near-optimal optimization and likelihood landscapes for the ITR estimation._ We show this claim by formulating the ITR estimation problem as a constrained, weighted, and smooth convex optimization problem. Our formulation is grounded in _angle-based direct learning_ (AD-Learning) framework [39], which is a recent framework for multi-category treatments with a linear decision function class assumption. While we focus on multi-category treatments--a relatively underexplored area--we believe that the principles are equally applicable to binary treatments with a linear decision function.

Besides advancing our theoretical understanding of the impact of covariate balancing weights, we propose a unified computational and statistical framework for the ITR estimation. Notably, we introduce a hard \(L_{1}\)-ball constraint to promote sparsity in regression coefficients, replacing the traditional soft \(L_{1}\)-penalization. This approach keeps the optimization objective smooth, enabling fast and accurate ITR estimation using _projected gradient descent_ (PGD). Furthermore, we integrate techniques such as (1) variable screening, (2) outcome augmentation, and (3) inverse variance weighting to account for heteroscedastic errors, which together show a synergistic effect.

**Related Work.** Most existing ITR approaches use IPW, which requires specifying a propensity score model for confounding control. Recent developments in causal inference have also introduced robust approaches that directly estimate weights based on balance-seeking objectives rather than relying solely on propensity scores. These approaches include entropy balancing weights [20], stable balancing weights [61] as well as DCBWs [24, 27]. However, despite the weights serving a critical role in the estimation process, these advancements have mainly focused on improving _average treatment effect_ (ATE) and are largely under-explored in the ITR literature. To the best of our knowledge, this is the first work to apply DCBWs for ITR-Learning under a weighted optimization framework. While recent works have taken a similar approach by utilizing weighting schemes and directly optimizing the weights rather than relying on estimated IPW [28, 26], our method differs in key aspects. Unlike [26], which involves the evaluation of policy effects, our approach directly learns the optimal decision function, bypassing the need for intermediate policy effect evaluations. Although [28] introduced the concept of retargeting the population covariate distribution, which is conceptually related to our analysis, their approach relies on a reference policy, whereas our method does not. We also explore various statistical techniques to improve ITR-Learning. First, we perform _variable screening_ using the distance covariance test [48] to retain key effect modifiers and precision variables that directly impact the outcome. Second, [50, 9] introduced _outcome augmentation_ to minimize estimator variance in clinical trials and observational studies by IPW involving binary treatments. Building on these foundations, our work extends these approaches to a broader range of weighting schemes, such as DCBW particularly for multi-category treatments. Lastly, with inverse variance weighting, as introduced in _stabilized angle-based direct learning_ (SABD-Learning) to address heteroscedasticity [44], this combined strategy enhances estimator precision and robustness.

**Contributions.** We summarize our contributions below.

* **Unified Framework of ITR-Learning**: We introduce a novel framework that addresses the limitations of existing ITR estimation methods by formulating the problem as a constrained, weighted, and smooth convex optimization problem. Under the framework, we propose a PGD algorithm for ITR-Learning under a hard \(L_{1}\)-constraint.
* **Improved Computational and Statistical Guarantees**: We establish convergence guarantees (Theorem 3.3). Furthermore, under mild assumptions, we demonstrate that the parameter of ITR-Learning can be consistently estimated with high probability (Theorem 3.5) and provide computational and sample complexity. In both cases, we demonstrate that using covariate balancing weights controls confounding factors and leads to better optimization and likelihood landscapes, resulting in improved computational and statistical performance.
* **Statistical Framework of ITR-Learning**: Our main contribution lies in providing a unified framework that combines DCBWs, variable screening, outcome augmentation, and inverse variance weighting in a synergistic and effective way. We demonstrated theoretical justifications for the combined approach, which, to our knowledge, have not been previously established.

Methods for ITR Estimation

### Model Setup

**Individualized Treatment Rule and Optimal Decision Function.** We observe a random vector \((\mathbf{X},A,Y)\) where \(\mathbf{X}=(X_{1},X_{2},\ldots,X_{p})\in\mathcal{X}\) denotes the \(p\)-dimensional vector of pre-treatment covariates, a received treatment \(A\in\mathcal{A}=\{1,2,\ldots,K\}\), and the corresponding outcome \(Y\). Without loss of generality, we assume higher \(Y\) values are more favorable. Let \(Y(a)\) denote the random variable that describes the potential outcome that would have been observed were the individual assigned to treatment \(a\in\mathcal{A}\). We make the following standard assumptions using the potential outcome framework [43]: (i) Stable Unit Treatment Value Assumption (SUTVA): \(Y=Y(A)\), (ii) positivity: \(0<\pi(a,\mathbf{X}):=\mathbb{P}(A=a|\mathbf{X})<1\) for all \(a\in\mathcal{A}\) and all \(\mathbf{X}\in\mathcal{X}\), (iii) no unmeasured confounding: \(Y(a)\perp A|\mathbf{X}\) for any \(a\in\mathcal{A}\).

An ITR, \(d(\mathbf{x})\), is a function mapping each covariate \(\mathbf{x}\) to one of the \(K\) treatments. According to [41], the ITR \(d\) can be measured by the value function \(V(d)\):

\[V(d):=\mathbb{E}[Y(d(\mathbf{X}))]=\mathbb{E}[Y|A=d(\mathbf{X})]=\mathbb{E} \left[\frac{Y\ \mathbf{1}(A=d(\mathbf{X}))}{\pi(A,\mathbf{X})}\right]\]

Then the optimal ITR \(d^{\text{opt}}\) is defined as a function that maximizes the expected potential outcome \(d^{\text{opt}}(\mathbf{x})\in\arg\max_{d\in\mathcal{D}}V(d)\) among all functions belonging to the treatment rule class (\(\mathcal{D}\)). Building on this, we adopt the following working model:

\[Y=\mu(\mathbf{X})+\sum_{k=1}^{K}\delta_{k}(\mathbf{X})\mathbf{I}(A=k)+\epsilon, \tag{1}\]

where \(\mu(\mathbf{X})\) is treatment-free effect, \(\delta_{k}(\mathbf{X})\) is interaction effect between covariates and \(k\)th treatment, and \(\epsilon\) is a random noise with mean zero and variance \(\sigma^{2}(A,\mathbf{X})\). For model identifiability, we assume \(\sum_{k=1}^{K}\delta_{k}(\mathbf{x})=0\) for all \(\mathbf{x}\). Under this model, \(\delta_{k}(\mathbf{X})\) determines the optimal ITR, while \(\mu(\mathbf{x})\) has no impact on the ITR. One approach to identifying the optimal ITR is to use \(K\) separate decision functions for each treatment with the sum-to-zero constraint, but this approach can be computationally inefficient [55].

Instead, one can opt for simplex coding as an alternative, which inherently satisfies the sum-to-zero constraint. In AD-Learning [39], each of the \(K\) treatments is represented as a vertex simplex on \(\mathbb{R}^{K-1}\), denoted as \(\mathbf{u}_{k}\), \(k=1,\ldots,K\). In particular, \(\mathbf{u}_{k}\) is defined as

\[\mathbf{u}_{k}=\begin{cases}\frac{1}{\sqrt{K-1}}\mathbf{1}_{K-1}&\text{ if }k=1,\\ -\frac{1+\sqrt{K}}{\sqrt{(K-1)^{3}}}\mathbf{1}_{K-1}+\sqrt{\frac{K}{K-1}} \mathbf{e}_{k-1}&\text{ if }k\geq 2,\end{cases}\]

where \(\mathbf{1}_{K-1}\) is a \(K-1\) dimensional vector with entries 1, and \(\mathbf{e}_{k-1}\) is a \(K-1\) dimensional vector with entries 0 except its \(k\)-th entry being 1. This vertex simplex has \(K\) vertices with equal angles between them and an origin at the center of the simplex. All \(\mathbf{u}_{k}\) have the same Euclidean norm \(1\). Then the optimal ITR is reformulated as follows:

\[d^{\text{opt}}(\mathbf{x})=\operatorname*{arg\,max}_{k\in\{1,\ldots,K\}} \mathbb{E}[Y|\mathbf{x},A=k]=\operatorname*{arg\,max}_{k\in\{1,\ldots,K\}} \mathbf{u}_{k}^{T}\underbrace{\mathbb{E}\left[\frac{Y\mathbf{u}}{\pi(A, \mathbf{x})}\bigg{|}\mathbf{x}\right]}_{=\mathbf{f}_{\text{opt}}(\mathbf{x})}, \tag{2}\]

where \(\mathbf{u}\) is a random treatment vector in \(\mathbb{R}^{K}\), corresponding to the random treatment \(A\). Then estimating the optimal ITR can be converted into estimating the decision function \(\mathbf{f}(\mathbf{x})=(f_{1}(\mathbf{x}),\ldots,f_{K-1}(\mathbf{x}))^{T}\), assigning a \(K-1\) dimensional vector to each covariate \(\mathbf{x}\). Here, while the optimal decision function \(\mathbf{f}_{\text{opt}}(\cdot)\) can take a generic form, we assume the linear decision function within the linear treatment rule class \(\mathcal{D}\), induced by the linear decision function class \(\mathcal{F}=\{\mathbf{f}(\mathbf{X})=\mathbf{B}^{T}\mathbf{X}:\mathbf{B}\in \mathbb{R}^{p\times(K-1)}\}\).

**Reduction to Weighted Convex Optimization.** Suppose we have a sample \((\mathbf{x}_{i},a_{i},y_{i})_{i=1}^{n}\) of size \(n\) from the joint population distribution for \((\mathbf{X},A,Y)\). Estimating the optimal decision function \(\mathbf{f}_{\text{opt}}\) from the observed finite sample can be reduced to solving a weighted convex optimization problem:

\[\min_{\mathbf{B}\in\mathbb{R}^{p\times(K-1)}}\ \frac{1}{n}\sum_{i=1}^{n}w(a_{i}, \mathbf{x}_{i})\,\ell(y_{i},\mathbf{x}_{i},a_{i};\mathbf{B})+R(\mathbf{B}). \tag{3}\]There are three key components in the problem (3): the per-sample objective function \(\ell\), the per-sample weights \(w\), and the regularization \(R(\mathbf{B})\). First, \(\ell\) denotes a model-dependent per-sample convex objective function given by

\[\ell(y_{i},\mathbf{x}_{i},a_{i};\mathbf{B})=\begin{cases}\frac{1}{2}\left(\frac {K}{K-1}y_{i}-\mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i}\right)^{2}& \text{for continuous outcome,}\\ \log\left(1+\exp(\mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i})\right)-y _{i}\mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i}&\text{for binary outcome.}\end{cases} \tag{4}\]

An important characteristic of causal inference problems is that only one of the possible treatments can be observed for each subject. Thus, it is critical to control confounding variables to isolate the causal effect of the treatment from other factors that might influence the outcome. This effectively is done by choosing the appropriate weight \(w_{i}=w(a_{i},\mathbf{x}_{i})\) for the \(i\)th subject. A classic approach is to use IPW: \(w(a,\mathbf{x})=1/\pi(a,\mathbf{x})\)[39].

### Proposed ITR Estimation Framework

**Reducing Finite Sample Bias.** The classic IPW approach requires specifying a propensity score model for confounding control. However, it is well-known that propensity score methods are highly sensitive to model misspecification, which yields biased estimates of causal effects [25; 30]. To address this issue, we investigate incorporating alternative, model-free weighting schemes such as _energy balancing weights_ (EBWs) [23; 24], which are a type of DCBWs. Other options for DCBWs include _maximum mean discrepancy_ (MMD) balancing weights [19; 27; 8] and Wasserstein distance-based balancing weights [52; 1]. Specifically, under the working model assumption (1), the optimal decision function \(\mathsf{f}_{\text{opt}}\) in (2) can be decomposed as

\[\mathbf{f}_{\text{opt}}(\mathbf{x})=\underbrace{\mu(\mathbf{x})\mathbb{E}\left[ \frac{\mathbf{u}}{\pi(A,\mathbf{x})}\middle|\mathbf{x}\right]}_{\text{treatment- free}}+\underbrace{\sum_{k=1}^{K}\delta_{k}(\mathbf{x})\mathbb{E}\left[\frac{ \mathbf{u}I(A=k)}{\pi(A,\mathbf{x})}\middle|\mathbf{x}\right]}_{\text{interaction}}+ \mathbb{E}\left[\frac{\mathbf{u}}{\pi(A,\mathbf{x})}\middle|\mathbf{x}\right] \mathbb{E}[\epsilon|\mathbf{x}]. \tag{5}\]

Since \(\mathbb{E}\big{[}\frac{\mathbf{u}}{\pi(A,\mathbf{x})}\middle|\mathbf{x}\right]=0\) and \(\mathbb{E}[\epsilon|\mathbf{x}]=0\), it follows that \(\mathbf{f}_{\text{opt}}(\mathbf{x})=\sum_{k=1}^{K}\delta_{k}(\mathbf{x}) \mathbf{u}_{k}\), depending only on the interaction term. However, suppose the true propensity score is not correctly estimated. In that case, the estimated decision function may be biased and include additional factors including the treatment-free effect, leading to sub-optimal treatment decisions. There are two possible sources for this estimation error. First, an incorrectly specified propensity score model might lead to an estimated propensity score \(\hat{\mathbb{P}}_{n}(A=a|\mathbf{x})\) that deviates from the true propensity score \(\pi(A=a|\mathbf{x})\). Second, even with correct specification, insufficient sample size may lead to inaccurate finite-sample approximation \(\hat{\mathbb{P}}_{n}(A=a|\mathbf{x})\) of the true propensity score \(\pi(A=a|\mathbf{x})\)[31]. It results in systematic bias for the estimated coefficient of the treatment-free term:

\[\hat{\mathbb{E}}_{n}\left[\frac{\mathbf{u}}{\pi(A,\mathbf{x})}\middle|\mathbf{ x}\right]=\sum_{k=1}^{K}\frac{\mathbf{u}_{k}}{\pi(k,\mathbf{x})}\hat{\mathbb{P}}_{n}(A =k|\mathbf{x})\neq 0.\]

Instead of using IPW, we use DCBWs such as EBWs. Such weights are data-driven and do not rely on specific model assumptions or large sample approximation. Furthermore, EBWs are known to promote independence between treatment and confounders [23], so \(\hat{\mathbb{E}}_{n}[\mathbf{u}w(A,\mathbf{x})\,|\,\mathbf{x}]\approx\hat{ \mathbb{E}}_{n}[\mathbf{u}]\,\hat{\mathbb{E}}_{n}[w(A,\mathbf{x})\,|\,\mathbf{ x}]=0\) since \(\hat{\mathbb{E}}_{n}[\mathbf{u}]=0\). Thus, EBWs may reduce the unfavorable impacts of bias on the finite-sample decision function.

**Improving Optimization Guarantees.** To our best knowledge, the impact of the choice of the regularization \(R(\mathbf{B})\) on the model parameter \(\mathbf{B}\) on the detailed optimization landscape, especially in terms of the convergence rate and statistical estimation error bounds has been under-investigated in the literature. Contrary to the literature, we propose to use the combination of _soft \(L_{2}\)-regularization and a hard \(L_{1}\)-ball constraint:_

\[\min_{\mathbf{B}\in\mathbb{R}^{p\times(K-1)},\,\|\mathbf{B}\|\leq\lambda_{1}} \left[\mathcal{L}(\mathbf{B}):=\frac{1}{n}\sum_{i=1}^{n}w(a_{i},\mathbf{x}_{i })\,\ell(y_{i},x_{i},a_{i};\mathbf{B})+\frac{\lambda_{2}}{2}\|\mathbf{B}\|_{F }^{2}\right]. \tag{6}\]

The above formulation has a smooth objective function that is unaffected by the hard \(L_{1}\)-ball constraint. Also, \(L_{2}\)-regularization ensures that the objective is at least \(\lambda_{2}\)-strongly convex. Thus, the PGD algorithm (with suitable stepsize) enjoys a strong exponential convergence guarantee toward the global optimum. In contrast, standard approaches with \(L_{1}\)-penalization loss need sub-gradient methods since it is non-smooth. It is known that the convergence rate of the sub-gradient methods is much slower than gradient methods [6; 18].

We use the classical PGD to solve the convex-constrained optimization problem in (6). In each iteration, it involves conducting gradient descent with a suitable stepsize \(\alpha_{t}\), followed by a projection \(\Pi_{\mathcal{B}}\) onto the \(L_{1}\)-ball \(\mathcal{B}:=\{\mathbf{B}\in\mathbb{R}^{p\times(K-1)}\,:\,\|\mathbf{B}\|\leq \lambda_{1}\}\):

\[\mathbf{B}_{t}\leftarrow\Pi_{\mathcal{B}}\big{(}\mathbf{B}_{t-1}-\alpha_{t} \nabla_{\mathbf{B}}\mathcal{L}(\mathbf{B}_{t-1})\big{)}. \tag{7}\]

For the projection onto the \(L_{1}\)-projection, we use the algorithm in [14]. Detailed implementation of this PGD algorithm is discussed in Algorithm 1 in the appendix.

**Additional Improvements by Variance and Dimension Reduction.** To ensure the stability of estimates for both balancing weights and outcome regression models, a variable screening step is recommended. We propose adapting distance covariance test [48] for its computational efficiency and its ability to detect complex dependencies between treatment and outcome. By focusing on screened variables that significantly impact the outcome, we can improve the estimation of outcome regression models. Additionally, estimating balancing weights based on effect modifiers rather than all variables provides more reliable weighting estimates. Hence, the screening approach serves to reduce bias and enhance the effectiveness of ITR-Learning. See Algorithm 2 in the appendix.

Lastly, we propose using both augmented outcomes and inverse variance weighting for variance reduction in learning ITRs. Under the working model assumption in (1), the interaction effect determines optimal treatment assignments, while the treatment-free effect acts as additional noise. Outcome augmentation helps reduce variance by mitigating residual variability due to the treatment-free effect. Nevertheless, misspecification of either the outcome or treatment-free effect models can induce heteroscedastic errors due to residual treatment-free effects [35]. In such scenarios, using inverse variance weighting, introduced in SABD-Learning, enhances robustness against misspecification [44]. Consequently, the combination of outcome augmentation and inverse variance weighting offers a robust and efficient approach to ITR-Learning, as they are complementary methods. Details are discussed in the appendix C.3 and C.4.

## 3 Statement of results

Our theoretical analysis shows that a "weighted design matrix", specifically a \(p(K-1)\times p(K-1)\) positive semi-definite matrix, plays a central role in the local landscape analysis of the ITR estimation problem:

\[\mathbf{\Psi}:=\frac{1}{n}\sum_{i=1}^{n}\omega(a_{i},\mathbf{x}_{i})(\mathbf{u }_{a_{i}}\mathbf{u}_{a_{i}}^{T})\otimes\left(\mathbf{x}_{i}\mathbf{x}_{i}^{T} \right), \tag{8}\]

where \(\otimes\) denotes the Kronecker product. Without loss of generality, we assume \(\sum_{i}w(a_{i},\mathbf{x}_{i})=n\). Note that the weights \(w_{i}=w(a_{i},\mathbf{x}_{i})\), \(i=1,\ldots,n\) affect the eigenvalues of the weighted design matrix \(\mathbf{\Psi}\). Our analysis reveals that, for the improved convergence rate of the PGD algorithm (Theorem 3.3) as well as a smaller statistical estimation error (Theorem 3.5), we need to choose the weights \(w_{i}\) such that the minimum eigenvalue of \(\mathbf{\Psi}\) is as large as possible. Roughly speaking, this is achieved when \(w_{1},\ldots,w_{n}\) are 'covariate balancing weights' where the weighted sample covariance matrices conditional for each treatment are approximately the same. In the special case when the covariates are discrete and one-hot encoded, the optimal weights are the ones that exactly balance the covariate distributions given treatments. Thus this provides an explicit spectral characterization of the DCBWs in causal inference literature. See the appendix D for more discussions.

### Computational Guarantees

In order to control the strong convexity and the smoothness parameter of the ITR objective (6), we require that the eigenvalues of \(\mathbf{\Psi}\) are uniformly bounded.

**Assumption 3.1** (Eigenvalue bounds on the weighted design matrix).: There are constants \(0\leq\lambda^{-}\leq\lambda^{+}\) such that the eigenvalues of \(\mathbf{\Psi}\) in (8) is between \(\lambda^{-}\) and \(\lambda^{+}\),For each subject \(i=1,\ldots,n\), let \(z_{i}\) and \(p_{i}\) denote its 'activation' and 'predictive probability', where

\[z_{i}:=\mathbf{u}_{\mathbf{a}_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i},\qquad p_{i}: =\exp(z_{i})/(1+\exp(z_{i})). \tag{9}\]

We need the activation \(z_{i}\) to be uniformly bounded for our theoretical analysis of the binary outcome. Since the simplex vectors \(\mathbf{u}_{a_{i}}\) have unit length, it is enough to require the following assumption.

**Assumption 3.2** (Bounded activation).: There exists a constant \(M>0\) such that for all model parameter \(\mathbf{B}\in\mathbb{R}^{p\times(K-1)}\) in the constraint set \(\mathcal{B}\) and observed sample \(i=1,\ldots,n\), \(\|\mathbf{B}^{T}\mathbf{x}_{i}\|\leq M\). Consequently, there exist constants \(0<\alpha^{-}\leq\alpha^{+}\leq 1/4\) such that \(\alpha^{-}\leq p_{i}(1-p_{i})\leq\alpha^{+}\) for all \(i\).

With our hard \(L_{1}\)-norm constraint on the model parameter \(\mathbf{B}\), Assumption 3.2 can be easily verified whenever the covariate vectors \(\mathbf{x}_{i}\) are uniformly bounded, which is a standard in the literature.

We establish the convergence rate of Algorithm 1 for ITR-Learning with stepsizes that are fixed but sufficiently small and diminishing rate. An informal statement is given in Theorem 3.3 below, and a full statement is provided in Theorem E.1 in the appendix. Define

\[\mu:=\begin{cases}\lambda^{-}+\lambda_{2}&L:=\begin{cases}\lambda^{+}+\lambda_ {2}&\text{for continuous outcome}\\ \alpha^{-}\lambda^{+}+\lambda_{2}&\text{for binary outcome}\end{cases}\end{cases} \tag{10}\]

We say a function \(f:\mathbb{R}^{p}\to\mathbb{R}\) is \(\mu\)_-strongly convex_ if \(f(x)-\frac{\mu}{2}\|x\|^{2}\) is convex and \(L\)_-smooth_ if \(f\) is differentiable and \(\nabla f\) is \(L\)-Lipschitz continuous.

**Theorem 3.3** (Convergence rate of PGD for ITR-Learning).: _Let \((\mathbf{B}_{t})_{t\geq 0}\) denote the sequence of parameters obtained by the PGD algorithm (7) for ITR-Learning problem (6) with arbitrary initialization \(\mathbf{B}_{0}\). Suppose Assumptions 3.1 and 3.2 hold. Let \(\mathbf{B}^{\star}\) denote the unique global optimum of (6). Then the following hold:_

**(i)**: _(Optimization landscape) The ITR objective_ \(\mathcal{L}(\mathbf{B})\) _in (_6_), is_ \(\mu\)_-strongly convex and_ \(L\)_-smooth, where_ \(\mu,L\) _are as in (_10_)._
**(ii)**: _(Linear convergence with fixed stepsize) Assume constant stepsize_ \(\alpha_{t}\equiv\alpha<2/L\)_. Denote the contraction constant_ \(\rho(\alpha):=\max\{|1-\alpha L|,|1-\alpha\mu|\}\in(0,1)\)_. Then_ \(\|\mathbf{B}_{t}-\mathbf{B}^{\star}\|_{F}^{2}\leq\rho(\alpha)^{t}\|\mathbf{B} _{0}-\mathbf{B}^{\star}\|_{F}^{2}\) _for all_ \(t\geq 1\)_._
**(iii)**: _(Sublinear convergence with diminishing stepsize) Assume diminishing stepsize_ \(\alpha_{t}=\frac{\beta}{\gamma+t}\)_, where_ \(\beta=\beta(\mu,L)>0\) _is a sufficiently large constant and_ \(\gamma>0\) _is an arbitrary constant. Then_ \(\|\mathbf{B}_{t}-\mathbf{B}^{\star}\|^{2}\leq\frac{\nu}{\gamma+t}\) _for all_ \(t\geq 1\) _for some constant_ \(\nu>0\)_._

Our algorithm 1 with soft \(L_{2}\)-penalization and hard constraint with \(L_{1}\)-ball is guaranteed to converge to the true solution \(\mathbf{B}^{\star}\) exponentially fast provided stepsizes \(\alpha_{t}\) are fixed and sufficiently small.

### Statistical Guarantees

Next, we introduce generative models for the ITR estimation problem, for which we will establish statistical estimation guarantees in Theorem 3.5. Fix a joint distribution \(\pi\) for the pair \((\mathbf{X},A)\) of covariates \(\mathbf{X}\in\mathbb{R}^{p}\) and treatment \(A\in\{1,\ldots,K\}\). Fix a true parameter \(\mathbf{B}_{\star}\in\mathbb{R}^{p\times(K-1)}\). Then we assume \(n\) i.i.d. samples \((\mathbf{x}_{i},a_{i},y_{i})\) are drawn as \((\mathbf{x}_{i},a_{i})\sim\pi\) and

\[y_{i}|(\mathbf{x}_{i},a_{i})\sim\begin{cases}N\left(\frac{K-1}{K}z_{i},\sigma^ {2}\right)&\text{for continuous outcome}\\ \text{Ber}\left(\frac{\exp(z_{i})}{1+\exp(z_{i})}\right)&\text{for binary outcome}\end{cases},\quad\text{where }z_{i}:=\mathbf{u}_{a_{i}}^{T}\mathbf{B}_{\star}^{T}\mathbf{x}_{i}, \tag{11}\]

where \(N(\mu,\sigma^{2})\) denotes a normal distribution with mean \(\mu\) and variance \(\sigma^{2}\), whereas \(\text{Ber}(p)\) denotes a Bernoulli distribution with mean \(p\). We assume that \(\pi\) does not depend on \(\mathbf{B}_{\star}\) and the noise variance \(\sigma^{2}\) for the continuous case is known. We then seek to estimate the true parameter \(\mathbf{B}_{\star}\) from the observed samples. It is easy to check that the negative log-likelihood function (up to additive constants) coincides with the per-sample loss function \(\ell\) in (4). Therefore, the weighted convex optimization problem (3) corresponds to weighted MLE under the generative models in (11) (see Section H in the appendix for details).

We assume that the weighted gradient of the per-sample loss \(\ell\) in (4) has uniformly bounded third moments for our statistical analysis of the generative ITR model above.

**Assumption 3.4** (Bounded moments at \(\mathbf{B}_{\star}\) and weights).: Suppose \((\mathbf{x}_{i},a_{i},y_{i})\) follows the generative model above. Denote \(U_{i}:=w_{i}\nabla_{\mathbf{B}}\ell(y_{i},\mathbf{x}_{i},a_{i};\mathbf{B}_{\star })\in\mathbb{R}^{p\times(K-1)}\) for \(w_{i}:=w(a_{i},\mathbf{x}_{i})\) and \(\overline{U}_{i}:=U_{i}-\mathbb{E}[U_{i}]\). Suppose there are constants \(D_{1},d_{1}\in(0,\infty)\) such that \(\mathbb{E}[\|\overline{U}_{i}\|_{F}^{3}]<D_{1}\) and \(\min_{k,l}\text{Var}(\overline{U}_{i}(k,l))>d_{1}\). Also, \(\sum_{i=1}^{n}w_{i}^{3}/\left(\sum_{i=1}^{n}w_{i}^{2}\right)^{3/2}=O(n^{-1/2})\).

Now we state our main result regarding the statistical estimation guarantee for the true generative model parameter \(\mathbf{B}_{\star}\). With high probability, Algorithm 1 can recover \(\mathbf{B}_{\star}\) up to a statistical error \(O(n^{-1/2})\).

**Theorem 3.5** (Statistical estimation guarantee).: _Let \((\mathbf{x}_{i},a_{i},y_{i})_{i=1}^{n}\) be i.i.d. observations from the generative models in (11) with true parameter \(\mathbf{B}_{\star}\in\mathbb{R}^{p\times(K-1)}\) such that \(\|\mathbf{B}_{\star}\|_{1}\leq\lambda_{1}\) for some \(\lambda_{1}\geq 0\). Suppose Assumptions 3.1, 3.2, and 3.4 hold. Let \(\widehat{\mathbf{B}}_{T}\) denote the weighted MLE obtained after \(T\) iterations of PGD algorithm (7) for (3). Fix a constant \(\varepsilon>0\)._

_Then there exists a constant \(C=C(\varepsilon)>0\) such that with probability at least \(1-\varepsilon\) and for \(\mu\) as in (10) but with \(\lambda^{\pm}=\lambda^{\pm}(\mathbb{E}[\mathbf{\Psi}])\),_

\[\|\mathbf{B}_{\star}-\widehat{\mathbf{B}}_{T}\|_{F}\leq\frac{C\sqrt{(p/n)\log \epsilon^{-1}}}{\mu}+\frac{8\lambda_{2}\|\mathbf{B}_{\star}\|_{F}}{\mu}, \tag{12}\]

_provided \(n,T\) are large enough. More specifically, the above holds when_

**(i)**: _(Sample complexity)_ \(n\geq 1\) _large s.t._ \(\frac{(\sum_{i=1}^{n}w_{i}^{2})^{3}}{(\sum_{i=1}^{n}w_{i}^{2})^{2}}\geq C_{1} \varepsilon^{-2}\) _for some explicit constant_ \(C_{1}>0\)_;_
**(ii)**: _(Computational complexity)_ \(T=O(\log n)\) _and_ \(O(n^{-1})\) _when constant or diminishing stepsize as in Theorem_ 3.3 _is used, respectively._

_Furthermore, we get \(\sqrt{n}\)-consistent estimation whenever \(\lambda_{2}=O(n^{-1/2}\|\mathbf{B}_{\star}\|_{F}^{-1})\)._

Notice that both terms in the error bound (12) are proportional to \(1/\mu\), which depends both on the minimum eigenvalue \(\lambda^{-}\) of \(\mathbb{E}[\mathbf{\Psi}]\) as well as the \(L_{2}\)-regularization parameter \(\lambda_{2}\). Therefore, choosing the weighting function \(\omega(A,\mathbf{X})\) to maximize \(\lambda^{-}\) helps minimize the overall statistical estimation error. (Detailed analysis and discussion on this point can be found in the appendix H.) Another easy way to increase \(\mu\) is to use large \(L_{2}\)-regularization, but as in the second term in (12), using \(L_{2}\)-regularization gives estimation bias of \(O(\lambda_{2}\|\mathbf{B}_{\star}\|_{F}/\mu)\). Hence, we need to choose \(\lambda_{2}\) is small enough so that this bias term is also of \(O(n^{-1/2})\). Then we obtain an overall \(\sqrt{n}\)-consistent estimator.

The key insight in our proof of Theorem 3.5 is that we can decompose the total estimation error \(\|\mathbf{B}_{\star}-\widehat{\mathbf{B}}_{T}\|_{F}\) into computational and statistical parts:

\[\|\mathbf{B}_{\star}-\widehat{\mathbf{B}}_{T}\|_{F}\leq\underbrace{\| \widehat{\mathbf{B}}-\widehat{\mathbf{B}}_{T}\|_{F}}_{\text{computational error}}+\underbrace{\|\mathbf{B}_{\star}-\widehat{\mathbf{B}}\|_{F}}_{\text{ statistical error}}, \tag{13}\]

where \(\widehat{\mathbf{B}}\) denotes the exact MLE. The computational error vanishes as the number \(T\) of PGD iterations tends to infinity according to Theorem 3.3. For the statistical error, we show

\[\mathbb{P}\left(\|\mathbf{B}_{\star}-\widehat{\mathbf{B}}\|_{F}\leq\frac{2C}{ \sqrt{n}}+\frac{8\lambda_{2}\|\mathbf{B}_{\star}\|_{F}}{\mu}\right)\geq 1-O \left(\frac{\sum_{i=1}^{n}w_{i}^{3}}{(\sum_{i=1}^{n}w_{i}^{2})^{3/2}}\right). \tag{14}\]

Thus, the'skewness' of the weights \(w_{i}\) measured by the ratio \(\sqrt{n}\sum_{i=1}^{n}w_{i}^{3}/\left(\sum_{i=1}^{n}w_{i}^{2}\right)^{3/2}\) (which arises from Berry-Esseen bound) acts as a scalar multiple to the standard statistical error of \(O(n^{-1/2})\) per the central limit theorem. Hence, more balanced weights lead to fewer statistical estimation errors. If this skewness ratio is uniformly bounded, we can ensure a small statistical error with probability at least \(1-\varepsilon\), provided we have at least \(O(\varepsilon^{-2})\) samples. Then, we simply need to ensure the PGD iteration \(T\) is large enough so that the computational error is at most \(C/\sqrt{n}\) to achieve the high-probability total estimation error bound in Theorem 3.5. Another insight from the theorem is that maximizing \(\lambda_{\min}(\mathbb{E}[\mathbf{\Psi}])\) promotes heterogeneous balancing weights \(w_{i}\), and we get increased sample complexity through the bound \(\frac{(\sum_{i=1}^{n}w_{i}^{2})^{3}}{(\sum_{i=1}^{n}w_{i}^{2})^{2}}\geq C_{1} \varepsilon^{-2}\) (e.g., if \(w_{i}\equiv 1\), this is \(n\gtrapprox\epsilon^{-2}\), but if \(w_{1}=n,w_{2}=\cdots=w_{n}=0\), this may not be satisfied for any \(n\)).

## 4 Simulation Studies

We investigate four simulation settings, each designed to evaluate various factors influencing the performance of different ITR estimators. For each setting, we mimic a randomized trial (no-confounding) and an observational study (with confounding) as per [44]. We consider \(4\) treatments with sample sizes \(n=200,600,1000\), and covariate dimensions \(p=20,40,60\). The outcome \(Y\) follows the model (1) with \(\epsilon|A,\mathbf{X}\sim N\big{(}0,\sigma^{2}(A,\mathbf{X})\big{)}\). We evaluate method performance using accuracy (i.e., correct identification of the optimal treatment for each observation) and empirical value on a test dataset of \(10,000\) observations. Each simulation setting is replicated independently \(100\) times.

In this section, we provide partial experiment results according to the proposed estimation framework in Section 2.2. Specifically, we focus on the results from two scenarios: one mimicking randomized trials with linear ITR as the optimal rule and the other mimicking observational studies with nonlinear ITR as the optimal rule for continuous outcome. Since our proposed method assumes linear treatment rule class, the latter involves a situation where our model's prespecified treatment rule class is misspecified. Specifically, we use following treatment-free effect function \(\mu(\mathbf{X})\) and interaction effect function \(\delta(\mathbf{X})\) for each scenario:

1. Randomized Trial: Linear ITR as the true optimal \[\mu(\mathbf{X}) =1+2X_{1}+2X_{2},\] \[\delta(\mathbf{X}) =\begin{cases}0.75+1.5X_{1}+1.5X_{2}+1.5X_{3}+1.5X_{4},A=1;0.75+1.5X_{1}-1.5X_{2}-1.5X_{3}+1.5X_{4},A=2;\\ 0.75+1.5X_{1}-1.5X_{2}+1.5X_{3}-1.5X_{4},A=3;0.75-1.5X_{1}+1.5X_{2}-1.5X_{3}-1. 5X_{4},A=4,\end{cases}\]
2. Observational Study: Nonlinear ITR as the true optimal \[\mu(\mathbf{X}) =1+2X_{1}+2X_{2}+2X_{4}-2X_{4}^{2}+2X_{1}X_{2}+2e^{-X_{1}X_{2}}+ \sin(X_{3}),\] \[\delta(\mathbf{X}) =\begin{cases}0.5+1.0X_{1}-2.0X_{4}+0.5X_{4}^{2},A=1;\ 1.0+1.0X_{1}+1.0X _{4}-1.0X_{4}^{2},A=2;\\ 1.5+2.0X_{1}-1.0X_{4}-1.0X_{4}^{2},A=3;\ 1.0-1.0X_{1}-1.0X_{4}-1.0X_{4}^{2},A=4. \end{cases}\]

in the working model (1). Further details about simulation settings, other scenarios, and corresponding results are available in the appendix I.

Figure 1: **Accuracy Comparison: proposed methods vs. benchmark methods. All subplots in the same row share the same simulation setting, focusing on randomized trials with linear ITR as the true optimal rule (top) and observational studies with nonlinear ITR as the true optimal rule (bottom). Each subplot presents (Left) accuracy comparisons based on weights, illustrating the difference between the standard IPW approach of AD-Learning with the proposed approach using EBW, (Middle) accuracy comparisons based on optimization algorithms, illustrating the difference between the standard \(L_{1}\)-penalized approach against the proposed constrained optimization with PGD, (Right) evaluation of accuracies between existing standard approaches and the proposed method, which integrates EBWs, variance and dimension reduction techniques implemented through PGD. Error bars represent the standard errors of the mean (SEM) of accuracies across multiple simulations.**

**Using Distributional Covariate Balancing Weights.** In the left subplots of Figure 1, the incorporation of DCBWs (e.g., EBWs) shows improvements in classification rates for ITR-Learning. While EBWs indeed provide more effective balancing weights in observational studies, as depicted in the lower left subplot in Figure 1, it also results in improvements even in randomized trial settings where the true propensity score is known. This is achieved by reducing finite sample imbalances, as shown in the upper left subplot in Figure 1. Moreover, when the underlying true treatment rule class is nonlinear, the performance of AD-Learning suffers due to a misalignment between the assumed linear treatment rule class and the actual class. In this scenario, using EBWs yields accuracy improvements compared to the conventional IPW approach. Further details can be found in the appendix I.

**Using PGD for Constrained Optimization.** The middle subplots of Figure 1 show accuracy comparisons between standard penalized approach and constrained optimization using PGD. Both optimization algorithms are based on the same proposed statistical approach, integrating EBWs with SABD-Learning, outcome augmentation, and additional variable screening. For solving the \(L_{1}\)-penalized regression problems, the R package _glmnet_ is a standard choice, which is based on cyclic coordinate descent with simulated-annealing-style hyperparameter tuning. However, _glmnet_ does not handle hard \(L_{1}\)-ball constraints required for our method. Therefore, to ensure a fair comparison, we directly implemented standard (projected) (sub)gradient descent to solve optimization problems for both penalized and constrained problems. In our experiment reported in Figure 1, we observe significant improvements using our PGD method, especially in the second scenario. One possible explanation for this improvement is that large penalization may yield a perturbed solution to the ITR problem. If the minimizer of the ITR problem lies on the boundary of the \(L_{1}\)-ball, the solutions of constrained optimization and penalized optimization coincide. However, if the minimizer is the interior point of \(L_{1}\)-ball, constrained optimization can find the exact solution, while the penalization may induce a perturbed solution. Furthermore, the trajectory of PGD for the constraint approach appears to be significantly more stable than that of the subgradient descent in the penalization approach (see Figure 2).

**Performance Evaluations between the Proposed Method and Benchmark Methods.** In the right subplots of Figure 1, we evaluate accuracies by comparing benchmark methods to our proposed approach. The benchmarks include AD-Learning, SABD-Learning, a treatment-covariate interaction model with \(L_{1}\)-regularization 'linear', and two popular tree-based methods:'policytree' [47, 59] and double-machine learning-based 'causalDML' [10, 29]. Our proposed method integrates EBWs with SABD-Learning, outcome augmentation, and additional variable screening for variance and dimension reduction, implemented via PGD. However, causalDML requires a sufficiently large sample size, thereby making its evaluation unfeasible under a sample size of 200. Our numerical analysis provides evidence that our proposed approach outperforms existing methods in both scenarios. 'linear' performs well in the upper subplot, which simulates a randomized trial with a true optimal linear rule, as covariates are balanced and the model accurately captures the decision function. However, in the lower subplot, simulating an observational study with a nonlinear rule, the model performs poorly because it lacks balancing weights to address confounding and cannot fully capture nonlinear relationships. Further detailed analysis, including the effectiveness of additional techniques such as variable screening for performance improvement, is provided in the appendix I.

Additionally, our proposed method processes each dataset (p = 60, n = 1000) in an average of 4.40 seconds, compared to 2.01 seconds for the penalization method using _glmnet_ in R. Other methods exhibit scalability challenges, with policytree averaging 12.02 seconds and causalDML taking 46.75 seconds. This demonstrates that computational efficiency is a notable advantage of our approach.

Figure 2: **Comparison of Optimization Methods by \(\boldsymbol{\lambda_{1}}\). \(\lambda_{1}\) is \(L_{1}\)-ball size of the constraint set in the constrained optimization and the regularization parameter with additional \(L_{1}\)-regularization of the model parameter in the penalized regression, respectively.**

## 5 Applications

We apply the proposed methods to two datasets from AIDS Clinical Trials Group (ACTG) 175 [21] and email marketing [22]. ACTG is a randomized trial for 2,139 patients with HIV infection who were randomly assigned to four different treatments. Twelve covariates including age and gender were used for the analysis. The outcome is the change in CD4 cell count from baseline to 20 weeks, where larger values are preferable. Email marketing dataset has 64,000 customers who were randomly chosen to receive an e-mail campaign among three different marketing methods. Eight covariates with historical customer attributes and their pairwise interactions were used. The outcome is whether customers visited the website in the following two weeks. The goal of the analysis using these two datasets is to find the best treatment/marketing methods based on individuals' attributes. Similar to [44], we randomly split the data into a training set of \(\{200,400,800,1000,1200\}\) observations for the ACTG dataset and \(\{1000,3000,5000\}\) observations for the email dataset. The remaining observations were used for test data with 10 iterations. We evaluate method performance using the empirical value function on a test dataset. The benchmarking methods include AD-Learning, SABD-Learning, and two tree-based methods (policytree [47] and causalDML [29]). However, causalDML requires a sufficiently large sample size, thereby making its evaluation unfeasible under a sample size of 200. For the binary outcome in Email dataset, SABD-Learning is not used, since the homoscedastic assumption is unnecessary for logistic regression. The results are shown in Table 1.

## 6 Discussion and Limitations

Our method introduces a unified, robust framework for estimating ITR. By formulating the problem as a weighted, constrained optimization problem, we incorporate DCBWs to control confounding and propose the PGD algorithm for sparse ITRs with computational and statistical guarantees. Additionally, we propose variable screening and efficient augmentation, which together show synergistic effects. We demonstrate our method with continuous and binary outcomes, and it can be readily extended to other outcomes, such as censored outcomes. Although we focus on learning linear ITR to facilitate the demonstration of our idea, it can be extended to nonlinear ITR using the basis function [34]. For example, by taking the covariate powers up to M, the linear function class becomes the class of degree-M polynomial functions. The linear function class can also serve as a useful approximation to nonlinear decision functions.

We acknowledge that although most, if not all, types of ITR-Learning frameworks can benefit from using DCBWs, variable screening, and augmentation, our proposed framework for theoretical analysis relies on the specific assumption that the model can be formulated as a constrained, weighted, and convex optimization problem. Furthermore, we restricted the decision function to the linear function class. Generalizing our theoretical analysis to include nonlinear function classes with nonconvex problems, such as boosting or deep neural networks, is an interesting direction for future investigation. Another limitation of our method is its reliance on standard assumptions to identify optimal ITRs using observational data, including the assumption of no unmeasured confounding. An interesting future direction would be to extend our framework to cases where this assumption does not hold, utilizing instrumental variables [12; 38] and proximal causal learning [40; 45].

Our work significantly impacts society by improving patient health outcomes through personalized care and advancing medical research in disease mechanisms and drug development. Beyond healthcare, these methods can be applied in marketing strategies based on customer characteristics.

\begin{table}
\begin{tabular}{|c||c|c c c c c|} \hline Dataset & Training size & AD & SABD & policytree & causalDML & **Proposed** \\ \hline \multirow{4}{*}{ACTG} & 200 & 37.624 (6.9) & 33.645 (6.7) & 36.871 (4.6) & - & **42.301** (5.7) \\  & 400 & 51.329 (3.2) & 53.644 (1.9) & 47.191 (2) & 49.552 (0.2) & **54.868** (3.4) \\  & 800 & 56.021 (1.5) & 56.700 (1.6) & 52.786 (2.6) & 55.506 (2.1) & **59.200** (2.5) \\  & 1000 & 56.825 (2.2) & 56.963 (2.1) & 56.005 (1.7) & 56.591 (2.1) & **59.552** (2.5) \\  & 1200 & 55.092 (2.6) & 55.221 (2.8) & **58.819** (3.4) & 57.104 (2.8) & 58.207 (2.9) \\ \hline \multirow{3}{*}{Email} & 1000 & 0.169 (0.0) & - & 0.171 (0.0) & 0.172 (0.0) & **0.175** (0.0) \\  & 3000 & 0.176 (0.0) & - & 0.177 (0.0) & 0.177 (0.0) & **0.181** (0.0) \\ \cline{1-1}  & 5000 & 0.177 (0.0) & - & 0.178 (0.0) & 0.178 (0.0) & **0.182** (0.0) \\ \hline \end{tabular}
\end{table}
Table 1: Average empirical value functions across different approaches for ACTG/Email datasets. Mean values with the corresponding standard errors of the mean (SEM) in parentheses are provided. The highest-performing methods are marked in bold.

## Acknowledgement

The authors would like to thank Dr. Michael Kosorok and Dr. Haoda Fu for their valuable comments and encouragement on the early version of this work. Lee J. and Chen G.'s efforts were partially supported by NSF Grant DMS-2054346.

## References

* [1] Serge Assaad, Shuxi Zeng, Chenyang Tao, Shounak Datta, Nikhil Mehta, Ricardo Henao, Fan Li, and Lawrence Carin. Counterfactual representation learning with balancing weights. In _International Conference on Artificial Intelligence and Statistics_, pages 1972-1980. PMLR, 2021.
* [2] Susan Athey and Stefan Wager. Policy learning with observational data. _Econometrica_, 89(1):133-161, 2021.
* [3] Laura B Balzer, Maya L Petersen, Mark J van der Laan, and Search Collaboration. Targeted estimation and inference for the sample average treatment effect in trials with and without pair-matching. _Statistics in medicine_, 35(21):3717-3732, 2016.
* [4] Leon Bottou. Large-scale machine learning with stochastic gradient descent. In _Proceedings of COMPSTAT'2010_, pages 177-186. Springer, 2010.
* [5] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* [6] Stephen Boyd, Lin Xiao, and Almir Mutapcic. Subgradient methods. _lecture notes of EE392o, Stanford University, Autumn Quarter_, 2004(01), 2003.
* [7] Guanhua Chen, Donglin Zeng, and Michael R Kosorok. Personalized dose finding using outcome weighted learning. _J. Amer. Statist. Assoc._, 111(516):1509-1521, 2016.
* [8] Rui Chen, Jared D Huling, Guanhua Chen, and Menggang Yu. Robust sample weighting to facilitate individualized treatment rule learning for a target population. _Biometrika_, 111(1):309-329, 2024.
* [9] Shuai Chen, Lu Tian, Tianxi Cai, and Menggang Yu. A general statistical framework for subgroup identification and comparative treatment scoring. _Biometrics_, 73(4):1199-1209, 2017.
* [10] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, and Whitney Newey. Double/debiased/neyman machine learning of treatment effects. _Am. Econ. Rev._, 107(5):261-65, 2017.
* [11] Francis S Collins and Harold Varmus. A new initiative on precision medicine. _New England journal of medicine_, 372(9):793-795, 2015.
* [12] Yifan Cui and Eric Tchetgen Tchetgen. A semiparametric instrumental variable approach to optimal treatment regimes under endogeneity. _Journal of the American Statistical Association_, 116(533):162-173, 2021.
* [13] Alicia Curth and Mihaela van der Schaar. Nonparametric estimation of heterogeneous treatment effects: From theory to learning algorithms. In _International Conference on Artificial Intelligence and Statistics_, pages 1810-1818. PMLR, 2021.
* [14] John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efficient projections onto the l 1-ball for learning in high dimensions. In _Proceedings of the 25th international conference on Machine learning_, pages 272-279, 2008.
* [15] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle properties. _Journal of the American statistical Association_, 96(456):1348-1360, 2001.

* [16] Jianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature space. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 70(5):849-911, 2008.
* [17] William Feller. _An introduction to probability theory and its applications, Volume 2_, volume 81. John Wiley & Sons, 1991.
* [18] Jean-Louis Goffin. On convergence rates of subgradient optimization methods. _Mathematical programming_, 13:329-347, 1977.
* [19] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. _The Journal of Machine Learning Research_, 13(1):723-773, 2012.
* [20] Jens Hainmueller. Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. _Political analysis_, 20(1):25-46, 2012.
* [21] Scott M Hammer, David A Katzenstein, Michael D Hughes, Holly Gundacker, Robert T Schooley, Richard H Haubrich, W Keith Henry, Michael M Lederman, John Phair, Manette Niu, et al. A trial comparing nucleoside monotherapy with combination therapy in hiv-infected adults with cd4 cell counts from 200 to 500 per cubic millimeter. _New England Journal of Medicine_, 335(15):1081-1090, 1996.
* [22] Kevin Hillstrom. Minethatdata e-mail analytics and data mining challenge. [https://www.uplift-modeling.com/en/v0.3.1/api/datasets/fetch_hillstrom.html](https://www.uplift-modeling.com/en/v0.3.1/api/datasets/fetch_hillstrom.html), 2008. Accessed: 2024-04-28.
* [23] Jared D Huling, Noah Greifer, and Guanhua Chen. Independence weights for causal inference with continuous treatments. _Journal of the American Statistical Association_, 119(546):1657-1670, 2024.
* [24] Jared D Huling and Simon Mak. Energy balancing of covariate distributions. _Journal of Causal Inference_, 12(1):20220029, 2024.
* [25] Kosuke Imai and Marc Ratkovic. Covariate balancing propensity score. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 76(1):243-263, 2014.
* [26] Nathan Kallus. Balanced policy evaluation and learning. _Advances in neural information processing systems_, 31, 2018.
* [27] Nathan Kallus. Generalized optimal matching methods for causal inference. _The Journal of Machine Learning Research_, 21(1):2300-2353, 2020.
* [28] Nathan Kallus. More efficient policy learning via optimal retargeting. _Journal of the American Statistical Association_, 116(534):646-658, 2021.
* [29] Michael C Knaus. Double machine learning-based programme evaluation under unconfoundedness. _The Econometrics Journal_, 25(3):602-627, 2022.
* [30] Brian K Lee, Justin Lessler, and Elizabeth A Stuart. Weight trimming and propensity score weighting. _PloS one_, 6(3):e18174, 2011.
* [31] Fan Li, Kari Lock Morgan, and Alan M Zaslavsky. Balancing covariates via propensity score weighting. _Journal of the American Statistical Association_, 113(521):390-400, 2018.
* [32] Runze Li, Wei Zhong, and Liping Zhu. Feature screening via distance correlation learning. _Journal of the American Statistical Association_, 107(499):1129-1139, 2012.
* [33] Haixu Ma, Donglin Zeng, and Yufeng Liu. Learning individualized treatment rules with many treatments: A supervised clustering approach using adaptive fusion. _Advances in Neural Information Processing Systems_, 35:15956-15969, 2022.
* [34] Jacob M Maronge, Jared D Huling, and Guanhua Chen. A reluctant additive model framework for interpretable nonlinear individualized treatment rules. _The Annals of Applied Statistics_, 17(4):3384-3402, 2023.

* Mo and Liu [2022] Weibin Mo and Yufeng Liu. Efficient learning of optimal individualized treatment rules for heteroscedastic or misspecified treatment-free effect models. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 84(2):440-472, 2022.
* Nie and Wager [2021] Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. _Biometrika_, 108(2):299-319, 2021.
* Pan et al. [2019] Wenliang Pan, Xueqin Wang, Heping Zhang, Hongtu Zhu, and Jin Zhu. Ball covariance: A generic measure of dependence in banach space. _Journal of the American Statistical Association_, 115(529):307-317, 2019.
* Pu and Zhang [2021] Hongming Pu and Bo Zhang. Estimating optimal treatment rules with an instrumental variable: A partial identification learning approach. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 83(2):318-345, 2021.
* Qi et al. [2020] Zhengling Qi, Dacheng Liu, Haoda Fu, and Yufeng Liu. Multi-armed angle-based direct learning for estimating optimal individualized treatment rules with various outcomes. _Journal of the American Statistical Association_, 115(530):678-691, 2020.
* Qi et al. [2023] Zhengling Qi, Rui Miao, and Xiaoke Zhang. Proximal learning for individualized treatment regimes under unmeasured confounding. _Journal of the American Statistical Association_, pages 1-14, 2023.
* Qian and Murphy [2011] Min Qian and Susan A Murphy. Performance guarantees for individualized treatment rules. _Annals of statistics_, 39(2):1180, 2011.
* Ross [2014] Brian C Ross. Mutual information between discrete and continuous data sets. _PloS one_, 9(2):e87357, 2014.
* Rubin [1980] Donald B Rubin. Randomization analysis of experimental data: The fisher randomization test comment. _Journal of the American statistical association_, 75(371):591-593, 1980.
* Shah et al. [2022] Kushal S Shah, Haoda Fu, and Michael R Kosorok. Stabilized direct learning for efficient estimation of individualized treatment rules. _Biometrics_, 2022. doi: 10.1111/biom.13818.
* Shen and Cui [2024] Tao Shen and Yifan Cui. Optimal treatment regimes for proximal causal learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* Shi et al. [2019] Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the estimation of treatment effects. _Advances in neural information processing systems_, 32, 2019.
* Sverdrup et al. [2020] Erik Sverdrup, Ayush Kanodia, Zhengyuan Zhou, Susan Athey, and Stefan Wager. policytree: Policy learning via doubly robust empirical welfare maximization over trees. _Journal of Open Source Software_, 5(50):2232, 2020.
* Szekely et al. [2007] Gabor J Szekely, Maria L Rizzo, and Nail K Bakirov. Measuring and testing dependence by correlation of distances. _The Annals of Statistics_, 35(6):2769-2794, 2007.
* Tang et al. [2023] Dingke Tang, Dehan Kong, Wenliang Pan, and Linbo Wang. Ultra-high dimensional variable selection for doubly robust causal inference. _Biometrics_, 79(2):903-914, 2023.
* Tian et al. [2014] Lu Tian, Ash A Alizadeh, Andrew J Gentles, and Robert Tibshirani. A simple method for estimating interactions between a treatment and a large number of covariates. _Journal of the American Statistical Association_, 109(508):1517-1532, 2014.
* Vershynin [2018] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Villani et al. [2009] Cedric Villani et al. _Optimal transport: old and new_, volume 338. Springer, 2009.
* Wager and Athey [2018] Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using random forests. _Journal of the American Statistical Association_, 113(523):1228-1242, 2018.

* Zhang et al. [2012] Baqun Zhang, Anastasios A Tsiatis, Eric B Laber, and Marie Davidian. A robust method for estimating optimal treatment regimes. _Biometrics_, 68(4):1010-1018, 2012.
* Zhang and Liu [2014] Chong Zhang and Yufeng Liu. Multicategory angle-based large-margin classification. _Biometrika_, 101(3):625-640, 2014.
* Zhao et al. [2022] Qingyuan Zhao, Dylan S Small, and Ashkan Ertefaie. Selective inference for effect modification via the lasso. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 84(2):382-413, 2022.
* Zhao et al. [2012] Yingqi Zhao, Donglin Zeng, A John Rush, and Michael R Kosorok. Estimating individualized treatment rules using outcome weighted learning. _Journal of the American Statistical Association_, 107(499):1106-1118, 2012.
* Zhou et al. [2017] Xin Zhou, Nicole Mayer-Hamblett, Umer Khan, and Michael R Kosorok. Residual weighted learning for estimating individualized treatment rules. _Journal of the American Statistical Association_, 112(517):169-187, 2017.
* Zhou et al. [2023] Zhengyuan Zhou, Susan Athey, and Stefan Wager. Offline multi-action policy learning: Generalization and optimization. _Operations Research_, 71(1):148-183, 2023.
* Zhu et al. [2017] Ruoqing Zhu, Ying-Qi Zhao, Guanhua Chen, Shuangge Ma, and Hongyu Zhao. Greedy outcome weighted tree learning of optimal personalized treatment rules. _Biometrics_, 73(2):391-400, 2017.
* Zubizarreta [2015] Jose R Zubizarreta. Stable weights that balance covariates for estimation with incomplete outcome data. _Journal of the American Statistical Association_, 110(511):910-922, 2015.

Preliminaries

In this section, we review several notations and basic facts on linear algebra and matrix calculus.

The Kronecker product, denoted by \(\otimes\), is a binary operation that combines two matrices to create a new matrix. Given two matrices \(\mathbf{A}=(a_{ij})\in\mathbb{R}^{m\times n}\) and \(\mathbf{B}\in\mathbb{R}^{p\times q}\), then \(\mathbf{A}\otimes\mathbf{B}\) is an \(mp\times nq\) matrix defined as

\[\mathbf{A}\otimes\mathbf{B}=\begin{bmatrix}a_{11}\mathbf{B}&\cdots&a_{1n} \mathbf{B}\\ \vdots&\ddots&\vdots\\ a_{m1}\mathbf{B}&\cdots&a_{mn}\mathbf{B}\end{bmatrix} \tag{15}\]

If \(\mathbf{A}^{\prime}\in\mathbb{R}^{m\times n^{\prime}}\), then for the horizontally stacked matrix \([\mathbf{A},\mathbf{A}^{\prime}]\in\mathbb{R}^{m\times(n+n^{\prime})}\),

\[[\mathbf{A},\mathbf{A}^{\prime}]\otimes\mathbf{B}=[\mathbf{A} \otimes\mathbf{B},\mathbf{A}^{\prime}\otimes\mathbf{B}]. \tag{16}\]

For each \(\mathbf{A}=[\mathbf{a}_{1},\ldots,\mathbf{a}_{n}]\in\mathbb{R}^{m\times n}\), let \(\operatorname{vec}(\mathbf{A}):=[\mathbf{a}_{1}^{T},\ldots,\mathbf{a}_{n}^{T} ]^{T}\in\mathbb{R}^{mn}\) denote its vectorization. The _commutation matrix_\(\mathbf{C}^{(a,b)}\), which is a special instance of \(ab\times ab\) permutation matrix. Namely, for each integers \(a,b\geq 1\), there exists a unique matrix \(\mathbf{C}^{(a,b)}\in\{0,1\}^{ab\times ab}\) such that for all \(\mathbf{A}\in\mathbb{R}^{a\times b}\), we have \(\mathbf{C}^{(a,b)}\operatorname{vec}(\mathbf{A})=\operatorname{vec}(\mathbf{A }^{T})\). Note that \((\mathbf{C}^{(a,b)})^{T}=\mathbf{C}^{(b,a)}\). Furthermore, \((\mathbf{C}^{(a,b)})^{T}\mathbf{C}^{(a,b)}=\mathbf{I}_{ab}\) since \((\mathbf{C}^{(a,b)})^{T}\mathbf{C}^{(a,b)}\operatorname{vec}(\mathbf{A})= \operatorname{vec}(\mathbf{A}^{T})=\operatorname{vec}(\mathbf{A})\). Hence \(\mathbf{C}^{(a,b)}\) is positive semi-definite. Note that \(\mathbf{C}^{(a,1)}=\mathbf{I}_{a}=\mathbf{C}^{(1,a)}\) since if \(\mathbf{A}\in\mathbb{R}^{a\times 1}\) or \(\mathbf{A}\in\mathbb{R}^{1\times a}\), \(\operatorname{vec}(\mathbf{A})=\operatorname{vec}(\mathbf{A}^{T})\). Also, for every matrices \(\mathbf{A}\in\mathbb{R}^{m\times n}\) and \(\mathbf{B}\in\mathbb{R}^{r\times q}\),

\[\mathbf{C}^{(r,m)}(\mathbf{A}\otimes\mathbf{B})=(\mathbf{B}\otimes\mathbf{A}) \mathbf{C}^{(q,n)}. \tag{17}\]

Also recall the relations for vectorizing product of matrices: for \(\mathbf{A}\in\mathbb{R}^{a\times b}\), \(\mathbf{B}\in\mathbb{R}^{b\times c}\), and \(\mathbf{C}\in\mathbb{R}^{c\times d}\),

\[\operatorname{vec}(\mathbf{A}\mathbf{B}) =(\mathbf{I}_{\text{c}}\otimes\mathbf{A})\operatorname{vec}( \mathbf{B})=(\mathbf{B}^{T}\otimes\mathbf{I}_{a})\operatorname{vec}(\mathbf{A }), \tag{18}\] \[\operatorname{vec}(\mathbf{A}\mathbf{B}\mathbf{C}) =(\mathbf{C}^{T}\otimes\mathbf{A})\operatorname{vec}(\mathbf{B })=(\mathbf{I}_{d}\otimes\mathbf{A}\mathbf{B})\operatorname{vec}(\mathbf{C}) =(\mathbf{C}^{T}\mathbf{B}^{T}\otimes\mathbf{I}_{a})\operatorname{vec}( \mathbf{A}). \tag{19}\]

For \(\mathbf{A},\mathbf{B},\mathbf{C}\), and \(\mathbf{D}\) are matrices of compatible sizes allowing the matrix products \(\mathbf{A}\mathbf{C}\) and \(\mathbf{B}\mathbf{D}\),

\[(\mathbf{A}\otimes\mathbf{B})(\mathbf{C}\otimes\mathbf{D})=(\mathbf{A}\mathbf{ C}\otimes\mathbf{B}\mathbf{D}).\]

Next, recall that if \(f:\mathbb{R}^{a\times 1}\rightarrow\mathbb{R}^{b\times 1}\) and \(g:\mathbb{R}^{b\times 1}\rightarrow\mathbb{R}^{c\times 1}\) are differentiable functions, then the Jacobian \(J_{f}\) can be written by using gradients as \(J_{f}(x)=\left(\nabla_{x}f^{T}\right)^{T}\). By chain rule \(J_{g\circ f}(x)=J_{g}(f(x))J_{f}(x)\), we have the following chain rule for gradients

\[\nabla_{x}\left(g(f(x))^{T}\right)=\nabla_{x}\left(f(x)^{T}\right) \nabla_{f(x)}\left(g(f(x))^{T}\right). \tag{20}\]

We will also frequently use the following fact: For \(\mathbf{A}\in\mathbb{R}^{a\times b}\),

\[\nabla_{\operatorname{vec}(\mathbf{A})}\operatorname{vec}(\mathbf{A })^{T} =\mathbf{I}_{ab}, \tag{21}\] \[\nabla_{\operatorname{vec}(\mathbf{A})}\operatorname{vec}(\mathbf{ A}^{T})^{T} =\nabla_{\operatorname{vec}(\mathbf{A})}\operatorname{vec}(\mathbf{A})^{T}\mathbf{C}^{( b,a)}=\mathbf{C}^{(b,a)}. \tag{22}\]

For two matrices \(\mathbf{A},\mathbf{B}\) of the same size, we write \(\mathbf{A}\preceq\mathbf{B}\) if \(\mathbf{B}-\mathbf{A}\) is positive semi-definite. The partial ordering \(\preceq\) is called the _Loewner ordering_.

Details of model setup

### Details of angle-based approach

Consider the working model given by Equation (1) in the main text:

\[Y=\mu(\mathbf{X})+\sum_{k=1}^{K}\delta_{k}(\mathbf{X})\mathbf{I}(A=k)+\epsilon.\]

To make the optimal ITR identifiable, we impose the constraint \(\sum_{k=1}^{K}\delta_{k}(\mathbf{x})=0\) for all covariates \(\mathbf{x}\). Under this model, \(\delta_{k}(\mathbf{x})\) determines the optimal ITR for an individual with covariate \(\mathbf{x}\), while \(\mu(\mathbf{x})\) has no impact on the ITR. However, using \(K\) functions with the sum-to-zero constraint can be computationally inefficient [55]. Instead, one can opt for simplex coding as an alternative, which inherently satisfies the sum-to-zero constraint.

In AD-Learning, proposed by [39], each of the \(K\) treatments is represented as a vertex simplex on \(\mathbb{R}^{K-1}\), denoted as \(\mathbf{u}_{k}\), \(k=1,\ldots,K\). In particular, \(\mathbf{u}_{k}\) is defined as

\[\mathbf{u}_{k}=\begin{cases}\frac{1}{\sqrt{K-1}}\mathbf{1}_{K-1}&\text{ if }k=1,\\ -\frac{1+\sqrt{K}}{\sqrt{(K-1)^{3}}}\mathbf{1}_{K-1}+\sqrt{\frac{K}{K-1}} \mathbf{e}_{k-1}&\text{ if }k\geq 2,\end{cases}\]

where \(\mathbf{1}_{K-1}\) is a \(K-1\) dimensional vector with entries 1, and \(\mathbf{e}_{k-1}\) is a \(K-1\) dimensional vector with entries 0 except its \(k\)-th entry being 1. This vertex simplex has \(K\) vertices with equal angles between them and an origin at the center of the simplex. All \(\mathbf{u}_{k}\) have the same Euclidean norm \(1\). To estimate the optimal ITR, a decision function \(\mathbf{f}(\mathbf{x})=(f_{1}(\mathbf{x}),\ldots,f_{K-1}(\mathbf{x}))^{T}\) is constructed, assigning a \(K-1\) dimensional vector to each covariate \(\mathbf{x}\). Here, \(\mathbf{f}(\cdot)\) can take a generic form, while we assume the linear decision function in the main text. The optimal ITR is reformulated by the authors as follows:

\[d^{\text{opt}}(\mathbf{x})=\operatorname*{arg\,max}_{k\in\{1,\ldots,K\}} \mathbf{u}_{k}^{T}\mathbf{f}(\mathbf{x})=\operatorname*{arg\,min}_{k\in\{1, \ldots,K\}}\angle(\mathbf{u}_{k},\mathbf{f}(\mathbf{x})),\]

where \(\angle(\cdot,\cdot)\) represents the angle between two vectors. Note that due to \(\sum_{k=1}^{K}\mathbf{u}_{k}=\mathbf{0}\), \(\sum_{k=1}^{K}\mathbf{u}_{k}^{T}\mathbf{f}(\mathbf{x})=0\) for any given function \(\mathbf{f}\) and covariate value \(\mathbf{x}\). Then we can use

\[\left(\sum_{k=1}^{K}\mathbf{u}_{k}\mathbf{I}(A=k)\right)^{T}\mathbf{f}( \mathbf{x})\]

instead of \(\sum_{k=1}^{K}\delta_{k}(\mathbf{X})\mathbf{I}(A=k)\) to encode the treatment and covariate interaction. Specifically, we use \(\mathbf{u}_{A}\) to denote \(\left(\sum_{k=1}^{K}\mathbf{u}_{k}\mathbf{I}(A=k)\right)^{T}\) and assume \(\mathbf{f}(\mathbf{x})\) to be linear in \(\mathbf{x}\) (with the coefficient matrix \(\mathbf{B}\)). Therefore, terms pertaining to ITRs can be expressed as \(\mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i}\).

### Details of binary outcome

Again, under prespecified treatment rule class \(\mathcal{D}\), the optimal ITR is given by

\[d^{\text{opt}}(\mathbf{x})=\operatorname*{arg\,max}_{k\in\{1,\ldots,K\}} \mathbb{E}[Y|\mathbf{x},A=k]=\operatorname*{arg\,max}_{k\in\{1,\ldots,K\}} \mathbf{u}_{k}^{T}\underbrace{\mathbb{E}\left[\frac{Y\mathbf{u}}{\pi(A, \mathbf{x})}\left|\ \mathbf{x}\right.\right]}_{=:\mathbf{f}_{\text{opt}}(\mathbf{x})}.\]

For the binary response setting, we assume the following logistic model

\[\mathbb{P}[Y=1|\mathbf{x},A=k]=\frac{\exp(\mathbf{u}_{k}^{T}\mathbf{f}_{\text {opt}}(\mathbf{x}))}{1+\exp(\mathbf{u}_{k}^{T}\mathbf{f}_{\text{opt}}(\mathbf{ x}))}.\]Under this model, \(\mathbf{f}_{\text{opt}}\) is indeed the optimal decision function under the angle-based direct learning framework since

\[d^{\text{opt}}(\mathbf{x}) =\operatorname*{arg\,max}_{k\in\{1,\dots,K\}}\mathbb{P}[Y=1| \mathbf{x},A=k] \tag{23}\] \[=\operatorname*{arg\,max}_{k\in\{1,\dots,K\}}\frac{\exp(\mathbf{ u}_{k}^{T}\mathbf{f}_{\text{opt}}(\mathbf{x}))}{1+\exp(\mathbf{u}_{k}^{T} \mathbf{f}_{\text{opt}}(\mathbf{x}))}\] \[=\operatorname*{arg\,max}_{k\in\{1,\dots,K\}}\mathbf{u}_{k}^{T} \mathbf{f}_{\text{opt}}(\mathbf{x}).\]

Furthermore, it can be checked that the function \(\mathbf{f}_{\text{opt}}\) is _an_ optimal solution to maximizing the IPW-weighted log-likelihood of the logistic regression model:

\[\operatorname*{arg\,min}_{\mathbf{f}}\mathbb{E}\left[-\frac{Y\mathbf{u}^{T} \mathbf{f}}{\pi(A,\mathbf{x})}+\frac{\log(\exp(\mathbf{u}^{T}\mathbf{f})+1)}{ \pi(A,\mathbf{x})}\right], \tag{24}\]

under the assumption that the expectation and derivative w.r.t. \(\mathbf{f}\) can be exchanged ([39]).

For the effective and robust ITR estimation, replacing IPW with DCBW leads to

\[\operatorname*{arg\,min}_{\mathbf{f}}\mathbb{E}\left[w(\mathbf{x},A)\left\{-Y \mathbf{u}^{T}\mathbf{f}+\log(\exp(\mathbf{u}^{T}\mathbf{f})+1)\right\}\right]. \tag{25}\]

Now consider restricting it to the linear decision function class \(\mathcal{F}=\{\mathbf{f}(\mathbf{X})=\mathbf{B}^{T}\mathbf{X}:\mathbf{B}\in \mathbb{R}^{p\times(K-1)}\}\). Then the above (25) becomes

\[\operatorname*{arg\,max}_{\mathbf{B}\in\mathbb{R}^{p\times(K-1)}}\mathbb{E} \left[w(\mathbf{x},A)\{Y\mathbf{u}^{T}\mathbf{B}^{T}\mathbf{x}-\log(\exp( \mathbf{u}^{T}\mathbf{B}^{T}\mathbf{x})+1)\}\right].\]

Then, we can estimate the optimal parameter \(\mathbf{B}\) by solving the following empirical loss minimization

\[\operatorname*{arg\,min}_{\mathbf{B}\in\mathbb{R}^{p\times(K-1)}}\frac{1}{n} \sum_{i=1}^{n}w(\mathbf{x}_{i},a_{i})\left(-y_{i}\mathbf{u}_{i}^{T}\mathbf{B} ^{T}\mathbf{x}_{i}+\log(\exp(\mathbf{u}^{T}\mathbf{B}^{T}\mathbf{x}_{i})+1) \right)\quad\text{ s.t }\quad\|\mathbf{B}\|_{1}\leq z, \tag{26}\]

where we imposed additionally the \(L_{1}\)-penalization \(\|\mathbf{B}\|_{1}\leq z,\) on \(\mathbf{B}\) to promote a sparse solution. By the proposed algorithm 1, we can efficiently solve the problem (26).

Details of proposed approaches

To keep the appendix as self-contained as possible, we provide descriptions of the building blocks of the proposed methods.

### Use of distributional covariate balancing weights

In this section, we provide the background about the use of DCBWs for constructing effective and robust ITRs. For additional details, please refer to the original papers.

Many existing ITR-Learning approaches such as AD-Learning and SABD-Learning use _inverse probability (of treatment) weighting_ (IPW), which requires specifying a propensity score model for confounding control. However, propensity score methods have long been known to be highly sensitive to model misspecification, which yields biased estimates of causal effects [25, 30]. For this reason, methods that directly estimate weights based on balance-seeking objectives instead of using IPW have been proposed. These approaches include entropy balancing weights [20], stable balancing weights [61] as well as DCBWs [24, 27]. However, despite the weights serving a critical role in the estimation process, these advancements have mainly focused on improving _average treatment effect_ (ATE) and are largely under-explored in the ITR literature. Significant improvements in the performance of ITRs are often left on the table when the weights are not treated with the same level of emphasis and focus as the other aspects of ITR learning.

To tackle these challenges, we propose the use of DCBWs, such as _energy balancing weights_ (EBWs) [24] and _maximum mean discrepancy_ (MMD) balancing weights [19, 27, 8], as an alternative to IPW for ITR-Learning. Unlike traditional approaches, these methods do not rely on any pre-specified functional forms for constructing weights. Instead, they focus on minimizing the distance of the weighted empirical distributions of covariates across treatment groups to reduce potential confounding. Even for data from randomized trials, they can help reduce finite sample covariate imbalance. The emphasis on finite sample balance and the model-independent nature of distributional weighting methods effectively mitigate biases due to model misspecification, resulting in more precise and reliable ITR estimation.

In the remainder of this section, we provide additional details about EBWs. The EBWs are derived from the concept of "weighted energy distance" which modifies the energy distance measure to allow for measurement of the distance between a weighted empirical distribution and a target empirical distribution. This measure captures the distance between a weighted _empirical cumulative distribution function_ (ECDF) of covariates for a specific treatment group \(a\), denoted as \(\{\mathbf{X}_{i}\}_{i:A_{i}=a}\), and the combined ECDF of covariates, denoted as \(\{\mathbf{X}_{i}\}_{i=1}^{n}\). Specifically, the weighted energy distance is defined as:

\[\mathcal{E}\left(F_{n,a,\mathbf{w}},F_{n}\right)\equiv \frac{2}{n_{a}}\sum_{i=1}^{n}\sum_{j=1}^{n}w_{i}\mathbf{I}\left(A _{i}=a\right)\left\|\mathbf{X}_{i}-\mathbf{X}_{j}\right\|_{2}-\frac{1}{n_{a}^ {2}}\sum_{i=1}^{n}\sum_{j=1}^{n}w_{i}w_{j}\mathbf{I}\left(A_{i}=A_{j}=a\right) \left\|\mathbf{X}_{i}-\mathbf{X}_{j}\right\|_{2}\] \[-\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}\left\|\mathbf{X}_{i }-\mathbf{X}_{j}\right\|_{2}.\]

where \(n_{a}=\sum_{i=1}^{n}\mathbf{I}(A_{i}=a)\). The weighted energy distance measures the distance between the weighted distribution of covariates among those with \(A_{i}=a\) and the empirical distribution of covariates in the full sample; it takes value 0 if and only if these two distributions are identical.

The optimal weights are obtained by minimizing the sum of all the weighted energy distances of all treatment groups:

\[\mathbf{w}_{n}\in\operatorname*{argmin}_{\mathbf{w}=(w_{1},\ldots,w_{n})}\sum _{a\in\mathcal{A}}\mathcal{E}\left(F_{n,a,\mathbf{w}},F_{n}\right)\text{ s.t. }\sum_{i=1}^{n}w_{i}I\left(A_{i}=a\right)=n_{a} \tag{27}\]

Thus, the EBWs encourage the distributions of covariates for each treatment group to look like that of the target population, i.e. the full sample. Since (27) is a quadratic objective function along with linear constraints, it can be efficiently solved through interior point methods. Another advantage lies in the fact that it does not require parameter tuning. The estimated weights are effective in balancing all covariates of the covariate distributions, not just limited to the first or second moments. Lastly, the data-driven characteristic of EBWs that do not rely on specific model assumptions contributes to more robust ITR estimates.

### Implementation of the PGD algorithm

We provide Algorithm 1 to solve the problem (6) using a _projected gradient descent_ (PGD). The algorithm applies to either continuous or binary outcomes. For the projection onto the \(L_{1}\)-ball, we use the algorithm in [14].

```
Input: Data set \((\mathbf{x}_{i},a_{i},y_{i})_{i=1}^{n}\in\mathbb{R}^{n\times p}\times\{1,2, \cdots,K\}^{n}\times\mathbb{R}^{n}\) (\(\triangleright\)\(y_{i}\) can be continuous or binary); loss function \(L\) Parameters: \(\mathbf{B}\in\mathbb{R}^{p\times(K-1)}\) (model coefficient), \(z\geq 0\) (\(L_{1}\)-ball size); \(\lambda_{1}\geq 0\) (additional \(L_{2}\)-penalization for \(R(\mathbf{B})\)); \(T\in\mathbb{N}\) (number of iterations), \(\alpha_{t}\geq 0\) (stepsize). Constraints:\(L_{1}\)-ball \(\mathcal{B}:=\{\mathbf{B}\in\mathbb{R}^{p\times(K-1)}\,|\,\|\mathbf{B}\|_{1}\leq z\}\)  Compute \(\mathbf{x}_{i}^{*}:=\mathbf{x}_{i}\mathbf{u}_{a_{i}}^{T}\) for all \(i=1,\cdots,n\) for faster computation.  Choose any point from \(\mathbf{B}_{0}\in\mathcal{B}\) as initialization. For\(t=1,2,\ldots,T\)do: \[\mathbf{B}_{t}\leftarrow\Pi_{\mathcal{B}}\bigg{(}\mathbf{B}_{t-1}-\alpha_{t} \nabla_{\mathbf{B}}\mathcal{L}(\mathbf{B}_{t-1})\bigg{)}.\] Endfor Output:\(\mathbf{B}_{T}\)
```

**Algorithm 1** Projected Gradient Descent Algorithm to Estimate Decision Function for ITR-Learning

To help the convenient implementation of the above PGD algorithm, we provide the gradient of the ITR objective below. For the continuous outcome, we have (see Proposition F.1)

\[\nabla_{\mathbf{B}}\,\mathcal{L}(\mathbf{B})=\frac{1}{n}\sum_{i=1}^{n}\omega(a _{i},\mathbf{x}_{i})\,\left(\mathbf{x}_{i}\mathbf{x}_{i}^{T}\mathbf{B}\mathbf{ u}_{a_{i}}\mathbf{u}_{a_{i}}^{T}-\frac{K}{K-1}y_{i}\mathbf{x}_{i}\mathbf{u}_{a_{i}}^ {T}\right)+\lambda_{2}\mathbf{B}. \tag{28}\]

For the binary outcome, we have (see Proposition F.2)

\[\nabla_{\mathbf{B}}\,\mathcal{L}(\mathbf{B})=\frac{1}{n}\left(\sum_{i=1}^{n} \omega(a_{i},\mathbf{x}_{i})\left(\frac{\exp(\mathbf{u}_{a_{i}}^{T}\mathbf{B}^ {T}\mathbf{x}_{i})}{1+\exp(\mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i}) }-y_{i}\right)\mathbf{x}_{i}\mathbf{u}_{a_{i}}^{T}\right)+\lambda_{2}\mathbf{B}. \tag{29}\]

### Additional improvements by dimension reduction: variable screening procedure

High dimensional covariates can significantly affect the performance of methods for estimating weights, outcome regression models, and the final ITR. Specifically, using screened variables is beneficial in estimating outcome regression models by focusing on fewer variables that significantly impact the outcome. Additionally, estimating balancing weights based on effect modifiers rather than all variables provides more reliable weighting estimates. Hence, the screening approach serves to reduce bias and enhance the efficiency of ITR-Learning.

Several techniques have been developed to choose appropriate variables in causal inference. One popular approach in the ITR setting is the group-lasso method introduced in [33] which filters out variables that only affect the treatment-free effect. For the ATE estimation, _sure independence screening_ (SIS) method [16; 32] is widely used, which involves an initial screening step based on Pearson correlations between each covariate and the outcome. The covariates are then ranked based on these correlations, and only the top-ranked ones are used in the downstream analysis. While one could also use similar variable screening methods in the ITR setting, the use of Pearson correlation may limit their effectiveness in capturing nonlinear relationships, potentially leading to biased estimates of the ITR.

To address this issue, alternative approaches that employ non-linear dependence tests are explored, including the ball covariance, mutual information test, and distance covariance [42; 37; 48]. Ball covariance, previously applied to variable selection in ATE estimation, has been found to be effective [49]. Distance covariance is another way to capture the nonlinear dependence between two random vectors/variables [48]. The test compares the observed distances between pairs of observations to the expected distances under the assumption of independence. A zero value of distance covariance indicates independence between two random vectors. If the observed distances are significantly far from the expected distances, it indicates statistical dependence between the random vectors. The \(p\)-value is calculated by comparing the observed empirical distance with the distribution of the same test statistic obtained from random permutations of the data under the assumption of independence. We propose adapting the distance covariance due to its computational efficiency and empirical performance for the settings we explored.

Now, we introduce our proposed variable screening procedure. In a setting with \(K\) treatments, we aim to find which covariates have conditional dependence on the outcome given the treatment. Doing so allows the identification of covariates that either modify the treatment effect or otherwise affect the outcome (i.e. are precision variables). To do so, for each covariate \(X_{j}\) where \(j\in\{1,\ldots,p\}\), conduct the distance covariance test between covariate and outcome \(K\) times, conditioning on each treatment value. This conditioning on the treatment creates subgroups based on treatment assignments, allowing the assessment of the dependence between the covariate and the outcome within each subgroup. Then we set the \(p\)-value of the covariate \(X_{j}\) as the minimum p-value from these \(K\) tests. This approach ensures that we retain as many relevant variables as possible during this preliminary variable screening stage. For the same reason, we avoid the use of multiple comparison adjustments such as Bonferroni correction. Instead, we opt to select variables with a significance level of 5%. We perform the same steps for all covariates in the dataset. These procedures are summarized in Algorithm 2.

```
Input:\(\mathbf{X}\in\mathbf{R}^{n\times p};\ Y\in\mathbf{R}^{n};\ A\in\{1,\ldots,K\}\).  Initialize \(\hat{\mathbf{X}}=\{\}\). for\(j=1,2,\ldots,p\)do for\(k=1,2,\ldots,K\)do  Perform distance covariance test\((X_{j},Y|A=k)\) and obtain \(p\)-value, \(p_{j,k}\). endfor \(p_{j}\leftarrow\min(p_{j,1},p_{j,2},\ldots,p_{j,K})\). if\(p_{j}<0.05\)then \(\hat{\mathbf{X}}\leftarrow\hat{\mathbf{X}}\cup\{\mathbf{X}_{j}\}\). endif endfor Output:\(\hat{\mathbf{X}}\), the set of screened variables containing effect modifiers and precision variables.
```

**Algorithm 2** Variable Screening using Distance Covariance

This conditional independence test is crucial to retain effect modifiers and precision variables while filtering out instrumental variables associated with treatment but not the outcome [49]. While acknowledging that the procedure is ad-hoc, the simulation results show that it effectively reduces the number of covariates to estimate covariate balancing weights and outcome regression models, which improves efficiency and helps avoid overfitting.

Additional improvements by variance reduction: combining inverse variance weighting with augmented outcome

Our additional recommendation includes dually using both augmented outcomes and inverse variance weighting for variance reduction. Under our working model (1), the outcome can be explained by the sum of the treatment-free effects of covariates, the interaction effect between covariates and each treatment, and random noise. As the interaction effect alone determines optimal treatment assignments, the treatment-free effect acts as extra noise. This noise can be mitigated using outcome model augmentation. One way to implement outcome augmentation is to use the augmented outcomes \(\hat{Y}:=Y-\widehat{\mu}(\mathbf{X})\) in place of the original outcomes for continuous outcomes when applying an ITR estimation method. Flexible techniques such as random forest and neural networks can be utilized to estimate the treatment-free effect. This is the first implementation for multi-category treatments with DCBWs, whereas previous work focused on binary treatments with IPW [44].

Augmented outcomes can refine ITR estimates when correctly specified. Yet, misspecification of either outcome or treatment-free effect models can lead to heteroscedastic errors due to residual treatment-free effects. This is where SABD-Learning's inverse variance weighting helps, providing robustness against misspecification. SABD-Learning extends AD-Learning designed for homoscedasticity [39] to account for heteroscedasticity, i.e. when the conditional variance of the (potential) outcome given the covariates is not constant [44]. This method estimates the conditional variance of outcomes and reweights the final objective function by adjusting each sample's contribution based on the inverse of the estimated variance. Therefore, we propose to combine outcome augmentation and inverse variance weighting for robust and efficient ITR learning, as they are complementary methods.

The key idea behind outcome augmentation is to add an asymptotically unbiased quantity to the objective function (negative log-likelihood function) so that the new optimizer for the 'augmented objective function' has a provably smaller variance than the original estimator. To do this, let \(h\) be some unknown (to be determined) augmentation function depending on the covariate. The key property of asymptotic unbiasedness is

\[\frac{1}{n}\sum_{i=1}^{n}w(a_{i},\mathbf{x}_{i})\mathbf{u}_{a_{i}}^{T}\mathbf{ B}^{T}h(\mathbf{x}_{i})\to 0\quad\text{as $n\rightarrow\infty$ in probability for all $\mathbf{B}\in\mathbb{R}^{p\times(K-1)}$.} \tag{30}\]

We will justify this property in Corollary C.3 toward the end of this section. Given (30), we have

\[\ell(\mathbf{B})\approx\ell(\mathbf{B})+\frac{1}{n}\sum_{i=1}^{n}w_{i}\mathbf{ u}_{a_{i}}^{T}\mathbf{B}^{T}h(\mathbf{x}_{i}), \tag{31}\]

where \(\approx\) here means asymptotic equality in probability as \(n\rightarrow\infty\). Denote the 'augmented objective function' on the right-hand side as \(\ell_{aug}(\mathbf{B})\).

In the remainder of this section, we introduce the optimal augmented function \(h_{\text{opt}}(\mathbf{X})\) that minimizes the variance of the estimator, while maintaining asymptotic unbiasedness. We demonstrate that outcome augmentation remains valid when IPW is replaced with DCBWs, such as EBWs and MMD balancing weights. Our approach extends the work of [50, 9] to a more general setting, incorporating other types of covariate balancing weights that have asymptotic consistency (38) in a multi-category treatment setting.

Let \(S(\mathbf{B})\) be the derivative of the original objective function \(\ell(\mathbf{B})\) with respect to \(\mathbf{B}\). Then setting the optimal \(h(\mathbf{X})\) is equivalent to minimizing the conditional variance of

\[\widehat{\mathbf{B}}_{a}-\mathbf{B}^{\star}\approx\underbrace{S(\mathbf{B})-w(A,\mathbf{X})h(\mathbf{X})\mathbf{u}_{A}^{T}|\mathbf{X}=\mathbf{x}}_{=:\mathbf{Z }}\]

where \(\widehat{\mathbf{B}}_{a}\) is the minimizer of \(\ell_{\text{aug}}(\mathbf{B})\), and \(\mathbf{B}^{\star}\) is the minimizer of \(\mathbb{E}[\ell(\mathbf{B})]\). Using the property of asymptotic unbiasedness Theorem C.2, which is \(\mathbb{E}[\mathbf{Z}]=0\), we need to minimize \(\mathbb{E}[\|\mathbf{Z}\|_{F}^{2}]\) with respect to \(h\) to minimize the conditional variance of \(\mathbf{Z}\). Using the property of matrix trace,

\[\nabla_{h}\mathbb{E}[\|\mathbf{Z}\|_{F}^{2}]=\nabla_{h}\mathbb{E}[\mathrm{tr}( \mathbf{Z}^{T}\mathbf{Z})]=-2\mathbb{E}[w(A,\mathbf{X})\,\mathbf{Z}\,\mathbf{u }_{A})|\mathbf{X}=\mathbf{x}],\]

and setting \(\nabla_{h}\mathbb{E}[\|\mathbf{Z}\|_{F}^{2}]=0\). Thus, \(h_{\text{opt}}(\mathbf{X})\) satisfies

\[\mathbb{E}[w(A,\mathbf{X})(S(\mathbf{B})-w(A,\mathbf{X})h_{\text{opt}}( \mathbf{X})\mathbf{u}_{A}^{T})\mathbf{u}_{A})|\mathbf{X}=\mathbf{x}]=0.\]

Since \(\mathbf{u}_{A}^{T}\mathbf{u}_{A}=1\), we have

\[h_{\text{opt}}(\mathbf{x})=\frac{\mathbb{E}[w(A,\mathbf{X})S(\mathbf{B}^{\star })\mathbf{u}_{A}|\mathbf{X}=\mathbf{x}]}{\mathbb{E}[w(A,\mathbf{X})^{2}| \mathbf{X}=\mathbf{x}]}. \tag{32}\]

Now, we determine the functional form of \(h_{\text{opt}}(\mathbf{X})\) for both continuous and binary outcomes. For simplicity, let us define

\[w:=w(A,\mathbf{X}),\quad t_{i}:=\mathbf{u}_{a_{i}}^{T}\mathbf{B} ^{T}\mathbf{x}_{i}\] \[R_{1}(\mathbf{x}):=\mathbb{E}[w^{2}\mathbf{u}_{A}|\mathbf{X}= \mathbf{x}],\quad R_{2}(\mathbf{x}):=\mathbb{E}[w^{2}Y|\mathbf{X}=\mathbf{x}], \quad c_{1}=E[w^{2}|\mathbf{X}=\mathbf{x}].\]Then for continuous outcomes, using (28) without regularization term for \(S(\mathbf{B})\) and (32), we have

\[\ell_{aug}(\mathbf{B})\] \[=\frac{1}{n}\sum_{i=1}^{n}\left[w_{i}\left(\frac{K}{K-1}y_{i}- \mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i}\right)^{2}-w_{i}\mathbf{u}_ {a_{i}}^{T}\mathbf{B}^{T}h_{\text{opt}}(\mathbf{x}_{i})\right]\] \[=\frac{1}{n}\sum_{i=1}^{n}\left[w_{i}\left(\frac{K}{K-1}y_{i}- \mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i}\right)^{2}-w_{i}\mathbf{u}_ {a_{i}}^{T}\mathbf{B}^{T}\frac{1}{\widehat{c_{1}}}\left(\mathbf{x}_{i}\mathbf{ x}_{i}^{T}\mathbf{B}\widehat{R}_{1}(\mathbf{x}_{i})-\frac{2K}{K-1}\mathbf{x}_{i} \widehat{R}_{2}(\mathbf{x}_{i})\right)\right]\] \[=\frac{1}{n}\sum_{i=1}^{n}w_{i}\left[\left(\frac{K}{K-1}y_{i}-t_ {i}\right)^{2}-t_{i}\left(\mathbf{x}_{i}^{T}\mathbf{B}\frac{1}{\widehat{c_{1} }}\widehat{R}_{1}(\mathbf{x}_{i})\right)+t_{i}\left(\frac{2K}{K-1}\frac{1}{ \widehat{c_{1}}}\widehat{R}_{2}(\mathbf{x}_{i})\right)\right]\] \[=\frac{1}{n}\sum_{i=1}^{n}w_{i}\left[\left(t_{i}-\frac{K}{K-1} \left(y_{i}-\frac{1}{\widehat{c_{1}}}\widehat{R}_{2}(\mathbf{x}_{i})\right) \right)^{2}+\text{constant w.r.t }\mathbf{B}\right],\]

assuming \(\widehat{R}_{1}(\mathbf{x}_{i})/\widehat{c}_{1}=0\). Therefore, we obtain the optimal augmented function with the minimum estimation variance by properly defining \(R_{2}(\mathbf{X})\). Our simulation studies demonstrate that setting \(R_{2}(\mathbf{X})=\mathbb{E}[Y|\mathbf{X}=\mathbf{x}]\) leads to significant improvement.

Similarly, for binary outcomes, let us define

\[Q_{1}(\mathbf{x}) :=\mathbb{E}\left[w^{2}\frac{\exp(\mathbf{u}_{A}^{T}\mathbf{B}^{T }\mathbf{X})}{1+\exp(\mathbf{u}_{A}^{T}\mathbf{B}^{T}\mathbf{X})}\bigg{|} \mathbf{X}=\mathbf{x}\right],\qquad Q_{2}(\mathbf{x}):=\mathbb{E}[w^{2}Y| \mathbf{X}=\mathbf{x}].\]

Then using (29) without regularization term for \(S(\mathbf{B})\) and (32), we have

\[\ell_{\text{aug}}(\mathbf{B}) =\frac{1}{n}\sum_{i=1}^{n}w_{i}\left[-y_{i}\mathbf{u}_{a_{i}}^{T} \mathbf{B}^{T}\mathbf{x}_{i}+\log(1+\exp(\mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T} \mathbf{x}_{i}))+\mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}h_{\text{opt}}(\mathbf{x }_{i})\right]\] \[=\frac{1}{n}\sum_{i=1}^{n}w_{i}\left[-y_{i}t_{i}+\log(1+\exp(t_{i }))+\frac{1}{\widehat{c_{1}}}t_{i}(\widehat{Q}_{1}(\mathbf{x})-\widehat{Q}_{ 2}(\mathbf{x}))\right]\] \[=\frac{1}{n}\sum_{i=1}^{n}w_{i}\left[-t_{i}\left(y_{i}-\frac{1}{ \widehat{c_{1}}}(\widehat{Q}_{1}(\mathbf{x})-\widehat{Q}_{2}(\mathbf{x})) \right)+\log(1+\exp(t_{i}))\right].\]

Therefore, by selecting proper \(Q_{1}(\mathbf{X})\) and \(Q_{2}(\mathbf{X})\), the minimizer of \(\ell_{\text{aug}}(\mathbf{B})\) has smaller variance than that of the original estimator.

Now we prove asymptotic unbiasedness of \(\frac{1}{n}\sum_{i=1}^{n}w(a_{i},\mathbf{x}_{i})\mathbf{u}_{a_{i}}^{T}\mathbf{ B}^{T}h(\mathbf{x}_{i})\). First, we consider IPW-based conditions for multi-category treatments.

**Lemma C.1**.: _Let \(\mathcal{A}=\{1,\ldots,K\}\) be the possible treatment sets. Assume we have i.i.d. observations \((\mathbf{x}_{i},a_{i})\) from a population distribution of \((\mathbf{X},A)\) such that \(\mathbb{P}(A=a\,|\,\mathbf{x})\) is uniformly positive for all \(a,\mathbf{x}\). Define_

\[T:=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{\mathbb{P}(A=a_{i}|\mathbf{x}_{i})}h( \mathbf{x}_{i})g(a_{i}). \tag{33}\]

_If \(g\) is any function with \(\sum_{j\in\mathcal{A}}g(j)=0\), then for any function \(h(\mathbf{X}):\mathbb{R}^{p}\rightarrow\mathbb{R}^{p}\), then \(T\to 0\) as \(n\rightarrow\infty\) in probability._

Proof.: First note that by iterated expectation,

\[\mathbb{E}[T]=\mathbb{E}\left[\frac{h(\mathbf{X})g(A)}{\mathbb{P} (A|\mathbf{X})}\right] =\mathbb{E}_{\mathbf{X}}\left[\mathbb{E}_{A}\left[\frac{h(\mathbf{ X})g(A)}{\mathbb{P}(A|\mathbf{X})}\bigg{|}\mathbf{X}\right]\right] \tag{34}\] \[=\mathbb{E}_{\mathbf{X}}\left[\sum_{j\in\mathcal{A}}\frac{h( \mathbf{X})g(j)}{\mathbb{P}(A=j|\mathbf{X})}\mathbb{P}(A=j|\mathbf{X})\right]\] (35) \[=\mathbb{E}_{\mathbf{X}}\left[h(\mathbf{X})\sum_{j\in\mathcal{A }}g(j)\right]\] (36) \[=0. \tag{37}\]Then by the standard weak law of large numbers for i.i.d. samples, the empirical mean \(T\) converges in probability to its population mean \(\mathbb{E}[T]=0\) as \(n\to\infty\). 

Note that in the statement of the Lemma C.1, the choice of such \(g(j)\) can be each treatment vertex under the AD-Learning [39]. Now we replace IPW \(\mathbb{P}(A=a_{i}|\mathbf{x}_{i})\) with covariate balancing weights \(w(a_{i},\mathbf{x}_{i})\) that has the following asymptotic consistency property:

\[\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}\left(w(a_{i},\mathbf{x}_{i})-\frac{ 1}{\mathbb{P}(a_{i}|\mathbf{x}_{i})}\right)^{2}=0. \tag{38}\]

For instance, _maximum mean discrepancy_ (MMD) balancing weights, a type of distributional covariate energy balancing weight, satisfy the property (38) [8].

**Theorem C.2** (Asymptotic unbiasedness of augmented function).: _Assume we have i.i.d. observations \((\mathbf{x}_{i},a_{i})\) from population distribution of \((\mathbf{X},A)\). If \(g:\mathcal{A}\to\mathbb{R}\) is any bounded function with \(\sum_{j\in\mathcal{A}}g(j)=0\), then for any bounded function \(h(\mathbf{x}):\mathbb{R}^{p}\to\mathbb{R}^{p}\), define_

\[T_{0}:=\frac{1}{n}\sum_{i=1}^{n}w(a_{i},\mathbf{x}_{i})h(\mathbf{x}_{i})g(a_{ i}), \tag{39}\]

_where the weights \(w(a_{i},\mathbf{x}_{i})\) satisfy (38). Then \(T_{0}\to 0\) as \(n\to\infty\) in probability. That is, \(T_{0}\) is an asymptotically mean zero, making it suitable to be used as the augmented function._

Proof.: By the weak law of large numbers for i.i.d. samples, the empirical mean \(T_{0}\) converges in probability to its population mean \(\mathbb{E}[T_{0}]=\mathbb{E}[w(A,\mathbf{X})h(\mathbf{X})g(A)]\). By continuous mapping theorem, \(\|T_{0}\|^{2}\) converges to \(\|\mathbb{E}[T_{0}]\|^{2}\) as \(n\to\infty\) in probability. Then we have

\[\|\mathbb{E}[T_{0}]\|^{2}=\|\mathbb{E}[w(A,\mathbf{X})h(\mathbf{X} )g(A)]\|^{2} \stackrel{{(a)}}{{=}}\left\|\mathbb{E}[w(A,\mathbf{X} )h(\mathbf{X})g(A)]-\mathbb{E}\left[\frac{h(\mathbf{X})g(A)}{\mathbb{P}(A| \mathbf{X})}\right]\right\|^{2}\] \[=\left\|\mathbb{E}\left[\left(w(A,\mathbf{X})-\frac{1}{\mathbb{P }(A|\mathbf{X})}\right)h(\mathbf{X})g(A)\right]\right\|^{2}\] \[\stackrel{{(b)}}{{\leq}}\mathbb{E}\left[\left\|\left( w(A,\mathbf{X})-\frac{1}{\mathbb{P}(A|\mathbf{X})}\right)h(\mathbf{X})g(A)\right\|^{2}\right]\] \[\leq C\,\mathbb{E}\left[\left(w(A,\mathbf{X})-\frac{1}{\mathbb{P }(A|\mathbf{X})}\right)^{2}\right]\] \[\stackrel{{(c)}}{{=}}\lim_{n\to\infty}C\,\frac{1}{n }\sum_{i=1}^{n}\left(w(a_{i},\mathbf{x}_{i})-\frac{1}{\mathbb{P}(a_{i}|\mathbf{ x}_{i})}\right)^{2}\to 0,\]

where \(C\) is any constant such that \(\|h(\mathbf{X})g(A)\|^{2}\leq C\). For (a), we use Lemma C.1; for (b) and (c), we use Jensen's inequality and the law of large numbers, respectively. The last term converges to 0 due to the property (38). Lastly, since \(\|T_{0}\|^{2}\) converges to 0 in probability, we have \(T_{0}\) converges to 0 in probability by continuous mapping theorem. 

The following corollary is useful in deriving outcome augmentation under the AD-Learning framework.

**Corollary C.3**.: _Assume the AD-learning setting. For any function \(h(\mathbf{X})\) depending only on the covariate \(\mathbf{X}\), and for each parameter matrix \(\mathbf{B}\in\mathbb{R}^{p\times(K-1)}\), denoting,_

\[H_{0}(\mathbf{B}):=w(A,\mathbf{X})\mathbf{u}_{A}^{T}\mathbf{B}^{T}h(\mathbf{X }),\]

_we have \(\mathbb{E}[H_{0}(\mathbf{B})]=0\). Furthermore, assuming the weights \(w(A,\mathbf{X})\) satisfying the consistency property (38), in probability as \(n\to\infty\),_

\[\frac{1}{n}\sum_{i=1}^{n}w(a_{i},\mathbf{x}_{i})\mathbf{u}_{a_{i}}^{T}\mathbf{ B}^{T}h(\mathbf{x}_{i})\to 0. \tag{40}\]

Proof.: Follows immediately from Theorem C.2 by noting that \(\sum_{a=1}^{K}\mathbf{u}_{a}=0\)Spectral characterization of optimal covariate balancing weight

Recall the definition of the weighted design matrix:

\[\mathbf{\Psi}:=\frac{1}{n}\sum_{i=1}^{n}w_{i}(\mathbf{u}_{a_{i}}\mathbf{u}_{a_{i}} ^{T})\otimes\left(\mathbf{x}_{i}\mathbf{x}_{i}^{T}\right). \tag{41}\]

This is the \(p(K-1)\times p(K-1)\) design matrix encoding the sample of patients with covariate \(\mathbf{x}_{i}\) and observed treatment \(a_{i}\), for \(i=1,\ldots,n\). We are free to choose the weight \(w_{i}=\omega(a_{i},\mathbf{x}_{i})\) for the \(i\)th patient as a function as long as they sum to one. What is the best way to choose such weights? Our main results (Theorems 3.3, 3.5) suggest that it is best to choose them in a way that the minimum eigenvalue of \(\mathbf{\Psi}\) is as large as possible.

To illustrate this point, suppose for simplicity the covariates are categorical and assume values in a finite set \(\mathcal{X}\). We decompose the summation in (41) according to the treatment and covariate as follows:

\[\mathbf{\Psi} =\frac{1}{n}\sum_{(a,\mathbf{x})\in\mathcal{A}\times\mathcal{X}}w (a,\mathbf{x})\underbrace{(\mathbf{u}_{a_{i}}\mathbf{u}_{a_{i}}^{T})\otimes \left(\mathbf{x}_{i}\mathbf{x}_{i}^{T}\right)}_{=:\mathbf{\Sigma}(a,\mathbf{x} )}\underbrace{\sum_{i=1}^{n}\mathbf{1}(a_{i}=a,\,\mathbf{x}_{i}=\mathbf{x})}_ {=:n(a,\mathbf{x})} \tag{42}\] \[=\sum_{(a,\mathbf{x})\in\mathcal{A}\times\mathcal{X}}w(a,\mathbf{x}) \frac{n(a,\mathbf{x})}{n}\mathbf{\Sigma}(a,\mathbf{x}). \tag{43}\]

Note that \(n(a,\mathbf{x})\) is the number of patients with covariate \(\mathbf{x}\) and treatment \(a\), while \(\mathbf{\Sigma}(a,\mathbf{x})\) is the \(p(K-1)\times p(K-1)\) covariance matrix representing the treatment-covariate pair \((a,\mathbf{x})\).

1. Under IPW \(w(a,\mathbf{x})=\frac{1}{\pi(a|\mathbf{x})}\), since \(n(a,\mathbf{x})\sim\text{Binom}(n,\pi(a|\mathbf{x}))\) (under suitable assumption), using a law of large numbers and central limit theorem, we have \[w(a,\mathbf{x})\frac{n(a,\mathbf{x})}{n}=1+O_{\mathbb{P}}(n^{-1/2}).\] Consequently, the covariance matrix \(\mathbf{\Sigma}(a,\mathbf{x})\) of every treatment-covariate pair \((a,\mathbf{x})\) contributes nearly equally to the weighted design matrix \(\mathbf{\Psi}\). If distinct covariates are encoded as one-hot encoding vectors, then IPW maximizes the minimum eigenvalue of \(\mathbf{\Psi}\). To see this, for simplicity, assume we have binary treatments (\(K=2\) and \(u_{1}=1\), \(u_{2}=-1\)) and two discrete covariates (e.g., 'Male' and 'Female') encoded as one-hot vectors \((1,0)^{T}\) and \((0,1)^{T}\). Then we can write \(\mathbf{\Psi}\) as \[n\mathbf{\Psi} =w(1,M)n(1,M)\begin{bmatrix}1&0\\ 0&0\end{bmatrix}+w(2,M)n(2,M)\begin{bmatrix}1&0\\ 0&0\end{bmatrix}\] \[\qquad+w(1,F)n(1,F)\begin{bmatrix}0&0\\ 0&1\end{bmatrix}+w(2,F)n(2,F)\begin{bmatrix}0&0\\ 0&1\end{bmatrix}\] \[=\begin{bmatrix}w(1,M)n(1,M)+w(2,M)n(2,M)&0\\ 0&w(1,F)n(1,F)+w(2,F)n(2,F)\end{bmatrix}.\] Notice that the minimum eigenvalue of the last \(2\times 2\) matrix is the minimum of its two diagonal entries. With IPW, each weighted count \(w(a,x)n(a,x)\) becomes a constant, and \(\mathbf{\Psi}\) above becomes the identity matrix, achieving the largest possible minimum eigenvalue. Also, in this case, \(\mathbf{\Psi}\) has condition number 1 so the ITR optimization landscape becomes well-conditioned.
2. Using the fact that the minimum eigenvalue is a positive homogeneous and concave function on the space of symmetric matrices, we have \[\lambda_{\min}(\mathbf{\Psi})\geq\sum_{a\in\mathcal{A}}\lambda_{\min}\bigg{(} \underbrace{\sum_{\mathbf{x}\in\mathcal{X}}w(a,\mathbf{x})\frac{n(a,\mathbf{x })}{n}\mathbf{\Sigma}(a,\mathbf{x})}_{=:\mathbf{\Sigma}_{w}(a)}\bigg{)}.\]Here \(\mathbf{\Sigma}_{w}(a)\) is the aggregated weighted covariance matrix for sub-population with treatment \(a\). In the above derivation, we used that \(\mathbf{\Psi}=\sum_{a}\mathbf{\Sigma}_{w}(a)\) by (42) and definition and the following computation using Jensen's inequality:

\[\lambda_{\min}\left(\sum_{a}\mathbf{\Sigma}_{w}(a)\right) =|\mathcal{A}|\lambda_{\min}\left(\sum_{a}\frac{1}{|\mathcal{A}|} \mathbf{\Sigma}_{w}(a)\right)\] \[\geq|\mathcal{A}|\sum_{a}\frac{1}{|\mathcal{A}|}\lambda_{\min}( \mathbf{\Sigma}_{w}(a))=\sum_{a}\lambda_{\min}(\mathbf{\Sigma}_{w}(a)).\]

If we choose the weights so that these matrices are identical, then each treatment group contributes equally to the lower bound above for \(\lambda_{\min}(\mathbf{\Psi})\). This condition is satisfied, for example, when \(w\) is distributional covariate balancing weights, which ensure that the covariate distributions across all treatment groups are identical. There is a remaining degree of freedom for specifying the common covariate distribution, denoted by \(\mu\), for the treatment groups. To maximize the lower bound for \(\lambda_{\min}(\mathbf{\Psi})\), \(\mu\) should be chosen to maximize \(\lambda_{\min}(\mathbf{\Sigma}_{w}(\cdot))\). The specific solution to this optimization depends on the structure of \(\mathbf{\Psi}\). In three-way balancing methods [8], \(\mu\) is typically chosen to be close to the target population distribution. Another common approach, as discussed earlier, is IPW, which sets \(\mu\) as the uniform distribution over the covariate space \(\mathcal{X}\).

[MISSING_PAGE_FAIL:26]

Proof.: Let \(\bar{\mathbf{B}}_{t}:=\mathbf{B}_{t}-\alpha_{t}\nabla_{\mathbf{B}}f(\mathbf{B}_{t})\). Then

\[\|\bar{\mathbf{B}}_{t}-\mathbf{B}^{\star}\|_{F}^{2} =\|\bar{\mathbf{B}}_{t}-\mathbf{B}_{t}\|_{F}^{2}+\|\mathbf{B}_{t} -\mathbf{B}^{\star}\|_{F}^{2}+2(\bar{\mathbf{B}}_{t}-\mathbf{B}_{t},\,\mathbf{B }_{t}-\mathbf{B}^{\star})\] \[=\alpha_{t}^{2}\|\nabla_{\mathbf{B}}f(\mathbf{B}_{t})\|_{F}^{2}+\| \mathbf{B}_{t}-\mathbf{B}^{\star}\|_{F}^{2}-2\alpha_{t}\langle\mathbf{B}_{t}- \mathbf{B}^{\star},\nabla_{\mathbf{B}}f(\mathbf{B}_{t})\rangle\] \[\leq\alpha_{t}^{2}\|\nabla_{\mathbf{B}}f(\mathbf{B}_{t})\|_{F}^{2 }+\|\mathbf{B}_{t}-\mathbf{B}^{\star}\|_{F}^{2}-2\alpha_{t}\langle\mathbf{B}_ {t}-\mathbf{B}^{\star},\nabla_{\mathbf{B}}f(\mathbf{B}_{t})-\nabla_{\mathbf{B }}f(\mathbf{B}^{\star})\rangle\]

using that \(\langle\nabla f(\mathbf{B}^{\star}),\mathbf{B}_{t}-\mathbf{B}^{\star}\rangle\geq 0\) since \(\mathbf{B}^{\star}\) is a stationary point of \(f\) over \(\mathcal{B}\). Using the co-coercivity of \(\mu\)-strongly convex and \(L\)-smooth functions and, we have

\[\|\bar{\mathbf{B}}_{t}-\mathbf{B}^{\star}\|_{F}^{2} \leq\alpha_{t}^{2}\|\nabla f(\mathbf{B}_{t})\|_{F}^{2}+\|\mathbf{ B}_{t}-\mathbf{B}^{\star}\|_{F}^{2}\] \[\quad-2\alpha_{t}\left(\frac{\mu L}{\mu+L}\|\mathbf{B}_{t}- \mathbf{B}^{\star}\|^{2}+\frac{1}{\mu+L}\|\nabla f(\mathbf{B}_{t})-\nabla f( \mathbf{B}^{\star})\|_{F}^{2}\right)\] \[\leq\left(1-\frac{2\alpha_{t}\mu L}{\mu+L}\right)\|\mathbf{B}_{t} -\mathbf{B}^{\star}\|_{F}^{2}+\alpha_{t}^{2}\|\nabla f(\mathbf{B}_{t})\|_{F}^{ 2}-\frac{2\alpha_{t}}{\mu+L}\|\nabla f(\mathbf{B}_{t})-\nabla f(\mathbf{B}^{ \star})\|_{F}^{2}.\]

By Young's inequality,

\[\|\nabla f(\mathbf{B}_{t})\|_{F}^{2}\leq 2\|\nabla f(\mathbf{B}_{t})-\nabla f( \mathbf{B}^{\star})\|_{F}^{2}+2\|\nabla f(\mathbf{B}^{\star})\|_{F}^{2}.\]

Thus the above yields, given that \(\alpha_{t}\leq 1/(\mu+L)\),

\[\|\bar{\mathbf{B}}_{t}-\mathbf{B}^{\star}\|_{F}^{2} \leq\left(1-\frac{2\alpha_{t}\mu L}{\mu+L}\right)\|\mathbf{B}_{t} -\mathbf{B}^{\star}\|_{F}^{2}+2\alpha_{t}\left(\alpha_{t}-\frac{1}{\mu+L} \right)\|\nabla f(\mathbf{B}_{t})-\nabla f(\mathbf{B}^{\star})\|_{F}^{2}\] \[\quad+2\alpha_{t}^{2}\|\nabla f(\mathbf{B}^{\star})\|_{F}^{2}\] \[\leq\left(1-\frac{2\alpha_{t}\mu L}{\mu+L}\right)\|\mathbf{B}_{t} -\mathbf{B}^{\star}\|_{F}^{2}+2\alpha_{t}^{2}\|\nabla f(\mathbf{B}^{\star})\|_ {F}^{2}.\]

Since the projection onto the convex constraint set \(\mathcal{B}\), \(\Pi_{\mathcal{B}}\), is non-expansive,

\[\|\mathbf{B}_{t+1}-\mathbf{B}^{\star}\|_{F}^{2}\leq\|\bar{\mathbf{B}}_{t}- \mathbf{B}^{\star}\|_{F}^{2}\leq(1-\alpha_{t}\rho)\,\|\mathbf{B}_{t}-\mathbf{B }^{\star}\|_{F}^{2}+\alpha_{t}^{2}\sigma^{2},\]

where we have denoted \(\rho:=\frac{2\mu L}{\mu+L}\) and \(\sigma^{2}:=2\|\nabla f(\mathbf{B}^{\star})\|_{F}^{2}\).

Now, we show **(i)** (47). Suppose \(\alpha_{t}\equiv\alpha>0\). Note that \(\mathbf{B}^{\star}\) is a unique minimizer of \(f\) over \(\mathcal{B}\) because of strong convexity. Since \(-\nabla f(\mathbf{B}^{\star})\) lies in the normal cone of \(\mathcal{B}\) at \(\mathbf{B}^{\star}\), we have \(\mathbf{B}^{\star}=\Pi_{\mathcal{B}}(\mathbf{B}^{\star}-\alpha\nabla f( \mathbf{B}^{\star}))\). Using non-expansiveness of convex projection \(\Pi_{\mathcal{B}}\),

\[\|\mathbf{B}_{t+1}-\mathbf{B}^{\star}\|_{F} =\|\Pi_{\mathcal{B}}(\mathbf{B}_{t}-\alpha\nabla f(\mathbf{B}_{t }))-\Pi_{\mathcal{B}}(\mathbf{B}^{\star}-\alpha\nabla f(\mathbf{B}^{\star}))\|_ {F}\] \[\leq\|(\mathbf{B}_{t}-\alpha\nabla f(\mathbf{B}_{t}))-(\mathbf{B} ^{\star}-\alpha\nabla f(\mathbf{B}^{\star}))\|_{F}\] \[=\left\|\int_{0}^{1}(I-\alpha\nabla^{2}f(\mathbf{B}_{t}+s( \mathbf{B}^{\star}-\mathbf{B}_{t})))(\mathbf{B}_{t}-\mathbf{B}^{\star})\,ds \right\|_{F}\] \[\leq\underbrace{\max\{|1-\alpha L|,|1-\alpha\mu|\}}_{=:\rho( \alpha)}\|\mathbf{B}_{t}-\mathbf{B}^{\star}\|_{F}.\]

For the last inequality, since the eigenvalues of \(\nabla^{2}f(\tilde{\mathbf{B}})\) are contained in the interval \([\mu,L]\), the eigenvalues of \(I-\alpha\nabla^{2}f(z)\) are between \(\min(1-\alpha L,\,1-\alpha\mu)\) and \(\max(1-\alpha L,\,1-\alpha\mu)\). Note that \(\rho(\alpha)\) is minimized at \(\alpha=\frac{2}{\mu+L}\) with minimum value \(\frac{L-\mu}{L+\mu}\). It follows that for any \(t\geq 0\),

\[\|\mathbf{B}_{t+1}-\mathbf{B}^{\star}\|_{F}\leq\rho(\alpha)^{t+1}\,\|\mathbf{B}_ {0}-\mathbf{B}^{\star}\|_{F}.\]

Next, we show **(ii)** (48) by induction. Indeed, it holds for \(t=1\) by the choice of \(\nu\). Denoting \(\hat{t}:=\gamma+t\), by the induction hypothesis and by the choices of \(\alpha_{t}=\beta/\hat{t}\) and \(\nu\geq\frac{\beta^{2}\sigma^{2}}{\beta\rho-1}\),

\[\|\mathbf{B}_{t+1}-\mathbf{B}^{\star}\|_{F}^{2} \leq\left(1-\frac{\beta\rho}{\hat{t}}\right)\frac{\nu}{\hat{t}}+ \frac{\beta^{2}\sigma^{2}}{\hat{t}^{2}}\] \[\leq\frac{\hat{t}-1}{(\hat{t}-1)(\hat{t}+1)}\nu=\frac{\nu}{\gamma +t+1}.\]This shows **(ii)**. 

Next, we establish the following global landscape result of the ITR learning problem (6) by computing the Hessian of the objective \(\mathcal{L}(\mathbf{B})\) in (6).

**Lemma E.3** (Local landscape of the ITR objective).: _Let \(\mathcal{L}(\mathbf{B})\) denote the ITR objective in (6)._

**(i)**: _Suppose Assumption_ 3.1 _holds. For the continuous outcome,_

\[(\lambda^{-}+\lambda_{2})\mathbf{I}_{p(K-1)}\preceq\nabla_{\mathrm{vec}( \mathbf{B})}\nabla_{\mathrm{vec}(\mathbf{B})^{T}}\mathcal{L}(\mathbf{B}) \preceq(\lambda^{+}+\lambda_{2})\mathbf{I}_{p(K-1)}, \tag{49}\]

_where the constants_ \(\lambda^{\pm}\) _are defined in Assumption_ 3.1_._
**(ii)**: _Further Assumptions_ 3.1 _and_ 3.2 _hold. For the binary outcome,_

\[(\alpha^{-}\lambda^{-}+\lambda_{2})\mathbf{I}_{p(K-1)}\preceq\nabla_{\mathrm{ vec}(\mathbf{B})}\nabla_{\mathrm{vec}(\mathbf{B})^{T}}\mathcal{L}(\mathbf{B}) \preceq(\alpha^{+}\lambda^{+}+\lambda_{2})\mathbf{I}_{p(K-1)}. \tag{50}\]

_where the constants_ \(\lambda^{\pm}\) _and_ \(\alpha^{\pm}\) _are defined in Assumptions_ 3.1 _and_ 3.2_, respectively._

To maintain the flow, we write down the proof of Lemma E.3 to Section F.

Now we can easily deduce Theorem E.1.

Proof of Theorem e.1.: Lemma E.3 shows that the eigenvalues of the Hessian \(\nabla_{\mathrm{vec}(\mathbf{B})}\nabla_{\mathrm{vec}(\mathbf{B})^{T}}\mathcal{L}\) of the ITR objective in (6) are uniformly bounded between constants \(\mu\) and \(L\) as defined in (10). Under Assumptions 3.1 and 3.2, we have \(0<\mu\leq L<\infty\). This shows that \(\mathcal{L}\) is \(\mu\)-strongly convex and \(L\)-smooth over \(\mathcal{B}\), as claimed in Theorem E.1**(i)**.

Next, noting that \(\|\cdot\|_{F}\leq\|\cdot\|_{1}\), triangle inequality and the fact that \(\hat{\mathbf{B}}_{0},\mathbf{B}^{\star}\in\mathcal{B}\) yields that, for any \(t\geq 0\),

\[\|\mathbf{B}_{t}-\mathbf{B}^{\star}\|_{F}^{2} \leq(\|\mathbf{B}_{t}\|_{F}+\|\mathbf{B}_{\star}\|_{F})^{2} \tag{51}\] \[\leq(\|\mathbf{B}_{t}\|_{1}+\|\mathbf{B}_{\star}\|_{1})^{2}\] (52) \[\leq 4\lambda_{1}^{2}. \tag{53}\]

Hence the assertions **(i)**-**(ii)** follow directly from combining Lemmas E.2 and E.3. 

## Appendix F Proof of Lemma E.3

In this section, we prove Lemma E.3. We will first compute the gradient and the Hessian of the ITR objective in (6) separately for the continuous and binary outcome. We will then prove Lemma E.3 at the end of the section.

### Continuous outcome

**Proposition F.1**.: _Let \(\mathcal{L}(\mathbf{B})\) denote the ITR objective in (6) with continuous outcome. Then_

\[\nabla_{\mathbf{B}}\,\mathcal{L}(\mathbf{B}) =\frac{1}{n}\sum_{i=1}^{n}\omega(a_{i},\mathbf{x}_{i})\,\left( \mathbf{x}_{i}\mathbf{x}_{i}^{T}\mathbf{B}\mathbf{u}_{a_{i}}\mathbf{u}_{a_{i}} ^{T}-\frac{K}{K-1}y_{i}\mathbf{x}_{i}\mathbf{u}_{a_{i}}^{T}\right)+\lambda_{2} \mathbf{B}, \tag{54}\] \[\nabla_{\mathrm{vec}(\mathbf{B})}\nabla_{\mathrm{vec}(\mathbf{B}) ^{T}}\mathcal{L}(\mathbf{B}) =\mathbf{\Psi}+\lambda_{2}\,\mathbf{I}_{p(K-1)}, \tag{55}\]

_where \(\mathbf{\Psi}\) is the weighted design matrix defined at (8)._

Proof.: Recall that the ITR objective for continuous outcome is given by

\[\mathcal{L}(\mathbf{B}) =\frac{1}{2n}\sum_{i=1}^{n}\omega(a_{i},\mathbf{x}_{i})\left( \frac{K}{K-1}y_{i}-\mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i}\right) ^{2}+\frac{\lambda_{2}}{2}||\mathbf{B}||_{F}^{2}.\]It is trivial to show (54). For the Hessian, using (16),

\[\nabla^{2}_{\mathbf{B}}\ \mathcal{L}(\mathbf{B}) =\nabla_{\mathrm{vec}(\mathbf{B})}\nabla_{\mathrm{vec}(\mathbf{B})^ {T}}\ \mathcal{L}(\mathrm{vec}(\mathbf{B}))\] \[=\nabla_{\mathrm{vec}(\mathbf{B})}\ \mathrm{vec}\left(\frac{1}{n} \sum_{i=1}^{n}\omega(a_{i},\mathbf{x}_{i})\ \bigg{(}\mathbf{x}_{i}\mathbf{x}_{i}^{T} \mathbf{B}\mathbf{u}_{a_{i}}\mathbf{u}_{a_{i}}^{T}-\frac{K}{K-1}y_{i}\mathbf{ x}_{i}\mathbf{u}_{a_{i}}^{T}\bigg{)}\right)^{T}+\lambda_{2}\nabla^{2}_{\mathbf{B}}( \mathbf{B})\] \[=\nabla_{\mathrm{vec}(\mathbf{B})}\ \mathrm{vec}\left(\frac{1}{n} \sum_{i=1}^{n}\omega(a_{i},\mathbf{x}_{i})\ \big{(}\mathbf{x}_{i}\mathbf{x}_{i}^{T} \mathbf{B}\mathbf{u}_{a_{i}}\mathbf{u}_{a_{i}}^{T}\big{)}\right)^{T}+\lambda_ {2}\,\mathbf{I}_{p(K-1)}\] \[=\nabla_{\mathrm{vec}(\mathbf{B})}\ \left[\frac{1}{n} \sum_{i=1}^{n}\omega(a_{i},\mathbf{x}_{i})\,\mathrm{vec}(\mathbf{B})^{T}\bigg{(} (\mathbf{u}_{a_{i}}\mathbf{u}_{a_{i}}^{T})^{T}\otimes\big{(}\mathbf{x}_{i} \mathbf{x}_{i}^{T}\big{)}\bigg{)}^{T}\right]+\lambda_{2}\,\mathbf{I}_{p(K-1)}\] \[=\frac{1}{n}\sum_{i=1}^{n}\omega(a_{i},\mathbf{x}_{i})\bigg{(}( \mathbf{u}_{a_{i}}\mathbf{u}_{a_{i}}^{T})^{T}\otimes\big{(}\mathbf{x}_{i} \mathbf{x}_{i}^{T}\big{)}\bigg{)}^{T}+\lambda_{2}\,\mathbf{I}_{p(K-1)}\] \[=\frac{1}{n}\sum_{i=1}^{n}\omega(a_{i},\mathbf{x}_{i})(\mathbf{u} _{a_{i}}\mathbf{u}_{a_{i}}^{T})\otimes\big{(}\mathbf{x}_{i}\mathbf{x}_{i}^{T} \big{)}+\lambda_{2}\,\mathbf{I}_{p(K-1)}\] \[=\mathbf{\Psi}+\lambda_{2}\,\mathbf{I}_{p(K-1)}\]

This shows the assertion. 

### Binary outcome

Given the current model parameter \(\mathbf{B}\in\mathbb{R}^{p\times(K-1)}\), define the scalars \(z_{i},p_{i},w_{i}\) for the \(i\)-th sample by

\[z_{i}:=\mathbf{u}_{\mathbf{a}_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i},\qquad p_{ i}:=\frac{\exp(z_{i})}{1+\exp(z_{i})},\qquad w_{i}:=w(a_{i},\mathbf{x}_{i}). \tag{56}\]

For each binary label \(y\in\{0,1\}\) and activation \(z\in\mathbb{R}\), recall the negative log-likelihood of observing \(y\) under the logistic model Ber \((p_{i})\) is given by

\[\ell(y,z)=\log(1+\exp(z))-yz.\]

An easy computation shows

\[\dot{\mathbf{h}}(y,z) :=\frac{\partial}{\partial z}\ell(y,z)=\frac{\exp(z)}{1+\exp(z)}-y, \tag{57}\] \[\ddot{\mathbf{h}}(y,z) :=\frac{\partial^{2}}{\partial z^{2}}\ell(y,z)=\left(\frac{\exp( z)}{1+\exp(z)}\right)\left(1-\frac{\exp(z)}{1+\exp(z)}\right)\leq\frac{1}{4}. \tag{58}\]

For the forthcoming computations, define matrices

\[\mathbf{K} :=[\dot{\mathbf{h}}(y_{1},z_{1}),\ldots,\dot{\mathbf{h}}(y_{n},z_ {n})]\in\mathbb{R}^{1\times n}, \tag{59}\] \[\mathbf{M} :=\mathrm{diag}\left(\ddot{\mathbf{h}}(y_{1},z_{1}),\ldots,\ddot{ \mathbf{h}}(y_{n},z_{n})\right)\in\mathbb{R}^{n\times n},\] (60) \[\mathbf{\Phi} :=[\mathbf{u}_{a_{1}}\otimes\mathbf{x}_{1},\ldots,\mathbf{u}_{a_ {n}}\otimes\mathbf{x}_{n}]\in\mathbb{R}^{p(K-1)\times n},\] (61) \[\mathbf{W} :=\mathrm{diag}(w_{1},\ldots,w_{n})\in\mathbb{R}^{n\times n}. \tag{62}\]

**Proposition F.2**.: _Let \(\mathcal{L}(\mathbf{B})\) denote the ITR objective in (6) with binary outcome. Suppose Assumption 3.1 holds. Let \(z_{i}:=\mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i}\) for \(i=1,\ldots,n\). Then_

\[\nabla_{\mathbf{B}}\,\mathcal{L}(\mathbf{B}) =\frac{1}{n}\left(\sum_{i=1}^{n}w_{i}\dot{\mathbf{h}}(y_{i},z_{ i})\mathbf{x}_{i}\mathbf{u}_{a_{i}}^{T}\right)+\lambda_{2}\mathbf{B}, \tag{63}\] \[\nabla_{\mathrm{vec}(\mathbf{B})}\nabla_{\mathrm{vec}(\mathbf{B})^ {T}}\mathcal{L}(\mathbf{B}) =\frac{1}{n}\mathbf{\Phi}(\mathbf{M}\mathbf{W})\mathbf{\Phi}^{T} +\lambda_{2}\mathbf{I}_{p(K-1)} \tag{64}\]

[MISSING_PAGE_FAIL:30]

Then using (66) with (72), we get

\[\nabla_{\mathrm{vec}(\mathbf{B})}\nabla_{\mathrm{vec}(\mathbf{B})^{T }}\mathcal{L}(\mathbf{B}) =\nabla_{\mathrm{vec}(\mathbf{B})}\left[\frac{1}{n}\,\mathrm{vec}( \mathbf{K})^{T}\mathbf{W}\mathbf{\Phi}^{T}+\lambda_{2}\mathbf{I}_{\mathrm{p}(K -1)}\right] \tag{73}\] \[=\frac{1}{n}\mathbf{\Phi}(\mathbf{M}\mathbf{W})\mathbf{\Phi}^{T}+ \lambda_{2}\mathbf{I}_{\mathrm{p}(K-1)}. \tag{74}\]

This shows the assertion. 

### Proof of Lemma e.3

We are now ready to give proof of Lemma e.3.

Proof of Lemma e.3.: Recall that \(\mathbf{\Phi}:=[\mathbf{u}_{a_{1}}\otimes\mathbf{x}_{1},\ldots,\mathbf{u}_{a_{n }}\otimes\mathbf{x}_{n}]\). Note that

\[\frac{1}{n}\mathbf{\Phi}\mathbf{W}\mathbf{\Phi}^{T} =\frac{1}{n}\sum_{i=1}^{n}w_{i}(\mathbf{u}_{a_{i}}\otimes\mathbf{ x}_{i})(\mathbf{u}_{a_{i}}\otimes\mathbf{x}_{i})^{T} \tag{75}\] \[=\frac{1}{n}\sum_{i=1}^{n}w_{i}(\mathbf{u}_{a_{i}}\mathbf{u}_{a_{ i}}^{T}\otimes\mathbf{x}_{i}\mathbf{x}_{i}^{T})\] (76) \[=\mathbf{\Psi}, \tag{77}\]

where \(\mathbf{\Psi}\) is the weighted design matrix defined at (8). (In particular, since \(\mathbf{W}\) is a diagonal matrix of nonnegative entries, this shows that \(\mathbf{\Psi}\) is positive semi-definite.)

According to Propositions F.1 and F.2, we have

\[\nabla_{\mathrm{vec}(\mathbf{B})}\nabla_{\mathrm{vec}(\mathbf{B}) ^{T}}\mathcal{L}(\mathbf{B})=\begin{cases}n^{-1}\mathbf{\Phi}\mathbf{W}\mathbf{ \Phi}^{T}+\lambda_{2}\mathbf{I}_{\mathrm{p}(K-1)}&\text{for continuous outcome}\\ n^{-1}\mathbf{\Phi}\mathbf{M}\mathbf{W}\mathbf{\Phi}^{T}+\lambda_{2}\mathbf{I}_{ \mathrm{p}(K-1)}&\text{for binary outcome}.\end{cases} \tag{78}\]

Thus the assertion for the continuous outcome follows immediately. For the case of a binary outcome, by Assumption 3.2 we have

\[\alpha^{-}\mathbf{I}_{n}\preceq\mathbf{M}\preceq\alpha^{+}\mathbf{I}_{n}. \tag{79}\]

Then

\[\alpha^{-}\left(\frac{1}{n}\mathbf{\Phi}\mathbf{W}\mathbf{\Phi}^{T}\right) \preceq\frac{1}{n}\mathbf{\Phi}(\mathbf{M}\mathbf{W})\mathbf{\Phi}^{T}\preceq \alpha^{+}\left(\frac{1}{n}\mathbf{\Phi}\mathbf{W}\mathbf{\Phi}^{T}\right). \tag{80}\]

It follows that

\[\alpha^{-}\lambda_{\min}\left(\frac{1}{n}\mathbf{\Phi}\mathbf{W} \mathbf{\Phi}^{T}\right)\mathbf{I}_{\mathrm{p}(K-1)}\preceq\frac{1}{n} \mathbf{\Phi}(\mathbf{M}\mathbf{W})\mathbf{\Phi}^{T}\preceq\alpha^{+}\lambda _{\max}\left(\frac{1}{n}\mathbf{\Phi}\mathbf{W}\mathbf{\Phi}^{T}\right) \mathbf{I}_{\mathrm{p}(K-1)} \tag{81}\]

This shows the assertion for the binary outcome holds.

A non-asymptotic consistency of weighted, constrained, and regularized MLE

In this section, we provide a general result on the non-asymptotic consistency of MLE in a general setting. Here, observations are assumed to be i.i.d. from a generative model, and the unknown true parameter of the generative model may lie on the boundary of the parameter space.

A generic sample \((X,Y)\) consists of a random covariate \(X\) and outcome \(Y\). (In the ITR setting, we will specialize \(X\) to be the pair of the patient's covariate and the corresponding treatment.) We assume the covariate \(X\) follows a probability distribution \(\mu(\cdot)\) and the conditional distribution of \(Y\) given \(X=x\) is parameterized as \(\pi_{\mathbf{\theta}_{\star}}(\,\cdot\,|\,x)\) for an unknown parameter \(\mathbf{\theta}_{\star}\). Hence the generic random sample \((X,Y)\) follows the parameterized distribution \(\pi_{\mathbf{\theta}_{\star}}(\,y\,|\,x)\mu(x)\). Assume we know a smooth parametric family of conditional distributions \(\mathbf{\theta}\mapsto\pi_{\mathbf{\theta}}(\,\cdot\,|\,x)\), where the true parameter \(\mathbf{\theta}_{\star}\) lies in a convex parameter space \(\mathbf{\Theta}\subseteq\mathbb{R}^{d}\). Our goal is to estimate \(\mathbf{\theta}_{\star}\) from i.i.d. samples \((x_{i},y_{i})\), \(i=1,\ldots,n\) drawn from \(\pi_{\mathbf{\theta}_{\star}}(\,\cdot\,|\,\cdot)\mu(\cdot)\).

To estimate \(\mathbf{\theta}_{\star}\), a natural approach is to maximize the empirical joint likelihood function. Since the marginal distribution of the covariate \(x\) is independent of the parameter \(\mathbf{\theta}\), we can apply the method of "reweighting" the covariate distribution by introducing an additional random weight \(\omega(x)\) that we can control. Namely, fix a weighting function \(\omega(\cdot):\mathcal{X}\rightarrow[0,\infty)\), where \(\mathcal{X}\) is covariate space, that assigns a nonnegative weight to each covariate value \(x\). Consider then the following weighted expected log-likelihood maximization problem:

\[\operatorname*{arg\,min}_{\mathbf{\theta}\in\mathbf{\Theta}}\,\mathbb{E}_{(X,Y)}\left[ -\omega(X)\log\pi_{\mathbf{\theta}_{\star}}(Y\,|\,X)\right], \tag{82}\]

where we omit the marginal distribution \(\mu(X)\) as it is independent of \(\mathbf{\theta}\).

Recall that we get to design the weighting function \(\omega(\cdot)\). We first observe that the true parameter \(\mathbf{\theta}_{\star}\) is a stationary point of the weighted expected log-likelihood function \(\mathbb{E}\left[-\omega(X)\log\pi_{\mathbf{\theta}_{\star}}(Y\,|\,X)\right]\) for arbitrary weighting function \(\omega\). This can be justified under the mild assumption of exchangeable of the expectation and derivatives (which holds under twice continuous differentiability of the log-likelihood function by the dominated convergence theorem). Indeed, note that

\[\mathbb{E}_{(X,Y)}\left[\omega(X)\nabla_{\mathbf{\theta}}\log\pi_{\bm {\theta}}(Y\,|\,X)\right] =\mathbb{E}_{(X,Y)}\left[\omega(X)\frac{\nabla_{\mathbf{\theta}}\pi_{ \mathbf{\theta}}(Y\,|\,X)}{\pi_{\mathbf{\theta}}(Y\,|\,X)}\right]\] \[=\mathbb{E}_{X}\left[\mathbb{E}_{Y}\left[\omega(X)\frac{\nabla_{ \mathbf{\theta}}\pi(Y\,|\,X)}{\pi_{\mathbf{\theta}}(Y\,|\,X)}\,X\right]\right]\] \[=\mathbb{E}_{X}\left[\omega(X)\,\mathbb{E}_{Y\sim\pi_{\mathbf{\theta }_{\star}}(\,\cdot\,|\,X)}\left[\frac{\nabla_{\mathbf{\theta}}\pi_{\mathbf{\theta}}(Y \,|\,X)}{\pi_{\mathbf{\theta}}(Y\,|\,X)}\,\middle|\,X\right]\right]=0,\]

where the last equality follows from

\[\mathbb{E}_{Y\sim\pi_{\mathbf{\theta}_{\star}}(\,\cdot\,|\,X)}\left[ \frac{\nabla_{\mathbf{\theta}}\pi_{\mathbf{\theta}_{\star}}(Y|X)}{\pi_{\mathbf{\theta}_{ \star}}(Y\,|\,X)}\,\middle|\,X\right] =\int\nabla_{\mathbf{\theta}}\pi_{\mathbf{\theta}_{\star}}(y\,|\,X)\,dy\] \[=\nabla_{\mathbf{\theta}}\int\pi_{\mathbf{\theta}_{\star}}(y\,|\,X)\,dy\] \[=\nabla_{\mathbf{\theta}}1=0.\]

Therefore, \(\mathbf{\theta}_{\star}\) is a critical point of \(\mathbb{E}\left[\omega(X)\log\pi_{\mathbf{\theta}}(Y\,|\,X)\right]\). In particular, if we assume that the log-likelihood function \(\log\pi_{\mathbf{\theta}}(y\,|\,x)\mu(x)\) is convex in \(\mathbf{\theta}\), then \(\mathbf{\theta}_{\star}\) is a global maximizer of \(\mathbb{E}\left[\omega(X)\log\pi_{\mathbf{\theta}}(Y\,|\,X)\right]\).

Next, we introduce the finite-sample approximation of (82). Denote \(w_{i}:=\omega(x_{i})\) for the weight of the \(i\)th sample \((x_{i},y_{i})\). Then the weighted negative log-likelihood of the observed samples under the model parameter \(\mathbf{\theta}\) is

\[\mathcal{L}(\mathbf{\theta}):=-\frac{1}{n}\sum_{i=1}^{n}w_{i}\underbrace{\log\pi_ {\mathbf{\theta}}(y_{i}\,|\,\mathbf{x}_{i})}_{=:\mathcal{L}_{\mathbf{\theta}}(\mathbf{ \theta})}+R(\mathbf{\theta}), \tag{83}\]

where \(R(\mathbf{\theta})\) is a suitable choice of regularizer for parameter \(\mathbf{\theta}\). The regularization term can be considered as prior knowledge about the model parameter \(\mathbf{\theta}\).

Let \(\mathbf{\hat{\theta}}_{n}\) denote a (possibly non-unique) minimizer of the above function over \(\mathbf{\Theta}\). This is a minimizer of the loss function \(\mathcal{L}\) over the constraint set \(\mathbf{\Theta}\), which we call the _weighted, constrained, and regularized MLE_ of \(\mathbf{\theta}_{\star}\).

As in the standard MLE analysis, we assume that the negative log-likelihood function \(\mathbf{\theta}\mapsto\mathcal{L}_{0}(\mathbf{\theta})\) is differentiable and strictly convex for all \(x\in\mathbb{R}^{p}\). Statistical literature typically assumes that the true parameter lies in the interior of the constrained parameter space, \(\mathbf{\Theta}\), a convex subset of \(\mathbb{R}^{d}\). In contrast, we allow the true parameter \(\mathbf{\theta}_{\star}\) to lie on the boundary of \(\mathbf{\Theta}\). Regardless of that, we have seen above that the gradient of the weighted expected log-likelihood function vanishes at the true parameter \(\mathbf{\theta}_{\star}\).

In this setting, we aim to provide a high-probability guarantee that there exists a global minimizer of (83) that is close to the true parameter \(\mathbf{\theta}_{\star}\)_for an arbitrary weighting function_\(\omega(\cdot)\), which we will optimize later for the best statistical guarantee. In Theorem G.1 below, we obtain such a result in the non-asymptotic, weighted, constrained, and regularized setting. For its proof, we combine a classical approach from [15] with concentration inequalities, namely, a classical Berry-Esseen bound for deviations from the standard normal distribution for independent but non-identically distributed random variables and a uniform McDirmid bound (Lemma G.2). The Berry-Esseen bound controls the linear term in the second-order Taylor expansion of the log-likelihood function (\(T_{n}(\mathbf{\theta})\) in (96)), while the McDirmid bound controls the quadratic term (\(S_{n}(\mathbf{\theta})\) in (96)). By using an \(\varepsilon\)-net argument, the latter concentration inequality can be extended to a setting where the random variables are parameterized within a compact set.

**Theorem G.1** (Non-asymptotic consistency of weighted, constrained, and regularized MLE).: _Consider the constrained, regularized, and weighted MLE problem (83) with unknown parameter \(\mathbf{\theta}_{\star}\) from a convex subset \(\mathbf{\Theta}\subseteq\mathbb{R}^{d}\). Fix an arbitrary weighting function \(\omega(\cdot)\). Assume the following holds:_

**(a1)**: _(Smoothness) For each sample_ \((x,y)\)_, the per-sample negative log-likelihood function_ \(\mathbf{\theta}\mapsto\log\pi_{\mathbf{\theta}}(y\,|\,x)\mu(x)\) _is strictly convex, three-times continuously differentiable, and_ \(R(\mathbf{\theta})\) _is differentiable. Furthermore, denote_ \(U:=\omega(X)\nabla_{\mathbf{\theta}}\log\pi_{\mathbf{\theta}_{\star}}(Y\,|\,X)\in \mathbb{R}^{d}\) _and_ \(\overline{U}:=U-\mathbb{E}[U]\)_. Suppose there are constants_ \(D_{1},d_{1}\in(0,\infty)\) _such that for all_ \(i\)_,_

\[\mathbb{E}[\|\overline{U}\|^{3}]<D_{1},\qquad\min_{1\leq k\leq d} \text{Var}(U(k))>d_{1}. \tag{84}\]
**(a2)**: _(First-order optimality) The true parameter_ \(\mathbf{\theta}_{\star}\) _is a stationary point of the expected negative weighted log-likelihood function_ \(\overline{\mathcal{L}}_{0}(\mathbf{\theta}):=\mathbb{E}_{\mathbf{\theta}_{\star}}\left[ -\omega(X)\log\pi_{\mathbf{\theta}}(Y\,|\,X)\right]\) _over_ \(\mathbf{\Theta}\)_:_

\[\langle\nabla_{\mathbf{\theta}}\,\overline{\mathcal{L}}_{0}(\mathbf{\theta}_{\star}), \,\mathbf{\theta}-\mathbf{\theta}_{\star}\rangle\geq 0\quad\forall\mathbf{\theta}\in\mathbf{ \Theta}. \tag{85}\]
**(a3)**: _(Approximate second-order optimality) Let_ \(\bar{\mathcal{L}}(\mathbf{\theta}):=\overline{\mathcal{L}}_{0}(\mathbf{\theta})+R(\bm {\theta})\) _denote the expected regularized negative log- likelihood function. Then the regularized Fisher information_ \(\nabla^{2}\bar{\mathcal{L}}(\mathbf{\theta})\) _is positive definite at_ \(\mathbf{\theta}=\mathbf{\theta}_{\star}\) _with minimum eigenvalue_ \(\rho>0\)_._

_Fix a constant \(C>0\). Let \(D=Cn^{-1/2}+\frac{8\|\nabla R(\mathbf{\theta}_{\star})\|}{\rho}\) and \(M=M(D)>0\) denote the supremum of the absolute values of all third-order partial derivatives of \(\mathcal{L}\) over all \(\mathbf{\theta}\) with \(\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|\leq D\). Suppose \(\|\nabla R(\mathbf{\theta}_{\star})\|\) is small enough so that_

\[D\leq\frac{3\rho}{4M(D)}. \tag{86}\]

_Then there are constants \(c_{1},c_{2},c_{3}>0\) such that_

\[\mathbb{P}\left(\left\|\mathbf{\theta}_{\star}-\operatorname*{arg\, min}_{\mathbf{\theta}\in\mathbf{\Theta}}\mathcal{L}(\mathbf{\theta})\right\|\leq D\right)\geq 1 -c_{1}\exp\left(-\frac{C^{2}\rho^{2}}{2\cdot 32^{2}d}\right)-\frac{c_{2}\sum_{i=1}^{n}w_{i}^{3}}{ \left(\sum_{i=1}^{n}w_{i}^{2}\right)^{3/2}}\] \[-O(\exp(-c_{3}n)). \tag{87}\]

_That is, with high probability explicitly depending on \(C\), \(\rho\), \(d\), and \(n\), there exists a global minimizer of \(\mathbf{\theta}\mapsto\mathcal{L}(\mathbf{\theta})\) in \(\Theta\) within distance \(D\) from \(\mathbf{\theta}_{\star}\)._

It is important to notice that the minimum eigenvalue \(\rho\) of the weighted regularized Fisher information \(\nabla^{2}\overline{\mathcal{L}}(\mathbf{\theta})\) depends on the choice of weighting function \(\omega(\cdot)\). Therefore, Theorem G.1 suggests that it is best to choose the weighting function to maximize the minimum eigenvalue of the weighted regularized Fisher information in order to minimize the statistical estimation error. We elaborate more on this point in the proof of Theorem 3.5.

We devote the rest of this section to proving Theorem G.1 using Lemma G.2 and Theorem G.3.

**Lemma G.2** (A uniform McDirmid's inequality).: _Let \(\mathbf{X}_{1},\ldots,\mathbf{X}_{n}\) be independent random vectors in \(\mathbb{R}^{p}\) from a joint distribution \(\pi\). Fix a compact parameter space \(\mathbf{\Theta}\subseteq\mathbb{R}^{d}\) and \(f_{\mathbf{\theta}}:\mathbb{R}^{d}\to[-J,J]\) is a bounded functional for each \(\mathbf{\theta}\in\mathbf{\Theta}\) such that_

\[\|f_{\mathbf{\theta}}-f_{\mathbf{\theta}^{\prime}}\|_{\infty}\leq L\|\mathbf{ \theta}-\mathbf{\theta}^{\prime}\|,\qquad\forall\mathbf{\theta},\mathbf{ \theta}^{\prime}\in\mathbf{\Theta} \tag{88}\]

_for some constant \(L>0\). Further assume that \(\mathbb{E}[f_{\mathbf{\theta}}(\mathbf{X}_{k})]=0\) for all \(\mathbf{\theta}\in\mathbf{\Theta}\) and \(k=1,\ldots,n\). Then there exist constant \(J>0\) such that for each \(n\geq 0\), and \(\eta>0\),_

\[\mathbb{P}\left(\sup_{\mathbf{\theta}\in\mathbf{\Theta}}\left|\frac{1}{n}\sum _{k=1}^{n}f_{\mathbf{\theta}}(\mathbf{X}_{k})\right|\geq\eta\right)\leq\left( \frac{2L\operatorname{diam}(\mathbf{\Theta})}{\eta}\right)^{d}\exp\left(- \frac{\eta^{2}n}{2J^{2}}\right). \tag{89}\]

Proof.: Since \(\mathbf{\Theta}\subseteq\mathbb{R}^{d}\) is compact, it can be covered by a finite number of \(L_{2}\)-balls of any given radius \(\varepsilon>0\). Let \(\mathcal{U}_{\varepsilon}\) be such an open cover using the least number of balls of radius \(\varepsilon>0\). Let \(N(\varepsilon)=|\mathcal{U}_{\varepsilon}|\) denote the least number of such balls to cover \(\mathbf{\Theta}\). Moreover, let \(\operatorname{diam}(\mathbf{\Theta})\) denote the diameter of \(\mathbf{\Theta}\), which is finite since \(\mathbf{\Theta}\) is compact. Then \(\mathbf{\Theta}\) is contained in a \(d\)-dimensional box of side length \(\operatorname{diam}(\mathbf{\Theta})\). This box can be covered by \((\operatorname{diam}(\mathbf{\Theta})/\varepsilon)^{d}\) cubes of side length \(\varepsilon\). Covering each such cube of side length \(\varepsilon\) by a ball of radius \(\varepsilon\), it follows that

\[N(\varepsilon)\leq\left(\frac{\operatorname{diam}(\mathbf{\Theta})}{ \varepsilon}\right)^{d}. \tag{90}\]

Next, fix \(\eta>0\), \(\mathbf{\theta}\in\mathbf{\Theta}\), and \(\varepsilon>0\). Let \(\mathbf{\theta}_{1},\cdots,\mathbf{\theta}_{N(\varepsilon)}\) be the centers of balls in the open cover \(\mathcal{U}_{\varepsilon}\). Then there exists \(1\leq j\leq N(\varepsilon)\) such that \(\|\mathbf{\theta}-\mathbf{\theta}_{j}\|<\varepsilon\). By the hypothesis, \(f_{\mathbf{\theta}}\) depends on \(\mathbf{\theta}\) uniformly continuously with respect to the supremum norm. Hence there exists \(\delta=\delta(\varepsilon)>0\) such that

\[\|f_{\mathbf{\theta}}-f_{\mathbf{\theta}_{j}}\|_{\infty}\leq L\varepsilon. \tag{91}\]

Denote \(H_{n}(\mathbf{\theta}):=n^{-1}\sum_{k=1}^{n}f_{\mathbf{\theta}}(\mathbf{X}_{k})\). Then this yields, almost surely,

\[|H_{n}(\mathbf{\theta})-H_{n}(\mathbf{\theta}_{j})|\leq L\varepsilon. \tag{92}\]

Furthermore, by the assumption, \(\|f_{\mathbf{\theta}}\|_{\infty}\) is uniformly bounded by \(J>0\). It follows that for each \(\mathbf{\theta}\in\mathbf{\Theta}\), \(H_{n}(\mathbf{\theta})\) changes its value at most by \(M\) when one of \(\mathbf{X}_{1},\ldots,\mathbf{X}_{n}\) is replaced arbitrarily. Therefore, by the standard McDirmid's inequality (see Theorem 2.9.1. in [51]) and a union bound, with choosing \(\varepsilon=\eta/(2L)\),

\[\mathbb{P}\left(|H_{n}(\mathbf{\theta})|\geq\eta\right)\leq\sum_{j=1}^{N(\eta /2L)}\mathbb{P}\left(|H_{n}(\mathbf{\theta}_{j})|\geq\eta/2\right)\leq K\left( \frac{2L\operatorname{diam}(\mathbf{\Theta})}{\eta}\right)^{d}\exp\left(-\frac {\eta^{2}n}{2J^{2}}\right). \tag{93}\]

The above holds for all \(n\geq 1\) and \(\eta>0\). This shows the assertion. 

Next, we recall the classical Berry-Esseen theorem for the rate of convergence of normal approximation for the sum of independent random variables due to Feller.

**Theorem G.3** (Berry-Esseen, Theorem 2 in Ch. XVI.5 of Feller '91[17]).: _Let \(X_{1},X_{2},\ldots,X_{n}\) be independent and not necessarily identically distributed random variables with zero means and finite variances. Define \(W=\sum_{i=1}^{n}X_{i}\) let \(F\) be the distribution function of \(W\) and \(\Phi\) be the standard normal distribution function. Then_

\[\|F-\Phi\|_{\infty}\leq\frac{6\sum_{i=1}^{n}\mathbb{E}[|X_{i}|^{3}]}{\left( \sum_{i=1}^{n}\mathbb{E}[X_{i}^{2}]\right)^{3/2}}. \tag{94}\]

Now we prove Theorem G.1.

Proof of Theorem G.1.: Suppose we have \(n\) i.i.d. observed samples \((x_{i},y_{i})\) for \(i=1,\ldots,n\) from distribution \(\pi_{\mathbf{\theta}_{\star}}(y\,|\,x)\mu(x)\). Also by the assumption, \(\mathcal{L}_{0}\) is twice continuously differentiable and \(n^{-1}\sum_{i=1}^{n}w_{i}=1\), so \(\mathbb{E}[\nabla\mathcal{L}_{0}]=\nabla\mathbb{E}[\mathcal{L}_{0}]\) and \(\mathbb{E}[\nabla^{2}\mathcal{L}_{0}]=\nabla^{2}\mathbb{E}[\mathcal{L}_{0}]\) by the dominated convergence theorem.

Note that, for any \(r>0\),

\[\left\{\left\|\mathbf{\theta}_{\star}-\operatorname*{arg\,min}_{\mathbf{\theta}\in\mathbf{ \Theta}}\mathcal{L}(\mathbf{\theta})\right\|\leq r\right\}=\left\{\inf_{\begin{subarray} {c}\mathbf{\theta}\in\mathbf{\Theta}\\ \|\mathbf{\theta}-\mathbf{\theta}_{\star}\|=r\end{subarray}}\mathcal{L}(\mathbf{\theta})- \mathcal{L}(\mathbf{\theta}_{\star})>0\right\}. \tag{95}\]

The inclusion \(\subseteq\) is clear. Conversely, since \(\mathcal{L}\) is strictly convex, the event on the right implies that its unique global minimizer is within \(r\) from \(\mathbf{\theta}_{\star}\), as desired. Thus in order to show the main result in (87), it suffices to show that the probability of the event on the right with \(r=D=Cn^{-1/2}+\frac{8\|\nabla R(\mathbf{\theta}_{\star})\|}{\rho}\) is at least the right-hand side of (87).

Fix \(\mathbf{\theta}\in\mathbf{\Theta}\) such that \(\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|=D\). We introduce two random variables that we will bound to be small by using some concentration inequalities:

\[T_{n}(\mathbf{\theta}) :=\frac{\sqrt{n}}{\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|}\left\langle \nabla_{\mathbf{\theta}}\mathcal{L}_{0}(\mathbf{\theta}_{\star})-\mathbb{E}\left[ \omega(X)\nabla_{\mathbf{\theta}}\mathcal{L}_{0}(\mathbf{\theta}_{\star})\right],\,\mathbf{ \theta}-\mathbf{\theta}_{\star}\right\rangle, \tag{96}\] \[S_{n}(\mathbf{\theta}) :=\frac{1}{\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|^{2}}(\mathbf{\theta}- \mathbf{\theta}_{\star})^{T}\left(\nabla_{\mathbf{\theta}}\nabla_{\mathbf{\theta}^{T}} \mathcal{L}(\mathbf{\theta}_{\star})-\nabla_{\mathbf{\theta}}\nabla_{\mathbf{\theta}^{T}} \left(\mathbb{E}\left[\omega(X)\mathcal{L}(\mathbf{\theta}_{\star})\right]\right) \right)(\mathbf{\theta}-\mathbf{\theta}_{\star}). \tag{97}\]

Roughly speaking, these quantities are of order 1 with high probability, \(T_{n}\) by Central Limit Theorem and \(S_{n}\) by the law of large numbers. Later in this proof, we will use non-asymptotic versions of these classical limit theorems to obtain high probability bounds for related quantities.

Since \(\mathbf{\theta}\mapsto\mathcal{L}(\mathbf{\theta})\) is assumed to be three-time continuously differentiable, the quantity \(M\geq 0\) in the assertion is well-defined and is finite. Then, using the Taylor expansion, we may write

\[\mathcal{L}(\mathbf{\theta})-\mathcal{L}(\mathbf{\theta}_{\star})\geq \left\langle\nabla_{\mathbf{\theta}}\mathcal{L}(\mathbf{\theta}_{\star}),\,\mathbf{\theta }-\mathbf{\theta}_{\star}\right\rangle+\frac{1}{2}(\mathbf{\theta}-\mathbf{\theta}_{\star })^{T}\nabla_{\mathbf{\theta}}\nabla_{\mathbf{\theta}^{T}}\mathcal{L}(\mathbf{\theta}_{ \star})(\mathbf{\theta}-\mathbf{\theta}_{\star})\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-\frac{M( \|\mathbf{\theta}-\mathbf{\theta}_{\star}\|)}{6}\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|^{3}. \tag{98}\]

We will lower bound the first two terms on the right-hand side above. Note that

\[\left\langle\nabla_{\mathbf{\theta}}\mathcal{L}(\mathbf{\theta}_{\star}), \,\mathbf{\theta}-\mathbf{\theta}_{\star}\right\rangle =\left\langle\nabla_{\mathbf{\theta}}\mathcal{L}_{0}(\mathbf{\theta}_{ \star}),\,\mathbf{\theta}-\mathbf{\theta}_{\star}\right\rangle-\mathbb{E}\left[\omega (X)\langle\nabla_{\mathbf{\theta}}\mathcal{L}_{0}(\mathbf{\theta}_{\star}),\,\mathbf{ \theta}-\mathbf{\theta}_{\star}\right]\] \[\qquad\qquad\qquad\qquad+\left\langle\nabla_{\mathbf{\theta}} \mathbb{E}[\omega(X)\mathcal{L}_{0}(\mathbf{\theta}_{\star})],\,\mathbf{\theta}-\mathbf{ \theta}_{\star}\right\rangle+\left\langle\nabla R(\mathbf{\theta}_{\star}),\,\mathbf{ \theta}-\mathbf{\theta}_{\star}\right\rangle\] \[\overset{(a)}{\geq}\left\langle\nabla_{\mathbf{\theta}}\mathcal{L}_ {0}(\mathbf{\theta}_{\star}),\,\mathbf{\theta}-\mathbf{\theta}_{\star}\right\rangle- \mathbb{E}\left[\omega(X)\langle\nabla_{\mathbf{\theta}}\mathcal{L}_{0}(\mathbf{\theta }_{\star}),\,\mathbf{\theta}-\mathbf{\theta}_{\star}\right\rangle\right]\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-\|\nabla R(\mathbf{ \theta}_{\star})\|\cdot\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|\] \[\overset{(b)}{=}-\frac{\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|}{ \sqrt{n}}\,T_{n}(\mathbf{\theta})-\|\nabla R(\mathbf{\theta}_{\star})\|\cdot\|\mathbf{ \theta}-\mathbf{\theta}_{\star}\|,\]

where for (a) we use the fact that \(\mathbf{\theta}_{\star}\) is a stationary point of \(\mathbb{E}[\mathcal{L}_{0}(\mathbf{\theta})]\) over \(\mathbf{\Theta}\) and Cauchy-Schwarz inequality; for (b) we use the definition of \(T_{n}(\mathbf{\theta})\).

Next, we turn our attention to the second-order term in the Taylor expansion (98). Recall that from the assumption **(a3)** in Theorem G.1,

\[\mathbb{E}\left[\omega(X)\nabla_{\mathbf{\theta}}\nabla_{\mathbf{\theta}^{T}} \mathcal{L}(\mathbf{\theta}_{\star})\right]=\nabla_{\mathbf{\theta}}\nabla_{\mathbf{ \theta}^{T}}\left(\mathbb{E}\left[\omega(X)\mathcal{L}(\mathbf{\theta}_{\star}) \right]\right)\succeq\rho\mathbf{I}_{p}, \tag{99}\]

where \(\rho>0\) is a constant. It follows that

\[(\mathbf{\theta}-\mathbf{\theta}_{\star})^{T}\nabla_{\mathbf{\theta}}\nabla_{ \mathbf{\theta}^{T}}\mathcal{L}(\mathbf{\theta}_{\star})(\mathbf{\theta}-\mathbf{\theta}_{ \star}) \tag{100}\] \[\geq(\mathbf{\theta}-\mathbf{\theta}_{\star})^{T}\left[\nabla_{\mathbf{\theta }}\nabla_{\mathbf{\theta}^{T}}\mathcal{L}(\mathbf{\theta}_{\star})-\nabla_{\mathbf{\theta}} \nabla_{\mathbf{\theta}^{T}}\left(\mathbb{E}\left[\omega(X)\mathcal{L}(\mathbf{\theta}_{ \star})\right]\right)\right](\mathbf{\theta}-\mathbf{\theta}_{\star})+\rho\|\mathbf{\theta}- \mathbf{\theta}_{\star}\|^{2}\] (101) \[\geq\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|^{2}\left(S_{n}(\mathbf{\theta})+ \rho\right). \tag{102}\]Combining the above inequalities with noting that \(\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|^{2}=D^{2}\), we obtain

\[\frac{\mathcal{L}(\mathbf{\theta})-\mathcal{L}(\mathbf{\theta}_{\star})}{\| \mathbf{\theta}-\mathbf{\theta}_{\star}\|^{2}}\] \[\geq\frac{1}{\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|^{2}}\bigg{[} \langle\nabla_{\mathbf{\theta}}\mathcal{L}(\mathbf{\theta}_{\star}),\,\mathbf{\theta}- \mathbf{\theta}_{\star}\rangle+\frac{1}{2}(\mathbf{\theta}-\mathbf{\theta}_{\star})^{T} \nabla_{\mathbf{\theta}}\nabla_{\mathbf{\theta}^{T}}\mathcal{L}(\mathbf{\theta}_{\star})( \mathbf{\theta}-\mathbf{\theta}_{\star})\] \[\qquad\qquad\qquad-\frac{M(\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|)}{ 6}\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|^{3}\bigg{]}\] \[\geq-\frac{1}{\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|}\underbrace{ \bigg{[}\frac{T_{n}(\mathbf{\theta})}{\sqrt{n}}+\|\nabla R(\mathbf{\theta}_{\star})\| \bigg{]}}_{=:I_{1}}+\frac{1}{2}(S_{n}(\mathbf{\theta})+\rho)-\frac{M(\|\mathbf{\theta} -\mathbf{\theta}_{\star}\|)}{6}\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|}_{=:I_{2}}\]

Note that \(I_{1}\geq 0\) if

\[\frac{\rho}{8}D\geq\|\nabla R(\mathbf{\theta}_{\star})\|\quad\text{ and}\quad\frac{\rho}{8}D\geq\frac{M(D)}{6}D^{2}.\]

The choice of \(D\) holds the former condition, and the latter by the assumption (86). Thus, \(I_{1}\geq 0\).

We now take infimum over all \(\mathbf{\theta}\in\Theta\) such that \(\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|=D\). It suffices to show that the random variable \(I_{2}\) defined above is positive with high probability, since \(I_{1}\) does not depend on \(\mathbf{\theta}\). To this end, write

\[\inf_{\begin{subarray}{c}\mathbf{\theta}\in\Theta\\ \|\mathbf{\theta}-\mathbf{\theta}_{\star}\|=D\end{subarray}}I_{2} \tag{103}\]

Then the last expression in (103) is at least \(\rho/16\) if \(A\geq-\rho/16\) and \(B\geq\rho/8\). By the hypothesis, \(D=O(1)\) so it is uniformly bounded. Then by the uniform McDirmid's inequality in Lemma G.2, there exist constants \(C^{\prime},C^{\prime\prime}>0\) such that

\[\mathbb{P}\left(B<\frac{\rho}{8}\right)=\mathbb{P}\left(\inf_{ \begin{subarray}{c}\mathbf{\theta}\in\Theta\\ \|\mathbf{\theta}-\mathbf{\theta}_{\star}\|=D\end{subarray}}S_{n}(\mathbf{\theta})<-\frac{ \rho}{4}\right)\leq D^{d}C^{\prime}\exp(-C^{\prime\prime}n). \tag{104}\]

Next, we will show the following inequalities: For \(K=6D_{1}/d_{1}^{3/2}\),

\[\mathbb{P}\left(A<-\frac{\rho}{16}\right) =\mathbb{P}\left(\inf_{\begin{subarray}{c}\mathbf{\theta}\in\Theta\\ \|\mathbf{\theta}-\mathbf{\theta}_{\star}\|=D\end{subarray}}T_{n}(\mathbf{\theta})\geq \frac{\sqrt{n}D\rho}{16}\right)\] \[\stackrel{{(c)}}{{\leq}}d\left(\mathbb{P}\left(Z \geq\frac{\sqrt{n}D\rho}{32\sqrt{d}}\right)+\frac{K\sum_{i=1}^{n}w_{i}^{3}}{ \sqrt{n}\left(\sum_{i=1}^{n}w_{i}^{2}\right)^{3/2}}\right)\] \[\stackrel{{(d)}}{{\leq}}d\left(\frac{32\sqrt{d}}{C \rho\sqrt{2\pi}}\exp\left(-\frac{nD^{2}\rho^{2}}{2\cdot 32^{2}d}\right)+\frac{K \sum_{i=1}^{n}w_{i}^{3}}{\sqrt{n}\left(\sum_{i=1}^{n}w_{i}^{2}\right)^{3/2}}\right)\] \[\stackrel{{(e)}}{{\leq}}d\left(\frac{32\sqrt{d}}{C \rho\sqrt{2\pi}}\exp\left(-\frac{C^{2}\rho^{2}}{2\cdot 32^{2}d}\right)+\frac{K \sum_{i=1}^{n}w_{i}^{3}}{\sqrt{n}\left(\sum_{i=1}^{n}w_{i}^{2}\right)^{3/2}} \right), \tag{105}\]

where \(Z\sim N(0,1)\) is an independent standard normal random variable. (d) above is a simple consequence of the standard Gaussian tail bound \(\mathbb{P}(N(0,1)>x)\leq\frac{e^{-x^{2}/2}}{x\sqrt{2\pi}}\) and (d), (e) are from the choice of \(D\) which yields \(nD^{2}\geq C^{2}\). We will show (c) at the end of this proof. Note that for any two events \(E_{1},E_{2}\) defined on the same probability space, \(\mathbb{P}(E_{1}\cap E_{2})\geq\mathbb{P}(E_{1})+\mathbb{P}(E_{2})-1\). Thenby combining (103), (104), and (105), we have

\[\mathbb{P}\left(\inf_{\begin{subarray}{c}\mathbf{\theta}\in\mathbf{\Theta}\\ \|\mathbf{\theta}-\mathbf{\theta}_{\star}\|=D\end{subarray}}\frac{\mathcal{L}(\mathbf{ \theta})-\mathcal{L}(\mathbf{\theta}_{\star})}{\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|^ {2}}\geq\frac{\rho}{16}\right)\] \[\geq\mathbb{P}\left(A\geq-\frac{\rho}{16}\right)+\mathbb{P}\left(B \geq\frac{\rho}{8}\right)-1\] \[\geq 1-d\left(\frac{32\sqrt{d}}{C\rho\sqrt{2\pi}}\exp\left(-\frac{ C^{2}\rho^{2}}{2\cdot 32^{2}d}\right)+\frac{K\sum_{i=1}^{n}w_{i}^{3}}{\left(\sum_{i=1}^{n}w_{i}^{2} \right)^{3/2}}\right)+\left(1-D^{d}C^{\prime}\exp(-C^{\prime\prime}n)\right)-1\] \[=1-c_{1}\exp\left(-\frac{C^{2}\rho^{2}}{2\cdot 32^{2}d} \right)-\frac{c_{2}\sum_{i=1}^{n}w_{i}^{3}}{\left(\sum_{i=1}^{n}w_{i}^{2} \right)^{3/2}}-O(\exp(-c_{3}n)),\]

with the corresponding constants \(c_{1},c_{2},c_{3}>0\). Then the assertion (87) will follow using (95). It remains to verify (c) in (105). To this end, write

\[\nabla_{\mathbf{\theta}}\mathcal{L}_{0}(\mathbf{\theta}_{\star})-\mathbb{E}\left[\omega (X)\nabla_{\mathbf{\theta}}\mathcal{L}_{0}(\mathbf{\theta}_{\star})\right]=-\frac{1}{n} \sum_{i=1}^{n}w_{i}\overline{U}_{i},\]

where \(U_{i}=\omega(X_{i})\nabla_{\mathbf{\theta}}\log\pi_{\mathbf{\theta}_{\star}}(Y_{i}\,|\, X_{i})\in\mathbb{R}^{d}\) and \(\overline{U}_{i}=U_{i}-\mathbb{E}[U_{i}]\) as in the statement. Then by using Cauchy-Schwarz inequality and noting that \(\|\mathbf{\theta}-\mathbf{\theta}_{\star}\|=D\), we get

\[T_{n}(\mathbf{\theta})=\left\langle-\frac{1}{\sqrt{n}}\sum_{i=1}^{n}w_{i} \overline{U}_{i},\;\frac{\mathbf{\theta}-\mathbf{\theta}_{\star}}{D}\right\rangle\leq \left\|\frac{1}{\sqrt{n}}\sum_{i=1}^{n}w_{i}\overline{U}_{i}\right\|=\sqrt{ \sum_{k=1}^{d}\left|\frac{1}{\sqrt{n}}\sum_{i=1}^{n}w_{i}\overline{U}_{i}(k) \right|^{2}}.\]

It is important to note that the distribution of the random variable on the last term above does not depend on \(\mathbf{\theta}\). Note that \(w_{i}\overline{U}_{i}\) for \(i=1,\ldots,n\) are independent mean zero (but possibly non-identical distributions due to the weights \(w_{i}\)) random vectors in \(\mathbb{R}^{d}\), and by the hypothesis, their respective coordinates have uniformly bounded variances. Hence by a union bound,

\[\mathbb{P}\left(\inf_{\begin{subarray}{c}\mathbf{\theta}\in\mathbf{\Theta}\\ \|\mathbf{\theta}-\mathbf{\theta}_{\star}\|=D\end{subarray}}T_{n}(\mathbf{\theta})\geq t \right)\leq\sum_{k=1}^{d}\mathbb{P}\left(\left|\frac{1}{\sqrt{n}}\sum_{i=1}^{ n}w_{i}\overline{U}_{i}(k)\right|\geq\frac{t}{\sqrt{d}}\right). \tag{106}\]

Let \(Q_{n}^{k}=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}w_{i}\overline{U}_{i}(k)\). Then by the Berry-Esseen Theorem (Theorem G.3) and the hypothesis, for \(Z\sim N(0,1)\),

\[\sup_{z\in\mathbb{R}}\left|\mathbb{P}\left(Q_{n}^{k}\leq z\right) -\mathbb{P}\left(Z\leq z\right)\right| \leq\frac{6\,\sum_{i=1}^{n}w_{i}^{3}\mathbb{E}[\|\overline{U}_{i }\|^{3}]}{\left(\sum_{i=1}^{n}w_{i}^{2}\text{Var}(\overline{U}_{i}(k))\right) ^{3/2}}\] \[=\frac{6\,\mathbb{E}[\|\overline{U}_{1}\|^{3}]\sum_{i=1}^{n}w_{i} ^{3}}{\left(\text{Var}(\overline{U}_{i}(k))\right)^{3/2}\left(\sum_{i=1}^{n}w_ {i}^{2}\right)^{3/2}}\] \[\leq\frac{6D_{1}\sum_{i=1}^{n}w_{i}^{3}}{d_{1}^{3/2}\left(\sum_{ i=1}^{n}w_{i}^{2}\right)^{3/2}}\] \[=\frac{K\sum_{i=1}^{n}w_{i}^{3}}{\left(\sum_{i=1}^{n}w_{i}^{2} \right)^{3/2}}\quad\text{for }k=1,\ldots,d.\]

with \(K=6D_{1}/d_{1}^{3/2}\). Combining with (106) and using a triangle inequality, we obtain

\[\mathbb{P}\left(\inf_{\begin{subarray}{c}\mathbf{\theta}\in\mathbf{\Theta}\\ \|\mathbf{\theta}-\mathbf{\theta}_{\star}\|=D\end{subarray}}T_{n}(\mathbf{\theta})\geq t \right)\leq d\left(\mathbb{P}\left(Z\geq\frac{t}{2\sqrt{d}}\right)+\frac{K \sum_{i=1}^{n}w_{i}^{3}}{\left(\sum_{i=1}^{n}w_{i}^{2}\right)^{3/2}}\right). \tag{107}\]

Thus (c) in (105) follows. 
Proof of Theorem 3.5

In this section, we provide proof of the computational and statistical estimation guarantee stated in Theorem (3.5) under the generative ITR model (11). Two key ingredients in our proof are the computational guarantee (Theorem. E.1) and non-asymptotic estimation guarantee of constrained and regularized MLE (Theorem. G.1).

As we have briefly explained in the main text, the connection between the weighted convex optimization (6) for ITR learning and the statistical estimation problem under the generative model (11) is established by writing down the corresponding maximum likelihood estimation problem. To see this, recall that covariate-treatment pair \((\mathbf{x}_{i},a_{i})\) follows a joint distribution \(\pi\) that does not depend on the true parameter \(\mathbf{B}_{\star}\). Further, assume that \(\pi\) admits a density function \(f_{\pi}\). Then, for the continuous setting, the joint log-likelihood of observing the triple \((\mathbf{x}_{i},a_{i},y_{i})\) gives under the generative model with parameter \(\mathbf{B}\)

\[-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(y_{i}-\frac{K-1}{K} \mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i}\right)^{2}+\sum_{i=1}^{n} \log f_{\pi}(\mathbf{x}_{i},a_{i})+\text{constant}.\]

Thus, the standard (unweighted) maximum likelihood estimation of the true parameter \(\mathbf{B}_{\star}\) can be written as

\[\min_{\mathbf{B}\in\mathbb{R}^{p\times(K-1)},\,\|\mathbf{B}\|\leq \lambda_{1}}\sum_{i=1}^{n}\left(\frac{K}{K-1}y_{i}-\mathbf{u}_{a_{i}}^{T} \mathbf{B}^{T}\mathbf{x}_{i}\right)^{2},\]

where we imposed additional \(L_{1}\)-ball constraint for the parameter \(\mathbf{B}\). Now, if we use covariate-balancing weight \(w(\mathbf{x}_{i},a_{i})\) for each subject \(i\), the corresponding weighted MLE problem reads as

\[\min_{\mathbf{B}\in\mathbb{R}^{p\times(K-1)},\,\|\mathbf{B}\|\leq \lambda_{1}}\sum_{i=1}^{n}w(\mathbf{x}_{i},a_{i})\left(\frac{K}{K-1}y_{i}- \mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i}\right)^{2}, \tag{108}\]

which is exactly the weighted convex optimization problem (6) for continuous outcome. A similar discussion applies to the binary outcome simply by noting that the joint log-likelihood is given by

\[\sum_{i=1}^{n}\left(-\log\left(1+\exp(\mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T} \mathbf{x}_{i})\right)+y_{i}\mathbf{u}_{a_{i}}^{T}\mathbf{B}^{T}\mathbf{x}_{i }\right)+\sum_{i=1}^{n}\log f_{\pi}(\mathbf{x}_{i},a_{i}).\]

Proof of Theorem 3.5.: Consider the generative ITR model (11) with true parameter \(\mathbf{B}_{\star}\). Fix \(\varepsilon>0\). Let \((\widehat{\mathbf{B}}_{t})_{t\geq 0}\) denote the estimated parameters by using the PGD algorithm (7) for the weighted empirical maximum likelihood estimation problem (6). Decompose the total estimation error into computational and statistical parts as

\[\|\mathbf{B}_{\star}-\widehat{\mathbf{B}}_{T}\|_{F}\leq \underbrace{\|\widehat{\mathbf{B}}-\widehat{\mathbf{B}}_{T}\|_{F}}_{=:I_{1 }}+\underbrace{\|\widehat{\mathbf{B}}-\widehat{\mathbf{B}}_{\star}\|_{F}}_{=: I_{2}}.\]

By Theorem E.1, we have

\[I_{1}\leq\begin{cases}4\rho(\alpha)^{T}\lambda_{1}^{2}&\text{with constant stepsize}\\ \frac{\nu}{\gamma+T}&\text{with diminishing stepsize}.\end{cases}\]

We wish to apply our general result on weighted and constrained MLE in (Theorem G.1) with \(L_{2}\)-regularization \(R(\mathbf{B})=\frac{\lambda_{2}}{2}\|\mathbf{B}\|_{F}^{2}\) in order to get a high-probability bound on the term \(I_{2}\). For this, we first verify the hypothesis in Theorem G.1 for our generative ITR model (11). Assumption **(a1)** in Theorem G.1 follows from Assumption 3.4. For Assumption **(a3)** in Theorem G.1, denoting by \(\rho\) the minimum eigenvalue of the expected regularized Fisher information, by Lemma E.3 (also see (10)),

\[\rho=\mu=\begin{cases}\lambda^{-}+\lambda_{2}&\text{for continuous outcome}\\ \alpha^{-}\lambda^{-}+\lambda_{2}&\text{for binary outcome}.\end{cases}\]

where \(\lambda_{-}\) is the minimum eigenvalue of the expected weighted design matrix \(\mathbb{E}[\mathbf{\Psi}]\) in (8) (see Assumption 3.1). The above quantity is positive under the hypothesis.

Lastly, for Assumption **(a2)** in Theorem G.1, we need to show that the true parameter \(\mathbf{B}_{\star}\) is a global maximizer of the expected log-likelihood function of our generative ITR model (11). This can be shown by a standard argument under the mild assumption that expectations and derivatives are exchangeable. For simplicity, we prove this for the case when \(\mathbf{B}_{\star}\) is in the interior of the constraint set \(\mathbf{\Theta}\). (The general case can be shown similarly by considering the first-order optimality condition for stationary points.) First note that, if we denote the likelihood of observing the triple \((X,A,Y)\) under the model parameter \(\mathbf{B}\) as \(p_{\mathbf{B}}(X,A,Y)=p_{\mathbf{B}}(Y\,|\,X,A)\,p(X,A)\), then \(\log p_{\mathbf{B}}(X,A,Y)\) is a concave function in \(\mathbf{B}\). Thus we only need to show that the true parameter \(\mathbf{B}_{\star}\) is a critical point of the expected weighted log-likelihood function \(\mathbb{E}\left[\omega(A,X)\log p_{\mathbf{B}}(X,A,Y)\right]\). Indeed,

\[\nabla_{\mathbf{B}}\mathbb{E} \left[\omega(A,X)\log p_{\mathbf{B}}(X,A,Y)\right]\] \[=\mathbb{E}\left[\omega(A,X)\nabla_{\mathbf{B}}\left(\log p_{ \mathbf{B}}(Y\,|\,X,A)+\log p(X,A)\right)\right]\] \[=\mathbb{E}\left[\omega(A,X)\nabla_{\mathbf{B}}\log p_{\mathbf{B} }(Y\,|\,X,A)\right]\] \[=\mathbb{E}_{X}\left[\mathbb{E}_{(Y,A)}\left[\omega(A,X)\frac{ \nabla_{\mathbf{B}}p_{\mathbf{B}}(Y|X,A)}{p_{\mathbf{B}}(Y\,|\,X,A)}\,\Big{|} \,X\right]\right]\] \[=\mathbb{E}_{X}\left[\sum_{a=0}^{K}\omega(a,X)\pi(a,X)\,\mathbb{E} _{Y\sim p_{\mathbf{B}_{\star}}(\,\cdot\,|\,X,a)}\left[\frac{\nabla_{\mathbf{B}} p_{\mathbf{B}}(Y|X,A)}{p_{\mathbf{B}}(Y\,|\,X,A)}\,\Big{|}\,X,A=a\right]\right],\]

where we have used iterated expectation to condition first on the covariate \(X\) and then integrate out the treatment-response pair \((A,Y)\). Now note that

\[\mathbb{E}_{Y\sim p_{\mathbf{B}_{\star}}(\,\cdot\,|\,X,a)}\left[ \frac{\nabla_{\mathbf{B}}p_{\mathbf{B}_{\star}}(Y|X,A)}{p_{\mathbf{B}_{\star} }(Y\,|\,X,A)}\,\Big{|}\,X,A=a\right] =\int\nabla_{\mathbf{B}}p_{\mathbf{B}_{\star}}(Y|X,A)\,dy \tag{109}\] \[=\nabla_{\mathbf{B}}\int p_{\mathbf{B}_{\star}}(Y|X,A)\,dy\] (110) \[=\nabla_{\mathbf{B}}1=0. \tag{111}\]

Thus \(\mathbf{B}_{\star}\) is a critical point of \(\mathbb{E}\left[\log p_{\mathbf{B}}(X,A,Y)\right]\). By the concavity of the log-likelihood function, it follows that \(\mathbf{B}_{\star}\) is a global maximizer of \(\mathbb{E}\left[\omega(A,X)\log p_{\mathbf{B}}(X,A,Y)\right]\). In particular, this verifies the assumption G.1 **(a2)**.

Now we can apply Theorem G.1 with \(L_{2}\)-regularization \(R(\mathbf{B})=\frac{\lambda_{2}}{2}\|\mathbf{B}\|_{F}^{2}\) for our generative ITR model. Denoting

\[D:=\frac{C}{\sqrt{n}}+\frac{8\|\nabla R(\mathbf{B}_{\star})\|_{F}}{\mu}=\frac {C}{\sqrt{n}}+\frac{8\lambda_{2}\|\mathbf{B}_{\star}\|_{F}}{\mu} \tag{112}\]

for \(C>0\) any constant, this gives

\[\mathbb{P}\left(I_{2}>D\right)\leq c_{1}\exp\left(-\frac{C^{2}\mu^{2}}{2\cdot 3 2^{2}d}\right)+\frac{c_{2}\sum_{i=1}^{n}w_{i}^{3}}{\left(\sum_{i=1}^{n}w_{i}^{ 2}\right)^{3/2}}+O(\exp(-c_{3}n)),\]

where \(c_{1},c_{2},c_{3}>0\) are constants and \(d=p(K-1)\). By Assumption 3.4, the quantities \(\sqrt{n}\sum_{i=1}^{n}w_{i}^{3}/\left(\sum_{i=1}^{n}w_{i}^{2}\right)^{3/2}\) are uniformly bounded in \(n\). Hence we can first choose \(n\) large enough so that the last two terms combined in the right-hand side in (113) are at most \(\varepsilon/2\), for which \(n\gtrsim\varepsilon^{-2}\) is sufficient. Then we can choose \(C>0\) large enough so that the first term on the right-hand side of (113) is at most \(\varepsilon/2\). For this, we choose

\[C=\frac{32\sqrt{2d}}{\mu}\sqrt{\log\frac{2c_{1}}{\varepsilon}}=\Omega\left( \frac{\sqrt{p\log\varepsilon^{-1}}}{\mu}\right)\]

Accordingly, we can choose \(T\) large enough so that \(I_{1}\leq C/\sqrt{n}\). Then the above gives

\[\mathbb{P}\left(\|\mathbf{B}_{\star}-\widehat{\mathbf{B}}_{T}\|_{F}>\frac{2C}{ \sqrt{n}}+\frac{8\lambda_{2}\|\mathbf{B}_{\star}\|_{F}}{\mu}\right)\leq\mathbb{ P}\left(I_{2}\geq D\right)\leq\varepsilon. \tag{113}\]

This shows the assertion.

Additional simulation details and results

The code supporting this study is available at [https://github.com/ljw9510/effective-ITR](https://github.com/ljw9510/effective-ITR), with plans for release as an R package soon.

### Details of data-generating mechanism

In this section, we explain how data are generated and treatment assignments are allocated in our simulation studies. The outcome is generated under the working model assumption given by Equation 1 with heteroscedastic random noise \(\epsilon\sim N(0,\sigma^{2}(A,\mathbf{X}))\). We have four different settings/cases for the treatment-free effect and interaction effect functions, which are outlined below.

#### Case 1: Linear interaction functions with simple treatment-free effect functions

For Case 1, our randomized study aligns with Scenario 7 and our observational study mirrors Scenario 8 described in [44]. Specifically, we consider the following settings for the treatment-free effects and interaction effects:

\[\begin{split}&\text{Randomized trial:}\\ &\mu(\mathbf{X})=1+2X_{1}+2X_{2},\\ &\delta(\mathbf{X})=\begin{cases}0.75+1.5X_{1}+1.5X_{2}+1.5X_{3}+1. 5X_{4},A=1;\\ 0.75+1.5X_{1}-1.5X_{2}-1.5X_{3}+1.5X_{4},A=2;\\ 0.75+1.5X_{1}-1.5X_{2}+1.5X_{3}-1.5X_{4},A=3;\\ 0.75-1.5X_{1}+1.5X_{2}-1.5X_{3}-1.5X_{4},A=4,\end{cases}\\ &\text{Observational study:}\\ &\mu(\mathbf{X})=1+X_{5}+3X_{6}+2X_{1}X_{2},\\ &\delta(\mathbf{X})=\begin{cases}0.5+2X_{1}+X_{2}+X_{3},A=1;\quad 1+X_{1}-X_{2}-X_{3},A=2;\\ 1.5+3X_{1}-X_{2}+X_{3},A=3;\quad 1-X_{1}-X_{2}+X_{3},A=4,\end{cases}\end{split}\]

#### Case 2: Linear interaction functions with complicated treatment-free effect functions

For Case 2, we consider the following treatment-free effect function for model assumptions of \(Y\):

\[\mu(\mathbf{X})=1+X_{5}+X_{5}^{2}+2e^{-X_{1}X_{2}}+\sin(X_{3}).\]

#### Case 3: Non-linear interaction functions with simple treatment-free effect functions

For Case 3, we consider the following non-linear interaction functions for model assumptions of \(Y\):

\[\begin{split}&\mu(\mathbf{X})=1+2X_{1}+2X_{2}+2X_{4}-2X_{4}^{2}+2X_{1 }X_{2},\\ &\delta(\mathbf{X})=\begin{cases}0.5+1.0X_{1}-2.0X_{4}+0.5X_{4}^{2 },A=1;\ 1.0+1.0X_{1}+1.0X_{4}-1.0X_{4}^{2},A=2;\\ 1.5+2.0X_{1}-1.0X_{4}-1.0X_{4}^{2},A=3;\ 1.0-1.0X_{1}-1.0X_{4}-1.0X_{4}^{2},A=4. \end{cases}\end{split}\]

#### Case 4: Non-linear interaction functions with a complicated treatment-free effect function

For Case 4, we consider the following complicated treatment-free effect function for model assumptions of \(Y\):

\[\mu(\mathbf{X})=1+2X_{1}+2X_{2}+2X_{4}-2X_{4}^{2}+2X_{1}X_{2}+2e^{-X_{1}X_{2}} +\sin(X_{3}).\]

In all four cases, we adopt the same treatment assignment principles and variance function \(\sigma^{2}(A,\mathbf{X})\) that induces heteroscedastic error for observational studies as outlined in [44]:

\[\begin{split}&\pi(A,\mathbf{X})=\begin{cases}0.25\cdot\mathbf{I}(X_{1} <0)+0.4\cdot\mathbf{I}(X_{1}>0),A=1,\\ 0.25\cdot\mathbf{I}(X_{1}<0)+0.2\cdot\mathbf{I}(X_{1}>0),A=2,3,4.\end{cases}\\ &\sigma^{2}(A,\mathbf{X})=0.25+2X_{2}\cdot\mathbf{I}(X_{2}>0)+X_{3} \cdot\mathbf{I}(X_{3}>0,A=1)+X_{4}\cdot\mathbf{I}(X_{4}>0,A=2)\,.\end{split}\]For randomized trials, each individual has an equal probability of assigning each treatment, therefore, no propensity score estimation was made for AD-learning and SABD-learning. For the heteroscedastic error for randomized trials, \(\sigma^{2}(A,\mathbf{X})=0.25+0.2\left(1.5-X_{2}\right)^{2}\) was used.

### Implementation details

All numerical experiments were performed on a 2022 Macbook Air with M1 chip and 16 GB of RAM.

In the computation of the final ITR estimates, we use two different approaches: one with weighted multivariate regression with \(L_{1}\)-regularization for the penalized approach (named 'penalized' in the main text). The other uses the proposed PGD algorithm with \(L_{1}\)-ball (named 'constrained' in the main text). To ensure a fair comparison, both the penalized method and the constrained optimization algorithm were implemented using code developed by the author. We do not consider additional \(L_{2}\)-regularization in this setting. To determine the regularization constant or \(L_{1}\)-ball size, we use mean squared error as the criterion. The iterate number \(T\) of the PGD algorithm is 1000.

In our simulation study, we employ the default configuration of the random forest algorithm as implemented in the randomForest package in R to estimate the residual variance function in SABD-Learning, the treatment-free effect, and the propensity score. This default configuration includes the construction of an ensemble of 500 decision trees, with each tree's splitting criterion determined using the Gini index. The number of variables considered for splitting at each node is set to the square root of the total number of covariates. For model performance evaluation, we utilize the _out-of-bag_ (OOB) error estimate.

### Additional simulation results

The goal of these additional simulations in the appendix is to demonstrate, under the pre-specified linear ITR, that DCBW, variable screening, and augmentation can synergistically benefit the final ITR. Methods using energy balancing weights are denoted as "_e" and those employing the distance covariance test for variable screening are indicated as "_s" in the table. Note that the augmented "SABD_e_s" corresponds to " proposed+penalized" in the main text. In this section, we include the performance of various methods implemented only by the penalized approach to compare the effectiveness of the proposed statistical methods. The simulation was implemented using package _glmnet_ in R.

#### i.3.1 Treatment Decision Accuracy

Here, we show the averages and standard deviations of treatment decision accuracy from the additional simulation studies. The first four tables present the results of experiments that mimic randomized trials, and the remaining four tables show the results of observational studies.

The simulation results indicate that individual components alone do not significantly improve performance. Instead, the synergistic effect of combining these components is essential for the ITR-Learning. For example, in high-dimensional settings, using DCBW alone balances the empirical distributions of all covariates (up to 60), including a significant portion of irrelevant ones. Additionally, estimated decision functions may include irrelevant variables, reducing the impact of DCBW. Similarly, using variable screening alone remains challenging due to model misspecification or highly nonlinear outcomes. Combining other methods, such as outcome augmentation, helps mitigate model misspecification effects. This combined application ensures efficient and robust optimal ITR estimation across diverse scenarios.

Below is a detailed investigation of each scenario.

First, we examine Case 1, where the treatment rule class is correctly specified (Tables 1, 5). With a relatively simple treatment-free effect, AD-Learning's performance is comparable to SABD-Learning, particularly following outcome augmentation. When combined with EBWs as AD_e, SABD_e, these methods exhibit improved performance over their original versions (AD, SABD). In randomized trials, EBWs effectively reduce finite sample imbalance compared to IPW with true propensity scores. In observational studies, energy balancing yields more effective balancing weights, resulting in higher accuracy.

Next, we delve into Case 2, involving a more complicated treatment-free effect function compared to Case 1 (Tables 2, 6). Notably, since the treatment-free effect does not affect ITR learning, a misspecified treatment-free effect function introduces heterogeneity and increases noise variance. Then we can see the synergistic effectiveness of outcome augmentation and inverse variance weighting in SABD-Learning, effectively handling additional heterogeneity in ITR estimation (See "augmented" panel in Tables 2, 6). However, it's important to note that all variants of AD-Learning show subpar performance, even with augmented outcomes. AD-Learning results in an intercept-only model frequently as its estimated decision function, which suggests that the mean squared error criterion in LASSO regularization may not be effective for selecting models. In such instances, different penalty types or performance metrics could prove beneficial.

Case 3 and Case 4 handle situations where the underlying treatment rule class is misspecified (Tables 3, 4, 7, and 8). Particularly, in Case 4, estimating ITR becomes even more challenging due to a highly non-linear covariate-outcome relationship, which leads to suboptimal results of the variants of AD-Learning as in Case 2. Our results demonstrate that the more robust EBWs outperform standard IPW, particularly in observational studies. Additionally, variable screening enhances ITR learning in a high-dimensional setting. Employing variable screening (SABD_s) enhances the performance of SABD, especially with limited sample sizes, as it aids in selecting effect modifiers and precision variables, thereby facilitating decision function estimation. However, even with perfect screening, estimating the decision function remains challenging due to model misspecification or highly nonlinear outcomes. Simultaneous use of EBWs and screening for SABD (SABD_s) further elevates performance, likely influenced by dimensionality in nonparametric balancing weight methods.

The significant enhancements from integrating DCBWs, variable screening, outcome augmentation, and inverse variance weighting underscore their collective importance. Their combined application ensures efficient and robust optimal ITR estimation across diverse scenarios.

[MISSING_PAGE_FAIL:43]

[MISSING_PAGE_FAIL:44]

[MISSING_PAGE_FAIL:45]

[MISSING_PAGE_FAIL:46]

[MISSING_PAGE_FAIL:47]

[MISSING_PAGE_FAIL:48]

[MISSING_PAGE_FAIL:49]

[MISSING_PAGE_FAIL:50]

#### 1.3.2 Empirical values

In this subsection, we show the averages and standard deviations of empirical values from the additional simulation studies. The empirical values of the estimated rule \(\mathbb{E}[Y(\hat{d}(\mathbf{X}))]\) are computed by utilizing the generated potential outcomes on the test dataset.

We include the empirical value of the optimal rule \(\mathbb{E}[Y(d^{\text{opt}}(\mathbf{X}))]\) as well as the values obtained by assigning everyone to each of the four classes \(\mathbb{E}[Y(K)]\) for \(K=1,\ldots,4\) as benchmarks, assuming the higher value of empirical value is better. Again, the first four tables present the results of experiments that mimic randomized trials, and the remaining four tables show the results of observational studies.

Here, we can observe the benefits of the three components that we found from the treatment decision accuracy results. In addition, the benchmarks of the empirical values reveal some intriguing insights. Firstly, the optimal empirical value provides information on how challenging the current scenario is. For instance, in the randomized trial of Case 1 (Table 9), the optimal empirical value is not significantly different from the mean empirical value evaluated by current methods. Especially with augmented outcomes using a sample size of 1000, it yields a value quite similar to the optimal. On the other hand, in the observational study of Case 4 (Table 16), the obtained empirical values show significant differences from the optimal value. Furthermore, the mean empirical value evaluated from AD-Learning with a sample size of 200 is not markedly different from the empirical value obtained by assigning all individuals to treatment 1. This implies that the estimated ITR provides results akin to a "one-size-fits-all" outcome in small sample sizes, and this finding is not irrelevant to the fact that a majority of AD-Learning results in an intercept-only model as the final decision function under conditions of highly non-linear covariate-outcome relationships. In such instances, we can explore alternative methods to further advance our current results such as other penalty types or performance metrics.

[MISSING_PAGE_EMPTY:52]

[MISSING_PAGE_EMPTY:53]

[MISSING_PAGE_FAIL:54]

\begin{table}
\begin{tabular}{l c c c c c c c c c}  & & \(AD\) & \(AD\_e\) & \(AD\_s\) & \(AD\_e,s\) & \(SABD\) & \(SABD\_e\) & \(SABD\_s\) & \(SABD\_e,s\) \\ \hline n & p & & & & & & Original & & & \\ \hline  & 20 & 3.979 & 4.033 & 4.058 & 4.052 & 4.192 & 4.132 & 4.381 & 4.366 \\  & & (0.55) & (0.53) & (0.57) & (0.58) & (0.58) & (0.61) & (0.58) & (0.65) \\
200 &

[MISSING_PAGE_FAIL:56]

\begin{table}
\begin{tabular}{l c c c c c c c c c}  & & \(AD\) & \(AD\_e\) & \(AD\_s\) & \(AD\_e,s\) & \(SABD\) & \(SABD\_e\) & \(SABD\_s\) & \(SABD\_e,s\) \\ \hline n & p & & & & & & & Original & & \\ \hline  & 20 & 7.592 & 7.598 & 7.608 & 7.696 & 7.561 & 7.736 & 7.715 & 7.927 \\  & & (0.73) & (0.74) & (0.75) & (0.78) & (0.67) & (0.80) & (0.72) & (0.87) \\
200 & 40 & 7.462 & 7.452 & 7.540 & 7.554 & 7.46

[MISSING_PAGE_FAIL:58]

[MISSING_PAGE_EMPTY:59]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Our paper discusses the limitations of the work in the section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide the full set of assumptions in the main text and complete proof in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our paper fully discloses all the information needed to reproduce all simulation results and data applications of the paper. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide open access to the code, with sufficient instructions to faithfully reproduce the main experimental results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify all the training and test details including data splits, hyperparameters, how they were chosen, and type of optimizer for the benchmark. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the means and standard deviations with 100 (10) iterations in our simulation studies (applications), respectively. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide sufficient information on the computer resources in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We strictly follow the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss both potential positive societal impacts and negative societal impacts of our work in the section 6. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the original papers and include the URL for the webpage. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We introduce our proposed with sufficient details in the paper. The documentation is provided alongside our code. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We used open-source datasets and our paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: our paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.