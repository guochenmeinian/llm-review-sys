# Meta-Learning with Neural Bandit Scheduler

 Yunzhe Qi

University of Illinois at Urbana-Champaign

Champaign, IL

yunzheq2@illinois.edu

&Yikun Ban

University of Illinois at Urbana-Champaign

Champaign, IL

yikunb2@illinois.edu

&Tianxin Wei

University of Illinois at Urbana-Champaign

Champaign, IL

twei10@illinois.edu

&Jiaru Zou

University of Illinois at Urbana-Champaign

Champaign, IL

jiaruz2@illinois.edu

&Huaxiu Yao

University of North Carolina at Chapel Hill

Chapel Hill, NC

huaxiu@cs.unc.edu

&Jingrui He

University of Illinois at Urbana-Champaign

Champaign, IL

jingrui@illinois.edu

Equal Contribution.

###### Abstract

Meta-learning has been proven an effective learning paradigm for training machine learning models with good generalization ability. Apart from the common practice of uniformly sampling the meta-training tasks, existing methods working on task scheduling strategies are mainly based on pre-defined sampling protocols or the assumed task-model correlations, and greedily make scheduling decisions, which can lead to sub-optimal performance bottlenecks of the meta-model. In this paper, we propose a novel task scheduling framework under Contextual Bandits settings, named BASS, which directly optimizes the task scheduling strategy based on the status of the meta-model. By balancing the exploitation and exploration in meta-learning task scheduling, BASS can help tackle the challenge of limited knowledge about the task distribution during the early stage of meta-training, while simultaneously exploring potential benefits for forthcoming meta-training iterations through an adaptive exploration strategy. Theoretical analysis and extensive experiments are presented to show the effectiveness of our proposed framework.

## 1 Introduction

Meta-learning algorithms [19] have been receiving increased attention due to their strong generalization performance across a wide range of tasks [32, 20]. Most existing meta-learning methods often assume a uniform distribution when drawing meta-training tasks, treating each task as equally important [19]. However, this assumption can possibly fail in real-world scenarios. For example, during data collection, candidate training tasks can be subject to noise perturbation, leading to performance bottlenecks in the meta-model if noisy and clean tasks are treated on an equal footing [43, 45]. In addition, some tasks can pose greater challenges for the meta-model to adapt to, necessitating a more flexible allocation of computational resources during the meta-training process. Furthermore, the task distribution may be skewed, with "tail" tasks receiving inadequate attention under uniform sampling. As a result, the task scheduling methods [14, 51, 31, 24, 29] have been proposed for a refined meta-training strategy.

Existing scheduling approaches mainly aim to improve meta-training strategies based on various pre-defined criteria and assumptions, including both non-adaptive and adaptive methods. Among them, non-adaptive methods working on the gradient that updates trainable parameters [31; 13] have proven their superiority over meta-training methods with uniform sampling. But they are unable to adjust the task scheduling strategy adaptively based on the status of the meta-model. On the other hand, adaptive methods aim to schedule the tasks based on Curriculum Learning [14; 52; 50] or adaptively sample the tasks based on the task adaptation difficulty (loss) [51]. However, the existing approaches are all greedy algorithms, which means that they tend to make locally optimal decisions based on the current knowledge (i.e., exploitation only). As the learner only has limited knowledge regarding the task and data distribution at the early stage of meta-training, the greedy strategy can lead to the sub-optimal meta-model due to multiple reasons, such as being misled by the noisy tasks or affected by the skewness of the task distribution. Here, we use Figure 2 to illustrate an example where the greedy approach (only exploitation) may be trapped in sub-optimal solutions.

One intuitive solution for the aforementioned problems is tackling the exploitation-exploration dilemma [4] in meta-training process: balancing the exploitation of current knowledge of selected tasks, and the exploration of under-explored tasks for potential long-term benefits. Therefore, unlike existing approaches with greedy strategies, we propose a novel task scheduling framework under the Contextual Bandits settings [15; 28; 4; 1; 54; 53; 2], named **BA**ndit **TaSk **S**cheduler (BASS). In each meta-training iteration, we formulate each candidate meta-training task as an arm under the contextual bandit settings, and the corresponding arm reward will naturally be the meta-model generalization ability score after including this candidate task (arm) into the meta-training process. To achieve this objective, BASS directly learns the mapping from the meta-parameters to the meta-model generalization ability score, instead of depending on the hand-crafted criteria or assumptions. This design also enables us to update meta-parameters and BASS simultaneously within one round of meta-optimization. In particular, instead of greedily scheduling the meta-training tasks solely based on the current knowledge (i.e., exploitation), BASS leverages an additional adaptive exploration module with two different exploration objectives to explore for unrevealed benefits. Our main contributions can be summarized as follows:

* **[Problem and Proposed Framework]** We are the first to formulate the meta-learning task scheduling problem under the Contextual Bandits settings, where we optimize the meta-model w.r.t. chosen task batches in each meta-training iteration. To tackle this problem, we propose a novel bandit-based meta-learning task scheduling framework named BASS, which is model-agnostic and can be applied to various meta-learning frameworks. Different from existing works, BASS directly learns the relationship between the meta-model parameters and the meta-model generalization ability. In addition, instead of greedily exploiting the current knowledge as in existing works, BASS utilizes a novel exploration module to adaptively plan for the future meta-training iterations.
* **[Theoretical Analysis]** Under the general meta-learning settings as well as standard assumptions of over-parameterized neural networks and neural bandits, we derive the theoretical performance guarantee for the proposed BASS framework in terms of regret bound.
* **[Experiments]** To demonstrate the effectiveness of BASS, we compare our method against seven strong baselines on three real data sets, with different specifications. In addition, complementary experiments as well as a case study on Ensemble Inference are also provided to better understand the property and behavior of BASS.

## 2 Related Works

In this section, we briefly review the related works of our proposed BASS framework from the aspects of task scheduling in meta-learning and contextual Bandits.

Figure 1: Exploitation and exploration in meta-training iteration \(k\in[K]\). E.g., tasks 1, 2, 3 are “freequent” tasks in a skewed task distribution, and tasks 4, 5, 6 are from task distribution “tail”. Exploring the “tail” tasks can help improve the meta-model generalization performance (Subsec. 6.2).

Task Scheduling in Meta-Learning.By considering each meta-training task to be equally important, many existing works sample the training tasks uniformly from the given task distribution [19; 40]. Then, with fixed sampling strategies, [24; 35] propose to assign the task sampling probability based on the quantity of task information, and [31] utilizes a probabilistic sampling method based on class-pairs. Meanwhile, there are also works try to improve task-specific gradients for the randomly sampled tasks [13; 34]. On the other hand, Curriculum-based approaches [14; 52; 50] schedule the tasks based on the difficulty of task adaptation, and [51] propose to adaptively schedule the tasks based on the assumed correlation between the difficulty of meta-training tasks and the meta-model generalization ability. However, the existing works are all greedy approaches that solely focus on the instant benefits, which can lead to sub-optimal meta-models due to the potential performance bottleneck. Compared with existing works, our proposed BASS directly learns the mapping from the meta-parameters to the generalization loss with no pre-defined criteria. With the adaptive exploration strategy, our proposed BASS helps tackle the insufficient knowledge regarding the task distribution by balancing the exploitation and exploration, as well as focusing on the long-term effects.

Contextual Bandits.Contextual bandits algorithms aim to solve the sequential decision making problem under the online learning settings, such as online recommendation [28; 49; 6]. Assuming that the mapping from arm contexts to rewards is linear, linear upper confidence bound (UCB) algorithms [15; 28; 4] are proposed to solve the exploitation-exploration dilemma. After kernel-based methods [46; 17] being applied to deal with the non-linear setting where the reward mapping is assumed to be a function in Reproducing Kernel Hilbert Space (RKHS), neural bandit algorithms [54; 53; 7; 5; 39; 8] are proposed to utilize neural networks for the reward and confidence bound estimation. In particular, neural bandit algorithms have demonstrated their superior performance over linear and kernel-based algorithms [54], thanks to the representation power of neural networks. Moreover, instead of using non-negative UCB, EE-Net [9; 10] achieves adaptive exploration by adopting an additional neural network to estimate the potential gain of reward estimations. In this paper, we model the task scheduling problem under the Contextual Bandit settings to balance the exploitation and exploration dilemma regarding the meta-task scheduling.

## 3 Problem Definition and Learning Objective

Under the settings of few-shot supervised meta-learning [19], our goal is to train a meta-model \(\mathcal{F}(\cdot;\mathbf{\Theta})\) with parameters \(\mathbf{\Theta}\in\mathbb{R}^{p}\) that can generalize well to meta-testing tasks, where \(p\) is the number of trainable meta-parameters. The meta-model parameters are initialized as \(\mathbf{\Theta}^{(0)}\). Note that in this work, the trainable parameters are all represented by column vectors, for the simplicity of notation. Following similar settings in [19; 51], in each training iteration \(k\in[K]\), we will receive a pool of candidate training tasks \(\Omega^{(k)}_{\text{task}}=\{\mathcal{T}_{k,i}\}_{i\in\mathcal{N}}\) where its cardinality \(|\Omega^{(k)}_{\text{task}}|=N_{\text{task}}\) and the index set \(\mathcal{N}=\{1,\dots,N_{\text{task}}\}\). Given a task distribution \(\mathcal{P}(\mathcal{T})\), we draw each candidate task \(\mathcal{T}_{k,i}\) from \(\mathcal{P}(\mathcal{T})\) to form the candidate pool, i.e., \(\mathcal{T}_{k,i}\sim\mathcal{P}(\mathcal{T})\). Each task \(\mathcal{T}_{k,i}\) is also associated with a support data set \(D^{s}_{k,i}\) and a query data set \(D^{\prime}_{k,i}\), where the samples (including their labels) of \(D^{s}_{k,i},D^{\prime}_{k,i}\) are drawn from the corresponding task data distribution \(\mathcal{D}_{\mathcal{T}_{k,i}}\) of task \(\mathcal{T}_{k,i}\).

Meta-training Process.Then, we need to choose a batch of \(B\) tasks \(\Omega_{k}=\{\mathcal{T}_{k,\widehat{i}}\}_{\widehat{i}\in\widehat{\mathcal{N }}_{k}}\subset\Omega^{(k)}_{\text{task}}\) for each meta-training iteration \(k\in[K]\), where \(\widehat{\mathcal{N}}_{k}\subset\mathcal{N}\) refers to the indices of chosen tasks. With \(\mathbf{\Theta}^{(k-1)}\) being the meta-model parameters after completing the \((k-1)\)-th meta-training iteration, for each **chosen** meta-training task \(\mathcal{T}_{k,\widehat{i}},\widehat{i}\in\widehat{\mathcal{N}}_{k}\), we run Gradient Descent (GD) for \(J\) steps on its support set \(D^{s}_{k,\widehat{i}}\) to obtain the task-specific parameters \(\mathbf{\Theta}^{(J)}_{k,\widehat{i}}\). This refers to the _inner-loop optimization_. In this work, we consider a loss function \(\mathcal{L}(\cdot;\cdot)\) that maps the meta-model parameters and the input sample (or sample set) to the loss value, with the range \([0,1]\) (e.g., the MSE loss on normalized user ratings [27]). Denoting \(\mathbf{\Theta}^{(0)}_{k,\widehat{i}}=\mathbf{\Theta}^{(k-1)}\) in meta-training iteration \(k\), each GD step is represented as

\[\mathbf{\Theta}^{(j)}_{k,\widehat{i}}=\mathbf{\Theta}^{(j-1)}_{k,\widehat{i}} -\eta_{1}\cdot\nabla_{\mathbf{\Theta}}\mathcal{L}(D^{s}_{k,\widehat{i}}; \mathbf{\Theta}^{(j-1)}_{k,\widehat{i}}),\quad\forall j\in[J]\] (1)

where \(\eta_{1}\in\mathbb{R}^{+}\) is the learning rate of GD. For the simplicity of notation, we formulate the above \(J\)-iteration _inner-loop optimization_ as an operator \(\mathcal{I}(\cdot,\cdot):\mathcal{T}\times\mathbf{\Theta}\mapsto\mathbf{\Theta}\), such that

\[\mathbf{\Theta}^{(J)}_{k,\widehat{i}}=\mathcal{I}(\mathcal{T}_{k,\widehat{i}} \,,\,\mathbf{\Theta}^{(k-1)}).\] (2)Then, we optimize the meta-parameters through the _outer-loop optimization_ with query sets

\[\bm{\Theta}^{(k)}=\ \bm{\Theta}^{(k-1)}[\Omega_{k}]=\ \bm{\Theta}^{(k-1)}-\eta_{2} \cdot\nabla_{\bm{\Theta}}\big{(}\frac{1}{|\Omega_{k}|}\sum_{\tau_{k,\bar{i}} \in\Omega_{k}}\mathcal{L}(D_{k,\bar{i}}^{q};\bm{\Theta}_{k,\bar{i}}^{(J)})\big{)}\] (3)

where \(\eta_{2}\in\mathbb{R}^{+}\) is the learning rate. Here, the trained meta-model parameters \(\bm{\Theta}^{(K)}\) is expected to minimize the generalization loss \(\mathbb{E}_{\mathcal{T}\sim\mathcal{P}(\mathcal{T}),\bm{x}\sim\mathcal{D}_{ \mathcal{T}}}\big{[}\mathcal{L}\big{(}\bm{x};\mathcal{I}(\mathcal{T},\bm{ \Theta}^{(K)})\big{)}\big{]},\) where we let \(\mathcal{D}_{\mathcal{T}}\) being the associated data distribution of task \(\mathcal{T}\).

**Evaluating Task Batch Benefits.** Recall that in meta-training iteration \(k\in[K]\), the meta-model will be updated based on the chosen batch of \(B\) tasks (**Eq. 3**), and we will need to evaluate the benefit in terms of the whole task batch. By the task scheduling problem definition, the learner will _select one task batch_\(\Omega_{k}\subset\Omega_{\text{task}}^{(k)}\) based on its strategy, and the chosen task batch \(\Omega_{k}\) will be used for meta-training in this iteration \(k\). Here, we define the reward of the corresponding task batch \(\Omega_{k}\) as

\[h\big{(}\bm{\Theta}^{(k-1)}[\Omega_{k}]\big{)}=1-\mathbb{E}_{\mathcal{T}\sim \mathcal{P}(\mathcal{T}),\bm{x}\sim\mathcal{D}_{\mathcal{T}}}\big{[}\mathcal{ L}\big{(}\bm{x};\mathcal{I}(\mathcal{T},\bm{\Theta}^{(k-1)}[\Omega_{k}]) \big{)}\big{]}\] (4)

where \(\bm{\Theta}^{(k-1)}[\Omega_{k}]\) refer to the meta-parameters after adapting \(\bm{\Theta}^{(k-1)}\) to task batch \(\Omega_{k}\) based on **Eq. 3**. For simplicity, we let \(h:\mathbb{R}^{p}\mapsto\mathbb{R}\) be the mapping function conditioned on distribution \(\mathcal{P}(\mathcal{T})\), which maps the trained meta-model parameters \(\bm{\Theta}^{(k-1)}[\Omega_{k}]\) to task batch rewards.

**Learning Objective.** Up to meta-training iteration \(k\), we have \(\Omega^{*}(K)=\{\Omega_{1}^{*},\dots,\Omega_{K}^{*}\}\) being a series of unknown _optimal_ task batches that minimizes the generalization loss. Each optimal batch is \(\Omega_{k}^{*}=\{\mathcal{T}_{k,i^{*}}\}_{i^{*}\in\mathcal{N}_{k}^{*}}\subset \Omega_{\text{task}}^{(k)},k\in[K]\) including a total of \(|\Omega_{k}^{*}|=B\) tasks, with the indices \(\mathcal{N}_{k}^{*}\subset\mathcal{N}\). The corresponding optimal meta-parameters are defined as \(\bm{\Theta}^{(K),*}=\arg\inf_{\bm{\Theta}\in\mathcal{W}}\Big{[}\mathbb{E}_{ \mathcal{T}\sim\mathcal{P}(\mathcal{T}),\bm{x}\sim\mathcal{D}_{\mathcal{T}}} \big{[}\mathcal{L}\big{(}\bm{x};\mathcal{I}(\mathcal{T},\bm{\Theta})\big{)} \big{)}\Big{]}\Big{]},\) where \(\mathcal{W}\subset\mathbb{R}^{p}\) refers to the reachable parameter space, which contains the possible trained meta-parameter values, after observing the available candidate tasks \(\{\Omega_{\text{task}}^{(k)}\},k\in[K]\) and the randomly initialized meta-parameters \(\bm{\Theta}^{(0)}\). Meanwhile, denoting our selection of the meta-training task batches as \(\Omega(K)=\{\Omega_{1},\dots,\Omega_{K}\}\), we will have the corresponding trained meta-parameters \(\bm{\Theta}^{(K)}\). Here, we can define the \(K\)-iteration regret as

\[R(K)=\mathbb{E}_{\mathcal{T}\sim\mathcal{P}(\mathcal{T}),\bm{x}\sim\mathcal{D} _{\mathcal{T}}}\bigg{[}\mathcal{L}\big{(}\bm{x};\mathcal{I}(\mathcal{T},\bm{ \Theta}^{(K)})\big{)}-\mathcal{L}\big{(}\bm{x};\mathcal{I}(\mathcal{T},\bm{ \Theta}^{(K),*})\big{)}\bigg{]}\] (5)

which measures the difference of the generalization ability between the trained meta-parameters \(\bm{\Theta}^{(K)}\) and the optimal meta-parameters \(\bm{\Theta}^{(K),*}\), after \(K\) meta-training iterations.

## 4 Proposed Framework: BASS

In this section, we introduce our proposed BASS framework, which simultaneously optimizes the meta-model and the task scheduling strategy on the fly. The pseudo-code is presented in **Algo.** 1, and the illustration for each meta-training iteration \(k\in[K]\) is shown in **Figure** 2.

Figure 2: In meta-training iteration \(k\in[K]\), the BASS framework overview. We only need one round of the optimization process (LHS of the figure) to update the meta-model and BASS.

**Remark 1** (Task Scheduling with Contextual Bandits).: _By the problem definition, there are \(N_{\text{batch}}=\binom{N_{\text{batch}}}{B}\) candidate task batches in each meta-training iteration, which can be a large number. In this case, enumerating all possible task batches and estimating their rewards will be time consuming. Therefore, under the Contextual Bandits settings, BASS alternatively considers **each candidate task**\(\mathcal{T}_{k,i}\in\Omega_{\text{task}}^{(k)}\)**as an arm**, and directly chooses \(B\) arms as the meta-training tasks \(\Omega_{k}\subset\Omega_{\text{task}}^{(k)}\). As a result, BASS can (1) reduce the arm space size from \(\mathcal{O}(N_{\text{batch}})\) to \(\mathcal{O}(N_{\text{task}})\), while (2) enjoy the performance guarantee (**Section**5) in terms of regret bound (**Eq. 5**)._

Section Outline.Here, we will first present our definition of the **arm contexts** (**Subsec. 4.1**), whose formulation is challenging, because our settings are different from conventional Contextual Bandits where arm contexts are readily available from the environment. Then, applying two neural networks \(f_{1},f_{2}\) for exploitation and exploration respectively, we formulate the **arm benefit score** (**Subsec. 4.2**), which measures the benefit if we include the corresponding arm (task) into the current meta-training process. Next, we define the **arm rewards** and **exploration scores** as the labels for training \(f_{1},f_{2}\) respectively. In particular, to deal with the challenge of achieving exploration in task scheduling, we incorporate the information and the dynamics w.r.t. meta-optimizations, and formulate two separate exploration objectives for a refined exploration strategy (**Subsec. 4.3**). Finally, we update BASS with GD (**Subsec. 4.4**), and train the meta-model with chosen tasks.

### Formulating Arm Contexts

To encode the information from both the task side and the meta-model side, for each candidate task (i.e., arm) \(\mathcal{T}_{k,i}\in\Omega_{\text{task}}^{(k)}\) in meta-training iteration \(k\in[K]\), we formulate its arm contexts as the meta-parameters after task adaptations, denoted by

\[\bm{\chi}_{k,i}^{s}:=\bm{\Theta}_{k,i}^{(J)}=\mathcal{I}(\mathcal{T}_{k,i},\bm {\Theta}^{(k-1)});\quad\bm{\chi}_{k,i}^{q}:=\bm{\Theta}^{(k-1)}[\mathcal{T}_{k, i}]=\bm{\Theta}^{(k-1)}-\eta_{2}\nabla_{\bm{\Theta}}\mathcal{L}(D_{k,i}^{q}; \bm{\Theta}_{k,i}^{(J)})\] (6)

where \(\bm{\Theta}_{k,i}^{(J)}\) are the task-specific parameters after adapting meta-parameters \(\bm{\Theta}^{(k-1)}\) to task \(\mathcal{T}_{k,i}\) with inner-loop optimization (**Eqs. 1**-2); while \(\bm{\Theta}^{(k-1)}[\mathcal{T}_{k,i}]\) refer to the meta-parameters after adapting the current \(\bm{\Theta}^{(k-1)}\) to task \(\mathcal{T}_{k,i}\), with both inner-loop and outer-loop optimization (as in **Eq. 3**). In particular, we assign each arm \(\mathcal{T}_{k,i}\) with two different arm contexts to model the dynamics of meta-parameters w.r.t. inner-loop optimization and outer-loop optimization respectively. For example, the variance of the corresponding data distribution \(\mathcal{D}_{\mathcal{T}_{k,i}}\) can be high. In this case, the support set \(D_{k,i}^{s}\) and the query set \(D_{k,i}^{q}\) will be considerably different, which tends to make the correspondingarm contexts \(\chi^{s}_{k,i}\), \(\chi^{q}_{k,i}\) divergent. As a result, the gradient vectors \(\nabla_{\theta}f_{1}(\chi^{s}_{k,i})\), \(\nabla_{\theta}f_{1}(\chi^{q}_{k,i})\) will likely be distinct from each other. Alternatively, if the support set \(D^{s}_{k,i}\) and the query set \(D^{q}_{k,i}\) are not significantly distinct (the distance between \(\chi^{s}_{k,i}\) and \(\chi^{q}_{k,i}\) is also likely to be relatively small), these two gradient vectors tend to change dramatically when adapting to \(\mathcal{T}_{k,i}\). The reason is possibly that the exploitation model \(f_{1}\) is not well adapted to this task \(\mathcal{T}_{k,i}\). For both scenarios above, it can be beneficial to include more exploration for the task \(\mathcal{T}_{k,i}\), and the target is helping \(f_{1}\) better learn the reward for this task by actively acquiring the knowledge of it. And the two formulated arm contexts can provide important reference for our exploration module.

**Remark 2** (Recycling Arm Contexts).: _In order to derive the arm contexts (**Eq. 6**), the gradients for the outer-loop optimization \(\nabla_{\bm{\Theta}}\mathcal{L}(D^{q}_{k,\widehat{i}},\bm{\Theta}^{(J)}_{k, \widehat{i}}),\widehat{i}\in\widehat{\mathcal{N}}_{k}\) of the chosen arms \(\mathcal{T}_{k,\widehat{i}}\in\Omega_{k}\) are calculated. As a result, these gradients can be **recycled** to update the meta-model parameters based on **Eq. 3** (line 15, **Algo. 1**), which helps reduce the computational cost when updating the meta-model._

### Estimating Benefit Scores for Tasks

To determine which arms (tasks) should be included to the meta-training iteration \(k\), we formulate the **arm benefit score** estimation for each candidate arm \(\mathcal{T}_{k,i}\in\Omega^{(k)}_{\text{task}}\). The estimated benefit score consists of two parts: (1) the estimated _arm reward_ of choosing this task based on existing knowledge (i.e., exploitation); (2) and the _exploration score_ for the future potential benefit (i.e., exploration). Inspired by recent advances in neural bandits [9], we introduce two separate neural networks, \(f_{1}(\cdot;\bm{\theta}_{1})\) and \(f_{2}(\cdot;\bm{\theta}_{2})\), to estimate the _arm reward_ and _exploration score_ respectively. The exploitation network \(f_{1}(\cdot;\bm{\theta}_{1})\) aims to learn the mapping \(h(\cdot)\) from arm contexts (i.e., meta-parameters) to rewards, while the exploration network \(f_{2}(\cdot;\bm{\theta}_{2})\) aims to learn the uncertainty of reward estimations as the exploration criterion. Different from conventional bandit models, e.g. [9], that works on static arm contexts given by the environment, our design alternatively leverages the evolving information from both the task (arm) side and meta-parameters side, across meta-training iterations. In addition, we consider the dynamics of meta-optimizations for a more comprehensive modeling of the exploration aspect, and the details will be introduced later. Here, given a candidate arm \(\mathcal{T}_{k,i}\in\Omega^{(k)}_{\text{task}}\), its estimated benefit score \(\widehat{s}_{k,i}\) is formulated as

\[\widehat{s}_{k,i}=\alpha\cdot\widehat{r}_{k,i}+\widehat{e}_{k,i}=\alpha\cdot f _{1}(\bm{\chi}^{q}_{k,i};\bm{\theta}^{(k-1)}_{1})+f_{2}\bigg{(}[\nabla_{\bm{ \theta}}f_{1}(\bm{\chi}^{s}_{k,i});\;\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{q}_{ k,i})];\bm{\theta}^{(k-1)}_{2}\bigg{)}\] (7)

where \(\alpha\in(0,1]\) is the exploration coefficient to balance exploitation and exploration. Notice that \(f_{2}(\cdot;\bm{\theta}_{2})\) will take the _concatenated gradient_ of \(f_{1}(\cdot;\bm{\theta}_{1})\) w.r.t. both arm contexts \(\bm{\chi}^{s}_{k,i},\bm{\chi}^{q}_{k,i}\) as the input, represented by \([\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{s}_{k,i});\nabla_{\bm{\theta}}f_{1}(\bm {\chi}^{q}_{k,i})]\). And the output will be the exploration score estimation \(\hat{e}_{k,i}\). To obtain \(\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{q}_{k,i})\), we also calculate \(f_{1}(\bm{\chi}^{q}_{k,i};\bm{\theta}_{1})\) and run the back-propagation. Afterwards, we choose the top-\(B\) arms with the highest estimated benefit scores \(\widehat{s}_{k,i},i\in\mathcal{N}\), as the chosen task batch \(\Omega_{k}\subset\Omega^{(k)}_{\text{task}}\) (line 10, **Algo. 1**).

**Design Intuition.** First, recent advances of neural Contextual Bandits [54; 9; 38] have shown that the uncertainty of reward estimations is directly related to the gradients of the estimation model. Therefore, we leverage an exploration module \(f_{2}(\cdot;\bm{\theta}_{2})\) to directly learn this unknown relationship. Second, since \(D^{s}_{k,i},D^{q}_{k,i}\) are from the same data distribution \(\mathcal{D}_{\mathcal{T}_{k,i}},\) if these two gradients \(\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{s}_{k,i}),\nabla_{\bm{\theta}}f_{1}(\bm {\chi}^{q}_{k,i})\) are distinct, the reason can be: (1) the variance of the data distribution \(\mathcal{D}_{\mathcal{T}_{k,i}}\) is high (due to the potentially noisy or difficult task); or (2) the gradients of \(f_{1}(\cdot;\bm{\theta}_{1})\) tend to change significantly when adapting to task \(\mathcal{T}_{k,i}\). In both cases, it can be harder for \(f_{1}(\cdot;\bm{\theta}_{1})\) to accurately predict the arm reward \(r_{k,i}\), and the meta-model can fail to properly adapt to the task \(\mathcal{T}_{k,i}\). In this case, we apply the concatenated gradients w.r.t. both arm contexts as the input of \(f_{2}(\cdot;\bm{\theta}_{2})\), in order to provide the information for \(f_{2}(\cdot;\bm{\theta}_{2})\) to evaluate exploration scores.

**Network Architecture and Parameter Initialization.** Here, we consider \(f_{1}(\cdot;\bm{\theta}_{1}),f_{2}(\cdot;\bm{\theta}_{2})\) to be two \(L\)-layer fully-connected (FC) networks with network width \(m\), while \(\bm{\theta}=\{\bm{\theta}_{1},\bm{\theta}_{2}\}\) refer to their trainable parameters. For their randomly initialized parameters \(\bm{\theta}^{(0)}=\{\bm{\theta}_{1}^{(0)},\bm{\theta}_{2}^{(0)}\}\), the weight matrix entries for the first \(L-1\) layers are drawn from the Gaussian distribution \(N(0,2/m)\), while the entries of the last layer (\(L\)-th layer) are sampled from \(N(0,1/m)\).

**Remark 3** (Reducing Input Complexity).: _The input of \(f_{1}(\cdot;\bm{\theta}_{1})\) is the arm context \(\bm{\chi}^{q}_{k,i}\), whose dimensionality is the number of meta-parameters \(p\). A similar situation also exists for the exploration network \(f_{2}(\cdot;\bm{\theta}_{2})\). Inspired by the idea of learning dense low-dimensional representations with Convolutional Neural Networks (CNNs) (e.g., [44]), we apply the **average pooling** approach to approximate original inputs for reducing the running time and space complexity in practice. To show its effectiveness, we will apply this approach on BASS for all the experiments in Section 6._

### Formulating Arm Rewards and Exploration Scores

Different from the conventional neural bandit algorithms [53; 54; 9] where the reward is provided by the environment oracle, we need to carefully design the arm rewards to reflect the arm benefit in terms of the meta-model's generalization ability. Analogous to task batch rewards (**Eq. 4**), we formulate the single **arm reward**\(r_{k,i}=h(\bm{\Theta}^{(k-1)}[\mathcal{T}_{k,i}])=1-\mathbb{E}_{\mathcal{T} \sim\mathcal{P}(\mathcal{T}),\bm{x}\sim\mathcal{D}_{\mathcal{T}}}\big{[} \mathcal{L}\big{(}\bm{x};\mathcal{I}(\mathcal{T},\bm{\Theta}^{(k-1)}[ \mathcal{T}_{k,i}])\big{)}\big{]}\) for arm \(\mathcal{T}_{k,i}\). Since it is impractical to calculate the arm reward by enumerating over \(\mathcal{P}(\mathcal{T})\), we sample a batch of validation tasks \(\Omega_{k}^{\text{valid}}\) to derive the unbiased reward approximation, denoted by

\[\widetilde{r}_{k,i}=1-\frac{1}{|\Omega_{k}^{\text{valid}}|}\sum_{\mathcal{T} ^{\text{valid}}\in\Omega_{k}^{\text{valid}}}\mathcal{L}\big{(}D_{\mathcal{T} ^{\text{valid}}}^{q};\mathcal{I}(\mathcal{T}^{\text{valid}},\bm{\Theta}^{(k-1 )}[\mathcal{T}_{k,i}])\big{)}.\] (8)

Here, we adopt the single-step inner-loop optimization [19; 40] to derive \(\mathcal{I}(\mathcal{T}_{k,\widehat{i}};\bm{\Theta}^{(k-1)}[\mathcal{T}_{k,i}])\) in **Eq. 8**, in order to save the computational cost in practice. Under the few-shot settings, the computation of arm rewards is efficient, since the support set is generally small for inner-loop optimization. The approximation error here can be bounded by the concentration inequality, as the validation tasks \(\Omega_{k}^{\text{valid}}\) are sampled from \(\mathcal{P}(\mathcal{T})\).

On the other hand, to formulate the **exploration score**\(e_{k,i}\) (i.e., the label for \(f_{2}(\cdot;\bm{\theta}_{2})\)), we consider two separate exploration objectives: (1) the prediction uncertainty for the exploitation module \(f_{1}(\cdot;\bm{\theta}_{1})\), which is \(r_{k,i}-f_{1}(\bm{\chi}_{k,i}^{q};\bm{\theta}_{1})\); (2) the validation loss of the meta-model, which represents the difficulty of adapting to \(\mathcal{T}_{k,i}\), inspired by the "task difficulty measure" in Curriculum Learning [52; 50]. As a result, with \(\mathcal{L}_{k,i}=\mathcal{L}(D_{k,i}^{q};\bm{\Theta}_{k,i}^{(J)})\) being the validation loss of arm \(\mathcal{T}_{k,i}\), we formulate the exploration score as \(e_{k,i}=\alpha\cdot\big{(}r_{k,i}-f_{1}(\bm{\chi}_{k,i}^{q};\bm{\theta}_{1}^{( k-1)})\big{)}+(1-\alpha)\cdot\mathcal{L}_{k,i}\). Analogously, with the approximated reward \(\widetilde{r}_{k,i}\) (**Eq. 8**), we calculate the exploration score approximation by

\[\widetilde{e}_{k,i}=\alpha\cdot\big{(}\widetilde{r}_{k,i}-f_{1}(\bm{\chi}_{k, i}^{q};\bm{\theta}_{1}^{(k-1)})\big{)}+(1-\alpha)\cdot\mathcal{L}_{k,i}.\] (9)

Here, the exploration coefficient \(\alpha\in(0,1]\) (in **Eq. 7**) is also used to balance our two exploration objectives, which are (1) prediction uncertainty \(r_{k,i}-f_{1}(\cdot;\bm{\theta}_{1})\): if the exploitation model \(f_{1}(\cdot;\bm{\theta}_{1})\) is under-estimating the arm reward, leading to the positive residual \(r_{k,i}-f_{1}(\cdot;\bm{\theta}_{1})\), we will have a high exploration score to enhance the exploration for this arm; otherwise, when \(r_{k,i}-f_{1}(\cdot;\bm{\theta}_{1})\) is negative, it indicates an excessively high estimation, which will alternatively lower the exploration score to compensate for the over-estimation. With a higher \(\alpha\) value, our exploration strategy will focus more on the behavior of the exploitation model \(f_{1}(\cdot)\); (2) the difficulty of task adaptation (i.e., validation loss) \(\mathcal{L}_{k,i}\): if the current meta-model does not generalize well to arm \(\mathcal{T}_{k,i}\), the validation loss \(\mathcal{L}_{k,i}\) will be high, which will also lead to a high exploration score. In this way, our formulation considers two different exploration objectives as well as the dynamics of meta-optimizations (base on concatenated network gradients), for a refined exploration strategy.

### Updating Bandit Scheduler Parameters

After updating the meta-parameters (Line 14, **Algo. 1**, **Remark 2**) with tasks \(\Omega_{k}=\{\mathcal{T}_{k,\widehat{i}}\}_{\widehat{i}\in\bar{\mathcal{N}}_{k}}\), we proceed to update the parameters of BASS (Line 15, **Algo. 1**). Recall that \(f_{1}(\cdot;\bm{\theta}_{1})\) tries to learn the reward mapping function \(h(\cdot)\), and \(f_{2}(\cdot;\bm{\theta}_{2})\) aims to learn the exploration score. Given the selected arms \(\Omega_{k}\), with \(\eta_{1}^{\bm{\theta}},\eta_{2}^{\bm{\theta}}\in\mathbb{R}^{+}\) being the learning rates, we apply the GD and quadratic loss to update the parameters of BASS, denoted by \(\bm{\theta}_{1}^{(k)}=\bm{\theta}_{1}^{(k-1)}-\eta_{1}^{\bm{\theta}}\cdot\nabla _{\bm{\theta}_{1}}\big{(}\frac{1}{B}\sum_{\mathcal{T}_{k,\widehat{i}}\in \Omega_{k}}\big{|}f_{1}(\bm{\chi}_{k,\widehat{i}}^{q};\bm{\theta}_{1}^{(k-1 )})-\widetilde{r}_{k,\widehat{i}}^{\bm{\cdot}}\big{|}^{2}\big{)},\)\(\bm{\theta}_{2}^{(k)}=\bm{\theta}_{2}^{(k-1)}-\eta_{2}^{\bm{\theta}}\cdot\nabla_{\bm{ \theta}_{2}}\big{(}\frac{1}{B}\sum_{\mathcal{T}_{k,\widehat{i}}\in\Omega_{k}} \big{|}f_{2}\big{(}[\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{k,\widehat{i}}^{s}); \nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{k,\widehat{i}}^{q}]);\bm{\theta}_{2}^{(k- 1)}\big{)}-\widetilde{e}_{k,\widehat{i}}^{\bm{\cdot}}\big{|}^{2}\big{)}\). We refer to **Eqs. 8-9** for calculating the approximated arm reward \(\widetilde{r}_{k,\widehat{i}}\) and exploration score \(\widetilde{e}_{k,\widehat{i}}\).

## 5 Theoretical Analysis

Recall that in each iteration \(k\in[K]\), we receive candidate arms (tasks) \(\Omega_{\text{task}}^{(k)}=\{\mathcal{T}_{k,i}\}_{i\in\mathcal{N}}\), and each arm \(\mathcal{T}_{k,i}\) is associated with two context vectors \(\bm{\chi}_{k,i}^{s},\bm{\chi}_{k,i}^{q}\). For the sake of analysis, we normalize these two contexts such that \(\|\bm{\chi}_{k,i}^{k}\|_{2}=\|\bm{\chi}_{k,i}^{q}\|_{2}=1\), and set the exploration coefficient \(\alpha=1\). Following the existing work [47; 48], we let the meta-model \(\mathcal{F}(\cdot;\bm{\Theta})\) be a \(L_{\mathcal{F}}\)-layer FC network with Gaussian Initialization, with the network width \(m_{\mathcal{F}}\). Note that our results can also be generalized to other network architectures, such as CNN and ResNet [22], based on the analysis of over-parameterized neural networks [11; 48; 3]. For the theoretical analysis, we adopt Sigmoid activation for \(f_{1}\) and ReLU for \(f_{2}\), in order to make \(f_{1}\) Lipschitz smooth under over-parameterization settings. Then, we draw trained parameters of BASS with \(\{\bm{\theta}_{1}^{(k)},\bm{\theta}_{2}^{(k)}\}\sim\{\widetilde{\bm{\theta}}_ {1}^{(\tau)},\widetilde{\bm{\theta}}_{2}^{(\tau)}\}_{\tau\in[k]}\). Here, starting from the randomly initialized parameters \(\{\bm{\theta}_{1}^{(0)},\bm{\theta}_{2}^{(0)}\}\), each parameter pair \(\{\widetilde{\bm{\theta}}_{1}^{(\tau)},\widetilde{\bm{\theta}}_{2}^{(\tau)}\},\tau\in[k]\) is separately trained on past arm rewards \(\{\tau_{r^{\prime},\widetilde{z}}\}_{\tau^{\prime}\in[\tau];\widetilde{z}\in \mathcal{N}_{r^{\prime}}}^{\text{-}}\) and exploration scores \(\{e_{r^{\prime},\widetilde{z}}\}_{\tau^{\prime}\in[\tau];\widetilde{z}\in \mathcal{N}_{r^{\prime}}}^{\text{-}}\) with \(J_{\bm{\theta}}\)-iteration GD. Next, similar to existing neural bandit works (e.g., [54; 9; 53]) and the works on meta-model convergence analysis (e.g., [47; 48]), we have the following separateness assumption.

**Assumption 5.1** (\(\rho\)-Separateness).: _After \(K\) meta-training iterations, for every pair of arm contexts \(\bm{\chi}_{k,i}^{q},\bm{\chi}_{k^{\prime},i^{\prime}}^{q}\) with \(k,k^{\prime}\in[K]\) such that the corresponding arms \(\mathcal{T}_{k,i}\in\Omega_{k}\wedge\mathcal{T}_{k^{\prime},i^{\prime}}\in \Omega_{k^{\prime}}\), if \((k,i)\neq(k^{\prime},i^{\prime})\), we have \(\|\bm{\chi}_{k,i}^{q}-\bm{\chi}_{k^{\prime},i^{\prime}}^{q}\|_{2}\geq\rho\) where \(0<\rho\leq\mathcal{O}(\frac{1}{L})\)._

The assumption above is mild because of two main reasons: (1) since \(L\) is manually chosen (e.g., \(L=2\)), we can easily satisfy the condition \(0<\rho\leq\mathcal{O}(\frac{1}{L})\) as long as no two arm contexts are identical; (2) since the meta-parameters \(\bm{\Theta}^{(k)},k\in[K]\) are constantly changing, the corresponding arm contexts will also be distinct across different meta-training iterations. Additional discussions on this assumption are in Appendix B. With standard settings of over-parameterized neural networks [47; 3; 11] and the definition of regret \(R(K)\) in Eq. 5, we have the following **Theorem**5.2.

**Theorem 5.2**.: _Define \(\delta\in(0,1)\), \(0<\xi_{1},\xi_{2}\leq\mathcal{O}(1/K)\), \(\xi_{f}=\max\{\xi_{1},\xi_{2}\}\), \(0<\rho\leq\mathcal{O}(1/L)\), \(c_{\xi}>0\), \(\xi_{L}=(c_{\xi})^{L}\). Suppose the network width \(m\geq\Omega\big{(}\text{Poly}(K,L,N_{\text{task}},\rho^{-1})\log(1/\delta) \big{)};m_{\mathcal{F}}\geq\Omega\big{(}\text{Poly}(K,L_{\mathcal{F}},N_{ \text{task}})\cdot\log(1/\delta)\big{)}\). Then, let the learning rates be \(\eta_{1},\eta_{2}=\Theta\big{(}\frac{m_{\mathcal{F}}^{-1}}{\text{Poly}(K,N_{ \text{task}},L_{\mathcal{F}})}\big{)};\)\(\eta_{\bm{\theta}}^{1},\eta_{\bm{\theta}}^{2}=\Theta\big{(}\frac{\rho\cdot m ^{-1}}{\text{Poly}(K,N_{\text{task}},L)}\big{)}\cdot J_{\bm{\theta}}=\Theta \big{(}\frac{\text{Poly}(K,N_{\text{task}},L)}{\rho\cdot\delta^{2}}\cdot\log( \frac{1}{\xi_{1}})\big{)}\). Following **Algo.** 1, with probability at least \(1-\delta\), the \(K\)-round \(R(K)\) of BASS could be bounded by_

\[R(K)\leq\mathcal{O}\bigg{(}\frac{1}{\sqrt{K}}(\sqrt{2\xi_{f}}+\frac{3L}{\sqrt{ 2}}+(1+2\gamma_{1})\sqrt{2\log(K/\delta)})\bigg{)}+\mathcal{O}(\frac{\xi_{L}^ {2}KJB\sqrt{L_{\mathcal{F}}}}{\sqrt{m_{\mathcal{F}}}})+\gamma_{m}\] (10)

_where \(\gamma_{m}=\mathcal{O}(\frac{1}{m^{1/c_{\gamma}}}),c_{\gamma}>1\), and \(\gamma_{1}=\mathcal{O}(1)\) with sufficient network width \(m\) of BASS._

The proof is presented in Appendix C. Here, the first term on the RHS is scaled by the \(1/\sqrt{K}\) term, which means the regret bound will shrink along with more iterations \(K\). The second term on the RHS is scaled by \(1/\sqrt{m_{\mathcal{F}}}\), which makes it a diminutive term under the over-parameterization settings. Since the network depth \(L\) of BASS is a small integer (we apply \(L=2\) for experiments in Section 6), \(\xi_{L}\) will also be a relatively small constant. Meanwhile, \(\gamma_{m}\) will also decrease significantly with increasingly large network width \(m\) of BASS. In contrast, with a convex loss function (e.g., \(L_{2}\) loss or cross-entropy loss) and the same over-parameterization settings, the regret upper bound of the _uniform sampling_ strategy [19; 40] can possibly scale up to \(1\) for the worst-case scenario, and the upper bound will not decrease with more iterations \(K\) (Appendix C.13). Alternatively, BASS works under the bandit settings, by directly measuring the meta-model performance difference w.r.t. the chosen task batch and the optimal one. With more iterations \(K\), BASS tends to make more accurate scheduling decisions, which makes our regret bound possible. For the existing works, [51] prove that they can improve the optimization landscape with the assumed correlation of task difficulties and meta-model generalization ability. [13] show that their self-paced strategy can improve the model robustness when facing noisy training tasks. Different from previous works, we provide the performance guarantee for the proposed BASS under the neural bandit framework.

## 6 Experiments

In this section, we compare BASS against seven strong baselines, including: (1) Uniform Sampling; non-adaptive self-paced methods and task schedulers (2) SPL [26], (3) Focal-Loss (FOCAL) [30], (4) DAML [29], (5) GCP [31], (6) PAML [24]; and the adaptive task scheduler (7) ATS [51]. Since GCP is not originally compatible with our problem settings and can only work with classification problems, we properly adapt it by choosing the tasks with the highest probabilities, and apply it on the classification data sets. ANIL [40] is adopted as the backbone meta-learning framework. Due to page limit, we include the complementary experiments (e.g., parameter study for \(\alpha\), effects of different levels of task skewness), and the configurations to Appendix **Section** A.

### Real Data Sets with Noisy Meta-Training Tasks

We adopt Drug [36], Mini-ImageNet (M-ImageNet) [42] and CIFAR-100 (CIFAR) [25] data sets under the few-shot learning scenario. Similar to [51], we apply classification accuracy as the evaluation criterion for the Mini-ImageNet and CIFAR-100 data sets, and consider the squared Pearson coefficient for the Drug data set. For each meta-learning iteration \(k\in[K]\), the learner is given a candidate pool of 10 tasks (i.e., \(|\Omega_{\text{task}}^{(k)}|=N_{\text{task}}=10\)), and it will need to choose a batch of \(B=2\) tasks as the training tasks \(\Omega_{k}\) for this iteration. For the Mini-ImageNet and CIFAR-100 data sets, we consider half of the meta-training tasks are perturbed by the label flipping noise [21], where the chance of a label being flipped is \(\epsilon\in[0,1]\). As the Drug data set stands for a regression problem, we draw the label noise from the Gaussian distribution \(N(0,\epsilon^{2})\). The experimental settings are under 1-shot or 5-shot, 5-Way (for classification data sets) learning scenario with the noise level \(\epsilon=0.5\). The experiment results are shown in **Table 1** and **Figure 3**. For the average ranking column, we exclude results from the M-ImageNet (1) setting, since BASS and the baselines tend to train a meta-model performing "random guessing". More discussions are in the next paragraph.

Here, BASS can generally outperform the baselines by directly learning the mapping from the meta-parameters to the arm rewards, as well as balancing the exploitation and exploration. ATS also achieves good performance as it adaptively learns the correlation between the task adaptation difficulty and task scheduling, which proves that it is necessary to apply the adaptive scheduling strategy instead of staying with a fixed protocol. Meanwhile, BASS can generally train the meta-model more efficiently (**Figure 3**), leading to good performances at the early stage of meta-training. In particular, for the Mini-ImageNet data set under the 1-shot, 5-way settings, all the algorithms fail to train an effective meta-model. Here, under the 5-way classification scenario, all the methods will likely generate a meta-model performing "random guessing" (around \(20\%\) accuracy). In this case, utilizing Ensemble Inference techniques [12; 16] can help alleviate this problem, and we include further discussion in the case study (**Subsec.**6.3).

### Effects of the Skewed Task Distribution

The skewed task distribution \(\mathcal{P}(\mathcal{T})\) commonly exists in real-word cases. For instance, consider an animal image classification data set where each class (i.e., task) corresponds to one kind of animals. In this case, field classes can be considered as "frequent" tasks in the task distribution due to their large quantity and strong mutual correlations, compared with "tail" tasks like kangaroo classes. In this case, paying insufficient attention to the "tail" classes can impair the

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Algo. \(\backslash\) Data & Drug (1) & Drug (5) & M-ImageNet (1) & M-ImageNet (5) & CIFAR (1) & CIFAR (5) & **Avg. Rank** \\ \hline Uniform & 0.210\(\pm\)0.013 & 0.220\(\pm\)0.001 & _0.201\(\pm\)0.002_ & 0.301\(\pm\)0.025 & 0.234\(\pm\)0.029 & 0.526\(\pm\)0.011 & 4.4 \\ SPL & **0.244\(\pm\)0.008** & 0.236\(\pm\)0.004 & _0.203\(\pm\)0.002_ & 0.240\(\pm\)0.018 & 0.200\(\pm\)0.002 & 0.367\(\pm\)0.039 & 5.0 \\ FOCAL & 0.222\(\pm\)0.024 & 0.223\(\pm\)0.003 & _0.200\(\pm\)0.000_ & 0.316\(\pm\)0.029 & 0.231\(\pm\)0.024 & 0.485\(\pm\)0.006 & 4.4 \\ DAML & 0.146\(\pm\)0.009 & 0.177\(\pm\)0.003 & _0.201\(\pm\)0.001_ & 0.310\(\pm\)0.016 & 0.247\(\pm\)0.003 & 0.414\(\pm\)0.025 & 5.4 \\ GCP & N/A & N/A & _0.201\(\pm\)0.001_ & 0.282\(\pm\)0.016 & 0.243\(\pm\)0.007 & 0.508\(\pm\)0.009 & 6.0 \\ PAML & 0.192\(\pm\)0.020 & 0.205\(\pm\)0.009 & _0.199\(\pm\)0.001_ & 0.218\(\pm\)0.013 & 0.199\(\pm\)0.004 & 0.316\(\pm\)0.022 & 7.2 \\ ATS & 0.230\(\pm\)0.002 & 0.237\(\pm\)0.014 & _0.201\(\pm\)0.001_ & 0.334\(\pm\)0.053 & 0.257\(\pm\)0.048 & 0.515\(\pm\)0.015 & 2.4 \\ \hline
**BASS** & 0.242\(\pm\)0.012 & **0.245\(\pm\)0.006** & _0.198\(\pm\)0.004_ & **0.351\(\pm\)0.012** & **0.272\(\pm\)0.025** & **0.553\(\pm\)0.008** & **1.2** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on real data sets [data set (shot); results \(\pm\) standard deviation]. _For 1-shot M-ImageNet, all the methods end up with an invalid meta-model performing "random guessing"._

Figure 3: Accuracy results (5-shot, \(\epsilon=0.5\)). BASS can achieve a good performance at early meta-training stage.

generalization performance of the trained meta-model. Thus, to investigate the effects of when the task distribution \(\mathcal{P}(\mathcal{T})\) is skewed, we randomly choose some tasks from \(\mathcal{P}(\mathcal{T})\), and assign them with higher sampling probabilities (weights). This corresponds to the situation when \(\mathcal{P}(\mathcal{T})\) is skewed, so that sampling from \(\mathcal{P}(\mathcal{T})\) will likely lead to similar tasks. Here, with the CIFAR-100 data set, we sample 10 tasks and assign them with higher sampling probabilities (weights) (\(5\) tasks with \(10\%\), 5 tasks with \(5\%\)), while the rest of the tasks equally share the remaining \(25\%\) probability.

### Case Study: BASS-aided Ensemble Inference

From **Figure**3, we notice that BASS can train a meta-model that achieves good generalization performance at the early stage of meta-training. One application of this property is using BASS to assist meta-learning models under the Ensemble Inference settings, where separate models are combined to enhance the generalization ability. One renowned ensemble approach is the model-parameter ensemble [41, 33]. With a collection of \(N_{E}\) individual models \(\{\mathcal{F}(\cdot;\mathbf{\Theta}_{i})\}_{i\in[N_{E}]}\) of the same architecture, the ensemble model will be \(\mathcal{F}_{E}(\cdot;\mathbf{\Theta}_{E})\), and its parameters \(\mathbf{\Theta}_{E}=\frac{1}{N_{E}}\sum_{i\in[N_{E}]}\mathbf{\Theta}_{i}\) are the averaged parameters across individual models. Then, the ensemble model \(\mathcal{F}_{E}(\cdot;\mathbf{\Theta}_{E})\) will be applied as the inference model for downstream problems. Here, one natural way of obtaining the individual models \(\mathcal{F}(\cdot;\mathbf{\Theta}_{i}),i\in[N_{E}]\) is deeming the models from different training iterations as the individual models for ensemble [12, 16]. Here, we conduct experiments using the ensemble techniques with individual models \(\mathcal{F}(\cdot;\mathbf{\Theta}_{i})\) trained by baselines and BASS. We choose the top \(N_{E}=10\) models with the smallest validation loss across different meta-training iterations as the individual models \(\{\mathcal{F}(\cdot;\mathbf{\Theta}_{i})\}_{i\in[N_{E}]}\) for ensemble. The results are shown in **Table**3. We label the ensemble version of BASS as "BASS-E", and the non-ensemble version as "BASS-S".

Compared with the non-ensemble settings (**Table**1), we see that the BASS-aided ensemble model can generally perform better. In particular, the ensemble model can improve the meta-model inference performance in significantly difficult cases, such as the Mini-ImageNet under the 1-shot setting (**Subsec.**6.1). As a result, BASS can help generate high-quality ensemble model with the insufficient knowledge problem at the early stage of meta-training, as well as plan for the future meta-training iterations with the adaptive exploration strategy. We include both theoretical analyses and a comprehensive set of experiments to demonstrate the effectiveness of our proposed framework as well as its key properties.

Figure 4: Average weights of chosen meta-training tasks. The testing accuracy vs. iterations needed. BASS can actively exploring for “tail” tasks, and requires much fewer iterations for the same performance (as few as \(\sim 1/3\) of baselines’ iterations).

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Align \(\circ\) Data & M-ImageNet (1) & M-ImageNet (5) & CIFAR (1) & CIFAR (5) \\ \hline Uniform & 0.231\(\pm\)0.014 & 0.313\(\pm\)0.027 & 0.270\(\pm\)0.014 & 0.534\(\pm\)0.012 \\ SPL & 0.218\(\pm\)0.006 & 0.298\(\pm\)0.004 & 0.219\(\pm\)0.005 & 0.363\(\pm\)0.038 \\ FOCAL & 0.204\(\pm\)0.005 & 0.347\(\pm\)0.030 & 0.235\(\pm\)0.015 & 0.499\(\pm\)0.003 \\ DAML & 0.222\(\pm\)0.001 & 0.326\(\pm\)0.031 & 0.261\(\pm\)0.008 & 0.432\(\pm\)0.019 \\ GCP & 0.226\(\pm\)0.006 & 0.297\(\pm\)0.011 & 0.268\(\pm\)0.019 & 0.512\(\pm\)0.015 \\ PAML & 0.213\(\pm\)0.024 & 0.232\(\pm\)0.009 & 0.232\(\pm\)0.009 & 0.336\(\pm\)0.029 \\ ATS & 0.202\(\pm\)0.002 & 0.334\(\pm\)0.052 & 0.313\(\pm\)0.081 & 0.517\(\pm\)0.017 \\ \hline
**BASS-S** & 0.198\(\pm\)0.004 & 0.351\(\pm\)0.012 & 0.272\(\pm\)0.025 & **0.553\(\pm\)0.008** \\
**BASS-E** & **0.242\(\pm\)0.004** & **0.366\(\pm\)0.003** & **0.327\(\pm\)0.010** & 0.551\(\pm\)0.004 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ensemble case study [dataset (shot); results \(\pm\) standard deviation].

## Acknowledgments and Disclosure of Funding

This work is supported by National Science Foundation under Award No. IIS-1947203, IIS-2117902, IIS-2137468, IIS-2002540, and Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agencies or the government.

## References

* [1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24:2312-2320, 2011.
* [2] Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In _ICML_, pages 127-135. PMLR, 2013.
* [3] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International Conference on Machine Learning_, pages 242-252. PMLR, 2019.
* [4] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47(2-3):235-256, 2002.
* [5] Yikun Ban and Jingrui He. Convolutional neural bandit: Provable algorithm for visual-aware advertising. _arXiv preprint arXiv:2107.07438_, 2021.
* [6] Yikun Ban and Jingrui He. Local clustering in contextual multi-armed bandits. In _Proceedings of the Web Conference 2021_, pages 2335-2346, 2021.
* [7] Yikun Ban, Jingrui He, and Curtiss B Cook. Multi-facet contextual bandits: A neural network perspective. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 35-45, 2021.
* [8] Yikun Ban, Yunzhe Qi, Tianxin Wei, and Jingrui He. Neural collaborative filtering bandits via meta learning. _arXiv preprint arXiv:2201.13395_, 2022.
* [9] Yikun Ban, Yuchen Yan, Arindam Banerjee, and Jingrui He. EE-net: Exploitation-exploration neural networks in contextual bandits. In _International Conference on Learning Representations_, 2022.
* [10] Yikun Ban, Yuchen Yan, Arindam Banerjee, and Jingrui He. Neural exploitation and exploration of contextual bandits. _arXiv preprint arXiv:2305.03784_, 2023.
* [11] Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural networks. _Advances in Neural Information Processing Systems_, 32:10836-10846, 2019.
* [12] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and Sungrae Park. Swad: Domain generalization by seeking flat minima. _Advances in Neural Information Processing Systems_, 34:22405-22418, 2021.
* [13] Dong Chen, Lingfei Wu, Siliang Tang, Xiao Yun, Bo Long, and Yueting Zhuang. Robust meta-learning with sampling noise and label noise via eigen-reptile. _arXiv preprint arXiv:2206.01944_, 2022.
* [14] Yudong Chen, Xin Wang, Miao Fan, Jizhou Huang, Shengwen Yang, and Wenwu Zhu. Curriculum meta-learning for next poi recommendation. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 2692-2702, 2021.
* [15] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In _AISTATS_, pages 208-214, 2011.

* [16] Xu Chu, Yujie Jin, Wenwu Zhu, Yasha Wang, Xin Wang, Shanghang Zhang, and Hong Mei. Dna: Domain generalization with diversified neural averaging. In _International Conference on Machine Learning_, pages 4010-4034. PMLR, 2022.
* [17] Aniket Anand Deshmukh, Urun Dogan, and Clay Scott. Multi-task learning for contextual bandits. In _NeurIPS_, pages 4848-4856, 2017.
* [18] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _International Conference on Machine Learning_, pages 1675-1685. PMLR, 2019.
* [19] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.
* [20] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In _International Conference on Machine Learning_, pages 1568-1577. PMLR, 2018.
* [21] B Han, Q Yao, X Yu, G Niu, M Xu, W Hu, I Tsang, and M Sugiyama. Robust training of deep neural networks with extremely noisy labels. In _Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS)_, volume 2, page 4, 2020.
* [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [23] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _arXiv preprint arXiv:1806.07572_, 2018.
* [24] Jean Kaddour, Steindor Semundsson, et al. Probabilistic active meta-learning. _Advances in Neural Information Processing Systems_, 33:20813-20822, 2020.
* [25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [26] M Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. _Advances in neural information processing systems_, 23, 2010.
* [27] Hoyeep Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, and Sehee Chung. Melu: Meta-learned user preference estimator for cold-start recommendation. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 1073-1082, 2019.
* [28] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In _WWW_, pages 661-670, 2010.
* [29] Xiaomeng Li, Lequan Yu, Yueming Jin, Chi-Wing Fu, Lei Xing, and Pheng-Ann Heng. Difficulty-aware meta-learning for rare disease diagnosis. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, pages 357-366. Springer, 2020.
* [30] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In _Proceedings of the IEEE international conference on computer vision_, pages 2980-2988, 2017.
* [31] Chenghao Liu, Zhihao Wang, Doyen Sahoo, Yuan Fang, Kun Zhang, and Steven CH Hoi. Adaptive task sampling for meta-learning. In _European Conference on Computer Vision_, pages 752-769. Springer, 2020.
* [32] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. _arXiv preprint arXiv:1806.09055_, 2018.

* [33] Shiwei Liu, Tianlong Chen, Zahra Atashgahi, Xiaohan Chen, Ghada Sokar, Elena Mocanu, Mykola Pechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Deep ensembling with no overhead for either training or testing: The all-round blessings of dynamic sparsity. _arXiv preprint arXiv:2106.14568_, 2021.
* [34] Zhuoqun Liu, Yuankun Jiang, Chenglin Li, Wenrui Dai, Junni Zou, and Hongkai Xiong. Adaptive task sampling and variance reduction for gradient-based meta-learning. 2022.
* [35] Ricardo Luna Gutierrez and Matteo Leonetti. Information-theoretic task selection for meta-reinforcement learning. _Advances in Neural Information Processing Systems_, 33:20532-20542, 2020.
* [36] Eric J Martin, Valery R Polyakov, Xiang-Wei Zhu, Li Tian, Prasenjit Mukherjee, and Xin Liu. All-assay-max2 pqsar: activity predictions as accurate as four-concentration ic50s for 8558 novartis assays. _Journal of chemical information and modeling_, 59(10):4450-4459, 2019.
* [37] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1406-1415, 2019.
* [38] Yunzhe Qi, Yikun Ban, and Jingrui He. Neural bandit with arm group graph. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1379-1389, 2022.
* [39] Yunzhe Qi, Yikun Ban, and Jingrui He. Graph neural bandits. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1920-1931, 2023.
* [40] Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? towards understanding the effectiveness of maml. In _International Conference on Learning Representations_, 2019.
* [41] Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Gallinari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. _arXiv preprint arXiv:2205.09739_, 2022.
* [42] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.
* [43] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In _International conference on machine learning_, pages 4334-4343. PMLR, 2018.
* [44] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gregoire Mesnil. Learning semantic representations using convolutional neural networks for web search. In _Proceedings of the 23rd international conference on world wide web_, pages 373-374, 2014.
* [45] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting. _Advances in neural information processing systems_, 32, 2019.
* [46] Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, and Nelo Cristianini. Finite-time analysis of kernelised contextual bandits. _arXiv preprint arXiv:1309.6869_, 2013.
* [47] Haoxiang Wang, Ruoyu Sun, and Bo Li. Global convergence and generalization bound of gradient-based meta-learning with deep neural nets. _arXiv preprint arXiv:2006.14606_, 2020.
* [48] Haoxiang Wang, Yite Wang, Ruoyu Sun, and Bo Li. Global convergence of maml and theory-inspired neural architecture search for few-shot learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9797-9808, June 2022.
* [49] Qingyun Wu, Huazheng Wang, Quanquan Gu, and Hongning Wang. Contextual bandits in a collaborative environment. In _SIGIR_, pages 529-538, 2016.

* [50] Tongtong Wu, Xuekai Li, Yuan-Fang Li, Gholamreza Haffari, Guilin Qi, Yujin Zhu, and Guoqiang Xu. Curriculum-meta learning for order-robust continual relation extraction. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 10363-10369, 2021.
* [51] Huaxiu Yao, Yu Wang, Ying Wei, Peilin Zhao, Mehrdad Mahdavi, Defu Lian, and Chelsea Finn. Meta-learning with an adaptive task scheduler. _Advances in Neural Information Processing Systems_, 34:7497-7509, 2021.
* [52] Ji Zhang, Jingkuan Song, Yazhou Yao, and Lianli Gao. Curriculum-based meta-learning. In _Proceedings of the 29th ACM International Conference on Multimedia_, pages 1838-1846, 2021.
* [53] Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural thompson sampling. _arXiv preprint arXiv:2010.00827_, 2020.
* [54] Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration. In _International Conference on Machine Learning_, pages 11492-11502. PMLR, 2020.

Appendix: Experiments (Continue)

### Further Details for the Experiment Settings

For the data partitioning, we have the Mini-ImageNet and CIFAR-100 data sets divided into the partitions \(64:16:20\), which correspond to the training set, validation set and the testing set respectively. Each class is corresponding to a task. Then, for the Drug data set, we partition the tasks into \(4100:76:100\) representing the training set, validation set and the testing set.

For our BASS, we apply two \(2\)-layer FC networks for \(f_{1}(\cdot;\bm{\theta}_{1})\), \(f_{2}(\cdot;\bm{\theta}_{2})\) respectively, and set network width \(m=200\). For deriving approximated arm rewards, we let \(|\Omega_{k}^{\text{valid}}|=5\). Recall that we apply the approximation approach mentioned in **Remark 3** to reduce the space complexity and time complexity in practice for the experiments. Here, we tune the pooling step such that the inputs of \(f_{1}(\cdot;\bm{\theta}_{1}),f_{1}(\cdot;\bm{\theta}_{1})\) are approximately 50 and 20 respectively. For the learning rate, we find the learning rate for BASS with grid search from \(\{0.01,0.001,0.0001\}\), and choose the learning rates for the meta-model \(\eta_{1}=0.01,\eta_{2}=0.001\). The meta-model architecture as well as its learning rates will stay the same for all the baselines and our proposed BASS. For the CIFAR-100 and Mini-ImageNet data sets, we use the the meta-model with four convolutional blocks where the network width of each block is 32, followed by an FC layer as the output layer. For the Drug data set, we apply a meta-model with two FC layers, where the network width is 500. All the experiments are performed on a Linux machine with Intel Xeon CPU, 128GB RAM, and Tesla V100 GPU. Code will be made available at https://github.com/yunzhe0306/Bandit_Task_Scheduler.

### Effect of the Task Noise Magnitude

We conduct the experiments to show the effects of the noise magnitude factor \(\epsilon\) on the Drug and CIFAR-100 data sets. The experiment results are shown in **Table**4.

With increasing noise magnitude \(\epsilon\), the performances of the meta-model trained by baselines and our BASS tend to drop, which is intuitive. In particular, for the CIFAR-100 data set, when we increase \(\epsilon\), the performance difference between BASS and the other baselines tends to increase. This can be the reason that the greedy baselines with no exploration strategies can be more susceptible to the task noise perturbation, which can lead to the sub-optimal performances of the meta-model.

From the **Table**5, we can see that when there is no noise, the overall performance does not differ significantly across different methods. The reason could be that since the meta-learning backbone remains the same for all the methods, the meta-model performance upper bound can be similar for different scheduling algorithms, without the presence of other confounding factors (e.g., noise, task distribution skewness). In the practical application scenarios with noisy data, BASS-guided meta-models tend to perform well in presence of task noise and skewness compared with baselines, as presented by our experiments in the main body.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Algo. \(\backslash\) Data & Drug (0.3) & Drug (0.5) & CIFAR100 (0.3) & CIFAR100 (0.5) \\ \hline Uniform & 0.218\(\pm\)0.007 & 0.220\(\pm\)0.001 & 0.655\(\pm\)0.009 & 0.526\(\pm\)0.011 \\ SPL & 0.243\(\pm\)0.008 & 0.236\(\pm\)0.004 & 0.625\(\pm\)0.017 & 0.367\(\pm\)0.039 \\ FOCAL & 0.224\(\pm\)0.019 & 0.223\(\pm\)0.003 & 0.638\(\pm\)0.010 & 0.485\(\pm\)0.006 \\ DAML & 0.182\(\pm\)0.025 & 0.177\(\pm\)0.003 & 0.543\(\pm\)0.017 & 0.414\(\pm\)0.025 \\ GCP & N/A & N/A & 0.653\(\pm\)0.005 & 0.508\(\pm\)0.009 \\ PAML & 0.186\(\pm\)0.006 & 0.205\(\pm\)0.009 & 0.537\(\pm\)0.009 & 0.316\(\pm\)0.022 \\ ATS & 0.239\(\pm\)0.011 & 0.237\(\pm\)0.014 & 0.651\(\pm\)0.001 & 0.505\(\pm\)0.015 \\ \hline
**BASS (Ours)** & **0.258\(\pm\)0.003** & **0.245\(\pm\)0.006** & **0.657\(\pm\)0.005** & **0.553\(\pm\)0.008** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison with baselines with different noise magnitude [data set (noise magnitude \(\epsilon\)) ; final results \(\pm\) standard deviation].

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Data \(\backslash\) Algo. & Uniform & SPL. & FOCAL-LOSS & DAML. & GCP & PAML & ATS & BASS \\ \hline Drug & 0.206\(\pm\)0.012 & 0.234\(\pm\)0.006 & 0.240\(\pm\)0.003 & 0.190\(\pm\)0.002 & N/A & 0.220\(\pm\)0.010 & 0.233\(\pm\)0.001 & **0.256\(\pm\)0.003** \\ M-ImageNet & 0.576\(\pm\)0.016 & 0.554\(\pm\)0.004 & 0.582\(\pm\)0.005 & 0.437\(\pm\)0.015 & 0.564\(\pm\)0.002 & 0.467\(\pm\)0.007 & 0.561\(\pm\)0.004 & **0.586\(\pm\)0.008** \\ CIFAR & 0.681\(\pm\)0.010 & 0.681\(\pm\)0.008 & 0.692\(\pm\)0.023 & 0.662\(\pm\)0.027 & 0.681\(\pm\)0.016 & 0.640\(\pm\)0.011 & 0.695\(\pm\)0.035 & **0.697\(\pm\)0.029** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Experiment results of noise-free settings on three real data sets (5-way, 5-shot).

### Parameter Study for Exploration Coefficient

As in **Eq.**7 and **Eq.**9, BASS involves an exploration coefficient \(\alpha\) to balance the exploitation-exploration and the two exploration objectives. Here, we conduct the parameter study for the exploration coefficient \(\alpha\), and include the results with no exploration (i.e., removing \(f_{2}\)).

From the results in **Table**6, we see that the exploration module can indeed improve the performance of BASS compared with the performance with no exploration. This also fits our initial argument that the greedy algorithm alone can lead to sub-optimal performances of meta-model. By properly choosing the \(\alpha\) value, we will be able to achieve a good balance between exploitation and exploration, as well as between the two exploration objectives. Here, setting \(\alpha\in[0.5,0.7]\) will be good enough to achieve satisfactory performances. Meanwhile, we also note that even with no exploration, our BASS still achieves good performances by directly learning the correlation between the adapted meta-parameter and the generalization score, and refining the scheduling strategy based on the status of the meta-model.

### Running Time Comparison

In Figure5, we include the running time comparison with baselines. We can see that BASS can achieve significant improvement in terms of the running time, and can take as little as 50% of ATS's running time. The intuition is that our proposed BASS only needs one round of the optimization process to update the meta-model and BASS. On the other hand, from Algorithm1 of the ATS paper [51], we see that ATS requires two optimization rounds for each meta-training iteration to (1) update the scheduler with the temporal meta-model, and (2) update the actual meta-model respectively. Based on the figure on the RHS, we also see that BASS can achieve a relatively good balance between computational cost and performance.

### Performances with Different Task Skewness Settings

In Table7, we include the experiments with different levels of skewness. Here, we see that with less skewness levels (the skewness level reduces from Setting 1 to Setting 3), the accuracy of BASS as well as the baselines will continue to improve, while BASS still maintains decent performances.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Algo. \(\backslash\) Data & Drug (1) & Drug (5) & CIFAR100 (1) & CIFAR100 (5) \\ \hline No Exploration & 0.234\(\pm\)0.003 & 0.239\(\pm\)0.012 & 0.256\(\pm\)0.027 & 0.537\(\pm\)0.012 \\ \(\alpha=0.1\) & 0.231\(\pm\)0.005 & 0.233\(\pm\)0.013 & 0.264\(\pm\)0.051 & 0.522\(\pm\)0.024 \\ \(\alpha=0.3\) & 0.228\(\pm\)0.013 & 0.231\(\pm\)0.008 & 0.268\(\pm\)0.047 & 0.528\(\pm\)0.014 \\ \(\alpha=0.5\) & 0.236\(\pm\)0.004 & **0.245\(\pm\)0.006** & **0.272\(\pm\)0.025** & **0.553\(\pm\)0.008** \\ \(\alpha=0.7\) & **0.242\(\pm\)0.012** & 0.227\(\pm\)0.006 & 0.241\(\pm\)0.005 & 0.543\(\pm\)0.021 \\ \(\alpha=1.0\) & 0.236\(\pm\)0.002 & 0.235\(\pm\)0.013 & 0.266\(\pm\)0.006 & 0.537\(\pm\)0.005 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison among different \(\alpha\) values [dataset (shot) ; final results \(\pm\) standard deviation].

Figure 5: Running time results (including training both the scheduler and the meta-model). “D”, “M” and “C” refer to the ”Drug”, ”Mini-ImageNet”, ”CIFAR-100” data sets respectively. BASS can take as little as approximately 50% of ATS’s running time. On the RHS, we have the scatter plot in terms of running time vs. performance on the Mini-ImageNet dataset.

### Performances with Different Batch Size

With Table 8, we include additional experiments with different batch sizes \(B\), in comparison with the ATS and the uniform sampling approach. Here, we see that with larger \(B\) values, the accuracy of BASS as well as the baselines will generally improve.

### Performances with Different Embedding Approaches of Arm Contexts

In Table 9, we include additional experiments with different levels of average pooling, such that after the average pooling, the dimensionality of the pooled vector representation will fall into \(\{20,100,500\}\). We see that overly small dimensionality of the average-pooled vector representation (e.g., 20) can lead to sub-optimal performance of the BASS framework. In addition, we see that setting the dimensionality to 50 can generally lead to good enough performance.

Here, we also include additional experimental results using MLP to map the original context into the lower dimensional space instead of using our proposed average pooling (Remark 3). Results are shown in Table 10. Here, we use the one-layer MLP with the ReLU activation to embed the original meta-parameters to the low-dimensional vector representations. We can see that the MLP-based method can indeed lead to some performance improvement. But in general, the performance difference between MLP-based embedding and the average-pooling vector representation is subtle. We also note that the MLP-based mapping approach is more time consuming compared with the average pooling approach, since we also need to train the additional embedding layer, which has a considerable number of trainable parameters.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Skewness Setting \textbackslash{} Algo. & Uniform & ATS & **BASS** \\ \hline Skewness Setting 1 & 0.375\(\pm\)0.009 & 0.382\(\pm\)0.007 & 0.408\(\pm\)0.008 \\ Skewness Setting 2 & 0.429\(\pm\)0.012 & 0.448\(\pm\)0.006 & 0.460\(\pm\)0.013 \\ Skewness Setting 3 & 0.497\(\pm\)0.008 & 0.502\(\pm\)0.010 & 0.539\(\pm\)0.009 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results for different skewness levels on CIFAR-100 data set (5-shot). (1) Setting 1 is the original setting in paper Subsec. 5.2. (2) For Setting 2, we assign 5 tasks with 8% sampling probability, 5 tasks with 3%, and the rest of the tasks equally share the 45% probability. (3) For Setting 3, we assign 5 tasks with 5%, 5 tasks with 2%, while the rest of the tasks equally share the 65% probability.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Dimensionality & Original avg-pooled (50) & 50 & 100 & 200 \\ \hline Accuracy & 0.553\(\pm\)0.008 & 0.558\(\pm\)0.013 & 0.560\(\pm\)0.012 & 0.553\(\pm\)0.015 \\ \hline \hline \end{tabular}
\end{table}
Table 10: With CIFAR-100 (5-shot), different dimensionality of the one-layer MLP(with ReLU)-embedded vector representation of the meta-parameters. ”original avg-pooled (50)” refers to the average-pooled vector representation (Remark 3) with dimensionality of 50.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \(B\) (batch size) \textbackslash{} Algo. & Uniform & ATS & **BASS** \\ \hline
1 & 0.459\(\pm\)0.009 & 0.449\(\pm\)0.010 & 0.472\(\pm\)0.012 \\
2 & 0.526\(\pm\)0.011 & 0.515\(\pm\)0.015 & 0.553\(\pm\)0.008 \\
3 & 0.570\(\pm\)0.012 & 0.563\(\pm\)0.007 & 0.588\(\pm\)0.010 \\
5 & 0.581\(\pm\)0.005 & 0.571\(\pm\)0.007 & 0.586\(\pm\)0.009 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results for different \(B\) values (batch sizes) on CIFAR-100 data set (5-shot).

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Dimensionality & 20 & 50 & 100 & 500 \\ \hline Accuracy & 0.541\(\pm\)0.008 & 0.553\(\pm\)0.008 & 0.558\(\pm\)0.006 & 0.555\(\pm\)0.010 \\ \hline \hline \end{tabular}
\end{table}
Table 9: With CIFAR-100 (5-shot), different dimensionality of the average-pooled vector representation (Remark 3) of the meta-parameters.

### Additional Experiments on the "DomainNet" data set

In Table 11, we include additional experiments on the new "DomainNet" data set [37]. Within the "real" domain, we filter 100 classes that have at least 600 images. In this way, with each class being a task with 600 images, we will have a total of 100 tasks. Compared with image data sets in our paper (Mini-ImageNet and CIFAR-100), we increase the image resolution of "DomainNet" by resizing its images to 128\(\times\)128 pixels. Following the settings in our paper, we divide tasks into \(64:16:20\) portions that correspond to the training set, validation set and the test set respectively. For the few-shot settings, we formulate the problem to be 5-shot, 5-way / 7-way with uniform sampling and ATS as baselines. With a higher image resolution of the "DomainNet" data set, BASS can still maintain the good performance compared with the baselines.

## Appendix B Appendix: Additional Discussion on the Necessity of Assumption 5.1

We would like to mention that in order to finish the convergence and generalization analysis for the neural Contextual Bandit works (e.g., [54; 2; 9]), the separateness assumption of the arm context is the minimum requirement of the data set. This is because the training data needs to be non-degenerate (i.e., every pairs of samples are distinct) to ensure that the neural network can consistently converge, as indicated by Assumption 2.1 in [3]. Therefore, our Assumption 5.1 regarding the arm separateness aims to ensure that the BASS is able to adequately learn the underlying reward mapping function with sufficient information. Comparing with the existing works, in the convergence analysis works on meta-learning [47; 48], they measure the arm separateness in terms of the minimum eigenvalue \(\lambda_{0}\) (with \(\lambda_{0}>0\)) of the Neural Tangent Kernel (NTK) [23] matrix, which is comparable with our Euclidean separateness \(\rho\). For existing neural bandit works, Assumption 5.1 in [9] is similar to our separateness assumption. Meanwhile, Assumption 4.2 in [54] and Assumption 3.4 from [53] also imply that no two arms are the same in terms of the minimum NTK matrix eigenvalue \(\lambda_{0}>0\).

## Appendix C Appendix: Limitation

One potential limitation of BASS is that its improvement over baselines may not be significant when dealing with noise-free settings and non-skewed task distributions (**Table**5). Meanwhile, the non-adaptive FOCAL-LOSS [30] tends to achieve a similar performance comparing with the adaptive method ATS [51], while enjoying an advantage in terms of the computational cost. In practical terms, although BASS can generally achieve the decent performance and enjoys a smaller computational cost than ATS, the practitioner still needs to consider whether their task distribution is noisy or skewed in order to strike a good balance between the computational resource needed and the meta-model performance, as BASS can achieve a more significant advantage over baselines given the noisy or skewed task distribution.

## Appendix D Appendix: Theoretical Analysis

In this section, we present the proof for **Theorem**5.2. Here, instead of directly going for the batch setting where we adopt training task batch \(\Omega_{k}\) for each iteration \(k\in[K]\) (\(|\Omega_{k}|=|\Omega_{k}^{*}|=B\)), we first introduce the results of the single-task setting (Subsec. D.1), i.e., \(|\Omega_{k}|=|\Omega_{k}^{*}|=1\). Then, the results will be extended to the batch settings as in Subsec. D.2. Recall that for the meta-model, we first consider it to be a \(L_{\mathcal{F}}\)-layer fully-connected (FC) network (of width \(m_{\mathcal{F}}\) for the theoretical analysis (lines 237-239). In particular, we follow the settings in [3] for the Gaussian initialization of weight matrices. For the weight matrix elements in meta-model's first \((L_{\mathcal{F}}-1)\) layers, we draw each of them from the Gaussian distribution \(\mathcal{N}(0,2/m_{\mathcal{F}})\). Then, for the weight matrix elements of the last layer (\(L_{\mathcal{F}}\)-th layer), we draw each of them from the Gaussian distribution \(\mathcal{N}(0,1)\).

\begin{table}
\begin{tabular}{c c c c} \hline \hline Setting \(\backslash\) Algo. & Uniform & ATS & **BASS** \\ \hline
5-way & 0.475\(\pm\)0.002 & 0.483\(\pm\)0.006 & 0.511\(\pm\)0.012 \\
7-way & 0.411\(\pm\)0.005 & 0.372\(\pm\)0.009 & 0.435\(\pm\)0.008 \\ \hline \hline \end{tabular}
\end{table}
Table 11: Results for the ”DomainNet” data set (noise level 0.5, 5-shot settings).

### Single-task settings

For the brevity of notation, we denote the scheduler output \(f(\bm{\Theta}^{(K-1)}[\mathcal{T}_{k,i}];\bm{\theta}^{(k-1)})=f_{1}(\bm{\chi}_{k, i}^{\circ};\bm{\theta}_{1}^{(k-1)})+f_{2}\big{(}[\nabla_{\bm{\theta}}f_{1}(\bm{ \chi}_{k,i}^{\circ});\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{k,i}^{\circ})];\bm{ \theta}_{2}^{(k-1)}\big{)},\) which corresponds to the definition in **Eq.**7. In this case, \(\mathcal{T}(K)=\{\mathcal{T}_{1},\ldots,\mathcal{T}_{K}\}\) refer to the chosen tasks and \(\mathcal{T}^{*}(K)=\{\mathcal{T}_{1}^{*},\ldots,\mathcal{T}_{K}^{*}\}\) are the optimal ones. Based on the problem definition, we will have

\[R_{\text{single}}(K)=\mathbb{E}_{\mathcal{T}\sim\mathcal{P}( \mathcal{T}),\bm{x}\sim\mathcal{D}_{\mathcal{T}}}\bigg{[}\mathcal{L}\big{(} \bm{x};\mathcal{I}(\mathcal{T},\bm{\Theta}^{(K)})\big{)}\bigg{]}-\mathbb{E}_{ \mathcal{T}\sim\mathcal{P}(\mathcal{T}),\bm{x}\sim\mathcal{D}_{\mathcal{T}}} \bigg{[}\mathcal{L}\big{(}\bm{x};\mathcal{I}(\mathcal{T},\bm{\Theta}^{(K-1),* }[\mathcal{T}_{K}^{*}])\big{)}\bigg{]}\] \[=h(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}])-h(\bm{\Theta}^{(K- 1)}[\mathcal{T}_{K}])\] \[=h(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}])-h(\bm{\chi}_{K}^{ *};\tilde{\bm{\theta}}^{(K-1)})+f(\bm{\chi}_{K}^{*};\tilde{\bm{\theta}}^{(K-1 )})-f(\bm{\chi}_{K};\bm{\theta}^{(K-1)})\] \[\qquad\qquad+f(\bm{\chi}_{K};\bm{\theta}^{(K-1)})-h(\bm{\Theta}^{ (K-1)}[\mathcal{T}_{K}])\] \[\leq h(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}])-f(\bm{\chi}_{K} ^{*};\tilde{\bm{\theta}}^{(K-1)})+f(\bm{\chi}_{K}^{*};\tilde{\bm{\theta}}^{(K- 1)})-f(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}^{*}];\bm{\theta}^{(K-1)})\] \[\qquad\qquad+f(\bm{\chi}_{K};\bm{\theta}^{(K-1)})-h(\bm{\Theta}^{ (K-1)}[\mathcal{T}_{K}])\] \[=h(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}])-f(\bm{\Theta}^{(K -1),*}[\mathcal{T}_{K}^{*}];\tilde{\bm{\theta}}^{(K-1)})+f(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}];\tilde{\bm{\theta}}^{(K-1)})-f(\bm{\Theta}^{(K-1)}[ \mathcal{T}_{K}^{*}];\bm{\theta}^{(K-1)})\] \[\qquad\qquad+f(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}];\bm{\theta}^ {(K-1)})-h(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}])\] \[\leq|h(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}])-f(\bm{\Theta} ^{(K-1),*}[\mathcal{T}_{K}^{*}];\tilde{\bm{\theta}}^{(K-1)})|+\underbrace{|f (\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}];\tilde{\bm{\theta}}^{(K-1)})-f( \bm{\Theta}^{(K-1)}[\mathcal{T}_{K}^{*}];\bm{\theta}^{(K-1)})|}_{I_{0}}\] \[\qquad\qquad+|f(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}];\bm{\theta} ^{(K-1)})-h(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}])|\]

where the first inequality is due to the arm pulling mechanism, i.e., \(f(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}^{*}];\bm{\theta}^{(K-1)})\leq f(\bm{ \Theta}^{(K-1)}[\mathcal{T}_{K}];\bm{\theta}^{(K-1)})\). Here, \(f(\cdot;\tilde{\bm{\theta}}^{(K-1)})\) is defined as the "shadow" bandit model that are trained on optimal tasks \(\{\mathcal{T}_{1}^{*},\mathcal{T}_{2}^{*},\ldots,\mathcal{T}_{K-1}^{*}\}\) and the corresponding meta-model parameters. Here, denote \(\bm{\chi}_{K}=\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}]\in\mathbb{R}^{p}\) as the arm context given the arm \(\mathcal{T}_{K}\) and the meta-model parameter \(\bm{\Theta}^{(K-1)}\); similarly, we have \(\bm{\chi}_{K}^{*}=\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}]\in\mathbb{R}^{p}\) being the arm context given the arm \(\mathcal{T}_{K}^{*}\) and the meta-model parameter \(\bm{\Theta}^{(K-1),*}\). Thus, for the term \(I_{0}\) on the RHS, we have

\[I_{0} =|f(\chi^{*};\tilde{\bm{\theta}}^{(K-1)})-f(\bm{\Theta}^{(K-1)}[ \mathcal{T}_{K}^{*}];\bm{\theta}^{(K-1)})|\] \[=|f(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}];\tilde{\bm{ \theta}}^{(K-1)})-f(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}];\bm{\theta}^{(K-1 )})\] \[\qquad\qquad\qquad\qquad+f(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{ *}];\bm{\theta}^{(K-1)})-f(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}^{*}];\bm{\theta}^{ (K-1)})|\] \[\leq|f(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}];\tilde{\bm{ \theta}}^{(K-1)})-f(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}];\bm{\theta}^{(K-1 )})|\] \[\qquad\qquad\qquad\qquad+|f(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{ *}];\bm{\theta}^{(K-1)})-f(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}^{*}];\bm{ \theta}^{(K-1)})|.\]

Then, inserting the inequality will lead to

\[R(K) \leq\underbrace{|h(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}])-f(\chi_{K };\bm{\theta}^{(K-1)})|}_{I_{1}}+\underbrace{|f(\chi_{K}^{*};\tilde{\bm{ \theta}}^{(K-1)})-h(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}])|}_{I_{2}}\] \[\qquad+\underbrace{|f(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}]; \tilde{\bm{\theta}}^{(K-1)})-f(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}];\bm{ \theta}^{(K-1)})|}_{I_{3}}\] \[\qquad+\underbrace{|f(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}]; \bm{\theta}^{(K-1)})-f(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}^{*}];\bm{\theta}^{(K- 1)})|}_{I_{4}}.\]

Here, the terms \(I_{1},I_{2}\) refer to the approximation error for the two bandit models (our possessed model \(f(\cdot;\bm{\theta}^{(K-1)})\) and the pseudo model \(f(\cdot;\tilde{\bm{\theta}}^{(K-1)})\)). Then, the third term \(I_{3}\) bounds the output difference when given the same input \(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}]\) to two separate bandit models, and the finalterm \(I_{4}\) refers to the difference of the meta-model parameters when adapted to the same task with two individual sets of parameters. Here, the terms \(I_{1},I_{2}\) can be bounded by **Lemma** D.1, **Corollary** D.2. Then, the point is to bound the difference term \(I_{4}\) when given different inputs to the bandit model.

#### d.1.1 Bounding error terms and assembling the regret bound

**[Bounding term \(I_{3}\)]** For error term \(I_{3}\), it focuses on bounding the output difference between two bandit models \(f(\cdot;\tilde{\bm{\theta}}^{(K-1)}),f(\cdot;\bm{\theta}^{(K-1)})\) given the same input \(\bm{\Theta}^{(K-1),*}[\mathcal{T}^{*}_{K}]\), and we have

\[I_{3} =|f(\bm{\Theta}^{(K-1),*}[\mathcal{T}^{*}_{K}];\tilde{\bm{\theta}} ^{(K-1)})-f(\bm{\Theta}^{(K-1),*}[\mathcal{T}^{*}_{K}];\bm{\theta}^{(K-1)})|=|f (\bm{\chi}^{*}_{K};\tilde{\bm{\theta}}^{(K-1)})-f(\bm{\chi}^{*}_{K};\bm{\theta }^{(K-1)})\] \[\leq\underbrace{|f_{1}(\bm{\chi}^{*}_{K};\tilde{\bm{\theta}}^{(K -1)}_{1})-f_{1}(\bm{\chi}^{*}_{K};\bm{\theta}^{(K-1)}_{1})|}_{I_{3.1}}\] \[\qquad+\underbrace{|f_{2}\bigg{(}[\nabla_{\tilde{\bm{\theta}}}f_{ 1}(\bm{\chi}^{*,*}_{K});\;\nabla_{\tilde{\bm{\theta}}}f_{1}(\bm{\chi}^{q,*}_{K })];\tilde{\bm{\theta}}^{(K-1)}_{2}\bigg{)}-f_{2}\bigg{(}[\nabla_{\bm{\theta} }f_{1}(\bm{\chi}^{*,*}_{K});\;\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{q,*}_{K})]; \bm{\theta}^{(K-1)}_{2}\bigg{)}|}_{I_{3.2}}.\]

With the defined \(\xi_{L}\), applying **Lemma** D.11 as well as **Corollary** D.12, we will have

\[I_{3.1}\leq\bigg{(}1+\mathcal{O}(\frac{KL^{3}\log^{5/6}(m)}{\rho^{1/3}m^{1/6}} )\bigg{)}\cdot\mathcal{O}(\frac{K^{3}L}{\rho\sqrt{m}}\log(m))+\mathcal{O} \bigg{(}\frac{K^{4}L^{2}\log^{11/6}(m)}{\rho^{4/3}m^{1/6}}\bigg{)}\]

Then, for term \(I_{3.2}\), we have

\[I_{3.2}=|f_{2}\bigg{(}[\nabla_{\tilde{\bm{\theta}}}f_{1}(\bm{ \chi}^{*,*}_{K});\;\nabla_{\tilde{\bm{\theta}}}f_{1}(\bm{\chi}^{q,*}_{K})]; \tilde{\bm{\theta}}^{(K-1)}_{2}\bigg{)}-f_{2}\bigg{(}[\nabla_{\bm{\theta}}f_{ 1}(\bm{\chi}^{*,*}_{K});\;\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{q,*}_{K})]; \bm{\theta}^{(K-1)}_{2}\bigg{)}|\] \[\leq|f_{2}\bigg{(}[\nabla_{\tilde{\bm{\theta}}}f_{1}(\bm{\chi}^{ s,*}_{K});\;\nabla_{\tilde{\bm{\theta}}}f_{1}(\bm{\chi}^{q,*}_{K})];\tilde{\bm{ \theta}}^{(K-1)}_{2}\bigg{)}-f_{2}\bigg{(}[\nabla_{\tilde{\bm{\theta}}}f_{1}( \bm{\chi}^{s,*}_{K});\;\nabla_{\tilde{\bm{\theta}}}f_{1}(\bm{\chi}^{q,*}_{K})]; \bm{\theta}^{(K-1)}_{2}\bigg{)}|\] \[\qquad\qquad+|f_{2}\bigg{(}[\nabla_{\tilde{\bm{\theta}}}f_{1}( \bm{\chi}^{s,*}_{K});\;\nabla_{\tilde{\bm{\theta}}}f_{1}(\bm{\chi}^{q,*}_{K})]; \bm{\theta}^{(K-1)}_{2}\bigg{)}-f_{2}\bigg{(}[\nabla_{\bm{\theta}}f_{1}(\bm{ \chi}^{s,*}_{K});\;\nabla_{\tilde{\bm{\theta}}}f_{1}(\bm{\chi}^{q,*}_{K})]; \bm{\theta}^{(K-1)}_{2}\bigg{)}|.\]

Here, for the first term on the RHS, we apply **Lemma** D.11 as well as **Corollary** D.12 to bound.

Then, for the second term, with Gaussian initialization of weight matrices, for the over-parameterized FC network \(f\) with Lipschitz-smooth activation functions (e.g., Sigmoid), we can have \(|f(\bm{x})-f(\bm{x}^{\prime})|,\|\nabla f(\bm{x})-\nabla f(\bm{x}^{\prime})\| \leq\xi\cdot\|\bm{x}-\bm{x}^{\prime}\|\) due to its Lipschitz continuity / smoothness property [47, 18]. Meanwhile, we also have the Lipschitz continuity property for over-parameterized FC network \(f^{\prime}\) with ReLU activation [3], such that \(|f^{\prime}(\bm{x})-f^{\prime}(\bm{x}^{\prime})|\leq\xi^{\prime}\cdot\|\bm{x}- \bm{x}^{\prime}\|\). By the Gaussian initialization of BASS's weight matrices and the properties of over-parameterized neural networks [3, 47, 18], we have \(\xi_{L}=\max\{\xi,\xi^{\prime}\}\leq\mathcal{O}(c_{\xi}^{L})\) being the Lipschitz constant for our \(f_{1},f_{2}\), where \(c_{\xi}>1\) is a small constant. Applying the conclusion above, we will have

\[\big{|}f_{2}\bigg{(}[\nabla_{\tilde{\bm{\theta}}}f_{1}(\bm{\chi}^ {s,*}_{K});\;\nabla_{\tilde{\bm{\theta}}}f_{1}(\bm{\chi}^{q,*}_{K})];\bm{ \theta}^{(K-1)}_{2}\bigg{)}-f_{2}\bigg{(}[\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{ s,*}_{K});\;\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{q,*}_{K})];\bm{\theta}^{(K-1)}_{2} \bigg{)}\big{|}\] \[\qquad\leq\xi_{L}\cdot\big{\|}[\nabla_{\tilde{\bm{\theta}}}f_{1}( \bm{\chi}^{s,*}_{K});\;\nabla_{\tilde{\bm{\theta}}}f_{1}(\bm{\chi}^{q,*}_{K})]- \xi_{L}\cdot[\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{s,*}_{K});\;\nabla_{\bm{ \theta}}f_{1}(\bm{\chi}^{q,*}_{K})]\big{\|}\] \[\qquad\leq\xi_{L}\cdot\big{\|}\nabla_{\tilde{\bm{\theta}}}f_{1}( \bm{\chi}^{s,*}_{K})-\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{s,*}_{K})\big{\|}+\xi_ {L}\cdot\big{\|}\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{q,*}_{K})-\nabla_{\tilde{ \bm{\theta}}}f_{1}(\bm{\chi}^{q,*}_{K})\big{\|}\] \[\qquad\leq\xi_{L}\cdot\frac{KL^{4}\log^{5/6}(m)}{\rho^{1/3}m^{1/6}}\]

where the last inequality is by Theorem 5 in [3] and the proof of **Lemma** D.11. With the above results, it will give us

\[I_{3}\leq\bigg{(}1+\mathcal{O}(\frac{KL^{3}\log^{5/6}(m)}{\rho^{1/3}m^{1/6}}) \bigg{)}\mathcal{O}(\frac{K^{3}L}{\rho\sqrt{m}}\log(m))+\mathcal{O}\bigg{(} \frac{K^{4}L^{2}\log^{11/6}(m)}{\rho^{4/3}m^{1/6}}\bigg{)}+\frac{\xi_{L}KL^{4} \log^{5/6}(m)}{\rho^{1/3}m^{1/6}}\]

**[Bounding term \(I_{4}\)]** On the other hand, applying the analogous procedure for term \(I_{4}\), denoting \(\bm{\chi}_{K}^{*}=\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}]\) and \(\bar{\bm{\chi}}_{K}^{*}=\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}^{*}]\) for the brevity of notation, we can have

\[I_{4} =|f(\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}];\bm{\theta}^{(K-1) })-f(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}^{*}];\bm{\theta}^{(K-1)})|\] \[\leq\underbrace{\xi_{L}\cdot\|\bm{\Theta}^{(K-1),*}[\mathcal{T}_ {K}^{*}]-\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}^{*}]\|_{2}}_{I_{4,1}}\] \[\qquad+\underbrace{|f_{2}\bigg{(}[\nabla_{\bm{\theta}}f_{1}(\bm{ \chi}_{K}^{*,*});\ \nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{K}^{*,*})];\bm{\theta}_{2}^{(K-1)} \bigg{)}-f_{2}\bigg{(}[\nabla_{\bm{\theta}}f_{1}(\bar{\bm{\chi}}_{K}^{*,*}); \ \nabla_{\bm{\theta}}f_{1}(\bar{\bm{\chi}}_{K}^{q,*})];\bm{\theta}_{2}^{(K-1)} \bigg{)}|}_{I_{4,2}}.\]

where \(\bm{\chi}_{K}^{s,*},\bm{\chi}_{K}^{q,*}\) respectively represents the support set and query set for task \(\mathcal{T}_{K}^{*}\) and the meta-parameters \(\bm{\Theta}^{(K-1),*}\). Similar notation also applies to \(\bar{\bm{\chi}}_{K}^{*}=\bm{\Theta}^{(K-1)}[\mathcal{T}_{K}^{*}]\). And the inequality is due to the fact that ReLU networks are naturally Lipschitz continuous w.r.t. some coefficient \(\xi_{L}\) when they are wide enough [3], as we have discussed above.

**[Bounding term \(I_{4.1}\)]** Based on the meta-optimization procedure (inner-loop optimization + outer-loop optimization), we have

\[I_{4.1} =\xi_{L}\cdot\|\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}]-\bm{ \Theta}^{(K-1)}[\mathcal{T}_{K}^{*}]\|_{2}\] \[=\xi_{L}\cdot\|\bigg{(}\bm{\Theta}^{(K-1),*}-\eta_{2}\cdot\nabla_ {\bm{\Theta}}\mathcal{L}(D_{K}^{q,*};\bm{\Theta}_{K}^{(J),*})\bigg{)}-\bigg{(} \bm{\Theta}^{(K-1)}-\eta_{2}\cdot\nabla_{\bm{\Theta}}\mathcal{L}(D_{K}^{q,*}; \bm{\Theta}_{K}^{(J)})\bigg{)}\|_{2}\]

where \(\bm{\Theta}_{K}^{(J),*}\) is the task-specific parameter of \(\mathcal{T}_{K}^{*}\) after adapting on \(\bm{\Theta}^{(K-1),*}\) with inner-loop optimization, and the \(\bm{\Theta}_{K}^{(J)}\) is the similar parameter after adapting on \(\bm{\Theta}^{(K-1)}\). Here, we simplify the formula by representing the gradient derivation (inner-loop + outer-loop) with the mapping \(H:\mathcal{T}\times\bm{\Theta}\mapsto\mathbb{R}^{p}\), which leads to

\[\|\bm{\Theta}^{(K-1),*}[\mathcal{T}_{K}^{*}]-\bm{\Theta}^{(K-1)}[ \mathcal{T}_{K}^{*}]\|_{2}\] \[=\|\bigg{(}\bm{\Theta}^{(K-1),*}-\eta_{2}\cdot\nabla_{\bm{ \Theta}}\mathcal{L}(D^{q};\bm{\Theta}_{K}^{(J),*})\bigg{)}-\bigg{(}\bm{\Theta }^{(K-1)}-\eta_{2}\cdot\nabla_{\bm{\Theta}}\mathcal{L}(D^{q};\bm{\Theta}_{K}^ {(J)})\bigg{)}\|_{2}\] \[=\|(\bm{\Theta}^{(K-1),*}-\bm{\Theta}^{(K-1)})-\eta_{2}\cdot \bigg{(}H(\mathcal{T}_{K}^{*},\bm{\Theta}^{(K-1),*})-H(\mathcal{T}_{K}^{*}, \bm{\Theta}^{(K-1)})\bigg{)}\|_{2}\] \[=\|(\bm{\Theta}^{(K-2),*}-\bm{\Theta}^{(K-2)})-\eta_{2}\cdot \bigg{(}H(\mathcal{T}_{K}^{*},\bm{\Theta}^{(K-1),*})-H(\mathcal{T}_{K}^{*}, \bm{\Theta}^{(K-1)})\bigg{)}\] \[\qquad-\eta_{2}\cdot\bigg{(}H(\mathcal{T}_{K-1}^{*},\bm{\Theta} ^{(K-2),*})-H(\mathcal{T}_{K-1}^{*},\bm{\Theta}^{(K-2)})\bigg{)}\|_{2}\] \[\leq\sum_{k\in[K]}\eta_{2}\cdot\big{\|}H(\mathcal{T}_{k}^{*},\bm{ \Theta}^{(k-1),*})-H(\mathcal{T}_{k}^{*},\bm{\Theta}^{(k-1)})\big{\|}_{2}\]

Recall that the past arms, including the actual chosen arms \(\{\mathcal{T}_{1},\mathcal{T}_{2},\dots,\mathcal{T}_{K}\}\) as well as the optimal ones \(\{\mathcal{T}_{1}^{*},\mathcal{T}_{2}^{*},\dots,\mathcal{T}_{K}^{*}\}\) are all from the candidate pool where each candidate arm is drawn i.i.d. from the task distribution \(\mathcal{P}(\mathcal{T})\). Therefore, denoting the bound as \(\|H(\mathcal{T}_{K}^{*},\bm{\Theta}^{(K-1),*})-H(\mathcal{T}_{K}^{*},\bm{ \Theta}^{(K-1)})\|_{2}\leq S_{1}(K),\) we can have the upper bound as \(I_{4.1}\leq\eta_{2}\cdot\xi_{L}K\cdot S_{1}(K).\)

Then, for the term \(S_{1}(K)\), by definition we have \(\|H(\mathcal{T}_{K}^{*},\bm{\Theta}^{(K-1),*})-H(\mathcal{T}_{K}^{*},\bm{ \Theta}^{(K-1)})\|\leq S_{1}(K),\) applying mean-reduction for the sample loss will further leads to

\[\|H(\mathcal{T}_{K}^{*},\bm{\Theta}^{(K-1),*}) -H(\mathcal{T}_{K}^{*},\bm{\Theta}^{(K-1)})\|=\|\nabla_{\bm{ \Theta}}\mathcal{L}(D_{K}^{q,*};\bm{\Theta}_{K}^{(J),*})-\nabla_{\bm{\Theta}} \mathcal{L}(D_{K}^{q,*};\bm{\Theta}_{K}^{(J)})\|_{2}\] \[=\|\frac{1}{|D_{K}^{q,*}|}\sum_{\bm{x}\in D_{K}^{q,*}}\nabla_{\bm{ \Theta}}\mathcal{L}(\bm{x};\bm{\Theta}_{K}^{(J),*})-\frac{1}{|D_{K}^{q,*}|}\sum_ {\bm{x}\in D_{K}^{q,*}}\nabla_{\bm{\Theta}}\mathcal{L}(\bm{x};\bm{\Theta}_{K}^ {(J)})\|_{2}\] \[\leq\frac{1}{|D_{K}^{q,*}|}\sum_{\bm{x}\in D_{K}^{q,*}}\|\nabla_{ \bm{\Theta}}\mathcal{L}(\bm{x};\bm{\Theta}_{K}^{(J),*})-\nabla_{\bm{\Theta}} \mathcal{L}(\bm{x};\bm{\Theta}_{K}^{(J)})\|_{2}.\]

This inequality essentially bound the gradient difference when given the same input task \(\mathcal{T}_{K}^{*}\) w.r.t. different sets of model parameters. Based on the conclusion from Lemma 9 of [47] and LemmaB.3 of [11], we have \(\|\nabla_{\bm{\Theta}_{i}}f(x;\bm{\Theta}_{K})\|_{F},\|\nabla_{\bm{\Theta}_{i}} \mathcal{L}(x;\bm{\Theta}_{K})\|_{F}\leq\mathcal{O}(\sqrt{m_{\mathcal{F}}}), \forall l\in[L_{\mathcal{F}}]\) for any set of parameters within the sphere \(\bm{\Theta}_{K}\in\mathcal{B}(\bm{\Theta}_{0},\omega)\) where \(\bm{\Theta}_{0}\) is the center and \(\omega\) is the corresponding radius (which is a small value). With a total of \(L_{\mathcal{F}}\) layers for the meta-model and each layer of \(m_{\mathcal{F}}\) hidden units, this will give us \(\|\nabla_{\bm{\Theta}}\mathcal{L}(\bm{x};\bm{\Theta}_{K}^{(J),*})\|_{2},\| \nabla_{\bm{\Theta}}\mathcal{L}(\bm{x};\bm{\Theta}_{K}^{(J)})\|_{2}\leq \mathcal{O}(\sqrt{m_{\mathcal{F}}L_{\mathcal{F}}})\) (**Lemma** D.15). And this makes \(S_{1}(K)\leq\mathcal{O}(\sqrt{m_{\mathcal{F}}L_{\mathcal{F}}})\). Since we have \(\eta_{1},\eta_{2}\leq\mathcal{O}(\frac{1}{m_{\mathcal{F}}})\), summarizing the results above, the upper bound can then be derived.

**[Bounding term \(I_{4.2}\)]** Next, we proceed to bound \(I_{4.2}\), which will be

\[I_{4.2} =|f_{2}\bigg{(}[\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{K}^{s,*}); \;\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{K}^{q,*})];\bm{\theta}_{2}^{(K-1)}\bigg{)} -f_{2}\bigg{(}[\nabla_{\bm{\theta}}f_{1}(\bar{\bm{\chi}}_{K}^{s,*});\;\nabla_{ \bm{\theta}}f_{1}(\bar{\bm{\chi}}_{K}^{q,*})];\bm{\theta}_{2}^{(K-1)}\bigg{)}|\] \[\leq\xi_{L}\cdot\big{\|}\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{K}^{ s,*});\;\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{K}^{q,*})\big{]}-[\nabla_{\bm{ \theta}}f_{1}(\bar{\bm{\chi}}_{K}^{s,*});\;\nabla_{\bm{\theta}}f_{1}(\bar{\bm {\chi}}_{K}^{q,*})]\big{\|}_{2}\] \[\leq\xi_{L}^{2}\cdot\big{\|}\nabla_{K}^{s,*}-\bar{\bm{\chi}}_{K}^ {s,*}\big{\|}_{2}+\xi_{L}^{2}\cdot\big{\|}\bm{\chi}_{K}^{s,*}-\bar{\bm{\chi}}_ {K}^{q,*}\big{\|}_{2}\]

where the inequalities are due to the Lipschitz continuity / smoothness properties of over-parameterized FC networks as we discussed above. Here, we notice that the second term on the RHS can be bounded by directly applying the proving procedure of term \(I_{4.1}\). Then, for the first term on the RHS, we can following a similar procedure as for \(I_{4.1}\), by

\[\big{\|}\bm{\chi}_{K}^{s,*}-\bar{\bm{\chi}}_{K}^{s,*}\big{\|}_{2} =\big{\|}\mathcal{I}(\mathcal{T}_{k}^{*},\;\bm{\Theta}^{(k-1),*})- \mathcal{I}(\mathcal{T}_{k}^{*},\;\bm{\Theta}^{(k-1)})\big{\|}_{2}\] \[=\|\bigg{(}\bm{\Theta}^{(K-1),*}-\eta_{1}\cdot\sum_{j\in[J]} \nabla_{\bm{\Theta}}\mathcal{L}(D_{K}^{s,*};\bm{\Theta}_{K}^{(j),*})\bigg{)}- \bigg{(}\bm{\Theta}^{(K-1)}-\eta_{1}\cdot\sum_{j\in[J]}\nabla_{\bm{\Theta}} \mathcal{L}(D_{K}^{s,*};\bm{\Theta}_{K}^{(j)})\bigg{)}\|_{2}\] \[=\|\big{(}\bm{\Theta}^{(K-2),*}-\bm{\Theta}^{(K-2)})\big{)}-\big{(} \eta_{1}\cdot\sum_{j\in[J]}\nabla_{\bm{\Theta}}\mathcal{L}(D_{K-1}^{s,*};\bm{ \Theta}_{K-1}^{(j)})-\eta_{1}\cdot\sum_{j\in[J]}\nabla_{\bm{\Theta}}\mathcal{ L}(D_{K-1}^{s,*};\bm{\Theta}_{K-1}^{(j),*}\big{)}\] \[\qquad\qquad-\big{(}\eta_{1}\cdot\sum_{j\in[J]}\nabla_{\bm{\Theta }}\mathcal{L}(D_{K}^{s,*};\bm{\Theta}_{K}^{(j)})-\eta_{1}\cdot\sum_{j\in[J]} \nabla_{\bm{\Theta}}\mathcal{L}(D_{K}^{s,*};\bm{\Theta}_{K}^{(j),*})\|_{2}\] \[\leq\eta_{1}\cdot\sum_{k\in[K]}\|\sum_{j\in[J]}\nabla_{\bm{\Theta }}\mathcal{L}(D_{k}^{s,*};\bm{\Theta}_{K}^{(j)})-\eta_{1}\cdot\sum_{j\in[J]} \nabla_{\bm{\Theta}}\mathcal{L}(D_{k}^{s,*};\bm{\Theta}_{k}^{(j),*})\|_{2}\] \[\leq\mathcal{O}(\eta_{1}\cdot KJ\sqrt{m_{\mathcal{F}}L_{\mathcal{F }}})\]

where the last inequality is due to **Lemma** D.15 and by iterating through \(K\) meta-training iterations. Summing up the results above, we will have \(I_{4.2}\leq\mathcal{O}(\eta_{2}\xi_{L}^{2}\cdot K\sqrt{m_{\mathcal{F}}L_{ \mathcal{F}}})+\mathcal{O}(\eta_{1}\xi_{L}^{2}\cdot KJ\sqrt{m_{\mathcal{F}}L_{ \mathcal{F}}})\).

**[Summing up the results]** Then, combining all the results, we would have

\[R_{\text{single}}(K)\leq\mathcal{O}(\frac{1}{\sqrt{K}})\cdot\bigg{(}\sqrt{2 \xi_{1}}+\frac{3L}{\sqrt{2}}+(1+2\gamma_{1})\sqrt{2\log(\frac{K}{\delta})} \bigg{)}+\mathcal{O}(\frac{\xi_{L}^{2}KJ\sqrt{L_{\mathcal{F}}}}{\sqrt{m_{ \mathcal{F}}}})+\gamma_{m}\]

where

\[\gamma_{1}=2+\mathcal{O}\left(\frac{K^{3}L}{\rho\sqrt{m}}\log m \right)+\mathcal{O}\left(\frac{L^{2}K^{4}}{\rho^{4/3}m^{1/6}}\log^{11/6}(m)\right)\] \[\gamma_{m}=\bigg{(}1+\mathcal{O}(\frac{KL^{3}\log^{5/6}(m)}{\rho ^{1/3}m^{1/6}})\bigg{)}\mathcal{O}(\frac{K^{3}L}{\rho\sqrt{m}}\log(m))+ \mathcal{O}\bigg{(}\frac{K^{4}L^{2}\log^{11/6}(m)}{\rho^{4/3}m^{1/6}}\bigg{)}+ \frac{\xi_{L}KL^{4}\log^{5/6}(m)}{\rho^{1/3}m^{1/6}}\]

Note that the majority of the terms above can be cancelled to \(\mathcal{O}(1)\) with proper networks width \(m\) indicated in **Theorem** 5.2. With increasingly large network width \(m\), these terms will also become diminutive enough to achieve our regret bound in the main body.

### Extending the result to the batch settings (Proof of Theorem 5.2)

With the results and conclusions from Subsection D.1, we proceed to provide the proof of **Theorem** 5.2 under the batch settings. Recall that in our original problem formulation and Algorithm 1, we are expected to select a batch of \(B\) arms in each meta-training iteration, denoted by \(\{\Omega_{k}\}_{k\in[K]}\). Note that each of the candidate arms from \(\Omega_{\text{task}}^{(k)}\) are drawn i.i.d. from the task distribution \(\mathcal{P}(\mathcal{T})\). Meantime,we will have the corresponding optimal arm batches, denoted by \(\{\Omega_{k}^{*}\}_{k\in[K]}\), which minimizes the loss objective in **Eq. 5**. Recall that we update the meta-model parameters with

\[\bm{\Theta}^{(k)}=\bm{\Theta}^{(k-1)}-\eta_{2}\cdot\nabla_{\bm{\Theta}}\bigg{(} \frac{1}{|\Omega_{k}|}\sum_{\mathcal{T}_{k,i}\in\Omega_{k}}\mathcal{L}(D_{k,i} ^{q};\bm{\Theta}_{k,i}^{(J)})\bigg{)}=\bm{\Theta}^{(k-1)}-\frac{\eta_{2}}{| \Omega_{k}|}\sum_{\mathcal{T}_{k,i}\in\Omega_{k}}\nabla_{\bm{\Theta}}\bigg{(} \mathcal{L}(D_{k,i}^{q};\bm{\Theta}_{k,i}^{(J)})\bigg{)}\]

where \(\bm{\Theta}_{k,i}^{(J)}\) is the task-specific parameter for \(\mathcal{T}_{k,i}\) after the inner-loop optimization for \(J\) steps.

Analogously, for the notation brevity and the sake of analysis, we denote \(f(\bm{\Theta}^{(K-1)}[\Omega_{K}];\bm{\theta}^{(k-1)})=f_{1}(\bm{\chi}_{k}^{q}; \bm{\theta}_{1}^{(k-1)})+f_{2}\bigg{(}[\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{k }^{s});\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{k}^{q})];\bm{\theta}_{2}^{(k-1)} \bigg{)}\) where we have \(\bm{\chi}_{k}^{q}:=\bm{\Theta}^{(K-1)}[\Omega_{K}]\) being the meta-parameters adapted to batch of tasks \(\Omega_{K}\), and the batch-specific parameter is defined as \(\bm{\chi}_{k}^{s}:=\frac{1}{|\Omega_{K}|}\sum_{\mathcal{T}_{k,i}\in\Omega_{K }}[\bm{\Theta}_{k,i}^{(J)}]\). Then, the regret under the batch setting can be denoted by

\[R(K)=R_{\text{batch}}(K)=\mathbb{E}_{\mathcal{T}\sim\mathcal{P} (\mathcal{T}),\bm{x}\sim\mathcal{D}_{\mathcal{T}}}\bigg{[}\mathcal{L}\big{(} \bm{x};\mathcal{I}(\mathcal{T},\bm{\Theta}^{(K)})\big{)}\bigg{]}-\mathbb{E}_{ \mathcal{T}\sim\mathcal{P}(\mathcal{T}),\bm{x}\sim\mathcal{D}_{\mathcal{T}}} \bigg{[}\mathcal{L}\big{(}\bm{x};\mathcal{I}(\mathcal{T},\bm{\Theta}^{(K),*} )\big{)}\bigg{]}\] \[=h(\bm{\Theta}^{(K-1),*}[\Omega_{K}^{*}])-h(\bm{\Theta}^{(K-1)}[ \Omega_{K}])\] \[=h(\bm{\Theta}^{(K-1),*}[\Omega_{K}^{*}])-f(\bm{\Theta}^{(K-1),*} [\Omega_{K}^{*}];\tilde{\bm{\theta}}^{(K-1)})+f(\bm{\Theta}^{(K-1),*}[\Omega_{ K}^{*}];\tilde{\bm{\theta}}^{(K-1)})-f(\bm{\Theta}^{(K-1)}[\Omega_{K}];\bm{\theta}^{(K-1)})\] \[\qquad\qquad+f(\bm{\Theta}^{(K-1)}[\Omega_{K}];\bm{\theta}^{(K-1) })-h(\bm{\Theta}^{(K-1)}[\Omega_{K}]),\]

and after applying properties of the arm pulling mechanism, it is equivalent to

\[R(K)\leq h(\bm{\Theta}^{(K-1),*}[\Omega_{K}^{*}])-f(\bm{\Theta} ^{(K-1),*}[\Omega_{K}^{*}];\tilde{\bm{\theta}}^{(K-1)})\] \[\qquad+f(\bm{\Theta}^{(K-1),*}[\Omega_{K}^{*}];\tilde{\bm{\theta} }^{(K-1)})-\hat{f}(\bm{\Theta}^{(K-1),*}[\Omega_{K}^{*}];\tilde{\bm{\theta}}^ {(K-1)})\] \[\qquad+\hat{f}(\bm{\Theta}^{(K-1),*}[\Omega_{K}^{*}];\tilde{\bm{ \theta}}^{(K-1)})-\hat{f}(\bm{\Theta}^{(K-1)}[\Omega_{K}^{*}];\bm{\theta}^{(K- 1)})\] \[\qquad+\hat{f}(\bm{\Theta}^{(K-1)}[\Omega_{K}];\bm{\theta}^{(K-1) })-f(\bm{\Theta}^{(K-1)}[\Omega_{K}];\bm{\theta}^{(K-1)})\ +f(\bm{\Theta}^{(K-1)}[\Omega_{K}];\bm{\theta}^{(K-1)})-h(\bm{\Theta}^{(K-1) }[\Omega_{K}])\] \[\leq\underbrace{|h(\bm{\Theta}^{(K-1),*}[\Omega_{K}^{*}])-f(\bm {\Theta}^{(K-1),*}[\Omega_{K}^{*}];\tilde{\bm{\theta}}^{(K-1)})|}_{\mathcal{I }_{5}}+\underbrace{|f(\bm{\Theta}^{(K-1),*}[\Omega_{K}^{*}];\tilde{\bm{\theta} }^{(K-1)})-\hat{f}(\bm{\Theta}^{(K-1),*}[\Omega_{K}^{*}];\tilde{\bm{\theta}}^ {(K-1)})|}_{\mathcal{I}_{6}}\] \[\qquad+\underbrace{|\hat{f}(\bm{\Theta}^{(K-1),*}[\Omega_{K}^{*}] ;\tilde{\bm{\theta}}^{(K-1)})-\hat{f}(\bm{\Theta}^{(K-1)}[\Omega_{K}^{*}]; \bm{\theta}^{(K-1)})|}_{\mathcal{I}_{7}}+\underbrace{|\hat{f}(\bm{\Theta}^{(K- 1)}[\Omega_{K}];\bm{\theta}^{(K-1)})-f(\bm{\Theta}^{(K-1)}[\Omega_{K}];\bm{ \theta}^{(K-1)})|}_{\mathcal{I}_{8}}\] \[\qquad\qquad+\underbrace{|f(\bm{\Theta}^{(K-1)}[\Omega_{K}];\bm{ \theta}^{(K-1)})-h(\bm{\Theta}^{(K-1)}[\Omega_{K}])|}_{\mathcal{I}_{9}}\]

where the average value of estimated benefit scores for individual tasks \(\mathcal{T}_{K,i}\in\Omega_{K}\) is represented as \(\widehat{f}(\bm{\Theta}^{(K-1)}[\Omega_{K}])=\frac{1}{|\Omega_{K}|}\cdot\sum_ {\mathcal{T}_{K,i}\in\Omega_{K}}f(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K,i}])= \frac{1}{|\Omega_{K}|}\cdot\sum_{\mathcal{T}_{K,i}\in\Omega_{K}}f\big{(}\bm{ \Theta}^{(K-1)}-\eta_{2}\cdot\nabla_{\bm{\Theta}}\mathcal{L}(D_{K,i}^{q};\bm{ \Theta}_{K,i}^{(J)});\bm{\theta}^{(K-1)}\big{)},\) and the inequality is due to the pulling mechanism of BASS. Here, \(I_{5},I_{9}\) individually correspond to \(I_{1},I_{2}\) in the single-task setting and can be bounded by **Lemma** D.3, **Corollary** D.4. Term \(I_{7}\) can be upper bounded by \(I_{3}+I_{4}\) from the single-task setting above. Then, for the rest terms \(I_{6},I_{8}\), we proceed to bound them separately.

#### d.2.1 Bounding error terms and assembling the regret bound

We begin with the term \(I_{8}\), and then proceed to \(I_{6}\). For the chosen batch of tasks \(\Omega_{K}\) in the round \(K\), we will have \(f_{1}(\bm{\Theta}^{(K-1)}[\Omega_{K}])=f_{1}\big{(}\bm{\Theta}^{(K-1)}-\eta_{2} \cdot\nabla_{\bm{\Theta}}\big{(}\frac{1}{|\Omega_{K}|}\sum_{\mathcal{T}_{K,i} \in\Omega_{K}}\mathcal{L}(D_{K,i}^{q};\bm{\Theta}_{K,i}^{(J)})\big{)};\bm{ \theta}_{1}^{(K-1)}\big{)},\)

In this case, the average value of estimation sampling probabilities for tasks \(\mathcal{T}_{K,i}\in\Omega_{K}\) is

\[\widehat{f} (\bm{\Theta}^{(K-1)}[\Omega_{K}])=\widehat{f}_{1}(\bm{\Theta}^{(K-1) }[\Omega_{K}])+\widehat{f}_{2}(\bm{\Theta}^{(K-1)}[\Omega_{K}])\] \[=\frac{1}{|\Omega_{K}|}\sum_{\mathcal{T}_{K,i}\in\Omega_{K}} \bigg{[}f_{1}(\bm{\Theta}^{(K-1)}[\mathcal{T}_{K,i}];\bm{\theta}_{1}^{(K-1)})+f_ {2}\big{(}[\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{K,i}^{*});\ \nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{K,i}^{q})];\bm{\theta}_{2}^{(K-1)} \big{)}\bigg{]}\]

**[Bounding the \(f_{1}\) output difference]** Next, let us first proceed to bound the output difference with respect to the exploitation module \(f_{1}\), where we can transform this term into

\[f_{1}(\bm{\Theta}^{(K-1)}[\Omega_{K}])-\widehat{f}_{1}(\bm{\Theta}^ {(K-1)}[\Omega_{K}])\] \[\qquad\qquad\qquad-\frac{1}{|\Omega_{K}|}\cdot\sum_{\mathcal{T}_{K,j}\in\Omega_{K}}f_{1}\bigg{(}\bm{\Theta}^{(k-1)}-\eta_{2}\cdot\nabla_{\bm{ \Theta}}\mathcal{L}(D^{q}_{K,j};\bm{\Theta}^{(J)}_{K,j});\bm{\theta}^{(K-1)}_{1 }\bigg{)}\] \[\qquad\qquad\qquad\qquad-\frac{1}{|\Omega_{K}|}\cdot\sum_{ \mathcal{T}_{K,j}\in\Omega_{K}}f_{1}\bigg{(}\bm{\Theta}^{(k-1)}-\eta_{2}\cdot \nabla_{\bm{\Theta}}\mathcal{L}(D^{q}_{K,j};\bm{\Theta}^{(J)}_{K,j});\bm{ \theta}^{(K-1)}_{1}\bigg{)}\] \[\qquad\qquad\qquad\qquad\qquad-f_{1}\big{(}\bm{\Theta}^{(k-1)}- \eta_{2}\cdot\nabla_{\bm{\Theta}}\mathcal{L}(D^{q}_{K,j};\bm{\Theta}^{(J)}_{K,j});\bm{\theta}^{(K-1)}_{1}\big{)}\bigg{)}.\]

Then, applying the Lipschitz continuity property will lead to

\[f_{1}(\bm{\Theta}^{(K-1)}[\Omega_{K}])-\widehat{f}_{1}(\bm{\Theta }^{(K-1)}[\Omega_{K}])\] \[\qquad\qquad\qquad\leq\frac{\eta_{2}\cdot\xi_{L}}{|\Omega_{K}|} \cdot\sum_{\mathcal{T}_{K,i}\in\Omega_{K}}\|\nabla_{\bm{\Theta}}\big{(}\frac{ 1}{|\Omega_{K}|}\sum_{\mathcal{T}_{K,j}\in\Omega_{K}}\mathcal{L}(D^{q}_{K,j}; \bm{\Theta}^{(J)}_{K,j})\big{)}-\nabla_{\bm{\Theta}}\mathcal{L}(D^{q}_{K,i}; \bm{\Theta}^{(J)}_{K,i})\|_{2}.\]

Here, by the definition of the outer-loop optimization of first-order meta-learning, we will have an alternative form the inequality, denoted by

\[f_{1}(\bm{\Theta}^{(K-1)}[\Omega_{K}])-\widehat{f}_{1}(\bm{ \Theta}^{(K-1)}[\Omega_{K}])\leq\] \[\qquad\qquad\frac{\eta_{2}\cdot\xi_{L}}{|\Omega_{K}|}\cdot\bigg{(} \sum_{\mathcal{T}_{K,i}\in\Omega_{K}}\|\frac{1}{|\Omega_{K}|}\sum_{\mathcal{T }_{K,j}\in\Omega_{K}}\nabla_{\bm{\Theta}}\big{(}\mathcal{L}(D^{q}_{K,j};\bm{ \Theta}^{(J)}_{K,j})\big{)}-\nabla_{\bm{\Theta}}\mathcal{L}(D^{q}_{K,i};\bm{ \Theta}^{(J)}_{K,i})\|_{2}\bigg{)}.\]

For the term in the parentheses on the RHS, substituting the backward operation with the \(H(\cdot,\cdot)\) mapping function, we have

\[\sum_{\mathcal{T}_{K,i}\in\Omega_{K}}\|\frac{1}{|\Omega_{K}|}\sum _{\mathcal{T}_{K,j}\in\Omega_{K}}\!\!\nabla_{\bm{\Theta}}\!\big{(}\mathcal{L} (D^{q}_{K,j};\bm{\Theta}^{(J)}_{K,j})\big{)}-\nabla_{\bm{\Theta}}\mathcal{L}( D^{q}_{K,i};\bm{\Theta}^{(J)}_{K,i})\|_{2}\] \[\qquad\qquad\qquad=\sum_{\mathcal{T}_{K,i}\in\Omega_{K}}\|\frac{ 1}{|\Omega_{K}|}\sum_{\mathcal{T}_{K,j}\in\Omega_{K}}\nabla_{\bm{\Theta}} \big{(}\mathcal{L}(D^{q}_{K,j};\bm{\Theta}^{(J)}_{K,j})\big{)}-\frac{1}{| \Omega_{K}|}\sum_{\mathcal{T}_{K,j}\in\Omega_{K}}\nabla_{\bm{\Theta}}\mathcal{ L}(D^{q}_{K,i};\bm{\Theta}^{(J)}_{K,i})\|_{2}\] \[\qquad\qquad\qquad\leq\frac{1}{|\Omega_{K}|}\sum_{\mathcal{T}_{K,i }\in\Omega_{K}}\sum_{\mathcal{T}_{K,j}\in\Omega_{K}}\|\nabla_{\bm{\Theta}} \mathcal{L}(D^{q}_{K,j};\bm{\Theta}^{(J)}_{K,j})-\nabla_{\bm{\Theta}}\mathcal{ L}(D^{q}_{K,i};\bm{\Theta}^{(J)}_{K,i})\|_{2}\] \[\qquad\qquad\qquad=\frac{1}{|\Omega_{K}|}\sum_{\mathcal{T}_{K,i} \in\Omega_{K}}\sum_{\mathcal{T}_{K,j}\in\Omega_{K}}\|H(\mathcal{T}_{K,i},\bm{ \Theta}^{(K-1)})-H(\mathcal{T}_{K,j},\bm{\Theta}^{(K-1)})\|_{2}\] \[\qquad\qquad\qquad\leq|\Omega_{K}|\cdot S_{1}(K).\]

with \(|\Omega_{K}|=B\). Therefore, the \(f_{1}\) part of error term \(I_{8}\) could be bounded by \(f_{1}(\bm{\Theta}^{(K-1)}[\Omega_{K}])-\widehat{f}_{1}(\bm{\Theta}^{(K-1)}[ \Omega_{K}])\leq\eta_{2}\cdot\xi_{L}\cdot B\cdot S_{1}(K)\). where the upper bound \(S_{1}(K)\leq\mathcal{O}(\sqrt{m_{\mathcal{F}}L_{\mathcal{F}}})\) can be found in the procedure bounding term \(I_{4.1}\).

**[Bounding the \(f_{2}\) output difference]** Then, with \(\bm{\chi}_{K}=\bm{\Theta}^{(K-1)}[\Omega_{K}]\), we proceed to bound the output difference with respect to the exploration module, which is represented by

\[f_{2}(\bm{\Theta}^{(K-1)}[\Omega_{K}])-\widehat{f}_{2}(\bm{\Theta} ^{(K-1)}[\Omega_{K}])\] \[\quad=\frac{1}{|\Omega_{K}|}\cdot\sum_{\mathcal{T}_{K,j}\in\Omega _{K}}\bigg{(}f_{2}\big{(}[\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{s}_{K});\; \nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{q}_{K})];\bm{\theta}^{(K-1)}_{2}\big{)}-f _{2}\big{(}[\nabla_{\bm{\theta}}f_{1}(\bm{\chi}^{s}_{K,j});\;\nabla_{\bm{ \theta}}f_{1}(\bm{\chi}^{q}_{K,j})];\bm{\theta}^{(K-1)}_{2}\big{)}\bigg{)}\]By adopting the Lipschitz continuity property of \(f_{2}\), we will have

\[f_{2} (\boldsymbol{\Theta}^{(K-1)}[\Omega_{K}])-\widehat{f}_{2}( \boldsymbol{\Theta}^{(K-1)}[\Omega_{K}])\] \[\leq\frac{\xi_{L}}{|\Omega_{K}|}\cdot\sum_{\mathcal{T}_{K,j}\in \Omega_{K}}\left\|[\nabla_{\boldsymbol{\theta}}f_{1}(\boldsymbol{\chi}_{K}^{s}) ;\;\nabla_{\boldsymbol{\theta}}f_{1}(\boldsymbol{\chi}_{K}^{q})]-[\nabla_{ \boldsymbol{\theta}}f_{1}(\boldsymbol{\chi}_{K,j}^{s});\;\nabla_{\boldsymbol {\theta}}f_{1}(\boldsymbol{\chi}_{K,j}^{q})]\right\|_{2}\] \[\leq\frac{\xi_{L}}{|\Omega_{K}|}\cdot\sum_{\mathcal{T}_{K,j}\in \Omega_{K}}\left\|\nabla_{\boldsymbol{\theta}}f_{1}(\boldsymbol{\chi}_{K}^{s}) -\nabla_{\boldsymbol{\theta}}f_{1}(\boldsymbol{\chi}_{K,j}^{s})\right\|_{2}+ \left\|\nabla_{\boldsymbol{\theta}}f_{1}(\boldsymbol{\chi}_{K}^{q})-\nabla_{ \boldsymbol{\theta}}f_{1}(\boldsymbol{\chi}_{K,j}^{q})\right\|_{2}\] \[\leq\frac{\xi_{L}^{2}}{|\Omega_{K}|}\cdot\sum_{\mathcal{T}_{K,j} \in\Omega_{K}}\left\|\boldsymbol{\chi}_{K}^{s}-\boldsymbol{\chi}_{K,j}^{s} \right\|_{2}+\left\|\boldsymbol{\chi}_{K}^{q}-\boldsymbol{\chi}_{K,j}^{q} \right\|_{2}\] \[=\frac{\xi_{L}^{2}\cdot\eta_{2}}{|\Omega_{K}|}\cdot\sum_{ \mathcal{T}_{K,j}\in\Omega_{K}}\|\nabla_{\boldsymbol{\Theta}}\big{(}\frac{1}{ |\Omega_{K}|}\sum_{\mathcal{T}_{K,i}\in\Omega_{K}}\mathcal{L}(D_{K,i}^{q}; \boldsymbol{\Theta}_{K,i}^{(J)})\big{)}-\nabla_{\boldsymbol{\Theta}}\mathcal{L }(D_{K,j}^{q};\boldsymbol{\Theta}_{K,j}^{(J)})\|_{2}\] \[\qquad+\frac{\xi_{L}^{2}}{|\Omega_{K}|}\cdot\sum_{\mathcal{T}_{K,j }\in\Omega_{K}}\left\|\boldsymbol{\chi}_{K}^{s}-\boldsymbol{\chi}_{K,j}^{s} \right\|_{2}\] \[\leq\eta_{2}\cdot\xi_{L}^{2}B\cdot S_{1}(K)+\frac{\xi_{L}^{2} \cdot\eta_{1}}{|\Omega_{K}|}\cdot\sum_{\mathcal{T}_{K,j}\in\Omega_{K}}\|\frac {1}{|\Omega_{K}|}\sum_{\mathcal{T}_{K,i}\in\Omega_{K}}\boldsymbol{\Theta}_{K, i}^{(J)}-\frac{1}{|\Omega_{K}|}\sum_{\mathcal{T}_{K,i}\in\Omega_{K}} \boldsymbol{\Theta}_{K,j}^{(J)}\|_{2}\]

where the last inequality is by applying the conclusion when bounding the output difference w.r.t. the exploitation module \(f_{1}\). Then, for the second term on the RHS,

\[\sum_{\mathcal{T}_{K,j}\in\Omega_{K}}\|\frac{1}{|\Omega_{K}|}\sum _{\mathcal{T}_{K,i}\in\Omega_{K}}\boldsymbol{\Theta}_{K,i}^{(J)}-\frac{1}{| \Omega_{K}|}\sum_{\mathcal{T}_{K,i}\in\Omega_{K}}\boldsymbol{\Theta}_{K,j}^{( J)}\|_{2}\leq\frac{1}{|\Omega_{K}|}\sum_{\mathcal{T}_{K,j}\in\Omega_{K}}\sum_{ \mathcal{T}_{K,i}\in\Omega_{K}}\|\boldsymbol{\Theta}_{K,i}^{(J)}-\boldsymbol{ \Theta}_{K,j}^{(J)}\|_{2}\] \[\leq\frac{1}{|\Omega_{K}|}\sum_{\mathcal{T}_{K,j}\in\Omega_{K}} \sum_{\mathcal{T}_{K,i}\in\Omega_{K}}\|\big{(}\boldsymbol{\Theta}^{K-1}-\sum_{ \tau\in[\tau]}\nabla_{\boldsymbol{\Theta}}\mathcal{L}(D_{K,i}^{s};\boldsymbol {\Theta}_{K,i}^{(\tau)})\big{)}-\big{(}\boldsymbol{\Theta}^{K-1}-\sum_{\tau \in[J]}\nabla_{\boldsymbol{\Theta}}\mathcal{L}(D_{K,j}^{s};\boldsymbol{\Theta} _{K,j}^{(\tau)})\big{)}\|_{2}\] \[=\frac{1}{|\Omega_{K}|}\sum_{\mathcal{T}_{K,j}\in\Omega_{K}}\sum_ {\mathcal{T}_{K,i}\in\Omega_{K}}\|\sum_{\tau\in[J]}\nabla_{\boldsymbol{\Theta}} \mathcal{L}(D_{K,i}^{s};\boldsymbol{\Theta}_{K,i}^{(\tau)})-\sum_{\tau\in[J]} \nabla_{\boldsymbol{\Theta}}\mathcal{L}(D_{K,j}^{s};\boldsymbol{\Theta}_{K,j}^ {(\tau)})\|_{2}\] \[\leq|\Omega_{K}|J\cdot\mathcal{O}(\sqrt{m_{\mathcal{F}}L_{ \mathcal{F}}})\]

where the last inequality is due to **Lemma** D.15. Summing up all the results above will give us the upper bound for \(f_{2}\) output difference \(f_{2}(\boldsymbol{\Theta}^{(K-1)}[\Omega_{K}])-\widehat{f}_{2}(\boldsymbol{ \Theta}^{(K-1)}[\Omega_{K}])\leq\mathcal{O}(\eta_{2}\cdot\xi_{L}^{2}B\cdot \sqrt{m_{\mathcal{F}}L_{\mathcal{F}}}+\eta_{1}\cdot BJ\cdot\sqrt{m_{\mathcal{F }}L_{\mathcal{F}}})\).

**[Similar procedure for term \(I_{6}\)]** Analogously, we can apply the same derivation for the error term \(I_{6}\), which leads to

\[I_{6} =f(\boldsymbol{\Theta}^{(K-1),*}[\Omega_{K}^{*}];\tilde{\boldsymbol {\theta}}^{(K-1)})-\hat{f}(\boldsymbol{\Theta}^{(K-1),*}[\Omega_{K}^{*}];\tilde{ \boldsymbol{\theta}}^{(K-1)})\] \[=f_{1}\bigg{(}\boldsymbol{\Theta}^{(k-1),*}-\eta_{2}\nabla_{ \boldsymbol{\Theta}}\big{(}\frac{1}{|\Omega_{K}^{*}|}\sum_{\mathcal{T}_{i}^{*} \in\Omega_{K}^{*}}\mathcal{L}(D_{i^{*}}^{q};\boldsymbol{\Theta}_{i^{*}}^{(J)}) \big{)};\tilde{\boldsymbol{\theta}}_{1}^{(K-1)}\bigg{)}\] \[\qquad\qquad-\frac{1}{|\Omega_{K}^{*}|}\cdot\sum_{\mathcal{T}_{i} ^{*}\in\Omega_{K}^{*}}f_{1}\bigg{(}\boldsymbol{\Theta}^{(k-1),*}-\eta_{2} \nabla_{\boldsymbol{\Theta}}\mathcal{L}(D_{i^{*}}^{q};\boldsymbol{\Theta}_{i^{*}}^ {(J)});\tilde{\boldsymbol{\theta}}_{1}^{(K-1)}\bigg{)}.\]

Following a similar procedure as that of term \(I_{8}\) will give us a similar bound as

\[I_{6}=f(\boldsymbol{\Theta}^{(K-1),*}[\Omega_{K}^{*}];\tilde{ \boldsymbol{\theta}}^{(K-1)})-\hat{f}(\boldsymbol{\Theta}^{(K-1),*}[\Omega_{K}^{*}]; \tilde{\boldsymbol{\theta}}^{(K-1)})\] \[\qquad\qquad\qquad\qquad\qquad\qquad\leq\mathcal{O}(\eta_{2} \cdot\xi_{L}^{2}B\cdot\sqrt{m_{\mathcal{F}}L_{\mathcal{F}}}+BJ\cdot\sqrt{m_{ \mathcal{F}}L_{\mathcal{F}}}\eta_{1}).\]

where the learning rate \(\eta_{1},\eta_{2}\leq\mathcal{O}(\frac{1}{m_{\mathcal{F}}})\) is a small value. Then, the upper bounds for error terms \(I_{6},I_{8}\) are given as desired.

**[Assembling the results]** Then, combining all the results, we would have

\[R(K)\leq\mathcal{O}(\frac{1}{\sqrt{K}})\cdot\bigg{(}\sqrt{2\xi_{1}}+\frac{3L}{ \sqrt{2}}+(1+2\gamma_{1})\sqrt{2\log(\frac{K}{\delta})}\bigg{)}+\mathcal{O}( \frac{\xi_{L}^{2}KBJ\sqrt{L_{\mathcal{F}}}}{\sqrt{m_{\mathcal{F}}}})+\gamma_{m}\]where

\[\gamma_{1}=2+\mathcal{O}\left(\frac{K^{3}L}{\rho\sqrt{m}}\log m\right)+ \mathcal{O}\left(\frac{L^{2}K^{4}}{\rho^{4/3}m^{1/6}}\log^{11/6}(m)\right)\] \[\gamma_{m}=\bigg{(}1+\mathcal{O}(\frac{KL^{3}\log^{5/6}(m)}{\rho^ {1/3}m^{1/6}})\bigg{)}\mathcal{O}(\frac{K^{3}L}{\rho\sqrt{m}}\log(m))+\mathcal{O }\bigg{(}\frac{K^{4}L^{2}\log^{11/6}(m)}{\rho^{4/3}m^{1/6}}\bigg{)}+\frac{\xi_ {L}KL^{4}\log^{5/6}(m)}{\rho^{1/3}m^{1/6}}\]

Similarly, with proper networks width \(m\) as in **Theorem**5.2, the majority of the terms above can be cancelled to \(\mathcal{O}(1)\). With increasingly large network width \(m\) under the over-parameterization settings, \(\gamma_{1},\gamma_{m}\) will also become diminutive.

### Performance Guarantee for the Exploitation and Exploration Modules

In this subsection, we would like to give the performance guarantee for the proposed BASS framework, and the corresponding performance bound can be applied to derive an upper bound for the error terms \(I_{1},I_{2}\) for the single-task settings and \(I_{5},I_{9}\) under the batch settings. Up to meta-training iteration \(k\in[K]\) (before updating the meta-parameters and BASS), we denote all the past records received as \(\mathcal{P}_{k-1}\). Before presenting the main lemmas, we first introduce the following operator. Inspired by [3], with two arbitrary vectors \(\tilde{\boldsymbol{\chi}},\boldsymbol{\chi}\) such that \(\|\tilde{\boldsymbol{\chi}}\|_{2}\leq 1\), \(\|\boldsymbol{\chi}\|_{2}=1\), we have the following operator

\[\phi(\tilde{\boldsymbol{\chi}},\boldsymbol{\chi})=(\frac{\tilde{\boldsymbol {\chi}}}{\sqrt{2}},\frac{\boldsymbol{\chi}}{2},c)\] (11)

as the concatenation of the two vectors \(\frac{\tilde{\boldsymbol{\chi}}}{\sqrt{2}}\), \(\frac{\boldsymbol{\chi}}{2}\) and one constant \(c\), where \(c=\sqrt{\frac{3}{4}-(\frac{\|\tilde{\boldsymbol{\chi}}\|_{2}}{\sqrt{2}})^{2}} \geq\frac{1}{2}\). And this operator transforms the transformed vector into unit norm, \(\|\phi(\tilde{\boldsymbol{\chi}},\boldsymbol{\chi})\|_{2}=1\). The idea of this operator is to make the gradients \(\nabla_{\boldsymbol{\theta}}f_{1}(\cdot;\boldsymbol{\theta}_{1})\) of the exploitation model, which is the input of the exploration model \(f_{2}(\cdot;\boldsymbol{\theta}_{2})\), comply with the normalization requirement and the separateness assumption (**Assumption**5.1). For the sake of analysis, we will adopt this operation in the following proof. Note that this operator is just one possible solution, and our results could be easily generalized to other forms of input gradients under the unit-length and separateness assumption. Similar ideas are also applied in previous works [9]. We begin to bound the single-task settings with the following lemma.

**Lemma D.1**.: _For the constants \(c^{\prime}_{g}>0\), \(0<\rho\leq\mathcal{O}(\frac{1}{L})\) and \(\xi_{1}\in(0,1)\), given the past records \(\mathcal{P}_{k-1}\), we suppose \(m,\eta_{1},\eta_{2}\) satisfy the conditions in **Theorem**5.2, and randomly draw the parameter \(\{\boldsymbol{\theta}_{1}^{(k)},\boldsymbol{\theta}_{2}^{(k)}\}\sim\{ \widetilde{\boldsymbol{\theta}}_{1}^{(\tau)},\widetilde{\boldsymbol{\theta}}_ {2}^{(\tau)}\}_{\tau\in[k]}\). Consider the past records \(\mathcal{P}_{k-1}\) up to round \(k\) are generated by a fixed policy when witness the candidate arms \(\{\Omega_{\text{task}}^{(\tau)}\}_{\tau\in[k]}\). Then, with probability at least \(1-\delta\) given an arm-reward pair \((\mathcal{T}_{k,\tilde{\tau}},r_{k,\tilde{\tau}})\), we have_

\[\mathbb{E}_{\mathcal{T}_{k,i}\sim\mathcal{P}(\mathcal{T})} \bigg{[}|f_{2}\bigg{(}\phi(\frac{[\nabla_{\boldsymbol{\theta}}f_{1}( \boldsymbol{\chi}_{k,\tilde{\tau}}^{s});\ \nabla_{\boldsymbol{\theta}}f_{1}(\boldsymbol{\chi}_{k,\tilde{\tau}}^{q})]}{c^ {\prime}_{g}L},\boldsymbol{\chi}_{k,\tilde{\tau}}^{q});\boldsymbol{\theta}_{2 }^{(k-1)}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\boldsymbol{\chi}_{k,\tilde{\tau}}^{q} ;\boldsymbol{\theta}_{1}^{(k-1)})\bigg{)}|\ \Omega_{\text{task}}^{(k)},\mathcal{P}_{k-1}\bigg{]}\] \[\qquad\qquad\leq\frac{1}{\sqrt{k}}\cdot\bigg{(}\sqrt{2\xi_{1}}+ \frac{3L}{\sqrt{2}}+(1+2\gamma_{1})\sqrt{2\log(\frac{k}{\delta})}\bigg{)}\]

_where_

\[\gamma_{1}=2+\mathcal{O}\left(\frac{k^{3}L}{\rho\sqrt{m}}\log m \right)+\mathcal{O}\left(\frac{L^{2}k^{4}}{\rho^{4/3}m^{1/6}}\log^{11/6}(m) \right).\]

**Proof.** The proof of this lemma is inspired by Lemma C.1 from [9]. First, we can derive the output upper bound

\[\bigg{|}f_{2}\bigg{(}\phi(\frac{[\nabla_{\boldsymbol{\theta}}f_{1 }(\boldsymbol{\chi}_{k,\tilde{\tau}}^{s});\ \nabla_{\boldsymbol{\theta}}f_{1}(\boldsymbol{\chi}_{k,\tilde{\tau}}^{q})]}{c^ {\prime}_{g}L},\boldsymbol{\chi}_{k,\tilde{\tau}}^{q});\boldsymbol{\theta}_{2 }^{(k-1)}\bigg{)}-\bigg{(}r_{k}-f_{1}(\boldsymbol{\chi}_{k,\tilde{\tau}}^{q}; \boldsymbol{\theta}_{1}^{(k-1)})\bigg{)}\bigg{|}\] \[\qquad\qquad\leq 1+2\gamma_{1}\]by triangle inequality and applying the generalization result of FC networks (**Lemma D.5**) on \(f_{1}(\cdot;\bm{\theta}_{1}),f_{2}(\cdot;\bm{\theta}_{2})\).

For the brevity of notation, we use \(\nabla f_{1}(\mathcal{T}_{k,\widetilde{\gamma}})\) to denote \(\phi(\frac{[\nabla_{\bm{\theta}}f_{1}(\mathcal{X}_{k,\widetilde{\gamma}}^{q}); \ \nabla_{\bm{\theta}}f_{1}(\bm{\mathbf{X}}_{k,\widetilde{\gamma}}^{q})]}{c_{s}^ {q}L},\bm{\mathbf{X}}_{k,\widetilde{\gamma}}^{q})\) and apply \((\bm{\mathbf{\chi}}_{k},r_{k})\) as \((\bm{\mathbf{\chi}}_{k,\widetilde{\gamma}}^{q},r_{k,\widetilde{z}})\) for the following proof. Define the difference sequence as

\[V_{\tau}^{(1)}=\mathbb{E}\bigg{[}\bigg{|}f_{2}\bigg{(}\nabla f_{ 1}(\mathcal{T}_{\tau,\widetilde{\gamma}});\bm{\theta}_{2}^{(\tau-1)}\bigg{)}- \bigg{(}r_{\tau}-f_{1}(\bm{\mathbf{\chi}}_{\tau};\bm{\theta}_{1}^{(\tau-1)}) \bigg{)}\bigg{|}\bigg{]}\] \[-\bigg{|}f_{2}\bigg{(}\nabla f_{1}(\mathcal{T}_{\tau,\widetilde{ \gamma}});\bm{\theta}_{2}^{(\tau-1)}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{ \mathbf{\chi}}_{\tau};\bm{\theta}_{1}^{(\tau-1)})\bigg{)}\bigg{|}|F_{\tau} \bigg{]}=0.\]

Since the past rewards and the received arm-reward pairs \((\bm{\mathbf{\chi}}_{\tau},r_{\tau})\) are generated by the same reward mapping function, we have the expectation

\[\mathbb{E}[V_{\tau}^{(1)}\big{|}F_{\tau}]= \mathbb{E}\bigg{[}\bigg{|}f_{2}\bigg{(}\nabla f_{1}(\mathcal{T}_{ \tau,\widetilde{\gamma}});\bm{\theta}_{2}^{(\tau-1)}\bigg{)}-\bigg{(}r_{\tau} -f_{1}(\bm{\mathbf{\chi}}_{\tau};\bm{\theta}_{1}^{(\tau-1)})\bigg{)}\bigg{|} \bigg{]}\] \[-\mathbb{E}\bigg{[}\bigg{|}f_{2}\bigg{(}\nabla f_{1}(\mathcal{T}_ {\tau,\widetilde{\gamma}});\bm{\theta}_{2}^{(\tau-1)}\bigg{)}-\bigg{(}r_{\tau} -f_{1}(\bm{\mathbf{\chi}}_{\tau};\bm{\theta}_{1}^{(\tau-1)})\bigg{)}\bigg{|}|F _{\tau}\bigg{]}=0.\]

where \(F_{\tau}\) denotes the filtration given the past records \(\mathcal{P}_{\tau}\), up to round \(\tau\in[k]\). This also gives the fact that \(V_{\tau}^{(1)}\) is a martingale difference sequence. Then, after applying the martingale difference sequence over \([k]\), we have

\[\frac{1}{k}\sum_{\tau\in[k]}V_{\tau}^{(1)}= \frac{1}{k}\sum_{\tau\in[k]}\mathbb{E}\bigg{[}\bigg{|}f_{2}\bigg{(} \nabla f_{1}(\mathcal{T}_{\tau,\widetilde{\gamma}});\bm{\theta}_{2}^{(\tau-1) }\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{\mathbf{\chi}}_{\tau};\bm{\theta}_{1}^{( \tau-1)})\bigg{)}\bigg{|}\bigg{]}\] \[-\frac{1}{k}\sum_{\tau\in[k]}\bigg{|}f_{2}\bigg{(}\nabla f_{1}( \mathcal{T}_{\tau,\widetilde{\gamma}});\bm{\theta}_{2}^{(\tau-1)}\bigg{)}- \bigg{(}r_{\tau}-f_{1}(\bm{\mathbf{\chi}}_{\tau};\bm{\theta}_{1}^{(\tau-1)}) \bigg{)}\bigg{|}.\]

Then, by applying the Azuma-Hoeffding inequality, it leads to

\[\mathbb{P}\bigg{[}\frac{1}{k}\sum_{\tau\in[k]}V_{\tau}^{(1)}-\frac{1}{k}\sum_ {\tau\in[k]}\mathbb{E}[V_{\tau}^{(1)}]\geq(1+2\gamma_{1})\sqrt{\frac{2\log(1/ \delta)}{k}}\bigg{]}\leq\delta\]

Since the expectation of \(V_{\tau}^{(1)}\) is zero, with the probability at least \(1-\delta\) and an existing set of parameters \(\bm{\theta}_{2}\) s.t. \(\|\bm{\theta}_{2}-\bm{\theta}_{2}^{(0)}\|\leq\mathcal{O}\left(\frac{k^{3}}{ \rho\sqrt{m}}\log m\right)\), the above inequality implies

\[\frac{1}{k}\sum_{\tau\in[k]}V_{\tau}^{(1)}\leq(1+2\gamma_{1}) \sqrt{\frac{2\log(1/\delta)}{k}}\implies\] \[\mathbb{E}_{\mathcal{T}_{k,i}\sim\mathcal{P}(\tau)}\mathbb{E}_{ \{\bm{\theta}_{1}^{(k-1)},\bm{\theta}_{2}^{(k-1)}\}}\bigg{[}\bigg{|}f_{2} \bigg{(}\nabla f_{1}(\mathcal{T}_{\tau,\widetilde{\gamma}});\bm{\theta}_{2}^{(k -1)}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{\mathbf{\chi}}_{\tau};\bm{\theta}_{1}^{ (k-1)})\bigg{)}\bigg{|}\bigg{]}\] \[=\frac{1}{k}\sum_{\tau\in[k]}\mathbb{E}\bigg{[}\bigg{|}f_{2} \bigg{(}\nabla f_{1}(\mathcal{T}_{\tau,\widetilde{\gamma}});\bm{\theta}_{2}^{( \tau-1)}\bigg{)}-\bigg{(}r_{k}-f_{1}(\bm{\mathbf{\chi}}_{\tau};\bm{\theta}_{1}^{ (\tau-1)})\bigg{)}\bigg{|}\bigg{]}\] \[\leq\frac{1}{k}\sum_{\tau\in[k]}\bigg{|}f_{2}\bigg{(}\nabla f_{1} (\mathcal{T}_{\tau,\widetilde{\gamma}});\bm{\theta}_{2}^{(\tau-1)}\bigg{)}- \bigg{(}r_{\tau}-f_{1}(\bm{\mathbf{\chi}}_{\tau};\bm{\theta}_{1}^{(\tau-1)}) \bigg{)}\bigg{|}+(1+2\gamma_{1})\sqrt{\frac{2\log(1/\delta)}{k}}\] \[\underset{\text{(ii)}}{\leq}\frac{1}{k}\sum_{\tau\in[k]}\bigg{|} f_{2}\bigg{(}\nabla f_{1}(\mathcal{T}_{\tau,\widetilde{\gamma}});\bm{\theta}_{2} \bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{\mathbf{\chi}}_{\tau};\bm{\theta}_{1}^{(\tau-1 )})\bigg{)}\bigg{|}+\frac{3L}{\sqrt{2k}}+(1+2\gamma_{1})\sqrt{\frac{2\log(1/ \delta)}{k}}\] \[\leq\frac{1}{\sqrt{k}}\sqrt{\sum_{\tau\in[k]}\bigg{|}f_{2}\bigg{(} \nabla f_{1}(\mathcal{T}_{\tau,\widetilde{\gamma}});\bm{\theta}_{2}\bigg{)}- \bigg{(}r_{\tau}-f_{1}(\bm{\mathbf{\chi}}_{\tau};\bm{\theta}_{1}^{(\tau-1)}) \bigg{)}\bigg{|}^{2}}+\frac{3L}{\sqrt{2k}}+(1+2\gamma_{1})\sqrt{\frac{2\log(1/ \delta)}{k}}\] \[\underset{\text{(ii)}}{\leq}\sqrt{\frac{2\xi_{1}}{k}+\frac{3L}{ \sqrt{2k}}}+(1+2\gamma_{1})\sqrt{\frac{2\log(1/\delta)}{k}}.\]where the first equality is due to the sampling of candidate tasks and the model parameters. Here, the upper bound (i) is derived by applying the conclusions of **Lemma** D.6 and **Lemma** D.10, and the inequality (ii) is derived by adopting **Lemma** D.6 while defining the empirical loss to be \(\frac{1}{2}\sum_{\tau\in[k]}\left|f_{2}\bigg{(}\nabla f_{1}(\mathcal{T}_{\tau, \widehat{i}});\bm{\theta}_{2}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{\chi}_{\tau}; \bm{\theta}_{1}^{(\tau-1)})\bigg{)}\right|^{2}\leq\xi_{1}\). Finally, applying the union bound would give the aforementioned results.

Here, analogous to the trained parameters, we consider the shadow parameters as \(\{\bm{\theta}_{1}^{(k),*},\bm{\theta}_{2}^{(k),*}\}\sim\{\widetilde{\bm{ \theta}_{1}^{(r),*}},\widetilde{\bm{\theta}_{2}^{(\tau),*}}\}_{\tau\in[k]}\). Similarly, each pair \(\{\widetilde{\bm{\theta}_{1}^{(\tau),*}},\widetilde{\bm{\theta}_{2}^{(\tau),*}}\}\) is separately trained on past received rewards of the optimal arm(s) \(\{r_{\tau^{\prime},i^{*}}\}_{\tau^{\prime}\in[\tau],\mathcal{T}_{\tau^{ \prime},i^{*}}\in\Omega_{k}^{*}}\) and past exploration scores of the optimal arm(s) \(\{e_{\tau^{\prime},i^{*}}\}_{\tau^{\prime}\in[\tau],\mathcal{T}_{\tau^{ \prime},i^{*}}\in\Omega_{k}^{*}}\) with \(J_{\bm{\theta}}\)-iteration GD, starting from the random initialization \(\{\bm{\theta}_{1}^{(0)},\bm{\theta}_{2}^{(0)}\}\).

**Corollary D.2**.: _For the constants \(0<\rho\leq\mathcal{O}(1/L)\) and \(\xi_{1}\in(0,1)\), given the past records \(\mathcal{P}_{k-1}\), we suppose \(m_{1},J\) satisfy the conditions in **Theorem** 5.2, and randomly draw the parameters \(\{\bm{\theta}_{1}^{(k),*},\bm{\theta}_{2}^{(k),*}\}\sim\{\widetilde{\bm{ \theta}_{1}^{(\tau),*}},\widetilde{\bm{\theta}_{2}^{(\tau),*}}\}_{\tau\in[k]}\). For the optimal arm \(\mathcal{T}_{k,i^{*}}\in\Omega_{\text{task}}^{k}\), consider its union set with the the collection of past optimal arms \(\mathcal{P}_{k-1}^{*}\cup\{\mathcal{T}_{k,i^{*}},r_{k,i^{*}}\}\) are generated by a fixed policy when witness the candidate arms \(\{\Omega_{\text{task}}^{(\tau)}\}_{\tau\in[k]}\), with \(\mathcal{P}_{k-1}^{*}\) being the collection chosen by this policy. Then, with probability at least \(1-\delta\), we have_

\[\mathbb{E}_{\mathcal{T}_{k,i}\sim\mathcal{P}(\mathcal{T})}\bigg{[} \left|f_{2}\bigg{(}\phi(\frac{[\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{k}^{*,*} );\,\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{k}^{q,*})]}{c_{\bm{\theta}}^{\prime} L},\bm{\chi}_{k}^{q,*});\bm{\theta}_{2}^{(k-1),*}\right)-\bigg{(}r_{\tau}-f_{1}(\bm{ \chi}_{k}^{q,*};\bm{\theta}_{1}^{(k-1),*})\bigg{)}\right|\big{|}\Omega_{\text{ task}}^{(k)},\mathcal{P}_{k-1}^{*}\bigg{]}\] \[\leq\frac{1}{\sqrt{k}}\cdot\bigg{(}\sqrt{2\xi_{1}}+\frac{3L}{ \sqrt{2}}+(1+\gamma_{1})\sqrt{2\log(\frac{k}{\delta})}\bigg{)}+\Gamma_{k}\]

_where \(r_{\tau,i^{*}}\) is the corresponding reward generated by the mapping function given an arm \(\bm{\chi}_{\tau,i^{*}}\), and_

\[\Gamma_{k}=\bigg{(}1+\mathcal{O}(\frac{kL^{3}\log^{5/6}(m)}{\rho^{1/3}m^{1/6} })\bigg{)}\cdot\mathcal{O}(\frac{k^{4}L}{\rho\sqrt{m}}\log(m))+\mathcal{O} \bigg{(}\frac{k^{5}L^{2}\log^{11/6}(m)}{\rho^{4/3}m^{1/6}}\bigg{)}.\]

**Proof.** This corollary is the direct application of Lemma D.1 by following a similar proof procedure. First, suppose the shadow models \(f_{1}(\cdot;\bm{\theta}_{2})\), \(f_{2}(\cdot;\bm{\theta}_{2})\) are trained on the alternative trajectory \(\mathcal{P}_{k-1}^{*}\). Analogous to the proof of Lemma D.1, we can define the following martingale difference sequence with regard to the previous records \(\mathcal{P}_{k-1}^{*}\) up to round \(\tau\in[t]\) as

\[V_{\tau}^{(1),*}=\mathbb{E}\bigg{[}\left|f_{2}\bigg{(}\nabla f_{ 1}(\mathcal{T}_{\tau,i^{*}});\bm{\theta}_{2}^{(\tau-1),*}\bigg{)}-\bigg{(}r_{ \tau}^{*}-f_{1}(\bm{\chi}_{\tau}^{*};\bm{\theta}_{1}^{(\tau-1),*})\bigg{)} \right|\bigg{]}\] \[-\bigg{|}f_{2}\bigg{(}\nabla f_{1}(\mathcal{T}_{\tau,i^{*}});\bm {\theta}_{2}^{(\tau-1),*}\bigg{)}-\bigg{(}r_{\tau}^{*}-f_{1}(\bm{\chi}_{\tau}^{* };\bm{\theta}_{1}^{(\tau-1),*})\bigg{)}\bigg{|}F_{\tau}^{*}\bigg{]}=0.\]

Since the records in set \(\mathcal{P}_{k-1}^{*}\) are sharing the same reward mapping function, we have the expectation

\[\mathbb{E}[V_{\tau}^{(1),*}\big{|}F_{\tau}^{*}]= \mathbb{E}\bigg{[}\left|f_{2}\bigg{(}\nabla f_{1}(\mathcal{T}_{ \tau,i^{*}});\bm{\theta}_{2}^{(\tau-1),*}\bigg{)}-\bigg{(}r_{\tau}^{*}-f_{1}( \bm{\chi}_{\tau}^{*};\bm{\theta}_{1}^{(\tau-1),*})\bigg{)}\right|\bigg{]}\] \[-\mathbb{E}\bigg{[}\left|f_{2}\bigg{(}\nabla f_{1}(\mathcal{T}_{ \tau,i^{*}});\bm{\theta}_{2}^{(\tau-1),*}\bigg{)}-\bigg{(}r_{\tau}^{*}-f_{1}( \bm{\chi}_{\tau}^{*};\bm{\theta}_{1}^{(\tau-1),*})\bigg{)}\right|\big{|}F_{\tau} ^{*}\bigg{]}=0.\]

where \(F_{\tau}^{*}\) denotes the filtration given the past records \(\mathcal{P}_{k-1}^{*}\). The mean value of \(V_{\tau}^{(1),*}\) across different time steps will be

\[\frac{1}{k}\sum_{\tau\in[k]}V_{\tau}^{(1),*}= \frac{1}{k}\sum_{\tau\in[k]}\mathbb{E}\bigg{[}\bigg{|}f_{2}\bigg{(} \nabla f_{1}(\mathcal{T}_{\tau,i^{*}});\bm{\theta}_{2}^{(\tau-1),*}\bigg{)}- \bigg{(}r_{\tau}^{*}-f_{1}(\bm{\chi}_{\tau}^{*};\bm{\theta}_{1}^{(\tau-1),*}) \bigg{)}\bigg{|}\bigg{]}\] \[-\frac{1}{k}\sum_{\tau\in[k]}\bigg{|}f_{2}\bigg{(}\nabla f_{1}( \mathcal{T}_{\tau,i^{*}});\bm{\theta}_{2}^{(\tau-1),*}\bigg{)}-\bigg{(}r_{ \tau}^{*}-f_{1}(\bm{\chi}_{\tau}^{*};\bm{\theta}_{1}^{(\tau-1),*})\bigg{)} \bigg{|}.\]with the expectation of zero. Afterwards, applying the Azuma-Hoeffding inequality, with a constant \(\delta\in(0,1)\), it leads to

\[\mathbb{P}\bigg{[}\frac{1}{k}\sum_{\tau\in[k]}V_{\tau}^{(1),*}-\frac{1}{k}\sum_{ \tau\in[k]}\mathbb{E}[V_{\tau}^{(1),*}]\geq(1+2\gamma_{1})\sqrt{\frac{2\log(1/ \delta)}{k}}\bigg{]}\leq\delta\]

To bound the output difference between the shadow model \(f_{1}(\cdot;\bm{\theta}_{1}^{(k-1),*}),f_{2}(\cdot;\bm{\theta}_{2}^{(k-1),*})\) and the model we trained based on received records \(f_{1}(\cdot;\bm{\theta}_{1}^{(k-1)}),f_{2}(\cdot;\bm{\theta}_{2}^{(k-1)})\), we apply the conclusion from **Lemma** D.11, which leads to that given arbitrary input vectors \(\bm{x},\bm{x}^{\prime}\), we have

\[|f_{1}(\bm{x};\bm{\theta}_{1}^{(k-1),*})-f_{1}(\bm{x};\bm{\theta }_{1}^{(k-1)})|,\ |f_{2}(\bm{x}^{\prime};\bm{\theta}_{2}^{(k-1),*})-f_{2}(\bm{x}^{\prime};\bm{ \theta}_{2}^{(k-1)})|\leq\] \[\bigg{(}1+\mathcal{O}(\frac{kL^{3}\log^{5/6}(m)}{\rho^{1/3}m^{1/ 6}})\bigg{)}\cdot\mathcal{O}(\frac{k^{3}L}{\rho\sqrt{m}}\log(m))+\mathcal{O} \bigg{(}\frac{k^{4}L^{2}\log^{11/6}(m)}{\rho^{4/3}m^{1/6}}\bigg{)}.\]

Finally, combining all the results will finish the proof.

We will also be able to have the performance guarantee under the batch settings. Recall that given a batch of chosen tasks \(\Omega_{k}\subset\Omega_{\text{task}}^{(k)}\), \(k\in[K]\), we have the meta-parameters adapted to this batch of tasks being \(\bm{\Theta}^{(K-1)}[\Omega_{K}]\), which we consider as the input for the \(f_{1}(\cdot;\bm{\theta}_{1})\) model, where the tasks within each collection are sampled from the task distribution. Thus, chosen task batches from different iterations are also independent from each other. Intuitively, we can also define the corresponding reward for arm batch \(\Omega_{k}\) as \(r_{k}=h(\bm{\Theta}^{(k-1)}[\Omega_{k}])\). Then, we bound the batch settings with the following lemma and corollary.

**Lemma D.3**.: _For the constants \(c_{g}^{\prime}>0\), \(\rho\in(0,\mathcal{O}(\frac{1}{L}))\) and \(\xi_{1}\in(0,1)\), given the past records \(\mathcal{P}_{k-1}\), we suppose \(m,\eta_{1},\eta_{2}\) satisfy the conditions in **Theorem** 5.2, and randomly draw the parameter \(\{\bm{\theta}_{1}^{(k)},\bm{\theta}_{2}^{(k)}\}\sim\{\widetilde{\bm{\theta}}_ {1}^{(\tau)},\widetilde{\bm{\theta}}_{2}^{(\tau)}\}_{\tau\in[k]}\). Consider the past records \(\mathcal{P}_{k-1}\) up to round \(k\) are generated by a fixed policy when witness the candidate arms \(\{\Omega_{\text{task}}^{(\tau)}\}_{\tau\in[k]}\). Then, with probability at least \(1-\delta\) given the pair of chosen arm batch and the reward \((\Omega_{k},r_{k})\) in round \(k\), we have_

\[\mathbb{E}_{\mathcal{T}_{k,i}\sim\mathcal{P}(\mathcal{T})}\bigg{[}| f_{2}\bigg{(}\phi(\frac{[\nabla\bm{\theta}f_{1}(\bm{\chi}_{k}^{ *});\,\nabla\bm{\theta}f_{1}(\bm{\chi}_{k}^{q})]}{c_{g}^{\prime}L},\bm{\chi}_ {k}^{q});\bm{\theta}_{2}^{(k-1)}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{\chi}_{k}^ {q};\bm{\theta}_{1}^{(k-1)})\bigg{)}\bigg{]}\,|\Omega_{\text{task}}^{(k)}, \mathcal{P}_{k-1}|\] \[\leq\frac{1}{\sqrt{k}}\cdot\bigg{(}\sqrt{2\xi_{1}}+\frac{3L}{ \sqrt{2}}+(1+2\gamma_{1})\sqrt{2\log(\frac{k}{\delta})}\bigg{)}\]

_where_

\[\gamma_{1}=2+\mathcal{O}\left(\frac{k^{3}L}{\rho\sqrt{m}}\log m\right)+ \mathcal{O}\left(\frac{L^{2}k^{4}}{\rho^{4/3}m^{1/6}}\log^{1/6}(m)\right).\]

**Proof.** The proof of this lemma is analogous to the proof of **Lemma** D.1. First, we can derive the output upper bound

\[\bigg{|}f_{2}\bigg{(}\phi(\frac{[\nabla\bm{\theta}f_{1}(\bm{\chi }_{k}^{*});\,\nabla\bm{\theta}f_{1}(\bm{\chi}_{k}^{q})]}{c_{g}^{\prime}L},\bm{ \chi}_{k}^{q});\bm{\theta}_{2}^{(k-1)}\bigg{)}-\bigg{(}r_{k}-f_{1}(\bm{\chi}_{ k}^{q};\bm{\theta}_{1}^{(k-1)})\bigg{)}\bigg{|}\] \[\leq\bigg{|}f_{2}\bigg{(}\phi(\frac{[\nabla\bm{\theta}f_{1}(\bm {\chi}_{k}^{*});\,\nabla\bm{\theta}f_{1}(\bm{\chi}_{k}^{q})]}{c_{g}^{\prime}L}, \bm{\chi}_{k}^{q});\bm{\theta}_{2}^{(k-1)}\bigg{)}\bigg{|}+\bigg{|}f_{1}(\bm{ \chi}_{k}^{q};\bm{\theta}_{1}^{(k-1)})\bigg{|}+1\] \[\leq 1+2\gamma_{1}\]

by triangle inequality and applying the generalization result of FC networks (**Lemma** D.5) on \(f_{1}(\cdot;\bm{\theta}_{1}),f_{2}(\cdot;\bm{\theta}_{2})\), where \(c_{g}^{\prime}>0\) is a positive number to scale the concatenated gradient vector.

For the brevity of notation, we use \(\nabla f_{1}(\Omega_{k})\) to denote \(\phi(\frac{[\nabla\bm{\theta}f_{1}(\bm{\chi}_{k}^{*});\,\nabla\bm{\theta}f_{1}( \bm{\chi}_{k}^{*})]}{c_{g}^{\prime}L},\bm{\chi}_{k}^{q})\) and apply \((\bm{\chi}_{k},r_{k})\) as \((\bm{\chi}_{k}^{q},r_{k})\) for the following proof. Define the difference sequence as

\[V_{\tau}^{(2)}=\mathbb{E}\bigg{[}\bigg{|}f_{2}\bigg{(}\nabla f_{ 1}(\Omega_{\tau});\bm{\theta}_{2}^{(\tau-1)}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{ \chi}_{\tau};\bm{\theta}_{1}^{(\tau-1)})\bigg{)}\bigg{|}\bigg{]}\] \[-\bigg{|}f_{2}\bigg{(}\nabla f_{1}(\Omega_{\tau});\bm{\theta}_{2}^{( \tau-1)}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{\chi}_{\tau};\bm{\theta}_{1}^{(\tau-1 )})\bigg{)}\bigg{|}.\]Since the past rewards and the received arm batch-reward pairs \((\bm{\chi}_{\tau},r_{\tau})\) are generated by the same reward mapping function, we have the expectation

\[\mathbb{E}[V_{\tau}^{(2)}\big{|}F_{\tau}]= \mathbb{E}\bigg{[}\bigg{|}f_{2}\bigg{(}\nabla f_{1}(\Omega_{\tau}); \bm{\theta}_{2}^{(\tau-1)}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{\chi}_{\tau};\bm {\theta}_{1}^{(\tau-1)})\bigg{)}\bigg{|}\bigg{]}\] \[-\mathbb{E}\bigg{[}\bigg{|}f_{2}\bigg{(}\nabla f_{1}(\Omega_{\tau} );\bm{\theta}_{2}^{(\tau-1)}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{\chi}_{\tau}; \bm{\theta}_{1}^{(\tau-1)})\bigg{)}\bigg{|}\big{|}F_{\tau}\bigg{]}=0.\]

where \(F_{\tau}\) denotes the filtration given the past records \(\mathcal{P}_{\tau}\), up to round \(\tau\in[k]\). This also gives the fact that \(V_{\tau}^{(2)}\) is a martingale difference sequence. Then, after applying the martingale difference sequence over \([k]\), we have

\[\frac{1}{k}\sum_{\tau\in[k]}V_{\tau}^{(2)}= \frac{1}{k}\sum_{\tau\in[k]}\mathbb{E}\bigg{[}\bigg{|}f_{2}\bigg{(} \nabla f_{1}(\Omega_{\tau});\bm{\theta}_{2}^{(\tau-1)}\bigg{)}-\bigg{(}r_{\tau }-f_{1}(\bm{\chi}_{\tau};\bm{\theta}_{1}^{(\tau-1)})\bigg{)}\bigg{|}\bigg{]}\] \[-\frac{1}{k}\sum_{\tau\in[k]}\bigg{|}f_{2}\bigg{(}\nabla f_{1}( \Omega_{\tau});\bm{\theta}_{2}^{(\tau-1)}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{ \chi}_{\tau};\bm{\theta}_{1}^{(\tau-1)})\bigg{)}\bigg{|}.\]

By the Azuma-Hoeffding inequality, it leads to \(\mathbb{P}\bigg{[}\frac{1}{k}\sum_{\tau\in[k]}V_{\tau}^{(2)}-\frac{1}{k}\sum_ {\tau\in[k]}\mathbb{E}[V_{\tau}^{(2)}]\geq(1+2\gamma_{1})\sqrt{\frac{2\log(1/ \delta)}{k}}\bigg{]}\leq\delta\). As we have discussed, the tasks within each collection are sampled from the task distribution, which makes chosen task batches from different iterations \(\Omega_{k},k\in[K]\) are also independent from each other. Since the expectation of \(V_{\tau}^{(2)}\) is zero, with the probability at least \(1-\delta\) and an existing set of parameters \(\bm{\theta}_{2}\) s.t. \(\|\bm{\theta}_{2}-\bm{\theta}_{2}^{(0)}\|\leq\mathcal{O}\left(\frac{k^{3}}{ \rho\sqrt{m}}\log m\right)\), the above inequality implies

\[\frac{1}{k}\sum_{\tau\in[k]} V_{\tau}^{(1)}\leq(1+2\gamma_{1})\sqrt{\frac{2\log(1/\delta)}{k}} \implies\] \[\mathbb{E}_{\mathcal{T}_{k,i}\sim\mathcal{P}(\mathcal{T})}\mathbb{ E}_{\{\bm{\theta}_{1}^{(k-1)},\bm{\theta}_{2}^{(k-1)}\}}\bigg{[}\bigg{|}f_{2} \bigg{(}\nabla f_{1}(\Omega);\bm{\theta}_{2}^{(k-1)}\bigg{)}-\bigg{(}r-f_{1}( \bm{\chi};\bm{\theta}_{1}^{(k-1)})\bigg{)}\bigg{|}\bigg{]}\] \[=\frac{1}{k}\sum_{\tau\in[k]}\mathbb{E}\bigg{[}\bigg{|}f_{2} \bigg{(}\nabla f_{1}(\Omega_{\tau});\bm{\theta}_{2}^{(\tau-1)}\bigg{)}-\bigg{(} r_{k}-f_{1}(\bm{\chi}_{\tau};\bm{\theta}_{1}^{(\tau-1)})\bigg{)}\bigg{|}\bigg{]}\] \[\leq\frac{1}{k}\sum_{\tau\in[k]}\bigg{|}f_{2}\bigg{(}\nabla f_{1} (\Omega_{\tau});\bm{\theta}_{2}^{(\tau-1)}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{ \chi}_{\tau};\bm{\theta}_{1}^{(\tau-1)})\bigg{)}\bigg{|}+(1+2\gamma_{1})\sqrt{ \frac{2\log(1/\delta)}{k}}\] \[\leq\frac{1}{\sqrt{k}}\sqrt{\sum_{\tau\in[k]}\bigg{|}f_{2}\bigg{(} \nabla f_{1}(\Omega_{\tau});\bm{\theta}_{2}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm {\chi}_{\tau};\bm{\theta}_{1}^{(\tau-1)})\bigg{)}\bigg{|}^{2}}+\frac{3L}{\sqrt {2k}}+(1+2\gamma_{1})\sqrt{\frac{2\log(1/\delta)}{k}}\] \[\leq\sqrt{\frac{2\xi_{2}}{k}}+\frac{3L}{\sqrt{2k}}+(1+2\gamma_{1}) \sqrt{\frac{2\log(1/\delta)}{k}}.\]

where the first equality is due to the sampling of candidate tasks and the model parameters. Here, the upper bound (i) is derived by applying the conclusions of **Lemma** D.6 and **Lemma** D.10, and the inequality (ii) is derived by adopting **Lemma** D.6 while defining the empirical loss to be \(\frac{1}{2}\sum_{\tau\in[k]}\bigg{|}f_{2}\bigg{(}\nabla f_{1}(\Omega_{\tau}); \bm{\theta}_{2}\bigg{)}-\bigg{(}r_{\tau}-f_{1}(\bm{\chi}_{\tau};\bm{\theta}_{1}^ {(\tau-1)})\bigg{)}\bigg{|}^{2}\leq\xi_{2}\). Finally, applying the union bound would give the aforementioned results.

Analogously, we consider the shadow parameters as \(\{\bm{\theta}_{1}^{(k),*},\bm{\theta}_{2}^{(k),*}\}\sim\{\widetilde{\bm{\theta}}_{ 1}^{(\tau),*},\widetilde{\bm{\theta}}_{2}^{(\tau),*}\}_{\tau\in[k]}\) where each pair \(\{\widetilde{\bm{\theta}}_{1}^{(\tau),*},\widetilde{\bm{\theta}}_{2}^{(\tau),*}\}\) is separately trained on past received rewards of the optimal arm(s) \(\{\tau_{\tau^{\prime},i^{*}}\}_{\tau^{\prime}\in[\tau],\mathcal{T}_{\tau^{ \prime},i^{*}}\in\Omega_{k}^{*}}\) and past exploration scores of the optimal arms) \(\{e_{\tau^{\prime},i^{*}}\}_{\tau^{\prime}\in[\tau],\mathcal{T}_{\tau^{\prime}, i^{*}}\in\Omega_{k}^{*}}\) with \(J_{\bm{\theta}}\)-iteration GD starting from the random initialization \(\{\bm{\theta}_{0}^{(0)},\bm{\theta}_{2}^{(0)}\}\).

**Corollary D.4**.: _For the constants \(\rho\in(0,\mathcal{O}(\frac{1}{L}))\) and \(\xi_{1}\in(0,1)\), given the past records \(\mathcal{P}_{k-1}\), we suppose \(m,\eta_{1},J\) satisfy the conditions in **Theorem 5.2**, and randomly draw the parameters \(\{\bm{\theta}_{1}^{(k),*},\bm{\theta}_{2}^{(k),*}\}\sim\{\widetilde{\bm{ \theta}}_{1}^{(\tau),*},\widetilde{\bm{\theta}}_{2}^{(\tau),*}\}_{\tau\in[k]}\). For the optimal arm batch \(\Omega_{k}^{*}\subset\Omega_{\text{task}}^{k}\), consider its union set with the the collection of past optimal arms \(\mathcal{P}_{k-1}^{*}\cup\{\Omega_{k}^{*},r_{k}^{*}\}\) are generated by a fixed policy when witness the candidate arms \(\{\Omega_{\text{task}}^{(\tau)}\}_{\tau\in[k]}\), with \(\mathcal{P}_{k-1}^{*}\) being the collection chosen by this policy. Then, with probability at least \(1-\delta\), we have_

\[\mathbb{E}_{\mathcal{T}_{k,i}\sim\mathcal{P}(\mathcal{T})}\bigg{[} |f_{2}\bigg{(} \phi(\frac{[\nabla_{\bm{\theta}}f_{1}(\bm{\chi}_{k}^{q,*})]}{c_{g}^{ \prime}L},\bm{\chi}_{k}^{q,*});\bm{\theta}_{2}^{(k-1),*}\bigg{)}-\bigg{(}r_{ \tau}-f_{1}(\bm{\chi}_{k}^{q,*};\bm{\theta}_{1}^{(k-1),*})\bigg{)}|\,\big{|} \Omega_{\text{task}}^{(k)},\mathcal{P}_{k-1}^{*}\bigg{]}\] \[\leq\frac{1}{\sqrt{k}}\cdot\bigg{(}\sqrt{2\xi_{2}}+\frac{3L}{ \sqrt{2}}+(1+\gamma_{1})\sqrt{2\log(\frac{k}{\delta})}\bigg{)}+\Gamma_{k}\]

_where \(r_{\tau,i^{*}}\) is the corresponding reward generated by the mapping function given an arm \(\bm{\chi}_{\tau,i^{*}}\), and_

\[\Gamma_{k}=\bigg{(}1+\mathcal{O}(\frac{kL^{3}\log^{5/6}(m)}{\rho^{1/3}m^{1/6}} )\bigg{)}\cdot\mathcal{O}(\frac{k^{4}L}{\rho\sqrt{m}}\log(m))+\mathcal{O} \bigg{(}\frac{k^{5}L^{2}\log^{11/6}(m)}{\rho^{4/3}m^{1/6}}\bigg{)}.\]

This corollary is a directly application of **Lemma** D.3 and can be obtained with a similar proof as in **Corollary** D.2.

### Ancillary Lemmas

Applying \(\mathcal{P}_{k-1}\) as the training data, we have the following properties for the over-parameterized FC network \(f(\cdot;\bm{\theta})\) after GD.

**Lemma D.5**.: _For the constants \(\rho\in(0,\mathcal{O}(\frac{1}{L}))\) and \(\xi_{1}\in(0,1)\), given the past records \(\mathcal{P}_{k-1}\) up to time step \(k\), we suppose \(m,\eta_{1},J_{1}\) satisfy the conditions in **Theorem 5.2**. Then, with probability at least \(1-\delta\), given a sample-label pair \((\bm{x},r)\), we have_

\[|f(\bm{x};\bm{\theta}^{(k)})|\leq\gamma_{1}=2+\mathcal{O}\left(\frac{k^{3}L}{ \rho\sqrt{m}}\log m\right)+\mathcal{O}\left(\frac{L^{2}k^{4}}{\rho^{4/3}m^{1/6} }\log^{11/6}(m)\right).\]

**Proof.** The LHS of the inequality could be written as

\[|f(\bm{x};\bm{\theta})|\leq |f(\bm{x};\bm{\theta})-f(\bm{x};\bm{\theta}^{(0)})-\langle\nabla_{ \bm{\theta}^{(0)}}f(\bm{x};\bm{\theta}^{(0)}),\bm{\theta}-\bm{\theta}^{(0)}\rangle|\] \[+|f(\bm{x};\bm{\theta}^{(0)})+\langle\nabla_{\bm{\theta}^{(0)}}f( \bm{x};\bm{\theta}^{(0)}),\bm{\theta}-\bm{\theta}^{(0)}\rangle|.\]

Here, we could bound the first term on the RHS with **Lemma** D.7. Applying **Lemma** D.8 on the second term, and recalling \(\|\bm{\theta}-\bm{\theta}^{(0)}\|_{2}\leq\omega\), would give

\[|f(\bm{x};\bm{\theta})| \leq 2+\|\nabla_{\bm{\theta}^{(0)}}f(\bm{x};\bm{\theta}^{(0)})\|_{2} \|\bm{\theta}-\bm{\theta}^{(0)}\|_{2}+\] \[\qquad\mathcal{O}(\omega^{1/3}L^{2}\sqrt{m\log(m)})\cdot\|\bm{ \theta}-\bm{\theta}^{(0)}\|_{2}\] \[\leq 2+\mathcal{O}(L)\cdot\|\bm{\theta}-\bm{\theta}^{(0)}\|_{2}+ \mathcal{O}(L^{2}\sqrt{m\log(m)})(\|\bm{\theta}-\bm{\theta}^{(0)}\|_{2})^{ \frac{4}{3}}.\]

Then, applying the conclusion of **Lemma** D.6 would lead to

\[|f(\bm{x};\bm{\theta})| \leq 2+\mathcal{O}(L)\cdot\mathcal{O}\left(\frac{k^{3}}{\rho\sqrt{m}} \log m\right)+\mathcal{O}(L^{2}\sqrt{m\log(m)})\left(\mathcal{O}(\frac{k^{3}}{ \rho\sqrt{m}}\log m)\right)^{\frac{4}{3}}\] \[=2+\mathcal{O}\left(\frac{k^{3}L}{\rho\sqrt{m}}\log m\right)+ \mathcal{O}\left(\frac{L^{2}k^{4}}{\rho^{4/3}m^{1/6}}\log^{11/6}(m)\right)= \gamma_{1}.\]

**Lemma D.6** (Theorem 1 from [3]).: _For any \(0<\xi_{1}\leq 1\), \(0<\rho\leq{\cal O}(\frac{1}{L})\). Given the past records \({\cal P}_{k-1}\), suppose \(m,\eta_{1},J\) satisfy the conditions in **Theorem 5.2**, then with probability at least \(1-\delta\), we could have_

1. \({\cal L}(\bm{\theta})\leq\xi_{1}\) _after_ \(J\) _iterations of GD._
2. _For any_ \(j\in[J]\)_,_ \(\|\bm{\theta}^{(j)}-\bm{\theta}^{(0)}\|\leq{\cal O}\left(\frac{k^{3}}{\rho \sqrt{m}}\log m\right)\)_._

In particular, **Lemma D.6** above provides the convergence guarantee for \(f(\cdot;\bm{\theta})\) after certain rounds of GD training on the past records \({\cal P}_{k-1}\).

**Lemma D.7** (Lemma 4.1 in [11]).: _Assume a constant \(\omega\) such that \({\cal O}(m^{-3/2}L^{-3/2}[\log(TnL^{2}/\delta)]^{3/2})\leq\omega\leq{\cal O}( L^{-6}[\log m]^{-3/2})\) and \(n\) training samples. With randomly initialized \(\bm{\theta}^{(0)}\), for parameters \(\bm{\theta},\bm{\theta}^{\prime}\) satisfying \(\|\bm{\theta}-\bm{\theta}^{(0)}\|,\|\bm{\theta}-\bm{\theta}^{(0)}\|\leq\omega\), we have_

\[|f(\bm{x};\bm{\theta})-f(\bm{x};\bm{\theta}^{\prime})-\langle\nabla_{\bm{ \theta}^{\prime}}f(\bm{x};\bm{\theta}^{\prime}),\bm{\theta}-\bm{\theta}^{ \prime}\rangle|\leq{\cal O}(\omega^{1/3}L^{2}\sqrt{m\log(m)})\|\bm{\theta}- \bm{\theta}^{\prime}\|\]

_with the probability at least \(1-\delta\)._

**Lemma D.8**.: _Assume \(m,\eta_{1},J\) satisfy the conditions in **Theorem 5.2** and \(\bm{\theta}^{(0)}\) is randomly initialized. Then, with probability at least \(1-\delta\) and given an arm \(\|\bm{x}\|_{2}=1\), we have_

1. \(|f(\bm{x};\bm{\theta}^{(0)})|\leq 2\)_,_
2. \(\|\nabla_{\bm{\theta}^{(0)}}f(\bm{x};\bm{\theta}^{(0)})\|_{2}\leq{\cal O}(L)\)_._

**Proof.** The conclusion (1) is a direct application of Lemma 7.1 in [3]. Suppose the parameters of the \(L\)-layer FC network are \(\bm{\theta}=\{\bm{\theta}_{1},\ldots,\bm{\theta}_{L}\}\). For conclusion (2), applying Lemma 7.3 in [3], for each layer \(\bm{\theta}_{l}\in\{\bm{\theta}_{1},\ldots,\bm{\theta}_{L}\}\), we have

\[\|\nabla_{\bm{\theta}_{l}}f(\bm{x};\bm{\theta}^{(0)})\|_{2}=\|(\bm{\theta}_{L }\bm{D}_{L-1}\cdots\bm{D}_{l+1}\bm{\theta}_{l+1})\cdot(\bm{D}_{l+1}\bm{\theta} _{l+1}\cdots\bm{D}_{1}\bm{\theta}_{1})\cdot\bm{x}^{\intercal}\|_{2}={\cal O}( \sqrt{L}).\]

where \(\bm{D}\) is the diagonal matrix corresponding to the activation function. Then, we could have the conclusion that

\[\|\nabla_{\bm{\theta}^{(0)}}f(\bm{x};\bm{\theta}^{(0)})\|_{2}=\sqrt{\sum_{l \in[L]}\|\nabla_{\bm{\theta}_{l}}f(\bm{x};\bm{\theta}^{(0)})\|_{2}^{2}}={\cal O }(L).\]

\(\square\)

**Lemma D.9** (Theorem 5 in [3]).: _Assume \(m,\eta_{1},J\) satisfy the conditions in **Theorem 5.2** and \(\bm{\theta}^{(0)}\) being randomly initialized. Then, with probability at least \(1-\delta\), and for all parameter \(\bm{\theta}\) such that \(\|\bm{\theta}-\bm{\theta}^{(0)}\|_{2}\leq\omega\), we have_

\[\|\nabla_{\bm{\theta}}f(\bm{x};\bm{\theta})-\nabla_{\bm{\theta}^{(0)}}f(\bm{x };\bm{\theta}^{(0)})\|_{2}\leq{\cal O}(\omega^{1/3}L^{3}\sqrt{\log(m)})\]

**Lemma D.10**.: _Assume \(m,\eta_{1}\) satisfy the condition in **Theorem 5.2**. For notation brevity, suppose the training sample-label pairs are \(\{\bm{x}_{\tau},r_{\tau}\}_{\tau\in[k]}\). With the probability at least \(1-\delta\), we have_

\[\sum_{\tau\in[k]}|f(\bm{x}_{\tau};\bm{\theta}^{(\tau)})-r_{\tau}|\leq\sum_{ \tau\in[k]}|f(\bm{x}_{\tau};\bm{\theta}^{(k)})-r_{\tau}|+\frac{3L\sqrt{2k}}{2}\]

**Proof.** With the notation from Lemma 4.3 in [11], set \(R=\frac{k^{3}\log(m)}{\delta}\), \(\nu=R^{2}\), and \(\epsilon=\frac{LR}{\sqrt{2\nu k}}\). Then, considering the loss function to be \({\cal L}(\bm{\theta}):=\sum_{\tau\in[k]}|f(\bm{x}_{\tau};\bm{\theta})-r_{\tau}|\) would complete the proof. \(\square\)

**Lemma D.11**.: _Consider a randomly initialized L-layer ReLU fully-connected network \(f(\cdot;\bm{\theta}_{0})\). For any \(0<\xi_{2}\leq 1\), \(0<\rho\leq{\cal O}(\frac{1}{L})\). Let there be two sets of training samples \({\cal P}_{k},{\cal P}_{k}^{\prime}\) with the unit-length and the \(\rho\)-separateness assumption, and let \(\bm{\theta}\) be the trained parameter on \({\cal P}_{k}\) while \(\bm{\theta}^{\prime}\) is the trained parameter on \({\cal P}_{k}^{\prime}\). Suppose the conditions in **Theorem 5.2** are satisfied. Then, with probability at least \(1-\delta\), we have_

\[|f(\bm{x};\bm{\theta})-f(\bm{x};\bm{\theta}^{\prime})|\leq\left(1+{\cal O}( \frac{kL^{3}\log^{5/6}(m)}{\rho^{1/3}m^{1/6}})\right)\cdot{\cal O}(\frac{k^{3}L} {\rho\sqrt{m}}\log(m))+{\cal O}\!\left(\frac{k^{4}L^{2}\log^{11/6}(m)}{\rho^{4/ 3}m^{1/6}}\right)\]

_when given a new sample \(\bm{x}\in\mathbb{R}^{d}\)._

**Proof.** First, based on the conclusion from Theorem 1 from [3] and regarding the \(t\) samples, the trained the parameters satisfy \(\|\bm{\theta}-\bm{\theta}_{0}\|_{2},\|\bm{\theta}^{\prime}-\bm{\theta}_{0}\|_{2} \leq\mathcal{O}(\frac{k^{3}}{\rho\sqrt{m}}\log(m))=\omega\) where \(\bm{\theta}_{0}\) is the randomly initialized parameter. Then, we could have

\[\|\nabla_{\bm{\theta}}f(\bm{x};\bm{\theta})\|_{2} \leq\|\nabla_{\bm{\theta}_{0}}f(\bm{x};\bm{\theta}_{0})\|_{2}+\| \nabla_{\bm{\theta}}f(\bm{x};\bm{\theta})-\nabla_{\bm{\theta}_{0}}f(\bm{x}; \bm{\theta}_{0})\|_{2}\] \[\leq\bigg{(}1+\mathcal{O}(\frac{kL^{3}\log^{5/6}(m)}{\rho^{1/3} m^{1/6}})\bigg{)}\cdot\mathcal{O}(L)\]

w.r.t. the conclusion from Theorem 1 and Theorem 5 of [3]. Then, regarding the Lemma 4.1 from [11], we would have

\[|f(\bm{x};\bm{\theta})-f(\bm{x};\bm{\theta}^{\prime})-\langle \nabla_{\bm{\theta}^{\prime}}f(\bm{x};\bm{\theta}^{\prime}),\bm{\theta}-\bm{ \theta}^{\prime}\rangle|\leq\mathcal{O}(\omega^{1/3}L^{2}\sqrt{m\log(m)}) \cdot\|\bm{\theta}-\bm{\theta}^{\prime}\|_{2}.\]

Therefore, the our target could be reformed as

\[|f(\bm{x};\bm{\theta})-f(\bm{x};\bm{\theta}^{\prime})|\leq\| \nabla_{\bm{\theta}^{\prime}}f(\bm{x};\bm{\theta}^{\prime})\|_{2}\|\bm{\theta }-\bm{\theta}^{\prime}\|_{2}+\mathcal{O}(\omega^{1/3}L^{2}\sqrt{m\log(m)}) \cdot\|\bm{\theta}-\bm{\theta}^{\prime}\|_{2}\] \[\quad\leq\bigg{(}1+\mathcal{O}(\frac{kL^{3}\log^{5/6}(m)}{\rho^{ 1/3}m^{1/6}})\bigg{)}\cdot\mathcal{O}(L)\cdot\omega+\mathcal{O}(\omega^{4/3}L^ {2}\sqrt{m\log(m)})\]

Substituting the \(\omega\) with its value would complete the proof.

\(\Box\)

**Corollary D.12**.: _Following a similar settings as in **Lemma D.11**, consider a randomly initialized \(L\)-layer fully-connected network \(f(\cdot;\bm{\theta}_{0})\) with Sigmoid activation. For any \(0<\xi_{2}\leq 1\), \(0<\rho\leq\mathcal{O}(\frac{1}{L})\). Let there be two sets of training samples \(\mathcal{P}_{k},\mathcal{P}^{\prime}_{k}\) with the unit-length and the \(\rho\)-separateness assumption, and let \(\bm{\theta}\) be the trained parameter on \(\mathcal{P}_{k}\) while \(\bm{\theta}^{\prime}\) is the trained parameter on \(\mathcal{P}^{\prime}_{k}\). Suppose the conditions in **Theorem 5.2** are satisfied. Then, with probability at least \(1-\delta\), we have_

\[|f(\bm{x};\bm{\theta})-f(\bm{x};\bm{\theta}^{\prime})|\leq\bigg{(}1+\mathcal{ O}(\frac{kL^{3}\log^{5/6}(m)}{\rho^{1/3}m^{1/6}})\bigg{)}\cdot\mathcal{O}( \frac{k^{3}L}{\rho\sqrt{m}}\log(m))+\mathcal{O}\bigg{(}\frac{k^{4}L^{2}\log^{ 11/6}(m)}{\rho^{4/3}m^{1/6}}\bigg{)}\]

_when given a new sample \(\bm{x}\in\mathbb{R}^{d}\)._

**Proof.** This corollary is an intuitive extension of **Lemma** D.11. Since the result from Theorem 1 of [3] also applies to Lipschitz-smooth (i.e., Sigmoid) activation functions, combining the proof of **Lemma** D.11 and the result from Lemma 7 in [47] will give the conclusion.

\(\Box\)

### Regret Bound for Uniform Sampling

**Lemma D.13** (Regret Bound for the Uniform Sampling Approach).: _When applying the uniform sampling as in most meta-learning frameworks, we denote the corresponding sampled task series as \(\Omega_{u}(K)\). We will have \(R_{u}(K)=\mathbb{E}_{\mathcal{T}\sim\mathcal{P}(\mathcal{T}),\bm{x}\sim \mathcal{D}_{\mathcal{T}}}\bigg{[}\mathcal{L}(\bm{x};\mathcal{I}(\mathcal{T}, \bm{\Theta}_{u}^{(K)}))-\mathcal{L}(\bm{x};\mathcal{I}(\mathcal{T},\bm{ \Theta}^{(K),*}))\bigg{]}\). where \(\bm{\Theta}_{u}^{(K)}\) refer to the meta-parameters trained with uniform sampling. With \(\|\bm{\Theta}_{u}^{(K)}-\bm{\Theta}^{(K),*}\|_{2}\leq\omega\), we have the regret bound for the uniform sampling as_

\[R_{u}(K) =\mathbb{E}_{\mathcal{T}\sim\mathcal{P}(\mathcal{T}),\bm{x}\sim \mathcal{D}_{\mathcal{T}}}\bigg{[}\mathcal{L}(\bm{x};\mathcal{I}(\mathcal{T}, \bm{\Theta}_{u}^{(K)}))-\mathcal{L}(\bm{x};\mathcal{I}(\mathcal{T},\bm{ \Theta}^{(K),*}))\bigg{]}\] \[\leq\sqrt{m_{\mathcal{F}}L_{\mathcal{F}}}\cdot\omega+\mathcal{O} (\omega^{4/3}L_{\mathcal{F}}^{3}\sqrt{m_{\mathcal{F}}\log(m_{\mathcal{F}})})+ \mathcal{O}(\sqrt{\frac{L_{\mathcal{F}}}{m_{\mathcal{F}}}})\] \[\leq\min\bigg{\{}\mathcal{O}\bigg{(}KL_{\mathcal{F}}+\frac{K^{4/ 3}L_{\mathcal{F}}^{11/3}\sqrt{\log(m_{\mathcal{F}})}}{m_{\mathcal{F}}^{1/6}}+ \sqrt{\frac{L_{\mathcal{F}}}{m_{\mathcal{F}}}}\bigg{)},\ 1\bigg{\}}\]

**Proof.** Here, for the simplicity of notation, we denote \(\bm{\Theta}=\mathcal{I}(\mathcal{T},\bm{\Theta})\), and neglect the expectation terms. Note that the difference between adapted meta-parameters and the original meta-parameters issmall enough and can be well-bounded. We will then have

\[R_{u}(K) =\mathbb{E}_{\mathcal{T}\sim\mathcal{P}(\mathcal{T}),\bm{x}\sim \mathcal{D}_{\mathcal{T}}}\bigg{[}\mathcal{L}(\bm{x};\mathcal{I}(\mathcal{T}, \bm{\Theta}_{u}^{(K)}))-\mathcal{L}(\bm{x};\mathcal{I}(\mathcal{T},\bm{\Theta} ^{(K),*}))\bigg{]}\] \[=\widetilde{\mathcal{L}}(\widetilde{\bm{\Theta}}_{u}^{(K)})- \widetilde{\mathcal{L}}(\widetilde{\bm{\Theta}}^{(K),*})\]

where the two sets of meta-parameters are trained with uniformly sampled tasks and the optimal tasks, and \(\widetilde{\bm{\Theta}}\) is used to denote the adapted meta-parameters \(\mathcal{I}(\mathcal{T},\bm{\Theta})\) for simplicity. With any convex loss function (e.g., \(L_{2}\) loss or cross-entropy loss) under the over-parameterization settings, we will have the generalization loss being almost convex w.r.t. the meta-parameters as in **Lemma** D.14, which leads to

\[\widetilde{\mathcal{L}}(\widetilde{\bm{\Theta}}_{u}^{(K)})- \widetilde{\mathcal{L}}(\widetilde{\bm{\Theta}}^{(K),*})\leq\langle\nabla_{ \widetilde{\bm{\Theta}}}\widetilde{\mathcal{L}}(\widetilde{\bm{\Theta}}_{u}^{ (K)}),\widetilde{\bm{\Theta}}_{u}^{(K)}-\widetilde{\bm{\Theta}}^{(K),*}\rangle+\epsilon\] \[\quad\leq\|\nabla_{\widetilde{\bm{\Theta}}}\widetilde{\mathcal{L} }(\widetilde{\bm{\Theta}}_{u}^{(K)})\|_{2}\|\widetilde{\bm{\Theta}}_{u}^{(K)}- \widetilde{\bm{\Theta}}^{(K),*}\|_{2}+\epsilon\] \[\quad\leq\|\nabla_{\widetilde{\bm{\Theta}}}\widetilde{\mathcal{L} }(\widetilde{\bm{\Theta}}_{u}^{(K)})\|_{2}\|\bm{\Theta}_{u}^{(K)}-\bm{\Theta}^{ (K),*}\|_{2}+\eta_{1}\cdot\mathcal{O}(\sqrt{m_{\mathcal{F}}L_{\mathcal{F}}})+\epsilon\] \[\quad\stackrel{{\text{(i)}}}{{\leq}}\sqrt{m_{ \mathcal{F}}L_{\mathcal{F}}}\cdot\omega+\mathcal{O}(\omega^{4/3}L_{\mathcal{F}} ^{3}\sqrt{m_{\mathcal{F}}\log(m_{\mathcal{F}})})+\mathcal{O}(\sqrt{\frac{L_{ \mathcal{F}}}{m_{\mathcal{F}}}})\] \[\quad\stackrel{{\text{(ii)}}}{{\leq}}\mathcal{O} \bigg{(}KL_{\mathcal{F}}+\frac{K^{4/3}L_{\mathcal{F}}^{11/3}\sqrt{\log(m_{ \mathcal{F}})}}{m_{\mathcal{F}}^{1/6}}+\sqrt{\frac{L_{\mathcal{F}}}{m_{ \mathcal{F}}}}\bigg{)}\] \[\quad\stackrel{{\text{(iii)}}}{{\Longrightarrow}} \widetilde{\mathcal{L}}(\widetilde{\bm{\Theta}}_{u}^{(K)})- \widetilde{\mathcal{L}}(\widetilde{\bm{\Theta}}^{(K),*})\leq\min\bigg{\{} \mathcal{O}\bigg{(}KL_{\mathcal{F}}+\frac{K^{4/3}L_{\mathcal{F}}^{11/3}\sqrt{ \log(m_{\mathcal{F}})}}{m_{\mathcal{F}}^{1/6}}+\sqrt{\frac{L_{\mathcal{F}}}{m_ {\mathcal{F}}}}\bigg{)},\;1\bigg{\}}\]

where \(\epsilon=\mathcal{O}(\omega^{4/3}L_{\mathcal{F}}^{3}\sqrt{m_{\mathcal{F}}\log( m_{\mathcal{F}})})>0\), and \(\|\bm{\Theta}_{u}^{(K),*}-\bm{\Theta}_{u}^{(K)}\|_{2}\leq\omega\). Here, the first inequality is due to **Lemma** D.14 and the convexity of the loss function. The third inequality is due to the upper bound for meta-model gradients **(Lemma** D.15). The (i) is due to **Lemma** D.16 and sufficiently small learning rate \(\eta_{1}\leq\mathcal{O}(\frac{1}{m_{\mathcal{F}}})\). Based on **Lemma** D.15, we will have \(\|\nabla_{\bm{\Theta}}\mathcal{L}(\bm{x};\bm{\Theta}_{K}^{(J),*})\|_{2},\| \nabla_{\bm{\Theta}}\mathcal{L}(\bm{x};\bm{\Theta}_{K}^{(J)})\|_{2}\leq \mathcal{O}(\sqrt{m_{\mathcal{F}}L_{\mathcal{F}}})\). Since we have \(\eta_{1},\eta_{2}\leq\mathcal{O}(\frac{1}{m})\), starting from randomly initialized \(\bm{\Theta}^{(0)}\), the parameter shift caused by GD can be upper bounded by \(\|\bm{\Theta}_{u}^{(K),*}-\bm{\Theta}_{u}^{(K)}\|_{2}\leq\omega=\mathcal{O}(K \cdot\sqrt{\frac{L_{\mathcal{F}}}{m_{\mathcal{F}}}})\). The implication (iii) is because the loss function \(\mathcal{L}(\cdot;\cdot)\) has the value range [0, 1]. 

Here, we notice that the RHS of the regret bound in **Lemma** D.13 has two terms. Although the second term can be reduced to \(\mathcal{O}(1)\) with sufficiently large meta-model width \(m_{\mathcal{F}}>\mathcal{O}(\text{Poly}(K,L,\rho^{-1}))\), the first term tends to grow along with more iterations \(K\) and the larger meta-model width \(m_{\mathcal{F}}\). The reason is that the radius for the parameter shift during meta-training \(\omega\) can be as large as \(\mathcal{O}(\frac{1}{\sqrt{m_{\mathcal{F}}}})\), which means that it cannot cancel out the effects of gradient norms, which have the order of \(\mathcal{O}(\sqrt{m_{\mathcal{F}}})\). In this case, we will not able to include a \(m_{\mathcal{F}}\) term to the denominator to scale down the regret with \(m_{\mathcal{F}}\), and make the upper bound narrower than 1.

**Lemma** D.14.: _Given an arbitrary sample \(\bm{x}\) and its label, let \(\widetilde{\mathcal{L}}(\bm{\Theta})=\mathcal{L}(\bm{x};\bm{\Theta})\). Suppose \(m_{\mathcal{F}},\eta_{1},\eta_{2}\) satisfy the conditions in Theorem 5.2. With probability at least \(1-\mathcal{O}(KL_{\mathcal{F}}^{2})\cdot\exp[-\Omega(m_{\mathcal{F}}\omega^{2/3} L_{\mathcal{F}})]\) over randomness of \(\bm{\Theta}^{(0)}\), for all \(k\in[K]\), and \(\bm{\Theta},\bm{\Theta}^{\prime}\) satisfying \(\|\bm{\Theta}-\bm{\Theta}^{(0)}\|_{2}\leq\omega\) and \(\|\bm{\Theta}^{\prime}-\bm{\Theta}^{(0)}\|_{2}\leq\omega\) with \(\omega\leq\mathcal{O}(L_{\mathcal{F}}^{-6}[\log m_{\mathcal{F}}]^{-3/2})\), it holds uniformly that_

\[\widetilde{\mathcal{L}}(\bm{\Theta})-\widetilde{\mathcal{L}}(\bm{\Theta}^{\prime}) \leq\langle\nabla_{\bm{\Theta}}\widetilde{\mathcal{L}}(\bm{\Theta}),\bm{\Theta}- \bm{\Theta}^{\prime}\rangle+\epsilon.\]

_with \(\epsilon=\mathcal{O}(\omega^{4/3}L_{\mathcal{F}}^{3}\sqrt{\log m_{\mathcal{F}}}))\) being a small constant._

proof.This proof follows an analogous approach as the proof of Lemma 4.2 in [11]. Let \(\nabla_{\mathcal{F}}\widetilde{\mathcal{L}}(\bm{\Theta}^{\prime})\) be the derivative of \(\widetilde{\mathcal{L}}\) with respective to \(\mathcal{F}(\bm{x};\bm{\Theta})\). Then, it holds that \(|\nabla_{\mathcal{F}}\widetilde{\mathcal{L}}(\bm{\Theta}^{\prime})|\leq\mathcal{O }(1)\) based on 

**Lemma D.15**.: Then, by convexity of \(\widetilde{\mathcal{L}}\), we have

\[\widetilde{\mathcal{L}}(\mathbf{\Theta}^{\prime})- \widetilde{\mathcal{L}}(\mathbf{\Theta})\] \[\overset{\text{(i)}}{\geq}\nabla_{\mathcal{F}}\widetilde{ \mathcal{L}}(\mathbf{\Theta})\cdot(\mathcal{F}(\mathbf{x};\mathbf{\Theta}^{ \prime})-\mathcal{F}(\mathbf{x};\mathbf{\Theta}))\] \[\overset{\text{(ii)}}{\geq}\nabla_{\mathcal{F}}\widetilde{ \mathcal{L}}(\mathbf{\Theta}^{\prime})\cdot\langle\nabla_{\mathbf{\Theta}} \mathcal{F}(\mathbf{x};\mathbf{\Theta}),\mathbf{\Theta}^{\prime}-\mathbf{ \Theta}\rangle\] \[\qquad-|\nabla_{\mathcal{F}}\widetilde{\mathcal{L}}(\mathbf{ \Theta}^{\prime})|\cdot|\mathcal{F}(\mathbf{x};\mathbf{\Theta}^{\prime})- \mathcal{F}(\mathbf{x};\mathbf{\Theta})-\langle\nabla\mathcal{F}(\mathbf{x}; \mathbf{\Theta}),\mathbf{\Theta}^{\prime}-\mathbf{\Theta}\rangle|\] \[\geq\langle\nabla_{\mathbf{\Theta}}\widetilde{\mathcal{L}}( \mathbf{\Theta}),\mathbf{\Theta}^{\prime}-\mathbf{\Theta}\rangle-|\nabla_{ \mathcal{F}}\widetilde{\mathcal{L}}(\mathbf{\Theta}^{\prime})|\cdot|\mathcal{F }(\mathbf{x};\mathbf{\Theta}^{\prime})-\mathcal{F}(\mathbf{x};\mathbf{\Theta}) -\langle\nabla\mathcal{F}(\mathbf{x};\mathbf{\Theta}),\mathbf{\Theta}^{ \prime}-\mathbf{\Theta}\rangle|\] \[\overset{\text{(iii)}}{\geq}\langle\nabla_{\mathbf{\Theta}} \widetilde{\mathcal{L}}(\mathbf{\Theta}),\mathbf{\Theta}^{\prime}-\mathbf{ \Theta}\rangle-\mathcal{O}(\omega^{4/3}L_{\mathcal{F}}^{3}\sqrt{m_{\mathcal{F} }\log(m_{\mathcal{F}})})\] \[\geq\langle\nabla_{\mathbf{\Theta}}\widetilde{\mathcal{L}}( \mathbf{\Theta}),\mathbf{\Theta}^{\prime}-\mathbf{\Theta}\rangle-\epsilon\]

where (i) is due to the convexity of the loss function \(\mathcal{L}\), (ii) is an application of triangle inequality, and (iii) is the application of and **Lemma** D.16. Finally, denoting \(\epsilon=\mathcal{O}(\omega^{4/3}L_{\mathcal{F}}^{3}\sqrt{m_{\mathcal{F}}\log m _{\mathcal{F}}})\) will complete the proof.

**Lemma D.15**.: _Suppose \(m_{\mathcal{F}},\eta_{1},\eta_{2}\) satisfy the conditions in Theorem 5.2. With probability at least \(1-\mathcal{O}(KL_{\mathcal{F}})\cdot\exp(-\Omega(m_{\mathcal{F}}\omega^{2/3}L _{\mathcal{F}}))\) over the random initialization, \(\mathbf{\Theta}\) satisfying \(\|\mathbf{\Theta}-\mathbf{\Theta}^{(0)}\|_{2}\leq\omega\) with \(\omega\leq\mathcal{O}(L_{\mathcal{F}}^{-9/2}[\log m_{\mathcal{F}}]^{-3})\), it holds uniformly that_

\[\|\nabla_{\mathbf{\Theta}}\mathcal{F}(\mathbf{x};\mathbf{\Theta} )\|_{2} \leq\mathcal{O}(\sqrt{m_{\mathcal{F}}L_{\mathcal{F}}}),\] \[\|\nabla_{\mathbf{\Theta}}\mathcal{L}(\mathbf{x};\mathbf{\Theta} )\|_{2} \leq\mathcal{O}(\sqrt{m_{\mathcal{F}}L_{\mathcal{F}}}).\]

**Proof.** This lemma is a direct application of Lemma 9 of [47] and Lemma B.2, B.3 of [11].

**Lemma D.16**.: _Suppose \(m_{\mathcal{F}},\eta_{1},\eta_{2}\) satisfy the conditions in Theorem 5.2. With probability at least \(1-\mathcal{O}(KL_{\mathcal{F}})\cdot\exp(-\Omega(m_{\mathcal{F}}\omega^{2/3} L_{\mathcal{F}}))\), for all \(t\in[T],i\in[k]\), \(\mathbf{\Theta},\mathbf{\Theta}^{\prime}\) (or \(\Theta,\Theta^{\prime}\) ) satisfying \(\|\mathbf{\Theta}-\mathbf{\Theta}^{(0)}\|_{2},\|\mathbf{\Theta}^{\prime}- \mathbf{\Theta}^{(0)}\|_{2}\leq\omega\) with \(\omega\leq\mathcal{O}(L_{\mathcal{F}}^{-9/2}[\log m_{\mathcal{F}}]^{-3})\), it holds uniformly that_

\[|\mathcal{F}(\mathbf{x};\mathbf{\Theta})-\mathcal{F}(\mathbf{x};\mathbf{\Theta }^{\prime})-\langle\nabla_{\mathbf{\Theta}^{\prime}}\mathcal{F}(\mathbf{x}; \mathbf{\Theta}^{\prime}),\mathbf{\Theta}-\mathbf{\Theta}^{\prime}\rangle| \leq\mathcal{O}(w^{1/3}L_{\mathcal{F}}^{2}\sqrt{m_{\mathcal{F}}\log(m_{ \mathcal{F}})})\|\mathbf{\Theta}-\mathbf{\Theta}^{\prime}\|_{2}.\]

**Proof.** The proof for this lemma directly follows the proof of Lemma 4.1 in [11] and Lemma 7 in [47].