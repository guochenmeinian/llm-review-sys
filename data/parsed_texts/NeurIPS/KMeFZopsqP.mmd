# First Order Methods with Markovian Noise: from Acceleration to Variational Inequalities

 Aleksandr Beznosikov

Innopolis University, Skoltech, MIPT, Yandex

&Sergey Samsonov

HSE University

&Marina Sheshukova

HSE University

Alexander Gasnikov

MIPT, Skoltech, IITP RAS

&Alexey Naumov

HSE University

&Eric Moulines

Ecole polytechnique

###### Abstract

This paper delves into stochastic optimization problems that involve Markovian noise. We present a unified approach for the theoretical analysis of first-order gradient methods for stochastic optimization and variational inequalities. Our approach covers scenarios for both non-convex and strongly convex minimization problems. To achieve an optimal (linear) dependence on the mixing time of the underlying noise sequence, we use the randomized batching scheme, which is based on the multilevel Monte Carlo method. Moreover, our technique allows us to eliminate the limiting assumptions of previous research on Markov noise, such as the need for a bounded domain and uniformly bounded stochastic gradients. Our extension to variational inequalities under Markovian noise is original. Additionally, we provide lower bounds that match the oracle complexity of our method in the case of strongly convex optimization problems.

## 1 Introduction

Stochastic gradient methods are an essential ingredient for solving various optimization problems, with a wide range of applications in various fields such as machine learning [36, 37], empirical risk minimization problems [96], and reinforcement learning [93, 85, 69]. Various stochastic gradient descent methods (SGD) and their accelerated versions [75, 31] have been extensively studied under different statistical frameworks [17, 97]. The standard assumption for stochastic optimization algorithms is to consider independent and identically distributed noise variables. However, the growing usage of stochastic optimization methods in reinforcement learning [10, 87, 25] and distributed optimization [63, 18, 65] has led to increased interest in problems with Markovian noise. Despite this, existing theoretical works that consider Markov noise have significant limitations, and their analysis often results in suboptimal finite-time error bounds.

Our research aims to fill the gap in the existing literature on the first-order Markovian setting. By focusing on uniformly geometrically ergodic Markov chains, we obtain finite-time complexity bounds for achieving \(\varepsilon\)-accurate solutions that scale linearly with the mixing time of the underlying Markov chain. Our approach is based on careful applications of randomized batch size schemes and provides a unified view on both non-convex and strongly convex minimization problems, as well as variational inequalities.

**Our contributions.** Our main contributions are the following:

\(\diamond\) **Accelerated SGD.** We provide the first analysis of SGD, including the Nesterov accelerated SGD method, with Markov noise without the assumption of bounded domain and uniformly bounded stochastic gradient estimates. Our results are summarised in Table 1 and Section 2.1 and cover both strongly convex and non-convex scenarios. Our findings for non-convex minimization problems complement the results obtained in [21].

\(\diamond\)**Lower bounds.** In Section 2.2 we give the lower bounds showing that the presence of mixing time in the upper complexity bounds is not an artefact of the proof. This is consistent with the results reported in [71].

\(\diamond\)**Extensions.** In Section 2.4 we provide, as far as we know, the first analysis for variational inequalities with general stochastic Markov oracle, arbitrary optimization set, and arbitrary composite term. Our finite-time performance analysis provides complexity bounds in terms of oracle calls that scale linearly with the mixing time of the underlying chain, which is an improvement over the bounds obtained in [99] for the Markov setting.

**Related works.** Next, we briefly summarize the related works.

\(\diamond\)**Stochastic gradient methods.** Numerous research papers have reported significant improvements achieved by accelerated methods for stochastic optimization with stochastic gradient oracles involving independent and identically distributed (i.i.d.) noise. These methods have been extensively studied in theory [44, 14, 16, 58, 61, 26, 30, 97, 94, 3, 39, 102] and have shown practical success [55, 91]. The finite-time analysis of first-order methods in i.i.d. noise settings has been extensively studied by many authors, as discussed in [59] and references therein. In Table 1 we include only some important results because i.i.d. setting is not in the interest of this paper.

While the literature on i.i.d. noise is extensive, existing research on the first-order Markovian setting is relatively sparse. In this study, we focus on Markov chains that are uniformly geometrically ergodic, and we refer the reader to Section 2 for detailed definitions. We note that the complexity bounds which scale linearly with the mixing time of the underlying general Markov chain are currently available only for general convex and non-convex minimization problems. Namely, [23] has investigated a version of the ergodic mirror descent algorithm that yields optimal convergence rates for Lipschitz, general convex and non-convex problems. Recently, [21] proposed a random batch size algorithm that adapts to the mixing time of the underlying chain for non-convex optimization with a compact domain. In particular, [21, Theorem 4.3] yields optimal complexity rates in terms of the number of oracle calls required for non-convex problems, which is consistent with the results obtained in [23]. Unlike previous studies, this method is insensitive to the mixing time of the noise sequence.

For the general case of Markovian noise the finite-time analysis of non-accelerated SGD-type algorithms was carried out in [90] and [19]. However, [90] heavily relies on the bounded domain assumption and uniformly bounded stochastic gradient oracles, while its bound in [90, Theorem 5] has a suboptimal dependence on the mixing time of the underlying chain, see Table 1. Additionally, [90] does not cover the strongly convex setting. On the other hand, [19] covers both non-convex and strongly convex settings, but the bounds of [19, Theorem 1] has terms that are _exponential_ in the mixing time, and a careful examination reveals suboptimal dependence on the initial condition for strongly convex problems when SGD is applied.

In the study of Nesterov-accelerated SGD with Markovian noise, the authors of [20] considered the use of a batch size of 1 and achieved a rate of forgetting the initial condition that matches that of the i.i.d. noise setting. However, their result is suboptimal in terms of the variance terms in both non-convex and strongly convex settings, as detailed in Table 1. We emphasize that the case of unbounded gradient oracles with Markov noise is not treated in contrast to the i.i.d. setup [97, 62].

The above papers deal with general Markovian noise optimization. But there are also results that deal with Markovian stochasticity with a finite state space. Here we can highlight the work [28], where the author gives quite extensive results and achieves linear scaling by mixing time in the non-convex as well as strongly convex cases. Recently, numerous papers have appeared dealing with the special scenario of distributed optimization [89]. [99] investigates the generalization and stability of Markov SGD with special attention to the excess variance guarantees. We note that first, these algorithms only need to deal with a very special case of Markov gradients, and second, the corresponding dependence on the mixing time of the Markov chain is again quadratic. At the same time, there exist particular results, e.g. [71], which provide a lower bound for the particular finite sum problems in the Markovian setting.

\(\diamond\)**Variational inequalities.** Variational inequalities [29] have been an active area of research in applied mathematics for more than half a century [78, 41, 86]. VI cover important special cases, e.g., minimization over a convex domain, saddle point or min-max and fixed point problems. computational game theory [29], robust [7] and nonsmooth [73, 72] optimization, supervised [51, 4] and unsupervised [103, 5] learning, image denoising [27, 11]. In the last 5 years, variational inequalities and their special cases have attracted much interest in the machine learning community due to new connections to reinforcement learning [79; 50], adversarial training [64], and GANs [15; 33; 66; 12; 60; 82].

Variational inequalities (VI) and saddle point problems have their own well-established theory and methods. Unlike minimization problems, solving variational inequalities doesn't rely on (accelerated) gradient descent. Instead, the extragradient method [57], various modified versions [72; 42], or similar techniques [95] are recommended as the basic and theoretically optimal methods. While deterministic methods have long been used for solving variational inequalities, stochastic methods have gained importance only in the last 15 years, following pioneering works by [49; 52]. We summarise the results on methods for stochastic variational inequalities with the Lipschitz operator and smooth stochastic saddle point problems in Table 2. The number of papers dealing with stochastic VIs and saddle point problems is small compared to those dealing with stochastic optimization, we include in Table 2 papers with the i.i.d. noise (which we do not do for stochastic optimization). The only competing work dealing with Markovian noise in saddle point problems consider the finite sum problem and thus the finite Markov chain [99], therefore we do not include it in Table 2. Moreover, the results from [99] has much worse oracle complexity guarantees \(\mathcal{O}(\tau^{2}/\varepsilon^{2})\) in terms of \(\tau\). There are more papers dealing with stochastic finite-sum variational inequalities or saddle point problems, but in the i.i.d. setting [12; 80; 104; 2; 8]. We also do not consider in Table 2 because of the difference in the stochastic oracle structure. It is important to note that, unlike most previous works, we consider the most general formulation of VI itself for an arbitrary optimization set and composite term.

**Notations and definitions.** Let \((\mathsf{Z},\mathsf{d}_{\mathsf{Z}})\) be a complete separable metric space endowed with its Borel \(\sigma\)-field \(\mathcal{Z}\). Let \((\mathsf{Z}^{\mathbb{N}},\mathscr{Z}^{\mathbb{N}})\) be the corresponding canonical process. Consider the Markov kernel \(\mathsf{Q}\) defined on \(\mathsf{Z}\times\mathcal{Z}\), and denote by \(\mathbb{P}_{\xi}\) and \(\mathbb{E}_{\xi}\) the corresponding probability distribution and the expected value with initial distribution \(\xi\). Without loss of generality, we assume that \((Z_{k})_{k\in\mathbb{N}}\) is the corresponding canonical process. By construction, for any \(A\in\mathcal{Z}\), it holds that \(\mathbb{P}_{\xi}(Z_{k}\in A|Z_{k-1})=\mathrm{Q}(Z_{k-1},A)\), \(\mathbb{P}_{\xi}\)-a.s. If \(\xi=\delta_{z}\), \(z\in\mathsf{Z}\), we write \(\mathbb{P}_{z}\) and \(\mathbb{E}_{z}\) instead of \(\mathbb{P}_{\delta_{z}}\) and \(\mathbb{E}_{\delta_{z}}\), respectively. For \(x^{1},\ldots,x^{k}\) being the iterates of any stochastic first-order method, we denote \(\mathcal{F}_{k}=\sigma(x^{j},j\leq k)\) and write \(\mathbb{E}_{k}\) as an alias for \(\mathbb{E}[\cdot|\mathcal{F}_{k}]\). We also write \(\mathbb{N}^{*}:=\mathbb{N}\setminus\{0\}\). For the sequences \((a_{n})_{n\in\mathbb{N}}\) and \((b_{n})_{n\in\mathbb{N}}\) we write \(a_{n}\lesssim b_{n}\) if there exists a constant \(c\) such that that \(a_{n}\leq cb_{n}\) for all \(n\in\mathbb{N}\).

## 2 Main results

**Assumptions.** In this paper we study the minimization problem

\[\min_{x\in\mathbb{R}^{d}}f(x):=\mathbb{E}_{Z\sim\pi}[F(x,Z)]\,,\] (1)

where the access to the function \(f\) and its gradient is available only through the (unbiased) noisy oracle \(F(x,Z)\) and \(\nabla F(x,Z)\), respectively. In the following presentation we impose at least one of the following regularity constraint on the underlying function \(f\) itself:

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \cline{2-7} \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{**Unbiased**} & \multicolumn{1}{c|}{**Unbiased**} & \multicolumn{1}{c|}{**General**} & \multicolumn{1}{c|}{**Conditional probability distribution**} & \multicolumn{1}{c|}{**Model probability distribution**} \\ \cline{2-7} \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{**Model probability**} & \multicolumn{1}{c|}{**Domain**} & \multicolumn{1}{c|}{**Gradient**} & \multicolumn{1}{c|}{**General**} & \multicolumn{1}{c|}{**Gradient**} & \multicolumn{1}{c|}{**Gradient**} & \multicolumn{1}{c|}{**Gradient probability distribution**} & \multicolumn{1}{c|}{**Gradient probability distribution**} \\ \hline \multirow{3}{*}{**Algorithm**} & **592 (\(\mathsf{Z}_{k}\), 12)** & ✓ & ✗ & Nx & ✓ & \(\partial\left(\left(\mu\delta^{\prime}\right)-\left(\mu^{\prime}\right) \left(\mu^{\prime}\right)\left(\mu^{\prime}\right)\left(\mu^{\prime}+\delta^{ \prime}\right)\right)\) & \(\partial\left(\left(\mu\delta^{\prime}\right)-\left(\mu^{\prime}\right)\left( \mu^{\prime}+\delta^{\prime}\right)\right)\) \\ \cline{2-7}  & **4891 (\(\mathsf{Z}_{k}\), 17)** & ✓ & ✗ & ✗ & ✓ & \(\partial\left(\left(\mu\delta^{\prime}\right)-\left(\mu^{\prime}\right) \left(\mu^{\prime}+\delta^{\prime}\right)\right)\) & \(\partial\left(\left(\left(\mu\delta^{\prime}\right)\left(\mu^{\prime}+\delta^{ \prime}\right)\right)\right)\) \\ \hline \multirow{3}{*}{**Algorithm**} & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & \(\partial\left(\left(\mu\delta^{\prime}\right)-\left(\mu^{\prime}\right)\left( \mu^{\prime}+\delta^{\prime}\right)\right)\) & \(\partial\left(\left(\mu\delta^{\prime}\right)\left(\mu^{\prime}+\delta^{ \prime}\right)\right)\) \\ \cline{2-7}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & \(\partial\left(\left(\mu\delta^{\prime}\right)-\left(\mu^{\prime}\right)\left( \mu^{\prime}+\delta^{\prime}\right)\right)\) \\ \hline \multirow{3}{*}{**Algorithm**} & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & \(\partial\left(\left(\mu\delta^{\prime}\right)-\left(\mu^{\prime}\right)\left( \mu^{\prime}+\delta^{\prime}\right)\right)\) & \(\partial\left(\left(\mu\delta^{\prime}\right)\left(\mu^{\prime}+\delta^{\prime} \right)\right)\) \\ \cline{2-7}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & \(\partial\left(\left(\mu\delta^{\prime}\right)-\left(\mu^{\prime}\right)\left( \mu^{\prime}+\delta^{\prime}\right)\right)\) \\ \cline{2-7}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & \(\partial\left(\left(\mu\delta^{\prime}\right)\left(\mu^{\prime}+\delta^{\prime} \right)\right)\) \\ \hline \multirow{3}{*}{**Algorithm**} & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \cline{1-1}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \cline{1-1}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \cline{1-1}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \cline{1-1}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \cline{1-1}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \cline{1-1}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \cline{1-1}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \cline{1-1}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \cline{1-1}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \cline{1-1}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \cline{1-1}  & **802 (\(\mathsf{Z}_{k}\), 12)** & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & �

**A 1**.: _The function \(f\) is \(L\)-smooth on \(\mathbb{R}^{d}\) with \(L>0\), i.e., it is differentiable and there is a constant \(L>0\) such that the following inequality holds for all \(x,y\in\mathbb{R}^{d}\):_

\[\|\nabla f(x)-\nabla f(y)\|\leq L\|x-y\|.\]

**A 2**.: _The function \(f\) is \(\mu\)-strongly convex on \(\mathbb{R}^{d}\), i.e., it is continuously differentiable and there is a constant \(\mu>0\) such that the following inequality holds for all \(x,y\in\mathbb{R}^{d}\):_

\[(\mu/2)\|x-y\|^{2}\leq f(x)-f(y)-\left\langle\nabla f(y),x-y\right\rangle.\] (2)

Next we specify our assumptions on the sequence of noise variables \(\{Z_{i}\}_{i=0}^{\infty}\). We consider here the general setting of \(\{Z_{i}\}_{i=0}^{\infty}\) being a time-homogeneous Markov chain. Such problems naturally arise in stochastic optimization. In the empirical risk minimization problems it naturally appears in the context of non-random minibatch choice. Indeed, a random choice of a batch number may lose to a non-random one, see [67, 56]. A wide range of problems dealing with Markovian noise is spawned by the reinforcement learning methods. The usual MDP setting falls naturally inside this paradigm, moreover, the analysis of non-tabular RL problems requires to deal with the general state-space Markov noise. Here the potential range of applications include the policy evaluation methods, such as the temporal difference methods [92], and policy optimization algorithms, such as policy gradient family, e.g. the celebrated REINFORCE algorithm [100].

We denote by \(\mathrm{Q}\) the Markov kernel corresponding to the sequence \(\{Z_{i}\}_{i=0}^{\infty}\) and impose the following assumption on the mixing properties of \(\mathrm{Q}\):

**A 3**.: \(\{Z_{i}\}_{i=0}^{\infty}\) _is a stationary Markov chain on \((\mathsf{Z},\mathcal{Z})\) with Markov kernel \(\mathrm{Q}\) and unique invariant distribution \(\pi\). Moreover, \(\mathrm{Q}\) is uniformly geometrically ergodic with mixing time \(\tau\in\mathbb{N}\), i.e., for every \(k\in\mathbb{N}\),_

\[\mathsf{\Delta}(\mathrm{Q}^{k})=\sup_{z_{z},z^{\prime}\in\mathsf{Z}}(1/2)\| \mathrm{Q}^{k}(z,\cdot)-\mathrm{Q}^{k}(z^{\prime},\cdot)\|_{\mathsf{TV}}\leq( 1/4)^{\lfloor k/\tau\rfloor}\,.\] (3)

The assumption A 3 is classical in the literature on optimization methods with Markovian noise and has been considered in particular in recent works [90, 21, 20]. In particular, this assumption covers finite state-space Markov chains with irreducible and aperiodic transition matrix considered in

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \cline{3-10} \multicolumn{1}{c|}{} & \multicolumn{3}{c|}{**Statement**} & \multicolumn{3}{c|}{**Stochasticity**} & \multicolumn{1}{c|}{} \\ \cline{2-10} \multicolumn{1}{c|}{} & **Method** & **VI7** & **Any set** & **Composite?** & **Unbounded?** & **Markovian?** & **Oracle complexity** \\ \hline \multirow{10}{*}{**A**} & **SPLE [33, 42] & ✓ & ✓ & ✗ & ✗ & ✗ & \(\tilde{\mathcal{O}}\left(\frac{1}{\mu}\log\frac{10^{-\alpha}\tau^{2}+\frac{ \alpha^{2}}{\mu^{2}\tau}}{\mu^{2}\tau}\right)^{(1)}\) & \multirow{2}{*}{\(\Delta\)} & \multirow{2}{*}{\(\tilde{\mathcal{O}}\left(\frac{1}{\mu}\log\frac{10^{-\alpha}\tau^{2}+\frac{ \alpha^{2}}{\mu^{2}\tau}}\right)^{(1)}\)} & \multirow{2}{*}{\(\tilde{\mathcal{O}}\left(\frac{1}{\mu}\log\frac{10^{-\alpha}\tau^{2}+ \frac{\alpha^{2}}{\mu^{2}\tau}}{\mu^{2}\tau}\right)^{(1)}\)} & \multirow{2}{*}{\(\tilde{\mathcal{O}}\left(\frac{1}{\mu}\log\frac{10^{-\alpha}\tau^{2}+\frac{ \alpha^{2}}{\mu^{2}\tau}}{\mu^{2}\tau}\right)^{(1)}\)} \\  & **SSQ**[53] & ✓ & ✓ & ✗ & ✗ & ✗ & \(\tilde{\mathcal{O}}\left(\frac{1}{\mu}\log\frac{10^{-\alpha}\tau^{2}+\frac{ \alpha^{2}}{\mu^{2}\tau}}{\mu^{2}\tau}\right)\) & \multirow{2}{*}{\(\tilde{\mathcal{O}}\left(\frac{1}{\mu}\log\frac{10^{-\alpha}\tau^{2}+ \frac{\alpha^{2}}{\mu^{2}\tau}}{\mu^{2}\tau}\right)\)} & \multirow{2}{*}{\(\tilde{\mathcal{O}}\left(\frac{1}{\mu}\log\frac{10^{-\alpha}\tau^{2}+\frac{ \alpha^{2}}{\mu^{2}\tau}}{\mu^{2}\tau}\right)\)} & \multirow{2}{*}{\(\tilde{\mathcal{O}}\left(\frac{1}{\mu}\log\frac{10^{-\alpha}\tau^{2}+\frac{ \alpha^{2}}{\mu^{2}\tau}}{\mu^{2}\tau}\right)\)} & \multirow{2}{*}{\(\tilde{\mathcal{O}}\left(\frac{1}{\mu}\log\frac{10^{-\alpha}\tau^{2}+ \frac{\alpha^{2}}{\mu^{2}\tau}}{\mu^{2}\tau}\right)\)} & \multirow{2}

[28]. Yet our definition of the mixing time \(\tau\) is more classical in the probability literature [81], and is slightly different from the one considered e.g. in [28, 65]. Next we specify our assumptions on stochastic gradient:

**A 4**.: _For all \(x\in\mathbb{R}^{d}\) it holds that \(\mathbb{E}_{\pi}[\nabla F(x,Z)]=\nabla f(x)\). Moreover, for all \(z\in\mathsf{Z}\) and \(x\in\mathbb{R}^{d}\) it holds that_

\[\|\nabla F(x,z)-\nabla f(x)\|^{2}\leq\sigma^{2}+\delta^{2}\|\nabla f(x)\|^{2}\,.\] (4)

The assumption A 4 resembles the strong growth condition [97], which is classical for the over-parametrized learning setup [98, 97]. The main difference is that A 4 concerns the almost sure bound in (4), which is unavoidable when dealing with uniformly geometrically ergodic Markovian noise A 3. Note that it is possible that the quantity \(\delta^{2}\) in (4) is not instance-independent and scales with the ratio \(L/\mu\) from A 1-A 2 in the particular problems. With the assumptions A 3 and A 4 we can prove the result on the mean spurred error of the stochastic gradient estimate computed over batch size \(n\) under arbitrary initial distribution. This result is summarized below in Lemma 1:

**Lemma 1**.: _Assume A 3 and A 4. Then, for any \(n\geq 1\) and \(x\in\mathbb{R}^{d}\), it holds that_

\[\mathbb{E}_{\pi}[\|n^{-1}\sum_{i=1}^{n}\nabla F(x,Z_{i})-\nabla f(x)\|^{2}] \leq\tfrac{8\tau}{n}\left(\sigma^{2}+\delta^{2}\|\nabla f(x)\|^{2}\right).\] (5)

_Moreover, for any initial distribution \(\xi\) on \((\mathcal{Z},\mathcal{Z})\), that_

\[\mathbb{E}_{\xi}[\|n^{-1}\sum_{i=1}^{n}\nabla F(x,Z_{i})-\nabla f(x)\|^{2}] \leq\tfrac{C_{1}\tau}{n}\left(\sigma^{2}+\delta^{2}\|\nabla f(x)\|^{2}\right),\] (6)

_where \(C_{1}=16(1+\tfrac{1}{\ln^{2}4})\)._

Proof.: We first prove (5). Note that due to [81, Proposition \(3.4\)] the Markov kernel \(\mathrm{Q}\) under A 3 admits a positive pseudospectral gap \(\gamma_{ps}>0\) such that \(1/\gamma_{ps}\leq 2\tau\). Thus, applying the statement of [81, Theorem \(3.2\)], we get under A 4 that

\[\mathbb{E}_{\pi}[\|n^{-1}\sum_{i=1}^{n}\nabla F(x,Z_{i})-\nabla f(x)\|^{2}] \leq\tfrac{4\mathbb{E}_{\pi}[\|\nabla F(x,Z_{1})-\nabla f(x)\|^{2}]}{n\gamma _{ps}}\leq\tfrac{8\tau}{n}\left(\sigma^{2}+\delta^{2}\|\nabla f(x)\|^{2} \right).\]

To prove the second part we use the maximal exact coupling construction and follow, e.g., [24, Theorem 1]. The complete proof is given in Appendix B.1. 

The proof of Lemma 1 simplifies the arguments in [21, Lemma 4] and allows us to obtain tighter values for the constants when dealing with the randomized batch size. Note that it is especially important to have the result for MSE under arbitrary initial distribution \(\xi\), since in the proofs of our main results we will inevitably deal with the conditional expectations w.r.t. the previous iterate. We provide more details on the bias and variance of the Markov SGD gradients in the next section.

### Accelerated method

We begin with a version of Nesterov accelerated SGD with randomized batch size, described in Algorithm 1. Due to the unboundedness of the stochastic gradient variance (see A 4), using of the classical Nesterov accelerated method [76, Section 2.2.] does not give the desired result, it is necessary to introduce an additional momentum [74, 97]. We use our own version, but partially similar to [74, 97]. The main feature of Algorithm 1 is that the number of samples used during the \(k\)-th gradient computation scales as \(2^{J_{k}}\), where \(J_{k}\) comes from a truncated geometric distribution. The truncation parameter needs to be adopted (see Theorem 1) in order to control the computational complexity of the algorithm.

Randomized batch size allows for efficient _bias_ reduction in the stochastic gradient estimates and can be seen as a particular case of the so called multilevel MCMC [35, 34]. In the optimization context this approach was successfully used by [21] for the non-convex problems. Indeed, this bias naturally appears under the Markovian stochastic gradients oracles. It is easy to see that, with the counter \(T^{k}\) defined in Line 9, we have

\[\mathbb{E}_{k}[\nabla F(x^{k},Z_{T^{k}+i})]\neq\nabla f(x^{k})\,.\]

Below we show how the bias of the gradient estimate scales with the truncation parameter \(M\). The statement of Lemma 2 yields that the gradient estimates \(g_{k}\) introduced above have the bias, which decreases _quadratically_ with \(M\).

**Lemma 2**.: _Assume A 3 and A 4. Then for the gradient estimates \(g^{k}\) from Algorithm 1 it holds that \(\mathbb{E}_{k}[g^{k}]=\mathbb{E}_{k}[g^{k}_{\lfloor\log_{2}M\rfloor}]\). Moreover,_

\[\mathbb{E}_{k}[\|\nabla f(x^{k})-g^{k}\|^{2}] \lesssim\left(\tau B^{-1}\log_{2}M+\tau^{2}B^{-2}\right)(\sigma^ {2}+\delta^{2}\|\nabla f(x^{k})\|^{2})\,,\] \[\|\nabla f(x^{k})-\mathbb{E}_{k}[g^{k}]\|^{2} \lesssim\tau^{2}M^{-2}B^{-2}(\sigma^{2}+\delta^{2}\|\nabla f(x^{ k})\|^{2})\,.\]The proof and the statement with explicit constants are given in Appendix B.2. Note that the Lemma 2 is a natural counterpart of the deterministic bound Lemma 1. Moreover, it gives the idea of the trade-off between the parameters \(B\) and \(M\). Namely, the expected number of oracle calls to compute \(g_{k}\) is \(\mathcal{O}(B\log_{2}(M))\) with the bias scaling as \(M^{-2}\). Thus the increase of \(M\) drastically reduced the bias with only a logarithmic payment in variance. At the same time, gradient variance scales as \((\tau/B)^{2}\), but the increase of \(B\) is much more expensive for the computational cost of the whole procedure. Taking into account the considerations above, we can prove the following result:

**Theorem 1**.: _Assume A 1 - A 4. Let problem (1) be solved by Algorithm 1. Then for any \(b\in\mathbb{N}^{*}\), \(\gamma\in(0;\frac{3}{4L}]\), and \(\beta,\theta,\eta,p,M,B\) satisfying_

\[p\simeq(1+(1+\gamma L)[\delta^{2}\tau b^{-1}+\delta^{2}\tau^{2}b^{-2}])^{-1}, \quad\beta\simeq\sqrt{p^{2}\mu\gamma},\quad\eta\simeq\sqrt{\frac{1}{\mu \gamma}},\]

\[\theta\simeq\tfrac{p\eta^{-1}-1}{\beta p\eta^{-1}-1},\quad M\simeq\max\{2;\sqrt {p^{-1}(1+p/\beta)}\},\quad B=\left[b\log_{2}M\right],\]

_it holds that_

\[\mathbb{E}\left[\|x^{N}-x^{*}\|^{2}+\tfrac{6}{\mu}(f(x^{N}_{f})-f (x^{*}))\right]\lesssim\exp\bigl{(}-N\sqrt{\tfrac{p^{2}\mu\gamma}{3}}\bigr{)} \bigl{[}\|x^{0}-x^{*}\|^{2}+\tfrac{6}{\mu}(f(x^{0})-f(x^{*}))\bigr{]}\] \[+\tfrac{p\sqrt{\gamma}}{\mu^{2/2}}\left(\sigma^{2}\tau b^{-1}+ \sigma^{2}\tau^{2}b^{-2}\right).\] (7)

The proof is provided in Appendix B.3. The result of Theorem 1 can be rewritten as an upper complexity bound under an appropriate choice of the remaining free parameter \(b\):

**Corollary 1**.: _Under the conditions of Theorem 1, choosing \(b=\tau\) and \(\gamma\) as_

\[\gamma\simeq\min\left\{\tfrac{1}{L};\tfrac{1}{p^{2}\mu N^{2}}\ln\left(\max \left\{2;\tfrac{\mu^{2}N[\|x^{0}-x^{*}\|^{2}+6\mu^{-1}(f(x^{0}_{f})-f(x^{*}))] }{\sigma^{2}}\right\}\right)\right\},\]

_in order to achieve \(\varepsilon\)-approximate solution (in terms of \(\mathbb{E}[\|x-x^{*}\|^{2}]\lesssim\varepsilon\)) it takes_

\[\tilde{\mathcal{O}}\left(\tau\left[(1+\delta^{2})\sqrt{\tfrac{L}{\mu}}\log \tfrac{1}{\varepsilon}+\tfrac{\sigma^{2}}{\mu^{2}\varepsilon}\right]\right) \quad\text{oracle calls}\,.\] (8)

The results of Corollary 1 are obtained with fixed parameters of the method. In Corollary 1 these parameters are selected a bit artificially, e.g., the stepsize \(\gamma\) depends on the iteration horizon \(N\). In Appendix B.4 we show how one can similar results, but with a decreasing stepsize.

**Comparison.** Running the procedure above requires to know the mixing time \(\tau\). Estimating the mixing time from a single trajectory of the running Markov chain is known to be computationally hard problem, see e.g. [101] and references therein. At the same time, methods, which share the same (optimal) linear scaling of the sample complexity w.r.t. the mixing time also share the same drawback as our method. In particular, it holds true for the EMD algorithm [23], SGD-DD algorithm [71], and usual SGD with Markovian data [28]. At the same time, in the non-convex scenario the paper [21] is truly oblivious to mixing time, allowing to obtain sample complexity rates for non-convex problems, which are homogeneous w.r.t. \(\tau\) with AdaGrad-type learning rate. An interesting direction for the future work to suggest a procedure that would allow to generalize the results of [21] to accelerated SGD setting.

It is possible that the sample complexity bound (8) is worse than the respective bounds for non-accelerated SGD with Markov data, provided that \(\delta^{2}\) grows quickly with \(L/\mu\). At the same time, this drawback is shared by the classical results on learning under the strong growth condition, see e.g. [97]. As it is shown in [62], the respective rates can be worse than the ones obtained by usual SGD even under the i.i.d. noise setting, see Appendix F.3 in [62]. Making the analysis of accelerated SGD 'backward compatible' w.r.t. the rates of usual SGD requires to perform analysis in terms of additional problem-specific quantities, see [47, 62].

The closest equivalent of the result Corollary 1 is given by [20, Theorem 3]. However, the corresponding bound of [20, Theorem 3] is incomplete, since the factor \(\tau^{2}\) is lost in the proof (see equations \((64-66)\)). With this completion, the bound of [20, Theorem 3] yields a variance term of order \(\tilde{\mathcal{O}}\left(\frac{\sigma^{2}\tau^{2}}{\mu^{2}\varepsilon}\right)\), which is suboptimal with respect to \(\tau\). Moreover, the corresponding analysis relies heavily on the assumption of a bounded domain. In [28], the author considers Markovian noise with a finite number of states and manages to obtain a rather interesting result of the form \(O\left(\frac{L}{\mu}\log\frac{1}{\varepsilon}+\frac{L\tau\sigma^{2}}{\mu^{3} \varepsilon}\right)\). Here the first term does not depend on \(\tau\), and the second consists only \(\sigma^{*}\) (stochasticity in \(\sigma^{*}\)), but the price for this is an additional factor \(L/\mu\) in the second term and more strict assumption that all realizations \(F(\cdot,z)\) are smooth and strongly convex. In the context of overparameterized learning, our results are almost consistent with the bound of [97, Theorem 1] under i.i.d. sampling. The difference is that the term \(\delta^{2}\) in A 4 can be more pessimistic than the expectation bound in [97].

### Lower bounds

We start with a lower bound for the complexity of Markovian stochastic optimization under the assumptions A 1 -A 4. Below we provide a result that highlights that the bound of Theorem 1 is tight provided that \(\delta\) does not scale with the instance-dependent quantities, e.g., condition number \(L/\mu\).

**Theorem 2**.: _There exists an instance of the optimization problem satisfying assumptions A 1 -A 4 with \(\delta=1\) and arbitrary \(\sigma\geq 0,L,\mu>0,\tau\in\mathbb{N}^{*}\), such that for any first-order gradient method it takes at least_

\[N=\Omega\left(\tau\sqrt{\frac{L}{\mu}}\log\frac{1}{\varepsilon}+\frac{\tau \sigma^{2}}{\mu^{2}\varepsilon}\right)\]

_oracle calls in order to achieve \(\mathbb{E}[\|x^{N}-x^{*}\|^{2}]\leq\varepsilon\)._

The proof is provided in Appendix B.5. The idea of the constructed lower for deterministic part bound \(\Omega\big{(}\tau\sqrt{\frac{L}{\mu}}\log\frac{1}{\varepsilon}\big{)}\) goes back to [76, Theorem 2.1.13]. The stochastic part lower bound goes back to the classical statistical reasoning, and is well explained for i.i.d. noise in [59, Chapter 4.1]. Our adaptation for Markovian setting is based on Le Cam's theory, see [1, Theorem 8], and also [105]. For the case of Markov noise this lower bound is, to the best of our knowledge, original. The closest result to ours is the stochastic term lower bound in [28, Proposition 1], but it is valid only for the vanilla stochastic gradient methods. Below we provide another lower bound showing that the dependence of the sample complexity Corollary 1 on \(\delta\) is not an artefact of the proof.

**Proposition 1**.: _There exists an instance of the optimization problem satisfying assumptions A 1 -A 4 with arbitrary \(L,\mu>0,\tau\in\mathbb{N}^{*}\), \(\delta=\frac{L}{\mu}\), and \(\sigma=0\), such that for any first-order gradient method it takes at least_

\[N=\Omega\left(\tau\frac{L}{\mu}\log\frac{1}{\varepsilon}\right)\]

_gradient calls in order to achieve \(\mathbb{E}[\|x^{N}-x^{*}\|^{2}]\leq\varepsilon\)._

This lower bound is adapted from the information-theoretic lower bound [71]. The detailed proof can be found in Appendix B.5. Recent studies [54, 71, 13] have revealed the impossibility of accelerating stochastic gradient descent (SGD) for online linear regression problems with specific noise structures. To address this issue, researchers have proposed various solutions, such as the MaSS algorithm [62] and the approach presented in [48]. However, these methods rely heavily on the particular structure of the online regression setup. Another question that naturally arises is whether one can get rid of the dependence on \(\tau\) in the deterministic part of (8) if \(\delta=0\). The following counterexample shows that this is not the case in general.

**Proposition 2**.: _There exists an instance of the optimisation problem satisfying assumptions A 1 -A 4 with with arbitrary \(L,\mu>0,\tau\in\mathbb{N}^{*}\), \(\sigma=1,\delta=0\), such that for any first-order gradient method it takes at least_

\[N=\Omega\left(\left(\tau+\sqrt{\frac{L}{\mu}}\right)\log\frac{1}{\varepsilon}\right)\]oracle calls in order to achieve \(\mathbb{E}[\|x^{N}-x^{*}\|^{2}]\leq\varepsilon\)._

The proof is provided in Appendix B.5.

### Non-convex problems

Now we proceed with a randomized batch size version of the simple SGD algorithm. It is summarized in Algorithm 2 and can be shown to achieve optimal rates of convergence for smooth non-convex problems. For the case of non-convex problems with Markov noise similar analysis appeared in [21, Theorem 4].

```
1:Parameters: stepsize \(\gamma>0\), number of iterations \(K\), bound on batchsize \(B\), mixing time \(\tau\);
2:Initialization: choose \(x^{0}\in\mathcal{X}\)
3:for\(k=0,1,2,\ldots,N-1\)do
4: Sample \(J_{k}\sim\text{Geom}\left(\frac{1}{2}\right)\)
5:\(g^{k}=g_{0}^{k}+\begin{cases}2^{J_{k}}\left(g_{J_{k}}^{k}-g_{J_{k}-1}^{k} \right),&\text{if }2^{J_{k}}\leq M\\ 0,&\text{otherwise}\end{cases}\) with \(g_{j}^{k}=2^{-j}B^{-1}\sum_{i=1}^{2^{j}B}\nabla f(x^{k},Z_{T^{k}+i})\)
6:\(x^{k+1}=x^{k}-\gamma g^{k}\)
7:\(T^{k+1}=T^{k}+2^{J_{k}}B\)
8:endfor ```

**Algorithm 2**Randomized GD

By balancing the values of \(B\) and \(M\) with Lemma 2, we establish the following result:

**Theorem 3**.: _Assume A 1, A 3, A 4. Let problem (1) be solved by Algorithm 2. Let \(f^{*}\) be a global (maybe not unique) minimum of \(f\). Then for any \(b\in\mathbb{N}^{*}\), and \(\gamma\), \(M\) satisfying_

\[\gamma\lesssim(L[1+\delta^{2}\tau b^{-1}+\delta^{2}\tau^{2}b^{-2}])^{-1},\quad M \simeq\max\{2;\sqrt{\gamma^{-1}L^{-1}}\},\quad B=\lceil b\log_{2}M\rceil,\]

_it holds that_

\[\mathbb{E}\left[\tfrac{1}{N}\sum_{k=0}^{N-1}\|\nabla f(x^{k})\|^{2}\right] \lesssim\tfrac{f(x^{0})-f^{*}}{\gamma N}+L\gamma\cdot\left[\sigma^{2}\tau b^{ -1}+\sigma^{2}\tau^{2}b^{-2}\right].\]

The proof is provided in Appendix B.6. The next corollary immediately follows from the theorem.

**Corollary 2**.: _Under the conditions of Theorem 3, if we choose \(b=\tau\) and \(\gamma\) given by_

\[\gamma\simeq\min\left\{\tfrac{1}{L(1+\delta^{2})};\;\sqrt{\tfrac{f(x^{0})-f^{ *}}{LN\sigma^{2}}}\right\},\]

_then to achieve \(\varepsilon\)-solution (in terms of \(\mathbb{E}[\|\nabla f(x)\|^{2}]\lesssim\varepsilon^{2}\)) we need_

\[\tilde{\mathcal{O}}\left(\tau\cdot\left[\tfrac{(1+\delta^{2})L(f(x^{0})-f^{*} )}{\varepsilon^{2}}+\tfrac{L(f(x^{0})-f^{*})\sigma^{2}}{\varepsilon^{4}} \right]\right)\quad\text{oracle calls}.\]

**Comparison.** The respective bound for the non-convex setting provided in [20, Theorem 1] yields the sample complexity of order \(\tilde{\mathcal{O}}\left(\tfrac{\tau^{2}L(f(x^{0})-f^{*}))\sigma^{2}}{ \varepsilon^{4}}\right)\). Also we can note the results of [28, Theorem 2] with the following estimate \(O\left(\tfrac{\tau(L(f(x^{0})-f(x^{*}))+\sigma^{2})}{\varepsilon^{2}}+\tfrac{ \tau(L(f(x^{0})-f(x^{*}))+\sigma^{2})\sigma^{2}}{\varepsilon^{4}}\right)\).

To achieve linear convergence rates in the non-convex setting we can use the Polyak-Lojasiewicz (PL) condition [83]. The respective result is provided in Appendix B.7.

### Variational inequalities

In this section, we are interested in the following problem:

\[\text{Find }x^{*}\in\mathcal{X}\text{ such that }\langle F(x^{*}),x-x^{*}\rangle+r(x)-r(x^{*}) \geq 0\text{ for all }x\in\mathcal{X}.\] (9)

Here \(F:\mathbb{R}^{d}\to\mathbb{R}^{d}\) an operator, \(\mathcal{X}\) a convex set, and \(r:\mathbb{R}^{d}\to\mathbb{R}\) is a regularization term (a suitable lower semicontinuous convex function) which is assumed to have a simple structure. As mentioned earlier, this problem is quite general and covers a wide range of possible problem formulations. For example, if the operator \(F\) is the gradient of a convex function \(f\), then the problem (9) is equivalent to the composite minimization problem [6], i.e., minimization of \(f(x)+r(x)\). In the meantime, (9) is also a reformulation of the min-max problem

\[\min_{x_{1}\in\mathcal{X}_{1}}\max_{x_{2}\in\mathcal{X}_{2}}r_{1}(x_{1})+g(x_{ 1},x_{2})-r_{2}(x_{2}),\] (10)

with convex-concave continuously differentiable \(g\), convex sets \(\mathcal{X}_{1}\), \(\mathcal{X}_{2}\) and convex functions \(r_{1}\), \(r_{2}\). Using the first-order optimality conditions, it is easy to verify that (10) is equivalent to (9) with \(x=(x_{1}^{T},x_{2}^{T})^{T}\), \(F(x)=(\nabla_{x_{1}}f(x_{1},x_{2})^{T},-\nabla_{x_{2}}f(x_{1},x_{2})^{T})^{T}\), and \(r(x)=r_{1}(x_{1})+r_{2}(x_{2})\).

**A 5**.: _The operator \(F\) is \(L\)-Lipschitz continuous on \(\mathcal{X}\) with \(L>0\), i.e., the following inequality holds for all \(x,y\in\mathcal{X}\):_

\[\|F(x)-F(y)\|\leq L\|x-y\|,\qquad\forall x,y\in\mathcal{X}.\]

**A 6**.: _The operator \(F\) is \(\mu_{F}\)-strongly monotone on \(\mathcal{X}\), i.e., the following inequality holds for all \(x,y\in\mathcal{X}\):_

\[\langle F(x)-F(y),x-y\rangle\geq\mu_{F}\|x-y\|^{2}.\] (11)

_The function \(r\) is \(\mu_{r}\)-strongly convex on \(\mathcal{X}\), i.e. for all \(x,y\in\mathcal{X}\) and any \(r^{\prime}(x)\in\partial r(x)\) we have_

\[r(y)\geq r(x)+\langle r^{\prime}(x),y-x\rangle+(\mu_{r}/2)\|x-y\|^{2}.\] (12)

These two assumptions are more than standard for the study of variational inequalities and are found in all the papers from Table 2. We consider two cases: strongly monotone/convex with \(\mu_{F}+\mu_{r}>0\) and monotone/convex with \(\mu_{F}+\mu_{r}=0\).

**A 7**.: _For all \(x\in\mathbb{R}^{d}\) it holds that \(\mathbb{E}_{\pi}[F(x,Z)]=F(x)\). Moreover, for all \(z\in\mathbb{Z}\) and \(x\in\mathcal{X}\) it holds that_

\[\|F(x,z)-F(x)\|^{2}\leq\sigma^{2}+\Delta^{2}\|x-x^{*}\|^{2}\,,\] (13)

_where \(x^{*}\) is some point from the solution set._

A 7 is found in the literature on variational inequalities [43, 45, 38] and is considered to be analog to A 4 on overparametrized learning.

Just as the Nesterov accelerated method is optimal for smooth convex minimization problems, the ExtraGradient method [57, 72, 52] is optimal for monotone variational inequalities. Therefore, we take it as a base. On the extrapolation step (Line 4) of Algorithm 3, we simply collect a batch of size \(B\), but on the main step (Line 8) we use the randomization as in Algorithm 1. The next theorem gives the convergence of our method.

**Theorem 4**.: _Assume A 5, A 6 with \(\mu_{F}+\mu_{r}>0\), A 3, A 7. Let problem (9) be solved by Algorithm 3. Then for any \(b\in\mathbb{N}^{*}\), and \(\gamma\), \(M\) satisfying_

\[\gamma\lesssim\min\left\{(\mu_{F}+\mu_{r})^{-1};L^{-1};(\mu_{F}+ \mu_{r})(\Delta^{2}\tau b^{-1}+\Delta^{2}\tau^{2}b^{-2})^{-1};\sqrt{\Delta^{- 2}\tau^{-1}b}\right\},\] \[M\simeq\max\{2;\sqrt{\gamma^{-1}(\mu_{F}+\mu_{r})^{-1}}\},\quad B =\lceil b\log_{2}M\rceil\,,\]

_it holds that_

\[\mathbb{E}\left[\|x^{N}-x^{*}\|^{2}\right]\lesssim\exp\left(-\tfrac{N(\mu_{F }+\mu_{r})\gamma}{2}\right)\|x^{0}-x^{*}\|^{2}+\tfrac{\gamma}{\mu}(\sigma^{2} \tau b^{-1}+\sigma^{2}\tau^{2}b^{-2})\,.\]

The proof is postponed to Appendix B.8. One can get an estimate on oracle complexity.

```
1:Parameters: stepsize \(\gamma>0\), number of iterations \(N\)
2:Initialization: choose \(x^{0}\in\mathcal{X}\)
3:for\(k=0,1,2,\ldots,N-1\)do
4:\(x^{k+1/2}=\text{prox}_{\gamma r}\big{(}x^{k}-\gamma B^{-1}\sum_{i=1}^{B}F(x^ {k},Z_{T^{k}+i})\big{)}\)
5:\(T^{k+1/2}=T^{k}+B\)
6: Sample \(J_{k}\sim\text{Geom}\left(\frac{1}{2}\right)\)
7:\(g^{k}=g_{0}^{k}+\begin{cases}2^{J_{k}}\left(g_{J_{k}}^{k}-g_{J_{k-1}}^{k} \right),&\text{if }2^{J_{k}}\leq M\\ 0,&\text{otherwise}\end{cases}\) with \(g_{j}^{k}=2^{-j}B^{-1}\sum_{i=1}^{2^{j}\cdot B}F(x^{k+1/2},Z_{T^{k+1/2}+i})\)
8:\(x^{k+1}=\text{prox}_{\gamma r}\big{(}x^{k}-\gamma g^{k}\big{)}\)
9:\(T^{k+1}=T^{k+1/2}+2^{J_{k}}B\)
10:endfor ```

**Algorithm 3**Randomized ExtraGradient

**Corollary 3**.: _Under the conditions of Theorem 4, if we choose \(b=\tau\) and \(\gamma\) as follows_

\[\gamma\simeq\min\left\{\tfrac{1}{\mu_{F}+\mu_{r}};\frac{1}{L};\tfrac{\mu_{F}+ \mu_{r}}{\Delta^{2}};\frac{1}{\Delta};\frac{1}{N(\mu_{F}+\mu_{r})}\ln\left( \max\left\{2;\tfrac{\mu N\|x^{0}-x^{*}\|^{2}}{\sigma^{2}}\right\}\right)\right\},\]

_then to achieve \(\varepsilon\)-solution (in terms of \(\mathbb{E}[\|x-x^{*}\|^{2}]\lesssim\varepsilon\)) we need_

\[\tilde{\mathcal{O}}\left(\tau\cdot\left[\left(1+\tfrac{L}{\mu_{F}+\mu_{r}}+ \tfrac{\Delta}{\mu_{F}+\mu_{r}}+\tfrac{\Delta^{2}}{(\mu_{F}+\mu_{r})^{2}} \right)\log\tfrac{1}{\varepsilon}+\tfrac{\sigma^{2}}{\mu^{2}\varepsilon}\right] \right)\quad\text{oracle calls}.\]Note that one provide an (almost) matching lower complexity bounds for variational inequalities via lower bounds for saddle point problems, which are a special case of variational inequalities. The method for obtaining lower bounds for saddle point problems is reduced to obtaining estimates for the strongly convex minimization problem (see [106, 40] for respective deterministic lower bounds), which we provide in Section 2.2. Similarly, the question of constructing a lower bound which is tight w.r.t. \(\Delta\) remains open.

For the monotone case, we use the _gap function_ as a convergence criterion:

\[\text{Gap}(x)=\sup_{y\in\mathcal{X}}\left[\langle F(y),x-y\rangle+r(x)-r(y) \right]\,.\] (14)

Such a criterion is standard and classical for monotone variational inequalities [72, 52]. An important assumption for the gap function is the boundedness of the set \(\mathcal{X}\).

**A 8**.: _The set \(\mathcal{X}\) is bounded and has a diameter \(D\), i.e., for all \(x,y\in\mathcal{X}\): \(\|x-y\|^{2}\leq D^{2}\)._

A 8 can be slightly relaxed. We need to use a simple trick from [77]. In particular, we need to consider \(\mathcal{C}\) - a compact subset of \(\mathcal{X}\) and change \(\mathcal{X}\) to \(\mathcal{C}\) in (14). But such a technique is rather technical and does not change the essence. Finally, the following result holds.

**Theorem 5**.: _Assume A 5, A 6 with \(\mu_{F}+\mu_{r}=0\), A 8, A 3, A 7. Let problem (9) be solved by Algorithm 3. Then for any \(B\in\mathbb{N}^{*}\), and \(\gamma\), \(M\) satisfying \(\gamma\lesssim L^{-1}\,,\;M=\sqrt{N}\), it holds that_

\[\mathbb{E}\left[\text{Gap}(\bar{x}^{N})\right]\lesssim\tfrac{D^{2}}{\gamma N }+\gamma(\tau B^{-1}\log_{2}N+\tau^{2}B^{-2})(\sigma^{2}+\Delta^{2}D^{2})\;\; \text{where}\;\;\bar{x}^{N}=\tfrac{1}{N}\sum_{k=0}^{N-1}x^{k+1/2}\,.\]

The proof is postponed to Appendix B.9. The following corollary holds.

**Corollary 4**.: _Under the conditions of Theorem 5, if we choose \(B=\tau\) and \(\gamma\) as follows_

\[\gamma\simeq\min\left\{\tfrac{1}{L};\sqrt{\tfrac{D^{2}}{(\sigma^{2}+\Delta^{2 }D^{2})N}}\right\},\]

_then to achieve \(\varepsilon\)-solution (in terms of \(\mathbb{E}[\text{Gap}(x)]\lesssim\varepsilon\)) we need_

\[\tilde{\mathcal{O}}\left(\tau\left[\tfrac{LD^{2}}{\varepsilon}+\tfrac{\sigma^ {2}D^{2}+\Delta^{2}D^{4}}{\varepsilon^{2}}\right]\right)\quad\text{oracle calls}.\]

**Comparison**. These results is the first for variational inequalities with Markovian stochasticity, either in the strongly monotone or monotone cases. The only close work is [99]. The authors work with convex-concave saddle point problems and provide the following estimate on the oracle complexity \(\mathcal{O}\left(\tau^{2}\cdot\tfrac{G^{4}}{\varepsilon^{2}}+\tfrac{D^{2}}{ \varepsilon^{2}}\right)\) (with \(G\) - the uniform bound of the operator), which is worse than ours at least in terms of \(\tau\). Moreover, the authors consider the case of a finite Markov chain, which is a special case of our setup.

## 3 Conclusion

In this paper, we present a unified random batch size framework that achieves optimal finite-time performance for non-convex and strongly convex optimization problems with Markov noise, as well as for variational inequalities. Unlike existing methods, our framework relaxes the assumptions typically imposed on the domain and stochastic gradient oracle. We also provide a variety of lower bounds, which are to the best of our knowledge original in the Markov setting.

## Acknowledgments

This research of A. Beznosikov has been supported by The Analytical Center for the Government of the Russian Federation (Agreement No. 70-2021-00143 dd. 01.11.2021, IGK 000000D730321P5Q0002). E. Moulines received support from the grant ANR-19-CHIA-002 SCAI and parts of his work has been done under the auspices of Lagrange Center for maths and computing.

## References

* 204, 2019.
* [2] Ahmet Alacaoglu and Yura Malitsky. Stochastic variance reduction for variational inequality methods. In _Conference on Learning Theory_, pages 778-816. PMLR, 2022.

* [3] Necdet Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar. A universally optimal multistage accelerated stochastic gradient method. _Advances in neural information processing systems_, 32, 2019.
* [4] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Optimization with sparsity-inducing penalties. _arXiv preprint arXiv:1108.0775_, 2011.
* [5] Francis Bach, Julien Mairal, and Jean Ponce. Convex sparse matrix factorizations. _arXiv preprint arXiv:0812.1869_, 2008.
* [6] Amir Beck. _First-order methods in optimization_. Society for Industrial and Applied Mathematics (SIAM), 2017.
* [7] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. _Robust Optimization_. Princeton University Press, 2009.
* [8] Aleksandr Beznosikov, Eduard Gorbunov, Hugo Berard, and Nicolas Loizou. Stochastic gradient descent-ascent: Unified theory and new efficient methods. In _International Conference on Artificial Intelligence and Statistics_, pages 172-235. PMLR, 2023.
* [9] Aleksandr Beznosikov, Valentin Samokhin, and Alexander Gasnikov. Distributed saddle-point problems: Lower bounds, optimal and robust algorithms. _arXiv preprint arXiv:2010.13112_, 2020.
* [10] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation. In _Conference on learning theory_, pages 1691-1692. PMLR, 2018.
* [11] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. _Journal of mathematical imaging and vision_, 40(1):120-145, 2011.
* [12] Tatjana Chavdarova, Gauthier Gidel, Francois Fleuret, and Simon Lacoste-Julien. Reducing noise in gan training with variance reduced extragradient. _Advances in Neural Information Processing Systems_, 32, 2019.
* [13] You-Lin Chen, Sen Na, and Mladen Kolar. Convergence analysis of accelerated stochastic gradient descent under the growth condition. _arXiv preprint arXiv:2006.06782_, 2020.
* [14] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch algorithms via accelerated gradient methods. _Advances in neural information processing systems_, 24, 2011.
* [15] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with optimism. _arXiv preprint arXiv:1711.00141_, 2017.
* [16] Olivier Devolder et al. Stochastic first order methods in smooth convex optimization. Technical report, CORE, 2011.
* [17] Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach. Harder, better, faster, stronger convergence rates for least-squares regression. _The Journal of Machine Learning Research_, 18(1):3520-3570, 2017.
* [18] Alexandros G. Dimakis, Soummya Kar, Jose M. F. Moura, Michael G. Rabbat, and Anna Scaglione. Gossip algorithms for distributed signal processing. _Proceedings of the IEEE_, 98(11):1847-1864, 2010.
* [19] Thinh T. Doan. Finite-time analysis of markov gradient descent. _IEEE Transactions on Automatic Control_, 68(4):2140-2153, 2023.
* [20] Thinh T Doan, Lam M Nguyen, Nhan H Pham, and Justin Romberg. Convergence rates of accelerated markov gradient descent with applications in reinforcement learning. _arXiv preprint arXiv:2002.02873_, 2020.

* [21] Ron Dorfman and Kfir Yehuda Levy. Adapting to mixing time in stochastic optimization with markovian data. In _International Conference on Machine Learning_, pages 5429-5446. PMLR, 2022.
* [22] R. Douc, E. Moulines, P. Priouret, and P. Soulier. _Markov chains_. Springer Series in Operations Research and Financial Engineering. Springer, 2018.
* [23] John C Duchi, Alekh Agarwal, Mikael Johansson, and Michael I Jordan. Ergodic mirror descent. _SIAM Journal on Optimization_, 22(4):1549-1578, 2012.
* [24] Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, and Marina Sheshukova. Rosenthal-type inequalities for linear statistics of markov chains. _arXiv preprint arXiv:2303.05838_, 2023.
* [25] Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, and Hoi-To Wai. On the stability of random matrix product with markovian noise: Application to linear stochastic approximation and td learning. In _Conference on Learning Theory_, pages 1711-1752. PMLR, 2021.
* [26] Pavel Dvurechensky and Alexander Gasnikov. Stochastic intermediate gradient method for convex problems with stochastic inexact oracle. _Journal of Optimization Theory and Applications_, 171:121-145, 2016.
* [27] Ernie Esser, Xiaoqun Zhang, and Tony F Chan. A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science. _SIAM Journal on Imaging Sciences_, 3(4):1015-1046, 2010.
* [28] Mathieu Even. Stochastic gradient descent under Markovian sampling schemes. _arXiv preprint arXiv:2302.14428_, 2023.
* [29] F. Facchinei and J.S. Pang. _Finite-Dimensional Variational Inequalities and Complementarity Problems_. Springer Series in Operations Research and Financial Engineering. Springer New York, 2007.
* [30] Alexander Vladimirovich Gasnikov and Yu E Nesterov. Universal method for stochastic composite optimization problems. _Computational Mathematics and Mathematical Physics_, 58:48-64, 2018.
* [31] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368, 2013.
* [32] Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. _Mathematical Programming_, 155(1-2):267-305, 2016.
* [33] Gauthier Gidel, Hugo Berard, Gaetan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A variational inequality perspective on generative adversarial networks. _arXiv preprint arXiv:1802.10551_, 2018.
* [34] Michael B. Giles. Multilevel monte carlo path simulation. _Operations Research_, 56(3):607-617, 2008.
* [35] Peter W Glynn and Chang-han Rhee. Exact estimation for markov chain equilibrium expectations. _Journal of Applied Probability_, 51(A):377-389, 2014.
* [36] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In _Advances in Neural Information Processing Systems (NIPS)_, 2014.
* [37] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. _Deep Learning_. MIT Press, Cambridge, MA, USA, 2016. http://www.deeplearningbook.org.
* [38] Eduard Gorbunov, Hugo Berard, Gauthier Gidel, and Nicolas Loizou. Stochastic extragradient: General analysis and improved rates. In _International Conference on Artificial Intelligence and Statistics_, pages 7865-7901. PMLR, 2022.

* [39] Eduard Gorbunov, Marina Danilova, Innokentiy Shibaev, Pavel Dvurechensky, and Alexander Gasnikov. Near-optimal high probability complexity bounds for non-smooth stochastic optimization with heavy-tailed noise. _arXiv preprint arXiv:2106.05958_, 2021.
* [40] Yuze Han, Guangzeng Xie, and Zhihua Zhang. Lower complexity bounds of finite-sum optimization problems: The results and construction. _arXiv preprint arXiv:2103.08280_, 2021.
* [41] P. T. Harker and J.-S. Pang. Finite-dimensional variational inequality and nonlinear complementarity problems: a survey of theory, algorithms and applications. _Mathematical programming_, 1990.
* [42] Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. _Advances in Neural Information Processing Systems_, 32, 2019.
* [43] Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. _Advances in Neural Information Processing Systems_, 33:16223-16234, 2020.
* [44] Chonghai Hu, Weike Pan, and James Kwok. Accelerated gradient methods for stochastic optimization and online learning. _Advances in Neural Information Processing Systems_, 22, 2009.
* [45] A. N. Iusem, A. Jofre, R. I. Oliveira, and P. Thompson. Extragradient method with variance reduction for stochastic variational inequalities. _SIAM Journal on Optimization_, 27(2):686-724, 2017.
* [46] Alfredo N Iusem, Alejandro Jofre, and Philip Thompson. Incremental constraint projection methods for monotone stochastic variational inequalities. _Mathematics of Operations Research_, 44(1):236-263, 2019.
* [47] Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating stochastic gradient descent for least squares regression. In _Conference On Learning Theory_, pages 545-604. PMLR, 2018.
* [48] Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing stochastic gradient descent for least squares regression: Mini-batching, averaging, and model misspecification. _Journal of Machine Learning Research_, 18(223):1-42, 2018.
* [49] Houyuan Jiang and Huifu Xu. Stochastic approximation approaches to the stochastic variational inequality problem. _IEEE Transactions on Automatic Control_, 53(6):1462-1475, 2008.
* [50] Yujia Jin and Aaron Sidford. Efficiently solving MDPs with stochastic mirror descent. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, volume 119, pages 4890-4900. PMLR, 2020.
* [51] Thorsten Joachims. A support vector method for multivariate performance measures. pages 377-384, 01 2005.
* [52] Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. _Stochastic Systems_, 1(1):17-58, 2011.
* [53] Aswin Kannan and Uday V Shanbhag. Optimal stochastic extragradient schemes for pseudomonotone stochastic variational inequality problems and their variants. _Computational Optimization and Applications_, 74(3):779-820, 2019.
* [54] Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham Kakade. On the insufficiency of existing momentum schemes for stochastic optimization. In _2018 Information Theory and Applications Workshop (ITA)_, pages 1-9. IEEE, 2018.
* [55] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.

* [56] Anastasia Koloskova, Nikita Doikov, Sebastian U Stich, and Martin Jaggi. Shuffle SGD is always better than SGD: Improved analysis of SGD with arbitrary data orders. _arXiv preprint arXiv:2305.19259_, 2023.
* [57] G. M. Korpelevich. The extragradient method for finding saddle points and other problems. _Matecon_, 12:35-49, 1977.
* [58] Guanghui Lan. An optimal method for stochastic composite optimization. _Mathematical Programming_, 133(1-2):365-397, 2012.
* [59] Guanghui Lan. _First-order and Stochastic Optimization Methods for Machine Learning_. 01 2020.
* [60] Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks. In Kamalika Chaudhuri and Masashi Sugiyama, editors, _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, volume 89 of _Proceedings of Machine Learning Research_, pages 907-915. PMLR, 16-18 Apr 2019.
* [61] Qihang Lin, Xi Chen, and Javier Pena. A smoothing stochastic gradient method for composite optimization. _Optimization Methods and Software_, 29(6):1281-1301, 2014.
* [62] Chaoyue Liu and Mikhail Belkin. Accelerating sgd with momentum for over-parameterized learning. _arXiv preprint arXiv:1810.13395_, 2018.
* [63] Cassio G. Lopes and Ali H. Sayed. Incremental adaptive strategies over distributed networks. _IEEE Transactions on Signal Processing_, 55(8):4064-4077, 2007.
* [64] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations (ICLR)_, 2018.
* [65] Xianghui Mao, Kun Yuan, Yubin Hu, Yuantao Gu, Ali H. Sayed, and Wotao Yin. Walkman: A communication-efficient random-walk algorithm for decentralized optimization. _IEEE Transactions on Signal Processing_, 68:2513-2528, 2020.
* [66] Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. _arXiv preprint arXiv:1807.02629_, 2018.
* [67] Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshuffling: Simple analysis with vast improvements. _Advances in Neural Information Processing Systems_, 33:17309-17320, 2020.
* [68] Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richtarik, and Yura Malitsky. Revisiting stochastic extragradient. In _International Conference on Artificial Intelligence and Statistics_, pages 4573-4582. PMLR, 2020.
* [69] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533, 2015.
* [70] Eric Moulines and Francis Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 24. Curran Associates, Inc., 2011.
* [71] Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least squares regression with markovian data: Fundamental limits and algorithms. _Advances in neural information processing systems_, 33:16666-16676, 2020.

* [72] Arkadi Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 15(1):229-251, 2004.
* [73] Yu Nesterov. Smooth minimization of non-smooth functions. _Mathematical programming_, 103(1):127-152, 2005.
* [74] Yu. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. _SIAM Journal on Optimization_, 22(2):341-362, 2012.
* [75] Yu. E. Nesterov. A method for solving the convex programming problem with convergence rate \(O(1/k^{2})\). _Dokl. Akad. Nauk SSSR_, 269(3):543-547, 1983.
* [76] Yurii Nesterov. _Introductory lectures on convex optimization: A basic course_, volume 87. Springer Science & Business Media, 2003.
* [77] Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and related problems. _Mathematical Programming_, 109(2):319-344, 2007.
* [78] J. Von Neumann and O. Morgenstern. _Theory of games and economic behavior_. Princeton University Press, 1944.
* [79] Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan P. How, and John Vian. Deep decentralized multi-task multi-agent reinforcement learning under partial observability. In _Proceedings of the 34th International Conference on Machine Learning (ICML)_, volume 70, pages 2681-2690. PMLR, 2017.
* [80] Balamurugan Palaniappan and Francis Bach. Stochastic variance reduction methods for saddle-point problems. _Advances in Neural Information Processing Systems_, 29, 2016.
* 32, 2015.
* [82] Wei Peng, Yu-Hong Dai, Hui Zhang, and Lizhi Cheng. Training gans with centripetal acceleration. _Optimization Methods and Software_, 35(5):955-973, 2020.
* [83] B.T. Polyak. Gradient methods for the minimisation of functionals. _USSR Computational Mathematics and Mathematical Physics_, 3(4):864-878, 1963.
* 407, 1951.
* [85] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* 49, 06 2010.
* [87] Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd learning. In _Conference on Learning Theory_, pages 2803-2830. PMLR, 2019.
* [88] Sebastian U Stich. Unified optimal analysis of the (stochastic) gradient method. _arXiv preprint arXiv:1907.04232_, 2019.
* [89] Tao Sun, Dongsheng Li, and Bao Wang. Adaptive Random Walk Gradient Descent for Decentralized Optimization. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 20790-20809. PMLR, 17-23 Jul 2022.
* [90] Tao Sun, Yuejiao Sun, and Wotao Yin. On Markov chain gradient descent. _Advances in neural information processing systems_, 31, 2018.

* Suttskever et al. [2013] Ilya Suttskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In Sanjoy Dasgupta and David McAllester, editors, _Proceedings of the 30th International Conference on Machine Learning_, volume 28 of _Proceedings of Machine Learning Research_, pages 1139-1147, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR.
* Sutton [1988] Richard S Sutton. Learning to predict by the methods of temporal differences. _Machine learning_, 3:9-44, 1988.
* Sutton and Barto [2018] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. The MIT Press, second edition, 2018.
* Taylor and Bach [2019] Adrien Taylor and Francis Bach. Stochastic first-order methods: non-asymptotic and computer-aided analyses via potential functions. In _Conference on Learning Theory_, pages 2934-2992. PMLR, 2019.
* Tseng [2000] Paul Tseng. A modified forward-backward splitting method for maximal monotone mappings. _SIAM Journal on Control and Optimization_, 38(2):431-446, 2000.
* Van der Vaart [2000] Aad W Van der Vaart. _Asymptotic statistics_, volume 3. Cambridge university press, 2000.
* Vaswani et al. [2019] Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of SGD for over-parameterized models and an accelerated perceptron. In _The 22nd international conference on artificial intelligence and statistics_, pages 1195-1204. PMLR, 2019.
* Vaswani et al. [2019] Sharan Vaswani, Aaron Mishkin, Issam H. Laradji, Mark Schmidt, Gauthier Gidel, and Simon Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. _CoRR_, abs/1905.09997, 2019.
* Wang et al. [2022] Puyu Wang, Yunwen Lei, Yiming Ying, and Ding-Xuan Zhou. Stability and generalization for markov chain stochastic gradient methods. _arXiv preprint arXiv:2209.08005_, 2022.
* Williams [1992] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine learning_, 8:229-256, 1992.
* Wolfer and Kontorovich [2019] Geoffrey Wolfer and Aryeh Kontorovich. Estimating the mixing time of ergodic markov chains. In _Conference on Learning Theory_, pages 3120-3159. PMLR, 2019.
* Woodworth and Srebro [2021] Blake E Woodworth and Nathan Srebro. An even more optimal stochastic optimization algorithm: minibatching and interpolation learning. _Advances in Neural Information Processing Systems_, 34:7333-7345, 2021.
* Xu et al. [2005] Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans. Maximum margin clustering. In L. Saul, Y. Weiss, and L. Bottou, editors, _Advances in Neural Information Processing Systems_, volume 17. MIT Press, 2005.
* Yang et al. [2020] Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 1153-1165. Curran Associates, Inc., 2020.
* Yu [1997] Bin Yu. _Assouad, Fano, and Le Cam_, pages 423-435. Springer New York, New York, NY, 1997.
* Zhang et al. [2019] Junyu Zhang, Mingyi Hong, and Shuzhong Zhang. On lower iteration complexity bounds for the saddle point problems. _arXiv preprint arXiv:1912.07481_, 2019.