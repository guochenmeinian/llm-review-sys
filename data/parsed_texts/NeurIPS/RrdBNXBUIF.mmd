Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering

 Yijun Dong

Courant Institute of Mathematical Sciences

New York University

New York, NY

yd1319@nyu.edu

&Kevin Miller

Oden Institute for Computational

Engineering & Science

University of Texas at Austin

Austin, TX

ksmiller@utexas.edu

&Qi Lei

Courant Institute of Mathematical Sciences

& Center of Data Science

New York University

New York, NY

q1518@nyu.edu

&Rachel Ward

Oden Institute for Computational

Engineering & Science

University of Texas at Austin

Austin, TX

rward@math.utexas.edu

Equal contribution.

###### Abstract

Despite the empirical success and practical significance of (relational) knowledge distillation that matches (the relations of) features between teacher and student models, the corresponding theoretical interpretations remain limited for various knowledge distillation paradigms. In this work, we take an initial step toward a theoretical understanding of relational knowledge distillation (RKD), with a focus on semi-supervised classification problems. We start by casting RKD as spectral clustering on a population-induced graph unveiled by a teacher model. Via a notion of clustering error that quantifies the discrepancy between the predicted and ground truth clusterings, we illustrate that RKD over the population provably leads to low clustering error. Moreover, we provide a sample complexity bound for RKD with limited unlabeled samples. For semi-supervised learning, we further demonstrate the label efficiency of RKD through a general framework of cluster-aware semi-supervised learning that assumes low clustering errors. Finally, by unifying data augmentation consistency regularization into this cluster-aware framework, we show that despite the common effect of learning accurate clusterings, RKD facilitates a "global" perspective through spectral clustering, whereas consistency regularization focuses on a "local" perspective via expansion.

## 1 Introduction

The immense volume of training data is a vital driving force behind the unparalleled power of modern deep neural networks. Nevertheless, data collection can be quite resource-intensive; in particular, data labeling may be prohibitively costly. With an aim to lessen the workload associated with data labeling, semi-supervised learning capitalizes on more easily accessible unlabeled data via diverse approaches of pseudo-labeling. For instance, data augmentation consistency regularization (Sohn et al., 2020; Berthelot et al., 2019) generates pseudo-labels from the current model predictions on carefully designed data augmentations. Alternatively, knowledge distillation (Hinton et al., 2015) can be leveraged to gauge representations of unlabeled samples via pretrained teacher models.

Meanwhile, as the scale of training state-of-the-art models explodes, computational costs and limited data are becoming significant obstacles to learning from scratch. In this context, (data-free) knowledge distillation (Liu et al., 2021; Wang and Yoon, 2021) (where pretrained models are taken as teacher oracles without access to their associated training data) is taking an increasingly crucial role in learning efficiently from powerful existing models.

Despite the substantial progresses in knowledge distillation (KD) in practice, its theoretical understanding remains limited, possibly due to the abundant diversity of KD strategies. Most existing theoretical analyses of KD (Ji and Zhu, 2020; Allen-Zhu and Li, 2020; Harutyunyan et al., 2023; Hsu et al., 2021; Wang et al., 2022) focus on feature matching between the teacher and student models (Hinton et al., 2015), while a much broader spectrum of KD algorithms (Liu et al., 2019; Park et al., 2019; Chen et al., 2021; Qian et al., 2022) demonstrates appealing performance in practice. Recently, _relational knowledge distillation_ (RKD) (Park et al., 2019; Liu et al., 2019) has achieved remarkable empirical successes by matching the inter-feature relationships (instead of features themselves) between the teacher and student models. This drives us to focus on RKD and motivates our analysis with the following question:

_What perspective of the data does relational knowledge distillation learn, and how efficiently?_

(RKD learns spectral clustering.)In light of the connection between RKD and instance relationship graphs (IRG) (Liu et al., 2019)2, intuitively, RKD learns the ground truth geometry through a graph induced by the teacher model. We formalize this intuition from a spectral clustering perspective (Section 4). Specifically, for a multi-class classification problem, we introduce a notion of _clustering error_ (Definition 3.1) that measures the difference between the predicted and ground truth class partitions. Through low clustering error guarantees, we illustrate that RKD over the population learns the ground truth partition (presumably unveiled by the teacher model) via _spectral clustering_ (Section 4.1). Moreover, RKD on sufficiently many unlabeled samples learns a similar partition with low clustering error (Section 4.2). By appealing to standard generalization analysis (Wainwright, 2019; Bartlett and Mendelson, 2003; Wei and Ma, 2019) for unbiased estimates of the parameterized RKD loss, we show that the _unlabeled sample complexity of RKD_ is dominated by a polynomial term in the model complexity (Theorem 4.2, Theorem 4.3).

Footnote 2: An IRG is a graph with samples as vertices and sample relationships, characterized by their mutual distances in a feature space, as edges. RKD encourages the student model to learn such IRGs from the teacher model.

As one of the most influential and classical unsupervised learning strategies, clustering remains an essential aspect of modern learning algorithms with weak/no supervision. For example, contrastive learning (Caron et al., 2020; Grill et al., 2020; Chen and He, 2021) has proven to learn good representations without any labels by encouraging (spectral) clustering (HaoChen et al., 2021; Lee et al., 2021; Parulekar et al., 2023). However, the effects of clustering in the related schemes of semi-supervised learning (SSL) are less well investigated. Therefore, we pose a follow-up question:

_How does cluster-aware semi-supervised learning improve label efficiency and generalization?_

Figure 1: The complementary perspectives of RKD and DAC regarding the data. RKD learns the pairwise relations among data (_e.g._, edge \(w_{\mathbf{x}\mathbf{x}^{\prime}}\) that characterizes the similarity between \(\mathbf{x}\) and \(\mathbf{x}^{\prime}\)) over the population “globally” via spectral clustering, as illustrated in Section 4. Alternatively, DAC discovers the “local” clustering structure through the expansion of neighborhoods based on overlapping augmentation sets (_e.g._, \(\mathcal{A}(\mathbf{x})\cap\mathcal{A}\left(\mathbf{x}^{\prime\prime}\right) \neq\emptyset\)), as elaborated in Section 6.

(Cluster-aware SSL is label-efficient.)We formulate a cluster-aware SSL framework (Section 5) where assuming a low clustering error (Definition 3.1) endowed by learning from unlabeled data, the label complexity scales linearly in the number of clusters, up to a logarithmic factor (Theorem 5.1).

Since clustering is a rather generic notion that can be facilitated from various perspectives, to better understand spectral clustering as learned by RKD, we compare the clustering mechanism of RKD with that of another common example of cluster-aware SSL--data augmentation consistency (DAC) regularization, while exploring the following question:

_What are the similarities and discrepancies between different cluster-aware strategies?_

(RKD and DAC learn clustering from complementary perspectives.)We unify the existing analysis (Yang et al., 2023) tailored for DAC regularization (Sohn et al., 2020) into the cluster-aware SSL framework (Section 6). In particular, while facilitating clustering similarly to RKD, DAC reduces the clustering error through an expansion-based mechanism characterized by "local" neighborhoods induced by data augmentations (Theorem 6.2). This serves as a complementary perspective to RKD which approximates spectral clustering on a graph Laplacian that reflects the underlying "global" structure of the population (Figure 1).

## 2 Related Works

Relational knowledge distillation.Since the seminal work of Hinton et al. (2015), knowledge distillation has become a foundational method for time and memory-efficient deep learning. Recent years have seen different KD strategies. Some works learn to directly match the output (response) (Hinton et al., 2015; Chen et al., 2017; Ba and Caruana, 2014) or intermediate layers (features) (Romero et al., 2014; Zagoruyko and Komodakis, 2016; Kim et al., 2018) of the teacher network. More recently, Park et al. (2019) introduced the idea of relational knowledge distillation (RKD), and concurrent work (Liu et al., 2019) introduced the "instance relationship graph" (IRG). They essentially presented similar ideas to learn the inter-feature relationship between samples instead of matching models' responses or features individually. A comprehensive survey of relational (relation-based) KD can be found in (Gou et al., 2021).

Despite the empirical success stories, the theoretical understanding of KD remained limited. Prior theoretical work is mostly constrained to linear classifier/nets (Phuong and Lampert, 2019; Ji and Zhu, 2020) or neural tangent kernel (NTK) analysis (Harutyunyan et al., 2023). Hsu et al. (2021) leverages knowledge distillation to improve the sample complexity analysis of large neural networks with potentially vacuous generalization bounds.

Data augmentation consistency regularization.Data augmentation serves as an implicit or explicit regularization to improve sample efficiency and avoid overfitting. Initially, people applied simple transformations to labeled samples and add them to the training set (Krizhevsky et al., 2017; Simard et al., 2002; Simonyan and Zisserman, 2014; He et al., 2016; Cubuk et al., 2018). Traditional transformations preserve image semantics, including (random) perturbations, distortions, scales, crops, rotations, and horizontal flips. Subsequent augmentations may alter the labels jointly with the input samples, e.g. Mixup (Zhang et al., 2017), Cutout (DeVries and Taylor, 2017), and Cutmix (Yun et al., 2019). Theoretical works have demonstrated different functionalities of data augmentation such as incorporating invariances (Mei et al., 2021), amplifying strong features (Shen et al., 2022), or reducing variance during the learning procedure (Chen et al., 2020).

Recent practices explicitly add consistency regularization to enforce prediction or representation similarities (Laine and Aila, 2016; Bachman et al., 2014; Sohn et al., 2020). Not only does this way incorporates unlabeled samples, but Yang et al. (2023) also theoretically demonstrated other benefits: explicit consistency regularization reduces sample complexity more significantly and handles misspecification better than simply adding augmented data into the training set.

Graph-based learning.Although we leverage graphs to model the underlying clustering structure of the population, the RKD algorithm that we study applies to generic data distributions and does not involve any graph constructions over the training data. The latter is crucial for modern learning settings as graph construction can be prohibitively expensive on common large-scale datasets.

Nevertheless, assuming that explicit graph construction is affordable/available, there exists a rich history of nonparametric graph-based semi-supervised classification models for transductive learn ing (Zhu et al., 2003; Bertozzi and Merkurjev, 2019; Zhou et al., 2003; Belkin et al., 2004; Belkin and Niyogi, 2004). These methods have seen success in achieving high accuracy results in the low-label rate regime (Calder et al., 2020; Bertozzi and Merkurjev, 2019). Furthermore, assuming inherited graph structures of data, recent progress in graph neural networks (GNN) (Zhou et al., 2020) and graph convolutional networks (GCN) (Welling and Kipf, 2016) has provided a connection between these classical graph-based methods and powerful deep learning models for a variety of problem settings (Zhou et al., 2020).

## 3 Problem Setup

Notations.For any event \(e\), let \(\mathbbm{1}\left\{e\right\}=1\) if \(e\) is true, and \(\mathbbm{1}\left\{e\right\}=0\) otherwise. For any positive integers \(n,K\in\mathbb{N}\), we denote \([n]=\left\{1,\ldots,n\right\}\) and \(\Delta_{K}\triangleq\left\{\left(p_{1},\ldots,p_{K}\right)\in[0,1]^{K}\, \middle|\,\sum_{k=1}^{K}p_{k}=1\right\}\). For a finite set \(\mathcal{X}\), \(\left|\mathcal{X}\right|\) denotes the size of \(\mathcal{X}\), and \(\mathrm{Unif}\left(\mathcal{X}\right)\) denotes the uniform distribution over \(\mathcal{X}\) (_i.e._, \(\mathrm{Unif}(\mathbf{x})=1/\left|\mathcal{X}\right|\) for all \(\mathbf{x}\in\mathcal{X}\)). For a distribution \(P\) over \(\mathcal{X}\) and any \(n\in\mathbb{N}\), \(P^{n}\) represents the joint distribution over \(\mathcal{X}^{n}\) such that \(\left\{\mathbf{x}_{i}\right\}_{i\in[n]}\sim P^{n}\) is a set of \(n\)_i.i.d._ samples. Given any matrix \(\mathbf{M}\in\mathbb{R}^{n\times k}\) (\(n\geq k\) without loss of generality), let \(\sigma\left(\mathbf{M}\right)=\left\{\sigma_{i}\left(\mathbf{M}\right)\,|\,i \in[k]\right\}\) be the singular values of \(\mathbf{M}\) with \(\sigma_{1}\left(\mathbf{M}\right)\geq\cdots\geq\sigma_{k}\left(\mathbf{M}\right)\). While for any symmetric \(\mathbf{M}\in\mathbb{R}^{n\times n}\), let \(\lambda\left(\mathbf{M}\right)=\left\{\lambda_{i}\left(\mathbf{M}\right)\,|\,i \in[n]\right\}\) be the eigenvalues of \(\mathbf{M}\) such that \(\lambda_{1}\left(\mathbf{M}\right)\leq\cdots\leq\lambda_{n}\left(\mathbf{M}\right)\). Given any labeling function \(y:\mathcal{X}\rightarrow[K]\), let \(\overrightarrow{y}:\mathcal{X}\rightarrow\left\{0,1\right\}^{K}\) be its one-hot encoding.

### Spectral Clustering on Population-induced Graph

We consider a \(K\)-class classification problem over an unknown data distribution \(P:\mathcal{X}\times[K]\rightarrow[0,1]\) (while overloading \(P\) for the probability measure over \(\mathcal{X}\) without ambiguity). Let \(y_{*}:\mathcal{X}\rightarrow[K]\) be the ground truth classification that introduced a natural partition \(\left\{\mathcal{X}_{k}\right\}_{k\in[K]}\) where \(\mathcal{X}_{k}\triangleq\left\{\mathbf{x}\in\mathcal{X}\,\left|\,y_{*}\left( \mathbf{x}\right)=k\right\}\) such that \(\bigcup_{k=1}^{K}\mathcal{X}_{k}=\mathcal{X}\) and \(\mathcal{X}_{k}\cap\mathcal{X}_{k^{\prime}}=\emptyset\) for all \(k\neq k^{\prime}\). Meanwhile, we specify a function class \(\mathcal{F}\subset\left\{f:\mathcal{X}\rightarrow\mathbb{R}^{K}\right\}\) to learn from where each \(f\in\mathcal{F}\) is associated with a prediction function \(y_{f}\left(\mathbf{x}\right)\triangleq\operatorname*{argmax}_{k\in[K]}f\left( \mathbf{x}\right)_{k}\).

We characterize the geometry of the data population3\(\mathcal{X}\) with an undirected weighted graph \(G_{\mathcal{X}}=\left(\mathcal{X},\mathbf{W}(\mathcal{X})\right)\) such that: (i) For every vertex pair \(\mathbf{x},\mathbf{x}^{\prime}\in\mathcal{X}\), the edge weight \(w_{\mathbf{x}\mathbf{x}^{\prime}}\geq 0\) characterizes the similarity between \(\left(\mathbf{x},\mathbf{x}^{\prime}\right)\). For example, \(w_{\mathbf{x}\mathbf{x}^{\prime}}\) between a pair of samples from the _same_ class is larger than that between a pair from _different_ classes (ii) \(P(\mathbf{x})=w_{\mathbf{x}}\triangleq\sum_{\mathbf{x}^{\prime}\in\mathcal{X}} w_{\mathbf{x}\mathbf{x}^{\prime}}\) such that \(\sum_{\mathbf{x}\in\mathcal{X}}\sum_{\mathbf{x}^{\prime}\in\mathcal{X}}w_{ \mathbf{x}\mathbf{x}^{\prime}}=1\), intuitively implying that a more representative instance \(\mathbf{x}\) (corresponding to a larger \(w_{\mathbf{x}}\)) has a higher probability of being sampled. We remark that such population-induced graph \(G_{\mathcal{X}}\) is reminiscent of the population augmentation graph for studying contrastive learning4(HaoChen et al., 2021).

Footnote 3: For demonstration purposes, we assume an arbitrarily large, but finite, population \(\left|\mathcal{X}\right|<\infty\) (as in HaoChen et al. (2021), _e.g._, \(\mathcal{X}=\mathbb{F}^{d}\) where \(\mathbb{F}\) is the set of all floating-point numbers with finite precision). Meanwhile, our analyses can be generalized to any compact set \(\mathcal{X}\) via functional analysis tools (_e.g._, replacing the adjacency matrix \(\mathbf{W}(\mathcal{X})\) with an adjacency operator).

Footnote 4: The key difference between the population augmentation graph (HaoChen et al., 2021) and the population-induced graph described here is that for analyzing RKD, we consider a graph over the original, instead of the augmented, population. It’s also worth highlighting that we introduce such graph structure only for the sake of analysis, while our data distribution is generic. Therefore, we focus on learning from a general function class \(\mathcal{F}\) without constraining to graph neural networks.

Let \(\mathbf{W}\left(\mathcal{X}\right)\) with \(\mathbf{W}_{\mathbf{x}\mathbf{x}^{\prime}}\left(\mathcal{X}\right)=w_{ \mathbf{x}\mathbf{x}^{\prime}}\) be the weighted adjacency matrix of \(G_{\mathcal{X}}\); let \(\mathbf{D}\left(\mathcal{X}\right)\) be the diagonal matrix of the weighted degrees \(\left\{w_{\mathbf{x}}\right\}_{\mathbf{x}\in\mathcal{X}}\). The (normalized) graph Laplacian takes the form \(\mathbf{L}\left(\mathcal{X}\right)=\mathbf{I}-\overline{\mathbf{W}}( \mathcal{X})\) where \(\overline{\mathbf{W}}(\mathcal{X})\triangleq\mathbf{D}\left(\mathcal{X} \right)^{-1/2}\mathbf{W}\left(\mathcal{X}\right)\mathbf{D}\left(\mathcal{X} \right)^{-1/2}\) is the normalized adjacency matrix. Then, the _spectral clustering_(von Luxburg, 2007) on \(G_{\mathcal{X}}\) can be expressed as a (rank-\(K\)) Nystrom approximation of \(\overline{\mathbf{W}}(\mathcal{X})\) in terms of the weighted outputs \(\mathbf{D}(\mathcal{X})^{\frac{1}{2}}f\left(\mathcal{X}\right)\):

\[\mathcal{F}_{\mathbf{L}\left(\mathcal{X}\right)}\triangleq\operatorname*{ argmin}_{f\in\mathcal{F}}\left\{R\left(f\right)\triangleq\left\|\overline{ \mathbf{W}}(\mathcal{X})-\mathbf{D}(\mathcal{X})^{\frac{1}{2}}f\left(\mathcal{ X}\right)f\left(\mathcal{X}\right)^{\top}\mathbf{D}(\mathcal{X})^{\frac{1}{2}} \right\|_{F}^{2}\right\}.\] (1)

To quantify the alignment between the ground truth class partition and the clustering predicted by \(f\in\mathcal{F}\), we introduce the notion of clustering error.

**Definition 3.1** (Clustering error).: _Given any \(f\in\mathcal{F}\), we define the majority labeling_

\[\widetilde{y}_{f}\left(\mathbf{x}\right)\triangleq\operatorname*{argmax}_{k\in \left[K\right]}\mathbb{P}_{\mathbf{x}^{\prime}\sim P\left(\mathbf{x}\right)} \left[y_{*}(\mathbf{x}^{\prime})=k\mid y_{f}(\mathbf{x}^{\prime})=y_{f}( \mathbf{x})\right],\] (2)

_along with the minority subsets associated with \(f\): \(M\left(f\right)\triangleq\left\{\mathbf{x}\in\mathcal{X}\mid\widetilde{y}_{f} (\mathbf{x})\neq y_{*}\left(\mathbf{x}\right)\right\}\) such that \(P\left(M(f)\right)\) quantifies the clustering error of \(f\). For any \(\mathcal{F}^{\prime}\subseteq\mathcal{F}\), let \(\mu\left(\mathcal{F}^{\prime}\right)\triangleq\sup_{f\in\mathcal{F}^{\prime}} P\left(M\left(f\right)\right)\)._

Intuitively, \(M(f)\) characterizes the difference between \(K\)-partition of \(\mathcal{X}\) by the ground truth \(y_{*}\) and by the prediction function \(y_{f}\) associated with \(f\); while \(\mu\left(\mathcal{F}^{\prime}\right)\) quantifies the worse-case clustering error of all functions \(f\in\mathcal{F}^{\prime}\). In Section 4.1, we will demonstrate that spectral clustering on \(G_{\mathcal{X}}\) (Equation (1)) leads to provably low clustering error \(\mu\left(\mathcal{F}_{\mathbf{L}\left(\mathcal{X}\right)}\right)\).

### Relational Knowledge Distillation

For RKD, we assume access to a proper teacher model \(\psi:\mathcal{X}\rightarrow\mathcal{W}\) (for a latent feature space \(\mathcal{W}\)) that induces a graph-revealing kernel: \(k_{\psi}\left(\mathbf{x},\mathbf{x}^{\prime}\right)=\frac{w_{\mathbf{x}\mathbf{ x}^{\prime}}}{w_{\mathbf{x}\mathbf{x}^{\prime}}}\). Then, the spectral clustering on \(G_{\mathcal{X}}\) in Equation (1) can be interpreted as the _population RKD loss_:

\[R\left(f\right)=\mathbb{E}_{\mathbf{x},\mathbf{x}^{\prime}\sim P\left(\mathbf{ x}\right)^{2}}\left[\left(k_{\psi}\left(\mathbf{x},\mathbf{x}^{\prime}\right)-f( \mathbf{x})^{\top}f(\mathbf{x}^{\prime})\right)^{2}\right].\]

While with only limited unlabeled samples \(\mathbf{X}^{u}=\left\{\mathbf{x}_{j}^{u}\mid j\in\left[N\right]\right\}\sim P (\mathbf{x})^{N}\) in practice, we consider the analogous _empirical RKD loss_:

\[\mathcal{F}_{\mathbf{L}\left(\mathbf{X}^{u}\right)}\triangleq\operatorname*{ argmin}_{f\in\mathcal{F}}\left\{\widehat{R}_{\mathbf{X}^{u}}\left(f \right)\triangleq\frac{2}{N}\sum_{i=1}^{N/2}\left(f\left(\mathbf{x}_{2i-1}^{u }\right)^{\top}f\left(\mathbf{x}_{2i}^{u}\right)-k_{\psi}\left(\mathbf{x}_{2i -1}^{u},\mathbf{x}_{2i}^{u}\right)\right)^{2}\right\},\] (3)

where \(\widehat{R}_{\mathbf{X}^{u}}(f)\) serves as an unbiased estimate for \(R(f)\) (Proposition D.1).

Further, for _semi-supervised setting_ with a small set of labeled samples \(\left(\mathbf{X},\mathbf{y}\right)=\left\{\left(\mathbf{x}_{i},y_{i}\right) \right\}_{i\in\left[n\right]}\sim P\left(\mathbf{x},y\right)^{n}\) (usually \(n\ll N\)) _independent of \(\mathbf{X}^{u}\)_, let \(\ell:\mathbb{R}^{K}\times\left[K\right]\rightarrow\left\{0,1\right\}\) be the zero-one loss: \(\ell\left(f(\mathbf{x}),y\right)=\mathds{1}\left\{y_{f}(\mathbf{x})\neq y\right\}\). We denote \(\mathcal{E}\left(f\right)\triangleq\mathbb{E}_{\left(\mathbf{x},y\right)\sim P }\left[\ell\left(f(\mathbf{x}),y\right)\right]\) and \(\widehat{\mathcal{E}}\left(f\right)\triangleq\frac{1}{n}\sum_{i=1}^{n}\ell \left(f(\mathbf{x}_{i}),y_{i}\right)\) as the population and empirical losses, respectively, and consider a proper learning setting with the ground truth \(f_{*}\triangleq\operatorname*{argmin}_{f\in\mathcal{F}}\mathcal{E}\left(f\right)\). Then, SSL with RKD aims to find \(\min_{f\in\mathcal{F}}\widehat{\mathcal{E}}(f)+\widehat{R}_{\mathbf{X}^{u}}(f)\). Alternatively, for an overparameterized setting5 with \(\left(\operatorname*{argmin}_{f\in\mathcal{F}}\widehat{\mathcal{E}}\left(f \right)\right)\cap\mathcal{F}_{\mathbf{L}\left(\mathbf{X}^{u}\right)}\neq\emptyset\), SSL with RKD can be formulated as: \(\min_{f\in\mathcal{F}_{\mathbf{L}\left(\mathbf{X}^{u}\right)}}\widehat{ \mathcal{E}}\left(f\right)\).

Footnote 5: Overparameterization is a ubiquitous scenario when learning with neural networks: \(f\in\mathcal{F}\) is parameterized by \(\theta\in\Theta\) with \(\dim(\Theta)>N+n\) such that the minimization problem has infinitely many solutions.

## 4 Relational Knowledge Distillation as Spectral Clustering

In this section, we show that minimizing either the population RKD loss (Section 4.1) or the empirical RKD loss (Section 4.2) leads to low clustering errors \(\mu\left(\mathcal{F}_{\mathbf{L}\left(\mathcal{X}\right)}\right)\) and \(\mu\left(\mathcal{F}_{\mathbf{L}\left(\mathbf{X}^{u}\right)}\right)\), respectively.

### Relational Knowledge Distillation over Population

Starting with minimization of the population RKD loss \(f\in\mathcal{F}_{\mathbf{L}\left(\mathcal{X}\right)}=\operatorname*{argmin}_{f\in \mathcal{F}}R\left(f\right)\) (Equation (1)), let \(\lambda\left(\mathbf{L}\left(\mathcal{X}\right)\right)=\left(\lambda_{1},\dots, \lambda_{\left|\mathcal{X}\right|}\right)\) be the eigenvalues of the graph Laplacian in the ascending order, \(0=\lambda_{1}\leq\dots\leq\lambda_{\left|\mathcal{X}\right|}\) with an arbitrary breaking of ties.

A key assumption of RKD is that the teacher model \(\psi\) (with the corresponding graph-revealing kernel \(k_{\psi}\)) is well aligned with the underlying ground truth partition, formalized as below:

**Assumption 4.1** (Approximate teacher models).: _For \(G_{\mathcal{X}}\) unveiled through the teacher model \(k_{\psi}\left(\cdot,\cdot\right)\), we assume \(\lambda_{K+1}>0\) (i.e., \(G_{\mathcal{X}}\) has at most \(K\) disconnected components); while the ground truth classes \(\left\{\mathcal{X}_{k}\right\}_{k\in\left[K\right]}\) are well-separated by \(G_{\mathcal{X}}\) such that \(\alpha\triangleq\frac{\sum_{k\in\left[K\right]}\sum_{k\in\mathcal{X}_{k}}\sum_{k \in\mathcal{X}^{\prime}}q_{\mathcal{X}_{k}}w_{\mathbf{x}\mathbf{x}^{\prime}}}{2 \sum_{\mathbf{x}\in\mathcal{X}}\sum_{\mathbf{x}^{\prime}\in\mathcal{X}}w_{ \mathbf{x}\mathbf{x}^{\prime}}}\ll 1\) (i.e., the fraction of weights of inter-class edges is sufficiently small)._In particular, \(\alpha\in[0,1]\) reflects the extent to which the ground truth classes \(\{\mathcal{X}_{k}\}_{k\in[K]}\) are separated from each other, as quantified by the fraction of edge weights between nodes of disparate classes. In the ideal case, \(\alpha=0\) when the ground truth classes are perfectly separated such that every \(\mathcal{X}_{k}\) is a connected component of \(G_{\mathcal{X}}\), while \(\lambda_{K+1}>\lambda_{K}=0\).

Meanwhile, to reduce the generic function class \(\mathcal{F}\) down to a class of reasonable prediction functions, we introduce the following regularity conditions on the boundedness and margin:

**Assumption 4.2** (\(\beta\)-skeleton boundedness and \(\gamma\)-margin).: _For any \(f\in\mathcal{F}\) with \(P\left(M(f)\cap\mathcal{X}_{k}\right)\leq P\left(\mathcal{X}_{k}\right)/2\) for all \(k\in[K]\), we assume there exists a skeleton subset \(\mathbf{S}=\left[\mathbf{s}_{1};\ldots;\mathbf{s}_{K}\right]\in\mathcal{X}^{K}\) with6\(\mathbf{s}_{k}=\operatorname*{argmax}_{\mathbf{x}\in\mathcal{X}\setminus M(f)}f(\mathbf{x})_{k}\) such that \(y_{f}\left(\mathbf{s}_{k}\right)=k\) for every \(k\in[K]\) and \(\operatorname*{rank}\left(f(\mathbf{S})\right)=K\)._

Footnote 6: Recall the majority labeling (Equation (2)) and the associated minority subset \(M\left(f\right)\) which intuitively describes the difference between clustering of the \(y_{*}\) and that of \(y_{f}\). We denote \(f(\mathbf{x})=[f(\mathbf{x})_{1},\ldots,f(\mathbf{x})_{K}]\).

1. _[label=()]_
2. _We say that_ \(f\) _is_ \(\beta\)_-skeleton bounded if_ \(\sigma_{1}\left(f(\mathbf{S})\right)\leq\beta\) _for some reasonably small_ \(\beta\)_._
3. _Let the_ \(k\)_-th margin of_ \(f\) _be_ \(\gamma_{k}\triangleq f\left(\mathbf{s}_{k}\right)_{k}-\max_{\mathbf{x}\in M(f) :y_{f}(\mathbf{x})\neq k}f\left(\mathbf{x}\right)_{k}\)_. We say that_ \(f\) _has a_ \(\gamma\)_-margin if_ \(\min_{k\in[K]}\gamma_{k}>\gamma\) _for some sufficiently large_ \(\gamma>0\)_._

Intuitively, \(\mathbf{S}\) can be viewed as a set of \(K\) samples where \(f\) makes the "most confident" prediction in each class. Assumption 4.2 ensures the boundedness of predictions made on such a skeleton subset \(\left\|f\left(\mathbf{S}\right)\right\|_{2}\leq\beta\), as well as a margin \(\gamma\) by which the "most confident" prediction of \(f\) in each class \(\mathbf{s}_{k}\) can be separated from the minority samples predicted to lie in other classes \(\{\mathbf{x}\in M(f)|y_{f}(\mathbf{x})\neq k\}\). As a toy example, when \(y_{f}:\mathcal{X}\rightarrow[K]\) is surjective, and the columns in \(f\left(\mathcal{X}\right)\in\mathbb{R}^{|\mathcal{X}|\times K}\) consist of identity vectors of the predicted clusters \(f\left(\mathbf{x}\right)_{k}=\mathbbm{1}\left\{y_{f}\left(\mathbf{x}\right)=k\right\}\), Assumption 4.2 is satisfied with \(\beta=1\) and \(\gamma=1\). Meanwhile, the following Remark 4.1 highlights that although Assumption 4.2 cannot be guaranteed with spectral clustering alone, it is generally satisfied in the semi-supervised learning setting with additional supervision/regularization (_cf._ Example C.1).

**Remark 4.1** (Limitation of spectral clustering alone).: _Notice that for a generic function class \(\mathcal{F}\), spectral clustering alone is not sufficient to guarantee Assumption 4.2, especially the existence of a skeleton subset \(\mathbf{S}\) or a large enough margin \(\gamma\). As counter-exemplified in Example C.1, Equation (1) can suffer from large clustering error \(\mu\left(\mathcal{F}_{\mathbf{L}(\mathcal{X})}\right)\) when applied alone and failing to satisfy Assumption 4.2._

_To learn predictions with accurate clustering, RKD requires either (i) additional supervision/regularization in end-to-end settings like semi-supervised learning or (ii) further fine-tuning in unsupervised representation learning settings like contrastive learning [11]. For end-to-end learning in practice, RKD is generally coupled with standard KD (i.e., feature matching) and weak supervision [20, 21, 22, 13], both of which help the learned function \(f\) satisfy Assumption 4.2 with a reasonable margin \(\gamma\)._

Throughout this work, we assume \(\mathcal{F}\) is sufficiently regularized (with weak supervision in Section 5 and consistency regularization in Section 6) such that Assumption 4.2 always holds. Under Assumption 4.1 and Assumption 4.2, the clustering error of RKD over the population \(\mu\left(\mathcal{F}_{\mathbf{L}(\mathcal{X})}\right)\) is guaranteed to be small given a good teacher model \(\psi\) that leads to a negligible \(\alpha\):

**Theorem 4.1** (RKD over population, proof in Appendix C.1).: _Under Assumption 4.1 and Assumption 4.2 for every \(f\in\mathcal{F}_{\mathbf{L}(\mathcal{X})}\), the clustering error with the population (Equation (1)) satisfies_

\[\mu\left(\mathcal{F}_{\mathbf{L}(\mathcal{X})}\right)\leq 2\max\left(\frac{ \beta^{2}}{\gamma^{2}},1\right)\cdot\frac{\alpha}{\lambda_{K+1}}.\]

Theorem 4.1 suggests that the clustering error over the population is negligible under mild regularity assumptions (_i.e._, Assumption 4.2) when (i) the ground truth classes are well-separated by \(G_{\mathcal{X}}\) revealed through the teacher model \(k_{\psi}\left(\cdot,\cdot\right)\) (_i.e._, \(\alpha\ll 1\) in Assumption 4.1) and (ii) the \((K+1)\)th eigenvalue \(\lambda_{K+1}\) of the graph Laplacian \(\mathbf{L}(\mathcal{X})\) is not too small. As we review in Appendix C.3, the existing result Lemma C.4 [Louis and Makaychev, 2014] unveils the connection between \(\lambda_{K+1}\) and the sparsest \(K\)-partition (Definition C.1) of \(G_{\mathcal{X}}\). Intuitively, a reasonably large \(\lambda_{K+1}\) implies that the partition of the \(K\) ground truth classes is the "only" partition of \(G_{\mathcal{X}}\) into \(K\) components by removing a _sparse_ set of edges (Corollary C.5).

### Relational Knowledge Distillation on Unlabeled Samples

We now turn our attention to a more practical scenario with limited unlabeled samples and bound the clustering error \(\mu\left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\) granted by minimizing the empirical RKD loss (Equation (3)).

To cope with \(\mathcal{F}\ni f:\mathcal{X}\rightarrow\mathbb{R}^{K}\), we recall the notion of Rademacher complexity for vector-valued functions from Maurer (2016). Let \(\mathrm{Rad}(\rho)\) be the Rademacher distribution (_i.e._, with uniform probability \(\frac{1}{2}\) over \(\{-1,1\}\)) and \(\bm{\rho}\sim\mathrm{Rad}^{N\times K}\) be a random matrix with _i.i.d._ Rademacher entries. Given any \(N\in\mathbb{N}\), with \(f(\mathbf{x})_{k}\) denoting the \(k\)-th entry of \(f(\mathbf{x})\in\mathbb{R}^{K}\), we define

\[\mathfrak{R}_{N}\left(\mathcal{F}\right)\triangleq\mathbb{E}_{\begin{subarray}{ c}\mathbf{X}^{u}\sim P(\mathbf{x})^{N}\\ \bm{\rho}\sim\mathrm{Rad}^{N\times K}\end{subarray}}\left[\sup_{f\in\mathcal{ F}}\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{K}\rho_{ik}\cdot f\left(\mathbf{x}^{u}_{i} \right)_{k}\right]\] (4)

as the Rademacher complexity of the vector-valued function class \(\mathcal{F}\ni f:\mathcal{X}\rightarrow\mathbb{R}^{K}\).

Since the empirical RKD loss is an unbiased estimate of its population correspondence (Proposition D.1), we can leverage the standard generalization analysis in terms of Rademacher complexity to provide an unlabeled sample complexity for RKD.

**Theorem 4.2** (Unlabeled sample complexity of RKD, proof in Appendix D.2).: _Assume there exist \(B_{f},B_{k_{\psi}}>0\) such that \(\left\|f(\mathbf{x})\right\|_{2}^{2}\leq B_{f}\) and \(k_{\psi}\left(\mathbf{x},\mathbf{x}^{\prime}\right)\leq B_{k_{\psi}}\) for all \(\mathbf{x},\mathbf{x}^{\prime}\in\mathcal{X},\;f\in\mathcal{F}\). Given any \(f_{|\mathbf{X}^{u}}\in\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\), \(f_{|\mathcal{X}}\in\mathcal{F}_{\mathbf{L}(\mathcal{X})}\), \(\delta\in(0,1)\), with probability at least \(1-\delta/2\) over \(\mathbf{X}^{u}\sim P(\mathbf{x})^{N}\),_

\[R\left(f_{|\mathbf{X}^{u}}\right)-R\left(f_{|\mathcal{X}}\right)\leq 16\sqrt{2B_ {f}}\left(B_{f}+B_{k_{\psi}}\right)\mathfrak{R}_{N/2}\left(\mathcal{F}\right) +2\left(B_{k_{\psi}}+B_{f}\right)^{2}\sqrt{\frac{\log\left(4/\delta\right)}{N }}.\]

Theorem 4.2 implies that the unlabeled sample complexity of RKD is dominated by the Rademacher complexity of \(\mathcal{F}\). In Appendix D.3, we further concretize Theorem 4.2 by instantiating \(\mathfrak{R}_{N/2}\left(\mathcal{F}\right)\) via existing Rademacher complexity bounds for neural networks (Golowich et al., 2018).

With \(\Delta\to 0\) as \(N\) increasing (Theorem 4.2), the clustering error of minimizing the empirical RKD loss can be upper bounded as follows.

**Theorem 4.3** (RKD on unlabeled samples, proof in Appendix D.4).: _Under Assumption 4.1 and Assumption 4.2 for every \(f\in\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\), given any \(\delta\in(0,1)\), if there exists \(\Delta<\left(1-\lambda_{K}\right)^{2}\) such that \(R\left(f_{|\mathbf{X}^{u}}\right)\leq R\left(f_{|\mathcal{X}}\right)+\Delta\) for all \(f_{|\mathbf{X}^{u}}\in\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\) and \(f_{|\mathcal{X}}\in\mathcal{F}_{\mathbf{L}(\mathcal{X})}\) with probability at least \(1-\delta/2\) over \(\mathbf{X}^{u}\sim P(\mathbf{x})^{N}\), then error of clustering with the empirical graph (Equation (3)) satisfies the follows: for any \(K_{0}\in[K]\) such that \(\lambda_{K_{0}}<\lambda_{K+1}\) and \(C_{K_{0}}\triangleq\frac{\left(1-\lambda_{K_{0}}\right)^{2}-\left(1-\lambda_{ K}\right)^{2}}{\Delta}=O(1)\),_

\[\mu\left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\leq 2\max\left( \frac{\beta^{2}}{\gamma^{2}},1\right)\cdot\left(\frac{\alpha}{\lambda_{K+1}}+ \frac{\left(1+\left(K-K_{0}\right)C_{K_{0}}\right)\Delta}{\left(1-\lambda_{K_{0 }}\right)^{2}-\left(1-\lambda_{K+1}\right)^{2}}\right).\]

Theorem 4.3 ensures that given sufficient unlabeled samples (proportional to the Rademacher complexity of \(\mathcal{F}\)) such that \(\Delta\to 0\), the clustering error \(\mu\left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\) from empirical RKD is nearly as \(\mu\left(\mathcal{F}_{\mathbf{L}(\mathcal{X})}\right)\) from population RKD, up to an additional error term that scales linearly in \(\Delta\).

## 5 Label Efficiency of Cluster-aware Semi-supervised Learning

In this section, we demonstrate the label efficiency of learning from a function class with low clustering error (Definition 3.1), like the ones endowed by RKD discussed in Section 4.

Specifically, given any cluster-aware function subclass \(\mathcal{F}^{\prime}\subseteq\mathcal{F}\) with low clustering error (_e.g._, \(\mathcal{F}_{\mathbf{L}(\mathcal{X})}\) and \(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\)), for a set of \(n\)_i.i.d._ labeled samples \((\mathbf{X},\mathbf{y})\sim P(\mathbf{x},y)^{n}\), we have the following generalization guarantee:

**Theorem 5.1** (Label complexity, proof in Appendix B.1).: _Given any cluster-aware \(\mathcal{F}^{\prime}\subseteq\mathcal{F}\) with \(\mu(\mathcal{F}^{\prime})\ll 1\), assuming that \((\mathbf{X},\mathbf{y})\) contains at least one sample per class, for any \(\delta\in(0,1)\), with probability at least \(1-\delta/2\) over \((\mathbf{X},\mathbf{y})\sim P(\mathbf{x},y)^{n}\), \(\widehat{f}\in\operatorname*{argmin}_{f\in\mathcal{F}^{\prime}}\widehat{ \mathcal{E}}(f)\) satisfies_

\[\mathcal{E}\left(\widehat{f}\right)-\mathcal{E}\left(f_{*}\right)\leq 4\sqrt{ \frac{2K\log(2n)}{n}+2\mu\left(\mathcal{F}^{\prime}\right)}+\sqrt{\frac{2\log \left(4/\delta\right)}{n}},\] (5)

e.g., _conditioned on \(\mathbf{X}^{u}\), \(\widehat{f}_{|\mathbf{X}^{u}}\in\operatorname*{argmin}_{f\in\mathcal{F}_{ \mathbf{L}(\mathbf{X}^{u})}}\widehat{\mathcal{E}}(f)\) satisfies Equation (5) with \(\mu\left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\)._Theorem 5.1 implies that with low clustering error (_e.g._, endowed by unsupervised methods like RKD in Theorem 4.1/Theorem 4.3, DAC in Theorem 6.2), the labeled sample complexity is as low as \(\widetilde{O}\left(K\right)\), linear in the number of clusters and asymptotically optimal7 up to a logarithmic factor.

Footnote 7: Notice that assuming at least one labeled sample per class (_i.e._, \(n\geq K\)) is necessary for any generalization guarantees. Otherwise, depending on the missing classes, the excess risk in Equation (5) can be arbitrarily bad.

**Remark 5.1** (Class imbalance, elaborated in Appendix B.2).: _In Theorem 5.1, while the label complexity scales as \(n=\widetilde{O}(K)\) with i.i.d. sampling, we meanwhile assume that \(\left(\mathbf{X},\mathbf{y}\right)\) contains at least one labeled sample per class. Intuitively, when the \(K\) classes are balanced (i.e., \(\left|\mathcal{X}_{k}\right|=\left|\mathcal{X}\right|/K\) for all \(k\in\left[K\right]\)), an analogy to the coupon collector problem implies that \(n=O\left(K\log(K)\right)\) i.i.d. labeled samples are sufficient in expectation. Nevertheless, with class imbalance, the label complexity can be much worse (e.g., when \(\left|\mathcal{X}_{K}\right|\ll\left|\mathcal{X}_{k}\right|\) for all \(k\in\left[K-1\right]\), collecting one label \(K\) takes \(1/P\left(\mathcal{X}_{K}\right)\gg K\) labeled samples). In Appendix B.2, we show that such label inefficiency can be circumvented by leveraging a cluster-aware prediction \(f\in\mathcal{F}^{\prime}\) and drawing \(O\left(\log(K)\right)\) labeled samples uniformly from each of the \(K\) predicted clusters, instead of i.i.d. from the entire population._

**Remark 5.2** (Coreset selection).: _While Theorem 5.1 yields an asymptotically optimal label complexity guarantee with i.i.d. sampling, it is of significant practical value to be more judicious in selecting labeled samples, especially for the low-label-rate regime (e.g., \(n=O(K)\)). Therefore, in the experiments (Appendix A), we further investigate--alongside i.i.d. sampling8--a popular coreset selection method (Bilmes, 2022; Krause and Golovin, 2014), where the coreset \(\left(\mathbf{X},\mathbf{y}\right)\) is statistically more representative of the data distribution \(P\)._

Footnote 8: For the low-label-rate experiments, to ensure consistent performance, we follow the common practice (Calder et al., 2020) and slightly deviate from the assumption by sampling _i.i.d._ from each ground truth class instead.

_In particular, coreset selection can be recast as a facility location problem (Krause and Golovin, 2014) characterized by the teacher model: \(\max_{\mathbf{X}\subset\mathcal{X}}\;\sum_{\mathbf{x}^{\prime}\in\mathcal{X}} \max_{\mathbf{x}\in\mathbf{X}}\;k_{\psi}(\mathbf{x},\mathbf{x}^{\prime})\), whose optimizers can be approximated heuristically via the stochastic greedy (Mirzasoleiman et al., 2015) submodular optimization algorithm (Schreiber et al., 2020). Intuitively, the facility location objective encourages the coreset \(\mathbf{X}\) to be representative of the entire population \(\mathcal{X}\) in a pairwise similarity sense, and the coreset approximation identified by the stochastic greedy method is nearly optimal (in the facility location objective) up to a multiplicative constant (Mirzasoleiman et al., 2015)._

## 6 Data Augmentation Consistency Regularization as Clustering

In light of the broadness of the notion of clustering, investigating the discrepancy between different types of cluster-awareness is crucial for understanding the effect of spectral clustering brought by RKD beyond the low clustering error demonstrated in Section 4. In this section, we leverage the existing theoretical tools from Wei and Ma (2019); Cai et al. (2021) to unify DAC regularization into the cluster-aware semi-supervised learning framework in Theorem 5.1. Specifically, we demonstrate the effect of DAC as a "local" expansion-based clustering (Yang et al., 2023), which works in complement with the "global" spectral clustering facilitated by RKD.

Start by recalling the formal notion of expansion-based data augmentation (Definition 6.1) and data augmentation consistency (DAC) error (Definition 6.2) from Wei et al. (2021):

**Definition 6.1** (Expansion-based data augmentation ((Wei et al., 2021) Definition 3.1)).: _For any \(\mathbf{x}\in\mathcal{X}\), we consider a set of class-invariant data augmentations \(\mathcal{A}\left(\mathbf{x}\right)\subset\mathcal{X}\) such that \(\left\{\mathbf{x}\right\}\subsetneq\mathcal{A}(\mathbf{x})\subseteq\mathcal{X} _{y_{*}(x)}\). We say \(\mathbf{x},\mathbf{x}^{\prime}\in\mathcal{X}\) lie in neighborhoods of each other if their augmentation sets have a non-empty intersection: \(\mathrm{NB}\left(\mathbf{x}\right)\triangleq\left\{\mathbf{x}^{\prime}\in \mathcal{X}\;|\;\mathcal{A}(\mathbf{x})\cap\mathcal{A}(\mathbf{x}^{\prime}) \neq\emptyset\right\}\) for \(\mathbf{x}\in\mathcal{X}\); and \(\mathrm{NB}(S)\triangleq\bigcup_{\mathbf{x}\in S}\mathrm{NB}\left(\mathbf{x}\right)\) for \(S\subseteq\mathcal{X}\). Then, we quantify strength of such data augmentations via the \(c\)-expansion property (\(c>1\)): for any \(S\subseteq\mathcal{X}\) such that \(P\left(S\cap\mathcal{X}_{k}\right)\leq P(\mathcal{X}_{k})/2\;\forall\;k\in\left[K\right]\),_

\[P\left(\mathrm{NB}(S)\cap\mathcal{X}_{k}\right)>\min\left\{c\cdot P\left(S \cap\mathcal{X}_{k}\right),P(\mathcal{X}_{k})\right\}\;\forall\;k\in\left[K\right].\]

**Definition 6.2** (DAC error (Wei et al., 2021) (3.3)).: _For any \(f\in\mathcal{F}\) and \(\mathcal{F}^{\prime}\subseteq\mathcal{F}\), let_

\[\nu\left(f\right)\triangleq\mathbb{P}_{\mathbf{x}\sim P(\mathbf{x})}\left[ \exists\;\mathbf{x}^{\prime}\in\mathcal{A}(\mathbf{x})\text{ s.t. }y_{f}(\mathbf{x})\neq y_{f}(\mathbf{x}^{\prime})\right],\quad\nu\left( \mathcal{F}^{\prime}\right)\triangleq\sup_{f\in\mathcal{F}^{\prime}}\nu(f).\]We adopt the theoretical framework in Wei et al. (2021) for generic neural networks with smooth activation function \(\phi\) and consider

\[\mathcal{F}=\left\{f(\mathbf{x})=\mathbf{A}_{p}\phi\left(\cdots\mathbf{A}_{2} \phi(\mathbf{A}_{1}\mathbf{x})\cdots\right)\biggm{|}\mathbf{A}_{\iota}\in \mathbb{R}^{d_{\iota}\times d_{\iota-1}}\forall\,\iota\in[p],\;d=\max_{\iota=0, \cdots,p}d_{\iota}\right\}.\]

With such function class \(\mathcal{F}\) of \(p\)-layer neural networks with maximum width \(d\) and weights \(\left\{\mathbf{A}_{\iota}\right\}_{\iota=1}^{p}\), we recall the notion of _all-layer margin_ from Wei et al. (2021) Appendix C.2. By decomposing \(f=f_{2p-1}\circ\cdots\circ f_{1}\) such that \(f_{2\iota-1}(\mathbf{z})=\mathbf{A}_{\iota}\mathbf{z}\) for all \(\iota\in[p]\) and \(f_{2\iota}(\mathbf{z})=\phi(\mathbf{z})\) for all \(\iota\in[p-1]\), we consider a perturbation \(\boldsymbol{\delta}=\left(\boldsymbol{\delta}_{1},\cdots,\boldsymbol{\delta} _{2p-1}\right)\) (where \(\boldsymbol{\delta}_{2\iota-1},\boldsymbol{\delta}_{2\iota}\in\mathbb{R}^{d_{ \iota}}\)) to each layer of \(f\):

\[f_{1}\left(\mathbf{x},\boldsymbol{\delta}\right) =f_{1}\left(\mathbf{x},\boldsymbol{\delta}_{1}\right)=f_{1}( \mathbf{x})+\boldsymbol{\delta}_{1}\left\|\mathbf{x}\right\|_{2},\] \[f_{\iota}\left(\mathbf{x},\boldsymbol{\delta}\right) =f_{\iota}\left(\mathbf{x},\boldsymbol{\delta}_{1},\cdots, \boldsymbol{\delta}_{\iota}\right)=f_{\iota}\left(f_{\iota-1}\left(\mathbf{x},\boldsymbol{\delta}\right)\right)+\boldsymbol{\delta}_{\iota}\left\|f_{-1} \left(\mathbf{x},\boldsymbol{\delta}\right)\right\|_{2}\]

such that \(f(\mathbf{x},\boldsymbol{\delta})=f_{2p-1}\left(\mathbf{x},\boldsymbol{\delta}\right)\). Then, the all-layer margin \(m:\mathcal{F}\times\mathcal{X}\times[K]\rightarrow\mathbb{R}_{\geq 0}\) is defined as the minimum norm of \(\boldsymbol{\delta}\) that is sufficient to perturb the classification of \(f(\mathbf{x})\):

\[m\left(f,\mathbf{x},y\right)\triangleq\min_{\boldsymbol{\delta}}\sqrt{\sum_{ \iota=1}^{2p-1}\left\|\boldsymbol{\delta}_{\iota}\right\|_{2}^{2}}\quad\text{ s.t.}\quad\operatorname*{argmax}_{k\in[K]}f\left(\mathbf{x},\boldsymbol{\delta} \right)_{k}\neq y.\] (6)

Moreover, Wei et al. (2021) introduced the _robust margin_ for the expansion-based data augmentation \(\mathcal{A}\) (Definition 6.1), which intuitively quantifies the worst-case preservation of the label-relevant information in the augmentations of \(\mathbf{x}\), measured with respect to \(f\):

\[m_{\mathcal{A}}\left(f,\mathbf{x}\right)\triangleq\min_{\mathbf{x}^{\prime}\in \mathcal{A}(\mathbf{x})}m\left(f,\mathbf{x}^{\prime},y_{f}(\mathbf{x})\right).\] (7)

We say the expansion-based data augmentation \(\mathcal{A}\) is _margin-robust_ with respect to the ground truth \(f_{\ast}\) if \(\min_{\mathbf{x}\in\mathcal{X}}m_{\mathcal{A}}\left(f_{\ast},\mathbf{x} \right)>0\) is reasonably large.

Then, we cast DAC regularization9 as reducing the function class \(\mathcal{F}\) via constraints on the robust margins at the \(N\) unlabeled samples \(\mathbf{X}^{u}\): for some \(0<\tau<\min_{\mathbf{x}\in\mathcal{X}}m_{\mathcal{A}}\left(f_{\ast},\mathbf{x}\right)\),

Footnote 9: In Appendix E.2, we further bridge the conceptual notion of DAC regularization in Equation (8) with common DAC regularization algorithms in practice like FixMatch (Sohn et al., 2020).

\[\mathcal{F}_{\mathbf{X}^{u}}\triangleq\left\{f\in\mathcal{F}\mid m_{ \mathcal{A}}\left(f,\mathbf{x}_{i}^{u}\right)\geq\tau\;\forall\;i\in[N]\right\}.\] (8)

Leverage the existing unlabeled sample complexity bound for DAC error from Wei et al. (2021):

**Proposition 6.1** (Unlabeled sample complexity of DAC ((Wei et al., 2021) Theorem 3.7)).: _Given any \(\delta\in(0,1)\), with probability at least \(1-\delta/2\) over \(\mathbf{X}^{u}\),_

\[\nu\left(\mathcal{F}_{\mathbf{X}^{u}}^{\tau}\right)\leq\widetilde{O}\left( \frac{\sum_{\iota=1}^{p}\sqrt{d}\left\|\mathbf{A}_{\iota}\right\|_{E}}{\tau \sqrt{N}}\right)+O\left(\sqrt{\frac{\log\left(1/\delta\right)+p\log\left(N \right)}{N}}\right),\]

_where \(\widetilde{O}\left(\cdot\right)\) suppresses polylogarithmic factors in \(N\) and \(d\)._

The clustering error of Equation (8) is as low as its DAC error, up to a constant factor that depends on the augmentation strength, characterized by the \(c\)-expansion (Definition 6.1):

**Theorem 6.2** (Expansion-based clustering, proof in Appendix E.3).: _For Equation (8) with \(\mathcal{A}\) admitting \(c\)-expansion (Definition 6.1), the clustering error is upper bounded by the DAC error in Proposition 6.1:_

\[\mu\left(\mathcal{F}_{\mathbf{X}^{u}}^{\tau}\right)\leq\max\left\{\frac{2}{c-1},2\right\}\cdot\nu\left(\mathcal{F}_{\mathbf{X}^{u}}^{\tau}\right).\]

In Theorem 6.2, (i) \(c\) characterizes the augmentation strength (_i.e._, the perturbation on label-irrelevant information), whereas (ii) \(\tau\) quantifies the margin robustness (_i.e._, the preservation of label-relevant information) for the expansion-based data augmentation. Ideally, we would like both \(c\) and \(\tau\) to be reasonably large, while there exist trade-offs between the augmentation strength and margin robustness (_e.g._, overly strong augmentations inevitably perturb label-relevant information).

**Remark 6.1** (Clustering with DAC v.s. RKD).: _As Figure 1 demonstrated, in contrast to RKD that unveils the spectral clustering of a population-induced graph from a "global" perspective, DAC alternatively learns a "local" clustering structure through the expansion of neighborhoods \(\mathrm{NB}\left(\cdot\right)\) characterized by sets of data augmentations \(\mathcal{A}(\cdot)\) (Definition 6.1)._

_Therefore, DAC and RKD provide complementary perspectives for each other. On one hand, the "local" clustering structure revealed by DAC effectively ensures a reasonable margin in Assumption 4.2 (Remark 4.1). On the other hand, when the augmentation strength \(c\) is insufficient (i.e., \(\mathcal{A}(\cdot)\) is not expansive enough) for DAC to achieve a low clustering error \(\mu\left(\mathcal{F}_{\mathbf{X}^{u}}\right)\) (Theorem 6.2), the supplementary "global" perspective of RKD connects the non-overlapping neighborhoods in the same classes and brings better classification accuracies, as we empirically verified in Appendix A._

## 7 Discussions, Limitations, and Future Directions

This work provides a theoretical analysis of relational knowledge distillation (RKD) in the semi-supervised classification setting from a clustering perspective. Through a cluster-aware semi-supervised learning (SSL) framework characterized by a notion of low clustering error, we demonstrate that RKD achieves a nearly optimal label complexity (linear in the number of clusters) by learning the underlying geometry of the population as revealed by the teacher model via spectral clustering. With a unified view of data augmentation consistency (DAC) regularization in the cluster-aware SSL framework, we further illustrate the complementary "global" and "local" perspectives of clustering learned by RKD and DAC, respectively.

As an appealing future direction, domain adaptation in knowledge distillation is a common scenario in practice where the training data of the teacher and student models come from different distributions. Although distributional shifts can implicitly reflect the alignment between the ground truth and the teacher model (Assumption 4.1) in our analysis, an explicit and specialized study on RKD in the domain adaptation setting may lead to better theoretical guarantees and deeper insights.

Another avenue for future exploration involves RKD on different (hidden) layers of the neural network. In this work, we consider RKD on the output layer following the standard practice (Park et al., 2019), whereas RKD on additional hidden layers has been shown to further improve the performance (Liu et al., 2019). While the analysis in Section 4 remains valid for hidden-layer features of low dimensions (\(\approx K\)), Assumption 4.1 tends to fail (_i.e., \(\alpha\)_ can be large) for high-dimensional features, which may require separate and careful consideration.

## Acknowledgement

YD was supported in part by the Office of Naval Research N00014-18-1-2354, NSF DMS-1952735, DOE ASCR DE-SC0022251, and UT Austin Graduate School Summer Fellowship. KM was supported by the Peter J. O'Donnell Jr. Postdoctoral Fellowship at the Oden Institute at the University of Texas, Austin. QL acknowledges the support of NYU Research Catalyst Prize and Whitehead Fellowship for Junior Faculty in Biomedical and Biological Sciences. RW was supported in part by AFOSR MURI FA9550-19-1-0005, NSF DMS-1952735, NSF IFML grant 2019844, NSF DMS-N2109155, and NSF 2217033. The authors wish to thank IFML for generously providing computational resources, as well as the anonymous reviewers for their helpful comments and suggestions.

## References

* Allen-Zhu and Li (2020) Z. Allen-Zhu and Y. Li. Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. _arXiv preprint arXiv:2012.09816_, 2020.
* Ba and Caruana (2014) J. Ba and R. Caruana. Do deep nets really need to be deep? _Advances in neural information processing systems_, 27, 2014.
* Bachman et al. (2014) P. Bachman, O. Alsharif, and D. Precup. Learning with pseudo-ensembles. _Advances in neural information processing systems_, 27, 2014.
* Bartlett and Mendelson (2003) P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. _J. Mach. Learn. Res._, 3(null):463-482, Mar. 2003. ISSN 1532-4435.
* Belkin and Niyogi (2004) M. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds. _Machine learning_, 56:209-239, 2004.
* Bickel et al. (2019)M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised learning on large graphs. In _Learning Theory: 17th Annual Conference on Learning Theory, COLT 2004, Banff, Canada, July 1-4, 2004. Proceedings 17_, pages 624-638. Springer, 2004.
* Berthelot et al. [2019] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel. Mixmatch: A holistic approach to semi-supervised learning. _Advances in neural information processing systems_, 32, 2019.
* Bertozzi and Merkurjev [2019] A. L. Bertozzi and E. Merkurjev. Graph-based optimization approaches for machine learning, uncertainty quantification and networks. In _Handbook of Numerical Analysis_, volume 20, pages 503-531. Elsevier, 2019.
* Bilmes [2022] J. Bilmes. Submodularity In Machine Learning and Artificial Intelligence. _Arxiv_, abs/2202.00132, Jan 2022.
* Cai et al. [2021] T. Cai, R. Gao, J. Lee, and Q. Lei. A theory of label propagation for subpopulation shift. In _International Conference on Machine Learning_, pages 1170-1182. PMLR, 2021.
* Calder et al. [2020] J. Calder, B. Cook, M. Thorpe, and D. Slepcev. Poisson learning: Graph-based semi-supervised learning at very low label rates. In _Proceedings of the 37th International Conference on Machine Learning_, pages 1306-1316. Proceedings of Machine Learning Research, Nov. 2020.
* Caron et al. [2020] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin. Unsupervised learning of visual features by contrasting cluster assignments. 2020.
* Chen et al. [2021] B. Chen, P. Li, Z. Yan, B. Wang, and L. Zhang. Deep metric learning with graph consistency. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 982-990, 2021.
* Chen et al. [2017] G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker. Learning efficient object detection models with knowledge distillation. _Advances in neural information processing systems_, 30, 2017.
* Chen et al. [2020] S. Chen, E. Dobriban, and J. H. Lee. A group-theoretic framework for data augmentation. _The Journal of Machine Learning Research_, 21(1):9885-9955, 2020.
* Chen and He [2021] X. Chen and K. He. Exploring simple siamese representation learning. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 15750-15758, 2021.
* Cubuk et al. [2018] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation policies from data. _arXiv preprint arXiv:1805.09501_, 2018.
* Cubuk et al. [2020] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le. Randaugment: Practical automated data augmentation with a reduced search space. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_, pages 702-703, 2020.
* Deng et al. [2009] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* DeVries and Taylor [2017] T. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout. _arXiv preprint arXiv:1708.04552_, 2017.
* Eckart and Young [1936] C. Eckart and G. Young. The approximation of one matrix by another of lower rank. _Psychometrika_, 1(3):211-218, Sep 1936. ISSN 1860-0980.
* Golowich et al. [2018] N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks. In _Conference On Learning Theory_, pages 297-299. PMLR, 2018.
* Golub and Van Loan [2013] G. H. Golub and C. F. Van Loan. _Matrix computations_. JHU press, Baltimore, MD, USA, 2013.
* Gou et al. [2021] J. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge distillation: A survey. _International Journal of Computer Vision_, 129:1789-1819, 2021.
* Grill et al. [2020] J.-B. Grill, F. Strub, F. Altche, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:21271-21284, 2020.
* Ghahramani et al. [2018]J. Z. HaoChen, C. Wei, A. Gaidon, and T. Ma. Provable guarantees for self-supervised deep learning with spectral contrastive loss. _Advances in Neural Information Processing Systems_, 34:5000-5011, 2021.
* Harutyunyan et al. [2023] H. Harutyunyan, A. S. Rawat, A. K. Menon, S. Kim, and S. Kumar. Supervision complexity and its role in knowledge distillation. _arXiv preprint arXiv:2301.12245_, 2023.
* He et al. [2016] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hinton et al. [2015] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* Hsu et al. [2021] D. Hsu, Z. Ji, M. Telgarsky, and L. Wang. Generalization bounds via distillation. _arXiv preprint arXiv:2104.05641_, 2021.
* Huang et al. [2017] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.
* Ji and Zhu [2020] G. Ji and Z. Zhu. Knowledge distillation in wide neural networks: Risk bound, data efficiency and imperfect teacher. _Advances in Neural Information Processing Systems_, 33:20823-20833, 2020.
* Kim et al. [2018] J. Kim, S. Park, and N. Kwak. Paraphrasing complex network: Network compression via factor transfer. _Advances in neural information processing systems_, 31, 2018.
* Krause and Golovin [2014] A. Krause and D. Golovin. Submodular function maximization. In _Tractability: Practical Approaches to Hard Problems_. Cambridge University Press, February 2014.
* Krizhevsky and Hinton [2009] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009.
* Krizhevsky et al. [2017] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. _Communications of the ACM_, 60(6):84-90, 2017.
* Laine and Aila [2016] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. _arXiv preprint arXiv:1610.02242_, 2016.
* Ledoux and Talagrand [2013] M. Ledoux and M. Talagrand. _Probability in Banach Spaces: isoperimetry and processes_. Springer Science & Business Media, 2013.
* Lee et al. [2021] J. D. Lee, Q. Lei, N. Saunshi, and J. Zhuo. Predicting what you already know helps: Provable self-supervised learning. _Advances in Neural Information Processing Systems_, 34:309-323, 2021.
* Liu et al. [2019] Y. Liu, J. Cao, B. Li, C. Yuan, W. Hu, Y. Li, and Y. Duan. Knowledge distillation via instance relationship graph. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7096-7104, 2019.
* Liu et al. [2021] Y. Liu, W. Zhang, J. Wang, and J. Wang. Data-free knowledge transfer: A survey. _arXiv preprint arXiv:2112.15278_, 2021.
* Louis and Makarychev [2014] A. Louis and K. Makarychev. Approximation algorithm for sparsest k-partitioning. In _Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms_, pages 1244-1255. SIAM, 2014.
* Ma et al. [2022] Y. Ma, Y. Chen, and Z. Akata. Distilling knowledge from self-supervised teacher by embedding graph alignment. _arXiv preprint arXiv:2211.13264_, 2022.
* Maurer [2016] A. Maurer. A vector-contraction inequality for rademacher complexities. In _International Conference on Algorithmic Learning Theory_, pages 3-17. Springer, 2016.
* Mei et al. [2021] S. Mei, T. Misiakiewicz, and A. Montanari. Learning with invariances in random features and kernel models. In _Conference on Learning Theory_, pages 3351-3418. PMLR, 2021.
* Maurer et al. [2017]B. Mirzasoleiman. _Big Data Summarization Using Submodular Functions_. PhD thesis, ETH Zurich, 2017.
* Mirzasoleiman et al. [2015] B. Mirzasoleiman, A. Badanidiyuru, A. Karbasi, J. Vondrak, and A. Krause. Lazier than lazy greedy. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 29, 2015.
* Park et al. [2019] W. Park, D. Kim, Y. Lu, and M. Cho. Relational knowledge distillation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3967-3976, 2019.
* Parulekar et al. [2023] A. Parulekar, L. Collins, K. Shanmugam, A. Mokhtari, and S. Shakkottai. Infonce loss provably learns cluster-preserving representations. _arXiv preprint arXiv:2302.07920_, 2023.
* Phan [2021] H. Phan. huyvnphan/pytorch_cifar10, jan 2021.
* Phuong and Lampert [2019] M. Phuong and C. Lampert. Towards understanding knowledge distillation. In _International Conference on Machine Learning_, pages 5142-5151. PMLR, 2019.
* Qian et al. [2022] Q. Qian, H. Li, and J. Hu. Improved knowledge distillation via full kernel matrix transfer. In _Proceedings of the 2022 SIAM International Conference on Data Mining (SDM)_, pages 612-620. SIAM, 2022.
* Romero et al. [2014] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. _arXiv preprint arXiv:1412.6550_, 2014.
* Schreiber et al. [2020] J. Schreiber, J. Bilmes, and W. S. Noble. apricot: Submodular selection for data summarization in python. _Journal of Machine Learning Research_, 21(161):1-6, 2020.
* Shen et al. [2022] R. Shen, S. Bubeck, and S. Gunasekar. Data augmentation as feature manipulation. In _International Conference on Machine Learning_, pages 19773-19808. PMLR, 2022.
* Simard et al. [2002] P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. Transformation invariance in pattern recognition--tangent distance and tangent propagation. In _Neural networks: tricks of the trade_, pages 239-274. Springer, 2002.
* Simonyan and Zisserman [2014] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* Sohn et al. [2020] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. _Advances in neural information processing systems_, 33:596-608, 2020.
* von Luxburg [2007] U. von Luxburg. A tutorial on spectral clustering. _Statistics and Computing_, 17(4):395-416, Dec. 2007. ISSN 1573-1375.
* Wainwright [2019] M. J. Wainwright. _High-Dimensional Statistics: A Non-Asymptotic Viewpoint_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.
* Wang et al. [2022] H. Wang, S. Lohit, M. J. Jones, and Y. Fu. What makes a" good" data augmentation in knowledge distillation-a statistical perspective. In _Advances in Neural Information Processing Systems_, 2022.
* Wang and Yoon [2021] L. Wang and K.-J. Yoon. Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2021.
* Wei and Ma [2019] C. Wei and T. Ma. Data-dependent sample complexity of deep neural networks via lipschitz augmentation. _Advances in Neural Information Processing Systems_, 32, 2019.
* Wei et al. [2021] C. Wei, K. Shen, Y. Chen, and T. Ma. Theoretical analysis of self-training with deep networks on unlabeled data. In _International Conference on Learning Representations_, 2021.
* Welling and Kipf [2016] M. Welling and T. N. Kipf. Semi-supervised classification with graph convolutional networks. In _J. International Conference on Learning Representations (ICLR 2017)_, 2016.
* Yang et al. [2023] S. Yang, Y. Dong, R. Ward, I. S. Dhillon, S. Sanghavi, and Q. Lei. Sample efficiency of data augmentation consistency regularization. In _International Conference on Artificial Intelligence and Statistics_, pages 3825-3853. PMLR, 2023.
* Zhang et al. [2019]S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6023-6032, 2019.
* Zagoruyko and Komodakis (2016a) S. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. _arXiv preprint arXiv:1612.03928_, 2016a.
* Zagoruyko and Komodakis (2016b) S. Zagoruyko and N. Komodakis. Wide residual networks. _arXiv preprint arXiv:1605.07146_, 2016b.
* Zhang et al. (2017) H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* Zhou et al. (2003) D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Scholkopf. Learning with local and global consistency. In S. Thrun, L. Saul, and B. Scholkopf, editors, _Advances in Neural Information Processing Systems_, volume 16. MIT Press, 2003.
* Zhou et al. (2020) J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun. Graph neural networks: A review of methods and applications. _AI Open_, 1:57-81, 2020. ISSN 2666-6510.
* Zhu et al. (2003) X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using Gaussian fields and harmonic functions. In _Proceedings of the 20th International Conference on International Conference on Machine Learning_, pages 912-919, Washington, DC, USA, Aug. 2003. AAAI Press. ISBN 978-1-57735-189-4.

Experiments

In this section, we present experimental results on CIFAR-10/100 [Krizhevsky and Hinton, 2009] to demonstrate the efficacy of combining DAC and RKD (_i.e._, the "local" and "global" perspectives of clustering) for semi-supervised learning in the low-label-rate regime. The experiment code can be found at https://github.com/dydjdongyijun/Semi_Supervised_Knowledge_Distillation.

We consider the (data-free) knowledge distillation setting, wherein the student model cannot access the training data of the teacher model. That is, the student models only have access to limited training samples, with very few labels, drawn from a potentially different data distribution than which the teacher model is pretrained on. More precisely, given a labeled sample set \((\mathbf{X},\mathbf{y})\) of size \(n\) and an unlabeled sample set \(\mathbf{X}^{u}\) of size \(N\) (\(n\ll N\))10, we train a student model (WideResNet [Zagoruyko and Komodakis, 2016b]) in the low-label-rate regime with as few as \(4\) labeled samples per class (_i.e._, \(n=4K\), or \(n=40\) for CIFAR-10 and \(n=400\) for CIFAR-100), along with \(N=25000,40000\), and \(50000\) unlabeled samples (_i.e._, \(50\%\), \(80\%\), \(100\%\) of the entire CIFAR-10/100 training set).

Footnote 10: It is worth mentioning that, in practice, we relax the assumption of the independence between \((\mathbf{X},\mathbf{y})\) and \(\mathbf{X}^{u}\) and recycle \(\mathbf{X}\) in \(\mathbf{X}^{u}\).

Then, combining DAC and RKD, we solve the following objective in the experiments:

\[\min_{f\in\mathcal{F}} \widehat{\mathcal{L}}_{(\mathbf{X},\mathbf{y})}^{\mathrm{CE}}(f )\quad+\underbrace{\lambda_{\mathrm{DAC}}\cdot\widehat{\mathcal{L}}_{\mathbf{ X}^{u}}^{\mathrm{DAC}}(f)}_{\text{Unsupervised DAC loss}}+\underbrace{\lambda_{\mathrm{RKD}}\cdot\widehat{R}_{\mathbf{X}^{u}}(f)}_{\text{ empirical RKD loss}},\]

where \(\widehat{\mathcal{L}}_{(\mathbf{X},\mathbf{y})}^{\mathrm{CE}}(f)=\frac{1}{n}\sum_{i=1}^{n}\ell_{\mathrm{CE}}\left(y_{i}|f\left(\mathbf{x}_{i}\right)\right)\) denotes the supervised cross-entropy loss with \(\ell_{\mathrm{CE}}\left(y|f\left(\mathbf{x}\right)\right)=-\log\left( \mathtt{softmax}\left(f(\mathbf{x})\right)_{y}\right)\). The DAC and RKD losses are as described in Equation (9) and Equation (3), respectively. For the regularization hyper-parameters, we have \(\lambda_{\mathrm{DAC}}=1\) and \(\lambda_{\mathrm{RKD}}=0\) for DAC (only), while \(\lambda_{\mathrm{RKD}}=0.001\) for DAC + RKD.

DAC via FixMatch.We take FixMatch [Sohn et al., 2020]-a state-of-the-art algorithm in the low-label-rate regime-as the semi-supervised learning baseline. By combining RKD with FixMatch, we show that RKD brings additional boosts to the classification accuracy, especially with unsuitable data augmentations and/or limited unlabeled data.

Following the formulation and implementation in Sohn et al. [2020], FixMatch involves a pair of weak and strong data augmentations \((\mathbf{x}^{w},\mathbf{x}^{s})\) for each unlabeled sample \(\mathbf{x}^{u}\). For a current model \(f\), the DAC loss given by FixMatch can be expressed as

\[\begin{split}&\widehat{\mathcal{L}}_{\mathbf{X}^{u}}^{\mathrm{ DAC}}(f)\triangleq\\ &\mathrm{mean}\left(\left\{\ell_{\mathrm{CE}}\left(y_{f}\left( \mathbf{x}_{i}^{w}\right)\middle|f\left(\mathbf{x}_{i}^{s}\right)\right) \middle|\max\left(\mathtt{softmax}\left(\frac{f\left(\mathbf{x}_{i}^{w}\right) }{T}\right)\right)\geq\tau_{\mathrm{DAC}},\;i\in[N]\right\}\right),\end{split}\] (9)

where \(T=1\) is a fixed temperature hyper-parameter. \(\tau_{\mathrm{DAC}}\) is the threshold below which the pseudo-label gauged on \(\mathbf{x}_{i}^{w}\) is considered "with low confidence" and therefore discarded. We set \(\tau_{\mathrm{DAC}}=0.95\) for CIFAR-10 and \(\tau_{\mathrm{DAC}}=0.8\) for CIFAR-100.

In FixMatch, the weak augmentations include random flips and crops, whereas the strong augmentations additionally involve RandAugment [Cubuk et al., 2020]. To investigate the scenarios with unsuitable data augmentations and/or limited unlabeled data, we conduct experiments with various (i) augmentation strength (_i.e._, magnitude of transformations) \(m=2,6,10\) in RandAugment [Cubuk et al., 2020] and (ii) unlabeled sample size \(N=25000,40000,50000\) (_i.e._, \(50\%,80\%,100\%\) of the entire CIFAR-10/100 training set).

Rkd.We warm up with the _in-distribution_ case in the CIFAR-10 experiments (Table 1) where the teacher and student models are (pre)trained on the same dataset (_i.e._, CIFAR-10). Concretely, the teacher model \(\psi:\mathcal{X}\rightarrow\mathcal{W}\) is a Densenet 161 [Huang et al., 2017, Phan, 2021] pretrained on the entirety of the CIFAR-10 training set (via supervised ERM).

Taking a step further, we consider an _out-of-distribution_ scenario in the CIFAR-100 experiments (Table 2) where the teacher model is pretrained on ImageNet [Deng et al., 2009] via an unsupervised contrastive learning method-SwAV [Caron et al., 2020]. Intuitively, we choose teacher modelspretrained via SwAV as it encourages both data clustering and consistency of cluster assignment under data augmentations without involving supervision, which tends to provide representations that generalize better in a cluster-aware semi-supervised learning setting.

Given the teacher models, the output (response) of each sample is taken as the corresponding teacher feature such that \(\mathcal{W}=\mathbb{R}^{K}\). In light of the empirical success of RKD based on simple kernel functions (Park et al., 2019; Liu et al., 2019), we focus on the (shifted) cosine kernel, \(k_{\psi}\left(\mathbf{x},\mathbf{x}^{\prime}\right)=1+\frac{\psi\left(\mathbf{ x}\right)^{\top}\psi\left(\mathbf{x}^{\prime}\right)}{\left\|\psi\left(\mathbf{x} \right)\right\|_{2}\left\|\psi\left(\mathbf{x}^{\prime}\right)\right\|_{2}}\), in the experiments.

With data augmentation (_e.g._, when coupling with FixMatch), we assume that weak augmentations (_i.e._, random flips and crops) bring little perturbations on responses of the teacher model (_i.e._, the teacher features), intuitively as the predictions made by a good teacher model ought to be invariance under weak augmentations. Thereby, we can considerably reduce memory burden and accelerate the RKD process by storing the teacher features \(\psi\left(\mathbf{x}\right)\) of the original data \(\mathbf{x}\in\mathcal{X}\) offline, instead of inferring every augmented sample \(\mathbf{x}^{\prime}\in\mathcal{A}(\mathbf{x})\) with the large teacher model \(\psi\left(\mathbf{x}^{\prime}\right)\) online.

Labeled data selection.As alluded to in Remark 5.2, the success of low-label-rate (i.e., \(n\ll N\)) semi-supervised classification naturally depends on the quality of the (extremely) limited amount of labeled data to represent the underlying population. We perform experiments in which the selection of labeled data is non-adaptive (Uniform) or adaptive (StochasticGreedy). The non-adaptive setting, Uniform, selects labeled data uniformly in each ground truth class11. This contrasts with the strictly _i.i.d._ sampling assumption used in our theoretical results but constitutes a reasonable alternative that has been used in other low-label rate semi-supervised classification works (_e.g._, (Calder et al., 2020)).

Footnote 11: For fair comparison and consistent performance among different configurations in the experiments, we use a separate random number generator with a fixed random seed to ensure that the same set of labeled samples is selected for all experiments.

We also utilize a coreset selection strategy (StochasticGreedy) for identifying "representative" inputs to use as labeled data in an adaptive manner. This labeled set is selected via a stochastic greedy (Mirzasoleiman et al., 2015) optimization problem to (approximately) minimize a facility location objective12. An in-depth discussion of coreset selection methods is outside the scope of the current work, but we refer the interested reader to the enlightening references (Bilmes, 2022; Krause and Golovin, 2014; Mirzasoleiman, 2017) regarding submodular optimization methods in machine learning.

Footnote 12: We use the Apricot (Schreiber et al., 2020) submodular optimization codebase in Python.

Results.From Table 1 and Table 2, we observe that the incorporation of RKD with FixMatch (_i.e._, adding the "global" perspective of RKD to the "local" perspective of DAC) generally brings additional improvements to the classification accuracy, especially when the data augmentation strength is inappropriate and/or the number of unlabeled data is limited.

\begin{table}
\begin{tabular}{c c c|c c c} \hline \hline \multirow{2}{*}{Label selection} & \multirow{2}{*}{Sample sizes \((n,N)\)} & \multirow{2}{*}{SSL algorithm} & \multicolumn{2}{c}{Augmentation Strength} \\  & & & _High_ & _Medium_ & _Weak_ \\ \hline \multirow{4}{*}{Uniform} & (40, 5000) & FixMatch & 89.47 \(\pm\) 3.83 & 87.89 \(\pm\) 1.43 & 81.36 \(\pm\) 1.41 \\  & FixMatch + RKD & **89.88 \(\pm\) 0.41** & **89.10 \(\pm\) 1.14** & **84.61 \(\pm\) 1.23** \\ \cline{2-6}  & (40, 40000) & FixMatch & 87.60 \(\pm\) 1.21 & 85.30 \(\pm\) 1.35 & 83.74 \(\pm\) 3.07 \\  & FixMatch + RKD & **90.19 \(\pm\) 0.19** & **88.65 \(\pm\) 1.81** & **86.68 \(\pm\) 1.06** \\ \cline{2-6}  & (40, 25000) & FixMatch & 81.73 \(\pm\) 1.27 & 81.43 \(\pm\) 2.98 & 76.84 \(\pm\) 1.98 \\  & FixMatch + RKD & **87.06 \(\pm\) 2.04** & **87.24 \(\pm\) 0.39** & **82.75 \(\pm\) 2.35** \\ \hline \multirow{4}{*}{StochasticGreedy} & (40, 50000) & FixMatch & 85.48 \(\pm\) 1.67 & 89.39 \(\pm\) 0.82 & 78.98 \(\pm\) 0.52 \\  & FixMatch + RKD & **91.19 \(\pm\) 0.26** & **90.00 \(\pm\) 0.64** & **87.11 \(\pm\) 4.33** \\ \cline{1-1} \cline{2-6}  & (40, 40000) & FixMatch & 84.51 \(\pm\) 1.34 & 84.88 \(\pm\) 6.68 & 70.70 \(\pm\) 5.32 \\ \cline{1-1} \cline{2-6}  & FixMatch + RKD & **85.68 \(\pm\) 1.19** & **88.31 \(\pm\) 1.36** & **85.12 \(\pm\) 3.49** \\ \cline{1-1} \cline{2-6}  & (40, 25000) & FixMatch & 79.74 \(\pm\) 8.02 & 73.97 \(\pm\) 1.98 & 73.86 \(\pm\) 3.29 \\ \cline{1-1} \cline{2-6}  & FixMatch + RKD & **83.72 \(\pm\) 4.73** & **82.09 \(\pm\) 3.27** & **80.65 \(\pm\) 3.67** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Top-1 accuracy on CIFAR-10.

Training details.Throughout the experiments, we used weight decay \(0.0005\). We train the student model via stochastic gradient descent (SGD) with Nesterov momentum \(0.9\) for \(2^{17}\) iterations (batches) with a batch size \(64*8=2^{9}\) (consisting of \(64\) labeled samples and \(64*7\) unlabeled samples). The initial learning rate is \(0.03\), decaying with a cosine scheduler. The test accuracies are evaluated \(2^{7}\) times in total evenly throughout the \(2^{17}\) iterations (_i.e._, one test evaluation after every \(2^{10}\) iterations) on an EMA model with a decay rate \(0.999\). The average and standard deviation of the best test accuracy (_i.e._, early stopping with the maximum patience \(128\)) are reported for each experiment over \(3\) arbitrary random seeds. Both CIFAR-10/100 experiments are conducted on one NVIDIA A40 GPU.

## Appendix B Proofs and Discussions for Section 5

### Proof of Theorem 5.1

**Lemma B.1** (Rademacher complexity with cluster-aware functions (generalization of Yang et al. (2023) Theorem 8)).: _For a fixed \(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\), let_

\[\ell\circ\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}=\left\{\ell(f(\cdot),\cdot ):\mathcal{X}\times\mathcal{Y}\rightarrow\left\{0,1\right\}\bigm{|}f\in \mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right\}\]

_be the loss function class. Then, its Rademacher complexity can be upper-bounded by_

\[\mathfrak{R}_{n}\left(\ell\circ\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right) \leq\sqrt{\frac{2K\log(2n)}{n}+2\mu\left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u })}\right)}.\] (10)

Proof of Lemma b.1.: Given a set of _i.i.d._ samples \((\mathbf{X},\mathbf{y})\sim P(\mathbf{x},y)^{n}\), let

\[\widehat{\mathfrak{S}}_{(\mathbf{X},\mathbf{y})}\left(\ell\circ\mathcal{F}_{ \mathbf{L}(\mathbf{X}^{u})}\right)\triangleq\left|\left\{\left(\ell\left(f( \mathbf{x}_{1}),y_{1}\right),\ldots,\ell\left(f(\mathbf{x}_{n}),y_{n}\right) \right)\bigm{|}f\in\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right\}\right|.\]

denote the number of distinct patterns over \((\mathbf{X},\mathbf{y})\) in \(\ell\circ\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\). Then, by Massart's finite lemma, the empirical Rademacher complexity with respect to \((\mathbf{X},\mathbf{y})\) is upper bounded by

\[\widehat{\mathfrak{R}}_{(\mathbf{X},\mathbf{y})}\left(\ell\circ\mathcal{F}_{ \mathbf{L}(\mathbf{X}^{u})}\right)\leq\sqrt{\frac{2\log\left(\widehat{ \mathfrak{S}}_{(\mathbf{X},\mathbf{y})}\left(\ell\circ\mathcal{F}_{\mathbf{L} (\mathbf{X}^{u})}\right)\right)}{n}}.\]

\begin{table}
\begin{tabular}{c c c|c c c} \hline \hline \multirow{2}{*}{Label selection} & \multirow{2}{*}{Sample sizes \((n,N)\)} & \multirow{2}{*}{SSL algorithm} & \multicolumn{3}{c}{Augmentation Strength} \\  & & & _High_ & _Medium_ & _Weak_ \\ \hline \multirow{5}{*}{Uniform} & \multirow{3}{*}{(400, 5000)} & FixMatch & 46.85 \(\pm\) 1.68 & 45.67 \(\pm\) 0.51 & 45.84 \(\pm\) 0.35 \\  & & FixMatch + RKD & **49.33 \(\pm\) 0.42** & **47.97 \(\pm\) 0.37** & **47.65 \(\pm\) 0.17** \\ \cline{2-6}  & \multirow{3}{*}{(400, 40000)} & FixMatch & 44.11 \(\pm\) 0.51 & 42.98 \(\pm\) 0.56 & 42.74 \(\pm\) 0.42 \\  & & FixMatch + RKD & **46.24 \(\pm\) 0.64** & **45.78 \(\pm\) 0.94** & **45.13 \(\pm\) 1.08** \\ \cline{2-6}  & \multirow{3}{*}{(400, 25000)} & FixMatch & 36.60 \(\pm\) 0.03 & 35.86 \(\pm\) 0.51 & 33.59 \(\pm\) 1.54 \\  & & FixMatch + RKD & **37.87 \(\pm\) 0.98** & **36.69 \(\pm\) 0.41** & **35.69 \(\pm\) 1.51** \\ \hline \multirow{5}{*}{StochasticGreedy} & \multirow{3}{*}{(400, 50000)} & FixMatch & 47.45 \(\pm\) 0.30 & 48.64 \(\pm\) 0.35 & 46.79 \(\pm\) 1.05 \\  & & FixMatch + RKD & **50.41 \(\pm\) 1.41** & **50.67 \(\pm\) 1.62** & **49.09 \(\pm\) 1.31** \\ \cline{2-6}  & \multirow{3}{*}{(400, 40000)} & FixMatch & 45.93 \(\pm\) 0.21 & 44.78 \(\pm\) 1.51 & 44.62 \(\pm\) 0.72 \\ \cline{1-1}  & & FixMatch + RKD & **47.62 \(\pm\) 0.67** & **47.28 \(\pm\) 0.56** & **46.07 \(\pm\) 0.86** \\ \cline{1-1} \cline{2-6}  & \multirow{3}{*}{(400, 25000)} & FixMatch & 39.40 \(\pm\) 0.31 & 39.01 \(\pm\) 0.68 & 38.22 \(\pm\) 0.42 \\ \cline{1-1}  & & FixMatch + RKD & **40.88 \(\pm\) 0.61** & **40.73 \(\pm\) 0.46** & **39.20 \(\pm\) 0.73** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Top-1 accuracy on CIFAR-100.

By the concavity of \(\sqrt{\log\left(\cdot\right)}\), we know that,

\[\mathfrak{R}_{n}\left(\ell\circ\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u })}\right)= \mathbb{E}_{(\mathbf{X},\mathbf{y})}\left[\widehat{\mathfrak{R}}_{ \mathbf{(X},\mathbf{y})}\left(\ell\circ\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u })}\right)\right]\] (11) \[\leq \mathbb{E}_{(\mathbf{X},\mathbf{y})}\left[\sqrt{\frac{2\log\left( \widehat{\mathfrak{S}}_{(\mathbf{X},\mathbf{y})}\left(\ell\circ\mathcal{F}_{ \mathbf{L}(\mathbf{X}^{u})}\right)\right)}{n}}\right]\] \[\leq \sqrt{\frac{2\log\left(\mathbb{E}_{(\mathbf{X},\mathbf{y})}\left[ \widehat{\mathfrak{S}}_{(\mathbf{X},\mathbf{y})}\left(\ell\circ\mathcal{F}_{ \mathbf{L}(\mathbf{X}^{u})}\right)\right]\right)}{n}}.\]

Since \(P\left(M(f)\right)\leq\mu\left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})} \right)\leq\frac{1}{2}\) for all \(f\in\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\), by union bound, we have

\[\mathbb{E}_{(\mathbf{X},\mathbf{y})}\left[\widehat{\mathfrak{S}}_ {(\mathbf{X},\mathbf{y})}\left(\ell\circ\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u })}\right)\right]\] (12) \[\leq (2n)^{K}\sum_{\iota=0}^{n}\binom{n}{\iota}\left(2\mu\left( \mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\right)^{\iota}\left(1-\mu \left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\right)^{n-\iota}\left( \begin{matrix}n-\iota+1\\ \min\left\{K,n-\iota\right\}-1\end{matrix}\right)^{2K+\iota}\] \[\leq (2n)^{K}\sum_{\iota=0}^{n}\binom{n}{\iota}\left(2\mu\left( \mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\right)^{\iota}\left(1-\mu \left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\right)^{n-\iota}\] \[= (2n)^{K}\left(1-\mu\left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u}) }\right)+2\mu\left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\right)^{n}\] \[\leq (2n)^{K}e^{n\mu\left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})} \right)},\]

where \(\iota\) counts the number of minority samples in the \(n\) samples. Plugging in this last line into Equation (11) yields Equation (10). 

Proof of Theorem 5.1.: Since \(\ell\circ\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\) is \(1\)-bounded with the zero-one loss \(\ell\), Lemma F.1 implies that with probability at least \(1-\delta/2\) over \((\mathbf{X},\mathbf{y})\),

\[\mathcal{E}\left(\widehat{f}_{|\mathbf{X}^{u}}\right)-\mathcal{E}\left(f_{ \ast}\right)\leq 4\mathfrak{R}_{n}\left(\ell\circ\mathcal{F}_{\mathbf{L}( \mathbf{X}^{u})}\right)+\sqrt{\frac{2\log\left(4/\delta\right)}{n}}.\]

Then, incorporating the upper bound of \(\mathfrak{R}_{n}\left(\ell\circ\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\) from Lemma B.1 completes the proof. 

### Cluster-wise Labeling with Cluster-aware Predictions

Recall the notions of majority labeling \(\widetilde{y}_{f}(\cdot)\) (Equation (2)) and minority subset \(M\left(f\right)\) (Definition 3.1). We first specify the cluster-aware predictions that are useful for labeling with the following non-degeneracy assumption.

**Assumption B.1** (\((m_{0},c_{0})\)-non-degenerate predictions).: _For any prediction function \(f\in\mathcal{F}\), we say \(f\) is \((m_{0},c_{0})\)-non-degenerate if the following properties are satisfied:_

1. \(y_{f}:\mathcal{X}\rightarrow[K]\) _is surjective (i.e.,_ \(\left\{y_{f}\left(\mathbf{x}\right)\mid\mathbf{x}\in\mathcal{X}\right\}=[K]\)_)_
2. \(\left|\mathcal{X}_{k}^{f}\right|\geq m_{0}\) _for all_ \(k\in[K]\)_, where we define_ \(\mathcal{X}_{k}^{f}\triangleq\left\{\mathbf{x}\in\mathcal{X}\mid\widetilde{y} _{f}(\mathbf{x})=k\right\}\)__
3. _There exists some_ \(c_{0}\geq 2\) _such that_ \(c_{0}\cdot P\left(M(f)\cap\mathcal{X}_{k}^{f}\right)\leq P\left(\mathcal{X}_{k }^{f}\right)\) _for all_ \(k\in[K]\)_._

It is worth pointing out that although the majority labeling \(\widetilde{y}_{f}(\cdot)\) depends on the ground truth and therefore remains unknown without label acquisition, under Assumption B.1, the associated partition \(\left\{\mathcal{X}_{k}^{f}\right\}_{k\in[K]}\) over \(\mathcal{X}\) depends only on \(f\) and is known without labeling. This is because \(y_{f}(\cdot)\) and \(\widetilde{y}_{f}(\cdot)\) induce the same partition over \(\mathcal{X}\) when both \(y_{f}(\cdot)\) and \(\widetilde{y}_{f}(\cdot)\) are non-degenerate (_i.e._, being surjective functions onto \([K]\), a necessary condition of Assumption B.1 (i) and (ii)). We also notice that Assumption B.1 (iii) can be generally satisfied with a reasonably large \(c\) when learning via Equation (3) with a sufficiently small \(\mu\left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\) (_cf._ Theorem 4.1 or Theorem 4.3). For example, when \(c_{0}\cdot\mu\left(\mathcal{F}_{\mathbf{L}(\mathbf{X}^{u})}\right)\leq\min_{k \in K}P\left(\mathcal{X}_{k}^{f}\right)\), Assumption B.1 (iii) is automatically satisfied.

With the cluster-aware predictions in Assumption B.1, we show that \(O\left(\log(K)\right)\)_i.i.d._ labeled samples per predicted cluster are sufficient to ensure at least one labeled sample per class. That is, the cluster-wise labeling with \(\left(m_{0},c_{0}\right)\)-non-degenerate predictions guarantees a label complexity of \(O\left(K\log(K)\right)\) even with class imbalance.

**Proposition B.2** (Cluster-wise labeling).: _Given any \(\left(m_{0},c_{0}\right)\)-non-degenerate prediction function \(f\in\mathcal{F}\) (Assumption B.1) with sufficiently large \(m_{0}\gg\log_{c_{0}}\left(2K\right)\) and \(c_{0}\geq 2\), for each \(k\in[K]\), acquire labels of \(m\) (\(m\leq m_{0}\)) unlabeled samples \(\left\{\mathbf{x}_{ki}^{u}\right\}_{i\in[m]}\sim\mathrm{Unif}\left(\mathcal{ X}_{k}^{f}\right)^{m}\) drawn i.i.d. from \(\mathcal{X}_{k}^{f}\). Then, for any \(\delta\in\left(\frac{2K}{c_{0}^{m_{0}}},1\right)\), when \(m\geq\log_{c_{0}}\left(\frac{2K}{\delta}\right)\), \(\left\{y_{*}\left(\mathbf{x}_{ki}^{u}\right)\right\}_{k\in[K],i\in[m]}=[K]\) have at least one ground truth label per class with probability at least \(1-\delta\)._

Proof of Proposition b.2.: We first observe that

\[\mathbb{P}\left[\left\{y_{*}\left(\mathbf{x}_{ki}^{u}\right) \right\}_{k\in[K],i\in[m]}=[K]\right]= \mathbb{P}\left[\forall\;k^{\prime}\in[K],\;\exists\;k\in[K],i\in[m] \;\mathrm{s.t.}\;y_{*}\left(\mathbf{x}_{ki}^{u}\right)=k^{\prime}\right]\] \[\geq \mathbb{P}\left[\forall\;k\in[K],\;\exists\;i\in[m]\;\mathrm{s.t. }\;y_{*}\left(\mathbf{x}_{ki}^{u}\right)=k\right]\] \[= \prod_{k=1}^{K}\mathbb{P}_{\left\{\mathbf{x}_{ki}^{u}\right\}_{i \in[m]}\sim\mathrm{Unif}\left(\mathcal{X}_{k}^{f}\right)^{m}}\left[\exists\;i \in[m]\;\mathrm{s.t.}\;y_{*}\left(\mathbf{x}_{ki}^{u}\right)=k\right],\]

where

\[\mathbb{P}\left[\exists\;i\in[m]\;\mathrm{s.t.}\;y_{*}\left( \mathbf{x}_{ki}^{u}\right)=k\right]= 1-\mathbb{P}\left[\forall\;i\in[m],\;y_{*}\left(\mathbf{x}_{ki}^ {u}\right)\neq k\right]\] \[= 1-\left(\mathbb{P}_{\mathbf{x}\sim\mathrm{Unif}\left(\mathcal{X}_ {k}^{f}\right)}\left[y_{*}\left(\mathbf{x}\right)\neq\widetilde{y}_{f}\left( \mathbf{x}\right)\right]\right)^{m}\] \[= 1-\left(\mathbb{P}_{\mathbf{x}\sim\mathrm{Unif}\left(\mathcal{X}_ {k}^{f}\right)}\left[\mathbf{x}\in M(f)\right]\right)^{m}\] \[= 1-\left(\frac{P\left(M(f)\cap\mathcal{X}_{k}^{f}\right)}{P\left( \mathcal{X}_{k}^{f}\right)}\right)^{m}.\]

Since Assumption B.1 implies \(P\left(M(f)\cap\mathcal{X}_{k}^{f}\right)\Big{/}P\left(\mathcal{X}_{k}^{f} \right)\leq\frac{1}{c_{0}}\) for all \(k\in[K]\), we have

\[\mathbb{P}\left[\left\{y_{*}\left(\mathbf{x}_{ki}^{u}\right)\right\}_{k\in[K], i\in[m]}=[K]\right]\geq\left(1-\left(\frac{1}{c_{0}}\right)^{m}\right)^{K}\geq \exp\left(-\frac{2K}{c_{0}^{m}}\right)\geq 1-\frac{2K}{c_{0}^{m}},\]

where the second and the third inequalities follow from the facts that \(1-q\geq\exp\left(-2q\right)\) for all \(0\leq q\leq\frac{1}{2}\) and \(\exp\left(-q\right)\geq 1-q\) for all \(0\leq q\leq 1\), respectively. It is straightforward to observe that when \(m\geq\log_{c_{0}}\left(\frac{2K}{\delta}\right)\), we have \(\frac{2K}{c_{0}^{m}}\leq\delta\). With \(\min_{k\in[K]}\left|\mathcal{X}_{k}^{f}\right|=m_{0}\), taking up to \(m\leq m_{0}\) unlabeled samples for each \(k\in[K]\), we can achieve \(\delta\) as small as \(\delta_{\min}=\frac{2K}{c_{0}^{m_{0}}}\). 

## Appendix C Proofs and Discussions for Section 4.1

In Appendix C.1, we proceed with the proof of low clustering error guarantee (Theorem 4.1) for minimizing the population RKD loss (Equation (1)). In Appendix C.2, we extend the discussion in Remark 4.1 on the limitation of spectral clustering alone in the end-to-end setting via an illustrative toy counter-example. In Appendix C.3, we clarify the connection between our results and corresponding spectral graph theoretic notions of sparsest \(k\)-partitions.

### Proof of Theorem 4.1

Given any labeling function \(y:\mathcal{X}\rightarrow[K]\), let \(\overrightarrow{y}:\mathcal{X}\rightarrow\left\{0,1\right\}^{K}\) be the one-hot encoded labeling function such that for \(y\left(\mathcal{X}\right)\in[K]^{|\mathcal{X}|}\), \(\overrightarrow{y}\left(\mathcal{X}\right)\in\left\{0,1\right\}^{|\mathcal{ X}|\times K}\). Further, given \(f\in\mathcal{F}\), we denote \(\mathbf{U}_{\mathcal{X}}\triangleq\mathbf{D}\left(\mathcal{X}\right)^{1/2} \overrightarrow{y}_{f}^{\prime}\left(\mathcal{X}\right)\), \(\mathbf{F}_{\mathcal{X}}\triangleq\mathbf{D}\left(\mathcal{X}\right)^{1/2}f \left(\mathcal{X}\right)\), and \(\mathbf{Y}_{\mathcal{X}}\triangleq\mathbf{D}\left(\mathcal{X}\right)^{1/2} \overrightarrow{y}_{*}^{\prime}(\mathcal{X})\) such that \(\mathbf{U}_{\mathcal{X}},\mathbf{F}_{\mathcal{X}},\mathbf{Y}_{\mathcal{X}} \in\mathbb{R}^{|\mathcal{X}|\times K}\).

Since \(\mathbf{L}(\mathcal{X})=\mathbf{I}-\overline{\mathbf{W}}(\mathcal{X})\), the classical argument with Gershgorin circle theorem (Golub and Van Loan, 2013, Theorem 7.2.1) implies that \(0\preccurlyeq\mathbf{L}(\mathcal{X})\preccurlyeq\mathbf{2}\mathbf{I}\), whereas \(k_{\psi}\) being positive semi-definite further implies that \(\overline{\mathbf{W}}(\mathcal{X})\succcurlyeq 0\) and therefore \(0\preccurlyeq\mathbf{L}(\mathcal{X})\preccurlyeq\mathbf{I}\) (see the proof of Claim C.3). Consider the spectral decomposition of \(\mathbf{L}\left(\mathcal{X}\right)=\mathbf{I}-\overline{\mathbf{W}}(\mathcal{ X})\),

\[\mathbf{L}\left(\mathcal{X}\right)=\sum_{i=1}^{|\mathcal{X}|}\lambda_{i}\cdot \mathbf{v}_{i}\mathbf{v}_{i}^{\top}\quad\text{s.t.}\quad\mathbf{D}\left( \mathcal{X}\right)^{-1/2}\mathbf{W}\left(\mathcal{X}\right)\mathbf{D}\left( \mathcal{X}\right)^{-1/2}=\sum_{i=1}^{|\mathcal{X}|}\left(1-\lambda_{i} \right)\cdot\mathbf{v}_{i}\mathbf{v}_{i}^{\top},\] (13)

where \(\left\{\mathbf{v}_{i}\in\mathbb{R}^{|\mathcal{X}|}\right\}_{i\in\left\| \mathcal{X}\right\|}\) are the orthonormal eigenvectors associated with \(0=\lambda_{1}\leq\cdots\leq\lambda_{|\mathcal{X}|}\leq 1\), respectively, that form a basis of \(\mathbb{R}^{|\mathcal{X}|}\).

**Lemma C.1**.: _Given any \(f\in\mathcal{F}\) that satisfies Assumption 4.2, we have_

\[P\left(M(f)\right)=\mathbb{E}_{\mathbf{x}\sim P}\left[\mathbf{1}\left\{\widetilde {y}_{f}(\mathbf{x})\neq y_{*}(\mathbf{x})\right\}\right]\leq 2\max\left( \frac{\beta^{2}}{\gamma^{2}},1\right)\cdot\min_{\mathbf{Z}\in\mathbb{R}^{K \times K}}\left\|\mathbf{Y}_{\mathcal{X}}-\mathbf{F}_{\mathcal{X}}\mathbf{Z} \right\|_{F}^{2}.\]

Proof of Lemma c.1.: Without ambiguity, we overload the notation of each sample \(\mathbf{x}\in\mathcal{X}\) as the corresponding index in \(\left[|\mathcal{X}|\right]\) such that, given any sample subset \(S=\left\{\mathbf{s}_{i}\right\}_{i\in\left[|\mathcal{S}|\right]}\subset \mathcal{X}\), following the MATLAB notation for matrix indexing, \(\mathbf{\Pi}_{S}\triangleq\mathbf{I}_{|\mathcal{X}|}\left(:,S\right)\in \mathbb{R}^{|\mathcal{X}|\times|S|}\) where \(\mathbf{I}_{|\mathcal{X}|}\) denotes the \(|\mathcal{X}|\times|\mathcal{X}|\) identity matrix. With respect to \(S\), we also denote \(\mathbf{F}_{S}\triangleq\mathbf{F}_{\mathcal{X}}\left(S,:\right)=\mathbf{\Pi }_{S}^{\top}\mathbf{F}_{\mathcal{X}}\) and \(\mathbf{Y}_{S}\triangleq\mathbf{Y}_{\mathcal{X}}\left(S,:\right)=\mathbf{\Pi }_{S}^{\top}\mathbf{Y}_{\mathcal{X}}\).

We first show that for any \(\mathbf{S}=\left[\mathbf{s}_{1};\ldots;\mathbf{s}_{K}\right]\in\mathcal{X}^{K}\) that satisfies \(\mathrm{rank}\left(\mathbf{F}_{S}\right)=K\),

\[\left\|\mathbf{Y}_{\mathcal{X}}-\mathbf{F}_{\mathcal{X}}\mathbf{F}_{S}^{-1} \mathbf{Y}_{S}\right\|_{F}^{2}\leq 2\min_{\mathbf{Z}\in\mathbb{R}^{K\times K}} \left\|\mathbf{Y}_{\mathcal{X}}-\mathbf{F}_{\mathcal{X}}\mathbf{Z}\right\|_{F }^{2}.\] (14)

To see this, we start by replacing \(\mathbf{Z}\) on the right-hand side of Equation (14) with the corresponding least square solution such that

\[\min_{\mathbf{Z}\in\mathbb{R}^{K\times K}}\left\|\mathbf{Y}_{\mathcal{X}}- \mathbf{F}_{\mathcal{X}}\mathbf{Z}\right\|_{F}^{2}=\left\|\mathbf{Y}_{\mathcal{ X}}-\mathbf{F}_{\mathcal{X}}\mathbf{F}_{\mathcal{X}}^{\dagger}\mathbf{Y}_{ \mathcal{X}}\right\|_{F}^{2}\]

where \(\mathbf{F}_{\mathcal{X}}^{\dagger}=\left(\mathbf{F}_{\mathcal{X}}^{\top} \mathbf{F}_{\mathcal{X}}\right)^{-1}\mathbf{F}_{\mathcal{X}}^{\top}\). Then, we can decompose the left-hand side of Equation (14) via the orthogonal projection \(\mathbf{F}_{\mathcal{X}}\mathbf{F}_{\mathcal{X}}^{\dagger}\):

\[\left\|\mathbf{Y}_{\mathcal{X}}-\mathbf{F}_{\mathcal{X}}\mathbf{F}_{S}^{-1} \mathbf{Y}_{S}\right\|_{F}^{2}= \left\|\left(\mathbf{I}-\mathbf{F}_{\mathcal{X}}\mathbf{F}_{\mathcal{ X}}^{\dagger}\right)\left(\mathbf{Y}_{\mathcal{X}}-\mathbf{F}_{\mathcal{X}} \mathbf{F}_{S}^{-1}\mathbf{Y}_{S}\right)\right\|_{F}^{2}+\left\|\mathbf{F}_{ \mathcal{X}}\mathbf{F}_{\mathcal{X}}^{\dagger}\left(\mathbf{Y}_{\mathcal{X}}- \mathbf{F}_{\mathcal{X}}\mathbf{F}_{S}^{-1}\mathbf{Y}_{S}\right)\right\|_{F}^{2}\]

where \(\mathbf{P}_{F}\triangleq\mathbf{F}_{\mathcal{X}}\mathbf{F}_{\mathcal{X}}^{\dagger} =\mathbf{F}_{\mathcal{X}}\left(\mathbf{F}_{\mathcal{X}}^{\top} \mathbf{F}_{\mathcal{X}}\right)^{-1}\mathbf{F}_{\mathcal{X}}^{\top}\) and \(\mathbf{P}_{S}\triangleq\mathbf{F}_{\mathcal{X}}\left(\mathbf{\Pi}_{S}^{\top} \mathbf{F}_{\mathcal{X}}\right)^{-1}\mathbf{\Pi}_{S}^{\top}\). By observing that

\[\mathbf{P}_{S}\mathbf{P}_{F}=\mathbf{F}_{\mathcal{X}}\left(\mathbf{\Pi}_{S}^{ \top}\mathbf{F}_{\mathcal{X}}\right)^{-1}\mathbf{\Pi}_{S}^{\top}\mathbf{F}_{ \mathcal{X}}\left(\mathbf{F}_{\mathcal{X}}^{\top}\mathbf{F}_{\mathcal{X}} \right)^{-1}\mathbf{F}_{\mathcal{X}}^{\top}=\mathbf{P}_{F}=\mathbf{P}_{F}\mathbf{ P}_{F},\]

we can upper bound \(\left\|\left(\mathbf{P}_{F}-\mathbf{P}_{S}\right)\mathbf{Y}_{\mathcal{X}} \right\|_{F}^{2}\) such that

\[\left\|\left(\mathbf{P}_{F}-\mathbf{P}_{S}\right)\mathbf{Y}_{ \mathcal{X}}\right\|_{F}^{2}= \left\|\left(\mathbf{P}_{F}-\mathbf{P}_{S}\right)\left(\mathbf{I}- \mathbf{P}_{F}\right)\mathbf{Y}_{\mathcal{X}}\right\|_{F}^{2}\] \[\leq \left\|\left(\mathbf{I}-\mathbf{P}_{F}\right)\mathbf{Y}_{\mathcal{X} }\right\|_{F}^{2}\] \[= \left\|\mathbf{Y}_{\mathcal{X}}-\mathbf{F}_{\mathcal{X}}\mathbf{F}_{ \mathcal{X}}^{\dagger}\mathbf{Y}_{\mathcal{X}}\right\|_{F}^{2},\]

which leads to Equation (14).

Now, we consider the sample subset \(\mathbf{S}\in\mathcal{X}^{K}\) described in Assumption 4.2. For given \(f\in\mathcal{F}\), partitioning \(\mathcal{X}\) into the majority and minority subsets, \(\mathcal{X}\backslash M(f)\) and \(M(f)\), respectively yields

\[\left\|\mathbf{Y}_{\mathcal{X}}-\mathbf{F}_{\mathcal{X}}\mathbf{F}_{S}^{-1} \mathbf{Y}_{\mathcal{S}}\right\|_{F}^{2}\] \[= \left(1-P(M(f))\right)\cdot\mathbb{E}_{\mathbf{x}\sim P( \mathbf{x})}\left[\left\|\overrightarrow{y}_{*}^{\star}(\mathbf{x})- \overrightarrow{y}_{*}^{\star}(\mathbf{S})^{\top}f(\mathbf{S})^{-\top}f( \mathbf{x})\right\|_{2}^{2}\bigg{|}\mathbf{x}\in M(f)\right]\] \[\geq P(M(f))\cdot\mathbb{E}_{\mathbf{x}\sim P(\mathbf{x})}\left[\left\| \overrightarrow{y}_{*}^{\star}(\mathbf{x})-\overrightarrow{y}_{*}^{\star}( \mathbf{S})^{\top}f(\mathbf{S})^{-\top}f(\mathbf{x})\right\|_{2}^{2}\bigg{|} \mathbf{x}\in M(f)\right].\]Since \(\mathbf{s}_{k}\notin M(f)\), we have \(\overline{y}_{*}^{\gamma}(\mathbf{S})=\overline{\widetilde{y}_{f}^{\prime}}( \mathbf{S})\) whose rows lie in the canonical bases of \(\left\{\mathbf{e}_{k}\in\mathbb{R}^{K}\right\}_{k\in[K]}\); whereas for \(\mathbf{x}\in M(f)\), we have \(y_{*}(\mathbf{x})\neq\widetilde{y}_{f}(\mathbf{x})\). Therefore, in the case when \(y_{*}(\mathbf{x})=y_{*}(\mathbf{s}_{k})\) for some \(k\in[K]\),

\[\left\|\overline{y}_{*}^{\gamma}(\mathbf{x})-\overline{y}_{*}^{ \gamma}(\mathbf{S})^{\top}f(\mathbf{S})^{-\top}f(\mathbf{x})\right\|_{2}^{2}\] \[= \left\|f(\mathbf{S})^{-\top}\left(f(\mathbf{S})^{\top}\overline {y}_{*}^{\gamma}(\mathbf{S})\overline{y}_{*}^{\gamma}(\mathbf{x})-f(\mathbf{x })\right)\right\|_{2}^{2}\] \[\geq \sigma_{K}\left(f(\mathbf{S})^{-\top}\right)^{2}\cdot\left\|f( \mathbf{x})-\sum_{k\in[K]:y_{*}(\mathbf{x})=y_{*}(\mathbf{s}_{k})}f(\mathbf{ s}_{k})\right\|_{2}^{2},\]

where \(\sigma_{K}\left(f(\mathbf{S})^{-\top}\right)=1/\beta\), and with \(\widetilde{y}_{f}(\mathbf{s}_{k})=y_{*}(\mathbf{s}_{k})=y_{*}(\mathbf{x})\neq \widetilde{y}_{f}(\mathbf{x})\) implying \(y_{f}(\mathbf{s}_{k})\neq y_{f}(\mathbf{x})\),

\[\left\|f(\mathbf{x})-\sum_{k\in[K]:y_{*}(\mathbf{x})=y_{*}(\mathbf{s}_{k})}f( \mathbf{s}_{k})\right\|_{2}^{2}\geq\gamma^{2},\]

we have

\[\left\|\overline{y}_{*}^{\gamma}(\mathbf{x})-\overline{y}_{*}^{\gamma}( \mathbf{S})^{\top}f(\mathbf{S})^{-\top}f(\mathbf{x})\right\|_{2}^{2}\geq\frac {\gamma^{2}}{\beta^{2}}.\]

Meanwhile, in the case when \(y_{*}(\mathbf{x})\neq y_{*}(\mathbf{s}_{k})\) for all \(k\in[K]\), we have \(\overline{y}_{*}^{\gamma}(\mathbf{S})\overline{y}_{*}^{\gamma}(\mathbf{x})= \mathbf{0}\), and therefore

\[\left\|\overline{y}_{*}^{\gamma}(\mathbf{x})-\overline{y}_{*}^{\gamma}( \mathbf{S})^{\top}f(\mathbf{S})^{-\top}f(\mathbf{x})\right\|_{2}^{2}\geq\left \|\overline{y}_{*}^{\gamma}(\overline{y}_{*}^{\gamma}(\mathbf{x})-\overline{y }_{*}^{\gamma}(\mathbf{S})^{\top}f(\mathbf{S})^{-\top}f(\mathbf{x}))\right\|_ {2}^{2}=1.\]

Overall, we've shown that

\[\mathbb{E}_{\mathbf{x}\sim P(\mathbf{x})}\left[\left\|\overline{y}_{*}^{ \gamma}(\mathbf{x})-\overline{y}_{*}^{\gamma}(\mathbf{S})^{\top}f(\mathbf{S}) ^{-\top}f(\mathbf{x})\right\|_{2}^{2}\Bigm{|}\mathbf{x}\in M(f)\right]\geq \min\left(\frac{\gamma^{2}}{\beta^{2}},1\right),\]

which leads to

\[P(M(f))\leq\max\left(\frac{\beta^{2}}{\gamma^{2}},1\right)\left\|\mathbf{Y}_{ \mathcal{X}}-\mathbf{F}_{\mathcal{X}}\mathbf{F}_{S}^{-1}\mathbf{Y}_{S}\right\| _{F}^{2}.\] (15)

Finally, combining Equation (14) and Equation (15) completes the proof. 

**Lemma C.2**.: _With \(\mathbf{y}_{k}\in[0,1]^{|\mathcal{X}|}\) denoting the \(k\)-th column of \(\mathbf{Y}_{\mathcal{X}}\triangleq\mathbf{D}\left(\mathcal{X}\right)^{1/2} \overline{y}_{*}^{\gamma}(\mathcal{X})\in[0,1]^{|\mathcal{X}|\times K}\),_

\[\sum_{k\in[K]}\mathbf{y}_{k}^{\top}\mathbf{L}\left(\mathcal{X} \right)\mathbf{y}_{k}=\alpha.\]

Proof of Lemma c.2.: It is sufficient to observe that, by construction, for every \(k\in[K]\),

\[\mathbf{y}_{k}^{\top}\mathbf{L}\left(\mathcal{X}\right)\mathbf{y }_{k}= \mathbf{y}_{k}^{\top}\mathbf{y}_{k}-\mathbf{y}_{k}^{\top}\mathbf{ D}(\mathcal{X})^{-1/2}\mathbf{W}(\mathcal{X})\mathbf{D}(\mathcal{X})^{-1/2} \mathbf{y}_{k}\] \[= \sum_{\mathbf{x}\in\mathcal{X}}w_{\mathbf{x}}\cdot\mathbb{1}\left\{ y_{*}(\mathbf{x})=k\right\}-\sum_{\mathbf{x}\in\mathcal{X}}\sum_{\mathbf{x}^{ \prime}\in\mathcal{X}}w_{\mathbf{x}\mathbf{x}^{\prime}}\cdot\mathbb{1}\left\{ y_{*}(\mathbf{x})=k\right\}\cdot\mathbb{1}\left\{y_{*}(\mathbf{x}^{ \prime})=k\right\}\] \[= \sum_{\mathbf{x}\in\mathcal{X}}\mathbb{1}\left\{y_{*}(\mathbf{x})= k\right\}\left(w_{\mathbf{x}}-\sum_{\mathbf{x}^{\prime}\in\mathcal{X}}w_{ \mathbf{x}\mathbf{x}^{\prime}}\cdot\mathbb{1}\left\{y_{*}(\mathbf{x})=y_{*}( \mathbf{x}^{\prime})\right\}\right)\] \[= \sum_{\mathbf{x}\in\mathcal{X}}\mathbb{1}\left\{y_{*}(\mathbf{x})= k\right\}\sum_{\mathbf{x}^{\prime}\in\mathcal{X}}w_{\mathbf{x}\mathbf{x}^{\prime}} \cdot\mathbb{1}\left\{y_{*}(\mathbf{x})\neq y_{*}(\mathbf{x}^{\prime})\right\}.\]

Meanwhile, since \(\sum_{\mathbf{x}\in\mathcal{X}}\sum_{\mathbf{x}^{\prime}\in\mathcal{X}}w_{ \mathbf{x}\mathbf{x}^{\prime}}=1\), we have \(\alpha=\frac{1}{2}\sum_{k\in[K]}\sum_{\mathbf{x}\in\mathcal{X}_{k}}\sum_{\mathbf{x} ^{\prime}\in\mathcal{X}_{k}}w_{\mathbf{x}\mathbf{x}^{\prime}}\) and

\[\sum_{k\in[K]}\mathbf{y}_{k}^{\top}\mathbf{L}\left(\mathcal{X} \right)\mathbf{y}_{k}= \sum_{\mathbf{x}\in\mathcal{X}}\sum_{\mathbf{x}^{\prime}\in\mathcal{X}}w_{ \mathbf{x}\mathbf{x}^{\prime}}\cdot\mathbb{1}\left\{y_{*}(\mathbf{x})\neq y_{*}( \mathbf{x}^{\prime})\right\}\sum_{k\in[K]}\mathbb{1}\left\{y_{*}(\mathbf{x})=k\right\}\] \[= \sum_{\mathbf{x}\in\mathcal{X}}\sum_{\mathbf{x}^{\prime}\in\mathcal{X }}w_{\mathbf{x}\mathbf{x}^{\prime}}\cdot\mathbb{1}\left\{y_{*}(\mathbf{x})\neq y_{*}( \mathbf{x}^{\prime})\right\}\] \[= \frac{1}{2}\sum_{k\in[K]}\sum_{\mathbf{x}\in\mathcal{X}_{k}}\sum_{ \mathbf{x}^{\prime}\notin\mathcal{X}_{k}}w_{\mathbf{x}\mathbf{x}^{\prime}}=\alpha.\]

**Claim C.3**.: _For all \(f\in\mathcal{F}_{\mathbf{L}(\mathcal{X})}\) minimizing Equation (1), \(\operatorname{Range}\left(\mathbf{D}(\mathcal{X})^{\frac{1}{2}}f\left(\mathcal{ X}\right)\right)=\operatorname{span}\left\{\mathbf{v}_{1},\ldots,\mathbf{v}_{K}\right\}\)._

Proof of Claim C.3.: Since \(0\ll\mathbf{L}(\mathcal{X})\preccurlyeq 2\mathbf{I}\), we have \(-\mathbf{I}\preccurlyeq\overline{\mathbf{W}}(\mathcal{X})\prec\mathbf{I}\). Meanwhile, let be the population kernel matrix such that \(k_{\psi}\left(\mathcal{X},\mathcal{X}\right)_{\mathbf{x}\mathbf{x}^{\prime}}=k _{\psi}\left(\mathbf{x},\mathbf{x}^{\prime}\right)\). Then by definition, \(k_{\psi}\left(\mathcal{X},\mathcal{X}\right)=\mathbf{D}(\mathcal{X})^{-1} \mathbf{W}(\mathcal{X})\mathbf{D}(\mathcal{X})^{-1}\), and therefore

\[\mathbf{I}\succcurlyeq\overline{\mathbf{W}}(\mathcal{X})=\mathbf{D}(\mathcal{ X})^{-\frac{1}{2}}\mathbf{W}(\mathcal{X})\mathbf{D}(\mathcal{X})^{-\frac{1}{2}}= \mathbf{D}(\mathcal{X})^{\frac{1}{2}}k_{\psi}\left(\mathcal{X},\mathcal{X} \right)\mathbf{D}(\mathcal{X})^{\frac{1}{2}}\succcurlyeq 0.\]

That is, the eigenvalues of \(\mathbf{L}(\mathcal{X})\) and those of \(\overline{\mathbf{W}}(\mathcal{X})\) satisfies

\[\lambda_{i}\triangleq\lambda_{i}\left(\mathbf{L}(\mathcal{X})\right)=1- \lambda_{|\mathcal{X}|-i+1}\left(\overline{\mathbf{W}}(\mathcal{X})\right) \quad\forall\ i\in[|\mathcal{X}|],\]

while the eigenvectors are shared such that, with the spectral decomposition of \(\mathbf{L}(\mathcal{X})\) in Equation (13),

\[\overline{\mathbf{W}}(\mathcal{X})=\sum_{i=1}^{|\mathcal{X}|}\left(1-\lambda_ {i}\right)\cdot\mathbf{v}_{i}\mathbf{v}_{i}^{\top}.\]

Therefore, the \(K\) eigenvalues of \(\overline{\mathbf{W}}(\mathcal{X})\) of maximal modulus correspond exactly to the \(K\) eigenvalues of \(\mathbf{L}(\mathcal{X})\) that are closest to \(0\), the eigenvalues whose eigenspaces are relevant for spectral clustering.

Recalling \(\mathbf{F}_{\mathcal{X}}\triangleq\mathbf{D}\left(\mathcal{X}\right)^{\frac{1 }{2}}f\left(\mathcal{X}\right)\), we can rewrite Equation (1) as

\[R\left(f\right)=\left\|\overline{\mathbf{W}}(\mathcal{X})-\mathbf{F}_{ \mathcal{X}}\mathbf{F}_{\mathcal{X}}^{\top}\right\|_{F}^{2}.\]

It follows directly from the Eckart-Young-Mirsky theorem [1] that \(\operatorname{Range}\left(\mathcal{F}_{\mathcal{X}}\right)\) is the subspace spanned by the eigenvectors \(\mathbf{V}_{K}=\left[\mathbf{v}_{1},\ldots,\mathbf{v}_{K}\right]\in\mathbb{R} ^{|\mathcal{X}|\times K}\) associated with the maximum \(K\) eigenvalues of \(\overline{\mathbf{W}}(\mathcal{X})\):

\[\mathcal{F}_{\mathbf{L}(\mathcal{X})}=\left\{f\in\mathcal{F}\middle|\exists \,\mathbf{Z}\in\mathbb{R}^{K\times K}\text{ s.t.}\,\mathbf{D}\left(\mathcal{X} \right)^{\frac{1}{2}}f\left(\mathcal{X}\right)=\mathbf{V}_{K}\mathbf{Z} \right\}.\]

Proof of Theorem 4.1.: Recall \(\mathbf{F}_{\mathcal{X}}\triangleq\mathbf{D}\left(\mathcal{X}\right)^{1/2}f \left(\mathcal{X}\right)\), \(\mathbf{Y}_{\mathcal{X}}\triangleq\mathbf{D}\left(\mathcal{X}\right)^{1/2} \overrightarrow{y_{*}}(\mathcal{X})\), and let \(\mathbf{y}_{k}\in[0,1]^{|\mathcal{X}|}\) be the \(k\)-th column of \(\mathbf{Y}_{\mathcal{X}}\). Leveraging Lemma C.1, we have for any \(f\in\mathcal{F}\),

\[P\left(M(f)\right)\leq 2\max\left(\frac{\beta^{2}}{\gamma^{2}},1\right)\min_{\mathbf{z }_{\mathbf{z}}\in\mathbb{R}^{K\times K}}\left\|\mathbf{Y}_{\mathcal{X}}- \mathbf{F}_{\mathcal{X}}\mathbf{Z}\right\|_{F}^{2}\] \[= 2\max\left(\frac{\beta^{2}}{\gamma^{2}},1\right)\min_{\left\{ \mathbf{z}_{k}\in\mathbb{R}^{K}\right\}_{k\in[K]}}\sum_{k\in[K]}\left\| \mathbf{y}_{k}-\mathbf{F}_{\mathcal{X}}\mathbf{z}_{k}\right\|_{2}^{2}.\]

Recall that the orthonormal eigenvectors \(\left\{\mathbf{v}_{i}\in\mathbb{R}^{|\mathcal{X}|}\right\}_{i\in[|\mathcal{X} |]}\) associated with eigenvalues \(0=\lambda_{1}\leq\cdots\leq\lambda_{|\mathcal{X}|}\) of \(\mathbf{L}\left(\mathcal{X}\right)\) form a basis of \(\mathbb{R}^{|\mathcal{X}|}\). For every \(k\in[K]\), \(\left(\mathbf{y}_{k}-\mathbf{F}_{\mathcal{X}}\mathbf{z}_{k}\right)\in\mathbb{R} ^{|\mathcal{X}|}\) can be decomposed as the linear combination of \(\left\{\mathbf{v}_{i}\right\}_{i\in[|\mathcal{X}|]}\) such that

\[\left\|\mathbf{y}_{k}-\mathbf{F}_{\mathcal{X}}\mathbf{z}_{k}\right\|_{2}^{2}= \sum_{i=1}^{|\mathcal{X}|}\left(\mathbf{v}_{i}^{\top}\left(\mathbf{y}_{k}- \mathbf{F}_{\mathcal{X}}\mathbf{z}_{k}\right)\right)^{2}\]

Meanwhile, Claim C.3 implies that, for \(f\in\mathcal{F}_{\mathbf{L}(\mathcal{X})}\), with \(\mathbf{V}_{K}=\left[\mathbf{v}_{1},\ldots,\mathbf{v}_{K}\right]\in\mathbb{R} ^{|\mathcal{X}|\times K}\), we have \(\operatorname{Range}\left(\mathbf{F}_{\mathcal{X}}\right)=\operatorname{Range} \left(\mathbf{V}_{K}\right)\). That is, \(\mathbf{v}_{i}^{\top}\mathbf{F}_{\mathcal{X}}\mathbf{z}_{k}=0\) for all \(i>K\), and by taking

\[\mathbf{z}_{k}=\left(\mathbf{V}_{K}^{\top}\mathbf{F}_{\mathcal{X}}\right)^{-1} \mathbf{V}_{K}^{\top}\mathbf{y}_{k}\quad\Rightarrow\quad\sum_{i=1}^{K}\left( \mathbf{v}_{i}^{\top}\left(\mathbf{y}_{k}-\mathbf{F}_{\mathcal{X}}\mathbf{z}_{k} \right)\right)^{2}=0.\]

Overall, for every \(k\in[K]\), there exists \(\mathbf{z}_{k}\in\mathbb{R}^{K}\) such that

\[\left\|\mathbf{y}_{k}-\mathbf{F}_{\mathcal{X}}\mathbf{z}_{k}\right\|_{2}^{2}= \sum_{i=K+1}^{|\mathcal{X}|}\left(\mathbf{v}_{i}^{\top}\mathbf{y}_{k}\right)^{2} \leq\frac{1}{\lambda_{K+1}}\sum_{i=K+1}^{|\mathcal{X}|}\lambda_{i}\cdot\left( \mathbf{v}_{i}^{\top}\mathbf{y}_{k}\right)^{2}\leq\frac{\mathbf{y}_{k}^{\top} \mathbf{L}\left(\mathcal{X}\right)\mathbf{y}_{k}}{\lambda_{K+1}},\]and Lemma C.2 implies that

\[\min_{\left\{\mathbf{z}_{k}\in\mathbb{R}^{K}\right\}_{k\in[K]}}\sum_{k\in[K]} \|\mathbf{y}_{k}-\mathbf{F}_{\mathcal{X}}\mathbf{z}_{k}\|_{2}^{2}\leq\sum_{k \in[K]}\frac{\mathbf{y}_{k}^{\top}\mathbf{L}\left(\mathcal{X}\right)\mathbf{y} _{k}}{\lambda_{K+1}}=\frac{\alpha}{\lambda_{K+1}},\]

which completes the proof. 

### Limitation of Spectral Clustering Alone

To ground the discussion in Remark 4.1, here, we raise a toy counter-example where spectral clustering alone fails to satisfy Assumption 4.2 and suffers from a large clustering error. Intuitively, such a pitfall of spectral clustering is caused by a suboptimal choice of basis for the predictions \(f(\mathcal{X})\) in Equation (1) due to the inherited rotation invariance of the minimizers of \(R(f)\).

**Example C.1** (Pitfall of spectral clustering alone).: _We consider a binary classification problem with two balanced ground truth classes \(\left\{\mathcal{X}_{1},\mathcal{X}_{2}\right\}\) perfectly partitioned in \(G_{\mathcal{X}}\) as disconnected components. Then, when the prediction \(y_{f}\left(\mathbf{x}\right)\triangleq\operatorname*{argmax}_{k\in[K]}f\left( \mathbf{x}\right)_{k}\) is made with a uniformly random break of ties, Equation (1) alone suffers from \(\mu\left(\mathcal{F}_{\mathbf{L}\left(\mathcal{X}\right)}\right)\geq\frac{1} {4}\) in expectation, due to the violation of Assumption 4.2._

_Nevertheless, with as few as one labeled sample per class \(\left\{\left(\mathbf{x}_{i},i\right)\mid i=1,2\right\}\), weak supervision via the cross-entropy loss_

\[\min_{f\in\mathcal{F}_{\mathbf{L}\left(\mathcal{X}\right)}}\left\{\widehat{ \mathcal{L}}_{\mathrm{CE}}\left(f\right)\triangleq-\sum_{i=1}^{2}\log\left( \mathtt{softmax}\left(f\left(\mathbf{x}_{i}\right)\right)_{i}\right)\right\}\]

_facilitates satisfaction of Assumption 4.2 with a provably large margin \(\gamma\). For instance, assuming \(\left\|f(\mathbf{x})\right\|_{2}\leq\left\|f(\mathbf{s})\right\|_{2}\leq\beta\) and \(w_{\mathbf{x}}=1/|\mathcal{X}|\) for all \(\mathbf{x}\in\mathcal{X}\) (simplified for illustration purposes), for any \(f\in\mathcal{F}_{\mathbf{L}\left(\mathcal{X}\right)}\) under weak supervision such that_

\[f\in\left\{f\in\mathcal{F}_{\mathbf{L}\left(\mathcal{X}\right)}\;\middle|\; \log\left(1+e^{-\sqrt{2}\beta}\right)\leq\widehat{\mathcal{L}}_{\mathrm{CE}} (f)<\log\left(1+e^{-\beta}\right)\right\},\]

_Assumption 4.2 is satisfied with_

\[\gamma\geq-\log\left(e^{\widehat{\mathcal{L}}_{\mathrm{CE}}(f)}-1\right)- \sqrt{2\beta^{2}-\log\left(e^{\widehat{\mathcal{L}}_{\mathrm{CE}}(f)}-1\right) ^{2}}.\]

_In particular, with the minimum feasible cross-entropy loss \(\widehat{\mathcal{L}}_{\mathrm{CE}}(f)=\log\left(1+e^{-\sqrt{2}\beta}\right)\), we have \(\gamma\geq\sqrt{2}\beta\)._

_We remark that with DAC regularizations like FixMatch (Sohn et al., 2020) and MixMatch (Berthelot et al., 2019), model predictions are matched against pseudo-labels that are gauged based on data augmentations, usually via the cross-entropy loss. Therefore, DAC has analogous effects on the margins of the learned prediction functions \(f\) as the additional supervision described above._

_Moreover, when coupling Equation (1) with the conventional KD (i.e., feature matching), the learned prediction functions \(f\) inherit the boundedness and margin of the teacher model, which are presumably satisfactory._

Rationale for Example C.1.: Recall that \(\lambda_{1}\leq\cdots\leq\lambda_{|\mathcal{X}|}\) are the eigenvalues of the graph Laplacian \(\mathbf{L}\left(\mathcal{X}\right)=\mathbf{I}-\overline{\mathbf{W}}\left( \mathcal{X}\right)\), and let \(\left\{\mathbf{v}_{1},\cdots,\mathbf{v}_{|\mathcal{X}|}\right\}\) be the associated normalized eigenvectors. For a binary classification with \(\left\{\mathcal{X}_{1},\mathcal{X}_{2}\right\}\) perfectly separated in \(G_{\mathcal{X}}\), we can write \(\lambda_{1}=\lambda_{2}=0\), while \(\mathbf{v}_{1},\mathbf{v}_{2}\) are the scaled identity vectors of \(\mathcal{X}_{1},\mathcal{X}_{2}\), respectively, such that \(\mathbf{v}_{i}(\mathbf{x})=\frac{1}{\sqrt{|\mathcal{X}_{i}|}}\mathbf{1}\left\{ \mathbf{x}\in\mathcal{X}_{i}\right\}\) for \(i=1,2\).

By denoting \(\mathbf{V}_{2}=\left[\mathbf{v}_{1},\mathbf{v}_{2}\right]\in\mathbb{R}^{| \mathcal{X}|\times 2}\), the Eckart-Young-Mirsky theorem (Eckart and Young, 1936) implies that, for any orthogonal matrix \(\mathbf{Q}=\left[\mathbf{q}_{1};\mathbf{q}_{2}\right]\in\mathbb{R}^{2\times 2}\),

\[f\left(\mathcal{X}\right)=\mathbf{D}\left(\mathcal{X}\right)^{-\frac{1}{2}} \mathbf{V}_{2}\operatorname*{diag}\left(1-\lambda_{1},1-\lambda_{2}\right)^{ \frac{1}{2}}\mathbf{Q}=\mathbf{D}\left(\mathcal{X}\right)^{-\frac{1}{2}} \mathbf{V}_{2}\mathbf{Q}\]

is a minimizer of \(R(f)\) in Equation (1). That is, \(f(\mathbf{x})=\frac{1}{\sqrt{w_{\mathbf{x}}|\mathcal{X}_{i}|}}\mathbf{q}_{i}\) for \(\mathbf{x}\in\mathcal{X}_{i}\), \(i=1,2\).

Spectral clustering alone.However, by taking \(\mathbf{q}_{1}=\frac{1}{\sqrt{2}}\left[1;1\right]\) and \(\mathbf{q}_{2}=\frac{1}{\sqrt{2}}\left[-1;1\right]\), with a uniformly random break of ties, the corresponding \(f\in\mathcal{F}_{\mathbf{L}(\mathcal{X})}\) will misclassify half of \(\mathbf{x}\in\mathcal{X}_{1}\) as \(\mathcal{X}_{2}\) in expectation. Then, since \(P\left(\mathcal{X}_{1}\right)=P\left(\mathcal{X}_{2}\right)=\frac{1}{2}\), by Definition 3.1, we have \(M(f)=\left\{\mathbf{x}\in\mathcal{X}_{1}\mid y_{f}(\mathbf{x})=2\right\}\) such that \(\mu\left(\mathcal{F}_{\mathbf{L}(\mathcal{X})}\right)\geq P\left(M(f)\right)= \frac{1}{4}\) in expectation.

Spectral clustering in Example C.1 fails to yield low clustering error due to the potential violation of Assumption 4.2. For example, consider any prediction function \(f\in\mathcal{F}_{\mathbf{L}(\mathcal{X})}\) described in Example C.1.

1. When \(\operatorname*{argmin}_{\mathbf{x}\in\mathcal{X}\setminus M(f)}w_{\mathbf{x}} \subseteq\mathcal{X}_{1}\), recalling \(\mathbf{s}_{k}=\operatorname*{argmax}_{\mathbf{x}\in\mathcal{X}\setminus M(f )}f(\mathbf{x})_{k}\) and \(f(\mathbf{x})=\frac{1}{\sqrt{w_{\mathbf{x}}\left|\mathcal{X}_{i}\right|}} \mathbf{q}_{i}\) for \(\mathbf{x}\in\mathcal{X}_{i}\) such that, since \(\left|\mathcal{X}_{1}\right|=\left|\mathcal{X}_{2}\right|\), \[\mathbf{s}_{1}\in\mathcal{X}_{1},\qquad\mathbf{s}_{2}\in\operatorname*{argmax}_{ \mathbf{x}\in\mathcal{X}\setminus M(f)}f(\mathbf{x})_{2}=\operatorname*{argmax }_{\mathbf{x}\in\mathcal{X}\setminus M(f)}\frac{1}{\sqrt{w_{\mathbf{x}}\left| \mathcal{X}_{i}\right|}}=\operatorname*{argmin}_{\mathbf{x}\in\mathcal{X} \setminus M(f)}w_{\mathbf{x}}\subseteq\mathcal{X}_{1},\] there does not exist a skeleton subset \(\mathbf{S}=\left[\mathbf{s}_{1};\mathbf{s}_{2}\right]\) as described in Assumption 4.2.
2. Otherwise, suppose one can find a skeleton subset \(\mathbf{S}=\left[\mathbf{s}_{1};\mathbf{s}_{2}\right]\) as described in Assumption 4.2. Without loss of generality, by taking \(\left(\operatorname*{argmin}_{\mathbf{x}\in\mathcal{X}}w_{\mathbf{x}}\right) \cap M(f)\neq\emptyset\) in the problem setup, there exists \(\mathbf{x}\in M(f)\subset\mathcal{X}_{1}\) (with \(y_{f}(\mathbf{x})=2\)) such that \(w_{\mathbf{x}}\leq w_{\mathbf{s}_{1}}\). Therefore, \[\gamma_{1}\triangleq f\left(\mathbf{s}_{1}\right)_{1}-\max_{\mathbf{x}\in M( f):y_{f}(\mathbf{x})\neq 1}f\left(\mathbf{x}\right)_{1}\leq\frac{1}{ \sqrt{2w_{\mathbf{s}_{1}}\left|\mathcal{X}_{1}\right|}}-\frac{1}{\sqrt{2w_{ \mathbf{x}}\left|\mathcal{X}_{1}\right|}}\leq 0\] implies a trivial margin \(\gamma\leq 0\), which violates \(\gamma>0\) in Assumption 4.2.

With additional weak supervision.We first observe that for both \(i\in[2]\),

\[-\log\left(\operatorname*{\mathtt{softmax}}\left(f\left(\mathbf{x}_{i}\right) \right)_{i}\right)\leq e^{\widehat{\mathcal{L}}_{\mathrm{CE}}(f)},\]

which implies that

\[f\left(\mathbf{x}_{1}\right)_{1}-f\left(\mathbf{x}_{1}\right)_{2}\geq-\log \left(e^{\widehat{\mathcal{L}}_{\mathrm{CE}}(f)}-1\right),\quad f\left( \mathbf{x}_{2}\right)_{2}-f\left(\mathbf{x}_{2}\right)_{1}\geq-\log\left(e^{ \widehat{\mathcal{L}}_{\mathrm{CE}}(f)}-1\right)\]

where \(-\log\left(e^{\widehat{\mathcal{L}}_{\mathrm{CE}}(f)}-1\right)>0\) since \(\widehat{\mathcal{L}}_{\mathrm{CE}}(f)<\log\left(1+e^{-\beta}\right)<\log(2)\).

Since we assume \(\left\|f(\mathbf{x})\right\|_{2}\leq\beta\) for all \(\mathbf{x}\in\mathcal{X}\), \(f\left(\mathbf{x}_{i}\right)_{1}^{2}+f\left(\mathbf{x}_{i}\right)_{2}^{2}\leq \beta^{2}\) for both \(i=1,2\), we therefore have

\[f\left(\mathbf{x}_{1}\right)_{1}-f\left(\mathbf{x}_{2}\right)_{1}\geq-\log \left(e^{\widehat{\mathcal{L}}_{\mathrm{CE}}(f)}-1\right)-\sqrt{2\beta^{2}- \log\left(e^{\widehat{\mathcal{L}}_{\mathrm{CE}}(f)}-1\right)^{2}},\]

This similarly holds for \(f\left(\mathbf{x}_{2}\right)_{2}-f\left(\mathbf{x}_{1}\right)_{2}\).

Recall now that \(f(\mathbf{x})=\frac{1}{\sqrt{w_{\mathbf{x}}\left|\mathcal{X}_{i}\right|}} \mathbf{q}_{i}\) for \(\mathbf{x}\in\mathcal{X}_{i}\), \(i=1,2\) and \(\mathbf{x}_{i}\in\mathcal{X}_{i}\) for \(i=1,2\). Assuming \(w_{\mathbf{x}}=1/\left|\mathcal{X}\right|\) for all \(\mathbf{x}\in\mathcal{X}\), we have

\[f(\mathbf{x})=\begin{cases}\sqrt{\frac{w_{\mathbf{x}_{1}}}{w_{\mathbf{x}}}}f \left(\mathbf{x}_{1}\right)=f\left(\mathbf{x}_{1}\right),&\mathbf{x}\in \mathcal{X}_{1}\\ \sqrt{\frac{w_{\mathbf{x}_{2}}}{w_{\mathbf{x}}}}f\left(\mathbf{x}_{2}\right)=f \left(\mathbf{x}_{2}\right),&\mathbf{x}\in\mathcal{X}_{2}\end{cases}.\]

Therefore, \(\gamma_{1}\geq f\left(\mathbf{x}_{1}\right)_{1}-f\left(\mathbf{x}_{2}\right)_{1}\) and \(\gamma_{2}\geq f\left(\mathbf{x}_{2}\right)_{2}-f\left(\mathbf{x}_{1}\right)_{2}\), which together imply that

\[\gamma\geq-\log\left(e^{\widehat{\mathcal{L}}_{\mathrm{CE}}(f)}-1\right)-\sqrt{2 \beta^{2}-\log\left(e^{\widehat{\mathcal{L}}_{\mathrm{CE}}(f)}-1\right)^{2}}.\]

Now examine the two extremes of \(\log\left(1+e^{-\sqrt{2}\beta}\right)\leq\widehat{\mathcal{L}}_{\mathrm{CE}}(f)< \log\left(1+e^{-\beta}\right)\). When the cross-entropy loss achieves its minimum feasible value \(\widehat{\mathcal{L}}_{\mathrm{CE}}(f)=\log\left(1+e^{-\sqrt{2}\beta}\right)\), we have that \(\gamma\geq\sqrt{2}\beta\), whereas the margin can be nearly trivial \(\gamma\geq\epsilon_{\mathrm{CE}}\) with \(\epsilon_{\mathrm{CE}}\to 0\) when the cross-entropy loss is on the larger side \(\widehat{\mathcal{L}}_{\mathrm{CE}}(f)\to\log\left(1+e^{-\beta}\right)\).

### Connection with Sparsest \(k\)-partition

We now identify the connection between the eigenvalue \(\lambda_{K+1}\) and spectral graph theoretic notions of clusteredness of the underlying graph. We recall the notion of sparsest \(k\)-partitions (Louis and Makarychev, 2014) based on the Dirichlet conductance of subgraphs.

**Definition C.1** (Sparsest \(k\)-partition, HaoChen et al. (2021) Definition 3.4, Louis and Makarychev (2014) Problem 1.1).: _Given a weighted undirected graph \(G=(\mathcal{X},w)\), for any \(k\in[|\mathcal{X}|]\), let \(\mathcal{S}=\big{\{}\left\{S_{i}\right\}_{i\in[k]}\big{|}\cup_{i\in[k]}S_{i}= \mathcal{X},\;S_{i}\cap S_{j}=\emptyset,\;S_{i}\neq\emptyset\;\forall i\neq j, \;i,j\in[k]\big{\}}\) be the set of all \(k\)-partitions of \(G\). The sparsest \(k\)-partition of \(G\) is defined as_

\[\phi_{k}\triangleq\min_{\left\{S_{i}\right\}_{i\in[k]}}\max_{\mathcal{S}\in[ k]}\phi_{G}(S_{i}),\]

_where \(\phi_{G}(S)\triangleq\left(\sum_{\mathbf{x}\in S}\sum_{\mathbf{x}^{\prime} \notin S}w_{\mathbf{x}\mathbf{x}^{\prime}}\right)/\left(\sum_{\mathbf{x}\in S }w_{\mathbf{x}\mathbf{x}}\right)\) is the Dirichlet conductance of \(S\subseteq\mathcal{X}\)._

It is worth mentioning that \(\phi_{k}\) is a non-decreasing function in \(k\)(HaoChen et al., 2021). In particular, \(\phi_{k}=0\) when the graph has at least \(k\) disconnected components; whereas \(\lambda_{K+1}>0\) (Assumption 4.1) implies that \(\phi_{K+1}>0\).

Meanwhile, Louis and Makarychev (2014) unveils the following connection between the sparsest \(k\)-partition of the graph and the \(k^{\prime}\)th smallest eigenvalue of its graph Laplacian:

**Lemma C.4** (Louis and Makarychev (2014) Proposition 1.2).: _Given any weighted undirected graph \(G=(\mathcal{X},w)\), let \(0=\lambda_{1}\leq\cdots\leq\lambda_{|\mathcal{X}|}\) be the eigenvalues of the normalized graph Laplacian in the ascending order. For any \(k,k^{\prime}\in[|\mathcal{X}|]\), \(k<k^{\prime}\), there exists a partition \(\left\{S_{i}\right\}_{i\in[k]}\in\mathcal{S}\) of \(\mathcal{X}\) such that, for all \(i\in[k]\),_

\[\phi_{G}\left(S_{i}\right)\lesssim\mathrm{poly}\left(\frac{k}{k^{\prime}-k} \right)\sqrt{\log\left(k\right)\lambda_{k^{\prime}}}.\]

Leveraging the existing result Lemma C.4(Louis and Makarychev, 2014) from spectral graph theory, we have the following corollary.

**Corollary C.5** (Clustering in terms of sparsest \(k\)-partitions).: _Under Assumption 4.1 and Assumption 4.2 for every \(f\in\mathcal{F}_{\mathbf{L}(\mathcal{X})}\), error of clustering with the population (Equation (1)) satisfies that, if \(\alpha>0\) and \(\phi_{k}>0\) for all \(k\in[K]\), then_

\[\mu\left(\mathcal{F}_{\mathbf{L}(\mathcal{X})}\right)\lesssim\max\left(\frac{ \beta^{2}}{\gamma^{2}},1\right)\cdot\mathrm{poly}\left(\frac{k}{K+1-k}\right) \log(k)\frac{\alpha}{\phi_{k}^{2}}.\]

We also notice that when \(\alpha=0\), Theorem 4.1 automatically implies that \(\mu\left(\mathcal{F}_{\mathbf{L}(\mathcal{X})}\right)=0\). Intuitively, Corollary C.5 implies that except for the partition of the \(K\) ground truth classes \(\left\{\mathcal{X}\right\}_{k\in[K]}\) (_i.e._, when \(\alpha>0\)), \(G_{\mathcal{X}}\) cannot be partitioned into other \(K\) components by removing a sparse set of edges from \(G_{\mathcal{X}}\).

Proof of Corollary c.5.: Lemma C.4 implies that for any \(k\in[K]\), there exists a partition \(\left\{S_{i}\right\}_{i\in[k]}\in\mathcal{S}\) of \(\mathcal{X}\) such that

\[\phi_{k}\leq\max_{i\in[k]}\phi_{G}\left(S_{i}\right)\lesssim\mathrm{poly} \left(\frac{k}{K+1-k}\right)\sqrt{\log\left(k\right)\lambda_{K+1}},\]

and therefore

\[\frac{1}{\lambda_{K+1}}\lesssim\mathrm{poly}\left(\frac{k}{K+1-k}\right)\log( k)\frac{1}{\phi_{k}^{2}},\]

which completes the proof when recalling Theorem 4.1. 

## Appendix D Proofs and Discussions for Section 4.2

Here, we show that \(\widehat{R}_{\mathbf{X}^{u}}(f)\) shares the same minimizer as the population Laplacian regularizer \(R(f)\) in expectation (Proposition D.1) and provide generalization bounds for \(\widehat{R}_{\mathbf{X}^{u}}(f)\) (Theorem 4.2) accordingly.

### Unbiased Estimations of Population Laplacian

**Proposition D.1**.: \(\widehat{R}_{\mathbf{X}^{u}}(f)\) _(Equation (3)) serves as an unbiased estimate for \(R(f)\) (Equation (1)):_

\[R\left(f\right)=\mathbb{E}_{\mathbf{X}^{u}\sim P\left(\mathbf{x}\right)^{N}} \left[\widehat{R}_{\mathbf{X}^{u}}\left(f\right)\right].\]

Proof of Proposition D.1.: We first observe that \(R(f)\) can be expanded as following:

\[R\left(f\right)\triangleq \left\|\overline{\mathbf{W}}(\mathcal{X})-\mathbf{D}(\mathcal{X}) ^{\frac{1}{2}}f\left(\mathcal{X}\right)f\left(\mathcal{X}\right)^{\top} \mathbf{D}(\mathcal{X})^{\frac{1}{2}}\right\|_{F}^{2}\] \[= \left\|\mathbf{D}(\mathcal{X})^{-\frac{1}{2}}\mathbf{W}(\mathcal{ X})\mathbf{D}(\mathcal{X})^{-\frac{1}{2}}\right\|_{F}^{2}+\left\|\mathbf{D}( \mathcal{X})^{\frac{1}{2}}f\left(\mathcal{X}\right)f\left(\mathcal{X}\right)^{ \top}\mathbf{D}(\mathcal{X})^{\frac{1}{2}}\right\|_{F}^{2}\] \[-2\operatorname{tr}\left(\mathbf{W}(\mathcal{X})f\left(\mathcal{ X}\right)f\left(\mathcal{X}\right)^{\top}\right)\] \[= \sum_{\mathbf{x}\in\mathcal{X}}\sum_{\mathbf{x}^{\prime}\in \mathcal{X}}\frac{w_{\mathbf{x}\mathbf{x}^{\prime}}^{2}}{w_{\mathbf{x}}w_{ \mathbf{x}^{\prime}}}-2w_{\mathbf{x}\mathbf{x}^{\prime}}f(\mathbf{x})^{\top}f( \mathbf{x}^{\prime})+w_{\mathbf{x}}w_{\mathbf{x}^{\prime}}\left(f(\mathbf{x}) ^{\top}f(\mathbf{x}^{\prime})\right)^{2}\] \[= \sum_{\mathbf{x}\in\mathcal{X}}\sum_{\mathbf{x}^{\prime}\in \mathcal{X}}w_{\mathbf{x}}w_{\mathbf{x}^{\prime}}\cdot\left(\left(\frac{w_{ \mathbf{x}\mathbf{x}^{\prime}}}{w_{\mathbf{x}}w_{\mathbf{x}^{\prime}}}\right)^ {2}-\frac{2w_{\mathbf{x}\mathbf{x}^{\prime}}}{w_{\mathbf{x}}w_{\mathbf{x}^{ \prime}}}f(\mathbf{x})^{\top}f(\mathbf{x}^{\prime})+\left(f(\mathbf{x})^{\top }f(\mathbf{x}^{\prime})\right)^{2}\right).\]

Then, with \(k_{\psi}\left(\mathbf{x},\mathbf{x}^{\prime}\right)=\frac{w_{\mathbf{x}\mathbf{ x}^{\prime}}}{w_{\mathbf{x}}w_{\mathbf{x}^{\prime}}}\), \(\widehat{R}_{\mathbf{X}^{u}}\left(f\right)\) in Equation (3), \(w_{\mathbf{x}}=P\left(\mathbf{x}\right)\), and \(w_{\mathbf{x}^{\prime}}=P\left(\mathbf{x}^{\prime}\right)\), we have

\[R\left(f\right)= \mathbb{E}_{\mathbf{x},\mathbf{x}^{\prime}\sim P\left(\mathbf{ x}\right)^{2}}\left[k_{\psi}\left(\mathbf{x},\mathbf{x}^{\prime}\right)^{2}-2k_{ \psi}\left(\mathbf{x},\mathbf{x}^{\prime}\right)f(\mathbf{x})^{\top}f( \mathbf{x}^{\prime})+\left(f(\mathbf{x})^{\top}f(\mathbf{x}^{\prime})\right) ^{2}\right]\] \[= \mathbb{E}_{\mathbf{x},\mathbf{x}^{\prime}\sim P\left(\mathbf{x} \right)^{2}}\left[\left(f(\mathbf{x})^{\top}f(\mathbf{x}^{\prime})-k_{\psi} \left(\mathbf{x},\mathbf{x}^{\prime}\right)\right)^{2}\right]\] \[= \mathbb{E}_{\mathbf{X}^{u}\sim P\left(\mathbf{x}\right)^{N}}\left[ \widehat{R}_{\mathbf{X}^{u}}\left(f\right)\right].\]

### Proof of Theorem 4.2

Proof of Theorem 4.2.: By defining

\[\widehat{r}_{\mathbf{x},\mathbf{x}^{\prime}}(f)\triangleq\left(f(\mathbf{x})^ {\top}f(\mathbf{x}^{\prime})-k_{\psi}\left(\mathbf{x},\mathbf{x}^{\prime} \right)\right)^{2},\]

we have \(R(f)=\mathbb{E}_{\mathbf{x},\mathbf{x}^{\prime}\sim P\left(\mathbf{x}\right)^{2 }}\left[\widehat{r}_{\mathbf{x},\mathbf{x}^{\prime}}(f)\right]\) and \(\widehat{R}_{\mathbf{X}^{u}}\left(f\right)=\frac{2}{N}\sum_{i=1}^{N/2}\widehat {r}_{\mathbf{x}_{2i-1}^{*},\mathbf{x}_{2i}^{*}}(f)\) as

\[\widehat{R}_{\mathbf{X}^{u}}\left(f\right)= \frac{2}{N}\sum_{i=1}^{N/2}\left(f\left(\mathbf{x}_{2i-1}^{u} \right)^{\top}f\left(\mathbf{x}_{2i}^{u}\right)-k_{\psi}\left(\mathbf{x}_{2i-1}^ {u},\mathbf{x}_{2i}^{u}\right)\right)^{2},\]

where \(\left\{\left(\mathbf{x}_{2i-1}^{u},\mathbf{x}_{2i}^{u}\right)\right\}_{i=1}^{N/2}\) are _i.i.d._ given \(\mathbf{X}^{u}\sim P(\mathbf{x})^{N}\). Consider the function class

\[\widehat{r}_{\cdot,\cdot}\circ\mathcal{F}=\left\{\widehat{r}_{\cdot,\cdot}(f): \mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}|f\in\mathcal{F}\right\}.\]

Since \(0\leq k_{\psi}(\mathbf{x},\mathbf{x}^{\prime})\leq B_{k_{\psi}}\), and by Cauchy-Schwarz inequality, \(f(\mathbf{x})^{\top}f(\mathbf{x}^{\prime})\leq\left\|f(\mathbf{x})\right\|_{2} \left\|f(\mathbf{x}^{\prime})\right\|_{2}\leq B_{f}\), we have \(0\leq\widehat{r}_{\mathbf{x},\mathbf{x}^{\prime}}(f)\leq\left(B_{k_{\psi}}+B_{f }\right)^{2}\) for all \(\mathbf{x},\mathbf{x}^{\prime}\in\mathcal{X}\), which means that \(\widehat{r}_{\cdot,\cdot}\circ\mathcal{F}\) is \(\left(B_{k_{\psi}}+B_{f}\right)^{2}\)-bounded. Leveraging Lemma F.1 gives that, with probability at least \(1-\delta/2\) over \(\mathbf{X}^{u}\),

\[R\left(f_{|\mathbf{X}^{u}}\right)-R\left(f_{|\mathcal{X}}\right)\leq 4 \mathfrak{R}_{N/2}\left(\widehat{r}_{\cdot,\cdot}\circ\mathcal{F}\right)+2 \left(B_{k_{\psi}}+B_{f}\right)^{2}\sqrt{\frac{\log\left(4/\delta\right)}{N}}.\]Now, it remains to show that \(\mathfrak{R}_{N/2}\left(\widehat{r}_{\cdot,\cdot}\circ\mathcal{F}\right)\leq 4 \sqrt{2B_{f}}\left(B_{f}+B_{k_{\psi}}\right)\mathfrak{R}_{N/2}\left(\mathcal{F}\right)\). For this, we recall Equation (4) and observe that

\[\mathfrak{R}_{N/2}\left(\widehat{r}_{\cdot,\cdot}\circ\mathcal{F}\right)= \mathbb{E}_{\begin{subarray}{c}\mathbf{X}^{u}\sim P(\mathbf{x}_{2i})^{N} \\ \boldsymbol{\rho}\sim\mathrm{Rad}^{\frac{N}{2}}\end{subarray}}\left[\sup_{f\in \mathcal{F}}\frac{2}{N}\sum_{i=1}^{N/2}\rho_{i}\cdot\left(f\left(\mathbf{x}_{2i -1}^{u}\right)^{\top}f\left(\mathbf{x}_{2i}^{u}\right)-k_{\psi}\left(\mathbf{x }_{2i-1}^{u},\mathbf{x}_{2i}^{u}\right)\right)^{2}\right],\]

where with Talagrand's lemma (Ledoux and Talagrand, 2013, Theorem 4.12) for compositions with scalar Lipschitz functions,

\[\mathfrak{R}_{N/2}\left(\widehat{r}_{\cdot,\cdot}\circ\mathcal{F}\right)\leq 2 \left(B_{f}+B_{k_{\psi}}\right)\mathbb{E}_{\mathbf{X}^{u},\boldsymbol{\rho}} \left[\sup_{f\in\mathcal{F}}\frac{2}{N}\sum_{i=1}^{N/2}\rho_{i}\cdot f\left( \mathbf{x}_{2i-1}^{u}\right)^{\top}f\left(\mathbf{x}_{2i}^{u}\right)\right].\]

Since \(f\left(\mathbf{x}_{2i-1}^{u}\right)^{\top}f\left(\mathbf{x}_{2i}^{u}\right) \leq\frac{1}{2}\left(\left\|f\left(\mathbf{x}_{2i-1}^{u}\right)\right\|_{2}^{2 }+\left\|f\left(\mathbf{x}_{2i}^{u}\right)\right\|_{2}^{2}\right)\),

\[\mathbb{E}_{\mathbf{X}^{u},\boldsymbol{\rho}}\left[\sup_{f\in \mathcal{F}}\frac{2}{N}\sum_{i=1}^{N/2}\rho_{i}\cdot f\left(\mathbf{x}_{2i-1}^ {u}\right)^{\top}f\left(\mathbf{x}_{2i}^{u}\right)\right]\] \[\leq \mathbb{E}_{\mathbf{X}^{u},\boldsymbol{\rho}}\left[\sup_{f\in \mathcal{F}}\frac{2}{N}\sum_{i=1}^{N/2}\rho_{i}\cdot\frac{1}{2}\left(\left\|f \left(\mathbf{x}_{2i-1}^{u}\right)\right\|_{2}^{2}+\left\|f\left(\mathbf{x}_{2 i}^{u}\right)\right\|_{2}^{2}\right)\right]\] \[\leq \frac{1}{2}\left(\mathbb{E}_{\mathbf{X}^{u},\boldsymbol{\rho}} \left[\sup_{f\in\mathcal{F}}\frac{2}{N}\sum_{i=1}^{N/2}\rho_{i}\cdot\left\|f \left(\mathbf{x}_{2i-1}^{u}\right)\right\|_{2}^{2}\right]+\mathbb{E}_{\mathbf{ X}^{u},\boldsymbol{\rho}}\left[\sup_{f\in\mathcal{F}}\frac{2}{N}\sum_{i=1}^{N/2} \rho_{i}\cdot\left\|f\left(\mathbf{x}_{2i}^{u}\right)\right\|_{2}^{2}\right]\right)\] \[= \mathbb{E}_{\mathbf{X}^{u},\boldsymbol{\rho}}\left[\sup_{f\in \mathcal{F}}\frac{2}{N}\sum_{i=1}^{N/2}\rho_{i}\cdot\left\|f\left(\mathbf{x}_ {2i}^{u}\right)\right\|_{2}^{2}\right].\]

By the vector-contraction inequality for Rademacher complexities (Maurer, 2016, Corollary 4), since \(\left\|f\left(\mathbf{x}\right)\right\|_{2}^{2}\) is \(\left(2\sqrt{B_{f}}\right)\)-Lipschitz in \(f(\mathbf{x})\), with \(f(\mathbf{x})_{k}\) denoting the \(k\)-th entry of \(f(\mathbf{x})\in\mathbb{R}^{K}\),

\[\mathbb{E}_{\mathbf{X}^{u},\boldsymbol{\rho}}\left[\sup_{f\in \mathcal{F}}\frac{2}{N}\sum_{i=1}^{N/2}\rho_{i}\cdot\left\|f\left(\mathbf{x}_{2 i}^{u}\right)\right\|_{2}^{2}\right]\leq 2\sqrt{2B_{f}}\cdot\mathbb{E}_{\begin{subarray}{c}\mathbf{X}^{u} \sim P(\mathbf{x})^{\frac{N}{2}}\\ \boldsymbol{\rho}\sim\mathrm{Rad}^{\frac{N}{2}}\times K\end{subarray}}\left[ \sup_{f\in\mathcal{F}}\frac{2}{N}\sum_{i=1}^{N/2}\sum_{k=1}^{K}\rho_{ik}\cdot f \left(\mathbf{x}_{i}^{u}\right)_{k}\right]\] \[= 2\sqrt{2B_{f}}\mathfrak{R}_{N/2}\left(\mathcal{F}\right).\]

Overall, we have \(\mathfrak{R}_{N/2}\left(\widehat{r}_{\cdot,\cdot}\circ\mathcal{F}\right)\leq 4 \sqrt{2B_{f}}\left(B_{f}+B_{k_{\psi}}\right)\mathfrak{R}_{N/2}\left(\mathcal{F}\right)\) which completes the proof. 

### Unlabeled Sample Complexity with Deep Neural Networks

Here, we ground Theorem 4.2 by exemplifying \(\mathfrak{R}_{N/2}(\mathcal{F})\) leveraging the existing generalization bound (Golowich et al., 2018) for deep neural networks.

**Claim D.2** (Rademacher complexity of deep neural networks (Golowich et al., 2018)).: _Recall the notion of Rademacher complexity for a vector-valued function class \(\mathcal{F}\ni f:\mathcal{X}\rightarrow\mathbb{R}^{K}\) from Equation (4). Adopting the existing results in Golowich et al. (2018) and following the notations in Section 6, we consider \(\mathcal{F}\) as a class of deep neural networks:_

\[\mathcal{F}=\left\{f(\mathbf{x})=\mathbf{A}_{p}\phi\left(\cdots\mathbf{A}_{2} \phi(\mathbf{A}_{1}\mathbf{x})\cdots\right)\mid\left\|\mathbf{A}_{i}\right\|_ {F}\leq B_{\mathcal{F},\iota}\;\forall\;\iota\in[p]\right\}\]

_with weight matrices \(\{\mathbf{A}_{i}\mid\iota\in[p]\}\) and a \(1\)-Lipschitz, positive-homogeneous activation function \(\phi\left(\cdot\right)\) (e.g., ReLU) applied entry-wisely. Let \(B_{\mathcal{F}}=\prod_{i=1}^{p}B_{\mathcal{F},\iota}\) and \(\left\|\mathbf{x}\right\|_{2}\leq B_{\mathcal{X}}\) for all \(\mathbf{x}\in\mathcal{X}\). Then, by Golowich et al. (2018) Theorem 1, we have_

\[\mathfrak{R}_{N/2}(\mathcal{F})\leq\frac{\left(2\sqrt{p\log(2)}+\sqrt{2}\right)KB_ {\mathcal{X}}B_{\mathcal{F}}}{\sqrt{N}}.\]

[MISSING_PAGE_FAIL:28]

Meanwhile, the assumption \(\Delta<\left(1-\lambda_{K}\right)^{2}\) implies that \(\mathrm{rank}\left(\mathbf{P}_{\mathbf{U}}^{\perp}\right)=\left|\mathcal{X} \right|-K\), and thus

\[\sum_{i=1}^{\left|\mathcal{X}\right|}\left\|\mathbf{P}_{\mathbf{U}}^{\perp} \mathbf{v}_{i}\right\|_{2}^{2}=\left\|\mathbf{P}_{\mathbf{U}}^{\perp}\right\|_ {F}^{2}=\mathrm{tr}\left(\mathbf{P}_{\mathbf{U}}^{\perp}\right)=\mathrm{rank} \left(\mathbf{P}_{\mathbf{U}}^{\perp}\right)=\left|\mathcal{X}\right|-K.\] (18)

Otherwise by contradiction, suppose \(\mathrm{rank}\left(\mathbf{P}_{\mathbf{U}}\right)=\mathrm{rank}\left(\mathbf{ U}_{\mathcal{X}}\right)<K\), then Eckart-Young-Mirsky theorem [Eckart and Young, 1936] implies that

\[\min_{\mathrm{rank}\left(\mathbf{U}_{\mathcal{X}}\right)<K}\left\|\overline{ \mathbf{W}}(\mathcal{X})-\mathbf{U}_{\mathcal{X}}\mathbf{U}_{\mathcal{X}}^{ \top}\right\|_{F}^{2}=\sum_{i=K}^{\left|\mathcal{X}\right|}\left(1-\lambda_{i} \right)^{2}>\sum_{i>K}\left(1-\lambda_{i}\right)^{2}+\Delta=R\left(f_{\left| \mathcal{X}\right)}+\Delta,\]

which contradicts \(R\left(f_{\left|\mathbf{X}^{u}\right)}\leq R\left(f_{\left|\mathcal{X}\right) }+\Delta\).

Furthermore, since \(\left\|\mathbf{v}_{i}\right\|_{2}=1\) and \(\mathbf{P}_{\mathbf{U}}^{\perp}\) is an orthogonal projection, for all \(i\in\left[\left|\mathcal{X}\right|\right]\),

\[0\leq\left\|\mathbf{P}_{\mathbf{U}}^{\perp}\mathbf{v}_{i}\right\|_{2}^{2}\leq 1.\] (19)

For every \(i\in\left[\left|\mathcal{X}\right|\right]\), we denote \(\xi_{i}\triangleq\left\|\mathbf{P}_{\mathbf{U}}^{\perp}\mathbf{v}_{i}\right\|_ {2}^{2}\). Then, upper bounding \(\left\|\mathbf{P}_{\mathbf{U}}^{\perp}\mathbf{V}_{K}\right\|_{F}^{2}=\sum_{i= 1}^{K}\xi_{i}\) can be recast as a linear programming problem:

\[\begin{array}{rl}\left(\mathrm{P}\right):&\max_{\xi\in\mathbb{R}^{\left| \mathcal{X}\right|}}\sum_{i=1}^{K}\xi_{i}\\ &\mathrm{s.t.}\quad 0\leq\xi_{i}\leq 1,\\ &&\sum_{i=1}^{\left|\mathcal{X}\right|}\xi_{i}=\left|\mathcal{X} \right|-K,\\ &&\sum_{i=1}^{\left|\mathcal{X}\right|}\left(1-\lambda_{i}\right)^{2}\xi_{i} \leq\sum_{i>K}\left(1-\lambda_{i}\right)^{2}+\Delta,\end{array}\] (20)

whose dual can be expressed as

\[\begin{array}{rl}\left(\mathrm{D}\right):&\min_{\left(\omega_{1},\cdots, \omega_{\left|\mathcal{X}\right|},\omega_{s},\omega_{\Delta}\right)}\sum_{i=1}^ {\left|\mathcal{X}\right|}\omega_{i}-\left(\left|\mathcal{X}\right|-K\right) \omega_{s}+\left(\Delta+\sum_{i>K}\left(1-\lambda_{i}\right)^{2}\right)\omega _{\Delta}\\ &\mathrm{s.t.}\quad\omega_{1},\cdots,\omega_{\left|\mathcal{X}\right|}, \omega_{s},\omega_{\Delta}\geq 0,\\ &&\omega_{i}-\omega_{s}+\left(1-\lambda_{i}\right)^{2}\omega_{\Delta}\geq 1 \quad\forall\;i\leq K,\\ &&\omega_{i}-\omega_{s}+\left(1-\lambda_{i}\right)^{2}\omega_{\Delta}\geq 0 \quad\forall\;i>K.\end{array}\] (21)

By taking

\[\begin{array}{rl}\omega_{i}=0&\forall\;i\leq K_{0},\\ \omega_{i}=\frac{\left(1-\lambda_{K_{0}}\right)^{2}-\left(1-\lambda_{i} \right)^{2}}{\left(1-\lambda_{K_{0}}\right)^{2}-\left(1-\lambda_{K+1}\right)^{2 }}&\forall\;K_{0}<i\leq K,\\ \omega_{i}=\frac{\left(1-\lambda_{K+1}\right)^{2}-\left(1-\lambda_{i} \right)^{2}}{\left(1-\lambda_{K_{0}}\right)^{2}-\left(1-\lambda_{K+1}\right)^{2 }}&\forall\;K+1\leq i\leq\left|\mathcal{X}\right|,\\ \omega_{s}=\frac{\left(1-\lambda_{K+1}\right)^{2}}{\left(1-\lambda_{K_{0}} \right)^{2}-\left(1-\lambda_{K+1}\right)^{2}},\\ \omega_{\Delta}=\frac{1}{\left(1-\lambda_{K_{0}}\right)^{2}-\left(1- \lambda_{K+1}\right)^{2}},\end{array}\]the dual optimum can be upper bounded by

\[\left(\mathrm{D}\right)= \sum_{i=1}^{K}\omega_{i}+\Delta\cdot\omega_{\Delta}\] \[= \frac{\Delta}{\left(1-\lambda_{K_{0}}\right)^{2}-\left(1-\lambda_{ K+1}\right)^{2}}+\sum_{i=K_{0}+1}^{K}\frac{\left(1-\lambda_{K_{0}}\right)^{2}- \left(1-\lambda_{i}\right)^{2}}{\left(1-\lambda_{K_{0}}\right)^{2}-\left(1- \lambda_{K+1}\right)^{2}}\] \[\leq \frac{\left(1+\left(K-K_{0}\right)C_{K_{0}}\right)\Delta}{\left(1 -\lambda_{K_{0}}\right)^{2}-\left(1-\lambda_{K+1}\right)^{2}},\]

given \(C_{K_{0}}\triangleq\frac{\left(1-\lambda_{K_{0}}\right)^{2}-\left(1-\lambda_{ K}\right)^{2}}{\Delta}=O(1)\).

With both the primal Equation (20) and the dual Equation (21) being feasible and bounded, the strong duality then implies that

\[\left\|\mathbf{P}\mathbf{\frac{1}{U}}\mathbf{V}_{K}\right\|_{F}^{2}=\sum_{i=1 }^{K}\xi_{i}=\left(\mathrm{P}\right)=\left(\mathrm{D}\right)\leq\frac{\left( 1+\left(K-K_{0}\right)C_{K_{0}}\right)\Delta}{\left(1-\lambda_{K_{0}}\right)^ {2}-\left(1-\lambda_{K+1}\right)^{2}},\]

which, when plugging back into Equation (16), completes the proof. 

## Appendix E Proofs and Discussions for Section 6

### Expansion-based Data Augmentation

For the detailed analysis, we further recall the following notion of constant expansion and its relation with the concept of \(c\)-expansion in Definition 6.1 from Wei et al. (2021).

**Definition E.1** (\(\left(q,\xi\right)\)-constant expansion Wei et al. (2021)).: _We say the expansion-based data augmentation (Definition 6.1) satisfies \(\left(q,\xi\right)\)-constant expansion if for any \(S\subseteq\mathcal{X}\) such that \(P(S)\geq q\) and \(P\left(S\cap\mathcal{X}_{k}\right)\leq P\left(\mathcal{X}_{k}\right)/2\) for all \(k\in\left[K\right]\),_

\[P\left(\mathrm{NB}(S)\right)>\min\left\{P(S),\xi\right\}+P(S).\]

**Lemma E.1** (\(c\)-expansion implies constant expansion Wei et al. (2021)).: _The \(c\)-expansion property (Definition 6.1) implies \(\left(\frac{\xi}{c-1},\xi\right)\)-constant expansion (Definition E.1) for any \(\xi>0\)._

Proof of Lemma e.1.: Consider a subset \(S\subseteq\mathcal{X}\) such that \(P(S)\geq q\) and \(P\left(S\cap\mathcal{X}_{k}\right)\leq P\left(\mathcal{X}_{k}\right)/2\). Let \(S_{k}\triangleq S\cap\mathcal{X}_{k}\) for every \(k\in\left[K\right]\).

1. If \(c\in\left(1,2\right)\), since \(P\left(\mathcal{X}_{k}\right)-P\left(S_{k}\right)\geq P\left(S_{k}\right)\geq \left(c-1\right)P\left(S_{k}\right)\) \[P\left(\mathrm{NB}\left(S\right)\setminus S\right)\geq \sum_{k=1}^{K}P\left(\mathrm{NB}\left(S_{k}\right)\right)-P\left(S _{k}\right)\] \[> \sum_{k=1}^{K}\min\left\{\left(c-1\right)P\left(S_{k}\right),P \left(\mathcal{X}_{k}\right)-P\left(S_{k}\right)\right\}\] \[\geq \sum_{k=1}^{K}(c-1)P(S_{k})=(c-1)P(S)\geq(c-1)q=\xi\]
2. If \(c\geq 2\), since \(c-1\geq 1\) and \(P\left(\mathcal{X}_{k}\right)-P\left(S_{k}\right)\geq P\left(S_{k}\right)\), we have \[P\left(\mathrm{NB}\left(S\right)\setminus S\right)> \sum_{k=1}^{K}\min\left\{\left(c-1\right)P\left(S_{k}\right),P \left(\mathcal{X}_{k}\right)-P\left(S_{k}\right)\right\}\geq\sum_{k=1}^{K}P \left(S_{k}\right)=P(S)\] Overall, we have \[P\left(\mathrm{NB}(S)\right)=P\left(\mathrm{NB}\left(S\right)\setminus S \right)+P(S)>\min\left\{P(S),\xi\right\}+P(S).\]

### Data Augmentation Consistency Regularization

To bridge Equation (8) with common DAC regularization algorithms in practice, in Example E.1, we instantiate FixMatch [20] - a state-of-the-art semi-supervised learning algorithm that leverages DAC regularization by encouraging similar predictions of proper weak and strong data augmentations \(\mathbf{x}^{w},\mathbf{x}^{s}\in\mathcal{A}(\mathbf{x})\) of the same sample \(\mathbf{x}\).

**Example E.1** (FixMatch [20]).: _FixMatch minimizes the loss between the outputs of the strong augmentation \(f(\mathbf{x}^{s})\) and the pseudo-label of the weak augmentation \(y_{f}\left(\mathbf{x}^{w}\right)\) when the confidence of such pseudo-labeling is high (i.e., \(\max_{k\in[K]}f\left(\mathbf{x}^{w}\right)_{k}\) is larger than a threshold)._

_To connect FixMatch with Equation (8), we consider a fixed (unknown) margin-robust neighborhood \(\mathcal{F}^{\prime}\supseteq\mathcal{F}^{\prime}_{\mathbf{X}^{u}}\ni g\) where the strong augmentation characterizes the worst-case (minimum) robust margin \(\mathbf{x}^{s}\in\bigcap_{f\in\mathcal{F}^{\prime}}\operatorname*{argmin}_{ \mathbf{x}^{\prime}\in\mathcal{A}(\mathbf{x})}m\left(f,\mathbf{x}^{\prime},y _{f}(\mathbf{x})\right)\); while the weak augmentation preserves the classification \(y_{f}\left(\mathbf{x}^{w}\right)=y_{f}\left(\mathbf{x}\right)\)._

_Then, for any \(f\in\mathcal{F}^{\prime}\) during learning, minimizing the cross-entropy loss between \(\mathtt{softmax}\left(f(\mathbf{x}^{s})\right)\) and the one-hot label \(\overline{y}^{\prime}_{f}\left(\mathbf{x}^{w}\right)\) is equivalent to enforcing a large enough margin \(m\left(f,\mathbf{x}^{s},y_{f}(\mathbf{x})\right)\geq\tau\), whereas \(m\left(f,\mathbf{x}^{\prime},y_{f}(\mathbf{x})\right)\geq m\left(f,\mathbf{x} ^{s},y_{f}(\mathbf{x})\right)\geq\tau\) for all \(\mathbf{x}^{\prime}\in\mathcal{A}(\mathbf{x})\) by construction implies \(m_{\mathcal{A}}\left(f,\mathbf{x}\right)\geq\tau\) as required by the constraints in Equation (8)._

### Proof of Theorem 6.2

Proof of Theorem 6.2.: We first recall from Lemma E.1 that \(c\)-expansion implies \(\left(\frac{2\nu\left(f\right)}{c-1},2\nu(f)\right)\)-constant expansion (Definition E.1) for any \(f\in\mathcal{F}\). Therefore, it is sufficient to show that with \(\left(q,2\nu(f)\right)\)-constant expansion (i.e., \(q=\frac{2\nu(f)}{c-1}\)),

\[P\left(M(f)\right)\leq\max\left\{q,2\nu(f)\right\}\quad\forall\;f\in\mathcal{ F}^{\prime}_{\mathbf{X}^{s}}.\] (22)

For any \(f\in\mathcal{F}^{\prime}_{\mathbf{X}^{s}}\), we notice that Equation (22) is automatically satisfied when \(P\left(M(f)\right)<q\). Otherwise, when \(P\left(M(f)\right)\geq q\), then along with \(P\left(S\cap\mathcal{X}_{k}\right)\leq P\left(\mathcal{X}_{k}\right)/2\) for all \(k\in[K]\), \(\left(q,2\nu(f)\right)\)-constant expansion implies

\[P\left(\operatorname{NB}\left(M(f)\right)\right)>\min\left\{P\left(M(f)\right), 2\nu(f)\right\}+P\left(M(f)\right).\] (23)

Meanwhile, we observe that by union bound,

\[P\left(\operatorname{NB}\left(M(f)\right)\setminus M(f)\right)\leq 2\mathbb{P} \left[\exists\;\mathbf{x}^{\prime}\in\mathcal{A}(\mathbf{x})\,\text{s.t.}\;y_{f }(\mathbf{x})\neq y_{f}\left(\mathbf{x}^{\prime}\right)\right]=2\nu(f)\] (24)

because for any \(\mathbf{x}\in\operatorname{NB}\left(M(f)\right)\setminus M(f)\), there exists \(\mathbf{x}^{\prime}\in M(f)\) such that one can find some \(\mathbf{x}^{\prime\prime}\in\mathcal{A}(\mathbf{x})\cap\mathcal{A}\left( \mathbf{x}^{\prime}\right)\); and therefore exactly one of the follows must hold

1. if \(\mathbf{x}^{\prime\prime}\in M(f)\), then \(\widetilde{y}_{f}\left(\mathbf{x}^{\prime\prime}\right)\neq y_{*}\left( \mathbf{x}\right)=\widetilde{y}_{f}\left(\mathbf{x}\right)\) implies \(y_{f}\left(\mathbf{x}^{\prime\prime}\right)\neq y_{f}\left(\mathbf{x}\right)\) for \(\mathbf{x}^{\prime\prime}\in\mathcal{A}\left(\mathbf{x}\right)\);
2. if \(\mathbf{x}^{\prime\prime}\notin M(f)\), then \(\widetilde{y}_{f}\left(\mathbf{x}^{\prime\prime}\right)=y_{*}\left(\mathbf{x} \right)\neq\widetilde{y}_{f}\left(\mathbf{x}^{\prime}\right)\) implies \(y_{f}\left(\mathbf{x}^{\prime\prime}\right)\neq y_{f}\left(\mathbf{x}^{\prime} \right)\) for \(\mathbf{x}^{\prime\prime}\in\mathcal{A}\left(\mathbf{x}^{\prime}\right)\).

Leveraging Equation (23) and Equation (24), we have

\[P\left(M(f)\right)\geq P\left(\operatorname{NB}\left(M(f)\right)\right)-P\left( \operatorname{NB}\left(M(f)\right)\setminus M(f)\right)\] \[> \min\left\{P\left(M(f)\right),2\nu(f)\right\}+P\left(M(f)\right)-2 \nu(f),\]

which leads to \(\min\left\{P\left(M(f)\right),2\nu(f)\right\}<2\nu(f)\) and implies \(P\left(M(f)\right)<2\nu(f)\). 

## Appendix F Technical Lemmas

Here, we briefly review the standard Rademacher-complexity-based generalization analysis based on McDiarmid's inequality and a classical symmetrization argument Wainwright (2019); Bartlett and Mendelson (2003).

**Lemma F.1** (Generalization guarantee with Rademacher complexity).: _Given a distribution over a set \(\mathcal{Z}\)13, we consider a set of i.i.d. samples \(\left\{\mathbf{z}_{i}\right\}_{i\in[n]}\sim\mathcal{Z}^{n}\) with \(\mathbf{Z}=\left[\mathbf{z}_{1},\ldots,\mathbf{z}_{n}\right]\) and a \(B\)-bounded function class \(\mathcal{H}=\left\{h:\mathcal{Z}\rightarrow\mathbb{R}\;|\;|h(\mathbf{z})-h( \mathbf{z}^{\prime})|\leq B\;\forall\;\mathbf{z},\mathbf{z}^{\prime}\in \mathcal{Z}\right\}\). By denoting_

Footnote 13: Without ambiguity, we overload the notation \(\mathcal{Z}\) as the set as well as the distribution over the set.

\[H(h)\triangleq\mathbb{E}_{\mathbf{z}\sim\mathcal{Z}}\left[h(\mathbf{z})\right] \quad\text{and}\quad\widehat{H}_{\mathbf{Z}}(h)\triangleq\frac{1}{n}\sum_{i=1}^{ n}h(\mathbf{z}_{i})\quad\text{s.t.}\quad\mathbb{E}_{\mathbf{Z}\sim\mathcal{Z}^{n}} \left[\widehat{H}_{\mathbf{Z}}(h)\right]=H(h),\]_then for \(h^{*}\in\operatorname*{argmin}_{h\in\mathcal{H}}H(h)\) and \(\widehat{h}\in\operatorname*{argmin}_{h\in\mathcal{H}}\widehat{H}_{\mathbf{Z}}(h)\), we have that with probability at least \(1-\delta/2\) over \(\mathbf{Z}\),_

\[H(\widehat{h})-H(h^{*})\leq 4\mathfrak{R}_{n}\left(\mathcal{H}\right)+\sqrt{ \frac{2B^{2}\log(4/\delta)}{n}}.\]

_where \(\mathfrak{R}_{n}\left(\mathcal{H}\right)\) is the Rademacher complexity of \(\mathcal{H}\),_

\[\mathfrak{R}_{n}\left(\mathcal{H}\right)\triangleq\mathbb{E}_{ \begin{subarray}{c}\mathbf{Z}\sim\mathcal{Z}^{n}\\ \boldsymbol{\rho}\sim\operatorname*{Rad}^{n}\end{subarray}}\left[\sup_{h\in \mathcal{H}}\ \frac{1}{n}\sum_{i=1}^{n}\rho_{i}\cdot h(\mathbf{z}_{i})\right], \quad\operatorname*{Rad}(\rho)=\begin{cases}1/2,&\rho=-1\\ 1/2,&\rho=1\\ 0,&\rho\neq\pm 1\end{cases}.\]

Proof of Lemma r.1.: Let \(g\left(\mathbf{z}_{1},\ldots,\mathbf{z}_{n}\right)\triangleq g(\mathbf{Z}) \triangleq\sup_{h\in\mathcal{H}}H(h)-\widehat{H}_{\mathbf{Z}}(h)\). Then \(h\in\mathcal{H}\) being \(B\)-bounded implies that for any \(\mathbf{z}\sim\mathcal{Z}\) and \(i\in[n]\),

\[\left|g(\mathbf{z}_{1},\ldots,\mathbf{z}_{i},\ldots,\mathbf{z}_{n})-g( \mathbf{z}_{1},\ldots,\mathbf{z},\ldots,\mathbf{z}_{n})\right|\leq\frac{B}{n}.\]

Therefore, by McDiarmid's inequality (Bartlett and Mendelson, 2003),

\[\mathbb{P}\left[g(\mathbf{Z})\geq\mathbb{E}[g(\mathbf{Z})]+t\right]\leq\exp \left(-\frac{2nt^{2}}{B^{2}}\right),\]

where \(\exp\left(-\frac{2nt^{2}}{B^{2}}\right)\leq\frac{\delta}{4}\) when \(t\geq\sqrt{\frac{B^{2}\log(4/\delta)}{2n}}\).

Meanwhile, by a classical symmetrization argument (e.g., proof of Wainwright (2019) Theorem 4.10), we can bound the expectation \(\mathbb{E}[g(\mathbf{Z})]\): for an independent sample set \(\mathbf{Z}^{\prime}\sim\mathcal{Z}^{n}\) that is also independent of \(\mathbf{Z}\),

\[\mathbb{E}\left[g(\mathbf{Z})\right]= \mathbb{E}_{\mathbf{Z}}\left[\sup_{h\in\mathcal{H}}\ H(h)- \widehat{H}_{\mathbf{Z}}(h)\right]\] \[= \mathbb{E}_{\mathbf{Z}}\left[\sup_{h\in\mathcal{H}}\ \mathbb{E}_{ \mathbf{Z}^{\prime}}\left[\widehat{H}_{\mathbf{Z}^{\prime}}(h)-\widehat{H}_{ \mathbf{Z}}(h)\ \Big{|}\ \mathbf{Z}\right]\right]\] \[\leq \mathbb{E}_{\mathbf{Z}}\left[\mathbb{E}_{\mathbf{Z}^{\prime}} \left[\sup_{h\in\mathcal{H}}\ \widehat{H}_{\mathbf{Z}^{\prime}}(h)-\widehat{H}_{ \mathbf{Z}}(h)\ \bigg{|}\ \mathbf{Z}\right]\right]\] \[= \mathbb{E}_{(\mathbf{Z},\mathbf{Z}^{\prime})}\left[\sup_{h\in \mathcal{H}}\ \widehat{H}_{\mathbf{Z}^{\prime}}(h)-\widehat{H}_{\mathbf{Z}}(h)\right],\]

where in the last line we have used the law of iterated conditional expectation. Then, with Rademacher random variables \(\boldsymbol{\rho}=(\rho_{1},\ldots,\rho_{n})\sim\operatorname*{Rad}^{n}\), we have

\[\mathbb{E}\left[g(\mathbf{Z})\right]\leq \mathbb{E}_{(\mathbf{Z},\mathbf{Z}^{\prime},\boldsymbol{\rho})} \left[\sup_{h\in\mathcal{H}}\ \frac{1}{n}\sum_{i=1}^{n}\rho_{i}\cdot(h(\mathbf{z}_{i}^{\prime})-h( \mathbf{z}_{i}))\right]\] \[\leq 2\mathbb{E}_{(\mathbf{Z},\boldsymbol{\rho})}\left[\sup_{h\in \mathcal{H}}\ \frac{1}{n}\sum_{i=1}^{n}\rho_{i}\cdot h(\mathbf{z}_{i})\right]\] \[\leq 2\mathfrak{R}_{n}\left(\mathcal{H}\right).\]

Therefore, with probability at least \(1-\delta/4\) over \(\mathbf{Z}\), we have

\[\sup_{h\in\mathcal{H}}H(h)-\widehat{H}_{\mathbf{Z}}(h)\leq 2\mathfrak{R}_{n} \left(\mathcal{H}\right)+\sqrt{\frac{B^{2}\log(4/\delta)}{2n}},\]

while the same tail bound holds for \(\sup_{h\in\mathcal{H}}-H(h)+\widehat{H}_{\mathbf{Z}}(h)\) via an analogous argument.

By recalling the definition of \(\widehat{h}\) and \(h^{*}\), we can decompose the excess risk \(H(\widehat{h})-H(h^{*})\) as

\[H(\widehat{h})-H(h^{*}) =\left(H(\widehat{h})-\widehat{H}_{\mathbf{Z}}(\widehat{h})\right) +\left(\widehat{H}_{\mathbf{Z}}(\widehat{h})-\widehat{H}_{\mathbf{Z}}(h^{*}) \right)+\left(\widehat{H}_{\mathbf{Z}}(h^{*})-H(h^{*})\right)\] \[\leq\left(H(\widehat{h})-\widehat{H}_{\mathbf{Z}}(\widehat{h}) \right)+\left(\widehat{H}_{\mathbf{Z}}(h^{*})-H(h^{*})\right)\]where \(\widehat{H}_{\mathbf{Z}}(\widehat{h})-\widehat{H}_{\mathbf{Z}}(h^{*})\leq 0\) by the basic inequality. With both \(\widehat{h},h^{*}\in\mathcal{H}\), we have

\[H(\widehat{h})-H(h^{*})\leq 2\sup_{h\in\mathcal{H}}\,\left|H(h)-\widehat{H}_{ \mathbf{Z}}(h)\right|,\]

and therefore

\[\mathbb{P}\left[H(\widehat{h})-H(h^{*})\geq t\right]\leq\mathbb{P}\left[\sup _{h\in\mathcal{H}}H(h)-\widehat{H}_{\mathbf{Z}}(h)\geq\frac{t}{2}\right]+ \mathbb{P}\left[\sup_{h\in\mathcal{H}}-H(h)+\widehat{H}_{\mathbf{Z}}(h)\geq \frac{t}{2}\right].\]

Overall, with probability at least \(1-\delta/2\) over \(\mathbf{Z}\),

\[H(\widehat{h})-H(h^{*})\leq 4\mathfrak{R}_{n}\left(\mathcal{H}\right)+\sqrt{ \frac{2B^{2}\log(4/\delta)}{n}}.\]