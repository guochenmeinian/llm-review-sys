# On the Generalization Properties of Diffusion Models

 Puheng Li

Department of Statistics

Stanford University

puhengli@stanford.edu

&Zhong Li1

Machine Learning Group

Microsoft Research Asia

lzhong@microsoft.com

&Huishuai Zhang

Machine Learning Group

Microsoft Research Asia

huzhang@microsoft.com

&Jiang Bian

Machine Learning Group

Microsoft Research Asia

jiabia@microsoft.com

This work was done when Puheng Li was an undergraduate student at Peking University and a research intern at MSRA. The first two authors contributed equally to this work and are ordered alphabetically.Corresponding author.

###### Abstract

Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error (\(O(n^{-2/5}+m^{-4/5})\)) on both the sample size \(n\) and the model capacity \(m\), evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when _early-stopped_. Furthermore, we extend our quantitative analysis to a _data-dependent_ scenario, wherein target distributions are portrayed as a succession of densities with progressively increasing distances between modes. This precisely elucidates the _adverse_ effect of "_modes shift_" in ground truths on the model generalization. Moreover, these estimates are not solely theoretical constructs but have also been confirmed through numerical simulations. Our findings contribute to the rigorous understanding of diffusion models' generalization properties and provide insights that may guide practical applications.

## 1 Introduction

As an emerging family of deep generative models, diffusion models (DMs; [16; 45]) have experienced a surge in popularity, owing to their unparalleled performance in a wide range of applications ([22; 39; 68; 15; 3; 35; 4; 60; 33; 69; 64; 63; 53]). This has led to notable commercial successes, such as DALL-E ([34]), Imagen ([38]), and Stable Diffusion ([36]). Mathematically, diffusion models learn an unknown underlying distribution through a two-stage process: (i) first, successively and gradually injecting random noises (forward process); (ii) then reversing the forward process through denoising for sampling purposes (reverse process). To achieve this, an equivalent formulation of diffusion models called score-based generative models (SGMs; [50; 52]) is employed. SGMs implement the aforementioned two-stage process via the continuous dynamics represented by a joint group of coupled stochastic differential equations (SDEs) ([54; 20]).

Despite their impressive empirical performance, the theoretical foundation of DMs/SGMs remains underexplored. Generally, fundamental theoretical questions can be categorized into several aspects. By considering machine learning models as mathematical function classes from certain spaces, one can identify three central aspects: approximation, optimization and generalization. At the forefront lies the generalization problem, which aims to characterize the learning error between the learned and ground truth distributions.

The development of generalization theory for diffusion models is pressing due to both theoretical and practical concerns:

* In theory, the generalization issues of generative modeling (or learning for distributions) may exhibit as the memorization phenomenon, if the modeled distribution is eventually trained to converge to the empirical distribution only associated with training samples. Intuitively, memorization arises from two reasons: (i) it is useful for the hypothesis space to be large enough to approximate highly complex underlying target distributions (universal convergence; [65]); (ii) the underlying distribution is unknown in practice, and one can only use a dataset with finite samples drawn from the target distribution. Rigorous mathematical characterizations of memorization are developed for bias potential models and GANs in [66] and [67], respectively. A natural question is, does a similar phenomenon occur for diffusion models? To answer this, a thorough investigation of generalization properties for DMs/SGMs is required.
* In practice, the generalization capability of diffusion models is also an essential requirement, as the memorization can lead to potential privacy and copyright risks when models are deployed. Similar to other generative models and large language models (LLMs) [5; 70; 19; 6], diffusion models can also memorize and leak training samples [5; 46], hence can be subsequently attacked using specific procedures and algorithms [28; 18; 62]. Although there are defense methods developed to meet privacy and copyright standards ([11; 14; 58]), these approaches are often heuristic, without providing sufficient quantitative understandings particularly on diffusion models. Therefore, a comprehensive investigation of the generalization foundation of diffusion models, including both theoretical and empirical aspects, is of utmost importance in improving principled tutorial guidance in practice.

The current work develops the generalization theory of diffusion models in a mathematically rigorous manner. Our main results include the following:

* We derive an upper bound of the generalization gap for diffusion models along the training dynamics. This result suggests, with _early-stopping_, the generalization error of diffusion models scales polynomially small on the sample size (\(O(n^{-2/5})\)) and the model capacity (\(O(m^{-4/5})\)). Notably, the generalization error also escapes from the curse of dimensionality.
* This "uniform" bound is further extended to a _data-dependent_ setting, where a sequence of unidimensional Gaussian mixtures distributions with an increasing modes' distance is considered as the ground truth. This result characterizes the effect of "_modes shift_" quantitatively, which implies that the generalization capability of diffusion models is _adversely_ affected by the distance between high-density regions of target distributions.
* The theoretical findings are numerically verified in simulations.

The rest of this paper is organized as follows. In Section 2, we discuss the related work on the convergence and training fronts of diffusion models and also the generalization aspects of other generative modeling methods. Section 3 is the central part, which includes the problem formulation, main results, and consequences. Section 4 includes numerical verifications on synthetic and real-world datasets3. All the details of proofs and experiments are found in the appendices.

Footnote 3: Code is available at https://github.com/lphLeo/Diffusion_Generalization

## 2 Related Work

We review the related work on diffusion models concerning the central results in this paper.

* First, on the convergence theory, [7; 9] established elaborate error estimates between the modeled and target distribution given the discretization and time-dependent score matching tolerance. Compared to the present work, they did not evolve the concrete training dynamics since the setting therein focuses on the properties of optimizers.
* Second, on the training front, [51] proposed a set of techniques to enhance the training performance of score-based generative models, scaling diffusion models to images of higher resolution, but without any characterization of possible generalization improvements. Similar to [7; 9], [49] also provided error estimates between the modeled and target distributions in a point-wise sense and again did not evolve the detailed training dynamics.
* Third, on the generalization and memorization side, corresponding theories are developed for bias potential models and GANs in [66] and [67], respectively, where the modeled distribution learns the ground truth with early-stopping and diverges or converges to the empirical distribution only associated with training samples after sufficiently long training time. The current work extends the mathematical analysis to the case of diffusion models under a data-dependent setting.
* As a supplement, we also discuss related literature regarding low-density learning. [50] illustrated the difficulty of learning from low-density regions with toy formulations and simulations, which motivates the sampling method of annealed Langevin dynamics as the predecessor of (score-based) diffusion models. [21] restudied similar problems under the pure score matching regime (without the denoising or time-dependent dynamics) and attributed the difficulty to increasing isoperimetry of distributions with modes shift. [41] defined the Hardness score and numerically justified that decreasing manifold densities leads to the increasing Hardness score, and consequently applied the Hardness score as the regularization to the sampling process to enhance synthetic images from low-density regions. As a comparison, this work establishes a mathematically rigorous estimate on the generalization gap that _quantitatively_ depends on the range of low-density regions (between modes) in target distributions for (score-based) diffusion models that requires _denoising_.

## 3 Formulation and Results

In this section, we first introduce the problem setup. Next, we state the main theoretical results, subsequent consequences, and possible connections. The numerical illustration is provided at last.

### Problem Formulation

Since DMs/SGMs have already grown into a large family of generative models with an enormous number of variants, there are various ways to define the parameterization of diffusion models. Here, we adopt (one of) the most fundamental architectures proposed in [54], where the forward perturbation and reverse sampling process are both implemented by a joint group of coupled (stochastic) differential equations. See Figure 1 for an illustration of the problem formulation.

Figure 1: Illustration of the problem formulation and important notations.

Forward perturbation.We start with the setting of unsupervised learning. Given an unlabeled dataset \(\mathcal{D}_{\bm{x}}=\{\bm{x}_{i}\}_{i=1}^{n}\subset\mathbb{R}^{d}\) with the sample \(\bm{x}_{i}\overset{\text{i.i.d.}}{\sim}p_{0}(\bm{x})\), where \(p_{0}\) denotes the underlying (ground truth or target) distribution, the forward diffusion process is defined as

\[d\bm{x}=\bm{f}(\bm{x},t)dt+g(t)d\bm{W}_{t},\quad\bm{x}(0)\sim p_{0}.\] (1)

Here, the drift coefficient \(\bm{f}(\cdot,t):\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\) is a time-dependent vector-valued function, and the diffusion coefficient \(g(\cdot):\mathbb{R}_{\geq 0}\mapsto\mathbb{R}\) is a scalar function, and \(\bm{W}_{t}\) denotes the standard Wiener process (a.k.a., Brownian motion). The SDE (1) has a unique, strong solution under certain regularity conditions (i.e., globally Lipschitz coefficients in both state and time; see [31]). From now on, we denote by \(p_{t}(\bm{x}(t))\) the marginal distribution of \(\bm{x}(t)\), and let \(p_{t|s}(\bm{x}(t)|\bm{x}(s))\) be the (perturbation) transition kernel from \(\bm{x}(s)\) to \(\bm{x}(t),0\leq s<t\leq T<\infty\) with \(T\) as the time horizon. By appropriately selecting \(\bm{f}\) and \(g\), one can force the SDE (1) to converge to a prior distribution (typically a Gaussian). Common examples include the (time-rescaled) Ornstein-Uhlenbeck (OU) process,4 which is a special case of the linear version of (1):

Footnote 4: The OU process is the unique time-homogeneous Markov process which is also a Gaussian process, with the stationary distribution as the standard Gaussian distribution.

\[d\bm{x}=f(t)\bm{x}dt+g(t)d\bm{W}_{t},\quad\bm{x}(0)\sim p_{0}.\] (2)

Reverse sampling.According to [1] and [54], both the following reverse-time SDE and probability flow ODE share the same marginal distribution as the forward-time SDE (1):

\[d\bm{x}=\Big{[}\bm{f}(\bm{x},t)-g^{2}(t)\nabla_{\bm{x}}\log p_{t }(\bm{x})\Big{]}dt+g(t)d\bar{\bm{W}}_{t},\] (3) \[d\bm{x}=\Big{[}\bm{f}(\bm{x},t)-\frac{1}{2}g^{2}(t)\nabla_{\bm{ x}}\log p_{t}(\bm{x})\Big{]}dt,\] (4)

where \(\bar{\bm{W}}_{t}\) is a standard Wiener process when time flows backwards from \(T\) to \(0\), and \(dt\) is an infinitesimal negative time step. With the initial condition \(\bm{x}(T)\sim p_{T}\approx\pi\), where \(\pi\) is a known prior distribution such as the Gaussian noise, one can (numerically) solve (3) or (4) to transform noises into samples from \(p_{0}\), which is exactly the goal of generative modeling.

Loss objectives.The only remaining task is to estimate the unknown (Stein) score function \(\nabla_{\bm{x}}\log p_{t}(\bm{x})\). This is achieved by minimizing the following weighted sum of denoising score matching ([57]) objectives:

\[\mathcal{L}(\bm{\theta};\lambda(\cdot)):=\mathbb{E}_{t\sim\mathcal{U}(0,T)} \left[\lambda(t)\cdot\mathbb{E}_{\bm{x}(0)\sim p_{0}}\left[\mathbb{E}_{\bm{x}( t)\sim p_{t|0}}\left[\|\bm{s}_{t,\bm{\theta}}(\bm{x}(t))-\nabla_{\bm{x}(t)}\log p _{t|0}(\bm{x}(t)|\bm{x}(0))\|_{2}^{2}\right]\right]\right],\] (5)

with \(\bm{\theta}^{*}:=\arg\min_{\bm{\theta}}\mathcal{L}(\bm{\theta};\lambda(\cdot))\), where \(\mathcal{U}(0,T)\) denotes the uniform distribution over \([0,T]\), and \(\lambda(t):[0,T]\mapsto\mathbb{R}_{+}\) is a weighting function, which is typically selected as

\[\lambda(t)\propto 1/\sqrt{\mathbb{E}_{\bm{x}(t)\sim p_{t|0}}[\|\nabla_{\bm{x}(t)} \log p_{t|0}(\bm{x}(t)|\bm{x}(0))\|_{2}^{2}]}\] (6)

according to ([54]). The score function \(\bm{s}_{t,\bm{\theta}}:\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\) is time-dependent and can be parameterized as a neural network (encoded with the time information) such as the U-net([37]) architecture commonly applied in the field of image segmentation. Alternatively, one can also define the time-dependent score matching loss

\[\tilde{\mathcal{L}}(\bm{\theta};\lambda(\cdot)):=\mathbb{E}_{t\sim\mathcal{U}(0,T)}\left[\lambda(t)\cdot\mathbb{E}_{\bm{x}(t)\sim p_{t}}\left[\|\bm{s}_{t,\bm {\theta}}(\bm{x}(t))-\nabla_{\bm{x}(t)}\log p_{t}(\bm{x}(t))\|_{2}^{2}\right] \right],\] (7)

which is equivalent to (5) up to a constant independent of \(\bm{\theta}\) by [57, 49].

In practice, expectations in the objective (5) can be respectively estimated with empirical means over time steps in \([0,T]\), data samples from \(p_{0}\) and \(p_{t|0}\), which is efficient when the drift coefficient \(\bm{f}(\cdot,t)\) is linear. Specifically, if the forward-time SDE takes the form of (2), the transition kernel \(p_{t|0}\) has a closed form ([40])

\[p_{t|0}(\bm{x}(t)|\bm{x}(0))=\mathcal{N}(\bm{x}(t);r(t)\bm{x}(0),r^{2}(t)v^{2}(t )\bm{I}_{d}),\] (8)

where \(\mathcal{N}(\bm{x};\bm{\mu},\bm{\Sigma})\) denotes the multivariate Gaussian distribution evaluated at \(\bm{x}\) with the expectation \(\bm{\mu}\) and covariance \(\bm{\Sigma}\), and \(r(t):=e^{\int_{0}^{t}f(\zeta)d\zeta}\), \(v(t):=\sqrt{\int_{0}^{t}\frac{g^{2}(\zeta)}{r^{2}(\zeta)}d\zeta}\).

Training.We aim to investigate the gradient flow training dynamics over the empirical loss

\[\frac{d}{d\tau}\hat{\bm{\theta}}_{n}(\tau)=-\nabla_{\hat{\bm{\theta}}_{n}(\tau)} \hat{\mathcal{L}}_{n}(\hat{\bm{\theta}}_{n}(\tau);\lambda(\cdot)),\quad\hat{ \bm{\theta}}_{n}(0):=\hat{\bm{\theta}}_{n}^{0},\] (9)

where \(\hat{\mathcal{L}}_{n}\) is the Monte-Carlo estimation of \(\mathcal{L}\) defined in (5) on the training dataset,5 with an auxiliary gradient flow over the population loss

Footnote 5: Recall the dataset \(\mathcal{D}_{\bm{x}}=\{\bm{x}_{i}\}_{i=1}^{n}\subset\mathbb{R}^{d}\), and we take \(\mathcal{D}_{t}=\{t_{i}\}_{i=1}^{n}\subset[0,T]\) for convenience.

\[\frac{d}{d\tau}\bm{\theta}(\tau)=-\nabla_{\bm{\theta}(\tau)} \mathcal{L}(\bm{\theta}(\tau);\lambda(\cdot)),\quad\bm{\theta}(0):=\bm{ \theta}^{0}=\hat{\bm{\theta}}_{n}^{0}.\] (10)

In both cases, the weighting function \(\lambda(\cdot)\) is selected as in (6). Denote the score function learned at the training time \(\tau\) evaluated at the SDE time \(t\) with respect to the empirical loss and population loss as \(\bm{s}_{t,\hat{\bm{\theta}}_{n}(\tau)}(\bm{x}(t))\) and \(\bm{s}_{t,\bm{\theta}(\tau)}(\bm{x}(t))\), respectively. The corresponding density functions, denoted by \(p_{t,\hat{\bm{\theta}}_{n}(\tau)}(\bm{x}(t))\) and \(p_{t,\bm{\theta}(\tau)}(\bm{x}(t))\), are obtained by solving

\[\nabla_{\bm{x}(t)}\log p_{t,\hat{\bm{\theta}}_{n}(\tau)}(\bm{x}(t) )=\bm{s}_{t,\hat{\bm{\theta}}_{n}(\tau)}(\bm{x}(t)),\quad\nabla_{\bm{x}(t)} \log p_{t,\bm{\theta}(\tau)}(\bm{x}(t))=\bm{s}_{t,\bm{\theta}(\tau)}(\bm{x}(t )),\] (11)

and then normalizing, respectively.

Score networks.We parameterize the score function \(\bm{s}_{t,\bm{\theta}}\) as the following random feature model

\[\bm{s}_{t,\bm{\theta}}(\bm{x}):=\frac{1}{m}\bm{A}\sigma(\bm{W} \bm{x}+\bm{U}\bm{e}(t))=\frac{1}{m}\sum_{i=1}^{m}\bm{a}_{i}\sigma(\bm{w}_{i}^ {\top}\bm{x}+\bm{u}_{i}^{\top}\bm{e}(t)),\] (12)

where \(\sigma\) is the ReLU activation function, \(\bm{A}=(\bm{a}_{1},\dots,\bm{a}_{m})\in\mathbb{R}^{d\times m}\) is the _trainable_ parameter, while \(\bm{W}=(\bm{w}_{1},\dots,\bm{w}_{m})^{\top}\in\mathbb{R}^{m\times d}\) and \(\bm{U}=(\bm{u}_{1},\dots,\bm{u}_{m})^{\top}\in\mathbb{R}^{m\times d_{x}}\) are randomly initialized and _frozen_ during training, and \(\bm{e}:\mathbb{R}_{\geq 0}\mapsto\mathbb{R}^{d_{x}}\) is the embedding function concerning the time information. Assume that \(\bm{a}_{i}\), \(\bm{w}_{i}\) and \(\bm{u}_{i}\) are i.i.d. sampled from an underlying distribution \(\rho\). Then, as \(m\to\infty\), we get

\[\bm{s}_{t,\bm{\theta}}(\bm{x})\to\bar{\bm{s}}_{t,\tilde{\bm{ \theta}}}(\bm{x}) :=\mathbb{E}_{(\bm{a},\bm{w},\bm{u})\sim\rho}\big{[}\bm{a}\sigma( \bm{w}^{\top}\bm{x}+\bm{u}^{\top}\bm{e}(t))\big{]}\] \[=\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}}\left[\bm{a}(\bm{w},\bm{ u})\sigma(\bm{w}^{\top}\bm{x}+\bm{u}^{\top}\bm{e}(t))\right],\] (13)

with \(\bm{a}(\bm{w},\bm{u}):=\frac{1}{\rho_{0}(\bm{w},\bm{u})}\int_{\mathbb{R}^{d}} \bm{a}\rho(\bm{a},\bm{w},\bm{u})d\bm{a}\) and \(\rho_{0}(\bm{w},\bm{u}):=\int_{\mathbb{R}^{d}}\rho(\bm{a},\bm{w},\bm{u})d\bm{a}\). By the positive homogeneity property of the ReLU activation, we can assume that \(\|\bm{w}\|_{1}+\|\bm{u}\|_{1}\leq 1\) w.l.o.g.

One can view \(\bar{\bm{s}}_{t,\tilde{\bm{\theta}}}(\bm{x})\) as a continuous version of the random feature model. Correspondingly, the optimal solution is denoted as \(\bar{\bm{\theta}}^{*}\) when replacing the parameterized score function \(\bm{s}_{t,\bm{\theta}}(\bm{x})\) in the loss objective (5) or (7) by \(\bar{\bm{s}}_{t,\tilde{\bm{\theta}}}(\bm{x})\). Define the kernel \(k_{\rho_{0}}(\bm{x},\bm{x}^{\prime}):=\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}} \left[\sigma(\bm{w}^{\top}\bm{x}+\bm{u}^{\top}\bm{e}(t))\sigma(\bm{w}^{\top}\bm{ x}^{\prime}+\bm{u}^{\top}\bm{e}(t))\right]\), and let \(\mathcal{H}_{k_{\rho_{0}}}\) be the induced reproducing kernel Hilbert space (RKHS; [2]), we have \(\bar{\bm{s}}_{t,\tilde{\bm{\theta}}}\in\mathcal{H}_{k_{\rho_{0}}}\) if the RKHS norm \(\big{\|}\bar{\bm{s}}_{t,\tilde{\bm{\theta}}}\big{\|}_{\mathcal{H}_{k_{\rho_{0}}} }^{2}:=\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}}\|\bm{a}(\bm{w},\bm{u})\|_{2}^ {2}=\|\|\bm{a}\|_{2}\|_{2}^{2}(\rho_{0})<\infty\), and the corresponding discrete version can be defined by the empirical average, i.e., \(\|\bm{s}_{t,\bm{\theta}}\|_{\mathcal{H}_{k_{\rho_{0}}}}^{2}:=\frac{1}{m}\|\bm{A} \|_{F}^{2}=\frac{1}{m}\sum_{i=1}^{m}\|\bm{a}(\bm{w}_{i},\bm{u}_{i})\|_{2}^{2}\).

**Remark 1**.: _There are more modern and complex mathematical tools such as neural tangent kernels (NTKs) and mean fields that can be selected as the score networks. Employing these modern tools is valuable at least for theoretical completeness and we leave these as the future work.6_

Footnote 6: For example, the extension to NTKs is possible, since the training of random feature models follows a specific NTK regime (only the last layer is updated) in the output space (instead of the parameter space), which can be properly analyzed in the infinite-width regime.

The goal is to measure and bound the generalization error evolving with the gradient flow training dynamics (9) between the learned distribution and target distribution, using the common Kullback-Leibler (KL) divergence.

**Definition 1** (KL divergence).: _Given two distributions \(p\) and \(q\), the KL divergence from \(q\) to \(p\) is defined as \(D_{\mathrm{KL}}(p||q)=\int_{\mathbb{R}^{d}}p(\bm{x})\log\left(\frac{p(\bm{x})}{q( \bm{x})}\right)d\bm{x}\)._

Based on the above definitions, the generalization gap along the gradient flow training dynamics (9) is formulated as \(D_{\mathrm{KL}}\left(p_{0}||p_{0,\hat{\bm{\theta}}_{n}(\tau)}\right)\), which is only a function of the training time \(\tau\). The goal is to estimate \(D_{\mathrm{KL}}\left(p_{0}||p_{0,\hat{\bm{\theta}}_{n}(\tau)}\right)\).

### Main Results

In this section, we state the main results of the generalization capability of the (score-based) diffusion models and how it evolves as the training proceeds. Based on the formulation in Section 3.1, we theoretically derive several upper bounds to estimate \(D_{\mathrm{KL}}\left(p_{0}\|p_{0,\boldsymbol{\hat{\theta}}_{n}(\tau)}\right)\) under different settings. The results cover both the positive and negative aspects: in the data-independent setting where the target distribution has finite support, the generalization gap is proved to be small; while in the data-dependent setting where the target distribution possesses shift modes, the generalization is adversely affected by the modes' distance.

#### 3.2.1 Data-Independent Generalization Gap

In this section, we provide the characterization of the generalization capability for diffusion models given a target distribution defined on a finite domain. Generally, the KL divergence from the learned distribution \(p_{0,\boldsymbol{\hat{\theta}}_{n}(\tau)}\) at the training time \(\tau\) to the target distribution \(p_{0}\) can be estimated as follows.

**Theorem 1**.: _Suppose that the target distribution \(p_{0}\) is continuously differentiable and has a compact support set, i.e., \(||\boldsymbol{x}||_{\infty}\) is uniformly bounded, and there exists a reproducing kernel Hilbert space (RKHS) \(\mathcal{H}\) (:=\(\mathcal{H}_{k_{\mathcal{H}_{0}}}\)) such that \(\boldsymbol{\bar{s}}_{0,\bar{\boldsymbol{\theta}}}\). \(\in\mathcal{H}\). Assume that the initial loss, trainable parameters, the embedding function \(\mathbf{e}(t)\) and weighting function \(\lambda(t)\) are all bounded. Then for any \(\delta>0\), \(\delta\ll 1\), with the probability of at least \(1-\delta\), we have_

\[D_{\mathrm{KL}}\left(p_{0}\|p_{0,\boldsymbol{\hat{\theta}}_{n}(\tau)}\right) \lesssim\left[\frac{\tau^{4}}{mn}+\frac{\tau^{3}}{m^{2}}+\frac{1}{\tau}\right] +\left[\frac{1}{m}+\bar{\bar{\mathcal{L}}}\left(\boldsymbol{\bar{\theta}}^{ \star}\right)+\tilde{\mathcal{L}}\left(\boldsymbol{\theta}^{\star}\right) \right]+D_{\mathrm{KL}}\left(p_{T}\|\pi\right),\quad\tau\geq 1,\]

_where \(\lesssim\) hides the term \(d\log(d+1)\), the polynomials of \(\log(1/\delta^{2})\), finite RKHS norms and universal positive constants only depending on \(T\)._

**Remark 2**.: _Since \(p_{0}\) is compactly supported, the target score function \(\boldsymbol{s}_{0}(\boldsymbol{x})=\nabla_{\boldsymbol{x}}\log p_{0}( \boldsymbol{x})\) is also defined on a compact domain. According to [12, 13], \(\boldsymbol{s}_{0}\) is contained in the Barron function space with a finite Barron norm, and hence in a certain RKHS with a finite RKHS norm. Therefore, it is reasonable to require that the global minimizer \(\boldsymbol{s}_{0,\boldsymbol{\theta}^{\star}}\) or \(\bar{\boldsymbol{s}}_{0,\boldsymbol{\hat{\theta}}^{\star}}\) is also contained in some RKHS._

Proof sketch.Theorem 1 is proved via the following procedure.

1. According to Theorem 1 in [49], the KL divergence on the left-hand side can be upper bounded by the population loss of the trained model up to a small error. That is, \[D_{\mathrm{KL}}\left(p_{0}\|p_{0,\boldsymbol{\hat{\theta}}_{n}(\tau)}\right) \leq\tilde{\mathcal{L}}(\boldsymbol{\hat{\theta}}_{n}(\tau);g^{2}(\cdot))+D_{ \mathrm{KL}}\left(p_{T}\|\pi\right).\] (14)
2. We use the model trained with respect to the population loss (10) to perform the decomposition: \[\tilde{\mathcal{L}}(\boldsymbol{\hat{\theta}}_{n}(\tau)) =\left[\tilde{\mathcal{L}}(\boldsymbol{\hat{\theta}}_{n}(\tau))- \tilde{\mathcal{L}}(\boldsymbol{\theta}(\tau))\right]+\tilde{\mathcal{L}}( \boldsymbol{\theta}(\tau))\] \[\lesssim\left[\tilde{\mathcal{L}}(\boldsymbol{\hat{\theta}}_{n}( \tau))-\tilde{\mathcal{L}}(\boldsymbol{\theta}(\tau))\right]+\bar{\tilde{ \mathcal{L}}}(\bar{\boldsymbol{\theta}}(\tau))+\text{Monte Carlo}\triangleq I _{1}+I_{2}+I_{3},\] (15) where we omit the weighting function \(g^{2}(\cdot)\) for simplicity. Here, \(\bar{\tilde{\mathcal{L}}}\) is the loss objective obtained by replacing \(\boldsymbol{s}_{t,\boldsymbol{\theta}}\) in \(\tilde{\mathcal{L}}\) (defined in (7)) by \(\bar{\boldsymbol{s}}_{t,\boldsymbol{\theta}}\), and \(I_{3}\) summarizes the resulting Monte Carlo error.
3. \(I_{3}\) can be estimated via a similar argument as in [24] (Lemma 48).
4. \(I_{2}\) can be upper bounded via a standard analysis on the gradient flow dynamics over convex objectives.
5. \(I_{1}\) can be reduced as the norm product of \(\boldsymbol{s}_{0,\boldsymbol{\hat{\theta}}_{n}(\tau)}\), \(\boldsymbol{s}_{0,\boldsymbol{\theta}(\tau)}\) and their gap, then 1. the former can be bounded with a square-root rate growth via a general norm estimate of parameters trained under the gradient flow dynamics; 2. the latter can be estimated by the Rademacher complexity (see e.g., Chapter 26 in [43]).

Combining all above gives the desired result. The detailed proof is found in Appendix A.1.

Discussion on error bounds.The three error terms are further analyzed as follows.

* The first term is the main error, which implies an _early-stopping_ generalization gap. In fact, if one selects an early-stopping time \(\tau_{\text{es}}\) as \(\tau_{\text{es}}=\Theta\left(n^{\frac{2}{5}}\right)\), and let \(m\sim n\), we have \[D_{\mathrm{KL}}\left(p_{0}\|p_{0,\boldsymbol{\theta}_{n}(\tau_{\text{es}})} \right)\lesssim\left(1/n\right)^{\frac{2}{5}}+\left(1/m\right)^{\frac{4}{5}}.\] (16)
* The second term is \(m\)-dependent and corresponds to the approximation error, which is \(o(1)\) when \(m\gg 1\). In fact, the random feature model is a universal approximator to Lipschitz continuous functions on a compact domain (Theorem 6 in [17]). See more details in Appendix A.1 (the last paragraph).
* The third term is exponentially small in \(T\) since \(\pi\) (e.g. the Gaussian density) is log-Sobolev, according to a classical result in e.g. [55] (Theorem 3.20, Theorem 3.24 and Remark 3.26).

**Remark 3**.: _In practice, it is common to use the test error to evaluate the generalization performance. For diffusion models, a straightforward approach is to compute the negative log-likelihood (averaged in bits/dim; equivalent to the KL divergence) on the test dataset during training with the instantaneous change-of-variable formula ([8]) and probability flow ODE (defined in (4)), where the true score function \(\nabla_{\boldsymbol{x}}\log p_{t}(\boldsymbol{x})\) is replaced by \(\boldsymbol{s}_{t,\boldsymbol{\theta}(\tau)}(\boldsymbol{x})\)._

**Remark 4**.: _Previous literature has established similar bounds for bias potential models ([66]) and GANs ([67]). Theorem 1 extends the corresponding results to the setting of diffusion models. Furthermore, this upper bound is finer in the sense that it incorporates the information regarding the model capacity (the hidden dimension \(m\) in this case), which shows that more parameters benefit the learning and generalization, as expected._

#### 3.2.2 Data-Dependent Generalization Gap

In Section 3.2.1, we derive estimates on the generalization error for diffusion models along the training dynamics, where the target distribution is assumed to be finitely supported. In reality, this is often not the case, where target distributions usually possess distant multi-modes, from simple Gaussian mixtures to complicated Boltzmann distributions of physical systems ([29, 30]). Under these settings, the above analysis in Section 3.2.1 can not directly apply since the data domain is unbounded. It remains a problem to quantitatively characterize the generalization behavior of diffusion models given these target distributions with distant multi-modes or modes shift.

To provide a fine-grained demonstration of the generalization capability of diffusion models when applied to learn distributions with distant multi-modes, as an illustrating example, the Gaussian mixture with two modes is selected as the target distribution.

**Theorem 2**.: _Suppose the target distribution \(p_{0}\) is a one-dimensional 2-mode Gaussian mixture: \(p_{0}(x)=q_{1}\mathcal{N}(x;-\mu,1)+q_{2}\mathcal{N}(x;\mu,1)\), where \(\mu>\sqrt{\log(1/\delta^{2})}\), \(q_{1}\), \(q_{2}>0\) with \(q_{1}+q_{2}=1\) are all constants. Under the conditions of Theorem 1 (except the uniform boundness of inputs), we have_

\[D_{\mathrm{KL}}\left(p_{0}\|p_{0,\boldsymbol{\theta}_{n}(\tau)}\right) \lesssim\mathrm{Poly}(\mu)\left[\frac{\tau^{4}}{mn}+\frac{\tau^{3}}{m^{2}} \right]+\frac{1}{\tau}+\left[\frac{\mu^{2}}{m}+\bar{\bar{\mathcal{L}}}\left( \boldsymbol{\theta}^{*}\right)+\tilde{\mathcal{L}}\left(\boldsymbol{\theta}^ {*}\right)\right]+D_{\mathrm{KL}}\left(p_{T}\|\pi\right),\]

_where \(\tau\geq 1\), \(\lesssim\) hides the polynomials of \(\log(1/\delta^{2})\), finite RKHS norms and universal positive constants only depending on \(T\)._

Proof sketch.Theorem 2 is proved following a similar procedure with Theorem 1, except the input data \(x\) does not have a uniform bound here. This problem mainly affects the last step (5 (b)) in the proof sketch of Theorem 1, and can be handled by using the fact

\[|x|\in[\mu-\sqrt{\log(1/\delta^{2})},\mu+\sqrt{\log(1/\delta^{2})}]=\Theta(\mu)\] (17)

given the target Gaussian mixture distribution. The detailed proof is found in Appendix A.2.

**Remark 5**.: _Theorem 2 indicates that, even for a simple target distribution (e.g. a one-dimensional 2-mode Gaussian mixture), the generalization error of diffusion models can be polynomially large regarding the modes' distance. Although Theorem 2 provides only an upper bound, the modes shift effect holds due to the model-target inconsistency (the last paragraph in Appendix A.2) and the following consistent experiments (Section 4.1.2)._

Figure 2: An illustration of modes shift.

Numerical Verifications

In this section, we numerically verify the previous theoretical results and insights (early-stopping generalization and modes shift effect) on both synthetic datasets and real-world datasets.

### Simulations on Synthetic Datasets

#### 4.1.1 Early-Stopping Generalization

First, we illustrate the early-stopping generalization gap established in Theorem 1. We select the one-hidden-layer neural network with Swish activations as the score network, which is trained using the SGD optimizer with a fixed learning rate \(0.5\). The target distribution is set to be a one-dimensional 2-mode Gaussian mixture with the modes' distance equalling 6, and the number of data samples is 1000. We measure the KL divergence from the trained model to the target distribution \(D_{\mathrm{KL}}\left(p_{0}\|p_{0,\theta_{\star}(\tau)}\right)\) along with the training epochs.

From Figure 3, one can observe that the KL divergence achieves its minimum at approximately the \(800\)-th training epoch, and it starts to increase after this turning point. The experimental results are consistent with Theorem 1 (over multiple runs), which states that there exist early-stopping times when diffusion models can generalize well, indicating the effectiveness of our upper bound. Further, the KL divergence begins to oscillate after the minimum point, which may suggest a phase transition in the training dynamics, and the transition point is around the (optimal) early-stopping time.

#### 4.1.2 Modes Shift Effect

Next, we numerically test the relationship between the modes' distance and generalization (density estimation) performance. All the configurations remain the same as Section 4.1.1, except that the target Gaussian mixtures have different modes' distances.

In Figure 4, the modeled distributions exhibit the following two-stage dynamics: (i) first gradually fitting the two modes (epoch \(=100\to 1000\)); (ii) then diverging (epoch \(=1000\to 1900\)). This aligns with the KL divergence dynamics (Figure 3) and again verifies the corresponding theoretical results (Theorem 1). However, Figure 5 shows that when the modes are distant from each other, there is _difficulty_ in the learning process. In Figure 5, the optimal generalization is achieved at epoch \(=100\), but is still far from well generalizing. As the training proceeds, the learned model is almost always a single-mode distribution (epoch \(=1000\), \(1900\)). This phenomenon is aligned with the results established in Theorem 2, which states that when there are distant modes in the target distribution, the generalization performance is relatively poor.

Figure 4: The training dynamics when the distance between two modes is 6 (\(\mu=3\)).

Figure 3: The KL divergence dynamics.

### Simulations on Real-World Datasets

In this subsection, we verify our results on the MNIST dataset using the standard U-net architecture as the score network, which suggests that the adverse effect of modes shift on the generalization performance of diffusion models also appears _in general_.

The setup is as follows. First, we perform a \(K\)-means clustering on \(\mathcal{D}\) (\(\mathcal{D}\) denote the MNIST dataset) to get \(\mathcal{D}=\bigcup_{k=1}^{K}\mathcal{D}_{k}\), and \(\bar{\bm{x}}_{k}\) as the center of \(\mathcal{D}_{k}\), \(k=1,2,\cdots,K\). Let \((i^{*},j^{*}):=\operatorname*{arg\,max}_{i\neq j}\|\bar{\bm{x}}_{i}-\bar{\bm{x }}_{j}\|_{2}\), and \(\mathcal{D}_{\text{farthest}}:=\mathcal{D}_{i^{*}}\bigcup\mathcal{D}_{j^{*}}\). \(\mathcal{D}_{\text{nearest}}\) is similarly constructed by \(\operatorname*{arg\,min}\) indices. Then, by randomly selecting the same number of data samples and using the same configuration, we train two separate diffusion models on \(\mathcal{D}_{\text{farthest}}\) and \(\mathcal{D}_{\text{nearest}}\), respectively, and then perform inference (sampling). The training loss curves and sampling results are shown in Figure 6 and Figure 7, respectively. One can observe a significant performance gap: the diffusion model trained on \(\mathcal{D}_{\text{farthest}}\) appears a higher learning loss and worse sampling quality compared to those of \(\mathcal{D}_{\text{nearest}}\).

Figure 5: The training dynamics when the distance between two modes is 30 (\(\mu=15\)).

Figure 6: The training loss dynamics.

Figure 7: Sampling of the farthest (left) and nearest (right) clusters.

### Discussion

We compare the results developed in this work with former corresponding literature as follows:

* The previous work [21] also studied the adverse effect of modes shift, which particularly reported a contrastive simulation indicating the degraded performance when modeling Gaussian mixtures with the increasing distance between modes (see Figure 2 in [21] and compare with Figure 4 and Figure 5). However, the results therein are established and tested under the "pure" score matching setting, without the denoising or time-dependent dynamics. As a comparison, Theorem 2 establishes a theoretical estimate on the generalization gap for diffusion models that requires _denoising_, and this upper bound _directly_ depends on the distance between modes of target distributions, instead of a circuitous characterization in [21] to attribute the difficulty of learning modes shift to increased isoperimetry of corresponding target distributions.
* Similar adverse effect of modes shift has also been theoretically analyzed and numerically verified on recurrent neural networks (RNNs), see e.g., [23; 24]. There, the modes shift is understood as a type of long-term memory. This is the phenomenon of the "curse of memory": When there is long-term memory in the target, it requires a large number of parameters for the approximation. Meanwhile, the training process will suffer from severe slowdowns. Both of these effects can be exponentially more pronounced with increasing memory or modes shift.
* The very recent work [42] also considered the problem of learning Gaussian mixtures using the denoising diffusion probabilistic model (DDPM) objective, but under a _teacher-student_ setting. That is, given the Gaussian mixtures target, [42] parametrized the score network model in the same form of the score function target, with the goal to identify true parameters. Consequently, there are all positive convergence results developed in [42], despite that the time and sample complexity increases with the distance between modes. As a comparison, Theorem 2 adopts a pre-selected score network model without incorporating any information from the ground truth, and hence establish negative results concerning the modes shift. In fact, if the goal is to identify only true positions of the target Gaussian mixture (this is exactly the setting of [42]), the teacher-student setup seems not necessary (see Figure 5, where the true position is also learned efficiently using the one-hidden-layer Swish neural network as the score network, but the modes are always weighed incorrectly). In addition, [42] did not take the whole denoising dynamics into account. That is, the gradient descent (GD) analysis therein was performed on the denoising score matching objective successively at only two time stages: a larger \(t_{1}\) ("high noise") and a smaller \(t_{2}\) ("low noise"), which is often not the case in practice.

## 5 Conclusion

In this paper, we provide a theoretical analysis of the fundamental generalization aspect of training diffusion models under both the data-independent and data-dependent settings, and early-stopping estimates of the generalization gap along the training dynamics are derived. Quantitatively, the data-independent results indicate a polynomially small generalization error that escapes from the curse of dimensionality, while the data-dependent results suggest the adverse effect of modes shift in target distributions. Numerical simulations have illustrated and verified these theoretical analyses. This work forms a basic starting point for understanding the intricacies of modern deep generative modeling and corresponding central concerns such as memorization, privacy, and copyright arising from practical applications and business products. More broadly, the approach here may have the potential to be extended to other variants in the diffusion models family, including a general SDE-based design space ([20]), consistency models ([48]), rectified flows ([27; 26]), Schrodinger bridges ([59; 56; 10; 44; 47; 25]), etc. These are certainly worthy of future exploration.

## Acknowledgements

We would like to thank Dr. Hongkang Yang for helpful discussions, and all the reviewers' valuable feedback and insightful suggestions to improve this work.

## References

* [1] Brian D. O. Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.
* [2] Nachman Aronszajn. Theory of reproducing kernels. _Transactions of the American Mathematical Society_, 68(3):337-404, 1950.
* [3] Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* [4] Omri Avraham, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18208-18218, 2022.
* [5] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In _USENIX Security Symposium_, pages 5253-5270, 2023.
* [6] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. In _International Conference on Learning Representations_, 2023.
* [7] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. _International Conference on Machine Learning_, 202:4735-4763, 2023.
* [8] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K. Duvenaud. Neural ordinary differential equations. _Advances in Neural Information Processing Systems_, 31:6571-6583, 2018.
* [9] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: Theory for diffusion models with minimal data assumptions. In _International Conference on Learning Representations_, 2023.
* [10] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709, 2021.
* [11] Tim Dockhorn, Tianshi Cao, Arash Vahdat, and Karsten Kreis. Differentially private diffusion models. _Transactions on Machine Learning Research_, 2023.
* [12] Weinan E, Chao Ma, and Lei Wu. A priori estimates of the population risk for two-layer neural networks. _Communications in Mathematical Sciences_, 17(5):1407-1425, 2019.
* [13] Weinan E, Chao Ma, and Lei Wu. The Barron space and the flow-induced function spaces for neural network models. _Constructive Approximation_, 55(1):369-406, 2022.
* [14] Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena, Robert Stanforth, Jamie Hayes, Soham De, Samuel L Smith, Olivia Wiles, and Borja Balle. Differentially private diffusion models generate useful synthetic images. In _International Workshop on Trustworthy Federated Learning in Conjunction with IJCAI_, 2023.
* [15] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and Lingpeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. In _International Conference on Learning Representations_, 2023.
* [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [17] Daniel Hsu, Clayton H. Sanford, Rocco Servedio, and Emmanouil Vasileios Vlatakis-Gkaragkounis. On the approximation power of two-layer networks of random ReLUs. _Conference on Learning Theory_, 134:2423-2461, 2021.

* [18] Hailong Hu and Jun Pang. Membership inference of diffusion models. _arXiv preprint arXiv:2301.09956_, 2023.
* [19] Matthew Jagielski, Om Thakkar, Florian Tramer, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Guha Thakurta, Nicolas Papernot, and Chiyuan Zhang. Measuring forgetting of memorized training examples. In _International Conference on Learning Representations_, 2023.
* [20] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.
* [21] Frederic Koehler, Alexander Heckett, and Andrej Risteski. Statistical efficiency of score matching: The view from isoperimetry. In _International Conference on Learning Representations_, 2023.
* [22] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single image super-resolution with diffusion probabilistic models. _Neurocomputing_, 479:47-59, 2022.
* [23] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. On the curse of memory in recurrent neural networks: Approximation and optimization analysis. In _International Conference on Learning Representations_, 2021.
* [24] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and optimization theory for linear continuous-time recurrent neural networks. _Journal of Machine Learning Research_, 23(42):1-85, 2022.
* [25] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos Theodorou, Weili Nie, and Anima Anandkumar. I\({}^{2}\)SB: Image-to-image Schrodinger bridge. _International Conference on Machine Learning_, 202:22042-22062, 2023.
* [26] Qiang Liu. Rectified flow: A marginal preserving approach to optimal transport. _arXiv preprint arXiv:2209.14577_, 2022.
* [27] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In _International Conference on Learning Representations_, 2023.
* [28] Tomoya Matsumoto, Takayuki Miura, and Naoto Yanai. Membership inference attacks against diffusion models. In _IEEE Security and Privacy Workshops_, pages 77-83, 2023.
* [29] Laurence Illing Midgley, Vincent Stimper, Gregor N. C. Simm, Bernhard Scholkopf, and Jose Miguel Hernandez-Lobato. Flow annealed importance sampling bootstrap. In _International Conference on Learning Representations_, 2023.
* [30] Frank Noe, Simon Olsson, Jonas Kohler, and Hao Wu. Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning. _Science_, 365(6457):eaaw1147, 2019.
* [31] Bernt Oksendal. _Stochastic Differential Equations_. Springer, 2003.
* [32] Jakiw Pidstrigach. Score-based generative models detect manifolds. _Advances in Neural Information Processing Systems_, 35:35852-35865, 2022.
* [33] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: A diffusion probabilistic model for text-to-speech. _International Conference on Machine Learning_, 139:8599-8608, 2021.
* [34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [35] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. _International Conference on Machine Learning_, 139:8857-8868, 2021.

* [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical Image Computing and Computer-Assisted Intervention_, pages 234-241. Springer, 2015.
* [38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* [39] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(4):4713-4726, 2023.
* [40] Simo Sarkka and Arno Solin. _Applied Stochastic Differential Equations_, volume 10. Cambridge University Press, 2019.
* [41] Vikash Sehwag, Caner Hazirbas, Albert Gordo, Firat Ozgenel, and Cristian Canton. Generating high fidelity data from low-density regions using diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11492-11501, 2022.
* [42] Kulin Shah, Sitan Chen, and Adam Klivans. Learning mixtures of gaussians using the DDPM objective. _Advances in Neural Information Processing Systems_, 36, 2023.
* [43] Shai Shalev-Shwartz and Shai Ben-David. _Understanding Machine Learning: From Theory to Algorithms_. Cambridge University Press, 2014.
* [44] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion Schrodinger bridge matching. _Advances in Neural Information Processing Systems_, 36, 2023.
* [45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. _International Conference on Machine Learning_, 37:2256-2265, 2015.
* [46] Gowthami Sompealli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6048-6058, 2023.
* [47] Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, and Charlotte Bunne. Aligned diffusion Schrodinger bridges. _Conference on Uncertainty in Artificial Intelligence_, 216:1985-1995, 2023.
* [48] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. _International Conference on Machine Learning_, 202:32211-32252, 2023.
* [49] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. _Advances in Neural Information Processing Systems_, 34:1415-1428, 2021.
* [50] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in Neural Information Processing Systems_, 32, 2019.
* [51] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. _Advances in Neural Information Processing Systems_, 33:12438-12448, 2020.
* [52] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. _Conference on Uncertainty in Artificial Intelligence_, 115:574-584, 2020.

* [53] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solving inverse problems in medical imaging with score-based generative models. In _International Conference on Learning Representations_, 2022.
* [54] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021.
* [55] Ramon Van Handel. Probability in high dimension. _Lecture Notes (Princeton University)_, 2014.
* [56] Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving Schrodinger bridges via maximum likelihood. _Entropy_, 23(9):1134, 2021.
* [57] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural Computation_, 23(7):1661-1674, 2011.
* [58] Nikhil Vyas, Sham M. Kakade, and Boaz Barak. On provable copyright protection for generative models. _International Conference on Machine Learning_, 202:35277-35299, 2023.
* [59] Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep generative learning via Schrodinger bridge. _International Conference on Machine Learning_, 139:10794-10804, 2021.
* [60] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7623-7633, 2023.
* [61] Lei Wu and Weijie J. Su. The implicit regularization of dynamical stability in stochastic gradient descent. _International Conference on Machine Learning_, 202:37656-37684, 2023.
* [62] Yixin Wu, Ning Yu, Zheng Li, Michael Backes, and Yang Zhang. Membership inference attacks against text-to-image generation models. _arXiv preprint arXiv:2210.00968_, 2022.
* [63] Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, and Tommi S. Jaakkola. Crystal diffusion variational autoencoder for periodic material generation. In _International Conference on Learning Representations_, 2022.
* [64] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In _International Conference on Learning Representations_, 2022.
* [65] Hongkang Yang. A mathematical framework for learning probability distributions. _Journal of Machine Learning_, 1(4):373-431, 2022.
* [66] Hongkang Yang and Weinan E. Generalization and memorization: The bias potential model. _Mathematical and Scientific Machine Learning_, 145:1013-1043, 2022.
* [67] Hongkang Yang and Weinan E. Generalization error of GAN from the discriminator's perspective. _Research in the Mathematical Sciences_, 9(1):1-31, 2022.
* [68] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion probabilistic modeling for video generation. _Entropy_, 25(10):1469, 2023.
* [69] Jongmin Yoon, Sung Ju Hwang, and Juho Lee. Adversarial purification with score-based generative models. _International Conference on Machine Learning_, 139:12062-12072, 2021.
* [70] Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramer, and Nicholas Carlini. Counterfactual memorization in neural language models. _Advances in Neural Information Processing Systems_, 36, 2023.

Technical Results and Proofs

### Data-Independent Generalization Gap

To derive the theorem for the generalization error of this score-based generative model, we first give the following lemmas.

**Lemma 1** (Forward perturbation estimates).: _Consider the forward diffusion process with the linear drift coefficient (2). For any \(\delta>0\), \(\delta\ll 1\), with the probability of at least \(1-\delta\), we have_

\[\|\bm{x}(t)\|_{\infty}\lesssim C_{T}\left(\|\bm{x}(0)\|_{\infty}+\sqrt{\log(1/ \delta^{2})}\right),\] (18)

_where \(C_{T}:=\max\limits_{t\in[0,T]}\left\{r(t),r(t)v(t)\right\}\)._

Proof.: When the drift coefficient \(\bm{f}(\cdot,t):\mathbb{R}^{d}\mapsto\mathbb{R}^{d}\) is linear to \(\bm{x}\), i.e., \(\bm{f}(\bm{x},t)=f(t)\bm{x}\), the transition kernel \(p_{t|0}\) has a closed form (8)

\[p_{t|0}(\bm{x}(t)|\bm{x}(0))=\mathcal{N}(\bm{x}(t);r(t)\bm{x}(0),r^{2}(t)v^{2}( t)\bm{I}_{d})\] (19)

with \(r(t):=e^{\int_{0}^{t}f(\zeta)d\zeta}\), \(v(t):=\sqrt{\int_{0}^{t}\frac{g^{2}(\zeta)}{r^{2}(\zeta)}d\zeta}\). Hence, we have

\[\bm{x}(t)=r(t)\bm{x}(0)+r(t)v(t)\bm{z},\quad\bm{z}\sim\mathcal{N}(\bm{0},\bm{ I}_{d}).\] (20)

For any \(\epsilon\sim\mathcal{N}(0,1)\), \(c>1\), we have

\[\mathbb{P}\{\epsilon:|\epsilon|>c\} =2\int_{c}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}dx\] \[\leq\frac{1}{\sqrt{2\pi}}\int_{c}^{+\infty}2xe^{-x^{2}/2}dx= \frac{1}{\sqrt{2\pi}}\int_{c^{2}}^{+\infty}e^{-x/2}dx=\sqrt{\frac{2}{\pi}}e^{ -c^{2}/2}.\]

Let \(\delta=\Theta(e^{-c^{2}/2})\), we get

\[\mathbb{P}\{\epsilon:|\epsilon|\leq\sqrt{\log(1/\delta^{2})}\}\geq 1-\delta.\] (21)

Hence, for any \(\delta\in(0,1)\) with \(\delta\ll 1\), with the probability of at least \(1-\delta\), we have

\[\|\bm{x}(t)\|_{\infty}\lesssim C_{T}\left(\|\bm{x}(0)\|_{\infty}+\sqrt{\log(1 /\delta^{2})}\right)\] (22)

with \(C_{T}:=\max\limits_{t\in[0,T]}\left\{r(t),r(t)v(t)\right\}\). The proof is completed. 

**Lemma 2** (Theorem 1 in [49]).: _We have_

\[D_{\mathrm{KL}}\left(p_{0}\|p_{0,\bm{\theta}_{n}(\tau)}\right)\leq\tilde{ \mathcal{L}}(\hat{\bm{\theta}}_{n}(\tau);g^{2}(\cdot))+D_{\mathrm{KL}}\left(p _{T}\|\pi\right).\]

**Lemma 3**.: _Both of the loss objectives \(\tilde{\mathcal{L}}(\bm{\theta};\lambda(\cdot))\) and \(\bar{\tilde{\mathcal{L}}}(\bar{\bm{\theta}};\lambda(\cdot))\) are quadratic (and hence convex)._

Proof.: The convexity arises from the following points: (i) The score matching loss objectives \(\tilde{\mathcal{L}}(\bm{\theta};\lambda(\cdot))\) (defined in (7)) and \(\bar{\tilde{\mathcal{L}}}(\bar{\bm{\theta}};\lambda(\cdot))\) are \(L^{2}\)-metrics between the score network model and target score function; (ii) The score networks are defined as random feature models (see (12) and (13)) that are linear to trainable parameters. Therefore, using some trace techniques and basic variational calculations, it is not hard to derive the fact that the loss objectives are quadratic and hence convex with respect to trainable parameters.

(1) For \(\tilde{\mathcal{L}}(\bm{\theta};\lambda(\cdot))\), recall that \(\bm{s}_{t,\bm{\theta}}(\bm{x}(t))=\frac{1}{m}\bm{A}\sigma(\bm{W}\bm{x}(t)+ \bm{U}\bm{e}(t))\), and let \(\bm{s}_{t}(\bm{x}(t)):=\nabla_{\bm{x}(t)}\log p_{t}(\bm{x}(t))\), \(\bm{h}_{1}(\bm{x},t):=(\sqrt{\lambda(t)}/\sqrt{m})\sigma(\bm{W}\bm{x}+\bm{U} \bm{e}(t))\), \(\bm{h}_{2}(\bm{x},t):=\sqrt{\lambda(t)}\bm{s}_{t}(\bm{x})\), we have

\[\tilde{\mathcal{L}}(\bm{\theta};\lambda(\cdot)) =\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t)\sim p_{t }}\big{[}\bm{h}_{1}^{\top}(\bm{x}(t),t)(\bm{A}/\sqrt{m})^{\top}(\bm{A}/\sqrt{ m})\bm{h}_{1}(\bm{x}(t),t)\] \[\quad\quad-2\bm{h}_{2}^{\top}(\bm{x}(t),t)(\bm{A}/\sqrt{m})\bm{h} _{1}(\bm{x}(t),t)\big{]}+\text{constant}.\]Since for any \(\bm{h},\bar{\bm{h}},\bm{B}\), we have

\[\mathbb{E}_{t}\mathbb{E}_{\bm{x}(t)}[\bm{h}^{\top}(\bm{x}(t),t)\bm{B }\bar{\bm{h}}(\bm{x}(t),t)] =\mathbb{E}_{t}\mathbb{E}_{\bm{x}(t)}[\text{trace}(\bm{B}\bar{\bm{ h}}(\bm{x}(t),t)\bm{h}^{\top}(\bm{x}(t),t))]\] \[=\text{trace}(\bm{B}\mathbb{E}_{t}\mathbb{E}_{\bm{x}(t)}[\bar{\bm {h}}(\bm{x}(t),t)\bm{h}^{\top}(\bm{x}(t),t)]),\]

we further get

\[\tilde{\mathcal{L}}(\bm{\theta};\lambda(\cdot))=\frac{1}{m}\text{trace}(\bm {A}^{\top}\bm{A}\bm{B}_{1})-\frac{2}{\sqrt{m}}\text{trace}(\bm{A}\bm{B}_{2})+ \text{constant},\]

where

\[\bm{B}_{1}:=\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x} (t)\sim p_{t}}[\bm{h}_{1}(\bm{x}(t),t)\bm{h}_{1}^{\top}(\bm{x}(t),t)],\quad \bm{B}_{2}:=\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t)\sim p_{t }}[\bm{h}_{1}(\bm{x}(t),t)\bm{h}_{2}^{\top}(\bm{x}(t),t)].\]

Here, \(\bm{B}_{1}\) is a positive semi-definite matrix, since \(\bm{v}^{\top}\bm{B}_{1}\bm{v}=\mathbb{E}_{t}\mathbb{E}_{\bm{x}(t)}[(\bm{v}^{ \top}\bm{h}_{1}(\bm{x}(t),t))^{2}]\geq 0\) for any \(\bm{v}\). Notice that for any \(\bm{A},\bm{B}\),

\[\text{trace}(\bm{A}^{\top}\bm{A}\bm{B}) =\text{trace}(\bm{A}\bm{B}\bm{A}^{\top})=\sum_{i,j}\bm{B}_{ij}(\bm {A}_{:,j})^{\top}\bm{A}_{:,i}=\text{vec}(\bm{A})^{\top}(\bm{B}\otimes\bm{I}) \text{vec}(\bm{A}),\] \[\text{trace}(\bm{A}\bm{B}) =\sum_{j}(\bm{A}_{:,j})^{\top}(\bm{B}^{\top})_{:,j}=\text{vec}( \bm{A})^{\top}\text{vec}(\bm{B}^{\top}),\]

where \(\otimes\) denotes the Kronecker product. Hence

\[\tilde{\mathcal{L}}(\bm{\theta};\lambda(\cdot))=\frac{1}{m}\text{vec}(\bm{A} )^{\top}(\bm{B}_{1}\otimes\bm{I})\text{vec}(\bm{A})-\frac{2}{\sqrt{m}}\text{ vec}(\bm{B}_{2}^{\top})^{\top}\text{vec}(\bm{A})+\text{constant}\] (23)

is a quadratic function. It is straightforward to show that the eigenvalues of \(\bm{B}_{1}\otimes\bm{I}\) are the same as \(\bm{B}_{1}\) but with multiplicity,7

Footnote 7: In fact, if \(\bm{A}\in\mathbb{R}^{n\times n}\), \(\bm{B}\in\mathbb{R}^{m\times m}\) have the eigenvalues \(\{\nu_{i}\}_{i=1}^{n}\), \(\{\mu_{j}\}_{j=1}^{m}\), respectively, the eigenvalues of \(\bm{A}\otimes\bm{B}\) are \(\nu_{i}\mu_{j}\), \(i=1,\cdots,n\), \(j=1,\cdots,m\). This is due to the following argument. By JordanChevalley decomposition, there exist invertible matrices \(\bm{P},\bm{Q}\) such that \(\bm{A}=\bm{P}\bm{\Lambda}\bm{P}^{-1}\), \(\bm{B}=\bm{Q}\bm{\Delta}\bm{Q}^{-1}\), where \(\bm{\Lambda},\bm{\Delta}\) are upper triangular matrices. Therefore, we have

\[\bm{A}\otimes\bm{B}=(\bm{P}\bm{\Lambda}\bm{P}^{-1})\otimes(\bm{Q}\bm{\Delta} \bm{Q}^{-1})=(\bm{P}\otimes\bm{Q})(\bm{\Lambda}\otimes\bm{\Delta})(\bm{P}^{-1 }\otimes\bm{Q}^{-1})=(\bm{P}\otimes\bm{Q})(\bm{\Lambda}\otimes\bm{\Delta})( \bm{P}\otimes\bm{Q})^{-1}.\]

 That is, the matrices \(\bm{A}\otimes\bm{B}\) and \(\bm{\Lambda}\otimes\bm{\Delta}\) are similar. Notice that \(\bm{\Lambda}\otimes\bm{\Delta}\) is still an upper triangular matrix with diagonal elements \(\nu_{i}\mu_{j}\), \(i=1,\cdots,n\), \(j=1,\cdots,m\), we obtain the desired result.

\[\nabla^{2}_{\bm{\theta}}\tilde{\mathcal{L}}(\bm{\theta};\lambda(\cdot))=\nabla ^{2}_{\text{vec}(\bm{A})}\tilde{\mathcal{L}}(\bm{\theta};\lambda(\cdot))=2(\bm {B}_{1}\otimes\bm{I})\]

is positive semi-definite, i.e., the loss is convex with respect to trainable parameters.

(2) For \(\bar{\bar{\bar{\mathcal{L}}}}(\bm{\bar{\theta}};\lambda(\cdot))\), notice that

\[\|\bm{\bar{s}}_{t,\bm{\bar{\theta}}}(\bm{x})\|_{2}^{2} =\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}}\left[\sigma(\bm{w}^{\top }\bm{x}+\bm{u}^{\top}\bm{e}(t))\bm{a}^{\top}(\bm{w},\bm{u})\right]\mathbb{E} _{(\bm{w}^{\prime},\bm{u}^{\prime})\sim\rho_{0}}\left[\bm{a}(\bm{w}^{\prime}, \bm{u}^{\prime})\sigma(\bm{w}^{\prime\top}\bm{x}+\bm{u}^{\prime\top}\bm{e}(t)) \right]\] \[=\mathbb{E}_{(\bm{w},\bm{u}),(\bm{w}^{\prime},\bm{u}^{\prime})\sim \rho_{0}}\left[\bm{a}^{\top}(\bm{w},\bm{u})\sigma(\bm{w}^{\top}\bm{x}+\bm{u}^ {\top}\bm{e}(t))\sigma(\bm{w}^{\prime\top}\bm{x}+\bm{u}^{\prime\top}\bm{e}(t)) \bm{a}(\bm{w}^{\prime},\bm{u}^{\prime})\right].\]

Let \(\bm{v}:=(\bm{w},\bm{u})\), \(\bm{v}^{\prime}:=(\bm{w}^{\prime},\bm{u}^{\prime})\), \(\bm{z}(t):=(\bm{x}^{\top}(t),\bm{e}^{\top}(t))^{\top}\), we get

\[\bar{\bar{\mathcal{L}}}(\bm{\bar{\theta}};\lambda(\cdot)) =\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t)\sim p_{t} }\left[\lambda(t)\left(\|\bm{\bar{s}}_{t,\bm{\bar{\theta}}}(\bm{x}(t))\|_{2}^ {2}-2\bm{s}_{t}^{\top}(\bm{x}(t))\bm{\bar{s}}_{t,\bm{\bar{\theta}}}(\bm{x}(t))+ \|\bm{s}_{t}(\bm{x}(t))\|_{2}^{2}\right)\right]\] \[\quad-2\bm{s}_{t}^{\top}(\bm{x}(t))\mathbb{E}_{\bm{v}\sim p_{t} }\left[\bm{a}(\bm{v})\sigma(\bm{v}^{\top}\bm{z}(t))\sigma(\bm{v}^{\prime\top} \bm{z}(t))\sigma(\bm{v}^{\prime\top}\bm{z}(t))\bm{a}(\bm{v}^{\prime})\right]\] \[\quad-2\bm{s}_{t}^{\top}(\bm{x}(t))\mathbb{E}_{\bm{v}\sim\rho_{0} }\left[\bm{a}(\bm{v})\sigma(\bm{v}^{\top}\bm{z}(t))\right)\right]+\text{constant}\] \[=\mathbb{E}_{\bm{v},\bm{v}^{\prime}\sim\rho_{0}}\left[\bm{a} ^{\top}(\bm{v})\big{(}\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t) \sim p_{t}}\big{[}\lambda(t)\sigma(\bm{v}^{\top}\bm{z}(t))\sigma(\bm{v}^{\prime \top}\bm{z}(t))\big{]}\big{)}\bm{a}(\bm{v}^{\prime})\right]\] \[\quad-2\mathbb{E}_{\bm{v}\sim\rho_{0}}\big{[}\big{(}\mathbb{E}_{t \sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t)\sim p_{t}}\big{[}\lambda(t)\bm{s}_{t }^{\top}(\bm{x}(t))\sigma(\bm{v}^{\top}\bm{z}(t))\big{)}\big{]}\bm{a}(\bm{v} )\big{]}+\text{constant}.\]Then for any \(\bm{\phi}\in L^{2}(\rho_{0})\), by symmetry we have

\[\delta\bar{\bar{\mathcal{L}}}(\cdot;\lambda(\cdot))[\bm{\bar{\theta}}, \bm{\phi}] =\lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\bar{\bar{\mathcal{L}}}( \bm{\bar{\theta}}+\epsilon\bm{\phi};\lambda(\cdot))-\bar{\bar{\mathcal{L}}}( \bm{\bar{\theta}};\lambda(\cdot))\right)\] \[=\mathbb{E}_{\bm{v},\bm{v}^{\prime}\sim\rho_{0}}\big{[}\bm{ \phi}^{\top}(\bm{v})\big{(}\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x }(t)\sim p_{t}}\big{[}\lambda(t)\sigma(\bm{v}^{\top}\bm{z}(t))\sigma(\bm{v}^{ \prime\top}\bm{z}(t))\big{]}\bm{a}(\bm{v}^{\prime})\] \[\quad+\bm{a}^{\top}(\bm{v})\big{(}\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t)\sim p_{t}}\big{[}\lambda(t)\sigma(\bm{v}^{\top}\bm{z} (t))\sigma(\bm{v}^{\prime\top}\bm{z}(t))\big{]}\big{)}\bm{\phi}(\bm{v}^{\prime })\big{]}\] \[\quad-2\mathbb{E}_{\bm{v}\sim\rho_{0}}\big{[}\big{(}\mathbb{E}_{ t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t)\sim p_{t}}\big{[}\lambda(t) \bm{s}_{t}^{\top}(\bm{x}(t))\sigma(\bm{v}^{\top}\bm{z}(t))\big{]}\big{)}\bm{ \phi}(\bm{v})\big{]}\] \[=2\mathbb{E}_{\bm{v},\bm{v}^{\prime}\sim\rho_{0}}\big{[}\bm{a}^{ \top}(\bm{v}^{\prime})(\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t) \sim p_{t}}\big{[}\lambda(t)\sigma(\bm{v}^{\top}\bm{z}(t))\sigma(\bm{v}^{\prime \top}\bm{z}(t))\big{]}\big{)}\bm{\phi}(\bm{v})\big{]}\] \[\quad-2\mathbb{E}_{\bm{v}\sim\rho_{0}}\big{[}\big{(}\mathbb{E}_{ t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t)\sim p_{t}}\big{[}\lambda(t) \bm{s}_{t}^{\top}(\bm{x}(t))\sigma(\bm{v}^{\top}\bm{z}(t))\big{]}\big{)}\bm{ \phi}(\bm{v})\big{]}\] \[=2\big{\langle}\mathbb{E}_{\bm{v}^{\prime}\sim\rho_{0}}\big{[}K( \bm{v},\bm{v}^{\prime};\lambda(\cdot))\bm{a}(\bm{v}^{\prime})\big{]}-\bm{k}( \bm{v};\lambda(\cdot)),\bm{\phi}(\bm{v})\big{\rangle}_{L^{2}(\rho_{0})},\]

where

\[K(\bm{v},\bm{v}^{\prime};\lambda(\cdot)) :=\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t)\sim p_ {t}}\big{[}\lambda(t)\sigma(\bm{v}^{\top}\bm{z}(t))\sigma(\bm{v}^{\prime\top }\bm{z}(t))\big{]},\] \[\bm{k}(\bm{v};\lambda(\cdot)) :=\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t)\sim p_ {t}}\big{[}\lambda(t)\bm{s}_{t}(\bm{x}(t))\sigma(\bm{v}^{\top}\bm{z}(t))\big{]}.\]

This yields

\[\frac{\delta\bar{\bar{\mathcal{L}}}(\cdot;\lambda(\cdot))}{\delta\bm{\bar{ \theta}}}=2\mathbb{E}_{\bm{v}^{\prime}\sim\rho_{0}}\big{[}K(\bm{v},\bm{v}^{ \prime};\lambda(\cdot))\bm{a}(\bm{v}^{\prime})\big{]}-2\bm{k}(\bm{v};\lambda( \cdot)),\] (24)

and

\[=\mathbb{E}_{\bm{v},\bm{v}^{\prime}\sim\rho_{0}}\big{[}\bm{a}_{ 1}^{\top}(\bm{v})K(\bm{v},\bm{v}^{\prime};\lambda(\cdot))\bm{a}_{1}(\bm{v}^{ \prime})\big{]}-2\mathbb{E}_{\bm{v}\sim\rho_{0}}\big{[}\bm{k}^{\top}(\bm{v}; \lambda(\cdot))\bm{a}_{1}(\bm{v})\big{]}\] \[\quad+2\langle\mathbb{E}_{\bm{v}^{\prime}\sim\rho_{0}}\big{[}K( \bm{v},\bm{v}^{\prime};\lambda(\cdot))\bm{a}_{1}(\bm{v}^{\prime})\big{]}-\bm{k }(\bm{v};\lambda(\cdot)),\bm{a}_{2}(\bm{v})-\bm{a}_{1}(\bm{v})\rangle_{L^{2}( \rho_{0})}\] \[\quad-\mathbb{E}_{\bm{v},\bm{v}^{\prime}\sim\rho_{0}}\big{[}\bm{a }_{2}^{\top}(\bm{v})K(\bm{v},\bm{v}^{\prime};\lambda(\cdot))\bm{a}_{2}(\bm{v}^ {\prime})\big{]}+2\mathbb{E}_{\bm{v}\sim\rho_{0}}\big{[}\bm{k}^{\top}(\bm{v}; \lambda(\cdot))\bm{a}_{2}(\bm{v})\big{]}\] \[=\,-\mathbb{E}_{\bm{v},\bm{v}^{\prime}\sim\rho_{0}}\big{[}(\bm{a }_{2}(\bm{v})-\bm{a}_{1}(\bm{v}))^{\top}K(\bm{v},\bm{v}^{\prime};\lambda( \cdot))(\bm{a}_{2}(\bm{v}^{\prime})-\bm{a}_{1}(\bm{v}^{\prime}))\big{]}\] \[=\,-\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t)\sim p_ {t}}\big{[}\lambda(t)\mathbb{E}_{\bm{v},\bm{v}^{\prime}\sim\rho_{0}}\big{[}( \bm{a}_{2}(\bm{v})-\bm{a}_{1}(\bm{v}))^{\top}\sigma(\bm{v}^{\top}\bm{z}(t)) \sigma(\bm{v}^{\prime\top}\bm{z}(t))(\bm{a}_{2}(\bm{v}^{\prime})-\bm{a}_{1}( \bm{v}^{\prime}))\big{]}\big{]}\] \[=\,-\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t)\sim p_ {t}}\big{[}\lambda(t)\left\|\mathbb{E}_{\bm{v}\sim\rho_{0}}\big{[}(\bm{a}_{2}( \bm{v})-\bm{a}_{1}(\bm{v}))\sigma(\bm{v}^{\top}\bm{z}(t))\big{]}\right\|_{2}^ {2}\big{]}\leq 0,\]

hence the functional \(\bar{\bar{\mathcal{L}}}(\bm{\bar{\theta}};\lambda(\cdot))\) is convex with respect to \(\bm{\bar{\theta}}\) (given any positive weighting function \(\lambda(\cdot)\)). The proof is completed. 

Since the weighting function is fixed in our analysis, we omit the notation \(\lambda(\cdot)\) without ambiguity in the following contents.

**Lemma 4**.: _For any \(\tau>0\) and \(\bm{\theta},\bm{\bar{\theta}}\), we have_

\[\bar{\bar{\mathcal{L}}}\left(\bm{\bar{\theta}}(\tau)\right)-\bar{\bar{\mathcal{L}}} \left(\bm{\bar{\theta}}\right)\lesssim\frac{\big{\|}\bm{\bar{s}}_{0,\bm{\bar{ \theta}}_{0}}\big{\|}_{\mathcal{H}}^{2}+\big{\|}\bm{\bar{s}}_{0,\bm{\bar{\theta}} }\big{\|}_{\mathcal{H}}^{2}}{\tau},\quad\bar{\mathcal{L}}\left(\bm{\theta}( \tau)\right)-\bar{\mathcal{L}}\left(\bm{\theta}\right)\lesssim\frac{\|\bm{s}_{0, \bm{\theta}_{0}}\|_{\mathcal{H}}^{2}+\|\bm{s}_{0,\bm{\theta}}\|_{\mathcal{H}}^ {2}}{\tau}.\]

Proof.: (1) For the loss objective \(\bar{\bar{\mathcal{L}}}\), we define the Lyapunov function

\[\bar{E}(\tau):=\tau\left(\bar{\bar{\mathcal{L}}}\left(\bm{\bar{\theta}}(\tau) \right)-\bar{\bar{\mathcal{L}}}\left(\bm{\bar{\theta}}\right)\right)+\frac{1}{ 2}\left\|\bm{a}_{\tau}-\bm{a}\right\|_{L^{2}(\rho_{0})}^{2}.\]

Then, we have

\[\frac{d}{d\tau}\bar{E}(\tau) =\Big{(}\bar{\bar{\mathcal{L}}}\left(\bm{\bar{\theta}}(\tau)\right)- \bar{\bar{\mathcal{L}}}\left(\bm{where the last inequality holds since

\[\frac{d}{d\tau}\bar{\bar{\mathcal{L}}}\left(\bar{\bm{\theta}}(\tau)\right)=\left\langle \frac{\delta\bar{\bar{\mathcal{L}}}}{\delta\bar{\bm{\theta}}}\right|_{\bar{\bm {\theta}}=\bar{\bm{\theta}}(\tau)},\frac{d}{d\tau}\bar{\bm{\theta}}(\tau) \right\rangle_{L^{2}(\rho_{0})}=-\left\langle\frac{\delta\bar{\bar{\mathcal{L }}}}{\delta\bar{\bm{\theta}}}\right|_{\bar{\bm{\theta}}=\bar{\bm{\theta}}(\tau )},\frac{\delta\bar{\bar{\mathcal{L}}}}{\delta\bar{\bm{\theta}}}\bigg{|}_{\bar {\bm{\theta}}=\bar{\bm{\theta}}(\tau)}\right\rangle_{L^{2}(\rho_{0})}\leq 0.\]

By convexity, for any \(\tau_{1},\tau_{2}\), it holds that

\[\bar{\bar{\mathcal{L}}}\left(\bar{\bm{\theta}}(\tau_{1})\right)+\left\langle \bm{a}_{\tau_{2}}-\bm{a}_{\tau_{1}},\frac{\delta\bar{\bar{\mathcal{L}}}}{ \delta\bar{\bm{\theta}}}\right|_{\bar{\bm{\theta}}=\bar{\bm{\theta}}(\tau_{1} )}\right\rangle_{L^{2}(\rho_{0})}\leq\bar{\bar{\mathcal{L}}}\left(\bar{\bm{ \theta}}(\tau_{2})\right),\]

hence \(\frac{d}{d\tau}\bar{E}(\tau)\leq 0\). We conclude that \(\bar{E}(\tau)\leq\bar{E}(0)\), or equivalently

\[\tau\left(\bar{\bar{\mathcal{L}}}\left(\bar{\bm{\theta}}(\tau)\right)-\bar{ \bar{\mathcal{L}}}\left(\bar{\bm{\theta}}\right)\right)+\frac{1}{2}\left\|\bm {a}_{\tau}-\bm{a}\right\|_{L^{2}(\rho_{0})}^{2}\leq\frac{1}{2}\left\|\bm{a}_{ 0}-\bm{a}\right\|_{L^{2}(\rho_{0})}^{2}.\]

Therefore, note that \(\left\|\bar{\bm{s}}_{0,\bar{\bm{\theta}}}\right\|_{\mathcal{H}}^{2}:=\mathbb{ E}_{\rho_{0}}[\left\|\bm{a}\right\|_{2}^{2}]\) (let \(\mathcal{H}:=\mathcal{H}_{k_{\rho_{0}}}\)), we obtain

\[\bar{\bar{\mathcal{L}}}\left(\bar{\bm{\theta}}(\tau)\right)-\bar{\bar{\mathcal{ L}}}\left(\bar{\bm{\theta}}\right)\lesssim\frac{\left\|\bm{a}_{0}-\bm{a} \right\|_{L^{2}(\rho_{0})}^{2}}{\tau}\lesssim\frac{\left\|\bar{\bm{s}}_{0, \bar{\bm{\theta}}_{0}}\right\|_{\mathcal{H}}^{2}+\left\|\bar{\bm{s}}_{0,\bar{ \bm{\theta}}}\right\|_{\mathcal{H}}^{2}}{\tau},\]

which gives the desired estimate.

(2) For the loss objective \(\bar{\mathcal{L}}\), the argument is almost the same, except replacing the Lyapunov function by

\[E(\tau):=\tau\left(\bar{\mathcal{L}}\left(\bm{\theta}(\tau)\right)-\bar{ \mathcal{L}}\left(\bm{\theta}\right)\right)+\frac{1}{2m}\left\|\bm{A}_{\tau}- \bm{A}\right\|_{F}^{2},\]

and \(\langle\cdot,\cdot\rangle_{L^{2}(\rho_{0})}\) by \(\langle\cdot,\cdot\rangle\). Note that \(\bm{\theta}=\text{vec}(\bm{A})/\sqrt{m}\) and \(\left\|\bm{s}_{0,\bm{\theta}}\right\|_{\mathcal{H}}^{2}=\left\|\bm{A}\right\|_ {F}^{2}/m\), we obtain

\[\bar{\mathcal{L}}\left(\bm{\theta}(\tau)\right)-\bar{\mathcal{L}}\left(\bm{ \theta}\right)\lesssim\frac{\left\|\bm{A}_{0}-\bm{A}\right\|_{F}^{2}}{m\tau} \lesssim\frac{\left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}^{2}+ \left\|\bm{s}_{0,\bm{\theta}}\right\|_{\mathcal{H}}^{2}}{\tau},\]

which completes the proof. 

**Lemma 5**.: _Suppose that the loss objectives \(\bar{\mathcal{L}}\), \(\bar{\mathcal{L}}^{(n)}\), \(\bar{\bar{\mathcal{L}}}\), \(\bar{\bar{\mathcal{L}}}^{(n)}\) are bounded at the initialization, then for any \(\tau>0\), we have_

\[\left\|\bm{s}_{0,\bm{\theta}(\tau)}\right\|_{\mathcal{H}},\left\|\bm{s}_{0, \bar{\bm{\theta}}_{n}(\tau)}\right\|_{\mathcal{H}}\lesssim\left\|\bm{s}_{0, \bm{\theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\frac{\tau}{m}},\quad\left\|\bar{ \bm{s}}_{0,\bar{\bm{\theta}}(\tau)}\right\|_{\mathcal{H}},\left\|\bar{\bm{s}}_ {0,\bar{\bm{\theta}}_{n}(\tau)}\right\|_{\mathcal{H}}\lesssim\left\|\bar{\bm{ s}}_{0,\bar{\bm{\theta}}_{0}}\right\|_{\mathcal{H}}+\sqrt{\tau}.\]

Proof.: It's sufficient to prove \(\left\|\bar{\bm{s}}_{0,\bar{\bm{\theta}}(\tau)}\right\|_{\mathcal{H}}\lesssim \sqrt{\tau}\), and the rest part follows similarly.

Since

\[\left\|\bm{a}_{\tau}\right\|_{L^{2}(\rho_{0})}\frac{d}{d\tau}\left\|\bm{a}_{\tau} \right\|_{L^{2}(\rho_{0})}=\frac{1}{2}\frac{d}{d\tau}\left\|\bm{a}_{\tau} \right\|_{L^{2}(\rho_{0})}^{2}=\left\langle\bm{a}_{\tau},\frac{d}{d\tau}\bm{a}_{ \tau}\right\rangle_{L^{2}(\rho_{0})}=\left\langle\bm{a}_{\tau},-\frac{\delta \bar{\bar{\mathcal{L}}}}{\delta\bar{\bm{\theta}}}\right|_{\bar{\bm{\theta}}=\bar{ \bm{\theta}}(\tau)}\right\rangle_{L^{2}(\rho_{0})},\]

applying Cauchy-Schwartz inequality yields

\[\frac{d}{d\tau}\left\|\bm{a}_{\tau}\right\|_{L^{2}(\rho_{0})} =\left\langle\frac{\bm{a}_{\tau}}{\left\|\bm{a}_{\tau}\right\|_{L ^{2}(\rho_{0})}},-\frac{\delta\bar{\bar{\mathcal{L}}}}{\delta\bar{\bm{\theta}}} \right|_{\bar{\bm{\theta}}=\bar{\bm{\theta}}(\tau)}\right\rangle_{L^{2}(\rho_{0 })}\] \[\leq\left\|\frac{\delta\bar{\bar{\mathcal{L}}}}{\delta\bar{\bm{ \theta}}}\right\|_{\bar{\bm{\theta}}=\bar{\bm{\theta}}(\tau)}\right\|_{L^{2}( \rho_{0})}=\sqrt{-\frac{d}{d\tau}\bar{\bar{\mathcal{L}}}\left(\bar{\bm{\theta}} \left(\tau\right)\right)}.\]

Thus, again by Cauchy-Schwartz inequality, for any \(\tau>\tau_{0}\geq 0\), we have

\[\left\|\bm{a}_{\tau}\right\|_{L^{2}(\rho_{0})}-\left\|\bm{a}_{\tau_{0}} \right\|_{L^{2}(\rho_{0})} \leq\int_{\tau_{0}}^{\tau}\sqrt{-\frac{d}{ds}\bar{\bar{\mathcal{L}}} \left(\bar{\bm{\theta}}(s)\right)}ds\] \[\leq\sqrt{\tau-\tau_{0}}\sqrt{-\bar{\bar{\mathcal{L}}}\left(\bar{ \bm{\theta}}(\tau)\right)+\bar{\bar{\mathcal{L}}}\left(\bar{\bm{\theta}}(\tau_{0}) \right)}\] \[\leq\sqrt{\tau}\sqrt{\bar{\bar{\mathcal{L}}}\left(\bar{\bm{\theta}}(0 )\right)}.\]By choosing \(\tau_{0}=0\), we have \(\left\|\bm{a}_{\tau}\right\|_{L^{2}(\rho_{0})}\lesssim\left\|\bm{\bar{s}}_{0,\bm{ \bar{\theta}}(0)}\right\|_{\mathcal{H}}+\sqrt{\tau}\), hence \(\left\|\bm{\bar{s}}_{0,\bm{\bar{\theta}}(\tau)}\right\|_{\mathcal{H}}\lesssim \left\|\bm{\bar{s}}_{0,\bm{\bar{\theta}}(0)}\right\|_{\mathcal{H}}+\sqrt{\tau}\), which completes the proof. 

**Lemma 6** (Monte Carlo estimates).: _Define the Monte Carlo error_

\[\operatorname{Err}_{\mathrm{MC}}=\operatorname{Err}_{\mathrm{MC}}(\bm{\theta},\bm{\bar{\theta}};T,\lambda(\cdot)):=\mathbb{E}_{t\sim\mathcal{U}(0,T)}\left[ \lambda(t)\cdot\mathbb{E}_{\bm{x}(t)\sim p_{t}}\left[\|\bm{s}_{t,\bm{\theta}}( \bm{x}(t))-\bar{s}_{t,\bm{\theta}}(\bm{x}(t))\|_{2}^{2}\right]\right].\] (25)

_Suppose that \(\|\bm{x}(0)\|_{\infty}\leq 1\), and the trainable parameter \(\bm{a}\) and embedding function \(\bm{e}(\cdot)\) are both bounded. Then, given any \(\bm{\tilde{\theta}}\), for any \(\delta>0\), \(\delta\ll 1\), with the probability of at least \(1-\delta\), there exists \(\bm{\theta}\) such that_

\[\operatorname{Err}_{\mathrm{MC}}\lesssim\frac{\log^{2}(1/\delta^{2})}{m}d,\] (26)

_where \(\lesssim\) hides universal positive constants only depending on \(T\)._

Proof.: Fix any \(\bm{\bar{\theta}}\). According to Lemma 1, for any \(\delta>0\), \(\delta\ll 1\), with the probability of at least \(1-\delta\), we have

\[\|\bm{x}(t)\|_{\infty}\lesssim C_{T}\left(1+\sqrt{\log(1/\delta^{2})}\right) \triangleq C_{T,\delta}.\] (27)

Based on the representation (13), for any \(\bm{W}=(\bm{w}_{1},\ldots,\bm{w}_{m})^{\top}\in\mathbb{R}^{m\times d}\), \(\bm{U}=(\bm{u}_{1},\ldots,\bm{u}_{m})^{\top}\in\mathbb{R}^{m\times d_{c}}\) with \((\bm{w}_{i},\bm{u}_{i})\sim\rho_{0}\), \(i=1,\cdots,m\), let \(\bm{A}:=(\bm{a}_{1},\ldots,\bm{a}_{m})\in\mathbb{R}^{d\times m}\) with \(\bm{a}_{i}:=\bm{a}(\bm{w}_{i},\bm{u}_{i})\) for \(i=1,\cdots,m\), and

\[\bm{s}_{t,\bm{\theta}}(\bm{x}):=\frac{1}{m}\bm{A}\sigma(\bm{W}\bm{x}+\bm{U}\bm {e}(t))=\frac{1}{m}\sum_{i=1}^{m}\bm{a}_{i}\sigma(\bm{w}_{i}^{\top}\bm{x}+\bm{ u}_{i}^{\top}\bm{e}(t)),\] (28)

then \(\mathbb{E}_{\bm{W},\bm{U}}[\bm{s}_{t,\bm{\theta}}(\bm{x})]=\mathbb{E}_{(\bm{w },\bm{u})\sim\rho_{0}}\left[\bm{a}(\bm{w},\bm{u})\sigma(\bm{w}^{\top}\bm{x}+ \bm{u}^{\top}\bm{e}(t))\right]=\bar{s}_{t,\bm{\tilde{\theta}}}(\bm{x}).\) For \(k=1,\cdots,d\), let

\[Z_{t,k}(\bm{W},\bm{U}) :=\left\|s_{t,\bm{\theta},k}(\bm{x})-\bar{s}_{t,\bm{\tilde{\theta} },k}(\bm{x})\right\|_{L^{2}(p_{t})}=\mathbb{E}_{\bm{x}\sim p_{t}}^{1/2}\left[ \left|s_{t,\bm{\theta},k}(\bm{x})-\bar{s}_{t,\bm{\tilde{\theta}},k}(\bm{x}) \right|^{2}\right]\] \[=\mathbb{E}_{\bm{x}\sim p_{t}}^{1/2}\left[\left|\frac{1}{m}\sum_ {i=1}^{m}a_{i,k}\sigma(\bm{w}_{i}^{\top}\bm{x}+\bm{u}_{i}^{\top}\bm{e}(t))- \mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}}\left[a_{k}(\bm{w},\bm{u})\sigma(\bm{ w}^{\top}\bm{x}+\bm{u}^{\top}\bm{e}(t))\right]\right|^{2}\right].\]

If \((\tilde{\bm{W}},\tilde{\bm{U}})\) is different from \((\bm{W},\bm{U})\) at only one component indexed by \(i\), we have

\[\left|Z_{t,k}(\bm{W},\bm{U})-Z_{t,k}(\tilde{\bm{W}},\tilde{\bm{ U}})\right|\] \[=\left|\left\|s_{t,\bm{\theta},k}(\bm{x})-\bar{s}_{t,\bm{\tilde{ \theta}},k}(\bm{x})\right\|_{L^{2}(p_{t})}-\left\|s_{t,\bm{\tilde{\theta}},k}( \bm{x})-\bar{s}_{t,\bm{\tilde{\theta}},k}(\bm{x})\right\|_{L^{2}(p_{t})}\right|\] \[\overset{\text{(i)}}{\leq}\left\|s_{t,\bm{\theta},k}(\bm{x})-s_{ t,\bm{\tilde{\theta}},k}(\bm{x})\right\|_{L^{2}(p_{t})}\] \[=\frac{1}{m}\left\|a_{i,k}\sigma(\bm{w}_{i}^{\top}\bm{x}+\bm{u}_ {i}^{\top}\bm{e}(t))-\tilde{a}_{i,k}\sigma(\tilde{\bm{w}}_{i}^{\top}\bm{x}+ \tilde{\bm{u}}_{i}^{\top}\bm{e}(t))\right\|_{L^{2}(p_{t})}\] \[\leq\frac{1}{m}\left(\left|a_{i,k}\right|\left\|\sigma(\bm{w}_{i} ^{\top}\bm{x}+\bm{u}_{i}^{\top}\bm{e}(t))\right\|_{L^{2}(p_{t})}+\left|\tilde{a }_{i,k}\right|\left\|\sigma(\tilde{\bm{w}}_{i}^{\top}\bm{x}+\tilde{\bm{u}}_{i}^ {\top}\bm{e}(t))\right\|_{L^{2}(p_{t})}\right)\] \[\overset{\text{(ii)}}{\leq}\frac{1}{m}\left(\left|a_{i,k}\right| \left(\|\bm{w}_{i}\|_{1}C_{T,\delta}+\|\bm{u}_{i}\|_{1}\|\bm{e}(t)\|_{\infty} \right)+\left|\tilde{a}_{i,k}\right|\left(\|\tilde{\bm{w}}_{i}\|_{1}C_{T,\delta }+\|\tilde{\bm{u}}_{i}\|_{1}\|\bm{e}(t)\|_{\infty}\right)\right)\] \[\overset{\text{(iii)}}{\leq}\frac{1}{m}\left(\left|a_{i,k}\right| +\left|\tilde{a}_{i,k}\right|\right)\left(C_{T,\delta}+\|\bm{e}(t)\|_{\infty}\right)\] \[\overset{\text{(iv)}}{\lesssim}\frac{1}{m}\left(C_{T,\delta}+C_ {T,\bm{e}}\right),\]

where (i) is from the triangle inequality, (ii) is due to the fact that \(|\sigma(y)|=|\text{ReLU}(y)|\leq|y|\) for any \(y\in\mathbb{R}\), the triangle and Holder's inequality and (27), (iii) follows from the positive homogeneity property of the ReLU activation, and (iv) is due to the boundness of the trainable parameter \(\bm{a}\) and embedding function \(\bm{e}(\cdot)\). By McDiarmid's inequality (see e.g., Lemma 26.4 in [43]), for any \(\delta>0\), with the probability of at least \(1-\delta\), we have

\[\left|Z_{t,k}(\bm{W},\bm{U})-\mathbb{E}_{\bm{W},\bm{U}}[Z_{t,k}( \bm{W},\bm{U})]\right| \lesssim\frac{1}{m}\left(C_{T,\delta}+C_{T,\bm{e}}\right)\sqrt{m \log(2/\delta)/2}\] (29) \[\lesssim\left(C_{T,\delta}+C_{T,\bm{e}}\right)\sqrt{\frac{\log(1/ \delta)}{m}}.\] (30)

Since

\[\mathbb{E}_{\bm{W},\bm{U}}\left[Z_{t,k}^{2}(\bm{W},\bm{U})\right]\] \[=\mathbb{E}_{\bm{W},\bm{U}}\left[\mathbb{E}_{\bm{x}\sim p_{t}} \left[\left|s_{t,\bm{\theta},k}(\bm{x})-\bar{s}_{t,\bm{\theta},k}(\bm{x}) \right|^{2}\right]\right]\] \[\overset{\text{(v)}}{=}\mathbb{E}_{\bm{x}\sim p_{t}}\left[ \mathbb{E}_{\bm{W},\bm{U}}\left[\left|s_{t,\bm{\theta},k}(\bm{x})-\bar{s}_{t, \bm{\theta},k}(\bm{x})\right|^{2}\right]\right]\] \[=\frac{1}{m^{2}}\mathbb{E}_{\bm{x}\sim p_{t}}\left[\mathbb{E}_{ \bm{W},\bm{U}}\left[\left|\sum_{i=1}^{m}\left(a_{i,k}\sigma(\bm{w}_{i}^{\top} \bm{x}+\bm{u}_{i}^{\top}\bm{e}(t))-\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}} \left[a_{k}(\bm{w},\bm{u})\sigma(\bm{w}^{\top}\bm{x}+\bm{u}^{\top}\bm{e}(t)) \right]\right)\right|^{2}\right]\right]\] \[=\frac{1}{m^{2}}\mathbb{E}_{\bm{x}\sim p_{t}}\Bigg{[}\mathbb{E}_{ \bm{W},\bm{U}}\Bigg{[}\sum_{i=1}^{m}\left(a_{i,k}\sigma(\bm{w}_{i}^{\top}\bm{x} +\bm{u}_{i}^{\top}\bm{e}(t))-\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}}\left[a_{ k}(\bm{w},\bm{u})\sigma(\bm{w}^{\top}\bm{x}+\bm{u}^{\top}\bm{e}(t))\right] \right)\] \[\qquad\qquad\qquad\qquad\qquad\qquad\times\left(a_{j,k}\sigma( \bm{w}_{j}^{\top}\bm{x}+\bm{u}_{j}^{\top}\bm{e}(t))-\mathbb{E}_{(\bm{w},\bm{u} )\sim\rho_{0}}\left[a_{k}(\bm{w},\bm{u})\sigma(\bm{w}^{\top}\bm{x}+\bm{u}^{ \top}\bm{e}(t))\right]\right)\Bigg{]}\Bigg{]}\] \[=\frac{1}{m^{2}}\mathbb{E}_{\bm{x}\sim p_{t}}\Bigg{[}\sum_{i=1}^ {m}\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}}\Bigg{[}\Big{(}a_{k}(\bm{w},\bm{u} )\sigma(\bm{w}^{\top}\bm{x}+\bm{u}^{\top}\bm{e}(t))-\mathbb{E}_{(\bm{w},\bm{ u})\sim\rho_{0}}\left[a_{k}(\bm{w},\bm{u})\sigma(\bm{w}^{\top}\bm{x}+\bm{u}^{ \top}\bm{e}(t))\right]\Big{)}^{2}\Bigg{]}\] \[\qquad\qquad\qquad\qquad\times\mathbb{E}_{(\bm{w}_{j},\bm{u}_{j} )\sim\rho_{0}}\Bigg{[}\Big{(}a_{j,k}\sigma(\bm{w}_{j}^{\top}\bm{x}+\bm{u}_{j}^ {\top}\bm{e}(t))-\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}}\left[a_{k}(\bm{w}, \bm{u})\sigma(\bm{w}^{\top}\bm{x}+\bm{u}^{\top}\bm{e}(t))\right]\Big{)} \Bigg{]}\Bigg{]}\] \[=\frac{1}{m^{2}}\mathbb{E}_{\bm{x}\sim p_{t}}\Bigg{[}\sum_{i=1}^ {m}\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}}\Bigg{[}\Big{(}a_{k}(\bm{w},\bm{u} )\sigma(\bm{w}^{\top}\bm{x}+\bm{u}^{\top}\bm{e}(t))-\mathbb{E}_{(\bm{w},\bm{u })\sim\rho_{0}}\left[a_{k}(\bm{w},\bm{u})\sigma(\bm{w}^{\top}\bm{x}+\bm{u}^{ \top}\bm{e}(t))\right]\Big{)}^{2}\Bigg{]}_{\text{}}\] \[\leq\frac{1}{m}\mathbb{E}_{\bm{x}\sim p_{t}}\left[\mathbb{E}_{( \bm{w},\bm{u})\sim\rho_{0}}\left[\Big{(}a_{k}(\bm{w},\bm{u})\sigma(\bm{w}^{ \top}\bm{x}+\bm{u}^{\top}\bm{e}(t))\Big{)}^{2}\right]\right]\] \[\overset{\text{(ii)}}{\leq}\frac{1}{m}\mathbb{E}_{\bm{x}\sim p_{t }}\left[\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}}\left[\Big{(}|a_{k}(\bm{w}, \bm{u})|\left(\|\bm{w}\|_{1}C_{T,\delta}+\|\bm{u}\|_{1}\|\bm{e}(t)\|_{\infty} \right)\Big{)}^{2}\right]\right]\] \[\overset{\text{(iii)}}{\leq}\frac{1}{m}\mathbb{E}_{\bm{x}\sim p_{t }}\left[\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}}\left[\Big{(}|a_{k}(\bm{w},\bm {u})|\left(C_{T,\delta}+\|\bm{e}(t)\|_{\infty}\right)\Big{)}^{2}\right]\right]\] \[\overset{\text{(iv)}}{\lesssim}\frac{1}{m}\mathbb{E}_{\bm{x}\sim p _{t}}\left[\mathbb{E}_{(\bm{w},\bm{u})\sim\rho_{0}}\left[\left(C_{T,\delta}+C_{T,\bm{e}}\right)^{2}\right]\right]\] (31) \[\lesssim\left(C_{T,\delta}+C_{T,\bm{e}}\right)^{2}\frac{1}{m},\] (32)

where (v) is due to Fubini's theorem, and (ii), (iii), (iv) is the same as before. By the triangle inequality, Jensen's inequality, (29) and (31), we obtain

\[\mathbb{E}_{\bm{x}\sim p_{t}}\left[\left\|s_{t,\bm{\theta}}(\bm{x})-\bar{s}_{t, \bm{\theta}}(\bm{x})\right\|_{2}^{2}\right]=\sum_{k=1}^{d}\mathbb{E}_{\bm{x} \sim p_{t}}\left[\left|s_{t,\bm{\theta},k}(\bm{x})-\bar{s}_{t,\bm{\theta},k}( \bm{x})\right|^{2}\right]\]\[I_{3,1}\lesssim\bar{\bar{\mathcal{L}}}(\bar{\bm{\theta}}(\tau))+ \bar{\bar{\mathcal{L}}}(\bar{\bm{\theta}}^{*})=I_{2}+\bar{\bar{\mathcal{L}}}( \bar{\bm{\theta}}^{*})\lesssim\bar{\bar{\mathcal{L}}}\left(\bar{\bm{\theta}}^{* }\right)+\frac{1}{\tau}\left(\left\|\bar{\bm{s}}_{0,\bar{\bm{\theta}}_{0}} \right\|_{\mathcal{H}}^{2}+\left\|\bar{\bm{s}}_{0,\bar{\bm{\theta}}^{*}} \right\|_{\mathcal{H}}^{2}\right),\]and similarly,

\[I_{3,3}\lesssim\tilde{\mathcal{L}}\left(\bm{\theta}^{*}\right)+\frac{1}{\tau} \left(\left\|s_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}^{2}+\left\|s_{0,\bm{ \theta}^{*}}\right\|_{\mathcal{H}}^{2}\right).\]

Hence, for any \(\delta>0\), \(\delta\ll 1\), with the probability of at least \(1-\delta\), it holds that

\[I_{3}\lesssim\frac{\log^{2}(1/\delta^{2})}{m}d+\tilde{\mathcal{L}}\left(\bm{ \theta}^{*}\right)+\tilde{\mathcal{L}}\left(\bm{\theta}^{*}\right)+\frac{1}{ \tau}\left(\left\|\bm{\bar{s}}_{0,\bm{\bar{\theta}}_{0}}\right\|_{\mathcal{H} }^{2}+\left\|\bm{\bar{s}}_{0,\bm{\bar{\theta}}^{*}}\right\|_{\mathcal{H}}^{2} +\left\|s_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}^{2}+\left\|s_{0,\bm{\theta }^{*}}\right\|_{\mathcal{H}}^{2}\right).\]

For the term \(I_{1}\), we have

\[\sqrt{\tilde{\mathcal{L}}(\hat{\bm{\theta}}_{n}(\tau))}-\sqrt{ \tilde{\mathcal{L}}(\bm{\theta}(\tau))}\] \[=\] \[=\] \[\leq\] \[= \left\{\mathbb{E}_{t\sim\mathcal{U}(0,T)}\mathbb{E}_{\bm{x}(t) \sim p_{t}}\left[\lambda(t)\left\|\frac{1}{m}\sum_{i=1}^{m}\hat{\bm{a}}_{i}( \tau)\sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))-\frac{1}{m} \sum_{i=1}^{m}\bm{a}_{i}(\tau)\sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{ \top}\bm{e}(t))\right\|_{2}^{2}\right]\right\}^{\frac{1}{2}}.\]

Notice that

\[\left\|\frac{1}{m}\sum_{i=1}^{m}(\hat{\bm{a}}_{i}(\tau)-\bm{a}_{i} (\tau))\sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right\| ^{2}\] \[\leq \frac{1}{m^{2}}\left(\sum_{i=1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)- \bm{a}_{i}(\tau)\right\|_{2}^{2}\left|\sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{ u}_{i}^{\top}\bm{e}(t))\right|\right)^{2}\] \[\leq \frac{1}{m^{2}}\sum_{i=1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)-\bm{a} _{i}(\tau)\right\|_{2}^{2}\sum_{i=1}^{m}\left|\sigma(\bm{w}_{i}^{\top}\bm{x}( t)+\bm{u}_{i}^{\top}\bm{e}(t))\right|^{2}\] \[\lesssim \frac{1}{m^{2}}\sum_{i=1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)-\bm{a} _{i}(\tau)\right\|_{2}^{2}\sum_{i=1}^{m}\left(\left\|\bm{w}_{i}\right\|_{1}^{ 2}\left\|\bm{x}(t)\right\|_{\infty}^{2}+\left\|\bm{u}_{i}\right\|_{1}^{2} \left\|\bm{e}(t)\right\|_{\infty}^{2}\right)\] \[\lesssim \frac{1}{m^{2}}\sum_{i=1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)-\bm{a} _{i}(\tau)\right\|_{2}^{2}\sum_{i=1}^{m}\left(C_{T,\delta}^{2}+C_{T,\bm{e}}^ {2}\right),\]

which gives

\[\sqrt{\tilde{\mathcal{L}}(\hat{\bm{\theta}}_{n}(\tau))}-\sqrt{ \tilde{\mathcal{L}}(\bm{\theta}(\tau))} \lesssim\left\{\frac{1}{m}\sum_{i=1}^{m}\left\|\hat{\bm{a}}_{i}( \tau)-\bm{a}_{i}(\tau)\right\|_{2}^{2}\left(C_{T,\delta}^{2}+C_{T,\bm{e}}^{2} \right)\right\}^{\frac{1}{2}}\] \[\leq\left(C_{T,\delta}+C_{T,\bm{e}}\right)\left\{\frac{1}{m}\sum_ {i=1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)-\bm{a}_{i}(\tau)\right\|_{2}^{2}\right\} ^{\frac{1}{2}}.\]

Here, the triangle inequality, Cauchy-Schwartz inequality, the fact that \(|\sigma(y)|=|\text{ReLU}(y)|\leq|y|\) for any \(y\in\mathbb{R}\), Holder's inequality, the positive homogeneity property of the ReLU activation, and the boundness of the input data, embedding function \(\bm{e}(t)\) and weighting function \(\lambda(t)\). Thus, we get

\[\tilde{\mathcal{L}}(\hat{\bm{\theta}}_{n}(\tau))-\tilde{\mathcal{L}}( \bm{\theta}(\tau))\] \[\lesssim \frac{1}{m}\left(C_{T,\delta}^{2}+C_{T,\bm{e}}^{2}\right)\sum_{i= 1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)-\bm{a}_{i}(\tau)\right\|_{2}^{2}+\sqrt{ \tilde{\mathcal{L}}(\bm{\theta}(\tau))}\left(C_{T,\delta}+C_{T,\bm{e}}\right) \left\{\frac{1}{m}\sum_{i=1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)-\bm{a}_{i}(\tau) \right\|_{2}^{2}\right\}^{\frac{1}{2}}\] \[\lesssim \sqrt{\tilde{\mathcal{L}}(\bm{\theta}^{*})+\frac{1}{\tau}\left( \left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}^{2}+\left\|\bm{s}_{0,\bm{\theta}^{*}}\right\|_{\mathcal{H}}^{2}\right)}\left(C_{T,\delta}+C_{T, \bm{e}}\right)\left\{\frac{1}{m}\sum_{i=1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)- \bm{a}_{i}(\tau)\right\|_{2}^{2}\right\}^{\frac{1}{2}}\] \[+\frac{1}{m}\left(C_{T,\delta}^{2}+C_{T,\bm{e}}^{2}\right)\sum_{i =1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)-\bm{a}_{i}(\tau)\right\|_{2}^{2},\]

where the last inequality follows from Lemma 4. We further deduce that

\[\frac{1}{m}\sum_{i=1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)-\bm{a}_{i}( \tau)\right\|_{2}^{2}\] \[= \frac{1}{m}\sum_{i=1}^{m}\left\|\int_{0}^{\tau}\frac{d}{d\tau_{0} }(\hat{\bm{a}}_{i}(\tau_{0})-\bm{a}_{i}(\tau_{0}))d\tau_{0}\right\|_{2}^{2}\] \[= \frac{1}{m}\sum_{i=1}^{m}\left\|\int_{0}^{\tau}\left(\nabla_{\bm {\theta}_{i}(\tau_{0})}\tilde{\mathcal{L}}(\bm{\theta}(\tau_{0}))-\nabla_{\hat {\bm{\theta}}_{n,i}(\tau_{0})}\tilde{\tilde{\mathcal{L}}}_{n}(\hat{\bm{\theta} }_{n}(\tau_{0}))\right)d\tau_{0}\right\|_{2}^{2}\] \[= \frac{1}{m^{2}}\sum_{i=1}^{m}\left\|\int_{0}^{\tau}\left(2\mathbb{ E}_{t\sim\mathcal{U}(0,T)}\left[\lambda(t)\mathbb{E}_{\bm{x}(t)\sim p_{t}} \left[\left(\bm{s}_{t,\bm{\theta}(\tau_{0})}(\bm{x}(t))-\nabla_{\bm{x}(t)} \log p_{t}(\bm{x}(t))\right)\sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top }\bm{e}(t))\right]\right]\right.\] \[\left.-2\mathbb{E}_{t\sim\mathcal{U}(0,T)}\left[\lambda(t) \mathbb{E}_{\bm{x}(t)\sim p_{t}^{(n)}}\left[\left(\bm{s}_{t,\bm{\theta}_{n}( \tau_{0})}(\bm{x}(t))-\nabla_{\bm{x}(t)}\log p_{t}(\bm{x}(t))\right)\sigma( \bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right]\right]\right)d \tau_{0}\right\|_{2}^{2}.\]

where \(p_{t}^{(n)}\) denotes the empirical distribution of \(p_{t}\). Note that

\[\|\bm{\theta}\|_{2}^{2}=\left\|\operatorname{vec}(\bm{A})\|_{2}^{2}/m=\left\| \bm{A}\right\|_{F}^{2}/m=\left\|\bm{s}_{0,\bm{\theta}}\right\|_{\mathcal{H}} ^{2},\] (35)

by Lemma 5 we get \(\|\bm{\theta}(\tau)\|_{2}=\left\|\bm{s}_{0,\bm{\theta}(\tau)}\right\|_{\mathcal{ H}}\lesssim\left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\tau/m}\). For any \(t\in[0,T]\), define the function space

\[\mathcal{F}_{t}:=\left\{\bm{f}_{1}(\bm{x}(t);\bm{\theta}_{1}(\tau))f_{2}(\bm{ x}(t);\bm{\theta}_{2}):\,\bm{f}_{1}\in\mathcal{F}_{1,t},f_{2}\in\mathcal{F}_{2,t} \right\},\]

where

\[\mathcal{F}_{1,t}:=\left\{\bm{s}_{t,\bm{\theta}(\tau)}(\bm{x}(t))-\nabla_{\bm {x}(t)}\log p_{t}(\bm{x}(t)):\|\bm{\theta}(\tau)\|_{2}\lesssim\left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\tau/m}\right\},\]

\[\mathcal{F}_{2,t}:=\left\{\sigma(\bm{w}^{\top}\bm{x}(t)+\bm{u}^{\top}\bm{e}(t) ):\|\bm{w}\|_{1}+\|\bm{u}\|_{1}\leq 1\right\}.\]

Then, according to Theorem A.5 in [61], for any \(\delta\in(0,1)\), with the probability at least \(1-\delta\) over the choice of the dataset \(\mathcal{D}_{\bm{x}}=\{\bm{x}_{i}\}_{i=1}^{n}\), it holds that

\[\left|\mathbb{E}_{\bm{x}(t)\sim p_{t}}\left[\left(\bm{s}_{t,\bm{ \theta}(\tau)}(\bm{x}(t))-\nabla_{\bm{x}(t)}\log p_{t}(\bm{x}(t))\right) \sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right]\right.\] \[\left.-\mathbb{E}_{\bm{x}(t)\sim p_{t}^{(n)}}\left[\left(\bm{s}_ {t,\bm{\theta}_{n}(\tau)}(\bm{x}(t))-\nabla_{\bm{x}(t)}\log p_{t}(\bm{x}(t)) \right)\sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right]\right.\] \[\leq \left|\mathbb{E}_{\bm{x}(t)\sim p_{t}}\left[\left(\bm{s}_{t,\bm{ \theta}(\tau)}(\bm{x}(t))-\nabla_{\bm{x}(t)}\log p_{t}(\bm{x}(t))\right)\sigma( \bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right]\right.\] \[\left.-\mathbb{E}_{\bm{x}(t)\sim p_{t}^{(n)}}\left[\left(\bm{s}_ {t,\bm{\theta}(\tau)}(\bm{x}(t))-\nabla_{\bm{x}(t)}\log p_{t}(\bm{x}(t)) \right)\sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right]\right.\] \[\left.+\left|\mathbb{E}_{\bm{x}(t)\sim p_{t}^{(n)}}\left[\left( \bm{s}_{t,\bm{\theta}(\tau)}(\bm{x}(t))-\nabla_{\bm{x}(t)}\log p_{t}(\bm{x}(t)) \right)\sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right]\right.\] \[\left.-\mathbb{E}_{\bm{x}(t)\sim p_{t}^{(n)}}\left[\left(\bm{s}_ {t,\bm{\theta}_{n}(\tau)}(\bm{x}(t))-\nabla_{\bm{x}(t)}\log p_{t}(\bm{x}(t)) \right)\sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right]\right.\] \[\lesssim \widetilde{\operatorname{Rad}}_{n}(\mathcal{F}_{t})+\sup_{\bm{f} \in\mathcal{F}_{t},\,\,\bm{x}(t)\in[-C_{T,\delta},C_{T,\delta}]^{d}}\left|\bm{f} (\bm{x}(t))\right|\sqrt{\frac{\log(2/\delta)}{n}}\] \[+\left|\mathbb{E}_{\bm{x}(t)\sim p_{t}^{(n)}}\left[\left(\bm{s}_ {t,\bm{\theta}(\tau)}(\bm{x}(t))-\bm{s}_{t,\bm{\theta}_{n}(\tau)}(\bm{where \(\widehat{\operatorname{Rad}}_{n}\) denotes the (empirical) Rademacher complexity of \(\mathcal{F}_{t}\) on \(\mathcal{D}_{\bm{x}}=\{\bm{x}_{i}\}_{i=1}^{n}\), and all the inequalities hold in the element-wise sense.

(i) For \(J_{1}\), according to Lemma A.6 in [61], we have

\[\widehat{\operatorname{Rad}}_{n}(\mathcal{F}_{t}) \leq\left(\sup_{\bm{f}_{1}\in\mathcal{F}_{1,t},\,\bm{x}(t)\in[-C_ {T,\delta},C_{T,\delta}]^{d}}|\bm{f}_{1}(\bm{x}(t))|+\sup_{f_{2}\in\mathcal{F}_ {2,t},\,\bm{x}(t)\in[-C_{T,\delta},C_{T,\delta}]^{d}}|f_{2}(\bm{x}(t))|\right)\] \[\cdot\left(\widehat{\operatorname{Rad}}_{n}(\mathcal{F}_{1,t})+ \widehat{\operatorname{Rad}}_{n}(\mathcal{F}_{2,t})\right).\]

Note that

\[\left|\sigma(\bm{w}^{\top}\bm{x}(t)+\bm{u}^{\top}\bm{e}(t))\right| \leq\left|\bm{w}^{\top}\bm{x}(t)+\bm{u}^{\top}\bm{e}(t)\right|\] \[\lesssim\left\|\bm{w}\right\|_{1}\|\bm{x}(t)\|_{\infty}+\|\bm{u} \|_{1}\|\bm{e}(t)\|_{\infty}\] \[\leq C_{T,\delta}+C_{T,\bm{e}},\] (36)

which yields

\[\left|\bm{s}_{t,\bm{\theta}(\tau)}(\bm{x}(t))\right| =\left|\frac{1}{\sqrt{m}}\sum_{i=1}^{m}\bm{\theta}_{i}(\tau) \sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right|\] \[\leq\frac{1}{\sqrt{m}}\sum_{i=1}^{m}\left|\bm{\theta}_{i}(\tau) \right|\left|\sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right|\] \[\leq\frac{1}{\sqrt{m}}\left(C_{T,\delta}+C_{T,\bm{e}}\right)\sum_ {i=1}^{m}\left|\bm{\theta}_{i}(\tau)\right|\] \[\leq\left(C_{T,\delta}+C_{T,\bm{e}}\right)\left\|\bm{\theta}(\tau )\right\|_{2}\] \[\lesssim\left(C_{T,\delta}+C_{T,\bm{e}}\right)\left(\left\|\bm{s} _{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\tau/m}\right),\] (37)

and hence

\[\left|\bm{f}_{1}(\bm{x}(t))\right| =\left|\bm{s}_{t,\bm{\theta}(\tau)}(\bm{x}(t))-\nabla_{\bm{x}(t) }\log p_{t}(\bm{x}(t))\right|\] \[\lesssim\left(C_{T,\delta}+C_{T,\bm{e}}\right)\left(\left\|\bm{s} _{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\tau/m}\right)+C_{T,\delta}^ {\prime},\] \[\left|f_{2}(\bm{x}(t))\right| =\left|\sigma(\bm{w}^{\top}\bm{x}(t)+\bm{u}^{\top}\bm{e}(t)) \right|\lesssim C_{T,\delta}+C_{T,\bm{e}},\]

where \(C_{T,\delta}^{\prime}:=\max_{\bm{x}(t)\in[-C_{T,\delta},C_{T,\delta}]^{d}} \left|\nabla_{\bm{x}(t)}\log p_{t}(\bm{x}(t))\right|\). This gives

\[\widehat{\operatorname{Rad}}_{n}(\mathcal{F}_{t}) \lesssim\left(C_{T,\delta}+C_{T,\bm{e}}\right)\left(\left\|\bm{s} _{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\tau/m}+1\right)\left(\widehat {\operatorname{Rad}}_{n}(\mathcal{F}_{1,t})+\widehat{\operatorname{Rad}}_{n} (\mathcal{F}_{2,t})\right).\]

Let

according to Lemma 26.6 in [43], we get \(\widehat{\operatorname{Rad}}_{n}(\mathcal{F}_{1,t})\leq\widehat{\operatorname{ Rad}}_{n}(\mathcal{F}_{1,t}^{\prime})\). Since

\[n\widehat{\operatorname{Rad}}_{n}(\mathcal{F}_{1,t}^{\prime}) =\mathbb{E}_{\bm{\xi}}\left[\sup_{\|\bm{\theta}(\tau)\|_{2}\lesssim \left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\tau/m}}\sum_{j=1 }^{n}\xi_{j}\bm{s}_{t,\bm{\theta}(\tau)}(\bm{x}_{j}(t))\right]\] \[=\mathbb{E}_{\bm{\xi}}\left[\sup_{\|\bm{\theta}(\tau)\|_{2}\lesssim \left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\tau/m}}\sum_{j=1 }^{n}\xi_{j}\frac{1}{\sqrt{m}}\sum_{i=1}^{m}\bm{\theta}_{i}(\tau)\sigma(\bm{w} _{i}^{\top}\bm{x}_{j}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right]\] \[\leq\frac{1}{\sqrt{m}}\mathbb{E}_{\bm{\xi}}\left[\sup_{\|\bm{ \theta}(\tau)\|_{2}\lesssim\left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H }}+\sqrt{\tau/m}}\sum_{i=1}^{m}\bm{\theta}_{i}(\tau)\sum_{j=1}^{n}\xi_{j} \sigma(\bm{w}_{i}^{\top}\bm{x}_{j}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right]\]\[\leq\frac{1}{\sqrt{m}}\mathbb{E}_{\bm{\xi}}\left[\sup_{\|\bm{\theta}( \tau)\|_{2}\lesssim\left\|\bm{\theta}_{0},\bm{\theta}_{0}\right\|_{\mathcal{H}}+ \sqrt{\tau/m}}\sum_{i=1}^{m}|\bm{\theta}_{i}(\tau)|\sup_{\|\bm{w}_{i}\|_{1}+\| \bm{u}_{i}\|_{1}\leq 1,\,i\in[n]}\left|\sum_{j=1}^{n}\xi_{j}\sigma(\bm{w}_{i}^{\top}\bm{ x}_{j}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\right|\right]\] \[\leq\mathbb{E}_{\bm{\xi}}\left[\sup_{\|\bm{\theta}(\tau)\|_{2} \lesssim\left\|\bm{\theta}_{0},\bm{\theta}_{0}\right\|_{\mathcal{H}}+\sqrt{ \tau/m}}\|\bm{\theta}(\tau)\|_{2}\sup_{\|\bm{w}\|_{1}+\|\bm{u}\|_{1}\leq 1} \left|\sum_{j=1}^{n}\xi_{j}\sigma(\bm{w}^{\top}\bm{x}_{j}(t)+\bm{u}^{\top}\bm{ e}(t))\right|\right]\] \[\lesssim\left(\left\|\bm{s}_{0},\bm{\theta}_{0}\right\|_{\mathcal{ H}}+\sqrt{\tau/m}\right)\mathbb{E}_{\bm{\xi}}\left[\sup_{\|\bm{w}\|_{1}+\| \bm{u}\|_{1}\leq 1}\left|\sum_{j=1}^{n}\xi_{j}\sigma(\bm{w}^{\top}\bm{x}_{j}(t)+ \bm{u}^{\top}\bm{e}(t))\right|\right],\]

where the \(\{\xi_{i}\}_{i=1}^{n}\) are independent random variables with the distribution \(\mathbb{P}(\xi_{i}=1)=\mathbb{P}(\xi_{i}=-1)=1/2\), and obviously,

\[\sup_{\|\bm{w}\|_{1}+\|\bm{u}\|_{1}\leq 1}\sum_{j=1}^{n}\xi_{j}\sigma(\bm{w}^{ \top}\bm{x}_{j}(t)+\bm{u}^{\top}\bm{e}(t))\geq\sum_{j=1}^{n}\xi_{j}\sigma(\bm{ 0}_{d}^{\top}\bm{x}_{j}(t)+\bm{0}_{d}^{\top}\bm{e}(t))=0,\]

we have

\[\sup_{\|\bm{w}\|_{1}+\|\bm{u}\|_{1}\leq 1}\left|\sum_{j=1}^{n} \xi_{j}\sigma(\bm{w}^{\top}\bm{x}_{j}(t)+\bm{u}^{\top}\bm{e}(t))\right|\] \[\leq\max\left\{\sup_{\|\bm{w}\|_{1}+\|\bm{u}\|_{1}\leq 1}\sum_{j=1}^ {n}\xi_{j}\sigma(\bm{w}^{\top}\bm{x}_{j}(t)+\bm{u}^{\top}\bm{e}(t)),\,\sup_{ \|\bm{w}\|_{1}+\|\bm{u}\|_{1}\leq 1}\sum_{j=1}^{n}(-\xi_{j})\sigma(\bm{w}^{\top}\bm{x}_ {j}(t)+\bm{u}^{\top}\bm{e}(t))\right\}\] \[\leq\sup_{\|\bm{w}\|_{1}+\|\bm{u}\|_{1}\leq 1}\sum_{j=1}^{n} \xi_{j}\sigma(\bm{w}^{\top}\bm{x}_{j}(t)+\bm{u}^{\top}\bm{e}(t))+\sup_{\|\bm{w }\|_{1}+\|\bm{u}\|_{1}\leq 1}\sum_{j=1}^{n}(-\xi_{j})\sigma(\bm{w}^{\top}\bm{x}_ {j}(t)+\bm{u}^{\top}\bm{e}(t)),\]

and by symmetry

\[\mathbb{E}_{\bm{\xi}}\left[\sup_{\|\bm{w}\|_{1}+\|\bm{u}\|_{1}\leq 1} \left|\sum_{j=1}^{n}\xi_{j}\sigma(\bm{w}^{\top}\bm{x}_{j}(t)+\bm{u}^{\top}\bm{e }(t))\right|\right]\] \[\leq\mathbb{E}_{\bm{\xi}}\left[\sup_{\|\bm{w}\|_{1}+\|\bm{u}\|_{1 }\leq 1}\sum_{j=1}^{n}\xi_{j}\sigma(\bm{w}^{\top}\bm{x}_{j}(t)+\bm{u}^{\top}\bm{e}(t ))\right]+\mathbb{E}_{\bm{\xi}}\left[\sup_{\|\bm{w}\|_{1}+\|\bm{u}\|_{1}\leq 1} \sum_{j=1}^{n}(-\xi_{j})\sigma(\bm{w}^{\top}\bm{x}_{j}(t)+\bm{u}^{\top}\bm{e}(t))\right]\] \[=2\mathbb{E}_{\bm{\xi}}\left[\sup_{\|\bm{w}\|_{1}+\|\bm{u}\|_{1} \leq 1}\sum_{j=1}^{n}\xi_{j}\sigma(\bm{w}^{\top}\bm{x}_{j}(t)+\bm{u}^{\top}\bm{e}(t ))\right]=2n\widehat{\mathrm{Rad}}_{n}(\mathcal{F}_{2,t}),\]

i.e., \(\widehat{\mathrm{Rad}}_{n}(\mathcal{F}_{1,t})\leq\widehat{\mathrm{Rad}}_{n}( \mathcal{F}_{1,t}^{\prime})\lesssim 2\left(\left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{ \mathcal{H}}+\sqrt{\tau/m}\right)\widehat{\mathrm{Rad}}_{n}(\mathcal{F}_{2,t})\). According to Lemma 26.9 (contraction lemma) and Lemma 26.11 in [43], we have

\[\widehat{\mathrm{Rad}}_{n}(\mathcal{F}_{2,t})\leq\left(\|\bm{x}(t)\|_{\infty}+ \|\bm{e}(t)\|_{\infty}\right)\sqrt{\frac{2\log(4d)}{n}}\lesssim\left(C_{T,\delta} +C_{T,\bm{e}}\right)\sqrt{\frac{\log(d+1)}{n}}.\]

Combining above, we obtain

\[J_{1}=\widehat{\mathrm{Rad}}_{n}(\mathcal{F}_{t})\lesssim\left(C_{T,\delta}+C_{T, \bm{e}}\right)^{2}\left(\left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}+ \sqrt{\tau/m}+1\right)^{2}\sqrt{\frac{\log(d+1)}{n}}.\]

(ii) For \(J_{2}\), by (36) and (37), we get

\[|\bm{f}(\bm{x}(t))| =\left|\bm{s}_{t,\bm{\theta}(\tau)}(\bm{x}(t))-\nabla_{\bm{x}(t)} \log p_{t}(\bm{x}(t))\right|\left|\sigma(\bm{w}^{\top}\bm{x}(t)+\bm{u}^{\top}\bm{ e}(t))\right|\] \[\lesssim\left(C_{T,\delta}+C_{T,\bm{e}}\right)^{2}\left(\left\| \bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\tau/m}\right)+C_{T, \delta}^{\prime}\left(C_{T,\delta}+C_{T,\bm{e}}\right),\]which gives

\[J_{2}\lesssim\left(C_{T,\delta}+C_{T,\bm{e}}\right)^{2}\left(\left\| \bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\tau/m}+1\right)\sqrt{ \frac{\log(1/\delta)}{n}}.\]

(iii) For \(J_{3}\), we similarly have

\[\left|\bm{s}_{t,\bm{\theta}_{n}(\tau)}(\bm{x}(t))\right|\lesssim \left(C_{T,\delta}+C_{T,\bm{e}}\right)\left(\left\|\bm{s}_{0,\bm{\theta}_{0}} \right\|_{\mathcal{H}}+\sqrt{\tau/m}\right),\]

hence by (36) and (37), we get

\[J_{3} \leq\mathbb{E}_{\bm{x}(t)\sim p_{t}^{(n)}}\Big{|}\left(\bm{s}_{t, \bm{\theta}(\tau)}(\bm{x}(t))-\bm{s}_{t,\bm{\hat{\theta}}_{n}(\tau)}(\bm{x}(t) )\right)\sigma(\bm{w}_{i}^{\top}\bm{x}(t)+\bm{u}_{i}^{\top}\bm{e}(t))\Big{|}\] \[\lesssim\left(C_{T,\delta}+C_{T,\bm{e}}\right)^{2}\left(\left\| \bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\tau/m}\right).\]

Combining (i), (ii) and (iii), we obtain

\[\frac{1}{m}\sum_{i=1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)-\bm{a}_{i} (\tau)\right\|_{2}^{2} \lesssim\frac{1}{m^{2}}\sum_{i=1}^{m}\left\|\int_{0}^{\tau}\mathbb{ E}_{t\sim\mathcal{U}(0,T)}\left[\lambda(t)(J_{1}+J_{2}+J_{3})\mathbf{1}_{d} \right]d\tau_{0}\right\|_{2}^{2}\] \[\lesssim\frac{1}{m^{2}}\sum_{i=1}^{m}\left\|\int_{0}^{\tau}(J_{1} +J_{2}+J_{3})\mathbf{1}_{d}d\tau_{0}\right\|_{2}^{2}\] \[=(J_{1}+J_{2}+J_{3})^{2}\tau^{2}\frac{d}{m}\] \[\lesssim\tau^{2}\frac{d}{m}\left(C_{T,\delta}+C_{T,\bm{e}}\right) ^{4}\Bigg{[}\left(\left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}^{2} +\frac{\tau}{m}+1\right)^{2}\] \[\quad\cdot\left(\sqrt{\frac{\log(d+1)}{n}}+\sqrt{\frac{\log(1/ \delta)}{n}}\right)^{2}+\left(\left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{ \mathcal{H}}^{2}+\frac{\tau}{m}\right)\Bigg{]},\]

which gives

\[I_{1}= \tilde{\mathcal{L}}(\bm{\hat{\theta}}_{n}(\tau))-\tilde{\mathcal{ L}}(\bm{\theta}(\tau))\] \[\lesssim\] \[\quad+\left(C_{T,\delta}^{2}+C_{T,\bm{e}}^{2}\right)\frac{1}{m} \sum_{i=1}^{m}\left\|\hat{\bm{a}}_{i}(\tau)-\bm{a}_{i}(\tau)\right\|_{2}^{2}\] \[\lesssim\] \[\quad\cdot\tau\sqrt{\frac{d}{m}}\left[\left(\left\|\bm{s}_{0,\bm {\theta}_{0}}\right\|_{\mathcal{H}}^{2}+\tau+1\right)\left(\sqrt{\frac{\log(d+ 1)}{n}}+\sqrt{\frac{\log(1/\delta)}{n}}\right)+\left(\left\|\bm{s}_{0,\bm{ \theta}_{0}}\right\|_{\mathcal{H}}+\sqrt{\frac{\tau}{m}}\right)\right]\] \[\quad+\tau^{2}\frac{d}{m}\left(C_{T,\delta}+C_{T,\bm{e}}\right) ^{6}\Bigg{[}\left(\left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H}}^{2 }+\tau+1\right)^{2}\left(\sqrt{\frac{\log(d+1)}{n}}+\sqrt{\frac{\log(1/\delta )}{n}}\right)^{2}+\left(\left\|\bm{s}_{0,\bm{\theta}_{0}}\right\|_{\mathcal{H }}^{2}+\frac{\tau}{m}\right)\Bigg{]}\] \[\lesssim\] \[\quad\cdot\Bigg{\{}\left(\sqrt{\tilde{\mathcal{L}}(\bm{\theta}^{ *})}+\frac{1}{\sqrt{m\tau}}\left(\left\|\bm{A}_{0}\right\|_{F}+\left\|\bm{A}_{ *}\right\|_{F}\right)\right)\] \[\quad+\tau\sqrt{\frac{d}{m}}\Bigg{[}\left(\tau+1\right)\left( \sqrt{\frac{\log(d+1)}{n}}+\sqrt{\frac{\log(1/\delta)}{n}}\right)+\left(\frac {\left\|\bm{A}_{0}\right\|_{F}}{\sqrt{m}}+\sqrt{\frac{\tau}{m}}\right) \Bigg{]}\Bigg{\}}\] (38)\[\lesssim\log^{3}(1/\delta^{2})\tau\sqrt{\frac{d}{m}}\Bigg{[}\left( \tau+1\right)\left(\sqrt{\frac{\log(d+1)}{n}}+\sqrt{\frac{\log(1/\delta)}{n}} \right)+\left(\frac{1}{\sqrt{m}}+\sqrt{\frac{\tau}{m}}\right)\Bigg{]}\] \[\quad\cdot\Bigg{\{}\left(\sqrt{\tilde{\mathcal{L}}(\boldsymbol{ \theta}^{*})}+\frac{1}{\sqrt{m\tau}}\right)+\tau\sqrt{\frac{d}{m}}\Bigg{[} \left(\tau+1\right)\left(\sqrt{\frac{\log(d+1)}{n}}+\sqrt{\frac{\log(1/\delta)} {n}}\right)+\left(\frac{1}{\sqrt{m}}+\sqrt{\frac{\tau}{m}}\right)\Bigg{]} \Bigg{\}},\]

where \(\lesssim\) hides universal positive constants only depending on \(T\).

Combining all above estimates yields

\[D_{\mathrm{KL}}\left(p_{0}\|p_{0,\boldsymbol{\theta}_{n}(\tau)}\right)\] \[\lesssim I_{1}+I_{2}+I_{3}+D_{\mathrm{KL}}\left(p_{T}\|\pi\right)\] \[\lesssim\log^{3}(1/\delta^{2})\tau\sqrt{\frac{d}{m}}\Bigg{[} \left(\tau+1\right)\left(\sqrt{\frac{\log(d+1)}{n}}+\sqrt{\frac{\log(1/\delta )}{n}}\right)+\frac{1}{\sqrt{m}}\left(1+\sqrt{\tau}\right)\Bigg{]}\] \[\quad\cdot\Bigg{\{}\left(\sqrt{\tilde{\mathcal{L}}(\boldsymbol{ \theta}^{*})}+\frac{1}{\sqrt{m\tau}}\right)+\tau\sqrt{\frac{d}{m}}\Bigg{[} \left(\tau+1\right)\left(\sqrt{\frac{\log(d+1)}{n}}+\sqrt{\frac{\log(1/\delta )}{n}}\right)+\frac{1}{\sqrt{m}}\left(1+\sqrt{\tau}\right)\Bigg{]}\Bigg{\}}\] \[\quad+\left(\bar{\bar{\mathcal{L}}}\left(\boldsymbol{\bar{ \theta}}^{*}\right)+\tilde{\mathcal{L}}\left(\boldsymbol{\theta}^{*}\right) \right)+\frac{\log^{2}(1/\delta^{2})}{m}d+\frac{1}{\tau}\left(\left\|\boldsymbol {\bar{s}}_{0,\boldsymbol{\bar{\theta}}_{0}}\right\|_{\mathcal{H}}^{2}+\left\| \boldsymbol{\bar{s}}_{0,\boldsymbol{\theta}_{0}}\right\|_{\mathcal{H}}^{2}+ \left\|\boldsymbol{s}_{0,\boldsymbol{\theta}}\right\|_{\mathcal{H}}^{2}+\left\| \boldsymbol{s}_{0,\boldsymbol{\theta}}\right\|_{\mathcal{H}}^{2}\right)+D_{ \mathrm{KL}}\left(p_{T}\|\pi\right).\]

If one only focuses on the \((m,n)\)-dependence, i.e. the dependence on the model capacity and sample size, this upper bound can be further simplified as

\[D_{\mathrm{KL}}\left(p_{0}\|p_{0,\boldsymbol{\bar{\theta}}_{n}( \tau)}\right)\] \[\overset{\mathrm{(vi)}}{\lesssim}\left(\frac{\tau^{2}}{\sqrt{mn} }+\frac{\tau\sqrt{\tau}}{m}\right)\cdot\left(\sqrt{\tilde{\mathcal{L}}( \boldsymbol{\theta}^{*})}+\frac{1}{\sqrt{m\tau}}+\frac{\tau^{2}}{\sqrt{mn}}+ \frac{\tau\sqrt{\tau}}{m}\right)\] \[\quad+\left(\bar{\bar{\mathcal{L}}}\left(\boldsymbol{\bar{ \theta}}^{*}\right)+\tilde{\mathcal{L}}\left(\boldsymbol{\theta}^{*}\right) \right)+\frac{1}{m}+\frac{1}{\tau}+D_{\mathrm{KL}}\left(p_{T}\|\pi\right)\] \[\leq \left(\sqrt{\tilde{\mathcal{L}}(\boldsymbol{\theta}^{*})}+\frac{1 }{\sqrt{m\tau}}+\frac{\tau^{2}}{\sqrt{mn}}+\frac{\tau\sqrt{\tau}}{m}\right)^{2 }+\left(\bar{\bar{\mathcal{L}}}\left(\boldsymbol{\bar{\theta}}^{*}\right)+ \tilde{\mathcal{L}}\left(\boldsymbol{\theta}^{*}\right)\right)+\frac{1}{m}+ \frac{1}{\tau}+D_{\mathrm{KL}}\left(p_{T}\|\pi\right)\] \[\lesssim \left(\tilde{\mathcal{L}}(\boldsymbol{\theta}^{*})+\frac{1}{m \tau}+\frac{\tau^{4}}{mn}+\frac{\tau^{3}}{m^{2}}\right)+\left(\bar{\bar{ \mathcal{L}}}\left(\boldsymbol{\bar{\theta}}^{*}\right)+\tilde{\mathcal{L}} \left(\boldsymbol{\theta}^{*}\right)\right)+\frac{1}{m}+\frac{1}{\tau}+D_{ \mathrm{KL}}\left(p_{T}\|\pi\right)\] \[\lesssim \frac{\tau^{4}}{mn}+\frac{\tau^{3}}{m^{2}}+\frac{1}{\tau}+\frac{1 }{m}+\left(\bar{\bar{\mathcal{L}}}\left(\boldsymbol{\bar{\theta}}^{*}\right)+ \tilde{\mathcal{L}}\left(\boldsymbol{\theta}^{*}\right)\right)+D_{\mathrm{KL}} \left(p_{T}\|\pi\right),\]

where we assume \(\tau\geq 1\) for simplicity, and \(\overset{\mathrm{(vi)}}{\lesssim}\) hides the term \(d\log(d+1)\), the polynomials of \(\log(1/\delta^{2})\) and finite RKHS norms \(\|\cdot\|_{\mathcal{H}}\). The proof is completed. 

Approximation errors.For the (universal) approximation, we discuss in two points:

* First, the approximation is a separate problem that can be analyzed independently of training and generalization. While it is beyond the scope of current work, we have included the approximation error in the final estimates. When the approximation by random feature models fails, the generalization error is supposed to be significant.
* In addition, the random feature model can approximate Lipschitz continuous functions on a compact domain (Theorem 6 in [17]). Notice that the forward diffusion process defines a random path \((\boldsymbol{x}(t),t)_{t\in[0,T]}\) contained in a rectangular domain \(R_{T,\delta}:=[-C_{T,\delta},C_{T,\delta}]^{d}\times[0,T]\subset\mathbb{R}^{d+1}\) with \(C_{T,\delta}:=C_{T}(C_{\boldsymbol{x}}+\sqrt{\log(1/\delta^{2})})\) (use Lemma 1 and the boundness of inputs), one can apply Theorem 6 in [17] to bound (7) on the domain \(R_{T,\delta}\) in \(\mathbb{R}^{d+1}\) to obtain approximation results for Lipschitz continuous target score functions.

### Data-Dependent Generalization Gap

**Lemma 7** (Forward perturbation estimates, Gaussian mixtures).: _Suppose that \(x\) is sampled from a one-dimensional 2-mode Gaussian mixture: \(p_{0}(x)=q_{1}\mathcal{N}(x;-\mu,1)+q_{2}\mathcal{N}(x;\mu,1)\), where \(\mu>0\), \(q_{1},\,q_{2}>0\) with \(q_{1}+q_{2}=1\) are all constants. Then for any \(\delta>0\), \(\delta\ll 1\), \(\mu>\sqrt{\log(1/\delta^{2})}\), with the probability of at least \(1-\delta\), we have_

\[|x(t)|\lesssim C_{T}\left(\mu+\sqrt{\log(1/\delta^{2})}\right)\triangleq C_{T, \mu,\delta}.\] (39)

Proof.: It is straightforward to verify that

\[\mathbb{P}\left(\{x:|x-\mu|\leq\sqrt{\log(1/\delta^{2})}\}\cup\{x :|x+\mu|\leq\sqrt{\log(1/\delta^{2})}\}\right)\] \[= \,\mathbb{P}\{x:|x-\mu|\leq\sqrt{\log(1/\delta^{2})}\}+\mathbb{P }\{x:|x+\mu|\leq\sqrt{\log(1/\delta^{2})}\}\] \[= \,\int_{\mu-\sqrt{\log(1/\delta^{2})}}^{\mu+\sqrt{\log(1/\delta^ {2})}}p_{0}(x)dx+\int_{-\mu-\sqrt{\log(1/\delta^{2})}}^{-\mu+\sqrt{\log(1/ \delta^{2})}}p_{0}(x)dx\] \[\geq \,q_{2}\int_{\mu-\sqrt{\log(1/\delta^{2})}}^{\mu+\sqrt{\log(1/ \delta^{2})}}\mathcal{N}(x;\mu,1)dx+q_{1}\int_{-\mu-\sqrt{\log(1/\delta^{2})}} ^{-\mu+\sqrt{\log(1/\delta^{2})}}\mathcal{N}(x;-\mu,1)dx\] \[= \,q_{2}\int_{-\sqrt{\log(1/\delta^{2})}}^{\sqrt{\log(1/\delta^{2} )}}\mathcal{N}(x;0,1)dx+q_{1}\int_{-\sqrt{\log(1/\delta^{2})}}^{\sqrt{\log(1/ \delta^{2})}}\mathcal{N}(x;0,1)dx\] \[= \,(q_{1}+q_{2})\cdot\mathbb{P}\{\epsilon:|\epsilon|\leq\sqrt{ \log(1/\delta^{2})}\}\geq 1-\delta,\]

where the last inequality applies (21). That is, for any \(\delta>0\), \(\delta\ll 1\), with the probability of at least \(1-\delta\), we have

\[|x|\in[\mu-\sqrt{\log(1/\delta^{2})},\mu+\sqrt{\log(1/\delta^{2})}]=\Theta( \mu).\] (40)

Hence, Lemma 1 gives

\[|x(t)|\lesssim C_{T}\left(\mu+\sqrt{\log(1/\delta^{2})}\right),\] (41)

which completes the proof. 

**Lemma 8** (Monte Carlo estimates, Gaussian mixtures).: _Define the Monte Carlo error_

\[\mathrm{Err}_{\mathrm{MC}}=\mathrm{Err}_{\mathrm{MC}}(\bm{\theta},\bar{\bm{ \theta}};T,\lambda(\cdot)):=\mathbb{E}_{t\sim\mathcal{U}(0,T)}\left[\lambda(t )\cdot\mathbb{E}_{x(t)\sim p_{t}}\left[\|s_{t,\bm{\theta}}(x(t))-\bar{s}_{t, \bm{\theta}}(x(t))\|_{2}^{2}\right]\right].\] (42)

_Suppose that the trainable parameter \(\bm{a}\) and embedding function \(\bm{e}(\cdot)\) are both bounded, and \(x\) is sampled from a one-dimensional 2-mode Gaussian mixture: \(p_{0}(x)=q_{1}\mathcal{N}(x;-\mu,1)+q_{2}\mathcal{N}(x;\mu,1)\), where \(\mu>0\), \(q_{1}\), \(q_{2}>0\) with \(q_{1}+q_{2}=1\) are all constants. Then, given any \(\bar{\bm{\theta}}\), for any \(\delta>0\), \(\delta\ll 1\), \(\mu>\sqrt{\log(1/\delta^{2})}\), with the probability of at least \(1-\delta\), there exists \(\bm{\theta}\) such that_

\[\mathrm{Err}_{\mathrm{MC}}\lesssim\mu^{2}\frac{\log(1/\delta)}{m},\] (43)

_where \(\lesssim\) hides universal positive constants only depending on \(T\)._

Proof.: According to Lemma 7, we just need to follow the proof of Lemma 6 by replacing \(C_{T,\delta}\) by \(C_{T,\mu,\delta}\). Notably, based on (33) in the proof of Lemma 6, one can finally derive

\[\mathrm{Err}_{\mathrm{MC}}\lesssim\mu^{2}\frac{\log(1/\delta)}{m},\]

which gives the desired estimates. 

Proof of Theorem 2.: We decompose the loss \(\tilde{\mathcal{L}}(\hat{\bm{\theta}}_{n}(\tau))\) in the same way as in the proof of Theorem 1. In fact, Theorem 2 is similarly proved by replacing \(C_{T,\delta}\) in the proof of Theorem 1 by \(C_{T,\mu,\delta}\). Note that \(C_{T,\mu,\delta}=C_{T}\left(\mu+\sqrt{\log(1/\delta^{2})}\right)\lesssim\mu\) for \(\mu\gg 1\), the proof is completed.

**Remark 6**.: _A standard variance is used here for convenience. In general, if \(\text{var}=o(\mu)\) as \(\mu\rightarrow+\infty\) (e.g. a bounded var), similar analysis and results are supposed to hold. However, this is different when \(\text{var}=\Theta(\mu)\) as \(\mu\rightarrow+\infty\), since the modes are not separated in this case, and we can not characterize modes shift by simply varying \(\mu\)._

**Remark 7**.: _Simply scaling down inputs seems not to resolve the adverse effect of modes shift, since the ground truth \(\mu\) is unknown. One can use the input scale to approximate \(\mu\) on toy datasets, but it is not that trivial in practice, particularly for real-world applications with multiple high-dimensional modes in varied scales._

The model-target inconsistency.Informally, there is inconsistency between the score network model and target score function. In fact, given the target Gaussian mixture \(p_{0}(x)=q_{1}\mathcal{N}(x;-\mu,1)+q_{2}\mathcal{N}(x;\mu,1)\), the target score function is

\[s_{0}(x)=-x+\frac{q_{2}\mathcal{N}(x;\mu,1)-q_{1}\mathcal{N}(x;-\mu,1)}{q_{2} \mathcal{N}(x;\mu,1)+q_{1}\mathcal{N}(x;-\mu,1)}\mu,\]

which gives \(s_{0}(x)\approx-x+\mu\), \(x\geq 0\) and \(s_{0}(x)\approx-x-\mu\), \(x\leq 0\). While the score network model is

\[s_{0,\theta}(x)\approx\left(\frac{1}{m}\sum_{i:w_{i}>0}a_{i}w_{i}\right)x+ \left(\frac{1}{m}\sum_{i:w_{i}>0}a_{i}\bm{u}_{i}^{\top}\bm{e}(0)\right),\]

where "\(\approx\)" holds since \(|x|=\Theta(\mu)\) by (40) (\(\mu\gg 1\)). Both \(s_{0}\) and \(s_{0,\theta}\) are linear functions, but they have unmatched scales in slopes and intercepts: \(O(1)\) and \(O(\mu)\) for \(s_{0}\), but both \(O(a)\)'s for \(s_{0,\theta}\). That is, modeling Gaussian mixtures with large modes distances as random feature models is inconsistent.

**Remark 8**.: _Recall that the goal of this work is to estimate \(D_{\mathrm{KL}}\left(p_{0}\|p_{0,\hat{\bm{\theta}}_{n}(\tau)}\right)\), which aims at the density estimation characterization. This is different from [32], where the equivalence of learned and target data manifolds (support sets) is studied._

## Appendix B Additional Experiments

### Early-Stopping Generalization

We illustrate the early-stopping generalization gap established in Theorem 1 using the Adam optimizer. All the configurations remain the same as Section 4.1 except that the learning rate is now \(10^{-3}\).

From Figure 8, one can observe that the KL divergence achieves its minimum at the 1000th training epoch, and it starts to increase after this turning point. The plot is aligned with Theorem 1, which states that there exists an optimal early-stopping time when the model can generalize well, indicating the effectiveness of the upper bound. Further, the KL divergence begins to oscillate after the minimum point (1000th training epoch), which may suggest a phase transition in the KL divergence dynamics, and the transition point is around the optimal early-stopping time. The finding is aligned with SGD setting.

### Modes Shift Effect

We further test the relationship between the modes' distance and the generalization performance using the Adam optimizer under the same configurations.

In Figure 9 and Figure 10, it is shown that the training of modeled distributions exhibits the same two-stage dynamics as the SGD setting, indicating that _the modes shift effect holds not particularly for a certain optimizer_.

### Model Capacity Dependency

We also numerically study the dependency of generalization on the model capacity. Following the same configurations in Section 4.1.1, we conduct experiments for different hidden dimensions varying from \(2^{1}\) to \(2^{11}\).

Figure 8: The KL divergence dynamics under the Adam optimizer.

Figure 10: The Adam training dynamics when the distance between two modes is 30 (\(\mu=15\)).

Figure 9: The Adam training dynamics when the distance between two modes is 6 (\(\mu=3\)).

In Figure 11, the left plot shows the KL divergence from the trained distribution at the 1000th training epoch to the target distribution, and the right plot shows the time duration of the modeled distribution to generalize. Here, the generalization criterion we select is \(D_{\mathrm{KL}}\leq 10^{-1}\), and we stop training at epoch \(=10000\). Both the two plots in Figure 11 indicate that increasing model capacity benefits the generalization, which also verifies the corresponding theoretical results (\(m\)-dependency in Theorem 1 and Theorem 2).

Figure 11: Left: The KL divergence from the model trained after 1000 epochs to the target distribution under different hidden dimensions. Right: The earliest training epoch when \(D_{\mathrm{KL}}\leq 10^{-1}\). Here, \(\times\) means that the model does not generalize when the training is stopped at the maximum training epoch 10000.