# Diffusion Spectral Representation for Reinforcement Learning

Dmitry Shribak

Georgia Tech

shribak@gatech.edu

&Chen-Xiao Gao

Nanjing University

gaocx@lamda.nju.edu.cn

&Yitong Li

Georgia Tech

yli3277@gatech.edu

&Chenjun Xiao

CUHK(SZ)

chenjunx@cuhk.edu.cn

&Bo Dai

Georgia Tech

bodai@cc.gatech.edu

Equal Contribution. Correspondence to: Bo Dai <bodai@cc.gatech.edu>

38th Conference on Neural Information Processing Systems (NeurIPS 2024).

###### Abstract

Diffusion-based models have achieved notable empirical successes in reinforcement learning (RL) due to their expressiveness in modeling complex distributions. Despite existing methods being promising, the key challenge of extending existing methods for broader real-world applications lies in the computational cost at inference time, _i.e._, sampling from a diffusion model is considerably slow as it often requires tens to hundreds of iterations to generate even one sample. To circumvent this issue, we propose to leverage the flexibility of diffusion models for RL from a representation learning perspective. In particular, by exploiting the connection between diffusion models and energy-based models, we develop _Diffusion Spectral Representation (Diff-SR)_, a coherent algorithm framework that enables extracting sufficient representations for value functions in Markov decision processes (MDP) and partially observable Markov decision processes (POMDP). We further demonstrate how Diff-SR facilitates efficient policy optimization and practical algorithms while explicitly bypassing the difficulty and inference cost of sampling from the diffusion model. Finally, we provide comprehensive empirical studies to verify the benefits of Diff-SR in delivering robust and advantageous performance across various benchmarks with both fully and partially observable settings.

## 1 Introduction

Diffusion models have demonstrated remarkable generative modeling capabilities, achieving significant success in producing high-quality samples across various domains such as images and videos (Ramesh et al., 2021; Saharia et al., 2022; Brooks et al., 2024). In comparison to other generative approaches, diffusion models stand out for their ability to represent complex, multimodal data distributions, a strength that can be attributed to two primary factors. First, diffusion models progressively denoise data by reversing a diffusion process. This iterative refinement process empowers them to capture complicated patterns and structures within the data distribution, thus enabling the generation of samples with unprecedented accuracy (Ho et al., 2020). Second, diffusion models exhibit impressive mode coverage, effectively addressing a common issue of mode collapse encountered in other generative approaches (Song et al., 2020).

The potential of diffusion models is increasingly being investigated for sequential decision-making tasks. The inherent flexibility of diffusion models to accurately capture complex data distributionsmakes them exceptionally suitable for both model-free and model-based methods in reinforcement learning (RL). There are three main approaches that attempts to apply diffusion models, including _diffusion policy_(Wang et al., 2022; Chi et al., 2023), _diffusion-based planning_(Janner et al., 2022; Jackson et al., 2024; Du et al., 2024), and _diffusion world model_(Ding et al., 2024; Rigter et al., 2023). Empirical results indicate that diffusion-based approaches can stabilize the training process and enhance empirical performance compared with their conventional counterparts, especially in environments with high-dimensional inputs.

The flexibility of diffusion models, however, also comes with a substantial inference cost: generating even a single sample from a diffusion model is notably slow, typically requiring tens to thousands of iterations (Ho et al., 2020; Lu et al., 2022; Zhang and Chen, 2022; Song et al., 2023). Furthermore, prior works also consider generating multiple samples to enhance quality, which further exacerbates this issue (Rigter et al., 2023). The computational demands are particularly problematic for RL, since whether employing a diffusion-based policy or diffusion world model, the learning agent must frequently query the model for interactions with the environment during the learning phase or when deployed in the environment. This becomes the key challenge when extending diffusion-based methods for broader applications with more complex state spaces. Meanwhile, the planning with exploration issue has not been explicitly considered in the existing diffusion-based RL algorithms. The flexibility of diffusion models in fact induces extra difficulty in the implementation of the principle of optimism in the face of uncertainty in the planning step to balance the inherent trade-off between exploration vs. exploitation, which is indispensable in the online setting to avoid suboptimal policies (Lattimore and Szepesvari, 2020; Amin et al., 2021).

In conclusion, there has been insufficient work considering both efficiency and computation tractability for planning and exploration in a unified and coherent perspective, when applying diffusion model for sequential decision-making. This raises a very natural question, _i.e._,

_Can we exploit the flexibility of diffusion models with efficient planning and exploration for RL?_

In this paper, we provide an **affirmative** answer to this question, based on our key observation that diffusion models, beyond their conventional role as generative tools, can play a crucial role in learning sufficient representations for RL. Specifically,

* By exploiting the energy-based model view of the diffusion model, we develop a coherent algorithmic framework _Diffusion Spectral Representation (Diff-SR)_, designed to learn representations that capture the latent structure of the transition function in Section 3.2;
* We then show that such diffusion-based representations are sufficiently expressive to represent the value function of any policy, which paves the way for efficient planning and exploration, circumventing the need for sample generation from the diffusion model, and thus, avoiding the inference costs associated with prior diffusion-based methods in Section 3.3;
* We conduct comprehensive empirical studies to validate the benefits of Diff-SR, in both fully and partially observable RL settings, demonstrating its robust, superior performance and efficiency across various benchmarks in Section 5.

## 2 Preliminaries

In this section, we briefly introduce the Markov Decision Processes, as the standard mathematical abstraction for RL, and diffusion models, as the building block for our algorithm design.

Markov Decision Processes (MDPs).We consider _Markov Decision Processes_(Puterman, 2014) specified by the tuple \(\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathbb{P},r,\gamma,\mu_{0}\rangle\), where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, \(\mathbb{P}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\) is the transition function, \(r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is the reward function, \(\gamma\in[0,1)\) is the discount factor, \(\mu_{0}\in\Delta(\mathcal{S})\) is the initial state distribution2. The value function specifies the discounted cumulative rewards obtained by following a policy \(\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})\), \(V^{\pi}(s)=\mathbb{E}^{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{t})| s_{0}=s\right]\), where \(\mathbb{E}^{\pi}\) denotes the expectation under the distribution induced by the interconnection of \(\pi\) and the environment. The state-action value function is defined by

Footnote 2: We use the standard notation \(\Delta(\mathcal{X})\) to denote the set of probability distributions over a finite set \(\mathcal{X}\)

\[Q^{\pi}(s,a)=r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim\mathbb{P}(\cdot|s,a)}\left[ V^{\pi}(s^{\prime})\right]\.\]

The goal of RL is to find an optimal policy that maximizes the policy value, _i.e._, \(\pi^{*}=\operatorname*{argmax}_{\pi}\mathbb{E}_{s\sim\mu_{0}}[V^{\pi}(s)]\).

For any MDP, one can always factorize the transition operator through the singular value decomposition (SVD), _i.e._,

\[\mathbb{P}(s^{\prime}|s,a)=\left\langle\phi^{*}(s,a),\mu^{*}(s^{\prime})\right\rangle\] (1)

with \(\left\langle\cdot,\cdot\right\rangle\) defined as the inner product. Yao et al. (2014); Jin et al. (2020); Agarwal et al. (2020) considered a subset of MDPs, in which \(\phi^{*}\left(s,a\right)\in\mathbb{R}^{d}\) with finite \(d\), which is known as _Linear/Low-rank MDP_. The subclass is then generalized for infinite spectrum with fast decay eigenvalues (Ren et al., 2022). Leveraging this specific structure of the function class, this spectral view serves as an instrumental framework for examining the statistical and computational attributes of RL algorithms in the context of function approximation. In fact, the most significant advantage of exploiting said spectral structure is that we can represent its state-value function \(Q^{\pi}(s,a)\) as a linear function with respect to \(\left[r(s,a),\phi^{*}(s,a)\right]\) for _any policy_\(\pi\),

\[Q^{\pi}(s,a)=r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim\mathbb{P}(\cdot|s,a)} \left[V^{\pi}(s^{\prime})\right]=r(s,a)+\left\langle\phi^{*}(s,a),\int_{ \mathcal{S}}\mu^{*}(s^{\prime})V^{\pi}(s^{\prime})ds^{\prime}\right\rangle.\] (2)

It's important to highlight that in many practical scenarios, the feature mapping \(\phi^{*}\) is often unknown. In the meantime, the learning of the \(\phi^{*}\) is essentially equivalent to un-normalized conditional density estimation, which is notoriously difficult (LeCun et al., 2006; Song and Kingma, 2021; Dai et al., 2019). Besides the optimization intractability in learning, the coupling of exploration to learning also compounds the difficulty: learning \(\phi^{*}\) requires full-coverage data to capture \(\mathbb{P}(s^{\prime}|s,a)\), while the design of exploration strategy often relies on an accurate \(\phi^{*}\)(Jin et al., 2020; Yang et al., 2020). Recently, a range of spectral representation learning algorithms has emerged to address these challenges and provide an estimate of \(\phi^{*}\) in both online and offline settings (Uehara et al., 2021). However, existing methods either require designs of negative samples (Ren et al., 2022; Qiu et al., 2022; Zhang et al., 2022), or rely on additional assumptions on \(\phi^{*}\)(Ren et al., 2022; 20).

Diffusion Models.Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) are composed of a forward Markov process gradually perturbing the observations \(x_{0}\sim p_{0}\left(x\right)\) to a target distribution \(x_{T}\sim q_{T}\left(x\right)\) with corruption kernel \(q_{t+1|t}\), and a backward Markov process recovering the original observations distribution from the noisy \(x_{T}\). After \(T\) steps, the forward process forms a joint distribution,

\[q_{0:T}\left(x_{0:T}\right)=p_{0}\left(x_{0}\right)\prod_{t=0}^{T-1}q_{t+1|t }\left(x_{t+1}|x_{t}\right).\]

The reverse process can be derived by Bayes' rule from the joint distribution, _i.e._,

\[q_{t|t+1}\left(x_{t}|x_{t+1}\right)=\frac{q_{t+1|t}\left(x_{t+1}|x_{t}\right) q_{t}\left(x_{t}\right)}{q_{t+1}\left(x_{t+1}\right)},\]

with \(q_{t}\left(x_{t}\right)\) as the marginal distribution at \(t\)-step. Although we can obtain the expression of reverse kernel \(q_{t|t+1}(\cdot|\cdot)\) from Bayes's rule, it is usually intractable. Therefore, the reverse kernel is usually parameterized with a neural network, denoted as \(p^{\theta}(x_{t}|x_{t+1})\). Recognizing that the diffusion models are a special class of latent variable models (Sohl-Dickstein et al., 2015; Ho et al., 2020), maximizing the ELBO emerges as a natural choice for learning,

\[\ell_{\textit{elbo}}\left(\theta\right)=\mathbb{E}_{p_{0}\left(x\right)} \left[D_{KL}\left(q_{T|0}||q_{T}\right)+\sum_{t=1}^{T-1}\mathbb{E}_{q_{t|0}} \left[D_{KL}\left(q_{t|t+1,0}||p^{\theta}_{t|t+1}\right)\right]-\mathbb{E}_{q_ {10}}\left[\log p^{\theta}_{0|1}\left(x_{0}|x_{1}\right)\right]\right]\]

For continuous domain, the forward process of corruption usually employs Gaussian noise, _i.e._, \(q_{t+1|t}(x_{t+1}|x_{t})=\mathcal{N}(x_{t+1};\sqrt{1-\beta_{t+1}}x_{t},\beta_{ t+1}I)\), where \(t\in\left\{0,\ldots,T-1\right\}\). The kernel for backward process is also Gaussian and can be parametrized as \(p^{\theta}\left(x_{t}|x_{t+1}\right)=\mathcal{N}\left(x_{t};\frac{1}{\sqrt{1- \beta_{t+1}}}\left(x_{t+1}+\beta_{t+1}s^{\theta}\left(x_{t+1},t+1\right) \right),\beta_{t+1}I\right)\), where \(s^{\theta}\left(x_{t+1},t+1\right)\) denotes the score network with parameters \(\theta\). The ELBO can be specified as

\[\ell_{sm}\left(\theta\right)=\sum_{t=1}^{T}\left(1-\alpha_{t}\right)\mathbb{E }_{p_{0}}\mathbb{E}_{q_{t|0}}\left[\left\|s^{\theta}\left(x_{t},t\right)- \nabla_{x_{t}}\log q_{t|0}\left(x_{t}|x_{0}\right)\right\|^{2}\right],\] (3)

where \(\alpha_{t}=\prod_{i=1}^{t}(1-\beta_{i})\). With the learned \(s^{\theta}\left(x,t\right)\), the samples can be generated by sampling \(x_{T}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\), and then following the estimated reverse Markov chain with \(p^{\theta}\left(x_{t}|x_{t+1}\right)\) iteratively. To ensure the quality of samples from diffusion models, the reverse Markov chain requires tens to thousands of iterations, which induces high computational costs for diffusion model applications.

Random Fourier Features.Random Fourier features (Rahimi and Recht, 2007; Dai et al., 2014) allow us to approximate infinite-dimensional kernels using finite-dimensional feature vectors. Bochner's theorem states that a continuous function of the form \(k(x,y)=k(x-y)\) can be represented by a Fourier transform of a bounded positive measure (Bochner, 1932). For Gaussian kernel \(k(x-y)\), consider the feature \(z_{\omega}(x)=\exp(-\mathrm{i}\omega^{\top}x)\), with \(\omega\sim\mathcal{N}(0,I)\):

\[k(x-y) =\left[\int p(\omega)\exp(-\mathrm{i}\omega^{\top}(x-y))\mathrm{ d}\omega\right]=\mathbb{E}_{\omega}[-\exp(\mathrm{i}\omega^{\top}(x-y))]]\] (4) \[=\mathbb{E}_{\omega}\left[z_{\omega}(x)z_{\omega}(y)^{*}\right] =\left\langle z_{\omega}(x),z_{\omega}(y)\right\rangle_{\mathcal{N}(\omega)}.\]

where \(\left\langle\cdot,\cdot\right\rangle_{\mathcal{N}(\omega)}\) is a shorthand for \(\mathbb{E}_{\omega\sim\mathcal{N}(0,I)}[(\cdot,\cdot)]\). By sampling \(\omega_{1},\omega_{2},\ldots,\omega_{N}\sim\mathcal{N}(0,I)\), we can approximate \(k(x-y)\) with the inner product of finite-dimentional vectors \(\hat{\bm{z}}_{\omega}(x)=\frac{1}{\sqrt{N}}(z_{\omega_{1}}(x),z_{\omega_{2}}( x),\ldots,z_{\omega_{N}}(x))\) and \(\hat{\bm{z}}_{\omega}(y)=\frac{1}{\sqrt{N}}(z_{\omega_{1}}(y),z_{\omega_{2}}( y),\ldots,z_{\omega_{N}}(y))\).

## 3 Diffusion Spectral Representation for Efficient Reinforcement Learning

It is well known that the generation procedure of diffusion models becomes the major barrier for real-world application, especially in RL. Moreover, the complicated generation procedure makes the uncertainty estimation for exploration intractable. The spectral representation \(\phi^{*}(s,a)\) provides an efficient way for planning and exploration, as illustrated in Section 2, which inspires our _Diffusion Spectral Representation (Diff-SR)_ for RL, as our answer to the motivational question. As we will demonstrate below, the representation view of diffusion model enjoys the flexibility and also enables efficient planning and exploration, while directly bypasses the cost of sampling. For simplicity, we introduce Diff-SR in MDP setting in the main text. However, the proposed Diff-SR is also applicable for POMDPs as shown in Appendix B. We first illustrate the inherent challenges of applying diffusion models for representation learning in RL.

### An Impossible Reduction to Latent Variable Representation (Ren et al., 2022a)

In the latent variable representation (LV-Rep) (Ren et al., 2022a), the latent variable model is exploited for spectral representation \(\phi^{*}\left(s,a\right)\) in (1), by which an arbitrary state-value function can be linearly represented, and thus, efficient planning and exploration is possible. Specifically, in the LV-Rep, one considers the factorization of dynamics as

\[\mathbb{P}(s^{\prime}|s,a)=\int p(z|s,a)p(s^{\prime}|z)dz=\left\langle p(z|s,a ),p(s^{\prime}|z)\right\rangle_{L_{2}}.\] (5)

By recognizing the connection between (5) and SVD of transition operator (1), the learned latent variable model \(p(z|s,a)\) can be used as \(\phi^{*}(s,a)\) for linearly representing the \(Q^{\pi}\)-function for an arbitrary policy \(\pi\).

Since the diffusion model can be recast as a special type of latent variable model (Ho et al., 2020), the first straightforward idea is to extend LV-Rep with diffusion models for \(p(z|s,a)\). We consider the following forward process that perturbs each \(z_{i-1}\) with Gaussian noises:

\[p(z_{i}|z_{i-1},s,a),\quad\forall i=1,\ldots,k,\]

with \(z_{0}=s^{\prime}\). Then, following the definition of the diffusion model, the backward process can be set as

\[q(z_{i-1}|z_{i},s,a),\quad\forall i=0,\ldots,k,\] (6)

which are Gaussian distributions that denoise the perturbed latent variables. Thus, the dynamics model can be formulated as

\[\mathbb{P}(s^{\prime}|s,a)=\int\prod_{i=2}^{k}q(z_{i-1}|z_{i},s,a)q(s^{\prime }|z_{1},s,a)d\left\{z_{i}\right\}_{i=1}^{k}.\] (7)

Indeed, Equation (7) converts the diffusion model to a latent variable model for dynamics modeling. However, the dependency of \((s,a)\) in \(q(s^{\prime}|z_{1},s,a)\) correspondingly induces the undesirable dependency of \((s,a)\) into \(\mu(s^{\prime})\) in (1), and therefore the factorization provided by the diffusion model cannot linearly represent the \(Q^{\pi}\)-function -- the vanilla LV-Rep reduction from diffusion models is impossible.

### Diffusion Spectral Representation from Energy-based View

Instead of reducing to LV-Rep, in this work we extract the spectral representations by exploiting the relationship between diffusion models and energy-based models (EBMs).

Spectral Representation from EBMs.We parameterize the transition operator \(\mathbb{P}(s^{\prime}|s,a)\) using an EBM, _i.e._,

\[\mathbb{P}(s^{\prime}|s,a)=\exp\left(\psi(s,a)^{\top}\nu\left(s^{\prime}\right)- \log Z\left(s,a\right)\right),\,Z\left(s,a\right)=\int\exp\left(\psi(s,a)^{\top }\nu\left(s^{\prime}\right)\right)ds^{\prime}.\] (8)

By simple algebra manipulation,

\[\psi(s,a)^{\top}\nu\left(s^{\prime}\right)=-\frac{1}{2}\left(\left\|\psi\left( s,a\right)-\nu\left(s^{\prime}\right)\right\|^{2}-\left\|\psi\left(s,a\right) \right\|^{2}-\left\|\nu\left(s^{\prime}\right)\right\|^{2}\right),\]

we obtain the quadratic potential function, leading to

\[\mathbb{P}(s^{\prime}|s,a)\propto\exp\left(\left\|\psi\left(s,a\right)\right\|^ {2}\!/2\right)\exp\left(-\left\|\psi\left(s,a\right)-\nu\left(s^{\prime} \right)\right\|^{2}\!/2\right)\exp\left(\left\|\nu\left(s^{\prime}\right) \right\|^{2}\!/2\right).\] (9)

The term \(\exp\left(-\frac{\left\|\psi\left(s,a\right)-\nu\left(s^{\prime}\right)\right\| ^{2}}{2}\right)\) is the Gaussian kernel, for which we apply the random Fourier feature (Rahimi and Recht, 2007, Dai et al., 2014) and obtain the spectral decomposition of (8),

\[\mathbb{P}(s^{\prime}|s,a)=\left\langle\phi_{\omega}\left(s,a\right),\mu_{ \omega}\left(s^{\prime}\right)\right\rangle_{\mathcal{N}(\omega)},\] (10)

where \(\omega\sim\mathcal{N}(0,I)\), and

\[\phi_{\omega}\left(s,a\right)=\exp\left(-\mathbf{i}\omega^{\top}\psi\left(s,a \right)\right)\exp\left(\left\|\psi\left(s,a\right)\right\|^{2}\!/2-\log Z \left(s,a\right)\right),\] (11)

\[\mu_{\omega}\left(s^{\prime}\right)=\exp\left(-\mathbf{i}\omega^{\top}\nu \left(s^{\prime}\right)\right)\exp\left(\left\|\nu\left(s^{\prime}\right) \right\|^{2}\!/2\right).\] (12)

This bridges the factorized EBMs (8) to SVD, offering a spectral representation for efficient planning and exploration, as will be shown subsequently.

Exploiting the random feature to connect EBMs to spectral representation for RL was first proposed by Nachum and Yang (2021) and Ren et al. (2022), but only Gaussian dynamics \(p(s^{\prime}|s,a)\) has been considered for its closed-form \(Z\left(s,a\right)\) and tractable MLE. Equation (8) is also discussed in (Zhang et al., 2023; Ouhamma et al., 2023; Zheng et al., 2022) for exploration with UCB-style bonuses. Due to the notorious difficulty in MLE of EBMs caused by the intractability of \(Z\left(s,a\right)\)(Zhang et al., 2022), only special cases of (8) have been practically implemented. How to efficiently exploit the flexibility of general EBMs in practice still remains an open problem.

Representation Learning via Diffusion.We revisit the EBM understanding of diffusion models, which not only justifies the flexibility of diffusion models, but more importantly, paves the way for efficient learning of spectral representations (11) through Tweedie's identity for diffusion models.

Given \((s,a)\), we consider perturbing the samples from dynamics \(s^{\prime}\sim\mathbb{P}(s^{\prime}|s,a)\) with Gaussian noise, _i.e._, \(\mathbb{P}(\tilde{s}^{\prime}|s^{\prime};\beta)=\mathcal{N}\left(\sqrt{1- \beta}s^{\prime},\beta I\right)\). Then, we parametrize the corrupted dynamics as

\[\mathbb{P}(\tilde{s}^{\prime}|s,a;\beta)=\int\mathbb{P}(\tilde{s}^{\prime}|s^{ \prime};\beta)\mathbb{P}(s^{\prime}|s,a)ds^{\prime}\propto\exp\left(\psi\left( s,a\right)^{\top}\nu\left(\tilde{s}^{\prime},\beta\right)\right),\] (13)

where \(\psi\left(s,a\right)\) is shared across all noise levels \(\beta\), and \(\mathbb{P}(\tilde{s}^{\prime}|s,a;\alpha)\rightarrow\mathbb{P}(s^{\prime}|s,a)\) with \(\tilde{s}^{\prime}\to s^{\prime}\), as \(\beta\to 0\), there is no noise corruption on \(s^{\prime}\).

**Proposition 1** (Tweedie's Identity (Efron, 2011)).: _For arbitrary corruption \(\mathbb{P}\left(\tilde{s}^{\prime}|s^{\prime};\beta\right)\) and \(\beta\) in \(\mathbb{P}\left(\tilde{s}^{\prime}|s,a;\beta\right)\), we have_

\[\nabla_{\tilde{s}^{\prime}}\log\mathbb{P}(\tilde{s}^{\prime}|s,a;\beta)= \mathbb{E}_{\mathbb{P}(s^{\prime}|\tilde{s}^{\prime},s,a;\beta)}\left[\nabla_{ \tilde{s}^{\prime}}\log\mathbb{P}(\tilde{s}^{\prime}|s^{\prime};\beta)\right].\] (14)

This can be easily verified by simple calculation, _i.e._,

\[\nabla_{\tilde{s}^{\prime}}\log\mathbb{P}(\tilde{s}^{\prime}|s,a; \beta)=\frac{\nabla_{\tilde{s}^{\prime}}\mathbb{P}(\tilde{s}^{\prime}|s,a;\beta )}{\mathbb{P}(\tilde{s}^{\prime}|s,a;\beta)}=\frac{\nabla_{\tilde{s}^{\prime}} \int\mathbb{P}(\tilde{s}^{\prime}|s^{\prime};\beta)\mathbb{P}(s^{\prime}|s,a)ds^ {\prime}}{\mathbb{P}(\tilde{s}^{\prime}|s,a;\beta)}\] \[=\int\frac{\nabla_{\tilde{s}^{\prime}}\log\mathbb{P}(\tilde{s}^{ \prime}|s^{\prime};\beta)\mathbb{P}(\tilde{s}^{\prime}|s^{\prime};\beta) \mathbb{P}(\tilde{s}^{\prime}|s,a)}{\mathbb{P}(\tilde{s}^{\prime}|s,a;\beta)} ds^{\prime}=\mathbb{E}_{\mathbb{P}(s^{\prime}|s^{\prime},s,a;\beta)}\left[\nabla_{ \tilde{s}^{\prime}}\log\mathbb{P}(\tilde{s}^{\prime}|s^{\prime};\beta)\right].\] (15)

For Gaussian perturbation with (13), Tweedie's identity (14) is applied as

\[\psi\left(s,a\right)^{\top}\nabla_{\tilde{s}^{\prime}}\nu\left(\tilde{s}^{ \prime},\beta\right)=\mathbb{E}_{p(s^{\prime}|\tilde{s}^{\prime},s,a;\beta)} \left[\frac{\sqrt{1-\beta}s^{\prime}-\tilde{s}^{\prime}}{\beta}\right]\]

\[\Rightarrow\tilde{s}^{\prime}+\beta\psi\left(s,a\right)^{\top}\nabla_{\tilde{s}^{ \prime}}\nu\left(\tilde{s}^{\prime},\beta\right)=\sqrt{1-\beta}\mathbb{E}_{ \mathbb{P}(s^{\prime}|\tilde{s}^{\prime},s,a;\beta)}\left[s^{\prime}\right].\] (16)

Let \(\zeta\left(\tilde{s}^{\prime};\beta\right)=\nabla_{\tilde{s}^{\prime}}\nu\left( \tilde{s}^{\prime},\beta\right)\), we can learn \(\psi\left(s,a\right)\) and \(\zeta\left(\tilde{s}^{\prime};\beta\right)\) by matching both sides of (16),

\[\min_{\psi,\zeta}\ \mathbb{E}_{\beta}\mathbb{E}_{(s,a,\tilde{s}^{\prime})}\left[\left\| \tilde{s}^{\prime}+\beta\psi\left(s,a\right)^{\top}\zeta\left(\tilde{s}^{ \prime},\beta\right)-\sqrt{1-\beta}\mathbb{E}_{\mathbb{P}(s^{\prime}|\tilde{s}^{ \prime},s,a;\beta)}\left[s^{\prime}\right]\right\|^{2}\right]\] (17)which shares the same optimum of

\[\min_{\psi,\zeta}\ \ell_{\text{diff}}\left(\psi,\zeta\right):=\mathbb{E}_{\beta} \mathbb{E}_{\left(s,a,\tilde{s}^{\prime},s^{\prime}\right)}\left[\left\|\tilde{s }^{\prime}+\beta\psi\left(s,a\right)^{\top}\zeta\left(\tilde{s}^{\prime}, \beta\right)-\sqrt{1-\beta}s^{\prime}\right\|^{2}\right].\] (18)

The equivalence of (17) and (18) is provided in Appendix A.

Diffusion Spectral Representation for \(Q\)-function.The loss described by (18) estimates the score function \(\psi\left(s,a\right)^{\top}\zeta\left(\tilde{s}^{\prime},\beta\right)\) for diffusion models. In the context of generating samples, the score function suffices to drive the reverse Markov chain process. However, when deriving the random feature \(\phi_{\omega}\) defined in (11), the partition function \(Z(s,a)\) is indispensable. Furthermore, the random feature \(\phi_{\omega}(s,a)\) is only conceptual with infinite dimensions where \(\omega\sim\mathcal{N}\left(0,I\right)\). Next, we will proceed to analyze the structure of \(Z(s,a)\) and finally construct the spectral representation with the learned \(\psi\left(s,a\right)\).

We first illustrate \(Z(s,a)\) is also linearly representable by random features of \(\psi\left(s,a\right)\),

**Proposition 2**.: _Denote \(\rho_{\omega}\left(s,a\right):=\exp\left(-\mathbf{i}\omega^{\top}\psi\left(s,a\right)+\|\psi\left(s,a\right)\|^{2}/2\right)\), the partition function is linearly representable by \(\rho_{\omega}\left(s,a\right)\), i.e., \(Z\left(s,a\right)=\left\langle\rho_{\omega}\left(s,a\right),u\right\rangle_{ \mathcal{N}\left(\omega\right)}\)._

Proof.: We have \(\phi_{\omega}\left(s,a\right)=\frac{\rho_{\omega}\left(s,a\right)}{Z\left(s,a \right)}\), and \(\mathbb{P}\left(s^{\prime}|s,a\right)=\left\langle\frac{\rho_{\omega}\left(s,a \right)}{Z\left(s,a\right)},\mu_{\omega}\left(s^{\prime}\right)\right\rangle_{ \mathcal{N}\left(\omega\right)}\), which implies

\[\int\mathbb{P}\left(s^{\prime}|s,a\right)ds^{\prime}=1\Rightarrow\left\langle \frac{\rho_{\omega}\left(s,a\right)}{Z\left(s,a\right)},\underbrace{\int\mu_{ \omega}\left(s^{\prime}\right)ds^{\prime}}_{u}\right\rangle=1\Rightarrow \left\langle\rho_{\omega}\left(s,a\right),u\right\rangle_{\mathcal{N}\left( \omega\right)}=Z\left(s,a\right).\]

Plug this into (11) and (2), we can represent the \(Q^{\pi}\)-function for arbitrary \(\pi\) as

\[Q^{\pi}\left(s,a\right)=\left\langle\frac{\rho_{\omega}\left(s,a\right)}{ \left\langle\rho_{\omega}\left(s,a\right),u\right\rangle},\xi^{\pi}\right\rangle _{\mathcal{N}\left(\omega\right)}=\left\langle\underbrace{\exp\left(-\mathbf{i }\omega^{\top}\psi\left(s,a\right)-\log\left\langle\exp\left(-\mathbf{i} \omega^{\top}\psi\left(s,a\right)\right),u\right\rangle_{\mathcal{N}\left( \omega\right)}\right)}_{\varphi_{\omega,\omega}\left(s,a\right)},\xi^{\pi} \right\rangle_{\mathcal{N}\left(\omega\right)},\] (19)

which eliminates the explicit partition function calculation.

We thereby construct _Diffusion Spectral Representation (Diff-SR)_, a tractable finite-dimensional representation, by approximating \(\varphi_{\omega,u}(s,a)\) with some neural network upon \(\psi\left(s,a\right)\). Specifically, in the definition of \(\varphi_{\omega,u}\left(s,a\right)\) in (19), it contains Fourier basis \(\exp\left(-\mathbf{i}\omega^{\top}\psi\left(s,a\right)\right)\), suggesting the use of trigonometry functions (Rahimi and Recht, 2007) upon \(\psi\left(s,a\right)\), _i.e._, \(\sin\left(W_{1}^{\top}\psi\left(s,a\right)\right)\). Meanwhile, it also contains a product with \(\frac{1}{\left\langle\exp\left(-\mathbf{i}\omega^{\top}\psi\left(s,a\right) \right),u\right\rangle_{\mathcal{N}\left(\omega\right)}}\), suggesting the additional non-linearity over \(\sin\left(W_{1}^{\top}\psi\left(s,a\right)\right)\). Therefore, we consider the finite-dimensional neural network \(\phi_{\theta}\left(s,a\right)=\mathtt{elu}\left(W_{2}\sin\left(W_{1}^{\top} \psi\left(s,a\right)\right)\right)\in\mathbb{R}^{d}\) with learnable parameters \(\theta=\left(W_{1},W_{2}\right)\), to approximate the infinite-dimensional \(\varphi_{\omega}\left(s,a\right)\) with \(\omega\sim\mathcal{N}\left(0,I\right)\).

Remark (Connection to sufficient dimension reduction (Sasaki and Hyvarinen, 2018)):The theoretical properties of the factorized potential function in EBMs (8) have been investigated in (Sasaki and Hyvarinen, 2018). Specifically, the factorization is actually capable of universal approximation. Moreover, \(\psi\left(s,a\right)\) also constructs an implementation of sufficient dimension reduction (Fukumizu et al., 2009), _i.e._, given \(\psi\left(s,a\right)\), we have \(\mathbb{P}\left(s^{\prime}|s,a\right)=\mathbb{P}\left(s^{\prime}|\psi\left(s,a \right)\right)\), or equivalently \(s^{\prime}\perp\left(s,a\right)\middle|\psi\left(s,a\right)\). These properties justify the expressiveness and sufficiency of the learned \(\psi\left(s,a\right)\). However, \(\psi\left(s,a\right)\) in (Sasaki and Hyvarinen, 2018) is only estimated up to the partition function \(Z(s,a)\), which makes it not directly applicable for planning and exploration in RL as we discussed.

### Policy Optimization with Diffusion Spectral Representation

With the approximated finite-dimensional representation \(\phi_{\theta}\), the \(Q\)-function can be represented as a linear function of \(\phi_{\theta}\) and a weight vector \(\xi\in\mathbb{R}^{d}\),

\[Q_{\xi,\theta}(s,a)=\phi_{\theta}(s,a)^{\top}\xi\] (20)

This approximated value function can be integrated into any model-free algorithm for policy optimization. In particular, we update \((\xi,\theta)\) with the standard TD learning objective

\[\ell_{\text{critic}}(\xi,\theta)=\mathbb{E}_{s,a,r,s^{\prime}\sim\mathcal{D}} \left[\left(r+\gamma\mathbb{E}_{a^{\prime}\sim\pi}[Q_{\bar{\xi},\bar{\theta}}( s^{\prime},a^{\prime})]-Q_{\xi,\theta}(s,a)\right)^{2}\right]\,,\] (21)

where \(\pi\) is the learning algorithm's current policy, \((\bar{\xi},\bar{\theta})\) is the target network, \(\mathcal{D}\) is the replay buffer. We apply the _double Q-network trick_ to stabilize training [23]. In particular, two weights \((\xi_{1},\theta_{1}),(\xi_{2},\theta_{2})\) are initialized and updated independently according to (21). Then the policy is updated by considering \(\max_{\pi}\mathbb{E}_{s\sim\mathcal{D},a\sim\pi}[\min_{i\in\{1,2\}}Q_{\xi_{i}, \theta_{i}}(s,a)]\). Algorithm 2 presents the pseudocode of online RL with Diff-SR. This learning framework is largely consistent with standard online reinforcement learning (RL) approaches, with the primary distinction being the incorporation of diffusion representations. As more data is collected, Line 7 specifies the update of the diffusion representation \(\phi_{\theta}\) using Algorithm 1 and the latest buffer \(\mathcal{D}\).

Remark (Exploration with Diffusion Spectral Representation):Following [14], even \(\exp\left(-\mathbf{i}\omega^{\top}\psi\left(s,a\right)\right)\) is not enough for linearly representing \(Q^{\pi}\), we still can exploit \(\exp\left(-\mathbf{i}\omega^{\top}\psi\left(s,a\right)\right)\) for bonus calculation to implement the principle of optimism in the face of uncertainty for exploration in RL, _i.e._,

\[b(s,a)=1-K^{\top}\left(s,a\right)\left(K+\lambda I\right)^{-1}K\left(s,a\right),\] (22)

where \(\mathcal{D}\) is a dataset of state action pairs, \(k\left((s,a),(s^{\prime},a^{\prime})\right):=\exp\left(-\frac{\left\|\psi(s,a )-\psi\left(s^{\prime},a^{\prime}\right)\right\|^{2}}{2}\right)\), \(K(s,a):=\left[k((s,a),(s,a)_{i})\right]_{(s,a)_{i}\in\mathcal{D}}\), and \(K=\left[K((s,a)_{j})\right]_{(s,a)_{j}\in\mathcal{D}}\). The bonus (22) derivation is straightforward by applying the connection between random feature and kernel, similar to [10]. In practice, we can also approximate UCB bonus with Diff-SR as

\[b(s,a)=\phi_{\bar{\theta}}(s,a)^{\top}\left(\sum_{s^{\prime},a^{\prime}\in \mathcal{D}}\phi_{\bar{\theta}}(s,a)\phi_{\bar{\theta}}(s,a)^{\top}+\lambda I \right)^{-1}\phi_{\bar{\theta}}(s,a).\] (23)

These bonuses are subsequently added to the reward in (21) to facilitate exploration.

Remark (Comparison to Spectral Representation):Both the proposed Diff-SR and the existing spectral representation for RL [10, 11, 12, 13] are extracting the representation for \(Q\)-function. However, we emphasize that the major difference lies in that existing spectral representations seek low-rank linear representations, while our representation is sufficient for representing \(Q\)-function, but in a nonlinear form, as we revealed in (19). This nonlinearity in fact comes from the partition function \(Z\left(s,a\right)\), which is constrained to be \(1\) in [10, 11], and thus, less flexible for representation. Meanwhile, even with nonlinear representation, it has been shown that the corresponding bonus is still enough for exploration [14], without additional computational cost.

Related Work

**Representation learning in RL.** Learning good abstractions for the raw states and actions based on the structure of the environment dynamics is thought to facilitate policy optimization. To effectively capture the information in said dynamics, existing representation learning methods employ various techniques, such as reconstruction (Watter et al., 2015; Hafner et al., 2019; Fujimoto et al., 2023), successor features (Dayan, 1993; Kulkarni et al., 2016; Barreto et al., 2017), bisimulation (Ferms et al., 2004; Gelada et al., 2019; Zhang et al., 2021), contrastive learning (Oord et al., 2018; Nachum and Yang, 2021), and spectral decomposition (Mahadevan and Maggioni, 2007; Wu et al., 2018; Duan et al., 2019; Ren et al., 2022b). Previous works also leverage the assumption that the transition kernel possesses a low-rank spectral structure, which permits linear representations for the state-action value function and provably sample-efficient reinforcement learning (Jin et al., 2020; Yang and Wang, 2020; Agarwal et al., 2020; Uehara et al., 2022). Based on this, there have been several attempts towards both practical and theoretically grounded reinforcement learning algorithms by extracting the spectral representations from the transition kernel (Ren et al., 2022; Zhang et al., 2022; Ren et al., 2022; Zhang et al., 2023a). Our approach aligns with this paradigm but distinguishes itself by learning these representations via diffusion and enjoying the flexibility of energy-based modeling.

**Diffusion model for RL.** By virtue of their ability to model complex and multimodal distributions, diffusion models present themselves as well-suited candidates for specific components in reinforcement learning. For example, diffusion models can be utilized to synthesize complex behaviors (Janner et al., 2022; Ajay et al., 2022; Chi et al., 2023; Du et al., 2024), represent multimodal policies (Wang et al., 2022; Hansen-Estruch et al., 2023; Chen et al., 2023), or provide behavior regularizations (Chen et al., 2023). Another line of research utilizes the diffusion model as the world model. Among them, DWM (Ding et al., 2024) and SynthER (Lu et al., 2023) train a diffusion model with off-policy dataset and augment the training dataset with synthesized data, while PolyGRAD (Rigter et al., 2023) and PGD (Jackson et al., 2024) sample from the diffusion model with policy guidance to generate near on-policy trajectory. On a larger scale, UniSim (Yang et al., 2023) employs a video diffusion model to learn a real-world simulator that accommodates instructions in various modalities. All of these methods incur great computational costs because they all involve iteratively sampling from the diffusion model to generate actions or trajectories. In contrast, our method leverages the capabilities of diffusion models from the perspective of representation learning, thus circumventing the generation costs.

## 5 Experiments

We evaluate our method with state-based MDP tasks (Gym-MuJoCo locomotion (Todorov et al., 2012)) and image-based POMDP tasks (Meta-World Benchmark (Yu et al., 2020)) in this section. Besides, we also provide experiments with state-based POMDP tasks in Appendix E. Our code is publicly released at the project website.

### Results of Gym-MuJoCo Tasks

In this first set of experiments, we compare Diff-SR against both model-based and model-free baseline algorithms, using the implementations provided by MBBL (Wang et al., 2019). For representation-based RL algorithms, we include LV-Rep (Ren et al., 2022), SPEDE (Ren et al., 2022) and Deep

Figure 1: The performance curves of the Diff-SR and baseline methods on MBBL tasks. We report the mean (solid line) and one standard deviation (shaded area) across 4 random seeds.

Successor Feature (DeepSF) (Barreto et al., 2017) as baselines. Note that the SPEDE is a special case of Gaussian EBM. We also include PolyGRAD (Rigter et al., 2023), a recent method that utilizes diffusion models for RL as an additional baseline. All algorithms are executed for 200K environment steps and we report the mean and standard deviation of performances across 4 random seeds. More implementation details including the hyper-parameters can be found in Appendix C.

Table 1 presents the results, demonstrating that Diff-SR achieves significantly better or comparable performance to all baseline methods in most tasks except for Humanoid-ET. Specifically, in Ant and Walker, Diff-SR outperforms the second highest baseline LV-Rep by 90% and 48%. Moreover, Diff-SR consistently surpasses PolyGRAD in nearly all environments. We also provide the learning curves of Diff-SR and baseline methods in Figure 1 for an illustrative interpretation of the sample efficiency Diff-SR brings.

**Computational Efficiency and Runtime Comparison** Compared to other diffusion-based RL algorithms, Diff-SR harnesses diffusion's flexibility while circumventing the time-consuming sampling process. To showcase this, we record the runtime of Diff-SR and PolyGRAD on MBBL tasks using workstations equipped with Quadro RTX 6000 cards. Results in Figure 2 illustrate that Diff-SR is about \(4\times\) faster than PolyGRAD, and such advantage is consistent across all environments. We provide a per-task breakdown of the runtime results in Appendix 4 due to space constraints.

### Results of Meta-World Tasks

As the most difficult setting, we evaluate Diff-SR with 8 visual-input tasks selected from the Meta-World Benchmark. Rather than directly diffuse over the space of raw pixels, we resort to techniques similar to the Latent Diffusion Model (LDM) (Rombach et al., 2022) which first encodes the raw

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline  & & HalfCheath & Reacher & Humanoid-ET & Pendulum & **L**-Pendulum \\ \hline \multirow{4}{*}{Model-Based RL} & ME-TRPO* & \(2283.7\pm 900.4\) & \(-13.4\pm 5.2\) & \(72.9\pm 8.9\) & \(\mathbf{177.3\pm 1.9}\) & \(-126.2\pm 86.6\) \\  & PETS-RS* & \(966.9\pm 471.6\) & \(-40.1\pm 6.9\) & \(109.6\pm 102.6\) & \(167.9\pm 35.8\) & \(-12.1\pm 25.1\) \\  & PETS-CEM* & \(2795.3\pm 879.9\) & \(-12.3\pm 5.2\) & \(110.8\pm 90.1\) & \(167.4\pm 53.0\) & \(-20.5\pm 28.9\) \\  & Best MBBL* & \(3639.0\pm 1135.8\) & \(\mathbf{-4.1\pm 0.1}\) & \(1377.0\pm 150.4\) & \(\mathbf{177.3\pm 1.9}\) & \(\mathbf{0.0\pm 0.0}\) \\  & PolyGRAD & \(2563.5\pm 204.2\) & \(-20.7\pm 1.9\) & \(1026.6\pm 58.7\) & \(166.3\pm 6.3\) & \(-3.5\pm 4.8\) \\ \hline \multirow{4}{*}{Model-Free RL} & PPO* & \(17.2\pm 84.4\) & \(-17.2\pm 0.9\) & \(451.4\pm 39.1\) & \(163.4\pm 8.0\) & \(-40.8\pm 21.0\) \\  & TRPO* & \(-12.0\pm 85.5\) & \(-10.1\pm 0.6\) & \(289.8\pm 5.2\) & \(166.7\pm 7.3\) & \(-27.6\pm 15.8\) \\  & SAC* (3-layer) & \(4000.7\pm 202.1\) & \(-6.4\pm 0.5\) & \(\mathbf{179.4\pm 458.3}\) & \(168.2\pm 9.5\) & \(-0.2\pm 0.1\) \\ \hline \multirow{4}{*}{Representation RL} & DeepSF! & \(4180.4\pm 113.8\) & \(-16.8\pm 3.6\) & \(168.6\pm 5.1\) & \(168.6\pm 5.1\) & \(-0.2\pm 0.3\) \\  & SPEDE! & \(4210.3\pm 92.6\) & \(-7.2\pm 1.1\) & \(886.9\pm 95.2\) & \(169.5\pm 0.6\) & \(0.0\pm 0.0\) \\  & LV-Rep! & \(5557.6\pm 439.5\) & \(-5.8\pm 0.3\) & \(1086\pm 278.2\) & \(167.1\pm 3.1\) & \(\mathbf{0.0\pm 0.0}\) \\  & **Diff-SR** & \(\mathbf{7223.53\pm 437.1}\) & \(-6.5\pm 2.0\) & \(821.3\pm 118.9\) & \(162.3\pm 3.0\) & \(\mathbf{0.0\pm 0.1}\) \\ \hline \multirow{4}{*}{Model-Based RL} & \multirow{4}{*}{ME-TRPO*} & Anti-ET & Hopper-ET & S-Humanoid-ET & CartPole & Walker-ET \\  & & \(42.6\pm 21.1\) & \(1272.5\pm 500.9\) & \(-154.9\pm 534.3\) & \(160.1\pm 69.1\) & \(-1609.3\pm 657.5\) \\  & PETS-RS* & \(130.0\pm 148.1\) & \(205.8\pm 36.5\) & \(320.7\pm 182.2\) & \(195.0\pm 28.0\) & \(312.5\pm 493.4\) \\  & PETS-CEM* & \(81.6\pm 145.8\) & \(129.3\pm 36.0\) & \(355.1\pm 157.1\) & \(195.5\pm 3.0\) & \(260.2\pm 356.9\) \\  & Best MBBL* & \(275.4\pm 309.1\) & \(1272.5\pm 500.9\) & \(\mathbf{1084.3\pm 77.0}\) & \(\mathbf{200.0\pm 0.0}\) & \(312.5\pm 493.4\) \\  & PolyGRAD & \(43.6\pm 158.6\) & \(1151.6\pm 182.3\) & \(\mathbf{1110.1\pm 181.8}\) & \(193.4\pm 11.5\) & \(268.4\pm 77.3\) \\ \hline \multirow{4}{*}{Model-Free RL} & PPO* & \(80.1\pm 17.3\) & \(758.0\pm 62.0\) & \(454.3\pm 36.7\) & \(86.5\pm 7.8\) & \(306.1\pm 17.2\) \\  & TRPO* & \(116.8\pm 47.3\) & \(237.4\pm 33.5\) & \(281.3\pm 10.9\) & \(47.3\pm 15.7\) & \(229.5\pm 27.1\) \\  & SAC* (3-layer) & \(2012.7\pm 571.3\) & \(1815.5\pm 655.1\) & \(834.6\pm 313.1\) & \(\mathbf{199.4\pm 0.4}\) & \(2216.4\pm 678.7\) \\ \hline \multirow{4}{*}{Representation RL} & DeepSF! & \(768.1\pm 441.1\) & \(548.9\pm 253.3\) & \(533.8\pm 154.9\) & \(194.5\pm 5.8\) & \(165.6\pm 127.9\) \\  & SPEDED! & \(806.2\pm 60.2\) & \(732.2\pm 63.9\) & \(986.4\pm 154.7\) & \(138.2\pm 39.5\) & \(501.6\pm 204.0\) \\  & LV-Rep! & \(2511.8\pm 460.0\) & \(2204.8\pm 496.0\) & \(963.1\pm 45.1\) & \(\mathbf{200.7\pm 0.2}\) & \(2523.5\pm 333.9\) \\  & **Diff-SR** & \(\mathbf{4788.6\pm 623.1}\) & \(\mathbf{2800.5\pm 95.4}\) & \(\mathbf{1160.3\pm 150.1}\) & \(\mathbf{199.9\pm 1.1}\) & \(\mathbf{3722.2\pm 406.1}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performances of Diff-SR and baseline RL algorithms after 200K environment steps. Results are averaged across 4 random seeds and a window size of 10K steps. Results marked with * are taken from MBBL (Wang et al., 2019) and \(\dagger\) are taken from LV-Rep (Ren et al., 2022).

Figure 2: Runtime comparison between Diff-SR vs. LV-Rep vs. diffusion-based RL (PolyGRAD).

observation image to a 1-D compact latent vector and afterward performs the diffusion process over the latent space. To deal with the partial observability, we truncate the history frames with length \(L\), encode these \(L\) frames into their latent embeddings, concatenate them together, and treat them as the state of the agent. The learning of diffusion representation thus translates into predicting the next frame's latent embedding with the action and \(L\)-step concatenated embedding. Following existing practices (Zhang et al., 2023), we set \(L=3\) for all Meta-World tasks. We use DrQ-V2 (Yarats et al., 2021) as the base RL algorithm, and more details of the implementations are deferred to Appendix F.

The performance curves of Diff-SR and the baseline algorithms are presented in Figure 3. We see that Diff-SR achieves a greater than \(90\%\) success rate for seven of the tasks, 4 more tasks than the second best baseline \(\mu\)LV-Rep. Overall, Diff-SR exhibits superior performance, faster convergence speed, and stable optimization in most of the tasks compared to the baseline methods. Finally, although Diff-SR does not require sample generation, we present the reconstruction results to validate the efficacy of the score function \(\psi\left(s,a\right)^{\top}\zeta\left(\vec{s}^{\prime},\beta\right)\) in Figure 6.

## 6 Conclusions and Discussions

We introduce Diff-SR, a novel algorithmic framework designed to leverage diffusion models for reinforcement learning from a representation learning perspective. The primary contribution of our work lies in exploiting the connection between diffusion models and energy-based models, thereby enabling the extraction of spectral representations of the transition function. We demonstrate that such diffusion-based representations can sufficiently express the value function of any policy, facilitating efficient planning and exploration while mitigating the high inference costs typically associated with diffusion-based methods. Empirically, we conduct comprehensive studies to validate the effectiveness of Diff-SR in both fully and partially observable sequential decision-making problems. Our results underscore the robustness and advantages of Diff-SR across various benchmarks. However, the main limitation of this study is that Diff-SR has not yet been evaluated with real-world and multi-task data. Future work will focus on testing Diff-SR on real-world applications, such as robotic control.

Figure 3: Performance curves for image-based POMDP tasks from Meta-World. We report the mean (solid line) and the standard deviation (shaded area) of performances across 5 random seeds.

## References

* Agarwal et al. (2020) Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and representation learning of low rank mdps. _Advances in neural information processing systems_, 33:20095-20107, 2020a.
* Agarwal et al. (2020) Alekh Agarwal, Sham M. Kakade, Akshay Krishnamurthy, and Wen Sun. FLAMBE: structural complexity and representation learning of low rank mdps. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020b.
* Ajay et al. (2022) Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision-making? _arXiv preprint arXiv:2211.15657_, 2022.
* Amin et al. (2021) Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup. A survey of exploration methods in reinforcement learning. _arXiv preprint arXiv:2109.00157_, 2021.
* Barreto et al. (2017) Andre Barreto, Will Dabney, Remi Munos, Jonathan J. Hunt, Tom Schaul, David Silver, and Hado van Hasselt. Successor features for transfer in reinforcement learning. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 4055-4065, 2017.
* Bochner (1932) Salomon Bochner. Vorlesungen uber fouriersche integrale. In _Akademische Verlagsgesellschaft_, 1932.
* Brooks et al. (2024) Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/video-generation-models-as-world-simulators.
* Chen et al. (2023a) Huayu Chen, Cheng Lu, Zhengyi Wang, Hang Su, and Jun Zhu. Score regularized policy optimization through diffusion behavior. In _The Twelfth International Conference on Learning Representations_, 2023a.
* Chen et al. (2023b) Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. Offline reinforcement learning via high-fidelity generative behavior modeling. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Rigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023b.
* Chi et al. (2023) Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. _arXiv preprint arXiv:2303.04137_, 2023.
* Chua et al. (2018) Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* Clavera et al. (2018) Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-based reinforcement learning via meta-policy optimization. In _Conference on Robot Learning_, pages 617-629. PMLR, 2018.
* Dai et al. (2014) Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and Le Song. Scalable kernel methods via doubly stochastic gradients. _Advances in neural information processing systems_, 27, 2014.
* Dai et al. (2019) Bo Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le Song, and Dale Schuurmans. Exponential family estimation via adversarial dynamics embedding. _Advances in Neural Information Processing Systems_, 32, 2019.
* Dayan (1993) Peter Dayan. Improving generalization for temporal difference learning: The successor representation. _Neural Comput._, 5(4):613-624, 1993.
* Dwork et al. (2019)Marc Peter Deisenroth and Carl Edward Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Lise Getoor and Tobias Scheffer, editors, _Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011_, pages 465-472. Omnipress, 2011.
* Ding et al. [2024] Zihan Ding, Amy Zhang, Yuandong Tian, and Qinqing Zheng. Diffusion world model. _arXiv preprint arXiv:2402.03570_, 2024.
* Du et al. [2024] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. _Advances in Neural Information Processing Systems_, 36, 2024.
* Duan et al. [2019] Yaqi Duan, Zheng Tracy Ke, and Mengdi Wang. State aggregation learning from markov transition data. In _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 4488-4497, 2019.
* Efron [2011] Bradley Efron. Tweedie's formula and selection bias. _Journal of the American Statistical Association_, 106(496):1602-1614, 2011.
* Efroni et al. [2022] Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, and Sobhan Miryoosefi. Provable reinforcement learning with a short-term memory. In _International Conference on Machine Learning_, pages 5832-5850. PMLR, 2022.
* Ferns et al. [2004] Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. In _UAI '04, Proceedings of the 20th Conference in Uncertainty in Artificial Intelligence, Banff, Canada, July 7-11, 2004_, pages 162-169. AUAI Press, 2004.
* Fujimoto et al. [2018] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pages 1587-1596. PMLR, 2018.
* 16, 2023_, 2023.
* Fukumizu et al. [2009] Kenji Fukumizu, Francis R Bach, and Michael I Jordan. Kernel dimension reduction in regression. 2009.
* Gangwani et al. [2020] Tanmay Gangwani, Joel Lehman, Qiang Liu, and Jian Peng. Learning belief representations for imitation learning in pomdps. In _uncertainty in artificial intelligence_, pages 1061-1071. PMLR, 2020.
* Gelada et al. [2019] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Bellemare. Deepmdp: Learning continuous latent space models for representation learning. In _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 2170-2179. PMLR, 2019.
* Guo et al. [2023] Jiacheng Guo, Zihao Li, Huazheng Wang, Mengdi Wang, Zhuoran Yang, and Xuezhou Zhang. Provably efficient representation learning with tractable planning in low-rank pomdp. In _International Conference on Machine Learning_, pages 11967-11997. PMLR, 2023.
* Guo et al. [2018] Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Bernardo A Pires, and Remi Munos. Neural predictive belief representations. 2018.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 1861-1870. PMLR, 10-15 Jul 2018.
* Hafner et al. [2019a] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. 2019a.

Danijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 2555-2565. PMLR, 2019b.
* Hafner et al. (2021) Danijar Hafner, Timothy P. Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* Hansen-Estruch et al. (2023) Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. Idql: Implicit q-learning as an actor-critic method with diffusion policies. _arXiv preprint arXiv:2304.10573_, 2023.
* Heess et al. (2015) Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning continuous control policies by stochastic value gradients. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* Jackson et al. (2024) Matthew Thomas Jackson, Michael Tryfan Matthews, Cong Lu, Benjamin Ellis, Shimon Whiteson, and Jakob Foerster. Policy-guided diffusion. _arXiv preprint arXiv:2404.06356_, 2024.
* Janner et al. (2022) Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. _arXiv preprint arXiv:2205.09991_, 2022.
* Jin et al. (2020) Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on learning theory_, pages 2137-2143. PMLR, 2020.
* Kulkarni et al. (2016) Tejas D Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel J Gershman. Deep successor reinforcement learning. _arXiv preprint arXiv:1606.02396_, 2016.
* Kurutach et al. (2018) Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble trust-region policy optimization, 2018.
* Lattimore and Szepesvari (2020) Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* LeCun et al. (2006) Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. _Predicting structured data_, 1(0), 2006.
* Lee et al. (2020) Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. volume 33, pages 741-752, 2020.
* Levine and Abbeel (2014) Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014.
* Lu et al. (2022) Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _Advances in Neural Information Processing Systems_, 35:5775-5787, 2022.
* 16, 2023_, 2023.
* Luo et al. (2021) Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees, 2021.
* Mahadevan and Maggioni (2007) Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A laplacian framework for learning representation and control in markov decision processes. _J. Mach. Learn. Res._, 8:2169-2231, 2007.
* Ma et al. (2018)Ofir Nachum and Mengjiao Yang. Provable representation learning for imitation with contrastive fourier features. _Advances in Neural Information Processing Systems_, 34:30100-30112, 2021.
* Nagabandi et al. [2018] Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. In _2018 IEEE International Conference on Robotics and Automation (ICRA)_, pages 7559-7566, 2018. doi: 10.1109/ICRA.2018.8463189.
* Ni et al. [2021] Tianwei Ni, Benjamin Eysenbach, and Ruslan Salakhutdinov. Recurrent model-free rl can be a strong baseline for many pomdps. 2021.
* Oord et al. [2018] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv preprint arXiv:1807.03748_, 2018.
* Ouhamma et al. [2023] Reda Ouhamma, Debabrota Basu, and Odalric Maillard. Bilinear exponential family of mdps: frequentist regret bound with tractable exploration & planning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 9336-9344, 2023.
* Puterman [2014] Martin L Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* Qiu et al. [2022] Shuang Qiu, Lingxiao Wang, Chenjia Bai, Zhuoran Yang, and Zhaoran Wang. Contrastive ucb: Provably efficient contrastive self-supervised learning in online reinforcement learning. In _International Conference on Machine Learning_, pages 18168-18210. PMLR, 2022.
* Rahimi and Recht [2007] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. _Advances in neural information processing systems_, 20, 2007.
* Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International conference on machine learning_, pages 8821-8831. PMLR, 2021.
* Ren et al. [2022a] Tongzheng Ren, Chenjun Xiao, Tianjun Zhang, Na Li, Zhaoran Wang, Sujay Sanghavi, Dale Schuurmans, and Bo Dai. Latent variable representation for reinforcement learning. _arXiv preprint arXiv:2212.08765_, 2022a.
* Ren et al. [2022b] Tongzheng Ren, Tianjun Zhang, Lisa Lee, Joseph E Gonzalez, Dale Schuurmans, and Bo Dai. Spectral decomposition representation for reinforcement learning. _arXiv preprint arXiv:2208.09515_, 2022b.
* Ren et al. [2022c] Tongzheng Ren, Tianjun Zhang, Csaba Szepesvari, and Bo Dai. A free lunch from the noise: Provable and practical exploration for representation learning. In _Uncertainty in Artificial Intelligence_, pages 1686-1696. PMLR, 2022c.
* Rigter et al. [2023] Marc Rigter, Jun Yamada, and Ingmar Posner. World models via policy-guided trajectory diffusion. _arXiv preprint arXiv:2312.08533_, 2023.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pages 10674-10685. IEEE, 2022.
* Saharia et al. [2022] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in neural information processing systems_, 35:36479-36494, 2022.
* Sasaki and Hyvarinen [2018] Hiroaki Sasaki and Aapo Hyvarinen. Neural-kernelized conditional density estimation. _arXiv preprint arXiv:1806.01754_, 2018.
* Schulman et al. [2015] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Francis Bach and David Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 1889-1897, Lille, France, 07-09 Jul 2015. PMLR.
* Schulman et al. [2018]John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.
* Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.
* Song and Kingma (2021) Yang Song and Diederik P Kingma. How to train your energy-based models. _arXiv preprint arXiv:2101.03288_, 2021.
* Song et al. (2020) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* Song et al. (2023) Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. _arXiv preprint arXiv:2303.01469_, 2023.
* Tassa et al. (2012) Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behaviors through online trajectory optimization. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 4906-4913, 2012. doi: 10.1109/IROS.2012.6386025.
* Todorov et al. (2012) Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In _2012 IEEE/RSJ International Conference on Intelligent Robots and Systems_, pages 5026-5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109.
* Uehara et al. (2021) Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline rl in low-rank mdps. _arXiv preprint arXiv:2110.04652_, 2021.
* December 9, 2022_, 2022.
* Wang et al. (2019) Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement learning, 2019.
* Wang et al. (2022) Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning. _arXiv preprint arXiv:2208.06193_, 2022.
* Watter et al. (2015) Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin A. Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 2746-2754, 2015.
* Weigand et al. (2021) Stephan Weigand, Pascal Klink, Jan Peters, and Joni Pajarinen. Reinforcement learning using guided observability. 2021.
* Wu et al. (2018) Yifan Wu, George Tucker, and Ofir Nachum. The laplacian in rl: Learning representations with efficient approximations. _arXiv preprint arXiv:1810.04586_, 2018.
* Yang and Wang (2020) Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 10746-10756. PMLR, 2020.
* Yang et al. (2023) Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. _arXiv preprint arXiv:2310.06114_, 2023.
* Yang et al. (2020) Zhuoran Yang, Chi Jin, Zhaoran Wang, Mengdi Wang, and Michael Jordan. Provably efficient reinforcement learning with kernel and neural function approximations. _Advances in Neural Information Processing Systems_, 33:13903-13916, 2020.
* Yang et al. (2020)* Yao et al. (2014) Hengshuai Yao, Csaba Szepesvari, Bernardo Avila Pires, and Xinhua Zhang. Pseudo-mdps and factored linear action models. In _2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)_, pages 1-9. IEEE, 2014.
* Yarats et al. (2021) Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning, 2021.
* Yu et al. (2020) Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In _Conference on robot learning_, pages 1094-1100. PMLR, 2020.
* Zhang et al. (2021) Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* Zhang et al. (2023a) Hongming Zhang, Tongzheng Ren, Chenjun Xiao, Dale Schuurmans, and Bo Dai. Provable representation with efficient planning for partially observable reinforcement learning. _arXiv preprint arXiv:2311.12244_, 2023a.
* Zhang and Chen (2022) Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. _arXiv preprint arXiv:2204.13902_, 2022.
* Zhang et al. (2022) Tianjun Zhang, Tongzheng Ren, Mengjiao Yang, Joseph Gonzalez, Dale Schuurmans, and Bo Dai. Making linear mdps practical via contrastive representation learning. In _International Conference on Machine Learning_, pages 26447-26466. PMLR, 2022.
* Zhang et al. (2023b) Tianjun Zhang, Tongzheng Ren, Chenjun Xiao, Wenli Xiao, Joseph E Gonzalez, Dale Schuurmans, and Bo Dai. Energy-based predictive representations for partially observed reinforcement learning. In _Uncertainty in Artificial Intelligence_, pages 2477-2487. PMLR, 2023b.
* Zheng et al. (2022) Siriu Zheng, Lingxiao Wang, Shuang Qiu, Zuyue Fu, Zhuoran Yang, Csaba Szepesvari, and Zhaoran Wang. Optimistic exploration with learned features provably solves markov decision processes with neural dynamics. In _The Eleventh International Conference on Learning Representations_, 2022.

Detailed Derivations

**Proposition 3**.: _Equation (17) shares the same optimal solutions with Equation (18)._

Proof.: Denote \(b=\sqrt{1-\beta}\mathbb{E}_{\mathcal{(}s^{\prime}|^{S}\!\left[s^{\prime},s,a; \beta\right)}\left[s^{\prime}\right]\) for simplicity, and we have

\[\mathbb{E}_{\beta}\mathbb{E}_{\left(s,a,\tilde{s}^{\prime},s^{ \prime}\right)}\left[\left\|\tilde{s}^{\prime}+\beta\psi\left(s,a\right)^{\top }\zeta\left(\tilde{s}^{\prime},\beta\right)-\sqrt{1-\beta}s^{\prime}\right\|^ {2}\right]\] \[= \mathbb{E}_{\beta}\mathbb{E}_{\left(s,a,\tilde{s}^{\prime},s^{ \prime}\right)}\left[\left\|\tilde{s}^{\prime}+\beta\psi\left(s,a\right)^{\top }\zeta\left(\tilde{s}^{\prime},\beta\right)-b+b-\sqrt{1-\beta}s^{\prime}\right\| ^{2}\right]\] \[= \mathbb{E}_{\beta}\mathbb{E}_{\left(s,a,\tilde{s}^{\prime},s^{ \prime}\right)}\left[\left\|\tilde{s}^{\prime}+\beta\psi\left(s,a\right)^{\top }\zeta\left(\tilde{s}^{\prime},\beta\right)-b\right\|^{2}\right]\] \[+ \underbrace{\left(1-\beta\right)\mathbb{E}_{\beta}\mathbb{E}_{ \left(s,a,\tilde{s}^{\prime},s^{\prime}\right)}\left[\left\|s^{\prime}- \mathbb{E}_{\mathcal{(}s^{\prime}|^{S}\!\left[s^{\prime},s,a;\beta\right)} \left[s^{\prime}\right]\right\|^{2}\right]}_{\text{constant}}.\]

The second term equals to \(0\) comes from the definition of \(b\). Moreover, since \(b\) is independent w.r.t. \(s^{\prime}\), we have

\[\mathbb{E}_{\beta}\mathbb{E}_{\left(s,a,\tilde{s}^{\prime},s^{ \prime}\right)}\left[\left\|\tilde{s}^{\prime}+\beta\psi\left(s,a\right)^{\top }\zeta\left(\tilde{s}^{\prime},\beta\right)-b\right\|^{2}\right]=\mathbb{E}_{ \beta}\mathbb{E}_{\left(s,a,\tilde{s}^{\prime}\right)}\left[\left\|\tilde{s}^ {\prime}+\beta\psi\left(s,a\right)^{\top}\zeta\left(\tilde{s}^{\prime},\beta \right)-b\right\|^{2}\right].\]

We conclude that

\[\mathbb{E}_{\beta}\mathbb{E}_{\left(s,a,\tilde{s}^{\prime},s^{ \prime}\right)}\left[\left\|\tilde{s}^{\prime}+\beta\psi\left(s,a\right)^{\top }\zeta\left(\tilde{s}^{\prime},\beta\right)-\sqrt{1-\beta}s^{\prime}\right\|^ {2}\right]\] \[=\mathbb{E}_{\beta}\mathbb{E}_{\left(s,a,\tilde{s}^{\prime} \right)}\left[\left\|\tilde{s}^{\prime}+\beta\psi\left(s,a\right)^{\top} \zeta\left(\tilde{s}^{\prime},\beta\right)-b\right\|^{2}\right]+\text{constant}.\]

Therefore, we obtain the claim. 

## Appendix B Generalization for Partially Observable RL

In this section we discuss how to generalize Diff-SR to Partially Observable MDP (POMDP). We follow the definition of a POMDP given in [Efroni et al., 2022], which is formally denoted as a tuple \(\mathcal{P}=\left(\mathcal{S},\mathcal{A},\mathcal{O},r,H,\rho_{0},\mathbb{P}, \mathbb{O}\right)\), where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, and \(\mathcal{O}\) is the observation space. The positive integer \(H\) denotes the horizon length, \(\rho_{0}\) is the initial state distribution, \(r:\mathcal{O}\times\mathcal{A}\rightarrow[0,1]\) is the reward function, \(\mathbb{P}(\cdot|s,a):\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\) is the transition kernel capturing dynamics over latent states, and \(\mathbb{O}(\cdot|s):\mathcal{S}\rightarrow\Delta(\mathcal{O})\) is the emission kernel, which induces an observation from a given state.

The agent starts at a state \(s_{0}\) drawn from \(\rho_{0}(s)\). At each step \(h\), the agent selects an action \(a\) from \(\mathcal{A}\). This leads to the generation of a new state \(s_{h+1}\) following the distribution \(\mathbb{P}(\cdot|s_{h},a_{h})\), from which the agent observes \(o_{h+1}\) according to \(\mathbb{O}(\cdot|s_{h+1})\). The agent also receives a reward \(r(o_{h+1},a_{h+1})\). Observing \(o\) instead of the true state \(s\) leads to a non-Markovian transition between observations, which means we need to consider policies \(\pi_{h}:\mathcal{O}\times(\mathcal{A}\times\mathcal{O})^{h}\rightarrow\Delta (\mathcal{A})\) that depend on the entire history, denoted by \(\tau_{h}=\left\{o_{0},a_{0},\cdots,o_{h}\right\}\). Let \([H]:=\left\{0,\ldots,H\right\}\).

The value functions are defined by

\[V_{h}^{\pi}(b_{h})=\mathbb{E}\left[\sum_{t=h}^{H}r(o_{t},a_{t})|b_{h}\right], \,Q_{h}^{\pi}(b_{h},a_{h})=r(o_{h},a_{h})+\mathbb{E}_{\mathbb{P}_{b}}\left[V_{h+ 1}^{\pi}(b_{h+1})\right].\] (24)

**Definition 1** (\(L\)-decodability [Efroni et al., 2022]).: \(\forall h\in[H]\)_, define_

\[x_{h}\in\mathcal{X}:=(\mathcal{O}\times\mathcal{A})^{L-1}\times \mathcal{O},\] (25) \[x_{h}=(o_{h-L+1},a_{h-L+1},\cdots,o_{h}).\]

_A POMDP is \(L\)-decodable if there exists a decoder \(p^{*}:\mathcal{X}\rightarrow\Delta(\mathcal{S})\) such that \(p^{*}(x_{h})=b(\tau_{h})\)._That is, under \(L\)-decodability assumption, it is sufficient to recover the belief state by an \(L\)-step memory \(x_{h}\) rather than the entire history \(\tau_{h}\). This implies that we can parameterize the Q-value as a function of the observation history \(Q_{h}^{\pi}(x_{h},a_{h})\), rather that the unknown belief state \(Q_{h}^{\pi}(b_{h}(x_{h}),a_{h})\). By exploiting this \(L\)-decodability, Zhang et al. (2023) propose a probable efficient linear function approximation of \(Q_{h}^{\pi}(x_{h},a_{h})\) by considering a \(L\)-step prediction \(\mathbb{P}^{\pi}(x_{h+L}|x_{h},a_{h})\).

Inspired by this, we apply EBM for \(\mathbb{P}^{\pi}(x_{h+L}|x_{h},a_{h})\),

\[\mathbb{P}^{\pi}(x_{h+L}|x_{h},a_{h}) =\exp\left(\psi(x_{h},a_{h})^{\top}\nu\left(x_{h+L}\right)-\log Z \left(x_{h},a_{h}\right)\right),\] (26) \[Z\left(x_{h},a_{h}\right) =\int\exp\left(\psi(x_{h},a_{h})^{\top}\nu\left(x_{h+L}\right) \right)\mathrm{d}\,x_{h+L}\,.\] (27)

We then apply the techniques presented in Section 3.2 to learn diffusion representation \(\phi(x_{h},a_{h})\in\mathbb{R}^{d}\) as an approximation to the random features

\[\phi_{\omega}\left(x_{h},a_{h}\right)=\exp\left(-\mathbf{i}\omega^{\top}\psi \left(x_{h},a_{h}\right)\right)\exp\left(\|\psi\left(x_{h},a_{h}\right)\|^{2 }/2-\log Z\left(x_{h},a_{h}\right)\right)\,.\] (28)

The learned representation is subsequently utilized to parameterize the value function \(Q_{h}^{\pi}(x_{h},a_{h})\) for policy optimization as demonstrated by (Zhang et al., 2023). In particular, we consider value function approximation \(Q_{\xi,\theta}(x_{h},a_{h})=\phi_{\theta}(x_{h},a_{h})^{\top}\xi\) and update it by

\[\ell_{\text{critic}}(\xi)=\mathbb{E}_{x,a,r,x^{\prime}\sim\mathcal{D}}\left[ \left(r+\gamma\mathbb{E}_{a^{\prime}\sim\pi}[Q_{\xi,\theta}(x^{\prime},a^{ \prime})]-Q_{\xi,\theta}(x,a)\right)^{2}\right]\,,\] (29)

The policy, now conditioned on the \(L\)-step history \(x\), is updated by \(\max_{\pi}\mathbb{E}_{x\sim\mathcal{D},a\sim\pi}[\min_{i\in\{1,2\}}Q_{\xi, \theta}(x,a)]\). We refer interested readers to (Zhang et al., 2023) for more details on representation learning in POMDPs.

## Appendix C Details and Analysis for Fully Observable MDP Experiments

### Baseline Methods

For baseline methods, we include ME-TRPO (Kurutach et al., 2018), PETS (Chua et al., 2018), and the best model-based results among Luo et al. (2021); Deisenroth and Rasmussen (2011); Heess et al. (2015); Clavera et al. (2018); Nagabandi et al. (2018); Tassa et al. (2012) and Levine and Abbeel (2014) from MBBL (Wang et al., 2019). For model-free algorithms, we include PPO (Schulman et al., 2017), TRPO (Schulman et al., 2015) and SAC (Haarnoja et al., 2018).

### Experiment Setups

We implemented our algorithm based on Soft Actor-Critic (SAC). We use _feature update ratio_ to denote the frequency of updating the diffusion representations as compared to critic updates. We

\begin{table}
\begin{tabular}{l l} \hline Hyperparameter & Value \\ \hline Actor Learning Rate & 0.003 \\ Critic Learning Rate & 0.0003 \\ Learning Rate for \(\psi,\zeta,\theta\) & 0.0001 \\ Actor Hidden Layer Dimensions & (256, 256) \\ Diff-SR Representation Dimension & 256 \\ Discount factor \(\gamma\) & 0.99 \\ Critic Soft Update Factor \(\tau\) & 0.005 \\ Batch Size & 1024 \\ Number of Noise Levels & 1000 \\ \(\psi\) Network Width & 256 \\ \(\psi\) Network Hidden Depth & 1 \\ \(\zeta\) Network Width & 512 \\ \(\zeta\) Network Hidden Depth & 1 \\ \hline \end{tabular}
\end{table}
Table 2: Hyperparameters used for Diff-SR in state-based MDP environments.

sweep the value of this parameter within [1, 3, 5, 10, 20] for all MuJoCo experiments and report the configuration that achieved the best results. Other hyper-parameters are listed in Table 2. For evaluation, we test all methods every 5,000 environment steps by simulating 10 episodes and recording the cumulative return. The reported results are the average return over the last four evaluations and four random seeds.

## Appendix D Computational Efficiency and Runtime Comparison

Full results are presented in Figure 4. We use exactly the same experimental setups as used in Table 1, which has been described in 5.1. We observe that in all cases, Diff-SR is about \(4\times\) faster than PolyGRAD. Such efficiency can be attributed to the fact that while we utilize diffusion to train the representations, we do not have to sample from the diffusion model iteratively, which is the main computational bottleneck for SOTA diffusion-based RL algorithms.

## Appendix E Details and Analysis for State-based Partially Observable MDP Experiments

### Implementation Details

We also experiment Diff-SR with state-based Partially Observable MDP (POMDP) tasks. We contruct a partially observable variant based on OpenAI gym MuJoco [Todorov et al., 2012]. Adhereing to standard methodology, we mask the velocity components within the observations presented to the agent, effectively rendering the tasks partially observable [Ni et al., 2021, Weigand et al., 2021, Gangwani et al., 2020]. Under this masking scheme, a single observation is insufficient for decision-making. Therefore, the agent must aggregate past observations to infer the missing information and select appropriate actions. The reconstruction of missing information can be done by aggregating a past history of length \(L\), as demonstrated in Definition 1.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline  & HalfCheetah & Humanoid & Walker & Ant & Hopper & Pendulum \\ \hline
**Diff-SR** & **3864.2 \(\pm\) 482.3** & 650.1 \(\pm\) 57.4 & **1860.5 \(\pm\) 912.1** & **3189.7 \(\pm\) 720.7** & **1357.6 \(\pm\) 506.4** & **167.4 \(\pm\) 4.4** \\ \(\mu\)LV-Rep & 3596.2 \(\pm\) 874.5 & **806.7 \(\pm\) 120.7** & 1298.1\(\pm\) 276.3 & 1621.4 \(\pm\) 472.3 & 1096.4 \(\pm\) 130.4 & 168.2 \(\pm\) 5.3 \\ Dreamer-v2 & 2863.8 \(\pm\) 386 & 672.5 \(\pm\) 36.6 & 1305.8 \(\pm\) 234.2 & 1252.1 \(\pm\) 284.2 & 758.3 \(\pm\) 115.8 & **172.3 \(\pm\) 8.0** \\ SAC-MLP & 1612.0 \(\pm\) 223 & 242.1 \(\pm\) 43.6 & 736.5 \(\pm\) 65.6 & 1612.0 \(\pm\) 223 & 614.15 \(\pm\) 67.6 & 163.6 \(\pm\) 9.3 \\ SLAC & 3012.4 \(\pm\) 724.6 & 387.4 \(\pm\) 69.2 & 536.5 \(\pm\) 123.2 & 1134.8 \(\pm\) 326.2 & 739.3 \(\pm\) 98.2 & 167.3 \(\pm\) 11.2 \\ PSR & 2679.75 \(\pm\)386 & 534.4 \(\pm\) 36.6 & 862.4 \(\pm\) 355.3 & 1128.3 \(\pm\) 166.6 & 818.8 \(\pm\) 87.2 & 159.4 \(\pm\) 9.2 \\ PolyGRAD & 987.1 \(\pm\) 374.6 & 764.5 \(\pm\) 91.2 & 211.4 \(\pm\) 100.4 & 493.0 \(\pm\) 95.4 & 527.8 \(\pm\) 97.7 & 174.0 \(\pm\) 6.1 \\ \hline Best-FO & 7223.5\(\pm\)437.15 & 823.1\(\pm\)118.9 & 3722.2\(\pm\)406.1 & 4788.6\(\pm\)623.1 & 2800.5\(\pm\)95.4 & 162.3\(\pm\)3.0 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Performance on various continuous control problems with partial observation. We average results across 4 random seeds and a window size of 10K. Diff-SR achieves a similar or better performance compared to the baselines. Here, Best-FO denotes the performance of Diff-SR using full observations as inputs, providing a reference on how well an algorithm can achieve most in our tests.

Figure 4: Per-task runtime of Diff-SR, LV-Rep and PolyGRAD on tasks from MBBL.

The POMDP experiment follows a similar setup to the fully observable one described in Appendix C. In the partially observable setting, velocity information is masked, and we concatenate the past \(L=3\) observations follow (Zhang et al., 2023a). The universal hyperparameters used for POMDP are listed in Table 4. For each individual environment, we explored an array of parameters and chose the highest-performing configuration. Specifically, the critic and actor learning rates were varied across \(\{0.0015,0.00015\}\), the model learning rate was varied across \(\{0.0001,0.0003,0.0008\}\) and the feature update ratio was varied across \(\{1,3,5,10,20\}\).

We evaluate six baselines in our experiments: a diffusion approach, PolyGRAD (Rigter et al., 2023), two model-based approaches, Dreamer (Hafner et al., 2019, 2021) and Stochastic Latent Actor-Critic (SLAC) (Lee et al., 2020), a model-free baseline, SAC-MLP, that concatenates history sequences (past four observations) as input to an MLP layer for both the critic and policy, and the neural PSR (Guo et al., 2018). We also compared to a representation-based baseline, \(\mu\)LV-Rep (Zhang et al., 2023a). All methods are evaluated using the same procedure as in the fully observable setting. As a reference, we also provide the best performance achieved in the fully observable setting (without velocity masking), denoted as Best-FO, which serves as a benchmark for the optimal result an algorithm can achieve in our tests.

### Results and Analysis

Table 3 presents all experiment results, showing effectiveness of Diff-SR in partially observable continuous control tasks. The proposed method delivers superior results in 4 out of 6 tasks (HalfCheetah, Walker, Ant, Hopper). It significantly outperforms other algorithm in Walker and Ant, and achieved a comparable result with the lowest standard deviation on Pendulum, indicating consistent performance.

The wall time comparison between Diff-SR and PolyGRAD is shown in Figure 5. In the Humanoid task, Diff-SR is approximately 3 times faster than PolyGRAD, whereas in the other tasks, it is about 4 times faster.

### Ablations And Modifications

Masking ObservationsObservations consist of both velocities and positions. Masking velocities, rather than positions, more accurately reflects real-world scenarios where positions are typically directly observed, whereas velocities must be inferred. In the humanoid environment, we specifically mask only the q-velocity component.

Figure 5: Per-task running time of Diff-SR and PolyGRAD on tasks from MBBL with partial observation.

Random ActionAt the beginning of training and evaluation, we randomly sample actions from the action space. This is necessary because we use concatenated observations to sample actions from the policy. This approach might explain the larger standard deviation observed in POMDP compared to the fully observable case.

## Appendix F Image-based Partially Observable MDP Experiment

### Implementation Details

Instead of directly diffusing over the raw pixel space, we used the VAE structure from Zhang et al. (2023) to first encode the pixel observation \(o\) into a 1-D latent embedding \(e\). To deal with the partial observability, we also follow Zhang et al. (2023) to concatenate the embeddings of the past three observations (denoted as \(o^{3}\)) together as the state of the agent, denoted as \(e^{3}\). Let the next frame be \(o^{\prime}\) and its embedding be \(e^{\prime}\), the representation learning objective thus translates to fitting the score function \(\psi(e^{3},a)\) and \(\zeta(\tilde{e}^{\prime},\beta)\), i.e. performing the diffusion in the latent space. In the following paragraphs, we will detail the architectures for Diff-SR.

Diffusion Representation LearningUnlike previous state-based experiments, we formalize the network \(\psi\) and \(\zeta\) using the LN_ResNet architecture proposed by IDQL (Hansen-Estruch et al., 2023). Compared to standard MLP networks, LN_ResNet is equipped with layer normalization and skip connections, making it expressive enough for diffusion modeling. For representation learning, it is worthwhile to note that, apart from the loss objectives defined in Eq (18), we also preserve the gradients of the representation networks and train them with the critic's loss defined in (21). This design choice aligns with previous works (Zhang et al., 2023) and encourages the representations to contain task-relevant information.

RL optimizationWe develop our code based on DrQ-V2 (Yarats et al., 2021), a model-free RL algorithm designed for tasks with visual inputs. We preserved most of the design choices from DrQ-V2, except for the architectures of the actor and the critic. For the actor network, it receives the concatenated embeddings \(e^{3}\) and outputs an action. The critic networks receive the diffusion representation \(\phi_{\theta}\) as defined in the main text and predict the Q-values.

The hyper-parameters for the score functions \(\psi,\zeta\), the actor and the critic networks are listed in Table 5.

### Generation Results

In addition to the learning curves, we also present the qualitative generation results using the learned score functions. Figure 6 illustrates the progression of the diffusion process over time. In the figure, we sample a batch of data \((o^{3},a,o^{\prime})\) from the replay buffer, and the first row depicts the ground-truth target image \(o^{\prime}\). Starting from the second row, we sample a random Gaussian noise \(\tilde{e}^{\prime}_{1000}\sim\mathcal{N}(0,I)\) and iteratively apply the learned reverse diffusion process using the guidance of \((e^{3},a)\) to generate the latent embeddings at various stages of the diffusion. For every 200 steps, we pass the latent embedding to the decoder to obtain reconstructed images \(\tilde{o}^{\prime}_{t}\) and visualize them in the figure.

\begin{table}
\begin{tabular}{l l} \hline \hline Hyperparameter & Value \\ \hline Actor Hidden Layer Dimensions & (512, 512) \\ Diff-SR Representation Dimension & 512 / 128 (cheetah) \\ Discount Factor \(\gamma\) & 0.99 \\ Critic Soft Update Factor \(\tau\) & 0.005 \\ Batch Size & 1024 \\ Number of Noise Levels & 1000 \\ \(\psi\) Network Width & 512 \\ \(\psi\) Network Hidden Depth & 1 \\ \(\zeta\) Network Width & 256 \\ \(\zeta\) Network Hidden Depth & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyperparameters used for Diff-SR in state-based POMDP experiments.

The second row corresponds to the initial noisy embeddings and exhibits significant distortion and noise. As the denoising steps progress, we observe a gradual reduction in noise and an increasing clarity in the generated images. By the time we reach \(\tilde{o}^{\prime}_{600}\), the overall structure of the scene becomes more discernible, although some artifacts remain. Further along the denoising process, the images exhibit substantial improvements in terms of detail and realism. The final output, \(\tilde{o}^{\prime}_{0}\), closely resembles the original observations, indicating the effectiveness of our score functions in capturing the underlying information about the data.

\begin{table}
\begin{tabular}{l l} \hline Hyper-parameters & Value \\ \hline Actor Learning Rate & 0.0001 \\ Critic Learning Rate & 0.0001 \\ Learning Rate for \(\psi,\zeta,\theta\) & 0.0003 \\ Actor Hidden Layer Dimensions & (1024, 1024) \\ Diff-SR Representation Dimension & 1024 \\ Discount Factor \(\gamma\) & 0.99 \\ Critic Soft Update Factor \(\tau\) & 0.01 \\ Batch Size & 1024 \\ Number of Noise Levels & 1000 \\ LN\_ResNet Layer Width for \(\psi\) & 512 \\ LN\_ResNet Layer Width for \(\zeta\) & 512 \\ \# of LN\_ResNet Layers for \(\psi\) & 4 \\ \# of LN\_ResNet Layers for \(\zeta\) & 2 \\ \end{tabular}
\end{table}
Table 5: Hyperparameters used for Diff-SR in image-based POMDP experiments.

Figure 6: The generation results of Diff-SR.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims are that (1) by exploiting the energy-based model view of the diffusion model, we develop an algorithm to learn a latent variable representation of the transition function (Section 3.2) and (2) we demonstrate that said representation is sufficiently expressive to linearly represent the value function, paving the way for efficient planning and exploration, circumventing the need for sample generation from the diffusion model, and thus, avoiding the inference costs associated with prior diffusion-based methods (Section 3.3). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the assumptions in the derivations and experiments. We also address limitations in the conclusion. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide full sets of assumptions in the main text body and if an additional proof is required, we supplement in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide a representation training algorithm implementation description in Algorithm 2 and full online pipeline implementation in Algorithm 1. We provide experimental setup and procedures in 5. We also provide additional implementation details and hyperparameter set in C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have included all experimental details; code is released on our website. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including h ow to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide full algorithm description in Algorithm 1 and 2, and experiment details in Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For all experimental results, we run over multiple random seeds and provide error bars for each result. Guidelines:* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We run our experiments on a Quadro RTX 6000 and provide run time results. Our experiments can be done on standard graphics cards and do not require excessive computational capabilities. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We acknowledge and affirm the importance of the NeurIPS Code of Ethics. We have carefully considered the ethical implications of our work throughout the research and development process. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts**Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We develop reinforcement learning algorithms with broad applicability. These algorithms can be used for diverse applications, some with positive outcomes and others with potential risks. Our contribution lies in the general method, not its specific use case. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: This project does not use data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have credited creators and original owners of assets (e.g., code, data, models) used in the paper. The license and terms of use are explicitly mentioned and properly respected.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The assets are not released at this time. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA].

Justification: The paper does not involve IRB approval, crowdsourcing nor research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.