# GIC: Gaussian-Informed Continuum for Physical Property Identification and Simulation

 Junhao Cai\({}^{1*}\) Yuji Yang\({}^{2*}\) Weihao Yuan\({}^{3\dagger}\) Yisheng He\({}^{3}\)

**Zilong Dong\({}^{3}\) Liefeng Bo\({}^{3}\) Hui Cheng\({}^{2}\) Qifeng Chen\({}^{1}\)**

\({}^{1}\)The Hong Kong University of Science and Technology, \({}^{2}\)Sun Yat-sen University, \({}^{3}\)Alibaba Group

\({}^{*}\) Equal contribution, order determined by coin toss. \({}^{\dagger}\) Corresponding author.

###### Abstract

This paper studies the problem of estimating physical properties (system identification) through visual observations. To facilitate geometry-aware guidance in physical property estimation, we introduce a novel hybrid framework that leverages 3D Gaussian representation to not only capture explicit shapes but also enable the simulated continuum to render object masks as 2D shape surrogates during training. We propose a new dynamic 3D Gaussian framework based on motion factorization to recover the object as 3D Gaussian point sets across different time states. Furthermore, we develop a coarse-to-fine filling strategy to generate the density fields of the object from the Gaussian reconstruction, allowing for the extraction of object continuums along with their surfaces and the integration of Gaussian attributes into these continuums. In addition to the extracted object surfaces, the Gaussian-informed continuum also enables the rendering of object masks during simulations, serving as 2D-shape guidance for physical property estimation. Extensive experimental evaluations demonstrate that our pipeline achieves state-of-the-art performance across multiple benchmarks and metrics. Additionally, we illustrate the effectiveness of the proposed method through real-world demonstrations, showcasing its practical utility. Our project page is at https://jukgei.github.io/project/gic.

## 1 Introduction

Identifying the physical properties of objects (i.e., system identification) is essential for numerous applications such as games, digital twins, and robotic manipulation [1, 2, 3]. Although humans can intuitively deduce the underlying physical properties with a single glance when the object undergoes deformation, estimating the properties with only visual observations remains challenging for computational perceptual algorithms.

To tackle this challenge, many established methods [4, 5, 6] adopt the assumption of elastic material [7] and perform physics-based modeling based on mass-spring systems (MSS) or finite element method (FEM) to model and simulate the dynamics of the objects. Such an assumption inevitably restricts the ability to simulate more general types beyond elastic materials, such as fluids or granular media. Another problem of previous methods lies in that many methods [8, 9, 10] require the ground-truth full knowledge of object geometry for the identification, which limits their practicality. Some subsequent methods [5, 4] turn to recover the geometries and physical properties from observations in a decoupled manner. Specifically, these methods first extract object geometries by making use of stereo observations or dynamic neural reconstruction [11] from RGB video sequences, and then perform simulation directly on the point clouds or after the tetrahedral mesh conversion. While these methods introduce explicit geometries to guide the estimation of physical properties, the noisy reconstruction results usually lead to degraded system identification performance.

Recently, PAC-NeRF [12] integrates neural radiance fields (NeRF) [13] with a continuum dynamic model to tackle the above problems. The object geometries and physical properties are captured in a unified framework. Despite its effectiveness, this method possesses two limitations. Firstly, the implicit shapes represented by NeRF often lead to inferior geometries, which might cause inaccurate trajectories during simulation. Secondly, PAC-NeRF renders the novel views of deformed objects based on the appearance radiance field reconstructed from the static scene, which might introduce texture distortion, particularly when objects undergo significant deformations, resulting in discrepancies between the rendered and the observed images [14].

To address these limitations, this paper proposes a novel hybrid solution based on 3D Gaussians [15; 16] and material point method (MPM) [17; 18]. The core strength of this work is that we make use of both 3D shapes from dynamic 3D Gaussian reconstruction and 2D shapes rendered by the Gaussian-informed continuum for physical property estimation.

To generate more precise shapes to reason physical property, we first propose a _motion-factorized dynamic 3D Gaussian network_ to conduct dynamic scene reconstruction. We then extract the continuum from the recovered 3D Gaussians at each frame by leveraging a _coarse-to-fine filling strategy_ to generate the density field of the object progressively. The resulting density fields can be used to sample continuum particles for simulation and extract object surfaces as 3D-shape supervision in physical property estimation. To eliminate the appearance distortion caused by large deformation in PAC-NeRF, we further assign Gaussian attributes to the continuum particles where the opacity and scale attributes are evaluated from the density field. Such _Gaussian-informed continuum_ are able to render object masks during simulation, which can be regarded as a 2D-shape representation to guide the estimation and effectively avoid using inferior rendering results for learning physical properties.

To demonstrate the superiority of the proposed method over other baselines, we conduct three types of experiments, including evaluations of physical properties, dynamic reconstruction, and future state simulation. We also demonstrate a real-world application in digital twins and robotic manipulation, showing the applicability of the proposed method in real-world scenarios.

Our contributions are summarized as follows.

* We propose a novel hybrid pipeline that takes advantage of the 3D Gaussian representation of the object to both acquire 3D shapes and empower the simulated continuum to render 2D shapes for physical property estimation.
* We propose a novel dynamic 3D Gaussian framework with motion factorization to achieve more precise dynamic reconstruction. We also propose a coarse-to-fine filling strategy to generate the density field of the object, which can be utilized to extract object surfaces and obtain Gaussian-informed continuum particles.
* Extensive experiments show that our pipeline attains state-of-the-art performance on existing benchmarks with a wide range of metrics. We also present a real-world demonstration to show the efficiency of the proposed method.

## 2 Related Work

**Dynamic reconstruction**. Reconstructing dynamic scenes from monocular or multi-view video(s) is a long-standing problem in the computer vision community [19; 20]. Previous works exploit neural implicit representation [21; 22] for non-rigid reconstruction. These methods either reconstruct the scene in a frame-wise manner [23; 24] or maintain a canonical shape and model the deformation with a neural network [25; 26; 11; 27]. While effective for novel view synthesis, these methods often require extensive training time and can result in noisy deformations owing to the implicit representation, which may compromise the utility of the recovered geometries for physical property estimation [12]. Recent progress in 3D Gaussian Splitting (3DGS) technique [15] stands out to be a prevalent method for 3D reconstruction and novel view synthesis because of the abilities of explicit shape modeling and extremely fast view rendering. Similar to non-rigid NeRF, many follow-up works extend the 3DGS into 4D by treating each frame separately [28] or decomposing a scene into a canonical 3D Gaussian point cloud and a deformation model that warps the canonical shape into a specific scene [16; 29; 30]. In this paper, we draw upon these prior studies [16; 29] and propose a novel motion-factorized dynamic 3D Gaussian network to achieve better performance on reconstruction and novel view synthesis.

**System identification**. Understanding the physics laws of the 3D world is beneficial for simulation [31; 32; 33; 34; 6] and manipulation [3; 36; 37; 38; 3; 38]. However, unveiling these properties from visual information is an extremely difficult task due to the ambiguity introduced by incomplete observation and the high degrees of freedom of the scene. Early works [39; 40] study the problem by learning physical properties via interactions. With recent improvements in differentiable physics simulation [17; 18; 41; 42; 43; 44; 45], many methods turn to evaluate the physical properties by comparing the rendering results with 2D ground truth given the prior knowledge about the object geometry. VEO [5] presents a differentiable simulator to learn patterns from 4D reconstruction and force-displacement measurements. Another approach [4] eliminates the dependence of captured forces by proposing an iteration framework between deformation tracking and parameter optimization. While these methods demonstrate promising results, the inferior reconstruction might lead to degraded performance, and the assumption of elastic material restricts the applicability. PAC-NeRF [12] instead proposes a single framework to recover both the unknown geometry and physical properties of deformable objects from multi-view video sequences. However, the inferior geometries and blurry rendered images might have detrimental effects on physical property reasoning. In this work, we adopt MPM as our simulation framework following the approach used in PAC-NeRF due to its ability to simulate a variety of materials [46; 47; 48; 6]. Unlike previous approaches, we utilize dynamic 3D Gaussians to reconstruct explicit 3D geometries and generate simulatable continuum particles. Furthermore, we enhance the particles with Gaussian attributes, facilitating the rendering of 2D shapes, and thereby improving physical parameter estimation.

## 3 Preliminary

In this section, we briefly review the core idea of 3D Gaussian Splatting (3DGS) [15] and introduce its point-based alpha blending to render depth maps and foreground masks. Typically, 3DGS utilizes 3D Gaussians, each defined by a central point \(\mu_{0}\), a covariance matrix \(\Sigma_{0}\), a density value \(\sigma\), and a color attribute \(c\), to efficiently render images from specific viewpoints. Each point is denoted as

\[G(x)=\exp(-\frac{1}{2}(x-\mu_{0})^{T}\Sigma_{0}^{-1}(x-\mu_{0})),\] (1)

where \(\Sigma_{0}\) can be factorized as \(\Sigma_{0}=R_{0}S_{0}S_{0}^{T}R_{0}^{T}\), in which \(R_{0}\) is a rotation matrix represented by a quaternion vector \(r_{0}\in\mathbb{R}^{4}\), and \(S_{0}\) is a a diagonal scaling matrix characterized by a 3D vector \(s_{0}\in\mathbb{R}^{3}\). If we consider isotropic Gaussian representation, the scaling matrix can be written as \(s_{0}I\), where \(s_{0}\) is a scalar and \(I\) is the identity matrix. When performing splatting, the 3D Gaussians are projected into 2D with the covariance matrix defined as \(\Sigma_{0}^{\prime}=JW\Sigma_{0}W^{T}J^{T}\), where \(J\) is the Jacobian of affine approximation of the projective transformation [49], and \(W\) is the viewing transformation matrix. The rendered color \(I(u)\) with its foreground mask \(A(u)\) at pixel \(u\) are then evaluated by integrating \(N\) ordered slanted Gaussians via the point-based alpha blending. Since the depth of each Gaussian point at a specific view can be obtained according to its transformation matrix, we can further render the depth map \(D\) using the same blending method [50; 16], as

\[I(u)=\sum_{i\in N}T_{i}\alpha_{i}c_{i},\qquad A(u)=\sum_{i\in N}T_{i}\alpha_{i },\qquad D(u)=\sum_{i\in N}T_{i}\alpha_{i}d_{i},\] (2)

where \(T_{i}=\prod_{j=1}^{i-1}(1-\alpha_{j})\) is the accumulated transmittance, \(\alpha_{i}\) is the probability of termination at point \(i\), and \(d_{i}\) is the depth of the Gaussian point at the specific view.

## 4 Method

### Problem Definition and Overview

In this work, we aim to reconstruct the geometries and the physical properties of various object types from multi-view videos. Formally, given a set of video sequences \(\{V_{i}|i=1...n\}\) with moving object and the corresponding camera extrinsic and intrinsic parameters \(\{(T_{i},K_{i})|i=1...n\}\), the goal of this task is to recover the explicit geometries of the object represented by continuum particles \(P(t)\) and its corresponding physical parameters \(\Theta\) (e.g., Young's modulus \(E\) and Poisson's ratio \(\nu\) for elastic objects). We follow the assumption in PAC-NeRF and PhysGaussian [12; 51] that the object types (e.g., elastic, granular, Newtonian/non-Newtonian, plastic) are known and the physical phenomenon follows continuum mechanics [52; 17].

The overview of the proposed pipeline is illustrated in Fig. 1, which consists of three modules: a motion-factorized dynamic 3D Gaussian network (Sec. 4.2) for 4D reconstruction of the object, a coarse-to-fine density field generation strategy (Sec. 4.3) for continuum generation, surface extraction, and Gaussian attribute assignment, and a procedure (Sec. 4.4) showing how we leverage Gaussian-informed continuum and extracted surfaces to estimate physical properties.

### Motion-factorized Dynamic 3D Gaussian Network

Our dynamic 3D Gaussian network follows existing frameworks [16; 29; 30] that simultaneously maintain a canonical 3D Gaussian set and a deformation field modeled by a neural network to warp the canonical shape into object states at specific times. The core idea of this pipeline, presented in Fig. 2, is that the motion of every point in the object can be decomposed into a small range of motion bases.

**Architecture**. We first factorize the entire motion into \(N_{m}\) bases that are modeled by a fully connected neural network, where every basis shares a common backbone except the final layer. The output of each basis consists of the deformations at position \(d\mu_{i}(t)\in\mathbb{R}^{3}\) and at scale \(ds_{i}(t)\in\mathbb{R}\). To model the exact deformation for each position, we next propose a lightweight coefficient network that maps the positions at canonical space with specific time to their corresponding motion coefficients \(w(\mu_{0},t)\in\mathbb{R}^{N_{m}}\). Therefore, the deformed position and the scale for each Gaussian point are evaluated by the linear combination of the motion basis according to the motion coefficients:

\[\mu(t)=\mu_{0}+\sum_{i=1}^{N_{m}}w_{i}(\mu_{0},t)d\mu_{i}(t),\qquad s(t)=s_{0} +\sum_{i=1}^{N_{m}}w_{i}(\mu_{0},t)ds_{i}(t).\] (3)

In this work, we regard all the Gaussians as isotropic kernels, which has been demonstrated as an effective way to simplify the model and better reconstruct the scene [6; 53]. We should note that

Figure 1: Overview. (a) **Continuum Generation:** Given a series of multi-view images capturing a moving object, the motion-factorized dynamic 3D Gaussian network is trained to reconstruct the dynamic object as 3D Gaussian point sets across different time states. From the reconstructed results, we employ a coarse-to-fine strategy to generate density fields to recover the continuums and extract object surfaces. The continuum is endowed with Gaussian attributes to allow mask rendering. (b) **Identification:** The MPM simulates the trajectory with the initial continuum \(\mathbb{P}(0)\) and the physical parameters \(\Theta\). The simulated object surfaces and the rendered masks are then compared against the previously extracted surfaces (colored in blue) and the corresponding masks from the dataset. The differences are quantified to guide the parameter estimation process. (c) **Simulation:** Digital twin demonstrations are displayed. Simulated objects (colored by stress increasing from blue to red), characterized by the properties estimated from observation, exhibit behavior consistent with real-world objects.

although previous works [29; 54] also perform motion decomposition modeling, our pipeline shows two major differences: 1) instead of modeling each basis with an independent neural network, our module shares a common backbone. Our key observation is that for reconstructing a dynamic object, all points on the object should follow a similar moving tendency, and the final heads of the neural network are sufficient to model the details of different parts of the object; 2) to increase the ability to fit high rank of the dynamic scene [16], we model the motion coefficients as time-variant variables rather than constant Gaussian attributes [29].

**Optimization**. We employ the same setting in [16] to train our pipeline. Concretely, the canonical 3D Gaussians are initialized with points randomly sampled from the given bounding box of the scene. We start training the deformation network after 3,000 iterations of warm-up for the 3D Gaussians. Similar to previous works [16; 29], we optimize the pipeline by computing the L1 norm and Structural Similarity Index Measure (SSIM) between the rendered image \(I\) and the ground truth image \(\tilde{I}\). Moreover, since large scales may lead to inaccurate reconstructed shapes [55], we thus perform L1 norm on the scale attributes of all the points to recover more fine-grand shapes of the object. Therefore, the overall loss function is defined as:

\[\mathcal{L}_{gs}=\mathcal{L}_{1}(I,\tilde{I})+\lambda_{1}\mathcal{L}_{ssim}(I,\tilde{I})+\lambda_{2}\mathcal{L}_{1}(s(t)),\] (4)

where \(\lambda_{1}\) and \(\lambda_{2}\) are balancing hyperparameters. More in-depth analysis of the proposed pipeline, including implementation details and effects of scale regularization, are presented in Appendix A.1.

### Gaussian-informed Continuum Generation

**Coarse-to-fine density field generation**. Since the reconstructed Gaussian particles are served for rendering only, meaning that they are not evenly distributed on the objects, they cannot be directly used for simulation [51]. Therefore, we propose a novel coarse-to-fine filling strategy to iteratively generate density fields of the object based on the reconstructed Gaussian particles from Eqn. 3 and the internal particles filtered by the rendered depth maps. The proposed strategy is presented in Alg. 1. The implementation details and visual results are illustrated in Appendix A.2.

Concretely, the internal particles, initialized by uniform sampling from the bounding box of Gaussian particles, are filtered by projecting the particles to various images to compare the projected depth with rendered depth values (lines 1-6 in Alg. 1). The resulting particles can roughly represent the shape of the object. However, as denoted in Eqn. 2, the rendered depth maps are evaluated in an accumulated manner, making them less precise in representing the object surface.

Therefore, We employ a coarse-to-fine filling strategy by iteratively upsampling the density field and reassigning the densities on the indices computed from both the Gaussian and internal particles (lines 8-16 in Alg. 1). Fig. 3 provides a sketch illustration of the proposed strategy. Specifically, due to the large grid size at the initial stage, the object is completely inside the voxels with high densities. Next, we sequentially perform upsampling (line 10), mean filtering (line 13), and reassigning the

Figure 2: The pipeline of the proposed dynamic 3D Gaussian network. The motion network backbone consists of 8 fully connected (FC) layers. The output of the motion block is fed to \(N_{m}\) heads to generate motion residuals. The coefficient network contains 4 FC layers.

field (line 14) at each iteration. The first two operations produce more fine-grained shapes, and the reassigning operation ensures high densities at the surface to avoid over-erosion caused by the first two steps. Finally, the continuum particles with the corresponding object surfaces can be extracted by thresholding the density field (lines 16-17 in Alg. 1).

```
0: Gaussian particles at time \(t\): \(\mathbb{P}_{G}(t)=\{(\mu(t),s(t),\sigma,c)\}\); \(n\) pairs of camera extrinsic and intrinsic parameters: \(\{(\tilde{T}_{i},K_{i})|i=1...n\}\); parameters: grid size \(\Delta x\); number of upsampling steps \(n_{u}\); thresholds \(th_{min}\), \(th_{min}\);
0: Continuum particles \(\tilde{P}(t)\) and the corresponding surface \(\tilde{S}(t)\);
1: Randomly sample an initial particle set \(P_{in}\) from the bounding box of \(\{\mu(t)\}\);
2:for\(i\gets 1,n\)do
3:\(\tilde{D}_{i}=GaussianSplitting(\mathbb{P}_{G}(t),T_{i},K_{i})\); \(\triangleright\) render depth map at view \(i\)
4:\((u_{in},v_{in}),d_{in}\gets Proj(P_{in},T_{i},K_{i})\); \(\triangleright\) obtain image indices and depths of \(P_{in}\) at view \(i\)
5:\(P_{in}\gets P_{in}[\tilde{D}_{i}(u_{in},v_{in})\leq d_{in}]\); \(\triangleright\) filter out particles that are outside the object
6:endfor
7: Initialize the zero-value density field \(F(t)\) with \(\Delta x\) and the bounding box of \(\{\mu(t)\}\);
8:for\(j\gets 1,n_{u}\)do
9:if\(j\neq 1\)then
10:\(F(t)-TriminearInterpolation(F(t),2)\)\(\triangleright\) upsample \(F(t)\) with scale factor 2
11:\(F(t)[p,q,r]=1\), where \(p,q,r\gets Discretize(P_{in}\cup\{\mu(t)\})\);
12:endif
13:\(F(t)\gets MeanFiltering(F(t))\);
14:\(F(t)[p,q,r]=1\), where \(p,q,r\gets Discretize(P_{in}\cup\{\mu(t)\})\);
15:endfor
16:\(\tilde{P}(t)\gets GetPosition(th_{min}\leq F(t))\);
17:\(\tilde{S}(t)\gets GetPosition(th_{min}\leq F(t)\leq th_{max})\); ```

**Algorithm 1** Pseudo code for coarse-to-fine filling

**Gaussian-informed continuum**. In PAC-NeRF, the particles are equipped with appearance features to enable image rendering for the continuum at different states. We can also achieve this function by treating the particles as Gaussian kernels and re-train the particles using the visual data. However, this process is cumbersome and will also face the same issue in PAC-NeRF where distorted RGB images will be rendered when large deformation occurs. Therefore, instead of injecting appearance attributes, we opt to assign density and scale attributes to the particles where the densities originate from the density field, and the scale attributes can be directly obtained by the field grid size. The Gaussian-informed continuum is defined as a set of triplets:

\[\mathbb{P}_{\tilde{P}}=\{(\tilde{p},s_{\Delta x},\sigma_{F})\},\] (5)

where \(\tilde{p}\in\tilde{P}\), \(s_{\Delta x}=\Delta x/2^{n_{u}}\), and \(\sigma_{F}=F[Discretize(\tilde{p})]\) (we neglect \(t\) in the notation for simplicity). Therefore, we only render object masks as 2D shape surrogates for supervision.

Figure 3: Sketch illustration of the coarse-to-fine filling strategy. Gaussian and internal particles are depicted in green and blue, respectively. (a) Voxels containing particles are assigned high densities. (b) Following the upsampling and smoothing of the field, densities near boundaries become blurred (indicated in light yellow). (c) The particles are again used to correct the voxels that contain particles with high densities. (d) and (e) repeat the previous operations to achieve a more detailed shape.

### Geometry-aware Physical Property Estimation

With the Gaussian-informed continuum at initial state \(\mathbb{P}_{\hat{P}}(0)\) and the extracted surfaces \(\tilde{S}(t)\) in place, we can employ MPM to perform simulation on the continuum and evaluate the difference in terms of both the 3D and 2D shapes. Concretely, after a rollout by MPM given the current estimation of physical parameters, we obtain a trajectory \(P(t)\) with corresponding object surfaces \(S(t)\). We thus can render object masks over the trajectory. Then the loss of the current rollout can be computed as:

\[\mathcal{L}_{ppe}=\frac{1}{m}\sum_{i=1}^{m}[\mathcal{L}_{CD}(S(t_{i}),\tilde{S} (t_{i}))+\frac{1}{n}\sum_{j=1}^{n}\mathcal{L}_{1}(A_{j}(t_{i}),\bar{A}_{j}(t_{ i}))],\] (6)

where \(\mathcal{L}_{CD}\) and \(\mathcal{L}_{1}\) are chamfer distance and L1 norm respectively, \(S(t_{i})\) denotes the simulated surface at time \(t_{i}\), \(A_{j}(t_{i})\) is the rendered mask at view \(j\), and \(\bar{A}_{j}(t_{i})\) represents the object mask of the image extracted from video \(V_{j}\) at time \(t_{i}\). Due to the differential property of the simulator, the evaluated loss is used to optimize the target physical parameters \(\Theta\).

## 5 Experiments

**Datasets**. To thoroughly assess our proposed method, we employ two sources of data introduced by PAC-NeRF [12] and Spring-Gaus [6]. Concretely, PAC-NeRF contributes two synthetic datasets generated by MLS-MPM framework [18]. Each object in both datasets includes RGB images from 11 distinct viewpoints, with approximately 14 frames per viewpoint. The datasets feature a range of materials, including elastic and plastic objects, granular media, and both Newtonian and non-Newtonian fluids. The first dataset contains 45 cross-shape objects with different initial conditions and ground-truth values of physical properties, while the second one consists of 9 objects with different shapes. The interpretation of the physical parameters is listed in Appendix A.9 and A.10. Spring-Gaus generates a synthetic dataset of elastic objects and collects a real-world dataset containing both static and dynamic scenes. The synthetic data contains 30 frames in each of 10 viewpoints. While the real-world data only contains 3 viewpoints for each object in the dynamic scene, it captures 50-70 images from various viewpoints for the static scene. Moreover, we follow previous works [12; 6] and use the off-the-shelf matting [56] or segmentation [57] techniques to obtain object masks.

**Baselines**. For dynamic reconstruction, we compare with PAC-NeRF and the current state-of-the-art deformable 3D Gaussian method DefGS [16] on the PAC-NeRF synthetic dataset. More comparison of our dynamic 3D Gaussian pipeline on other widely-used datasets such as D-NeRF [25] is presented in Appendix A.1.3. For system identification, we employ PAC-NeRF as the baseline and evaluate the performance using the two datasets introduced in PAC-NeRF. To further demonstrate the precision of the proposed method in terms of geometry recovery and future prediction, we perform experiments on the Spring-Gaus synthetic dataset and compare the results with PAC-NeRF and Spring-Gaus.

**Metrics**. The evaluation metrics in the experiments include 1) Chamfer Distance (CD), with units expressed in \(10^{3}mm^{2}\); 2) Earth Mover's Distance (EMD); 3) Peak Signal-to-Noise Ratio (PSNR); 4) Structural Similarity Index Metric (SSIM) [58]; and 5) Mean Absolute Error (MAE), with values scaled by a factor of \(100\). The first two metrics are used to evaluate discrepancies between the reconstructed and ground-truth point clouds. PSNR and SSIM are leveraged on the Spring-Gaus dataset to validate the precision of future state prediction. We compute the mean absolute error for the evaluation of physical property estimation.

### Evaluation on PAC-NeRF Synthetic Dataset

**Comparison on dynamic reconstruction**. In this experiment, we first perform dynamic Gaussian reconstruction on the cross-shaped object dataset using DefGS and our proposed method, respectively. We then employ the same filling strategy on the reconstructed Gaussians at each time state to generate the continuum, which is regarded as the final recovered geometry of the object and used to make comparisons with the oracle shape to compute CD and EMD. Since PAC-NeRF jointly recovers both geometries and physical parameters, we use the final estimated results to generate the trajectory for evaluation.

The results, reported in Tab. 1, show that our method outperforms the baselines on both metrics and achieves more precise reconstruction performance on most objects. Specifically, we find thatthe NeRF representation used by PAC-NeRF usually leads to overly large shape generation. While DefGS performs well on elastic objects, its performance degenerates when modeling objects with large deformations, such as granular media and fluids. Our method can better handle these objects due to the flexibility of trajectory representation.

**Comparison on system identification**. We evaluate the performance of system identification of the two datasets proposed by PAC-NeRF. For the first dataset, we compute the MAE of the parameters for each type of object. To demonstrate the effectiveness of the 2D shape representation, we also conduct experiments on the second dataset by only using masks for supervision on our method, namely "Ours*". For the second dataset, we execute 10 times of our method with different random seeds for each object instance and report the mean value of the estimation results. The training details are illustrated in Appendix A.3.

The results, reported in Tab. 2 and Tab. 3, show that the proposed hybrid pipeline can achieve more accurate estimation over a wide range of entries and objects, which demonstrate the effectiveness of the geometry-aware guidance. Fig. 4 visualizes the RGB images rendered by PAC-NeRF and the masks rendered by our method. We can see that when large deformation occurs, the rendered RGB image becomes distorted, while the rendered mask can effectively reduce such effect and get better perfor

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multicolumn{1}{c}{\multirow{2}{*}{Metrics}} & \multicolumn{2}{c}{CD \(\downarrow\)} & \multicolumn{2}{c}{EMD \(\downarrow\)} \\ \hline \multicolumn{1}{c}{Methods} & PAC-NeRF [12] & DefGS [16] & Ours & PAC-NeRF [12] & DefGS [16] & Ours \\ \hline \multicolumn{1}{c}{Newtonian} & 0.277 & 0.269 & **0.243** & 0.027 & 0.027 & **0.025** \\ \multicolumn{1}{c}{Non-Newtonian} & 0.236 & 0.216 & **0.195** & 0.025 & 0.024 & **0.022** \\ \multicolumn{1}{c}{Elasticity} & 0.238 & 0.191 & **0.178** & 0.025 & 0.022 & **0.02** \\ \multicolumn{1}{c}{Plasticine} & 0.429 & 0.213 & **0.196** & 0.029 & 0.024 & **0.022** \\ \multicolumn{1}{c}{Sand} & **0.212** & 0.281 & 0.25 & 0.025 & 0.028 & **0.025** \\ \multicolumn{1}{c}{Mean} & 0.278 & 0.234 & **0.212** & 0.026 & 0.025 & **0.023** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Dynamic Reconstruction on PAC-NeRF Dataset

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multicolumn{1}{c}{\multirow{2}{*}{Type}} & Parameters & PAC-NeRF & Ours* & Ours \\ \hline \multirow{3}{*}{Newtonian} & \(\log_{10}(\mu)\) & 11.6\(\pm\)6.60 & 1.53\(\pm\)1.45 & **1.53\(\pm\)**1.31 \\  & \(\log_{10}(\kappa)\) & 16.7\(\pm\)5.37 & 16.0\(\pm\)22.4 & **14.8\(\pm\)**19.2 \\  & \(v\) & 0.86\(\pm\)1.45 & 0.20\(\pm\)0.08 & **0.20\(\pm\)**0.07 \\ \hline \multirow{3}{*}{Non-Newtonian} & \(\log_{10}(\mu)\) & 24.1\(\pm\)21.9 & 32.9\(\pm\)44.6 & **13.5\(\pm\)**18.2 \\  & \(\log_{10}(\kappa)\) & 44.0\(\pm\)26.3 & 17.7\(\pm\)20.2 & **12.9\(\pm\)**16.8 \\ \multicolumn{1}{c}{Non-Newtonian} & \(\log_{10}(\tau_{\nu})\) & 5.09\(\pm\)7.41 & **3.74\(\pm\)**3.72 & 4.80\(\pm\)3.92 \\  & \(\log_{10}(\eta)\) & **28.7\(\pm\)**23.3 & 34.9\(\pm\)24.1 & 40.7\(\pm\)24.6 \\  & \(v\) & 0.29\(\pm\)0.13 & 0.68\(\pm\)0.28 & **0.19\(\pm\)**0.09 \\ \hline \multirow{3}{*}{Elasticity} & \(\log_{10}(E)\) & 3.02\(\pm\)3.72 & 3.27\(\pm\)4.13 & **2.43\(\pm\)**3.29 \\  & \(\nu\) & 4.35\(\pm\)5.08 & 3.10\(\pm\)2.00 & **2.52\(\pm\)**2.03 \\ \multicolumn{1}{c}{} & \(v\) & **0.50\(\pm\)**0.23 & 0.78\(\pm\)0.26 & 0.82\(\pm\)0.32 \\ \hline \multirow{3}{*}{Plasticine} & \(\log_{10}(E)\) & 83.8\(\pm\)68.4 & 28.1\(\pm\)24.4 & **25.6\(\pm\)**29.4 \\  & \(\log_{10}(\tau_{\nu})\) & 11.2\(\pm\)14.5 & **1.24\(\pm\)**0.90 & 1.67\(\pm\)1.21 \\ \multicolumn{1}{c}{} & \(\nu\) & 18.9\(\pm\)15.7 & 10.2\(\pm\)5.34 & **9.59\(\pm\)**5.00 \\ \multicolumn{1}{c}{} & \(v\) & 0.56\(\pm\)0.17 & **0.13\(\pm\)**0.04 & 0.22\(\pm\)0.10 \\ \hline \multirow{3}{*}{Sand} & \(\theta_{fric}\) & 4.89\(\pm\)1.10 & 4.21\(\pm\)0.08 & **4.18\(\pm\)**0.52 \\  & \(v\) & 0.21\(\pm\)0.08 & 0.24\(\pm\)0.08 & **0.17\(\pm\)**0.05 \\ \hline \hline \end{tabular}
\end{table}
Table 2: System identification performance on PAC-NeRF cross-shaped object Dataset

Figure 4: Comparison between rendered and ground-truth images. (a) Rendered RGB images by PAC-NeRF. (b) Rendered masks by our method. (c)-(d) Ground-truth RGB images and masks. The mask-based supervision can introduce fewer discrepancies compared with the RGB-based guidance when the estimated shapes are correct.

[MISSING_PAGE_FAIL:9]

static scene, 2) transform the static Gaussian set to the initial state of the dynamic scene based on a registration network similar as iNeRF [6; 59], and 3) perform system identification from the dynamic observation by our method "Ours*" due to the lack of sufficient images for dynamic reconstruction. Subsequently, we establish robotic platforms in both simulated and real-world environments, each equipped with UR10 robot arms configured identically. We then execute grasp attempts on both the reconstructed objects with the estimated properties in the simulation and the corresponding real-world objects under the same configuration. The results of more objects, and more details about the training and the experiment setting are presented in Appendix A.5. From the results shown in Fig. 5, we see that our method demonstrates its capability to effectively model the deformation experienced by the objects upon impact with a surface. Furthermore, by applying identical gripper forces to both the simulated and real-world versions of the objects, we observe similar deformation behaviors. This consistency in deformation under identical conditions supports that the estimated physical parameters closely mirror the real-world properties of the objects.

## 6 Conclusion and Limitations

This paper proposes a novel solution that leverages the 3D Gaussian representation of objects to acquire explicit shapes while concurrently enabling the simulated continuum to render 2D shapes to facilitate the estimation of physical properties. A novel motion-factorized dynamic 3D Gaussian framework is proposed to reconstruct precise dynamic scenes. Object surfaces and Gaussian-informed continuum are obtained by utilizing the proposed coarse-to-fine density field generation strategy. Extensive experiments demonstrate the efficacy and applicability of our method.

Despite the performance we achieve, this method still suffers from limitations, such as the assumption of continuum mechanics, the requirements of multi-view images with known camera poses, and the need for prior knowledge of object constitutive models. Integrating the pose-free method [60] or generalized constitutive [61] model with our method will be an interesting direction for future work.

From the perspective of application, while this method can yield accurate estimations, it may pose risks for fragile objects, as the interaction required for property inference could potentially cause damage. Moreover, the computational demands of our framework are substantial which require at least 1.5 hours to simultaneously recover both the geometry and physical properties of each object. Future work could explore leveraging multi-model large language models [62] and large reconstruction models [63; 64; 65; 66] to facilitate the recovery process.

## 7 Acknowledgements

This research was supported by the Research Grant Council of the Hong Kong Special Administrative Region under grant number 16212623. We thank Licheng Zhong for providing us with details about real data collection and links for purchasing objects for real-world experiments.

Figure 5: Real-world application. Left: Identification and future state simulation. Right: Grasping simulation. The stress on the simulated object is indicated by blue (low) to red (high). The gripper widths from top to bottom are set to 6cm, 4.5cm, and 3.5cm, respectively.

## References

* [1] Hang Yin, Anastasia Varava, and Danica Kragic. Modeling, learning, perception, and control methods for deformable object manipulation. _Science Robotics_, 6(54):eabd8803, 2021.
* [2] Haochen Shi, Huazhe Xu, Samuel Clarke, Yunzhu Li, and Jiajun Wu. Robocook: Long-horizon elasto-plastic object manipulation with diverse tools. In _Proceedings of the Conference on Robot Learning (CoRL)_, pages 642-660. PMLR, 2023.
* [3] Haochen Shi, Huazhe Xu, Zhiao Huang, Yunzhu Li, and Jiajun Wu. Robocraft: Learning to see, simulate, and shape elasto-plastic objects in 3d with graph networks. _The International Journal of Robotics Research (IJRR)_, 43(4):533-549, 2024.
* [4] Bin Wang, Longhua Wu, KangKang Yin, Uri Ascher, Libin Liu, and Hui Huang. Deformation capture and modeling of soft objects. _ACM Transactions on Graphics (TOG)_, 34(4):1-12, 2015.
* [5] Hsiao-yu Chen, Edith Tretschk, Tuur Stuyck, Petr Kadlecek, Ladislav Kavan, Etienne Vouga, and Christoph Lassner. Virtual elastic objects. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 15827-15837, 2022.
* [6] Licheng Zhong, Hong-Xing Yu, Jiajun Wu, and Yunzhu Li. Reconstruction and simulation of elastic objects with spring-mass 3d gaussians. _arXiv preprint arXiv:2403.09434_, 2024.
* [7] Matthias Muller and Markus H Gross. Interactive virtual materials. In _Graphics interface_, volume 2004, pages 239-246, 2004.
* [8] Miguel Jaques, Michael Burke, and Timothy Hospedales. Physics-as-inverse-graphics: Unsupervised physical parameter estimation from video. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2020.
* [9] Pingchuan Ma, Tao Du, Joshua B Tenenbaum, Wojciech Matusik, and Chuang Gan. Risp: Rendering-invariant state predictor with differentiable simulation and rendering for cross-domain parameter estimation. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2021.
* [10] Pingchuan Ma, Peter Yichen Chen, Bolei Deng, Joshua B Tenenbaum, Tao Du, Chuang Gan, and Wojciech Matusik. Learning neural constitutive laws from motion observations for generalizable pde dynamics. In _International Conference on Machine Learning (ICML)_, pages 23279-23300. PMLR, 2023.
* [11] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhofer, Christoph Lassner, and Christian Theobalt. Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR)_, pages 12959-12970, 2021.
* [12] Xuan Li, Yi-Ling Qiao, Peter Yichen Chen, Krishna Murthy Jatavallabhula, Ming Lin, Chenfanfu Jiang, and Chuang Gan. Pac-nerf: Physics augmented continuum neural radiance fields for geometry-agnostic system identification. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2022.
* [13] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5459-5469, 2022.
* [14] Yutao Feng, Yintong Shang, Xuan Li, Tianjia Shao, Chenfanfu Jiang, and Yin Yang. Pienerf: Physics-based interactive elastodynamics with nerf. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2024.
* [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Transactions on Graphics (TOG)_, 42(4):1-14, 2023.
* [16] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR)_, 2024.

* [17] Chenfanfu Jiang, Craig Schroeder, Joseph Teran, Alexey Stomakhin, and Andrew Selle. The material point method for simulating continuum materials. In _Acm siggraph 2016 courses_, pages 1-52. ACM New York, NY, USA, 2016.
* [18] Yuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin Zhu, Andre Pradhana, and Chenfanfu Jiang. A moving least squares material point method with displacement discontinuity and two-way rigid body coupling. _ACM Transactions on Graphics (TOG)_, 37(4):1-14, 2018.
* [19] Li Zhang, Brian Curless, and Steven M Seitz. Spacetime stereo: Shape recovery for dynamic scenes. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR)_, volume 2, pages II-367. IEEE, 2003.
* [20] Richard A Newcombe, Dieter Fox, and Steven M Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR)_, pages 343-352, 2015.
* [21] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.
* [22] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _Advances in Neural Information Processing Systems (NeurIPS)_, 34:27171-27183, 2021.
* [23] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5521-5531, 2022.
* [24] Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast learning of neural implicit surfaces for multi-view reconstruction. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 3295-3306, 2023.
* [25] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10318-10327, 2021.
* [26] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. Hypernerf: a higher-dimensional representation for topologically varying neural radiance fields. _ACM Transactions on Graphics (TOG)_, 40(6):1-12, 2021.
* [27] Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 130-141, 2023.
* [28] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In _Proceedings of the International Conference on 3D Vision (3DV)_, 2024.
* [29] Agelos Kratimos, Jiahui Lei, and Kostas Daniilidis. Dynmf: Neural motion factorization for real-time dynamic view synthesis with 3d gaussian splatting. _arXiv preprint arXiv:2312.00112_, 2023.
* [30] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang. 4d gaussian splatting for real-time dynamic scene rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2024.
* [31] Junbang Liang, Ming Lin, and Vladlen Koltun. Differentiable cloth simulation for inverse problems. _Advances in Neural Information Processing Systems (NeurIPS)_, 32, 2019.

* Raissi et al. [2019] Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _Journal of Computational physics_, 378:686-707, 2019.
* Sundaresan et al. [2022] Priya Sundaresan, Rika Antonova, and Jeannette Bohgl. Diffcloud: Real-to-sim from point clouds with differentiable simulation and rendering of deformable objects. In _Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 10828-10835. IEEE, 2022.
* Li et al. [2022] Yifei Li, Tao Du, Kui Wu, Jie Xu, and Wojciech Matusik. Diffcloth: Differentiable cloth simulation with dry frictional contact. _ACM Transactions on Graphics (TOG)_, 42(1):1-20, 2022.
* Li et al. [2024] Jinxi Li, Ziyang Song, and Bo Yang. Nvfi: Neural velocity fields for 3d physics learning from dynamic videos. _Advances in Neural Information Processing Systems (NeurIPS)_, 36, 2024.
* Liang et al. [2024] Xiao Liang, Fei Liu, Yutong Zhang, Yuelei Li, Shan Lin, and Michael Yip. Real-to-sim deformable object manipulation: Optimizing physics models with residual mappings for robotic surgery. In _Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)_, pages 15471-15477. IEEE, 2024.
* Zheng et al. [2024] Dongzhe Zheng, Siqiong Yao, Wenqiang Xu, and Cewu Lu. Differentiable cloth parameter identification and state estimation in manipulation. _IEEE Robotics and Automation Letters (RA-L)_, 2024.
* Qiao et al. [2022] Yi-Ling Qiao, Alexander Gao, and Ming Lin. Neuphysics: Editable neural geometry and physics from monocular videos. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:12841-12854, 2022.
* Frank et al. [2010] Barbara Frank, Rudiger Schmedding, Cyrill Stachniss, Matthias Teschner, and Wolfram Burgard. Learning the elasticity parameters of deformable objects with a manipulation robot. In _Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 1877-1883. IEEE, 2010.
* Xu et al. [2019] Zhenjia Xu, Jiajun Wu, Andy Zeng, Joshua B Tenenbaum, and Shuran Song. Densephsnet: Learning dense physical object representations via multi-step dynamic interactions. In _Proceedings of the Robotics: Science and Systems_, 2019.
* Murthy et al. [2020] J Krishna Murthy, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Martin Weiss, Breandan Considine, Jerome Parent-Levesque, Kevin Xie, Kenny Erleben, et al. gradsim: Differentiable simulation for system identification and visuomotor control. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2020.
* Geilinger et al. [2020] Moritz Geilinger, David Hahn, Jonas Zehnder, Moritz Bacher, Bernhard Thomaszewski, and Stelian Coros. Add: Analytically differentiable dynamics for multi-body systems with frictional contact. _ACM Transactions on Graphics (TOG)_, 39(6):1-15, 2020.
* Heiden et al. [2021] Eric Heiden, Miles Macklin, Yashraj Narang, Dieter Fox, Animesh Garg, and Fabio Ramos. Discect: A differentiable simulation engine for autonomous robotic cutting. In _Proceedings of the Robotics: Science and Systems_, 2021.
* Du et al. [2021] Tao Du, Kui Wu, Pingchuan Ma, Sebastien Wah, Andrew Spielberg, Daniela Rus, and Wojciech Matusik. Diffpd: Differentiable projective dynamics. _ACM Transactions on Graphics (TOG)_, 41(2):1-21, 2021.
* Qiao et al. [2021] Yiling Qiao, Junbang Liang, Vladlen Koltun, and Ming Lin. Differentiable simulation of soft multi-body systems. _Advances in Neural Information Processing Systems (NeurIPS)_, 34:17123-17135, 2021.
* Jiang et al. [2015] Chenfanfu Jiang, Craig Schroeder, Andrew Selle, Joseph Teran, and Alexey Stomakhin. The affine particle-in-cell method. _ACM Transactions on Graphics (TOG)_, 34(4):1-10, 2015.

* [47] Yonghao Yue, Brenanan Smith, Christopher Batty, Changxi Zheng, and Eitan Grinspun. Continuum foam: A material point method for shear-dependent flows. _ACM Transactions on Graphics (TOG)_, 34(5):1-20, 2015.
* [48] Gergely Klar, Theodore Gast, Andre Pradhana, Chuyuan Fu, Craig Schroeder, Chenfanfu Jiang, and Joseph Teran. Drucker-prager elastoplasticity for sand animation. _ACM Transactions on Graphics (TOG)_, 35(4):1-12, 2016.
* [49] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Surface splatting. In _Proceedings of the 28th annual conference on Computer graphics and interactive techniques_, pages 371-378, 2001.
* [50] Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2024.
* [51] Tianyi Xie, Zeshun Zong, Yuxin Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physics-integrated 3d gaussians for generative dynamics. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2024.
* [52] Eduardo WV Chaves. _Notes on continuum mechanics_. Springer Science & Business Media, 2013.
* [53] Vladimir Yugay, Yue Li, Theo Gevers, and Martin R Oswald. Gaussian-slam: Photo-realistic dense slam with gaussian splatting. _arXiv preprint arXiv:2312.10070_, 2023.
* [54] Kai Katsumata, Duc Minh Vo, and Hideki Nakayama. An efficient 3d gaussian representation for monocular/multi-view dynamic scenes. _arXiv preprint arXiv:2311.12897_, 2023.
* [55] Hanlin Chen, Chen Li, and Gim Hee Lee. Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance. _arXiv preprint arXiv:2312.00846_, 2023.
* [56] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian L Curless, Steven M Seitz, and Ira Kemelmacher-Shlizerman. Real-time high-resolution background matting. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8762-8771, 2021.
* [57] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 4015-4026, 2023.
* [58] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. _IEEE transactions on image processing_, 13(4):600-612, 2004.
* [59] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting neural radiance fields for pose estimation. In _Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 1323-1330. IEEE, 2021.
* [60] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2024.
* [61] Haozhe Su, Xuan Li, Tao Xue, Chenfanfu Jiang, and Mridul Aanjaneya. A generalized constitutive model for versatile mpm simulation and inverse learning with differentiable physics. _Proceedings of the ACM on Computer Graphics and Interactive Techniques_, 6(3):1-20, 2023.
* [62] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.

* [63] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from a single image. _arXiv preprint arXiv:2403.02151_, 2024.
* [64] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In _European Conference on Computer Vision (ECCV)_, pages 1-18. Springer, 2024.
* [65] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convolutional reconstruction model. _arXiv preprint arXiv:2403.05034_, 2024.
* [66] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from a single image with sparse-view large reconstruction models. _arXiv preprint arXiv:2404.07191_, 2024.
* [67] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2015.
* [68] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 16632-16642, 2023.
* [69] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbaek Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12479-12488, 2023.
* [70] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Niessner, and Qi Tian. Fast dynamic radiance fields with time-aware neural voxels. In _SIGGRAPH Asia 2022 Conference Papers_, pages 1-9, 2022.
* [71] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot learning. _arXiv preprint arXiv:2108.10470_, 2021.
* [72] Isabella Huang, Yashraj Narang, Clemens Eppner, Balakumar Sundaralingam, Miles Macklin, Ruzena Bajcsy, Tucker Hermans, and Dieter Fox. Defgraspsim: Physics-based simulation of grasp outcomes for 3d deformable objects. _IEEE Robotics and Automation Letters_, 7(3):6274-6281, 2022.
* [73] William E. Lorensen and Harvey E. Cline. Marching cubes: A high resolution 3d surface construction algorithm. _ACM SIGGRAPH Computer Graphics_, page 163-169, 1998.
* [74] Yixin Hu, Teseo Schneider, Bolun Wang, Denis Zorin, and Daniele Panozzo. Fast tetrahedral meshing in the wild. _ACM Transactions on Graphics (TOG)_, 39(4):117-1, 2020.

Appendix

### Motion-factorized Dynamic 3D Gaussian Network

#### a.1.1 Implementation details

We employ temporal and positional encoding to the time \(t\) and position \(\mu_{0}\), respectively, to introduce features with various frequencies. Specifically, the encoding module is denoted as \(\gamma(x)=\left(\sin(2^{k}\pi x),\cos(2^{k}\pi x)\right)_{k=0}^{L-1}\), where \(L=10\) for both \(t\) and \(\mu_{0}\).

All the modules within the proposed network are composed of fully connected layers. The intermediate layers are uniformly designed, featuring both input and output channels configured to 256, and employ ReLU activation. For training, we adhere to the protocol established in [16], utilizing the Adam optimizer [67] with the same learning rate as specified in [16]. The total number of iterations is set at 40,000, with densification and pruning operations conducted every 500 steps until reaching 15,000 iterations. Additionally, the number of motions \(N_{m}\) is set to 8 for all objects in our network. \(\lambda_{1}\) and \(\lambda_{2}\) in Eqn. 4 are all set to \(1\). All the experiments are conducted on a single A10 GPU.

#### a.1.2 Effects of scale regularization

When addressing the deformation of objects such as fluids or granular media, the network may struggle to fit transformations accurately due to significant discrepancies between the canonical and target shapes. As a compensatory mechanism, the network may employ Gaussians with enlarged scales to mitigate shape distortions during image rendering. This effect is visualized in the top row of Fig. 6. To rectify this issue, we implement scale regularization during network training, which enforces Gaussian kernels to maintain smaller scales. The efficacy of this operation is demonstrated in the second row of Fig. 6, where it is evident that scale regularization enables the reconstruction of more precise shapes for rendering.

#### a.1.3 Evaluation on D-NeRF Dataset

To further evaluate the performance of our method in terms of novel view synthesis, we conduct the experiment on the D-NeRF [25] dataset, which is a widely used benchmark consisting of moving items with data captured by a monocular camera. We compute PSNR on the D-NeRF test set and compare our method with previous dynamic approaches, including Tensor4D [68], K-Planes [69], TiNeuVox [70], and DefGS [16]. The results, reported in Tab. 5, demonstrate the proposed dynamic 3D Gaussian pipeline can also achieve superior performance on rendering.

Figure 6: Visualization of trophy sequences. Row 1: rendering results from the network trained without scale regularization. Row 1: rendering results from the network trained with scale regularization.

### Gaussian-informed Continuum Generation

#### a.2.1 Implementation details

In Alg. 1, the number of iterations, denoted as \(n_{u}\), is uniformly set to 4 for all objects. We set the initial grid size \(\Delta x\) according to the volume of the object. For most objects, \(\Delta x=0.1\), while for small items such as toothpaste in PAC-NeRF dataset, \(\Delta x=0.01\). The parameters \(th_{min}\) and \(th_{max}\) are set to 0.5 and 0.8, respectively. The resulting particle count ranges from approximately 50,000 to 100,000.

#### a.2.2 Visualization of coarse-to-fine filling

Fig. 7 visualizes the filling results of our proposed coarse-to-fine strategy with different numbers of iterations, along with the results from PAC-NeRF and ground-truth shapes. The qualitative results show that our method can generate more accurate shapes compared with PAC-NeRF, which tends to recover over-large shapes. We should note that we cannot recover the cat-shaped object as in [12], though we use the code officially implemented by PAC-NeRF without any modification.

### Training details on PAC-NeRF Dataset

The training process is divided into two sub-processes, where we perform system identification after estimating the initial velocity of the object using the first three frames of data. Both processes use Adam [67] optimizer to tune the parameters.

### More Experiments on Spring-Gaus Synthetic Dataset

Besides performing evaluation on the simulated future states in Sec. 5.2, we also evaluate CD and EMD on states existing in the training data, and the results are reported in Tab. 6. It is obvious to see that our method outperforms the baselines by a large margin, which further demonstrates the performance of our method in terms of reconstruction and identification.

### Experiment Setting for Spring-Gaus Real-world Dataset

**Training details**. The dynamic scenes in Spring-Gaus [6] contain only three viewpoints, which are insufficient for dynamic 3D Gaussian reconstruction. Conversely, the static scenes incorporate 50 to 70 images captured from various viewpoints. Following the protocol established in Spring-Gaus, we reconstruct 3D Gaussian points from the static scenes using the traditional 3D Gaussian Splatting

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline  & & torus & cross & cream & apple & paste & chess & banana & Mean \\ \hline \multirow{3}{*}{\begin{tabular}{c} **C**D \\ \end{tabular} } & Spring-Gaus [6] & 0.17 & 0.48 & 0.36 & 0.38 & 0.19 & 1.80 & 2.60 & 0.85 \\  & PAC-NeRF [12] & 4.92 & 1.10 & 0.77 & 1.11 & 3.14 & 0.96 & 2.77 & 2.11 \\  & Ours & **0.13** & **0.13** & **0.14** & **0.15** & **0.17** & **0.41** & **0.03** & **0.17** \\ \hline \multirow{3}{*}{
\begin{tabular}{c} **C**D \\ \end{tabular} } & Spring-Gaus [6] & 0.040 & 0.037 & 0.031 & 0.033 & **0.022** & 0.063 & 0.052 & 0.040 \\  & PAC-NeRF [12] & 0.056 & 0.052 & 0.041 & 0.045 & 0.054 & 0.052 & 0.062 & 0.052 \\ \cline{1-1}  & Ours & **0.020** & **0.020** & **0.019** & **0.020** & 0.025 & **0.036** & **0.011** & **0.022** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Dynamic Reconstruction on Spring-Gaus Synthetic Dataset

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Hell} & \multirow{2}{*}{Mutant} & \multirow{2}{*}{Hook} & \multicolumn{2}{c}{Bouncing} & \multirow{2}{*}{T-Rex} & \multirow{2}{*}{Stand Up} & \multirow{2}{*}{
\begin{tabular}{c} Jumping \\ Jacks \\ \end{tabular} } & \multirow{2}{*}{Mean} \\  & \multicolumn{1}{c}{Warrior} & & & & & & \\ \hline Tensor4D [68] & 31.26 & 29.11 & 28.63 & 24.47 & 23.86 & 30.56 & 24.2 & 27.44 \\ K-Planes [69] & 24.58 & 32.5 & 28.12 & 40.05 & 30.43 & 33.1 & 31.11 & 31.41 \\ TiNeuVox [70] & 27.1 & 31.87 & 30.61 & 40.23 & 31.25 & 34.61 & 33.49 & 32.74 \\ DefGS [16] & 41.54 & 42.63 & 37.42 & 41.01 & **38.1** & 44.62 & 37.72 & 40.43 \\ Ours & **41.97** & **42.93** & **38.04** & **41.26** & 37.54 & **45.32** & **38.86** & **40.85** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results of PSNR (\(\uparrow\)) on D-NeRF [25] DatasetFigure 7: Visualization of Coarse-to-fine Filling. (a)-(d) are the filling results by our method with different times of upsampling operations. (e) visualize the point clouds recovered by PAC-NeRF. (f) shows the ground-truth shapes.

[MISSING_PAGE_FAIL:19]

### Notation of Algorithm 1

#### Necessity of 2D mask supervision

To evaluate the necessity of 2D mask supervision, we perform system identification on 45 cross-shaped object instances in the PAC-NeRF dataset by our method but with only object surface supervision. The results are reported in Tab. 9. It is obvious to see that combining both 2D and 3D shapes as supervision can achieve more accurate performance compared to using 3D shapes only. Therefore, we believe that utilizing 2D mask supervision to some extent makes up for the errors introduced by the 3D object surfaces extracted from dynamic 3D Gaussians.

#### Physical Properties

In this work, we simulate five types of materials, including elasticity, plasticine, granular media, Newtonian fluids, and non-Newtonian fluids. Each material exhibits distinct physical properties. We provide a brief introduction to the properties of each material.

_Elasticity:_ The Young's modulus (\(E\)) is a measure of the stiffness of a solid material, quantifying the relationship between stress and strain in a material under elastic deformation. The Poisson's ratio (\(\nu\)) describes the tendency of a material to expand or contract along its width when it is stretched or compressed along its length.

_Plasticine_: The yield stress (\(\tau_{Y}\)) is the minimum stress that a material requires to transition from elastic deformation to plastic deformation, marking the onset of permanent deformation. Both Young's modulus (\(E\)) and Poisson's ratio (\(\nu\)) exhibit characteristics similar to those of elastic materials.

_Granular Media_: The friction angle (\(\theta_{fric}\)) is a measure of the inherent resistance of a granular material to sliding or shearing, directly related to the angle at which a material can be piled without slumping.

\begin{table}
\begin{tabular}{c c} \hline \hline Operator or symbol & Explanation \\ \hline \(\mathbb{P}_{G}(t)\) & Gaussian particle set at time \(t\) \\ \(\tilde{P}(t)\) & Sampled continuum particles at time \(t\) \\ \(\tilde{S}(t)\) & Sampled surface particles at time \(t\), \(\tilde{S}(t)\subset\tilde{P}(t)\) \\ \(F(t)\) & 3D Density field at time \(t\) \\ \(Proj\) & Operation projecting 3D particles into 2D image indices according to the camera parameters \\ \(Discertize\) & Operation mapping particle positions to voxel indices on the density field \\ \(GetPosition\) & Operation returning 3D positions of the binary field \\ \hline \hline \end{tabular}
\end{table}
Table 8: Notation of Algorithm 1

\begin{table}
\begin{tabular}{c c c c} \hline \hline Type & Parameters & w/o masks & w/ masks \\ \hline \multirow{3}{*}{Newtonian} & \(\log_{10}(\mu)\) & 2.19\(\pm\)2.90 & **1.53\(\pm\)1.31** \\  & \(\log_{10}(\kappa)\) & 24.2\(\pm\)22.2 & **14.8\(\pm\)19.2** \\  & \(v\) & 0.20\(\pm\)0.08 & **0.20\(\pm\)0.07** \\ \hline \multirow{3}{*}{Newtonian} & \(\log_{10}(\mu)\) & 19.4\(\pm\)27.7 & **13.5\(\pm\)18.2** \\  & \(\log_{10}(\kappa)\) & 24.0\(\pm\)24.8 & **12.9\(\pm\)16.8** \\ \cline{1-1}  & \(\log_{10}(\tau_{Y})\) & **4.58\(\pm\)9.11** & 4.80\(\pm\)3.92 \\ \cline{1-1}  & \(\log_{10}(\eta)\) & 49.1\(\pm\)40.5 & **40.7\(\pm\)24.6** \\ \cline{1-1}  & \(v\) & 1.33\(\pm\)0.54 & **0.19\(\pm\)0.09** \\ \hline \multirow{3}{*}{Elasticity} & \(\log_{10}(E)\) & 2.85\(\pm\)1.94 & **2.43\(\pm\)3.29** \\  & \(\nu\) & 3.97\(\pm\)2.64 & **2.52\(\pm\)2.03** \\ \cline{1-1}  & \(v\) & **0.22\(\pm\)0.10** & 0.82\(\pm\)0.32 \\ \hline \multirow{3}{*}{Plasticine} & \(\log_{10}(E)\) & **25.6\(\pm\)27.4** & 25.6\(\pm\)29.4 \\ \cline{1-1}  & \(\log_{10}(\tau_{Y})\) & 9.04\(\pm\)2.37 & **1.67\(\pm\)1.21** \\ \cline{1-1}  & \(v\) & 1.16\(\pm\)0.00 & **0.22\(\pm\)0.10** \\ \hline \multirow{3}{*}{Sand} & \(\theta_{fric}\) & **2.55\(\pm\)2.03** & 4.18\(\pm\)0.52 \\ \cline{1-1}  & \(v\) & 0.31\(\pm\)0.18 & **0.17\(\pm\)0.05** \\ \hline \hline \end{tabular}
\end{table}
Table 9: System identification with/without mask supervision_Newtonian fluids_: The bulk modulus (\(\kappa\)) is a measure of a material's resistance to uniform compression, quantifying how much it compresses under a given amount of external pressure. Fluid viscosity (\(\mu\)) describes a fluid's resistance to flow, quantifying how much it resists deformation at a given rate.

_Non-Newtonian fluids_: The plasticity viscosity (\(\eta\)) refers to the measure of a viscoplastic material's resistance to deformation, which defines how it behaves under stress beyond its yield point. The bulk modulus (\(\kappa\)) and fluid viscosity (\(\mu\)) are comparable to those of Newtonian fluids, while the yield stress (\(\gamma_{Y}\)) is akin to that of plasticine.

### Constitutive Models

A constitutive model describes how a material responds to stress, strain, or other external forces. It defines the material's behavior by relating stress and strain through constitutive equations, which can capture complex behaviors such as elasticity, plasticity, and fracture. The MPM simulator is capable of modeling a diverse range of materials by employing various constitutive models. In this work, we have implemented simulations for five distinct types of materials: elasticity, plasticine, granular, Newtonian fluids, and non-Newtonian fluids.

**Elasticity**. We use the Neo-Hookean model, which is a common nonlinear hyperelastic model, to simulate the elasticity of materials and predict deformations. The Cauchy stress for this model is defined by

\[J\bm{\sigma}=\mu\left(\mathbf{FF^{\intercal}}\right)+\left[\lambda\log(J)-\mu \right]\mathbf{I},\] (7)

where the \(\mathbf{F}\) is the deformation gradient, \(J=\det(\mathbf{F})\) and \(\mu,\lambda\) are the Lame parameters, which are related to the material properties of Young's modulus (\(E\)) and Poisson's ratio (\(\nu\)) as:

\[\mu=\frac{E}{2(1+\nu)},\qquad\lambda=\frac{E\nu}{(1+\nu)(1-2\nu)}.\] (8)

**Plasticine**. We use the Saint Venant-Kirchhoff Model (StVK) together with von Mises yield criterion to simulate the plasticine. For this model, the stress is defined as:

\[J\bm{\sigma}=\mathbf{F}\left[2\mu\mathbf{G}+\lambda\mathrm{Tr}(\mathbf{G}) \mathbf{I}\right]\mathbf{F^{\intercal}},\] (9)

where \(\mathbf{G}=\frac{1}{2}\left(\mathbf{F^{\intercal}F-I}\right)\) is the Green strain. The von Mises yield criterion serves as a tool to assess whether the deformation exceeds the recoverable limit. The deformation gradient will be mapped back onto the boundary of elastic region using the following projection:

\[\mathcal{Z}(\mathbf{F})=\begin{cases}\mathbf{F}&\delta\gamma\leq 0\\ \mathbf{U}\exp(\bm{\epsilon}-\delta\gamma\frac{\hat{\epsilon}}{\|\bm{\epsilon }\|})\mathbf{V^{\intercal}}&\text{otherwise}\end{cases},\] (10)

where the \(\delta\gamma=\|\bm{\epsilon}\|-\frac{\gamma_{Y}}{2\mu}\), \(\bm{\epsilon}=\log(\Sigma)\) is the normalized Hencky strain. The \(\mathbf{U},\bm{\Sigma}\) and \(\mathbf{V}\) can be obtained by performing Singular Value Decomposition (SVD) on deformation gradient \(\mathbf{F}\).

**Granular Media**. Similar to plasticine, the StVK constitutive model is used to simulate granular media. Drucker-Prager yield criteria [48] is selected as the yielding condition. It is defined as follows:

\[\text{Tr}(\bm{\epsilon})>0,\quad\text{or}\quad\delta\gamma=\|\bm{\epsilon} \|_{F}+\alpha\frac{(d\lambda+2\mu)\text{Tr}(\bm{\epsilon})}{2\mu}>0,\] (11)

where \(d\) is the spatial dimension, \(\alpha=\sqrt{\frac{2}{3}}\frac{2\sin\theta_{fric}}{3-\sin\theta_{fric}}\) and \(\theta_{fric}\) is the friction angle. The deformation gradient return mapping is defined by

\[\mathcal{Z}(\mathbf{F})=\begin{cases}\mathbf{U}\mathbf{V^{\intercal}}&\text{ Tr}(\bm{\epsilon})>0\\ \mathbf{F}&\delta\gamma\leq 0,\text{Tr}(\bm{\epsilon})\leq 0\\ \mathbf{U}\exp\left(\bm{\epsilon}-\delta\gamma\ \frac{\hat{\epsilon}}{\|\bm{\epsilon}\|} \right)\mathbf{V^{\intercal}}&\text{otherwise}\end{cases}.\] (12)

**Newtonian Fluid**. We adopt the approach used in PAC-NeRF [12], which employs a J-based fluid model combined with a viscosity term to simulate Newtonian fluids. The stress for this model is defined by

\[J\bm{\sigma}=\frac{1}{2}\mu(\nabla\mathbf{v}+\nabla\mathbf{v^{\intercal}})+ \kappa(J-\frac{1}{J^{6}}),\] (13)where \(\mu\) and \(\kappa\) represent the fluid viscosity and the bulk modulus, respectively.

**Non-Newtonian Fluid**. We employ the viscoplastic model [47] to simulate non-Newtonian fluids. Although we continue to utilize the von Mises criteria to delineate the elastic region, the presence of viscoplasticity implies that deformation will not be immediately reverted onto the yield surface. It is defined as follows:

\[\mathcal{Z}(\mathbf{F})=\begin{cases}\mathbf{F}&\delta\gamma\leq 0\\ \mathbf{U}\exp(\frac{\hat{s}}{2\mu}\hat{\bm{\epsilon}}+\frac{1}{d}\text{Tr}( \epsilon)\mathbf{I})\mathbf{V^{\intercal}}&\text{otherwise}\end{cases},\] (14)

\[\hat{\mu} =\frac{\mu}{d}\text{Tr}(\bm{\Sigma}^{2}),\] (15) \[\bm{s} =2\mu\hat{\epsilon},\] \[\hat{s} =\|\bm{s}\|-\frac{\delta\gamma}{1+\frac{\eta}{2\mu\Delta t}}\]

where \(d\) is the spatial dimension. The \(\mathbf{U},\bm{\Sigma}\) and \(\mathbf{V}\) can be obtained by performing Singular Value Decomposition (SVD) on deformation gradient \(\mathbf{F}\).

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction sections already reflect the paper's contributions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: The limitations have been discussed in the conclusion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: This paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have tried to include all the details and referenced work for reproduction. We will also release the code of our method. Guidelines: ** The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: The code is not included for now. But we will release the code to the public soon. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: These details are described in the experiment section and appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The error bars are reported in the experiments section. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: This is described in the appendix section. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This work conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The used assets are properly cited in the experiments section. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.