[MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:2]

Provable score estimation.There is a rich literature giving Bayes-optimal algorithms for various natural denoising problems via methods inspired by statistical physics, like approximate message passing (AMP) (e.g. [13, 14, 15, 16, 17]) and natural gradient descent (NGD) on the TAP free energy [13, 15, 16]. The abovementioned works [1, 2] (see also [20]) build on these techniques to give algorithms for the denoising problems that arise in their implementation of stochastic localization. These works on denoising via AMP or NGD are themselves part of a broader literature on variational inference, a suitable literature review would be beyond the scope of this work, see e.g. [1, 13, 14, 15].

We are not aware of any provable algorithms for score estimation explicitly in the context of _distribution learning_. That said, it may be possible to extract a distribution learning result from [1]. While their algorithm was for sampling from the Sherrington-Kirkpatrick (SK) model given the Hamiltonian rather than training examples as input, if one is instead given training examples drawn from the SK measure, then at sufficiently high temperature one can approximately recover the Hamiltonian [1]. In this case, a suitable modification [1] should be able to yield an algorithm for approximately generating fresh samples from the SK model given training examples.

Learning mixtures of Gaussians.The literature on provable algorithms for learning Gaussian mixture models is vast, dating back to the pioneering work of Pearson [11], and we cannot do justice to it here. We mention only works whose quantitative guarantees are closest in spirit to ours and refer to the introduction of [10] for a comprehensive overview of recent works in this direction. For mixtures of identity-covariance Gaussians in high dimensions, the strongest existing guarantee is a polynomial-time algorithm [10] for learning the centers as long as their pairwise separation slightly exceeds \(\Omega(\sqrt{\log K})\) based on a sophisticated instantiation of method of moments inspired by the quasipolynomial-time algorithms of [12, 13, 14]. By the lower bound in [13], this is essentially optimal. In contrast, our Theorem 2 only applies given one initializes in a neighborhood of the true parameters of the mixture. We also note the exponential-time spectral algorithm of [15] and quasipolynomial-time tensor-based algorithm of [16], which achieve _density estimation_ even in the regime where the centers are arbitrarily closely spaced and learning the centers is information-theoretically impossible.

A separate line of work has investigated the "textbook" algorithm for learning Gaussian mixtures, namely the EM algorithm [1, 2, 13, 14, 15, 16, 17, 18]. Notably, for balanced mixtures of two Gaussians with the same covariance, [13] showed that finite-sample EM with random initialization converges exponentially quickly to the true centers. For mixtures of \(K\) Gaussians with identity covariance, [12, 13] showed that from an initialization sufficiently close to the true centers, finite-sample EM converges exponentially quickly to the true centers as long as their pairwise separation is \(\Omega(\sqrt{\log K})\). In particular, [13] establish this local convergence as long as every center estimate is initialized at distance at most \(\Delta/2\) away from the corresponding true center, where \(\Delta\) is the minimum separation between any pair of true centers; this radius of convergence is provably best possible for EM.

Lastly, we note that there are many works giving parameter recovery algorithms mixtures of Gaussians with general mixing weights and covariances, all of which are based on method of moments [13, 14, 15, 16, 17, 18, 19, 20]. Unfortunately, for general mixtures of \(K\) Gaussians, these algorithms run in time at least \(d^{O(K)}\), and there is strong evidence [12, 13] that this is unavoidable for computationally efficient algorithms.

### Technical overview

We begin by describing in greater detail the algorithm we analyze in this work. For the sake of intuition, in this overview we will focus on the case of mixtures of two Gaussians \((K=2)\) where the centers are well-separated and symmetric about the origin, that is, the data distribution is given by

\[q=\frac{1}{2}\mathcal{N}(\mu^{*},\mathrm{Id})+\frac{1}{2}\mathcal{N}(-\mu^{*},\mathrm{Id})\,.\] (1)

At the end of the overview, we briefly discuss the key challenges for handling smaller separation and general \(K\).

Loss function, architecture of the score function and student network.The algorithmic task at the heart of score estimation is that of denoising. Formally, for some noise level \(t>0\), we are given a noisy sample

\[X_{t}=\exp(-t)X_{0}+\sqrt{1-\exp(-2t)}Z_{t}\,,\]

where \(X_{0}\) is a clean sample drawn from the data distribution \(q\), and \(Z_{t}\sim\mathcal{N}(0,\mathrm{Id})\). Conditioning on \(X_{t}\) induces some posterior distribution over the noise \(Z_{t}\), and our goal is to form an estimate \(s\) for the mean of this posterior which achieves small error on average over the randomness of \(X_{0}\) and \(Z_{t}\). That is, we would like to minimize the _DDPM objective_, which up to rescaling is given by1

Footnote 1: The real DDPM objective is slightly different, see (5). The latter is what we actually consider in this paper, but this distinction is unimportant for the intuition in this overview.

\[L_{t}(s)=\mathbb{E}_{X_{0},Z_{t}}\|s(X_{t})-Z_{t}\|^{2}\,.\]

As discussed in the introduction, the algorithm of choice for minimizing this objective in practice is gradient descent on some student network. To motivate our choice of architecture, note that when the data distribution is given by (1), the true minimizer of \(L_{t}\) is, up to scaling,

\[\tanh(\langle\mu^{*}_{t},x\rangle)\mu^{*}_{t}-x\,,\quad\text{where }\mu^{*}_{t} \triangleq\mu^{*}\exp(-t)\,.\] (2)

See Appendix A for the derivation. Notably, Eq. (2) is exactly a two-layer neural network with \(\tanh\) activation. As a result, we use the same architecture for our student network when running gradient descent. That is, given weights \(\mu\in\mathbb{R}^{d}\), our student network is given by \(s_{\mu}(x)\triangleq\tanh(\mu^{\top}x)\mu-x\). The exact gradient updates on \(\mu\) are given in Lemma C.2.

As we discuss next, depending on whether the noise level \(t\) is large or small, this update closely approximates the update in one of two well-studied algorithms for learning mixtures of Gaussians: power method and EM respectively.

Learning mixtures of two Gaussians.We first provide a brief overview of the analysis and then go into the details of the analysis. We start with mixtures of two Gaussians of the form (1) where \(\|\mu^{*}\|\) is \(\Omega(1)\). In this case, we analyze the following two-stage algorithm. We first use gradient descent on the DDPM objective with large \(t\) starting from random initialization. We show that gradient descent in this "high noise" regime resembles a type of power iteration and gives \(\mu\) that has a nontrivial correlation with \(\mu^{*}_{t}\). Starting from this \(\mu\), we then run gradient descent with small \(t\). We show that the gradient descent in this "small noise" regime corresponds to the EM algorithm and converges exponentially quickly to the ground truth.

Large noise level: connection to power iteration.When \(t\) is large, we show that gradient descent on the DDPM objective is closely approximated by power iteration. More precisely, in this regime, the negative gradient of \(L_{t}(s_{\mu})\) is well-approximated by

\[-\nabla_{\mu}L_{t}(s_{\mu})\approx(2\mu^{*}_{t}\mu^{*\top}_{t}-r\mathrm{Id}) \,\mu\,,\]

where \(r\) is a scalar that depends on \(\mu\) (See Lemma 8). So the result of a single gradient update with step size \(\eta\) starting from \(\mu\) is given by

\[\mu^{\prime}\triangleq\mu-\eta\nabla_{\mu}L_{t}(s_{\mu})\approx((1-\eta r) \,\mathrm{Id}+2\eta\mu^{*}_{t}\mu^{*\top}_{t})\mu\,.\] (3)

This shows us that each gradient step can be approximated by one step of power iteration (without normalization) on the matrix \((1-\eta r)\,\mathrm{Id}+2\eta\mu^{*}_{t}\mu^{*\top}_{t}\). It is know that running enough iterations of the latter from a random initialization will converge in angular distance to the top eigenvector, which in this case is given by \(\mu^{*}_{t}\). This suggests that if we can keep the approximation error in (3) under control, then gradient descent on \(\mu\) will also allow us to converge to a neighborhood of the ground truth. We implement this strategy in Lemma 10. Next, we argue that once we are in a neighborhood of the ground truth, we can run GD on the DDPM objective at _low noise level_ to refine our estimate.

Low noise level: connection to the EM algorithm.When \(t\) is small, we show that gradient descent on the DDPM objective is closely approximated by EM. Here, our analysis uses the fact that \(\mu^{*}\) is sufficiently large and requires that we initialize \(\mu\) to have sufficiently large correlation with the true direction \(\mu^{*}_{t}\). We can achieve the latter using the large-\(t\) analysis in the previous section.

Provided we have this, when \(t\) is small it turns out that the negative gradient is well-approximated by

\[-\nabla_{\mu}L_{t}(s_{\mu})\approx\mathbb{E}_{X\sim\mathcal{N}(\mu^{*}_{t}, \mathrm{Id})}[\tanh(\langle\mu,X\rangle)X]-\mu\,.\]Note that the expectation is precisely the "M"-step in the EM algorithm for learning mixtures of two Gaussians (see e.g. Eq. (2.2) of [17]). We conclude that a single gradient update with step size \(\eta\) starting from \(\mu\) is given by mixing the old weights \(\mu\) with the result of the "M"-step in EM:

\[\mu^{\prime}\triangleq\mu-\eta\nabla_{\mu}L_{t}(s_{\mu})\approx(1-\eta)\mu+ \eta\underbrace{\mathbb{E}_{X\sim\mathcal{N}(\mu^{*}_{t},\mathrm{Id})}[\tanh( \langle\mu,X\rangle)X]}_{\text{``M'' step in the EM algorithm}}.\]

[13] and [17] showed that EM converges exponentially quickly to the ground truth \(\mu^{*}_{t}\) from a warm start, and we leverage ingredients from their analysis to prove the same guarantee for gradient descent on the DDPM objective at small noise level \(t\) (see Lemma 12).

Extending to small separation.Next, suppose we instead only assume that \(\|\mu^{*}\|\) is \(\Omega(1/\mathrm{poly}(d))\), i.e. the two components in the mixture may have small separation. The above analysis breaks down for the following reason: while it is always possible to show that gradient descent at large noise level converges in _angular distance_ to the ground truth, if \(\|\mu^{*}\|\) is small, then we cannot translate this to convergence in Euclidean distance.

We circumvent this as follows. Extending the connection between gradient descent at large \(t\) and power iteration, we show that a similar analysis where we instead run _projected_ gradient descent over the ball of radius \(\|\mu^{*}\|\) yields a solution arbitrarily close to the ground truth, even without the EM step.2 The projection step can be thought of as mimicking the normalization step in power iteration.

Footnote 2: Note that although \(\mu^{*}\) is unknown, we can estimate its norm from samples.

It might appear to the reader that this projected gradient-based approach is strictly superior to the two-stage algorithm described at the outset. However, in addition to obviating the need for a projection step when separation is large, our analysis for the two-stage algorithm has the advantage of giving much more favorable statistical rates. Indeed, we can show that the sample complexity of the two-stage algorithm has optimal dependence on the target error (\(1/\varepsilon^{2}\)), whereas we can only show a suboptimal dependence (\(1/\varepsilon^{8}\)) for the single-stage algorithm.

Extending to general \(K\).The connection between gradient descent on the DDPM objective at small \(t\) and the EM algorithm is sufficiently robust that for general \(K\), our analysis for \(K=2\) can generalize once we replace the ingredients from [13] and [17] with the analogous ingredients in existing analyses for EM with \(K\) Gaussians. For the latter, it is known that if the centers of the Gaussians have separation \(\Omega(\sqrt{\log\min(K,d)})\), then EM will converge from a warm start [14, 15]. By carefully tracking the error in approximating the negative gradient with the "M"-step in EM, we are able to show that gradient descent on the DDPM objective at small \(t\) achieves the same guarantee.

### Preliminaries

Diffusion models.Throughout the paper, we use either \(q\) or \(q_{0}\) to denote the data distribution and \(X\) or \(X_{0}\) to denote the corresponding random variable on \(\mathbb{R}^{d}\). The two main components in diffusion models are the _forward process_ and the _reverse process_. The forward process transforms samples from the data distribution into noise, for instance via the _Ornstein-Uhlenbeck (OU) process_:

\[\mathrm{d}X_{t}=-X_{t}\,\mathrm{d}t+\sqrt{2}\,\mathrm{d}W_{t}\quad\text{with} \quad X_{0}\sim q_{0}\,,\]

where \((W_{t})_{t\geq 0}\) is a standard Brownian motion in \(\mathbb{R}^{d}\). We use \(q_{t}\) to denote the law of the OU process at time \(t\). Note that for \(X_{t}\sim q_{t}\),

\[X_{t}=\exp(-t)X_{0}+\sqrt{1-\exp(-2t)}Z_{t}\quad\text{with}\quad X_{0}\sim q_ {0},\ \ Z_{t}\sim\mathcal{N}(0,\mathrm{Id})\,.\]

The reverse process then transforms noise into samples, thus performing generative modeling. Ideally, this could be achieved by running the following stochastic differential equation for some choice of terminal time \(T\):

\[\mathrm{d}X_{t}^{\leftarrow}=\{X_{t}^{\leftarrow}+2\nabla_{x}\ln q_{T-t}(X_{t }^{\leftarrow})\}\,\mathrm{d}t+\sqrt{2}\,\mathrm{d}W_{t}\quad\text{with}\quad X _{0}^{\leftarrow}\sim q_{T}\,,\]

where now \(W_{t}\) is the reversed Brownian motion. In this reverse process, the iterate \(X_{t}^{\leftarrow}\) is distributed acccording to \(q_{T-t}\) for every \(t\in[0,T]\), so that the final iterate \(X_{T}^{\leftarrow}\) is distributed according to the data distribution \(q_{0}\). The function \(\nabla_{x}\ln q_{t}\) is called the _score function_, and because it depends on \(q\) which is unknown, in practice one estimates it by minimizing the _score matching loss_

\[\min_{s_{t}}\ \ \mathbb{E}_{X_{t}\sim q_{t}}[\|s_{t}(X_{t})-\nabla_{x}\ln q_{t}( X_{t})\|^{2}]\,.\] (4)

A standard calculation (see e.g. Appendix A of [1]) shows that this is equivalent to minimizing the _DDPM objective_ in which one wants to predict the noise \(Z_{t}\) from the noisy observation \(X_{t}\), i.e.

\[\min_{s_{t}}\ \ L_{t}(s_{t})=\mathbb{E}_{X_{0},Z_{t}}\Big{[}\Big{\|}s_{t}(X_ {t})+\frac{Z_{t}}{\sqrt{1-\exp(-2t)}}\Big{\|}^{2}\Big{]}\,.\] (5)

While we have provided background on diffusion models for context, in this work we focus specifically on the optimization problem (5).

Mixtures of Gaussians.We consider the case of learning mixtures of \(K\) equally weighted Gaussians:

\[q=q_{0}=\frac{1}{K}\sum_{i=1}^{K}\mathcal{N}(\mu_{i}^{*},\mathrm{Id}),\] (6)

where \(\mu_{i}^{*}\) denotes the mean of the \(i^{\text{th}}\) Gaussian component. We define \(\theta^{*}=\{\mu_{1}^{*},\mu_{2}^{*}\ldots,\mu_{K}^{*}\}\). For the mixtures of two Gaussians, we can simplify the data distribution as

\[q=q_{0}=\frac{1}{2}\mathcal{N}(\mu^{*},\mathrm{Id})+\frac{1}{2}\mathcal{N}(- \mu^{*},\mathrm{Id}).\] (7)

Note that distribution in Eq. (7) is equivalent to the distribution Eq. (6) with \(K=2\) because shifting the latter by its mean will give the former distribution, and furthermore the necessary shift can be estimated from samples. The following is immediate:

**Lemma 3**.: _If \(q_{0}\) is a mixture of \(K\) Gaussians as in Eq. (6), then for any \(t>0\), \(q_{t}\) is the mixture of \(K\) Gaussians given by_

\[q_{t}=\frac{1}{K}\sum_{i=1}^{K}\mathcal{N}(\mu_{i,t}^{*},\mathrm{Id})\ \ \text{ where }\ \mu_{i,t}^{*}\triangleq\mu_{i}^{*}\exp(-t)\,.\] (8)

See Appendix A for a proof of this fact. We can see that the means of \(q_{t}\) get rescaled according to the noise level \(t\). We also define \(\theta_{t}^{*}=\{\mu_{1,t}^{*},\mu_{2,t}^{*},\ldots,\mu_{K,t}^{*}\}\).

**Lemma 4**.: _The score function for distribution \(q_{t}\), for any \(t>0\), is given by_

\[\nabla_{x}\ln q_{t}(x)=\sum_{i=1}^{K}w_{i,t}^{*}(x)\mu_{i,t}^{*}-x\,,\ \ \ \ \ \text{ where }\ \ \ \ w_{i,t}^{*}(x)=\frac{\exp(-\|x-\mu_{i,t}^{*}\|^{2}/2)}{\sum_{j=1}^{K}\exp(-\| x-\mu_{j,t}^{*}\|^{2}/2)}.\]

_For a mixture of two Gaussians, the score function simplifies to_

\[\nabla_{x}\log q_{t}(x)=\tanh(\mu_{t}^{*\top}x)\mu_{t}^{*}-x\,,\ \ \ \ \ \text{ where }\ \ \ \ \mu_{t}^{*}\triangleq\mu^{*}\exp(-t)\]

See Appendix A for the calculation.

Recall that \(\nabla_{x}\log q_{t}(x)\) is the minimizer for the score-matching objective given in Eq. (4). Therefore, we parametrize our student network architecture similarly to the optimal score function. Our student architecture for mixtures of \(K\) Gaussians is

\[s_{\theta_{t}}(x)=\sum_{i=1}^{K}w_{i,t}(x)\mu_{i,t}-x\,,\ \ \ \ \ \text{ where }\ \ \ \ w_{i,t}(x) \triangleq\frac{\exp(-\|x-\mu_{i,t}\|^{2}/2)}{\sum_{j=1}^{K}\exp(-\|x- \mu_{j,t}\|^{2}/2)}\] (9) \[\mu_{i,t} \triangleq\mu_{i}\exp(-t).\]

where \(\theta_{t}=\{\mu_{1,t},\mu_{2,t},\ldots,\mu_{K,t}\}\) denotes the set of parameters at the noise scale \(t\). For mixtures of two Gaussians, we simplify the student architecture as follows:

\[s_{\theta_{t}}(x)=\tanh(\mu_{t}^{\top}x)\mu_{t}-x\,,\ \ \ \text{ where }\ \ \mu_{t} \triangleq\mu\exp(-t).\]

As \(\theta_{t}\) only depends on \(\mu_{t}\) in the case of mixtures of two Gaussians, we simplify the notation of the score function from \(s_{\theta_{t}}(x)\) to \(s_{\mu_{t}}(x)\) in that case. We use \(\hat{\mu}_{t}\) and \(\hat{\mu}_{t}^{*}\) to denote the unit vector along the direction of \(\mu_{t}\) and \(\mu_{t}^{*}\) respectively. Note that we often use \(\mu_{t}\) (or \(\theta_{t}\)) to denote the current iterate of gradient descent on the DDPM objective and \(\mu_{t}^{\prime}\) to denote the iterate after taking a gradient descent step from \(\mu_{t}\).

Expectation-Maximization (EM) algorithm.The EM algorithm is composed of two steps: the E-step and the M-step. For mixtures of Gaussians, the E-step computes the expected log-likelihood based on the current mean parameters and the M-step maximizes this expectation to find a new estimate of the parameters.

**Fact 5** (See e.g., [17, 14, 15] for more details).: _When \(X\) is the mixture of \(K\) Gaussian and \(\{\mu_{1},\mu_{2},\ldots,\mu_{K}\}\) are current estimates of the means, the population EM update for all \(i\in\{1,2,\ldots,K\}\) is given by_

\[\mu_{i}^{\prime}=\frac{\mathbb{E}_{X}[w_{i}(X)X]}{\mathbb{E}_{X}[w_{i}(X)]}, \quad\text{where}\ \ w_{i}(X)=\frac{\exp(-\|X-\mu_{i}\|^{2}/2)}{\sum_{j=1}^{K}\exp(-\|X-\mu_{j}\| ^{2}/2)}.\]

_The EM update for mixtures of two Gaussians given in Eq. (7) simplifies to_

\[\mu^{\prime}=\mathbb{E}_{X\sim\mathcal{N}(\mu^{*},\mathrm{Id})}[\tanh(\mu^{ \top}X)X].\]

An analogous version of the EM algorithm, called the gradient EM algorithm, takes a gradient step in the direction of the M-step instead of optimizing the objective in the M-step fully.

**Fact 6** (See e.g., [14, 15] for more details).: _For all \(i\in\{1,2,\ldots,K\}\), the gradient EM-update for mixtures of \(K\) Gaussian is given by_

\[\mu_{i}^{\prime}=\mu_{i}+\eta\,\mathbb{E}_{X}[w_{i}(X)(X-\mu_{i})],\]

_where \(\eta\) is the learning rate._

## 2 Warmup: mixtures of two Gaussians with constant separation

In this section, we formally state our result for learning mixtures of two Gaussians with constant separation. This case highlights the main proof techniques, namely viewing gradient descent on the DDPM objective as power iteration and as the EM algorithm.

### Result and algorithm

**Theorem 7**.: _There is an absolute constant \(c>0\) such that the following holds. Suppose a mixture of two Gaussians with the mean parameter \(\mu^{*}\) satisfies \(\|\mu^{*}\|>c\). Then, for any \(\varepsilon>0\), there is a procedure that calls Algorithm 1 at two different noise scales \(t\) and outputs \(\tilde{\mu}\) such that \(\|\tilde{\mu}-\mu^{*}\|\leq\varepsilon\) with high probability. Moreover, the algorithm has time and sample complexity \(\mathrm{poly}(d)/\varepsilon^{2}\) (see Theorem C.1 for more precise quantitative bounds)._

Algorithm.The algorithm has two stages. In the first stage we run gradient descent on the DDPM objective described in Algorithm 1 from a random Gaussian initialization and noise scale \(t_{1}\) for a fixed number of iterations \(H\) where \(t_{1}=\Theta(\log d)\) ("high noise") and \(H=\mathrm{poly}(d,1/\varepsilon)\). In the second stage, the procedure uses the output of the first step as initialization and runs Algorithm 1 at a "low noise" scale of \(t_{2}=\Theta(1)\).

### Proof outline of Theorem 7

We provide a proof sketch of correctness of the above algorithm and summarize the main technical lemmas here. All proofs of the following lemmas can be found in Appendix C.

Part I: Analysis of high noise regime and connection to power iteration.We show that in the large noise regime, the negative gradient \(-\nabla L_{t}(s_{t})\) is well-approximated by \(2\mu_{t}^{*}{\mu_{t}}^{\top}\mu_{t}-3\|\mu_{t}\|^{2}\,\mu_{t}\). Recall that this result is the key to showing the resemblance between gradient descent and power iteration. Concretely, we show the following lemma:

**Lemma 8** (See Lemma C.3 for more details).: _For \(t=\Theta(\log d)\), the gradient descent update on the DDPM objective \(L_{t}(s_{t})\) can be approximated with \(2\mu_{t}^{*}{\mu_{t}}^{\top}\mu_{t}-3\|\mu_{t}\|^{2}\,\mu_{t}\):_

\[\left\|\left(-\nabla L_{t}(s_{t})\right)-\left(2\mu_{t}^{*}{\mu_{t}^{*}}^{ \top}\mu_{t}-3\|\mu_{t}\|^{2}\,\mu_{t}\right)\right\|\leq\mathrm{poly}(1/d).\]From Lemma 8, it immediately follows that \(\mu^{\prime}t\), the result of taking a single gradient step starting from \(\mu_{t}\), is well-approximated by the result of taking a single step of power iteration for a matrix whose leading eigenvector is \(\mu_{t}^{*}\):

\[\mu_{t}^{\prime}=\mu_{t}-\eta\nabla L_{t}(s_{\mu})\approx(\operatorname{Id}(1- 3\eta\|\mu_{t}\|^{2})+2\mu_{t}^{*}\mu_{t}^{*\top})\mu_{t}\,.\]

The second key element is to show that as a consequence of the above power iteration update, the gradient descent converges in _angular distance_ to the leading eigenvector. Concretely, we show the following lemma:

**Lemma 9** (Informal, see Lemma C.5 for more details).: _Suppose \(\mu_{t}^{\prime}\) is the iterate after one step of gradient descent on the DDPM objective from \(\mu_{t}\). Denote the angle between \(\mu_{t}\) and \(\mu_{t}^{*}\) to be \(\theta\) and between \(\mu_{t}^{\prime}\) and \(\mu_{t}^{*}\) to be \(\theta^{\prime}\). In this case, we show that_

\[\tan\theta^{\prime}=\max\left(\kappa_{1}\tan\theta,\kappa_{2}\right),\]

_where \(\kappa_{1}<1\) and \(\kappa_{2}\leq 1/\mathrm{poly}(d)\)._

Note \(\tan\theta^{\prime}<\tan\theta\) implies that \(\theta^{\prime}<\theta\) or equivalently \(\langle\hat{\mu}_{t}^{\prime},\hat{\mu}_{t}^{*}\rangle>\langle\hat{\mu}_{t}, \hat{\mu}_{t}^{*}\rangle\). Thus, the above lemma shows that by taking a gradient step in the DDPM objective, the angle between \(\mu_{t}\) and \(\mu_{t}^{*}\) decreases. By iterating this, we obtain the following lemma:

**Lemma 10** (Informal, see Lemma C.6 for more details).: _Running gradient descent from a random initialization on the DDPM objective \(L_{t}(s_{\mu})\) for \(t=O(\log d)\) gives \(\mu_{t}\) for which \(\langle\hat{\mu}_{t},\hat{\mu}_{t}^{*}\rangle\) is \(\Omega(1)\)._

Note that we cannot keep running gradient descent at this high noise scale and hope to achieve \(\mu\) such that\(\|\mu-\mu^{*}\|\) is \(O(\varepsilon)\). This is because Lemma 9 can only guarantee that the angle between \(\mu_{t}\) and \(\mu_{t}^{*}\) is \(O(\varepsilon)\), but this does not imply \(\|\mu-\mu^{*}\|\) is \(O(\varepsilon)\). Instead, as described in Part II, we will proceed with a smaller noise scale.

Part II: Analysis of low noise regime and connection to EM.In the low noise regime, we run Algorithm 1 using the output from Part I as our initialization. Our analysis here shows that whenever the initialization \(\mu_{t}\) satisfies the condition of \(\langle\hat{\mu}_{t},\hat{\mu}_{t}^{*}\rangle\) being \(\Omega(1),\|\mu_{t}-\mu_{t}^{*}\|\) contracts after every gradient step. To start with, we show that the result of a _population_ gradient step on the DDPM objective \(L_{t}(s_{\mu})\) results in the following:

\[\mu_{t}^{\prime}=(1-\eta)\mu_{t}+\eta\operatorname{\mathbb{E}}_{x\sim \mathcal{N}(\mu_{t}^{*},\operatorname{Id})}[\tanh(\mu_{t}^{\top}x)x]+\eta G( \mu_{t},\mu_{t}^{*}),\]

where \(\mu_{t}^{\prime}\) is the parameter after a gradient step, \(\eta\) is the learning rate, and function \(G\) is given by

\[G(\mu,\mu^{*})=\operatorname{\mathbb{E}}_{x\sim\mathcal{N}(\mu^{*}, \operatorname{Id})}[-\frac{1}{2}\tanh^{\prime\prime}(\mu^{\top}x)\|\mu\|^{2}x+ \tanh^{\prime}(\mu^{\top}x)\mu^{\top}xx-\tanh^{\prime}(\mu^{\top}x)\mu].\]

Note we use the population gradient here only for simplicity; in the Appendix we show that empirical estimates of the gradient suffice. After some calculation, we can show that

\[\big{\|}\mu_{t}^{\prime}-\mu_{t}^{*}\big{\|}\leq(1-\eta)\|\mu_{t}-\mu_{t}^{*} \|+\eta\|\operatorname{\mathbb{E}}_{x\sim\mathcal{N}(\mu_{t}^{*},\operatorname {Id})}[\tanh(\mu_{t}^{\top}x)x]-\mu_{t}^{*}\|+\eta\big{\|}G(\mu_{t},\mu_{t}^{* })\big{\|}\,.\] (10)

Using Fact 5, we know that \(\operatorname{\mathbb{E}}_{x\sim\mathcal{N}(\mu_{t}^{*},\operatorname{Id})}[ \tanh(\mu_{t}^{\top}x)x]\) is precisely the result of one step of EM starting from \(\mu_{t}\), and it is known [17] that the EM update contracts the distance between \(\mu_{t}\) and \(\mu_{t}^{*}\) as follows:

\[\|\operatorname{\mathbb{E}}_{x\sim\mathcal{N}(\mu_{t}^{*}, \operatorname{Id})}[\tanh(\mu_{t}^{\top}x)x]-\mu_{t}^{*}\|\leq\lambda_{1}\| \mu_{t}-\mu_{t}^{*}\|\quad\text{for some }\lambda_{1}<1\] (11)

It remains to control the second term in Eq. (10), for which we prove the following:

**Lemma 11** (Informal, see Lemma C.9 for more details).: _When \(\|\mu^{*}\|=\Omega(1)\) and the noise scale \(t=\Theta(1)\), then for every \(\mu\) with \(\langle\hat{\mu},\hat{\mu}^{*}\rangle\) being \(\Omega(1)\), the following inequality holds:_

\[\|G(\mu_{t},\mu_{t}*)\|\leq\lambda_{2}\|\mu_{t}-\mu_{t}^{*}\|\quad\text{for some }\lambda_{2}<1\,.\]

Combining Eq. (11) and Lemma 11 with Eq. (10), we have

\[\big{\|}\mu_{t}^{\prime}-\mu_{t}^{*}\big{\|}\leq(1-\eta(1-\lambda_{1}-\lambda_{ 2}))\|\mu_{t}-\mu_{t}^{*}\|\,.\] (12)

We can set parameters to ensure that \(\lambda_{1}+\lambda_{2}<1\) and therefore that \(\|\mu_{t}-\mu_{t}^{*}\|\) contracts with each gradient step. Applying Lemma 11 and Eq. (12), we obtain the following lemma summarizing the behavior of gradient descent on the DDPM objective in the low noise regime.

**Lemma 12** (Informal).: _For any \(\varepsilon>0\) and for the noise scale \(t=\Theta(1)\), starting from an initialization \(\mu_{t}\) for which \(\langle\hat{\mu}_{t},\hat{\mu}_{t}^{*}\rangle=\Omega(1)\), running gradient descent on the DDPM objective \(L_{t}(s_{\mu})\) will give us mean parameter \(\tilde{\mu}\) such that \(\|\tilde{\mu}-\mu^{*}\|\leq O(\varepsilon)\)._

Combining Lemma 10 and Lemma 12, we obtain our first main result, Theorem 7, for learning mixtures of two Gaussians with constant separation. For the full technical details, see Appendix C.

## 3 Extensions: small separation and more components

### Mixtures of two Gaussians with small separation

In this section, we briefly sketch how the ideas from Section 2 can be extended to give our second main result, namely on learning mixtures of two Gaussians even with _small separation_. We defer the full technical details to Appendix D.

**Theorem 13**.: _Suppose a mixture of two Gaussians has mean parameter \(\mu^{*}\) that satisfies \(\|\mu^{*}\|=\Omega(\frac{1}{\operatorname{poly}(d)})\). Then, for any \(\varepsilon>0\), there exists a modification of Algorithm 1 that provides an estimate \(\mu\) such that \(\|\mu-\mu^{*}\|\leq O(\varepsilon)\) with high probability. Moreover, the algorithm has time and sample complexity \(\operatorname{poly}(d)/\varepsilon^{8}\) (see Theorem D.1 for more precise quantitative bounds)._

Algorithm modification.The algorithm that we analyze runs _projected_ gradient descent on the DDPM objective but only in the high noise scale regime where \(t=O(\log d)\). At each step, we project the iterate \(\mu\) to the ball of radius \(R\), where \(R\) is an empirical estimate for \(\|\mu^{*}\|\) obtained by drawing samples \(x_{1},\ldots,x_{n}\) from the data distribution and forming \(R\triangleq(\frac{1}{n}\sum_{i=1}^{n}\|x_{i}\|^{2}-d)^{1/2}\).

Proof sketch.Lemma 9 and Lemma 10 apply even when the components of the mixture have small separation, and they show that running gradient descent on the DDPM objective results in \(\mu_{t}\) and \(\mu_{t}^{*}\) being \(O(1)\) close in angular distance. Although our analysis can be extended to show that gradient descent can achieve \(O(\varepsilon)\) angular distance, this does not guarantee that \(\|\mu_{t}-\mu_{t}^{*}\|\) is \(O(\varepsilon)\). If in addition to being \(O(\varepsilon)\) close in angular distance, we also have that \(\|\mu_{t}\|\approx\|\mu_{t}^{*}\|\), then it is easy to see that \(\|\mu_{t}-\mu_{t}^{*}\|\) is indeed \(O(\varepsilon)\).

Observe that if \(R\) is approximately equal to \(\|\mu_{t}^{*}\|\), then the projection step in our algorithm ensures that our final estimate \(\mu_{t}\) satisfies this additional condition of \(\|\mu_{t}\|\approx\|\mu_{t}^{*}\|\). It is not hard to show that \(R^{2}\) is an unbiased estimate of \(\|\mu_{t}^{*}\|^{2}\), so standard concentration shows that taking \(n=\operatorname{poly}(d,\frac{1}{\varepsilon})\) suffices to ensure that \(R\) is sufficiently close to \(\|\mu_{t}^{*}\|\).

### Mixtures of \(K\) Gaussians, from a warm start

In this section, we state our third main result, namely for learning mixtures of \(K\) Gaussians given by Eq. (6) from a warm start, and provide an overview of how the ideas from Section 2 can be extended to obtain this result.

**Assumption 14**.: _(Separation) For a mixture of \(K\) Gaussians given by Eq. (6), for every pair of components \(i,j\in\{1,2,\ldots,K\}\) with \(i\neq j\), we assume that the separation between their means \(\|\mu_{i}^{*}-\mu_{j}^{*}\|\geq C\sqrt{\log(\min(K,d))}\) for sufficiently large absolute constant \(C>0\)._

**Assumption 15**.: _(Initialization) For each component \(i\in\{1,2,\ldots,K\}\), we have an initialization \(\mu_{i}^{(0)}\) with the property that \(\|\mu_{i}^{(0)}-\mu_{i}^{*}\|\leq C^{\prime}\sqrt{\log(\min(K,d))}\) for sufficiently small absolute constant \(C^{\prime}>0\)._

**Theorem 16**.: _Suppose a mixture of \(K\) Gaussians satisfies Assumption 14. Then, for any \(\varepsilon=\Theta(1/\operatorname{poly}(d))\), running gradient descent on the DDPM objective (Algorithm 1) at low noise scale \(t=O(1)\) and with initialization satisfying Assumption 15 results in mean parameters \(\{\mu_{i}\}_{i=1}^{K}\) such that with high probability, the mean parameters satisfy \(\|\mu_{i}-\mu_{i}^{*}\|\leq O(\varepsilon)\) for each \(i\in\{1,2,\ldots,K\}\). Additionally, the runtime and sample complexity of the algorithm is \(\operatorname{poly}(d,1/\varepsilon)\) (see Theorem E.1 for more precise quantitative bounds)._

We provide a brief overview of the proof here. The full proof can be found in Appendix E.

Proof sketch.For learning mixtures of two Gaussians, we have already established the connection between gradient descent on the DDPM objective and the EM algorithm. For mixtures of \(K\) Gaussians, however, in a local neighborhood around the ground truth parameters \(\theta^{*}\), we show an equivalence between _gradient_ EM (recall gradient EM performs one-step of gradient descent on the "M" step objective) and gradient descent on the DDPM objective. In particular, our main technical lemma (Lemma E.4) shows that for noise scale \(t=\Theta(1)\) and for any \(\mu_{i}\) that satisfies \(\|\mu_{i}-\mu_{i}^{*}\|\leq O(\sqrt{\log(\min(K,d))})\), we have

\[-\nabla_{\mu_{i,t}}L_{t}(s_{\theta_{t}})\approx\mathbb{E}_{X_{t}}[w_{i,t}(X_{ t})(X_{t}-\mu_{i,t})].\]

Therefore, the iterate \(\mu_{i,t}^{\prime}\) resulting from a single gradient step on the DDPM objective \(L_{t}(s_{\theta_{t}})\) with learning rate \(\eta\) is given by

\[\mu_{1,t}^{\prime}=\mu_{1,t}-\eta\nabla_{\mu_{1,t}}L_{t}(s_{\theta_{t}}) \approx\mu_{1,t}+\eta\,\mathbb{E}_{X_{t}}[w_{1,t}(X_{t})(X_{t}-\mu_{1,t})].\] (13)

Comparing Fact 6 with Eq. (13), we see the correspondence in this regime between gradient descent on the DDPM objective to gradient EM. Using this connection and an existing local convergence guarantee from the gradient EM literature [21, 17], we obtain our main theorem for mixtures of \(K\) Gaussians. Full details can be found in Appendix E.

## Acknowledgments

SC would like to thank Sinho Chewi, Khashayar Gatmiry, Frederic Koehler, and Holden Lee for enlightening discussions on sampling and score estimation. KS and AK are supported by the NSF AI Institute for Foundations of Machine Learning (IFML). SC is supported by NSF Award 2103300.

## References

* [AG22] Ahmed El Alaoui and Jason Gaitonde. Bounds on the covariance matrix of the sherrington-kirkpatrick model. _arXiv preprint arXiv:2212.02445_, 2022.
* [BDD23] Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. _arXiv preprint arXiv:2305.16860_, 2023.
* [BDJ\({}^{+}\)22] Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and Santosh S Vempala. Robustly learning mixtures of k arbitrary gaussians. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1234-1247, 2022.
* [BKM17] David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. _Journal of the American statistical Association_, 112(518):859-877, 2017.
* [BM11] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. _IEEE Transactions on Information Theory_, 57(2):764-785, 2011.
* [BMR22] Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and Langevin sampling. _arXiv preprint 2002.00107_, 2022.
* [BRST21] Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous lwe. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 694-707, 2021.
* [BS15] Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. _SIAM Journal on Computing_, 44(4):889-911, 2015.
* [BWY17] Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the em algorithm: From population to sample-based analysis. 2017.
* [CCL\({}^{+}\)23a] Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ode is provably fast. _arXiv preprint arXiv:2305.11798_, 2023.

* [CCL\({}^{+}\)23b] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In _The Eleventh International Conference on Learning Representations_, 2023.
* [CDD23] Sitan Chen, Giannis Daras, and Alexandros G Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for ddim-type samplers. _arXiv preprint arXiv:2303.03384_, 2023.
* [Cel22] Michael Celentano. Sudakov-fermique post-amp, and a new proof of the local convexity of the tap free energy. _arXiv preprint arXiv:2208.09550_, 2022.
* [CFM21] Michael Celentano, Zhou Fan, and Song Mei. Local convexity of the tap free energy and amp convergence for z2-synchronization. _arXiv preprint arXiv:2106.11428_, 2021.
* [CLL22] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: user-friendly bounds under minimal smoothness assumptions. _arXiv preprint arXiv:2211.01916_, 2022.
* [DB22] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. _Transactions on Machine Learning Research_, 2022.
* [DBTHD21] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrodinger bridge with applications to score-based generative modeling. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 17695-17709. Curran Associates, Inc., 2021.
* [DHKK20] Ilias Diakonikolas, Samuel B Hopkins, Daniel Kane, and Sushrut Karmalkar. Robustly learning any clusterable mixture of gaussians. _arXiv preprint arXiv:2005.06417_, 2020.
* [DK20] Ilias Diakonikolas and Daniel M Kane. Small covers for near-zero sets of polynomials and learning latent variable models. In _2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)_, pages 184-195. IEEE, 2020.
* [DKS17] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 73-84. IEEE, 2017.
* [DKS18] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean estimation and learning mixtures of spherical gaussians. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1047-1060, 2018.
* [DMM09] David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for compressed sensing. _Proceedings of the National Academy of Sciences_, 106(45):18914-18919, 2009.
* [DMM10] David L Donoho, Arian Maleki, and Andrea Montanari. Message passing algorithms for compressed sensing: I. motivation and construction. In _2010 IEEE information theory workshop on information theory (ITW 2010, Cairo)_, pages 1-5. IEEE, 2010.
* [DS07] Sanjoy Dasgupta and Leonard J Schulman. A probabilistic analysis of em for mixtures of separated, spherical gaussians. _Journal of Machine Learning Research_, 8:203-226, 2007.
* [DTZ17] Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of em suffice for mixtures of two gaussians. In _Conference on Learning Theory_, pages 704-710. PMLR, 2017.
* [EAMS22] Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. Sampling from the sherrington-kirkpatrick gibbs measure via algorithmic stochastic localization. In _2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)_, pages 323-334. IEEE, 2022.

* [Eld13] Ronen Eldan. Thin shell implies spectral gap up to polylog via a stochastic localization scheme. _Geometric and Functional Analysis_, 23(2):532-569, 2013.
* [Eld20] Ronen Eldan. Taming correlations through entropy-efficient measure decompositions with applications to mean-field approximation. _Probability Theory and Related Fields_, 176(3-4):737-755, 2020.
* [HJA20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [HL18] Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1021-1034, 2018.
* [HP15] Moritz Hardt and Eric Price. Tight bounds for learning a mixture of two gaussians. In _Proceedings of the forty-seventh annual ACM symposium on Theory of computing_, pages 753-760, 2015.
* [Kab03] Yoshiyuki Kabashima. A cdma multiuser detection algorithm on the basis of belief propagation. _Journal of Physics A: Mathematical and General_, 36(43):11111, 2003.
* [Kan21] Daniel M Kane. Robust learning of mixtures of gaussians. In _Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 1246-1258. SIAM, 2021.
* [KC20] Jeongyeol Kwon and Constantine Caramanis. The em algorithm gives sample-optimality for learning mixtures of well-separated gaussians. In _Conference on Learning Theory_, pages 2425-2487. PMLR, 2020.
* [KMV10] Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two gaussians. In _Proceedings of the forty-second ACM symposium on Theory of computing_, pages 553-562, 2010.
* [KSS18] Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1035-1046, 2018.
* [LL22] Allen Liu and Jerry Li. Clustering mixtures with almost optimal separation in polynomial time. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1248-1261, 2022.
* [LLT22] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [LLT23] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In _International Conference on Algorithmic Learning Theory_, pages 946-985. PMLR, 2023.
* [LM23] Allen Liu and Ankur Moitra. Robustly learning general mixtures of gaussians. _Journal of the ACM_, 2023.
* [LWCC23] Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic convergence for diffusion-based generative models. _arXiv preprint arXiv:2306.09251_, 2023.
* [LWYL22] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: understanding and extending diffusion generative models. _arXiv preprint arXiv:2208.14699_, 2022.
* [MM09] Marc Mezard and Andrea Montanari. _Information, physics, and computation_. Oxford University Press, 2009.

* [MV10] Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians. In _2010 IEEE 51st Annual Symposium on Foundations of Computer Science_, pages 93-102. IEEE, 2010.
* [MV21] Andrea Montanari and Ramji Venkataraman. Estimation of low-rank matrices via approximate message passing. _The Annals of Statistics_, 49(1), 2021.
* [MW23] Andrea Montanari and Yuchen Wu. Posterior sampling from the spiked models via diffusion processes. _arXiv preprint arXiv:2304.11449_, 2023.
* [Pea94] Karl Pearson. Contributions to the mathematical theory of evolution. _Philosophical Transactions of the Royal Society of London. A_, 185:71-110, 1894.
* [Pid22] Jakiw Pidstrigach. Score-based generative models detect manifolds. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 35852-35865. Curran Associates, Inc., 2022.
* [RBL\({}^{+}\)22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [RDN\({}^{+}\)22] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [RV17] Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated gaussians. In _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 85-96. IEEE, 2017.
* [SCS\({}^{+}\)22] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. _arXiv preprint arXiv:2205.11487_, 2022.
* [SDWMG15] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* [SE19] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. _Advances in Neural Information Processing Systems_, 32, 2019.
* [SN21] Nimrod Segol and Boaz Nadler. Improved convergence guarantees for learning gaussian mixture models by em and gradient em. _Electronic journal of statistics_, 15(2):4510-4544, 2021.
* [SOAJ14] Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimal-sample estimators for spherical gaussian mixtures. _Advances in Neural Information Processing Systems_, 27, 2014.
* [SSDK\({}^{+}\)20] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* [Ver] Roman Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Number 47 in Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.
* [VW04] Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. _Journal of Computer and System Sciences_, 68(4):841-860, 2004. Special Issue on FOCS 2002.

* [WJ\({}^{+}\)08] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. _Foundations and Trends(r) in Machine Learning_, 1(1-2):1-305, 2008.
* [WY22] Andre Wibisono and Kaylee Y. Yang. Convergence in KL divergence of the inexact Langevin algorithm with application to score-based generative models. _arXiv preprint 2211.01512_, 2022.
* [XHM16] Ji Xu, Daniel J Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures of two gaussians. _Advances in Neural Information Processing Systems_, 29, 2016.
* [YYS17] Bowei Yan, Mingzhang Yin, and Purnamrita Sarkar. Convergence analysis of gradient em for multi-component gaussian mixture. _arXiv preprint arXiv:1705.08530_, 2017.
* [ZLS20] Ruofei Zhao, Yuanzhi Li, and Yuekai Sun. Statistical convergence of the em algorithm on gaussian mixture models. 2020.