Towards Reliable Model Selection for Unsupervised Domain Adaptation: An Empirical Study and A Certified Baseline

Dapeng Hu\({}^{1}\)1  Mi Luo\({}^{3}\) Jian Liang\({}^{4,5}\)2  Chuan-Sheng Foo\({}^{2,1}\)

\({}^{1}\)Centre for Frontier AI Research, A*STAR, Singapore

\({}^{2}\)Institute for Infocomm Research, A*STAR, Singapore

\({}^{3}\)National University of Singapore

\({}^{4}\)NLPR \(\&\) MAIS, Institute of Automation, Chinese Academy of Sciences

\({}^{5}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

Footnote 1: This work was completed while Dapeng (lhxxhb16@gmail.com) was a scientist at A*STAR.

Footnote 2: Corresponding author: Jian Liang (liangjian92@gmail.com)

###### Abstract

Selecting appropriate hyperparameters is crucial for unlocking the full potential of advanced unsupervised domain adaptation (UDA) methods in unlabeled target domains. Although this challenge remains under-explored, it has recently garnered increasing attention with the proposals of various model selection methods. Reliable model selection should maintain performance across diverse UDA methods and scenarios, especially avoiding highly risky worst-case selections--selecting the model or hyperparameter with the worst performance in the pool. _Are existing model selection methods reliable and versatile enough for different UDA tasks?_ In this paper, we provide a comprehensive empirical study involving 8 existing model selection approaches to answer this question. Our evaluation spans 12 UDA methods across 5 diverse UDA benchmarks and 5 popular UDA scenarios. Surprisingly, we find that none of these approaches can effectively avoid the worst-case selection. In contrast, a simple but overlooked ensemble-based selection approach, which we call EnsV, is both theoretically and empirically certified to avoid the worst-case selection, ensuring high reliability. Additionally, EnsV is versatile for various practical but challenging UDA scenarios, including validation of open-partial-set UDA and source-free UDA. Finally, we call for more attention to the reliability of model selection in UDA: avoiding the worst-case is as significant as achieving peak selection performance and should not be overlooked when developing new model selection methods. Code is available at https://github.com/LHXXHB/EnsV.

## 1 Introduction

Deep learning has achieved incredible advancements in various tasks through supervised learning with large labeled datasets [1]. However, obtaining labels can be expensive, and deep models often struggle to generalize to unlabeled data from unseen distributions [2]. Domain adaptation [3] tackles this challenge by transferring knowledge from a labeled source domain to a target domain with limited labels but a similar task. Unsupervised domain adaptation [4] (UDA), particularly, has garnered significant attention due to its practical assumption that the target domain is entirely unlabeled, witnessing the development of many effective methods [5, 6, 7, 8] and practical settings [9, 10, 11, 12].

However, successful applications of UDA methods across diverse tasks rely heavily on selecting appropriate hyperparameters. Sub-optimal hyperparameters can cause state-of-the-art UDA methods to underperform compared to the source-trained model without target-domain adaptation [19; 18]. This phenomenon emphasizes the significance of model selection, also called hyperparameter selection or validation, in UDA. Taking the typical one-hyperparameter validation task of a given UDA method as an example, we need to determine the optimal value of a hyperparameter \(\eta\) among a set of \(m\) different candidate values \(\{\eta_{i}\}_{i=1}^{m}\). By applying these different \(\eta_{i}\) with the same UDA method, we can obtain a set of \(m\) different models with the parameter weights \(\{\theta_{i}\}_{i=1}^{m}\). The goal is to identify the candidate model that exhibits the best performance on the unlabeled target domain and subsequently adopt the associated hyperparameter value for \(\eta\). This model selection problem remains challenging and under-explored in UDA due to cross-domain distribution shifts and the absence of labeled target data.

Existing approaches can be categorized into two types. The first type involves leveraging labeled source data for target-domain model selection [9; 14; 15; 16]. The second type designs unsupervised metrics based on priors of the learned target-domain structure and utilizes the metrics for model selection [17; 19; 18; 20]. It is natural to ask: Are these approaches reliable in model selection tasks, i.e., can they maintain good performance for various practical UDA tasks?

To answer this question, we conduct an extensive empirical study to assess the performance of all selection methods across various practical UDA settings, including closed-set UDA [21], partial-set UDA [10], open-partial-set UDA [11], and source-free UDA [12; 22]. Notably, the model selection problem of open-partial-set UDA has not been investigated before. Surprisingly, we find that despite their specific designs, all these methods encounter challenges in avoiding the selection of poor or even the worst models across various UDA methods and settings. This renders the adaptation ineffective or even harmful, thereby constraining their adoption by researchers and practitioners in the community [18]. For instance, Table 1 compares the worst-case selection statistics of all these model selection methods across various practical UDA settings. These settings include standard closed-set UDA and partial-set UDA, which have been extensively studied in prior works [15; 19], and source-free UDA, where the model selection problem has not been widely investigated. The comparison reveals that all the methods occasionally or even frequently suffer from worst-case model selection situations, indicating high unreliability.

In contrast, we note that a simple ensemble-based validation baseline, dubbed EnsV, can effectively avoid the worst-case selection. Through a straightforward theoretical analysis of the ensemble, we observe that it is guaranteed to surpass the worst candidate model's performance. Our introduced EnsV takes a further simple step, utilizing the ensemble as a role model for directly assessing candidate models during the model selection process. This strategy ensures the secure avoidance of selecting the worst candidate model, thereby enhancing the reliability of model selection. Moreover, EnsV only uses target-domain predictions inferred by all candidate models. This eliminates the need for specific domain shift assumptions and access to source data, while also requiring no additional effort, such as time and memory, as all models are provided within the given problem context. This simplicity and versatility make EnsV suitable for various practical UDA scenarios, including the unexplored challenges of validation for UDA with unknown open classes [19]. Despite EnsV not being certified for peak-performance selection, we hope that, as the first to focus on the practical

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline Method & Closed-set UDA & Partial-set UDA & Source-free UDA \\ \hline SourceRisk [9] & 16 / 110 & 2 / 24 & n.a. \\ IWCV [14] & 15 / 110 & 3 / 24 & n.a. \\ DEV [15] & 9 / 110 & 1 / 24 & n.a. \\ RV [16] & 2 / 110 & 1 / 24 & n.a. \\ \hline Entropy [17] & 15 / 131 & 7 / 24 & 16 / 17 \\ InfoMax [18] & 9 / 131 & 12 / 24 & 16 / 17 \\ SND [19] & 33 / 131 & 3 / 24 & 11 / 17 \\ Corr-C [20] & 80 / 131 & 4 / 24 & 3 / 17 \\ EnsV (Ours) & **0 / 131** & **0 / 24** & **0 / 17** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics for worst-case selections by various model selection methods are provided across 110 closed-set UDA tasks (potentially an additional 21 tasks on DomainNet [13]), 24 partial-set UDA tasks, and 17 source-free UDA tasks (only for applicable methods). These statistics represent the count of worst-case selections divided by the total count of tasks, with **bold** font indicating the best worst-case avoidance. ‘n.a.’ indicates that certain methods are not applicable without source data.

aspect of worst-case avoidance in model selection, our empirical study and simple baseline can inspire future efforts in developing more reliable model selection methods.

## 2 Related Work

**Unsupervised domain adaptation** (UDA) is initially studied in a closed-set setting (CDA) where only covariate shift [14] is considered as the domain shift, and the two domains share the same label set. Recent research has explored many real-world UDA scenarios by incorporating label shift, where the two domains have distinct label sets. This includes partial-set UDA (PDA) [10], where several source classes are missing in the target domain, open-set UDA (ODA) [23], where the target domain contains samples from unknown classes, and open-partial-set UDA (OPDA) [11], where there are only some overlaps in the label sets across domains. More recently, source-free UDA settings (SFUDA) [24; 12] have been explored, where only the source model instead of source data is available for target adaptation, potentially addressing privacy concerns in the source domain. Subsequently, in the context of black-box domain adaptation [22], the privacy of the source domain is fully safeguarded. Specifically, the research community has made significant efforts to develop effective UDA methods in image classification [9; 6] and semantic segmentation [25; 26], which can be seen through two distinct research directions. The first direction focuses on aligning the distributions across domains by minimizing specific discrepancy measures [27; 28; 21; 29; 30] or using adversarial learning to maximize domain confusion [9]. Especially, adversarial learning has become a popular approach and has been explored at different levels for domain alignment, including image-level [31], manifold-level [9; 32; 6], and prediction-level [5; 25; 26; 33]. The second direction focuses on target-oriented learning, aiming to learn a good structure for the target domain. This includes self-training approaches [34; 12; 35] and target-specific regularizations [7; 8; 36]. To thoroughly assess the efficacy of model selection baselines, we opt for a diverse set of UDA methods across various UDA scenarios in our model selection experiments and then utilize these baselines to choose the appropriate hyperparameters for different UDA methods.

**Model selection**  for out-of-distribution (OOD) testing data is crucial for practical model deployment, but it remains challenging. Although the problem has attracted increasing attention in both domain generalization (DG) [37; 38] and UDA [18; 19], it remains relatively under-explored. In DG, since target data is not available for model selection, existing methods usually estimate the general OOD performance with multiple source domains. Differently, in UDA, thanks to the transductive setting, target data can be used for model selection in various ways. Efforts to address UDA model selection can be broadly categorized into two lines. Early approaches focused on estimating the target domain risk through labeled source data. SourceRisk [9] utilized a hold-out labeled source validation set to guide model selection based on source risk. To mitigate the impact of domain shift on source estimation, [14] introduced Importance-Weighted Cross-Validation (IWCV), which re-weights source risk using a source-target density ratio estimated in the input space. Building upon this, [15] improved IWCV by introducing Deep Embedded Validation (DEV), which estimates the density ratio in the feature space and offers lower variance. [16] proposed a novel Reverse Validation approach (RV) that leveraged reversed source risk for selection. However, source-based validation methods often necessitate additional model training to handle domain shifts, rendering them cumbersome and less reliable. In contrast, recent model selection methods have shifted their focus exclusively to

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline \hline Method & covariate & label & w/o & w/o extra & w/o & worst-case \\  & shift & shift & source data & hyperparameter & extra training & avoidance \\ \hline SourceRisk [9] & ✗ & ✗ & ✗ & ✗ & ✓ & ✗ \\ IWCV [14] & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ DEV [15] & ✓ & ✗ & ✗ & ✗ & ✗ \\ RV [16] & ✓ & ✗ & ✗ & ✗ & ✗ \\ \hline Entropy [17] & ✓ & ✗ & ✓ & ✓ & ✗ \\ InfoMax [18] & ✓ & ✗ & ✓ & ✓ & ✓ & ✗ \\ SND [19] & ✓ & ✓ & ✓ & ✗ & ✓ & ✗ \\ Corr-C [20] & ✓ & ✗ & ✓ & ✓ & ✓ & ✗ \\ EnsV (Ours) & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparisons of unsupervised model selection approaches used for UDA.

unlabeled target data, employing specifically designed metrics for model selection. For instance, [17] introduced the mean Shannon's Entropy of target predictions as a model selection metric, promoting confident predictions. [18] proposed the use of Input-Output Mutual Information Maximization (InfoMax)[39] as a metric, augmented with class-balance regularization over Entropy. [19] introduced Soft Neighborhood Density (SND), a novel metric focusing on neighborhood consistency. [20] presented Corr-C, a class correlation-based metric that evaluates both class diversity and prediction certainty simultaneously. Our EnsV baseline aligns with the latter line of research. Importantly, it operates without making any assumptions about cross-domain distribution shifts or the learned target-domain structure, making it suitable for a variety of UDA scenarios. A comprehensive comparison, as presented in Table 2, underscores that EnsV stands out as a simple and versatile approach.

**Ensemble** methods, which harness the collective power of a pool of models through prediction averaging, have been extensively studied in the machine learning community for enhancing model performance [40; 41; 42; 43] and improving model calibration [44; 45]. In the era of deep learning, the efficiency of ensembling has garnered significant attention due to the high training cost of deep models. Efficient solutions have been proposed, such as using partially shared parameters [46; 47; 48] and leveraging intermediate snapshots [49; 50; 51]. Recently, weight averaging has gained attention as an efficient alternative to prediction averaging during inference [52; 53; 54; 55; 56]. In addition, diversity is considered crucial for effective ensembles. Various approaches have been explored to achieve diverse checkpoints, including bootstrapping [57], random initializations [58], tuning hyperparameters [59; 60; 53], and combining multiple strategies [61]. Different from mainstream ensemble applications, our work innovatively and elegantly applies ensemble to help address the open problem of unsupervised model selection in various domain adaptation scenarios. In addition, [62] leverages ensembles for hyperparameter selection in CDA but directly uses prediction-based ensembling as the output, unlike our EnsV, which includes a selection step.

## 3 Methodology

We consider a \(C\)-way image classification task to introduce the concept of unsupervised domain adaptation (UDA). In UDA, we typically have a labeled source domain \(\mathcal{D}_{\mathrm{s}}=\{(x_{\mathrm{s}}^{s},y_{\mathrm{s}}^{s})\}_{i=1}^{n _{\mathrm{s}}}\) comprising \(n_{\mathrm{s}}\) annotated source images \(x_{\mathrm{s}}\) and their corresponding labels \(y_{\mathrm{s}}\). Additionally, there is an unlabeled target domain, \(\mathcal{D}_{\mathrm{t}}=\{x_{\mathrm{t}}^{i}\}_{i=1}^{n_{\mathrm{t}}}\), containing only \(n_{\mathrm{t}}\) unlabeled target images \(x_{\mathrm{t}}\). Despite the tasks being similar, there exist data distribution shifts between the two domains. The primary objective of UDA is to accurately predict the unavailable target labels, \(\{y_{\mathrm{t}}^{i}\}_{i=1}^{n_{\mathrm{t}}}\), by leveraging a discriminative mapping \(f(x,\theta)\), which is learned using data from two domains. Here, \(\theta\in\mathbb{R}^{d}\) represents the parameter weights of the trained UDA model. When presented with an input image \(x\), the model generates a probability prediction vector, \(p=f(x,\theta)\), where \(p\in\mathbb{R}^{C}\) and \(\sum_{i=1}^{C}p^{i}=1\).

Model selection in UDA is essentially equivalent to the hyperparameter selection challenge. Here, we aim to determine the optimal value for the hyperparameter \(\eta\) from a set of \(m\) candidate values \(\{\eta_{i}\}_{i=1}^{m}\). The hyperparameter \(\eta\) can encompass various aspects, including the learning rate, loss

Figure 1: **Left**: Depiction of the unsupervised model selection problem in domain adaptation scenarios, where the objective is to identify the optimal model for the unlabeled target domain. **Right:** Overview of our approach, EnsV, for model selection, which relies solely on predictions of target data by all candidate models.

coefficients, architectural settings, training iterations, and more. By training UDA models using the \(m\) different values of \(\eta\), we obtain corresponding models with weights denoted as \(\{\theta_{i}\}_{i=1}^{m}\). In UDA, the objective of model selection is to pinpoint the model \(\theta_{k}\) that demonstrates the best performance on the unlabeled target domain. Subsequently, we select the corresponding hyperparameter \(\eta_{k}\) as the optimal choice for potential adaptation with unlabeled target samples from the exact target domain. We illustrate the problem setting in Figure 1. Without loss of generality, in this paper, we assume \(m\) is greater than \(1\), and candidate models have different weights \(\theta\), resulting in different discriminative mappings of \(f(x,\theta)\). For clarity, we treat both \(\theta\) and the model interchangeably in the presentation. This also applies to model selection, hyperparameter selection, and validation.

### Ensemble: The Overlooked 'Free Lunch' in Model Selection

Model selection in UDA is challenging due to the absence of labeled target data for directly evaluating candidate models. Existing selection approaches typically address this challenge from two perspectives: leveraging labeled source data [15] or designing unsupervised metrics based on specific assumed priors [19]. Surprisingly, we've observed that all existing model selection methods treat each candidate model independently, overlooking the collective potential offered by the off-the-shelf ensemble created by these candidates. In this paper, unless otherwise specified, the ensemble refers to prediction-based ensembling, which involves averaging probability predictions across all models to obtain the averaged prediction, i.e., \(\frac{1}{m}\sum_{i=1}^{m}f(x,\theta_{i})\) for a sample \(x\).

In contrast, we first investigate the potential of the ensemble within the model selection problem. When contemplating the use of the ensemble, two primary concerns often arise, one concerning low efficiency due to training multiple models and the other related to the potential lack of diversity among candidate models. Upon closer inspection of model selection, we observe that the problem setting inherently offers a range of pre-existing candidate models, effectively addressing the efficiency concern without requiring extra model training. Furthermore, all candidate models are trained using a UDA method with varying hyperparameter values, resulting in diverse yet effective discriminative abilities. This naturally mitigates the diversity concern. _Interestingly, the ensemble emerges as a 'free lunch' in UDA model selection, a previously overlooked insight._ To delve deeper into the effectiveness of the ensemble, we present a theoretical analysis grounded in the proposition below.

**Proposition 1**: _Given negative log-likelihood (NLL) as the loss function, defined as \(l(p,y)=-\log p^{y}\), and considering a random sample \(x\) with label \(y\), the following inequality can be established between the loss of the ensemble \(\frac{1}{m}\sum_{i=1}^{m}f(x,\theta_{i})\), the averaged loss of all models \(\{\theta_{i}\}_{i=1}^{m}\), and the loss of the worst one \(\theta_{\mathrm{worst}}\):_

\[l(\frac{1}{m}\sum_{i=1}^{m}f(x,\theta_{i}),y)<\frac{1}{m}\sum_{i=1}^{m}l(f(x, \theta_{i}),y)<l(f(x,\theta_{\mathrm{worst}}),y).\]

Kindly refer to the Appendix for the proof. This proposition theoretically guarantees that the ensemble strictly outperforms the worst candidate model.

### Ensemble-based Validation (EnsV): Ensemble as A Role Model for Model Selection

Intuitively, we employ the previously mentioned off-the-shelf ensemble as a reliable role model and select the model that generates predictions closest to this role model among all candidates. To begin with, for each unlabeled target sample \(x\), we consider the ensemble \(\frac{1}{m}\sum_{i=1}^{m}f(x,\theta_{i})\) as a reliable estimation of its unavailable ground truth. This enables us to obtain reliable predictions for all target data, denoted as \(\{\frac{1}{m}\sum_{i=1}^{m}f(x_{j},\theta_{i})\}_{j=1}^{n_{t}}\). These ensembles can be viewed as the output of a reliable role model, aiding in accurate model selection in the subsequent step. We then utilize the role model to assess all candidate models and select the one with the highest similarity. For simplicity, EnsV involves direct measurement of accuracy between the role model output \(\{\frac{1}{m}\sum_{i=1}^{m}f(x_{j},\theta_{i})\}_{j=1}^{n_{t}}\) and the predictions made by each candidate model, such as \(\{f(x_{j},\theta_{i})\}_{j=1}^{n_{t}}\) for the model with weights \(\theta_{i}\). We select the model \(\theta_{k}\) with the highest accuracy and determine the optimal value \(\eta_{k}\) for the hyperparameter \(\eta\). Figure 1 provides a vivid illustration of our approach, EnsV. Guided by a reliable role model, EnsV can safely avoid selecting the worst candidate model, a distinct advantage over all existing model selection approaches.

## 4 Experiments

### Setup

**Datasets** Our experiments encompass diverse and widely-used image classification benchmarks: (_i_) _Office-31_[63] with 31 classes and 3 domains (Amazon (A), DSLR (D), and Webcam (W)); (_ii_) _Office-Home_[64] with 65 classes and 4 domains (Art (Ar), Clipart (Cl), Product (Pr), and Real-World (Re)); (_iii_) _VisDA_[65] with 12 classes and 2 domains (training (T) and validation (V)); and (_iv_) _DomainNet-126_[13; 5] with 126 classes and 4 domains (Real (R), Clipart (C), Painting (P), and Sketch (S)). Additionally, we conduct experiments in synthetic-to-real semantic segmentation, specifically targeting the transfer from _GTAV_[66] to _Cityscapes_[67].

**UDA methods** In our experiments, we assess all the model selection approaches listed in Table 2. Kindly refer to the Appendix for detailed introductions of them. With these approaches, we perform model selection for various UDA methods across different UDA settings. For CDA of image classification, we consider ATDOC [35], BNM [8], CDAN [6], MCC [36], MDD [33], and SAFN [7]. For PDA, we consider PADA [10] and SAFN [7]. For OPDA, we consider DANCE [11]. For SFUDA, we consider the white-box method SHOT [12] and the black-box method DINE [22]. For domain adaptive semantic segmentation, we consider AdaptSeg [25] and AdvEnt [26]. Following previous model selection studies [15; 19], we primarily focus on one-hyperparameter validation and present the comprehensive hyperparameter settings for all UDA methods in the Appendix. For each hyperparameter, we generally explore 7 candidate values. Additionally, we perform two types of challenging two-hyperparameter validation tasks. For classification tasks, we select the bottleneck dimension as the second hyperparameter from 4 options: \(256,512,1024,2048\) in MCC and MDD. For segmentation tasks, following SND [19], we select the training iteration as the second hyperparameter from 8 options, ranging from 16,000 to 30,000 iterations at intervals of 2,000 iterations, in AdaptSeg and AdvEnt.

**Implementation details** For all UDA methods, we train UDA models using the Transfer Learning Library* or the official GitHub code on a single RTX TITAN 16GB GPU with a batch size of 32 and a total number of iterations of 5000. Unless specified, checkpoints are saved at the last iteration. We adopt ResNet-101 [68] for _VisDA_ and segmentation tasks, ResNet-34 [68] for _DomainNet_, and ResNet-50 [68] for other benchmarks. We assess the selection performance of all model selection methods on our trained models for fair comparisons. As a result, comparing our reported values with those from the original papers [15; 19] would be inappropriate. We repeat trials with three random seeds and report the mean for results. Source-based validation methods allocate \(80\%\) of the source data for training and the remaining \(20\%\) for validation.

\begin{table}
\begin{tabular}{l|c c c c|c c c c c|c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{ATDOC [35]} & \multicolumn{3}{c|}{BNM [1]} & \multicolumn{3}{c|}{CDAN [6]} \\  & \(\sim\)Ar & \(\sim\)Cl & \(\sim\)Pr & \(\sim\)Re & avg & \(\sim\)Ar & \(\sim\)Cl & \(\sim\)Pr & \(\sim\)Re & avg & \(\sim\)Ar & \(\sim\)Cl & \(\sim\)Pr & \(\sim\)Re & avg \\ \hline SourceRisk [7] & 66.83 & 52.54 & 78.57 & 78.67 & 68.59 & 62.44 & 50.74 & 77.35 & 77.46 & 76.63 & 55.00 & 42.65 & 99.50 & 68.31 & 58.99 \\ IVCV [14] & 67.97 & 54.03 & 83.31 & 79.26 & 69.89 & 65.66 & 48.16 & 74.09 & 73.28 & 65.52 & 63.11 & 41.24 & 67.17 & 71.93 & 60.41 \\ DEV [15] & 67.39 & 54.23 & 77.78 & 79.39 & 69.70 & 68.56 & 56.39 & 73.92 & 79.59 & 68.41 & 67.32 & 57.04 & 68.76 & 76.91 & 67.49 \\ \hline FV [16] & 68.85 & 56.13 & 78.93 & 79.64 & 79.08 & 68.25 & 56.75 & **78.08** & 76.70 & 70.44 & 67.66 & 36.74 & 56.01 & 70.78 & 68.92 \\ Entropy [17] & 63.67 & 55.58 & 76.34 & 78.36 & 68.60 & 66.28 & 54.95 & 74.15 & 77.64 & 68.14 & 67.66 & **57.36** & 67.37 & 77.45 & 69.76 \\ InfMax [18] & 63.67 & 55.63 & 77.61 & 78.66 & 68.62 & 54.98 & 74.15 & 77.64 & 81.66 & 67.94 & **75.36** & 76.37 & 77.45 & 69.76 \\ SND [19] & 63.67 & 55.03 & 76.54 & 77.54 & 68.34 & 66.28 & 54.93 & 74.15 & 77.64 & 81.66 & **79.34** & **75.66** & 79.76 & 76.80 & 70.04 \\ Corr [20] & 63.51 & 50.39 & 73.89 & 73.88 & 68.42 & 58.10 & 48.57 & 67.99 & 79.06 & 68.30 & 53.84 & 41.21 & 64.96 & 67.65 & 56.91 \\ EnsV & **67.90** & **58.08** & **79.81** & **80.11** & **71.74** & **68.61** & **75.38** & **70.94** & **79.00** & 67.88 & **57.46** & **77.39** & **78.19** & **70.25** \\ \hline Worat & **62.92** & 50.39 & 73.39 & 78.38 & 68.52 & 58.10 & 45.37 & 68.96 & 79.09 & 67.50 & 53.30 & 41.21 & 47.28 & 67.75 & 68.56 & 36.60 \\ Best & 68.97 & 58.35 & 80.27 & 80.85 & 27.04 & 68.93 & 57.51 & 78.43 & 79.57 & 71.11 & 68.19 & 57.00 & 77.44 & 78.19 & 70.43 \\ \hline \multirow{2}{*}{Method} & \(\sim\)Ar & \(\sim\)Cl & \(\sim\)Pr & \(\sim\)Re & avg & \(\sim\)Ar & \(\sim\)Cl & \(\sim\)Pr & \(\sim\)Re & avg & \(\sim\)Ar & \(\sim\)Cl & \(\sim\)Pr & \(\

[MISSING_PAGE_FAIL:7]

with other target-specific validation methods on the large-scale benchmark _DomainNet-126_ and in two extra practical UDA settings: OPDA and SFUDA.

#### 4.1.1 CDA

We compare all target-specific validation methods on the large-scale benchmark _DomainNet-126_ (Table 6). EnsV consistently keeps the leading validation performance, while other approaches exhibit high variance.

#### 4.1.2 SPUDA

In some-free UDA, where source-based model selection methods are not applicable due to no access to source data, we select SHOT for the white-box setting on _Office-31_ and DINE for the black-box setting on _VisDA_ (Table 8). EnsV consistently maintains near-best selections, while other target-based approaches frequently make worst-case selections.

#### 4.1.3 Worst-model selection comparisons

For empirical evidence of the superiority of EnsV, we compare EnsV with other target-specific methods, specifically focusing on worst-case avoidance, through specific examples presented in Table 9. In short, EnsV consistently avoids the worst selections, while other methods often encounter significant challenges.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c|}{CDAN [6]} & \multicolumn{4}{c|}{BNM [5]} & \multicolumn{4}{c|}{ATDOC [7]} & \multicolumn{4}{c|}{_DNet_} \\  & \(\sim\)C & \(\sim\)P & \(\sim\)R & \(\sim\)S & avg & \(\sim\)C & \(\sim\)P & \(\sim\)R & \(\sim\)S & avg & \(\sim\)C & \(\sim\)P & \(\sim\)R & \(\sim\)S & avg \\ \hline Entropy [17] & **67.99** & **63.80** & 74.12 & **93.34** & **66.66** & 67.36 & 64.28 & 74.31 & 66.99 & 62.66 & 63.75 & 61.53 & 79.90 & 52.17 & 64.34 & 64.55 \\ IndMax [18] & **67.99** & **65.80** & 74.42 & **59.34** & **66.66** & 67.05 & 64.28 & 74.31 & 65.67 & 65.33 & 63.75 & 61.85 & 79.60 & 52.17 & 64.34 & 65.44 \\ SND [19] & **67.09** & 66.48 & 74.24 & **59.34** & 63.63 & 56.56 & 54.50 & 74.31 & 42.37 & 56.93 & 63.75 & 61.85 & 79.60 & 47.00 & 63.05 & 62.12 \\ Corr-C [20] & 75.35 & 62.88 & 74.42 & 54.63 & 62.32 & 59.75 & 63.41 & **72.62** & 43.27 & 60.79 & 59.98 & 52.27 & 74.42 & 35.69 & 62.59 & 61.90 \\ EnsV & 65.88 & 65.27 & **74.44** & 57.65 & 67.58 & **66.06** & **72.53** & 67.94 & **73.03** & **66.44** & **80.01** & **61.73** & **70.12** & **61.73** \\ \hline Worst & 57.35 & 66.70 & 73.43 & 57.14 & 60.44 & 57.95 & 75.40 & 74.31 & 42.37 & **56.74** & 59.78 & 63.74 & 74.00 & 68.19 & 67.93 \\ Best & 67.09 & 65.80 & 74.44 & 59.43 & 66.66 & 67.86 & 66.50 & 78.68 & 58.49 & 67.88 & 70.30 & 68.44 & 80.38 & 62.23 & 70.34 & 68.29 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Validation accuracy (\(\%\)) of CDA on _DomainNet-126_ (_DNet_).

\begin{table}
\begin{tabular}{l|c c c c c c|c c c c c|c c c} \hline \hline Method & \multicolumn{4}{c|}{SHOT [12]} & \multicolumn{4}{c|}{SHOT [12]} & \multicolumn{4}{c|}{DINE [22]} \\  & \(\sim\)Ar & \(\sim\)Cl & \(\sim\)P & \(\sim\)R & \(\sim\)P & \(\sim\)R & \(\sim\)P & \(\sim\)A & \(\sim\)D & \(\sim\)W & avg & T\(\sim\)V \\ \hline Entropy [17] & 38.29 & 26.08 & 36.51 & 35.92 & 17.10 & 32.19 & 37.69 & 46.40 & 45.33 & 25.39 & 33.75 & 39.37 & 34.27 \\ IndMax [18] & 38.29 & 26.08 & 36.51 & 35.92 & 17.10 & 32.19 & 37.69 & 46.40 & 45.33 & 25.39 & 33.75 & 39.37 & 34.25 \\ SND [19] & 1.00 & 0.00 & 12.73 & 0.00 & 42.84 & 1.95 & 19.72 & 19.59 & 35.69 & 25.39 & 0.90 & 28.40 & 14.98 \\ Corr-C [20] & 1.00 & 0.00 & 12.73 & 0.00 & 42.84 & 1.95 & 19.77 & 11.99 & 35.69 & 69.02 & 0.00 & 28.40 & 18.62 \\ EnsV & **38.40** & **75.96** & **66.57** & **71.76** & **78.17** & **69.99** & **77.42** & **48.15** & **69.40** & **81.84** & **67.54** & **84.31** & **68.96** \\ \hline Worst & 1.00 & 0.00 & 12.73 & 0.00 & 17.10 & 1.959 & 19.77 & 11.99 & 35.69 & 25.39 & 60.00 & 28.40 & 12.84 \\ Best & 67.09 & 76.96 & 66.57 & 77.16 & 75.17 & 69.69 & 77.42 & 64.32 & 72.87 & 81.84 & 67.54 & 84.31 & 72.98 \\ \hline \hline \end{tabular}
\end{table}
Table 7: H-score [69, 70] (\(\%\)) of an OPDA method DANCE [11] on _Office-Home_.

\begin{table}
\begin{tabular}{l|c c c c c c|c c c c|c c c c} \hline \hline  & \multicolumn{4}{c|}{CDAN [6]} & \multicolumn{4}{c|}{BNM [5]} & \multicolumn{4}{c|}{ATDOC [7]} & \multicolumn{4}{c}{_DNet_} \\ Method & ATDOC & ATDOC & BNM & BNM & MDD & SAFN & PADA & PADA & DANCE & DANCE & SHOT & DINE \\  & Cl\(-\)Ar & C\(-\)S & Ar\(-\)P & \(\sim\)P & \(\sim\)P & \(\sim\)Cl & \(\sim\)P & \(\sim\)P & \(\sim\)Ar & Re\(-\)Ar & Re\(-\)Ar & Pr\(-\)R & D\(-\)A & T\(-\)V \\ \hline Entropy [17] & 59.25 & 46.43 & 67.04 & 40.95 & 55.85 & 43.30 & 75.94 & 70.52 & 25.39 & 45.53 & 71.21 & 71.99 \\ InfoMax [18] & 59.25 & 46.43 & 67.04 & 54.93 & 55.85 & 43.30 & 78.02 & 70.52 & 25.39 & 45.53 & 71.21 & 71.99 \\ SND [19] & 59.25 & 46.43 & 67.04 & 40.95 & 21.60 & 43.30 & 55.94 & 74.66 & 25.39 & 35.69 & 7

### Further Analysis

**Validation with two hyperparameters** We conduct two-hyperparameters model selection experiments with a large pool of model candidates, i.e., 28 models for image classification (Table 10) and 48 models for image segmentation (Table 11). EnsV consistently achieves near-optimal selections in both scenarios, surpassing other versatile validation methods such as Entropy and SND.

**Robustness to architectures** In our experiments, we evaluate the robustness of EnsV across various ResNet backbone variants, observing consistent success across different scales. We also conduct validation experiments using the ViT-B architecture [71] on the R\(\rightarrow\)S task with BNM. The validation results, presented in Table 12, demonstrate that EnsV achieves the best selection. However, all other target-based methods except SND make the worst selection.

\begin{table}
\begin{tabular}{l|c c c|c c c c c c c} \hline \hline Method & \multicolumn{3}{c|}{MDD [3]} & \multicolumn{3}{c|}{MC [50]} \\ \cline{3-11}  & \(\text{Ar}\rightarrow\text{Cl}\) & \(\text{Cl}\rightarrow\text{Pr}\) & \(\text{Pr}\rightarrow\text{Re}\) & \(\text{Re}\rightarrow\text{Ar}\) & \(\text{avg}\) & \(\text{Ar}\rightarrow\text{Cl}\) & \(\text{Cl}\rightarrow\text{Pr}\) & \(\text{Re}\rightarrow\text{Ar}\) & \(\text{avg}\) & AVG \\ \hline SourceRisk & 55.99 & 73.15 & 78.77 & 69.39 & 69.33 & 57.91 & 76.84 & 81.13 & 72.89 & 72.19 & 70.76 \\ INCV [14] & 37.89 & 22.92 & 80.42 & 58.43 & 62.24 & 46.09 & 77.74 & 80.68 & 74.45 & 69.74 & 66.08 \\ DEV [15] & 52.60 & 21.11 & 53.36 & 67.07 & 41.64 & 59.47 & 76.84 & 81.94 & 74.08 & 73.08 & 67.26 \\ BV [16] & **57.90** & 72.25 & 80.83 & 70.79 & 70.37 & 59.13 & 76.84 & 82.03 & 71.98 & 72.50 & 71.44 \\ \hline Entropy [77] & 57.21 & **73.19** & 80.06 & **72.31** & 70.69 & 59.75 & 77.77 & 82.37 & 74.33 & 73.56 & 72.13 \\ InfMax [18] & **57.89** & 72.92 & 80.06 & **72.31** & **70.72** & 59.70 & **78.73** & **52.85** & 70.33 & 72.84 & 71.78 \\ SND [19] & 38.10 & 56.45 & 70.03 & 65.10 & 57.42 & 53.49 & 74.97 & 72.75 & 74.12 & 69.69 & 63.69 \\ Conv-C [20] & 30.17 & 44.47 & 57.15 & 50.76 & 45.71 & 44.90 & 56.75 & 74.32 & 67.61 & 60.90 & 53.31 \\ Env-P & 59.91 & 27.24 & **80.93** & 71.16 & 20.44 & **63.90** & 78.21 & 82.28 & **74.91** & **74.07** & **72.26** \\ \hline Worst & 30.17 & 39.81 & 53.36 & 50.76 & 43.53 & 43.02 & 36.75 & 73.47 & 67.24 & 60.12 & 51.83 \\ Best & 57.59 & 73.35 & 80.93 & 72.52 & 71.10 & 61.10 & 78.94 & 83.04 & 75.36 & 74.61 & 72.86 \\ \hline \hline \end{tabular}
\end{table}
Table 10: CDA accuracy (\(\%\)) on _Office-Home_ when two hyperparameters are validated.

\begin{table}
\begin{tabular}{l|c} \hline \hline Method & BNM [8] \\ \hline Entropy [17] & 28.21 \\ InfoMax [18] & 28.21 \\ SND [19] & 52.42 \\ Corr-C [20] & 28.21 \\ EnsV & **55.16** \\ \hline Worst & 28.21 \\ Best & 55.16 \\ \hline \hline \end{tabular}
\end{table}
Table 12: CDA accuracy (\(\%\)) of BNM with ViT as the backbone.

Figure 2: For the 28 candidate models available in the two-hyperparameter selection task with MDD on Ar\(\rightarrow\)Cl, we first rank them based on their respective actual target-domain accuracy. We then start with only the best candidate model in the pool and gradually add increasingly inferior models in ascending order of their accuracy. The figure illustrates how adding more inferior models affects the performance of the ensemble and model selection.

#### Robustness to poor candidates

Ensuring the ensemble's resilience to poor models is crucial for its broad effectiveness. We assess this by conducting a two-hyperparameters model selection task for MDD on Ar\(\rightarrow\)Cl. We consider a challenging scenario where only the best candidate model is initially in the pool, gradually adding an increasing number of inferior models. This setup allows us to examine how the ensemble performs when dominated by inferior models. From the results shown in Figure 2, we find that both Ensemble and EnsV still consistently achieve performance above the median, demonstrating their resilience. In contrast, SND [19], a state-of-the-art method, struggles to surpass the median accuracy.

## 5 Discussions

#### Limitations

EnsV selects the candidate model that most closely matches the ensemble's performance. While EnsV consistently avoids selecting the worst-performing models, its performance can be suboptimal if the ensemble (role model) itself is suboptimal. EnsV may face challenges in the following scenarios:

* Single high-performing vs. poor model: when the model pool contains only one high-performing model and one poor model, EnsV may struggle. In such cases, the ensemble of just these two models might not accurately reflect the quality of the good model.
* Small performance differences: if the performance differences among all candidate models are small, EnsV may have difficulty distinguishing between them. In such scenarios, using ensemble results for model selection may not provide the fine-grained control needed to accurately differentiate between the candidates.
* Predominantly poor models: when the majority of models in the pool are poor, with only a few having normal or good performance, EnsV may encounter issues. An ensemble composed mostly of poor models may produce results that are closer to the poor models, leading to a final selection of a suboptimal candidate.

#### Takeaways

Following a thorough empirical comparison of existing UDA model selection approaches, several key conclusions emerge:

* The significance of model selection in influencing the deployment performance of UDA methods becomes evident. Relying on fixed hyperparameters or limited analyses is inadequate. We emphasize the importance of increased attention and transparent reporting of validation methods, consistent with recommendations in [15; 19; 18].
* Among existing validation methods, we recommend the reverse validation (RV) approach, which, despite being overlooked in previous studies [15; 19; 18], proves to be the most reliable method for widely studied closed-set UDA scenarios when source data is available. However, it requires additional model re-training, making it less lightweight compared to target-based validation methods. Moreover, all existing model selection methods demonstrate unreliability across diverse UDA methodologies and real-world settings such as open-set and source-free UDA. These methods struggle to maintain effectiveness, posing a significant risk to the successful application of UDA in various scenarios.
* Regarding our proposed baseline, EnsV, we believe it is a simple and versatile model selection method that is certified to avoid worst-case selections. While it may not always achieve peak performance, especially when the ensemble result is suboptimal, EnsV offers valuable insights for future explorations in reliable model selection methods.

## Acknowledgements

Jian Liang was funded by the Beijing Nova Program under Grant Z211100002121108, the National Natural Science Foundation of China under Grant 62276256, and the Young Elite Scientists Sponsorship Program by CAST (2023QNRC001).

## References

* [1] Russakovsky, O., J. Deng, H. Su, et al. Imagenet large scale visual recognition challenge. _International Journal of Computer Vision_, 2015.
* [2] Hendrycks, D., K. Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. _arXiv preprint arXiv:1610.02136_, 2016.
* [3] Pan, S. J., Q. Yang. A survey on transfer learning. _IEEE Transactions on Knowledge and Data Engineering_, 2009.
* [4] Pan, S. J., I. W. Tsang, J. T. Kwok, et al. Domain adaptation via transfer component analysis. _IEEE Transactions on Neural Networks_, 2010.
* [5] Saito, K., K. Watanabe, Y. Ushiku, et al. Maximum classifier discrepancy for unsupervised domain adaptation. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2018.
* [6] Long, M., Z. Cao, J. Wang, et al. Conditional adversarial domain adaptation. In _Advances in Neural Information Processing Systems_. 2018.
* [7] Xu, R., G. Li, J. Yang, et al. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In _IEEE International Conference on Computer Vision_. 2019.
* [8] Cui, S., S. Wang, J. Zhuo, et al. Towards discriminability and diversity: Batch nuclear-norm maximization under label insufficient situations. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2020.
* [9] Ganin, Y., V. Lempitsky. Unsupervised domain adaptation by backpropagation. In _International Conference on Machine Learning_. 2015.
* [10] Cao, Z., L. Ma, M. Long, et al. Partial adversarial domain adaptation. In _European Conference on Computer Vision_. 2018.
* [11] Saito, K., D. Kim, S. Sclaroff, et al. Universal domain adaptation through self supervision. In _Advances in Neural Information Processing Systems_. 2020.
* [12] Liang, J., D. Hu, J. Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In _International Conference on Machine Learning_. 2020.
* [13] Peng, X., Q. Bai, X. Xia, et al. Moment matching for multi-source domain adaptation. In _IEEE International Conference on Computer Vision_. 2019.
* [14] Sugiyama, M., M. Krauledat, K.-R. Muller. Covariate shift adaptation by importance weighted cross validation. _Journal of Machine Learning Research_, 2007.
* [15] You, K., X. Wang, M. Long, et al. Towards accurate model selection in deep unsupervised domain adaptation. In _International Conference on Machine Learning_. 2019.
* [16] Ganin, Y., E. Ustinova, H. Ajakan, et al. Domain-adversarial training of neural networks. _Journal of Machine Learning Research_, 2016.
* [17] Morerio, P., J. Cavazza, V. Murino. Minimal-entropy correlation alignment for unsupervised deep domain adaptation. _arXiv preprint arXiv:1711.10288_, 2017.
* [18] Musgrave, K., S. Belongie, S.-N. Lim. Benchmarking validation methods for unsupervised domain adaptation. _arXiv preprint arXiv:2208.07360_, 2022.
* [19] Saito, K., D. Kim, P. Teterwak, et al. Tune it the right way: Unsupervised validation of domain adaptation via soft neighborhood density. In _IEEE International Conference on Computer Vision_. 2021.
* [20] Tu, W., W. Deng, T. Gedeon, et al. Assessing model out-of-distribution generalization with softmax prediction probability baselines and a correlation method, 2023.
* [21] Long, M., Y. Cao, J. Wang, et al. Learning transferable features with deep adaptation networks. In _International Conference on Machine Learning_. 2015.
* [22] Liang, J., D. Hu, J. Feng, et al. Dine: Domain adaptation from single and multiple black-box predictors. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2022.
* [23] Panareda Busto, P., J. Gall. Open set domain adaptation. In _IEEE International Conference on Computer Vision_. 2017.

* [24] Li, R., Q. Jiao, W. Cao, et al. Model adaptation: Unsupervised domain adaptation without source data. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2020.
* [25] Tsai, Y.-H., W.-C. Hung, S. Schulter, et al. Learning to adapt structured output space for semantic segmentation. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2018.
* [26] Vu, T.-H., H. Jain, M. Bucher, et al. Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2019.
* [27] Gong, B., Y. Shi, F. Sha, et al. Geodesic flow kernel for unsupervised domain adaptation. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2012.
* [28] Fernando, B., A. Habrard, M. Sebban, et al. Unsupervised visual domain adaptation using subspace alignment. In _IEEE International Conference on Computer Vision_. 2013.
* [29] Sun, B., K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In _European Conference on Computer Vision, Workshop_. 2016.
* [30] Yang, Y., S. Soatto. Fda: Fourier domain adaptation for semantic segmentation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2020.
* [31] Hoffman, J., E. Tzeng, T. Park, et al. Cycada: Cycle-consistent adversarial domain adaptation. In _International Conference on Machine Learning_. 2018.
* [32] Tzeng, E., J. Hoffman, K. Saenko, et al. Adversarial discriminative domain adaptation. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2017.
* [33] Zhang, Y., T. Liu, M. Long, et al. Bridging theory and algorithm for domain adaptation. In _International Conference on Machine Learning_. 2019.
* [34] Shu, R., H. H. Bui, H. Narui, et al. A dirt-t approach to unsupervised domain adaptation. _arXiv preprint arXiv:1802.08735_, 2018.
* [35] Liang, J., D. Hu, J. Feng. Domain adaptation with auxiliary target domain-oriented classifier. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2021.
* [36] Jin, Y., X. Wang, M. Long, et al. Minimum class confusion for versatile domain adaptation. In _European Conference on Computer Vision_. 2020.
* [37] Arpit, D., H. Wang, Y. Zhou, et al. Ensemble of averages: Improving model selection and boosting performance in domain generalization. In _Advances in Neural Information Processing Systems_. 2022.
* [38] Chen, Y., K. Zhou, Y. Bian, et al. Pareto invariant risk minimization: Towards mitigating the optimization dilemma in out-of-distribution generalization. In _International Conference on Learning Representations_. 2023.
* [39] Bridle, J., A. Heading, D. MacKay. Unsupervised classifiers, mutual information and'phantom targets. In _Advances in Neural Information Processing Systems_. 1991.
* [40] Perrone, M. P., L. N. Cooper. When networks disagree: Ensemble methods for hybrid neural networks. In _How We Learn; How We Remember: Toward An Understanding Of Brain And Neural Systems: Selected Papers of Leon N Cooper_. World Scientific, 1995.
* [41] Opitz, D., R. Maclin. Popular ensemble methods: An empirical study. _Journal of Artificial Intelligence Research_, 1999.
* [42] Bauer, E., R. Kohavi. An empirical comparison of voting classification algorithms: Bagging, boosting, and variants. _Machine Learning_, 1999.
* [43] Dietterich, T. G. Ensemble methods in machine learning. In _Multiple Classifier Systems: First International Workshop_. 2000.
* [44] Lakshminarayanan, B., A. Pritzel, C. Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In _Advances in Neural Information Processing Systems_. 2017.
* [45] Ovadia, Y., E. Fertig, J. Ren, et al. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. In _Advances in Neural Information Processing Systems_. 2019.
* [46] Lee, S., S. Purushwalkam, M. Cogswell, et al. Why m heads are better than one: Training a diverse ensemble of deep networks. _arXiv preprint arXiv:1511.06314_, 2015.

* [47] Wen, Y., D. Tran, J. Ba. Batchensemble: an alternative approach to efficient ensemble and lifelong learning. _arXiv preprint arXiv:2002.06715_, 2020.
* [48] Dusenberry, M., G. Jerfel, Y. Wen, et al. Efficient and scalable bayesian neural nets with rank-1 factors. In _International Conference on Machine Learning_. 2020.
* [49] Huang, G., Y. Li, G. Pleiss, et al. Snapshot ensembles: Train 1, get m for free. _arXiv preprint arXiv:1704.00109_, 2017.
* [50] Garipov, T., P. Izmailov, D. Podoprikhin, et al. Loss surfaces, mode connectivity, and fast ensembling of dnns. In _Advances in Neural Information Processing Systems_. 2018.
* [51] Benton, G., W. Maddox, S. Lotfi, et al. Loss surface simplexes for mode connecting volumes and fast ensembling. In _International Conference on Machine Learning_. 2021.
* [52] Izmailov, P., D. Podoprikhin, T. Garipov, et al. Averaging weights leads to wider optima and better generalization. _arXiv preprint arXiv:1803.05407_, 2018.
* [53] Wortsman, M., G. Ilharco, S. Y. Gadre, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _International Conference on Machine Learning_. 2022.
* [54] Matena, M. S., C. A. Raffel. Merging models with fisher-weighted averaging. In _Advances in Neural Information Processing Systems_. 2022.
* [55] Rame, A., J. Zhang, L. Bottou, et al. Pre-train, fine-tune, interpolate: a three-stage strategy for domain generalization. In _Advances in Neural Information Processing Systems, Workshop_. 2022.
* [56] Rame, A., K. Ahuja, J. Zhang, et al. Recycling diverse models for out-of-distribution generalization. _arXiv preprint arXiv:2212.10445_, 2022.
* [57] Freund, Y., R. E. Schapire, et al. Experiments with a new boosting algorithm. In _International Conference on Machine Learning_. 1996.
* [58] Fort, S., H. Hu, B. Lakshminarayanan. Deep ensembles: A loss landscape perspective. _arXiv preprint arXiv:1912.02757_, 2019.
* [59] Wenzel, F., J. Snoek, D. Tran, et al. Hyperparameter ensembles for robustness and uncertainty quantification. In _Advances in Neural Information Processing Systems_. 2020.
* [60] Zaidi, S., A. Zela, T. Elsken, et al. Neural ensemble search for uncertainty estimation and dataset shift. In _Advances in Neural Information Processing Systems_. 2021.
* [61] Gontijo-Lopes, R., Y. Dauphin, E. D. Cubuk. No one representation to rule them all: Overlapping features of training methods. _arXiv preprint arXiv:2110.12899_, 2021.
* [62] Dinu, M.-C., M. Holzleitner, M. Beck, et al. Addressing parameter choice issues in unsupervised domain adaptation by aggregation. In _International Conference on Learning Representations_. 2023.
* [63] Saenko, K., B. Kulis, M. Fritz, et al. Adapting visual category models to new domains. In _European Conference on Computer Vision_. 2010.
* [64] Venkateswara, H., J. Eusebio, S. Chakraborty, et al. Deep hashing network for unsupervised domain adaptation. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2017.
* [65] Peng, X., B. Usman, N. Kaushik, et al. Visda: The visual domain adaptation challenge. _arXiv preprint arXiv:1710.06924_, 2017.
* [66] Richter, S. R., V. Vineet, S. Roth, et al. Playing for data: Ground truth from computer games. In _European Conference on Computer Vision_. 2016.
* [67] Cordts, M., M. Omran, S. Ramos, et al. The cityscapes dataset for semantic urban scene understanding. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2016.
* [68] He, K., X. Zhang, S. Ren, et al. Deep residual learning for image recognition. In _IEEE Conference on Computer Vision and Pattern Recognition_. 2016.
* [69] Fu, B., Z. Cao, M. Long, et al. Learning to detect open classes for universal domain adaptation. In _European Conference on Computer Vision_. 2020.

* [70] Bucci, S., M. R. Loghmani, T. Tommasi. On the effectiveness of image rotation for open set domain adaptation. In _European Conference on Computer Vision_. 2020.
* [71] Dosovitskiy, A., L. Beyer, A. Kolesnikov, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_. 2021.
* [72] Cortes, C., M. Mohri, M. Riley, et al. Sample selection bias correction theory. In _Algorithmic Learning Theory_. 2008.
* [73] Zhong, E., W. Fan, Q. Yang, et al. Cross validation framework to choose amongst models and datasets for transfer learning. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_. 2010.
* [74] Grandvalet, Y., Y. Bengio. Semi-supervised learning by entropy minimization. In _Advances in Neural Information Processing Systems_. 2004.
* [75] Chapelle, O., A. Zien. Semi-supervised classification by low density separation. In _International Workshop on Artificial Intelligence and Statistics_. 2005.

## Checklist

The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [**TODO**] to [Yes], [No], or [N/A]. You are strongly encouraged to include a **justification to your answer**, either by referencing the appropriate section of your paper or providing a brief inline description. For example:

* Did you include the license to the code and datasets? [Yes] See Section.
* Did you include the license to the code and datasets? [No] The code and the data are proprietary.
* Did you include the license to the code and datasets? [N/A]

Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See the Discussions. 3. Did you discuss any potential negative societal impacts of your work? [No] No obvious negative societal impacts. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Report the mean values with three random seeds. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [N/A] 3. Did you include any new assets either in the supplemental material or as a URL? [N/A] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]Proof of Proposition 1

We first prove the first inequality using Jensen's inequality, which states that for a real-valued, convex function \(\varphi\) with its domain as a subset of \(\mathbb{R}\) and numbers \(t_{1},\dots,t_{n}\) in its domain, the inequality \(\varphi\left(\frac{1}{n}\sum_{i=1}^{n}t_{i}\right)\leq\frac{1}{n}\sum_{i=1}^{n} \varphi(t_{i})\) holds. Given that \(-\log\) is convex, and assuming \(m>1\) with candidate models having different parameter weights \(\theta\), resulting in distinct discriminative mappings of \(f(x,\theta)\), we can strictly obtain \(l(\frac{1}{m}\sum_{i=1}^{m}f(x,\theta_{i}),y)<\frac{1}{m}\sum_{i=1}^{m}l(f(x, \theta_{i}),y)\) without the equal situation. Next, we leverage the property of inequalities to prove the second inequality. Here, \(\theta_{\mathrm{worst}}\) denotes the worst candidate model, i.e., the model with the largest loss. For any other candidate model \(\theta_{i}\), we have \(l(f(x,\theta_{i}),y)<l(f(x,\theta_{\mathrm{worst}}),y)\). This ensures that \(\frac{1}{m}\sum_{i=1}^{m}l(f(x,\theta_{i}),y)<\frac{1}{m}\sum_{i=1}^{m}l(f(x, \theta_{\mathrm{worst}}),y)\), or explicitly, \(\frac{1}{m}\sum_{i=1}^{m}l(f(x,\theta_{i}),y)<l(f(x,\theta_{\mathrm{worst}}),y)\). Substituting the NLL loss with any strongly convex loss function would still uphold the proposition.

## Appendix B Model Selection Baselines

Let \(\{p_{t}^{i}\}_{i=1}^{n_{\mathrm{t}}}\) represent the output probability vectors of all \(n_{\mathrm{t}}\) target samples, and let \(P\in\mathbb{R}^{n_{\mathrm{t}}\times C}\) denote the total probability matrix. We introduce the respective computation involved in the existing model selection approaches.

Source risk.The SourceRisk approach [9] utilizes a held-out validation set from the source domain to select the model \(\theta_{k}\) that performs best on this set as the final decision. However, this method has limited effectiveness in scenarios with severe domain shifts between the source and target domains. Additionally, it introduces additional hyperparameters for dataset splitting, which can further complicate the model selection process.

Importance-weighted source risk.Directly taking source risk as target risk is unreliable due to domain distribution shifts between domains. To address this challenge, [14] propose Importance-Weighted Cross Validation (IWCV), which re-weights the source risk using a source-target density ratio estimated in the input space. [15] further enhance IWCV by introducing Deep Embedded Validation (DEV), which estimates the density ratio in the feature space using a domain discriminator and controls the variance. Both IWCV and DEV rely on the importance weighting technique [72], which assumes that the target distribution is included in the source distribution [14], making the weighting unreliable in scenarios with severe covariate shift and label shift. In addition, both IWCV and DEV involve hyperparameters and extra model training during the density ratio estimation process.

Reversed source risk.Building upon the concept of reverse cross-validation [73], [16] propose a novel Reverse Validation approach (RV). This method first conducts source-to-target adaptation to obtain a UDA model, which enables the acquisition of pseudo labels for the target unlabeled data. Subsequently, Reverse Validation performs a reversed adaptation from the pseudo-labeled target to the source and utilizes the source risk in this reversed adaptation task for validation. Reverse Validation relies on the symmetry between domains and cannot handle label shifts. Additionally, this approach involves hyperparameters for dataset splitting.

Entropy.[17] propose using the mean Shannon's Entropy of target-domain predictions as a validation metric, prioritizing predictions with high certainty. The underlying intuition is that a good decision boundary should avoid crossing high-density regions in the target structure [74; 75]. Lower Entropy scores indicate better model performance for this metric.

\[\text{Entropy}=-\frac{1}{n_{\mathrm{t}}}\sum_{i=1}^{n_{\mathrm{t}}}\sum_{j=1} ^{C}P_{ij}\log P_{ij}\]

Information maximization.The Entropy score only considers sample-wise certainty, which can be misleading when high-certainty predictions are biased towards a small fraction of classes [19]. To address this challenge, [18] utilize input-output mutual information maximization (InfoMax) [39] as a validation metric. In contrast to Entropy, InfoMax includes an additional class-balance regularization by encouraging the averaged prediction \(\bar{p}=\frac{1}{n_{\mathrm{t}}}\sum_{i=1}^{n_{\mathrm{t}}}P_{ij},\quad\bar{p }\in\mathbb{R}^{C}\) to be even. Higher InfoMaxscores indicate better model performance according to this metric.

\[\text{InfoMax}=-\sum_{j=1}^{C}\bar{p}\log\bar{p}+\frac{1}{n_{\text{t}}}\sum_{i=1} ^{n_{\text{t}}}\sum_{j=1}^{C}P_{ij}\log P_{ij}\]

Neighborhood consistency.[19] introduce Soft Neighborhood Density (SND), a novel metric that focuses on the property of neighborhood consistency. SND leverages softmax predictions as features and constructs a sample-to-sample similarity matrix. This matrix is transformed into a probabilistic distribution using the softmax function: \(S=\text{softmax}(PP^{T}/\tau),\quad S\in\mathbb{R}^{n_{\text{t}}\times n_{\text {t}}}\). Here, \(\tau\) is a small temperature parameter that sharpens the distribution, enabling the difference between nearby and distant samples. SND favors high neighborhood consistency by prioritizing samples whose predictions are similar to other samples within the same neighborhood, resulting in higher SND scores.

\[\text{SND}=-\frac{1}{n_{\text{t}}}\sum_{i=1}^{n_{\text{t}}}\sum_{j=1}^{n_{ \text{t}}}S_{ij}\log S_{ij}\]

Class correlation.[20] introduce Corr-C, a class correlation-based metric that evaluates both class diversity and prediction certainty. Corr-C calculates the cosine similarity between the class correlation matrix and an identity matrix. Lower Corr-C scores are indicative of better model performance based on this metric.

\[\text{Corr-C}=\frac{\text{sum}(\text{diag}(P^{T}P))}{\|P^{T}P\|_{\text{F}}}\]

We can generally classify model selection baselines into two categories: source domain-based methods, including SourceRisk, IWCV, DEV, and RV, and target domain-specific methods, encompassing Entropy, InfoMax, SND, and Corr-C. Recent model selection studies [19, 18, 20] predominantly align with the target domain-specific approach. This trend arises because access to source data restricts UDA to closed-set UDA and often involves additional model training, making the validation process even more complex than UDA model training. In contrast, target domain-specific methods are more straightforward and effective [19]. EnsV, our proposed method, also falls within the category of target domain-specific methods, but fortunately with enhanced reliability due to a theoretical guarantee designed to avert worst-case model selection scenarios.

## Appendix C Hyperparameter Configurations

In our main experiments, we adopt the setting of previous studies [15, 19] by tuning a single hyperparameter for various UDA methods. The comprehensive hyperparameter settings can be found in Table 13.

## Appendix D Full Model Selection Results

For a comprehensive study, we further consider the parameter weight-based ensemble [53] as our role model, and the EnsV variant based on this role model is denoted as 'EnsV-W'. While the parameter weight-based ensemble also shows competitiveness, it requires all candidate models to share the same architecture and lacks a theoretical guarantee of the ensemble performance. Thus, we recommend the simple and generic prediction-based ensemble, i.e., the default 'EnsV'.

In our experiments, we perform hyperparameter selection for both classification and segmentation tasks. For open-partial-set UDA experiments, we utilize the H-score (\(\%\)) [69, 70] metric, which combines the accuracy of known classes and unknown samples. For semantic segmentation tasks, we employ the mean intersection-over-union (mIoU) (\(\%\)) [25, 26] metric. As for other classification tasks, we adopt the accuracy (\(\%\)) metric. Kindly refer to Table 14 to Table 29 for the complete model selection results.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \hline Method & \(\mathrm{Ar}\rightarrow\mathrm{Cl}\) & \(\mathrm{Ar}\rightarrow\mathrm{Pr}\) & \(\mathrm{Ar}\rightarrow\mathrm{Re}\) & \(\mathrm{Cl}\rightarrow\mathrm{Ar}\) & \(\mathrm{Cl}\rightarrow\mathrm{Pr}\) & \(\mathrm{Cl}\rightarrow\mathrm{Re}\) & \(\mathrm{Pr}\rightarrow\mathrm{Ar}\) & \(\mathrm{Pr}\rightarrow\mathrm{Cl}\) & \(\mathrm{Pr}\rightarrow\mathrm{Re}\) & \(\mathrm{Re}\rightarrow\mathrm{Ar}\) & \(\mathrm{Re}\rightarrow\mathrm{Cl}\) & \(\mathrm{Re}\rightarrow\mathrm{Pr}\) & aug \\ \hline SourceStrix [4] & 54.14 & **77.31** & 78.17 & **66.87** & 74.36 & 57.00 & 61.38 & 48.04 & 76.06 & 71.16 & 58.14 & **60.55** & 68.59 \\ HVCV [14] & 55.85 & 76.57 & 78.88 & 66.25 & 74.50 & 78.33 & 65.60 & 48.04 & 80.58 & **72.06** & 58.14 & 83.37 & 69.89 \\ DVE [15] & 54.14 & 76.55 & 78.88 & 66.25 & 74.36 & 77.67 & 64.77 & 51.29 & 81.62 & 71.16 & **59.98** & 28.43 & 69.70 \\ FVC [16] & 54.86 & 76.12 & 50.01 & 66.25 & 78.80 & 78.83 & 67.82 & 55.62 & 50.58 & 71.78 & 58.40 & 83.37 & 70.85 \\ \hline Entropy [17] & 55.88 & 74.14 & 78.88 & 59.25 & 77.42 & 77.67 & 64.19 & 54.39 & 78.54 & 67.37 & 57.23 & 80.96 & 68.69 \\ InfIndAs [15] & 57.85 & 77.16 & 72.90 & 57.64 & 76.50 & 75.46 & 69.14 & 54.39 & 78.54 & 67.57 & 56.61 & 80.96 & 68.82 \\ WFI [16] & **58.67** & 77.44 & 78.89 & 59.25 & 74.52 & 75.21 & 64.19 & 54.39 & 78.54 & 67.57 & 56.61 & 80.96 & 68.34 \\ Emery [17] & 53.40 & 67.04 & 59.73 & 69.56 & 69.54 & 61.85 & 48.04 & 76.06 & 69.30 & 57.11 & 80.31 & 65.42 \\ Enx-W & **57.85** & 76.57 & **81.04** & 66.25 & **79.48** & **78.52** & **67.94** & 55.62 & **82.17** & 71.9 & 59.24 & 84.03 & 71.72 \\ EnxV & **57.85** & 76.37 & 80.54 & 66.25 & 78.52 & **75.82** & **75.94** & **57.07** & **82.17** & 71.9 & 59.24 & 84.03 & **71.74** \\ \hline Worst & 51.41 & 72.00 & 76.04 & 59.25 & 69.36 & 67.54 & 61.85 & 48.04 & 76.06 & 67.57 & 51.71 & 80.31 & 65.26 \\ Best & 38.01 & 77.31 & 81.04 & 66.91 & 79.48 & 78.52 & 67.94 & 57.07 & 82.17 & 72.06 & 59.98 & 84.03 & 72.04 \\ \hline \hline \end{tabular}
\end{table}
Table 16: Validation accuracy (\(\%\)) of a closed-set UDA method CDAN [6] on _Office-Home_.

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline UDA method & UDA type & Hyperparameter & Search space & Default value \\ \hline ATDOC [35] & CDA & loss coefficient & \(\{0.02,0.05,0.1,\) \\  & self-training} & \(\lambda\) & \(0.2,0.5,1.0,2.0\) \\ \hline BNM [8] & CDA & loss coefficient & \(\{0.02,0.05,0.1,\) \\  & output regularization} & \(\lambda\) & \(0.2,0.5,1.0,2.0\) \\ \hline CDAN [6] & CDA & loss coefficient & \(\{0.05,0.1,0.2,\) \\  & feature alignment} & \(\lambda\) & \(0.5,1.0,2.0,\) \\ \hline MCC [36] & CDA & temperature & \(\{1.0,1.5,2.0,\) \\  & output regularization} & \(T\) & \(2.5,3.0,3.5,4.0\) \\ \hline MDD [33] & CDA & margin factor & \(\{0.5,1.0,2.0,\) \\  & output alignment} & \(\gamma\) & \(3.0,4.0,5.0,6.0\) \\ \hline SAFN [7] & CDA/PDA & loss coefficient & \(\{0.002,0.005,0.01,\) \\  & feature regularization} & \(\lambda\) & \(0.02,0.05,0.1,\) \\ \hline PADA [10] & PDA & loss coefficient & \(\{0.05,0.1,0.2,\) \\  & feature alignment} & \(\lambda\) & \(0.5,1.0,2.0,\) \\ \hline DANCE [11] & OPDA & loss coefficient & \(\{0.02,0.05,0.1,\) \\  & self-supervision} & \(\eta\) & \(0.2,0.5,1.0,2.0\) \\ \hline SHOT [12] & white-box SFUDA & loss coefficient & \(\{0.03,0.05,0.1,\) \\  & hypothesis transfer} & \(\beta\) & \(0.3,0.5,1.0,3.0\) \\ \hline DINE [35] & black-box SFUDA & loss coefficient & \(\{0.05,0.1,\) \\  & knowledge distillation} & \(\beta\) & \(0.5,1.0,2.0,\) \\ \hline AdaptSeg [25] & segmentation & loss coefficient & \(\{0.001,0.003,0.001,\) \\  & output alignment} & \(\lambda\) & \(0.003,0.01,0.03\) \\ \hline AdvEnt [26] & segmentation & loss coefficient & \(\{0.0001,0.0003,0.001,\) \\  & output alignment} & \(\lambda\) & \(0.003,0.01,0.03\) \\ \hline AdvEnt [26] & segmentation & loss coefficient & \(\{0.0001,0.0003,0.001,\) \\  & output alignment} & \(\lambda\) & \(0.003,0.01,0.03\) \\ \hline \hline \end{tabular}
\end{table}
Table 14: Validation accuracy (\(\%\)) of a closed-set UDA method ATDOC [35] on _Office-Home_.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \hline Method & \(\mathrm{Ar}\rightarrow\mathrm{Cl}\) & \(\mathrm{Ar}\rightarrow\mathrm{Pr}\) & \(\mathrm{Ar}\rightarrow\mathrm{Re}\) & \(\mathrm{Cl}\rightarrow\mathrm{Ar}\) & \(\mathrm{Cl}\rightarrow\mathrm{Pr}\) & \(\mathrm{Cl}\rightarrow\mathrm{Re}\) & \(\mathrm{Pr}\rightarrow\mathrm{Ar}\) & \(\mathrm{Pr}\rightarrow\mathrm{Cl}\) & \(\mathrm{Pr}\rightarrow\mathrm{Re}\) & \(\mathrm{Re}\rightarrow\mathrm{Ar}\)

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c c} \hline \hline Method & \(\mathrm{Ar\to Cl}\) & \(\mathrm{Ar\to Pr}\) & \(\mathrm{Ar\to Re}\) & \(\mathrm{Cl\to Ar}\) & \(\mathrm{Cl\to Pr}\) & \(\mathrm{Cl\to Re}\) & \(\mathrm{Pr\to Ar}\) & \(\mathrm{Pr\to Ct}\) & \(\mathrm{Pr\to Re}\) & \(\mathrm{Re\to Ar}\) & \(\mathrm{Re\to Cl}\) & \(\mathrm{Re\to Pr}\) & \(\mathrm{arg\) \\ \hline SourceRisk [9] & 54.85 & 73.35 & 77.05 & 58.76 & 69.95 & 72.23 & 60.03 & 51.02 & 77.36 & 68.81 & 57.42 & 82.80 & 66.94 \\ HVC [14] & 56.40 & 69.52 & 76.59 & 58.76 & 64.40 & 69.43 & 61.89 & 56.43 & 76.82 & 19.94 & 56.68 & 84.43 & 67.39 \\ DEV [15] & 57.71 & **75.42** & 77.05 & 58.76 & **73.99** & 70.51 & **63.95** & 56.43 & 80.26 & 70.54 & 56.68 & 82.14 & 68.54 \\ RX [16] & **58.05** & **75.42** & 76.59 & 63.54 & 69.95 & **73.34** & **63.95** & 51.02 & **80.38** & **72.23** & **58.17** & **84.43** & 68.96 \\ \hline Entropy [17] & 57.73 & 74.54 & **78.22** & **64.07** & **72.99** & **73.34** & **63.95** & 55.85 & **80.38** & 71.61 & 59.31 & 84.28 & 69.72 \\ InfoAs [18] & **50.86** & **74.54** & **78.22** & **64.07** & **72.99** & **73.34** & **63.95** & 55.85 & **80.38** & 71.61 & 59.31 & 84.28 & 69.72 \\ SND [19] & **50.85** & **75.24** & 77.05 & 41.99 & **72.59** & **48.06** & 37.08 & 21.60 & 80.26 & 71.94 & 34.39 & **84.25** & 53.06 \\ Conv-Cl [20] & 39.08 & 59.47 & 69.61 & 44.99 & 54.58 & 48.06 & 37.08 & 21.60 & 64.22 & 61.31 & 34.39 & 75.57 & 50.08 \\ Env-W & 54.89 & **75.42** & 78.01 & 61.98 & **72.99** & 72.23 & 63.08 & 56.43 & 79.66 & **72.33** & **60.02** & **83.56** & 69.23 \\ \hline Entropy [17] & 45.93 & 69.72 & 75.49 & 55.29 & 62.83 & 67.22 & 68.53 & 54.26 & 43.30 & 75.69 & 70.00 & 49.99 & 80.62 & 62.99 \\ InfoAs [18] & 50.47 & 69.72 & 75.49 & 62.46 & **79.98** & 63.65 & 61.23 & 43.30 & 75.69 & 70.00 & 55.37 & 80.60 & 65.31 \\ SND [19] & 45.93 & 64.36 & 70.60 & 55.29 & 60.13 & 62.50 & 54.26 & 43.30 & 71.43 & 61.15 & 49.99 & 76.64 & 61.57 \\ Env-W & **51.23** & 72.07 & **76.61** & **64.46** & **70.98** & 71.26 & **63.66** & **50.22** & 77.48 & 70.99 & **57.14** & **81.46** & **67.28** \\ Env-W & 51.07 & **72.27** & **77.30** & 63.58 & 70.29 & **73.10** & 62.71 & 49.69 & **77.71** & **71.45** & 55.78 & 30.96 & 67.01 \\ \hline Weiet & 45.93 & 64.36 & 70.60 & 55.29 & 60.13 & 63.20 & 54.26 & 43.30 & 77.43 & 61.45 & 49.99 & 76.64 & 59.38 \\ Best & 51.73 & 72.27 & 77.30 & 64.65 & 70.98 & 71.70 & 63.66 & 50.52 & 77.71 & 71.45 & 57.16 & 81.46 & 67.55 \\ \hline \hline \end{tabular}
\end{table}
Table 19: Validation accuracy (\(\%\)) of a closed-set UDA method SARN [7] on _Office-Home_.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c} \hline \hline Method & \(\mathrm{Ar\to Cl}\) & \(\mathrm{Ar\to Pr}\) & \(\mathrm{Ar\to Re}\) & \(\mathrm{Cl\to Ar}\) & \(\mathrm{Cl\to Pr}\) & \(\mathrm{Cl\to Re}\) & \(\mathrm{Pr\to Ar}\) & \(\mathrm{Pr\to Ct}\) & \(\mathrm{Pr\to Re}\) & \(\mathrm{Re\to Ar}\) & \(\mathrm{Re\to Cl}\) & \(\mathrm{Re\to Pr}\) & \(\mathrm{arg\) \\ \hline SourceRisk [9] & 54.85 & 73.35 & 77.05 & 58.76 & 69.95 & 72.23 & 60.03 & 51.02 & 77.36 & 68.81 & 57.42 & 82.50 & 66.94 \\ HVC [14] & 56.0 & 69.52 & 76.59 & 58.76 & 64.40 & 69.43 & 61.89 & 56.43 & 76.82 & 19.14 & 56.68 & **84.43** & 67.39 \\ DEV [15] & 57.71 & **75.42** & 77.05 & 58.76 & **71.99** & 70.51 & **63.95** & 56.43 & 80.26 & 70.54 & 56.68 & 82.14 & 68.54 \\ RX [16] & **58.05** & **75.42** & 76.59 & 63.54 & 69.95 & **73.34** & **63.95** & 51.02 & **80.38** & **72.23** & 58.17 & **84.43** & 68.96 \\ \hline Entropy [17] & 57.73 & 74.54 & **78.22** & **64.07** & **72.99** & **73.34** & **63.95** & 55.85 & **80.38** & 71.61 & 59.31 & 84.28 & 69.72 \\ InfoAs [18] & **50.85** & **75.42** & 77.05 & 44.99 & **72.59** & **48.06** & 37.08 & 21.60 & 80.26 & 71.94 & 34.39 & **84.25** & 53.06 \\ Conv-Cl [20] & 39.08 & 59.47 & 69.61 & 44.99 & 54.58 & 48.06 & 37.08

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c} \hline \hline Method & \(\mathrm{C}\rightarrow\mathrm{S}\) & \(\mathrm{P}\rightarrow\mathrm{C}\) & \(\mathrm{P}\rightarrow\mathrm{R}\) & \(\mathrm{R}\rightarrow\mathrm{C}\) & \(\mathrm{R}\rightarrow\mathrm{P}\) & \(\mathrm{R}\rightarrow\mathrm{S}\) & \(\mathrm{S}\rightarrow\mathrm{P}\) & avg \\ \hline Entropy [17] & 54.62 & 61.57 & 74.31 & 65.15 & 65.15 & 65.15 & 54.93 & 54.22 & 61.00 \\ IndMax [18] & 56.42 & 68.95 & 74.31 & 65.15 & 65.15 & 54.93 & 54.22 & 64.59 & 54.59 & 54.45 \\ SND [19] & 43.78 & 61.57 & 74.31 & 51.55 & 54.40 & 40.95 & 54.59 & 54.55 & 55.38 \\ ConT-C [20] & 43.78 & 60.03 & **77.62** & 60.65 & 61.52 & 64.24 & 47.58 & 59.46 & 60.69 \\ EnsV-W & **58.48** & 65.42 & **77.62** & 66.05 & 67.93 & 64.58 & 52.66 & 59.95 & 69.09 \\ EnsV & 57.73 & **69.63** & **77.62** & **66.06** & **61.79** & **57.75** & **67.65** & **64.34** & **65.84** & **66.37** & **66.38** \\ \hline Worst & 43.78 & 60.03 & 74.31 & 51.55 & 54.40 & 40.95 & 54.39 & 54.39 & 54.32 \\ Best & 58.48 & 69.63 & 78.68 & 66.10 & 67.79 & 58.50 & 66.20 & 66.34 & \\ \hline \hline \end{tabular}
\end{table}
Table 24: Validation accuracy (\(\%\)) of a of a closed-set UDA method PADO [35] on _DomainNet-126_.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c} \hline \hline Method & \(\mathrm{C}\rightarrow\mathrm{S}\) & \(\mathrm{P}\rightarrow\mathrm{C}\) & \(\mathrm{P}\rightarrow\mathrm{R}\) & \(\mathrm{R}\rightarrow\mathrm{C}\) & \(\mathrm{R}\rightarrow\mathrm{P}\) & \(\mathrm{R}\rightarrow\mathrm{S}\) & \(\mathrm{S}\rightarrow\mathrm{P}\) & avg \\ \hline Entropy [17] & 56.42 & 61.57 & 74.31 & 65.15 & 65.15 & 40.55 & 63.42 & 61.00 \\ IndMax [18] & 56.42 & 68.95 & 74.31 & 65.15 & 65.15 & 54.93 & 54.32 & 64.05 \\ SND [19] & 43.78 & 61.57 & 74.31 & 51.55 & 54.40 & 40.95 & 54.59 & 54.45 \\ CorT-C [20] & 43.78 & 60.03 & **77.62** & 59.47 & 67.19 & 40.55 & 59.45 & 58.38 \\ EnsV-W & **58.48** & 68.42 & **77.62** & 66.05 & 67.97 & **57.56** & **64.34** & 65.76 \\ EnsV & 57.73 & **69.63** & **77.62** & **66.06** & **67.97** & **57.65** & **64.34** & **65.84** \\ \hline Worst & 43.78 & 60.03 & 74.31 & 51.55 & 54.40 & 40.95 & 54.39 & 54.39 & 54.32 \\ Best & 58.48 & 69.63 & 78.68 & 66.10 & 67.79 & 58.50 & 65.20 & 66.34 \\ \hline \hline \end{tabular}
\end{table}
Table 23: Validation accuracy (\(\%\)) of a closed-set UDA method ATDOC [35] on _DomainNet-126_.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c} \hline \hline Method & \(\mathrm{C}\rightarrow\mathrm{S}\) & \(\mathrm{P}\rightarrow\mathrm{C}\) & \(\mathrm{P}\rightarrow\mathrm{R}\) & \(\mathrm{R}\rightarrow\mathrm{C}\) & \(\mathrm{R}\rightarrow\mathrm{P}\) & \(\mathrm{R}\rightarrow\mathrm{S}\) & \(\mathrm{S}\rightarrow\mathrm{P}\) & avg \\ \hline Entropy [17] & 56.42 & 61.57 & 74.31 & 65.15 & 65.15 & 65.15 & 40.95 & 63.42 & 61.00 \\ IndMax [18] & 56.42 & 68.95 & 74.31 & 65.15 & 65.15 & 54.93 & 54.32 & 64.05 \\ SND [19] & 43.78 & 61.57 & 74.31 & 51.55 & 54.40 & 40.95 & 54.59 & 54.45 \\ CorT-C [20] & 43.78 & 60.03 & **77.62** & 59.47 & 67.19 & 40.55 & 59.45 & 58.38 \\ EnsV-W & **58.48** & 68.42 & **77.62** & 66.05 & 67.97 & **57.56** & **64.34** & 65.76 \\ EnsV & 57.73 & **69.63** & **77.62** & **66.06** & **67.97** & **57.56** & **64.34** & **65.84** \\ \hline Worst & 43.78 & 60.03 & 74.31 & 51.55 & 54.40 & 40.95 & 54.39 & 54.32 \\ Best & 58.48 & 69.63 & 78.68 & 66.10 & 67.79 & 58.50 & 66.20 & 66.34 \\ \hline \hline \end{tabular}
\end{table}
Table 24: Validation accuracy (\(\%\)) of a closed-set UDA method ATDOC [35] on _DomainNet-126_.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c} \hline \hline Method & \(\mathrm{C}\rightarrow\mathrm{S}\) & \(\mathrm{P}\rightarrow\mathrm{C}\) & \(\mathrm{P}\rightarrow\mathrm{R}\) & \(\mathrm{R}\rightarrow\mathrm{C}\) & \(\mathrm{R}\rightarrow\mathrm{P}\) & \(\mathrm{R}\rightarrow\mathrm{S}\) & \(\mathrm{S}\rightarrow\mathrm{P}\) & avg \\ \hline Entropy [17] & 56.42 & 61.57 & 74.31 & 65.15 & 65.15 & 40.95 & 63.42 & 61.00 \\ IndMax [18] & 56.42 & 68.95 & 74.31 & 65.15 & 65.15 & 54.93 & 54.32 & 64.05 \\ SND [19] & 43.78 & 61.57 & 74.31 & 51.55 & 54.40 & 40.95 & 54.59 & 54.45 \\ CorT-C [20] & 43.78 & 60.03 & **77.62** & 59.47 & 67.19 & 40.55 & 59.45 & 58.38 \\ EnsV-W & **58.48** & 68.42 & **77.62** & 66.05 & 67.97 & **57.56** & **64.34** & 65.76 \\ EnsV & 57.73 & **69.63** & **77.62** & **66.06** & **67.97** & **57.56** & **

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c} \hline \hline Method & A \(\rightarrow\) C & Ar \(\rightarrow\) Pr & Ar \(\rightarrow\) Re & Cl \(\rightarrow\) Ar & Cl \(\rightarrow\) Pr & Cl \(\rightarrow\) Re & Pr \(\rightarrow\) Ar & Pr \(\rightarrow\) Cl & Pr \(\rightarrow\) Re & Re \(\rightarrow\) Ar & Re \(\rightarrow\) Cl & Re \(\rightarrow\) Pr & avg \\ \hline Entropy [17] & 49.14 & 76.17 & 79.23 & 60.57 & 73.94 & 74.00 & 60.69 & 48.66 & 79.73 & 68.89 & 53.56 & 81.93 & 67.21 \\ InfMax [18] & 49.14 & 76.17 & 79.23 & 60.57 & 73.94 & 74.00 & 60.69 & 48.66 & 79.73 & 68.89 & 53.56 & 81.93 & 67.21 \\ SND [19] & 49.14 & 76.17 & 79.23 & 60.57 & 76.59 & 74.00 & 64.28 & **54.55** & 79.73 & 68.89 & 55.81 & 81.93 & 68.66 \\ Corr-C [20] & 55.60 & 76.66 & 79.83 & 67.04 & 76.59 & 76.86 & 66.63 & **54.55** & 89.74 & **73.71** & 58.81 & **84.61** & 70.97 \\ EnsV-W & **56.36** & **77.81** & **81.36** & **68.27** & **78.78** & **78.91** & 63.80 & 54.52 & **82.01** & 73.01 & **59.45** & **84.61** & 71.74 \\ EnsV & **56.36** & **77.81** & **81.36** & **68.27** & **78.78** & **78.91** & **67.12** & 54.52 & **82.01** & 73.34 & **59.45** & **84.61** & **71.88** \\ \hline Werst & 49.14 & 76.17 & 79.25 & 60.57 & 73.94 & 74.00 & 60.69 & 48.66 & 79.73 & 68.89 & 53.56 & 81.93 & 67.21 \\ Best & 56.36 & 77.95 & 81.36 & 68.27 & 79.05 & 78.91 & 67.33 & 55.33 & 82.01 & 73.88 & 59.54 & 84.66 & 72.05 \\ \hline \hline \end{tabular}
\end{table}
Table 28: Validation accuracy (\(\%\)) of a white-box source-free UDA method SHOT [12] on _Office-Home_.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Method & A \(\rightarrow\) D & A \(\rightarrow\) W & D \(\rightarrow\) A & W \(\rightarrow\) A & avg \\ \hline Entropy [17] & 90.76 & 88.68 & 71.21 & 72.13 & 80.69 \\ InfoMax [18] & 90.76 & 88.68 & 71.21 & 72.13 & 80.69 \\ SND [19] & 90.76 & 88.68 & 71.21 & 72.13 & 80.69 \\ Corr-C [20] & 90.76 & 90.19 & 71.21 & 71.96 & 81.03 \\ EnsV-W & **94.78** & **91.82** & **75.15** & **74.55** & **84.08** \\ EnsV & **94.78** & **91.82** & **75.15** & **74.55** & **84.08** \\ \hline Worst & 90.76 & 88.68 & 71.21 & 71.92 & 80.64 \\ Best & 94.78 & 93.33 & 75.58 & 74.55 & 84.56 \\ \hline \hline \end{tabular}
\end{table}
Table 29: Validation accuracy (\(\%\)) of a white-box source-free UDA method SHOT [12] on _Office-31_.