# Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks

Andong Wang

RIKEN AIP

andong.wang@riken.jp

&Chao Li

RIKEN AIP

chao.li@riken.jp

&Mingyuan Bai

RIKEN AIP

mingyuan.bai@riken.jp

&Zhong Jin

China University of Petroleum-Beijing at Karamay

zhongjin@cupk.edu.cn

&Guoxu Zhou

Guangdong University of Technology

gx.zhou@gdut.edu.cn

&Qibin Zhao

RIKEN AIP

qibin.zhao@riken.jp

Qibin Zhao and Guoxu Zhou are the corresponding authors.

###### Abstract

Multi-channel learning has gained significant attention in recent applications, where neural networks with t-product layers (t-NNs) have shown promising performance through novel feature mapping in the transformed domain. However, despite the practical success of t-NNs, the theoretical analysis of their generalization remains unexplored. We address this gap by deriving upper bounds on the generalization error of t-NNs in both standard and adversarial settings. Notably, it reveals that t-NNs compressed with exact transformed low-rank parameterization can achieve tighter adversarial generalization bounds compared to non-compressed models. While exact transformed low-rank weights are rare in practice, the analysis demonstrates that through adversarial training with gradient flow, highly over-parameterized t-NNs with the ReLU activation can be implicitly regularized towards a transformed low-rank parameterization under certain conditions. Moreover, this paper establishes sharp adversarial generalization bounds for t-NNs with approximately transformed low-rank weights. Our analysis highlights the potential of transformed low-rank parameterization in enhancing the robust generalization of t-NNs, offering valuable insights for further research and development.

## 1 Introduction

Multi-channel learning is a task to extract representations from the data with multiple channels, such as multispectral images, time series, and multi-view videos, in an efficient and robust manner [24; 39; 60; 61]. Among the methods tackling this task, neural networks with t-product layers (t-NNs, see Eq. (5) for a typical example) [12; 36] came to the stage very recently with remarkable efficiency and robustness in various applications such as graph learning, remote sensing, and more [1; 11; 14; 32; 38; 39; 53; 58]. What distinguishes t-NNs from other networks is the inclusion of t-product layers, founded on the algebraic framework of tensor singular value decomposition (t-SVD) [19; 40; 60; 61]. Unlike traditional tensor decompositions, t-SVD explores the transformed low-rankness, i.e., the low-rank structure of a tensor in the transformed domain under an invertible linear transform [18]. The imposed transform in t-product layers provides additional expressivity to neural networks, whilethe controllable transformed low-rank structure in t-NNs enables a flexible balance between model accuracy and robustness [36; 38; 53].

Despite the impressive empirical performance of t-NNs, the theoretical foundations behind their success remain unclear. The lack of systematic theoretical analysis hinders deeper comprehension and exploration of more effective applications and robust performance of t-NNs. Furthermore, the inclusion of the additional transform in t-NNs renders the theoretical analysis more technically challenging compared to existing work on general neural networks [29; 37; 43; 55]. To address this challenge, we establish for the first time a theoretical framework for t-NNs to understand both the standard and robust generalization behaviors, providing both theoretical insights and practical guidance for the efficient and robust utilization of t-NNs. Specifically, we address the following fundamental questions:

* _Can we theoretically characterize the generalization behavior of general t-NNs?_ Yes. We derive the upper bounds on the generalization error for _general_ t-NNs in both standard and adversarial settings in Sec. 3.
* _How does exact transformed low-rankness influence the robust generalization of t-NNs?_ In Sec. 4.1, our analysis shows that t-NNs with _exactly_ transformed low-rank weights exhibit lower adversarial generalization bounds and require fewer samples, highlighting the benefits of transformed low-rank weights in t-NNs for improved robustness and efficiency.
* _How does adversarial learning of t-NNs affect the transformed ranks of their weight tensors?_ In Sec. 4.2, we deduce that weight tensors tend to be of transformed low-rankness _approximately_ for highly over-parameterized t-NNs with ReLU activation under adversarial training with gradient flow.
* _How is robust generalization impacted by approximately transformed low-rank weight tensors in t-NNs?_ In Sec. 4.3, we establish sharp adversarial generalization bounds for t-NNs with _approximately_ transformed low-rank weights by carefully bridging the gap with exact transformed low-rank parameterization. This finding again underscores the importance of incorporating transformed low-rank weights as a means to enhance the robustness of t-NNs.

## 2 Notations and Preliminaries

In this section, we introduce the notations and provide a concise overview of t-SVD and t-NNs, which play a central role in the subsequent analysis.

**Notations.** We use lowercase, lowercase boldface, and uppercase boldface letters to denote scalars, _e.g._, \(a\in\mathbb{R}\), vectors, _e.g._, \(\mathbf{a}\in\mathbb{R}^{m}\), and matrices, _e.g._, \(\mathbf{A}\in\mathbb{R}^{m\times n}\), respectively. Following the standard notations in Ref. [19], a 3-way tensor of size \(d\times 1\times\mathsf{c}\) is also called a _t-vector_ and denoted by underlined lowercase, _e.g._, \(\underline{\mathbf{x}}\), whereas a 3-way tensor of size \(m\times n\times\mathsf{c}\) is also called a _t-matrix_ and denoted by underlined uppercase, _e.g._, \(\underline{\mathbf{X}}\). We use a t-vector \(\underline{\mathbf{x}}\in\mathbb{R}^{d\times 1\times\mathsf{c}}\) to represent a multi-channel example, where \(\mathsf{c}\) denotes the number of channels and \(d\) is the number of features for each channel.

Given a matrix \(\mathbf{A}\in\mathbb{R}^{m\times n}\), its Frobenius norm (F-norm) and spectral norm are defined as \(\left\|\mathbf{A}\right\|_{\mathrm{F}}:=\sqrt{\sum_{i=1}^{\min\{m,n\}}\sigma_{ i}^{2}}\) and \(\left\|\mathbf{A}\right\|:=\max_{i}\sigma_{i}\), respectively, where \(\sigma_{i},\,i=1,\cdots,\min\{m,n\}\) are its singular values. The _stable rank_ of a non-zero matrix \(\mathbf{A}\) is defined as the squared ratio of its F-norm and spectral norm \(r_{\text{stb}}(\mathbf{A}):=\left\|\mathbf{A}\right\|_{\mathrm{F}}^{2}/ \left\|\mathbf{A}\right\|^{2}\). Given a tensor \(\underline{\mathbf{T}}\), define its \(l_{p}\)-norm and F-norm respectively as \(\left\|\underline{\mathbf{T}}\right\|_{l_{p}}:=\left\|\mathtt{vec}\left( \underline{\mathbf{T}}\right)\right\|_{l_{p}}\), and \(\left\|\underline{\mathbf{T}}\right\|_{\mathrm{F}}:=\left\|\mathtt{vec}\left( \underline{\mathbf{T}}\right)\right\|_{l_{2}}\), where \(\mathtt{vec}\left(\cdot\right)\) denotes the vectorization operation of a tensor [21]. Given \(\underline{\mathbf{T}}\in\mathbb{R}^{m\times n\times\mathsf{c}}\), let \(\underline{\mathbf{T}}_{::,i}\) denote its \(i^{\text{th}}\) frontal slice. The inner product between two tensors \(\mathbf{A},\underline{\mathbf{B}}\) is defined as \(\left\langle\mathbf{A},\mathbf{B}\right\rangle:=\mathtt{vec}\left(\mathbf{A} \right)^{\top}\mathtt{vec}\left(\mathbf{B}\right)\). The frontal-slice-wise product of two tensors \(\underline{\mathbf{A}},\underline{\mathbf{B}}\), denoted by \(\underline{\mathbf{A}}\odot\mathbf{B}\), equals a tensor \(\underline{\mathbf{T}}\) such that \(\underline{\mathbf{T}}_{::,i}=\underline{\mathbf{A}}_{::,:,i}\underline{ \mathbf{B}}_{::,:,i},\ i=1,\cdots,\mathsf{c}\)[19]. We use \(\left|\cdot\right|\) as the absolute value for a scalar and cardinality for a set. We use \(\circ\) to denote the function composition operation. Additional notations will be introduced upon their first occurrence.

### Tensor Singular Value Decomposition

The framework of tensor singular value decomposition (t-SVD) is based on the t-product under an invertible linear transform \(M\)[18]. In recent studies, the transformation matrix \(\mathbf{M}\) defining the transform \(M\) is _restricted to be orthogonal_[50] for better properties, which is also followed in this paper. Given any _orthogonal matrix_\(\mathbf{M}\in\mathbb{R}^{\mathsf{c}\times\mathsf{c}}\), define the associated linear transform \(M(\cdot)\) with its inverse \(M^{-1}(\cdot)\) on any \(\underline{\mathbf{T}}\in\mathbb{R}^{m\times n\times\mathsf{c}}\) as

\[M(\underline{\mathbf{T}}):=\underline{\mathbf{T}}\times_{3}\mathbf{M},\ \text{ and }\quad M^{-1}(\underline{\mathbf{T}}):=\underline{\mathbf{T}}\times_{3} \mathbf{M}^{-1},\] (1)

where \(\times_{3}\) denotes the tensor matrix product on mode-\(3\)[18].

**Definition 1** (t-product [18]).: _The t-product of any \(\underline{\mathbf{A}}\in\mathbb{R}^{m\times n\times\mathsf{c}}\) and \(\underline{\mathbf{B}}\in\mathbb{R}^{n\times k\times\mathsf{c}}\) under transform \(M\) in Eq. (1) is denoted and defined as \(\underline{\mathbf{A}}\ast_{M}\underline{\mathbf{B}}=\mathbf{C}\in\mathbb{R}^ {m\times k\times\mathsf{c}}\) such that \(M(\underline{\mathbf{C}})=M(\underline{\mathbf{A}})\odot M(\underline{ \mathbf{B}})\) in the transformed domain. Equivalently, we have \(\underline{\mathbf{C}}=M^{-1}(M(\underline{\mathbf{A}})\odot M(\underline{ \mathbf{B}}))\) in the original domain._

**Definition 2** (\(M\)-block-diagonal matrix).: _The \(M\)-block-diagonal matrix of any \(\underline{\mathbf{T}}\in\mathbb{R}^{m\times n\times\mathsf{c}}\), denoted by \(\widetilde{\mathbf{T}}_{M}\), is the block diagonal matrix whose diagonal blocks are the frontal slices of \(M(\underline{\mathbf{T}})\):_

\[\widetilde{\mathbf{T}}_{M}:=\texttt{bdag}(M(\underline{\mathbf{T}})):=\begin{bmatrix}M (\underline{\mathbf{T}})_{:,:,1}&&\\ &M(\underline{\mathbf{T}})_{:,:,2}&&\\ &&\ddots&\\ &&&M(\underline{\mathbf{T}})_{:,:,\mathsf{c}}\end{bmatrix}\in\mathbb{R}^{m \times n\mathsf{c}}.\]

In this paper, we also follow the definition of t-transpose, t-identity tensor, t-orthogonal tensor, and f-diagonal tensor given by Ref. [18], and thus the t-SVD is introduced as follows.

**Definition 3** (t-SVD, tubal rank [18]).: _Tensor Singular Value Decomposition (t-SVD) of \(\underline{\mathbf{T}}\in\mathbb{R}^{m\times n\times\mathsf{c}}\) under the invertible linear transform \(M\) in Eq. (1) is given as follows_

\[\underline{\mathbf{T}}=\underline{\mathbf{U}}\ast_{M}\underline{\mathbf{S}} \ast_{M}\underline{\mathbf{V}}^{\top},\] (2)

_where \(\underline{\mathbf{U}}\in\mathbb{R}^{m\times m\times\mathsf{c}}\) and \(\underline{\mathbf{V}}\in\mathbb{R}^{n\times n\times\mathsf{c}}\) are t-orthogonal, and \(\underline{\mathbf{S}}\in\mathbb{R}^{m\times n\times\mathsf{c}}\) is f-diagonal. The tubal rank of \(\underline{\mathbf{T}}\) is defined as the number of non-zero tubes of \(\underline{\mathbf{S}}\) in its t-SVD in Eq. (2), i.e., \(r_{\mathsf{i}}(\underline{\mathbf{T}}):=|\{i\,|\,\underline{\mathbf{S}}(i,i,:) \neq\bm{0},i\leq\min\{m,n\}\}|\)._

For any \(\underline{\mathbf{T}}\in\mathbb{R}^{m\times n\times\mathsf{c}}\) with the tubal rank \(r_{\mathsf{t}}(\underline{\mathbf{T}})\), we have following relationship between its t-SVD and the matrix SVD of its \(M\)-block-diagonal matrix [26; 50]:

\[\underline{\mathbf{T}}=\underline{\mathbf{U}}\ast_{M}\underline{\mathbf{S}} \ast_{M}\underline{\mathbf{V}}^{\top}\ \Leftrightarrow\ \widetilde{\mathbf{T}}_{M}=\widetilde{\mathbf{U}}_{M}\cdot\widetilde{\mathbf{S}} _{M}\cdot\widetilde{\mathbf{V}}_{M}^{\top},\quad\text{and}\qquad\mathsf{c} \cdot r_{\mathsf{t}}(\underline{\mathbf{T}})\geq\text{rank}(\widetilde{ \mathbf{T}}_{M}).\] (3)

As the \(M\)-block-diagonal matrix \(\widetilde{\mathbf{T}}_{M}\) is defined after transforming tensor \(\underline{\mathbf{T}}\) from the original domain to the transformed domain, the relationship \(\mathsf{c}\cdot r_{\mathsf{t}}(\underline{\mathbf{T}})\geq\text{rank}( \widetilde{\mathbf{T}}_{M})\) indicates that the tubal rank can be chosen as a measure of transformed low-rankness [26; 50].

### Neural Networks with t-Product Layer (t-NNs)

In this subsection, we will introduce the formulation of the t-product layer in t-NNs, which is designed for multi-channel feature learning.

**Multi-channel feature learning via t-product.** Suppose we have a multi-channel example represented by a t-vector \(\underline{\mathbf{x}}\in\mathbb{R}^{d\times 1\times\mathsf{c}}\), where \(\mathsf{c}\) is the number of channels and \(d\) is the number of features. We define an \(L\)-layer t-NN feature extractor \(\mathbf{f}(\underline{\mathbf{x}})\), to extract \(d_{L}\) features for each channel of \(\underline{\mathbf{x}}\):

\[\mathbf{f}(\underline{\mathbf{x}})=\mathbf{f}^{(L)}(\underline{\mathbf{x}}); \quad\mathbf{f}^{(l)}(\underline{\mathbf{x}})=\sigma(\underline{\mathbf{W}}^{ (l)}\ast_{M}\mathbf{f}^{(l-1)}(\underline{\mathbf{x}})),\ l=1,\cdots,L;\quad \mathbf{f}^{(0)}(\underline{\mathbf{x}})=\underline{\mathbf{x}},\] (4)

where the \(l\)-th layer \(\mathbf{f}^{(l)}\) first conducts t-product with weight tensor (t-matrix) \(\mathbf{W}^{(l)}\in\mathbb{R}^{d_{l}\times d_{l-1}\times\mathsf{c}}\) on the output of the \((l-1)\)-th layer as multi-channel features2\(\mathbf{f}^{(l-1)}(\underline{\mathbf{x}})\in\mathbb{R}^{d_{l-1}\times 1\times\mathsf{c}}\) to obtain a (\(d_{l}\times 1\times\mathsf{c}\))-dimensional representation and then uses the entry-wisely ReLU activation3\(\sigma(x)=\max\{x,0\}\) for nonlinearity.

Footnote 2: For simplicity, let \(d_{0}=d\) by treating the input example \(\underline{\mathbf{x}}\) as the \(0\)-th layer \(\mathbf{f}^{(0)}\).

Footnote 3: Although we consider ReLU activation in this paper, most of the main theoretical results (_e.g._, Theorems 3, 5, 6, 12, and 14) can be generalized to general Lipschitz activations with slight modifications in the proof.

**Remark**.: _Unlike Refs. [36; 32] and [53] whose nonlinear activation is performed in the transformed domain, the t-NN model in Eq. (4) considers the nonlinear activation in the original domain and hence is consistent with traditional neural networks._By adding a linear classification module with weight \(\mathbf{w}\in\mathbb{R}^{cd_{L}}\) after the feature exaction module in Eq. (4), we consider the following t-NN predictor whose sign can be utilized for binary classification:

\[f(\mathbf{x};\underline{\mathbf{W}}):=\mathbf{w}^{\top}{}_{\texttt{vec}}( \mathbf{f}^{L}(\mathbf{x})(\mathbf{x}))\in\mathbb{R}.\] (5)

Let \(\underline{\mathbf{W}}:=\left\{\underline{\mathbf{W}}^{(1)},\cdots, \underline{\mathbf{W}}^{(L)},\mathbf{w}\right\}\) be the collection of all the weights4. With a slight abuse of notation, let \(\|\underline{\mathbf{W}}\|_{\mathbf{F}}:=\sqrt{\|\mathbf{w}\|_{2}^{2}+\sum_{l =1}^{L}\|\underline{\mathbf{W}}^{(l)}\|_{\mathbf{F}}^{2}}\) denote the Euclidean norm of all the weights. The function class of general t-NNs whose weights are bounded in the Euclidean norm is defined as

Footnote 4: Here for the ease of notation presentation, we use the tensor notation \(\underline{\mathbf{W}}\) instead of the set notation \(\mathcal{W}\).

\[\mathfrak{F}:=\left\{f(\mathbf{x};\underline{\mathbf{W}})\ \ \big{|}\ \|\mathbf{w}\|_{2}\leq B_{w},\quad\| \underline{\mathbf{W}}^{(l)}\|_{\mathbf{F}}\leq B_{l},\quad l=1,\cdots,L \right\},\] (6)

with positive constants \(B_{w}\) and \(B_{l},\ l=1,\cdots,L\). Let \(B_{\underline{\mathbf{W}}}:=B_{w}\prod_{l=1}^{L}B_{l}\) for simplicity.

## 3 Standard and Robust Generalization Bounds for t-NNs

This section establishes both the standard and robust generalization bounds for any t-NN \(f\in\mathfrak{F}\).

### Standard Generalization for General t-NNs

Suppose we are given a training multi-channel dataset \(S\) consisting of \(N\) example-label pairs \(\{(\underline{\mathbf{x}}_{i},y_{i})\}_{i=1}^{N}\subset\mathbb{R}^{d\times 1 \times c}\times\{\pm 1\}\)_i.i.d._ drawn from an underlying data distribution \(P_{\underline{\mathbf{x}},y}\).

**Assumption 1**.: _Every input example \(\underline{\mathbf{x}}\in\mathbb{R}^{d\times 1\times c}\) has an upper bounded F-norm, i.e., \(\|\mathbf{x}\|_{\mathbf{F}}\leq B_{x}\), where \(B_{x}\) is a positive constant._

When a loss function \(\ell(f(\mathbf{x}_{i}),y_{i})\) is considered as the measure of the classification quality, we define the empirical and population risk for any \(f\in\mathfrak{F}\) as \(\hat{\mathcal{L}}(f):=N^{-1}\sum_{i=1}^{N}\ell(f(\underline{\mathbf{x}}_{i}),y _{i})\) and \(\mathcal{L}(f):=\mathbb{E}_{P_{(\mathbf{x},y)}}\left[\ell(f(\underline{\mathbf{ x}}),y)\right],\) respectively. Similar to Ref. [30], we make assumptions on the loss as follows.

**Assumption 2**.: _The loss \(\ell(h(\underline{\mathbf{x}}),y)\) can be expressed as \(\ell(h(\underline{\mathbf{x}}),y)=\exp(-\mathfrak{f}(yh(\underline{\mathbf{x} }))\) for any t-NN \(h\in\mathfrak{F}\), such that:_

1. _the range of loss_ \(\ell(\cdot,\cdot)\) _is_ \([0,B]\)_, where_ \(B\) _is a positive constant;_
2. _function_ \(\mathfrak{f}:\mathbb{R}\rightarrow\mathbb{R}\) _is_ \(C^{1}\)_-smooth;_
3. \(\mathfrak{f}^{\prime}(x)\geq 0\) _for any_ \(x\in\mathbb{R}\)_;_
4. _there exists_ \(b_{\mathfrak{f}}\geq 0\) _such that_ \(x\mathfrak{f}^{\prime}(x)\) _is non-decreasing for_ \(x\in(b_{\mathfrak{f}},+\infty)\)_, and the derivative_ \(x\mathfrak{f}^{\prime}(x)\rightarrow+\infty\) _as_ \(x\rightarrow+\infty\)_;_
5. _let_ \(\mathfrak{g}:[\mathfrak{f}(b_{\mathfrak{f}}),+\infty)\rightarrow[b_{\mathfrak{ f}},+\infty)\) _be the inverse function of_ \(\mathfrak{f}\) _on the domain_ \([b_{\mathfrak{f}},+\infty)\)_. There exist_ \(b_{\mathfrak{g}}\geq\max\{2\mathfrak{f}(b_{\mathfrak{f}}),\mathfrak{f}(2b_{ \mathfrak{f}})\}\) _and_ \(K\geq 1\)_, such that_ \(\mathfrak{g}^{\prime}(x)\leq K\mathfrak{g}^{\prime}(\theta x)\) _and_ \(\mathfrak{f}^{\prime}(y)\leq K\mathfrak{f}^{\prime}(\theta y)\) _for any_ \(x\in(b_{\mathfrak{g}},+\infty),y\in(\mathfrak{g}(b_{\mathfrak{g}}),+\infty)\) _and_ \(\theta\in[1/2,1)\)_._

Assumption _(A.1)_ is a natural assumption in generalization analysis [3, 59], and Assumptions _(A.2)_**-**(A.5) are the same as Assumption (B3) in Ref. [30]. According to Assumption _(A.2)_, the loss function \(\ell(\cdot,\cdot)\) satisfies the \(L_{\ell}\)-Lipschitz continuity

\[|\ell(h(\underline{\mathbf{x}}_{1}),y_{1})-\ell(h(\underline{\mathbf{x}}_{2}),y_{2})|\leq L_{\ell}|y_{1}h(\underline{\mathbf{x}}_{1})-y_{2}(\underline{ \mathbf{x}}_{2})|,\quad\text{with }L_{\ell}=\sup_{|q|\leq B_{\hat{f}}}\mathfrak{f}^{ \prime}(q)e^{-\mathfrak{f}(q)},\] (7)

where \(B_{\hat{f}}\) is an upper bound on the output of any t-NN \(h\in\mathfrak{F}\). The Lipschitz continuity is also widely assumed for generalization analysis of DNNs [55, 59]. Assumption 2 is satisfied by commonly used loss functions such as the logistic loss and the exponential loss.

The generalization gap \(\mathcal{L}(f)-\hat{\mathcal{L}}(f)\) of any function \(f\in\mathfrak{F}\) can be bounded as follows.

**Lemma 3** (Generalization bound for t-NNs).: _Under Assumptions 1 and 2, it holds for any \(f\in\mathfrak{F}\) that_

\[\mathcal{L}(f)-\hat{\mathcal{L}}(f)\leq\frac{L_{\ell}B_{x}B_{\mathbf{W}}}{ \sqrt{N}}(\sqrt{2\log(2(L+1))}+1)+3B\sqrt{\frac{t}{2N}},\] (8)

_with probability at least \(1-2e^{-t}\) for any \(t>0\)._

**Remark**.: _When the input example has channel number \(\mathsf{c}=1\), the generalization bound in Theorem 3 is consistent with the F-norm-based bound in Ref. [8]._

### Robust Generalization for General t-NNs

We study the adversarial generalization behavior of t-NNs in this section. We first make the following assumption on the adversarial perturbations.

**Assumption 4**.: _Given an input example \(\underline{\mathbf{x}}\), the adversarial perturbation is chosen within a radius-\(\xi\) ball of norm \(R_{\mathsf{a}}(\cdot)\) with compatibility constant [35] defined as \(\mathsf{C}_{R_{\mathsf{a}}}:=\sup_{\underline{\mathbf{y}}\neq\emptyset}R_{ \mathsf{a}}(\underline{\mathbf{x}})/\left\|\underline{\mathbf{x}}\right\|_{ \mathsf{F}}\)._

The assumption allows for much broader adversary classes than the commonly considered \(l_{p}\)-attacks [54, 55]. For example, if one treats the multi-channel data \(\underline{\mathbf{x}}\in\mathbb{R}^{d\times 1\times\mathsf{c}}\) as a matrix of dimensionality \(d\times\mathsf{c}\) and attacks it with nuclear norm attacks [17], then the constant \(\mathsf{C}_{R_{\mathsf{a}}}=\sqrt{\min\{d,\mathsf{c}\}}\).

Given an example-label pair \((\underline{\mathbf{x}},y)\), the adversarial loss for any predictor \(f\) is defined as \(\tilde{\ell}(f(\underline{\mathbf{x}}),y)=\max_{R_{\mathsf{a}}(\underline{ \mathbf{x}}^{\prime}-\underline{\mathbf{x}})\leq\xi}\tilde{\ell}(f( \underline{\mathbf{x}}^{\prime}),y)\). The empirical and population adversarial risks are thus defined as \(\hat{\mathcal{L}}^{\text{adv}}(f):=N^{-1}\sum_{i=1}^{N}\tilde{\ell}(f( \underline{\mathbf{x}}_{i}),y_{i})\) and \(\mathcal{L}^{\text{adv}}(f):=\mathbb{E}_{P_{(\mathbf{x},y)}}[\tilde{\ell}(f( \underline{\mathbf{x}}),y)]\), respectively. The adversarial generalization performance is measured by the adversarial generalization gap (AGP) defined as \(\mathcal{L}^{\text{adv}}(f)-\hat{\mathcal{L}}^{\text{adv}}(f)\). Let \(B_{\tilde{f}}:=(B_{x}+\xi\mathsf{C}_{R_{\mathsf{a}}})B_{\underline{\mathbf{ W}}}\). For any \(f\in\mathfrak{F}\), its AGP is bounded as follows.

**Theorem 5** (Adversarial generalization bound for t-NNs).: _Under Assumptions 1, 2, and 4, there exists a constant \(C\) such that for any \(f\in\mathfrak{F}\), it holds with probability at least \(1-2e^{-t}\)\((\forall t>0)\):_

\[\mathcal{L}^{\text{adv}}(f)-\hat{\mathcal{L}}^{\text{adv}}(f)\leq \frac{CL_{\tilde{f}}B_{\tilde{f}}}{\sqrt{N}}\sqrt{\mathsf{c}\sum_{ l=1}^{L}d_{l-1}d_{l}\log(3(L+1))}+3B\sqrt{\frac{t}{2N}}.\] (9)

**Remark**.: _When the input example has channel number \(\mathsf{c}=1\) and the attacker uses \(l_{p}\)-attack, the adversarial generalization bound in Theorem 5 degenerates to the one in Theorem 4 of Ref. [55]._

## 4 Transformed Low-rank Parameterization for Robust Generalization

### Robust Generalization with Exact Transformed Low-rank Parameterization

According to Theorem 5, the AGP bound scales with the square root of the parameter complexity, specifically as \(O(\sqrt{\mathsf{c}(\sum_{l}d_{l-1}d_{l})/N})\). This implies that achieving the desired adversarial accuracy may require a large number \(N\) of training examples. Furthermore, high parameter complexity leads to increased energy consumption, storage requirements, and computational cost when deploying large t-NN models, particularly on resource-constrained embedded and mobile devices.

To this end, we propose a transformed low-rank parameterization scheme to compress the original t-NN models \(\mathfrak{F}\). Specifically, given a vector of pre-set ranks \(\mathbf{r}=(r_{1},\cdots,r_{L})^{\top}\in\mathbb{R}^{L}\) where \(r_{l}\leq\min\{d_{l},d_{l-1}\}\), we consider the following subset of the original t-NNs:

\[\mathfrak{F}_{\mathbf{r}}:=\Big{\{}f\bigm{|}f\in\mathfrak{F},\text{and}\ r_{i}( \underline{\mathbf{W}}^{(l)})\leq r_{l},\ \ l=1,\cdots,L\Big{\}}.\] (10)

In the function set \(\mathfrak{F}_{\mathbf{r}}\), the weight tensor \(\underline{\mathbf{W}}^{(l)}\) of the \(l\)-th layer has the upper bounded tubal rank, which means low-rankness in the transformed domain5. We bound the AGP for any \(f\in\mathfrak{F}_{\mathbf{r}}\) as follows.

Footnote 5: For empirical implementations, one can adopt similar rank learning strategy to Ref. [15] to select a suitable rank parameter \(\mathbf{r}\). Due to the scope of this paper, we leave this for future work.

**Theorem 6** (Adversarial generalization bound for t-NNs with transformed low-rank weights).: _Under Assumptions 1, 2, and 4, there exists a constant \(C^{\prime}\) such that_

\[\mathcal{L}^{\text{adv}}(f_{\mathbf{r}})-\hat{\mathcal{L}}^{\text{adv}}(f_{ \mathbf{r}})\leq\frac{C^{\prime}L_{\ell}B_{\tilde{f}}}{\sqrt{N}}\sqrt{\mathsf{ c}\sum_{l=1}^{L}r_{l}(d_{l-1}+d_{l})\log(9(L+1))}+3B\sqrt{\frac{t}{2N}},\] (11)

_holds for any \(f_{\mathbf{r}}\in\mathfrak{F}_{\mathbf{r}}\) with probability at least \(1-2e^{-t}\)\((\forall t>0)\)._Comparing Theorem 6 with Theorem 5, we observe that the adversarial generalization bound under transformed low-rank parameterization has a better scaling, specifically \(O(\sqrt{c\sum_{t}r_{l}(d_{l-1}+d_{l})/N})\). This also implies that a smaller number \(N\) of training examples is required to achieve the desired accuracy, as well as reduced energy consumption, storage requirements, and computational cost. Please refer to Sec. A.1 in the appendix for numerical evidence.

### Implicit Bias of Gradient Flow for Adversarial Training of Over-parameterized t-NNs

Although Theorem 6 shows _exactly_ transformed low-rank parameterization leads to lower bounds, the well trained t-NNs on real data rarely have exactly transformed low-rank weights. In this section, we prove that the highly over-parameterized t-NNs, trained by adversarial training with gradient flow (GF), are _approximately_ of transformed low-rank parameterization under certain conditions.

First, the proposed t-NN \(f(\mathbf{x};\underline{\mathbf{W}})\) is said to be (positively) _homogeneous_ as the condition \(f(\underline{\mathbf{x}};a\underline{\mathbf{W}})=a^{L+1}f(\underline{\mathbf{ x}};\underline{\mathbf{W}})\) holds for any positive constant \(a\). Motivated by Ref. [29], we focus on the scale invariant adversarial perturbations defined as follows.

**Definition 4** (Scale invariant adversarial perturbation [29]).: _An adversarial perturbation \(\boldsymbol{\delta}_{i}(\underline{\mathbf{W}})\) is said to be scale invariant for \(f(\underline{\mathbf{x}};\underline{\mathbf{W}})\) at any given example \(\underline{\mathbf{x}}_{i}\) if it satisfies \(\boldsymbol{\delta}_{i}(a\underline{\mathbf{W}})=\boldsymbol{\delta}_{i}( \underline{\mathbf{W}})\) for any positive constant \(a\)._

**Lemma 7**.: _The \(l_{2}\)-FGM [34], FGSM [9], \(l_{2}\)-PGD and \(l_{\infty}\)-PGD [31] perturbations for the t-NNs are all scale invariant._

Then, we consider adversarial training of t-NNs with scale invariant adversarial perturbations by GF, which can be seen as gradient descent with infinitesimal step size. When using GF for the ReLU t-NNs, \(\underline{\mathbf{W}}\) changes continuously with time, and the trajectory of parameter \(\underline{\mathbf{W}}\) during training is an arc \(\underline{\mathbf{W}}:[0,\infty)\rightarrow\mathbb{R}^{\mathtt{dim}( \underline{\mathbf{W}})},t\mapsto\underline{\mathbf{W}}(t)\) that satisfies the differential inclusion [7; 30]

\[\frac{\mathsf{d}\mathbf{W}(t)}{\mathsf{d}t}\in-\partial^{\circ}\hat{\mathcal{ L}}^{\text{adv}}(\underline{\mathbf{W}}(t))\] (12)

for \(t\geq 0\)_a.e._, where \(\partial^{\circ}\hat{\mathcal{L}}^{\text{adv}}\) denotes the Clarke's subdifferential [7] with respect to \(\underline{\mathbf{W}}(t)\). If \(\hat{\mathcal{L}}^{\text{adv}}(\underline{\mathbf{W}})\) is actually a \(C^{1}\)-smooth function, the above differential inclusion reduces to

\[\frac{\mathsf{d}\mathbf{W}(t)}{\mathsf{d}t}=-\frac{\partial\hat{\mathcal{L}}^{ \text{adv}}(\mathbf{W}(t))}{\partial\mathbf{W}(t)}\] (13)

for any \(t\geq 0\), which corresponds to the GF with differential in the usual sense. However, for simplicity, we follow Refs. [45; 46] and still use Eq. (13) to denote Eq. (12) with a slight abuse of notation, even if \(\hat{\mathcal{L}}^{\text{adv}}\) does not satisfy differentiability but only local Lipschitzness 6.

Footnote 6: Note that the ReLU function is not differentiable at \(0\). Practical implementations of gradient methods define the derivative \(\sigma^{\prime}(0)\) to be a constant in \([0,1]\). In this work we assume for convenience that \(\sigma^{\prime}(0)=0\).

We also make an assumption on the training data as follows.

**Assumption 8** (Existence of a separability of adversarial examples during training).: _There exists a time \(t_{0}\) such that \(\hat{\mathcal{L}}^{\text{adv}}(t_{0})\leq N^{-1}\ell(b_{\text{f}})\)._

This assumption is a generalization of the separability condition in Refs. [29; 30]. Adversarial training can typically achieve this separability in practice, _i.e._, the model can fit adversarial examples of the training dataset, making the above assumption reasonable. Then, we obtain the following lemma.

**Lemma 9** (Convergence to the direction of a KKT point).: _Consider the hypothesis class \(\mathfrak{F}\) in Eq. (6). Under Assumptions 2 and 8, the limit point of normalized weights \(\{\underline{\mathbf{W}}(t)/\left\lVert\underline{\mathbf{W}}(t)\right\rVert _{\mathrm{F}}:t\geq 0\}\) of the GF for Eq. (13), i.e., the empirical adversarial risk with scale invariant adversarial perturbations \(\boldsymbol{\delta}_{i}(\underline{\mathbf{W}})\), is aligned with the direction of a KKT point of the minimization problem:_

\[\min_{\underline{\mathbf{W}}}\frac{1}{2}\left\lVert\underline{\mathbf{W}} \right\rVert_{\mathrm{F}}^{2},\qquad\text{s.t. }y_{i}f(\underline{\mathbf{x}}_{i}+\boldsymbol{\delta}_{i}( \underline{\mathbf{W}});\underline{\mathbf{W}})\geq 1,\quad i=1,\cdots,N.\] (14)

Building upon Lemma 9, we can establish that highly over-parameterized t-NNs undergoing adversarial training with GF will exhibit an implicit bias towards transformed low-rank weights.

**Theorem 10** (Implicit low-rankness for t-NNs induced by GF).: _Suppose there is an example \(\underline{\mathbf{x}}_{i}\) satisfying \(\|\underline{\mathbf{x}}_{i}\|_{\mathrm{F}}\leq 1\) in the training set \(S=\{(\underline{\mathbf{x}}_{i},y_{i})\}_{i=1}^{N}\). Suppose there is a \((J+1)\)-layer (\(J\geq 2\)) ReLU t-NN, denoted by \(g(\underline{\mathbf{x}};\underline{\mathbf{V}})\) with parameters \(\underline{\mathbf{V}}=(\underline{\mathbf{V}}^{(1)},\cdots,\underline{ \mathbf{V}}^{(J)},\mathbf{v})\), satisfying the conditions:_

1. _the dimensionality of the weight tensor_ \(\underline{\mathbf{V}}^{(j)}\in\mathbb{R}^{m_{j}\times m_{j-1}\times\mathrm{c}}\) _of the_ \(j\)_-th_ \(t\)_-product layer satisfies_ \(m_{j}\geq 2\)_,_ \(j=1,\cdots,J\)_;_
2. _there is a constant_ \(B_{v}>0\)_, such that the Euclidean norm of the weights_ \(\underline{\mathbf{V}}=(\mathbf{V}^{(1)},\cdots,\underline{\mathbf{V}}^{(L)}, \mathbf{v})\) _satisfy_ \(\|\underline{\mathbf{V}}^{(j)}\|_{\mathrm{F}}\leq B_{v}\) _for any_ \(j=1,\cdots,J\) _and_ \(\left\|\mathbf{v}\right\|_{2}\leq B_{v}\)_;_
3. _for all_ \(i\in\{1,\cdots,N\}\)_, we have_ \(y_{i}g(\underline{\mathbf{x}}_{i}+\underline{\delta}_{i}(\underline{\mathbf{V} });\underline{\mathbf{V}})\geq 1\)_._

_Then, we consider the class of over-parameterized t-NNs \(\mathfrak{F}=\{f(\underline{\mathbf{x}};\underline{\mathbf{W}})\}\) defined in Eq. (5) satisfying_

1. _the number_ \(L\) _of t-product layers is much greater than_ \(J\)_;_
2. _the dimensionality of weight_ \(\underline{\mathbf{W}}^{(l)}\in\mathbb{R}^{d_{l}\times d_{l-1}\times\mathrm{c}}\) _satisfies_ \(d_{l}\gg\max_{j\leq J}\{m_{j}\}\) _for any_ \(l\leq L\)_._

_Let \(\underline{\mathbf{W}}^{*}=(\underline{\mathbf{W}}^{*(1)},\cdots,\underline{ \mathbf{W}}^{*(L)},\mathbf{w}^{*})\) be a global optimum of Problem (14). Namely, \(\underline{\mathbf{W}}^{*}\) parameterizes a minimum-norm t-NN \(\overline{f(\underline{\mathbf{x}};\underline{\mathbf{W}}^{*})}\in\mathfrak{F}\) that labels the perturbed training set correctly with margin 1 under scale invariant adversarial perturbations. Then, we have_

\[\overline{\sum_{l=1}^{L}\left(r_{\mathrm{sub}}(\widetilde{\mathbf{W}}^{*(l)}_ {M})\right)^{-1/2}}\leq\overline{\left(1+\frac{1}{L}\right)\left(\frac{1}{B_ {v}}\right)^{\frac{l+1}{L+1}}\sqrt{\frac{L+1}{(J+1)+(cm_{J})(L-J)}}-\frac{1}{L}},\]

_where \(\widetilde{\mathbf{W}}^{*(l)}_{M}\) denotes the \(M\)-block-diagonal matrix of weight tensor \(\underline{\mathbf{W}}^{*(l)}\) for any \(l=1,\cdots,L\)._

By the above theorem, when \(L\) is sufficiently large, the harmonic mean of the square root of the stable rank of \(\widetilde{\mathbf{W}}^{*(l)}_{M}\), _i.e._, the \(M\)-block-diagonal matrix of weight tensor \(\underline{\mathbf{W}}^{*(l)}\), is approximately bounded by \(\sqrt{cm_{J}}\), which is significantly smaller than the square root of the dimensionality \(\sqrt{\min\{cd_{l},cd_{l-1}\}}\) according to condition _(C.5)_ in Theorem 10. Thus, \(f(\underline{\mathbf{x}};\underline{\mathbf{W}}^{*})\) has a nearly low-rank parameterization in the transformed domain. In our case, the weights \(\underline{\mathbf{W}}(\overline{t})\) generated by GF tend to have an infinite norm and to converge in direction to a transformed low-rank solution. Moreover, note that the ratio between the spectral norm and the F-norm is invariant to scaling, and hence it suggests that after a sufficiently long time, GF tends to reach a t-NN with transformed low-rank weight tensors. Refer to Sec. A.2 for numerical evidence supporting Theorem 10.

### Robust Generalization with Approximate Transformed Low-rank Parameterization

Theorem 10 establishes that for highly over-parameterized adversarial training with GF, well-trained t-NNs exhibit approximately transformed low-rank parameters under specific conditions. In this section, we analyze the AGP of t-NNs that possess an approximately transformed low-rank parameterization7.

Footnote 7: We use the tubal rank as a measure of low-rankness in the transformed domain for notation simplicity. One can also consider the average rank [52] or multi-rank [50] for more refined bounds with quite similar techniques.

Initially, by employing low-tubal-rank tensor approximation [20], one can always compress an _approximately_ low-tubal-rank parameterized t-NN \(f\) by a t-NN \(g\in\mathfrak{F}_{\mathbf{r}}\) with an _exact_ low-tubal-rank parameterization, ensuring a small distance between \(g\) and \(f\) in the parameter space. Now, the question is: _Can the small parametric distance between \(f\) and \(g\) also indicate a small difference in their adversarial generalization behaviors?_ To answer this question, we first define the \((\delta,\mathbf{r})\)-approximate low-tubal-rank parameterized functions.

**Definition 5** (\((\delta,\mathbf{r})\)-approximate low-tubal-rank parameterization).: _A t-NN \(f(\underline{\mathbf{x}};\underline{\mathbf{W}})\in\mathfrak{F}\) with weights \(\underline{\mathbf{W}}=(\mathbf{w},\underline{\mathbf{W}}^{(1)},\cdots, \underline{\mathbf{W}}^{(L)})\) is said to satisfy the \((\delta,\mathbf{r})\)-approximate low-tubal parameterization with tolerance \(\delta>0\) and rank \(\mathbf{r}=(r_{1},\cdots,r_{L})^{\top}\in\mathbb{N}^{L}\), if there is a t-NN \(g(\underline{\mathbf{x}};\underline{\mathbf{W}}_{\mathbf{r}})\in\mathfrak{F}_ {\mathbf{r}}\) whose weights \(\underline{\mathbf{W}}_{g}=(\mathbf{w},\underline{\mathbf{W}}^{(1)}_{r_{1}}, \cdots,\underline{\mathbf{W}}^{(L)}_{r_{L}})\) satisfy \(\|\underline{\mathbf{W}}^{(l)}_{r_{l}}-\underline{\mathbf{W}}^{(l)}\|_{ \mathrm{F}}\leq\delta\) for any \(l=1,\cdots,L\)._

Furthermore, let's consider the collection of t-NNs with approximately low-tubal-rank weights

\[\mathfrak{F}_{\delta,\mathbf{r}}:=\{f\in\mathfrak{F}\mid f\text{ satisfies the }(\delta,\mathbf{r})\text{-approximate low-tubal-rank parameterization}\}\.\] (15)Subsequently, we analyze the AGP for any \(f\in\mathfrak{F}_{\delta,\mathbf{r}}\) in terms of its low-tubal-rank compression \(g\in\mathfrak{F}_{\mathbf{r}}\). The idea is motivated by the work on compressed bounds for non-compressed but compressible models [43], originally developed for generalization analysis of NNs for standard training.

Under Assumption 2, we first define \(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}:=\{\tilde{f}:(\mathbf{x},y) \mapsto\min_{R_{\text{a}}(\mathbf{x}^{\prime}-\mathbf{x})\leq\xi}yf(\mathbf{x }^{\prime})\mid f\in\mathfrak{F}_{\delta,\mathbf{r}}\}\) as the adversarial version of \(\mathfrak{F}_{\delta,\mathbf{r}}\). To analyze the AGP of \(f\in\mathfrak{F}_{\delta,\mathbf{r}}\) through \(g\in\mathfrak{F}_{\mathbf{r}}\), we instead consider their adversarial counterparts \(\tilde{f}\in\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}\) and \(\tilde{g}\in\mathfrak{F}_{\mathbf{r}}^{\text{adv}}\), where \(\mathfrak{F}_{\mathbf{r}}^{\text{adv}}\) is defined as \(\mathfrak{F}_{\mathbf{r}}^{\text{adv}}:=\{\tilde{g}:(\mathbf{x},y)\mapsto\min _{R_{\text{a}}(\mathbf{x}-\mathbf{x}^{\prime})\leq\xi}yg(\mathbf{x}^{\prime} )\mid g\in\mathfrak{F}_{\mathbf{r}}\}\). Define the Minkowski difference of \(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}\) and \(\mathfrak{F}_{\mathbf{r}}^{\text{adv}}\) as \(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{ \text{adv}}:=\{\tilde{f}-\tilde{g}\mid\tilde{f}\in\mathfrak{F}_{\delta, \mathbf{r}}^{\text{adv}},\;\tilde{g}\in\mathfrak{F}_{\mathbf{r}}^{\text{adv} }\}\). The empirical \(L_{2}\)-norm of a t-NN \(h\in\mathfrak{F}\) on the training data \(S=\{(\underline{\mathbf{x}}_{i},y_{i})\}_{i=1}^{N}\) is defined as \(\|h\|_{S}:=\sqrt{N^{-1}\sum_{i=1}^{N}h^{2}(\underline{\mathbf{x}}_{i},y_{i})}\), and the population \(L_{2}\)-norm is \(\|h\|_{L_{2}}:=\sqrt{\mathbb{E}_{P(\mathbf{x},y)}[h^{2}(\underline{\mathbf{x} },y)]}\). Define the local Rademacher complexity of \(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{ \text{adv}}\) of radius \(\mathfrak{r}>0\) as \(\dot{R}_{\text{r}}(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F }_{\mathbf{r}}^{\text{adv}}):=\bar{R}_{N}(\{h\in\mathfrak{F}_{\delta,\mathbf{ r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{\text{adv}}\mid\|h\|_{L_{2}}\leq \mathfrak{r}\}),\) where \(\bar{R}_{N}(\mathcal{H})\) denotes the average Rademacher complexity of a function class \(\mathcal{H}\)[4].

The first part of the upcoming Theorem 12 shows that a small parametric distance between \(f\) and \(g\) leads to a small empirical \(L_{2}\)-distance in the adversarial output space. Specifically, for any \(f(\mathbf{x};\underline{\mathbf{W}})\in\mathfrak{F}_{\delta,\mathbf{r}}\) with compression \(g(\mathbf{x};\underline{\mathbf{W}}_{\mathbf{r}})\), their (adversarial) empirical \(L_{2}\)-distance \(\|\tilde{f}(\mathbf{x};\underline{\mathbf{W}})-\tilde{g}(\mathbf{x}; \underline{\mathbf{W}}_{\mathbf{r}})\|_{S}\) can be bounded by a small constant \(\hat{\mathfrak{r}}>0\) in linearity of \(\delta\). We also aim for a small population \(L_{2}\)-distance by first assuming the local Rademacher complexity \(\dot{R}_{\text{r}}(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F }_{\mathbf{r}}^{\text{adv}})\) can be bounded by a concave function of \(\mathfrak{r}\), following common practice in Rademacher complexity analysis [4, 43].

**Assumption 11**.: _For any \(\mathfrak{r}>0\), there exists a function \(\phi(\mathfrak{r}):[0,\infty)\rightarrow[0,\infty)\) such that \(\dot{R}_{\mathfrak{r}}(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F }_{\mathbf{r}}^{\text{adv}})\leq\phi(\mathfrak{r})\;\;\text{and}\;\;\phi( 2\mathfrak{r})\leq 2\phi(\mathfrak{r}).\)_

We further define \(\mathfrak{r}_{*}=\mathfrak{r}_{*}(t):=\inf\left\{\mathfrak{r}>0\;\big{|}16B_{f }\mathfrak{r}^{-2}\phi(\mathfrak{r})+B_{f}\mathfrak{r}^{-1}\sqrt{2t/N}+2tB_{ f}^{2}\mathfrak{r}^{-2}/N\leq 1/2\right\}\) for any \(t>0\), such that the population \(L_{2}\)-norm of any \(h\in\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{ \text{adv}}\) can be bounded by \(\|h\|_{L_{2}}^{2}\leq 2(\|h\|_{S}^{2}+\mathfrak{r}_{*}^{2})\) using the peeling argument [42, Theorem 7.7]. We then establish an adversarial generalization bound for approximately low-tubal-rank t-NNs as follows.

**Theorem 12** (Adversarial generalization bound for general approximately low-tubal-rank t-NNs).: _(I). For any \(f\in\mathfrak{F}_{\delta,\mathbf{r}}\) with adversarial proxy \(\tilde{f}\in\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}\), there exists a function \(g\in\mathfrak{F}_{\mathbf{r}}\) with adversarial proxy \(\tilde{g}\in\mathfrak{F}_{\mathbf{r}}^{\text{adv}}\), such that the empirical \(L_{2}\)-distance \(\|\tilde{f}-\tilde{g}\|_{S}\leq\delta B_{\tilde{f}}\sum_{l=1}^{L}B_{l}^{-1}= \hat{\mathfrak{r}}\)._

_(II). Let \(\dot{\mathfrak{r}}:=\sqrt{2(\tilde{\mathfrak{r}}^{2}+\mathfrak{r}_{*}^{2})}\). Under Assumptions 1, 2, 4, 11, there exist constants \(C_{1},C_{2}>0\) satisfying_

\[\begin{split}\mathcal{L}^{\text{adv}}(f)-\hat{\mathcal{L}}^{ \text{adv}}(f)&\leq\underbrace{\frac{C_{1}L_{\ell}B_{\tilde{f}}}{ \sqrt{N}}\sqrt{\mathfrak{c}\sum_{l=1}^{L}r_{l}(d_{l-1}+d_{l})\log(9(L+1))}+B \sqrt{\frac{t}{2N}}}_{\text{main term}}\\ &\quad+\underbrace{C_{2}\left(\Phi(\dot{\mathfrak{r}})+L_{\ell} \dot{\mathfrak{r}}\sqrt{\frac{t}{N}}+\frac{tL_{\ell}B_{\tilde{f}}}{N}\right)} _{\text{bias term}},\end{split}\] (16)

_for any \(f\in\mathfrak{F}_{\delta,\mathbf{r}}\) with probability at least \(1-4e^{-t}\) for any \(t>0\), where \(\Phi(\mathfrak{r})\) is defined as_

\[\Phi(\mathfrak{r}):=\bar{R}_{N}\left(\{\ell\circ\tilde{f}-\ell\circ\tilde{g}\mid \tilde{f}\in\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}},\tilde{g}\in \mathfrak{F}_{\mathbf{r}}^{\text{adv}},\|\tilde{f}-\tilde{g}\|_{L_{2}}\leq \mathfrak{r}\}\right).\]

The main term of the bound quantifies the complexity of functions in \(\mathfrak{F}_{\mathbf{r}}\) with exact low-tubal-rank parameterization in adversarial settings, which can be significantly smaller than that of \(\mathfrak{F}_{\delta,\mathbf{r}}\). On the other hand, the bias term captures the sample complexity required to bridge the gap between approximately low-tubal-rank parameterized \(\mathfrak{F}_{\delta,\mathbf{r}}\) and exactly low-tubal-rank parameterized \(\mathfrak{F}_{\mathbf{r}}\). As we usually observe \(\mathfrak{r}_{*}^{2}=o(1/\sqrt{N})\), setting \(\hat{\mathfrak{r}}=o_{p}(1)\) allows the bias term to decay faster than the main term, which is \(O(1/\sqrt{N})\). Theorem 12 suggests that _a small parametric distance between \(f\in\mathfrak{F}_{\delta,\mathbf{r}}\) and \(g\in\mathfrak{F}_{\mathbf{r}}\) also implies a small difference in their adversarial generalization behaviors_.

**A special case.** We also showcase a specific scenario where the weights of t-product layers exhibit a polynomial spectral decay in the transformed domain, leading to a considerably small AGP bound.

**Assumption 13**.: _Consider the setting where any \(t\)-NN \(f(\mathbf{x};\underline{\mathbf{W}})\in\mathfrak{F}_{\delta,\mathbf{r}}\) has tensor weights \(\underline{\mathbf{W}}^{(l)}\)\((l=1,\cdots,L)\) whose singular values in the transformed domain satisfy \(\sigma_{j}(M(\underline{\mathbf{W}}^{(l)})_{:,:,k})\leq V_{0}\cdot j^{-\alpha}\), where \(V_{0}>0\) is a constant, and \(\sigma_{j}(\cdot)\) is the \(j\)-th largest singular value of a matrix._

Under Assumption 13, the weight tensor \(\underline{\mathbf{W}}^{(l)}\) can be approximated by its optimal tubal-rank-\(r_{l}\) approximation \(\underline{\mathbf{W}}^{(l)}_{r_{l}}\) for any \(1\leq r_{l}\leq\min\{d_{l},d_{l-1}\}\) with error \(\|\underline{\mathbf{W}}^{(l)}-\underline{\mathbf{W}}^{(l)}_{r_{l}}\|_{ \mathrm{F}}\leq\sqrt{\mathsf{c}/(2\alpha-1)}V_{0}(r_{l}-1)^{(1-2\alpha)/2}\)[20], which can be much smaller than \(\|\underline{\mathbf{W}}^{(l)}_{r_{l}}\|_{\mathrm{F}}\) when \(\alpha>1/2\) is sufficiently large. Thus, we can find an exactly low-tubal-rank parameterized \(g\in\mathfrak{F}_{\delta,\mathbf{r}}\) for any \(f\in\mathfrak{F}_{\delta,\mathbf{r}}\) satisfying Assumption 13, such that the parametric distance between \(g\) and \(f\) is quite small. The following theorem shows that the small parametric distance also leads to a small AGP.

**Theorem 14**.: _Under Assumptions 1, 2, 4, and 13, if we let \(\hat{\mathfrak{t}}=V_{0}B_{\tilde{f}}\sum_{l=1}^{L}(r_{l}+1)^{-\alpha}B_{l}^{-1}\), then for any t-NN \(f\in\mathfrak{F}_{\delta,\mathbf{r}}\), there exists a function \(g\in\mathfrak{F}_{\hat{\mathfrak{r}}}\) whose t-product layer weights have tubal-rank exactly no greater than \(r_{l}\), satisfying \(\|\tilde{f}-\tilde{g}\|_{S}\leq\hat{\mathfrak{t}}\). Further, there is a constant \(C_{\alpha}\) only depending on \(\alpha\) such that the AGP, i.e., \(\mathcal{L}^{\mathrm{adv}}(f)-\hat{\mathcal{L}}^{\mathrm{adv}}(f)\), of any \(f\in\mathfrak{F}_{\delta,\mathbf{r}}\) can be upper bounded by_

\[C_{\alpha}L_{\ell}\bigg{\{}B_{\tilde{f}}E_{1}+\hat{\mathfrak{t}}\sqrt{E_{1}}+ E_{2}^{\frac{2\alpha}{2\alpha+1}}\left(B_{\tilde{f}}^{\frac{2\alpha-1}{2\alpha+1}}+1 \right)+\hat{\mathfrak{t}}^{\frac{2\alpha}{2\alpha+1}}\sqrt{E_{2}}+(\hat{ \mathfrak{t}}+\frac{B}{L_{\ell}})\sqrt{\frac{t}{N}}+\frac{1+tB_{\tilde{f}}}{N }\bigg{\}},\]

_for any \(t>0\) with probability at least \(1-4e^{-t}\), where \(E_{1}=N^{-1}\mathsf{c}\sum_{l=1}^{L}r_{l}(d_{l}+d_{l-1})\log(9NLB_{\tilde{f}}/ \sqrt{\mathsf{c}})\) and \(E_{2}=N^{-1}\mathsf{c}\sum_{l=1}^{L}\left(LV_{0}B_{\tilde{f}}B_{l}^{-1}\right) ^{1/\alpha}(d_{l}+d_{l-1})\log(9NLB_{\tilde{f}}/\sqrt{\mathsf{c}})\)._

This suggests that by choosing a sufficiently large \(\alpha>1/2\), where each weight tensor has a tubal-rank close to 1, we can attain a superior generalization error bound. It is important to note that the rank \(r_{l}\) can be arbitrarily chosen, and there exists a trade-off relationship between \(\hat{\mathfrak{t}}\) and \(E_{1}\). Therefore, by selecting the rank appropriately for a balanced trade-off, we can obtain an optimal bound as follows.

**Corollary 15**.: _Under the same assumption to Theorem 14, if we choose the parameter \(\mathbf{r}\) of tubal ranks in \(\mathfrak{F}_{\mathbf{r}}\) by \(r_{l}=\min\{\lceil\left(LV_{0}B_{\tilde{f}}B_{l}^{-1}\right)^{1/\alpha} \rceil,d_{l},d_{l-1}\}\), then there is a constant \(C_{\alpha}\) only depending on \(\alpha\) such that the AGP of any \(f\in\mathfrak{F}_{\delta,\mathbf{r}}\) can be upper bounded as_

\[\mathcal{L}^{\mathrm{adv}}(f)-\hat{\mathcal{L}}^{\mathrm{adv}}(f) \leq C_{\alpha}L_{\ell}\bigg{\{}B_{\tilde{f}}^{1-1/(2\alpha)}\sqrt{ \frac{\mathsf{c}\sum_{l=1}^{L}\left(LV_{0}B_{l}^{-1}\right)^{1/\alpha}(d_{l}+d _{l-1})\log(9NLB_{\tilde{f}}/\sqrt{\mathsf{c}})}{N}}\] \[\qquad\qquad\qquad\qquad+E_{2}^{\frac{2\alpha}{2\alpha+1}}\left(B _{\tilde{f}}^{\frac{2\alpha-1}{2\alpha+1}}+1\right)+\sqrt{E_{2}}+\frac{B}{L_{ \ell}}\sqrt{\frac{t}{N}}+\frac{1+tB_{\tilde{f}}}{N}\bigg{\}},\]

_with probability at least \(1-4e^{-t}\) for any \(t>0\)._

It is worth highlighting that the bound exhibits a linear dependency on the number of neurons in the t-product layers, represented as \(O(\sqrt{\mathsf{c}\sum_{l}(d_{l}+d_{l-1})/N})\). In contrast, Theorem 5 demonstrates a dependency on the total number of parameters, denoted as \(O(\sqrt{\mathsf{c}\sum_{l}d_{l}d_{l-1}/N})\). This observation suggests that employing the low-tubal-rank parameterization can potentially enhance adversarial generalization for t-NNs.

## 5 Related Works

**T-SVD-based data and function representation.** The unique feature of t-SVD-based data representation, in contrast to classical low-rank decomposition methods, is the presence of low-rankness in the transformed domain. This transformed low-rankness is crucial for effectively modeling real multi-channel data with both smoothness and low-rankness [24; 49; 50]. Utilized in t-product layers in DNNs [32; 36; 53], t-SVD has also been a workhorse for function representation and achieves impressive empirical performance. While t-SVD-based signal processing models have been extensively studied theoretically [13; 24; 40; 50; 60], the t-SVD-based learning model itself has not been thoroughly scrutinized until this paper. Hence, this study represents the first theoretical analysis of t-SVD-based learning models, contributing to the understanding of their theoretical foundations.

**Theoretical analysis methods.** Our analysis draws on norm-based generalization analysis [37] and implicit regularization of gradient descent-based learning [46] as related theoretical analysis methods. Norm-based generalization analysis plays a crucial role in theoretical analysis across various domains, including standard generalization analysis of DNNs [8], compressed models [22], non-compressed models [43], and adversarial generalization analysis [55; 59; 3]. Our work extends norm-based tools to analyze both standard and adversarial generalization in t-NNs, going beyond the traditional use of matrix products. For implicit regularization of gradient descent based learning, extensive past research has been conducted on implicit bias of GF for both standard and adversarial training of homogeneous networks building on matrix product layers, respectively [16; 30; 45]. We non-trivially extend these methods to analyze t-NNs and reveals that GF for over-parameterized ReLU t-NNs produces nearly transformed low-rank weights under scale invariant adversarial perturbations.

Our theoretical results notably deviate from the standard error bounds for fully connected neural networks (FNNs) in several ways:

* The generalization bounds in Lemma 3 and Theorem 5 for t-NNs diverge from their counterparts for FNNs in Refs. [8; 55; 59] due to the channel number c in t-NNs. Moreover, Theorem 5 encompasses a wider range of adversary classes than the \(l_{p}\)-attacks in the aforementioned references.
* The uniqueness of Theorem 6, compared to Refs. [55; 59], stems from its consideration of weight low-rankness in the adversarial generalization bound, suggesting possible robustness improvements in generalization.
* Our exploration of the implicit bias in GF for adversarial training presents a novel angle: the bias towards approximate transformed low-rankness in t-NNs. While Ref. [29] focuses on the implicit bias in adversarial training for FNNs, centered on KKT point convergence with exponential loss, our work delves deeper, considering a wider array of loss functions in adversarial training for t-NNs.
* A crucial distinction in our adversarial generalization bounds, detailed in Section 4.3, from non-adversarial bounds for FNNs [43] is the integration of the localized Rademacher complexity. This encompasses the Minkowski difference between adversarial counterparts of both approximately and exactly low-tubal-rank t-NNs as seen in Theorem 12.

## 6 Concluding Remarks

A thorough investigation of the generalization behavior of t-NNs is conducted for the first time. We derive upper bounds for the generalization gaps of standard and adversarially trained t-NNs and propose compressing t-NNs with a transformed low-rank structure for more efficient adversarial learning and tighter bounds on the adversarial generalization gap. Our analysis shows that adversarial training with GF in highly over-parameterized settings results in t-NNs with approximately transformed low-rank weights. We further establish sharp adversarial generalization bounds for t-NNs with approximately transformed low-rank weights. Our findings demonstrate that utilizing the transformed low-rank parameterization can significantly enhance the robust generalization of t-NNs, carrying both theoretical and empirical significance.

**Limitations.** While this paper adheres to the norm-based framework for capacity control [8; 37], it is worth noting that the obtained generalization bounds may be somewhat conservative. However, this limitation can be mitigated by employing more sophisticated analysis techniques, as evidenced by recent studies [2; 56; 57; 25].

**Discussions.** The inclination of adversarial training towards low-rank/sparse weights, and the reciprocal effects of parameter reduction on robustness, are currently at the forefront of ongoing research. This domain has witnessed a spectrum of observations and results [6; 23; 41; 51]. In this study, we propose that employing low-rank parameterization can enhance the adversarial robustness of t-NNs, as evidenced by our analysis of uniform adversarial generalization error bounds. However, despite these promising results, it is crucial to emphasize the necessity of a more exhaustive exploration of low-rank parameterization. Its implications, particularly when considered in the context of approximation, estimation, and optimization, are profound and warrant further dedicated research efforts. Such a comprehensive investigation will undoubtedly enhance our understanding and fully unlock the potential of low-rank parameterization in neural networks.

## Acknowledgments

We extend our deepest gratitude to Linfeng Sui and Xuyang Zhao for their indispensable support in implementing the Python code for t-NNs during the rebuttal phase. Our sincere appreciation also goes to both the area chair and reviewers for their unwavering dedication and meticulous attention given to this paper. This research was supported by RIKEN Incentive Research Project 100847-202301062011, by JSPS KAKENHI Grant Numbers JP20H04249 and JP23H03419, and in part by National Natural Science Foundation of China under Grants 62103110 and 62073087.

## References

* [1] J. An, W. Liu, Q. Liu, L. Guo, P. Ren, and T. Li. Dginet: Dynamic graph and interaction-aware convolutional network for vehicle trajectory prediction. _Neural Networks_, 151:336-348, 2022.
* [2] S. Arora, R. Ge, B. Neyshabur, and Y. Zhang. Stronger generalization bounds for deep nets via a compression approach. In _International Conference on Machine Learning_, pages 254-263. PMLR, 2018.
* [3] P. Awasthi, N. Frank, and M. Mohri. Adversarial learning guarantees for linear hypotheses and neural networks. In _International Conference on Machine Learning_, pages 431-441. PMLR, 2020.
* [4] P. L. Bartlett, O. Bousquet, and S. Mendelson. Localized rademacher complexities. In _Computational Learning Theory: 15th Annual Conference on Computational Learning Theory, COLT 2002 Sydney, Australia, July 8-10, 2002 Proceedings 15_, pages 44-58. Springer, 2002.
* [5] S. Boucheron, G. Lugosi, and P. Massart. _Concentration inequalities: A nonasymptotic theory of independence_. Oxford university press, 2013.
* [6] J. Cosentino, F. Zaiter, D. Pei, and J. Zhu. The search for sparse, robust neural networks. _arXiv preprint arXiv:1912.02386_, 2019.
* [7] J. Dutta, K. Deb, R. Tulshyan, and R. Arora. Approximate KKT points and a proximity measure for termination. _Journal of Global Optimization_, 56(4):1463-1499, 2013.
* [8] N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks. In _Conference On Learning Theory_, pages 297-299. PMLR, 2018.
* [9] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In _International Conference on Learning Representations_, 2015.
* [10] F. Graf, S. Zeng, M. Niethammer, and R. Kwitt. On measuring excess capacity in neural networks. _arXiv preprint arXiv:2202.08070_, 2022.
* [11] F. Han, Y. Miao, Z. Sun, and Y. Wei. T-ADAF: Adaptive data augmentation framework for image classification network based on tensor t-product operator. _Neural Processing Letters_, 2023.
* [12] L. Horesh, E. Newman, M. E. Kilmer, and H. Avron. Generating and managing deep tensor neural networks, Dec. 20 2022. US Patent 11,531,902.
* [13] J. Hou, F. Zhang, H. Qiu, J. Wang, Y. Wang, and D. Meng. Robust low-tubal-rank tensor recovery from binary measurements. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2021.
* [14] Z. Huang, X. Li, Y. Ye, and M. K. Ng. Mr-gcn: Multi-relational graph convolutional networks based on generalized tensor product. In _IJCAI_, volume 20, pages 1258-1264, 2020.
* [15] Y. Idelbayev and M. A. Carreira-Perpinan. Low-rank compression of neural nets: Learning the rank of each layer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8049-8059, 2020.

* [16] Z. Ji and M. Telgarsky. Directional convergence and alignment in deep learning. _Advances in Neural Information Processing Systems_, 33:17176-17186, 2020.
* [17] E. Kazemi, T. Kerdreux, and L. Wang. Trace-norm adversarial examples. _arXiv preprint arXiv:2007.01855_, 2020.
* [18] E. Kernfeld, M. Kilmer, and S. Aeron. Tensor-tensor products with invertible linear transforms. _Linear Algebra and its Applications_, 485:545-570, 2015.
* [19] M. E. Kilmer, K. Braman, et al. Third-order tensors as operators on matrices: A theoretical and computational framework with applications in imaging. _SIAM J MATRIX ANAL A_, 34(1):148-172, 2013.
* [20] M. E. Kilmer, L. Horesh, H. Avron, and E. Newman. Tensor-tensor algebra for optimal representation and compression of multiway data. _Proceedings of the National Academy of Sciences_, 118(28):e2015851118, 2021.
* [21] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. _SIAM Review_, 51(3):455-500, 2009.
* [22] J. Li, Y. Sun, J. Su, T. Suzuki, and F. Huang. Understanding generalization in deep learning via tensor methods. In _International Conference on Artificial Intelligence and Statistics_, pages 504-515. PMLR, 2020.
* [23] N. Liao, S. Wang, L. Xiang, N. Ye, S. Shao, and P. Chu. Achieving adversarial robustness via sparsity. _Machine Learning_, pages 1-27, 2022.
* [24] X. Liu, S. Aeron, V. Aggarwal, and X. Wang. Low-tubal-rank tensor completion using alternating minimization. _IEEE TIT_, 66(3):1714-1737, 2020.
* [25] S. Lotfi, M. Finzi, S. Kapoor, A. Potapczynski, M. Goldblum, and A. G. Wilson. PAC-Bayes compression bounds so tight that they can explain generalization. _Advances in Neural Information Processing Systems_, 35:31459-31473, 2022.
* [26] C. Lu. Transforms based tensor robust pca: Corrupted low-rank tensors recovery via convex optimization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1145-1152, 2021.
* [27] C. Lu, J. Feng, W. Liu, Z. Lin, S. Yan, et al. Tensor robust principal component analysis with a new tensor nuclear norm. _IEEE TPAMI_, 2019.
* [28] C. Lu, X. Peng, and Y. Wei. Low-rank tensor completion with a new tensor nuclear norm induced by invertible linear transforms. In _CVPR_, pages 5996-6004, 2019.
* [29] B. Lv and Z. Zhu. Implicit bias of adversarial training for deep neural networks. In _International Conference on Learning Representations_, 2022.
* [30] K. Lyu and J. Li. Gradient descent maximizes the margin of homogeneous neural networks. In _International Conference on Learning Representations_, 2020.
* [31] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.
* [32] O. A. Malik, S. Ubaru, L. Horesh, M. E. Kilmer, and H. Avron. Dynamic graph convolutional networks using the tensor \(M\)-product. In _Proceedings of the 2021 SIAM international conference on data mining (SDM)_, pages 729-737. SIAM, 2021.
* [33] O. L. Mangasarian and S. Fromovitz. The fritz john necessary optimality conditions in the presence of equality and inequality constraints. _Journal of Mathematical Analysis and applications_, 17(1):37-47, 1967.
* [34] T. Miyato, A. M. Dai, and I. Goodfellow. Adversarial training methods for semi-supervised text classification. In _International Conference on Learning Representations_, 2017.

* [35] S. Negahban, B. Yu, M. J. Wainwright, and P. K. Ravikumar. A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. In _Proceedings of Advances in Neural Information Processing Systems_, pages 1348-1356, 2009.
* [36] E. Newman, L. Horesh, H. Avron, and M. Kilmer. Stable tensor neural networks for rapid deep learning. _arXiv preprint arXiv:1811.06569_, 2018.
* [37] B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In _Conference on Learning Theory_, pages 1376-1401. PMLR, 2015.
* [38] F. Qian, Z. Liu, Y. Wang, S. Liao, S. Pan, and G. Hu. Dtae: Deep tensor autoencoder for 3-D seismic data interpolation. _IEEE Transactions on Geoscience and Remote Sensing_, 60:1-19, 2021.
* [39] F. Qian, Z. Liu, Y. Wang, Y. Zhou, and G. Hu. Ground truth-free 3-D seismic random noise attenuation via deep tensor convolutional neural networks in the time-frequency domain. _IEEE Transactions on Geoscience and Remote Sensing_, 60:1-17, 2022.
* [40] H. Qiu, Y. Wang, S. Tang, D. Meng, and Q. Yao. Fast and provable nonconvex tensor RPCA. In _International Conference on Machine Learning_, pages 18211-18249. PMLR, 2022.
* [41] D. Savostianova, E. Zangrando, G. Ceruti, and F. Tudisco. Robust low-rank training via approximate orthonormal constraints. _arXiv preprint arXiv:2306.01485_, 2023.
* [42] I. Steinwart and A. Christmann. _Support vector machines_. Springer Science & Business Media, 2008.
* [43] T. Suzuki, H. Abe, and T. Nishimura. Compression based bound for non-compressed network: unified generalization error analysis of large compressible deep neural network. In _International Conference on Learning Representations_, 2020.
* [44] M. Talagrand. A new look at independence. _The Annals of Probability_, pages 1-34, 1996.
* [45] N. Timor, G. Vardi, and O. Shamir. Implicit regularization towards rank minimization in ReLU networks. In _The 34th International Conference on Algorithmic Learning Theory_, 2023.
* [46] G. Vardi and O. Shamir. Implicit regularization in ReLU networks with the square loss. In _Conference on Learning Theory_, pages 4224-4258. PMLR, 2021.
* [47] R. Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* [48] A. Wang, Z. Jin, and G. Tang. Robust tensor decomposition via t-SVD: Near-optimal statistical guarantee and scalable algorithms. _Signal Processing_, 167:107319, 2020.
* [49] A. Wang, C. Li, Z. Jin, and Q. Zhao. Robust tensor decomposition via orientation invariant tubal nuclear norms. In _AAAI_, pages 6102-6109, 2020.
* [50] A. Wang, G. Zhou, Z. Jin, and Q. Zhao. Tensor recovery via \(*_{L}\)-spectral \(k\)-support norm. _IEEE Journal of Selected Topics in Signal Processing_, 15(3):522-534, 2021.
* [51] L. Wang, G. W. Ding, R. Huang, Y. Cao, and Y. C. Lui. Adversarial robustness of pruned neural networks. 2018.
* [52] Z. Wang, J. Dong, X. Liu, and X. Zeng. Low-rank tensor completion by approximating the tensor average rank. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4612-4620, 2021.
* [53] Z. Wu, L. Shu, Z. Xu, Y. Chang, C. Chen, and Z. Zheng. Robust tensor graph convolutional networks via t-SVD based graph augmentation. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 2090-2099, 2022.
* [54] D. Xia and M. Yuan. On polynomial time methods for exact low rank tensor completion. _arXiv preprint arXiv:1702.06980_, 2017.

* [55] J. Xiao, Y. Fan, R. Sun, and Z.-Q. Luo. Adversarial Rademacher complexity of deep neural networks. _arXiv preprint arXiv:2211.14966_, 2022.
* [56] J. Xiao, Y. Fan, R. Sun, J. Wang, and Z.-Q. Luo. Stability analysis and generalization bounds of adversarial training. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [57] J. Xiao, R. Sun, and Z.-Q. Luo. PAC-bayesian spectrally-normalized bounds for adversarially robust generalization. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [58] J. Yang, C. Fu, F. Deng, M. Wen, X. Guo, and C. Wan. Toward interpretable graph tensor convolution neural network for code semantics embedding. _ACM Transactions on Software Engineering and Methodology_, 2023.
* [59] D. Yin, R. Kannan, and P. Bartlett. Rademacher complexity for adversarially robust generalization. In _International Conference on Machine Learning_, pages 7085-7094. PMLR, 2019.
* [60] X. Zhang and M. K. Ng. Sparse nonnegative tensor factorization and completion with noisy observations. _IEEE Transactions on Information Theory_, 68(4):2551-2572, 2022.
* [61] X. Zhang and M. K.-P. Ng. Low rank tensor completion with poisson observations. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2021.

Appendix

**Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks**

In the appendix, we begin by presenting numerical evaluations of our theoretical findings. Subsequently, we introduce the additional notations and preliminaries related to t-SVD, followed by the proofs of the propositions mentioned in the main text.

Our analysis and proofs pertaining to t-NNs differ from those for FNNs as follows:

* Firstly, unlike the analysis for FNNs' generalization bounds based on Rademacher complexity in Refs. [8, 55, 59], we derive specific lemmas for standard and adversarial generalization bounds in t-NNs. This is due to the unique structure of (low-rank) t-product layers. We reformulate the t-product through an operator-like expression in Lemma 16, paving the way for pivotal Lemma 17, supporting the t-product-based "peeling argument." Additionally, we introduce Lemmas 37, 38, and Lemma 33 to handle t-product layer output norms and covering low-tubal-rank tensors.
* Secondly, proving the implicit bias of GF for adversarial training of t-NNs, specifically the approximately transformed low-rankness, is nontrivial in comparison to the proof in Ref. [29] for the implicit bias of adversarial training for FNNs. As we consider more general loss functions for t-NNs in contrast to the exponential loss for FNNs in Ref. [29], we first derive a more general convergence result to the direction of a KKT point for t-NNs Lemma 9, and then goes deeper by using a constructive approach to establish the approximately transformed low-rankness in Theorem 10.
* Thirdly, differing from Ref. [43] which focuses on standard FNN generalization, our approach delves into t-NNs' adversarial generalization. We achieve this by introducing the \((\delta,\mathbf{r})\)-parameterization, bounding localized Rademacher complexity for a Minkowski set in adversarial settings, and using low-tubal-rank approximations for tensor weights.

###### Contents

* 1 Introduction
* 2 Notations and Preliminaries
	* 2.1 Tensor Singular Value Decomposition
	* 2.2 Neural Networks with t-Product Layer (t-NNs)
* 3 Standard and Robust Generalization Bounds for t-NNs
	* 3.1 Standard Generalization for General t-NNs
	* 3.2 Robust Generalization for General t-NNs
* 4 Transformed Low-rank Parameterization for Robust Generalization
	* 4.1 Robust Generalization with Exact Transformed Low-rank Parameterization
	* 4.2 Implicit Bias of Gradient Flow for Adversarial Training of Over-parameterized t-NNs
	* 4.3 Robust Generalization with Approximate Transformed Low-rank Parameterization
* 5 Related Works
* 6 Concluding Remarks
* A Numerical Evaluations of the Theoretical Results

* 1 Effects of Exact Transformed Low-rank Weights on the Adversarial Generalization Gap
* 2 Implicit Bias of GF-based Adversarial Training to Approximately Transformed Low-rank Weight Tensors
* 3 Additional Regularization for a Better Low-rank Parameterized t-NN
* B Notations and Preliminaries of t-SVD
* B.1 Notations
* B.2 Additional Preliminaries of t-SVD
* C Standard and Adversarial Generalization Bounds for t-NNs
* C.1 Standard Generalization Bound for t-NNs
* C.2 Adversarial Generalization Bound for t-NNs
* C.3 Generalization Bound under Exact Low-tubal-rank Parameterization
* D Implicit bias towards low-rankness in the transformed domain
* D.1 Convergence to KKT points of Euclidean norm minimization in direction
* D.2 Technical Lemmas for Proving Lemma 9
* D.3 Proof of Theorem 10
* E Generalization bound of approximately low-tubal-rank t-NNs
* E.1 Several Useful Results
* E.2 Adversarial Generalization Gap under Assumption 13
* F Useful Notions and Lemmas
* F.1 Tools for Analyzing General DNNs
* F.2 Some Results for Analyzing t-NNs
Numerical Evaluations of the Theoretical Results

This section presents numerical evaluations for our theoretical results. All training process is conducted on nVidia A100 GPU. For additional information and access to the demo code, please visit the following URL: https://github.com/pingzaiwang/Analysis4TNN/.

### Effects of Exact Transformed Low-rank Weights on the Adversarial Generalization Gap

To validate the adversarial generalization bound in Theorem 6, we have conducted experiments on the MNIST dataset to explore the relationship between adversarial generalization gaps (AGP), weight tensor low-rankness, and training sample size. We consider binary classification of 3 and 7, with FSGM [9] attacks of strength \(20/255\). The t-NN consists of three t-product layers and one FC layer, with weight tensor dimensions of \(28\times 28\times 28\) for \(\underline{\textbf{W}}^{(1)}\), \(\underline{\textbf{W}}^{(2)}\), and \(\underline{\textbf{W}}^{(3)}\), and \(784\) for the FC weight **w**. As an input to the t-NN, each MNIST image of size \(28\times 28\) is treated as a t-vector of size \(28\times 1\times 28\).

Theorem 6 emphasizes: (_i_) lower weight tensor rank leads to smaller bounds on the adversarial generalization gaps, and (_ii_) the bound diminishes at a rate of \(O(1/\sqrt{N})\) as \(N\) increases. We explored this by conducting experiments, controlling the upper bounds of the tubal-rank to 4 and 28 for low and full tubal-rank cases, and systematically increasing the number of training samples.

Fig. 1 presents the results. The curves indicate that t-NNs with lower rank weight tensors have smaller robust generalization errors. Interestingly, the adversarial generalization errors seem to follow a linear relationship with \(1/\sqrt{N}\), approximately validating the generalization error bound in Theorem 6 by approximating the scaling behavior of the empirical errors.

### Implicit Bias of GF-based Adversarial Training to Approximately Transformed Low-rank Weight Tensors

We carried out experiments to confirm two theoretical statements related to the analysis of GF-based adversarial training.

**Statement A.2.1** Theorem 10 reveals that, under specific conditions, well-trained t-NNs with highly over-parameterized adversarial training using GF show nearly transformed low-rank parameters.

**Statement A.2.2** Lemma 22 asserts that the empirical adversarial risk approaches zero, and the F-norm of the weights grows infinitely as \(t\) approaches infinity.

In continuation of the experimental settings in Sec. A.1, we focus on binary classification on MNIST under FGSM attacks. The t-NN is structured with three t-product layers and one FC layer, with weight dimensions set to \(D\times 28\times 28\) for \(\underline{\textbf{W}}^{(1)}\), \(D\times D\times 28\) for \(\underline{\textbf{W}}^{(2)}\) and \(\underline{\textbf{W}}^{(3)}\), and \(28D\) for the FC weight **w**. Our experiments involve setting values of \(D\) to \(128\) and \(256\), respectively, and we track

Figure 1: The adversarial generalization gaps plotted against the training sample size (\(N\)) for t-NNs, both with and without transformed low-rank weight tensors, using the MNIST dataset. In (a), the adversarial generalization gaps are presented against the sample size \(N\), while in (b), they are plotted against \(1/\sqrt{N}\).

[MISSING_PAGE_FAIL:18]

### Additional Regularization for a Better Low-rank Parameterized t-NN

It is natural to ask: _instead of using adversarial training with GF in highly over-parameterized settings to train a approximately transformed low-rank t-NN, is it possible to apply some extra regularizations in training to achieve a better low-rank parameterization_?

Yes, it is possible to apply additional regularizations during training to achieve a better low-rank representation in t-NNs. Instead of relying solely on adversarial training with gradient flow in highly over-parameterized settings, these extra regularizations can potentially promote and enforce low-rankness in the network.

To validate the concern regarding the addition of an extra regularization term, we performed a preliminary experiment. In this experiment, we incorporated the tubal nuclear norm [48] as an explicit regularizer to induce low-rankness in the transformed domain. Specifically, we add the tubal nuclear norm regularization to the t-NN with three t-product layer \(D=128\) in Sec. A.2 with a regularization parameter \(0.01\), and keep the other settings the same as Sec. A.2. We explore how the stable ranks of tensor weights evolve with the epoch number with/without tubal nuclear norm regularization.

The experimental results are depicted in Fig. 5. According to Fig. 5, it becomes evident that the introduction of the explicit low-rank regularizer significantly enforced low-rankness in the transform domain of the weight tensors.

Figure 4: Curves of the F-norms of the \(M\)-block-diagonal matrix derived from the weight tensors, plotted against epoch numbers, using the MNIST dataset. Two t-NN size settings are showcased: (a) \(D=128\) and (b) \(D=256\).

Figure 5: Curves of the stable ranks of the \(M\)-block-diagonal matrix of the weight tensors versus the epoch number in the scenario where \(D=128\), both with and without the tubal nuclear norm regularization, using the MNIST dataset.

Notations and Preliminaries of t-SVD

### Notations

For simplicity, we use \(c\), \(c_{0}\), \(C\) etc. to denote constants whose values can vary from line to line. We use We first give the most commonly used notations in Table 1.

\begin{table}
\begin{tabular}{l l|l l} \hline \hline \multicolumn{4}{l}{_Notations for t-SVD_} \\ \(\mathbf{x}\in\mathbb{R}^{d\times 1\times\mathbf{c}}\) & a t-vector & \(\mathbf{T}\in\mathbb{R}^{m\times n\times\mathbf{c}}\) & a t-matrix \\ \(\mathbf{M}\in\mathbb{R}^{\mathbf{c}\times\mathbf{c}}\) & an orthogonal matrix & \(M(\cdot)\) & transform via \(\mathbf{M}\) in Eq. (1) \\ \(*_{M}\) & t-product & \(\widetilde{\mathbf{T}}_{M}\) & \(M\)-block diagonal matrix of \(\mathbf{T}\) \\ \(\mathbf{T}_{\cdots,i}\) & \(i\)-th frontal slice of \(\mathbf{T}\) & \(r_{\text{t}}(\cdot)\) & tensor tubal rank \\ \(\|\cdot\|_{\text{sp}}\) & tensor spectral norm & \(\|\cdot\|_{\text{F}}\) & tensor F-norm \\ \(\|\cdot\|\) & matrix spectral norm & & \\ \hline \multicolumn{4}{l}{_Notations for data representation_} \\ c & number of channels & \(d\) & number of features per channel \\ \(\underline{\mathbf{x}}_{i}\in\mathbb{R}^{d\times 1\times\mathbf{c}}\) & a multi-channel example & \(y_{i}\in\{\pm\}\) & label of multi-channel data \(\underline{\mathbf{x}}_{i}\) \\ \(S\) & training sample of size \(N\) & \(\underline{\boldsymbol{\delta}}_{i}\) & scale invariant adv. perturbation \\ \(R_{\text{a}}(\cdot)\) & norm used for attack & \(\underline{\mathbf{x}}_{i}^{\prime}\) & adv. perturbed version of \(\underline{\mathbf{x}}_{i}\) \\ \(B_{x}\) & upper bound on \(\|\underline{\mathbf{x}}\|_{\text{F}}\) & \(\xi\) & radius of \(R_{\text{a}}(\cdot)\) for adv. attack \\ \hline \multicolumn{4}{l}{_Notations for network structure_} \\ \(L\) & number of t-product layers of a general t-NN & \\ \(\underline{\mathbf{W}}^{(l)}\) & weight tensor of \(l\)-th t-product layer with dimensionality \(d_{l}\times d_{l-1}\times\mathbf{c}\) & \\ \(\overline{\mathbf{w}}\) & weight vector of fully connected layer with dimensionality \(cd_{L}\) & \\ \(f(\underline{\mathbf{x}};\underline{\mathbf{W}})\) & a general t-NN with weights \(\underline{\mathbf{W}}=(\underline{\mathbf{W}}^{(1)},\cdots,\underline{ \mathbf{W}}^{(L)},\mathbf{w})\) & \\ \(B_{\underline{\mathbf{W}}}\) & bound on product of Euclidean norms of weights of \(f\in\mathfrak{F}\), i.e., \(B_{\underline{\mathbf{W}}}=B_{w}\prod_{i=1}^{L}B_{l}\) \\ \hline \multicolumn{4}{l}{_Notations for model analysis_} \\ \(\tilde{f}(\underline{\mathbf{x}},y)\) & adversarial version of \(f(\underline{\mathbf{x}})\) which maps \((\underline{\mathbf{x}},y)\) to \(\inf_{R_{\text{a}}(\mathbf{x}^{\prime}-\underline{\mathbf{x}})\leq\xi}y_{i} f(\underline{\mathbf{x}}^{\prime})\) & \\ \(B_{\tilde{f}}\) & bound on the output of \(\tilde{f}(\underline{\mathbf{x}},y)\) given as \(B_{\tilde{f}}:=(B_{x}+\xi C_{R_{\text{a}}})B_{\underline{\mathbf{W}}}\) & \\ \(\ell(f(\underline{\mathbf{x}}),y)\) & loss function with range \([0,B]\), and Lipstchitz constant \(L_{\ell}\) (See Assumption 2) & \\ \(\tilde{\mathcal{L}}\), \(\mathcal{L}\) & standard empirical and population risk, respectively & \\ \(\tilde{\mathcal{L}}^{\text{adv}}\), \(\mathcal{L}^{\text{adv}}\) & empirical and population risk, respectively & \\ \(\mathfrak{F}\), \(\mathfrak{F}^{\text{adv}}\) & function class of t-NNs and its adversarial version, respectively & \\ \(\mathfrak{F}\), \(\mathfrak{F}^{\text{adv}}\) & function class of low-tubal-rank parameterized t-NNs and adversarial version, resp. & \\ \hline \multicolumn{4}{l}{_Notations for implicit bias analysis (Sec. 4.2)_} \\ \(\tilde{q}_{i}\), \(\tilde{q}_{m}\), \(\tilde{\gamma}\) & example robust, sample robust, and smoothly normalized robust margin, resp. & \\ \(\mathfrak{f},\mathfrak{g},b_{\text{f}},b_{\text{g}},K\) & auxillary functions and constants to chareterize \(\ell(\cdot,\cdot)\) (See Assumption 2) & \\ \hline \multicolumn{4}{l}{_Notations for the analysis of apprximately transformed low-rank parameterized models (Sec. 4.3)_} \\ \(\tilde{R}_{N}\), \(\hat{R}_{S}\), \(\hat{R}_{\text{r}}\) & average, empirical, and localized Rademacher complexity, resp. & \\ \(\mathfrak{F}_{\delta,\mathbf{r}}\), \(\mathfrak{F}^{\text{adv}}_{\delta,\mathbf{r}}\) & function class of nearly low-tubal-rank parameterized t-NNs and adv. version, resp. & \\ \(\|f\|_{S}\), \(\|f\|_{L_{2}}\) & empirical \(L_{2}\)-norm on sample \(S\) and population \(L_{2}\)-norm of a function \(f\), resp. & \\ \(\boldsymbol{e}=\{\varepsilon_{i}\}_{i=1}^{N}\) & _i.i.d._ Rademacher variables, i.e., \(\varepsilon_{i}\) equals to \(1\) or \(-1\) with equal probability & \\ \hline \hline \end{tabular}
\end{table}
Table 1: List of main notations

### Additional Preliminaries of t-SVD

We give additional notions and propositions about t-SVD omitted in the main body of the paper.

**Definition 6** ([18]).: _The t-transpose of \(\mathbf{T}\in\mathbb{R}^{m\times n\times\mathsf{c}}\) under the \(M\) transform in Eq. (1), denoted by \(\mathbf{T}^{\top}\), satisfies_

\[M(\mathbf{T}^{\top})_{:,:,k}=(M(\mathbf{T})_{:,:,k})^{\top},\ k=1,\cdots, \mathsf{c}.\]

**Definition 7** ([18]).: _The t-identity tensor \(\mathbf{I}\in\mathbb{R}^{m\times m\times\mathsf{c}}\) under \(M\) transform in Eq. (1) is the tensor such that each frontal slice of \(M(\mathbf{I})\) is a \(\mathsf{c}\times\mathsf{c}\) identity matrix,i.e,_

\[M(\mathbf{I})_{:,:,k}=\mathbf{I},\ k=1,\cdots,\mathsf{c}.\]

Given the appropriate dimensions, it is trivial to verify that \(\mathbf{\underline{T}}*_{M}\mathbf{I}=\mathbf{\underline{T}}\) and \(\mathbf{I}*_{M}\mathbf{\underline{T}}=\mathbf{\underline{T}}\).

**Definition 8** ([18]).: _A tensor \(\mathbf{Q}\in\mathbb{R}^{d\times d\times d_{3}}\) is t-orthogonal under \(M\) transform in Eq. (1) if it satisfies_

\[\mathbf{\underline{Q}}^{\top}*_{M}\mathbf{\underline{Q}}=\mathbf{\underline{ Q}}*_{M}\mathbf{\underline{Q}}^{\top}=\mathbf{I}.\]

**Definition 9** ([19]).: _A tensor is called f-diagonal if all its frontal slices are diagonal matrices._

**Definition 10** (Tensor t-spectral norm [28]).: _The tensor t-spectral norm of any tensor \(\mathbf{T}\) under \(M\) transform in Eq. (1) is defined as the matrix spectral norm of its \(M\)-block-diagonal matrix \(\mathbf{\widetilde{I}}_{M}\), i.e.,_

\[\left\|\mathbf{T}\right\|_{\mathrm{sp}}:=\left\|\mathbf{\widetilde{T}}_{M} \right\|.\]

**Lemma 16**.: _For any t-matrix \(\mathbf{\underline{W}}\in\mathbb{R}^{m\times n\times\mathsf{c}}\) and t-vector \(\mathbf{\underline{x}}\in\mathbb{R}^{n\times 1\times c}\), the t-product \(\mathbf{\underline{W}}*_{M}\mathbf{\underline{x}}\) defined under \(M\) transform in Eq. (1) is equivalent to a linear operator \(\mathrm{op}(\mathbf{\underline{W}})\) on \(\mathtt{unfold}(\mathbf{\underline{x}})\) in the orginal domain defined as follows_

\[\mathrm{op}(\mathbf{\underline{W}})(\mathbf{\underline{x}})=(\mathbf{M}^{-1} \otimes\mathbf{I}_{m})\bigg{[}\mathtt{bdiag}\big{(}(\mathbf{\underline{M}} \otimes\mathbf{I}_{m})\mathtt{unfold}(\mathbf{\underline{W}})\big{)}( \mathbf{\underline{M}}\otimes\mathbf{I}_{n})\bigg{]}\mathtt{unfold}(\mathbf{ \underline{x}}),\] (17)

_where \(\otimes\) denotes the Kronecker product, and the operations of \(\mathtt{unfold}(\mathbf{\underline{W}})\) and \(\mathtt{unfold}(\mathbf{\underline{x}})\) are given explicitly as follows_

\[\mathtt{unfold}(\mathbf{\underline{W}})=\begin{bmatrix}\mathbf{\underline{W} }_{:,:,1}\\ \mathbf{\underline{W}}_{:,:,2}\\ \vdots\\ \mathbf{\underline{W}}_{:,:,\mathsf{c}}\end{bmatrix}\in\mathbb{R}^{m\mathsf{c} \times n},\quad\mathtt{unfold}(\mathbf{\underline{x}})=\begin{bmatrix}\mathbf{ \underline{X}}_{:,1},\\ \mathbf{\underline{X}}_{:,1},2\\ \vdots\\ \mathbf{\underline{X}}_{:,1},\mathsf{c}\end{bmatrix}\in\mathbb{R}^{n\mathsf{c}}.\]

Since Eq. (17) is a straightforward reformulation of the definition of t-product in [36, Definition 6.3], the proof is simply omitted.

According to Lemma 16, we have the following remark on the relationship between t-NNs and fully connected neural networks (FNNs).

**Remark** (Connnection with FNNs).: _The t-NNs and FNNs can be treated as special cases of each other._

1. _When the channel number_ \(\mathsf{c}=1\)_, the t-product becomes to standard matrix multi-lication and the proposed t-NN predictor Eq. (_5_) degenerates to an_ \((L+1)\)_-layer FNN, which means the FNN is a special case of the t-NN._
2. _On the other hand, by the definition of t-product, the t-NN_ \(f(\cdot;\mathbf{\underline{W}})\) _in Eq. (_5_) has the compounding representation as an FNN:_ \[f_{\mathbf{\underline{W}}}=\mathbf{w}\circ\sigma\circ\mathrm{op}(\mathbf{ \underline{W}}^{(L)})\circ\sigma\circ\mathrm{op}(\mathbf{\underline{W}}^{(L-1)} )\circ\cdots\circ\sigma\circ\mathrm{op}(\mathbf{\underline{W}}^{(1)}).\] _Thus, t-NN can also be seen as a special case of FNN._Standarad and Adversarial Generalization Bounds for t-NNs

### Standard Generalization Bound for t-NNs

**Lemma 17**.: _Consider the ReLU activation. For any t-vector-valued function set \(\mathcal{H}\) and any convex and monotonically increasing function \(g:\mathbb{R}\rightarrow[0,\infty)\),_

\[\mathbb{E}_{\boldsymbol{\varepsilon}}\left[\sup_{\mathbf{h}\in \mathcal{H},\underline{\mathbf{W}}:\left\|\underline{\mathbf{W}}\right\|_{ \text{F}}\leq R}g\left(\left\|\sum_{i=1}^{N}\varepsilon_{i}\sigma(\underline{ \mathbf{W}}*_{M}\mathbf{h}(\underline{\mathbf{x}}_{i}))\right\|_{\text{F}} \right)\right]\leq 2\mathbb{E}_{\boldsymbol{\varepsilon}}\left[\sup_{\mathbf{h}\in \mathcal{H}}g\left(R\left\|\sum_{i=1}^{N}\varepsilon_{i}\mathbf{h}( \underline{\mathbf{x}}_{i})\right\|_{\text{F}}\right)\right],\]

_where \(R>0\) is a constant._

Proof.: This lemma is a direct corollary of Lemma 1 in Ref. [8] by using Eq. (17). 

Proof of Lemma 3.: According to Lemma 29, we can upper bound the generalization error of \(\ell\circ f\) for any \(f\in\mathfrak{F}\) through the (empirical) Rademacher complexity \(\hat{R}_{S}(\ell\circ\mathfrak{F})\) where \(\ell\circ\mathfrak{F}:=\{\ell\circ f\mid f\in\mathfrak{F}\}\). Further regarding the \(L_{\ell}\)-Lipschitzness8 of the loss function \(\ell\), we have \(\hat{R}_{S}(\ell\circ\mathfrak{F})\leq L_{\ell}\hat{R}_{S}(\mathfrak{F})\) by the Talagrand's contraction lemma (Lemma 30). Then, it remains to bound \(\hat{R}_{S}(\mathfrak{F})\).

Footnote 8: This is a natural consequence of _(A.2)_ in Assumption 2. See Eq. (7).

To upper bound \(\hat{R}_{S}(\mathfrak{F})\), we follow the proof of [8, Theorem 1]. By Jensen's inequality, the (scaled) Rademacher complexity \(N\hat{R}_{S}(\mathfrak{F})=\mathbb{E}_{\boldsymbol{\varepsilon}}\sup_{f\in \mathfrak{F}}\sum_{i=1}^{N}\varepsilon_{i}f(\underline{\mathbf{x}}_{i})\) satisfies

\[\frac{1}{\lambda}\log\exp\left(\lambda\cdot\mathbb{E}_{\boldsymbol{\varepsilon }}\sup_{f\in\mathfrak{F}}\sum_{i=1}^{N}\varepsilon_{i}f(\underline{\mathbf{x }}_{i})\right)\leq\frac{1}{\lambda}\log\left(\mathbb{E}_{\boldsymbol{ \varepsilon}}\sup_{f\in\mathfrak{F}}\exp\lambda\sum_{i=1}^{N}\varepsilon_{i}f (\underline{\mathbf{x}}_{i})\right),\] (18)

where \(\lambda>0\) is an arbitrary parameter. Then, we can use a "peeling" argument [8, 37] as follows.

The Rademacher complexity can be upper bounded as

\[N\hat{R}_{S}(\mathfrak{F}) =\mathbb{E}_{\boldsymbol{\varepsilon}}\sup_{\begin{subarray}{c} \mathbf{f}^{(L)},\left\|\mathbf{W}\right\|_{\text{L}}\leq B_{w}\end{subarray}} \sum_{i=1}^{N}\varepsilon_{i}\mathbf{w}^{\top}\mathbf{f}^{(L)}(\underline{ \mathbf{x}}_{i})\] \[\leq\frac{1}{\lambda}\log\mathbb{E}_{\boldsymbol{\varepsilon}} \sup_{\begin{subarray}{c}\|\underline{\mathbf{W}}\|_{\text{L}}\leq B_{L}\\ \|\underline{\mathbf{W}}^{(L)}\|_{\text{F}}\leq B_{L}\end{subarray}}\exp\left( B_{w}\lambda\left\|\sum_{i=1}^{N}\varepsilon_{i}\sigma\bigg{(}\underline{ \mathbf{W}}^{(L)}*_{M}\mathbf{f}^{(L-1)}(\underline{\mathbf{x}}_{i})\bigg{)} \right\|_{\text{F}}\right)\] \[\leq\frac{1}{\lambda}\log\left(2\cdot\mathbb{E}_{\boldsymbol{ \varepsilon}}\sup_{\begin{subarray}{c}\mathbf{f}^{(L-2)}\\ \|\underline{\mathbf{W}}^{(L-1)}\|_{\text{F}}\leq B_{L-1}\end{subarray}}\exp \left(B_{w}B_{L}\lambda\left\|\sum_{i=1}^{N}\varepsilon_{i}\bigg{(}\underline{ \mathbf{W}}^{(L-1)}*_{M}\mathbf{f}^{(L-2)}(\underline{\mathbf{x}}_{i})\bigg{)} \right\|_{\text{F}}\right)\right)\] \[\leq\cdots\] \[\leq\frac{1}{\lambda}\log\left(2^{L-1}\cdot\mathbb{E}_{ \boldsymbol{\varepsilon}}\sup_{\begin{subarray}{c}\|\underline{\mathbf{W}}^{(1 )}\|_{\text{F}}\leq B_{1}\end{subarray}}\exp\left(B_{w}\prod_{l=2}^{L}B_{l} \lambda\left\|\sum_{i=1}^{N}\varepsilon_{i}\bigg{(}\underline{\mathbf{W}}^{(1 )}*_{M}\mathbf{f}^{(0)}(\underline{\mathbf{x}}_{i})\bigg{)}\right\|_{\text{F}} \right)\right)\] \[\leq\frac{1}{\lambda}\log\left(2^{L}\cdot\mathbb{E}_{\boldsymbol {\varepsilon}}\exp\left(B_{w}\prod_{l=1}^{L}B_{l}\lambda\left\|\sum_{i=1}^{N} \varepsilon_{i}\underline{\mathbf{x}}_{i}\right\|_{\text{F}}\right)\right).\] (19)

Letting \(B_{\underline{\mathbf{W}}}=B_{w}\prod_{l=1}^{L}B_{l}\), define a random variable

\[Z=B_{\underline{\mathbf{W}}}\cdot\left\|\sum_{i=1}^{N}\varepsilon_{i} \underline{\mathbf{x}}_{i}\right\|_{\text{F}}\]as a function of random variables \(\{\varepsilon_{i}\}_{i}\). Then

\[\frac{1}{\lambda}\log\left(2^{L}\cdot\mathbb{E}\exp\lambda Z\right)=\frac{L\log 2 }{\lambda}+\frac{1}{\lambda}\log\left(\mathbb{E}\exp\lambda(Z-\mathbb{E}Z) \right)+\mathbb{E}Z.\] (20)

By Jensen's inequality, \(\mathbb{E}Z\) can be upper bounded by

\[\mathbb{E}Z =B_{\underline{\mathbf{W}}}\mathbb{E}\left\|\sum_{i=1}^{N} \varepsilon_{i}\mathbf{x}_{i}\right\|_{\mathrm{F}}\leq B_{\underline{ \mathbf{W}}}\sqrt{\mathbb{E}\left\|\sum_{i=1}^{N}\varepsilon_{i}\mathbf{x}_{i }\right\|_{\mathrm{F}}^{2}}=B_{\underline{\mathbf{W}}}\sqrt{\mathbb{E}_{ \boldsymbol{\varepsilon}}\left[\sum_{i,j=1}^{N}\varepsilon_{i}\varepsilon_{j} (\texttt{vec}(\underline{\mathbf{x}}_{i}))^{\top}(\texttt{vec}(\underline{ \mathbf{x}}_{j}))\right]}\] \[=B_{\underline{\mathbf{W}}}\sqrt{\sum_{i=1}^{N}\left\|\underline {\mathbf{x}}_{i}\right\|_{\mathrm{F}}^{2}}.\]

To handle the \(\log\left(\mathbb{E}\exp\lambda(Z-\mathbb{E}Z)\right)\) term in Eq. (20), note that \(Z\) is a deterministic function of the _i.i.d._ random variables \(\{\varepsilon_{i}\}_{i}\), and satisfies

\[Z(\varepsilon_{1},\cdots,\varepsilon_{i},\cdots,\varepsilon_{m})-Z( \varepsilon_{1},\cdots,-\varepsilon_{i},\cdots,\varepsilon_{m})\leq 2B_{ \underline{\mathbf{W}}}\left\|\underline{\mathbf{x}}_{i}\right\|_{\mathrm{F}}.\]

This means that \(Z\) satisfies a bounded-difference condition, which by the proof of [5, Theorem 6.2], implies that \(Z\) is sub-Gaussian, with the variance factor

\[v=\frac{1}{4}(2B_{\underline{\mathbf{W}}}\left\|\underline{\mathbf{x}}_{i} \right\|_{\mathrm{F}})^{2}=B_{\underline{\mathbf{W}}}^{2}\sum_{i=1}^{N}\left\| \underline{\mathbf{x}}_{i}\right\|_{\mathrm{F}}^{2}.\]

and satisfies

\[\frac{1}{\lambda}\log\left(\mathbb{E}\exp\lambda(Z-\mathbb{E}Z)\right)\leq \frac{1}{\lambda}\frac{\lambda^{2}B_{\underline{\mathbf{W}}}^{2}\sum_{i=1}^{N} \left\|\underline{\mathbf{x}}_{i}\right\|_{\mathrm{F}}^{2}}{2}=\frac{\lambda B _{\underline{\mathbf{W}}}^{2}\sum_{i=1}^{N}\left\|\underline{\mathbf{x}}_{i} \right\|_{\mathrm{F}}^{2}}{2}.\]

Choosing \(\lambda=\frac{\sqrt{2L\log 2}}{B_{\underline{\mathbf{W}}}\sqrt{\sum_{i=1}^{N} \left\|\underline{\mathbf{x}}_{i}\right\|_{\mathrm{F}}^{2}}}\) and using the above inequality, we get that Eq. (19) can be upper bounded as follows

\[\frac{1}{\lambda}\log\left(2^{L}\cdot\mathbb{E}\exp\lambda Z\right) \leq\mathbb{E}Z+\sqrt{2L\log 2}B_{\underline{\mathbf{W}}}\sqrt{ \sum_{i=1}^{N}\left\|\underline{\mathbf{x}}_{i}\right\|_{\mathrm{F}}^{2}}\] \[\leq(\sqrt{2L\log 2}+1)B_{\underline{\mathbf{W}}}\sqrt{\sum_{i=1}^{N} \left\|\underline{\mathbf{x}}_{i}\right\|_{\mathrm{F}}^{2}}.\]

Further applying Lemma 29 completes the proof. 

### Adversarial Generalization Bound for t-NNs

Proof of Theorem 5.: According to Theorem 2 and Eq. (4) in Ref. [3], the adversarial generalization gap of \(\ell\circ f\) for any \(f\in\mathfrak{F}\) with \(L_{\ell}\)-Lipschitz continuous loss function \(\ell\) satisfying Assumption 2 can be upper bounded by \(L_{\ell}\hat{R}_{S}(\mathfrak{F}^{\mathrm{adv}})\), where \(\hat{R}_{S}(\mathfrak{F}^{\mathrm{adv}})\) is the empirical Rademacher complexity of the adversarial version \(\mathfrak{F}^{\mathrm{adv}}\) of the function set \(\mathfrak{F}\) defined as follows

\[\mathfrak{F}^{\mathrm{adv}}:=\{\tilde{f}:(\underline{\mathbf{x}},y)\mapsto \min_{R_{i}(\underline{\mathbf{x}}-\mathbf{x}^{\prime})\leq\xi}yf(\underline {\mathbf{x}}^{\prime})\;\big{|}\;f\in\mathfrak{F}\}.\] (21)

To bound \(\hat{R}_{S}(\mathfrak{F}^{\mathrm{adv}})\), we use the Dudley's inequality (Lemma 31) which requires to compute the covering number of \(\mathfrak{F}^{\mathrm{adv}}\).

Let \(\mathbf{C}_{l}\) be the \(\delta_{l}\)-covering of \(\{\underline{\mathbf{W}}^{(l)}\;|\;\left\|\underline{\mathbf{W}}^{(l)}\right\| _{\mathrm{F}}\leq B_{l}\},\;\forall l=1,\cdots,L\). Consider the following subset of \(\mathfrak{F}\) whose t-matrix weights are all in \(\mathbf{C}_{l}\):

\[\mathfrak{F}_{c}:=\left\{f_{c}:\mathbf{x}\mapsto f_{\mathcal{W}_{c}}( \underline{\mathbf{x}})\;|\;\mathcal{W}_{c}=(\mathbf{w},\underline{\mathbf{W}} ^{(1)}_{c},\cdots,\underline{\mathbf{W}}^{(L)}_{c}),\;\underline{\mathbf{W}}^ {(l)}_{c}\in\mathbf{C}_{l}\right\}\]with adversarial version

\[\mathfrak{F}_{c}^{\text{adv}}:=\left\{\tilde{f}_{c}:(\underline{\mathbf{x}},y) \mapsto\inf_{R_{t}(\underline{\mathbf{x}}-\mathbf{x}^{\prime})\leq\xi}yf_{c}( \underline{\mathbf{x}}^{\prime})\mid f_{c}\in\mathfrak{F}_{c}\right\}.\]

For all \(\tilde{f}\in\mathfrak{F}^{\text{adv}}\), we need to find the smallest distance to \(\mathfrak{F}_{c}^{\text{adv}}\), i.e. we need to calculate

\[\sup_{\tilde{f}\in\mathfrak{F}^{\text{adv}}}\inf_{\tilde{f}_{c}\in\mathfrak{F }_{c}^{\text{adv}}}\left\|\tilde{f}-\tilde{f}_{c}\right\|_{S}.\]

For all \((\underline{\mathbf{x}}_{i},y_{i})\in S\), given \(\tilde{f}\) and \(\tilde{f}_{c}\) with \(\left\|\underline{\mathbf{W}}^{(l)}-\underline{\mathbf{W}}^{(l)}_{c}\right\| _{\text{F}}\leq\delta_{l},\ l=1,\cdots,L\), consider

\[|\tilde{f}(\underline{\mathbf{x}}_{i},y_{i})-\tilde{f}_{c}(\underline{ \mathbf{x}}_{i},y_{i})|=\left|\inf_{R_{t}(\underline{\mathbf{x}}-\mathbf{x}^{ \prime})\leq\xi}y_{i}f(\mathbf{x}_{i}^{\prime})-\inf_{R_{t}(\underline{ \mathbf{x}}-\mathbf{x}^{\prime})\leq\xi}y_{i}f_{c}(\mathbf{x}_{i}^{\prime}) \right|.\]

Letting \(\underline{\mathbf{x}}_{i}^{f}=\text{arginf}_{R_{t}(\underline{\mathbf{x}}_{i }-\mathbf{x}_{i}^{\prime})\leq\xi}y_{i}f(\underline{\mathbf{x}}_{i}^{\prime})\) and \(\underline{\mathbf{x}}_{i}^{c}=\text{arginf}_{R_{t}(\underline{\mathbf{x}}_{i }-\mathbf{x}_{i}^{\prime})\leq\xi}y_{i}f_{c}(\underline{\mathbf{x}}_{i}^{ \prime})\), we have

\[|\tilde{f}(\underline{\mathbf{x}}_{i},y_{i})-\tilde{f}_{c}(\underline{ \mathbf{x}}_{i},y_{i})|=|y_{i}f(\underline{\mathbf{x}}_{i}^{f})-y_{i}f_{c}( \underline{\mathbf{x}}_{i}^{c})|.\]

Let

\[\underline{\mathbf{x}}_{i}^{\xi}=\begin{cases}\underline{\mathbf{x}}_{i}^{c}& \text{if}\quad y_{i}f(\underline{\mathbf{x}}_{i}^{f})\geq y_{i}f_{c}(\underline {\mathbf{x}}_{i}^{c})\\ \underline{\mathbf{x}}_{i}^{f}&\text{if}\quad y_{i}f(\underline{\mathbf{x}}_{i }^{c})<y_{i}f_{c}(\underline{\mathbf{x}}_{i}^{c})\end{cases}.\]

Then,

\[|\tilde{f}(\underline{\mathbf{x}}_{i},y_{i})-\tilde{f}_{c}(\underline{ \mathbf{x}}_{i},y_{i})|=|y_{i}f(\underline{\mathbf{x}}_{i}^{f})-y_{i}f_{c}( \underline{\mathbf{x}}_{i}^{g})|\leq|y_{i}f(\underline{\mathbf{x}}_{i}^{\xi} )-y_{i}f_{c}(\underline{\mathbf{x}}_{i}^{\xi})|=|f(\underline{\mathbf{x}}_{i }^{\xi})-f_{c}(\underline{\mathbf{x}}_{i}^{\xi})|.\]

Let \(g_{l}(\underline{\mathbf{x}}^{\delta})=\mathbf{w}^{\top}\texttt{vec}\left( \sigma(\underline{\mathbf{W}}_{c}^{(L)}*_{M}\sigma(\underline{\mathbf{W}}_{ c}^{(L-1)}*_{M}\cdots*_{M}\sigma(\underline{\mathbf{W}}_{c}^{(l+1)}*_{M}\sigma( \underline{\mathbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{\mathbf{W}}^{( l)}*_{M}\cdots*_{M}\sigma(\underline{\mathbf{W}}^{(l)}*_{M}\] \[\underline{\mathbf{x}}^{\delta})\cdots)))\right)\) and \(g_{0}(\underline{\mathbf{x}}^{\delta})=f_{c}(\underline{\mathbf{x}}^{\delta})\). Then, we have

\[|f(\underline{\mathbf{x}}_{i}^{\xi})-f_{c}(\underline{\mathbf{x}}_{i}^{\xi})| \leq\sum_{l=1}^{L}|g_{l}(\underline{\mathbf{x}}_{i}^{\xi})-g_{l-1}( \underline{\mathbf{x}}_{i}^{\xi})|.\]

We can see that

\[\left\|\sigma(\underline{\mathbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{ \mathbf{W}}^{(1)}*_{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots))\right\|_{ \text{F}}\leq\prod_{l^{\prime}=1}^{l}\left\|\underline{\mathbf{W}}^{(l^{ \prime})}\right\|_{\text{F}}\left\|\underline{\mathbf{x}}_{i}^{\xi}\right\|_{ \text{F}}\leq\prod_{l^{\prime}=1}^{l}B_{l}B_{x,R_{t},\xi},\]

and

\[\left\|\sigma(\underline{\mathbf{W}}_{c}^{(l+1)}*_{M}\sigma( \underline{\mathbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{\mathbf{W}}^{( l)}*_{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots))-\sigma(\underline{\mathbf{W}}^{(l+1)}*_{M} \sigma(\underline{\mathbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{ \mathbf{W}}^{(1)}*_{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots))\right\|_{\text{F}}\] \[\leq\left\|\underline{\mathbf{W}}_{c}^{(l+1)}*_{M}\sigma( \underline{\mathbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{\mathbf{W}}^{(1)}* _{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots)-\underline{\mathbf{W}}^{(l+1)}*_{M} \sigma(\underline{\mathbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{\mathbf{W}} ^{(1)}*_{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots)\right\|_{\text{F}}\] \[\leq\left\|(\underline{\mathbf{W}}_{c}^{(l+1)}-\underline{\mathbf{W }}^{(l+1)})*_{M}\sigma(\underline{\mathbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma( \underline{\mathbf{W}}^{(1)}*_{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots)\right\|_ {\text{F}}\] \[\leq\left\|\underline{\mathbf{W}}_{c}^{(l+1)}-\underline{\mathbf{W }}^{(l+1)}\right\|_{\text{F}}\left\|\sigma(\underline{\mathbf{W}}^{(l)}*_{M} \cdots*_{M}\sigma(\underline{\mathbf{W}}^{(1)}*_{M}\underline{\mathbf{x}}_{i}^{ \xi})\cdots)\right\|_{\text{F}}\] \[\leq\delta_{l}\cdot\prod_{l^{\prime}=1}^{l-1}B_{l^{\prime}}B_{x,R_ {t},\xi}.\] (22)

Then, we have

\[|g_{l}(\underline{\mathbf{x}}_{i}^{\xi})-g_{l-1}(\underline{\mathbf{x}}_{i}^{ \xi})|\leq\delta_{l}\cdot B_{\mathbf{w}}\prod_{l^{\prime}\neq l}B_{l^{\prime}}B_{x,R_{t},\xi}=\frac{\delta_{l}B_{\tilde{f}}}{B_{l}},\]

where \(B_{\tilde{f}}\) is given in Lemma 39.

Letting \(\frac{\delta_{i}B_{\bar{f}}}{B_{i}}=\frac{\epsilon}{L}\) it gives

\[\sup_{\bar{f}\in\mathfrak{F}^{\mathrm{adv}}}\inf_{\tilde{f}_{c}\in\mathfrak{F}^{ \mathrm{adv}}_{c}}\left\|\tilde{f}-\tilde{f}_{c}\right\|_{S}\leq\sum_{l=1}^{L-1 }|g_{l}(\mathbf{x}_{i}^{\xi})-g_{l-1}(\mathbf{x}_{i}^{\xi})|\leq\epsilon.\]

Then, \(\mathfrak{F}^{\mathrm{adv}}_{c}\) is an \(\epsilon\)-covering of \(\mathfrak{F}^{\mathrm{adv}}\). We further proceed by computing the \(\epsilon\)-covering number of \(\mathfrak{F}^{\mathrm{adv}}\) as follows:

\[\mathsf{N}(\mathfrak{F}^{\mathrm{adv}},\left\|\cdot\right\|_{S},\epsilon) \leq|\mathfrak{F}^{\mathrm{adv}}_{c}|=\prod_{l=1}^{L}|\mathbf{C}_{l}|\leq \prod_{l=1}^{L}\left(\frac{3B_{\bar{l}}}{\delta_{l}}\right)^{cd_{l-1}d_{l}} \leq\left(\frac{3LB_{\bar{f}}}{\epsilon}\right)^{\sum_{l=1}^{L}cd_{l-1}d_{l}}.\]

where the second inequality is due to Lemma 32.

Then, we use Dudley's integral in Lemma 31 to upper bound \(\hat{R}_{S}(\mathfrak{F}^{\mathrm{adv}})\) as follows

\[\hat{R}_{S}(\mathfrak{F}^{\mathrm{adv}}) \leq\inf_{\delta>0}\left(8\delta+\frac{12}{\sqrt{N}}\int_{\delta }^{D_{\bar{f}}/2}\sqrt{\log\mathsf{N}(\mathfrak{F}^{\mathrm{adv}},\left\|\cdot \right\|_{S},\epsilon)}\mathrm{d}\epsilon\right)\] \[\leq\inf_{\delta>0}\left(8\delta+\frac{12}{\sqrt{N}}\int_{\delta}^ {D_{\bar{f}}/2}\sqrt{\big{(}\sum_{l=1}^{L}cd_{l-1}d_{l}\big{)}\log\big{(}3LB_{ \bar{f}}/(\epsilon)\big{)}}\mathrm{d}\epsilon\right)\] \[=\inf_{\delta>0}\left(8\delta+\frac{12D_{\bar{f}}\sqrt{\sum_{l=1} ^{L}cd_{l-1}d_{l}}}{\sqrt{N}}\int_{\delta/D_{\bar{f}}}^{1/2}\sqrt{\log\big{(}3 L/(2t)\big{)}}\mathrm{d}t\right),\]

where the diameter \(D_{\bar{f}}\) of \(\mathfrak{F}^{\mathrm{adv}}\) is given in Lemma 40 and we can find from Lemma 40 that \(D_{\bar{f}}=2B_{\bar{f}}\) in our setting.

Following Ref. [55], let \(\delta\to 0\), and use integration by part, we obtain \(\int_{0}^{1/2}\sqrt{\log\big{(}3L/(2t)\big{)}}\mathrm{d}t\leq\sqrt{\log 3L}\). Hence, we have

\[\hat{R}_{S}(\mathfrak{F}^{\mathrm{adv}})\leq\frac{24B_{\mathbf{w}}\prod_{l=1 }^{L}B_{l}\bigg{(}\sqrt{\sum_{l=1}^{L}cd_{l-1}d_{l}}\bigg{)}B_{x,R_{*},\xi}}{ \sqrt{N}},\]

and the proof can be completed by using Lemma 29. 

### Generalization Bound under Exact Low-tubal-rank Parameterization

Proof of Theorem 6.: The idea is similar to the proof of Theorem 5. According to Theorem 2 and Eq. (4) in Ref. [3], the adversarial generalization gap of \(\ell\circ f\) for any \(f\in\mathfrak{F}_{\mathbf{r}}\) with \(L_{\ell}\)-Lipschitz continuous loss function \(\ell\) satisfying Assumption 2 can be upper bounded by \(L_{\bar{\ell}}\hat{R}_{S}(\mathfrak{F}^{\mathrm{adv}}_{\mathbf{r}})\), where \(\hat{R}_{S}(\mathfrak{F}^{\mathrm{adv}}_{\mathbf{r}})\) is the empirical Rademacher complexity of the adversarial version \(\mathfrak{F}^{\mathrm{adv}}_{\mathbf{r}}\) of function set \(\mathfrak{F}_{\mathbf{r}}\) defined as follows

\[\mathfrak{F}^{\mathrm{adv}}_{\mathbf{r}}:=\{\tilde{f}:(\mathbf{x},y)\mapsto \min_{R_{u}(\mathbf{z}-\mathbf{x}^{\prime})\leq\xi}yf(\mathbf{x}^{\prime}) \mid f\in\mathfrak{F}_{\mathbf{r}}\}.\] (23)

To bound \(\hat{R}_{S}(\mathfrak{F}^{\mathrm{adv}}_{\mathbf{r}})\), we first use the Dudley's inequality (Lemma 31) and compute the covering number of \(\mathfrak{F}^{\mathrm{adv}}_{\mathbf{r}}\).

Let \(\mathbf{C}_{l}\) be the \(\delta_{l}\)-covering of \(\{\underline{\mathbf{W}}^{(l)}\mid\left\|\underline{\mathbf{W}}^{(l)}\right\| _{\mathbf{F}}\leq B_{l}\) and \(r_{l}(\underline{\mathbf{W}}^{(l)})\leq r_{l}\},\;\forall l=1,\cdots,L\). Consider the following subset of \(\mathfrak{F}_{\mathbf{r}}\) whose t-matrix weights are all in \(\mathbf{C}_{l}\):

\[\mathfrak{F}_{c}:=\left\{f_{c}:\underline{\mathbf{x}}\mapsto f(\underline{ \mathbf{x}};\underline{\mathbf{W}}_{c})\mid\underline{\mathbf{W}}_{c}=( \mathbf{w},\underline{\mathbf{W}}_{c}^{(1)},\cdots,\underline{\mathbf{W}}_{c}^{ (L)}),\;\underline{\mathbf{W}}_{c}^{(l)}\in\mathbf{C}_{l}\right\}\]

with adversarial version

\[\mathfrak{F}^{\mathrm{adv}}_{c}:=\left\{\tilde{f}_{c}:(\underline{\mathbf{x}}, y)\mapsto\inf_{R_{u}(\mathbf{z}-\mathbf{x}^{\prime})\leq\xi}yf_{c}(\mathbf{x}^{ \prime})\mid f_{c}\in\mathfrak{F}_{c}\right\}.\]For all \(\tilde{f}\in\mathfrak{F}_{\textbf{r}}^{\text{adv}}\), we need to find the smallest distance to \(\mathfrak{F}_{c}^{\text{adv}}\), i.e. we need to calculate

\[\sup_{\tilde{f}\in\mathfrak{F}_{\textbf{r}}^{\text{adv}}}\inf_{\tilde{f}_{c}\in \mathfrak{F}_{c}^{\text{adv}}}\left\|\tilde{f}-\tilde{f}_{c}\right\|_{S}.\]

For all \((\textbf{x}_{i},y_{i})\in S\), given \(\tilde{f}\) and \(\tilde{f}_{c}\) with \(\left\|\underline{\textbf{W}}^{(l)}-\underline{\textbf{W}}^{(l)}_{c}\right\| _{\text{F}}\leq\delta_{l},\ l=1,\cdots,L\), consider

\[|\tilde{f}(\underline{\textbf{x}}_{i},y_{i})-\tilde{f}_{c}(\underline{\textbf{ x}}_{i},y_{i})|=\left|\inf_{R_{\textbf{x}}(\underline{\textbf{x}}-\textbf{x}^{ \prime})\leq\xi}y_{i}f(\underline{\textbf{x}}_{i}^{\prime})-\inf_{R_{\textbf{ x}}(\underline{\textbf{x}}-\textbf{x}^{\prime})\leq\xi}y_{i}f_{c}(\underline{ \textbf{x}}_{i}^{\prime})\right|.\]

Letting \(\textbf{x}_{i}^{f}=\text{arginf}_{R_{\textbf{x}}(\textbf{x}_{i}-\textbf{x}_ {i}^{\prime})\leq\xi}y_{i}f(\textbf{x}_{i}^{\prime})\) and \(\textbf{x}_{i}^{c}=\text{arginf}_{R_{\textbf{x}}(\textbf{x}_{i}-\textbf{x}_ {i}^{\prime})\leq\xi}y_{i}f_{c}(\textbf{x}_{i}^{\prime})\), we have

\[|\tilde{f}(\textbf{x}_{i},y_{i})-\tilde{f}_{c}(\textbf{x}_{i},y_{i})|=|y_{i}f (\textbf{x}_{i}^{f})-y_{i}f_{c}(\textbf{x}_{i}^{c})|.\]

Let

\[\textbf{x}_{i}^{\xi}=\begin{cases}\textbf{x}_{i}^{c}&\text{if}\quad y_{i}f( \textbf{x}_{i}^{f})\geq y_{i}f_{c}(\textbf{x}_{i}^{c})\\ \textbf{x}_{i}^{f}&\text{if}\quad y_{i}f(\textbf{x}_{i}^{c})<y_{i}f_{c}( \textbf{x}_{i}^{c})\end{cases}\]

Then,

\[|\tilde{f}(\textbf{x}_{i},y_{i})-\tilde{f}_{c}(\textbf{x}_{i},y_{i})|=|y_{i}f (\textbf{x}_{i}^{f})-y_{i}f_{c}(\textbf{x}_{i}^{g})|\leq|y_{i}f(\textbf{x}_{i }^{\xi})-y_{i}f_{c}(\textbf{x}_{i}^{\xi})|=|f(\textbf{x}_{i}^{\xi})-f_{c}( \textbf{x}_{i}^{\xi})|.\]

Let \(g_{l}(\textbf{x}^{\delta})=\textbf{w}^{\top}{}_{\text{vec}}\coth(\sigma( \underline{\textbf{W}}^{(L)}_{c}*_{M}\sigma(\underline{\textbf{W}}^{(L-1)}_{ c}*_{M}\cdots*_{M}\sigma(\underline{\textbf{W}}^{(l+1)}_{c}*_{M}\sigma( \underline{\textbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{\textbf{W}}^{(l )}*_{M}\cdots*_{M}\sigma(\underline{\textbf{W}}^{(1)}*_{M}\]

\(\textbf{x}^{\delta})\cdots)))\) and \(g_{0}(\textbf{x}^{\delta})=f_{c}(\textbf{x}^{\delta})\). Then, we have

\[|f(\textbf{x}_{i}^{\xi})-f_{c}(\textbf{x}_{i}^{\xi})|\leq\sum_{l=1}^{L}|g_{l}( \textbf{x}_{i}^{\xi})-g_{l-1}(\textbf{x}_{i}^{\xi})|.\]

We can see that

\[\left\|\sigma(\underline{\textbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{ \textbf{W}}^{(1)}*_{M}\textbf{x}_{i}^{\xi})\cdots))\right\|_{\text{F}}\leq \prod_{l^{\prime}=1}^{l}\left\|\underline{\textbf{W}}^{(l^{\prime})}\right\|_ {\text{F}}\left\|\textbf{x}_{i}^{\xi}\right\|_{\text{F}}\leq\prod_{l^{\prime}= 1}^{l}B_{l}B_{x,R_{\textbf{x}},\xi}\]

and

\[\left\|\sigma(\underline{\textbf{W}}^{(l+1)}_{c}*_{M}\sigma( \underline{\textbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{\textbf{W}}^{(1 )}*_{M}\underline{\textbf{x}}_{i}^{\xi})\cdots))-\sigma(\underline{\textbf{W} }^{(l+1)}*_{M}\sigma(\underline{\textbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma( \underline{\textbf{W}}^{(1)}*_{M}\underline{\textbf{x}}_{i}^{\xi})\cdots)) \right\|_{\text{F}}\] \[\leq\left\|\underline{\textbf{W}}^{(l+1)}_{c}*_{M}\sigma( \underline{\textbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{\textbf{W}}^{(1 )}*_{M}\underline{\textbf{x}}_{i}^{\xi})\cdots)-\underline{\textbf{W}}^{(l+1)} *_{M}\sigma(\underline{\textbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{ \textbf{W}}^{(1)}*_{M}\underline{\textbf{x}}_{i}^{\xi})\cdots)\right\|_{\text{F}}\] \[\leq\left\|\underline{\textbf{W}}^{(l+1)}_{c}-\underline{\textbf{W} }^{(l+1)}\right\|_{\text{F}}\left\|\sigma(\underline{\textbf{W}}^{(l)}*_{M} \cdots*_{M}\sigma(\underline{\textbf{W}}^{(1)}*_{M}\underline{\textbf{x}}_{i}^{ \xi})\cdots)\right\|_{\text{F}}\] \[\leq\delta_{l}\cdot\prod_{l^{\prime}=1}^{l-1}B_{l^{\prime}}B_{x,R_ {\textbf{x}},\xi}.\]

Then, we have

\[|g_{l}(\underline{\textbf{x}}_{i}^{\xi})-g_{l-1}(\underline{\textbf{x}}_{i}^{ \xi})|\leq\delta_{l}\cdot B_{\textbf{w}}\prod_{l^{\prime}\neq l}B_{l^{\prime}}B_{ x,R_{\textbf{x}},\xi}=\frac{\delta_{l}B_{\tilde{f}}}{B_{l}}.\]

Letting \(\frac{\delta_{l}B_{\tilde{f}}}{B_{l}}=\frac{\epsilon}{L}\) gives

\[\sup_{\tilde{f}\in\mathfrak{F}_{\textbf{r}}^{\text{adv}}}\inf_{\tilde{f}_{c}\in \mathfrak{F}_{\textbf{r}}^{\text{adv}}}\left\|\tilde{f}-\tilde{f}_{c}\right\|_{S} \leq\sum_{l=1}^{L-1}|g_{l}(\textbf{x}_{i}^{\xi})-g_{l-1}(\underline{\textbf{x}}_{i }^{\xi})|\leq\epsilon.\]Then, \(\mathfrak{F}_{\mathbf{r}}^{\text{adv}}\) is an \(\epsilon\)-covering of \(\mathfrak{F}_{\mathbf{r}}^{\text{adv}}\). We further proceed by computing the \(\epsilon\)-covering number of \(\mathfrak{F}_{\mathbf{r}}^{\text{adv}}\) as follows:

\[\mathsf{N}(\mathfrak{F}_{\mathbf{r}}^{\text{adv}},\left\|\cdot\right\|_{S}, \epsilon)\leq|\mathfrak{F}_{\mathbf{c}}^{\text{adv}}|=\prod_{l=1}^{L}| \mathbf{C}_{l}|\leq\prod_{l=1}^{L}\left(\frac{9B_{l}}{\delta_{l}}\right)^{ \mathfrak{c}r_{l}(d_{l-1}+d_{l}+1)}\leq\left(\frac{9LB_{\tilde{f}}}{\epsilon }\right)^{\sum_{l=1}^{L}\mathfrak{c}r_{l}(d_{l-1}+d_{l}+1)},\]

where the inequality \((i)\) holds due to Lemma 33.

Then, we use Dudley's integral to upper bound \(\hat{R}_{S}(\mathfrak{F}_{\mathbf{r}}^{\text{adv}})\) as follows

\[\hat{R}_{S}(\mathfrak{F}_{\mathbf{r}}^{\text{adv}}) \leq\inf_{\delta>0}\left(8\delta+\frac{12}{\sqrt{N}}\int_{ \delta}^{D_{\tilde{f}}/2}\sqrt{\log\mathsf{N}(\mathfrak{F}_{\mathbf{r}}^{ \text{adv}},\left\|\cdot\right\|_{S},\epsilon)}\text{d}\epsilon\right)\] \[\leq\inf_{\delta>0}\left(8\delta+\frac{12}{\sqrt{N}}\int_{\delta} ^{D_{\tilde{f}}/2}\sqrt{\sum_{l=1}^{L}\mathsf{c}r_{l}(d_{l-1}+d_{l}+1)\log \left(9LB_{\tilde{f}}/\epsilon\right)}\text{d}\epsilon\right)\] \[=\inf_{\delta>0}\left(8\delta+\frac{12D_{\tilde{f}}\sqrt{\sum_{l= 1}^{L}\mathsf{c}r_{l}(d_{l-1}+d_{l}+1)}}{\sqrt{N}}\int_{\delta/D_{\tilde{f}}}^ {1/2}\sqrt{\log\left(9L/(2t)\right)}\text{d}t\right),\]

where the diameter \(D_{\tilde{f}}\) of \(\mathfrak{F}^{\text{adv}}\) is given in Lemma 40 and we have \(D_{\tilde{f}}=2B_{\tilde{f}}\). Following Ref. [55], let \(\delta\to 0\), and use interation by part, we obtain\(\int_{0}^{1/2}\sqrt{\log\left(9L/(2t)\right)}\text{d}t\leq\sqrt{\log 9L}\). Further applying Lemma 29 completes the proof. 

## Appendix D Implicit bias towards low-rankness in the transformed domain

Recent research has shown that GF maximizes the margin of homogeneous networks during standard training, which leads to an implicit bias towards margin maximization [16, 30]. Moreover, it has been demonstrated that this implicit bias also extends to adversarial margin maximization during adversarial training of multi-homogeneous fully connected neural networks with exponential loss [29]. Our analysis builds on these findings by showing that this implicit bias also holds for adversarial training of t-NNs when the adversarial perturbation is scale invariant [29].

First, it is straightforward to see that any t-NN \(f\in\mathfrak{F}\) is homogeneous as follows

\[f(\underline{\mathbf{x}};a\underline{\mathbf{W}})=a^{L+1}f(\underline{ \mathbf{x}};\underline{\mathbf{W}}),\] (24)

for any positive constant \(a\).

**Lemma 18** (Euler's theorem on t-NNs).: _For any t-NN \(f\in\mathfrak{F}\), we have_

\[\left\langle\frac{\partial f(\underline{\mathbf{x}};\underline{\mathbf{W}})}{ \partial\underline{\mathbf{W}}},\underline{\mathbf{W}}\right\rangle=(L+1)f( \underline{\mathbf{x}};\underline{\mathbf{W}}).\] (25)

Proof.: By taking derivatives with respect to \(a\) on both sides of Eq. (24), we obtain

\[\left\langle\frac{\partial f(\underline{\mathbf{x}};a\underline{\mathbf{W}})} {\partial(a\underline{\mathbf{W}})},\frac{\text{d}(a\underline{\mathbf{W}})}{ \text{d}a}\right\rangle=(L+1)a^{L}f(\underline{\mathbf{x}};\underline{ \mathbf{W}}),\]

which immediately results in Eq. (25). 

In this section, we follow the setting of Ref. [29] where the adversarial perturbation \(\underline{\delta}_{i}\) is scale invariant. As Lemma 7 shows, \(l_{2}\)-FGM [34], FGSM [9], \(l_{2}\)-PGD and \(l_{\infty}\)-PGD [31] perturbations for the t-NNs are all scale invariant.

Proof of Lemma 7.: Note that by taking derivatives with respect to \(\underline{\mathbf{x}}\) on both sides of Eq. (24), we have

\[\frac{\partial f(\underline{\mathbf{x}};a\underline{\mathbf{W}})}{\partial \underline{\mathbf{x}}}=a^{L+1}\frac{\partial f(\underline{\mathbf{x}}; \underline{\mathbf{W}})}{\partial\underline{\mathbf{x}}},\] (26)

Therefore, any \(\frac{\partial f(\underline{\mathbf{x}};\underline{\mathbf{W}})}{\partial \underline{\mathbf{x}}}\) is positive homogeneous. Then, for any non-zero \(\underline{\mathbf{z}}=\frac{\partial f(\underline{\mathbf{x}};\underline{ \mathbf{W}})}{\partial\underline{\mathbf{z}}}\), we prove Lemma 7 in the following cases:* \(l_{2}\)-FGM perturbation [34]. The \(l_{2}\)-FGM perturbtion is defined as \(\underline{\delta}_{\text{FGM}}(\underline{\mathbf{W}})=\xi y\tilde{\ell}^{ \prime}\underline{\mathbf{z}}\norm{y\tilde{\ell}^{\prime}\mathbf{z}}_{\text{F}}^ {-1}=-\xi y\underline{\mathbf{z}}\norm{\mathbf{z}}_{\text{F}}^{-1},\) because \(\tilde{\ell}^{\prime}\leq 0\). Using Eq. (26), we have \[\underline{\delta}_{\text{FGM}}(a\underline{\mathbf{W}})=-\frac{\xi y\cdot a^{ L+1}\underline{\mathbf{z}}}{\norm{a^{L+1}\underline{\mathbf{z}}}_{\text{F}}}= \underline{\delta}_{\text{FGM}}(\underline{\mathbf{W}}).\]
* FGSM perturbations [9]. The FGSM perturbtion is taken as \(\underline{\delta}_{\text{FGSM}}(\underline{\mathbf{W}})=\xi\texttt{sgn}( \xi y\tilde{\ell}^{\prime}\underline{\mathbf{z}})\). Using Eq. (26), we have \[\underline{\delta}_{\text{FGSM}}(a\underline{\mathbf{W}})=\xi\texttt{sgn}( \xi y\tilde{\ell}^{\prime}a^{L+1}\underline{\mathbf{z}})=\underline{\delta}_ {\text{FGSM}}(\underline{\mathbf{W}}).\]
* \(l_{2}\)-PGD perturbation [31]. The \(l_{2}\)-PGD perturbtion is taken as \[\underline{\delta}_{\text{FGD}}^{j+1}(\underline{\mathbf{W}})=\mathfrak{P}_{ \mathbb{B}_{2}(0,\xi)}\left[\underline{\delta}_{\text{FGD}}^{j}(\underline{ \mathbf{W}})-\rho\frac{\xi y\underline{\mathbf{z}}}{\norm{\mathbf{z}}_{\text{F }}}\right],\] (27) where \(j\) is the attack step, \(\mathfrak{P}_{\mathbb{B}_{2}(\xi)}\) is the projector onto \(l_{2}\)-norm ball of radius \(\xi\), and \(\rho\) is the learning rate. We prove by induction. For \(j=0\), we have \[\underline{\delta}_{\text{FGD}}^{1}(a\underline{\mathbf{W}})=\mathfrak{P}_{ \mathbb{B}_{2}(0,\xi)}\left[-\rho\frac{\xi ya^{L+1}\underline{\mathbf{z}}}{ \norm{a^{L+1}\underline{\mathbf{z}}}_{\text{F}}}\right]=\underline{\delta}_{ \text{FGD}}^{1}(\underline{\mathbf{W}}).\] If we have \(\underline{\delta}_{\text{FGD}}^{j}(a\underline{\mathbf{W}})=\underline{ \delta}_{\text{FGD}}^{j}(\underline{\mathbf{W}})\), then for \(j+1\), we have \[\begin{split}\underline{\delta}_{\text{FGD}}^{j+1}(a\underline{ \mathbf{W}})&=\mathfrak{P}_{\mathbb{B}_{2}(0,\xi)}\left[ \underline{\delta}_{\text{FGD}}^{j}(a\underline{\mathbf{W}})-\rho\frac{\xi ya ^{L+1}\underline{\mathbf{z}}}{\norm{a^{L+1}\underline{\mathbf{z}}}_{\text{F}} }\right]\\ &=\mathfrak{P}_{\mathbb{B}_{2}(0,\xi)}\left[\underline{\delta}_{ \text{FGD}}^{j}(\underline{\mathbf{W}})-\rho\frac{\xi y\underline{\mathbf{z}} }{\norm{\mathbf{z}}_{\text{F}}}\right]\\ &=\underline{\delta}_{\text{FGD}}^{j+1}(\underline{\mathbf{W}}). \end{split}\] (28)
* \(l_{\infty}\)-PGD perturbation [31]. Since the scale invariance of this pertubation can be proved very similarly to that of \(l_{2}\)-PGD perturbations, we just omit it.

For an original example \(\underline{\mathbf{x}}_{i}\), the margin for its adversarial example \(\underline{\mathbf{x}}_{i}+\underline{\delta}_{i}(\underline{\mathbf{W}})\) is defined as \(\tilde{q}_{i}(\underline{\mathbf{W}}):=y_{i}f(\underline{\mathbf{x}}_{i}+ \underline{\delta}_{i}(\underline{\mathbf{W}});\underline{\mathbf{W}})\); for sample \(S=\{(\underline{\mathbf{x}}_{i},y_{i})\}_{i=1}^{N}\), the margin for the \(N\) corresponding examples is denoted by \(\tilde{q}_{m}(\underline{\mathbf{W}})\) where \(m\in\text{argmin}_{m=1,\cdots,N}\,y_{i}f(\underline{\mathbf{x}}_{i}+\underline {\delta}_{i}(\underline{\mathbf{W}}):\underline{\mathbf{W}})\).

Let \(\rho=\norm{\mathbf{W}}_{\text{F}}\) for simplicity. We use the normalized parameter \(\underline{\mathbf{W}}:=\underline{\mathbf{W}}/\rho\) to denote the direction of the weights \(\underline{\mathbf{W}}\). We introduce the normalized margin of \((\underline{\mathbf{x}}_{i},y_{i})\) as \(\tilde{q}_{i}(\underline{\mathbf{W}}):=\tilde{q}_{i}(\underline{\mathbf{W}} )=\tilde{q}_{i}(\underline{\mathbf{W}})\rho^{-(L+1)}\), and similarly define the normalized robust margin of the sample \(S\) as \(\hat{q}_{m}(\underline{\mathbf{W}}):=\tilde{q}_{m}(\underline{\mathbf{W}})= \tilde{q}_{m}(\underline{\mathbf{W}})\rho^{-(L+1)}\).

Note that the adversarial empirical risk can be written as \(\hat{\mathcal{L}}^{\text{adv}}=\frac{1}{N}\sum_{i=1}^{N}e^{-\int(\tilde{q}_{i} (\underline{\mathbf{W}}))}\). Motivated by [30] which uses the LogSumExp function to smoothly approximate the normalized standard margin, we define the smoothed normalized margin \(\tilde{\gamma}\) as follows.

**Definition 11** (Smoothed normalized robust margin).: _For a loss function9\(\ell\) satisfying Assumption 2, the smoothed normalized robust margin is defined as_

Footnote 9: In this paper, the loss function satisfying Assumption 2 belongs to the class of _margin-based loss function_[42, Definition 2.24]. That is, although the loss function \(\ell(f(\underline{\mathbf{x}}),y)\) is a binary function of \(f(\underline{\mathbf{x}})\) and \(y\), there is a unary function \(\mathsf{I}(\cdot)\), such that \(\ell(f(\underline{\mathbf{x}}),y)=\mathsf{I}(yf(\underline{\mathbf{x}}))\). With a slight abuse of notation, we simply use \(\ell^{-1}(\cdot)\) to denote \(\mathsf{I}^{-1}(\cdot)\), i.e., if \(z=\ell(f(\underline{\mathbf{x}}),y)\) then we have \(\ell^{-1}(z)=yf(\underline{\mathbf{x}})\).

\[\tilde{\gamma}(\underline{\mathbf{W}}):=\frac{\ell^{-1}(N\hat{\mathcal{L}}^{ \text{adv}})}{\rho^{L+1}}=\frac{\mathfrak{g}(\frac{1}{N\hat{\mathcal{L}}^{\text {adv}}})}{\rho^{L+1}}=\frac{\mathfrak{g}\left(-\log\left(\sum_{i=1}^{N}e^{-\int( \tilde{q}_{i}(\underline{\mathbf{W}}))}\right)\right)}{\rho^{L+1}}.\] (29)To better understand the relation between the normalized sample robust margin \(\hat{q}_{m}\) and the smoothed normalized robust margin \(\hat{\gamma}\), we provide the following lemma.

**Lemma 19** (Adapted from Lemma A.5 of Ref. [30]).: _Under Assumption 2, we have the following properties about the robust marin \(\tilde{q}_{m}\):_

* \(\mathfrak{f}(\tilde{q}_{m})-\log N\leq\log\frac{1}{N\mathcal{L}^{\text{adv}} }\leq\mathfrak{f}(\tilde{q}_{m}).\)__
* _If_ \(\log\frac{1}{N\mathcal{L}^{\text{adv}}}>\mathfrak{f}(b_{\mathfrak{f}})\)_, then there exists_ \(\xi\in(\mathfrak{f}(\tilde{q}_{m})-\log N,\mathfrak{f}(\tilde{q}_{m}))\cap(b_ {\mathfrak{f}},\infty)\) _such that_ \[\hat{q}_{m}-\rho^{-(L+1)}\mathfrak{g}^{\prime}(\xi)\log N\leq\tilde{\gamma} \leq\hat{q}_{m},\] _which shows the smoothed normalized margin_ \(\tilde{\gamma}\) _is a rough approximation of the normalized robust margin_ \(\hat{q}_{m}\)_._
* _For a sequence_ \(\{\underline{\mathbf{W}}_{s}\mid s\in\mathbb{N}\}\)_, if_ \(\hat{\mathcal{L}}^{\text{adv}}(\underline{\mathbf{W}}_{s})\to 0\)_, then_ \(|\tilde{\gamma}(\underline{\mathbf{W}}_{s})-\hat{q}_{m}(\underline{\mathbf{W} }_{s})|\to 0\)_._

### Convergence to KKT points of Euclidean norm minimization in direction

The KKT condition for the optimization problem Eq. (14) are

\[\begin{split}\underline{\mathbf{W}}-\sum_{i=1}^{N}\lambda_{i} \frac{\partial\hat{q}_{i}}{\partial\underline{\mathbf{W}}}&= \mathbf{0},\\ \lambda_{i}(\tilde{q}_{i}-1)&=0,\quad i=1,\cdots,N, \end{split}\] (30)

where the dual variables \(\lambda_{i}\geq 0\). We define the approximate KKT point in a similar manner to Ref. [29] as follows.

**Definition 12** (Approximate KKT points).: _The \((\kappa,\iota)\)-approximate KKT points of the optimization problem are those feasible points which satisfy the following two conditions:_

\[\begin{split}\text{Condition (I):}&\qquad\left\| \underline{\mathbf{W}}-\sum_{i=1}^{N}\lambda_{i}\frac{\partial\tilde{q}_{i}}{ \partial\underline{\mathbf{W}}}\right\|_{\text{F}}\leq\kappa,\\ \text{Condition (II):}&\qquad\lambda_{i}(\tilde{q}_{i}-1) \leq\iota,\ \ i=1,\cdots,N,\end{split}\] (31)

_where \(\kappa,\iota>0\) and \(\lambda_{i}\geq 0\)._

Proof of Lemma 9.: Let \(\underline{\mathbf{W}}:=\underline{\mathbf{W}}/\tilde{q}_{m}^{\frac{1}{L+1}}\) denote the scaled version of \(\underline{\mathbf{W}}(t)\) such that the sample robust margin \(\tilde{q}_{m}=1\). Thus we have \(f(\underline{\mathbf{x}};\underline{\mathbf{W}})=\tilde{q}_{m}f(\underline{ \mathbf{x}};\underline{\mathbf{W}})\) by homogeneity of t-NNs. According to Lemma 18, we further have

\[(L+1)f(\underline{\mathbf{x}};\underline{\mathbf{W}})=\left\langle\underline{ \mathbf{W}},\frac{\partial f(\underline{\mathbf{x}};\underline{\mathbf{W}})}{ \partial\underline{\mathbf{W}}}\right\rangle,\quad(L+1)f(\underline{\mathbf{x }};\underline{\mathbf{W}})=\left\langle\underline{\mathbf{W}},\frac{\partial f (\underline{\mathbf{x}};\underline{\mathbf{W}})}{\partial\underline{\mathbf{W }}}\right\rangle,\]

leading to

\[\left\langle\underline{\mathbf{W}},\frac{\partial f(\underline{\mathbf{x}}; \underline{\mathbf{W}})}{\partial\underline{\mathbf{W}}}\right\rangle=\frac{ 1}{\hat{q}_{m}^{L/(L+1)}}\frac{\partial f(\underline{\mathbf{x}};\underline{ \mathbf{W}})}{\partial\underline{\mathbf{W}}}.\]

We will prove that \(\underline{\mathbf{W}}\) is a \((\kappa,\iota)\)-KKT point of Problem (14) with \((\kappa,\iota)\rightarrow\mathbf{0}\).

Let \(\underline{\mathbf{W}}(t):=\frac{\text{d}\mathbf{W}(t)}{\text{d}t}\) for simplicity. By the chain rule and GF update rule, we have

\[\underline{\mathbf{W}}=\frac{1}{N}\sum_{i=1}^{N}e^{-\mathfrak{f}(\tilde{q}_{i} )}\mathfrak{f}^{\prime}(\tilde{q}_{i})\frac{\partial\tilde{q}_{i}}{\partial \underline{\mathbf{W}}}.\]

Using the homogeneity of t-NNs, we obtain

\[\frac{1}{2}\frac{\text{d}\left\|\underline{\mathbf{W}}\right\|_{\text{F}}^{2} }{\text{d}t}=\left\langle\underline{\mathbf{W}},\underline{\mathbf{W}} \right\rangle=\left\langle\frac{1}{N}\sum_{i=1}^{N}e^{-\mathfrak{f}(\tilde{q}_{ i})}\mathfrak{f}^{\prime}(\tilde{q}_{i})\frac{\partial\tilde{q}_{i}}{\partial \underline{\mathbf{W}}},\underline{\mathbf{W}}\right\rangle=(L+1)\cdot\frac{ 1}{N}\sum_{i=1}^{N}e^{-\mathfrak{f}(\tilde{q}_{i})}\mathfrak{f}^{\prime}( \tilde{q}_{i})\tilde{q}_{i}.\]

By letting \(\nu(t)=\sum_{i=1}^{N}e^{-\mathfrak{f}(\tilde{q}_{i})}\mathfrak{f}^{\prime}( \tilde{q}_{i})\tilde{q}_{i}\), we obtain \(\left\langle\underline{\mathbf{W}},\underline{\mathbf{W}}\right\rangle=(L+1) \nu/N\).

We construct the dual variables \(\lambda_{i}\) in Problem (31) in terms of \(\underline{\mathbf{W}}\) as follows

\[\lambda_{i}(t):=\frac{1}{N}\tilde{q}_{m}^{1-2/(L+1)}\cdot\rho e^{-\mathfrak{f}( \tilde{q}_{i})}\mathfrak{f}^{\prime}(\tilde{q}_{i})\left\|\underline{\mathbf{W }}\right\|_{\text{F}}^{-1}.\] (32)

To prove \(\underline{\mathbf{W}}\) is a \((\kappa,\iota)\)-KKT point of Problem (14), we need to check the conditions in Problem (31).

**Step 1: Check Condition (I) of the \((\kappa,\iota)\)-approximate KKT conditions**. We check the Condition (I) in Problem (31) for all \(t>t_{0}\) as follows

\[\begin{split}\left\|\underline{\mathbf{W}}-\sum_{i=1}^{N}\lambda _{i}\frac{\partial\tilde{q}_{i}(\underline{\mathbf{W}})}{\partial\underline{ \mathbf{W}}}\right\|_{\text{F}}^{2}&\overset{(i)}{\|\underline {\mathbf{W}}\|_{\text{F}}}-\frac{\underline{\mathbf{W}}}{\left\|\underline{ \mathbf{W}}\right\|_{\text{F}}}\cdot\frac{\rho}{\tilde{q}_{m}^{1/(L+1)}} \right\|_{\text{F}}^{2}\\ &=\frac{\rho^{2}}{\tilde{q}_{m}^{2/(L+1)}}\left\|\frac{\mathbf{W }}{\left\|\underline{\mathbf{W}}\right\|_{\text{F}}}-\frac{\underline{ \mathbf{W}}}{\left\|\underline{\mathbf{W}}\right\|_{\text{F}}}\right\|_{\text {F}}^{2}\\ &\overset{(ii)}{=}\frac{1}{\tilde{\gamma}^{2/(L+1)}}\left(2-2 \left\langle\frac{\mathbf{W}}{\left\|\underline{\mathbf{W}}\right\|_{\text{F} }},\frac{\underline{\mathbf{W}}}{\left\|\underline{\mathbf{W}}\right\|_{\text{ F}}}\right\rangle\right)\\ &\overset{(iii)}{\leq}\frac{2}{\tilde{\gamma}(t_{0})^{2/(L+1)}} \left(1-\left\langle\frac{\mathbf{W}}{\left\|\underline{\mathbf{W}}\right\|_{ \text{F}}},\frac{\underline{\mathbf{W}}}{\left\|\underline{\mathbf{W}}\right\| _{\text{F}}}\right\rangle\right)\\ &:=\kappa^{2}(t),\end{split}\] (33)

where equality \((i)\) is obtained by using the definition that \(\underline{\mathbf{W}}=\underline{\mathbf{W}}/\tilde{q}_{m}^{\frac{L+1}{m}}\), the fact \(\tilde{q}_{i}(\underline{\mathbf{W}})=y_{i}f(\underline{\mathbf{x}}_{i}+ \boldsymbol{\delta}_{i}(\underline{\mathbf{W}});\underline{\mathbf{W}})=y_{i} \cdot\tilde{q}_{m}f(\underline{\mathbf{x}}_{i}+\boldsymbol{\delta}_{i}( \underline{\mathbf{W}});\underline{\mathbf{W}})=\tilde{q}_{i}(\underline{ \mathbf{W}})\) due to the scale invariance of the adversarial perturbation \(\boldsymbol{\delta}_{i}(\underline{\mathbf{W}})\) and the homogeneity of t-NN, and the chain rule in computing \(\frac{\partial\tilde{q}_{i}}{\partial\underline{\mathbf{W}}}\) as follows

\[\sum_{i=1}^{N}\lambda_{i}\frac{\partial\tilde{q}_{i}(\underline{\mathbf{W}})}{ \partial\underline{\mathbf{W}}}=\sum_{i=1}^{N}\frac{\lambda_{i}}{\hat{q}_{m}^ {\frac{L}{m+1}}}\frac{\partial\tilde{q}_{i}}{\partial\underline{\mathbf{W}}}= \left\|\underline{\mathbf{W}}\right\|_{\text{F}}^{-1}\frac{\rho}{\tilde{q}_{m }^{\frac{L}{m}}}\cdot\frac{1}{N}\sum_{i=1}^{N}e^{-\mathfrak{f}(\tilde{q}_{i})} \mathfrak{f}^{\prime}(\tilde{q}_{i})\frac{\partial\tilde{q}_{i}}{\partial \underline{\mathbf{W}}}=\frac{\underline{\mathbf{W}}}{\left\|\underline{ \mathbf{W}}\right\|_{\text{F}}}\frac{\rho}{\tilde{q}_{m}^{\frac{L}{m+1}}}.\]

In Eq. (33), equality \((ii)\) holds by property \((b)\) in Lemma 19; \((iii)\) holds by the non-decreasing property of \(\tilde{\gamma}(t)\) for all \(t\in[t_{0},\infty)\) in Lemma 22.

Note that Eq. (33) indicates that \(\kappa(t)\) is in terms of the cosine of the angle between \(\underline{\mathbf{W}}(t)\) and \(\underline{\mathbf{W}}(t)\). We can further obtain that \(\kappa(t)\to 0\) as \(t\to\infty\) by showing the angle between \(\underline{\mathbf{W}}(t)\) and \(\underline{\mathbf{W}}(t)\) approximates \(0\) which was originally observed by Ref. [30] for standard training10 on a fixed training sample \(S\).

Footnote 10: The Lemma C.12 in [30] which was intended for standard training, can be safely extended to the adversarial settings in this paper. This is because by our construction for adversarial training with scale invariant adversarial perturbations, the adversarial traing margin are locally Lipschitz and the prediction function \(f(\underline{\mathbf{x}}+\boldsymbol{\delta}(\underline{\mathbf{W}}); \underline{\mathbf{W}})\) is positively homogeneous with respect to \(\underline{\mathbf{W}}\).

**Lemma 20** (Adapted from Lemma C.12 in Ref. [30]).: _Under Assumption 2 and Assumption 8 for t-NNs, the angle between \(\underline{\mathbf{W}}(t)\) and \(\underline{\mathbf{W}}(t)\) approximates \(0\) as \(t\to\infty\) along the trajectory of adversarial training with scale invariant adversarial perturbations, i.e.,_

\[\lim_{t\to\infty}\left\langle\frac{\mathbf{W}(t)}{\left\|\underline{\mathbf{W}} (t)\right\|_{\text{F}}},\frac{\underline{\mathbf{W}}(t)}{\left\|\underline{ \mathbf{W}}(t)\right\|_{\text{F}}}\right\rangle\to 1.\]

Note that according to Assumption 8 we have \(\tilde{\gamma}(t_{0})\geq b_{l}\rho(t_{0})^{-(L+1)}\) which means \(\tilde{\gamma}(t_{0})\) cannot be arbitrarily close to \(0\). Thus, by invoking Lemma 20, we obtain that

\[\lim_{t\to\infty}\kappa^{2}(t)=\lim_{t\to\infty}\frac{2}{\tilde{\gamma}(t_{0})^{ 2/(L+1)}}\left(1-\left\langle\frac{\mathbf{W}}{\left\|\underline{\mathbf{W}} \right\|_{\text{F}}},\frac{\underline{\mathbf{W}}}{\left\|\underline{\mathbf{W}} \right\|_{\text{F}}}\right\rangle\right)=0.\] (34)

**Step 2: Check Condition (II) of the \((\kappa,\iota)\)-approximate KKT conditions**. We check the Condition (II) in Problem (31) for all \(t>t_{0}\) as follows

\[\begin{split}\sum_{i=1}^{N}\lambda_{i}(\tilde{q}_{i}(\underline{ \mathbf{\dot{W}}})-1)&\overset{(i)}{=}\sum_{i=1}^{N}\frac{1}{N} \tilde{q}_{m}^{1-2/(L+1)}\rho e^{-\mathfrak{f}(\tilde{q}_{i})}\mathfrak{f}^{ \prime}(\tilde{q}_{i})\left\|\underline{\mathbf{\dot{W}}}\right\|_{\mathrm{F} }^{-1}\cdot(\tilde{q}_{i}(\underline{\mathbf{W}})\tilde{q}_{m}^{-1}-1)\\ &=\rho\left\|\underline{\mathbf{\dot{W}}}\right\|_{\mathrm{F}}^{- 1}\tilde{q}_{m}^{-2/(L+1)}\cdot\frac{1}{N}\sum_{i=1}^{N}e^{-\mathfrak{f}( \tilde{q}_{i})}\mathfrak{f}^{\prime}(\tilde{q}_{i})(\tilde{q}_{i}-\tilde{q}_{ m}),\end{split}\] (35)

where \((i)\) is due to the definition that \(\underline{\mathbf{\dot{W}}}=\underline{\mathbf{\dot{W}}}/\tilde{q}_{m}^{\frac {1}{L+1}}\) and the fact that \(\tilde{q}_{i}(\underline{\mathbf{W}})=y_{i}f(\mathbf{x}_{i}+\boldsymbol{ \delta}_{i}(\underline{\mathbf{W}});\underline{\mathbf{W}})=y_{i}\cdot \tilde{q}_{m}f(\underline{\mathbf{x}}_{i}+\boldsymbol{\delta}_{i}(\underline {\mathbf{\dot{W}}});\underline{\mathbf{\dot{W}}})=\tilde{q}_{i}(\underline{ \mathbf{\dot{W}}})\) due to the scale invariance of the adversarial pertabation \(\boldsymbol{\delta}_{i}(\underline{\mathbf{\dot{W}}})\) and the homogenouvity of t-NN.

To upper bound Eq. (35), first note that \(\left\|\underline{\mathbf{\dot{W}}}\right\|_{\mathrm{F}}\geq\left\langle \underline{\mathbf{\dot{W}}},\underline{\mathbf{\dot{W}}}\right\rangle=\left \langle\frac{1}{N}\sum_{i=1}^{N}e^{-\mathfrak{f}(\tilde{q}_{i})}\mathfrak{f}^ {\prime}(\tilde{q}_{i})\frac{\partial\tilde{q}_{i}}{\partial\underline{ \mathbf{\dot{W}}}},\underline{\mathbf{\dot{W}}}\right\rangle=\rho^{-1}(L+1) \nu/N\), in which \(\nu\) can be further lower bounded as

\[\nu\overset{(i)}{\geq}\frac{\mathfrak{g}(\log\frac{1}{N\hat{\mathcal{L}}^{ \text{adv}}})}{\mathfrak{f}^{\prime}(\log\frac{1}{N\hat{\mathcal{L}}^{\text{ adv}}})}N\hat{\mathcal{L}}^{\text{adv}}\overset{(ii)}{\geq}\frac{1}{2K}\log\frac{1}{N \hat{\mathcal{L}}^{\text{adv}}}\cdot N\hat{\mathcal{L}}^{\text{adv}}\overset{ (iii)}{\geq}\frac{1}{2K}e^{-\mathfrak{f}(\tilde{q}_{m})}\log\frac{1}{N\hat{ \mathcal{L}}^{\text{adv}}},\] (36)

where \((i)\) is due to Lemma 23, \((ii)\) holds because of Lemma 24, and \((iii)\) uses \(N\hat{\mathcal{L}}^{\text{adv}}=\sum_{i}e^{-\mathfrak{f}(\tilde{q}_{i})}\geq e ^{-\mathfrak{f}(\tilde{q}_{m})}\). Combing Eq. (35) and Eq. (36) yields

\[\begin{split}\sum_{i}\lambda_{i}(\tilde{q}_{i}(\underline{\mathbf{ \dot{W}}})-1)&\leq\frac{2K\tilde{q}_{m}^{-2/(L+1)}\rho^{2}}{(L+1) \log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}}}\sum_{i=1}^{N}e^{\mathfrak{f}( \tilde{q}_{m})-\mathfrak{f}(\tilde{q}_{i})}\mathfrak{f}^{\prime}(\tilde{q}_{i} )(\tilde{q}_{i}-\tilde{q}_{m})\\ &\overset{(i)}{\geq}\frac{2K\tilde{\gamma}^{-2/(L+1)}}{(L+1) \log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}}}\sum_{i=1}^{N}e^{\mathfrak{f}( \tilde{q}_{m})-\mathfrak{f}(\tilde{q}_{i})}\mathfrak{f}^{\prime}(\tilde{q}_{i} )(\tilde{q}_{i}-\tilde{q}_{m}),\end{split}\] (37)

where \((i)\) uses \(\tilde{q}_{m}^{-2/(L+1)}\rho^{2}\leq\tilde{\gamma}^{-2/(L+1)}\) by Lemma 19.

In Eq. (37), if \(\tilde{q}_{i}>\tilde{q}_{m}\), then there exists an \(\xi_{i}\in(\tilde{q}_{m},\tilde{q}_{i})\) such that \(\mathfrak{f}(\tilde{q}_{m})-\mathfrak{f}(\tilde{q}_{i})=\mathfrak{f}^{\prime}( \xi_{i})(\tilde{q}_{i}-\tilde{q}_{m})\) by the mean value theorem. Further, we know that \(\mathfrak{f}^{\prime}(\tilde{q}_{i})\leq K^{\lceil\log_{2}(\tilde{q}_{i}/\xi_ {i})\rceil}\mathfrak{f}^{\prime}(\xi_{i})\) by Assumption 2. Note that \(\lceil\log_{2}(\tilde{q}_{i}/\xi_{i})\rceil\leq\log_{2}(2B_{0}\rho^{L+1}/ \tilde{q}_{m})\leq\log_{2}(2B_{0}\tilde{\gamma})\), where

\[B_{0}(t): =\sup\left\{\tilde{q}_{i}\rho^{-(L+1)}\mid\underline{\mathbf{ \dot{W}}}\neq\boldsymbol{0}\right\}=\sup\left\{\tilde{q}_{i}\mid\| \underline{\mathbf{\dot{W}}}\|_{\mathrm{F}}=1\right\}.\]

Then, for all \(t>t_{0}\), we have

\[\begin{split}\sum_{i}\lambda_{i}(\tilde{q}_{i}(\underline{\mathbf{ \dot{W}}})-1)&\leq\frac{2K\tilde{\gamma}^{-2/(L+1)}\rho^{2}}{(L+1) \log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}}}K^{\log_{2}(2B_{0}/\tilde{ \gamma})}\sum_{i:\tilde{q}_{i}\neq\tilde{q}_{m}}e^{\mathfrak{f}^{\prime}(\xi_{i}) (\tilde{q}_{i}-\tilde{q}_{m})}\mathfrak{f}^{\prime}(\xi_{i})(\tilde{q}_{i}- \tilde{q}_{m})\\ &\overset{(i)}{\leq}\frac{2K\tilde{\gamma}^{-2/(L+1)}}{(L+1)\log \frac{1}{N\hat{\mathcal{L}}^{\text{adv}}}}K^{\log_{2}(2B_{0}/\tilde{\gamma})} \cdot Ne\\ &\overset{(ii)}{\leq}\frac{2KNe\tilde{\gamma}^{-2/(L+1)}}{(L+1) \log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}}}\left(\frac{B_{0}}{\tilde{ \gamma}}\right)^{\log_{2}(2K)}\\ &\overset{(iii)}{\leq}\frac{2KNe}{(L+1)\tilde{\gamma}(t_{0})^{-2/(L+ 1)}}\left(\frac{B_{0}}{\tilde{\gamma}(t_{0})}\right)^{\log_{2}(2K)}\cdot\log \frac{1}{N\hat{\mathcal{L}}^{\text{adv}}}\\ &:=\iota(t),\end{split}\] (38)

where \((i)\) holds because the function \(x\mapsto e^{-z}z\) on \((0,\infty)\) has maximum \(e\) at \(z=1\); \((ii)\) is due to \(a^{\log_{e}b}=b^{\log_{e}a}\); \((iii)\) holds by the non-decreasing property of \(\tilde{\gamma}(t)\) for all \(t\in[t_{0},\infty)\) in Lemma 22. Note that by Lemma 22, we have \(\lim_{t\to\infty}\hat{\mathcal{L}}^{\text{adv}}(t)=0\), which further yields

\[\lim_{t\to\infty}\iota(t)=0.\] (39)

**Step 3: Check the condition for convergence to KKT point**. According to Eq. (34) and Eq. (39), the limit point of \(\underline{\mathbf{\dot{W}}}(t)\) satisfy the \((\kappa,\iota)\)-approximate KKT conditions of Problem 14 along thetrajectory of adversarial training of t-NN with scale invariant adversarial perturbations where \(\lim_{t\to\infty}(\kappa(t),\iota(t))=\mathbf{0}\). Then, we need to check the condition between \((\kappa,\iota)\)-approximate points and KKT points.

According to Ref. [30], the KKT condition becomes a necessary condition for global optimality of Problem (14) when the Mangasarian-Fromovitz Constraint Qualification (MFCQ) [33] is satisfied. It is straightforward to see that Problem (14) satisfies the MFCQ condition, i.e.,

\[\left\langle\frac{\partial\tilde{q}_{i}}{\partial\underline{\mathbf{W}}}, \underline{\mathbf{W}}\right\rangle=(L+1)\tilde{q}_{i}\geq 0.\]

at every feasible point \(\underline{\mathbf{W}}\). Then restating the theorem in Ref. [7] regarding the relation between \((\kappa,\iota)\)-approximate KKT point and KKT point in our setting yields the following result.

**Theorem 21** (Theorem 3.6 in Ref. [7] and Theorem C.4 in Ref. [30]).: _Let \(\{\underline{\mathbf{W}}(j)\mid j\in\mathbb{N}\}\) be a sequence of feasible point of Problem (14), and \(\underline{\mathbf{W}}(j)\) is a \((\kappa(j),\iota(j))\)-approximate KKT point for all \(j\) with two sequences11\(\{\kappa(j)>0\mid j\in\mathbb{N}\}\) and \(\{\iota(j)>0\mid j\in\mathbb{N}\}\) and \(\lim_{j\to\infty}(\kappa(j),\iota(j))=\mathbf{0}\). If \(\lim_{j\to\infty}\underline{\mathbf{W}}(j)=\underline{\mathbf{W}}^{*}\), and MFCQ holds at \(\underline{\mathbf{W}}^{*}\), then \(\underline{\mathbf{W}}^{*}\) is a KKT point of Problem (14)._

Footnote 11: Using the same method to [30, Lemma C.12], we can construct the two sequences, i.e., \(\{\kappa(j)>0\mid j\in\mathbb{N}\}\) and \(\{\iota(j)>0\mid j\in\mathbb{N}\}\), based on Eq. (33) and Eq. (35).

Recall that \(\underline{\mathbf{W}}=\underline{\mathbf{W}}/\tilde{q}_{i}^{\frac{1}{\iota +1}}\). Then, it can be concluded that the limit point of \(\{\underline{\mathbf{W}}(t)/\left\|\underline{\mathbf{W}}(t)\right\|_{\mathrm{ F}}:t>0\}\) of GF for empirical adversarial risk \(\hat{\mathcal{L}}^{\text{adv}}(\underline{\mathbf{W}}):=\frac{1}{N}\sum_{i=1} ^{N}e^{-\mathfrak{f}(y_{i}f(\mathbf{x}_{i}+\underline{\theta}_{i}(\underline{ \mathbf{W}}),\underline{\mathbf{W}}))}\) with scale invariant perturbations \(\underline{\theta}_{i}\) is aligned with the direction of a KKT point of Problem (14). 

### Technical Lemmas for Proving Lemma 9

**Lemma 22**.: _Under Assumption 2, we have the following statements for GF-based adversarial training in Eq. (13) with scale invariant perturbations:_

1. _For a.e._ \(t\in(t_{0},\infty)\)_, the smoothed normalized robust margin_ \(\tilde{\gamma}(\underline{\mathbf{W}}(t))\) _defined in Eq. (_29_) is non-decreasing, i.e.,_ \[\frac{\mathsf{d}\tilde{\gamma}(\underline{\mathbf{W}}(t))}{\mathsf{d}t}\geq 0.\]
2. _The adversarial objective_ \(\hat{\mathcal{L}}^{\text{adv}}(\underline{\mathbf{W}}):=\frac{1}{N}\sum_{i=1} ^{N}e^{-\mathfrak{f}(y_{i}f(\mathbf{x}_{i}+\underline{\theta}_{i}(\underline{ \mathbf{W}}):\underline{\mathbf{W}}))}\) _with scale invariant pertubations_ \(\underline{\theta}_{i}\) _converges to zero as_ \(t\to\infty\)_, i.e.,_ \[\lim_{t\to\infty}\hat{\mathcal{L}}^{\text{adv}}(\underline{\mathbf{W}}(t))=0,\] (40) _and the Euclidean norm of the t-NN weights diverges, i.e.,_ \[\lim_{t\to\infty}\left\|\underline{\mathbf{W}}(t)\right\|_{\mathrm{F}}=\infty.\] (41)

Proof of Lemma 22.: We follow the idea of Ref. [30] to prove Lemma 22 as follows.

**Step 1: Prove Part (I)**. We prove **(I)** by showing the following results for all \(t\geq t_{0}\),

\[\frac{\mathsf{d}\log\rho}{\mathsf{d}t}>0\quad\text{and}\quad\frac{\mathsf{d} \log\tilde{\gamma}}{\mathsf{d}t}\geq(L+1)\left(\frac{\mathsf{d}\log\rho}{ \mathsf{d}t}\right)^{-1}\left\|\frac{\mathbf{d}\underline{\mathbf{W}}}{\mathsf{ d}t}\right\|_{\mathrm{F}}.\]

Recalling the quantity \(\nu(t)=\sum_{i=1}^{N}e^{-\mathfrak{f}(\tilde{q}_{i})}\mathfrak{f}(\tilde{q}_{i}) \tilde{q}_{i}\), by chain rule we obtain

\[\frac{\mathsf{d}\log\rho}{\mathsf{d}t}=\frac{1}{2\rho^{2}}\frac{\mathsf{d} \rho^{2}}{\mathsf{d}t}=\frac{1}{\rho^{2}}\left\langle\frac{1}{N}\sum_{i=1}^{N}e^ {-\mathfrak{f}(\tilde{q}_{i})}\mathfrak{f}^{\prime}(\tilde{q}_{i})\frac{ \partial\tilde{q}_{i}}{\partial\underline{\mathbf{W}}},\underline{\mathbf{W} }\right\rangle=\frac{\nu(L+1)}{\rho^{2}N}\stackrel{{(i)}}{{\geq}}0,\]

where \((i)\) holds due to Lemma 23.

By the chaining rule, we also have

\[\frac{\text{d}\log\tilde{\gamma}}{\text{d}t} =\frac{\text{d}}{\text{d}t}\left(\log\left(\log\frac{1}{N\hat{ \mathcal{L}}^{\text{adv}}}\right)-(L+1)\log\rho\right)\] \[=\frac{\mathfrak{g}^{\prime}(\log\frac{1}{N\hat{\mathcal{L}}^{ \text{adv}}})}{\mathfrak{g}(\log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}})} \cdot\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}}\cdot\left(-\frac{\text{d}N\hat {\mathcal{L}}^{\text{adv}}}{\text{d}t}\right)-(L+1)^{2}\cdot\frac{\nu(t)}{ \rho^{2}}\] \[=\frac{1}{\nu(t)}\cdot\left(-\frac{\text{d}N\hat{\mathcal{L}}^{ \text{adv}}}{\text{d}t}\right)-(L+1)^{2}\cdot\frac{\nu(t)}{\rho^{2}}\] \[=\frac{1}{\nu(t)}\cdot\left(-\frac{\text{d}N\hat{\mathcal{L}}^{ \text{adv}}}{\text{d}t}-(L+1)^{2}\cdot\frac{\nu(t)^{2}}{\rho^{2}}\right).\]

Note that according to Eq. (13), we have

\[\frac{\text{d}\hat{\mathcal{L}}^{\text{adv}}}{\text{d}t}=\left\langle\frac{ \partial\hat{\mathcal{L}}^{\text{adv}}}{\partial\underline{\mathbf{W}}}, \frac{\text{d}\underline{\mathbf{W}}}{\text{d}t}\right\rangle=-\left\|\frac{ \text{d}\underline{\mathbf{W}}}{\text{d}t}\right\|_{\text{F}}^{2},\]

for \(t\geq 0\) almost everywhere. One the other hand, we have

\[L\nu(t)=\left\langle\underline{\mathbf{W}},\frac{\text{d}\underline{\mathbf{W }}}{\text{d}t}\right\rangle.\]

Thus, we obtain

\[\frac{\text{d}\log\tilde{\gamma}}{\text{d}t}\geq\frac{1}{N\nu}\left(\left\| \frac{\text{d}\underline{\mathbf{W}}}{\text{d}t}\right\|_{\text{F}}^{2}- \left\langle\underline{\hat{\mathbf{W}}},\frac{\text{d}\underline{\mathbf{W} }}{\text{d}t}\right\rangle^{2}\right)\geq\frac{1}{N\nu(t)}\left\|(\mathbf{I}- \texttt{vec}(\underline{\hat{\mathbf{W}}})\texttt{vec}(\underline{\hat{ \mathbf{W}}})^{\top})\texttt{vec}(\frac{\text{d}\underline{\mathbf{W}}}{ \text{d}t})\right\|^{2}.\]

By the chain rule,

\[\frac{\text{d}\hat{\mathbf{W}}}{\text{d}t}=\frac{1}{\rho}(\mathbf{I}- \texttt{vec}(\underline{\hat{\mathbf{W}}})\texttt{vec}(\underline{\hat{ \mathbf{W}}})^{\top})\texttt{vec}(\frac{\text{d}\underline{\mathbf{W}}}{\text {d}t}),\]

for \(t>0\) allmost everywhere. So, we have

\[\frac{\text{d}}{\text{d}t}\log\tilde{\gamma}\geq\frac{\rho^{2}}{N\nu(t)} \left\|\frac{\text{d}\hat{\mathbf{W}}}{\text{d}t}\right\|_{\text{F}}^{2}=(L+1) \left(\frac{\text{d}}{\text{d}t}\log\rho\right)^{-1}\left\|\frac{\text{d} \hat{\mathbf{W}}}{\text{d}t}\right\|_{\text{F}}^{2}\geq 0.\]

**Step 2: Prove Part (II)**. Motivated by [30, Lemma B.8], we prove **(II)** as follows. First, note that

\[-\frac{\text{d}\hat{\mathcal{L}}^{\text{adv}}}{\text{d}t}=\left\|\frac{\text {d}\underline{\mathbf{W}}}{\text{d}t}\right\|_{\text{F}}^{2}\geq\left\langle \underline{\hat{\mathbf{W}}},\underline{\dot{\mathbf{W}}}\right\rangle^{2}= \rho^{-2}N^{-2}(L+1)^{2}\nu^{2}.\]

By lower bounding \(\nu\) with Lemma 23 and replacing \(\rho\) with \((\mathfrak{g}(\log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}})/\tilde{\gamma})^ {1/(L+1)}\) by the definition smoothed normalized robust margin of \(\tilde{\gamma}\) in Eq. (29), we obtain

\[-\frac{\text{d}\hat{\mathcal{L}}^{\text{adv}}}{\text{d}t} \geq(L+1)^{2}\left(\frac{\mathfrak{g}(\log\frac{1}{N\hat{\mathcal{L }}^{\text{adv}}})}{\mathfrak{g}^{\prime}(\log\frac{1}{N\hat{\mathcal{L}}^{ \text{adv}}})}N\hat{\mathcal{L}}^{\text{adv}}\right)^{2}\cdot\left(\frac{ \tilde{\gamma}}{\mathfrak{g}(\log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}})} \right)^{2/(L+1)}\] \[\geq(L+1)^{2}\tilde{\gamma}(t_{0})^{2/(L+1)}\cdot\frac{\mathfrak{g }(\log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}})^{2-2/(L+1)}}{\mathfrak{g}^{ \prime}(\log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}})^{2}}\cdot(N\hat{ \mathcal{L}}^{\text{adv}})^{2},\]

where the last inequality holds due to the non-decreasing property of \(\tilde{\gamma}\) in Part (**I**). Then, we obtain

\[\frac{\mathfrak{g}^{\prime}(\log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}})^{2}} {\mathfrak{g}(\log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}})^{2-2/(L+1)}}\cdot \frac{\text{d}}{\text{d}t}\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}}\geq(L+1)^{2 }\tilde{\gamma}(t_{0})^{2/(L+1)}.\]

By integrating on both sides from \(t_{0}\) to \(t\), we obtain

\[G\left(\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}}\right)\geq(L+1)^{2}\tilde{ \gamma}(t_{0})^{2/(L+1)}(t-t_{0}),\] (42)where

\[G(z):=\int_{1/(N\hat{\mathcal{L}}^{\text{adv}}(t_{0}))}^{z}\frac{\mathsf{g}^{ \prime}(\log u)^{2}}{g(\log u)^{2-2/(L+1)}}\mathsf{d}u.\]

We use proof by contradiction to show the empirical training risk \(\hat{\mathcal{L}}^{\text{adv}}\) converges to zero. Note that \(\frac{1}{(N\hat{\mathcal{L}}^{\text{adv}})}\) is non-decreasing. If \(\frac{1}{(N\hat{\mathcal{L}}^{\text{adv}})}\) does not grow to \(\infty\), then neither does \(G(\frac{1}{(N\hat{\mathcal{L}}^{\text{adv}})})\). But the RHS of Eq. (42) grows to \(\infty\), which is a contradiction. Therefore, \(\lim_{t\to\infty}N\hat{\mathcal{L}}^{\text{adv}}=0\). Hence \(\lim_{t\to\infty}\hat{\mathcal{L}}^{\text{adv}}(t)=0\) and \(\lim_{t\to\infty}\rho(t)=\infty\). 

**Lemma 23** (Adapted from Lemma B.5 of Ref. [30]).: _The quantity \(\nu=\sum_{i}e^{-\mathsf{f}(\bar{q}_{i})}\mathsf{f}(\bar{q}_{i})\bar{q}_{i}\) has a lower bound for all \(t\in(t_{0},\infty)\),_

\[\nu(t)\geq\frac{\mathsf{g}(\log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}})}{ \mathsf{g}^{\prime}(\log\frac{1}{N\hat{\mathcal{L}}^{\text{adv}}})}N\hat{ \mathcal{L}}^{\text{adv}}.\] (43)

**Lemma 24** (Lemma D.1 of Ref. [30]).: _For \(\mathsf{f}(\cdot)\) and \(\mathsf{g}(\cdot)\) in Assumption 2, we have_

\[\frac{\mathsf{g}(x)}{\mathsf{g}^{\prime}(x)}\in\left[\frac{1}{2K}x,2Kx\right], \forall x\in[b_{\mathsf{g}},\infty)\quad\text{and}\quad\frac{\mathsf{f}(q)}{ \mathsf{f}^{\prime}(q)}\in\left[\frac{1}{2K}q,2Kq\right],\forall q\in[\mathsf{ g}(b_{\mathsf{g}}),\infty).\]

### Proof of Theorem 10

Note that the t-NN \(g(\underline{\mathbf{x}};\underline{\mathbf{V}})\) with weights \(\underline{\mathbf{V}}=(\underline{\mathbf{V}}^{(1)},\cdots,\underline{ \mathbf{V}}^{(J)},\mathbf{v})\) in Theorem 6 has the following structure

\[g(\underline{\mathbf{x}};\underline{\mathbf{V}}) =\mathbf{v}^{\top}\mbox{\tiny vec}(\mathsf{g}(\underline{\mathbf{ x}}))\] \[\mathsf{g}(\underline{\mathbf{x}}) =\mathsf{g}^{(J)}(\underline{\mathbf{x}})\in\mathbb{R}^{m_{J} \times 1\times\mathbf{c}}\] \[\mathsf{g}^{(j)}(\underline{\mathbf{x}}) =\sigma(\underline{\mathbf{V}}^{(j)}*_{M}\mathsf{g}^{(j-1)}( \underline{\mathbf{x}}))\in\mathbb{R}^{m_{j}\times 1\times\mathbf{c}},\ \forall j=1, \cdots,J\] \[\mathsf{g}^{(0)}(\underline{\mathbf{x}}) =\underline{\mathbf{x}}.\]

Let \(\alpha=(\frac{1}{B_{v}})^{\frac{L-J}{L+1}}\) and \(\beta=(\frac{1}{B_{v}})^{-\frac{J+1}{L+1}}\), where \(L\) is a sufficiently large integer greater than \(J\). We then construct a t-NN \(h(\underline{\mathbf{x}};\underline{\mathbf{H}})\) of \(L\) t-product layers which perfectly realizes \(g(\underline{\mathbf{x}};\underline{\mathbf{V}})\). Specifically, we construct \(h\) with weights \(\underline{\mathbf{H}}=(\underline{\mathbf{H}}^{(1)},\cdots,\underline{ \mathbf{H}}^{(L)},\mathbf{h})\) satisfying the following equation

\[\underline{\mathbf{H}}=(\underbrace{\alpha\mathbf{V}^{(1)},\cdots,\alpha \underline{\mathbf{V}}^{(J)}}_{\text{first $J$ t-product layers}},\underbrace{\beta\mathbf{I},\cdots,\beta\mathbf{I}}_{ \text{last $(L-J)$ t-product layers}},\mathbf{v}),\]

or more clearly

\[h(\underline{\mathbf{x}};\underline{\mathbf{V}}) =\mathbf{v}^{\top}\mbox{\tiny vec}(\mathbf{h}(\underline{\mathbf{ x}}))\] \[\mathbf{h}(\underline{\mathbf{x}}) =\mathbf{h}^{(L)}(\underline{\mathbf{x}})\in\mathbb{R}^{m_{J} \times 1\times\mathbf{c}}\] \[\mathbf{h}^{(l)}(\underline{\mathbf{x}}) =\sigma(\beta\underline{\mathbf{I}}*_{M}\mathbf{h}^{(l-1)}( \underline{\mathbf{x}}))\in\mathbb{R}^{m_{J}\times 1\times\mathbf{c}},\ \forall l=J+1, \cdots,L\] \[\mathbf{h}^{(j)}(\underline{\mathbf{x}}) =\sigma(\alpha\underline{\mathbf{V}}^{(j)}*_{M}\mathbf{h}^{(j-1)}( \underline{\mathbf{x}}))\in\mathbb{R}^{m_{j}\times 1\times\mathbf{c}},\ \forall j=1, \cdots,J\] \[\mathbf{h}^{(0)}(\underline{\mathbf{x}}) =\underline{\mathbf{x}},\]

where \(\underline{\mathbf{I}}\) is the t-identity tensor.

It is easy to prove that for any input \(\underline{\mathbf{x}}\), the input and output of \(g(\underline{\mathbf{x}};\underline{\mathbf{V}})=h(\underline{\mathbf{x}}; \underline{\mathbf{H}})\), it can also be proved that

\[\min_{R_{\underline{\mathbf{x}}}(\underline{\mathbf{x}};\underline{\mathbf{ H}})}\leq\xi yg(\underline{\mathbf{x}}^{\prime};\underline{\mathbf{V}})=\min_{R_{ \underline{\mathbf{x}}}(\underline{\mathbf{x}}^{\prime})\leq\xi}yh(\underline {\mathbf{x}}^{\prime};\underline{\mathbf{H}}).\]

Therefore, \(h(\underline{\mathbf{x}};\underline{\mathbf{H}})\) can also robustly classify \((\underline{\mathbf{x}}_{i},y_{i})_{i=1}^{N}\) because \(g(\underline{\mathbf{x}};\underline{\mathbf{V}})\) can robustly classify \((\underline{\mathbf{x}}_{i},y_{i})_{i=1}^{N}\).

Then, we consider the class of over-parameterized t-NNs \(\mathfrak{F}=\{f(\underline{\mathbf{x}};\underline{\mathbf{W}})\}\) defined in Eq. (14) with dimensionality of weight \(\underline{\mathbf{W}}^{(l)}\in\mathbb{R}^{d_{l}\times d_{l-1}\times\mathbf{c}}\) satisfying \(d_{l}\gg\max_{j\leq J}\{m_{j}\}\) for all \(l=1,\cdots,L\). Specifically, we construct \(\overline{f}\) with weights

\[\underline{\mathbf{W}}=(\underline{\mathbf{W}}^{(1)},\cdots,\underline{\mathbf{ W}}^{(L)},\mathbf{w}),\]and structure

\[f(\underline{\mathbf{x}};\underline{\mathbf{W}}) =\mathbf{w}^{\top}{}_{\forall\mathbf{v}\in\mathcal{C}}(\mathbf{f}( \underline{\mathbf{x}}))\] \[\mathbf{f}(\underline{\mathbf{x}}) =\mathbf{f}^{(L)}(\underline{\mathbf{x}})\in\mathbb{R}^{d_{L} \times 1\times\mathbf{c}}\] \[\mathbf{f}^{(l)}(\underline{\mathbf{x}}) =\sigma(\underline{\mathbf{W}}^{(l)}*_{M}\mathbf{f}^{(l-1)}( \underline{\mathbf{x}}))\in\mathbb{R}^{d_{l}\times 1\times\mathbf{c}},\;\forall l=1, \cdots,L\] \[\mathbf{f}^{(0)}(\underline{\mathbf{x}}) =\underline{\mathbf{x}}.\]

Note that according to our construction there is a function \(f(\underline{\mathbf{x}};\underline{\mathbf{W}}_{h})\in\mathfrak{F}\) with weights \(\underline{\mathbf{W}}_{h}=(\underline{\mathbf{W}}_{h}^{(1)},\cdots, \underline{\mathbf{W}}_{h}^{(L)},\mathbf{w}_{h})\) satisfying

\[(\underline{\mathbf{W}}_{h}^{(l)})_{i_{1},i_{2},i_{3}}=\begin{cases}( \underline{\mathbf{H}}^{(l)})_{i_{1},i_{2},i_{3}}&\text{if }i_{1}\leq m_{l},i_{2}\leq m_{l-1},l\leq J\\ (\underline{\mathbf{H}}^{(l)})_{i_{1},i_{2},i_{3}}&\text{if }i_{1}\leq m_{J},i_{2} \leq m_{J},l=J+1,\cdots,L\\ 0&\text{otherwise}\end{cases}\]

and

\[\mathbf{w}_{i}=\begin{cases}\mathbf{h}_{i}&\text{if }i\leq\mathbf{c}m_{J}\\ 0&\text{otherwise}\end{cases}.\]

We can also see that \(h^{\prime}(\underline{\mathbf{x}}+\underline{\boldsymbol{\delta}})=h( \underline{\mathbf{x}}+\underline{\boldsymbol{\delta}})\) for any \(\underline{\mathbf{x}}\in\mathbb{R}^{d\times 1\times\mathbf{c}}\) and any \(\underline{\boldsymbol{\delta}}\) satisfying \(R_{a}(\underline{\boldsymbol{\delta}})\leq\xi\). Thus, we can say that the weight \(\underline{\mathbf{W}}_{h}\) of \(f(\underline{\mathbf{x}};\underline{\mathbf{W}}_{h})\) is a feasible solution to Problem (14), i.e.,

\[\min_{R_{a}(\underline{\boldsymbol{\delta}}_{i})\leq\xi}y_{i}f(\underline{ \mathbf{x}}_{i}+\underline{\boldsymbol{\delta}}_{i};\underline{\mathbf{W}}_{h })\geq 1,\forall i=1,\cdots,N.\]

Now consider the optimal solution \(\underline{\mathbf{W}}^{*}=(\underline{\mathbf{W}}^{*(1)},\cdots,\underline{ \mathbf{W}}^{*(L)},\underline{\mathbf{W}}^{*})\) to Problem (14). Then according to the optimality of \(\underline{\mathbf{W}}^{*}\) and the feasibility of \(\underline{\mathbf{W}}_{h}\) to Problem 14, we have

\[\left\|\underline{\mathbf{W}}^{*}\right\|_{\mathrm{F}}^{2}\leq \left\|\underline{\mathbf{W}}_{h}\right\|_{\mathrm{F}}^{2} \leq\alpha^{2}\cdot B_{b}^{2}\cdot(J+1)+\beta^{2}\cdot(\mathsf{c}m_{J})\cdot(L -J)\] (44) \[=B_{v}^{\frac{2(J+1)}{L+1}}\left(K+1+(\mathsf{c}m_{J})(L-J)\right)\]

and

\[\min_{R_{a}(\underline{\boldsymbol{\delta}}_{i})\leq\xi}y_{i}f(\underline{ \mathbf{x}}_{i}+\underline{\boldsymbol{\delta}}_{i};\underline{\mathbf{W}}^{* })\geq 1,\forall i=1,\cdots,N.\] (45)

As there is an example \((\underline{\mathbf{x}}^{*},y^{*})\) satisfying \(\left\|\underline{\mathbf{x}}^{*}\right\|_{\mathrm{F}}\leq 1\) in the training set \(S=\{(\underline{\mathbf{x}}_{i},y_{i})\}_{i=1}^{N}\subseteq\mathbb{R}^{d\times 1 \times\mathbf{c}}\times\{\pm 1\}\). Then according to Eq. (45), we have

\[y^{*}f(\underline{\mathbf{x}}^{*};\underline{\mathbf{W}}^{*})=y^{*}f( \underline{\mathbf{x}}^{*}+\mathbf{0};\underline{\mathbf{W}}^{*})\geq\min_{R_ {a}(\underline{\boldsymbol{\delta}})\leq\xi}y^{*}f(\underline{\mathbf{x}}^{* }+\underline{\boldsymbol{\delta}};\underline{\mathbf{W}}^{*})\geq 1,\]

which means

\[1\leq f(\underline{\mathbf{x}}^{*}) \leq\left\|\underline{\mathbf{x}}^{*}\right\|_{\mathrm{F}}\left\| \underline{\mathbf{W}}^{*(l)}\right\|_{\mathrm{sp}}\leq\left\|\underline{ \mathbf{W}}^{*}\right\|_{\mathrm{2}}\prod_{l=1}^{L}\left\|\underline{ \mathbf{W}}^{*(l)}\right\|_{\mathrm{sp}}\] \[\leq\left(\frac{1}{L+1}\left(\left\|\mathbf{w}^{*}\right\|_{2}+ \sum_{l=1}^{L}\left\|\underline{\mathbf{W}}^{*(l)}\right\|_{\mathrm{sp}} \right)\right)^{1/(L+1)}\]

indicating that

\[\frac{1}{L+1}\left(\left\|\mathbf{w}^{*}\right\|_{2}+\sum_{l=1}^{L}\left\| \underline{\mathbf{W}}^{*(l)}\right\|_{\mathrm{sp}}\right)>1.\]

On the other hand, according to Eq. (44) and Lemma 25, we have

\[\left\|\underline{\mathbf{W}}^{*(1)}\right\|_{\mathrm{F}}=\cdots=\left\| \underline{\mathbf{W}}^{*(L)}\right\|_{\mathrm{F}}=\left\|\mathbf{w}^{*} \right\|_{2}\leq\left(\frac{1}{(L+1)}B_{v}^{\frac{2(J+1)}{L+1}}\left(K+1+( \mathsf{c}m_{J})(L-J)\right)\right)^{1/2}.\]

Therefore, we obtain

\[\frac{1}{L+1}\left(\frac{\left\|\underline{\mathbf{w}}^{*}\right\|_{2}}{\left\| \underline{\mathbf{w}}^{*}\right\|_{2}}+\sum_{l=1}^{L}\frac{\left\|\underline{ \mathbf{W}}^{*(l)}\right\|_{\mathrm{sp}}}{\left\|\underline{\mathbf{W}}^{*(l)} \right\|_{\mathrm{F}}}\right)\geq\left(\frac{1}{B_{v}}\right)^{\frac{J+1}{L+1}} \sqrt{\frac{L+1}{(J+1)+(\mathsf{c}m_{J})(L-J)}}.\]Note that

\[\left\|\mathbf{W}^{(l)}\right\|_{\text{F}}=\left\|\widetilde{\mathbf{W}}_{M}^{*(l) }\right\|_{\text{F}}\quad\text{and}\quad\left\|\mathbf{W}^{(l)}\right\|_{\text {sp}}=\left\|\widetilde{\mathbf{W}}_{M}^{*(l)}\right\|,\]

where \(\widetilde{\mathbf{W}}_{M}^{*(l)}\) denotes the \(M\)-block-diagonal matrix of weight tensor \(\underline{\mathbf{W}}^{*(l)}\). Then, we have

\[\frac{1}{L}\sum_{l=1}^{L}\frac{\left\|\widetilde{\mathbf{W}}_{M}^{*(l)} \right\|}{\left\|\widetilde{\mathbf{W}}_{M}^{*(l)}\right\|_{\text{F}}}\geq \left(1+\frac{1}{L}\right)\left(\frac{1}{B_{v}}\right)^{\frac{j+1}{L+1}}\sqrt {\frac{L+1}{(J+1)+(\mathfrak{c}m_{J})(L-J)}}-\frac{1}{L}.\]

Taking the reciprocal of both sides gives

\[\frac{L}{\sum_{l=1}^{L}\left(r_{\text{stb}}(\widetilde{\mathbf{W}}_{M}^{*(l)} )\right)^{-1/2}}\leq\frac{1}{\left(1+\frac{1}{L}\right)\left(\frac{1}{B_{v}} \right)^{\frac{j+1}{L+1}}\sqrt{\frac{L+1}{(J+1)+(\mathfrak{c}m_{J})(L-J)}}- \frac{1}{L}}.\]

**Lemma 25**.: _For every \(1\leq i,j\leq L\), we have \(\left\|\underline{\mathbf{W}}^{*(i)}\right\|_{\text{F}}=\left\|\underline{ \mathbf{W}}^{*(j)}\right\|_{\text{F}}=\left\|\mathbf{w}^{*}\right\|_{2}\)._

Proof.: Let \(1\leq i<j\leq L\). For \(\mu>0\), we define a t-NN \(f_{\mu}(\underline{\mathbf{x}};\underline{\mathbf{V}})\) with weights \(\underline{\mathbf{V}}=(\mathbf{V}^{(1)},\cdots,\underline{\mathbf{V}}^{L}, \mathbf{v})\) which are constructed from \(f(\underline{\mathbf{x}};\underline{\mathbf{W}}^{*})\) whose weights \(\underline{\mathbf{W}}^{*}=(\underline{\mathbf{W}}^{*(1)},\cdots,\underline{ \mathbf{W}}^{*(L)},\underline{\mathbf{w}}^{*})\) is an optimal solution to Problem (14). Specifically, the construction of \(\underline{\mathbf{V}}=(\underline{\mathbf{V}}^{(1)},\cdots,\underline{ \mathbf{V}}^{L},\mathbf{v})\) is given as follows:

\[\forall l=1,\cdots,L,\quad\underline{\mathbf{V}}^{(l)}=\begin{cases} \begin{array}{cc}\underline{\mathbf{W}}^{*(l)}&\text{if $l\neq i$ and $1\neq j$}\\ \mu\underline{\mathbf{W}}^{*(i)}&\text{if $l=i$}\\ \mu^{-1}\underline{\mathbf{W}}^{*(j)}&\text{if $1=j$}\end{array}.\end{cases}\]

Note that for every input example \(\underline{\mathbf{x}}\) and perturbation \(\underline{\boldsymbol{\delta}}\), \(f_{\mu}(\underline{\mathbf{x}}+\underline{\boldsymbol{\delta}};\underline{ \mathbf{V}})=f(\underline{\mathbf{x}}+\underline{\boldsymbol{\delta}}; \underline{\mathbf{W}}^{*})\), then \(\underline{\mathbf{V}}\) is also feasible to Problem (14). Note that we have

\[\frac{\text{d}}{\text{d}\mu}\left(\left\|\mu\underline{\mathbf{W}}^{*(i)} \right\|_{\text{F}}^{2}+\left\|\mu^{-1}\underline{\mathbf{W}}^{*(j)}\right\|_ {\text{F}}^{2}\right)=2\mu\left\|\underline{\mathbf{W}}^{*(i)}\right\|_{\text {F}}^{2}-2\mu^{-3}\left\|\underline{\mathbf{W}}^{*(j)}\right\|_{\text{F}}^{2}.\]

When \(\mu=1\) the above expression equals \(2\left\|\underline{\mathbf{W}}^{*(i)}\right\|_{\text{F}}^{2}-2\left\|\underline {\mathbf{W}}^{*(j)}\right\|_{\text{F}}^{2}\). Hence, if \(\left\|\underline{\mathbf{W}}^{*(i)}\right\|_{\text{F}}\neq\left\|\underline{ \mathbf{W}}^{*(j)}\right\|_{\text{F}}\), then the derivative at \(\mu\) is non-zero, which leads to a contradiction to the optimality of \(\mathbf{W}^{*}\) to Problem (14). Note that if we consider changing norms of \(\underline{\mathbf{W}}^{*(i)}\) and \(\mathbf{w}\) instead of \(\underline{\mathbf{W}}^{*(i)}\) and \(\underline{\mathbf{W}}^{*(j)}\), the same conclusion also holds. Thus, the optimality of \(\underline{\mathbf{W}}^{*}\) strictly leads to \(\left\|\underline{\mathbf{W}}^{*(i)}\right\|_{\text{F}}=\left\|\underline{ \mathbf{W}}^{*(j)}\right\|_{\text{F}}=\left\|\underline{\mathbf{w}}^{*} \right\|_{2}\). 

## Appendix E Generalization bound of approximately low-tubal-rank t-NNs

Proof of Theorem 12.: Given a \((\delta,\mathbf{r})\)-approximately low-tubal-rank parameterized t-NN \(f(\underline{\mathbf{x}};\underline{\mathbf{W}})\in\mathfrak{F}_{\delta, \mathbf{r}}\subset\mathfrak{F}_{\delta,\mathbf{r}}\), let \(g(\underline{\mathbf{x}};\underline{\mathbf{W}}_{\mathbf{r}})\in\mathfrak{F}_ {\mathbf{r}}\) be its compressed version whose t-product layer weight tensors have tubal-ranks upper bounded by \(\mathbf{r}\).

**Step 1: Upper bound the adversarial empirical \(L_{2}\)-distance between \(f\) and \(g\).** Consider function \(g(\underline{\mathbf{x}})=g(\underline{\mathbf{x}};\underline{\mathbf{W}}_{ \mathbf{r}})\) parameterized by \(\underline{\mathbf{W}}_{\mathbf{r}}=(\underline{\mathbf{W}}_{r_{1}}^{(1)}, \cdots,\underline{\mathbf{W}}_{r_{L}}^{(L)},\mathbf{w})\) as the function whose t-product layer weights are low-tubal-rank approximations of \(f(\underline{\mathbf{x}};\underline{\mathbf{W}})\). Let \(\tilde{f}(\underline{\mathbf{x}},y)=\inf_{R_{\text{s}}(\underline{\mathbf{x}} -\underline{\mathbf{x}}^{\prime})\leq\xi}yf(\underline{\mathbf{x}}^{\prime})\) and \(\tilde{g}(\underline{\mathbf{x}},y)=\inf_{R_{\text{s}}(\underline{\mathbf{x}} -\underline{\mathbf{x}}^{\prime})\leq\xi}yg(\underline{\mathbf{x}}^{\prime})\) denote the adversarial versions of \(f\) and \(g\), respectively.

We first bound the adversarial empirical \(L_{2}\)-distance between \(f\) and \(g\) as follows.

\[|\tilde{f}(\underline{\mathbf{x}}_{i},y_{i})-\tilde{g}(\underline{\mathbf{x}}_{i},y_{i})|=|\inf_{R_{\text{s}}(\underline{\mathbf{x}}_{i}-\underline{\mathbf{x}}^{ \prime}_{i})\leq\xi}y_{i}f(\underline{\mathbf{x}}^{\prime}_{i})-\inf_{R_{\text{s}}( \underline{\mathbf{x}}_{i}-\underline{\mathbf{x}}^{\prime}_{i})\leq\xi}y_{i}g( \underline{\mathbf{x}}^{\prime}_{i})|.\]Letting \(\mathbf{x}_{i}^{f}=\text{arginf}_{R_{t}(\mathbf{x}_{i}-\mathbf{x}_{i}^{f})\leq\xi} \,y_{i}f(\mathbf{x}_{i}^{f})\) and \(\mathbf{x}_{i}^{g}=\text{arginf}_{R_{t}(\mathbf{x}_{i}-\mathbf{x}_{i}^{f})\leq \xi}\,y_{i}g(\mathbf{x}_{i}^{f})\), we have \(|\tilde{f}(\mathbf{x}_{i},y_{i})-\tilde{g}(\mathbf{x}_{i},y_{i})|=|y_{i}f( \mathbf{x}_{i}^{f})-y_{i}g(\mathbf{x}_{i}^{g})|\). By letting

\[\mathbf{x}_{i}^{\xi}=\begin{cases}\mathbf{x}_{i}^{g}&\text{if}\quad y_{i}f( \mathbf{x}_{i}^{f})\geq y_{i}g(\mathbf{x}_{i}^{g})\\ \mathbf{x}_{i}^{f}&\text{otherwise}\end{cases},\]

we obtain

\[|\tilde{f}(\mathbf{x}_{i},y_{i})-\tilde{g}(\mathbf{x}_{i},y_{i})|=|y_{i}f( \mathbf{x}_{i}^{f})-y_{i}g(\mathbf{x}_{i}^{g})|\leq|y_{i}f(\mathbf{x}_{i}^{\xi })-y_{i}g(\mathbf{x}_{i}^{\xi})|=|f(\mathbf{x}_{i}^{\xi})-g(\mathbf{x}_{i}^{ \xi})|.\]

Let \(h_{l}(\mathbf{x}^{\xi})=\mathbf{w}^{\top}{}_{\text{v}\in\mathbb{C}}\bigg{(} \sigma(\underline{\mathbf{W}}_{r_{1}}^{(L)}*_{M}\sigma(\underline{\mathbf{W}}_ {r-1}^{(L-1)}*_{M}\cdots*_{M}\sigma(\underline{\mathbf{W}}_{r}^{(l+1)}*_{M} \sigma(\underline{\mathbf{W}}^{(l)}*_{M}\cdots*_{M}\sigma(\underline{\mathbf{ W}}^{(1)}*_{M}\]

\(\mathbf{x}^{\xi})\cdots)))\bigg{)}\) and \(h_{0}(\mathbf{x}^{\xi})=g(\mathbf{x}^{\xi})\). Then, we have

\[|f(\mathbf{x}_{i}^{\xi})-g(\mathbf{x}_{i}^{\xi})|\leq\sum_{l=1}^{L}|h_{l}( \mathbf{x}_{i}^{\xi})-h_{l-1}(\mathbf{x}_{i}^{\xi})|.\]

We can see that for any \(l=1,\cdots,L\):

\[\Big{\|}\sigma(\underline{\mathbf{W}}^{(l-1)}*_{M}\cdots*_{M}\sigma( \underline{\mathbf{W}}^{(1)}*_{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots)) \Big{\|}_{\text{F}}\leq\prod_{l^{\prime}=1}^{l-1}\Big{\|}\underline{\mathbf{W }}^{(l^{\prime})}\Big{\|}_{\text{F}}\,\Big{\|}\mathbf{x}_{i}^{\xi}\Big{\|}_{ \text{F}}\leq\prod_{l^{\prime}=1}^{l-1}\Big{\|}\underline{\mathbf{W}}^{(l^{ \prime})}\Big{\|}_{\text{F}}\,B_{x,R_{*}\xi},\]

and

\[\Big{\|}\sigma(\underline{\mathbf{W}}_{r_{1}}^{(l)}*_{M}\sigma( \underline{\mathbf{W}}^{(l-1)}*_{M}\cdots*_{M}\sigma(\underline{\mathbf{W}}^{ (1-1)}*_{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots))-\sigma(\underline{\mathbf{ W}}^{(l)}*_{M}\sigma(\underline{\mathbf{W}}^{(l-1)}*_{M}\sigma( \underline{\mathbf{W}}^{(1)}*_{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots)) \Big{\|}_{\text{F}}\] \[\leq\Big{\|}\underline{\mathbf{W}}_{r_{1}}^{(l)}*_{M}\sigma( \underline{\mathbf{W}}^{(l-1)}*_{M}\cdots*_{M}\sigma(\underline{\mathbf{W}}^{ (1)}*_{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots)-\underline{\mathbf{W}}^{(l)}* _{M}\sigma(\underline{\mathbf{W}}^{(l-1)}*_{M}\cdots*_{M}\sigma(\underline{ \mathbf{W}}^{(1)}*_{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots)\Big{\|}_{\text{F}}\] \[=\Big{\|}(\underline{\mathbf{W}}_{r_{1}}^{(l)}-\underline{\mathbf{ W}}^{(l)})*_{M}\sigma(\underline{\mathbf{W}}^{(l-1)}*_{M}\cdots*_{M}\sigma( \underline{\mathbf{W}}^{(1)}*_{M}\underline{\mathbf{x}}_{i}^{\xi})\cdots)\Big{\|} _{\text{F}}\] \[\leq\Big{\|}\underline{\mathbf{W}}_{r_{1}}^{(l)}-\underline{\mathbf{ W}}^{(l)}\Big{\|}_{\text{F}}\,\Big{\|}\sigma(\underline{\mathbf{W}}^{(l-1)}*_{M} \cdots*_{M}\sigma(\underline{\mathbf{W}}^{(1)}*_{M}\underline{\mathbf{x}}_{i}^{ \xi})\cdots)\Big{\|}_{\text{F}}\] \[\leq\delta_{l}\prod_{l^{\prime}=1}^{l}\Big{\|}\underline{\mathbf{ W}}^{(l^{\prime})}\Big{\|}_{\text{F}}\,B_{x,R_{*}\xi}.\]

Thus, we have

\[|h_{l}(\mathbf{x}_{i}^{\xi})-h_{l-1}(\mathbf{x}_{i}^{\xi})|\leq B_{\mathbf{w}} \prod_{j\neq l}B_{j}\delta B_{x,R_{*}\xi}=\frac{\delta B_{f}}{B_{l}}.\]

This gives

\[|f(\mathbf{x}_{i}^{\xi})-g(\mathbf{x}_{i}^{\xi})|\leq\sum_{l=1}^{L}|h_{l}( \mathbf{x}_{i}^{\xi})-h_{l-1}(\mathbf{x}_{i}^{\xi})|\leq\sum_{l=1}^{L}\frac{ \delta B_{f}}{B_{l}}.\]

Then, we can set

\[\hat{\mathbf{t}}=\delta B_{f}\sum_{l=1}^{L}B_{l}^{-1}.\] (46)

**Step 2: Divide and conquer the adversarial gap.** To upper bound the adversarial gap \(\mathcal{L}^{\text{adv}}(f)-\hat{\mathcal{L}}^{\text{adv}}(f)\) of \(f\) by using the properties of its compressed version \(g\), we first decompose the adversarial gap into three terms as follows

\[\mathcal{L}^{\text{adv}}(f)-\hat{\mathcal{L}}^{\text{adv}}(f)\] \[=\underbrace{\big{[}(\mathcal{L}^{\text{adv}}(f)-\mathcal{L}^{ \text{adv}}(g))-(\hat{\mathcal{L}}^{\text{adv}}(f)-\hat{\mathcal{L}}^{\text{ adv}}(g))\big{]}}_{\text{F}}+\underbrace{\big{(}\mathcal{L}^{\text{adv}}(g)-\hat{ \mathcal{L}}^{\text{adv}}(g)\big{)}}_{\text{F}}.\] (47)_Step 2.1: Upper bound_ **II**. We first consider the event \(\mathcal{E}_{1}\) in which term **II** is upper bounded with high probability. As \(g\in\mathfrak{F}_{\mathbf{r}}\), the term **II** has already been upper bounded according to Theorem 6 as

\[\textbf{II}\leq\frac{C^{\prime}L_{\ell}B_{\tilde{f}}}{\sqrt{N}}\sqrt{\mathsf{c} \sum_{l=1}^{L}r_{l}(d_{l-1}+d_{l})\log(9(L+1))}+3B\sqrt{\frac{t}{2N}},\] (48)

with high probability \(1-2e^{-t}\).

_Step 2.2: Upper bound_ **I**. Note that term **I** can be written as

\[\begin{split}&\mathcal{L}^{\text{adv}}(f)-\mathcal{L}^{\text{adv }}(g)-(\hat{\mathcal{L}}^{\text{adv}}(f)-\hat{\mathcal{L}}^{\text{adv}}(g))\\ &=\frac{1}{N}\sum_{i=1}^{N}\left(\ell(\tilde{f}(\textbf{x}),y)- \ell(\tilde{g}(\textbf{x}),y)-\mathbb{E}[\ell(\tilde{f}(\textbf{x}),y)- \ell(\tilde{g}(\textbf{x}),y)]\right).\end{split}\] (49)

_Step (2.2.1): Characterize the concentration behavior of \(\ell\circ\tilde{f}-\ell\circ\tilde{g}\)._ Given a constant \(\mathsf{r}>0\), consider the event \(\mathcal{E}_{2}(\mathsf{r})\) in which \(\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathsf{r}\) already holds with high probability. Then, conditioned on Event \(\mathcal{E}_{2}(\mathsf{r})\), by using the \(L_{\ell}\)-Lipschitz continuity of the loss function \(\ell(\cdot,\cdot)\) derived from Assumption 2, it can be proved that \(\ell(\tilde{f}(\textbf{x}),y)-\ell(\tilde{g}(\textbf{x}),y)\) also has a small population \(L_{2}\)-norm with high probability.

Regarding Eq. (49), it is natural to characterize the concentration behavior of centered random variable \(\ell(\tilde{f}(\textbf{x}),y)-\ell(\tilde{g}(\textbf{x}),y)-\mathbb{E}[\ell( \tilde{f}(\textbf{x}),y)-\ell(\tilde{g}(\textbf{x}),y)]\).

* First, its variance under event \(\mathcal{E}_{2}\) can be upper bounded by \[\begin{split}&\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}} \leq\mathsf{r}}\texttt{Var}\left(\ell(\tilde{f}(\textbf{x}),y)-\ell(\tilde{g} (\textbf{x}),y)-\mathbb{E}[\ell(\tilde{f}(\textbf{x}),y)-\ell(\tilde{g}( \textbf{x}),y)]\right)\\ &=\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathsf{r}} \texttt{Var}\left(\ell(\tilde{f}(\textbf{x}),y)-\ell(\tilde{g}(\textbf{x}),y) \right)\\ &=\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathsf{r}} \texttt{E}_{(\textbf{x},y)}\left[(\ell(\tilde{f}(\textbf{x},y))-\ell(\tilde{g }(\textbf{x},y)))^{2}-\mathbb{E}_{(\textbf{x},y)}[\ell(\tilde{f}(\textbf{x},y) )-\ell(\tilde{g}(\textbf{x},y))]\right]\\ &\leq\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathsf{r }}\texttt{E}_{(\textbf{x},y)}\left[\ell(\tilde{f}(\textbf{x},y))-\ell(\tilde{g }(\textbf{x},y))\right]^{2}\\ &\leq\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathsf{ r}}L_{\ell}^{2}\mathbb{E}_{(\textbf{x},y)}\left[\tilde{f}(\textbf{x},y)-\tilde{g}( \textbf{x},y)\right]^{2}\\ &=\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathsf{r}} L_{\ell}^{2}\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}^{2}\\ &\leq L_{\ell}^{2}\mathsf{r}^{2}\end{split}\]
* Second, we upper bound its \(L_{\infty}\)-norm. First, Lemma 39 indicates that for any \(h\in\mathfrak{F}\) with adversarial version \(\tilde{h}\in\mathfrak{F}^{\text{adv}}\), we have \(\left\|\tilde{h}\right\|_{L_{\infty}}\leq B_{\tilde{f}}:=B_{\underline{ \textbf{W}}}B_{x,R_{\kappa},\xi}\). Then, by \(\mathfrak{F}_{\mathbf{r}}\subset\mathfrak{F}_{\delta,\textbf{r}}\subset \mathfrak{F}\), we have \(\left\|\tilde{f}\right\|_{L_{\infty}}\leq B_{\tilde{f}}\) and \(\left\|\tilde{g}\right\|_{L_{\infty}}\leq B_{\tilde{f}}\). Therefore, we can upper bound the \(L_{\infty}\)-norm of \(\ell(\tilde{f}(\mathbf{\underline{x}}),y)-\ell(\tilde{g}(\mathbf{\underline{x}}),y )-\mathbb{E}[\ell(\tilde{f}(\mathbf{\underline{x}}),y)-\ell(\tilde{g}(\mathbf{ \underline{x}}),y)]\) as follows

\[\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathfrak{r}} \left\|\ell(\tilde{f}(\mathbf{\underline{x}}),y)-\ell(\tilde{g}(\mathbf{ \underline{x}}),y)-\mathbb{E}[\ell(\tilde{f}(\mathbf{\underline{x}}),y)- \ell(\tilde{g}(\mathbf{\underline{x}}),y)]\right\|_{L_{\infty}}\] \[\leq\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq \mathfrak{r}}\left\|\ell(\tilde{f}(\mathbf{\underline{x}}),y)-\ell(\tilde{g}( \mathbf{\underline{x}}),y)\right\|_{L_{\infty}}+\mathbb{E}\left[\left\|\ell( \tilde{f}(\mathbf{\underline{x}}),y)-\ell(\tilde{g}(\mathbf{\underline{x}}),y )\right\|_{L_{\infty}}\right]\] \[\leq\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq \mathfrak{r}}L_{\ell}\sup_{(\mathbf{\underline{x}},y)}|y\tilde{f}(\mathbf{ \underline{x}}))-y\tilde{g}(\mathbf{\underline{x}})|+\mathbb{E}[L_{\ell}\sup _{(\mathbf{\underline{x}},y)}|y\tilde{f}(\mathbf{\underline{x}})-y\tilde{g}( \mathbf{\underline{x}})|]\] \[\leq\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq \mathfrak{r}}4L_{\ell}B_{\tilde{f}}.\]

Then, the Talagrand's concentration inequality (Lemma 35) yields that with probability at least \(1-e^{-t}\):

\[\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathfrak{r}} (\mathcal{L}^{\text{adv}}(f)-\mathcal{L}^{\text{adv}}(g))-(\hat{\mathcal{L}}^{ \text{adv}}(f)-\hat{\mathcal{L}}^{\text{adv}}(g))\] \[\leq 2\underbrace{\mathbb{E}\left[\sup_{\left\|\tilde{f}-\tilde{g} \right\|_{L_{2}}\leq\mathfrak{r}}(\mathcal{L}^{\text{adv}}(f)-\mathcal{L}^{ \text{adv}}(g))-(\hat{\mathcal{L}}^{\text{adv}}(f)-\hat{\mathcal{L}}^{\text{ adv}}(g))\right]}_{(\mathbf{I})}+\frac{\sqrt{2t}L_{\ell}\mathfrak{r}}{\sqrt{N}}+\frac{8tL_{\ell}B_{ \tilde{f}}}{N}.\]

Then, by the the standard symmetrization argument [47], we obtain an upper bound on term \((\mathbf{I})\) as follows:

\[\mathbb{E}\left[\sup_{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}} \leq\mathfrak{r}}(\mathcal{L}^{\text{adv}}(f)-\mathcal{L}^{\text{adv}}(g))-( \hat{\mathcal{L}}^{\text{adv}}(f)-\hat{\mathcal{L}}^{\text{adv}}(g))\right]\] (50) \[\leq 2\mathbb{E}_{(\mathbf{\underline{x}}_{i},y_{i})_{i=1}^{N}} \mathbb{E}_{(\varepsilon_{i})_{i=1}^{N}}\sup_{\left\|\tilde{f}-\tilde{g} \right\|_{L_{2}}\leq\mathfrak{r}}\left[\frac{1}{N}\varepsilon_{i}\left(\ell( \tilde{f}(\mathbf{\underline{x}}_{i},y_{i}))-\ell(\tilde{g}(\mathbf{ \underline{x}}_{i},y_{i}))\right)\right]\] \[=2\Phi(\mathfrak{r}),\]

where \(\Phi(\mathfrak{r})\) is defined as

\[\Phi(\mathfrak{r}):=\bar{R}_{N}\left(\left\{\ell\circ\tilde{f}-\ell\circ \tilde{g}\;\big{|}\;\tilde{f}\in\mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv} },\tilde{g}\in\mathfrak{F}_{\mathbf{r}}^{\text{adv}},\;\left\|\tilde{f}- \tilde{g}\right\|_{L_{2}}\leq\mathfrak{r}\right\}\right).\]

Thus, there is a constant \(C>0\) such that for any \(\tilde{f}-\tilde{g}\in(\mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv}}- \mathfrak{F}_{\mathbf{r}}^{\text{adv}})\) satisfying \(\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathfrak{r}\), it holds with probability at least \(1-e^{-t}\) that

\[(\mathcal{L}^{\text{adv}}(f)-\mathcal{L}^{\text{adv}}(g))-(\hat{\mathcal{L}}^{ \text{adv}}(f)-\hat{\mathcal{L}}^{\text{adv}}(g))\leq C\left(\Phi(\mathfrak{r}) +L_{\ell}\mathfrak{r}\sqrt{\frac{t}{N}}+\frac{tL_{\ell}B_{\tilde{f}}}{N}\right).\] (51)

We denote the above event by \(\mathcal{E}_{3}(\mathfrak{r})\). Note that Event \(\mathcal{E}_{3}(\mathfrak{r})\) is conditioned on Event \(\mathcal{E}_{2}(\mathfrak{r})\).

_Step (2.2.2): Upper bound the probability of Event \(\mathcal{E}_{2}(\mathfrak{r}):=\left\{\left\|\tilde{f}-\tilde{g}\right\|_{L_{2} }\leq\mathfrak{r}\right\}\)._ We further bound the probability of Event \(\mathcal{E}_{2}(\mathfrak{r})\) in which \(\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}^{2}}\leq\mathfrak{r}\) holds. Generally speaking, \(\tilde{f}\) and \(\tilde{g}\) are date dependent and we can only bound the empirical \(L_{2}\)-distance between them. However, the local Rademacher complexity is characterized by the population \(L_{2}\)-norm. Thus, we need to bound the population \(L_{2}\)-distance between \(\tilde{f}\) and \(\tilde{g}\). Motivated by [43], we use the ratio type empirical process to bound the bound the population \(L_{2}\)-distance.

According to Assumption 11, their exists a function \(\phi:[0,\infty)\to[0,\infty)\) such that

\[\dot{R}_{\mathfrak{r}}(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F }_{\mathbf{r}}^{\text{adv}})\leq\phi(\mathfrak{r})\;\;\text{and}\;\;\phi(2 \mathfrak{r})\leq 2\phi(\mathfrak{r}),\;(\forall\mathfrak{r}>0).\]

Define the quantity \(\Gamma(\mathfrak{r}):=\mathbb{E}\left[\sup_{h}\left(\frac{1}{N}\sum_{i=1}^{N} \varepsilon_{i}h^{2}(\mathbbm{x}_{i},y_{i})\;\big{|}\;h\in(\mathfrak{F}_{ \delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{\text{adv}}):\left\| h\right\|_{L_{2}}\leq\mathfrak{r}\right)\right].\) Then, we have

\[\Gamma(\mathfrak{r}) =\mathbb{E}\left[\sup_{h}\left(\frac{1}{N}\sum_{i=1}^{N} \varepsilon_{i}h^{2}(\mathbbm{x}_{i},y_{i})\;\big{|}\;h\in(\mathfrak{F}_{ \delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{\text{adv}}): \left\|h\right\|_{L_{2}}\leq\mathfrak{r}\right)\right]\] \[\overset{(i)}{\leq}2B_{f}\mathbb{E}\left[\sup_{h}\left(\frac{1}{ N}\sum_{i=1}^{N}\varepsilon_{i}h(\mathbbm{x}_{i},y_{i})\;\big{|}\;h\in( \mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{ \text{adv}}):\left\|h\right\|_{L_{2}}\leq\mathfrak{r}\right)\right]\] \[\leq 2B_{f}\dot{R}_{\mathfrak{r}}(\mathfrak{F}_{\delta,\mathbf{r} }^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{\text{adv}})\overset{(ii)}{\leq}2B_ {f}\phi(\mathfrak{r}),\]

where \((i)\) is by the Talagrand's contraction lemma (Lemma 30).

We can verify that the square \(h^{2}(\cdot)\) of any function \(h\in\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{ \text{adv}}\) satisfies

* its \(L_{\infty}\)-norm is upper bounded by \(B_{f}^{2}\), i.e., \(\left\|h^{2}\right\|_{L_{\infty}}=\sup_{(\mathbbm{x},y)}|h^{2}(\mathbbm{x},y)| \leq B_{f}^{2}\).
* its second-order moment satisfies \(\mathbb{E}_{\mathbbm{x},y}\left[(h^{2}(\mathbbm{x},y)^{2})\right]\leq\mathbb{ E}_{\mathbbm{x},y}\left[B_{f}^{2}(h^{2}(\mathbbm{x},y))\right]=B_{f}^{2}\mathbb{E}_{ \mathbbm{x},y}\left[h^{2}(\mathbbm{x},y)\right]\).

Thus, \(h^{2}\) satisfy the conditions in Eq. (7.6) and Eq. (7.7) of [42] with parameters \(B=B_{f}^{2},V=B_{f}^{2}\) and \(\vartheta=1\). Noting that we have upper bounded \(\Gamma(\mathfrak{r})\) by \(2B_{f}\phi(\mathfrak{r})\), then by the peeling trick [42, Eq. (7.17)], we can show for any \(\mathfrak{r}>\inf\{\sqrt{\mathbb{E}[h^{2}]}:h\in(\mathfrak{F}_{\delta, \mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{\text{adv}}):\left\|h \right\|_{L_{2}}\leq\mathfrak{r}\}\) and \(t>0\) that

\[\mathbb{P}\left[\sup_{h\in\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}- \mathfrak{F}_{\mathbf{r}}^{\text{adv}}}\frac{\left\|h\right\|_{L_{2}}^{2}- \left\|h\right\|_{S}^{2}}{\left\|h\right\|_{L_{2}}^{2}+\mathfrak{r}^{2}}\geq 8 \frac{2B_{f}\phi(\mathfrak{r})}{\mathfrak{r}^{2}}+B_{f}\sqrt{\frac{2t}{ \mathfrak{r}^{2}N}}+B_{f}^{2}\frac{2t}{\mathfrak{r}^{2}N}\right]\leq e^{-t}.\]

We further define a function \(\mathfrak{r}_{*}=\mathfrak{r}_{*}(t)\) as

\[\mathfrak{r}_{*}(t):=\inf\left\{\mathfrak{r}>0\;\bigg{|}\;\frac{16B_{f}\phi( \mathfrak{r})}{\mathfrak{r}^{2}}+B_{f}\sqrt{\frac{2t}{\mathfrak{r}^{2}N}}+B_{ f}^{2}\frac{2t}{\mathfrak{r}^{2}N}\leq\frac{1}{2}\right\},\] (52)

which is useful to bound the ratio of the empirical \(L_{2}\)-norm and the population \(L_{2}\)-norm of an elements \(h\in\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{ \text{adv}}\) with probability at least \(1-e^{t}\):

\[\frac{\left\|h\right\|_{L_{2}}^{2}-\left\|h\right\|_{S}^{2}}{\left\|h\right\|_{ L_{2}}^{2}+\mathfrak{r}_{*}^{2}}\leq\frac{1}{2}\quad\Rightarrow\quad\left\|h \right\|_{L_{2}}^{2}\leq 2(\left\|h\right\|_{S}^{2}+\mathfrak{r}_{*}^{2}).\]

Recalling that \(\left\|\tilde{f}-\tilde{g}\right\|_{S}\leq\hat{\mathfrak{r}}\), we obtain that the probability of Event \(\mathcal{E}_{2}(\hat{\mathfrak{r}})\) with \(\hat{\mathfrak{r}}=\sqrt{2(\hat{\mathfrak{r}}^{2}+\mathfrak{r}_{*}^{2}(t))}\) is at least \(1-e^{-t}\).

_Step 2.3: Combining Events \(\mathcal{E}_{1},\mathcal{E}_{2}(\hat{\mathfrak{r}}),\mathcal{E}_{3}(\hat{ \mathfrak{r}})\)._ By combining Eqs. (46), (47), (51) and (48) along with their underlying events \(\mathcal{E}_{1},\mathcal{E}_{2}(\hat{\mathfrak{r}}),\mathcal{E}_{3}(\hat{ \mathfrak{r}})\), we obtain

\[\mathcal{L}^{\text{adv}}(f)-\hat{\mathcal{L}}^{\text{adv}}(f) \leq\frac{C_{1}L_{\ell}B_{f}}{\sqrt{N}}\sqrt{\mathfrak{c}\sum_{l=1}^{L}r_{l }(d_{l-1}+d_{l})\log(9(L+1))}+B\sqrt{\frac{t}{2N}}\] \[\quad+C_{2}\left(\Phi(\hat{\mathfrak{r}})+L_{\ell}\hat{\mathfrak{ r}}\sqrt{\frac{t}{N}}+\frac{tL_{\ell}B_{f}}{N}\right),\]

with probability at least \(1-4e^{-t}\).

### Several Useful Results

According to Theorem 12, it remains to upper bound \(\Phi(\dot{\mathfrak{r}})\). In this subsection, we derive upper bounds on \(\Phi(\mathfrak{r})\) in terms of covering numbers of the considered function sets \(\mathfrak{F}_{\mathbf{r}}^{\text{adv}}\) and \(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}\).

Consider the supremum of the empirical \(L_{2}\)-norm of any function \((\tilde{f}-\tilde{g})\in(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}- \mathfrak{F}_{\mathbf{r}}^{\text{adv}})\) on sample \(S=\{(\underline{\mathbf{x}}_{i},y_{i})\}_{i=1}^{N}\) when the population \(L_{2}\)-norm is bounded by a given radius \(\mathfrak{r}>0\) as follows

\[\beta_{S}=\beta_{S}(\mathfrak{r})=\sup\left\{\left\|\tilde{f}-\tilde{g} \right\|_{S}\ \left|\ \left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathfrak{r},\,\tilde{f}\in \mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}},\tilde{g}\in\mathfrak{F}_{ \mathbf{r}}^{\text{adv}}\right\}.\] (53)

Recall that we have assumed \(\hat{R}_{\mathfrak{r}}(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}- \mathfrak{F}_{\mathbf{r}}^{\text{adv}})\leq\phi(\mathfrak{r})\). We now give an explict example of \(\phi(\mathfrak{r})\) in terms of the sum of covering entropy of \(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}\) and \(\mathfrak{F}_{\mathbf{r}}^{\text{adv}}\)

\[\begin{split}&\hat{R}_{\mathfrak{r}}(\mathfrak{F}_{\delta, \mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{\text{adv}})\\ &\stackrel{{(i)}}{{\leq}}\mathbb{E}_{S}\hat{R}_{ \mathfrak{r}}\left(\left\{\left(\tilde{f}-\tilde{g}\right)\ \in(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{ \text{adv}}),\ \left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathfrak{r}\right\}\right)\\ &\stackrel{{(ii)}}{{\leq}}\inf_{a}\left[a+\mathbb{E}_{S }\int_{a}^{\beta_{S}}\sqrt{\frac{\log\mathsf{N}\left(\left\{\left(\tilde{f}- \tilde{g}\right)\ \in(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{ \text{adv}}),\ \left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathfrak{r}\right\} \right),\left\|\cdot\right\|_{S},\epsilon)}{N}}\ \mathsf{d}\epsilon\\ &\stackrel{{(iii)}}{{\leq}}\frac{1}{N}+\mathbb{E}_{S} \int_{1/N}^{\beta_{S}}\sqrt{\frac{\log\mathsf{N}\left(\left\{\left(\tilde{f}- \tilde{g}\right)\ \in(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{ \text{adv}}),\ \left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathfrak{r}\right\} \right),\left\|\cdot\right\|_{S},\epsilon)}{N}}\\ &\stackrel{{(iv)}}{{\leq}}\frac{1}{N}+\mathbb{E}_{S} \int_{1/N}^{\beta_{S}}\sqrt{\frac{\log\mathsf{N}(\mathfrak{F}_{\delta,\mathbf{ r}}^{\text{adv}},\left\|\cdot\right\|_{S},\epsilon/2)+\log\mathsf{N}(\mathfrak{F}_{ \mathbf{r}}^{\text{adv}},\left\|\cdot\right\|_{S},\epsilon/2)}{N}}\ \mathsf{d}\epsilon\\ &=:\phi(\mathfrak{r}),\end{split}\] (54)

where \((i)\) is the definition of localized Rademacher complexity; \((ii)\) is due to Dudley's inequality (Lemma 31); \((iii)\) is obtained by letting \(a=1/N\); \((iv)\) holds by Lemma 34.

**Lemma 26**.: _We can upper bound \(\Phi(\mathfrak{r})\) by using \(\phi(\mathfrak{r})\) as follows_

\[\Phi(\mathfrak{r})\leq CL_{\ell}\phi(\mathfrak{r}).\] (55)

Proof.: Recall that \(\Phi(\mathfrak{r})\) is the average Rademacher complexity of the function set

\[\left\{\ell\circ\tilde{f}-\ell\circ\tilde{g}\ \Big{|}\ \left\|\tilde{f}-\tilde{g} \right\|_{L_{2}}\leq\mathfrak{r},\ \tilde{f}\in\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}},\tilde{g}\in \mathfrak{F}_{\mathbf{r}}^{\text{adv}}\right\}.\]

As \(\ell(\cdot)\) is \(L_{\ell}\)-Lipschitz, we have for any functions \(\tilde{f}\in\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}},\tilde{g}\in \mathfrak{F}_{\mathbf{r}}^{\text{adv}}\) satisfying \(\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathfrak{r}\)

\[\begin{split}\sup_{\tilde{f},\tilde{g}}\left\|\ell\circ\tilde{f}- \ell\circ\tilde{g}\right\|_{S}&=\sup_{\tilde{f},\tilde{g}} \sqrt{\sum_{i=1}^{N}\frac{1}{N}\Big{(}\ell(\tilde{f}(\underline{\mathbf{x}}_{i}, y_{i})-\ell(\tilde{g}(\underline{\mathbf{x}}_{i},y_{i})\Big{)}^{2}} \\ &\stackrel{{(i)}}{{\leq}}\sup_{\tilde{f},\tilde{g}} \sqrt{\sum_{i=1}^{N}\frac{1}{N}\Big{(}L_{\ell}(\tilde{f}(\underline{\mathbf{x}}_{i },y_{i})-\tilde{g}(\underline{\mathbf{x}}_{i},y_{i}))\Big{)}^{2}}\\ &=\sup_{\tilde{f},\tilde{g}}L_{\ell}\sqrt{\sum_{i=1}^{N}\frac{1}{N} \Big{(}\tilde{f}(\underline{\mathbf{x}}_{i},y_{i})-\tilde{g}(\underline{ \mathbf{x}}_{i},y_{i})\Big{)}^{2}}\\ &=L_{\ell}\sup_{\tilde{f},\tilde{g}}\left\|\tilde{f}-\tilde{g} \right\|_{S}\\ &=L_{\ell}\beta_{S},\end{split}\]where \((i)\) holds because the loss function \(\ell\) is \(L_{\ell}\)-Lischitz continous.

To bound \(\Phi(\mathfrak{r})\), we first bound the its empirical version using the Dudley's inequity (Lemma 31) up to a constant as follows

\[\inf_{a>0}\left[a+\int_{a}^{L_{\ell}\beta_{S}}\sqrt{\frac{\log \mathsf{N}\left(\left\{\ell\circ\tilde{f}-\ell\circ\tilde{g}\mid\tilde{f} \in\mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv}},\tilde{g}\in\mathfrak{F}_ {\mathfrak{r}}^{\text{adv}},\;\left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq \mathfrak{r}\right\}\right),\left\|\cdot\right\|_{S},\epsilon}{N}}\;\mathrm{d} \epsilon\right]\] (56) \[\stackrel{{(i)}}{{\leq}}\frac{L_{\ell}}{N}+\int_{L_{ \ell}/N}^{L_{\ell}\beta_{S}}\sqrt{\frac{\log\mathsf{N}\left(\left\{\ell\circ \tilde{f}-\ell\circ\tilde{g}\mid\tilde{f}\in\mathfrak{F}_{\delta,\mathfrak{r }}^{\text{adv}},\tilde{g}\in\mathfrak{F}_{\mathfrak{r}}^{\text{adv}},\; \left\|\tilde{f}-\tilde{g}\right\|_{L_{2}}\leq\mathfrak{r}\right\}\right), \left\|\cdot\right\|_{S},\epsilon}{N}}\;\mathrm{d}\epsilon\] \[\stackrel{{(ii)}}{{\leq}}\frac{L_{\ell}}{N}+\int_{L_{ \ell}/N}^{L_{\ell}\beta_{S}}\sqrt{\frac{\log\mathsf{N}\left(\left\{\tilde{f}- \tilde{g}\mid\tilde{f}\in\mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv}}, \tilde{g}\in\mathfrak{F}_{\mathfrak{r}}^{\text{adv}},\;\left\|\tilde{f}- \tilde{g}\right\|_{L_{2}}\leq\mathfrak{r}\right\}\right),\left\|\cdot\right\|_ {S},\epsilon/L_{\ell}}{N}}\;\mathrm{d}\epsilon\] \[\stackrel{{(iii)}}{{\leq}}\frac{L_{\ell}}{N}+\int_{1/ N}^{\beta_{S}}\sqrt{\frac{\log\mathsf{N}\left(\left\{\tilde{f}-\tilde{g}\mid \tilde{f}\in\mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv}},\tilde{g}\in \mathfrak{F}_{\mathfrak{r}}^{\text{adv}},\;\left\|\tilde{f}-\tilde{g}\right\| _{L_{2}}\leq\mathfrak{r}\right\}\right),\left\|\cdot\right\|_{S},t}{N}}\;L_{ \ell}\mathrm{d}t\] \[=L_{\ell}\phi(\mathfrak{r}),\]

where in \((i)\) we let \(a=L_{\ell}/N\); \((ii)\) holds by the Lipschitzness of \(\ell\) and the definition of covering number; we use change of variable \(t=\epsilon/L_{\ell}\) in \((iii)\).

By taking expectations on the RHS of Eq. (56) with respect to the sample \(S\), we obtain Eq. (55).

To determine an appropriate radius of the population \(L_{2}\)-norm \(\dot{\mathfrak{r}}=2\sqrt{\dot{\mathfrak{r}}^{2}+\mathfrak{r}_{*}^{2}}\), we need to compute the value of \(\mathfrak{r}_{*}\) of satisfying Eq. (52). Using Eq. (54), we show how to compute \(\phi(\mathfrak{r})\) when the covering numbers of \(\mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv}}\) and \(\mathfrak{F}_{\mathfrak{r}}^{\text{adv}}\) satisfy a special bound.

**Lemma 27** (Adapted from Lemma 3 in Ref. [43]).: _Suppose that the covering numbers of \(\mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv}}\) and \(\mathfrak{F}_{\mathfrak{r}}^{\text{adv}}\) satisfy_

\[\sup_{S}\log\mathsf{N}(\mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv}},\left\| \cdot\right\|_{S},\epsilon/2)+\sup_{S}\log\mathsf{N}(\mathfrak{F}_{\mathfrak{ r}}^{\text{adv}},\left\|\cdot\right\|_{S},\epsilon/2)\leq a_{1}+a_{2}\log( \epsilon^{-1})+a_{3}\epsilon^{-2q}\] (57)

_for some \(q\leq 1\). Then, it holds that_

* _The bound_ \(\phi(\mathfrak{r})\) _of the local Rademacher complexity_ \(\dot{R}_{\mathfrak{r}}(\mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv}}- \mathfrak{F}_{\mathfrak{r}}^{\text{adv}})\) _of radius_ \(\mathfrak{r}\) _can be upper bounded as_ \[\phi(\mathfrak{r})\leq C\max\bigg{\{}\frac{1}{N}+B_{\tilde{f}} \frac{a_{1}+a_{2}\log N}{N}+\mathfrak{r}\sqrt{\frac{a_{1}+a_{2}\log N}{N}},\] (58) _for a universal constant_ \(C>0\) _and a constant_ \(C_{q}>0\) _which only depends on_ \(q\leq 1\)_._
* _In particular, the quantity_ \(\mathfrak{r}_{*}(t)\) _satisfying Eq. (_52_) can be upper bounded as_ \[\mathfrak{r}_{*}^{2}(t)\leq C\left[B_{\tilde{f}}\frac{a_{1}+a_{2}\log N}{N}+( \frac{a_{3}}{N})^{\frac{1}{1+q}}\left(B_{\tilde{f}}^{\frac{1-q}{1+q}}+1\right)+ \frac{1+tB_{\tilde{f}}}{N}\right].\]Proof of Lemma 27.: According to Eq. (54) which gives \(\phi(\mathfrak{r})\) and the definition of \(\beta_{S}\) in Eq. (53), we need to upper bound

\[\mathbb{E}_{S}\int_{1/N}^{\beta_{S}}\sqrt{\frac{\log\mathsf{N}( \mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv}},\left\lVert\cdot\right\rVert_{S },\epsilon/2)+\log\mathsf{N}(\mathfrak{F}_{\mathfrak{r}}^{\text{adv}},\left\lVert \cdot\right\rVert_{S},\epsilon/2)}{N}}\;\mathsf{d}\epsilon\] \[\leq\mathbb{E}_{S}\int_{1/N}^{\beta_{S}}\sqrt{\frac{a_{1}+a_{2} \log(\epsilon^{-1})+a_{3}\epsilon^{-2q}}{N}}\;\mathsf{d}\epsilon\] \[\leq\mathbb{E}_{S}\int_{1/N}^{\beta_{S}}\sqrt{\frac{a_{1}+a_{2} \log(\epsilon^{-1})}{N}}\;\mathsf{d}\epsilon+\mathbb{E}_{S}\int_{1/N}^{\beta_ {S}}\sqrt{\frac{a_{3}\epsilon^{-2q}}{N}}\;\mathsf{d}\epsilon\] \[\leq\sqrt{a_{1}+a_{2}\log N}\sqrt{\mathbb{E}_{S}\beta_{S}}+\frac {\sqrt{a_{3}}}{1-q}\mathbb{E}_{S}\beta_{S}^{1-q}\] \[\leq\underbrace{\sqrt{a_{1}+a_{2}\log N}\sqrt{2B_{\tilde{f}}\phi (\mathfrak{r})+\mathfrak{r}^{2}}}_{\mathbf{I}}+\underbrace{\frac{\sqrt{a_{3}} }{1-q}\left(2B_{\tilde{f}}\phi(\mathfrak{r})+\mathfrak{r}^{2}\right)^{\frac{1 -q}{2}}}_{\mathbf{I}}.\]

Hence, if \(\mathbf{I}\geq\mathbf{II}\), then

\[\phi(\mathfrak{r}) \leq C\left(\frac{1}{N}+\sqrt{\frac{a_{1}+a_{2}\log N}{N}}+ \sqrt{2B_{\tilde{f}}\phi(\mathfrak{r})+\mathfrak{r}^{2}}\right)\] \[\leq\frac{C}{N}+C^{2}B_{\tilde{f}}\frac{a_{1}+a_{2}\log N}{N}+C \mathfrak{r}\sqrt{\frac{a_{1}+a_{2}\log N}{N}}+\frac{\phi(\mathfrak{r})}{2},\]

which leads to

\[\phi(\mathfrak{r})\leq\frac{2C}{N}+2C^{2}B_{\tilde{f}}\frac{a_{1}+a_{2}\log N }{N}+2C\mathfrak{r}\sqrt{\frac{a_{1}+a_{2}\log N}{N}}.\]

If \(\mathbf{I}<\mathbf{II}\), then by using Young's inequality we obtain

\[\phi(\mathfrak{r}) \leq C\left(\frac{1}{N}+\frac{\sqrt{a_{3}}}{1-q}\left(2B_{\tilde {f}}\phi(\mathfrak{r})+\mathfrak{r}^{2}\right)^{\frac{1-q}{2}}\right)\] \[\leq\frac{C}{N}+C\left(q\left(\frac{c_{1}^{1-q}C^{2}a_{3}}{N(1-q) ^{2}}\right)^{\frac{1}{1+q}}+(1-q)\frac{2B_{\tilde{f}}\phi(\mathfrak{r})}{c_ {1}}+\sqrt{\frac{a_{3}}{N(1-q)}\mathfrak{r}^{2(1-q)}}\right),\]

for any \(c_{1}>0\). Thus, by taking \(c_{1}=4C(1-q)B_{\tilde{f}}\), we obtain

\[\phi(\mathfrak{r}) \leq\frac{2C}{N}+2qc\left(\frac{(4C(1-q)B_{\tilde{f}})^{1-q}C^{2 }a_{3}}{N(1-q)^{2}}\right)^{\frac{1}{1+q}}+2C\sqrt{\frac{a_{3}}{N(1-q)} \mathfrak{r}^{2(1-q)}}\] \[\leq\frac{2C}{N}+\frac{2qc^{2}\cdot 4^{\frac{1-q}{1+q}}}{1-q} \left(\frac{B_{\tilde{f}}^{1-q}a_{3}}{N}\right)^{\frac{1}{1+q}}+\frac{2C}{(1- q)}\mathfrak{r}^{(1-q)}\sqrt{\frac{a_{3}}{N}}\] \[\leq\frac{C_{q}}{N}+C_{q}\left(\frac{B_{\tilde{f}}^{1-q}a_{3}}{ N}\right)^{\frac{1}{1+q}}+C_{q}\mathfrak{r}^{(1-q)}\sqrt{\frac{a_{3}}{N}},\]

where \(C_{q}>0\) is a universal constant only depending on \(q\).

Then, we obtain the bound on \(\phi(\mathfrak{r})\) in Eq. (58). The bound on \(\mathfrak{r}_{*}^{2}\) can be obtained by simple calculations based on Eqs. (52) and (58). 

**Lemma 28**.: _When the population \(L_{2}\)-norm of any function \(h=\tilde{f}-\tilde{g}\in(\mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv}}- \mathfrak{F}_{\mathfrak{r}}^{\text{adv}})\) is upper bounded by \(\mathfrak{r}\), its squared empirical \(L_{2}\)-norm can be upper bounced as follows:_

\[\mathbb{E}_{S}\left[\sup_{h}\left(\frac{1}{N}\sum_{i=1}^{N}h(\mathbf{\underline{ x}}_{i},y_{i})^{2}\;\bigg{|}\;h\in(\mathfrak{F}_{\delta,\mathfrak{r}}^{\text{adv}}- \mathfrak{F}_{\mathfrak{r}}^{\text{adv}})\text{ and }\left\lVert h\right\rVert_{L_{2}}\leq \mathfrak{r}\right)\right]\leq 2B_{\tilde{f}}\dot{R}_{\mathfrak{r}}(\mathfrak{F}_{\delta,\mathfrak{ r}}^{\text{adv}}-\mathfrak{F}_{\mathfrak{r}}^{\text{adv}})+\mathfrak{r}^{2}.\] (59)Proof.: The squared empirical \(L_{2}\)-norm can be upper bounced based on the population \(L_{2}\)-norm as follows

\[\mathbb{E}_{S}\left[\sup\left(\frac{1}{N}\sum_{i=1}^{N}h(\underline{ \mathbf{x}}_{i},y_{i})^{2}\;\big{|}\;h\in(\mathfrak{F}_{\delta,\mathbf{r}}^{ \text{adv}}-\mathfrak{F}_{\mathbf{r}}^{\text{adv}})\text{ and }\left\|h\right\|_{L_{2}}\leq \mathfrak{r}\right)\right]\] \[\leq\mathbb{E}\left[\sup\left(\frac{1}{N}\sum_{i=1}^{N}h( \underline{\mathbf{x}}_{i},y_{i})^{2}-\mathbb{E}_{(\underline{\mathbf{x}}_{i},y_{i})}[h(\underline{\mathbf{x}},y)^{2}]\;\big{|}\;h\in(\mathfrak{F}_{\delta,\mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{\text{adv}})\text{ and }\left\|h\right\|_{L_{2}}\leq \mathfrak{r}\right)\right]+\mathfrak{r}^{2}\] \[\overset{(i)}{\leq}2B_{\tilde{f}}\mathbb{E}_{S,\boldsymbol{ \varepsilon}}\left[\sup\left(\frac{1}{N}\sum_{i=1}^{N}\varepsilon_{i}h( \underline{\mathbf{x}}_{i},y_{i})^{2}\;\big{|}\;h\in(\mathfrak{F}_{\delta, \mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{\text{adv}})\text{ and }\left\|h\right\|_{L_{2}}\leq \mathfrak{r}\right)\right]+\mathfrak{r}^{2}\] \[=2B_{\tilde{f}}\dot{R}_{\mathfrak{r}}(\mathfrak{F}_{\delta, \mathbf{r}}^{\text{adv}}-\mathfrak{F}_{\mathbf{r}}^{\text{adv}})+\mathfrak{ r}^{2}\] \[\leq 2B_{\tilde{f}}\phi(\mathfrak{r})+\mathfrak{r}^{2},\] (60)

where \((i)\) is due to the symmertrization argument [47] and \(\boldsymbol{\varepsilon}=\{\varepsilon_{i}\}_{i=1}^{N}\) are _i.i.d._ Rademacher variables, and \((ii)\) holds because of the contraction inequality (Lemma 30). 

### Adversarial Generalization Gap under Assumption 13

Proof of Theorem 14.: In this situation, we can see that for any \(1\leq r_{l}\leq\min\{d_{l},d_{l-1}\}\), we can approximate \(\underline{\mathbf{W}}^{(l)}\) with its optimal tubal-rank-\(r_{l}\) approximation tensor \(\underline{\mathbf{W}}^{(l)}_{r_{l}}\) according to [20, Theorem 3.7] and achieve the following approximation error bound on F-norm

\[\left\|\underline{\mathbf{W}}^{(l)}-\underline{\mathbf{W}}^{(l)}_ {r_{l}}\right\|_{\text{F}} =\left\|\underline{\mathbf{W}}^{(l)}\times_{3}\mathbf{M}- \underline{\mathbf{W}}^{(l)}_{r_{l}}\times_{3}\mathbf{M}\right\|_{\text{F}}\] \[\leq\sqrt{\sum_{k=1}^{\mathfrak{c}}\sum_{j=r_{l}+1}^{\min\{d_{l}, d_{l-1}\}}}\sigma_{j}^{2}(\underline{\mathbf{W}}^{(l)}\times_{3}\mathbf{M})_{:,:,k}\] \[\leq\sqrt{\sum_{k=1}^{\mathfrak{c}}\sum_{j=r_{l}+1}^{\min\{d_{l}, d_{l-1}\}}}(V_{0}\cdot j^{-\alpha})^{2}\] \[\leq\sqrt{\sum_{k=1}^{\mathfrak{c}}\frac{1}{2\alpha-1}V_{0}^{2}( r_{l}-1)^{1-2\alpha}}\] \[\overset{(i)}{\leq}\sqrt{\frac{\mathfrak{c}}{2\alpha-1}}V_{0}r_{ l}^{(1-2\alpha)/2}:=\delta_{l}^{\text{F}},\]

where \((i)\) holds because of Lemma 41.

We also have a specral norm bound for \(\underline{\mathbf{W}}^{(l)}-\underline{\mathbf{W}}^{(l)}_{r_{l}}\) according to [20, Theorem 3.7] under Assumption 13 as follows

\[\left\|\underline{\mathbf{W}}^{(l)}-\underline{\mathbf{W}}^{(l)}_{r_{l}} \right\|_{\text{sp}}\leq V_{0}(r_{l}+1)^{-\alpha}:=\delta_{l}^{\text{sp}},\]

and we also have

\[\sqrt{\mathfrak{c}}\delta_{l}^{\text{sp}}\leq\delta_{l}^{\text{F}}.\] (61)

Consider function \(g(\underline{\mathbf{x}})=g(\underline{\mathbf{x}};\underline{\mathbf{W}}_{ \mathbf{r}})\) parameterized by \(\underline{\mathbf{W}}_{\mathbf{r}}=(\underline{\mathbf{W}}^{(1)}_{r_{1}}, \cdots,\underline{\mathbf{W}}^{(L)}_{r_{L}},\mathbf{w})\) as the function whose t-product layer weights are low-tubal-rank approximations of \(f(\underline{\mathbf{x}};\underline{\mathbf{W}})\). Let \(\tilde{f}(\underline{\mathbf{x}},y)=\inf_{R_{\mathfrak{r}}(\underline{ \mathbf{x}}-\mathbf{x}^{\prime})\leq\xi}yf(\underline{\mathbf{x}}^{\prime})\) and \(\tilde{g}(\underline{\mathbf{x}},y)=\inf_{R_{\mathfrak{r}}(\underline{ \mathbf{x}}-\mathbf{x}^{\prime})\leq\xi}yg(\underline{\mathbf{x}}^{\prime})\) denote the adversarial versions of \(\tilde{f}\) and \(g\), respectively.

This helps us bounding \(\hat{\mathfrak{r}}\) as follows

\[|\tilde{f}(\underline{\mathbf{x}}_{i},y_{i})-\tilde{g}(\underline{\mathbf{x}} _{i},y_{i})|=|\inf_{R_{\mathfrak{r}}(\underline{\mathbf{x}}_{i}-\underline{ \mathbf{x}}_{i}^{\prime})\leq\xi}y_{i}f(\underline{\mathbf{x}}_{i}^{\prime})- \inf_{R_{\mathfrak{r}}(\underline{\mathbf{x}}_{i}-\underline{\mathbf{x}}_{i}^{ \prime})\leq\xi}y_{i}g(\underline{\mathbf{x}}_{i}^{\prime})|.\]

[MISSING_PAGE_EMPTY:45]

Then, we obtain the covering entropy of \(\widehat{\mathfrak{F}}_{\delta,\mathbf{r}}^{\text{adv}}\) by

\[\begin{split}\log\mathsf{N}(\widehat{\mathfrak{F}}_{\delta, \mathbf{r}}^{\text{adv}},\left\|\cdot\right\|_{S},\epsilon)&\leq \log\prod_{l=1}^{L}\left(\frac{9B_{l}}{\delta_{l}^{\text{F}}}\right)^{r_{l}(d_ {l}+d_{l-1}+1)\mathsf{c}}\\ &\overset{(i)}{\leq}\log\prod_{l=1}^{L}\left(\frac{9B_{l}}{ \sqrt{\mathsf{c}}\delta_{l}^{\text{sp}}}\right)^{r_{l}(d_{l}+d_{l-1}+1)\mathsf{ c}}\\ &\leq\sum_{l=1}^{L}r_{l}(d_{l}+d_{l-1}+1)\mathsf{c}\log(9LB_{ \tilde{f}}/(\sqrt{\mathsf{c}}\epsilon))\\ &\leq\sum_{l}(A_{l}\epsilon^{-\frac{1}{a}})(d_{l}+d_{l-1}+1) \mathsf{c}\left(\log(\epsilon^{-1})+\log(9LB_{\tilde{f}}/(\sqrt{\mathsf{c}})) \right),\end{split}\] (63)

where \((ii)\) holds by Eq. (61).

Since \(\widehat{\mathfrak{F}}_{\mathbf{r}}^{\text{adv}}\subset\widehat{\mathfrak{F}}_ {\delta,\mathbf{r}}^{\text{adv}}\), we have

\[\log\mathsf{N}(\widehat{\mathfrak{F}}_{\mathbf{r}}^{\text{adv}},\left\|\cdot \right\|_{S},\epsilon)\leq\log\mathsf{N}(\widehat{\mathfrak{F}}_{\delta, \mathbf{r}}^{\text{adv}},\left\|\cdot\right\|_{S},\epsilon)\leq\sum_{l=1}^{L} r_{l}(d_{l}+d_{l-1}+1)\mathsf{c}\log(9LB_{\tilde{f}}/(\sqrt{\mathsf{c}} \epsilon))\]

To use Lemma 27, we bound \(\log\mathsf{N}(\widehat{\mathfrak{F}}_{\delta,\mathbf{r}}^{\text{adv}},\left\| \cdot\right\|_{S},\epsilon/2)+\mathsf{N}(\widehat{\mathfrak{F}}_{\mathbf{r}} ^{\text{adv}},\left\|\cdot\right\|_{S},\epsilon/2)\) when \(\epsilon\geq 2/N\) as follows:

\[\log\mathsf{N}(\widehat{\mathfrak{F}}_{\delta,\mathbf{r}}^{\text{adv}},\left\| \cdot\right\|_{S},\epsilon/(2))+\mathsf{N}(\widehat{\mathfrak{F}}_{\mathbf{r} }^{\text{adv}},\left\|\cdot\right\|_{S},\epsilon/2)\leq a_{1}+a_{2}\log( \epsilon^{-1})+a_{3}\epsilon^{-\frac{1}{a}},\]

where

\[a_{1} =\log(9LB_{\tilde{f}}/\sqrt{\mathsf{c}})a_{2},\] \[a_{2} =\mathsf{c}\sum_{l=1}^{L}r_{l}(d_{l}+d_{l-1}+1),\] \[a_{3} =\left(\log N+\log(9LB_{\tilde{f}}/\sqrt{\mathsf{c}})\right) \mathsf{c}\sum_{l=1}^{L}A_{l}(d_{l}+d_{l-1}+1).\]

For simplicity, further let

\[\begin{split} E_{1}&=\frac{a_{1}+a_{2}\log N}{N}= \frac{\mathsf{c}\sum_{l=1}^{L}r_{l}(d_{l}+d_{l-1}+1)}{N}\log(9NLB_{\tilde{f}}/ \sqrt{\mathsf{c}}),\\ E_{2}&=\frac{a_{3}}{N}=\frac{\mathsf{c}\sum_{l=1}^ {L}A_{l}(d_{l}+d_{l-1}+1)}{N}\log(9NLB_{\tilde{f}}/\sqrt{\mathsf{c}}).\end{split}\] (64)

Then, according to Lemma 27, we have

\[\mathsf{c}_{*}^{2}(t)\leq C\left\{B_{\tilde{f}}E_{1}+\frac{1+tB_{\tilde{f}}}{ N},\quad E_{2}^{\frac{2a_{3}}{2a+1}}\left(B_{\tilde{f}}^{\frac{2a-1}{2a+1}}+1 \right)\right\},\] (65)

which further leads to

\[\begin{split}&\Phi(\dot{\mathsf{r}})+L_{\ell}\dot{\mathsf{r}} \sqrt{\frac{t}{N}}+\frac{tL_{\ell}B_{\tilde{f}}}{N}\\ &\leq 2L_{\ell}\phi(\dot{\mathsf{r}})+2L_{\ell}(\dot{\mathsf{r}}+ \mathsf{r}_{*})\sqrt{\frac{t}{N}}+\frac{tL_{\ell}B_{\tilde{f}}}{N}\\ &\leq C_{q}L_{\ell}\max\left\{\frac{1}{N}+B_{\tilde{f}}E_{1}+( \dot{\mathsf{r}}+\mathsf{r}_{*})\sqrt{E_{1}},\;E_{2}^{\frac{2a}{2a+1}}B_{ \tilde{f}}^{\frac{2a-1}{2a+1}}+(\dot{\mathsf{r}}+\mathsf{r}_{*})^{\frac{2a_{3} }{2a+1}}\sqrt{E_{2}}\right\}\\ &\quad+2L_{\ell}(\dot{\mathsf{r}}+\mathsf{r}_{*})\sqrt{\frac{t}{N} }+\frac{tL_{\ell}B_{\tilde{f}}}{N}.\end{split}\] (66)

Note that

\[(\dot{\mathsf{r}}+\mathsf{r}_{*})\sqrt{E_{1}}=\dot{\mathsf{r}}\sqrt{E_{1}}+ \mathsf{r}_{*}\sqrt{E_{1}}\leq\dot{\mathsf{r}}\sqrt{E_{1}}+\frac{1}{2}\mathsf{ r}_{*}^{2}+\frac{1}{2}E_{1},\]\[(\hat{\mathbf{r}}+\mathbf{r}_{*})^{\frac{2\alpha}{2\alpha+1}}\sqrt{E_{2}}\leq\hat{ \mathbf{r}}^{\frac{2\alpha}{2\alpha+1}}\sqrt{E_{2}}+\mathbf{r}_{*}^{\frac{2 \alpha}{2\alpha+1}}\sqrt{E_{2}},\]

\[(\hat{\mathbf{r}}+\mathbf{r}_{*})\sqrt{\frac{t}{N}}\leq\hat{\mathbf{r}}\sqrt{ \frac{t}{N}}+\frac{1}{2}(\mathbf{r}_{*}^{2}+\frac{t}{N}).\]

Then by simple calculation, we have

\[\Phi(\hat{\mathbf{r}})+L_{\ell}\hat{\mathbf{r}}\sqrt{\frac{t}{N}} +\frac{tL_{\ell}B_{\hat{f}}}{N}\] \[\overset{(i)}{\leq}C_{\alpha}L_{\ell}\bigg{\{}B_{\hat{f}}E_{1}+ \hat{\mathbf{r}}\sqrt{E_{1}}+E_{2}^{\frac{2\alpha}{2\alpha+1}}\left(B_{\hat{f }}^{\frac{2\alpha-1}{2\alpha+1}}+1\right)+\hat{\mathbf{r}}^{\frac{2\alpha}{2 \alpha+1}}\sqrt{E_{2}}+\hat{\mathbf{r}}\sqrt{\frac{t}{N}}+\frac{1+tB_{\hat{f }}}{N}\bigg{\}}.\]

Proof of Corollary 15.: The bound in Corollary 15 can be directly obtained if we choose the parameter \(\mathbf{r}\) of tubal ranks in \(\mathbf{\tilde{x}_{r}}\) by \(r_{l}=\min\{\lceil\left(LV_{0}B_{\hat{f}}B_{l}^{-1}\right)^{1/\alpha}\rceil,d _{l},d_{l-1}\}\). 

## Appendix F Useful Notions and Lemmas

In this section, we provide several notions and lemmas which are used in the previous analysis.

### Tools for Analyzing General DNNs

We briefly list the tools used in this paper for analyzing the generalization error of general DNNs, including Rademacher complexity, covering number, and concentration inequalities, _etc._.

**Definition 13** (Rademacher complexity).: _Given an i.i.d. sample \(S:=\{(\underline{\mathbf{x}}_{i},y_{i})\}_{i=1}^{N}\) of size \(N\) and a function class \(\mathcal{H}\), the empirical Rademacher complexity of \(\mathcal{H}\) is defined as_

\[\hat{R}_{S}(\mathcal{H}):=\mathbb{E}_{\varepsilon_{1},\cdots,\varepsilon_{N}} \left[\sup_{h\in\mathcal{H}}\frac{1}{N}\varepsilon_{i}h(\underline{\mathbf{x }}_{i},y_{i})\right],\]

_where \(\varepsilon_{1},\cdots,\varepsilon_{N}\) are i.i.d. Rademacher variables, i.e., \(\varepsilon_{i}\) equals to \(1\) or \(-1\) with equal probability. The average Rademacher complexity is further defined as_

\[\bar{R}_{N}=\mathbb{E}_{\hat{S}}\hat{R}_{S}(\mathcal{H}).\]

**Lemma 29** ([4]).: _Given an i.i.d. sample \(S:=\{(\underline{\mathbf{x}}_{i},y_{i})\}_{i=1}^{N}\) of size \(N\), a loss function \(\ell(h(\cdot),y)\) taking values in \([0,B]\), the generalization error of any function \(f\) in hypothesis set \(\mathcal{F}\) satisfies_

\[\mathcal{L}(f)\leq\hat{\mathcal{L}}(f)+2\hat{R}_{S}(\ell\circ\mathcal{F})+3B \sqrt{\frac{t}{2N}},\] (67)

_with probability at least \(1-e^{-t}\) for all \(t\geq 0\)._

**Lemma 30** (Talagrand's contraction lemma [44]).: _Given function set \(\mathcal{F}\) and \(L_{\ell}\)-Lipschitz function \(\ell\), for a function sets defined as \(l_{\mathcal{F}}:=\{\ell\circ f\bigm{|}f\in\mathcal{F}\}\), we have_

\[\hat{R}_{S}(\mathcal{F})\leq L_{\ell}\hat{R}_{S}(\mathcal{F}).\]

**Definition 14** (\(\epsilon\)-covering net).: _Let \(\epsilon>0\) and \((\mathcal{X},d(\cdot,\cdot))\) be a metric space, where \(d(\cdot,\cdot)\) is a (pseudo)-metric. We say \(\mathcal{Z}\subset\mathcal{X}\) is an \(\epsilon\)-covering net of \(\mathcal{X}\), if for any \(x\in\mathcal{X}\), there exists \(z\in\mathcal{Z}\) such that \(d(x,z)\leq\epsilon\). Define the smallest \(|\mathcal{Z}|\) as the \(\epsilon\)-covering number of \(\mathcal{X}\) and denote as \(\mathsf{N}(\mathcal{X},d(\cdot,\cdot),\epsilon)\)._

Given a traning dataset \(S=\{\underline{\mathbf{x}}_{i},y_{i}\}_{i=1}^{N}\) and a function set \(\mathcal{F}\). Consider the output space of the space of \(\mathcal{F}\) restricted on \(S\), i.e., \(\mathcal{F}|_{S}=\{\left(f(\underline{\mathbf{x}}_{1},y_{1}),\cdots,f( \underline{\mathbf{x}}_{N},y_{N})\right)^{\top}\bigm{|}f\in\mathcal{F}\}\). Then, define a pseudo-norm of \(\mathcal{F}|_{S}\) as:\(\|f\|_{S}:=N^{-1}\sqrt{\sum_{i=1}^{N}f(\underline{\mathbf{x}}_{i},y_{i})^{2}}\). Then, the Rademacher complexity of \(\mathcal{F}\) could be upper bounded by the \(\epsilon\)-covering number of \(\mathcal{F}\) under the empirical \(l_{2}\)-pseudo-metric by the Dudley's inequality as follows:

**Lemma 31** (Dudley's integral inequality [47]).: _The Rademacher complexity \(\hat{R}_{S}(\mathcal{F})\) satisfies_

\[\hat{R}_{S}(\mathcal{F})\leq\inf_{\delta>0}\left[8\delta+\frac{12}{\sqrt{N}} \int_{\delta}^{\max_{f\in\mathcal{F}}\left\|f\right\|_{S}}\sqrt{\log\mathsf{N} (\mathcal{F},\left\|\cdot\right\|_{S},\epsilon)}d\epsilon\right].\] (68)

**Lemma 32** (Covering number of norm balls [47]).: _Let \(\mathbf{B}\) be a \(l_{p}\)-norm ball with radius \(W\). Let \(d(\mathbf{x}_{1},\mathbf{x}_{2})=\left\|\mathbf{x}_{1}-\mathbf{x}_{2}\right\|_ {p}\). Define the \(\epsilon\)-covering number of \(\mathbf{B}\) as \(\mathsf{N}(\mathbf{B},d(\cdot,\cdot),\epsilon)\), we have_

\[\mathsf{N}(\mathbf{B},d(\cdot,\cdot),\epsilon)\leq\left(1+\frac{2W}{\epsilon} \right)^{d}.\] (69)

**Lemma 33** (Covering number of low-tubal-rank tensors).: _For the set of tensors \(\mathbf{T}_{r}:=\left\{\mathbf{T}\in\mathbb{R}^{m\times n\times\mathsf{c}} \left|\,r_{\text{t}}(\mathbf{T})\leq r,\ \left\|\mathbf{T}\right\|_{\text{F}}\leq 1\right\}\) with \(r\leq\min\{m,n\}\), its \(\epsilon\)-covering number can be upper bounded by_

\[\mathsf{N}(\mathbf{T}_{r},\left\|\cdot\right\|_{\text{F}},\epsilon)\leq\left( \frac{9}{\epsilon}\right)^{(m+n+1)r\mathsf{c}}.\] (70)

Proof.: Consider the reduced t-SVD [20] of a tensor \(\underline{\mathbf{X}}=\underline{\mathbf{U}}*_{M}\underline{\mathbf{S}}*_{M} \underline{\mathbf{V}}^{\top}\in\mathbf{T}_{r}\), where \(*_{M}\) denotes the t-product induced by the linear transform \(M(\cdot)\) defined in Eq. (1), \(\underline{\mathbf{U}}\in\mathbb{R}^{m\times r\times\mathsf{c}}\) and \(\underline{\mathbf{V}}\in\mathbb{R}^{n\times r\times\mathsf{c}}\) are (semi)-t-orthogonal tensors, and \(\underline{\mathbf{S}}\in\mathbb{R}^{r\times r\times\mathsf{c}}\) is an f-diagonal tensor. As \(\underline{\mathbf{T}}\in\overline{\mathbf{T}}_{r}\), we have \(\left\|\underline{\mathbf{S}}\right\|_{\text{F}}=\left\|\underline{\mathbf{X} }\right\|_{\text{F}}\leq 1\). The idea is to cover \(\overline{\mathbf{T}}_{r}\) by covering the set of factor tensors \(\underline{\mathbf{U}},\underline{\mathbf{V}}\) and \(\underline{\mathbf{S}}\).

Let \(\mathbf{D}\subset\mathbb{R}^{r\times r\times\mathsf{c}}\) be the set of f-diagonal tensors with F-norm equal to one. We take \(\mathbf{D}^{c}\) to be an \(\epsilon/3\)-covering net for \(\mathbf{D}\). Then by Lemma 32, we have \(\left|\mathbf{D}^{c}\right|\leq(9/\epsilon)^{r\mathsf{c}}\). Next, let \(\mathbf{O}_{m,r}=\left\{\underline{\mathbf{U}}\in\mathbb{R}^{m\times r\times \mathsf{c}}\mid\underline{\mathbf{U}}*_{M}\underline{\mathbf{U}}^{\top}= \underline{\mathbf{I}}\right\}\). To cover \(\mathbf{O}_{m,r}\), we consider the \(\left\|\cdot\right\|_{\infty,2,2}\)-norm defined as

\[\left\|\underline{\mathbf{X}}\right\|_{\infty,2,2}:=\max_{i}\left\|\underline {\mathbf{X}}_{,i,i},\right\|_{\text{F}}.\]

Let \(\mathbf{Q}_{m,r}:=\left\{\underline{\mathbf{X}}\in\mathbb{R}^{m\times r \times\mathsf{c}}\mid\left\|\underline{\mathbf{X}}\right\|_{\infty,2,2}\leq 1\right\}\). Then, we have \(\mathbf{O}_{m,r}\subset\mathbf{Q}_{m,r}\) by the definition of t-orthogonal tensors. Letting \(\mathbf{Q}^{c}_{m,r}\) be an \(\epsilon/3\)-covering net of \(\mathbf{Q}_{m,r}\), then we obtain \(\left|\mathbf{Q}^{c}_{m,r}\right|\leq(9/\epsilon)^{mr\mathsf{c}}\) by Theorem 32. Similarly, an \(\epsilon/3\)-covering net of \(\mathbf{Q}_{n,r}\) satisfies \(\left|\mathbf{Q}^{c}_{n,r}\right|\leq(9/\epsilon)^{mr}\).

Now construct a set \(\mathbf{T}^{c}_{r}=\left\{\underline{\mathbf{U}}^{c}*_{M}\underline{\mathbf{S} }^{c}*_{M}(\underline{\mathbf{V}}^{c})^{\top}\mid\underline{\mathbf{U}}^{c} \in\mathbf{Q}^{c}_{m,r},\ \underline{\mathbf{S}}^{c}\in\mathbf{D}^{c},\underline{\mathbf{V}}^{c}\in \mathbf{Q}^{c}_{n,r}\right\}\). Then, we have

\[\left|\mathbf{T}^{c}_{r}\right|=\left|\mathbf{Q}^{c}_{m,r}\right|\cdot\left| \underline{\mathbf{S}}^{c}\right|\cdot\left|\mathbf{Q}^{c}_{n,r}\right|\leq(9/ \epsilon)^{(m+n+1)r\mathsf{c}}.\]

Net, we will show that \(\mathbf{T}^{c}_{r}\) is an \(\epsilon\)-covering net of \(\mathbf{T}_{r}\), i.e., for any \(\underline{\mathbf{X}}\in\mathbf{T}_{r}\), there is an \(\underline{\mathbf{X}}^{c}\in\mathbf{T}^{c}_{r}\) satisfying \(\left\|\underline{\mathbf{X}}-\underline{\mathbf{X}}^{c}\right\|_{\text{F}}\leq\epsilon\).

Given \(\underline{\mathbf{X}}\in\mathbf{T}_{r}\), consider its reduced t-SVD as \(\underline{\mathbf{X}}=\underline{\mathbf{U}}*_{M}\underline{\mathbf{S}}*_{M} \underline{\mathbf{V}}^{\top}\in\mathbf{T}_{r}\). Then, there exists \(\underline{\mathbf{X}}^{c}=\underline{\mathbf{U}}^{c}*_{M}\underline{\mathbf{S} }^{c}*_{M}(\underline{\mathbf{V}}^{c})^{\top}\) with \(\underline{\mathbf{U}}^{c}\in\mathbf{Q}^{c}_{m,r},\ \underline{\mathbf{S}}^{c}\in\mathbf{D}^{c},\underline{\mathbf{V}}^{c}\in \mathbf{Q}^{c}_{n,r}\) satisfying \(\left\|\underline{\mathbf{U}}-\underline{\mathbf{U}}^{c}\right\|_{\infty,2,2}\leq \epsilon/3\), \(\left\|\underline{\mathbf{S}}-\underline{\mathbf{S}}^{c}\right\|_{F}\leq\epsilon/3\), and \(\left\|\underline{\mathbf{Y}}-\underline{\mathbf{V}}^{c}\right\|_{\infty,2,2}\leq \epsilon/3\). This gives

\[\left\|\underline{\mathbf{X}}-\underline{\mathbf{X}}^{c}\right\|_{ \text{F}}\] \[=\left\|\underline{\mathbf{U}}*_{M}\underline{\mathbf{S}}*_{M} \underline{\mathbf{V}}^{\top}-\underline{\mathbf{U}}^{c}*_{M}\underline{\mathbf{S} }*_{M}\underline{\mathbf{V}}^{\top}+\underline{\mathbf{U}}^{c}*_{M}\underline{ \mathbf{S}}*_{M}\underline{\mathbf{V}}^{\top}-\underline{\mathbf{U}}^{c}*_{M} \underline{\mathbf{S}}^{c}*_{M}\underline{\mathbf{V}}^{\top}\right.\] \[\left.\hskip 142.26378pt+\underline{\mathbf{U}}^{c}*_{M}\underline{ \mathbf{S}}^{c}*_{M}\underline{\mathbf{V}}^{\top}-\underline{\mathbf{U}}^{c}*_{M} \underline{\mathbf{S}}*_{M}\underline{\mathbf{V}}^{\top}-\underline{\mathbf{U}}^{c}*_{M} \underline{\mathbf{S}}^{c}*_{M}\underline{\mathbf{V}}^{\top}\right.\] \[\left.\hskip 142.26378pt+\underline{\mathbf{U}}^{c}*_{M}\underline{ \mathbf{S}}^{c}*_{M}\underline{\mathbf{V}}^{\top}-\underline{\mathbf{U}}^{c}*_{M} \underline{\mathbf{S}}^{c}*_{M}\underline{\mathbf{V}}^{\top}-\underline{\mathbf{U}}^{c}*_{M} \underline{\mathbf{S}}^{c}*_{M}\underline{\mathbf{V}}^{\top}\right.\] \[\left.\hskip 142.26378pt+\underline{\mathbf{U}}^{c}*_{M}\underline{ \mathbf{S}}^{c}*_{M}\underline{\mathbf{V}}^{\top}-\underline{\mathbf{U}}^{c}*_{M} \underline{\mathbf{S}}*_{M}\underline{\mathbf{V}}^{\top}\right.\right\] \[\left.\hskip 142.26378pt+\underline{\mathbf{U}}^{c}*_{M}\underline{ \mathbf{S}}^{c}*_{M}\underline{\mathbf{V}}^{\top}-\underline{\mathbf{U}}^{c}*_{M} \underline{\mathbf{S}}^{c}*_{M}\underline{\mathbf{V}}^{\top}\right.\right\] \[\left.\hskip 142.26378For the first term, note that since \(\underline{\mathbf{V}}\) is a t-orthogonal tensor,

\[\left\|(\underline{\mathbf{U}}-\underline{\mathbf{U}}^{c})\ast_{M}\underline{ \mathbf{S}}\ast_{M}\underline{\mathbf{V}}^{\top}\right\|_{\mathrm{F}}=\left\|( \underline{\mathbf{U}}-\underline{\mathbf{U}}^{c})\ast_{M}\underline{\mathbf{ S}}\right\|_{\mathrm{F}}.\]

and

\[\left\|(\underline{\mathbf{U}}-\underline{\mathbf{U}}^{c})\ast_{M} \underline{\mathbf{S}}\right\|_{\mathrm{F}}^{2} =\sum_{i=1}^{r}\left\|(\underline{\mathbf{U}}-\underline{\mathbf{ U}}^{c})_{:,i,:}\ast_{M}\underline{\mathbf{S}}_{i,i,:}\right\|_{\mathrm{F}}^{2}\] \[=\sum_{i=1}^{r}\left\|(\underline{\mathbf{U}}-\underline{ \mathbf{U}}^{c})_{:,i,:}\right\|_{\mathrm{F}}^{2}\left\|\underline{\mathbf{S}} _{i,i,:}\right\|_{\mathrm{F}}^{2}\] \[\leq(\sum_{i=1}^{r}\left\|\underline{\mathbf{S}}_{i,i,:}\right\|_ {\mathrm{F}}^{2})\max_{i}\left\|(\underline{\mathbf{U}}-\underline{\mathbf{U}}^ {c})_{:,i,:}\right\|_{\mathrm{F}}^{2}\] \[\leq\left\|\underline{\mathbf{S}}\right\|_{\mathrm{F}}^{2}\left\| \underline{\mathbf{U}}-\underline{\mathbf{U}}^{c}\right\|_{\infty,2,2}\] \[\leq(\epsilon/3)^{2}.\]

Hence, \(\left\|(\underline{\mathbf{U}}-\underline{\mathbf{U}}^{c})\ast_{M}\underline{ \mathbf{S}}\ast_{M}\underline{\mathbf{V}}^{\top}\right\|_{\mathrm{F}}\leq \epsilon/3\). Similarly, we have \(\left\|\underline{\mathbf{U}}^{c}\ast_{M}\underline{\mathbf{S}}^{c}\ast_{M}( \underline{\mathbf{V}}-\underline{\mathbf{V}}^{c})^{\top}\right\|_{\mathrm{F}} \leq\epsilon/3\). The middle term can be bounded \(\left\|\underline{\mathbf{U}}^{c}\ast_{M}(\underline{\mathbf{S}}-\underline{ \mathbf{S}}^{c})\ast_{M}\underline{\mathbf{V}}^{\top}\right\|_{\mathrm{F}}\leq \left\|\underline{\mathbf{S}}-\underline{\mathbf{S}}^{c}\right\|_{\mathrm{F}}\leq \epsilon/3\) due to the property of t-orthogonal tensors \(\underline{\mathbf{U}}^{c}\) and \(\underline{\mathbf{V}}^{c}\)[20]. Therefore, for any \(\underline{\mathbf{X}}\in\boldsymbol{\mathsf{T}}_{r}\), there is an \(\underline{\mathbf{X}}^{c}\in\boldsymbol{\mathsf{T}}_{r}^{c}\) satisfying \(\left\|\underline{\mathbf{X}}-\underline{\mathbf{X}}^{c}\right\|_{\mathrm{F}}\leq\epsilon\). 

**Lemma 34** (Covering number bounds for composition and addition [10]).: _Let \(\mathcal{F}_{1}\) and \(\mathcal{F}_{2}\) be classes of functions on normed space \((\mathcal{X},\left\|\cdot\right\|_{\mathcal{X}})\rightarrow(\mathcal{Y},\left\| \cdot\right\|_{\mathcal{Y}})\) and let \(\mathcal{F}\) be a class of \(c\)-Lipschitz functions \((\mathcal{Y},\left\|\cdot\right\|_{\mathcal{Y}})\rightarrow(\mathcal{Z},\left\| \cdot\right\|_{\mathcal{Z}})\). Then for any \(X\in\mathcal{X}^{N}\) and \(\epsilon_{\mathcal{F}_{1}},\epsilon_{\mathcal{F}_{2}},\epsilon_{\mathcal{F}}>0\), it holds that_

\[\mathsf{N}(\{f_{1}+f_{2}\mid f_{1}\in\mathcal{F}_{1},f_{2}\in\mathcal{F}_{2} \},\epsilon_{\mathcal{F}_{1}}+\epsilon_{\mathcal{F}_{2}},\left\|\cdot\right\|_ {X})\leq\mathsf{N}(\mathcal{F}_{1},\epsilon_{\mathcal{F}_{1}},\left\|\cdot \right\|_{X})\mathsf{N}(\mathcal{F}_{2},\epsilon_{\mathcal{F}_{2}},\left\| \cdot\right\|_{X}).\]

_and_

\[\mathsf{N}(\{f\circ f_{1}\mid f\in\mathcal{F},f_{1}\in\mathcal{F}_{1}\}, \epsilon_{\mathcal{F}}+c\epsilon_{\mathcal{F}_{1}},\left\|\cdot\right\|_{X}) \leq\mathsf{N}(\mathcal{F}_{1},\epsilon_{\mathcal{F}_{1}},\left\|\cdot\right\|_ {X})\sup_{f_{1}\in\mathcal{F}_{1}}\mathsf{N}(\mathcal{F},\epsilon_{\mathcal{F }},\left\|\cdot\right\|_{f_{1}(X)}).\]

_Specifically, if \(\mathcal{F}=\{f\}\) is a singleton, we have_

\[\mathsf{N}(\{f\circ f_{1}\mid f\in\mathcal{F},f_{1}\in\mathcal{F}_{1}\}, \mathrm{c}\epsilon_{\mathcal{F}_{1}},\left\|\cdot\right\|_{X})\leq\mathsf{N}( \mathcal{F}_{1},\epsilon_{\mathcal{F}_{1}},\left\|\cdot\right\|_{X}).\]

**Lemma 35** (Simplified Talagrand's concentration inequality [42]).: _Let \(\mathcal{F}\) be a function class on \(\mathcal{X}\) that is separable with respect to \(L_{\infty}\)-norm, and \(\{\mathbf{x}_{i}\}_{i=1}^{N}\) be i.i.d. random variables in \(\mathcal{X}\). Furthermore, suppose there exist constants \(V\geq\) and \(U\geq 0\) such that \(V=\sup_{f\in\mathcal{F}}\mathbb{E}[(f-\mathbb{E}[f])^{2}]\) and \(U=\sup_{f\in\mathcal{F}}\left\|f\right\|_{L_{\infty}}\). Letting \(Z:=\sup_{f\in\mathcal{F}}|N^{-1}\sum_{i=1}^{N}f(\mathbf{x}_{i})-\mathbb{E}[f]|\), then it holds for all \(t>0\) that_

\[\mathbb{P}\left[Z\geq 2\mathbb{E}[Z]+\sqrt{\frac{2Vt}{N}}+\frac{2Ut}{N}\right] \leq e^{-t}.\]

### Some Results for Analyzing t-NNs

In this subsection, we present several fundamental statements described as lemmas for analyzing t-NNs. First, we give Lemmas 36-38, which are used in the analysis of the t-product layers.

**Lemma 36**.: _Let \(\sigma(\cdot):\mathbb{R}\rightarrow\mathbb{R}\) be a \(L_{\infty}\)-Lipschitz function, i.e., \(|\sigma(x)-\sigma(y)|\leq L_{\sigma}|x-y|,\forall x,y\in\mathbb{R}\). If it is applied element-wisely to any two real vectors \(\mathbf{x}\) and \(\mathbf{y}\), then it holds that_

\[\left\|\sigma(\mathbf{x})-\sigma(\mathbf{y})\right\|_{l_{p}}\leq L_{\sigma} \left\|\mathbf{x}-\mathbf{y}\right\|_{l_{p}}.\]

**Lemma 37**.: _The following inequalities hold:_

\[\left\|\underline{\mathbf{T}}\right\|_{\mathrm{F}}\leq\left\|\underline{\mathbf{T} }\right\|_{\mathrm{F}},\quad\text{and}\quad\left\|\underline{\mathbf{W}}\ast_{M} \underline{\mathbf{x}}\right\|_{\mathrm{F}}\leq\left\|\underline{\mathbf{W}} \right\|_{\mathrm{sp}}\left\|\underline{\mathbf{x}}\right\|_{\mathrm{F}}\leq\left\| \underline{\mathbf{W}}\right\|_{\mathrm{F}}\left\|\underline{\mathbf{x}}\right\|_{ \mathrm{F}}.\]

Proof.: According to the definition of the \(M\) transform in Eq. (1) and the orthogonality of \(\mathbf{M}\), we have

\[\left\|\underline{\mathbf{T}}\right\|_{\mathrm{sp}}=\left\|\widetilde{\mathbf{T} }_{M}\right\|\leq\left\|\widetilde{\mathbf{T}}_{M}\right\right\|_{\mathrm{F}}= \left\|M(\underline{\mathbf{T}})\right\|_{\mathrm{F}}=\left\|\underline{ \mathbf{T}}\right\|_{\mathrm{F}},\]and

\[\left\|\underline{\mathbf{W}}\ast_{M}\underline{\mathbf{x}}\right\|_{\mathrm{F}}= \left\|\widetilde{\mathbf{W}}_{M}\cdot\widetilde{\mathbf{x}}_{M}\right\|_{ \mathrm{F}}\overset{(i)}{\leq}\left\|\widetilde{\mathbf{W}}_{M}\right\|\left\| \widetilde{\mathbf{x}}_{M}\right\|_{\mathrm{F}}=\left\|\mathbf{W}\right\|_{ \mathrm{sp}}\left\|\mathbf{x}\right\|_{\mathrm{F}}\leq\left\|\mathbf{W}\right\| _{\mathrm{F}}\left\|\mathbf{x}\right\|_{\mathrm{F}},\]

where inequality \((i)\) holds because \(\left\|\mathbf{A}\mathbf{B}\right\|_{\mathrm{F}}\leq\left\|\mathbf{A}\right\| \left\|\mathbf{B}\right\|_{\mathrm{F}}\) for any matrices \(\mathbf{A},\mathbf{B}\) with appropriate dimensions. 

**Lemma 38** (The t-product layer is Lipschitz continuous).: _Suppose the activation function is \(L_{\sigma}\)-Lipschitz, then a layer of t-product layer \(h(\underline{\mathbf{x}})=\sigma(\underline{\mathbf{W}}\ast_{M}\underline{ \mathbf{x}})\) is at most \(L_{\sigma}\left\|\underline{\mathbf{W}}\right\|_{\mathrm{F}}\)-Lipschitz._

Proof.: According to the Lipschitzness of the activation function, we have

\[\left\|h(\underline{\mathbf{x}}_{1})-h(\underline{\mathbf{x}}_{2 })\right\|_{\mathrm{F}} =\left\|\sigma(\underline{\mathbf{W}}\ast_{M}\underline{ \mathbf{x}}_{1})-\sigma(\underline{\mathbf{W}}\ast_{M}\underline{\mathbf{x}}_ {2})\right\|_{\mathrm{F}}\] \[\leq L_{\sigma}\left\|\underline{\mathbf{W}}\ast_{M}\underline{ \mathbf{x}}_{1}-\underline{\mathbf{W}}\ast_{M}\underline{\mathbf{x}}_{2}\right\| _{\mathrm{F}}\] \[=L_{\sigma}\left\|\underline{\mathbf{W}}\ast_{M}(\underline{ \mathbf{x}}_{1}-\underline{\mathbf{x}}_{2})\right\|_{\mathrm{F}}\] \[=L_{\sigma}\left\|\underline{\mathbf{W}}\right\|_{\mathrm{F}} \left\|\underline{\mathbf{x}}_{1}-\underline{\mathbf{x}}_{2}\right\|_{\mathrm{F }}.\]

We then present Lemma 39 and Lemma 40 which are used in upper bounding the input and output of t-NNs in adversarial settings.

**Lemma 39**.: _Given a fixed example \(\underline{\mathbf{x}}\in\mathbb{R}^{d\times 1\times\mathbf{c}}\), if an adversary \(\underline{\mathbf{x}}^{\prime}\) satisfies \(R_{\mathrm{a}}(\underline{\mathbf{x}}-\underline{\mathbf{x}}^{\prime})\leq\xi\), then it holds that_

\[\left\|\underline{\mathbf{x}}^{\prime}\right\|_{\mathrm{F}}\leq B_{x}+\xi \mathbb{C}_{R_{\mathrm{a}}}.\]

**Lemma 40**.: _The \(L_{\infty}\)-norm of any \(f(\underline{\mathbf{x}};\underline{\mathbf{W}})\in\mathfrak{F}^{\mathrm{ adv}}\) defined on the set of input examples \(\mathcal{X}\) is upper bounded by_

\[\sup_{f\in\mathfrak{F}^{\mathrm{adv}}}\left\|\tilde{f}\right\|_{L_{\infty}} \leq B_{\tilde{f}}:=B_{\underline{\mathbf{W}}}B_{x,R_{\mathrm{a}},\xi}.\] (71)

_The diameter of \(\mathfrak{F}^{\mathrm{adv}}\) is can be upper bounded as follows_

\[D_{\tilde{f}}:=2\sup_{\tilde{f}\in\mathfrak{F}^{\mathrm{adv}}}\left\|\tilde{ f}\right\|_{S}\leq 2B_{\underline{\mathbf{W}}}B_{x,R_{\mathrm{a}},\xi}.\] (72)

Proof.: For any \(f\in\mathfrak{F}\), given an example \((\underline{\mathbf{x}},y)\in\mathcal{X}\times\{\pm 1\}\), let \(\underline{\mathbf{x}}^{*}\in\operatorname{arginf}_{R_{\mathrm{a}}(\underline{ \mathbf{x}}-\underline{\mathbf{x}}^{\prime})\leq\xi}yf(\underline{\mathbf{x}}^ {\prime})\) be one adversarial example. Then, we have

\[\left|\tilde{f}(\underline{\mathbf{x}},y)\right| =\left|\inf_{R_{\mathrm{a}}(\underline{\mathbf{x}}^{*})\leq\xi} y_{i}f(\underline{\mathbf{x}}^{\prime})\right|\] \[=\left|f(\underline{\mathbf{x}}^{*})\right|\] \[=\left|\mathbf{w}^{\top}\mathtt{vec}\left(\mathbf{h}^{(L)}( \underline{\mathbf{x}}^{*})\right)\right|\] \[\leq\left\|\mathbf{w}\right\|\left\|\underline{\mathbf{w}}( \underline{\mathbf{W}}^{(L)}\ast_{M}\mathbf{h}^{(L-1)}(\underline{\mathbf{x}}^ {*})\right)\right\|_{\mathrm{F}}\] \[\leq\left\|\mathbf{w}\right\|\left\|\underline{\mathbf{w}}( \underline{\mathbf{W}}^{(L)}\ast_{M}\mathbf{h}^{(L-1)}(\underline{\mathbf{x}}^ {*})\right)\right\|_{\mathrm{F}}\] \[\leq\left\|\mathbf{w}\right\|\left\|\underline{\mathbf{w}}( \underline{\mathbf{W}}^{(L)}\right\|_{\mathrm{F}}\left\|\mathbf{h}^{(L-1)}( \underline{\mathbf{x}}^{*})\right\|_{\mathrm{F}}\] \[\leq\cdots\] \[\leq\mathbf{w}\prod_{l=1}^{L}B_{l}\left\|\mathbf{h}^{(0)}( \underline{\mathbf{x}}^{*})\right\|_{\mathrm{F}}\] \[\leq B_{\mathbf{w}}\prod_{l=1}^{L}B_{l}B_{x,R_{\mathrm{a}},\xi}\] \[=:B_{\tilde{f}},\]which also implies

\[D_{\tilde{f}}=2\max_{\tilde{f}\in\mathfrak{G}^{\text{sh}}}\left\|\tilde{f} \right\|_{S}\leq 2B_{\textbf{w}}\prod_{l=1}^{L}B_{l}B_{x,R_{u},\xi}.\]

Lemma 41 helps upper bounding the F-norm of residuals after low-tubal-rank approximation of the weight tensors of t-NNs under Assumption 13.

**Lemma 41**.: _Given constants \(a>0,\alpha>1\), suppose a sequence \(\{z_{j}\}_{j=1}^{\infty}\) satisfying polynomial decay \(z_{j}\leq aj^{-\alpha}\), then for any positive integer \(n\), we have_

\[\sum_{j>n}z_{j}\leq\frac{an^{1-\alpha}}{\alpha-1}.\]

Proof.: We compute the sum of the sequence using integration as follows:

\[\sum_{j>n}z_{j}\leq\int_{n}^{\infty}at^{-\alpha}\text{d}t=\frac{a}{\alpha-1}t ^{1-\alpha}\big{|}_{n}^{\infty}\leq\frac{an^{1-\alpha}}{\alpha-1}.\]