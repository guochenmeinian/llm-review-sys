# Faster Local Solvers for Graph Diffusion Equations

 Jiahe Bai \({}^{1}\)  Baojian Zhou \({}^{1,2}\)1  Deqing Yang \({}^{1,2}\)  Yanghua Xiao \({}^{2}\)

\({}^{1}\) the School of Data Science, Fudan University,

\({}^{2}\) Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University

jhbai20,bjzhou,yangdeqing,shawyh@fudan.edu.cn

Footnote 1: Corresponding author

###### Abstract

Efficient computation of graph diffusion equations (GDEs), such as Personalized PageRank, Katz centrality, and the Heat kernel, is crucial for clustering, training neural networks, and many other graph-related problems. Standard iterative methods require accessing the whole graph per iteration, making them time-consuming for large-scale graphs. While existing _local solvers_ approximate diffusion vectors through heuristic local updates, they often operate sequentially and are typically designed for specific diffusion types, limiting their applicability. Given that diffusion vectors are highly localizable, as measured by the _participation ratio_, this paper introduces a novel framework for approximately solving GDEs using a _local diffusion process_. This framework reveals the suboptimality of existing local solvers. Furthermore, our approach effectively localizes standard iterative solvers by designing simple and provably sublinear time algorithms. These new local solvers are highly parallelizable, making them well-suited for implementation on GPUs. We demonstrate the effectiveness of our framework in quickly obtaining approximate diffusion vectors, achieving up to a hundred-fold speed improvement, and its applicability to large-scale dynamic graphs. Our framework could also facilitate more efficient _local message-passing_ mechanisms for GNNs.

## 1 Introduction

Graph diffusion equations (GDEs), such as Personalized PageRank (PPR) [41, 44, 62], Katz centrality (Katz) [46], Heat kernel (HK) [21], and Inverse PageRank (IPR) [51], are fundamental tools for modeling graph data. These score vectors for nodes in a graph capture various aspects of their importance or influence. They have been successfully applied to many graph learning tasks including local clustering [2, 81], detecting communities [49], semi-supervised learning [78, 80], node embeddings [63, 67], training graph neural networks (GNNs) [10, 16, 19, 31, 75], and many other applications [32]. Specifically, given a propagation matrix \(\bm{M}\) associated with an undirected graph \(\mathcal{G}(\mathcal{V},\mathcal{E})\), a general graph diffusion equation is defined as

\[\bm{f}\triangleq\sum_{k=0}^{\infty}c_{k}\bm{M}^{k}\bm{s},\] (1)

where \(\bm{f}\) is the diffusion vector computed from a source vector \(\bm{s}\), and the sequence of coefficients \(c_{k}\) satisfies \(c_{k}\geq 0\). Equation (1) represents a system of linear, constant-coefficient ordinary differential equation, \(\dot{\bm{x}}(t)=\bm{M}\bm{x}(t)\), with an initial condition \(\bm{x}(0)\) and \(t\geq 0\). Standard solvers [34, 57] for computing \(\bm{f}\) require access to matrix-vector product operations \(\bm{M}\bm{x}\), typically involving \(\mathcal{O}(m)\) operations, where \(m\) is the total number of edges in \(\mathcal{G}\). This could be time-consuming when dealing with large-scale graphs [42].

A key property of \(\bm{f}\) is the high localization of its entry magnitudes, which reside in a small portion of \(\mathcal{G}\). Figure 1 demonstrates this localization property of \(\bm{f}\) on PPR, Katz, and HK. Leveraging this locality property allows for more efficient approximation using _local iterative solvers_, which heuristically [2; 7; 49] or their variants [4; 12] are known to be closely related to Gauss-Seidel [49; 18]. However, current local push-based methods are fundamentally sequential iterative solvers and focused on specific types such as PPR or HK. These disadvantages limit their applicability to modern GPU architectures and their generalizability.

By leveraging the locality of \(\bm{f}\), we propose, for the first time, a general local iterative framework for solving GDEs using a _local diffusion process_. A novel component of our framework is to model _local diffusion_ as a _locally evolving set process_ inspired by its stochastic counterpart [58]. We use this framework to localize commonly used standard solvers, demonstrating faster local methods for approximating \(\bm{f}\). For example, the local gradient descent is simple, provably sublinear, and highly parallelizable. It can be accelerated further by using local momentum. **Our contributions are**

* By demonstrating that popular diffusion vectors, such as PPR, Katz, and HK, have strong localization properties using the participation ratio, we propose a novel graph diffusion framework via a _local diffusion process_ for efficiently approximating GDEs. This framework effectively tracks most energy during diffusion while maintaining local computation.
* To demonstrate the power of our proposed framework, we prove that APPR [2], a widely used local push-based algorithm, can be treated as a special case. We provide better diffusion-based bounds \(\tilde{\Theta}(\overline{\mathrm{vol}}(\mathcal{S}_{t})/(\alpha\cdot\overline {\gamma}_{t}))\) where \(\overline{\mathrm{vol}}(\mathcal{S}_{t})/\overline{\gamma}_{t}\) is a lower bound of \(1/\epsilon\). This bound is effective for both PPR and Katz and is empirically smaller than \(\Theta(1/(\alpha\epsilon))\), previously well-known for APPR.
* We design simple and fast local methods based on standard gradient descent for APPR and Katz, which admit runtime bounds of \(\min(\overline{\mathrm{vol}}(\mathcal{S}_{t})/(\alpha\cdot\overline{\gamma}_{ t}),1/(\alpha\epsilon))\) for both cases. These methods are GPU-friendly, and we demonstrate that this iterative solution is significantly faster on GPU architecture compared to APPR. When the propagation matrix \(\bm{M}\) is symmetric, we show that this local method can be accelerated further using the local Chebyshev method for PPR and Katz.
* Experimental results on GDE approximation of PPR, HK, and Katz demonstrate that these local solvers significantly accelerate their standard counterparts. We further show that they can be naturally adopted to approximate dynamic diffusion vectors. Our experiments indicate that these local methods for training are twice as fast as standard PPR-based GNNs. These results may suggest a novel _local message-passing_ approach to train commonly used GNNs. _All proofs, detailed experimental settings, and related works are postponed to the appendix. Our code is publicly available at https://github.com/JiaheBai/Faster-Local-Solver-for-GDEs._

## 2 GDEs, Localization, and Existing Local Solvers

**Notations.** We consider an undirected graph \(\mathcal{G}(\mathcal{V},\mathcal{E})\) where \(\mathcal{V}=\{1,2,\ldots,n\}\) is the set of nodes, and \(\mathcal{E}\) is the edge set with \(|\mathcal{E}|=m\). The degree matrix is \(\bm{D}=\mathbf{diag}(d_{1},d_{2},\ldots,d_{n})\), and \(\bm{A}\) is the adjacency matrix of \(\mathcal{G}\). The _volume_ of subset nodes \(\mathcal{S}\) is \(\mathrm{vol}(\mathcal{S})=\sum_{v\in\mathcal{S}}d_{v}\). The set of _neighbors_ of node \(v\) is denoted by \(\mathcal{N}(v)\). The standard basis for node \(s\) is \(\bm{e}_{s}\). The support of \(\bm{x}\) is defined as \(\mathrm{supp}(\bm{x})=\{u:x_{u}\neq 0\}\).

\begin{table}
\begin{tabular}{c|c|c|c} \hline Equ. & \(\bm{M}\) & \(c_{k}\) & \(\bm{s}\) \\ \hline PPR [62] & \(\bm{AD}^{-1}\) & \(\alpha(1-\alpha)^{k}\) & \(\bm{e}_{s}\) \\ \hline Katz [46] & \(\bm{A}\) & \(\alpha^{k}\) & \(\bm{e}_{s}\) \\ \hline HK [21] & \(\bm{AD}^{-1}\) & \(e^{-\tau}\tau^{k}/k!\) & \(\bm{e}_{s}\) \\ \hline IPR [51] & \(\bm{AD}^{-1}\) & \(\frac{\rho^{k}}{(\theta^{k}+\theta^{10})^{2}}\) & \(\bm{e}_{s}\) \\ \hline APPNP [30] & \(\bm{D}^{-\frac{1}{2}}\bm{AD}^{-\frac{1}{2}}\) & \(\alpha(1-\alpha)^{k}\) & \(\bm{x}\) \\ \hline \end{tabular}
\end{table}
Table 1: Example GDEs with their corresponding propagation matrix \(\bm{M}\), coefficients \(c_{k}\), and source \(\bm{s}\).

Figure 1: The maximal participation ratio \(p(\bm{f})\)=\((\sum_{i=1}^{n}|f_{i}|^{2})^{2}/(n\sum_{i=1}^{n}|f_{i}|^{4})\) of example diffusion vectors \(\bm{f}\) over 18 graphs, ordered from small (_cora_) to large (_ogbn-papers100M_). The ratio \(p(\bm{f})\) is normalized by the number of nodes \(n\).

Many graph learning tools can be represented as diffusion vectors. Table 1 presents examples widely used as graph learning tools. The coefficient \(c_{k}\) usually exhibits exponential decay, so most of the energy of \(\bm{f}\) is related to only the first few \(c_{k}\). We revisit these GDEs and existing local solvers.

**Personalized PageRank.** Given the weight decaying strategy \(c_{k}=\alpha(1-\alpha)^{k}\) and a predefined damping factor \(\alpha\in(0,1)\) with \(\bm{M}=\bm{A}\bm{D}^{-1}\), the analytic solution for PPR is

\[\bm{f}_{\text{PPR}}=\alpha(\bm{I}-(1-\alpha)\bm{A}\bm{D}^{-1})^{-1}\bm{e}_{s}.\] (2)

These vectors can be used to find a local graph cut or train GNNs [8, 10, 25, 79]. The well-known approximate PPR (APPR) method [2] locally updates the estimate-residual pair \((\bm{x},\bm{r})\) as follows

\[\text{APPR}:\qquad\qquad\bm{x}\leftarrow\bm{x}+\bm{\alpha}\bm{r}_{u}\cdot\bm {e}_{u},\qquad\bm{r}\leftarrow\bm{r}-\bm{r}_{u}\cdot\bm{e}_{u}+(1-\bm{\alpha}) \bm{r}_{u}\cdot\bm{A}\bm{D}^{-1}\cdot\bm{e}_{u}.\]

Here, each _active_ node \(u\) has a large residual \(r_{u}\geq\epsilon\alpha d_{u}\) with initial values \(\bm{x}\gets 0\) and \(\bm{r}\leftarrow\alpha\bm{e}_{s}\). The runtime bound for reaching \(r_{u}<\epsilon\alpha d_{u}\) for all nodes is graph-independent and is \(\Theta(1/(\alpha\epsilon))\), leveraging the monotonicity property. Previous analyses [49, 18] suggest that APPR is a localized version of the Gauss-Seidel (GS) iteration and thus has fundamental sequential limitations.

**Katz centrality.** Given the weight \(c_{k}=\alpha^{k+1}\) with \(\alpha\in(0,1/\|\bm{A}\|_{2})\) and a different propagation matrix \(\bm{M}=\bm{A}\). The solution for Katz centrality can then be rewritten as:

\[\bm{f}_{\text{Katz}}=(\bm{I}-\alpha\bm{A})^{-1}\bm{e}_{s}.\] (3)

The goal is to approximate the Katz centrality \(((\bm{I}-\alpha\bm{A})^{-1}-\bm{I})\bm{e}_{s}\). Similar to APPR, a local solver [11] for Katz centrality, denoted as AKatz, can be designed as follows

\[\text{AKatz}:\qquad\qquad\bm{x}\leftarrow\bm{x}+r_{u}\bm{e}_{u},\qquad\bm{r} \leftarrow\bm{r}-r_{u}\bm{e}_{u}+\alpha\bm{r}_{u}\bm{A}\bm{e}_{u}.\]

AKatz is a coordinate descent method [11]. Under the nonnegativity assumption of \(\bm{r}\) (i.e., \(\alpha\leq 1/d_{\max}\)), a convergence bound for the residual \(\|\bm{r}^{t+1}\|_{1}\) is provided.

**Heat Kernel.** The heat kernel (HK) [21] is useful for finding smaller but more precise local graph cuts. It uses a different weight decaying \(c_{k}=\tau^{k}e^{-\tau}/k!\) and the vector is then defined as

\[\bm{f}_{\text{HK}}=\exp\left\{-\tau\left(\bm{I}-\bm{M}\right)\right\}\bm{e}_{s},\]

where \(\tau\) is the temperature parameter of the HK equation and \(\bm{M}=\bm{A}\bm{D}^{-1}\). Unlike the previous two, this equation does not admit an analytic solution. Following the technique developed in [48, 49], given an initial vector \(\bm{s}\) and temperature \(\tau\), the HK vector can be approximated using the first \(N\) terms of the Taylor polynomial approximation: define \(\bm{x}_{N}=\sum_{k=0}^{N}\bm{v}_{k}\) with \(\bm{v}_{0}=\bm{e}_{s}\) and \(\bm{v}_{k+1}=\bm{M}\bm{v}_{k}/k\) for \(k=0,\ldots,N\). It is known that \(\|\bm{f}_{\text{HK}}-\bm{x}_{N}\|_{1}\leq 1/(N!N)\). This is equivalent to solving the linear system \(\left(\bm{I}_{N+1}\otimes\bm{I}_{n}-\bm{S}_{N+1}\otimes\bm{M}\right)\bm{v}= \bm{e}_{1}\otimes\bm{e}_{s}\), where \(\otimes\) denotes the Kronecker product. Then, the push-based method, namely AHK, has the following updates

\[\text{\bf AHK}:\quad\bm{v}\leftarrow\bm{v}+r_{u}(\bm{e}_{k}\otimes\bm{e}_{u}),\qquad\bm{r}\leftarrow\bm{r}-r_{u}\left(\bm{I}_{N\!+\!1}\otimes\bm{I}_{n}-\bm {S}_{N\!+\!1}\otimes\bm{M}\right)\left(\bm{e}_{k}\otimes\bm{e}_{u}\right).\]

There are many other types of GDEs, such as Inverse PageRank [51], which generalize PPR. Many GNN propagation layers, such as SGC [69] and APPNP [30], can be formulated as GDEs. For all these \(\bm{f}\), we use the _participation ratio_[54] to measure its localization ability, defined as

\[p(\bm{f})=\Big{(}\sum_{i=1}^{n}|f_{i}|^{2}\Big{)}^{2}\big{/}\Big{(}n\cdot\sum_{ i=1}^{n}|f_{i}|^{4}\Big{)}.\]

When the entries in \(\bm{f}\) are uniformly distributed, such that \(f_{i}\sim\mathcal{O}(1/n)\), then \(p(\bm{f})=\mathcal{O}(1)\). However, in the extremely sparse case where \(\bm{f}\) is a standard basis vector, \(p(\bm{f})=1/n\) indicates the sparsity effect. Figure 1 illustrates the participation ratios, showing that almost all vectors have ratios below \(0.01\). Additionally, larger graphs tend to have smaller participation ratios.

APPR, AKatz, and AHK all have fundamental limitations due to their reliance on sequential Gauss-Seidel-style updates, which limit their parallelization ability. In the following sections, we will develop faster local methods using the local diffusion framework for solving PPR and Katz.

Faster Solvers via Local Diffusion Process

We introduce a local diffusion process framework, which allows standard iterative solvers to be effectively localized. By incorporating this framework, we show that the computation of PPR and Katz defined in (2) and (3) can be locally approximated sequentially or in parallel on GPUs.

### Local diffusion process

To compute \(\bm{f}_{\text{PPR}}\) and \(\bm{f}_{\text{KATZ}}\), it is equivalent to solving the linear systems, which can be written as

\[\bm{Q}\bm{x}=\bm{s},\] (4)

where \(\bm{Q}\) is a (symmetric) positive definite matrix with eigenvalues bounded by \(\mu\) and \(L\), i.e., \(\mu\leq\lambda(\bm{Q})\leq L\). For PPR, we solve \((\bm{I}-(1-\alpha)\bm{A}\bm{D}^{-1})\bm{x}=\alpha\bm{e}_{s}\), which is equivalent to solving \((\bm{I}-(1-\alpha)\bm{D}^{-1/2}\bm{A}\bm{D}^{-1/2})\bm{D}^{-1/2}\bm{x}=\alpha \bm{D}^{-1/2}\bm{e}_{s}\). For Katz, we compute \((\bm{I}-\alpha\bm{A})\bm{x}=\bm{e}_{s}\). The vector \(\bm{s}\) is sparse, with \(\operatorname{supp}(\bm{s})\ll n\). We define _local diffusion process_, a locally evolving set procedure inspired by its stochastic counterpart [58] as follows

**Definition 3.1** (Local diffusion process).: Given an input graph \(\mathcal{G}\), a source distribution \(\bm{s}\), precision tolerance \(\epsilon\), and a local iterative method \(\mathcal{A}\) with parameter \(\theta\) for solving (4), the local diffusion process is defined as a process of updates \(\{(\bm{x}^{(t)},\bm{r}^{(t)},\mathcal{S}_{t})\}_{0\leq t\leq T}\). Specifically, it follows the dynamic system

\[\left(\bm{x}^{(t+1)},\bm{r}^{(t+1)},\mathcal{S}_{t+1}\right)=\phi\left(\bm{x}^ {(t)},\bm{r}^{(t)},\mathcal{S}_{t};\bm{s},\epsilon,\mathcal{G},\mathcal{A}_{ \theta}\right),\quad 0\leq t\leq T.\] (5)

where \(\phi\) is a mapping via some iterative solver \(\mathcal{A}_{\theta}\). For all three diffusion processes (PPR, Katz, and HK), we set \(\mathcal{S}_{0}=\{s\}\). We say this process _converges_ when \(\mathcal{S}_{T}=\emptyset\) if there exists such \(T\); the generated sequence of active nodes are \(\mathcal{S}_{t}\). The total number of operations of the local solver \(\mathcal{A}_{\theta}\) is

\[\mathcal{T}_{\mathcal{A}_{\theta}}=\sum_{t=0}^{T-1}\operatorname{vol}( \mathcal{S}_{t})=T\cdot\overline{\operatorname{vol}}(\mathcal{S}_{T}),\]

where we denote the average of active volume as \(\overline{\operatorname{vol}}(\mathcal{S}_{T})=T^{-1}\sum_{t=0}^{T-1} \operatorname{vol}(\mathcal{S}_{t})\).

Intuitively, we can treat this local diffusion process as a random walk on a Markov Chain defined on the space of all subsets of \(\mathcal{V}\), where the transition probability represents the operation cost. More efficient local solvers try to find a _shorter_ random walk from \(\mathcal{S}_{0}\) to \(\mathcal{S}_{T}\). The following two subsections demonstrate the power of this process in designing local methods for PPR and Katz.

### Sequential local updates via Successive Overrelaxation (SOR)

Given the estimate \(\bm{x}^{(t)}\) at \(t\)-th iteration, we define residual \(\bm{r}^{(t)}=\bm{s}-\bm{Q}\bm{x}^{(t)}\) and \(\bm{Q}\) be the matrix induced by \(\bm{A}\) and \(\bm{D}\). The typical local Gauss-Seidel with Successive Overrelaxation has the following online estimate-residual updates (See Section 11.2 of [34]): at each time \(t=0,1,\dots,T-1\), for each active node \(u_{i}\in\mathcal{S}_{t}=\{u_{1},u_{2},\dots,u_{|\mathcal{S}_{t}|}\}\) with \(i=1,2,\dots,|\mathcal{S}_{t}|\), we update each \(u_{i}\)-th entry of \(\bm{x}\) and corresponding residual \(\bm{r}\) as the following

\[\boxed{\text{LocalSOR}:}\quad\bm{x}^{(\bm{t}+\bm{t}_{t+1})}=\bm{x}^{(\bm{t}+ \bm{t}_{i})}+\bm{\omega}\cdot\bm{\tilde{\bm{e}}}_{u_{i}}^{(\bm{t}+\bm{t}_{i})}, \quad\bm{r}^{(\bm{t}+\bm{t}_{i+1})}=\bm{r}^{(\bm{t}+\bm{t}_{i})}-\bm{\omega} \cdot\bm{Q}\cdot\bm{\tilde{\bm{e}}}_{u_{i}}^{(\bm{t}+\bm{t}_{i})},\] (6)

where \(\bm{t}_{i}\triangleq(i-1)/|\mathcal{S}_{t}|\) is the current time, and update unit vector is \(\tilde{\bm{e}}_{u_{i}}^{(t+t_{i})}\triangleq r_{u_{i}}^{(t+t_{i})}\bm{e}_{u_{ i}}/q_{u_{i}u_{i}}\). Note that each update of (6) is \(\Theta(d_{u_{i}})\). If \(\mathcal{S}_{t}=\mathcal{V}\), it reduces to the standard GS-SOR [34]. Therefore, APPR is a special case of (6), a local variant of GS-SOR with \(\omega=1\). Figure 2 illustrates this procedure. APPR updates \(\bm{x}\) at some entries and keeps track of large magnitudes of \(\bm{r}\) per iteration, while LocalSOR allows for a better choice of \(\omega\); hence, the total number of operations is reduced. We establish the following fundamental property of LocalSOR.

**Theorem 3.2** (Properties of local diffusion process via LocalSOR).: _Let \(\bm{Q}\triangleq\bm{I}-\beta\bm{P}\) where \(\bm{P}\geq\bm{0}_{n\times n}\) and \(P_{\mathrm{uv}}\neq 0\,\text{if}\,(u,v)\in\mathcal{E}\); 0 otherwise. Define maximal value \(P_{\mathrm{max}}=\max_{u\in\mathcal{V}}\|\bm{P}\bm{e}_{u}\|_{1}\). Assume that \(\bm{r}^{(0)}\geq\bm{0}\) is nonnegative and \(P_{\mathrm{max}}\), \(\beta\) are such that \(\beta P_{\mathrm{max}}<1\), given the updates of (6), then the local diffusion process of \(\phi\left(\bm{x}^{(t)},\bm{r}^{(t)},\mathcal{S}_{t};\bm{s},\epsilon,\mathcal{G},\mathcal{A}_{\theta}=(\text{LocalSOR},\omega)\right)\) with \(\omega\in(0,1)\) has the following properties_

1. _Nonnegativity._ \(\bm{r}^{(t+t_{i})}\geq\bm{0}\) _for all_ \(t\geq 0\) _and_ \(t_{i}=(i-1)/|\mathcal{S}_{t}|\) _with_ \(i=1,2,\dots,|\mathcal{S}_{t}|\)
**Monotonicity property**.: \(\|\bm{r}^{(0)}\|_{1}\geq\cdots\|\bm{r}^{(t+t_{i})}\|_{1}\geq\|\bm{r}^{(t+t_{i+1})}\| _{1}\cdots\).

_If the local diffusion process converges (i.e., \(\mathcal{S}_{T}=\emptyset\)), then \(T\) is bounded by_

\[T\leq\frac{1}{\omega\overline{\gamma}_{T}(1-\beta P_{\max})}\ln\frac{\|\bm{r}^ {(0)}\|_{1}}{\|\bm{r}^{(T)}\|_{1}},\text{ where }\overline{\gamma}_{T}\triangleq\frac{1}{T}\sum_{t=0}^{T-1} \left\{\gamma_{t}\triangleq\frac{\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t _{i})}}{\|\bm{r}^{(t)}\|_{1}}\right\}.\]

Based on Theorem 3.2, we establish the following sublinear time bounds of LocalSOR for PPR.

**Theorem 3.3** (Sublinear runtime bound of LocalSOR for PPR).: _Let \(\mathcal{I}_{T}=\operatorname{supp}(\bm{r}^{(T)})\). Given an undirected graph \(\mathcal{G}\) and a target source node \(s\) with \(\alpha\in(0,1),\omega=1\), and provided \(0<\epsilon\leq 1/d_{s}\), the run time of LocalSOR in Equ. (6) for solving \((\bm{I}-(1-\alpha)\bm{A}\bm{D}^{-1})\bm{f}_{\text{PPR}}=\alpha\bm{e}_{s}\) with the stop condition \(\|\bm{D}^{-1}\bm{r}^{(T)}\|_{\infty}\leq\alpha\epsilon\) and initials \(\bm{x}^{(0)}=\bm{0}\) and \(\bm{r}^{(0)}=\alpha\bm{e}_{s}\) is bounded as the following_

\[\mathcal{T}_{\text{LocalSOR}}\leq\min\left\{\frac{1}{\epsilon\alpha},\frac{ \overline{\operatorname{vol}}(\mathcal{S}_{T})}{\alpha\overline{\gamma}_{T}} \ln\frac{C_{\text{PPR}}}{\epsilon}\right\},\qquad\text{ where }\overline{\overline{ \operatorname{vol}}(\mathcal{S}_{T})}\leq\frac{1}{\epsilon}.\] (7)

_where \(C_{\text{PPR}}=1/((1-\alpha)|\mathcal{I}_{T}|)\). The estimate \(\bm{x}^{(T)}\) satisfies \(\|\bm{D}^{-1}(\bm{x}^{(T)}-\bm{f}_{\text{PPR}})\|_{\infty}\leq\epsilon\)._

The above theorem demonstrates the usefulness of our framework. It shows a new evolving bound where \(\overline{\operatorname{vol}}(\mathcal{S}_{T})/\overline{\alpha}_{T}\leq 1/\epsilon\) as long as \(\bm{Q}\) satisfying certain assumption (It is true for PPR and Katz). Similarly, applying Theorem 3.2, we have the following result for approximating \(\bm{f}_{\text{Katz}}\).

**Corollary 3.4** (Runtime bound of LocalSOR for Katz).: _Let \(\mathcal{I}_{T}=\operatorname{supp}(\bm{r}^{(T)})\) and \(C_{\text{Katz}}=1/((1-\alpha)|\mathcal{I}_{T}|)\). Given an undirected graph \(\mathcal{G}\) and a target source node \(s\) with \(\alpha\in(0,1/d_{\max}),\omega=1\), and provided \(0<\epsilon\leq 1/d_{s}\), the run time of LocalSOR in Equ. (6) for solving \((\bm{I}-\alpha\bm{A})\bm{x}=\bm{e}_{s}\) with the stop condition \(\|\bm{D}^{-1}\bm{r}^{(T)}\|_{\infty}\leq\epsilon\) and initials \(\bm{x}^{(0)}=\bm{0}\) and \(\bm{r}^{(0)}=\bm{e}_{s}\) is bounded as the following_

\[\mathcal{T}_{\text{LocalSOR}}\leq\min\left\{\frac{1}{\epsilon(1-\alpha d_{ \max})},\frac{\overline{\operatorname{vol}}(\mathcal{S}_{T})}{(1-\alpha d_{ \max})\overline{\gamma}_{T}}\ln\frac{C_{\text{Katz}}}{\epsilon}\right\},\text{ where }\overline{\overline{\operatorname{vol}}(\mathcal{S}_{T})}\leq\frac{1}{\epsilon}.\] (8)

_The estimate \(\hat{\bm{f}}_{\text{Katz}}=\bm{x}^{(T)}-\bm{e}_{s}\) satisfies \(\|\hat{\bm{f}}_{\text{Katz}}-\bm{f}_{\text{Katz}}\|_{2}\leq\|(\bm{I}-\alpha \bm{A})^{-1}\bm{D}\|_{1}\cdot\epsilon\)._

**Accerlation when \(\bm{Q}\) is Stieltjes matrix (\(\omega^{*}=2/(1+\sqrt{1-(\alpha-1)^{2}})\)).** It is well-known that when \(\omega\in(1,2]\), GS-SOR has acceleration ability when \(\bm{Q}\) is Stieltjes. However, it is fundamentally

Figure 2: The first row illustrates the local diffusion process of APPR [2] over a toy network topology adopted from [40]. It uses \(T_{\text{APPR}}=6\) local iterations with \(\mathcal{T}_{\text{APPR}}=42\) operations and additive error \(\approx 0.29241\). The second row shows the process of LocalSOR (\(\omega=1.19\approx\omega^{*}\)). It uses \(T_{\text{LocalSOR}}=5\) local iterations with \(\mathcal{T}_{\text{LocalSOR}}=28\) operations and additive error \(\approx 0.21479\). LocalSOR uses fewer local iterations, costs less total active volume, and obtains better approximate solutions. We choose the source node \(s=0\) with \(\epsilon=0.02\) and \(\alpha=0.25\).

difficult to prove the runtime bound as the monotonicity property no longer holds. It has been conjectured that a runtime of \(\tilde{\mathcal{O}}(1/(\sqrt{\alpha}\epsilon))\) can be achieved [27]. Incorporating our bound, a new conjecture could be \(\tilde{\mathcal{O}}(\overline{\operatorname{vol}}(\mathcal{S}_{T})/(\sqrt{ \alpha}\overline{\gamma}_{T}))\): If \(\bm{Q}\) is Stieltjes, then with the proper choice of \(\omega\), one can achieve speedup convergence to \(\tilde{\mathcal{O}}(\overline{\operatorname{vol}}(\mathcal{S}_{T})/(\sqrt{ \alpha}\overline{\alpha}_{T}))\)? We leave it as an open problem.

### Parallelizable local updates via GD and Chebyshev

The fundamental limitation of LocalSOR is its reliance on essentially sequential online updates, which may be challenging to utilize with GPU-type computing resources. _Interestingly, we developed a local iterative method that is embarrassingly simpler, highly parallelizable, and provably sublinear for approximating PPR and Katz_. First, one can reformulate the equation of PPR and Katz as 2

Footnote 2: For the PPR equation, \((\bm{I}-(1-\alpha)\bm{A}\bm{D}^{-1})\bm{x}=\alpha\bm{e}_{s}\), we can symmetrize this linear system by rewriting it as \((\bm{I}-(1-\alpha)\bm{D}^{-1/2}\bm{A}\bm{D}^{-1/2})\bm{D}^{-1/2}\bm{x}=\alpha \bm{D}^{-1/2}\bm{e}_{s}\). The solution is then recovered by \(\bm{D}^{1/2}\bm{x}^{(t)}\).

\[\bm{x}_{t}^{*}=\operatorname*{arg\,min}_{\bm{x}\in\mathbb{R}^{n}}f(\bm{x}) \triangleq\frac{1}{2}\bm{x}^{\top}\bm{Q}\bm{x}-\bm{s}^{\top}\bm{x},\] (9)

A natural idea for solving the above is to use standard GD. Hence, following a similar idea of local updates, we simultaneously update solutions for \(\mathcal{S}_{t}\). We propose the following local GD.

\[\boxed{\text{LocalGD}:}\qquad\qquad\qquad\bm{x}^{(t+1)}=\bm{x}^{(t)}+\ \bm{r}_{\mathcal{S}_{t}}^{(t)},\qquad\qquad\bm{r}^{(t+1)}=\bm{r}^{(t)}-\bm{Q} \bm{r}_{\mathcal{S}_{t}}^{(t)}\] (10)

**Theorem 3.5** (Properties of local diffusion process via LocalGD).: _Let \(\bm{Q}\triangleq\bm{I}-\beta\bm{P}\) where \(\bm{P}\geq\bm{0}_{n\times n}\) and \(P_{wv}\neq 0\) if \((u,v)\in\mathcal{E}\); 0 otherwise. Define maximal value \(P_{\max}=\max_{\mathcal{S}_{t}\subseteq\mathcal{V}}\|\bm{P}\bm{r}_{\mathcal{S} _{t}}\|_{1}/\|\bm{r}_{\mathcal{S}_{t}}\|_{1}\). Assume that \(\bm{r}^{(0)}=\bm{s}\geq\bm{0}\) and \(P_{\max}\), \(\beta\) are such that \(\beta P_{\max}<1\), given the updates of (10), then the local diffusion process of \(\phi\left(\mathcal{S}_{t},\bm{x}^{(t)},\bm{r}^{(t)};\mathcal{G},\mathcal{A}_{ \theta}=(\text{LocalGD},\mu,L)\right)\) has the following properties_

1. _Nonnegativity._ \(\bm{r}^{(t)}\geq\bm{0}\) _for all_ \(t\geq 0\)_._
2. _Monotonicity property._ \(\|\bm{r}^{(0)}\|_{1}\geq\cdots\|\bm{r}^{(t)}\|_{1}\geq\|\bm{r}^{(t+1)}\|_{1}\cdots.\)__

_If the local diffusion process converges (i.e., \(\mathcal{S}_{T}=\emptyset\)), then \(T\) is bounded by_

\[T\leq\frac{1}{\overline{\gamma}_{T}(1-\beta P_{\max})}\ln\frac{\|\bm{r}^{(0)} \|_{1}}{\|\bm{r}^{(T)}\|_{1}},\text{ where }\overline{\gamma}_{T}\triangleq\frac{1}{T}\sum_{t=0}^{T-1} \left\{\gamma_{t}\triangleq\frac{\|\bm{r}_{\mathcal{S}_{t}}^{(t)}\|_{1}}{\| \bm{r}^{(t)}\|_{1}}\right\}.\]

Indeed, the above local updates have properties that are quite similar to those of SOR. Based on Theorem 3.5, we establish the sublinear runtime bounds of LocalGD for solving PPR and Katz.

**Corollary 3.6** (Convergence of LocalGD for PPR and Katz).: _Let \(\mathcal{I}_{T}=\operatorname*{supp}(\bm{r}^{(T)})\) and \(C=\frac{1}{(1-\alpha)|\mathcal{I}_{T}|}\). Use LocalGD to approximate PPR or Katz by using iterative procedure (10). Denote \(\mathcal{T}_{\text{PPR}}\) and \(\mathcal{T}_{\text{Katz}}\) as the total number of operations needed by using LocalGD, they can then be bounded by_

\[\mathcal{T}_{\text{PPR}}\leq\min\left\{\frac{1}{\alpha_{\text{PPR}}},\frac{ \overline{\operatorname{vol}}(\mathcal{S}_{T})}{\alpha_{\text{PPR}}\cdot \overline{\gamma}_{T}}\ln\frac{C}{\epsilon}\right\},\quad\frac{\overline{ \operatorname{vol}}(\mathcal{S}_{T})}{\overline{\gamma}_{T}}\leq\frac{1}{\epsilon}\] (11)

_for a stop condition \(\|\bm{D}^{-1}\bm{r}^{(t)}\|_{\infty}\leq\alpha_{\text{PPR}}\cdot\epsilon\). For solving Katz, then the toal runtime is bounded by_

\[\mathcal{T}_{\text{Katz}}\leq\min\left\{\frac{1}{(1-\alpha_{\text{Katz}} \cdot d_{\max})\epsilon},\frac{\overline{\operatorname{vol}}(\mathcal{S}_{T})} {(1-\alpha_{\text{Katz}}\cdot d_{\max})\overline{\gamma}_{T}}\ln\frac{C}{ \epsilon}\right\},\quad\frac{\overline{\operatorname{vol}}(\mathcal{S}_{T})}{ \overline{\gamma}_{T}}\leq\frac{1}{\epsilon}\] (12)

_for a stop condition \(\|\bm{D}^{-1}\bm{r}^{(t)}\|_{\infty}\leq\epsilon d_{u}\). The estimate of equality is the same as that of LocalSOR._

_Remark 3.7_.: Note that LocalGD is quite different from iterative hard-thresholding methods [43] where the time complexity of the thresholding operator is \(\mathcal{O}(n)\) at best, hence not a sublinear algorithm.

**Accerrelated local Chebyshev.** One can extend the Chebyshev method [39], an optimal iterative solver for (9). Following [24], Chebyshev polynomials \(T_{n}\) are defined via following recursions

\[\mathcal{T}_{t}(x)=2x\mathcal{T}_{t-1}(x)-\mathcal{T}_{t-2}(x),\quad\text{ for }k\geq 2,\quad\text{ with }\mathcal{T}_{0}(t)=1,\mathcal{T}_{1}(t)=x.\] (13)Given \(\delta_{1}=\frac{L-\mu}{L+\mu},\bm{x}_{1}=\bm{x}_{0}-\frac{2}{L+\mu}\bm{\nabla}f \left(\bm{x}_{0}\right)\), the standard Chevyshev method is defined as

\[\bm{x}_{k}=\bm{x}_{k-1}-\frac{4\delta_{k}}{L-\mu}\nabla f\left(\bm{x}_{k-1} \right)+\left(1-2\delta_{k}\frac{L+\mu}{L-\mu}\right)\left(\bm{x}_{k-2}-\bm{x} _{k-1}\right),\delta_{k}=\frac{1}{2\frac{L+\mu}{L-\mu}-\delta_{k-1}},\]

where \(\mu\leq\lambda(\bm{Q})\leq L\). For example, for approximating PPR, we propose the following local updates

\[\boxed{\text{LocalCH}:\bm{\pi}^{(t+1)}=\bm{\pi}^{(t)}+\frac{2 \delta_{t+1}}{1-\alpha}\bm{D}^{1/2}\bm{r}_{\mathcal{S}_{t}}^{(t)}+\delta_{t: t+1}(\bm{\pi}^{(t)}-\bm{\pi}^{(t-1)})_{\mathcal{S}_{t}},\delta_{t+1}=\left( \frac{2}{1-\alpha}-\delta_{t}\right)^{-1}}\] \[\bm{D}^{1/2}\bm{r}^{(t+1)}=\bm{D}^{1/2}\bm{r}^{(t)}-(\bm{\pi}^{(t +1)}-\bm{\pi}^{(t)})+(1-\alpha)\bm{A}\bm{D}^{-1}(\bm{\pi}^{(t+1)}-\bm{\pi}^{(t)}).\]

The sublinear runtime analysis for LocalCH is complicated since it does not follow the monotonicity property during the updates. Whether it admits, an accelerated rate remains an open problem.

## 4 Applications to Dynamic GDEs and GNN Propagation

Our accelerated local solvers are ready for many applications. We demonstrate here that they can be incorporated to approximate dynamic GDEs and GNN propagation. We consider the _discrete-time dynamic graph_[47] which contains a sequence of snapshots of the underlying graphs, i.e., \(\mathcal{G}_{0},\mathcal{G}_{1},\dots,\mathcal{G}_{T}\). The transition from \(\mathcal{G}_{t-1}\) to \(\mathcal{G}_{t}\) consists of a list of events \(\mathcal{O}_{t}\) involving edge deletions or insertions. The graph \(\mathcal{G}_{t}\) is then represented as: \(\mathcal{G}_{0}\xrightarrow{\mathcal{O}_{1}}\mathcal{G}_{1}\xrightarrow{ \mathcal{O}_{2}}\mathcal{G}_{2}\xrightarrow{\mathcal{O}_{3}}\cdots\mathcal{G} _{T-1}\xrightarrow{\mathcal{O}_{T}}\mathcal{G}_{T}\). The goal is to calculate an approximate \(\bm{f}_{t}\) for the PPR linear system \(\bm{Q}_{t}\bm{f}_{t}=\alpha\bm{e}_{s}\) at time \(t\). That is,

\[\left(\bm{I}-(1-\alpha)\bm{A}_{t}\bm{D}_{t}^{-1}\right)\bm{f}_{t}=\alpha\bm{ e}_{s}.\] (14)

The key advantage of updating the above equation (presented in Algorithm 3) is that the APPR algorithm is used as a primary component for updating, making it much cheaper than computing from scratch. Consequently, we can apply our local solver LocalSOR to the above equation, and we found that it significantly sped up dynamic GDE calculations and dynamic PPR-based GNN training.

## 5 Experiments

**Datasets.** We conduct experiments on 18 graphs ranging from small-scale (_cora_) to large-scale (_papers100M_), mainly collected from Stanford SNAP [45] and OGB [42] (see details in Table 3). We focus on the following tasks: 1) approximating diffusion vectors \(\bm{f}\) with fixed stop conditions using both CPU and GPU implementations; 2) approximating dynamic PPR using local methods and training dynamic GNN models based on InstantGNN models [75]. 3

Footnote 3: More detailed experimental setups and additional results are in the appendix.

**Baselines and Experimental Setups.** We consider four methods with their localized counterparts: Gauss-Seidel (GS)/LocalGS, Successive Overrelaxation (SOR)/LocalSOR, Gradient Descent (GD)/LocalGD, and Chebyshev (CH)/LocalCH.4 Specifically, we use the stop condition \(\|\bm{D}^{-1}\bm{r}^{(t)}\|_{\infty}\leq\epsilon\alpha\) for PPR and \(\|\bm{D}^{-1}\bm{r}^{(t)}\|_{\infty}\leq\epsilon\) for Katz, while we follow the parameter settings used in [49] for HK. For LocalSOR, we use the optimal parameter \(\omega^{*}\) suggested in Section 3.2. We randomly sample 50 nodes uniformly from lower to higher-degree nodes for all experiments. All results are averaged over these 50 nodes. We conduct experiments using Python 3.10 with CuPy and Numba on a server with 80 cores, 256GB of memory, and two 28GB NVIDIA-4090 GPUs.

Footnote 4: Note that the approximate local solvers of APPR, AHK, and AKatz will be referred to as GS.

### Results on efficiency of local GDE solvers

**Local solvers are faster and use fewer operations.** We first investigate the efficiency of the proposed local solvers for computing PPR and Katz centrality. We set \((\alpha_{\text{PPR}}=0.1,\epsilon=1/n)\) for PPR and \((\alpha_{\text{Katz}}=1/(\|\bm{A}\|_{2}+1),\epsilon=1/m)\) for Katz. We use a temperature of \(\tau=10\) and \(\epsilon=1/\sqrt{n}\) for HK. All algorithms for HK estimate the number of iterations by considering the truncated error of Taylor approximation. Table 2 presents the _speedup ratio_ of four local methods over their standard counterparts. For five graph datasets, the speedup is more than 100 times in terms of the number of operations in most cases. This strongly demonstrates the efficiency of these local solvers. Figure4 presents all such results. Key observations are: 1) _All local methods significantly speed up their global counterparts on all datasets_. This strongly indicates that when \(\epsilon\) is within a certain range, local solvers for GDEs are much cheaper than their global counterparts. 2) _Among all local methods, LocalGS and LocalGD have the best overall performance_. This may seem counterintuitive since LocalSOR and LocalCH are more efficient in convergence rate. However, the number of iterations needed for \(\epsilon=1/n\) or \(\epsilon=1/m\) is much smaller. Hence, their improvements are less significant.

**Which local GDE solvers are the best under what settings?** In the lower precision setting, previous results suggest that LocalSOR and LocalGD perform best overall. To test the proposed LocalSOR efficiency, we use the _ogbn-arxiv_ dataset to evaluate the local algorithm efficiency under a high precision setting. Figure 5 illustrates the performance of LocalSOR and LocalGS for PPR and Katz. As \(\epsilon\) becomes smaller, the speedup of LocalSOR over LocalGS becomes more significant.

**When do local GDE solvers (not) work?** The next critical question is when standard solvers can be effectively localized, meaning under which \(\epsilon\) conditions we see speedup over standard solvers. We conduct experiments to determine when local GDE solvers show speedup over their global counterparts for approximating Katz, HK, and PPR on the _wiki-talk_ graph dataset for different \(\epsilon\) values, ranging from lower precision \((2^{-17})\) to high precision \((2^{-32})\). As illustrated in Figure 6,

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline Graph & SOR/LocalSOR & GS/LocalGS & GD/LocalGD & CH/LocalCH \\ \hline Citeseer & 86.21 & 114.89 & 157.41 & 5.86 \\ \hline ogbn-arxiv & 183.43 & 392.26 & 528.03 & 101.88 \\ \hline ogbn-products & 667.39 & 765.97 & 904.21 & 417.41 \\ \hline wiki-talk & 169.67 & 172.95 & 336.53 & 30.45 \\ \hline ogbn-papers100M & 243.68 & 809.47 & 1137.41 & 131.30 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Speedup ratio of computing PPR vectors. Let \(\mathcal{T_{A}}\) and \(\mathcal{T_{\text{Local}A}}\) be the number of operations of the standard algorithm \(\mathcal{A}\) and the local solver Local\(\mathcal{A}\), respectively. The speedup ratio \(=\mathcal{T_{A}}/\mathcal{T_{\text{Local}A}}\).

Figure 4: Number of operations required for four representative methods and their localized counterparts over 18 graphs. The graph index is sorted according to the performance of LocalGS.

Figure 5: The number of operations as a function of \(\epsilon\) for comparing LocalSOR and LocalGS.

Figure 6: Running time (seconds) as a function of \(\epsilon\) for HK, Katz, and PPR GDEs on the _wiki-talk_ dataset. We use \(\alpha_{\text{PPR}}=0.1\), \(\alpha_{\text{Katz}}=1/(\|\bm{A}\|_{2}+1)\), and \(\tau_{\text{HK}}=10\) with 50 sampled source nodes.

when \(\epsilon\) is in the lower range, local methods are much more efficient than the standard counterparts. As expected, the speedup decreases and becomes worse than the standard counterparts when high precision is needed. This indicates the broad applicability of our framework; the required precisions in many real-world graph applications are within a range where our framework is effective.

### Local GDE solvers on GPU-architecture

We conducted experiments on the efficiency of the GPU implementation of LocalGD and GD, specifically using LocalGD's GPU implementation to compare with other methods. Figure 7 presents the running time of global and local solvers as a function of \(\epsilon\) on the _wiki-talk_ dataset. LocalGD is the fastest among a wide range of \(\epsilon\). This indicates that, when a GPU is available and \(\epsilon\) is within the effective range, LocalGD (GPU) can be much faster than standard GD (GPU) and other methods based on CPUs.

We observed similar patterns for computing Katz in Figure 11.

### Dynamic PPR approximating and training GNN models

To test the applicability of the proposed local solvers, we apply LocalSOR to GNN propagation and test its efficiency on _ogbn-arxiv_ and _ogbn-products_. We use the InstantGNN [75] model (essentially APPNP [30]), following the experimental settings in [75]. Specifically, we set \(\alpha=0.1\), \(\epsilon=10^{-2}/n\), and \(\omega=\omega^{*}\) to the optimal value for our cases. For _ogbn-arxiv_, we randomly partition the graph into 16 snapshots (each snapshot with 59,375 edges), where the initial graph \(\mathcal{G}_{0}\) contains 17.9% of the edges. With a similar performance on testing accuracy, as illustrated in Figure 8, the InstantGNN model with LocalSOR (Alg. 7) has a significantly shorter training time than its local propagation counterpart (Algo. 6). Figure 9 presents the accumulated operations over these 16 snapshots. The faster local solver is LocalSOR (Dynamic), which dynamically updates \((\bm{x},\bm{r})\) for (14) according to Algo. 5 and Algo. 3, whereas LocalSOR (Static) updates all approximate PPR vectors from scratch at the start of each snapshot for (14). We observed a similar pattern for LocalCH, where the number of operations is significantly reduced compared to static ones.

Figure 8: InstantGNN model using local iterative solver (LocalSOR) to do propagation.

Figure 7: Comparison of running time (seconds) for CPU and GPU implementations.

Figure 9: Acculmulated total number of operations of local solvers on the _ogbn-arxiv_ dataset.

## 6 Limitations and Conclusion

When \(\epsilon\) is sufficiently small, the speedup is insignificant, and it is unknown whether more efficient local solvers can be designed under this setting. Although we observed acceleration in practice, the accelerated bounds for LocalSOR and LocalCH have not been proven. Another limitation of local solvers is that they inherit the limitations of their standard counterparts. This paper mainly develops local solvers for PPR and Katz, and it remains interesting to consider other types of GDEs.

We propose the local diffusion process, a local iterative algorithm framework based on the locally evolving set process. Our framework is powerful in capturing existing local iterative solvers such as APPR for GDEs. We then demonstrate that standard iterative solvers can be effectively localized, achieving sublinear runtime complexity when monotonicity properties hold in these local solvers. Extensive experiments show that local solvers consistently speed up standard solvers by a hundredfold. We also show that these local solvers could help build faster GNN models [31; 13; 14; 20]. We expect many GNN models to benefit from these local solvers using a _local message passing_ strategy, which we are actively investigating. Several open problems are worth exploring, such as whether these empirically accelerated local solvers admit accelerated sublinear runtime bounds without the monotonicity assumption.

## Acknowledgments and Disclosure of Funding

The authors would like to thank the anonymous reviewers for their helpful comments. The work of Baojian Zhou is sponsored by Shanghai Pujiang Program (No. 22PJ1401300) and the National Natural Science Foundation of China (No. KRH2305047). The work of Deqing Yang is supported by Chinese NSF Major Research Plan No.92270121. The computations in this research were performed using the CFFF platform of Fudan University.

## References

* [1] Reid Andersen, Christian Borgs, Jennifer Chayes, John Hopcraft, Vahab S Mirrokni, and Shang-Hua Teng. Local computation of PageRank contributions. In _Algorithms and Models for the Web-Graph: 5th International Workshop, WAW 2007, San Diego, CA, USA, December 11-12, 2007. Proceedings 5_, pages 150-165. Springer, 2007.
* [2] Reid Andersen, Fan Chung, and Kevin Lang. Local graph partitioning using PageRank vectors. In _2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06)_, pages 475-486. IEEE, 2006.
* [3] Konstantin Avrachenkov, Paulo Goncalves, and Marina Sokol. On the choice of kernel and labelled data in semi-supervised learning methods. In _Algorithms and Models for the Web Graph: 10th International Workshop, WAW 2013, Cambridge, MA, USA, December 14-15, 2013, Proceedings 10_, pages 56-67. Springer, 2013.
* [4] Konstantin Avrachenkov, Nelly Litvak, Danil Nemirovsky, and Natalia Osipova. Monte carlo methods in PageRank computation: When one iteration is sufficient. _SIAM Journal on Numerical Analysis_, 45(2):890-904, 2007.
* [5] Siddhartha Banerjee and Peter Lofgren. Fast bidirectional probability estimation in markov models. _Advances in Neural Information Processing Systems_, 28, 2015.
* [6] Michele Benzi and Paola Boito. Matrix functions in network analysis. _GAMM-Mitteilungen_, 43(3):e202000012, 2020.
* [7] Pavel Berkhin. Bookmark-coloring algorithm for personalized PageRank computing. _Internet Mathematics_, 3(1):41-62, 2006.
* [8] Aleksandar Bojchevski, Johannes Gasteiger, Bryan Perozzi, Amol Kapoor, Martin Blais, Benedek Rozemberczki, Michal Lukasik, and Stephan Gunnemann. Scaling graph neural networks with approximate PageRank. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 2464-2473, 2020.
* [9] Aleksandar Bojchevski and Stephan Gunnemann. Deep Gaussian embedding of graphs: Unsupervised inductive learning via ranking. _arXiv preprint arXiv:1707.03815_, 2017.
* [10] Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Martin Blais, Amol Kapoor, Michal Lukasik, and Stephan Gunnemann. Is PageRank all you need for scalable graph neural networks. In _ACM KDD, MLG Workshop_, 2019.
* [11] Francesco Bonchi, Pooya Esfandiar, David F Gleich, Chen Greif, and Laks VS Lakshmanan. Fast matrix computations for pairwise and columnwise commute times and Katz scores. _Internet Mathematics_, 8(1-2):73-112, 2012.
* [12] Christian Borgs, Michael Brautbar, Jennifer Chayes, and Shang-Hua Teng. A sublinear time algorithm for PageRank computations. In _International Workshop on Algorithms and Models for the Web-Graph_, pages 41-53. Springer, 2012.
* [13] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele Rossi. Grand: Graph neural diffusion. In _International Conference on Machine Learning_, pages 1407-1418. PMLR, 2021.

* [14] Benjamin Chamberlain, James Rowbottom, Davide Eynard, Francesco Di Giovanni, Xiaowen Dong, and Michael Bronstein. Beltrami flow and neural diffusion on graphs. _Advances in Neural Information Processing Systems_, 34:1594-1609, 2021.
* [15] Deli Chen, Yankai Lin, Guangxiang Zhao, Xuancheng Ren, Peng Li, Jie Zhou, and Xu Sun. Topology-imbalance learning for semi-supervised node classification. _Advances in Neural Information Processing Systems_, 34:29885-29897, 2021.
* [16] Ming Chen, Zhewei Wei, Bolin Ding, Yaliang Li, Ye Yuan, Xiaoyong Du, and Ji-Rong Wen. Scalable graph neural networks via bidirectional propagation. _Advances in neural information processing systems_, 33:14556-14566, 2020.
* [17] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In _International conference on machine learning_, pages 1725-1735. PMLR, 2020.
* [18] Zhen Chen, Xingzhi Guo, Baojian Zhou, Deqing Yang, and Steven Skiena. Accelerating personalized PageRank vector computation. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 262-273, 2023.
* [19] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized PageRank graph neural network. In _International Conference on Learning Representations (ICLR)_, 2021.
* [20] Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. GREAD: Graph neural reaction-diffusion networks. In _International Conference on Machine Learning_, pages 5722-5747. PMLR, 2023.
* [21] Fan Chung. The heat kernel as the PageRank of a graph. _Proceedings of the National Academy of Sciences_, 104(50):19735-19740, 2007.
* [22] Timothy BP Clark and Adrian Del Maestro. Moments of the inverse participation ratio for the laplacian on finite regular graphs. _Journal of Physics A: Mathematical and Theoretical_, 51(49):495003, 2018.
* [23] Haoran Deng, Yang Yang, Jiahe Li, Haoyang Cai, Shiliang Pu, and Weihao Jiang. Accelerating dynamic network embedding with billions of parameter updates to milliseconds. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 414-425, 2023.
* [24] Alexandre d'Aspremont, Damien Scieur, Adrien Taylor, et al. Acceleration methods. _Foundations and Trends(r) in Optimization_, 5(1-2):1-245, 2021.
* [25] Alessandro Epasto, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin, and Peilin Zhong. Differentially private graph learning via sensitivity-bounded personalized PageRank. _Advances in Neural Information Processing Systems_, 35:22617-22627, 2022.
* [26] Illes J Farkas, Imre Derenyi, Albert-Laszlo Barabasi, and Tamas Vicsek. Spectra of "real-world" graphs: Beyond the semicircle law. _Physical Review E_, 64(2):026704, 2001.
* [27] Kimon Fountoulakis and Shenghao Yang. Open problem: Running time complexity of accelerated \(\ell_{1}\)-regularized pagerank. In _Conference on Learning Theory_, pages 5630-5632. PMLR, 2022.
* [28] Dongqi Fu, Zhigang Hua, Yan Xie, Jin Fang, Si Zhang, Kaan Sancak, Hao Wu, Andrey Malevich, Jingrui He, and Bo Long. VCR-graphormer: A mini-batch graph transformer via virtual connections. In _The Twelfth International Conference on Learning Representations_, 2024.
* [29] Dongqi Fu, Dawei Zhou, and Jingrui He. Local motif clustering on time-evolving graphs. In _Proceedings of the 26th ACM SIGKDD International conference on knowledge discovery & data mining_, pages 390-400, 2020.

* [30] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate: Graph neural networks meet personalized PageRank. In _International Conference on Learning Representations (ICLR)_, 2019.
* [31] Johannes Gasteiger, Stefan Weissenberger, and Stephan Gunnemann. Diffusion improves graph learning. _Advances in neural information processing systems_, 32, 2019.
* [32] David F Gleich. PageRank beyond the web. _siam REVIEW_, 57(3):321-363, 2015.
* [33] David F Gleich, Kyle Kloster, and Huda Nassar. Localization in seeded PageRank. _arXiv preprint arXiv:1509.00016_, 2015.
* [34] Gene H Golub and Charles F Van Loan. _Matrix computations_. JHU press, 2013.
* [35] Thomas Guhr, Axel Muller-Groeling, and Hans A Weidenmuller. Random-matrix theories in quantum physics: common concepts. _Physics Reports_, 299(4-6):189-425, 1998.
* [36] Wentian Guo, Yuchen Li, Mo Sha, and Kian-Lee Tan. Parallel personalized PageRank on dynamic graphs. _Proceedings of the VLDB Endowment_, 11(1):93-106, 2017.
* [37] Xingzhi Guo, Baojian Zhou, and Steven Skiena. Subset node representation learning over large dynamic graphs. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 516-526, 2021.
* [38] Xingzhi Guo, Baojian Zhou, and Steven Skiena. Subset node anomaly tracking over large dynamic graphs. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 475-485, 2022.
* [39] Martin H Gutknecht and Stefan Rollin. The chebyshev iteration revisited. _Parallel Computing_, 28(2):263-283, 2002.
* [40] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30, 2017.
* [41] Taher H Haveliwala. Topic-sensitive PageRank. In _Proceedings of the 11th international conference on World Wide Web_, pages 517-526, 2002.
* [42] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A large-scale challenge for machine learning on graphs. _arXiv preprint arXiv:2103.09430_, 2021.
* [43] Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding methods for high-dimensional m-estimation. _Advances in neural information processing systems_, 27, 2014.
* [44] Glen Jeh and Jennifer Widom. Scaling personalized web search. In _Proceedings of the 12th international conference on World Wide Web_, pages 271-279, 2003.
* [45] Leskovec Jure. Snap datasets: Stanford large network dataset collection. _Retrieved December 2021 from http://snap. stanford. edu/data_, 2014.
* [46] Leo Katz. A new status index derived from sociometric analysis. _Psychometrika_, 18(1):39-43, 1953.
* [47] Seyed Mehran Kazemi, Rishab Goel, Kshitij Jain, Ivan Kobyzev, Akshay Sethi, Peter Forsyth, and Pascal Poupart. Representation learning for dynamic graphs: a survey. _The Journal of Machine Learning Research_, 21(1):2648-2720, 2020.
* [48] Kyle Kloster and David F Gleich. A nearly-sublinear method for approximating a column of the matrix exponential for matrices from large, sparse networks. In _Algorithms and Models for the Web Graph: 10th International Workshop, WAW 2013, Cambridge, MA, USA, December 14-15, 2013, Proceedings 10_, pages 68-79. Springer, 2013.
* [49] Kyle Kloster and David F Gleich. Heat kernel based community detection. In _Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 1386-1395, 2014.

* [50] Isabel M Kloumann, Johan Ugander, and Jon Kleinberg. Block models and personalized PageRank. _Proceedings of the National Academy of Sciences_, 114(1):33-38, 2017.
* [51] Pan Li, I Chien, and Olgica Milenkovic. Optimizing generalized pagerank methods for seed-expansion community detection. _Advances in Neural Information Processing Systems_, 32, 2019.
* [52] Yiming Li, Yanyan Shen, Lei Chen, and Mingxuan Yuan. Zebra: When temporal graph neural networks meet temporal personalized PageRank. _Proceedings of the VLDB Endowment_, 16(6):1332-1345, 2023.
* [53] Peter Lofgren, Siddhartha Banerjee, and Ashish Goel. Bidirectional PageRank estimation: From average-case to worst-case. In _Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12_, pages 164-176. Springer, 2015.
* [54] Travis Martin, Xiao Zhang, and Mark EJ Newman. Localization and centrality in networks. _Physical review E_, 90(5):052808, 2014.
* [55] Frank McSherry. A uniform approach to accelerated PageRank computation. In _Proceedings of the 14th international conference on World Wide Web_, pages 575-582, 2005.
* [56] Dingheng Mo and Siqiang Luo. Agenda: Robust personalized PageRanks in evolving graphs. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 1315-1324, 2021.
* [57] Cleve Moler and Charles Van Loan. Nineteen dubious ways to compute the exponential of a matrix, twenty-five years later. _SIAM review_, 45(1):3-49, 2003.
* [58] Ben Morris and Yuval Peres. Evolving sets and mixing. In _Proceedings of the Thirty-Fifth Annual ACM Symposium on Theory of Computing (STOC)_, page 279-286, New York, NY, USA, 2003. Association for Computing Machinery.
* [59] Huda Nassar, Kyle Kloster, and David F Gleich. Strong localization in personalized PageRank vectors. In _Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12_, pages 190-202. Springer, 2015.
* [60] Huda Nassar, Kyle Kloster, and David F Gleich. Localization in seeded PageRank. _Internet Mathematics_, 2017.
* [61] Lorenzo Orecchia, Sushant Sachdeva, and Nisheeth K Vishnoi. Approximating the exponential, the lanczos method and an \(o(m)\)-time spectral algorithm for balanced separator. In _Proceedings of the forty-fourth annual ACM symposium on Theory of computing_, pages 1141-1160, 2012.
* [62] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank Citation Ranking: Bringing Order to the Web. Technical report, Stanford Digital Library Technologies Project, 1998.
* [63] Stefan Postavaru, Anton Tsitsulin, Filipe Miguel Goncalves de Almeida, Yingtao Tian, Silvio Lattanzi, and Bryan Perozzi. Instantembedding: Efficient local node representations. _arXiv preprint arXiv:2010.06992_, 2020.
* [64] Purnamrita Sarkar, Andrew W Moore, and Amit Prakash. Fast incremental proximity search in large graphs. In _Proceedings of the 25th international conference on Machine learning_, pages 896-903, 2008.
* [65] Ingo Scholtes. When is a network a network? multi-order graphical model selection in pathways and temporal networks. In _Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1037-1046, 2017.
* [66] Jieming Shi, Renchi Yang, Tianyuan Jin, Xiaokui Xiao, and Yin Yang. Realtime top-k personalized PageRank over large graphs on gpus. _Proceedings of the VLDB Endowment_, 13(1):15-28, 2019.

* [67] Anton Tsitsulin, Marina Munkhoeva, Davide Mottin, Panagiotis Karras, Ivan Oseledets, and Emmanuel Muller. FREDE: anytime graph embeddings. _Proceedings of the VLDB Endowment_, 14(6):1102-1110, 2021.
* [68] Hanzhi Wang, Mingguo He, Zhewei Wei, Sibo Wang, Ye Yuan, Xiaoyong Du, and Ji-Rong Wen. Approximate graph propagation. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 1686-1696, 2021.
* [69] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In _International conference on machine learning_, pages 6861-6871. PMLR, 2019.
* [70] Hao Wu, Junhao Gan, Zhewei Wei, and Rui Zhang. Unifying the global and local approaches: an efficient power iteration with forward push. In _Proceedings of the 2021 International Conference on Management of Data_, pages 1996-2008, 2021.
* [71] Hao Yin, Austin R Benson, Jure Leskovec, and David F Gleich. Local higher-order graph clustering. In _Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 555-564, 2017.
* [72] Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, Andrey Malevich, Rajgopal Kannan, Viktor Prasanna, Long Jin, and Ren Chen. Decoupling the depth and scope of graph neural networks. _Advances in Neural Information Processing Systems_, 34:19665-19679, 2021.
* [73] Hongyang Zhang, Peter Lofgren, and Ashish Goel. Approximate personalized PageRank on dynamic graphs. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1315-1324, 2016.
* [74] Jialin Zhao, Yuxiao Dong, Ming Ding, Evgeny Kharlamov, and Jie Tang. Adaptive diffusion in graph neural networks. _Advances in neural information processing systems_, 34:23321-23333, 2021.
* [75] Yanping Zheng, Hanzhi Wang, Zhewei Wei, Jiajun Liu, and Sibo Wang. Instant graph neural networks for dynamic graphs. In _Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining_, pages 2605-2615, 2022.
* [76] Baojian Zhou, Yifan Sun, and Reza Babanezhad Harikandeh. Fast online node labeling for very large graphs. In _International Conference on Machine Learning_, pages 42658-42697. PMLR, 2023.
* [77] Baojian Zhou, Yifan Sun, Reza Babanezhad Harikandeh, Xingzhi Guo, Deqing Yang, and Yanghua Xiao. Iterative methods via locally evolving set process. _arXiv preprint arXiv:2410.15020_, 2024.
* [78] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Scholkopf. Learning with local and global consistency. _Advances in neural information processing systems_, 16, 2003.
* [79] Qi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. Shift-robust GNNs: Overcoming the limitations of localized graph training data. _Advances in Neural Information Processing Systems_, 34:27965-27977, 2021.
* [80] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using Gaussian fields and harmonic functions. In _Proceedings of the 20th International conference on Machine learning (ICML-03)_, pages 912-919, 2003.
* [81] Zeyuan Allen Zhu, Silvio Lattanzi, and Vahab Mirrokni. A local algorithm for finding well-connected clusters. In _International Conference on Machine Learning_, pages 396-404. PMLR, 2013.

Related Work

**Disclaimer**: The local diffusion process introduced in this paper shares a similar conceptual foundation with our concurrent work [77], where we propose the locally evolving set process. However, these two approaches address distinct problems. In this work, our focus is on investigating a different class of equations and exploring whether the local diffusion process can accelerate the training of Graph Neural Networks (GNNs).

**Graph diffusion equations (GDEs) and localization.** Graph diffusion vectors can either be exactly represented as a linear system or be approximations of an underlying linear system. The general form defined in Equation (1) has appeared in various literature [49, 51] (see more examples in [68]) or can be found in the survey works of [32]. The locality and localization properties of the diffusion vector \(\bm{f}\), including the PPR vector, have been explored in [64, 2, 59, 33, 60]. Inspired by methods for measuring the dynamic localization of quantum chaos [35, 22], which are also used for analyzing graph eigenfunctions [11, 26, 54], we use the _inverse participation ratio_ to measure the localization ability of \(\bm{f}\). However, there is a lack of a local framework for solving GDEs efficiently.

**Standard solvers for GDEs.** The computation of a graph diffusion equation approximates the exponential of a propagation matrix, either through an inherently Neumann series or as an analytic system of linear equations. Well-established iterative methods, such as those approximating via the Neumann series or Pade approximations, exist (see more details in the surveys [57, 6, 61] and [34]). Each iteration requires computing the matrix-vector dot product, requiring \(\mathcal{O}(m)\) operations for all the above methods. This is the computation we aim to avoid.

**Local methods for GDEs.** The most well-known local solver for solving PPR was initially proposed in [2] for local clustering. PPR, as a fundamental tool for graph learning, has been applied to many core problems, including graph neural networks [8, 17, 69, 15], graph diffusion [31, 74], and its generalizations [51, 19] for GNNs or clustering [50, 71]. Many variants have been proposed for solving such equations in different forms using local methods that couple local methods with Monte Carlo sampling strategies, achieving sublinear time [5, 53]. The work of [55] attempts to accelerate PPR computation via the Gauss-Southwell method, but it does not work well in practice [48]. A notable finding from [70] is that the local computation of PPR is equivalent to power iteration when the precision is high. There are also works on temporal PPR vector computation [66, 36, 56, 52], and our work can be immediately applied to these. Some local variants of APPR have been proposed for Katz and HK [11, 49]. However, these local solvers are sequential and lack the ability to be easily implemented on GPUs. Our local framework addresses this challenge.

**PPR on dynamic graphs.** Ranking on dynamic graphs has many applications [65], including motif clustering [71, 29], embedding on large-scale graphs [63, 23, 37, 38, 76, 18], and designing graph neural networks [75, 72]. The local computation via forward push (a.k.a. APPR algorithm [2]) or reverse push (a.k.a. reverse APPR [1]) is widely used in GNN propagation and bidirectional propagation algorithms [16, 28]. Our framework helps to design more efficient dynamic GNNs.

## Appendix B Missing Proofs

### Notations

We list all notations used in our proofs here.

* \(\bm{e}_{u}\): The indicator vector where the \(u\)-th entry is 1, and 0 otherwise.
* \(\mathcal{N}(u)\): Set of neighbors of node \(u\).
* \(d_{u}\): The degree of node \(u\).
* \(\mathcal{G}(\mathcal{V},\mathcal{E})\): The underlying graph with the set of nodes \(\mathcal{V}\) and set of edges \(\mathcal{E}\).
* \(\bm{A}\): Adjacency matrix of \(\mathcal{G}\).
* \(\bm{D}\): Degree matrix of \(\mathcal{G}\) when \(\mathcal{G}\) is undirected.
* \(\bm{\Pi}\): Personalized PPR matrix, defined as \(\bm{\Pi}=\alpha\left(\bm{I}-(1-\alpha)\bm{A}\bm{D}^{-1}\right)^{-1}\).
* \(\alpha_{\text{PPR}}\): Damping factor \(\alpha_{\text{PPR}}\in(0,1)\) for the PPR equation.
* \(\alpha_{\text{Katz}}\): Parameter \(\alpha_{\text{Katz}}\in(0,1)\) for the Katz equation.

* \(\epsilon\): The error tolerance.
* \(\|\bm{D}^{-1}\bm{r}\|_{\infty}\leq\alpha_{\text{PRR}}\epsilon\): The stop condition for local solvers of PPR.
* \(\|\bm{D}^{-1}\bm{r}\|_{\infty}\leq\epsilon\): The stop condition for local solvers of Katz.

### Formulation of PPR and Katz

In this section, we restate all theorems and present all missing proofs. We first restate the two target linear systems we aim to solve, as introduced in Section 3. Recall PPR is defined as \(\left(\bm{I}-(1-\alpha_{\text{PRR}})\bm{A}\bm{D}^{-1}\right)\bm{f}_{\text{PRR }}=\alpha_{\text{PRR}}\bm{e}_{s}\). It can be equivalently written as

\[\text{PPR:}\qquad\bm{Qx}=\bm{s},\quad\bm{Q}=\left(\bm{I}-(1-\alpha_{\text{PRR }})\bm{D}^{-1/2}\bm{A}\bm{D}^{-1/2}\right)\bm{x}=\alpha_{\text{PRR}}\bm{D}^{- 1/2}\bm{e}_{s},\] (15)

where \(\alpha_{\text{PRR}}\in(0,1)\) and \(\bm{x}^{*}=\alpha\bm{Q}^{-1}\bm{D}^{-1/2}\bm{e}_{s}\). Hence, \(\bm{f}_{\text{PRR}}=\bm{D}^{1/2}\bm{x}^{*}\). The Katz centrality can be written as

\[\text{Katz:}\qquad\bm{Qx}=\bm{s},\quad(\bm{I}-\alpha_{\text{Katz}}\bm{A})\, \bm{x}=\bm{e}_{s},\] (16)

where \(\alpha_{\text{Katz}}\in(0,1/\|\bm{A}\|_{2})\). Then \(\bm{f}_{\text{Katz}}=\bm{x}^{*}-\bm{e}_{s}\). 5

Footnote 5: We will arbitrarily write \(\alpha_{\text{PRR}}\) and \(\alpha_{\text{Katz}}\) as \(\alpha\) when the context is clear.

**Proposition B.1** ([3, 53]).: _Let \(\mathcal{G}(\mathcal{V},\mathcal{E})\) be an undirected graph and \(u\) and \(v\) be two vertices in \(\mathcal{V}\). Denote \(\bm{f}_{s}:=\alpha(\bm{I}-(1-\alpha)\bm{A}\bm{D}^{-1})\bm{e}_{s}\) as the PPR vector of a source node \(s\). Then_

\[d_{u}\cdot f_{u}[v]=d_{v}\cdot f_{v}[u].\]

Proof.: We directly follow the proof strategy in [53] for completeness. For path \(P=\{s,v_{1},v_{2},\ldots,v_{k},t\}\) in \(\mathcal{G}\), we denote its length as \(\ell(P)\) (here \(\ell(P)=k+1\) ), and define its reverse path to be \(\bar{P}=\{t,v_{k},\ldots,v_{2},v_{1},s\}-\) note that \(\ell(P)=\ell(\bar{P})\). Moreover, we know that a random walk starting from \(s\) traverses path \(P\) with probability \(\mathbb{P}[P]=\frac{1}{d_{s}}\cdot\frac{1}{d_{e_{1}}}\cdot\cdots\cdot\frac{1}{ d_{v_{k}}}\), and thus, it is easy to see that we have

\[\mathbb{P}[P]\cdot d_{s}=\mathbb{P}[\bar{P}]\cdot d_{t}.\]

Now let \(\mathcal{P}_{st}\) denote the paths in \(G\) starting at \(s\) and terminating at \(t\). Then, we can re-write \(f_{s}[t]\) as:

\[f_{s}[t] =\sum_{P\in\mathcal{P}_{st}}\alpha(1-\alpha)^{\ell(P)}\mathbb{P} [P]\] \[=\sum_{P\in\mathcal{P}_{st}}\alpha(1-\alpha)^{\ell(P)}\frac{d_{t} }{d_{s}}\mathbb{P}[\bar{P}]\] \[=\frac{d_{t}}{d_{s}}\sum_{\bar{P}\in\mathcal{P}_{ts}}\alpha(1- \alpha)^{\ell(\bar{P})}\mathbb{P}[\bar{P}]\] \[=\frac{d_{t}}{d_{s}}f_{t}[s],\]

We adopt the typical implementation of APPR as presented in Algo. 1.

### Missing proofs of LocalSOR

**Theorem 3.2** (Properties of local diffusion process via LocalSOR).: _Let \(\bm{Q}\triangleq\bm{I}-\beta\bm{P}\) where \(\bm{P}\geq\bm{0}_{n\times n}\) and \(P_{uv}\neq 0\) if \((u,v)\in\mathcal{E}\); 0 otherwise. Define maximal value \(P_{\max}=\max_{u\in\mathcal{V}}\|\bm{P}\bm{e}_{u}\|_{1}\). Assume that \(\bm{r}^{(0)}\geq\bm{0}\) is nonnegative and \(P_{\max}\), \(\beta\) are such that \(\beta P_{\max}<1\), given the updates of (6), then the local diffusion process of \(\phi\left(\bm{x}^{(t)},\bm{r}^{(t)},\mathcal{S}_{t};\bm{s},\epsilon,\mathcal{G },\mathcal{A}_{\theta}=(\text{LocalSOR},\omega)\right)\) with \(\omega\in(0,1)\) has the following properties_

1. _Nonnegativity._ \(\bm{r}^{(t+t_{i})}\geq\bm{0}\) _for all_ \(t\geq 0\) _and_ \(t_{i}=(i-1)/|\mathcal{S}_{t}|\) _with_ \(i=1,2,\ldots,|\mathcal{S}_{t}|\)_._
2. _Monotonicity property._ \(\|\bm{r}^{(0)}\|_{1}\geq\cdots\|\bm{r}^{(t+t_{i})}\|_{1}\geq\|\bm{r}^{(t+t_{i+ 1})}\|_{1}\cdots\)_._

_If the local diffusion process converges (i.e., \(\mathcal{S}_{T}=\emptyset\)), then \(T\) is bounded by_

\[T\leq\frac{1}{\omega\overline{\gamma}_{T}(1-\beta P_{\max})}\ln\frac{\|\bm{r}^ {(0)}\|_{1}}{\|\bm{r}^{(T)}\|_{1}},\;\text{where}\;\overline{\gamma}_{T}\triangleq \frac{1}{T}\sum_{t=0}^{T-1}\left\{\gamma_{t}\triangleq\frac{\sum_{i=1}^{| \mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}}{\|\bm{r}^{(t)}\|_{1}}\right\}.\]

Proof.: At each step \(t\) and \(t_{i}=(i-1)/|\mathcal{S}_{t}|\) where \(i=1,2,\ldots,|\mathcal{S}_{t}|\), recall LocalSOR for solving \(\bm{Q}\bm{x}=\bm{s}\) has the following updates

\[\bm{x}^{(t+t_{i+1})}=\bm{x}^{(t+t_{i})}+\frac{\omega\cdot r_{u_{i}}^{(t+t_{i}) }}{q_{u_{i}u_{i}}}\bm{e}_{u_{i}},\quad\bm{r}^{(t+t_{i+1})}=\bm{r}^{(t+t_{i})}- \frac{\omega\cdot r_{u_{i}}^{(t+t_{i})}}{q_{u_{i}u_{i}}}\bm{Q}\cdot\bm{e}_{u_{ i}}.\]

Since we define \(\bm{Q}\) as \(\bm{Q}=\bm{I}-\beta\bm{P}\), where \(P_{uu}=0\) for all \(u\in\mathcal{V}\), and using \(q_{u_{i}u_{i}}=1\), we continue to have the updates of \(\bm{r}\) as follows

\[\bm{r}^{(t+t_{i+1})} =\bm{r}^{(t+t_{i})}-\frac{\omega\cdot r_{u_{i}}^{(t+t_{i})}}{q_{u _{i}u_{i}}}\cdot\bm{Q}\bm{e}_{u_{i}}\] \[=\bm{r}^{(t+t_{i})}-\omega\cdot r_{u_{i}}^{(t+t_{i})}\bm{e}_{u_{i }}+\omega\beta r_{u_{i}}^{(t+t_{i})}\cdot\bm{P}\bm{e}_{u_{i}}.\]

By using induction, we show the nonnegativity of \(\bm{r}^{(t)}\). First of all, \(\bm{r}^{(0)}\geq\bm{0}\) by our assumption. Let us assume \(\bm{r}^{(t+t_{i})}\geq\bm{0}\), then the above equation gives \(\bm{r}^{(t+t_{i+1})}\geq\bm{0}\) since \(\omega\leq 1\) and \(\bm{P}\geq\bm{0}_{n\times n}\) by our assumption. Since we assume \(\omega\leq 1\) and \(\bm{P}\geq\bm{0}_{n\times n}\), the above equation can be written as \(\bm{r}^{(t+t_{i+1})}+\omega\cdot r_{u_{i}}^{(t+t_{i})}\bm{e}_{u_{i}}=\bm{r}^{( t+t_{i})}+\omega\beta r_{u_{i}}^{(t+t_{i})}\cdot\bm{P}\bm{e}_{u_{i}}\). By taking \(\|\cdot\|_{1}\) on both sides, we have

\[\|\bm{r}^{(t+t_{i+1})}\|_{1}+\omega r_{u_{i}}^{(t+t_{i})}=\|\bm{r}^{(t+t_{i})} \|_{1}+\omega\beta r_{u_{i}}^{(t+t_{i})}\|\bm{P}\bm{e}_{u_{i}}\|_{1}\]

To make an effective reduction, \(\beta\) should be such that \(\beta\|\bm{P}\bm{e}_{u_{i}}\|_{1}<1\) for all \(u_{i}\in\mathcal{V}\). Summing over \(i=1,2,\ldots,|\mathcal{S}_{t}|\) of the above equation, we then have the following

\[\|\bm{r}^{(t+1)}\|_{1} =\left(1-\frac{\omega\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t _{i})}\left(1-\beta\|\bm{P}\bm{e}_{u_{i}}\|_{1}\right)}{\|\bm{r}^{(t)}\|_{1}} \right)\|\bm{r}^{(t)}\|_{1}\] \[\leq(1-\omega\gamma_{t}\left(1-\beta P_{\max}\right))\|\bm{r}^{(t )}\|_{1}.\] (17)

Note that we defined \(\gamma_{t}=\frac{\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}}{\|\bm{r}^ {(t)}\|_{1}}\) and \((1-\beta P\max)\leq(1-\beta P_{u_{i}})\) for all \(u_{i}\in\mathcal{S}_{t}\). The inequality of (3.1) leads to the following bound

\[\|\bm{r}^{(T)}\|_{1}\leq\prod_{t=0}^{T-1}\left(1-\omega\gamma_{t}\left(1-\beta P _{\max}\right)\right)\|\bm{r}^{(0)}\|_{1}.\]

To further simply the above upper bound, since each term \(1-\omega\gamma_{t}\left(1-\beta P_{\max}\right)\geq 0\) during the updates, it follows that \(\omega\gamma_{t}\left(1-\beta P_{\max}\right)\in(0,1)\). If there exists \(T\) such that \(\mathcal{S}_{T}=\emptyset\), then we can obtain an upper bound of \(T\) as

\[\ln\frac{\|\bm{r}^{(T)}\|_{1}}{\|\bm{r}^{(0)}\|_{1}} \leq\sum_{t=0}^{T-1}\ln\left(1-\omega\gamma_{t}\left(1-\beta P_{ \max}\right)\right)\] \[\leq-\sum_{t=0}^{T-1}\omega\gamma_{t}\left(1-\beta P_{\max} \right),\]which leads to

\[T\leq\frac{1}{\omega\overline{\gamma}_{T}(1-\beta P_{\max})}\ln\frac{\|\bm{r}^{(0 )}\|_{1}}{\|\bm{r}^{(T)}\|_{1}}.\]

**Theorem 3.3** (Sublinear runtime bound of LocalSOR for PPR).: _Let \(\mathcal{I}_{T}=\operatorname{supp}(\bm{r}^{(T)})\). Given an undirected graph \(\mathcal{G}\) and a target source node \(s\) with \(\alpha\in(0,1),\omega=1\), and provided \(0<\epsilon\leq 1/d_{s}\), the run time of LocalSOR in Equ. (6) for solving \((\bm{I}-(1-\alpha)\bm{AD}^{-1})\bm{f}_{\text{\footnotesize PPR}}=\alpha\bm{e}_ {s}\) with the stop condition \(\|\bm{D}^{-1}\bm{r}^{(T)}\|_{\infty}\leq\alpha\epsilon\) and initials \(\bm{x}^{(0)}=\bm{0}\) and \(\bm{r}^{(0)}=\alpha\bm{e}_{s}\) is bounded as the following_

\[\mathcal{T}_{\text{\footnotesize LocalSOR}}\leq\min\left\{\frac{1}{\epsilon \alpha},\frac{\overline{\operatorname{vol}}(\mathcal{S}_{T})}{\alpha\overline {\gamma}_{T}}\ln\frac{C_{\text{\footnotesize PPR}}}{\epsilon}\right\},\qquad \text{ where }\frac{\overline{\operatorname{vol}}(\mathcal{S}_{T})}{\overline{ \gamma}_{T}}\leq\frac{1}{\epsilon}.\] (18)

_where \(C_{\text{\footnotesize PPR}}=1/((1-\alpha)|\mathcal{I}_{T}|)\). The estimate \(\bm{x}^{(T)}\) satisfies \(\|\bm{D}^{-1}(\bm{x}^{(T)}-\bm{f}_{\text{\footnotesize PPR}})\|_{\infty}\leq\epsilon\)._

Proof.: Recall \(\mathcal{S}_{t}=\{u_{1},u_{2},\ldots,u_{|\mathcal{S}_{t}|}\}\) be the set of active nodes processed in \(t\) iteration. For convenience, we denote \(|\mathcal{S}_{t}|=k\). By LocalSOR defined in (6) for solving

\[\underbrace{\left(\bm{I}-(1-\alpha)\bm{AD}^{-1}\right)}_{\bm{Q}}\bm{f}_{ \text{\footnotesize PPR}}=\underbrace{\alpha\bm{e}_{s}}_{s}.\]

We have the following online iteration updates for each active node \(u_{i}\in\mathcal{S}_{t}\) (recall \(q_{u_{i}u_{i}}=1\)).

\[\bm{x}^{(t+\frac{i}{k})} =\bm{x}^{(t+\frac{i-1}{k})}+\omega\cdot r_{u_{i}}^{(t+\frac{i-1}{ k})}\cdot\bm{e}_{u_{i}},\quad\text{ for }i=1,\ldots,k\text{ and }u_{i}\in\mathcal{S}_{t},\] \[\bm{r}^{(t+\frac{i}{k})} =\bm{r}^{(t+\frac{i-1}{k})}-\omega\cdot r_{u_{i}}^{(t+\frac{i-1}{ k})}\cdot\bm{e}_{u_{i}}+\omega\cdot(1-\alpha)\cdot r_{u_{i}}^{(t+\frac{i-1}{k})} \cdot\bm{AD}^{-1}\bm{e}_{u_{i}}.\]

Note for each active node \(u_{i}\), we have \(r_{u_{i}}^{(t+\frac{i-1}{k})}\geq\epsilon\alpha d_{u_{i}}\). The total operations for LocSOR is

\[\mathcal{T}_{\text{\footnotesize LocalSOR}} :=\sum_{t=0}^{T-1}\operatorname{vol}(\mathcal{S}_{t})\] \[=\sum_{t=0}^{T-1}\sum_{i=1}^{k}d_{u_{i}}\] \[\leq\sum_{t=0}^{T-1}\sum_{i=1}^{k}\frac{r_{u_{i}}^{(t+(i-1)/k))}} {\epsilon\alpha}\] \[=\sum_{t=0}^{T-1}\sum_{i=1}^{k}\frac{\|\bm{r}^{(t+\frac{i-1}{k})} \|_{1}-\|\bm{r}^{(t+\frac{i}{k})}\|_{1}}{\omega\epsilon\alpha^{2}}\] \[=\frac{\|\bm{r}^{(0)}\|_{1}-\|\bm{r}^{(T)}\|_{1}}{\omega\epsilon \alpha^{2}}\] \[\leq\frac{\|\bm{r}^{(0)}\|_{1}}{\omega\epsilon\alpha^{2}}\] \[=\frac{1}{\epsilon\alpha},\]

where the last equality follows from \(\omega=1\) and \(\|\bm{r}^{(0)}\|_{1}=\alpha\). Next, we focus on the proof of our newly derived bound \(\tilde{O}(\overline{\operatorname{vol}}(\mathcal{S}_{T})/(\alpha\overline{ \gamma}T))\). To check the upper bound of \(T\), from Theorem 3.2 by noticing \(\beta=(1-\alpha)\) and \(P_{\max}=1\), this leads to the following

\[T \leq\frac{1}{\omega\overline{\gamma}_{T}(1-\beta P_{\max})}\ln \frac{\|\bm{r}^{(0)}\|_{1}}{\|\bm{r}^{(T)}\|_{1}}\] \[=\frac{1}{\alpha\overline{\gamma}_{T}}\ln\frac{\|\bm{r}^{(0)}\|_{ 1}}{\|\bm{r}^{(T)}\|_{1}}.\]Next, we try to give a lower bound of \(\bm{r}^{(T)}\). Note that after the last iteration \(T\), for each nonzero residual \(r_{u}^{(T)}\neq 0\), \(u\in\mathcal{I}_{T}\), there is at least one possible update that happened at node \(u\): 1) Node \(u\) has a neighbor \(v_{u}\in\mathcal{N}(u)\), which was active. This neighbor \(v_{u}\) pushed some residual \((1-\alpha)r_{v_{u}}^{(t^{\prime})}/d_{v_{u}}\) to \(u\) where the time \(t^{\prime}<T\). Hence, for all \(u\in\mathcal{I}_{T}\), we have

\[\|\bm{r}^{(T)}\|_{1} =\sum_{u\in\mathcal{I}_{T}}r_{u}^{(T)}\] \[\geq\sum_{u\in\mathcal{I}_{T}}\frac{(1-\alpha)r_{v_{u}}^{(t^{ \prime})}}{d_{v_{u}}}\] \[\geq\sum_{u\in\mathcal{I}_{T}}\frac{(1-\alpha)\epsilon\alpha d_{v _{u}}}{d_{v_{u}}}\] \[=\sum_{u\in\mathcal{I}_{T}}(1-\alpha)\epsilon\alpha\] \[\geq\alpha\epsilon(1-\alpha)|\mathcal{I}_{T}|,\]

where the first equality is due to the nonnegativity of \(\bm{r}\) guaranteeing by Theorem 3.2 and the second inequality is due to the fact that \(r_{u}^{(t^{\prime})}\) was active residuals before the push operation. Applying the above lower bound of \(\|\bm{r}^{(T)}\|_{1}\), we obtain

\[\frac{\|\bm{r}^{(0)}\|_{1}}{\|\bm{r}^{(T)}\|_{1}} \leq\frac{\|\bm{r}^{(0)}\|_{1}}{\alpha\epsilon(1-\alpha)|\mathcal{ I}_{T}|}\] \[=\frac{1}{\epsilon(1-\alpha)|\mathcal{I}_{T}|}\] \[:=\frac{C_{\text{PPR}}}{\epsilon},\]

where \(C_{\text{PPR}}=1/((1-\alpha)|\mathcal{I}_{T}|)\) and \(\mathcal{I}_{T}=\operatorname{supp}(\bm{r}^{(T)})\). Combine the above inequality and the upper bound \(T\), we obtain our new local diffusion-based bounds. The rest is to show a lower bound of \(1/\epsilon\). By using the active node condition, for \(u_{i}\in\mathcal{S}_{t}\), we have

\[\alpha\epsilon\operatorname{vol}(\mathcal{S}_{t}) \leq\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}\] \[\implies\alpha\epsilon\sum_{t=0}^{T-1}\operatorname{vol}( \mathcal{S}_{t}) \leq\sum_{t=0}^{T-1}\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}.\] (19)

We continue to have

\[\frac{\overline{\operatorname{vol}}(\mathcal{S}_{T})}{\overline{ \gamma}_{T}} =\frac{\sum_{t=0}^{T-1}\operatorname{vol}\left(\mathcal{S}_{t} \right)}{\sum_{t=0}^{T-1}\gamma_{t}}\] \[\leq\frac{\sum_{t=0}^{T-1}\operatorname{vol}\left(\mathcal{S}_{t }\right)}{\sum_{t=0}^{T-1}\frac{\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_ {i})}}{\|\bm{r}^{(0)}\|_{1}}}\] \[=\alpha\frac{\sum_{t=0}^{T-1}\operatorname{vol}\left(\mathcal{S}_ {t}\right)}{\sum_{t=0}^{T-1}\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}}\] \[\leq\frac{1}{\epsilon},\]

where the first inequality follows from the fact that \(\frac{\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}}{\|\bm{r}^{(0)}\|_{1} }\leq\frac{\sum_{i}=1^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}}{\|\bm{r}^{(t)} \|_{1}}\) for \(t=0,1,\ldots,T-1\), due to the monotonicity property of \(\|\bm{r}^{(t)}\|_{1}\). The last inequality is from (19). Combining these, we finish the proof of the sublinear runtime bound. The rest is to prove the estimation quality. Note \(\|\bm{D}^{-1}\bm{r}^{(T)}\|_{\infty}\leq\alpha\epsilon\) directly implies \(\|\bm{D}^{-1}(\bm{x}^{(T)}-\bm{f_{\text{PPR}}})\|_{\infty}\leq\epsilon\) by using Proposition B.1 and the corresponding stop condition.

_Remark B.2_.: The above theorem shares the proof strategy we provided in [77]. Here, we use a slightly different formulation of the linear system.

**Corollary 3.4** (Runtime bound of LocalSOR for Katz).: _Let \(\mathcal{I}_{T}=\operatorname{supp}(\bm{r}^{(T)})\) and \(C_{\text{Katz}}=1/((1-\alpha)|\mathcal{I}_{T}|)\). Given an undirected graph \(\mathcal{G}\) and a target source node \(s\) with \(\alpha\in(0,1/d_{\max}),\omega=1\), and provided \(0<\epsilon\leq 1/d_{s}\), the run time of LocalSOR in Equ. (6) for solving \((\bm{I}-\alpha\bm{A})\bm{x}=\bm{e}_{s}\) with the stop condition \(\|\bm{D}^{-1}\bm{r}^{(T)}\|_{\infty}\leq\epsilon\) and initials \(\bm{x}^{(0)}=\bm{0}\) and \(\bm{r}^{(0)}=\bm{e}_{s}\) is bounded as the following_

\[\mathcal{T}_{\text{LocalSOR}}\leq\min\left\{\frac{1}{\epsilon(1-\alpha d_{ \max})},\frac{\overline{\operatorname{vol}}(\mathcal{S}_{T})}{(1-\alpha d_{ \max})\overline{\gamma}_{T}}\ln\frac{C_{\text{Katz}}}{\epsilon}\right\},\; \text{where}\;\frac{\overline{\operatorname{vol}}(\mathcal{S}_{T})}{ \overline{\gamma}_{T}}\leq\frac{1}{\epsilon}.\] (20)

_The estimate \(\hat{\bm{f}}_{\text{Katz}}=\bm{x}^{(T)}-\bm{e}_{s}\) satisfies \(\|\hat{\bm{f}}_{\text{Katz}}-\bm{f}_{\text{Katz}}\|_{2}\leq\|(\bm{I}-\alpha \bm{A})^{-1}\bm{D}\|_{1}\epsilon\)._

Proof.: The linear system \((\bm{I}-\alpha\bm{A})\bm{x}=\bm{s}\) using LocalSOR updates can be written as

\[\bm{x}^{(t+t_{i}+\Delta t)} =\bm{x}^{(t+t_{i})}+\frac{\omega r_{u_{i}}^{(t+t_{i})}}{q_{u_{i}u _{i}}}\bm{e}_{u_{i}},\] \[\bm{r}^{(t+t_{i}+\Delta t)} =\bm{r}^{(t+t_{i})}-\frac{\omega r_{u_{i}}^{(t+t_{i})}}{q_{u_{i}u _{i}}}\bm{e}_{u_{i}}+\omega r_{u_{i}}^{(t+t_{i})}\alpha\bm{A}\frac{\bm{e}_{u_{ i}}}{q_{u_{i}u_{i}}},\]

where \(q_{u_{i}u_{i}}=1\). The updates of \(\bm{r}^{(t+t_{i})}\) can be simplified as the following

\[\bm{r}^{(t+t_{i}+\Delta t)} =\bm{r}^{(t+t_{i})}-\omega r_{u_{i}}^{(t+t_{i})}\bm{e}_{u_{i}}+ \omega r_{u_{i}}^{(t+t_{i})}\alpha\bm{A}\bm{e}_{u_{i}}.\]

The proof of nonnegativity of \(\bm{r}^{(t)}\) follows the similar induction as in previous theorem by noticing the spectral radius of \(\bm{A}\) is \(|\lambda(\bm{A})|\leq d_{\max}\). Hence, we have

\[\bm{r}^{(t+1)} =\bm{r}^{(t)}-\omega\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t _{i})}\bm{e}_{u_{i}}+\omega\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i}) }\alpha\bm{A}\bm{e}_{u_{i}},\]

Move the negative term to left and take \(\ell_{1}\)-norm on both sides, we have

\[\omega\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})} =\|\bm{r}^{(t)}\|_{1}-\|\bm{r}^{(t+1)}\|_{1}+\omega\sum_{i=1}^{| \mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}\alpha d_{u_{i}}\] \[\|\bm{r}^{(t+1)}\|_{1} =\left(1-\frac{\omega\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t _{i})}-\omega\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}\alpha d_{u_{i} }}{\|\bm{r}^{(t)}\|_{1}}\right)\|\bm{r}^{(t)}\|_{1}\] \[\leq\left(1-\frac{\omega(1-\alpha d_{\max})\sum_{i=1}^{|\mathcal{ S}_{t}|}r_{u_{i}}^{(t+t_{i})}}{\|\bm{r}^{(t)}\|_{1}}\right)\|\bm{r}^{(t)}\|_{1}.\]

It leads to the following

\[\omega\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}-\omega\sum_{i=1}^{| \mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}\alpha d_{u_{i}}=\|\bm{r}^{(t)}\|_{1}- \|\bm{r}^{(t+1)}\|_{1}\]

Since each \(r_{u_{i}}^{(t+t_{i})}\geq\epsilon d_{u_{i}}\), \(\operatorname{vol}(\mathcal{S}_{t})\leq\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i} }^{(t+t_{i})}/\epsilon\), we continue have

\[\omega\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}-\omega \sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}\alpha d_{u_{i}} =\omega\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})}\left(1- \alpha d_{u_{i}}\right)\] \[\geq\omega\sum_{i=1}^{|\mathcal{S}_{t}|}r_{u_{i}}^{(t+t_{i})} \left(1-\alpha d_{\max}\right).\]

Finally, the above inequality gives the following upper runtime bound as

\[\sum_{t=0}^{T-1}\operatorname{vol}(\mathcal{S}_{t}) \leq\sum_{t=0}^{T-1}\frac{\|\bm{r}^{(t)}\|_{1}-\|\bm{r}^{(t+1)}\| _{1}}{\omega\epsilon(1-\alpha d_{\max})}\] \[=\frac{\|\bm{r}^{(0)}\|_{1}-\|\bm{r}^{(T)}\|_{1}}{\omega\epsilon(1- \alpha d_{\max})}\] \[\leq\frac{1}{\omega\epsilon(1-\alpha d_{\max})}.\]By letting \(\beta=\alpha\) and \(\bm{P}=\bm{A}\) with \(P_{\max}=d_{\max}\), we apply Theorem 3.2, to obtain the local diffusion-based bound as

\[\|\bm{r}^{(t+1)}\|_{1} \leq(1-\omega(1-\alpha d_{\max})\beta_{t})\,\|\bm{r}^{(t)}\|_{1}\] \[\|\bm{r}^{(T)}\|_{1} \leq\prod_{t=0}^{T-1}\left(1-\omega(1-\alpha d_{\max})\beta_{t} \right)\|\bm{r}^{(0)}\|_{1}\] \[=\prod_{t=0}^{T-1}\left(1-\omega(1-\alpha d_{\max})\beta_{t} \right).\]

Using the similar technique provided in the previous case, and letting \(\omega=1\) we can have

\[\mathcal{T}_{\mathrm{Katz}}\leq\frac{\overline{\mathrm{vol}}(\mathcal{S}_{T}) }{(1-\alpha d_{\max})\overline{\gamma}_{T}}\ln\frac{C_{\mathrm{Katz}}}{ \epsilon}.\]

To check the estimate equality, we have

\[\bm{r}^{(T)} =\bm{e}_{s}-(\bm{I}-\alpha\bm{A})\bm{x}^{(T)}\] \[\implies(\bm{I}-\alpha\bm{A})^{-1}\bm{r}^{(T)} =\left((\bm{I}-\alpha\bm{A})^{-1}-\bm{I}\right)\bm{e}_{s}+\bm{e}_ {s}-\bm{x}^{(T)}\] \[=\bm{f}_{\mathrm{Katz}}+\bm{e}_{s}-\bm{x}^{(T)}.\]

Let \(\hat{\bm{f}}_{\mathrm{Katz}}:=\bm{x}^{(T)}-\bm{e}_{s}\) This leads to

\[\|\hat{\bm{f}}_{\mathrm{Katz}}-\bm{f}_{\mathrm{Katz}}\|_{2} =\|(\bm{I}-\alpha\bm{A})^{-1}\bm{D}\bm{D}^{-1}\bm{r}^{(T)}\|_{2}\] \[\leq\|(\bm{I}-\alpha\bm{A})^{-1}\bm{D}\|_{1}\cdot\|\bm{D}^{-1}\bm {r}^{(T)}\|_{\infty}\] \[\leq\|(\bm{I}-\alpha\bm{A})^{-1}\bm{D}\|_{1}\epsilon.\]

### Missing proofs of LocalGD

**Theorem 3.5** (Properties of local diffusion process via LocalGD).: _Let \(\bm{Q}\triangleq\bm{I}-\beta\bm{P}\) where \(\bm{P}\geq\bm{0}_{n\times n}\) and \(P_{uv}\neq 0\) if \((u,v)\in\mathcal{E}\); 0 otherwise. Define maximal value \(P_{\max}=\max_{\mathcal{S}_{t}\subseteq\gamma}\|\bm{Pr}_{\mathcal{S}_{t}}\|_{ 1}/\|\bm{r}_{\mathcal{S}_{t}}\|_{1}\). Assume that \(\bm{r}^{(0)}=\bm{s}\geq\bm{0}\) and \(P_{\max}\), \(\beta\) are such that \(\beta P_{\max}<1\), given the updates of (10), then the local diffusion process of \(\phi\left(\mathcal{S}_{t},\bm{x}^{(t)},\bm{r}^{(t)};\mathcal{G},\mathcal{A}_{ \theta}=(\text{LocalGD},\mu,L)\right)\) has the following properties_

1. _Nonnegativity._ \(\bm{r}^{(t)}\geq\bm{0}\) _for all_ \(t\geq 0\)_._
2. _Monotonicity property._ \(\|\bm{r}^{(0)}\|_{1}\geq\cdots\|\bm{r}^{(t)}\|_{1}\geq\|\bm{r}^{(t+1)}\|_{1}\cdots.\)__

_If the local diffusion process converges (i.e., \(\mathcal{S}_{T}=\emptyset\)), then \(T\) is bounded by_

\[T\leq\frac{1}{\overline{\gamma}_{T}(1-\beta P_{\max})}\ln\frac{\|\bm{r}^{(0)} \|_{1}}{\|\bm{r}^{(T)}\|_{1}},\text{ where }\overline{\gamma}_{T}\triangleq\frac{1}{T}\sum_{t=0}^{T-1} \left\{\gamma_{t}\triangleq\frac{\|\bm{r}_{\mathcal{S}_{t}}^{(t)}\|_{1}}{\| \bm{r}^{(t)}\|_{1}}\right\}.\]

Proof.: At each step \(t\), recall LocalGD for solving \(\bm{Q}\bm{x}=(\bm{I}-\beta\bm{P})\bm{x}=\bm{s}\) has the following updates

\[\bm{x}^{(t+1)}=\bm{x}^{(t)}+\bm{r}_{\mathcal{S}_{t}}^{(t)},\qquad\qquad\bm{r}^ {(t+1)}=\bm{r}^{(t)}-\bm{Q}\bm{r}_{\mathcal{S}_{t}}^{(t)}.\]

Since \(\bm{Q}=\bm{I}-\beta\bm{P}\), we continue to have the updates of \(\bm{r}\) as the following

\[\bm{r}^{(t+1)} =\bm{r}^{(t)}-(\bm{I}-\beta\bm{P})\bm{r}_{\mathcal{S}_{t}}^{(t)}\] \[=\bm{r}^{(t)}-\bm{r}_{\mathcal{S}_{t}}^{(t)}+\beta\bm{Pr}_{ \mathcal{S}_{t}}^{(t)}.\]

By using induction, we show the nonnegativity of \(\bm{r}^{(t)}\). First of all, \(\bm{r}^{(0)}\geq\bm{0}\) by our assumption. Let us assume \(\bm{r}^{(t)}\geq\bm{0}\). Then, the above equation gives \(\bm{r}^{(t+1)}\geq\bm{0}\) since \(\beta\geq 0\) and \(\bm{P}\geq\bm{0}_{n\times n}\) by our assumption. The above equation can be written as

\[\bm{r}^{(t+1)}+\bm{r}_{\mathcal{S}_{t}}^{(t)}=\bm{r}^{(t)}+\beta\bm{Pr}_{ \mathcal{S}_{t}}^{(t)}\]Taking \(\|\cdot\|_{1}\) on both sides, we have

\[\|\bm{r}^{(t+1)}\|_{1}+\|\bm{r}_{\mathcal{S}_{t}}^{(t)}\|_{1}=\|\bm{r}^{(t)}\|_{1} +\beta\|\bm{Pr}_{\mathcal{S}_{t}}^{(t)}\|_{1}.\]

To make an effective reduction, \(\beta\) should be such that \(\beta\|\bm{Pr}_{\mathcal{S}_{t}}^{(t)}\|_{1}<\|\bm{r}_{\mathcal{S}_{t}}^{(t)}\|_ {1}\) for all \(\mathcal{S}_{t}\subseteq\mathcal{V}\). For this, we then have the following

\[\|\bm{r}^{(t+1)}\|_{1} =\left(1-\frac{\|\bm{r}_{\mathcal{S}_{t}}^{(t)}\|_{1}-\beta\|\bm{ Pr}_{\mathcal{S}_{t}}^{(t)}\|_{1}}{\|\bm{r}^{(t)}\|_{1}}\right)\|\bm{r}^{(t)}\|_{1}\] \[\leq(1-\gamma_{t}\left(1-\beta P_{\max}\right))\|\bm{r}^{(t)}\|_{1},\] (21)

where note we defined \(\gamma_{t}=\frac{\|\bm{r}_{\mathcal{S}_{t}}^{(t)}\|_{1}}{\|\bm{r}^{(t)}\|_{1}}\) and assumed \(\|\bm{Pr}_{\mathcal{S}_{t}}^{(t)}\|_{1}\leq P_{\max}\|\bm{r}_{\mathcal{S}_{t} }^{(t)}\|_{1}\). Apply (21) from \(t=0\) to \(T\), it leads to the following bound

\[\|\bm{r}^{(T)}\|_{1}\leq\prod_{t=0}^{T-1}\left(1-\gamma_{t}\left(1-\beta P_{ \max}\right)\right)\|\bm{r}^{(0)}\|_{1}\]

Note each of the term \(1-\gamma_{t}\left(1-\beta P_{\max}\right)\geq 0\) during the updates, then \(\gamma_{t}\left(1-\beta P_{\max}\right)\in(0,1).\) To check the upper bound of \(T\), we have

\[\ln\frac{\|\bm{r}^{(T)}\|_{1}}{\|\bm{r}^{(0)}\|_{1}} \leq\sum_{t=0}^{T-1}\ln\left(1-\gamma_{t}\left(1-\beta P_{\max} \right)\right)\] \[\leq-\sum_{t=0}^{T-1}\gamma_{t}\left(1-\beta P_{\max}\right),\]

which leads to

\[T\leq\frac{1}{\overline{\gamma}_{T}(1-\beta P_{\max})}\ln\frac{\|\bm{r}^{(0)} \|_{1}}{\|\bm{r}^{(T)}\|_{1}}.\]

**Corollary 3.6** (Convergence of LocalGD for PPR and Katz).: _Let \(\mathcal{I}_{T}=\operatorname{supp}(\bm{r}^{(T)})\) and \(C=\frac{1}{(1-\alpha)\|\mathcal{I}_{T}\|}\). Use LocalGD to approximate PPR or Katz by using iterative procedure (10). Denote \(\mathcal{T}_{\text{PPR}}\) and \(\mathcal{T}_{\text{Katz}}\) as the total number of operations needed by using LocalGD, they can then be bounded by_

\[\mathcal{T}_{\text{PPR}}\leq\min\left\{\frac{1}{\alpha_{\text{PPR}}\cdot\epsilon },\frac{\overline{\operatorname{vol}}(\mathcal{S}_{T})}{\alpha_{\text{PPR}} \cdot\overline{\gamma}_{T}}\ln\frac{C}{\epsilon}\right\},\quad\frac{\overline {\operatorname{vol}}(\mathcal{S}_{T})}{\overline{\gamma}_{T}}\leq\frac{1}{\epsilon}\] (22)

_for a stop condition \(\|\bm{D}^{-1}\bm{r}^{(t)}\|_{\infty}\leq\alpha_{\text{PPR}}\cdot\epsilon\). For solving Katz, then the deal runtime is bounded by_

\[\mathcal{T}_{\text{Katz}}\leq\min\left\{\frac{1}{(1-\alpha_{\text{Katz}}\cdot d _{\max})\epsilon},\frac{\overline{\operatorname{vol}}(\mathcal{S}_{T})}{(1- \alpha_{\text{Katz}}\cdot d_{\max})\overline{\gamma}_{T}}\ln\frac{C_{2}}{ \epsilon}\right\},\quad\frac{\overline{\operatorname{vol}}(\mathcal{S}_{T})}{ \overline{\gamma}_{T}}\leq\frac{1}{\epsilon}\] (23)

_for a stop condition \(\|\bm{D}^{-1}\bm{r}^{(t)}\|_{\infty}\leq\epsilon d_{u}\). The estimate equality is the same as of LocalSOR._

Proof.: We first show graph-independent bound \(\mathcal{O}(1/(\alpha\epsilon))\) for PPR computation. Since \(\bm{r}^{(t+1)}=\bm{r}^{(t)}-\bm{Q}\bm{r}_{\mathcal{S}_{t}}^{(t)}\), then rearrange it to6

Footnote 6: To simplify the proof, here \(\bm{r}\) represents \(\bm{D}^{1/2}\bm{r}\).

\[\bm{r}^{(t+1)}+\bm{r}_{\mathcal{S}_{t}}^{(t)}=\bm{r}^{(t)}+(1-\alpha)\bm{A} \bm{D}^{-1}\bm{r}_{\mathcal{S}_{t}}^{(t)}.\]

Note that entries in \(\bm{r}^{(t)}\) are nonnegative. It leads to

\[\|\bm{r}^{(t+1)}\|_{1}+\|\bm{r}_{\mathcal{S}_{t}}^{(t)}\|_{1}=\|\bm{r}^{(t)}\|_ {1}+\|(1-\alpha)\bm{A}\bm{D}^{-1}\bm{r}_{\mathcal{S}_{t}}^{(t)}\|_{1},\]where note \(\|(1-\alpha)\bm{AD}^{-1}\bm{r}_{\mathcal{S}_{t}}^{(t)}\|_{1}=(1-\alpha)\|\bm{r}_{ \mathcal{S}_{t}}^{(t)}\|_{1}\), then we have

\[\|\bm{r}_{\mathcal{S}_{t}}^{(t)}\|_{1}=(\|\bm{r}^{(t)}\|_{1}-\|\bm{r}^{(t+1)}\|_ {1})/\alpha.\]

At step \(t\), LocalGD accesses the indices in \(\mathcal{S}_{t}\). By the active node condition \(\epsilon\alpha d_{u_{i}}\leq r_{u_{i}}^{(t)}\), we have

\[\operatorname{vol}(\mathcal{S}_{t}) =\sum_{i=1}^{k}d_{u_{i}}\] \[\leq\sum_{i=1}^{k}\frac{r_{u_{i}}^{(t)}}{\epsilon\alpha}\] \[=\frac{\|\bm{r}_{\mathcal{S}_{t}}^{(t)}\|_{1}}{\epsilon\alpha}\] \[=\frac{\|\bm{r}^{(t)}\|_{1}-\|\bm{r}^{(t+1)}\|_{1}}{\epsilon \alpha^{2}}.\]

Then the total run time of LocalGD is

\[\mathcal{T}_{\text{PPR}} :=\sum_{t=0}^{T-1}\operatorname{vol}(\mathcal{S}_{t})\] \[\leq\frac{1}{\epsilon\alpha^{2}}\sum_{t=0}^{T-1}\left(\|\bm{r}^{( t)}\|_{1}-\|\bm{r}^{(t+1)}\|_{1}\right)\] \[=\frac{\|\bm{r}^{(0)}\|_{1}-\|\bm{r}^{(T)}\|_{1}}{\epsilon\alpha^ {2}}\] \[\leq\frac{1}{\epsilon\alpha},\]

where the last inequality is due to \(\|\bm{r}^{(t)}\|_{1}=\alpha\). Therefore, the total run time is \(\mathcal{O}(1/(\alpha\epsilon))\). Follow the similar technique, we have sublinear runtime bound for Katz, i.e., \(\mathcal{T}_{\text{Kata}}\leq\frac{1}{(1-\alpha_{\text{Kau}}\cdot d_{\max})\epsilon}\). Next, we show the local diffusion bound for PPR. Recall LocalGD has the initial \(\bm{x}^{(0)}=\bm{0},\bm{r}^{(0)}=\alpha\bm{e}_{s}\) and the following updates

\[\bm{x}^{(t+1)}=\bm{x}^{(t)}+\bm{r}_{\mathcal{S}_{t}}^{(t)},\quad\bm{r}^{(t+1)} =\bm{r}^{(t)}-\bm{Q}\bm{r}_{\mathcal{S}_{t}}^{(t)},\quad\mathcal{S}_{t}= \left\{u:r_{u}^{(t)}\geq\epsilon\alpha d_{u},u\in\mathcal{V}\right\}.\]

Since \(\beta=1-\alpha\) and \(\bm{P}=\bm{AD}^{-1}\), by applying Theorem 3.5, we have the upper bound of \(T\)

\[\|\bm{r}^{(T)}\|_{1}=\prod_{t=0}^{T-1}\left(1-\alpha\gamma_{t}\right)\|\bm{r}^ {(0)}\|_{1}\qquad\Rightarrow\qquad T\leq\frac{1}{\alpha\overline{\gamma}_{T}} \ln\frac{\|\bm{r}^{(0)}\|_{1}}{\|\bm{r}^{(T)}\|_{1}}.\]

Note that each nonzero \(r_{u}^{(T)}\) has at least part of the magnitude from the push operation of an active node. This means each nonzero of \(\bm{r}^{(T)}\) satisfies

\[r_{u}^{(T)} \geq\frac{(1-\alpha)r_{v}^{(\tilde{t})}}{d_{v}}\] \[\geq\frac{(1-\alpha)\alpha\epsilon d_{v}}{d_{v}}=(1-\alpha)\alpha \epsilon,\text{ for }u\in\mathcal{I}_{T}\] \[\Rightarrow\quad\|\bm{r}^{(T)}\|_{1} \geq(1-\alpha)\alpha\epsilon\|\mathcal{I}_{T}|,\]

where \(\tilde{t}\leq T\). Hence, we have

\[T \leq\frac{1}{\alpha\overline{\gamma}_{T}}\ln\frac{\|\bm{r}^{(0)} \|_{1}}{\|\bm{r}^{(T)}\|_{1}}\] \[\leq\frac{1}{\alpha\overline{\gamma}_{T}}\ln\frac{\|\bm{r}^{(0)} \|_{1}}{\alpha\epsilon(1-\alpha)|\mathcal{I}_{T}|}\] \[:=\frac{1}{\alpha\overline{\gamma}_{T}}\ln\frac{C}{\epsilon},\]where \(C=\frac{1}{(1-\alpha)|Z_{T}|}\). To see the additive error, note that \(\bm{r}^{(T)}:=\bm{b}-\bm{Q}\bm{x}^{(T)}=\alpha\bm{e}_{s}-\left(\bm{I}-(1-\alpha) \bm{A}\bm{D}^{-1}\right)\bm{x}^{(T)}\). It has the following equality

\[\bm{f}_{s}-\bm{x}^{(T)}=\left(\bm{I}-(1-\alpha)\bm{A}\bm{D}^{-1}\right)^{-1} \bm{r}^{(T)}\]

To estimate the bound \(|f_{s}[u]-x_{u}^{(T)}|\), we need to know \((\left(\bm{I}-(1-\alpha)\bm{A}\bm{D}^{-1}\right)^{-1}\bm{r}^{(T)})_{u}\). Specifically, the \(u\)-th entry of the above is

\[f_{s}[u]-x_{u}^{(T)}=\frac{1}{\alpha}\sum_{v\in\mathcal{V}}f_{v}[u]r_{v}^{(T)}\]

By Proposition B.1, we know \(d_{u}f_{u}[v]=d_{v}f_{v}[u]\), then we continue to have

\[f_{s}[u]-x_{u}^{(T)} =\frac{1}{\alpha}\sum_{v\in\mathcal{V}}\frac{d_{u}f_{u}[v]}{d_{v }}r_{v}^{(T)}\] \[\leq\frac{1}{\alpha}\sum_{v\in\mathcal{V}}d_{u}f_{u}[v]\epsilon\alpha\] \[=\epsilon d_{u}\sum_{v\in\mathcal{V}}f_{u}[v]\] \[=\epsilon d_{u},\]

where the first inequality is due to \(r_{v}^{(T)}<\epsilon\alpha d_{v}\), and the last equality follows from \(\sum_{v\in\mathcal{V}}f_{u}[v]=1\). Combining all inequalities for \(u\in\mathcal{V}\), we have the estimate \(|\bm{D}^{-1}(\hat{\bm{f}}-\bm{f}_{\text{PPR}})|_{1}\leq\epsilon\). We omit the proof of the sublinear runtime bound of LocalGD for Katz, as it largely follows the proof technique in Corollary 3.4. For the two lower bounds of \(1/\epsilon\), i.e., \(\overline{\operatorname{vol}}(\mathcal{S}_{T})/\overline{\gamma}_{T}\leq\frac {1}{\epsilon}\), it directly follows from the monotonicity and nonnegativity properties stated in Theorem 3.2 and Theorem 3.5. 

_Remark B.3_.: The part of the theorem shares the similar proof strategy we provided in [77]. Here, we use a slightly different formulation of the linear system.

### Implementation Details

* **Chebyshev for PPR.** Let \(\bm{Q}=\bm{I}-(1-\alpha)\bm{D}^{-1/2}\bm{A}\bm{D}^{-1/2}\) and \(\bm{b}=\alpha\bm{D}^{-1/2}\bm{e}_{s}\). Then \(\bm{f}^{(t)}=\bm{D}^{1/2}\bm{x}^{(t)}\). The eigenvalue of \(\bm{Q}\) is in range \([\alpha,2-\alpha]\). So, we let \(L=2-\alpha\) and \(\mu=\alpha\). 1. \(\delta_{1}=1-\alpha,\bm{x}^{(0)}=\bm{0},\bm{D}^{1/2}\bm{r}^{(0)}=\alpha\bm{e}_ {s}\). 2. When \(t=1\), we have \[\bm{D}^{1/2}\bm{x}^{(1)} =\bm{D}^{1/2}\bm{x}^{(0)}-\bm{D}^{1/2}\bm{\nabla}f(\bm{x}^{(0)})\] \[=\alpha\bm{e}_{s}\] \[\bm{D}^{1/2}\bm{r}^{(1)} =\alpha\bm{e}_{s}-(\bm{I}-(1-\alpha)\bm{A}\bm{D}^{-1})\bm{D}^{1/2 }\bm{x}^{(1)}\] \[=\alpha(1-\alpha)\bm{A}\bm{D}^{-1}\bm{e}_{s}.\]
3. When \(t\geq 1\), we have \[\delta_{t+1} =\left(\tfrac{2}{1-\alpha}-\delta_{t}\right)^{-1}\] \[\hat{\bm{f}}^{(t)} =\frac{2\delta_{t+1}}{1-\alpha}\bm{D}^{1/2}\bm{r}^{(t)}+\delta_{t }\delta_{t+1}\bm{D}^{1/2}\bm{\Delta}^{(t)}\] \[\bm{f}^{(t+1)} =\bm{f}^{(t)}+\hat{\bm{f}}^{(t)}\] \[\bm{D}^{1/2}\bm{r}^{(t+1)} =\bm{D}^{1/2}\bm{r}^{(t)}-\left(\bm{I}-(1-\alpha)\bm{A}\bm{D}^{-1} \right)\hat{\bm{f}}^{(t)}.\] For LocalCH, we change the update \(\hat{\bm{f}}^{(t)}\) to the update \(\hat{\bm{f}}^{(t)}_{\mathcal{S}_{t}}\). The final estimate \(\hat{\bm{f}}:=\bm{D}^{1/2}\bm{x}^{(T)}\).
* **Chebyshev for Katz.** We want to solve \((\bm{I}-\alpha\bm{A})\bm{x}=\bm{e}_{s}\). We assume \(\mu=1-\alpha\lambda_{\max}(\bm{A}),L=1-\alpha\lambda_{\min}(\bm{A})\). Then, the updates of LocalCH for Katz centrality are 1. When \(t=0\), \(\bm{x}^{(0)}=\bm{0},\bm{r}^{(0)}=\bm{e}_{s}\). To obtain an initial value \(\delta_{1}\), we have \(\delta_{1}=\alpha d_{\max}\)7 Footnote 7: This is because \(|\lambda(\bm{A})|\leq d_{\max}\).

1. When \(t=0\), \(\bm{x}^{(0)}=\bm{0},\bm{r}^{(0)}=\bm{e}_{s}\). To obtain an initial value \(\delta_{1}\), we have \(\delta_{1}=\alpha d_{\max}\)2. When \(t=1\), we have \[\bm{x}^{(1)} =\bm{x}^{(0)}+\frac{2}{L+\mu}\bm{r}^{(0)}=\frac{2}{L+\mu}\bm{e}_{s}\] \[\bm{r}^{(1)} =\bm{b}-(\bm{I}-\alpha\bm{A})\bm{x}^{(1)}\] \[=\bm{e}_{s}-(\bm{I}-\alpha\bm{A})\frac{2}{L+\mu}\bm{e}_{s},\bm{r}^ {(1)}\] \[=(1-\tfrac{2}{L+\mu})\bm{e}_{s}+\frac{2\alpha}{L+\mu}\bm{A}\bm{e}_ {s}.\]
3. When \(t\geq 1\), it updates the estimate-residual pair as \[\delta_{t+1} =\left(\tfrac{2}{\alpha d_{\max}}-\delta_{t}\right)^{-1}\] \[\hat{\bm{x}}^{(t)} =\tfrac{4\delta_{t+1}}{L-\mu}\bm{r}^{(t)}+\delta_{t}\delta_{t+1} \bm{\Delta}^{(t)}\] \[\bm{x}^{(t+1)} =\bm{x}^{(t)}+\hat{\bm{x}}^{(t)}\] \[\bm{r}^{(t+1)} =\bm{r}^{(t)}-(\bm{I}-\alpha\bm{A})\,\hat{\bm{x}}^{(t)}.\] For LocalCH, we change the update \(\hat{\bm{x}}^{(t)}\) to the update \(\hat{\bm{x}}^{(t)}_{\mathcal{S}_{t}}\). The final estimate is then \(\hat{\bm{f}}:=\hat{\bm{x}}^{(T)}-\bm{e}_{s}\).

We omit the details of LocalSOR and LocalGD for PPR and Katz as they can directly follow from (6) and (10), respectively. We implement all our proposed local solvers via the FIFO Queue data structure.

### FIFO-Queue and Priority-Queue

We examine the different data structures of local methods and explore two types: the First-In-First-Out (FIFO) Queue, which requires constant time \(\mathcal{O}(1)\) for updates, and the Priority Queue, which is used to implement the Gauss-Southwell algorithm as described in [48, 7]. We test the number of operations needed using these two data structures on five small graphs, including Cora\((n=19793,m=126842)\), Cora-ML\((n=2995,m=16316)\), Citeseer\((n=4230,m=10674)\), Dblp\((n=17716,m=105734)\), and Pubmed\((n=19717,m=88648)\) as used in [9] and downloaded from https://github.com/abojchevski/graph2gauss.

Figure 10 presents the ratio of the total number of operations needed by APPR-FIFO and APPR-Priority-Queue. We tried four different settings; the performance of one does not dominate the other, and the Priority Queue is suitable for some nodes but not all nodes.

Figure 10: The number of operations ratio between the APPR (FIFO-Queue) and Gauss-Southwell (Priority-Queue). The number of operations of APPR is defined as \(\mathcal{T}_{\text{FIFO-Queue}}=\sum_{t=0}^{T-1}\mathcal{S}_{t}\), \(\mathcal{T}_{\text{Priority-Queue}}=\sum_{k=0}^{K}\left(d_{u_{k}}+\log_{2}| \operatorname{supp}(\bm{r}^{(k)})|\right)\) where \(K\) is the total number of push operations used in Gauss-Southwell iteration and \(\operatorname{supp}(\bm{r}^{(k)})=\{u:r_{u}^{(k)}\neq 0,u\in\mathcal{V}\}\) and \(\log_{2}|\operatorname{supp}(\bm{r}^{(k)})|\) is the number of operations needed by the priority queue for maintaining all residuals in \(\bm{r}^{(k)}\).

[MISSING_PAGE_FAIL:27]

[MISSING_PAGE_FAIL:28]

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline \multirow{2}{*}{Graph} & \multirow{2}{*}{Vertices} & \multirow{2}{*}{Avg.Deg.} & \multicolumn{4}{c}{Participation Ratios} \\ \cline{5-7}  & & & Min & Mean & Median & Max \\ \hline Cora & 2708 & 3.90 & 1.01 & 7.62 & 3.34 & 30.24 \\ Citeseer & 3279 & 2.78 & 1.01 & 7.03 & 2.09 & 40.77 \\ pubmed & 19717 & 4.50 & 1.01 & 25.71 & 2.23 & 173.21 \\ ogbn-proteins & 132534 & 597.00 & 1.00 & 2181.89 & 2997.35 & 3397.70 \\ ogbn-arxiv & 169343 & 13.67 & 1.00 & 18.06 & 6.04 & 47.12 \\ reddit & 232965 & 491.99 & 1.00 & 3364.93 & 4412.49 & 4757.13 \\ ogbl-ppa & 576039 & 73.72 & 1.00 & 507.92 & 43.88 & 2984.20 \\ com-youtube & 1134890 & 5.27 & 1.00 & 10.86 & 8.65 & 42.78 \\ as-skitter & 1694616 & 13.09 & 1.00 & 52.56 & 5.07 & 702.30 \\ ogbn-mag & 1939743 & 21.75 & 1.00 & 8.24 & 6.71 & 182.96 \\ ogbn-products & 2385902 & 51.81 & 1.00 & 100.53 & 27.33 & 649.78 \\ wiki-talk & 2388953 & 3.90 & 1.00 & 404.15 & 587.18 & 701.41 \\ com-orkut & 3072441 & 76.28 & 1.00 & 637.67 & 50.15 & 1824.24 \\ cit-patent & 3764117 & 8.77 & 1.00 & 37.49 & 6.01 & 315.39 \\ soc-lj1 & 4843953 & 17.69 & 1.00 & 358.72 & 5.00 & 2122.33 \\ wiki-en21 & 6216199 & 51.74 & 1.00 & 110.88 & 156.60 & 189.79 \\ com-friendster & 65608366 & 55.06 & 1.00 & 8475.52 & 9.02 & 36776.54 \\ ogbn-papers100M & 111059433 & 29.10 & 1.00 & 97.53 & 7.93 & 726.71 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Unnormalized participation ratios for Katz(\(\alpha=1/(1+\|\bm{A}\|_{2})\))**

Figure 11: Comparison of running time (seconds) for CPU and GPU implementations for Katz.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline \multirow{2}{*}{Graph} & \multirow{2}{*}{Vertices} & \multirow{2}{*}{Avg.Deg.} & \multicolumn{4}{c}{Participation Ratios} \\ \cline{4-7}  & & & Min & Mean & Median & Max \\ \hline Cora & 2708 & 3.90 & 1.11 & 2.57 & 2.60 & 5.68 \\ Citeseer & 3279 & 2.78 & 1.14 & 2.76 & 2.63 & 5.22 \\ pubmed & 19717 & 4.50 & 1.04 & 2.10 & 2.12 & 4.19 \\ ogbn-proteins & 132534 & 597.00 & 1.00 & 1.26 & 1.01 & 2.01 \\ ogbn-arxiv & 169343 & 13.67 & 1.03 & 1.84 & 1.65 & 4.12 \\ reddit & 232965 & 491.99 & 1.00 & 1.27 & 1.03 & 2.25 \\ ogbl-ppa & 576039 & 73.72 & 1.00 & 1.34 & 1.06 & 2.88 \\ com-youtube & 1134890 & 5.27 & 1.01 & 1.85 & 1.99 & 3.39 \\ as-skitter & 1694616 & 13.09 & 1.04 & 2.02 & 2.19 & 4.99 \\ ogbn-mag & 1939743 & 21.75 & 1.01 & 1.70 & 1.57 & 2.84 \\ ogbn-products & 2385902 & 51.81 & 1.00 & 1.46 & 1.19 & 2.84 \\ wiki-talk & 2388953 & 3.90 & 1.00 & 1.61 & 1.68 & 2.87 \\ ogbn-papers100M & 3072441 & 76.28 & 1.00 & 1.29 & 1.05 & 2.19 \\ cit-patent & 3764117 & 8.77 & 1.01 & 1.67 & 1.55 & 2.99 \\ soc-lj1 & 4843953 & 17.69 & 1.00 & 1.68 & 1.73 & 3.73 \\ wiki-en21 & 6216199 & 51.74 & 1.00 & 1.34 & 1.11 & 2.22 \\ com-friendster & 65608366 & 55.06 & 1.00 & 1.46 & 1.20 & 2.23 \\ ogbn-papers100M & 111059433 & 29.10 & 1.00 & 1.45 & 1.19 & 2.15 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Unnormalized participation ratios for PPR (\(\alpha=0.1\))**Figure 12: Running time (seconds) as a function of \(\epsilon\) for PPR on several datasets with \(\alpha=0.1\).

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline \multirow{2}{*}{Graph} & \multirow{2}{*}{Vertices} & \multirow{2}{*}{Avg.Deg.} & \multicolumn{3}{c}{Participation Ratios} \\ \cline{5-8}  & & & Min & Mean & Median & Max \\ \hline cora & 2708 & 3.90 & 1.65 & 8.43 & 6.62 & 34.85 \\ citeseer & 3279 & 2.78 & 1.82 & 7.69 & 5.81 & 35.71 \\ pubmed & 19717 & 4.50 & 1.15 & 26.02 & 7.09 & 242.95 \\ ogbn-proteins & 132534 & 597.00 & 437.31 & 5743.38 & 5712.33 & 12874.41 \\ ogbn-arxiv & 169343 & 13.67 & 1.95 & 15.48 & 10.61 & 109.58 \\ reddit & 232965 & 491.99 & 30.26 & 2697.53 & 2468.45 & 5395.28 \\ ogbl-ppa & 576039 & 73.72 & 2.02 & 1052.49 & 766.77 & 3860.21 \\ com-youtube & 1134890 & 5.27 & 1.06 & 8.66 & 4.73 & 58.02 \\ as-skitter & 1694616 & 13.09 & 1.51 & 13.04 & 7.18 & 45.05 \\ ogbn-mag & 1939743 & 21.75 & 1.50 & 2.63 & 1.96 & 16.90 \\ ogbn-products & 2385902 & 51.81 & 2.16 & 124.51 & 72.87 & 775.54 \\ wiki-talk & 2388953 & 3.90 & 1.00 & 8.19 & 1.43 & 65.72 \\ com-orkut & 3072441 & 76.28 & 6.02 & 655.85 & 367.02 & 7942.79 \\ cit-patent & 3764117 & 8.77 & 1.76 & 30.51 & 21.03 & 273.80 \\ soc-lj1 & 4843953 & 17.69 & 2.29 & 59.08 & 24.23 & 573.48 \\ wiki-en21 & 6216199 & 51.74 & 3.00 & 34.05 & 32.49 & 164.69 \\ com-friendster & 65608366 & 55.06 & 2.05 & 323.12 & 83.67 & 3092.69 \\ ogbn-papers100M & 111059433 & 29.10 & 1.44 & 51.49 & 21.40 & 275.48 \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Unnormalized participation ratios for HK(\(\tau=10\))**Figure 14: Running time (seconds) as a function of \(\epsilon\) for HK on several datasets with \(\tau=10\).

Figure 13: Running time (seconds) as a function of \(\epsilon\) for Katz on several datasets with \(\alpha=1/(\|\bm{A}\|_{2}+1)\).

Figure 15: Running time (seconds) of SOR and LocalSOR as a function of \(\epsilon\) for PPR on the _wiki-talk_ dataset with different \(\alpha\).

### The efficiency of local methods on different types of graphs (tested on grid graphs).

Our local algorithms do not depend on graph types. However, our key assumption is that diffusion vectors are highly localizable, measured by the participation ratio. As demonstrated in Figure 1, almost all diffusion vectors have low participation ratios collected from 18 real-world graphs. These graphs are diverse, ranging from citation networks and social networks to gene structure graphs. To further illustrate this, we conducted experiments where the graphs are grid graphs, and the diffusion vectors have high participation ratios (about \(3.56\times 10^{-6}\), with a grid size of 1000x1000, i.e., \(10^{6}\) nodes and 1,998,000 edges). We set \(\alpha_{PPR}=0.1\). The figure below presents the running time as a function of \(\epsilon\) over a grid graph with 50 sampled source nodes. The results indicate that local solvers are more efficient than global ones even when \(\epsilon\) is very small.

Figure 16: Accumulated number of operations on some dynamic graphs.

Figure 17: The performance of InstantGNN using SOR and GS propagation on ogbn-products.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect our contributions and scope. We introduced a novel framework for approximately solving graph diffusion equations (GDEs) using a local diffusion process. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed several limitations of our proposed work in the last section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The paper thoroughly presents theoretical results, including the full set of assumptions necessary for each theorem and lemma. Each proof is detailed and follows logically from the stated assumptions, ensuring correctness. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide comprehensive details on the experimental setup, including descriptions of the datasets used, parameter settings, and the specific algorithms applied. We also provided our code for review and will make it public upon publication. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide open access to the data and code, along with instructions in the supplemental material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We follow the existing experimental setup and provide detailed information on the training and testing settings used for InstantGNN with LocalSOR. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper reports error bars and other relevant information about the statistical significance of the experiments. We use 50 randomly sampled nodes and plot the mean and standard deviation of the results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The experiments were conducted using Python 3.10 with CuPy and Numba libraries on a server with 80 cores, 256GB of memory, and two NVIDIA-4090 GPUs with 28GB each. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research adheres to all aspects of the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There are no broader societal impacts, as it focuses on technical contributions. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not involve data or models with a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not involve licenses for existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not involve new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing and research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve institutional review board (IRB) approvals or equivalent for research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.