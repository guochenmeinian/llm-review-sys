# Alignment with human representations supports

robust few-shot learning

 Ilia Sucholutsky

Department of Computer Science

Princeton University

is2961@princeton.edu

&Thomas L. Griffiths

Departments of Psychology and Computer Science

Princeton University

tomg@princeton.edu

###### Abstract

Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both natural adversarial attacks and domain shifts. Our results suggest that human alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.

## 1 Introduction

As AI systems are increasingly deployed in settings that involve interactions with humans, exploring the extent to which these systems are aligned with humans becomes more significant. While this exploration has largely focused on the alignment of the _values_ of AI systems with humans [7, 16], the alignment of their _representations_ is also important. Representing the world in the same way is a precursor to being able to express common values and to comprehensible generalization. To the extent that humans have accurate representations of the world, representational alignment is also an effective source of inductive bias that might make it possible to learn from limited data.

As a motivating example, imagine a meeting between a 16th century alchemist and a 21st century chemist. They live in the same physical world and are intimately familiar with the materials that comprise it, but they would have significant difficulty expressing their values and generalizing the results of an experiment they observe together. The alchemist would likely learn poorly from examples of a reaction demonstrated by the chemist, not having the right inductive biases for the way the world actually works. The alchemist and the chemist lack representational alignment - they represent the world in fundamentally different ways - and this impedes generalization and learning.

In this paper, we provide a theoretical and empirical investigation of the consequences of representational alignment with humans for AI systems, using popular computer vision tasks as a way to study this phenomenon. We define representational alignment as the degree to which the latent representations of a model match the latent representations of humans for the same set of stimuli, and refer to models that are representationally aligned with humans as being "human-aligned." Several recent papers have proposed ways to measure [23, 24], explain [26, 20], and even improve [29, 6] the representational alignment of models. However, many models that score low on these alignment metrics still have high performance on downstream tasks like image classification [20, 26, 6].

So, are there any real advantages (or disadvantages) to using human-aligned models? To answer this question, we develop an information-theoretic framework for analyzing representational alignment. This framework enables us to make predictions about emergent behavior in human-aligned models.

In particular, our framework predicts that few-shot transfer learning performance should have a U-shaped relationship with alignment. To verify the predictions of our framework and to probe for additional properties that arise as a consequence of alignment, we conduct a series of experiments in which we assess the downstream properties of human-aligned models compared to their non-aligned counterparts. From our experiments comparing 491 large computer-vision models to over 425,000 human judgments (across 1200 participants), we identify three properties:

* Models with either high or low alignment with humans are better at few-shot learning (FSL) than models with medium alignment, even correcting for pre-training performance.
* Human-aligned models are more robust to natural adversarial examples, even when correcting for classification performance on the pre-training dataset.
* Human-aligned models are more robust to domain shift, even when correcting for classification performance on the pre-training dataset.

The U-shaped relationship between alignment and few-shot learning helps to explain why previous results have not consistently observed benefits of representational alignment. Overall, our results suggest that representational alignment can provide real and significant benefits for downstream tasks, but that it may be a sufficient rather than necessary condition for these benefits to emerge.

Figure 1: Schematic of representational alignment between two agents, Alice and Bob. **A**: Shared data (\(x\)) is shown to both agents (images are from the animal subset of the similarity datasets from Peterson et al. [29]). **B**: Both agents form representations (\(f_{A}(x)\) and \(f_{B}(x)\)) of the objects they observe. **C**: Agents are asked to produce pairwise similarity matrices corresponding to their representations. The similarity judgments can then be compared to measure alignment between the agents.

Figure 2: Schematic of triplet-based supervised learning where one agent is a teacher and the other a student. **A**: A new object is shown only to the Teacher. **B**: The Teacher forms a representation of the object and sends the Student a triplet relating the new object to two objects previously observed by both agents. **C**: The Student interprets the triplet in their own representation space and eliminates the half-plane where the new object cannot be located (shaded in red) according to the triplet.

Related Work

With AI systems entering mainstream usage, aligning these models with human values and understanding is increasingly important [7; 16]. This concept, referred to as AI alignment, is an important but still largely open problem [43], in part due to the difficulty of formalizing it [34], or even reaching consensus on a definition [16]. In this paper, we focus on formalizing a specific aspect of AI alignment: representational alignment.

Representational alignment is a measure of agreement between the representations of two learning agents (one of whom is typically a human). There are numerous names, definitions, measures, and uses of this form of alignment across various fields, including cognitive science, neuroscience, and machine learning. Some of the other names include latent space alignment [38], concept(ual) alignment [35; 26], system alignment [8; 30; 1], representational similarity analysis (RSA) [17], and model alignment [24]. Shepard [32] proposed that human representations can be recovered by using behavioral data to measure the similarity of a set of stimuli and then finding embeddings that satisfy those similarity associations using methods like multi-dimensional scaling (MDS). Similarly, in neuroscience, Representational Similarity Analysis (RSA) is a popular technique for relating neural, behavioral, and computational representations of the same set of stimuli via similarity analysis [17]. While similarity analysis has clearly proven itself to be a powerful method, exhaustively collecting pairwise similarity judgments is expensive (\(O(N^{2})\) judgments for \(N\) stimuli) and there have been numerous proposals aiming to develop more efficient methods of recovering human representations.

Jamieson and Nowak [15] proposed an active learning scheme for querying for human judgments using triplets of the form "is \(a\) closer to \(b\) than to \(c\)?" and derived bounds on the number of required queries for lossless completion of the full similarity matrix using such queries. When an approximate solution is acceptable, Peterson et al. [29] showed that pre-trained computer vision models can be used to approximate human perceptual similarity judgments over images. Marjieh et al. [23] showed that human perceptual similarity can be more accurately, but still efficiently, approximated from natural language descriptions of the stimuli of interest (for example by using large language models to estimate similarity over pairs of these descriptions). Marjieh et al. [24] extended this result to more domains (vision, audio, and video) and measured alignment for hundreds of pre-trained models.

Several recent studies have also attempted to identify what design choices lead to improved representational alignment in models [20; 26; 6], although Moschella et al. [25] found that even with variation in design choices, many models trained on the same dataset end up learning similar'relative representations' (embeddings projected into a relational form like a similarity matrix), or in other words, converge to the same representational space. Tucker et al. [38] showed that representational alignment emerges not only in static settings like image classification, but also dynamic reinforcement learning tasks involving human-robot interaction. Several studies have also focused on studying alignment specifically in humans, both between different people and for a single person but across multiple tasks and domains [8; 30].

Although several recent papers have proposed ways to measure [23; 24], explain [26; 20], and even improve [29; 6] the representational alignment of models, few have focused on studying the downstream impact of a model being representationally aligned with humans, and many studies simply rely on the intuition that better alignment leads to better performance to justify pursuing increased alignment. While there is recent evidence to suggest that alignment may help humans learn across domains and perform zero-shot generalization [1], there is also evidence to suggest that alignment may not always be beneficial for models, with models scoring low on alignment metrics achieving higher performance on downstream tasks like image classification [20; 26; 6]. Our goal in this paper is to conduct an in-depth study into the downstream effects of representational alignment. Our theoretical and empirical results both validate and explain the apparently conflicting results seen in previous literature as special cases of the (more complex than previously suspected) effects of representational alignment.

## 3 Theory

If we want to measure representational alignment across different architectures with potentially mismatched embedding coordinate spaces and dimensionalities, then we need a definition of representation spaces that enables comparisons of such spaces. Inspired by the cognitive science literature on using non-metric similarity triplets to recover hidden psychological representations, and building on rigorous computational theory that analyzes such triplets [15], we propose a triplet-based definition of representation learning. We summarize representational alignment under our framework in the schematic shown in Figure 1.

**Definition 3.1**.: As proposed by Jamieson and Nowak [15], for an ordered set of \(n\) objects in \(d\) dimensions represented as a vector \(x\in\mathds{R}^{nd}\), a **similarity triplet** corresponding to the question 'is \(x_{i}\) closer to \(x_{j}\) than to \(x_{k}\)?' is a membership query of the form \(1_{x\in r_{ijk}}\) where \(r_{ijk}=\{x\in\mathds{R}^{nd}:|x_{i}-x_{j}|<|x_{i}-x_{k}|\}\).

**Definition 3.2**.: For an ordered set of objects \(x\in\mathds{R}^{nd}\) and model \(M\) with embeddings \(f_{M}:\mathds{R}^{d}\rightarrow\mathds{R}^{d_{M}}\), the **triplet-based representation space** of \(M\) is the set of all triplets \(S_{M}(x)=\{(i,j,k)\in\{1,...,n\}^{3}:1_{x\in r_{ijk}^{M}},i\neq j,j\neq k,k \neq i\}\) where \(r_{ijk}^{M}=\{x\in\mathds{R}^{nd}:|f_{M}(x_{i})-f_{M}(x_{j})|<|f_{M}(x_{i})-f_ {M}(x_{k})|\}\).

_Remark 3.3_.: Since switching the order of \(j\) and \(k\) in a triplet deterministically flips the query between 0 and 1, it is easy to see that, for a set of \(n\) objects, a representation space is determined by a set of just \(\frac{n(n-1)(n-2)}{2}\) unique triplets. Thus, for a set of \(n\) objects, \(S_{M}(x)\) can be represented as a binary vector of length \(\frac{n(n-1)(n-2)}{2}\) and we use this interpretation in the remainder of this section.

**Definition 3.4**.: Consider a training set \(x\in\mathds{R}^{nd}\) and a corresponding target representation space \(S_{T}(x)\). We define **representation learning** with a parametrized model \(M_{\theta}\) as optimizing the objective \(\min_{\theta}\ell(S_{T}(x),S_{M_{\theta}}(x))\) where \(\ell\) is a loss function penalizing divergence between the target and learned representation spaces.

Sucholutsky et al. [36] showed that many ML objectives and associated supervision signals, ranging from similarity judgments for contrastive learning to labels for classification, can be converted into similarity triplets making them compatible with this definition of representation learning. We also immediately get an intuitive definition of alignment between two agents (e.g. a model and a person) as the rate of agreement on matched triplets (also known as simple match coefficient).

**Definition 3.5**.: Consider a dataset \(x\in\mathds{R}^{nd}\) and two agents, \(A\) and \(B\), with corresponding representation spaces \(S_{A}(x)\) and \(S_{B}(x)\). We define **representational misalignment** between \(A\) and \(B\) as \(D_{R}(A,B;x)=\frac{||S_{A}(x)-S_{B}(x)||_{1}}{n(n-1)(n-2)/2}\). **Representational alignment** is then simply \(1-D_{R}(A,B;x)\).

**Definition 3.6**.: Consider a single set of three points \(t=(x_{i},x_{j},x_{k})\in X\subseteq\mathds{R}^{3d}\) sampled uniformly at random, and two agents, \(A\) and \(B\). We define **stochastic representational misalignment** between \(A\) and \(B\) as \(D_{P}(A,B;X)=P(1_{x\in r_{ijk}^{A}}=1_{x\in r_{ijk}^{B}})\). **Stochastic representational alignment** is then simply \(1-D_{P}(A,B;X)\).

Since each triplet corresponds to one bit of information [15], and we have a probabilistic definition of alignment, we can now use information theory to define what it means for one agent to supervise another (potentially misaligned) agent. We visualize supervised learning under our framework in Figure 2. For perfectly aligned models, Jamieson and Nowak [15] derive the following result.

**Lemma 3.7**.: _Consider input space \(X\subseteq\mathds{R}^{nd}\), shared data \(x\sim X\), new object \(c\in\mathbb{R}^{d}\), and models \(A\) and \(B\) with \(D_{P}(A,B;X)=0\). \(\Omega(d\log(n))\) triplet queries are required for \(B\) to identify the location of \(c\) relative to all objects in \(x\)._

**Proposition 3.8**.: _For input space \(X\subseteq\mathds{R}^{nd}\), shared data \(x\sim X\), new object \(c\in\mathbb{R}^{d}\), and models \(A,B\) with \(D_{P}(A,B;X)=\epsilon\), \(\Omega(\frac{d\log(n)}{1+\epsilon\log(\epsilon)+(1-\epsilon)\log(1-\epsilon)})\) triplets are required for \(B\) to learn \(c\)._

Proof.: Consider a communication game involving Alice and Bob, a dataset \(x\) that they both have their own representations for (we call this'shared data'), and a new object \(c\) that only Alice can see. Alice can only communicate with Bob by giving binary responses to triplet queries based on Alice's own representations of the objects. The goal is for Bob to learn the location of \(c\) with respect to the other objects in \(x\). \(D_{P}(A,B;X)=\epsilon\) is the probability for any triplet drawn from \(x\cup c\) to be flipped in Bob's representation space relative to Alice's. This is equivalent to Alice and Bob communicating over a binary symmetric channel where the probability of a bit flip is \(\epsilon\). The capacity of this channel is \(C_{\epsilon}=1-H(\epsilon,1-\epsilon)=1+\epsilon\log(\epsilon)+(1-\epsilon) \log(1-\epsilon)\). For error-free communication over this channel, the highest achievable rate is \(R<C_{\epsilon}\), meaning at least \(\frac{1}{C_{\epsilon}}\) bits are required to communicate each useful bit over this channel. By Lemma 3.7, \(\Omega(d\log(n))\) useful bits are required for Bob to learn the location of \(c\) over a channel with no error, and thus a total of \(\Omega(\frac{d\log(n)}{C_{\epsilon}})\) bits are required.

**Theorem 3.9** (Few-shot learning).: _Consider input space \(X\subseteq\mathbb{R}^{nd}\), shared data \(x\sim X\), new objects \(c\in\mathbb{R}^{md}\) and three models \(A\), \(B_{1}\), and \(B_{2}\) with \(D_{P}(A,B_{1};X)=\epsilon_{B_{1}}\) and \(D_{P}(A,B_{2};X)=\epsilon_{B_{2}}\). If \(|0.5-\epsilon_{B_{1}}|<|0.5-\epsilon_{B_{2}}|\), then \(B_{2}\) requires fewer queries to learn \(c\) than \(B_{1}\) does._

Proof.: From Proposition 3.8, it immediately follows that \(B_{1}\) requires \(\Omega(\frac{md\log(n)}{C_{\epsilon_{B_{1}}}})\) queries and \(B_{2}\) requires \(\Omega(\frac{md\log(n)}{C_{\epsilon_{B_{2}}}})\) queries. \(C_{\epsilon}:[0,1]\rightarrow[0,1]\) is a symmetric convex function with a minimum of 0 at \(\epsilon=0.5\) and maxima of 1 at \(\epsilon=0,1\). Thus, if \(|0.5-\epsilon_{B_{1}}|<|0.5-\epsilon_{B_{2}}|\), then \(\Omega(\frac{md\log(n)}{C_{\epsilon_{B_{1}}}})>\Omega(\frac{md\log(n)}{C_{ \epsilon_{B_{2}}}})\). 

Suchoutsky et al. [36] showed that commonly used supervision signals, like hard and soft classification labels, can be converted into sets of triplets. Reducing the number of required triplets to learn the location of a new object is equivalent to reducing the number of required labels. **It follows from Theorem 3.9 that high or low alignment leads to high few-shot learning performance on downstream tasks like classification** (where learning a new class can be formulated as learning the location of the centroid or boundaries of that class using a small number of labels), **while medium alignment leads to low few-shot learning performance**. Intuitively, this comes as a result of mutual information between the teacher and student representations being minimized when alignment is at \(0.5\). In other words, if Bob knows that most bits coming from Alice are flipped (i.e. \(D_{P}(A,B)=\epsilon>0.5\)), then going forward, Bob can flip all the bits coming from Alice to achieve a lower error rate (i.e. \(D_{P}(\bar{A},B)=1-\epsilon<0.5\)).

We note that generalizing under domain-shift can be considered a form of zero-shot transfer learning and selecting adversarial examples can be seen as selecting objects that maximize representational disagreement between a model and a human. Under these interpretations, our framework also predicts that highly-aligned models are robust to both domain shift and adversarial examples though the framing in these cases is less intuitive than for the few-shot learning case. We share some preliminary theoretical analyses of robustness properties in the Supplement.

## 4 Experiments

Our theoretical analysis shows how representational alignment can serve as a source of inductive bias that reduces the number of bits that need to be acquired from data. In particular, Theorem 3.9 predicts an unexpected U-shaped relationship between alignment and few-shot learning performance. We now present a series of experiments designed to test this prediction and examine whether it extends to robustness to natural adversarial examples and domain shift.

**Models.** The pre-trained models evaluated in this paper are taken from the PyTorch Image Models package [41]. The full list of 491 models used in our experiments can be found in the Supplement.

**Data.** All of the models used in this paper were pre-trained on ImageNet-1k [31] and had their performance evaluated on the ImageNet-1k validation set. Adversarial robustness was measured on the ImageNet-A dataset of natural adversarial examples that have been found to foil most ImageNet models [11]. Zero-shot domain shift robustness was measured on the ImageNet-R dataset of renditions (e.g., paintings, toys, statues, etc.) of ImageNet classes [10] and the ImageNet-Sketch dataset of black-and-white sketches of the ImageNet classes [40]. All ImageNet and ImageNet variant results come from the PyTorch Image Models package [41]. Few-shot transfer learning performance was evaluated on the CIFAR100 [18] test set with \(n\in\{1,5,10,20,40,80\}\) examples per class used for few-shot learning and the remaining examples used for evaluation. We measure alignment by computing the three metrics described below on the six image datasets from Peterson et al. [29] and their respective sets of similarity judgments consisting of over 425,000 judgments across 1200 participants, as described by Marjieh et al. [23]. We average each alignment metric over the six datasets.

**Alignment metrics.** The representational alignment metrics we consider are correlation over pairwise similarity judgments and agreement between similarity triplets (i.e., the proportion of triplets that have the same response for the matched sets of representations). Similarity triplets provide non-metric information since in each query we discard the actual pairwise distances and only retain which one is larger. Analogously, in the pairwise case, if we use Spearman rank correlation then we are comparingthe relative magnitudes of pairwise distances (of which the triplets form a subset) and discarding the magnitudes. On the other hand, Pearson correlation over pairwise similarities takes into account the magnitudes of the pairwise similarity judgments which, if accurate, can provide more information about fine-grained differences that affect alignment. We use all three metrics in our analyses and refer to them as Spearman pairwise alignment, Pearson pairwise alignment, and triplet alignment.

**Few-shot learning.** We measure few-shot transfer learning performance using three methods: linear probing and two classifier heads. For linear probing, we take the embeddings coming from the penultimate layer of a model and fit a logistic regression using'scikit-learn' [28]. For the classifier heads, we use a one- and two-hidden layer neural network implemented in PyTorch [27], both with an input dropout rate of 0.8 and the latter with a ReLU hidden layer. Recent work [19] has shown that linear probing usually performs on par with fine-tuning and is a more accurate measure of the quality of internal representations that fine-tuning distorts. The heads are provided in the supplement and fixed hyperparameter values (selected based on exploratory experiments to confirm stable convergence) were used for every dataset/model for consistency.

**Correcting correlation for ImageNet performance.** For each of our experiments, ImageNet-1k performance can be a confounding variable when trying to understand the relationship between alignment and a downstream property. To account for this, when measuring correlation between alignment and other properties, we compute partial correlation with ImageNet-1k Top-1 validation accuracy as a covariate using the 'Pingouin' Python package [39].

### Results

**Alignment predicts few-shot learning performance.** For each alignment metric, we compare the \(n\)-shot transfer learning performance of the five models with the highest alignment, the five models with the lowest alignment, and the five models with the nearest-to-the-mean alignment as shown in Figure 3. We find that according to all three alignment metrics, the most aligned models have far better \(n\)-shot learning performance at all levels of \(n\).

**Alignment predicts adversarial robustness.** For each alignment metric, we also compare the performance on natural adversarial examples of the five models with the highest alignment, the five models with the lowest alignment, and the five models with the nearest-to-the-mean alignment as seen in Table 1. We again find that according to all three alignment metrics, the most aligned models perform far better on both Top-1 and Top-5 predictions for ImageNet-A.

**Alignment predicts domain-shift robustness.** Finally, for each alignment metric, we also compare the domain-shift robustness of the five models with the highest alignment, the five models with the

Figure 3: **Left**: Average \(n\)-shot transfer learning performance using linear probing, a 1-layer classification head, and a 2-layer classification head on CIFAR100 of the five models with highest, lowest, and closest-to-the-mean levels for each of Pearson (\(\rho_{P}\)), Spearman (\(\rho_{S}\)), and triplet alignment. **Right**: Pearson correlation (with 95% confidence intervals) between \(n\)-shot transfer learning performance using each classification head on CIFAR100 and each \(z^{2}\) alignment metric.

[MISSING_PAGE_FAIL:7]

Interactions of architecture and alignment.The results above suggest that when alignment is measured on near-distribution datasets, it can predict about \(20\%\) of the variance in one-shot learning performance. This is impressive given the large number of factors we know have an impact on downstream performance (architecture, optimizer, random seed, initialization, early termination, hardware effects on stochasticity, and many other hyperparameters). We also investigated whether some of these factors can predict alignment or interact with it to affect its relationship with downstream performance. Going through the results of our 491 models (see the Supplemental Information for the full results), we find the following insights ranked from most supporting evidence to least: 1) knowledge distillation[12] seems to increase both alignment and few-shot learning performance; 2) increasing depth/size tends to increase at least one of alignment and FSL performance (increasing ResNet[9] depth improves alignment but not FSL performance increasing depth for Resnet-RS[3] increases few-shot learning performance and not alignment; increasing Swin Transformer [22] size increases both); 3) pre-training on ImageNet21k/22k before fine-tuning on ImageNet1k increases both alignment and few-shot learning performance; 4) batch-normalization [14] for VGG [33] leads to lower alignment but slightly higher FSL performance; 5) pruning EfficientNet [37] seems to slightly decrease alignment but maintain FSL performance. We also find that for many architectures, the models cluster together (i.e. almost all MobileNets [13] have low-to-medium alignment and low FSL performance, almost all NFNets [4] have high alignment and medium-to-high FSL performance, all BEiT models [2] have very low alignment and high FSL performance). These findings suggest that architecture and training decisions can impact alignment as well as its effect on downstream properties. Thus researchers may be able to directly optimize for alignment when selecting, designing, or optimizing model architectures.

## 5 Discussion

Our experimental results confirm our theoretical predictions - not only do highly-aligned models exhibit better few-shot learning and robustness properties than models with lower alignment, but we also observe the U-shaped relationship with alignment predicted in Theorem 3.9. We now dissect the theoretical and empirical results via three key questions.

**Which alignment metric should we be using?** We find that Spearman pairwise alignment is almost perfectly correlated with triplet alignment (\(\rho=0.992\)). This suggests that triplets, though only a small fraction of the full set of quadruplets (i.e., all queries of the form "X is closer to Y than A is to B")., already capture the majority of the non-metric information contained in the entire set of quadruplets. Pearson pairwise alignment has a strong, though not quite as strong, correlation with triplet alignment (\(\rho=0.824\)). While all three metrics have statistically significant correlations with the downstream properties we care about, Pearson pairwise alignment seems to have the strongest correlations. This suggests that there is recoverable metric information about representations encoded in the magnitudes of human similarity judgments, even when these judgments are potentially noisy due to being elicited

Figure 4: **Left: Comparing 1-shot learning performance on CIFAR100 to Pearson pairwise alignment for all 491 models. Right: Pearson correlation (with 95% confidence intervals) between \(n\)-shot transfer learning performance using linear probing and the Pearson (\(\rho_{P}\)) \(z^{2}\) alignment metric for CIFAR100, FMNIST, and MNIST.**

without anchor stimuli that would ground the scale. The information-theoretic representation learning framework would need to be extended in future work to quantify this additional information.

**Which positive downstream properties do human-aligned models exhibit?** As predicted by our information-theoretic representation learning framework, our experiments suggest that very human-aligned models are better at few-shot transfer learning, more robust to natural adversarial examples, and more robust to test-time domain shift than models with lower degrees of alignment. The correlations between alignment and each downstream property were positive and statistically significant and, in every experiment we conducted, the models with the highest level of alignment outperformed the other models. These results seem to confirm the intuition that human alignment is useful in tasks where we want to use human supervision to elicit human-like behavior. While the models with the highest level of alignment clearly exhibit the best downstream performance across all three sets of tasks, our results suggest an additional unexpected insight: there is a U-shaped relationship between alignment and two of the properties we test: robustness to domain shift and few-shot transfer learning performance. Thus, while a high degree of alignment may be sufficient for eliciting desirable properties in models, it does not appear to be necessary. In fact, in cases where achieving high alignment is impractical (e.g., due to limitations on human-labeled data), it is possible that better results may be achieved by avoiding alignment altogether.

**Are there downsides and limitations to representational alignment?** We have shown that alignment is not a domain-agnostic metric and that its downstream effects are strongest when evaluated on in-distribution datasets. It is also clear that increasing alignment can damage performance across multiple criteria when that increase moves the alignment level into the medium range. But are there downsides to increasing alignment of models that are already at or past that range? Throughout this study, one of our key assumptions was that the task being solved is designed and specified by humans, or at least easily solvable by humans. However, there are numerous domains where humans have poor performance or where our representations of the problem or stimuli are not helpful for solving the task and a different set of inductive biases are required. For example, many domains targeted by deep learning - such as protein folding, drug design, and social network analysis - require geometric inductive biases [5]. In these cases, the goal should be to achieve alignment with the underlying laws governing the system of interest (e.g., physical forces or mathematical laws), rather than with humans. Finally, we note that as in all cases where models are trained with human data, social biases found in the human data may be reflected or even amplified in representationally-aligned models.

## 6 Conclusion

Our findings confirm the intuition that representational alignment with humans elicits desirable human-like traits in models, including the ability to generalize from small data and lower susceptibility to adversarial attacks and domain shift. However, as both our theory and experiments suggest, increasing alignment is not always desirable, first due to a U-shaped relationship between alignment and desirable traits, and second, because there are domains where human representations simply are not useful. Notably, our discovery of the U-shaped relationship serves to resolve the tension between previously conflicting findings regarding whether alignment improves performance. We hope that our framework and results motivate further study into both the positive and negative consequences of aligning models with humans across diverse domains. We believe that representational alignment is a quantifiable and tangible route for making progress on the general AI alignment problem by allowing us to measure agreement between models and humans even over abstract domains.

**Limitations and broader impact.** We discussed the limitations and potential negative impacts of _alignment_ in Section 5, but we also want to note the potential limitations and impacts of our _study of alignment_. First, we note that our approach assumes access to internal representations of models, but many new models today are being released via closed-source APIs that provide access only to model outputs. We hope that this and future studies on representational alignment will encourage developers to provide more open access to model internals. Second, we note that our experiments only evaluated alignment with the aggregate representational space of a large group of people which may not actually be individually representative of any of those people. Our proposed theory allows for alignment with either individuals or groups, but it is difficult to collect a large set of similarity judgments from individual participants. To ensure that we can understand whose voices, biases, and beliefs our models are actually aligned with, we believe that finding a way to account for individual differences is an important future direction.

## Acknowledgements

We would like to thank Lukas Mutenthaler for excellent discussions that helped shape some of the ideas explored in this paper. This work was supported by an ONR grant (N00014-18-1-2873) to TLG and an NSERC fellowship (567554-2022) to IS.

## References

* Aho et al. [2022] K. Aho, B. D. Roads, and B. C. Love. System alignment supports cross-domain learning and zero-shot generalisation. _Cognition_, 227:105200, 2022. ISSN 0010-0277. doi: https://doi.org/10.1016j.cognition.2022.105200.
* Bao et al. [2022] H. Bao, L. Dong, S. Piao, and F. Wei. BEit: BERT pre-training of image transformers. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=p-BhZS25904.
* Bello et al. [2021] I. Bello, W. Fedus, X. Du, E. D. Cubuk, A. Srinivas, T.-Y. Lin, J. Shlens, and B. Zoph. Revisiting resnets: Improved training and scaling strategies. _Advances in Neural Information Processing Systems_, 34:22614-22627, 2021.
* Brock et al. [2021] A. Brock, S. De, S. L. Smith, and K. Simonyan. High-performance large-scale image recognition without normalization. In _International Conference on Machine Learning_, pages 1059-1071. PMLR, 2021.
* Bronstein et al. [2021] M. M. Bronstein, J. Bruna, T. Cohen, and P. Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _arXiv preprint arXiv:2104.13478_, 2021.
* Fel et al. [2022] T. Fel, I. Felipe, D. Linsley, and T. Serre. Harmonizing the object recognition strategies of deep neural networks with humans. _arXiv preprint arXiv:2211.04533_, 2022.
* Gabriel [2020] I. Gabriel. Artificial intelligence, values, and alignment. _Minds and machines_, 30(3):411-437, 2020.
* Goldstone and Rogosky [2002] R. L. Goldstone and B. J. Rogosky. Using relations within conceptual systems to translate across conceptual systems. _Cognition_, 84(3):295-320, 2002. ISSN 0010-0277. doi: https://doi.org/10.1016/S0010-0277(02)00053-7.
* He et al. [2016] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Hendrycks et al. [2021] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli, M. Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8340-8349, 2021.
* Hendrycks et al. [2021] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15262-15271, 2021.
* Hinton et al. [2015] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* Howard et al. [2017] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. _arXiv preprint arXiv:1704.04861_, 2017.
* Ioffe and Szegedy [2015] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning_, pages 448-456. pmlr, 2015.
* Jamieson and Nowak [2011] K. G. Jamieson and R. D. Nowak. Low-dimensional embedding using adaptively selected ordinal data. _2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 1077-1084, 2011.
* Kirchner et al. [2022] J. H. Kirchner, L. Smith, J. Thibodeau, K. McDonell, and L. Reynolds. Researching alignment research: Unsupervised analysis. _arXiv preprint arXiv:2206.02841_, 2022.

- connecting the branches of systems neuroscience. _Frontiers in Systems Neuroscience_, 2, 2008. ISSN 1662-5137. doi: 10.3389/neuro.06.004.2008.
* Krizhevsky et al. [2009] A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Kumar et al. [2022] A. Kumar, A. Raghunathan, R. M. Jones, T. Ma, and P. Liang. Fine-tuning can distort pre-trained features and underperform out-of-distribution. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=UYneFzXSJWh.
* Kumar et al. [2022] M. Kumar, N. Houlsby, N. Kalchbrenner, and E. D. Cubuk. Do better ImageNet classifiers assess perceptual similarity better? _Transactions of Machine Learning Research_, 2022.
* Lecun et al. [1998] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
* Liu et al. [2021] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* Marjieh et al. [2022] R. Marjieh, I. Sucholutsky, T. R. Sumers, N. Jacoby, and T. L. Griffiths. Predicting human similarity judgments using large language models. _arXiv preprint arXiv:2202.04728_, 2022.
* Marjieh et al. [2022] R. Marjieh, P. van Rijn, I. Sucholutsky, T. R. Sumers, H. Lee, T. L. Griffiths, and N. Jacoby. Words are all you need? Capturing human sensory similarity with textual descriptors. _arXiv preprint arXiv:2206.04105_, 2022.
* Moschella et al. [2022] L. Moschella, V. Maiorca, M. Fumero, A. Norelli, F. Locatello, and E. Rodola. Relative representations enable zero-shot latent space communication. _arXiv preprint arXiv:2209.15430_, 2022.
* Mutenthaler et al. [2022] L. Mutenthaler, J. Dippel, L. Linhardt, R. A. Vandermeulen, and S. Kornblith. Human alignment of neural network representations. _arXiv preprint arXiv:2211.01201_, 2022.
* Paszke et al. [2019] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Inc., 2019.
* Pedregosa et al. [2011] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* Peterson et al. [2018] J. C. Peterson, J. T. Abbott, and T. L. Griffiths. Evaluating (and improving) the correspondence between deep neural networks and human representations. _Cognitive science_, 42(8):2648-2669, 2018.
* Roads and Love [2020] B. D. Roads and B. C. Love. Learning as the unsupervised alignment of conceptual systems. _Nature Machine Intelligence_, 2(1):76-82, 2020.
* Russakovsky et al. [2015] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. ImageNet large scale visual recognition challenge. _International journal of computer vision_, 115(3):211-252, 2015.
* Shepard [1980] R. N. Shepard. Multidimensional scaling, tree-fitting, and clustering. _Science_, 210(4468):390-398, 1980.
* Simonyan and Zisserman [2014] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* Soares and Fallenstein [2014] N. Soares and B. Fallenstein. Aligning superintelligence with human interests: A technical research agenda. _Machine Intelligence Research Institute (MIRI) technical report_, 8, 2014.
* Stolk et al. [2016] A. Stolk, L. Verhagen, and I. Toni. Conceptual alignment: How brains achieve mutual understanding. _Trends in Cognitive Sciences_, 20(3):180-191, 2016. ISSN 1364-6613. doi: https://doi.org/10.1016/j.tics.2015.11.007.
* Sucholutsky et al. [2022] I. Sucholutsky, R. Marjieh, N. Jacoby, and T. L. Griffiths. On the informativeness of supervision signals. _arXiv preprint arXiv:2211.01407_, 2022.

* [37] M. Tan and Q. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _International conference on machine learning_, pages 6105-6114. PMLR, 2019.
* [38] M. Tucker, Y. Zhou, and J. A. Shah. Latent space alignment using adversarially guided self-play. _International Journal of Human-Computer Interaction_, 38(18-20):1753-1771, 2022.
* [39] R. Vallat. Pingouin: Statistics in Python. _Journal of Open Source Software_, 3(31):1026, 2018. doi: 10.21105/joss.01026.
* [40] H. Wang, S. Ge, Z. Lipton, and E. P. Xing. Learning robust global representations by penalizing local predictive power. _Advances in Neural Information Processing Systems_, 32, 2019.
* [41] R. Wightman. PyTorch Image Models. https://github.com/rwightman/pytorch-image-models, 2019.
* [42] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
* [43] E. Yudkowsky. The AI alignment problem: why it is hard, and where to start. _Symbolic Systems Distinguished Speaker_, 2016.

Additional theoretical results

**Definition A.1**.: Consider model \(A\) with input space \(X\subseteq\mathbbm{R}^{nd}\), previously observed data \(x\sim X\), and \(k\) class centroids \(c\in\mathbbm{R}^{kd}\) learned by \(A\). We define **domain shift** as an update to the class centroids \(c\to c^{*}\in\mathbbm{R}^{kd}\). **Domain shift sensitivity** is then the proportion of triplets flipped as a result of this update.

\[\sigma_{A}(c,c^{*}):=E[\frac{||S_{A}(x;c)-S_{A}(x;c^{*})||_{1}}{|S_{A}(x;c)|}]\]

From this definition and Theorem 3.9, it immediately follows that sensitivity to domain shift should have the same U-shaped relationship with alignment that few-shot learning does in cases where the teacher model is robust to domain shift.

**Corollary A.2**.: _(**Alignment and domain-shift robustness)**. Consider input space \(X\subseteq\mathbbm{R}^{nd}\), shared data \(x\sim X\), and three models, \(A\), \(B_{1}\), and \(B_{2}\) with \(D_{P}(A,B_{1};X)=\epsilon_{B_{1}}\) and \(D_{P}(A,B_{2};X)=\epsilon_{B_{2}}\). Let \(c\in\mathbbm{R}^{kd}\) be \(k\) class centroids learned by \(A\), \(B_{1}\) and \(B_{2}\). If \(\sigma_{A}(c,c^{*})=0\) and \(|0.5-\epsilon_{B_{1}}|<|0.5-\epsilon_{B_{2}}|\), then \(\sigma_{B_{1}}(c,c^{*})<\sigma_{B_{2}}(c,c^{*})\)._

We can also use this framework to define robustness to adversarial examples. We assume that an adversarial example is an object that maximizes perceptual (i.e. representational) disagreement between the teacher and the student.

**Definition A.3**.: Consider input space \(X\subseteq\mathbbm{R}^{nd}\), shared data \(x\sim X\), and two models, \(A\) and \(B\), with \(D_{P}(A,B_{1};X)=\epsilon_{B}\). An **adversarial example** is an object \(e\in\mathbb{R}^{d}\) that maximizes disagreement between \(A\) and \(B\) on \(S(x;e)\), the subset of \(n(n-1)/2\) triplets relating the objects in \(x\) to \(e\).

\[e=\max_{X}||S_{A}(x;e)-S_{B}(x;e)||_{1}\] (1)

Using Definition 3.6 we immediately get the following result.

**Lemma A.4**.: _Consider an input space \(X\subseteq\mathbbm{R}_{nd}\), and two agents, \(A\) and \(B\). \(D_{P}(A,B;X)=E[\frac{||S_{A}(X)-S_{B}(X)||_{1}}{n(n-1)(n-2)/2}]\)._

We can now show that a model that is more aligned with the teacher will, on average, also be more robust to adversarial examples.

**Theorem A.5**.: _(**Alignment and adversarial robustness)**. Consider input space \(X\subseteq\mathbbm{R}^{nd}\), shared data \(x\sim X\), and three models, \(A\), \(B_{1}\), and \(B_{2}\) with \(D_{P}(A,B_{1};X)=\epsilon_{B_{1}}\) and \(D_{P}(A,B_{2};X)=\epsilon_{B_{2}}\). If \(\epsilon_{B_{1}}<\epsilon_{B_{2}}\), then \(E[\max_{e\in x}||S_{A}(x;e)-S_{B_{1}}(x;e)||_{1}]<E[\max_{e\in X}||S_{A}(x;e)- S_{B_{2}}(x;e)||_{1}]\)._

Proof.: Note that for a set of \(k\) binomial random variables \(X_{i}\sim Bin(n,p)\), the expectation of the \(k\)-th order statistic is \(E[X_{(k)}]=\sum_{x=0}^{n}(1-F(x;n,p)^{k})\) where \(F(x;n,p)=P(X_{i}\leq x)\). In the case of adversarial examples, let \(X_{i}\) be a random variable corresponding to the set of objects sampled uniformly from the input space \(X\subseteq\mathbbm{R}^{nd}\) then \(U=||S_{A}(X;e)-S_{B_{1}}(X;e)||_{1}\), \(Y\sim Bin(n(n-1)/2,\epsilon_{B_{1}})\) and similarly \(V=||S_{A}(X;e)-S_{B_{2}}(X;e)||_{1},V\sim Bin(n(n-1)/2,\epsilon_{B_{1}})\). In that case, the expected disagreement of \(A\) and \(B_{1}\) on an adversarial example is \(E[U_{(n)}]=\sum_{x=0}^{n(n-1)/2}(1-F(x;n(n-1)/2,\epsilon_{B_{1}})^{n})\) and for \(A\) and \(B_{2}\) it is \(E[V_{(n)}]=\sum_{x=0}^{n(n-1)/2}(1-F(x;n(n-1)/2,\epsilon_{B_{2}})^{n})\). If \(\epsilon_{B_{1}}<\epsilon_{B_{2}}\), then \(F(x;n(n-1)/2,\epsilon_{B_{1}})>F(x;n(n-1)/2,\epsilon_{B_{2}})\) and thus \(E[U_{(n)}]<E[V_{(n)}]\). 

_Remark A.6_.: While this theorem shows that increased alignment generally leads to increased adversarial robustness, this relies on a representational metric of adversarial examples. However, in practice, adversarial robustness is often measured using hard classification error as a simple proxy. This proxy does not capture the fine-grained degree of misalignment between humans and a model on each example. As a result, when measuring adversarial robustness using this proxy, the effect of alignment may be dampened by the U-shaped effect seen in other classification settings as mentioned above.

## Appendix B List of 491 models used in experiments

adv_inception_v3, bat_resnet26ts, beit_base_patch16_224, beit_base_patch16_384, beit_large_patch16_224, beit_large_patch16_384, botnet26t_256, cair_s24_224, cair_s24_384,caint_s36_384, caint_xs24_384, caint_xs24_224, caint_xs24_384, caint_xs36_224, caint_xs36_384, coat_lite_mini, coat_lite_small, coat_lite_tiny, coat_mini, coat_tiny, convit_small, convit_tiny, convmixer_1024_20_ks9_p14, convmixer_1536_20, convmixer_768_32, convnext_base, convnext_base_384_in22ft1k, convnext_base_in22ft1k, convnext_large, convnext_large, _g384_in22ft1k, convnext_large, in_22ft1k, convnext_small, convnext_tiny, cspdarknet53, cspresnet50, cglest_base_patch16_224, deit_base_patch16_384, deit_small_patch16_224, deit_tiny_patch16_224, densenet121, densenet161, densenet169, densenet201, densenetblur121d, dla102, dla102x2, dla102x2, dla169, dla34, dla46_c, dla64x_c, dla60, dla60, dla60_res2net, dla60_res2next, dla60x, dla60x_c, dm_nfnet_60, dm_nfnet_61, dm_nfnet_f2, dpn107, dpn131, dpn68, dpn68b, dpn92, dpn98, eca_botnext26ts_256, eca_halonext26ts, eca_nfnet_10, eca_nfnet_12, eca_resnet34s, eca_resnext26ts, ecaresnet101d, ecaresnet101d_pruned, ecaresnet269d, ecaresnet26t, ecaresnet50d, ecaresnet50t, ecaresnet50t, ecaresnetlight, efficientnet_b0, efficientnet_b1, efficientnet_b2, efficientnet_b2, pruned, efficientnet_b3, efficientnet_b3, pruned, efficientnet_b4, efficientnet_el, efficientnet_el_pruned, efficientnet_em, efficientnet_es, efficientnet_es, pruned, efficientnet_lite0, eficientnet_v2_rw_m, efficientnet2_rw_s, efficientnet2_rw_t, ens_adv_inception_resnet_v2, ese_vvonet19b_dw, ese_vonet39b, fbnet5_100, fbnetv3_0, fbnetv3_g, gc_efficientnet2_rw_t, gecresnet33ts, gcresnet50t, cgresnext26ts, cgresnext50ts, gernet_l, gernet_m, gernet_s, ghostnet_100, gluon_inception_v3, gluon_resnet101_v1b, gluon_resnet101_v1c, gluon_resnet101_v1d, gluon_resnet101_v1s, gluon_resnet152_v1b, gluon_resnet152_v1c, gluon_resnet152_v1d, gluon_resnet152_v1s, gluon_resnet18_v1b, gluon_resnet34_v1b, gluon_resnet50_v1b, gluon_resnet50_v1c, gluon_resnet50_v1d, gluon_resnet50_v1s, gluon_resnet101_32x4d, gluon_resnet401_64x4d, gluon_resnet50_32x4d, gluon_senet154, gluon_seresnext101_32x4d, gluon_seresnext101_64x4d, gluon_seresnext50_32x4d, gluon_xception65, gmixer_24_224, gmlp_s16_224, halo2botnet50ts_256, halonet250ts, halonet250, haloreorens_a, hardcorens_b, hardcorens_b, hardcorens_c, hardcorens_d, hardcorens_f, hrnet_w18, hrnet_w18_small_v2, hrnet_w30, hrnet_w40, hrnet_w44, hrnet_w48, hrnet_w64, ig_resnext101_32x16d, ig_resnext101_32x8d, inception_resnet_v2, inception_v3, inception_v4, jx_nest_base, jx_nest_small, jx_nest_tiny, lambda_resnet26pt_256, lambda_resnet26t, lambda_resnet50ts, lamhalobotnet50ts_256, lcnet_050, lcnet_075, lcnet_100, legacy_senet154, legacy_seresnet101, legacy_seresnet152, legacy_seresnet18, legacy_seresnet34, legacy_seresnet50 legacy_seresnext50, legacy_seresnext101_32x4d, legacy_seresnext26_32x4d, legacy_seresnext50_32x4d, mixer_b16_224, mixer_b16_224_mil, mixnet_l, mixnet_m, mixnet_s, mixnet_xl, mnasnet_100, mnasnet_small, mobilenetv2_050, mobilenetv2_100, mobilenetv2_110d, mobilenetv2_120d, mobilenetv2_140, mobilenetv3_large_100, mobilenetv3_large_100, mpiet_b_224, pit_s224, pit_s224, pit_s224, pinst_x224, pinst_x224, pinstemelarge, regnetx_002, regnetx_004, regnetx_006, regnetx_006, regnetx_016, regnetx_032, regnetx_040, regnetx_064, regnetx_080, regnetx_120, regnetx_160, regnetx_320, regnety_002, regnety_004, regnety_006, regnety_008, regnety_016, regnety_032, regnety_040, regnety_080, regnety_120, regnety_160, regnety_320, regnetz_b16, regnetz_c16, regnetz_d32, regnetz_d8, regnetz_e8, repvgg_a2, repvgg_b0, repvgg_b1, repvgg_a5,10,2, repvgg_b2, repvgg_b2g4, repvgg_b3g4, res2net101_26w4_s, res2net50_14w4_8s, res2net50_26w4_s, res2net50_26w4_s, res2net50_26w4_s, res2net50_26w4_8s, res2net50_26w4_s, res2net50_48w4_2s, res2net50_48w4_2s, res2net50_48w4_2s, res2net50_48w4_2s, res2net50_48w4_s, resnet50_48w4_s, resnet50_48w4_s, resnet50

[MISSING_PAGE_FAIL:15]

## Appendix D Outlier examples

We compared performance on the original 'noisy' ImageNet and relabeled ImageNet-ReaL [10]. All models had better performance on ReaL, but the improvement has a weak inverted-U relationship with alignment. This may seem surprising, but our understanding is that the ReaL dataset was collected by relabelling images on which models (pre-2020) disagreed with the original labels. Models with low performance by today's standards (and alignment near 0.5) were used to select which images to relabel, so it is unsurprising that models with alignment near 0.5 benefit most from this relabeling. This suggests that ImageNet labels may need to be revisited once again as ReaL relabelling may have been biased.

## Appendix E Reproducibility: Code and results data

All code and full results data are provided as part of the supplemental information. We will share them publicly after the anonymity period is over. All experiments were conducted on an AWS "x1.16xlarge" instance (no GPUs).

Figure 5: Comparing Pearson pairwise alignment and ImageNet validation accuracy.