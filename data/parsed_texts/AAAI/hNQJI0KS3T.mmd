# RDR: the Recap, Deliberate, and Respond Method for Enhanced Language Understanding

Yuxin Zi1*, Hariram Veeramani2*, Kaushik Roy1, Amit Sheth1

###### Abstract

Natural language understanding (NLU) using neural network pipelines often requires additional context that is not solely present in the input data, such as external knowledge graphs. Through prior research, it has been evident that NLU benchmarks are susceptible to manipulation by neural models - these models exploit statistical artifacts within the encoded external knowledge to artificially inflate performance metrics for downstream tasks. Our proposed approach, known as the Recap, Deliberate, and Respond (RDR) paradigm, addresses this issue by incorporating three distinct objectives within the neural network pipeline. The Recap objective involves paraphrasing the input text using a paraphrasing model in order to summarize and encapsulate salient information of the input. Deliberate refers to encoding the external graph information that is relevant to entities in the input text using a graph embedding model. Finally, Respond employs a classification head model that integrates representations from the Recap and Deliberate steps to generate the final prediction. By cascading these three models and minimizing a combined loss, we mitigate the potential of the model gaming the benchmark, while establishing a robust method for capturing the underlying semantic patterns to achieve accurate predictions. We conduct tests on multiple GLUE benchmark tasks to evaluate the effectiveness of the RDR method. Our results demonstrate improved performance compared with competitive baselines, with an enhancement of up to 2% on standard evaluation metrics. Furthermore, we analyze the observed behavior of semantic understanding of the RDR models, emphasizing their ability to avoid gaming the benchmark while accurately capturing the true underlying semantic patterns.

1Artificial Intelligence Institute, University of South Carolina, Columbia, SC, USA

2University of California, Los Angeles (UCLA)

yzi@email.sc.edu, hariram@ucla.edu, kaushikr@email.sc.edu, amit@sc.edu

## Introduction

Previous research in the field of natural language understanding (NLU) and neural network pipelines has acknowledged the necessity of incorporating additional context beyond the input data [1]. To address this limitation, one well-established approach is to integrate external knowledge graphs as supplementary context [13]. These knowledge graphs contain structured information about entities, relationships, and concepts. This external information enables the neural network to infer semantic connections between entities, even when such relations are not explicitly stated in the data alone. Thus, the neural network is able to uncover implicit or missing contextual associations.

However, a notable concern has been raised by previous research on the vulnerability of NLU benchmarks when facing manipulation by neural models [1, 1]. This issue casts doubt on the reliability and generalizability of the performance metrics reported by neural models on benchmark datasets, and has undergone extensive scrutiny within the NLU community. Researchers have explored various methods to mitigate benchmark gaming to ensure the NLU models exhibit authentic language understanding. These methods include introducing more comprehensive evaluation protocols, employing adversarial testing, and integrating external knowledge into training. However, the first two approaches do not redesign the training pipeline to enhance the model's language understanding capability. Although the knowledge integration method does modify the training procedure, it remains unclear whether the external knowledge is factually and sufficiently enforced into the integration process.

We propose a novel approach called the Recap, Deliberate, and Respond (RDR) paradigm, which addresses these limitations by integrating three distinct objectives within the neural network pipeline. The first objective, Recap, involves paraphrasing the input text using a dedicated model. This process captures the essential and salient information in the input. The second objective, Deliberate, focuses on encoding external graph information that relates to the entities appeared in the input text. This step utilizes a graph embedding model to leverage the knowledge within the knowledge graphs. By integrating this external context into the neural network pipeline, the Deliberate objective enhances the model's capability of comprehending relationships between entities and extract relevant information for downstream tasks. The final objective in the RDR paradigm is Respond, which employs a classification head model. This model utilizes representations from the Recap and Deliberate modules to generate the final prediction. By incorporating insights from the Recap and Deliberate stages, the Respond objective enables more accurate and informed predictions. The cascading structure of these three objectives, along with minimizing a combined loss, prevents the modelfrom artificially inflate performance metrics through exploiting statistical artifacts. Our robust methodology facilitates the capturing of the true underlying semantic patterns of the input data with the assistance of external knowledge, which lead to more reliable and accurate predictions.

To evaluate the effectiveness of the RDR paradigm, we test our method on multiple GLUE benchmark tasks that involve sentence similarity, textual entailment, and natural language inference [22, 23, 24, 25, 26]. The results demonstrate superior performance compared to competitive baselines, with improvements of up to 2% on standard evaluation metrics. These findings highlight the capability of the RDR approach to enhance NLU performance of neural models. Furthermore, with inference examples from the RDR models, we discuss the model's robustness for semantic understanding against statistical artifacts. We find that the introduction of the Recap and Deliberate objectives leads to better comprehension of the underlying semantic patterns in both data and external knowledge.

## RDR-Methodology

Figure 1 shows an overview of the traditional training pipeline for integrating external knowledge within neural networks, and our RDR method.

### Traditional Method

**Notations: Functions, and their Inputs and Outputs**

* _Input Text:_\(x\), Tokenized Text: \(T(x)\)
* _Language Model, Input, Function and Output:_\(x^{\prime}=f(T(x),\theta)\)
* _Subgraph Extractor, Input, Function, and Output:_\(KG_{x}=subgraph\_extract(T(x),KG)\), here \(KG\) is a large knowledge graph (e.g., ConceptNet).
* _Graph Embedding Model, Input, Function and Output:_\(e_{x}=Aggr(g(KG_{x},i\in KG_{x},\theta^{\prime}))\), here \(Aggr\) is an aggregation function (e.g., average of all node embeddings in \(KG_{x}\)), \(i\in KG_{x}\) denotes the nodes in subgraph \(KG_{x}\).
* _Embedding Fusion Model with Classification Head, Input, Function, and Output:_\(z=h(e_{x},x^{\prime},\theta^{{}^{\prime\prime}})\)
* _Loss:_ Cross Entropy (CE) loss with ground truth denoted as \(y\), \(CE(z,y)\).

### Forward Pass and Loss Calculation During Training

The steps for the traditional method are as follows:

1. [leftmargin=*]
2. Feed the tokenized text \(T(x)\) into a language model, obtaining the embedding \(x^{\prime}\).
3. Apply an _off-the-shelf_ graph extraction method to extract a subgraph \(KG_{x}\) from the larger knowledge graph \(KG\).
4. Apply a graph embedding model \(Aggr(g(KG_{x},\theta^{\prime}))\) to obtain the graph embedding for \(x\), denoted as \(e_{x}\).
5. Pass the language model embedding \(x^{\prime}\) and the subgraph embedding \(e_{x}\) into an embedding fusion model with a classification head \(h(e_{x},x^{\prime},\theta^{{}^{\prime\prime}})\) to obtain the logits \(z\). Compute the loss using logits \(z\) and ground truth \(y\).

### The RDR Method

**Notations: Functions, and their Inputs and Outputs**

* _Input Text:_\(x\), Tokenized Text: \(T(x)\)
* _Panaphrasing Model, Input, Function and Output:_\(x^{\prime}=f(T(x),\theta)\)
* _Panaphrasing Loss:_ The discrepancy between the paraphrased text \(x^{\prime}\) and the original text \(x\) is measured using cross entropy loss between the logits from the model \(f\) and the ground truth distribution of tokens in \(x\). We denote this loss as \(PL(x^{\prime},x)\).
* _Subgraph Extractor, Input, Function, and Output:_\(KG_{x}=subgraph\_extract(T(x),KG)\), here \(KG\) is a large knowledge graph (e.g., ConceptNet).
* _Graph Embedding Model, Input, Function and Output:_\(e_{x}=Aggr(g(KG_{x},i\in KG_{x},\theta^{\prime}))\), here \(Aggr\) is an aggregation function (e.g., average of all node embeddings in \(KG_{x}\)), \(i\in KG_{x}\) denotes the nodes in subgraph \(KG_{x}\).
* _Embedding Loss Calculator:_ First, all links in \(KG_{x}\) are predicted using the model \(Aggr(g(KG_{x},i\in KG_{x},\theta^{\prime}))\). We define a link between two nodes \(i\) and \(j\) to be exist if \(||g(KG_{x},i,\theta^{\prime})-g(KG_{x},j,\theta^{\prime})||\leq\tau\), where \(tau\) is a "closeness" threshold set empirically. Then, we calculate link prediction metrics, e.g., mean reciprocal rank (MRR), hits@k, etc, and denote as \(GEL(x,KG_{x})\).
* _Embedding Fusion Model with Classification Head, Input, Function, and Output:_\(z=h(e_{x},x^{\prime},\theta^{{}^{\prime\prime}})\)
* _Response Loss:_ Cross Entropy (CE) loss with ground truth denoted as \(y\), \(RL(z,y)\).
* _Total Loss:_\(L=PL(x^{\prime},x)+GEL(x,KG_{x})+RL(z,y)\).

### Forward Pass and Loss Calculation During Training

We describe the steps of our RDR method:

1. [leftmargin=*]
2. Fed the tokenized text \(T(x)\) into a language model, obtaining the embedding \(x^{\prime}\). Calculate the paraphrasing loss \(PL(x^{\prime},x)\).
3. Apply an _off-the-shelf_ graph extraction method to extract a subgraph \(KG_{x}\) from the larger knowledge graph \(KG\).
4. Apply a graph embedding model \(Aggr(g(KG_{x},\theta^{\prime}))\) to obtain the graph embedding for \(x\), denoted as \(e_{x}\). Compute the link prediction loss as \(GEL(x,KG_{x})\).
5. Pass the language model embedding \(x^{\prime}\) and the subgraph embedding \(e_{x}\) into an embedding fusion model with a classification head \(h(e_{x},x^{\prime},\theta^{{}^{\prime\prime}})\) to obtain the logits \(z\). Compute the response loss as \(RL(z,y)\)
6. Compute the total loss as \(L=PL(x^{\prime},x)+GEL(x,KG_{x})+RL(z,y)\).

## Experiments and Results

Our implementation utilizes task-specific hyperparameters, previously identified as optimal for the GLUE benchmark, except that we train for 1 epoch instead of 3. Throughout the training and evaluation process, a batch size of 8 is employed. We also integrate task-specific knowledge graphs or subgraph representations, amounting to up to 10% of the total knowledge graph triples. Our approach adheresto the predefined train-validation split established by the GLUE benchmark. The reported results (Table 1) represent the average of two independent runs. The knowledge graphs utilized in this study include DBPedia, ConceptNet, Wiktionary, WordNet, and the OpenCyc Ontology. These knowledge graphs consist of interconnected objects and their relationships, forming semantic associations [1, 16, 17]. Figure 2 illustrates the process of extracting subgraphs from the input text.

Approximately 300K subgraphs are obtained from the knowledge graphs across all inputs. The relationships include Antonym, DistinctFrom, EtymologicallyRelatedTo, LocatedNear, RelatedTo, SimilarTo, Synonym, AtLocation, CapableOf, Causes, CausesDesire, CreatedBy, DefinedAs, DerivedFrom, Desires, Entails, ExternalURL, FormOf, HasA, HasContext, HasFirstSubevent, HasLastSubevent, HasPrerequisite, HasProperty, InstanceOf, IsA, MadeOf,

Figure 1: (**a**) A traditional neural network pipeline which is enhanced with external knowledge to handle GLUE tasks such as entailment, similarity, and other types of natural language inference tasks. Initially, the tokenized text undergoes encoding by a language model, which outputs an embedding. Following that, a method based on graph embedding is employed to extract and embed a subgraph that is relevant to the input text. This involves extracting entities within a certain distance threshold from the entities present in the text. Subsequently, the two embeddings - the language model embedding and the graph embedding, are merged and passed through a classification head to obtain the predicted logits. To train this model, the cross-entropy loss between the predicted logits and the actual output is minimized. (**b**) The RDR paradigm. The tokenized input goes through a paraphrasing model, and a paraphrasing loss is calculated. Additionally, the graph-embedding-based subgraph extraction method is compared against a ground truth subgraph, then a graph embedding loss is computed. The total loss is the sum of the losses from the paraphrasing loss, graph embedding loss, and classification head loss.

MannerOf, MotivatedByGoal, ObstructedBy, PartOf, ReceivesAction, SenseOf, SymbolOf, and UsedFor.

We experiment with the best-performing models from huggingface with \(\leq\) 500 million parameters, i.e., BERT, RoBERTa, and ALBERT. We note that RoBERTa is especially suited for entailment tasks Devlin et al. (2018); Liu et al. (2019); Lan et al. (2019). Table 1 shows the accuracies for all the models with and without the RDR training method. For the graph embedding model in the RDR model, we use the TransE algorithm Bordes et al. (2013). The RDR method shows significant improvements over the baselines, with just 10% of the knowledge graph. We choose a random 10% of the knowledge graph triples for each training run to avoid the observed phenomenon of gaming of evaluation benchmarks by language understanding models, i.e., to avoid the model fitting spurious patterns in the additional knowledge as a means to achieve high accuracy scores.

## Conclusion and Future Work

This paper presents the formalization and initial experimental outcomes of a new training approach known as "Recap, Deliberate, and Respond" (RDR). We demonstrate that RDR achieves superior performance compared with baseline methods. RDR also shows resilience against manipulation of benchmarks (as evidenced by observing performance improvements when using only a random 10% of the available knowledge during each training iteration). Subsequent research will explore the utilization of diverse knowledge sources, including domain-specific knowledge, broader general knowledge (e.g., from Wiki, Unified Medical Language System), and others. We will also experiment using large SOTA models (e.g., LLMs such as mistral, llama, falcon, and ChatGPT) and diverse geometrical embeddings (e.g., ComPIEx, HolE, DistMult) within the RDR framework.

\begin{table}
\begin{tabular}{l|l|l|l|l|l|l|l|l} \hline \hline M & \multicolumn{2}{l|}{**MNLI-**} & \multicolumn{2}{l|}{**MNLI-**} & \multicolumn{2}{l|}{**QNLI**} & \multicolumn{2}{l|}{**QQP**} & \multicolumn{1}{c|}{**WNLI**} & \multicolumn{1}{c|}{**MRPC**} & \multicolumn{1}{c}{**RTE**} \\  & \multicolumn{1}{c|}{**M**} & \multicolumn{1}{c|}{**MM**} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & \\ \hline B & 82.44 & 83.52 & 90.49 & 90.1 & 54.92 & 82.11 & 66.06 \\ R\({}_{\text{B}}\) & 83.31 & 84.47 & 91.25 & 90.78 & 56.33 & 82.86 & 67.87 \\ R & 85.07 & 85.19 & 91.06 & 90.17 & 56.33 & 86.03 & 62.81 \\ R\({}_{\text{B}}\) & 85.78 & 85.95 & 91.85 & 90.96 & 57.75 & 86.76 & 63.53 \\ A & 84.34 & 85.32 & 90.6 & 90.25 & 57.75 & 86.27 & 66.06 \\ R\({}_{\text{A}}\) & 85.03 & 85.82 & 91.18 & 90.74 & 59.15 & 87 & 66.43 \\ \hline \hline \end{tabular}
\end{table}
Table 1: \(\mathbf{R_{B}}\): RDR\({}_{\text{B}}\), \(\mathbf{R_{A}}\): RDR\({}_{\text{A}}\), \(\mathbf{R_{R}}\): RDR\({}_{\text{R}}\), \(\mathbf{M}\): MODEL, **MNLI-M**: MNLI-MATCHED, **MNLI-MM**: MNLI-MISMATCHED, B: BERT-BASE, R: ROBERTa-BASE, A: ALBERT-BASE-V2. Results for RDR method compared to models that do not use the RDR method. We see improvements of up to 2%, on average 1% using only 10% of the knowledge graphs triples showing promise of the RDR methodology for improved language understanding.

Figure 2: Illustration of the process of extracting subgraphs from the knowledge graph given an input instance. For the pretrained Graph Encoder Network, we use ConceptNet’s Numberbatch embeddings and use a span length of three in our experiments.

## Acknowledgements

This research is built upon prior work [15, 16, 17, 18, 19, 20], and supported in part by NSF Award 2335967 "EAGER: Knowledge-guided neurosymbolic AI" with guardrails for safe virtual health assistants". Opinions are those of the authors and do not reflect the opinions of the sponsor [16, 17, 18, 19, 21, 22, 23].

## References

* Auer et al. (2007) Auer, S.; Bizer, C.; Kobilarov, G.; Lehmann, J.; Cyganiak, R.; and Ives, Z. 2007. Dbpedia: A nucleus for a web of open data. In _The semantic web_, 722-735. Springer.
* Bender and Koller (2020) Bender, E. M.; and Koller, A. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In _Proceedings of the 58th annual meeting of the association for computational linguistics_, 5185-5198.
* Bordes et al. (2013) Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. _Advances in neural information processing systems_, 26.
* Demszky et al. (2018) Demszky, D.; Guu, K.; and Liang, P. 2018. Transforming question answering datasets into natural language inference datasets. _arXiv preprint arXiv:1809.02922_.
* Devlin et al. (2018) Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_.
* Dolan and Brockett (2005) Dolan, W. B.; and Brockett, C. 2005. Automatically Constructing a Corpus of Sentential Paraphrases. In _Proceedings of the Third International Workshop on Paraphrasing (IWP2005)_.
* Lan et al. (2019) Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; and Soricut, R. 2019. Albert: A lite bert for self-supervised learning of language representations. _arXiv preprint arXiv:1909.11942_.
* Liu et al. (2019) Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_.
* Matuszek et al. (2006) Matuszek, C.; Witbrock, M.; Cabral, J.; and DeOliveira, J. 2006. An introduction to the syntax and content of Cyc. _UMBC Computer Science and Electrical Engineering Department Collection_.
* McCoy, Pavlick, and Linzen (2019) McCoy, R. T.; Pavlick, E.; and Linzen, T. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. _arXiv preprint arXiv:1902.01007_.
* Poliak (2020) Poliak, A. 2020. A survey on recognizing textual entailment as an NLP evaluation. _arXiv preprint arXiv:2010.03061_.
* Rawte et al. (2022) Rawte, V.; Chakraborty, M.; Roy, K.; Gaur, M.; Faldu, K.; Kikani, P.; Akbari, H.; and Sheth, A. 2022. TDLR: Top (Semantic)-Down (Syntactic) Language Representation. _UMBC Faculty Collection_.
* Roy et al. (2023a) Roy, K.; Garg, T.; Palit, V.; Zi, Y.; Narayanan, V.; and Sheth, A. 2023a. Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust. _arXiv preprint arXiv:2305.04989_.
* Roy et al. (2023b) Roy, K.; Gaur, M.; Soltani, M.; Rawte, V.; Kalyan, A.; and Sheth, A. 2023b. ProKnow: Process knowledge for safety constrained and explainable question generation for mental health diagnostic assistance. _Frontiers in big Data_, 5:1056728.
* Roy et al. (2021) Roy, K.; Zhang, Q.; Gaur, M.; and Sheth, A. 2021. Knowledge infused policy gradients with upper confidence bound for relational bandits. In _Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part 1 21_, 35-50. Springer.
* Roy et al. (2023c) Roy, K.; Zi, Y.; Gaur, M.; Malekar, J.; Zhang, Q.; Narayanan, V.; and Sheth, A. 2023c. Process Knowledge-infused Learning for Clinician-friendly Explanations. _arXiv preprint arXiv:2306.09824_.
* Roy et al. (2022) Roy, K.; Zi, Y.; Narayanan, V.; Gaur, M.; and Sheth, A. 2022. KSAT: Knowledge-infused Self Attention Transformer-Integrating Multiple Domain-Specific contexts. _arXiv preprint arXiv:2210.04307_.
* Sharma et al. (2019) Sharma, L.; Graesser, L.; Nangia, N.; and Evci, U. 2019. Natural language understanding with the quora question pairs dataset. _arXiv preprint arXiv:1907.01041_.
* Sheth et al. (2021) Sheth, A.; Gaur, M.; Roy, K.; and Faldu, K. 2021. Knowledge-intensive language understanding for explainable AI. _IEEE Internet Computing_, 25(5): 19-24.
* Sheth et al. (2022) Sheth, A.; Gaur, M.; Roy, K.; Venkataraman, R.; and Khandelwal, V. 2022. Process Knowledge-Infused AI: Toward User-Level Explainability, Interpretability, and Safety. _IEEE Internet Computing_, 26(5): 76-84.
* Sheth, Roy, and Gaur (2023) Sheth, A.; Roy, K.; and Gaur, M. 2023. Neurosymbolic Artificial Intelligence (Why, What, and How). _IEEE Intelligent Systems_, 38(3): 56-62.
* Speer, Chin, and Havasi (2017) Speer, R.; Chin, J.; and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In _Proceedings of the AAAI conference on artificial intelligence_, volume 31.
* Venkataraman et al. (2023) Venkataraman, R.; Roy, K.; Raj, K.; Prasad, R.; Zi, Y.; Narayanan, V.; and Sheth, A. 2023. Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes. _arXiv preprint arXiv:2306.01805_.
* Wang et al. (2018) Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_.
* Zhu et al. (2023) Zhu, C.; Xu, Y.; Ren, X.; Lin, B. Y.; Jiang, M.; and Yu, W. 2023. Knowledge-augmented methods for natural language processing. In _Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining_, 1228-1231.
* Zi et al. (2023) Zi, Y.; Roy, K.; Narayanan, V.; Gaur, M.; and Sheth, A. 2023. IERL: Interpretable Ensemble Representation Learning-Combining Crowdourced Knowledge and Distributed Semantic Representations. _arXiv preprint arXiv:2306.13865_.