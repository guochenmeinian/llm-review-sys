# SleepFM: Multi-modal Representation Learning for Sleep across ECG, EEG and Respiratory Signals

 Rahul Thapa1, Bryan He2, Magnus Ruud Kjaer3, Gauri Ganjoo3, Hyatt Moore3,

**Emmanuel Mignot3, James Zou1,**

###### Abstract

Sleep is a complex physiological process involving multiple modalities across the body. We curate a large dataset of simultaneous polysomnography (PSG) recordings comprising electrical brain activity (EEG), heart rhythms (ECG), and respiratory patterns from over 14,000 participants, totaling over 100,000 hours of sleep data. We develop _SleepFM_, the first multi-modal foundation model for sleep learned through contrastive learning on this highly heterogeneous physiological data. When evaluated on a held-out test set, _SleepFM_ significantly improves retrieval performance over 500x over random chance. A logistic regression model trained on _SleepFM_'s learned embeddings achieves strong performance on sleep stage classification (macro AUPRC 0.69) and apnea detection (AUPRC 0.71), outperforming an end-to-end trained CNN for sleep stage classification (AUPRC 0.579) and apnea detection (AUPRC 0.56). We find representations learned using an innovative leave-one-out approach during contrastive learning significantly improve downstream task performance compared to representations from standard pairwise contrastive learning. This work demonstrates the value of holistic multi-modal sleep modeling.

1 Department of Biomedical Data Science, Stanford School of Medicine, Stanford, CA, USA

2 Department of Computer Science, Stanford School of Engineering, Stanford, CA, USA

3 Department of Psychiatry and Behavioral Sciences, Stanford School of Medicine, Stanford, CA, USA

rthapa84@stanford.edu

## Introduction

Sleep monitoring is a critical aspect for not only understanding sleep disorders but also gaining valuable insights into overall brain, pulmonary, and heart health [22]. Polysomnography (PSG), a comprehensive overnight sleep study, serves as a powerful tool by recording various physiological signals during sleep, including electroencephalogram (EEG), electroocolograms (EOG), and electrocardiogram (ECG) [13]. Traditionally, PSG data analysis involved manual visual inspection, a labor-intensive and time-consuming process prone to errors [1, 12]. Recent advancements in supervised deep learning have shown promise in automating sleep stage classification, particularly for disorders like apnea [11, 10]. However, most methods rely on labeled data from a narrow task. They rarely leverage the full breadth of physiological dynamics across diverse PSG modalities.

In parallel, contrastive learning has emerged as a powerful technique in other domains, such as radiology and pathology, where it pairs images with corresponding medical reports to learn rich medical image representations [11, 12, 13, 14, 15]. However, PSG representation learning by pairing different channels via multi-modal contrastive learning has been less explored. While some unimodal contrastive learning methods have been applied to ECG data [16, 1, 17, 18, 19], they lack the ability to compare different modalities effectively in latent space, which is crucial for transfer learning. Additionally, [13] developed SimCLR-like contrastive learning models pre-trained using multi-modal clinical time series data including ECG signals and structured time series data, and [1] utilized a large collection of electronic health records (EHRs) to learn ECG representations through contrastive learning between ECG, structured and unstructured EHR data. However, these studies primarily focused on ECG data rather than the broader spectrum of PSG modalities investigated here.

**Our Contribution** We introduce _SleepFM_, a sleep foundation model trained using contrastive learning on a multi-modal PSG dataset comprising of 14K instances from a sleep study conducted at a major US academic hospital dating back to 1999. By capitalizing on EEG, ECG, and respiratory modalities from PSG, _SleepFM_ exhibits superior performance in tasks such as retrieval, sleep stage classification, and apnea event classification, outperforming end-to-end trained CNN models. Additionally, our study highlights the potential of our methodology in scenarios with limited data availability, demonstrating promising results in a few-shot evaluation setting. To our knowledge, this is the first attempt to build and evaluate a foundation model for sleep.

## Method

### Datasets and Preprocessing

Our dataset encompasses PSG records from a US Sleep Clinic dating back to 1999. Comprising 14,068 recordings, this dataset features diverse waveforms, such as EEG, ECG, and EOG, collected over 8 hours per individual. All the

[MISSING_PAGE_FAIL:2]

### Model Training

Our model pretraining, involves contrastive learning optimization with stochastic gradient descent (SGD) using a momentum of 0.9 and an initial learning rate set to 1e-2. We use cross-entropy as our loss function. Training spans 20 epochs with early stopping based on validation loss, employing a batch size of 32. Hyperparameters draw from similar models in prior literature [14].

Upon pretraining completion, we generate embeddings for the train, validation, and test sets, utilizing the learned modality encoders. These training embeddings drive the training of a logistic regression classifier. The classifier's performance undergoes evaluation on the test set for both sleep stage and apnea detection tasks. For comparison, we define a baseline EfficientNet architecture akin to our pre-trained model encoder but solely trained via supervised learning on the entire (pretraining + training) dataset for classification tasks. This model is trained end-to-end from scratch using cross-entropy loss between predicted and true labels, optimized by SGD with a step decay learning rate schedule. Mirroring the pretraining phase, this model undergoes training for 20 epochs with a batch size of 32, aligning hyperparameters with our model pretraining strategy. All model training was executed on a single NVIDIA Tesla V100S GPU with 32GB of memory. Pretraining each epoch consumed approximately 4 hours, while baseline supervised training required roughly 2 hours on the same GPU.

## Experiments

### Retrieval Analysis

We assessed our model's capabilities by retrieving one modality's closest embeddings from the test set based on another modality's embeddings. Computing cosine similarity between ECG and EEG embeddings generated a ranked list, allowing us to gauge retrieval performance. Evaluation was measured using recall@10 and median rank metrics. **Recall@10**: Measures the true paired item's appearance within the top 10 recommendations. Higher values indicate more accurate retrieval. **Median rank**: Determines the median position of the true paired item in rankings; a lower median rank signifies a more consistent ranking of the correct pair. We assessed the retrieval performance using 90,000 randomly selected 30-second clips encompassing all modalities from the test set. The baseline Recall@10 performance stands at \(10/90000=0.0001\).

### Downstream Classification Tasks

We used the embeddings learned by the three models to train a logistic regression model. This model was employed to classify sleep stages and apnea events, and evaluation was performed on a held-out test dataset. Sleep stage classification is a multi-class classification task, with 5 classes: Wake, Stage 1, Stage 2, Stage 3, and REM. Apnea classification is a binary classification task. We compared our model's performance with baseline model, trained on all three modalities, for sleep stage and apnea event classification. Our evaluation relied on two primary metrics: AUROC (Area Under the Receiver Operating Characteristic curve) and AUPRC (Area Under the Precision-Recall Curve).

### Few-Shot Evaluation

We performed few-shot evaluation by steadily increase the number of participants \(k\) that each model sees from \(k=1\) to the full training dataset, and record the model's AUROC and AUPRC at each \(k\). Note that each patient contributes multiple training clips. We consider values of \(k\in\{1,2,4,8,16,32,64,128,1265\}\), where 1265 is the size of the full training set. For supervised CNN, few-shot examples are the only training examples seen by the model. For the pretrained models, we use embeddings of these few-shot examples to train a logistic regression model.

## Results

### Retrieval Analysis

Retrieval evaluation exhibited significant improvement compared to baseline metrics. Our model achieved over 500x-7000x higher recall@10 than the baseline as shown in Tables 2 and 1. Pairwise contrastive learning yield better overall retrieval performance than leave-one-out, most likely because the retrieval evaluation directly maps the training procedure of pairwise. One observable trend across both retrieval evaluation is relatively lower retrieval performance between Respiratory and other modalities, specially, Respiratory and EEG. The discrepancy in retrieval performance between EEG-Respiratory signals compared to EEG-ECG or ECG-Respiratory pairs might stem from the closer similarity and shared electrical nature between EEG and ECG signals. Both EEG and ECG capture electrical activities within the body, potentially resulting in more recognizable patterns and facilitating better correspondence.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{**Median Rank**} & \multicolumn{2}{c}{**Recall@10**} \\ \cline{2-7}  & ECG & Resp & EEG & ECG & Resp & EEG \\ \hline ECG & - & 2 & 1 & - & 0.81 & 0.74 \\ Resp & 2 & - & 5 & 0.82 & - & 0.60 \\ EEG & 1 & 6 & - & 0.82 & 0.58 & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: Retrieval on the test set for model trained with pairwise contrastive learning. Resp is for Respiratory. Random baseline for Recall@10 = 0.0001

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{**Median Rank**} & \multicolumn{2}{c}{**Recall@10**} \\ \cline{2-7}  & ECG & Resp & EEG & ECG & Resp & EEG \\ \hline ECG & - & 19 & 7 & - & 0.39 & 0.58 \\ Resp & 21 & - & 400 & 0.38 & - & 0.05 \\ EEG & 13 & 416 & - & 0.46 & 0.05 & - \\ \hline \hline \end{tabular}
\end{table}
Table 2: Retrieval on the test set for model trained with leave-one-out contrastive learning. Resp is for Respiratory. Random baseline for Recall@10 = 0.0001

### Downstream Classification Tasks

We focused on two relevant sleep study tasks: sleep stage and apnea classification as shown in Table 3. Notably, across all metrics, the logistic regression model trained using representations from our pretrained model outperforms the end-to-end trained CNN. This superiority holds true across all sleep stage classes as well as on aggregated class metrics. Model pretrained with leave-one-out contrastive learning performs better than the one pretrained with pairwise contrastive learning. Similarly, the apnea classification metrics, displayed in Table 4, underscore our approach's superiority over supervised CNN models. Here as well, the model pretrained with leave-one-out contrastive learning significantly outperforms the model pretrained with pairwise.

### Few-Shot Evaluation

The results for our few shot analysis is presented in Figure 2. We observe that across all the few shot settings, our model significantly outperforms baseline supervised CNN model for both sleep stage and apnea classification. Notably, the leave-one-out model significantly outperforms pairwise model across all shots, specially for apnea classification.

## Conclusion

Our study utilizes multi-modal PSG data and representation learning to improve identification of sleep events, advancing sleep medicine. The primary contributions involve developing and evaluating _SleepFM_, a multi-modal contrastive learning model, on a 14K PSG recordings. _SleepFM_ exhibited strong performance across retrieval, sleep stage, and apnea classification, surpassing supervised CNNs. The methodology centers on two contrastive learning approaches, leave-one-out and pairwise, which both effectively unified ECG, EEG, and respiratory signal representations and demonstrated efficacy in limited data scenarios. For retrieval, pairwise contrastive learning outperformed leave-one-out. For all downstream tasks, leave-one-out significantly outperforms pairwise.

**Limitations.** We primarily trained and evaluated on one

\begin{table}
\begin{tabular}{l c c} \hline \hline  & **AUROC** & **AUPRC** \\ \hline
**Leave-One-Out CL** & \(\mathbf{0.941_{\pm.002}}\) & \(\mathbf{0.711_{\pm.006}}\) \\
**Pairwise CL** & \(0.902_{\pm.003}\) & \(0.586_{\pm.007}\) \\
**Supervised CNN** & \(0.843_{\pm.002}\) & \(0.555_{\pm.005}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Apnea classification metrics. Baseline here is a supervised CNN trained on the entire (pretraining + training) dataset to classify apnea. The leave-one-out and pairwise models are logistic regression models trained on the embeddings generated from only the training dataset. Prevalence of apnea event is 0.017. \(\pm\) represents 95% CI.

Figure 2: Sleep apnea classification. The x-axis represents number of participants that the model was trained on. In case of pairwise and leave-one-out, we select embeddings from \(k\) number of participants to train a logistic regression model. In case of supervised CNN, we train the model end-to-end on \(k\) number of participants to classify either sleep stages or apnea. Testing is done on the entire test set. For each shot, we average the performance across 3 replicates.

dataset, thus model generalizability to other datasets is unknown. Testing on diverse datasets from different sleep clinics and demographics is crucial for validating robustness across populations. Additionally, while we focused on sleep stage and apnea detection, exploring other tasks like arousal detection, periodic leg movements, and narcolepsy could provide a more comprehensive clinical assessment.

## References

* S. Bannur, S. Hyland, Q. Liu, F. Perez-Garcia, M. Ilse, D. C. Castro, B. Boecking, H. Sharma, K. Bouzid, A. Thieme, et al. 2023 (2023)Learning to exploit temporal structure for biomedical vision-language processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15016-15027. Cited by: SS1.
* B. Boashash and S. Ouelha (2016)Automatic signal abnormality detection using time-frequency features and machine learning: a newborn EEG seizure case study. Knowledge-Based Systems106, pp. 38-50. Cited by: SS1.
* B. Boecking, N. Usuyama, S. Bannur, D. Castro, A. Schwaighofer, S. Hyland, M. Wetscherek, T. Naumann, A. Nori, J. Alvarez-Valle, et al. (2022)Making the most of text semantics to improve biomedical vision-language processing. In European conference on computer vision, pp. 21. Cited by: SS1.
* N. Diamant, E. Reinertsen, S. Song, A. D. Aguirre, C. M. Stultz, and P. Batra (2022)Patient contrastive learning: a performant, expressive, and practical approach to electrocardiogram modeling. PLoS computational biology18 (2), pp. e1009862. Cited by: SS1.
* B. Gopal, R. Han, G. Raghupathi, A. Ng, G. Tison, and P. Rajpurkar (2021)3KG: contrastive learning of 12-lead electrocardiograms using physiologically-inspired augmentations. In Machine Learning for Health, pp. 156-167. Cited by: SS1.
* A. R. Hassan and M. I. H. Bhuiyan (2017)Automated identification of sleep states from EEG signals by means of ensemble empirical mode decomposition and random under sampling boosting. Computer methods and programs in biomedicine140, pp. 201-210. Cited by: SS1.
* S. Huang, L. Shen, M. P. Lungren, and S. Yeung (2021)Gloria: a multimodal global-local representation learning framework for label-efficient medical image recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3942-3951. Cited by: SS1.
* D. Kiyasseh, T. Zhu, and D. A. Clifton (2021)Clocs: contrastive learning of cardiac signals across space, time, and patients. In International Conference on Machine Learning, pp. 5606-5615. Cited by: SS1.
* M. H. Kryger, T. Roth, and W. C. Dement (2010)Principles and practice of sleep medicine fifth edition. Cited by: SS1.
* S. K. Lalam, H. K. Kunderu, S. Ghosh, H. Kumar, S. Awasthi, A. Prasad, F. Lopez-Jimenez, Z. I. Attia, S. Asirvatham, P. Friedman, et al. (2023)ECG representation learning with multi-modal EHR data. Transactions on Machine Learning Research. Cited by: SS1.
* M. Y. Lu, B. Chen, A. Zhang, D. F. Williamson, R. J. Chen, T. Ding, L. P. Le, Y. Chuang, and F. Mahmood (2023)Visual language pretrained multiple instance zero-shot transfer for histopathology images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19764-19775. Cited by: SS1.
* T. Mehari and N. Strothoff (2022)Self-supervised representation learning from 12-lead ECG data. Computers in biology and medicine141, pp. 105114. Cited by: SS1.
* T. E. Nassi, W. Ganglberger, H. Sun, A. A. Bucklin, S. Biswal, M. J. van Putten, R. J. Thomas, and M. B. Westover (2021)Automated respiratory event detection using deep neural networks. arXiv preprint arXiv:2101.04635. Cited by: SS1.
* J. Oh, H. Chung, J. Kwon, D. Hong, and E. Choi (2022)Lead-agnostic self-supervised learning for local and global representations of electrocardiogram. In Conference on Health, Inference, and Learning, pp. 338-353. Cited by: SS1.
* A. N. Olesen, P. Jennum, E. Mignot, and H. B. Sorensen (2021)Msed: a multi-modal sleep event detection model for clinical sleep analysis. arXiv preprint arXiv:2101.02530. Cited by: SS1.
* D. Ouyang, J. Theurer, N. R. Stein, J. W. Hughes, P. Elias, B. He, N. Yuan, G. Duffy, R. K. Sandhu, J. Ebinger, et al. (2022)Electrocardiographic deep learning for predicting post-procedural mortality. arXiv preprint arXiv:2205.03242. Cited by: SS1.
* A. Raghu, P. Chandak, R. Alam, J. Guttag, and C. Stultz (2022)Contrastive pre-training for multimodal medical time series. In NeurIPS 2022 Workshop on Learning from Time Series for Health, Cited by: SS1.
* M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. Chen (2018)Mobilenetv2: inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4510-4520. Cited by: SS1.
* M. Tan and Q. Le (2019)Efficientnet: rethinking model scaling for convolutional neural networks. In International conference on machine learning, pp. 6105-6114. Cited by: SS1.
* S. L. Worley (2018)The extraordinary importance of sleep: the detrimental effects of inadequate sleep on health and public safety drive an explosion of sleep research. Pharmacy and Therapeutics43 (12), pp. 758. Cited by: SS1.
* L. Zhuang, M. Dai, Y. Zhou, and L. Sun (2022)Intelligent automatic sleep staging model based on cnn and lstm. Frontiers in Public Health10, pp. 946833. Cited by: SS1.

Figure 3: Raw signal data and corresponding events from a patient PSG.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & pretrain & train & valid & test \\ \hline participants (count) & 11,261 & 1,265 & 141 & 1,401 \\ events (count) & 10,611,314 & 1,190,392 & 130,380 & 1,314,267 \\ Duration (hours) & 88,427 & 9,920 & 1,086 & 10,952 \\ \hline Male (\%) & 49.85 & 50.15 & 47.12 & 53.04 \\ Female (\%) & 43.83 & 43.99 & 48.08 & 41.79 \\ Unknown (\%) & 6.32 & 5.86 & 4.8 & 5.17 \\ Age (years) & 42.19 \(\pm\) 19.63 & 43.02 \(\pm\) 20.33 & 40.41 \(\pm\) 19.98 & 41.9 \(\pm\) 19.92 \\ \hline TSD (mins) & 376.78 \(\pm\) 90.84 & 376.44 \(\pm\) 90.62 & 371.22 \(\pm\) 84.9 & 374.25 \(\pm\) 87.49 \\ WASO (mins) & 79.4 \(\pm\) 60.54 & 79.68 \(\pm\) 62.3 & 78.76 \(\pm\) 57.27 & 81.46 \(\pm\) 62.76 \\ SE (mins) & 88.63 \(\pm\) 246.43 & 91.93 \(\pm\) 91.68 & 91.49 \(\pm\) 53.46 & 92.36 \(\pm\) 64.17 \\ SL (mins) & 22.16 \(\pm\) 32.75 & 21.23 \(\pm\) 31.57 & 28.99 \(\pm\) 87.76 & 22.53 \(\pm\) 32.6 \\ REML (mins) & 151.97 \(\pm\) 102.64 & 149.41 \(\pm\) 97.72 & 148.63 \(\pm\) 99.93 & 154.87 \(\pm\) 103.53 \\ \hline Stage 1 (\%) & 9.35 \(\pm\) 9.18 & 9.31 \(\pm\) 8.75 & 8.18 \(\pm\) 7.68 & 9.04 \(\pm\) 8.86 \\ Stage 2 (\%) & 64.97 \(\pm\) 14.67 & 64.79 \(\pm\) 14.72 & 64.76 \(\pm\) 14.66 & 64.97 \(\pm\) 14.72 \\ Stage 3 (\%) & 10.18 \(\pm\) 13.22 & 10.2 \(\pm\) 13.19 & 10.9 \(\pm\) 12.68 & 10.32 \(\pm\) 13.57 \\ REM (\%) & 15.5 \(\pm\) 7.85 & 15.7 \(\pm\) 8.01 & 16.16 \(\pm\) 6.84 & 15.67 \(\pm\) 7.88 \\ \hline AHI (h\({}^{-1}\)) & 22.15 \(\pm\) 79.3 & 22.77 \(\pm\) 19.14 & 22.15 \(\pm\) 18.48 & 20.89 \(\pm\) 16.96 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Demographics table. REM: Rapid Eye Movement; AHI: Apnea-Hypopnea Index, a measure used in sleep medicine to assess the severity of sleep apnea; WASO: Wake After Sleep Onset, the total time spent awake after initially falling asleep; SL: Sleep Latency, the time it takes to transition from wakefulness to sleep; REML: REM Sleep Latency, the time it takes to enter REM sleep after falling asleep; SE: Sleep Efficiency, the percentage of time spent asleep while in bed; TSD: Total Sleep Duration, the overall duration of sleep.

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Modality** & **Channel** & **Description** \\ \hline Respiratory & Chest & Measures expansion and effort to breathe. Vital in detecting sleep apnea and hypopmeas. \\  & Snore & Detects vibrations or sound near airway openings during breathing. Identifies snoring patterns. \\  & SpO2 & Measures blood oxygen saturation using a clip on the fingertip or earlobe. Important for identifying variations in oxygen levels. \\  & Abdomen & Measures expansion and effort to breathe. Complements the chest belt in detecting respiratory efforts. \\  & Pulse Rate & Calculated from fingertip or ECG signals. Indicates respiratory disturbances. \\  & Nasal Pressure & Detects airflow limitations and obstructions, aiding in identifying nasal breathing difficulties. \\  & Oral Therm & Assesses nasal/oral breathing temperature. Detects mouth breathing affecting sleep quality. \\ \hline Sleep Stages & E1 & Electroocologram near the left eye, monitoring eye movements for sleep stages. \\  & M1 & Electromyogram on chin muscles. Monitors muscle activity for sleep cycles. \\  & M2 & Monitors chin muscle activity, aiding in differentiating REM and NREM sleep. \\  & C3 & EEG on the left hemisphere. Captures brainwave patterns for sleep staging. \\  & C4 & EEG on the right hemisphere. Captures brainwave patterns for sleep staging. \\  & O1 & EEG on the back left of the head. Captures brainwave patterns during NREM sleep. \\  & O2 & EEG on the back right of the head. Captures brainwave patterns during NREM sleep. \\  & Fz & Frontal EEG on the forehead. Captures brainwave patterns related to cognitive processes. \\  & Fp1 & Prefrontal EEG on the left forehead. Monitors brainwave patterns related to emotional processing. \\  & Fp2 & Prefrontal EEG on the right forehead. Monitors brainwave patterns related to emotional processing. \\ \hline ECG & ECG\_L & Left ECG electrode measures heart’s electrical activity, aiding in detecting arrhythmias. \\  & ECG\_R & Right ECG electrode monitors heart’s electrical activity to identify arrhythmias. \\ \hline \hline \end{tabular}
\end{table}
Table 6: Data modalities, their associated channels and descriptions.

\begin{table}
\begin{tabular}{l l l l|l l l} \hline \hline  & \multicolumn{3}{c|}{**AUROC**} & \multicolumn{3}{c}{**AUPRC**} \\ \cline{2-7}  & \multicolumn{1}{c}{**ECG**} & **Respiratory**} & **EEG** & **ECG** & **Respiratory** & **EEG** \\ \hline Wake & \(0.934_{\pm.001}\) & \(0.846_{\pm.001}\) & \(0.942_{\pm.001}\) & \(0.829_{\pm.004}\) & \(0.652_{\pm.003}\) & \(0.857_{\pm.002}\) \\ Stage 1 & \(0.786_{\pm.002}\) & \(0.676_{\pm.002}\) & \(0.801_{\pm.002}\) & \(0.193_{\pm.002}\) & \(0.127_{\pm.001}\) & \(0.211_{\pm.003}\) \\ Stage 2 & \(0.874_{\pm.001}\) & \(0.728_{\pm.001}\) & \(0.888_{\pm.001}\) & \(0.860_{\pm.001}\) & \(0.708_{\pm.001}\) & \(0.873_{\pm.001}\) \\ Stage 3 & \(0.919_{\pm.001}\) & \(0.788_{\pm.001}\) & \(0.927_{\pm.001}\) & \(0.638_{\pm.003}\) & \(0.307_{\pm.002}\) & \(0.679_{\pm.002}\) \\ REM & \(0.939_{\pm.001}\) & \(0.789_{\pm.001}\) & \(0.944_{\pm.001}\) & \(0.745_{\pm.003}\) & \(0.388_{\pm.003}\) & \(0.724_{\pm.003}\) \\ \hline
**Macro Avg** & \(0.891\) & \(0.765\) & \(0.900\) & \(0.436\) & \(0.484\) & \(0.669\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Sleep stage classification metrics for model trained with leave-one-out contrastive learning. After having trained the model with all three modalities, we extract embeddings for each modality separately and train a logistic regression with each modality to identify sleep stages. \(\pm\) represents 95% confidence intervals.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & **ECG** & **Respiratory** & **EEG** \\ \hline AUROC & \(0.750_{\pm.003}\) & \(0.916_{\pm.003}\) & \(0.733_{\pm.004}\) \\ AUPRC & \(0.041_{\pm.001}\) & \(0.456_{\pm.006}\) & \(0.036_{\pm.001}\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Apnea classification metrics for model trained with pairwise contrastive learning. After having trained the model with three modalities, we extract embeddings for each modality separately and train a logistic regression with each modality to identify apnea. \(\pm\) represents 95% confidence intervals.

\begin{table}
\begin{tabular}{l c c|c c c} \hline \hline  & \multicolumn{2}{c|}{**AUROC**} & \multicolumn{2}{c}{**AUPRC**} \\ \cline{2-6}  & **ECG** & **Respiratory** & **EEG** & **ECG** & **Respiratory** & **EEG** \\ \hline Wake & \(0.940_{\pm.001}\) & \(0.877_{\pm.001}\) & \(0.945_{\pm.001}\) & \(0.838_{\pm.002}\) & \(0.710_{\pm.002}\) & \(0.866_{\pm.001}\) \\ Stage 1 & \(0.791_{\pm.002}\) & \(0.701_{\pm.002}\) & \(0.812_{\pm.002}\) & \(0.199_{\pm.002}\) & \(0.140_{\pm.001}\) & \(0.225_{\pm.002}\) \\ Stage 2 & \(0.876_{\pm.001}\) & \(0.760_{\pm.001}\) & \(0.891_{\pm.001}\) & \(0.862_{\pm.001}\) & \(0.737_{\pm.001}\) & \(0.872_{\pm.001}\) \\ Stage 3 & \(0.917_{\pm.001}\) & \(0.806_{\pm.001}\) & \(0.925_{\pm.001}\) & \(0.627_{\pm.002}\) & \(0.339_{\pm.003}\) & \(0.645_{\pm.003}\) \\ REM & \(0.939_{\pm.001}\) & \(0.839_{\pm.001}\) & \(0.953_{\pm.001}\) & \(0.761_{\pm.003}\) & \(0.499_{\pm.003}\) & \(0.797_{\pm.002}\) \\ \hline
**Macro Avg** & \(0.892\) & \(0.796\) & \(0.905\) & \(0.657\) & \(0.484\) & \(0.680\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Sleep stage classification metrics for model trained with pairwise contrastive learning. After having trained the model with all three modalities, we extract embeddings for each modality separately and train a logistic regression with each modality to identify sleep stages. \(\pm\) represents 95% confidence intervals.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & **ECG** & **Respiratory** & **EEG** \\ \hline AUROC & \(0.735_{\pm.004}\) & \(0.925_{\pm.002}\) & \(0.735_{\pm.004}\) \\ AUPRC & \(0.040_{\pm.001}\) & \(0.697_{\pm.006}\) & \(0.040_{\pm.001}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Apnea classification metrics for model trained with leave-one-out contrastive learning. After having trained the model with all three modalities, we extract embeddings for each modality separately and train a logistic regression with each modality to identify apnea. \(\pm\) represents 95% confidence intervals.