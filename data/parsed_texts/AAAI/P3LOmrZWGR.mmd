# CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation

Zhihong Chen1*, Maya Varma1*, Jean-Benoit Delbrouck1*, Magdalini Paschali1

Louis Blankemeier1, Dave Van Veen1, Jeya Maria Jose Valanarasu1, Alaa Youssef1

Joseph Paul Cohen1, Eduardo Pontes Reis1, Emily B. Tsai1, Andrew Johnston1

Cameron Olsen1, Tanishq Mathew Abraham2, Sergios Gatidis1

###### Abstract

Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation. In this work, we present (i) _CheXinstruct_ - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets; (ii) _CheXagent_ - an instruction-tuned FM capable of analyzing and summarizing CXRs; and (iii) _CheXbench_ - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks by up to 97.5%.1

Footnote 1: Our project is at [https://stanford-aimi.github.io/chexagent.html](https://stanford-aimi.github.io/chexagent.html).

1Stanford University 2Stability AI

{zhihongc,mvarma2,jbdel,paschali,akshaysc,langlotz}@stanford.edu

## Introduction

Foundation models (FMs) have recently emerged as a powerful class of models capable of performing a diverse range of reasoning and comprehension tasks [1]. In this work, we present the following three components, also summarized in Fig. 1, to help create capable and robust FMs for chest X-ray (CXR) interpretation:

1. [leftmargin=*]
2. _CheXinstruct_ is an instruction-tuning dataset with 6M instruction-image-answer triplets designed to improve the ability of FMs to interpret CXRs. We collect instructions from 34 tasks and 65 unique datasets, spanning categories including coarse- and fine-grained image understanding, question answering, and text generation.
3. _CheXagent_ is an instruction-tuned foundation model with 8B parameters capable of analyzing images, understanding text, and generating responses. Our methodology for developing CheXagent includes training (1) a clinical LLM capable of understanding radiology reports, (2) a vision encoder capable of reading CXRs, and (3) a network to bridge the vision and language modalities. We then perform instruction-tuning using data from CheXinstruct.
4. _CheXbench_ is a novel benchmark designed to rigorously evaluate FMs across two evaluation axes: image perception and textual understanding. We introduce 8 tasks across 7 CXR datasets, and we evaluate performance using close-ended multiple-choice predictions as well as open-ended text generation.

We use CheXbench to compare CheXagent with six previous FMs from both general and medical domains. We further provide an evaluation of potential model bias and highlight performance disparities across demographic factors of sex, race and age to improve model transparency.

## Data: CheXinstruct

CheXinstruct seeks to cover a broad range of tasks to support training CXR FMs. These tasks can either (i) improve the abilities of FMs to understand CXRs or (ii) improve clinical decision making. This dataset is comprised of five categories of tasks, each categorized by their specific capabilities:

* Coarse-grained Image Understanding, which defines the overall understanding of CXRs, e.g., view classification [17], and disease classification [16, 15, 14, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29].
* Fine-grained Image Understanding, which defines the localized understanding of CXRs, e.g., abnormality detection [16, 17, 22],

Figure 1: Overview of the proposed pipeline: CheXinstruct is a curation of datasets for instruction-tuning across various CXR tasks, CheXagent is our clinical FM for CXR interpretation, and CheXbench is our comprehensive FM evaluation benchmark. Two example CXR interpretation tasks include local findings generation and open-ended visual question answering (VQA).

abnormality grounding [14], and foreign object detection [23].
* _Question Answering_, which defines the ability to respond to a question, e.g., close-ended and open-ended visual question answering (VQA) [22, 15, 16], findings summarization [14], and local findings generation [17].
* _Miscellaneous_: This category defines the miscellaneous abilities that are critical for a CXR FM, e.g., report evaluation [23, 17], and natural language explanation [23].

## Model: CheXagent

The aim of CheXagent is a model that can "see" images \(x_{I}\) and/or "read" text \(x_{T}\) and generate "responses" \(y\). Our training process for CheXagent involves the following stages:

**Stage 0: Train a clinical LLM**: Our starting point is Mistral-7B-v0.1 [17] due to its proven robust reasoning abilities in diverse benchmarks. To infuse the model with comprehensive medical and clinical knowledge, we utilize five distinct text sources for training: (i) PMC articles, (ii) MIMIC-IV, and (iii) Wikipedia. **Stage 1: Train a vision encoder for CXR**: Our model architecture reflects that of [14]. For training purposes, we utilize datasets comprising image-text pairs, specifically from MIMIC-CXR, PadChest, and BIMCV-COVID-19. **Stage 2: Train a vision-language bridge**: Following the training of the clinical LLM and the CXR vision encoder, we focus on developing a bridge model, \(\mathcal{M}_{b}\). This model is designed to map visual data to the corresponding language (semantic) space. For training \(\mathcal{M}_{b}\), we employ the same datasets as in Stage 1. **Stage 3: Instruction tuning**: Upon completing Stage 2, The model is trained on CheXinstruct, taking into account two primary factors: (i) reserving certain task-dataset pairs exclusively for evaluation purposes, and (ii) determining optimal dataset ratios to ensure balanced training across different capabilities.

## Evaluation: CheXbench

CheXbench is structured with two evaluation axes, crafted to assess crucial aspects of CXR interpretation: image perception and textual understanding. For the former, we introduce 6 tasks across 7 datasets: View Classification, Binary Disease Classification, Single Disease Identification, Multi-Disease Identification, Visual-Question-Answering, and Image-Text Reasoning; For the latter, we introduce 2 tasks: Findings Section Generation and Findings Summarization.

In our study, we employ CheXbench to compare CheXagent against two general-domain instruction-tuned FMs, InstructBLIP and BLIP2, which achieve state-of-the-art performance in previous research [14]. Additionally, we compare CheXagent with four medical FMs: XrayGPT, MedFlamingo, RadFM, and LLaVA-Med [23, 24, 25]. This comparison aims to provide a comprehensive understanding of CheXagent's performance in relation to both general and medical-specific models.

Table 1 provides results on CheXbench. For image perception tasks, CheXagent demonstrates superior performance across image perception tasks, achieving an average improvement of 97.5% over general-domain FMs and an average improvement of 55.7% over medical FMs; For text understanding tasks, CheXagent outperforms all medical FMs on CheXpert on findings section generation and also achieve promising performance on findings summarization.

## Conclusion

In this work, we design a complete scheme for training CXR FMs by introducing CheXinstruct, CheXagent, and CheXbench. Experimental results demonstrate the effectiveness of this scheme.

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline \multirow{2}{*}{**Task**} & \multirow{2}{*}{**Dataset**} & \multicolumn{3}{c}{**General-domain FMs**} & \multicolumn{3}{c}{**Medical-domain FMs**} & \multicolumn{3}{c}{**CheXagent**} \\  & & **BLIP-2** & **InstructBLIP** & **XrayGPT** & **MedFlamingo** & **RadFM** & **LLaVA-Med** & **(Ours)** \\ \hline \multirow{2}{*}{View Classification} & MIMIC-CXR & 28.8 & 25.3 & 24.0 & 25.0 & 28.5 & 23.3 & 97.5 \\  & CheXpert & 38.0 & 34.0 & 33.0 & 39.0 & 37.0 & 30.0 & 96.7 \\ \hline \multirow{2}{*}{Binary Disease Classification} & SIM & 53.0 & 54.0 & 50.0 & 50.0 & 50.0 & 49.0 & **64.0** \\  & RSNA & 50.0 & 60.0 & 50.0 & 50.0 & 50.0 & 44.0 & **81.0** \\  & CheXpert & 51.5 & 53.2 & 51.5 & 48.5 & 55.8 & 47.6 & **76.0** \\ \hline \multirow{2}{*}{Single Disease Identification} & OpenI & 40.2 & 40.2 & 45.4 & 39.0 & 42.2 & 43.8 & 47.0 \\  & MIMIC-CXR & 26.6 & 22.6 & 24.1 & 25.6 & 27.2 & 26.7 & 30.3 \\  & CheXpert & 21.3 & 19.5 & 23.7 & 26.0 & 26.6 & 26.0 & 29.6 \\ \hline \multirow{2}{*}{Multi Disease Identification} & OpenI & 48.5 & 54.4 & **59.2** & 46.1 & 52.8 & 53.9 & 55.6 \\  & MIMIC-CXR & 30.0 & 25.3 & 39.0 & 14.7 & 23.3 & 28.7 & **55.3** \\  & CheXpert & 4.3 & 6.1 & 3.9 & 7.1 & 23.6 & 2.1 & **52.1** \\ \hline \multirow{2}{*}{Visual Question Answering} & Rad-Restrant & 41.2 & 42.4 & 38.6 & 45.5 & 48.5 & 34.9 & **57.1** \\  & SL-ACE & 74.3 & 86.4 & 52.4 & 64.8 & 58.0 & 55.5 & 78.1 \\ \hline \multirow{2}{*}{Image-Text Reasoning} & OpenI & 47.9 & 52.6 & 52.4 & 54.7 & 54.0 & 45.8 & **59.0** \\ \cline{2-2}  & Findings Section Generation & CheXpert & - & - & 9.0 & 1.7 & 5.1 & 4.2 & **15.6** \\ \hline \multirow{2}{*}{Findings Summarization} & MIMIC-CXR & - & - & - & - & - & - & **40.3** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison between CheXagent and general domain and medical domain FMs on CheXbench. For image perception tasks, we report accuracy; For Findings Generation and Findings Summarization, we report RadGraph Score and Rouge-L, respectively.

## References

* [1]S. Baen, D. Kyung, J. Ryu, E. Cho, G. Lee, S. Kweon, J. Oh, L. Li, T. Chang, et al. (2023) EHRXQA: A multi-modal question answering dataset for electronic health records with chest X-ray images. arXiv preprint arXiv:2310.18652. Cited by: SS1.
* [2]S. Bannur, S. Hyland, Q. Liu, F. Perez-Garcia, M. Ilse, D. C. de Castro, B. Boecking, H. Sharma, K. Bouzid, A. Schwaighofer, et al. (2023) MS-CXR-T: learning to exploit temporal structure for biomedical vision-language processing. Cited by: SS1.
* [3]A. Ben Abacha, S. A. Hasan, V. V. Datla, D. Demner-Fushman, and H. Muller (2019) Vqa-med: overview of the medical visual question answering task at imageclef 2019. In Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes, pp. 9-12. Cited by: SS1.
* [4]B. Boecking, N. Usuyama, S. Bannur, D. Castro, M. Schwaighofer, S. Hyland, M. Wetscherek, T. Naumann, A. Nori, J. Alvarez-Valle, et al. (2022) Making the most of text semantics to improve biomedical vision-language processing. In European conference on computer vision, pp. 1-21. Cited by: SS1.
* [5]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselt, E. Brunskill, et al. (2021) On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. Cited by: SS1.
* [6]A. Bustos, A. Pertusa, J. Salinas, and M. De La Iglesia-Vaya (2020) Padchest: a large chest x-ray image dataset with multi-label annotated reports. Medical image analysis66, pp. 101797. Cited by: SS1.
* [7]Z. Chen, M. Varma, X. Wan, C. Langlotz, and J. Delbrouck (2023) Toward Expanding the Scope of Radiology Report Summarization to Multiple Annotations and Modalities. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Toronto, Canada, pp. 469-484. Cited by: SS1.
* [8]D. Demner-Fushman, S. Antani, M. Simpson, and G. R. Thoma (2012) Design and development of a multimodal biomedical information retrieval system. Journal of Computing Science and Engineering6 (2), pp. 168-177. Cited by: SS1.
* [9]S. Feng, D. Azzollini, J. S. Kim, C. Jin, S. P. Gordon, J. Yeoh, E. Kim, M. Han, A. Lee, et al. (2021) Curation of the candid-ptx dataset with free-text reports. Radiology: Artificial Intelligence3 (6), pp. e210136. Cited by: SS1.
* [10]G. Holste, S. Wang, A. Jaiswal, Y. Yang, M. Lin, Y. Peng, and A. Wang (2023) CXR-LT: multi-label long-tailed classification on chest X-Rays. PhysioNet. Cited by: SS1.
* [11]X. Hu, L. Gu, Q. An, M. Zhang, L. Liu, K. Kobayashi, T. Harada, R. M. Summers, and Y. Zhu (2023) Expert knowledge-aware image difference graph representation learning for difference-aware medical visual question answering. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 4156-4165. Cited by: SS1.
* [12]J. Irvin, P. Rajpurkar, M. Mo, Y. Yu, S. Curea-Ilcus, C. Chute, H. Marklund, B. Haghgoo, R. Ball, K. Shpanskaya, et al. (2019) Chexpert: a large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33, pp. 590-597. Cited by: SS1.
* [13]S. Jaeger, S. Candemir, S. Antani, Y. Wang, X. Ju, X. Lu, and G. Thoma (2014) Two public chest X-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery4 (6), pp. 475. Cited by: SS1.
* [14]A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. (2023) MIST 7B. arXiv preprint arXiv:2310.06825. Cited by: SS1.
* [15]A. E. Johnson, T. J. Pollard, N. R. Greenbaum, M. P. Lungren, C. Deng, Y. Peng, Z. Lu, R. G. Mark, S. J. Berkowitz, and S. Horng (2019) MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042. Cited by: SS1.
* [16]M. Kayser, C. Emde, O. Camburu, G. Parsons, B. Papiez, and T. Lukasiewicz (2022) Explaining chest x-ray pathologies in natural language. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 701-713. Cited by: SS1.
* [17]B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan (2023) Seed-bench: benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125. Cited by: SS1.
* [18]C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao (2023) Llava-med: training a large language-and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890. Cited by: SS1.
* [19]J. Li, D. Li, S. Savarese, and S. Hoi (2023) BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597. Cited by: SS1.
* [20]Y. Miura, Y. Zhang, E. Tsai, C. Langlotz, and D. Jurafsky (2021) Improving factual completeness and consistency of image-to-text radiology report generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5288-5304. Cited by: SS1.
* [21]M. Moor, Q. Huang, S. Wu, M. Yasunaga, C. Zakka, Y. Dalmia, E. P. Reis, P. Rajpurkar, and J. Leskovec (2023) Med-flamingo: a multimodal medical few-shot learner. arXiv preprint arXiv:2307.15189. Cited by: SS1.
* [22]H. Nguyen, K. Lam, L. T. Le, H. H. Pham, D. Q. Tran, D. B. Tran, D. D. Dinh, et al. (2022) VinDr-cXR: an open dataset of chest X-rays with radiologist's annotations. Scientific Data9 (1), pp. 429. Cited by: SS1.
* [23]M. Pavlova, T. Tuinstra, H. Aboutalebi, A. Zhao, H. Gunraj, and A. Wong (2022) COVIDX CXR-3: a large-scale, open-source benchmark dataset of chest X-ray images for computer-aided COVID-19 diagnostics. arXiv preprint arXiv:2206.03671. Cited by: SS1.
* [24]O. Palka, S. Koitka, J. Ruckert, F. Nensa, and C. M. Friedrich (2018) Radiology objects in COntext (ROCO): amultimodal image dataset. In _Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis_, 180-189. Springer.
* Pellegrini et al. (2023) Pellegrini, C.; Keicher, M.; Ozsoy, E.; and Navab, N. 2023. Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. In _International Conference on Medical Image Computing and Computer-Assisted Intervention_, 409-419. Springer.
* Pham et al. (2022) Pham, H. H.; Tran, T. T.; and Nguyen, H. Q. 2022. VinDr-PCXR: An open, large-scale pediatric chest X-ray dataset for interpretation of common thoracic diseases. _PhysioNet_.
* Reis et al. (2022) Reis, E. P.; de Paiva, J. P.; da Silva, M. C.; Ribeiro, G. A.; Paiva, V. F.; Bulgarelli, L.; Lee, H. M.; Santos, P. V.; Brito, V. M.; Amaral, L. T.; et al. 2022. BRAX, Brazilian labeled chest x-ray dataset. _Scientific Data_, 9(1): 487.
* Shih et al. (2019) Shih, G.; Wu, C. C.; Halabi, S. S.; Kohli, M. D.; Prevedello, L. M.; Cook, T. S.; Sharma, A.; Amorosa, J. K.; Arteaga, V.; Galperin-Aizenberg, M.; et al. 2019. Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia. _Radiology: Artificial Intelligence_, 1(1): e180041.
* Thawkar et al. (2023) Thawkar, O.; Shaker, A.; Mullappilly, S. S.; Cholakkal, H.; Anwer, R. M.; Khan, S.; Laaksonen, J.; and Khan, F. S. 2023. Xraygpt: Chest radiographs summarization using medical vision-language models. _arXiv preprint arXiv:2306.07971_.
* Vaya et al. (2020) Vaya, M. D. L. I.; Saborit, J. M.; Montell, J. A.; Pertusa, A.; Bustos, A.; Cazorla, M.; Galant, J.; Barber, X.; Orozco-Beltran, D.; Garcia-Garcia, F.; et al. 2020. BIMCV COVID-19+: a large annotated dataset of RX and CT images from COVID-19 patients. _arXiv preprint arXiv:2006.01174_.
* Wang et al. (2017) Wang, X.; Peng, Y.; Lu, L.; Lu, Z.; Bagheri, M.; and Summers, R. M. 2017. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 2097-2106.
* Wu et al. (2023) Wu, C.; Zhang, X.; Zhang, Y.; Wang, Y.; and Xie, W. 2023. Towards generalist foundation model for radiology. _arXiv preprint arXiv:2308.02463_.
* Xue et al. (2015) Xue, Z.; Candemir, S.; Antani, S.; Long, L. R.; Jaeger, S.; Demner-Fushman, D.; and Thoma, G. R. 2015. Foreign object detection in chest X-rays. In _2015 IEEE international conference on bioinformatics and biomedicine (BIBM)_, 956-961. IEEE.
* Yu et al. (2023) Yu, F.; Endo, M.; Krishnan, R.; Pan, I.; Tsai, A.; Reis, E. P.; Fonseca, E. K. U. N.; Lee, H. M. H.; Abad, Z. S. H.; Ng, A. Y.; et al. 2023. Evaluating progress in automatic chest x-ray radiology report generation. _Patterns_, 4(9).
* Zhang et al. (2023) Zhang, X.; Wu, C.; Zhao, Z.; Lin, W.; Zhang, Y.; Wang, Y.; and Xie, W. 2023. Pmc-vqa: Visual instruction tuning for medical visual question answering. _arXiv preprint arXiv:2305.10415_.