###### Abstract

Discharge summaries are essential yet time-consuming documents octores write at the end of a patient's hospital stay. They are the primary form of communication between hospital and community care teams. The automatic generation of summaries could reduce the administrative burden on doctors. We propose to use large language models, few-shot prompted by clinical guidance, to perform this task. Unlike previous supervised approaches, our method does not require a large training dataset, can accept full-length physician notes as inputs and is explicitly guided by clinical best practice. We implemented such a system using Royal College of Physicians London guidelines, GPT-4-turbo and MIMIC-III physician notes. 53 summaries were evaluated by 11 clinicians and found to have a micro accuracy of 0.81. Finally, we discuss methodical limitations and the required future improvements to the evaluation framework.

Automated Generation of Hospital Discharge Summaries Using Clinical Guidelines and Large Language Models Simon Ellershaw\({}^{1}\), Christopher Tomlinson\({}^{1,2,3}\), Oliver Burton\({}^{4}\), Thomas Frost\({}^{1}\), John Gerrard Hanrahan\({}^{4,5}\), Danyal Z Khan \({}^{4,5}\), Hugo Layard Horsfall\({}^{4,5}\), Mollie Little\({}^{4}\), Evaleen Malgapo\({}^{1}\), Joachim Starup-Hansen\({}^{4}\), Jack Ross\({}^{4}\), George Woodward\({}^{4}\), Martinique Vella-Baldacchino\({}^{6}\), Kawsar Noor\({}^{1,2,3}\), Anoop D Shah\({}^{1,2,3}\) Richard JB Dobson\({}^{1,2,3,7}\)

\({}^{1}\)Institute of Health Informatics, University College London, London, United Kingdom

\({}^{2}\)National Institute for Health and Care Research Biomedical Research Centre, University College London Hospitals National Health Service Foundation Trust, London, United Kingdom

\({}^{3}\)Health Data Research UK, London, United Kingdom

\({}^{4}\)University College London Hospitals NHS Foundation Trust, London, UK

\({}^{5}\)Wellcome/EPSRC Centre for Interventional and Surgical Sciences, University College London, London, UK

\({}^{6}\)MSK Lab, Imperial College London, London, UK

\({}^{7}\)Department of Biostatistics and Health Informatics, King's College London, London, UK

Corresponding Author: simon.ellershaw.20@ucl.ac.uk

## Introduction

A clinician must write a discharge summary at the end of every patient's hospital stay. The summary communicates to the post-hospital care team what has happened to the patient during their hospital stay and their ongoing care plan [13]. However, this manual process adds to clinicians' workloads and can be of varying quality [16].

Therefore, the automation of this process using machine learning models has been proposed as a solution [2]. Current state-of-the-art approaches [14] fine-tune encoder-decoder models [17] to map a set of clinician notes to a discharge summary. However, this supervised approach faces challenges due to the limited training data, extended length of clinician notes and variable ground truth quality [18].

Recently, the scaling of the training and size of natural language auto-regressive transformers has led to a new class of models known as large language models (LLMs) [15]. LLMs have shown the ability to learn from a few examples, accept inputs over 100,000 words and attain state-of-the-art performance on several benchmark tasks, including text summarisation [19, 20]. Such model properties could solve several problems currently faced in the automatic generation of discharge summaries.

This work presents the first LLM-based discharge summary generator to be tested on full clinical notes and evaluated by clinicians. Our key contribution is the use of clinical guidelines to prompt the LLM with the desired format and content of a summary instead of learning this from the data.

## Methodology

We converted guidelines from the UK's Royal College of Physicians London (RCP) [15], see Fig 2, to a JSON schema. We excluded the medication section, which requires the non-trivial merging of structured e-prescribing data with the extraction of the reasons for any medication changes from the clinical notes.

Following this, we created a fixed prompt of a system message containing the JSON schema and a one-shot example generated from an exemplar RCP discharge summary [15]. For full details of this process see Appendix 2.

To test the efficacy of the method, we used the freely-available MIMIC-III v1.4 dataset [1, 1, 13, 14]. We filtered the notes table for hospital admissions for which a discharge summary exists and so could be generated and at least one physician note. Next, we removed extraneous characters, artefacts from the anonymity process and the notes were deduplicated by keeping only the first occurrence of aline of text.

For our experiments, we used GPT-4-turbo version 1106-Preview (OpenAI 2023a), with temperature=0, due to its strong benchmark performance (Liang et al., 2022) and 128k context window, which allowed all sets of tested physician notes to be accepted in a single query.

One round of qualitative evaluation was performed with a clinician using a sample of 5 hospital admissions. We used this feedback to adjust the description of a select number of fields. For a complete list see Table 2.

We evaluated the final system using a team of 11 UK-qualified doctors and physician associates with prior experience writing discharge summaries. After reading the physician notes and clinical guidelines, the clinicians were asked to evaluate the number of times the following errors occurred for each discharge summary field: missed severe, missed minor, additional hallucination and additional not relevant. A missed error was categorised as severe if it had the potential to meet the NHS England (NHS England National Patient Safety Team, 2023) definition of medium to severe levels of harm. Each clinician evaluated five summaries, of which one was duplicated with another clinician to allow the calculation of inter-annotator agreement.

## Results

53 discharge summaries were generated and evaluated. The median input physician notes length after de-duplication was 4996 tokens and the fixed prompt was 5057 tokens, measured using the cl100k_base tokeniser (OpenAI, 2021). The median inference time was 40.59s at a median API cost of $0.12. The model extracted 25.07\(\%\) of the generated elements verbatim from the input physician notes. For a further breakdown of these metrics, see Table 3.

We found the median number of errors per summary to be 7, with the error proportions to be 36.28\(\%\) missed severe, 27.44\(\%\) missed minor, 14.55\(\%\) added hallucination and 21.73\(\%\) added not relevant. One summary failed to conform to the JSON schema. We calculated the percentage agreement between annotators, see Eqn 8, to be 59.72\(\%\).

To calculate the performance metrics in Table 1, we used Equations 1-7, defining a missed error as a false negative and an addition error as a false positive. Table 4 shows a per-field view of the same results. The GP Practice section is excluded from analysis, as the GP is not a role in the American healthcare system and so the section was never filled.

## Discussion

While the metrics in Table 1 show promise for many fields, safety-critical errors, such as missed severe and hallucinations, highlight the challenges in using LLMs for discharge summarisation and the need for clinician-in-the-loop review at the point of use. However, this in turn poses the risk of automation bias arising over time

The evaluation of this work was limited to a single centre's ICU data due to data-availability, in scale due to the labour-intensive nature of clinical evaluation and the low inter-annotator agreement metric shows the variability of clinical review for this task. Therefore, the development of a clinical grounded, scalable and systematically repeatable evaluation framework is vital future work.

The key strength of this work is that, to the author's knowledge, it is the first to show the effectiveness of using clinical guidelines to prompt LLMs for administrative medical tasks, such as discharge summarisation. This overcomes the main limitations of supervised approaches, namely the need for large labelled datasets and the inherent biases encoded in training on real-world data of variable quality.

## Conclusion

This work proposes a method to generate draft hospital discharge summaries using clinical guidelines to prompt LLMs. Unlike supervised training, this requires only a single training example and explicitly follows current best practices. A team of clinicians evaluated such a system using GPT-4-turbo, RCP guidelines and physician notes from MIMIC-III to have a micro accuracy of 0.81. However, further development of the evaluation framework is required for the improvement and safe deployment to clinical practice of such a method.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Section** & **Recall** & **Precision** & **Acc** \\ \hline Admission Details & 0.90 & 0.95 & 0.85 \\ Allergies And Adverse Reaction & 0.98 & 1.00 & 0.98 \\ Clinical Summary & 0.76 & 0.92 & 0.71 \\ Diagnoses & 0.84 & 0.94 & 0.80 \\ Discharge Details & 0.93 & 0.96 & 0.89 \\ Patient Demographics & 1.00 & 0.84 & 0.84 \\ Plan And Requested Actions & 0.90 & 0.88 & 0.80 \\ Social Context & 0.96 & 0.88 & 0.84 \\ \hline Macro Average & 0.91 & 0.92 & 0.84 \\ Micro Average & 0.86 & 0.92 & 0.81 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Recall, precision and accuracy metrics per section for discharge summaries generated from MIMIC-III notes as evaluated by clinicians.

Figure 1: Shows the proposed method, which combines discharge summary guidelines and physician notes into an LLM prompt in order to produce a discharge summary for review by a clinician.

[MISSING_PAGE_FAIL:3]

Searle, T.; Ibrahim, Z.; Teo, J.; and Dobson, R. J. 2023. Discharge summary hospital course summarisation of in patient Electronic Health Record text with clinical concept guided deep pre-trained Transformer models. _Journal of Biomedical Informatics_, 141: 104358.

## Appendix 2-LLM Prompt

To form the LLM prompt, firstly, we take guidelines written by the RCP, see Fig 2, (Royal College of Physicians 2021) and using the title and description of each section convert

Figure 2: A copy of the RCP rib sheet outlining their guidelines for discharge summary writing (Royal College of Physicians 2021).

[MISSING_PAGE_FAIL:5]

Table 2 shows the alterations to the prompt descriptions after 1 round of qualitative clinical evaluation.

## Appendix 3- Metric Equations

In order to calculate the performance metrics shown in Tables 1 and 4, we first defined the evaluation of each field as a 4-dimensional vector (sum missing severe errors, sum missing minor errors, sum additional hallucination errors, sum additional not relevant errors).

From this definition we calculated the number of additional errors for a given field \(f\) summed across all generated summaries as the number of false positives, \(FP_{f}\) and likewise for missing errors and false negatives \(FN_{f}\). The number of positive predictions for a field, \(P_{f}\), is defined as either the length of list type fields or the number of sentences for string type fields. Therefore, the number of true positives, \(TP_{f}\), for a field \(f\) is

\[TP_{f}=P_{f}-FP_{f} \tag{1}\]

From this and given that true negatives do not exist in this framework, the field's precision, \(p_{f}\), recall, \(r_{f}\), F1, \(F1_{f}\) and accuracy, \(acc_{f}\) scores, can be calculated,

\[p_{f}=\frac{TP_{f}}{TP_{f}+FP_{f}}, \tag{2}\]

\[r_{f}=\frac{TP_{f}}{TP_{f}+FN_{f}}, \tag{3}\]

\[F1_{f}=2\times\frac{p_{f}\times r_{f}}{p_{f}+r_{f}}, \tag{4}\]

\[acc_{f}=\frac{TP_{f}}{TP_{f}+FP_{f}+FN_{f}}. \tag{5}\]

We found the average precision scores by averaging across all fields

\[p_{macro}=\frac{1}{|p|}\sum_{f}p_{f}. \tag{6}\]

Or by first pooling across fields

\[p_{micro}=\frac{\sum_{f}TP_{f}}{\sum_{f}TP_{f}+\sum_{f}FP_{f}}. \tag{7}\]

Similar equations hold for averaging recall, F1 and accuracy.

To calculate the inter-annotator agreement for the set of all doubly evaluated field, \(f\), we defined two 2-D vector (\(FN_{f1}\), \(FP_{f1}\)) and (\(FN_{f2}\), \(FP_{f2}\)) one for each evaluator. \(FN\) and \(FP\) were chosen as they are the evaluation defined inputs to Eqn 7. \(A_{o}\) was then calculated as

\[A_{o}=\frac{\sum_{f}\delta\{(FN_{f1},FP_{f1}),(FN_{f1},FP_{f1})\}}{|f|} \tag{8}\]

where the \(\delta\) function is defined as

\[\delta_{a,b}=\begin{cases}1,a=b\\ 0,a\neq b.\end{cases} \tag{9}\]

\begin{table}
\begin{tabular}{c c c} \hline \hline Section & Field & Change to Description \\ \hline Admission Details & Reason For Admission & Added- “This should be symptoms and not the diagnosis.” \\  & Admission Method & Remove- “May be autopopulated” \\ Diagnoses & Secondary Diagnoses & Added- “Do not include diagnoses made before this hospital admission.” \\ Clinical Summary & Procedures & Added- “Do not include procedures performed before this hospital admission.” \\  & Investigation Results & Added- “, chest x-ray, mri scan, etc. Each investigation is a separate element in the list.” \\ PlanAndRequestedActions & Post Discharge Plan and Requested Actions & Added- Do not include jobs that are still to be done in hospital before discharge.” \\  & Next Appointment Details & Added- “Note date and contact details if available.” \\ \hline \hline \end{tabular}
\end{table}
Table 2: A table showing the alterations made to the field descriptions of the RCP discharge summary JSON schema after 1 round of clinical evaluation.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{4}{c}{Percentile} \\  & 25th & 50th & 75th & Max \\ \hline De-Duplicated Physician Note Length / Tokens & 2793 & 4996. & 8772 & 95682 \\ Output Note Length / Tokens & 705 & 807 & 884 & 1234 \\ Inference Time / secs & 33.41 & 40.60 & 48.61 & 125.95 \\ Inference Cost / \$ & 0.10 & 0.12 & 0.16 & 1.04 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Table of system properties when tested on MIMIC-III notes. The fixed prompt length is 5057 tokens. We calculated token lengths using cl100k_base tokenizer (OpenAI 2021)

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Section & Field & Mean & Number & Proportion & Recall & Precision F1 & Acc \\  & & of Elements & of & Blank & & & \\  & & & & Values & & & \\ \hline Admission Details & Admission Method & 1.00 & 0.00 & 0.93 & 0.96 & 0.94 & 0.89 \\  & Reason For Admission & 1.00 & 0.00 & 0.79 & 0.92 & 0.85 & 0.74 \\  & Relevant Past Medical & 8.34 & 0.08 & 0.91 & 0.95 & 0.93 & 0.87 \\  & And Mental Health History & & & & & & \\ Allergies And Adverse Reaction & Causative Agent & 1.87 & 0.00 & 0.98 & 1.00 & 0.99 & 0.98 \\  & Description Of Reaction & 1.87 & 0.09 & 0.98 & 1.00 & 0.99 & 0.98 \\ Clinical Summary & Clinical Summary & 4.28 & 0.00 & 0.71 & 0.98 & 0.82 & 0.70 \\  & Investigation Results & 4.30 & 0.04 & 0.75 & 0.86 & 0.80 & 0.67 \\  & Procedures & 2.36 & 0.28 & 0.87 & 0.94 & 0.91 & 0.83 \\ Diagnoses & Primary Diagnosis & 1.00 & 0.00 & 0.83 & 0.94 & 0.88 & 0.79 \\  & Secondary Diagnoses & 3.45 & 0.13 & 0.84 & 0.94 & 0.89 & 0.80 \\ Discharge Details & Discharge Destination & 1.00 & 0.00 & 0.93 & 0.96 & 0.94 & 0.89 \\ Patient Demographics & Safety Alerts & 1.74 & 0.72 & 1.00 & 0.84 & 0.91 & 0.84 \\ Plan And Requested Actions & Information And Advice & 1.40 & 0.55 & 0.98 & 0.80 & 0.88 & 0.79 \\  & Given & & & & & \\  & Next Appointment Details & 1.00 & 0.72 & 1.00 & 0.89 & 0.94 & 0.89 \\  & Patient And Career Concerns Expectations And & 1.25 & 0.62 & 0.89 & 0.83 & 0.86 & 0.75 \\  & & Wishes & & & & & \\  & Post Discharge Plan And & 7.89 & 0.00 & 0.88 & 0.90 & 0.89 & 0.80 \\  & Requested Actions & & & & & & \\ Social Context & Social Context & 2.89 & 0.17 & 0.96 & 0.88 & 0.91 & 0.84 \\ \hline Macro Average & & & & & & 0.90 & 0.92 & 0.90 & 0.83 \\ Micro Average & & & & & & 0.86 & 0.92 & 0.89 & 0.81 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Evaluation metrics per discharge summary field, including mean number of elements and proportion of blank values per field as well as recall, precision, F1 and accuracy.