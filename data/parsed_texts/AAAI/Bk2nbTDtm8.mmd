# CRAFT-MD: A Conversational Evaluation Framework for Comprehensive Assessment of Clinical LLMs

Shreya Johri1\({}^{,}\)2, Jaehwan Jeong1\({}^{,}\)3, Benjamin A. Tran, MD4, Daniel I. Schlessinger, MD5, Shannon Wongyibulsin, MD, PhD6, Zhuo Ran Cai, MD4, Roxana Daneshjou, MD, PhD2,3, Pranav Rajpurkar, PhD1\({}^{,}\)3

Footnote 1: These authors contributed equally.

Footnote 2: Correspondence to: sjohri@g.harvard.edu

Footnote 3: These authors share co-senior authorship.

###### Abstract

The integration of Large Language Models (LLMs) into clinical diagnostics has the potential to transform patient-doctor interactions. However, the readiness of these models for real-world clinical application remains inadequately tested. This paper introduces the Conversational **R**easoning Assessment **F**ramework for **T**esting in **M**edicine (CRAFT-MD), a novel approach for evaluating clinical LLMs. Unlike traditional methods that rely on structured medical exams, CRAFT-MD focuses on natural dialogues, using simulated AI agents to interact with LLMs in a controlled, ethical environment. We applied CRAFT-MD to assess the diagnostic capabilities of GPT-4 and GPT-3.5 in the context of skin diseases. Our experiments revealed critical insights into the limitations of current LLMs in terms of clinical conversational reasoning, history taking, and diagnostic accuracy, emphasising the need to evaluate clinical LLMs beyond static exam-questions. The introduction of CRAFT-MD marks a significant advancement in LLM testing, aiming to ensure that these models augment medical practice effectively and ethically.

1Department of Biomedical Informatics, Harvard Medical School

2Department of Biomedical Data Science, Stanford University

3Department of Dermatology, Stanford University

4Department of Computer Science, Stanford University

5Medstar Georgetown University Hospital/Washington Hospital Center, Department of Dermatology

6Department of Dermatology, Northwestern University

7Division of Dermatology, David Geffen School of Medicine at the University of California, Los Angeles

## Introduction

Doctor-patient conversations enable physicians to uncover key details that guide their clinical decisions. However, the mounting pressure of escalating patient numbers, lack of access to care [11], short consultation times [13, 22], and the expedited adoption of telemedicine due to the COVID-19 pandemic [23] have presented formidable challenges to this conventional model of interaction. As these factors risk compromising the quality of history taking and thereby diagnostic accuracy [2], there is an urgent need for innovative solutions that can enhance the efficacy of these crucial conversations.

New advances in Large Language Models (LLMs), could present a potential solution to this problem [24, 25, 26, 27]. These AI models have the ability to engage in nuanced and complex conversations, making them ideal candidates for extracting comprehensive patient histories and assisting physicians in generating differential diagnoses [28, 29, 30]. However, a considerable gap remains in assessing these models' readiness for application in real-world clinical scenarios [22, 26, 25]. The predominant method for evaluating LLMs in the medical field involves medical exam-type questions, with a strong emphasis on multiple-choice formats [26, 27, 28]. Although there are instances where LLMs are tested on free-response and reasoning tasks [22, 26, 25], or for medical conversation summarization and care plan generation [26], these are less common. However, none of these assessments explore LLMs' ability for engaging in interactive patient conversations, a crucial aspect of their potential role in revolutionizing healthcare delivery.

## Methods

To address the evaluative shortfall, we propose a new framework for evaluation of clinical LLMs, called the **C**onversational **R**easoning **A**ssessment **F**ramework for **T**esting in **M**edicine (CRAFT-MD). CRAFT-MD allows multi-faceted testing of clinical abilities of LLMs, including medical history gathering and open-ended diagnosis, by employing AI agents in simulations to represent patients or graders, rather than relying completely on human evaluators. This strategy significantly enhances the scalability of evaluations and allows for broader and quicker testing, keepingpace with the rapid evolution of LLMs (Figure 1).

## Results

We applied the CRAFT-MD framework on 140 case vignettes focused on skin diseases, sourced from both an online question bank1 (100 cases) and 40 newly created cases, encompassing a variety of skin conditions seen in both primary care and specialist settings. Our evaluations focused on the performance of GPT-4 and GPT-3.5 (versions "gpt-4-0314" and "gpt-3.5-turbo-0301") across 10 simulations per case vignette, revealing several limitations in clinical LLMs' conversational reasoning abilities (Appendix Figure 1, Table 1). In 4-choice multiple choice questions (MCQs), multi-turn conversations decreased accuracy versus vignettes. Notably, multi-turn conversations did not improve over single-turn, but summarizing conversations into concise paragraphs increased accuracy, indicating inability to synthesize across dialogues. Importantly, vignettes had the highest accuracy compared to all conversational setups, indicating limitations in medical history gathering skills. Replacing 4-choice MCQs with free response questions (FRQs), we observed a further decrease in accuracy across all experimental setups, with similar trends for inability to synthesize information and take medical histories. Removing physical exam details further decreased accuracy, indicating potential benefit of multimodal integration in LLMs. Code and data for reproducing experimental results is available online2.

Footnote 1: [https://www.clinicaladvisor.com/](https://www.clinicaladvisor.com/)

Footnote 2: [https://github.com/rajpurkarlab/craft-md](https://github.com/rajpurkarlab/craft-md)

## Conclusion

Recent studies showing high diagnostic accuracy on medical exam questions for LLMs such as GPT-4 may present an overly optimistic outlook for clinical use case, as these evaluations overlook crucial real-world complexities. CRAFT-MD reveals significant deficiencies in LLMs' abilities to gather thorough patient histories, synthesize information over dialogues, and clinical reasoning for diagnosis without answer choices. This work emphasizes the need for responsible and comprehensive evaluation of clinical LLMs.

## Acknowledgments

S.J. is supported by the 2023 Quad Fellowship. We thank the Microsoft Accelerating Foundation Models Research (AFMR) program for providing Azure credits.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{2}{c}{**GPT-4**} & \multicolumn{2}{c}{**GPT-3.5**} \\ \hline
**Type** & **MCQ** & **FRQ** & **MCQ** & **FRQ** \\ \hline Vignette & 0.919 & 0.684 & 0.833 & 0.546 \\ Multi-turn conversation & 0.854 & 0.431 & 0.724 & 0.468 \\ Single-turn conversation & 0.868 & 0.581 & 0.745 & 0.383 \\ Summarized conversation & 0.856 & 0.607 & 0.810 & 0.474 \\ Multi-turn conversation & & & & \\ (without physical exam) & 0.774 & 0.324 & 0.642 & 0.318 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Experimental Results. MCQ = 4-choice Multiple Choice Questions; FRQ = Free Response Questions.

Figure 1: CRAFT-MD evaluates clinical LLMs through simulated doctor-patient consultations with a patient-AI agent using predefined case vignettes. The clinical LLM’s objective is to elicit essential medical history from the patient-AI agent and formulate a diagnosis. A grader-AI agent assesses the clinical LLM’s accuracy by comparing the clinical LLM’s diagnosis to the established ground truth diagnosis. Additionally, medical experts conducts qualitative analysis of the interactions among the clinical LLM, patient-AI agent, and grader-AI agent to thoroughly assess the LLM’s clinical reasoning.

## References

* A. Ali, O. Y. Tang, I. D. Connolly, J. S. Fridley, J. H. Shin, P. L. Zadnik Sullivan, D. Cielo, A. A. Oyleese, C. E. Dobberstein, A. E. Telfeian, Z. L. Gokaslan, and W. F. Asaad (2023)Performance of chatgPT, GPT-4, and Google Bard on a neurosurgery oral boards preparation question bank. Neurosurgery. Cited by: SS1.
* J. Au Yeung, Z. Kraljevic, A. Luintel, A. Balston, E. Idowu, R. J. Dobson, and J. T. Teo (2023)AI chatbots not yet ready for clinical use. Front. Digit. Health5, pp. 1161098. Cited by: SS1.
* J. W. Ayers, A. Poliak, M. Dredze, E. C. Leas, Z. Zhu, J. B. Kelley, D. J. Faix, A. M. Goodman, C. A. Longhurst, M. Hogarth, and D. M. Smith (2023)Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. JAMA Intern. Med.183 (6), pp. 589-596. Cited by: SS1.
* S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Y. Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang (2023)Sparks of artificial general intelligence: early experiments with GPT-4. Cited by: SS1.
* N. Fijacko, L. Gosak, G. Stiglic, C. T. Picard, and M. Douna (2023)Can chatgPT pass the life support exams without entering the american heart association course?. Resuscitation185 (109732), pp. 109732. Cited by: SS1.
* an open-source collection of medical conversational AI models and training data. Cited by: SS1.
* G. Irving, A. L. Neves, H. Dambha-Miller, A. Oishi, H. Tagashira, A. Verho, and J. Holden (2017)International variations in primary care physician consultation time: a systematic review of 67 countries. BMJ Open7 (10), pp. e017902. Cited by: SS1.
* T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon, C. Elpanio, M. Madriaga, R. Aggabao, G. Diaz-Candido, J. Maningo, and V. Tseng (2023)Performance of chatgPT on usdline: potential for AI-assisted medical education using target language models. PLOS Digit. Health2 (2), pp. e000198. Cited by: SS1.
* K. E. Lasser, D. U. Himmelstein, and S. Woolhandler (2006)Access to care, health status, and health disparities in the united states and canada: results of a cross-national population-based survey. Am. J. Public Health96 (7), pp. 1300-1307. Cited by: SS1.
* P. Lee, S. Bubeck, and J. Petro (2023)Benefits, limits, and risks of GPT-4 as an AI chatbot for medicine. N. Engl. J. Med.388 (13), pp. 1233-1239. Cited by: SS1.
* B. A. Lowell, C. W. Froelich, D. G. Federman, and R. S. Kirsner (2001)Dermatology in primary care: prevalence and patient disposition. J. Am. Acad. Dermatol.45 (2), pp. 250-255. Cited by: SS1.
* M. Moor, O. Banerjee, Z. S. H. Abad, H. M. Krumholz, J. Leskovec, E. J. Topol, and P. Rajpurkar (2023)Foundation models for generalist medical artificial intelligence. Nature616 (7956), pp. 259-265. Cited by: SS1.
* V. Nair, E. Schumacher, G. Tso, and A. Kannan (2023)DERA: enhancing large language model completions with dialog-enabled resolving agents. Cited by: SS1.
* H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz (2023)Capabilities of GPT-4 on medical challenge problems. Cited by: SS1.
* P. Rajpurkar, E. Chen, O. Banerjee, and E. J. Topol (2022)AI in health and medicine. Nat. Med.28 (1), pp. 31-38. Cited by: SS1.
* A. Sarraju, D. Bruemmer, E. Van Iterson, L. Cho, F. Rodriguez, and L. Laffin (2023)Appropriateness of cardiovascular disease prevention recommendations obtained from a popular online chat-based artificial intelligence model. JAMA329 (10), pp. 842-844. Cited by: SS1.
* N. H. Shah, D. Entwistle, and M. A. Pfeffer (2023)Creation and adoption of large language models in medicine. JAMA330 (9), pp. 866-869. Cited by: SS1.
* M. Shanahan, K. McDonell, and L. Reynolds (2023)Role play with large language models. Nature623 (7987), pp. 493-498. Cited by: SS1.
* J. Shaver (2022)The state of telehealth before and after the COVID-19 pandemic. Prim. Care49 (4), pp. 517-530. Cited by: SS1.
* K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Gole-Lewis, S. Pfohl, P. Payne, M. Senerviratne, P. Gamble, C. Kelly, A. Babiker, N. Scharli, A. Chowdhory, P. Mansfield, D. Demershman, D. Aguera Y Arcas, B. Webster, G. S. Corrado, Y. Matias, K. Chou, J. Tomaev, N. Liu, A. Rajkomar, J. Barral, C. Semturs, A. Karthikesalingam, and V. Natarajan (2023)Large language models encode clinical knowledge. Nature620 (7972), pp. 172-180. Cited by: SS1.
* E. Strong, A. DiGiammarino, Y. Weng, A. Kumar, P. Hosamani, J. Hom, and J. H. Chen (2023)Chatbot vs medical student performance on free-response clinical reasoning examinations. JAMA Intern. Med.183 (9), pp. 1028-1030. Cited by: SS1.
* J. L. C. Wong, R. C. Vincent, and A. Al-Sharqi (2017)Dermatology consultations: how long do they take?. Future Hosp. J.4 (1), pp. 23-26. Cited by: SS1.
* M. Wornow, Y. Xu, R. Thapa, B. Patel, E. Steinberg, S. Fleming, M. A. Pfeffer, J. Fries, and N. H. Shah (2023)The shaky foundations of large language models and foundation models for electronic health records. NPJ Digit. Med.6 (1), pp. 135. Cited by: SS1.

[MISSING_PAGE_EMPTY:4]