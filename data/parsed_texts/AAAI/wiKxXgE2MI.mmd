# Quality and Diversity Both Matters When Merging Models

Anonymous submission

###### Abstract

Generalization to distribution shifts is a primary goal in modern machine learning literature. Ensemble methods, including both output-space ensemble and weight-space ensemble (model merging), are renowned for their robust generalization capabilities over multi-task settings, leveraging the diverse features from source models to improve cross-task transferability. While most studies on model merging focus on constructing diverse pools of task vectors obtained from foundation models trained on different tasks, we also emphasize the quality of each source. In this paper, we introduce a novel method for selectively merging task vectors to achieve superior generalization on target domains. Our approach uniquely considers both the diversity and quality of individual models. Using Determinantal Point Processes (DPP), we propose a probabilistic framework that optimally selects which models to average in a plug-and-play manner, ensuring a balanced consideration of quality and diversity. Theoretical support is provided for our hypothesis that this dual consideration yields a tighter generalization error bound for the unified model. Empirically, we present experiments in an out-of-distribution setting where there is significant violation in identically distributed conditions between the source and target domains.

## Introduction

In the modern era of machine learning, addressing the challenge of distribution shift is crucial, as the assumption of identical distribution between source and target domains may not hold in real-world scenarios. The importance of this issue is magnified in the context of large-scale, foundation models that are typically fine-tuned on diverse sources of datasets. A promising solution to this challenge is merging deep learning models together. Deep ensembles, which are combinations of diverse models, are known for their ability to generalize well on distribution shifts due to diverse features from the source models, enhancing the model's ability to transfer across various tasks. In practical settings, we already possess a variety of fine-tuned models with bless of foundation models and abundance of datasets. Therefore, selectively averaging weights based on quality and diversity in a training-free manner becomes essential.

In this paper, we introduce a novel method for selectively averaging neural networks to achieve solutions with superior generalization on target domains. Unlike existing methods, our approach explicitly considers both the diversity and quality of individual models. We propose a probabilistic framework that optimally selects models to average, ensuring a balanced consideration of both quality and diversity. We provide theoretical support for the hypothesis that considering both quality and diversity yields a tighter generalization error bound for the averaged model.

The contributions of this research are summarized as follows:

* We introduce a novel model merging strategy to exploit both the quality and diversity of source models.
* We provide a generalization error bound for ensemble classifier, supported by theoretical proof.
* Our method demonstrates superior performance in non-i.i.d. settings where the assumption of identical distribution in the source domain is violated.

## Preliminaries and Related Works

### Averaging Model Weights

Averaging the weights of models is a powerful approach in finding good solution in deep learning. [17] posits that averaging weights leads to wider optima in the loss surface, thereby enhancing generalization ability. By simply averaging multiple checkpoints during the training process, the solution tends to converge to flatter minima compared to the traditional Stochastic Gradient Descent solution. Diverse Weight Averaging for Out-of-Distribution (OOD) Generalization (DiWA) [14] averages weights obtained from independent training runs that share the same initialization, thereby increasing functional diversity across the averaged models. This work explains the success of weight averaging in OOD scenarios by highlighting the empirical similarity between weight averaging and output ensembling.

Due to the abundance of fine-tuned models and efficiency of foundation models, modern weight averaging methodologies utilize diverse fine-tuned models. Model Soup [13] averages diverse fine-tuned weights that vary across hyperparameter configurations yielding good generalization ability under distribution shift. Model Rata-toulike [1] recycles diverse fine-tuned models for OOD generalization. This approach aims to maximize weight diversity by leveraging the diversity in auxiliary tasks. It averages multiple weights fine-tuned from different initializations, each trained on different auxiliary tasks. The rationale for this ensemble's improved generalization in OOD scenarios is that fine-tunings of the same pre-trained foundation model are linearly connected in the loss landscape, despite different initializations, thus allowing successful averaging and yielding a flatter solution.

### Model Merging for Multi-Task Learning (MTL)

Recently, impressed by arithmetic of embedding vectors in language models, weight averaging has been extended to merging task vectors [11], which are obtained by subtracting pre-trained weight from task-specific fine-tuned weights. Such extension enabled semantic insights on MTL. Several approaches have extended the idea of merging task vectors using various heuristics, such as resolving interference due to redundant parameter values and aligning signs of weights [23], or preserving the important parameters defined via Fisher Information Matrix [24]. [22] extended typical 8 computer vision classifications up to 20 tasks, while proposing novel heuristic by eliminating exclusively task-specific weights to improve general performance in MTL. To overcome its limitation in using equal merging coefficients, [22] introduced test time adaptation and layer-wise merging. [25] introduced MLP layer of Transformer to flexibly adapt to test tasks and its experiments extensively discussed the generalization and robustness capability of merging models in MTL.

### Determinantal Point Processes

Determinantal Point Processes (DPP) have gained considerable attention in the machine learning community due to their ability to model diversity and provide elegant solutions for subset selection problems. Originally introduced in the context of quantum physics, DPPs have since found applications in a variety of fields including computer vision [10], information retrieval [26], and recommendation systems [14]. One of the pioneering works in applying DPPs to machine learning is done by [17], which provided a comprehensive framework for DPPs and demonstrated their effectiveness in diverse subset selection tasks.

Given a set of data \(\mathcal{Y}=x_{1},\ldots,x_{N}\), a point process \(\mathcal{P}\) is a probability measure over the set of all subsets of \(\mathcal{Y}\). \(\mathcal{P}\) is a DPP if a random subset \(\mathbf{Y}\) sampled according to \(\mathcal{P}\) satisfies:

\[\mathcal{P}_{\mathbf{L}}(\mathbf{Y}=Y)=\frac{\det(\mathbf{L}_{Y})}{\sum_{Y \subset\mathcal{Y}}\det(\mathbf{L}_{Y})}=\frac{\det(\mathbf{L}_{Y})}{\det( \mathbf{L}+\mathbf{I})}\propto\det(\mathbf{L}_{Y})\]

The DPP kernel \(\mathbf{L}\) is characterized by a similarity matrix \(\mathbf{S}\), where \(S_{ij}\) defines the similarity between two items \((\mathbf{x}_{i},\mathbf{x}_{j})\).

## Methodology

### Notation

Let \(T\) represent the target or test domain and \(S\) represent the source or train domain. The distribution of the source domain is denoted as \(D_{S}=\{(x_{i},y_{i})\}\sim S\). We define \(h\in\mathcal{H}\) as a sampled classifier or hypothesis, where \(h:\mathcal{X}\rightarrow\mathcal{Y}\) and \(h\approx f(x,\theta_{m})\). Here, \(f\) is the labeling function (ground truth) parametrized by \(\theta_{m}\).

The classification performance of \(h\) for a single data point \((x,y)\) is measured by \(\ell(h(x),y)\). The expected loss (risk function) over all data points for an arbitrary data distribution \(D\) is defined as \(\mathcal{L}_{D}(h)=\mathbb{E}_{(x,y)\sim D}[\ell(h(x),y)]\), assuming that \(\mathcal{L}(h)\) is convex with a range of \([0,1]\).

The parameter or weight specifying each classifier \(h\) is denoted by \(\theta\), thus \(h=h(\cdot;\theta)\). Finally, \(\rho\) represents the ensemble distribution, or ensemble strategy.

**Problem Setup** Given \(N\) (fine-tuned) models and source data, we aim to choose \(M\) models to generate ensemble that can generalize well on target domain \(T\).

### Generalization Error Bound

We suggest the generalization error bound of selectively merged neural network classifier as follows:

**Proposition 1**.: _Target risk of weight averaged model is bounded by source risk of individual models and diversity of softmax outputs._

\[\mathcal{L}_{T}(h_{\text{WA}})=\mathcal{L}_{T}(h_{\text{ENS}})+O( \Delta^{2})\] \[\leq\frac{1}{M}\sum_{m=1}^{M}\mathcal{L}_{S_{m}}(\theta_{m})- \mathbb{D}(\rho)+d_{1}(D_{S},D_{T})+\nu+O(\Delta^{2})\]

* \(\mathcal{L}_{S_{m}}(\theta_{m})\) _is source risk of_ \(m\)_-th model._
* \(\mathbb{D}(\rho)\) _is diversity of the ensemble of selected models._
* \(d_{1}(D_{S},D_{T})\) _is the divergence between source and target domain._
* \(\nu\) _is the difference in labeling functions across the two domains._
* \(O(\Delta^{2})\) _is an approximation error between weight-space averaging and output-space averaging._

We start by approximating weight average \(h_{\text{WA}}\) with output ensemble \(h_{\text{ENS}}\). Various researches on weight averaging including [10], [11], and [12] have shown the relationship between weight-space averaging and output-space averaging by employing Taylor expansion.

**Lemma 1**.: _Suppose we are given \(\{\theta_{m}\}_{m=1}^{M}\) with \(M\) fine-tuned models. Denoting \(\Delta_{\theta_{M}}=\max_{m}\|\theta_{m}-\theta_{\text{WA}}\|_{2}\), \(\forall(x,y)\in\mathcal{X}\times\mathcal{Y}\):_

\[h_{\text{WA}}(x)=h_{\text{ENS}}(x)+O(\Delta_{\theta_{M}}^{2}),\] \[\ell(h_{\text{WA}}(x),y)=\ell(h_{\text{ENS}}(x),y)+O(\Delta_{ \theta_{M}}^{2}).\]

Then, according to [1], we bound target risk with respect to individual source risks and the divergence between two distributions. Here, the divergence and the constant \(\nu\) is irreducible.

**Lemma 2**.: _For a ensemble classifier (hypothesis) \(h_{\text{ENS}}\),_

\[\mathcal{L}_{T}(h_{\text{ENS}})\leq\mathcal{L}_{S}(h_{\text{ENS}})+d_{1}(D_{S},D_{T})+\nu,\]

\[\nu=\min\left\{\mathbb{E}_{D_{S}}\left[\left|f_{S}(x)-f_{T}(x)\right|\right], \mathbb{E}_{D_{T}}\left[\left|f_{S}(x)-f_{T}(x)\right|\right]\right\}\]

[MISSING_PAGE_FAIL:3]

presents two-dimensional embeddings of eight datasets obtained from models fine-tuned on Figure 1(a) GTSRB and Figure 1(b) RESISC45. A notable trend is the similarity in embeddings between RESISC45 (brown) and SUN397 (pink), which largely overlap. The selection of SUN397 but not RESISC45 by DPP indicates that the heuristic effectively samples diverse subsets of tasks for merging.

## Conclusion and Future Works

In this work, we propose a training-free model merging framework that explicitly considers both the diversity and quality of source models. To the best of our knowledge, this is the first approach to incorporate the quality of source models--beyond merely the diversity of ensemble members--and to provide a generalization error bound for the merged solution. The framework is advantageous in its plug-and-play compatibility, enabling seamless application to existing methods.

We validate the effectiveness of our approach both theoretically and empirically. By deriving a generalization error bound and conducting experiments on image classification tasks, we demonstrate that merging diverse fine-tuned models (task vectors) of high quality leads to improved generalization ability. Furthermore, by strategically selecting models for averaging, our method achieves superior performance in generalization and robustness, even under non-i.i.d. settings where the assumption of identically distributed input data does not hold in the test domain.

Looking ahead, we aim to extend this framework to large language models, building upon the foundation laid by existing works in the domain of language foundation models. Additionally, there is potential to enhance diversity further by leveraging larger task pools, as suggested by [24]. Future work will also seek to broaden the theoretical foundation by incorporating more intuitive measures for the generalization error bound, such as the flatness of the loss surface and the diversity of neural network weights, with connections to linear mode connectivity.

## References

* [Ben-David et al.2010] Ben-David, S.; Blitzer, J.; Crammer, K.; Kulesza, A.; Pereira, F.; and Vaughan, J. W. 2010. A theory of learning from different domains. _Machine learning_, 79: 151-175.
* [Cheng, Han, and Lu2017] Cheng, G.; Han, J.; and Lu, X. 2017. Remote sensing image

\begin{table}
\begin{tabular}{l|c c c c c c|c c c c} \hline \hline
**METHOD** & \multicolumn{6}{c|}{**SEN TASKS**} & \multicolumn{6}{c|}{**UNSEEN TASKS**} & \multirow{2}{*}{**AVG.**} \\  & SUN397 & & \multicolumn{1}{c}{CARS} & RESISC45 & \multicolumn{1}{c}{DTD} & \multicolumn{1}{c}{SVHN} & \multicolumn{1}{c}{GTSRB} & \multicolumn{1}{c}{**AVG.**} \\  & & & & & & & & & & \\ \hline TASK ARITHMETIC & 0.6338 & 0.6227 & 0.7532 & 0.5798 & 0.8466 & 0.804 & 0.7067 & 0.7727 & 0.4559 & 0.6143 & 0.6836 \\
**TASK ARITHMETIC+DPP** & 0.568 & 0.6478 & 0.7775 & 0.5936 & 0.8534 & 0.8238 & **0.7107** & 0.773 & 0.4756 & **0.6243** & **0.6891** \\ TIES-MERGING & 0.6719 & 0.6573 & 0.7737 & 0.5718 & 0.8852 & 0.8445 & **0.7341** & 0.7929 & 0.3670 & 0.5800 & 0.6955 \\
**TIES-MERGING+DPP** & 0.6853 & 0.5504 & 0.7938 & 0.5835 & 0.8922 & 0.8568 & 0.727 & 0.7916 & 0.4219 & **0.6067** & **0.6969** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Generalization results on two unseen tasks when merging ViT-B/32 models on six tasks.

Figure 1: Above figures show 2 dimensional visualization of embeddings of 8 tasks obtained from (a) GTSRB (b) RESISC45 fine-tuned weight.

\begin{table}
\begin{tabular}{l|c c c c c|c c c c c} \hline \hline
**METHOD** & **CARS** & **EUROSAT** & **RESISC45** & **GTSRB** & **AVG.** & **CARS** & **EUROSAT** & **RESISC45** & **GTSRB** & **AVG.** \\ \hline \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{CLEAN TEST SET} & \multicolumn{6}{c}{CORRUPTED TEST SET (MOTION BLUR)} \\ TIES-MERGING (4) & 65.2 & 83.3 & 78.1 & 67.4 & 73.5 & 64.4 & 53.9 & 76.4 & 57.1 & 62.9 \\ TIES-MERGING (8) & 63.1 & 73.1 & 73.4 & 76.2 & 71.5 & 61.1 & 47.5 & 70.3 & 65.3 & 61.05 \\
**TIES-MERGING+DPP** & 64.5 & 77.0 & 77.4 & 83.1 & **75.5** & 62.3 & 52.0 & 74.4 & 73.6 & **65.6** \\ \hline \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{CORRUPTED TEST SET (IMPULES NOISE)} & \multicolumn{6}{c}{CORRUPTED TEST SET (GAUSSIAN NOISE)} \\ TIES-MERGING (4) & 60.2 & 45.6 & 69.8 & 38.3 & 53.5 & 61.8 & 47.3 & 73.1 & 42.3 & 56.1 \\ TIES-MERGING (8) & 59.5 & 46.9 & 65.5 & 52.6 & 56.1 & 61.0 & 43.6 & 68.0 & 56.1 & 57.17 \\
**TIES-MERGING+DPP** & 59.9 & 51.2 & 70.1 & 59.9 & **60.3** & 61.51 & 48.1 & 72.43 & 63.42 & **61.54** \\ \hline \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{CORRUPTED TEST SET (PIXELATE)} & \multicolumn{6}{c}{CORRUPTED TEST SET (SPATTER)} \\ TIES-MERGING (4) & 3.3 & 31.8 & 18.0 & 58.5 & 27.9 & 61.3 & 52.9 & 70.3 & 48.1 & 58.2 \\ TIES-MERGING (8) & 2.9 & 31.7 & 15.0 & 72.9 & 30.62 & 59.6 & 52.5 & 66.2 & 65.8 & 61.03 \\
**TIES-MERGING+DPP** & 2.9 & 34.4 & 15.4 & 80.0 & **33.2** & 58.6 & 56.4 & 68.6 & 76.9 & **65.1** \\ \hline \multicolumn{1}{c|}{} & \multicolumn{6}{c|}{CORRUPTED TEST SET (CONTRAST)} & \multicolumn{6}{c}{CORRUPTED TEST SET (JPEG COMPRESSION)} \\ TIES-MERGING (4) & 64.2 & 52.4 & 74.8 & 63.5 & 63.7 & 65.0 & 59.5 & 77.9 & 53.2 & 63.9 \\ TIES-MERGING (8) & 60.9 & 47.3 & 68.4 & 70.9 & 61.88 & 62.9 & 54.1 & 73.2 & 67.0 & 64.3 \\
**TIES-MERGING+DPP** & 62.4 & 52.0 & 72.9 & 78.9 & **66.6** & 64.5 & 58.3 & 77.3 & 73.5 & **68.4** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Robustness results on four corrupted tasks when merging ViT-B/32 models.

scene classification: Benchmark and state of the art. _Proceedings of the IEEE_, 105(10): 1865-1883.
* Cimpoi et al. (2014) Cimpoi, M.; Maji, S.; Kokkinos, I.; Mohamed, S.; and Vedaldi, A. 2014. Describing textures in the wild. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, 3606-3613.
* Dhaheim et al. (2023) Dhaheim, N.; Mollenhoff, T.; Ponti, E. M.; Gurevych, I.; and Khan, M. E. 2023. Model merging by uncertainty-based gradient matching. _arXiv preprint arXiv:2310.12808_.
* Elfeki et al. (2019) Elfeki, M.; Couprie, C.; Riviere, M.; and Elhoseiny, M. 2019. Gdpp: Learning diverse generations using determinantal point processes. In _International conference on machine learning_, 1774-1783. PMLR.
* Helber et al. (2019) Helber, P.; Bischke, B.; Dengel, A.; and Borth, D. 2019. Euroast: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, 12(7): 2217-2226.
* Hendrycks and Dietterich (2019) Hendrycks, D.; and Dietterich, T. 2019. Benchmarking neural network robustness to common corruptions and perturbations. _arXiv preprint arXiv:1903.12261_.
* Ilharco et al. (2022) Ilharco, G.; Ribeiro, M. T.; Wortsman, M.; Gururangan, S.; Schmidt, L.; Hajishirzi, H.; and Farhadi, A. 2022. Editing models with task arithmetic. _arXiv preprint arXiv:2212.04089_.
* Izmailov et al. (2018) Izmailov, P.; Podoprikhin, D.; Garipov, T.; Vetrov, D.; and Wilson, A. G. 2018. Averaging weights leads to wider optima and better generalization. _arXiv preprint arXiv:1803.05407_.
* Krause et al. (2013) Krause, J.; Stark, M.; Deng, J.; and Fei-Fei, L. 2013. 3d object representations for fine-grained categorization. In _Proceedings of the IEEE international conference on computer vision workshops_, 554-561.
* Kulesza et al. (2012) Kulesza, A.; Taskar, B.; et al. 2012. Determinantal point processes for machine learning. _Foundations and Trends(r) in Machine Learning_, 5(2-3): 123-286.
* LeCun et al. (1998) LeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11): 2278-2324.
* Liu et al. (2022) Liu, Y.; Walder, C.; and Xie, L. 2022. Determinantal Point Process Likelihoods for Sequential Recommendation. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_, 1653-1663.
* Matena and Raffel (2022) Matena, M. S.; and Raffel, C. A. 2022. Merging models with fisher-weighted averaging. _Advances in Neural Information Processing Systems_, 35: 17703-17716.
* Netzer et al. (2011) Netzer, Y.; Wang, T.; Coates, A.; Bissacco, A.; Wu, B.; Ng, A. Y.; et al. 2011. Reading digits in natural images with unsupervised feature learning. In _NIPS workshop on deep learning and unsupervised feature learning_, volume 2011, 4. Granada.
* Ortega et al. (2022) Ortega, L. A.; Cabanas, R.; and Masegosa, A. 2022. Diversity and generalization in neural network ensembles. In _International Conference on Artificial Intelligence and Statistics_, 11720-11743. PMLR.
* Radford et al. (2021) Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, 8748-8763. PMLR.
* Rame et al. (2022) Rame, A.; Kirchmeyer, M.; Rahier, T.; Rakotomamonjy, A.; Gallinari, P.; and Cord, M. 2022. Diverse weight averaging for out-of-distribution generalization. _Advances in Neural Information Processing Systems_, 35: 10821-10836.
* Song et al. (2018) Song, Y.; Yan, R.; Feng, Y.; Zhang, Y.; Zhao, D.; and Zhang, M. 2018. Towards a Neural Conversation Model with Diversity Net Using Determinantal Point Processes.
* Stalkamp et al. (2012) Stalkamp, J.; Schlipsing, M.; Salmen, J.; and Igel, C. 2012. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. _Neural networks_, 32: 323-332.
* Tang et al. (2024a) Tang, A.; Shen, L.; Luo, Y.; Hu, H.; Du, B.; and Tao, D. 2024a. Fusionbench: A comprehensive benchmark of deep model fusion. _arXiv preprint arXiv:2406.03280_.
* Tang et al. (2024b) Tang, A.; Shen, L.; Luo, Y.; Yin, N.; Zhang, L.; and Tao, D. 2024b. Merging Multi-Task Models via Weight-Ensembling Mixture of Experts. _arXiv preprint arXiv:2402.00433_.
* Wang et al. (2024) Wang, K.; Dimitriadis, N.; Ortiz-Jimenez, G.; Fleuret, F.; and Frossard, P. 2024. Localizing Task Information for Improved Model Merging and Compression. _arXiv preprint arXiv:2405.07813_.
* Wortsman et al. (2022a) Wortsman, M.; Ilharco, G.; Gadre, S. Y.; Roelofs, R.; Gontijo-Lopes, R.; Morcos, A. S.; Namkoong, H.; Farhadi, A.; Carmon, Y.; Kornblith, S.; et al. 2022a. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In _International conference on machine learning_, 23965-23998. PMLR.
* Wortsman et al. (2022b) Wortsman, M.; Ilharco, G.; Kim, J. W.; Li, M.; Kornblith, S.; Roelofs, R.; Lopes, R. G.; Hajishirzi, H.; Farhadi, A.; Namkoong, H.; et al. 2022b. Robust fine-tuning of zero-shot models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, 7959-7971.
* Xiao et al. (2010) Xiao, J.; Hays, J.; Ehinger, K. A.; Oliva, A.; and Torralba, A. 2010. Sun database: Large-scale scene recognition from abbey to zoo. In _2010 IEEE computer society conference on computer vision and pattern recognition_, 3485-3492. IEEE.
* Yadav et al. (2024) Yadav, P.; Tam, D.; Choshen, L.; Raffel, C. A.; and Bansal, M. 2024. Ties-merging: Resolving interference when merging models. _Advances in Neural Information Processing Systems_, 36.
* Yang et al. (2023) Yang, E.; Wang, Z.; Shen, L.; Liu, S.; Guo, G.; Wang, X.; and Tao, D. 2023. Adamerging: Adaptive model merging for multi-task learning. _arXiv preprint arXiv:2310.02575_.