# Memorize and Rank: Enabling Large Language Models for Medical Event Prediction

Anonymous submission

###### Abstract

Medical event prediction produces patient's potential diseases given their visit history. It is personalized yet requires an in-depth understanding of domain knowledge. Existing works integrate clinical knowledge into the prediction with techniques like concept embedding, patient records as knowledge graphs, and external knowledge bases, leaving the knowledge obtained through the pretraining of modern Large Language Models untouched. We introduce Mera, a clinical event prediction model that bridges pertaining natural language knowledge with medical code. We apply contrastive learning on a predicted ranking list for task-specialized optimization. With concept memorization through fine-tuning, we equip the LLM with an in-depth understanding to recall the natural language definitions for medical code during inference. Experimental results on MIMIC datasets show that Mera outperforms state-of-the-art models.

## 1 Introduction

Electronic Health Records, which store patient status and diagnoses made by physicians associated with timestamps, represent valuable domain expertise and clinical operation patterns [1]. The diagnosis judgments are made by clinicians based on their medical knowledge, which is largely acquired from years-long education grounded in textbooks and literature, as well as their years of accumulated experience, often implicit and mined in large-scale clinical data. Medical event prediction aims to predict future patient events given their history [13]. The events are normally presented in medical code format, such as ICD-9 disease codes, with a large candidate space to choose from (13,000 disease candidates in ICD-9) [14]. A reliable medical event prediction system enables efficiency improvement for hospital operations, early warning of potential diseases for patients [15], optimized clinical resource and facility allocation [2], and better risk estimation for sustainable insurance [16].

There are two significant challenges for medical event prediction, which have motivated many existing works but have still not been solved. First, what would be the best practice to introduce domain-specific clinical knowledge into the model? Existing works create concept initial embeddings from disease names, or enrich patient representation with external disease ontology. But there is still a significant gap between the primary knowledge modality, _i.e_. natural language, with the model's hidden representation. Second, how can we deal with the large candidate space when producing future event predictions and exploit the supervision signals that could be induced from the candidate space? Existing works mainly consider the task as a \(k\)-way classification task where \(k\) is the number of possible medical events that could happen and then apply binary cross entropy loss for each medical code. The dependencies among candidates and the structure of the medical code ontology are mostly ignored.

Generative Language Models (LM), especially recently introduced Large Language Models (LLM), are trained to predict the next token, follow task instructions, and align with human preference. These models excel in language understanding and reasoning capabilities, as shown in their performance on science-based benchmarks. Additionally, the LLMs absorb a large amount of knowledge mined in literature and online corpus during the pretraining stage. However, there is still an underexplored field of using LLM for medical event prediction due to the aforementioned gap between natural language and medical code and the gap between the token-level optimization process and the outcome-level large candidate output space. These challenges hinder the application of generative LM to diagnosis prediction tasks, while the state-of-the-art models are still graph neural network-based without full utilization of natural language knowledge [22, 23, 24]. There are works that use transformer-based LM for clinical outcome prediction, but they either have a heavy focus on using natural language medical notes as input [25, 26], or discard the pretrained knowledge [27, 28, 10].

To tackle these challenges, we propose Mera, an LLM for medical event prediction with medical code understanding and rich supervision over the output space. The patient's history diagnosis results are formulated as linear sequences and the LLM is expected to produce the probability distribution for the diagnosis result for the next visit. Different from the ordinary decoding process and token-level optimization, we produce the event prediction result from the probabilities distribution of producing corresponding tokens. We apply contrastive learning to force the model to distinguish true diagnosis events from false ones. The contrastive learning process is extended to multiple levels in the hierarchical organization of the medical code definitions. The model is learned to distinguish the true events from a pool of potential outcomes while the pool is getting more relevant to the true events. To equip the LM with an understanding of the medical code, we fine-tune the LM to memorize the mapping between medical code and their definitions.

We validate the effectiveness of our method in diagnosis prediction tasks on MIMIC-III and MIMIC-IV datasets. We observe that Mera yields significant improvement to the existing state-of-the-art medical event prediction model.

## 2 Method

### Task Formulation

The diagnoses for patients are represented with the ICD-9 code ontology \(O\) defined by domain experts, containing medical codes \(\{c_{1},c_{2},...,c_{|O|}\}\) where \(|O|\) is the total number of codes. The codes are organized as a tree structure where the codes are leaf nodes. For each medical code \(c\), it is part of the code groups \(\{g_{1}^{c},...,g_{depth(O)}^{c}\}\) according to its ancestors from the most coarse 1st level to the finest \(depth(O)\) level. There is also a one-to-one mapping between the code \(c\) and its natural language definition \(def_{c}\). For example, the medical code 250.23 stands for "Diabetes with hyperosmolarity, type I [juvenile type], uncontrolled". It belongs to a top-level group for all "Endocrine, Nutritional, and Metabolic Diseases and Immunity Disorders", and further belongs to the 2nd-level disease group, "Diabetes mellitus" and fine-grained disease group "type I uncontrolled diabetes". Given an electronic health records collection containing medical records for \(n\) patients \(\{P_{1},P_{2},...,P_{n}\}\), patient history diagnosis can be articulated as a sequence of admission instances \(\{V_{1}^{P_{i}},V_{2}^{P_{i}},...,V_{T}^{P_{i}}\}\) for a patient \(P_{i}\) where \(T\) is the number of existing visits. For a particular visit \(V_{j}\), the medical judgment made by clinicians as a result of the visit is an unordered set of diagnosis \(\{d_{1}^{V_{j}},d_{2}^{V_{j}},...,d_{m}^{V_{j}}\}\) in the format of \(m\) unique ICD-9 code (\(d\in O\)).

For the **diagnosis prediction** task, We aim to predict the diagnosis for the potential next visit \(V_{T+1}\) by selecting from the medical code ontology \(O\), which can be described as \(f_{dp}:\{V_{1},V_{2},...,V_{T}\}\to V_{T+1}\). For the **heart failure prediction** task, which can be described as a binary classification function \(f_{hf}:\{V_{1},V_{2},...,V_{T}\}\to 0,1\), we are more focused and aims to predict whether a patient would encounter heart failure in any of the future visits.

### Overview of Mera

Mera takes an existing large language model \(LM\) pretrained with a natural language corpus. The model takes the input sequence expressing the patient's history \(seq_{history}\) and outputs a probability distribution over the possible next token \(P\left(w_{1}^{o}\mid seq_{history}\right)\).

There are three steps involved as a pipeline: 1) Training the model to memorize medical codes used to represent the diagnoses; 2) Training the model to learn causal and temporal relations between visits and intra-visit patterns from patient diagnosis; 3) During inference, performing autoregressive generation to produce diagnosis prediction result given an unseen patient history input.

### Medical Concept Memorization

To bridge medical codes on \(O\) and the natural language knowledge learned through pertaining, we fine-tune \(LM\) on synthetic question-answering pairs to create a memorization of the code and natural language definition mapping.

Bidirectional code and definition memorization.For each code \(c_{i}\) and the natural language definition \(def_{c_{i}}\), we create two input-output pairs. The first pair prompts the LM with the question "What is the definition of ICD-9 code \(c_{i}\)" and the target answer of "\(def_{c_{i}}\)" to train the model recall definition given code. The second pair helps the model memorize the mapping reversely by asking "What is the ICD-9 code with the definition of "\(def_{c_{i}}\)" with an expected answer of "\(c_{i}\)". Note that we only apply loss on the completion given the input question while not requiring the model to learn the process of the reconstruction of the code or definition.

Coding ontology structure memorization.Besides the short-term memorization, memorization through fine-tuning enables us to embed dependency information among codes in \(LM\). For each category level \(1..depth(O)\) on the ICD-9 code ontology \(O\), the curated pairs map the code to its category, such as ("What is the first-level category of the ICD-9 code 998.51?", "Injury and Poisoning.").

### Capturing Inter-visit Relations

The second phase of fine-tuning aims to equip the model with temporal and causal understanding among diagnoses across patient visits. Given a patient history for patient \(P_{i}\), \(\{V_{1}^{P_{i}},V_{2}^{P_{i}},...,V_{T}^{P_{i}}\}\), we create a sequence of tokens \(Seq_{history}\) to represent the history as input of \(LM\). Diagnosis medical codes within a visit \(V_{i}\) are concatenated to form a token segment for a visit and the visit segments are further concatenated with a separator phrase to indicate a new visit.

Input sequence perturbation.The order of patient visits is crucial to convey the causal and dependent relation as the diagnosis in a later visit is conditioned on the previous diagnosis. However, the order of diagnosis codes within a particular visit does not matter as they are made simultaneously. An ideal medical event model should preserve the first kind of order while ignoring the second. To achieve this goal with a sequential LM, we propose to create multiple variants of the input patient history sequence. Each variant keeps the same visit order but shuffles the diagnosis code within each visit. By observing the data instances with shuffled order and the same target distribution, we teach the LM to ignore the order of diagnosis code with a model-agnostic design.

Optimization on ranking.After observing the input sequence of patient history, \(LM\) is expected to output the probability distribution of the first diagnosis code \(P\left(w_{1}^{o}\mid seq_{history}\right)\) of the upcoming visit \(V_{T+1}\). Ordinary language modeling applies cross-entropy loss on the probability of predicting the correct next token. We further provide fine-grained supervision on the probability distribution of the new token. The code candidate space includes\(\{c_{1},c_{2},...,c_{|O|}\}\). There is \(|pos|\) diseases that do happen in the next visit \(\{c_{1}^{pos},c_{2}^{pos},...,c_{|pos|}^{pos}\}\) and \(|neg|\) diagnosis that is not included in the next visit \(\{c_{1}^{heg},c_{2}^{neg},...,c_{|neg|}^{neg}\}\) among all code candidates (\(|pos|+|neg|=|O|\)). We obtain the probability of predicting each code candidate as the next token \(P\left(w_{1}^{o}\mid seq_{history}\right)=\{p_{1},p_{2},...,p_{|O|}\}\) by applying a softmax over the logits of all candidate codes. We then apply the training objective to the probability distribution.

**Label space-driven hierarchical contrastive learning.** We design training objectives to equip the model to identify the positive diagnosis among a group of candidate diagnoses sharing similar properties. With such a design, the model is forced to learn the subtle differences among similar diseases and specifically optimize itself for the diagnosis capability for various granularity of the decision space. For a particular \(i\)-th category level on the output candidate space \(O\), we identify positive diagnosis codes that do appear in the next visit. We then apply InfoNCE loss [1, 1, 13] shown below to parameterize the model to identify the correct diagnosis among all the same-category candidates. The produced loss for each subgroup is then added across different subgroups of the same ontology level and then further aggregated by the sum over different ontology levels.

### Modeling Intra-visit Dependencies

Besides training the model to reason between visits, there are many implicit, unspoken rules and dependencies mined in the large pool of diagnoses within each visit. For example, among the same group of similar diseases, the clinician normally only chooses the most representative code for the patient's status; some diseases might suppress or correlate with the appearance of other diseases. Without modeling the intra-visit dependencies, we ignore real-life clinic operation patterns, which could lead the model to yield similar but unrealistic diagnosis predictions. The diagnosis prediction made for a specific visit should consider other diagnoses for the same visit. To model the intra-visit dependencies, we create multiple teaching forcing training instances, each including a segment of the target diagnosis sequence to simulate the diagnosis process that partial diagnosis decisions are provided. For each instance, we include \(k\) diagnosis in the target diagnosis list \(V_{T+1}\) where \(k\in\{0,..,|V_{T+1}|\}\) in the end of the input sequence \(seq_{history}\). We then apply contrastive learning over the ranking list of the probability distribution of the next output token \(P\left(w_{k+1}^{o}\mid seq_{history}:w_{1}^{o},...,w_{k}^{o}\right)\). For the positive diagnosis code for the \(k+1\)-th output diagnosis token, we consider all diagnoses in \(V_{T+1}\) except \(w_{1}^{o},...,w_{k}^{o}\) as those diagnoses have been predicted in previous output steps.

### Autoregressive Decoding during Inference

After the two fine-tuning stages, learning the mapping between medical code and natural language knowledge and learning inferring diagnosis, the produced \(LM\) can be used to perform inference for unseen patient history. Given the \(seq_{history}\), \(LM\) performs autoregressive decoding to output the discrete diagnosis label with the highest probability in the ranking list for each output step until the end-of-sentence token is generated.

## 3 Experiments

### Experimental Setup

**Datasets.** We use MIMIC-III [15] and MIMIC-IV [15] EHR datasets containing patient records to evaluate the effectiveness of Mera. They are large, de-identified, and publicly available collections of medical records collected at the Beth Israel Deaconess Medical Center in Boston, Massachusetts, USA. The MIMIC-III dataset focuses on the patients who are eventually admitted to ICU, and the MIMIC-IV dataset includes both ICU patients and other patients. We conduct data preprocessing following previous works [13, 14]. We keep the patients with more than one visit in the datasets. We use the full ICD-9 code (instead of a simplified and higher-level code, which makes the task easier) to represent the diagnosis. To avoid data duplication during the overlapping time range between the two datasets, only the information of patients from MIMIC-IV with multiple visits between 2013 and 2019 was used. They are randomly split based on patients into training, validation and testing sets. For MIMIC-III and IV, the training, validation, and test sets contain 6000, 493, and 1000 patients, and 8000, 1000, and 1000 patients respectively. For patient history, we use the last visit as the label, while the earlier visit is input.

**Evaluation metrics.** For diagnosis prediction, we report the weighted F1 score and Recall@k metrics, where k is the number of top-ranked predicted diseases to consider. The weighted F1 score measures the accuracy of disease prediction by calculating the harmonic mean of precision and recall. They are both higher, the better. For heart failure prediction, we report AUC, which measures the area under the

Figure 1: The model design of Mera.

receiver operating characteristic curve, and F1 score, which evaluates the balance between precision and recall.

Baselines._RNN/CNN and attention-based models:_ RETAIN [20] employs two attention mechanisms to model two-way visit-disease mapping. Dipole [14] proposes a bidirectional RNN to address the issue of lengthy medical visit records. Timeline [1] designs an attention mechanism that combines time intervals and attention weights of each entity. HiTANet [21] employs a hierarchical temporal attention mechanism. Deepr [22] predicts future risks from medical records by converting records into discrete element sequences and using a CNN to detect predictive local clinical patterns. _Graph-based models:_ GRAM [20] employs the structure of medical ontologies. G-BERT [23] integrates pretrained language models and fully considers the hierarchical information found in ICD-9 codes. CGL [15] introduces a collaborative graph learning model. Chet [15] computes the diagnosis neighbor and global neighbor for each disease. MCDP [16] presents a methodology that uses hyperbolic space and multi-modal contrastive loss to preserve the hierarchical structure of diagnostic codes. KGxDP [23] formulates each patient as a personalized medical KG, combining medical KGs with patient admission history.

### Performance of Diagnosis Prediction

We show the performance on the diagnosis prediction task in Table 1. We observe that the graph-based models, in general, yield better performance compared with the RNN/CNN-based sequential model. The best-performing baseline would be the KGxDP model, which uses GNN to introduce spatial features and utilizes G-BERT pretraining knowledge to initialize the initial representation of the GNN.

We observe that our proposed model Mera achieves significantly better performance for both diagnosis prediction and heart failure prediction tasks for both datasets. There is an almost 9-point higher weighted F1 score and more than 8-point higher recall@20 for MIMIC-IV.

### Performance of Memorization

We further evaluate whether the trained model, after the medical concept memorization fine-tuning, can recall the medical code given definition sentences or recall the definition sentences given the ICD-9 code. We report code accuracy and definition accuracy in Table 2, and the model has to produce the exact same code and definition to count as a hit. We observe that the LM can remember the code-definition mapping almost perfectly, indicating the effectiveness of our proposed memorization technique.

## 4 Conclusion

By integrating domain-specific clinical knowledge and addressing the complexities of a large candidate space, Mera bridges the gap between natural language processing and medical code understanding. Our rigorous validation of MIMIC datasets has established Mera as a leading approach for medical event prediction.

## Ethical Considerations

The trained diagnosis prediction model inherits bias from multiple sources, including pre-training corpus, medical records distribution used for fine-tuning and more. The model should be fully evaluated before it is considered to be deployed in real clinical scenarios. The outcome of the diagnosis prediction model should not be used to serve as a factor to trace the discrimination label for specific diseases. Hospitals and insurance companies should not use the predicted diagnosis as the reason or motivation to change their service to patients.

## Reproducibility Statement

Our code will be released along with the published paper.

## References

* Y. An, H. Tang, B. Jin, Y. Xu, and X. Wei (2023)KAMPNet: multi-source medical knowledge augmented medication prediction network with multi-level graph contrastive learning. BMC Medical Informatics and Decision Making23 (1), pp. 243. Cited by: SS1.
* T. Bai, S. Zhang, B. L. Egleston, and S. Vucetic (2018)Interpretable representation learning for healthcare via capturing disease progression through time. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, London, United Kingdom, pp. 43-51. Cited by: SS1.
* J. H. Caufield, Y. Zhou, Y. Bai, D. A. Liem, A. O. Garlid, K. Chang, Y. Sun, P. Ping, and W. Wang (2019)A comprehensive typing system for information extraction from clinical narratives. medRxiv19009118. Cited by: SS1.
* E. Choi, M. T. Bahadori, L. Song, W. F. Stewart, and J. Sun (2017)GRAM: graph-based attention model for healthcare representation learning. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 787-795. Cited by: SS1.
* E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart (2016)RETAIN: an interpretable predictive model for healthcare using reverse time attention mechanism. In Advances in Neural Information Processing Systems, pp. 29. Cited by: SS1.
* W. Hsu, S. X. Han, C. W. Arnold, A. A. Bui, and D. R. Enzmann (2016)A data-driven approach for quality assessment of radiologic interpretations. Journal of the American Medical Informatics Association23 (1), pp. e152-e156. Cited by: SS1.
* A. E. W. Johnson, L. Bulgarelli, A. Shen, A. Gayles, A. Shammout, S. Horng, T. J. Pollard, S. Hao, B. Moody, B. Gow, L. Lehman, and R. G. Celi (2023)MIMIC-IV, a freely accessible electronic health record dataset. Scientific Data10 (1), pp. 1. Cited by: SS1.
* A. E. W. Johnson, T. J. Pollard, L. Shen, L. Lehman, M. H. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. Anthony Celi, and R. G. Mark (2016)MIMIC-III, a freely accessible critical care database. Scientific Data3 (1), pp. 160035. Cited by: SS1.
* R. Li and J. Gao (2022)Multi-modal contrastive learning for healthcare data analytics. In 2022 IEEE 10th International Conference on Healthcare Informatics (ICHI), pp. 120-127. Cited by: SS1.
* Y. Li, M. Mamouei, G. Salimi-Khorshidi, S. Rao, A. Hassaine, D. Canoy, T. Lukasiewicz, and K. Rahimi (2023)Hi-BEHRT: hierarchical transformer-based model for accurate prediction of clinical events using multimodal longitudinal electronic health records. IEEE Journal of Biomedical and Health Informatics27 (2), pp. 1106-1117. Cited by: SS1.
* C. Lu, T. Han, and Y. Ning (2022)Context-aware health event prediction via transition functions on dynamic disease graphs. Proceedings of the AAAI Conference on Artificial Intelligence36 (4), pp. 4567-4574. Cited by: SS1.
* C. Lu, C. K. Reddy, P. Chakraborty, S. Kleinberg, and Y. Ning (2021)Collaborative graph learning with auxiliary text for temporal event prediction in healthcare. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, Montreal, Canada, pp. 3529-3535. Cited by: SS1.
* J. Luo, M. Ye, C. Xiao, and F. Ma (2020)HiTANet: hierarchical time-aware attention networks for risk prediction on electronic health records. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, Virtual Event CA, pp. 647-656. Cited by: SS1.
* F. Ma, R. Chitta, J. Zhou, Q. You, T. Sun, and J. Gao (2017)Dipole: diagnosis prediction in healthcare via attention-based bidirectional recurrent neural networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1903-1911. Cited by: SS1.
* M. D. Ma, M. Chen, T. Wu, and N. Peng (2021)Hyper-expan: taxonomy expansion with hyperbolic representation learning. In Findings of the Association for Computational Linguistics: EMNLP 2021, Punta Cana, Dominican Republic, pp. 4182-4194. Cited by: SS1.
* Y. Meng, C. Xiong, P. Bajaj, P. Bennett, J. Han, X. Song, et al. (2021)Coco-lm: correcting and contrasting text sequences for language model pretraining. Advances in Neural Information Processing Systems34, pp. 23102-23114. Cited by: SS1.
* M. A. Morid, O. R. L. Sheng, and J. Dunbar (2023)Time series prediction using deep learning methods in healthcare. ACM Transactions on Management Information Systems14 (1), pp. 2:1-2:29. Cited by: SS1.
* P. Nguyen, T. Tran, N. Wickramasinghe, and S. Venkatesh (2017)Vmatht deepr: a convolutional net for medical records. IEEE Journal of Biomedical and Health Informatics21 (1), pp. 22-30. Cited by: SS1.
* S. Niu, J. Ma, L. Bai, Z. Wang, L. Guo, and X. Yang (2024)EHR-knowgen: knowledge-enhanced multimodal learning for disease diagnosis generation. Information Fusion102, pp. 102069. Cited by: SS1.

Oord, A. v. d.; Li, Y.; and Vinyals, O. 2018. Representation learning with contrastive predictive coding. _ArXiv preprint_, abs/1807.03748.
* Pang et al. (2021) Pang, C.; Jiang, X.; Kalluri, K. S.; Spotnitz, M.; Chen, R.; Perotte, A.; and Natarajan, K. 2021. CEHR-BERT: Incorporating Temporal Information from Structured EHR Data to Improve Prediction Tasks. In _Proceedings of Machine Learning for Health_, 239-260. PMLR.
* Rasmy et al. (2021) Rasmy, L.; Xiang, Y.; Xie, Z.; Tao, C.; and Zhi, D. 2021. Med-BERT: Pretrained Contextualized Embeddings on Large-Scale Structured Electronic Health Records for Disease Prediction. _npj Digital Medicine_, 4(1): 1-13.
* Rochefort et al. (2015) Rochefort, C. M.; Buckeridge, D. L.; and Forster, A. J. 2015. Accuracy of using automated methods for detecting adverse events from electronic health record data: a research protocol. _Implementation Science_, 10(1): 1-9.
* Shang et al. (2019) Shang, J.; Ma, T.; Xiao, C.; and Sun, J. 2019. Pre-Training of Graph Augmented Transformers for Medication Recommendation. In _Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence_, 5953-5959. Macao, China: International Joint Conferences on Artificial Intelligence Organization. ISBN 978-0-9992411-4-1.
* Wang et al. (2023) Wang, H.; Gao, C.; Dantona, C.; Hull, B.; and Sun, J. 2023. DRG-LLaMA : Tuning LLaMA Model to Predict Diagnosis-related Group for Hospitalized Patients. arxiv:2309.12625.
* Wu et al. (2023) Wu, J.; He, K.; Mao, R.; Li, C.; and Cambria, E. 2023. MEGACare: Knowledge-guided Multi-View Hypergraph Predictive Framework for Healthcare. _Information Fusion_, 100: 101939.
* Yadav et al. (2013) Yadav, K.; Sarioglu, E.; Smith, M.; and Choi, H.-A. 2013. Automated outcome classification of emergency department computed tomography imaging reports. _Academic Emergency Medicine_, 20(8): 848-854.
* Yang et al. (2023) Yang, Z.; Lin, Y.; Xu, Y.; Hu, J.; and Dong, S. 2023. Interpretable Disease Prediction via Path Reasoning over Medical Knowledge Graphs and Admission History. _Knowledge-Based Systems_, 281: 111082.
* Zhang et al. (2019)