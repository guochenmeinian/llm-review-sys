# CRAB: Assessing the Strength of Causal Relationships

Between Real-World Events

Angelika Romanou, Syrielle Montariol1, Debjit Paul1,

Leo Laugier, Karl Aberer, Antoine Bosselut

EPFL

{firstname.lastname}@epfl.ch

These authors contributed equally.

Copyright & 2024, Association for the Advancement of Artificial Intelligence (www.aai.org). All rights reserved.

###### Abstract

Understanding narratives requires reasoning about the cause-and-effect relationships between events mentioned in the text. While existing foundation models yield impressive results in many NLP tasks requiring reasoning, it is unclear whether they understand the complexity of the underlying network of _causal relationships_ of events in narratives. In this work, we present _CRAB_, a new **C**ausal **R**easoning **A**ssessment **B**enchmark designed to evaluate causal understanding of events in real-world narratives. _CRAB_ contains fine-grained, contextual causality annotations for \(\sim 2.7\)K pairs of real-world events that describe various newsurrowly event timelines (_e.g._, the acquisition of Twitter by Elon Musk). Using _CRAB_, we measure the performance of several large language models, demonstrating that most systems achieve poor performance on the task. Motivated by classical causal principles, we also analyze the causal structures of groups of events in _CRAB_, and find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains. We make our dataset and code available to the research community.

## Introduction

Understanding narratives requires understanding the cause-and-effect relationships between interconnected sub-events of those narratives. When reading text, humans immediately induce potential causal links between the events presented as part of a larger scenario [1, 1]. For example, in Figure 1, when reading an article about the acquisition of Twitter in 2022, a reader would implicitly assign causal links between events such as E2: "Elon Musk closes 44 billion dollar deal to buy Twitter" and E3: "Twitter delists from the NYSE".

However, building accurate causal mental models of the situations depicted in narratives poses several complex challenges. First, human causality judgments are rarely binary. Instead, they fall on a spectrum depending on human perception of other mediating or confounding events [1]. For example, in Figure 1, E2 is a mediator event for the causal relationship of E1 and E3, likely affecting the human perception of the causal relationship between E1 and E3. Second, causality judgments depend on the context depicting the events in question -- a context that can affect perceptions of causality. For example, a high causal judgment might be assigned between E4 and E6 in Figure 1. However, the introduction of new information about E5, another potential cause of E6, might downgrade the perceived intensity of a causal link between E4 and E6. Finally, because context is critically important to judging causal relationships of events, and most narratives offer an incomplete (and sometimes biased) reporting of particular scenarios, multiple sources may be required to paint an accurate picture of the causal relationships between multiple interconnected events.

Addressing these challenges, we introduce _CRAB_, a new **C**ausal **R**easoning **A**ssessment **B**enchmark that contains fine-grained, contextual causality annotations of real-world events that happened in the past ten years and received extensive media coverage. To collect the proposed benchmark, we design a crowdsourcing framework motivated by standard causal principles from cognitive science [1] and actual causality [1].

Figure 1: Events from _CRAB_ that lead to event E6, forming causal sub-structures with links of various causal strength.

2016) that study how humans perceive and express causality and responsibility among events. Using this knowledge frame, we automatically extract the events of newsworthy stories by integrating large pre-trained LMs into the dataset creation loop. We then construct causal graphs -- combinations of inter-connected events forming different causal chains and frames, as presented in Figure 2 -- from the extracted events and assess the strength of the causal relationships between these events using human annotators.

Our resulting benchmark, _CRAB_, contains \(\sim\)2.7K high-quality event pairs, their causal score, and the respective documents in which the events appeared. All the events are grouped into 1.8K causal frames and 352 causal chains. We use this benchmark to assess the abilities of state-of-the-art (SoTA) models to understand and reason over the causal relationships of real-world events present in a set of contexts (_i.e._, online news articles). Our analysis reveals that LLMs can capture explicit causal statements through pre-training, but they face difficulty applying causal reasoning to new scenarios, limiting their generalization and accuracy in offering predictions and explanations. We further stratify our results based on the structures of causal frames and chains, showing that they struggle with assessing the causality between events derived from complex causal structures compared to simple linear causal chains, especially when these events are extracted from different documents.

## Preliminaries on Causality

In this section, we define the main causality concepts that we use to create and analyze _CRAB_.

Actual CausalityActual causality refers to the causal relationship between specific events and their causes in the real world [11] and seeks to understand the precise mechanisms by which one event leads to another, going beyond mere correlation. Research in causal inference has attempted to formalize actual causality using causal models that map how humans perceive and attribute cause and responsibility to events and their outcomes. However, human perception of causality usually depends on background context, implicit biases, epistemic state, and lack of information, making the task of actual causality attribution challenging to formalize [10, 12]. Additionally, in cases where the responsibility of an event can be attributed to more than one preceding event, observers tend to assign different attribution to the contributing causes [13]. Therefore, when events are described with natural language, the causal judgments are not binary but relative, enabling comparisons between causal events [10].

Causal Frames & Causal ChainsHumans tend to attribute different degrees of causality between contributory events, relying primarily on domain and commonsense knowledge [11]. Causality research refers to this set of candidate events relevant to a particular outcome event as a Causal Frame [11]. We construct _CRAB_ to collect causal frame subgraphs, where each event is associated with its potential causes. Similarly, we explore the chain of events across time that leads to an outcome event E [14]. We define the causal chain of an outcome event as the set of paths ending at E in

Figure 2: _Left:_ Different structures of causal frames inspired by the responsibility assessment concepts presented in Halpern 2016; _right:_ causal chains [14] present in _CRAB_. The patterns in structures are formulated based on the different causal judgment scores among events. The colors of cause-nodes represent the causality strength they have towards the event-node E.

the event's causal graph. _CRAB_ leverages both concepts of causal chains and temporality, providing a testbed to assess the ability of language models to perform causal reasoning in different causal chain structures as depicted in Figure 2 (right).

## Dataset Construction

OverviewWe consider a set of documents covering a news story. Each document reports several events associated with that story. The time-ordered list of these extracted events defines a _timeline_ associated with the story. From a timeline and its set of documents, we can build a _causal event graph_. Our goal is to identify the causal relations between the events in the graph using the documents in which these events are mentioned. The full data creation pipeline is described in Figure 3.

Event ExtractionWe select news articles from 20 stories about major events that happened around the world and we extract the main events mentioned in each document. In contrast to prior work [23], we use a generative approach to extract the main events given a news piece. Specifically, we prompt GPT-3 (text-davinci-003, Ouyang et al.2022) to extract the main events from a given document (similar to veyseh2021). Because generative methods come with the limitation of hallucinations and wrong outputs, we manually filter extracted events to keep only the valid generations. After filtering, our dataset contains 384 unique events. As causality is conditioned on temporality, we manually create a timeline of the extracted events for each of the 20 stories, by considering all documents associated with the story. While building these timelines, we disambiguate the events mentioned in different documents by merging differently phrased instances of the same event.

Event Causality LinkingIn the final stage of our pipeline, we collect causality judgments about all event pairs extracted from the documents related to a specific story (2730 pairs). Motivated by the way cognitive studies capture judgments about actual causality [11], we define the causality between real-world events not as a single binary score but as a continuous value from 0 to 100, enabling finer analysis and predictions of causal judgment. We qualify 44 Amazon Mechanical Turk workers, and for each pair of events, task 7 workers with providing a judgment for the causal link between the events (see Appendix for details regarding annotators' agreement scores).

## Dataset Analysis

_CRAB_ consists of a set of 173 documents regarding 20 different stories discussing newsworthy real-world events. It contains 384 extracted unique event instances and 2730 event pairwise causality scores (see Tables 5 and 6 in Appendix for additional descriptive statistics). The experiments

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline \multirow{2}{*}{**Type of pairs**} & \multicolumn{3}{c}{**Pairwise Event Causality Score**} \\  & **Below 20** & **20-50** & **50-80** & **Above 80** \\ \hline
**In-doc** & 3.9 & 25 & 26.6 & 44.4 \\
**Cross-doc** & 13.1 & 37.6 & 25.3 & 24 \\ \hline
**All pairs** & 11.9 & 35.9 & 25.5 & 26.7 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Percentage of pairs present in the _CRAB_, per causality score class.

Figure 3: _CRAB_ data pipeline overview: We collect documents covering newsworthy stories, create a timeline with the main events extracted from the documents for each story, and crowdsource human annotations on the causal judgments between the events _(score 0 to 100)_. Based on these scores, we generate a causal graph for each story that can be filtered on different causal score thresholds. _CRAB_ can also be viewed from the perspective of causal frames and causal chains. Same-color events originate from the same document.

presented in the following section are based on these 4 classes reported in Table 1. We stratify the dataset and get the causal frame of each event and we categorize these subgraphs based on the strength of causal scores between them (in-degree edges of the causal frame graph). Similarly, we extract causal chains from _CRAB_ based on the three causal structures; _Mediation_, _Confounding_, and _Collision_(Pearl, 2009), depicted in Figure 2 (right).

## Experimental Setup

To evaluate how language models reason about causality, we define different experimental frameworks covering various causality assessment scenarios, similar to Kucman et al. (2023). We investigate three tasks in ascending order of complexity in terms of causal structure: pairwise causality inference, graded causality inference, and causal chain inference. We use two decoder-only instruction-following API-based models, _GPT-3_ (text-davinci-003, 175B size) and _GPT-4_, with the settings suggested by OpenAI (2023): a _temperature_ of 0.3 and a _maximum length_ of 256 tokens. We additionally test _CRAB_ using _Flan-Alpaca-GPT4-XL_(Chia et al., 2023), an open-source 3B size encoder-decoder model fine-tuned on instruction-following datasets: FLAN (Longpre et al., 2023) and GPT4-Alpaca.1

Footnote 1: [https://instruction-tuning-with-gpt-4.github.io/](https://instruction-tuning-with-gpt-4.github.io/)

Pairwise Causality InferenceTo evaluate Pairwise Causality Inference, we first prompt the model to generate a scalar causality score between two events given a context (the documents from which the events were extracted), mimicking the benchmark human annotations. We mapped the causality intervals to descriptions of different degrees of causality and augmented the prompt instructions with score ranges and their explanations. The 4 classes and definitions are as follows (i) _High causality_: a link between events that are definitely causally related to each other (causal score above 80), (ii) _Medium causality_: a link between events that might be slightly causally related to each other (causal score between 50 and 80), (iii) _Low causality_: a link between events that have a little causal connection (causal score between 20 and 50), and (iv) _No causality_: independent events (causal score lower than 20). We compare it with the average annotators' score in _CRAB_. The full prompt can be found in Table 12 in Appendix. We then evaluate the model's answer by mapping the generated score to the four classes and computing the Macro-F1 score (**Pairwise Causality Score** in Table 2). We also experiment with binary and multi-class classification tasks (**Binary Pairwise Causality** and **Multi-class Pairwise Causality** in Table 2, respectively), prompting the model to output a causality class instead of a raw score. The prompts for these tasks can be found in Tables 9 and 10 in Appendix.

Graded Causality InferenceAs previously described, one of the main concepts in actual causality is graded causation or responsibility (Halpern, 2016), which is the relative degree to which an event causally contributes to an effect. Thus, we go beyond pairwise causality and prompt models to rank the events that contributed more to the effect. We create a Multiple Choice Question (MCQ) Answering task that asks the model to provide the most contributory to an effect cause among several events. We construct the dataset for the experiment by using the causal frames of each event and selecting 4 possible causes. We then ask the model to select, based on these 4 choices, the cause with the highest causality score (see the prompt in Table 11 in Appendix).

Causal Chain InferencePairwise causality provides a strong indication of the causal relationship between two events. However, these events are usually part of a larger chain of events with complex causal patterns. In this experiment, we consider not only the relations between the causes

\begin{table}
\begin{tabular}{l l|c|c c|c c} \hline \hline
**Tasks** & **Models** & **All pairs** & **In-doc** & **Cross-doc** & **Pre-Jan 2022** & **Post-Jan 2022** \\ \hline
**Pairwise Causality Score** & Flan-Alpaca & 21.6 & 14.9 & 22.4 & 22.0 & 21.3 \\  & GPT-3 & 25.8 & 24.4 & 25.4 & 26.6 & 25.2 \\  & GPT-4 & **54.7** & **59.0** & **53.7** & **56.4** & **53.5** \\ \hline
**Multi-class Pairwise Causality** & Flan-Alpaca & 11.0 & 12.2 & 10.8 & 11.2 & 10.7 \\  & GPT-3 & 35.0 & 27.4 & 34.9 & 35.0 & 34.5 \\  & GPT-4 & **45.6** & **46.1** & **45.0** & **43.1** & **46.7** \\ \hline
**Binary Pairwise Causality** & Flan-Alpaca & 60.1 & 73.8 & 56.7 & 62.1 & 58.7 \\  & GPT-3 & 57.2 & 67.0 & 55.0 & 56.9 & 57.5 \\  & GPT-4 & **73.9** & **80.0** & **72.6** & **76.5** & **72.0** \\ \hline
**Graded Causality**_(MCQ)_ & Flan-Alpaca & 39.9 & 53.2 & 29.3 & 44.1 & 35.4 \\  & GPT-3 & **59.7** & **70.9** & **50.7** & **64.5** & **54.5** \\  & GPT-4 & 53.8 & 67.3 & 43.1 & 63.3 & 44.5 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Macro F1-scores on SoTA LLMs on all _Pairwise Causality Inference_ tasks and the _Graded Causality Inference_ MCQ task. For the MCQ task, we stratify the results for in-doc and cross-doc based on whether the effect & correct cause are extracted from the same document.

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

observational data: methods and benchmarks. _The Journal of Machine Learning Research_, 17(1): 1103-1204.

OpenAI, R. 2023. GPT-4 technical report. _arXiv_, 2303-08774.

Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35: 27730-27744.

O'Neill, K.; Quillien, T.; and Henne, P. 2022. A counterfactual model of causal judgment in double prevention. In _Conference in computational cognitive neuroscience_.

Pearl, J. 2009. Causal inference in statistics: An overview. _Statistics Surveys_, 3: 96-146.

Pearl, J.; and Mackenzie, D. 2018. _The book of why: the new science of cause and effect_. Basic books.

Roemmele, M.; Bejan, C. A.; and Gordon, A. S. 2011. Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. In _AAAI spring symposium: logical formalizations of commonsense reasoning_, 90-95.

Romanou, A.; Smeros, P.; Castillo, C.; and Aberer, K. 2020. Scilens news platform: a system for real-time evaluation of news articles. _arXiv preprint arXiv:2008.12039_.

Schick, T.; Dwivedi-Yu, J.; Dessi, R.; Raileanu, R.; Lomeli, M.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_.

Shi, P.; and Lin, J. 2019. Simple bert models for relation extraction and semantic role labeling. _arXiv preprint arXiv:1904.05255_.

Smeros, P.; Castillo, C.; and Aberer, K. 2019. Scilens: Evaluating the quality of scientific news articles using social media and scientific literature indicators. In _The World Wide Web Conference_, 1747-1758.

Srivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid, A.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; Garriga-Alonso, A.; et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _arXiv preprint arXiv:2206.04615_.

Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.

Tu, M.; Wang, G.; Huang, J.; Tang, Y.; He, X.; and Zhou, B. 2019. Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs. _arXiv preprint arXiv:1905.07374_.

Veyseh, A. P. B.; Lai, V. D.; Dernoncourt, F.; and Nguyen, T. H. 2021. Unleash GPT-2 Power for Event Detection. In _Annual Meeting of the Association for Computational Linguistics_.

Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.; Le, Q.; and Zhou, D. 2022. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_.

Welbl, J.; Stenetorp, P.; and Riedel, S. 2018. Constructing datasets for multi-hop reading comprehension across documents. _Transactions of the Association for Computational Linguistics_, 6: 287-302.

Wolff, P.; and Shepard, J. 2013. Causation, touch, and the perception of force. In _Psychology of learning and motivation_, volume 58, 167-202. Elsevier.

Yu, B.; Li, Y.; and Wang, J. 2019. Detecting causal language use in science findings. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, 4664-4674.

Zhang, C.; Bauer, S.; Bennett, P.; Gao, J.; Gong, W.; Hilmkil, A.; Jennings, J.; Ma, C.; Minka, T.; Pawlowski, N.; et al. 2023. Understanding causality with large language models: Feasibility and opportunities. _arXiv preprint arXiv:2304.05524_.

## Appendix

### Dataset Construction Details

News Article SelectionBased on a selection of 20 distinctive stories, we crawl the web and select the top 20 news articles per story. When extracting articles related to a story that happened many years before, we noticed that the retrieved articles also covered recent events that were loosely related to the story's main events. Therefore, we use a time window of 9 months when extracting the articles for each story to keep only articles that have been published around the time that the respective story happened.

\begin{table}
\begin{tabular}{c|c} \hline \hline
**Causal chain** & **\# Chains** \\ \hline
**Mediation** & 774 \\
**Confounding** & 924 \\
**Collider** & 115 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Number of chains for different types of causal chains (see Figure 2).

\begin{table}
\begin{tabular}{c|c c c} \hline \hline
**Causal frame** & **\# frames** & **\# pairs** & **In-doc** & **Cross-doc** \\ \hline
**SD** & 24 & 85 & 25 & 60 \\
**D** & 18 & 70 & 22 & 48 \\
**SC** & 30 & 254 & 26 & 228 \\
**C** & 98 & 789 & 103 & 686 \\
**N** & 33 & 100 & 18 & 82 \\
**M** & 149 & 1386 & 156 & 1230 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Number of frames for different types of causal frames (see Figure 2) along with the number of event pairs additionally stratified for the in- and cross-doc cases. SD is for Strong Direct Causality; D is for Direct Causality; SC is for Strong Contributory Causality; C is for Contributory Causality; N is for No Causality; M is for Mixed Causality.

Event ExtractionUsing a generative approach for event extraction has two main benefits, confirmed through extensive experimental analysis. First, when prompted correctly, generative models successfully output structured information at the requested semantic abstraction, which leads to higher precision when extracting events. Second, the semantic granularity of the events we want to extract is between sentence and document level, meaning that we aim for the main events covered in the article and not syntactic events as existing works use (Ebner et al., 2020). Similarly to Smeros, Castillo, and Aberer (2019) and Romanou et al. (2020), we filter the top 20 news articles per story. We remove the ones with less than 100 words and those with paywalls or provide re-directions to the original referred news article. Finally, we clip the article to 250 words and round to the end of the sentence token. We end up using a total number of 384 news documents as the main test bed for event extraction.

Crowdsourcing Causality ScoresFor the in-document annotations, we show the annotators the documents and ask them to assess 3 event pairs. For the cross-document ones, we give 2 documents at a time, along with 5 event pairs. Figure 4 depicts the pair creation process that served as input for Amazon Mechanical Turk experiments. To select native English speakers, we focus on the group of workers whose locations are in the USA.

We also ran a 2-phase qualification where we evaluated the quality of annotators on our task and selected the ones with a higher than 80% score on our qualification task. Finally, 44 out of 400 workers are selected as qualified. We pay each worker $0.80 for doing every 3 annotations for the in-document event pairs and $0.90 for doing 5 annotations for the cross-document pairs. Figures 6 and 7 depict the task instructions and annotation script used for crowdsourcing.

Inter-rater agreementWe have a total of 44 workers scoring the causality between 2730 pairs of events. All pairs are annotated by at least 7 annotators and, at most, 10 (around 21.3k annotations). We divide the causality score into 4 equal classes and compute Krippendorff's \(\alpha\). We consider the ground truth as the majority vote. Table 8 shows Krippendorff's \(\alpha\) for different groups of classes. As expected, the agreement to discriminate between the lowest causality class and the highest one is the highest, while it is harder for annotators to agree on discriminating between nearby classes. Krippendorff's \(\alpha\) for all classes is 0.27, and 0.33 for the further classes. We note that the high number of annotators per sample increases the raw number of disagreements. Moreover, contrary to classical annotation situations where a small number of annotators label each sample, the Amazon Turk settings involve many different annotators participating in a task. Thus each sample is annotated by different workers, augmenting the variance and decreasing the agreement rate.

Expert AnnotationGiven the low agreement, we select pairs where the average score falls on the boundary of the 4 classes and the variance between annotators is high to be validated by experts. This subset consists of 26.7% of the benchmark. This step is done by asking three expert annotators (NLP researchers who are familiar with the task of causal inference) further to annotate event pairs' causal scores and classes. Given the average causal score, the experts were asked to choose which of the neighboring classes was a better class for the event pair, updating the score accordingly. The inter-rater agreement, using Krippendorph's alpha, between experts is 0.70. These expert-validated causal scores and the remaining low-variance samples are used for _CRAB_.

Figure 4: Dataset construction pipeline. Once events are extracted from the documents, we order them on time and formulate story timelines. Conditioned on the temporal ordering, we create all combinations of events in the document and pass the in-document pairs to annotators. We perform a similar process for the event pairs extracted from different documents (cross-doc), including an extra step of merging document timelines into one before taking the pair combinations.

### Experimental Results with Fine-Tuned LMs

We initially evaluated our proposed dataset on decoder-only models because decoder-only models (especially API-based ones such as GPT-3 / 3.5 / 4) have become important pillars of AI products, motivating researchers to benchmark their capabilities and identify their biases and limitations. However, it is important to additionally evaluate our causal benchmark on different architectures and inference techniques, providing additional insight into the difficulty of the task. On that note, we fine-tuned DeBERTa-v3-large [14] and Llama2-7B [21] models.

We fine-tune both models on the 3 different pairwise causality tasks presented in our paper. For each task, we create 3 different train/test splits (75%/25% ratio) to study the generalization ability of the models after fine-tuning; Date: we select 5 out of the 20 most recent stories for the test set and the rest for the train, Story: we randomly select 5 stories for the test set and the rest for the train, and Random: we randomly split the event pairs dataset, regardless of the story or the date.

The results in Table 7 show that fine-tuned DeBERTa-large (encoder-only) models fail to perform well on CRAB, showing that our benchmark challenges the current state-of-the-art fine-tuned methods. Compared to decoder-only models in a few-shots setting, DeBERTa-large tends to underperform when splitting by story, except for the easier binary pairwise causality prediction task. Additionally, as expected, the experiments with the random data split have higher scores, which validate the information leakage of the context from the train to test set and verify that models rely on the context (articles) when assessing the causal relationship of the two events. A subsequent study on how fine-tuning improves pre-trained LLMs causal reasoning abilities would be interesting, and we hope that our paper provides a strong benchmark for pursuing this research direction.

\begin{table}
\begin{tabular}{l l|c c c} \hline \hline
**Causality Tasks** & **Model** & **Split by Date** & **Split by Story** & **Random split** \\ \hline
**Pairwise Causality Score** & DeBERTa-large & 21.6 & 21.4 & 22.9 \\  & Llama2 7B & **24.3** & **26.3** & **32.8** \\ \hline
**Multi-class Pairwise Causality** & DeBERTa-large & **29.4** & **35.8** & **60.8** \\  & Llama2 7B & 23.2 & 23.1 & 32.7 \\ \hline
**Binary Pairwise Causality** & DeBERTa-large & **62.5** & **74.2** & **76.6** \\  & Llama2 7B & 51.1 & 51.9 & 58.5 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Macro F1-scores on test set for fine-tuned models on all Pairwise Causality Inference tasks. We stratify the results for Date, Story and Random splits. The best performance for each causality task is bolded.

\begin{table}
\begin{tabular}{c c c} \hline \hline Causality classes & Size & \(\alpha\) \\ \hline \([1,2]\) & 1310 & 0.04 \\ \([2,3]\) & 1295 & 0.08 \\ \([3,4]\) & 1429 & 0.12 \\ \([1,4]\) & 1444 & 0.38 \\ \([1,2,3,4]\) & 2730 & 0.28 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Krippendorff’s \(\alpha\) for different groups of classes.

[MISSING_PAGE_FAIL:10]

[MISSING_PAGE_EMPTY:11]

[MISSING_PAGE_FAIL:12]

[MISSING_PAGE_FAIL:13]