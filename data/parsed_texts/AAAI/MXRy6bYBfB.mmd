# EEGFormer: Towards Transferable and Explainable Large-Scale

EEG Foundation Model

 Yuqi Chen1, Kan Ren2, Kaitao Song1, Yansen Wang1,

Yifan Wang2, Dongsheng Li1, Lili Qiu1

1 Microsoft Research 2 ShanghaiTech University

yangsenwang@microsoft.com renkan@shanghaitech.edu.cn

###### Abstract

Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance on various downstream tasks but also provide explainable outcomes of the useful patterns within the data. To validate the effectiveness of our model, we extensively evaluate it on various downstream tasks and assess the performance under different transfer settings. Furthermore, we demonstrate how the learned model exhibits transferable anomaly detection performance and provides valuable explainability of the acquired patterns via self-supervised learning.

## Introduction

Scalp electroencephalography (EEG) is physiological signal data that provides valuable insight into the human brain activities and has extensive applications in healthcare, e.g., disease diagnosis and medical monitoring [1, 13, 14, 15]. Despite the ease of collecting EEG signals, comprehending and interpreting them often requires extensive expertise from medical professionals. To address this challenge, recent research has focused on leveraging self-supervised learning techniques to learn meaningful representations from EEG data [22, 23, 16]. These learned representations can then be fine-tuned for various downstream tasks, including seizure detection [13, 15], abnormal detection [13, 14, 15], etc. However, these existing works focus on pretraining upon each individual dataset corresponding to a single downstream task and fail to leverage the power of abundant data. In this paper, our primary interest lies in exploring the potential of self-supervised learning using abundant large-scale unlabeled data without human annotations.

Moreover, explainability is a crucial concern when applying machine learning models to real-world applications [12, 10, 11], particularly in the healthcare community [12, 13]. Prior research [13, 14] has predominantly relied on end-to-end model learning, which poses challenges for human comprehension. Models that lack explainability have the potential to yield unsafe and irrational outcomes, thereby increasing the risk of severe medical malpractice.

To address the above issues, we introduce EEGFormer as a solution for large-scale EEG pretraining. Our primary objective is to investigate a discrete representation learning approach [26, 13, 12, 14] specifically designed for EEG pretraining. We provide evidence that the utilization of vector-quantized Transformer [26] model can learn universal representations on EEG signals with adaptable performance on various downstream tasks compared to the conventional mask reconstruction strategy [27]. Furthermore, the learned codebook and the discrete indices provide explainable outcomes of the useful patterns within the data.

The contribution of the paper can be summarized as below:

* We propose a novel pretraining strategy for EEG data. EEGFormer adopts a discrete representation learning algorithm along with reconstruction loss.
* We harness the plentiful EEG data available in the TUH Corpus [1] to construct a foundational EEG model. This marks the pioneering effort in pretraining with a massive 1.7TB EEG dataset.
* We conduct a comprehensive analysis of the pretrained foundation model EEGFormer, evaluating its performance on four downstream corpora sourced from theTUH corpus. Additionally, we explore its transferability by applying it to the Neonate dataset Stevenson et al. (2019) for neonatal seizure detection.
* We provide an in-depth analysis of the learned codebook and demonstrate that the pretraining algorithm can provide transferable and explainable representations.

## Related Work

Pretraining for Time-Series DataSelf-supervised learning for time-series data is a highly significant research hotspot. Many non-Transformer models have been developed to learn the representation of time series Franceschi et al. (2019); Tonekaboni et al. (2021); Yue et al. (2022); Eldele et al. (2021). Recently, Nie et al. (2022) introduced a Transformer-based approach that segments time series into patches, which leads to promising outcomes across various forecasting datasets. Furthermore, researchers are growing interested in utilizing pretrained large language models (LLMs) to enhance time series analysis Zhou et al. (2023); Gruver et al. (2023). These methods are mainly on forecasting tasks and lack practical considerations of the model adaptation to different downstream tasks.

Pretraining for EEG dataElectroencephalograms (EEGs) are widely employed for diagnosing neurological, and psychiatric, as well as in brain-machine interface applications. In the field of EEG signals, self-supervised learning has emerged as a promising approach Tang et al. (2021); Jiang et al. (2021); Kostas et al. (2021). SeqCLR Mohsenvand et al. (2020) introduces a set of data augmentations for EEG and extends the SimCLR Chen et al. (2020) framework to extract channel-wise features on time-series EEG data. MMM Yi et al. (2023) focuses on spatial and topological modeling of EEG data and breaks the boundaries between different EEG topologies. However, these methods rely on end-to-end model learning, which lacks explainability. In this paper, we propose a new pretraining strategy that can provide an explainable representation. Moreover, these methods either apply self-supervision within the same dataset or test for a single downstream task, which cannot fully unleash the power of the self-supervised pretraining paradigm. In this paper, our approach diverges the existing methods by leveraging the extensive multiple datasets of different tasks for pretraining purposes.

## EEGFormer: Vector-Quantized Pretraining Transformer for EEG Data

This work aims to present a novel pretraining algorithm to derive a universal, transferable, and explainable EEG foundation model. In this paper, we focus on learning temporal patterns among multi-channel EEG data. Specifically, we view EEG data as a multi-variate time series data, i.e., \(X\in\mathbb{R}^{L\times C}\), where \(L\) represents the length of the time series, and \(C\) represents the number of channels (or variates) 1. Our primary goal is to develop a self-supervised learning algorithm that optimally leverages unlabelled data while enhancing explainability. To accomplish this, we introduce a customized vector-quantized pretraining approach designed for EEG data, as illustrated in Figure 1. EEG signals can be encoded into discrete tokens, enabling explanation through the analysis of these tokens, as is discussed in experiments. During the fine-tuning stage, the model and the codebook can be further fine-tuned to integrate specific domain-specific knowledge. In the subsequent subsections, we will provide a detailed description of the overall framework, including the preprocessing, EEG slicing, encoding module, decoding module, training algorithm, and fine-tuning processes.

Footnote 1: We mitigate the sample rate discrepancy by resampling the EEG data to a uniform rate of 250 Hz. Further, our analysis focuses on fixed-length 12-second EEG data following Tang et al. (2021). Thus, throughout the experiment, \(L\) equals to 3000.

Feature PreprocessingConverting EEG signals to the frequency domain is a common preprocessing technique. Inspired by Tang et al. (2021), given a time domain EEG signals, we perform fast Fourier transformation (FFT) to obtain frequency domain amplitude as input features.

Slice & EncodeTo pretrain a time-series tokenizer, we first apply instance normalization to the frequency domain inputs. Then, we split each univariate time series into non-overlapped (or overlapped) segments Nie et al. (2022). Specifically, for each variate (or channel), i.e., \(x_{c}\in\mathbb{R}^{L}\) for the \(c^{\text{th}}\) variate. Denote the patch length as \(P\) and the stride as \(S\), the patching

Figure 1: Overview of EEGFormer. Initially, multi-variate EEG signals are segmented into patches, which are then passed through a Transformer encoder. Subsequently, a vector-quantized model is employed to generate discrete indices. These indices are then fed into a shallow Transformer decoder.

process will generate a sequence of patches \(\mathrm{x}_{c}\in\mathbb{R}^{P\times N}\), where \(N=\left(\left\lfloor\frac{L-P}{S}\right\rfloor+2\right)\) indicates the number of patches. Given the input EEG data \(x_{c}\in\mathbb{R}^{P\times N}\) for \(c\in[1,\ldots,C]\), it is necessary to add position embedding before input to the Transformer encoder. Specifically, we map the dimension to \(D\) via learnable weight matrix \(\mathbf{w}_{p}\in\mathbb{R}^{P\times D}\) and adopt learnable position embedding, i.e., \(\mathbf{w}_{pos}\in\mathbb{R}^{N\times D}\). Hence, the input vector is given by \(\hat{x}_{c}=\mathrm{x}_{c}^{\top}\mathbf{w}_{p}+\mathbf{w}_{pos}\). Finally, we forward \(\hat{x}_{c}\) into a stack of Transformer encoder layers in a channel-independent manner [20].

Vector QuantizerThe vector quantizer looks up the nearest neighbor in the codebook for each patch representation \(\mathbf{h}_{i}\). Let \(\left\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{K}\right\}\) denote the embeddings in the codebook. For the \(i^{\text{th}}\) patch, its quantized code is calculated as \(\mathbf{z}_{i}=\operatorname*{arg\,min}_{j}\left\lVert\mathbf{h}_{i}-\mathbf{v}_{j} \right\rVert_{2}\), where \(j\in\{1,2,\ldots,K\}\). After quantizing the hidden vectors to discrete tokens, we obtain the codebook embeddings \(\mathbf{V}_{z}=\left\{\mathbf{v}_{z_{i}}\right\}_{i=1}^{N}\).

Pretraining Stage: Decode & ReconstructWe further forward the codebook embeddings from the vector quantizer into a shallow Transformer model [14]. Upon passing through the decoder model, each variate generates an output denoted as \(\hat{h}_{c}\in\mathbb{R}^{N\times D}\). We map the outputs to the same shape as the input through \(\mathbf{w}_{o}\in\mathbb{R}^{D\times P}\) and \(\mathbf{b}_{o}\in\mathbb{R}^{P}\), i.e., \(x_{o}=\hat{h}_{c}\mathbf{w}_{o}+\mathbf{b}_{o}\). Finally, we reshape the output to match the shape of \(X\), denoted as \(X_{rec}\). The pertaining objective of EEGFormer for each sample \(X\in\mathcal{D}\) is to minimize

\[\ell_{rec}=\left\lVert X_{rec}-X\right\rVert_{2}^{2}+\left\lVert\operatorname {sg}\left[\mathbf{H}\right]-\mathbf{V}_{\mathbf{Z}}\right\rVert_{2}^{2}+\left\lVert\mathbf{H }-\operatorname{sg}\left[\mathbf{V}_{\mathbf{Z}}\right]\right\rVert_{2}^{2}\, \tag{1}\]

where \(\operatorname{sg}[\cdot]\) stands for the stop-gradient operator which is an identity at the forward pass while having zero gradients during the backward pass [23]2.

Footnote 2: In Eq. (1), \(\mathbf{H}\) denotes the hidden vectors for all the variates, whereas \(\mathbf{h}\) stands for a single variate. Similarly for \(\mathbf{Z}\) and \(\mathbf{z}\).

Fine-tuning Stage: Decode & LinearTo facilitate downstream fine-tuning, we utilize the pretrained model weights of both the encoder and the decoder modules. After obtaining the outputs \(\hat{H}\in\mathbb{R}^{C\times N\times D}\) from the decoder model, we concatenate all the outputs and transform them into \(c\in\mathcal{R}^{\mathcal{K}}\), where \(\mathcal{K}\) denotes the number of classes for the classification task. The loss function for the fine-tuning stage is:

\[\ell_{cls}=-\log c_{l}+\left\lVert\operatorname{sg}\left[\mathbf{H}\right]-\mathbf{V} _{\mathbf{Z}}\right\rVert_{2}^{2}+\left\lVert\mathbf{H}-\operatorname{sg}\left[\mathbf{V} _{\mathbf{Z}}\right]\right\rVert_{2}^{2}\, \tag{2}\]

where \(l\) is the label of the sample.

## Experimental Results

Datasets DescriptionWe pretrain our model on the Temple University EEG Corpus (TUH Corpus) 3, which has collected over 1.7TB of unlabelled EEG data that are suitable for pretraining. We evaluate our model on five downstream datasets. i) TUAB corpus for abnormal detection of EEG data. ii) TUAR corpus for classifying artifacts. iii) TUSL corpus for classifying slowing events. v) TUSL corpus for seizure detection. vi) Neonate dataset [20] for neonatal seizures detection. Notably, the Neonate dataset is not a subset of the TUH dataset. Therefore, we consider the transferability of our pretraining strategy.

Footnote 3: [https://isip.piconepress.com/projects/tuh_eeg/](https://isip.piconepress.com/projects/tuh_eeg/)

Parameter SettingWe vary the encoder layers from \(6\) to \(12\), and the codebook size, i.e., \(K\), from \(512\) to \(2048\). The decoder is a 3-layer Transformer. We set \(D\) to \(128\). Specifically, EEGFormer \({}_{s}\) adopts a 6-layer encoder and \(K=512\), EEGFormer \({}_{b}\) adopts an 8-layer encoder and \(K=1024\), and EEGFormer \({}_{l}\) adopts a 12-layer encoder and \(K=2048\).

Compared BaselinesWe compare EEGFormer with several baselines specifically for EEG data. i) EEGNet [1] adopts a fully convolution network for EEG data. ii) TCN [1] adopts a dilated convolutional neural network. iii) EEG-GNN [1] adopts a graph neural network for capturing spatiotemporal dependencies in EEGs. v) GraphS4mer [1] further adopts structured state space models or multivariate biosignals. Additionally, we also compare EEGFormer with self-supervised baselines. BrainBERT [1] adopts neural signal processing techniques for producing superresolution time-frequency representations and pretrain with mask reconstruction loss.

Evaluation MetricsFor detection tasks, we adopt the area under the receiver operating characteristic (AUROC) and the area under the precision-recall curve (AUPRC) for evaluation. For multi-classification tasks, we adopt macro AUROC (M-AUROC) and macro AUPRC (M-AUPRC) for evaluation.

Main ResultsThe experimental results presented in Table 1 clearly illustrate the effectiveness of our pretraining strategy in both in-dataset and transfer settings. Quantitatively, compared with the best baseline results, EEGFormer \({}_{l}\) achieves a **9.02%** improvement on the Neonate dataset and a **13.23%** on the TUSZ under the AUPRC metric. Additionally, we conduct experiments with different model sizes. Specifically, EEGFormer \({}_{s}\) and EEGFormer \({}_{b}\) demonstrate an average AUROC of 0.822 and 0.829, respectively, as well as an average AUPRC of 0.575 and 0.574, respectively.

Influence of Pretrain EpochsWe conducted experiments to examine the impact of pretraining epochs on various downstream corpora. The results of these experiments are illus

Figure 2: Influence of pretrain epochs on two TUH corpus.

trated in Figure 2, Specifically, the results indicate that a longer pretraining period leads to notable enhancements in the performance of the downstream tasks.

Compared with Other SettingsTable 2 compares the performance of EEGFormer\({}_{l}\) using fine-tuning, linear probing, and supervising from scratch. By just fine-tuning the model's prediction head, i.e., linear probing), the performance of our model is already comparable with the supervised model, i.e., GraphS4mer. Specifically, EEGFormer\({}_{l}\) with linear probe outperforms GraphS4mer by **1.73%** on the TUAR dataset under the M-AUPRC metric. Thus, we demonstrate that serves as a strong foundation model for EEG data. Furthermore, fine-tuning consistently surpasses the performance of both supervised learning and linear probing, demonstrating the effectiveness of large-scale pretraining.

Towards Seizure LocalizationAfter the pertaining state, each EEG signal is discretized into multiple indices denoted as \(I\in[1,\dots,K]^{C\times N}\). To perform seizure detection in the TUSZ corpus using these pretrained indices, we first extract n-gram features for each data (e.g., 2-gram, 3-gram, and 4-gram). Next, we adopt a naive Bayes classifier based on n-gram features. Notably, we achieve an AUPRC of 0.292 and an AUROC of 0.741, without the need for fine-tuning the pretrained weight. Additionally, we extract the top-3 significant features with high posterior probability leading to seizure events, from the naive Bayes model. Figure 3 presents two cases, where the highlighted regions indicate the localization of seizures. It is worth noting that in the right figure, the highlighted segments correspond to the spike and slow wave complex in all the frontal lobe (Fz), parietal lobe (Pz), and temporal lobe (T3, T6), which indicates an epileptiform discharge (EPSP) followed by the refractory period of the affected neuron population after the large and synchronized neuron EPSP, which is often treated as one of the most important patterns for the diagnosis of epilepsy and the onset of a seizure event. Hence, these patterns are significant in enhancing the explainability of the pretrained model.

## Conclusion

In this paper, we present a novel method called EEGFormer for self-supervised learning using large-scale EEG data. Our approach learns a discrete codebook and representations of EEG signals simultaneously. We extensively evaluate our pretraining algorithm on various downstream tasks to demonstrate its effectiveness. Additionally, we conduct an analysis to highlight the explainability of our pretraining model.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Model & Pretrain & Metric & TUAB & TUAR\({}^{*}\) & TUSL\({}^{*}\) & TUSZ & Neonate \\ \hline \multirow{2}{*}{EEGNet} & \multirow{2}{*}{\(\bigtimes\)} & (M-)AUROC & 0.841 \(\pm\).011 & 0.752 \(\pm\).006 & 0.635 \(\pm\).015 & 0.820 \(\pm\).030 & 0.793 \(\pm\).019 \\  & & (M-)AUPRC & 0.832 \(\pm\).011 & 0.433 \(\pm\).025 & 0.351 \(\pm\).006 & 0.470 \(\pm\).017 & 0.499 \(\pm\).044 \\ \hline \multirow{2}{*}{TCN} & \multirow{2}{*}{\(\bigtimes\)} & (M-)AUROC & 0.841 \(\pm\).004 & 0.687 \(\pm\).011 & 0.545 \(\pm\).009 & 0.817 \(\pm\).004 & 0.731 \(\pm\).020 \\  & & (M-)AUPRC & 0.831 \(\pm\).002 & 0.408 \(\pm\).009 & 0.344 \(\pm\).001 & 0.383 \(\pm\).010 & 0.398 \(\pm\).025 \\ \hline \multirow{2}{*}{EEG-GNN} & \multirow{2}{*}{\(\bigtimes\)} & (M-)AUROC & 0.840 \(\pm\).005 & 0.837 \(\pm\).022 & **0.721 \(\pm\).009** & 0.780 \(\pm\).006 & 0.760 \(\pm\).010 \\  & & (M-)AUPRC & 0.832 \(\pm\).004 & **0.488 \(\pm\).015** & 0.381 \(\pm\).004 & 0.388 \(\pm\).023 & 0.419 \(\pm\).021 \\ \hline \multirow{2}{*}{GraphS4mer} & \multirow{2}{*}{\(\bigtimes\)} & (M-)AUROC & 0.864 \(\pm\).006 & 0.833 \(\pm\).006 & 0.632 \(\pm\).017 & 0.822 \(\pm\).034 & 0.719 \(\pm\).007 \\  & & (M-)AUPRC & 0.862 \(\pm\).008 & 0.461 \(\pm\).024 & 0.359 \(\pm\).001 & 0.491 \(\pm\).001 & 0.374 \(\pm\).013 \\ \hline \hline \multirow{2}{*}{BrainBERT} & \multirow{2}{*}{\(\bigtimes\)} & (M-)AUROC & 0.853 \(\pm\).002 & 0.753 \(\pm\).012 & 0.588 \(\pm\).013 & 0.814 \(\pm\).009 & 0.734 \(\pm\).019 \\  & & (M-)AUPRC & 0.846 \(\pm\).003 & 0.350 \(\pm\).014 & 0.352 \(\pm\).003 & 0.386 \(\pm\).018 & 0.398 \(\pm\).027 \\ \hline \multirow{2}{*}{EEGFormer\({}_{l}\)} & \multirow{2}{*}{\(\bigtimes\)} & (M-)AUROC & **0.876 \(\pm\).003** & **0.852 \(\pm\).004** & 0.679 \(\pm\).013 & **0.883 \(\pm\).005** & **0.833 \(\pm\).017** \\  & & (M-)AUPRC & **0.872 \(\pm\).001** & 0.483 \(\pm\).014 & **0.389 \(\pm\).003** & **0.556 \(\pm\).008** & **0.544 \(\pm\).026** \\ \hline \hline \multirow{2}{*}{Improvement} & \multirow{2}{*}{\(\bigtimes\)} & (M-)AUROC & +1.39\% & +1.79\% & -6.18\% & +7.42\% & +5.04\% \\  & & (M-)AUPRC & +1.16\% & -1.03\% & +2.10\% & +13.23\% & +9.02\% \\ \hline \hline \end{tabular}
\end{table}
Table 1: Experimental results on various downstream tasks. Within the table, \({}^{*}\) indicates a multi-classification task.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model & Type & Metric & TUAR & TUSL \\ \hline \multirow{2}{*}{GraphS4mer} & \multirow{2}{*}{Sup} & M-AUROC & 0.833 \(\pm\).006 & 0.632 \(\pm\).017 \\  & & M-AUPRC & 0.461 \(\pm\).024 & 0.359 \(\pm\).001 \\ \hline \multirow{2}{*}{EEGFormer\({}_{l}\)} & \multirow{2}{*}{Sup} & M-AUROC & 0.822 \(\pm\).012 & 0.703 \(\pm\).033 \\  & & M-AUPRC & 0.447 \(\pm\).015 & 0.374 \(\pm\).003 \\ \hline \multirow{2}{*}{EEGFormer\({}_{l}\)} & \multirow{2}{*}{LP} & M-AUROC & 0.827 \(\pm\).000 & 0.657 \(\pm\).017 \\  & & M-AUPRC & 0.469 \(\pm\).002 & 0.359 \(\pm\).003 \\ \hline \multirow{2}{*}{EEGFormer\({}_{l}\)} & \multirow{2}{*}{FT} & M-AUROC & 0.852 \(\pm\).004 & 0.679 \(\pm\).013 \\  & & M-AUPRC & 0.483 \(\pm\).014 & 0.389 \(\pm\).003 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Linear probe results on TUSL and TUAR corpus. Within the table, Sup stands for supervised learning from scratch, FT stands for self-supervised and fine-tuned, and LP stands for self-supervised and linear probing.

Figure 3: Explanation results from naive Bayes model.

## References

* A. Ali, T. Schnake, O. Eberle, G. Montavon, K. Muller, and L. Wolf (2022)XAI for transformers: better explanations through conservative propagation. In International Conference on Machine Learning, pp. 435-451. Cited by: SS1.
* S. Bai, J. Z. Kolter, and V. Koltun (2018)An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271. Cited by: SS1.
* T. Chen, S. Kornblith, M. Norouzi, and G. Hinton (2020)A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. Cited by: SS1.
* M. Darvishi-Bayazi, M. S. Ghaemi, T. Lesort, M. R. Arefin, J. Faubert, and I. Rish (2023)Amplifying pathological detection in EEG Signaling Pathways through Cross-Dataset Transfer Learning. arXiv preprint arXiv:2309.10910. Cited by: SS1.
* E. Eldele, M. Ragab, Z. Chen, M. Wu, C. K. Kwoh, X. Li, and C. Guan (2021)Time-series representation learning via temporal and contextual contrasting. arXiv preprint arXiv:2106.14112. Cited by: SS1.
* P. Esser, R. Rombach, and B. Ommer (2021)Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873-12883. Cited by: SS1.
* V. Fortuin, M. Huser, F. Locatello, H. Strathmann, and G. Ratsch (2018)Som-vae: interpretable discrete representation learning on time series. arXiv preprint arXiv:1806.02199. Cited by: SS1.
* J. Franceschi, A. Dieuleveut, and M. Jaggi (2019)Unsupervised scalable representation learning for multivariate time series. Advances in neural information processing systems32. Cited by: SS1.
* N. Gruver, M. Finzi, S. Qiu, and A. G. Wilson (2023)Large language models are zero-shot time series forecasters. arXiv preprint arXiv:2310.07820. Cited by: SS1.
* F. F. Gulamali, A. S. Sawant, I. Hofer, M. Levin, B. S. Singh, and G. N. Mickarni (2023)Clinically relevant unsupervised online representation learning of ICU waveforms. In ICLR 2023 Workshop on Time Series Representation Learning for Health, Cited by: SS1.
* A. Harati, S. Lopez, I. Obeid, J. Picone, M. Jacobson, and S. Tobochnik (2014)The tuh eeg code curpus: a big data resource for automated EEG interpretation. In 2014 IEEE signal processing in medicine and biology symposium (SPMB), pp. 1-5. Cited by: SS1.
* X. Jiang, J. Zhao, B. Du, and Z. Yuan (2021)Self-supervised contrastive learning for EEG-based sleep staging. In 2021 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. Cited by: SS1.
* D. Kostas, S. Arroca-Ouellette, and F. Rudzicz (2021)BENDR: using transformers and a contrastive self-supervised learning task to learn from massive amounts of EEG data. Frontiers in Human Neuroscience15, pp. 653659. Cited by: SS1.
* V. J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung, and B. J. Lance (2018)EEGNet: a compact convolutional neural network for EEG-based brain-computer interfaces. Journal of neural engineering15 (5), pp. 056013. Cited by: SS1.
* K. K. Leung, C. Rooke, J. Smith, S. Zuberi, and M. Volkovs (2022)Temporal dependencies in feature importance for time series prediction. In The Eleventh International Conference on Learning Representations, Cited by: SS1.
* R. Li, Y. Wang, and B. Lu (2021)A multi-domain adaptive graph convolutional network for EEG-based emotion recognition. In Proceedings of the 29th ACM International Conference on Multimedia, pp. 5565-5573. Cited by: SS1.
* R. Li, Y. Wang, W. Zheng, and B. Lu (2022)A multi-view spectral-spatial-temporal masked autoencoder for decoding emotions with self-supervised learning. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 6-14. Cited by: SS1.
* Z. Li, Y. Fang, Y. Li, K. Ren, Y. Wang, X. Luo, J. Duan, C. Huang, D. Li, and L. Qiu (2023)Protecting the future: neonatal seizure detection with spatial-temporal modeling. arXiv preprint arXiv:2307.05382. Cited by: SS1.
* C. H. Mendoza-Cardenas, A. Meek, and A. J. Brockmeier (2023)Labeling EEG components with a bag of waveforms from learned dictionaries. In ICLR 2023 Workshop on Time Series Representation Learning for Health, Cited by: SS1.
* M. N. Mohsenvand, M. R. Izadi, and P. Maes (2020)Contrastive representation learning for electroencephalogram classification. In Machine Learning for Health, pp. 238-253. Cited by: SS1.
* Y. Nie, N. H. Nguyen, P. Sinthong, and J. Kalagnanam (2022)A time series is worth 64 words: long-term forecasting with transformers. arXiv preprint arXiv:2211.14730. Cited by: SS1.
* Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei (2022)Beit v2: masked image modeling with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366. Cited by: SS1.
* T. Song, S. Liu, W. Zheng, Y. Zong, Z. Cui, Y. Li, and X. Zhou (2021)Variational instance-adaptive graph for EEG emotion recognition. IEEE Transactions on Affective Computing. Cited by: SS1.
* N. J. Stevenson, K. Tapani, L. Lauronen, and S. Vanhatalo (2019)A dataset of neonatal EEG recordings with seizure annotations. Scientific data6 (1), pp. 1-8. Cited by: SS1.
* S. Tang, J. A. Dunnmon, K. A. Saab, X. Zhang, Q. Huang, F. Dubost, D. L. Rubin, and C. Lee-Messer (2021)Self-supervised graph neural networks for improved electroencephalogram scale seizure analysis. arXiv preprint arXiv:2104.08336. Cited by: SS1.
* S. Tonekaboni, D. Eytan, and A. Goldenberg (2021)Unsupervised representation learning for time series with temporal neighborhood coding. arXiv preprint arXiv:2106.00750. Cited by: SS1.
* A. Van Den Oord, O. Vinyals, et al. (2017)Neural discrete representation learning. Advances in neural information processing systems30. Cited by: SS1.

[MISSING_PAGE_POST]

Wang, C.; Subramaniam, V.; Yaari, A. U.; Kreiman, G.; Katz, B.; Cases, I.; and Barbu, A. 2023. BrainBERT: Self-supervised representation learning for intracranial recordings. _arXiv preprint arXiv:2302.14367_.
* Ye et al. (2022) Ye, M.; Chen, C. P.; and Zhang, T. 2022. Hierarchical dynamic graph convolutional network with interpretability for EEG-based emotion recognition. _IEEE Transactions on Neural Networks and Learning Systems_.
* Yi et al. (2023) Yi, K.; Wang, Y.; Ren, K.; and Li, D. 2023. Learning Topology-Agnostic EEG Representations with Geometry-Aware Modeling. In _Thirty-seventh Conference on Neural Information Processing Systems_.
* Yue et al. (2022) Yue, Z.; Wang, Y.; Duan, J.; Yang, T.; Huang, C.; Tong, Y.; and Xu, B. 2022. Ts2vec: Towards universal representation of time series. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, 8980-8987.
* Zhou et al. (2023) Zhou, T.; Niu, P.; Wang, X.; Sun, L.; and Jin, R. 2023. One Fits All: Power General Time Series Analysis by Pretrained LM. _arXiv preprint arXiv:2302.11939_.