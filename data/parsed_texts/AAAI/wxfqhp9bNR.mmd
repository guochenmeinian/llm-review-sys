# Improving Multi-Hop Reasoning in LLMs by Learning from Rich Human Feedback

Nitish Joshi1, Koushik Kalyanaraman 2, Zhiting Hu3, Kumar Chellapilla2, He He1, Li Erran Li2,

1New York University, 2Amazon Web Services, 3UC San Diego

nitish@nyu.edu, erranli@gmail.com

###### Abstract

Recent large language models (LLMs) have enabled tremendous progress in natural language understanding. However, they are prone to generate confident but nonsensical reasoning chains, a significant obstacle to establishing trust with users. In this work, we aim to incorporate rich human feedback on such incorrect model generated reasoning chains for multi-hop reasoning to improve performance on these tasks. To do so, we collect two such datasets of human feedback in the form of (correction, explanation, error type) for StrategyQA and Sports Understanding datasets1, and evaluate several algorithms to learn from such feedback. We show that fine-tuning on such small datasets of rich human feedback can improve model's performance of generating the correct final answers, and also improves the model's ability of judging the correctness of it's own answer.

Footnote 1: The data and code is available at - [https://github.com/joshinh/rich-feedback-reasoning](https://github.com/joshinh/rich-feedback-reasoning).

## Introduction

With the onset of large language models (LLMs) [4, 1], the field has seen tremendous progress on various NLP benchmarks. Among them, the progress has been striking on relatively simpler tasks such as short context or factual question answering [1], compared to harder tasks which require reasoning such as multi-hop question answering [23]. Even though LLMs may not be best at generating correct reasoning chains or explanations for such hard tasks [1], the prompting abilities of LLMs have the potential to provide partially correct (and relevant) facts required to answer the question. Relatedly, recent work has found that without any finetuning LLMs cannot self-correct their reasoning yet [12], suggesting the need for human intervention.

Motivated by this, we try to address the following research question -- _can we improve reasoning of LLMs by learning from human feedback on model-generated reasoning chains?_ Figure 1 provides an overview of our approach -- we first prompt the model to generate reasoning chains for multi-hop questions, then collect diverse human feedback on these chains for diagnosis and propose training algorithms to learn from the collected data.

We collected diverse feedback including correction to model's generation, explanation of why the generation was wrong and error type for a total of 2.2k examples from two datasets which we will publicly release. We propose multiple training algorithms to learn from the collected feedback including a multitask algorithm, a variant of self-consistency in chain-of-thought prompting [24], and a refinement algorithm where we refine the model generated reasoning chain. We use the proposed algorithms on Llama2 [25] and find that they either improve model's reasoning ability (sports understanding dataset) or perform comparable to in-context learning (strategyQA dataset). More importantly, we find that the fine-tuned model is sometimes better at judging if it's own answer is correct compared to the base (not finetuned) Llama2 model, an important practical ability in order to use LLMs more widely.

Our main contributions can be summarized as: (1) a dataset of rich human feedback including natural language feedback for 2.2k examples for multi-hop reasoning ; (2)

Figure 1: Overview of the process, where we first prompt LLMs to generate reasoning chains for multi-hop questions, collect diverse feedback on the generations including categorical feedback and natural language feedback, and use multiple training algorithms to learn from them.

novel algorithms to learn from diverse feedback to both improve reasoning performance and which can make LLMs better at judging their own correctness.2

Footnote 2: We believe that the dataset could also be potentially very useful for evaluation in verification [11] where the task is to identify and describe the error in models’ generation.

## Related Work

**Learning from Feedback.** Learning from human feedback in the form of rewards [16, 15] has become an effective paradigm for improving LLMs [14, 15, 17]. Most feedback datasets either provide sparse feedback such as binary feedback [1, 1] or provide natural language feedback but for narrow space of tasks such as summarization [16]. In comparison, we create a dataset of rich human feedback including natural language feedback for much harder reasoning tasks.

**LLM self-correction.** In contrast to learning from human feedback, recent works have explored if LLMs can self-correct their answers. Specifically, madaan2023learning use an iterative feedback and refinement procedure to improve performance; welleck2022learning introduce self-correctors by separating the generator and the corrector; bai2022learning use LLMs to generate feedback based on a 'constitution'. Nevertheless in the context of reasoning, huang2023learning show that LLMs struggle at self-correction and performance might even deteriorate. Given this shortcoming of LLMs' self-correction ability, we collect a rich human feedback dataset for reasoning and demonstrate its utility.

## Data Collection

Here, we describe the details of the feedback we collected and the annotation protocol followed during data collection. We collected feedback for model generations based on two reasoning based datasets: StrategyQA [1] and Sports Understanding, part of BigBench [14]. We used GPT-J [21]

\begin{table}
\begin{tabular}{|p{142.3pt}|} \hline
**Question:** Is the voice of the Genie from Disney’s Aladdin still alive? **Answer**: The answer is no because the Genie was voiced by comedian Robin Williams. Robin Williams died in 2014. **Question**: Johnny Gaudreau nutmegged the defender. Is this sentence plausible? **Answer**: The answer is no because Johnny Gaudreau is an American professional ice hockey player. Nutmeg which means passing ball through the opponent’s leg is a term from football. \\ \hline \end{tabular}
\end{table}
Table 1: Examples from StrategyQA (top) and Sports Understanding (bottom) used to prompt the language model.

Figure 2: The interface used to collect feedback from annotators, displaying all the diverse feedback we collect for an examples from StrategyQA.

suzaki 2021) to generate answers for StrategyQA and FlannT5 Chung et al. (2022) to generate answers for sports understanding dataset.3 In each case, the model was prompted with \(k\) in-context examples containing question, answer and explanation such as the ones showed in Table 1, followed by the test question.

Footnote 3: These models were chosen based on the best state-of-the-art open models at the time of data collection.

Figure 2 shows the interface we used -- annotators are given the question, model generated answer and the explanation split into steps. For each question, we collected the following feedback:

**Subquestions**: Decompose the original question into simpler subquestions required to answer the original question. This task was added after a pilot where we found that adding this task helps to 'prime' the annotators and improve quality of the rest of the tasks.

**Correction**: Annotators are provided with a free-form text box pre-filled with the model generated answer and explanation, and asked to edit it to obtain the correct answer and explanation.

**Error Type**: Among the most common types of error we found in the model generations (Factual Error, Missing Facts, Irrelevant Facts and Logical Inconsistency), annotators were asked to pick one or more of the error types which apply to given answer and explanation.

**Error Description**: The annotators were instructed to not only classify the errors but also to give a comprehensive justification for their categorization, including pinpointing the exact step where the mistake occurred and how it applies to the answer and explanation provided.

We used private internal vendors as the annotators. The data collection took place over multiple rounds. We first conducted two small pilots of 30 examples and 200 examples respectively, after which the annotator team were given detailed feedback on the annotation over a video call. We then conducted the data collection over two batches for StrategyQA, and over one batch for Sports Understanding giving periodic feedback throughout -- a total of 10 annotators worked on the task over a period of close to one month.

### Dataset Statistics

We gathered feedback on a total of 1565 examples for StrategyQA and 796 examples for Sports Understanding. Table 2 illustrates the percentage of examples that were error-free in the model generation and the proportion of examples that contained a specific error type. It's worth noting that some examples may have more than one error type.

## Learning Algorithms

For each question \(q\), and model generated answer (with explanation) \(m\), we have the following feedback collected: correct answer and explanation \(c\), type of error present in \(m\) (denoted by \(t\)) and error description \(d\), as described in section. Table 3 provides an example with all the feedback.

Multitask Learning.A simple baseline to learn from the diverse feedback available, is to treat each of them as a separate task. More concretely, we fine-tune Llama2 with the following objective:

\[\text{maximize}\ \ p(c|q)+p(t|q,m)+p(d|q,m) \tag{1}\]

For each term in Eq 1, we use a separate instruction appropriate for the task (e.g. 'Predict error in the given answer'). We also convert the categorical variable \(t\) into a natural language sentence. During inference, we use the instruction for the term \(p(c|q)\) ('Predict the correct answer for the given question') to generate the answer for the test question.

Weighted Self Consistency.Motivated by the success of self-consistency Wang et al. (2022) in chain-of-thought prompting, we propose a weighted variant of it. Instead of treating each sampled explanation as correct and considering the aggregate vote, we instead first consider whether the explanation is correct and then aggregate accordingly.

We first fine-tune Llama2 with the same objective as in equation 1. During inference, given a test question \(q\), we sample multiple possible answers (with the instruction for \(p(c|q)\)): \(a_{1},a_{2},..,a_{m}\). For each sampled answer \(a_{i}\), we use the instruction for the term \(p(t|q,m)\) i.e. 'Predict error in the given answer' to identify if it contains error: \(t_{i}=\text{argmax}\ \ p(t|q,a_{i})\). Each answer \(a_{i}\) is assigned a weight of 1 if it is correct, otherwise it is assigned a weight of \(\alpha<1\) (tunable hyperparameter). The final answer is obtained by considering a weighted vote over all the answers \(a_{1}\) to \(a_{n}\).

Refinement.In the previous proposed methods, the model directly generates the correct answer \(c\) conditioned on the question \(q\). Instead, here we propose to refine the model generated answer \(m\) to obtain the correct answer for a given

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Error Type** & **StrategyQA** & **Sports Und.** \\ \hline None & 17.6\% & 31.28\% \\ Factual Error & 27.6\% & 38.1\% \\ Missing Facts & 50.4\% & 46.1\% \\ Irrelevant Facts & 14.6\% & 3.9\% \\ Logical Inc. & 11.2\% & 5.2\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: Percentage of examples in each dataset where the model generation had the particular error type. Note that a example might contain more than one error type.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Question (\(q\))**: Is the voice of the Genie from Disney’s Aladdin still alive? \\
**Model Generation (\(m\))**: The answer is yes because Genne is voiced by Robin Williams. He is still alive. \\
**Correction (\(c\))**: The answer is no because the Genie was voiced by Robin Williams. Robin Williams died in 2014. \\
**Error Type (\(t\))**: Factual Error \\
**Error Description (\(d\))**: In step 2, the explanation incorrectly mentions that he is still alive when instead he died in 2014. \\ \hline \hline \end{tabular}
\end{table}
Table 3: Example of a question, model generation and the feedback we collect for StrategyQA.

[MISSING_PAGE_FAIL:4]

## Acknowledgement

We thank Hanlin Zhang for the helpful discussion, data collection, and experimentation.

## References

* Y. Bai, A. Jones, K. Nousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Ienghan, N. Joseph, S. Kadavath, J. Kermion, T. Conerly, S. ElShowk, N. Hatfield-Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. B. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan (2022)Training a helpful and harmless assistant with reinforcement learning from human feedback. arXivabs/2204.05862. Cited by: SS1.
* Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Nousse, K. Lukvsiute, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, T. Bowman, S. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. B. Brown, and J. Kaplan (2022)Conditional ai: harmlessness from ai feedback. arXivabs/2212.08073. Cited by: SS1.
* T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Sigler, S. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020)Language models are few-shot learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Eds.), Advances in Neural Information Processing Systems, Vol. 33, pp. 1877-1901. Cited by: SS1.
* P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei (2017)Deep reinforcement learning from human preferences. Advances in neural information processing systems30. Cited by: SS1.
* H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Chen, X. Chowdhivery, S. Narang, G. Mishra, A. W. Yu, V. Zhao, Y. Huang, A. M. Dai, H. Yu, S. Petrov, E. Chen, J. Devlin, A. Roberts, D. Zhou, Q. Le, and J. Wei (2022)Scaling instruction-finetuned language models. arXivabs/2210.11416. Cited by: SS1.
* H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma, et al. (2022)Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Cited by: SS1.
* J. Devlin, M. Chang, K. Lee, and K. Toutanova (2019)BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota, pp. 4171-4186. Cited by: SS1.
* K. Ethayarajh, Y. Choi, and S. Swayamdipta (2022)Understanding dataset difficulty with \(\mathcal{V}\)-Usable information. In Proceedings of the 39th International Conference on Machine Learning, Vol. 162 of Proceedings of Machine Learning Research, pp. 5988-6008. Cited by: SS1.
* M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant (2021)Did Aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics9, pp. 346-361. Cited by: SS1.
* A. Glaese, N. McAleese, M. Trkebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al. (2022)Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375. Cited by: SS1.
* J. E. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, and W. Chen (2021)LoRA: low-rank adaptation of large language models. ArXivabs/2106.09685. Cited by: SS1.
* J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou (2023)Large language models cannot self-correct reasoning yet. ArXivabs/2310.01798. Cited by: SS1.
* X. L. Li, V. Shrivastava, S. Li, T. Hashimoto, and P. Liang (2023)Benchmarking and improving generator-validator consistency of language models. ArXivabs/2310.01846. Cited by: SS1.
* A. Maadan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark (2023)Self-refine: iterative refinement with self-feedback. ArXivabs/2303.17651. Cited by: SS1.
* L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. (2022)Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155. Cited by: SS1.
* P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang (2016)SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392. Cited by: SS1.
* A. Saparov and H. He (2022)Language models are greedy reasoners: a systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240. Cited by: SS1.
* J. Scheurer, J. A. Campos, J. S. Chan, A. Chen, K. Cho, and E. Perez (2022)Training language models with natural language feedback. arXiv preprint arXiv:2204.14146. Cited by: SS1.
* A. Srivastava, et al. (2022)Beyond the imitation game: quantifying and extrapolating the capabilities of language models. ArXivabs/2206.04615. Cited by: SS1.
* A. Srivastava, et al. (2022)Beyond the imitation game: quantifying and extrapolating the capabilities of language models. ArXivabs/2206.04615. Cited by: SS1.
* A. Srivastava et al. (2022)Beyond the imitation game: quantifying and extrapolating the capabilities of language models. ArXivabs/2206.04615. Cited by: SS1.
* A. Srivastava et al. (2022)Beyond the imitation game: quantifying and extrapolating the capabilities of language models. ArXivabs/2206.04615. Cited by: SS1.
* A. Srivastava, et al. (2022)Beyond the imitation game: quantifying and extrapolating the capabilities of language models. ArXivabs/2206.04615. Cited by: SS1.
* A. Srivastava et al. (2022)Beyond the imitation game: quantifying and extrapolating the capabilities of language models. ArXivabs/2206.04615. Cited by: SS1.

Touvron, H.; Martin, L.; Stone, K. R.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D. M.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A. S.; Hosseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabas, M.; Klounann, I. M.; Korenex, A. V.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.; Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E. M.; Subramanian, R.; Tan, X.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. _ArXiv_, abs/2307.09288.
* Wang and Komatsuzaki (2021) Wang, B.; and Komatsuzaki, A. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax).
* Wang et al. (2022) Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; and Zhou, D. 2022. Self-Consistency Improves Chain of Thought Reasoning in Language Models. _ArXiv_, abs/2203.11171.
* Welleck et al. (2022) Welleck, S.; Lu, X.; West, P.; Brahman, F.; Shen, T.; Khashabi, D.; and Choi, Y. 2022. Generating Sequences by Learning to Self-Correct. _ArXiv_, abs/2211.00053.
* Yang et al. (2018) Yang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W.; Salakhutdinov, R.; and Manning, C. D. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, 2369-2380. Brussels, Belgium: Association for Computational Linguistics.
* Ziegler et al. (2019) Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford, A.; Amodei, D.; Christiano, P.; and Irving, G. 2019. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1909.08593_.

## Experimental Details

For all the experiments, we use a batch size of 16, learning rate \(1e-4\) with Adam Optimizer and a total of 5 epochs of training. To finetune large models, we use a parameter efficient finetuning method -- LoRA Hu et al. (2021). We apply LoRA to the projection matrices for the queries and values. We set the rank \(r=16\), \(\alpha=16\) and the dropout to 0.05 as the LoRA hyperparameters. For the main experiments in Table 4, we report mean and standard deviation over 3 runs. All models were trained on a A100 GPU.

## Prompts

For the experiment to judge the correctness of the generated reasoning chain, we use a 4-shot prompt for the baseline which is shown in Table 7.

Q: Can preventive healthcare reduce STI transmission? The answer is yes because STIs are caused by bacteria and viruses. Preventive healthcare can reduce the risk of contracting STIs.

E: The output is correct.

Q: Is cow methane safer for environment than cars? The answer is no because Cars produce carbon dioxide and methane. Methane is a greenhouse gas.

E: The output is wrong.

Q: Does the central processing unit usually have a dedicated fan? The answer is yes because The central processing unit (CPU) is the main component of a computer. The CPU is the part of the computer that processes information. The CPU is usually cooled by a fan.

E: The output is correct.

Q: Did Al Unser Jr. win the Space Race? The answer is no because Al Unser Jr. was the first American to win the Indianapolis 500. The Space Race was a competition between the United States and the Soviet Union to be the first to put a man on the moon.

E: The output is wrong.

Q: Is the United States Capitol located near the White House?

## Example Generations

In this section, we illustrate some examples of the generated reasoning chains before and after finetuning.

**Before finetuning**:

Q: Jacob Markstrom backhanded a shot. Is this sentence plausible?

A: The answer is no because Jacob Markstrom is a Swedish professional ice hockey player. In ice hockey, a backhanded shot is a shot that is struck with the back of the stick.

**After finetuning**:

Q: Jacob Markstrom backhanded a shot. Is this sentence plausible?

\begin{table}
\begin{tabular}{l}
**Experiment Details** \\ \end{tabular}
\end{table}
Table 7: The 4-shot prompt used for the baseline (first row) in Table 6.

A: The answer is yes because Jacob Markstrom is a professional ice hockey goaltender. Backhanding a shot is a common technique in ice hockey.

In the above example, the base model generated a correct reasoning chain but had a logical inconsistency and still predicted 'no'. After finetuning, the model predicts 'yes' which is the correct answer.

**Before finetuning**:

Q: Brandon Lowe took a left footed shot. Is this sentence plausible?

A: The answer is yes because Brandon Lowe is an American professional baseball player. In baseball, a left footed shot refers to a batted ball that is hit with the left foot.

**After finetuning**:

Q: Brandon Lowe took a left footed shot. Is this sentence plausible?

A: The answer is no because Brandon Lowe is a baseball player. Baseball players do not take left-footed shots.

In the above example, the base model has a factual error where it hallucinates a left footed shot in baseball. After finetuning, the model predicts the correct reasoning chain and the correct answer 'no'.

**Before finetuning**:

Q: Evgeni Malkin passed the puck. Is this sentence plausible?

A: The answer is no because Evgeni Malkin is a Russian ice hockey player. In ice hockey, a puck is a small, hard rubber disc that is used to play the game.

**After finetuning**:

Q: Evgeni Malkin passed the puck. Is this sentence plausible? A: The answer is yes because Evgeni Malkin is a professional ice hockey player. Passing the puck is a common action in ice hockey.

In this example, similar to the first one, there is a logical inconsistency in the base model -- the model predicts 'no' even though the (correct) predicted reasoning chain would suggest 'yes'. After finetuning, this error is fixed.

## 5 Limitations

In this work, we only consider four types of error types (Factual Error, Missing Facts, Logical Inconsistency, Irrelevant Fact) and leave a more flexible feedback categorization for future work. Additionally, we also collect feedback for only a small amount of examples (\(\approx\)1.5k for StrategyQA and \(<\) 1k for Sports Understanding).