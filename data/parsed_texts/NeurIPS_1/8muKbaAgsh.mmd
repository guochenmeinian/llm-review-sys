# Towards Stable Backdoor Purification through Feature Shift Tuning

Rui Min\({}^{1}\)1, Zeyu Qin\({}^{1}\)1, Li Shen\({}^{2}\), Minhao Cheng\({}^{1}\)

\({}^{1}\)Department of Computer Science & Engineering, HKUST

\({}^{2}\)JD Explore Academy

{rminaa, zeyu.qin}@connect.ust.hk

mathshenli@gmail.com

minhaocheng@ust.hk

Equal contribution. Email to zeyu.qin@connect.ust.hk

Footnote 1: footnotemark:

###### Abstract

It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based defenses. Therefore, it is necessary to disentangle the backdoor and clean features in order to improve backdoor purification. To address this, we introduce Feature Shift Tuning (FST), a method for tuning-based backdoor purification. Specifically, FST encourages feature shifts by actively deviating the classifier weights from the originally compromised weights. Extensive experiments demonstrate that our FST provides consistently stable performance under different attack settings. Without complex parameter adjustments, FST also achieves much lower tuning costs, only \(10\) epochs. Our codes are available at [https://github.com/AISafety-HKUST/stable_backdoor_purification](https://github.com/AISafety-HKUST/stable_backdoor_purification).

## 1 Introduction

Deep Neural Networks (DNNs) are shown vulnerable to various security threats. One of the main security issues is backdoor attack [6, 9, 10] that inserts malicious backdoors into DNNs by manipulating the training data or controlling the training process.

To alleviate backdoor threats, many defense methods [36] have been proposed, such as robust training [13, 17, 38] and post-processing purification methods [20, 37, 41]. However, robust training methods require complex modifications to model training process [13, 38], resulting in substantially increased training costs, particularly for large models. Pruning-based purification methods are sensitive to hyperparameters and model architecture [37, 41], which makes them hard to deploy in real-world applications.

Fine-tuning (FT) methods have been adopted to improve models' robustness against backdoor attacks [14; 20; 36] since they can be easily combined with existing training methods and various model architectures. Additionally, FT methods require less computational resources, making them one of the most popular transfer learning paradigms for large pretrained models [5; 21; 25; 28; 35; 40]. However, FT methods have not been sufficiently evaluated under various attack settings, particularly in the more practical low poisoning rate regime [2; 4]. Therefore, we begin by conducting a thorough assessment of widely-used tuning methods, vanilla FT and simple Linear Probing (LP), under different attack configurations.

In this work, we focus on _data-poisoning backdoor attacks_, as they efficiently exploit security risks in more practical settings [3; 4; 9; 27]. We start our study on _whether these commonly used FT methods can efficiently and consistently purify backdoor triggers in diverse scenarios_. We observe that _vanilla FT and LP can not achieve stable robustness against backdoor attacks while maintaining clean accuracy_. What's worse, although they show promising results under high poisoning rates (\(20\%,10\%\)), they completely fail under low poisoning rates (\(5\%,1\%,0.5\%\)). As shown in Figure 3, we investigate this failure mode and find that model's learned features (representations before linear classifier) have a significant difference under different poisoning rates, especially in terms of separability between clean features and backdoor features. For low poisoning rate scenarios, clean and backdoor features are tangled together so that the simple LP is not sufficient for breaking mapping between input triggers and targeted label without feature shifts. Inspired by these findings, we first try two simple strategies (shown in Figure 1), _FE-tuning_ and _FT-init_ based on LP and FT, respectively, to encourage shifts on learned features. In contrast to LP, FE-tuning only tunes the feature extractor with the frozen and re-initialized linear classifier. FT-init first randomly initializes the linear classifier and then conducts end-to-end tuning. Experimental results show that these two methods, especially FE-tuning, improve backdoor robustness for low poisoning rates, which confirms the importance of promoting shifts in learned features.

Though these two initial methods can boost backdoor robustness, they still face an unsatisfactory trade-off between defense performance and clean accuracy. With analysis of the mechanism behind those two simple strategies, we further proposed a stronger defense method, _Feature Shift Tuning_ (FST) (shown in Figure 1). Based on the original classification loss, FST contains an extra penalty, \(\left\langle\mathbf{w},\mathbf{w}^{ori}\right\rangle\), the inner product between the tuned classifier weight \(\mathbf{w}\) and the original backdoored classifier weight \(\mathbf{w}^{ori}\). During the end-to-end tuning process, FST can actively shift backdoor features by encouraging the difference between \(\mathbf{w}\) and \(\mathbf{w}^{ori}\) (shown in Figure 3). Extensive experiments have demonstrated that FST achieves better and more stable defense performance across various attack settings with maintaining clean accuracy compared with existing defense methods. Our method also significantly improves efficiency with fewer tuning costs compared with other tuning methods, which makes it a more convenient option for practical applications. To summarize, our contributions are:

* We conduct extensive evaluations on various tuning strategies and find that while vanilla Fine-tuning (FT) and simple Linear Probing (LP) exhibit promising results in high poisoning rate scenarios, they fail completely in low poisoning rate scenarios.
* We investigate the failure mode and discover that the reason behind this lies in varying levels of entanglement between clean and backdoor features across different poisoning rates. We further propose two initial methods to verify our analysis.
* Based on our initial experiments, we propose Feature Shift Tuning (FST). FST aims to enhance backdoor purification by encouraging feature shifts that increase the separability between clean and backdoor features. This is achieved by actively deviating the tuned classifier weight from its originally compromised weight.
* Extensive experiments show that FST outperforms existing backdoor defense and other tuning methods. This demonstrates its superior and more stable defense performance against various poisoning-based backdoor attacks while maintaining accuracy and efficiency.

## 2 Background and related work

**Backdoor Attacks.** Backdoor attacks aim to mislead the backdoored model to exhibit abnormal behavior on samples stamped with the backdoor trigger but behave normally on all benign samples. They can be classified into 2 categories [36]: **(1)** data-poisoning attacks: the attacker inserts a backdoor trigger into the model by manipulating the training sample \((\mathbf{x},\mathbf{y})\in(\mathcal{X},\mathcal{Y})\), like adding a small patch in clean image \(\mathbf{x}\) and assign the corresponding class \(\mathbf{y}\) to an attacker-designated target label \(\mathbf{y}_{t}\). [3, 6, 9, 10, 18, 33]; **(2)** training-control attacks: the attacker can control both the training process and training data simultaneously [23, 24]. With fewer assumptions about attackers' capability, data-poisoning attacks are much more practical in real-world scenarios [4, 9, 29] and have led to increasingly serious security risks [3, 27]. Therefore, in this work, we mainly focus on data-poisoning backdoor attacks.

**Backdoor Defense.** Existing backdoor defense strategies could be roughly categorized into robust training [13, 17, 38] and post-processing purification methods [20, 37, 41]. Robust training aims to prevent learning backdoored triggers during the training phase. However, their methods suffer from accuracy degradation and significantly increase the model training costs [17, 38], which is impractical for large-scale model training. Post-processing purification instead aims to remove the potential backdoor features in a well-trained model. The defender first identifies the compromised neurons and then prunes or unlearns them [20, 34, 37, 41, 42]. However, pruning-based and unlearning methods also sacrifice clean accuracy and lack generalization across different model architectures.

**Preliminaries of backdoor fine-tuning methods.** Without crafting sophisticated defense strategies, recent studies [14, 20, 27, 42] propose defense strategies based on simple Fine-tuning. Here, we introduce two widely used fine-tuning paradigms namely vanilla Fine-tuning (FT) and Linear Probing (LP) (shown in Figure 1) since they would serve as two strong baselines in our following sections. For each tuned model, we denote the feature extractor as \(\mathbf{\phi}(\mathbf{\theta};\mathbf{x}):\mathcal{X}\rightarrow\mathbf{\phi}(\mathbf{x})\) and linear classifier as \(\mathbf{f}(\mathbf{w};\mathbf{x})=\mathbf{w}^{T}\mathbf{\phi}(\mathbf{x}):\mathbf{\phi}(\mathbf{x})\rightarrow \mathcal{Y}\). To implement the fine-tuning, both tuning strategies need a set of training samples denoted as \(\mathcal{D}_{T}\subset(\mathcal{X},\mathcal{Y})\) to update the model parameters while focusing on different parameter space. Following previous works [5, 14, 16, 20, 21], we implement vanilla FT by updating the whole parameters \(\{\mathbf{\theta},\mathbf{w}\}\); regarding the LP, we only tunes the linear classifier \(\mathbf{f}(\mathbf{w})\) without modification on the frozen \(\mathbf{\phi}(\mathbf{\theta})\).

**Evaluation Metrics.** We take two evaluation metrics, including _Clean Accuracy (C-Acc)_ (i.e., the prediction accuracy of clean samples) and _Attack Success Rate (ASR)_ (i.e., the prediction accuracy of poisoned samples to the target class). A lower ASR indicates a better defense performance.

## 3 Revisiting Backdoor Robustness of Fine-tuning Methods

In this section, we evaluate the aforementioned two widely used fine-tuning paradigms' defensive performance against backdoor attacks with various poisoning rates (FT and LP). Despite that the vanilla FT has been adopted in previous defense work [14, 20, 36, 42], it has not been sufficiently evaluated under various attack settings, particularly in more practical low poisoning rate scenarios. The simple LP method is widely adopted in transfer learning works [5, 16, 21, 22, 28] but still rarely tested for improving model robustness against backdoor attacks. For FT, we try various learning rates during tuning: \(0.01,0.02\), and \(0.03\). For LP, following [16], we try larger learning rates: \(0.3,0.5\), and \(0.7\). We also demonstrate these two methods in Figure 1.

We conduct evaluations on widely used CIFAR-10 [15] dataset with ResNet-18 [11] and test \(4\) representative data-poisoning attacks including BadNet [10], Blended [6], SSBA [18] and Label-Consistent attack (LC) [33]. We include various poisoning rates, \(20\%\), \(10\%5\%\), \(1\%\), and \(0.5\%\) for attacks except for LC since the maximum poisoning rate for LC on CIFAR-10 is \(10\%\). Following previous work [36], we set the target label \(\mathbf{y}_{t}\) as class \(0\). Additional results on other models and datasets are shown in _Appendix C.1_. We aim to figure out _whether these commonly used FT methods can efficiently and consistently purify backdoor triggers in various attack settings_.

Figure 1: The first two methods, LP and vanilla FT, are adopted in Section 3.1. The middle two methods, FE-tuning and FT-init, are proposed in Section 3.2. The final one, FST, is our method introduced in Section 4.

### Revisiting Fine-tuning under Various Poisoning Rates

We stress that the defense method should effectively purify the backdoor triggers while maintaining good clean accuracy. Therefore, we mainly focus on defense performance with a satisfactory clean accuracy level (around \(92\%\)). The results are shown in Figure 2.

_Vanilla FT and LP can purify backdoored models for high poisoning rates._ From Figure 2, We observe that for high poisoning rates (\(\geq 10\%\) for BadNet, Blended, and SSBA; \(5\%\) for LC), both vanilla FT and LP can effectively mitigate backdoor attacks. Specifically, LP (purple lines) significantly reduces the ASR below \(5\%\) on all attacks without significant accuracy degradation (\(\leq 2.5\%\)). Compared with vanilla FT, by simply tuning the linear classifier, LP can achieve better robustness and clean accuracy.

_Vanilla FT and LP both fail to purify backdoor triggers for low poisoning rates._ In contrast to their promising performance under high poisoning rates, both tuning methods fail to defend against backdoor attacks with low poisoning rates (\(\leq 5\%\) for BadNet, Blended, and SSBA; \(<5\%\) for LC). Vanilla FT with larger learning rates performs slightly better on Blended attacks, but it also sacrifices clean accuracy, leading to an intolerant clean accuracy drop. The only exception is related to the SSBA results, where the ASR after tuning at a \(0.5\%\) poisoning rate is low. This can be attributed to the fact that the original backdoored models have a relatively low ASR.

### Exploration of Unstable Defense Performance of Fine-tuning Methods

From the results, a question then comes out: **What leads to differences in defense performance of tuning methods under various poisoning rate settings?** We believe the potential reason for this phenomenon is that _the learned features from feature extractors of backdoored models differ at different poisoning rates, especially in terms of the separability between clean features and backdoor features._ We conduct T-SNE visualizations on learned features from backdoored models with Blended attack (\(10\%\) and \(1\%\) poisoning rates). The results are shown in Figure 3. The targeted class samples are marked with Red color and Black points are backdoored samples. As shown in first two figures,

Figure 3: We take T-SNE visualizations on features from feature extractors with _Blended attack_. We adopt half of the CIFAR-10 test set and ResNet-18. Each color denotes each class, and **Black** points represent backdoored samples. The targeted class is **0 (Red)**. **(1)** The first two figures are feature visualizations of original backdoored models with \(10\%\) and \(1\%\) poisoning rates; **(2)** The rest \(4\) figures are feature visualizations after using different tuning methods for \(1\%\) poisoning rate. We also add the corresponding ASR.

Figure 2: The clean accuracy and ASR of \(4\) attacks on CIFAR-10 and ResNet-18. The \(\mathbf{x}\)-axis means the poisoning rates. The blue and purple lines represent FT and LP, respectively. For FT, we try various learning rates during tuning: \(0.01,0.02\), and \(0.03\). For LP, we try learning rates: \(0.3,0.5,\) and \(0.7\).

under high poisoning rate (\(10\%\)), backdoor features (black points) are clearly separable from clean features (red points) and thus could be easily purified by only tuning the \(\mathbf{f}(\mathbf{w})\); however for low poisoning rate (\(1\%\)), clean and backdoor features are tangled together so that the simple LP is not sufficient for breaking mapping between input triggers and targeted label without further feature shifts. Furthermore, as depicted in the third figure of Figure 3, though vanilla FT updates the whole network containing both \(\mathbf{\theta}\) and \(\mathbf{w}\), it still suffers from providing insufficient feature modification, leading to the failure of backdoor defense.

**Can Feature Shift Help Enhance the Purification Performance of Fine-tuning?** Based on our analysis, we start with a simple strategy in that we could improve backdoor robustness for low poisoning rates by encouraging shifts in learned features. We then propose separate solutions for both LP and vanilla FT to evaluate the effect of feature shift as well as the robustness against two low poisoning rates, \(1\%\) and \(0.5\%\). As shown in Table 1, specifically, for the LP, we freeze \(\mathbf{f}(\mathbf{w})\) and only tune \(\mathbf{\phi}(\mathbf{\theta})\). However, our initial experiments show that directly tuning \(\mathbf{\phi}(\mathbf{\theta})\) does not sufficiently modify learned features since the fixed backdoor linear classifier (denoted as \(\mathbf{f}(\mathbf{w}^{ori})\)) may still restrict modifications of learned features. Inspired by the previous work [27], we first randomly re-initialize the linear classifier and then tune only \(\mathbf{\phi}(\mathbf{\theta})\) (denoted as FE-tuning). For vanilla FT, we also fine-tune the whole network with a randomly re-initialized linear classifier (denoted as FT-init). More implementation details of FE-tuning and FT-init are shown in _Appendix_B.3.

**Evaluations Verify That Encouraging Feature Shift Could Help Purify Backdoored Models.** We observe that both FE-tuning and FT-init could significantly enhance the performance of backdoor purification compared to previous fine-tuning with an average drop of \(77.70\%\) and \(35.61\%\) on ASR respectively. Specifically, FE-tuning leads to a much larger improvement, an ASR decrease of over \(74\%\). As shown in the fifth figure of Figure 3, _after FE-tuning, backdoor features could be clearly separated from the clean features of the target class (red)_. However, this simple strategy also leads to a decrease in clean accuracy (around \(2.9\%\)) in comparison to the original LP, due to the totally randomized classifier. While for the FT-init, the robustness improvements are less significant. As shown in the fourth figure of Figure 3, simply fine-tuning the backdoor model with re-initialized \(\mathbf{f}(\mathbf{w})\) can not lead to enough shifts on backdoor features under the low poisoning rate. The backdoor and clean features of the target class still remain tangled, similar to the original and vanilla FT.

In summary, the results of these two initial methods confirm the fact that _encouraging shifts on learned features is an effective method for purifying backdoors at low poisoning rates._ However, both methods still experience a serious clean accuracy drop or fail to achieve satisfactory improvements in robustness. To further enhance the purification performance, we propose a stronger tuning defense in Section 4 that addresses both issues in a unified manner.

## 4 Feature Shift Tuning: Unified Method to Achieve Stable Improvements

Based on our initial findings, we propose a stronger tuning defense paradigm called **Feature Shift Tuning (FST)**. FST is an end-to-end tuning method and actively shifts features by encouraging the discrepancy between the tuned classifier weight \(\mathbf{w}\) and the original backdoored classifier weight \(\mathbf{w}^{ori}\). The illustration of FST is shown in Figure 1. Formally, starting with reinitializing the linear classifier

\begin{table}
\begin{tabular}{c|c|c c|c c|c c|c c|c c} \hline \multirow{2}{*}{Attack} & \multirow{2}{*}{
\begin{tabular}{c} Puisoning \\ rate \\ \end{tabular} } & \multicolumn{2}{c|}{No defense} & \multicolumn{2}{c|}{LP} & \multicolumn{2}{c|}{FE-tuning} & \multicolumn{2}{c|}{Vanilla FT} & \multicolumn{2}{c}{FT-init} \\ \cline{3-14}  & & C-Acc(\(\uparrow\)) & ASR(\(\downarrow\)) & C-Acc(\(\uparrow\)) & ASR(\(\downarrow\)) & C-Acc(\(\uparrow\)) & ASR(\(\downarrow\)) & C-Acc(\(\uparrow\)) & ASR(\(\downarrow\)) & C-Acc(\(\uparrow\)) & ASR(\(\downarrow\)) \\ \hline \multirow{3}{*}{BadNet} & \(1\%\) & 94.52 & 100 & **94.02** & 100 & 91.56 & **3.18** & 92.12 & 99.83 & 93.37 & 16.72 \\  & 0.5\% & 94.79 & 100 & **94.37** & 100 & 92.37 & **7.41** & 92.56 & 99.07 & 93.90 & 79.32 \\ \hline \multirow{2}{*}{Blended} & \(1\%\) & 95.13 & 98.12 & **94.51** & 98.98 & 92.03 & **8.50** & 92.97 & 82.36 & 93.88 & 61.23 \\  & 0.5\% & 94.45 & 92.46 & **94.29** & 93.72 & 91.84 & **6.48** & 92.95 & 75.99 & 93.71 & 49.80 \\ \hline \multirow{2}{*}{SSBA} & \(1\%\) & 94.83 & 79.54 & **94.23** & 82.39 & 91.99 & **5.58** & 92.59 & 30.16 & 93.51 & 21.04 \\  & 0.5\% & 94.50 & 50.20 & **94.37** & 51.67 & 91.31 & **2.50** & 92.36 & 12.69 & 93.41 & 6.69 \\ \hline \multirow{2}{*}{LC} & \(1\%\) & 94.33 & 99.16 & **94.23** & 99.98 & 91.70 & 64.97 & 92.65 & 94.83 & 93.54 & 89.86 \\  & 0.5\% & 94.89 & 100 & **94.78** & 100 & 91.65 & **22.22** & 92.67 & 99.96 & 93.83 & 96.16 \\ \hline \multicolumn{2}{c|}{Average} & 94.68 & 89.94 & **94.35** & 90.91 & 91.81 & **15.11** & 92.61 & 74.36 & 93.64 & 52.60 \\ \hline \end{tabular}
\end{table}
Table 1: Purification performance of fine-tuning against four types of backdoor attacks with low poisoning rates. All the metrics are measured in percentage (%).

\(\mathbf{w}\), our method aims to solve the following optimization problem:

\[\min_{\mathbf{\theta},\mathbf{w}}\left\{\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim\mathcal{D}_{T}}[ \mathcal{L}(\mathbf{f}(\mathbf{w};\mathbf{\phi}(\mathbf{\theta};\mathbf{x})),\mathbf{y})]+\alpha\left< \mathbf{w},\mathbf{w}^{ori}\right>,\right.\ \ s.t.\left\|\mathbf{w}\right\|_{2}=C\right\} \tag{1}\]

where \(\mathcal{L}\) stands for the original classification cross-entropy loss over whole model parameters to maintain clean accuracy and \(\left<\cdot,\cdot\right>\) denotes the inner product. By adding the regularization on the \(\left<\mathbf{w},\mathbf{w}^{ori}\right>\), FST encourages discrepancy between \(\mathbf{w}\) and \(\mathbf{w}^{ori}\) to guide more shifts on learned features of \(\mathbf{\phi}(\mathbf{\theta})\). The \(\alpha\) balances these two loss terms, the trade-off between clean accuracy and backdoor robustness from feature shift. While maintaining clean accuracy, a larger \(\alpha\) would bring more feature shifts and better backdoor robustness. We simply choose the inner-product \(\left<\mathbf{w},\mathbf{w}^{ori}\right>\) to measure the difference between weights of linear classifiers, since it is easy to implement and could provide satisfactory defensive performance in our initial experiments. Other metrics for measuring differences can also be explored in the future.

```
0: Tuning dataset \(\mathcal{D}_{T}=\{\mathbf{x}_{i},\mathbf{y}_{i}\}_{i=1}^{N}\); backdoored model \(\mathbf{f}(\mathbf{w}^{ori};\mathbf{\phi}(\mathbf{\theta}))\); learning rate \(\eta\); factor \(\alpha\); tuning iterations \(I\)
0: Purified model
1: Initialize \(\mathbf{w}^{0}\) with random weights
2: Initialize \(\mathbf{\theta}^{0}\) with \(\mathbf{\theta}\)
3:for\(i=0,1,\ldots,I\)do
4: Sample mini-batch \(\mathcal{B}_{i}\) from tuning set \(\mathcal{D}_{T}\)
5: Calculate gradients of \(\mathbf{\theta}^{i}\) and \(\mathbf{w}^{i}\): \(\mathbf{g}^{i}_{\theta}=\nabla_{\mathbf{\theta}^{i}}\frac{1}{|\mathcal{B}_{i}|}\sum_{ \mathbf{x}\in\mathcal{B}_{i}}\mathcal{L}(\mathbf{f}(\mathbf{w}^{i};\mathbf{\phi}(\mathbf{\theta}^{ i};\mathbf{x})),\mathbf{y})\); \(\mathbf{g}^{i}_{w}=\nabla_{\mathbf{w}^{i}}[\frac{1}{|\mathcal{B}_{i}|}\sum_{\mathbf{x}\in \mathcal{B}_{i}}\mathcal{L}(\mathbf{f}(\mathbf{w}^{i};\mathbf{\phi}(\mathbf{\theta}^{i};\mathbf{x} )),\mathbf{y})+\alpha\left<\mathbf{w}^{i},\mathbf{w}^{ori}\right>]\);
6: Update model parameters \(\mathbf{\theta}^{i+1}=\mathbf{\theta}^{i}-\eta g^{i}_{\theta},\mathbf{w}^{i+1}=\mathbf{w}^{i}- \eta g^{i}_{w}\)
7: Norm projection of the linear classifier \(\mathbf{w}^{i+1}=\frac{\mathbf{w}^{i+1}}{\|\mathbf{w}^{ori+1}\|_{2}}\|\mathbf{w}^{ori}\|_{2}\)
8:endfor
9:return Purified model \(\mathbf{f}(\mathbf{w}^{I};\mathbf{\phi}(\mathbf{\theta}^{I}))\)
```

**Algorithm 1** Feature Shift Tuning (FST)

**Projection Constraint.** To avoid the \(\mathbf{w}\) exploding and the inner product dominating the loss function during the fine-tuning process, we add an extra constraint on the norm of \(\mathbf{w}\) to stabilize the tuning process. To reduce tuning cost, we directly set it as \(\|\mathbf{w}^{ori}\|\) instead of manually tuning it. _With the constraint to shrink the feasible set, our method quickly converges in just a few epochs while achieving significant robustness improvement._ Compared with previous tuning methods, our method further enhances robustness against backdoor attacks and also greatly improves tuning efficiency (Shown in Section 5.3). Shrinking the range of feasible set also brings extra benefits. _It significantly reduces the requirement for clean samples during the FST process._ As shown in Figure 7 (c,d), FST consistently performs well across various tuning data sizes, even when the tuning set only contains 50 samples. The ablation study of \(\alpha\) is also provided in Section 5.3 and showcases that our method is not sensitive to the selection of \(\alpha\) in a wide range. The overall optimization procedure is summarized in Algorithm 1.

**Unified Improvement for Previous Tuning Methods.** We discuss the connections between FST and our previous FE-tuning and FT-init to interpret why FST could achieve unified improvements on backdoor robustness and clean accuracy. Our objective function Eq.1 could be decomposed into three parts, by minimizing \(\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim\mathcal{D}_{T}}\mathcal{L}(\mathbf{f}(\mathbf{w}; \mathbf{\phi}(\mathbf{\theta};\mathbf{x})),\mathbf{y})\) over \(\mathbf{w}\) and \(\mathbf{\theta}\) respectively, and \(\alpha\left<\mathbf{w},\mathbf{w}^{ori}\right>\) over \(\mathbf{w}\). Compared with FE-tuning, FST brings an extra loss term on \(\min_{\mathbf{w}}\mathbb{E}_{(\mathbf{x},\mathbf{y})\sim\mathcal{D}_{T}}\mathcal{L}(\mathbf{ f}(\mathbf{w};\mathbf{\phi}(\mathbf{\theta};\mathbf{x})),\mathbf{y})+\alpha\left<\mathbf{w},\mathbf{w}^{ori}\right>\) to update linear classifier \(\mathbf{w}\). In other words, while encouraging feature shifts, FST updates the linear classifier with the original loss term to improve the models' clean performance. Compared with the FT-init, by adopting \(\alpha\left<\mathbf{w},\mathbf{w}^{ori}\right>\), FST encourages discrepancy between tuned \(\mathbf{w}\) and original \(\mathbf{w}^{ori}\) to guide more shifts on learned features.

**Weights of Linear Classifier Could Be Good Proxy of Learned Features.** Here, we discuss why we choose the discrepancy between linear classifiers as our regularization in FST. Recalling that our goal is to introduce more shifts in learned features, especially for backdoor features. Therefore, the most direct approach is to explicitly increase the difference between backdoor and clean feature distributions. However, we can not obtain backdoor features without inputting backdoor triggers. We consider that _weights of the original compromised linear classifier could be a good proxy of learned features_. Therefore, we encourage discrepancy between tuned \(\mathbf{w}\) and original \(\mathbf{w}^{ori}\) rather than trying to explicitly promote discrepancy between feature distributions.

Experiments

### Experimental Settings

**Datasets and Models.** We conduct experiments on four widely used image classification datasets, CIFAR-10 [15], GTSRB [32], Tiny-ImageNet [8], and CIFAR-100 [15]. Following previous works [13, 20, 24, 36], we implement backdoor attacks on ResNet-18 [11] for both CIFAR-10 and GTSRB and also explore other architectures in Section 5.3. For CIFAR-100 and Tiny-ImageNet, we adopt pre-trained SwinTransformer [21] (Swin). For both the CIFAR-10 and GTSRB, we follow the previous work [41] and leave \(2\%\) of original training data as the tuning dataset. For the CIFAR-100 and Tiny-ImageNet, we note that a small tuning dataset would hurt the model performance and therefore we increase the tuning dataset to \(5\%\) of the training set.

**Attack Settings.** All backdoor attacks are implemented with BackdoorBench 1. We conduct evaluations against \(6\) representative data-poisoning backdoors, including \(4\) dirty-label attacks (BadNet [10], Blended attack [6], WaNet [24], SSBA[18]) and \(2\) clean-label attacks (SIG attack [1], and Label-consistent attack (LC) [33]). For all the tasks, we set the target label \(\mathbf{y}_{t}\) to 0, and focus on low poisoning rates, \(5\%\), \(1\%\), and \(0.5\%\) in the main experimental section. We also contain experiments of **high poisoning rates**, \(10\%\), \(20\%\), and \(30\%\), in _Appendix_ C.2. For the GTSRB dataset, we do not include LC since it can not insert backdoors into models (ASR \(<10\%\)). Out of all the attacks attempted on Swin and Tiny-ImageNet, only BadNet, Blended, and SSBA were able to successfully insert backdoor triggers into models at low poisoning rates. Other attacks resulted in an ASR of less than \(20\%\). Therefore, we only show the evaluations of these three attacks. More details about attack settings and trigger demonstrations are shown in _Appendix_ B.2.

Footnote 1: [https://github.com/SCLBD/backdoorbench](https://github.com/SCLBD/backdoorbench)

**Baseline Defense Settings.** We compare our FST with \(4\) tuning-based defenses including Fine-tuning with Sharpness-Aware Minimization (FT+SAM), a simplified version adopted from [42], Natural Gradient Fine-tuning (NGF) [14], FE-tuning and FT-init proposed in Section 3.2. We also take a comparison with \(2\) extra strategies including Adversarial Neural Pruning (ANP) [37] and Implicit Backdoor Adversarial Unlearning (I-BAU) [38] which achieve outstanding performance in BackdoorBench [37]. The implementation details of baseline methods are shown in the _Appendix_ B.3. For our FST, we adopt SGD with an initial learning rate of \(0.01\) and set the momentum as \(0.9\) for both CIFAR-10 and GTSRB datasets and decrease the learning rate to \(0.001\) for both CIFAR-100 and Tiny-ImageNet datasets to prevent the large degradation of the original performance. We fine-tune the models for \(10\) epochs on the CIFAR-10; \(15\) epochs on the GTSRB, CIFAR-100 and Tiny-ImageNet. We set the \(\alpha\) as 0.2 for CIFAR-10; \(0.1\) for GTSRB; \(0.001\) for both the CIFAR-100 and Tiny-ImageNet.

### Defense Performance against Backdoor Attacks

In this section, we show the performance comparison between FST with tuning-based backdoor defenses (FT+SAM [42], NGF [14], FE-tuning and FT-init) and current state-of-the-art defense methods, ANP [37] and I-BAU [38]. We demonstrate results of CIFAR-10, GTSRB, and Tiny-ImageNet on Table 2, 3, and 4, respectively. We leave results on CIFAR-100 to _Appendix_ C.3.

Experimental results show that _our proposed FST achieves superior backdoor purification performance compared with existing defense methods_. Apart from Tiny-ImageNet, FST achieves the best performances on CIFAR-10 and GTSRB. The average ASR across all attacks on three datasets are below \(1\%\), \(0.52\%\) in CIFAR-10, \(0.41\%\) in GTSRB, and \(0.19\%\) in Tiny-ImageNet, respectively. Regarding two tuning defense methods, FT+SAM and NGF, FST significantly improves backdoor robustness with larger ASR average drops on three datasets by \(34.04\%\) and \(26.91\%\). Compared with state-of-the-art methods, ANP and I-BAU, on CIFAR-10 and GTSRB, our method outperforms with a large margin by \(11.34\%\) and \(32.16\%\) on average ASR, respectively. ANP is only conducted in BatchNorm layers of ConvNets in source codes. Therefore, it can not be directly conducted in SwinTransformer. Worth noting is that _our method achieves the most stable defense performance across various attack settings_ with \(0.68\%\) on the average standard deviation for ASR. At the same time, our method still maintains high clean accuracy with \(93.07\%\) on CIFAR-10, \(96.17\%\) on GTSRB, and \(79.67\%\) on Tiny-ImageNet on average. Compared to ANP and I-BAU, FST not only enhances backdoor robustness but also improves clean accuracy with a significant boost on the clean accuracy by \(2.5\%\) and \(1.78\%\), respectively.

Following the experimental setting in Section 3.2, we also conduct feature visualization of FST in Figure 3. It can be clearly observed that our FST significantly shifts backdoor features and makes them easily separable from clean features of the target class like FE-tuning. Combined with our outstanding performance, this further verifies that actively shifting learned backdoor features can effectively mitigate backdoor attacks. We also evaluate two initial methods, FE-tuning and FT-init. FE-tuning performs best on Tiny-ImageNet, leading to a little ASR drop by \(0.18\%\) compared with FST. However, it also significantly sacrifices clean accuracy, leading to a large C-ACC drop by \(8.1\%\) compared with FST. FST outperforms them by a large margin for backdoor robustness on CIFAR-10 and GTSRB. As we expected, FST achieves stable improvements in robustness and clean accuracy compared with FE-tuning and FT-init.

**Adaptive Attacks.** To further test the limit of FST, we conduct an adaptive attack evaluation for it. To bypass defense methods based on latent separation property, adaptive poisoning attacks [26] actively suppress the latent separation between backdoor and clean features by adopting an extremely low poisoning rate and adding regularization samples. Their evaluations also show that these attacks successfully bypass existing strong latent separation-based defenses. Hence, we believe it is also equally a powerful adaptive attack against our FST method. We evaluate our method against adaptive attacks. The results of the Adaptive-Blend attack are shown in Table 2, 3. The results of more adaptive attacks and various regularization samples are shown in _Appendix_C.4. We could observe that FST could still effectively purify such an adaptive attack achieving an average drop in ASR by \(81.66\%\) and \(91.75\%\) respectively for the CIFAR-10 and GTSRB.

Figure 4: The T-SNE visualizations of Adaptive-Blend attack (150 payload and 150 regularization samples). Each color denotes each class, and **Black** points represent backdoored samples. The targeted class is **0** (**Red**). The left figure represents the original backdoored model and the right represents the model purified with FST.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \multirow{2}{*}{Attack} & \multicolumn{2}{c|}{Training} & \multicolumn{2}{c|}{No. defenses} & \multicolumn{2}{c|}{ADV} & \multicolumn{2}{c|}{IF-task} & \multicolumn{2}{c|}{NFP} & \multicolumn{2}{c|}{IF-tuning(Class)} & \multicolumn{2}{c|}{FT-train (Class)} & \multicolumn{2}{c|}{FT-train (Class)} & \multicolumn{2}{c}{NFP} & \multicolumn{2}{c}{GTSRB} \\ \cline{3-14}  & m & C-ACC & MSP & 0.54 & C-ACC & MSP & 0.54 & C-ACC & MSP & 0.54 & C-ACC & MSP & 0.54 & C-ACC & MSP & 0.54 & C-ACC & MSP & 0.54 \\ \hline \multirow{2}{*}{Backdoor} & 57 & 94.82 & 90.09 & 91.62 & 0.52 & 1.24 & 1.84 & 91.29 & 2.09 & 2.09 & 2.08 & 91.54 & 3.10 & **91.52** & 3.06 & 20.21 & 91.74 & **6.08** \\  & 57 & 94.52 & 90.00 & 91.62 & 0.52 & 1.72 & 1.84 & 91.29 & 2.09 & 1.09 & 1.09 & 1.09 & 1.09 & **91.52** & **91.57** & 10.51 & **6.08** \\  & 67.55 & 90.00 & 84.81 & 12.17 & 1.04 & 1.09 & 1.09 & 4.08 & 91.84 & 91.84 & 91.74 & 91.49 & 91.72 & 91.50 & **6.08** \\ \hline \multirow{2}{*}{Blendered} & 57 & 94.61 & 90.76 & 90.76 & 91.88 & 53.17 & 90.45 & 45.01 & 91.26 & 23.46 & 10.18 & 11.08 & **91.53** & 90.44 & **90.37** & **3.07** \\  & 57 & 94.61 & 90.66 & 90.53 & 1.76 & 90.57 & 1.70 & 91.26 & 91.26 & 91.26 & 91.26 & 91.18 & 91.18 & 91.18 & 91.18 & 91.18 & 91.18 \\ \hline \multirow{2}{*}{Backdoor} & 57 & 94.61 & 90.66 & 90.53 & 1.76 & 90.57 & 1.70 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 & 91.26 \\  & 57 & 94.61 & 90.66 & 90.53 & 1.76 & 90.57 & 1.72 & 91.26 & 91.26 & 91.

[MISSING_PAGE_FAIL:9]

well across various tuning data sizes. Even if the tuning dataset is reduced to only \(0.1\%\) of the training dataset (50 samples in CIFAR-10), our FST can still achieve an overall ASR of less than \(2\%\).

**Analysis on model architecture.** We extend the experiments on other model architectures including VGG19-BN [31], ResNet-50 [11], and DenseNet161 [12] which are widely used in previous studies [14; 37; 42]. Here we show the results of BadNet and Blended attacks with \(5\%,1\%\), and \(0.5\%\) poisoning rates on the CIFAR-10 dataset. Notably, the structure of VGG19-BN is slightly different where its classifier contains more than one linear layer. Our initial experiments show that directly applying FST to the last layer fails to purify backdoors. Therefore, we simply extend our methods to the whole classifier and observe a significant promotion. We leave the remaining attacks and more implementation details in the _Appendix_D.2. The results presented in Figure 8 show that our FST significantly enhances backdoor robustness for all three architectures by reducing ASR to less than \(5\%\) on average. This suggests that our approach is not influenced by variations in model architecture.

## 6 Conclusion and limitations

In this work, we concentrate on practical Fine-tuning-based backdoor defense methods. We conduct a thorough assessment of widely used tuning methods, vanilla FT and LP. The experiments show that they both completely fail to defend against backdoor attacks with low poisoning rates. Our further experiments reveal that under low poisoning rate scenarios, the backdoor and clean features from the compromised target class are highly entangled together, and thus disentangling the learned features is required to improve backdoor robustness. To address this, we propose a novel defense approach called Feature Shift Tuning (FST), which actively promotes feature shifts. Through extensive evaluations, we demonstrate the effectiveness and stability of FST across various poisoning rates, surpassing existing strategies. However, our tuning methods assume that _the defender would hold a clean tuning set_ which may not be feasible in certain scenarios. Additionally, _they also lead to a slight compromise on accuracy in large models though achieving robustness_. This requires us to pay attention to protect learned pretraining features from being compromised during robust tuning [7; 16; 19; 39].

Figure 8: The purification performance against \(3\) different model architectures on the CIFAR-10 dataset, where Ba- is short for BadNet and Bl- is short for Blended.

Figure 6: The ASR results of four backdoor attacks with varying tuning epochs of tuning methods.

Figure 7: (a) and (b) show C-ACC and ASR of FST with various \(\alpha\). (c) and (d) show the C-ACC and ASR of various sizes of tuning datasets. Experiments are conducted on CIFAR-10 dataset with ResNet-18.

## References

* [1] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In _2019 IEEE International Conference on Image Processing (ICIP)_, pages 101-105. IEEE, 2019.
* [2] Nicholas Carlini and Andreas Terzis. Poisoning and backdooring contrastive learning. In _International Conference on Learning Representations_, 2021.
* [3] Nicholas Carlini and Andreas Terzis. Poisoning and backdooring contrastive learning. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=iC4UHbQ01Mp](https://openreview.net/forum?id=iC4UHbQ01Mp).
* [4] Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tramer. Poisoning web-scale training datasets is practical. _arXiv preprint arXiv:2302.10149_, 2023.
* [5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [6] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. _arXiv preprint arXiv:1712.05526_, 2017.
* [7] Yongqiang Chen, Wei Huang, Kaiwen Zhou, Yatao Bian, Bo Han, and James Cheng. Towards understanding feature learning in out-of-distribution generalization. _arXiv preprint arXiv:2304.11327_, 2023.
* [8] Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an alternative to the cifar datasets. _arXiv preprint arXiv:1707.08819_, 2017.
* [9] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [10] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. _IEEE Access_, 7:47230-47244, 2019.
* [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, June 2016.
* [12] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4700-4708, 2017.
* [13] Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=TysmJ-0HdKI](https://openreview.net/forum?id=TysmJ-0HdKI).
* [14] Nazmul Karim, Abdullah Al Arafat, Umar Khalid, Zhishan Guo, and Nazanin Rahnavard. In search of smooth minima for purifying backdoor in deep neural networks.
* [15] Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
* [16] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=UYneFzXSJWh](https://openreview.net/forum?id=UYneFzXSJWh).
* [17] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learning: Training clean models on poisoned data. _Advances in Neural Information Processing Systems_, 34:14900-14912, 2021.
* [18] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 16463-16472, 2021.
* [19] Yong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujui Yang, and Tong Zhang. Spurious feature diversification improves out-of-distribution generalization. _arXiv preprint arXiv:2309.17230_, 2023.

* [20] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In _International Symposium on Research in Attacks, Intrusions, and Defenses_, pages 273-294. Springer, 2018.
* [21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* [22] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In _International Conference on Machine Learning_, pages 7721-7735. PMLR, 2021.
* [23] Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. _Advances in Neural Information Processing Systems_, 33:3454-3464, 2020.
* imperceptible warping-based backdoor attack. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=eEnBKTtJ0x](https://openreview.net/forum?id=eEnBKTtJ0x).
* [25] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and Quoc V Le. Combined scaling for zero-shot transfer learning. _arXiv preprint arXiv:2111.10050_, 2021.
* [26] Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting the assumption of latent separability for backdoor defenses. In _The eleventh international conference on learning representations_, 2023.
* [27] Zeyu Qin, Liuyi Yao, Daoyuan Chen, Yaliang Li, Bolin Ding, and Minhao Cheng. Revisiting personalized federated learning: Robustness against backdoor attacks. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '23, page 4743-4755, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701030.
* [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [29] Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. Back to the drawing board: A critical evaluation of poisoning attacks on production federated learning. In _2022 IEEE Symposium on Security and Privacy (SP)_, pages 1354-1371. IEEE, 2022.
* [30] Reza Shokri et al. Bypassing backdoor detection algorithms in deep learning. In _2020 IEEE European Symposium on Security and Privacy (EuroS&P)_, pages 175-183. IEEE, 2020.
* [31] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* [32] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. _Neural networks_, 32:323-332, 2012.
* [33] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks. _arXiv preprint arXiv:1912.02771_, 2019.
* [34] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In _2019 IEEE Symposium on Security and Privacy (SP)_, pages 707-723. IEEE, 2019.
* [35] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=gEZrGCozdqR](https://openreview.net/forum?id=gEZrGCozdqR).
* [36] Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, and Chao Shen. Backdoorbench: A comprehensive benchmark of backdoor learning. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [37] Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. _Advances in Neural Information Processing Systems_, 34:16913-16925, 2021.
** [38] Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of backdoors via implicit hypergradient. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?id=MeeQkFYV2w](https://openreview.net/forum?id=MeeQkFYV2w).
* [39] Jianyu Zhang and Leon Bottou. Learning useful representations for shifting tasks and distributions. In _International Conference on Machine Learning_, pages 40830-40850. PMLR, 2023.
* [40] Lin Zhang, Li Shen, Liang Ding, Dacheng Tao, and Ling-Yu Duan. Fine-tuning global model via data-free knowledge distillation for non-iid federated learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10174-10183, 2022.
* [41] Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Data-free backdoor removal based on channel lipschitzness. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part V_, pages 175-191. Springer, 2022.
* [42] Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, and Baoyuan Wu. Enhancing fine-tuning based backdoor defense with sharpness-aware minimization. _arXiv preprint arXiv:2304.11823_, 2023.

Social Impact

Deep Neural Networks (DNNs) are extensively applied in today's society especially for some safety-critical scenarios like autonomous driving and face verification. However, the data-hungry nature of these algorithms requires operators to collect massive amounts of data from diverse sources, making source tracing difficult and increasing the risk of potential malicious issues. For example, attackers can blend poisoned data into benign samples and embed backdoors into models without training control, posing a significant threat to model deployment. Therefore, to mitigate these risks, defenders must remove potential backdoors from models before real-world deployment, ensuring safety and trustworthiness. Our work focuses on a lightweight plug-and-play defense strategy applicable in real scenarios with minimal modifications to existing pipelines. We hope to appeal to the community to prioritize practical defensive strategies that enhance machine learning security.

## Appendix B Experimental Settings

### Datasets and Models.

Following previous works [17; 36; 37; 38] in backdoor literature, we conduct our experiments on four widely used datasets including CIFAR-10, GTSRB, Tiny-ImageNet, and CIFAR-100.

* CIFAR-10 and GTSRB are two widely used datasets in backdoor literature containing images of \(32*32\) resolution of 10 and 43 categories respectively. Following [37; 41], we separate \(2\%\) clean samples from the whole training dataset for backdoor defense and leave the rest training images to implement backdoor models. For these two datasets, we utilize the ResNet-18 to construct the backdoor models.
* CIFAR-100 and Tiny-ImageNet are two datasets with larger scales compared to the CIFAR-10 and GTSRB which contain images with \(64*64\) resolution of 100 and 200 categories respectively. For these two datasets, we enlarge the split ratio and utilize \(5\%\) of the training dataset as backdoor defense since a smaller defense set is likely to hurt the model performance. For these two datasets, we utilize the pre-trained SwinTransformer (pre-trained weights on ImageNet are provided by _PyTorch_) to implement backdoor attacks since we find that training these datasets on ResNet-18 from scratch would yield a worse model performance with C-Acc (\(<70\%\)) on average and therefore is not practical in real scenarios.

### Attack Configurations

We conducted all the experiments with 4 NVIDIA 3090 GPUs.

We implement \(6\) representative poisoning-based attacks and an adaptive attack called Adaptive-Blend [26]. For \(6\) representative attacks, most of them are built with the default configurations2 in BackdoorBench [36]. For the BadNet, we utilize the checkerboard patch as backdoor triggers and stamp the pattern at the lower right corner of the image; for the Blended, we adopt the Hello-Kitty pattern as triggers and set the blend ratio as \(0.2\) for both training and inference phase; for WaNet, we set the size of the backward warping field as 4 and the strength of the wrapping field as 0.5; for SIG, we set the amplitude and frequency of the sinusoidal signal as \(40\) and \(6\) respectively; for SSBA and LC, we adopt the pre-generated invisible trigger from BackdoorBench. For the extra adaptive attack, we utilize the official implementation 3 codes and set both the poisoning rate and cover rate as \(0.003\) following the original paper. The visualization of the backdoored images is shown in Figure 9.

Footnote 2: [https://github.com/SCLBD/backdoorbench](https://github.com/SCLBD/backdoorbench)

Footnote 3: [https://github.com/Unispac/Circumventing-Backdoor-Defenses](https://github.com/Unispac/Circumventing-Backdoor-Defenses)

For CIFAR-10 and GTSRB, we train all the backdoor models with an initial learning rate of \(0.1\) except for the WaNet since we find a large initial learning rate would make the attack collapse, and therefore we decrease the initial learning rate to 0.01. All the backdoor models are trained for \(100\) epochs and \(50\) epochs for CIFAR-10 and GTSRB respectively. For CIFAR-100 and Tiny-ImageNet, we adopt a smaller learning rate of \(0.001\) and fine-tune each model for \(10\) epochs since the SwinTransformer is already pre-trained on ImageNet and upscale the image size up to \(224*224\) before feeding the image to the network.

### Baseline Defense Configurations

We evaluate \(4\) tuning-based defenses and \(2\) extra state-of-the-art defense strategies including both ANP and I-BAU for comparison. For tuning-based defenses, we mainly consider \(2\) recent works including FT+SAM and NGF, and we also compare another \(2\) baseline tuning strategies including FE-tuning and FT-init proposed in our paper. For all defense settings, we set the batch size as \(128\) on CIFAR10 and GTSRB and set the batch size as \(32\) on CIFAR-100 and Tint-ImageNet due to the memory limit.

* FT+SAM: Upon completion of our work, the authors of [42] had not yet made their source code publicly available. Therefore, we implemented a simplified version of their FT-SAM algorithm, where we replaced the optimizer with SAM in the original FT algorithm and called it FT+SAM. For both CIFAR-10 and GTSRB, we set the initial learning rate as \(0.01\) and fine-tune models with \(100\) epochs. We set the \(\rho\) as \(8\) and \(10\) for CIFAR-10 and GTSRB respectively since we find the original settings (\(\rho=2\) for CIFAR-10 and \(\rho=8\) for GTSRB) are not sufficient for backdoor purification in our experiments. For CIFAR-100 and Tiny-ImageNet, we set the initial learning rate as \(0.001\) and \(\rho\) as \(6\), and fine-tune the backdoor model for \(20\) epochs for fair comparison.
* NGF: We adopt the official implementation 4 for NGF. For CIFAR-10 and GTSRB, we set the tuning epochs as \(100\) and the initial learning rate as \(0.015\) and \(0.05\) respectively. While for CIFAR-100 and Tiny-ImageNet, we set the tuning epochs as \(20\) and the initial learning rate as \(0.002\). Footnote 4: [https://github.com/kr-anonymous/ngf-animus](https://github.com/kr-anonymous/ngf-animus)
* FE-tuning: For FE-tuning, we first re-initialize and freeze the parameters in the head. We then only fine-tune the remaining feature extractor. For CIFAR-10 and GTSRB, we set the initial learning rate as \(0.01\) and fine-tune the backdoor model with \(100\) epochs; while for CIFAR-100 and Tiny-ImageNet, we set the initial learning rate as \(0.005\) and fine-tune the backdoor model with \(20\) epochs.
* FT-init: For FT-init, we randomly re-initialize the linear head and fine-tune the whole model architecture. For CIFAR-10 and GTSRB, we set the initial learning rate as \(0.01\) and fine-tune the backdoor model with \(100\) epochs; while for CIFAR-100 and Tiny-ImageNet, we set the initial learning rate as \(0.005\) and fine-tune the backdoor model with \(20\) epochs.
* ANP: We follow the implementation in BackdoorBench and set the perturbation budget as \(0.4\) and the trade-off coefficient as \(0.2\) following the original configuration. We find that within a range of thresholds, the model performance and backdoor robustness are related to the selected threshold. Therefore, we set a threshold range (from \(0.4\) to \(0.9\)) and present the purification results with low ASR and meanwhile maintain the model's performance.
* I-BAU: We follow the implementation in BackdoorBench and set the initial learning rate as \(1e^{-4}\) and utilize \(5\) iterations for fixed-point approximation.

Figure 9: Example images of backdoored samples from CIFAR-10 dataset with \(6\) attacks.

## Appendix C Additional Experimental results

### Additional Results of Revisiting Fine-tuning

In this section, we provide additional experimental results for Section 3 to explore the potential influence of the dataset and model selection. Specifically, in addition to our initial experiments of revisiting fine-tuning on CIFAR-10 with ResNet-18, we further vary the model capacity (ResNet-50 on CIFAR-10), the model architecture (DenseNet-161 on CIFAR-10), and the dataset (ResNet-18 on GTSRB). As mentioned in Section 3.1, we mainly focus on defense performance with a satisfactory clean accuracy level (\(92\%\) on CIFAR-10, \(97\%\) on GTSRB). We tune hyperparameters based on this condition. All the experimental results are shown in Figure 10 respectively. These additional results also demonstrate that Vanilla FT and LP could purify backdoored models for high poisoning rates but fail to defend against low poisoning rates attacks. The only exception is the SSBA results since the original backdoored models have a relatively low ASR, as mentioned in Section 3.1.

### Additional Results of High Poisoning Rates

Our previous experiments in Section 5.2 have demonstrated our FST's superior defense capacity against backdoor attacks with low poisoning rates. In this section, we further extend our attack scenarios with more poisoning samples by increasing the poisoning rate to 10%, 20%, and 30%. We conduct experiments on CIFAR-10 and GTSRB with ResNet-18 and present the experimental results in Table 5. We observe that the FST could easily eliminate the embedded backdoor as expected while preserving a high clean accuracy of the models.

### Additional Results on CIFAR-100 Dataset

We evaluate our FST on the CIFAR-100 dataset with the results shown in Table 6. Since some attacks show less effectiveness under a low poisoning rate with ASR \(<25\%\), we hence only report the results where the backdoor attack is successfully implemented (original ASR \(\geq 25\%\)). We note that our FST could achieve excellent purification performance across all attack types on the CIFAR-100 dataset with an average ASR \(0.36\%\) which is \(65.9\%\) and \(9.46\%\) lower than the ASR of two other tuning strategies, FT+SAM and NGF respectively. Although the FE-tuning could achieve a lower

Figure 10: The Evaluation of vanilla FT and LP: (a) ResNet-50 on CIFAR-10. (b) Dense-161 on CIFAR-10. (c) ResNet-18 on GTSRB.

[MISSING_PAGE_FAIL:17]

(Bypass-Blend). All the experimental results along with three poisoning rates are shown in Table 8. The results reveal that our FST could still mitigate the Bypass attack which emphasizes the importance of feature shift in backdoor purification.

### Additional Results of Projection Constraint Analysis

In this section, we first provide additional analysis of the projection constraint with more attacks (WaNet, SSBA, SIG, and LC) on the CIFAR-10 dataset. We show the experimental results in Figure 12. We get the same observations shown in Section 5.3, where the inclusion of the projection term plays a crucial role in stabilizing and accelerating the convergence process of the FST. This results in a rapid and satisfactory purification of the models within a few epochs.

## Appendix D Extra Ablation Studies

### Efficiency Analysis

We compare the backdoor purification efficiency of our FST with other tuning methods on the remaining three datasets including GTSRB, CIFAR-100, and Tiny-ImageNet. We select three

\begin{table}
\begin{tabular}{c|c|c c|c c} \hline \hline Attack & \multicolumn{2}{c|}{Regularization} & \multicolumn{2}{c|}{No defense} & \multicolumn{2}{c}{FST} \\ \cline{3-6}  & samples & C-Acc(\(\uparrow\)) & ASR(\(\downarrow\)) & C-Acc(\(\uparrow\)) & ASR(\(\downarrow\)) \\ \hline \multirow{2}{*}{Adaptive-Patch} & 150 & 94.55 & 96.77 & 93.58 & 0.28 \\  & 300 & 94.59 & 61.23 & 91.99 & 2.21 \\  & 450 & 94.52 & 54.23 & 91.41 & 5.42 \\ \hline \multirow{2}{*}{Adaptive-Blend} & 150 & 94.86 & 83.03 & 94.35 & 1.37 \\  & 200 & 94.33 & 78.40 & 92.08 & 0.78 \\  & 300 & 94.12 & 68.99 & 92.29 & 1.39 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Defense results of Adaptive-Blend and Adaptive-Patch attacks with various regularization samples. The metrics C-ACC and ASR are measured in percentage.

\begin{table}
\begin{tabular}{c|c|c c|c c} \hline \hline Attack & \multicolumn{2}{c|}{Poisoning} & \multicolumn{2}{c|}{No defense} & \multicolumn{2}{c}{FST} \\ \cline{3-6}  & rate & C-Acc(\(\uparrow\)) & ASR(\(\downarrow\)) & C-Acc(\(\uparrow\)) & ASR(\(\downarrow\)) \\ \hline \multirow{2}{*}{Bypass-Patch} & 5\% & 89.81 & 96.28 & 87.85 & 0.02 \\  & 1\% & 90.04 & 93.90 & 87.83 & 0.03 \\  & 0.5\% & 89.50 & 58.83 & 87.61 & 0.73 \\ \hline \multirow{2}{*}{Bypass-Blend} & 5\% & 87.79 & 99.54 & 89.14 & 0.13 \\  & 1\% & 89.70 & 83.66 & 88.11 & 0.08 \\  & 0.5\% & 89.47 & 85.52 & 87.13 & 0.12 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Defense results of Bypass attacks with three different poisoning rates.

Figure 12: We demonstrate the experimental results with and without projection constraint (w/ C and w/o C, respectively) of four backdoor attacks, namely the WaNet, SSBA, SIG, and LC. The experiments are conducted with three poisoning rates (5%, 1%, and 0.5%) and varying tuning epochs.

representative attacks (BadNet, Blend, and SSBA) with poisoning rate \(1\%\) which could be successfully implemented across three datasets and we present our experimental results in Figure 13, 14 and 15. The experimental results demonstrate that _our FST is efficient compared to the other \(4\) tuning-based backdoor defense which could constantly depress the ASR under a low-value range (usually \(<5\%\)) with only a few epochs_. Besides, we also note that in the GTSRB dataset, both the ASR of FE-tuning and FT-init would increase as the tuning epoch increases indicating the model is gradually recovering the previous backdoor features. Our FST, however, maintains a low ASR along the tuning process which verifies the stability of our method.

### Diverse Model Architecture

We conduct comprehensive evaluations on three model architectures (VGG19-BN, ResNet-50, and DenseNet-161) on the CIFAR-10 dataset with all \(6\) representative poisoning-based backdoor attacks and one adaptive attack, and our experimental results are shown in Table 9, 10 and 11. During our initial experiments, we note that our method is less effective for VGG19-BN. One possible reason is that the classifier of VGG19-BN contains more than one layer which is slightly different from our previously used structure ResNet-18. Therefore, one direct idea is to extend our original last-layer regularization to all the last linear layers of VGG19-BN. For implementation, we simply change the original \(\alpha\left\langle\mathbf{w},\mathbf{w}^{ori}\right\rangle\) to \(\alpha\sum_{i}\left\langle\mathbf{w}_{i},\mathbf{w}_{i}^{ori}\right\rangle\) where \(i\) indicates each linear layer. Based on this, we obtain an obvious promotion of backdoor defense performances (shown in Figure 16) without sacrificing clean accuracy.

Following the results in Table 9, our FST could achieve better and much more stable performance across all attack settings with an average ASR of \(6.18\%\) and a standard deviation of \(11.64\%\). Compared with the four tuning-based defenses, our FST could achieve \(29\%\) lower on ASR average across all the attack settings; compared with the other two state-of-the-art defensive strategies, our FST achieves a much lower ASR while getting a much smaller C-Acc drop (\(<3.5\%\)). For the other two architectures, we note that our FST could achieve the best performance across all attack settings with an average ASR of \(2.7\%\) and maintain clean accuracy (the drop of C-Acc \(<1.9\%\)).

Figure 14: The ASR results of three representative attacks with various tuning epochs. Our experiments are conducted on CIFAR-100 with SwinTransformer.

Figure 13: The ASR results of three representative attacks with various tuning epochs. Our experiments are conducted on GTSRB with ResNet-18.

\begin{table}
\begin{tabular}{c|c|c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Attack} & \multirow{2}{*}{runtime} & Training & No-Baseline & \multicolumn{3}{c|}{AMP} & \multicolumn{3}{c|}{B-MAP} & \multicolumn{3}{c|}{FT-SLAM} & \multicolumn{3}{c|}{MAP} & \multicolumn{3}{c|}{RF} & \multicolumn{3}{c|}{F-frame (Goto)} & \multicolumn{3}{c|}{FT-frame (Goto)} & \multicolumn{3}{c|}{FT-frame (Goto)} & \multicolumn{3}{c|}{FT-frame (Goto)} & \multicolumn{3}{c}{FT-frame (Goto)} \\ \cline{3-14}  & & & & C-MonT & ASRU & C-MonT & ASRU & C-MonT & ASRU & C-MonT & ASRU & C-MonT & ASRU & C-MonT & ASRU & C-MonT & ASRU & C-MonT & ASRU \\ \hline \multirow{2}{*}{Baseline} & \(\%\) & 94.92 & 100.0 & 88.42 & 0.36 & 0.36 & 0.36 & 0.36 & 2.33 & 0.02 & 4.37 & 8.37 & 3.41 & 9.63 & 2.14 & 9.24 & 3.38 & **0.02** & **0.24** \\  & & \(\%\) & 94.76 & 90.08 & 87.12 & 0.39 & 0.42 & 2.01 & 8.00 & 3.36 & 1.86 & 7.73 & 4.18 & 9.12 & 1.09 & **0.32** & 4.44 & 1.09 & **0.02** \\ \hline \multirow{2}{*}{Baseline} & \(\%\) & 94.30 & 96.51 & 91.59 & 2.22 & 0.10 & 6.72 & 9.05 & 2.14 & 5.33 & 82.29 & 4.30 & 9.12 & 10.21 & 9.22 & 4.32 & **0.21** & **0.20** \\  & & \(\%\) & 94.08 & 90.54 & 90.93 & 2.77 & 9.09 & 4.61 & 9.07 & 1.73 & 8.79 & 10.25 & 9.39 & 2.51 & 9.27 & 52.33 & **0.31** & **0.31** \\ \hline \multirow{2}{*}{Baseline} & \(\%\) & 92.11 & 92.13 & 83.40 & 1.29 & 8.32 & 2.58 & 9.18 & 9.02 & 9.02 & 9.14 & 1.00 & 9.22 & 1.11 & 9.24 & 0.08 & (9.21) & **0.33** \\  & & \(\%\) & 92.79 & 91.01 & 8.51 & 1.08 & 1.26 & 9.02 & 9.02 & 9.02 & 9.02 & 1.00 & 9.18 & 1.13 & **0.27** & 0.09 & (9.21) & **0.33** \\ \hline \multirow{2}{*}{SNDA} & \(\%\) & 93.91 & 91.73 & 80.01 & **6.33** & 9.12 & 6.49 & 9.02 & 1.00 & 9.02 & 1.00 & 9.18 & 1.13 & **0.27** & 0.09 & (9.21) & **0.33** \\  & & \(\%\) & 94.34 & 45.42 & 90.19 & **6.72** & 91.78 & 1.39 & 9.08 & 1.34 &Figure 16: We compare regularizing the whole linear layers (denoted as -all) with regularizing only the last linear layer (denoted as -last). We evaluate on CIFAR-10 dataset using BadNet and Blended attacks with 3 poisoning rate settings. Experimental results demonstrate that we could achieve a superior purification performance by regularizing the whole linear layers than the last-layer-only regularization without sacrificing the model performance.

Figure 15: The ASR results of three representative attacks with various tuning epochs. Our experiments are conducted on Tiny-ImageNet with SwinTransformer.