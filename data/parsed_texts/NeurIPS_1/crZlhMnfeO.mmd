# RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency

 Zhuoman Liu Bo Yang Yan Luximon Ajay Kumar Jinxi Li

vLAR Group, The Hong Kong Polytechnic University

{zhuo-man.liu, jinxi.li}@connect.polyu.hk bo.yang@polyu.edu.hk

Corresponding Author

###### Abstract

In this paper, we study the problem of continuous 3D shape representations. The majority of existing successful methods are coordinate-based implicit neural representations. However, they are inefficient to render novel views or recover explicit surface points. A few works start to formulate 3D shapes as ray-based neural functions, but the learned structures are inferior due to the lack of multi-view geometry consistency. To tackle these challenges, we propose a new framework called **RayDF**. It consists of three major components: 1) the simple ray-surface distance field, 2) the novel dual-ray visibility classifier, and 3) a multi-view consistency optimization module to drive the learned ray-surface distances to be multi-view geometry consistent. We extensively evaluate our method on three public datasets, demonstrating remarkable performance in 3D surface point reconstruction on both synthetic and challenging real-world 3D scenes, clearly surpassing existing coordinate-based and ray-based baselines. Most notably, our method achieves a \(1000\times\) faster speed than coordinate-based methods to render an \(800\times 800\) depth image, showing the superiority of our method for 3D shape representation. Our code and data are available at [https://github.com/vLAR-group/RayDF](https://github.com/vLAR-group/RayDF)

## 1 Introduction

Learning accurate and efficient 3D shape representations is crucial for many cutting-edge applications in the fields of machine vision and robotics. Recent advances in 3D coordinate-based neural representations including occupancy fields (OF) [46, 11], un/signed distance fields (U/SDF) [54, 14], and radiance fields (NeRF) [47], have shown great potential to recover complex shapes from RGB/D images and/or point clouds. Although these methods and their variants have achieved excellent performance in a wide range of downstream tasks such as shape reconstruction [55, 58, 60], novel view synthesis [4, 72, 51, 5], and scene understanding [85, 86, 73], obtaining 3D shapes or 2D views from their trained networks is computationally expensive, due to the requirement of extensive network evaluations to regress surface points.

Very recently, a number of works start to represent 3D models as ray-based neural functions. By simply taking individual light rays as input, the methods LFN [64] and NeuLF [40] learn to directly predict the radiance (RGB) values, while PRIF [23] and DDF [3] straightly estimate the surface hitting points of input rays. Compared with coordinate-based representations, these ray-based methods are clearly more efficient to infer 3D geometry and render 2D views, as every single ray only needs to query the trained network once forward. Nevertheless, the learned 3D geometries by these methods are still lack of fidelity, primarily because they fail to explicitly take into account geometry consistency across multiple views, resulting in the network likely over-fitting individual training rays but unable to generalize to unseen rays in testing.

In this paper, we aim to address this key issue of ray-based neural representations by explicitly integrating multi-view geometry consistency into the network design. In particular, we introduce anew pipeline as shown in Figure 1. It consists of two independent neural networks together with a particular training module: 1) the main ray-surface distance network, 2) an auxiliary dual-ray visibility classifier, and 3) a multi-view consistency optimization module.

The **main network** simply takes a ray as input and directly infers the distance between ray origin and its hitting point on the surface. Basically, this network elegantly represents 3D shapes as ray-based implicit functions, denoted as _ray-surface distance fields_, keeping the unique advantage in efficiency for shape extraction and rendering. The **auxiliary network** takes a pair of rays as input and predicts their mutual visibility. In fact, this is a simple binary classifier, aiming at distinguishing whether any two rays hit at the same surface point or not, _i.e_., the mutual visibility. Having a trained auxiliary network at hand, the **multi-view consistency optimization module** specifies how to effectively leverage the learned dual-ray visibility to train the main network, thus driving learned ray-surface distances to be multi-view consistent from any seen or unseen viewing angles.

Since our ray-surface distance fields primarily aim at representing accurate 3D shapes, the whole pipeline is designed to be trained on depth images, although light fields (radiance) can be optionally learned in parallel if color images are also available in training. In this regard, the closest works to ours are PRIF [23] and DDF [3], neither of which explicitly considers the multi-view geometry consistency. Overall, compared with all existing coordinate-based representations, our method keeps the superiority in efficiency thanks to the ray-based formulation. Compared with the existing ray-based approaches, ours excels at learning accurate 3D geometries thanks to the multi-view consistency for **ray**-surface **distance** fields. Our method is called **RayDF** and our contributions are:

* We employ a straightforward ray-surface distance field for representing 3D shapes. This formulation is inherently more efficient than existing coordinate-based representations.
* We design a new dual-ray visibility classifier to learn the spatial relationships of any pair of rays, enabling the learned ray-surface distance fields to be multi-view geometry consistent.
* We demonstrate superior 3D shape reconstruction accuracy and efficiency on multiple datasets, showing significantly better results than existing coordinate-based and ray-based baselines.

## 2 Related Work

**Explicit 3D Shape Representations:** Classic methods to recover explicit 3D geometry of objects and scenes mainly include SfM [53] and SLAM [7]systems such as Colmap [62] and ORB-SLAM [49]. Another classic method is space carving [45; 37], which involves the process of carving out the voxel grid by utilizing multiple distinct views to obtain a robust approximation within the voxel space. To model explicit 3D structures, deep learning-based methods have shown impressive progress in recovering voxel grids [15; 80; 81], point clouds [21], octree [71], polygon meshes [34; 26] and shape primitives [87]. A comprehensive survey of these methods can be found in [6; 28]. Thanks to the large-scale datasets [10] for training sophisticated neural networks [29; 31], these methods demonstrate excellent results in many downstream tasks such as shape reconstruction [76; 70], generation [38], and semantic scene perception [67; 17; 25]. However, the fidelity of these discrete shape representations is primarily limited by their spatial resolutions and memory footprint.

**Coordinate-based Implicit 3D Shape Representations:** To avoid the discretization issue of traditional 3D representations, there has been a growing interest in developing implicit neural 3D representations using simple MLPs to represent continuous 3D shapes. Inspired by the seminal _coordinate-based_ methods [11; 46; 54] which have shown great success in encoding high-quality 3D shapes, a plethora of follow-up works have been developed to tackle various vision tasks in [65; 79; 2; 12; 13; 20; 33; 43; 42; 27; 83]. These coordinate-based representations can be generally classified as: 1) occupancy fields (OF) [46; 11], 2) signed distance fields (SDF) [54], 3) unsigned

Figure 1: The general workflow and components of our framework.

distance fields (NDF) [14; 74], and 4) radiance fields (NeRF) [47]. More related methods can be found in surveys [24; 18]. Among them, the OF/SDF/NDF based methods demonstrate exceptional accuracy in recovering continuous 3D structures thanks to the simple level-set formulation, while the NeRF based methods achieve an unprecedented level of fidelity in rendering 2D views thanks to the successful volume rendering equation. However, all these coordinate-based representations inevitably require dense 3D location sampling and evaluations on trained networks to regress explicit surface points, though advanced techniques [41; 57; 69; 39; 30; 56; 50; 82; 48] can mitigate this issue somewhat but at the expense of a large memory increase. In this paper, our RayDF only needs a single network evaluation to regress every surface point, while still achieving on par or better reconstruction accuracy with existing OF/SDF/NDF methods.

**Ray-based Implicit 3D Shape Representations:** To overcome the inefficiency of coordinate-based 3D representations, a number of works start to formulate 3D shapes as ray-based neural functions via MLPs. Ray-based methods simply take individual rays as input and directly output either radiance values (RGB), _i.e. light fields_, or surface points, _i.e. distance fields_. LFN [64] and NeuLF [40] are among the early works of light fields to encode 3D representations from observed color images. Other light field works include [22; 61; 78; 52; 68; 75; 9; 1]. PRIF [23], DDF [3] and DRDF [36] are the early ray-based distance fields to learn 3D shapes from observed depth scans. Although these methods have shown very encouraging results for novel view rendering or surface reconstruction, they are usually limited to datasets with small baselines across multi-views. Basically, this is because the multi-view geometry consistency is not taken into account in their network designs. By contrast, in our RayDF pipeline, we explicitly introduce a dual-ray visibility classifier to aid our ray-surface distance fields to be multi-view shape consistent during training.

## 3 RayDF

### Overview

Our pipeline consists of two networks and an optimization module. As shown in Figure 2, for the main network: **ray-surface distance field \(f_{\Theta}\)**, it takes a single oriented ray \(\mathbf{r}\) as input, and directly regresses the distance \(d\) between ray starting point and its surface hitting point. For the input ray \(\mathbf{r}\), we opt for the conventional spherical parameterization [32], as it supports querying from 360\({}^{\circ}\) viewing angles. In particular, a fix-sized sphere is predefined with a relatively large diameter \(D\) as a convex hull in which the target 3D scene is bounded. Each surface point \(\mathbf{p}\) of the target scene can be regarded as a directional ray penetrating the sphere with two intersection points on the sphere. Each intersection point is parameterized by two variables specifying the angles with regard to the sphere center. Then, for each ray, we have a 4D parameterization \(\mathbf{r}=(\theta^{in},\phi^{in},\theta^{out},\phi^{out})\). Formally, the ray-surface distance field is defined below and implementation details are in Appendix A.1.

\[d=f_{\Theta}(\mathbf{r}),\quad\text{where }\mathbf{r}=(\theta^{in},\phi^{in},\theta^{ out},\phi^{out}),\quad\Theta\text{ are trainable parameters of MLPs} \tag{1}\]

For the auxiliary network: **dual-ray visibility classifier \(h_{\Theta}\)**, it takes a pair of rays as input and simply predicts their mutual visibility, aiming at explicitly modeling the mutual spatial relationships between any pairs of rays. This network, once well-trained, will play a key role in the third component: **multi-view consistency optimization**. Detailed designs are discussed in Sections 3.2&3.3.

### Dual-ray Visibility Classifier

The main ray-surface network alone can indeed well fit many training ray-distance pairs, but there is no mechanism driving its output distances, _i.e._, surface geometry, to be consistent across multiple views, especially for unseen views. For example, as illustrated in the leftmost block of Figure 3, if

Figure 2: The left block illustrates the spherical parameterization of an input ray \(\mathbf{r}\), and the right block shows our simple MLP-based ray-surface distance field \(f_{\Theta}\).

two input rays \(\mathbf{r}_{1}\) and \(\mathbf{r}_{2}\) are mutually visible in the scene, both rays must hit at the same surface point, then the corresponding ray-surface distances \(d_{1}\) and \(d_{2}\) must satisfy a transformation equation \(\mathbf{r}_{1}^{in}+d_{1}\mathbf{r}_{1}^{\mathbf{d}}=\binom{x_{1}}{y_{1}}=\mathbf{r}_{2}^{in}+d _{2}\mathbf{r}_{2}^{\mathbf{d}}\), where \(\mathbf{r}^{\mathbf{d}}=\frac{\mathbf{r}^{out}-\mathbf{r}^{in}}{\|\mathbf{r}^{out}-\mathbf{r}^{in}\|}\) and \(\mathbf{r}^{*}=\left(\begin{smallmatrix}\sin\theta^{*}\cos\phi^{*}\\ \sin\theta^{*}\sin\phi^{*}\end{smallmatrix}\right)\), \(*\in\{in,out\}\) (More details in Section A.1.3), according to both rays' angles. Similarly, if input rays \(\mathbf{r}_{1}\) and \(\mathbf{r}_{3}\) are mutually invisible, then they should never satisfy the transformation equation because both never meet at the same surface point. From this fundamental principle, we can see that the mutual visibility between any two rays is crucial to inform the ray-surface distance network to be multi-view consistent.

To this end, we design a binary classifier to discriminate the visibility of an input pair of rays. A naive idea is to simply concat two rays as input, feeding into MLPs to regress 0/1, where 1 represents _visible_ and 0 otherwise. However, such a design fails to retain the symmetry of two rays. Here symmetry means that the visibility of two rays must be invariant to the input order of two rays. With this point, as illustrated in the right block of Figure 3, our visibility classifier \(h_{\Phi}\) is formally defined as:

\[h_{\Phi}:\quad MLP_{S}\Big{[}\frac{g(\theta_{1}^{in},\phi_{1}^{in},\theta_{1}^ {out},\phi_{1}^{out})+g(\theta_{2}^{in},\phi_{2}^{in},\theta_{2}^{out},\phi_{2 }^{out})}{2}\oplus k(x_{1},y_{1},z_{1})\Big{]}\to 0/1 \tag{2}\]

where \((\theta_{1}^{in},\phi_{1}^{in},\theta_{1}^{out},\phi_{1}^{out})\) and \((\theta_{2}^{in},\phi_{2}^{in},\theta_{2}^{out},\phi_{2}^{out})\) are the parameterizations of two input rays \(\mathbf{r}_{1}\) and \(\mathbf{r}_{2}\) respectively; \(g()\) is a shared single fully-connected layer followed by an average pooling, thus guaranteeing the symmetry of input rays. Note that, the surface hitting point of \(\mathbf{r}_{1}\), _i.e._, \((x_{1},y_{1},z_{1})\), is also encoded via a separate single fully-connected layer \(k()\), followed by a concatenation operation denoted by \(\oplus\), thus enhancing the pooled ray features, as we empirically find that such an enhancement could notably improve the classifier's accuracy. Implementation details are in Appendix A.1.

### Multi-view Consistency Optimization

With the designed main ray-surface distance network \(f_{\Theta}\) and the auxiliary dual-ray visibility classifier \(h_{\Phi}\) at hand, we introduce the crucial multi-view consistency optimization module to train both networks. In particular, given \(K\) posed depth images (\(H\times W\)) of a static 3D scene as the whole training data, our training module consists of two stages.

**Stage 1 - Training Dual-ray Visibility Classifier**

The key to training this classifier is to create correct data pairs. First of all, all raw depth values are converted to ray-surface distance values. For a specific \(i^{th}\) ray (pixel) in the \(k^{th}\) image, we project its ray-surface point back to the remaining \((K-1)\) scans, obtaining the corresponding \((K-1)\) distance values. We set 10 millimeters as the _closeness_ threshold to determine whether the projected \((K-1)\) rays are visible in the \((K-1)\) images. In total, we generate \(K*H*W*(K-1)\) pairs of rays together with 0/1 labels. The standard cross-entropy loss function is adopted to optimize our dual-ray visibility classifier. More details on training data generation and implementation are in Appendix A.1.3.

Note that, this classifier is trained in a scene-specific fashion. Once the network is well-trained, it basically encodes the relationships between any two rays of a specific scene into network weights.

**Stage 2 - Training Ray-surface Distance Network**

The ultimate goal of our whole pipeline is to optimize the main ray-surface network and drive it to be multi-view geometry consistent. However, this is non-trivial as simply fitting the network with ray-surface data points cannot generalize to unseen rays, which can be seen in our ablation study in Section 4.5. In this regard, we fully leverage the well-trained visibility classifier to aid our training of ray-surface distance network. Particularly, this stage consists of the following key steps:

Figure 3: The leftmost block illustrates the mutual visibility of a pair of rays. The remaining block shows the symmetric design of our dual-ray visibility classifier \(h_{\Phi}\).

* Step 1: All depth images are converted to ray-surface distances, generating \(K*H*W\) training ray-distance pairs for a specific 3D scene.
* Step 2: As illustrated in Figure 4, for a specific training ray (\(\mathbf{r},d\)), called _primary ray_, we uniformly sample \(M\) rays \(\{\mathbf{r}^{1}\cdots\mathbf{r}^{n}\cdots\mathbf{r}^{M}\}\), called _multi-view rays_, in a ball centering at the surface point \(\mathbf{p}\). We then calculate the distance between surface point \(\mathbf{p}\) and the bounding sphere along each of \(M\) rays, obtaining multi-view distances \(\{\hat{d}^{1}\cdots\hat{d}^{m}\cdots\hat{d}^{M}\}\). This can be easily achieved according to the given distance \(d\) in the training set. \(M\) is simply set as \(20\) and more details are in Appendix A.4.
* Step 3: We establish \(M\) pairs of rays \(\{(\mathbf{r},\mathbf{p},\mathbf{r}^{1})\cdots(\mathbf{r},\mathbf{p},\mathbf{r}^{m})\cdots(\mathbf{r},\mathbf{ p},\mathbf{r}^{M})\}\) and then feed them into the well-trained visibility classifier \(h_{\Theta}\), inferring their visibility scores \(\{v^{1}\cdots v^{m}\cdots v^{M}\}\).
* Step 4: We feed the primary ray and all sampled \(M\) multi-view rays \(\{\mathbf{r},\mathbf{r}^{1}\cdots\mathbf{r}^{m}\cdots\mathbf{r}^{M}\}\) into the ray-surface distance network \(f_{\Theta}\), estimating their surface distances \(\{\hat{d},\hat{d}^{1}\cdots\hat{d}^{m}\cdots\hat{d}^{M}\}\). Since the network \(f_{\Theta}\) is randomly initialized, thus the estimated distances are inaccurate in the beginning.
* Step 5: We design the following multi-view consistency loss function to optimize the ray-surface distance network until convergence: \[\ell_{mv}=\frac{1}{\sum_{m=1}^{M}v^{m}+1}\left(|\hat{d}-d|+\sum_{m=1}^{M}\left( |\hat{d}^{m}-\hat{d}^{m}|*v^{m}\right)\right)\] (3) Basically, this simple loss drives the network to not only fit the primary ray-surface distance (seen rays in the training set), but also satisfy that the visible multi-view rays (unlimited unseen rays in the training set) also have accurate distance estimations.

### Surface Normal Derivation and Outlier Points Removal

In the above Sections 3.1&3.2&3.3, we have two network designs and an optimization module to train them separately. Nevertheless, we empirically find that the main ray-surface distance network may predict inaccurate distance values, particularly for rays near sharp edges. Essentially, this is because the actual ray-surface distances may be discontinuous at sharp edges given extreme viewing angle changes. This shape discontinuity is actually a common challenge for almost all existing implicit neural representations, because modern neural networks can only model continuous functions in theory.

Fortunately, a nice property of our ray-surface distance field is that the normal vector at every estimated 3D surface point can be easily derived in a closed-form expression using auto differentiation of the network. In particular, given an input ray \(\mathbf{r}=(\mathbf{\theta}^{in},\mathbf{\phi}^{in},\mathbf{\theta}^{out},\mathbf{\phi}^{out})\), and its estimated ray-surface distance \(\hat{d}\) from network \(f_{\Theta}\), the corresponding normal vector \(\mathbf{n}\) of that estimated surface point can be derived as a specific function shown below.

\[\mathbf{n}=Q\Big{(}\frac{\partial\hat{d}}{\partial\mathbf{r}},\mathbf{r},D\Big{)},\ \text{ details of the function $Q$ and derivation are in Appendix A.2}. \tag{4}\]

With this normal vector, we may choose to add an additional loss to regularize the estimated surface points to be as smooth as possible. Yet, we empirically find that the overall performance improvement on an entire 3D scene is rather limited, as these extremely discontinuous cases are actually sparse.

In this regard, we turn to simply removing the predicted surface points, _i.e._, outliers, whose normal vectors' Euclidean distances are larger than a threshold in the network inference stage. In fact, PRIF [23] also adopts a similar strategy to filter out outliers. Note that, advanced smoothing or interpolating techniques may be integrated to improve our framework, which is left for future exploration.

## 4 Experiments

Our method is evaluated on three types of public datasets: 1) the object-level synthetic Blender dataset from the original NeRF paper [47], 2) the scene-level synthetic DM-SR dataset from the recent DM-NeRF paper [73], and 3) the scene-level real-world ScanNet dataset [16].

**Baselines:** We carefully select the following six successful and representative implicit neural shape representations as our baselines: 1) OF [46], 2) DeepSDF [54], 3) NDF [14], 4) NeuS [77], 5) DS-NeRF [19], 6) LFN [64], and 7) PRIF [23]. The OF/DeepSDF/NDF/NeuS methods are coordinate-based level-set methods, showing outstanding performance in modeling 3D structures. DS-NeRF

Figure 4: Multi-view ray sampling.

is a depth-supervised NeRF [47], inheriting excellent capability in rendering 2D views. LFN and PRIF are two ray-based methods with superior efficiency in generating 2D views. We note that there are many sophisticated variants of these baselines, achieving SOTA performance on various datasets. Nevertheless, we do not intend to comprehensively compare with them, basically because many of their techniques such as more advanced implementations, adding additional conditions, replacing with more powerful backbones, _etc._, can be easily integrated into our framework as well. We leave these potential improvements for future exploration but only focus on our vanilla ray-surface distance field in this paper. For a fair comparison, all baselines are supervised with the same amount of depth scans as ours, carefully trained from scratch in the same scene-specific fashion. More details about the implementation of all baselines and possible minor adaptations are in Appendix A.3.1.

**Metrics:** For evaluation metrics of shape reconstruction, we report: 1) the per ray-surface **absolute distance error (ADE)** in centimeters averaged across all testing images, 2) the **Chamfer distance (CD)**[21] between the whole reconstructed structure and the ground truth shape for each scene. As to our method, we use the existing TSDF fusion [84] to obtain a full mesh from predicted depths at the testing views, on which we uniformly sample 30K points. Another 30K points are uniformly sampled from the ground truth 3D mesh for calculating CD. The outlier point removal is only applied to clean the reconstructed point clouds when calculating CD. No additional post-processing steps are employed when computing all ADE scores. Apparently, ADE is more accurate to evaluate the surface estimation of our method, because the CD scores may be biased due to the extra TSDF fusion. In this regard, the CD scores are just presented as a complementary metric to show the general shape quality in the whole 3D scene space. More details about how to obtain the reconstructed 3D full shape for each baseline and our method are in Appendix A.3.2. For evaluation metrics of appearance reconstruction, _i.e._ novel view synthesis of 2D color images, the standard **PSNR**, **SSIM**, and **LPIPS** scores are reported following NeRF [47]. We present **Accuracy** (%) and **F1-Score** (%) as evaluation metrics of the dual-ray visibility classifier. For qualitative results, refer to Figure 5 and Appendix A.7.

### Efficiency of RayDF

Similar to the existing ray-based methods LFN and PRIF, our RayDF also has the superiority in efficiency to generate 2D images or recover the explicit surface points. To quantitatively compare the efficiency with baselines, we conduct a simple experiment to generate an \(800\times 800\) depth image on a computer with a single NVIDIA RTX 3090 GPU card and a CPU of AMD Ryzen 7. As shown in Table 1, it can be seen that, not surprisingly, the coordinate-based methods OF DeepSDF/NDF/NeuS/ DS-NeRF are extremely slow to render a high-resolution depth image. In particular, OF [46] needs a large number of small steps to gradually approach the surface point along a given light ray direction, while DeepSDF [54] and NDF [14] rely on expensive sphere tracing to regress the points. NeuS [77] and DS-NeRF [19] need extra post-processing steps to calculate depth values from densities. By contrast, our RayDF and the existing ray-based methods achieve more than 1000\(\times\) faster speed to render a dense depth view.

### Evaluation on Blender Dataset

The Blender dataset from NeRF [47] consists of pathtraced images of 8 synthetic objects with complicated geometry and realistic materials. Each object has 100 views for training and 200 novel views for testing. Each image has \(800\times 800\) pixels. Since the released dataset does not include depth images, we just use the provided Blender files to generate additional depth scans of the same resolution exactly following the original 2D view poses. Note that, the physical sizes of these models are about 2.5 meters in length/height/width, so the sphere diameter \(D\) is set as 3 meters in our method. For the baselines and our method, we conduct the following two groups of experiments.

* 3D Shape Representation Only:** Since our RayDF needs depth scans in training to learn continuous 3D shape representations, we conduct this group of experiments only on multi-view depth images. All baselines are also trained on the same number of depth views.
* 3D Shape and Appearance Representations:** Our RayDF is also flexible to add a parallel branch to output radiance field, _i.e._, RGB values, so that the continuous appearance representations can be simultaneously learned. In this regard, we conduct this group of experiments on both RGB images and depth images. The baselines NeuS/ DS-NeRF/ LFN/ PRIF are also trained on the same RGBD images for comparison. The other three methods OF/ DeepSDF/ NDF are not

\begin{table}
\begin{tabular}{r|r} \hline \hline  & Time \\ \hline OF [46] & 286.057 \\ DeepSDF [54] & 17.590 \\ NDF [14] & 28.310 \\ NeuS [77] & 32.793 \\ DS-NeRF [19] & 25.612 \\ LFN [64] & 0.017 \\ PRIF [23] & **0.013** \\
**RayDF (Ours)** & 0.019 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Rendering time consumption (seconds).

included here, because it is non-trivial to add RGB supervision due to their level-set formulation.

Details of the parallel branch for all methods are in Appendix A.3.

**Analysis:** Table 2 shows the quantitative comparison. We can see that: 1) Our RayDF achieves significantly better results for explicit surface recovering on both groups of experiments, especially on the most important ADE metric, demonstrating the clear superiority over both coordinate and ray based baselines. 2) Our method also achieves comparable performance with DS-NeRF for novel view synthesis, being better than LFN and PRIF and showing the flexibility of our framework.

### Evaluation on DM-SR Dataset

The DM-SR dataset from the recent DM-NeRF paper [73] consists of 8 synthetic complex 3D indoor rooms. The room types and designs follow Hypersim dataset [59], and the rendering trajectories for both training and testing images follow the Blender dataset of NeRF [47]. For each 3D scene, there are 300 views for training and 100 novel views for testing. Each view has \(400\times 400\) pixels. Each scene has a physical size of about 10 meters in length/height/width, so the sphere diameter \(D\) is set as 11 meters in our method. Similarly, we conduct the following two groups of experiments.

* 3D Shape Representation Only:** We conduct this group of experiments only on multi-view depth images for all methods.
* 3D Shape and Appearance Representations:** We conduct this group of experiments on both RGB images and depth images for NeuS/ DS-NeRF/ LFN/ PRIF and our method.

**Analysis:** From Table 3, it can be seen that our method again surpasses all baselines on the most critical ADE metric in both groups of experiments, though the rough metric CD scores are just comparable with baselines potentially due to the inaccuracy incurred by external TSDF fusion. In the meantime, our method still obtains high-quality novel view synthesis, without noticeably sacrificing shape representation when trained with both RGB images and depth scans in Group 2.

### Evaluation on ScanNet Dataset

We further evaluate our method on the challenging real-world 3D dataset ScanNet [16]. We randomly select 6 scenes for evaluation. Originally, each scene has more than 2000 RGBD scans in a relatively

\begin{table}
\begin{tabular}{r|c c||c c c c c} \hline \hline  & \multicolumn{3}{c||}{Group 1} & \multicolumn{3}{c}{Group 2} \\  & \multicolumn{3}{c||}{CD (\(\times 10^{-3}\))\(\downarrow\)} & \multirow{2}{*}{ADE\(\downarrow\)} & \multirow{2}{*}{CD (\(\times 10^{-3}\))\(\downarrow\)} & \multirow{2}{*}{PSNR\(\uparrow\)} & \multirow{2}{*}{SSIM \(\uparrow\)} & \multirow{2}{*}{LPIPS \(\downarrow\)} \\  & & & mean / median & & & & mean / median & \\ \hline OF [46] & 10.57 & 2.982 / 0.706 & - & - & - & - & - & - \\ DeepSDF [54] & 12.95 & 3.382 / 0.679 & - & - & - & - & - & - \\ NDF [14] & 12.14 & **2.976** / 0.831 & - & - & - & - & - & - \\ NeuS [77] & 11.88 & 4.756 / 0.907 & 12.10 & 4.662 / 0.938 & **27.19** & 0.910 & 0.100 \\ DS-NeRF [19] & 13.22 & 117.270 / 1.135 & 14.64 & 143.295 / 1.760 & 26.63 & **0.933** & **0.063** \\ LFN [64] & 24.47 & 89.425 / 15.681 & 12.33 & 60.289 / 1.230 & 23.20 & 0.888 & 0.125 \\ PRIF [23] & 14.68 & 20.764 / 1.677 & 14.56 & 21.279 / 1.693 & 23.31 & 0.874 & 0.152 \\
**RayDF (Ours)** & **7.97** & 3.388 / **0.663** & **8.17** & **3.295 / 0.755** & 26.52 & 0.910 & 0.099 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Quantitative results of all baselines and our method in experiments of Groups 1&2. All scores are averaged out across 8 scenes of the Blender dataset [47].

\begin{table}
\begin{tabular}{r|c c||c c c c} \hline \hline  & \multicolumn{3}{c||}{Group 1} & \multicolumn{3}{c}{Group 2} \\  & \multicolumn{3}{c||}{CD (\(\times 10^{-3}\))\(\downarrow\)} & \multirow{2}{*}{ADE\(\downarrow\)} & \multirow{2}{*}{CD (\(\times 10^{-3}\))\(\downarrow\)} & \multirow{2}{*}{PSNR\(\uparrow\)} & \multirow{2}{*}{SSIM \(\uparrow\)} & \multirow{2}{*}{LPIPS \(\downarrow\)} \\  & & mean / median & & & & mean / median & \\ \hline OF [46] & 15.83 & 11.402 / 4.888 & - & - & - & - & - \\ DeepSDF [54] & 16.97 & **11.281** / 5.087 & - & - & - & - & - \\ NDF [14] & 22.41 & 12.300 / 5.911 & - & - & - & - & - \\ NeuS [77] & 9.94 & 12.744 / **4.620** & 11.66 & 15.017 / 5.308 & **33.22** & 0.965 & 0.054 \\ DS-NeRF [19] & 10.77 & 25.380 / 6.032 & 10.77 & 25.548 / 6.102 & 31.83 & **0.977** & **0.026** \\ LFN [64] & 18.30 & 13.673 / 5.372 & 18.51 & **14.085** / 5.349 & 30.86 & 0.930 & 0.111 \\ PRIF [23] & 11.89 & 25.993 / 5.159 & 11.77 & 24.842 / **5.156** & 31.01 & 0.933 & 0.111 \\
**RayDF (Ours)** & **7.41** & 14.272 / 5.353 & **7.97** & 14.251 / 5.300 & 30.32 & 0.940 & 0.113 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative results of all baselines and our method in experiments of Groups 1&2. All scores are averaged out across 8 scenes of the DM-SR dataset [73].

[MISSING_PAGE_FAIL:8]

**Analysis:** Table 5 (1) clearly shows that, without the aid of our dual-ray visibility classifier, the main ray-surface distance field completely collapses and cannot predict reasonable distance values for novel rays in the test set. In Table 5 (2) and (3), with the input of surface point position, the classifier achieves higher accuracy and higher F1-score, thus providing the ray-surface distance network with more accurate visibility information to predict precise distance values. In addition, concatenating two input rays directly disrupts the inherent symmetry of input rays. This is shown in Table 5 (4), where the classifier trained in this way achieves high accuracy but exhibits a low F1-score. This indicates that such a classifier is significantly less robust than the one trained with symmetric input rays. Table 6 demonstrates that a less robust classifier results in the ray-surface distance network predicting inaccurate distances.

In addition to the dual-ray visibility classifier, we also perform various ablation studies on our entire pipeline on the Blender dataset.

**Ablation on Multi-view Rays Sampling:** We conduct an ablation study with different values of \(M\in\{10,20,40\}\) to determine the optimal number of multi-view rays for training a ray-surface distance network. Table 7 demonstrates that increasing the sampled multi-view rays enhances the accuracy of the ray-surface distance network. Although \(M=40\) achieves lower ADE and CD values, the substantial GPU memory and extended training time outweigh the limited performance improvement. Therefore, we opt to sample multi-view rays with \(M=20\) to train our ray-surface distance network in all main experiments.

**Ablation on Sparse Depth Supervision:** With the advancement of existing techniques of depth estimation from RGB images, it is quite feasible to obtain sparse depth signals using existing techniques such as SfM or learning-based monocular depth estimators. In this regard, we additionally provide experimental results using sparse depth supervision. In Table 8, our method maintains satisfactory performance, even with only 1% depth values during training. We hypothesize that such robustness comes from our multi-view consistency constraint, because many depth values in the training set may be redundant thanks to our effective classifier.

**Ablation of One-stage RayDF:** In the paper, we adopt a two-stage training strategy for our dual-ray visibility classifier and ray-surface distance network. For a comparison, we simply train both networks simultaneously. However, not surprisingly, the performance of one-stage training drops noticeably as shown in Table 9. This drop in performance is primarily because the classifier is inaccurate at the early stage and unlikely to provide effective constraints for the ray-surface distance network, given the similar number of training steps. Nevertheless, exploring one-stage training remains an intriguing direction for our future research efforts.

For a more comprehensive analysis, we provide extensive ablation studies and additional qualitative as well as detailed quantitative results of each scene in the Blender dataset in Appendix A.4.

## 5 Conclusion

In this paper, we have shown that it is truly possible to efficiently and accurately learn 3D shape representations by using a multi-view consistent ray-based framework. In contrast to the existing coordinate-based methods, we instead use a simple ray-surface distance field to represent 3D shape geometries, which is further driven by a novel dual-ray visibility classifier to be multi-view shape consistent. Extensive experiments on multiple datasets demonstrate the extremely high rendering efficiency and outstanding performance of our approach. It would be interesting to extend our framework with more advanced techniques such as faster implementation and additional regularizations.

\begin{table}
\begin{tabular}{l|c c} \hline \hline \multirow{2}{*}{Depth Sparsity} & \multirow{2}{*}{ADE\(\downarrow\)} & CD (\(\times 10^{-3}\)) \(\downarrow\) \\  & & mean / median \\ \hline
1\% & 8.54 & **3.301** / 0.937 \\
5\% & 8.70 & 3.250 / 0.920 \\
10\% & 8.68 & 3.313 / 0.924 \\
**100\% (RayDF)** & **7.97** & 3.388 / **0.663** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Ablation study on sparse depth supervision.

\begin{table}
\begin{tabular}{l|c c} \hline \hline  & ADE\(\downarrow\) & CD (\(\times 10^{-3}\)) \(\downarrow\) \\  & & mean / median \\ \hline one-stage & 12.41 & 4.032 / **0.659** \\
**two-stage (RayDF)** & **7.97** & **3.388** / 0.663 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Ablation study on one/two-stage training.

Figure 5: Qualitative results of all methods on the three datasets. More qualitative results can be found in Appendix A.7 and our project page: [https://vlar-group.github.io/RayDF.html](https://vlar-group.github.io/RayDF.html)

**Acknowledgement:** This work was supported in part by Research Grants Council of Hong Kong under Grants 15225522 & 25207822 & 15606321, in part by National Natural Science Foundation of China under Grant 62271431, and in part by The Hong Kong Polytechnic University under Grant P0031501.

## References

* [1] B. Attal, J. Huang, C. Richardt, M. Zollhoefer, J. Kopf, M. O'Toole, and C. Kim. HyperReel: High-fidelity 6-DoF video with ray-conditioned sampling. _CVPR_, 2023.
* [2] M. Atzmon and Y. Lipman. SAL: Sign Agnostic Learning of Shapes from Raw Data. _CVPR_, 2020.
* [3] T. Aumentado-Armstrong, S. Tsogkas, S. Dickinson, and A. Jepson. Representing 3D Shapes with Probabilistic Directed Distance Fields. _CVPR_, 2022.
* [4] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. M.-b. Pratul, and C. V. Mar. Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. _ICCV_, 2021.
* [5] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. _CVPR_, 2022.
* [6] M. Berger, A. Tagliasacchi, L. M. Seversky, P. Alliez, J. A. Levine, A. Sharf, and C. T. Silva. A Survey of Surface Reconstruction from Point Clouds. _Computer Graphics Forum_, 36(1):301-329, 2017.
* [7] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. D. Reid, and J. J. Leonard. Past, Present, and Future of Simultaneous Localization and Mapping: Towards the Robust-Perception Age. _TRO_, 2016.
* [8] E. Camahort, A. Lerios, and D. S. Fussell. Uniformly sampled light fields. _Eurographics Workshop on Rendering Techniques_, 1998.
* [9] J. Cao, H. Wang, P. Chemerys, V. Shakhrai, J. Hu, Y. Fu, D. Makoviichuk, S. Tulyakov, and J. Ren. Real-Time Neural Light Field on Mobile Devices. _CVPR_, 2023.
* [10] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu. ShapeNet: An Information-Rich 3D Model Repository. _arXiv:1512.03012_, 2015.
* [11] Z. Chen and H. Zhang. Learning Implicit Fields for Generative Shape Modeling. _CVPR_, 2019.
* [12] Z. Chen, Y. Zhang, K. Genova, S. Fanello, S. Bouaziz, C. Haene, R. Du, C. Keskin, T. Funkhouser, and D. Tang. Multiresolution Deep Implicit Functions for 3D Shape Representation. _ICCV_, 2021.
* [13] J. Chibane, T. Alldieck, and G. Pons-Moll. Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion. _CVPR_, 2020.
* [14] J. Chibane, A. Mir, and G. Pons-Moll. Neural Unsigned Distance Fields for Implicit Function Learning. _NeurIPS_, 2020.
* [15] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction. _ECCV_, 2016.
* [16] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Niessner. ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes. _CVPR_, 2017.
* [17] A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and M. Niessner. ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans. _CVPR_, 2018.
* [18] L. De Luigi, A. Cardace, R. Spezialetti, P. Z. Ramirez, S. Salti, and L. Di Stefano. Deep Learning on Implicit Neural Representations of Shapes. _ICLR_, 2023.
* [19] K. Deng, A. Liu, J.-Y. Zhu, and D. Ramanan. Depth-supervised NeRF: Fewer Views and Faster Training for Free. _CVPR_, 2022.
* [20] Y. Duan, H. Zhu, H. Wang, L. Yi, R. Nevatia, and L. J. Guibas. Curriculum DeepSDF. _ECCV_, 2020.
* [21] H. Fan, H. Su, and L. Guibas. A Point Set Generation Network for 3D Object Reconstruction from a Single Image. _CVPR_, 2017.

* [22] B. Y. Feng and A. Varshney. Signet: Efficient neural representation for light fields. _ICCV_, 2021.
* [23] B. Y. Feng, Y. Zhang, D. Tang, and R. Du. PRIF: Primary Ray-based Implicit Function. _ECCV_, 2022.
* [24] A. T. O. Fried, J. T. V. Sitzmann, S. L. K. Sunkavalli, and R. M.-b. T. S. J. Saragih. State of the Art on Neural Rendering. _Computer Graphics Forum_, 2020.
* [25] G. Gkioxari, J. Malik, and J. Johnson. Mesh R-CNN. _ICCV_, 2019.
* [26] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and M. Aubry. A Papier-Mache Approach to Learning 3D Surface Generation. _CVPR_, 2018.
* [27] Guandao Yang, S. Belongie, B. Hariharan, and V. Koltun. Geometry Processing with Neural Fields. _NeurIPS_, 2021.
* [28] X.-f. Han, H. Laga, and M. Bennamoun. Image-based 3D Object Reconstruction: State-of-the-Art and Trends in the Deep Learning Era. _TPAMI_, 2019.
* [29] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. _CVPR_, 2016.
* [30] P. Hedman, P. P. Srinivasan, B. Mildenhall, J. T. Barron, and P. Debevec. Baking Neural Radiance Fields for Real-Time View Synthesis. _ICCV_, 2021.
* [31] H. Huang, D. Yang, G. Dai, Z. Han, Y. Wang, K.-M. Lam, F. Yang, S. Huang, Y. Liu, and M. He. Agtgan: Unpaired image translation for photographic ancient character generation. _ACM MM_, 2022.
* [32] I. Ihm, S. Park, R. K. Lee, and M.-g. Seoul. Rendering of Spherical Light Fields. _PG_, 1997.
* [33] Y. Jiang, D. Ji, Z. Han, and M. Zwicker. SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape Optimization. _CVPR_, 2020.
* [34] H. Kato, Y. Ushiku, and T. Harada. Neural 3D Mesh Renderer. _CVPR_, 2018.
* [35] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. _ICLR_, 2015.
* [36] N. Kulkarni, J. Johnson, and D. F. Fouhey. Directed Ray Distance Functions for 3D Scene Reconstruction. _ECCV_, 2022.
* [37] K. N. Kutulakos and S. M. Seitz. A theory of shape by space carving. _IJCV_, 2000.
* [38] C.-H. Lin, C. Kong, and S. Lucey. Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction. _AAAI_, 2018.
* [39] D. B. Lindell, J. N. P. Martel, and G. Wetzstein. AutoInt: Automatic Integration for Fast Neural Volume Rendering. _CVPR_, 2021.
* [40] C. Liu, Z. Li, J. Yuan, and Y. Xu. NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field. _Eurographics Symposium on Rendering_, 2022.
* [41] L. Liu, J. Gu, K. Z. Lin, T.-S. Chua, and C. Theobalt. Neural Sparse Voxel Fields. _NeurIPS_, 2020.
* [42] S. Liu, S. Saito, W. Chen, and H. Li. Learning to Infer Implicit Surfaces without 3D Supervision. _NeurIPS_, 2019.
* [43] S. Liu, Y. Zhang, S. Peng, B. Shi, M. Pollefeys, and Z. Cui. DIST: Rendering Deep Implicit Signed Distance Function with Differentiable Sphere Tracing. _CVPR_, 2020.
* [44] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. _ICLR_, 2017.
* [45] W. Matusik, C. Buehler, R. Raskar, S. J. Gortler, and L. McMillan. Image-based visual hulls. _SIGGRAPH_, 2000.
* [46] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy Networks: Learning 3D Reconstruction in Function Space. _CVPR_, 2019.
* [47] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. _ECCV_, 2020.
* [48] T. Muller, A. Evans, C. Schied, and A. Keller. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. _SIGGRAPH_, 2022.

* [49] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos. ORB-SLAM: a versatile and accurate monocular SLAM system. _TRO_, 31(5), 2015.
* [50] T. Neff, P. Stadlbauer, M. Parger, A. Kurz, C. R. A. Chaitanya, A. Kaplanyan, and M. Steinberger. DONeRF: Towards Real-Time Rendering of Neural Radiance Fields using Depth Oracle Networks. _EGSR_, 2021.
* [51] M. Niemeyer, J. T. Barron, B. Mildenhall, M. S. M. Sajjadi, A. Geiger, and N. Radwan. RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs. _CVPR_, 2022.
* [52] J. Ost, I. Laradji, A. Newell, Y. Bahat, and F. Heide. Neural Point Light Fields. _CVPR_, 2022.
* [53] O. Ozyesil, V. Voroninski, R. Basri, and A. Singer. A Survey of Structure from Motion. _Acta Numerica_, 2017.
* [54] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation. _CVPR_, 2019.
* [55] S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and A. Geiger. Convolutional Occupancy Networks. _ECCV_, 2020.
* [56] D. Rebain, W. Jiang, S. Yazdani, K. Li, K. M. Yi, and A. Tagliasacchi. DeRF: Decomposed Radiance Fields. _CVPR_, 2021.
* [57] C. Reiser, S. Peng, Y. Liao, and A. Geiger. KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs. _ICCV_, 2021.
* [58] E. Remelli, A. Lukoianov, S. R. Richter, B. Guillard, T. Bagautdinov, P. Baque, and P. Fua. MeshSDF: Differentiable Iso-Surface Extraction. _NeurIPS_, 2020.
* [59] M. Roberts and N. Paczan. Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding. _ICCV_, 2021.
* [60] S. Saito, Z. Huang, R. Natsume, S. Morishima, A. Kanazawa, and H. Li. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. _ICCV_, 2019.
* [61] M. S. M. Sajjadi, H. Meyer, E. Pot, U. Bergmann, K. Greff, N. Radwan, S. Vora, M. Lucic, D. Duckworth, A. Dosovitskiy, J. Uszkoreit, T. Funkhouser, and A. Tagliasacchi. Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. _CVPR_, 2022.
* [62] J. L. Schonberger and J.-M. Frahm. Structure-from-Motion Revisited. _CVPR_, 2016.
* [63] V. Sitzmann, J. N. P. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein. Implicit Neural Representations with Periodic Activation Functions. _NeurIPS_, 2020.
* [64] V. Sitzmann, S. Rezchikov, W. T. Freeman, J. B. Tenenbaum, and F. Durand. Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. _NeurIPS_, 2021.
* [65] V. Sitzmann, M. Zollhofer, and G. Wetzstein. Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations. _NeurIPS_, 2019.
* [66] L. N. Smith and N. Topin. Super-convergence: Very fast training of neural networks using large learning rates. _Artificial intelligence and machine learning for multi-domain operations applications_, 2019.
* [67] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic Scene Completion from a Single Depth Image. _CVPR_, 2017.
* [68] M. Suhail, C. Esteves, L. Sigal, and A. Makadia. Light Field Neural Rendering. _CVPR_, 2022.
* [69] T. Takikawa, J. Litalien, K. Yin, K. Kreis, C. Loop, D. Nowrouzezahrai, A. Jacobson, M. McGuire, and S. Fidler. Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes. _CVPR_, 2021.
* [70] J. Tang, X. Han, J. Pan, K. Jia, and X. Tong. A Skeleton-bridged Deep Learning Approach for Generating Meshes of Complex Topologies from Single RGB Images. _CVPR_, 2019.
* [71] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs. _ICCV_, 2017.
* [72] A. Trevithick and B. Yang. GRF: Learning a General Radiance Field for 3D Representation and Rendering. _ICCV_, 2021.

* [73] B. Wang, L. Chen, and B. Yang. DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images. _ICLR_, 2023.
* [74] B. Wang, Z. Yu, B. Yang, J. Qin, T. Breckon, L. Shao, N. Trigoni, and A. Markham. RangeUDF: Semantic Surface Reconstruction from 3D Point Clouds. _arXiv:2204.09138_, 2022.
* [75] H. Wang, J. Ren, Z. Huang, K. Olszewski, M. Chai, Y. Fu, and S. Tulyakov. R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. _ECCV_, 2022.
* [76] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang. Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. _ECCV_, 2018.
* [77] P. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. _NeurIPS_, 2021.
* [78] P. Wang, Y. Liu, G. Lin, J. Gu, L. Liu, T. Komura, and W. Wang. Progressively-connected Light Field Network for Efficient View Synthesis. _arXiv:2207.04465_, 2022.
* [79] Q. Xu, W. Wang, D. Ceylan, R. Mech, and U. Neumann. DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction. _NeurIPS_, 2019.
* [80] B. Yang, S. Rosa, A. Markham, N. Trigoni, and H. Wen. Dense 3D Object Reconstruction from a Single Depth View. _TPAMI_, 2019.
* [81] B. Yang, S. Wang, A. Markham, and N. Trigoni. Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D Reconstruction. _IJCV_, 128:53-73, 2020.
* [82] A. Yu, S. Fridovich-keil, and A. Kanazawa. Plenoxels: Radiance Fields without Neural Networks. _CVPR_, 2022.
* [83] S. Zakharov, W. Kehl, A. Bhargava, and A. Gaidon. Autolabeling 3D Objects with Differentiable Rendering of SDF Shape Priors. _CVPR_, 2020.
* [84] A. Zeng, S. Song, M. Niessner, M. Fisher, J. Xiao, and T. Funkhouser. 3DMatch: Learning Local Geometric Descriptors From RGB-D Reconstructions. _CVPR_, 2017.
* [85] C. Zhang, Z. Cui, Y. Zhang, B. Zeng, M. Pollefeys, and S. Liu. Holistic 3D Scene Understanding from a Single Image with Implicit Representation. _CVPR_, 2021.
* [86] S. Zhi, T. Laidlow, S. Leutenegger, and A. J. Davison. In-Place Scene Labelling and Understanding with Implicit Scene Representation. _ICCV_, 2021.
* [87] C. Zou, E. Yumer, J. Yang, D. Ceylan, and D. Hoiem. 3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks. _ICCV_, 2017.

Appendix

### Network Architecture and Training Details

We provide the details of our ray-surface distance field and the auxiliary dual-ray visibility classifier.

#### a.1.1 Ray-surface Distance Field

We use a 13-layer SIREN [63] with 1024 hidden units to learn a ray-surface distance field. We set the batch size as 8192 and train the network for 10 epochs with the Adam [35] optimizer and a cosine annealing strategy [44] with the learning rate initialized as \(10^{-5}\) and decayed to \(10^{-8}\).

**Radiance Branch:** To learn a radiance field, we take an additional 2-layer SIREN with 1024 hidden units as the radiance branch and output RGB values with sigmoid activation. For optimization together with the distance field, we adopt the same multi-view consistency as the distance field with a mean-squared loss and set the loss weight to 1.

#### a.1.2 Dual-ray Visibility Classifier

We take a SIREN layer as the ray-feature encoder \(g()\) and another SIREN layer as the point-feature encoder \(k()\), followed by a 7-layer SIREN with 512 hidden units and output the visibility with sigmoid activation. Both the ray-feature encoder and the point-feature encoder are with 512 hidden units. The batch sizes are 2048 for Blender dataset and ScanNet dataset, and 1024 for DM-SR dataset. We train the network for 5 epochs with the Adam optimizer and a cycle annealing strategy [66] with the maximum learning rate \(10^{-4}\).

#### a.1.3 Training Data Generation

* For a scan with extrinsic parameters \([\text{R}|\text{t}]\) and intrinsic parameters K, all rays of this scan start from the camera position \(\mathbf{o}=(t_{x},t_{y},t_{z})\). The ray orientation of pixel \((u,v)\) can be denoted as \(\mathbf{m}_{0}=\text{RK}^{-1}\binom{u}{1}\), and the unit ray direction is \(\mathbf{m}=\mathbf{m}_{0}/||\mathbf{m}_{0}||\). Given an oriented ray \(\mathbf{r}\) starting from \(\mathbf{o}\) and a pre-defined sphere with a diameter \(D\), we follow the two-sphere parameterization [8] to compute two intersections \(\mathbf{p}^{in},\mathbf{p}^{out}\) and construct our input parameters \(\mathbf{r}=(\mathbf{\theta}^{in},\mathbf{\phi}^{in},\mathbf{\theta}^{out},\mathbf{\phi}^{out})\).
* To obtain the ray-surface distance \(d\) from the raw depth value \(\tilde{d}\) of pixel \((u,v)\), we have \(d=\tilde{d}\sqrt{(u-c_{y})^{2}+(v-c_{x})^{2}+f^{2}}/f-d_{0}\), where \(f,c_{y},c_{x}\) are the camera intrinsic parameters (_i.e._, focal length, the center coordinates at y-axis and x-axis of image plane), and \(d_{0}=||\mathbf{p}^{in}-\mathbf{\phi}||\).
* For a ray \(\mathbf{r}\) at the pixel (u, v) of a specific scan, we have the first ray-sphere intersection \(\mathbf{p}^{in}\) and ray direction \(\mathbf{m}\), then we can compute the ray-surface point \(\mathbf{p}=\mathbf{p}^{in}+d\mathbf{m}\). To reproject this point to the remaining \((K-1)\) scans, for example, we calculate the pixel coordinate \((u^{k},v^{k})\) of the \(k^{th}\) scan using its extrinsic parameters \([\text{R}|\text{t}]^{k}\) and intrinsic parameters \(\text{K}^{k}\): \[(u^{k},v^{k},1)=\text{K}^{k}([\text{R}|\text{t}]^{k})^{-1}\mathbf{p}/z_{\mathbf{p}}\] (5) Then we can query the raw depth value at \((u^{k},v^{k})\) from the \(k^{th}\) scan, and obtain its ray-surface distance \(d^{k}_{u^{k},v^{k}}\) and the ray parameters \(\mathbf{r}^{k}_{u^{k},v^{k}}\) (simplified as \(d^{k},\mathbf{r}^{k}\)) following the previous steps. Besides, we calculate the multi-view distance \(\tilde{d}^{k}\) between the ray-surface point \(\mathbf{p}\) and the first intersection \(\mathbf{p}^{in}_{k}\) of ray \(\mathbf{r}^{k}\): \(\tilde{d}^{k}=||\mathbf{p}-\mathbf{p}^{in}_{k}||\). We set a _closeness_ threshold \(\epsilon=10\) millimeters to determine whether the projected \(k^{th}\) ray is visible: \[v^{k}=\begin{cases}1,&\text{if }|\tilde{d}^{k}-d^{k}|\leq\epsilon\\ 0,&\text{otherwise}\end{cases}\] (6)
* For both training and inference, the values of input parameters are normalized to be [-1, 1]. In particular, the latitude \(\theta:=2\theta/\pi-1\) and the longitude \(\phi:=\phi/\pi\). The input surface hitting point \((x_{1},y_{1},z_{1})\) of our visibility classifier is also normalized to [-1, 1] by subtracting the sphere center and then dividing by the sphere radius \(D/2\). The ground-truth ray-surface distance \(d\) and the multi-view distance \(\tilde{d}^{k}\) are scaled to be [0, 1] by dividing by the sphere diameter \(D\).

### Derivation of Surface Normal

In this section, we derive the formula of surface normal from our ray-surface distance field.

As mentioned in Section A.1.3, given a ray starting from \(\mathbf{o}\) with direction \(\mathbf{m}\) and a fix-sized sphere with a diameter \(D\) centered at the coordinate origin, we can construct the input parameters \(\mathbf{r}=(\theta^{\mathbf{m}},\phi^{\mathbf{m}},\theta^{\mathbf{out}},\phi^{\mathbf{out}})\) from a _ray-sphere intersect function \(F(\mathbf{o},\mathbf{m},D)=\mathbf{r}\)_.

By converting \(\mathbf{m}\) to the spherical coordinate system, we have \(\frac{\partial\mathbf{r}}{\partial\mathbf{\theta_{m}}}\) and \(\frac{\partial\mathbf{r}}{\partial\mathbf{\theta_{m}}}\), which is the gradient of the above intersection function \(F()\)_w.r.t._ the ray direction \(\mathbf{m}\).

As shown in Figure 6, to compute the surface normal of a predicted surface point \(\mathbf{\hat{p}}\) along the ray direction \(\mathbf{m}\), we define a _sphere_ of radius \(R=D\hat{d}+d_{0}\) centered at the camera position \(\mathbf{o}\) using spherical coordinates:

\[\Phi(\theta_{\mathbf{m}},\phi_{\mathbf{m}})=(R\sin\theta_{\mathbf{m}}\cos\phi_{\mathbf{m}},R \sin\theta_{\mathbf{m}}\sin\phi_{\mathbf{m}},R\cos\theta_{\mathbf{m}}) \tag{7}\]

In general, the formula for a unit normal vector is \(\mathbf{n}=\frac{\frac{\partial\Phi}{\partial\mathbf{\theta_{m}}}\times\frac{\partial \Phi}{\partial\mathbf{\theta_{m}}}}{\|\frac{\partial\Phi}{\partial\mathbf{\theta_{m}}} \times\frac{\partial\Phi}{\partial\mathbf{\theta_{m}}}\|}\). From Eq. 7, we have

\[\frac{\partial\Phi}{\partial\phi_{\mathbf{m}}}=D\begin{bmatrix}(\frac{\partial\hat{ d}}{\partial\mathbf{\theta_{m}}}\cos\phi_{\mathbf{m}}-(\hat{d}+\frac{d_{0}}{D})\sin\phi_{ \mathbf{m}})\sin\theta_{\mathbf{m}}\\ (\frac{\partial\hat{d}}{\partial\mathbf{\theta_{m}}}\sin\phi_{\mathbf{m}}+(\hat{d}+ \frac{d_{0}}{D})\cos\phi_{\mathbf{m}})\sin\theta_{\mathbf{m}}\end{bmatrix} \tag{8}\]

\[\frac{\partial\Phi}{\partial\theta_{\mathbf{m}}}=D\begin{bmatrix}(\frac{\partial \hat{d}}{\partial\mathbf{\theta_{m}}}\sin\theta_{\mathbf{m}}+(\hat{d}+\frac{d_{0}}{D}) \cos\theta_{\mathbf{m}})\cos\phi_{\mathbf{m}}\\ (\frac{\partial\hat{d}}{\partial\mathbf{\theta_{m}}}\sin\theta_{\mathbf{m}}+(\hat{d}+ \frac{d_{0}}{D})\cos\theta_{\mathbf{m}})\sin\phi_{\mathbf{m}}\\ \frac{\partial\hat{d}}{\partial\mathbf{\theta_{m}}}\cos\theta_{\mathbf{m}}-(\hat{d}+ \frac{d_{0}}{D})\sin\theta_{\mathbf{m}}\end{bmatrix} \tag{9}\]

Here, we can replace \(\frac{\partial\hat{d}}{\partial\mathbf{\theta_{m}}}=\frac{\partial\hat{d}}{\partial \mathbf{r}}\frac{\partial\mathbf{r}}{\partial\mathbf{\theta_{m}}}\) and \(\frac{\partial\hat{d}}{\partial\mathbf{\theta_{m}}}=\frac{\partial\hat{d}}{\partial \mathbf{r}}\frac{\partial\mathbf{r}}{\partial\mathbf{\theta_{m}}}\), so the surface normal \(\mathbf{n}\) can be denoted as a function \(Q(\frac{\partial\hat{d}}{\partial\mathbf{r}},\mathbf{r},D,\hat{d})\).

### Additional Implementation Details

In this section, we provide details of implementation and some minor adaptations of baselines, as well as the workflow for 3D shape reconstruction.

#### a.3.1 Baselines

**OF/DeepSDF/NDF:** Pre-processing on mesh before training is required for OF/DeepSDF/NDF. For fair comparisons, we use all depth images from training views to reconstruct the training mesh, which is then used for sampling occupancies/signed distances/unsigned distances of a \(256^{3}\) voxel grid for training. We follow the official settings of hyperparameters and network architecture to train on the three datasets. For inference, we apply sphere tracing for DeepSDF and NDF to render depth images from testing views. The sphere tracing will stop after 100 iterations or when the predicted distance is smaller than a threshold at 0.005 meters. To avoid missing the intersections, we follow the damped sphere tracing [14] from NDF, _i.e._, marching with \(\alpha\cdot f(\mathbf{p})\) with \(\alpha=0.6\). Since sphere tracing for OF can only take a fixed step size (_e.g._, 0.005 meters), it requires about 1000 iterations to march the surface, which is time-consuming. We thus construct a testing mesh by querying a value grid of occupancies and use ray-mesh intersections to render depth images for evaluation.

**NeuS/DS-NeRF:** In general, we follow the same network architecture, training schedule, and color supervision from NeuS and DS-NeRF. The vanilla DS-NeRF adopts sparse 3D points from structure-from-motion (SFM) and uses the reprojection error for depth supervision. For a fair comparison, we provide the same dense depth supervision from training views as ours on all three datasets for both

Figure 6: Sphere for surface normal computation.

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_FAIL:20]

#### a.4.5 Ablation Studies on Post-processing

We follow the outlier removal step in Section 3.4 as post-processing to obtain clean 3D point clouds for further mesh reconstruction and evaluation. In all experiments, we set the outlier threshold as 5 for post-processing. We conduct additional experiments w/ and w/o the outlier removal step on the Blender dataset. In Table 26, the experiment without post-processing yields higher CD errors compared to the one with post-processing. The qualitative results are shown in Figure 13.

### Limitations of RayDF

Since our RayDF takes the conventional spherical parameterization for the input ray, _i.e._, a sphere is predefined to bound the target 3D scene, our RayDF with such a parameterization cannot represent the 3D scene with rays starting from a position inside the sphere. However, the proposed distance field with a visibility classifier of our RayDF is flexible to different ray parameterizations. How to integrate distinct parameterizations of input rays for different types of 3D scenes with our ray-surface distance field and the dual-ray visibility classifier can be a future exploration direction.

### Additional Quantitative Results

In this section, we report the detailed quantitative results of each scene on the three datasets.

**Blender:** In Tables 28 and 29, we show the quantitative results of the experiments in Group 1. Our RayDF achieves significantly better performance than the other baselines on most scenes, especially on complex scenes (_e.g._, Drums, Ficus, and Ship). In Tables 30, 31, 32, 33 and 34, we report the geometry and appearance metrics in Group 2. Our method achieves comparable performance with DS-NeRF for novel view synthesis while recovering the 3D shape with fewer outliers.

**DM-SR:** In Tables 35 and 36, we report the quantitative comparisons in Group 1 and our RayDF performs better than the other baselines on those indoor scenes with more complex objects. As shown in Tables 37, 38, 39, 40 and 41, our method still achieves high-quality results for novel view synthesis on all scenes in Group 2 experiments.

**ScanNet:** In Tables 42 and 43, our RayDF performs significantly better than the other baselines on the real-world scenes. To represent these real-world 3D scenes with much more noise and complex geometry, our method outperforms all baselines while obtaining photo-realistic novel view synthesis results, as shown in Tables 44, 45, 46, 47 and 48.

Moreover, we provide comparisons of optimization time with baselines in Table 27. We compare the average training time of our method and all baselines on each scene of Blender dataset, using a single NVIDIA RTX 3090 GPU card and a CPU of AMD Ryzen 7. Since we apply a two-stage training strategy, our method is not as fast as baselines during optimization. Nevertheless, once our ray-surface distance network is optimized, it can achieve superior efficiency in rendering novel views, as shown in Table 1.

[MISSING_PAGE_FAIL:22]

Figure 17: Qualitative results of all methods on _Drums_ in Blender dataset.

Figure 18: Qualitative results of all methods on _Reception_ in DM-SR dataset.

Figure 19: Qualitative results of all methods on _Study_ in DM-SR dataset.

Figure 20: Qualitative results of all methods on _Scene0005_00_ in ScanNet dataset.

Figure 21: Qualitative results of all methods on _Scene0030_00_ in ScanNet dataset.

\begin{table}
\begin{tabular}{r|c c c c c c c c c c} \hline  & Chair & Drums & Ficus & Hotdog & Lego & Materials & Mic & Ship & Avg. \\ \hline NeuS [77] & 0.108 & 0.096 & 0.029 & 0.068 & 0.114 & 0.080 & **0.037** & 0.250 & 0.100 \\ DS-NeRF [19] & **0.055** & **0.058** & **0.016** & **0.044** & **0.036** & **0.034** & **0.037** & **0.223** & **0.063** \\ LRN [64] & 0.140 & 0.117 & 0.032 & 0.074 & 0.187 & 0.117 & 0.063 & 0.267 & 0.125 \\ PRIF [23] & 0.157 & 0.181 & 0.075 & 0.090 & 0.207 & 0.149 & 0.062 & 0.292 & 0.152 \\
**RayDF (Ours)** & 0.100 & 0.093 & 0.030 & 0.063 & 0.125 & 0.103 & 0.039 & 0.239 & 0.099 \\ \hline \end{tabular}
\end{table}
Table 34: LPIPS\(\downarrow\) scores of all baselines and our method in experiments of Group 2 on the 8 scenes of Blender dataset.

[MISSING_PAGE_FAIL:29]

\begin{table}
\begin{tabular}{r|c c c c c c c c} \hline  & Scene0004\_00 & Scene0005\_00 & Scene0009\_00 & Scene0010\_00 & Scene0030\_00 & Scene0031\_00 & Avg. \\ \hline NeuS [77] & 33.43 & 24.05 & 24.18 & 25.84 & 26.57 & 10.67 & 24.12 \\ DS-NeRF [19] & 12.18 & 7.99 & 7.01 & 8.00 & 6.87 & 5.00 & 7.84 \\ LN [64] & 10.75 & 5.49 & 6.17 & 7.34 & 5.87 & 5.00 & 6.77 \\ PRIF [23] & 13.95 & 9.98 & 9.17 & 9.20 & 14.63 & 6.98 & 10.65 \\
**RayDF (Ours)** & **8.49** & **4.16** & **5.22** & **5.14** & **4.48** & **4.39** & **5.31** \\ \hline \end{tabular}
\end{table}
Table 44: ADE\(\downarrow\) scores of all baselines and our method in experiments of Group 2 on 6 scenes of ScanNet dataset.

\begin{table}
\begin{tabular}{r|c c c c c c|c} \hline  & Scene0004\_00 & Scene0005\_00 & Scene0009\_00 & Scene0010\_00 & Scene0030\_00 & Scene0031\_00 & Avg. \\ \hline OF [46] & 26.29 & 13.61 & 13.15 & 11.76 & 23.26 & 6.97 & 15.84 \\ DeepSDF [54] & 19.47 & 11.86 & 10.64 & 9.92 & 14.60 & 5.94 & 12.07 \\ NDP [14] & 31.13 & 20.47 & 21.89 & 17.23 & 29.38 & 12.79 & 22.15 \\ NeuS [77] & 25.75 & 15.76 & 17.79 & 15.58 & 17.50 & 10.82 & 17.20 \\ DS-NeRF [19] & 10.85 & 6.01 & 6.42 & 7.11 & 5.67 & **3.75** & 6.64 \\ IFN [64] & 10.50 & 5.22 & 6.07 & 6.90 & 5.58 & 4.90 & 6.53 \\ PRIF [23] & 13.98 & 9.36 & 9.66 & 9.17 & 12.54 & 7.21 & 10.32 \\
**RayDF (Ours)** & **8.66** & **4.26** & **5.33** & **5.29** & **4.50** & 4.46 & **5.42** \\ \hline \end{tabular}
\end{table}
Table 42: ADE\(\downarrow\) scores of all baselines and our method in experiments of Group 1 on 6 scenes of ScanNet dataset.

\begin{table}
\begin{tabular}{r|c c c c c c|c} \hline  & Scene0004\_00 & Scene0005\_00 & Scene0009\_00 & Scene0010\_00 & Scene0030\_00 & Scene0031\_00 & Avg. \\ \hline OF [46] & 26.29 & 13.61 & 13.15 & 11.76 & 23.26 & 6.97 & 15.84 \\ DeepSDF [54] & 19.47 & 11.86 & 10.64 & 9.92 & 14.60 & 5.94 & 12.07 \\ NDP [14] & 31.13 & 20.47 & 21.89 & 17.23 & 29.38 & 12.79 & 22.15 \\ NeuS [77] & 25.75 & 15.76 & 17.79 & 15.58 & 17.50 & 10.82 & 17.20 \\ DS-NeRF [19] & 10.85 & 6.01 & 6.42 & 7.11 & 5.67 & **3.75** & 6.64 \\ IFN [64] & 10.50 & 5.22 & 6.07 & 6.90 & 5.58 & 4.90 & 6.53 \\ PRIF [23] & 13.98 & 9.36 & 9.66 & 9.17 & 12.54 & 7.21 & 10.32 \\
**RayDF (Ours)** & **8.66** & **4.26** & **5.33** & **5.29** & **4.50** & 4.46 & **5.42** \\ \hline \end{tabular}
\end{table}
Table 43: CD (\(\times 10^{-3}\))\(\downarrow\) (mean / median) scores of all baselines and our method in experiments of Group 1 on 6 scenes of ScanNet dataset.

\begin{table}
\begin{tabular}{r|c c c c c c|c} \hline  & Scene0004\_00 & Scene0005\_00 & Scene0009\_00 & Scene0010\_00 & Scene0030\_00 & Scene0031\_00 & Avg. \\ \hline NeuS [77] & 33.825 / 11.200 & 19.468 / 8.325 & 46.224 / 4.593 & 27.426 / 2.857 & 44.060 / 3.475 & 104.02 / 3.500 & 67.504 / 5.658 \\ DS-NeRF [19] & 39.896 / 7.466 & 6.186 / 1.487 & 7.052 / 2.171 & 2.170 / 0.992 & 11.084 / 1.484 & 5.434 / 2.638 & 14.642 / 2.950 \\ LRN [64] & **18.979** / **4.586** & **3.838** / 1.104 & **3.901** / **1.697** & 2.882 / 0.700 & 15.685 /