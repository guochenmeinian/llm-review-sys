# Improved Convergence in High Probability of

Clipped Gradient Methods with Heavy Tailed Noise

 Ta Duy Nguyen

Department of Computer Science

Boston University

taduy@bu.edu

&Thien Hang Nguyen

Khoury College of Computer Sciences

Northeastern University

nguyen.thien@northeastern.edu

&Alina Ene

Department of Computer Science

Boston University

aene@bu.edu

&Huy Le Nguyen

Khoury College of Computer Sciences

Northeastern University

hu.nguyen@northeastern.edu

###### Abstract

In this work, we study the convergence _in high probability_ of clipped gradient methods when the noise distribution has heavy tails, i.e., with bounded \(p\)th moments, for some \(1<p\leq 2\). Prior works in this setting follow the same recipe of using concentration inequalities and an inductive argument with union bound to bound the iterates across all iterations. This method results in an increase in the failure probability by a factor of \(T\), where \(T\) is the number of iterations. We instead propose a new analysis approach based on bounding the moment generating function of a well chosen supermartingale sequence. We improve the dependency on \(T\) in the convergence guarantee for a wide range of algorithms with clipped gradients, including stochastic (accelerated) mirror descent for convex objectives and stochastic gradient descent for nonconvex objectives. Our high probability bounds achieve the optimal convergence rates and match the best currently known in-expectation bounds. Our approach naturally allows the algorithms to use time-varying step sizes and clipping parameters when the time horizon is unknown, which appears difficult or even impossible using existing techniques from prior works. Furthermore, we show that in the case of clipped stochastic mirror descent, several problem constants, including the initial distance to the optimum, are not required when setting step sizes and clipping parameters.

## 1 Introduction

Stochastic optimization is a well-studied area with many applications ranging from machine learning, to operation research, numerical linear algebra and beyond. In contrast to deterministic algorithms, stochastic algorithms might fail, and a pertinent question is how often does failure happen and how to increase the success rate. These questions are especially important in critical applications where failure is not tolerable, or when a single run is costly in time and resources. Fortunately, the standard stochastic gradient descent (SGD) algorithm has been shown to converge with _high probability_ under a _light-tailed noise_ distribution such as sub-Gaussian distributions [23, 12, 27, 14, 11, 10, 18], which gives strong guarantee on the success of single runs. However, recent observations in popular deep learning applications, such as training attention models [33] and convolutional networks [30], reveal a more challenging optimization landscape: the gradient noises follow _heavy-tailed_ distributions, where the variance may be infinite [29, 33, 9], whereasthe standard light-tailed setting assumes that all the moments are bounded. Heavy-tailed gradient noises can cause algorithms like SGD to fail, and this mismatch between theory and practice has been suggested to be one of the reasons for the strong preference of adaptive methods like Adam over SGD in modern settings [33].

In this work, we consider the setting of _heavy-tailed noise_ proposed by Zhang et al., (2020) [33], where the (unbiased) gradient noise only has bounded \(p\)th moments, for some \(p\in(1,2]\). While standard SGD can fail to converge when the variance is unbounded, i.e. when \(p<2\), [33] show that SGD with appropriate clipping (or _Clipped-SGD_) converges _in expectation_ under heavy-tailed noise, where the convergence rate depends on \(O\left(\frac{1}{\delta}\right)\) if \(\delta\) is the targeted maximum failure probability. It is more desirable, however, to obtain convergence results in _high probability_, where the convergence rate depends instead on \(O(\log\frac{1}{\delta})\), which gives better guarantees for single runs.

Recent follow-up works [2, 28, 19] show that variants of Clipped-SGD in fact converge with high probability. This is a pleasing result, extending the earlier work by [7] for \(p=2\). However, there are several shortcomings of these results when compared with the corresponding bounds in the light-tailed setting. First, the clipped algorithm uses a fixed step size and a fixed clipping parameter depending on the number of iterations, which precludes results with _unknown_ time horizons. Secondly, the convergence guarantees are worse than the light-tailed bounds by a \(\log T\) factor, even for fixed step sizes and clipping parameters. These issues beg a qualitative question:

_Is heavy-tailed noise inherently harder than light-tailed noise?_

In this work, we answer the above question for Clipped-SGD and the general clipped (accelerated) stochastic mirror descent (_Clipped-SMD_) algorithm. We give an improved analysis framework that not only gives tighter bounds matching the light-tailed noise setting, but also allows for step sizes and clipping parameters for unknown time horizons. Furthermore, we show that this framework is applicable to various settings, from finding minimizers of convex functions with arbitrarily large domains using (accelerated) mirror descent, to finding stationary points for non-convex functions using gradient descent.

### Contributions and Techniques

Our work addresses several open questions posed by previous works including handling general domains and dealing with an unknown time horizon under heavy-tailed noise. Qualitatively, we close the logarithmic suboptimality gap and achieve the optimal rate in several settings. More specifically:

\(-\) We demonstrate a novel approach to analyze clipped gradient methods in high probability that is general and applies to various standard settings. In the convex setting, we analyze Clipped-SMD and clipped stochastic accelerated mirror descent. In the non-convex setting, we analyze Clipped-SGD. Using our new analysis, we show that clipped methods attain time-optimal convergence in high probability for both convex and nonconvex objectives under heavy-tailed gradient noise. In the convex setting, we obtain an \(O\left(T^{\frac{1-p}{p}}\right)\) convergence rate for arbitrary (not necessarily compact) convex domains for Clipped-SMD and \(O\left(T^{\frac{1-p}{p}}\sigma+T^{-2}\right)\) for accelerated Clipped-SMD, where \(\sigma\) is the noise parameter. These rates are time-optimal and match the lower bounds in [26, 31]. In the nonconvex setting, we obtain the optimal convergence rate of \(O\left(T^{\frac{2-2p}{3p-2}}\right)\) for clipped-SGD. This bound is also time-optimal and matches the lower bound in [33]; it also complements the in-expectation convergence of clipped-SGD provided by [33].

\(-\) Previous works for heavy-tailed noises follow the recipe of using Freedman-type inequalities [4, 3] as a _blackbox_ and bound the iterates inductively for all iterations. This process incurs an additional \(\log T\) dependency in the final convergence rate; in other words, the success probability goes from \(1-\delta\) to \(1-T\delta\). The step sizes and clipping parameters of this approach depend on the time horizon \(T\) to enable the union bound and induction across all iterations in the analysis, excluding the important case when the time horizon is unknown. Our whitebox approach forgoes the aforementioned induction, not only circumventing the \(\log T\) loss but also allowing for an unknown time horizon. We further show that our analysis allows for a choice of step size and clipping parameters that do not depend on generally unknown parameters like the noise-parameter \(\sigma\), the failure probability \(\delta\), and the initial distance to the optimum, all of which appear impossible using only the techniques from prior works.

\(-\) Our whitebox approach analyzes the moment generating function of a well chosen martingale difference sequence to obtain tight rates for stochastic gradient methods. This approach is closest to the work of [18], which only work in the light-tailed noise setting. In contrast to the light-tailed noise setting where all the moments are well controlled, the heavy-tailed setting often requires algorithms to incorporate gradient clipping for controlling the possibly infinite moments. However, this makes the gradient estimate biased and requires more careful attention to control the bias propagating through the algorithm. Naively applying the technique in [18] is not enough to handle heavy-tailed noise. Rather, as will be shown in our analysis, we introduce a novel history-dependent weights for the martingale sequence that is able to cope with the propagating bias term of clipped methods for heavy-tailed noise across various settings.

### Related Works

High probability convergence for light-tailed noises.Convergence in high probability of stochastic gradient algorithms has been established for sub-Gaussian noises in a number of prior works, including [23, 12, 27, 14, 11, 10] for convex problems with bounded domain (or bounded Bregman diameter) or with strong convexity. Other works [17, 20, 16] study convergence of variants of SGD for nonconvex objectives, where they consider sub-Gaussian and sub-Weibull noises. The most relevant to ours in this line of work is the one by [18], where a whitebox approach is employed to obtain tight rates for stochastic gradient methods in the light-tailed noise setting. However, their technique is not directly applicable in the heavy-tailed noise setting, where we need to introduce new ideas to handle the biases introduced by gradient clipping.

High probability convergence for noises with bounded variance and heavy tails.The design of new gradient algorithms and their analysis in the presence of heavy-tailed noises has drawn significant recent interest. Starting from the work [25] which propose Clipped-SGD to handle exploding gradients in recurrent neural networks, the recent works [30, 29, 33, 9] give new motivation for clipped methods in the context of convolutional networks and attention deep networks that attempts to explain the dominance of adaptive methods over SGD in practical modern scenarios.

While the convergence in expectation of vanilla SGD has been extensively studied [5, 23, 13, 18], only recently has the convergence of Clipped-SGD with heavy tailed noises been closely examined. There, [33] first show the convergence in expectation of Clipped-SGD for nonconvex functions and provide a matching lower bound. In the convex regime, several works with different clipping strategies for the case of \(p=2\) have shown high probability convergence for smooth problems with bounded domain [22, 24], smooth unconstrained problems [7], and non-smooth problems [8]. A variant of Clipped-SGD that utilizes momentum [2] has also been shown to converge with high probability for bounded \(p\)th moments gradient noise. However, the analysis in [2] requires a strong assumption which implies that the true gradients are bounded, a restrictive assumption that excludes objectives like quadratic functions.

More recently, [28, 19, 34] give nearly-optimal convergence rates for several Clipped-SGD variants. These works follow the recipe of using Freedman-type inequalities [4, 3] as a blackbox and bound the iterates inductively for all iterations, which incur an additional \(\log T\) dependency in the final convergence rate. We show in our work that existing convergence rates can be tightened up and improved. Tight lower bounds for the optimal convergence rate have been shown by [26, 31] for convex objectives and by [33] for nonconvex settings. In both cases, our paper provides optimal convergence guarantees.

In a related but different line of work, [32] show that vanilla SGD can converge with heavy tailed noise for a special type of strongly convex functions, and [31] show that stochastic mirror descent converges in expectation for a special choice of mirror maps, although only for strongly convex objectives with bounded domains.

## 2 Preliminaries: Assumptions and Notations

We study the problem \(\min_{x\in\mathcal{X}}f(x)\) where \(f:\mathbb{R}^{d}\to\mathbb{R}\) and \(\mathcal{X}\) is the domain of the problem. In the convex setting, we assume that \(\mathcal{X}\) is a convex set but not necessarily compact. We let \(\|\cdot\|\) be an arbitrary norm and \(\left\|\cdot\right\|_{*}\) be its dual norm. In the nonconvex setting, we take \(\mathcal{X}\) to be \(\mathbb{R}^{d}\) and consider only the \(\ell_{2}\) norm.

### Assumptions

Our paper works with the following assumptions:

**(1) Existence of a minimizer**: In the convex setting, we assume that there exists \(x^{*}\in\arg\min_{x\in\mathcal{X}}f(x)\). We let \(f^{*}=f(x^{*})\).

**(1') Existence of a finite lower bound**: In the nonconvex setting, we assume that \(f\) admits a finite lower bound, i.e., \(f^{*}:=\inf_{x\in\mathbb{R}^{d}}f(x)>-\infty\).

**(2) Unbiased estimator**: We assume that our algorithm is allowed to query a stochastic first-order oracle that returns a history-independent, unbiased gradient estimator \(\widehat{\nabla}f(x)\) of \(\nabla f(x)\) for any \(x\in\mathcal{X}\). That is, conditioned on the history and the queried point \(x\), we have \(\mathbb{E}[\widehat{\nabla}f(x)\mid x]=\nabla f(x)\).

**(3) Bounded \(p\)th moment noise**: We assume that there exists \(\sigma>0\) such that for some \(1<p\leq 2\) and for any \(x\in\mathcal{X}\), \(\widehat{\nabla}f(x)\) satisfies \(\mathbb{E}[\|\widehat{\nabla}f(x)-\nabla f(x)\|_{*}^{p}\mid x]\leq\sigma^{p}\).

**(4) \(L\)-smoothness**: We consider the class of \(L\)-smooth functions: for all \(x,y\in\mathbb{R}^{d}\), \(\left\|\nabla f(x)-\nabla f(y)\right\|_{*}\leq L\left\|x-y\right\|.\)

### Gradient Clipping Operator and Notations

We introduce the gradient clipping operator and its general properties used in Clipped-SMD (Algorithm 2) and Clipped-SGD (Algorithm 1). Let \(x_{t}\) be the output at iteration \(t\) of an algorithm of interest. We denote by \(\widehat{\nabla}f(x_{t})\) the stochastic gradient obtained by querying the gradient oracle. The clipped gradient estimate \(\widehat{\nabla}f(x_{t})\) is taken as

\[\widehat{\nabla}f(x_{t})=\min\left\{1,\frac{\lambda_{t}}{\left\| \widehat{\nabla}f(x_{t})\right\|_{*}}\right\}\widehat{\nabla}f(x_{t}), \tag{1}\]

where \(\lambda_{t}\) is the clipping parameter used in iteration \(t\). In subsequent sections, we let \(\Delta_{t}:=f(x_{t})-f^{*}\) denote the optimal function value gap at \(x_{t}\). We let \(\mathcal{F}_{t}=\sigma\left(\widehat{\nabla}f(x_{1}),\ldots,\widehat{\nabla}f (x_{t})\right)\) be the natural filtration at time \(t\) and define the following notations for the stochastic error, the deviation, and the bias of the clipped gradient estimate at time \(t\):

\[\theta_{t}=\widehat{\nabla}f(x_{t})-\nabla f(x_{t});\quad\theta_{t}^{u}= \widehat{\nabla}f(x_{t})-\mathbb{E}\left[\widehat{\nabla}f(x_{t})\mid\mathcal{ F}_{t-1}\right];\quad\theta_{t}^{b}=\mathbb{E}\left[\widehat{\nabla}f(x_{t})\mid \mathcal{F}_{t-1}\right]-\nabla f(x_{t}).\]

Note that \(\theta_{t}^{u}+\theta_{t}^{b}=\theta_{t}\). Regardless of the convexity of the function \(f\), the following lemma provides upper bounds for these quantities. These bounds can be found in prior works [7; 33; 19; 28] for the

\begin{table}
\begin{tabular}{|l|c|c|c|} \hline  & Assumptions & Convex Setting (Clipped-SMD) & Non-convex Setting (Clipped-SGD) \\ \hline \hline Lower bound & \(p\in(1,2]\) & \(\Omega\left(T^{\frac{1-p}{p}}\right)\)[31] & \(\Omega\left(T^{\frac{-2p-1}{3p-2}}\right)\)[33] \\ \hline Previous & & & \\ high-probability & Known \(T\) & \(\bar{O}\left(T^{\frac{1-p}{p}}\right)\)[28] & \(\bar{O}\left(T^{\frac{1-p}{p}}\right)\)[28] \\ results & & & \\ \hline Our results & Known \(T\) & \(O\left(T^{\frac{1-p}{p}}\right)\)(Thm 4.1) & \(O\left(T^{\frac{1-p+2}{3p-2}}\right)\)(Thm 3.1) \\ \cline{2-4}  & Unknown \(T\) & \(\bar{O}\left(T^{\frac{1-p}{p}}\right)\)(Thm 4.4) & \(\bar{O}\left(T^{\frac{1-p+2}{3p-2}}\right)\)(Thm B.2) \\ \hline \end{tabular}
\end{table}
Table 1: Previous and new results for high-probability convergence (with failure probability \(\delta\)) of clipped SMD and SGD under heavy tailed noise: \(\mathbb{E}[\|\widehat{\nabla}f(x)-\nabla f(x)\|_{*}^{p}\mid x]\leq\sigma^{p}\) for some \(p\in(1,2]\), where \(\widehat{\nabla}f(x)\) denotes the stochastic gradient at \(x\) for the objective \(f\). For the convex setting, the error bounds are for the optimality gaps \(\frac{1}{T}\sum_{t=1}^{T}f(x_{t})-f^{*}\). For the nonconvex setting, we bound the gradient norm \(\frac{1}{T}\sum_{t=1}^{T}\left\|\nabla f(x_{t})\right\|^{2}\). Here, \(\tilde{O}\left(\cdot\right)\) hides polylog \(T\) factors. Note that, for simplicity, we do not compare against results in more specialized settings such as bounded domain or bounded gradients, as well as other variants of clipped SGD.

special case of \(\ell_{2}\) norm. The extension to the general norm follows in the same manner, which we omit in this work.

**Lemma 2.1**.: _For stochastic gradients \(\widehat{\nabla}f(x_{t})\) with bounded \(p\)th moment noise, the clipped gradients \(\widetilde{\nabla}f(x_{t})\) satisfy the following properties:_

\[\left\|\theta_{t}^{u}\right\|_{*}=\left\|\widetilde{\nabla}f(x_{t})-\mathbb{E} \left[\widetilde{\nabla}f(x_{t})\mid\mathcal{F}_{t-1}\right]\right\|_{*}\leq 2 \lambda_{t}. \tag{2}\]

_Furthermore, if \(\left\|\nabla f(x_{t})\right\|_{*}\leq\frac{\lambda_{t}}{2}\) then_

\[\left\|\theta_{t}^{b}\right\|_{*} =\left\|\mathbb{E}\left[\widetilde{\nabla}f(x_{t})\mid\mathcal{F }_{t-1}\right]-\nabla f(x_{t})\right\|_{*}\leq 4\sigma^{p}\lambda_{t}^{1-p}; \tag{3}\] \[\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\right] =\mathbb{E}\left[\left\|\widetilde{\nabla}f(x_{t})-\mathbb{E}_{t} \left[\widetilde{\nabla}f(x_{t})\right]\right\|_{*}^{2}\mid\mathcal{F}_{t-1} \right]\leq 40\sigma^{p}\lambda_{t}^{2-p}. \tag{4}\]

Finally, we state a simple but important lemma that bounds the moment generating function of a zero-mean bounded random variable. The proof can be found in, for example, equation (3) of [1].

**Lemma 2.2**.: _Let \(X\) be a random variable such that \(\mathbb{E}\left[X\right]=0\) and \(\left|X\right|\leq R\) almost surely. Then for \(0\leq\lambda\leq\frac{1}{R}\)_

\[\mathbb{E}\left[\exp\left(\lambda X\right)\right]\leq\exp\left(\frac{3}{4} \lambda^{2}\mathbb{E}\left[X^{2}\right]\right).\]

## 3 Clipped Stochastic Gradient Descent for Nonconvex Functions

In this section, we study the convergence of Clipped-SGD for nonconvex functions. Here, we consider the domain to be \(\mathbb{R}^{d}\) equipped with the standard \(\ell_{2}\) norm. We first outline a blackbox concentration argument to show convergence in high probability of Algorithm 1 and then follow-up with a more powerful whitebox approach that allows for a tight high probability convergence analysis.

```
Parameters: initial point \(x_{1}\), step sizes \(\{\eta_{t}\}\), clipping parameters \(\{\lambda_{t}\}\) for \(t=1\) to \(T\)do \(\widetilde{\nabla}f(x_{t})=\min\left\{1,\frac{\lambda_{t}}{\left\|\nabla f(x_ {t})\right\|}\right\}\widehat{\nabla}f(x_{t})\) \(x_{t+1}=x_{t}-\eta_{t}\widehat{\nabla}f(x_{t})\)
```

**Algorithm 1** Clipped-SGD

**Comparison to previous works.** In the simple setting of known time horizon and without momentum for Clipped-SGD, the \(\widetilde{O}(T^{\frac{2-2p}{3p-2}})\) convergence rate has not been shown before to the best of our knowledge. The recent work by [28] study this case and only give a suboptimal rate of \(\widetilde{O}(T^{\frac{1-p}{p}})\). Note that [2, 19] study other variants of Clipped-SGD with momentums incorporated. Although [2, 19] achieve the nearly-optimal time dependency of \(\widetilde{O}(T^{\frac{2-2p}{3p-2}})\) in the non-convex settings, they rely on using blackbox concentration inequalities which result in a suboptimal convergence rate that also requires a known time horizon.

We first present the guarantee for known time horizon \(T\) via our whitebox approach in Theorem 3.1 and defer the statement for unknown \(T\) in Theorem B.2 to the Appendix.

**Theorem 3.1**.: _Assume that \(f\) satisfies Assumption (1'), (2), (3), (4). Let \(\gamma:=\max\left\{\log\frac{1}{3};1\right\}\) and \(\Delta_{1}:=f(x_{1})-f^{*}\). For known time horizon \(T\), we choose \(\lambda_{t}\) and \(\eta_{t}\) such that_

\[\lambda_{t} :=\lambda:=\max\left\{\left(\frac{8\gamma}{\sqrt{L\Delta_{1}}} \right)^{\frac{1}{p-1}}T^{\frac{1}{3p-2}}\sigma^{\frac{p}{p-1}};2\sqrt{90L \Delta_{1}};32^{\frac{1}{3p-2}}\sigma T^{\frac{1}{3p-2}}\right\}\] \[\eta_{t} :=\eta:=\frac{\sqrt{\Delta_{1}}T^{\frac{1-p}{3p-2}}}{8\lambda \sqrt{L}\gamma}=\frac{\sqrt{\Delta_{1}}}{8\sqrt{L}\gamma}\min\left\{\left( \frac{8\gamma}{\sqrt{L\Delta_{1}}}\right)^{\frac{-1}{p-1}}T^{\frac{-p}{3p-2}} \sigma^{\frac{-p}{p-1}};\frac{T^{\frac{1-p}{3p-2}}}{2\sqrt{90L\Delta_{1}}}; \frac{T^{\frac{-p}{3p-2}}}{32^{1/p}\sigma}\right\}.\]_Then with probability at least \(1-\delta\)_

\[\frac{1}{T}\sum_{t=1}^{T}\left\|\nabla f(x_{t})\right\|^{2} \leq 720\sqrt{\Delta_{1}}L\gamma\max\left\{\left(\frac{8\gamma}{ \sqrt{L\Delta_{1}}}\right)^{\frac{1}{p-1}}T^{\frac{2-2p}{3p-2}}\sigma\tau^{ \frac{p}{p-1}};\right.\] \[\left.2\sqrt{90L\Delta_{1}}T^{\frac{1-2q}{3p-2}};32^{1/p}\sigma T ^{\frac{3-2q}{3p-2}}\right\}=O\left(T^{\frac{2-2p}{3p-2}}\right).\]

_Remark 3.2_.: In comparison to the corresponding results in [28] (Theorem E.2), while our result achieves a poly \(T\) factor better rate when \(p<2\), the dependency on \(\log\frac{1}{\delta}\) in our result contains a dependency on \(p\) while the result in [28] does not. That term can dominate the convergence rate in the regime when \(\delta\) is very small and \(p\) is very close to \(1\). Hence, an open question is to remove such dependency on \(p\) for the \(\log\frac{1}{\delta}\) term while still maintain the optimal rate on \(T\).

Now, we turn to the analysis, starting with the key Lemma 3.3 (proof in the Appendix).

**Lemma 3.3**.: _Assume that \(f\) satisfies Assumption (1'), (2), (3), (4) and \(\eta_{t}\leq\frac{1}{L}\) then for all \(t\geq 1\),_

\[\frac{\eta_{t}}{2}\left\|\nabla f(x_{t})\right\|^{2} \leq\Delta_{t}-\Delta_{t+1}+\left(L\eta_{t}^{2}-\eta_{t}\right) \left\langle\nabla f(x_{t}),\theta_{t}^{u}\right\rangle+\frac{3\eta_{t}}{2} \left\|\theta_{t}^{b}\right\|^{2}\] \[+L\eta_{t}^{2}\left(\left\|\theta_{t}^{u}\right\|^{2}-\mathbb{E} \left[\left\|\theta_{t}^{u}\right\|^{2}\mid\mathcal{F}_{t-1}\right]\right)+L \eta_{t}^{2}\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|^{2}\mid\mathcal{F}_{ t-1}\right]. \tag{5}\]

_Remark 3.4_.: In Lemma 3.3, we decompose the RHS into appropriate terms that allow us to define a martingale. This lemma helps us understand why we can achieve a better convergence rate \(O(T^{\frac{2-2p}{3p-2}})\) (for minimizing the norm squared of the gradient) in comparison to the best rate of \(O(T^{\frac{1-p}{3p-2}})\) in the convex setting. We focus on the error term \(\left\langle\nabla f(x_{t}),\theta_{t}\right\rangle=\left\langle\nabla f(x_{t} ),\theta_{t}^{u}\right\rangle+\left\langle\nabla f(x_{t}),\theta_{t}^{b}\right\rangle\) on the RHS of (5). Since this error contains the gradient \(\nabla f(x_{t})\), we leverage some of the gain \(\|\nabla f(x_{t})\|^{2}\) on the LHS of 5: we use Cauchy-Schwarz to bound \(\left\langle\nabla f(x_{t}),\theta_{t}^{b}\right\rangle\leq\frac{1}{2}\|\nabla f (x_{t})\|^{2}+\frac{1}{2}\|\theta_{t}^{b}\|^{2}\) and use the some of the gain to absorb the first term. Then setting our parameters \(\lambda_{t},\eta_{t}\) appropriately to balance the remaining terms helps us achieve the \(O(T^{\frac{2-2p}{3p-2}})\) rate. Contrast this to the convex setting in the next section: the mismatch between the error term that contains the distance term \(\|x^{*}-x_{t}\|\) and the gain term that contains the function value gap \(f(x_{t})-f^{*}\) prevents us from using the gain to absorb some of the error. Thus, this explains the convergence rate discrepancy between the convex case and the non-convex setting (see also Remark 4.6).

Before giving a sketch of our whitebox approach, we present a sketch of a blackbox argument that gives a nearly time-optimal convergence rate. This approach has an additional \(\log T\) factor in the final rate but will serve as a point of comparison for our new techniques, which will close the logarithmic gap.

**Blackbox approach.** The key lies in the following lemma, which yields the near optimal \(\widetilde{O}(T^{\frac{2-2p}{3p-2}})\) convergence rate of Clipped-SGD. In this case, we assume that the clipping parameters \(\lambda_{t}\) and the step sizes \(\eta_{t}\) are fixed. Note that the success probability is only \(1-T\delta\). This result uses Lemma 3.3 and Freedman's inequality (Theorem A.1) primarily as a _blackbox_ to bound the error terms inductively by the initial function value gap to optimality.

**Lemma 3.5**.: _For \(1\leq N\leq T+1\), let \(\eta_{t}=\eta\), \(\lambda_{t}=\lambda\) (the specific choices are omitted here for brevity) and \(E_{N}\) be the event that for all \(k=1,\ldots N\),_

\[L\eta^{2}\sum_{t=1}^{k-1}\left\|\theta_{t}^{u}\right\|^{2}+\left(L\eta^{2}- \eta\right)\sum_{t=1}^{k-1}\left\langle\nabla f(x_{t}),\theta_{t}^{u}\right \rangle+\frac{3\eta}{2}\sum_{t=1}^{k-1}\left\|\theta_{t}^{b}\right\|^{2}\leq \Delta_{1}.\]

_Then \(E_{N}\) happens with probability at least \(1-\frac{(N-1)\delta}{T}\) for each \(N\in[T+1]\)._

With the above lemma, we can obtain a near-optimal convergence rate. However, this rate is still suboptimal due to the use of \(T\) union bounds as part of the induction proof. We now discuss an improved analysis that closes the remaining gap.

**Whitebox approach.** Our whitebox approach defines a novel supermartingale difference sequence \(Z_{t}\) (shown below) and analyzes its moment generating function from first principles. The sequence is 

[MISSING_PAGE_FAIL:7]

```
Parameters: initial point \(x_{1}\), step sizes \(\{\eta_{t}\}\), clipping parameters \(\{\lambda_{t}\}\), \(\psi\) is \(1\)-strongly convex wrt \(\|\cdot\|\) for \(t=1\) to \(T\) do \(\widetilde{\nabla}f(x_{t})=\min\left\{1,\frac{\lambda_{t}}{\left\|\widetilde{ \nabla}f(x_{t})\right\|_{*}}\right\}\widehat{\nabla}f(x_{t})\) \(x_{t+1}=\arg\min_{x\in\mathcal{X}}\left\{\eta_{t}\left\langle\widetilde{ \nabla}f(x_{t}),x\right\rangle+\mathbf{D}_{\psi}\left(x,x_{t}\right)\right\}\)
```

**Algorithm 2** Clipped-SMD

## 4 Clipped Stochastic Mirror Descent for Convex Objectives

In this section, we present and analyze the Clipped Stochastic Mirror Descent algorithm (Algorithm 2) under heavy-tailed noise, with a general domain and arbitrary norm.

We define the Bregman divergence \(\mathbf{D}_{\psi}(x,y)=\psi(x)-\psi(y)-\left\langle\nabla\psi(y),x-y\right\rangle\), where \(\psi:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is a \(1\)-strongly convex differentiable function with respect to the norm \(\|\cdot\|\) on \(\mathcal{X}\). We assume for convenience that \(\operatorname{dom}\left(\psi\right)=\mathbb{R}^{d}\). Algorithm 2 is a generalization of Clipped-SGD for convex functions to an arbitrary norm. The only difference from the standard Stochastic Mirror Descent algorithm is the use of the clipped gradient \(\widetilde{\nabla}f(x_{t})\) in place of the true stochastic gradient \(\widehat{\nabla}f(x_{t})\) when computing the new iterate \(x_{t+1}\).

Prior works such as [7] only consider the setting where the global minimizer lies in \(\mathcal{X}\). Our algorithm and analysis does not require this restriction and instead only uses the following initial gradient estimate assumption from [22]:

**(5) Initial gradient estimate**: Let \(x_{1}\) be the initial point. We assume that we have access to an upperbound \(\nabla_{1}\) of \(\|\nabla f(x_{1})\|_{*}\) i.e. \(\|\nabla f(x_{1})\|_{*}\leq\nabla_{1}\). This assumption is justified as follows. If the noise parameter \(\sigma\) defined in assumption (3) is known, we can use the procedure of [21] to estimate \(\|\nabla f(x_{1})\|_{*}\): we take \(O\left(\ln\left(1/\delta\right)\right)\) stochastic gradient samples at \(x_{1}\), and let \(g_{1}\) be the geometric median of these samples; we then set \(\nabla_{1}:=\left\|g_{1}\right\|_{*}+10\sigma\). It follows from [21] that \(\|\nabla f(x_{1})\|_{*}\leq\nabla_{1}\) holds with probability at least \(1-\delta\). If the domain contains the global optimum \(x^{*}\) (\(\nabla f(x^{*})=0\)) and the initial distance \(\|x_{1}-x^{*}\|\) is known, we have the following alternative upper bound that follows from \(\nabla f(x^{*})=0\) and smoothness:\(\|\nabla f(x_{1})\|_{*}=\|\nabla f(x_{1})-\nabla f(x^{*})\|_{*}\leq L\left\|x_{1}-x ^{*}\right\|\).

**Convergence guarantees**. We first state the convergence guarantee for this algorithm in Theorem 4.1 which works for an arbitrary norm and a general domain which may not include the global optimum. In this theorem, we assume that we know several problem parameters to show the main idea of our analysis. In Theorem 4.4, we remove the knowledge of the problem parameters.

**Theorem 4.1**.: _Assume that convex \(f\) satisfies Assumptions (1), (2), (3), (4) and (5). Let \(\gamma=\max\left\{\log\frac{1}{\delta};1\right\}\); \(R_{1}=\sqrt{2\mathbf{D}_{\psi}\left(x^{*},x_{1}\right)}\), and assume that \(\nabla_{1}\) is an upper bound of \(\|\nabla f(x_{1})\|_{*}\). For known \(T\), we choose \(\lambda_{t}\) and \(\eta_{t}\) such that_

\[\lambda_{t} =\lambda=\max\left\{\left(\frac{26T}{\gamma}\right)^{1/p}\sigma;2 \left(3LR_{1}+\nabla_{1}\right)\right\}\text{, and}\] \[\eta_{t} =\eta=\frac{R_{1}}{24\lambda_{t}\gamma}=\frac{R_{1}}{24\gamma} \min\left\{\left(\frac{26T}{\gamma}\right)^{-1/p}\sigma^{-1};\frac{1}{2} \left(3LR_{1}+\nabla_{1}\right)^{-1}\right\}.\]

_Then with probability at least \(1-\delta\)_

\[\frac{1}{T}\sum_{t=2}^{T+1}\Delta_{t}\leq 48R_{1}\max\left\{26^{\frac{1}{p}}T^ {\frac{1-p}{p}}\sigma\gamma^{\frac{p-1}{p}};2\left(3LR_{1}+\nabla_{1}\right)T^ {-1}\gamma\right\}=O\left(T^{\frac{1-p}{p}}\right).\]

_Remark 4.2_.: This theorem shows that the convergence rate for the known time horizon case is \(O(T^{\frac{1-p}{p}})\). This rate is known to be optimal, matching the lower bounds shown in [26; 31]. The above guarantee is also adaptive to \(\sigma\), i.e., when \(\sigma\to 0\), we obtain the standard \(O(T^{-1})\) convergence rate of deterministic mirror descent.

_Remark 4.3_.: The term \(\nabla_{1}\) in the above theorem comes from the inexact estimation of \(\left\|\nabla f(x_{1})\right\|_{*}\). If we assume that the global optimum lies in the domain \(\mathcal{X}\), we can simply select \(\nabla_{1}=LR_{1}\) without using the estimation procedure, as discussed in (5).

In Theorem 4.1, we use the initial distance \(R_{1}\) to the optimal solution to set the step sizes and clipping parameters. This information is generally not available, but can be avoided. For example, for constrained problems where the domain radius is bounded by \(R\), we can replace \(R_{1}\) in Theorem 4.1 by \(R\) without change in the dependency. However, for the general problem, we present Theorem 4.4, where we do not require knowledge of the constants \(T,\sigma,\delta\) or \(R_{1}\) to set the step sizes and clipping parameters. However, we still need the mild assumption of knowing an upper bound \(\nabla_{1}\) on \(\left\|\nabla f(x_{1})\right\|_{*}\). As discussed in (5), \(\nabla_{1}\) can be estimated with good accuracy when \(\sigma\) is known.

**Theorem 4.4**.: _Assume that convex \(f\) satisfies Assumption (1), (2), (3), (4) and (5). Let \(\gamma=\max\left\{\log\frac{1}{3};1\right\}\); \(R_{1}=\sqrt{2\mathbf{D}_{\psi}\left(x^{*},x_{1}\right)}\), and assume that \(\nabla_{1}\) is an upper bound of \(\left\|\nabla f(x_{1})\right\|_{*}\). We choose \(\lambda_{t}\) and \(\eta_{t}\) such that_

\[\lambda_{t} =\max\left\{\left(52t(1+\log t)^{2}c_{2}\right)^{1/p};2\left(L \max_{i\leq t}\left\|x_{i}-x_{1}\right\|+\nabla_{1}\right);\frac{Lc_{1}}{6} \right\}\text{, and}\] \[\eta_{t} =\frac{c_{1}}{24\lambda_{t}}=\frac{c_{1}}{24}\min\left\{\left(5 2t(1+\log t)^{2}c_{2}\right)^{-1/p};\frac{1}{2\left(L\max_{i\leq t}\left\|x_{ i}-x_{1}\right\|+\nabla_{1}\right)};\frac{6}{Lc_{1}}\right\},\]

_where the absolute constants \(c_{1}\) and \(c_{2}\) are to ensure the correctness of the dimensions. Then, with probability at least \(1-\delta\), we have_

\[\frac{1}{T}\sum_{t=2}^{T+1}\Delta_{t} \leq\frac{8}{Tc_{1}}\left(R_{1}+\frac{c_{1}}{3}\left(\gamma+\frac {2\sigma^{p}}{c_{2}}\right)\right)^{2}\max\left\{\left(52T(1+\log T)^{2}c_{2} \right)^{1/p};\right.\] \[\left.4R_{1}L+\frac{2c_{1}}{3}L\left(\gamma+\frac{2\sigma^{p}}{c _{2}}\right)+2\nabla_{1};\frac{Lc_{1}}{6}\right\}=\widetilde{O}\left(T^{\frac {1-p}{p}}\right).\]

**Sketch of the analysis**. In the remainder of this section, we provide a sketch of the analysis for Theorem 4.1, which starts with the following lemma.

**Lemma 4.5**.: _Assume that convex \(f\) satisfies Assumption (1), (2), (3), (4) and \(\eta_{t}\leq\frac{1}{4L}\), the iterate sequence \((x_{t})_{t\geq 1}\) output by Algorithm 2 satisfies the following:_

\[\eta_{t}\Delta_{t+1} \leq\mathbf{D}_{\psi}\left(x^{*},x_{t}\right)-\mathbf{D}_{\psi} \left(x^{*},x_{t+1}\right)+\eta_{t}\left\langle x^{*}-x_{t},\theta_{t}^{u} \right\rangle+\eta_{t}\left\langle x^{*}-x_{t},\theta_{t}^{b}\right\rangle\] \[+2\eta_{t}^{2}\left(\left\|\theta_{t}^{u}\right\|_{*}^{2}- \mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid\mathcal{F}_{t-1} \right]\right)+2\eta_{t}^{2}\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^ {2}\mid\mathcal{F}_{t-1}\right]+2\eta_{t}^{2}\left\|\theta_{t}^{b}\right\|_{*} ^{2}.\]

_Remark 4.6_.: In contrast to Remark 3.4, there is a mismatch between the gain \(\Delta_{t+1}\) and the loss \(\left\langle x^{*}-x_{t},\theta_{t}\right\rangle\). Since the distance \(\left\|x^{*}-x_{t}\right\|\) and the function value gap \(\Delta_{t}\) cannot be related in the general convex case, we do not obtain the same rate as in the nonconvex case.

We now define the following terms for \(t\geq 1\):

\[Z_{t} :=z_{t}\Bigg{(}\eta_{t}\Delta_{t+1}+\mathbf{D}_{\psi}\left(x^{*},x_{t+1}\right)-\mathbf{D}_{\psi}\left(x^{*},x_{t}\right)-\eta_{t}\left\langle x ^{*}-x_{t},\theta_{t}^{b}\right\rangle-2\eta_{t}^{2}\left\|\theta_{t}^{b} \right\|_{*}^{2}\] \[\quad-2\eta_{t}^{2}\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_ {*}^{2}\mid\mathcal{F}_{t-1}\right]\Bigg{)}-\left(\frac{3}{8\lambda_{t}^{2}}+24 z_{t}^{2}\eta_{t}^{4}\lambda_{t}^{2}\right)\mathbb{E}\left[\left\|\theta_{t}^{u} \right\|^{2}\mid\mathcal{F}_{t-1}\right],\] \[\text{where }z_{t} :=\frac{1}{2\eta_{t}\lambda_{t}\max_{i\leq t}\sqrt{2\mathbf{D}_{ \psi}\left(x^{*},x_{i}\right)+16Q\eta_{t}^{2}\lambda_{t}^{2}}}\]

for a constant \(Q\geq 1\). We also define \(S_{t}:=\sum_{i=1}^{t}Z_{i}\). We have the following lemma, which is analogous to Lemma 3.6 in the nonconvex case.

**Lemma 4.7**.: _For any \(\delta>0\), let \(E(\delta)\) be the event that for all \(1\leq k\leq T\)_

\[\sum_{t=1}^{k}z_{t}\eta_{t}\Delta_{t+1} +z_{k}\mathbf{D}_{\psi}\left(x^{*},x_{k+1}\right)\leq z_{1} \mathbf{D}_{\psi}\left(x^{*},x_{1}\right)+\log\frac{1}{\delta}+\sum_{t=1}^{k}z_ {t}\eta_{t}\left\langle x^{*}-x_{t},\theta_{t}^{b}\right\rangle\] \[+2\sum_{t=1}^{k}z_{t}\eta_{t}^{2}\left\|\theta_{t}^{b}\right\|_{* }^{2}+\sum_{t=1}^{k}\left(\left(2z_{t}\eta_{t}^{2}+\frac{3}{8\lambda_{t}^{2}}+24 z_{t}^{2}\eta_{t}^{4}\lambda_{t}^{2}\right)\mathbb{E}\left[\left\|\theta_{t}^{u} \right\|_{*}^{2}\mid\mathcal{F}_{t-1}\right]\right). \tag{6}\]

_Then \(\Pr\left[E(\delta)\right]\geq 1-\delta\)._```
Parameters: initial point \(y_{1}=z_{1}\), step sizes \(\{\eta_{t}\}\), clipping parameters \(\{\lambda_{t}\}\), and mirror map \(\psi\), where \(\psi\) is \(1\)-strongly convex wrt \(\left\|\cdot\right\|\). For \(t=1\) to \(T\) do: Set \(\alpha_{t}=\frac{2}{t+1}\). \(x_{t}=\left(1-\alpha_{t}\right)y_{t}+\alpha_{t}z_{t}\). \(\widetilde{\nabla}f(x_{t})=\min\left\{1,\frac{\lambda_{t}}{\left\|\widetilde{ \nabla}f(x_{t})\right\|_{*}}\right\}\widehat{\nabla}f(x_{t})\). \(z_{t+1}=\arg\min_{x\in\mathcal{X}}\Big{\{}\eta_{t}\left\langle\widetilde{ \nabla}f(x_{t}),x\right\rangle+\mathbf{D}_{\psi}\left(x,z_{t}\right)\Big{\}}\). \(y_{t+1}=\left(1-\alpha_{t}\right)y_{t}+\alpha_{t}z_{t+1}\).
```

**Algorithm 3** Clipped-ASMD

We now specify the choice of \(\eta_{t}\) and \(\lambda_{t}\). The following proposition gives a general condition for the choice of \(\eta_{t}\) and \(\lambda_{t}\) that gives the right convergence rate in time \(T\).

**Proposition 4.8**.: _We assume that the event \(E(\delta)\) from Lemma 4.7 happens. Suppose that for some \(\ell\leq T\), there are constants \(C_{1},C_{2},C_{3}\), and \(A\) such that for all \(t\leq\ell\)_

1. \(\lambda_{t}\eta_{t}=C_{1}\)_;_ 2. \(\sum_{t=1}^{\ell}\left(\frac{1}{\lambda_{t}}\right)^{p}\leq C_{2}\)_;_ 3. \(\left(\frac{1}{\lambda_{t}}\right)^{2p}\leq C_{3}\left(\frac{1}{\lambda_{t}} \right)^{p}\)_;_ 4. \(\left\|\nabla f(x_{t})\right\|_{*}\leq\frac{\lambda_{t}}{2}\)_._

_Then for all \(t\leq\ell+1\)_

\[\sum_{i=1}^{t}\eta_{i}\Delta_{i+1}+\mathbf{D}_{\psi}\left(x^{*},x_{t+1}\right) \leq\frac{1}{2}\left(R_{1}+8AC_{1}\right)^{2}\]

_for \(A\geq\max\left\{\log\frac{1}{\delta}+26\sigma^{p}C_{2}+\frac{2\sigma^{2p}C_{2} C_{3}}{A};1\right\}\)._

Theorem 4.1 follows from Proposition 4.8. Both proofs can be found in the Appendix.

## 5 Accelerated Stochastic Mirror Descent and Extensions

In Section D in the Appendix, we also show the convergence and its analysis for Clipped Accelerated Stochastic Mirror Descent (Algorithm 3). We require the following additional assumption:

**(5') Global minimizer**: We assume that \(\nabla f(x^{*})=0\).

In other words, we assume that the global minimizer lies in the domain of the problem. This assumption is consistent with the works of [7, 28]. Our analysis readily extends to non-smooth settings, and more generally to functions that satisfy \(f(y)-f(x)\leq\left\langle\nabla f(x),y-x\right\rangle+G\left\|y-x\right\|+ \frac{L}{2}\left\|y-x\right\|^{2},\quad\forall y,x\in\mathcal{X}.\) This condition is satisfied by both Lipschitz functions (when \(L=0\)) and smooth functions (when \(G=0\)). The key step is to extend Lemma 4.5. The proof follows from [15] and can be found in the Appendix.

## 6 Conclusion

In this work, we propose a new approach to design and analyze various clipped gradient algorithms in the presence of heavy-tailed noise. Our analysis applies to various standard settings, including Clipped-SMD and accelerated Clipped-SMD for convex objectives with general domains and Clipped-SGD for nonconvex objectives, and gives optimal high probability rates in all settings. Our algorithms allow for setting step-sizes and clipping parameters when the time horizon and problem parameters such as the initial distance are unknown. For future work, since our algorithms have the limitation of still requiring the knowledge of parameters like \(L\) and \(p\), it is of great interest to investigate the existence of a _fully-adaptive_ method, like Adagrad, that converges under heavy-tailed noise without requiring the knowledge of _any_ problem parameter. Finally, it would be interesting to extend our techniques to the setting of variational inequalities under heavy-tailed noise [6].

## Acknowledgement

TDN and AE were supported in part by NSF CAREER grant CCF-1750333, NSF grant III-1908510, and an Alfred P. Sloan Research Fellowship. THN and HN were supported by NSF CAREER grant CCF-1750716 and NSF grant 2311649.

## References

* [1] Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 19-26. JMLR Workshop and Conference Proceedings, 2011.
* [2] Ashok Cutkosky and Harsh Mehta. High-probability bounds for non-convex stochastic optimization with heavy tails. _Advances in Neural Information Processing Systems_, 34:4883-4895, 2021.
* [3] Kacha Dzhaparidze and JH Van Zanten. On bernstein-type inequalities for martingales. _Stochastic processes and their applications_, 93(1):109-117, 2001.
* [4] David A Freedman. On tail probabilities for martingales. _the Annals of Probability_, pages 100-118, 1975.
* [5] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368, 2013.
* [6] Eduard Gorbunov, Marina Danilova, David Dobre, Pavel Dvurechenskii, Alexander Gasnikov, and Gauthier Gidel. Clipped stochastic methods for variational inequalities with heavy-tailed noise. _Advances in Neural Information Processing Systems_, 35:31319-31332, 2022.
* [7] Eduard Gorbunov, Marina Danilova, and Alexander Gasnikov. Stochastic optimization with heavy-tailed noise via accelerated gradient clipping. _Advances in Neural Information Processing Systems_, 33:15042-15053, 2020.
* [8] Eduard Gorbunov, Marina Danilova, Innokentiy Shibaev, Pavel Dvurechensky, and Alexander Gasnikov. Near-optimal high probability complexity bounds for non-smooth stochastic optimization with heavy-tailed noise. _arXiv preprint arXiv:2106.05958_, 2021.
* [9] Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu. The heavy-tail phenomenon in sgd. In _International Conference on Machine Learning_, pages 3964-3975. PMLR, 2021.
* [10] Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for non-smooth stochastic gradient descent. In _Conference on Learning Theory_, pages 1579-1613. PMLR, 2019.
* [11] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. _The Journal of Machine Learning Research_, 15(1):2489-2512, 2014.
* [12] Sham M Kakade and Ambuj Tewari. On the generalization ability of online strongly convex programming algorithms. _Advances in Neural Information Processing Systems_, 21, 2008.
* [13] Ahmed Khaled and Peter Richtarik. Better theory for sgd in the nonconvex world. _arXiv preprint arXiv:2002.03329_, 2020.
* [14] Guanghui Lan. An optimal method for stochastic composite optimization. _Mathematical Programming_, 133(1):365-397, 2012.
* [15] Guanghui Lan. _First-order and stochastic optimization methods for machine learning_. Springer, 2020.
* [16] Shaojie Li and Yong Liu. High probability guarantees for nonconvex stochastic gradient descent with heavy tails. In _International Conference on Machine Learning_, pages 12931-12963. PMLR, 2022.

* [17] Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive sgd with momentum. _arXiv preprint arXiv:2007.14294_, 2020.
* [18] Zijian Liu, Ta Duy Nguyen, Thien Hang Nguyen, Alina Ene, and Huy Le Nguyen. High probability convergence of stochastic gradient methods. _International Conference on Machine Learning_, 2023.
* [19] Zijian Liu, Jiawei Zhang, and Zhengyuan Zhou. Breaking the lower bound with (little) structure: Acceleration in non-convex stochastic optimization with heavy-tailed noise. _arXiv preprint arXiv:2302.06763_, 2023.
* [20] Liam Madden, Emiliano Dall'Anese, and Stephen Becker. High probability convergence and uniform stability bounds for nonconvex stochastic gradient descent. _arXiv preprint arXiv:2006.05610_, 2020.
* [21] Stanislav Minsker. Geometric median and robust estimation in banach spaces. _Bernoulli_, pages 2308-2335, 2015.
* [22] Alexander V Nazin, Arkadi S Nemirovsky, Alexandre B Tsybakov, and Anatoli B Juditsky. Algorithms of robust stochastic optimization based on mirror descent method. _Automation and Remote Control_, 80(9):1607-1627, 2019.
* [23] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on optimization_, 19(4):1574-1609, 2009.
* [24] Daniela A Parletta, Andrea Paudice, Massimiliano Pontil, and Saverio Salzo. High probability bounds for stochastic subgradient schemes with heavy tailed noise. _arXiv preprint arXiv:2208.08567_, 2022.
* [25] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem. _CoRR, abs/1211.5063_, 2(417):1, 2012.
* [26] Maxim Raginsky and Alexander Rakhlin. Information complexity of black-box convex optimization: A new look via feedback information theory. In _2009 47th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 803-510. IEEE, 2009.
* [27] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. _arXiv preprint arXiv:1109.5647_, 2011.
* [28] Abdurakhmon Sadiev, Marina Danilova, Eduard Gorbunov, Samuel Horvath, Gauthier Gidel, Pavel Dvurechensky, Alexander Gasnikov, and Peter Richtarik. High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded variance. _arXiv preprint arXiv:2302.00999_, 2023.
* [29] Umut Simsekli, Mert Gurbuzbalaban, Thanh Huy Nguyen, Gael Richard, and Levent Sagun. On the heavy-tailed theory of stochastic gradient descent for deep neural networks. _arXiv preprint arXiv:1912.00018_, 2019.
* [30] Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gradient noise in deep neural networks. pages 5827-5837, 2019.
* [31] Nuri Mert Vural, Lu Yu, Krishna Balasubramanian, Stanislav Volgushev, and Murat A Erdogdu. Mirror descent strikes again: Optimal stochastic convex optimization under infinite noise variance. In _Conference on Learning Theory_, pages 65-102. PMLR, 2022.
* [32] Hongjian Wang, Mert Gurbuzbalaban, Lingjiong Zhu, Umut Simsekli, and Murat A Erdogdu. Convergence rates of stochastic gradient descent under infinite noise variance. _Advances in Neural Information Processing Systems_, 34:18866-18877, 2021.
* [33] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? _Advances in Neural Information Processing Systems_, 33:15383-15393, 2020.
* [34] Jiujia Zhang and Ashok Cutkosky. Parameter-free regret in high probability with heavy tails. In _Advances in Neural Information Processing Systems_, 2022.

## Appendix A Freedman's inequality

**Lemma A.1** (Freedman's inequality).: _Let \((X_{t})_{t\geq 1}\) be a martingale difference sequence. Assume that there exists a constant \(c>0\) such that \(\left|X_{t}\right|\leq c\) almost surely for all \(t\geq 1\) and define \(\sigma_{t}^{2}=\mathbb{E}\left[X_{t}^{2}\mid X_{t-1},\ldots,X_{1}\right]\). Then for all \(b>0\), \(F>0\) and \(T\geq 1\)_

\[\Pr\left[\left|\sum_{t=1}^{T}X_{t}\right|>b\text{ and }\sum_{t=1}^{T}\sigma_{t}^{2 }\leq F\right]\leq 2\exp\left(-\frac{b^{2}}{2F+2cb/3}\right).\]

## Appendix B Missing Proofs from Section 3

Proof of Lemma 3.3.: By the smoothness of \(f\) and the update \(x_{t+1}=x_{t}-\frac{1}{\eta_{t}}\widetilde{\nabla}f(x_{t})\) we have

\[f(x_{t+1})-f(x_{t})\] \[\leq \left\langle\nabla f(x_{t}),x_{t+1}-x_{t}\right\rangle+\frac{L}{ 2}\left\|x_{t+1}-x_{t}\right\|^{2}\] \[= -\eta_{t}\left\langle\nabla f(x_{t}),\widetilde{\nabla}f(x_{t}) \right\rangle+\frac{L\eta_{t}^{2}}{2}\left\|\widetilde{\nabla}f(x_{t})\right\| ^{2}\] \[= -\eta_{t}\left\langle\nabla f(x_{t}),\theta_{t}+\nabla f(x_{t}) \right\rangle+\frac{L\eta_{t}^{2}}{2}\left\|\theta_{t}+\nabla f(x_{t})\right\| ^{2}\] \[= -\eta_{t}\left\|\nabla f(x_{t})\right\|^{2}-\eta_{t}\left\langle \nabla f(x_{t}),\theta_{t}\right\rangle+\frac{L\eta_{t}^{2}}{2}\left\|\theta_{ t}\right\|^{2}+\frac{L\eta_{t}^{2}}{2}\left\|\nabla f(x_{t})\right\|^{2}+L \eta_{t}^{2}\left\langle\nabla f(x_{t}),\theta_{t}\right\rangle\] \[= -\left(\eta_{t}-\frac{L\eta_{t}^{2}}{2}\right)\left\|\nabla f(x_{ t})\right\|^{2}+\frac{L\eta_{t}^{2}}{2}\left\|\theta_{t}\right\|^{2}+\left(L \eta_{t}^{2}-\eta_{t}\right)\left\langle\nabla f(x_{t}),\theta_{t}\right\rangle\] \[= -\left(\eta_{t}-\frac{L\eta_{t}^{2}}{2}\right)\left\|\nabla f(x_{ t})\right\|^{2}+\frac{L\eta_{t}^{2}}{2}\left\|\theta_{t}\right\|^{2}+\underbrace{ \left(L\eta_{t}^{2}-\eta_{t}\right)}_{\leq 0}\left\langle\nabla f(x_{t}),\theta_{t}^{u}+ \theta_{t}^{b}\right\rangle.\]

Using Cauchy-Schwarz, we have \(\left\langle\nabla f(x_{t}),\theta_{t}^{b}\right\rangle\leq\frac{1}{2}\left\| \nabla f(x_{t})\right\|^{2}+\frac{1}{2}\left\|\theta_{t}^{b}\right\|^{2}\). Thus, we derive

\[\Delta_{t+1}-\Delta_{t} \leq -\left(\frac{2\eta_{t}-L\eta_{t}^{2}}{2}\right)\left\|\nabla f(x_{ t})\right\|^{2}+\frac{L\eta_{t}^{2}}{2}\left\|\theta_{t}\right\|^{2}+\left(L \eta_{t}^{2}-\eta_{t}\right)\left\langle\nabla f(x_{t}),\theta_{t}^{u}\right\rangle\] \[+\frac{\eta_{t}-L\eta_{t}^{2}}{2}\left\|\nabla f(x_{t})\right\|^{2 }+\frac{\eta_{t}-L\eta_{t}^{2}}{2}\left\|\theta_{t}^{b}\right\|^{2}\] \[\leq -\frac{\eta_{t}}{2}\left\|\nabla f(x_{t})\right\|^{2}+\frac{L\eta _{t}^{2}}{2}\left\|\theta_{t}\right\|^{2}+\left(L\eta_{t}^{2}-\eta_{t}\right) \left\langle\nabla f(x_{t}),\theta_{t}^{u}\right\rangle+\frac{\eta_{t}}{2} \left\|\theta_{t}^{b}\right\|^{2}\] \[\leq -\frac{\eta_{t}}{2}\left\|\nabla f(x_{t})\right\|^{2}+L\eta_{t}^{ 2}\left\|\theta_{t}^{u}\right\|^{2}+\left(L\eta_{t}^{2}-\eta_{t}\right)\left\langle \nabla f(x_{t}),\theta_{t}^{u}\right\rangle+\left(L\eta_{t}^{2}+\frac{\eta_{t} }{2}\right)\left\|\theta_{t}^{b}\right\|^{2}\] \[\leq -\frac{\eta_{t}}{2}\left\|\nabla f(x_{t})\right\|^{2}+L\eta_{t}^ {2}\left\|\theta_{t}^{u}\right\|^{2}+\left(L\eta_{t}^{2}-\eta_{t}\right)\left\langle \nabla f(x_{t}),\theta_{t}^{u}\right\rangle+\frac{3\eta_{t}}{2}\left\|\theta_{t }^{b}\right\|^{2},\]

where the third inequality is due to \(\left\|\theta_{t}\right\|^{2}\leq 2\left\|\theta_{t}^{u}\right\|^{2}+2\left\|\theta_{t}^{b} \right\|^{2}\), and the last inequality is due to \(\eta_{t}\leq\frac{1}{L}\). Rearranging, adding, and subtracting \(\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|^{2}\mid\mathcal{F}_{t-1}\right]\), we obtain the lemma. 

Detailed proof of Lemma 3.5.: We state the following simple properties of the choice of \(\eta\) and \(\lambda\) in Theorem 3.1. We have

\[\frac{1}{L}\left(\frac{\sigma}{\lambda}\right)^{p} \leq\eta \tag{7}\] \[\eta \leq\frac{1}{L}\] (8) \[\left(\frac{\sigma}{\lambda}\right)^{p}T^{\frac{p}{3p-2}} \leq\frac{1}{32} \tag{9}\]\[TL\left(\frac{\sigma}{\lambda}\right)^{p}\lambda^{2}\eta^{2}\leq\frac{\Delta_{1}}{20 48}. \tag{10}\]

We will now prove by induction on \(N\) that \(E_{N}\) happens with probability at least \(1-\frac{(N-1)\delta}{T}\). For \(N=1\), the event happens with probability \(1\). Suppose that for some \(N\leq T\), \(\Pr\left[E_{N}\right]\geq 1-\frac{(N-1)\delta}{T}\). We will prove that \(\Pr\left[E_{N+1}\right]\geq 1-\frac{N\delta}{T}\).

Since the LHS of (5) is non-negative, for \(k\leq N\), we have, under the event \(E_{N}\),

\[\Delta_{k} \leq\Delta_{1}+\left(L\eta^{2}-\eta\right)\sum_{t=1}^{k-1}\left< \nabla f(x_{t}),\theta_{t}^{u}\right>+L\eta^{2}\sum_{t=1}^{k-1}\left(\left\| \theta_{t}^{u}\right\|^{2}-\mathbb{E}_{t}\left[\left\|\theta_{t}^{u}\right\|^{ 2}\right]\right)\] \[\quad+\frac{3\eta}{2}\sum_{t=1}^{k-1}\left\|\theta_{t}^{b}\right\| ^{2}+L\eta^{2}\sum_{t=1}^{k-1}\mathbb{E}_{t}\left[\left\|\theta_{t}^{u}\right\| ^{2}\right]\leq 2\Delta_{1}.\]

From the induction hypothesis and Lemma 3.3, we have that for all \(k\leq N\), \(\Delta_{k}\leq 2\Delta_{1}\). Since the LHS of (5) is non-negative, by summing over \(t\) from \(1\) to \(N\) we have,

\[\Delta_{N+1} \leq\underbrace{\left(\eta-L\eta^{2}\right)\sum_{t=1}^{N}\left<- \nabla f(x_{t}),\theta_{t}^{u}\right>}_{A}+\underbrace{\frac{3\eta}{2}\sum_{t= 1}^{N}\left\|\theta_{t}^{b}\right\|^{2}}_{B}\] \[\quad+\underbrace{L\eta^{2}\sum_{t=1}^{N}\left(\left\|\theta_{t}^ {u}\right\|^{2}-\mathbb{E}_{t}\left[\left\|\theta_{t}^{u}\right\|^{2}\right] \right)}_{C}+\underbrace{L\eta^{2}\sum_{t=1}^{N}\mathbb{E}_{t}\left[\left\| \theta_{t}^{u}\right\|^{2}\right]}_{D}.\]

The bounds for \(B\) and \(D\) are straightforward from Lemma 2.1. First, with probability \(1\), we have \(\left\|\theta_{t}^{u}\right\|\leq 2\lambda\). By the smoothness of \(f\) and the fact that \(f\) is bounded below, we have

\[\left\|\nabla f(x_{t})\right\|\leq\sqrt{2L\Delta_{t}}.\]

Furthermore, when the event \(E_{N}\) happens, we have

\[\left\|\nabla f(x_{t})\right\|\leq\sqrt{2L\Delta_{t}}\leq\sqrt{4L\Delta_{1}} \leq\frac{\lambda}{2}.\]

Thus, we can apply Lemma 2.1 and obtain \(\left\|\theta_{t}^{b}\right\|\leq 4\sigma^{p}\lambda^{1-p}\) and \(\mathbb{E}_{t}\left[\left\|\theta_{t}^{u}\right\|^{2}\right]\leq 40\sigma^{p} \lambda^{2-p}\).

Upperbound for \(B\).By (3), when the event \(E_{N}\) happens,

\[B =\frac{3\eta}{2}\left\|\theta_{t}^{b}\right\|^{2}\leq\frac{3\eta} {2}\sum_{t=1}^{N}16\sigma^{2p}\lambda^{2-2p}=24\sigma^{2p}\lambda^{2-2p}\eta N\] \[\leq 24T\left(\frac{\sigma}{\lambda}\right)^{2p}\lambda^{2}\eta \leq 24TL\left(\frac{\sigma}{\lambda}\right)^{p}\lambda^{2}\eta^{2}\leq\frac {3\Delta_{1}}{256}.\]

Upperbound for \(D\).By 4, when the event \(E_{N}\) happens,

\[D =L\eta^{2}\sum_{t=1}^{N}\mathbb{E}_{t}\left[\left\|\theta_{t}^{u} \right\|^{2}\right]\leq L\eta^{2}\sum_{t=1}^{N}40\sigma^{p}\lambda^{2-p}\] \[\leq 40\sigma^{p}\lambda^{2-p}L\eta^{2}N\leq 40LT\left(\frac{\sigma}{ \lambda}\right)^{p}\left(\lambda\eta\right)^{2}\leq\frac{5\Delta_{1}}{256}.\]

To bound \(A\) and \(C\), we use Freedman's inequality (Theorem A.1). We define, for \(t\geq 1\), the following random variables

\[Z_{t}=\begin{cases}-\nabla f(x_{t})&\text{if }\Delta_{t}\leq 2\Delta_{1}\\ 0&\text{otherwise}.\end{cases}\]

Thus \(\left\|Z_{t}\right\|\leq\left\|\nabla f(x_{t})\right\|\leq 2\sqrt{L\Delta_{1}}\) for all \(t\).

Upperbound for \(A\).Instead of bounding \(A=\left(\eta-L\eta^{2}\right)\sum_{t=1}^{N}\left\langle-\nabla f(x_{t}),\theta_{t} ^{u}\right\rangle\), we will bound \(A^{\prime}=\left(\eta-L\eta^{2}\right)\sum_{t=1}^{N}\left\langle Z_{t},\theta_{t }^{u}\right\rangle\). We check the conditions to apply Freedman's inequality. First \(\mathbb{E}_{t}\left[\left(\eta-L\eta^{2}\right)\left\langle Z_{t},\theta_{t}^{u }\right\rangle=0\right]\). Further, with probability \(1\), \(\left\|\theta_{t}^{u}\right\|^{2}\leq 2\lambda\), and \(Z_{t}\leq 2\sqrt{L\Delta_{1}}\), thus\(\left|\left(\eta-L\eta^{2}\right)\left\langle Z_{t},\theta_{t}^{u}\right\rangle \right|\leq\left(\eta-L\eta^{2}\right)\left\|Z_{t}\right\|\left\|\theta_{t}^{u }\right\|\leq 4\sqrt{L\Delta_{1}}\left(\eta-L\eta^{2}\right)\lambda\leq 4\sqrt{L \Delta_{1}}\eta\lambda\). Hence, \(\left\{\left(\eta-L\eta^{2}\right)\left\langle Z_{t},\theta_{t}^{u}\right\rangle\right\}\) is a bounded martingale difference sequence. Therefore, for constant \(a\) and \(F\) to be chosen we have

\[\Pr\left[\left|\sum_{t=1}^{N}\left(\eta-L\eta^{2}\right)\left\langle Z _{t},\theta_{t}^{u}\right\rangle\right|>a\text{ and }\sum_{t=1}^{N}\mathbb{E}_{t} \left[\left(\left(\eta-L\eta^{2}\right)\left\langle Z_{t},\theta_{t}^{u}\right \rangle\right)^{2}\right]\leq F\ln\frac{4T}{\delta}\right]\] \[\leq 2\exp\left(-\frac{a^{2}}{2F\ln\frac{4T}{\delta}+\frac{8}{3} \sqrt{L\Delta_{1}}\eta\lambda a}\right)\]

We choose \(a\) such that

\[2\exp\left(-\frac{a^{2}}{2F\ln\frac{4T}{\delta}+\frac{8}{3}\sqrt{L\Delta_{1}} \eta\lambda a}\right)=\frac{\delta}{2T}\]

which gives

\[a=\left(\frac{4}{3}\sqrt{L\Delta_{1}}\eta\lambda+\sqrt{\frac{16L \Delta_{1}\eta^{2}\lambda^{2}}{9}+2F}\right)\ln\frac{4T}{\delta}.\]

If we choose \(F=64L\Delta_{1}\sigma^{p}\lambda^{2-p}\eta^{2}T\), we can easily show that \(a\leq\frac{7\Delta_{1}}{12}\). Therefore, with probability at least \(1-\frac{\delta}{2T}\) we have

\[E_{A}=\Bigg{\{}\text{either}\,A^{\prime}\leq\left|\sum_{t=1}^{N} \left(\eta-L\eta^{2}\right)\left\langle Z_{t},\theta_{t}^{u}\right\rangle \right|\leq\frac{7\Delta_{1}}{12}\] \[\text{ or }\sum_{t=1}^{N}\mathbb{E}_{t}\left[\left(\left(\eta-L \eta^{2}\right)\left\langle Z_{t},\theta_{t}^{u}\right\rangle\right)^{2} \right]>F\ln\frac{4T}{\delta}\Bigg{\}}.\]

Also notice that under the event \(E_{N}\), we have

\[\sum_{t=1}^{N}\mathbb{E}_{t}\left[\left(\left(\eta-L\eta^{2} \right)\left\langle Z_{t},\theta_{t}^{u}\right\rangle\right)^{2}\right]\] \[\leq \eta^{2}\sum_{t=1}^{N}\mathbb{E}_{t}\left[\left\|Z_{t}\right\|^{ 2}\left\|\theta_{t}^{u}\right\|^{2}\right]\leq 4\eta^{2}L\Delta_{1}\sum_{t=1}^{N} \mathbb{E}_{t}\left[\left\|\theta_{t}^{u}\right\|^{2}\right]\] \[\leq 64L\Delta_{1}\sigma^{p}\lambda^{2-p}\eta^{2}N\leq 64\Delta_{1} LT\left(\frac{\sigma}{\lambda}\right)^{p}\lambda^{2}\eta^{2}\leq F\leq F\ln\frac{4T}{ \delta}. \tag{11}\]

Under \(E_{N}\), we have that \(Z_{t}=-\nabla f(x_{t})\) for all \(t\leq N\). Therefore, when \(E_{N}\cap E_{A}\) happens, we have \(A=A^{\prime}\leq a\).

Upperbound for \(C\).We check the conditions to apply Freedman's inequality. First, \(\mathbb{E}_{t}\left[L\eta^{2}\left(\left\|\theta_{t}^{u}\right\|^{2}-\mathbb{E }_{t}\left[\left\|\theta_{t}^{u}\right\|^{2}\right]\right)\right]=0\). Further, with probability \(1\), \(\left\|\theta_{t}^{u}\right\|^{2}\leq 2\lambda\), thus\(\left|L\eta^{2}\left(\left\|\theta_{t}^{u}\right\|^{2}-\mathbb{E}_{t}\left[\left\| \theta_{t}^{u}\right\|^{2}\right]\right)\right|\leq L\eta^{2}\left(4\lambda^{ 2}+4\lambda^{2}\right)=8L\lambda^{2}\eta^{2}\). Hence \(\left\{L\eta^{2}\left(\left\|\theta_{t}^{u}\right\|^{2}-\mathbb{E}_{t}\left[ \left\|\theta_{t}^{u}\right\|^{2}\right]\right)\right\}\) is a bounded martingale difference sequence. Applying Freedman's inequality for constants \(c\) and \(G\) to be chosen, we have

\[\Pr\left[\left|L\eta^{2}\sum_{t=1}^{N}\left(\left\|\theta_{t}^{u }\right\|^{2}-\mathbb{E}_{t}\left[\left\|\theta_{t}^{u}\right\|^{2}\right] \right)\right]>c\text{ and }\sum_{t=1}^{N}\mathbb{E}_{t}\left[\left(L\eta^{2}\left( \left\|\theta_{t}^{u}\right\|^{2}-\mathbb{E}_{t}\left[\left\|\theta_{t}^{u} \right\|^{2}\right]\right)\right)^{2}\right]\leq G\ln\frac{4T}{\delta}\right]\] \[\leq 2\exp\left(-\frac{c^{2}}{2G\ln\frac{4T}{\delta}+\frac{16}{3}L \lambda^{2}\eta^{2}c}\right)\]We choose \(c\) such that

\[2\exp\left(-\frac{c^{2}}{2G\ln\frac{4T}{\delta}+\frac{16}{3}L\lambda^{2}\eta^{2}c} \right)=\frac{\delta}{2T}\]

which gives

\[c=\left(\frac{8}{3}L\lambda^{2}\eta^{2}+\sqrt{\frac{64L^{2}\lambda^{4}\eta^{4}}{ 9}+2G}\right)\ln\frac{4T}{\delta}\]

If we choose \(G=256L^{2}\sigma^{p}\lambda^{4-p}\eta^{4}T\), a simple calculation shows that \(c\leq\frac{7\Delta_{1}}{48}\). we can show that with probability at least \(1-\frac{\delta}{2T}\), the following event happens

\[E_{C} =\Bigg{\{}\text{either}\;C\leq\Bigg{|}L\eta^{2}\sum_{t=1}^{N} \left(\left\|\theta_{t}^{u}\right\|^{2}-\mathbb{E}_{t}\left[\left\|\theta_{t} ^{u}\right\|^{2}\right]\right)\Bigg{|}\leq\frac{7\Delta_{1}}{48}\] \[\text{or}\;\sum_{t=1}^{N}\mathbb{E}_{t}\left[\left(L\eta^{2}\left( \left\|\theta_{t}^{u}\right\|^{2}-\mathbb{E}_{t}\left[\left\|\theta_{t}^{u} \right\|^{2}\right]\right)\right)^{2}\right]\geq G\ln\frac{4T}{\delta}\Bigg{\}}.\]

Notice that when \(G=256L^{2}\sigma^{p}\lambda^{4-p}\eta^{4}T\), under \(E_{N}\) we have

\[\sum_{t=1}^{N}\mathbb{E}_{t}\left[\left(L\eta^{2}\left(\left\| \theta_{t}^{u}\right\|^{2}-\mathbb{E}_{t}\left[\left\|\theta_{t}^{u}\right\|^ {2}\right]\right)\right)^{2}\right]\] \[\leq 8L\lambda^{2}\eta^{2}\sum_{t=1}^{N}\mathbb{E}_{t}\left[\left|L \eta^{2}\left(\left\|\theta_{t}^{u}\right\|^{2}-\mathbb{E}_{t}\left[\left\| \theta_{t}^{u}\right\|^{2}\right]\right)\right|\right]\leq 16L^{2}\lambda^{2}\eta^{4} \sum_{t=1}^{N}\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|^{2}\right]\] \[\leq 256L^{2}\sigma^{p}\lambda^{4-p}\eta^{4}N\leq G<G\ln\frac{4T}{ \delta}. \tag{12}\]

Therefore, when \(E_{N}\cap E_{C}\) happens, we have \(C\leq c\).

Finally, combining all the bounds for \(A,B,C,D\) using union bound and selecting \(\lambda\) and \(\eta\) appropriately to simplify the constants, we obtain the lemma. 

Proof of Lemma 3.6.: We have

\[\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t-1}\right] \exp\left(\left(3z_{t}^{2}L\eta_{t}^{2}\Delta_{t}+6L^{2}z_{t}^{2}\eta_{t}^{4} \lambda_{t}^{2}\right)\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|^{2}\mid \mathcal{F}_{t-1}\right]\right)\] \[\overset{(a)}{\leq}\mathbb{E}\left[\exp\left(z_{t}\left(\left(L \eta_{t}^{2}-\eta_{t}\right)\left\langle\nabla f(x_{t}),\theta_{t}^{u}\right\rangle +L\eta_{t}^{2}\left(\left\|\theta_{t}^{u}\right\|^{2}-\mathbb{E}\left[\left\| \theta_{t}^{u}\right\|^{2}\mid\mathcal{F}_{t-1}\right]\right)\right)\right) \mid\mathcal{F}_{t-1}\right]\] \[\overset{(b)}{\leq}\exp\left(\mathbb{E}\left[\frac{3}{4}\left(z _{t}\left(\left(L\eta_{t}^{2}-\eta_{t}\right)\left\langle\nabla f(x_{t}),\theta _{t}^{u}\right\rangle+L\eta_{t}^{2}\left(\left\|\theta_{t}^{u}\right\|^{2}- \mathbb{E}\left[\left\|\theta_{t}^{u}\right\|^{2}\mid\mathcal{F}_{t-1}\right] \right)\right)\right)^{2}\mid\mathcal{F}_{t-1}\right]\right)\] \[\overset{(c)}{\leq}\exp\left(\mathbb{E}\left[\frac{3}{2}z_{t}^{ 2}\eta_{t}^{2}\left\|\nabla f(x_{t})\right\|^{2}\left\|\theta_{t}^{u}\right\|^ {2}\mid\mathcal{F}_{t-1}\right]+\mathbb{E}\left[\frac{3}{2}L^{2}z_{t}^{2}\eta_{t }^{4}\left\|\theta_{t}^{u}\right\|^{4}\mid\mathcal{F}_{t-1}\right]\right)\] \[\overset{(d)}{\leq}\exp\left(3z_{t}^{2}L\eta_{t}^{2}\Delta_{t} \mathbb{E}\left[\left\|\theta_{t}^{u}\right\|^{2}\mid\mathcal{F}_{t-1}\right] +6L^{2}z_{t}^{2}\eta_{t}^{4}\lambda_{t}^{2}\mathbb{E}\left[\left\|\theta_{t}^{ u}\right\|^{2}\mid\mathcal{F}_{t-1}\right]\right)\] \[=\exp\left(\left(3z_{t}^{2}L\eta_{t}^{2}\Delta_{t}+6L^{2}z_{t}^{2 }\eta_{t}^{4}\lambda_{t}^{2}\right)\mathbb{E}\left[\left\|\theta_{t}^{u}\right\| ^{2}\mid\mathcal{F}_{t-1}\right]\right)\]

For \((a)\) we use Lemma 3.3. For \((b)\) we use Lemma 2.2. Notice that

\[\mathbb{E}\left[\left\langle\nabla f(x_{t}),\theta_{t}^{u}\right\rangle\right]= \mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}-\mathbb{E}\left[\left\| \theta_{t}^{u}\right\|_{*}^{2}\mid\mathcal{F}_{t-1}\right]\right]=0,\]

and since \(\left\|\theta_{t}^{u}\right\|\leq 2\lambda_{t}\) and \(\left\|\nabla f(x_{t})\right\|\leq\sqrt{2L\Delta_{t}}\) for an \(L\)-smooth function, we have

\[\left|\left(L\eta_{t}^{2}-\eta_{t}\right)\left\langle\nabla f(x_{t}),\theta_{t}^ {u}\right\rangle+L\eta_{t}^{2}\left(\left\|\theta_{t}^{u}\right\|^{2}-\mathbb{E }\left[\left\|\theta_{t}^{u}\right\|^{2}\mid\mathcal{F}_{t-1}\right]\right)\right|\]\[\leq 2\eta_{t}\lambda_{t}\left\|\nabla f(x_{t})\right\|+L\eta_{t}^{2} \left(\|\theta_{t}^{u}\|^{2}+\mathbb{E}\left[\|\theta^{u}\|^{2}\mid\mathcal{F}_{ t-1}\right]\right)\] \[\leq 2\eta_{t}\lambda_{t}\left\|\nabla f(x_{t})\right\|+8L\eta_{t}^{2 }\lambda_{t}^{2}\] \[\leq 2\eta_{t}\lambda_{t}\sqrt{2L\Delta_{t}}+8L\eta_{t}^{2}\lambda_{t }^{2}.\]

Thus \(z_{t}\leq\frac{1}{2\eta_{t}\lambda_{t}\sqrt{2L\Delta_{t}}+8L\eta_{t}^{2} \lambda_{t}^{2}}\). For \((c)\) we use \((a+b)^{2}\leq 2a^{2}+2b^{2}\) and \(\mathbb{E}\left[\left(X-\mathbb{E}\left[X\right]\right)^{2}\right]\leq \mathbb{E}\left[X^{2}\right]\). For \((d)\), we use \(\left\|\nabla f(x_{t})\right\|^{2}\leq 2L\Delta_{t}\) and \(\left\|\theta_{t}^{u}\right\|\leq 2\lambda_{t}\). We obtain

\[\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t-1}\right]\leq 1.\]

Therefore

\[\mathbb{E}\left[\exp\left(S_{t}\right)\mid\mathcal{F}_{t-1}\right] =\exp\left(S_{t-1}\right)\mathbb{E}\left[\exp\left(Z_{t}\right) \mid\mathcal{F}_{t-1}\right]\] \[\leq\exp\left(S_{t-1}\right)\]

which means \((\exp\left(S_{t}\right))_{t\geq 1}\) is a supermartingale. By Ville's inequality, we have, for all \(k\geq 1\)

\[\Pr\left[S_{k}\geq\log\frac{1}{\delta}\right]\leq\delta\mathbb{E}\left[\exp \left(S_{1}\right)\right]\leq\delta.\]

In other words, with probability at least \(1-\delta\), for all \(k\geq 1\)

\[\sum_{t=1}^{k}Z_{t}\leq\log\frac{1}{\delta}.\]

Plugging in the definition of \(Z_{t}\) we have

\[\frac{1}{2}\sum_{t=1}^{k}z_{t}\eta_{t}\left\|\nabla f(x_{t}) \right\|^{2}+\sum_{t=1}^{k}\left(z_{t}\Delta_{t+1}-z_{t}\Delta_{t}\right)\] \[\leq \log\frac{1}{\delta}+\sum_{t=1}^{k}\frac{3z_{t}\eta_{t}}{2}\left\| \theta_{t}^{b}\right\|^{2}\] \[+\sum_{t=1}^{k}\left(\left(3z_{t}^{2}L\eta_{t}^{2}\Delta_{t}+6L^{ 2}z_{t}^{2}\eta_{t}^{4}\lambda_{t}^{2}+z_{t}L\eta_{t}^{2}\right)\mathbb{E} \left[\|\theta_{t}^{u}\|^{2}\mid\mathcal{F}_{t-1}\right]\right).\]

Note that we have \(z_{t}\) is a decreasing sequence by construction (see the proof of Proposition 3.7 below). Hence, the LHS of the above inequality can be bounded by

\[\text{LHS} =\frac{1}{2}\sum_{t=1}^{k}z_{t}\eta_{t}\left\|\nabla f(x_{t}) \right\|^{2}+z_{k}\Delta_{k+1}-z_{1}\Delta_{1}+\sum_{t=2}^{k}\left(z_{k-1}-z_{ k}\right)\Delta_{k}\] \[\geq\frac{1}{2}\sum_{t=1}^{k}z_{t}\eta_{t}\left\|\nabla f(x_{t}) \right\|^{2}+z_{k}\Delta_{k+1}-z_{1}\Delta_{1}.\]

We obtain the desired inequality. 

Proof of Proposition 3.7.: We will prove by induction on \(k\) that

\[\frac{1}{2}\sum_{i=1}^{k}\eta_{i}\left\|\nabla f(x_{i})\right\|^{2}+\Delta_{k+ 1}\leq\left(\sqrt{\Delta_{1}}+2\sqrt{A}C_{1}\right)^{2}.\]

The base case \(k=0\) is trivial. Suppose the statement is true for all \(t\leq k\leq\ell\). Now we show for \(k+1\). Recall that

\[z_{t}=\frac{1}{2P_{t}\eta_{t}\lambda_{t}\max_{i\leq t}\sqrt{2L\Delta_{i}}+8Q_{t }L\eta_{t}^{2}\lambda_{t}^{2}}.\]

Let us choose

\[P_{t}=\frac{C_{1}}{\lambda_{t}\eta_{t}\sqrt{2L}}\geq 1\]

[MISSING_PAGE_EMPTY:18]

\[\begin{split}\stackrel{{(b)}}{{\leq}}& \Delta_{1}+2\sqrt{\Delta_{1}}\sqrt{A}C_{1}+2C_{1}\left(\sqrt{\Delta_{1}}+4\sqrt{A }C_{1}\right)\log\frac{1}{\delta}+48\sigma^{2p}C_{2}C_{3}\\ &+80\sigma^{p}\left(\frac{3\left(\sqrt{\Delta_{1}}+2\sqrt{A}C_{1} \right)}{2C_{1}}+\frac{7}{4}\right)C_{3}\\ \leq&\Delta_{1}+2\sqrt{\Delta_{1}}\sqrt{A}C_{1}+2C_{ 1}\left(\sqrt{\Delta_{1}}+4\sqrt{A}C_{1}\right)\left(\log\frac{1}{\delta}+\frac {60\sigma^{p}C_{3}}{C_{1}^{2}}\right)\\ &+48\sigma^{2p}C_{2}C_{3}+140\sigma^{p}C_{3}\\ \stackrel{{(c)}}{{\leq}}&\Delta_{1}+2 \sqrt{\Delta_{1}}\sqrt{A}C_{1}+2C_{1}\left(\sqrt{\Delta_{1}}+4\sqrt{A}C_{1} \right)\frac{\sqrt{A}}{8}+AC_{1}^{2}\\ \leq&\left(\sqrt{\Delta_{1}}+2\sqrt{A}C_{1}\right)^{ 2}.\end{split}\]

For \((a)\), we use \(\left(\frac{1}{\lambda_{t}}\right)^{p}\leq C_{2}L\eta_{t}\) and the induction hypothesis. For \((b)\), we use \(\sum_{t=1}^{T}L\left(\frac{1}{\lambda_{t}}\right)^{p}\lambda_{t}^{2}\eta_{t}^{2} \leq C_{3}\) and \(Q_{t}\geq 1\). For \((c)\), we have

\[\log\frac{1}{\delta}+\frac{60\sigma^{p}C_{3}}{C_{1}^{2}} \leq\frac{\sqrt{A}}{8}\] \[48\sigma^{2p}C_{2}C_{3}+140\sigma^{p}C_{3} \leq AC_{1}^{2},\]

since

\[A\geq 64\left(\log\frac{1}{\delta}+\frac{60\sigma^{p}C_{3}}{C_{1}^{2}}\right)^{ 2}+\frac{48\sigma^{2p}C_{2}C_{3}+140\sigma^{p}C_{3}}{C_{1}^{2}}.\]

This concludes the proof. 

**Lemma B.1**.: _The choices of \(\eta_{t}\) and \(\lambda_{t}\) in Theorem 3.1 satisfy the condition (1)-(3) of Proposition 3.7 for_

\[C_{1} =\frac{\sqrt{\Delta_{1}}}{4\sqrt{2}\gamma},\] \[C_{2} =\frac{1}{\sigma^{p}},\] \[C_{3} =\frac{\Delta_{1}}{2048\sigma^{p}\gamma}.\]

Proof.: We verify for the first case. The second follows exactly the same. First, we have \(p>1\) hence

\[\eta_{t}\lambda_{t}\sqrt{2L}=\frac{\sqrt{\Delta_{1}}T^{\frac{1-p}{3p-2}}}{8 \sqrt{L}\gamma}\sqrt{2L}\leq\frac{\sqrt{\Delta_{1}}}{4\sqrt{2}\gamma}=C_{1}.\]

Since \(\eta_{t}=\frac{\sqrt{\Delta_{1}}T^{\frac{1-p}{3p-2}}}{8\lambda_{t}\sqrt{L}\gamma}\), \(p>1\) and \(\lambda_{t}\geq\left(\frac{8\gamma}{\sqrt{L}\Delta_{1}}\right)^{\frac{1}{p-1}}T ^{\frac{1}{3p-2}}\sigma^{\frac{p}{p-1}}\)

\[\eta_{t}\lambda_{t}^{p} =\frac{\sqrt{\Delta_{1}}T^{\frac{1-p}{3p-2}}}{8\sqrt{L}\gamma} \lambda_{t}^{p-1}\] \[\geq\frac{\sqrt{\Delta_{1}}T^{\frac{1-p}{3p-2}}}{8\sqrt{L}\gamma }\frac{8\gamma}{\sqrt{L\Delta_{1}}}T^{\frac{p-1}{3p-2}}\sigma^{p}\] \[=\frac{\sigma^{p}}{L}\]

which gives

\[\frac{1}{L\eta_{t}}\left(\frac{1}{\lambda_{t}}\right)^{p}\leq\frac{1}{\sigma^{ p}}=C_{2}.\]

[MISSING_PAGE_EMPTY:20]

**Theorem B.2**.: _Assume that \(f\) satisfies Assumption (1'), (2), (3), (4). Let \(\gamma=\max\left\{\log\frac{1}{\delta};1\right\}\) and \(\Delta_{1}=f(x_{1})-f^{*}\). For unknown \(T\), we choose \(\lambda_{t}\) and \(\eta_{t}\) such that_

\[\lambda_{t} =\max\left\{\left(\frac{8\gamma}{\sqrt{L\Delta_{1}}}\right)^{\frac {1}{p-1}}\left(2t\left(1+\log t\right)^{2}\right)^{\frac{1}{3p-2}}\sigma^{ \frac{p}{p-1}};2\sqrt{90L\Delta_{1}};32^{\frac{1}{p}}\sigma\left(2t\left(1+\log t \right)^{2}\right)^{\frac{1}{3p-2}}\right\},\] \[\eta_{t} =\frac{\sqrt{\Delta_{1}}\left(2t\left(1+\log t\right)^{2}\right)^ {\frac{1-p}{3p-2}}}{8\lambda_{t}\sqrt{L}\gamma}.\]

_Then with probability at least \(1-\delta\)_

\[\frac{1}{T}\sum_{t=1}^{T}\left\|\nabla f(x_{t})\right\|^{2} \leq 720\sqrt{\Delta_{1}L}\gamma\max\Bigg{\{}\left(\frac{8 \gamma}{\sqrt{L\Delta_{1}}}\right)^{\frac{1}{p-1}}\left(2\left(1+\log T\right) ^{2}\right)^{\frac{p}{3p-2}}\sigma^{\frac{p}{p-1}}T^{\frac{2-2p}{3p-2}};\] \[2\sqrt{90L\Delta_{1}}\left(2\left(1+\log T\right)^{2}\right)^{ \frac{p-1}{3p-2}}T^{\frac{1-2p}{3p-2}};32^{\frac{1}{p}}\sigma\left(2\left(1+ \log T\right)^{2}\right)^{\frac{p}{3p-2}}T^{\frac{2-2p}{3p-2}}\Bigg{\}}.\]

We again verify the conditions of Proposition 3.7 for the choices of \(\eta_{t}\) and \(\lambda_{t}\) in Theorem B.2.

**Lemma B.3**.: _The choices of \(\eta_{t}\) and \(\lambda_{t}\) in Theorem B.2 satisfy the condition (1)-(3) of Proposition 3.7 for_

\[C_{1} =\frac{\sqrt{\Delta_{1}}}{4\sqrt{2}\gamma},\] \[C_{2} =\frac{1}{\sigma^{p}},\] \[C_{3} =\frac{\Delta_{1}}{2048\sigma^{p}\gamma}.\]

The proof utilizes the following fact:

**Fact B.4**.: _We have \(\sum_{t=1}^{\infty}\frac{1}{2t\left(1+\log t\right)^{2}}<1\)._

Proof.: First, we have \(p>1\) hence

\[\eta_{t}\lambda_{t}\sqrt{2L} =\frac{\sqrt{\Delta_{1}}\left(2t\left(1+\log t\right)^{2}\right)^ {\frac{1-p}{3p-2}}}{8\sqrt{L}\gamma}\sqrt{2L}\] \[\leq\frac{\sqrt{\Delta_{1}}}{4\sqrt{2}\gamma}=C_{1}.\]

Since \(\eta_{t}=\frac{\sqrt{\Delta_{1}}T^{\frac{1-p}{3p-2}}}{8\lambda_{t}\sqrt{L} \gamma}\), \(p>1\) and \(\lambda_{t}\geq\left(\frac{8\gamma}{\sqrt{L\Delta_{1}}}\right)^{\frac{1}{p-1}} \left(2t\left(1+\log t\right)^{2}\right)^{\frac{1-p}{3p-2}}\sigma^{\frac{p}{p -1}}\)

\[\eta_{t}\lambda_{t}^{p} =\frac{\sqrt{\Delta_{1}}\left(2t\left(1+\log t\right)^{2}\right)^ {\frac{1-p}{3p-2}}}{8\sqrt{L}\gamma}\lambda_{t}^{p-1}\] \[\geq\frac{\sqrt{\Delta_{1}}\left(2t\left(1+\log t\right)^{2} \right)^{\frac{1-p}{3p-2}}}{8\sqrt{L}\gamma}\frac{8\gamma}{\sqrt{L\Delta_{1}} }\left(2t\left(1+\log t\right)^{2}\right)^{\frac{p-1}{3p-2}}\sigma^{p}\] \[=\frac{\sigma^{p}}{L},\]

which gives

\[\frac{1}{L\eta_{t}}\left(\frac{1}{\lambda_{t}}\right)^{p}\leq\frac{1}{\sigma^{ p}}=C_{2}.\]Finally, we have \(\lambda_{t}\geq 32^{\frac{1}{p}}\sigma\left(2t\left(1+\log t\right)^{2}\right)^{ \frac{1-p}{3p-2}}\), hence

\[\left(\frac{1}{\lambda_{t}}\right)^{p}\left(2t\left(1+\log t\right)^{2}\right)^ {\frac{p}{3p-2}}\leq\frac{1}{32\sigma^{p}}. \tag{13}\]

Therefore,

\[\sum_{t=1}^{T}L\left(\frac{1}{\lambda_{t}}\right)^{p}\lambda_{t}^ {2}\eta_{t}^{2} =\sum_{t=1}^{T}L\left(\frac{1}{\lambda_{t}}\right)^{p}\left(2t \left(1+\log t\right)^{2}\right)^{\frac{2-2p}{3p-2}}\left(\frac{\sqrt{\Delta_{1 }}}{8\sqrt{L}\gamma}\right)^{2}\] \[=\sum_{t=1}^{T}L\frac{1}{2t\left(1+\log t\right)^{2}}\left(\frac{ 1}{\lambda_{t}}\right)^{p}\left(2t\left(1+\log t\right)^{2}\right)^{\frac{p}{3 p-2}}\frac{\Delta_{1}}{64\gamma^{2}}\] \[\leq\sum_{t=1}^{T}L\frac{1}{2t\left(1+\log t\right)^{2}}\frac{1} {32\sigma^{p}}\frac{\Delta_{1}}{64\gamma^{2}}\] (by ( 13 )) \[=\frac{1}{32\sigma^{p}}\frac{\Delta_{1}}{64\gamma^{2}}\sum_{t=1} ^{T}\frac{1}{2t\left(1+\log t\right)^{2}}\] \[\leq\frac{1}{32\sigma^{p}}\frac{\Delta_{1}}{64\gamma^{2}}\leq \frac{\Delta_{1}}{2048\sigma^{p}\gamma}.\] (by Fact B.4 )

Proof of Theorem b.2.: Note that

\[\eta_{t} =\frac{\sqrt{\Delta_{1}}\left(2t\left(1+\log t\right)^{2}\right) ^{\frac{1-p}{3p-2}}}{8\lambda_{t}\sqrt{L}\gamma}\] \[\leq\frac{\left(2t\left(1+\log t\right)^{2}\right)^{\frac{1-p}{3 p-2}}}{16L\gamma\sqrt{90}}\] \[\leq\frac{1}{L}.\]

Note that with Lemma B.3, verifying the conditions of Proposition 3.7 is identical to the proof of theorem 3.1. We have that with probability at least \(1-\delta\), event \(E(\delta)\) from 3.7 happens. We have with probability at least \(1-\delta\):

\[\frac{1}{2}\sum_{t=1}^{T}\eta_{t}\left\|\nabla f(x_{t})\right\|^{2}+\Delta_{k+ 1}\leq 45\Delta_{1}.\]

Since \(\eta_{t}\) is decreasing, we have

\[\frac{1}{T}\sum_{t=1}^{T}\left\|\nabla f(x_{t})\right\|^{2}\leq\frac{90\Delta_ {1}}{T\eta_{T}}.\]

This means that

\[\frac{1}{T}\sum_{t=1}^{T}\left\|\nabla f(x_{t})\right\|^{2} \leq 720\sqrt{\Delta_{1}L}\gamma\max\Bigg{\{}\left(\frac{8\gamma}{ \sqrt{L\Delta_{1}}}\right)^{\frac{1}{p-1}}\left(2\left(1+\log T\right)^{2} \right)^{\frac{p}{3p-2}}\sigma^{\frac{p}{p-1}}T^{\frac{2-2p}{3p-2}};\] \[2\sqrt{90L\Delta_{1}}\left(2\left(1+\log T\right)^{2}\right)^{ \frac{p-1}{3p-2}}T^{\frac{1-2p}{3p-2}};32^{\frac{1}{2}}\sigma\left(2\left(1+ \log T\right)^{2}\right)^{\frac{p}{3p-2}}T^{\frac{2-2p}{3p-2}}\Bigg{\}}.\]Missing Proofs from Section 4

**Lemma C.1**.: _Suppose that \(\eta_{t}\leq\frac{1}{4L}\) and assume \(f\) satisfies Assumption (1), (2), (3) as well as the following condition_

\[f(y)-f(x)\leq\left\langle\nabla f(x),y-x\right\rangle+G\left\|y-x\right\|+\frac{ L}{2}\left\|y-x\right\|^{2},\quad\forall y,x\in\mathcal{X}. \tag{14}\]

_Then the iterate sequence \((x_{t})_{t\geq 1}\) output by Algorithm 2 satisfies the following:_

\[\eta_{t}\Delta_{t+1} \leq\mathbf{D}_{\psi}\left(x^{*},x_{t}\right)-\mathbf{D}_{\psi} \left(x^{*},x_{t+1}\right)+\eta_{t}\left\langle x^{*}-x_{t},\theta_{t}^{u} \right\rangle+\eta_{t}\left\langle x^{*}-x_{t},\theta_{t}^{b}\right\rangle\] \[\quad+2\eta_{t}^{2}\left(\left\|\theta_{t}^{u}\right\|_{*}^{2}- \mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid\mathcal{F}_{t-1} \right]\right)+2\eta_{t}^{2}\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^ {2}\mid\mathcal{F}_{t-1}\right]+2\eta_{t}^{2}\left\|\theta_{t}^{b}\right\|_{*} ^{2}+2G^{2}\eta_{t}^{2}.\]

Proof.: By condition (14) and convexity,

\[f\left(x_{t+1}\right)-f\left(x^{*}\right) \leq\underbrace{f\left(x_{t+1}\right)-f\left(x_{t}\right)}_{ \text{condition (\ref{eq:f})}}+\underbrace{f\left(x_{t}\right)-f\left(x^{*} \right)}_{\text{convexity}}\] \[\leq\left\langle\nabla f\left(x_{t}\right),x_{t+1}-x_{t}\right\rangle +\frac{L}{2}\left\|x_{t}-x_{t+1}\right\|^{2}+G\left\|x_{t}-x_{t+1}\right\|+ \left\langle\nabla f\left(x_{t}\right),x_{t}-x^{*}\right\rangle\] \[=\left\langle\nabla f\left(x_{t}\right),x_{t+1}-x^{*}\right\rangle +\frac{L}{2}\left\|x_{t}-x_{t+1}\right\|^{2}+G\left\|x_{t}-x_{t+1}\right\|\] \[=\left\langle\theta_{t},x^{*}-x_{t+1}\right\rangle+\left\langle \widetilde{\nabla}f(x_{t}),x_{t+1}-x^{*}\right\rangle+\frac{L}{2}\left\|x_{t}- x_{t+1}\right\|^{2}+G\left\|x_{t}-x_{t+1}\right\|.\]

By the optimality condition, we have

\[\left\langle\eta_{t}\widetilde{\nabla}f(x_{t})+\nabla_{x}\mathbf{D}_{\psi} \left(x_{t+1},x_{t}\right),x^{*}-x_{t+1}\right\rangle\geq 0\]

and thus

\[\left\langle\eta_{t}\widetilde{\nabla}f(x_{t}),x_{t+1}-x^{*}\right\rangle\leq \left\langle\nabla_{x}\mathbf{D}_{\psi}\left(x_{t+1},x_{t}\right),x^{*}-x_{t+ 1}\right\rangle.\]

Note that

\[\left\langle\nabla_{x}\mathbf{D}_{\psi}\left(x_{t+1},x_{t}\right), x^{*}-x_{t+1}\right\rangle =\left\langle\nabla\psi\left(x_{t+1}\right)-\nabla\psi\left(x_{t} \right),x^{*}-x_{t+1}\right\rangle\] \[=\mathbf{D}_{\psi}\left(x^{*},x_{t}\right)-\mathbf{D}_{\psi} \left(x_{t+1},x_{t}\right)-\mathbf{D}_{\psi}\left(x^{*},x_{t+1}\right).\]

Thus

\[\eta_{t}\left\langle\widetilde{\nabla}f(x_{t}),x_{t+1}-x^{*}\right\rangle \leq\mathbf{D}_{\psi}\left(x^{*},x_{t}\right)-\mathbf{D}_{\psi} \left(x^{*},x_{t+1}\right)-\mathbf{D}_{\psi}\left(x_{t+1},x_{t}\right)\] \[\leq\mathbf{D}_{\psi}\left(x^{*},x_{t}\right)-\mathbf{D}_{\psi} \left(x^{*},x_{t+1}\right)-\frac{1}{2}\left\|x_{t+1}-x_{t}\right\|^{2},\]

where we have used that \(\mathbf{D}_{\psi}\left(x_{t+1},x_{t}\right)\geq\frac{1}{2}\left\|x_{t+1}-x_{t} \right\|^{2}\) by the strong convexity of \(\psi\).

Combining the two inequalities, and using the assumption that \(L\eta_{t}\leq\frac{1}{4}\), we obtain

\[\eta_{t}\Delta_{t+1}+\mathbf{D}_{\psi}\left(x^{*},x_{t+1}\right)- \mathbf{D}_{\psi}\left(x^{*},x_{t}\right)\] \[\leq\eta_{t}\left\langle\theta_{t},x^{*}-x_{t+1}\right\rangle+ \frac{L\eta_{t}}{2}\left\|x_{t}-x_{t+1}\right\|^{2}+G\eta_{t}\left\|x_{t}-x_{t+ 1}\right\|-\frac{1}{2}\left\|x_{t+1}-x_{t}\right\|^{2}\] \[\leq\eta_{t}\left\langle\theta_{t},x^{*}-x_{t}\right\rangle+\eta _{t}\left\langle\theta_{t},x_{t}-x_{t+1}\right\rangle-\frac{3}{8}\left\|x_{t+ 1}-x_{t}\right\|^{2}+G\eta_{t}\left\|x_{t}-x_{t+1}\right\|\] \[\leq\eta_{t}\left\langle\theta_{t},x^{*}-x_{t}\right\rangle+\eta _{t}^{2}\left\|\theta_{t}\right\|_{*}^{2}+2G^{2}\eta_{t}^{2}\] \[\leq\eta_{t}\left\langle\theta_{t}^{u}+\theta_{t}^{b},x^{*}-x_{t }\right\rangle+2\eta_{t}^{2}\left\|\theta_{t}^{u}\right\|_{*}^{2}+2\eta_{t}^{2 }\left\|\theta_{t}^{b}\right\|_{*}^{2}+2G^{2}\eta_{t}^{2}.\]

This is what we want to show. 

Proof of Lemma 4.7.: We have \[\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t-1}\right] \times\exp\left(\left(\frac{3}{8\lambda_{t}^{2}}+24z_{t}^{2}\eta_{t}^{4}\lambda_ {t}^{2}\right)\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid \mathcal{F}_{t-1}\right]\right)\] \[\stackrel{{(a)}}{{\leq}} \mathbb{E}\left[\exp\left(z_{t}\left(\eta_{t}\left\langle x^{*}-x_ {t},\theta_{t}^{u}\right\rangle+2\eta_{t}^{2}\left(\left\|\theta_{t}^{u}\right\| _{*}^{2}-\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid\mathcal{F}_ {t-1}\right]\right)\right)\right)\mid\mathcal{F}_{t-1}\right]\] \[\stackrel{{(b)}}{{\leq}} \exp\left(\mathbb{E}\left[\frac{3}{4}\left(z_{t}\left(\eta_{t}\left\langle x ^{*}-x_{t},\theta_{t}^{u}\right\rangle+2\eta_{t}^{2}\left(\left\|\theta_{t}^{u }\right\|_{*}^{2}-\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid \mathcal{F}_{t-1}\right]\right)\right)\right)^{2}\mid\mathcal{F}_{t-1}\right]\right)\] \[\stackrel{{(c)}}{{\leq}} \exp\left(\left(\frac{3}{2}z_{t}^{2}\eta_{t}^{2}\left\|x^{*}-x_{t} \right\|^{2}\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid\mathcal{ F}_{t-1}\right]+6z_{t}^{2}\eta_{t}^{4}\mathbb{E}\left[\left\|\theta_{t}^{u} \right\|_{*}^{4}\mid\mathcal{F}_{t-1}\right]\right)\right)\] \[\stackrel{{(d)}}{{\leq}} \exp\left(\left(\frac{3}{2}z_{t}^{2}\eta_{t}^{2}\left\|x^{*}-x_{t}\right\|^ {2}+24z_{t}^{2}\eta_{t}^{4}\lambda_{t}^{2}\right)\mathbb{E}\left[\left\|\theta _{t}^{u}\right\|_{*}^{2}\mid\mathcal{F}_{t-1}\right]\right)\] \[\stackrel{{(e)}}{{\leq}} \exp\left(\left(\frac{3}{8\lambda_{t}^{2}}+24z_{t}^{2}\eta_{t}^{4}\lambda_ {t}^{2}\right)\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid \mathcal{F}_{t-1}\right]\right).\]

For \((a)\), we use Lemma 4.5. For \((b)\), we use Lemma 2.2. Notice that

\[\mathbb{E}\left[\left\langle x^{*}-x_{t},\theta_{t}^{u}\right\rangle\right]= \mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}-\mathbb{E}\left[\left\| \theta_{t}^{u}\right\|_{*}^{2}\mid\mathcal{F}_{t-1}\right]\right]=0,\]

and since \(\left\|\theta_{t}^{u}\right\|_{*}\leq 2\lambda_{t}\), we have

\[\left|\eta_{t}\left\langle x^{*}-x_{t},\theta_{t}^{u}\right\rangle +2\eta_{t}^{2}\left(\left\|\theta_{t}^{u}\right\|_{*}^{2}-\mathbb{E}\left[ \left\|\theta_{t}^{u}\right\|_{*}^{2}\mid\mathcal{F}_{t-1}\right]\right)\right|\] \[\leq\eta_{t}\left\|x^{*}-x_{t}\right\|\left\|\theta_{t}^{u} \right\|_{*}+2\eta_{t}^{2}\left(\left\|\theta_{t}^{u}\right\|_{*}^{2}+\mathbb{E }\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid\mathcal{F}_{t-1}\right]\right)\] \[\leq 2\eta_{t}\lambda_{t}\left\|x^{*}-x_{t}\right\|+16\eta_{t}^{2 }\lambda_{t}^{2}\] \[\leq 2\eta_{t}\lambda_{t}\sqrt{2\mathbf{D}_{\psi}\left(x^{*},x_{t} \right)}+16\eta_{t}^{2}\lambda_{t}^{2}.\]

Thus, \(z_{t}\leq\frac{1}{2\eta_{t}\lambda_{t}\sqrt{2\mathbf{D}_{\psi}\left(x^{*},x_{t }\right)}+16\eta_{t}^{2}\lambda_{t}^{2}}\). For \((c)\), we use the inequalities \((a+b)^{2}\leq 2a^{2}+2b^{2}\) and \(\mathbb{E}\left[\left(X-\mathbb{E}\left[X\right]\right)^{2}\right]\leq \mathbb{E}\left[X^{2}\right]\). For \((d)\), we use the fact \(\left\|\theta_{t}^{u}\right\|_{*}^{2}\leq 4\lambda_{t}^{2}\) to get \(\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{4}\mid\mathcal{F}_{t-1} \right]\leq 4\lambda_{t}^{2}\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid \mathcal{F}_{t-1}\right]\). For \((e)\), we use the fact that \(\left\|\theta_{t}^{u}\right\|_{*}\leq 2\lambda_{t}\) and

\[z_{t}\eta_{t}\left\|x^{*}-x_{t}\right\|\leq\frac{\eta_{t}\left\|x^{*}-x_{t} \right\|}{2\eta_{t}\lambda_{t}\sqrt{2\mathbf{D}_{\psi}\left(x^{*},x_{t}\right) }}\leq\frac{1}{2\lambda_{t}}.\]

We obtain \(\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t-1}\right]\leq 1\). Therefore

\[\mathbb{E}\left[\exp\left(S_{t}\right)\mid\mathcal{F}_{t-1}\right]=\exp\left(S _{t-1}\right)\mathbb{E}\left[\exp\left(Z_{t}\right)\mid\mathcal{F}_{t-1} \right]\leq\exp\left(S_{t-1}\right).\]

which means \((\exp\left(S_{t}\right))_{t\geq 1}\) is a supermartingale. By Ville's inequality, we have, for all \(k\geq 1\)

\[\Pr\left[S_{k}\geq\log\frac{1}{\delta}\right]\leq\delta\mathbb{E}\left[\exp \left(S_{1}\right)\right]\leq\delta.\]

In other words, with probability at least \(1-\delta\), for all \(k\geq 1\)

\[\sum_{t=1}^{k}Z_{t}\leq\log\frac{1}{\delta}.\]

Plugging in the definition of \(Z_{t}\) we have

\[\sum_{t=1}^{k}z_{t}\eta_{t}\Delta_{t+1}+\sum_{t=1}^{k}\left(z_{t} \mathbf{D}_{\psi}\left(x^{*},x_{t+1}\right)-z_{t}\mathbf{D}_{\psi}\left(x^{*},x_ {t}\right)\right)\] \[\leq \log\frac{1}{\delta}+\sum_{t=1}^{k}z_{t}\eta_{t}\left\langle x^ {*}-x_{t},\theta_{t}^{b}\right\rangle+2\sum_{t=1}^{k}z_{t}\eta_{t}^{2}\left\| \theta_{t}^{b}\right\|_{*}^{2}\]

[MISSING_PAGE_EMPTY:25]

\[\left\|\nabla f(x_{k+1})\right\|_{*}\leq\frac{\lambda_{k+1}}{2}.\]

By 4.8 we have

\[\left\|x_{k+1}-x^{*}\right\|\leq\sqrt{2\mathbf{D}_{\psi}\left(x^{*},x_{k+1} \right)}\leq R_{1}+8AC_{1}=2R_{1}.\]

[MISSING_PAGE_EMPTY:27]

Therefore, from Lemma 4.7, we have

\[\eta_{T}\sum_{t=1}^{T}\Delta_{t+1}+\mathbf{D}_{\psi}\left(x^{*},x_{T +1}\right) \leq\frac{1}{2}\left(R_{1}+8AC_{1}\right)^{2}\] \[=\frac{1}{2}\left(R_{1}+\frac{c_{1}}{3}\left(\gamma+\frac{2\sigma ^{p}}{c_{2}}\right)\right)^{2}\]

which gives

\[\frac{1}{T}\sum_{t=2}^{T+1}\Delta_{t} \leq\frac{1}{2T\eta_{T}}\left(R_{1}+\frac{c_{1}}{3}\left(\gamma+ \frac{2\sigma^{p}}{c_{2}}\right)\right)^{2}\] \[=\frac{8}{Tc_{1}}\left(R_{1}+\frac{c_{1}}{3}\left(\gamma+\frac{2 \sigma^{p}}{c_{2}}\right)\right)^{2}\max\left\{\left(52T(1+\log T)^{2}c_{2} \right)^{1/p};2\left(L\max_{i\leq T}\|x_{i}-x_{1}\|+\nabla_{1}\right);\frac{L}{ 8}\right\}.\]

Note that

\[\|x_{i}-x_{1}\| \leq\|x_{i}-x^{*}\|+\|x_{1}-x^{*}\|\] \[\leq 2R_{1}+\frac{c_{1}}{3}\left(\gamma+\frac{2\sigma^{p}}{c_{2} }\right)\]

which gives us the final convergence rate. 

## Appendix D Clipped Accelerated Stochastic Mirror Descent

In this section, we extend the analysis of Clipped-SMD to the case of Clipped Accelerated Stochastic Mirror Descent (Algorithm 3). We will see that the analysis is basically the same with little modification. We present in Algorithm 3 the clipped version of accelerated stochastic mirror descent (see [15]), where the clipped gradient \(\widehat{\nabla}f(x_{t})\) is used to update the iterates in place of the stochastic gradient \(\widehat{\nabla}f(x_{t})\).

We use the following additional assumption:

**(5') Global minimizer**: We assume that \(\nabla f(x^{*})=0\).

**Theorem D.1**.: _Assume that \(f\) satisfies Assumption (1), (2), (3), (4) and (5'). Let \(\gamma=\max\left\{\log\frac{1}{\delta};1\right\}\); and \(R_{1}=\sqrt{2\mathbf{D}_{\psi}\left(x^{*},x_{1}\right)}\)._

_1. For known_ \(T\)_, we choose a constant_ \(c\) _and_ \(\lambda_{t}\) _and_ \(\eta_{t}\) _such that_

\[c =\max\left\{10^{4};\frac{4\left(T+1\right)\left(\frac{26T}{\gamma }\right)^{\frac{1}{p}}\sigma}{\gamma LR_{1}}\right\},\] \[\lambda_{t} =\frac{cR_{1}\gamma L\alpha_{t}}{8}=\max\left\{\frac{10^{4}R_{1} \gamma L}{6(t+1)};\frac{T+1}{t+1}\left(\frac{26T}{\gamma}\right)^{1/p}\sigma \right\},\] \[\eta_{t} =\frac{1}{3c\gamma^{2}L\alpha_{t}}=\frac{R_{1}}{24\gamma}\min \left\{\frac{4(t+1)}{10^{4}R_{1}\gamma L};\frac{t+1}{T+1}\left(\frac{26T}{ \gamma}\right)^{-1/p}\sigma^{-1}\right\}.\]

_Then with probability at least \(1-\delta\)_

\[f\left(y_{T+1}\right)-f\left(x^{*}\right)\leq 6\max\left\{10^{4}L\gamma^{2}R_{1} ^{2}(T+1)^{-2};4R_{1}\left(T+1\right)^{-1}\left(26T\right)^{\frac{1}{p}} \gamma^{\frac{p-1}{p}}\sigma\right\}.\]

_2. For unknown_ \(T\)_, we choose_ \(c_{t}\)_,_ \(\lambda_{t}\) _and_ \(\eta_{t}\) _such that_\[\lambda_{t}=\frac{c_{t}R_{1}\gamma L\alpha_{t}}{8}=\max\left\{\frac{10^{4}R_{1} \gamma L}{4(t+1)};\left(\frac{52t\left(1+\log t\right)^{2}}{\gamma}\right)^{1/p }\sigma\right\},\]

\[\eta_{t}=\frac{1}{3c_{t}\gamma^{2}L\alpha_{t}}=\frac{R_{1}}{24\gamma}\min\left\{ \frac{4(t+1)}{10^{4}R_{1}\gamma L};\left(\frac{52t\left(1+\log t\right)^{2}}{ \gamma}\right)^{-1/p}\sigma^{-1}\right\}.\]

_Then with probability at least \(1-\delta\)_

\[f\left(y_{T+1}\right)-f\left(x^{*}\right)\leq 6\max\left\{10^{4}L\gamma^{2}R_{1}^{2 }(T+1)^{-2};4R_{1}\left(T+1\right)^{-1}\left(52T\left(1+\log T\right)^{2} \right)^{\frac{1}{p}}\gamma^{\frac{p-1}{p}}\sigma\right\}.\]

_Remark D.2_.: One feature of the accelerated algorithm is the interpolation between the two regimes: When \(\sigma\) is large, the algorithm achieves the \(O\left(T^{\frac{1-p}{p}}\right)\) convergence rate, which is the same as unaccelerated algorithms; however, when \(\sigma\) is sufficiently small, the algorithm achieves the accelerated \(O\left(T^{-2}\right)\) rate.

We also start the analysis of accelerated stochastic mirror descent with the following lemma.

**Lemma D.3**.: _Assume that \(f\) satisfies Assumption (1), (2), (3), (4) and \(\eta_{t}\leq\frac{1}{2L\alpha_{t}}\), the iterate sequence \((x_{t})_{t\geq 1}\) output by Algorithm 2 satisfies the following_

\[\frac{\eta_{t}}{\alpha_{t}}\left(f\left(y_{t+1}\right)-f\left(x^{ *}\right)\right)-\frac{\eta_{t}\left(1-\alpha_{t}\right)}{\alpha_{t}}\left(f \left(y_{t}\right)-f\left(x^{*}\right)\right)+\mathbf{D}_{\psi}\left(x^{*},z_{ t+1}\right)-\mathbf{D}_{\psi}\left(x^{*},z_{t}\right)\] \[\leq \eta_{t}\left(\theta_{t}^{u},x^{*}-z_{t}\right)+\eta_{t}\left\langle \theta_{t}^{b},x^{*}-z_{t}\right\rangle+2\eta_{t}^{2}\left(\left\|\theta_{t}^{ u}\right\|_{*}^{2}-\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid \mathcal{F}_{t-1}\right]\right)+2\eta_{t}^{2}\left\|\theta_{t}^{b}\right\|_{* }^{2}+2\eta_{t}^{2}\mathbb{E}\left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid \mathcal{F}_{t-1}\right].\]

Proof of Lemma d.3.: We have

\[f\left(y_{t+1}\right)-f\left(x^{*}\right) =\underbrace{f\left(y_{t+1}\right)-f\left(x_{t}\right)}_{\text{ smoothness}}+\underbrace{f\left(x_{t}\right)-f\left(x^{*}\right)}_{\text{ convexity}}\] \[\leq\left\langle\nabla f\left(x_{t}\right),y_{t+1}-x_{t}\right\rangle +\frac{L}{2}\left\|y_{t+1}-x_{t}\right\|^{2}\] \[\quad+\alpha_{t}\left\langle\nabla f\left(x_{t}\right),x_{t}-x^{ *}\right\rangle+\left(1-\alpha_{t}\right)\left(f\left(x_{t}\right)-f\left(x^{ *}\right)\right)\] \[=\underbrace{\left(1-\alpha_{t}\right)\left\langle\nabla f \left(x_{t}\right),y_{t}-x_{t}\right\rangle}_{\text{convexity}}+\alpha_{t} \left\langle\nabla f\left(x_{t}\right),z_{t+1}-x^{*}\right\rangle\] \[\quad+\frac{L\alpha_{t}^{2}}{2}\left\|z_{t+1}-z_{t}\right\|^{2}+ \left(1-\alpha_{t}\right)\left(f\left(x_{t}\right)-f\left(x^{*}\right)\right)\] \[\leq\left(1-\alpha_{t}\right)\left(f\left(y_{t}\right)-f\left(x_ {t}\right)\right)+\left(1-\alpha_{t}\right)\left(f\left(x_{t}\right)-f\left(x^ {*}\right)\right)\] \[\quad+\alpha_{t}\left\langle\widetilde{\nabla}f(x_{t}),z_{t+1}-x ^{*}\right\rangle+\frac{L\alpha_{t}^{2}}{2}\left\|z_{t+1}-z_{t}\right\|^{2}.\]

By the optimality condition, we have

\[\left\langle\eta_{t}\widetilde{\nabla}f(x_{t})+\nabla_{x}\mathbf{D}_{\psi} \left(z_{t+1},z_{t}\right),x^{*}-z_{t+1}\right\rangle\geq 0\]

and thus

\[\left\langle\eta_{t}\widetilde{\nabla}f(x_{t}),z_{t+1}-x^{*}\right\rangle\leq \left\langle\nabla_{x}\mathbf{D}_{\psi}\left(z_{t+1},z_{t}\right),x^{*}-z_{t+1 }\right\rangle.\]

Note that

\[\left\langle\nabla_{x}\mathbf{D}_{\psi}\left(z_{t+1},z_{t}\right),x^{*}-z_{t+1 }\right\rangle =\left\langle\nabla\psi\left(z_{t+1}\right)-\nabla\psi\left(z_{t} \right),x^{*}-z_{t+1}\right\rangle\] \[=\mathbf{D}_{\psi}\left(x^{*},z_{t}\right)-\mathbf{D}_{\psi} \left(z_{t+1},z_{t}\right)-\mathbf{D}_{\psi}\left(x^{*},z_{t+1}\right).\]Thus

\[\eta_{t}\left\langle\widetilde{\nabla}f(x_{t}),z_{t+1}-x^{*}\right\rangle \leq\mathbf{D}_{\psi}\left(x^{*},z_{t}\right)-\mathbf{D}_{\psi} \left(x^{*},z_{t+1}\right)-\mathbf{D}_{\psi}\left(z_{t+1},z_{t}\right)\] \[\leq\mathbf{D}_{\psi}\left(x^{*},z_{t}\right)-\mathbf{D}_{\psi} \left(x^{*},z_{t+1}\right)-\frac{1}{2}\left\|z_{t+1}-z_{t}\right\|^{2}\]

where we have used that \(\mathbf{D}_{\psi}\left(z_{t+1},z_{t}\right)\geq\frac{1}{2}\left\|z_{t+1}-z_{t} \right\|^{2}\) by the strong convexity of \(\psi\). We have

\[f\left(y_{t+1}\right)-f\left(x^{*}\right) \leq\left(1-\alpha_{t}\right)\left(f\left(y_{t}\right)-f\left(x^{ *}\right)\right)+\alpha_{t}\left\langle\theta_{t},x^{*}-z_{t+1}\right\rangle\] \[+\frac{\alpha_{t}}{\eta_{t}}\mathbf{D}_{\psi}\left(x^{*},z_{t} \right)-\frac{\alpha_{t}}{\eta_{t}}\mathbf{D}_{\psi}\left(x^{*},z_{t+1}\right) +\left(\frac{L\alpha_{t}^{2}}{2}-\frac{\alpha_{t}}{2\eta_{t}}\right)\left\|z_ {t+1}-z_{t}\right\|^{2}.\]

Dividing both sides by \(\frac{\alpha_{t}}{\eta_{t}}\) and using the condition \(L\eta_{t}\alpha_{t}\leq\frac{1}{2}\), we have

\[\frac{\eta_{t}}{\alpha_{t}}\left(f\left(y_{t+1}\right)-f\left(x^{ *}\right)\right)+\mathbf{D}_{\psi}\left(x^{*},z_{t+1}\right)-\mathbf{D}_{\psi }\left(x^{*},z_{t}\right)\] \[\leq \frac{\eta_{t}\left(1-\alpha_{t}\right)}{\alpha_{t}}\left(f\left( y_{t}\right)-f\left(x^{*}\right)\right)+\eta_{t}\left\langle\theta_{t},x^{*}-z_{t}\right\rangle\] \[\quad+\eta_{t}\left\langle\theta_{t},z_{t}-z_{t+1}\right\rangle- \frac{1-L\eta_{t}\alpha_{t}}{2}\left\|z_{t+1}-z_{t}\right\|^{2}\] \[\leq \frac{\eta_{t}\left(1-\alpha_{t}\right)}{\alpha_{t}}\left(f\left( y_{t}\right)-f\left(x^{*}\right)\right)+\eta_{t}\left\langle\theta_{t},x^{*}-z_{t}\right\rangle\] \[\quad+\frac{\eta_{t}^{2}\left\|\theta_{t}\right\|_{*}^{2}}{2 \left(1-L\eta_{t}\alpha_{t}\right)}\] \[\leq \frac{\eta_{t}\left(1-\alpha_{t}\right)}{\alpha_{t}}\left(f\left( y_{t}\right)-f\left(x^{*}\right)\right)+\eta_{t}\left\langle\theta_{t}^{u}+\theta_{t}^{b},x^{* }-z_{t}\right\rangle\] \[\quad+2\eta_{t}^{2}\left\|\theta_{t}^{u}\right\|_{*}^{2}+2\eta_{ t}^{2}\left\|\theta_{t}^{b}\right\|_{*}^{2}\]

as needed. 

Similarly to the previous section, we define the following variables

\[Z_{t} =z_{t}\Bigg{(}\frac{\eta_{t}}{\alpha_{t}}\left(f\left(y_{t+1} \right)-f\left(x^{*}\right)\right)-\frac{\eta_{t}\left(1-\alpha_{t}\right)}{ \alpha_{t}}\left(f\left(y_{t}\right)-f\left(x^{*}\right)\right)+\mathbf{D}_{ \psi}\left(x^{*},z_{t+1}\right)-\mathbf{D}_{\psi}\left(x^{*},z_{t}\right)\] \[\quad-\eta_{t}\left\langle\theta_{t}^{b},x^{*}-z_{t}\right\rangle -2\eta_{t}^{2}\left\|\theta_{t}^{b}\right\|_{*}^{2}-2\eta_{t}^{2}\mathbb{E} \left[\left\|\theta_{t}^{u}\right\|_{*}^{2}\mid\mathcal{F}_{t-1}\right]\Bigg{)} -\left(\frac{3}{8\lambda_{t}^{2}}+24z_{t}^{2}\eta_{t}^{4}\lambda_{t}^{2}\right) \mathbb{E}\left[\left\|\theta_{t}^{u}\right\|^{2}\mid\mathcal{F}_{t-1}\right],\]

where \(z_{t}=\frac{1}{2\eta_{t}\lambda_{t}\max_{i\leq t}\sqrt{2\mathbf{D}_{\psi} \left(x^{*},x_{i}\right)+16Q\eta_{t}^{2}\lambda_{t}^{2}}}\)

for a constant \(Q\geq 1\). We also let \(S_{t}=\sum_{i=1}^{t}Z_{i}\). Following the same analysis as in previous sections, we can obtain Lemma D.4 and Proposition D.5, for which we will omit the proofs here. The only step we need to pay attention to when showing Lemma D.4 is when we bound the sum

\[\sum_{t=1}^{k}\frac{z_{t}\eta_{t}}{\alpha_{t}}\left(f\left(y_{t+1}\right)-f \left(x^{*}\right)\right)-\frac{z_{t}\eta_{t}\left(1-\alpha_{t}\right)}{\alpha _{t}}\left(f\left(y_{t}\right)-f\left(x^{*}\right)\right).\]

If we assume \(\frac{\eta_{t-1}}{\alpha_{t-1}}\geq\frac{\eta_{t}\left(1-\alpha_{t}\right)}{ \alpha_{t}}\), since \(z_{t}\) is a decreasing sequence and \(\alpha_{1}=0\), we can lower bound the above sum by the last term \(\frac{z_{t}\eta_{t}}{\alpha_{t}}\left(f\left(y_{t+1}\right)-f\left(x^{*}\right)\right)\), which gives us the desired inequality.

**Lemma D.4**.: _Assume that for all \(t\geq 1\), \(\eta_{t}\) satisfies \(\frac{\eta_{t-1}}{\alpha_{t-1}}\geq\frac{\eta_{t}\left(1-\alpha_{t}\right)}{ \alpha_{t}}\). For any \(\delta>0\), let \(E(\delta)\) be the event that for all \(1\leq k\leq T\)_

\[\frac{z_{k}\eta_{k}}{\alpha_{k}}\left(f\left(y_{k+1}\right)-f\left(x^{*}\right) \right)+z_{k}\mathbf{D}_{\psi}\left(x^{*},x_{k+1}\right)\]\[\leq z_{1}\mathbf{D}_{\psi}\left(x^{*},x_{1}\right)+\log\frac{1}{\delta}+\sum _{t=1}^{k}z_{t}\eta_{t}\left\langle x^{*}-x_{t},\theta_{t}^{b}\right\rangle+2 \sum_{t=1}^{k}z_{t}\eta_{t}^{2}\left\|\theta_{t}^{b}\right\|_{*}^{2}\] \[+\sum_{t=1}^{k}\left(\left(2z_{t}\eta_{t}^{2}+\frac{3}{8\lambda_{t }^{2}}+24z_{t}^{2}\eta_{t}^{4}\lambda_{t}^{2}\right)\mathbb{E}\left[\left\| \theta_{t}^{u}\right\|_{*}^{2}\mid\mathcal{F}_{t-1}\right]\right).\]

_Then \(\Pr\left[E(\delta)\right]\geq 1-\delta\)._

Finally, we state a general condition for the choice of \(\eta_{t}\) and \(\lambda_{t}\), which follows exactly the same as in Proposition 4.8. The proof for Theorem D.1 is a direct consequence of this.

**Proposition D.5**.: _We assume that the event \(E(\delta)\) from Lemma D.4 happens. Suppose that for some \(\ell\leq T\), there are constants \(C_{1}\) and \(C_{2}\) such that for all \(t\leq\ell\)_

1. \(\lambda_{t}\eta_{t}=C_{1}\)_;_ 2. \(\sum_{t=1}^{\ell}\left(\frac{1}{\lambda_{t}}\right)^{p}\leq C_{2}\)_;_ 3. \(\left(\frac{1}{\lambda_{t}}\right)^{2p}\leq C_{3}\left(\frac{1}{\lambda_{t}} \right)^{p}\)_;_ 4. \(\left\|\nabla f(x_{t})\right\|_{*}\leq\frac{\lambda_{t}}{2}\)_._

_Then for all \(t\leq\ell+1\)_

\[\frac{\eta_{t}}{\alpha_{t}}\left(f\left(y_{t+1}\right)-f\left(x^{*}\right) \right)+\mathbf{D}_{\psi}\left(x^{*},z_{t+1}\right)\leq\frac{1}{2}\left(R_{1} +8AC_{1}\right)^{2}\]

_for \(A\geq\max\left\{\log\frac{1}{\delta}+26\sigma^{p}C_{2}+\frac{2\sigma^{2p}C_{2} C_{3}}{A};1\right\}.\)_

Proof of Theorem D.1.: 1. Note that \(\eta_{t}\leq\frac{1}{2c\gamma^{2}L\alpha_{t}}\leq\frac{1}{2L\alpha_{t}}\) and

\[\frac{\eta_{t-1}}{\alpha_{t-1}} =\frac{t^{2}}{8c\gamma^{2}L}\] \[\frac{\eta_{t}\left(1-\alpha_{t}\right)}{\alpha_{t}} =\frac{(t+1)(t-1)}{8c\gamma^{2}L}\]

thus \(\frac{\eta_{t-1}}{\alpha_{t-1}}\geq\frac{\eta_{t}\left(1-\alpha_{t}\right)}{ \alpha_{t}}\). We have that with probability at least \(1-\delta\), event \(E(\delta)\) happens. Conditioning on this event, in 4.8 We choose

\[C_{1}=\frac{R_{1}}{24\gamma};\quad C_{2}=\frac{\gamma}{26\sigma^{p}};\quad C_{ 3}=\frac{\gamma}{26T\sigma^{p}};\quad A=3\gamma.\]

We can verify the conditions of Proposition D.5 similarly as in previous section for these choices of \(C_{1},C_{2}\), and \(C_{3}\).

We will show by induction that for all \(t\geq 1\), \(\left\|\nabla f(x_{t})\right\|_{*}\leq\frac{\lambda_{t}}{2}\) and \(\max\left\{\left\|x_{t}-x^{*}\right\|,\left\|y_{t}-x^{*}\right\|,\left\|z_{t}- x^{*}\right\|\right\}\leq 2R_{1}\).

For \(t=1\), notice that \(x_{1}=y_{1}=z_{1}\). Thus, we have

\[\left\|\nabla f(x_{1})\right\|_{*}=\left\|\nabla f(x_{1})-\nabla f(x^{*}) \right\|_{*}\leq LR_{1}\leq\frac{\lambda_{1}}{2}.\]

Now assume that the claim holds for \(1\leq t\leq k\). By Proposition D.5, we know that

\[\frac{2\eta_{k}}{\alpha_{k}}f\left(y_{k+1}\right)-f\left(x^{*}\right)+\left\|z _{k+1}-x^{*}\right\|^{2}\leq 4R_{1}^{2}.\]

Furthermore

\[\left\|y_{k+1}-x^{*}\right\| \leq\left(1-\alpha_{k}\right)\left\|y_{k}-x^{*}\right\|+\alpha_{k} \left\|z_{k+1}-x^{*}\right\|\leq 2R_{1}\] \[\left\|x_{k+1}-x^{*}\right\| \leq\left(1-\alpha_{k}\right)\left\|y_{k+1}-x^{*}\right\|+\alpha_{k} \left\|z_{k+1}-x^{*}\right\|\leq 2R_{1}\]

For \(k\geq 1\) we have \(\alpha_{k+1}=\frac{2}{k+2}<1\); \(\frac{\alpha_{k+1}}{1-\alpha_{k+1}}=\frac{2}{k}\leq\frac{4}{k+2}\leq 2\alpha_{t+1}\) and \(\alpha_{t}\leq\frac{3}{2}\alpha_{t+1}\). Hence,

\[\left\|\nabla f(x_{k+1})\right\|_{*} \leq\left\|\nabla f(x_{k+1})-\nabla f(y_{k+1})\right\|_{*}+\left\| \nabla f(y_{k+1})-\nabla f(x^{*})\right\|_{*}\] \[\leq L\left\|x_{k+1}-y_{k+1}\right\|+\sqrt{2L\left(f\left(y_{k+1} \right)-f\left(x^{*}\right)\right)}\] \[\leq\frac{L\alpha_{k+1}\left\|x_{k+1}-z_{k+1}\right\|}{1-\alpha_{ k+1}}+2R_{1}\sqrt{\frac{L\alpha_{t}}{2\eta_{t}}}\]\[\leq 4LR_{1}\frac{\alpha_{k+1}}{1-\alpha_{k+1}}+2\sqrt{\frac{3}{2}}c \gamma R_{1}L\alpha_{t}\] \[\leq 8\gamma LR_{1}\alpha_{t+1}+3\sqrt{\frac{3}{2}}c\gamma LR_{1} \alpha_{t+1}\] \[\leq(8+3\sqrt{\frac{3}{2}}c)R_{1}\gamma L\alpha_{t+1}\] \[=\frac{16(8+3\sqrt{\frac{3}{2}}c)\lambda_{t+1}}{2c}\leq\frac{ \lambda_{t+1}}{2}\]

as needed. Therefore, we have

\[\frac{\eta_{T}}{\alpha_{T}}\left(f\left(y_{T+1}\right)-f\left(x^{*}\right) \right)+\mathbf{D}_{\psi}\left(x^{*},x_{T+1}\right)\leq 2R_{1}^{2}\]

which gives

\[f\left(y_{T+1}\right)-f\left(x^{*}\right) \leq\frac{2R_{1}^{2}\alpha_{T}}{\eta_{T}}=6R_{1}^{2}c\gamma^{2}L \alpha_{T}^{2}\] \[=6\max\left\{10^{4}L\gamma^{2}R_{1}^{2}(T+1)^{-2};6R_{1}\left(T+ 1\right)^{-1}\left(26T\right)^{\frac{1}{p}}\gamma^{\frac{p-1}{p}}\sigma\right\}.\]

2. Following the similar steps to the proof of Theorem D.1, and noticing that \(\left(c_{t}\right)\) is a increasing sequence, we obtain the convergence rate.