# Direct Diffusion Bridge using Data Consistency for Inverse Problems

Hyungjin Chung\({}^{1}\)   Jeongsol Kim\({}^{1}\)   Jong Chul Ye\({}^{2}\)

\({}^{1}\) Dept. of Bio and Brain Engineering

\({}^{2}\)Graduate School of AI

Korea Advanced Institute of Science and Technology (KAIST)

{hj.chung, jeongsol, jong.ye}@kaist.ac.kr

###### Abstract

Diffusion model-based inverse problem solvers have shown impressive performance, but are limited in speed, mostly as they require reverse diffusion sampling starting from noise. Several recent works have tried to alleviate this problem by building a diffusion process, directly bridging the clean and the corrupted for specific inverse problems. In this paper, we first unify these existing works under the name Direct Diffusion Bridges (DDB), showing that while motivated by different theories, the resulting algorithms only differ in the choice of parameters. Then, we highlight a critical limitation of the current DDB framework, namely that it does not ensure data consistency. To address this problem, we propose a modified inference procedure that imposes data consistency without the need for fine-tuning. We term the resulting method data Consistent DDB (CDDB), which outperforms its inconsistent counterpart in terms of both perception and distortion metrics, thereby effectively pushing the Pareto-frontier toward the optimum. Our proposed method achieves state-of-the-art results on both evaluation criteria, showcasing its superiority over existing methods. Code is open-sourced at [https://github.com/HJ-harry/CDDB](https://github.com/HJ-harry/CDDB)

## 1 Introduction

Diffusion models [15, 38] have become the de facto standard of recent vision foundation models [32, 31, 33]. Among their capabilities is the use of diffusion models as generative priors that can serve as plug-and-play building blocks for solving inverse problems in imaging [18, 38, 21, 5]. **D**iffusion model-based inverse problem solvers (DIS) have shown remarkable performance and versatility, as one can leverage the powerful generative prior regardless of the given problem at hand, scaling to linear [18, 38, 21], non-linear [5, 36], and noisy problems [21, 5].

Although there are many advantages of DIS, one natural limitation is its slow inference. Namely, the overall process of inference--starting from Gaussian noise and being repeatedly denoised to form a clean image--is kept the same, although there are marginal changes made to keep the sampling process consistent with respect to the given measurement. In such cases, the distance between the reference Gaussian distribution and the data distribution remains large, requiring inevitably a large number of sampling steps to achieve superior sample quality. On the other hand, the distribution of the measurements is much more closely related to the distribution of the clean images. Thus, intuitively, it would cost us much less compute if we were allowed to start the sampling process directly from the measurement, as in the usual method of direct inversion in supervised learning schemes.

Interestingly, several recent works aimed to tackle this problem under several different theoretical motivations: 1) Schrodinger bridge with paired data [26], 2) a new formulation of the diffusion processvia constant-speed continual degradation [9], and 3) Ornstein-Uhlenbeck stochastic differential equation (OU-SDE) [28]. While developed from distinct motivations, the resulting algorithms can be understood in a unifying framework with minor variations: we define this new class of methods as **Direct** **D**iffusion **B**ridges (DDB; Section 3.1). In essence, DDB defines the diffusion process from the clean image distribution in \(t=0\) to the measurement distribution in \(t=1\) as the convex combination between the paired data, such that the samples \(\mathbf{x}_{t}\) goes through continual degradation as \(t=0\to 1\). In training, one trains a time-conditional neural network \(G_{\theta}\) that learns a mapping to \(\mathbf{x}_{0}\) for all timesteps, resulting in an iterative sampling procedure that _reverts_ the measurement process.

Using such an iterative sampling process, one can flexibly choose the number of neural function evaluations (NFE) to generate reconstructions that meet the desiderata: with low NFE, less distortion can be achieved as the reconstruction regresses towards the mean [9]; with high NFE, one can opt for high perceptual quality at the expense of some distortion from the ground truth. This intriguing property of DDB creates a Pareto-frontier of reconstruction quality, where our desire would be to maximally pull the plot towards high perception and low distortion (bottom right corner of Fig. 0(c)).

In this work, we assert that DDB is missing a crucial component of _data consistency_, and devise methods to make the models _consistent_ with respect to the given measurement by only modifying the sampling algorithm, without any fine-tuning of the pre-trained model. We refer to this new class of models as data **C**onsistent **D**irect **D**iffusion **B**ridge (CDDB; Section 3.2), and show that CDDB is capable of pushing the Pareto-frontier further towards the optima (lower distortion: Fig. 0(a), higher perception: Fig. 0(b), overall trend: Fig. 0(c)) across a variety of tasks. Theoretically, we show that CDDB is a generalization of DDS (Decomposed Diffusion Sampling) [6], a recently proposed method tailored for DIS with Gaussian diffusion, which guarantees stable and fast sampling. We then propose another variation, CDDB-deep, which can be derived as the DDB analogue of the DPS [5] by considering _deeper_ gradients, which even further boosts the performance for certain tasks and enables the application to nonlinear problems where one cannot compute gradients in the usual manner (e.g. JPEG restoration). In the experiments, we showcase the strengths of each algorithm and show how one can flexibly construct and leverage the algorithms depending on the circumstances.

## 2 Background

### Diffusion models

Diffusion models [15; 38; 22; 19] defines the forward data noising process \(p(\mathbf{x}_{t}|\mathbf{x}_{0})\) as

\[\mathbf{x}_{t}=\alpha_{t}\mathbf{x}_{0}+\sigma_{t}\mathbf{z},\,\mathbf{z}\sim\mathcal{N}(0, \mathbf{I})\,\,\mathrm{for}\,\,t\in[0,1], \tag{1}\]

where \(\alpha_{t},\sigma_{t}\) controls the signal component and the noise component, respectively, and are usually designed such that \(\alpha_{t}^{2}+\sigma_{t}^{2}=1\)[15; 22]. Starting from the data distribution \(p_{\mathrm{data}}:=p(\mathbf{x}_{0})\), the noising process in (1) gradually maps \(p(\mathbf{x}_{t})\) towards isotropic Gaussian distribution as \(t\to 1\), i.e. \(p(\mathbf{x}_{1})\simeq\mathcal{N}(0,\mathbf{I})\). Training a neural network to _reverse_ the process amounts to training a residual denoiser

\[\min_{\theta}\mathbb{E}_{\mathbf{x}_{t}\sim p(\mathbf{x}_{t}|\mathbf{x}_{0}),\mathbf{x}_{0} \sim p_{\mathrm{data}}(\mathbf{x}_{0}),\mathbf{\epsilon}\sim\mathcal{N}(0,\mathbf{I})} \left[\|\mathbf{\epsilon}_{\theta}^{(t)}(\mathbf{x}_{t})-\mathbf{\epsilon}\|_{2}^{2}\right], \tag{2}\]

Figure 1: Quantitative metric of I\({}^{2}\)SB [26] (denoted DDB) vs. proposed CDDB on sr4x-bicubic task.

such that \(\mathbf{\epsilon}^{(t)}_{\theta^{*}}(\mathbf{x}_{t})\simeq\frac{\mathbf{x}_{t}-\alpha_{t}\mathbf{x} _{0}}{\sigma_{t}}\). Furthermore, it can be shown that epsilon matching is equivalent to the denoising score matching (DSM) [16; 37] objective up to a constant with different parameterization

\[\min_{\theta}\mathbb{E}_{\mathbf{x}_{t},\mathbf{x}_{0},\mathbf{\epsilon}}\left[\|\mathbf{s}^{( t)}_{\theta}(\mathbf{x}_{t})-\nabla_{\mathbf{x}_{t}}\log p(\mathbf{x}_{t}|\mathbf{x}_{0})\|_{2}^{2} \right], \tag{3}\]

such that \(\mathbf{s}^{(t)}_{\theta^{*}}(\mathbf{x}_{t})\simeq-\frac{\mathbf{x}_{t}-\alpha_{t}\mathbf{x} _{0}}{\sigma_{t}^{2}}=-\mathbf{\epsilon}^{(t)}_{\theta^{*}}(\mathbf{x}_{t})/\sigma_{t}\). Moreover, for optimal \(\theta^{*}\) and under regularity conditions, \(\mathbf{s}_{\theta^{*}}(\mathbf{x}_{t})=\nabla_{\mathbf{x}_{t}}\log p(\mathbf{x}_{t})\). Then, sampling from the distribution can be performed by solving the reverse-time generative SDE/ODE [38; 19] governed by the score function. It is also worth mentioning that the posterior mean, or the so-called denoised estimate can be computed via Tweedie's formula [13]

\[\hat{\mathbf{x}}_{0|t}:=\mathbb{E}_{p(\mathbf{x}_{0}|\mathbf{x}_{t})}[\mathbf{x}_{0}|\mathbf{x}_{t }]=\frac{1}{\alpha_{t}}(\mathbf{x}_{t}+\sigma_{t}^{2}\nabla_{\mathbf{x}_{t}}\log p(\bm {x}_{t}))\simeq\frac{1}{\alpha_{t}}(\mathbf{x}_{t}+\sigma_{t}^{2}\mathbf{s}^{(t)}_{ \theta^{*}}(\mathbf{x}_{t})). \tag{4}\]

In practice, DDPM/DDIM solvers [15; 35] work by iteratively refining these denoised estimates.

### Diffusion model-based inverse problem solving with gradient guidance

Suppose now that we are given a measurement \(\mathbf{y}\) obtained through some Gaussian linear measurement process \(\mathbf{A}\), where our goal is to sample from the posterior distribution \(p(\mathbf{x}|\mathbf{y})\). Starting from the sampling process of running the reverse SDE/ODE to sample from the prior distribution, one can modify the score function to adapt it for posterior sampling [38; 5]. By Bayes rule, \(\nabla_{\mathbf{x}_{t}}\log p(\mathbf{x}_{t}|\mathbf{y})=\nabla_{\mathbf{x}_{t}}\log p(\mathbf{x}_ {t})+\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{t})\), where \(\nabla_{\mathbf{x}_{t}}\log p(\mathbf{x}_{t})\simeq\mathbf{s}_{\theta^{*}}(\mathbf{x}_{t})\). However, \(\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{t})\) is intractable. Several methods have been proposed to approximate this time-dependent likelihood, two of the most widely used being DPS [5] and IIGDM [36]. DPS proposes the following Jensen approximation1

Footnote 1: The term \(\sum_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{t})\) is called the _mean_ error term.

\[\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{t})\overset{(\mathrm{DPS})}{\simeq} \nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\hat{\mathbf{x}}_{0|t})=\frac{\partial\hat{\mathbf{x }}_{0|t}}{\partial\mathbf{x}_{t}}\frac{\partial\|\mathbf{A}\hat{\mathbf{x}}_{0|t}-\mathbf{y} \|_{2}^{2}}{\partial\hat{\mathbf{x}}_{0|t}}=\underbrace{\frac{\partial\hat{\mathbf{x }}_{0|t}}{\partial\mathbf{x}_{t}}}_{\mathrm{J}}\underbrace{\mathbf{A}^{\top}(\mathbf{y}- \mathbf{A}\hat{\mathbf{x}}_{0|t})}_{\mathrm{V}}, \tag{5}\]

of which the chain rule is based on the denominator layout notation [41]. Here, we see that the gradient term can be represented as the Jacobian (J) vector (V) product (JVP). In the original implementation of DPS, the two terms are not computed separately, but computed directly as \(\nabla_{\mathbf{x}_{t}}\|\mathbf{y}-\mathbf{A}\hat{\mathbf{x}}_{0|t}\|_{2}^{2}\), where the whole term can be handled with backpropagation. By this choice, DPS can also handle non-linear operators when the gradients can be computed, e.g. phase retrieval, forward model given as a neural network. On the other hand, IIGDM proposes

\[\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{t})\overset{(\mathrm{IIGDM})}{\simeq }\mathcal{N}(\mathbf{A}\hat{\mathbf{x}}_{0|t},\mathbf{A}\mathbf{A}^{\top}+\mathbf{I})=\underbrace{ \frac{\partial\hat{\mathbf{x}}_{0|t}}{\partial\mathbf{x}_{t}}}_{\mathrm{J}}\underbrace{ \mathbf{A}^{\dagger}(\mathbf{y}-\mathbf{A}\hat{\mathbf{x}}_{0|t})}_{\mathrm{V}}, \tag{6}\]

where \(\mathbf{A}^{\dagger}:=\mathbf{A}^{\top}(\mathbf{A}\mathbf{A}^{\top})^{-1}\) is the Moore-Penrose pseudo-inverse. Using the JVP for implementation, it is no longer required that the whole term is differentiable. For this reason, IIGDM can be applied to cases where we have non-differentiable, non-linear measurements given that an operation analogous to pseudo-inverse can be derived, e.g. JPEG restoration. Notably, the update step of DPS can be achieved by simply pre-conditioning IIGDM with \(\mathbf{A}^{\top}\mathbf{A}\). Implementing DIS with DPS (5) or IIGDM (6) amounts to augmenting the gradient descent steps in between the ancestral sampling iterations.

While these methods are effective and outperforms the prior projection-based approaches [38; 21], they also have several drawbacks. Namely, the incorporation of the U-Net Jacobian is slow, compute-heavy, and often unstable [12; 34]. For example, when applied to MRI reconstruction in medical imaging, DPS results in noisy reconstructions [6] possibly due to unstable incorporation of the Wirtinger derivatives [23], and IIGDM is hard to use as it is non-trivial to compute \(\mathbf{A}^{\dagger}\). In order to circumvent these issues, DDS [6] proposed to use numerical optimization (i.e. conjugate gradients; CG) in the clean image domain, bypassing the need to compute J. Consequently, DDS achieves fast and stable reconstructions for inverse problems in medical imaging.

## 3 Main Contributions

### Direct Diffusion Bridge

We consider the case where we can sample \(\mathbf{x}_{0}:=\mathbf{x}\sim p(\mathbf{x})\), and \(\mathbf{x}_{1}:=\mathbf{y}\sim p(\mathbf{y}|\mathbf{x})^{2}\), i.e. paired data for training. Adopting the formulation of I\({}^{2}\)SB [26] we define the posterior of \(\mathbf{x}_{t}\) to be the product of Gaussians \(\mathcal{N}(\mathbf{x}_{t};\mathbf{x}_{0},\gamma_{t}^{2})\) and \(\mathcal{N}(\mathbf{x}_{t};\mathbf{x}_{1},\bar{\gamma}_{t}^{2})\), such that

\[p(\mathbf{x}_{t}|\mathbf{x}_{0},\mathbf{x}_{1})=\mathcal{N}\left(\mathbf{x}_{t};\frac{\bar{ \gamma}_{t}^{2}}{\gamma_{t}^{2}+\bar{\gamma}_{t}^{2}}\mathbf{x}_{0}+\frac{\gamma_{ t}^{2}}{\gamma_{t}^{2}+\bar{\gamma}_{t}^{2}}\mathbf{x}_{1},\frac{\gamma_{t}^{2}\bar{ \gamma}_{t}^{2}}{\gamma_{t}^{2}+\bar{\gamma}_{t}^{2}}\mathbf{I}\right). \tag{7}\]

Note that the sampling of \(\mathbf{x}_{t}\) from (7) can be done by the reparametrization trick

\[\mathbf{x}_{t}=(1-\alpha_{t})\mathbf{x}_{0}+\alpha_{t}\mathbf{x}_{1}+\sigma_{t}\mathbf{z},\; \mathbf{z}\sim\mathcal{N}(0,\mathbf{I}), \tag{8}\]

where \(\alpha_{t}:=\frac{\gamma_{t}^{2}}{\gamma_{t}^{2}+\bar{\gamma}_{t}^{2}},\, \sigma_{t}^{2}:=\frac{\gamma_{t}^{2}\bar{\gamma}_{t}^{2}}{\gamma_{t}^{2}+\bar{ \gamma}_{t}^{2}}\)3. This diffusion bridge introduces a _continual_ degradation process by taking a convex combination of \((\mathbf{x}_{0},\mathbf{x}_{1})\), starting from the clean image at \(t=0\) to maximal degradation at \(t=1\), with additional stochasticy induced by the noise component \(\sigma_{t}\). Our goal is to train a time-dependent neural network that maps any \(\mathbf{x}_{t}\) to \(\mathbf{x}_{0}\) that recovers the clean image. The training objective for I\({}^{2}\)SB [26] analogous to denoising score matching (DSM) [16] reads

Footnote 3: We ignore scaling constants that are related to the measurement noise for simplicity. For implementation, this can be absorbed into the choice of step sizes.

\[\min_{\theta}\mathbb{E}_{\mathbf{y}\sim p(\mathbf{y}|\mathbf{x}),\;\mathbf{x}\sim p(\mathbf{x}), \;t\sim U(0,1)}\left[\|\mathbf{s}_{\theta}(\mathbf{x}_{t})-\frac{\mathbf{x}_{t}-\mathbf{x}_{0} }{\gamma_{t}}\|_{2}^{2}\right], \tag{9}\]

which is also equivalent to training a residual network \(G_{\theta}\) with \(\min_{\theta}\mathbb{E}[\|G_{\theta}(\mathbf{x}_{t})-\mathbf{x}_{0}\|_{2}^{2}]\). For brevity, we simply denote the trained networks as \(G_{\theta^{*}}\) even if it is parametrized otherwise. Once the network is trained, we can reconstruct \(\mathbf{x}_{0}\) starting from \(\mathbf{x}_{1}\) by, for example, using DDPM ancestral sampling [15], where the posterior for \(s<t\) reads

\[p(\mathbf{x}_{s}|\mathbf{x}_{0},\mathbf{x}_{t})=\mathcal{N}(\mathbf{x}_{s};(1-\alpha_{s|t}^{2 })\mathbf{x}_{0}+\alpha_{s|t}^{2}\mathbf{x}_{t},\sigma_{s|t}^{2}\mathbf{I}), \tag{10}\]

with \(\alpha_{s|t}^{2}:=\frac{\gamma_{t}^{2}}{\gamma_{t}^{2}}\), \(\sigma_{s|t}^{2}:=\frac{(\gamma_{t}^{2}-\gamma_{t}^{2})\gamma_{t}^{2}}{\gamma _{t}^{2}}\). At inference, \(\mathbf{x}_{0}\) is replaced with a neural network-estimated \(\hat{\mathbf{x}}_{0|t}\) to yield \(\mathbf{x}_{s}\sim p(\mathbf{x}_{s}|\hat{\mathbf{x}}_{0|t},\mathbf{x}_{t})\).

However, when the motivation is to introduce 1) a tractable training objective that learns to recover the clean image along the degradation trajectory, and 2) devise a sampling method to gradually revert the degradation process, we find that the choices made for the parameters in (8),(7) can be arbitrary, as long as the marginal can be retrieved in the sampling process (10). In Table 1, we summarize the

\begin{table}
\begin{tabular}{l l l} \hline \hline  & \(\mathbf{I}^{2}\)SB [26] & **InDI [9]** \\ \hline
**Definition** & & \\ Motivation & SchrÃ¶dinger bridge & Small-step MMSE \\  & \(\beta_{t}=\begin{cases}\beta_{\min}+2\beta_{d}t,&t\in[0,0.5)\\ 2\beta_{\max}-2\beta_{d}t,&t\in[0.5,1.0]\end{cases}\) & - \\ Base process & \(\gamma_{t}=\int_{0}^{t}\beta_{\tau}\,d\tau,\,\tilde{\gamma}_{t}=\int_{t}^{1} \beta_{\tau}\,d\tau\) & - \\  & Linear symmetric & Const \\ \hline \multicolumn{3}{l}{**Diffusion process**} & \\ \(\alpha_{t}\) & \(\gamma_{t}^{2}/(\gamma_{t}^{2}+\bar{\gamma}_{t}^{2})\) & \(t\) \\ \(\sigma_{t}^{2}\) & \(\gamma_{t}^{2}\tilde{\gamma}_{t}^{2}/(\gamma_{t}^{2}+\bar{\gamma}_{t}^{2})\) & \(t^{2}\epsilon_{t}^{2}\) \\ \hline
**Sampling** & & \\ \(\alpha_{s|t}^{2}\) & \(\gamma_{s}^{2}/\gamma_{t}^{2}\) & \(s/t\) \\ \(\sigma_{s|t}^{2}\) & \(\frac{(\gamma_{t}^{2}-\gamma_{t}^{2})\gamma_{s}^{2}}{\gamma_{t}^{2}}\) & \(s^{2}(\epsilon_{s}^{2}-\epsilon_{t}^{2})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison between different types of DDB. \(\beta_{\rm d}:=\beta_{\max}-\beta_{\min}\). Further details are given in Appendix B.

choices made in [9, 26] to emphasize that the difference stems mostly from the parameter choices and not something fundamental. Concretely, sampling \(\mathbf{x}_{t}\) from paired data can always be represented as (8): a convex combination of \(\mathbf{x}_{0}\) and \(\mathbf{x}_{1}\) with some additional noise. Reverse diffusion at inference can be represented as (10): a convex combination of \(\mathbf{x}_{0}\) and \(\mathbf{x}_{t}\) with some stochasticity. We define the methods that belong to this category as Direct Diffusion Bridge (DDB) henceforth. Below, we formally state the equivalence between the algorithms, with proofs given in Appendix A.

**Theorem 1**.: _Let the parameters of InDI [9] in Table 1 be \(t:=\frac{\gamma_{t}^{2}}{\gamma_{t}^{2}+\gamma_{t}^{2}}\), \(\epsilon_{t}^{2}:=\frac{\gamma_{t}^{2}}{\gamma_{t}^{2}}(\gamma_{t}^{2}+\bar{ \gamma}_{t}^{2})\). Then, InDI and \(I^{2}\)SB are equivalent._

The equivalence relation will be useful when we derive our CDDB algorithm in Section. 3.2. As a final note, IR-SDE [28] does not strictly fall into this category as the sampling process is derived from running the reverse SDE. However, the diffusion process can still be represented as (8) by setting \(\alpha_{t}=1-e^{-\bar{\theta}_{t}}\), \(\sigma_{t}^{2}=\lambda^{2}(1-e^{-2\bar{\theta}_{t}})\), and the only difference comes from the sampling procedure.

### Data Consistent Direct Diffusion Bridge

MotivationRegardless of the choice in constructing DDB, there is a crucial component that is missing from the framework. While the sampling process (10) starts directly from the measurement (or equivalent), as the predictions \(\hat{\mathbf{x}}_{0|t}=G_{\theta}(\mathbf{x}_{t})\) are imperfect and are never guaranteed to preserve the measurement condition \(\mathbf{y}=\mathbf{A}\mathbf{x}\), the trajectory can easily deviate from the desired path, while the residual blows up. Consequently, this may result in inferior sample quality, especially in terms of distortion. In order to mitigate this downside, our strategy is to keep the DDB sampling strategy (10) intact and augment the steps to constantly _guide_ the trajectory to satisfy the data consistency, similar in spirit to gradient guidance in DIS. Here, we focus on the fact that the clean image estimates \(\hat{\mathbf{x}}_{0|t}\) is produced at every iteration, which can be used to compute the residual with respect to the measurement \(\mathbf{y}\). Taking a gradient step that minimizes this residual after every sampling step results in Algorithm 1, which we name data Consistent DDB (CDDB). In the following, we elaborate on how the proposed method generalizes DDS which was developed for DIS.

```
0:\(G_{\theta^{*}},\mathbf{x}_{1},\alpha_{i},\sigma_{i},\alpha_{i-1|i}^{2},\sigma_{i- 1|i}^{2},\rho_{i}\)
1:for\(i=N-1\) to \(0\)do
2:\(\hat{\mathbf{x}}_{0|i}\gets G_{\theta^{*}}(\mathbf{x}_{i})\)
3:\(\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\)
4:\(\mathbf{x}^{\prime}_{i-1}\leftarrow(1-\alpha_{i-1|i}^{2})\hat{\mathbf{x}}_{0|i}\) \(+\alpha_{i-1|i}^{2}\mathbf{x}_{i}+\sigma_{i-1|i}\mathbf{z}\)
5:\(g\leftarrow\frac{\partial\mathbf{x}_{0|i}}{\partial\mathbf{x}_{i}}\mathbf{A}^{\dagger}(\mathbf{ y}-\mathbf{A}\hat{\mathbf{x}}_{0|i})\)
6:\(\mathbf{x}_{i-1}\leftarrow\mathbf{x}^{\prime}_{i-1}+\rho_{i-1}\mathbf{g}\)
7:endfor
8:return\(\mathbf{x}_{0}\)
```

**Algorithm 2** CDDB (deep)

CDDB as a generalization of DDS [6]Rewriting (10) with reparameterization trick

\[\mathbf{x}_{s}=\underbrace{\hat{\mathbf{x}}_{0|t}}_{\mathrm{Denoise}( \mathbf{x}_{t})}+\underbrace{\alpha_{s|t}^{2}(\mathbf{x}_{t}-\hat{\mathbf{x}}_{0|t})}_{ \mathrm{Noise}(\mathbf{x}_{t})}+\sigma_{s|t}\mathbf{z},\quad\hat{\mathbf{x}}_{0|t}:=G_{ \theta^{*}}(\mathbf{x}_{t}) \tag{11}\]

we see that the iteration decomposes into three terms: the denoised component, the deterministic noise, and the stochastic noise. The key observation of DDIM [35] is that if the score network is fully expressive, then the deterministic noise term \(\mathbf{x}_{t}-\hat{\mathbf{x}}_{0|t}\) becomes Gaussian such that it satisfies the total variance condition

\[\left(\alpha_{s|t}^{2}\sigma_{t}\right)^{2}+\sigma_{s|t}^{2}=\sigma_{s}^{2}, \tag{12}\]

allowing (11) to restore the correct marginal \(\mathcal{N}(\mathbf{x}_{s};\mathbf{x}_{0},\sigma_{s}^{2})\). Under this condition, DDS showed that using a few step of numerical optimization ensure the updates from the denoised image \(\hat{\mathbf{x}}_{0|t}\) remain on the clean manifold. Furthermore, subsequent noising process using deterministic and stochastic noises can then be used to ensure the transition to the correct noisy manifold [6].

Under this view, our algorithm can be written concisely as

\[\mathbf{x}_{s}\leftarrow\underbrace{\hat{\mathbf{x}}_{0|t}+\rho\mathbf{A}^{\top}(\mathbf{y}-\mathbf{A} \hat{\mathbf{x}}_{0|t})}_{\mathrm{CDenoise}(\mathbf{x}_{t})}+\underbrace{\alpha_{s|t}^{2 }(\mathbf{x}_{t}-\hat{\mathbf{x}}_{0|t})}_{\mathrm{Noise}(\mathbf{x}_{t})}+\sigma_{s|t}\mathbf{ z}, \tag{13}\]

where we make the update step only to the clean denoised component, and leave the other components as is. In order to achieve proper sampling that obeys the marginals, it is important to show that the remaining components constitute the correct noise variance and the condition assuming Gaussianity should be (12). In the following, we show that this is indeed satisfied for the two cases of direction diffusion bridge (DDB):

**Theorem 2**.: _The total variance condition (12) is satisfied for both I\({}^{2}\)SB and InDI._

Proof.: For InDI, considering the noise variance \(\sigma_{t}=t\epsilon_{t}\) in Table 1,

\[\left(\alpha_{s|t}^{2}\sigma_{t}\right)^{2}+\sigma_{s|t}^{2}=\frac{s^{2}}{t^{2 }}t^{2}\epsilon_{t}^{2}+s^{2}(\epsilon_{s}^{2}-\epsilon_{t}^{2})=s^{2}\epsilon _{s}^{2}=\sigma_{s}^{2}. \tag{14}\]

Due to the equivalence in Theorem 1, the condition is automatically satisfied in I\({}^{2}\)SB. We show that this is indeed the case in Appendix A. 

In other words, given that the gradient descent update step in \(\mathrm{CDenoise}(\mathbf{x}_{t})\) does not leave the clean data manifold, it is guaranteed that the intermediate samples generated by (13) will stay on the correct noisy manifold [6]. In this regard, CDDB can be thought of as the DDB-generalized version of DDS. Similar to DDS, CDDB does not require the computation of heavy U-Net Jacobians and hence introduces negligible computation cost to the inference procedure, while being robust in the choice of step size.

CDDB-deepAs shown in DPS and IIGDM, taking _deeper_ gradients by considering U-Net Jacobians is often beneficial for reconstruction performance. Moreover, it even provides way to impose data consistency for non-linear inverse problems, where standard gradient methods are not feasible. In order to devise an analogous method, we take inspiration from DPS, and propose to augment the solver with a gradient step that maximizes the time-dependent likelihood (w.r.t. the measurement) \(p(\mathbf{y}|\mathbf{x}_{t})\). Specifically, we use the Jensen approximation from [5]

\[p(\mathbf{y}|\mathbf{x}_{t}) =\int p(\mathbf{y}|\mathbf{x}_{0})p(\mathbf{x}_{0}|\mathbf{x}_{t})\,d\mathbf{x}_{0} \tag{15}\] \[=\mathbb{E}_{p(\mathbf{x}_{0}|\mathbf{x}_{t})}[p(\mathbf{y}|\mathbf{x}_{0})] \simeq p(\mathbf{y}|\mathbb{E}[\mathbf{x}_{0}|\mathbf{x}_{t}])=p(\mathbf{y}|\hat{\mathbf{x}}_{0|t }),\]

where the last equality is naturally satisfied from the training objective (9). Using the approximation used in (15), the correcting step under the Gaussian measurement model yields

\[\nabla_{\mathbf{x}_{t}}\log p(\mathbf{y}|\mathbf{x}_{t})\simeq\nabla_{\mathbf{x}_{t}}\|\mathbf{y}- \mathbf{A}\hat{\mathbf{x}}_{0|t}\|_{2}^{2}. \tag{16}\]

Implementing (16) in the place of the shallow gradient update step of Algorithm 1, we achieve CDDB-deep (see Algorithm 2). From our initial experiments, we find that preconditioning with \(\mathbf{A}^{\dagger}\) as in IIGDM improves performance by a small margin, and hence use this setting as default.

## 4 Experiments

### Setup

Model, DatasetFor a representative DDB, we choose I\({}^{2}\)SB [26] along with the pre-trained model weights for the following reasons: 1) it is open-sourced4, 2) it stands as the current state-of-the-art, 3) the model architecture is based on ADM [11], which induces fair comparison against other DIS methods. All experiments are based on ImageNet 256\(\times\)256 [10], a benchmark that is considered to be much more challenging for inverse problem solving based on generative models [5], compared to more focused datasets such as FFHQ [20]. We follow the standards of [26] and test our method on the following degradations: sr4x-{bicubic, pool}, deblur-{uniform, gauss}, and JPEG restoration with 1k validation images.

Footnote 4: [https://github.com/NVlabs/I2SB](https://github.com/NVlabs/I2SB)

[MISSING_PAGE_FAIL:7]

mainly focused on perceptual metrics [5; 36; 26], mainly because these methods excel on these metrics, but often compromising distortion metrics. DDB methods often take this to the extreme, where one can achieve the best PSNR with very little NFE, and the PSNR consistently degrades as one increases the NFE [9; 26] (See Fig. 1c). Despite this fact, the standard setting in DDB methods is to set a high NFE as one can achieve much improved perceptual quality. On the other hand, conventional iterative methods are often highly optimized for less distortion, albeit with low perceptual quality. While this trade-off may seem imperative, we show that CDDB can improve both aspects, putting it in the place of the state-of-the-art on most experiments (See Tab. 2, 3). A similar trend can be observed in Fig. 2, where we see that CDDB greatly improves the performance of DDB, while also outperforming DIS and iterative optimization methods.

CDDB pushes forward the Pareto-frontierIt is widely known that there exists an inevitable trade-off of distortion when aiming for higher perceptual quality [3; 24]. This phenomenon has been reconfirmed consistently in the recent DIS [5; 36] and DDS [9; 26] methods. For DDB, one can flexibly control this trade-off by simply choosing different NFE values, creating a Pareto-frontier with higher NFE tailored towards perceptual quality. While this property is intriguing, the gain we achieve when increasing the NFE decreases _exponentially_, and eventually reaches a bound when NFE > 1000. In contrast, we show in Fig. 1 that CDDB pushes the bound further towards the optima. Specifically, 20 NFE CDDB _outperforms_ 1000 NFE DDB in PSNR by \(>2\) db, while having lower FID (i.e. better perceptual quality). To this point, CDDB induces dramatic acceleration (> 50\(\times\)) to DDB.

CDDB vs. CDDB-deepThe two algorithms presented in this work share the same spirit but have different advantages. CDDB generally has higher speed and stability, possibly due to guaranteed convergence. As a result, it robustly increases the performance of SR and deblurring. In contrast, considering the case of inpainting and JPEG restoration, CDDB cannot improve the performance of DDB. For inpainting, the default setting of I\({}^{2}\)SB ensures consistency by iteratively applying replacement, as implemented in [40]. As the measurement stays in the pixel space, the gradients cannot impose any constraint on the missing pixel values. CDDB-deep is useful in such a situation, as the U-Net Jacobian has a global effect on _all_ the pixels, improv

Figure 4: Results on JPEG restoration (QF=10). CDDB recovers texture details (row 1,3), and color details (row 2).

Figure 3: Results on inpainting (Left) and deblurring (Right). For inpainting, boundaries are corrected (row 1) and artifacts are corrected/removed (row 2,3). For deblurring, halo artifacts are corrected, and grid patterns from the background are alleviated.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Method & PSNR \(\uparrow\) & SSIM \(\downarrow\) & LPIPS \(\downarrow\) & FID \(\downarrow\) \\ \hline
**CDDB** (ours) & **26.34** & 0.837 & **0.263** & **19.48** \\
**PSB**[26] & 26.12 & 0.832 & 0.266 & 20.35 \\ \hline IIGDM [36] & 26.09 & **0.842** & 0.282 & 30.27 \\ DDRM [21] & 26.33 & 0.829 & 0.330 & 47.02 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative evaluation of the JPEG restoration (QF = 10) task.

ing the performance by inducing coherence. CDDB-deep also enables the extension to nonlinear inverse problems where one cannot take standard gradient steps. This is illustrated for the case of JPEG restoration in Tab. 3 and Fig. 4, where we see overall improvement in performance compared to I\({}^{2}\)SB.

Noise robustnessDIS methods are often designed such that they are robust to measurement noise [21; 5]. In contrast, this is not the case for DDB as they are trained in a supervised fashion: If not explicitly trained with synthetic adding of noise, the method does not generalize well to noisy measurements, as can be seen in Fig. 5. On the other hand, note that with CDDB, we are essentially incorporating a Gaussian likelihood model, which naturally enhances the robustness to noise. As a result, while I\({}^{2}\)SB tends to propagate noise (best seen in the background), we do not observe such artifacts when using CDDB.

## 5 Discussion

Extension to other related worksGoing beyond the paired inverse problem setting and considering the Schrodinger Bridge (SB) problem [25; 8], or more generally transport mapping problems [27] between the two unmatched distributions, it is often desirable to control the deviation from the start of sampling. A concrete example would be the case of image-to-image translation [43] where one does not want to alter the content of the image. As CDDB can be thought of as a regularization method that penalizes the deviation from the starting point, the application is general and can be extended to such SB problems at inference time by using the gradients that minimize the distance from the start point. We leave this direction for future work.

Data consistency in supervised learning frameworksThe first applications of supervised deep learning to solve inverse problems in medical imaging (e.g. CT [17], MRI reconstruction [39]) mostly involved directly inverting the measurement signal without considering the measurement constraints. The works that followed [1; 14] naturally extended the algorithms by incorporating measurement consistency steps in between the forward passes through the neural network. Analogously, CDDB is a natural extension of DDB but with high flexibility, as we do not have to pre-determine the number of forward passes [1] or modify the training algorithm [14].

## 6 Conclusion

In this work, we unify the seemingly different algorithms under the class of direct diffusion bridges (DDB) and identify the crucial missing part of the current methods: data consistency. Our train-free modified inference procedure named consistent DDB (CDDB) fixes this problem by incorporating consistency-imposing gradient steps in between the reverse diffusion steps, analogous to the recent DIS methods. We show that CDDB can be seen as a generalization of representative DIS methods (DDS, DPS) in the DDB framework. We validate the superiority of our method with extensive experiments on diverse inverse problems, achieving state-of-the-art sample quality in both distortion and perception. Consequently, we show that CDDB can push the Pareto-frontier of the reconstruction toward the desired optimum.

Limitations and societal impactThe proposed method assumes prior knowledge of the forward operator. While we limit our scope to non-blind inverse problems, the extension of CDDB to blind inverse problems [7; 30] will be a possible direction of research. Moreover, for certain inverse problems (e.g. inpainting), even when do observe improvements in qualitative results, the quantitative metrics tend to slightly decrease overall. Finally, inheriting from DDS/DIS methods, our method relies on strong priors that are learned from the training data distribution. This may potentially lead to reconstructions that intensify social bias and should be considered in practice.

Figure 5: Results on noisy SR\(\times\)4 reconstruction. I\({}^{2}\)SB propagates noise to the reconstruction. CDDB effectively removes noise.

## Acknowledgments and Disclosure of Funding

This research was supported by the KAIST Key Research Institute (Interdisciplinary Research Group) Project, by the National Research Foundation of Korea under Grant NRF-2020R1A2B5B03001980, by the Korea Medical Device Development Fund grant funded by the Korea government (the Ministry of Science and ICT, the Ministry of Trade, Industry and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) (Project Number: 1711137899, KMDP_PR_20200901.0015), by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program(KAIST)), and by the Field-oriented Technology Development Project for Customs Administration through National Research Foundation of Korea(NRF) funded by the Ministry of Science & ICT and Korea Customs Service(NRF-2021M3I1A1097938).

## References

* [1] Hemant K Aggarwal, Merry P Mani, and Mathews Jacob. MoDL: Model-based deep learning architecture for inverse problems. _IEEE transactions on medical imaging_, 38(2):394-405, 2018.
* [2] Thilo Balke, Fernando Davis, Cristina Garcia-Cardona, Michael McCann, Luke Pfister, and Brendt Wohlberg. Scientific Computational Imaging COde (SCICO). Software library available from [https://github.com/lanl/scico](https://github.com/lanl/scico), 2022.
* [3] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 6228-6237, 2018.
* [4] Stanley H Chan, Xiran Wang, and Omar A Elgendy. Plug-and-play ADMM for image restoration: Fixed-point convergence and applications. _IEEE Transactions on Computational Imaging_, 3(1):84-98, 2016.
* [5] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In _International Conference on Learning Representations_, 2023.
* [6] Hyungjin Chung, Suhyeon Lee, and Jong Chul Ye. Fast diffusion sampler for inverse problems by geometric decomposition. _arXiv preprint arXiv:2303.05754_, 2023.
* [7] Hyungjin Chung, Dohoon Ryu, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Solving 3d inverse problems using pre-trained 2d diffusion models. _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023.
* [8] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709, 2021.
* [9] Mauricio Delbracio and Peyman Milanfar. Inversion by direct iteration: An alternative to denoising diffusion for image restoration. _arXiv preprint arXiv:2303.11435_, 2023.
* [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [11] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* [12] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In _International conference on machine learning_. PMLR, 2023.
* [13] Bradley Efron. Tweedie's formula and selection bias. _Journal of the American Statistical Association_, 106(496):1602-1614, 2011.

* Gupta et al. [2018] Harshit Gupta, Kyong Hwan Jin, Ha Q Nguyen, Michael T McCann, and Michael Unser. Cnn-based projected gradient descent for consistent ct image reconstruction. _IEEE transactions on medical imaging_, 37(6):1440-1453, 2018.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* Hyvarinen and Dayan [2005] Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* Jin et al. [2017] Kyong Hwan Jin, Michael T McCann, Emmanuel Froustey, and Michael Unser. Deep convolutional neural network for inverse problems in imaging. _IEEE Transactions on Image Processing_, 26(9):4509-4522, 2017.
* Kadkhodaie and Simoncelli [2021] Zahra Kadkhodaie and Eero P Simoncelli. Stochastic solutions for linear inverse problems using the prior implicit in a denoiser. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* Karras et al. [2022] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _Proc. NeurIPS_, 2022.
* Karras et al. [2019] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4401-4410, 2019.
* Kawar et al. [2022] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Kingma et al. [2021] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. _arXiv preprint arXiv:2107.00630_, 2021.
* Kreutz-Delgado [2009] Ken Kreutz-Delgado. The complex gradient operator and the cr-calculus. _arXiv preprint arXiv:0906.4835_, 2009.
* Ledig et al. [2017] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4681-4690, 2017.
* Leonard [2013] Christian Leonard. A survey of the schrodinger problem and some of its connections with optimal transport. _arXiv preprint arXiv:1308.0215_, 2013.
* Liu et al. [2023] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar. I\({}^{2}\)SB: Image-to-Image Schrodinger Bridge. In _International conference on machine learning_. PMLR, 2023.
* Liu et al. [2023] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In _The Eleventh International Conference on Learning Representations_, 2023.
* Luo et al. [2023] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sjolund, and Thomas B Schon. Image restoration with mean-reverting stochastic differential equations. In _International conference on machine learning_. PMLR, 2023.
* Miljkovic et al. [2012] Sladjana Miljkovic, Marko Miladinovic, Predrag Stanimirovic, and Igor Stojanovic. Application of the pseudoinverse computation in reconstruction of blurred images. _Filomat_, 26(3):453-465, 2012.
* Murata et al. [2023] Naoki Murata, Koichi Saito, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Gibbsddrm: A partially collapsed gibbs sampler for solving blind inverse problems with denoising diffusion restoration. In _International conference on machine learning_. PMLR, 2023.

* [31] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* [34] Tim Salimans and Jonathan Ho. Should ebms model the energy or the score? In _Energy Based Models Workshop-ICLR 2021_, 2021.
* [35] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _9th International Conference on Learning Representations, ICLR_, 2021.
* [36] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse problems. In _International Conference on Learning Representations_, 2023.
* [37] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [38] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _9th International Conference on Learning Representations, ICLR_, 2021.
* [39] Shanshan Wang, Zhenghang Su, Leslie Ying, Xi Peng, Shun Zhu, Feng Liang, Dagan Feng, and Dong Liang. Accelerating magnetic resonance imaging via deep learning. In _2016 IEEE 13th international symposium on biomedical imaging (ISBI)_, pages 514-517. IEEE, 2016.
* [40] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. In _The Eleventh International Conference on Learning Representations_, 2023.
* [41] Jong Chul Ye. _Geometry of Deep Learning_. Springer, 2022.
* [42] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep CNN for image denoising. _IEEE transactions on image processing_, 26(7):3142-3155, 2017.
* [43] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _Proceedings of the IEEE international conference on computer vision_, pages 2223-2232, 2017.