# On the Optimal Time Complexities in Decentralized Stochastic Asynchronous Optimization

Alexander Tyurin

KAUST, AIRI, Skoltech

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia AIRI, Moscow, Russia Skoltech

Skolkovo Institute of Science and Technology, Moscow, Russia

Peter Richtarik

KAUST

King Abdullah University of Science and Technology, Thuwal, Saudi Arabia AIRI, Moscow, Russia Skolkovo Institute of Science and Technology, Moscow, Russia

###### Abstract

We consider the decentralized stochastic asynchronous optimization setup, where many workers asynchronously calculate stochastic gradients and asynchronously communicate with each other using edges in a multigraph. For both homogeneous and heterogeneous setups, we prove new time complexity lower bounds under the assumption that computation and communication speeds are bounded. We develop a new nearly optimal method, Fragile SGD, and a new optimal method, Ameile SGD, that converge under arbitrary heterogeneous computation and communication speeds and match our lower bounds (up to a logarithmic factor in the homogeneous setting). Our time complexities are new, nearly optimal, and provably improve all previous asynchronous/synchronous stochastic methods in the decentralized setup.

## 1 Introduction

We consider the smooth nonconvex optimization problem

\[\min_{x\in\mathbb{R}^{d}}\Big{\{}f(x):=\mathbb{E}_{\xi\sim\mathcal{D}_{\xi}} \left[f(x;\xi)\right]\Big{\}}, \tag{1}\]

where \(f\,:\,\mathbb{R}^{d}\times\mathbb{S}_{\xi}\rightarrow\mathbb{R},\) and \(\mathcal{D}_{\xi}\) is a distribution on a non-empty set \(\mathbb{S}_{\xi}.\) For a given \(\varepsilon>0,\) we want to find a possibly random point \(\bar{x},\) called an \(\varepsilon\)-stationary point, such that \(\mathbb{E}[\left\|\nabla f(\bar{x})\right\|^{2}]\leq\varepsilon.\) We analyze the heterogeneous setup and the convex setup with smooth and non-smooth functions in Sections C and D.

### Decentralized setup with times

We investigate the following decentralized asynchronous setup. Assume that we have \(n\) workers/nodes with the associated computation times \(\{h_{i}\},\) and communications times \(\{\rho_{i\to j}\}.\) It takes less or equal to \(h_{i}\in[0,\infty]\) seconds to compute a stochastic gradient by the \(i^{\text{th}}\) node, and less or equal \(\rho_{i\to j}\in[0,\infty]\) seconds to send _directly_ a vector \(v\in\mathbb{R}^{d}\) from the \(i^{\text{th}}\) node to the \(j^{\text{th}}\) node (it is possible that \(h_{i}=\infty\) and \(\rho_{i\to j}=\infty\)). All computations and communications can be done asynchronously and in parallel. We would like to emphasize that \(h_{i}\in[0,\infty]\) and \(\rho_{i\to j}\in[0,\infty]\) are only upper bounds, and the real and effective computation and communication times can be arbitrarily heterogeneous and random. For simplicity of presentation, we assume the upper bounds are static; however, in Section 5.5, we explain that our result can be trivially extended to the case when the upper bounds are dynamic.

We consider any _weighted directed multigraph_ parameterized by a vector \(h\in\mathbb{R}^{n}\) such that \(h_{i}\in[0,\infty],\) and a matrix of distances \(\{\rho_{i\to j}\}_{i,j}\in\mathbb{R}^{n\times n}\) such that \(\rho_{i\to j}\in[0,\infty]\) for all \(i,j\in[n]\) and\(\rho_{i\to i}=0\) for all \(i\in[n]\). Every worker \(i\) is connected to any other worker \(j\) with two edges \(i\to j\) and \(j\to i\). For this setup, it would be convenient to define _the distance of the shortest path from worker \(i\) to worker \(j\)_ :

\[\tau_{i\to j}:=\min_{\text{path}\in P_{i\to j}}\sum_{(u,v)\in\text{ path}}\rho_{u\to v}\in[0,\infty], \tag{2}\] \[\text{where }P_{i\to j}:=\left\{\left[(k_{1},k_{2}),\ldots,(k_{m},k_{m+1}) \right]\big{|}\,\forall m\in\mathbb{N}\,\forall p\in[m+1]\,\forall k_{p}\in[n],\right.\] \[\left.\qquad\qquad\qquad\qquad k_{1}=i,k_{m+1}=j,\forall j\in\{2, \ldots,m\}\,k_{j-1}\neq k_{j}\neq k_{j+1}\right\}\]

is the set of all possible paths without loops from worker \(i\) to worker \(j\) for all \(i,j\in[n]\). One can easily show that the triangle inequality \(\tau_{i\to j}\leq\tau_{i\to k}+\tau_{k\to j}\) holds for all \(i,j,k\in[n]\). Note that \(\tau_{i\to j}\leq\rho_{i\to j}\) for all \(i,j\in[n]\). It is important to distinguish \(\tau_{i\to j}\) and \(\rho_{i\to j}\) because it is possible that \(\tau_{i\to j}<\rho_{i\to j}=\infty\) if workers \(i\) and \(j\) are connected by an edge \(\rho_{i\to j}=\infty,\) and there is a path through other workers (see Fig. 1).

We work with the following standard assumption from smooth nonconvex stochastic optimization literature.

**Assumption 1**.: \(f\) _is differentiable and \(L\)-smooth, i.e., \(\|\nabla f(x)-\nabla f(y)\|\leq L\,\|x-y\|\,\forall x,y\in\mathbb{R}^{d}\)._

**Assumption 2**.: _There exist \(f^{*}\in\mathbb{R}\) such that \(f(x)\geq f^{*}\) for all \(x\in\mathbb{R}^{d}\)._

**Assumption 3**.: _For all \(x\in\mathbb{R}^{d}\), stochastic gradients \(\nabla f(x;\xi)\) are unbiased and \(\sigma^{2}\)-variance-bounded, i.e., \(\mathbb{E}_{\xi}[\nabla f(x;\xi)]=\nabla f(x)\) and \(\mathbb{E}_{\xi}[\|\nabla f(x;\xi)-\nabla f(x)\|^{2}]\leq\sigma^{2},\) where \(\sigma^{2}\geq 0\). We also assume that computation and communication times are statistically independent of stochastic gradients._

## 2 Previous Results

### Time complexity with one worker

For the case when \(n=1\) and \(\tau_{1\to 1}=0,\) convergence rates and time complexities of problem (1) are well-understood. It is well-known that the stochastic gradient method (SGD), i.e., \(x^{k+1}=x^{k}-\gamma\nabla f(x^{k};\xi^{k})\), where \(\{\xi^{k}\}\) are i.i.d. from \(\mathcal{D}_{\xi}\), has the optimal _oracle complexity_\(\Theta(\nicefrac{{L\Delta}}{{\varepsilon}}+\nicefrac{{\sigma^{2}L\Delta}}{{ \varepsilon^{2}}})\)(Ghadimi and Lan, 2013; Arjevani et al., 2022). Assuming that the computation time of one stochastic gradient is bounded by \(h_{1},\) we can conclude that the optimal time complexity is

\[T_{\text{single}}^{\tau=0}:=\Theta\left(h_{1}\times\left(\frac{L\Delta}{ \varepsilon}+\frac{\sigma^{2}L\Delta}{\varepsilon^{2}}\right)\right) \tag{3}\]

seconds in the worst case.

### Parallel optimization without communication costs

Assume that \(n>1\) and \(\tau_{i\to j}=0\) for all \(i,j\in[n],\) and computation times of stochastic stochastic gradients are arbitrarily heterogeneous. The simplest baseline method in this setup is Minibatch SGD,

Figure 1: _On the left:_ an example of a multigraph with \(n=6\). The edges with \(\rho_{i\to j}=\infty\) are omitted. The shortest distance between nodes \(5\) and \(3\) is \(\tau_{5\to 3}=\rho_{5\to 1}+\rho_{1\to 2}+\rho_{2\to 3}\). Note that \(\rho_{5\to 3}=\infty\). _On the right:_ an example of a spanning tree that illustrates the shortest paths from every node to node \(3\). The shortest distance between nodes \(6\) and \(3\) is \(\tau_{6\to 3}=\infty\) because \(\rho_{6\to i}=\infty\) for all \(i\neq 6\).

i.e.,

\[x^{k+1}=x^{k}-\tfrac{\gamma}{n}\sum\limits_{i=1}^{n}\nabla f(x^{k};\xi_{i}^{k}), \tag{4}\]

where \(\{\xi_{i}^{k}\}\) are i.i.d. from \(\mathcal{D}_{\xi}\) and the gradient \(\nabla f(x^{k};\xi_{i}^{k})\) is calculated in worker \(i\) in parallel. This method waits for stochastic gradients from all workers; thus, it is not robust to "stragglers" and in the worst case the time complexity of such an algorithm is

\[T_{\text{mini}}^{r=0}:=\Theta\left(\max\limits_{i\in[n]}h_{i}\times\left(\tfrac {L\Delta}{e}+\tfrac{\sigma^{2}L\Delta}{ne^{2}}\right)\right),\]

which depends on the time \(\max_{i\in[n]}h_{i}\) of the slowest worker. There are many other more advanced methods including Picky SGD(Cohen et al., 2021), Asynchronous SGD(e.g., (Recht et al., 2011; Nguyen et al., 2018; Mishchenko et al., 2022; Koloskova et al., 2022)), and Rennala SGD(Tyurin and Richtarik, 2023) that are designed to be robust to workers' chaotic computation times. Under the assumption that the computation times of the workers are heterogeneous and bounded by \(\{h_{i}\},\)Tyurin and Richtarik (2023) showed that Rennala SGD is the first method that achieves the optimal time complexity

\[T_{\text{Rennala}}^{r=0}:=\Theta\left(\min\limits_{m\in[n]}\left[\left(\tfrac{1} {m}\sum\limits_{i=1}^{m}\tfrac{1}{h_{n_{i}}}\right)^{-1}\left(\tfrac{L\Delta}{ e}+\tfrac{\sigma^{2}L\Delta}{me^{2}}\right)\right]\right), \tag{5}\]

where \(\pi\) is a permutation that sorts \(h_{i}:h_{\pi_{1}}\leq\cdots\leq h_{\pi_{n}}\). For instance, one can see that \(T_{\text{Rennala}}^{r=0}\leq T_{\text{mini}}^{r=0}\) for all parameters.

### Parallel optimization with communication costs \(\tau_{i\to j}\)

We now consider the setup where workers' communication times can not be ignored. This problem leads to a research field called _decentralized optimization_. This setup is the primary case for us. All \(n\) workers calculate stochastic gradients in parallel and communicate with each other. Numerous works consider this setup, and we refer to Yang et al. (2019); Koloskova (2024) for detailed surveys. Typically, methods in this setting use the gossip matrix framework (Duchi et al., 2011; Shi et al., 2015; Koloskova et al., 2021) and get an _iteration converge rate_ that depends on the spectral gap of a mixing matrix. However, such rates do not give the physical time of algorithms (see also Section B).

Let us consider a straightforward baseline: Minibatch SGD. We can implement (4) in a way that all workers calculate one stochastic gradient (takes at most \(\max_{i\in[n]}h_{i}\) seconds) and then aggregate them to one _pivot worker_\(j^{*}\) (takes at most \(\max_{i\in[n]}\tau_{i\to j^{*}}\) seconds). Then, pivot worker \(j^{*}\) calculatesa new point \(x^{k+1}\) and broadcasts it to all workers (takes \(\max_{i\in[n]}\tau_{j^{*}\to i}\) seconds). One can easily see that the time complexity of such a procedure is4

Footnote 4: because \(\max_{i,j\in[n]}\tau_{i\to j}\leq\max_{i\in[n]}\tau_{i\to j^{*}}+\max_{i\in[n]} \tau_{j^{*}\to i}\leq 2\max_{i,j\in[n]}\tau_{i\to j}\) by the triangle inequality

\[T_{\text{mini}}:=\Theta\left(\max\left\{\max_{i,j\in[n]}\tau_{i\to j},\max_{i \in[n]}h_{i}\right\}\left(\tfrac{L\Delta}{\varepsilon}+\tfrac{\sigma^{2}L \Delta}{n\varepsilon^{2}}\right)\right). \tag{6}\]

We can analyze any other asynchronous decentralized method, which will be done with more advanced methods in Section 5.4.

But what is the best possible (optimal) time complexity we can get in the setting from Section 1.1?

Unlike the setups from Sections 2.1 and 2.2 when the communication times are zero (\(\tau_{i\to j}=0\) for all \(i,j\in[n]\)), the optimal time complexity and an optimal method for the case \(\tau_{i\to j}\geq 0\) for all \(i,j\in[n]\) are not known. Our main goal in this paper is to solve this problem.

## 3 Contributions

We consider the class of functions that satisfy the setup and the assumptions from Section 1.1 and show that (informally) it is impossible to develop a method that will converge faster than (7) seconds. Next, we develop a new asynchronous stochastic method, Fragile SGD, that is _nearly_ optimal (i.e., almost matches this lower bound; see Table 1 and Corollary 1). This is the first such method. It provably improves on Asynchronous SGD (Even et al., 2024) and all other synchronous and asynchronous methods (Bornstein et al., 2023). We also consider the heterogeneous setup (see Table 2 and Section C), where we discover the optimal time complexity by proving another lower bound and developing a new optimal method, Ameile SGD, with weak assumptions. The developed methods can guarantee the _iteration complexity_\(\mathcal{O}\left(\nicefrac{{L\Delta}}{{\varepsilon}}\right)\) with arbitrarily heterogeneous random computation and communication times (Theorems 4 and 8). Our findings are extended to the convex setup in Section D, where we developed new accelerated methods, Accelerated Fragile SGD and Accelerated Ameile SGD.

## 4 Lower Bound

In order to construct our lower bound, we consider any (zero-respecting) method that can be represented by Protocol 1. This protocol captures all virtually distributed synchronous and asynchronous methods, such as Minibatch SGD, SWIFT (Bornstein et al., 2023), Asynchronous SGD (Even et al., 2024), and Gradient Tracking (Koloskova et al., 2021).

For all such methods we prove the following theorem.

**Theorem 1** (Lower Bound; Simplified Presentation of Theorem 19).: _Consider Protocol 1 with \(\nabla f(\cdot;\cdot)\). We take any \(h_{i}\geq 0\) and \(\tau_{i\to j}\geq 0\) for all \(i,j\in[n]\) such that \(\tau_{i\to j}\leq\tau_{i\to k}+\tau_{k\to j}\) for all \(i,k,j\in[n]\). We fix \(L,\Delta,\varepsilon,\sigma^{2}>0\) that satisfy the inequality \(\varepsilon<cL\Delta\) for some universal constant \(c\). For any (zero-respecting) algorithm, there exists a function \(f,\) which satisfy Assumptions 1, 2 and \(f(0)-f^{*}\leq\Delta\), and a stochastic gradient mapping \(\nabla f(\cdot;\cdot),\) which satisfies Assumption 3, such that the required time to find \(\varepsilon\)-solution is_

\[\Omega\left(\frac{1}{\log n+1}\frac{L\Delta}{\varepsilon}\min_{j \in[n]}t^{*}(\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i}]_{i=1}^{n},[\tau_{i \to j}]_{i=1}^{n})\right) \tag{7}\] \[\stackrel{{\text{Def}\,2}}{{\equiv}}\Omega\left( \frac{1}{\log n+1}\frac{L\Delta}{\varepsilon}\min_{j\in[n]}\min_{k\in[n]}\max \left\{\max\{\tau_{\pi_{j,k}\to j},h_{\pi_{j,k}}\},\frac{\sigma^{2}}{ \varepsilon}\left(\sum\limits_{i=1}^{k}\frac{1}{n_{\pi_{j,i}}}\right)^{-1} \right\}\right), \tag{8}\]

_where, for \(j\in[n],\)\(\pi_{j,\cdot}\) is a permutation that sorts \(\{\max\{\tau_{i\to j},h_{i}\}\}_{i=1}^{n},\) i.e., \(\max\{\tau_{\pi_{j,1}\to j},h_{\pi_{j,1}}\}\leq\cdots\leq\max\{\tau_{\pi_{j,n }\to j},h_{\pi_{j,n}}\}.\)_

```
1:Init \(S_{i}=\emptyset\) (all available information) on worker \(i\) for all \(i\in[n]\)
2:Run the following two loops in each worker in parallel
3:while True do
4: Calculate a new point \(x_{i}^{k}\) based on \(S_{i}\) (takes \(0\) seconds)
5: Calculate a stochastic gradient \(\nabla f(x_{i}^{k};\xi)\) (or \(\nabla f_{i}(x_{i}^{k};\xi)\)) \(\xi\sim\mathcal{D}_{\xi}\) (takes \(h_{i}\) seconds)
6: Atomic add \(\nabla f(x_{i}^{k};\xi)\) (or \(\nabla f_{i}(x_{i}^{k};\xi)\)) to \(S_{i}\) (atomic operation, takes \(0\) seconds)
7:endwhile
8:while True do
9: Send(a) any vector from \(\mathbb{R}^{d}\) based on \(S_{i}\) to any worker \(j\) and go to the next step of this loop without waiting (takes \(\tau_{i\to j}\) seconds to send; worker \(j\) adds this vector to \(S_{j}\))
10:endwhile (a): When we prove the lower bounds, we allow algorithms to send as many vectors as they want in parallel from worker \(i\) to worker \(j\) for all \(i\neq j\in[n]\).
```

**Protocol 1** Simplified Presentation of Protocol 8

The intuition and meaning of the formula (8) is discussed in Section 5.2. Note that if we take \(n=1\) and \(\tau_{1\to 1}=0\) our lower bound reduces to the lower bound (3) up to a log factor. Moreover, if we take \(n>1\) and \(\tau_{i\to j}=0\) for all \(i,j\in[n],\) then (8) reduces to (5) up to a log factor. Thus, (8) is nearly consistent with the lower bounds from (Arjevani et al., 2022; Tyurin and Richtarik, 2023). We get an extra \(\log n\) factor due to the generality of our setup. The reason is technical, and we explain it in Section E.5. In a nutshell, the lower problem reduces to the analysis of the concentration of the time series \(y^{T}:=\min_{j\in[n]}y_{j}^{T}\) and \(y_{j}^{T}:=\min_{i\in[n]}\left\{y_{i}^{T-1}+h_{i}\eta_{i}^{T}+\tau_{i\to j} \right\},\) where \(y_{i}^{0}=0\) for all \(i\in[n],\) and \(\{\eta_{i}^{k}\}\) are i.i.d. geometric random variables. This analysis is not trivial due to the \(\min_{i\in[n]}\) operations. Virtually all previous works that analyzed lower bounds did not have such a problem because they analyzed time series with a sum structure (e.g., \(\bar{y}^{T}:=\bar{y}^{T-1}+\varrho^{T},\) where \(\{\varrho^{k}\}\) are some random variables, and \(\bar{y}^{0}=0\)).

Let us define an auxiliary function to simplify readability.

**Definition 2** (Equilibrium Time).: A mapping \(t^{*}:\mathbb{R}_{\geq 0}\times\mathbb{R}_{\geq 0}^{n}\times\mathbb{R}_{\geq 0}^{n} \rightarrow\mathbb{R}_{\geq 0}\) with inputs \(s\) (scalar), \([h_{i}]_{i=1}^{n}\) (vector), and \([\bar{\tau}_{i}]_{i=1}^{n}\) (vector) is called the _equilibrium time_ if it is defined as follows. Find a permutation5\(\pi\) that sorts \(\max\{\bar{\tau}_{i},h_{i}\}\) as \(\max\{\bar{\tau}_{\pi_{1}},h_{\pi_{1}}\}\leq\cdots\leq\max\{\bar{\tau}_{\pi_{ n}},h_{\pi_{n}}\}.\) Then the mapping returns the value

Footnote 5: It is possible that a permutation is not unique, then the result of the mapping does not depend on the choice.

\[t^{*}(s,[h_{i}]_{i=1}^{n},[\bar{\tau}_{i}]_{i=1}^{n})\equiv\min_{k\in[n]}\max \left\{\max\{\bar{\tau}_{\pi_{k}},h_{\pi_{k}}\},s\left(\sum\limits_{i=1}^{k} \frac{1}{n_{\pi_{i}}}\right)^{-1}\right\}\in[0,\infty]. \tag{9}\]

## 5 New Method: Fragile SGD

We introduce a novel optimization method characterized by time complexities that closely align with the lower bounds established in Section 4. Our algorithms leverage _spanning trees_. A spanning tree is a tree (undirected unweighted graph) encompassing all workers. The edges of spanning trees are _virtual_ and not related to the edges defined in Section 1.1 (see Fig. 1).

[MISSING_PAGE_FAIL:6]

**Definition 3** (mapping \(\text{next}_{T,j}(i)\)).: Take a spanning tree \(T\) and fix any worker \(j\in[n]\). For \(i=j,\) we define \(\text{next}_{T,j}(i)=0\). For all \(i\neq j\in[n]\), we define \(\text{next}_{T,j}(i)\) as the index of the next worker on the path of the spanning tree \(T\) from worker \(i\) to worker \(j\).

Our new method, Fragile SGD, is presented in Algorithms 2, 4, and 3. While Fragile SGD seems to be lengthy, the idea is pretty simple. All workers do three jobs in parallel: calculate stochastic gradients, receive vectors, and send vectors through spanning trees. A pivot worker aggregates all stochastic gradients in \(g^{k}\) and, at some moment, does \(x^{k+1}=x^{k}-\gamma g^{k}\). The algorithms formalize this idea.

Algorithm 2 requires a starting point \(x^{0}\), a stepsize \(\gamma\), a batch size \(S\), the index \(j^{*}\) of a pivot worker, and spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}}\) for the input. We need two spanning \(\overline{st}_{\text{bc}}\) and \(\overline{st}\) trees because, in general, the fastest communication of a vector from \(j^{*}\) to \(i\) and from \(i\) to \(j^{*}\) should be arranged through two different paths. Algorithm 2 starts \(n+1\) processes running in parallel. Note that the pivot worker \(j^{*}\) runs two parallel processes, called Process \(0\) and Process \(j^{*}\), and any other worker \(i\) runs one Process \(i\). Process \(0\) broadcasts a new point \(x^{k}\) through \(\overline{st}_{\text{bc}}\) to all other processes and goes to the loop where it waits for messages from Process \(j^{*}\). Process \(i\) starts three functions that will be running in parallel: i) the first function's job is to receive a new point, broadcast it further, and start the calculation of stochastic gradients, ii) the second function receives stochastic gradients from all previous processes that are sending vectors to worker \(j^{*}\), iii) the third function sends vectors the next worker on the path to \(j^{*}\). By the definition of \(\text{next}_{\overline{st},j^{*}}(\cdot)\), all calculated stochastic vectors are sent to worker \(j^{*}\), where they are first aggregated in Process \(j^{*}\), and then, since \(\text{next}_{\overline{st},j^{*}}^{*}(j^{*})=0,\) Process \(j^{*}\) will send \(g^{k}_{j^{*},\text{send}}\) to Process \(0\). This process waits for the moment when the number of stochastic gradients \(s^{k}\) aggregated in \(g^{k}\) is greater or equal to \(S\). When it happens, the loop stops, and Process \(0\) does a gradient-like step. The structure of the algorithm and the idea of spanning trees resemble the ideas from (Vogels et al., 2021; Tyurin and Richtarik, 2023). The main observation is that this algorithm is equivalent to \(x^{k+1}=x^{k}-\frac{\gamma}{s^{k}}\sum_{i=1}^{s^{k}}\nabla f(x^{k};\xi^{k}_{i}),\) where \(s^{k}\geq S\) and \(\{\xi^{k}_{i}\}\) are i.i.d. samples. Note that all stochastic gradients calculated at points \(x^{0},\ldots,x^{k-1}\) will be ignored in the \(k^{\text{th}}\) iteration of Algorithm 3.

**Theorem 4**.: _Let Assumptions 1, 2, and 3 hold. We take \(\gamma=1/2L,\) batch size \(S=\max\{\left\lceil\nicefrac{{\sigma^{2}}}{{\varepsilon}}\right\rceil,1\},\) any pivot worker \(j^{*}\in[n],\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}}\) in Algorithm 2. For all \(K\geq 16L\Delta/\varepsilon,\) we get \(\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2} \right]\leq\varepsilon.\)_

The proof is simple and uses standard techniques from (Lan, 2020; Khaled and Richtarik, 2022). The result of the theorem holds even if \(h_{i}=\infty\) and \(\tau_{i\to j}=\infty\) for all \(i,j\in[n]\) because \(h_{i}\) and \(\tau_{i\to j}\) are only upper bounds on the real computation and communications speeds. In Algorithm 3, each iteration \(k\) can be arbitrarily slow, and still, the result of Theorem 4 holds and the method converges after \(O(L\Delta/\varepsilon)\) iterations. The next result gives time complexity guarantees for our algorithm.

**Theorem 5**.: _Consider the assumptions and the parameters from Theorem 4. For any pivot worker \(j^{*}\in[n]\) and spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}}\), Algorithm 2 converges after at most_

\[\Theta\left(\frac{L\Delta}{\varepsilon}t^{*}(\nicefrac{{\sigma^{2}}}{{ \varepsilon}},[h_{i}]_{i=1}^{n},[\mu_{i\to j^{*}}+\mu_{j^{*}\to i}]_{i=1}^{n})\right) \tag{10}\]

_seconds, where \(\mu_{i\to j^{*}}\) (\(\mu_{j^{*}\to i}\)) is an upper bound on the times required to send a vector from worker \(i\) to worker \(j^{*}\) (from worker \(j^{*}\) to worker \(i\)) along the spanning tree \(\overline{st}_{\text{bc}}\))._

Note that our method does not need the knowledge of \(\{h_{i}\}\) and \(\{\mu_{i\to j}\}\) to guarantee the time complexity rate, and it _automatically_ obtains it.

**Corollary 1**.: _Consider the assumptions and the parameters from Theorem 5. Let us take a pivot worker \(j^{*}=\arg\min\limits_{j\in[n]}t^{*}(\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h _{i}]_{i=1}^{n},[\tau_{i\to j}+\tau_{j\to i}]_{i=1}^{n}),\) and a spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\text{bc}}\)) that connects every worker \(i\) to worker \(j^{*}\) (worker \(j^{*}\) to every worker \(i\)) with the shortest distance \(\tau_{i\to j^{*}}\) (\(\tau_{j^{*}\to i}\)). Then Algorithm 2 converges after at most_

\[T_{*} :=\Theta\left(\frac{L\Delta}{\varepsilon}\min\limits_{j\in[n]}t^{*}( \nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i}]_{i=1}^{n},[\tau_{i\to j}+\tau_{j \to i}]_{i=1}^{n})\right) \tag{11}\] \[\stackrel{{\text{Def 2}}}{{=}}\Theta\left(\frac{L\Delta}{ \varepsilon}\min\limits_{j\in[n]}\min\limits_{k\in[n]}\max\left\{\max\{\tau_{ \pi_{j,k}\to j}+\tau_{j\to\pi_{j,k}},h_{\pi_{j,k}}\},\frac{\sigma^{2}}{ \varepsilon}\left(\sum\limits_{i=1}^{k}\frac{1}{h_{\pi_{j,i}}}\right)^{-1} \right\}\right) \tag{12}\]

_seconds, where, for all \(j\in[n],\)\(\pi_{j,\cdot}\) is a permutation that sorts \(\{\max\{\tau_{i\to j}+\tau_{j\to i},h_{i}\}\}_{i=1}^{n}.\)_This corollary has better time complexity guarantees than Theorem 5 because, by the definition of \(\tau_{i\to j},\tau_{i\to j}\leq\mu_{i\to j}\) for al \(i,j\in[n]\). However, it requires the particular choice of a pivot worker and spanning trees.

### Discussion

Comparing the lower bound (7) with the upper bound (11), one can see that Fragile SGD has a _nearly optimal time complexity_. If we ignore the \(\log n\) factor in (7) and assume that \(\tau_{i\to j}=\tau_{j\to i}\) for all \(i,j\in[n],\) which is a weak assumption in many applications, then Fragile SGD is optimal.

Unlike most works (Even et al., 2024; Lian et al., 2018; Koloskova et al., 2021) in the decentralized setting, our time complexity guarantees _do not_ depend on the spectral gap of the mixing matrix that defines the topology of the multigraph. The structure of the multigraph is coded in the times \(\{\tau_{i\to j}\}\). We believe that this is an advantage of our guarantees since (11) defines a physical time instead of an iteration rate that depends on the spectral gap.

### Interpretation of the upper and lower bounds (11) and (7)

One interesting property of our algorithm is that _some workers will potentially never contribute to optimization because either their computations are too slow or communication times to \(j^{*}\) are too large_. Thus, only a subset of the workers should work to get the optimal time complexity!

Assume in this subsection that the computation and communication are fixed to \(\{h_{i}\}\) and \(\{\tau_{i\to j}\}\). One can see that (12) is the \(\min_{j\in[n]}\min_{k\in[n]}\) over some formula. In view of our algorithm, an index \(j^{*}\) that minimizes in (12) is the index of a pivot worker that is the most "central" in the multigraph. An index \(k^{*}\) that minimizes \(\min_{k\in[n]}\) defines a set of workers \(\{\pi_{j^{*},1},\ldots\pi_{j^{*},k^{*}}\}\) that can potentially contribute to optimization. The algorithm and and the time complexity _will not_ depend on workers \(\{\pi_{j^{*},k^{*}+1},\ldots\pi_{j^{*},n}\}\) because they are too slow or they are too far from worker \(j^{*}\). Thus, up to a constant factor, we have \(T_{*}=\nicefrac{{L\Delta}}{{\varepsilon}}\max\big{\{}\max\{\tau_{\pi_{j^{*},k^ {*}}\to j^{*}}+\tau_{j^{*}\to\pi_{j^{*},k^{*}}},h_{\pi_{j^{*},k^{*}}}\},\sigma ^{2}\!/\!\varepsilon\big{(}\sum_{i=1}^{k^{*}}\nicefrac{{1}}{{h_{\pi_{j^{*},i^ {*}}}}}\big{)}^{-1}\big{\}},\) where \(\tau_{\pi_{j^{*},k^{*}}\to j^{*}}+\tau_{j^{*}\to\pi_{j^{*},k^{*}}}\) is the time required to communicate with the farthest worker that can contribute to optimization, \(h_{\pi_{j^{*},k^{*}}}\) is the computation time of the slowest worker that can contribute to optimization, and \(\nicefrac{{\sigma^{2}}}{{\varepsilon}}\big{(}\sum_{i=1}^{k^{*}}\nicefrac{{1}}{ {h_{\pi_{j^{*},i}}}}\big{)}^{-1}\) is the time required to "eliminate" enough noise before the algorithm does an update of \(x^{k}\).

### Limitations

To get the nearly optimal complexity, it is crucial to select the right pivot worker \(j^{*}\) and spanning trees according to the rules of Corollary 1, which depend on the knowledge of the bounds of times. For now, we believe that is this a price for the optimality. Note that Theorem 5 does not require this knowledge and it works with any \(j^{*}\) and any spanning tree; thus, we can use any heuristic to estimate an optimal \(j^{*}\) and optimal spanning trees. One possible strategy is to estimate the performance of workers and the communication channels using load testings.

### Comparison with previous methods

Let us discuss the time complexities of previous methods. Note that none of the previous methods can converge faster than (7) due to our lower bound. First, consider (6) of Minibatch SGD. This time complexity depends on the slowest computation time \(\max_{i\in[n]}h_{i}\) and the slowest communication times \(\max_{i,j\in[n]}\tau_{i\to j}\). In the asynchronous setup, it is possible that one the workers is a straggler, i.e., \(\max_{i\in[n]}h_{i}\approx\infty,\) and Minibatch SGD can be arbitrarily slow. Our time complexities (10) and (12) are robust to stragglers, and ignore them. Assume the last worker \(n\) is a straggler and \(h_{n}=\infty,\) then one can take permutations with \(\pi_{j,n}=n\) for all \(j\in[n],\) and the minimum operator \(\min_{k\in[n]}\) in (12) will not choose \(k=n\) because \(\max\{\tau_{\pi_{j,n}\to j}+\tau_{j\to\pi_{j,n}},h_{\pi_{j,n}}\}=\infty\) for all \(j\in[n]\).

We now consider a recent work by Even et al. (2024), where the authors analyzed Asynchronous SGD in the decentralized setting. In the homogeneous setting, their converge rate depends on the maximum compute delay and, thus, is not robust to stragglers. For the case \(\tau_{i\to j}=0,\) our time complexity (11) reduces to (5). At the same time, it was shown (Tyurin and Richtarik, 2023) that the time complexity of Asynchronous SGD for \(\tau_{i\to j}=0\) is strongly worse than (5); thus, the result by Even et al. (2024) is suboptimal in our setting even if \(\tau_{i\to j}=0\) for all \(i,j\in[n]\). The papers by Bornstein et al. (2023); Lian et al. (2018) also consider the same setting, and they share a logic in that they sample a random worker and _wait_ while it is calculating a stochastic gradient. If one of the workers is a straggler, they can wait arbitrarily long, while our method automatically ignores slow computations.

### Time complexity with dynamic bounds

We can easily generalize Theorem 5 to the case when bounds on the times are not static.

**Theorem 6**.: _Consider the assumptions and the parameters from Theorem 4. In each iteration \(k\) of Algorithm 3, the computation times of worker \(i\) are bounded by \(h_{i}^{k}\). Let us fix any pivot worker \(j^{*}\in[n]\) and any spanning trees \(\overline{st}\) and \(\overline{st_{\text{bc}}}\). Then Algorithm 2 converges after at most_

\[\Theta\left(\sum_{k=0}^{\lceil 16L\Delta/\varepsilon\rceil}t^{*}(\nicefrac{{ \sigma^{2}}}{{\varepsilon}},[h_{i}^{k}]_{i=1}^{n},[\mu_{i\to j^{*}}^{k}+\mu_{ j^{*}\to i}^{k}]_{i=1}^{n})\right) \tag{13}\]

_seconds, where \(\mu_{i\to j^{*}}^{k}\) (\(\mu_{j^{*}\to i}^{k}\)) is an upper bound on times required to send a vector from worker \(i\) to worker \(j^{*}\) (from worker \(j^{*}\) to worker \(i\)) along the spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\text{bc}}\)) in iteration \(k\) of Algorithm 3._

This result is more general than (10), and it shows that our method is robust to changing computation \(\{h_{i}^{k}\}\) and communication \(\{\mu_{i\to j}^{k}\}\) times bounds during optimization processes. For instance, worker \(n\) can have either slow computation or communication to \(j^{*}\) in the first iteration, i.e., \(\max\{h_{n}^{1},\mu_{n\to j^{*}}^{1},\mu_{j^{*}\to n}^{1}\}\approx\infty\), then our method will ignore it, but if \(\max\{h_{n}^{2},\mu_{n\to j^{*}}^{2},\mu_{j^{*}\to n}^{2}\}\) is small in the second iteration, then our method can potentially use the stochastic gradients from worker \(n\).

## 6 Example: Line or Circle

Let us consider Line graphs where we can get more explicit and interpretable formulas for (11). We analyze ND-Mesh, ND-Torus, and Star graphs in Section A. Surprisingly, even in some simple cases like Line or Star graphs, as far as we know, we provide new time complexity results and insights. In Section J, we show that our theoretical results are supported by computational experiments.

We take a Line graph with the computation speeds \(h_{i}=h\) for all \(i\in[n],\) and the communication speeds of the edges \(\rho_{i\to i+1}=\rho_{i+1\to i}=\rho\) for all \(i\in[n-1]\) and \(\rho_{i\to j}=\infty\) for all other \(i,j\in[n]\). One can easily show the time required to send a vector between two workers \(i,j\in[n]\) equals \(\tau_{i\to j}=\tau_{j\to i}=\rho|i-j|.\) See an example with \(n=7\) in Fig. 2. We can substitute these values to (11) and get

\[T_{\text{line}}=\tfrac{L\Delta}{\varepsilon}\min_{j\in[n]}\min_{k\in[n]} \max\left\{\max\{\rho|j-\pi_{j,k}|,h\},\tfrac{\sigma^{2}h}{\varepsilon k} \right\}, \tag{14}\]

where \(\pi_{j,1}=j,\pi_{j,2},\pi_{j,3}=j+1,j-1\) or \(\pi_{j,2},\pi_{j,3}=j-1,j+1\) (only for \(n-1\geq j\geq 2\)) and so forth. For simplicity, assume that \(n\) is odd, then, clearly, \(j^{*}=\tfrac{n-1}{2}+1\) minimizes \(\min_{j\in[n]}\) and \(T_{\text{line}}=\)

\[\tfrac{L\Delta}{\varepsilon}\min_{d\in\{0,\ldots,\tfrac{n-1}{2}\}}\max\left\{ \rho d,h,\tfrac{\sigma^{2}h}{\varepsilon(2d+1)}\right\}\simeq\tfrac{L\Delta}{ \varepsilon}\left[h+\left\{\begin{array}{ll}\nicefrac{{\sigma^{2}h}}{{ \varepsilon}},&\text{if }\nicefrac{{\sqrt{\sigma^{2}h}/\varepsilon}}{{\varepsilon}} \leq 1,\\ \nicefrac{{\sqrt{\rho\sigma^{2}h}/\varepsilon}}{{\varepsilon}},&\text{if }n> \nicefrac{{\sqrt{\sigma^{2}h}/\varepsilon}}{{\varepsilon}}>1,\\ \nicefrac{{\sigma^{2}h}}{{n\varepsilon}},&\text{if }\nicefrac{{\sqrt{\sigma^{2}h}/ \varepsilon}}{{\varepsilon}\geq n}\end{array}\right\}. \tag{15}\]According to (15), there are three time complexity regimes: i) slow communication, i.e., \(\sqrt{\nicefrac{{\sigma^{2}h}}{{\varepsilon\rho}}}\leq 1,\) this inequality means that \(\rho\) is so large, that communication between workers will not increase the convergence speed, and the best strategy is to work with only one worker!, ii) medium communication, i.e., \(n>\sqrt{\nicefrac{{\sigma^{2}h}}{{\varepsilon\rho}}}>1,\) more than one worker will participate in the optimization process; however, _not all of them!_, some workers will not contribute since their distances \(\tau_{j^{*}\rightarrow\cdot}\) to the pivot worker \(j^{*}\) are large, iii) fast communication, i.e., \(\sqrt{\nicefrac{{\sigma^{2}h}}{{\varepsilon\rho}}}\geq n\), all \(n\) workers will participate in optimization because \(\rho\) is small.

As far as we know, the result (15) is new even for such a simple structure as a line. Note that these regimes are fundamental and can not be improved due to our lower bound (up to logarithmic factors). For Circle graphs, the result is the same up to a constant factor.

## 7 Heterogeneous Setup

In Section C (in more details), we consider and analyze the problem

\[\min_{x\in\mathbb{R}^{d}}\Big{\{}f(x):=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{ \xi_{i}\sim\mathcal{D}_{i}}\left[f_{i}(x;\xi_{i})\right]\Big{\}},\]

where \(f_{i}:\mathbb{R}^{d}\times\mathbb{S}_{\xi_{i}}\rightarrow\mathbb{R}^{d}\) and \(\xi_{i}\) are random variables with some distributions \(\mathcal{D}_{i}\) on \(\mathbb{S}_{\xi_{i}}\). For all \(i\in[n],\) worker \(i\) can only access \(f_{i}\). We show that the optimal time complexity is

\[\Theta\left(\frac{L\Delta}{\varepsilon}\max\Big{\{}\max_{i,j\in[n]}\mu_{i \to j},\max_{i\in[n]}h_{i},\frac{\sigma^{2}}{ne}\left(\frac{1}{n}\sum \limits_{i=1}^{n}h_{i}\right)\Big{\}}\right) \tag{16}\]

in the heterogeneous setting achieved by a new method, Amelie SGD (Algorithm 5). Amelie SGD is closely related to Rennala SGD but with essential algorithmic changes to make it work with heterogeneous functions. The obtained complexity (16) is worse than (12), which is expected because the heterogeneous setting is more challenging than the homogeneous setting.

## 8 Highlights of Experiments

In Section J, we present experiments with quadratic optimization problems, logistic regression, and a neural network to substantiate our theoretical findings. Here, we focus on highlighting the results from the logistic regression experiments:

On _MNIST_ dataset (LeCun et al., 2010) with \(100\) workers, Fragile SGD is much faster and has better test accuracy than Minibatch SGD.

## Acknowledgments and Disclosure of Funding

The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST): i) KAUST Baseline Research Scheme, ii) Center of Excellence for Generative AI, under award number 5940, iii) SDAIA-KAUST Center of Excellence in Artificial Intelligence and Data Science. The work of A.T. was partially supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021).

Figure 3: The communication time \(\rho=10\) seconds (Slow communication) in 2D-Mesh

## References

* Arjevani et al. (2022) Arjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J., Srebro, N., and Woodworth, B. (2022). Lower bounds for non-convex stochastic optimization. _Mathematical Programming_, pages 1-50.
* Bornstein et al. (2023) Bornstein, M., Rabbani, T., Wang, E., Bedi, A. S., and Huang, F. (2023). SWIFT: Rapid decentralized federated learning via wait-free model communication. _In The 11th International Conference on Learning Representations (ICLR)_.
* Carmon et al. (2020) Carmon, Y., Duchi, J. C., Hinder, O., and Sidford, A. (2020). Lower bounds for finding stationary points i. _Mathematical Programming_, 184(1):71-120.
* Cohen et al. (2021) Cohen, A., Daniely, A., Drori, Y., Koren, T., and Schain, M. (2021). Asynchronous stochastic optimization robust to arbitrary delays. _Advances in Neural Information Processing Systems_, 34:9024-9035.
* Duchi et al. (2011) Duchi, J. C., Agarwal, A., and Wainwright, M. J. (2011). Dual averaging for distributed optimization: Convergence analysis and network scaling. _IEEE Transactions on Automatic control_, 57(3):592-606.
* Even et al. (2024) Even, M., Koloskova, A., and Massoulie, L. (2024). Asynchronous SGD on graphs: A unified framework for asynchronous decentralized and federated optimization. _In Artificial Intelligence and Statistics. PMLR_.
* Floyd (1962) Floyd, R. W. (1962). Algorithm 97: shortest path. _Communications of the ACM_, 5(6):345-345.
* Ghadimi and Lan (2013) Ghadimi, S. and Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368.
* Huang et al. (2022) Huang, X., Chen, Y., Yin, W., and Yuan, K. (2022). Lower bounds and nearly optimal algorithms in distributed learning with communication compression. _Advances in Neural Information Processing Systems (NeurIPS)_.
* Khaled and Richtarik (2022) Khaled, A. and Richtarik, P. (2022). Better theory for SGD in the nonconvex world. _Transactions on Machine Learning Research_.
* Koloskova (2024) Koloskova, A. (2024). Optimization algorithms for decentralized, distributed and collaborative machine learning. Technical report, EPFL.
* Koloskova et al. (2021) Koloskova, A., Lin, T., and Stich, S. U. (2021). An improved analysis of gradient tracking for decentralized machine learning. _Advances in Neural Information Processing Systems_, 34:11422-11435.
* Koloskova et al. (2022) Koloskova, A., Stich, S. U., and Jaggi, M. (2022). Sharper convergence guarantees for asynchronous SGD for distributed and federated learning. _Advances in Neural Information Processing Systems (NeurIPS)_.
* Lan (2020) Lan, G. (2020). _First-order and stochastic optimization methods for machine learning_. Springer.
* LeCun et al. (2010) LeCun, Y., Cortes, C., and Burges, C. (2010). Mnist handwritten digit database. _ATT Labs [Online]. Available: [http://yann.lecun.com/exdb/mnist_](http://yann.lecun.com/exdb/mnist_), 2.
* Lian et al. (2018) Lian, X., Zhang, W., Zhang, C., and Liu, J. (2018). Asynchronous decentralized parallel stochastic gradient descent. In _International Conference on Machine Learning_, pages 3043-3052. PMLR.
* Liu et al. (2024) Liu, Y., Lin, T., Koloskova, A., and Stich, S. U. (2024). Decentralized gradient tracking with local steps. _Optimization Methods and Software_, pages 1-28.
* Lu and De Sa (2021) Lu, Y. and De Sa, C. (2021). Optimal complexity in decentralized training. In _International Conference on Machine Learning_, pages 7111-7123. PMLR.
* Mishchenko et al. (2022) Mishchenko, K., Bach, F., Even, M., and Woodworth, B. (2022). Asynchronous SGD beats minibatch SGD under arbitrary delays. _Advances in Neural Information Processing Systems (NeurIPS)_.
* Nesterov (1983) Nesterov, Y. (1983). A method of solving a convex programming problem with convergence rate o (1/k** 2). _Doklady Akademii Nauk SSSR_, 269(3):543.
* Nesterov (1983)Nguyen, L., Nguyen, P. H., Dijk, M., Richtarik, P., Scheinberg, K., and Takac, M. (2018). SGD and hogwild! convergence without the bounded gradients assumption. In _International Conference on Machine Learning_, pages 3750-3758. PMLR.
* Recht et al. (2011) Recht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. _Advances in Neural Information Processing Systems_, 24.
* Scaman et al. (2017) Scaman, K., Bach, F., Bubeck, S., Lee, Y. T., and Massoulie, L. (2017). Optimal algorithms for smooth and strongly convex distributed optimization in networks. In _International Conference on Machine Learning_, pages 3027-3036. PMLR.
* Shi et al. (2015) Shi, W., Ling, Q., Wu, G., and Yin, W. (2015). A proximal gradient algorithm for decentralized composite optimization. _IEEE Transactions on Signal Processing_, 63(22):6013-6023.
* Tyurin et al. (2024) Tyurin, A., Pozzi, M., Ilin, I., and Richtarik, P. (2024). Shadowheart SGD: Distributed asynchronous SGD with optimal time complexity under arbitrary computation and communication heterogeneity. _arXiv preprint arXiv:2402.04785_.
* Tyurin and Richtarik (2023) Tyurin, A. and Richtarik, P. (2023). Optimal time complexities of parallel stochastic optimization methods under a fixed computation model. _Advances in Neural Information Processing Systems (NeurIPS)_.
* Van Handel (2014) Van Handel, R. (2014). Probability in high dimension. _Lecture Notes (Princeton University)_.
* Vogels et al. (2021) Vogels, T., He, L., Koloskova, A., Karimireddy, S. P., Lin, T., Stich, S. U., and Jaggi, M. (2021). RelaySum for decentralized deep learning on heterogeneous data. _Advances in Neural Information Processing Systems_, 34.
* Yang et al. (2019) Yang, T., Yi, X., Wu, J., Yuan, Y., Wu, D., Meng, Z., Hong, Y., Wang, H., Lin, Z., and Johansson, K. H. (2019). A survey of distributed optimization. _Annual Reviews in Control_, 47:278-305.

###### Contents

* 1 Introduction
	* 1.1 Decentralized setup with times
* 2 Previous Results
	* 2.1 Time complexity with one worker
	* 2.2 Parallel optimization without communication costs
	* 2.3 Parallel optimization with communication costs \(\tau_{i\to j}\)
* 3 Contributions
* 4 Lower Bound
* 5 New Method: Fragile SGD
	* 5.1 Discussion
	* 5.2 Interpretation of the upper and lower bounds (11) and (7)
	* 5.3 Limitations
	* 5.4 Comparison with previous methods
	* 5.5 Time complexity with dynamic bounds
* 6 Example: Line or Circle
* 7 Heterogeneous Setup
* 8 Highlights of Experiments
* A More Examples
* A.1 ND-Mesh or ND-Torus
* A.2 Star graph
* A.3 General case
* B On the Connection to the Gossip Framework
* C Heterogeneous Setup
* C.1 Lower bound
* C.2 Amelie SGD: optimal method in the heterogeneous setting
* C.3 Discussion
* C.4 Comparison with previous methods
* D Convex Functions in the Homogeneous and Heterogeneous Setups
* D.1 Assumptions in convex world
* D.2 Homogeneous setup and nonsmooth case
* D.3 Homogeneous setup and smooth case

* 4 Heterogeneous setup and nonsmooth case
* 5 Heterogeneous setup and smooth case
* 6 On lower bounds
* 7 Previous works
* E Lower Bound: Diving Deeper into the Construction
* 1 Description of Protocols 8 and 1
* 2 Lower bound
* 3 Proof sketch of Theorem 19
* 4 Full proof of Theorem 19
* 5 Proof of Lemma 2
* 6 Proof of Lemma 3
* F Lower Bound in the Heterogeneous Setup
* G Proof of the Time Complexity for Homogeneous Case
* H Proof of the Time Complexity for Heterogeneous Case
* I Classical SGD Theory
* J Experiments
* 1 Experiments with Logistic Regression: Fast vs Slow Communication
* 1 Experiments with ResNet-18

## Appendix A More Examples

### ND-Mesh or ND-Torus

We now consider a generalization of Line graphs: ND-Mesh graphs. In Figures 3(a) and 3(b), we present examples of 2D-Mesh and 3D-Mesh. For simplicity, assume that \(n=(2k+1)^{N}\) for some \(k\in\mathbb{N}\). The computation speeds \(h_{i}=h\) for all \(i\in[n]\), and the communicate speeds of the edges \(\rho_{i\to j}=\rho\) if workers \(i\) and \(j\) are connected in a mesh and \(\rho_{i\to j}=\infty\) for all other \(i,j\in[n]\). Using geometrical reasoning, it is clear an index \(j^{*}\), that minimizes (11), corresponds to the worker in the middle of a graph (13 in Figures 3(a) and 14 in Figures 3(b)). Therefore,

\[T_{\text{2D-Mesh}}=\Theta\left(\frac{L\Delta}{\varepsilon}\min_{k\in[n]}\max \left\{\tau_{\pi_{j^{*},k}\to j^{*}},h,\frac{\sigma^{2}h}{k\varepsilon} \right\}\right).\]

For now, let us consider a 2D-Mesh graph. The number of workers, that have the length of the shortest path to \(j^{*}\) equals to \(0\), is \(1\). The number of workers, that have the length of the shortest path to \(j^{*}\) less or equal to \(\rho\), is \(5\). The number of workers, that have the length of the shortest path to \(j^{*}\) less or equal to \(2\rho\), is \(13\). In general, the number of workers, that have the length of the shortest path to \(j^{*}\) less or equal to \(\sqrt{k}\rho\), is \(\Theta(k)\) for all \(k\in\left\{0,\ldots,\Theta\left(n\right)\right\}\). It means

\[T_{\text{2D-Mesh}} =\Theta\left(\frac{L\Delta}{\varepsilon}\min_{k\in\left\{0,\ldots,\Theta\left(n\right)\right\}}\max\left\{\sqrt{k}\rho,h,\frac{\sigma^{2}h}{(k +1)\varepsilon}\right\}\right)\] \[=\Theta\left(\frac{L\Delta}{\varepsilon}\left[h+\begin{cases} \frac{\sigma^{2}h}{\varepsilon},&\left(\frac{\sigma^{2}h}{\rho\varepsilon} \right)^{2/3}\leq 1,\\ \rho^{2/3}\left(\frac{\sigma^{2}h}{\varepsilon}\right)^{1/3},&n>\left(\frac{ \sigma^{2}h}{\rho\varepsilon}\right)^{2/3}>1,\\ \frac{\sigma^{2}h}{n\varepsilon},&\left(\frac{\sigma^{2}h}{\rho\varepsilon} \right)^{2/3}\geq n\end{cases}\right]\right).\]

Using the same reasoning, for the general case with a ND-Mesh graph, we get

\[T_{\text{ND-Mesh}} =\Theta\left(\frac{L\Delta}{\varepsilon}\min_{k\in\left\{0, \ldots,\Theta\left(n\right)\right\}}\max\left\{k^{1/N}\rho,h,\frac{\sigma^{2} h}{(k+1)\varepsilon}\right\}\right)\] \[=\Theta\left(\frac{L\Delta}{\varepsilon}\left[h+\begin{cases} \frac{\sigma^{2}h}{\varepsilon},&\left(\frac{\sigma^{2}h}{\rho\varepsilon} \right)^{N/(N+1)}\leq 1,\\ \rho^{N/(N+1)}\left(\frac{\sigma^{2}h}{\varepsilon}\right)^{1/(N+1)},&n>\left( \frac{\sigma^{2}h}{\rho\varepsilon}\right)^{N/(N+1)}>1,\\ \frac{\sigma^{2}h}{n\varepsilon},&\left(\frac{\sigma^{2}h}{\rho\varepsilon} \right)^{N/(N+1)}\geq n\end{cases}\right]\right).\]

As in Line graphs, these complexities have three regimes depending on the problem's parameters. Up to a constant factor, the same conclusions apply to ND-Torus.

Figure 4: Examples of ND-Mesh graphs. For all \(i\neq j\in[n]\), edges \(i\to j\) and \(j\to i\) are merged and visualized with one undirected edge.

### Star graph

Let us consider Star graphs with different computation and communication speeds. Let us fix a graph with \(n+1\) workers, where one worker with the index \(n+1\) is in the center, and all other \(n\) workers are only directly connected to worker \(n+1\). Then,

\[\tau_{i\to j}=\rho_{i\to n+1}+\rho_{n+1\to j}\quad\forall i\neq j\in[n+1]\text{ and }\tau_{i\to i}=0\quad\forall i\in[n].\]

Using these constraints, we can simplify (11). There are two possible best strategies: i) a pivot worker \(j^{*}\) works locally without communications, ii) a pivot worker \(j^{*}\) works with communications but it would necessary require to communicate through the central worker; in this case, the central worker \(n+1\) is a pivot worker. Therefore, we get

\[T_{\text{star}}=\min\left[\frac{L\Delta}{\varepsilon}\min_{j\in[ n]}\max\left\{h_{j},\frac{\sigma^{2}h_{j}}{\varepsilon}\right\}\right.\] \[\left.\frac{L\Delta}{\varepsilon}\min_{k\in[n]}\max\left\{\max\{ \rho_{\pi_{n+1,k}\to n+1}+\rho_{n+1\to\pi_{n+1,k}},h_{\pi_{n+1,k}}\},\frac{ \sigma^{2}}{\varepsilon}\left(\sum_{i=1}^{k}\frac{1}{h_{\pi_{n+1,i}}}\right)^ {-1}\right\}\right]\]

since \(\tau_{n+1\to\pi_{n+1,k}}=\rho_{n+1\to\pi_{n+1,k}}\) for all \(k\in[n+1]\). Let us slightly simplify the result and assume that broadcasting from the central worker is fast, i.e., \(\rho_{n+1\to i}\leq\rho_{i\to n+1}\) for all \(i\in[n+1],\) then

\[T_{\text{star}}=\min\left[\underbrace{\frac{L\Delta}{\varepsilon} \min_{j\in[n]}\max\left\{h_{j},\frac{\sigma^{2}h_{j}}{\varepsilon}\right\}}_{T _{\text{star}}},\right.\] \[\underbrace{\frac{L\Delta}{\varepsilon}\min_{k\in[n]}\max\left\{ \max\{\rho_{\pi_{n+1,k}\to n+1},h_{\pi_{n+1,k}}\},\frac{\sigma^{2}}{\varepsilon }\left(\sum_{i=1}^{k}\frac{1}{h_{\pi_{n+1,i}}}\right)^{-1}\right\}}_{T_{\text {fast}}},\right.\]

Tyurin et al. (2024) also considered Star graphs and showed that \(T_{\text{fast}}\) comm. is the optimal time complexity for methods that communicate through the central worker. Our result is more general since we also consider decentralized methods and capture the term \(T_{\text{slow}}\) comm. that can be potentially smaller if communication is slow. Tyurin et al. (2024) conjectured that \(T_{\text{star}}\) is the optimal bound, and we proved it (up to log factor) here.

### General case

We show how one can use our generic result (11) to get an explicit formula in some cases. For the general case with arbitrary communication and computation times, the minimizers \(j\) and \(k\) in (11) can be found in the following way. First, we have to find \(\tau_{i\to j}\) using any algorithm that solves _the all-pairs shortest path problem_ (e.g., Floyd-Warshall algorithm (Floyd, 1962)). Once we know \(\tau_{i\to j},\) we should sort \(\{\max\{\tau_{i\to j}+\tau_{j\to i},h_{i}\}\}_{k=1}^{n}\) for all \(j\in[n]\) to find the permutations. Finally, we have enough information to calculate \(t^{*}\) from Definition 2.

## Appendix B On the Connection to the Gossip Framework

Most of the previous methods were designed for a different setting, gossip-type communication. In fact, our setting is more general than the gossip communication. Indeed, recall that in the gossip communication, worker \(i\) is allowed to get vectors from other workers through the operation \(\sum_{j=1}^{n}w_{ij}x_{j},\) where \(w_{ij}\in\{0,1\}\). This is equivalent to our setting for the case when the communication time \(\rho_{ij}=\infty\) when \(w_{ij}\) is zero, and \(\rho_{ij}=1\) if \(w_{ij}\) is not zero, and worker \(i\) sums the received vectors. But our setting is richer since we allow different communication and computation times and allow workers to do whatever they like with vectors (not only to sum).

Moreover, the gossip framework codes the communication graph through the matrix \(\{w_{ij}\}\). We propose to code graphs through the times \(\{\rho_{ij}\}\) (\(\{\tau_{ij}\}\)). Our approach is closer to real scenarios because, as we explained previously, it includes the gossip framework.

Heterogeneous Setup

We now consider the heterogeneous setting. The only difference is instead of (1), we consider the following problem.

\[\min_{x\in\mathbb{R}^{d}}\Big{\{}f(x):=\frac{1}{n}\sum_{i=1}^{n}\underbrace{ \mathbb{E}_{\xi_{i}\sim\mathcal{D}_{i}}\left[f_{i}(x;\xi_{i})\right]}_{f_{i}(x) :=}\Big{\}}, \tag{17}\]

where \(f_{i}:\,\mathbb{R}^{d}\times\mathbb{S}_{\xi_{i}}\rightarrow\mathbb{R}^{d}\) and \(\xi_{i}\) are random variables with some distributions \(\mathcal{D}_{i}\) on \(\mathbb{S}_{\xi_{i}}\). We now present our upper and lower bounds and discuss them.

### Lower bound

In Section F, we prove the following lower bound.

**Theorem 7** (Lower Bound; Simplified Presentation of Theorem 20).: _Consider Protocol 1 with \(\nabla f_{i}(\cdot;\cdot)\). We take any \(h_{i}\geq 0\) and \(\tau_{i\to j}\geq 0\) for all \(i,j\in[n]\) such that \(\tau_{i\to j}\leq\tau_{i\to k}+\tau_{k\to j}\) for all \(i,k,j\in[n]\). We fix \(L,\Delta,\varepsilon,\sigma^{2}>0\) that satisfy the inequality \(\varepsilon<cL\Delta\) for some universal constant \(c\). For any (zero-respecting) algorithm, there exists a function \(f=\frac{1}{n}\sum_{i=1}^{n}f_{i},\) which satisfy Assumptions 1, 2 and \(f(0)-f^{*}\leq\Delta\), and stochastic gradient mappings \(\nabla f_{i}(\cdot;\cdot)\), which satisfy Assumption 3 \(\left(\mathbb{E}_{\xi}[\nabla f_{i}(x;\xi)]=\nabla f_{i}(x)\right)\) and \(\mathbb{E}_{\xi}[\|\nabla f_{i}(x;\xi)-\nabla f_{i}(x)\|^{2}]\leq\sigma^{2}\)), such that the required time to find \(\varepsilon\)-solution is_

\[\Omega\left(\frac{L\Delta}{\varepsilon}\max\left\{\max_{i,j\in[n]}\tau_{i \to j},\max_{i\in[n]}h_{i},\frac{\sigma^{2}}{n\varepsilon}\left(\frac{1}{n} \sum_{i=1}^{n}h_{i}\right)\right\}\right)\]

_seconds._

### Amelie SGD: optimal method in the heterogeneous setting

We now present a new method based on Malenia SGD from (Tyurin and Richtarik, 2023) and our Fragile SGD. As in Fragile SGD, we also have \(n+1\) processes running in all workers. The main idea is that all workers calculate stochastic gradients in parallel and accumulate them locally. Process 0 zero waits for the moment when \(\frac{n}{b_{j^{*}}}\geq\frac{S}{n}\). Process \(i\) calculates \(b_{i}\) in Line 20 of Algorithm 7, and sends it to Process \(\text{next}_{\overline{st},j^{*}}(i)\). Thus, \(b_{j^{*}}\) accumulates the sum \(\sum_{i=1}^{n}\frac{1}{s_{i}^{k}},\) which decreases with time since \(s_{i}^{k}\) is the number of calculated stochastic gradients in worker \(i\) in different moments of time. Therefore, \(\frac{n}{b_{j^{*}}}\geq\frac{S}{n}\) will hold at some point of time if workers continue computing gradients. Finally, Algorithm 6 runs all reduce and does the update of \(x^{k}\).

**Theorem 8**.: _Let Assumptions 1 and 2 hold for the function \(f\) and Assumption 3 holds for the functions \(f_{i}\) for all \(i\in[n]\). We take \(\gamma=1/2L,\) the parameter \(S=\max\{\left\lceil\nicefrac{{\sigma^{2}}}{{\varepsilon}}\right\rceil,n\},\) any pivot worker \(j^{*}\in[n],\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\mathrm{bc}},\) in Algorithm 5. For all iterations number \(K\geq 16L\Delta/\varepsilon,\) we get \(\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2 }\right]\leq\varepsilon.\)_

Note that Theorem 8 states the convergence of Amelie SGD even if the computation and communication speeds are unbounded.

**Theorem 9**.: _Consider the assumptions and the parameters from Theorem 8. For any pivot worker \(j^{*}\in[n]\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\mathrm{bc}},\) Algorithm 5 converges after at most_

\[\Theta\left(\frac{L\Delta}{\varepsilon}\max\left\{\max_{i,j\in[n]}\mu_{i \to j},\max_{i\in[n]}h_{i},\frac{\sigma^{2}}{n\varepsilon}\left(\frac{1}{n} \sum_{i=1}^{n}h_{i}\right)\right\}\right)\]

_seconds, where \(\mu_{i\to j^{*}}^{k}\) (\(\mu_{j^{*}\to i}^{k}\)) is an upper bound on times required to send a vector from worker \(i\) to worker \(j^{*}\) (from worker \(j^{*}\) to worker \(i\)) along the spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\mathrm{bc}}\)) for all \(i\in[n]\)._```
1:Input: starting point \(x^{0}\), stepsize \(\gamma\), parameter \(S\), pivot worker \(j^{*}\), spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}}\)
2:Start Process 0 (Alg. 6) in worker \(j^{*}\)
3:Start Process \(i\) (Alg. 7) in all workers for all \(i\in[n]\) (including worker \(j^{*}\))
```

**Algorithm 6** Process 0 (running in worker \(j^{*}\))

```
1:for\(k=0,1,\ldots,K-1\)do
2: Broadcast \(x^{k}\) to all workers using the spanning tree \(\overline{st}_{\text{bc}}\)
3: Init \(s^{k}=0\)
4:while\(s^{k}<\frac{S}{n}\)do
5: Wait for a message \(b_{j^{*}}\) from Process \(j^{*}\)
6: Calculate \(s^{k}=\frac{n}{b_{j^{*}}}\)
7:endwhile
8: Run all reduce with \(\{\frac{1}{s^{k}_{i}}g^{k}_{i}\}_{i=1}^{n}\) to find \(g^{k}=\frac{1}{n}\sum\limits_{i=1}^{n}\frac{1}{s^{k}_{i}}g^{k}_{i}\) using the spanning tree \(\overline{st}\)
9:\(x^{k+1}=x^{k}-\gamma g^{k}\)
10:endfor
```

**Algorithm 7** Process \(i\) (running in worker \(i\))

```
1:while True do
2: Get a new point \(x^{k}\) broadcasted by Process 0
3: Init \((g^{k}_{i},s^{k}_{i})=(0,0)\)
4: Init \(b_{i,p}=\infty\) for all \(p\in[n]\) s.t. \(\text{next}_{\overline{st},j^{*}}(p)=i\)
5: Run the following three functions in parallel and go to Line 24
6:functionCalculateStochasticGradients
7:while True
8: Calculate \(\nabla f_{i}(x^{k};\xi),\quad\xi\sim\mathcal{D}_{i}\)
9: Run atomic add \(g^{k}_{i}=g^{k}_{i}+\nabla f_{i}(x^{k};\xi),s^{k}_{i}=s^{k}_{i}+1\)
10:endwhile
11:endfunction
12:functionReceiveCountersFromPreviousWorkers
13:while True
14: Wait for a message \(b_{p}\) from any Process \(p\) s.t. \(\text{next}_{\overline{st},j^{*}}(p)=i\)
15: Run atomic update \(b_{i,p}=b_{p}\)
16:endwhile
17:endfunction
18:functionSendCounterToNextWorker
19:while True
20: Run atomic sum \(b_{i}=\sum\limits_{p\in[n]:\text{next}_{\overline{st},j^{*}}(p)=i}b_{i,p}+ \frac{1}{s^{k}_{i}}\)
21: Send \(b_{i}\in\mathbb{R}\cup\{\infty\}\) (one float) to Process \(\text{next}_{\overline{st},j^{*}}(i)\) and wait while it sending (Process \(j^{*}\) sends to Process \(0\) by the definition of \(\text{next}_{\overline{st},j^{*}}(\cdot)\))
22:endwhile
23:endfunction
24: Wait for new point. If receives new point, stop all computations in functions, and continue
25: Ignore all non-received messages
26:endwhile
```

**Algorithm 8** Process 0 (running in worker \(j^{*}\))

**Corollary 2**.: _Consider the assumptions and the parameters from Theorem 9. Let us take any pivot worker \(j^{*}\in[n]\) and a spanning tree \(\overline{\operatorname{st}}\) (spanning tree \(\overline{\operatorname{st}}_{\text{bc}}\)) that connects every worker \(i\) to worker \(j^{*}\) (worker \(j^{*}\) to worker \(i\)) with the shortest distance \(\tau_{i\to j^{*}}\) (\(\tau_{j^{*}\to i}\)), then Algorithm 5 converges after at most_

\[\Theta\left(\frac{L\Delta}{\varepsilon}\max\left\{\max_{i,j\in[n]}\tau_{i\to j },\max_{i\in[n]}h_{i},\frac{\sigma^{2}}{n\varepsilon}\left(\frac{1}{n}\sum_{i= 1}^{n}h_{i}\right)\right\}\right) \tag{18}\]

_seconds._

### Discussion

Corollary 2 together with Theorem 7 states that the time complexity (18) is optimal. The result is pessimistic since the time complexity depends on the "diameter" \(\max_{i,j\in[n]}\tau_{i\to j}\) and the slowest performance \(\max_{i\in[n]}h_{i}\). A similar dependence was observed in (Lu and De Sa, 2021; Tyurin and Richtarik, 2023). As in (Tyurin and Richtarik, 2023), the stochastic term \(\nicefrac{{\sigma^{2}}}{{n\varepsilon}}\left(\nicefrac{{1}}{{n}}\sum_{i=1}^{n }h_{i}\right)\) depends on the average of \(\{h_{i}\}\) if \(\nicefrac{{\sigma^{2}}}{{\varepsilon}}\) is large.

### Comparison with previous methods

Let us consider Minibatch SGD described in Section 2.3. This method converges after

\[\Theta\left(\frac{L\Delta}{\varepsilon}\max\bigg{\{}\max_{i,j\in[n]}\tau_{i \to j},\max_{i\in[n]}h_{i},\frac{\sigma^{2}}{n\varepsilon}\max\{\max_{i,j\in[n ]}\tau_{i\to j},\max_{i\in[n]}h_{i}\}\bigg{\}}\right)\]

seconds in the heterogeneous setting. If \(\nicefrac{{\sigma^{2}}}{{\varepsilon}}\) is large, then Amelie SGD can be at least

\[\max\{\max_{i,j\in[n]}\tau_{i\to j},\max_{i\in[n]}h_{i}\}/\left(\frac{1}{n} \sum_{i=1}^{n}h_{i}\right)\]

times faster than Minibatch SGD. There are many more advanced methods (Lu and De Sa, 2021; Vogels et al., 2021; Even et al., 2024) that work in decentralized stochastic heterogeneous setting. Lu and De Sa (2021) developed similar lower and upper bounds, but there are at least three main differences: i) they derived an _iteration complexity_ instead of a time complexity and assumed the performances of all workers are the same ii) the obtained lower bound holds only for one particular multigraph while our complexity holds for any multigraph iii) they assumed the smoothness of \(f_{i}\), while we consider the smoothness of \(f\). The RelaySGD and Gradient Tracking methods by Vogels et al. (2021); Liu et al. (2024) wait for the slowest worker; thus, they depend on \(\max_{i\in[n]}h_{i}\) in all regimes of \(\nicefrac{{\sigma^{2}}}{{\varepsilon}}\), unlike our method. Even et al. (2024) consider the heterogeneous asynchronous setting, but their method assumes the similarity of the functions \(f_{i}\), which is not required in our method, and Amelie SGD converges even if there is no similarity of functions.

## Appendix D Convex Functions in the Homogeneous and Heterogeneous Setups

We will be slightly more brief in the convex setting since the idea, the structure of time complexities, and the general approach do not change significantly. For instance, instead of the time complexity (11) that we get for the nonconvex case, in the nonsmooth convex case, we get (20). Thus, we will get \(\Theta\left(\frac{M^{2}R^{2}}{\varepsilon^{2}}\min_{j\in[n]}t^{*\left(\nicefrac {{\sigma^{2}}}{{M^{2}}},\ldots\right)}\right)\) instead of \(\Theta\left(\frac{L\Delta}{\varepsilon}\min_{j\in[n]}t^{*\left(\nicefrac{{ \sigma^{2}}}{{\varepsilon}},\ldots\right)}\right).\) The same idea applies to the smooth convex case. In the convex setting, we need the following assumptions.

### Assumptions in convex world

**Assumption 4**.: _The function \(f\) is convex and attains a minimum at some point \(x^{*}\in\mathbb{R}^{d}\)._

**Assumption 5**.: _The function \(f\) is \(M\)-Lipschitz, i.e.,_

\[\left|f(x)-f(y)\right|\leq M\left\|x-y\right\|,\quad\forall x,y\in\mathbb{R}^ {d}\]

_for some \(M\in(0,\infty]\)._

**Assumption 6**.: _For all \(x\in\mathbb{R}^{d},\) stochastic (sub)gradients \(\nabla f(x;\xi)\) are unbiased and are \(\sigma^{2}\)-variance-bounded, i.e., \(\mathbb{E}_{\xi\sim\mathcal{D}}\left[\nabla f(x;\xi)\right]\in\partial f(x)\) and \(\mathbb{E}_{\xi\sim\mathcal{D}}\left[\left\|\nabla f(x;\xi)-\mathbb{E}_{\xi\sim \mathcal{D}}\left[\nabla f(x;\xi)\right]\right\|^{2}\right]\leq\sigma^{2},\) where \(\sigma^{2}\geq 0.\)_

### Homogeneous setup and nonsmooth case

**Theorem 10**.: _Let Assumptions 4, 5 and 6 hold. Choose any \(\varepsilon>0.\) Let us take the batch size \(S=\max\left\{\left\lceil\nicefrac{{\sigma^{2}}}{{M^{2}}}\right\rceil,1\right\},\) stepsize \(\gamma=\frac{\varepsilon}{M^{2}+\sigma^{2}/S}\in\left[\frac{\varepsilon}{2M^{ 2}},\frac{\varepsilon}{M^{2}}\right],\) any pivot worker \(j^{*}\in[n],\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}}\) in Algorithm 2. Then after \(K\geq 2M^{2}R^{2}/\varepsilon^{2}\) iterations the method guarantees \(\mathbb{E}\left[f(\widehat{x}^{K})\right]-f(x^{*})\leq\varepsilon,\) where \(\widehat{x}^{K}=\frac{1}{K}\sum_{k=0}^{K-1}x^{k}\) and \(R=\left\|x^{*}-x^{0}\right\|.\)_

Proof.: The proof of Theorem 10 is almost the same as in Theorem 4 (see Section G). The proof of Theorem 4 states that the steps of Fragile SGD are equivalent to the classical SGD method. Thus, we can use the classical result from the literature (Lan, 2020). Using Theorem 22, we get

\[\mathbb{E}\left[f(\widehat{x}^{K})\right]-f(x^{*})\leq\varepsilon\]

if

\[K\geq\frac{2M^{2}\left\|x^{*}-x^{0}\right\|^{2}}{\varepsilon^{2}}\geq\frac{ \left(M^{2}+\frac{\sigma^{2}}{S}\right)\left\|x^{*}-x^{0}\right\|^{2}}{ \varepsilon^{2}}\]

for the stepsize

\[\gamma=\frac{\varepsilon}{M^{2}+\frac{\sigma^{2}}{S}}\in\left[\frac{ \varepsilon}{2M^{2}},\frac{\varepsilon}{M^{2}}\right],\]

where we use the fact that \(S\geq\nicefrac{{\sigma^{2}}}{{M^{2}}}.\) 

**Theorem 11**.: _Consider the assumptions and the parameters from Theorem 10. For any pivot worker \(j^{*}\in[n]\) and spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}},\) Algorithm 2 converges after at most_

\[\Theta\left(\frac{M^{2}R^{2}}{\varepsilon^{2}}t^{*}(\nicefrac{{\sigma^{2}}}{{M ^{2}}},[h_{i}]_{i=1}^{n},[\mu_{i\to j^{*}}+\mu_{j^{*}\to i}]_{i=1}^{n})\right) \tag{19}\]

_seconds, where \(\mu_{i\to j^{*}}\) (\(\mu_{j^{*}\to i}\)) is an upper bound on times required to send a vector from worker \(i\) to worker \(j^{*}\) (from worker \(j^{*}\) to worker \(i\)) along the spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\text{bc}}\))._

Proof.: The proof is identical to the proof of Theorem 5. 

**Corollary 3**.: _Consider the assumptions and the parameters from Theorem 11. Let us take a pivot worker \(j^{*}=\arg\min_{j\in[n]}t^{*}(\nicefrac{{\sigma^{2}}}{{M^{2}}},[h_{i}]_{i=1}^ {n},[\tau_{i\to j}+\tau_{j\to i}]_{i=1}^{n}),\) and a spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\text{bc}}\)) that connects every worker \(i\) to worker \(j^{*}\) (worker \(j^{*}\) to every worker \(i\)) with the shortest distance \(\tau_{i\to j^{*}}\) (\(\tau_{j^{*}\to i}\)). Then Algorithm 2 converges after at most_

\[\Theta\left(\frac{M^{2}R^{2}}{\varepsilon^{2}}\min_{j\in[n]}t^{*}(\nicefrac{{ \sigma^{2}}}{{M^{2}}},[h_{i}]_{i=1}^{n},[\tau_{i\to j}+\tau_{j\to i}]_{i=1}^{n })\right) \tag{20}\]

_seconds._

### Homogeneous setup and smooth case

In the homogeneous and smooth case, we will slightly modify Fragile SGD. Instead of Line 8 of Algorithm 3, we use the following steps:

\[\gamma_{k+1} =\gamma\cdot(k+1),\quad\alpha_{k+1}=2/(k+2) \tag{21}\] \[y^{k+1} =(1-\alpha_{k+1})x^{k}+\alpha_{k+1}u^{k},\qquad(u^{0}=x^{0})\] \[u^{k+1} =u^{k}-\frac{\gamma_{k+1}}{s^{k}}g^{k},\] \[x^{k+1} =(1-\alpha_{k+1})x^{k}+\alpha_{k+1}u^{k+1}.\]

We will call such a method the Accelerated Fragile SGD method. The idea is to use the acceleration technique from (Lan, 2020) (which is based on (Nesterov, 1983)).

**Theorem 12**.: _Let Assumptions 4, 1 and 3 hold. Choose any \(\varepsilon>0.\) Let us take the batch size \(S=\max\left\{\left[\left(\sigma^{2}R\right)/(\varepsilon^{3/2}\sqrt{L})\right], 1\right\},\gamma=\min\left\{\frac{3R^{2}S}{4\sigma^{2}(K+1)(K+2)^{2}}\right\},\) any pivot worker \(j^{*},\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}}\) in Accelerated Method 2 (Accelerated Fragile SGD), then after \(K\geq\frac{8\sqrt{L}R}{\sqrt{\varepsilon}}\) iterations the method guarantees that \(\mathbb{E}\left[f(x^{K})\right]-f(x^{*})\leq\varepsilon,\) where \(R=\left\|x^{*}-x^{0}\right\|.\)_

Proof.: Using the same reasoning as in Theorem 4, Accelerated Fragile SGD is just the classical accelerated stochastic gradient method with a batch size greater or equal to \(S.\) We can use Proposition 4.4 from Lan (2020). For the stepsize

\[\gamma=\min\left\{\frac{1}{4L},\left[\frac{3R^{2}S}{4\sigma^{2}(K+1)(K+2)^{2} }\right]^{1/2}\right\},\]

we have

\[\mathbb{E}\left[f(x^{K})\right]-f(x^{*})\leq\frac{4LR^{2}}{K^{2}}+\frac{4\sqrt {\sigma^{2}R^{2}}}{\sqrt{SK}}.\]

Therefore,

\[\mathbb{E}\left[f(x^{K})\right]-f(x^{*})\leq\varepsilon\]

if

\[K\geq\frac{8\sqrt{L}R}{\sqrt{\varepsilon}}\geq 8\max\left\{\frac{\sqrt{L}R}{ \sqrt{\varepsilon}},\frac{\sigma^{2}R^{2}}{\varepsilon^{2}S}\right\},\]

where we use the choice of \(S.\) 

**Theorem 13**.: _Consider the assumptions and the parameters from Theorem 12. For any pivot worker \(j^{*}\in[n]\) and spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}},\) Accelerated Algorithm 2 (Accelerated Fragile SGD) converges after at most_

\[\Theta\left(\frac{\sqrt{L}R}{\sqrt{\varepsilon}}t^{*}\left(\frac{\sigma^{2}R} {\varepsilon^{3/2}\sqrt{L}},[h_{i}]_{i=1}^{n},[\mu_{i\to j^{*}}+\mu_{j^{*}\to i} ]_{i=1}^{n}\right)\right) \tag{22}\]

_seconds, where \(\mu_{i\to j^{*}}\) (\(\mu_{j^{*}\to i}\)) is an upper bound on times required to send a vector from worker \(i\) to worker \(j^{*}\) (from worker \(j^{*}\) to worker \(i\)) along the spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\text{bc}}\))._

Proof.: The proof is identical to the proof of Theorem 5. 

**Corollary 4**.: _Consider the assumptions and the parameters from Theorem 13. Let us take a pivot worker \(j^{*}=\arg\min_{j\in[n]}t^{*}\left(\frac{\sigma^{2}R}{\varepsilon^{3/2}\sqrt{L }},[h_{i}]_{i=1}^{n},[\tau_{i\to j}+\tau_{j\to i}]_{i=1}^{n}\right),\) and a spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\text{bc}}\)) that connects every worker \(i\) to worker \(j^{*}\) (worker \(j^{*}\) to every worker \(i\)) with the shortest distance \(\tau_{i\to j^{*}}\) (\(\tau_{j^{*}\to i}\)). Then Accelerated Algorithm 2 (Accelerated Fragile SGD) converges after at most_

\[\Theta\left(\frac{\sqrt{L}R}{\sqrt{\varepsilon}}\min_{j\in[n]}t^{*}\left(\frac {\sigma^{2}R}{\varepsilon^{3/2}\sqrt{L}},[h_{i}]_{i=1}^{n},[\tau_{i\to j}+ \tau_{j\to i}]_{i=1}^{n}\right)\right) \tag{23}\]

_seconds._

### Heterogeneous setup and nonsmooth case

Consider the optimization problem (17).

**Theorem 14**.: _Let Assumptions 4, 5 hold for the function \(f\) and Assumption 6 holds for the functions \(f_{i}\) for all \(i\in[n].\) Choose any \(\varepsilon>0.\) Let us take the batch size \(S=\max\left\{\left\lceil\sigma^{2}/M^{2}\right\rceil,n\right\},\)\(\gamma=\frac{\varepsilon}{M^{2}+\sigma^{2}/S}\in\left[\frac{\varepsilon}{2M^{2}}, \frac{\varepsilon}{M^{2}}\right]\), any pivot worker \(j^{*}\in[n],\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}},\) in Algorithm 5, then after \(K\geq 2^{M^{2}R^{2}}/\varepsilon^{2}\) iterations the method guarantees that \(\mathbb{E}\left[f(\widehat{x}^{K})\right]-f(x^{*})\leq\varepsilon,\) where \(\widehat{x}^{K}=\frac{1}{K}\sum_{k=0}^{K-1}x^{k}\) and \(R=\left\|x^{*}-x^{0}\right\|.\)_Proof.: The proof of Theorem 14 is almost the same as in Theorems 8 (see Section H). The proof of Theorem 8 states that the steps of Amelie SGD are equivalent to the classical SGD method. Thus, we can use the classical result from the literature (Lan, 2020). Using Theorem 22, we get

\[\mathbb{E}\left[f(\widehat{x}^{K})\right]-f(x^{*})\leq\varepsilon\]

if

\[K\geq\frac{2M^{2}\left\|x^{*}-x^{0}\right\|^{2}}{\varepsilon^{2}}\geq\frac{(M^ {2}+\frac{\sigma^{2}}{S})\left\|x^{*}-x^{0}\right\|^{2}}{\varepsilon^{2}}\]

for the stepsize

\[\gamma=\frac{\varepsilon}{M^{2}+\frac{\sigma^{2}}{S}}\in\left[\frac{\varepsilon }{2M^{2}},\frac{\varepsilon}{M^{2}}\right],\]

where we use the fact that \(S\geq\nicefrac{{\sigma^{2}}}{{M^{2}}}\). 

**Theorem 15**.: _Consider the assumptions and the parameters from Theorem 14. For any pivot worker \(j^{*}\in[n]\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\mathrm{bc}},\) Algorithm 5 converges after at most_

\[\Theta\left(\frac{M^{2}R^{2}}{\varepsilon^{2}}\max\left\{\max_{i,j\in[n]}\mu_{ i\to j},\max_{i\in[n]}h_{i},\frac{\sigma^{2}}{nM^{2}}\left(\frac{1}{n}\sum_{i=1 }^{n}h_{i}\right)\right\}\right) \tag{24}\]

_seconds, where \(\mu_{i\to j^{*}}^{k}\) (\(\mu_{j^{*}\to i}^{k}\)) is an upper bound on times required to send a vector from worker \(i\) to worker \(j^{*}\) (worker \(j^{*}\) to worker \(i\)) along the spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\mathrm{bc}}\)) for all \(i\in[n].\)_

Proof.: The proof is identical to the proof of Theorem 9. 

**Corollary 5**.: _Consider the assumptions and the parameters from Theorem 15. Let us take any pivot worker \(j^{*}\in[n]\) and a spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\mathrm{bc}}\)) that connects every worker \(i\) to worker \(j^{*}\) (worker \(j^{*}\) to worker \(i\)) with the shortest distance \(\tau_{i\to j^{*}}\) (\(\tau_{j^{*}\to i}\)), then Algorithm 5 converges after at most_

\[\Theta\left(\frac{M^{2}R^{2}}{\varepsilon^{2}}\max\left\{\max_{i,j\in[n]}\tau_ {i\to j},\max_{i\in[n]}h_{i},\frac{\sigma^{2}}{nM^{2}}\left(\frac{1}{n}\sum_{i =1}^{n}h_{i}\right)\right\}\right)\]

_seconds._

### Heterogeneous setup and smooth case

Consider the optimization problem (17). In this section, we use the same idea as in Section D.3. We will modify Amelie SGD and, instead of Line 9 from Algorithm 6, we use the lines (21). We call such a method the Accelerated Amelie SGD method.

**Theorem 16**.: _Let Assumptions 4 and 1 hold for the function \(f\) and Assumption 3 holds for the functions \(f_{i}.\) Choose any \(\varepsilon>0.\) Let us take the batch size \(S=\max\left\{\left\lceil(\sigma^{2}R)/(\varepsilon^{3/2}\sqrt{L})\right\rceil, n\right\},\)_

\[\gamma=\min\left\{\frac{1}{4L},\left\lceil\frac{3R^{2}S}{4\sigma^{2}(K+1)(K+2) ^{2}}\right\rceil^{1/2}\right\},\text{ any pivot worker $j^{*}$},\text{ and any spanning trees $\overline{st}$ and $\overline{st}_{\mathrm{bc}}$}\]

_in Accelerated Method 5 (Accelerated Amelie SGD), then after \(K\geq\frac{8\sqrt{L}R}{\sqrt{\varepsilon}}\) iterations the method guarantees that \(\mathbb{E}\left[f(x^{K})\right]-f(x^{*})\leq\varepsilon,\) where \(R=\left\|x^{*}-x^{0}\right\|.\)_

Proof.: Accelerated Amelie SGD is equivalent to the accelerated stochastic gradient method with a mini-batch from Lan (2020). The proof repeats the proofs of Theorem 12 and Theorem 8. 

**Theorem 17**.: _Consider the assumptions and the parameters from Theorem 16. For any pivot worker \(j^{*}\in[n]\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\mathrm{bc}},\) Accelerated Algorithm 5 (Accelerated Amelie SGD) converges after at most_

\[\Theta\left(\frac{\sqrt{L}R}{\sqrt{\varepsilon}}\max\left\{\max_{i,j\in[n]} \mu_{i\to j},\max_{i\in[n]}h_{i},\frac{\sigma^{2}R}{n\varepsilon^{3/2}\sqrt{L }}\left(\frac{1}{n}\sum_{i=1}^{n}h_{i}\right)\right\}\right) \tag{25}\]

_seconds, where \(\mu_{i\to j^{*}}^{k}\) (\(\mu_{j^{*}\to i}^{k}\)) is an upper bound on times required to send a vector from worker \(i\) to worker \(j^{*}\) (worker \(j^{*}\) to worker \(i\)) along the spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\mathrm{bc}}\)) for all \(i\in[n].\)_Proof.: The proof is identical to the proof of Theorem 9. 

**Corollary 6**.: _Consider the assumptions and the parameters from Theorem 17. Let us take any pivot worker \(j^{*}\in[n]\) and a spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{bc}\)) that connects every worker \(i\) to worker \(j^{*}\) (worker \(j^{*}\) to worker \(i\)) with the shortest distance \(\tau_{i\to j^{*}}\) (\(\tau_{j^{*}\to i}\)), then Accelerated Algorithm 5_ (Accelerated Amelie SGD) _converges after at most_

\[\Theta\left(\frac{\sqrt{L}R}{\sqrt{\varepsilon}}\max\left\{\max_{i,j\in[n]}\tau _{i\to j},\max_{i\in[n]}h_{i},\frac{\sigma^{2}R}{n\varepsilon^{3/2}\sqrt{L}} \left(\frac{1}{n}\sum_{i=1}^{n}h_{i}\right)\right\}\right) \tag{26}\]

_seconds._

### On lower bounds

In previous subsections, we provide upper bounds on the time complexities for different classes of convex functions and optimization problems. Using the same reasoning as in Section 4 and (Tyurin and Richtarik, 2023)[Section B], we conjecture that the obtained upper bounds are tight and optimal (up to log factors in the homogeneous case).

### Previous works

Let us consider the time complexities (20) and (23) (accelerated rate) in the homogeneous and convex cases. When \(\tau_{i\to j}=0\) for all \(i,j\in[n]\), Even et al. (2024) recovers the time complexity (nonaccelerated in the smooth case) of Asynchronous SGD (Mishchenko et al., 2022; Koloskova et al., 2022) which is suboptimal (Tyurin and Richtarik, 2023). When the communication is free, we recover the time complexity (accelerated in the smooth case) of Accelerated Rennala SGD from (Tyurin and Richtarik, 2023), which is optimal if \(\tau_{i\to j}=0\) for all \(i,j\in[n]\).

In the heterogeneous and convex setting, (26) improves the result from (Even et al., 2024) since (26) is an accelerated rate and does not depend on a quantity that measures the similarity of the functions \(f_{i}\). When \(\sigma=0\), the result (26) is consistent with the lower bound from (Scaman et al., 2017). However, when \(\sigma>0\), as far as we know, the time complexity (26) is new in the convex smooth setting.

## Appendix E Lower Bound: Diving Deeper into the Construction

```
1:Input: function \(f\) (or functions \(f_{i}\)) computation oracles \(\{O_{i}\}_{i=1}^{n}\in\mathcal{O}(f)\) communication oracles \(\{C^{p}_{i\to j}\}_{i\in[n],j\in[n],p\geq 1}\), algorithm \(\{(M^{k},L^{k},D^{k},P^{k}_{1},\ldots,P^{k}_{n},V^{k}_{1},\ldots,V^{k}_{n})\}_{ k=0}^{\infty}\ \in\ \mathcal{A}\)
2:\(s^{h,0}_{i}=s^{\tau_{0}}_{i,j,p}=0\) for all \(i,j\in[n],\,p\in\mathbb{N}\)
3:for\(k=0,\ldots,\infty\)do
4:\((t^{k+1},c^{k+1})=M^{k}(g^{1}_{1},\ldots g^{1}_{n},g^{2}_{1},\ldots,g^{2}_{n},\ldots,g^{k}_{1},\ldots,g^{k}_{n})\)\(\triangleright\)\(t^{k+1}\geq t^{k}\)
5:if\(c^{k+1}=0\)then
6:\(i^{k+1}=L^{k}(g^{1}_{1},\ldots g^{1}_{n},g^{2}_{1},\ldots,g^{2}_{n},\ldots,g^ {k}_{1},\ldots,g^{k}_{n})\)
7:\(x^{k}_{i^{k+1}}=P^{k}_{i^{k+1}}(g^{1}_{i^{k+1}},\ldots,g^{k}_{i^{k+1}})\)
8:\((s^{h,k+1}_{i^{k+1}},g^{k+1}_{i^{k+1}})=O_{i^{k+1}}(t^{k+1},x^{k}_{i^{k+1}},s^ {h,k}_{i^{k+1}})\)\(\triangleright\)\(\forall j\neq i^{k+1}\) : \(s^{h,k+1}_{j}=s^{h,k}_{j},\quad g^{k+1}_{j}=0\)
9:else
10:\(i^{k+1},j^{k+1},p^{k+1}=D^{k}(g^{1}_{1},\ldots g^{1}_{n},g^{2}_{1},\ldots,g^{2} _{n},\ldots,g^{k}_{1},\ldots,g^{k}_{n})\)
11:\(v^{k}_{i^{k+1}}=V^{k}_{i^{k+1}}(g^{1}_{i^{k+1}},\ldots,g^{k}_{i^{k+1}})\)
12:\((s^{\tau,k+1}_{i^{k+1},p^{k+1}},g^{k+1}_{j^{k+1}})=C^{p^{k+1}}_{i^{k+1}\to j^{ k+1}}(t^{k+1},v^{k}_{i^{k+1}},s^{\tau,k}_{i^{k+1},j^{k+1},p^{k+1}})\)\(\triangleright\)\(\forall j\neq j^{k+1}:g^{k+1}_{j}=0,\,\forall i\neq i^{k+1},j\neq j^{k+1},p\neq p ^{k+1}:s^{\tau,k+1}_{i,j,p}=s^{\tau,k}_{i,j,p}\)
13:endif
14:endfor
```

**Protocol 8** Time Multiple Oracles Protocol with Communication

In Section 4, we present a brief overview and simplified theorem for the lower bound. We now provide a formal and strict mathematical construction.

### Description of Protocols 8 and 1

One way how we can formalize Protocol 1 is to use Protocol 8. Let us explain it. Using Protocol 8, we consider any possible method that works in our distributed asynchronous setting. The mapping \(M^{k}\) of the algorithm returns the time \(t^{k+1}\) (ignore for now) and \(c^{k+1}\). If \(c^{k+1}=0\), then the algorithm decides to start the calculation of a stochastic gradient: it determines the index \(i^{k+1}\) of a worker that will start the calculation, then, using all locally available information \(g^{1}_{i^{k+1}},\ldots,g^{k}_{i^{k+1}}\), it calculates a new point \(x^{k}_{i^{k+1}},\) and passes this point to the computation oracle \(O_{i^{k+1}}\) that will return a new stochastic gradient after \(h_{i^{k+1}}\) seconds. If \(c^{k+1}=1,\) then the algorithm decides to communicate a vector: it returns the indices \(i^{k+1}\) and \(j^{k+1}\) of two workers that will communicate, and the index \(p^{k+1}\in\mathbb{N}\) of a communication oracle, calculates \(v^{k}_{i^{k+1}}\) in worker \(i^{k+1}\) using only all locally available information \(g^{1}_{i^{k+1}},\ldots,g^{k}_{i^{k+1}},\) and passes it to the communication oracle \(C^{p^{k+1}}_{i^{k+1}\to j^{k+1}}\) that will send the vector after \(\tau_{i^{k+1}\to j^{k+1}}\) seconds.

The computation oracles we define as

\[O_{i} :\underbrace{\mathbb{R}_{\geq 0}}_{\text{time}}\times\underbrace{ \mathbb{R}^{d}}_{\text{point}}\times\underbrace{(\mathbb{R}_{\geq 0}\times\mathbb{R}^{d} \times\{0,1\})}_{\text{input state}}\rightarrow\underbrace{(\mathbb{R}_{ \geq 0}\times\mathbb{R}^{d}\times\{0,1\})}_{\text{output state}}\times\mathbb{R}^{d}\] \[\text{such that}\qquad O_{i}(t,x,(s_{t},s_{x},s_{q}))=\begin{cases}((t,x,1),&0),&s_{q}=0,\\ ((s_{t},s_{x},1),&0),&s_{q}=1,t<s_{t}+h_{i},\\ ((0,0,0),&\nabla f(s_{x};\xi)),&s_{q}=1,t\geq s_{t}+h_{i},\end{cases} \tag{27}\]

where \(\xi\sim\mathcal{D}\).

The communication oracles we define as

\[C^{p}_{i\to j} :\underbrace{\mathbb{R}_{\geq 0}}_{\text{time}}\times\underbrace{ \mathbb{R}^{d}}_{\text{point}}\times\underbrace{(\mathbb{R}_{\geq 0}\times\mathbb{R}^{d} \times\{0,1\})}_{\text{input state}}\rightarrow\underbrace{(\mathbb{R}_{ \geq 0}\times\mathbb{R}^{d}\times\{0,1\})}_{\text{output state}}\times\mathbb{R}^{d}\]

[MISSING_PAGE_FAIL:25]

Constraints 1-5 define domains of the mappings. Constraint 6 is required to ensure that the time sequence \(t^{k}\) does not decrease. Constraint 7 is a standard assumption that an algorithm is zero-respecting (Arjevani et al., 2022).

**Theorem 19**.: _Consider Protocol 8. We take any \(h_{i}\geq 0\) and \(\tau_{i\to j}\geq 0\) for all \(i,j\in[n]\) such that \(\tau_{i\to j}\leq\tau_{i\to k}+\tau_{k\to j}\) for all \(i,k,j\in[n].\) We fix \(L,\Delta,\varepsilon,\sigma^{2}>0\) that satisfy the inequality \(\varepsilon<c^{\prime}L\Delta\). For any algorithm \(A\in\mathcal{A}_{\textit{ar}},\) there exists a function \(f,\) which satisfy Assumptions 1, 2 and \(f(0)-f^{*}\leq\Delta\), and a stochastic gradient mapping \(\nabla f(\cdot;\cdot),\) which satisfy Assumption 3, such that \(\mathbb{E}\left[\inf_{k\in S_{t},i\in[n]}\left\|\nabla f(x_{i}^{k})\right\|^{2 }\right]>\varepsilon,\) where \(S_{t}:=\left\{k\in\mathbb{N}_{0}\,|\,t^{k}\leq t\right\},\)_

\[t=c\times\frac{1}{\log n+1}\frac{L\Delta}{\varepsilon}\min_{j\in[n]}\min_{k\in [n]}\max\left\{\max\{\tau_{\pi_{j,k}\to j},h_{\pi_{j,k}}\},\frac{\sigma^{2}}{ \varepsilon}\left(\sum_{i=1}^{k}\frac{1}{h_{\pi_{j,i}}}\right)^{-1}\right\},\]

_where \(\pi_{j,\cdot}\) is a permutation that sorts \(\max\{h_{i},\tau_{i\to j}\},\) i.e.,_

\[\max\{h_{\pi_{j,1}},\tau_{\pi_{j,1}\to j}\}\leq\cdots\leq\max\{h_{\pi_{j,n}}, \tau_{\pi_{j,n}\to j}\}\]

_for all \(j\in[n].\) The quantities \(c^{\prime}\) and \(c\) are universal constants. The sequences \(x^{k}\) and \(t^{k}\) are defined in Protocol 8._

### Proof sketch of Theorem 19

Let us provide a proof sketch that will give intuition behind the theorem. The full proof starts in Section E.4.

#### Proof Sketch.

**Part 1: Construct a function and stochastic gradient**

The first part of the proof is standard (Carmon et al., 2020; Arjevani et al., 2022; Tyurin and Richtarik, 2023; Huang et al., 2022; Lu and De Sa, 2021), and we delegate it to Section E.4. For any algorithm, we construct oracles and a "worst-case" function such that

\[\inf_{k\in S_{t},i\in[n]}\left\|\nabla f(x_{i}^{k})\right\|^{2}>2 \varepsilon\inf_{k\in S_{t},i\in[n]}\mathbbm{1}\left[\text{prog}(x_{i}^{k})<T \right], \tag{29}\]

where \(\text{prog}(x):=\max\{i\geq 0\,|\,x_{i}\neq 0\}\quad(x_{0}\equiv 1)\) and \(T\approx\nicefrac{{L\Delta}}{{\varepsilon}}.\) This inequality says that while all points in Protocol 8 have the last coordinate equals \(0\) by the time \(t,\) an algorithm can not find an \(\varepsilon\)-stationary point.

Since we assume that \(A\in\mathcal{A}_{\textit{ar}}\) is zero-respecting, the only way to discover the next non-zero coordinate is through stochastic gradients. They are constructed in the following way (Arjevani et al., 2022):

\[[\nabla f(x;\xi)]_{j}:=\nabla_{j}f(x)\left(1+1\,\left[j>\text{prog}(x)\right] \left(\frac{\xi}{p}-1\right)\right)\quad\forall x\in\mathbb{R}^{T},\]

and \(\xi\sim\text{Bernoulli}(p)\) for all \(i\in[n],\) where \(p\approx\nicefrac{{\varepsilon}}{{\sigma^{2}}}.\) We denote \([x]_{j}\) as the \(j^{\text{th}}\) index of a vector \(x\in\mathbb{R}^{T}.\) The stochastic gradient equals to the exact gradient except for the last non-zero coordinate: it zeros out it with the high probability \(1-p.\)

**Part 2: The Level Game**

In essence, the protocol is equivalent to the following collaborative game. Each worker \(i\) starts with level \(\ell_{i}=0.\) The goal is to reach level \(T\) with at least one worker as fast as possible. There are two ways how a worker can increase its level: i) worker \(i\) flips one coin per time from \(\xi\sim\text{Bernoulli}(p),\) it takes \(h_{i}\) seconds to flip one coin, and if the worker is lucky, i.e., \(\xi=1,\) then it moves to the next level \(\ell_{i}=\ell_{i}+1;\) ii) worker \(i\) can share its level with another worker \(j,\) and it takes \(\tau_{i\to j}\) seconds (we are allowed to run this operation again even if the previous is not finished). Both options can be executed in parallel. What is the minimum possible time to reach the game's goal?Since all workers have the levels equal to \(0\) at the beginning, they should flip coins in parallel and wait for the moment when at least one worker moves to level \(1\). We define \(\eta_{i}^{1}\) as the number of flips in worker \(i\) to get \(\xi=1\). Clearly, \(\eta_{i}^{1}\sim\text{Geometric}(p)\) are i.i.d. geometric random variables with the probability \(p\). With any strategy, it is _necessary_ to wait at least

\[y_{j}^{1}:=\min_{i\in[n]}\left\{h_{i}\eta_{i}^{1}+\tau_{i\to j}\right\}\]

seconds to reach level \(1\) in worker \(j\) because once worker \(i\) flips \(\xi=1\), it will take at least \(\tau_{i\to j}\) seconds to share level \(1\) to worker \(j\) due to the triangle inequality \(\tau_{i\to j}\leq\tau_{i\to k}+\tau_{k\to j}\) for all \(i,j,k\in[n]\), and we should minimize \(h_{i}\eta_{i}^{1}+\tau_{i\to j}\) over all workers. We now use mathematical induction to prove that it is necessary to wait at least

\[y_{j}^{T}:=\min_{i\in[n]}\left\{y_{i}^{T-1}+h_{i}\eta_{i}^{T}+\tau_{i\to j}\right\}\]

seconds to reach level \(T,\) where we define \(y_{i}^{0}:=0\) for all \(i\in[n]\) and \(\{\eta_{i}^{T}\}\) are i.i.d. random variables from \(\text{Geometric}(p).\) The base case is proven above. Assume that it is true for \(1,\ldots,T-1,\) then worker \(i\) requires at least \(y_{i}^{T-1}+h_{i}\eta_{i}^{T}\) seconds to flip a coin that moves to level \(T,\) it takes at least \(\tau_{i\to j}\) seconds to share the level \(T,\) so it is necessary to wait at least \(\min_{i\in[n]}\left\{y_{i}^{T-1}+h_{i}\eta_{i}^{T}+\tau_{i\to j}\right\}\) seconds in worker \(i\) to get level \(T.\) Ultimately, the minimum possible time to reach the game's goal is

\[y^{T}:=\min_{j\in[n]}y_{j}^{T}.\]

From the previous result, we can conclude that if \(t\leq\frac{1}{2}y^{T},\) then

\[\inf_{k\in\mathcal{S}_{t},i\in[n]}\left\|\nabla f(x_{i}^{k})\right\|^{2}>2\varepsilon \tag{30}\]

for any algorithm \(A\in\mathcal{A}_{\text{\it{sr}}}.\)

### Part 3: The high probability bound of \(y^{T}\)

Let us fix any deterministic value \(\bar{y}\in\mathbb{R}\) and take

\[t=\frac{1}{2}\bar{y}. \tag{31}\]

Using (30), we have

\[\mathbb{E}\left[\inf_{k\in\mathcal{S}_{t},i\in[n]}\left\|\nabla f(x_{i}^{k}) \right\|^{2}\right]\geq\mathbb{E}\left[\inf_{k\in\mathcal{S}_{t},i\in[n]} \left\|\nabla f(x_{i}^{k})\right\|^{2}\Big{|}y^{T}>\bar{y}\right]\mathbb{P} \left(y^{T}>\bar{y}\right)>\mathbb{P}\left(y^{T}>\bar{y}\right)2\varepsilon.\]

Thus, it is sufficient to find any \(\bar{y}\in\mathbb{R}\) such that

\[\mathbb{P}\left(y^{T}\leq\bar{y}\right)\leq\frac{1}{2}. \tag{32}\]

The sequence \(y^{T}\) is a well-define time series. In Lemma 2, we show that (32) holds with

\[\bar{y}=\Theta\left(\frac{1}{\log n+1}\frac{L\Delta}{\varepsilon}\min_{j\in[n ]}\min_{k\in[n]}\max\left\{\max\{\tau_{\pi_{j,k\to j}},h_{\pi_{j,k}}\},\frac{ \sigma^{2}}{\varepsilon}\left(\sum_{i=1}^{k}\frac{1}{h_{\pi_{j,i}}}\right)^{ -1}\right\}\right),\]

\(\pi_{j,\cdot}\) is a permutation that sorts \(\max\{h_{i},\tau_{i\to j}\},\) i.e.,

\[\max\{h_{\pi_{j,1}},\tau_{\pi_{j,1}\to j}\}\leq\cdots\leq\max\{h_{\pi_{j,n}}, \tau_{\pi_{j,n}\to j}\}\]

for all \(j\in[n].\) We substitute this \(\bar{y}\) to (31) and get the result of theorem.

### Full proof of Theorem 19

This section considers the standard "worst case" function that helps to provide lower bounds in the nonconvex world. Let us define

\[\text{prog}(x):=\max\{i\geq 0\,|\,x_{i}\neq 0\}\quad(x_{0}\equiv 1).\]

For any \(T\in\mathbb{N},\) Carmon et al. (2020); Arjevani et al. (2022) define

\[F_{T}(x):=-\Psi(1)\Phi(x_{1})+\sum_{i=2}^{T}\left[\Psi(-x_{i-1})\Phi(-x_{i})- \Psi(x_{i-1})\Phi(x_{i})\right], \tag{33}\]

where

\[\Psi(x)=\left\{\begin{array}{ll}0,&x\leq 1/2,\\ \exp\left(1-\frac{1}{(2x-1)^{2}}\right),&x\geq 1/2,\end{array}\right.\text{ and }\Phi(x)=\sqrt{e}\int_{-\infty}^{x}e^{-\frac{1}{2}t^{2}}dt.\]

We will only rely on the following facts.

**Lemma 1** (Carmon et al. (2020); Arjevani et al. (2022)).: _The function \(F_{T}\) satisfies:_

1. \(F_{T}(0)-\inf_{x\in\mathbb{R}^{T}}F_{T}(x)\leq\Delta^{0}T,\) _where_ \(\Delta^{0}=12.\)__
2. _The function_ \(F_{T}\) _is_ \(l_{1}\)_-smooth, where_ \(l_{1}=152.\)__
3. _For all_ \(x\in\mathbb{R}^{T},\)__\(\left\|\nabla F_{T}(x)\right\|_{\infty}\leq\gamma_{\infty},\) _where_ \(\gamma_{\infty}=23.\)__
4. _For all_ \(x\in\mathbb{R}^{T},\)__\(\text{prog}(\nabla F_{T}(x))\leq\text{prog}(x)+1.\)__
5. _For all_ \(x\in\mathbb{R}^{T},\) _if_ \(\text{prog}(x)<T,\) _then_ \(\left\|\nabla F_{T}(x)\right\|>1.\)__

**Theorem 19**.: _Consider Protocol 8. We take any \(h_{i}\geq 0\) and \(\tau_{i\to j}\geq 0\) for all \(i,j\in[n]\) such that \(\tau_{i\to j}\leq\tau_{i\to k}+\tau_{k\to j}\) for all \(i,k,j\in[n].\) We fix \(L,\Delta,\varepsilon,\sigma^{2}>0\) that satisfy the inequality \(\varepsilon<c^{\prime}L\Delta.\) For any algorithm \(A\in\mathcal{A}_{\pi},\) there exists a function \(f,\) which satisfy Assumptions 1, 2 and \(f(0)-f^{*}\leq\Delta,\) and a stochastic gradient mapping \(\nabla f(\cdot;\cdot),\) which satisfy Assumption 3, such that \(\mathbb{E}\left[\inf_{k\in S_{t},i\in[n]}\left\|\nabla f(x_{i}^{k})\right\|^{2 }\right]>\varepsilon,\) where \(S_{t}:=\left\{k\in\mathbb{N}_{0}\,|\,t^{k}\leq t\right\},\)_

\[t=c\times\frac{1}{\log n+1}\frac{L\Delta}{\varepsilon}\min_{j\in[n]}\min_{k\in [n]}\max\left\{\max\{\tau_{\pi_{j,k}\to j},h_{\pi_{j,k}}\},\frac{\sigma^{2}}{ \varepsilon}\left(\sum_{i=1}^{k}\frac{1}{h_{\pi_{j,i}}}\right)^{-1}\right\},\]

_where \(\pi_{j,\cdot}\) is a permutation that sorts \(\max\{h_{i},\tau_{i\to j}\},\) i.e.,_

\[\max\{h_{\pi_{j,1}},\tau_{\pi_{j,1}\to j}\}\leq\cdots\leq\max\{h_{\pi_{j,n}},\tau_{\pi_{j,n}\to j}\}\]

_for all \(j\in[n].\) The quantities \(c^{\prime}\) and \(c\) are universal constants. The sequences \(x^{k}\) and \(t^{k}\) are defined in Protocol 8._

Proof.: **Part 1: The Worst Case Function**

This part of the proof mirrors the proofs from Carmon et al. (2020); Arjevani et al. (2022); Tyurin and Richtarik (2023); Huang et al. (2022); Lu and De Sa (2021). We provide it for completeness. The goal of this part to construct a hard instance.

We fix \(\lambda>0,\)\(T\in\mathbb{N}\) and take the function \(f(x):=\frac{L\lambda^{2}}{l_{1}}F_{T}\left(\frac{x}{\lambda}\right),\) where the function \(F_{T}\) is defined in Section E.4. Note that the function \(f\) is \(L\)-smooth:

\[\left\|\nabla f(x)-\nabla f(y)\right\|=\frac{L\lambda}{l_{1}}\left\|\nabla F_ {T}\left(\frac{x}{\lambda}\right)-\nabla F_{T}\left(\frac{y}{\lambda}\right) \right\|\leq L\lambda\left\|\frac{x}{\lambda}-\frac{y}{\lambda}\right\|=L \left\|x-y\right\|\quad\forall x,y\in\mathbb{R}^{d},\]

where \(l_{1}\)-smoothness of \(F_{T}\) (Lemma 1). Let us take

\[T=\left\lfloor\frac{\Delta l_{1}}{L\lambda^{2}\Delta^{0}}\right\rfloor,\]then

\[f(0)-\inf_{x\in\mathbb{R}^{T}}f(x)=\frac{L\lambda^{2}}{l_{1}}(F_{T} \left(0\right)-\inf_{x\in\mathbb{R}^{T}}F_{T}(x))\leq\frac{L\lambda^{2}\Delta^{0 }T}{l_{1}}\leq\Delta.\]

We showed that the function \(f\) satisfy Assumptions 1, 2 and \(f(0)-f^{*}\leq\Delta\).

For each worker \(i\) we take an oracle \(O_{i}\), from (27) with the mapping \(g\) such that

\[[\nabla f(x;\xi)]_{j}:=\nabla_{j}f(x)\left(1+\mathbb{1}\left[j> \text{prog}(x)\right]\left(\frac{\xi}{p}-1\right)\right)\quad\forall x\in \mathbb{R}^{T},\]

and \(\mathcal{D}_{i}=\text{Bernouilli}(p)\) for all \(i\in[n],\) where \(p\in(0,1].\) We denote \([x]_{j}\) as the \(j^{\text{th}}\) index of a vector \(x\in\mathbb{R}^{T}.\) This stochastic gradient is unbiased and \(\sigma^{2}\)-variance-bounded. We have

\[\mathbb{E}\left[[\nabla f(x,\xi)]_{i}\right]=\nabla_{i}f(x)\left( 1+\mathbb{1}\left[i>\text{prog}(x)\right]\left(\frac{\mathbb{E}\left[\xi \right]}{p}-1\right)\right)=\nabla_{i}f(x)\]

for all \(i\in[T],\) and

\[\mathbb{E}\left[\left\|\nabla f(x;\xi)-\nabla f(x)\right\|^{2} \right]\leq\max_{j\in[T]}\left|\nabla_{j}f(x)\right|^{2}\mathbb{E}\left[\left( \frac{\xi}{p}-1\right)^{2}\right]\]

because the difference is non-zero only in one coordinate. Thus

\[\mathbb{E}\left[\left\|\nabla f(x,\xi)-\nabla f(x)\right\|^{2}\right] \leq\frac{\left\|\nabla f(x)\right\|_{\infty}^{2}\left(1-p\right) }{p}=\frac{L^{2}\lambda^{2}\left\|\nabla F_{T}\left(\frac{x}{\lambda}\right) \right\|_{\infty}^{2}\left(1-p\right)}{l_{1}^{2}p}\] \[\leq\frac{L^{2}\lambda^{2}\gamma_{\infty}^{2}(1-p)}{l_{1}^{2}p} \leq\sigma^{2},\]

where we use Lemma 1 and take

\[p=\min\left\{\frac{L^{2}\lambda^{2}\gamma_{\infty}^{2}}{\sigma^{2}l_{1}^{2}},1 \right\}.\]

Let us take

\[\lambda=\frac{\sqrt{2\varepsilon}l_{1}}{L}\]

to ensure that

\[\left\|\nabla f(x)\right\|^{2}=\frac{L^{2}\lambda^{2}}{l_{1}^{2}} \left\|\nabla F_{T}\left(\frac{x}{\lambda}\right)\right\|^{2}=2\varepsilon \left\|\nabla F_{T}\left(\frac{x}{\lambda}\right)\right\|^{2}\]

for all \(x\in\mathbb{R}^{T}.\) From Lemma 1, we know that if \(\text{prog}(x)<T,\) then \(\left\|\nabla F_{T}(x)\right\|>1.\) Thus, we get

\[\left\|\nabla f(x)\right\|^{2}>2\varepsilon\mathbb{1}\left[\text{prog}(x)<T\right] \tag{34}\]

Therefore,

\[T=\left\lfloor\frac{\Delta L}{2\varepsilon l_{1}\Delta^{0}}\right\rfloor \tag{35}\]

and

\[p=\min\left\{\frac{2\varepsilon\gamma_{\infty}^{2}}{\sigma^{2}},1\right\}. \tag{36}\]

The inequality (34) implies

\[\inf_{k\in S_{t},i\in[n]}\left\|\nabla f(x_{i}^{k})\right\|^{2}>2 \varepsilon\inf_{k\in S_{t},i\in[n]}\mathbb{1}\left[\text{prog}(x_{i}^{k})<T \right], \tag{37}\]

where \(\{x_{i}^{k}\}_{k=0}^{\infty}\) are sequences from Protocol 8.

**Part 2: Reduction to Lower Bound Time Series**

We now focus on (27). In (27), when the oracle in worker \(i\) calculates a new stochastic gradient, it samples a random variable \(\xi\sim\mathcal{D}\). This is equivalent to the procedure if we had \(T\) infinite sequences of i.i.d. Bernoulli random variables

\[\xi_{i}^{1,1},\xi_{i}^{1,2},\ldots (\text{prog}(s_{x})=0)\] \[\xi_{i}^{2,1},\xi_{i}^{2,2},\ldots (\text{prog}(s_{x})=1)\] \[\ldots\] \[\xi_{i}^{T,1},\xi_{i}^{T,2},\ldots (\text{prog}(s_{x})=T-1)\]

for all \(i\in[n]\) and the oracle would look at the progress of \(s_{x}\) and take the next non-taken Bernoulli random variable in the sequence that corresponds to that progress. For instance, if \(\text{prog}(s_{x})=j\) for the first time in worker \(i\), then the oracle will apply \(\xi_{i}^{j,1}\) in (27). The next time when it gets \(\text{prog}(s_{x})=j\) it will apply \(\xi_{i}^{j,2}\) and so on. One by one, we get i.i.d. Bernoulli random variables.

Let us define

\[\eta_{i}^{k}=\inf\{j\in\mathbb{N}\,|\,\xi_{i}^{k,j}=1\}\]

for all \(i\in[n]\) and \(k\in[T]\). This is the first Bernoulli random variable from the sequence \(\xi_{i}^{k,1},\xi_{i}^{k,2},\ldots\) that equals \(1\). The random variables \(\{\eta_{i}^{k}\}\) are i.i.d. geometrically distributed random variables with the probability \(p\).

The next steps mirror _Proof Sketch_ from Theorem 19. The oracles constructed in such a way that it takes \(h_{i}\) seconds to calculate a stochastic gradient, and at least \(\tau_{i\to j}\) seconds to send a vector from one worker to another. Using the same reasoning as in the Level Game in _Proof Sketch_ of Theorem 19, the first time moment when worker \(j\) can get a vector with the first non-zero coordinate greater or equal

\[y_{j}^{1}:=\min_{i\in[n]}\big{\{}h_{i}\eta_{i}^{1}+\tau_{i\to j}\big{\}}\]

because, at the beginning, each worker \(i\) calculates stochastic gradients with \(\text{prog}(s_{x})=0\) and should wait at least \(h_{i}\eta_{i}^{1}\) seconds to get a stochastic gradient with the progress equals \(1\). Then, it can share this vector with any other worker \(j\), but it takes at least \(\tau_{i\to j}\) seconds.

As in _Proof Sketch_ of Theorem 19, we can use mathematical induction to prove that it is necessary to wait at least

\[y_{j}^{T}:=\min_{i\in[n]}\big{\{}y_{i}^{T-1}+h_{i}\eta_{i}^{T}+\tau_{i\to j}\big{\}}\]

seconds to get a point such that \(\text{prog}(s_{x})=T\). The base case for \(y_{j}^{1}\) has been proven. Worker \(i\) requires at least \(y_{i}^{T-1}+h_{i}\eta_{i}^{T}\) seconds to wait for the moment when the corresponding oracle will return a stochastic gradient with progress \(T\) because \(y_{i}^{T-1}\) is the first time possible time to get a vector with progress \(T-1\) by the induction, and it will take at least additional \(h_{i}\eta_{i}^{T}\) seconds to calculate \(\eta_{i}^{T}\) vectors with \(\text{prog}(s_{x})=T-1\) in (27). Also, it takes at least \(\tau_{i\to j}\) seconds to share a vector, so it is necessary to wait at least \(\min_{i\in[n]}\{y_{i}^{T-1}+h_{i}\eta_{i}^{T}+\tau_{i\to j}\}\) seconds in worker \(i\) to get the first vector with progress \(T\).

In the end, the fastest possible time to get a vector with progress \(T\) is at least

\[y^{T}:=\min_{j\in[n]}y_{j}^{T}.\]

**Part 3: Reduction to the concentration of \(y^{T}\)**

The last statement means that \(\text{prog}(x_{i}^{k})<T\) for all \(i\in[n]\) and \(k\) such that \(t^{k}\leq\frac{1}{2}y^{T}\). Therefore, we obtain

\[\inf_{k\in\mathcal{S}_{i},i\in[n]}\big{\|}\nabla f(x_{i}^{k})\big{\|}^{2}>2\varepsilon \tag{38}\]

for

\[t\leq\frac{1}{2}y^{T},\]

where

\[T=\left\lfloor\frac{\Delta L}{2\varepsilon l_{1}\Delta^{0}}\right\rfloor= \left\lfloor c_{T}\cdot\frac{\Delta L}{\varepsilon}\right\rfloor\]and \(\eta_{i}^{j}\sim\text{Geometric}(p)\) with

\[p=\min\left\{\frac{2\varepsilon\gamma_{\infty}^{2}}{\sigma^{2}},1\right\}=\min \left\{c_{p}\cdot\frac{\varepsilon}{\sigma^{2}},1\right\},\]

where \(c_{T}=3648^{-1}\) and \(c_{p}=1058\) are universal constants. Let us fix any deterministic value \(\bar{y}\in\mathbb{R}\) and take

\[t=\frac{1}{2}\bar{y}. \tag{39}\]

Using (38), we have

\[\mathbb{E}\left[\inf_{k\in S_{t},i\in[n]}\left\|\nabla f(x_{i}^{k})\right\|^{ 2}\right]\geq\mathbb{E}\left[\inf_{k\in S_{t},i\in[n]}\left\|\nabla f(x_{i}^{k })\right\|^{2}\right|y^{T}>\bar{y}\right]\mathbb{P}\left(y^{T}>\bar{y}\right)> \mathbb{P}\left(y^{T}>\bar{y}\right)2\varepsilon.\]

Thus, it is sufficient to find any \(\bar{y}\in\mathbb{R}\) such that

\[\mathbb{P}\left(y^{T}\leq\bar{y}\right)\leq\frac{1}{2}. \tag{40}\]

In the following lemma we use the notation of this theorem. We prove it in Section E.5.

**Lemma 2**.: _With_

\[\bar{y}=c\times\frac{1}{\log n+1}\frac{L\Delta}{\varepsilon}\min_{j\in[n]} \min_{k\in[n]}\max\left\{\max\{\tau_{\pi_{j,k}\to j},h_{\pi_{j,k}}\},\frac{ \sigma^{2}}{\varepsilon}\left(\sum_{i=1}^{k}\frac{1}{h_{\pi_{j,i}}}\right)^{-1 }\right\}, \tag{41}\]

_we have \(\mathbb{P}\left(y^{T}\leq\bar{y}\right)\leq\frac{1}{2},\) where \(\pi_{j,\cdot}\) is a permutation that sorts \(\max\{h_{i},\tau_{i\to j}\},\) i.e.,_

\[\max\{h_{\pi_{j,1}},\tau_{\pi_{j,1}\to j}\}\leq\cdots\leq\max\{h_{\pi_{j,n}}, \tau_{\pi_{j,n}\to j}\}\]

_for all \(j\in[n].\) The quantity \(c\) is a universal constant._

Using Lemma 2, we can conclude that (40) holds and

\[\mathbb{E}\left[\inf_{k\in S_{t},i\in[n]}\left\|\nabla f(x_{i}^{k})\right\|^{ 2}\right]>\varepsilon\]

for

\[t=\frac{c}{2}\times\frac{1}{\log n+1}\frac{L\Delta}{\varepsilon}\min_{j\in[n] }\min_{k\in[n]}\max\left\{\max\{\tau_{\pi_{j,k}\to j},h_{\pi_{j,k}}\},\frac{ \sigma^{2}}{\varepsilon}\left(\sum_{i=1}^{k}\frac{1}{h_{\pi_{j,i}}}\right)^{-1 }\right\}.\]

### Proof of Lemma 2

**Lemma 2**.: _With_

\[\bar{y}=c\times\frac{1}{\log n+1}\frac{L\Delta}{\varepsilon}\min_{j\in[n]} \min_{k\in[n]}\max\left\{\max\{\tau_{\pi_{j,k}\to j},h_{\pi_{j,k}}\},\frac{ \sigma^{2}}{\varepsilon}\left(\sum_{i=1}^{k}\frac{1}{h_{\pi_{j,i}}}\right)^{-1 }\right\}, \tag{41}\]

_we have \(\mathbb{P}\left(y^{T}\leq\bar{y}\right)\leq\frac{1}{2},\) where \(\pi_{j,\cdot}\) is a permutation that sorts \(\max\{h_{i},\tau_{i\to j}\},\) i.e.,_

\[\max\{h_{\pi_{j,1}},\tau_{\pi_{j,1}\to j}\}\leq\cdots\leq\max\{h_{\pi_{j,n}}, \tau_{\pi_{j,n}\to j}\}\]

_for all \(j\in[n].\) The quantity \(c\) is a universal constant._

Proof.: Using the Chernoff method for any \(s>0,\) we get

\[\mathbb{P}\left(y^{k}\leq t\right) =\mathbb{P}\left(-sy^{k}\geq-st\right)=\mathbb{P}\left(e^{-sy^{k} }\geq e^{-st}\right)\leq e^{st}\mathbb{E}\left[e^{-sy^{k}}\right]\] \[=e^{st}\mathbb{E}\left[\exp\left(-s\min_{j\in[n]}y_{j}^{k}\right) \right]=e^{st}\mathbb{E}\left[\max_{j\in[n]}\exp\left(-sy_{j}^{k}\right) \right].\]We have a maximum operation that complicates the analysis. In response to this problem, we use a well-known trick that bounds a maximum by a sum.

\[\mathbb{P}\left(y^{k}\leq t\right)\leq e^{st}\sum_{j=1}^{n}\mathbb{E}\left[\exp \left(-sy_{j}^{k}\right)\right]\leq ne^{st}\max_{j\in[n]}\mathbb{E}\left[\exp \left(-sy_{j}^{k}\right)\right]. \tag{42}\]

We refer to (Van Handel, 2014)(Part II) for the explanation why it can be (almost) tight. This is the main reason why we get an extra \(\log n\) factor in (41). Let us consider the last exponent separately and use the same trick again:

\[\mathbb{E}\left[\exp\left(-sy_{j}^{k}\right)\right] =\mathbb{E}\left[\exp\left(-s\min_{i\in[n]}\left\{y_{i}^{k-1}+h_{ i}\eta_{i}^{k}+\tau_{i\to j}\right\}\right)\right]\] \[=\mathbb{E}\left[\max_{i\in[n]}\exp\left(-s\left\{y_{i}^{k-1}+h_{ i}\eta_{i}^{k}+\tau_{i\to j}\right\}\right)\right].\]

Next, we get

\[\mathbb{E}\left[\exp\left(-sy_{j}^{k}\right)\right] \leq\sum_{i=1}^{n}\mathbb{E}\left[\exp\left(-s\left\{y_{i}^{k-1} +h_{i}\eta_{i}^{k}+\tau_{i\to j}\right\}\right)\right]\] \[=\sum_{i=1}^{n}\mathbb{E}\left[e^{-s\left(h_{i}\eta_{i}^{k}+\tau_{ i\to j}\right)}\right]\mathbb{E}\left[\exp\left(-sy_{i}^{k-1}\right)\right].\]

In the last equality we use the independence. We now bound \(\mathbb{E}\left[\exp\left(-sy_{i}^{k-1}\right)\right]\) by \(\max_{i\in[n]}\mathbb{E}\left[\exp\left(-sy_{i}^{k-1}\right)\right]\) and get

\[\mathbb{E}\left[\exp\left(-sy_{j}^{k}\right)\right] \leq\sum_{i=1}^{n}\mathbb{E}\left[e^{-s\left(h_{i}\eta_{i}^{k}+ \tau_{i\to j}\right)}\right]\mathbb{E}\left[\exp\left(-sy_{i}^{k-1}\right)\right]\] \[\leq\left(\sum_{i=1}^{n}\mathbb{E}\left[e^{-s\left(h_{i}\eta_{i} ^{k}+\tau_{i\to j}\right)}\right]\right)\max_{i\in[n]}\mathbb{E}\left[\exp \left(-sy_{i}^{k-1}\right)\right].\]

Let us fix any \(s_{j}>0\) for all \(j\in[n]\) and take \(s=\max_{j\in[n]}s_{j}\). Then

\[\mathbb{E}\left[\exp\left(-sy_{j}^{k}\right)\right] \leq\left(\sum_{i=1}^{n}\mathbb{E}\left[e^{-s_{j}\left(h_{i}\eta_{i} ^{k}+\tau_{i\to j}\right)}\right]\right)\max_{i\in[n]}\mathbb{E}\left[\exp \left(-sy_{i}^{k-1}\right)\right]. \tag{43}\]

Let us fix \(\bar{t}_{j}>0\) and consider

\[a_{j}:=\sum_{i=1}^{n}\mathbb{E}\left[e^{-s_{j}\left(h_{i}\eta_{i}^{k}+\tau_{i \to j}\right)}\right]. \tag{44}\]

Then, we can use the following inequalities:

\[a_{j} \leq\sum_{i=1}^{n}\mathbb{E}\left[e^{-s_{j}\left(h_{i}\eta_{i}^{k }+\tau_{i\to j}\leq\bar{t}_{j}\right)}\mathbb{1}\left[h_{i}\eta_{i}^{k}+\tau_ {i\to j}\leq\bar{t}_{j}\right]+e^{-s_{j}\left(h_{i}\eta_{i}^{k}+\tau_{i\to j }\leq\bar{t}_{j}\right)}\left(1-\mathbb{1}\left[h_{i}\eta_{i}^{k}+\tau_{i\to j }\leq\bar{t}_{j}\right]\right)\right]\] \[=ne^{-s_{j}\bar{t}_{j}}+(1-e^{-s_{j}\bar{t}_{j}})\sum_{i=1}^{n} \mathbb{E}\left[\mathbb{1}\left[h_{i}\eta_{i}^{k}+\tau_{i\to j}\leq\bar{t}_{j }\right]\right]\] \[\leq ne^{-s_{j}\bar{t}_{j}}+\sum_{i=1}^{n}\mathbb{E}\left[\mathbb{ 1}\left[h_{i}\eta_{i}^{k}+\tau_{i\to j}\leq\bar{t}_{j}\right]\right]\] \[\leq ne^{-s_{j}\bar{t}_{j}}+\sum_{i=1}^{n}\mathbb{P}\left(h_{i} \eta_{i}^{k}+\tau_{i\to j}\leq\bar{t}_{j}\right)\] \[\leq ne^{-s_{j}\bar{t}_{j}}+\sum_{i=1}^{n}\mathbb{P}\left(h_{i} \eta_{i}^{k}\leq\bar{t}_{j}\right)\mathbb{1}\left[\tau_{i\to j}\leq\bar{t}_{j} \right].\]Since \(\eta_{i}^{k}\sim\text{Geometric}(p),\) we get

\[\mathbb{P}\left(h_{i}\eta_{i}^{k}\leq\bar{t}_{j}\right)=1-(1-p)^{ \left\lfloor\frac{\bar{t}_{j}}{h_{i}}\right\rfloor}\leq p\left\lfloor\frac{\bar {t}_{j}}{h_{i}}\right\rfloor.\]

Then

\[a_{j}\leq ne^{-s_{j}\bar{t}_{j}}+p\sum_{i=1}^{n}\left\lfloor \frac{\bar{t}_{j}}{h_{i}}\right\rfloor\mathbb{1}\left[\tau_{i\to j}\leq \bar{t}_{j}\right].\]

For all \(j\in[n],\) we take any permutation \(\pi_{j,\cdot}\) that sorts \(\max\{h_{i},\tau_{i\to j}\},\) i.e.,

\[\max\{h_{\pi_{j,1}},\tau_{\pi_{j,1}\to j}\}\leq\cdots\leq\max\{h_{ \pi_{j,n}},\tau_{\pi_{j,n}\to j}\}.\]

We have

\[a_{j}\leq ne^{-s_{j}\bar{t}_{j}}+p\sum_{i=1}^{n}\left\lfloor \frac{\bar{t}_{j}}{h_{\pi_{j,i}}}\right\rfloor\mathbb{1}\left[\tau_{\pi_{j,i} \to j}\leq\bar{t}_{j}\right]. \tag{45}\]

Recall that \(\bar{t}_{j}>0\) is a parameter. In the following technical lemma, we choose \(\bar{t}_{j}\) and show that the second term in (45) is "small." We prove it in Section E.6.

**Lemma 3**.: _For any \(n\geq 1,\)\(h_{i}\geq 0,\)\(\tau_{i\to j}\geq 0\) for all \(i,j\in[n],\) and \(p\in(0,1],\) we have_

\[p\sum_{i=1}^{n}\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}} \right\rfloor\mathbb{1}\left[\tau_{\pi_{j,i}\to j}\leq\bar{t}_{j} \right]\leq\frac{1}{8}\]

_for all \(j\in[n],\) where_

\[\bar{t}_{j}:=\frac{1}{8}\min_{k\in[n]}\max\left\{\max\{\tau_{\pi_ {j,k}\to j},h_{\pi_{j,k}}\},\left(\sum_{i=1}^{k}\frac{p}{h_{\pi_{j,i}}} \right)^{-1}\right\}. \tag{46}\]

_and \(\pi_{j,\cdot}\) is a permutation that sorts \(\max\{h_{i},\tau_{i\to j}\},\) i.e.,_

\[\max\{h_{\pi_{j,1}},\tau_{\pi_{j,1}\to j}\}\leq\cdots\leq\max\{h_{ \pi_{j,n}},\tau_{\pi_{j,n}\to j}\}\]

_for all \(j\in[n].\)_

Using Lemma 3 and (45), we get

\[a_{j}\leq ne^{-s_{j}\bar{t}_{j}}+\frac{1}{8}\]

for all \(j\in[n].\) Let us take

\[s_{j}=\frac{\log 8n}{\bar{t}_{j}} \tag{47}\]

to get

\[a_{j}\leq\frac{1}{8}+\frac{1}{8}\leq e^{-1}.\]

for all \(j\in[n].\) Using (44) and (43), we obtain

\[\mathbb{E}\left[\exp\left(-sy_{j}^{k}\right)\right]\leq e^{-1} \max_{i\in[n]}\mathbb{E}\left[\exp\left(-sy_{i}^{k-1}\right)\right], \tag{48}\]

for all \(j\in[n].\) We can conclude that

\[\max_{i\in[n]}\mathbb{E}\left[\exp\left(-sy_{i}^{k}\right)\right] \leq e^{-1}\max_{i\in[n]}\mathbb{E}\left[\exp\left(-sy_{i}^{k-1}\right) \right]\leq e^{-k},\]

where use the same reasoning in the recursion and \(y_{j}^{0}=0\) for all \(j\in[n].\) We substitute the inequality to (42):

\[\mathbb{P}\left(y^{k}\leq t\right)\leq ne^{st-k}=e^{st-k+\log n}\]It is sufficient to take \(k=T\) and any

\[t \leq\frac{1}{s}\left(T-\log n+\log\frac{1}{2}\right)\] \[=\frac{1}{8\log 8n}\min_{i\in[n]}\min_{k\in[n]}\max\left\{\max\{ \tau_{\pi_{j,k}\to j},h_{\pi_{j,k}}\},\left(\sum_{i=1}^{k}\frac{p}{h_{\pi_{j,i} }}\right)^{-1}\right\}\left(T-\log n+\log\frac{1}{2}\right)\]

to get

\[\mathbb{P}\left(y^{T}\leq t\right)\leq\frac{1}{2},\]

where we use the definitions \(s=\max_{i\in[n]}s_{i}\), (46) and (47). Recall the choice of \(T\) and \(p\) in (35) and (36): \(T=\left\lfloor c_{T}\cdot\frac{\Delta L}{\varepsilon}\right\rfloor\) and \(p=\min\left\{c_{p}\cdot\frac{\varepsilon}{\sigma^{2}},1\right\}\) for some universal constants \(c_{T}\) and \(c_{p}\). Since we have the condition \(\varepsilon<c^{\prime}L\Delta\) for some universal constant \(c^{\prime}\) in the conditions of Theorem 19, we can conclude that we can take

\[t=c\times\frac{1}{\log n+1}\frac{L\Delta}{\varepsilon}\min_{i\in[n]}\min_{k\in [n]}\max\left\{\max\{\tau_{\pi_{j,k}\to j},h_{\pi_{j,k}}\},\frac{\sigma^{2}}{ \varepsilon}\left(\sum_{i=1}^{k}\frac{1}{h_{\pi_{j,i}}}\right)^{-1}\right\},\]

where \(c\) is a universal constant. 

### Proof of Lemma 3

**Lemma 3**.: _For any \(n\geq 1,\)\(h_{i}\geq 0\),\(\tau_{i\to j}\geq 0\) for all \(i,j\in[n],\) and \(p\in(0,1],\) we have_

\[p\sum_{i=1}^{n}\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}}\right\rfloor 1\left[\tau_{\pi_{j,i}\to j}\leq\bar{t}_{j}\right]\leq\frac{1}{8}\]

_for all \(j\in[n],\) where_

\[\bar{t}_{j}:=\frac{1}{8}\min_{k\in[n]}\max\left\{\max\{\tau_{\pi_{j,k}\to j},h _{\pi_{j,k}}\},\left(\sum_{i=1}^{k}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}\right\}. \tag{46}\]

_and \(\pi_{j,\cdot}\) is a permutation that sorts \(\max\{h_{i},\tau_{i\to j}\},\) i.e.,_

\[\max\{h_{\pi_{j,1}},\tau_{\pi_{j,1}\to j}\}\leq\cdots\leq\max\{h_{\pi_{j,n}}, \tau_{\pi_{j,n}\to j}\}\]

_for all \(j\in[n].\)_

Let us define \(k^{*}\in[n]\) as the largest index that minimizes (46).

**(Part 1): bound \(\bar{t}_{j}\) by \(\max\{\tau_{\pi_{j,k^{*}\to j}},h_{\pi_{j,k^{*}\to i}}\}\)**

Let us consider the case \(k^{*}<n\). We have two options:

1. Let \(\max\{\tau_{\pi_{j,k^{*}}\to j},h_{\pi_{j,k^{*}}}\}\geq\left(\sum_{i=1}^{k^{*} }\frac{p}{h_{\pi_{j,i}}}\right)^{-1},\) then

\[\max\{\tau_{\pi_{j,k^{*}+1}\to j},h_{\pi_{j,k^{*}+1}}\}\geq\left(\sum_{i=1}^{ k^{*}+1}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}, \tag{49}\]

since \(\max\{\tau_{\pi_{j,i}\to j},h_{\pi_{j,i}}\}\) are sorted. Then, we get

\[\bar{t}_{j} <\frac{1}{8}\max\left\{\max\{\tau_{\pi_{j,k^{*}+1}\to j},h_{\pi_{j,k^{*}+1 }}\},\left(\sum_{i=1}^{k^{*}+1}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}\right\}\] \[\stackrel{{\eqref{eq:t_j}}}{{=}}\frac{1}{8}\max\{ \tau_{\pi_{j,k^{*}+1}\to j},h_{\pi_{j,k^{*}+1}}\}\leq\max\{\tau_{\pi_{j,k^{*}+1 }\to j},h_{\pi_{j,k^{*}+1}}\}\]The first inequality follows from the fact that \(k^{*}\) is the largest minimizer.

2. Let \(\max\{\tau_{\pi_{j,k^{*}}\to j},h_{\pi_{j,k^{*}}}\}<\left(\sum_{i=1}^{k^{*}}\frac{p }{h_{\pi_{j,i}}}\right)^{-1},\) then it is not possible that

\[\max\{\tau_{\pi_{j,k^{*}+1}\to j},h_{\pi_{j,k^{*}+1}}\}<\left(\sum_{i=1}^{k^{*}} \frac{p}{h_{\pi_{j,i}}}\right)^{-1}\text{ because it would yield the inequality}\]

\[\bar{t}_{j}=\frac{1}{8}\left(\sum_{i=1}^{k^{*}}\frac{p}{h_{\pi_{j,i}}}\right)^ {-1}\geq\frac{1}{8}\left(\sum_{i=1}^{k^{*}+1}\frac{p}{h_{\pi_{j,i}}}\right)^{ -1}=\frac{1}{8}\max\left\{\max\{\tau_{\pi_{j,k^{*}+1}\to j},h_{\pi_{j,k^{*}+1}} \},\left(\sum_{i=1}^{k^{*}+1}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}\right\}.\]

The last inequality contradicts the fact that \(k^{*}\) is the largest minimizer. Thus, if \(k^{*}<n\) and \(\max\{\tau_{\pi_{j,k^{*}}\to j},h_{\pi_{j,k^{*}}}\}<\left(\sum_{i=1}^{k^{*}} \frac{p}{h_{\pi_{j,i}}}\right)^{-1},\) then

\[\bar{t}_{j}<\frac{1}{8}\max\left\{\max\{\tau_{\pi_{j,k^{*}+1}\to j},h_{\pi_{j,k ^{*}+1}}\},\left(\sum_{i=1}^{k^{*}+1}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}\right\} =\frac{1}{8}\max\{\tau_{\pi_{j,k^{*}+1}\to j},h_{\pi_{j,k^{*}+1}}\}.\]

In total, we have

\[\bar{t}_{j}<\max\{\tau_{\pi_{j,k^{*}+1}\to j},h_{\pi_{j,k^{*}+1}}\}\]

if \(k^{*}<n\). Using this inequality, we get

\[p\sum_{i=1}^{n}\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}}\right\rfloor \mathbb{1}\left[\tau_{\pi_{j,i}\to j}\leq\bar{t}_{j}\right]=p\sum_{i=1}^{k^{*} }\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}}\right\rfloor\mathbb{1}\left[\tau _{\pi_{j,i}\to j}\leq\bar{t}_{j}\right] \tag{50}\]

for any \(k^{*}\).

**(Part 2)**

We have three options:

1. If \(\left(\sum_{i=1}^{k^{*}}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}\geq\max\{\tau_{ \pi_{j,k^{*}}\to j},h_{\pi_{j,k^{*}}}\},\) then, using (50) and \(\lfloor x\rfloor\leq x\) for all \(x\geq 0\), we get

\[p\sum_{i=1}^{n}\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}}\right\rfloor \mathbb{1}\left[\tau_{\pi_{j,i}\to j}\leq\bar{t}_{j}\right]\leq p\sum_{i=1}^{k ^{*}}\frac{\bar{t}_{j}}{h_{\pi_{j,i}}}=\frac{1}{8}\left(\sum_{i=1}^{k^{*}} \frac{p}{h_{\pi_{j,i}}}\right)^{-1}p\sum_{i=1}^{k^{*}}\frac{1}{h_{\pi_{j,i}}}= \frac{1}{8}.\]

2. If \(\left(\sum_{i=1}^{k^{*}}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}<\max\{\tau_{\pi_{j,k^{*}}\to j},h_{\pi_{j,k^{*}}}\}\) and \(k^{*}=1,\) then

\[p\sum_{i=1}^{n}\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}}\right\rfloor \mathbb{1}\left[\tau_{\pi_{j,i}\to j}\leq\bar{t}_{j}\right]=p\left\lfloor\frac {\bar{t}_{j}}{h_{\pi_{j,k^{*}}}}\right\rfloor\mathbb{1}\left[\tau_{\pi_{j,k^{* }}\to j}\leq\bar{t}_{j}\right]=0\]

because

\[\bar{t}_{j}=\frac{1}{8}\max\left\{\max\{\tau_{\pi_{j,k^{*}}\to j},h_{\pi_{j,k^ {*}}}\},\left(\sum_{i=1}^{k^{*}}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}\right\}= \frac{1}{8}\max\{\tau_{\pi_{j,k^{*}}\to j},h_{\pi_{j,k^{*}}}\}<\max\{\tau_{\pi_ {j,k^{*}}\to j},h_{\pi_{j,k^{*}}}\}.\]

3. If \(\left(\sum_{i=1}^{k^{*}}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}<\max\{\tau_{\pi_{j,k^{*}}\to j},h_{\pi_{j,k^{*}}}\}\) and \(k^{*}>1,\) then \(\bar{t}_{j}=\frac{1}{8}\max\{\tau_{\pi_{j,k^{*}}\to j},h_{\pi_{j,k^{*}}}\}\) and

\[\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}}\right\rfloor\mathbb{1}\left[\tau _{\pi_{j,i}\to j}\leq\bar{t}_{j}\right]=0\]

for all \(i\leq k^{*}\) such that \(\max\{\tau_{\pi_{j,i}\to j},h_{\pi_{j,i}}\}=\max\{\tau_{\pi_{j,k^{*}}\to j},h_{ \pi_{j,k^{*}}}\}.\) If this equality holds for all \(i\leq k^{*},\) then

\[p\sum_{i=1}^{n}\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}}\right\rfloor \mathbb{1}\left[\tau_{\pi_{j,i}\to j}\leq\bar{t}_{j}\right]=0.\]Otherwise, there exists \(\ell<k^{*}\) such that \(\max\{\tau_{\pi_{j,t}\to j},h_{\pi_{j,t}}\}<\max\{\tau_{\pi_{j,k} \to j},h_{\pi_{j,k^{*}}}\}\) and

\[p\sum_{i=1}^{n}\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}} \right\rfloor\mathbb{1}\left[\tau_{\pi_{j,i}\to j}\leq\bar{t}_{j} \right]=p\sum_{i=1}^{\ell}\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}}\right \rfloor\mathbb{1}\left[\tau_{\pi_{j,i}\to j}\leq\bar{t}_{j}\right].\]

It is not possible that \(\max\{\tau_{\pi_{j,t}\to j},h_{\pi_{j,t}}\}\geq\left(\sum_{i=1}^{\ell}\frac{p} {h_{\pi_{j,i}}}\right)^{-1}\) because it would yield the inequality

\[\bar{t}_{j}=\frac{1}{8}\max\{\tau_{\pi_{j,k^{*}}\to j},h_{\pi_{j,k^{*}}}\} >\frac{1}{8}\max\{\tau_{\pi_{j,t}\to j},h_{\pi_{j,t}}\}\] \[=\frac{1}{8}\max\left\{\max\{\tau_{\pi_{j,t}\to j},h_{\pi_{j,t}}\}, \left(\sum_{i=1}^{\ell}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}\right\}\]

that contradicts the fact that \(\bar{t}_{j}\) is the minimum (see (46)). Thus, we have

\[\bar{t}_{j}\leq\frac{1}{8}\max\left\{\max\{\tau_{\pi_{j,t}\to j},h_{\pi_{j,t}} \},\left(\sum_{i=1}^{\ell}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}\right\}=\frac{1} {8}\left(\sum_{i=1}^{\ell}\frac{p}{h_{\pi_{j,i}}}\right)^{-1}\]

and

\[p\sum_{i=1}^{n}\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}} \right\rfloor\mathbb{1}\left[\tau_{\pi_{j,i}\to j}\leq\bar{t}_{j}\right] =p\sum_{i=1}^{\ell}\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}} \right\rfloor\mathbb{1}\left[\tau_{\pi_{j,i}\to j}\leq\bar{t}_{j}\right] \leq p\sum_{i=1}^{\ell}\frac{\bar{t}_{j}}{h_{\pi_{j,i}}}\] \[\leq\frac{1}{8}\left(\sum_{i=1}^{\ell}\frac{p}{h_{\pi_{j,i}}} \right)^{-1}p\sum_{i=1}^{\ell}\frac{1}{h_{\pi_{j,i}}}\leq\frac{1}{8}.\]

In total, we have

\[p\sum_{i=1}^{n}\left\lfloor\frac{\bar{t}_{j}}{h_{\pi_{j,i}}} \right\rfloor\mathbb{1}\left[\tau_{\pi_{j,i}\to j}\leq\bar{t}_{j}\right] \leq\frac{1}{8}\]

for \(\bar{t}_{j}\) from (46).

## Appendix F Lower Bound in the Heterogeneous Setup

In this case, we consider the following oracle mappings. For all \(i\in[n],\) we define

\[O_{i}:\underbrace{\mathbb{R}_{\geq 0}}_{\text{time}}\times \underbrace{\mathbb{R}^{d}}_{\text{point}}\times\underbrace{(\mathbb{R}_{ \geq 0}\times\mathbb{R}^{d}\times\{0,1\})}_{\text{input\;state}}\rightarrow \underbrace{(\mathbb{R}_{\geq 0}\times\mathbb{R}^{d}\times\{0,1\})}_{\text{output\;state}} \times\mathbb{R}^{d}\] \[\text{such\;that}\qquad O_{i}(t,x,(s_{t},s_{x},s_{q}))=\begin{cases} ((t,x,1),&0),&s_{q}=0,\\ ((s_{t},s_{x},1),&0),&s_{q}=1,t<s_{t}+h_{i},\\ ((0,0,0),&\nabla f_{i}(s_{x};\xi)),&s_{q}=1,t\geq s_{t}+h_{i},\end{cases} \tag{51}\]

where \(\xi\sim\mathcal{D}.\) Unlike (27), the mapping (51) returns \(\nabla f_{i}(s_{x};\xi).\)

**Theorem 20**.: _Consider Protocol 8 with the mappings (51). We take any \(h_{i}\geq 0\) and \(\tau_{i\to j}\geq 0\) for all \(i,j\in[n]\) such that \(\tau_{i\to j}\leq\tau_{i\to k}+\tau_{k\to j}\) for all \(i,k,j\in[n].\) We fix \(L,\Delta,\varepsilon,\sigma^{2}>0\) that satisfy the inequality \(\varepsilon<c_{1}L\Delta.\) For any algorithm \(A\in\mathcal{A}_{\text{tr}},\) there exists a function \(f=\frac{1}{n}\sum_{j=1}^{n}f_{i},\) which satisfy Assumptions 1, 2 and \(f(0)-f^{*}\leq\Delta,\) and stochastic gradient mappings \(\nabla f_{i}(\cdot;\cdot),\) which satisfy Assumption 3 (\(\mathbb{E}_{\xi}[\nabla f_{i}(x;\xi)]=\nabla f_{i}(x)\) and \(\mathbb{E}_{\xi}[\|\nabla f_{i}(x;\xi)-\nabla f_{i}(x)\|^{2}]\leq\sigma^{2}\)), such that \(\mathbb{E}\left[\inf_{k\in\mathcal{S}_{i},i\in[n]}\left\|\nabla f(x_{i}^{k}) \right\|^{2}\right]>\varepsilon,\) where \(S_{t}:=\left\{k\in\mathbb{N}_{0}\left|\,t^{k}\leq t\right.\right\},\)_

\[t=c_{2}\times\frac{L\Delta}{\varepsilon}\max\left\{\max_{i,j\in[n]}\tau_{i \to j},\max_{i\in[n]}h_{i},\frac{\sigma^{2}}{n\varepsilon}\left(\frac{1}{n} \sum_{i=1}^{n}h_{i}\right)\right\},\]

_The quantities \(c_{1}\) and \(c_{2}\) are universal constants. The sequences \(x^{k}\) and \(t^{k}\) are defined in Protocol 8._Proof.: The last two terms in the max follow from Theorem A.2 by Tyurin and Richtarik (2023), who considered the same setup but with \(\tau_{i\to j}=0\) for all \(i,j\in[n]\). It is left to prove the first term. Let us fix \(\lambda>0\). Let us take any pair \(\widetilde{(i,j)}\) of workers such that \(\max_{i,j\in[n]}\tau_{i\to j}=\tau_{i\to j}\). Next, we split the blocks of the function \(F_{T}(x)\) from (33) and define two new functions:

\[F_{T,1}(x):=-\Psi(1)\Phi(x_{1})+\sum_{i\in\{2,\ldots,T\},i\,|\,2=1}\left[\Psi( -x_{i-1})\Phi(-x_{i})-\Psi(x_{i-1})\Phi(x_{i})\right], \tag{52}\]

and

\[F_{T,2}(x):=\sum_{i\in\{2,\ldots,T\},i\,|\,2=0}\left[\Psi(-x_{i-1})\Phi(-x_{i}) -\Psi(x_{i-1})\Phi(x_{i})\right].\]

We consider the following functions \(f_{i}:\)

\[f_{i}(x):=\begin{cases}\frac{nL\lambda^{2}}{l_{1}}F_{T,1}\left(\frac{x}{ \lambda}\right),&i=\widetilde{i},\\ \frac{nL\lambda^{2}}{l_{1}}F_{T,2}\left(\frac{x}{\lambda}\right),&i=\widetilde {j},\\ 0,&i\neq\widetilde{i}\text{ and }i\neq\widetilde{j}.\end{cases}\]

Then, we get

\[f(x)=\frac{1}{n}\sum_{i=1}^{n}f_{i}(x)=\frac{1}{n}\left(\frac{nL\lambda^{2}}{l _{1}}F_{T,1}\left(\frac{x}{\lambda}\right)+\frac{nL\lambda^{2}}{l_{1}}F_{T,2} \left(\frac{x}{\lambda}\right)\right)=\frac{L\lambda^{2}}{l_{1}}F_{T}\left( \frac{x}{\lambda}\right).\]

Let us show that the function \(f\) is \(L\)-smooth:

\[\left\|\nabla f(x)-\nabla f(y)\right\|=\frac{L\lambda}{l_{1}}\left\|\nabla F_{ T}\left(\frac{x}{\lambda}\right)-\nabla F_{T}\left(\frac{y}{\lambda}\right) \right\|\leq L\left\|x-y\right\|.\]

Let us take

\[T=\left\lfloor\frac{\Delta l_{1}}{L\lambda^{2}\Delta^{0}}\right\rfloor,\]

then

\[f(0)-\inf_{x\in\mathbb{R}^{T}}f(x)=\frac{L\lambda^{2}}{l_{1}}(F_{T}\left(0 \right)-\inf_{x\in\mathbb{R}^{T}}F_{T}(x))\leq\frac{L\lambda^{2}\Delta^{0}T}{ l_{1}}\leq\Delta.\]

We showed that the function \(f\) satisfy Assumptions 1, 2 and \(f(0)-f^{*}\leq\Delta\).

In the oracles \(O_{i}\), we simply take the non-stochastic mappings \(\nabla f_{i}(x;\xi):=\nabla f_{i}(x)\) that are unbiased and \(0\)-variance-bounded.

We take

\[\lambda=\frac{l_{1}\sqrt{\varepsilon}}{L}\]

to ensure that

\[\left\|\nabla f(x)\right\|^{2}=\frac{L^{2}\lambda^{2}}{l_{1}^{2}}\left\| \nabla F_{T}\left(\frac{x}{\lambda}\right)\right\|^{2}>\frac{L^{2}\lambda^{2} }{l_{1}^{2}}=\varepsilon\]

for all \(x\in\mathbb{R}^{T}\) such that \(\text{prog}(x)<T\). In the last inequality, we use Lemma 1. Thus

\[T=\left\lfloor\frac{\Delta L}{l_{1}\varepsilon\Delta^{0}}\right\rfloor.\]

Only workers \(\widetilde{i}\) and \(\widetilde{j}\) contain the information about the function \(f\). The function \(f\) is a zero-chain function, and we split it between workers \(\widetilde{i}\) and \(\widetilde{j}\). Due to this splitting, workers \(\widetilde{i}\) and \(\widetilde{j}\) have to communicate to find the next non-zero coordinate. Only worker \(\widetilde{i}\) can get a non-zero value in the first coordinate through the gradient of \(F_{T,1}\). Next, this worker can not get a non-zero value in the second coordinate due to the construction of (52). Thus, it has to pass a vector with a non-zero value in the first coordinate to worker \(\widetilde{j}\) because only this worker can get a non-zero value in the second coordinate. This communication takes at least \(\widetilde{\tau}_{i\to\widetilde{j}}\) seconds. Using the same reasoning, worker \(\widetilde{j}\) has to send a vector to worker \(\widetilde{i}\) once worker \(\widetilde{j}\) has discovered a non-zero value in the second coordinate.

An algorithm has to repeat such communications at least \(\frac{T-1}{2}\) times to find a vector \(x\in\mathbb{R}^{T}\) such that \(\text{prog}(x)=T\).

Thus, we get

\[\inf_{k\in S_{i},i\in[n]}\left\|\nabla f(x_{i}^{k})\right\|^{2}>\varepsilon\]

for

\[t=\tau_{i\to\bar{j}}\left(\frac{T-1}{2}\right)=\frac{\max_{i,j\in[n]}\tau_{i \to j}}{2}\left(\left\lfloor\frac{\Delta L}{l_{1}\varepsilon\Delta^{0}}\right \rfloor-1\right).\]

## Appendix G Proof of the Time Complexity for Homogeneous Case

**Theorem 4**.: _Let Assumptions 1, 2, and 3 hold. We take \(\gamma=1/2L,\) batch size \(S=\max\{\left\lceil\nicefrac{{\sigma^{2}}}{{\varepsilon}}\right\rceil,1\},\) any pivot worker \(j^{*}\in[n],\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}}\) in Algorithm 2. For all \(K\geq 16L\Delta/\varepsilon,\) we get \(\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2} \right]\leq\varepsilon.\)_

Proof.: Algorithm 2 produces the sequence \(x^{k}\) such as \(x^{k+1}=x^{k}-\frac{\gamma}{s^{k}}g^{k}=x^{k}-\gamma\bar{g}^{k},\) where \(\bar{g}^{k}:=\frac{1}{s^{k}}g^{k}.\) By the design of the algorithm,

\[\bar{g}^{k}=\frac{1}{s^{k}}\sum_{i=1}^{s^{k}}\nabla f(x^{k};\xi_{i}),\]

where the \(\xi_{i}\) are independent random samples and \(s^{k}\geq S.\) We do not dismiss the possibility that the computation and communication times are random, so \(s^{k}\) can be random. Assume \(\mathcal{V}_{k}\) is a \(\sigma\)-algebra generated by all computation and communication times and \(g^{0},\ldots,g^{k-1},\) then \(s^{k}\) is \(\mathcal{V}_{k}\)-measurable. Using the independence and Assumption 3, we have

\[\mathbb{E}\left[\left.\bar{g}^{k}\right|\mathcal{G}_{k}\right]=\mathbb{E}\left[ \left.\mathbb{E}\left[\left.\frac{1}{s^{k}}\sum_{i=1}^{s^{k}}\nabla f(x^{k}; \xi_{i})\right|\mathcal{V}_{k}\right]\right|\mathcal{G}_{k}\right]=\mathbb{E} \left[\left.\frac{1}{s^{k}}\sum_{i=1}^{s^{k}}\mathbb{E}\left[\left.\nabla f(x^ {k};\xi_{i})\right|\mathcal{V}_{k}\right]\right|\mathcal{G}_{k}\right]=\nabla f (x^{k})\]

and

\[\mathbb{E}\left[\left.\left\|\bar{g}^{k}-\nabla f(x^{k})\right\|^ {2}\right|\mathcal{G}_{k}\right]\] \[=\mathbb{E}\left[\left.\mathbb{E}\left[\left.\left\|\frac{1}{s^{ k}}\sum_{i=1}^{s^{k}}\nabla f(x^{k};\xi_{i})-\nabla f(x^{k})\right\|^{2} \right|\mathcal{V}_{k}\right]\right|\mathcal{G}_{k}\right]\] \[=\mathbb{E}\left[\left.\frac{1}{(s^{k})^{2}}\sum_{i=1}^{s^{k}} \mathbb{E}\left[\left.\left\|\nabla f(x^{k};\xi_{i})-\nabla f(x^{k})\right\| ^{2}\right|\mathcal{V}_{k}\right]\right|\mathcal{G}_{k}\right]\] \[\leq\mathbb{E}\left[\left.\frac{\sigma^{2}}{s^{k}}\right| \mathcal{G}_{k}\right]\leq\frac{\sigma^{2}}{S}.\]

where \(\mathcal{G}_{k}\) is a \(\sigma\)-algebra generated by \(\bar{g}^{0},\ldots,\bar{g}^{k-1}.\) We can use a well-known SGD result (Ghadimi and Lan, 2013; Khaled and Richtarik, 2022). Using Theorem 21, for the stepsize

\[\gamma=\frac{1}{2L}\min\left\{1,\frac{\varepsilon S}{\sigma^{2}}\right\},\]

we have

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2} \right]\leq\varepsilon,\]if

\[K\geq\frac{8\Delta L}{\varepsilon}+\frac{8\Delta L\sigma^{2}}{\varepsilon^{2}S}.\]

Using the choice of \(S\), we get that Algorithm 2 converges after

\[K\geq\frac{16\Delta L}{\varepsilon}\]

steps with

\[\gamma=\frac{1}{2L}\min\left\{1,\frac{\varepsilon S}{\sigma^{2}}\right\}=\frac{ 1}{2L}.\]

**Theorem 5**.: _Consider the assumptions and the parameters from Theorem 4. For any pivot worker \(j^{*}\in[n]\) and spanning trees \(\overline{st}\) and \(\overline{st}_{\mathrm{bc}}\), Algorithm 2 converges after at most_

\[\Theta\left(\frac{L\Delta}{\varepsilon}t^{*}(\sigma^{2}/_{\varepsilon},[h_{i}] ^{n}_{i=1},[\mu_{i\to j^{*}}+\mu_{j^{*}\to i}]^{n}_{i=1})\right) \tag{10}\]

_seconds, where \(\mu_{i\to j^{*}}\) (\(\mu_{j^{*}\to i}\)) is an upper bound on the times required to send a vector from worker \(i\) to worker \(j^{*}\) (from worker \(j^{*}\) to worker \(i\)) along the spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\mathrm{bc}}\))._

Proof.: Due to Theorem 4, we know that Algorithm 3 finds an \(\varepsilon\)-stationary point after at most \(K=\Theta\left(\frac{L\Delta}{\varepsilon}\right)\) iterations. It is left to bound the time of one iteration to prove the theorem.

For all \(j\in[n]\), we define \(\pi_{j,\cdot}\) as a permutation that sorts \(\{\max\{\mu_{i\to j}+\mu_{j\to i},h_{i}\}\}_{i=1}^{n}\) as

\[\max\{\mu_{\pi_{j,\cdot}\to j}+\mu_{j\to\pi_{j,\cdot}1},h_{\pi_{j,\cdot}1}\} \leq\cdots\leq\max\{\mu_{\pi_{j,\cdot}n\to j}+\mu_{j\to\pi_{j,\cdot}n},h_{\pi_ {j,\cdot}n}\}.\]

Let us define the index

\[k^{*}=\arg\min_{k\in[n]}\max\left\{\max\{\mu_{\pi_{j^{*},k}\to j^{*}}+\mu_{j^{ *}\to\pi_{j^{*},k}},h_{\pi_{j^{*},k}}\},S\left(\sum_{i=1}^{k}\frac{1}{h_{\pi_{ j^{*},i}}}\right)^{-1}\right\}.\]

and the set

\[A^{*}:=\{\pi_{j^{*},i}\in[n]\,|\,i\leq k^{*}\}\]

that represents a set of the "fastest" workers that can potentially contribute to an optimization process. We take

\[\begin{split}\bar{t}&:=2\max\left\{\max\{\mu_{\pi _{j^{*},k^{*}\to j^{*}}}+\mu_{j^{*}\to\pi_{j^{*},k^{*}}},h_{\pi_{j^{*},k^{*}} }\},S\left(\sum_{i=1}^{k^{*}}\frac{1}{h_{\pi_{j^{*},i}}}\right)^{-1}\right\} \\ &=2\min_{k\in[n]}\max\left\{\max\{\mu_{\pi_{j^{*},k}\to j^{*}}+ \mu_{j^{*}\to\pi_{j^{*},k}},h_{\pi_{j^{*},k}}\},S\left(\sum_{i=1}^{k}\frac{1}{ h_{\pi_{j^{*},i}}}\right)^{-1}\right\}.\end{split} \tag{53}\]

In the following steps of the proof we show that every iteration takes at most

(Step 1): Calculate enough stochastic gradients (Step 2): Send stochastic gradients to Process \(j^{*}\) (Step 3): Broadcast a new point seconds.

**(Step 1):** Since it takes at most \(h_{i}\) seconds to calculate a stochastic gradient in worker \(i\), all workers from the set \(A^{*}\) will calculate at least

\[\sum_{i\in A^{*}}\left\lfloor\frac{\bar{t}}{h_{i}}\right\rfloor=\sum_{i=1}^{k^ {*}}\left\lfloor\frac{\bar{t}}{h_{\pi_{j^{*},i}}}\right\rfloor \tag{54}\]

stochastic gradients after \(\bar{t}\) seconds at the point \(x^{k}\). We have

\[\bar{t}\geq 2\max\{\mu_{\pi_{j^{*},k^{*}}\to j^{*}}+\mu_{j^{*}\to\pi_{j^{*},k^{*}}}, h_{\pi_{j^{*},k^{*}}}\}\geq 2\max\{\mu_{\pi_{j^{*},i}\to j^{*}}+\mu_{j^{*}\to\pi_{j^{*},i}},h_{\pi_{j^{*},i}}\} \tag{55}\]for all \(i\leq k^{*}\) by the definition of the permutations \(\pi_{,\cdot,\cdot}\). Therefore,

\[\bar{t}\geq 2h_{\pi_{j^{*},i}}\]

for all \(i\leq k^{*}\). Thus, using (54) and \(\left\lfloor x\right\rfloor\geq\frac{\varepsilon}{2}\) for all \(x\geq 1\), we get

\[\sum_{i\in A^{*}}\left\lfloor\frac{\bar{t}}{h_{i}}\right\rfloor\geq\sum_{i=1}^ {k^{*}}\frac{\bar{t}}{2h_{\pi_{j^{*},i}}}\overset{\eqref{eq:def_t}}{\geq}S.\]

Therefore, after \(\bar{t}\) seconds the algorithm will calculate at least \(S\) stochastic gradients at the point \(x^{k}\) using the workers \(A^{*}\).

**(Step 2):** By the design of Algorithm 4, once a stochastic gradient \(\nabla f(x^{k};\bar{\xi})\) is calculated, it is added to \(g^{k}_{i,\text{next}}\). Then, \(g^{k}_{i,\text{next}}\) is assigned to \(g^{k}_{i,\text{send}}\) which is sent to Process \(\text{next}_{\overline{st},j^{*}}(i)\). Finally, Process \(\text{next}_{\overline{st},j^{*}}(i)\) receives \(g^{k}_{i,\text{send}}\) and adds it to \(g^{k}_{(\text{next}_{\overline{st},j^{*}}(i)),\text{next}}\). Thus, the stochastic gradient \(\nabla f(x^{k};\bar{\xi})\) is presented in the sum of \(g^{k}_{(\text{next}_{\overline{st},j^{*}}(i)),\text{next}}\) of Process \(\text{next}_{\overline{st},j^{*}}(i)\). At some point, Process 0 in worker \(j^{*}\) will receive a vector \(g^{k}_{,\text{send}}\) where the stochastic gradient \(\nabla f(x^{k};\bar{\xi})\) is presented.

Let us bound the time required to transmit a stochastic gradient to Process 0 of worker \(j^{*}\). Once a stochastic gradient \(\nabla f(x^{k};\bar{\xi})\) is calculated in Process \(i\) from \(A^{*}\), it is added to a vector \(g^{k}_{i,\text{next}}\) in Process \(i\). It will take at most \(2\rho_{i\rightarrow\text{next}_{\overline{st},j^{*}}(i)}\) seconds to transmit it to Process \(\text{next}_{\overline{st},j^{*}}(i)\) because it takes at most \(\rho_{i\rightarrow\text{next}_{\overline{st},j^{*}}(i)}\) seconds to wait for the transmission of a message \(g^{k}_{i,\text{send}}\) where the stochastic gradient \(\nabla f(x^{k};\bar{\xi})\) is not presented, and an additional \(\rho_{i\rightarrow\text{next}_{\overline{st},j^{*}}(i)}\) seconds to send the next \(g^{k}_{i,\text{send}}\) where it will present. After that, Process \(\text{next}_{\overline{st},j^{*}}(i)\) will receive \(g^{k}_{i,\text{send}}\), where the stochastic gradient \(\nabla f(x^{k};\bar{\xi})\) presents, and add the vector \(g^{k}_{i,\text{send}}\) to \(g^{k}_{(\text{next}_{\overline{st},j^{*}}(i)),\text{next}}\). Then, it will take at most \(2\rho_{\text{next}_{\overline{st},j^{*}}(i)\rightarrow\text{next}(\text{next} _{\overline{st},j^{*}}(i))}\) seconds to send a vector, where the stochastic gradient \(\nabla f(x^{k};\bar{\xi})\) presents, to Process \(\text{next}(\text{next}_{\overline{st},j^{*}}(i))\) and so forth. In total, after a finite number of such steps a stochastic gradient calculated in Process \(i\) will be transmitted to Process 0 of worker \(j^{*}\). From Definition 3 of \(\text{next}_{\overline{st},j^{*}}\), we can conclude that the vector \(\nabla f(x^{k};\bar{\xi})\) will be transmitted through the path between workers \(i\) and \(j^{*}\) in the spanning tree \(\overline{st}\). Thus, it will take at most \(2\mu_{i\rightarrow j^{*}}\) seconds by the definition of \(\mu_{i\mapsto j^{*}}\).

Using (55) and the definition of \(A^{*}\), we have

\[\bar{t}\geq 2\max\{\mu_{\pi_{j^{*},k^{*}}\to j^{*}}+\mu_{j^{*} \rightarrow\pi_{j^{*},k^{*}}},h_{\pi_{j^{*},k^{*}}}\}\geq 2\max\{\mu_{i \rightarrow j^{*}}+\mu_{j^{*}\to i},h_{i}\}\geq 2\mu_{i\to j^{*}}. \tag{56}\]

for all \(i\in A^{*}\). Therefore, it will take at most \(\bar{t}\) seconds to calculate at least \(S\) stochastic gradients, and at most \(\bar{t}\) seconds to send all these stochastic gradients to Process 0.

**(Step 3):** It is left to estimate the time of the broadcast steps (Lines 7-9 in Algorithm 4) through the spanning tree \(\overline{st}_{\text{bc}}\). By the definition of \(\mu_{j^{*}\to i}\), the time required to broadcast \(x^{k}\) to Process \(i\) through the spanning tree \(\overline{st}_{\text{bc}}\) is less or equal to \(2\mu_{j^{*}\to i}\) since, in all edges from \(j^{*}\) to \(i\), workers wait at most \(\rho_{\rightarrow\cdot}\) seconds while edges are blocked by previous communications, and additional \(\rho_{\rightarrow\cdot}\) seconds to send \(x^{k}\) to next workers. Using (55) and the definition of \(A^{*}\), we have

\[\bar{t}\geq 2\max\{\mu_{\pi_{j^{*},k^{*}}\to j^{*}}+\mu_{j^{*} \rightarrow\pi_{j^{*},k^{*}}},h_{\pi_{j^{*},k^{*}}}\}\geq 2\max\{\mu_{i \rightarrow j^{*}}+\mu_{j^{*}\to i},h_{i}\}\geq 2\mu_{j^{*}\to i}.\]

Thus, every worker from \(A^{*}\) will get \(x^{k}\) after \(\bar{t}\) seconds. By combining all times, we can conclude that every iteration in Algorithm 3 will take at most \(\bar{t}+\bar{t}+\bar{t}=3\bar{t}\) seconds.

It left to show that

\[\bar{t}=\text{O}\left(\max\left\{\max\{\mu_{\pi_{j^{*},k^{*}}\to j^{*}}+\mu_{j^{*} \rightarrow\pi_{j^{*},k^{*}}},h_{\pi_{j^{*},k^{*}}}\},\frac{\sigma^{2}}{ \varepsilon}\left(\sum_{i=1}^{k^{*}}\frac{1}{h_{\pi_{j^{*},i}}}\right)^{-1} \right\}\right). \tag{57}\]

If \(S>1,\) then \(S=\max\{\left\lceil\sigma^{2}/\varepsilon\right\rceil,1\}=\left\lceil\sigma^{2}/ \varepsilon\right\rceil\leq 2\sigma^{2}/\varepsilon\), so it is true. Otherwise, if \(S\leq 1,\) then

\[\bar{t} \leq 2\max\left\{\max\{\mu_{\pi_{j^{*},k^{*}}\to j^{*}}+\mu_{j^{*} \rightarrow\pi_{j^{*},k^{*}}},h_{\pi_{j^{*},k^{*}}}\},\left(\sum_{i=1}^{k^{*}} \frac{1}{h_{\pi_{j^{*},i}}}\right)^{-1}\right\}\] \[\leq 2\max\left\{\max\{\mu_{\pi_{j^{*},k^{*}}\to j^{*}}+\mu_{j^{*} \rightarrow\pi_{j^{*},k^{*}}},h_{\pi_{j^{*},k^{*}}}\},h_{\pi_{j^{*},k^{*}}}\right\}\] \[=2\max\{\mu_{\pi_{j^{*},k^{*}}\to j^{*}}+\mu_{j^{*} \rightarrow\pi_{j^{*},k^{*}}},h_{\pi_{j^{*},k^{*}}}\}\]and (57) holds. Notice that the r.h.s. of (57) equals to \(\mathrm{O}\left(t^{*}(\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i}]_{i=1}^{n},[ \mu_{i\to j^{*}}+\mu_{j^{*}\to i}]_{i=1}^{n})\right),\) where \(t^{*}\) is the equilibrium time defined in Definition 2. 

**Theorem 6**.: _Consider the assumptions and the parameters from Theorem 4. In each iteration \(k\) of Algorithm 3, the computation times of worker \(i\) are bounded by \(h_{i}^{k}\). Let us fix any pivot worker \(j^{*}\in[n]\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}}\). Then Algorithm 2 converges after at most_

\[\Theta\left(\sum_{k=0}^{\lceil 16L\Delta/\varepsilon\rceil}t^{*}(\nicefrac{{ \sigma^{2}}}{{\varepsilon}},[h_{i}^{k}]_{i=1}^{n},[\mu_{i\to j^{*}}^{k}+\mu_{ j^{*}\to i}^{k}]_{i=1}^{n})\right) \tag{13}\]

_seconds, where \(\mu_{i\to j^{*}}^{k}\) (\(\mu_{j^{*}\to i}^{k}\)) is an upper bound on times required to send a vector from worker \(i\) to worker \(j^{*}\) (from worker \(j^{*}\) to worker \(i\)) along the spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\text{bc}}\)) in iteration \(k\) of Algorithm 3._

Proof.: The proof is almost the same as in Theorem 5. If we fix a pivot worker \(j^{*},\) then the \(k^{\text{th}}\) iteration will finish after at most

\[c\times t^{*}(\nicefrac{{\sigma^{2}}}{{\varepsilon}},[h_{i}^{k}]_{i=1}^{n},[ \mu_{i\to j^{*}}^{k}+\mu_{j^{*}\to i}^{k}]_{i=1}^{n})\]

seconds, where \(c\) is a universal constant. According to Theorem 4, the number of iterations is at most \(\left\lceil\frac{16L\Delta}{\varepsilon}\right\rceil.\) Therefore, the total required time is at most

\[c\times\sum_{k=0}^{\lceil\frac{16L\Delta}{\varepsilon}\rceil}t^{*}(\nicefrac{ {\sigma^{2}}}{{\varepsilon}},[h_{i}^{k}]_{i=1}^{n},[\mu_{i\to j^{*}}^{k}+\mu_{ j^{*}\to i}^{k}]_{i=1}^{n}).\]

## Appendix H Proof of the Time Complexity for Heterogeneous Case

**Theorem 8**.: _Let Assumptions 1 and 2 hold for the function \(f\) and Assumption 3 holds for the functions \(f_{i}\) for all \(i\in[n].\) We take \(\gamma=1/2L,\) the parameter \(S=\max\{\left\lceil\nicefrac{{\sigma^{2}}}{{\varepsilon}}\right\rceil,n\},\) any pivot worker \(j^{*}\in[n],\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}},\) in Algorithm 5. For all iterations number \(K\geq 16L\Delta/\varepsilon,\) we get \(\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2} \right]\leq\varepsilon.\)_

Proof.: Algorithm 5 produces the sequence \(x^{k}\) such that

\[x^{k+1}=x^{k}-\gamma g^{k}=x^{k}-\gamma\left(\frac{1}{n}\sum_{i=1}^{n}\frac{ 1}{s_{i}^{k}}g_{i}^{k}\right)=x^{k}-\gamma\left(\frac{1}{n}\sum_{i=1}^{n}\frac {1}{s_{i}^{k}}\sum_{j=1}^{s_{i}^{k}}\nabla f_{i}(x^{k};\xi_{ij})\right),\]

where the \(\xi_{ij}\) are independent random samples. Using the independence and Assumption 3, we have \(\mathbb{E}\left[\left.g^{k}\right|\mathcal{G}_{k}\right]=\nabla f(x^{k})\) and

\[\mathbb{E}\left[\left.\left\|g^{k}-\nabla f(x^{k})\right\|^{2} \right|\mathcal{G}_{k}\right]\] \[=\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}\frac{1}{s_{i}^ {k}}\sum_{j=1}^{s_{i}^{k}}\nabla f_{i}(x^{k};\xi_{ij})-\frac{1}{n}\sum_{i=1}^{ n}\nabla f_{i}(x^{k})\right\|^{2}\right|\mathcal{G}_{k}\right]. \tag{58}\]

where \(\mathcal{G}_{k}\) is a \(\sigma\)-algebra generated by \(g^{0},\ldots,g^{k-1}.\) We do not dismiss the possibility that the computation and communication times are random, so \(s_{i}^{k}\) can be random. Assume \(\mathcal{V}_{k}\) is a \(\sigma\)-algebra generated by all computation and communication times and \(g^{0},\ldots,g^{k-1},\) then \(s_{i}^{k}\) is \(\mathcal{V}_{k}\)-measurable for all \(i\in[n].\) Using the independence of stochastic gradients and the times and the tower property, we get \[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right| \mathcal{G}_{k}\right]\] \[=\mathbb{E}\left[\left\|\frac{1}{n}\sum_{i=1}^{n}\frac{1}{s_{i}^{k }}\sum_{j=1}^{s_{i}^{k}}\nabla f_{i}(x^{k};\xi_{ij})-\frac{1}{n}\sum_{i=1}^{n }\nabla f_{i}(x^{k})\right\|^{2}\Bigg{|}\mathcal{V}_{k}\right]\Bigg{|}\mathcal{ G}_{k}\Bigg{]} \tag{59}\] \[=\frac{1}{n^{2}}\sum_{i=1}^{n}\mathbb{E}\left[\left\|\frac{1}{s_{ i}^{k}}\sum_{j=1}^{s_{i}^{k}}\nabla f_{i}(x^{k};\xi_{ij})-\nabla f_{i}(x^{k}) \right\|^{2}\Bigg{|}\mathcal{V}_{k}\right]\Bigg{|}\mathcal{G}_{k}\Bigg{]}\] \[=\frac{1}{n^{2}}\sum_{i=1}^{n}\mathbb{E}\left[\left.\frac{1}{(s_{ i}^{k})^{2}}\sum_{j=1}^{s_{k}^{k}}\mathbb{E}\left[\left\|\nabla f_{i}(x^{k}; \xi_{ij})-\nabla f_{i}(x^{k})\right\|^{2}\right|\mathcal{V}_{k}\right]\right| \mathcal{G}_{k}\right]\leq\frac{\sigma^{2}}{n^{2}}\mathbb{E}\left[\left.\sum_{i =1}^{n}\frac{1}{s_{i}^{k}}\right|\mathcal{G}_{k}\right]. \tag{60}\]

Algorithm 6 waits for the moment when \(s^{k}\geq\frac{S}{n}\), which is equivalent to

\[b_{j^{*}}\leq\frac{n^{2}}{S}. \tag{61}\]

The value \(b_{j^{*}}\) is calculated in Line 20 of Algorithm 7. Due to the asynchronous nature of the algorithm, we can only conclude that

\[b_{j^{*}}\geq\sum_{p\in[n]:\text{next}\pi_{j^{*}}(p)=j^{*}}b_{i,p}+\frac{1}{s_ {j^{*}}^{k}} \tag{62}\]

because \(s_{j^{*}}^{k}\) can be increased by the time when Process 0 will receive \(b_{j^{*}}\). Using Line 15 from Algorithm 7, we can unroll the recursion in (62) and get

\[b_{j^{*}}\geq\sum_{i=1}^{n}\frac{1}{s_{i}^{k}}. \tag{63}\]

Let us substitute this inequality to (61) and (60) and get

\[\mathbb{E}\left[\left\|g^{k}-\nabla f(x^{k})\right\|^{2}\right| \mathcal{G}_{k}\right]\leq\frac{\sigma^{2}}{S}.\]

As in Theorem 4, we can use the classical SGD result. Using Theorem 21, for the stepsize

\[\gamma=\frac{1}{2L}\min\left\{1,\frac{\varepsilon S}{\sigma^{2}}\right\},\]

we have

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k}) \right\|^{2}\right]\leq\varepsilon,\]

if

\[K\geq\frac{8\Delta L}{\varepsilon}+\frac{8\Delta L\sigma^{2}}{\varepsilon^{2} S}.\]

Using the choice of \(S\), we get that Algorithm 5 converges after

\[K\geq\frac{16\Delta L}{\varepsilon}\]

steps with

\[\gamma=\frac{1}{2L}\min\left\{1,\frac{\varepsilon S}{\sigma^{2}}\right\}= \frac{1}{2L}.\]

**Theorem 9**.: _Consider the assumptions and the parameters from Theorem 8. For any pivot worker \(j^{*}\in[n]\) and any spanning trees \(\overline{st}\) and \(\overline{st}_{\text{bc}},\) Algorithm 5 converges after at most_

\[\Theta\left(\frac{L\Delta}{\varepsilon}\max\left\{\max_{i,j\in[n]}\mu_{i\to j },\max_{i\in[n]}h_{i},\frac{\sigma^{2}}{n\varepsilon}\left(\frac{1}{n}\sum_{i= 1}^{n}h_{i}\right)\right\}\right)\]

_seconds, where \(\mu_{i\to j^{*}}^{k}\) (\(\mu_{j^{*}\to i}^{k}\)) is an upper bound on times required to send a vector from worker \(i\) to worker \(j^{*}\) (from worker \(j^{*}\) to worker \(i\)) along the spanning tree \(\overline{st}\) (spanning tree \(\overline{st}_{\text{bc}}\)) for all \(i\in[n].\)_

Proof.: Due to Theorem 8, the algorithm converges after \(K=\Theta\left(L\Delta/\varepsilon\right)\) iterations. Thus, it left to bound the time of one iteration. At the beginning of every iteration Process 0 broadcasts \(x^{k},\) which takes at most \(\max_{i,j\in[n]}\mu_{i\to j}\) seconds. Then, Algorithm 6 waits for the moment when \(s^{k}\geq\frac{S}{n},\) which is equivalent to \(\frac{n^{2}}{S}\geq b_{j^{*}}.\) Thus, Algorithm 6 waits for the moment when \(\frac{n^{2}}{S}\geq b_{j^{*}}.\) We will return to this fact later.

Let us consider the term

\[\frac{S}{n^{2}}\sum_{i=1}^{n}\frac{1}{s_{i}^{k}},\]

where \(s_{i}^{k}\) is the number of stochastic gradients calculated in worker \(i.\) Let us fix any time \(\bar{t}>0.\) Then, worker \(i\) will calculate at least \(\left\lfloor\frac{\bar{t}}{h_{i}}\right\rfloor\) stochastic gradients by the time \(\bar{t}.\) Using this, we get

\[\frac{S}{n^{2}}\sum_{i=1}^{n}\frac{1}{s_{i}^{k}}\leq\frac{S}{n^{2}}\sum_{i=1}^ {n}\frac{1}{\left\lfloor\frac{\bar{t}}{h_{i}}\right\rfloor}.\]

Let us take

\[\bar{t}=2\left(\max_{i\in[n]}h_{i}+\frac{S}{n}\left(\frac{1}{n}\sum_{i=1}^{n}h _{i}\right)\right).\]

Then, since \(\lfloor x\rfloor\geq\frac{\pi}{2}\) for all \(x\geq 1,\) we get \(\left\lfloor\frac{\bar{t}}{h_{i}}\right\rfloor\geq\frac{\bar{t}}{2h_{i}}\) and

\[\frac{S}{n^{2}}\sum_{i=1}^{n}\frac{1}{s_{i}^{k}}\leq\frac{2S}{n\bar{t}}\left( \frac{1}{n}\sum_{i=1}^{n}h_{i}\right).\]

Using \(\bar{t}\geq\frac{2S}{n}\left(\frac{1}{n}\sum_{i=1}^{n}h_{i}\right),\) we get

\[\frac{S}{n^{2}}\sum_{i=1}^{n}\frac{1}{s_{i}^{k}}\leq 1\]

and

\[\sum_{i=1}^{n}\frac{1}{s_{i}^{k}}\leq\frac{n^{2}}{S} \tag{64}\]

after at most \(\bar{t}\) seconds.

Recall that Algorithm 6 waits for the moment when \(\frac{n^{2}}{S}\geq b_{j^{*}}.\) Note that by the time when Process 0 receives \(b_{j^{*}},\) the counter \(s_{i}^{k}\) can be the same or increased; thus, \(b_{j^{*}}\) captures potentially outdated information about \(s_{i}^{k}\). We know that (64) holds after at most \(\bar{t}\) seconds. In Line 20 of Algorithm 7, Processes recursively collect \(\frac{1}{s_{i}^{k}}\) to \(b_{j^{*}}.\) Such a procedure will take at most \(2\max_{i\in[n]}\mu_{i\to j^{*}}\leq 2\max_{i,j\in[n]}\mu_{i\to j}\) seconds. Thus, the value \(b_{j^{*}}\) will be less or equal \(\frac{n^{2}}{S}\) after at most \(\bar{t}+2\max_{i,j\in[n]}\mu_{i\to j}\) seconds.

The all reduce operation in (8) will take at most \(\max_{i,j\in[n]}\mu_{i\to j}\) seconds. Thus, the total time of one iteration can be bounded by

\[\underbrace{\max_{i,j\in[n]}\mu_{i\to j}}_{\text{broadcast}}+( \bar{t}+2\max_{i,j\in[n]}\mu_{i\to j})+\underbrace{\max_{i,j\in[n]}\mu_{i \to j}}_{\text{all reduce}}\] \[=\text{O}\left(\max_{i,j\in[n]}\mu_{i\to j}+\max_{i\in[n]}h_{i}+ \frac{S}{n}\left(\frac{1}{n}\sum_{i=1}^{n}h_{i}\right)\right)\] \[=\text{O}\left(\max_{i,j\in[n]}\mu_{i\to j}+\max_{i\in[n]}h_{i}+ \frac{\sigma^{2}}{n\varepsilon}\left(\frac{1}{n}\sum_{i=1}^{n}h_{i}\right) \right).\]

seconds. 

## Appendix I Classical SGD Theory

We reprove the classical SGD result (Ghadimi and Lan, 2013; Khaled and Richtarik, 2022), for completeness.

**Theorem 21**.: _Let Assumptions 1 and 2 hold. We consider the SGD method:_

\[x^{k+1}=x^{k}-\gamma g(x^{k}),\]

_where_

\[\gamma=\frac{1}{2L}\min\left\{1,\frac{\varepsilon}{\sigma^{2}}\right\}\]

_For all \(k\geq 0,\) the vector \(g(x)\) is a random vector such that \(\mathbb{E}\left[\left.g(x^{k})\right|\mathcal{G}_{k}\right]=\nabla f(x^{k}),\)_

\[\mathbb{E}\left[\left.\left\|g(x^{k})-\nabla f(x^{k})\right\|^{2} \right|\mathcal{G}_{k}\right]\leq\sigma^{2}, \tag{65}\]

_where \(\mathcal{G}_{k}\) is a \(\sigma\)-algebra generated by \(g(x^{0}),\ldots,g(x^{k-1})\). Then_

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k}) \right\|^{2}\right]\leq\varepsilon\]

_for_

\[K\geq\frac{8\Delta L}{\varepsilon}+\frac{8\Delta L\sigma^{2}}{ \varepsilon^{2}}.\]

Proof.: From Assumption 1, we have

\[f(x^{k+1}) \leq f(x^{k})+\left\langle\nabla f(x^{k}),x^{k+1}-x^{k}\right\rangle +\frac{L}{2}\left\|x^{k+1}-x^{k}\right\|^{2}\] \[=f(x^{k})-\gamma\left\langle\nabla f(x^{k}),g(x^{k})\right\rangle +\frac{L\gamma^{2}}{2}\left\|g(x^{k})\right\|^{2}.\]

We denote \(\mathcal{G}^{k}\) as a sigma-algebra generated by \(g(x^{0}),\ldots,g(x^{k-1}).\) Using unbiasedness and (65), we obtain

\[\mathbb{E}\left[\left.f(x^{k+1})\right|\mathcal{G}^{k}\right] \leq f(x^{k})-\gamma\left(1-\frac{L\gamma}{2}\right)\left\|\nabla f (x^{k})\right\|^{2}+\frac{L\gamma^{2}\sigma^{2}}{2}\mathbb{E}\left[\left. \left\|g(x^{k})-\nabla f(x^{k})\right\|^{2}\right|\mathcal{G}^{k}\right]\] \[\leq f(x^{k})-\gamma\left(1-\frac{L\gamma}{2}\right)\left\|\nabla f (x^{k})\right\|^{2}+\frac{L\gamma^{2}\sigma^{2}}{2}.\]

Since \(\gamma\leq\nicefrac{{1}}{{L}}\), we get

\[\mathbb{E}\left[\left.f(x^{k+1})\right|\mathcal{G}^{k}\right] \leq f(x^{k})-\frac{\gamma}{2}\left\|\nabla f(x^{k})\right\|^{2}+\frac{L \gamma^{2}\sigma^{2}}{2}.\]We subtract \(f^{*}\) and take the full expectation to obtain

\[\mathbb{E}\left[f(x^{k+1})-f^{*}\right]\leq\mathbb{E}\left[f(x^{k})-f^{*}\right]- \frac{\gamma}{2}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|\right]^{2}+\frac {L\gamma^{2}\sigma^{2}}{2}.\]

Next, we sum the inequality for \(k\in\{0,\ldots,K-1\}\):

\[\mathbb{E}\left[f(x^{K})-f^{*}\right] \leq f(x^{0})-f^{*}-\sum_{k=0}^{K-1}\frac{\gamma}{2}\mathbb{E} \left[\left\|\nabla f(x^{k})\right\|^{2}\right]+\frac{KL\gamma^{2}\sigma^{2}}{2}\] \[=\Delta-\sum_{k=0}^{K-1}\frac{\gamma}{2}\mathbb{E}\left[\left\| \nabla f(x^{k})\right\|^{2}\right]+\frac{KL\gamma^{2}\sigma^{2}}{2}.\]

Finally, we rearrange the terms and use that \(\mathbb{E}\left[f(x^{K})-f^{*}\right]\geq 0\):

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2} \right]\leq\frac{2\Delta}{\gamma K}+L\gamma\sigma^{2}.\]

The choice of \(\gamma\) and \(K\) ensures that

\[\frac{1}{K}\sum_{k=0}^{K-1}\mathbb{E}\left[\left\|\nabla f(x^{k})\right\|^{2} \right]\leq\varepsilon.\]

**Theorem 22**.: _Let Assumptions 4 and 5 hold. We consider the SGD method:_

\[x^{k+1}=x^{k}-\gamma g(x^{k}),\]

_where_

\[\gamma=\frac{\varepsilon}{M^{2}+\sigma^{2}}\]

_For all \(k\geq 0,\) the vector \(g(x)\) is a random vector such that \(\mathbb{E}\left[\left.g(x^{k})\right|\mathcal{G}_{k}\right]\in\partial f(x^{k})\)_

\[\mathbb{E}\left[\left\|g(x^{k})-\mathbb{E}\left[\left.g(x^{k})\right|\mathcal{ G}_{k}\right]\right\|^{2}\right|\mathcal{G}_{k}\right]\leq\sigma^{2},\]

_where \(\mathcal{G}_{k}\) is a \(\sigma\)-algebra generated by \(g(x^{0}),\ldots,g(x^{k-1})\). Then_

\[\mathbb{E}\left[f\left(\frac{1}{K}\sum_{k=0}^{K-1}x^{k}\right)\right]-f(x^{*})\leq\varepsilon \tag{66}\]

_for_

Proof.: We denote \(\mathcal{G}^{k}\) as a sigma-algebra generated by \(g(x^{0}),\ldots,g(x^{k-1})\). Using the convexity, for all \(x\in\mathbb{R}^{d},\) we have

\[f(x)\geq f(x^{k})+\left\langle\mathbb{E}\left[\left.g(x^{k})\right|\mathcal{G} ^{k}\right],x-x^{k}\right\rangle=f(x^{k})+\mathbb{E}\left[\left.\left\langle g (x^{k}),x-x^{k}\right\rangle\right|\mathcal{G}^{k}\right].\]

Note that

\[\left\langle g(x^{k}),x-x^{k}\right\rangle =\left\langle g(x^{k}),x^{k+1}-x^{k}\right\rangle+\left\langle g (x^{k}),x-x^{k+1}\right\rangle\] \[=-\gamma\left\|g(x^{k})\right\|^{2}+\frac{1}{\gamma}\left\langle x ^{k}-x^{k+1},x-x^{k+1}\right\rangle\] \[=-\gamma\left\|g(x^{k})\right\|^{2}+\frac{1}{2\gamma}\left\|x^{k }-x^{k+1}\right\|^{2}+\frac{1}{2\gamma}\left\|x-x^{k+1}\right\|^{2}-\frac{1}{ 2\gamma}\left\|x-x^{k}\right\|^{2}\] \[=-\frac{\gamma}{2}\left\|g(x^{k})\right\|^{2}+\frac{1}{2\gamma} \left\|x-x^{k+1}\right\|^{2}-\frac{1}{2\gamma}\left\|x-x^{k}\right\|^{2}\]and

\[\mathbb{E}\left[\,\left\|g(x^{k})\right\|^{2}\right|\mathcal{G}^{k} \right]=\mathbb{E}\left[\,\left\|g(x^{k})-\mathbb{E}\left[\,g(x^{k})\right| \mathcal{G}^{k}\right]\big{\|}^{2}\Big{|}\mathcal{G}^{k}\right]+\left\|\mathbb{ E}\left[\,g(x^{k})\right|\mathcal{G}^{k}\right]\big{\|}^{2}\leq\sigma^{2}+M^{2}.\]

Therefore, we get

\[f(x^{k}) \leq f(x)+\mathbb{E}\left[\,\big{\langle}g(x^{k}),x^{k}-x\big{\rangle} \right|\mathcal{G}^{k}\right]\] \[=f(x)+\frac{\gamma}{2}\mathbb{E}\left[\,\left\|g(x^{k})\right\|^{ 2}\Big{|}\mathcal{G}^{k}\right]+\frac{1}{2\gamma}\left\|x-x^{k}\right\|^{2}- \frac{1}{2\gamma}\mathbb{E}\left[\,\left\|x-x^{k+1}\right\|^{2}\Big{|} \mathcal{G}^{k}\right]\] \[\leq f(x)+\frac{\gamma}{2}\left(M^{2}+\sigma^{2}\right)+\frac{1}{ 2\gamma}\left\|x-x^{k}\right\|^{2}-\frac{1}{2\gamma}\mathbb{E}\left[\,\left\| x-x^{k+1}\right\|^{2}\right]\mathcal{G}^{k}\right].\]

By taking the full expectation and summing the last inequality for \(t\) from \(0\) to \(K-1\), we obtain

\[\mathbb{E}\left[\sum_{k=0}^{K-1}f(x^{k})\right] \leq Kf(x)+\frac{K\gamma}{2}\left(M^{2}+\sigma^{2}\right)+\frac{1} {2\gamma}\left\|x-x^{0}\right\|^{2}-\frac{1}{2\gamma}\mathbb{E}\left[\,\left\| x-x^{K}\right\|^{2}\right]\] \[\leq Kf(x)+\frac{K\gamma}{2}\left(M^{2}+\sigma^{2}\right)+\frac{1} {2\gamma}\left\|x-x^{0}\right\|^{2}.\]

Let divide the last inequality by \(K\), take \(x=x^{*}\), and use the convexity:

\[\mathbb{E}\left[f\left(\frac{1}{K}\sum_{k=0}^{K-1}x^{k}\right) \right]-f(x^{*})\leq\frac{\gamma}{2}\left(M^{2}+\sigma^{2}\right)+\frac{1}{2 \gamma K}\left\|x^{*}-x^{0}\right\|^{2}.\]

The choices of \(\gamma\) and \(K\) ensure that (66) holds. 

## Appendix J Experiments

We now consider Fragile SGD with Minibatch SGD on quadratic optimization tasks with stochastic gradients. The working environment was emulated in Python 3.8 with one Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz. The homogeneous optimization problem (1) is constructed in the following way. We take

\[f(x)=\frac{1}{2}x^{\top}\mathbf{A}x-b^{\top}x\quad\forall x\in \mathbb{R}^{d},\]

\(d=1000,\)

\[\mathbf{A}=\frac{1}{4}\left(\begin{array}{cccc}2&-1&&0\\ -1&\ddots&\ddots&\\ &\ddots&\ddots&-1\\ 0&&-1&2\end{array}\right)\in\mathbb{R}^{d\times d},\quad\text{ and }\quad b=\frac{1}{4}\left[\begin{array}{c}-1\\ 0\\ \vdots\\ 0\end{array}\right]\in\mathbb{R}^{d}.\]

Let us define \([x]_{j}\) as the \(j^{\text{th}}\) index of a vector \(x\in\mathbb{R}^{d}\). All \(n\) workers calculate the stochastic gradients

\[[\nabla f(x;\xi)]_{j}:=[\nabla f(x)]_{j}\left(1+1\left[j>\text{prog}(x)\right] \left(\frac{\xi}{p}-1\right)\right)\quad\forall x\in\mathbb{R}^{d},\forall i \in[n],\]

where \(\xi\sim\text{Bernouilli}(p)\), \(p\in(0,1]\). In our experiments, we take \(p=0.001\) and the starting point \(x^{0}=[\sqrt{d},0,\dots,0]^{\top}\). We emulate our setup by considering that the \(i^{\text{th}}\) worker requires \(h_{i}=1\) second to calculate a stochastic gradient. And we assume that the workers have the structure of 2D-Mesh (see Figure 3(a)) and take \(\rho_{i\to j}=\rho\in\{0.1,1,10\}\) seconds for all edges that connect workers in 2D-Mesh. We take \(n=100\). In all methods we fine-tune step sizes from the set \(\{2^{i}\,|\,i\in[-20,20]\}\). In Fragile SGD, we fine-tune the batch size \(S\) from the set \(\{10,20,40,80,120\}\).

The results are presented in Figures 5, 6, and 7. The plots are fully consisted with Table 1. One can see that when the communication is fast (Fig. 5), there is no big difference between the methods because both Fragile SGD and Minibatch SGD use all workers in the optimization steps. However, when we start decreasing the communication speed, we observe that Fragile SGD converges faster.

We looked deeper into the optimization processes of Fragile SGD in Figure 7 and observed that only 13 of 100 workers contribute to the optimization process for the batch size \(S=120\). Other workers are too far away from the pivot worker, and their contributions can only slow down optimization.

Figure 5: The communication time \(\rho=0.1\) seconds (Fast communication)

Figure 6: The communication time \(\rho=1\) seconds (Medium speed communication)

Figure 7: The communication time \(\rho=10\) seconds (Slow communication)

### Experiments with Logistic Regression: Fast vs Slow Communication

We now repeat the previous experiments but with logistic regression on _MNIST_ dataset (LeCun et al., 2010) with \(100\) workers. We consider two regimes: fast and slow communication between workers. One can see that when the communication is fast, the gap between the methods is small, which is expected and compliant with the theory. However, Fragile SGD is much faster and has better test accuracy when the communication is slow.

### Experiments with ResNet-18

We test algorithms on an image recognition task, _CIFAR10_(Krizhevsky et al., 2009), with the _ResNet-18_(He et al., 2016) deep neural network (the number of parameters \(d\approx 10^{7}\)). We use the torus structure and 9 workers. We run all methods with the step sizes \(\{0.025,0.25,2.5\}\). Our findings from the low-scale experiments are also evident in the large-scale experiments. Fragile SGD converges faster than Minibatch SGD in terms of function values. When we compare accuracies on the test split of _MNIST_, the superiority of Fragile SGD is even more transparent.

Figure 8: The communication time \(\rho=0.1\) seconds (Fast communication)

Figure 10: _ResNet-18_ on _CIFAR10_ dataset with 9 workers and the torus structure with the communication time \(\rho=1\) seconds (Medium communication)

Figure 9: The communication time \(\rho=10\) seconds (Slow communication)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Table 1, the main part of the paper, Section C Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section 5.3 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Section1.1, and the appendix Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section J Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification: The code in the supplementary materials.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes]

Justification: Section J Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide the top 3 best plots for each algorithm to reduce randomness factors in Section J. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Section J Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We have read the code of ethics, and our paper does not violate it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We consider a mathematical problem for machine learning. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We consider a mathematical problem for machine learning. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ** If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code and documentation in the supplementary materials. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. *