# Provable Guarantees for Nonlinear Feature Learning

in Three-Layer Neural Networks

 Eshaan Nichani

Princeton University

eshnich@princeton.edu &Alex Damian

Princeton University

ad27@princeton.edu &Jason D. Lee

Princeton University

jasonlee@princeton.edu

###### Abstract

One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings - single-index models and functions of quadratic features - and show that in the latter setting three-layer networks obtain a sample complexity improvement over all existing guarantees for two-layer networks. Crucially, this sample complexity improvement relies on the ability of three-layer networks to efficiently learn _nonlinear_ features. We then establish a concrete optimization-based depth separation by constructing a function which is efficiently learnable via gradient descent on a three-layer network, yet cannot be learned efficiently by a two-layer network. Our work makes progress towards understanding the provable benefit of three-layer neural networks over two-layer networks in the feature learning regime.

## 1 Introduction

The success of modern deep learning can largely be attributed to the ability of deep neural networks to decompose the target function into a hierarchy of learned features. This feature learning process enables both improved accuracy [29] and transfer learning [21]. Despite its importance, we still have a rudimentary theoretical understanding of the feature learning process. Fundamental questions include understanding what features are learned, how they are learned, and how they affect generalization.

From a theoretical viewpoint, a fascinating question is to understand how depth can be leveraged to learn more salient features and as a consequence a richer class of hierarchical functions. The base case for this question is to understand which features (and function classes) can be learned efficiently by three-layer neural networks, but not two-layer networks. Recent work on feature learning has shown that two-layer neural networks learn features which are _linear_ functions of the input (see Section 1.2 for further discussion). It is thus a natural question to understand if three-layer networks can learn _nonlinear_ features, and how this can be leveraged to obtain a sample complexity improvement. Initial learning guarantees for three-layer networks [16, 5, 4], however, consider simplified model and function classes and do not discern specifically what the learned features are or whether a sample complexity improvement can be obtained over shallower networks or kernel methods.

On the other hand, the standard approach in deep learning theory to understand the benefit of depth has been to establish "depth separations" [51], i.e. functions that cannot be efficiently approximated by shallow networks, but can be via deeper networks. However, depth separations are solely concerned with the representational capability of neural networks, and ignore the optimization and generalization aspects. In fact, depth separation functions such as [51] are often not learnable via gradient descent [36]. To reconcile this, recent papers [45; 44] have established _optimization-based_ depth separation results, which are functions which cannot be efficiently learned using gradient descent on a shallow network but can be learned with a deeper network. We thus aim to answer the following question:

_What features are learned by gradient descent on a three-layer neural network, and can these features be leveraged to obtain a provable sample complexity guarantee?_

### Our contributions

We provide theoretical evidence that three-layer neural networks have provably richer feature learning capabilities than their two-layer counterparts. We specifically study the features learned by a three-layer network trained with a layer-wise variant of gradient descent (Algorithm 1). Our main contributions are as follows.

* Theorem 1 is a general purpose sample complexity guarantee for Algorithm 1 to learn an arbitrary target function \(f^{*}\). We first show that Algorithm 1 learns a feature roughly corresponding to a low-frequency component of the target function \(f^{*}\) with respect to the random feature kernel \(\mathbb{K}\) induced by the first layer. We then derive an upper bound on the population loss in terms of the learned feature. As a consequence, we show that if \(f^{*}\) possesses a hierarchical structure where it can be written as a 1D function of the learned feature (detailed in Section 3), then the sample complexity for learning \(f^{*}\) is equal to the sample complexity of learning the feature. This demonstrates that three-layer networks indeed perform hierarchical learning.
* We next instantiate Theorem 1 in two statistical learning settings which satisfy such hierarchical structure. As a warmup, we show that Algorithm 1 learns single-index models (i.e \(f^{*}(x)=g^{*}(w\cdot x)\)) in \(d^{2}\) samples, which is comparable to existing guarantees for two-layer networks and crucially has \(d\)-dependence not scaling with the degree of the link function \(g^{*}\). We next show that Algorithm 1 learns the target \(f^{*}(x)=g^{*}(x^{T}Ax)\), where \(g^{*}\) is either Lipschitz or a degree \(p=O(1)\) polynomial, up to \(o_{d}(1)\) error with \(d^{4}\) samples. This improves on all existing guarantees for learning with two-layer networks or via NTK-based approaches, which all require sample complexity \(d^{\Omega(p)}\). A key technical step is to show that for the target \(f^{*}(x)=g^{*}(x^{T}Ax)\), the learned feature is approximately \(x^{T}Ax\). This argument relies on the universality principle in high-dimensional probability, and may be of independent interest.
* We conclude by establishing an explicit optimization-based depth separation: We show that the target function \(f^{*}(x)=\operatorname{ReLU}(x^{T}Ax)\) for appropriately chosen \(A\) can be learned by Algorithm 1 up to \(o_{d}(1)\) error in \(d^{4}\) samples, whereas any two layer network needs either superpolynomial width or weight norm in order to approximate \(f^{*}\) up to comparable accuracy. This implies that such an \(f^{*}\) is not efficiently learnable via two-layer networks.

The above separation hinges on the ability of three-layer networks to learn the nonlinear feature \(x^{T}Ax\) and leverage this feature learning to obtain an improved sample complexity. Altogether, our work presents a general framework demonstrating the capability of three-layer networks to learn nonlinear features, and makes progress towards a rigorous understanding of feature learning, optimization-based depth separations, and the role of depth in deep learning more generally.

### Related Work

Neural Networks and Kernel Methods.Early guarantees for neural networks relied on the Neural Tangent Kernel (NTK) theory [31; 50; 23; 17]. The NTK theory shows global convergence by coupling to a kernel regression problem and generalization via the application of kernel generalization bounds [7; 15; 5]. The NTK can be characterized explicitly for certain data distributions [27; 39; 38], which allows for tight sample complexity and width analyses. This connection to kernel methods has also been used to study the role of depth, by analyzing the signal propagation and evolution of the NTK in MLPs [42; 48; 28], convolutional networks [6; 55; 56; 40], and residual networks [30]. However, the NTK theory is insufficient as neural networks outperform their NTK in practice [6; 32]. In fact, [27] shows that kernels cannot adapt to low-dimensional structure and require \(d^{k}\) samples to learn any degree \(k\) polynomials in \(d\) dimensions. Ultimately, the NTK theory fails to explain generalization or the role of depth in practical networks not in the kernel regime. A key distinction is that networks in the kernel regime cannot learn features [57]. A recent goal has thus been to understand the feature learning mechanism and how this leads to sample complexity improvements [53; 22; 58; 3; 25; 26; 20; 54; 33; 35]. Crucially, our analysis is _not_ in the kernel regime, and shows an improvement of three-layer networks over two-layer networks in the feature-learning regime.

Feature Learning.Recent work has studied the provable feature learning capabilities of two-layer neural networks. [9; 1; 2; 8; 13; 10] show that for isotropic data distributions, two-layer networks learn linear features of the data, and thus efficiently learn functions of low-dimensional projections of the input (i.e targets of the form \(f^{*}(x)=g(Ux)\) for \(U\in\mathbb{R}^{r\times d}\)). Here, \(x\mapsto Ux\) is the "linear feature." Such target functions include low-rank polynomials [18; 2] and single-index models [8; 13] for Gaussian covariates, as well as sparse boolean functions [1] such as the \(k\)-sparse parity problem [10] for covariates uniform on the hypercube. [43] draws connections from the mechanisms in these works to feature learning in standard image classification settings. The above approaches rely on layerwise training procedures, and our Algorithm 1 is an adaptation of the algorithm in [18].

Another approach uses the quadratic Taylor expansion of the network to learn classes of polynomials [9; 41] This approach can be extended to three-layer networks. [16] replace the outermost layer with its quadratic approximation, and by viewing \(z^{p}\) as the hierarchical function \((z^{p/2})^{2}\) show that their three-layer network can learn low rank, degree \(p\) polynomials in \(d^{p/2}\) samples. [5] similarly uses a quadratic approximation to improperly learn a class of three-layer networks via sign-randomized GD. An instantiation of their upper bound to the target \(g^{*}(x^{T}Ax)\) for degree \(p\) polynomial \(g^{*}\) yields a sample complexity of \(d^{p+1}\). However, [16; 5] are proved via opaque landscape analyses, do not concretely identify the learned features, and rely on nonstandard algorithmic modifications. Our Theorem 1 directly identifies the learned features, and when applied to the quadratic feature setting in Section 4.2 obtains an improved sample complexity guarantee independent of the degree of \(g^{*}\).

Depth Separations.[51] constructs a function which can be approximated by a poly-width network with large depth, but not with smaller depth. [24] is the first depth separation between depth 2 and 3 networks, with later works [46; 19; 47] constructing additional such examples. However, such functions are often not learnable via three-layer networks [34]. [36] shows that approximatability by a shallow (depth 3 network) is a necessary condition for learnability via a deeper network.

These issues have motivated the development of optimization-based, or algorithmic, depth separations, which construct functions which are learnable by a three-layer network but not by two-layer networks. [45] shows that certain ball indicator functions \(\mathbf{1}(\|x\|\geq\lambda)\) are not approximatable by two-layer networks, yet are learnable via GD on a special variant of a three-layer network with second layer width equal to 1. However, their network architecture is tailored for learning the ball indicator, and the explicit polynomial sample complexity (\(n\gtrsim d^{36}\)) is weak. [44] shows that a multi-layer mean-field network with a 1D bottleneck layer can learn the target \(\operatorname{ReLU}(1-\|x\|)\), which [47] previously showed was inaproximatable via two-layer networks. However, their analysis relies on the rotational invariance of the target function, and it is difficult to read off explicit sample complexity and width guarantees beyond being \(\operatorname{poly}(d)\). Our Section 4.2 shows that three-layer networks can learn a larger class of features (\(x^{T}Ax\) versus \(\|x\|\)) and functions on top of these features (any Lipschitz \(q\) versus \(\operatorname{ReLU}\)), with explicit dependence on the width and sample complexity needed (\(n,m_{1},m_{2}=\tilde{O}(d^{4})\)).

## 2 Preliminaries

### Problem Setup

Data distribution.Our aim is to learn the target function \(f^{*}:\mathcal{X}_{d}\rightarrow\mathbb{R}\),with \(\mathcal{X}_{d}\subset\mathbb{R}^{d}\) the space of covariates. We let \(\nu\) be some distribution on \(\mathcal{X}_{d}\), and draw two independent datasets \(\mathcal{D}_{1},\mathcal{D}_{2}\), each with \(n\) samples, so that each \(x\in\mathcal{D}_{1}\) or \(x\in\mathcal{D}_{2}\) is sampled i.i.d as \(x\sim\nu\). Without loss of generality, we normalize so \(\mathbb{E}_{x\sim\nu}[f^{*}(x)^{2}]\leq 1\). We make the following assumptions on \(\nu\):

**Definition 1** (Sub-Gaussian Vector).: _A mean-zero random vector \(X\in\mathbb{R}^{d}\) is \(\gamma\)-subGaussian if, for all unit vectors \(v\in\mathbb{R}^{d}\), \(\mathbb{E}[\exp(\lambda X\cdot v)]\leq\exp(\gamma^{2}\lambda^{2})\) for all \(\lambda\in\mathbb{R}\)._

**Assumption 1**.: \(\mathbb{E}_{x\sim\nu}[x]=0\) _and \(\nu\) is \(C_{\gamma}\)-subGaussian for some constant \(C_{\gamma}\)._

**Assumption 2**.: \(f^{*}\) _has polynomially growing moments, i.e there exist constants \((C_{f},\ell)\) such that \(\mathbb{E}_{x\sim\nu}\left[f^{*}(x)^{q}\right]^{1/q}\leq C_{f}q^{\ell}\) for all \(q\geq 1\)._

We note that Assumption 2 is satisfied by a number of common distributions and functions, and we will verify that Assumption 2 holds for each example in Section 4.

Three-layer neural network.Let \(m_{1},m_{2}\) be the two hidden layer widths, and \(\sigma_{1},\sigma_{2}\) be two activation functions. Our learner is a three-layer neural network parameterized by \(\theta=(a,W,b,V)\), where \(a\in\mathbb{R}^{m_{1}},W\in\mathbb{R}^{m_{1}\times m_{2}}\), \(b\in\mathbb{R}^{m_{1}}\), and \(V\in\mathbb{R}^{m_{2}\times d}\). The network \(f(x;\theta)\) is defined as:

\[f(x;\theta):=\frac{1}{m_{1}}a^{T}\sigma_{1}(W\sigma_{2}(Vx)+b)= \frac{1}{m_{1}}\sum_{i=1}^{m_{1}}a_{i}\sigma_{1}\Big{(}\langle w_{i},h^{(0)}( x)\rangle+b_{i}\Big{)}. \tag{1}\]

Here, \(w_{i}\in\mathbb{R}^{m_{2}}\) is the \(i\)th row of \(W\), and \(h^{(0)}(x):=\sigma_{2}(Vx)\in\mathbb{R}^{m_{2}}\) is the random feature embedding arising from the innermost layer. The parameter vector \(\theta^{(0)}:=(a^{(0)},W^{(0)},b^{(0)},V^{(0)})\) is initialized with \(a_{i}^{(0)}\sim_{iid}\operatorname{Unif}(\{\pm 1\})\), \(W^{(0)}=0\), the biases \(b_{i}^{(0)}\sim_{iid}\mathcal{N}(0,1)\), and the rows \(v_{i}^{(0)}\) of \(V^{(0)}\) drawn \(v_{i}\sim_{iid}\tau\), where \(\tau\) is the uniform measure on \(\mathcal{S}^{d-1}(1)\), the \(d\)-dimensional unit sphere. We make the following assumption on the activations, and note that the polynomial growth assumption on \(\sigma_{2}\) is satisfied by all activations used in practice.

**Assumption 3**.: \(\sigma_{1}\) _is the \(\operatorname{ReLU}\) activation, i.e \(\sigma_{1}(z)=\max(z,0)\), and \(\sigma_{2}\) has polynomial growth, i.e \(|\sigma_{2}(x)|\leq C_{\sigma}(1+|x|)^{\alpha_{\sigma}}\) for some constants \(C_{\sigma},\alpha_{\sigma}>0\)._

Training Algorithm.Let \(L_{i}(\theta)\) denote the empirical loss on dataset \(\mathcal{D}_{i}\); that is for \(i=1,2\): \(L_{i}(\theta):=\frac{1}{\hbar}\sum_{x\in\mathcal{D}_{i}}\left(f(x;\theta)-f^{* }(x)\right)^{2}\). Our network is trained via layer-wise gradient descent with sample splitting. Throughout training, the first layer weights \(V\) and second layer bias \(b\) are held constant. First, the second layer weights \(W\) are trained for \(t=1\) timesteps. Next, the outer layer weights \(a\) are trained for \(t=T-1\) timesteps. This two stage training process is common in prior works analyzing gradient descent on two-layer networks [18; 8; 1; 10], and as we see in Section 5, is already sufficient to establish a separation between two and three-layer networks. Pseudocode for the training procedure is presented in Algorithm 1.

``` Input: Initialization \(\theta^{(0)}\); learning rates \(\eta_{1},\eta_{2}\); weight decay \(\lambda\); time \(T\) {Stage 1: Train \(W\)} \(W^{(1)}\gets W^{(0)}-\eta_{1}\nabla_{W}L_{1}(\theta^{(0)})\) \(a^{(1)}\gets a^{(0)}\) \(\theta^{(1)}\leftarrow(a^{(1)},W^{(1)},b^{(0)},V^{(0)})\) {Stage 2: Train \(a\)} for\(t=2,\cdots,T\)do \(a^{(t)}\gets a^{(t-1)}-\eta_{2}\big{[}\nabla_{a}L_{2}(\theta^{(t-1)})+ \lambda a^{(t_and corresponding integral operator_

\[(\mathbb{K}f)(x):=\mathbb{E}_{x^{\prime}\sim\nu}[K(x,x^{\prime})f(x^{\prime})]. \tag{3}\]

We make the following assumption on \(\mathbb{K}\), which we verify for the examples in Section 4:

**Assumption 4**.: \(\mathbb{K}f^{*}\) _has polynomially bounded moments, i.e there exist constants \(C_{K},\chi\) such that, for all \(1\leq q\leq d\), \(\left\|\mathbb{K}f^{*}\right\|_{L^{q}(\nu)}\leq C_{K}q^{\chi}\left\|\mathbb{K}f ^{*}\right\|_{L^{2}(\nu)}\)._

We also require the definition of the Sobolev space:

**Definition 3**.: _Let \(\mathcal{W}^{2,\infty}([-1,1])\) be the Sobolev space of twice continuously differentiable functions \(q:[-1,1]\to\mathbb{R}\) equipped with the norm \(\left\|q\right\|_{k,\infty}:=\max_{s\leq k}\max_{x\in[-1,1]}\left|q^{(s)}(x)\right|\) for \(k=1,2\)._

### Notation

We use big \(O\) notation (i.e \(O,\Theta,\Omega\)) to ignore absolute constants (\(C_{\sigma},C_{f}\), etc.) that do not depend on \(d,n,m_{1},m_{2}\). We further write \(a_{d}\lesssim b_{d}\) if \(a_{d}=O(b_{d})\), and \(a_{d}=o(b_{d})\) if \(\lim_{d\to\infty}a_{d}/b_{d}=0\). Additionally, we use \(\tilde{O}\) notation to ignore terms that depend logarithmically on \(dnm_{1}m_{2}\). For \(f:\mathcal{X}_{d}\to\mathbb{R}\), define \(\left\|f\right\|_{L^{p}(\mathcal{X}_{d},\nu)}=(\mathbb{E}_{x\sim\nu}[f(x)^{p} ])^{1/p}\). To simplify notation we also call this quantity \(\left\|f\right\|_{L^{p}(\nu)}\), and \(\left\|g\right\|_{L^{p}(\mathcal{X}_{d},\nu)},\left\|g\right\|_{L^{p}(\tau)}\) are defined analogously for functions \(g:\mathcal{S}^{d-1}(1)\to\mathbb{R}\). When the domain is clear from context, we write \(\left\|f\right\|_{L^{p}},\left\|g\right\|_{L^{p}}\). We let \(L^{p}(\mathcal{X}_{d},\nu)\) be the space of \(f\) with finite \(\left\|f\right\|_{L^{p}(\mathcal{X}_{d},\nu)}\). Finally, we write \(\mathbb{E}_{x}\) and \(\mathbb{E}_{v}\) as shorthand for \(\mathbb{E}_{x\sim\nu}\) and \(\mathbb{E}_{v\sim\tau}\) respectively.

## 3 Main Result

The following is our main theorem which upper bounds the population loss of Algorithm 1:

**Theorem 1**.: _Select \(q\in\mathcal{W}^{2,\infty}([-1,1])\). Let \(\eta_{1}=\frac{m_{1}}{m_{2}}\overline{\eta}\), and assume \(n,m_{1},m_{2}=\tilde{\Omega}(\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-2})\) There exist \(\overline{\eta},\lambda,\eta_{2}\) such that after \(T=\operatorname{poly}(n,m_{1},m_{2},d,\left\|q\right\|_{2,\infty})\) timesteps, with high probability over the initialization and datasets the output \(\hat{\theta}\) of Algorithm 1 satisfies the population \(L^{2}\) loss bound_

\[\mathbb{E}_{x} \bigg{[}\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)}^{2}\bigg{]}\] \[\leq\tilde{O}\Bigg{(}\underbrace{\left\|q\circ(\overline{\eta} \cdot\mathbb{K}f^{*})-f^{*}\right\|_{L^{2}}^{2}}_{\text{\begin{subarray}{c} accuracy of\\ feature learning \end{subarray}}}+\underbrace{\frac{\left\|q\right\|_{1,\infty}^{2}\left\| \mathbb{K}f^{*}\right\|_{L^{2}}^{-2}}{\min(n,m_{1},m_{2})}}_{\text{\begin{subarray}{c} sample complexity of\\ feature learning\end{subarray}}}+\underbrace{\frac{\left\|q\right\|_{2,\infty}^{2} }{m_{1}}+\underbrace{\left\|q\right\|_{2,\infty}^{2}+1}_{\text{complexity of }q}}_{\text{\begin{subarray}{c}complexity of q\end{subarray}}}\Bigg{)} \tag{4}\]

The full proof of this theorem is in Appendix D. The population risk upper bound (4) has three terms:

1. The first term quantifies the extent to which feature learning is useful for learning the target \(f^{*}\), and depends on how close \(f^{*}\) is to having hierarchical structure. Concretely, if there exists \(q:\mathbb{R}\to\mathbb{R}\) such that the compositional function \(q\circ\overline{\eta}\cdot\mathbb{K}f^{*}\) is close to the target \(f^{*}\), then this first term is small. In Section 4, we show that this is true for certain _hierarchical_ functions. In particular, say that \(f^{*}\) satisfies the hierarchical structure \(f^{*}=g^{*}\circ h^{*}\). If the quantity \(\mathbb{K}f^{*}\) is nearly proportional to the true feature \(h^{*}\), then this first term is negligible. As such, we refer to the quantity \(\mathbb{K}f^{*}\) as the _learned feature_.
2. The second term is the sample (and width) complexity of learning the feature \(\mathbb{K}f^{*}\). It is useful to compare the \(\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-2}\) term to the standard kernel generalization bound, which requires \(n\gtrsim\left\langle f^{*},\mathbb{K}^{-1}f^{*}\right\rangle_{L^{2}}\). Unlike in the kernel bound, the feature learning term in (4) does not require inverting the kernel \(\mathbb{K}\) as it only requires a lower bound on \(\left\|\mathbb{K}f^{*}\right\|_{L^{2}}\). This difference can be best understood by considering the alignment of \(f^{*}\) with the eigenfunctions of \(\mathbb{K}\). Say that \(f^{*}\) has nontrivial alignment with eigenfunctions of \(\mathbb{K}\) with both small \((\lambda_{min})\) and large \((\lambda_{max})\) eigenvalues. Kernel methods require \(\Omega(\lambda_{min}^{-1})\) samples, which blows up when \(\lambda_{min}\) is small; the sample complexity of kernel methods depends on the high frequency components of \(f^{*}\). On the other hand, the guarantee in Theorem 1 scales with \(\|\mathbb{K}f^{*}\|_{L^{2}}^{2}=O(\lambda_{max}^{-2})\), which can be much smaller. In other words, the sample complexity of feature learning scales with the low-frequency components of \(f^{*}\). The feature learning process can thus be viewed as extracting the low-frequency components of the target.
3. The last two terms measure the complexity of learning the univariate function \(q\). In the examples in Section 4, the effect of these terms is benign.

Altogether, if \(f^{*}\) satisfies the hierarchical structure that its high-frequency components can be inferred from the low-frequency ones, then a good \(q\) for Theorem 1 exists and the dominant term in (4) is the sample complexity of feature learning term, which only depends on the low-frequency components. This is not the case for kernel methods, as small eigenvalues blow up the sample complexity. As we show in Section 4, this ability to ignore the small eigenvalue components of \(f^{*}\) during the feature learning process is critical for achieving good sample complexity in many problems.

### Proof Sketch

At initialization, \(f(x;\theta^{(0)})\approx 0\). The first step of GD on the population loss for a neuron \(w_{j}\) is thus

\[w^{(1)} =-\eta_{1}\nabla_{w_{j}}\mathbb{E}_{x}\bigg{[}\Big{(}f(x;\theta^{ (0)})-f^{*}(x)\Big{)}^{2}\bigg{]} \tag{5}\] \[=\eta_{1}\mathbb{E}_{x}\Big{[}f^{*}(x)\nabla_{w_{j}}f(x;\theta^{ (0)})\Big{]}\] (6) \[=\frac{1}{m_{2}}\overline{\eta}\mathbf{1}_{b_{j}^{(0)}\geq 0}a_{j }^{(0)}\mathbb{E}_{x}\Big{[}f^{*}(x)h^{(0)}(x)\Big{]}. \tag{7}\]

Therefore the network \(f(x^{\prime};\theta^{(1)})\) after the first step of GD is given by

\[f(x^{\prime};\theta^{(1)}) =\frac{1}{m_{1}}\sum_{j=1}^{m_{1}}a_{j}\sigma_{1}\Big{(}\langle w _{j}^{(1)},h^{(0)}(x^{\prime})\rangle+b_{j}\Big{)} \tag{8}\] \[=\frac{1}{m_{1}}\sum_{j=1}^{m_{1}}a_{j}\sigma_{1}\bigg{(}a_{j}^{( 0)}\cdot\overline{\eta}\frac{1}{m_{2}}\mathbb{E}_{x}\Big{[}f^{*}(x)h^{(0)}(x)^ {T}h^{(0)}(x^{\prime})\Big{]}+b_{j}\Big{)}\mathbf{1}_{b_{j}^{(0)}\geq 0}. \tag{9}\]

We first notice that this network now implements a 1D function of the quantity

\[\phi(x^{\prime}):=\overline{\eta}\frac{1}{m_{2}}\mathbb{E}_{x}\Big{[}f^{*}(x)h^ {(0)}(x)^{T}h^{(0)}(x^{\prime})\Big{]}. \tag{10}\]

Specifically, the network can be rewritten as

\[f(x^{\prime};\theta^{(1)})=\frac{1}{m_{1}}\sum_{j=1}^{m_{1}}a_{j}\sigma_{1} \Big{(}a_{j}^{(0)}\cdot\phi(x^{\prime})+b_{j}\Big{)}\cdot\mathbf{1}_{b_{j}^{(0 )}\geq 0}. \tag{11}\]

Since \(f\) implements a hierarchical function of the quantity \(\phi(x)\), we term \(\phi\) the _learned feature_.

The second stage of Algorithm 1 is equivalent to random feature regression. We first use results on ReLU random features to show that any \(q\in\mathcal{W}^{2,\infty}([-1,1])\) can be approximated on \([-1,1]\) as \(q(z)\approx\frac{1}{m_{1}}\sum_{j=1}^{m_{1}}a_{j}^{*}\sigma_{1}(a_{j}^{(0)}z+b _{j})\) for some \(\|a^{*}\|\lesssim\|q\|_{2,\infty}\) (Lemma 3). Next, we use the standard kernel Rademacher bound to show that the excess risk scales with the smoothness of \(q\) (Lemma 5). Hence we can efficiently learn functions of the form \(q\circ\phi\).

It suffices to compute this learned feature \(\phi\). For \(m_{2}\) large, we observe that

\[\phi(x^{\prime})=\frac{\overline{\eta}}{m_{2}}\sum_{j=1}^{m_{2}}\mathbb{E}_{x} [f^{*}(x)\sigma(x\cdot v)\sigma(x^{\prime}\cdot v)])\approx\overline{\eta} \mathbb{E}_{x}[f^{*}(x)K(x,x^{\prime})]=\overline{\eta}(\mathbb{K}f^{*})(x^{ \prime}). \tag{12}\]

The learned feature is thus approximately \(\overline{\eta}\cdot\mathbb{K}f^{*}\). Choosing \(\overline{\eta}\) so that \(|\phi(x^{\prime})|\leq 1\), we see that Algorithm 1 learns functions of the form \(\phi\circ(\overline{\eta}\cdot\mathbb{K}f^{*})\). Finally, we translate the above analysis to the finite sample gradient via standard concentration tools. Since the empirical estimate to \(\mathbb{K}f^{*}\) concentrates at a \(1/\sqrt{n}\) rate, \(n\gtrsim\|\mathbb{K}f^{*}\|_{L^{2}}^{-2}\) samples are needed to obtain a constant factor approximation (Lemma 7).

Examples

We next instantiate Theorem 1 in two specific statistical learning settings which satisfy the hierarchical prescription detailed in Section 3. As a warmup, we show that three-layer networks efficiently learn single index models. Our second example shows how three-layer networks can obtain a sample complexity improvement over existing guarantees for two-layer networks.

### Warmup: single index models

Let \(f^{*}=g^{*}(w^{*}\cdot x)\), for unknown direction \(w^{*}\in\mathbb{R}^{d}\) and unknown link function \(g^{*}:\mathbb{R}\rightarrow\mathbb{R}\), and take \(\mathcal{X}_{d}=\mathbb{R}^{d}\) with \(\nu=\mathcal{N}(0,I)\). Prior work [18; 13] shows that two-layer neural networks learn such functions with an improved sample complexity over kernel methods. Let \(\sigma_{2}(z)=z\), so that the network is of the form \(f(x;\theta)=\frac{1}{m_{1}}a^{T}\sigma_{1}(WYx)\). We can verify that Assumptions 1 to 4 are satisfied, and thus applying Theorem 1 in this setting yields the following:

**Theorem 2**.: _Let \(f^{*}(x)=g^{*}(w^{*}\cdot x)\), where \(\left\|w^{*}\right\|_{2}=1\). Assume that \(g^{*},(g^{*})^{\prime}\) and \((g^{*})^{\prime\prime}\) are polynomially bounded and that \(\mathbb{E}_{z\sim\mathcal{N}(0,1)}[(g^{*})^{\prime}(z)]\neq 0\). Then with high probability Algorithm 1 satisfies the population loss bound_

\[\mathbb{E}_{x}\bigg{[}\Big{(}f(x;\tilde{\theta})-f^{*}(x)\Big{)}^{2}\bigg{]}= \tilde{O}\bigg{(}\frac{d^{2}}{\min(n,m_{1},m_{2})}+\frac{1}{\sqrt{n}}\bigg{)}. \tag{13}\]

Given widths \(m_{1},m_{2}=\tilde{\Theta}(d^{2})\), \(n=\tilde{\Theta}(d^{2})\) samples suffice to learn \(f^{*}\), which matches existing guarantees for two-layer neural networks [13; 18]. We remark that prior work on learning single-index models under assumptions on the link function such as monotonicity or the condition \(\mathbb{E}_{z\sim\mathcal{N}(0,1)}[(g^{*})^{\prime}(z)]\neq 0\) require \(d\) samples [49; 37; 12]. However, our sample complexity improves on that of kernel methods, which require \(d^{p}\) samples when \(g^{*}\) is a degree \(p\) polynomial.

Theorem 2 is proved in Appendix E.1; a brief sketch is as follows. Since \(\sigma_{2}(z)=z\), the kernel is \(K(x,x^{\prime})=\mathbb{E}_{v}[(x\cdot v)(x^{\prime}\cdot v)]=\frac{x\cdot x^{ \prime}}{d}\). By an application of Stein's Lemma, the learned feature is

\[(\mathbb{K}f^{*})(x)=\frac{1}{d}\mathbb{E}_{x^{\prime}}[x\cdot x^{\prime}f^{*} (x)]=\frac{1}{d}x^{T}\mathbb{E}_{x^{\prime}}[\nabla f^{*}(x^{\prime})] \tag{14}\]

Since \(f^{*}(x)=g^{*}(w^{*}\cdot x)\), \(\nabla f^{*}(x)=w^{*}(g^{*})^{\prime}(w^{*}\cdot x)\), and thus

\[(\mathbb{K}f^{*})(x)=\frac{1}{d}\mathbb{E}_{z\sim\mathcal{N}(0,1)}[(g^{*})^{ \prime}(z)]w^{*}\cdot x\propto\frac{1}{d}w^{*}\cdot x. \tag{15}\]

The learned feature is proportional to the true feature, so an appropriate choice of \(\overline{\eta}\) and choosing \(q=g^{*}\) in Theorem 1 implies that \(\|\mathbb{K}f^{*}\|_{L^{2}}^{-2}=d^{2}\) samples are needed to learn \(f^{*}\).

### Functions of quadratic features

The next example shows how three-layer networks can learn nonlinear features, and thus obtain a sample complexity improvement over two-layer networks.

Let \(\mathcal{X}_{d}=\mathcal{S}^{d-1}(\sqrt{d})\), the sphere of radius \(\sqrt{d}\), and \(\nu\) the uniform measure on \(\mathcal{X}_{d}\). The integral operator \(\mathbb{K}\) has been well studied [39; 27; 38], and its eigenfunctions correspond to the spherical harmonics. Preliminaries on spherical harmonics and this eigendecomposition are given in Appendix F.

Consider the target \(f^{*}(x)=g^{*}(x^{T}Ax)\), where \(A\in\mathbb{R}^{d\times d}\) is a symmetric matrix and \(g^{*}:\mathbb{R}\rightarrow\mathbb{R}\) is an unknown link function. In contrast to a single-index model, the feature \(x^{T}Ax\) we aim to learn is a _quadratic_ function of \(x\). Since one can write \(x^{T}Ax=x^{T}\big{(}A-\mathrm{Tr}(A)\cdot\frac{d}{d}\big{)}x+\mathrm{Tr}(A)\), we without loss of generality assume \(\mathrm{Tr}(A)=0\). We also select the normalization \(\left\|A\right\|_{F}^{2}=\frac{d+2}{2d}=\Theta(1)\); this ensures that \(\mathbb{E}_{x\sim\nu}[(x^{T}Ax)^{2}]=1\). We first make the following assumptions on the target function.:

**Assumption 5**.: \(\mathbb{E}_{x}[f^{*}(x)]=0\)_, \(\mathbb{E}_{z\sim\mathcal{N}(0,1)}[(g^{*})^{\prime}(z)]=\Theta(1)\), \(g^{*}\) is \(1\)-Lipschitz, and \((g^{*})^{\prime\prime}\) has polynomial growth._

The first assumption can be achieved via a preprocessing step which subtracts the mean of \(f^{*}\), the second is a nondegeneracy condition, and the last two assume the target is sufficiently smooth.

We next require the eigenvalues of \(A\) to satisfy an incoherence condition:

**Assumption 6**.: _Define \(\kappa:=\left\|A\right\|_{op}\sqrt{d}\). Then \(\kappa=o(\sqrt{d}/\text{polylog}(d))\)._

Note that \(\kappa\leq\sqrt{d}\). If \(A\) has rank \(\Theta(d)\) and condition number \(\Theta(1)\), then \(\kappa=\Theta(1)\). Furthermore, when the entries of \(A\) are sampled i.i.d, \(\kappa=\tilde{\Theta}(1)\) with high probability by Wigner's semicircle law.

Finally, we make the following nondegeneracy assumption on the Gegenbauer decomposition of \(\sigma_{2}\) (defined in Appendix F). We show that \(\lambda_{2}^{2}(\sigma_{2})=O(d^{-2})\), and later argue that the following assumption is mild and indeed satisfied by standard activations such as \(\sigma_{2}=\operatorname{ReLU}\).

**Assumption 7**.: _Let \(\lambda_{2}(\sigma_{2})\) be the 2nd Gegenbauer coefficient of \(\sigma_{2}\). Then \(\lambda_{2}^{2}(\sigma_{2})=\Theta(d^{-2})\)._

We can verify Assumptions 1 to 4 hold for this setting, and thus applying Theorem 1 yields the following:

**Theorem 3**.: _Under Assumptions 5 to 7, with high probability Algorithm 1 satisfies the population loss bound_

\[\mathbb{E}_{x}\bigg{[}\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)}^{2}\bigg{]} \lesssim\tilde{O}\Bigg{(}\frac{d^{4}}{\min(n,m_{1},m_{2})}+\frac{1}{\sqrt{n}}+ \bigg{(}\frac{\kappa}{\sqrt{d}}\bigg{)}^{1/3}\Bigg{)} \tag{16}\]

We thus require sample size \(n=\tilde{\Omega}(d^{4})\) and widths \(m_{1},m_{2}=\tilde{\Omega}(d^{4})\) to obtain \(o_{d}(1)\) test loss.

Proof Sketch.The integral operator \(\mathbb{K}\) has eigenspaces corresponding to spherical harmonics of degree \(k\). In particular, [27] shows that, in \(L^{2}\),

\[\mathbb{K}f^{*}=\sum_{k\geq 0}c_{k}P_{k}f^{*}, \tag{17}\]

where \(P_{k}\) is the orthogonal projection onto the subspace of degree \(k\) spherical harmonics, and the \(c_{k}\) are constants satisfying \(c_{k}=O(d^{-k})\). Since \(f^{*}\) is an even function and \(\mathbb{E}_{x}[f^{*}(x)]=0\), truncating this expansion at \(k=2\) yields

\[\mathbb{K}f^{*}=\Theta(d^{-2})\cdot P_{2}f^{*}+O(d^{-4}), \tag{18}\]

It thus suffices to compute \(P_{2}f^{*}\). To do so, we draw a connection to the universality phenomenon in high dimensional probability. Consider two features \(x^{T}Ax\) and \(x^{T}Bx\) with \(\langle A,B\rangle=0\). We show that, when \(d\) is large, the distribution of \(x^{T}Ax\) approaches that of the standard Gaussian, while \(x^{T}Bx\) approaches a mixture of \(\chi^{2}\) and Gaussian random variables independent of \(x^{T}Ax\). As such, we show

\[\mathbb{E}_{x}\left[g^{*}(x^{T}Ax)x^{T}Bx\right] \approx\mathbb{E}_{x}\left[g^{*}(x^{T}Ax)\right]\cdot\mathbb{E}_{x} \left[x^{T}Bx\right]=0 \tag{19}\] \[\mathbb{E}_{x}\left[g^{*}(x^{T}Ax)x^{T}Ax\right] \approx\mathbb{E}_{z\sim\mathcal{N}(0,1)}\left[g^{*}(z)z\right]= \mathbb{E}_{z\sim\mathcal{N}(0,1)}\left[(g^{*})^{\prime}(z)\right]. \tag{20}\]

The second expression can be viewed as an approximate version of Stein's lemma, which was applied in Section 4.1 to compute the learned feature. Altogether, our key technical result (stated formally in Lemma 20) is that for \(f^{*}=g^{*}(x^{T}Ax)\), the projection \(P_{2}f^{*}\) satisfies

\[(P_{2}f^{*})(x)=\mathbb{E}_{z\sim\mathcal{N}(0,1)}[(g^{*})^{\prime}(z)]\cdot x ^{T}Ax+o_{d}(1) \tag{21}\]

The learned feature is thus \(\mathbb{K}f^{*}(x)=\Theta(d^{-2})\cdot(x^{T}Ax+o_{d}(1))\); plugging this into Theorem 1 yields the \(d^{4}\) sample complexity.

The full proof of Theorem 3 is deferred to Appendix E.2. In Appendix E.3 we show that when \(g^{*}\) is a degree \(p=O(1)\) polynomial, Algorithm 1 learns \(f^{*}\) in \(\tilde{O}(d^{4})\) samples with an improved error floor.

Comparison to two-layer networks.Existing guarantees for two-layer networks cannot efficiently learn functions of the form \(f^{*}(x)=g^{*}(x^{T}Ax)\) for arbitrary Lipschitz \(g^{*}\). In fact, in Section 5 we provide an explicit lower bound against two-layer networks efficiently learning a subclass of these functions. When \(g^{*}\) is a degree \(p\) polynomial, networks in the kernel regime require \(d^{2p}\gg d^{4}\) samples to learn \(f^{*}\)[27]. Improved guarantees for two-layer networks learn degree \(p^{\prime}\) polynomials in \(r^{p^{\prime}}\) samples when the target only depends on a rank \(r\ll d\) projection of the input [18]. However, \(g^{*}(x^{T}Ax)\) cannot be written in this form for some \(r\ll d\), and thus existing guarantees do not apply. We conjecture that two-layer networks require \(d^{\Omega(p)}\) samples when \(g^{*}\) is a degree \(p\) polynomial.

Altogether, the ability of three-layer networks to efficiently learn the class of functions \(f^{*}(x)=g^{*}(x^{T}Ax)\) hinges on their ability to extract the correct _nonlinear_ feature. Empirical validation of the above examples is given in Appendix A.

An Optimization-Based Depth Separation

We complement the learning guarantee in Section 4.2 with a lower bound showing that there exist functions in this class that cannot be approximated by a polynomial size two-layer network.

The class of candidate two-layer networks is as follows. For a parameter vector \(\theta=(a,W,b_{1},b_{2})\), where \(a\in\mathbb{R}^{m},W\in\mathbb{R}^{m\times d},b_{1}\in\mathbb{R}^{m},b_{2}\in \mathbb{R}\), define the associated two-layer network as

\[N_{\theta}(x):=a^{T}\sigma(Wx+b_{1})+b_{2}=\sum_{i=1}^{m}a_{i}\sigma(w_{i}^{T}x +b_{1,i})+b_{2}. \tag{22}\]

Let \(\left\|\theta\right\|_{\infty}:=\max(\left\|a\right\|_{\infty},\left\|W\right\| _{\infty},\left\|b_{1}\right\|_{\infty},\left\|b_{2}\right\|_{\infty})\) denote the maximum parameter value. We make the following assumption on \(\sigma\), which holds for all commonly-used activations.

**Assumption 8**.: _There exist constants \(C_{\sigma},\alpha_{\sigma}\) such that \(\left|\sigma(z)\right|\leq C_{\sigma}(1+\left|z\right|)^{\alpha_{\sigma}}\)._

Our main theorem establishing the separation is the following.

**Theorem 4**.: _Let \(d\) be a suffiently large even integer. Consider the target function \(f^{*}(x)=\operatorname{ReLU}(x^{T}Ax)-c_{0}\), where \(A=\frac{1}{\sqrt{d}}U\begin{pmatrix}0&I_{d/2}\\ I_{d/2}&0\end{pmatrix}U^{T}\) for some orthogonal matrix \(U\) and \(c_{0}=\mathbb{E}_{x\sim\nu}\left[\operatorname{ReLU}(x^{T}Ax)\right]\). Under Assumption 8, there exist constants \(C_{1},C_{2},C_{3},c_{3}\), depending only on \((C_{\sigma},\alpha_{\sigma})\), such that for any \(c_{3}\geq\epsilon\geq C_{3}d^{-2}\), any two layer neural network \(N_{\theta}(x)\) of width \(m\) and population \(L^{2}\) error bound \(\left\|N_{\theta}-f^{*}\right\|_{L^{2}}^{2}\leq\epsilon\) must satisfy \(\max(m,\left\|\theta\right\|_{\infty})\geq C_{1}\exp\left(C_{2}\epsilon^{-1/2} \log(d\epsilon)\right)\). However, Algorithm 1 outputs a predictor satisfying the population \(L^{2}\) loss bound_

\[\mathbb{E}_{x}\left[\left(f(x;\hat{\theta})-f^{*}(x)\right)^{2}\right]\lesssim O \Bigg{(}\frac{d^{4}}{n}+\sqrt{\frac{d}{n}}+d^{-1/6}\Bigg{)}. \tag{23}\]

_after \(T=\operatorname{poly}(d,m_{1},m_{2},n)\) timesteps._

The lower bound follows from a modification of the argument in [19] along with an explicit decomposition of the \(\operatorname{ReLU}\) function into spherical harmonics. We remark that the separation applies for any link function \(g^{*}\) whose Gegenbauer coefficients decay sufficiently slow. The upper bound follows from an application of Theorem 1 to a smoothed version of \(\operatorname{ReLU}\), since \(\operatorname{ReLU}\) is not in \(\mathcal{W}^{2,\infty}([-1,1])\). The full proof of the theorem is given in Appendix G.

Remarks.In order for a two-layer network to achieve test loss matching the \(d^{-1/6}\) error floor, either the width or maximum weight norm of the network must be \(\exp\bigl{(}\Omega(d^{\delta})\bigr{)}\) for some constant \(\delta\); this lower bound is superpolynomial in \(d\). As a consequence, gradient descent on a poly-width two-layer neural network with stable step size must run for superpolynomially many iterations in order for some weight to grow this large and thus converge to a solution with low test error. Therefore \(f^{*}\) is not learnable via gradient descent in polynomial time. This reduction from a weight norm lower bound to a runtime lower bound is made precise in [45].

We next describe a specific example of such an \(f^{*}\). Let \(S\) be a \(d/2\)-dimensional subspace of \(\mathbb{R}^{d}\), and let \(A=d^{-\frac{1}{2}}P_{S}-d^{-\frac{1}{2}}P_{S}^{\perp}\), where \(P_{S},P_{S}^{\perp}\) are projections onto the subspace \(S\) and and its orthogonal complement respectively. Then, \(f^{*}(x)=\operatorname{ReLU}\Bigl{(}2d^{-\frac{1}{2}}\|P_{S}x\|^{2}-d^{\frac{1 }{2}}\Bigr{)}\). [44] established an optimization-based separation for the target \(\operatorname{ReLU}(1-\|x\|)\), under a different distribution \(\nu\). However, their analysis relies heavily on the rotational symmetry of the target, and they posed the question of learning \(\operatorname{ReLU}(1-\|P_{S}x\|)\) for some subspace \(S\). Our separation applies to a similar target function, and crucially does not rely on this rotational invariance.

## 6 Discussion

In this work we showed that three-layer networks can both learn nonlinear features and leverage these features to obtain a provable sample complexity improvement over two-layer networks. There are a number of interesting directions for future work. First, can our framework be used to learn hierarchical functions of a larger class of features beyond quadratic functions? Next, since Theorem 1 is general purpose and makes minimal distributional assumptions, it would be interesting to understand if it can be applied to standard empirical datasets such as CIFAR-10, and what the learned feature \(\mathbb{K}f^{*}\) and hierarchical learning correspond to in this setting. Finally, our analysis studies the nonlinear feature learning that arises from a neural representation \(\sigma_{2}(Vx)\) where \(V\) is fixed at initialization. This alone was enough to establish a separation between two and three-layer networks. A fascinating question is to understand the additional features that can be learned when both \(V\) and \(W\) are jointly trained. Such an analysis, however, is incredibly challenging in feature-learning regimes.

## Acknowledgements

EN acknowledges support from a National Defense Science & Engineering Graduate Fellowship. AD acknowledges support from a NSF Graduate Research Fellowship. EN, AD, and JDL acknowledge support of the ARO under MURI Award W911NF-11-1-0304, the Sloan Research Fellowship, NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, ONR Young Investigator Award, and NSF CAREER Award 2144994. The authors would like to thank Itay Safran for helpful discussions.

## References

* [1] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In _Proceedings of Thirty Fifth Conference on Learning Theory_, pages 4782-4887, 2022.
* [2] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2552-2623. PMLR, 2023.
* [3] Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? _Advances in Neural Information Processing Systems_, 32, 2019.
* [4] Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep learning. _arXiv preprint arXiv:2001.04413_, 2020.
* [5] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. _Advances in neural information processing systems_, 32, 2019.
* [6] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [7] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning (ICML)_, 2019.
* [8] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. _Advances in Neural Information Processing Systems_, 35:37932-37946, 2022.
* [9] Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks. In _International Conference on Learning Representations_, 2020.
* [10] Boaz Barak, Benjamin L. Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [11] William Beckner. Sobolev inequalities, the poisson semigroup, and analysis on the sphere sn. _Proceedings of the National Academy of Sciences of the United States of America_, 89 11:4816-9, 1992.

* [12] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent on non-convex losses from high-dimensional inference. _The Journal of Machine Learning Research_, 22(1):4788-4838, 2021.
* [13] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. _Advances in Neural Information Processing Systems_, 35:9768-9783, 2022.
* [14] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL [http://github.com/google/jax](http://github.com/google/jax).
* [15] Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural networks. In _Advances in Neural Information Processing Systems_, 2019.
* [16] Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard Socher. Towards understanding hierarchical learning: Benefits of neural representations. _Advances in Neural Information Processing Systems_, 33:22134-22145, 2020.
* [17] Lenac Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [18] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In _Conference on Learning Theory_, pages 5413-5452. PMLR, 2022.
* [19] Amit Daniely. Depth separation for neural networks. In _Conference on Learning Theory_, pages 690-696. PMLR, 2017.
* [20] Amit Daniely and Eran Malach. Learning parities with neural networks. _Advances in Neural Information Processing Systems_, 33:20356-20365, 2020.
* [21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).
* [22] Simon S. Du and Jason D. Lee. On the power of over-parametrization in neural networks with quadratic activation. In _Proceedings of the 35th International Conference on Machine Learning_, Proceedings of Machine Learning Research, pages 1329-1338, 2018.
* [23] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks. In _International Conference on Learning Representations (ICLR)_, 2019.
* [24] Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In _Conference on learning theory_, pages 907-940. PMLR, 2016.
* [25] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy training of two-layers neural network. In _Advances in Neural Information Processing Systems_, 2019.
* [26] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural networks outperform kernel methods? In _Advances in Neural Information Processing Systems_, 2020.
* [27] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers neural networks in high dimension. _The Annals of Statistics_, 49:1029-1054, 2021.
* [28] Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. Exact convergence rates of the neural tangent kernel in the large depth limit. _arXiv preprint arXiv:1905.13654_, 2019.

* He et al. [2015] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 770-778, 2015.
* Huang et al. [2020] Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks generalize better than deep feedforward networks? -- a neural tangent kernel perspective. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 2698-2709. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/1c336b80f82bcc2cd2499b4c57261d-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1c336b80f82bcc2cd2499b4c57261d-Paper.pdf).
* Jacot et al. [2018] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* Lee et al. [2020] Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In _Advances in Neural Information Processing Systems_, 2020.
* Li et al. [2020] Yuanzhi Li, Tengyu Ma, and Hongyang R. Zhang. Learning over-parametrized two-layer neural networks beyond ntk. In _Proceedings of Thirty Third Conference on Learning Theory_, Proceedings of Machine Learning Research, pages 2613-2682, 2020.
* Malach and Shalev-Shwartz [2019] Eran Malach and Shai Shalev-Shwartz. Is deeper better only when shallow is good? In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/606555cf42a6719782a952aa33cfa2cb-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/606555cf42a6719782a952aa33cfa2cb-Paper.pdf).
* Malach et al. [2021] Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro. Quantifying the benefit of using differentiable learning over tangent kernels. In _Proceedings of the 38th International Conference on Machine Learning_, Proceedings of Machine Learning Research, pages 7379-7389, 2021.
* Malach et al. [2021] Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. The connection between approximation, depth separation and learnability in neural networks. In Mikhail Belkin and Samory Kpotufe, editors, _Proceedings of Thirty Fourth Conference on Learning Theory_, volume 134 of _Proceedings of Machine Learning Research_, pages 3265-3295. PMLR, 15-19 Aug 2021. URL [https://proceedings.mlr.press/v134/malach21a.html](https://proceedings.mlr.press/v134/malach21a.html).
* Mei et al. [2018] Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for nonconvex losses. _The Annals of Statistics_, 46(6A):2747-2774, 2018.
* Mei et al. [2022] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random features and kernel methods: hypercontractivity and kernel matrix concentration. _Applied and Computational Harmonic Analysis, Special Issue on Harmonic Analysis and Machine Learning_, 59:3-84, 2022.
* Montanari and Zhong [2020] Andrea Montanari and Yiqiao Zhong. The interpolation phase transition in neural networks: Memorization and generalization under lazy training. _arXiv preprint arXiv:2007.12826_, 2020.
* Nichani et al. [2020] Eshaan Nichani, Adityanarayanan Radhakrishnan, and Caroline Uhler. Increasing depth leads to u-shaped test risk in over-parameterized convolutional networks. _arXiv preprint arXiv:2010.09610_, 2020.
* Nichani et al. [2022] Eshaan Nichani, Yu Bai, and Jason D Lee. Identifying good directions to escape the ntk regime and efficiently learn low-degree plus sparse polynomials. _Advances in Neural Information Processing Systems_, 35:14568-14581, 2022.
* Poole et al. [2016] Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 29. Curran Associates, Inc., 2016. URL [https://proceedings.neurips.cc/paper/2016/file/148510031349642de5ca0c544f31b2ef-Paper.pdf](https://proceedings.neurips.cc/paper/2016/file/148510031349642de5ca0c544f31b2ef-Paper.pdf).

* Radhakrishnan et al. [2022] Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin. Feature learning in neural networks and kernel machines that recursively learn features. _arXiv preprint arXiv:2212.13881_, 2022.
* Ren et al. [2023] Yunwei Ren, Mo Zhou, and Rong Ge. Depth separation with multilayer mean-field networks. _arXiv preprint arXiv:2304.01063_, 2023.
* Safran and Lee [2022] Itay Safran and Jason Lee. Optimization-based separations for neural networks. In _Conference on Learning Theory_, pages 3-64. PMLR, 2022.
* Safran and Shamir [2017] Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural networks. In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning_, volume 70 of _Proceedings of Machine Learning Research_, pages 2979-2987. PMLR, 06-11 Aug 2017. URL [https://proceedings.mlr.press/v70/safran17a.html](https://proceedings.mlr.press/v70/safran17a.html).
* Safran et al. [2019] Itay Safran, Ronen Eldan, and Ohad Shamir. Depth separations in neural networks: What is actually being separated? In Alina Beygelzimer and Daniel Hsu, editors, _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 2664-2666. PMLR, 25-28 Jun 2019. URL [https://proceedings.mlr.press/v99/safran19a.html](https://proceedings.mlr.press/v99/safran19a.html).
* Schoenholz et al. [2017] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. In _International Conference on Learning Representations_, 2017. URL [https://openreview.net/forum?id=H1W1UN9gg](https://openreview.net/forum?id=H1W1UN9gg).
* Soltanolkotabi [2017] Mahdi Soltanolkotabi. Learning relus via gradient descent. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* Soltanolkotabi et al. [2018] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. _IEEE Transactions on Information Theory_, 65:742-769, 2018.
* Telgarsky [2016] Matus Telgarsky. Benefits of depth in neural networks. In _Conference on learning theory_, pages 1517-1539. PMLR, 2016.
* van Handel [2016] Ramon van Handel. Probability in high dimension, 2016. URL [http://web.math.princeton.edu/~rvan/APC550.pdf](http://web.math.princeton.edu/~rvan/APC550.pdf).
* Wei et al. [2018] Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and optimization of neural nets vs their induced kernel. _arXiv preprint arXiv:1810.05369_, 2018.
* Woodworth et al. [2020] Blake Woodworth, Suriya Gunasekar, Jason D. Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In _Conference on Learning Theory_, Proceedings of Machine Learning Research, pages 3635-3673, 2020.
* Xiao et al. [2018] Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 5393-5402. PMLR, 10-15 Jul 2018. URL [https://proceedings.mlr.press/v80/xiao18a.html](https://proceedings.mlr.press/v80/xiao18a.html).
* Xiao et al. [2020] Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz. Disentangling trainability and generalization in deep neural networks. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 10462-10472. PMLR, 13-18 Jul 2020. URL [https://proceedings.mlr.press/v119/xiao20b.html](https://proceedings.mlr.press/v119/xiao20b.html).
* Yang and Hu [2021] Greg Yang and Edward J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In _Proceedings of the 38th International Conference on Machine Learning_, 2021.
* Yehudai and Shamir [2019] Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding neural networks. In _Advances in Neural Information Processing Systems_, 2019.

## Limitations

One limitation of our work is that the innermost layer weights \(V\) are fixed throughout training, and our training procedure is layerwise. We remark that this is already enough to establish an algorithmic separation between two and three-layer networks (Section 5), and that prior works on two-layer networks also rely on layerwise training procedures. It is a very interesting direction of future work to understand if training \(V\) can lead to learning a larger class of functions. Another limitation is that our examples in Section 4 rely on the data distribution \(\nu\) being either the standard Gaussian or uniform on the sphere. This allows us to carefully characterize the integral operator \(\mathbb{K}\), and we remark that other works on two-layer networks make similar distributional assumptions (such as uniform on the hypercube). However, it is a very important direction of future work to extend these lines of work to more general data distributions.

## Appendix A Empirical Validation

We empirically verify our conclusions in the single index setting of Section 4.1 and the quadratic feature setting of Section 4.2:

Single Index SettingWe learn the target function \(g^{\star}(w^{\star}\cdot x)\) using Algorithm 1 where \(w^{\star}\in S^{d-1}\) is drawn randomly and \(g^{\star}(x)=\mathrm{sigmoid}(x)=\frac{1}{1+e^{-x}}\), which satisfies the condition \(\mathbb{E}_{z\sim\mathcal{N}(0,1)}[g^{\prime}(z)]\neq 0\). As in Theorem 2, we choose the initial activation \(\sigma_{2}(z)=z\). We optimize the hyperparameters \(\eta_{1},\lambda\) using grid search over a holdout validation set of size \(2^{15}\) and report the final error over a test set of size \(2^{15}\).

Quadratic Feature SettingWe learn the target function \(g^{\star}(x^{T}Ax)\) using Algorithm 1 where \(g^{\star}(x)=\mathrm{ReLU}(x)\). We ran our experiments with two different choices of \(A\):

* \(A\) is symmetric with random entries, i.e. \(A_{ij}\sim N(0,1)\) and \(A_{ji}=A_{ij}\) for \(i\leq j\).
* \(A\) is a random projection, i.e. \(A=\Pi_{S}\) where \(S\) is a random \(d/2\) dimensional subspace.

Figure 1: We ran Algorithm 1 on both the single index and quadratic feature settings described in Section 4. Each trial was run with 5 random seeds. The solid lines represent the medians and the shaded areas represent the min and max values. For every trial we recorded both the test loss on a test set of size \(2^{15}\) and the linear correlation between the learned feature map \(\phi(x)\) and the true intermediate feature \(h^{\star}(x)\) where \(h^{\star}(x)=x\cdot\beta\) for the single index setting and \(h^{\star}(x)=x^{T}Ax\) for the quadratic feature setting. Our results show that the test loss goes to \(0\) as the linear correlation between the learned feature map \(\phi\) and the true intermediate feature \(h^{\star}\) approaches \(1\).

Both choices of \(A\) were then normalized so that \(\operatorname{tr}A=0\) and \(\|A\|_{F}=1\) by subtracting the trace and dividing by the Frobenius norm. We chose initial activation \(\sigma_{2}(z)=\operatorname{ReLU}(z)\). We note that in both examples, \(\kappa=\Theta(1)\). As above, we optimize the hyperparameters \(\eta_{1},\lambda\) using grid search over a holdout validation set of size \(2^{15}\) and report the final error over a test set of size \(2^{15}\).

To focus on the sample complexity and avoid width-related bottlenecks, we directly simulate the infinite width limit (\(m_{2}\to\infty\)) of Algorithm 1 by computing the kernel \(\mathbb{K}\) in closed form. Finally, we run each trial with 5 random seeds and report the min, median, and max values in Figure 1.

Comparison Between Two and Three-Layer NetworksWe also show that the sample complexity separation between two and three layer networks persists in standard training settings. In Figure 2, we train both a two and three-layer neural network on the target \(f^{*}(x)=\operatorname{ReLU}(x^{T}Ax)\), where \(A\) is symmetric with random entries, as described above. Both networks are initialized using the \(\mu\)P parameterization [57] and are trained using SGD with momentum on all layers simultaneously. The input dimension is \(d=32\), and the widths are chosen to be \(256\) for the three-layer network and \(2048\) for the two-layer network, so that the parameter counts are approximately equal. Figure 2 plots the average test loss over 5 random seeds against sample size; here, we see that the three-layer network has a better sample complexity than the two-layer network.

Experimental Details.Our experiments were written in JAX [14], and were run on a single NVIDIA RTX A6000 GPU.

## Appendix B Notation

### Asymptotic Notation

Throughout the proof we will let \(C\) be a fixed but sufficiently large constant.

**Definition 4** (high probability events).: _Let \(\iota=C\log(dnm_{1}m_{2})\). We say that an event happens with high probability if it happens with probability at least \(1-\operatorname{poly}(d,n,m_{1},m_{2})e^{-\iota}\)._

**Example 5**.: _If \(z\sim N(0,1)\) then \(|z|\leq\sqrt{2\iota}\) with high probability._

Note that high probability events are closed under union bounds over sets of size \(\operatorname{poly}(d,n,m_{1},m_{2})\). We will also assume throughout that \(\iota\leq C^{-1}d\).

### Tensor Notation

For a \(k\)-tensor \(T\in(\mathbb{R}^{d})^{\otimes k}\), let \(\operatorname{Sym}\) be the symmetrization of \(T\) across all \(k\) axes, i.e

\[\operatorname{Sym}(T)_{i_{1},\cdots,i_{k}}=\frac{1}{k!}\sum_{\sigma\in S_{k}}T _{i_{\sigma(1)},\cdots,i_{\sigma(k)}}\]

Next, given tensors \(A\in(\mathbb{R}^{d})^{\otimes a},B\in(\mathbb{R}^{d})^{\otimes b}\), we let \(A\otimes B\in(\mathbb{R}^{d})^{\otimes a+b}\) be their tensor product.

Figure 2: Three-layer neural networks learn the target \(f^{*}(x)=\operatorname{ReLU}(x^{T}Ax)\) with better sample complexity than two-layer networks.

[MISSING_PAGE_FAIL:16]

Proof.: First consider \(v(a,b)=\frac{1_{b\in[0,1]}}{\mu(b)}2f^{\prime\prime}(-ab)\). Then when \(x\geq 0\) we have by integration by parts:

\[\mathbb{E}_{ab}[v(a,b)\sigma(ax+b)]\] \[=\int_{0}^{1}\left[f^{\prime\prime}(-b)\sigma(x+b)+f^{\prime \prime}(b)\sigma(-x+b)\right]db\] \[=f^{\prime}(0)x-f^{\prime}(-1)(x+1)+f^{\prime}(1)(-x+1)+\int_{0}^ {1}f^{\prime}(-b)db-\int_{x}^{1}f^{\prime}(b)db\] \[=f(x)+c_{1}+c_{2}x\]

where \(c_{1}=f(0)-f(1)-f(-1)+f^{\prime}(1)-f^{\prime}(-1)\) and \(c_{2}=f^{\prime}(0)-f^{\prime}(1)-f^{\prime}(-1)\). In addition when \(x<0\),

\[\mathbb{E}_{ab}[v(a,b)\sigma(ax+b)]\] \[=\int_{0}^{1}\left[f^{\prime\prime}(-b)\sigma(x+b)+f^{\prime \prime}(b)\sigma(-x+b)\right]db\] \[=f^{\prime}(0)x-f^{\prime}(-1)(x+1)+f^{\prime}(1)(-x+1)+\int_{-x }^{1}f^{\prime}(-b)db-\int_{0}^{1}f^{\prime}(b)db\] \[=f(x)+c_{1}+c_{2}x\]

so this equality is true for all \(x\). By Lemmas 1 and 2 we can subtract out the constant and linear terms so there exists \(v(a,b)\) such that

\[\mathbb{E}_{a,b}[v(a,b)\sigma(ax+b)]=f(x).\]

In addition, using that \(\mu(b)\gtrsim 1\) for \(b\in[0,1]\) gives that this \(v(a,b)\) satisfies

\[\sup_{a,b}|v(a,b)|\lesssim|c_{1}|+|c_{2}|+\max_{x\in[-1,1]}|f^{\prime\prime}(x )|\lesssim\sup_{\begin{subarray}{c}x\in[-1,1]\\ k\in\{0,1,2\}\end{subarray}}|f^{(k)}(x)|.\]

## Appendix D Proofs for Section 3

The following is a formal restatement of Theorem 1.

**Theorem 6**.: _Select \(q\in\mathcal{W}^{2,\infty}([-1,1])\). Let \(\eta_{1}=\frac{m_{1}}{m_{2}}\overline{\eta}\), and \(n\gtrsim\|\mathbb{K}f^{*}\|_{L^{2}}^{-2}t^{2\ell+2\alpha_{\sigma}+1},m_{1} \gtrsim\|\mathbb{K}f^{*}\|_{L^{2}}^{-2}\iota\), \(m_{2}\gtrsim\|\mathbb{K}f^{*}\|_{L^{2}}^{-2}\iota^{2\alpha_{\sigma}+1}\). There exists a choice of \(\overline{\eta}=\Theta(\iota^{-\chi}\|Kf^{*}\|_{L^{2}}^{-2})\), \(T=\text{poly}(d,n,m_{1},m_{2},\|q\|_{2,\infty})\), and \(\lambda,\eta_{2}\) such that with high probability the output \(\hat{\theta}\) of Algorithm 1 satisfies the population \(L^{2}\) loss bound_

\[\mathbb{E}_{x}\bigg{[}\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)}^{ 2}\bigg{]} \lesssim\|q\|_{1,\infty}^{2}\|\mathbb{K}f^{*}\|_{L^{2}}^{-2}\cdot \bigg{(}\frac{\iota^{2\ell+2\alpha_{\sigma}+2\chi+1}}{n}+\frac{\iota^{2\alpha _{\sigma}+2\chi+1}}{m_{2}}+\frac{\iota}{m_{1}}\bigg{)}\] \[\quad+\|q\circ(\overline{\eta}\cdot\mathbb{K}f^{*})-f^{*}\|_{L^{ 2}}^{2}+\frac{\|q\|_{2,\infty}^{2}\iota}{m_{1}}+\sqrt{\frac{\|q\|_{2,\infty}^{ 4}\iota+\iota^{4\ell+1}}{n}}\]

Proof of Theorem 6.: The gradient with respect to \(w_{j}\) is

\[\nabla_{w_{j}}L_{1}(\theta^{(0)}) =\frac{1}{n}\sum_{i}^{n}\Big{(}f(x_{i};\theta^{(0)})-f^{*}(x_{i}) \Big{)}\nabla_{w_{j}}f(x_{i};\theta^{(0)})\] \[=\sigma_{1}^{\prime}(b_{j}^{(0)})\cdot\frac{1}{n}\sum_{i=1}^{n} \Big{(}f(x_{i};\theta^{(0)})-f^{*}(x_{i})\Big{)}\frac{a_{j}^{(0)}}{m_{1}}h^{(0) }(x_{i})\] \[=\mathbf{1}_{b_{j}^{(0)}>0}\cdot\frac{1}{n}\sum_{i=1}^{n}\Big{(} f(x_{i};\theta^{(0)})-f^{*}(x_{i})\Big{)}\frac{a_{j}^{(0)}}{m_{1}}h^{(0)}(x_{i})\]Therefore

\[w_{j}^{(1)} =-\eta_{1}\nabla_{w_{j}}L_{1}(\theta^{(0)})\] \[=\mathbf{1}_{b_{j}^{(0)}>0}\cdot\frac{\overline{\eta}}{m_{2}}a_{j} ^{(0)}\frac{1}{n}\sum_{i=1}^{n}\Big{(}f^{*}(x_{i})-f(x_{i};\theta^{(0)})\Big{)} h^{(0)}(x_{i}).\]

One then has

\[f (x;(a,W^{(1)},b^{(1)},V^{(0)}))\] \[=\sum_{j=1}^{m_{1}}\frac{a_{j}}{m_{1}}\sigma_{1}\bigg{(}\mathbf{1 }_{b_{j}^{(0)}>0}\cdot a_{j}^{(0)}\cdot\frac{\overline{\eta}}{m_{2}}\frac{1}{n }\sum_{i=1}^{n}\Big{(}f^{*}(x_{i})-f(x_{i};\theta^{(0)})\Big{)}\langle h^{(0)} (x_{i}),h^{(0)}(x)\rangle+b_{j}\bigg{)}\] \[=\sum_{j=1}^{m_{1}}\frac{a_{j}}{m_{1}}\sigma_{1}(a_{j}^{(0)}\cdot \overline{\eta}\phi(x)+b_{j})\cdot\mathbf{1}_{b_{j}^{(0)}>0},\]

where \(\phi(x):=\frac{1}{m_{2}n}\sum_{i=1}^{n}\big{(}f^{*}(x_{i})-(x_{i};\theta^{(0)} )\big{)}\langle h^{(0)}(x_{i}),h^{(0)}(x)\rangle\).

The second stage of Algorithm 1 is equivalent to random feature regression. The next lemma shows that there exists \(a^{*}\) with small norm that acheives low empirical loss on the dataset \(\mathcal{D}_{2}\).

**Lemma 4**.: _There exists \(a^{*}\) with \(\left\|a^{*}\right\|_{2}\lesssim\left\|q\right\|_{2,\infty}\sqrt{m_{1}}\) such that \(\theta^{*}=\big{(}a^{*},W^{(1)},b^{(0)},V^{(0)}\big{)}\) satisfies_

\[L_{2}(\theta^{*}) \lesssim\left\|q\right\|_{1,\infty}^{2}\left\|\mathbb{K}f^{*} \right\|_{L^{2}}^{-2}\cdot\bigg{(}\frac{t^{2\ell+2\alpha_{\sigma}+2\chi+1}}{n} +\frac{\iota^{2\alpha_{\sigma}+2\chi+1}}{m_{2}}+\frac{\iota}{m_{1}}\bigg{)}\] \[\quad+\frac{\iota^{2\ell+1}}{n}+\frac{\left\|q\right\|_{2,\infty }^{2}\iota}{m_{1}}+\left\|q\circ(\overline{\eta}\cdot\mathbb{K}f^{*})-f^{*} \right\|_{L^{2}}^{2}\]

The proof of Lemma 4 is deferred to Appendix D.1. We first show that \(\phi\) is approximately proportional to \(\mathbb{K}f^{*}\), and then invoke the ReLU random feature expressivity results from Appendix C.

Next, set

\[\lambda= \|a^{*}\|_{2}^{-2}\bigg{(}\|q\|_{1,\infty}^{2}\|\mathbb{K}f^{*} \|_{L^{2}}^{-2}\cdot\bigg{(}\frac{t^{2\ell+2\alpha_{\sigma}+2\chi+1}}{n}+\frac {\iota^{2\alpha_{\sigma}+2\chi+1}}{m_{2}}+\frac{\iota}{m_{1}}\bigg{)}\] \[+\frac{\iota^{2\ell+1}}{n}+\frac{\left\|q\right\|_{2,\infty}^{2} \iota}{m_{1}}+\left\|q\circ(\overline{\eta}(\mathbb{K}f^{*}))-f^{*}\right\|_ {L^{2}}^{2}\bigg{)},\]

so that \(L_{2}(\theta^{*})\lesssim\left\|a^{*}\right\|_{2}^{2}\lambda\).

Define the regularized loss to be \(\hat{L}(a):=L_{2}\big{(}(a,W^{(1)},b^{(0)},V^{(0)})\big{)}+\frac{\lambda}{2} \|a\|_{2}^{2}\). Let \(a^{(\infty)}=\operatorname*{arg\,min}_{a}\hat{L}(a)\), and \(a^{(t)}\) be the predictor from running gradient descent for \(t\) steps initialized at \(a^{(0)}\). We first note that

\[\hat{L}(a^{(\infty)})\leq\hat{L}(a^{*})\lesssim\left\|a^{*}\right\|_{2}^{2}\lambda.\]

Next, we remark that \(\hat{L}(a)\) is \(\lambda\)-strongly convex. Additionally, we can write \(f(x;(a,W^{(1)},b^{(0)},V^{(0)}))=a^{T}\psi(x)\) where \(\psi(x)=\operatorname*{Vec}\Big{(}\frac{1}{m_{1}}\sigma_{1}(a_{j}^{(0)} \overline{\eta}\phi(x)+b_{j}^{(0)})\cdot\mathbf{1}_{b_{j}^{(0)}>0}\Big{)}\). In Appendix D.2 we show \(\sup_{x\in\mathcal{D}_{2}}\left\|\psi(x)\right\|\lesssim\frac{1}{m_{1}}\). Therefore

\[\lambda_{max}\Big{(}\nabla_{a}^{2}\hat{L}\Big{)}\leq\frac{2}{n}\sum_{x\in \mathcal{D}_{2}}\left\|\psi(x)\right\|^{2}\lesssim\frac{1}{m_{1}},\]

so \(\hat{L}\) is \(O(\frac{1}{m_{1}})+\lambda\) smooth. Choosing a learning rate \(\eta_{2}=\Theta(m_{1})\), after \(T=\tilde{O}\Big{(}\frac{1}{\lambda m_{1}}\Big{)}=\operatorname*{poly}(d,n,m_{1},m_{2},\left\|q\right\|_{2,\infty})\) steps we reach an iterate \(\hat{a}=a^{(T)}\), \(\hat{\theta}=(\hat{a},W^{(1)},b^{(0)},V^{(0)}))\) satisfying

\[L_{2}(\hat{\theta})\lesssim L_{2}(\theta^{*})\quad\text{and}\quad\left\|\hat{a }\right\|_{2}\lesssim\left\|a^{*}\right\|_{2}.\]For \(\tau>0\), define the truncated loss \(\ell_{\tau}\) by \(\ell_{\tau}(z)=\min(z^{2},\tau^{2})\). We have that \(\ell_{\tau}(z)\leq z^{2}\), and thus

\[\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\ell_{\tau}\Big{(}f(x;\hat{ \theta})-f^{*}(x)\Big{)}\leq L_{2}(\hat{\theta}).\]

Consider the function class

\[\mathcal{F}(B_{a}):=\{f(\cdot;(a,W^{(1)},b^{(0)},V^{(0)}):\left\|a \right\|_{2}\leq B_{a})\}\]

The following lemma bounds the empirical Rademacher complexity of this function class

**Lemma 5**.: _Given a dataset \(\mathcal{D}\), recall that the empirical Rademacher complexity of \(\mathcal{F}\) is defined as_

\[\mathcal{R}_{\mathcal{D}}(\mathcal{F}):=\mathbb{E}_{\sigma\in\{ \pm 1\}^{n}}\Bigg{[}\sup_{f\in\mathcal{F}}\frac{1}{n}\sum_{i=1}^{n}\sigma_{ i}f(x_{i})\Bigg{]}.\]

_Then with high probability_

\[\mathcal{R}_{\mathcal{D}_{2}}(\mathcal{F}(B_{a}))\lesssim\sqrt{ \frac{B_{a}^{2}}{nm_{1}}}.\]

Since \(\ell_{\tau}\) loss is \(2\tau\)-Lipschitz, the above lemma with \(B_{a}=O(\left\|a^{*}\right\|_{2})=O\Big{(}\left\|q\right\|_{2,\infty}\sqrt{m_ {1}}\Big{)}\) along with the standard empirical Rademacher complexity bound yields

\[\mathbb{E}_{x}\ell_{\tau}\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)} \lesssim\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\ell_{\tau}\Big{(}f(x; \hat{\theta})-f^{*}(x)\Big{)}+\tau\cdot\mathcal{R}_{\mathcal{D}_{2}}(\mathcal{ F})+\tau^{2}\sqrt{\frac{\iota}{n}}\] \[\lesssim\left\|q\right\|_{1,\infty}^{2}\left\|\mathbb{K}f^{*} \right\|_{L^{2}}^{-2}\cdot\left(\frac{\iota^{2\ell+2\alpha_{\sigma}+2\chi+1}}{n }+\frac{\iota^{2\alpha_{\sigma}+2\chi+1}}{m_{2}}+\frac{\iota}{m_{1}}\right)\] \[\quad+\left\|q\circ(\overline{\eta}\cdot\mathbb{K}f^{*})-f^{*} \right\|_{L^{2}}^{2}+\frac{\iota^{2\ell+1}}{n}+\frac{\left\|q\right\|_{2,\infty }^{2}\iota}{m_{1}}\] \[\quad+\tau\sqrt{\frac{\left\|q\right\|_{2,\infty}^{2}}{n}+\tau^{ 2}\sqrt{\frac{\iota}{n}}}.\]

Finally, we relate the \(\ell_{\tau}\) population loss to the \(\ell^{2}\) population loss.

**Lemma 6**.: _Let \(\tau=\Theta(\max(\iota^{\ell},\left\|q\right\|_{2,\infty}))\). Then with high probability over \(\hat{\theta}\),_

\[\mathbb{E}_{x}\bigg{[}\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)}^{2 }\bigg{]}\leq\mathbb{E}_{x}\ell_{\tau}\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)} +\frac{\left\|q\right\|_{2,\infty}^{2}}{m_{1}}.\]

Plugging in \(\tau\) and applying Lemma 6 yields

\[\mathbb{E}_{x}\bigg{[}\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)}^{2 }\bigg{]} \lesssim\left\|q\right\|_{1,\infty}^{2}\left\|\mathbb{K}f^{*} \right\|_{L^{2}}^{-2}\cdot\left(\frac{\iota^{2\ell+2\alpha_{\sigma}+2\chi+1}}{ n}+\frac{\iota^{2\alpha_{\sigma}+2\chi+1}}{m_{2}}+\frac{\iota}{m_{1}}\right)\] \[\quad+\left\|q\circ(\overline{\eta}\cdot\mathbb{K}f^{*})-f^{*} \right\|_{L^{2}}^{2}+\frac{\left\|q\right\|_{2,\infty}^{2}\iota}{m_{1}}+\sqrt {\frac{\left\|q\right\|_{2,\infty}^{4}t+\iota^{4\ell+1}}{n}}\]

as desired. 

### Proof of Lemma 4

We require three auxiliary lemmas, all of whose proofs are deferred to Appendix D.4. The first lemma bounds the error between the population learned feature \(\mathbb{K}f^{*}\) and the finite sample learned feature \(\phi\) over the dataset \(\mathcal{D}_{2}\).

**Lemma 7**.: _With high probability_

\[\sup_{x\in\mathcal{D}_{2}}|\phi(x)-(\mathbb{K}f^{*})(x)|\lesssim\sqrt{\frac{t^{2 \ell+2\alpha_{\sigma}+1}}{n}}+\sqrt{\frac{t^{2\alpha_{\sigma}+1}}{m_{2}}}+\sqrt {\frac{t}{m_{1}}}.\]

The second lemma shows that with appropriate choice of \(\eta\), the quantity \(\eta\phi(x)\) is small.

**Lemma 8**.: _Let \(n\gtrsim\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-2\ell^{2}\ell+2\alpha_{\sigma }+1},m_{1}\gtrsim\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-2}\iota\), and \(m_{2}\gtrsim\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-2}\iota^{2\alpha_{\sigma }+1}\). There exists \(\overline{\eta}=\tilde{\Theta}(\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-2})\) such that with high probability, \(\sup_{x\in\mathcal{D}_{2}}|\overline{\eta}\phi(x)|\leq 1\) and \(\sup_{x\in\mathcal{D}_{2}}|\overline{\eta}\cdot\mathbb{K}f^{*}(x)|\leq 1\)_

The third lemma expresses the compositional function \(q\circ\overline{\eta}\phi\) via an infinite width network.

**Lemma 9**.: _Assume that \(n,m_{1},m_{2}=\tilde{\Omega}(\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-2})\). There exists \(v:\{\pm 1\}\times\mathbb{R}\to\mathbb{R}\) such that \(\sup_{a,b}|v(a,b)|\leq\left\|q\right\|_{2,\infty}\) and, with high probability over \(\mathcal{D}_{2}\), the infinite width network_

\[f_{v}^{\infty}(x):=\mathbb{E}_{a,b}[v(a,b)\sigma_{1}(a\overline{\eta}\phi(x)+b) \mathbf{1}_{b>0}]\]

_satisfies_

\[f_{v}^{\infty}(x)=q(\overline{\eta}\phi(x))\]

_for all \(x\in\mathcal{D}_{2}\)._

Proof of Lemma 4.: Let \(v\) be the infinite width construction defined in Lemma 9. Define \(a^{*}\in\mathbb{R}^{m_{1}}\) to be the vector with \(a_{j}^{*}=v(a_{j}^{(0)},b_{j}^{(1)})\). We can decompose

\[L_{2}(\theta^{*}) =\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\left(f(x;\theta^{*})-f^{*}( x)\right)^{2}\] \[\lesssim\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\left(f(x;\theta^{*} )-f_{v}^{\infty}(x)\right)^{2}+\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\left(f_{ v}^{\infty}(x)-q(\overline{\eta}\phi(x))\right)^{2}\] \[+\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\left(q(\overline{\eta}\phi (x))-q(\overline{\eta}(\mathbb{K}f^{*}(x)))\right)^{2}+\frac{1}{n}\sum_{x\in \mathcal{D}_{2}}\left(q(\overline{\eta}(\mathbb{K}f^{*}(x))-f^{*}\right)^{2}\]

Take \(m_{1}\gtrsim\iota\). The first term is the error between the infinite width network \(f_{v}^{\infty}(x)\) and the finite width network \(f(x;\theta^{*})\). This error can be controlled via standard concentration arguments: by Corollary 2 we have that with high probability \(\left\|a^{*}\right\|\lesssim\frac{\left\|q\right\|_{2,\infty}}{\sqrt{m_{1}}}\), and by Lemma 17 we have

\[\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\left(f(x;\theta^{*})-f_{v}^{\infty}(x) \right)^{2}\lesssim\frac{\left\|q\right\|_{2,\infty}^{2}\iota}{m_{1}}.\]

Next, by Lemma 9 we get that the second term is zero with high probability. We next turn to the third term. Since \(q\) is \(\left\|q\right\|_{1,\infty}\)-Lipschitz on \([-1,1]\), and \(\sup_{x\in\mathcal{D}_{2}}|\overline{\eta}\phi(x)|\leq 1,\sup_{x\in\mathcal{D}_{2}} |(\overline{\eta}\cdot\mathbb{K}f^{*})(x)|\leq 1\) by Lemma 8, we can apply Lemma 7 to get

\[\sup_{x\in\mathcal{D}_{2}}|q(\overline{\eta}\phi(x))-q(\overline{ \eta}(\mathbb{K}f^{*})(x))| \leq\left\|q\right\|_{1,\infty}\sup_{x\in\mathcal{D}_{2}}| \overline{\eta}\phi(x)-\overline{\eta}(\mathbb{K}f^{*})(x)|\] \[\lesssim\left\|q\right\|_{1,\infty}\overline{\eta}\Bigg{(}\sqrt{ \frac{t^{2\ell+2\alpha_{\sigma}+1}}{n}}+\sqrt{\frac{t^{2\alpha_{\sigma}+1}}{m _{2}}}+\sqrt{\frac{\iota}{m_{1}}}\Bigg{)}\] \[\lesssim\left\|q\right\|_{1,\infty}\|\mathbb{K}f^{*}\|_{L^{2}}^{ -1}\Bigg{(}\sqrt{\frac{t^{2\ell+2\alpha_{\sigma}+1}}{n}}+\sqrt{\frac{t^{2\alpha _{\sigma}+1}}{m_{2}}}+\sqrt{\frac{\iota}{m_{1}}}\Bigg{)}\]

and thus

\[\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\left(q(\overline{\eta}\phi(x))-q( \overline{\eta}(\mathbb{K}f^{*})(x))\right)^{2}\lesssim\left\|q\right\|_{1, \infty}^{2}\|\mathbb{K}f^{*}\|_{L^{2}}^{-2}\cdot\left(\frac{t^{2\ell+2\alpha _{\sigma}+2\chi+1}}{n}+\frac{t^{2\alpha_{\sigma}+2\chi+1}}{m_{2}}+\frac{\iota}{ m_{1}}\right)\!.\]Finally, we must relate the empirical error between \(q\circ\overline{\eta}\cdot\mathbb{K}f^{*}\) and \(f^{*}\) to the population error. This can be done via standard concentration arguments: in Lemma 19, we show

\[\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\left(q(\overline{\eta}(\mathbb{K}f^{*})(x) )-f^{*}(x)\right)^{2}\lesssim\left\|q\circ(\overline{\eta}(\mathbb{K}f^{*}))-f ^{*}\right\|_{L^{2}}^{2}+\frac{\left\|q\right\|_{\infty}^{2}\iota+\iota^{2 \ell+1}}{n}.\]

Altogether,

\[L_{2}(\theta^{*}) \lesssim\left\|q\right\|_{1,\infty}^{2}\left\|\mathbb{K}f^{*} \right\|_{L^{2}}^{-2}\cdot\left(\frac{2\ell+2\alpha_{s}+2\chi+1}{n}+\frac{ \iota^{2\alpha_{s}+2\chi+1}}{m_{2}}+\frac{\iota}{m_{1}}\right)+\frac{\left\|q \right\|_{\infty}^{2}\iota+\iota^{2\ell+1}}{n}+\frac{\left\|q\right\|_{2, \infty}^{2}\iota}{m_{1}}\] \[\quad+\left\|q\circ(\overline{\eta}(\mathbb{K}f^{*}))-f^{*}\right\| _{L^{2}}^{2}\] \[\lesssim\left\|q\right\|_{1,\infty}^{2}\left\|\mathbb{K}f^{*} \right\|_{L^{2}}^{-2}\cdot\left(\frac{2\ell+2\alpha_{s}+2\chi+1}{n}+\frac{ \iota^{2\alpha_{s}+2\chi+1}}{m_{2}}+\frac{\iota}{m_{1}}\right)+\frac{\iota^{2 \ell+1}}{n}+\frac{\left\|q\right\|_{2,\infty}^{2}\iota}{m_{1}}\] \[\quad+\left\|q\circ(\overline{\eta}(\mathbb{K}f^{*}))-f^{*}\right\| _{L^{2}}^{2}\]

since \(\left\|\mathbb{K}\right\|_{op}\lesssim 1\) and thus \(\left\|\mathbb{K}f^{*}\right\|_{L^{2}}\gtrsim 1\). 

### Proof of Lemma 5

Proof.: The standard linear Rademacher bound states that for functions of the form \(x\mapsto w^{T}\psi(x)\) with \(\left\|w\right\|_{2}\lesssim B_{w}\), the empirical Rademacher complexity is upper bounded by

\[\frac{B_{w}}{n}\sqrt{\sum_{x\in\mathcal{D}}\left\|\psi(x)\right\|^{2}}.\]

We have that \(f(x;\theta)=a^{T}\psi(x)\), where

\[\psi(x)=\operatorname{Vec}\left(\frac{1}{m_{1}}\sigma_{1}(a_{j}^{(0)} \overline{\eta}\phi(x)+b_{j}^{(0)})\cdot\mathbf{1}_{b_{j}^{(0)}>0}\right).\]

By Lemma 8, with high probability, \(\sup_{x\in\mathcal{D}_{2}}\left|\overline{\eta}\phi(x)\right|\leq 1\). Thus for \(x\in\mathcal{D}_{2}\)

\[\left\|\psi(x)\right\|^{2}\leq\frac{1}{m_{1}^{2}}(1+\left|b_{j}\right|)^{2} \lesssim\frac{1}{m_{1}}\]

with high probability by Lemma 10. Altogether, we have

\[\mathcal{R}_{\mathcal{D}_{2}}(\mathcal{F}(B_{a}))\lesssim\sqrt{\frac{B_{a} \cdot n\cdot 1/m_{1}}{n}}=\sqrt{\frac{B_{a}^{2}}{nm_{1}}}.\]

### Proof of Lemma 6

Proof.: We can bound

\[\mathbb{E}_{x}\left[\left(f(x;\hat{\theta})-f^{*}(x)\right)^{2} \right]-\mathbb{E}_{x}\left[\ell_{\tau}\Big{(}f(x;\hat{\theta})-f^{*}(x) \Big{)}\right]\] \[=\mathbb{E}_{x}\left[\left(\left(f(x;\hat{\theta})-f^{*}(x) \right)^{2}-\tau^{2}\right)\cdot\mathbf{1}_{\left|f(x;\hat{\theta})-f^{*}(x) \right|\geq\tau}\right]\] \[\leq\mathbb{E}_{x}\left[\left(f(x;\hat{\theta})-f^{*}(x)\right) ^{2}\cdot\mathbf{1}_{\left|f(x;\hat{\theta})-f^{*}(x)\right|\geq\tau}\right]\] \[\leq\mathbb{E}_{x}\left[\left(f(x;\hat{\theta})-f^{*}(x)\right) ^{4}\right]^{1/2}\cdot\mathbb{P}\Big{(}\Big{|}f(x;\hat{\theta})-f^{*}(x)\Big{|} \geq\tau\Big{)}^{1/2}\] \[\lesssim\left[\mathbb{E}_{x}\left[f(x;\hat{\theta})^{4}\right]^ {1/2}+\mathbb{E}_{x}\left[f^{*}(x)^{4}\right]^{1/2}\right]\cdot\left[\mathbb{P }\Big{(}\Big{|}f(x;\hat{\theta})\Big{|}\geq\tau\Big{)}+\mathbb{P}(\left|f^{*}( x)\right|\geq\tau\Big{)}\right]\]Next, we bound \(f(x;\hat{\theta})\):

\[\Big{|}f(x;\hat{\theta})\Big{|} =\Bigg{|}\frac{1}{m_{1}}\sum_{j=1}^{m_{1}}\hat{a}_{j}\sigma_{1} \Big{(}a_{j}^{(0)}\cdot\overline{\eta}\phi(x)+b_{j}\Big{)}\mathbf{1}_{b_{j}^{(0 )}>0}\Bigg{|}\] \[\leq\frac{1}{m_{1}}\sum_{j=1}^{m_{1}}|\hat{a}_{j}|(|\overline{\eta }\phi(x)|+|b_{j}|)\] \[\leq\frac{\|\hat{a}\|_{2}}{\sqrt{m_{1}}}|\overline{\eta}\phi(x)|+ \frac{1}{m_{1}}\|\hat{a}\|_{2}\|b\|_{2}\]

By Lemma 10, with high probability over the initialization we have \(\|b\|_{2}\lesssim\sqrt{m_{1}}\). Next, by Lemma 4, with high probability over the initialization and dataset we have \(\|\hat{a}\|_{2}\lesssim\|a^{*}\|_{2}\lesssim\|q\|_{2,\infty}\sqrt{m_{1}}\). Thus with high probability we have

\[\Big{|}f(x;\hat{\theta})\Big{|}\lesssim\|q\|_{2,\infty}(|\overline{\eta}\phi(x) |+1)\]

uniformly over \(x\).

We naively bound

\[|\phi(x)|\leq\frac{1}{m_{2}}\Big{\|}h^{(0)}(x)\Big{\|}\cdot\frac{1}{n}\sum_{i= 1}^{n}\Big{\|}h^{(0)}(x_{i})\Big{\|}\Big{\|}f(x_{i};\theta^{(0)})-f^{*}(x_{i}) \Big{|}.\]

By Lemma 11 and Lemma 12, with high probability we have \(\big{|}f(x_{i};\theta^{(0)})-f^{*}(x_{i})\big{|}\lesssim\iota^{\ell}\) for all \(i\). With high probability, we also have \(\big{\|}h^{(0)}(x_{i})\big{\|}\lesssim\sqrt{m_{2}}\). Additionally, we have

\[\Big{\|}h^{(0)}(x)\Big{\|}_{2}^{4} =\left(\sum_{j=1}^{m_{2}}\sigma_{2}(x\cdot v_{j})^{2}\right)^{2}\] \[\leq m_{2}\sum_{j=1}^{m_{2}}\sigma_{2}(x\cdot v_{j})^{4}\] \[\lesssim m_{2}\sum_{j=1}^{m_{2}}(1+|x\cdot v_{j}|^{4\alpha_{ \sigma}})\]

Since \(x\) is \(C_{\gamma}\) subGaussian and \(\|v\|=1\), we have

\[\mathbb{E}_{x}\left\|h^{(0)}(x)\right\|_{2}^{4} \lesssim m_{2}^{2}\,\mathbb{E}\left|x\cdot v_{j}\right|^{4\alpha\sigma}\] \[\lesssim m_{2}^{2}.\]

Altogether,

\[\mathbb{E}_{x}|\phi(x)|^{4}\lesssim\iota^{4\ell},\]

and thus

\[\mathbb{E}_{x}\bigg{[}\Big{|}f(x;\hat{\theta})\Big{|}^{4}\bigg{]}^{1/2} \lesssim\|q\|_{2,\infty}^{2}\overline{\eta}^{2}\iota^{2\ell}\leq\|q\|_{2, \infty}^{2}m_{1}^{2}\|\mathbb{K}f^{*}\|_{L^{2}}^{-2}\iota^{2\ell}\]

By Assumption 2, \(\mathbb{E}_{x}\left[(f^{*}(x))^{4}\right]\lesssim 1\).

We choose \(\tau=\Theta(\max(\iota^{\ell},\|q\|_{2,\infty})\). By Lemma 7 and Lemma 8\(\big{\{}\big{|}f(x,\hat{\theta})\big{|}>\tau\big{\}}\) and \(\{|f^{*}(x)|>\tau\}\) are both high probability events. Therefore by choosing \(C\) sufficiently large, we have the bound

\[\Big{[}\mathbb{P}\Big{(}\Big{|}f(x;\hat{\theta})\Big{|}\geq\tau\Big{)}+ \mathbb{P}(|f^{*}(x)|\geq\tau)\Big{]}\leq m_{1}^{-4}\iota^{-2\ell}\]

Altogether, this gives

\[\mathbb{E}_{x}\left[\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)}^{2}\right]- \mathbb{E}_{x}\left[\ell_{\tau}\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)}\right] \lesssim\frac{\|q\|_{2,\infty}^{2}\|\mathbb{K}f^{*}\|_{L^{2}}^{-2}}{m_{1}^{2}}+ \frac{1}{m_{1}^{4}}\leq\frac{\|q\|_{2,\infty}^{2}}{m_{1}}.\]

[MISSING_PAGE_EMPTY:23]

### Concentration

**Lemma 10**.: _Let \(m_{1}\gtrsim\iota\). With high probability,_

\[\frac{1}{m_{1}}\sum_{i=1}^{m_{1}}\left(b_{i}^{(0)}\right)^{2}\lesssim 1.\]

Proof.: By Bernstein, we have

\[\left|\frac{1}{m_{1}}\sum_{i=1}^{m_{1}}\left(b_{i}^{(0)}\right)^{2}-1\right| \lesssim\sqrt{\frac{\iota}{m_{1}}}\leq 1.\]

**Lemma 11**.: _With high probability, \(\left|f(x;\theta^{(0)})\right|\lesssim\sqrt{\frac{\iota}{m_{1}}}\). for all \(x\)._

Proof.: We have

\[f(x;\theta^{(0)})=\frac{1}{m_{1}}\sum_{i=1}^{m_{1}}a_{i}\sigma_{1}(b_{i}).\]

Since \(a_{i}\sim\operatorname{Unif}(\{\pm 1\})\) and \(b_{i}\sim\mathcal{N}(0,1)\), the quantities \(a_{i}\sigma_{1}(b_{i})\) are \(1\)-subGaussian, and thus by Hoeffding with high probability we have

\[\left|f(x;\theta^{(0)})\right|\lesssim\sqrt{\frac{\iota}{m_{1}}}.\]

**Lemma 12**.: _With high probability_

\[\sup_{x\in\mathcal{D}_{2}}\mathbb{E}_{v}\big{[}\sigma_{2}(x\cdot v)^{4}\big{]} \lesssim 1\quad\text{and}\quad\sup_{j\in[m_{2}]}\mathbb{E}_{x}\big{[}\sigma_{ 2}(x\cdot v_{j})^{4}\big{]}\lesssim 1.\]

Proof.: First, we have that \(\sigma_{2}(x\cdot v)^{4}\lesssim 1+(x\cdot v)^{4\alpha_{\sigma}}\). Therefore

\[\mathbb{E}_{v}\big{[}\sigma_{2}(x\cdot v)^{4}\big{]}\lesssim 1+\mathbb{E}_{v} \big{[}(x\cdot v)^{4\alpha_{\sigma}}\big{]}\lesssim 1+\|x\|^{4\alpha_{\sigma}}d^{ -2\alpha_{\sigma}}.\]

Next, with high probability we have \(\sup_{x\in\mathcal{D}_{2}}\left|\left\|x\right\|^{2}-\mathbb{E}\|x\|^{2} \right|\leq\iota C_{\gamma}^{2}\lesssim\iota\). Since \(\mathbb{E}\|x\|^{2}\lesssim d\gamma_{x}^{2}\lesssim d\), we can thus bound

\[\sup_{x\in\mathcal{D}_{2}}\mathbb{E}_{v}\big{[}\sigma_{2}(x\cdot v )^{4}\big{]} \lesssim 1+\gamma_{v}^{4\alpha_{\sigma}}\sup_{x\in\mathcal{D}_{2}} \left\|x\right\|^{2}\] \[\lesssim 1+d^{-2\alpha_{\sigma}}(d+\iota)^{2\alpha_{\sigma}}\] \[\lesssim 1.\]

The proof for the other inequality is identical. 

**Lemma 13**.: _Let \(m_{2}\gtrsim\iota^{2\alpha_{\sigma}+1}\). With high probability, \(\sup_{x\in\mathcal{D}_{1}\cup\mathcal{D}_{2}}\left\|h^{(0)}(x)\right\|\lesssim \sqrt{m_{2}}\)_

Proof.: Since \(x\) is \(C_{\gamma}\) subGaussian and \(v\) is \(1/\sqrt{d}\) subGaussian, with high probability we have

\[x\cdot v\leq O(C_{\gamma}\iota)=O(\iota).\]

Union bounding over \(x\in\mathcal{D}_{2},j\in[m_{2}]\), with high probability we have \(\sup_{x\in\mathcal{D}_{1}\cup\mathcal{D}_{2},j\in[m_{2}]}|x\cdot v_{j}|\leq O (\iota)\). Therefore

\[\sup_{x,v_{j}}|\sigma_{2}(x\cdot v_{j})|\leq O(\iota^{\alpha_{\sigma}}).\]Next, see that

\[\frac{1}{m_{2}}\Big{\|}h^{(0)}(x)\Big{\|}^{2}=\frac{1}{m_{2}}\sum_{j=1} ^{m_{2}}\sigma_{2}(x\cdot v_{j})^{2}.\]

Pick truncation radius \(R=O(\iota^{\alpha_{\sigma}})\). By Hoeffding's inequality and a union bound over \(x\in\mathcal{D}_{1}\cup\mathcal{D}_{2}\), we have that with high probability

\[\sup_{x\in\mathcal{D}_{1}\cup\mathcal{D}_{2}}\left|\frac{1}{m_{2}} \sum_{j=1}^{m_{2}}\mathbf{1}_{\sigma_{2}(x\cdot v_{j})^{2}\leq R}\sigma_{2}(x \cdot v_{j})^{2}-\mathbb{E}_{v}\big{[}\mathbf{1}_{\sigma_{2}(x\cdot v_{j})^{2} \leq R}\sigma_{2}(x\cdot v)^{2}\big{]}\right|\lesssim\sqrt{\frac{\iota^{2\alpha _{\sigma}+1}}{m_{2}}}.\]

Observe that

\[\big{|}\mathbb{E}_{v}\big{[}\mathbf{1}_{\sigma_{2}(x\cdot v)^{2} \leq R}\sigma_{2}(x\cdot v)^{2}\big{]}-\mathbb{E}_{v}\big{[}\sigma(x\cdot v)^{ 2}\big{]}\big{|} =\mathbb{E}_{v}\big{[}\mathbf{1}_{\sigma_{2}(x\cdot v)^{2}>R}\sigma _{2}(x\cdot v)^{2}\big{]}\] \[\leq\mathbb{P}(\sigma_{2}(x\cdot v)^{2}>R)\mathbb{E}\big{[}\sigma _{2}(x\cdot v)^{4}\big{]}^{1/2}\] \[\leq\frac{1}{d}.\]

Therefore with high probability

\[\left|\frac{1}{m_{2}}\Big{\|}h^{(0)}(x)\Big{\|}^{2}-\mathbb{E}_{v} \big{[}\sigma(x\cdot v)^{2}\big{]}\right|\lesssim\sqrt{\frac{\iota^{2\alpha_{ \sigma}+1}}{m_{2}}}+\frac{1}{d}.\]

by Lemma 12. Lemma 12 also tells us that \(\mathbb{E}_{v}\big{[}\sigma_{2}(x\cdot v)^{2}\big{]}=O(1)\). Altogether, for \(m_{2}\gtrsim\iota^{2\alpha_{\sigma}+1}\) we have

\[\frac{1}{m_{2}}\Big{\|}h^{(0)}(x)\Big{\|}^{2}\leq 2\]

and hence \(\big{\|}h^{(0)}(x)\big{\|}\lesssim\sqrt{m_{2}}\), as desired. 

**Lemma 14**.: \(\mathbb{P}_{x\sim\nu}\big{(}|f^{*}(x)|\geq C_{f}e\iota^{\ell}\big{)}\leq e^{- \iota}\) _and \(\mathbb{P}_{x\sim\nu}(|(\mathbb{K}f^{*})(x)|\geq C_{K}e\iota^{\chi}\cdot\| \mathbb{K}f^{*}\|_{L^{2}})\leq e^{-\iota}\)_

Proof.: By Markov's inequality, we have

\[\mathbb{P}(|f^{*}(x)|>t)=\mathbb{P}(|f^{*}(x)|^{q}>t^{q})\leq\frac {\|f^{*}\|_{q}^{q}}{t^{q}}\leq\frac{C_{f}^{q}q^{q\ell}}{t^{q}}.\]

Choose \(t=C_{f}e\iota^{\ell}\). We select \(q=\frac{t}{e^{1-1/\ell}}\), which is at least \(1\) for \(C\) in the definition of \(\iota\) sufficiently large. Plugging in, we get

\[\mathbb{P}\big{(}|f^{*}(x)|>C_{f}e\iota^{\ell}\big{)}\leq\frac{C_ {f}^{q}\iota^{\ell}}{C_{f}^{q}e^{q\ell}\iota^{q\ell}}=e^{-q\ell}=e^{-\iota\ell e ^{1/\ell-1}}\leq e^{-\iota},\]

since \(\ell e^{1/\ell-1}\geq 1\).

An analogous derivation for the function \(\frac{\mathbb{K}f^{*}}{\|\mathbb{K}f^{*}\|_{L^{2}}}\) yields the second bound 

**Corollary 1**.: _With high probability, \(\sup_{x\in\mathcal{D}_{2}}|f^{*}(x)|\leq C_{f}e\cdot\iota^{\ell}\) and \(\sup_{x\in\mathcal{D}_{2}}|(\mathbb{K}f^{*})(x)|\lesssim C_{K}e\cdot\iota^{\chi }\|\mathbb{K}f^{*}\|_{L^{2}}\)_

Proof.: Union bounding the previous lemma over \(x\in\mathcal{D}_{2}\) yields the desired result. 

**Lemma 15**.: _With high probability,_

\[\left\|\frac{1}{n}\sum_{i=1}^{n}f^{*}(x_{i})h^{(0)}(x_{i})-\mathbb{E}_{x} \Big{[}f^{*}(x)h^{(0)}(x)\Big{]}\right\|_{2}\lesssim\sqrt{\frac{m_{2}\iota^{2 \ell+2\alpha_{\sigma}+1}}{n}}\]Proof.: Consider the quantity

\[\Bigg{|}\frac{1}{n}\sum_{i=1}^{n}f^{*}(x_{i})\sigma_{2}(x_{i}\cdot v_{j})-\mathbb{E }_{x}\Big{[}f^{*}(x)h^{(0)}(x_{i})\Big{]}\Bigg{|}.\]

With high probability, \(\sup_{x\in\mathcal{D}_{2},j\in[m_{2}]}|f^{*}(x_{i})\sigma_{2}(x_{i}\cdot v_{j})| \lesssim\iota^{\ell+\alpha_{\sigma}}\). Pick truncation radius \(R=O(\iota^{\ell+\alpha_{\sigma}})\). By Hoeffding, we have with high probability.

\[\Bigg{|}\frac{1}{n}\sum_{i=1}^{n}f^{*}(x_{i})\sigma_{2}(x_{i}\cdot v_{j})\mathbf{ 1}_{f^{*}(x_{i})\sigma_{2}(x_{i}\cdot v_{j})\leq R}-\mathbb{E}_{x}\big{[}f^{*} (x)\sigma_{2}(x_{i}\cdot v_{j})\mathbf{1}_{f^{*}(x_{i})\sigma_{2}(x_{i}\cdot v _{j})\leq R}\Bigg{]}\Bigg{|}\lesssim\sqrt{\frac{\iota^{2\ell+2\alpha_{\sigma}+1 }}{n}}.\]

Furthermore, note that

\[\big{|}\mathbb{E}_{x}\big{[}f^{*}(x)\sigma_{2}(x\cdot v_{j})\mathbf{ 1}_{f^{*}(x_{i})\sigma_{2}(x\cdot v_{j})\leq R}\big{]}-\mathbb{E}_{x}[f^{*}(x) \sigma_{2}(x_{i}\cdot v_{j})]\big{|}\] \[=\big{|}\mathbb{E}\big{[}\mathbf{1}_{f^{*}(x)\sigma_{2}(x\cdot v_{ j})>R}f^{*}(x)\sigma_{2}(x_{i}\cdot v_{j})\big{]}\big{|}\] \[\leq\mathbb{P}(f^{*}(x)\sigma_{2}(x\cdot v_{j})>R)\mathbb{E}_{x} \big{[}\sigma_{2}(x\cdot v_{j})^{4}\big{]}\] \[\lesssim\frac{1}{n}.\]

Thus

\[\Bigg{|}\frac{1}{n}\sum_{i=1}^{n}f^{*}(x_{i})\sigma_{2}(x_{i}\cdot v_{j})- \mathbb{E}_{x}\Big{[}f^{*}(x)h^{(0)}(x_{i})\Big{]}\Bigg{|}\lesssim\sqrt{\frac{ \iota^{2\ell+2\alpha_{\sigma}+1}}{n}}\]

By a union bound, the above holds with high probability for all \(j\in[m_{2}]\), and thus

\[\Bigg{\|}\frac{1}{n}\sum_{i=1}^{n}f^{*}(x_{i})h^{(0)}(x_{i})-\mathbb{E}_{x} \Big{[}f^{*}(x)h^{(0)}(x)\Big{]}\Bigg{\|}_{2}\lesssim\sqrt{\frac{m_{2}\iota^{2 \ell+2\alpha_{\sigma}+1}}{n}}\]

**Lemma 16**.: _With high probability,_

\[\sup_{x\in\mathcal{X}_{d}}\Bigg{|}\frac{1}{m_{2}}\sum_{i=1}^{m_{2}}\mathbb{E}_ {x^{\prime}}[f^{*}(x^{\prime})\sigma_{2}(x^{\prime}\cdot v_{j})\sigma_{2}(x \cdot v_{j})]-\mathbb{E}_{x^{\prime},v}[f^{*}(x)\sigma_{2}(x^{\prime}\cdot v) \sigma_{2}(x\cdot v)]\Bigg{|}\lesssim\sqrt{\frac{\iota^{2\alpha_{\sigma}+1}}{m _{2}}}\]

Proof.: Fix \(x\in\mathcal{D}_{2}\). Consider the random variables \(Z(v)=\mathbb{E}_{x^{\prime}}[f^{*}(x^{\prime})\sigma_{2}(x^{\prime}\cdot v) \sigma_{2}(x\cdot v)]\). With high probability we have \(|\sigma_{2}(x\cdot v_{j})|\lesssim\iota^{\alpha_{\sigma}}\) and thus

\[|Z(v_{j})|=|\mathbb{E}_{x^{\prime}}[f^{*}(x^{\prime})\sigma_{2}(x^{\prime} \cdot v_{j})\sigma_{2}(x\cdot v_{j})]|\lesssim\iota^{\alpha_{\sigma}}\cdot \mathbb{E}_{x^{\prime}}\big{[}f^{*}(x^{\prime})^{2}\big{]}^{1/2}\mathbb{E}\big{[} \sigma_{2}(x^{\prime}\cdot v_{j})^{2}\big{]}^{2}\lesssim\iota^{\alpha_{ \sigma}}.\]

For all \(j\in[m_{2}]\). Choosing truncation radius \(R=\iota^{\alpha_{\sigma}}\), with Hoeffding we have with high probability that

\[\Bigg{|}\frac{1}{m_{2}}\sum_{i=1}^{m_{2}}\mathbb{E}_{x^{\prime}}[f^{*}(x^{ \prime})\sigma_{2}(x^{\prime}\cdot v_{j})]\sigma_{2}(x\cdot v_{j})\mathbf{1}_{|Z (v_{j})|\leq R}-\mathbb{E}_{x^{\prime},v}\big{[}f^{*}(x^{\prime})\sigma_{2}(x ^{\prime}\cdot v)\sigma_{2}(x\cdot v)\mathbf{1}_{|Z(v)|\leq R}\Bigg{]}\Bigg{|} \lesssim\sqrt{\frac{\iota^{2\alpha_{\sigma}+1}}{m_{2}}}\]

Next, we have that

\[\big{|}\mathbb{E}_{x^{\prime},v}\big{[}f^{*}(x^{\prime})\sigma_{2 }(x^{\prime}\cdot v)\sigma_{2}(x\cdot v)\mathbf{1}_{|Z(v)|\leq R}\big{]}- \mathbb{E}_{x^{\prime},v}[f^{*}(x^{\prime})\sigma_{2}(x^{\prime}\cdot v)\sigma _{2}(x\cdot v)]\big{|}\] \[\qquad=\big{|}\mathbb{E}_{x^{\prime},v}\big{[}f^{*}(x^{\prime}) \sigma_{2}(x^{\prime}\cdot v)\sigma_{2}(x\cdot v)\mathbf{1}_{|Z(v)|>R}\big{]} \big{|}\] \[\qquad\leq\mathbb{P}(Z(v)>R)\cdot\mathbb{E}_{x^{\prime},v}\big{[}f ^{*}(x^{\prime})^{2}\sigma_{2}(x^{\prime}\cdot v)^{2}\sigma_{2}(x\cdot v)^{2} \big{]}^{1/2}\] \[\qquad\lesssim\frac{1}{m_{2}}.\]

Conditioning on the high probability event that \(\sup_{j\in[m_{2}]}|Z(v_{j})|\leq R\), we have with high probability that

\[\Bigg{|}\frac{1}{m_{2}}\sum_{i=1}^{m_{2}}\mathbb{E}_{x^{\prime}}[f^{*}(x^{ \prime})\sigma_{2}(x^{\prime}\cdot v_{j})]\sigma_{2}(x\cdot v_{j})-\mathbb{E}_{x ^{\prime},v}[f^{*}(x^{\prime})\sigma_{2}(x^{\prime}\cdot v)\sigma_{2}(x\cdot v) ]\Bigg{|}\lesssim\sqrt{\frac{\iota^{2\alpha_{\sigma}+1}}{m_{2}}}.\]

A union bound over \(x\in\mathcal{D}_{2}\) yields the desired result.

**Lemma 17**.: _With high probability,_

\[\sup_{x\in\mathcal{D}_{2}}\left|\frac{1}{m_{1}}\sum_{i=1}^{m_{1}}v(a_{i},b_{i}) \sigma_{1}(a_{i}\overline{\eta}\phi(x)+b_{i})\mathbf{1}_{b_{i}>0}-f_{v}^{\infty }(x)\right|\lesssim\sqrt{\frac{\left\|v\right\|_{\infty}^{2}\iota}{m_{1}}}\]

Proof.: Condition on the high probability event \(\sup_{x\in\mathcal{D}_{2}}\left|\overline{\eta}\phi(x)\right|\leq 1\). Next, note that whenever \(b>2\) that \(v(a,b)=0\). Therefore we can bound

\[\left|v(a,b)\sigma_{1}(a\overline{\eta}\phi(x)+b_{i})\mathbf{1}_{b_{i}>0} \right|\leq 2\left\|v\right\|_{\infty}\]

Therefore by Hoeffding's inequality we have that

\[\left|\frac{1}{m_{1}}\sum_{i=1}^{m_{1}}v(a_{i},b_{i})\sigma_{1}(a_{i} \overline{\eta}\phi(x)+b_{i})\mathbf{1}_{b_{i}>0}-f_{v}^{\infty}(x)\right| \lesssim\sqrt{\frac{\left\|v\right\|_{\infty}^{2}\iota}{m_{1}}}+\]

The desired result follows via a Union bound over \(x\in\mathcal{D}_{2}\). 

**Lemma 18**.: _With high probability,_

\[\left|\frac{1}{m_{1}}\sum_{i=1}^{m_{1}}v(a_{i},b_{i})^{2}-\left\|v\right\|_{L ^{2}}^{2}\right|\lesssim\sqrt{\frac{\left\|v\right\|_{\infty}^{4}\iota}{m_{1} }}.\]

Proof.: Note that

\[v(a_{i},b_{i})^{2}\leq\left\|v\right\|_{\infty}^{2}\]

Thus by Hoeffding's inequality we have that

\[\left|\frac{1}{m_{1}}\sum_{i=1}^{m_{1}}v(a_{i},b_{i})^{2}-\left\|v\right\|_{L ^{2}}^{2}\right|\lesssim\sqrt{\frac{\left\|v\right\|_{\infty}^{4}\iota}{m_{1}}}.\]

**Corollary 2**.: _Let \(m_{1}\gtrsim\iota\). Then with high probability_

\[\sum_{i=1}^{m_{i}}v(a_{i},b_{i})^{2}\lesssim\left\|v\right\|_{\infty}^{2}m_{1}.\]

Proof.: By the previous lemma, we have that

\[\left|\frac{1}{m_{1}}\sum_{i=1}^{m_{1}}v(a_{i},b_{i})^{2}-\left\|v\right\|_{L ^{2}}^{2}\right|\lesssim\sqrt{\frac{\left\|v\right\|_{\infty}^{4}\iota}{m_{1} }}\lesssim\left\|v\right\|_{\infty}^{2}.\]

Thus

\[\sum_{i=1}^{m_{i}}v(a_{i},b_{i})^{2}\lesssim\left\|v\right\|_{L^{2}}^{2}m_{1} +\left\|v\right\|_{\infty}^{2}m_{1}\lesssim\left\|v\right\|_{\infty}^{2}m_{1}.\]

**Lemma 19**.: _With high probability,_

\[\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\left(q(\overline{\eta}(\mathbb{K}f^{*}) (x))-f^{*}(x)\right)^{2}\lesssim\left\|q\circ(\overline{\eta}(\mathbb{K}f^{*} ))-f^{*}\right\|_{L^{2}}^{2}+\frac{\left\|q\right\|_{\infty}^{2}\iota+\iota^{2 \ell+1}}{n}.\]Proof.: Let \(S\) be the set of \(x\) so that \(\left|\overline{\eta}(\mathbb{K}f^{*})(x)\right|\leq 1\) and \(\left|f^{*}(x)\right|\lesssim\iota^{\ell}\). Consider the random variables \((q(\overline{\eta}(\mathbb{K}f^{*})(x))-f^{*}(x))^{2}\cdot\mathbf{1}_{x\in S}\). We have that

\[\left|\left(q(\overline{\eta}(\mathbb{K}f^{*})(x))-f^{*}(x)\right)^{2}\cdot \mathbf{1}_{x\in S}\right|\lesssim\sup_{z\in[-1,1]}\left|q(z)\right|^{2}+\iota ^{2\ell}.\]

and

\[\mathbb{E}\big{[}(q(\overline{\eta}(\mathbb{K}f^{*})(x))-f^{*}(x))^{2}\cdot \mathbf{1}_{x\in S}\big{]}^{2}\lesssim\left(\sup_{z\in[-1,1]}\left|q(z)\right|^ {2}+\iota^{2\ell}\right)\cdot\left\|q\circ(\overline{\eta}(\mathbb{K}f^{*}))-f ^{*}\right\|_{L^{2}}^{2}.\]

Therefore by Berstein's inequality we have that

\[\left|\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\left(q(\overline{\eta }(\mathbb{K}f^{*})(x))-f^{*}(x)\right)^{2}\mathbf{1}_{x\in S}-\left\|(q\circ( \overline{\eta}(\mathbb{K}f^{*}))-f^{*})\mathbf{1}_{x\in S}\right\|_{L^{2}}^ {2}\right|\] \[\lesssim\sqrt{\frac{\left(\sup_{z\in[-1,1]}\left|q(z)\right|^{2}+ \iota^{2\ell}\right)\cdot\left\|(q\circ(\overline{\eta}(\mathbb{K}f^{*}))-f^{*} )\mathbf{1}_{x\in S}\right\|_{L^{2}}^{2}\iota}{n}}+\frac{\sup_{z\in[-1,1]} \left|q(z)\right|^{2}+\iota^{2\ell}}{n}\iota\] \[\lesssim\left\|(q\circ(\overline{\eta}(\mathbb{K}f^{*}))-f^{*}) \mathbf{1}_{x\in S}\right\|_{L^{2}}^{2}+\frac{\sup_{z\in[-1,1]}\left|q(z) \right|^{2}+\iota^{2\ell}}{n}\iota.\]

Conditioning on the high probability event that \(x\in S\) for all \(x\in\mathcal{D}_{2}\), we get that

\[\frac{1}{n}\sum_{x\in\mathcal{D}_{2}}\left(q(\overline{\eta}( \mathbb{K}f^{*})(x))-f^{*}(x)\right)^{2} \lesssim\left\|(q\circ(\overline{\eta}(\mathbb{K}f^{*}))-f^{*}) \mathbf{1}_{x\in S}\right\|_{L^{2}}^{2}+\frac{\sup_{z\in[-1,1]}\left|q(z) \right|^{2}+\iota^{2\ell}}{n}\iota\] \[\leq\left\|q\circ(\overline{\eta}(\mathbb{K}f^{*}))-f^{*}\right\| _{L^{2}}^{2}+\frac{\sup_{z\in[-1,1]}\left|q(z)\right|^{2}\iota+\iota^{2\ell+1} }{n}.\]

## Appendix E Proofs for Section 4

### Single Index Model

Proof of Theorem 2.: It is easy to see that Assumptions 1 and 3 are satisfied. By assumption \(g^{*}\) is polynomially bounded, i.e there exist constants \(C_{g},\alpha_{g}\) such that

\[g^{*}(z)\leq C_{g}(1+\left|z\right|)^{\alpha_{g}}\leq C_{g}2^{ \alpha_{g}-1}(1+\left|z\right|^{\alpha_{g}}).\]

Therefore

\[\left\|f^{*}\right\|_{q}=\left\|g^{*}\right\|_{L^{q}(\mathbb{R}, \mathcal{N}(0,1)}\leq C_{g}2^{\alpha_{g}-1}(1+\mathbb{E}_{z\sim\mathcal{N}(0,1 )}\left[\left|z\right|^{\alpha_{g}q}\right]^{1/q})\leq C_{g}2^{\alpha_{g}} \alpha_{g}^{\alpha_{g}/2}q^{\alpha_{g}/2}.\]

Thus Assumption 2 is satisfied with \(C_{f}=C_{g}2^{\alpha_{g}}\alpha_{g}^{\alpha_{g}/2}\) and \(\ell=\alpha_{g}/2\).

Next, we see that

\[K(x,x^{\prime})=\mathbb{E}_{v}[(x\cdot v)(x^{\prime}\cdot v)]= \frac{x\cdot x^{\prime}}{d}\]

Therefore

\[(\mathbb{K}f)(x)=\mathbb{E}_{x^{\prime}}[f^{*}(x^{\prime})x^{ \prime}\cdot x/d]=\frac{1}{d}\mathbb{E}_{x^{\prime}}[\nabla f^{*}(x^{\prime})] \cdot x.\]

Furthermore, we have

\[\mathbb{E}_{x^{\prime}}[\nabla f^{*}(x^{\prime})]=\mathbb{E}_{x^{ \prime}}[w^{*}g^{\prime}(w^{*}\cdot x^{\prime})]=w^{*}\mathbb{E}_{z\sim\mathcal{ N}(0,1)}[g^{\prime}(z)].\]

Altogether, letting \(c_{1}=\mathbb{E}_{z\sim\mathcal{N}(0,1)}[g^{\prime}(z)]=\Omega(1)\), we have \((\mathbb{K}f^{*})(x)=\frac{1}{d}c_{1}x^{T}w^{*}\). Assumption 4 is thus satisfied with \(\chi=1/2\).

Next, see that \(\left\|\mathbb{K}f^{*}\right\|_{L^{2}}=c_{1}/d\). We select the test function \(q\) to be \(q(z)=g(\overline{\eta}^{-1}d/c_{1}\cdot z)\), so that

\[q(\overline{\eta}(\mathbb{K}f^{*})(x))=g(x^{*}\cdot x)=f^{*}(x).\]

Since \(\overline{\eta}=\Theta(d\iota^{-\chi})\), we see that

\[\sup_{z\in[-1,1]}|q(z)| =\sup_{z\in[-\Theta(\iota^{\chi}),\Theta(\iota^{\chi})]}|g(z)|= \operatorname{poly}(\iota)\] \[\sup_{z\in[-1,1]}|q^{\prime}(z)| =\overline{\eta}^{-1}d/c_{1}\sup_{z\in[-\Theta(\iota^{\chi}), \Theta(\iota^{\chi})]}|g^{\prime}(z)|=\operatorname{poly}(\iota)\] \[\sup_{z\in[-1,1]}|q^{\prime\prime}(z)| =\overline{\eta}^{-2}d^{2}/c_{1}^{2}\sup_{z\in[-\Theta(\iota^{ \chi}),\Theta(\iota^{\chi})]}|g^{\prime\prime}(z)|=\operatorname{poly}(\iota)\]

Therefore we can bound the population loss as

\[\mathbb{E}_{x}\bigg{[}\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)}^{2}\bigg{]} \lesssim\frac{d^{2}\operatorname{poly}(\iota)}{\min(n,m_{1},m_{2})}+\frac{1}{ \sqrt{n}}.\]

### Quadratic Feature

Throughout this section, we call \(x^{T}Ax\) a degree 2 _spherical harmonic_ if \(A\) is symmetric, \(\mathbb{E}\big{[}x^{T}Ax\big{]}=0\), and \(\mathbb{E}\left[(x^{T}Ax)^{2}\right]=1\). Then, we have that \(\operatorname{Tr}(A)=0\), and also

\[1=\mathbb{E}[x^{\otimes 4}](A^{\otimes 2})=3\chi_{2}I^{\otimes 2}(A^{\otimes 2}) =2\chi_{2}\|A\|_{F}^{2}\Longrightarrow\|A\|_{F}=\sqrt{\frac{1}{2\chi_{2}}}= \sqrt{\frac{d+2}{2d}}=\Theta(1).\]

See Appendix F for technical background on spherical harmonics.

Our goal is to prove the following key lemma, which states that the projection of \(f^{*}\) onto degree 2 spherical harmonics is approximately \(x^{T}Ax\).

**Lemma 20**.: _Let \(q\) be a \(L\)-Lipschitz function with \(|q(0)|\leq L\), and let the target \(f^{*}\) be of the form \(f^{*}(x)=q(x^{T}Ax)\), where \(x^{T}Ax\) is a spherical harmonic. Let \(c_{1}=\mathbb{E}_{z\sim\mathcal{N}(0,1)}[q^{\prime}(z)]\). Then_

\[\left\|P_{2}f^{*}-c_{1}x^{T}Ax\right\|_{L^{2}}\lesssim L\kappa^{1/6}d^{-1/12} \log d.\]

We defer the proof of this Lemma to Appendix E.2.1.

As a consequence, the learned feature \(\mathbb{K}f^{*}\) is approximately proportional to \(x^{T}Ax\).

**Lemma 21**.: _Recall \(c_{1}=\mathbb{E}_{z\sim\mathcal{N}(0,1)}[q^{\prime}(z)]\). Then_

\[\left\|\mathbb{K}f^{*}-\lambda_{2}^{2}(\sigma)c_{1}x^{T}Ax\right\|_{L^{2}} \lesssim L\kappa^{1/6}d^{-2-1/12}\log d\]

Proof.: Since \(\mathbb{E}[f^{*}(x)]=0\), \(P_{0}f^{*}=0\). Next, since \(f^{*}\) is an even function, \(P_{k}f^{*}=0\) for \(k\) odd. Thus

\[\left\|\mathbb{K}f^{*}-\lambda_{2}^{2}(\sigma)P_{2}f^{*}\right\|_{L^{2}} \lesssim d^{-4}.\]

Additionally, by Lemma 20 we have that

\[\left\|P_{2}f^{*}-c_{1}x^{T}Ax\right\|_{L^{2}}\lesssim\left\|T_{2}-c_{1}\cdot A \right\|_{F}\lesssim L\kappa^{1/6}d^{-1/12}\log d.\]

Since \(\lambda_{2}^{2}(\sigma)=\Theta(d^{-2})\), we have

\[\left\|\mathbb{K}f^{*}-\lambda_{2}^{2}(\sigma)c_{1}x^{T}Ax\right\|_{L^{2}} \lesssim L\kappa^{1/6}d^{-2-1/12}\log d.\]

**Corollary 3**.: _Assume \(\kappa=o(\sqrt{d})\). Then_

\[\left\|x^{T}Ax-\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-1}\mathbb{K}f^{*} \right\|_{L^{2}}\lesssim L\kappa^{1/6}d^{-1/12}\log d\]Proof.: \[\left\|x^{T}Ax-\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-1}\mathbb{K}f ^{*}\right\|_{L^{2}}\] \[=\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-1}\left\|x^{T}Ax\right\| \mathbb{K}f^{*}\right\|_{L^{2}}-\mathbb{K}f^{*}\right\|_{L^{2}}\] \[\leq\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-1}\left\|\mathbb{K}f^ {*}-\lambda_{2}^{2}(\sigma)c_{1}x^{T}Ax\right\|_{L^{2}}+\left\|\mathbb{K}f^{*} \right\|_{L^{2}}^{-1}\left\|\left\|\mathbb{K}f^{*}\right\|-\lambda_{2}^{2}( \sigma)|c_{1}|\right|\] \[\lesssim L\kappa^{1/6}d^{-2-1/12}\log d\|\mathbb{K}f^{*}\|_{L^{2}}^ {-1}\] \[\lesssim L\kappa^{1/6}d^{-1/12}\log d.\]

Proof of Theorem 3.: By our choice of \(\nu\), we see that Assumption 1 is satisfied. We next verify Assumption 2. Since \(f^{*}\) is 1-Lipschitz, we can bound \(|g^{*}(z)|\leq|g^{*}(0)|+|z|\), and thus

\[\mathbb{E}_{x}\left[g^{*}(x^{T}Ax)^{q}\right]^{1/q} \leq|g^{*}(0)|+\mathbb{E}_{x}\left[\left|x^{T}Ax\right|^{q}\right] ^{1/q}\] \[\leq|g^{*}(0)|+q\] \[\leq(1+g^{*}(0))q,\]

where we used Lemma 35. Thus Assumption 2 holds with \(\ell=1\).

Finally, we have

\[\mathbb{K}f^{*}=\sum_{k\geq 2}\lambda_{k}^{2}(\sigma_{2})P_{k}f^{*}.\]

By Lemma 21 we have \(\left\|\mathbb{K}f^{*}\right\|_{L^{2}}\geq\frac{1}{2}\lambda_{2}^{2}(\sigma)c_ {1}\) for \(d\) larger than some absolute constant. Next, by Lemma 35 we have for any \(q\leq\frac{1}{4}d^{2}\)

\[\left\|\mathbb{K}f^{*}\right\|_{q} \leq\sum_{k\geq 2}\lambda_{k}^{2}(\sigma_{2})\|P_{k}f^{*}\|_{q}\] \[\lesssim\sum_{k\geq 2}d^{-k}q^{k/2}\|P_{k}f^{*}\|_{L^{2}}\] \[\lesssim\sum_{k\geq 2}\left(\sqrt{q}/d\right)^{k}\] \[=\frac{q}{d^{2}}\cdot\frac{1}{1-\sqrt{q}d}\] \[\leq 2qd^{-2}\]

Therefore

\[\left\|\mathbb{K}f^{*}\right\|_{q}\lesssim\frac{4}{d^{2}\lambda_{2}^{2}( \sigma_{2})c_{1}}q\|\mathbb{K}f^{*}\|_{L^{2}}\lesssim q\left\|\mathbb{K}f^{*} \right\|_{L^{2}},\]

since \(\lambda_{2}^{2}(\sigma_{2})=\Omega(d^{-2})\). Thus Assumption 4 holds with \(\ell=1\).

Next, observe that \(\left\|\mathbb{K}f^{*}\right\|_{L^{2}}\lesssim d^{-2}\). We select the test function \(q\) to be \(q(z)=g^{*}(\overline{\eta}^{-1}\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-1}\cdot z)\). We see that

\[q(\overline{\eta}(\mathbb{K}f^{*})(x))=g^{*}\Big{(}\|\mathbb{K}f^{*}\|_{L^{2} }^{-1}(\mathbb{K}f^{*})(x)\Big{)},\]

and thus

\[\left\|f^{*}-q(\overline{\eta}(\mathbb{K}f^{*})(x))\right\|_{L^{2}} =\left\|g^{*}(x^{T}Ax)-g^{*}\Big{(}\|\mathbb{K}f^{*}\|_{L^{2}}^{- 1}(\mathbb{K}f^{*})(x)\Big{)}\right\|_{L^{2}}\] \[\lesssim\left\|x^{T}Ax-\left\|\mathbb{K}f^{*}\right\|_{L^{2}}^{-1 }\mathbb{K}f^{*}\right\|_{L^{2}}\] \[\lesssim\kappa^{1/6}d^{-1/12}\log d,\]where the first inequality follows from Lipschitzness of \(g^{*}\), and the second inequality is Corollary 3. Furthermore since \(\overline{\eta}=\Theta(\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}t^{-\chi})\), we get that \(\overline{\eta}^{-1}\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}=\Theta(\iota^{\chi})\), and thus

\[\sup_{z\in[-1,1]}|q(z)| =\sup_{z\in[-\Theta(\iota^{\chi}),\Theta(\iota^{\chi})]}|g^{*}(z)| =\mathrm{poly}(\iota)\] \[\sup_{z\in[-1,1]}|q^{\prime}(z)| =\overline{\eta}^{-1}\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}\sup_{z\in[- \Theta(\iota^{\chi}),\Theta(\iota^{\chi})]}|(g^{*})^{\prime}(z)|=\mathrm{poly} (\iota)\] \[\sup_{z\in[-1,1]}|q^{\prime\prime}(z)| =\left(\overline{\eta}^{-1}\|\mathbb{K}f^{*}\|_{L^{2}}^{-1} \right)^{2}\sup_{z\in[-\Theta(\iota^{\chi}),\Theta(\iota^{\chi})]}|(g^{*})^{ \prime\prime}(z)|=\mathrm{poly}(\iota)\]

Therefore by Theorem 6 we can bound the population loss as

\[\mathbb{E}_{x}\Big{|}f(x;\hat{\theta})-f^{*}(x)\Big{|}\lesssim\bigg{(}\frac{d^ {4}\,\mathrm{poly}(\iota)}{\min(n,m_{1},m_{2})}+\frac{1}{\sqrt{n}}+\kappa^{1 /3}d^{-1/6}\iota\bigg{)}.\]

#### e.2.1 Proof of Lemma 20

The high level sketch of the proof of Lemma 20 is as follows. Consider a second spherical harmonic \(x^{T}Bx\) satisfying \(\mathbb{E}[(x^{T}Ax)(x^{T}Bx)]=0\) (a simple computation shows that this is equivalent to \(\mathrm{Tr}(AB)=0\)). We appeal to a key result in universality to show that in the large \(d\) limit, the distribution of \(x^{T}Ax\) converges to a standard Gaussian; additionally, \(x^{T}Bx\) converges to an independent mean-zero random variable. As a consequence, we show that

\[\mathbb{E}[q(x^{T}Ax)x^{T}Ax]\approx\mathbb{E}_{z\sim\mathcal{N}(0,1)}[q(z)z]= \mathbb{E}_{z\sim\mathcal{N}(0,1)}[q^{\prime}(z)]=c_{1}.\]

and

\[\mathbb{E}[q(x^{T}Ax)x^{T}Bx]\approx\mathbb{E}[q(x^{T}Ax)]\cdot\mathbb{E}[x^{T} Bx]=0.\]

From this, it immediately follows that \(P_{2}f^{*}\approx c_{1}x^{T}Ax\).

The key universality theorem is the following.

**Definition 7**.: _For two probability measures \(\mu,\nu\), the Wasserstein 1-distance between \(\mu\) and \(\nu\) is defined as_

\[W_{1}(\mu,\nu):=\sup_{\|f\|_{Lip}\leq 1}|\mathbb{E}_{z\sim\mu}[f(z)]-\mathbb{E} _{z\sim\nu}[f(z)]|,\]

_where \(\|f\|_{Lip}:=\sup_{x\neq y}\frac{|f(x)-f(y)|}{\|x-y\|_{2}}\) is the Lipschitz norm of \(f\)._

**Lemma 22** ([52][Theorem 9.20]).: _1 Let \(z\sim\mathcal{N}(0,I_{d})\) be a standard Gaussian vector, and let \(f:\mathbb{R}^{d}\to\mathbb{R}\) satisfy \(\mathbb{E}[f(z)]=0,\mathbb{E}[(f(z))^{2}]=1\). Then_

\[W_{1}(\text{Law}(f(z)),\mathcal{N}(0,1))\lesssim\mathbb{E}\left[\left\|\nabla f (z)\right\|^{4}\right]^{1/4}\mathbb{E}\left[\left\|\nabla^{2}f(z)\right\|_{op }^{4}\right]^{1/4},\]

_where \(W_{1}\) is the Wasserstein 1-distance._

We next apply this lemma to show that the quantities \(x^{T}Ax+x^{T}Bx\) and \(x^{T}Ax+x\cdot u\) are approximately Gaussian, given appropriate operator norm bounds on \(A,B\).

**Lemma 23**.: _Let \(x^{T}Ax\) and \(x^{T}Bx\) be orthogonal spherical harmonics. Then, for constants \(c_{1},c_{2}\) with \(c_{1}^{2}+c_{2}^{2}=1\), we have that the random variable \(Y=c_{1}x^{T}Ax+c_{2}x^{T}Bx\) satisfies_

\[W_{1}(Y,\mathcal{N}(0,1))\lesssim\left\|A\right\|_{op}+\left\|B\right\|_{op}.\]

Proof.: Define the function \(f(z)=c_{1}d\frac{z^{T}Az}{\|z\|^{2}}+c_{2}d\frac{z^{T}Bz}{\|z\|^{2}}\), and let \(x=\frac{z\sqrt{d}}{\|z\|}\). Observe that when \(z\sim\mathcal{N}(0,I)\), we have \(x\sim\text{Unif}(\mathcal{S}^{d-1}(\sqrt{d}))\). Therefore \(f(z)\) is equal in distribution to \(Y\). Define \(f_{1}(z)=d\frac{z^{T}Az}{\|z\|^{2}}\). We compute

\[\nabla f_{1}(z)=2d\Bigg{(}\frac{Az}{\|z\|^{2}}-\frac{z^{T}Az\cdot z}{\|z\|^{4} }\Bigg{)}\]\[\nabla^{2}f_{1}(z)=2d\Bigg{(}\frac{A}{\left\|z\right\|^{2}}-\frac{2Azz^{T}A}{\left\| z\right\|^{4}}-\frac{2zz^{T}A}{\left\|z\right\|^{4}}-2\frac{z^{T}Az}{\left\|z \right\|^{4}}I+4\frac{z^{T}Azzz^{T}}{\left\|z\right\|^{6}}\Bigg{)}.\]

Thus

\[\left\|\nabla f_{1}(z)\right\|\leq 2d\Bigg{(}\frac{\left\|Az\right\|}{\left\|z \right\|^{2}}+\frac{\left|z^{T}Az\right|}{\left\|z\right\|^{3}}\Bigg{)}\leq \frac{\sqrt{d}}{\left\|z\right\|}\cdot\left\|Ax\right\|+\frac{\left|x^{T}Ax \right|}{\left\|z\right\|}.\]

and

\[\left\|\nabla^{2}f_{1}(z)\right\|_{op}\lesssim\frac{d}{\left\|z\right\|^{2}} \left\|A\right\|_{op}.\]

\(\left\|z\right\|^{2}\) is distributed as a chi-squared random variable with \(d\) degrees of freedom, and thus

\[\mathbb{E}\left[\left\|z\right\|^{-2k}\right]=\frac{1}{\prod_{j=1}^{k}(d-2j)}\]

Therefore

\[\mathbb{E}\Big{[}\left\|\nabla^{2}f_{1}(z)\right\|_{op}^{4}\Big{]}^{1/4} \lesssim d\left\|A\right\|_{op}\mathbb{E}\Big{[}\left\|z\right\|^{-8}\Big{]}^{ 1/4}\lesssim\left\|A\right\|_{op}.\]

and, using the fact that \(x\) and \(\left\|z\right\|\) are independent,

\[\mathbb{E}\Big{[}\left\|\nabla f_{1}(z)\right\|^{4}\Big{]}^{1/4}\lesssim\sqrt{ d}\mathbb{E}\Big{[}\left\|z\right\|^{-4}\Big{]}^{1/4}\mathbb{E}\Big{[}\left\|Ax \right\|^{4}\Big{]}^{1/4}+\mathbb{E}\Big{[}\left\|z\right\|^{-4}\Big{]}^{1/4} \mathbb{E}\big{[}(x^{T}Ax)^{4}\Big{]}^{1/4}\lesssim 1.\]

As a consequence, we have

\[\mathbb{E}\Big{[}\left\|\nabla f(z)\right\|^{4}\Big{]}^{1/4} \lesssim 1\quad\text{and}\quad\mathbb{E}\Big{[}\left\|\nabla^{2}f(z)\right\| _{op}^{4}\Big{]}^{1/4}\lesssim\left\|A\right\|_{op}+\left\|B\right\|_{op}.\]

Thus by Lemma 22 we have

\[W_{1}(Y,\mathcal{N}(0,1))=W_{1}(f(z),\mathcal{N}(0,1))\lesssim \left\|A\right\|_{op}+\left\|B\right\|_{op}.\]

**Lemma 24**.: _Let \(x^{T}Ax\) be a spherical harmonic, and \(\left\|u\right\|=1\). Then, for constants \(c_{1},c_{2}\) with \(c_{1}^{2}+c_{2}^{2}=1\), we have that the random variable \(Y=c_{1}x^{T}Ax+c_{2}x^{T}u\) satisfies_

\[W_{1}(Y,\mathcal{N}(0,1))\lesssim\left\|A\right\|_{op}.\]

_where \(W_{1}\) is the 1-Wasserstein distance._

Proof.: Define \(f_{2}(z)=\frac{\sqrt{d}z^{T}u}{\left\|z\right\|}\). We have

\[\nabla f_{2}(z)=\sqrt{d}\Bigg{(}\frac{u}{\left\|z\right\|}-\frac{zz^{T}u}{ \left\|z\right\|^{3}}\Bigg{)}.\]

and

\[\nabla^{2}f_{2}(z)=\sqrt{d}\Bigg{(}-\frac{uz^{T}+zu^{T}}{\left\|z \right\|^{3}}-\frac{z^{T}u}{\left\|z\right\|^{3}}I+3\frac{z^{T}uzz^{T}}{\left\| z\right\|^{5}}\Bigg{)}.\]

Thus

\[\left\|\nabla f_{2}(z)\right\|\lesssim\frac{\sqrt{d}}{\left\|z \right\|}\quad\text{and}\quad\left\|\nabla^{2}f_{2}(z)\right\|_{op}\lesssim \frac{\sqrt{d}}{\left\|z\right\|^{2}},\]

so

\[\mathbb{E}\Big{[}\left\|\nabla f_{1}(z)\right\|^{4}\Big{]}^{1/4} \lesssim 1\quad\text{and}\quad\mathbb{E}\Big{[}\left\|\nabla f_{1}(z) \right\|^{4}\Big{]}^{1/4}\lesssim\frac{1}{\sqrt{d}}.\]

We finish using the same argument as above.

Lemma 23 implies that, when \(\left\|A\right\|_{op},\left\|B\right\|_{op}\) are small, \((x^{T}Ax,x^{T}Bx)\) is close in distribution to the standard Gaussian in 2-dimensions. As a consequence, \(\mathbb{E}[q(x^{T}Ax)x^{T}Bx]\approx\mathbb{E}[q(z_{1})]\,\mathbb{E}[z_{2}]=0\), where \(z_{1},z_{2}\) are i.i.d Gaussians. This intuition is made formal in the following lemma.

**Lemma 25**.: _Let \(x^{T}Ax,x^{T}Bx\) be two orthogonal spherical harmonics. Then_

\[\left|\mathbb{E}\big{[}q(x^{T}Ax)x^{T}Bx\big{]}\right|\leq L\Big{(}\sqrt{ \left\|A\right\|_{op}\log d}+\left\|B\right\|_{op}\log d\Big{)}\]

Proof.: Define the function \(F(t)=\int_{0}^{t}q(s)ds\). Then \(F^{\prime}(t)=q(t)\), so by a Taylor expansion we get that

\[\left|F(x+\epsilon y)-F(x)-\epsilon yq(x)\right|\leq\epsilon^{2}y^{2}L.\]

Therefore

\[\left|\mathbb{E}\big{[}q(x^{T}Ax)x^{T}Bx\big{]}\right|\leq\epsilon L\mathbb{E} \big{[}(x^{T}Bx)^{2}\big{]}+\epsilon^{-1}\big{|}\mathbb{E}\big{[}F(x^{T}Ax+ \epsilon x^{T}Bx)\big{]}-\mathbb{E}\big{[}F(x^{T}Ax)\big{]}\right|.\]

Pick truncation radius \(R\), and define the function \(\overline{F}(z)=F(\max(-R,\min(R,z))\). \(\overline{F}\) has Lipschitz constant \(\sup_{z\in[-R,R]}\left|q(z)\right|\), and thus since \(W_{1}\big{(}x^{T}Ax,X(0,1)\big{)}\lesssim\left\|A\right\|_{op}\), we have

\[\left|\mathbb{E}\overline{F}(x^{T}Ax)-\mathbb{E}_{z\sim\mathcal{N}(0,1)} \overline{F}(z)\right|\lesssim\sup_{z\in[-R,R]}\left|q(z)\right|\cdot\left\|A \right\|_{op}.\]

Next, we have

Likewise,

\[\left|\mathbb{E}_{z}\big{[}\overline{F}(z)-F(z)\big{]}\right|\leq\mathbb{P}( \left|z\right|>R)\cdot\mathbb{E}\big{[}F(z)^{2}\big{]}^{1/2}.\]

Since \(q\) is \(L\)-Lipschitz, we can bound \(\left|F(z)\right|\leq\left|z\right||q(0)\right|+\frac{1}{2}L\left|z\right|^{2}\), and thus

\[\mathbb{E}\big{[}F(z)^{2}\big{]}\lesssim L^{2}\quad\text{and}\quad\mathbb{E} \big{[}F(x^{T}Ax)^{2}\big{]}\lesssim L^{2}.\]

The standard Gaussian tail bound yields \(\mathbb{P}(\left|z\right|>R)\lesssim\exp\bigl{(}-C_{1}R^{2}\bigr{)}\) for appropriate constant \(C_{1}\), and polynomial concentration yields \(\mathbb{P}\big{(}\left|x^{T}Ax\right|>R\big{)}\lesssim\exp(-C_{2}R)\) for appropriate constant \(C_{2}\). Thus choosing \(R=C_{3}\log d\) for appropriate constant \(C_{3}\), we get that

\[\left|\mathbb{E}\big{[}\overline{F}(x^{T}Ax)-F(x^{T}Ax)\big{]}\right|+\left| \mathbb{E}_{z}\big{[}\overline{F}(z)-F(z)\big{]}\right|\lesssim\frac{L}{d}.\]

Altogether, since \(\left|q(z)\right|\leq\left|q(0)\right|+L\left|z\right|\), we get that

\[\left|\mathbb{E}F(x^{T}Ax)-\mathbb{E}_{z\sim\mathcal{N}(0,1)}F(z)\right| \lesssim\left\|A\right\|_{op}L\log d.\]

By an identical calculation, we have that for \(\epsilon<1\),

Altogether, we get that

\[\left|\mathbb{E}\big{[}F(x^{T}Ax+\epsilon x^{T}Bx)\big{]}-\mathbb{ E}\big{[}F(x^{T}Ax)\big{]}\right|\] \[\qquad\leq\Big{(}\left\|A\right\|_{op}+\epsilon\left\|B\right\|_{ op}\Big{)}\cdot L\log d+\left|\mathbb{E}_{z\sim\mathcal{N}(0,1)}F(z)- \mathbb{E}_{z\sim\mathcal{N}(0,1)}F(z\sqrt{1+\epsilon^{2}})\right|.\]

Via a simple calculation, one sees that

\[\left|F(z\sqrt{1+\epsilon^{2}})-F(z)\right|\leq\left|q(0)\right||z|\Big{(} \sqrt{1+\epsilon^{2}}-1\Big{)}+\frac{L}{2}z^{2}\epsilon^{2}\lesssim L|z| \epsilon^{2}+Lz^{2}\epsilon^{2}.\]

Therefore

\[\left|\mathbb{E}\big{[}F(x^{T}Ax+\epsilon x^{T}Bx)\big{]}-\mathbb{E}\big{[}F(x ^{T}Ax)\big{]}\right|\lesssim L\Big{(}\Big{(}\left\|A\right\|_{op}+\epsilon \left\|B\right\|_{op}\Big{)}\log d+\epsilon^{2}\Big{)},\]

so

\[\left|\mathbb{E}\big{[}q(x^{T}Ax)x^{T}Bx\big{]}\right|\leq L\Big{(}\epsilon^{- 1}\left\|A\right\|_{op}\log d+\epsilon+\left\|B\right\|_{op}\log d\Big{)}.\]

Setting \(\epsilon=\sqrt{\left\|A\right\|_{op}\log d}\) yields

\[\left|\mathbb{E}\big{[}q(x^{T}Ax)x^{T}Bx\big{]}\right|\lesssim L\Big{(}\sqrt{ \left\|A\right\|_{op}\log d}+\left\|B\right\|_{op}\log d\Big{)},\]

as desired.

Similarly, we use the consequence of Lemma 24 that \((x^{T}Ax,u^{T}x)\) is close in distribution to a 2d standard Gaussian, and show that \(\mathbb{E}[q(x^{T}Ax)\big{(}(u^{T}x)^{2}-1)]\approx\mathbb{E}[q(z_{1})\big{(}z_{2 }^{2}-1\big{)}]=0\).

**Lemma 26**.: \[\big{|}\mathbb{E}\big{[}q(x^{T}Ax)(u^{T}x)^{2}\big{]}-\mathbb{E}_{ z}[q(z)]\big{|}\lesssim L\|A\|_{op}^{1/3}\log^{2/3}d.\]

Proof.: Let \(G(t)=\int_{0}^{t}F(s)ds\). Then \(G^{\prime}(t)=F(t),G^{\prime\prime}(t)=q(t)\), so a Taylor expansion yields

\[G(x+\epsilon y) =G(x)+\epsilon yF(x)+\epsilon^{2}y^{2}q(x)+O(\epsilon^{3}|y|^{3}L)\] \[G(x-\epsilon y) =G(x)-\epsilon yF(x)+\epsilon^{2}y^{2}q(x)+O(\epsilon^{3}|y|^{3}L).\]

Thus

\[\epsilon^{2}y^{2}q(x)=\frac{1}{2}(G(x+\epsilon y)+G(x-\epsilon y) -2G(x))+O(\epsilon^{3}|y|^{3}L).\]

Therefore

\[\big{|}\mathbb{E}\big{[}q(x^{T}Ax)(u^{T}x)^{2}\big{]}\big{|} \lesssim\epsilon L+\epsilon^{-2}\big{|}\mathbb{E}\big{[}G(x^{T}Ax+\epsilon u^ {T}x)+G(x^{T}Ax-\epsilon u^{T}x)-2G(x^{T}Ax)\big{]}\big{|}.\]

For truncation radius \(R\), define \(\overline{G}(z)=G(\max(-R,\min(R,z)))\). We get that \(G\) has Lipschitz constant \(\sup_{|z|\leq R}|F(z)|\lesssim LR^{2}\). Therefore

\[\big{|}\mathbb{E}\overline{G}(x^{T}Ax)-\mathbb{E}_{z\sim\mathcal{ N}(0,1)}\overline{G}(z)\big{|}\lesssim LR^{2}|A|_{op},\]

and by a similar argument in the previous lemma, setting \(R=C_{3}\log d\) yields

\[\big{|}\mathbb{E}\big{[}\overline{G}(x^{T}Ax)-G(x^{T}Ax)\big{]} \big{|}+\big{|}\mathbb{E}_{z}\overline{G}(z)-G(z)\big{|}\lesssim\frac{L}{d}.\]

Altogether,

\[\big{|}\mathbb{E}G(x^{T}Ax)-\mathbb{E}_{z}G(z)\big{|}\lesssim\|A \|_{op}L\log^{2}d.\]

By an identical calculation,

\[\Big{|}\mathbb{E}G(x^{T}Ax\pm\epsilon u^{T}x)-\mathbb{E}_{z}G(z \sqrt{1+\epsilon^{2}})\Big{|}\lesssim\|A\|_{op}\cdot L\log^{2}d.\]

Additionally, letting \(z,w\) be independent standard Gaussians,

\[\epsilon^{-2}\mathbb{E}_{z}\Big{[}2G(z\sqrt{1+\epsilon^{2}}-2G(z )\Big{]} =\epsilon^{-2}\mathbb{E}_{z,w}[G(z+\epsilon w)+G(z-\epsilon w)-2G( z)]\] \[=\mathbb{E}_{z,w}\big{[}q(z)w^{2}\big{]}+O(\epsilon L)\] \[=\mathbb{E}_{z}[q(z)]+O(\epsilon L).\]

Altogether,

\[\big{|}\mathbb{E}\big{[}q(x^{T}Ax)(u^{T}x)^{2}\big{]}-\mathbb{E}_ {z}[q(z)]\big{|} \lesssim\epsilon L+\epsilon^{-2}\|A\|_{op}\cdot L\log^{2}d\] \[\lesssim L\|A\|_{op}^{1/3}\log^{2/3}d,\]

where we set \(\epsilon=\|A\|_{op}^{1/3}\log^{2/3}d\). 

Lemma 25 shows that when \(\|B\|_{op}\ll 1\), \(\mathbb{E}[q(x^{T}Ax)x^{T}Bx]\approx 0\). However, we need to show this is true for all spherical harmonics, even those with \(\|B\|_{op}=\Theta(1)\). To accomplish this, we decompose \(B\) into the sum of a low rank component and small operator norm component. We use Lemma 25 to bound the small operator norm component, and Lemma 26 to bound the low rank component. Optimizing over the rank threshold yields the following desired result:

**Lemma 27**.: _Let \(A,B\) be orthogonal spherical harmonics. Then_

\[\big{|}\mathbb{E}\big{[}q(x^{T}Ax)x^{T}Bx\big{]}\big{|}\leq L\|A \|_{op}^{1/6}\log d.\]Proof.: Let \(\tau>\left\|A\right\|_{op}\) be a threshold to be determined later. Decompose \(B\) as follows:

\[B=\sum_{i=1}^{d}\lambda_{i}u_{i}u_{i}^{T}=\sum_{\left|\lambda_{i} \right|>\tau}\lambda_{i}\bigg{(}u_{i}u_{i}^{T}-\frac{1}{d}I\bigg{)}-\frac{1}{ \left\|A\right\|_{F}^{2}}\sum_{\left|\lambda_{i}\right|>\tau}u_{i}^{T}Au_{i} \cdot A+\tilde{B},\]

where

\[\tilde{B}=\sum_{\left|\lambda_{i}\right|\leq\tau}\lambda_{i}u_{i}u_{i}^{T}+I \cdot\frac{1}{d}\sum_{\left|\lambda_{i}\right|>\tau}\lambda_{i}+\frac{1}{ \left\|A\right\|_{F}^{2}}\sum_{\left|\lambda_{i}\right|>\tau}\lambda_{i}u_{i}^ {T}Au_{i}\cdot A.\]

By construction, we have,

\[\mathrm{Tr}\Big{(}\tilde{B}\Big{)}=\sum_{\left|\lambda_{i}\right|\leq\tau} \lambda_{i}+\sum_{\left|\lambda_{i}\right|>\tau}\lambda_{i}=\sum_{i\in[d]} \lambda_{i}=0\]

and

\[\left\langle\tilde{B},A\right\rangle=\sum_{\left|\lambda_{i}\right|\leq\tau} \lambda_{i}u_{i}^{T}Au_{i}+\sum_{\left|\lambda_{i}\right|>\tau}\lambda_{i}u_{i }^{T}Au_{i}=\left\langle A,B\right\rangle=0.\]

Therefore by Lemma 25,

\[\left|\mathbb{E}\Big{[}q(x^{T}Ax)x^{T}\tilde{B}x\Big{]}\right|\lesssim L\sqrt{ \left\|A\right\|_{op}\log d}\Big{\|}\tilde{B}\Big{\|}_{F}+L\Big{\|}\tilde{B} \Big{\|}_{op}\log d.\]

There are at most \(O(\tau^{-2})\) indices \(i\) satisfying \(\left|\lambda_{i}\right|>\tau\), and thus

\[\sum_{\left|\lambda_{i}\right|>\tau}\left|\lambda_{i}\right|\lesssim\sqrt{\tau ^{-2}\cdot\sum_{\left|\lambda_{i}\right|>\tau}\left|\lambda_{i}\right|^{2}} \leq\tau^{-1}.\]

We thus compute that

\[\left\|\tilde{B}\right\|_{F}^{2} \lesssim\sum_{\left|\lambda_{i}\right|\leq\tau}\lambda_{i}^{2}+ \frac{1}{d}\Bigg{(}\sum_{\lambda_{i}>\tau}\lambda_{i}\Bigg{)}^{2}+\left|\sum_ {\left|\lambda_{i}\right|>\tau}\lambda_{i}u_{i}^{T}Au_{i}\right|^{2}\] \[\lesssim\sum_{\left|\lambda_{i}\right|\leq\tau}\lambda_{i}^{2}+ \left\|A\right\|_{op}^{2}\left|\sum_{\left|\lambda_{i}\right|>\tau}\lambda_{i} \right|^{2}\] \[\lesssim 1+\tau^{-2}\left\|A\right\|_{op}^{2}\] \[\lesssim 1.\]

and

\[\left\|\tilde{B}\right\|_{op} \leq\tau+\left(\frac{1}{d}+\left\|A\right\|_{op}\right)\left| \sum_{\left|\lambda_{i}\right|>\tau}\lambda_{i}u_{i}^{T}Au_{i}\right|\] \[\lesssim\tau+\left\|A\right\|_{op}^{2}r^{-1}.\]

Next, since \(\left|\mathbb{E}\big{[}q(x^{T}Ax)\big{]}-\mathbb{E}_{z}[q(x)]\right|\lesssim L \left\|A\right\|_{op}\), Lemma 26 yields

\[\left|\mathbb{E}\Bigg{[}q(x^{T}Ax)\cdot\sum_{\left|\lambda_{i} \right|>\tau}\lambda_{i}\bigg{(}u_{i}u_{i}^{T}-\frac{1}{d}I\bigg{)}\Bigg{]} \leq\sum_{\left|\lambda_{i}\right|>\tau}\left|\lambda_{i}\right| \left|\mathbb{E}\big{[}q(x^{T}Ax)(u_{i}^{T}x)^{2}\big{]}-\mathbb{E}\big{[}q(x^{ T}Ax)\big{]}\right|\] \[\lesssim\sum_{\left|\lambda_{i}\right|>\tau}\left|\lambda_{i} \right|\cdot L\left\|A\right\|_{op}^{1/3}\log^{2/3}d\] \[\lesssim L\tau^{-1}\|A\|_{op}^{1/3}\log^{2/3}d.\]Finally,

\[\left|\mathbb{E}\left[q(x^{T}Ax)\cdot\frac{1}{\|A\|_{F}^{2}}\sum_{|\lambda_{i}|> \tau}\lambda_{i}u_{i}^{T}Au_{i}\cdot x^{T}Ax\right]\right|\lesssim L\left|\sum_ {|\lambda_{i}|>\tau}\lambda_{i}u_{i}^{T}Au_{i}\right|\lesssim L\|A\|_{op}\tau^{ -1}.\]

Altogether,

\[\left|\mathbb{E}\big{[}q(x^{T}Ax)x^{T}Bx\big{]}\right|\lesssim L\log d\Big{(}\| A\|_{op}^{1/2}+\tau+\|A\|_{op}^{2}\tau^{-1}+\tau^{-1}\|A\|_{op}^{1/3}+\|A\|_{op} \tau^{-1}\Big{)}\lesssim L\|A\|_{op}^{1/6}\log d.\]

where we set \(\tau=\|A\|_{op}^{1/6}\). 

Finally, we use the fact that \(x^{T}Ax\) is approximately Gaussian to show that \(\mathbb{E}[q(x^{T}Ax)x^{T}Ax]\approx c_{1}\).

**Lemma 28**.: _Let \(x^{T}Ax\) be a spherical harmonic. Then_

\[\left|\mathbb{E}[q(x^{T}Ax)x^{T}Ax]-c_{1}\right|\lesssim L\|A\|_{op}\log d\]

Proof.: Define \(H(z)=q(z)z\). For truncation radius \(R\), define \(\overline{H}(z)=H(\max(-R,\min(R,z)))\). For \(x,y\in[-R,R]\) we can bound

\[|H(x)-H(y)| =|q(x)x-q(y)y|\] \[\leq|x||q(x)-q(y)|+|q(y)|\|x-y\|\] \[\lesssim RL\|x-y\|.\]

Thus \(\overline{H}\) has Lipschitz constant \(O(RL)\). Since \(W_{1}(x^{T}Ax,\mathcal{N}(0,1))\lesssim\|A\|_{op}\), we have

\[\left|\mathbb{E}\,\overline{H}(x^{T}Ax)-\mathbb{E}_{z\sim\mathcal{N}(0,1)} \,\overline{H}(z)\right|\lesssim RL\|A\|_{op}.\]

Furthermore, choosing \(R=C\log d\) for appropriate constant \(C\), we have that

\[\left|\mathbb{E}_{x}[\overline{H}(x^{T}Ax)-H(x^{T}Ax)]\right|\leq\mathbb{P}(|x ^{T}Ax|>R)\cdot\mathbb{E}[H(x^{T}Ax)^{2}]^{1/2}\lesssim\frac{L}{d}\]

\[\left|\mathbb{E}_{z}[\overline{H}(z)-H(z)]\right|\leq\mathbb{P}(|z|>R)\cdot \mathbb{E}[H(z)^{2}]^{1/2}\lesssim\frac{L}{d}.\]

Altogether,

\[\left|\mathbb{E}_{x}[H(x^{T}Ax)]-\mathbb{E}_{z\sim\mathcal{N}(0,1)}[H(z)] \right|\lesssim L\|A\|_{op}\log d.\]

Substituting \(H(x^{T}Ax)=q(x^{T}Ax)x^{T}Ax\) and \(\mathbb{E}_{z\sim\mathcal{N}(0,1)}[H(z)]=\mathbb{E}_{z}[q(z)z]=\mathbb{E}_{z}[ q^{\prime}(z)]=c_{1}\) yields the desired bound. 

We are now set to prove Lemma 20.

Proof of Lemma 20.: Let \(P_{2}f^{*}=x^{T}T_{2}x\). Then \(\left\|P_{2}f^{*}-c_{1}x^{T}Ax\right\|_{L^{2}}\lesssim\left\|T_{2}-c_{1}A\right\| _{F}\).

Write \(T_{2}=\alpha A+A^{\perp}\), where \(\langle A^{\perp},A\rangle=0\). We first have that

\[\mathbb{E}\big{[}f^{*}(x)x^{T}A^{\perp}x\big{]}=\mathbb{E}\big{[}x^{T}T_{2}x \cdot x^{T}A^{\perp}x\big{]}=2\chi_{2}\langle T_{2},A^{\perp}\rangle=2\chi_{2} \big{\|}A^{\perp}\big{\|}_{F}^{2}.\]

Also, by Lemma 27, we have

\[\mathbb{E}\big{[}f^{*}(x)x^{T}A^{\perp}x\big{]}=\mathbb{E}\big{[}q(x^{T}Ax)x^{ T}A^{\perp}x\big{]}\lesssim\big{\|}A^{\perp}\big{\|}_{F}\cdot L\|A\|_{op}^{1/6} \log d.\]

Therefore \(\big{\|}A^{\perp}\big{\|}_{F}\lesssim L\|A\|_{op}^{1/6}\log d\). Next, see that

\[\mathbb{E}\big{[}f^{*}(x)x^{T}Ax\big{]}=2\alpha\chi_{2}\|A\|_{F}^{2}=\alpha,\]

so by Lemma 28 we have \(|c_{1}-\alpha|\lesssim L\|A\|_{op}\log d\). Altogether,

\[\left\|T_{2}-c_{1}A\right\|_{F}\leq|\alpha-c_{1}|\|A\|_{F}+\big{\|}A^{\perp} \big{\|}_{F}\lesssim L\|A\|_{op}^{1/6}\log d=L\kappa^{1/6}d^{-1/12}\log d.\]

### Improved Error Floor for Polynomials

When \(q\) is a polynomial of degree \(p=O(1)\), we can improve the exponent of \(d\) in the error floor.

**Theorem 7**.: _Assume that \(q\) is a degree \(p\) polynomial, where \(p=O(1)\). Under Assumption 5, Assumption 6, and Assumption 7, with high probability Algorithm 1 satisfies the population loss bound_

\[\mathbb{E}_{x}\bigg{[}\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)}^{2} \bigg{]}\lesssim\tilde{O}\bigg{(}\frac{d^{4}}{\min(n,m_{1},m_{2})}+\frac{1}{ \sqrt{n}}+\frac{\kappa^{2}}{d}\bigg{)}\]

The high level strategy to prove Theorem 7 is similar to that for Theorem 3, as we aim to show \(\mathbb{K}f^{*}\) is approximately proportional to \(x^{T}Ax\). Rather to passing to universality as in Lemma 20, however, we use an algebraic argument to estimate \(P_{2}f^{*}\).

The key algebraic lemma is the following:

**Lemma 29**.: _Let \(\operatorname{Tr}(A)=0\). Then_

\[A^{\tilde{\otimes}k}(I^{\otimes k-1})=\sum_{s=1}^{k}d_{k,s}A^{s} \cdot A^{\tilde{\otimes}k-s}(I^{\otimes(k-s)}),\]

_where the constants \(d_{k,s}\) are defined by_

\[d_{k,s}:=2^{s-1}\frac{(2k-2s-1)!!(k-1)!}{(2k-1)!!(k-s)!}\]

_and we denote \((-1)!!=1\)._

Proof.: The proof proceeds via a counting argument. We first have that

\[A^{\tilde{\otimes}k}(I^{\otimes k-1})=\sum_{(\alpha_{1},\ldots, \alpha_{k-1})\in[d]^{k-1}}\Big{(}A^{\tilde{\otimes}k}\Big{)}_{\alpha_{1}, \alpha_{1},\ldots,\alpha_{k-1},\alpha_{k-1},i,j}.\]

Consider any permutation \(\sigma\in S_{2k}\). We can map this permutation to the graph \(G(\sigma)\) on \(k\) vertices and \(k-1\) edges as follows: for \(m\in[k-1]\), if \(\sigma^{-1}(2m-1)\in\{2a-1,2a\}\) and \(\sigma^{-1}(2m)\in\{2b-1,2b\}\), then we draw an edge \(e(m)\) between \(a\) and \(b\). In the resulting graph \(G(\sigma)\) each node has degree at most \(2\), and hence there are either two vertices with degree \(1\) or one vertex with degree \(0\). For a vertex \(v\), let \(e_{1}(v),e_{2}(v)\in[k-1]\) be the two edges \(v\) is incident to if \(v\) has degree \(2\), and otherwise \(e_{1}(v)\) be the only edge \(v\) is incident to. For shorthand, let \((i_{1},\ldots,i_{2k})=(\alpha_{1},\alpha_{1},\ldots,\alpha_{k-1},\alpha_{k-1},i,j)\).

If there are two vertices \((u_{1},u_{2})\) with degree \(1\), we have that

\[\big{(}A^{\otimes k}\big{)}_{i_{\sigma(1)},i_{\sigma(2)},\cdots i _{\sigma(2k)}}=A_{i,\alpha_{e_{1}(u_{1})}}A_{j,\alpha_{e_{2}(u_{2})}}\prod_{v \neq u_{1},u_{2}}A_{\alpha_{e_{1}(v)},\alpha_{e_{2}(v)}}\]

Let \(u_{1},u_{2}\) be connected to eachother via a path of \(s\) total vertices, and let \(\mathcal{P}\) be the ordered set of vertices in this path. Via the matrix multiplication formula, one sees that

\[\sum_{(\alpha_{1},\ldots,\alpha_{k-1})\in[d]^{k-1}}\big{(}A^{ \otimes k}\big{)}_{i_{\sigma(1)},i_{\sigma(2)},\cdots i_{\sigma(2k)}}=(A^{s}) _{i,j}\sum\prod_{v\notin\mathcal{P}}A_{\alpha_{e_{1}(v)},\alpha_{e_{2}(v)}},\]

where the sum is over the \(k-s\)\(\alpha\)'s that are still remaining in \(\{e_{i}(v):v\notin\mathcal{P}\}\)

Likewise, if there is one vertex \(u_{1}\) with degree \(0\), we have

\[\big{(}A^{\otimes k}\big{)}_{i_{\sigma(1)},i_{\sigma(2)},\cdots i _{\sigma(2k)}}=A_{i,j}\prod_{v\neq u_{1}}A_{\alpha_{e_{1}(v)},\alpha_{e_{2}(v )}}.\]

and thus, since \(\mathcal{P}=\{u_{1}\}\)

\[\sum_{(\alpha_{1},\ldots,\alpha_{k-1})\in[d]^{k-1}}\big{(}A^{ \otimes k}\big{)}_{i_{\sigma(1)},i_{\sigma(2)},\cdots i_{\sigma(2k)}}=A_{i,j} \sum_{(\alpha_{1},\ldots,\alpha_{k-1})}\prod_{v\notin\mathcal{P}}A_{\alpha_{e _{1}(v)},\alpha_{e_{2}(v)}}.\]Altogether, we have that

\[A^{\tilde{\otimes}k}(I^{\otimes k-1})=\frac{1}{(2k)!}\sum_{\sigma\in S_{2k}}A^{s} \sum_{v\notin\mathcal{P}}A_{\alpha_{e_{1}(v)},\alpha_{e_{2}(v)}}\]

where \(s,\mathcal{P}\) are defined based on the graph \(\mathcal{G}(\sigma)\). Consider a graph with fixed path \(\mathcal{P}\), and let \(S_{\mathcal{P}}\) be the set of permutations which give rise to the path \(\mathcal{P}\). We have that

\[A^{\tilde{\otimes}k}(I^{\otimes k-1})=\frac{1}{(2k)!}\sum_{\mathcal{P}}A^{s} \sum_{\sigma\in S_{\mathcal{P}}}\sum_{v\notin\mathcal{P}}A_{\alpha_{e_{1}(v)}, \alpha_{e_{2}(v)}}.\]

There are \((k-1)\cdots(k-s+1)\) choices for the \(k\) edges to use in the path, and at each vertex \(v\) there are two choices for which edge should correspond to \(2v\) or \(2v+1\). Additionally, there are \(2\) ways to orient each edge. Furthermore, there are \(\frac{k!}{(k-s)!}\) ways to choose the ordering of the path. Altogether, there are \(2^{2s-1}\frac{(k-1)!}{(k-s)!}\frac{k!}{(k-s)!}\) ways to construct a path of length \(s\). We can thus write

\[A^{\tilde{\otimes}k}(I^{\otimes k-1})=\frac{1}{(2k)!}\sum_{s}A^{s}2^{2s-1} \frac{(k-1)!}{(k-s)!}\frac{k!}{(k-s)!}\sum\prod_{v\notin\mathcal{P}}A_{\alpha_ {e_{1}(v)},\alpha_{e_{2}(v)}},\]

where this latter sum is over all permutations where the mapping corresponding to vertices not on the path have not been decided, along with the sum over the unused \(\alpha\)'s. Reindexing, this latter sum is (letting \((i_{1},\ldots,i_{2k-2s})=(\alpha_{1},\alpha_{1},\ldots,\alpha_{k-s},\alpha_{k- s})\))

\[\sum_{\sigma\in S_{2k-2s}}\sum_{(\alpha_{1},\ldots,\alpha_{k-s})\in[d]^{k-s}} \prod A_{i_{\sigma(2j-1)},i_{\sigma(2j)}}=(2k-2s)!A^{\tilde{\otimes}k-s}(I^{ \otimes k-s})\]

Altogether, we obtain

\[A^{\tilde{\otimes}k}(I^{\otimes k-1}) =\sum_{s\geq 1}\frac{(2k-2s)!}{(2k)!}\frac{(k-1)!}{(k-s)!} \frac{k!}{(k-s)!}2^{2s-1}\cdot A^{s}\cdot A^{\tilde{\otimes}k-s}(I^{\otimes k- s})\] \[=\sum_{s\geq 1}\frac{k!}{(2k)!}\frac{(2k-2s)!}{(k-s)!}\frac{(k-1)! }{(k-s)!}\cdot A^{s}\cdot A^{\tilde{\otimes}k-s}(I^{\otimes k-s})\] \[=\sum_{s\geq 1}\frac{(2k-2s-1)!!2^{s}}{(2k-1)!!2^{s}}\frac{(k-1)! }{(k-s)!}2^{2s-1}\cdot A^{s}\cdot A^{\tilde{\otimes}k-s}(I^{\otimes k-s})\] \[=\sum_{s\geq 1}d_{k,s}\cdot A^{s}\cdot A^{\tilde{\otimes}k-s}(I^{ \otimes k-s}),\]

as desired. 

**Definition 8**.: _Define the operator \(\mathcal{T}:\mathbb{R}^{d\times d}\rightarrow\mathbb{R}^{d\times d}\) by \(\mathcal{T}(M)=M-\mathrm{Tr}(M)\cdot\frac{I}{d}\)._

**Lemma 30**.: _Let \(P_{2}f^{*}(x)=x^{T}T_{2}x\). Then \(\left\|T_{2}-\mathbb{E}_{x}\big{[}(g^{*})^{\prime}(x^{T}Ax)\big{]}\cdot A \right\|_{F}\lesssim\frac{\kappa}{\sqrt{d}}\)._

Proof.: Throughout, we treat \(p=O(1)\) and thus functions of \(p\) independent of \(d\) as \(O(1)\) quantities. Let \(q\) be of the form \(g^{*}(z)=\sum_{k=0}^{p}\alpha_{k}z^{k}\). We then have

\[f^{*}(x)=\sum_{k=0}^{p}\alpha_{k}A^{\tilde{\otimes}k}(x^{\otimes 2k})\]

Therefore \(P_{2}f^{*}(x)=x^{T}T_{2}x\), where

\[T_{2}:=\sum_{k=0}^{p}\alpha_{k}(2k-1)!!k\frac{\chi_{k+1}}{\chi_{2}}\mathcal{T} \Big{(}A^{\tilde{\otimes}k}(I^{\otimes k-1})\Big{)}.\]

Applying Lemma 29, one has

\[T_{2}=\sum_{s=0}^{p}\mathcal{T}(A^{s})\cdot\sum_{k=s}^{p}\alpha_{k}(2k-1)!!k \frac{\chi_{k+1}}{\chi_{2}}d_{k,s}A^{\tilde{\otimes}k-s}(I^{\otimes(k-s)}).\]Define

\[\beta_{s}=\sum_{k=s}^{p}\alpha_{k}(2k-1)!!k\frac{\chi_{k+1}}{\chi_{2}}d_{k,s}A^{ \tilde{\otimes}k-s}(I^{\otimes(k-s)})\]

We first see that

\[\beta_{1}=\sum_{k=1}^{p}(2k-3)!!\cdot k\alpha_{k}\frac{\chi_{k+1}}{\chi_{2}}A^{ \tilde{\otimes}k-1}(I^{\otimes(k-1)})\]

Next, see that

\[\frac{\chi_{k+1}}{\chi_{2}}=\frac{d(d+2)}{(d+2k)(d+2k-2)}\chi_{k-1}=\chi_{k-1}+ O(1/d).\]

Thus

\[\beta_{1} =\sum_{k=1}^{p}(2k-3)!!\cdot k\alpha_{k}\chi_{k-1}A^{\tilde{ \otimes}k-1}(I^{\otimes(k-1)})+O(1/d)\cdot\sum_{k=1}^{p}(2k-3)!!\cdot k|\alpha _{k}|A^{\tilde{\otimes}k-1}(I^{\otimes(k-1)})\] \[=\sum_{k=1}^{p}k\alpha_{k}A^{\otimes k-1}\big{(}\mathbb{E}[x^{ \otimes 2k-2}]\big{)}+O(1/d)\] \[=\mathbb{E}_{x}\big{[}(g^{*})^{\prime}(x^{T}Ax)\big{]}+O(1/d).\]

since

\[\Big{|}A^{\tilde{\otimes}k}(I^{\otimes k})\Big{|}\lesssim\mathbb{E}_{x}\big{[} (x^{T}Ax)^{k}\big{]}\lesssim\mathbb{E}_{x}\big{[}(x^{T}Ax)^{2}\big{]}^{k/2}=O(1)\]

where \(\mathrm{Tr}(A)=0\) implies \(\mathbb{E}_{x}\big{[}(x^{T}Ax)^{2}\big{]}=O(1)\) and we invoke spherical hypercontractivity (Lemma 35). Similarly, \(|\beta_{s}|=O(1)\), and thus

\[\big{\|}T_{2}-\mathbb{E}_{x}\big{[}(g^{*})^{\prime}(x^{T}Ax)\big{]}\cdot A \big{\|}_{F}\lesssim\frac{1}{d}+\sum_{s=2}^{p}\|\mathcal{T}(A^{s})\|_{F} \lesssim\frac{\kappa}{\sqrt{d}},\]

where we use the inequality

\[\|\mathcal{T}(X)\|_{F}\leq\|X\|_{F}+|\mathrm{Tr}(X)|\cdot\frac{1}{\sqrt{d}} \leq 2\|X\|_{F},\]

along with

\[\|A^{s}\|_{F}\leq\big{\|}A^{2}\big{\|}_{F}\leq\frac{\kappa}{\sqrt{d}}\]

for \(s\geq 2\). 

**Lemma 31**.: _Let \(c_{1}=\mathbb{E}_{x}\big{[}(g^{*})^{\prime}(x^{T}Ax)\big{]}\). Then \(\big{\|}\mathbb{K}f^{*}-\lambda_{2}^{2}(\sigma)c_{1}x^{T}Ax\big{\|}_{L^{2}} \lesssim\kappa d^{-5/2}\)_

Proof.: Since \(\mathbb{E}[f^{*}(x)]=0\), \(P_{0}f^{*}=0\). Next, since \(f^{*}\) is an even function, \(P_{k}f^{*}=0\) for \(k\) odd. Thus

\[\big{\|}\mathbb{K}f^{*}-\lambda_{2}^{2}(\sigma)P_{2}f^{*}\big{\|}_{L^{2}} \lesssim d^{-4}.\]

Additionally, by Lemma 30 we have that

\[\big{\|}P_{2}f^{*}-c_{1}x^{T}Ax\big{\|}_{L^{2}}\lesssim\|T_{2}-c_{1}\cdot A \|_{F}\lesssim\frac{\kappa}{\sqrt{d}}.\]

Since \(\lambda_{2}^{2}(\sigma)=\Theta(d^{-2})\), we have

\[\big{\|}\mathbb{K}f^{*}-\lambda_{2}^{2}(\sigma)c_{1}x^{T}Ax\big{\|}_{L^{2}} \lesssim\kappa d^{-5/2}.\]

**Corollary 4**.: _Assume \(\kappa=o(\sqrt{d})\). Then_

\[\Big{\|}x^{T}Ax-\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}\mathbb{K}f^{*} \Big{\|}_{L^{2}}\lesssim\kappa/\sqrt{d}\]Proof.: \[\left\|x^{T}Ax-\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}\mathbb{K}f^{*}\right\| _{L^{2}}\] \[=\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}\big{\|}x^{T}Ax\|\mathbb{K}f^{*} \|_{L^{2}}-\mathbb{K}f^{*}\big{\|}_{L^{2}}\] \[\leq\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}\big{\|}\mathbb{K}f^{*}-\lambda _{2}^{2}(\sigma)c_{1}x^{T}Ax\big{\|}_{L^{2}}+\|\mathbb{K}f^{*}\|_{L^{2}}^{-1} \big{\|}\|\mathbb{K}f^{*}\|-\lambda_{2}^{2}(\sigma)|c_{1}|\big{|}\] \[\lesssim\kappa d^{-5/2}\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}\] \[\lesssim\kappa/\sqrt{d}.\]

The proof of Theorem 7 follows directly from Corollary 4 in an identical manner to the proof of Theorem 3.

## Appendix F Preliminaries on Spherical Harmonics

In this section we restrict to \(\mathcal{X}_{d}=\mathcal{S}^{d-1}(\sqrt{d})\), the sphere of radius \(\sqrt{d}\), and \(\nu\) the uniform distribution on \(\mathcal{X}_{d}\).

The moments of \(\nu\) are given by the following [18]:

**Lemma 32**.: _Let \(x\sim\nu\). Then_

\[\mathbb{E}_{x}\big{[}x^{\otimes 2k}\big{]}=\chi_{k}\cdot(2k-1)!!I^{ \tilde{\otimes}k}\]

_where_

\[\chi_{k}:=\prod_{j=0}^{k-1}\left(\frac{d}{d+2j}\right)=\Theta(1).\]

For integer \(\ell\geq 0\), let \(V_{d,\ell}\) be the space of homogeneous harmonic polynomials on \(\mathbb{R}^{d}\) of degree \(\ell\) restricted to \(\mathcal{X}_{d}\). One has that \(V_{d,\ell}\) form an orthogonal decomposition of \(L^{2}(\nu)\)[27], i.e

\[L^{2}(\nu)=\bigoplus_{\ell=0}^{\infty}V_{d,\ell}\]

Homogeneous polynomials of degree \(\ell\) can be written as \(T(x^{\otimes\ell})\) for an \(\ell\)-tensor \(T\in(\mathbb{R}^{d})^{\otimes\ell}\). The following lemma characterizes \(V_{d,\ell}\):

**Lemma 33**.: \(T(x^{\otimes\ell})\in V_{d,\ell}\) _if and only if \(T(I)=0\)._

Proof.: By definition, a degree \(l\) homogeneous polynomial \(p(x)\in V_{d,l}\) if and only if \(\Delta p(x)=0\) for all \(x\in S^{d-1}\). Note that \(\nabla^{2}p(x)=l(l-1)T(x^{\otimes(l-2)})\) so this is satisfied if and only if

\[0=\operatorname{tr}T(x^{\otimes(l-2)})=T(x^{\otimes(l-2)}\otimes I )=\langle T(I),x^{\otimes(l-2)}\rangle.\]

As this must hold for all \(x\), this holds if and only if \(T(I)=0\). 

From the above characterization, we see that \(\dim(V_{d,k})=B(d,k)\), where

\[B(d,k)=\frac{2k+d-2}{k}\binom{k+d-3}{k-1}=\frac{(k+d-3)!(2k+d-2) }{k!(d-2)!}=(1+o_{d}(1))\frac{d^{k}}{k!}.\]

Define \(P_{\ell}:L^{2}(\nu)\to L^{2}(\nu)\) to be the orthogonal projection onto \(V_{d,\ell}\). The action of \(P_{0},P_{1},P_{2}\) on a homogeneous polynomial is given by the following lemma:

**Lemma 34**.: _Let \(T\in(\mathbb{R}^{d})^{\otimes 2k}\) be a symmetric \(2k\) tensor, and let \(p(x)=T(x^{\otimes 2k})\) be a polynomial. Then:_

\[P_{0}p =\chi_{k}(2k-1)!!T(I^{\otimes k})\] \[P_{1}p =0\] \[P_{2}p =\left\langle\frac{k(2k-1)!!\chi_{k+1}}{\chi_{2}}\bigg{(}T(I^{ \otimes k-1})-T(I^{\otimes k})\cdot\frac{I}{d}\bigg{)},xx^{T}\right\rangle\]

Proof.: First, we see

\[P_{0}p=\mathbb{E}\big{[}T(x^{\otimes 2k})\big{]}=\chi_{k}(2k-1)!!T(I^{ \otimes k}).\]

Next, since \(p\) is even, \(P_{1}p=0\). Next, let \(P_{2}p=x^{T}T_{2}x\). For symmetric \(B\) so that \(\operatorname{Tr}(B)=0\), we have that

\[\mathbb{E}\big{[}T(x^{\otimes 2k})x^{T}Bx\big{]}=\mathbb{E}\big{[}x^{T}T_{2} xx^{T}Bx\big{]}.\]

The LHS is

\[\mathbb{E}\big{[}T(x^{\otimes 2k})x^{T}Bx\big{]} =(2k+1)!!\chi_{k+1}(T\bar{\otimes}B)I^{\otimes k+1}\] \[=(2k+1)!!\chi_{k+1}\frac{2k}{2k+1}\langle T(I^{\otimes k-1}),B\rangle\] \[=2k\cdot(2k-1)!\chi_{k+1}\langle T(I^{\otimes k-1})-T(I^{\otimes k })\cdot\frac{I}{d},B\rangle,\]

where the last step is true since \(\operatorname{Tr}(B)=0\). The RHS is

\[\mathbb{E}\big{[}x^{T}T_{2}xx^{T}Bx\big{]} =3!!\chi_{2}(T_{2}\bar{\otimes}B)(I^{\otimes 2})\] \[=2\chi_{2}\langle T_{2},B\rangle.\]

Since these two quantities must be equal for all \(B\) with \(\operatorname{Tr}(B)=0\), and \(\operatorname{Tr}(T_{2})=0\), we see that

\[T_{2}=\frac{k(2k-1)!!\chi_{k+1}}{\chi_{2}}\bigg{(}T(I^{\otimes k -1})-T(I^{\otimes k})\cdot\frac{I}{d}\bigg{)},\]

as desired.

Polynomials over the sphere verify hypercontractivity:

**Lemma 35** (Spherical hypercontractivity [11, 38]).: _Let \(f\) be a degree \(p\) polynomial. Then for \(q\geq 2\)_

\[\|f\|_{L^{q}(\nu)}\leq(q-1)^{p/2}\|f\|_{L^{2}(\nu)}.\]

### Gegenbauer Polynomials

For an integer \(d>1\), let \(\mu_{d}\) be the density of \(x\cdot e_{1}\), where \(x\sim\operatorname{Unif}(\mathcal{S}^{d-1}(1))\) and \(e_{1}\) is a fixed unit vector. One can verify that \(\mu_{d}\) is supported on \([-1,1]\) and given by

\[d\mu_{d}(x)=\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma(\frac{d-1}{2})}(1 -x^{2})^{\frac{d-3}{2}}dx\]

where \(\Gamma(n)\) is the Gamma function. For convenience, we let \(Z_{d}:=\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma(\frac{d-1}{2})}=\frac{1}{\beta( \frac{d}{2},\frac{d-1}{2})}\) denote the normalizing constant.

The Gegenbauer polynomials \(\left(G_{k}^{(d)}\right)_{k\in\mathbb{Z}^{\geq 0}}\) are a sequence of orthogonal polynomials with respect to the density \(\mu_{d}\), defined as \(G_{0}^{(d)}(x)=1,G_{1}^{(d)}(x)=x\), and

\[G_{k}^{(d)}(x)=\frac{d+2k-4}{d+k-3}xG_{k-1}^{(d)}(x)-\frac{k-1}{ d+k-3}G_{k-2}^{(d)}(x). \tag{24}\]

[MISSING_PAGE_FAIL:42]

The proof of this lemma is deferred to Appendix G.1.

We also require a key result from [19], which lower bounds the approximation error of an inner product function.

**Definition 9**.: \(f:\mathcal{S}^{d-1}(1)\times\mathcal{S}^{d-1}(1)\to\mathbb{R}\) _is an inner product function if \(f(x,x^{\prime})=\phi(\langle x,x^{\prime}\rangle)\) for some \(\phi:[-1,1]\to\mathbb{R}\)._

**Definition 10**.: \(g:\mathcal{S}^{d-1}(1)\times\mathcal{S}^{d-1}(1)\to\mathbb{R}\) _is a separable function if \(g(x,x^{\prime})=\psi(\langle v,x\rangle,\langle v^{\prime},x^{\prime}\rangle)\) for some \(v,v^{\prime}\in\mathcal{S}^{d-1}(1)\) and \(\psi:[-1,1]^{2}\to\mathbb{R}\)._

Let \(\tilde{\nu}_{d}\) be the uniform distribution over \(\mathcal{S}^{d-1}(1)\times\mathcal{S}^{d-1}(1)\). We note that if \((x,x^{\prime})\sim\tilde{\nu}_{d}\), then \(\langle x,x^{\prime}\rangle\sim\mu_{d}\). For an inner product function \(f\), we thus have \(\left\|f\right\|_{L^{2}(\tilde{\nu}_{d})}=\left\|\phi\right\|_{L^{2}(\mu_{d})}\). We overload notation and let \(\left\|P_{k}^{(d)}f\right\|_{L^{2}(\mu_{d})}=\left\|P_{k}^{(d)}\phi\right\|_{ L^{2}(\mu_{d})}\).

**Lemma 39**.: _[_19_, Theorem 3]_ _Let \(f\) be an inner product function and \(g_{1},\ldots,g_{r}\) be separable functions. Then, for any \(k\geq 1\),_

\[\left\|f-\sum_{i=1}^{r}g_{i}\right\|_{L^{2}(\tilde{\nu}_{d})}^{2}\geq\left\|P_ {k}f\right\|_{L^{2}(\mu_{d})}\cdot\left(\left\|P_{k}f\right\|_{L^{2}(\mu_{d})} -\frac{2\sum_{i=1}^{r}\left\|g_{r}\right\|_{L^{2}(\tilde{\nu}_{d})}}{B(d,k)^{1/ 2}}\right)\!.\]

We now can prove Theorem 4

Proof of Theorem 4.: We begin with the lower bound. Let \(x=U\begin{pmatrix}x_{1}\\ x_{2}\end{pmatrix}\), where \(x_{1},x_{2}\in\mathbb{R}^{d/2}\). Assume that there exists some \(\theta\) such that \(\left\|f^{*}-N_{\theta}\right\|_{L^{2}(\mathcal{X})}\leq\epsilon\). Then

\[\epsilon^{2} \geq\mathbb{E}_{x}\Big{[}(f^{*}(x)-N_{\theta}(x))^{2}\Big{]}\] \[=\mathbb{E}_{r\sim\mu}\Big{[}\mathbb{E}_{x_{1}\sim\mathcal{S}^{d- 1}(\sqrt{r}),x_{2}\sim\mathcal{S}^{d-1}(\sqrt{d-r})}(f^{*}(x)-N_{\theta}(x))^{ 2}\Big{]},\]

where \(r\) is the random variable defined as \(r=\left\|x_{1}\right\|^{2}\) and \(\mu\) is the associated measure. The equality comes from the fact that conditioned on \(r\), \(x_{1}\) and \(x_{2}\) are independent and distributed uniformly on the spheres of radii \(\sqrt{r}\) and \(\sqrt{d-r}\), respectively. We see that \(\mathbb{E}r=d/2\), and thus

\[\mathbb{P}(\left|r-d/2\right|>d/4)\leq\exp(-\Omega(d))\]

Let \(\delta=\inf_{r\in[d/4,3d/4]}\mathbb{E}_{x_{1}\sim\mathcal{S}^{d-1}(\sqrt{r}), x_{2}\sim\mathcal{S}^{d-1}(\sqrt{d-r})}(f^{*}(x)-N_{\theta}(x))^{2}\). We get the bound

\[\epsilon^{2}\geq\delta\cdot\mathbb{P}(r\in[d/4,3d/4]),\]

and thus \(\delta\leq\epsilon^{2}(1-\exp(-\Omega(d)))\). Therefore there exists an \(r\in[d/4,3d/4]\) such that

\[\mathbb{E}_{x_{1}\sim\mathcal{S}^{d-1}(\sqrt{r}),x_{2}\sim\mathcal{S}^{d-1}( \sqrt{d-r})}(f^{*}(x)-N_{\theta}(x))^{2}\leq\epsilon^{2}/2.\]

Next, see that when \(\left\|x_{1}\right\|^{2}=r,\left\|x_{2}\right\|^{2}=d-r\), we have that

\[x^{T}Ax=\frac{2}{\sqrt{d}}\langle x_{1},x_{2}\rangle=2\sqrt{\frac{r(d-r)}{d}} \langle\overline{x}_{1},\overline{x}_{2}\rangle,\]

where now \(\overline{x}_{1},\overline{x}_{2}\sim\mathcal{S}^{d/2-1}(1)\) i.i.d. Defining \(q(z)=\mathrm{ReLU}\bigg{(}2\sqrt{\frac{r(d-r)}{d}}z\bigg{)}-c_{0}\), we thus have

\[\overline{q}(\langle\overline{x}_{1},\overline{x}_{2}\rangle)=\mathrm{ReLU}( x^{T}Ax)-c_{0}=f^{*}(x).\]

Furthermore, defining \(\overline{x}=\bigg{(}\frac{\overline{x}_{1}}{\overline{x}_{2}}\bigg{)}\), choosing the parameter vector \(\overline{\theta}=(a,\overline{W},b_{1},b_{2})\), where \(\overline{W}=WU\begin{pmatrix}\sqrt{r}\cdot I&0\\ 0&\sqrt{d-r}\cdot I\end{pmatrix}\) yields a network so that \(N_{\overline{\theta}}(\overline{x})=N_{\theta}(x)\). Therefore we get that the new network \(N_{\overline{\theta}}\) satisfies

\[\mathbb{E}_{\overline{x}_{1},\overline{x}_{2}}\Big{[}\big{(}\overline{q}( \langle\overline{x}_{1},\overline{x}_{2}\rangle)-N_{\overline{\theta}}( \overline{x})\big{)}^{2}\Big{]}\leq\epsilon^{2}/2,\]where \(\overline{x}_{1},\overline{x}_{2}\) are drawn i.i.d over \(\operatorname{Unif}(\mathcal{S}^{d/2-1}(1))\).

We aim to invoke Lemma 39. We note that \((\overline{x}_{1},\overline{x}_{2})\sim\tilde{\nu}_{d/2}\), and that \(\overline{q}\) is an inner product function. Define \(g_{i}(\overline{x})=a_{i}\sigma(\overline{w}_{i}^{T}\overline{x}+b_{1,i})\). We see that \(g_{i}\) is a separable function, and also that

\[N_{\overline{q}}(\overline{x})=\sum_{i=1}^{m}g_{i}(\overline{x})+b_{2}.\]

Hence \(N_{\overline{q}})\) is the sum of \(m+1\) separable functions. We can bound the a single function as

\[\left|g_{i}(\overline{x})\right| \leq\left|a_{i}\right|C_{\sigma}\big{(}1+\left|\overline{w}_{i}^{ T}\overline{x}+b_{1,i}\right|\big{)}^{\alpha_{\sigma}}\] \[\leq C_{\sigma}B(1+\sqrt{d}\left\|\overline{w}_{i}\right\|_{\infty }+B)^{\alpha_{\sigma}}\] \[\leq C_{\sigma}B(1+Bd^{3/2}/2+B)^{\alpha_{\sigma}}\] \[\leq(Bd^{3/2})^{\alpha_{\sigma}+1},\]

since \(\left\|\overline{W}\right\|_{\infty}\leq\max(\sqrt{r},\sqrt{d-r})\left\|WU \right\|_{\infty}\leq Bd/2\). Therefore by Lemma 39

\[\mathbb{E}_{\overline{x}_{1},\overline{x}_{2}}\Big{[}\big{(}\overline{q}( \langle\overline{x}_{1},\overline{x}_{2}\rangle)-N_{\overline{q}}(\overline{x} )\big{)}^{2}\Big{]}\geq\left\|P_{\geq k}\overline{q}\right\|_{L^{2}}\cdot \left(\left\|P_{\geq k}\overline{q}\right\|_{L^{2}}-\frac{2(m+1)(Bd^{3/2})^{ \alpha_{\sigma}+1}}{\sqrt{B_{d/2,k}}}\right)\!.\]

By Lemma 38, we have that

\[\left\|P_{\geq 2m}\mathrm{ReLU}(x)\right\|_{L^{2}(\mu_{d/2})}^{2}=\sum_{k\geq m }\frac{(2k-3)!!^{2}B(d/2,2k)}{\beta(\frac{1}{2},\frac{d/2-1}{2})^{2}\prod_{j= 0}^{k}(d/2+2j-1)^{2}}\]

Simplifying, we have that

\[\frac{(2k-3)!!^{2}B(d/2,2k)}{\prod_{j=0}^{k}(d/2+2j-1)^{2}} =\frac{(2k-3)!!^{2}}{(2k)!}\cdot\frac{(d/2+2k-3)!(d/2+4k-2)}{(d/ 2-2)!\prod_{j=0}^{k}(d/2+2j-1)^{2}}\] \[=\frac{(2k-3)!!^{2}}{(2k)!}\cdot\frac{(d/2+4k-2)}{(d/2+2k-1)^{2}( d/2+2k-3)}\prod_{j=0}^{k-2}\frac{d/2+2j}{d/2+2j-1}\] \[\geq\frac{(2k-3)!!^{2}}{(2k)!}\cdot\frac{(d/2+4k-2)}{(d/2+2k-1)^ {2}(d/2+2k-3)}\] \[\geq\frac{1}{2k(2k-1)(2k-2)}\cdot\frac{(d/2+4k-2)}{(d/2+2k-1)^{2} (d/2+2k-3)}\]

By Gautschi's inequality, we can bound

\[\beta\bigg{(}\frac{1}{2},\frac{d-2}{4}\bigg{)}=\frac{\sqrt{\pi}\Gamma(\frac{d -2}{4})}{\Gamma(\frac{d}{4})}\leq\sqrt{\pi}\bigg{(}\frac{d}{4}-1\bigg{)}^{-1/ 2}\leq 4d^{-1/2}.\]

Therefore

\[\left\|P_{2k}\mathrm{ReLU}(x)\right\|_{L^{2}(\mu_{d/2})}^{2} \geq\frac{1}{2k(2k-1)(2k-2)16}\cdot\frac{(d/2+4k-2)d}{(d/2+2k-1) ^{2}(d/2+2k-3)}\] \[\geq\frac{1}{128k^{3}}\cdot\frac{(d/2+4k-2)d}{(d/2+2k-1)^{2}(d/2+ 2k-3)}\] \[\geq\frac{1}{128k^{3}d}\]

for \(k\leq d/4\). Altogether,

\[\left\|P_{\geq 2m}\mathrm{ReLU}(x)\right\|_{L^{2}(\mu_{d/2})}^{2}\geq\frac{1}{128 d}\sum_{k=m}^{d/4}\frac{1}{k^{3}}\geq\frac{1}{512m^{2}d}\]for \(m\leq d/8\). Since \(\overline{q}(z)=\mathrm{ReLU}(2\sqrt{\frac{\tau(d-r)}{d}}z)-c_{0}=2\sqrt{\frac{\tau (d-r)}{d}}\mathrm{ReLU}(z)-c_{0}\), we have that

\[\left\|P_{\geq 2m}\overline{q}\right\|_{L^{2}(\mu)} \geq 4\frac{\tau(d-r)}{d}\|P_{\geq 2m}\mathrm{ReLU}(x)\|\] \[\geq\sqrt{3d}\cdot\sqrt{\frac{1}{512m^{2}d}}\] \[\geq\frac{1}{16m}.\]

We thus have, for any integer \(k<d/8\),

\[\epsilon^{2}/2 \geq\mathbb{E}_{\overline{q}_{1},\overline{x}_{2}}\Big{[}\big{(} \overline{q}((\overline{x}_{1},\overline{x}_{2}))-N_{\overline{q}}(\overline{ x})\big{)}^{2}\Big{]}\] \[\geq\left\|P_{\geq 2k}\overline{q}\right\|_{L^{2}}\cdot\left( \left\|P_{\geq 2k}\overline{q}\right\|_{L^{2}}-\frac{2(m+1)(Bd^{3/2})^{\alpha_{ \sigma}+1}}{\sqrt{B_{d/2,2k}}}\right)\]

Choose \(\epsilon\leq\frac{1}{512k^{2}}\); we then must have

\[\frac{2(m+1)(Bd^{3/2})^{\alpha_{\sigma}+1}}{\sqrt{B_{d/2,2k}}}\geq\frac{1}{32k},\]

or

\[(m+1)(Bd^{3/2})^{\alpha_{\sigma}+1} \geq\frac{1}{64k}B(d/2,2k)^{1/2}\geq d^{k}2^{-k}\cdot\frac{1}{64k \sqrt{(2k)!}}\] \[=C_{1}\exp\left(k\log d-\log k-\frac{1}{2}\log(2k)!-k\log 2\right)\] \[\geq C_{1}\exp\left(k\log d-\log k-k\log(2k)-k\log 2\right)\] \[\geq C_{1}\exp\!\left(k\log\frac{d}{k}-\log k-2k\log 2\right)\] \[\geq C_{1}\exp\!\left(C_{2}k\log\frac{d}{k}\right)\]

for any \(k\leq C_{3}d\). Selecting \(k=\left\lfloor\sqrt{\frac{1}{512\epsilon}}\right\rfloor\) yields

\[\max(m,B)\geq C_{1}\exp\!\left(C_{2}\epsilon^{-1/2}\log(d\epsilon)\right) \cdot d^{-3/2}\geq C_{1}\exp\!\left(C_{2}\epsilon^{-1/2}\log(d\epsilon)\right)\]

for \(\epsilon\) less than a universal constant \(c_{3}\).

We next show the upper bound,

It is easy to see that Assumptions 1 and 3 are satisfied. Next, since the verification of Assumptions 2 and 4 only required Lipschitzness, those assumptions are satisfied as well with \(\ell,\chi=1\). Finally, we have

\[\mathbb{E}_{x}\left[f^{*}(x)^{2}\right]\leq\mathbb{E}_{x}\left[ \mathrm{ReLU}^{2}(x^{T}Ax)\right]=\frac{1}{2}\,\mathbb{E}_{x}\left[(x^{T}Ax)^ {2}\right]=\frac{d}{d+2}<1.\]

Next, observe that \(\left\|\mathbb{K}f^{*}\right\|_{L^{2}}\lesssim d^{-2}\). Define \(\overline{A}=\sqrt{\frac{d+2}{2d}}A\). This scaling ensures \(\left\|x^{T}\overline{A}x\right\|_{L^{2}}=1\). Then, we can write \(f^{*}(x)=g^{*}(x^{T}\overline{A}x)\) for \(g^{*}(z)=\sqrt{\frac{2d}{d+2}}\mathrm{ReLU}(z)-c_{0}\). For \(\epsilon>0\), define the smoothed \(\mathrm{ReLU}\ \mathrm{ReLU}_{\epsilon}(z)\) as

\[\mathrm{ReLU}_{\epsilon}(z)=\begin{cases}0&z\leq-\epsilon\\ \frac{1}{4\epsilon}(x+\epsilon)^{2}&-\epsilon\leq 0\leq\epsilon\\ x&x\geq\epsilon\end{cases}.\]One sees that \(\mathrm{ReLU}_{\epsilon}\) is twice differentiable with \(\left\|\mathrm{ReLU}_{\epsilon}\right\|_{1,\infty}\leq 1\) and \(\left\|\mathrm{ReLU}_{\epsilon}\right\|_{2,\infty}=\frac{1}{2\epsilon}\)

We select the test function \(q\) to be \(q(z)=\sqrt{\frac{2d}{d+2}}\mathrm{ReLU}_{\epsilon}(\overline{\eta}^{-1}\| \mathbb{K}f^{*}\|_{L^{2}}^{-1}\cdot z)-c_{0}\). We see that

\[q(\overline{\eta}(\mathbb{K}f^{*})(x))=\mathrm{ReLU}_{\epsilon}\Big{(}\| \mathbb{K}f^{*}\|_{L^{2}}^{-1}(\mathbb{K}f^{*})(x)\Big{)},\]

and thus

\[\left\|f^{*}-q(\overline{\eta}(\mathbb{K}f^{*})(x))\right\|_{L^{2}}\] \[=\sqrt{\frac{2d}{d+2}}\left\|\mathrm{ReLU}(x^{T}\overline{A}x)- \mathrm{ReLU}_{\epsilon}\Big{(}\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}(\mathbb{K}f^{* })(x)\Big{)}\right\|_{L^{2}}\] \[\leq\left\|\mathrm{ReLU}(x^{T}\overline{A}x)-\mathrm{ReLU}_{ \epsilon}(x^{T}\overline{A}x)\right\|_{L^{2}}+\left\|\mathrm{ReLU}_{\epsilon}( x^{T}\overline{A}x)-\mathrm{ReLU}_{\epsilon}\Big{(}\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}( \mathbb{K}f^{*})(x)\Big{)}\right\|_{L^{2}}\] \[\lesssim\left\|\mathrm{ReLU}(x^{T}\overline{A}x)-\mathrm{ReLU}_{ \epsilon}(x^{T}\overline{A}x)\right\|_{L^{2}}+\left\|x^{T}\overline{A}x-\|Kf^{ *}\|_{L^{2}}^{-1}\mathbb{K}f^{*}\right\|_{L^{2}}\] \[\lesssim\left\|\mathrm{ReLU}(x^{T}\overline{A}x)-\mathrm{ReLU}_{ \epsilon}(x^{T}\overline{A}x)\right\|_{L^{2}}+Ld^{-1/12}\log d,\]

where the first inequality follows from Lipschitzness and the second inequality is Corollary 3, using \(\kappa=1\).

There exists a constant upper bound for the density of \(x^{T}\overline{A}x\), and thus we can upper bound

\[\left\|\mathrm{ReLU}(x^{T}\overline{A}x)-\mathrm{ReLU}_{\epsilon}(x^{T} \overline{A}x)\right\|_{L^{2}}^{2}\lesssim\int_{0}^{\epsilon}\frac{1}{\epsilon ^{2}}z^{4}dz\lesssim\epsilon^{3}.\]

Furthermore since \(\overline{\eta}=\Theta(\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}t^{-\chi})\), we get that \(\overline{\eta}^{-1}\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}=\Theta(\iota^{\chi})\), and thus

\[\sup_{z\in[-1,1]}|q(z)| =\sup_{z\in[-\Theta(\iota^{\chi}),\Theta(\iota^{\chi})]}|\mathrm{ ReLU}_{\epsilon}(z)|=\mathrm{poly}(\iota)\] \[\sup_{z\in[-1,1]}|q^{\prime}(z)| =\overline{\eta}^{-1}\|\mathbb{K}f^{*}\|_{L^{2}}^{-1}\sup_{z\in [-\Theta(\iota^{\chi}),\Theta(\iota^{\chi})]}|(\mathrm{ReLU}_{\epsilon})^{ \prime}(z)|=\mathrm{poly}(\iota)\] \[\sup_{z\in[-1,1]}|q^{\prime\prime}(z)| =\Big{(}\overline{\eta}^{-1}\|\mathbb{K}f^{*}\|_{L^{2}}^{-1} \Big{)}^{2}\sup_{z\in[-\Theta(\iota^{\chi}),\Theta(\iota^{\chi})]}|(\mathrm{ ReLU}_{\epsilon})^{\prime\prime}(z)|=\mathrm{poly}(\iota)\epsilon^{-1}\]

Therefore by Theorem 6 we can bound the population loss as

\[\mathbb{E}_{x}\bigg{[}\Big{(}f(x;\hat{\theta})-f^{*}(x)\Big{)}^{2}\bigg{]} \lesssim\tilde{O}\Bigg{(}\frac{d^{4}}{\min(n,m_{1},m_{2})}+\frac{\epsilon^{-2} }{m_{1}}+\sqrt{\frac{\epsilon^{-4}}{n}}+\epsilon^{3}+d^{-1/6}\Bigg{)}.\]

Choosing \(\epsilon=d^{-1/4}\) yields the desired result. As for the sample complexity, we have \(\left\|q\right\|_{2,\infty}=\tilde{O}(\epsilon^{-1})=\tilde{O}(d^{1/4})\), and so the runtime is \(\mathrm{poly}(d,m_{1},m_{2},n)\). 

### Proof of Lemma 38

Proof of Lemma 38.: For any integer \(k\), we define the quantities \(A_{2k}^{(d)},B_{2k+1}^{(d)}\) as

\[A_{2k}^{(d)} :=\int_{0}^{1}xG_{2k}^{(d)}(x)d\mu_{d}(x)\] \[B_{2k+1}^{(d)} =\int_{0}^{1}G_{2k+1}^{(d)}(x)d\mu_{d}(x).\]

We also let \(Z_{d}=\frac{\Gamma(d/2)}{\sqrt{\pi}\Gamma(\frac{d-1}{2})}\) to be the normalization constant.

Integration by parts yields

\[A_{2k}^{(d)} =Z_{d}\int_{0}^{1}G_{2k}^{(d)}(x)x(1-x^{2})^{\frac{d-3}{2}}dx\] \[=-Z_{d}\cdot G_{2k}^{(d)}(x)\cdot\frac{1}{d-1}(1-x^{2})^{\frac{d-1 }{2}}\big{|}_{0}^{1}+\frac{Z_{d}}{d-1}\cdot\frac{2k(2k+d-2)}{d-1}\int_{0}^{1}G_ {2k-1}^{(d+2)}(x)(1-x^{2})^{\frac{d-1}{2}}dx\] \[=\frac{Z_{d}}{d-1}G_{2k}^{(d)}(0)+\frac{Z_{d}}{Z_{d+2}}\cdot\frac{ 2k(2k+d-2)}{(d-1)^{2}}\cdot B_{2k-1}^{(d+2)}\]

From Corollary 5 we have

\[G_{2k}^{(d)}(0)=\frac{(2k-1)!!}{\Pi_{j=0}^{k-1}(d+2j-1)}(-1)^{k}.\]

Thus

\[A_{2k}^{(d)}=Z_{d}\cdot\frac{(2k-1)!!}{(d-1)\Pi_{j=0}^{k-1}(d+2j-1)}(-1)^{k}+ \frac{2k(2k+d-2)}{(d-1)^{2}}\cdot\frac{Z_{d}}{Z_{d+2}}B_{2k-1}^{(d+2)}. \tag{25}\]

The recurrence formula yields

\[B_{2k+1}^{(d)} =\int_{0}^{1}G_{2k+1}^{(d)}(x)d\mu_{d}(x) \tag{26}\] \[=\int_{0}^{1}\bigg{[}\frac{4k+d-2}{2k+d-2}xG_{2k}^{(d)}(x)-\frac{ 2k}{2k+d-2}G_{2k-1}^{(d)}(x)\bigg{]}d\mu_{d}(x)\] (27) \[=\frac{4k+d-2}{2k+d-2}A_{2k}^{(d)}-\frac{2k}{2k+d-2}B_{2k-1}^{(d)}. \tag{28}\]

I claim that

\[A_{2k}^{(d)}=\begin{cases}\frac{Z_{d}}{d-1}&k=0\\ (-1)^{k+1}Z_{d}\frac{(2k-3)!!}{\prod_{j=0}^{k}(d+2j-1)}&k\geq 1\end{cases}\quad \text{and}\quad B_{2k+1}^{(d)}=(-1)^{k}Z_{d}\frac{(2k-1)!!}{\prod_{j=0}^{k}(d +2j-1)}.\]

We proceed by induction on \(k\). For the base cases, we first have

\[A_{0}^{(d)}=\int_{0}^{1}xd\mu_{d}(x) =\int_{0}^{1}Z_{d}x(1-x^{2})^{\frac{d-2}{2}}\] \[=\int_{0}^{1}\frac{Z_{d}}{d-1}du\] \[=\frac{Z_{d}}{d-1},\]

where we use the substitution \(u=(1-x^{2})^{\frac{d-3}{2}}\). Next,

\[B_{1}^{(d)}=\int_{0}^{1}xd\mu_{d}(x)=A_{0}^{(d)}=\frac{Z_{d}}{d-1}.\]

Next, eq. (25) gives

\[A_{2}^{(d)} =Z_{d}\cdot\frac{-1}{(d-1)^{2}}+\frac{2d}{(d-1)^{2}}\cdot\frac{Z _{d}}{d+1}\] \[=\frac{Z_{d}}{(d-1)(d+1)}.\]

Finally, eq. (26) gives

\[B_{3}^{(d)} =\frac{d+2}{d}A_{2}^{(d)}-\frac{2}{d}B_{1}^{(d)}\] \[=\frac{Z_{d}}{d-1}\bigg{[}\frac{d+2}{d(d+1)}-\frac{2}{d}\bigg{]}\] \[=-\frac{Z_{d}}{(d-1)(d+1)}.\]Therefore the base case is proven for \(k=0,1\).

Now, assume that the claim is true for some \(k\geq 1\) for all \(d\). We first have

\[A_{2k+2}^{(d)} =Z_{d}\cdot\frac{(2k+1)!!}{(d-1)\Pi_{j=0}^{k}(d+2j-1)}(-1)^{k+1}+ \frac{(2k+2)(2k+d)}{(d-1)^{2}}\cdot\frac{Z_{d}}{Z_{d+2}}B_{2k+1}^{(d+2)}\] \[=Z_{d}\cdot\frac{(2k+1)!!}{(d-1)\Pi_{j=0}^{k}(d+2j-1)}(-1)^{k+1}+Z _{d}\cdot\frac{(2k+2)(2k+d)}{(d-1)^{2}}\cdot\frac{(2k-1)!!}{\Pi_{j=0}^{k}(d+2 j+1)}(-1)^{k}\] \[=(-1)^{k+1}Z_{d}\cdot\frac{(2k-1)!!}{\Pi_{j=0}^{k+1}(d+2j-1)} \bigg{[}\frac{(d+2k+1)(2k+1)}{d-1}-\frac{(2k+2)(2k+d)}{d-1}\bigg{]}\] \[=(-1)^{k+1}Z_{d}\cdot\frac{(2k-1)!!}{\Pi_{j=0}^{k+1}(d+2j-1)} \bigg{[}\frac{-d+1}{d-1}\bigg{]}\] \[=(-1)^{k+2}Z_{d}\cdot\frac{(2k-1)!!}{\Pi_{j=0}^{k+1}(d+2j-1)}.\]

Next, we have

\[B_{2k+3}^{(d)} =\frac{4k+d+2}{2k+d}A_{2k+2}^{(d)}-\frac{2k+2}{2k+d}B_{2k+1}^{(d)}\] \[=(-1)^{k}Z_{d}\bigg{[}\frac{4k+d+2}{2k+d}\frac{(2k-1)!!}{\Pi_{j=0 }^{k+1}(d+2j-1)}-\frac{2k+2}{2k+d}\frac{(2k-1)!!}{\prod_{j=0}^{k}(d+2j-1)} \bigg{]}\] \[=(-1)^{k}Z_{d}\frac{(2k-1)!!}{\prod_{j=0}^{k+1}(d+2j-1)}\bigg{[} \frac{(4k+d+2)-(2k+2)(d+2k+1)}{2k+d}\bigg{]}\] \[=(-1)^{k}Z_{d}\frac{(2k-1)!!}{\prod_{j=0}^{k+1}(d+2j-1)}\bigg{[} \frac{-(2k+1)(2k+d)}{2k+d}\bigg{]}\] \[=(-1)^{k+1}Z_{d}\frac{(2k+1)!!}{\prod_{j=0}^{k+1}(d+2j-1)}.\]

Therefore by induction the claim holds for all \(k,d\).

The Gegenbauer expansion of \(\operatorname{ReLU}\) is given by

\[\operatorname{ReLU}(x)=\sum_{i=0}^{\infty}\langle\operatorname{ReLU},G_{i}^{( d)}\rangle_{L^{2}(\mu_{d})}B(d,i)G_{i}^{(d)}(x).\]

Note that \(\operatorname{ReLU}(x)=\frac{1}{2}(x+|x|)\). Since \(|x|\) is even, the only nonzero odd Gegenbauer coefficient is for \(G_{1}^{(d)}\). In this case,

\[\langle\operatorname{ReLU},G_{1}^{(d)}\rangle_{L^{2}(\mu_{d})}=\frac{1}{2} \mathbb{E}_{x\sim\mu_{d}}[x^{2}]=\frac{1}{2d^{2}}.\]

Also, \(B(d,1)=d\). Next, we see that

\[\langle\operatorname{ReLU},G_{2k}^{(d)}\rangle_{L^{2}(\mu_{d})}=\int_{-1}^{1} \operatorname{ReLU}(x)G_{2k}^{(d)}(x)d\mu_{d}(x)=\int_{0}^{1}xG_{2k}^{(d)}(x)d \mu_{d}(x)=A_{2k}^{(d)}.\]

Plugging in our derivation for \(A_{2k}^{(d)}\) gives the desired result.