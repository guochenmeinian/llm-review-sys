# Dynamic Non-monotone Submodular Maximization

 Kiarash Banihashem

kiarash@umd.edu

University of Maryland

&Leyla Biabani

l.biabani@tue.nl

TU Eindhoven

&Samira Goudarzi

samirag@umd.edu

Department of Mathematics and Computer Science, Eindhoven University of Technology, the Netherlands.

&MohammadTaghi Hajiaghayi

hajiagha@cs.umd.edu

University of Maryland

&Peyman Jabbarzade

peymanj@umd.edu

University of Maryland

&Morteza Monemizadeh

m.monemizadeh@tue.nl

TU Eindhoven

###### Abstract

Maximizing submodular functions has been increasingly used in many applications of machine learning, such as data summarization, recommendation systems, and feature selection. Moreover, there has been a growing interest in both submodular maximization and dynamic algorithms. In 2020, Monemizadeh [46] and Lattanzi, Mitrovic, Norouzi-Fard, Tarnawski, and Zadimoghaddam [40] initiated developing dynamic algorithms for the monotone submodular maximization problem under the cardinality constraint \(k\). In 2022, Chen and Peng [15] studied the complexity of this problem and raised an important open question: "_Can we extend [fully dynamic] results (algorithm or hardness) to non-monotone submodular maximization?_". We affirmatively answer their question by demonstrating a reduction from maximizing a non-monotone submodular function under the cardinality constraint \(k\) to maximizing a monotone submodular function under the same constraint. Through this reduction, we obtain the first dynamic algorithms solving the non-monotone submodular maximization problem under the cardinality constraint \(k\). We've derived two algorithms, both maintaining an \((8+\epsilon)\)-approximate of the solution. The first algorithm requires \(\mathcal{O}(\epsilon^{-3}k^{3}\log^{3}(n)\log(k))\) oracle queries per update, while the second one requires \(\mathcal{O}(\epsilon^{-1}k^{2}\log^{3}(k))\). Furthermore, we showcase the benefits of our dynamic algorithm for video summarization and max-cut problems on several real-world data sets.

## 1 Introduction

Submodular functions are powerful tools for solving real-world problems as they provide a theoretical framework for modeling the famous "_diminishing returns_" [30] phenomenon arising in a variety of practical settings. Many theoretical problems such as those involving graph cuts, entropy-based clustering, coverage functions, and mutual information can be cast in the submodular maximization framework. As a result, submodular functions have been increasingly used in many applications of machine learning such as data summarization [52, 51, 50], feature selection [17, 19, 18, 38], and recommendation systems [24]. These applications include both the monotone and non-monotone versions of the maximization of submodular functions.

Applications of non-monotone submodular maximization.The general problem of non-monotone submodular maximization has been studied extensively in [27, 12, 11, 43, 5, 47]. Thisproblem has numerous applications in video summarization, movie recommendation [43], and revenue maximization in viral marketing [35]4. An important application of this problem appears in maximizing the difference between a monotone submodular function and a linear function that penalizes the addition of more elements to the set (e.g., the coverage and diversity trade-off). An illustrative example of this application is the maximum facility location in which we want to open a subset of facilities and maximize the total profit from served clients plus the cost of facilities we did not open [21]. Another important application occurs when expressing learning problems such as feature selection using weakly submodular functions [17, 38, 25, 49].

Footnote 4: The problem of selecting a subset of people in a social network to maximize their influence in a viral marketing campaign can be modeled as a constrained submodular maximization problem. When we introduce a cost, then the influence minus the cost is modeled as non-monotone submodular maximization problems [9, 8, 34].

Our contribution.In this paper, we consider the non-monotone submodular maximization problem under cardinality constraint \(k\) in the _fully dynamic setting_. In this model, we have a _universal ground set_\(V\). At any time \(t\), ground set \(V_{t}\subseteq V\) is the set of elements that are inserted but not deleted after their last insertion till time \(t\). More formally, we assume that there is a sequence of "updates" such that each update either inserts an element to \(V_{t-1}\) or deletes an element from \(V_{t-1}\) to form \(V_{t}\).

We assume that there is a (non-monotone) submodular function \(f\) that is defined over the universal ground set \(V\). Our goal is to maintain, at each point in time, a set of size at most \(k\) whose submodular value is maximum among any subset of \(V_{t}\) of size at most \(k\).

Since calculating such a set is known to be NP-hard [27] even in the offline setting (where you get all the items at the same time), we focus on providing algorithms with provable approximation guarantees, while maintaining fast update time. This is challenging as elements may be inserted or deleted, possibly in an adversarial order. While several dynamic algorithms exist for monotone submodular maximization, non-monotone submodular maximization is a considerably more challenging problem as adding elements to a set may decrease its value.

In STOC 2022, Chen and Peng [15] raised the following open question:

**Open problem:** "Can we extend [fully dynamic] results (algorithm or hardness) to non-monotone submodular maximization?"

In this paper, we answer their question affirmatively by providing the first dynamic algorithms for non-monotone submodular maximization.

To emphasize the significance of our result, it should be considered that although monotone submodular maximization under cardinality constraint has a tight \(\frac{e}{e-1}\) approximation algorithm in the offline mode and nearly tight (\(2+e\)) approximation algorithms for both streaming and dynamic settings, there is a hardness result for the non-monotone version stating that it is impossible to obtain a \(2.04\) (i.e., \(0.491\)) approximation algorithm for this problem even in the offline setting[31], and to the best of our knowledge, the current state of the art algorithms for this problem have \(2.6\) and \(3.6\) (i.e., \(0.385\) and \(0.2779\)) approximation guarantees for offline [10] and streaming settings [2], respectively.

We obtain our result, by proposing a general reduction from the problem of dynamically maintaining a non-monotone submodular function under cardinality constraint \(k\) to developing a dynamic thresholding algorithm for maximizing monotone submodular functions under the same constraint. We first define \(\tau\)-thresholding dynamic algorithms that we use in our reduction.

**Definition 1.1** (\(\tau\)-Thresholding Dynamic Algorithm).: Let \(\tau>0\) be a parameter. We say a dynamic algorithm is \(\tau\)-thresholding if at any time \(t\) of sequence \(\Xi\), it reports a set \(S_{t}\subseteq V_{t}\) of size at most \(k\) such that

* **Property 1:** either **a)**\(S_{t}\) has \(k\) elements and \(f(S_{t})\geq k\tau\), or **b)**\(S_{t}\) has less than \(k\) elements and for any \(v\in V_{t}\setminus S_{t}\), the marginal gain \(\Delta(v|S_{t})<\tau\).
* **Property 2:** The number of elements changed in any update, i.e, \(|S_{t+1}\setminus S_{t}|+|S_{t}\setminus S_{t+1}|\), is not more than the number of queries made by the algorithm during the update.

In the above definition, the first property reflects the main intuition of threshold-based algorithms, while the last property is a technical condition required in our analysis. It's worth noting that the thresholding technique has been used widely for optimizing submodular functions [46, 41, 26, 40, 16]. We next state our main result, which is a general reduction.

**Theorem 1.2** (Reduction Metatheorem).: _Suppose that \(f:2^{V}\to\mathbb{R}_{\geq 0}\) is a (possibly non-monotone) submodular function defined on subsets of a ground set \(V\) and let \(k\in\mathbb{N}\) be a parameter._

_Assume that for any given value of \(\tau\), there exists a \(\tau\)-thresholding dynamic algorithm with an expected (amortized) \(O(g(n,k))\) oracle queries per update. Then, there exist the following dynamic algorithms:_

* _A dynamic algorithm with an approximation guarantee of_ \((8+\varepsilon)\) _using an expected (amortized)_ \(O(k+\min(k,g(n,k))\cdot g(n,k)\cdot\varepsilon^{-1}\log(k))\) _oracle queries per update._
* _A dynamic algorithm maintaining a_ \((10+\varepsilon)\)_-approximate solution of the optimal value of_ \(f\) _using an expected (amortized)_ \(O(\min(k,g(n,k))\cdot g(n,k)\cdot\varepsilon^{-1}\log(k))\) _oracle calls per update._

In [46], Monemizadeh developed a dynamic algorithm for monotone submodular maximization under cardinality constraint \(k\), which requires an amortized \(O(\varepsilon^{-2}k^{2}\log^{3}(n))\) number of oracle queries per update. Interestingly, in the appendix, we show that this algorithm is indeed \(\tau\)-thresholding (for any given \(\tau\)). Now, if we use this \(\tau\)-thresholding dynamic algorithm inside our reduction Metatheorem 1.2, we obtain a dynamic algorithm that maintains a \((8+\varepsilon)\)-approximate solution using an expected amortized \(O(\varepsilon^{-3}k^{3}\log^{3}(n)\log(k))\) oracle queries per update.

The recent paper [6] of Banihashem, Biabani, Goudarzi, Hajiaghayi, Jabbarzade, and Monemizadeh develops a new dynamic algorithm for monotone submodular maximization under cardinality constraint \(k\), which uses an expected \(O(\varepsilon^{-1}k\log^{2}(k))\) number of oracle queries per update. A similar proof shows that this new algorithm is \(\tau\)-thresholding as well. We have provided its pseudocode and a detailed explanation on why this algorithm is indeed \(\tau\)-thresholding in the appendix. By exploiting this algorithm in our Reduction Metatheorem 1.2, we can reduce the number of oracle queries mentioned to an expected number of \(O(\varepsilon^{-2}k^{2}\log^{3}(k))\) per update.

The second result in Theorem 1.2 is also of interest as it can be used to devise a dynamic algorithm for non-monotone submodular maximization with polylogarithmic query complexity if one can provide a \(\tau\)-thresholding dynamic algorithm for maximizing monotone submodular functions (under the cardinality constraint \(k\)) with polylogarithmic query complexity.

### Preliminaries

Submodular maximization.Let \(V\) be a ground set of elements. We say a function \(f:2^{V}\to\mathbb{R}_{\geq 0}\) is a _submodular_ function if for any \(A,B\subseteq V\), \(f(A)+f(B)\geq f(A\cup B)+f(A\cap B)\). Equivalently, \(f\) is a submodular function if for any subsets \(A\subseteq B\subseteq V\) and for any element \(e\in V\setminus B\), it holds that \(f(A\cup\{e\})-f(A)\geq f(B\cup\{e\})-f(B)\). We define \(\Delta(e|A):=f(A\cup\{e\})-f(A)\) the _marginal gain_ of adding the element \(e\) to set \(A\). Similarly, we define \(\Delta(B|A):=f(A\cup B)-f(A)\) for any sets \(A,B\subseteq V\). Function \(f\) is _monotone_ if \(f(A)\leq f(B)\) holds for any \(A\subseteq B\subseteq V\), and it is _non-monotone_ if it is not necessarily the case. In the submodular maximization problem under cardinality constraint \(k\), we seek to compute a set \(S^{*}\) such that \(|S^{*}|\leq k\) and \(f(S^{*})=\max_{|S|\leq k}f(S)\), where \(f\) is a submodular function and \(k\in\mathbb{N}\) is a given parameter.

Query access model.Similar to recent dynamic works [40; 15], we assume the access to a submodular function \(f\) is given by an _oracle_. The oracle allows _set queries_ such that for every subset \(A\subseteq V\), one can query the value \(f(A)\). In this query access model, the marginal gain \(\Delta_{f}(e|A)\equiv f(A\cup\{e\})-f(A)\) for every subset \(A\subseteq V\) and an element \(e\in V\setminus A\), can be computed using two set queries. To do so, we first query \(f(A\cup\{e\})\) and then \(f(A)\).

Dynamic model.Let \(\Xi\) be a sequence of inserts and deletes of an underlying universe \(V\). We assume that \(f:2^{V}\to\mathbb{R}_{\geq 0}\) is a (possibly non-monotone) submodular function defined on subsets of the universe \(V\). We define time \(t\) to be the \(t^{\text{th}}\) update (i.e., insertion or deletion) of sequence \(\Xi\). We let \(\Xi_{t}\) be the sub-sequence of updates from the beginning of sequence \(\Xi\) till time \(t\) and denote by \(V_{t}\subseteq V\) the set of elements that are inserted but not deleted from the beginning of the sequence \(\Xi\) till any time \(t\). That is, \(V_{t}\) is the current ground set of elements. We let \(\widetilde{OPT}_{t}=\max_{S\subseteq V_{t}:|S|\leq k}f(S)\).

Query complexity.The _query complexity_ of a dynamic \(\alpha\)-approximate algorithm is the number of oracle queries that the algorithm must make to compute a solution \(S_{t}\) with respect to ground setwhose submodular value is an \(\alpha\)-approximation of \(OPT_{t}\). That is, \(|S_{t}|\leq k\) and \(f(S_{t})\geq\alpha\cdot OPT_{t}\). Observe that the dynamic algorithm remembers every query it has made so far. Thus results of queries made in previous times may help find \(S_{t}\) in current time \(t\).

Oblivious adversarial model.The dynamic algorithms that we develop in this paper are in the _oblivious adversarial model_ as is common for analysis of randomized data structures such as universal hashing [13]. The model allows the adversary, who is aware of the submodular function \(f\) and the algorithm that is going to be used, to determine all the arrivals and departures of the elements in the ground set \(V\). However, the adversary is unaware of the random bits used in the algorithm and so cannot choose updates adaptively in response to the randomly guided choices of the algorithm. Equivalently, we can suppose that the adversary prepares the full input (insertions and deletions) before the algorithm runs.

### Related Work

Offline algorithms.The offline version of non-monotone submodular maximization was first studied by Feige, Mirrokni, and Vondrak in [27]. They studied _unconstrained non-monotone submodular maximization_ and developed constant-factor approximation algorithms for this problem. In the offline query access model, they showed that a subset \(S\) chosen uniformly at random has a submodular value which is a \(4\)-approximation of the optimal value for this problem. In addition, they also described two local search algorithms. The first uses \(f\) as the objective function, and provides \(3\)-approximation and the second uses a noisy version of \(f\) as the objective function and achieves an improved approximation guarantee \(2.5\) for maximizing unconstrained non-monotone non-negative submodular functions. Interestingly, they showed \((2-\varepsilon)\)-approximation for symmetric submodular functions would require an exponential number of queries for any fixed \(\varepsilon>0\).

Oveis Gharan and Vondrak [32] showed that an extension of the \(2.5\)-approximation algorithm can be seen as _simulated annealing_ method which provides an improved approximation of roughly \(2.4\). Later, Buchbinder, Feldman, Naor, and Schwartz [11] at FOCS'12, presented a randomized linear time algorithm achieving a tight approximation guarantee of \(2\) that matches the known hardness result of [27]. Bateni, Hajiaghayi, and Zadimoghaddam [9, 8] and Gupta, Roth, Schoenebeck, and Talwar [34] independently studied non-monotone submodular maximization subject to cardinality constraint \(k\) in the offline and secretary settings. In particular, Gupta _et al._[34] obtained an offline \(6.5\)-approximation for this problem.

All of the aforementioned approximation algorithms are offline, where the whole input is given in the beginning, whereas the need for real-time analysis of rapidly changing data streams motivates the study of this problem in settings such as the dynamic model that we study in this paper.

Streaming algorithms.The dynamic model that we study in this paper is closely related to the streaming model [3, 36]. However, the difference between these two models is that in the streaming model, we maintain a data structure using which we compute a solution at the end of the stream and so, the time to extract the solution is not important as we do it once. However, in the dynamic model, we need to maintain a solution after every update, thus, the update time of a dynamic algorithm should be as fast as possible.

The known streaming algorithms [44, 28, 29] work in the insertion-only streaming model and they do not support deletions as well as insertions. Indeed, there are streaming algorithms [37, 45] for the monotone submodular maximization problem that support deletions, but the space and the update time of these algorithms depend on the number of deletions which could be \(\Omega(n)\), where \(n=|V|\) is the size of ground set \(V\).

For monotone submodular maximization, Badanidiyuru, Mirzasoleiman, Karbasi, and Krause [4] proposed an insertion-only streaming algorithm with a \((2+\varepsilon)\)-approximation guarantee under a cardinality constraint \(k\). Chekuri, Gupta, and Quanrud [14] presented (insertion-only) streaming algorithms for maximizing monotone and non-monotone submodular functions subject to \(p\)-matchoid constraint5. Later, Mirzasoleiman, Jegelka, and Krause [44] and Feldman, Karbasi, and Kazemi [28] developed streaming algorithms with better approximation guarantees for maximizing a non-monotone function under a \(p\)-matchoid constraint. Currently, the best streaming algorithm for maximizing a non-monotone submodular function subject to a cardinality constraint is due to Alaluf, Ene, Feldman, Nguyen, and Suh [1] whose approximation guarantee is \(3.6+\varepsilon\), improving the \(5.8\)-approximation guarantee that was proposed by Feldman _et al._[28].

Dynamic algorithms.At NeurIPS 2020, Lattanzi, Mitrovic, Norouzi-Fard, Tarnawski, and Zadimoghaddam [40] and Monemizadeh [46] initiated the study of submodular maximization in the dynamic model. They presented dynamic algorithms that maintain \((2+\varepsilon)\)-approximate solutions for maximizing a monotone submodular function subject to cardinality constraint \(k\). Later, at STOC 2022, Chen and Peng [15] studied the complexity of this problem and they proved that developing a \(c\)-approximation dynamic algorithm for \(c<2\) is not possible unless we use a number of oracle queries polynomial in the size of ground set \(V\). In 2023, Banihashem, Biabani, Goudarzi, Hajiaghayi, Jabbarzade, and Monemizadeh[7] developed an algorithm for monotone submodular maximization problem under cardinality constraint \(k\) using a polylogarithmic amortized update time. Concurrent works of Banihashem, Biabani, Goudarzi, Hajiaghayi, Jabbarzade, and Monemizadeh[6] and Duetting, Fusco, Lattanzi, Norouzi-Fard, and Zadimoghaddam[22] developed the first dynamic algorithms for monotone submodular maximization under a matroid constraint. Authors of [6] also improve the algorithm of [46] for monotone submodular maximization subject to cardinality constraint \(k\). There are also studies on the dynamic model of influence maximization, which shares similarities with submodular maximization [48].

In this paper, for the first time, we study the generalized version of their problem by presenting an algorithm for maximizing the non-monotone submodular functions in the dynamic setting.

## 2 Dynamic algorithm

In this section, we explain the algorithm that we use in the reduction that we stated in Metatheorm 1.2. The pseudocode of our algorithm is given in Algorithm 1, Algorithm 2, and Algorithm 3.

Such reductions were previously proposed in the offline model by [34], and later works extended this idea to the streaming model [14, 44]. We develop a reduction in the dynamic model inspired by these works, though in our proof, we require a tighter analysis to obtain the approximation guarantee in our setting.

We consider an arbitrary time \(t\) of sequence \(\Xi\) where \(V_{t}\) is the set of elements inserted before time \(t\), but not deleted after their last insertion. Let \(OPT^{*}_{t}=\max_{S\subseteq V_{t}:|S|\leq k}f(S)\). For simplicity, we drop \(t\) from \(V_{t}\) and \(OPT^{*}_{t}\), when it is clear from the context.

In the following, we assume that the value of \(OPT\) is known. Although the exact value of \(OPT^{*}\) is unknown, we can maintain parallel runs of our dynamic algorithm for different guesses of the optimal value. By using \((1+\varepsilon^{\prime})^{i}\), where \(i\in\mathbb{Z}\) as our guesses for the optimal value, one of our guesses \((1+\varepsilon^{\prime})\)-approximates the value of \(OPT^{*}\). We show that the output of our algorithm satisfies the approximation guarantee in the run whose \(OPT\)\((1+\varepsilon^{\prime})\)-approximates the value of \(OPT^{*}\). Later, in the appendix, we show that it is enough to consider each element \(e\) only in runs \(i\) for which we have \(\frac{\varepsilon^{\prime}}{k}\cdot(1+\varepsilon^{\prime})^{i}\leq f(e)\leq(1 +\varepsilon^{\prime})^{i}\). This method increases the query complexity of our dynamic algorithm by only a factor of \(O(\varepsilon^{-1}\log k)\).

Our approach for solving the non-monotone submodular maximization is to first run the thresholding algorithm with input set \(V\) to find a set \(S_{1}\) of at most \(k\) elements. Since \(f\) is non-monotone, subsets of \(S_{1}\) might have a higher submodular value than \(f(S_{1})\). Then, we use an \(\alpha\)-approximation algorithm (for \(0<\alpha\leq 1\)) to choose a set \(S^{\prime}_{1}\subseteq S_{1}\) with guarantee \(\mathbf{E}[f(S^{\prime}_{1})]\geq\alpha\cdot\max_{C\subseteq S_{1}}f(C)\). Next, we run the thresholding algorithm with the input set \(V\backslash S_{1}\) and compute a set \(S_{2}\). At the end, we return set \(S=\arg\max_{C\in\{S_{1},S_{1},S_{2}\}}f(C)\). Intuitively, for an optimal solution \(S^{*}\), if \(f(S_{1}\cap S^{*})\) is a good approximation of \(OPT\), then \(f(S^{\prime}_{1})\) is a good approximation of \(OPT\). On the other hand, if both \(f(S_{1})\) and \(f(S_{1}\cap S^{*})\) are small with respect to \(OPT\), then we can ignore the elements of \(S_{1}\) and show that we can find a set \(S_{2}\subseteq V\setminus S_{1}\) of size at most \(k\) whose submodular value is a good approximation of \(OPT\). The following lemma proves that the submodular value of \(S\) is a reliable approximation of the optimal solution. The formal proof of this lemma can be found in Section 2.

**Lemma 2.1** (Approximation Guarantee).: _Assuming that \(OPT^{*}\in[\frac{OPT}{1+\epsilon^{\prime}},OPT]\), the expected submodular value of set \(S\) is \(\mathbf{E}[f(S)]\geq(1-O(\epsilon^{\prime}))\frac{OPT^{*}}{6+\frac{1}{\epsilon}}\)._

Next, we explain the steps of our reduction in detail.

Let us first fix the threshold \(\tau=\frac{OPT}{k(3+1/(2\alpha))}\). Then, we fix a \(\tau\)-thresholding dynamic algorithm (for example, [46] or [6]) and suppose we denote it by DynamicThresholding. Before sequence \(\Xi\) of updates starts, we create two independent instances \(\mathcal{I}_{1}\) and \(\mathcal{I}_{2}\) of DynamicThresholding. The first instance will maintain set \(S_{1}\) and the second instance will maintain set \(S_{2}\). For instance \(\mathcal{I}_{i}\) where \(i\in\{1,2\}\), we consider the following subroutines:

* \(\textsc{Insert}_{\mathcal{I}_{i}}(v)\): This subroutine inserts an element \(v\) to instance \(\mathcal{I}_{i}\).
* \(\textsc{Delete}_{\mathcal{I}_{i}}(v)\): Invoking this subroutine will delete the element \(v\) from instance \(\mathcal{I}_{i}\).
* \(\textsc{Extract}_{\mathcal{I}_{i}}\): This subroutine returns the maintained set (of size at most \(k\)) of \(\mathcal{I}_{i}\).

Extracting \(S_{1}\).After the update at time \(t\), first, we would like to set \(Z=S_{1}^{-}\cup\{v\}\) or \(Z=S_{1}^{-}\setminus\{v\}\), if the update is the insertion of an element \(v\) or the deletion of an element \(v\), respectively, where \(S_{1}^{-}\) is the set \(S_{1}\) that instance \(\mathcal{I}_{1}\) maintains just before this update. To find set \(S_{1}^{-}\), we just need to invoke subroutine Extract\({}_{\mathcal{I}_{1}}\). If the update is an insertion, we insert it into instance \(\mathcal{I}_{1}\) using Insert\({}_{\mathcal{I}_{1}}(v,\tau)\), and if the update is a deletion, we delete \(v\) from both \(\mathcal{I}_{1}\) and \(\mathcal{I}_{2}\) using Delete\({}_{\mathcal{I}_{1}}(v)\) and Delete\({}_{\mathcal{I}_{2}}(v)\). We then invoke Extract\({}_{\mathcal{I}_{1}}\) once again to return set \(S_{1}\).

Extracting \(S_{1}^{\prime}\).Buchbinder _et al._[11] developed a method to extract a subset \(S_{1}^{\prime}\subseteq S_{1}\) whose submodular value is a good approximation of \(\max_{C\subseteq S_{1}}f(C)\). In this algorithm, we start with two solutions \(\emptyset\) and \(S_{1}\). The algorithm considers the elements (in arbitrary order) one at a time. For each element, it determines whether it should be added to the first solution or removed from the second solution. Thus, after a single pass over set \(S_{1}\), both solutions completely coincide, which is the solution that the algorithm outputs. They show that a (deterministic) greedy choice in each step obtains \(3\)-approximation of the best solution in \(S_{1}\). However, if we combine this greedy choice with randomization, we can obtain a \(2\)-approximate solution. Since we do a single pass over set \(S_{1}\), the number of oracle queries is \(O(|S_{1}|)\).

The second algorithm that we can use to extract \(S_{1}^{\prime}\) is a random sampling algorithm proposed by Feige _et al._[27], which choose every element in \(S_{1}\) with probability \(1/2\). They show that this random sampling returns a set \(S_{1}^{\prime}\) whose approximation factor is \(1/4\) of \(\max_{C\subseteq S_{1}}f(C)\), and its number of oracle calls is \(O(1)\). We denote either of these two methods by SubsetSelection.

Extracting \(S_{2}\).Next, we would like to update the set \(S_{2}\) that is maintained by instance \(\mathcal{I}_{2}\). To do this, for every element \(u\in Z\setminus S_{1}\), we add it to \(\mathcal{I}_{2}\) using Insert\({}_{\mathcal{I}_{2}}(u,\tau)\), and for every element \(u\in S_{1}\setminus Z\), we delete it from \(\mathcal{I}_{2}\) using Delete\({}_{\mathcal{I}_{2}}(u,\tau)\). Finally, when \(\mathcal{I}_{2}\) exactly includes all the current elements other than the ones in \(S_{1}\), we call subroutine Extract\({}_{\mathcal{I}_{2}}\) to return set \(S_{2}\).

**Corollary 2.2**.: _We obtain the \((8+\varepsilon)\) approximation guaranty stated in the Metatheorem 1.2 by using the local search method for our SubsetSelection, and we get the \((10+\varepsilon)\) approximation guaranty by utilizing the random sampling method for our SubsetSelection subroutine._

Proof.: These are immediate results of Lemma 2.1, and \(\alpha\) being \(\frac{1}{2}\) and \(\frac{1}{4}\) in the local search method and random sampling method, respectively. 

```
1:\(\tau\leftarrow\frac{OPT}{k(3+1/(2\alpha))}\), where \(\alpha\) is \(\frac{1}{2}\) or \(\frac{1}{4}\) based on the selection of algorithm for SubsetSelection.
2:Instantiate two independent instances \(\mathcal{I}_{1}\) and \(\mathcal{I}_{2}\) of DynamicThresholding for monotone submodular maximization under cardinality constraint \(k\) using \(\tau\)
```

**Algorithm 1** Initialization\((k,OPT)\)```
1:\(Z\leftarrow\textsc{Extract}_{\mathcal{I}_{1}}\)
2:if\(\textsc{Update}(v)\) is an insertion then
3: Invoke \(\textsc{Insert}_{\mathcal{I}_{1}}(v),\ Z\gets Z\cup\{v\}\)
4:else
5: Invoke \(\textsc{Delete}_{\mathcal{I}_{1}}(v),\ \textsc{Delete}_{\mathcal{I}_{2}}(v),\ Z \gets Z\setminus\{v\}\)
6:\(S_{1}\leftarrow\textsc{Extract}_{\mathcal{I}_{1}}\)
7:\(S^{\prime}_{1}\leftarrow\textsc{SubsetSelection}(S_{1})\)
8:for\(u\in S_{1}\setminus Z\)do
9:\(\textsc{Delete}_{\mathcal{I}_{2}}(u)\)
10:for\(u\in Z\setminus S_{1}\)do
11:\(\textsc{Insert}_{\mathcal{I}_{2}}(u)\)
12:\(S_{2}\leftarrow\textsc{Extract}_{\mathcal{I}_{2}}\)
13: Return \(\arg\max_{C\in\{S_{1},S^{\prime}_{1},S_{2}\}}f(C)\)
```

**Algorithm 2**Update\((v)\)

```
1:functionUniformSubset(\(S\))
2:\(T\leftarrow\emptyset\)
3:for\(s\in S\)do
4:if\(Coin(\frac{1}{2})\)then\(\triangleright\) With probability \(\frac{1}{2}\)
5:\(T\gets T\cup\{s\}\)
6:return\(T\)
7:functionLocalSearchSubset(\(S\))
8:\(X_{0}\leftarrow\emptyset,\ Y_{0}\gets S\).
9:for\(i=1\) to \(|S|\)do
10:\(a_{i}\gets f(X_{i-1}\cup\{s_{i}\})-f(X_{i-1}),\ b_{i}\gets f(Y_{i-1 }\setminus\{s_{i}\})-f(Y_{i-1})\)
11:\(a^{\prime}_{i}\leftarrow\max(a_{i},0),\ b^{\prime}_{i}\leftarrow\max(b_{i},0)\)
12:if\(a^{\prime}_{i}=b^{\prime}_{i}=0\)then\(a^{\prime}_{i}/(a^{\prime}_{i}+b^{\prime}_{i})=0\)
13:with probability \(a^{\prime}_{i}/(a^{\prime}_{i}+b^{\prime}_{i})\)do:
14:\(X_{i}\gets X_{i-1}\cup\{s_{i}\},\ Y_{i}\gets Y_{i-1}\)
15:elsedo:\(X_{i}\gets X_{i-1},\ Y_{i}\gets Y_{i-1}\setminus\{s_{i}\}\)
16:return\(X_{|S|}\) (or equivalently \(Y_{|S|}\))
```

**Algorithm 3**SubsetSelection\((S)\)

Analysis.In this section, we prove the correctness of our algorithms and analyze the number of oracle queries of our algorithms, which finishes the proof of Theorems 1.2.

Consider an arbitrary time \(t\). Let \(S_{t}\) be the reported set of DynamicThresholding at time \(t\). Recall that \(V_{t}\) is the ground set at time \(t\), and we drop the \(t\) for simplicity, so we use \(V\) and \(S\) to denote \(V_{t}\) and \(S_{t}\). We first present Lemma 2.3 whose proof is given in the appendix. Then we proceed to prove Lemma 2.1 and Theorem 1.2

**Lemma 2.3**.: _Suppose that set \(S\) satisfies Property 1.b of Definition 1.1. It means that \(S\) has less than \(k\) elements and for any \(v\in V\setminus S\), the marginal gain \(\Delta(v|S)<\tau\). Then, for any arbitrary subset \(C\subseteq V\), we have \(f(S)\geq f(S\cup C)-|C|\cdot\tau\)._

Proof of Lemma 2.1: Assume that at a fixed time \(t\), \(OPT^{*}\) and \(S^{*}\) are the optimal value and an optimal solution for the submodular maximization of function \(f\) under cardinality constraint \(k\). This means that \(|S^{*}|\leq k\) and \(f(S^{*})=OPT^{*}\). Recall that \(\tau=\frac{OPT}{k(S_{1}+\frac{\alpha}{\alpha k})}\), where \(OPT\) is our guess for the optimal value. Also, by assumption we have \(OPT^{*}\in[\frac{OPT}{1+\epsilon^{\prime}},OPT]\), or equivalently \(OPT\in[OPT^{*},(1+\epsilon^{\prime})OPT^{*}]\).

To prove the lemma, we claim that \(\max(\mathbf{E}[f(S_{1})],\mathbf{E}[f(S^{\prime}_{1}

To prove the claim, we consider two cases. The first case is when \(f(S_{1}\cap S^{*})\geq\frac{\tau k}{2\alpha}\) and the second case is if \(f(S_{1}\cap S^{*})<\frac{\tau k}{2\alpha}\).

Suppose the first case is true. Then, the subset selection algorithm (either random sampling method or local search) returns \(S_{1}^{\prime}\) for which \(\mathbf{E}[f(S_{1}^{\prime})]\geq\alpha\cdot\max_{S\subseteq S_{1}}f(S)\). Since \(S_{1}\cap S^{*}\subseteq S_{1}\), we have

For the latter case, we show that \(\mathbf{E}[f(S_{1})]+\mathbf{E}[f(S_{2})]\geq(1-O(\varepsilon^{\prime}))\frac{ OPT^{*}}{3+1/2\alpha}\), inferring \(\max\left(\mathbf{E}[f(S_{1})],\mathbf{E}[f(S_{2})]\right)\geq(1-O(\varepsilon ^{\prime}))\frac{OPT^{*}}{6+1/\alpha}\). Indeed, since \(S_{1}\) and \(S_{2}\) are reported by an \(\tau\)-thresholding algorithm, if \(|S_{1}|=k\) or \(|S_{2}|=k\), then \(\max(\mathbf{E}[f(S_{1})],\mathbf{E}[f(S_{2})])\) is at least \(\tau k=\frac{OPT}{3+1/2\alpha}\) by the first property of \(\tau\)-thresholding algorithms.

Now suppose that \(|S_{1}|,|S_{2}|<k\), which means that Property 1.b of Definition 1.1 holds for \(S_{1}\) and \(S_{2}\). Therefore, we have \(f(S_{1})\geq f(S_{1}\cup S^{*})-\tau|S^{*}|\) and \(f(S_{2})\geq f(S_{2}\cup(S^{*}\setminus S_{1}))-\tau|S^{*}\setminus S_{1}|\) by Lemma 2.3. Besides, we have \(f(S_{1}\cap S^{*})-\frac{\tau k}{2\alpha}<0\). Therefore,

\[f(S_{1})+f(S_{2})\geq f(S_{1}\cup S^{*})-\tau|S^{*}|+f(S_{2}\cup(S^{*}\setminus S _{1}))-\tau|S^{*}\setminus S_{1}|+f(S_{1}\cap S^{*})-\frac{\tau k}{2\alpha}\enspace.\]

Since \(|S^{*}\setminus S_{1}|\leq|S^{*}|\leq k\) we have

\[f(S_{1})+f(S_{2})\geq f(S_{1}\cup S^{*})+f(S_{2}\cup(S^{*}\setminus S_{1}))+f( S_{1}\cap S^{*})-(2+1/(2\alpha))\tau k\enspace.\]

Since \(S_{1}\cap S_{2}=\emptyset\) and \(f\) is submodular, we have \(f(S_{1}\cup S^{*})+f(S_{2}\cup(S^{*}\setminus S_{1}))\geq f(S_{1}\cup S_{2} \cup S^{*})+f(S^{*}\setminus S_{1})\). Additionally, by the submodularity and non-negativity of \(f\), we have \(f(S_{1}\cap S^{*})\geq f(S^{*})-f(S^{*}\setminus S_{1})\), because \(f(S^{*}\setminus S_{1})+f(S_{1}\cap S^{*})\geq f(S^{*})+f(\emptyset)\). By adding the last two inequalities and using the non-negativity of \(f\) once again, we get \(f(S_{1}\cup S^{*})+f(S_{2}\cup(S^{*}\setminus S_{1}))+f(S_{1}\cap S^{*})\geq f (S_{1}\cup S_{2}\cup S^{*})+f(S^{*})\geq f(S^{*})=OPT^{*}\). By putting everything together we have,

\[f(S_{1})+f(S_{2})\geq OPT^{*}-(2+1/(2\alpha))\tau k=OPT^{*}-(\frac{4\alpha+1 }{2\alpha})(\frac{OPT(2\alpha)}{6\alpha+1}).\]

By using the assumption that \(OPT\leq(1+\epsilon^{\prime})OPT^{*}\), we have,

\[f(S_{1})+f(S_{2})\geq OPT^{*}(1-(\frac{(4\alpha+1)(1+\epsilon^{\prime})}{6 \alpha+1}))\geq OPT^{*}(\frac{2\alpha-\epsilon^{\prime}(4\alpha+1)}{6\alpha+1 })=(1-O(\varepsilon^{\prime}))\frac{\[\mathbf{E}[\sum_{t=1}^{T}Q_{t}]\in O(T\cdot\min(k\cdot g(n,k),g(n,k)^{2}))\]

Proof.: Consider the case where the expected number of oracle calls made by the thresholding algorithm DynamicThreshold per each update is \(O(g(n,k))\). Per each update, our algorithm makes an update in instance \(\mathcal{I}_{1}\) causing \(O(g(n,k))\) oracle queries. Next, we make either \(O(k)\) or \(0\) oracle queries for the SubsetSelection subroutine, depending on the used method. We also make a series of updates in instance \(\mathcal{I}_{2}\), each causing \(O(g(n,k))\) oracle queries. The number of such updates is bounded by the number of changes in the output of instance \(\mathcal{I}_{1}\), which is bounded by both \(k\) and \(O(g(n,k))\) (according to the second property of Definition 1.1). These comprise all the oracle queries made by our algorithm at time \(t\). Therefore, the given bounds for this case hold. A detailed proof for the remaining bounds is provided in the appendix. 

## 3 Empirical results

In this section, we empirically study our \((8+\varepsilon)\)-approximation dynamic algorithm. We implement our codes in C++ and run them on a MacBook laptop with \(8\) GB RAM and \(M1\) processor. We empirically study the performance of our algorithm for video summarization and the Max-Cut problem.

Video summarization.Here, we use the Determinantal Point Process (DPP) which is introduced by [42], and combine it with our algorithm to capture a video summarization. We run our experiments on YouTube and Open Video Project (OVP) datasets from [20].

For each video, we use the linear method of [33] to extract a subset of frames and find a positive semi-definite kernel \(L\) with size \(n\times n\) where \(n\) is the number of extracted frames. Then, we try to find a subset \(S\) of frames such that it maximizes \(\frac{det(L_{S})}{det(L+L)}\) where \(L_{S}\) is the sub-matrix of \(L\) restricted to indices corresponding to frames \(S\). Since \(L\) is a positive semi-definite matrix, we have \(det(L_{S})\geq 0\). Interestingly, [39] showed that \(\log(det(L_{S}))\) is a non-monotone function. We use these properties and set \(f(S):=\log(det(L_{S})+1)\) to make \(f\) a non-monotone non-negative submodular function. Then we run our \((8+\varepsilon)\)-approximate dynamic algorithm to find the best \(S\) to maximize \(f(S)\) such that \(|S|\leq k\) for \(k\in[10]\).

First, we insert all frames to observe the quality of our algorithm. Figure 1 and 2 are the selected frames by our algorithm for Video 106 from YouTube and Video 36 from OVP, respectively, when we limit the number of selected frames to \(4\). Then, we create a sequence \(\Xi\) of updates of frames of each video. Similar to [40], we define the sequence as a sliding window model. That is, given a window of size \(W\) for a parameter \(W\in\mathbb{N}\), a frame is inserted at a time \(t\) and will be alive for a window of size \(W\) and then we delete that frame.

Figure 1: Video summarization of Susan Boyle’s performance on Britain’s Got Talent show (video 106) from YouTube.

Figure 2: Video summarization for “Senses And Sensitivity, Introduct. to Lecture 1 presenter” (video 36) from OVP.

To evaluate the performance of our algorithm, we benchmark (See Figure 3) the total number of query calls and the submodular value of set \(S\) of our algorithm and the streaming algorithm proposed for non-monotone submodular maximization so-called Sample-Streaming proposed in [28]. This algorithm works as follows: Upon arrival of an element \(u\), with probability \((1-q)\), for a parameter \(0<q<1\), we ignore \(u\), otherwise (i.e., with probability \(q\)), we do the following. If the size of set \(S\) that we maintain is less than \(k\), i.e, \(|S|<k\) and \(\Delta(u|S)>0\), we add \(u\) to \(S\). However, if \(|S|=k\), we select an element \(v\in S\) for which \(\Delta(v:S)\) is minimum possible, where \(\Delta(u:S)\) equals to \(\Delta(u|S_{u})\) where \(S_{u}\) are elements that arrived before \(u\) in sequence \(\Xi\). If \(\Delta(u|S)\geq(1+c)\Delta(v:S)\) for a constant \(c\), we replace \(v\) by \(u\); otherwise, we do nothing. Now we convert this streaming algorithm into a dynamic algorithm. To accomplish this, we restart Sample-Streaming after every deletion that deletes an element of solution set \(S\) that is reported by Sample-Streaming's outputs. That is, if a deletion does not touch any element in set \(S\), we do nothing; otherwise we restart the streaming algorithm.

We run our algorithm for \(\varepsilon=k/2\) and compare the total oracle calls and average output of our algorithm and Sample-Streaming in Figure 3. To prove the approximation guarantee of our dynamic algorithm, we assumed \(\varepsilon\leq 1\). However, in practice, it is possible to increase \(\varepsilon\) up to a certain level without affecting the output of the algorithm significantly. On the other hand, increasing \(\varepsilon\) reduces the total oracle calls and makes the algorithm faster. As you can see in Figure 3 plots (b) and (d), the submodular value of our algorithm is not worse than the Sample-Streaming algorithm whose approximation factor is \(3+2\sqrt{2}\approx 5.828\) which is better than our approximation factor. Thus, our algorithm has an outcome better than our expectation, while its total oracle calls are better than Sample-Streaming algorithm (look at Figure 3 plots (a) and (c)).

We also empirically study the celebrated Max-Cut problem which is a non-monotone submodular maximization function (See [27]). These experiments are given in the appendix.

## 4 Conclusion

In this paper, we studied non-monotone submodular maximization subject to cardinality constraint \(k\) in the dynamic setting by providing a reduction from this problem to maximizing monotone submodular functions under the cardinality constraint \(k\) with a certain kind of algorithms(\(\tau\)-thresholding algorithms). Moreover, we used our reduction to develop the first dynamic algorithms for this problem. In particular, both our algorithms maintain a solution set whose submodular value is a \((8+\varepsilon)\)-approximation of the optimal value and require \(O(\varepsilon^{-3}k^{3}\log^{3}(n)\log(k))\) and \(O(\varepsilon^{-1}k^{2}\log^{3}(k))\) oracle queries per update, respectively.

## 5 Acknowledgements

This work is partially supported by DARPA QuICC NSF AF:Small #2218678, and NSF AF:Small #2114269.

Figure 3: We plot the total number of query calls and the average output of our dynamic algorithm and Sample-Streaming on video 106 from YouTube and video 36 from OVP. In this figure, from left to right, Sub-figures (a) and (b) are the total oracle calls for video 106 and 36, respectively. Similarly, Sub-figures (c) and (d) are average submodular value for video 106 and 36, respectively.

## References

* Leibniz-Zentrum fur Informatik, 2020.
* [2] Naor Alaluf, Alina Ene, Moran Feldman, Huy L. Nguyen, and Andrew Suh. An optimal streaming algorithm for submodular maximization with a cardinality constraint. _Mathematics of Operations Research_, 47(4):2667-2690, 2022.
* [3] Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency moments. _J. Comput. Syst. Sci._, 58(1):137-147, 1999.
* [4] Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Streaming submodular maximization: massive data summarization on the fly. _Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining_, 2014.
* [5] Eric Balkanski, Adam Breuer, and Yaron Singer. Non-monotone submodular maximization in exponentially fewer iterations. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, NIPS'18, page 2359-2370, Red Hook, NY, USA, 2018. Curran Associates Inc.
* [6] Kiarash Banihashem, Leyla Biabani, Samira Goudarzi, MohammadTaghi Hajiaghayi, Peyman Jabbarzade, and Morteza Monemizadeh. Dynamic algorithms for matroid submodular maximization. In _Proceedings of the 2024 ACM-SIAM Symposium on Discrete Algorithms, SODA 2024, arXiv:2306.00959_.
* [7] Kiarash Banihashem, Leyla Biabani, Samira Goudarzi, Mohammadtaghi Hajiaghayi, Peyman Jabbarzade, and Morteza Monemizadeh. Dynamic constrained submodular optimization with polylogarithmic update time. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 1660-1691. PMLR, 23-29 Jul 2023.
* [8] MohammadHossein Bateni, Mohammad Taghi Hajiaghayi, and Morteza Zadimoghaddam. Submodular secretary problem and extensions. _ACM Trans. Algorithms_, 9(4):32:1-32:23, 2013.
* [9] MohammadHossein Bateni, MohammadTaghi Hajiaghayi, and Morteza Zadimoghaddam. Submodular secretary problem and extensions. In Maria J. Serna, Ronen Shaltiel, Klaus Jansen, and Jose D. P. Rolim, editors, _Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 13th International Workshop, APPROX 2010, and 14th International Workshop, RANDOM 2010, Barcelona, Spain, September 1-3, 2010. Proceedings_, volume 6302 of _Lecture Notes in Computer Science_, pages 39-52. Springer, 2010.
* [10] Niv Buchbinder and Moran Feldman. Constrained submodular maximization via a nonsymmetric technique. _Math. Oper. Res._, 44(3):988-1005, 2019.
* [11] Niv Buchbinder, Moran Feldman, Joseph Naor, and Roy Schwartz. A tight linear time (1/2)-approximation for unconstrained submodular maximization. _SIAM J. Comput._, 44(5):1384-1402, 2015.
* [12] Niv Buchbinder, Moran Feldman, Joseph (Seffi) Naor, and Roy Schwartz. Submodular maximization with cardinality constraints. In _Proceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA '14, page 1433-1452, USA, 2014. Society for Industrial and Applied Mathematics.
* [13] Larry Carter and Mark N. Wegman. Universal classes of hash functions (extended abstract). In _Proceedings of the 9th Annual ACM Symposium on Theory of Computing, May 4-6, 1977, Boulder, Colorado, USA_, pages 106-112, 1977.

* [14] Chandra Chekuri, Shalmoli Gupta, and Kent Quanrud. Streaming algorithms for submodular function maximization. In Magnus M. Halldorsson, Kazuo Iwama, Naoki Kobayashi, and Bettina Speckmann, editors, _Automata, Languages, and Programming_, pages 318-330, Berlin, Heidelberg, 2015. Springer Berlin Heidelberg.
* 24, 2022_, pages 1685-1698. ACM, 2022.
* [16] Yixin Chen and Alan Kuhnle. Practical and parallelizable algorithms for non-monotone submodular maximization with size constraint, 2022.
* [17] Abhimanyu Das and David Kempe. Algorithms for subset selection in linear regression. In Cynthia Dwork, editor, _Proceedings of the 40th Annual ACM Symposium on Theory of Computing, Victoria, British Columbia, Canada, May 17-20, 2008_, pages 45-54. ACM, 2008.
* July 2, 2011_, pages 1057-1064. Omnipress, 2011.
* [19] Abhimanyu Das and David Kempe. Approximate submodularity and its applications: Subset selection, sparse approximation and dictionary selection. _J. Mach. Learn. Res._, 19:3:1-3:34, 2018.
* [20] Sandra Eliza Fontes de Avila, Ana Paula Brandao Lopes, Antonio da Luz Jr., and Arnaldo de Albuquerque Araujo. VSUMM: A mechanism designed to produce static video summaries and a novel evaluation method. _Pattern Recognit. Lett._, 32(1):56-68, 2011.
* [21] Delbert Dueck and Brendan J. Frey. Non-metric affinity propagation for unsupervised image categorization. In _IEEE 11th International Conference on Computer Vision, ICCV 2007, Rio de Janeiro, Brazil, October 14-20, 2007_, pages 1-8. IEEE Computer Society, 2007.
* [22] Paul Duetting, Federico Fusco, Silvio Lattanzi, Ashkan Norouzi-Fard, and Morteza Zadimoghaddam. Fully dynamic submodular maximization over matroids. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 8821-8835. PMLR, 23-29 Jul 2023.
* [23] Rick Durrett. _Probability: Theory and Examples, 4th Edition_. Cambridge University Press, 2010.
* [24] Khalid El-Arini and Carlos Guestrin. Beyond keyword search: discovering relevant scientific literature. In Chid Apte, Joydeep Ghosh, and Padhraic Smyth, editors, _Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Diego, CA, USA, August 21-24, 2011_, pages 439-447. ACM, 2011.
* [25] Ethan R. Elenberg, Rajiv Khanna, Alexandros G. Dimakis, and Sahand N. Negahban. Restricted strong convexity implies weak submodularity. _CoRR_, abs/1612.00804, 2016.
* [26] Matthew Fahrbach, Vahab S. Mirrokni, and Morteza Zadimoghaddam. Non-monotone submodular maximization with nearly optimal adaptivity and query complexity. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 1833-1842. PMLR, 2019.
* [27] Uriel Feige, Vahab S. Mirrokni, and Jan Vondrak. Maximizing non-monotone submodular functions. _SIAM J. Comput._, 40(4):1133-1153, 2011.
* [28] Moran Feldman, Amin Karbasi, and Ehsan Kazemi. Do less, get more: Streaming submodular maximization with subsampling. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 730-740, 2018.

- (extended abstract). In Luca Aceto, Monika Henzinger, and Jiri Sgall, editors, _Automata, Languages and Programming
- 38th International Colloquium, ICALP 2011, Zurich, Switzerland, July 4-8, 2011, Proceedings, Part I_, volume 6755 of _Lecture Notes in Computer Science_, pages 342-353. Springer, 2011.
* [30] Satoru Fujishige. Theory of submodular programs: A fenchel-type min-max theorem and subgradients of submodular functions. _Math. Program._, 29(2):142-155, 1984.
* [31] Shayan Oveis Gharan and Jan Vondrak. Submodular maximization by simulated annealing. In _Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA '11, page 1098-1116, USA, 2011. Society for Industrial and Applied Mathematics.
* [32] Shayan Oveis Gharan and Jan Vondrak. Submodular maximization by simulated annealing. In Dana Randall, editor, _Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2011, San Francisco, California, USA, January 23-25, 2011_, pages 1098-1116. SIAM, 2011.
* [33] Boqing Gong, Wei-Lun Chao, Kristen Grauman, and Fei Sha. Diverse sequential subset selection for supervised video summarization. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, _Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada_, pages 2069-2077, 2014.
* 6th International Workshop, WINE 2010, Stanford, CA, USA, December 13-17, 2010. Proceedings_, volume 6484 of _Lecture Notes in Computer Science_, pages 246-257. Springer, 2010.
* [35] Jason Hartline, Vahab Mirrokni, and Mukund Sundararajan. Optimal marketing strategies over social networks. In _Proceedings of the 17th international conference on World Wide Web_, pages 189-198, 2008.
* [36] Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation. _J. ACM_, 53(3):307-323, 2006.
* [37] Ehsan Kazemi, Morteza Zadimoghaddam, and Amin Karbasi. Scalable deletion-robust submodular maximization: Data summarization with privacy and fairness constraints. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 2549-2558. PMLR, 2018.
* [38] Rajiv Khanna, Ethan R. Elenberg, Alexandros G. Dimakis, Sahand N. Negahban, and Joydeep Ghosh. Scalable greedy feature selection via weak submodularity. In Aarti Singh and Xiaojin (Jerry) Zhu, editors, _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA_, volume 54 of _Proceedings of Machine Learning Research_, pages 1560-1568. PMLR, 2017.
* [39] Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. _Found. Trends Mach. Learn._, 5(2-3):123-286, 2012.
* [40] Silvio Lattanzi, Slobodan Mitrovic, Ashkan Norouzi-Fard, Jakub Tarnawski, and Morteza Zadimoghaddam. Fully dynamic algorithm for constrained submodular optimization. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* San Diego, CA, USA_, volume 69 of _OASICS_, pages 18:1-18:10. Schloss Dagstuhl
- Leibniz-Zentrum fur Informatik, 2019.
* [42] Odile Macchi. The coincidence approach to stochastic point processes. _Advances in Applied Probability_, 7(1):83-122, 1975.
* [43] Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, and Amin Karbasi. Fast constrained submodular maximization: Personalized data summarization. In Maria Florina Balcan and Kilian Q. Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 1358-1367, New York, New York, USA, 20-22 Jun 2016. PMLR.
* [44] Baharan Mirzasoleiman, Stefanie Jegelka, and Andreas Krause. Streaming non-monotone submodular maximization: Personalized video summarization on the fly. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018_, pages 1379-1386. AAAI Press, 2018.
* [45] Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Deletion-robust submodular maximization: Data summarization with "the right to be forgotten". In Doina Precup and Yee Whye Teh, editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 2449-2458. PMLR, 2017.
* [46] Morteza Monemizadeh. Dynamic submodular maximization. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [47] Ashkan Norouzi-Fard, Jakub Tarnawski, Slobodan Mitrovic, Amir Zandieh, Aidasadat Mousavifar, and Ola Svensson. Beyond 1/2-approximation for submodular maximization on massive data streams. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 3826-3835. PMLR, 2018.
* [48] Binghui Peng. Dynamic influence maximization. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 10718-10731, 2021.
* [49] Sharon Qian and Yaron Singer. _Fast Parallel Algorithms for Statistical Subset Selection Problems_. Curran Associates Inc., Red Hook, NY, USA, 2019.
* [50] Ian Simon, Noah Snavely, and Steven M. Seitz. Scene summarization for online image collections. In _IEEE 11th International Conference on Computer Vision, ICCV 2007, Rio de Janeiro, Brazil, October 14-20, 2007_, pages 1-8. IEEE Computer Society, 2007.
* November 02, 2012_, pages 754-763. ACM, 2012.
* [52] Sebastian Tschiatschek, Rishabh K Iyer, Haochen Wei, and Jeff A Bilmes. Learning mixtures of submodular functions for image collection summarization. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc., 2014.