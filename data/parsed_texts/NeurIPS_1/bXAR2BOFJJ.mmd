# Bayesian Nonparametric Learning using the

Maximum Mean Discrepancy Measure for Synthetic Data Generation

Forough Fazeli Asl

Department of Mathematical and Statistical Sciences

University of Alberta

Edmonton, Canada

fazelias@ualberta.ca

&Michael Minyi Zhang

Department of Statistics and Actuarial Science

University of Hong Kong

Hong kong, China

mzhang18@hku.hk

&Lizhen Lin

Department of Mathematics

The University of Maryland

College Park, MD, USA

lizhen01@umd.edu

Corresponding author

###### Abstract

We introduce a Bayesian estimator for maximum mean discrepancy (MMD), enabling a novel approach to measure-based data generation. To demonstrate the adaptability of our method, we embed this estimator within a generative adversarial network (GAN) framework. This integration offers a powerful avenue for Bayesian nonparametric (BNP) learning, showcasing the estimator's broad applicability. Our BNP-driven GAN not only enhances sample diversity but also improves inferential accuracy, surpassing the performance of traditional methods. Further theoretical properties, proofs, and experiments are given by the Appendix.

## 1 Introduction

Data augmentation is the technique of generating synthetic data, often to train machine learning models when the data are scarce or the model is non-robust to perturbations in the data. When the likelihood is intractable to compute, evaluating the model's fit can be difficult. Maximum mean discrepancy (MMD) addresses this problem by enabling comparisons between distributions without explicit likelihoods through its feature matching properties. This property ensures that generated data matches the features of real data, making MMD an effective tool for evaluating and developing deep generative models. Bayesian nonparametric methods are a powerful tool with strong theoretical justifications but they have seen limited applications in MMD estimation. A key advantage of the Bayesian approach is that it incorporates expert knowledge through a prior distribution, offering regularization by introducing uncertainty in the sampling distribution via the Dirichlet process (DP). The absence of such methods in MMD estimation restricts statisticians who prefer Bayesian frameworks without making strong assumptions. This paper addresses this gap.

We propose a BNP estimator to accurately estimate the MMD between a parametric model and an unknown distribution by placing a DP prior on the unknown distribution. We extend the bootstrap method from [1] beyond posterior parameter inference, applying our estimator to training a generative adversarial network (GAN). Our approach uses the MMD estimator as a robust discriminator,combining MMD measurement with BNP inference to enhance GAN training, reduce mode collapse, and improve generator performance compared to frequentist (FNP) methods.

## 2 Previous work

Previous work on simulation-based inference has mainly used discrepancy measures from a frequentist nonparametric (FNP) perspective. Notably, GANs have been extensively explored in data augmentation and medical data synthesis where the fake images created by the GAN are used to supplement the training data [2; 3; 4; 5; 6]. A standard GAN features two neural networks: the generator \(\{G_{\mathbf{\omega}}\}_{\mathbf{\omega}\in\mathcal{V}}\) and the discriminator \(\{D_{\mathbf{\theta}}\}_{\mathbf{\theta}\in\mathbf{\Theta}}\)[7]. The generator tries to fool the discriminator into misidentifying generated samples as real. However, these models are expanded beyond the classic loss function, which could potentially introduce challenges such as mode collapse-memorizing certain modes of data distribution while overlooking other diversities-and training instability.

A Bayesian approach, known as approximate Bayesian computation (ABC), estimates model parameters through simulation by comparing summary statistics of simulated and observed data [8]. ABC faces challenges in selecting informative summary statistics, which affects the accuracy of posterior inference [9; 10]. One particularly attractive choice of statistic is to use the MMD [11]. As the threshold decreases, ABC converges to the standard Bayesian posterior, which can be sensitive to model misspecification and lacks robustness [1].

To improve robustness, generalized Bayesian inference (GBI) replaces the likelihood with a robust loss function [12]. Example applications of GBI include using MMD in pseudo-likelihood approaches [13] or stochastic gradient MCMC for posterior inference [14]. Despite these advancements, GBI's sensitivity to hyperparameters and the computational demands of MCMC remain challenges [1]. To address these issues, an MMD posterior bootstrap method has been developed, offering a more efficient alternative [1; 15; 16; 17].

## 3 Dirichlet process

To perform Bayesian nonparametric learning (BNPL), we first must take samples from the Dirichlet process. The DP is an infinite generalization of the Dirichlet distribution that is considered on the sample space denoted as \(\mathfrak{X}\), which possesses a \(\sigma\)-algebra \(\mathcal{A}\) comprising subsets of \(\mathfrak{X}\)[18]. \(F\) follows a DP with parameters \((a,H)\) with the notation \(F^{\text{Pri}}:=(F\sim DP(a,H))\), if for any measurable partition \(A_{1},\ldots,A_{k}\) of \(\mathfrak{X}\) with \(k\geq 2\), the joint distribution of the vector \((F(A_{1}),\ldots,F(A_{k}))\) follows a Dirichlet distribution characterized by parameters \((aH(A_{1}),\ldots,aH(A_{k}))\). Moreover, it is assumed that \(H(A_{j})=0\) implies \(F(A_{j})=0\) with probability one. The base measure \(H\) captures the prior knowledge regarding the data distribution, while \(a\) signifies the strength or intensity of this knowledge.

As a conjugate prior, the posterior distribution of \(F\) also follows a DP, denoted by \(F^{\text{Pos}}:=(F|\mathbf{X}_{1:n}\sim\text{DP}(a+n,H^{*}))\), for \(n\) independent and identically distributed (IID) draws, \(\big{(}\mathbf{X}_{1:n}\in\mathbb{R}^{d}\big{)}\), from the random probability measure \(F\) where \(H^{*}=a(a+n)^{-1}H+n(a+n)^{-1}F_{\mathbf{X}_{1:n}}\), and \(F_{\mathbf{X}_{1:n}}\) represents the empirical cumulative distribution function of the sample \(\mathbf{X}_{1:n}\).

To sample from the DP posterior, we use a finite approximation devised by Ishwaran and Zarepour [19], which allows for convenient simulation. In the context of posterior inference, this approximation is given by

\[F_{N}^{\text{Pos}}:=\sum_{i=1}^{N}J_{i,N}^{\text{Pos}}\delta_{\mathbf{V}_{i}^ {\text{Pos}}}, \tag{1}\]

where \(\left(J_{1:N,N}^{\text{Pos}}\right)\sim\text{Dirichlet}((a+n)/N,\ldots,(a+n)/N)\), \(\big{(}\mathbf{V}_{1:N}^{\text{Pos}}\big{)}\overset{\text{IID}}{\sim}H^{*}\), and \(\delta_{\mathbf{V}^{\text{Pos}}}\) is the Dirac delta measure. In this study, the variables \(J_{i,N}^{\text{Pos}}\) and \(\mathbf{V}_{i}^{\text{Pos}}\) represent the DP's weight and location, respectively. The sequence \((F_{N}^{\text{Pos}})_{N\geq 1}\) converges in distribution to \(F^{\text{Pos}}\), where \(F_{N}^{\text{Pos}}\) and \(F^{\text{Pos}}\) are random values in \(M_{1}(\mathbb{R}^{d})\), the space of probability measures on \(\mathbb{R}^{d}\) endowed with the topology of weak convergence [19]. Although the stick-breaking representation is a commonly employed series representation for DP inference [20], it lacks the necessary normalization terms to convert it into a probability measure [21]. Additionally, simulating from an infinite series is only feasible through using a random truncation approach to handle the terms within the series. In the subsequent sections, we investigate the efficacy of this approximation within a regularization method in a BNP generative model.

## 4 DPMD-GAN: A Bayesian Nonparametric Learning in Data Generation

In our BNPL method, we define a DP prior on \(F\), leading to a DP posterior on \(F\) given the data. The key idea is that any posterior on the generator's parameter space \(\mathcal{W}\) can be derived by mapping \(F^{pos}\) approximately through the push-forward measure

\[\boldsymbol{\omega}^{*}(F^{pos}):=\arg\min_{\boldsymbol{\omega}\in\mathcal{W}} \mathrm{MMD}_{\mathrm{BNP}}(F_{N}^{pos},F_{G_{\boldsymbol{\omega}}}), \tag{2}\]

where for a given sample \(\mathbf{Y}_{1:m}\sim F_{G_{\boldsymbol{\omega}}}\) the posterior-based MMD estimator is defined by

\[\mathrm{MMD}_{\mathrm{BNP}}^{2}(F_{1,N}^{pos},F_{2,m})=\sum_{\ell,t=1}^{N}J_{ \ell,N}^{*}J_{t,N}^{*}k(\mathbf{V}_{\ell}^{*},\mathbf{V}_{t}^{*})-\frac{2}{m} \sum_{\ell=1}^{N}\sum_{t=1}^{m}J_{\ell,N}^{*}k(\mathbf{V}_{\ell}^{*},\mathbf{ Y}_{t})+\frac{1}{m^{2}}\sum_{\ell,t=1}^{m}k(\mathbf{Y}_{\ell},\mathbf{Y}_{t}). \tag{3}\]

In this context, the discriminator \(D\) can be viewed as a black box that uses the MMD estimator to differentiate between the real and fake data, reducing the computational cost compared to a neural network-based discriminator. We discuss some of the properties of estimator (3) in the Appendix. Let \(\boldsymbol{\omega}^{*}\) be the optimal parameter of \(G_{\boldsymbol{\omega}}\) that minimizes \(\mathrm{MMD}_{\mathrm{BNP}}(F_{N}^{pos},F_{G_{\boldsymbol{\omega}},m})\). Since \(\mathrm{MMD}_{\mathrm{BNP}}(F_{N}^{pos},F_{G_{\boldsymbol{\omega}},m})\) serves as a BNP estimation of (5), it is crucial to evaluate this estimation's accuracy, focusing on the GAN's ability to generate realistic samples (generalization error) and handle outliers (robustness). Lemma 2 in the appendix addresses these aspects. While the previous statements provide upper bounds for the MMD estimator's expectation, the next lemma offers stochastic bounds on the estimation error to assess posterior consistency.

**Lemma 1**: _Building upon the general assumptions stated in Lemma 2, for a given sample \(\mathbf{X}_{1},\ldots,\mathbf{X}_{n}\) from distribution \(F\) in the probability space \((\mathfrak{X},\mathcal{A},\mathrm{Pr})\) and any \(\epsilon>0\), \(i.\quad\Pr\left(|\mathrm{MMD}(F_{N}^{pos},F_{G_{\boldsymbol{\omega}^{*}},m})- \mathrm{MMD}(F,F_{G_{\boldsymbol{\omega}^{\prime}}})|\geq h(n,m,K,\epsilon)+| \Delta_{1}|+|\Delta_{2}|\right)\leq 2\exp\frac{-\epsilon^{2}nm}{2K(n+m)},\)_

\[ii.\Pr\left(\mathrm{MMD}(F,F_{G_{\boldsymbol{\omega}^{*}}})>\epsilon\right) \leq\frac{1}{\epsilon}\left(\mathrm{MMD}(F,F_{G_{\boldsymbol{\omega}^{\prime }}})+\frac{2K}{\sqrt{n}}+\frac{4aK}{a+n}+2\sqrt{\frac{(a+n+N)K}{(a+n+1)N}}.\right),\]

_where, \(h(n,m,K,\epsilon)=2\sqrt{K}(\sqrt{n}+\sqrt{m})/\sqrt{nm}+\epsilon\), \(\Delta_{1}=\mathrm{MMD}(F_{N}^{pos},F_{G_{\boldsymbol{\omega}^{*}}})-\mathrm{ MMD}(F_{n},F_{G_{\boldsymbol{\omega}^{\prime}},m})\), and \(\Delta_{2}=\mathrm{MMD}(F,F_{G_{\boldsymbol{\omega}^{*}}})-\mathrm{MMD}(F,F_{G_ {\boldsymbol{\omega}^{\prime}}})\)._

A direct consequence of Lemma 1(ii) is that for a fixed value of \(a\), \(\Pr(\mathrm{MMD}(F,F_{G_{\boldsymbol{\omega}^{*}}})\geq\epsilon)\to 0\), as \(n\rightarrow\infty\) and \(N\rightarrow\infty\), for any \(\epsilon>0\), when \(\mathrm{MMD}(F,F_{G_{\boldsymbol{\omega}^{\prime}}})=0\) (well-specified case). This implies \(F_{G_{\boldsymbol{\omega}^{*}}}\) converges in probability to the data distribution \(F\) as the sample size increases in well-specified cases. A detailed guide on choosing DP hyperparameters and kernel settings is provided in the Appendix.

## 5 Experimental results

We consider the MNIST dataset including handwritten digits with 10 modes, bone marrow biopsy (BMB) histopathology, Labeled Faces in the Wild (LFW) Dataset, and brain MRI images to analyze the model performance. All data description are given by the Appendix. Following the design choices of [22], we use the Gaussian neural network for the generator with four hidden layers each having rectified linear units activation function and a sigmoid function for the output layer. We also set mini-batch sizes to be \(n_{mb}=1,000\) and use a mixture of six Gaussian kernels corresponding to the bandwidth parameters \(2,5,10,20,40,\) and \(80\) to train networks in \(40,000\) iterations. We generate samples from the trained BNP GAN using Algorithm 2 from the Appendix, as depicted in Figure 1-Row 2. The results of [22] are also presented by Figure 1-Row 3, as the frequentist counterpart of our BNP procedure. Based on these preliminary results, we can see that our generated images can, at least, replicate the results of [22] and in some cases produce sharper images. This result can also be deduced from the presented scores of MMD, Kernel Inception Distance (KID) [23] and FrechetInception Distance (FID) [24] in Table 1. The low scores suggest better performance, as smaller values indicate closer similarity to real images.

Conversely, our results show that the BNP GAN with a mixture of Gaussian kernels outperforms the single Gaussian kernel approach. To explore this further, Figure 2 in the Appendix presents samples from the trained generator using various \(\sigma\) values and the median heuristic \(\sigma_{MH}\). Note that \(\sigma_{MH}\) is updated in each iteration, so no specific value is reported. Although higher \(\sigma\) increases image diversity, the resolution remains below that achieved with the mixture kernel.

To assess whether the proposed discriminator used in the BNP GAN leads to faster or better convergence of the generated samples compared to the baseline proposed by [22], we consider the synthetic distribution \(\frac{1}{2}N(-\mathbf{1}_{d},I_{d})+\frac{1}{2}N(\mathbf{1}_{d},I_{d})\) as the true distribution and provide the corresponding MMD values for both models over 20,000 iterations in the data generation process, as shown in Figure 3 of Appendix. Our proposed GAN clearly displays a higher speed of convergence for the corresponding cost function to zero, and thus better performance compared to the baseline.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Scores & \multicolumn{8}{c}{Dataset} \\ \cline{2-9}  & \multicolumn{2}{c}{MNIST} & \multicolumn{2}{c}{BMB} & \multicolumn{2}{c}{LFW} & \multicolumn{2}{c}{MRI} \\ \cline{2-9}  & BNP & FNP & BNP & FNP & BNP & FNP & BNP & FNP \\ \hline MMD & \(0.0384\) & \(0.0404\) & \(0.0285\) & \(0.0315\) & \(0.0281\) & \(0.0302\) & \(0.2059\) & \(0.2231\) \\ KID & \(0.0034\) & \(0.0046\) & \(0.0030\) & \(0.0036\) & \(0.0019\) & \(0.0026\) & \(0.0260\) & \(0.0264\) \\ FID & \(35.560\) & \(37.934\) & \(17.006\) & \(17.264\) & \(14.010\) & \(14.473\) & \(87.975\) & \(87.831\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: The values of MMD, KID, and FID scores for four groups of datasets.

Figure 1: Generated samples from training dataset (Row 1), BNP model (Row 2), and FNP model (Row 3).

Concluding remarks

Our BNP approach effectively estimates the MMD between an unknown and an intractable parametric distribution, showing promise in training GANs by using the estimator as a discriminator to induce a posterior on the generator's parameters. The stick-breaking representation, however, lacks normalization and shows stochastic decrease, making it inefficient for simulating from a DP [21]. Exploring alternative DP approximations for MMD estimation is a promising direction for future research. Future work will also focus on generating 3D medical images to further improve results.

## References

* [1] C. Dellaporta, J. Knoblauch, T. Damoulas, and F.-X. Briol, "Robust Bayesian inference for simulator-based models via the MMD posterior bootstrap," in _International Conference on Artificial Intelligence and Statistics_. PMLR, 2022, pp. 943-970.
* [2] J. Mendes, T. Pereira, F. Silva, J. Frade, J. Morgado, C. Freitas, E. Negrao, B. F. de Lima, M. C. da Silva, A. J. Madureira _et al._, "Lung CT image synthesis using GANs," _Expert Systems with Applications_, vol. 215, p. 119350, 2023.
* [3] S. Menon, J. Mangalagiri, J. Galita, M. Morris, B. Saboury, Y. Yesha, Y. Yesha, P. Nguyen, A. Gangopadhyay, and D. Chapman, "CCS-GAN: COVID-19 CT scan generation and classification with very few positive training images," _Journal of Digital Imaging_, pp. 1-14, 2023.
* [4] R. Toda, A. Teramoto, M. Tsujimoto, H. Toyama, K. Imaizumi, K. Saito, and H. Fujita, "Synthetic CT image generation of shape-controlled lung cancer using semi-conditional InfoGAN and its applicability for type classification," _International Journal of Computer Assisted Radiology and Surgery_, vol. 16, pp. 241-251, 2021.
* [5] J. M. Wolterink, T. Leiner, M. A. Viergever, and I. Isgum, "Generative adversarial networks for noise reduction in low-dose CT," _IEEE Transactions on Medical Imaging_, vol. 36, no. 12, pp. 2536-2545, 2017.
* [6] T. Bu, Z. Yang, S. Jiang, G. Zhang, H. Zhang, and L. Wei, "3D conditional generative adversarial network-based synthetic medical image augmentation for lung nodule detection," _International Journal of Imaging Systems and Technology_, vol. 31, no. 2, pp. 670-681, 2021.
* [7] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial nets," _Advances in Neural Information Processing Systems_, vol. 27, pp. 2672-2680, 2014.
* [8] M. A. Beaumont, W. Zhang, and D. J. Balding, "Approximate Bayesian computation in population genetics," _Genetics_, vol. 162, no. 4, pp. 2025-2035, 2002.
* [9] M. G. Blum and O. Francois, "Non-linear regression models for approximate Bayesian computation," _Statistics and computing_, vol. 20, pp. 63-73, 2010.
* [10] K. Csillery, O. Francois, and M. G. Blum, "abc: an r package for approximate Bayesian computation (abc)," _Methods in ecology and evolution_, vol. 3, no. 3, pp. 475-479, 2012.
* [11] M. Park, W. Jitkrittum, and D. Sejdinovic, "K2-ABC: Approximate Bayesian computation with kernel embeddings," in _Artificial Intelligence and Statistics_. PMLR, 2016, pp. 398-407.
* [12] J. Jewson, J. Q. Smith, and C. Holmes, "Principles of bayesian inference using general divergence criteria," _Entropy_, vol. 20, no. 6, p. 442, 2018.
* [13] B.-E. Cherief-Abdellatif and P. Alquier, "MMD-Bayes: Robust Bayesian estimation via maximum mean discrepancy," in _Symposium on Advances in Approximate Bayesian Inference_. PMLR, 2020, pp. 1-21.
* [14] L. Pacchiardi and R. Dutta, "Generalized Bayesian likelihood-free inference using scoring rules estimators," _arXiv preprint arXiv:2104.03889_, 2021.
* [15] S. Lyddon, S. Walker, and C. C. Holmes, "Nonparametric learning from Bayesian models with randomized objective functions," _Advances in Neural Information Processing Systems_, vol. 31, 2018.
* [16] S. P. Lyddon, C. Holmes, and S. Walker, "General Bayesian updating and the loss-likelihood bootstrap," _Biometrika_, vol. 106, no. 2, pp. 465-478, 2019.

* [17] E. Fong, S. Lyddon, and C. Holmes, "Scalable nonparametric sampling from multimodal posteriors with the posterior bootstrap," in _International Conference on Machine Learning_. PMLR, 2019, pp. 1952-1962.
* [18] T. S. Ferguson, "A Bayesian analysis of some nonparametric problems," _Annals of Statistics_, vol. 1, no. 2, pp. 209-230, 1973.
* [19] H. Ishwaran and M. Zarepour, "Exact and approximate sum representations for the Dirichlet process," _Canadian Journal of Statistics_, vol. 30, no. 2, pp. 269-283, 2002.
* [20] J. Sethuraman, "A constructive definition of Dirichlet priors," _Statistica Sinica_, pp. 639-650, 1994.
* [21] M. Zarepour and L. Al-Labadi, "On a rapid simulation of the Dirichlet process," _Statistics & Probability Letters_, vol. 82, no. 5, pp. 916-924, 2012.
* [22] Y. Li, K. Swersky, and R. Zemel, "Generative moment matching networks," in _International Conference on Machine Learning_. PMLR, 2015, pp. 1718-1727.
* [23] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton, "Demystifying MMD GANs," in _International Conference on Learning Representations_, 2018.
* [24] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, "Gans trained by a two time-scale update rule converge to a local nash equilibrium," _Advances in neural information processing systems_, vol. 30, 2017.
* [25] A. Muller, "Integral probability metrics and their generating classes of functions," _Advances in applied probability_, vol. 29, no. 2, pp. 429-443, 1997.
* [26] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Scholkopf, and A. Smola, "A kernel two-sample test," _Journal of Machine Learning Research_, vol. 13, no. 1, pp. 723-773, 2012.
* [27] A. Terenin and D. Draper, "A noninformative prior on a space of distribution functions," _Entropy_, vol. 19, no. 8, p. 391, 2017.
* [28] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani, "Training generative neural networks via maximum mean discrepancy optimization," in _Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence_, 2015, pp. 258-267.
* [29] C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Poczos, "MMD-GAN: Towards deeper understanding of moment matching network," _Advances in Neural Information Processing Systems_, vol. 30, 2017.
* [30] F. Zhao, C. Lei, Q. Zhao, H. Yang, G. Ling, J. Liu, H. Zhou, and H. Wang, "Predicting the property contour-map and optimum composition of Cu-Co-Si alloys via machine learning," _Materials Today Communications_, vol. 30, p. 103138, 2022.
* [31] M. G. Genton, "Classes of kernels for machine learning: A statistics perspective," _Journal of machine learning research_, vol. 2, no. Dec, pp. 299-312, 2001.
* [32] B. Scholkopf, A. J. Smola, F. Bach _et al._, _Learning with kernels: Support vector machines, regularization, optimization, and beyond_. MIT Press, 2002.
* [33] W. Jitkrittum, Z. Szabo, K. P. Chwialkowski, and A. Gretton, "Interpretable distribution features with maximum testing power," _Advances in Neural Information Processing Systems_, vol. 29, 2016.
* [34] A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, K. Fukumizu, and B. K. Sriperumbudur, "Optimal kernel choice for large-scale two-sample tests," _Advances in Neural Information Processing Systems_, vol. 25, 2012.
* [35] D. J. Sutherland, H.-Y. Tung, H. Strathmann, S. De, A. Ramdas, A. Smola, and A. Gretton, "Generative models and model criticism via optimized maximum mean discrepancy," _arXiv preprint arXiv:1611.04488_, 2016.
* [36] A. Schrab, I. Kim, M. Albert, B. Laurent, B. Guedj, and A. Gretton, "Mmd aggregated two-sample test," _arXiv preprint arXiv:2110.15073_, 2021.
* [37] A. Schrab, I. Kim, B. Guedj, and A. Gretton, "Efficient aggregated kernel tests using incomplete \(u\)-statistics," _Advances in Neural Information Processing Systems_, vol. 35, pp. 18 793-18 807, 2022.

* [38] P. J. Huber, "Robust estimation of a location parameter," in _Breakthroughs in statistics: Methodology and distribution_. Springer, 1992, pp. 492-518.
* [39] B.-E. Cherief-Abdellatif and P. Alquier, "Finite sample properties of parametric MMD estimation: Robustness to misspecification and dependence," _Bernoulli_, vol. 28, no. 1, pp. 181-213, 2022.
* [40] J. M. Tomczak and M. Welling, "Improving variational auto-encoders using householder flow," _arXiv preprint arXiv:1611.09630_, 2016.
* [41] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller, "Labeled faces in the wild: A database for studying face recognition in unconstrained environments," in _Workshop on Faces in 'Real-Life' Images: Detection, Alignment, and Recognition_, 2008.

## Appendix A Maximum mean discrepancy distance

For a given data space \(\mathfrak{X}\), consider the random variables \(\mathbf{X}\) and \(\mathbf{Y}\), drawn from distributions \(F_{1}\) and \(F_{2}\) respectively. Here, \(F_{1}\) and \(F_{2}\) belong to \(\mathcal{B}(\mathfrak{X})\), which represents the set of Borel probability distributions on \(\mathfrak{X}\). We consider the discrepancy \(d:\mathcal{B}(\mathfrak{X})\times\mathcal{B}(\mathfrak{X})\rightarrow[0,\infty)\) through the integral pseudo-probability metric (IPM) [25], defined as shown in (4). The class of functions \(\mathcal{F}\) is designed to be rich enough to distinguish between \(F_{1}\) and \(F_{2}\), and restrictive enough to provide accurate estimates based on a finite sample.

\[d_{\text{IPM}}(F_{1},F_{2})=\sup_{h\in\mathcal{F}}|E_{F_{1}}(h(\mathbf{X}))-E_ {F_{2}}(h(\mathbf{Y})))|. \tag{4}\]

The MMD is then defined by considering \(\mathcal{F}=\{h\in\mathcal{H}_{k}|\,||h||_{\mathcal{H}_{k}}\leq 1\}\), which represents a unit ball in a reproducing kernel Hilbert space (RKHS) \(\mathcal{H}_{k}\) with associated kernel \(k:\mathfrak{X}\times\mathfrak{X}\rightarrow\mathbb{R}\). In this context, \(||\cdot||_{\mathcal{H}_{k}}\) denotes the norm function in the RKHS. The function \(k(\cdot,\cdot)\), is positive definite, such that for any function \(h\in\mathcal{H}_{k}\) and any \(\mathbf{X}\in\mathfrak{X}\), \(h(\mathbf{X})=\langle h,k(\mathbf{X},\cdot)\rangle_{\mathcal{H}_{k}}\), where \(\langle\cdot,\cdot\rangle_{\mathcal{H}_{k}}\) represents the inner product in \(\mathcal{H}_{k}\). Consider function \(\mu_{F_{1}}(\cdot)=E_{F_{1}}\,[k(\mathbf{X},\cdot)]\in\mathcal{H}_{k}\), which is defined as the kernel mean embedding of the distribution \(F_{1}\) in [26]. Then, for given \(\mathbf{X},\mathbf{X}^{\prime}\overset{i.i.d.}{\sim}F_{1},\mathbf{Y},\mathbf{ Y}^{\prime}\overset{i.i.d.}{\sim}F_{2}\), if \(E_{F}(\sqrt{k(\mathbf{X},\mathbf{X})})<\infty\) for all \(F\in\mathcal{B}(\mathfrak{X})\), the MMD is given by

\[\mathrm{MMD}^{2}(F_{1},F_{2})=||\mu_{F_{1}}-\mu_{F_{2}}||_{\mathcal{H}_{k}}^{ 2}=E_{F_{1}}[k(\mathbf{X},\mathbf{X}^{\prime})]-2E_{F_{1},F_{2}}[k(\mathbf{X},\mathbf{Y})]+E_{F_{2}}[k(\mathbf{Y},\mathbf{Y}^{\prime})]. \tag{5}\]

Note that \(\mathrm{MMD}^{2}(F_{1},F_{2})=0\) if and only if \(F_{1}=F_{2}\), when \(\mathcal{H}_{k}\) is a _universal_ RKHS defined on a _compact_ metric space \(\mathfrak{X}\) and \(k(\cdot,\cdot)\) is _continuous_[26, Theorem 5]. In practice, distributions \(F_{1}\) and \(F_{2}\) are not accessible, and then the biased, empirical estimator of (5) (V-statistic) is calculated using empirical distributions \(F_{1,n}\) and \(F_{2,m}\) as

\[\mathrm{MMD}^{2}(F_{1,n},F_{2,m})=\frac{1}{n^{2}}\sum_{i,j=1}^{n}k(\mathbf{X}_ {i},\mathbf{X}_{j})-\frac{2}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m}k(\mathbf{X}_{i}, \mathbf{Y}_{j})+\frac{1}{m^{2}}\sum_{i,j=1}^{m}k(\mathbf{Y}_{i},\mathbf{Y}_{j }), \tag{6}\]

where \(\mathbf{X}_{1},\ldots,\mathbf{X}_{n}\) is a sample from \(F_{1}\) and \(\mathbf{Y}_{1},\ldots,\mathbf{Y}_{m}\) is a sample generated from \(F_{2}\).

## Appendix B Choosing DP hyperparameters

In the context of approximating the posterior on the parameter space, the prior choice for \(F\) and determining the strength of belief becomes challenging. We consider a small value for \(a\) as a non-informative prior, following the suggestion by [1], thanks to its broad ability to characterize uncertainty [27]. However, it's important to note that setting \(a=0\) as done by [1] is not always well-defined mathematically, as the DP is only defined for \(a>0\). Therefore, we opt for \(a=10^{-6}\). In this case, the DP posterior remains invariant to the choice of \(H\).

## Appendix C Kernel Settings

In our method, we choose to use the standard radial basis function (RBF) kernel as its feature space corresponds to a universal RKHS. [28, 22] and [29] used the Gaussian kernel in training MMD-GANs because of its simplicity and good performance. [28] also evaluated some other RBF kernels such as the Laplacian and rational quadratic kernels to compare the results of the MMD-GANs with those obtained based on using Gaussian kernels. They found the best performance by applying the Gaussian kernel in the MMD cost function.

Hence, we consider the Gaussian kernel function in our proposed procedure. To choose the bandwidth parameter \(\sigma\), we follow the idea of considering a set of fixed values of \(\sigma\)'s such as \(\{\sigma_{1},\ldots,\sigma_{T}\}\), then compute the mixture of Gaussian kernels \(k(\cdot,\cdot)=\sum_{t=1}^{T}k_{G_{\sigma_{t}}}(\cdot,\cdot)\), to consider in (3). For each \(\sigma(t)\), \(0\leq k_{G_{\sigma_{t}}}(\cdot,\cdot)\leq 1\); hence, \(0\leq k(\cdot,\cdot)\leq T\), which satisfies the theoretical results presented in the paper. As it is mentioned in [22], this choice reflects a good performance in training MMD-GANs.

### Radial Basis Function Kernels Family

The construction of MMD-based procedures is proposed based on considering a kernel function with feature space corresponding to a universal RKHS. The radial basis function (RBF) kernel is the most well-known kernel family satisfying the above situation. For two vectors \(\mathbf{X},\mathbf{Y}\in\mathbb{R}^{d}\), the RBF kernel is represented by

\[k(\mathbf{X},\mathbf{Y})=h(||\mathbf{X}-\mathbf{Y}||/\sigma),\]

where, \(h\) is a function from the positive real numbers \(\mathbb{R}^{+}\) to \(\mathbb{R}^{+}\), \(||\cdot||\) represents the \(L^{2}\)-norm, and \(\sigma\) is the bandwidth parameter that indicates the kernel size. There are many functions assigned to \(h\), for example, the Gaussian, exponential, rational quadratic kernels, and Matern, represented by

\[h_{1}(x)=\exp{(-\frac{x^{2}}{2})},\:h_{2}(x)=\exp{(-x)},\:h_{3}(x)=\left(1+ \frac{x^{2}}{2\alpha}\right)^{-\alpha},\:h_{4}(x)=(1+\sqrt{2\nu}x)e^{-\sqrt{2 \nu}x},\]

respectively; where, \(\alpha\) in \(h_{3}\) is a positive-valued scale-mixture parameter, and the \(\nu\) in \(h_{4}\) is a parameter that controls the smoothness of the kernel results [30, 31].

One of the simplest kernel functions above is the Gaussian kernel, which is mostly used in machine learning problems and only depends on bandwidth parameter \(\sigma\). The Gaussian kernel tends to 0 and 1 when \(\sigma\to 0\) and \(\sigma\rightarrow\infty\), respectively. Both situations lead to \(\mathrm{MMD}^{2}\) being zero. Hence, the choice of the parameter \(\sigma\) has a crucial effect on the performance of this kernel. Numerous methods are proposed to choose the value of \(\sigma\), however, there is no definitive optimization method for this problem. The median heuristic is one of the first methods used in choosing \(\sigma\) empirically and will be denoted in our experimental results by \(\sigma_{MH}\). More precisely, for two samples \(\{\mathbf{X}_{i}\}_{i=1}^{n}\) and \(\{\mathbf{Y}_{i}\}_{i=1}^{m}\), the \(\sigma_{MH}\) is considered as the median of \(\{||\mathbf{X}_{i}-\mathbf{Y}_{j}||^{2}:\:1\leq i\leq n,1\leq j\leq m\}\), which is mostly used in kernel-based tests [32]. Selecting \(\sigma\) based on maximizing the power of two-sample problems is another strategy considered by [33]. The selection of the MMD bandwidth on held-out data to maximize power was first proposed by [34] for linear-time estimates and by [35] for quadratic-time estimates. Recently, bandwidth selection without data splitting has been proposed for quadratic [36] and linear [37] MMD estimates. Regarding the choice of \(\sigma\) in kernel-based GANs, a common idea is assigning several fixed values to \(\sigma\) and then considering the mixture of their corresponding Gaussian kernel. This strategy has received much attention and shown an acceptable performance in training GANs2.

Footnote 2: For further details, see [22] and [29].

## Appendix D Computational Algorithm

### Training the BNP GAN

```
1:Set \(a=10^{-6}\) to employ a non-informative prior leading DP posterior \(DP(n,F_{n})\).
2:Initialize \(N\).
3:\(r_{mn}\leftarrow\) Number of training iteration, \(n_{mb}\leftarrow\) Mini-batch size
4:\(\boldsymbol{\omega}_{0}\leftarrow\) An initial parameter for generator \(G_{\boldsymbol{\omega}}\), \(\{\mathbf{x}_{\ell}\}_{\ell=1}^{n}\leftarrow\) real dataset
5:for\(i\gets 0\) to \(r_{mb}\)do
6: Generate a random sample \(\{\mathbf{x}_{\ell}^{mb}\}_{\ell=1}^{n_{mb}}\) from real dataset \(\{\mathbf{x}_{\ell}\}_{\ell=1}^{n}\)
7: Generate a sample of noise vector \(\{\mathbf{u}_{\ell}\}_{\ell=1}^{n_{mb}}\) from uniform distribution \(U(-1,1)\)
8: Generate a sample from \(F_{G_{\boldsymbol{\omega}_{i}}}\), distribution of \(G_{\boldsymbol{\omega}_{i}}\), as \(\{\mathbf{y}_{\ell}=G_{\boldsymbol{\omega}_{i}}(\mathbf{u}_{\ell})\}_{\ell=1}^{ n_{mb}}\)
9: Generate a sample of size \(N\) from \(F^{pos}=F|\{\mathbf{x}_{\ell}^{mb}\}_{\ell=1}^{n_{mb}}\) using \(\sum_{i=1}^{N}J_{i,N}^{*}\delta_{\mathbf{v}_{i}^{*}}\).
10: Use generated samples in steps 9 and 10 to compute \(\mathrm{MMD}^{2}_{\mathrm{BNP}}(F_{N}^{pos},F_{G_{\boldsymbol{\omega}_{i}},N})\).
11: Compute the gradient: \[\frac{\partial\mathrm{MMD}^{2}_{\mathrm{BNP}}(F_{N}^{pos},F_{G_{\boldsymbol{ \omega}_{i}},m})}{\partial\boldsymbol{\omega}_{i}}=\frac{1}{2\sqrt{\mathrm{MMD }^{2}_{\mathrm{BNP}}(F_{N}^{pos},F_{G_{\boldsymbol{\omega}_{i}},m})}}\frac{ \partial\mathrm{MMD}^{2}_{\mathrm{BNP}}(F_{N}^{pos},F_{G_{\boldsymbol{\omega} _{i}},m})}{\partial\boldsymbol{\omega}}.\]
12: Use backpropagation for calculating partial derivatives \(\frac{\partial\mathbf{G}_{\boldsymbol{\omega}_{i}}(\mathbf{u}_{\ell})}{ \partial\boldsymbol{\omega}_{i}}\) in the previous step to update parameter \(\boldsymbol{\omega}_{i}\).
13:endfor
14:return\(\boldsymbol{\omega}^{*}\)\(\triangleright\) An optimized parameter for \(G_{\boldsymbol{\omega}}\) that minimizes the cost function.
```

**Algorithm 1** Pseudocode of training a GAN using the BNP approach

## Appendix E Theoretical proofs

**Proposition 1**: _For a non-negative real value \(a\) and fixed probability distribution \(H\), let \(F_{1}^{pri}:=F_{1}\sim DP(a,H)\) and \((J_{1,N},\ldots,J_{N,N})\sim\mbox{Dirichlet}(\frac{a}{N},\ldots,\frac{a}{N})\) be the weights in the approximation of \(F^{pri}\), given by [19]. Then, as \(a\to\infty\), \(i.\ J_{\ell,N}\xrightarrow{a.s.}\frac{1}{N},\) for any \(\ell\in\{1,\ldots,N\}\), \(ii.\ J_{\ell,N}J_{t,N}\xrightarrow{a.s.}\frac{1}{N^{2}},\) for any \(\ell,t\in\{1,\ldots,N\},\) where \(\ell\neq t\)._

**Proof of Proposition 1**: _Recall_

\[F_{N}^{pri}=\sum_{i=1}^{N}J_{i,N}\delta_{Y_{i}}. \tag{7}\]

_Since \(E_{F_{1}^{pri}}(J_{\ell,N})=\frac{1}{N}\), for any \(\ell\in\{1,\ldots,N\}\) and \(\epsilon>0\), Chebyshev's inequality implies_

\[\Pr\left\{|J_{\ell,N}-1/N|\geq\epsilon\right\}\leq\frac{Var(J_{\ell,N})}{ \epsilon^{2}},\]

_where, \(Var_{F_{1}^{pri}}(J_{\ell,N})=\frac{N-1}{N^{2}(a+1)}\). Assuming \(a=\kappa^{2}c\) for \(\kappa\in\mathbb{N}\) and a fixed positive number \(c\), gives_

\[\Pr\left\{|J_{\ell,N}-1/N|\geq\epsilon\right\}\leq\frac{1}{\kappa^{2}c \epsilon^{2}}.\]

_The convergence of series \(\sum_{\kappa=0}^{\infty}\kappa^{-2}\) implies \(\sum_{\kappa=0}^{\infty}\Pr\left\{|J_{\ell,N}-1/N|\geq\epsilon\right\}<\infty\). By letting \(a\to\infty\), the first Borel Cantelli lemma concludes \(|J_{\ell,N}-1/N|\xrightarrow{a.s.}0\) and the result of (i) follows. To prove (ii), it is enough to show \(\Pr\left\{\lim_{a\to\infty}(J_{\ell,N}J_{t,N})\neq\frac{1}{N^{2}}\right\}=0\). To prove this for the probability space \((\Omega,\mathcal{F},\Pr)\), let_

\[A =\left\{\omega\in\Omega:\lim_{a\to\infty}\left(J_{\ell,N}(\omega)J _{t,N}(\omega)\right)\neq\frac{1}{N^{2}}\right\},\ \ B=\left\{\omega\in\Omega: \lim_{a\to\infty}\left(J_{\ell,N}(\omega)\right)\neq\frac{1}{N}\right\},\] \[C =\left\{\omega\in\Omega:\lim_{a\to\infty}\left(J_{t,N}(\omega) \right)\neq\frac{1}{N}\right\},\]

_where, \(\Pr(B)\) and \(\Pr(C)\) are zero by (i). Since \(A\subseteq B\cup C\), then,_

\[1-\Pr\left\{\omega\in\Omega:\lim_{a\to\infty}\left(J_{\ell,N}(\omega)J_{t,N}( \omega)\right)=\frac{1}{N^{2}}\right\}=\Pr(A)\leq\Pr(B)+\Pr(C)=0,\]

_which concludes the result._

**Theorem 1**: _For a non-negative real value \(a\) and fixed probability distribution \(H\), let \(F_{1}^{pri}:=(F_{1}\sim DP(a,H))\) and \(k(\cdot,\cdot)\) be any continuous kernel function with feature space corresponding to a universal RKHS defined on a compact metric space \(\mathfrak{X}\). Assume that \(|k(\boldsymbol{z},\boldsymbol{z}^{\prime})|<K\), for any \(\boldsymbol{z},\boldsymbol{z}^{\prime}\in\mathbb{R}^{d}\). Then, for a given sample \(\mathbf{X}_{1},\ldots,\mathbf{X}_{n}\) from distribution \(F_{1}\), \(i.\) as \(a\to\infty\) (informative prior),_

1. \(\mathrm{MMD}^{2}_{\mathrm{BNP}}(F_{1,N}^{pos},F_{2,m})\xrightarrow{a.s.} \mathrm{MMD}^{2}(H_{N},F_{2,m})\)_,_
2. \(E(\mathrm{MMD}^{2}_{\mathrm{BNP}}(F_{1,N}^{pos},F_{2,m}))\to\mathrm{MMD}^{2} (H,F_{2})\)_,_ \(N\to\infty\)_, and_ \(m\to\infty\)_,_

\(ii.\) _as_ \(n\to\infty\) _(consistency),_

1. \(\mathrm{MMD}^{2}_{\mathrm{BNP}}(F_{1,N}^{pos},F_{2,m})\xrightarrow{a.s.} \mathrm{MMD}^{2}(F_{1,N},F_{2,m})\)_,_
2. \(E(\mathrm{MMD}^{2}_{\mathrm{BNP}}(F_{1,N}^{pos},F_{2,m}))\to\mathrm{MMD}^{2} (F_{1},F_{2})\)_, as_ \(N\to\infty\)_,_ \(n\to\infty\)_, and_ \(m\to\infty\)

[MISSING_PAGE_FAIL:11]

### Proof of Lemma 2

The proof of Lemma 2(i) relies on the proof given in [1, Theorem 9] which is expanded for infinite stick-breaking representation, while we consider the finite DP approximation given in (7). By employing a similar technique as in the previously mentioned theorem, we have

\[E\left(\mathrm{MMD}(F,F_{G_{\mathbf{\omega}^{*}}})\right) =E_{F}\left(E_{F^{pos}}\mathrm{MMD}(F,F_{G_{\mathbf{\omega}^{*}}})| \mathbf{X}_{1:n}\right)\] \[\leq\min_{\mathbf{\omega}\in\mathcal{W}}\mathrm{MMD}(F,F_{G_{\mathbf{ \omega}}})+2E_{F}\left(\mathrm{MMD}(F_{n},F)\right)+2E_{F^{pos}}\left(\mathrm{ MMD}(F_{N}^{pos},H^{*})\right)\] \[+2E_{F}\left(E_{H}(\mathrm{MMD}(F_{n},H^{*})|\mathbf{X}_{1:n}) \right).\]

Building on the results of [1, Lemma 7], we can establish that

\[E_{F^{pos}}\left(\mathrm{MMD}^{2}(F_{N}^{pos},H^{*})\right)\leq\sum_{\ell=1}^{ N}E_{F^{pos}}[J_{\ell,N}^{*^{2}}]E_{H^{*}}[k(\mathbf{V}_{\ell}^{*},\mathbf{V} _{\ell}^{*})]\leq\frac{(a+n+N)K}{(a+n+1)N},\]

where the right-hand side of the above inequality follows from the fact that \(k(\cdot,\cdot)\leq K\) and \(E_{F^{pos}}[J_{\ell,N}^{*^{2}}]=\frac{a+n+N}{(a+n+1)N^{2}}\). Now, the Jensen's inequality implies

\[E_{F^{pos}}\left(\mathrm{MMD}(F_{N}^{pos},H^{*})\right)\leq\sqrt{\frac{(a+n+N )K}{(a+n+1)N}}.\]

On the other hand, [39, Lemma 7.1] and [1, Lemma 8], respectively, imply that

\[E_{F}\left(\mathrm{MMD}(F_{n},F)\right)\leq\frac{K}{\sqrt{n}},E_{F}\left(E_{H} (\mathrm{MMD}(F_{n},H^{*})|\mathbf{X}_{1:n})\right)\leq\frac{2aK}{a+n},\]

which concludes the proof of (i). To establish (ii), we adopt the approach used in the proof of [1, Corollary 5]. Initially, we employ [39, Lemma 3.3] to bound \(\mathrm{MMD}(F_{0},F_{G_{\mathbf{\omega}^{*}}})\) by \(2\epsilon+\mathrm{MMD}(F,F_{G_{\mathbf{\omega}^{*}}})\), resulting in:

\[E\left(\mathrm{MMD}(F_{0},F_{G_{\mathbf{\omega}^{*}}})\right)\leq 2\epsilon+E \left(\mathrm{MMD}(F,F_{G_{\mathbf{\omega}^{*}}})\right).\]

Applying the result in (i) to the right-hand side of the above inequality implies:

\[E\left(\mathrm{MMD}(F_{0},F_{G_{\mathbf{\omega}^{*}}})\right)\leq 2\epsilon+\min_{ \mathbf{\omega}\in\mathcal{W}}\mathrm{MMD}(F,F_{G_{\mathbf{\omega}}})+\frac{2K}{\sqrt{ n}}+\frac{4aK}{a+n}+2\sqrt{\frac{(a+n+N)K}{(a+n+1)N}}.\]

Finally, we employ [39, Lemma 3.3] once again, but this time to bound \(\mathrm{MMD}(F,F_{G_{\mathbf{\omega}}})\) by \(2\epsilon+\mathrm{MMD}(F_{0},F_{G_{\mathbf{\omega}}})\) for any \(\mathbf{\omega}\in\mathcal{W}\), thereby completing the proof of (ii).

### Proof of Lemma 1

Let \(\mathcal{L}_{\mathrm{BNP}}(\mathbf{\omega})=\mathrm{MMD}(F_{N}^{pos},F_{G_{\mathbf{ \omega}}})\), \(\mathcal{L}_{n,m}(\mathbf{\omega})=\mathrm{MMD}(F_{n},F_{G_{\mathbf{\omega}},m})\), and \(\mathcal{L}(\mathbf{\omega})=\mathrm{MMD}(F,F_{G_{\mathbf{\omega}}})\). Then, for \(\mathbf{\omega}^{*}\in\mathcal{W}\), [26, Theorem 7] implies

\[\Pr\left(|\mathcal{L}_{n,m}(\mathbf{\omega}^{*})-\mathcal{L}(\mathbf{\omega}^{*})|>h(n,m,K,\epsilon)\right)<2\exp\frac{-\epsilon^{2}nm}{2K(n+m)}. \tag{11}\]

Hence, with a probability at least \(1-2\exp\frac{-\epsilon^{2}nm}{2K(n+m)}\),

\[|\mathcal{L}_{n,m}(\mathbf{\omega}^{*})-\mathcal{L}(\mathbf{\omega}^{*})|\leq h(n,m, K,\epsilon). \tag{12}\]

On the other hand, the triangle inequality implies

\[|\mathcal{L}_{\mathrm{BNP}}(\mathbf{\omega}^{*})-\mathcal{L}(\mathbf{\omega}^{\prime} )|\leq|\mathcal{L}_{n,m}(\mathbf{\omega}^{*})-\mathcal{L}(\mathbf{\omega}^{*})|+| \mathcal{L}_{\mathrm{BNP}}(\mathbf{\omega}^{*})-\mathcal{L}_{n,m}(\mathbf{\omega}^{*} )|+|\mathcal{L}(\mathbf{\omega}^{*})-\mathcal{L}(\mathbf{\omega}^{\prime})|. \tag{13}\]

Finally, the proof of (i) is concluded by considering inequality (12) in (13). To prove (ii), Markov's inequality implies

\[\Pr\left(\mathrm{MMD}(F,F_{G_{\mathbf{\omega}^{*}}})\geq\epsilon\right)\leq\frac{E \left(\mathrm{MMD}(F,F_{G_{\mathbf{\omega}^{*}}})\right)}{\epsilon}.\]

The result follows by substituting the bounds from Lemma 4(i) into the right-hand side of the above inequality.

[MISSING_PAGE_FAIL:13]

Figure 3: Learning rate: Values of the cost function in the proposed GAN and its frequentist counterpart [22] over \(20,000\) iterations.