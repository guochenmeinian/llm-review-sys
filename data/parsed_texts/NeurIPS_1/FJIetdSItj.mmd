# \(Lv\)-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K

 Tao Yuan\({}^{1}\), Xuefei Ning\({}^{2,}\), Dong Zhou\({}^{1}\), Zhijie Yang\({}^{1}\), Shiyao Li\({}^{1,2}\),

**Minghui Zhuang\({}^{1}\), Zheyue Tan\({}^{1}\), Zhuyu Yao\({}^{1}\), Dahua Lin\({}^{3,4}\),**

**Boxun Li\({}^{1,}\), Guohao Dai\({}^{1,5,}\), Shengen Yan\({}^{1}\), Yu Wang\({}^{2,}\)

\({}^{1}\) Infinigence-AI, \({}^{2}\) Tsinghua University, \({}^{3}\) Shanghai Artificial Intelligence Laboratory

\({}^{4}\) The Chinese University of Hong Kong, \({}^{5}\) Shanghai Jiao Tong University

###### Abstract

State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of \(256k\) or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (\(5k\)-\(21k\)), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces \(LV\)-Eval, a challenging long-context benchmark with five length levels (\(16k\), \(32k\), \(64k\), \(128k\), and \(256k\)) reaching up to \(256k\) words. \(LV\)-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of \(LV\)-Eval has incorporated three key techniques, namely confusing facts insertion (CFI), keyword and phrase replacement (KPR), and keyword-recall-based metric design. The advantages of \(LV\)-Eval include controllable evaluation across context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluation. We evaluate 12 LLMs on \(LV\)-Eval and conduct ablation studies on the benchmarking techniques. The results reveal that: (i) Commercial LLMs generally outperform open-source LLMs when evaluated within length levels shorter than their claimed context length. However, their overall performance is surpassed by open-source LLMs with longer context lengths. (ii) Extremely long-context LLMs, such as Yi-6B-200k and Llama3-8B-1M, exhibit a relatively gentle degradation of performance, but their absolute performances may not necessarily be higher than those of LLMs with shorter context lengths. (iii) LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of "needle in a haystack". (iv) Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation, and these concerns are alleviated in \(LV\)-Eval. All datasets and evaluation codes are released at: [https://github.com/infinigence/LVEval](https://github.com/infinigence/LVEval).

## 1 Introduction

Large language models (LLMs) have demonstrated exceptional performance on a variety of natural language processing tasks. The ability of long-context understanding is crucial for LLMs to deal with tasks based on longer contexts, such as books, lengthy chat history, and so on. Recently, extensive efforts have been devoted in enlarging the supported context length (i.e., the number of tokens that the model can accept as input) of LLMs. These efforts have pushed the supported context length of LLMs from \(2k\) tokens to \(32k\) tokens [1; 2; 3; 4; 5], and some models have achieved a remarkable context length of \(128k\) and \(200k\)[6; 7; 8].

In contrast to the rapid evolution of the models' supported context length, existing benchmarks have lagged behind. The average word count in current long-context benchmarks typically falls within the range of \(32k\)[9, 10, 11, 12, 13], considerably shorter compared to the supported context lengths of state-of-the-art long-context models. Moreover, previous benchmarks primarily consist of _unaltered_ public documents and articles. This could be problematic for two reasons: (i) the data might be involved in LLMs' training processes, and (ii) the facts within them might be common-sense facts found in other training resources. The presence of this issue, known as "knowledge leakage" [14], can lead to models answering questions with memorization or common-sense knowledge instead of understanding long-range contexts. Last but not least, the automatic metrics employed in most of the existing benchmarks are susceptible to the variations in answer format and the inclusion of irrelevant words. Such metrics struggle to accurately assess the answer quality.

To address these issues, we propose _LV_-Eval, a bilingual benchmark with up to \(256k\) words. _LV_-Eval incorporates distractions and confusions to make the test more challenging, replaces keywords and rephrases sentences to prevent knowledge leakage, and employs a more accurate metric. We summarizes the key characteristics of _LV_-Eval as follows:

* **Sufficiently long context length to evaluate state-of-the-art models**: _LV_-Eval comprises 5 length levels with word counts of \(16k\), \(32k\), \(64k\), \(128k\), and \(256k\). Test instances across these levels share the same set of question-answer (QA) pairs, and only differ in the context content and length. Testing on the same QA pairs with different context lengths facilitates a controllable evaluation of models' long-context ability.
* **Incorporation of distraction and confusion to increase difficulty**: When constructing the context for each test instance, we mix up distracting documents and supporting documents. This approach evaluates the model's ability in pinpointing key information in a large bunch of distracting texts. In addition, we insert confusing facts generated by GPT-4 and revised by human annotators into the context. This assesses the model's capability to accurately reason in the presence of interference.
* **Keyword and phrase replacement to mitigate knowledge leakage**: To mitigate the biased evaluation of long-context ability caused by knowledge leakage, we replace the keywords and phrases in the context and QA pairs. The replacement rules are annotated by human annotators. In this way, _LV_-Eval requires LLMs to rely on the understanding of context to answer questions rather than relying on memorization or common-sense knowledge.
* **Keyword-recall-based metric for more objective scoring**: Existing \(N\)-gram metrics such as the F1 score are sensitive to the format variations and non-informative words in the answer, which results in inaccurate scores. To address this, we manually annotate answer keywords and a blacklist of unrelated words. The golden answers are the critical words or sentences extracted from original ground-truth (GT) answers, while the word blacklist contains common and non-informative words such as 'the', 'a', 'of', and so on. The metric calculation follows a two-stage procedure: the first stage calculates the recall of golden answer keywords. if the recall exceeds a certain threshold, the second stage will remove all the blacklisted words and then calculate the F1 score between the prediction and the GT answer. This metric design can get scores with higher objectivity.

Findings.We evaluate 12 LLMs on _LV_-Eval and summarize the main findings as follows: (i) Commercial LLMs generally outperform open-source LLMs when evaluated within length levels shorter than their claimed context length. However, their overall performance is surpassed by

\begin{table}
\begin{tabular}{c c c c c c c} Benchmark & \#Datasets & Avg \#Words & Min/Max Words & Length Levels & Opt. Metric & Lang. \\ \hline ZeroSCROLLS [12] & 10 & 13,556 & 1,023/320,774 & none & & en \\ LoGLE [10] & 7 & 21,247 & 10,927/246,182 & none & & en \\ L-Eval [11] & 20 & 12,993 & 2,119/170,256 & none & & en \\ BAMBOO [13] & 10 & 5,067 & 229/14,858 & \(4k\),\(4k\),\(16k\) & & en+zh \\ LongBench [9] & 21 & 9,486 & 128/71,954 & \(0\)–\(4k\),\(4k\)–\(8k\),\(8k\)+ & & en+zh \\ \hline _LV_-Eval & 11 & 102,380 & 11,896/387,406 & \(16k\),\(32k\),\(64k\),\(128k\),\(256k\) & & en+zh \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of different long-context benchmarks. We count the number of words for the English datasets and the number of characters for the Chinese datasets. The punctuation marks are taken into account, while tabs, blank spaces, and newlines are not included.

open-source LLMs with longer context lengths. (ii) Extremely long-context LLMs, such as Yi-6B-200k and Llama3-8B-1M, exhibit a relatively gentle degradation of performance, but their absolute performances may not necessarily be higher than those of LLMs with shorter context lengths. (iii) LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of "needle in a haystack". (iv) Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation, and these concerns are alleviated in \(LV\)-Eval.

## 2 Related Work

Long-Context Benchmarks.Table 1 provides a summary of existing long-context benchmarks, including ZeroScrolls [12], LooGLE [10], L-Eval [11], BAMBOO [13], and LongBench [9]. ZeroScrolls, LooGLE, and L-Eval are monolingual benchmarks without explicit length level partition. Their average word counts are \(\sim\)14\(k\), \(\sim\)21\(k\) and \(\sim\)13.5\(k\), respectively. In order to evaluate the model's capability across various context lengths, BAMBOO and LongBench have designed various length levels. However, the word counts (\(\sim\)5\(k\), \(\sim\)9.5\(k\)) of the contexts in these two benchmarks are notably smaller than the supported context length of state-of-the-art long-context models, making them unsuitable for evaluating the claimed extremely long-context understanding ability. In contrast, \(LV\)-Eval contains five length levels, up to \(256k\) words, each with the same set of QA pairs for controllable evaluation.

In terms of metric design, L-Eval introduces a length-instruction-enhanced metric to mitigate the undesired impact of the answer length on metric scores. Additionally, L-Eval proposes to use LLMs to assist in scoring. In \(LV\)-Eval, we ask human annotators to mark the answer keywords and create a non-informative word blacklist, and propose a two-stage metric to focus more on the answer keywords while reducing the influences of non-informative words.

Long-Context Techniques.Considerable efforts have been devoted to enhancing the long-context abilities of LLMs. One line of work focuses on making LLMs have extended context sizes without fine-tuning and behave normally on inputs longer than their training context lengths. The design and extrapolation method of the position encoding module [15, 16, 17] is crucial for this goal. Besides, several sparse attention techniques [18, 19] have also been proposed to avoid model collapse. These sparse attention techniques also alleviate the quadratic complexity w.r.t. the sequence length.

There are many other strategies aimed at enabling LLMs to effectively leverage long input contexts. The most commonly utilized strategy is long-context fine-tuning [20, 21, 22]. For instance, YaRN [22] conducts fine-tuning with \(64k\) and \(128k\) context lengths starting with Llama2-7B/13B, and Yi-6B-200k [8] is trained with \(200k\) context length starting with its \(4k\) variant. Other strategies include the recurrent- or memory-based architecture [23, 24, 25, 26, 27], and the retrieval- or summarization-based context compression techniques [28, 29, 9, 26], and so on.

In this work, we evaluate LLMs of diverse context sizes, ranging from \(4k\) to \(200k\), most of which have incorporated advanced position encoding design and undergone long-context fine-tuning.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Task & Dataset & CFI & \#KPR & AK & Language & \#QA pairs & \#Contexts \\ \hline \multirow{8}{*}{Single-hop QA} & **lic-mixup** & ✓ & & ✓ & zh & 197 & 985 \\  & **loogle-SD-mixup** & & & ✓ & en & 160 & 800 \\  & **cmrc-mixup** & & 786 & & zh & 200 & 1,000 \\  & **multifieldqa-en-mixup** & ✓ & 476 & ✓ & en & 101 & 505 \\  & **multifieldqa-zh-mixup** & ✓ & 424 & ✓ & zh & 133 & 665 \\  & **factrecall-en** & ✓ & 3 & ✓ & en & 1 & 200\(\times\)5 \\  & **factrecall-zh** & ✓ & 3 & ✓ & zh & 1 & 200\(\times\)5 \\ \hline \multirow{8}{*}{Multi-hop QA} & **dureader-mixup** & & & & zh & 176 & 880 \\  & **loogle-CR-mixup** & & & ✓ & en & 99 & 495 \\  & **loogle-MR-mixup** & & & ✓ & en & 139 & 695 \\  & **hotpotwikiqa-mixup** & ✓ & 232 & ✓ & en & 124 & 620 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Data statistics of \(LV\)-Eval. The abbreviations “CFI”, “KPR”, “AK” stand for “Confusing Fact Insertion”, “Keyword and Phrase Replacement”, and “Answer Keywords”, respectively. “#KPR” is the number of KPR rules. Note that in **factrecall-en** and **factrecall-zh**, all QA pairs are the same across all test instances, i.e., there is only one unique QA pair for each of the two datasets.

## 3 _Lv_-Eval Benchmark

\(LV\)-Eval focuses on two types of QA tasks: single-hop QA and multi-hop QA, and is comprised of 11 QA datasets (6 in English and 5 in Chinese). The data statistics for _LV_-Eval are outlined in Table 2. Each test instance in _LV_-Eval comprises three parts: a context (\(C\)), a question (\(Q\)), and a GT answer (\(A\)), where \(C\) is a synthetic document containing the information required to answer \(Q\).

Datasets in _LV_-Eval are constructed with existing public datasets as the source, except for factrecall-en and factrecall-zh, which are constructed using the data from _PG19_[30] dataset and _Journey to the West_ book. Each dataset consists of five subsets of different lengths: \(16k\), \(32k\), \(64k\), \(128k\), and \(256k\). All five subsets share the same question-answer (QA) pairs, meaning there are five contexts of varying lengths for each QA pair. This allows for a controllable evaluation of models' long-context ability when testing the same set of questions with different context lengths. In total, _LV_-Eval comprises 1,729 QA pairs and 1,729\(\times\)5 = 8,645 synthetic contexts.

Figure 2 illustrates the construction process of _LV_-Eval. For **factrecall-en** and **factrecall-zh**, we write one QA pair for each dataset. For the rest 9 out of the 11 datasets, we first choose a specific number of QA pairs from existing QA datasets (Section 3.1). Then, for each unique QA pair, we go through three procedures to construct the context (Section 3.2):

1. **Context mixing up** (Section 3.2.1): We first construct five contexts of different lengths by mixing up supporting documents corresponding to the QA pair and several distracting documents. For **factrecall-en** and **factrecall-zh**, we mix the supporting evidence of the single QA pair with distracting documents from two books. For other datasets, the distracting documents are unrelated to the question and are chosen from the context documents corresponding to non-selected QA pairs in the same source dataset.
2. **Confusing Facts Insertion (CFI)** (Section 3.2.2): Then, in some datasets, we introduce confusing facts by generating them with GPT-4, manually revising them, and randomly inserting these into the context. These confusing facts bear similarities to the original supporting facts but are factually different, without contradicting the original information. This helps make the test instances more challenging.
3. **Keyword and Phrase Replacement (KPR)** (Section 3.2.3): Finally, to reduce the impacts of knowledge leakage on evaluation results, we manually replace some keywords and phrases in the context and the QA pairs.

When evaluating the generated answer, to mitigate the bias in existing metrics, we manually annotate the keywords in the GT answer and adjust the metric to focus more on the keywords (Section 3.3).

### Data Source and QA Pair Construction

We construct 11 datasets (see Table 2) using public data sources, including Long-instruction-en2zh [31], HotpotQA [32], 2WikiMultihopQA [33], DuReader [34], LooGLE [10], LongBench [9], CMRC 2018 [35], MultiFieldQA [9], PG-19 [30] and the book of _Journey to the West_. The construction of QA pairs in each dataset is elaborated in Appendix A.

Figure 1: The construction process of _LV_-Eval. “CF” is short for “Confusing Fact”.

### Context Construction

#### 3.2.1 Context Mixing Up

Can the LLMs identify the key evidences to answer the target question within a long context? To assess this ability, as shown in Figure 2, \(LV\)-Eval randomly mixes the supporting documents with various distracting documents to generate five contexts of varying length for a given QA pair. For 9 out of the 11 datasets (excluding **factrecall-en** and **factrecall-zh**), the distracting documents are chosen from the contexts corresponding to the non-selected QA pairs in the source dataset. For **factrecall-en** and **factrecall-zh**, the distracting documents are extracted from the _PG-19_ dataset and the book of _Journey to the West_.

For each length level, we sample distracting documents one by one until the cumulative word count meets the desired length level. Then, we shuffle the supporting and distracting documents, prepend a string "Passage i\n" to the \(i\)-th document, and concatenate them to form the final context.

Note that in **hotpotwikiqa-mixup** and **dureader-mixup**, where multiple supporting documents exist for each QA pair, instead of regarding the multiple supporting documents a single unit, we disperse and shuffle all supporting and distracting documents.

#### 3.2.2 Confusing Facts Insertion

Can the LLMs identify the key evidences correctly if there are confusing facts in the context? To assess this ability, we apply CFI in **hotpotwikiqa-mixup**, **lic-mixup**, **multifieldqa-en-mixup**, **multifieldqa-zh-mixup**, **factrecall-en**, and **factrecall-zh**, which inserts similar, factually different, non-contradictory facts into the context. These facts might mislead less meticulous models, leading them to generate incorrect answers.

The generation process of the confusing facts goes as follows. Firstly, we use the question and answer as the input, and prompt GPT-4 [7] to generate two descriptions that are close to the original fact. The prompt for GPT-4 is shown in Figure A7. Then, we ask human annotators to resolve any conflicts in the generated facts. As illustrated in Figure 3.2, the generated confusing fact "Albert Einstein was an Italian astronomer" is in conflict with the original fact. Therefore, the human annotator revise it to "Albert Beverley was an Italian astronomer". After this generation and revising process, we insert the confusing facts into a randomly picked position between two sentences in the context.

Figure 2: Steps for CFI. Firstly, we prompt GPT-4 to generate two descriptions that are close to the original fact. Then we ask human annotators to resolve any conflicts in the generated facts. For example, the first generated confusing fact "Albert Einstein was an Italian astronomer" is in conflict with the original fact and the human annotator revise it to "Albert Beverley was an Italian astronomer". Finally, the confusing facts are inserted into a randomly position in the context.

#### 3.2.3 Keyword and Phrase Replacement

Knowledge leakage is an important concern in LLM evaluation [14]. On the one hand, the test data are usually collected from open-access sources, and we cannot fully rule out the possibility of their being involved in some LLMs' training process. On the other hand, some common-sense questions can be answered without referencing the provided context. Consequently, LLMs might rely on memorization and common-sense knowledge to answer the questions rather than fully understanding the context. This will cause inflated benchmark scores to overrate the long-context ability of models.

To mitigate the influences of knowledge leakage on the evaluation results, we conduct KPR according to manually crafted rules in **hotpotwikiqa-mixup**, **cmrc-mixup**, **multifieldqa-en-mixup**, **multifieldqa-zh-mixup**, **factrecall-en**, and **factrecall-zh**. Specifically, given a QA pair, the annotators are asked to select keywords or phrases for replacement and write a substitute for each. After the selected keywords and phrases are replaced throughout the entire context, the annotators review the modified context to check and resolve any conflicts: If there are conflicts, the annotators are asked to revise the replacement rule until all conflicts are resolved. One example of the KPR process is shown in Figure 3.2.2. See Table 2 for the statistics of the number of replacement rules.

### Metric Design

The quality evaluation of natural language generation is challenging. Current \(N\)-gram metrics, such as the F1 score, treat all words equally. The neglect of differences in word importance leads to evaluation bias. For example, in the sentence "Attention is all you need", the word "attention" carries the key information and is more important. However, the answer "Attention matters" will get a lower score than the answer "CNN is all you need", which is not what we expected. To this end, we adopt a two-stage metric calculation process.

Specifically, to evaluate an answer \(A^{\prime}\), we first calculate the recall of several "answer keywords" in \(A^{\prime}\). When the recall exceeds a certain threshold (0.2 for Chinese dataset, 0.4 for English datasets), we calculate the F1 score between \(A^{\prime}\) and GT answer \(A\) as the final score for \(A^{\prime}\). otherwise, \(A^{\prime}\) gets a zero score. We manually annotate the answer keywords in the GT answer \(A\) for **hotpotwikiqa-mixup**, **lic-mixup**, **loogle-CR-mixup**, **loogle-MR-mixup**, **loogle-SD-mixup**, **multifieldqa-en-mixup**, and **multifieldqa-zh-mixup**. Figure 6 (a) shows an example, demonstrating how this two-stage calculation helps avoid some inflated high evaluation scores.

When calculating the F1 score between \(A^{\prime}\) and \(A\) in the second stage, we exclude common but non-informative words like 'the, 'a', 'of', and so on. The word blacklist is constructed as follows. We first summarized the word counts in the generations of Llama2-7B-Chat-hf and ChatGLM3

Figure 3: Steps for KPR. First, given a QA pair, the annotators are asked to select keywords or phrases to replace and write a substitute for each. Then, the selected keywords and phrases are replaced throughout the context and QA pair. Finally, annotators will check the modified context. If there is any conflict, the annotators are asked to revise the replacement rule until all conflicts are resolved.

6B-32K on all datasets and chose the top 100 words that matched the GT answer most frequently. Then, we manually annotate the non-informative words from the 100 words to construct the blacklist. Figure A6 (b) shows an example of how the word blacklist aids in calibrating the evaluation scores.

## 4 Evaluation

Models and Inference.We evaluate 2 commercial and 10 open-source LLMs on _LV_-Eval. Their information is summarized in Table A4. We follow the official implementation of all LLMs to conduct their inferences. Greedy sampling is used for generating tokens. For LLMs with a context window size smaller than the length of the data context, we truncate the data context in the middle, and concatenate the head and the tail of the context as input, ensuring that the QA instructions are fully contained within the input.

Metrics.For all tasks except **dureader-mixup** and **cmrc-mixup**, we evaluate the generated answers with our keyword-recall-based F1 metric, utilizing the annotated answer keywords and word blacklist. For **cmrc-mixup**, we omit the manual annotation of answer keywords since the answers in this dataset is already concise. Therefore, we use the F1 metric with word blacklist. In the case of **dureader-mixup**, where the GT answer lengths are relatively long, we do not manually annotate the answer keywords and use the ROUGH-L metric with the word blacklist.

### Compare LLMs on _Lv_-Eval

Figure 4 (a) shows the average scores across all 11 datasets of 12 LLMs at different length levels. We can see that (i) Commercial models do not always perform better than open-source models. For instance, ChatGLM3-6B-32k attains the highest accuracy on \(16k\) and \(32k\). (ii) Models exhibit distinct score trends. From the average scores in Figure 4 and the task-specific scores in Table A2, we can see that the model with the largest context window size, Llama3-8B-1M, exhibits the slowest decline of performance from \(16k\) to \(128k\). For example, its scores at the length level \(16k\) is lower than ChatGLM3-6B-32k and BlueLM-7B-32k-Chat. Nevertheless, as the length of input context increases, Llama3-8B-1M retains a higher score than these two models that need to truncate the input context. The similar phenomenon can be observed between Yi-6B-200 and two GPTs.

Figure 4 (b) shows the average scores across all 5 length levels of 12 LLMs on 5 types of tasks. We can see that (i) LLMs attain lower scores on multi-hop QA tasks compared to single-hop QA tasks. (ii) Confusing facts insertion adds complexity to the tasks, particularly evident in single-hop QA and single-hop confusion QA. See Appendix B for more detailed results.

Figure 4: Overall results on different length levels and types of datasets. (a) Average scores across all datasets of 12 LLMs at 5 length levels. (b) Average scores across all length levels of 12 LLMs on 5 types of datasets. “CQA” refers to datasets with CFI.

### Ablation Study of _Lv_-Eval Techniques

Confusing facts insertion.Table A4, A5, and A6 show the scores of multiple LLMs on dataset with and without CFI. We can see that (i) On **multifieldqa-en-mixup** and **multifieldqa-zh-mixup**, CFI leads to a notable degradation in the scores of LLMs. However, CFI in the **hotpotwikiqmixup** dataset does not result in severe degradation. (ii) Table A5 and A6 show that a strong model, ChatGLM3-6B-32k, exhibits the most substantial score degradation on data with CFI. For instance, the score of ChatGLM3-6B-32k degrades from \(41.46\) to \(31.97\) (a degradation of \(9.49\)) on the \(16k\) length level of **multifieldqa-en-mixup**, while the score degradation of other 5 LLMs falls within the range \([0.47,4.89]\). This observation suggests that current powerful LLMs may even be more susceptible to confusing information in the context. Future research is needed to enhance the models' ability to discern information that appears similar but is in fact unrelated. (iii) As the length of the input context increases, the score degradation becomes smaller. This phenomenon can be attributed to two factors: the truncation of confusing facts and a decrease in baseline performance.

Keyword and phrase replacement.The technique of KPR aims to eliminate the knowledge leakage and common-sense memorization of LLMs. Intuitively, for datasets sourced from Wikipedia and other widely used corpus, the risk of knowledge leakage is higher. From the results in Table A4, A5, and A6, we observe that: (i) KPR brings notable degradation of LLM scores on these three datasets suggesting that knowledge leakage exists in open-source corpus and can be mitigated by KPR. (ii) The extent of degradation is relatively consistent across different length levels.

We conduct an additional experiment to illustrate the knowledge leakage issue and the impact of KPR in Table 3. Specifically, we compare three settings: (i) Directly querying the LLMs to answer the question without the context ("direct (w.o. KPR)"). (ii) Applying KPR to the QA pair, and directly querying the LLMs without the context ("direct (w. KPR)"). (iii) Applying KPR to the QA pair and the context, and querying the LLMs to answer the question with the context ("w. context (w. KPR)").

Table 3 shows that without KPR, some LLMs can achieve a considerable score even without context. For instance, Yi-6B-200k and ChatGLM3-6B-32k achieve scores of 16.11 and 12.24, respectively, through memorization or common-sense knowledge. Applying KPR decreases the score without context (6.06 for Yi-6B-200k and 4.96 for ChatGLM3-6B-32k). This helps mitigate the influence of memorization or common-sense knowledge on the assessment of long-context understanding ability.

Case study on the fact-recall tasks.The **factrecall-en** and **factrecall-zh** datasets are constructed to evaluate the enhanced "needle in a haystack" [36] ability. The traditional "needle in a haystack" evaluation is basically a retrieval task, asking LLMs to find the answer or passkey in long context, which is too simple for majority of LLMs that they can easily get high scores after task oriented training. Therefore we enhance the "needle in a haystack" evaluation with CFI and KPR to assess LLM's positional consistency of retrieval while challenging their comprehension and anti-interference abilility. We show the ablation results of CFI and KPR in Figure 5 and Table A7. From the first column of sub-figure in Figure 5, we can see that ChatGLM3-6B-32k attains high accuracy on

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline \hline \multirow{2}{*}{Model Name} & \multirow{2}{*}{Ablation} & \multicolumn{4}{c}{**hotpotwikiqa-mixup**} \\ \cline{3-7}  & & \(16k\) & \(32k\) & \(64k\) & \(128k\) & \(256k\) \\ \hline \multirow{3}{*}{Llama2-7B-Chat-Inf} & direct (w. KPR) & & & 2.43 & & \\  & direct (w.o. KPR) & & & 3.52 & & \\  & w. context (w. KPR) & 3.99 & 1.30 & 1.84 & 0.81 & 0.75 \\ \hline \multirow{3}{*}{ChatGLM3-6B-32k} & direct (w. KPR) & & & 4.96 & & \\  & direct (w.o. KPR) & & & 12.24 & & \\  & w. context (w. KPR) & 16.98 & 14.76 & 9.02 & 8.31 & 6.68 \\ \hline \multirow{3}{*}{Yi-6B-200k} & direct (w. KPR) & & & & 6.06 & \\  & direct (w.o. KPR) & & & 16.11 & & \\  & w. context (w. KPR) & 23.55 & 18.94 & 9.94 & 7.66 & 2.01 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation results for KPR. “direct (w. KPR)”: Apply KPR and direct query without context; “direct (w.o. KPR)”: Direct query without context; “w. context (w. KPR)”: Apply KPR and query with context. Note that there is only one result in the first two rows in each section of the table, since the results of direct querying without context do NOT depend on the context length.

datasets without CFI and KPR, as long as the input context length is within its context size (32k). However, when either CFI (second column of sub-figure) or KPR (third column sub-figure) is applied, the retrieval accuracy decreases. The accuracy experiences a more severe degradation when both CFI and KPR are applied, particularly evident in **factrecall-zh**, where a performance collapse is observed. This indicates that there is room for improvement in the model's ability to accurately identify a specific piece of information from a long context in the presence of interference, and the original "needle in a haystack" could not be suit for reasonably evaluating long context capability.

**Keyword-recall-based metric.** For a given length level \(L_{d}\) of the dataset, if the single key information is uniformly distributed in the context, an LLM with a context window size \(L_{m}\) can only observe the key information for approximately \(\frac{L_{m}}{L_{d}}\) of the time. Thanks to our KPR technique, we can expect that the LLM cannot get the correct answer through memorization or common sense. Then, ideally, we would not expect to see a metric score much higher than \(\frac{L_{m}}{L_{d}}\). However, as shown in Table A3, when using the original F1 metric, due to the undesired matching of non-keywords and non-informative words, the metric score can be a lot higher than \(\frac{L_{m}}{L_{d}}\). For instance, ChatGLM3-6B-32k achieves a score of \(26.43\%\) on the \(256k\) length level of the **cmrc-mixup** dataset, which significantly exceeds \(\frac{L_{m}}{L_{d}}=12.5\%\). Fortunately, our keyword-recall-based metric with the word blacklist returns a score that aligns more closely with human expectations and is more reasonable.

## 5 Limitations and Negative Societal Impacts

\(LV\)-Eval includes QA and the "needle in a haystack" tasks, but does not encompass other task types such as summarization. Additionally, due to the high cost, we do not test some of the most recent LLMs, such as GPT-4-128k. As we release all the test data, one can intentionally overfit the benchmark by training on the test data to get a high score. In this case, training on \(LV\)-Eval datasets with KPR might lead to mistakes in common-sense knowledge, resulting in a very unreliable evaluation. Furthermore, a full evaluation on \(LV\)-Eval can cause a large token overhead (about 700M tokens for GPT-4's tokenizer), leading to considerable carbon emissions.

Figure 5: Ablation results of the “needle in a haystack” task on ChatGLM3-6B-32k. (a) **factrecall-en.** (b) **factrecall-zh.** In each of (a)(b), from left to right, the four sub-figures show the results of w.o. “CFI and KPR”, “w. CFI only”, “w. KPR only”, and “w. both CFI and KPR”, respectively. These results illustrate that CFI and KPR are effective in improving the task difficulty.

## Acknowledgments and Disclosure of Funding

We thank for the insightful discussion with Kai Chen and Songyang Zhang from Shanghai Artificial Intelligence Laboratory. Thanks to all of the annotators who contribute to the project.

## References

* [1] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [2] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. _arXiv preprint arXiv:2306.05685_, 2023.
* [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.
* [4] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. _arXiv preprint arXiv:2210.02414_, 2022.
* [5] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In _NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_, 2023.
* [6] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. _arXiv preprint arXiv:2309.00071_, 2023.
* [7] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [8] Yi. A series of large language models trained from scratch by developers at 01-ai. [https://github.com/01-ai/Yi](https://github.com/01-ai/Yi), 2023.
* [9] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. _arXiv preprint arXiv:2308.14508_, 2023.
* [10] Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. Loogle: Can long-context language models understand long contexts? _arXiv preprint arXiv:2311.04939_, 2023.
* [11] Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models. _arXiv preprint arXiv:2307.11088_, 2023.
* [12] Uri Shaham, Maor Iygi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding. _arXiv preprint arXiv:2305.14196_, 2023.
* [13] Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models. _arXiv preprint arXiv:2309.13345_, 2023.
* [14] Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. Don't make your llm an evaluation benchmark cheater. _arXiv preprint arXiv:2311.01964_, 2023.
* [15] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063, 2024.
* [16] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. _arXiv preprint arXiv:2108.12409_, 2021.

* [17] bloc97. Ntk-aware scaled rope allows llama models to extrapolate. [https://www.reddit.com/r/LocalLLaMA/comments/141z7j5/ntkaware_scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/141z7j5/ntkaware_scaled_rope_allows_llama_models_to_have/), 5 2023.
* [18] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. _arXiv preprint arXiv:2308.16137_, 2023.
* [19] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. _arXiv preprint arXiv:2309.17453_, 2023.
* [20] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. _arXiv preprint arXiv:2309.16039_, 2023.
* [21] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length of open-source llms truly promise? In _NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following_, 2023.
* [22] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. _arXiv preprint arXiv:2309.00071_, 2023.
* [23] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. _arXiv preprint arXiv:1901.02860_, 2019.
* [24] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In _International Conference on Learning Representations_, 2021.
* [25] Pedro Henrique Martins, Zita Marinho, and Andre FT Martins. \(\infty\)-former: Infinite memory transformer: Infinite memory transformer. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5468-5485, 2022.
* [26] Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. RecurrentgtP: Interactive generation of (arbitrarily) long text. _arXiv preprint arXiv:2305.13304_, 2023.
* [27] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. _arXiv preprint arXiv:2304.13343_, 2023.
* [28] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In _International Conference on Learning Representations_, 2019.
* [29] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In _International conference on machine learning_, pages 2206-2240. PMLR, 2022.
* [30] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. _arXiv preprint arXiv:1911.05507_, 2019.
* [31] yuyijiong. Long-instruction-en2zh. [https://huggingface.co/datasets/yuyijiong/Long-instruction-en2zh](https://huggingface.co/datasets/yuyijiong/Long-instruction-en2zh), 2023.
* [32] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. _arXiv preprint arXiv:1809.09600_, 2018.
* [33] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. _arXiv preprint arXiv:2011.01060_, 2020.

* [34] Hongxuan Tang, Hongyu Li, Jing Liu, Yu Hong, Hua Wu, and Haifeng Wang. Dureader_robust: A chinese dataset towards evaluating robustness and generalization of machine reading comprehension in real-world applications. _arXiv preprint arXiv:2004.11142_, 2020.
* [35] Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. A span-extraction dataset for chinese machine reading comprehension. _arXiv preprint arXiv:1810.07366_, 2018.
* [36] Greg Kamradt. Pressure testing gpt-4-128k with long context recall. [https://twitter.com/GregKamradt/status/1722386725635580292](https://twitter.com/GregKamradt/status/1722386725635580292), 2023.
* [37] AI@Meta. Llama 3 model card. [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md), 2024.
* [38] togetherAI. Llama-2-7b-32k-instruct -- and fine-tuning for llama-2 models with together api. [https://www.together.ai/blog/llama-2-7b-32k-instruct](https://www.together.ai/blog/llama-2-7b-32k-instruct), August 2023.
* [39] BlueLM Team. Bluelm: An open multilingual 7b language model. [https://github.com/vivo-ai-lab/BlueLM](https://github.com/vivo-ai-lab/BlueLM), 2023.
* [40] gradient.ai. Llama-3 8b gradient instruct 1048k model card. [https://huggingface.co/gradient/Llama-3-8B-Instruct-Gradient-1048k](https://huggingface.co/gradient/Llama-3-8B-Instruct-Gradient-1048k), 2024.
* [41] Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, et al. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. _arXiv preprint arXiv:2303.10420_, 2023.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Sec. 5. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See Sec. 5. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We open source our code and data, and provide the URL. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Due to the high cost, we only run 1 experiment using greedy decoding. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [No] It's hard to collect this information.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? We cite all the data sources. 2. Did you mention the license of the assets? We have included the license information in the supplementary.

* Did you include any new assets either in the supplemental material or as a URL? [Yes] We include new assets as a URL [https://huggingface.co/datasets/Infinigence/LVEval](https://huggingface.co/datasets/Infinigence/LVEval).
* Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] We didn't directly collect data from new human subjects, instead, we rely on some processing and annotation efforts to parse existing data sources, and we properly follow their license. Therefore, this question is not a major concern of our assets.
* Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] We didn't directly collect data from new human subjects, instead, we rely on some processing and annotation efforts to parse existing data sources, and we properly follow their license. Therefore, this question is not a major concern of our assets.
* If you used crowdsourcing or conducted research with human subjects...
* Did you include the full text of instructions given to participants and screenshots, if applicable? [No]
* Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [No]
* Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [No]
Detailed Construction of QA Pairs

Multi-hop QA.In a multi-hop QA task, the reasoning to derive the answer needs to gather multiple pieces of information from various locations in the context. We construct four multi-hop QA datasets: **dureader-mixup**, **loogle-CR-mixup**, **loogle-MR-mixup**, and **hotpotwikiqa-mixup**.

* **hotpotwikiqa-mixup** is originated from two Wikipedia-based multi-hop QA datasets: HotpotQA and 2WikiMultihopQA. HotpotQA contains 112,779 2-hop questions that are written by native speakers according to two given paragraphs as the context. 2WikiMultihopQA contains 192,606 5-hop questions that are synthesized using manually designed templates to prevent shortcut solutions. We select 124 samples from the two datasets.
* **loogle-MR-mixup** and **loogle-CR-mixup** originate from LooGLE's Long-dependency QA task, specifically the _Multiple information Retrieval_ and _Comprehension and Reasoning_ subtasks. The _Multiple information Retrieval_ task requires aggregation of the evidence that can be directly located in original sentences, while the _Comprehension and Reasoning_ task contains implicit evidence within the context, it requires multi-step reasoning to get the correct answers. We select 139 and 99 questions for **loogle-MR-mixup** and **loogle-CR-mixup**, respectively.
* **dureader-mixup** is built from the DuReader dataset. We first randomly select 200 instances and then manually remove 24 samples whose answers are longer than 360 words.

Single-hop QA.In a single-hop QA task, only a single evidence in the context is needed to derive the answer. We construct seven single-hop QA datasets: **lic-mixup**, **loogle-SD-mixup**, **cmrc-mixup**, **multifieldqa-en-mixup**, **multifieldqa-zh-mixup**, **factrecall-en**, and **factrecall-zh**.

* **lic-mixup** is originated from the Long-instruction-en2zh dataset on Hugging Face. Long-instruction-en2zh contains 8,000+ high-quality Chinese multi-doc QA data translated from English. We selected 197 QA pairs and their corresponding documents as supporting data, while the remaining documents serve as distracting data for context mixing.
* **loogle-SD-mixup** contains 160 unique QA pairs and 800 documents originated from the short-dependency QA task in LooGLE.
* **cmrc-mixup** is derived from the CMRC 2018 Public Datasets, designed for Chinese machine reading comprehension. It contains \(\sim\)20\(k\) questions annotated on Wikipedia documents by human experts. We manually pick 200 QA pairs and their corresponding documents as supporting QA pairs and documents.
* **multifieldqa-en-mixup** and **multifieldqa-zh-mixup** are built from the MultiFieldQA datasets in Long-Bench. We manually remove questions that can be answered using common-sense knowledge without referring to the context, and eventually get 101 and 133 unique QA pairs for **multifieldqa-en-mixup** and **multifieldqa-zh-mixup**, respectively.
* **factrecall-en** and **factrecall-zh** are two synthetic datasets designed to assess the LLMs' ability to identify a small piece of evidence ("fact") located at various locations within a lengthy context. As shown in Figure A10 A11, we write one English fact-question-answer pair for **factrecall-en** and one Chinese fact-question-answer pair for **factrecall-zh**. distracting documents are sourced from _PG-19_ dataset (English) and the book of _Journey to the West_ (Chinese) to create five contexts of different length levels. For each context, we generate 200 documents by inserting the fact at 200 evenly spaced positions within the context.

## Appendix B Detailed Evaluation Results

The information of all LLMs are listed in Table A4.

The detailed results on each dataset of the single-hop QA task type and multi-hop QA task type are shown in Figure A8 and Figure A9, respectively. We can see that (i) Among the multi-hop QA datasets, **loogle-CR-mixup** and **loogle-MR-mixup** are particularly challenging. Future research is needed to improve the ability to aggregate multiple pieces of evidence from a long context with distracting and confusing facts. (ii) For single-hop QA datasets, as expected, LLMs can achieve higher scores on datasets without CFI, including **logole-SD-mixup** and **cmrc-mixup**. (iii) Several LLMs, namely ChatGLM3-6B-32k, BlueLM-7B-32k-Chat, Yi-6B-200k, and Llama2-7B-32k-Instruct, can achieve relatively high scores on **factrecall-en**. This indicates that the "needle in a haystack" task might not be challenging enough, emphasizing the need to evaluate LLMs on other tasks, particularly multi-hop QA datasets. (iv) The performance gap between LLMs on **factrecall-en** and **factrecall-zh** is especially large, and some open-source LLMs with relatively small context sizes, namely Llama2-7B-Chat-hf (4k context window size), Qwen-7B-8k-Chat, and Vicuna-7B-16k-v1.5, even get

Figure 6: Keyword-recall-based two-stage metric calculation. (a) The vanilla F1 score (red) is inflated high. With the keyword recall-based metric, the final score is set to zero due to the low recall of answer keywords. (b) The vanilla F1 score (red) is inflated high due to irrelevant words. By filtering of blacklisted words, the final score is better calibrated.

Figure 7: Prompts used for GPT-4 to generate confusing facts.

near-zero scores. (v) A few LLMs have unbalanced performances on Chinese and English datasets, as illustrated by the results on **multifieldqa-en-mixup** and **multifieldqa-zh-mixup**. The detailed scores of all models on 5 length levels of all sub-datasets are shown in Table A1 A2.

## Appendix C Detailed Ablation Results

The ablation results of CFI/KPR and optimized metric are shown in Table A4 A5 A6 and Table A3 respectively.

## Appendix D Samples in \(Lv\)-Eval

The data samples of **factrecall-en** and **factrecall-zh** are shown in Figure A10 A11.

Figure A8: Average scores across all length levels of 12 LLMs on single-hop QA datasets.

[MISSING_PAGE_FAIL:18]

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline \hline \multirow{2}{*}{Model Name} & \multirow{2}{*}{Ablation} & \multicolumn{4}{c}{**hotpotwikiqa-mixup**} \\ \cline{3-7}  & & \(16k\) & \(32k\) & \(64k\) & \(128k\) & \(256k\) \\ \hline \multirow{3}{*}{Llama2-7B-Chat-hf} & w. both & 3.99 & 1.30 & 1.84 & 0.81 & 0.75 \\  & w. KPR & 4.10 & 1.56 & 1.36 & 0.63 & 0.88 \\  & w. CFI & 6.29 & 2.47 & 3.37 & 1.47 & 1.57 \\  & w.o. both & 6.48 & 2.48 & 2.98 & 1.29 & 1.57 \\ \hline \multirow{3}{*}{ChatGLM3-6B-32k} & w. both & 16.98 & 14.76 & 9.02 & 8.31 & 6.68 \\  & w. KPR & 21.32 & 13.04 & 9.99 & 6.56 & 6.12 \\  & w. CFI & 27.06 & 24.75 & 17.57 & 12.89 & 10.88 \\  & w.o. both & 28.48 & 21.96 & 18.89 & 11.31 & 10.69 \\ \hline \multirow{3}{*}{LongChat-7B-32k-v1.5} & w. both & 11.57 & 10.71 & 4.77 & 5.49 & 2.37 \\  & w. KPR & 11.07 & 6.17 & 5.27 & 5.31 & 3.06 \\  & w. CFI & 19.48 & 14.33 & 9.41 & 11.34 & 6.44 \\  & w.o. both & 18.79 & 12.44 & 9.94 & 11.33 & 7.47 \\ \hline \multirow{3}{*}{Llama2-7B-32k-Instruct} & w. both & 3.54 & 2.31 & 2.20 & 1.86 & 1.62 \\  & w. KPR & 4.41 & 2.67 & 2.37 & 2.04 & 1.39 \\  & w. CFI & 5.13 & 3.23 & 4.77 & 3.53 & 2.81 \\  & w.o. both & 5.44 & 3.98 & 4.85 & 3.28 & 2.78 \\ \hline \multirow{3}{*}{Yi-6B-200k} & w. both & 23.55 & 18.94 & 9.94 & 7.66 & 2.01 \\  & w. KPR & 23.84 & 13.77 & 6.52 & 6.69 & 3.84 \\  & w. CFI & 33.32 & 16.89 & 11.00 & 7.62 & 8.09 \\  & w.o. both & 30.71 & 17.62 & 10.43 & 10.17 & 8.51 \\ \hline \multirow{3}{*}{Vicuna-7B-16k-v1.5} & w. both & 2.63 & 2.19 & 2.05 & 1.04 & 1.85 \\  & w. KPR & 2.09 & 1.63 & 1.27 & 1.13 & 1.98 \\ \cline{1-1}  & w. CFI & 5.84 & 3.58 & 2.60 & 1.82 & 1.09 \\ \cline{1-1}  & w.o. both & 5.81 & 4.09 & 3.30 & 1.48 & 1.22 \\ \hline \hline \end{tabular}
\end{table}
Table A4: Ablation results on **hotpotwikiqa-mixup** for confusing facts insertion (CFI) and keyword and phrase replacement (KPR).

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline \hline \multirow{2}{*}{Model Name} & \multirow{2}{*}{Ablation} & \multicolumn{4}{c}{**multifieldqa-en-mixup**} \\ \cline{3-7}  & & \(16k\) & \(32k\) & \(64k\) & \(128k\) & \(256k\) \\ \hline \multirow{3}{*}{Llama2-7B-Chat-hf} & w. both & 8.81 & 5.55 & 1.58 & 2.54 & 1.49 \\  & w. KPR & 8.43 & 4.84 & 1.93 & 2.46 & 0.95 \\  & w. CFI & 9.05 & 6.08 & 3.29 & 3.59 & 1.44 \\  & w.o. both & 9.65 & 6.08 & 3.29 & 3.59 & 1.67 \\ \hline \multirow{3}{*}{ChatGLM3-6B-32k} & w. both & 25.40 & 12.78 & 12.32 & 9.89 & 4.24 \\  & w. KPR & 33.54 & 17.27 & 12.15 & 8.94 & 4.44 \\  & w. CFI & 31.97 & 19.80 & 14.12 & 10.54 & 6.40 \\  & w.o. both & 41.46 & 24.29 & 14.32 & 10.31 & 6.24 \\ \hline \multirow{3}{*}{LongChat-7B-32k-v1.5} & w. both & 12.02 & 7.58 & 7.84 & 3.11 & 4.22 \\  & w. KPR & 15.32 & 10.61 & 6.49 & 3.02 & 4.94 \\  & w. CFI & 15.56 & 8.77 & 13.16 & 9.88 & 8.65 \\  & w.o. both & 20.45 & 12.91 & 11.69 & 9.28 & 8.59 \\ \hline \multirow{3}{*}{Llama2-7B-32k-Instruct} & w. both & 8.03 & 4.96 & 4.12 & 3.90 & 2.13 \\  & w. KPR & 11.09 & 6.20 & 3.06 & 3.62 & 3.45 \\  & w. CFI & 7.81 & 6.12 & 3.92 & 4.03 & 3.38 \\  & w.o. both & 11.86 & 6.84 & 4.75 & 3.95 & 3.34 \\ \hline \multirow{3}{*}{Yi-6B-200k} & w. both & 10.01 & 9.24 & 8.83 & 5.98 & 4.69 \\  & w. KPR & 12.69 & 13.67 & 11.05 & 7.30 & 5.70 \\  & w. CFI & 12.02 & 9.70 & 11.19 & 5.91 & 7.29 \\  & w.o. both & 16.78 & 13.35 & 12.38 & 7.83 & 7.27 \\ \hline \multirow{3}{*}{Vicuna-7B-16k-v1.5} & w. both & 6.29 & 4.32 & 2.79 & 2.51 & 1.28 \\  & w. KPR & 8.07 & 4.32 & 2.67 & 2.65 & 1.31 \\ \cline{1-1}  & w. CFI & 9.02 & 6.66 & 5.40 & 2.94 & 2.37 \\ \cline{1-1}  & w.o. both & 9.49 & 6.88 & 5.52 & 2.90 & 2.09 \\ \hline \hline \end{tabular}
\end{table}
Table A5: Ablation results on **multifieldqa-en-mixup** for confusing facts insertion (CFI) and keyword and phrase replacement (KPR).

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline \hline \multirow{2}{*}{Model Name} & \multirow{2}{*}{Ablation} & \multicolumn{4}{c}{**multifieldqa-zh-mixup**} \\ \cline{3-7}  & & \(16k\) & \(32k\) & \(64k\) & \(128k\) & \(256k\) \\ \hline \multirow{4}{*}{Llama2-7B-Chat-hf} & w. both & 4.72 & 1.21 & 0.68 & 0.24 & 0.56 \\  & w. KPR & 5.45 & 1.26 & 1.06 & 0.21 & 0.57 \\  & w. CFI & 4.83 & 2.06 & 0.71 & 0.30 & 0.42 \\  & w.o. both & 5.49 & 2.17 & 0.62 & 0.30 & 0.42 \\ \hline \multirow{4}{*}{ChatGLM3-6B-32k} & w. both & 32.38 & 24.48 & 20.97 & 10.08 & 7.05 \\  & w. KPR & 44.90 & 40.23 & 23.03 & 14.26 & 7.50 \\  & w. CFI & 33.24 & 28.38 & 20.75 & 15.84 & 8.96 \\  & w.o. both & 44.80 & 42.65 & 27.66 & 17.73 & 9.51 \\ \hline \multirow{4}{*}{LongChat-7B-32k-v1.5} & w. both & 9.81 & 8.82 & 3.23 & 3.54 & 3.92 \\  & w. KPR & 11.29 & 10.24 & 4.24 & 3.60 & 3.89 \\  & w. CFI & 13.50 & 9.76 & 4.27 & 4.00 & 3.82 \\  & w.o. both & 16.59 & 11.31 & 5.13 & 3.96 & 3.82 \\ \hline \multirow{4}{*}{Llama2-7B-32k-Instruct} & w. both & 4.55 & 3.93 & 1.45 & 1.74 & 1.15 \\  & w. KPR & 5.99 & 3.88 & 1.92 & 2.72 & 1.17 \\  & w. CFI & 4.12 & 5.10 & 2.13 & 1.64 & 2.29 \\  & w.o. both & 7.62 & 5.04 & 2.37 & 1.73 & 2.29 \\ \hline \multirow{4}{*}{Yi-6B-200k} & w. both & 2.85 & 0.75 & 1.89 & 2.11 & 1.58 \\  & w. KPR & 4.62 & 4.43 & 2.51 & 3.60 & 2.18 \\  & w. CFI & 3.32 & 2.69 & 2.67 & 2.95 & 1.80 \\  & w.o. both & 4.47 & 5.61 & 3.58 & 4.07 & 2.59 \\ \hline \multirow{4}{*}{Vicuna-7B-16k-v1.5} & w. both & 5.82 & 4.45 & 2.03 & 0.88 & 1.26 \\  & w. KPR & 8.18 & 4.70 & 1.81 & 0.89 & 0.96 \\ \cline{1-1}  & w. CFI & 10.03 & 5.70 & 2.62 & 3.42 & 1.99 \\ \cline{1-1}  & w.o. both & 10.22 & 5.77 & 3.08 & 3.00 & 1.83 \\ \hline \end{tabular}
\end{table}
Table A6: Ablation results on **multifieldqa-zh-mixup** for confusing facts insertion (CFI) and keyword and phrase replacement (KPR).

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline \hline \multirow{2}{*}{Model Name} & \multirow{2}{*}{Ablation} & \multicolumn{5}{c}{**factrecall-en**} \\ \cline{3-7}  & & \(16k\) & \(32k\) & \(64k\) & \(128k\) & \(256k\) \\ \hline \multirow{3}{*}{Llama2-7B-Chat-hf} & w. both & 1.08 & 0.46 & 0.31 & 0.23 & 0.15 \\  & w. KPR & 1.08 & 0.46 & 0.31 & 0.23 & 0.15 \\  & w. CFI & 2.38 & 1.69 & 1.69 & 0.69 & 1.15 \\  & w.o. both & 2.69 & 2.00 & 1.77 & 0.77 & 1.23 \\ \hline \multirow{3}{*}{ChatGLM3-6B-32k} & w. both & 91.50 & 89.00 & 46.00 & 24.00 & 12.50 \\  & w. KPR & 100 & 98.50 & 49.50 & 25.00 & 13.00 \\  & w. CFI & 100 & 97.00 & 48.50 & 24.00 & 13.00 \\  & w.o. both & 100 & 98.50 & 49.50 & 25.00 & 13.00 \\ \hline \multirow{3}{*}{LongChat-7B-32k-v1.5} & w. both & 9.22 & 14.33 & 8.31 & 7.86 & 6.00 \\  & w. KPR & 42.25 & 29.80 & 11.06 & 8.86 & 7.00 \\  & w. CFI & 56.92 & 51.30 & 49.25 & 54.79 & 73.70 \\  & w.o. both & 65.48 & 71.43 & 64.03 & 64.26 & 85.75 \\ \hline \multirow{3}{*}{Llama2-7B-32k-Instruct} & w. both & 75.20 & 56.00 & 33.00 & 17.85 & 8.40 \\  & w. KPR & 80.00 & 72.00 & 37.00 & 19.05 & 8.80 \\  & w. CFI & 65.10 & 47.81 & 43.15 & 45.77 & 31.93 \\  & w.o. both & 66.45 & 62.16 & 64.45 & 61.82 & 39.13 \\ \hline \multirow{3}{*}{Yi-6B-200k} & w. both & 24.88 & 23.09 & 24.96 & 22.04 & 16.44 \\  & w. KPR & 41.78 & 38.87 & 37.42 & 34.96 & 19.07 \\  & w. CFI & 34.97 & 32.52 & 30.24 & 28.91 & 27.43 \\  & w.o. both & 36.89 & 33.72 & 32.96 & 32.36 & 31.17 \\ \hline \multirow{3}{*}{Vicuna-7B-16k-v1.5} & w. both & 0 & 0 & 0 & 0.25 & 0.20 \\  & w. KPR & 0.70 & 0.38 & 0 & 0.17 & 0 \\  & w. CFI & 7.06 & 9.74 & 4.59 & 2.76 & 2.21 \\  & w.o. both & 24.69 & 14.81 & 6.49 & 3.26 & 2.71 \\ \hline \hline \multirow{3}{*}{Model Name} & \multirow{2}{*}{Ablation} & \multicolumn{5}{c}{**factrecall-zh**} \\ \cline{3-7}  & & \(16k\) & \(32k\) & \(64k\) & \(128k\) & \(256k\) \\ \hline \multirow{3}{*}{Llama2-7B-Chat-hf} & w. both & 0 & 0 & 0 & 0 & 0 \\  & w. KPR & 0 & 0 & 0 & 0 & 0 \\  & w. CFI & 0 & 0 & 0 & 0 & 0 \\  & w.o. both & 1.07 & 0.92 & 0.80 & 0.71 & 0.64 \\ \hline \multirow{3}{*}{ChatGLM3-6B-32k} & w. both & 0 & 2.00 & 12.50 & 9.00 & 7.00 \\  & w. KPR & 91.83 & 78.00 & 41.00 & 17.17 & 8.50 \\  & w. CFI & 81.58 & 74.33 & 51.75 & 27.00 & 14.50 \\  & w.o. both & 63.19 & 68.33 & 67.26 & 63.04 & 58.23 \\ \hline \multirow{3}{*}{LongChat-7B-32k-v1.5} & w. both & 7.20 & 5.00 & 3.50 & 3.70 & 2.00 \\  & w. KPR & 20.26 & 7.50 & 5.50 & 3.70 & 2.50 \\  & w. CFI & 6.92 & 4.62 & 4.95 & 3.42 & 2.50 \\  & w.o. both & 37.26 & 33.28 & 29.77 & 26.76 & 24.38 \\ \hline \multirow{3}{*}{Llama2-7B-32k-Instruct} & both & 2.55 & 0.74 & 0.53 & 0.49 & 0.29 \\  & w. KPR & 2.92 & 1.60 & 0.45 & 0.49 & 0.43 \\  & w. CFI & 3.43 & 0.70 & 0.75 & 1.20 & 1.85 \\  & w.o. both & 27.08 & 23.45 & 20.69 & 18.53 & 16.86 \\ \hline \multirow{3}{*}{Yi-6B-200k} & w. both & 25.73 & 16.86 & 12.41 & 10.13 & 4.62 \\  & w. KPR & 22.63 & 17.92 & 8.02 & 3.07 \\  & w. CFI & 32.00 & 30.64 & 21.45 & 12.13 & 16.95 \\  & w.o. both & 30.40 & 30.15 & 29.60 & 29.21 & 28.71 \\ \hline \multirow{3}{*}{Vicuna-7B-16k-v1.5} & w. both & 0 & 0 & 0 & 0 & 0 \\  & w. KPR & 0 & 0 & 0 & 0 \\  & w. CFI & 0 & 0 & 0 & 0 \\  & w.o. both & 0.91 & 0.78 & 0.68 & 0.61 & 0.54 \\ \hline \hline \end{tabular}
\end{table}
Table A7: Ablation results on **factrecall-en** and **factrecall-zh** for confusing facts insertion (CFI) and keyword and phrase replacement (KPR).

Figure A11: A sample in **factrecall-zh**.