[MISSING_PAGE_FAIL:1]

different variants of Data Shapley have been proposed [29, 22, 63, 6, 39, 44, 69, 33, 60], reflecting its effectiveness in quantifying data point contributions to ML model training.

**KNN-Shapley.** While being a principled approach for data valuation, the exact calculation of the Shapley value is computationally prohibitive [30]. Various approximation algorithms for Data Shapley have been proposed [30, 28, 50, 64, 7, 48, 44, 62], but these approaches still require substantial computational resources due to model retraining. Fortunately, a breakthrough by [29] showed that computing the _exact_ Data Shapley for K-Nearest Neighbors (KNN), one of the oldest yet still popular ML algorithms, is surprisingly easy and efficient. KNN-Shapley quantifies data value based on KNN's Data Shapley score; it can be applied to large, high-dimensional CV/NLP datasets by calculating the value scores on the last-layer neural network embeddings. Owing to its superior efficiency and effectiveness in discerning data quality, KNN-Shapley is recognized as one of the most practical data valuation techniques nowadays [52]. It has been applied to various ML domains including active learning [24], continual learning [57], NLP [43, 42], and semi-supervised learning [9].

**Motivation: privacy risks in data valuation.** In this work, we study a critical, yet often overlooked concern in the deployment of data valuation: privacy leakage associated with data value scores released to data holders. The value of a single data point is always relative to other data points in the training set. This, however, can potentially reveal sensitive information about the rest of data holders in the dataset. This problem becomes even more complex when considering a strong threat model where multiple data holders collude, sharing their received data values to determine the membership of a particular individual. As data valuation techniques such as KNN-Shapley become increasingly popular and relevant in various applications, understanding and addressing the privacy challenges of data valuation methods is of utmost importance. In this work, we study this critical issue through the lens of differential privacy (DP) [15], a de-facto standard for privacy-preserving applications.

Our technical contributions are listed as follows.

**Privacy Risks & Challenges of Privatization for KNN-Shapley** (Section 3). We demonstrate that data value scores (specifically KNN-Shapley) indeed serve as a new channel for private information leakage, potentially exposing sensitive information about individuals in the dataset. In particular, we explicitly design a privacy attack (in Appendix B.2.2) where an adversary could infer the presence/absence of certain data points based on the variations in the KNN-Shapley scores, analogous to the classic membership inference attack on ML model [58]. Additionally, we highlight the technical challenges in incorporating the current KNN-Shapley technique with differential privacy, such as its large global sensitivity. All these results emphasize the need for a new, privacy-friendly approach to data valuation.

**TKNN-Shapley: an efficient, privacy-friendly data valuation technique** (Section 4.2). To address the privacy concerns, we derive a novel variant of KNN-Shapley. This new method considers the Data Shapley of an alternative form of KNN classifier called _Threshold-KNN_ (TKNN) [5], which takes into account all neighbors within a pre-specified threshold of a test example, rather than the exact \(K\) nearest neighbors. We derive the closed-form formula of Data Shapley for TKNN (i.e., TKNN-Shapley). We show that it can be computed _exactly_ with exact linear-time computational efficiency over the original KNN-Shapley. **DP-TKNN-Shapley** (Section 5). Importantly, we recognize that TKNN-Shapley only depends on three simple counting queries. Hence, TKNN-Shapley can be conveniently transformed into a differentially private version by DP's post-processing property. Moreover, we prove that such a DP variant satisfies several favorable properties, including (1) the high computational efficiency, (2) the capability to withstand collusion among data holders without compromising the privacy guarantee, and (3) the ease of integrating subsampling for privacy amplification.

**Numerical experiments** (Section 6). We evaluate the performance of TKNN-Shapley across 11 commonly used benchmark datasets and 2 NLP datasets. Key observations include: (1) TKNN-Shapley surpasses KNN-Shapley in terms of computational efficiency; (2) DP-TKNN-Shapley significantly outperforms the naively privatized KNN-Shapley in terms of privacy-utility tradeoff in discerning data quality; (3) even non-private TKNN-Shapley achieves comparable performance as KNN-Shapley.

Overall, our work suggests that TKNN-Shapley, being a privacy-friendly, yet more efficient and effective alternative to the original KNN-Shapley, signifies a milestone toward practical data valuation.

Background of Data Valuation

In this section, we formalize the data valuation problem for ML, and review the method of Data Shapley and KNN-Shapley.

**Setup & Goal.** Consider a dataset \(D:=\{z_{i}\}_{i=1}^{N}\) consisting of \(N\) data points where each data point \(z_{i}:=(x_{i},y_{i})\) is collected from a data owner \(i\). The objective of data valuation is to attribute a score to each training data point \(z_{i}\), reflecting its importance or quality in ML model training. Formally, we aim to determine a score vector \((\phi_{z_{i}})_{i=1}^{N}\), wherein \(\phi_{z_{i}}\) represents the value of data point \(z_{i}\). For any reasonable data valuation method, the value of a data point is always relative to other data points in the dataset. For instance, if a data point has many duplicates in the dataset, its value will likely be lower. Hence, \(\phi_{z_{i}}\) is a function of the leave-one-out dataset \(D_{-z_{i}}:=D\setminus\{z_{i}\}\). We write \(\phi_{z_{i}}(D_{-z_{i}})\) when we want to stress the dependency of a data value score with the rest of the data points.

**Utility Function.** Most of the existing data valuation techniques are centered on the concept of _utility function_, which maps an input dataset to a score indicating the usefulness of the training set. A common choice for utility function is the _validation accuracy_ of a model trained on the input training set. Formally, for a training set \(S\), a utility function \(v(S):=\texttt{acc}(\mathcal{A}(S))\), where \(\mathcal{A}\) is a learning algorithm that takes a dataset \(S\) as input and returns a model; \(\texttt{acc}(\cdot)\) is a metric function that evaluates the performance of a given model, e.g., the classification accuracy on a hold-out validation set.

### The Shapley Value

The Shapley value (SV) [56] is a classic concept from game theory to attribute the total gains generated by the coalition of all players. At a high level, it appraises each point based on the (weighted) average utility change caused by adding the point into different subsets of the training set. Formally, given a utility function \(v(\cdot)\) and a training set \(D\), the Shapley value of a data point \(z\in D\) is defined as

\[\phi_{z}\left(D_{-z};v\right):=\frac{1}{N}\sum_{k=1}^{N}\binom{N-1}{k-1}^{-1} \sum_{S\subseteq D_{-z},|S|=k-1}\left[v(S\cup\{z\})-v(S)\right] \tag{1}\]

For notation simplicity, when the context is clear, we omit the utility function and/or leave-one-out dataset, and write \(\phi_{z}(D_{-z})\), \(\phi_{z}(v)\) or \(\phi_{z}\) depending on the specific dependency we want to stress.

The popularity of the Shapley value is attributable to the fact that it is the _unique_ data value notion satisfying four axioms: Dummy player, Symmetry, Linearity, and Efficiency. We refer the readers to [23], [30] and the references therein for a detailed discussion about the interpretation and necessity of the four axioms in the ML context. Here, we introduce the _linearity_ axiom which will be used later.

**Theorem 1** (Linearity of the Shapley value [56]).: _For any of two utility functions \(v_{1},v_{2}\) and any \(\alpha_{1},\alpha_{2}\in\mathbb{R}\), we have \(\phi_{z}\left(\alpha_{1}v_{1}+\alpha_{2}v_{2}\right)=\alpha_{1}\phi_{z}\left( v_{1}\right)+\alpha_{2}\phi_{z}\left(v_{2}\right)\)._

### KNN-Shapley

Formula (1) suggests that the exact Shapley value can be computationally prohibitive in general, as it requires evaluating \(v(S)\) for all possible subsets \(S\subseteq D\). Surprisingly, [29; 61] showed that for \(K\)-Nearest Neighbor (KNN), the computation of the exact Data Shapley score is highly efficient. Following its introduction, KNN-Shapley has rapidly gained attention and follow-up works across diverse areas of machine learning [24; 43; 42; 9]. In particular, it has been recognized by recent studies as "the most practical data valuation technique capable of handling large-scale data effectively" [52; 33].

Specifically, the performance of an unweighted KNN classifier is typically evaluated by its validation accuracy. For a given validation set \(D^{(\mathrm{val})}=\{z_{i}^{(\mathrm{val})}\}_{i=1}^{N_{\mathrm{val}}}\), we can define KNN's utility function \(v_{D^{(\mathrm{val})}}^{\texttt{KNN}}\) on a non-empty training set \(S\) as \(v_{D^{(\mathrm{val})}}^{\texttt{KNN}}(S):=\sum_{z^{(\mathrm{val})}\in D^{( \mathrm{val})}}v_{z^{(\mathrm{val})}}^{\texttt{KNN}}(S)\), where

\[v_{z^{(\mathrm{val})}}^{\texttt{KNN}}(S):=\frac{1}{\min(K,|S|)}\sum_{j=1}^{ \min(K,|S|)}\mathbb{1}\left[y_{\pi^{(S)}(j;x^{(\mathrm{val})})}=y^{(\mathrm{ val})}\right] \tag{2}\]

is the probability of a (soft-label) KNN classifier in predicting the correct label for a validation point \(z^{(\mathrm{val})}=(x^{(\mathrm{val})},y^{(\mathrm{val})})\in D^{(\mathrm{ val})}\). In (2), \(\pi^{(S)}(i;x^{(\mathrm{val})})\) is the index of the \(i\)th closest data point in \(S\) to \(x^{(\mathrm{val})}\). When \(|S|=0\), \(v^{\texttt{KNH}}_{z_{z(\mathrm{val})}}(S)\) is set to the accuracy by random guessing. The distance is measured through suitable metrics such as \(\ell_{2}\) distance. Here is the main result in [29; 61]:

**Theorem 2** (KNN-Shapley [30; 61] (simplified version)).: _Consider the utility function in (2). Given a validation data point \(z^{(\mathrm{val})}=(x^{(\mathrm{val})},y^{(\mathrm{val})})\) and a distance metric \(d(\cdot,\cdot)\), if we sort the training set \(D=\{z_{i}=(x_{i},y_{i})\}_{i=1}^{N}\) according to \(d(x_{i},x^{(\mathrm{val})})\) in ascending order, then the Shapley value of each data point \(\phi^{\texttt{KNH}}_{z_{i}}\) corresponding to utility function \(v^{\texttt{KNH}}_{z^{(\mathrm{val})}}\) can be computed recursively as follows:_

\[\phi^{\texttt{KNH}}_{z_{N}}:=f_{N}(D),\ \ \text{and}\ \ \phi^{\texttt{KNH}}_{z_{i}}:= \phi^{\texttt{KNH}}_{z_{i+1}}+f_{i}(D)\ \ \text{ for }i=1,\ldots,N-1\]

_where the exact form of functions \(f_{i}(D)\) can be found in Appendix B.1._

**Runtime:** _the computation of all Shapley values \((\phi^{\texttt{KNH}}_{z_{1}},\ldots,\phi^{\texttt{KNH}}_{z_{N}})\) can be achieved in \(O(N\log N)\) runtime in total (dominated by the sorting data points in \(D\))._

**The Shapley value corresponding to full validation set:** _for a validation set \(D^{(\mathrm{val})}\), recall that \(v^{\texttt{KNH}}_{D^{(\mathrm{val})}}(S):=\sum_{z^{(\mathrm{val})}\in D^{( \mathrm{val})}}v^{\texttt{KNH}}_{z^{(\mathrm{val})}}(S)\). One can compute the Shapley value corresponding to \(v^{\texttt{KNH}}_{D^{(\mathrm{val})}}\) by summing each \(\phi^{\texttt{KNH}}_{z_{i}}\left(v^{\texttt{KNH}}_{z^{(\mathrm{val})}}\right)\), i.e., \(\phi^{\texttt{KNH}}_{z_{i}}\left(v^{\texttt{KNH}}_{D^{(\mathrm{val})}}\right)= \sum_{z^{(\mathrm{val})}\in D^{(\mathrm{val})}}\phi^{\texttt{KNH}}_{z_{i}}\left( v^{\texttt{KNH}}_{z^{(\mathrm{val})}}\right)\)._

Theorem 2 tells that for any validation data point \(z^{(\mathrm{val})}\), we can compute the _exact_ Shapley value \(\phi^{\texttt{KNH}}_{z_{i}}\left(D_{-z_{i}};v^{\texttt{KNH}}_{z^{(\mathrm{val}) }}\right)\) for _all_\(z_{i}\in D\) by using a recursive formula within a total runtime of \(O(N\log N)\). After computing the Shapley value \(\phi^{\texttt{KNH}}_{z_{i}}\left(v^{\texttt{KNH}}_{z^{(\mathrm{val})}}\right)\) for each \(z^{(\mathrm{val})}\in D^{(\mathrm{val})}\), one can compute the Shapley value corresponding to the full validation set by simply taking the sum of each \(\phi^{\texttt{KNH}}_{z_{i}}\left(v^{\texttt{KNH}}_{z^{(\mathrm{val})}}\right)\), due to the linearity property (Theorem 1) of the Shapley value.

**Remark 1** (Criteria of Computational Efficiency).: _As prior data valuation literature [23], we focus on the total runtime required to release all data value scores \(\left(\phi^{\texttt{KNH}}_{z_{1}},\ldots,\phi^{\texttt{KNH}}_{z_{N}}\right)\). This is because, in a practical data valuation scenario (e.g., profit allocation), we rarely want to compute the data value score for only a single data point; instead, a typical objective is to compute the data value scores for all data points within the training set._

## 3 Privacy Risks & Challenges of Privatization for KNN-Shapley

**Scenario.** Figure 1 illustrates the data valuation scenario and potential privacy leakages considered in our paper. Specifically, a centralized, trusted server collects data point \(z_{i}\) from data owner \(i\) for each \(i\in[N]\). The central server's role is to provide each data owner \(i\) with an assessment of the value of their data \(z_{i}\), e.g., the KNN-Shapley value \(\phi^{\texttt{KNH}}_{z_{i}}\). **A real life example:** Mayo Clinic has created a massive digital health patient data marketplace platform [68], where the patients submit part of their medical records onto the platform, and life science companies/labs pay a certain amount of money to purchase patients' data. The platform's responsibility is to gauge the worth of the data of each patient (i.e., the data owner) to facilitate fair compensation.

Figure 1: The potential privacy risks in data valuation arise from the dependency of the data value score \(\phi_{z_{i}}\) on the rest of the dataset \(D_{-z_{i}}\). Our goal is to privatize \(\phi_{z_{i}}(D_{-z_{i}})\) such that it provides strong differential privacy guarantee for the rest of the dataset \(D_{-z_{i}}\).

The privacy risks associated with KNN-Shapley (as well as other data valuation techniques) arise from the fact that \(\phi_{z_{i}}^{\text{\tiny\text{MRI}}}(D_{-z_{i}})\) depends on other data owners' data \(D_{-z_{i}}\). Consequently, the data value score \(\phi_{z_{i}}^{\text{\tiny\text{MRI}}}\) may inadvertently reveal private information (e.g., membership) about the rest of the dataset. The dependency of a data value score on the rest of the dataset is an unavoidable aspect of data valuation, as the value of a data point is inherently a relative quantity determined by its role within the complete dataset.

**Remark 2** (Other privacy risks in data valuation).: _It is important to note that in this work, we do not consider the privacy risks of revealing individuals' data to the central server. This is a different type of privacy risk that needs to be addressed using secure multi-party computation (MPC) technique [70], and it should be used together with differential privacy in practice. In addition, to use KNN-Shapley or many other data valuation techniques, the central server needs to maintain a clean, representative validation set, the privacy of which is not considered by this paper._

**A Simple Membership Inference (MI) Attack on KNN-Shapley (detailed in Appendix B.2.2).** We further illustrate the privacy risks of revealing data value scores with a concrete example. Analogous to the classic membership inference attack on ML model [58], in Appendix B.2.2 we show an example of privacy attack where an adversary could infer the presence/absence of certain data points in the dataset based on the variations in the KNN-Shapley scores. The design is analogous to the membership inference attack against ML models via the _likelihood ratio test_[8]. The AUROC score of the attack results is shown in Table 1. As we can see, our MIA attack can achieve a detection performance that is better than the random guess (0.5) for most of the settings. On some datasets, the attack performance can achieve \(>0.7\) AUROC. This demonstrates that privacy leakage in data value scores can indeed lead to non-trivial privacy attacks, and underscores the need for privacy safeguards in data valuation.1

Footnote 1: We stress that the goal here is to demonstrate that the data value scores can indeed serve as another channel of privacy leakage. We do _not_ claim any optimality of the attack we construct here. Improving MI attacks for data valuation is an interesting future work.

### Privatizing KNN-Shapley is Difficult

The growing popularity of data valuation techniques, particularly KNN-Shapley, underscores the critical need to mitigate the inherent privacy risks. In the quest for privacy protection, differential privacy (DP) [15] has emerged as the leading framework. DP has gained considerable recognition for providing robust, quantifiable privacy guarantees, thereby becoming the de-facto choice in privacy protection. In this section, we introduce the background of DP and highlight the technical difficulties in constructing a differentially private variant for the current version of KNN-Shapley.

**Background of Differential Privacy.** We use \(D,D^{\prime}\in\mathbb{N}^{\mathcal{X}}\) to denote two datasets with an unspecified size over space \(\mathcal{X}\). We call two datasets \(D\) and \(D^{\prime}\)_adjacent_ (denoted as \(D\sim D^{\prime}\)) if we can construct one by adding/removing one data point from the other, e.g., \(D=D^{\prime}\cup\{z\}\) for some \(z\in\mathcal{X}\).

**Definition 3** (Differential Privacy [15]).: _For \(\varepsilon,\delta\geq 0\), a randomized algorithm \(\mathcal{M}:\mathbb{N}^{\mathcal{X}}\rightarrow\mathcal{Y}\) is \((\varepsilon,\delta)\)-differentially private if for every pair of adjacent datasets \(D\sim D^{\prime}\) and for every subset of possible outputs \(E\subseteq\mathcal{Y}\), we have \(\Pr_{\mathcal{M}}[\mathcal{M}(D)\in E]\leq e^{\varepsilon}\Pr_{\mathcal{M}}[ \mathcal{M}(D^{\prime})\in E]+\delta\)._

That is, \((\varepsilon,\delta)\)-DP requires that for all adjacent datasets \(D,D^{\prime}\), the output distribution \(\mathcal{M}(D)\) and \(\mathcal{M}(D^{\prime})\) are close, where the closeness is measured by the parameters \(\varepsilon\) and \(\delta\). In our scenario, we would like to modify data valuation function \(\phi_{z_{i}}(D_{-z_{i}})\) to satisfy \((\varepsilon,\delta)\)-DP in order to protect the privacy of the rest of the dataset \(D_{-z_{i}}\). Gaussian mechanism [14] is a common way for privatizing a function; it introduces Gaussian noise aligned with the function's _global sensitivity_, which is the maximum output change when a single data point is added/removed from any given dataset.

**Definition 4** (Gaussian Mechanism [14]).: _Define the global sensitivity of a function \(f:\mathbb{N}^{\mathcal{X}}\rightarrow\mathbb{R}^{d}\) as \(\Delta_{2}(f):=\sup_{D\sim D^{\prime}}\left\|f(D)-f(D^{\prime})\right\|_{2}\). The Gaussian mechanism \(\mathcal{M}\) with noise level \(\sigma\) is then given by \(\mathcal{M}(D):=f(D)+\mathcal{N}\left(0,\sigma^{2}\mathbb{1}_{d}\right)\) where \(\mathcal{M}\) is \((\varepsilon,\delta)\)-DP with \(\sigma=\Delta_{2}(f)\sqrt{\log(1.25/\delta)}/\varepsilon\)._

\begin{table}
\begin{tabular}{r c c c c c c c} \hline \hline  & \(K=1\) & \(K=3\) & \(K=5\) & \(K=7\) & \(K=9\) & \(K=11\) & \(K=13\) & \(K=15\) \\ \hline
**Diffances** & 0.56 & 0.955 & 0.518 & 0.52 & 0.57 & 0.55 & 0.515 & 0.6 \\
**Parameters** & 0.674 & 0.513 & 0.565 & 0.505 & 0.588 & 0.512 & 0.502 \\
**CPU** & 0.765 & 0.548 & 0.520 & 0.572 & 0.588 & 0.512 & 0.612 & 0.615 \\
**Front** & 0.625 & 0.654 & 0.6 & 0.590 & 0.622 & 0.532 & 0.538 & 0.558 \\
**Celemental** & 0.542 & 0.683 & 0.626 & 0.665 & 0.603 & 0.607 & 0.602 & 0.60 \\
**Avginal** & 0.522 & 0.595 & 0.625 & 0.65 & 0.53 & 0.645 & 0.532 & 0.57 \\
**Cele** & 0.61 & 0.552 & 0.505 & 0.538 & 0.558 & 0.588 & 0.562 & 0.618 \\
**Wish** & 0.576 & 0.518 & 0.538 & 0.538 & 0.558 & 0.562 & 0.505 & 0.577 \\ \hline
**Wish** & 0.575 & 0.505 & 0.561 & 0.564 & 0.577 & 0.522 & 0.505 & 0.572 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results of the AUROC of the MI attack proposed in Appendix B.2.2 on KNN-Shapley. The higher the AUROC score is, the larger the privacy leakage is. The detailed algorithm description and experiment settings can be found in Appendix B.2.2.

**Challenges in making KNN-Shapley being differentially private (overview).** Here, we give an overview of the inherent difficulties in making the KNN-Shapley \(\phi_{z_{i}}^{\text{\tiny{\text{MN}}}}(D_{-z_{i}})\) (Theorem 2) to be differentially private, and we provide a more detailed discussion in Appendix B.3. **(1) Large global sensitivity:** In Appendix B.3, we show that the global sensitivity of \(\phi_{z_{i}}^{\text{\tiny{\text{MN}}}}(D_{-z_{i}})\) can significantly exceed the magnitude of \(\phi_{z_{i}}^{\text{\tiny{\text{MN}}}}\). Moreover, we prove that the global sensitivity bound cannot be further improved by constructing a specific pair of neighboring datasets that matches the bound. Hence, if we introduce random noise proportional to the global sensitivity bound, the resulting privatized data value score could substantially deviate from its non-private counterpart, thereby compromising the utility of the privatized data value scores. **(2) Computational challenges in incorporating privacy amplification by subsampling:** "Privacy amplification by subsampling" [4] is a technique where the subsampling of a dataset amplifies the privacy guarantees due to the reduced probability of an individual's data being included. Being able to incorporate such a technique is often important for achieving a decent privacy-utility tradeoff. However, in Appendix B.3 we show that the recursive nature of KNN-Shapley computation causes a significant increase in computational demand compared to non-private KNN-Shapley.

These challenges underscore the pressing need for new data valuation techniques that retain the efficacy and computational efficiency of KNN-Shapley, while being amenable to privatization.

## 4 The Shapley Value for Threshold-based Nearest Neighbor

Considering the privacy concerns and privatization challenges associated with the original KNN-Shapley method, we introduce _TKNN-Shapley_, a privacy-friendly alternative of KNN-Shapley which also achieves improved computational efficiency. At the core of this novel method is Threshold-KNN (TKNN) classifier, a simple variant of the KNN classifier. We will discuss how to incorporate DP for TKNN-Shapley in Section 5.

### Threshold-based Nearest Neighbor Classifier (TKNN)

Threshold-KNN (TKNN) [5; 72] is a variant of KNN classifier that considers neighbors within a pre-specified threshold of the query example, rather than exclusively focusing on the exact \(K\) nearest neighbors. Formally, for a training set \(S\) and a validation data point \(z^{(\text{val})}=(x^{(\text{val})},y^{(\text{val})})\), we denote \(\texttt{NB}_{x^{(\text{val})},\tau}(S):=\{(x,y)|(x,y)\in S,d(x,x^{(\text{val}) })\leq\tau\}\) the set of neighbors of \(x^{(\text{val})}\) in \(S\) within a pre-specified threshold \(\tau\). Similar to the utility function for KNN, we define the utility function for TKNN classifier when using a validation set \(D^{(\text{val})}\) as the aggregated prediction accuracy \(v_{D^{(\text{val})}}^{\text{TKNN}}(S):=\sum_{z^{(\text{val})}\in D^{(\text{val })}}v_{z^{(\text{val})}}^{\text{\tiny{\text{TKM}}}}(S)\) where

\[v_{z^{(\text{val})}}^{\text{\tiny{\text{TKM}}}}(S):=\begin{cases}\texttt{Constant}& |\texttt{NB}_{x^{(\text{val})},\tau}(S)|=0\\ \frac{1}{|\texttt{NB}_{x^{(\text{val})},\tau}(S)|}\sum_{(x,y)\in\texttt{NB}_{x ^{(\text{val})},\tau}(S)}\mathbb{1}[y=y^{(\text{val})}]&|\texttt{NB}_{x^{( \text{val})},\tau}(S)|>0\end{cases} \tag{3}\]

where \(\texttt{Constant}\) can be the trivial accuracy of random guess.

**Comparison with standard KNN. (1) Robustness to outliers.** Compared with KNN, TKNN is better equipped to deal with prediction phase outliers [5]. When predicting an outlier that is far from the entire training set, TKNN prevents the influence of distant, potentially irrelevant neighbors, leading to a more reliable prediction score for outliers. **(2) Inference Efficiency.** TKNN has slightly better computational efficiency compared to KNN, as it has \(O(N)\) instead of \(O(N\log N)\) inference time. This improvement is achieved because TKNN only requires the computation of neighbors within the threshold \(\tau\), rather than searching for the exact \(K\) nearest neighbors. **(3) TKNN is also a consistent estimator.** The consistency of standard KNN is a well-known theoretical result [26]. That is, for any target function that satisfies certain regularity conditions, KNN binary classifier/regressor is guaranteed to converge to the target function as the size of the training set grows to infinite. In Appendix C.1, we derived a similar consistency result for TKNN binary classifier/regressor.

**Remark 3** (Intuition: Why we consider TKNN?).: _The'recursive form' of KNN-Shapley is due to the sorting operation in the prediction of the standard KNN. The recursive form of the formula causes difficulties in incorporating KNN-Shapley with differential privacy. In contrast, TKNN avoids the recursive formula for its Data Shapley value; the intuition is that for TKNN, the selection of neighbors for prediction solely depends on the queried example and the validation data, and is independent of the other training data points._

### Data Shapley for TKNN (TKNN-Shapley)

With the introduction of TKNN classifier and its utility function, we now present our main result, the closed-form, efficiently computable Data Shapley formula for the TKNN classifier.

**Theorem 5** (TKNN-Shapley (simplified version)).: _Consider the utility function \(v_{z^{\text{\sc trank}}}^{\text{\sc trank}}\) in (3). Given a validation data point \(z^{\text{\sc val}}=(x^{\text{\sc val}},y^{\text{\sc val}})\) and a distance metric \(d(\cdot,\cdot)\), the Shapley value \(\phi_{z_{i}}^{\text{\sc trank}}\) of each training point \(z_{i}=(x_{i},y_{i})\in D\) corresponding to utility function \(v_{z^{\text{\sc trank}}}^{\text{\sc trank}}\) can be calculated as_

\[\phi_{z_{i}}^{\text{\sc trank}}=f\left(\textbf{C}_{z^{\text{\sc val}}}(D_{-z_{i }})\right) \tag{4}\]

_where \(\textbf{C}_{z^{\text{\sc val}}}:=(\textbf{c},\textbf{c}_{x^{\text{\sc val}}, \tau},\textbf{c}_{z^{\text{\sc val}},\tau}^{(+)})\) is a 3-dimensional function/vector s.t._

\[\textbf{c}=\textbf{c}(D_{-z_{i}}):=\left|D_{-z_{i}}\right| (\text{size of }D_{-z_{i}})\] \[\textbf{c}_{x^{\text{\sc val}},\tau}=\textbf{c}_{x^{\text{\sc val }},\tau}(D_{-z_{i}}):=1+\left|\textbf{N}_{\textbf{z}^{\text{\sc val}},\tau}(D_ {-z_{i}})\right| (1+\text{\# neighbors of }x^{\text{\sc val}}\text{ in }D_{-z_{i}})\] \[\textbf{c}_{z^{\text{\sc val}},\tau}^{(+)}=\textbf{c}_{z^{\text{ \sc val}},\tau}^{(+)}(D_{-z_{i}}):=\sum_{(x,y)\in\textbf{N}_{\textbf{z}^{\text {\sc val}},\tau}(D_{-z_{i}})}\mathbbm{1}[y=y^{\text{\sc val}}] (\text{\# same-label neighbors in }D_{-z_{i}})\]

_and \(f(\cdot)\) is a function whose exact form can be found in Appendix C.2._

**Runtime:**_the computation of all \((\phi_{z_{1}}^{\text{\sc trank}},\ldots,\phi_{z_{N}}^{\text{\sc trank}})\) can be achieved in \(O(N)\) runtime (see Appendix C.2.1)._

**The Shapley value when using full validation set:**_The Shapley value corresponding to the utility function \(v_{D^{\text{\sc trank}}}^{\text{\sc trank}}\) can be calculated as \(\phi_{z_{i}}^{\text{\sc trank}}\left(v_{D^{\text{\sc val}}}^{\text{\sc trank} }\right)=\sum_{z^{\text{\sc val}}\in D(\text{\sc val})}\phi_{z_{i}}^{\text{ \sc trank}}\left(v_{z^{\text{\sc trank}}}^{\text{\sc trank}}\right)\)._

The main technical challenges of proving Theorem 5 lies in showing \(\textbf{C}_{z^{\text{\sc val}}}=(\textbf{c},\textbf{c}_{x^{\text{\sc val}}, \tau},\textbf{c}_{z^{\text{\sc val}},\tau}^{(+)})\), three simple counting queries on \(D_{-z_{i}}\), are the key quantities for computing \(\phi_{z_{i}}^{\text{\sc trank}}\). In later part of the paper, we will often view \(\phi_{z_{i}}^{\text{\sc trank}}\) as a function of \(\textbf{C}_{z^{\text{\sc val}}}\) and denote \(\phi_{z_{i}}^{\text{\sc trank}}\left[\textbf{C}_{z^{\text{\sc val}}}\right]:= f(\textbf{C}_{z^{\text{\sc val}}})\) when we want to stress this dependency. The full version and the proof for TKNN-Shapley can be found in Appendix C.2.1. Here, we briefly discuss why TKNN-Shapley can achieve \(O(N)\) runtime _in total_.

**Efficient Computation of TKNN-Shapley.** As we can see, all of the quantities in \(\textbf{C}_{z^{\text{\sc val}}}\) are simply counting queries on \(D_{-z_{i}}\), and hence \(\textbf{C}_{z^{\text{\sc val}}}(D_{-z_{i}})\) can be computed in \(O(N)\) runtime for any \(z_{i}\in D\). A more efficient way to compute \(\textbf{C}_{z^{\text{\sc val}}}(D_{-z_{i}})\) for _all_\(z_{i}\in D\), however, is to first compute \(\textbf{C}_{z^{\text{\sc val}}}(D)\) on the full dataset \(D\), and we can easily show that each of \(\textbf{C}_{z^{\text{\sc val}}}(D_{-z_{i}})\) can be computed from that in \(O(1)\) runtime (see Appendix C.2.2 for details). Hence, we can compute TKNN-Shapley \(\phi_{z_{i}}^{\text{\sc trank}}(v_{z^{\text{\sc trank}}}^{\text{\sc trank}})\) for _all_\(z_{i}\in D\) within an overall computational cost of \(O(N)\).

**Comparison with KNN-Shapley.** TKNN-Shapley offers several advantages over the original KNN-Shapley. **(1) Non-recursive:** In contrast to the KNN-Shapley formula (Theorem 2), which is recursive, TKNN-Shapley has an explicit formula for computing the Shapley value of every point \(z_{i}\). This non-recursive nature not only simplifies the implementation, but also makes it straightforward to incorporate techniques like subsampling. **(2) Computational efficiency:** TKNN-Shapley has \(O(N)\) runtime in total, which is better than the \(O(N\log N)\) runtime for KNN-Shapley.

## 5 Differentially Private TKNN-Shapley

Having established TKNN-Shapley as an alternative to KNN-Shapley which shares many advantages, our next step is to develop a new version of TKNN-Shapley that incorporates differential privacy. In this section, we introduce our proposed Differentially Private TKNN-Shapley (DP-TKNN-Shapley).

Differentially private release of \(\phi_{z_{i}}^{\text{\sc trank}}(v_{z^{\text{\sc val}}}^{\text{\sc trank}})\)

As \(\textbf{C}_{z^{\text{\sc val}}}=(\textbf{c},\textbf{c}_{x^{\text{\sc val}}, \tau},\textbf{c}_{z^{\text{\sc val}},\tau}^{(+)})\) are the only quantities that depend on the dataset \(D_{-z_{i}}\) in (4), privatizing these quantities is sufficient for overall privacy preservation, as DP guarantee is preserved under any post-processing [16]. Since all of \(\textbf{c},\textbf{c}_{x^{\text{\sc val}},\tau}\) and \(\textbf{c}_{z^{\text{\sc val}},\tau}^{(+)}\) are just counting queries, the function of \(\textbf{C}_{z^{\text{\sc val}}}(\cdot)\) has global sensitivity \(\sqrt{3}\) in \(\ell_{2}\)-norm. This allows us to apply the commonly used Gaussian mechanism (Theorem 4) to release \(\textbf{C}_{z^{\text{\sc val}}}\) in a differentially private way, i.e., \(\widehat{\textbf{C}}_{z^{\text{\sc val}}}(D_{-z_{i}}):=\texttt{round}\left( \textbf{C}_{z^{\text{\sc val}}}(D_{-z_{i}})+\mathcal{N}\left(0,\sigma^{2} \mathbbm{1}_{3}\right)\right)\) where \(\widehat{\textbf{C}}_{z^{\text{\sc val}}}\) is the privatized version of \(\textbf{C}_{z^{\text{\sc val}}}\), and the function round rounds each entry of noisy value to the nearest integer. The differentially private version of TKNN-Shapley value \(\widehat{\phi}_{z_{i}}^{\text{\sc trank}}(D_{-z_{i}})\) is simply the value score computed by(4) but use the privatized quantities \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}\). The privacy guarantee of releasing DP-TKNN-Shapley \(\widehat{\phi}_{z_{i}}^{\texttt{TKNN}}(D_{-z_{i}})\) follows from the guarantee of Gaussian mechanism (see Appendix D.1 for proof).

**Theorem 6**.: _For any \(z^{\text{(val)}}\in D^{\text{(val)}}\), releasing \(\widehat{\phi}_{z_{i}}^{\texttt{TKNN}}(v_{z^{\text{(val)}}}^{\texttt{TKNN}}):= \phi_{z_{i}}^{\texttt{TKNN}}\left[\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D_{- z_{i}})\right]\) to data owner \(i\) is \((\varepsilon,\delta)\)-DP with \(\sigma=\sqrt{3}\cdot\sqrt{\log(1.25/\delta)}/\varepsilon\)._

While our approach may seem simple, we stress that simplicity is appreciated in DP as complex mechanisms pose challenges for correct implementation and auditing [47; 21].

### Advantages of DP-TKNN-Shapley (Overview)

We give an overview (detailed in Appendix D.2) of several advantages of DP-KNN-Shapley here, including the efficiency, collusion resistance, and simplicity in incorporating subsampling.

**By reusing privatized statistics, \(\widehat{\phi}_{z_{i}}^{\texttt{TKNN}}\) can be efficiently computed for all \(z_{i}\in D\).** Recall that in practical data valuation scenarios, it is often desirable to compute the data value scores for _all of \(z_{i}\in D\)_. As we detailed in Section 4.2, for TKNN-Shapley, such a computation only requires \(O(N)\) runtime in total if we first compute \(\mathbf{C}_{z^{\text{(val)}}}(D)\) and subsequently \(\mathbf{C}_{z^{\text{(val)}}}(D_{-z_{i}})\) for each \(z_{i}\in D\). In a similar vein, we can efficiently compute the DP variant \(\widehat{\phi}_{z_{i}}^{\texttt{TKNN}}\) for _all_\(z_{i}\in D\). To do so, we first calculate \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D)\) and then, for each \(z_{i}\in D\), we compute \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D_{-z_{i}})\). It is important to note that when releasing \(\widehat{\phi}_{z_{i}}^{\texttt{TKNN}}\) to individual \(i\), the data point they hold, \(z_{i}\), is not private to the individual themselves. Therefore, as long as \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D)\) is privatized, \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D_{-z_{i}})\) and hence \(\widehat{\phi}_{z_{i}}^{\texttt{TKNN}}\) also satisfy the same privacy guarantee due to DP's post-processing property.

**By reusing privatized statistics, DP-TKNN-Shapley is collusion resistance.** In Section 5.1, we consider the single Shapley value \(\phi_{z_{i}}^{\texttt{TKNN}}(D_{-z_{i}})\) as the function to privatize. However, when we consider the release of all Shapley values \((\widehat{\phi}_{z_{i}}^{\texttt{TKNN}},\ldots,\widehat{\phi}_{z_{N}}^{\texttt {TKNN}})\) as a unified mechanism, one can show that such a mechanism satisfies _joint differential privacy_ (JDP) [34] if we reuse the privatized statistic \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D)\) for the release of all \(\widehat{\phi}_{z_{i}}^{\texttt{TKNN}}\). The consequence of satisfying JDP is that our mechanism is resilient against collusion among groups of individuals without any privacy degradation. That is, even if an arbitrary group of individuals in \([N]\setminus i\) colludes (i.e., shares their respective \(\widehat{\phi}_{z_{i}}^{\texttt{TKNN}}\) values within the group), the privacy of individual \(i\) remains uncompromised. Our method also stands resilient in scenarios where a powerful adversary sends multiple data points and receives multiple value scores.

**Incorporating Privacy Amplification by Subsampling.** In contrast to KNN-Shapley, the non-recursive nature of DP-TKNN-Shapley allows for the straightforward incorporation of subsampling. Besides the privacy guarantee, subsampling also significantly boosts the computational efficiency of DP-TKNN-Shapley.

Differentially private release of \(\phi_{z_{i}}^{\texttt{TKNN}}(v_{D^{\text{(val)}}}^{\texttt{TKNN}})\)

Recall that the TKNN-Shapley corresponding to the full validation set \(D^{\text{(val)}}\) is \(\phi_{z_{i}}^{\texttt{TKNN}}\left(v_{D^{\text{(val)}}}^{\texttt{TKNN}}\right)= \sum_{z^{\text{(val)}}\in D^{\text{(val)}}\cup\sigma_{z_{i}}^{\texttt{TKNN}} }(D_{-z_{i}};z^{\text{(val)}})\). We can compute privatized \(\phi_{z_{i}}^{\texttt{TKNN}}\left(v_{D^{\text{(val)}}}^{\texttt{TKNN}}\right)\) by simply releasing \(\phi_{z_{i}}^{\texttt{TKNN}}(D_{-z_{i}};z^{\text{(val)}})\) for all \(z^{\text{(val)}}\in D^{\text{(val)}}\). To better keep track of the privacy cost, we use the current state-of-the-art privacy accounting technique based on the notion of the Privacy Loss Random Variable (PRV) [17]. We provide more details about the background of privacy accounting in Appendix D.3. We note that PRV-based privacy accountant computes the privacy loss numerically, and hence the final privacy loss has no closed-form expressions.

## 6 Numerical Experiments

In this section, we systematically evaluate the practical effectiveness of our proposed TKNN-Shapley method. Our evaluation aims to demonstrate the following points: **(1)** TKNN-Shapley offers _improved runtime efficiency_ compared with KNN-Shapley. **(2)** The differentially private version of TKNN-Shapley (DP-TKNN-Shapley) achieves _significantly better privacy-utility tradeoff compared to naively privatized KNN-Shapley in discerning data quality. **(3)** Non-private TKNN-Shapley maintains a _comparable performance_ to the original KNN-Shapley. These observations highlight TKNN-Shapley's potential for data valuation in real-life applications. Detailed settings for our experiments are provided in Appendix E.

**Remark 4**.: _Given that differential privacy offers a provable guarantee against any potential adversaries, our experiments prioritize evaluating the utility of DP-TKNN-Shapley rather than its privacy properties. However, for readers interested in understanding the efficacy of DP in safeguarding training data, we have included an evaluation of the proposed MI attack on DP-TKNN-Shapley in Appendix B.2.2, where it shows a significant drop in attack performance compared to non-DP version._

### Computational Efficiency

We evaluate the runtime efficiency of TKNN-Shapley in comparison to KNN-Shapley (see Appendix E.2 for details as well as more experiments). We choose a range of training data sizes \(N\), and compare the runtime of both methods at each \(N\). As demonstrated in Figure 2, TKNN-Shapley achieves better computational efficiency than KNN-Shapley across all training data sizes. In particular, TKNN-Shapley is around 30% faster than KNN-Shapley for large \(N\). This shows the computational advantage of TKNN-Shapley mentioned in Section 4.2.

### Discerning Data Quality

In this section, we evaluate the performance of both DP and non-DP version of TKNN-Shapley in discerning data quality. **Tasks:** We evaluate the performance of TKNN-Shapley on two standard tasks: mislabeled data detection and noisy data detection, which are tasks that are often used for evaluating the performance of data valuation techniques in prior works [39, 60]. Since mislabeled/noisy data often negatively affect the model performance, it is desirable to assign low values to these data points. In the experiment of mislabeled (or noisy) data detection, we randomly choose 10% of the data points and flip their labels (or add strong noise to their features). **Datasets:** We conduct our experiments on a diverse set of 13 datasets, where 11 of them have been used in previous data valuation studies [39, 60]. Additionally, we experiment on 2 NLP datasets (AG News [65] and DBPedia [2]) that have been rarely used in the past due to their high-dimensional nature and the significant computational resources required. **Settings & Hyperparameters of TKNN-/KNN-Shapley:** for both TKNN/KNN-Shapley, we use the popular cosine distance as the distance measure [53], which is always bounded in \([-1,+1]\). Throughout all experiments, we use \(\tau=-0.5\) and \(K=5\) for TKNN-/KNN-Shapley, respectively, as we found the two choices consistently work well across all datasets. We conduct ablation studies on the choice of hyperparameters in Appendix E.

#### 6.2.1 Experiment for Private Setting

**Baselines: (1) DP-KNN-Shapley without subsampling.** Recall from Section 3.1, the original KNN-Shapley has a large global sensitivity. Nevertheless, we can still use Gaussian mechanism to privatize it based on its global sensitivity bound. We call this approach as DP-KNN-Shapley. **(2) DP-KNN-Shapley with subsampling.** Recall from Section 3.1, it is computationally expensive to incorporate subsampling techniques for DP-KNN-Shapley (detailed in Appendix B.3.2). For instance, subsampled DP-KNN-Shapley with subsampling rate \(q=0.01\) generally takes \(\mathbf{30\times longer}\)**time** compared with non-subsampled counterpart. Nevertheless, we still compare with subsampled DP-KNN-Shapley for completeness. These two baselines are detailed in Appendix B.4. We also note that an unpublished manuscript [67] proposed a DP version of Data Shapley. However, their DP guarantee requires a hard-to-verify assumption of uniform stability (see Appendix A). Hence, we do not compare with [67].

**Results:** We evaluate the privacy-utility tradeoff of DP-TKNN-Shapley. Specifically, for a fixed \(\delta\), we examine the AUROC of mislabeled/noisy data detection tasks at different values of \(\varepsilon\), where \(\varepsilon\) is adjusted by changing the magnitude of Gaussian noise. In the experiment, we set the subsampling rate \(q=0.01\) for TKNN-Shapley and subsampled KNN-Shapley. Table 2 shows the results on 3 datasets, and we defer the results for the rest of 10 datasets to Appendix E.3.2. As we can see, **DP-TKNN-Shapley shows a substantially better privacy-utility tradeoff compared to both DP-KNN-Shapley with/without subsampling.** In particular, DP-TKNN-Shapley maintains a high

Figure 2: Runtime comparison of TKNN-Shapley and KNN-Shapley across various training data sizes \(N\). The plot shows the average runtime based on 5 independent runs. Experiments were conducted with an AMD 64-Core CPU Processor.

[MISSING_PAGE_FAIL:10]

## Acknowledgments

This work was supported in part by the National Science Foundation under grants CNS-2131938, CNS-1553437, CNS-1704105, CNS-2048091, IIS-2312794, IIS-2313130, OAC-2239622, the ARL's Army Artificial Intelligence Innovation Institute (A2I2), the Office of Naval Research Young Investigator Award, the Army Research Office Young Investigator Prize, Schmidt DataX award, Princeton E-ffiliates Award, Amazon-Virginia Tech Initiative in Efficient and Robust Machine Learning, the Commonwealth Cyber Initiative, a Google PhD Fellowship, and a Princeton's Gordon Y. S. Wu Fellowship. We are grateful to anonymous reviewers at NeurIPS for their valuable feedback.

## References

* [1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _Proceedings of the 2016 ACM SIGSAC conference on computer and communications security_, pages 308-318, 2016.
* [2] Soren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. Dbpedia: A nucleus for a web of open data. In _The Semantic Web: 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007+ ASWC 2007, Busan, Korea, November 11-15, 2007. Proceedings_, pages 722-735. Springer, 2007.
* [3] Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate impact on model accuracy. _Advances in neural information processing systems_, 32, 2019.
* [4] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight analyses via couplings and divergences. _Advances in Neural Information Processing Systems_, 31, 2018.
* [5] Jon L Bentley. Survey of techniques for fixed radius near neighbor searching. Technical report, Stanford Linear Accelerator Center, Calif.(USA), 1975.
* [6] Yatao Bian, Yu Rong, Tingyang Xu, Jiaxiang Wu, Andreas Krause, and Junzhou Huang. Energy-based learning for cooperative games, with applications to valuation problems in machine learning. _arXiv preprint arXiv:2106.02938_, 2021.
* [7] Mark Alexander Burgess and Archie C Chapman. Approximating the shapley value using stratified empirical bernstein sampling. In _IJCAI_, pages 73-81, 2021.
* [8] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In _2022 IEEE Symposium on Security and Privacy (SP)_, pages 1897-1914. IEEE, 2022.
* [9] Christie Courtnage and Evgueni Smirnov. Shapley-value data valuation for semi-supervised learning. In _Discovery Science: 24th International Conference, DS 2021, Halifax, NS, Canada, October 11-13, 2021, Proceedings 24_, pages 94-108. Springer, 2021.
* [10] Rachel Cummings, Hadi Elzayn, Emmanouil Pountourakis, Vasilis Gkatzelis, and Juba Ziani. Optimal data acquisition with privacy-aware agents. In _2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)_, pages 210-224. IEEE, 2023.
* [11] Andrea Dal Pozzolo, Olivier Caelen, Reid A Johnson, and Gianluca Bontempi. Calibrating probability with undersampling for unbalanced classification. In _2015 IEEE Symposium Series on Computational Intelligence_, pages 159-166. IEEE, 2015.
* [12] Irit Dinur and Kobbi Nissim. Revealing information while preserving privacy. In _Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems_, pages 202-210, 2003.
* [13] Pradeep Dubey, Abraham Neyman, and Robert James Weber. Value theory without efficiency. _Mathematics of Operations Research_, 6(1):122-128, 1981.

* [14] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In _Advances in Cryptology-EUROCRYPT 2006: 24th Annual International Conference on the Theory and Applications of Cryptographic Techniques, St. Petersburg, Russia, May 28-June 1, 2006. Proceedings 25_, pages 486-503. Springer, 2006.
* [15] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Theory of cryptography conference_, pages 265-284. Springer, 2006.
* [16] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* [17] Cynthia Dwork and Guy N. Rothblum. Concentrated differential privacy. _CoRR_, abs/1603.01887, 2016.
* [18] Cynthia Dwork, Guy N Rothblum, and Salil Vadhan. Boosting and differential privacy. In _2010 IEEE 51st Annual Symposium on Foundations of Computer Science_, pages 51-60. IEEE, 2010.
* [19] Cynthia Dwork, Adam Smith, Thomas Steinke, and Jonathan Ullman. Exposed! a survey of attacks on private data. _Annual Review of Statistics and Its Application_, 4:61-84, 2017.
* [20] Alireza Fallah, Ali Makhdoumi, Azarakhsh Malekian, and Asuman Ozdaglar. Optimal and differentially private data acquisition: Central and local mechanisms. In _Proceedings of the 23rd ACM Conference on Economics and Computation_, pages 1141-1141, 2022.
* [21] Marco Gaboardi, Michael Hay, and Salil Vadhan. A programming framework for opendp. _Manuscript, May_, 2020.
* [22] Amirata Ghorbani, Michael Kim, and James Zou. A distributional framework for data valuation. In _International Conference on Machine Learning_, pages 3535-3544. PMLR, 2020.
* [23] Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. In _International Conference on Machine Learning_, pages 2242-2251. PMLR, 2019.
* [24] Amirata Ghorbani, James Zou, and Andre Esteva. Data shapley valuation for efficient batch active learning. In _2022 56th Asilomar Conference on Signals, Systems, and Computers_, pages 1456-1462. IEEE, 2022.
* [25] Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy. _Advances in Neural Information Processing Systems_, 34:11631-11642, 2021.
* [26] Laszlo Gyorfi, Michael Kohler, Adam Krzyzak, and Harro Walk. _A distribution-free theory of nonparametric regression_, volume 1. Springer, 2002.
* [27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [28] Ferenc Illes and Peter Kerenyi. Estimation of the shapley value by ergodic sampling. _arXiv preprint arXiv:1906.05224_, 2019.
* [29] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang, Costas J Spanos, and Dawn Song. Efficient task-specific data valuation for nearest neighbor algorithms. _Proceedings of the VLDB Endowment_, 2019.
* [30] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Gurel, Bo Li, Ce Zhang, Dawn Song, and Costas J Spanos. Towards efficient data valuation based on the shapley value. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1167-1176. PMLR, 2019.
* [31] Ruoxi Jia, Fan Wu, Xuehui Sun, Jiacen Xu, David Dao, Bhavya Kailkhura, Ce Zhang, Bo Li, and Dawn Song. Scalability vs. utility: Do we have to sacrifice one for the other in data importance quantification? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8239-8247, 2021.

* [32] Justin Kang, Ramtin Pedarsani, and Kannan Ramchandran. The fair value of data under heterogeneous privacy constraints. _arXiv preprint arXiv:2301.13336_, 2023.
* [33] Bojan Karlas, David Dao, Matteo Interlandi, Bo Li, Sebastian Schelter, Wentao Wu, and Ce Zhang. Data debugging with shapley importance over end-to-end machine learning pipelines. _arXiv preprint arXiv:2204.11131_, 2022.
* [34] Michael Kearns, Mallesh Pai, Aaron Roth, and Jonathan Ullman. Mechanism design in large games: Incentives and privacy. In _Proceedings of the 5th conference on Innovations in theoretical computer science_, pages 403-410, 2014.
* [35] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In _International Conference on Machine Learning_, pages 1885-1894. PMLR, 2017.
* [36] Antti Koskela and Antti Honkela. Computing differential privacy guarantees for heterogeneous compositions using fft. _CoRR_, abs/2102.12412, 2021.
* [37] Antti Koskela, Joonas Jalko, and Antti Honkela. Computing tight differential privacy guarantees using fft. In _International Conference on Artificial Intelligence and Statistics_, pages 2560-2569. PMLR, 2020.
* [38] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [39] Yongchan Kwon and James Zou. Beta shapley: a unified and noise-reduced data valuation framework for machine learning. In _International Conference on Artificial Intelligence and Statistics_, pages 8780-8802. PMLR, 2022.
* [40] Yongchan Kwon and James Zou. Data-oob: Out-of-bag estimate as a simple and efficient data value. _ICML_, 2023.
* [41] Yann LeCun. The mnist database of handwritten digits. _http://yann. lecun. com/exdb/mnist/_, 1998.
* [42] Weixin Liang, Kai-Hui Liang, and Zhou Yu. Herald: An annotation efficient method to detect user disengagement in social conversations. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 3652-3665, 2021.
* [43] Weixin Liang, James Zou, and Zhou Yu. Beyond user self-reported likert scale ratings: A comparison model for automatic dialog evaluation. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 1363-1374, 2020.
* [44] Jinkun Lin, Anqi Zhang, Mathias Lecuyer, Jinyang Li, Aurojit Panda, and Siddhartha Sen. Measuring the effect of training data on deep learning predictions via randomized experiments. In _International Conference on Machine Learning_, pages 13468-13504. PMLR, 2022.
* [45] Jinfei Liu, Jian Lou, Junxu Liu, Li Xiong, Jian Pei, and Jimeng Sun. Dealer: an end-to-end model marketplace with differential privacy. _Proceedings of the VLDB Endowment_, 14(6), 2021.
* [46] Xinjian Luo, Yangfan Jiang, and Xiaokui Xiao. Feature inference attack on shapley values. In _Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security_, pages 2233-2247, 2022.
* [47] Min Lyu, Dong Su, and Ninghui Li. Understanding the sparse vector technique for differential privacy. _Proceedings of the VLDB Endowment_, 10(6), 2017.
* [48] Rory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling permutations for shapley value estimation. 2022.
* [49] Arvind Narayanan and Vitaly Shmatikov. How to break anonymity of the netflix prize dataset. _arXiv preprint cs/0610105_, 2006.

* [50] Ramin Okhrati and Aldo Lipani. A multilinear sampling algorithm to estimate shapley values. In _2020 25th International Conference on Pattern Recognition (ICPR)_, pages 7992-7999. IEEE, 2021.
* [51] OpenAI. Planning for agi and beyond, 2023.
* [52] Konstantin D Pandl, Fabian Feiland, Scott Thiebes, and Ali Sunyaev. Trustworthy machine learning for health care: scalable data valuation with the shapley value. In _Proceedings of the Conference on Health, Inference, and Learning_, pages 47-57, 2021.
* [53] Sabita Rajbanshi. Chapter 1: K nearest neighbors (supervised machine learning algorithm), 2021.
* [54] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, 11 2019.
* [55] Pierangela Samarati and Latanya Sweeney. Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression. 1998.
* [56] Lloyd S Shapley. A value for n-person games. _Contributions to the Theory of Games_, 2(28):307-317, 1953.
* [57] Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, and Jongseong Jang. Online class-incremental continual learning with adversarial shapley value. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 9630-9638, 2021.
* [58] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In _2017 IEEE symposium on security and privacy (SP)_, pages 3-18. IEEE, 2017.
* [59] Zhihua Tian, Jian Liu, Jingyu Li, Xinle Cao, Ruoxi Jia, and Kui Ren. Private data valuation and fair payment in data marketplaces. _arXiv preprint arXiv:2210.08723_, 2022.
* [60] Jiachen T Wang and Ruoxi Jia. Data banzhaf: A robust data valuation framework for machine learning. In _International Conference on Artificial Intelligence and Statistics_, pages 6388-6421. PMLR, 2023.
* [61] Jiachen T Wang and Ruoxi Jia. A note on" efficient task-specific data valuation for nearest neighbor algorithms". _arXiv preprint arXiv:2304.04258_, 2023.
* [62] Jiachen T Wang and Ruoxi Jia. A note on" towards efficient data valuation based on the shapley value". _arXiv preprint arXiv:2302.11431_, 2023.
* [63] Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, and Dawn Song. A principled approach to data valuation for federated learning. In _Federated Learning_, pages 153-167. Springer, 2020.
* [64] Tianhao Wang, Yu Yang, and Ruoxi Jia. Improving cooperative game theory-based data valuation via data utility learning. _ICLR 2022 Workshop on Socially Responsible Machine Learning_, 2022.
* [65] Zhen Wang, Xu Shan, Xiangxie Zhang, and Jie Yang. N24news: A new dataset for multimodal news classification. _arXiv preprint arXiv:2108.13327_, 2021.
* [66] Mark Warner. Warner & hawley introduce bill to force social media companies to disclose how they are monetizing user data, 2019.
* [67] Lauren Watson, Rayna Andreeva, Hao-Tsung Yang, and Rik Sarkar. Differentially private shapley values for data evaluation. _arXiv preprint arXiv:2206.00511_, 2022.
* [68] Cynthia Weiss. Mayo clinic platform: A patient's experience and beyond.
* [69] Zhaoxuan Wu, Yao Shu, and Bryan Kian Hsiang Low. Davinz: Data valuation using deep neural networks at initialization. In _International Conference on Machine Learning_, pages 24150-24176. PMLR, 2022.

* [70] Andrew C Yao. Protocols for secure computations. In _23rd annual symposium on foundations of computer science (sfcs 1982)_, pages 160-164. IEEE, 1982.
* [71] I-Cheng Yeh and Che-hui Lien. The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. _Expert systems with applications_, 36(2):2473-2480, 2009.
* [72] Yuqing Zhu, Xuandong Zhao, Chuan Guo, and Yu-Xiang Wang. Private prediction strikes back! Private kernelized nearest neighbors with individual Renyi filter. In _Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence_, pages 2586-2596, 2023.

Extended Related Work

KNN-Shapley & Its Applications.The Shapley value is known for being computationally expensive. Fortunately, [29] found that computing the Data Shapley for K-Nearest Neighbors (KNN), one of the most classic yet still popular ML algorithms, is surprisingly easy and efficient. To the best of our knowledge, unweighted KNN is the _only_ commonly-used ML model for which the exact Data Shapley can be efficiently computed, referred to as 'KNN-Shapley'. Owing to its superior computational efficiency and effectiveness in distinguishing data quality, KNN-Shapley has become one of the most popular and practical data valuation techniques. In the realm of ML research, for instance, [24] extends KNN-Shapley to active learning, [57] employs it in a continual learning setting, [43, 42] utilize KNN-Shapley for removing confusing samples in NLP applications, and [9] adopts KNN-Shapley for data valuation in semi-supervised learning. Furthermore, KNN-Shapley has also proven to be highly practical in real-life applications: [52] demonstrates that KNN-Shapley is the _only_ practical data valuation technique for valuing large amounts of healthcare data, and [33] builds the first data debugging system for end-to-end ML pipelines based on KNN-Shapley.

**Remark 5** (KNN-Shapley vs General Data Shapley).: _In comparison to the work of general Data Shapley [23] (including Beta Shapley [39] as well as Data Banzhaf [60]), KNN-Shapley may have the following differences: **(1)** KNN-Shapley focuses on KNN classifiers. As a result, the applicability of KNN-Shapley scores as a proxy of data points' value with respect to other ML models may not be straightforward. However, it is noteworthy that KNN is asymptotically Bayes optimal, implying that KNN-Shapley scores can be justified as a proxy for the data's value relative to the best possible model, i.e., the Bayes classifier, under certain asymptotic conditions. **(2)** For high-dimensional data, such as images, KNN-Shapley requires a public model to first map the original data into data embeddings, and evaluates the value of these embeddings rather than the original data. While this is indeed a constraint in certain scenario, it is important to recognize that utilizing a publicly available foundation model to convert original data into embeddings, followed by the fine-tuning of the model's last layer, has become a common practice. Therefore, in many situations, it might be more desirable to evaluate the value of data embeddings instead of the original data._

Privacy and Data Valuation.Few studies in the literature consider the privacy risks of data valuation. [59] explores a scenario where a trusted server does not exist, and different data holders collaboratively compute each other's Shapley values without actually examining the data holder's data points, utilizing Multi-party Computation (MPC) techniques. The privacy risks addressed in [59] are orthogonal to those in our paper, and we can combine both techniques for end-to-end privacy protection. Another orthogonal line of works [45, 20, 10, 32] studies the scenario of a central platform collecting private data from privacy-aware agents and offering a differentially private statistic computed from the submitted data as a service in return. Agents consider the privacy costs and benefits of obtaining the statistic when deciding whether to participate and reveal their data truthfully. [46] proposes a privacy attack on the Shapley value for feature attribution, while in this work we study the privacy risks when using the Shapley value for data valuation.

The unpublished manuscript by [67] is the work most closely related to ours. [67] explores how to make the Shapley values of a data point to be differentially private against the rest of the dataset. However, instead of focusing on KNN-Shapley, they study the privatization of the less practical, retrain-based Data Shapley [23, 30]. Furthermore, their algorithm is highly restrictive in that the differential privacy guarantee relies on the "uniform stability" assumption, which is not verifiable for modern learning algorithms such as neural networks. Moreover, while [67] argues the "uniform stability" assumption holds for Logistic regression on bounded data domain, it is unclear whether the uniform stability assumption still holds when the Logistic regression is trained by SGD (which may involve training stochasticity and early stopping). In addition, [67] does not release the implementation. Hence, in our experiment, we do not compare with it.

**Remark 6** (Brief background for the privacy risks in releasing aggregated statistics).: _Since 1998, researchers have observed that a lot of seemingly benevolent aggregate statistics of a dataset can be used to reveal sensitive information about individuals [55]. A classic example is Netflix Prize fiasco, where the researchers show that an anonymized dataset can leak many sensitive information about individuals [49]. Dinur and Nissim [12] proved that "revealing too many statistics too accurately leads to data privacy breach". A great amount of discussion and practical realization of these privacy attacks on aggregated statistics can be found in [19]. In 2020, the US Census Bureau used these privacy attacks to justify its use of differential privacy._KNN-Shapley score for an individual is one kind of aggregated statistic that depends on the rest of the dataset. Hence, KNN-Shapley score intrinsically reveals private information about the rest of the dataset (where we use membership inference attack as a concrete example in our paper). In addition, when users collude, their KNN-shapley values can be combined to make joint inferences about the rest of the dataset._Details about KNN-Shapley, its Privacy Risks & Challenges of Privatization

### Full version of KNN-Shapley

KNN-Shapley was originally proposed in [29] and was later refined in [61]. Specifically, [61] considers the KNN's utility function formula (2) we present in the main text. Here is the main result of [61]:

**Theorem 7** (KNN-Shapley [61]).: _Consider the utility function in (2). Given a validation data point \(z^{(\mathrm{val})}=(x^{(\mathrm{val})},y^{(\mathrm{val})})\) and a distance metric \(d(\cdot,\cdot)\), if we sort the training set \(D=\{z_{i}=(x_{i},y_{i})\}_{i=1}^{N}\) according to \(d(x_{i},x^{(\mathrm{val})})\) in ascending order, then the Shapley value of each data point \(\phi_{z_{i}}^{\textit{\tiny\text{EM}}}\) corresponding to utility function \(v_{z^{\textit{\tiny\text{EM}}}}^{\textit{\tiny\text{EM}}}\) can be computed recursively as follows:_

\[\phi_{z_{N}}^{\textit{\tiny\text{EM}}}=\frac{\mathbb{I}[N\geq 2]}{N}\left( \mathbb{I}[y_{N}=y^{(\mathrm{val})}]-\frac{\sum_{i=1}^{N-1}\mathbb{I}[y_{i}=y^ {(\mathrm{val})}]}{N-1}\right)\left(\sum_{j=1}^{\min(K,N)-1}\frac{1}{j+1} \right)+\frac{1}{N}\left(\mathbb{I}[y_{N}=y^{(\mathrm{val})}]-\frac{1}{C}\right)\] \[\phi_{z_{i}}^{\textit{\tiny\text{EM}}}=\phi_{z_{i+1}}^{\textit{ \tiny\text{EM}}}+\frac{\mathbb{I}[y_{i}=y^{(\mathrm{val})}]-\mathbb{I}[y_{i+1} =y^{(\mathrm{val})}]}{N-1}\left[\sum_{j=1}^{\min(K,N)}\frac{1}{j}+\frac{\mathbb{ I}[N\geq K]}{K}\left(\frac{\min(i,K)\cdot(N-1)}{i}-K\right)\right]\]

_where \(C\) denotes the number of classes for the classification task._

#### b.1.1 Older version of KNN-Shapley from [29]

Prior to the current formulation of KNN-Shapley introduced by [61, 29] proposed an older version of the algorithm. There is only a small distinction between the updated version by [61] and the older version from [29]. Specifically, the difference between the versions proposed by [61] and [29] lies in the utility function considered when computing the Shapley value. [61] use the utility function (2) as presented in the main text, while the utility function used in [29] is slightly less interpretable:

\[v_{z^{\textit{\tiny\text{EM}}}(\mathrm{val})}^{\textit{\tiny\text{EM}}\cdot \textit{\tiny\text{OLD}}}(S):=\frac{1}{K}\sum_{j=1}^{\min(K,|S|)}\mathbb{I}\left[ y_{\pi^{(S)}(j;x^{(\mathrm{val})})}=y^{(\mathrm{val})}\right] \tag{5}\]

That is, (2) in the maintext divides the number of correct predictions by \(\min(K,|S|)\), which can be interpreted as the likelihood of the soft-label KNN classifier predicting the correct label \(y^{(\mathrm{val})}\) for \(x^{(\mathrm{val})}\). On the other hand, the function above (5) divides the number of correct predictions by \(K\), which is less interpretable when \(|S|<K\).

Nevertheless, the main result in [29] shows the following:

**Theorem 8** (Older Version of KNN-Shapley from [29]2).: _Consider the utility function in (5). Given a validation data point \(z^{(\mathrm{val})}=(x^{(\mathrm{val})},y^{(\mathrm{val})})\) and a distance metric \(d(\cdot,\cdot)\), if we sort the training set \(D=\{z_{i}=(x_{i},y_{i})\}_{i=1}^{N}\) according to \(d(x_{i},x^{(\mathrm{val})})\) in ascending order, then the Shapley value of each data point \(\phi_{z_{i}}^{\textit{\tiny\text{EM}}\cdot\textit{\tiny\text{OLD}}}\) corresponding to utility function \(v_{z^{\textit{\tiny\text{EM}}}\cdot\textit{\tiny\text{OLD}}}^{\textit{\tiny \text{EM}}\cdot\textit{\tiny\text{OLD}}}\) can be computed recursively as follows:_

Footnote 2: We state a more generalized version which does not require \(N\geq K\).

\[\phi_{z_{N}}^{\textit{\tiny\text{EM}}\cdot\textit{\tiny\text{OLD}}} =\frac{\mathbb{I}[y_{N}=y_{\mathrm{val}}]}{\max(K,N)}\] \[\phi_{z_{i}}^{\textit{\tiny\text{EM}}\cdot\textit{\tiny\text{OLD} }} =\phi_{z_{i+1}}^{\textit{\tiny\text{EM}}\cdot\textit{\tiny\text{OLD}}}+ \frac{\mathbb{I}[y_{i}=y_{\mathrm{val}}]-\mathbb{I}[y_{i+1}=y_{\mathrm{val}}] }{K}\frac{\min(K,i)}{i}\]

As we can see, both versions of KNN-Shapley have recursive forms. Since the utility functions they consider are very similar to each other except for the normalization term for small subsets, the two versions of KNN-Shapley have very close value scores in practice and as shown in the experiments of [61], the two versions of KNN-Shapley perform very similarly.

**Remark 7** (**The use of older version of KNN-Shapley for DP-related experiments)**.: _In the experiments, we use the more advanced version of the KNN-Shapley from [61] except for the DP-related experiments. This is because the global sensitivity of KNN-Shapley is difficult to derive, and we can only derive the global sensitivity of the older version of KNN-Shapley from [29] (as we will show in Appendix B.3). Hence, we can only use the older version of KNN-Shapley from [29] as the baseline in DP-related experiments. It is important to note that for non-DP experiments, the performance of KNN-Shapley and its older variant is very close to each other, and **which version of the KNN-Shapley we use for non-DP experiments does not affect the final conclusion**._

### Settings & Additional Experiments for Privacy Risks for KNN-Shapley

#### b.2.1 Experiment for the changes of data value after eliminating nearby points

We first investigate the impact of removing a data point on the KNN-Shapley score of another data point that is close to the removed one. Specifically, we calculate the KNN-Shapley score for a chosen data point in the dataset, and then repeat the process after eliminating one of its nearby points from the dataset.

Settings.We use commonly used datasets in the past literature, and the details for data preprocessing can be found in Appendix E.1. We calculate the KNN-Shapley score for a randomly selected data point in the dataset, and then repeat the computation of the KNN-Shapley score after we eliminate its nearest neighbor from the dataset.

Results.The results for a variety of other datasets are shown in Figure 3. We can see a significant difference in the KNN-Shapley score of the investigated data point depending on whether the nearest data point has been removed. We remark that how the data value change (increases or decreases) when the nearest data point is excluded depends on the label of the nearest data point as well as the validation data being used.

For completeness, we also plot the same figures but for the older version of KNN-Shapley from [29], which shows similar results.

Figure 3: Curves of KNN-Shapley of a fixed data point as a function of \(K\) (the hyperparameter for KNN), with and without a particular nearby data point in the training set.

#### b.2.2 An Instantiation of Membership Inference Attack via Data Value Scores

The observation in Appendix B.2.1, demonstrates the potential for KNN-Shapley scores to leak membership information of other data points in the dataset. To further demonstrate such privacy risk, in this section we show an example of a privacy attack where an adversary could infer the presence/absence of certain data points in the dataset based on the variations in the KNN-Shapley scores, analogous to the classic membership inference attack on ML model [58]. The attack result further highlights the need for privacy protection when applying data valuation techniques.

**Remark 8** (Motivation of Membership Inference Attack).: _Membership inference attack, i.e., confirming one's membership in a database, could pose significant privacy risks. For example, if a medical database is known to contain data from patients with specific conditions, this could disclose members' health status. We choose MI attack as our main example of the privacy risk as it has long been recognized as a fundamental privacy challenge across various domains, and such attacks have been extended well into the field of machine learning._

**Remark 9**.: _We stress that our goal here is the proof-of-concept, where we demonstrate that the data value scores can indeed serve as another channel of privacy leakage, and can indeed lead to the design of membership inference attacks. We do not claim any optimality of the attack we construct here. Designing the best membership inference attack on data value scores can be the topic of a single paper on its own, and is an interesting future work._

Membership Inference Attack via KNN-Shapley Scores (Algorithm 1).Our membership inference attack technique leverages KNN-Shapley scores to detect the presence or absence of specific data points in a dataset. The design is analogous to the membership inference attack against ML models via the _likelihood ratio test_[8]. **Threat model:** The threat model we consider here is that the attacker can compute the value of any data point among any datasets (analogue to the setting of MIA against ML models where the attacker can train models on any datasets he/she constructs). Moreover,

Figure 4: Curves of KNN-Shapley (old version from [29]) of a fixed data point as a function of \(K\) (the hyperparameter for KNN), with and without a particular nearby data point in the training set.

the attacker can craft the data point it owns, send it to the server and obtain the data value score of its own data point.

The attack goes as follows: firstly, we create a shadow dataset \(D_{\text{shadow}}\) by randomly sampling from the data distribution \(\mathbb{D}\). \(D_{\text{shadow}}\) serves as the function of \(D_{-z}\) in the maintext. We repeat this sampling process \(T\) times, where \(T\) is a predefined number of iterations. For each iteration, we calculate the KNN-Shapley scores of a query example \(z=(x,y)\) both when it is included in the shadow dataset (IN) and when it is excluded (OUT). We thus collect sets of IN and OUT scores for the query example. Upon collecting all these scores, we calculate their respective mean and variance. Finally, we query the server about KNN-Shapley score of the query example \(z\) on the actual dataset \(D_{-z}\), which we refer to as the _target data value_ (i.e., the server takes a copy the target data point as the data it holds, and send to the central server). We perform a likelihood ratio test using the distributions of the IN and OUT scores. This test involves comparing the probability of the observed target data value given the Normal distribution of the IN scores with the probability of the same given the Normal distribution of the OUT scores.

**Intuition of the attack.** The intuition of the attack is as follows: if the target data point is in the dataset, then if the attacker makes a copy of the target data point and send it to the server, the server effectively has two exactly the same data point, which will result in a lower value of each data point. On the other hand, if the target data point is a non-member, then the data value queried by the attacker will be higher.

Experiment SettingsFor each dataset we experiment on, we select 200 data points (members) as the private dataset held by the central server. We pick another 200 data points which serve as non-members. Moreover, we leverage another 400 data points where we can subsample "shadow dataset" in Algorithm 1. We set the number of shadow datasets we sample as 32.

```
1:dataset \(D\), query example \(z:=(x,y)\), data distribution \(\mathbb{D}\).
2:\(\text{KNNSV}_{\text{in}}=\{\}\)
3:\(\text{KNNSV}_{\text{out}}=\{\}\)
4:for\(T\) times do
5:\(D_{\text{shadow}}\leftarrow^{\mathbb{S}}\mathbb{D}\) // Sample a shadow dataset
6:\(\text{KNNSV}_{\text{in}}\leftarrow\text{KNNSV}_{\text{in}}\cup\{\phi_{z}(D_{ \text{shadow}}\cup z)\}\) // collect IN data value
7:\(\text{KNNSV}_{\text{in}}\leftarrow\text{KNNSV}_{\text{in}}\cup\{\phi_{z}(D_{ \text{shadow}})\) // collect OUT data value
8:endfor
9:\(\mu_{\text{in}}\leftarrow\texttt{mean}(\text{KNNSV}_{\text{in}})\)
10:\(\mu_{\text{out}}\leftarrow\texttt{mean}(\text{KNNSV}_{\text{out}})\)
11:\(\sigma_{\text{in}}^{2}\leftarrow\texttt{var}(\text{KNNSV}_{\text{in}})\)
12:\(\sigma_{\text{out}}^{2}\leftarrow\texttt{var}(\text{KNNSV}_{\text{out}})\)
13:\(\phi_{\text{obs}}\leftarrow\phi_{z}(D)\) // query target data value
14:Return\(\Lambda=\dfrac{p(\phi_{\text{obs}}\ \mid\ \mathcal{N}(\mu_{\text{in}},\sigma_{\text{in}}^{2}))}{p(\phi_{\text{obs}}\ \mid\ \mathcal{N}(\mu_{\text{out}},\sigma_{\text{out}}^{2}))}\)
```

**Algorithm 1** - **Membership Inference Attack via KNN-Shapley Scores.** We collect data value scores of a copy of the target example on datasets with and without the target example, estimate mean and variance of the loss distributions, and compute a likelihood ratio test.

Results on KNN-Shapley and TKNN-Shapley.Table 1 in the main paper shows the AUROC score of the attack results on KNN-Shapley. For completeness, we additionally conducted the experiment of the proposed MIA against (non-private) TKNN-Shapley and the results are shown in Table 4. As we can see, our MIA attack can achieve a detection performance that is better than the random guess (0.5) for most of the settings. On some datasets, the attack performance can achieve \(>0.7\) AUROC. This demonstrates that privacy leakage in data value scores can indeed lead to non-trivial privacy attacks.

Results on DP-TKNN-Shapley.To directly demonstrate how DP-TKNN-Shapley can mitigate the privacy risk, we additionally conducted the experiment of evaluating the proposed MI attack on DP-TKNN-Shapley (see Table 5 and 6). Compared with the result on non-private TKNN-Shapley, we can see that the overall attack performance drops to around 0.5 (the performance of random guess).

The result shows that DP-TKNN-Shapley is indeed very effective against membership inference attacks.

We stress again that the point of this experiment is not claiming that the membership inference attack that we developed here is an optimal MI attack on data value scores; instead, this is a proof-of-concept for the claim that data value scores can leak private information about other data points in the dataset, and we instantiate a possible privacy attack that exploits this privacy risks. We believe it is interesting future work to improve the attack performance as well as explore other possible threat models.

### Challenges in making KNN-Shapley being differentially private

In this section, we give more details of the inherent difficulties in making the KNN-Shapley \(\phi_{z_{i}}^{\text{KNN}}(D_{-z_{i}})\) (i.e., Theorem 7) to be differentially private.

#### b.3.1 Large global sensitivity

We find it very challenging to tightly bound the global sensitivity of \(\phi_{z_{i}}^{\text{KNN}}(D_{-z_{i}})\). Moreover, we show that the global sensitivity of \(\phi_{z_{i}}^{\text{KNN}}(D_{-z_{i}})\) can significantly exceed the magnitude of \(\phi_{z_{i}}^{\text{KNN}}\) by showing a lower bound. Specifically, we prove that the global sensitivity bound is at least around \(O(1)\) by constructing a specific pair of neighboring datasets.

**Theorem 9** (Lower Bound for the global sensitivity of \(\phi_{z}^{\text{KNN}}(D_{-z})\)).: _For a data point \(z=(x,y)\) and validation data point \(z^{(\text{val})}=(x^{(\text{val})},y^{(\text{val})})\), denote the global sensitivity of \(\phi_{z}^{\text{KNN}}(D_{-z};z^{(\text{val})})\) as

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \(\tau=-0.9\) & \(\tau=-0.8\) & \(\tau=-0.7\) & \(\tau=-0.6\) & \(\tau=-0.5\) & \(\tau=-0.4\) & \(\tau=-0.3\) & \(\tau=-0.2\) & \(\tau=-0.1\) \\ \hline
**2DPlanes** & 0.50 & 0.523 & 0.513 & 0.534 & 0.518 & 0.492 & 0.484 & 0.445 & 0.556 \\
**Phoneme** & 0.507 & 0.529 & 0.524 & 0.472 & 0.476 & 0.488 & 0.543 & 0.512 & 0.47 \\
**CPU** & 0.492 & 0.487 & 0.504 & 0.485 & 0.473 & 0.489 & 0.453 & 0.49 & 0.508 \\
**Fraud** & 0.508 & 0.501 & 0.502 & 0.506 & 0.497 & 0.501 & 0.496 & 0.436 & 0.53 \\
**Creditcard** & 0.482 & 0.494 & 0.522 & 0.505 & 0.505 & 0.495 & 0.494 & 0.498 & 0.429 \\
**ApgAll** & 0.472 & 0.498 & 0.499 & 0.486 & 0.488 & 0.48 & 0.521 & 0.545 & 0.441 \\
**Click** & 0.502 & 0.529 & 0.501 & 0.508 & 0.518 & 0.496 & 0.545 & 0.47 & 0.456 \\
**Wind** & 0.533 & 0.539 & 0.478 & 0.504 & 0.493 & 0.498 & 0.529 & 0.499 & 0.419 \\
**Pol** & 0.498 & 0.513 & 0.509 & 0.51 & 0.482 & 0.492 & 0.489 & 0.52 & 0.528 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results of the AUROC of our MIA attack on DP-TKNN-Shapley \((\varepsilon=1.0)\).

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \(\tau=-0.9\) & \(\tau=-0.8\) & \(\tau=-0.7\) & \(\tau=-0.6\) & \(\tau=-0.5\) & \(\tau=-0.4\) & \(\tau=-0.3\) & \(\tau=-0.2\) & \(\tau=-0.1\) \\ \hline
**2DPlanes** & 0.549 & 0.799 & 0.738 & 0.688 & 0.53 & 0.665 & 0.6 & 0.558 & 0.628 \\
**Phoneme** & 0.777 & 0.679 & 0.736 & 0.673 & 0.704 & 0.692 & 0.588 & 0.522 & 0.5 \\
**CPU** & 0.757 & 0.672 & 0.638 & 0.635 & 0.512 & 0.518 & 0.58 & 0.545 & 0.508 \\
**Fraud** & 0.752 & 0.529 & 0.577 & 0.594 & 0.558 & 0.715 & 0.678 & 0.7 & 0.645 \\
**Cerdticard** & 0.55 & 0.501 & 0.664 & 0.685 & 0.59 & 0.56 & 0.597 & 0.552 & 0.52 \\
**ApgAll** & 0.506 & 0.529 & 0.608 & 0.574 & 0.558 & 0.507 & 0.569 & 0.571 & 0.526 \\
**Click** & 0.718 & 0.56 & 0.545 & 0.6 & 0.568 & 0.735 & 0.535 & 0.56 & 0.52 \\
**Wind** & 0.528 & 0.65 & 0.7 & 0.585 & 0.585 & 0.58 & 0.568 & 0.562 & 0.632 \\
**Pol** & 0.772 & 0.62 & 0.748 & 0.715 & 0.6 & 0.592 & 0.59 & 0.532 & 0.672 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results of the AUROC of our MI attack on TKNN-Shapley.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline  & \(\tau=-0.9\) & \(\tau=-0.8\) & \(\tau=-0.7\) & \(\tau=-0.6\) & \(\tau=-0.5\) & \(\tau=-0.4\) & \(\tau=-0.3\) & \(\tau=-0.2\) & \(\tau=-0.1\) \\ \hline
**2DPlanes** & 0.51 & 0.474 & 0.488 & 0.518 & 0.494 & 0.51 & 0.479 & 0.466 & 0.478 \\
**Phoneme** & 0.502 & 0.505 & 0.512 & 0.506 & 0.5 & 0.484 & 0.552 & 0.538 & 0.482 \\
**CPU** & 0.468 & 0.489 & 0.486 & 0.51 & 0.523 & 0.524 & 0.468 & 0.466 & 0.527 \\
**Fraud** & 0.479 & 0.474 & 0.492 & 0.513 & 0.48 & 0.484 & 0.485 & 0.484 & 0.578 \\
**Creditcard** & 0.518 & 0.495 & 0.511 & 0.522 & 0.501 & 0.483 & 0.448 & 0.521 & 0.535 \\
**ApgAll** & 0.484 & 0.476 & 0.471 & 0.492 & 0.486 & 0.5 & 0.493 & 0.434 & 0.454 \\
**Click** & 0.492 & 0.491 & 0.49 & 0.484 & 0.488 & 0.482 & 0.544 & 0.452 & 0.434 \\
**Wind** & 0.52 & 0.496 & 0.514 & 0.492 & 0.53 & 0.488 & 0.504 & 0.522 & 0.43 \\
**Pol** & 0.491 & 0.503 & 0.51 & 0.496 & 0.492 & 0.488 & 0.458 & 0.523 & 0.487 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results of the AUROC of our MIA attack on DP-TKNN-Shapley \((\varepsilon=0.5)\).

\(\Delta(\phi_{z}^{\text{KNN}};z^{\text{(val)}}):=\sup_{D_{-z}\sim D_{-z}^{\prime}} \left|\phi_{z}^{\text{KNN}}(D_{-z};z^{\text{(val)}})-\phi_{z}^{\text{KNN}}(D_{- z}^{\prime};z^{\text{(val)}})\right|\). We have_

\[\Delta(\phi_{z}^{\text{KNN}})\geq\frac{1}{2}\left(\mathbb{1}[y=y^{\text{(val) }}]-1/C\right)\]

_where \(C\) is the number of classes for the corresponding classification task._

Proof.: The proof idea is to construct two neighboring datasets: \(D=\{z_{2}\},D^{\prime}=\{z_{1},z_{2}\}\) where \(z_{1}=(x_{1},y_{1}),z_{2}=(x_{2},y_{2})\) are two data points and we let \(y_{1}=y^{\text{(val)}}\). Moreover, we let \(d(z_{1},z^{\text{(val)}})\leq d(z_{2},z^{\text{(val)}})\). From KNN-Shapley's formula in Theorem 7, we have

\[\phi_{z_{2}}^{\text{KNN}}(D)=\mathbb{1}[y_{2}=y^{\text{(val)}}]-1/C\]

and if \(K\geq 2\), we have

\[\phi_{z_{2}}^{\text{KNN}}(D^{\prime})=\frac{1}{4}\left(\mathbb{1}[y_{2}=y^{ \text{(val)}}]-\mathbb{1}[y_{1}=y^{\text{(val)}}]\right)+\frac{1}{2}\left( \mathbb{1}[y_{2}=y^{\text{(val)}}]-1/C\right)\]

, and if \(K=1\), we have

\[\phi_{z_{2}}^{\text{KNN}}(D^{\prime})=\frac{1}{2}\left(\mathbb{1}[y_{2}=y^{ \text{(val)}}]-1/C\right)\]

Since \(\mathbb{1}[y_{1}=y^{\text{(val)}}]=1\) (our condition), we have

\[\left|\phi_{z_{2}}^{\text{KNN}}(D)-\phi_{z_{2}}^{\text{KNN}}(D^{\prime}) \right|\geq\frac{1}{2}\left(\mathbb{1}[y_{2}=y^{\text{(val)}}]-1/C\right)\]

The above theorem tells us that the global sensitivity for KNN-Shapley is at the order of \(O(1)\). On the other hand, we can see from the formula of KNN-Shapley in Theorem 7 that the magnitude \(\phi_{z}^{\text{KNN}}\) for many of the data points \(z\) is at the order of \(O(1/N)\). Hence, if we apply the Gaussian mechanism and add random noise proportional to the global sensitivity bound, the resulting privatized data value score could substantially deviate from its non-private counterpart, thereby compromising the utility of the privatized data value scores.

Tight Global Sensitivity for the older version of KNN-Shapley from [29].As we said earlier, it is hard to bound the global sensitivity for \(\phi_{z}^{\text{KNN}}\). Moreover, even if we are able to bound the global sensitivity, the bound will highly likely be large compared with the magnitude of \(\phi_{z}^{\text{KNN}}\), as we can see from Theorem 9.

In order to find a reasonable baseline for comparing with DP-TKNN-Shapley, we consider the older version of the KNN-Shapley developed in [29], where we show that its global sensitivity can be _tightly_ bounded (but still large).

**Theorem 10** (Global sensitivity of \(\phi_{z}^{\text{KNN-OLD}}(D_{-z})\) from [29]).: _For a data point \(z=(x,y)\) and validation data point \(z^{\text{(val)}}=(x^{\text{(val)}},y^{\text{(val)}})\), denote the global sensitivity of \(\phi_{z}^{\text{KNN-OLD}}(D_{-z};z^{\text{(val)}})\) as \(\Delta(\phi_{z}^{\text{KNN-OLD}};z^{\text{(val)}}):=\sup_{D_{-z}\sim D_{-z}^{ \prime}}\left|\phi_{z}^{\text{KNN-OLD}}(D_{-z};z^{\text{(val)}})-\phi_{z}^{ \text{KNN-OLD}}(D_{-z}^{\prime};z^{\text{(val)}})\right|\). We have_

\[\Delta(\phi_{z}^{\text{KNN-OLD}};z^{\text{(val)}})\leq\frac{1}{K(K+1)}\]

Proof.: For any dataset \(D=D_{-z}\cup\{z\}\), we sort the data points according to the distance to \(x^{\text{(val)}}\), and we denote \(z_{j}\) for the \(j\)th closest data point to \(z^{\text{(val)}}\). WLOG, suppose \(z_{i}:=z\). We first write out the non-recursive expression for the older KNN-Shapley \(\phi_{z_{i}}^{\text{KNN-OLD}}\):

\[\phi_{z_{i}}^{\text{KNN-OLD}}=\frac{\mathbb{1}[y_{N}=y_{\text{val}}]}{\max(K, N)}+\sum_{j=i}^{N-1}\frac{\mathbb{1}[y_{j}=y_{\text{val}}]-\mathbb{1}[y_{j+1}=y_ {\text{val}}]}{K}\frac{\min(K,j)}{j}\]

If \(N\leq K\), then we have \(\phi_{z_{i}}^{\text{KNN-OLD}}=\frac{1}{K}[y_{i}=y^{\text{(val)}}]\) for all \(i\) (i.e., no privacy leakage in this case).

If \(N>K\), there are two cases:

**Case 1:**\(i\geq K\), then we have

\[\phi_{z_{i}}^{\texttt{KNN-OLD}}=\frac{\mathbb{1}[y_{i}=y^{(\text{val})}]}{i}- \sum_{j=i+1}^{N}\frac{1}{(j-1)j}\mathbb{1}[y_{j}=y^{(\text{val})}]\]

Hence, if we add/remove a data point \(z_{j}=(x_{j},y_{j})\) s.t. \(d(x_{j},x^{(\text{val})})\geq d(x_{i},x^{(\text{val})})\), we have

\[\left|\phi_{z_{i}}^{\texttt{KNN-OLD}}(D_{-z_{i}})-\phi_{z_{i}}^{\texttt{KNN-OLD }}(D^{\prime}_{-z_{i}})\right|\leq\frac{1}{(j-1)j}\leq\frac{1}{K(K+1)}\]

If we add a data point \(z_{j}=(x_{j},y_{j})\) s.t. \(d(x_{j},x^{(\text{val})})<d(x_{i},x^{(\text{val})})\), we have

\[\left|\phi_{z_{i}}^{\texttt{KNN-OLD}}(D_{-z_{i}})-\phi_{z_{i}}^{ \texttt{KNN-OLD}}(D^{\prime}_{-z_{i}})\right|\] \[=\left|\left(\frac{\mathbb{1}\left[y_{i}=y^{(\text{val})}\right] }{i}-\sum_{j=i+1}^{N}\frac{1}{(j-1)j}\mathbb{1}[y_{j}=y^{(\text{val})}]\right) -\left(\frac{\mathbb{1}\left[y_{i}=y^{(\text{val})}\right]}{i+1}-\sum_{j=i+2}^ {N}\frac{1}{(j-1)j}\mathbb{1}[y_{j}=y^{(\text{val})}]\right)\right|\] \[=\left|\left(\frac{1}{i}-\frac{1}{i+1}\right)\mathbb{1}[y_{i}=y^ {(\text{val})}]-\sum_{j=i+1}^{N}\left(\frac{1}{(j-1)j}-\frac{1}{(j+1)j}\right) \mathbb{1}[y_{j}=y^{(\text{val})}]\right|\] \[\leq\frac{1}{i(i+1)}\] \[\leq\frac{1}{K(K+1)}\]

When \(i\geq K+1\), the sensitivity analysis for remove a data point \(z_{j}=(x_{j},y_{j})\) s.t. \(d(x_{j},x^{(\text{val})})<d(x_{i},x^{(\text{val})})\) is similar to the analysis above where we also have

\[\left|\phi_{z_{i}}^{\texttt{KNN-OLD}}(D_{-z_{i}})-\phi_{z_{i}}^{\texttt{KNN-OLD }}(D^{\prime}_{-z_{i}})\right|\leq\frac{1}{K(K+1)}\]

**Case 2:**\(i<K\), then we have

\[\phi_{z_{i}}^{\texttt{KNN-OLD}}=\frac{\mathbb{1}[y_{i}=y^{(\text{val})}]}{K}- \sum_{j=K+1}^{N}\frac{1}{(j-1)j}\mathbb{1}[y_{j}=y^{(\text{val})}]\]

By a similar analysis, we can also show that

\[\left|\phi_{z_{i}}^{\texttt{KNN-OLD}}(D_{-z_{i}})-\phi_{z_{i}}^{\texttt{KNN-OLD }}(D^{\prime}_{-z_{i}})\right|\leq\frac{1}{K(K+1)}\]

The only remaining case that we haven't discussed yet is when \(i=K\), and we remove a data point \(z_{j}=(x_{j},y_{j})\) s.t. \(d(x_{j},x^{(\text{val})})<d(x_{i},x^{(\text{val})})\). In this case, we have

\[\left|\phi_{z_{i}}^{\texttt{KNN-OLD}}(D_{-z_{i}})-\phi_{z_{i}}^{ \texttt{KNN-OLD}}(D^{\prime}_{-z_{i}})\right|\] \[=0\leq\frac{1}{K(K+1)}\]

We stress that the bound \(\frac{1}{K(K+1)}\) is tight. For example, for the case where \(i=K\), then if we add another data point \(z^{*}=(x^{*},y^{*})\) s.t.

\[d(x_{i},x^{(\text{val})})=d(x_{K},x^{(\text{val})})\leq d(x^{*},x^{(\text{val })})\leq d(x_{K+1},x^{(\text{val})})\]

the change of the value \(\phi_{z_{i}}^{\texttt{KNN-OLD}}\) will be \(\frac{1}{K(K+1)}\).

#### b.3.2 Difficulty (computational challenge) in incorporating subsampling technique

"Privacy amplification by subsampling" [4] is a technique where the subsampling of a dataset amplifies the privacy guarantees due to the reduced probability of an individual's data being included. Being able to incorporate such a technique is often important for achieving a decent privacy-utility tradeoff. However, the recursive nature of KNN-Shapley computation makes it hard to incorporate the subsampling techniques.

Specifically, recall that in practical data valuation scenarios, it is often desirable to compute the data value scores for _all of_\(z_{i}\in D\). To apply the subsampling technique, we first need to create a subsampled dataset and compute the KNN-Shapley for the target data point \(z_{i}\), i.e., we need to compute \(\phi_{z}^{\textsc{KNN}}(\mathtt{sample}(D_{-z}))\). The subsampled dataset is usually constructed by sampling each data point independently with a probability \(q\) (this is usually referred to as Poisson subsampling [4]). If we view \(q\) as a constant, then the computation of \(\phi_{z}^{\textsc{KNN}}(\mathtt{sample}(D_{-z}))\) requires a runtime of \(\widetilde{O}(N)\) for the computation of _each_\(\phi_{z_{i}}^{\textsc{KNN}}\). This results in a final runtime of \(\widetilde{O}(N^{2})\) for the computation of _all_\((\phi_{z_{i}}^{\textsc{KNN}})_{z_{i}\in D}\), which is a significant increase in computational demand compared to the non-private KNN-Shapley. The recursive nature of KNN-Shapley computation significantly complicates the attempt of improving the computational efficiency. That is, it is not clear how to reuse the subsampled dataset to compute the KNN-Shapley score for those that are not sampled. Therefore, it seems that an \(\widetilde{O}(N^{2})\) runtime is necessary if we want to incorporate the subsampling technique.

### Baseline for experiments in Section 6.2.1

DP-KNN-Shapley.Given such an upper bound for the global sensitivity of \(\phi_{z}^{\textsc{KNN-OLD}}\), we can use Gaussian mechanism (Theorem 4) to privatize \(\phi_{z}^{\textsc{KNN-OLD}}\), i.e., we compute \(\widehat{\phi}_{z}^{\textsc{KNN-OLD}}(D_{-z}):=\phi_{z}^{\textsc{KNN-OLD}}(D_{-z })+\mathcal{N}\left(0,\frac{1}{K(K+1)}\right)\). We note that such a bound is still not satisfactory as the magnitude \(\phi_{z}^{\textsc{KNN-OLD}}\) for many of the data points \(z\) is at the order of \(O(1/N)\). Nevertheless, this is a reasonable baseline (if it is not the only one) that we can use for comparison in DP-related experiments.

DP-KNN-Shapley with subsampling.Despite the high computational cost associated with the incorporation of the subsampling technique, we still compare our approach with this computationally intensive baseline for completeness. More specifically, we compute \(\widehat{\phi}_{z}^{\textsc{KNN-OLD}}(D_{-z}):=\phi_{z}^{\textsc{KNN-OLD}}( \mathtt{sample}(D_{-z}))+\mathcal{N}\left(0,\left(\frac{1}{K(K+1)}\right)^{2}\right)\).

Details for TKNN-Shapley

### Consistency Result for TKNN

For a data point \(x\) and a threshold \(\tau\), we denote

\[S_{x,\tau}:=\{x^{\prime}|d(x,x^{\prime})\leq\tau\}\]

the ball of radius \(\tau\) centered at \(x\). Recall that the prediction rule of TKNN when given a training set \(D\) is

\[m_{D}(x;\tau):=\begin{cases}0&|\texttt{NB}_{x,\tau}(D)|=0\\ \frac{1}{|\texttt{NB}_{x,\tau}(D)|}\sum_{x^{\prime}\in\texttt{NB}_{x,\tau}(D) }m(x^{\prime})&|\texttt{NB}_{x,\tau}(D)|>0\end{cases}\]

**Theorem 11**.: _Suppose \(m\) is the target function that is Lipschitz on \(\texttt{supp}(\mu)\) where \(\mu\) is the probability measure of data distribution. As \(n\to\infty\), if \(\tau_{n}\to 0\) and \(n\mu(S_{x,\tau})\to\infty\) for all \(x\in\texttt{supp}(\mu)\), then_

\[\lim_{n\to\infty}\mathbb{E}_{x\sim\mu,D_{n}\sim\mu^{n}}\left[(m_{D_{n}}(x)-m(x) )^{2}\right]=0\]

Proof.: \[\mathbb{E}_{x,D_{n}}[(m_{D_{n}}(x)-m(x))^{2}]\] \[=\mathbb{E}_{x,D_{n}}[(m_{D_{n}}(x)-m(x))^{2}|\texttt{NB}_{x,\tau _{n}}(D_{n})|>0]\Pr_{x,D_{n}}[\texttt{NB}_{x,\tau_{n}}(D_{n})|>0]\] \[\quad+\mathbb{E}_{x,D_{n}}[(0-m(x))^{2}|\texttt{NB}_{x,\tau_{n}}( D_{n})|=0]\Pr_{x,D_{n}}[\texttt{NB}_{x,\tau_{n}}(D_{n})|=0]\]

\[\lim_{n\to\infty}\Pr_{x,D_{n}}[|\texttt{NB}_{x,\tau_{n}}(D_{n})|=0] =\lim_{n\to\infty}\mathbb{E}_{x}\left[\Pr_{D_{n}}\left[|\texttt{NB }_{x,\tau_{n}}(D_{n})|=0\right]\right]\] \[=\lim_{n\to\infty}\mathbb{E}_{x}\left[(1-\mu(S_{x,\tau_{n}}))^{n}\right]\] \[\leq\lim_{n\to\infty}\mathbb{E}_{x}\left[\frac{1}{1+n\mu(S_{x, \tau_{n}})}\right]\] \[=0\]

Suppose \(m\) has Lipschitz constant \(L\), i.e., for any pair of data points \(x,x^{\prime}\in\texttt{supp}(\mu)\), we have

\[|m(x)-m(x^{\prime})|\leq L\left\|x-x^{\prime}\right\|\]

\[\mathbb{E}_{x,D_{n}}\left[(m_{D_{n}}(x)-m(x))^{2}||\texttt{NB}_{x,\tau_{n}}( D_{n})|>0\right]\] \[=\mathbb{E}_{x,D_{n}}\left[\left(\frac{1}{|\texttt{NB}_{x,\tau_{ n}}(D_{n})|}\sum_{x^{\prime}\in\texttt{NB}_{x,\tau_{n}}(D_{n})}(m(x^{\prime})-m(x ))\right)^{2}||\texttt{NB}_{x,\tau_{n}}(D_{n})|>0\right]\] \[\leq\mathbb{E}_{x,D_{n}}\left[\frac{1}{|\texttt{NB}_{x,\tau_{n}}( D_{n})|}\sum_{x^{\prime}\in\texttt{NB}_{x,\tau_{n}}(D_{n})}(m(x^{\prime})-m(x ))^{2}||\texttt{NB}_{x,\tau_{n}}(D_{n})|>0\right]\] \[\leq\mathbb{E}_{x,D_{n}}\left[\frac{1}{|\texttt{NB}_{x,\tau_{n}} (D_{n})|}\sum_{x^{\prime}\in\texttt{NB}_{x,\tau_{n}}(D_{n})}L^{2}\left\|x-x^{ \prime}\right\|^{2}||\texttt{NB}_{x,\tau_{n}}(D_{n})|>0\right]\] \[\leq\mathbb{E}_{x,D_{n}}\left[\frac{1}{|\texttt{NB}_{x,\tau_{n}} (D_{n})|}\sum_{x^{\prime}\in\texttt{NB}_{x,\tau_{n}}(D_{n})}L^{2}\tau_{n}^{2}|| \texttt{NB}_{x,\tau_{n}}(D_{n})|>0\right]\] \[=L^{2}\tau_{n}^{2}\]

which \(\to 0\) as \(n\to\infty\).

### Proofs for TKNN-Shapley

Given a dataset \(S\) and a test data point \((x^{(\mathrm{val})},y^{(\mathrm{val})})\), let \(\mathtt{NB}_{x^{(\mathrm{val})},\tau}(S):=\{(x,y)|(x,y)\in S,d(x,x^{(\mathrm{val })})\leq\tau\}\) the nearest neighbor of \(x^{(\mathrm{val})}\) that is within a pre-specified threshold \(\tau\). Recall that in this case, the utility function of soft-label TKNN classifier becomes

\[v(S;(x^{(\mathrm{val})},y^{(\mathrm{val})})):=\begin{cases}1/C&|\mathtt{NB}_{x ^{(\mathrm{val})},\tau}(S)|=0\\ \frac{1}{|\mathtt{NB}_{x^{(\mathrm{val})},\tau}(S)|}\sum_{(x,y)\in\mathtt{NB}_ {x^{(\mathrm{val})},\tau}(S)}\mathbb{1}[y=y^{(\mathrm{val})}]&|\mathtt{NB}_{x ^{(\mathrm{val})},\tau}(S)|>0\end{cases}\]

where \(C\) is the number of classes for the classification task, and \(1/C\) is the random guess accuracy.

**Semivalue.** The class of data values that satisfy all the Shapley axioms except efficiency is called _semivalues_. It was originally studied in the field of economics and recently proposed to tackle the data valuation problem [39]. Unlike the Shapley value, semivalues are _not_ unique. The following theorem by the seminal work of [13] shows that every semivalue of a player \(z\) (in our case the player is a data point) can be expressed as the weighted average of marginal contributions \(v(S\cup\{z\})-v(S)\) across different subsets \(S\subseteq N\setminus\{z\}\).

**Theorem 12** (Representation of Semivalue [13]).: _A value function \(\phi_{\mathrm{semi}}\) is a semivalue, if and only if, there exists a weight function \(w:[N]\rightarrow\mathbb{R}\) such that \(\sum_{k=1}^{N}\binom{N-1}{k-1}w(k)=N\) and the value function \(\phi\) can be expressed as follows:_

\[\phi_{z_{i}}\left(D_{-z_{i}};v,w\right):=\frac{1}{N}\sum_{k=1}^{N}w(k)\sum_{ \begin{subarray}{c}S\subseteq D_{-z_{i}},\\ |S|=k-1\end{subarray}}[v(S\cup\{z\})-v(S)] \tag{6}\]

Semivalues subsume both the Shapley value and the Banzhaf value with \(w_{\mathrm{shap}}(k)=\binom{N-1}{k-1}^{-1}\) and \(w_{\mathrm{bankz}}(k)=\frac{N}{2N-1}\), respectively. In the following, we first discuss the general semivalue for soft-label TKNN classifier with a weight function \(w\). We then plug in the specific weight function for the Shapley value.

Our goal is to derive \(\phi_{z_{i}}\left(D_{-z_{i}};v,w\right)\), the semivalue of \(z_{i}=(x_{i},y_{i})\) with a weight function \(w\) when using soft-label TKNN classifier, i.e.,

\[\phi_{z_{i}}\left(D_{-z_{i}};v,w\right):=\frac{1}{N}\sum_{k=1}^{N}w(k)\sum_{ \begin{subarray}{c}S\subseteq D_{-z_{i}},\\ |S|=k-1\end{subarray}}[v(S\cup\{z_{i}\})-v(S)]\]

Consider a validation data point \((x^{(\mathrm{val})},y^{(\mathrm{val})})\), and recall that \(v(S):=v(S;(x^{(\mathrm{val})},y^{(\mathrm{val})}))\). Denote

\[\mathbf{c}_{x^{(\mathrm{val})},\tau} =\mathbf{c}_{x^{(\mathrm{val})},\tau}(D_{-z_{i}}):=1+\left|\mathtt{ NB}_{x^{(\mathrm{val})},\tau}(D_{-z_{i}})\right| (1+\text{\# neighbors of $x^{(\mathrm{val})}$ in $D_{-z_{i}}$})\] \[\mathbf{c}_{z^{(\mathrm{val})},\tau}^{(+)} =\mathbf{c}_{z^{(\mathrm{val})},\tau}^{(+)}(D_{-z_{i}}):=\sum_{(x,y)\in\mathtt{NB}_{x^{(\mathrm{val})},\tau}(D_{-z_{i}})}\mathbb{1}[y=y^{( \mathrm{val})}] (\text{\# same-label neighbors in $D_{-z_{i}}$})\]

**Case 1:** if \(\left\|x_{i}-x^{(\mathrm{val})}\right\|>\tau\), we have \(v(S\cup z_{i})-v(S)=0\) for any \(S\), hence \(\phi_{z_{i}}\left(D_{-z_{i}};v,w\right)=0\).

**Case 2:** if \(\left\|x_{i}-x^{(\mathrm{val})}\right\|\leq\tau\), we have the following:

\[v(S\cup z_{i})-v(S)=\] \[\begin{cases}\frac{1[y_{i}=y^{(\mathrm{val})}]}{1+|\mathtt{NB}_{x ^{(\mathrm{val})},\tau}(S)|}+\left(\frac{1}{1+|\mathtt{NB}_{x^{(\mathrm{val})},\tau}(S)|}-\frac{1}{|\mathtt{NB}_{x^{(\mathrm{val})},\tau}(S)|}\right)\sum_{ (x,y)\in\mathtt{NB}_{x^{(\mathrm{val})},\tau}(S)}\mathbb{1}[y=y_{\mathrm{val }}]&|\mathtt{NB}_{x^{(\mathrm{val})},\tau}(S)|>0\\ 1[y_{i}=y^{(\mathrm{val})}]-1/C&|\mathtt{NB}_{x^{(\mathrm{val})},\tau}(S)|=0 \end{cases}\]This means that the marginal contribution on \(S\) only depends on the subset \(\mathtt{NB}_{x^{(\text{val})},\tau}(S)\subseteq S\). Plugging in the above equation for \(v(S\cup z_{i})-v(S)\), we have

\[\phi_{z_{i}}\left(D_{-z_{i}};v,w\right)\] \[=\frac{1}{N}\sum_{k=0}^{N-1}w(k+1)\sum_{\begin{subarray}{c}S \subseteq D_{-z_{i}},\\ |S|=k\end{subarray}}[v(S\cup\{z_{i}\})-v(S)]\] \[=\frac{1}{N}\sum_{k=0}^{N-1}w(k+1)\sum_{\begin{subarray}{c}S \subseteq\mathtt{NB}_{x^{(\text{val})},\tau}(D_{-z_{i}}),\\ |S_{0}|\leq k\end{subarray}}\binom{N-\mathbf{c}_{x^{(\text{val})},\tau}}{k-|S _{0}|}\left[v(S_{0}\cup\{z_{i}\})-v(S_{0})\right]\] \[=\frac{1}{N}\sum_{k=0}^{N-1}w(k+1)\sum_{j=0}^{k}\sum_{S_{0} \subseteq\mathtt{NB}_{x^{(\text{val})},\tau}(D_{-z_{i}}),}\binom{N-\mathbf{c} _{x^{(\text{val})},\tau}}{k-j}\left[v(S_{0}\cup\{z_{i}\})-v(S_{0})\right]\] \[=\frac{1}{N}\sum_{k=0}^{N-1}w(k+1)\left\{\underbrace{\sum_{j=1}^{ k}\sum_{S_{0}\subseteq\mathtt{NB}_{x^{(\text{val})},\tau}(D_{-z_{i}}),\atop|S_{0}|=j} \binom{N-\mathbf{c}_{x^{(\text{val})},\tau}}{k-j}\left[\frac{1}{1+j}+\left( \frac{1}{1+j}-\frac{1}{j}\right)\sum_{(x,y)\in S_{0}}\mathbb{1}\left[y=y^{( \text{val})}\right]\right]}_{(*)}\right.\] \[\qquad\qquad\qquad\qquad\qquad\left.+\binom{N-\mathbf{c}_{x^{( \text{val})},\tau}}{k}\left[\mathbb{1}[y_{i}=y^{(\text{val})}]-1/C\right]\right\}\]

and

\[(*)=\sum_{j=1}^{k}\binom{N-\mathbf{c}_{x^{(\text{val})},\tau}}{k-j}\left[ \binom{\mathbf{c}_{x^{(\text{val})},\tau}-1}{j}\frac{\mathbb{1}[y_{i}=y^{( \text{val})}]}{1+j}+\left(\frac{1}{1+j}-\frac{1}{j}\right)\sum_{S_{0} \subseteq\mathtt{NB}_{x^{(\text{val})},\tau}(D_{-z_{i}})}\sum_{(x,y)\in S_{0} }\mathbb{1}\left[y=y^{(\text{val})}\right]\right]\]

\[\sum_{S_{0}\subseteq\mathtt{NB}_{x^{(\text{val})},\tau}(D_{-z_{i}}),\tau} \sum_{(x,y)\in S_{0}}\mathbb{1}[y=y^{(\text{val})}] =\sum_{\ell=0}^{j}\ell\binom{\mathbf{c}_{z^{(\text{val})},\tau} }{\ell}\binom{\mathbf{c}_{x^{(\text{val})},\tau}-1-\mathbf{c}_{z^{(\text{val })},\tau}^{(+)}}{j-\ell}\] \[=\mathbf{c}_{z^{(\text{val})},\tau}^{(+)}\sum_{\ell=1}^{j}\binom {\mathbf{c}_{z^{(\text{val})},\tau}^{(+)}-1}{\ell-1}\binom{\mathbf{c}_{x^{( \text{val})},\tau}-1-\mathbf{c}_{z^{(\text{val})},\tau}^{(+)}}{j-\ell}\] \[=\mathbf{c}_{z^{(\text{val})},\tau}^{(+)}\binom{\mathbf{c}_{x^{( \text{val})},\tau}-2}{j-1}\]Hence

\[(*)\] \[=\mathbbm{1}[\mathbf{c}_{x^{(\mathrm{val})},\tau}\geq 2]\mathbbm{1} [y_{i}=y^{(\mathrm{val})}]\sum_{j=1}^{k}\binom{N-\mathbf{c}_{x^{(\mathrm{val})}, \tau}}{k-j}\binom{\mathbf{c}_{x^{(\mathrm{val})},\tau}-1}{j}\frac{1}{1+j}\] \[\quad-\mathbf{c}_{z^{(\mathrm{val})},\tau}^{(+)}\sum_{j=1}^{k} \binom{N-\mathbf{c}_{x^{(\mathrm{val})},\tau}}{k-j}\binom{\mathbf{c}_{x^{( \mathrm{val})},\tau}-2}{j-1}\frac{1}{j(j+1)}\] \[=\mathbbm{1}[\mathbf{c}_{x^{(\mathrm{val})},\tau}\geq 2]\mathbbm{1} [y_{i}=y^{(\mathrm{val})}]\sum_{j=1}^{k}\binom{N-\mathbf{c}_{x^{(\mathrm{val})},\tau}}{k-j}\binom{\mathbf{c}_{x^{(\mathrm{val})},\tau}-1}{j+1}\frac{1}{ \mathbf{c}_{x^{(\mathrm{val})},\tau}}\] \[\quad-\mathbf{c}_{z^{(\mathrm{val})},\tau}^{(+)}\sum_{j=1}^{k} \binom{N-\mathbf{c}_{x^{(\mathrm{val})},\tau}}{k-j}\binom{\mathbf{c}_{x^{( \mathrm{val})},\tau}}{\mathbf{c}_{x^{(\mathrm{val})},\tau}(\mathbf{c}_{x^{( \mathrm{val})},\tau}-1)}\frac{1}{\mathbf{c}_{x^{(\mathrm{val})},\tau}}\] \[=\mathbbm{1}[\mathbf{c}_{x^{(\mathrm{val})},\tau}\geq 2] \left(\frac{\mathbbm{1}[y_{i}=y^{(\mathrm{val})}]}{\mathbf{c}_{x^{(\mathrm{val})},\tau}}-\frac{\mathbf{c}_{z^{(\mathrm{val})},\tau}^{(+)}}{\mathbf{c}_{x^{( \mathrm{val})},\tau}(\mathbf{c}_{x^{(\mathrm{val})},\tau}-1)}\right)\sum_{j=1} ^{k}\binom{N-\mathbf{c}_{x^{(\mathrm{val})},\tau}}{k-j}\binom{\mathbf{c}_{x^{( \mathrm{val})},\tau}}{j+1}\] \[=\mathbbm{1}[\mathbf{c}_{x^{(\mathrm{val})},\tau}\geq 2] \left(\frac{\mathbbm{1}[y_{i}=y^{(\mathrm{val})}]}{\mathbf{c}_{x^{(\mathrm{val })},\tau}}-\frac{\mathbf{c}_{z^{(\mathrm{val})},\tau}^{(+)}}{\mathbf{c}_{x^{( \mathrm{val})},\tau}(\mathbf{c}_{x^{(\mathrm{val})},\tau}-1)}\right)\underbrace {\left[\binom{N}{k+1}-\binom{N-\mathbf{c}_{x^{(\mathrm{val})},\tau}}{k+1}- \mathbf{c}_{x^{(\mathrm{val})},\tau}\binom{N-\mathbf{c}_{x^{(\mathrm{val})}, \tau}}{k}\right]}_{B(k)}\]

Hence,

\[\phi_{z_{i}}\left(D_{-z_{i}};v,w\right) =\frac{1}{N}\sum_{k=0}^{N-1}w(k+1)\left\{\mathbbm{1}[\mathbf{c}_{ x^{(\mathrm{val})},\tau}\geq 2]\left(\frac{\mathbbm{1}[y_{i}=y^{(\mathrm{val})}]}{ \mathbf{c}_{x^{(\mathrm{val})},\tau}}-\frac{\mathbf{c}_{z^{(\mathrm{val})}, \tau}^{(+)}}{\mathbf{c}_{x^{(\mathrm{val})},\tau}(\mathbf{c}_{x^{(\mathrm{val} )},\tau}-1)}\right)B(k)\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\left.+\binom{N-\mathbf{c}_{ x^{(\mathrm{val})},\tau}}{k}\left[\mathbbm{1}[y_{i}=y^{(\mathrm{val})}]-1/C \right]\right\}\] \[=\frac{\mathbbm{1}[\mathbf{c}_{x^{(\mathrm{val})},\tau}\geq 2]}{N} \left(\frac{\mathbbm{1}[y_{i}=y^{(\mathrm{val})}]}{\mathbf{c}_{x^{(\mathrm{val})},\tau}}-\frac{\mathbf{c}_{z^{(\mathrm{val})},\tau}^{(+)}}{\mathbf{c}_{x^{( \mathrm{val})},\tau}(\mathbf{c}_{x^{(\mathrm{val})},\tau}-1)}\right)\sum_{k=0} ^{N-1}w(k+1)B(k)\] \[\quad+\frac{1}{N}\left[\mathbbm{1}[y_{i}=y^{(\mathrm{val})}]-1/C \right]\sum_{k=0}^{N-1}w(k+1)\binom{N-\mathbf{c}_{x^{(\mathrm{val})},\tau}}{k}\]

#### c.2.1 The Shapley value for TKNN (TKNN-Shapley)

**Theorem 13** (Full version of TKNN-Shapley).: _Consider the utility function \(v_{z^{(\mathrm{val})}}^{\texttt{TKNN}}\) in (3). Given a validation data point \(z^{(\mathrm{val})}=(x^{(\mathrm{val})},y^{(\mathrm{val})})\), the Shapley value \(\phi_{z_{i}}^{\texttt{TKNN}}(v_{z^{(\mathrm{val})}}^{\texttt{TKNN}})\) of each training point \(z_{i}=(x_{i},y_{i})\in D\) can be calculated as follows:_

\[\phi_{z_{i}}^{\texttt{TKNN}}=\begin{cases}\mathbbm{1}[\mathbf{c}_{z^{(\mathrm{ val})},\tau}\geq 2]A_{1}A_{2}+\frac{\mathbbm{1}[y_{i}=y^{(\mathrm{val})}]-1/C}{ \mathbf{c}_{x^{(\mathrm{val})},\tau}}&d(x_{i},x^{(\mathrm{val})})\leq\tau\\ 0&d(x_{i},x^{(\mathrm{val})})>\tau\end{cases} \tag{7}\]

_where \(A_{1}=\frac{\mathbbm{1}[y_{i}=y^{(\mathrm{val})}]}{\mathbf{c}_{x^{(\mathrm{val})},\tau}}-\frac{\mathbf{c}_{z^{(\mathrm{val})},\tau}^{(+)}}{\mathbf{c}_{z^{( \mathrm{val})},\tau}(\mathbf{c}_{z^{(\mathrm{val})},\tau}-1)}\), \(A_{2}=\sum_{k=0}^{\mathsf{e}}\left[\frac{1}{k+1}-\frac{1}{k+1}\cdot\frac{\binom{ \mathbf{e}-k}{x^{(\mathrm{val})},\tau}}{\binom{\mathbf{e}-k}{x^{(\mathrm{val})}, \tau}}\right]-1\),_\[\mathbf{c} =\mathbf{c}(D_{-z_{i}}):=|D_{-z_{i}}|\] (_size of \[D_{-z_{i}}\]_) \[\mathbf{c}_{x^{\left(\mathrm{val}\right),\tau}} =\mathbf{c}_{x^{\left(\mathrm{val}\right),\tau}}(D_{-z_{i}}):=1+ \left|I\!I\!B_{x^{\left(\mathrm{val}\right),\tau}}(D_{-z_{i}})\right|\] (1+ _# neighbors of \[x^{\left(\mathrm{val}\right)}\] in \[D_{-z_{i}}\]_) \[\mathbf{c}_{z^{\left(\mathrm{val}\right),\tau}}^{\left(+\right)} =\mathbf{c}_{z^{\left(\mathrm{val}\right),\tau}}^{\left(+\right)} (D_{-z_{i}}):=\sum_{\left(x,y\right)\in I\!I\!B_{x^{\left(\mathrm{val}\right), \tau}}(D_{-z_{i}})}\mathbb{1}[y=y^{\left(\mathrm{val}\right)}]\] (_# same-label neighbors in \[D_{-z_{i}}\]_)

_and \(C\) is the number of classes for the classification task, and \(1/C\) is the random guess accuracy._

**Runtime:** _the computation of all \((\phi_{z_{1}}^{\texttt{TFAH}},\ldots,\phi_{z_{N}}^{\texttt{TFAH}})\) can be achieved in \(O(N)\) runtime in total._

**The Shapley value when using full validation set:** _The Shapley value corresponding to the utility function \(v_{D^{\left(\mathrm{val}\right)}}^{\texttt{TFAH}}\) can be calculated as \(\phi_{z_{i}}^{\texttt{TFAH}}\left(v_{D^{\left(\mathrm{val}\right)}}^{\texttt{ TFAH}}\right)=\sum_{z^{\left(\mathrm{val}\right)}\in D^{\left(\mathrm{val}\right)}} \phi_{z_{i}}^{\texttt{TFAH}}\left(v_{z^{\left(\mathrm{val}\right)}}^{\texttt{ TFAH}}\right)\)._

Proof.: For the Shapley value, we have \(w(k)=\binom{N-1}{k-1}^{-1}\).

\[\sum_{k=0}^{N-1}w(k+1)\cdot\binom{N}{k+1} =\sum_{k=0}^{N-1}\binom{N-1}{k}^{-1}\cdot\binom{N-\mathbf{c}_{x^{ \left(\mathrm{val}\right)},\tau}}{k+1}\] \[=\sum_{k=0}^{N-1}\frac{k!(N-1-k)!}{(N-1)!}\frac{(N-\mathbf{c}_{x ^{\left(\mathrm{val}\right)},\tau})!}{(k+1)!(N-\mathbf{c}_{x^{\left(\mathrm{val }\right)},\tau}-k-1)!}\] \[=\frac{(N-\mathbf{c}_{x^{\left(\mathrm{val}\right)},\tau})!}{(N-1 )!}\sum_{k=0}^{N-1}\frac{1}{k+1}\frac{(N-1-k)!}{(N-\mathbf{c}_{x^{\left( \mathrm{val}\right)},\tau}-k-1)!}\] \[=\frac{(N-\mathbf{c}_{x^{\left(\mathrm{val}\right)},\tau})!}{(N-1 )!}\sum_{k=0}^{N-1}\frac{1}{k+1}\binom{N-1-k}{\mathbf{c}_{x^{\left(\mathrm{val }\right)},\tau}}\] \[=N\binom{N}{\mathbf{c}_{x^{\left(\mathrm{val}\right)},\tau}}^{-1} \sum_{k=0}^{N-1}\frac{1}{k+1}\binom{N-1-k}{\mathbf{c}_{x^{\left(\mathrm{val} \right)},\tau}}\]

\[\sum_{k=0}^{N-1}w(k+1)\cdot\binom{N-\mathbf{c}_{x^{\left(\mathrm{ val}\right)},\tau}}{k} =\sum_{k=0}^{N-1}\binom{N-1}{k}^{-1}\cdot\binom{N-\mathbf{c}_{x^{ \left(\mathrm{val}\right)},\tau}}{k}\] \[=\frac{(N-\mathbf{c}_{x^{\left(\mathrm{val}\right)},\tau})!}{(N-1 )!}\sum_{k=0}^{N-1}\frac{(N-1-k)!}{(N-\mathbf{c}_{x^{\left(\mathrm{val} \right)},\tau}-k)!}\] \[=\binom{N-1}{\mathbf{c}_{x^{\left(\mathrm{val}\right)},\tau}}^{-1} \sum_{k=0}^{N-1}\binom{N-1-k}{\mathbf{c}_{x^{\left(\mathrm{val}\right)},\tau}}\]\[\phi_{z_{i}}(D_{-z_{i}})\] \[=\frac{\mathbb{I}[\mathbf{c}_{x^{\text{(val)}},\tau}\geq 2]}{N} \left(\frac{\mathbb{I}[y_{i}=y^{\text{(val)}}]}{\mathbf{c}_{x^{\text{(val)}}, \tau}}-\frac{\mathbf{c}_{z^{\text{(val)}},\tau}^{(+)}}{\mathbf{c}_{x^{\text{( val)}},\tau}(\mathbf{c}_{x^{\text{(val)}},\tau}-1)}\right)\left[\sum_{k=0}^{N-1} \left(\frac{1}{k+1}-\frac{1}{k+1}\frac{\binom{N-1-k}{\mathbf{c}_{x^{\text{(val)}}, \tau}}}{\binom{N}{\mathbf{c}_{x^{\text{(val)}},\tau}}}\right)-1\right]\] \[\quad+\left[\mathbb{I}[y_{i}=y^{\text{(val)}}]-1/C\right]\frac{1} {\mathbf{c}_{x^{\text{(val)}},\tau}}\] \[=\mathbb{I}[\mathbf{c}_{x^{\text{(val)}},\tau}\geq 2]\left(\frac{ \mathbb{I}[y_{i}=y^{\text{(val)}}]}{\mathbf{c}_{x^{\text{(val)}},\tau}}-\frac {\mathbf{c}_{z^{\text{(val)}},\tau}^{(+)}}{\mathbf{c}_{x^{\text{(val)}},\tau}( \mathbf{c}_{x^{\text{(val)}},\tau}-1)}\right)\left[\sum_{k=0}^{\mathbf{c}} \left(\frac{1}{k+1}-\frac{1}{k+1}\frac{\binom{\mathbf{c}_{x^{\text{(val)}}, \tau}}{\binom{\mathbf{c}+1}{\mathbf{c}_{x^{\text{(val)}},\tau}}}}{\binom{ \mathbf{c}+1}{\mathbf{c}_{x^{\text{(val)}},\tau}}}\right)-1\right]\] \[\quad+\left[\mathbb{I}[y_{i}=y^{\text{(val)}}]-1/C\right]\frac{1} {\mathbf{c}_{x^{\text{(val)}},\tau}}\]

#### c.2.2 Efficient Computation of TKNN-Shapley

As we can see, all of the quantities that are needed to compute TKNN-Shapley value \((\mathbf{c},\mathbf{c}_{x^{\text{(val)}},\tau},\mathbf{c}_{z^{\text{(val)}}, \tau}^{(+)})\) are simply counting queries on \(D_{-z_{i}}\), and hence \(\mathbf{C}_{z^{\text{(val)}}}(D_{-z_{i}})\) can be computed in \(O(N)\) runtime for any \(z_{i}\in D\). Since our goal is to release \(\phi_{z_{i}}^{\text{TRNN}}\) for _all_\(z_{i}\in D\), we need to compute \(\mathbf{C}_{z^{\text{(val)}}}(D_{-z_{i}})\) for _all_\(z_{i}\in D\). A more efficient way for this case is to first compute \(\mathbf{C}_{z^{\text{(val)}}}(D)\) on the full dataset \(D\), and then we can compute each of \(\mathbf{C}_{z^{\text{(val)}}}(D_{-z_{i}})\) by

\[\mathbf{c}(D_{-z_{i}}) =\mathbf{c}(D)-1\] \[\mathbf{c}_{x^{\text{(val)}},\tau}(D_{-z_{i}}) =\mathbf{c}_{x^{\text{(val)}},\tau}(D)-\mathbb{I}[z_{i}\in \mathtt{NB}_{x^{\text{(val)}},\tau}]\] \[\mathbf{c}_{z^{\text{(val)}},\tau}^{(+)}(D_{-z_{i}}) =\mathbf{c}_{z^{\text{(val)}},\tau}^{(+)}(D)-\mathbb{I}[z_{i} \in\mathtt{NB}_{x^{\text{(val)}},\tau}]\mathbb{I}[y_{i}=y^{\text{(val)}}]\]

which can be computed in \(O(1)\) runtime. Hence, we can compute TKNN-Shapley \(\phi_{z_{i}}^{\text{TRNN}}(v_{z^{\text{(val)}}}^{\text{TRNN}})\) for _all_\(z_{i}\in D\) within an overall computational cost of \(O(N)\).

Extra Details for DP-TKNN-Shapley

### Privacy Guarantee for DP-TKNN-Shapley

**Theorem 14** (Restatement of Theorem 6).: _For any \(z^{(\text{val})}\in D^{(\text{val})}\), releasing \(\widehat{\phi}^{\texttt{TKNN}}_{z_{i}}(v^{\texttt{TKNN}}_{z^{(\text{val})}}):= \phi^{\texttt{TKNN}}_{z_{i}}\Big{[}\widehat{\mathbf{C}}_{z^{(\text{val})}}(D_{ -z_{i}})\Big{]}\) to data owner \(i\) is \((\varepsilon,\delta)\)-DP with \(\sigma=\sqrt{3}\cdot\sqrt{\log(1.25/\delta)}/\varepsilon\)._

Proof.: The Gaussian mechanism is known to satisfy \((\varepsilon,\delta)\)-DP with \(\sigma=\Delta\cdot\sqrt{2\log(1.25/\delta)}/\varepsilon\)[16], where \(\Delta:=\sup_{D\sim D^{\prime}}\|f(D)-f(D^{\prime})\|\) represents the global sensitivity of the underlying function \(f\) in \(\ell_{2}\) norm. In our case, \(f\) calculates the 3-dimensional vector \(\Big{[}\mathbf{c},\mathbf{c}_{x^{(\text{val})},\tau},\mathbf{c}^{(+)}_{z^{( \text{val})},\tau}\Big{]}\). As each training data point may alter both \(\mathbf{c}_{x^{(\text{val})},\tau}\) and \(\mathbf{c}^{(+)}_{z^{(\text{val})},\tau}\) by 1, the sensitivity \(\Delta\) is \(\sqrt{3}\). Since both \(\widehat{\mathbf{c}}_{x^{(\text{val})},\tau}\) and \(\widehat{\mathbf{c}}^{(+)}_{z^{(\text{val})},\tau}\) are privatized using the Gaussian mechanism, the privacy guarantee of \(\widehat{\phi}_{z_{i}}\) is ensured by the post-processing property of differential privacy. 

### Advantages of DP-TKNN-Shapley

#### d.2.1 Computational Efficiency via Reusing Privatized Statistics

Recall that in practical data valuation scenarios, it is often desirable to compute the data value scores for _all of_\(z_{i}\in D\). As we detailed in Section 4.2 and Appendix C.2.2, for TKNN-Shapley, such a computation only requires \(O(N)\) runtime in total if we first compute \(\mathbf{C}_{z^{(\text{val})}}(D)\) and subsequently \(\mathbf{C}_{z^{(\text{val})}}(D_{-z_{i}})\) for each \(z_{i}\in D\). In a similar vein, we can efficiently compute the DP variant \(\widehat{\phi}^{\texttt{TKNN}}_{z_{i}}\) for _all_\(z_{i}\in D\). To do so, we first calculate \(\widehat{\mathbf{C}}_{z^{(\text{val})}}(D)\) and then, for each \(z_{i}\in D\), we compute \(\widehat{\mathbf{C}}_{z^{(\text{val})}}(D_{-z_{i}})\) as follows:

\[\widehat{\mathbf{c}}(D_{-z_{i}}) =\widehat{\mathbf{c}}(D)-1\] \[\widehat{\mathbf{c}}_{x^{(\text{val})},\tau}(D_{-z_{i}}) =\widehat{\mathbf{c}}_{x^{(\text{val})},\tau}(D)-\mathbbm{1}[z_{i }\in\mathbb{NB}_{x^{(\text{val})},\tau}]\] \[\widehat{\mathbf{c}}^{(+)}_{z^{(\text{val})},\tau}(D_{-z_{i}}) =\widehat{\mathbf{c}}^{(+)}_{z^{(\text{val})},\tau}(D)-\mathbbm{1 }[z_{i}\in\mathbb{NB}_{x^{(\text{val})},\tau}]\mathbbm{1}[y_{i}=y^{(\text{val })}]\]

which can be computed in \(O(1)\) runtime. Hence, we can compute DP-TKNN-Shapley \(\widehat{\phi}^{\texttt{TKNN}}_{z_{i}}(v^{\texttt{TKNN}}_{z^{(\text{val})}})\) for _all_\(z_{i}\in D\) within an overall computational cost of \(O(N)\). It is important to note that when releasing \(\widehat{\phi}^{\texttt{TKNN}}_{z_{i}}\) to individual \(i\), the data point they hold, \(z_{i}\), is not private to the individual themselves. Therefore, as long as \(\widehat{\mathbf{C}}_{z^{(\text{val})}}(D)\) is privatized, \(\widehat{\mathbf{C}}_{z^{(\text{val})}}(D_{-z_{i}})\) and hence \(\widehat{\phi}^{\texttt{TKNN}}_{z_{i}}\) also satisfy the same privacy guarantee due to DP's post-processing property.

#### d.2.2 Collusion Resistance via Reusing Privatized Statistics

In Section 5.1, we consider the single Shapley value \(\phi^{\texttt{TKNN}}_{z_{i}}(D_{-z_{i}})\) as the function to privatize. However, when we consider the release of all Shapley values \(\mathcal{M}(D):=(\widehat{\phi}^{\texttt{TKNN}}_{z_{1}}(D_{-z_{1}}),\ldots, \widehat{\phi}^{\texttt{TKNN}}_{z_{N}}(D_{-z_{N}}))\) as a whole mechanism, one can show that such a mechanism satisfies _joint differential privacy_ (JDP) [34] if we reuse the privatized statistic \(\widehat{\mathbf{C}}_{z^{(\text{val})}}(D)\) for the release of all \(\widehat{\phi}^{\texttt{TKNN}}_{z_{i}}\). The consequence of satisfying JDP is that our mechanism is resilient against collusion among groups of individuals without any privacy degradation. That is, even if an arbitrary group of individuals in \([N]\setminus i\) colludes (i.e., shares their respective \(\widehat{\phi}^{\texttt{TKNN}}_{z_{j}}\) values within the group), the privacy of individual \(i\) remains uncompromised. Our method also stands resilient in scenarios where a powerful adversary sends multiple data points and receives multiple value scores.

**Definition 15** (Joint Differential Privacy [34]).: _For \(\varepsilon,\delta\geq 0\), a randomized algorithm \(\mathcal{M}:\mathbb{N}^{\mathcal{X}}\rightarrow\mathcal{Y}^{N}\) is \((\varepsilon,\delta)\)-joint differentially private if for every possible pair of \(z,z^{\prime}\in\mathcal{X}\), for every \(i\in[N]\), and for every subset of possible outputs \(E\subseteq\mathcal{Y}^{N-1}\), we have_

\[\Pr_{\mathcal{M}}[\mathcal{M}(z\cup D_{-z})_{-i}\in E]\leq e^{\varepsilon}\Pr_ {\mathcal{M}}[\mathcal{M}(z^{\prime}\cup D_{-z})_{-i}\in E]+\delta\]

_where \(\mathcal{M}_{-i}\) denotes the output of \(\mathcal{M}\) that excludes the \(i\)th dimension._

**Theorem 16**.: _The mechanism of releasing of all noisy KNN-Shapley values_

\[\mathcal{M}(D):=(\widehat{\phi}_{z_{1}}^{\texttt{TKNN}}(D_{-z_{1}}),\ldots, \widehat{\phi}_{z_{N}}^{\texttt{TKNN}}(D_{-z_{N}}))\]

_satisfy \((\varepsilon,\delta)\)-JDP if \(\sigma=\sqrt{3}\cdot\sqrt{\log(1.25/\delta)}/\varepsilon\) and each \(\phi_{z_{i}}^{\texttt{TKNN}}\) is computed via reusing the privatized \(\mathbf{C}_{z^{\text{(val)}}}\)._

Proof.: The proof relies on the "post-processing" property of differential privacy.

First of all, we select \(\sigma=\sqrt{3}\cdot\sqrt{\log(1.25/\delta)}/\varepsilon\), ensuring that \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D)\) satisfies \((\varepsilon,\delta)\)-DP.

In the context of DP-TKNN-shapley, we calculate \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D_{-z_{i}})\) for each \(z_{i}\) as part of a post-processing step on \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D)\).

It is important to note that the released data value \(\widehat{\phi}_{z_{i}}^{\texttt{TKNN}}(D_{-z_{i}})\) is a function of \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D_{-z_{i}})\), and \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D_{-z_{i}})\) can be obtained as post-processing of \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D)\) given only the knowledge of \(z_{i}\). Hence, we can express the probability of \(\mathcal{M}(z_{i},D_{-z_{i}})_{-i}\) belonging to a certain set \(E\) as follows:

\[\Pr_{\mathcal{M}}[\mathcal{M}(z_{i},D_{-z_{i}})_{-i}\in E]=\Pr_{\widehat{ \mathbf{C}}_{z^{\text{(val)}}}(D)}[f_{D_{-z_{i}}}(\widehat{\mathbf{C}}_{z^{ \text{(val)}}}(D))\in E]\]

where

\[f_{D_{-z_{i}}}(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D))\] \[=\left(\widehat{\phi}_{z_{1}}^{\texttt{TKNN}}[\widehat{\mathbf{C} }_{z^{\text{(val)}}}(D_{-z_{1}})],\ldots,\phi_{z_{i-1}}^{\texttt{TKNN}}[ \widehat{\mathbf{C}}_{z^{\text{(val)}}}(D_{-z_{i-1}})],\phi_{z_{i+1}}^{\texttt {TKNN}}[\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D_{-z_{i+1}})],\ldots,\phi_{z _{N}}^{\texttt{TKNN}}[\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D_{-z_{N}})]\right)\]

We note that \(f_{D_{-z_{i}}}\) is a function that does _not_ depend on \(z_{i}\).

Then, for any \(z_{i},z_{i}^{\prime}\) and for any tuple of \(D_{-z_{i}}\), denote \(D:=z_{i}\cup D_{-z_{i}}\) and \(D^{\prime}:=z_{i}^{\prime}\cup D_{-z_{i}}\), we have

\[\Pr_{\mathcal{M}}[\mathcal{M}(z_{i},D_{-z_{i}})_{-i}\in E] =\Pr_{\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D)}[f_{D_{-z_{i}}} (\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D))\in E]\] \[=\Pr_{\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D)}[\widehat{ \mathbf{C}}_{z^{\text{(val)}}}(D)\in f_{D_{-z_{i}}}^{-1}(E)]\] \[\leq e^{\varepsilon}\Pr_{\widehat{\mathbf{C}}_{z^{\text{(val)}}} (D^{\prime})}[\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D^{\prime})\in f_{D_{-z _{i}}}^{-1}(E)]+\delta\] \[=e^{\varepsilon}\Pr_{\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D^{ \prime})}[f_{D_{-z_{i}}}(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D^{\prime})) \in E]+\delta\] \[=e^{\varepsilon}\Pr_{\mathcal{M}}[\mathcal{M}(z_{i}^{\prime},D_{ -z_{i}})\in E]+\delta\]

where the first inequality is due to the \((\varepsilon,\delta)\)-DP guarantee of \(\widehat{\mathbf{C}}_{z^{\text{(val)}}}(D)\). 

Hence, the privacy guarantee for our DP-TKNN-Shapley remains the same even if multiple attackers collude and share the received noisy data value scores with each other. As a comparison, the naive DP-KNN-Shapley which independently adds noise for each released data value score does not enjoy such powerful collusion resistance property.

#### d.2.3 Privacy Amplification by Subsampling

In order to further boost the privacy guarantee, we incorporate the privacy amplification by subsampling technique [4]. Specifically, before any computation, we first sample each data point independently with a probability \(q\) (this is usually referred to as Poisson subsampling). The differentially private quantities based on the subsampled dataset, denoted as \(\texttt{sample}(D)\), are calculated easily as follows:

\[\widehat{\mathbf{c}}(D) :=\texttt{round}\left(\mathbf{c}(\texttt{sample}(D))+\mathcal{N} \left(0,\sigma^{2}\right)\right)\] \[\widehat{\mathbf{c}}_{z^{\text{(val)}},\tau}(D) :=\texttt{round}\left(\mathbf{c}_{z^{\text{(val)}},\tau}(\texttt {sample}(D))+\mathcal{N}\left(0,\sigma^{2}\right)\right)\] \[\widehat{\mathbf{c}}_{z^{\text{(val)}},\tau}^{(+)}(D) :=\texttt{round}\left(\mathbf{c}_{z^{\text{(val)}},\tau}^{(+)}( \texttt{sample}(D))+\mathcal{N}\left(0,\sigma^{2}\right)\right)\]The function round rounds a noisy value to the nearest integer. If the noisy value is out of range (e.g., \(<0\)), then it is rounded to the nearest possible value within the valid range. By subsampling the data points, the privacy guarantee is amplified. This is because the subsampling process itself is a randomized operation that provides a level of privacy, making it harder to infer information about individual data points. When combined with the Gaussian mechanism, the overall privacy guarantee is improved.

The subsampling technique can be incorporated easily for DP-TKNN-Shapley as above, where the subsampling technique only improves the computational efficiency. As a comparison, incorporating subsampling technique for DP-KNN-Shapley will introduce a significantly higher computational cost, as we discussed in Appendix B.3.2.

Differentially private release of \(\phi_{z_{i}}^{\texttt{TWIN}}(v_{D^{\rm(val)}}^{\texttt{TWIN}})\) (Privacy Accounting)

Recall that the TKNN-Shapley corresponding to the full validation set \(D^{\rm(val)}\) is \(\phi_{z_{i}}^{\texttt{TWIN}}\left(v_{D^{\rm(val)}}^{\texttt{TWIN}}\right)= \sum_{z^{\rm(val)}\in D^{\rm(val)}}\phi_{z_{i}}^{\texttt{TWIN}}(D_{-z_{i}};z^ {\rm(val)})\). We can compute privatized \(\phi_{z_{i}}^{\texttt{TWIN}}\left(v_{D^{\rm(val)}}^{\texttt{TWIN}}\right)\) by simply releasing \(\phi_{z_{i}}^{\texttt{TWIN}}(D_{-z_{i}};z^{\rm(val)})\) for all \(z^{\rm(val)}\in D^{\rm(val)}\)3. To better keep track of the overall privacy cost, we use the current state-of-the-art privacy accounting technique based on the notion of the Privacy Loss Random Variable (PRV) [17]. Here, we provide more details about the background of the composition of differential privacy and the privacy accounting techniques we use.

Footnote 3: Directly privatizing \(\phi_{z_{i}}^{\texttt{TWIN}}\left(v_{D^{\rm(val)}}^{\texttt{TWIN}}\right)\) is very difficult. Releasing more privatized statistics for the ease of privacy analysis is common in DP, e.g., DP-SGD [1] releases all privatized gradients.

**Background: Composition of Differential Privacy ("Privacy Accounting").** In practice, multiple differentially private mechanisms may be applied to the same dataset, denoted as \(\mathcal{M}(D)=\mathcal{M}_{1}\circ\mathcal{M}_{2}(D):=(\mathcal{M}_{1}(D), \mathcal{M}_{2}(D))\).4 Differential privacy offers strong composition guarantees, and the guarantees are derived by various composition theorems or privacy accounting techniques, including the basic composition theorem [14], advanced composition theorem [18], and Moments Accountant [1]. For example, the basic composition theorem states that if \(\mathcal{M}_{1}\) is \((\varepsilon_{1},\delta_{1})\)-DP and \(\mathcal{M}_{2}\) is \((\varepsilon_{2},\delta_{2})\)-DP, then the composition of \(\mathcal{M}_{1}\) and \(\mathcal{M}_{2}\) is \((\varepsilon_{1}+\varepsilon_{2},\delta_{1}+\delta_{2})\)-DP. In our scenario, each individual mechanism is the release of \(\widehat{\phi}_{z_{i}}^{\texttt{TWIN}}(D_{-z_{i}};z^{\rm(val)})\) for each of \(z^{\rm(val)}\in D^{\rm(val)}\), and we would like to keep track of the privacy loss of releasing all of them.

Footnote 4: Multiple DP mechanisms can be _adaptively_ composed in the sense that the output of one mechanism can be used as an input to another mechanism, i.e., \(\mathcal{M}(D)=\mathcal{M}_{1}\circ\mathcal{M}_{2}(D):=(\mathcal{M}_{1}(D), \mathcal{M}_{2}(D,\mathcal{M}_{1}(D)))\).

**Privacy Accounting Techniques based on Privacy Loss Random Variable (PRV).** To better keep track of the privacy cost, we use the current state-of-the-art privacy accounting based on the notion of the Privacy Loss Random Variable (PRV) [17]. The PRV accountant was introduced in [37] and later refined in [36, 25]. For any DP algorithm, one can easily compute its \((\varepsilon,\delta)\) privacy guarantee based on the distribution of its PRV. The key property of PRVs is that, under (adaptive) composition, they simply add up; the PRV \(Y\) of the composition \(\mathcal{M}=\mathcal{M}_{1}\circ M_{2}\circ\cdots\circ M_{k}\) is given by \(Y=\sum_{i=1}^{k}Y_{i}\), where \(Y_{i}\) is the PRV of \(\mathcal{M}_{i}\). Therefore, one can then find the distribution of \(Y\) by convolving the distributions of \(Y_{1},Y_{2},\ldots,Y_{k}\). Prior works [36, 25] approximate the distribution of PRVs by truncating and discretizing them, then using the Fast Fourier Transform (FFT) to efficiently convolve the distributions. In our experiment, we use the FFT-based accountant from [25], the current state-of-the-art privacy accounting technique.

Additional Experiment Settings & Results

### Datasets

A comprehensive list of datasets and sources is summarized in Table 7. Similar to the existing data valuation literature [23; 39; 60; 30], we preprocess datasets for ease of training. For Fraud, Creditcard, and all datasets from OpenML, we subsample the dataset to balance positive and negative labels. For these datasets, if they have multi-class, we binarize the label by considering \(\mathbbm{1}[y=1]\). For the image dataset MNIST, CIFAR10, we apply a ResNet50 [27] that is pre-trained on the ImageNet dataset as the feature extractor. This feature extractor produces a 1024-dimensional vector for each image. We employ sentence embedding models [54] to extract features for the text classification dataset AGNews and DBPedia, and the extracted features are 1024-dimensional vectors for each text instance. We perform L2 normalization on the extracted features as part of the pre-processing step.

The size of each dataset we use is shown in Table 7. For some of the datasets, we use a subset of the full set. We stress that we are using a _much larger_ size of the dataset compared with prior data valuation literature [30; 39; 60] (these works usually only pick a very small subset of the full dataset, e.g., 2000 data points for binarized CIFAR10). The validation data size we use is 10% of the training data size.

### Settings & Additional Experiments for Runtime Experiments

For the runtime comparison experiment in Section 6.1, we follow similar experiment settings from prior study [40] and use a synthetic binary classification dataset. To generate the synthetic dataset, we sample data points from a \(d\)-dimensional standard Gaussian distribution. The labels are assigned based on the sign of the sum of the two features.

We choose a range of training data sizes \(N\), and compare the runtime of both TKNN/KNN-Shapley at each \(N\). We use 100 validation data points. For Figure 2 in the maintext, the data dimension \(d=10\). Here, we show additional experiment results when \(d=100\). As we can see from Figure 5, again TKNN-Shapley achieves better computational efficiency than KNN-Shapley across all training data sizes, and is around 30% faster than KNN-Shapley for large \(N\).

### Settings & Additional Experiments for Distinguishing Data Quality Experiments

#### e.3.1 Details for Mislabel/Noisy Data Detection Experiment

In the experiment of mislabeled (or noisy) data detection, we randomly choose 10% of the data points and flip their labels (or add strong noise to their features). For mislabel data detection, we flip 10% of the labels by picking an alternative label from the rest of the classes uniformly at random. For noisy data detection, we add zero-mean Gaussian noise to data features, where the standard deviation of the

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Dataset** & **Number of classes** & **Size of dataset** & **Source** \\ \hline MNIST & 10 & 50000 & [41] \\ CIFAR10 & 10 & 50000 & [38] \\ AGNews & 4 & 10000 & [65] \\ DBPedia & 14 & 10000 & [2] \\ Click & 2 & 2000 & [https://www.openml.org/d/1218](https://www.openml.org/d/1218) \\ Fraud & 2 & 2000 & [11] \\ Creditcard & 2 & 2000 & [71] \\ Apsfail & 2 & 2000 & [https://www.openml.org/d/41138](https://www.openml.org/d/41138) \\ Phoneme & 2 & 2000 & [https://www.openml.org/d/1489](https://www.openml.org/d/1489) \\ Wind & 2 & 2000 & [https://www.openml.org/d/847](https://www.openml.org/d/847) \\ Pol & 2 & 2000 & [https://www.openml.org/d/722](https://www.openml.org/d/722) \\ CPU & 2 & 2000 & [https://www.openml.org/d/761](https://www.openml.org/d/761) \\
2DPlanes & 2 & 2000 & [https://www.openml.org/d/727](https://www.openml.org/d/727) \\ \hline \hline \end{tabular}
\end{table}
Table 7: A summary of datasets used in Section 6s experiments.

Gaussian noise added to each feature dimension is equal to the average absolute value of the feature dimension across the full dataset.

#### e.3.2 Additional Experiment Results for Private Setting

Table 8 provides a comprehensive comparison of DP-TKNN-Shapley and DP-KNN-Shapley. We conducted the experiments on DP-KNN-Shapley with subsampling on only half of the datasets due to the considerable computational cost (at least \(30\times\) longer to execute compared to its non-subsampled counterpart). Additionally, because DP-KNN-Shapley injects Gaussian noise independently into the value score of each data holder, it does not have the property of being robust against collusion.

As evidenced by the results, DP-TKNN-Shapley consistently demonstrates a markedly superior privacy-utility tradeoff compared to DP-KNN-Shapley without subsampling across all datasets. Even when comparing with DP-KNN-Shapley with subsampling, DP-TKNN-Shapley still shows superior performance, and often significantly outshining this time-intensive and collusion-susceptible baseline. Notably, DP-TKNN-Shapley manages to maintain high AUROC values even when \(\varepsilon\approx 0.1\). The low performance of DP-KNN-Shapley can be attributed to its relatively high global sensitivity, as we discussed in Appendix B.3.1.

#### e.3.3 Additional Experiment Results for Non-private Setting

Here, we show full experimental results for various data valuation techniques in mislabel/noisy data detection scenarios, summarized in Tables 9 and 10. In line with the main text findings, TKNN-Shapley exhibits performance largely on par with KNN-Shapley across the majority of datasets, demonstrating that TKNN-Shapley effectively mirrors KNN-Shapley's ability to discern data quality. In addition, both KNN-Shapley and TKNN-Shapley significantly outperform traditional retrain-based data valuation techniques (LOO, Data Shapley, and Data Banzhaf), corroborating observations made in existing studies [31, 52]. The inferior performance of Data Shapley can be attributed to sample inefficiency and the inherent randomness during the retraining process [60].

**Settings for LOO, Data Shapley and Data Banzhaf.** Following the experiment settings in [23, 39] we use logistic regression as the ML model for these techniques. For Data Shapley, we use permutation sampling [48] for estimation. For Data Banzhaf, we use the maximum-sample-reuse (MSR) algorithm [60] for estimation. For each dataset, we train models on 10,000 subsets (which takes more than \(1000\times\) longer time compared with both TKNN/KNN-Shapley).

#### e.3.4 Ablation Study on the Choice of Threshold for TKNN-Shapley

KNN-Shapley is known for being effective on a wide range of choices of \(K\)[29]. In this experiment, we study the impact of different choices of \(\tau\) on the performance of TKNN-Shapley on the tasks of mislabeled/noisy data detection. The results are shown in Table 11 and 12. As we can see, the

Figure 5: Runtime comparison of TKNN-Shapley and KNN-Shapley across various training data sizes \(N\) when data dimension \(d=100\). The plot shows the average runtime based on 5 independent runs. Experiments were conducted with an AMD 64-Core CPU Processor.

[MISSING_PAGE_FAIL:37]

For completeness, we also show the impact of different choices of \(K\) on the performance of KNN-Shapley in Table 13 and 14. As we can see, KNN-Shapley's performance is quite stable against the different choices of \(K\).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Dataset** & **TKNN-Shapley** & **KNN-Shapley** & **Data Shapley** & **LOO** & **Data Banzhaf** \\ \hline
**2DPlanes** & 0.705 & 0.718 & 0.545 & 0.504 & 0.656 \\
**Phoneme** & 0.71 & 0.706 & 0.49 & 0.531 & 0.631 \\
**CPU** & 0.751 & 0.726 & 0.527 & 0.459 & 0.531 \\
**Fraud** & 0.818 & 0.794 & 0.502 & 0.526 & 0.582 \\
**Creditcard** & 0.611 & 0.64 & 0.535 & 0.513 & 0.506 \\
**Apsfail** & 0.841 & 0.666 & 0.533 & 0.465 & 0.566 \\
**Click** & 0.558 & 0.547 & 0.461 & 0.503 & 0.572 \\
**Wind** & 0.691 & 0.794 & 0.495 & 0.488 & 0.54 \\
**Pol** & 0.68 & 0.79 & 0.526 & 0.503 & 0.588 \\
**MNIST** & 0.613 & 0.815 & - & - & - \\
**CIFAR10** & 0.85 & 0.935 & - & - & - \\
**AG News** & 0.804 & 0.746 & - & - & - \\
**DBPedia** & 0.719 & 0.88 & - & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 10: Full Table for the AUROC of Noisy Data Detection (non-private setting). The results for LOO, Data Shapley, Data Banzhaf are omitted for MNIST, CIFAR10, AGNews and DBPedia as they require a significant amount of runtime (prior works typically use a subset for MNIST and CIFAR10 [23, 39, 60]).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Dataset** & **TKNN-Shapley** & **KNN-Shapley** & **Data Shapley** & **LOO** & **Data Banzhaf** \\ \hline
**2DPlanes** & 0.919 & 0.913 & 0.552 & 0.458 & 0.548 \\
**Phoneme** & 0.826 & 0.873 & 0.525 & 0.484 & 0.604 \\
**CPU** & 0.946 & 0.932 & 0.489 & 0.496 & 0.513 \\
**Fraud** & 0.96 & 0.967 & 0.488 & 0.495 & 0.534 \\
**Creditcard** & 0.662 & 0.646 & 0.517 & 0.487 & 0.536 \\
**Apsfail** & 0.958 & 0.948 & 0.496 & 0.485 & 0.506 \\
**Click** & 0.572 & 0.568 & 0.474 & 0.504 & 0.528 \\
**Wind** & 0.889 & 0.896 & 0.469 & 0.456 & 0.512 \\
**Pol** & 0.871 & 0.928 & 0.512 & 0.538 & 0.473 \\
**MNIST** & 0.962 & 0.974 & - & - & - \\
**CIFAR10** & 0.957 & 0.991 & - & - & - \\
**AG News** & 0.956 & 0.971 & - & - & - \\
**DBPedia** & 0.981 & 0.991 & - & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 9: Full Table for the AUROC of Mislabel Data Detection (non-private setting). The results for LOO, Data Shapley, Data Banzhaf are omitted for MNIST, CIFAR10, AGNews and DBPedia as they require a significant amount of runtime (prior works typically use a subset for MNIST and CIFAR10 [23, 39, 60]).

[MISSING_PAGE_EMPTY:39]