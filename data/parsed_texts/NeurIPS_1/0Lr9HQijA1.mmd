Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations

Hao Chen\({}^{1}\)

haoc3@andrew.cmu.edu

Ankit Shah\({}^{1}\)

Jindong Wang\({}^{2}\)

Ran Tao\({}^{1}\)

Yidong Wang\({}^{3}\)

Xiang Li\({}^{1}\)

Xing Xie\({}^{2}\)

Masashi Sugiyama\({}^{4,5}\)

Rita Singh\({}^{1}\)

Bhiksha Raj\({}^{1,6}\)

\({}^{1}\)Carnegie Mellon University

\({}^{2}\)Microsoft Research

\({}^{3}\)Peking University

\({}^{4}\) RIKEN AIP

\({}^{5}\)The University of Tokyo

\({}^{6}\)MBZUAI

###### Abstract

Learning with reduced labeling standards, such as noisy label, partial label, and supplementary unlabeled data, which we generically refer to as _imprecise_ label, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables. Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings, with closed-form learning objectives derived from the unified EM modeling. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first practical and unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain. Code is available at: [https://github.com/Hhhhhhao/General-Framework-Weak-Supervision](https://github.com/Hhhhhhao/General-Framework-Weak-Supervision).

## 1 Introduction

One of the critical challenges in machine learning is the collection of annotated data for model training [1, 2, 3, 4, 5, 6]. Ideally, every data instance would be fully annotated with precise labels. However, collecting such data can be expensive, time-consuming, and error-prone. Often, the labels can be intrinsically difficult to ascertain precisely. Factors such as a lack of annotator expertise and privacy concerns can also negatively affect the quality and completeness of the annotations.

In an attempt to circumvent this limitation, several methods have been proposed to permit model learning from the data annotated with reduced labeling standards, which are generally easier to obtain. We will refer to such labels as _imprecise_. Fig. 1 illustrates some typical mechanisms of label imprecision that are commonly addressed in the literature. Label imprecision requires a modification of the standard supervised training mechanism to build models for each specific case. For instance, _partial label learning_ (PLL) [7, 8, 9, 10, 11, 12, 13] allows instances to have a set of candidate labels, instead of a single definitive one. _Semi-supervised Learning_ (SSL) [14, 15, 16, 17, 18, 19, 20, 21, 22, 23] seeks to enhance the generalization ability when only a small set of labeled data is available, supplemented by a larger unlabeled set.

_Noisy label learning_ (NLL) [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37] deals with noisy scenarios where the labels are corrupted or incorrect. There is a greater variety of other forms of label imprecision, including crowd-sourcing [38, 39], programmable weak supervision [40, 41], and bag-level supervision [42, 43, 44, 45, 46, 47], among others.

While prior arts have demonstrated success in handling individual configurations of label imprecision, their approaches often differ substantially. They are tailored to a _specific_ form of imprecision, as depicted in Fig. 2. Such specificity not only imposes the necessity of devising a solution for emerging types of label imprecision scenarios, but also complicates the deployment in practical settings, where the annotations can be highly complex and may involve _multiple coexisting and interleaved_ imprecision configurations. For instance, considering a scenario where both noisy labels and partial labels appear together, it might be challenging to adapt previous methods in NLL or PLL to this scenario since they either rely on the assumption of definite labels [39] or the existence of the correct label among label candidates [48], thus requiring additional algorithmic design. In fact, a few recent works have attempted to address the combinations of imprecise labels in this way, such as partial noisy label [49, 50] and semi-supervised partial label learning [51, 52]. However, simply utilizing a more sophisticated or ad-hoc design can hardly scale to other settings. In addition, most of these approaches attempt to infer the correct labels given the imprecise information (_e.g._ through consistency with adjacent data [53, 14, 54], iterative refinement [55, 56], average over given labels [57, 58], etc., to train the model, which inevitably accumulates error during training.

In this paper, we formulate the problem from a different perspective: rather than taking the imprecise label information provided as a potentially noisy or incomplete attempt at assigning labels to instances, we treat it generically as the information that imposes a deterministic or statistical restriction of the actual applicable true labels. We then train the model over the distribution of all possible labeling entailed by the given imprecise information. More specifically, for a dataset with samples \(X\) and imprecise label information \(I\), we treat the inaccessible full and precise labels \(Y\) as a latent variable. The model is then trained to maximize the likelihood of the provided information \(I\). Since the likelihood computed over the joint probability \(P(X,I;\theta)=\sum_{Y}P(X,I,Y;\theta)\) must marginalize out \(Y\), the actual information \(I\) provided could permit a potentially exponential number of labeling. To deal with the resulting challenge of maximizing the logarithm of an expectation, we use the common approach of _expectation-maximization_ (EM) [59], where the E-step computes the expectation of \(P(X,I,Y;\theta)\) given the posterior of current belief \(P(Y|X,I;\theta^{t})\) at time step \(t\) and the M-step maximizes the tight variational lower bound over \(P(X,I;\theta)\). The overall framework is thus largely agnostic to the various nature of label imprecision, with the imprecise label only affecting the manner in which the posterior \(P(Y|X,I;\theta^{t})\) is computed. In fact, current approaches designed for various imprecise label scenarios can be treated as specific instances of our framework. Our approach can serve as a solution towards a _unified and generalized_ view for learning with _various_ imprecise labels.

While there exist earlier attempts on generalized or EM solutions for different (other) imprecise supervisions or fuzzy observations [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 66, 64], they usually require additional assumptions and approximations on the imprecise information for learnablility [48, 68], thus presenting limited scalability on practical settings [62]. On the contrary, the unified framework we propose subsumes all of these and naturally extends to the more practical "mixed" style of data, where different types of imprecise labels coexist. Moreover, for noisy labels, our framework inherently enables the learning of a _noise model_, as we will show in Section 3.2. Through comprehensive experiments, we demonstrate that the proposed imprecise label learning (ILL) framework not only outperforms previous methods

Figure 1: Illustration of the full label and imprecise label configurations. We use an example dataset of 4 training instances and 3 classes. (a) Full label, the annotation is a single true label; (b) Partial label, the annotation is a label candidate set containing true label; (c) Semi-supervised, only part of the dataset is labeled, and the others are unlabeled; (d) Noisy label, the annotation is mislabeled.

for dealing with single imprecise labels of PLL, NLL, and SSL, but also presents robustness and effectiveness for mixed imprecise label learning (MILL) settings, leveraging the full potential to more challenging scenarios. Our contributions are summarized as follows:

* We propose an EM framework towards the unification of learning from _various_ imprecise labels.
* We establish scalable and consistent state-of-the-art (SOTA) performance with the proposed method on partial label learning, semi-supervised learning, and noisy label learning, demonstrating our method's robustness in more diverse, complex label noise scenarios.
* To the best of our knowledge, our work is the first to show the robustness and effectiveness of a single unified method for handling the mixture of various imprecise labels.

## 2 Preliminary

In this section, we illustrate the notations and baselines from different imprecise label settings that adopt various solutions. We will show later how our proposed method generalize and subsumes these prior arts. Let \(\mathcal{X}\) denote the input space, and \(\mathcal{Y}=[C]:=\{1,\ldots,C\}\) represent the label space with \(C\) distinct labels. A fully annotated training dataset of size \(N\) is represented as \(\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\}_{i\in[N]}\). Learning with imprecise labels involves approximating the mapping function \(f\circ g:\mathcal{X}\rightarrow\mathcal{Y}\) from a training dataset where the true label \(y\) is not fully revealed from the annotation process. Here \(f\) is the backbone for feature extraction, \(g\) refers to the classifier built on top of the features, and the output from \(f\circ g\) is the predicted probability \(\mathbf{p}(y|\mathbf{x};\theta)\), where \(\theta\) is the learnable parameter for \(f\circ g\). In this study, we primarily consider three imprecise label configurations (as illustrated in Fig. 1) and their corresponding representative learning paradigms (as shown in Fig. 2), namely partial label learning, semi-supervised learning, and noisy label learning.

**Partial label learning (PLL)**. PLL aims to learn with a candidate label set \(\mathbf{s}\subset\mathcal{Y}\), where the ground truth label \(y\in\mathcal{Y}\) is concealed in \(\mathbf{s}\). The training data for partial labels thus becomes \(\mathcal{D}_{\mathrm{PLL}}=\{(\mathbf{x}_{i},\mathbf{s}_{i})\}_{i\in[N]}\). PICO [13] is a recent contrastive method that employs class prototypes to enhance label disambiguation (as shown in Fig. 2(a)). It optimizes the cross-entropy (CE)2 loss between the prediction of the augmented training sample \(\mathcal{A}_{w}(\mathbf{x})\) and the disambiguated labels \(\mathbf{\hat{s}}\). PiCO learns a set of class prototypes from the features associated with the same pseudo-targets. A contrastive loss, based on MOCO [70], is employed to better learn the feature space, drawing the projected and normalized features \(\mathbf{z}_{\mathrm{w}}\) and \(\mathbf{z}_{\mathrm{s}}\) of the two augmented versions of data \(\mathcal{A}_{\mathrm{w}}(\mathbf{x})\) and \(\mathcal{A}_{\mathrm{s}}(\mathbf{x})\)3 closer. The objective of PiCO is formulated as:

Footnote 2: For simplicity, we use \(\mathcal{L}_{\mathbf{CE}}\) for labels of the formats of class indices, one-hot vectors, and class probabilities.

Footnote 3: We use \(\mathcal{A}_{\mathrm{w}}\) to indicate the weaker data augmentation and \(\mathcal{A}_{\mathrm{s}}\) to indicate the stronger data augmentation.

\[\mathcal{L}_{\mathrm{PiCO}}=\mathcal{L}_{\mathrm{CE}}\left(\mathbf{p}(y| \mathcal{A}_{\mathrm{w}}(\mathbf{x});\theta),\hat{\mathbf{s}}\right)+ \mathcal{L}_{\mathrm{Cont}}\left(\mathbf{z}_{\mathrm{w}},\mathbf{z}_{\mathrm{s }},\mathcal{M}\right). \tag{1}\]

**Semi-supervised learning (SSL)**. For SSL, we can define the labeled dataset as \(\mathcal{D}_{\mathrm{SSL}}^{\mathrm{L}}=\{(\mathbf{x}_{i}^{\mathrm{l}},y_{i}^{ \mathrm{l}})\}_{i\in[N^{\mathrm{l}};\mathrm{l}}\), and the unlabeled dataset as \(\mathcal{D}^{\mathrm{U}}=\{\mathbf{x}_{j}^{\mathrm{u}}\}_{j\in[N^{\mathrm{L}} +1,N^{\mathrm{L}}+N^{\mathrm{U}}]}\), with \(N^{\mathrm{L}}\ll N^{\mathrm{U}}\). A general confidence-thresholding based self-training [53; 18] pipeline for SSL is shown in Fig. 2(b). Consider FixMatch [18] as an example; there are usually two loss components: the supervised CE loss on labeled data and the unsupervised CE loss on unlabeled data. For the unsupervised objective, the pseudo-labels \(\hat{y}^{\mathrm{u}}\) from the network itself are used to train on the unlabeled data. A "strong-weak" augmentation [53] is commonly adopted. To ensure the quality of the pseudo-labels,

Figure 2: Baseline model pipelines for various imprecise label configurations. (a) PiCO [13] for partial label learning. (b) FixMatch [18] for semi-supervised learning. (c) SOP [69] for noisy label learning. (d) The proposed unified framework. It accommodates _any_ imprecise label configurations and also mixed imprecise labels with an EM formulation.

only the pseudo-labels whose confidence scores \(\hat{p}^{\mathrm{u}}\) are greater than a threshold \(\tau\) are selected to participate in training:

\[\mathcal{L}_{\mathrm{Fix}}=\mathcal{L}_{\mathrm{CE}}\left(\mathbf{p}(y|\mathcal{A }_{\mathrm{w}}(\mathbf{x}^{\mathrm{l}});\theta),y^{\mathrm{l}}\right)+\mathbb{1 }\left(\hat{p}^{\mathrm{u}}\geq\tau\right)\mathcal{L}_{\mathrm{CE}}\left( \mathbf{p}(y|\mathcal{A}_{\mathrm{s}}(\mathbf{x}^{\mathrm{u}});\theta),\hat{y} ^{\mathrm{u}}\right). \tag{2}\]

**Noisy label learning (NLL)**. NLL aims at learning with a dataset of corrupted labels, \(\mathcal{D}_{\mathrm{NLL}}=\{(\mathbf{x}_{i},\hat{y}_{i})\}_{i\in[N]}\). We illustrate the NLL pipeline (in Fig. 2(c)) with the recent sparse over-parameterization (SOP) model [69], where a sparse _noise model_ consisting of parameters \(\mathbf{u}_{i},\mathbf{v}_{i}\in[-1,1]^{C}\) for each sample is adopted. The noise model transforms the network prediction from the true label distribution into the noisy label distribution. A CE loss and a mean-squared-error (MSE) loss optimize parameter \(\{\mathbf{u}_{i}\}\) and \(\{\mathbf{v}_{i}\}\) respectively:

\[\mathcal{L}_{\mathrm{SOP}}=\mathcal{L}_{\mathrm{CE}}\left(\phi\left(\mathbf{p}( y|\mathcal{A}_{\mathrm{w}}(\mathbf{x});\theta)+\mathbf{m}\right),\hat{y} \right)+\mathcal{L}_{\mathrm{MSE}}\left(\mathbf{p}(y|\mathcal{A}_{\mathrm{w}}( \mathbf{x});\theta)+\mathbf{m},\hat{y}\right), \tag{3}\]

where \(\phi\) denotes the \(L_{\infty}\) normalization and \(\mathbf{m}_{i}=\mathbf{u}_{i}\odot\mathbf{u}_{i}\odot\hat{\mathbf{y}}_{i}^{ \mathrm{oh}}-\mathbf{v}_{i}\odot\mathbf{v}_{i}\odot\left(1-\hat{\mathbf{y}}_{i }^{\mathrm{oh}}\right)\), with \(\hat{\mathbf{y}}_{i}^{\mathrm{oh}}\) referring to the one-hot version of \(y_{i}\). Consistency regularization with strong-weak augmentation and entropy class-balance regularization are additionally utilized for better performance in SOP [69].

## 3 Imprecise Label Learning

Although current techniques demonstrate potential in addressing particular forms of imprecise labels, they frequently fall short in adaptability and transferability to more complicated and more realistic scenarios where multiple imprecise label types coexist and interleave. This section first defines the proposed expectation-maximization (EM) formulation for learning with various imprecise labels. Then, we demonstrate that our unified framework seamlessly extends to partial label learning, semi-supervised label learning, noisy label learning, and the more challenging setting of mixed imprecise label learning. Connections and generalization to previous pipelines can also be drawn clearly under the proposed EM framework.

### A Unified Framework for Learning with Imprecise Labels

**Exploiting information from imprecise labels**. The challenge of learning with imprecise labels lies in learning effectively with inaccurate or incomplete annotation information. Per the analysis above, prior works catering to specific individual imprecise labels either explicitly or implicitly attempt to infer the precise labels from the imprecise label information. For example, partial label learning concentrates on the disambiguation of the ground truth label from the label candidates [13; 71; 50] or averaging equally over the label candidates [72]. In semi-supervised learning, after the model initially learns from the labeled data, the pseudo-labels are treated as correct labels and utilized to conduct self-training on the unlabeled data [73; 18]. Similarly, for noisy label learning, an integral part that helps mitigate overfitting to random noise is the implementation of an accurate noise model capable of identifying and rectifying the incorrect labels [33; 69], thereby ensuring the reliability of the learning process. However, inferring the correct labels from the imprecise labels or utilizing the imprecise labels directly can be very challenging and usually leads to errors accumulated during training [73; 74], which is also known as the confirmation bias. In this work, we take a different approach: we consider all possible labeling along with their likelihood that the imprecise labels fulfill to train the model, rather than using a single rectified label from the imprecise information. Such an approach also eliminates the requirements for designing different methods for various imprecise labels and provides a unified formulation instead, where closed-form solutions can be derived.

**A unified framework for learning with imprecise labels (ILL)**. Let \(\{\mathbf{x}_{i}\}_{i\in[N]}\) represent the features as realizations from \(X\) and \(\{y_{i}\}_{i\in[N]}\) represent their precise labels as realizations from \(Y\) for the training data. Ideally, \(Y\) would be fully specified for \(X\). In the imprecise label scenario, however, \(Y\) is not provided; instead we obtain imprecise label information \(I\). We view \(I\) not as _labels_, but more abstractly as a variable representing the _information_ about the labels. From this perspective, the actual labels \(Y\) would have a distribution \(P(Y|I)\), and \(I\) can present in various forms. When the information \(I\) provided is the precise true label of the data, \(P(Y|I)\) would be a delta distribution, taking a value \(1\) at the true label, and \(0\) elsewhere. If \(I\) represents partial labels, then \(P(Y|I)\) would have non-zero value over the candidate labels, and be \(0\) elsewhere. When \(I\) represents a set of noisy labels, \(P(Y|I)\) would represent the distribution of the true labels, given the noisy labels. When \(I\) does not contain any information, i.e., unlabeled data, \(Y\) can take any value.

By the maximum likelihood estimation (MLE) principle, we must estimate the model to maximize the likelihood of the datainformation we have been provided, namely \(X\) and \(I\). Let \(P(X,I;\theta)\) represent a parametric form for the joint distribution of \(X\) and \(I\)4 Explicitly considering the labels \(Y\), we have \(P(X,I;\theta)=\sum_{Y}P(X,Y,I;\theta)\). The maximum likelihood principle requires us to find:

Footnote 4: The actual parameters \(\theta\) may apply only to some component such as \(P(Y|X;\theta)\) of the overall distribution; we will nonetheless tag the entire distribution \(P(X,I;\theta)\) with \(\theta\) to indicate that it is dependent on \(\theta\) overall.

\[\theta^{*}=\operatorname*{arg\,max}_{\theta}\ \log P(X,I;\theta)=\operatorname*{ arg\,max}_{\theta}\ \log\sum_{Y}P(X,Y,I;\theta), \tag{4}\]

with \(\theta^{*}\) denotes the optimal value of \(\theta\). Eq. (4) features the log of an expectation and cannot generally be solved in closed-form, and requires iterative hill-climbing solutions. Of these, arguably the most popular is the expectation-maximization (EM) algorithm [59], which iteratively maximizes a tight variational lower bound on the log-likelihood. In our case, applying it becomes:

\[\begin{split}\theta^{t+1}&=\operatorname*{arg\, max}_{\theta}\operatorname*{\mathbb{E}}_{Y|X,I;\theta^{t}}\left[\log P(X,Y,I; \theta)\right]\\ &=\operatorname*{arg\,max}_{\theta}\operatorname*{\mathbb{E}}_{Y| X,I;\theta^{t}}\left[\log P(Y|X;\theta)+\log P(I|X,Y;\theta)\right],\end{split} \tag{5}\]

where \(\theta^{t}\) is the \(t^{\mathrm{th}}\) estimate of the optimal \(\theta\). Note that \(P(X;\theta)\) is omitted from Eq. (5) since \(P(X)\) does not rely on \(\theta\). The detailed derivation of the variational lower bound is shown in Appendix C.1. There are several implications from Eq. (5). (i) The expectation over the posterior \(P(Y|X,I;\theta^{t})\) equates to considering _all_ labeling entailed by the imprecise label information \(I\), rather than any single (possibly corrected) choice of label. For independent instances setting mostly studied in this paper, we can derive closed-form training objectives from this formulation as shown in Section 3.2. (ii) The property of the second term \(\log P(I|X,Y;\theta)\) is dependent on the nature of imprecise label \(I\). If \(I\) is derivable from true labels \(Y\), such as the actual labels or the label candidates, it can be reduced to \(P(I|Y)\), _i.e._, the probability of \(I\) is no longer dependent on \(X\) or \(\theta\) and thus can be ignored from Eq. (5). If \(I\) represents the noisy labels, \(P(I|X,Y;\theta)\) instead includes a potentially learnable noise model. (iii) It is a general framework towards the unification of any label configuration, including full labels, partial labels, low-resource labels, noisy labels, etc. In this work, we specialize the proposed EM framework to PLL, SSL, NLL, and the mixture of them in the following.

### Instantiating the Unified EM Formulation

We illustrate how to seamlessly expand the formulation from Eq. (5) to partial label learning, semi-supervised learning, noisy label learning, and mixture settings, with derived closed-form loss function5 for each setting here. The actual imprecise labels only affect the manner in which the posterior \(P(Y|X,I;\theta^{t})\) is computed for each setting. We show that all learning objectives derived from Eq. (5) naturally include a consistency term with the posterior as the soft target. We also demonstrate that the proposed unified EM framework closely connects with the prior arts, which reveals the potential reason behind the success of these techniques. Note that while we only demonstrate the application of the proposed framework to four settings here, it can also be flexibly extended to other settings. More details of derivation below are shown in Appendix C.

Footnote 5: To formulate the loss function, we convert the problem to minimization of the negative log-likelihood.

**Partial label learning (PLL)**. The imprecise label \(I\) for partial labels is defined as the label candidate sets \(S\) containing the true labels. These partial labels indicate that the posterior \(P(Y|X,S;\theta^{t})\) can only assign its masses on the candidate labels. Since \(S\) can be derived from true labels \(Y\), \(P(S|X,Y;\theta)\) reduces to \(P(S|Y)\), and thus can be ignored. We also demonstrate with instance dependent partial labels that maintains \(P(S|X,Y;\theta)\) in Appendix D.2.2. Defining the label candidates as \(\{\mathbf{s}_{i}\}_{i\in[N]}\) and substituting it in Eq. (5), we have the loss function of PLL derived using ILL framework:

\[\mathcal{L}_{\mathrm{ILL}}^{\mathrm{PLL}}=-\sum_{Y\in[C]}P(Y|X,S;\theta^{t}) \log P(Y|X;\theta)\equiv\mathcal{L}_{\mathrm{CE}}\left(\mathbf{p}(y|\mathcal{ A}_{\mathbf{s}}(\mathbf{x});\theta),\mathbf{p}(y|\mathcal{A}_{\mathbf{w}}( \mathbf{x}),\mathbf{s};\theta^{t})\right), \tag{6}\]

where \(\mathbf{p}(y|\mathcal{A}_{\mathbf{w}}(\mathbf{x}),\mathbf{s};\theta^{t})\) is the normalized probability that \(\sum_{k\in C}p_{k}=1\), and \(p_{k}=0,\forall k\in\mathbf{s}\). Eq. (6) corresponds exactly to consistency regularization [53], with the normalized predicted probability as the soft pseudo-targets. We use \(\mathcal{A}_{s}\) and \(\mathcal{A}_{w}\) to denote the strong and weak augmentation as stated earlier. This realization on PLL shares similar insights as [12] which exploits a gradually induced loss weight for PLL on multiple augmentations of the data. However, our framework is much simpler and more concise as shown in Appendix D.2.2, which does not require additional techniques.

**Semi-supervised learning (SSL)** In SSL, the input \(X\) consists of the labeled data \(X^{\rm L}\) and the unlabeled data \(X^{\rm U}\). The imprecise label for SSL is realized as the limited number of full labels \(Y^{\rm L}\) for \(X^{\rm L}\). The labels \(Y^{\rm U}\) for unlabeled \(X^{\rm U}\) are unknown and become the latent variable. Interestingly, for the unlabeled data, there is no constraint on possible labels it can take. The posterior \(P(Y^{\rm U}|X^{\rm L},X^{\rm U},Y^{\rm L};\theta)\), which is the actual prediction from the network, can be directly utilized as soft targets for self-training. Since \(Y^{\rm L}\) is conditionally independent with \(Y^{\rm U}\) given \(X\), the second term of Eq. (5): \(P(Y^{\rm L}|X^{\rm L},X^{\rm U},Y^{\rm U};\theta)\), is reduced to \(P(Y^{\rm L}|X^{\rm L};\theta)\), which corresponds to the supervised objective on labeled data. The loss function for SSL thus becomes:

\[\begin{split}\mathcal{L}^{\rm SSL}_{\rm ILLL}&=- \sum_{Y\in[C]}P(Y^{\rm U}|X^{\rm U},X^{\rm L},Y^{\rm L};\theta^{t})\log P(Y^{\rm U }|X^{\rm U},X^{\rm L};\theta)-\log P(Y^{\rm L}|X^{\rm L};\theta)\\ &\equiv\mathcal{L}_{\rm CE}\left(\mathbf{p}(y|\mathcal{A}_{\rm s}( \mathbf{x}^{\rm u});\theta),\mathbf{p}(y|\mathcal{A}_{\rm w}(\mathbf{x}^{\rm u });\theta^{t})\right)+\mathcal{L}_{\rm CE}\left(\mathbf{p}(y|\mathcal{A}_{\rm w }(\mathbf{x}^{\rm l});\theta),y^{\rm l}\right)\end{split} \tag{7}\]

The first term corresponds to the unsupervised consistency regularization usually employed in SSL, and the second term refers to the supervised CE loss only on labeled data. Eq. (7) has several advantages over the previous methods. It adopts the prediction as soft-targets of all possible labeling on unlabeled data, potentially circumventing the confirmation bias caused by pseudo-labeling and naturally utilizing all unlabeled data which resolves the quantity-quality trade-off commonly existing in SSL [18, 23]. It also indicates that previous pseudo-labeling with confidence threshold implicitly conducts the EM optimization, where the maximal probable prediction approximates the expectation, and the degree of the approximation is determined by the threshold \(\tau\), rationalizing the effectiveness of dynamic thresholding.

**Noisy label learning (NLL)**. Things become more complicated here since the noisy labels \(\hat{Y}\) do not directly reveal the true information about \(Y\), thus \(P(\hat{Y}|Y,X;\theta)\) inherently involves a noise model that needs to be learned. We define a simplified instance-independent6 noise transition model \(\mathcal{T}(\hat{Y}|Y;\omega)\) with parameters \(\omega\), and take a slightly different way to formulate the loss function for NLL from the ILL framework:

Footnote 6: A more complicated instance-dependent noise model \(\mathcal{T}(\hat{Y}|Y,X;\omega)\) can also be formulated under our unified framework, but not considered in this work. Also, since we use \(\mathcal{T}\) both in forward fashion and backward fashion, it is unidentifiable in this work.

\[\begin{split}\mathcal{L}^{\rm NLL}_{\rm ILLL}&=- \sum_{Y\in[C]}P(Y|X,\hat{Y};\theta^{t},\omega^{t})\log P(Y|X,\hat{Y};\theta, \omega^{t})-\log P(\hat{Y}|X;\theta,\omega)\\ &\equiv\mathcal{L}_{\rm CE}\left(\mathbf{p}(y|\mathcal{A}_{\rm s }(\mathbf{x}),\hat{y};\theta,\omega^{t}),\mathbf{p}(y|\mathcal{A}_{\rm w}( \mathbf{x}),\hat{y};\theta^{t},\omega^{t})\right)+\mathcal{L}_{\rm CE}\left( \mathbf{p}(\hat{y}|\mathcal{A}_{\rm w}(\mathbf{x});\theta,\omega),\hat{y} \right),\end{split} \tag{8}\]

where the parameters \(\omega\) and \(\theta\) are learned end-to-end. The first term corresponds to the consistency regularization of prediction conditioned on noisy labels and the second term corresponds to the supervised loss on noisy predictions that are converted from the ground truth predictions. Both quantities are computed using the noise transition model given the noisy label \(\hat{y}\):

\[\mathbf{p}(y|\mathbf{x},\hat{y};\theta,\omega^{t})\propto\mathbf{p}(y|\mathbf{ x};\theta)\mathcal{T}(\hat{y}|y;\omega^{t}),\text{and}\;\mathbf{p}(\hat{y}| \mathbf{x};\theta,\omega)=\sum_{y\in[C]}\mathbf{p}(y|\mathbf{x};\theta) \mathcal{T}(\hat{y}|y;\omega). \tag{9}\]

**Mixture imprecise label learning (MILL)**. We additionally consider a more practical setting, mixture of imprecise label learning, with partial labels, noisy labels, and unlabeled data interleaved together. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of SSL as shown in Eq. (7). The labeled data here present partial and noisy labels \(\hat{\mathbf{s}}\). Thus the noisy supervised objective in Eq. (9) becomes the supervised consistency regularization as in Eq. (6) of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels. Thus we have the loss function for MILL derived as:

\[\begin{split}\mathcal{L}^{\rm MILL}_{\rm ILLL}&= \mathcal{L}_{\rm CE}\left(\mathbf{p}\left(y\mid\mathcal{A}_{\rm s}(\mathbf{x} ^{\rm l}),\hat{\mathbf{s}}^{\rm l};\theta,\omega\right),\mathbf{p}\left(y \mid\mathcal{A}_{\rm w}(\mathbf{x}^{\rm l}),\hat{\mathbf{s}}^{\rm l};\theta^{t },\omega^{t}\right)\right)\\ &+\mathcal{L}_{\rm CE}\left(\mathbf{p}\left(\hat{y}\mid\mathcal{A }_{\rm w}(\mathbf{x}^{\rm l});\theta,\omega\right),\hat{\mathbf{s}}^{\rm l} \right)\\ &+\mathcal{L}_{\rm CE}\left(\mathbf{p}(y|\mathcal{A}_{\rm s}( \mathbf{x}^{\rm n});\theta),\mathbf{p}(y|\mathcal{A}_{\rm w}(\mathbf{x}^{\rm u });\theta^{t})\right)\end{split} \tag{10}\]We can compute both quantity through the noise transition model:

\[\mathbf{p}(y|\mathbf{x},\mathbf{\hat{s}};\theta,\omega^{t})\propto\mathbf{p}(y| \mathbf{x};\theta)\prod_{\hat{y}\in\mathbf{\hat{s}}}\mathcal{T}(y|\hat{y};\omega ^{t}),\text{and}\;\mathbf{p}(\hat{y}|\mathbf{x};\theta,\omega)=\sum_{y\in[C]} \mathbf{p}(y|\mathbf{x};\theta)\mathcal{T}(\hat{y}|y;\omega). \tag{11}\]

## 4 Experiments

In this section, we conduct extensive experiments to evaluate ILL. Albeit simple, the ILL framework achieves comparable state-of-the-art performance regarding previous methods on partial label learning, semi-supervised learning, and noisy label learning. Moreover, our experiments show that ILL could be easily extended to a more practical setting with a mixture of various imprecise label configurations. For all settings, we additionally adopt an entropy loss for balancing learned cluster sizes [75; 76], similarly as [69; 22]. Experiments are conducted with three runs using NVIDIA V100 GPUs.

### Partial Label Learning

**Setup**. Following [13], we evaluate our method on partial label learning setting using CIFAR-10 [77], CIFAR-100 [77], and CUB-200 [78]. We generate partially labeled datasets by flipping negative labels to false positive labels with a probability \(q\), denoted as a partial ratio. The \(C-1\) negative labels are then uniformly aggregated into the ground truth label to form a set of label candidates. We consider \(q\in\{0.1,0.3,0.5\}\) for CIFAR-10, \(q\in\{0.01,0.05,0.1\}\) for CIFAR-100, and \(q=0.05\) for CUB-200. We choose six baselines for PLL using ResNet-18 [1]: LWS [11], PRODEN [55], CC [9], MSE and EXP [79], and PiCO [13]. The detailed hyper-parameters, comparison with the more recent method R-CR [12] that utilizes a different training recipe and model [80], and comparison with instance-dependent partial labels [81] are shown in Appendix D.2.2.

**Results**. The results for PLL are shown in Table 1. Our method achieves the best performance compared to the baseline methods. Perhaps more surprisingly, on CIFAR-10 and CIFAR-100, our method even outperforms the fully-supervised reference, indicating the potential better generalization capability using the proposed framework, sharing similar insights as in Wu et al. [12]. While PiCO adopts a contrastive learning objective, our method still surpasses PiCO by an average of \(\mathbf{2.13}\%\) on CIFAR-10 and \(\mathbf{2.72}\%\) on CIFAR-100. Our approach can be further enhanced by incorporating contrastive learning objectives, potentially leading to more significant performance.

### Semi-Supervised Learning

**Setup**. For experiments of SSL, we follow the training and evaluation protocols of USB [82] on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select \(l/C\) samples from each class and treat the remaining samples as the unlabeled dataset. We present the results on CIFAR-100 and STL-10 [77] for image classification, and IMDB [83] and Amazon Review [84] for text classification. We compare with the current methods with confidence thresholding, such as FixMatch [18], AdaMatch [85], FlexMatch [20], FreeMatch [22], and SoftMatch [23]. We also compare with methods with the contrastive loss, CoMatch [86] and SimMatch [87]. A full comparison of the USB datasets and hyper-parameters is shown in Appendix D.3.

**Results**. We present the results for SSL on Table 2. Although no individual SSL algorithm dominates the USB benchmark [82], our method still shows competitive performance. Notably, our method

\begin{table}
\begin{tabular}{l|c c c|c c|c} \hline \hline Dataset & \multicolumn{3}{c|}{CIFAR-10} & \multicolumn{3}{c|}{CIFAR-100} & \multicolumn{1}{c}{CUB-200} \\ \hline Partial Ratio \(q\) & 0.1 & 0.3 & 0.5 & 0.01 & 0.05 & 0.1 & 0.05 \\ \hline Fully-Supervised & \multicolumn{3}{c|}{94.91\(\pm\)0.07} & \multicolumn{3}{c|}{73.56\(\pm\)10.10} & \multicolumn{1}{c}{-} \\ \hline LWS [11] & 90.30\(\pm\)0.80 & 88.99\(\pm\)1.43 & 86.16\(\pm\)0.85 & 65.78\(\pm\)0.02 & 59.56\(\pm\)0.33 & 53.53\(\pm\)0.08 & 39.74\(\pm\)0.47 \\ PRODEN [55] & 90.24\(\pm\)0.32 & 89.38\(\pm\)0.31 & 87.78\(\pm\)0.07 & 62.60\(\pm\)0.02 & 60.73\(\pm\)0.03 & 56.80\(\pm\)0.29 & 62.56\(\pm\)0.10 \\ CC [9] & 82.30\(\pm\)0.21 & 79.08\(\pm\)0.47 & 74.05\(\pm\)0.35 & 49.76\(\pm\)0.45 & 47.62\(\pm\)0.08 & 35.72\(\pm\)0.42 & 55.51\(\pm\)0.02 \\ MSE [79] & 79.97\(\pm\)0.48 & 75.65\(\pm\)0.26 & 67.09\(\pm\)0.06 & 49.17\(\pm\)0.40 & 66.02\(\pm\)1.42 & 43.81\(\pm\)0.19 & 22.07\(\pm\)2.36 \\ EXP [79] & 79.23\(\pm\)0.10 & 75.79\(\pm\)0.21 & 70.34\(\pm\)0.32 & 44.45\(\pm\)1.50 & 41.05\(\pm\)1.40 & 29.27\(\pm\)2.81 & 9.44\(\pm\)2.32 \\ PiCO [13] & 94.39\(\pm\)0.18 & 94.18\(\pm\)0.12 & 92.58\(\pm\)0.06 & 73.09\(\pm\)0.34 & 72.74\(\pm\)0.30 & 69.91\(\pm\)0.24 & **72.17\(\pm\)0.72** \\ \hline Ours & **96.37\(\pm\)0.08** & **96.26\(\pm\)0.03** & **95.16\(\pm\)0.05** & **75.31\(\pm\)0.19** & **74.58\(\pm\)0.03** & **74.00\(\pm\)0.02** & 70.77\(\pm\)0.29 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Accuracy of different partial ratio \(q\) on CIFAR-10, CIFAR-100, and CUB-200 for **partial label learning**. The best and the second best results are indicated in **bold** and underline respectively.

performs best on STL-10 with 40 labels and Amazon Review with 250 labels, outperforming the previous best by \(\mathbf{0.68}\%\) and \(\mathbf{1.33}\%\). In the other settings, the performance of our method is also very close to the best-performing methods. More remarkably, our method does not employ any thresholding, re-weighting, or contrastive techniques to achieve current results, demonstrating a significant potential to be further explored.

### Noisy Label Learning

Setup.We conduct the experiments of NLL following SOP [69] on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M [88], and WebVision [89]. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \(\eta\) into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. The introduced noise is then treated as ground truth labels to train the model. We mainly select three previous best methods as baselines: DivideMix [33]; ELR [32]; and SOP [69]. We also include the normal cross-entropy (CE) training and mixup [90] as baselines. More comparisons of other methods [91; 28] and on CIFAR-10N [92] with training details and more baselines [93; 28] are shown in Appendix D.4.

Results.We present the noisy label learning results in Table 3. The proposed method is comparable to the previous best methods. On synthetic noise of CIFAR-10, our method demonstrates the best performance on both symmetric noise and asymmetric noise. On CIFAR-100, our method generally produces similar results comparable to SOP. One may notice that our method shows inferior performance on asymmetric noise of CIFAR-100; we argue this is mainly due to the oversimplification of the noise transition model. Our method also achieves the best results on WebVision, outperforming the previous best by \(\mathbf{2.05}\%\). On Clothing1M, our results are also very close to DivideMix, which trains for 80 epochs compared to 10 epochs in ours.

### Mixed Imprecise Label Learning

Setup.We evaluate on CIFAR-10 and CIFAR-100 in a more challenging and realistic setting, the mixture of various imprecise label configurations, with unlabeled, partially labeled, and noisy labeled data existing simultaneously. We first sample the labeled dataset and treat other samples as the unlabeled. On the labeled dataset, we generate partial labels and randomly corrupt the true label of the partial labels. We set \(l\in\{1000,5000,50000\}\) for CIFAR-10, and \(l\in\{5000,10000,50000\}\) for

\begin{table}
\begin{tabular}{l|c c|c|c|c|c|c} \hline \hline Datasets & CIFAR-100 & STL-10 & IMDB & Amazon Review \\ \hline \# Labels \(l\) & 200 & 400 & 40 & 100 & 20 & 100 & 250 & 1000 \\ \hline AdaMatch [85] & 22.32±1.73 & 16.66±0.62 & 13.64±2.49 & 7.62±1.90 & 8.09±0.99 & 7.11±0.20 & 45.40±0.96 & **40.16±0.49** \\ FixMatch [18] & 29.60±0.90 & 19.56±0.52 & 16.15±1.89 & 8.11±0.68 & 7.22±3.33 & 7.33±0.13 & 47.61±0.83 & 43.05±0.54 \\ FlexMatch [20] & 26.76±1.12 & 18.24±0.36 & 14.40±3.11 & 8.17±0.78 & 7.82±0.77 & 7.41±0.38 & 45.73±1.60 & 42.25±0.33 \\ CoMatch [86] & 35.08±0.69 & 25.35±0.50 & 15.12±1.88 & 9.56±1.35 & 44.40±3.70 & 7.72±1.14 & 48.76±0.90 & 43.63±0.21 \\ SimMatch [87] & 23.78±1.08 & 17.06±0.18 & 17.74±3.20 & **7.55±1.52** & 7.93±0.55 & **7.08±0.33** & 45.91±0.95 & 42.21±0.30 \\ FreeMatch [22] & **21.40±0.30** & **15.65±0.26** & 12.73±3.22 & 8.52±0.53 & 8.94±0.21 & 7.59±0.45 & 46.41±0.60 & 42.64±0.06 \\ SoftMatch [23] & 22.67±1.32 & 16.84±0.66 & 13.55±3.16 & 7.84±1.72 & 7.76±0.58 & 7.97±0.72 & 45.29±0.92 & 42.21±0.30 \\ \hline Ours & 22.06±1.06 & 16.40±0.54 & **11.09±0.71** & 8.10±1.02 & **7.32±0.12** & 7.64±0.67 & **43.96±0.32** & 42.32±0.02 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Error rate of different number of labels \(l\) on CIFAR-100, STL-10, IMDB, and Amazon Review datasets for **semi-supervised learning**.

\begin{table}
\begin{tabular}{l|c c c|c c c|c|c} \hline \hline Dataset & \multicolumn{3}{c|}{CIFAR-10} & \multicolumn{3}{c|}{CIFAR-100} & \multicolumn{2}{c|}{Clothing1M} & WebVision \\ \hline Noise Type & \multicolumn{3}{c|}{Sym.} & \multicolumn{3}{c|}{Asym.} & \multicolumn{3}{c|}{Sym.} & \multicolumn{1}{c|}{Asym.} & \multicolumn{1}{c|}{Ins.} & \multicolumn{1}{c}{Ins.} \\ \hline Noise Ratio \(\eta\) & 0.2 & 0.5 & 0.8 & 0.4 & 0.2 & 0.5 & 0.8 & 0.4 & - & - \\ \hline CE & 87.20 & 80.70 & 65.80 & 82.20 & 58.10 & 47.10 & 23.80 & 43.30 & 69.10 & - \\ Mixup [90] & 93.50 & 87.90 & 72.30 & & 69.90 & 57.30 & 33.60 & 33.60 & - \\ DivideMix [33] & 96.10 & 94.60 & 93.20 & 93.40 & 77.10 & 74.60 & 60.20 & 72.10 & **74.26** & 77.32 \\ ELR [32] & 95.80 & 94.80 & 93.30 & 93.00 & 77.20 & 73.80 & 60.80 & 77.50 & 72.90 & 76.20 \\ SOP [69] & 96.30 & 95.50 & 94.00 & 93.80 & **7.88±0** & **7.59** & 63.30 & **78.00** & 73.50 & 76.60 \\ \hline Ours & **96.78±1.01** & **96.60±1.35** & **94.31±0.87** & **94.75±0CIFAR-100. For partial labels, we set \(q\in\{0.1,0.3,0.5\}\) for CIFAR-10, and \(q\in\{0.01,0.05,0.1\}\) for CIFAR-100. For noisy labels, we set \(\eta\in\{0,0.1,0.2,0.3\}\) for both datasets. Since there is no prior work that can handle all settings all at once, we compare on partial noisy label learning with PiCO+ [94], IRNet [71], and DALI [50]. Although there are also prior efforts on partial semi-supervised learning [51; 52], they do not scale on simple dataset even on CIFAR-10. Thus, we did not include them in comparison. We conduct additional validation of our method on more complex settings for partial noisy labels with unlabeled data to demonstrate its robustness to various imprecise labels.

**Results**. We report the comparison with partial noisy label learning methods in Table 4. Compared to previous methods, the proposed method achieves the best performance. Despite the simplicity, our method outperforms PiCO+ and DALI with mixup, showing the effectiveness of dealing with mixed imprecise labels. We also report the results of our methods on more mixed imprecise label configurations in Table 5. Our method demonstrates significant robustness against various settings of the size of labeled data, partial ratio, and noise ratio. Note that this is the first work that naturally deals with all three imprecise label configurations simultaneously, with superior performance than previous methods handling specific types or combinations of label configurations. This indicates the enormous potential of our work in realistic applications for handling more practical and complicated data annotations common in real world applications.

## 5 Conclusion

We present the imprecise label learning (ILL) framework, a unified and consolidated solution for learning from all types of imprecise labels. ILL effectively employs an expectation-maximization (EM) algorithm for maximum likelihood estimation (MLE) of the distribution over the latent ground truth labels \(Y\), imprecise label information \(I\), and data \(X\). It naturally extends and encompasses previous formulations for various imprecise label settings, achieving promising results. Notably, in scenarios where mixed configurations of imprecise labels coexist, our method exhibits substantial robustness against diverse forms of label imprecision. The potential **broader impact** of the ILL framework is substantial. It stands poised to transform domains where obtaining precise labels poses a challenge, offering a simple, unified, and effective approach to such contexts. Beyond the three imprecise label configurations we have demonstrated in this study, the ILL framework shows promise for an extension to more intricate scenarios such as multi-instance learning [42] and multi-label crowd-sourcing learning [38]. However, it is also crucial to acknowledge the **limitations** of the ILL

\begin{table}
\begin{tabular}{l|c|c c c|c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{\(q\)} & \multicolumn{3}{c|}{CIFAR-10, \(l\)=50000} & \multirow{2}{*}{\(q\)} & \multicolumn{3}{c}{CIFAR-100, \(l\)=50000} \\  & & \(\eta\)=0.1 & \(\eta\)=0.2 & \(\eta\)=0.3 & & \(\eta\)=0.1 & \(\eta\)=0.2 & \(\eta\)=0.3 \\ \hline PiCO+ [94] & & 93.64 & 93.13 & 92.18 & & 71.42 & 70.22 & 66.14 \\ IRNet [71] & & 93.44 & 92.57 & 92.38 & & 71.17 & 70.10 & 68.77 \\ DALI [50] & & 94.15 & 94.04 & 93.77 & & 72.26 & 71.98 & 71.04 \\ PiCO+ Mixup [50] & & 94.58 & 94.74 & 94.43 & & 70.54 & 74.31 & 71.79 \\ DALI Mixup [50] & & 95.83 & 95.86 & 95.75 & & 76.52 & 76.55 & 76.09 \\ Ours & & **96.47fa.1** & **96.09fa.20** & **95.83s.84*** & **77.53a.24** & **76.96a.04** & **76.43a.02** \\ \hline PiCO+ [94] & & 92.32 & 92.22 & 89.95 & & 69.40 & 66.67 & 62.24 \\ IRNet [71] & & 92.81 & 92.18 & 91.35 & & 70.73 & 69.33 & 68.09 \\ DALI [50] & & 93.44 & 93.25 & 92.42 & & 72.28 & 71.35 & 70.05 \\ PiCO+ Mixup [50] & & 94.02 & 94.03 & 92.94 & & 70.5 & 73.33 & 67.56 \\ DALI Mixup [50] & & 95.52 & 95.41 & 94.67 & & 76.87 & 75.23 & 74.49 \\ Ours & & **96.2a.02** & **95.87fa.14** & **95.22a.06** & & **77.07fa.16** & **76.34a.08** & **75.13a.63** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Accuracy comparison of **mixture of different imprecise labels**. We report results of full labels, partial ratio \(q\) of 0.1 (0.01) and 0.3 (0.05) for CIFAR-10 (CIFAR-100), and noise ratio \(\eta\) of 0.1, 0.2, and 0.3 for CIFAR-10 and CIFAR-100.

\begin{table}
\begin{tabular}{l|c|c c c c|c c c c} \hline \hline \multirow{2}{*}{\(l\)} & \multirow{2}{*}{\(q\)} & \multicolumn{3}{c|}{CIFAR10} & \multirow{2}{*}{\(l\)} & \multirow{2}{*}{\(q\)} & \multicolumn{3}{c}{CIFAR100} \\  & & & & & & & & \(\eta\)=0.0 & \(\eta\)=0.1 & \(\eta\)=0.2 & \(\eta\)=0.3 \\ \hline \multirow{4}{*}{5,000} & 0.1 & 95.29fa.08 & 93.90fa.01 & 92.02a.02a.08 & 89.02a.08 & 0.01 & 69.90fa.23 & 68.74fa.163 & 66.87fa.34 & 65.34fa.02 \\  & 0.3 & 95.13a.06 & 92.95a.07 & 90.14a.06 & 87.31a.02 & 10,000 & 0.05 & 69.58fa.50.23 & 68.08fa.02 & 66.78fa.40 & 64.83a.17 \\  & 0.5 & 95.04a.01 & 92.18a.52 & 88.39fa.08 & 83.09fa.08 & & 0.10 & 68.92a.04 & 67.15a.05 & 64.44a.12 & 60.26fa.36 \\ \hline \multirow{4}{*}{1,000} & 0.1 & 94.48a.09 & 91.68a.07 & 87.17a.05 & 81.04a.13 & & 0.01 & 65.66a.02 & 63.13a.207 & 60.93fa.56 & 53.86a.06 \\  & 0.3 & 94.35a.08 & 99.84a.00 & 82.66a.15 & 69.02a.16 & 5,000 & 0.05 & 65.60fa.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.00.0.00.00.0.00.00.0.00.00.00.0.00.00.00.00.00.0.00.00.0.00.00.00.0.00.00.0.00.0.00.00.0.00.00.0.00.00.0.00.00.00.00.00.0.00.00.00.0.00.00.

framework. Although its effectiveness has been substantiated on relatively smaller-scale datasets, additional empirical validation is necessary to assess its scalability to larger datasets. Furthermore, our study only considers balanced datasets; thus, the performance of the ILL framework when dealing with imbalanced data and open-set data still remains an open area for future exploration. We hope that our study will constitute a significant stride towards a comprehensive solution for imprecise label learning and catalyze further research in this crucial field.

## Acknowledge

Masashi Sugiyama was supported by the Institute for AI and Beyond, UTokyo.

## References

* [1]K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778. Cited by: SS1, SS2.
* [2]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin (2017) Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS)30. Cited by: SS1, SS2.
* [3]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018) Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Cited by: SS1, SS2.
* [4]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. (2020) An image is worth 16x16 words: transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Cited by: SS1, SS2.
* [5]A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), pp. 8748-8763. Cited by: SS1, SS2.
* [6]OpenAI (2020) Gpt-4 technical report (2020). Cited by: SS1, SS2.
* [7]T. Cour, B. Sapp, and B. Taskar (2011) Learning from partial labels. The Journal of Machine Learning Research12, pp. 1501-1536. Cited by: SS1, SS2.
* [8]J. Luo and F. Orabona (2010) Learning from candidate labeling sets. Advances in Neural Information Processing Systems (NeurIPS). Cited by: SS1, SS2.
* [9]L. Feng, J. Lv, B. Han, M. Xu, G. Niu, X. Geng, B. An, and M. Sugiyama (2020) Provably consistent partial-label learning. ArXivabs/2007.08929. Cited by: SS1, SS2.
* [10]D. Wang, M. Zhang, and L. Li (2019) Adaptive graph guided disambiguation for partial label learning. IEEE Transactions on Pattern Analysis and Machine Intelligence44, pp. 8796-8811. Cited by: SS1, SS2.
* [11]H. Wen, J. Cui, H. Hang, J. Liu, Y. Wang, and Z. Lin (2021) Leveraged weighted loss for partial label learning. In Proceedings of the International Conference on Machine Learning (ICML), pp. 11091-11100. Cited by: SS1, SS2.
* [12]D. Wu, D. Wang, and M. Zhang (2019) Revisiting consistency regularization for deep partial label learning. In Proceedings of the International Conference on Machine Learning (ICML), pp. 162 of. Cited by: SS1, SS2.
* [13]H. Wang, R. Xiao, Y. Li, L. Feng, G. Niu, G. Chen, and J. Zhao (2022) P1CO: contrastive label disambiguation for partial label learning. In International Conference on Learning Representations (ICLR), Cited by: SS1, SS2.
* [14]D. Lee et al. (2013) Pseudo-label: the simple and efficient semi-supervised learning method for deep neural networks. In Workshop on challenges in representation learning, ICML, pp. 896. Cited by: SS1, SS2.
* [15]L. Samuli and A. Timo (2017) Temporal ensembling for semi-supervised learning. In International Conference on Learning Representations (ICLR), Cited by: SS1, SS2.
* [16]D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel (2019) MixMatch: a holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems (NeurIPS)32. Cited by: SS1, SS2.

* [17] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In _International Conference on Learning Representations (ICLR)_, 2019.
* [18] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. _Advances in Neural Information Processing Systems (NeurIPS)_, 33, 2020.
* [19] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10687-10698, 2020.
* [20] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. _Advances in Neural Information Processing Systems (NeurIPS)_, 34, 2021.
* [21] Xudong Wang, Zhirong Wu, Long Lian, and Stella X Yu. Debiased learning from naturally imbalanced pseudo-labels. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR)_, pages 14647-14657, 2022.
* [22] Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan,, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, and Xing Xie. Freematch: Self-adaptive thresholding for semi-supervised learning. In _International Conference on Learning Representations (ICLR)_, 2023.
* [23] Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, and Marios Savvides. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. In _International Conference on Learning Representations (ICLR)_, 2023.
* [24] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2691-2699, 2015.
* [25] Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In _2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 2682-2686, 2016.
* [26] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In _International Conference on Learning Representations (ICLR)_, 2016.
* [27] Aritra Ghosh, Himanshu Kumar, and P. Shanti Sastry. Robust loss functions under label noise for deep neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, 2017.
* [28] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Wai-Hung Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* [29] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. _Advances in Neural Information Processing Systems (NeurIPS)_, 31, 2018.
* [30] Junnan Li, Yongkang Wong, Qi Zhao, and M. Kankanhalli. Learning to learn from noisy labeled data. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5046-5054, 2018.
* [31] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for robust learning with noisy labels. _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 322-330, 2019.

* [32] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. _Advances in Neural Information Processing Systems (NeurIPS)_, 33, 2020.
* [33] Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. In _International Conference on Learning Representations (ICLR)_, 2020.
* [34] Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Monazam Erfani, and James Bailey. Normalized loss functions for deep learning with noisy labels. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2020.
* [35] Qi Wei, Haoliang Sun, Xiankai Lu, and Yilong Yin. Self-filtering: A noise-aware sample selection for label noise with confidence penalization. In _European Conference on Computer Vision_, pages 516-532. Springer, 2022.
* [36] Yivan Zhang, Gang Niu, and Masashi Sugiyama. Learning noise transition matrix from only noisy labels via total variation regularization. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2021.
* [37] Qi Wei, Lei Feng, Haoliang Sun, Ren Wang, Chenhui Guo, and Yilong Yin. Fine-grained classification with noisy labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11651-11660, 2023.
* [38] Shahana Ibrahim, Tri Nguyen, and Xiao Fu. Deep learning from crowdsourced labels: Coupled cross-entropy minimization, identifiability, and regularization. In _International Conference on Learning Representations (ICLR)_, 2023.
* [39] Jiaheng Wei, Zhaowei Zhu, Tianyi Luo, Ehsan Amid, Abhishek Kumar, and Yang Liu. To aggregate or not? learning with separate noisy labels. _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, Aug 2023.
* [40] Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner. A survey on programmatic weak supervision, 2022.
* [41] Renzhi Wu, Shen-En Chen, Jieyu Zhang, and Xu Chu. Learning hyper label model for programmatic weak supervision. In _International Conference on Learning Representations (ICLR)_, 2023.
* [42] Maximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning. In _International Conference on Machine Learning (ICML)_, pages 2127-2136. PMLR, 2018.
* [43] Nan Lu, Gang Niu, Aditya Krishna Menon, and Masashi Sugiyama. On the minimal supervision for training any binary classifier from only unlabeled data. _arXiv preprint arXiv:1808.10585_, 2018.
* [44] Clayton Scott and Jianxin Zhang. Learning from label proportions: A mutual contamination framework. _Advances in Neural Information Processing Systems (NeurIPS)_, 33:22256-22267, 2020.
* [45] Y. Zhang, N. Charoenphakdee, Z. Wu, and M. Sugiyama. Learning from aggregate observations. pages 7993-8005, 2020.
* [46] Saurabh Garg, Yifan Wu, Alex Smola, Sivaraman Balakrishnan, and Zachary C. Lipton. Mixture proportion estimation and pu learning: A modern approach, 2021.
* [47] Lei Feng, Senlin Shu, Nan Lu, Bo Han, Miao Xu, Gang Niu, Bo An, and Masashi Sugiyama. Pointwise binary classification with pairwise confidence comparisons. In _International Conference on Machine Learning_, pages 3252-3262. PMLR, 2021.
* [48] Andrea Campagner. Learnability in "learning from fuzzy labels". In _2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)_, pages 1-6. IEEE, 2021.

* [49] Zheng Lian, Mingyu Xu, Lan Chen, Licai Sun, Bin Liu, and Jianhua Tao. Arnet: Automatic refinement network for noisy partial label learning. _arXiv preprint arXiv:2211.04774_, 2022.
* [50] Mingyu Xu, Zheng Lian, Lei Feng, Bin Liu, and Jianhua Tao. Dali: Dynamically adjusted label importance for noisy partial label learning, 2023.
* [51] Qian-Wei Wang, Yu-Feng Li, and Zhi-Hua Zhou. Partial label learning with unlabeled data. In _IJCAI_, pages 3755-3761, 2019.
* [52] Wei Wang and Min-Ling Zhang. Semi-supervised partial label learning via confidence-rated margin maximization. _Advances in Neural Information Processing Systems (NeurIPS)_, 33:6982-6993, 2020.
* [53] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. _Advances in Neural Information Processing Systems (NeurIPS)_, 33, 2020.
* [54] Yu Yao, Tongliang Liu, Mingming Gong, Bo Han, Gang Niu, and Kun Zhang. Instance-dependent label-noise learning under a structural causal model. _Advances in Neural Information Processing Systems_, 34:4409-4420, 2021.
* [55] Jiaqi Lv, Miao Xu, Lei Feng, Gang Niu, Xin Geng, and Masashi Sugiyama. Progressive identification of true labels for partial-label learning. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 6500-6510. PMLR, 2020.
* [56] Chidubem Arachie and Bert Huang. Constrained labeling for weakly supervised learning. In _Uncertainty in Artificial Intelligence_, pages 236-246. PMLR, 2021.
* [57] Eyke Hullermeier and Weiwei Cheng. Superset learning based on generalized loss minimization. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Porto, Portugal, September 7-11, 2015, Proceedings, Part II 15_, pages 260-275. Springer, 2015.
* [58] Jiaqi Lv, Biao Liu, Lei Feng, Ning Xu, Miao Xu, Bo An, Gang Niu, Xin Geng, and Masashi Sugiyama. On the robustness of average losses for partial-label learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, page 1-15, 2023.
* [59] Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. _Journal of the royal statistical society: series B (methodological)_, 39(1):1-22, 1977.
* [60] Thierry Denceux. Maximum likelihood estimation from fuzzy data using the em algorithm. _Fuzzy sets and systems_, 183(1):72-91, 2011.
* [61] Eyke Hullermeier. Learning from imprecise and fuzzy observations: Data disambiguation through generalized loss minimization. _International Journal of Approximate Reasoning_, 55(7):1519-1534, 2014.
* [62] Benjamin Quost and Thierry Denoeux. Clustering and classification of fuzzy data using the fuzzy em algorithm. _Fuzzy Sets and Systems_, 286:134-156, 2016.
* [63] Brendan Van Rooyen and Robert C Williamson. A theory of learning with corrupted labels. _J. Mach. Learn. Res._, 18(1):8501-8550, 2017.
* [64] Chen Gong, Jian Yang, Jane You, and Masashi Sugiyama. Centroid estimation with guaranteed efficiency: A general framework for weakly supervised learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(6):2841-2855, 2020.
* [65] Chao-Kai Chiang and Masashi Sugiyama. Unified risk analysis for weakly supervised learning. _arXiv preprint arXiv:2309.08216_, 2023.
* [66] Zixi Wei, Lei Feng, Bo Han, Tongliang Liu, Gang Niu, Xiaofeng Zhu, and Heng Tao Shen. A universal unbiased method for classification from aggregate observations. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 36804-36820. PMLR, 23-29 Jul 2023.

* [67] Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, and Zhi-Hua Zhou. Weakly supervised auc optimization: A unified partial auc approach. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2024.
* [68] Andrea Campagner. Learning from fuzzy labels: Theoretical issues and algorithmic solutions. _International Journal of Approximate Reasoning_, page 108969, 2023.
* [69] Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-parameterization. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the International Conference on Machine Learning (ICML)_, volume 162 of _Proceedings of Machine Learning Research_, pages 14153-14172. PMLR, 17-23 Jul 2022.
* [70] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. _Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, Jun 2020.
* [71] Zheng Lian, Mingyu Xu, Lan Chen, Licai Sun, Bin Liu, and Jianhua Tao. Irnet: Iterative refinement network for noisy partial label learning, 2022.
* [72] Eyke Hullermeier and Jurgen Beringer. Learning from ambiguously labeled examples. _Intelligent Data Analysis_, 10(5):419-439, 2006.
* [73] Eric Arazo, Diego Ortego, Paul Albert, Noel E O'Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In _2020 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2020.
* [74] Baixu Chen, Junguang Jiang, Ximei Wang, Jianmin Wang, and Mingsheng Long. Debiased pseudo labeling in self-training. _arXiv preprint arXiv:2202.07136_, 2022.
* [75] David MacKay John Bridle, Anthony Heading. Unsupervised classifiers, mutual information and 'phantom targets. _Advances in Neural Information Processing Systems (NeurIPS)_, 1991.
* [76] Armand Joulin and Francis Bach. A convex relaxation for weakly supervised classifiers. In _Proceedings of the International Conference on Machine Learning (ICML)_. PMLR, 2012.
* [77] Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
* [78] Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010.
* [79] Lei Feng, Takuo Kaneko, Bo Han, Gang Niu, Bo An, and Masashi Sugiyama. Learning with multiple complementary labels. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 3072-3081. PMLR, 2020.
* [80] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In _British Machine Vision Conference (BMVC)_. British Machine Vision Association, 2016.
* [81] Ning Xu, Congyu Qiao, Xin Geng, and Min-Ling Zhang. Instance-dependent partial label learning. _Advances in Neural Information Processing Systems_, 34:27119-27130, 2021.
* [82] Yidong Wang, Hao Chen, Yue Fan, Wang Sun, Ran Tao, Wenxin Hou, Renjie Wang, Linyi Yang, Zhi Zhou, Lan-Zhe Guo, Heli Qi, Zhen Wu, Yu-Feng Li, Satoshi Nakamura, Wei Ye, Marios Savvides, Bhiksha Raj, Takahiro Shinozaki, Bernt Schiele, Jindong Wang, Xing Xie, and Yue Zhang. Usb: A unified semi-supervised learning benchmark. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [83] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies_, pages 142-150, 2011.
* [84] Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In _Proceedings of the 7th ACM conference on Recommender systems_, pages 165-172, 2013.

* [85] David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, and Alex Kurakin. Adamatch: A unified approach to semi-supervised learning and domain adaptation. _International Conference on Learning Representations (ICLR)_, 2021.
* [86] Junnan Li, Caiming Xiong, and Steven CH Hoi. Comatch: Semi-supervised learning with contrastive graph regularization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR)_, pages 9475-9484, 2021.
* [87] Mingkai Zheng, Shan You, Lang Huang, Fei Wang, Chen Qian, and Chang Xu. Simmatch: Semi-supervised learning with similarity matching. _arXiv preprint arXiv:2203.06915_, 2022.
* [88] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 2691-2699, 2015.
* [89] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual learning and understanding from web data, 2017.
* [90] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* [91] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2233-2241, 2016.
* [92] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. _arXiv preprint arXiv:2110.12088_, 2021.
* [93] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In _International conference on machine learning_, pages 2304-2313. PMLR, 2018.
* [94] Haobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, and Junbo Zhao. Pico+: Contrastive label disambiguation for robust partial label learning, 2022.
* [95] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPR)_, pages 702-703, 2020.
* [96] Min-Ling Zhang and Fei Yu. Solving the partial label learning problem: An instance-based approach. In _IJCAI_, pages 4048-4054, 2015.
* [97] Chen Gong, Tongliang Liu, Yuanyan Tang, Jian Yang, Jie Yang, and Dacheng Tao. A regularization approach for instance-based superset label learning. _IEEE transactions on cybernetics_, 48(3):967-978, 2017.
* [98] Ning Xu, Jiaqi Lv, and Xin Geng. Partial label learning via label enhancement. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 5557-5564, 2019.
* [99] Zhenguo Wu, Jiaqi Lv, and Masashi Sugiyama. Learning with proper partial labels. _Neural Computation_, 35(1):58-81, Jan 2023.
* [100] Nam Nguyen and Rich Caruana. Classification with partial labels. In _Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 551-559, 2008.
* [101] Min-Ling Zhang, Bin-Bin Zhou, and Xu-Ying Liu. Partial label learning via feature-aware disambiguation. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 1335-1344, 2016.
* [102] Liping Liu and Thomas Dietterich. A conditional multinomial mixture model for superset label learning. _Advances in Neural Information Processing Systems (NeurIPS)_, 25, 2012.

* [103] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. _Advances in Neural Information Processing Systems (NeurIPS)_, 30, 2017.
* [104] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. _IEEE transactions on Pattern Analysis and Machine Intelligence_, 41(8):1979-1993, 2018.
* [105] Yi Xu, Lei Shang, Jinxing Ye, Qi Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong Jin. Dash: Semi-supervised learning with dynamic thresholding. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 11525-11536. PMLR, 2021.
* [106] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPR)_, 2019.
* [107] Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le. Meta pseudo labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 11557-11568, 2021.
* [108] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization, 2016.
* [109] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* [110] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. _IEEE Transactions on Neural Networks and Learning Systems_, page 1-19, 2022.
* [111] Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Normalized loss functions for deep learning with noisy labels. In _Proceedings of the International Conference on Machine Learning (ICML)_, 2020.
* [112] Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discriminative representations via the principle of maximal coding rate reduction, 2020.
* [113] Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. In _IEEE Transactions on pattern analysis and machine intelligence_, pages 447-461, 2016.
* [114] Curtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in dataset labels. _Journal of Artificial Intelligence Research_, 70:1373-1411, Apr 2021.
* [115] Yivan Zhang, Gang Niu, and Masashi Sugiyama. Learning noise transition matrix from only noisy labels via total variation regularization. In _Proceedings of the International Conference on Machine Learning (ICML)_, pages 12501-12512. PMLR, 2021.
* [116] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understanding and improving early stopping for learning with noisy labels, 2021.
* [117] Junnan Li, Caiming Xiong, and Steven CH Hoi. Learning from noisy data with robust representation learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR)_, pages 9485-9494, 2021.
* [118] Shikun Li, Xiaobo Xia, Shiming Ge, and Tongliang Liu. Selective-supervised contrastive learning with noisy labels. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, Jun 2022.
* [119] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework for learning with noisy labels. _2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, Jun 2018.

* [120] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, Jun 2019.
* [121] Hwanjun Song, Minseok Kim, and Jae-Gil Lee. SELFIE: Refurbishing unclean samples for robust deep learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the International Conference on Machine Learning (ICML)_, volume 97 of _Proceedings of Machine Learning Research_, pages 5907-5915. PMLR, 09-15 Jun 2019.
* [122] Eric Arazo, Diego Ortego, Paul Albert, Noel E. O'Connor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction, 2019.
* [123] Vinay Shukla, Zhe Zeng, Kareem Ahmed, and Guy Van den Broeck. A unified approach to count-based weakly-supervised learning. In _ICML 2023 Workshop on Differentiable Almost Everything: Differentiable Relaxations, Algorithms, Operators, and Simulators_, jul 2023.
* [124] Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression. In _ECAI_, volume 2, page 11, 2002.
* [125] Ning Xu, Jiaqi Lv, Biao Liu, Congyu Qiao, and Xin Geng. Progressive purification for instance-dependent partial label learning, 2022.
* [126] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 248-255, 2009.
* [127] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, 2019.
* [128] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 215-223. JMLR Workshop and Conference Proceedings, 2011.
* [129] Jiancheng Yang, Rui Shi, and Bingbing Ni. Medmnist classification decathlon: A lightweight automl benchmark for medical image analysis. In _IEEE 18th International Symposium on Biomedical Imaging (ISBI)_, pages 191-195, 2021.
* [130] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classification. _arXiv preprint arXiv:2110.14795_, 2021.
* [131] Jong-Chyi Su and Subhransu Maji. The semi-supervised inaturalist-aves challenge at fgvc7 workshop. _arXiv preprint arXiv:2103.06937_, 2021.
* [132] Yelp dataset: [http://www.yelp.com/dataset_challenge](http://www.yelp.com/dataset_challenge).
* [133] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. _Advances in Neural Information Processing Systems (NeurIPS)_, 28:649-657, 2015.
* [134] Ming-Wei Chang, Lev-Arie Ratinov, Dan Roth, and Vivek Srikumar. Importance of semantic representation: Dataless classification. In _AAAI_, volume 2, pages 830-835, 2008.
* [135] Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-dependent label noise: A sample sieve approach. _arXiv preprint arXiv:2010.02347_, 2020.

## Appendix A Notation

We present the notation table for each symbol used in this paper in Table 6.

## Appendix B Related Work

Many previous methods have been proposed for dealing with the specific types and some combinations of imprecise label configurations. We revisit the relevant work in this section, especially the state-of-the-art popular baselines for learning with individual and mixture imprecise label configurations.

**Partial label learning (PLL).** The prior arts can be roughly divided into identification-based for label disambiguation [96, 97, 98, 99] or average-based for utilizing all candidate labels [72, 7, 58]. The traditional average-based methods usually treat all candidate labels equally, which may involve the misleading false positive labels into training. To overcome these limitations, researchers have

\begin{table}
\begin{tabular}{l|l} \hline Notation & Definition \\ \hline \(\mathbf{x}\) & A training instance \\ \(y\) & A class index label \\ \(\{\mathbf{x}_{i}\}_{i\in[N]}\) & A set of data instances \(\mathbf{x}\) of size \(N\) \\ \(\{y_{i}\}_{i\in[N]}\) & A set of precise label indices \(y\) of size \(N\) \\ \(\{i\}\) & An imprecise label, which might contain multiple class indices \\ \(\{i\}_{i}\) & \(\{i\}_{i\in[N]}\) & A set of imprecise labels \([i]\) of size \(N\) \\ \(X\) & Random variable of training instance \\ \(\mathcal{X}\) & Input space where \(\mathbf{x}\) is drawn from \\ \(Y\) & Random variable of ground-truth labels \\ \(\mathcal{Y}\) & Label space where \(y\) is drawn from \\ \(I\) & Random variable of imprecise labels \\ \(f\) & Model backbone \\ \(g\) & Model classifier \\ \(h\) & Model multi-layer perceptron \\ \(f\circ g\) & Model mapping \(\mathcal{X}\rightarrow\mathcal{Y}\) \\ \(\theta\) & Learnable parameters of \(f\circ g\) \\ \(\mathbf{p}(y|\mathbf{x};\theta)\) & Output probability from model \(f\circ g\) \\ \(f\circ h\) & Model mapping \(\mathcal{X}\rightarrow\mathcal{Z}\), where \(Z\) is a projected feature space \\ \(\mathcal{D}\) & Dataset \\ \(\mathcal{L}\) & Loss function \\ \(\mathcal{A}_{\mathbf{w}}\) & Weak data augmentation, usually is HorizontalFlip \\ \(\mathcal{A}_{\mathbf{s}}\) & Strong data augmentation, usually is RandAugment [95] \\ \(\mathbf{z}_{\mathbf{w}}\) & Projected features from \(f\circ h\) on weakly-augmented data \\ \(\mathbf{z}_{\mathbf{s}}\) & Projected features from \(f\circ h\) on strongly-augmented data \\ \(\mathcal{M}\) & Memory queue in MoCo [70] \\ \(\mathbf{s}\) & A partial label, with ground-truth label contained \\ \(\{\mathbf{s}_{i}\}_{i\in[N]}\) & A set partial labels, with ground-truth label contained of size \(N\) \\ \(S\) & Random variable of partial label \\ \(\mathbf{x}^{1}\) & A labeled training example \\ \(y^{1}_{\mathbf{u}}\) & A labeled class index \\ \(\mathbf{x}^{\mathbf{u}}\) & A unlabeled training example \\ \(y^{\mathbf{u}}\) & A unknown class index for unlabeled data \\ \(X^{\mathbf{L}}\) & A set of labeled data instances \\ \(Y^{\mathbf{L}}\) & A set of labels for labeled data instances \\ \(X^{\mathbf{U}}\) & A set of unlabeled data instances \\ \(Y^{\mathbf{U}}\) & A set of unknown labels for unlabeled data instances \\ \(\tilde{y}^{\mathbf{u}}\) & The maximum predicted probability on unlabeled data \(\max(\mathbf{p}(y|\mathbf{x}^{\mathbf{u}};\theta))\) \\ \(\tilde{y}^{\mathbf{u}}\) & The pseudo-label from the predicted probability on unlabeled data \(\arg\max(\mathbf{p}(y|\mathbf{x}^{\mathbf{u}};\theta))\) \\ \(\tilde{r}\) & The threshold for confidence thresholding \\ \(\tilde{y}\) & A corrupted/noisy label \\ \(\tilde{y}^{\mathbf{oh}}\) & An one-hot version of the corrupted/noisy label \\ \(\tilde{Y}\) & Random variable of noisy labels \\ \(\mathbf{u},\mathbf{v},\mathbf{m}\) & Noise model related parameters in SOP [69] \\ \(\mathcal{T}(\tilde{y}|y;\omega)\) & The simplified noise transition model in ILL \\ \(\omega\) & The parameters in the simplified noise model \\ \hline \end{tabular}
\end{table}
Table 6: Notation Tableexplored identification-based methods, viewing the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion [100, 101] or the maximum likelihood criterion [102]. Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN [55] proposed a self-training strategy that disambiguates candidate labels using model outputs. CC [9] introduced classifier-consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS [11] relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, Wangt et al. [13] incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity. RCR involves consistency regularization into PLL recently [12].

**Semi-supervised learning (SSL).** SSL is a paradigm for learning with a limited labeled dataset supplemented by a much larger unlabeled dataset. Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying perturbations [103, 15, 104]. Self-training [14, 73, 18] is a widely-used approach for leveraging unlabeled data. Pseudo Label [14, 73], a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. Recent studies focus largely on generating high-quality pseudo-labels. MixMatch [16], for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch [17], UDA [53], and FixMatch [18] adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples. Methods such as Dash [105], FlexMatch [20], and FreeMatch [22] dynamically adjust these thresholds following a curriculum learning approach. SoftMatch [23] introduces a novel utilization of pseudo-labels through Gaussian re-weighting. SSL has also seen improvements through the incorporation of label propagation, contrastive loss, and meta learning [106, 107, 86, 87, 82].

**Noisy label learning (NLL).** Overfitting to the noisy labels could result in poor generalization performance, even if the training error is optimized towards zero [108, 109]. Several strategies to address the noisy labels have been proposed [110]. Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem [29, 31, 111, 112]. Additionally, methods that re-weight loss [113] have also been explored for learning with noisy labels. Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. Early works [26] incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward [91], prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data [114] or additional assumptions about the data [115]. Noise correction has shown promising results in noisy label learning recently [116, 117, 69, 118]. During the early learning phase, the model can accurately predict a subset of the mislabeled examples [32]. This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model [119, 120]. Co-Teaching uses multiple differently trained networks for correcting noisy labels [28]. SELFIE [121] corrects a subset of labels by replacing them based on past model outputs. Another study in [122] uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. Similarly, DivideMix [33] employs two networks for sample selection using a mixture model and Mixup [90].

**Mixture imprecise label settings**. Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning. PiCO+ [94], an extended version of PiCO [13], is tailored specifically for partial noisy labels. IRNet [71] uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL. DALI [50] is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness. Additionally, some work has focused on semi-supervised partial label learning [51, 52]. No existing research can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously, which underscores the novelty and significance of our work.

**Previous attempts towards unification of learning from imprecise labels.** There are earlier attempts for the generalized solutions of different kinds of imprecise labels/observations. Denoeux[60] proposed an EM algorithm for the likelihood estimation of fuzzy data and verified the algorithm on linear regression and uni-variate normal mixture estimation. Van Rooyen et al. [63] developed an abstract framework that generically tackles label corruption via the Markov transition. Quost et al. [62] further extended the EM algorithm of fuzzy data on the finite mixture of Gaussians. Gong et al. [64] proposed a general framework with centroid estimation for imprecise supervision. A unified partial AUC optimization approach was also proposed earlier [67]. Zhang et al. [45] and Wei et al. [66] proposed generalized solutions for aggregate observations. A unified solution based on dynamic programming for count-based weak supervision was also proposed [123] While relating to these works on the surface, ILL does not require any assumption on the imprecise information and generalizes well to more practical settings with noisy labels. Some other works for individual settings also related EM framework, but usually involved the approximation on the EM [124; 25; 13].

## Appendix C Methods

### Derivation of Variational Lower Bound

Evidence lower bound (ELBO), or equivalently variational lower bound [59], is the core quantity in EM. From Eq. (5), to model \(\log P(X,I;\theta)\), we have:

\[\log P(X,I;\theta) =\int Q(Y)\log P(X,I;\theta)dY \tag{12}\] \[=\int Q(Y)\log P(X,I;\theta)\frac{P(Y|X,I;\theta)}{P(Y|X,I;\theta )}dY\] \[=\int Q(Y)\log\frac{P(X,I,Y;\theta)Q(Y)}{P(Y|X,I;\theta)Q(Y)}dY\] \[=\int Q(Y)\log\frac{P(X,I,Y;\theta)}{Q(Y)}dY-\int Q(Y)\log\frac{ P(Y|X,I;\theta)}{Q(Y)}dY\]

where the first term is the ELBO and the second term is the KL divergence \(\mathcal{D}_{KL}(Q(Y)||P(Y|X,I;\theta))\). Replacing \(Q(Y)\) with \(P(Y|X,I;\theta^{t})\) at each iteration will obtain Eq. (5).

### Instantiations to Partial Label Learning

The imprecise label \(I\) for partial labels is defined as the label candidate sets \(S\) with \(\{\mathbf{s}_{i}\}_{i\in[N]}\) containing the true labels. Now we can derive Eq. (6) by replacing \(I\) with \(S\) in Eq. (5):

\[\mathbb{E}_{Y|X,I;\theta^{t}}\left[\log P(Y|X;\theta)+\log P(I|X,Y; \theta)\right] \tag{13}\] \[=\mathbb{E}_{Y|X,S;\theta^{t}}\left[\log P(Y|X;\theta)+\log P(I|X, Y;\theta)\right]\] \[=\sum_{Y}P(Y|X,S;\theta^{t})\left[\log P(Y|X;\theta)+\log P(I|X, Y;\theta)\right]\] \[=\sum_{Y}P(Y|X,S;\theta^{t})\left[\log P(Y|X;\theta)\right]+\log P (I|X,Y;\theta)\]

Note that \(P(I|Y,X;\theta)\) can be moved out of the expectation because it is a fixed quantity to any \(Y\). Now we replace \(Y\), \(X\), and \(S\) to \(y\), \(\mathbf{x}\), and \(\mathbf{s}\) for each instance, and converting the maximization problem to negative log-likelihood minimization problem to drive the loss function:

\[\mathcal{L}_{\mathrm{ILL}}^{\mathrm{PLL}}=-\frac{1}{N}\sum_{i}^{N}\mathbf{p}(y _{i}|\mathbf{x}_{i},\mathbf{s}_{i};\theta^{t})\log\mathbf{p}(y_{i}|\mathbf{x}_ {i};\theta)-\frac{1}{N}\sum_{i}^{N}\log\mathbf{p}(\mathbf{s}_{i}|\mathbf{x}_{i },y_{i};\theta). \tag{14}\]

The first term is the Cross-Entropy loss we derived in Eq. (6). If \(S\) is not instance-dependent, then knowing \(Y\) also knows \(S\), the second term thus can be ignored in Eq. (6). If \(S\) becomes instance-dependent, the second term can be maintained as a supervised term as in [12] to optimize \(\theta\).

### Instantiations to Semi-Supervised Learning

In SSL, the input \(X\) consists of the labeled data \(X^{\mathrm{L}}\) and the unlabeled data \(X^{\mathrm{U}}\). The imprecise label for SSL is realized as the limited number of full labels \(Y^{\mathrm{L}}\) for \(X^{\mathrm{L}}\). The labels \(Y^{\mathrm{U}}\) for unlabeled are unknown and become the latent variable. Thus we can write:

\[\begin{split}&\mathbb{E}_{Y|X,I;\theta^{t}}\left[\log P(Y|X;\theta)+ \log P(I|X,Y;\theta)\right]\\ &=\mathbb{E}_{Y^{\mathrm{U}}|X^{\mathrm{U}},X^{\mathrm{L}},Y^{ \mathrm{L}};\theta^{t}}\left[\log P(Y^{\mathrm{U}}|X^{\mathrm{U}},X^{\mathrm{L }};\theta)+\log P(Y^{\mathrm{L}}|X^{\mathrm{L}},X^{\mathrm{U}},Y^{\mathrm{U}}; \theta)\right]\\ &=\sum_{Y^{\mathrm{U}}}P(Y^{\mathrm{U}}|X^{\mathrm{U}};\theta^{t}) \left[\log P(Y^{\mathrm{U}}|X^{\mathrm{U}};\theta)\right]+\log P(Y^{\mathrm{L}} |X^{\mathrm{L}};\theta).\end{split} \tag{15}\]

The negative log-likelihood loss function for \(\{\mathbf{x}_{i}^{l},y_{i}^{l}\}_{i\in[N^{L}]}\) and \(\{\mathbf{x}^{u}\}_{i\in[N^{U}]}\) thus becomes:

\[\mathcal{L}_{\mathrm{ILL}}^{\mathrm{SSL}}=\mathcal{L}_{\mathrm{CE}}\left( \mathbf{p}(y|\mathbf{x}^{u};\theta),\mathbf{p}(y|\mathbf{x}^{u};\theta^{t}) \right)+\mathcal{L}_{\mathrm{CE}}\left(\mathbf{p}(y|\mathbf{x}^{\mathrm{L}}; \theta),y^{\mathrm{L}}\right) \tag{16}\]

### Instantiations to Noisy Label Learning

We denote the given noisy labels as \(\hat{Y}\). For noisy label learning, our method naturally supports a noise transition model \(\mathcal{T}(\hat{Y}|Y;\omega)\) with learnable parameter \(\omega\), as we will show in the following:

\[\begin{split}&\mathbb{E}_{Y|X,I;\theta^{t}}\left[\log P(Y|X; \theta)+\log P(I|X,Y;\theta)\right]\\ &=\mathbb{E}_{Y|X,\hat{Y},\theta^{t}}\left[\log P(Y,\hat{Y}|X; \theta)\right]\\ &=\mathbb{E}_{Y|X,\hat{Y},\theta^{t}}\left[\log P(Y|\hat{Y},X; \theta)+\log P(\hat{Y}|X;\theta)\right]\\ &=\sum_{Y}P(Y|\hat{Y},X;\theta^{t})\log P(Y|\hat{Y},X;\theta)+ \log P(\hat{Y}|X;\theta).\end{split} \tag{17}\]

The loss function is:

\[\mathcal{L}_{\mathrm{ILL}}^{\mathrm{NLL}}=\mathcal{L}_{\mathrm{CE}}\left( \mathbf{p}(y|\mathbf{x},\hat{y};\theta,\omega^{t}),\mathbf{p}(y|\mathbf{x}, \hat{y};\theta^{t},\omega^{t})\right)+\mathcal{L}_{\mathrm{CE}}\left(\mathbf{ p}(\hat{y}|\mathbf{x};\theta,\omega),\hat{y}\right) \tag{18}\]

Note that both term is computed from the noise transition matrix as mentioned in Eq. (9).

### Instantiations to Mixed Imprecise Label Learning

In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in Eq. (7). On the labeled data, it mainly follows the Eq. (9) of noisy label learning, with the noisy single label becoming the noisy partial labels \(\hat{\mathbf{s}}\). For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels:

\[\mathcal{L}_{\mathrm{CE}}\left(\mathbf{p}\left(y\mid\mathcal{A}_{\mathrm{s}}( \mathbf{x}),\hat{\mathbf{s}};\theta,\omega^{t}\right),\mathbf{p}\left(y\mid \mathcal{A}_{\mathrm{w}}(\mathbf{x}),\hat{y};\theta^{t},\omega^{t}\right) \right)+\mathcal{L}_{\mathrm{CE}}\left(\mathbf{p}\left(\hat{y}\mid\mathcal{A} _{\mathrm{w}}(\mathbf{x});\theta,\omega\right),\hat{\mathbf{s}}\right) \tag{19}\]

We can compute both quantity through the noise transition model:

\[\mathbf{p}(y|\mathbf{x},\hat{\mathbf{s}};\theta,\omega^{t})\propto\mathbf{p}(y |\mathbf{x};\theta)\prod_{\hat{y}\in\hat{\mathbf{s}}}\mathcal{T}(y|\hat{y}; \omega^{t}),\text{and}\;\mathbf{p}(\hat{y}|\mathbf{x};\theta,\omega)=\sum_{y \in[C]}\mathbf{p}(y|\mathbf{x};\theta)\mathcal{T}(\hat{y}|y;\omega). \tag{20}\]

## Appendix D Experiments

### Additional Training Details

We adopt two additional training strategies for the ILL framework. The first is the "strong-weak" augmentation strategy [53]. Since there is a consistency regularization term in each imprecise label formulation of ILL, we use the soft pseudo-targets of the weakly-augmented data to train the strongly-augmented data. The second is the entropy loss [75] for class balancing, which is also adopted in SOP [69] and FreeMatch [22]. We set the loss weight for the entropy loss uniformly for all experiments as 0.1.

### Partial Label Learning

#### d.2.1 Setup

Following previous work [125; 11; 13], we evaluate our method on partial label learning setting using CIFAR-10, CIFAR-100, and CUB-200 [78]. We generate partially labeled datasets by flipping negative labels to false positive labels with a probability \(q\), which is also denoted as a partial ratio. Specifically, the \(C-1\) negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. We consider \(q\in\{0.1,0.3,0.5\}\) for CIFAR-10, \(q\in\{0.01,0.05,0.1\}\) for CIFAR-100, and \(q=0.05\) for CUB-200. For CIFAR-10 and CIFAR-100, we use ResNet-18 [1] as backbone. We use SGD as an optimizer with a learning rate of \(0.01\), a momentum of \(0.9\), and a weight decay of \(1\mathrm{e}{-3}\). For CUB-200, we initialize the ResNet-18 [1] with ImageNet-1K [126] pre-trained weights. We train \(800\) epochs for CIFAR-10 and CIFAR-100 [77], and \(300\) epochs for CUB-200, with a cosine learning rate scheduler. For CIFAR-10 and CIFAR-100, we use an input image size of 32. For CUB-200, we use an input image size of 224. A batch size of 256 is used for all datasets. The choice of these parameters mainly follows PiCO [13]. We present the full hyper-parameters systematically in Table 7.

#### d.2.2 Discussion

We additionally compare our method with R-CR [12], which uses a different architecture as the results in Table 1. R-CR uses Wide-ResNet34x10 as backbone, and adopts multiple strong data augmentations. It also adjusts the loss weight along training. For fair comparison, we use the same architecture without multiple augmentation and the curriculum adjust on loss. The results are shown in Table 8, where our method outperforms R-CR on CIFAR-10 and is comparable on CIFAR-100.

We also provide the comparison of our method on instance-dependent partial label learning as proposed by Xu et al. [81; 125]. Due to the nature of instance-dependence, we maintain the term \(P(S|Y,X;\theta)\) from Eq. (5) as a supervised term for optimization. We compare our method with VALEN [81], RCR [12], PiCO [13], and POP [125] on MNIST, Kuzushiji-MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, with synthetic instance-dependent partial labels generated according to Xu et al. [125]. From the results in Table 9, we proposed method demonstrate the best performance across different datasets evaluated.

A recent work on PLL discussed and analyzed the robustness performance of different loss functions, especially the average-based methods [58]. We perform a similar analysis here for the derived loss function in ILL. Following the notation in [58], let \(\mathbf{s}\) denote the candidate label set, \(\mathbf{x}\) as the training instance, \(g\) as the probability score from the model, and \(f\) as the classifier \(f(\mathbf{x})=\underset{i\in\mathcal{Y}}{\arg\max}g_{i}(\mathbf{x})\), the average-based PLL can be formulated as:

\[\mathcal{L}_{avg-PLL}(f(\mathbf{x}),\mathbf{s})=\frac{1}{|\mathbf{s}|}\sum_{i\in \mathbf{s}}\ell(f(\mathbf{x}),i) \tag{21}\]

\begin{table}
\begin{tabular}{c|c c c} \hline \hline Hyper-parameter & CIFAR-10 & CIFAR-100 & CUB-200 \\ \hline Image Size & 32 & 32 & 224 \\ Model & ResNet-18 & ResNet-18 & 
\begin{tabular}{} \end{tabular} \\ Batch Size & 256 & 256 & 256 \\ Learning Rate & 0.01 & 0.01 & 0.01 \\ Weight Decay & 1e-3 & 1e-3 & 1e-5 \\ LR Scheduler & Cosine & Cosine & Cosine \\ Training Epochs & 800 & 800 & 300 \\ Classes & 10 & 100 & 200 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Hyper-parameters for **partial label learning** used in experiments.

\begin{table}
\begin{tabular}{l|c c|c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c|}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} \\  & 0.3 & 0.5 & 0.05 & 0.10 \\ \hline R-CR & 97.28\(\pm\)0.02 & 97.05\(\pm\)0.05 & 82.77\(\pm\)0.10 & 82.24\(\pm\)0.07 \\ Ours & 97.55\(\pm\)0.07 & 97.17\(\pm\)0.11 & 82.46\(\pm\)0.08 & 82.22\(\pm\)0.05 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison with R-CR in partial label learningLv et al. [58] compared different loss functions \(\ell\) on both noise-free and noisy PLL settings, where they find both theoretically and empirically that average-based PLL with _bounded_ loss are robust under mild assumptions. Empirical study in [58] suggests that both _Mean Absolute Error_ and _Generalized Cross-Entropy_ loss [29] that proposed for noisy label learning achieves the best performance and robustness for average-based PLL.

Our solution for PLL can be viewed as an instantiation of the average-based PLL as in [58] with:

\[\ell(f(\mathbf{x}),i)=-\bar{g}_{i}(\mathbf{x})\log g_{i}(\mathbf{x}) \tag{22}\]

where \(\bar{g}\) is normalized probability over \(\mathbf{s}\) with detached gradient. We can further show that the above loss function is bounded for \(0<\ell\leq\frac{1}{\bar{g}}\) and thus bounded for summation of all classes, which demonstrates robustness, as we show in Table 4.

### Semi-Supervised Learning

#### d.3.1 Setup

For experiments of SSL, we follow the training and evaluation protocols of USB [82] on image and text classification. To construct the labeled dataset for semi-supervised learning, we uniformly select \(l/C\) samples from each class and treat the remaining samples as the unlabeled dataset. For image classification tasks, ImageNet-1K [126] Vision Transformers [4] are used, including CIFAR-100 [77], EuroSAT [127], STL-10 [128], TissueMNIST [129, 130], Semi-Aves [131]. For text classification tasks, we adopt BERT [3] as backbone, including IMDB [83], Amazon Review [84], Yelp Review [132], AG News [133], Yahoo Answer [134]. The hyper-parameters strictly follow USB, and are shown in Table 10 and Table 11.

#### d.3.2 Results

In the main paper, we only provide the comparison on CIFAR-100, STL-10, IMDB, and Amazon Review. Here we provide the full comparison in Table 12 and Table 13. From the full results, similar conclusion can be drawn as in the main paper. Our ILL framework demonstrates comparable performance as previous methods.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline Hyper-parameter & CIFAR-100 & STL-10 & Euro-SAT & TissueMNIST & Semi-Aves \\ \hline Image Size & 32 & 96 & 32 & 32 & 224 \\ Model & ViT-S-P4-32 & ViT-B-P16-96 & ViT-S-P4-32 & ViT-T-P4-32 & ViT-S-P16-224 \\ Labeled Batch size & & & 16 & & \\ Learning Rate & 5e-4 & 1e-4 & 5e-5 & 5e-5 & 1e-3 \\ Weight Decay & & & 5e-4 & & \\ Layer Decay Rate & 0.5 & 0.95 & 1.0 & 0.95 & 0.65 \\ LR Scheduler & & & \(\eta=\eta_{0}\cos(\frac{2\pi b}{100\pi})\) & & \\ Training epochs & & & 20 & & \\ Classes & 100 & 10 & 10 & 10 & 200 \\ Model EMA Momentum & & & 0.0 & & \\ Prediction EMA Momentum & & & 0.999 & & \\ Weak Augmentation & & & Random Crop, Random Horizontal Flip & & \\ Strong Augmentation & & & RandAugment [95] & & \\ \hline \hline \end{tabular}
\end{table}
Table 10: Hyper-parameters of **semi-supervised learning** used in vision experiments of USB.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline  & MNIST & Kuzushiji-MNIST & Fashion-MNIST & CIFAR-10 & CIFAR-100 \\ \hline VALEN [81] & 99.03 & 90.15 & 96.31 & 92.01 & 71.48 \\ RCR [12] & 98.81 & 90.62 & 96.64 & 86.11 & 71.07 \\ PiCO [13] & 98.76 & 88.87 & 94.83 & 89.35 & 66.30 \\ POP [125] & **99.28** & 91.09 & 96.93 & 93.00 & 71.82 \\ \hline Ours & 99.19 & **91.35** & **97.01** & **93.86** & **72.43** \\ \hline \hline \end{tabular}
\end{table}
Table 9: Comparison on instance-dependent partial label learning 

### Noisy Label Learning

#### d.4.1 Setup

We conduct experiments of noisy label learning following SOP [69]. We evaluate the proposed method on both synthetic symmetric/asymmetric noise on CIFAR-10 and CIFAR-100, and more realistic and larger-scale instance noise on Clothing1M and WebVision. To introduce the synthetic symmetric noise to CIFAR-10 and CIFAR-100, we uniformly flip labels for a probability \(\eta\) into other classes. For asymmetric noise, we only randomly flip the labels for particular pairs of classes. For CIFAR-10 and CIFAR-100, we train PreAct-ResNet-18 with SGD using a learning rate of \(0.02\), a

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c} \hline \hline Datasets & \multicolumn{2}{c|}{IMDB} & \multicolumn{2}{c|}{AG News} & \multicolumn{2}{c|}{Amazon Review} & \multicolumn{2}{c|}{Yahoo Answers} & \multicolumn{2}{c}{Yelp Review} \\ \hline \# Labels & 200 & 100 & 40 & 200 & 200 & 1000 & 500 & 2000 & 250 & 1000 \\ \hline Pseudo-Label [14] & 45.45\(\pm\)4.08 & 19.67\(\pm\)4.01 & 19.49\(\pm\)0.03 & 14.69\(\pm\)1.08 & 53.45\(\pm\)1.95 & 47.00\(\pm\)0.07 & 37.70\(\pm\)0.08 & 32.72\(\pm\)0.31 & 54.51\(\pm\)0.85 & 47.33\(\pm\)0.05 \\ Mean-Teacher [103] & 20.062\(\pm\)5.18 & 13.97\(\pm\)4.19 & 15.71\(\pm\)1.23 & 13.93\(\pm\)0.04 & 52.14\(\pm\)0.52 & 47.66\(\pm\)0.08 & 33.53\(\pm\)0.26 & 50.06\(\pm\)0.47 & 47.21\(\pm\)0.17 \\ VAT [104] & 25.93\(\pm\)2.18 & 11.61\(\pm\)4.17 & 14.70\(\pm\)1.19 & 11.71\(\pm\)0.08 & 49.38\(\pm\)0.06 & 54.34\(\pm\)0.31 & 31.50\(\pm\)0.52 & 52.97\(\pm\)0.41 & 45.30\(\pm\)0.32 \\ MixMatch [16] & 26.12\(\pm\)1.63 & 15.47\(\pm\)0.13 & 13.50\(\pm\)1.15 & 17.50\(\pm\)0.09 & 49.53\(\pm\)0.06 & 46.51\(\pm\)0.37 & 35.70\(\pm\)0.07 & 33.62\(\pm\)0.14 & 45.30\(\pm\)0.32 \\ AdaMatch [85] & 8.094\(\pm\)0.09 & 7.11\(\pm\)0.03 & **11.73\(\pm\)0.17** & **11.22\(\pm\)0.06** & 46.72\(\pm\)0.72 & 42.27\(\pm\)0.32 & 32.75\(\pm\)0.35 & 30.44\(\pm\)0.33 & 45.40\(\pm\)0.06 & 40.16\(\pm\)0.09 \\ FixMatch [18] & 7.203\(\pm\)0.17 & 7.33\(\pm\)0.17 & 11.71\(\pm\)0.09 & 47.61\(\pm\)0.03 & 34.05\(\pm\)0.08 & **33.03\(\pm\)0.15** & 46.52\(\pm\)0.08 & 40.56\(\pm\)0.04 & 40.15\(\pm\)0.34 \\ FleMatch [20] & 7.82\(\pm\)0.17 & 7.41\(\pm\)0.13 & 16.38\(\pm\)0.19 & 12.08\(\pm\)0.17 & 45.71\(\pm\)0.08 & 43.50\(\pm\)0.08 & 33.56\(\pm\)0.11 & 31.30\(\pm\)0.18 & 43.85\(\pm\)0.14 \\ Dash [105] & 8.34\(\pm\)0.17 & 7.55\(\pm\)0.13 & 17.67\(\pm\)0.13 & 13.76\(\pm\)0.17 & 47.10\(\pm\)0.34 & 3.09\(\pm\)0.26 & 32.50\(\pm\)0.13 & 31.19\(\pm\)0.29 & 45.24\(\pm\)0.14 \\ CoMatch [86] & 7.44\(\pm\)0.10 & 7.72\(\pm\)1.14 & 11.95\(\pm\)0.15 & 17.50\(\pm\)0.08 & 47.60\(\pm\)0.08 & 33.66\(\pm\)0.02 & 33.48\(\pm\)0.08 & 30.26\(\pm\)0.35 & 45.40\(\pm\)0.12 & 42.07\(\pm\)0.26 \\ SimMatch [87] & 7.93\(\pm\)0.55 & **7.88\(\pm\)0.13** & 14.26\(\pm\)0.15 & 12.45\(\pm\)0.17 & 45.91\(\pm\)0.08 & 42.21\(\pm\)0.10 & 33.06\(\pm\)0.03 & 30.16\(\pm\)0.03 & 46.12\(\pm\)0.04 & 40.26\(\pm\)0.03 \\ FreeMatch [22] & 8.94\(\pm\)0.17 & 7.95\(\pm\)0.15 & 12.98\(\pm\)0.17 & 11.73\(\pm\)0.18 & 46.14\(\pm\)0.08 & 42.37\(\pm\)0.13 & 30.24\(\pm\)0.13 & 47.95\(\pm\)0.14 & 40.37\(\pm\)0.12 \\ SoftMatch [23] & 7.76\(\pm\)0.15 & 7.97\(\pm\)0.19 & 11.90\(\pm\)0.17 & 12.71\(\pm\)0.18 & 46.92\(\pm\)0.09 & 42.42\(\pm\)0.20 & 33.30\(\pm\)0.03 & 30.64\(\pm\)0.04 & 44.69\(\pm\)0.36 & 39.76\(\pm\)0.13 \\ \hline Ours & **7.32\(\pm\)0.02** & 7.

weight decay of \(1e-3\), and a momentum of \(0.9\). We train for \(300\) epochs with a cosine learning rate schedule and a batch size of 128. For WebVision, we use InceptionResNet-v2 as the backbone and set the batch size to \(32\). Other settings are similar to CIFAR-10. For Clothing1M, we use ImageNet-1K pre trained ResNet-50 as the backbone. We train it using SGD with an initial learning rate of \(2e\)-\(3\) for a total of 10 epochs, where the learning rate is reduced by 10 after 5 epochs. In addition, we also conduct experiments on CIFAR-10N and CIFAR-100N. We present the detailed hyper-parameters in Table 14.

#### d.4.2 Results

In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N [92] in Table 15. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in Table 16. As shown in Table 15, the proposed ILL framework achieves performance comparable to the previous best method, SOP [69]. On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the Worst case noise scenario. However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance.

### Mixed Imprecise Label Learning

#### d.5.1 Setup

To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. We first uniformly sample \(l/C\) labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset. Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio \(q\). After obtaining the partial labels, we randomly select \(\eta\) percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. For CIFAR-10, we set \(l\in\{1000,5000,50000\}\), and for CIFAR-100, we set \(l\in\{5000,10000,50000\}\). Similarly in

\begin{table}
\begin{tabular}{l|c c c c|c c} \hline \hline Hyper-parameter & CIFAR-10 (CIFAR-10N) & CIFAR-100 (CIFAR-100N) & Clothing1M & WebVision \\ \hline Image Size & 32 & 32 & 224 & 299 \\ Model & PreAct-ResNet-18 (ResNet-34) & PreAct-ResNet-18 (ResNet-34) & 
\begin{tabular}{c} ResNet-50 \\ (ImageNet-1K pretrained) \\ \end{tabular} & Inception-ResNet-v2 \\ Batch Size & 128 & 128 & 64 & 32 \\ Learning Rate & 0.02 & 0.02 & 0.002 & 0.02 \\ Weight Decay & 1e-3 & 1e-3 & 1e-3 & 5e-4 \\ LR Scheduler & Cosine & Cosine & MultiStep & MultiStep \\ Training Books & 300 & 300 & 10 & 100 \\ Classes & 10 & 100 & 14 & 50 \\ Noisy Matrix Scale & 1.0 & 2.0 & 0.5 & 2.5 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Hyper-parameters for **noisy label learning** used in experiments.

\begin{table}
\begin{tabular}{l|c c c c c|c c} \hline \hline Dataset & \multicolumn{5}{c|}{CIFAR-10N} & \multicolumn{2}{c}{CIFAR-100N} \\ \hline Noisy Type & Clean & Random 1 & Random 2 & Random 3 & Aggregate & Worst & Clean & Noisy \\ \hline CE & 92.92e0.11 & 85.02e0.65 & 86.46e1.79 & 85.16e0.61 & 87.77e0.38 & 77.69e1.55 & 76.70e0.74 & 55.50e0.66 \\ Forward [91] & 03.02e0.12 & 86.88e0.50 & 86.14e0.24 & 87.04e0.38 & 88.24e0.22 & 79.79e0.46 & 76.18e0.37 & 57.01e1.03 \\ Co-teaching [28] & 93.35e0.14 & 90.33e0.13 & 90.30e0.17 & 90.15e0.18 & 91.20e0.13 & 83.83e0.13 & 73.46e0.69 & 60.37e0.27 \\ DivideMix [33] & - & 95.16e0.19 & 95.23e0.07 & 95.21e0.14 & 95.01e0.71 & 92.56e0.42 & 71.13e0.48 \\ ELR [32] & 95.39e0.05 & 94.43e0.41 & 94.20e0.24 & 94.34e0.22 & 94.83e0.10 & 91.09e0.16 & 78.57e0.12 & 66.72e0.02 \\ CORES [135] & 94.16e0.11 & 94.45e0.14 & 94.88e0.31 & 94.74e0.03 & 95.25e0.09 & 91.66e0.09 & 73.87e0.16 & 55.72e0.42 \\ SOP [69] & **96.38e0.31** & 95.28e0.13 & 95.31e0.10 & 95.39e0.11 & 95.61e0.13 & 92.34e0.21 & **78.91e0.45** & 67.81e0.23 \\ \hline Ours & 96.21e0.29 & **96.06e0.407** & **95.98e0.12** & **96.10e0.40e** & **96.40e0.03** & **93.55e0.14** & 78.53e0.21 & **68.07e0.33** \\ \hline \hline \end{tabular}
\end{table}
Table 15: Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR-100N for **noisy label learning**. The best results are indicated in **bold**, and the second best results are indicated in underline. Our results are averaged over three independent runs with ResNet34 as the backbone.

the partial label setting, we set \(q\in\{0.1,0.3,0.5\}\) for CIFAR-10, and \(q\in\{0.01,0.05,0.1\}\) for CIFAR-100. For noisy labels, we set \(\eta\in\{0.1,0.2,0.3\}\) for both datasets.

#### d.5.2 Results

We provide a more complete version of Table 4 in Table 17. On partial noisy labels of CIFAR-10 with partial ratio \(0.5\) and of CIFAR-100 with partial ratio \(0.1\), most baseline methods are more robust or even fail to perform. However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios.

### Ablation on Strong-Augmentation and Entropy Loss

We provide the ablation study on the strong-augmentation and entropy loss components here, which are common techniques in each setting [18; 13; 69]. For example, in SSL, strong-weak augmentation is an important strategy for SSL algorithms widely used in existing works such as FixMatch [18] and FlexMatch [20]. Thus, it is important to adopt strong-weak augmentation to achieve better performance in SSL [22; 23; 82]. This is similar in PLL settings [13; 12]. PiCO [13; 12] also used strong augmentation). Strong-weak augmentation and entropy loss are also adopted in SOP [69] of NLL. However, we found these techniques are less important for our formulation of NLL. We provide an ablation study on the entropy loss of SSL, and both techniques for NLL and PLL here to demonstrate our discussions.

### Runtime Analysis

We provide the runtime analysis on CIFAR-100 of our method on different settings, compared with the SOTA baselines. We compute the average runtime from all training iterations on CIFAR-100. The results are shown in Table 21. Our method in general present faster runtime without complex design such as contrastive loss.

\begin{table}
\begin{tabular}{l|c c} \hline \hline Dataset & Clothing1M & WebVision \\ \hline CE & 69.10 & - \\ Forward [91] & 69.80 & 61.10 \\ MentorNet [93] & 66.17 & 63.00 \\ Co-Teaching [28] & 69.20 & 63.60 \\ DivideMix [33] & **74.76** & 77.32 \\ ELR [32] & 72.90 & 76.20 \\ CORES [135] & 73.20 & - \\ SOP [69] & 73.50 & 76.60 \\ \hline Ours & 74.02\(\pm\)0.12 & **79.37\(\pm\)0.09** \\ \hline \hline \end{tabular}
\end{table}
Table 16: Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for **noisy label learning**. The best results are indicated in **bold** and the second best results are indicated in underline. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone.

[MISSING_PAGE_FAIL:28]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We discussed our contribution in introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed our limitation in conclusion. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All are stated in Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We present all details in both main paper and Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification: We use publically available data and code will be released. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All results are obtained with 3 runs using different seeds and error bars are reported. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the standard deviation in the results, which are averaged over 3 independent runs across different experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We showed in experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: All behaviors follows NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discussed in conclusion. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: NA Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer:[Yes] Justification: We discussed in experiments. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: NA Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: NA. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: NA. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.