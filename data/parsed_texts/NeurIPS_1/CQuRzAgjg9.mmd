# Online Clustering of Bandits with Misspecified User Models

 Zhiyong Wang

The Chinese University of Hong Kong

zywang21@cse.cuhk.edu.hk

&Jize Xie

Shanghai Jiao Tong University

xjzzjl@sjtu.edu.cn

&Xutong Liu

The Chinese University of Hong Kong

liuxt@cse.cuhk.edu.hk

&Shuai Li

Shanghai Jiao Tong University

shuaili8@sjtu.edu.cn

&John C.S. Lui

The Chinese University of Hong Kong

cslui@cse.cuhk.edu.hk

Corresponding author.

###### Abstract

The contextual linear bandit is an important online learning problem where given arm features, a learning agent selects an arm at each round to maximize the cumulative rewards in the long run. A line of works, called the clustering of bandits (CB), utilize the collaborative effect over user preferences and have shown significant improvements over classic linear bandit algorithms. However, existing CB algorithms require well-specified linear user models and can fail when this critical assumption does not hold. Whether robust CB algorithms can be designed for more practical scenarios with misspecified user models remains an open problem. In this paper, we are the first to present the important problem of clustering of bandits with misspecified user models (CBMUM), where the expected rewards in user models can be perturbed away from perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB (representing the learned clustering structure with dynamic graph and sets, respectively), that can accommodate the inaccurate user preference estimations and erroneous clustering caused by model misspecifications. We prove regret upper bounds of \(O(\epsilon_{*}T\sqrt{md\log T}+d\sqrt{mT}\log T)\) for our algorithms under milder assumptions than previous CB works (notably, we move past a restrictive technical assumption on the distribution of the arms), which match the lower bound asymptotically in \(T\) up to logarithmic factors, and also match the state-of-the-art results in several degenerate cases. The techniques in proving the regret caused by misclustering users are quite general and may be of independent interest. Experiments on both synthetic and real-world data show our outperformance over previous algorithms.

## 1 Introduction

Stochastic multi-armed bandit (MAB) [2, 4, 22] is an online sequential decision-making problem, where the learning agent selects an action and receives a corresponding reward at each round, so as to maximize the cumulative reward in the long run. MAB algorithms have been widely applied inrecommendation systems and computer networks to handle the exploration and exploitation trade-off [20; 30; 38; 5].

To deal with large-scale applications, the contextual linear bandits [24; 9; 1; 29; 21] have been studied, where the expected reward of each arm is assumed to be perfectly linear in their features. Leveraging the contextual side information about the user and arms, linear bandits can provide more personalized recommendations [16]. Classical linear bandit approaches, however, ignore the often useful tool of collaborative filtering. To utilize the relationships among users, the problem of clustering of bandits (CB) has been proposed [12]. Specifically, CB algorithms adaptively partition users into clusters and utilize the collaborative effect of users to enhance learning performance.

Although existing CB algorithms have shown great success in improving recommendation qualities, there exist two major limitations. First, all previous works on CB [12; 25; 27; 39] assume that for each user, the expected rewards follow a _perfectly linear_ model with respect to the user preference vector and arms' feature vectors. In many real-world scenarios, due to feature noises or uncertainty [15], the reward may not necessarily conform to a perfectly linear function, or even deviates a lot from linearity [14]. Second, previous CB works assume that for users within the same cluster, their preferences are exactly the same. Due to the heterogeneity in users' personalities and interests, similar users may not have identical preferences, invalidating this strong assumption.

To address these issues, we propose a novel problem of clustering of bandits with misspecified user models (CBMUM). In CBMUM, the expected reward model of each user does not follow a perfectly linear function but with possible additive deviations. We assume users in the same underlying cluster share a common preference vector, meaning they have the same linear part in reward models, but the deviation parts are allowed to be different, better reflecting the varieties of user personalities.

The relaxation of perfect linearity and the reward homogeneity within the same cluster bring many challenges to the CBMUM problem. In CBMUM, we not only need to handle the uncertainty from the _unknown_ user preference vectors, but also have to tackle the additional uncertainty from model misspecifications. Due to such uncertainties, it becomes highly challenging to design a robust algorithm that can cluster the users appropriately and utilize the clustered information judiciously. On the one hand, the algorithm needs to be more tolerant in the face of misspecifications so that more similar users can be clustered together to utilize the collaborative effect. On the other hand, it has to be more selective to rule out the possibility of _misculustering_ users with large preference gaps.

### Our Contributions

This paper makes the following four contributions.

**New Model Formulation.** We are the first to formulate the clustering of bandits with misspecified user models (CBMUM) problem, which is more practical by removing the perfect linearity assumption in previous CB works.

**Novel Algorithm Designs.** We design two novel algorithms, RCLUMB and RSCLUMB, which robustly learn the clustering structure and utilize this collaborative information for faster user preference elicitation. Specifically, RCLUMB keeps updating a dynamic graph over all users, where users connected directly by edges are supposed to be in the same cluster. RCLUMB adaptively removes edges and recommends items based on historical interactions. RSCLUMB represents the clustering structure with sets, which are dynamicly merged and split during the learning process. Due to the page limit, we only illustrate the RCLUMB algorithm in the main paper. We leave the exposition, illustration, and regret analysis of the RSCLUMB algorithm in Appendix K.

To overcome the challenges brought by model misspecifications, we do the following key steps in the RCLUMB algorithm. (i) To ensure that with high probability, similar users will not be partitioned apart, we design a more tolerant edge deletion rule by taking model misspecifications into consideration. (ii) Due to inaccurate user preference estimations caused by model misspecifications, trivially following previous CB works [12; 25; 28] to directly use connected components in the maintained graph as clusters would _miscluster_ users with big preference gaps, causing a large regret. To be discriminative in cluster assignments, we filter users directly linked with the current user in the graph to form the cluster used in this round. With these careful designs of (i) and (ii), we can guarantee that with high probability, information of all similar users can be leveraged, and only users with close enough preferences might be _misclustered_, which will only mildly impair the learning accuracy. Additionally: (iii) we design an enlarged confidence radius to incorporate both the exploration bonus and the additional uncertainty from misspecifications when recommending arms. The design of RSCLUMB follows similar ideas, which we leave in the Appendix K due to page limit.

**Theoretical Analysis with Milder Assumptions**. We prove regret upper bounds for our algorithms of \(O(\epsilon_{*}T\sqrt{md\log T}+d\sqrt{mT}\log T)\) in CBMUM under much milder and practical assumptions (in arm generation distribution) than previous CB works, which match the state-of-the-art results in degenerate cases. Our proof is quite different from the typical proof flow of previous CB works (details in Appendix C). One key challenge is to bound the regret caused by _mischustering_ users with close but not the same preference vectors and use the inaccurate cluster-based information to recommend arms. To handle the challenge, we prove a key lemma (Lemma 5.7) to bound this part of regret. We defer its details in Section 5 and Appendix G. The techniques and results for bounding this part are quite general and may be of independent interest. We also give a regret lower bound of \(\Omega(\epsilon_{*}T\sqrt{d})\) for CBMUM, showing that our upper bounds are asymptotically tight with respect to \(T\) up to logarithmic factors. We leave proving a tighter lower bound for CBMUM as an open problem.

**Good Experimental Performance.** Extensive experiments on both synthetic and real-world data show the advantages of our proposed algorithms over the existing algorithms.

## 2 Related Work

Our work is closely related to two lines of research: online clustering of bandits (CB) and misspecified linear bandits (MLB). More discussions on related works can be found in Appendix A.

The paper [12] first formulates the CB problem and proposes a graph-based algorithm. The work [26] further considers leveraging the collaborative effects on items to guide the clustering of users. The work [25] considers the CB problem in the cascading bandits setting with random prefix feedback. The paper [27] also considers users with different arrival frequencies. A recent work [28] proposes the setting of clustering of federated bandits, considering both privacy protection and communication requirements. However, all these works assume that the reward model for each user follows a perfectly linear model, which is unrealistic in many real-world applications. To the best of our knowledge, this paper is the first work to consider user model misspecifications in the CB problem.

The work [14] first proposes the misspecified linear bandits (MLB) problem, shows the vulnerability of linear bandit algorithms under deviations, and designs an algorithm RLB that is only robust to non-sparse deviations. The work [23] proposes two algorithms to handle general deviations, which are modifications of the phased elimination algorithm [22] and LinUCB [1]. Some recent works [31; 11] use model selection methods to deal with unknown exact maximum model misspecification level. Note that the work [11] has an additional assumption on the access to an online regression oracle, and the paper [31] still needs to know an upper bound of the unknown exact maximum model deviation level. None of them consider the CB setting with multiple users, thus differing from ours.

We are the first to initialize the study of the important CBMUM problem, and propose a general framework for dealing with model misspecifications in CB problems. Our study is based on fundamental models on CB [12; 27] and MLB [23], the algorithm design ideas and theoretical analysis are pretty general. We leave incorporating the model selection methods [31; 11] into our framework to address the unknown exact maximum model misspecification level as an interesting future work.

## 3 Problem Setup

This section formulates the problem of "clustering of bandits with misspecified user models" (CB-MUM). We use boldface **lowercase** and boldface **CAPITALIZED** letters for vectors and matrices. We use \(\left|\mathcal{A}\right|\) to denote the number of elements in \(\mathcal{A}\), \(\left[m\right]\) to denote \(\left\{1,\dots,m\right\}\), and \(\left\|\mathbf{x}\right\|_{\mathbf{M}}=\sqrt{\mathbf{x}^{\top}\mathbf{M}\mathbf{x}}\) to denote the matrix norm of vector \(\mathbf{x}\) regarding the positive semi-definite (PSD) matrix \(\mathbf{M}\).

In CBMUM, there are \(u\) users denoted by \(\mathcal{U}=\left\{1,2,\dots,u\right\}\). Each user \(i\in\mathcal{U}\) is associated with an _unknown_ preference vector \(\mathbf{\theta}_{i}\in\mathbb{R}^{d}\), with \(\left\|\mathbf{\theta}_{i}\right\|_{2}\leq 1\). We assume there is an _unknown_ underlying clustering structure over users representing the similarity of their behaviors. Specifically, \(\mathcal{U}\) can be partitioned into a small number \(m\) (i.e., \(m\ll u\)) clusters, \(V_{1},V_{2},\dots V_{m}\), where \(\cup_{j\in[m]}V_{j}=\mathcal{U}\), and \(V_{j}\cap V_{j^{\prime}}=\emptyset\), for \(j\neq j^{\prime}\). We call these clusters _ground-truth clusters_ and use \(\mathcal{V}=\left\{V_{1},V_{2},\dots,V_{m}\right\}\) to denote the set of these clusters. Users in the same _ground-truth cluster_ share the same preference vector, while users from different _ground-truth clusters_ have different preference vectors. Letdenote the common preference vector for \(V_{j}\) and \(j(i)\in[m]\) denote the index of the _ground-truth cluster_ that user \(i\) belongs to. For any \(\ell\in\mathcal{U}\), if \(\ell\in V_{j(i)}\), then \(\mathbf{\theta}_{\ell}=\mathbf{\theta}_{i}=\mathbf{\theta}^{j(i)}\).

At each round \(t\in[T]\), a user \(i_{t}\in\mathcal{U}\) comes to be served. The learning agent receives a finite arm set \(\mathcal{A}_{t}\subseteq\mathcal{A}\) to choose from (with \(|\mathcal{A}_{t}|\leq C,\forall t\)), where each arm \(a\in\mathcal{A}\) is associated with a feature vector \(\mathbf{x}_{a}\in\mathbb{R}^{d}\), and \(\left\lVert\mathbf{x}_{a}\right\rVert_{2}\leq 1\). The agent assigns an appropriate cluster \(\overline{V}_{t}\) for user \(i_{t}\) and recommends an item \(a_{t}\in\mathcal{A}_{t}\) based on the aggregated historical information gathered from cluster \(\overline{V}_{t}\). After receiving the recommended item \(a_{t}\), user \(i_{t}\) gives a random reward \(r_{t}\in[0,1]\) to the agent. To better model real-world scenarios, we assume that the reward \(r_{t}\) follows a misspecified linear function of the item feature vector \(\mathbf{x}_{a_{t}}\) and the _unknown_ user preference vector \(\mathbf{\theta}_{i_{t}}\). Formally,

\[r_{t}=\mathbf{x}_{a_{t}}^{\top}\mathbf{\theta}_{i_{t}}+\mathbf{\epsilon}_{a_{t}}^{i_{t},t} +\eta_{t}\,, \tag{1}\]

where \(\mathbf{\epsilon}^{i_{t},t}=[\mathbf{\epsilon}_{1}^{i_{t},t},\mathbf{\epsilon}_{2}^{i_{t},t},\ldots,\mathbf{\epsilon}_{|\mathcal{A}_{t}|}^{i_{t},t}]^{\top}\in\mathbb{R}^{| \mathcal{A}_{t}|}\) denotes the _unknown_ deviation in the expected rewards of arms in \(\mathcal{A}_{t}\) from linearity for user \(i_{t}\) at \(t\), and \(\eta_{t}\) is the 1-sub-Gaussian noise. We allow the deviation vectors for users in the same _ground-truth cluster_ to be different.

We assume the clusters, users, items, and model misspecifications satisfy the following assumptions.

**Assumption 3.1** (Gap between different clusters).: The gap between any two preference vectors for different _ground-truth clusters_ is at least an _unknown_ positive constant \(\gamma\)

\[\left\lVert\mathbf{\theta}^{j}-\mathbf{\theta}^{j^{\prime}}\right\rVert_{2}\geq\gamma>0 \,,\forall j,j^{\prime}\in[m]\,,j\neq j^{\prime}\,.\]

**Assumption 3.2** (Uniform arrival of users).: At each round \(t\), a user \(i_{t}\) comes uniformly at random from \(\mathcal{U}\) with probability \(1/u\), independent of the past rounds.

**Assumption 3.3** (Item regularity).: At each time step \(t\), the feature vector \(\mathbf{x}_{a}\) of each arm \(a\in\mathcal{A}_{t}\) is drawn independently from a fixed but unknown distribution \(\rho\) over \(\{\mathbf{x}\in\mathbb{R}^{d}:\left\lVert\mathbf{x}\right\rVert_{2}\leq 1\}\), where \(\mathbb{E}_{\mathbf{x}\sim\rho}[\mathbf{x}\mathbf{x}^{\top}]\) is full rank with minimal eigenvalue \(\lambda_{x}>0\). Additionally, at any time \(t\), for any fixed unit vector \(\mathbf{\theta}\in\mathbb{R}^{d}\), \((\mathbf{\theta}^{\top}\mathbf{x})^{2}\) has sub-Gaussian tail with variance upper bounded by \(\sigma^{2}\).

**Assumption 3.4** (Bounded misspecification level).: We assume that there is a pre-specified maximum misspecification level parameter \(\epsilon_{*}\) such that \(\left\lVert\mathbf{\epsilon}^{i,t}\right\rVert_{\infty}\leq\epsilon_{*}\), \(\forall i\in\mathcal{U},t\in[T]\).

**Remark 1**.: All these assumptions basically follow previous works on CB [12, 13, 25, 3, 28] and MLB [23]. Note that Assumption 3.3 is less stringent and more practical than previous CB works which also put restrictions on the variance upper bound \(\sigma^{2}\). For Assumption 3.2, our results can easily generalize to the case where the user arrival follows any distributions with minimum arrival probability greater than \(p_{min}\). For Assumption 3.4, note that \(\epsilon_{*}\) can be an upper bound on the maximum misspecification level, not the exact maximum itself. In real-world applications, the deviations are usually small [14], and we can set a relatively big \(\epsilon_{*}\) as an upper bound. For more discussions please refer to Appendix B at \(t\). The goal of the agent is to minimize the expected cumulative regret

\[R(T)=\mathbb{E}[\sum_{t=1}^{T}(\mathbf{x}_{a_{t}^{\top}}\mathbf{\theta}_{i_{t}}+\mathbf{ \epsilon}_{a_{t}^{i_{t},t}}^{i_{t},t}-\mathbf{x}_{a_{t}}^{\top}\mathbf{\theta}_{i_{t}}- \mathbf{\epsilon}_{a_{t}}^{i_{t},t})]\,. \tag{2}\]

## 4 Algorithm

This section introduces our algorithm called "Robust CLUstering of Misspecified Bandits" (RCLUMB) (Algo.1). RCLUMB is a graph-based algorithm. The ideas and techniques of RCLUMB can be easily generalized to set-based algorithms. To illustrate this generalizability, we also design a set-based algorithm RSCLUMB. We leave the exposition and analysis of RSCLUMB in Appendix K.

For ease of interpretation, we define the coefficient

\[\zeta\triangleq 2\epsilon_{*}\sqrt{\frac{2}{\tilde{\lambda}_{x}}}\,, \tag{3}\]

where \(\tilde{\lambda}_{x}\triangleq\int_{0}^{\lambda_{x}}(1-e^{-\frac{(\lambda_{x}- \rho)^{2}}{2\sigma^{2}}})^{C}dx\). \(\zeta\) is theoretically the minimum gap between two users' preference vectors that an algorithm can distinguish with high probability, as supported by Eq.(50) in the proof of Lemma H.1 in Appendix H. Note that the algorithm does not require knowledge of \(\zeta\). We also make the following definition for illustration.

**Definition 4.1** (\(\zeta\)-close users and \(\zeta\)-good clusters).: Two users \(i,i^{\prime}\in\mathcal{U}\) are \(\zeta\)-close if \(\left\|\boldsymbol{\theta}_{i}-\boldsymbol{\theta}_{i^{\prime}}\right\|_{2}\leq\zeta\). Cluster \(\overline{V}\) is a \(\zeta\)-good cluster at time \(t\), if \(\forall\,i\in\overline{V}\), user \(i\) and the coming user \(i_{t}\) are \(\zeta\)-close.

We also say that two _ground-truth clusters_ are "\(\zeta\)-close" if their preference vectors' gap is less than \(\zeta\).

Now we introduce the process and intuitions of RCLUMB (Algo.1). The algorithm maintains an undirected user graph \(G_{t}=(\mathcal{U},E_{t})\), where users are connected with edges if they are inferred to be in the same cluster. We denote the connected component in \(G_{t-1}\) containing user \(i_{t}\) at round \(t\) as \(\tilde{V}_{t}\).

**Cluster Detection.**\(G_{0}\) is initialized to be a complete graph, and will be updated adaptively based on the interactive information. At round \(t\), user \(i_{t}\in\mathcal{U}\) comes to be served with a feasible arm set \(\mathcal{A}_{t}\) (Line 4). Due to model misspecifications, it is impossible to cluster users with exactly the same preference vector \(\boldsymbol{\theta}\), but similar users whose preference vectors are within the distance of \(\zeta\). According to the proof of Lemma H.1, after a sufficient time, with high probability, any pair of users directly connected by an edge in \(E_{t-1}\) are \(\zeta\)-close. However, if we trivially follow previous CB works [12, 25, 28] to directly use the connected component \(\tilde{V}_{t}\) as the inferred cluster for user \(i_{t}\) at round \(t\), it will cause a large regret. The reason is that in the worst case, the preference vector \(\boldsymbol{\theta}\) of the user in \(\tilde{V}_{t}\) who is \(h\)-hop away from user \(i_{t}\) could deviate by \(h\zeta\) from \(\boldsymbol{\theta}_{i_{t}}\), where \(h\) can be as large as \(|\tilde{V}_{t}|\). Based on this reasoning, our key point is to select the cluster \(\overline{V}_{t}\) as the users at most 1-hop away from \(i_{t}\) in the graph. In other words, after some interactions, \(\overline{V}_{t}\) forms a \(\zeta\)-good cluster with high probability; thus, RCLUMB can avoid using misleading information from dissimilar users for recommendations.

**Cluster-based Recommendation.** After finding the appropriate cluster \(\overline{V}_{t}\) for \(i_{t}\), the agent estimates the common user preference vector based on the historical information associated with cluster \(\overline{V}_{t}\) by

\[\hat{\boldsymbol{\theta}}_{\overline{V}_{t},t-1}=\operatorname*{arg\,min}_{ \boldsymbol{\theta}\in\mathbb{R}^{d}}\sum_{\stackrel{{\kappa| \boldsymbol{\theta}|=1}}{{i_{\kappa}\in\overline{V}_{t}}}}(r_{s}-\boldsymbol{ x}_{a}^{\top}\boldsymbol{\theta})^{2}+\lambda\left\|\boldsymbol{\theta}\right\|_{2}^{ 2}\,, \tag{4}\]

where \(\lambda>0\) is a regularization coefficient. Its closed-form solution is \(\hat{\boldsymbol{\theta}}_{\overline{V}_{t},t-1}=\overline{M}_{\overline{V}_{ t},t-1}^{-1}\overline{b}_{\overline{V}_{t},t-1}\), where \(\overline{M}_{\overline{V}_{t},t-1}=\lambda\boldsymbol{I}+\sum_{\stackrel{{ \kappa|\boldsymbol{\theta}|=1}}{{i_{\kappa}\in\overline{V}_{t}}}}\boldsymbol {a}_{a},\boldsymbol{x}_{a}^{\top}\), \(\overline{b}_{\overline{V}_{t},t-1}=\sum_{\stackrel{{\kappa| \boldsymbol{\theta}|=1}}{{i_{\kappa}\in\overline{V}_{t}}}}r_{a_{s}}\boldsymbol {x}_{a}\).

Based on this estimation, in Line 7, the agent recommends an arm using the UCB strategy

\[a_{t}=\operatorname*{argmax}_{a\in\mathcal{A}_{t}}\min\{1,\underbrace{ \boldsymbol{x}_{a}^{\top}\hat{\boldsymbol{\theta}}_{\overline{V}_{t},t-1}}_{ \tilde{R}_{a,t}}+\underbrace{\beta\left\|\boldsymbol{x}_{a}\right\|_{\overline {V}_{t},t-1}^{-1}+\epsilon_{*}\sum_{\stackrel{{\kappa|\boldsymbol{ \theta}|=1}}{{i_{\kappa}\in\overline{V}_{t}}}}}_{\overline{V}_{t},t-1}\left| \boldsymbol{x}_{a}^{\top}\overline{M}_{\overline{V}_{t},t-1}^{-1}\boldsymbol {x}_{a_{s}}\right|\}}_{C_{a,t}}\}\,, \tag{5}\]where \(\beta=\sqrt{\lambda}+\sqrt{2\log(\frac{1}{3})+d\log(1+\frac{T}{\lambda d})}\), \(\hat{R}_{a,t}\) denotes the estimated reward of arm \(a\) at \(t\), \(C_{a,t}\) denotes the confidence radius of arm \(a\) at round \(t\).

Due to deviations from linearity, the estimation \(\hat{R}_{a,t}\) computed by a linear function is no longer accurate. To handle the estimation uncertainty of model misspecifications, we design an enlarged confidence radius \(C_{a,t}\). The first term of \(C_{a,t}\) in Eq.(5) captures the uncertainty of online learning for the linear part, and the second term related to \(\epsilon_{a}\) reflects the additional uncertainty from deviations from linearity. The design of \(C_{a,t}\) theoretically relies on Lemma 5.6 which will be given in Section 5.

**Update User Statistics.** Based the feedback \(r_{t}\), in Line 8 and 9, the agent updates the statistics for user \(i_{t}\). Specifically, the agent estimates the preference vector \(\boldsymbol{\theta}_{i_{t}}\) by

\[\boldsymbol{\hat{\theta}}_{i_{t},t}=\operatorname*{arg\,min}_{ \boldsymbol{\theta}\in\mathbb{R}^{d}}\sum_{\genfrac{}{}{0.0pt}{}{z\in[t]}{i_{ s}=i_{t}}}\left(r_{s}-\boldsymbol{x}_{a_{s}}^{\top}\boldsymbol{\theta}\right)^{2 }+\lambda\left\|\boldsymbol{\theta}\right\|_{2}^{2}\,, \tag{6}\]

with solution \(\hat{\boldsymbol{\theta}}_{i_{t},t}=(\lambda\boldsymbol{I}+\boldsymbol{M}_{i_ {t},t})^{-1}\boldsymbol{b}_{i_{t},t}\,,\) where \(M_{i_{t},t}=\sum_{\genfrac{}{}{0.0pt}{}{z\in[t]}{i_{s}=i_{t}}}\boldsymbol{x} _{a_{s}}\boldsymbol{x}_{a_{s}}^{\top},\boldsymbol{b}_{i_{t},t}=\sum_{\genfrac{} {}{0.0pt}{}{z\in[t]}{i_{s}=i_{t}}}r_{a_{s}}\boldsymbol{x}_{a_{s}}\,.\)

**Update the Graph \(G_{t}\).** Finally, in Line 10, the agent verifies whether the similarities between user \(i_{t}\) and other users are still true based on the updated estimation \(\hat{\boldsymbol{\theta}}_{i_{t},t}\). For every user \(\ell\in\mathcal{U}\) connected with user \(i_{t}\) via edge \((i_{t},\ell)\in E_{t-1}\), if the gap between her estimated preference vector \(\hat{\boldsymbol{\theta}}_{\ell,t}\) and \(\hat{\boldsymbol{\theta}}_{i_{t},t}\) is larger than a threshold supported by Lemma H.1, the agent will delete the edge \((i_{t},\ell)\) to split them apart. The threshold in Line 10 is carefully designed, taking both estimation uncertainty in a linear model and deviations from linearity into consideration. As shown in the proof of Lemma H.1 (in Appendix H), using this threshold, with high probability, edges between users in the same _ground-truth clusters_ will not be deleted, and edges between users that are not \(\zeta\)-close will always be deleted. Together with the filtering step in Line 5, with high probability, the algorithm will leverage all the collaborative information of similar users and avoid misusing the information of dissimilar users. The updated graph \(G_{t}\) will be used in the next round.

## 5 Theoretical Analysis

In this section, we theoretically analyze the performance of the RCLUMB algorithm by giving an upper bound of the expected regret defined in Eq.(2). Due to the space limitation, we only show the main result (Theorem 5.3), key lemmas, and a sketched proof for Theorem 5.3. Detailed proofs, other technical lemmas, and the regret analysis of the RSLUMB algorithm can be found in the Appendix.

To state our main result, we first give two definitions as follows. The first definition is about the minimum separable gap constant \(\gamma_{1}\) of a CBMUM problem instance.

**Definition 5.1** (Minimum separable gap \(\gamma_{1}\)).: The minimum separable gap constant \(\gamma_{1}\) of a CBMUM problem instance is the minimum gap over the gaps among users that are greater than \(\zeta\) (Eq. (3))

\[\gamma_{1}=\min\{\left\|\boldsymbol{\theta}_{i}-\boldsymbol{\theta}_{\ell} \right\|_{2}:\left\|\boldsymbol{\theta}_{i}-\boldsymbol{\theta}_{\ell}\right\| _{2}>\zeta,\forall i,\ell\in\mathcal{U}\}\,,\text{with}\min\emptyset=\infty.\]

**Remark 2**.: In CBMUM, the role of \(\gamma_{1}-\zeta\) is similar to that of \(\gamma\) (given in Assumption 3.1) in the previous CB problem with perfectly linear models, quantifying the hardness of performing clustering on the problem instance. Intuitively, users are easier to cluster if \(\gamma_{1}\) is larger, and the deduction of \(\zeta\) shows the additional difficulty due to model divisions. If there are no misspecifications, i.e., \(\zeta=2\epsilon_{a}\sqrt{\frac{2}{\lambda_{s}}}=0\), then \(\gamma_{1}=\gamma\), recovering the minimum separable gap between clusters in the classic CB problem [12; 25] without model misspecifications.

The second definition is about the number of "hard-to-cluster users" \(\tilde{u}\).

**Definition 5.2** (Number of "hard-to-cluster users" \(\tilde{u}\)).: The number of "hard-to-cluster users" \(\tilde{u}\) is the number of users in the _ground-truth clusters_ which are \(\zeta\)-close to some other _ground-truth clusters_

\[\tilde{u}=\sum_{j\in[m]}|V_{j}|\times\mathbb{I}\{\exists j^{\prime}\in[m],j^{ \prime}\neq j:\left\|\boldsymbol{\theta}^{j^{\prime}}-\boldsymbol{\theta}^{j} \right\|_{2}\leq\zeta\}\,,\]

where \(\mathbb{I}\{\cdot\}\) denotes the indicator function of the argument, \(|V_{j}|\) denotes the number of users in \(V_{j}\).

**Remark 3**.: \(\tilde{u}\) captures the number of users who belong to different _ground-truth clusters_ but their gaps are less than \(\zeta\). These users may be merged into one cluster by mistake and cause certain regret.

The following theorem gives an upper bound on the expected regret achieved by RCLUMB.

**Theorem 5.3** (Main result on regret bound).: _Suppose that the assumptions in Section 3 are satisfied. Then the expected regret of the RCLUMB algorithm for \(T\) rounds satisfies_

\[R(T) \leq O\bigg{(}u\left(\frac{d}{\lambda_{x}(\gamma_{1}-\zeta)^{2}}+ \frac{1}{\lambda_{\tilde{\lambda}_{2}}^{2}}\right)\log T+\frac{\tilde{u}}{u} \frac{\epsilon_{*}\sqrt{d}T}{\tilde{\lambda}_{x}^{1.5}}+\epsilon_{*}T\sqrt{md \log T}+d\sqrt{mT}\log T\bigg{)} \tag{7}\] \[\leq O(\epsilon_{*}T\sqrt{md\log T}+d\sqrt{mT}\log T)\,, \tag{8}\]

_where \(\gamma_{1}\) is defined in Definition 5.1, and \(\tilde{u}\) is defined in Definition 5.2)._

**Discussion and Comparison.** The bound in Eq.(7) has four terms. The first term is the time needed to gather enough information to assign appropriate clusters for users. The second term is the regret caused by _mischustering_\(\zeta\)-close but not precisely similar users together, which is unavoidable with model misspecifications. The third term is from the preference estimation errors caused by model deviations. The last term is the usual term in CB with perfectly linear models [12, 25, 27].

Let us discuss how the parameters affect this regret bound.

\(\bullet\) If \(\gamma_{1}-\zeta\) is large, the gaps between clusters that are not "\(\zeta\)-close" are much greater than the minimum gap \(\zeta\) for the algorithm to distinguish, the first term in Eq.(7) will be small as it is easy to identify their dissimilarities. The role of \(\gamma_{1}-\zeta\) in CBMUM is similar to that of \(\gamma\) in the previous CB.

\(\bullet\) If \(\tilde{u}\) is small, indicating that few _ground-truth clusters_ are "\(\zeta\)-close", RCLUMB will hardly _mischuster_ different _ground-truth clusters_ together thus the second term in Eq.(7) will be small.

\(\bullet\) If the deviation level \(\epsilon_{*}\) is small, the user models are close to linearity and the misspecifications will not affect the estimations much, then both the second and third term in Eq.(7) will be small.

The following theorem gives a regret lower bound of the CBMUM problem.

**Theorem 5.4** (Regret lower bound for CBMUM).: _There exists a problem instance for the CBMUM problem such that for any algorithm \(R(T)\geq\Omega(\epsilon_{*}T\sqrt{d})\,.\)_

The proof can be found in Appendix F. The upper bounds in Theorem 5.3 asymptotically match this lower bound with respect to \(T\) up to logarithmic factors (and a constant factor of \(\sqrt{m}\) where \(m\) is typically small in real-applications), showing the tightness of our theoretical results. Additionally, we conjecture the gap for the \(m\) factor is due to the strong assumption that cluster structures are known to prove this lower bound, and whether there exists a tighter lower bound is left for future work.

We then compare our results with two degenerate cases. First, when \(m=1\) (indicating \(\tilde{u}=0\)), our setting degenerates to the MLB problem where all users share the same preference vector. In this case, our regret bound is \(O(\epsilon_{*}T\sqrt{d\log T}+d\sqrt{T}\log T)\), exactly matching the current best bound of MLB [23]. Second, when \(\epsilon_{*}=0\), our setting reduces to the CB problem with perfectly linear user models and our bounds become \(O(d\sqrt{mT}\log T)\), also perfectly match the existing best bound of the CB problem [25, 27]. The above discussions and comparisons show the tightness of our regret bounds. Additionally, we also provide detailed discussions on why trivially combining existing works on CB and MLB would not get any non-vacuous regret upper bound in Appendix D.

We define the following "good partition" for ease of interpretation.

**Definition 5.5** (Good partition).: RCLUMB does a "good partition" at \(t\), if the cluster \(\overline{V}_{t}\) assigned to \(i_{t}\) is a \(\zeta\)-good cluster, and it contains all the users in the same _ground-truth cluster_ as \(i_{t}\), i.e.,

\[\left\|\boldsymbol{\theta}_{i_{t}}-\boldsymbol{\theta}_{\ell}\right\|_{2}\leq \zeta,\forall\ell\in\overline{V}_{t}\,,\text{and}\,V_{j(i_{t})}\subseteq \overline{V}_{t}\,. \tag{9}\]

Note that when the algorithm does a "good partition" at \(t\), \(\overline{V}_{t}\) will contain all the users in the same _ground-truth cluster_ as \(i_{t}\) and may only contain some other \(\zeta\)-close users with respect to \(i_{t}\), which means the gathered information associated with \(\overline{V}_{t}\) can be used to infer user \(i_{t}\)'s preference with high accuracy. Also, it is obvious that under a "good partition", if \(\overline{V}_{t}\in\mathcal{V}\), then \(\overline{V}_{t}=V_{j(i_{t})}\) by definition.

Next, we give a sketched proof for Theorem 5.3.

Proof.: **[Sketch for Theorem 5.3]** The proof mainly contains two parts. First, we prove there is a sufficient time \(T_{0}\) for RCLUMB to get a "good partition" with high probability. Second, we prove the regret upper bound for RCLUMB after maintaining a "good partition". The most challenging part is to bound the regret caused by _mischustering_\(\zeta\)-close users after getting a "good partition".

**1. Sufficient time to maintain a "good partition".** With the item regularity (Assumption 3.3), we can prove after some \(T_{0}\) (defined in Lemma H.1 in Appendix H), RCLUMB will always have a"good partition". Specifically, after \(t\geq O\big{(}u\left(\frac{d}{\lambda_{x}(\gamma_{1}-\zeta)^{2}}+\frac{1}{\lambda_{ x}^{2}}\right)\log T\big{)}\), for any user \(i\in\mathcal{U}\), the gap between the estimated \(\hat{\mathbf{\theta}}_{i,t}\) and the ground-truth \(\mathbf{\theta}^{j(i)}\) is less than \(\frac{\gamma_{1}}{\lambda}\) with high probability. With this, we can get: for any two users \(i\) and \(\ell\), if their gap is greater than \(\zeta\), it will trigger the deletion of the edge \((i,\ell)\) (Line 10 of Algo.1) with high probability; on the other hand, when the deletion condition of the edge \((i,\ell)\) is satisfied, then \(\left\|\mathbf{\theta}^{j(i)}-\mathbf{\theta}^{j(\ell)}\right\|_{2}>0\), which means user \(i\) and \(\ell\) belong to different _ground-truth clusters_ by Assumption 3.1 with high probability. Therefore, we can get that with high probability, all those users in the same _ground-truth cluster_ as \(i_{t}\) will be directly connected with \(i_{t}\), and users directly connected with \(i_{t}\) must be \(\zeta\)-close to \(i_{t}\). By filtering users directly linked with \(i_{t}\) as the cluster \(\overline{V}_{t}\) (Algo.1 Line 5) and the definition of "good partition", we can ensure that RCLUMB will keep a "good partition" afterward with high probability.

**2. Bounding the regret after getting a "good partition".** After \(T_{0}\), with the "good partition", we can prove the following lemma that gives a bound of the difference between \(\hat{\mathbf{\theta}}_{\overline{V}_{t},t-1}\) and ground-truth \(\mathbf{\theta}_{i_{t}}\) in direction of action vector \(\mathbf{x}_{a}\), and supports the design of the confidence radius \(C_{a,t}\) in Eq.(5).

**Lemma 5.6**.: _With probability at least \(1-5\delta\) for some \(\delta\in(0,\frac{1}{5})\), \(\forall t\geq T_{0}\)_

\[\left|\mathbf{x}_{a}^{\top}(\mathbf{\theta}_{i_{t}}-\hat{\mathbf{\theta}}_{ \overline{V}_{t},t-1})\right|\leq\frac{\epsilon_{*}\sqrt{2d}}{\hat{\lambda}_{ x}^{\frac{3}{2}}}1\{\overline{V}_{t}\notin\mathcal{V}\}+\epsilon_{*}\sum_{ \begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\left|\mathbf{x}_{a}^{\top}\overline{M}_{ \overline{V}_{t},t-1}^{-1}\mathbf{x}_{a_{s}}\right|+\beta\left\|\mathbf{x}_{a}\right\| _{\overline{M}_{\overline{V}_{t},t-1}^{-1}}.\]

To prove this lemma, we consider the following two situations.

**(i) Assigning a perfect cluster for \(i_{t}\).** In this case, \(\overline{V}_{t}\in\mathcal{V}\), meaning the cluster assigned for user \(i_{t}\) is the same as her _ground-truth cluster_, i.e., \(\overline{V}_{t}=V_{j(i_{t})}\). Therefore, we have that \(\forall\ell\in\overline{V}_{t},\mathbf{\theta}_{\ell}=\mathbf{\theta}_{i_{t}}\). With careful analysis, we can bound \(\left|\mathbf{x}_{a}^{\top}(\mathbf{\theta}_{i_{t}}-\hat{\mathbf{\theta}}_{\overline{V}_{t },t-1})\right|\) by \(C_{a,t}\) (defined in Eq.(5)).

**(ii) Bounding the term of _misclustering_\(i_{t}\)'s \(\zeta\)-close users.** In this case, \(\overline{V}_{t}\notin\mathcal{V}\), meaning the algorithm _misclusters_ user \(i_{t}\), i.e., \(\overline{V}_{t}\neq V_{j(i_{t})}\). Thus, we do not have \(\forall\ell\in\overline{V}_{t},\mathbf{\theta}_{\ell}=\mathbf{\theta}_{i_{t}}\) anymore, but we have all the users in \(\overline{V}_{t}\) are \(\zeta\)-close to \(i_{t}\) (by "good partition"), i.e., \(\left\|\mathbf{\theta}_{i_{t}}-\mathbf{\theta}_{i_{t}}\right\|_{2}\leq\zeta,\forall \ell\in\overline{V}_{t}\). Then an additional term can be caused by using the information of \(i_{t}\)'s \(\zeta\)-close users in \(\overline{V}_{t}\) lying in different _ground-truth clusters_ from \(i_{t}\) to estimate \(\mathbf{\theta}_{i_{t}}\). It is highly challenging to bound this part.

We will get an extra term \(\left|\mathbf{x}_{a}^{\top}\overline{M}_{\overline{V}_{t},t-1}^{-1}\sum_{ \begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top}( \mathbf{\theta}_{i_{s}}-\mathbf{\theta}_{i_{t}})\right|\) when bounding the regret in this case, where \(\left\|\mathbf{\theta}_{\ell}-\mathbf{\theta}_{i_{t}}\right\|_{2}\leq\zeta,\forall \ell\in\overline{V}_{t}\). It is an easy-to-be-made mistake to directly drag \(\left\|\mathbf{\theta}_{i_{t}}-\mathbf{\theta}_{i_{t}}\right\|_{2}\) out to bound it by \(\left\|\mathbf{x}_{a}^{\top}\overline{M}_{\overline{V}_{t},t-1}^{-1}\sum_{ \begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top} \right\|_{2}\times\zeta\). With subtle analysis, we propose the following lemma to bound the above term.

**Lemma 5.7** (Bound of error caused by _misclustering_).: \(\forall t\geq T_{0}\)_, if the current partition by RCLUMB is a "good partition", and \(\overline{V}_{t}\notin\mathcal{V}\), then for all \(\mathbf{x}_{a}\in\mathbb{R}^{d},\left\|\mathbf{x}_{a}\right\|_{2}\leq 1\), with probability at least \(1-\delta\):_

\[\left|\mathbf{x}_{a}^{\top}\overline{M}_{\overline{V}_{t},t-1}^{-1}\sum_{ \begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top}( \mathbf{\theta}_{i_{s}}-\mathbf{\theta}_{i_{t}})\right|\leq\frac{\epsilon_{*}\sqrt{2d} }{\hat{\lambda}_{x}^{\frac{3}{2}}}.\]

This lemma is quite general. Please see Appendix G for details about its proof.

The expected occurrences of \(\{\overline{V}_{t}\notin\mathcal{V}\}\) is bounded by \(\frac{\hat{u}}{u}T\) with Assumption 3.2, Definition 5.2 and 5.5. The result follows by bounding the expected sum of the bounds for the instantaneous regret using Lemma 5.6 with delicate analysis due to the time-varying clustering structure kept by RCLUMB. 

## 6 Experiments

This section compares RCLUMB and RSCLUMB with CLUB [12], SCLUB [27], LinUCB with a single estimated vector for all users, LinUCB-Ind with separate estimated vectors for each user, and two modifications of LinUCB in [23] which we name as RLinUCB and RLinUCB-Ind. We use averaged reward as the evaluation metric, where the average is taken over ten independent trials.

### Synthetic Experiments

We consider a setting with \(u=1,000\) users, \(m=10\) clusters and \(T=10^{6}\) rounds. The preference and feature vectors are in \(d=50\) dimension with each entry drawn from a standard Gaussian distribution, and are normalized to vectors with \(\|.\|_{2}=1\)[27]. We fix an arm set with \(|\mathcal{A}|=1000\) items, at each round \(t\), 20 items are randomly selected to form a set \(\mathcal{A}_{t}\) for the user to choose from. We construct a matrix \(\mathbf{\epsilon}\in\mathbb{R}^{1,000\times 1,000}\) in which each element \(\mathbf{\epsilon}(i,j)\) is drawn uniformly from the range \((-0.2,0.2)\) to represent the deviation. At \(t\), for user \(i_{t}\) and the item \(a_{t}\), \(\mathbf{\epsilon}(i_{t},a_{t})\) will be added to the feedback as the deviation, which corresponds to the \(\mathbf{\epsilon}^{i_{t},t}_{a_{t}}\) defined in Eq.(1).

The result is provided in Figure 1(a), showing that our algorithms have clear advantages: RCLUMB improves over CLUB by 21.9%, LinUCB by 194.8%, LinUCB-Ind by 20.1%, SCLUB by 12.0%, RLinUCB by 185.2% and RLinUCB-Ind by 10.6%. The performance difference between RCLUMB and RSCLUMB is very small as expected. RLinUCB performs better than LinUCB; RLinUCB-Ind performs better than LinUCB-Ind and CLUB, showing that the modification of the recommendation policy is effective. The set-based RSCLUMB and SCLUB can separate clusters quicker and have advantages in the early period, but eventually RCLUMB catches up with RSCLUMB, and SCLUB is surpassed by RLinUCB-Ind because it does not consider misspecifications. RCLUMB and RSCLUMB perform better than RLinUCB-Ind, which shows the advantage of the clustering. So it can be concluded that both the modification for misspecification and the clustering structure are critical to improving the algorithm's performance. We also have done some ablation experiments on different scales of \(\epsilon^{*}\) in Appendix P, and we can notice that under different \(\epsilon^{*}\), our algorithms always outperform the baselines, and some baselines will perform worse as \(\epsilon^{*}\) increases.

### Experiments on Real-world Datasets

We conduct experiments on the Yelp data and the \(20m\) MovieLens data [17]. For both data, we have two cases due to the different methods for generating feedback. For case 1, we extract 1,000 items with most ratings and 1,000 users who rate most; then we construct a binary matrix \(\mathbf{H}^{1,000\times 1,000}\) based on the user rating [40; 42]: if the user rating is greater than 3, the feedback is 1; otherwise, the feedback is 0. Then we use this binary matrix to generate the preference and feature vectors by singular-value decomposition (SVD) [27; 25; 40]. Similar to the synthetic experiment, we construct a matrix \(\mathbf{\epsilon}\in\mathbb{R}^{1,000\times 1,000}\) in which each element is drawn uniformly from the range \((-0.2,0.2)\). For case 2, we extract 1,100 users who rate most and 1000 items with most ratings. We construct a binary feedback matrix \(\mathbf{H}^{1,100\times 1,000}\) based on the same rule as case 1. Then we select the first 100 rows \(\mathbf{H}^{100\times 1,000}_{1}\) to generate the feature vectors by SVD. The remaining 1,000 rows \(\mathbf{F}^{1,000\times 1,000}\)

Figure 1: The figures compare RCLUMB and RSCLUMB with the baselines. (a) shows the result on synthetic data, (b) and (c) show the results on Yelp dataset, (d) and (e) show the results on Movielens dataset. All experiments are under the setting of \(u=1,000\) users, \(m=10\) clusters, and \(d=50\). All results are averaged under \(10\) random trials. The error bars are standard deviations divided by \(\sqrt{10}\).

is used as the feedback matrix, meaning user \(i\) receives \(\mathbf{F}(i,j)\) as feedback while choosing item \(j\). In both cases, at time \(t\), we randomly select \(20\) items for the algorithms to choose from. In case 1, the feedback is computed by the preference and feature vector with misspecification, in case 2, the feedback is from the feedback matrix.

The results on Yelp are shown in Fig 1(b) and Fig 1(c). In case 1, RCLUMB improves CLUB by 45.1%, SCLUB by 53.4%, LinUCB-One by 170.1%, LinUCB-Ind by 46.2%, RLinUCB by 171.0% and RLinUCB-Ind by 21.5%. In case 2, RCLUMB improves over CLUB by 13.9%, SCLUB by 5.1%, LinUCB-One by 135.6%, LinUCB-Ind by 10.1%, RLinUCB by 138.6% and RLinUCB by 8.5%. It is notable that our modeling assumption 3.4 is violated in case 2 since the misspecification range is unknown. We set \(\epsilon_{*}=0.2\) following our synthetic dataset and it can still perform better than other algorithms. When the misspecification level is known as in case 1, our algorithms' improvement is significantly enlarged, e.g., RCLUMB improves over SCLUB from 5.1% to 53.4%.

The results on Movielens are shown in Fig 1(d) and 1(e). In case 1, RCLUMB improves CLUB by 58.8%, SCLUB by 92.1%, LinUCB-One by 107.7%, LinUCB-Ind by 61.5 %, RLinUCB by 109.5%, and RLinUCB-Ind by 21.3%. In case 2, RCLUMB improves over CLUB by 5.5%, SCLUB by 2.9%, LinUCB-One by 28.5%, LinUCB-Ind by 6.1%, RLinUCB by 29.3% and RLinUCB-Ind by 5.8%. The results are consistent with the Yelp data, confirming our superior performance.

## 7 Conclusion

We present a new problem of clustering of bandits with misspecified user models (CBMUM), where the agent has to adaptively assign appropriate clusters for users under model misspecifications. We propose two robust CB algorithms, RCLUMB and RSCLUMB. Under milder assumptions than previous CB works, we prove the regret bounds of our algorithms, which match the lower bound asymptotically in \(T\) up to logarithmic factors, and match the state-of-the-art results in several degenerate cases. It is challenging to bound the regret caused by _mischustering_ users with close but not the same preference vectors and use inaccurate cluster-based information to select arms. Our analysis to bound this part of the regret is quite general and may be of independent interest. Experiments on synthetic and real-world data demonstrate the advantage of our algorithms. We would like to state some interesting future works: (1) Prove a tighter regret lower bound for CBMUM, (2) Incorporate recent model selection methods into our fundamental framework to design robust algorithms for CBMUM with unknown exact maximum model misspecification level, and (3) Consider the setting with misspecifications in the underlying user clustering structure rather than user models.

## 8 Acknowledgement

The corresponding author Shuai Li is supported by National Key Research and Development Program of China (2022ZD0114804) and National Natural Science Foundation of China (62376154, 62006151, 62076161). The work of John C.S. Lui was supported in part by the RGC's GRF 14215722.

## References

* [1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* [2] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47(2):235-256, 2002.
* [3] Yikun Ban and Jingrui He. Local clustering in contextual multi-armed bandits. In _Proceedings of the Web Conference 2021_, pages 2335-2346, 2021.
* [4] Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends(r) in Machine Learning_, 5(1):1-122, 2012.
* [5] Kechao Cai, Xutong Liu, Yu-Zhen Janice Chen, and John CS Lui. An online learning approach to network application optimization with guarantee. In _IEEE INFOCOM 2018-IEEE Conference on Computer Communications_, pages 2006-2014. IEEE, 2018.
* [6] Leonardo Cella and Massimiliano Pontil. Multi-task and meta-learning with sparse linear bandits. In _Uncertainty in Artificial Intelligence_, pages 1692-1702. PMLR, 2021.
* [7] Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear bandits. In _International Conference on Machine Learning_, pages 1360-1370. PMLR, 2020.
* [8] Leonardo Cella, Karim Lounici, Gregoire Pacetau, and Massimiliano Pontil. Multi-task representation learning with stochastic linear bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 4822-4847. PMLR, 2023.
* [9] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 208-214. JMLR Workshop and Conference Proceedings, 2011.
* [10] Aniket Anand Deshmukh, Urun Dogan, and Clay Scott. Multi-task learning for contextual bandits. _Advances in neural information processing systems_, 30, 2017.
* [11] Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert. Adapting to misspecification in contextual bandits. _Advances in Neural Information Processing Systems_, 33:11478-11489, 2020.
* [12] Claudio Gentile, Shuai Li, and Giovanni Zappella. Online clustering of bandits. In _International Conference on Machine Learning_, pages 757-765. PMLR, 2014.
* [13] Claudio Gentile, Shuai Li, Purushottam Kar, Alexandros Karatzoglou, Giovanni Zappella, and Evans Erute. On context-dependent clustering of bandits. In _International Conference on machine learning_, pages 1253-1262. PMLR, 2017.
* [14] Avishek Ghosh, Sayak Ray Chowdhury, and Aditya Gopalan. Misspecified linear bandits. In _Thirty-First AAAI Conference on Artificial Intelligence_, 2017.
* [15] Jens Hainmueller and Chad Hazlett. Kernel regularized least squares: Reducing misspecification bias with a flexible and interpretable machine learning approach. _Political Analysis_, 22(2):143-168, 2014.
* [16] Negar Hariri, Bamshad Mobasher, and Robin Burke. Context adaptation in interactive recommender systems. In _Proceedings of the 8th ACM Conference on Recommender Systems_, pages 41-48, 2014.
* [17] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. _Acm transactions on interactive intelligent systems (tiis)_, 5(4):1-19, 2015.
* [18] Joey Hong, Branislav Kveton, Manzil Zaheer, and Mohammad Ghavamzadeh. Hierarchical bayesian bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 7724-7741. PMLR, 2022.

* Huang et al. [2021] Ruiquan Huang, Weiqiang Wu, Jing Yang, and Cong Shen. Federated linear contextual bandits. _Advances in neural information processing systems_, 34:27057-27068, 2021.
* Kohli et al. [2013] Pushmeet Kohli, Mahyar Salek, and Greg Stoddard. A fast bandit algorithm for recommendation to users with heterogenous tastes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 27, pages 1135-1141, 2013.
* Kong et al. [2023] Fang Kong, Canzhe Zhao, and Shuai Li. Best-of-three-worlds analysis for linear bandits with follow-the-regularized-leader algorithm. _arXiv preprint arXiv:2303.06825_, 2023.
* Lattimore and Szepesvari [2020] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Lattimore et al. [2020] Tor Lattimore, Csaba Szepesvari, and Gellert Weisz. Learning with good feature representations in bandits and in rl with a generative model. In _International Conference on Machine Learning_, pages 5662-5670. PMLR, 2020.
* Li et al. [2010] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In _Proceedings of the 19th international conference on World wide web_, pages 661-670, 2010.
* Li and Zhang [2018] Shuai Li and Shengyu Zhang. Online clustering of contextual cascading bandits. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* Li et al. [2016] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. Collaborative filtering bandits. In _Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval_, pages 539-548, 2016.
* Li et al. [2019] Shuai Li, Wei Chen, Shuai Li, and Kwong-Sak Leung. Improved algorithm on online clustering of bandits. In _Proceedings of the 28th International Joint Conference on Artificial Intelligence_, IJCAI'19, page 2923-2929. AAAI Press, 2019. ISBN 9780999241141.
* Liu et al. [2022] Xutong Liu, Haoru Zhao, Tong Yu, Shuai Li, and John Lui. Federated online clustering of bandits. In _The 38th Conference on Uncertainty in Artificial Intelligence_, 2022.
* Liu et al. [2023] Xutong Liu, Jinhang Zuo, Siwei Wang, John CS Lui, Mohammad Hajiesmaili, Adam Wierman, and Wei Chen. Contextual combinatorial bandits with probabilistically triggered arms. In _International Conference on Machine Learning_, pages 22559-22593. PMLR, 2023.
* Liu et al. [2023] Xutong Liu, Jinhang Zuo, Hong Xie, Carlee Joe-Wong, and John CS Lui. Variance-adaptive algorithm for probabilistic maximum coverage bandits with general feedback. In _IEEE INFOCOM 2023-IEEE Conference on Computer Communications_, pages 1-10. IEEE, 2023.
* Pacchiano et al. [2020] Aldo Pacchiano, My Phan, Yasin Abbasi Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and Csaba Szepesvari. Model selection in contextual stochastic bandit problems. _Advances in Neural Information Processing Systems_, 33:10328-10337, 2020.
* Shi and Shen [2021] Chengshuai Shi and Cong Shen. Federated multi-armed bandits. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 9603-9611, 2021.
* Soare et al. [2014] Marta Soare, Ouais Alsharif, Alessandro Lazaric, and Joelle Pineau. Multi-task linear bandits. In _NIPS2014 workshop on transfer and multi-task learning: theory meets practice_, 2014.
* Wan et al. [2021] Runzhe Wan, Lin Ge, and Rui Song. Metadata-based multi-task bandits with bayesian hierarchical models. _Advances in Neural Information Processing Systems_, 34:29655-29668, 2021.
* Wan et al. [2023] Runzhe Wan, Lin Ge, and Rui Song. Towards scalable and robust structured bandits: A meta-learning framework. In _International Conference on Artificial Intelligence and Statistics_, pages 1144-1173. PMLR, 2023.
* Wang et al. [2021] Zhi Wang, Chicheng Zhang, Manish Kumar Singh, Laurel Riek, and Kamalika Chaudhuri. Multitask bandit learning through heterogeneous feedback aggregation. In _International Conference on Artificial Intelligence and Statistics_, pages 1531-1539. PMLR, 2021.

* [37] Zhi Wang, Chicheng Zhang, and Kamalika Chaudhuri. Thompson sampling for robust transfer in multi-task bandits. _arXiv preprint arXiv:2206.08556_, 2022.
* [38] Zhiyong Wang, Xutong Liu, Shuai Li, and John CS Lui. Efficient explorative key-term selection strategies for conversational contextual bandits. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 10288-10295, 2023.
* [39] Zhiyong Wang, Jize Xie, Tong Yu, Shuai Li, and John Lui. Online corrupted user detection and regret minimization. _arXiv preprint arXiv:2310.04768_, 2023.
* [40] Junda Wu, Canzhe Zhao, Tong Yu, Jingyang Li, and Shuai Li. Clustering of conversational bandits for user preference learning and elicitation. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 2129-2139, 2021.
* [41] Qingyun Wu, Huazheng Wang, Quanquan Gu, and Hongning Wang. Contextual bandits in a collaborative environment. In _Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval_, pages 529-538, 2016.
* [42] Shi Zong, Hao Ni, Kenny Sung, Nan Rosemary Ke, Zheng Wen, and Branislav Kveton. Cascading bandits for large-scale recommendation problems. _arXiv preprint arXiv:1603.05359_, 2016.

## Appendix A More Discussions on Related Work

In this section, we will give more comparisions and discussions on some previous works that are related to our work to some extent.

There are some other works on bandits leveraging user (or task) relations, which have some relations with the clustering of bandits (CB) works to some extent, but are in different lines of research from CB, and are quite different from our work. First, besides CB, the work [41] also leverages user relations. Specifically, it utilizes a _known_ user adjacency graph to share context and payoffs among neighbors, whereas in CB, the user relations are _unknown_ and need to be learnt, thus the setting differs a lot from CB. Second, there are lines of works on multi-task learning [6, 10, 33, 8, 37, 36], meta-learning [35, 18, 7] and federated learning [32, 19], where multiple different tasks are solved jointly and share information. Note that all of these works do not assume an underlying _unknown_ user clustering structure which needs to be inferred by the agent to speed up learning. For works on multi-task learning [6, 10, 33, 8, 37, 36], they assume the tasks are related but no user clustering structures, and to the best of our knowledge, none of them consider model misspecifications, thus differing a lot from ours. For some recent works on meta-learning [35, 18, 34], they propose general Bayesian hierarchical models to share knowledge across tasks, and design Thompson-Sampling-based algorithms to optimize the Bayes regret, which are quite different from the line of CB works, and differ a lot from ours. And additionally, as supported by the discussions in the works [7, 36], multi-task learning and meta-learning are different lines of research from CB. For the works on federated learning [32, 19], they consider the privacy and communication costs among multiple servers, whose setting is also very different from the previous CB works and our work.

**Remark.** Again, we emphasize that the goal of this work is to initialize the study of the important CBMUM problem, and propose general design ideas for dealing with model misspecifications in CB problems. Therefore, our study is based on fundamental models on CB [12, 27] and MLB [23], and the algorithm design ideas and theoretical analysis are pretty general. We leave incorporating the more recent model selection methods [31, 11] into our framework to address the unknown exact maximum model misspecification level as an interesting future work. It would also be interesting to consider incorporating our methods and ideas of tackling model misspecifications into the studies of multi-task learning, meta learning and federated learning.

## Appendix B More Discussions on Assumptions

All the assumptions (Assumptions 3.1,3.2,3.3,3.4)in this work are natural and basically follow (or less stringent than) previous works on CB and MLB [12, 25, 27, 28, 23].

### Less Stringent Assumption on the Generating Distribution of Arm Vectors

We also make some contributions to relax a widely-used but stringent assumption on the generating distribution of arm vectors. Specifically, our Assumption 3.3 on item regularity relaxes the previous one used in previous CB works [12, 25, 27, 28] by removing the condition that the variance should be upper bounded by \(\frac{\lambda^{2}}{8\log(4|\mathcal{A}_{t}|)}\). For technical details on this, please refer to the theoretical analysis and discussions in Appendix J.

### Discussions on Assumption 3.4 about Bounded Misspecification Level

This assumption follows [23]. Note that this \(\epsilon_{*}\) can be an upper bound on the maximum misspecification level, not the exact maximum itself. In real-world applications, the deviations are usually small [14], and we can set a relatively big \(\epsilon_{*}\) (e.g., 0.2) to be the upper bound. Our experimental results support this claim. As shown in our experimental results on real-data case 2, even when \(\epsilon_{*}\) is unknown, our algorithms still perform well by setting \(\epsilon_{*}=0.2\). Some recent studies [31, 11] use model selection methods to theoretically deal with unknown exact maximum misspecification level in the single-user case, which is not the emphasis of this work. Additionally, the work [11] assumes that the learning agent has access to a regression oracle. And for the work [31], though their regret boundis dependent on the exact maximum misspecification level that needs not to be known by the agent, an upper bound of the exact maximum misspecification level is still needed. We leave incorporating their methods to deal with unknown exact maximum misspecification level as an interesting future work.

Discussions on Assumption 3.2 about the Theoretical Results under General User Arrival Distributions

The uniform arrival in Assumption 3.2 follows previous CB works [12; 25; 28], it only affects the \(T_{0}\) term, which is the time after which the algorithm maintains a "good partition" and is of \(O(u\log T)\). For an arbitrary arrival distribution, \(T_{0}\) becomes \(O(1/p_{min}\log T)\), where \(p_{min}\) is the minimal arrival probability of a user. And since it is a lower-order term (of \(O(\log T)\)), it will not affect the main order of our regret upper bound which is of \(O(\epsilon_{*}T\sqrt{md\log T}+d\sqrt{mT}\log T)\). The work [27] studies arbitrary arrivals and aims to remove the \(1/p_{min}\) factor in this term, but their setting is different. They make an additional assumption that users in the same cluster not only have the same preference vector, but also the same arrival probability, which is different from our setting and other classic CB works [12; 25; 28] where we only assume users in the same cluster share the same preference vector.

## Appendix C Highlight of the Theoretical Analysis

Our proof flow and methodologies are novel in clustering of bandits (CB), which are expected to inspire future works on model misspecifications and CB. The main challenge of the regret analysis in CBMUM is that due to the estimation inaccuracy caused by misspecifications, it is impossible to cluster all users exactly correctly, and it is highly non-trivial to bound the regret caused by **"misclustering" \(\zeta\)-close users**.

To the best of our knowledge, the common proof flow of previous CB works (e.g., [12; 25; 28]) can be summarized in two steps: The first is to prove a sufficient time \(T_{0}^{\prime}\) after which the algorithms can cluster all users **exactly correctly** with high probability. Note that the inferred clustering structure remains static after \(T_{0}^{\prime}\), making the analysis easy. Second, after the **correct static clustering**, the regret can be trivially bounded by bounding \(m\) (number of underlying clusters) independent linear bandit algorithms, resulting in a \(O(d\sqrt{mT}\log T)\) regret.

The above common proof flow is straightforward in CB with perfectly linear models, but it would fail to get a non-vacuous regret bound for CBMUM. In CBMUM, it is impossible to learn an exactly correct static clustering structure with model misspecifications. In particular, we prove that we can only expect the algorithm to cluster \(\zeta\)-close users together rather than cluster all users exactly correctly. Therefore, the previous flow can not be applied to the more challenging CBMUM problem.

We do the following to address the challenges in obtaining a tight regret bound for CBMUM. With the carefully-designed novel key components of RCLUMB, we can prove a sufficient time \(T_{0}\) after which RCLUMB can get a "good partition" (Definition 5.5) with high probability, which means the cluster \(\overline{V_{t}}\) assigned to \(i_{t}\) contains all users in the same ground-truth cluster as \(i_{t}\), and possibly some other \(i_{t}\)'s \(\zeta\)-close users. Intuitively, after \(T_{0}\), the algorithm can leverage all the information from the users' ground-truth clusters but may misuse some information from other \(\zeta\)-close users with preference gaps up to \(\zeta\), causing a regret of **"misclustering" \(\zeta\)-close users**. It is highly non-trivial to bound this part of regret, and the proof methods would be beneficial for future studies in CB in challenging cases when it is impossible to cluster all users exactly correctly. For details, please refer to the discussions "(ii) Bounding the term of misclustering it's \(\zeta\)-close users" in Section 5, the key Lemma 5.7 (Bound of error caused by misclustering), its proof and tightness discussion in Appendix G. Also, a more subtle analysis is needed to handle the time-varying inferred clustering structure since the "good partition" may change over time, whereas in the previous CB works, the clustering structure remains static after \(T_{0}^{\prime}\). For theoretical details on this, please refer to Appendix E.

Appendix D Discussions on why Trivially Combining Existing CB and MLB Works Could Not Achieve a Non-vacuous Regret Upper Bound

We consider discussing regret upper bounds for CB without considering misspecifications for three cases: (1) neither the clustering process nor the decision process considers misspecifications (previousCB algorithms); (2) the decision process does not consider misspecifications; (3) the clustering process does not consider misspecifications.

For cases (1) and (2), the decision process could contribute to the leading regret. We consider the case where there are \(m\) underlying clusters, with each cluster's arrival being \(T/m\), and the agent knows the underlying clustering structure. For this case, there exist some instances where the regret upper bound \(R(T)\) is strictly larger than \(\epsilon_{*}T\sqrt{m\log T}\) asymptotically in \(T\). Formally, in the discussion of "Failure of unmodified algorithm" in Appendix E in [23], they give an example to show that in the single-user case, the regret \(R_{1}(T)\) of the classic linear bandit algorithms without considering misspecifications will have: \(\lim\limits_{T\rightarrow+\infty}\frac{R_{1}(T)}{\epsilon_{*}T\sqrt{m\log T}}=+\infty\). In our problem with multiple users and \(m\) underlying clusters, even if we know the underlying clustering structure and keep \(m\) independent linear bandit algorithms with \(T_{i}\) for the cluster \(i\in[m]\) to leverage the common information of clusters, the best we can get is \(R_{2}(T)=\sum_{i\in[m]}R_{1}(T_{i})\). By the above results, if the decision process does not consider misspecifications, we have \(\lim\limits_{T\rightarrow+\infty}\frac{R_{2}(T)}{\epsilon_{*}T\sqrt{m\log T}}= \lim\limits_{T\rightarrow+\infty}\frac{mR_{1}(T/m)}{\epsilon_{*}T\sqrt{m\log T }}=+\infty\). Recall that the regret upper bound \(R(T)\) of our proposed algorithms is of \(O(\epsilon_{*}T\sqrt{md\log T}+d\sqrt{mT}\log T)\) (thus, we have \(\lim\limits_{T\rightarrow+\infty}\frac{R(T)}{\epsilon_{*}T\sqrt{m\log T}}<+\infty\)), which gives a proof that that the regret upper bound of our proposed algorithms is asymptotically much better than CB algorithms in cases (1)(2).

For case (3), if the clustering process does not use the more tolerant deletion rule in Line 10 of Algo.1, the gap between users linked by edges would possibly exceed \(\zeta\) (\(\zeta=2\epsilon_{*}\sqrt{\frac{2}{\lambda_{*}}}\)) even after \(T_{0}\), which will result in a regret upper bound no better than \(O(\epsilon_{*}u\sqrt{d}T)\). As the number of users \(u\) is usually huge in practice, this result is vacuous. The reasons for getting the above claim are as follows. Even if the clustering process further uses our deletion rule considering misspecifications, and the users linked by edges are within \(\zeta\) distance, failing to extract \(1\)-hop users (Line 5 in Algo.1) would cause the leading \(O(\epsilon_{*}u\sqrt{d}T)\) regret term, as in the worst case, the preference vector \(\theta\) of the user in \(\tilde{V}_{t}\) who is \(h\)-hop away from user \(i_{t}\) could deviate by \(h\zeta\) from \(\theta_{i_{t}}\), where \(h\) can be as large as \(u\), and it would make the second term in Eq.(8) a \(O(\epsilon_{*}u\sqrt{d}T)\) term. If we completely do not consider the misspecifications in the clustering process, the above user gap between users linked by edges would possibly exceed \(\zeta\), which will cause a regret upper bound worse than \(O(\epsilon_{*}u\sqrt{d}T)\).

## Appendix E Proof of Theorem 5.3

We first prove the result in the case when \(\gamma_{1}\) defined in Definition 5.1 is not infinity, i.e., \(4\epsilon_{*}\sqrt{\frac{2}{\lambda_{*}}}<\gamma_{1}<\infty\). The proof of the special case when \(\gamma_{1}=\infty\) will directly follow the proof of this case.

For the instantaneous regret \(R_{t}\) at round \(t\), with probability at least \(1-5\delta\) for some \(\delta\in(0,\frac{1}{5})\), at \(\forall t\geq T_{0}\):

\[R_{t} =(\mathbf{x}_{a_{t}^{*}}^{\top}\mathbf{\theta}_{i_{t}}+\mathbf{e}_{a_{t}^{*}} ^{i_{t},t})-(\mathbf{x}_{a_{t}}^{\top}\mathbf{\theta}_{i_{t}}+\mathbf{e}_{a_{t}^{*},t}^{i _{t},t}) \tag{10}\] \[=\mathbf{x}_{a_{t}^{*}}^{\top}(\mathbf{\theta}_{i_{t}}-\mathbf{\hat{\theta}}_ {\mathbf{V}_{t},t-1})+(\mathbf{x}_{a_{t}^{*}}^{\top}\mathbf{\hat{\theta}}_{\mathbf{V}_{t},t-1 }+C_{a_{t}^{*},t})-(\mathbf{x}_{a_{t}}^{\top}\mathbf{\hat{\theta}}_{\mathbf{V}_{t},t-1}+C_ {a_{t},t})\] \[\quad+\mathbf{x}_{a_{t}}^{\top}(\mathbf{\hat{\theta}}_{\mathbf{V}_{t},t-1}- \mathbf{\theta}_{i_{t}})+C_{a_{t},t}-C_{a_{t}^{*},t}+(\mathbf{e}_{a_{t}^{*}}^{i_{t},t}- \mathbf{e}_{a_{t}}^{i_{t},t})\] \[\leq 2C_{a_{t},t}+\frac{2\epsilon_{*}\sqrt{2d}}{\tilde{\lambda}_ {x}^{\frac{2}{3}}}\mathbb{I}\{\overline{V}_{t}\notin\mathcal{V}\}+2\epsilon_{ *}\,,\]

where the last inequality holds by the UCB arm selection strategy in Eq.(5), the concentration bound given in Lemma 5.6, and the fact that \(\left\|\mathbf{\epsilon}^{i,t}\right\|_{\infty}\leq\epsilon_{*},\forall i\in \mathcal{U},\forall t\).

We define the following events. Let

\[\mathcal{E}_{0} =\{R_{t}\leq 2C_{a_{t},t}+\frac{2\epsilon_{*}\sqrt{2d}}{\tilde{ \lambda}_{x}^{\frac{2}{3}}}\mathbb{I}\{\overline{V}_{t}\notin\mathcal{V}\}+2 \epsilon_{*},\text{for all }\{t:t\geq T_{0},\text{and the algorithm maintains a "good partition" at }t\}\}\,,\] \[\mathcal{E}_{1} =\{\text{the algorithm maintains a "good partition" for all }t\geq T_{0}\}\,,\] \[\mathcal{E} =\mathcal{E}_{0}\cap\mathcal{E}_{1}\,.\]\(\mathbb{P}(\mathcal{E}_{0})\geq 1-2\delta\). According to Lemma H.1, \(\mathbb{P}(\mathcal{E}_{1})\geq 1-3\delta\). Thus, \(\mathbb{P}(\mathcal{E})\geq 1-5\delta\) for some \(\delta\in(0,\frac{1}{5})\). Take \(\delta=\frac{1}{T}\), we can get that

\[\mathbb{E}[R(T)] =\mathbb{P}(\mathcal{E})\mathbb{I}\{\mathcal{E}\}R(T)+\mathbb{P}( \overline{\mathcal{E}})\mathbb{I}\{\overline{\mathcal{E}}\}R(T)\] \[\leq\mathbb{I}\{\mathcal{E}\}R(T)+5\times\frac{1}{T}\times T \tag{11}\] \[=\mathbb{I}\{\mathcal{E}\}R(T)+5\,,\]

where \(\overline{\mathcal{E}}\) denotes the complementary event of \(\mathcal{E}\), \(\mathbb{I}\{\mathcal{E}\}R(T)\) denotes \(R(T)\) under event \(\mathcal{E}\), \(\mathbb{I}\{\overline{\mathcal{E}}\}R(T)\) denotes \(R(T)\) under event \(\overline{\mathcal{E}}\), and we use \(R(T)\leq T\) to bound \(R(T)\) under event \(\overline{\mathcal{E}}\).

Then it remains to bound \(\mathbb{I}\{\mathcal{E}\}R(T)\):

\[\mathbb{I}\{\mathcal{E}\}R(T) \leq R(T_{0})+\mathbb{E}[\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1 }^{T}R_{t}]\] \[\leq T_{0}+2\mathbb{E}[\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1} ^{T}C_{a_{t},t}]+\frac{2\epsilon_{*}\sqrt{2d}}{\hat{\lambda}_{x}^{\frac{3}{2}} }\sum_{t=T_{0}+1}^{T}\mathbb{E}[\mathbb{I}\{\mathcal{E},\overline{V}_{t}\notin \mathcal{V}\}]+2\epsilon_{*}T \tag{12}\] \[=T_{0}+2\mathbb{E}[\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1}^{T}C _{a_{t},t}]+\frac{2\epsilon_{*}\sqrt{2d}}{\hat{\lambda}_{x}^{\frac{3}{2}}} \sum_{t=T_{0}+1}^{T}\mathbb{P}(\mathbb{I}\{\mathcal{E},\overline{V}_{t}\notin \mathcal{V}\})+2\epsilon_{*}T\] \[\leq T_{0}+2\mathbb{E}[\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1} ^{T}C_{a_{t},t}]+\frac{2\epsilon_{*}\sqrt{2d}}{\hat{\lambda}_{x}^{\frac{3}{2}} }\times\frac{\tilde{u}}{u}T+2\epsilon_{*}T\,, \tag{13}\]

where Eq.(12) follows from Eq.(10). Eq.(13) holds since under Assumption 3.2 about user arrival uniformness and by Definition 5.5 of "good partition", \(\mathbb{P}(\mathbb{I}\{\mathcal{E},\overline{V}_{t}\notin\mathcal{V}\})\leq \frac{\tilde{u}}{u},\forall t\geq T_{0}\), where \(\tilde{u}\) is defined in Definition 5.2.

Then we need to bound \(\mathbb{E}[\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1}^{T}C_{a_{t},t}]\):

\[\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1}^{T}C_{a_{t},t} =\Big{(}\sqrt{\lambda}+\sqrt{2\log(\frac{1}{\delta})+d\log(1+ \frac{T}{\lambda d})}\Big{)}\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1}^{T}\| \boldsymbol{x}_{a_{t}}\|_{\overline{\boldsymbol{M}}_{\overline{\boldsymbol{V }}_{t,t-1}}^{-1}}\] \[+\mathbb{I}\{\mathcal{E}\}\epsilon_{*}\sum_{t=T_{0}+1}^{T}\sum_{ \begin{subarray}{c}\epsilon\in(t-1)\\ i_{*}\in\overline{\boldsymbol{V}}_{t}\end{subarray}}\Big{|}\boldsymbol{x}_{a_{ t}}^{\top}\overline{\boldsymbol{M}}_{\overline{\boldsymbol{V}}_{t,t-1}}^{-1} \boldsymbol{x}_{a_{s}}\Big{|}. \tag{14}\]

Next, we bound the \(\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1}^{T}\|\boldsymbol{x}_{a_{t}}\|_{ \overline{\boldsymbol{M}}_{\overline{\boldsymbol{V}}_{t,t-1}}^{-1}}\) term in Eq.(14):

\[\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1}^{T}\|\boldsymbol{x}_{ a_{t}}\|_{\overline{\boldsymbol{M}}_{\overline{\boldsymbol{V}}_{t,t-1}}^{-1}} =\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1}^{T}\sum_{k=1}^{m} \mathbb{I}\{i_{t}\in\tilde{V}_{t,k}^{\prime}\}\,\|\boldsymbol{x}_{a_{t}}\|_{ \overline{\boldsymbol{M}}_{\overline{\boldsymbol{V}}_{t,k}^{\prime},t-1}^{-1}} \tag{15}\] \[\leq\mathbb{I}\{\mathcal{E}\}\sum_{t=T_{0}+1}^{T}\sum_{j=1}^{m} \mathbb{I}\{i_{t}\in V_{j}\}\,\|\boldsymbol{x}_{a_{t}}\|_{\overline{\boldsymbol{ M}}_{\overline{\boldsymbol{V}}_{j,t-1}}^{-1}}\] \[\leq\mathbb{I}\{\mathcal{E}\}\sum_{j=1}^{m}\sqrt{\sum_{t=T_{0}+1 }^{T}\mathbb{I}\{i_{t}\in V_{j}\}\sum_{t=T_{0}+1}^{T}\mathbb{I}\{i_{t}\in V_{j }\}\,\|\boldsymbol{x}_{a_{t}}\|_{\overline{\boldsymbol{M}}_{\overline{V}_{j,t -1}}^{-1}}^{2}}\] (16) \[\leq\mathbb{I}\{\mathcal{E}\}\sum_{j=1}^{m}\sqrt{2T_{V_{j},T}d\log (1+\frac{T}{\lambda d})}\] (17) \[\leq\mathbb{I}\{\mathcal{E}\}\sqrt{2\sum_{j=1}^{m}1\sum_{j=1}^{m} T_{V_{j},T}d\log(1+\frac{T}{\lambda d})}=\mathbb{I}\{\mathcal{E}\}\sqrt{2mdT \log(1+\frac{T}{\lambda d})}, \tag{18}\]where we use \(m_{t}\) to denote the number of connected components partitioned by the algorithm at \(t\), \(\tilde{V}^{\prime}_{t,k},k\in[m_{t}]\) to denote the connected components partitioned by the algorithm at \(t\), \(\overline{V}^{\prime}_{t,k}\subseteq\tilde{V}^{\prime}_{t,k}\) to denote the subset extracted to be the cluster \(\overline{V}_{t}\) for \(i_{t}\) from \(\tilde{V}^{\prime}_{t,k}\) conditioned on \(i_{t}\in\tilde{V}^{\prime}_{t,k}\), and \(T_{V_{j},T}\) to denote the number of times that the served users lie in the _ground-truth cluster_\(V_{j}\) up to time \(T\), i.e., \(T_{V_{j},T}=\sum_{t\in[T]}\mathbb{I}\{i_{t}\in V_{j}\}\).

The reasons for having Eq.(15) are as follows. Under event \(\mathcal{E}\), the algorithm will always have a "good partition" after \(T_{0}\). By Definition 5.5 and the proof process of Lemma H.1 about the edge deletion conditions, we can get \(m_{t}\leq m\) and if \(i_{t}\in\tilde{V}^{\prime}_{t,k},i_{t}\in V_{j}\), then \(V_{j}\subseteq\overline{V}^{\prime}_{t,k}\) since \(\overline{V}^{\prime}_{t,k}\) contains \(V_{j}\) and possibly other _ground-truth clusters_\(V_{n},n\in[m]\), whose preference vectors are \(\zeta\)-close to \(\boldsymbol{\theta}^{j}\). Therefore, by the definition of the regularized Gramian matrix, we can get \(M_{\overline{V}^{\prime}_{t,k},t-1}\succeq M_{V_{j},t-1},\forall t\geq T_{0}+1\). Thus by the above reasoning, \(\sum_{k=1}^{m_{t}}\mathbb{I}\{i_{t}\in\tilde{V}^{\prime}_{t,k}\}\|\boldsymbol{ x}_{a_{t}}\|_{\overline{\overline{M}^{-1}_{\overline{V}^{\prime}_{t,k},t-1}}} \leq\sum_{j=1}^{m}\mathbb{I}\{i_{t}\in V_{j}\}\left\|\boldsymbol{x}_{a_{t}} \right\|_{\overline{\overline{M}}^{-1}_{V_{j,t-1}}},\forall t\geq T_{0}+1\). Eq.(16) holds by the Cauchy-Schwarz inequality; Eq.(17) follows by the following technical Lemma J.2. Eq.(18) is from the Cauchy-Schwarz inequality and the fact that \(\sum_{j=1}^{m}T_{V_{j},T}=T\).

We then bound the last term in Eq.(14):

\[\mathbb{I}\{\mathcal{E}\}\epsilon_{*}\sum_{t=T_{0}+1}^{T}\sum_{ \stackrel{{ s\in[t-1]}}{{i_{s}\in\overline{V}_{t}}}}\left| \boldsymbol{x}_{a_{t}}^{\top}\overline{\boldsymbol{M}}\overline{V}_{t,t-1}^{ -1}\boldsymbol{x}_{a_{s}}\right| =\mathbb{I}\{\mathcal{E}\}\epsilon_{*}\sum_{t=T_{0}+1}^{T}\sum_{ k=1}^{m_{t}}\mathbb{I}\{i_{t}\in\tilde{V}^{\prime}_{t,k}\}\sum_{ \stackrel{{ s\in[t-1]}}{{i_{s}\in\overline{V}^{\prime}_{t,k}}}} \left|\boldsymbol{x}_{a_{t}}^{\top}\overline{\boldsymbol{M}}\overline{V}_{t,k,t-1}^{-1}\boldsymbol{x}_{a_{s}}\right|\] \[\leq\mathbb{I}\{\mathcal{E}\}\epsilon_{*}\sum_{t=T_{0}+1}^{T} \sum_{k=1}^{m_{t}}\mathbb{I}\{i_{t}\in\tilde{V}^{\prime}_{t,k}\}\sqrt{\sum_{ \stackrel{{ s\in[t-1]}}{{i_{s}\in\overline{V}^{\prime}_{t,k}}}} 1\sum_{\stackrel{{ s\in[t-1]}}{{i_{s}\in\overline{V}^{\prime}_{t,k }}}}\left|\boldsymbol{x}_{a_{t}}^{\top}\overline{\boldsymbol{M}}\overline{V}_{t,k,t-1}^{-1}\boldsymbol{x}_{a_{s}}\right|^{2}} \tag{19}\] \[\leq\mathbb{I}\{\mathcal{E}\}\epsilon_{*}\sum_{t=T_{0}+1}^{T} \sum_{k=1}^{m_{t}}\mathbb{I}\{i_{t}\in\tilde{V}^{\prime}_{t,k}\}\sqrt{\overline {V}_{t_{t,k},t-1}^{\prime}\left\|\boldsymbol{x}_{a_{t}}\right\|_{\overline{ \boldsymbol{M}}^{-1}_{\overline{V}^{\prime}_{t,k},t-1}}^{2}}\] (20) \[\leq\mathbb{I}\{\mathcal{E}\}\epsilon_{*}\sum_{t=T_{0}+1}^{T} \sqrt{\sum_{k=1}^{m_{t}}\mathbb{I}\{i_{t}\in\tilde{V}^{\prime}_{t,k}\}\sum_{k=1} ^{m_{t}}\mathbb{I}\{i_{t}\in\tilde{V}^{\prime}_{t,k}\}T_{\overline{V}^{\prime} _{t,k},t-1}\left\|\boldsymbol{x}_{a_{t}}\right\|_{\overline{\boldsymbol{M}}^{- 1}_{\overline{V}^{\prime}_{t,k},t-1}}^{2}}\] (21) \[\leq\mathbb{I}\{\mathcal{E}\}\epsilon_{*}\sqrt{T}\sqrt{\sum_{t=T_ {0}+1}^{T}1\sum_{t=T_{0}+1}^{T}\sum_{k=1}^{m_{t}}\mathbb{I}\{i_{t}\in\tilde{V}^{ \prime}_{t,k}\}\left\|\boldsymbol{x}_{a_{t}}\right\|_{\overline{\boldsymbol{M}}^ {-1}_{\overline{V}^{\prime}_{t,k},t-1}}^{2}}\] (22) \[\leq\mathbb{I}\{\mathcal{E}\}\epsilon_{*}\sqrt{T}\sqrt{T\sum_{t= T_{0}+1}^{T}\sum_{j=1}^{m}\mathbb{I}\{i_{t}\in V_{j}\}\left\|\boldsymbol{x}_{a_{t}} \right\|_{\overline{\boldsymbol{M}}^{-1}_{V_{j,t-1}}}^{2}}\] (23) \[=\mathbb{I}\{\mathcal{E}\}\epsilon_{*}T\sqrt{\sum_{j=1}^{m}\sum_{ t=T_{0}+1}^{T}\mathbb{I}\{i_{t}\in V_{j}\}\left\|\boldsymbol{x}_{a_{t}}\right\|_{ \overline{\boldsymbol{M}}^{-1}_{V_{j,t-1}}}^{2}}\] \[\leq\mathbb{I}\{\mathcal{E}\}\epsilon_{*}T\sqrt{2md\log(1+\frac{T }{\lambda d})}\,, \tag{25}\]where Eq.(19), Eq.(21) and Eq.(23) hold because of the Cauchy-Schwarz inequality, Eq.(20) holds since \(\overline{\mathbf{M}}_{\overline{\mathbf{V}}^{\prime}_{t,k},t-1}\succeq\sum_{\begin{subarray} {c}s\in[t-1]\\ i_{s}\in\overline{\mathbf{V}}_{t,k}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{ \top}\), Eq.(22) is because \(T_{\overline{\mathbf{V}}^{\prime}_{t,k},t-1}\leq T\), Eq. (24) follows from the same reasoning as Eq.(15), and Eq.(25) comes from the following technical Lemma J.2.

Finally, plugging Eq.(18) and Eq.(25) into Eq.(14), take expectation and plug it into Eq.(13), we can get:

\[R(T)\leq 5+T_{0}+\frac{\tilde{u}}{u}\times\frac{2\epsilon_{*}\sqrt{2d}T}{ \tilde{\lambda}_{x}^{\frac{3}{2}}}+2\epsilon_{*}T\bigg{(}1+\sqrt{2md\log(1+ \frac{T}{\lambda d})}\bigg{)}\] \[+2\bigg{(}\sqrt{\lambda}+\sqrt{2\log(T)+d\log(1+\frac{T}{\lambda d })}\bigg{)}\times\sqrt{2mdT\log(1+\frac{T}{\lambda d})}\,, \tag{26}\]

where

\[T_{0}=16u\log(\frac{u}{\delta})+4u\max\max\{\frac{8d}{\tilde{\lambda}_{x}( \frac{3}{4}-\epsilon_{*}\sqrt{\frac{1}{2\tilde{\lambda}_{x}}})^{2}}\log(\frac{ u}{\delta}),\frac{16}{\tilde{\lambda}_{x}^{2}}\log(\frac{8d}{\tilde{\lambda}_{x}^{2} \delta})\}\]

is given in the following Lemma H.1 in Appendix H.

## Appendix F Proof and Discussions of Theorem 5.4

In the work [23], they give a lower bound for misspecified linear bandits with a single user. The lower bound of \(R(T)\) is given by: \(R_{3}(T)\geq\epsilon_{*}T\sqrt{d}\). Therefore, suppose our problem with multiple users and \(m\) underlying clusters where the arrival times are \(T_{i}\) for each cluster, then for any algorithms, even if they know the underlying clustering structure and keep \(m\) independent linear bandit algorithms to leverage the common information of clusters, the best they can get is \(R(T)=\sum_{i\in[m]}R_{3}(T_{i})\geq\epsilon_{*}\sum_{i\in[m]}T_{i}\sqrt{d}= \epsilon_{*}T\sqrt{d}\), which gives a lower bound of \(O(\epsilon_{*}T\sqrt{d})\) for the CBMUM problem. Recall that the regret upper bound of our algorithms is of \(O(\epsilon_{*}T\sqrt{md\log T}+d\sqrt{mT}\log T)\), asymptotically matching this lower bound with respect to \(T\) up to logarithmic factors and with respect to \(m\) up to \(O(\sqrt{m})\) factors, showing the tightness of our theoretical results (where \(m\) are typically very small for real applications).

We conjecture that the gap for the \(m\) factor is due to the strong assumption that cluster structures are known to prove our lower bound, and whether there exists a tighter lower bound will be left for future work.

## Appendix G Proof of the key Lemma 5.7

In Lemma 5.7, we want to bound the term \(\bigg{|}\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_{\overline{\mathbf{V}}_{t,t-1}}^{1} \sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{\mathbf{V}}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{ \top}(\mathbf{\theta}_{i_{s}}-\mathbf{\theta}_{i_{s}})\bigg{|}\). By the definition of "good partition", we have \(\left\|\mathbf{\theta}_{i_{s}}-\mathbf{\theta}_{i_{t}}\right\|_{2}\leq\zeta,\forall i \leq\overline{V}_{t}\). It is an easy-to-be-made mistake to directly drag \(\left\|\mathbf{\theta}_{i_{s}}-\mathbf{\theta}_{i_{t}}\right\|_{2}\) out to upper bound it by \(\left\|\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_{\overline{\mathbf{V}}_{t,t-1}}^{1} \sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{\mathbf{V}}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{ \top}\right\|_{2}\times\zeta\) and then proceed. We need more careful analysis.

We first prove the following general lemma.

**Lemma G.1**.: _For vectors \(\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{k}\in\mathbb{R}^{d},\left\|\mathbf{x}_{i} \right\|_{2}\leq 1,\forall i\in[k]\), and vectors \(\mathbf{\theta}_{1},\mathbf{\theta}_{2},\ldots,\mathbf{\theta}_{k}\in\mathbb{R}^{d},\left\| \mathbf{\theta}_{i}\right\|_{2}\leq C,\forall i\in[k]\), where \(C>0\) is a constant, we have:_

\[\left\|\sum_{i=1}^{k}\mathbf{x}_{i}\mathbf{x}_{i}^{\top}\mathbf{\theta}_{i}\right\|_{2} \leq C\sqrt{d}\left\|\sum_{i=1}^{k}\mathbf{x}_{i}\mathbf{x}_{i}^{\top}\right\|_{2}\,.\]Proof.: Let \(\mathbf{X}\in\mathbb{R}^{d\times k}\) be a matrix such that it has \(\mathbf{x}_{i}\) s as its columns, i.e., \(\mathbf{X}=[\mathbf{x}_{1},\ldots,\mathbf{x}_{k}]=\begin{bmatrix}\mathbf{x}_{11}&x_{21}&\cdots& \mathbf{x}_{k1}\\ \mathbf{x}_{12}&x_{22}&\cdots&\mathbf{x}_{k2}\\ \vdots&\vdots&\ddots&\vdots\\ \mathbf{x}_{1d}&x_{2d}&\cdots&\mathbf{x}_{kd}\end{bmatrix}.\)

Let \(\mathbf{y}\in\mathbb{R}^{k\times 1}\) be a vector that has \(\mathbf{x}_{i}^{\top}\mathbf{\theta}_{i}\) s as its elements, i.e., \(\mathbf{y}=[\mathbf{x}_{1}^{\top}\mathbf{\theta}_{1},\ldots,\mathbf{x}_{k}^{\top}\mathbf{\theta}_ {k}]^{\top}\). Then we have:

\[\left\|\sum_{i=1}^{k}\mathbf{x}_{i}\mathbf{x}_{i}^{\top}\mathbf{\theta}_{i} \right\|_{2}^{2}=\left\|\mathbf{X}\mathbf{y}\right\|_{2}^{2} \leq\left\|\mathbf{X}\right\|_{2}^{2}\left\|\mathbf{y}\right\|_{2}^{2} \tag{27}\] \[=\left\|\mathbf{X}\right\|_{2}^{2}\sum_{i=1}^{k}(\mathbf{x}_{i}^{\top} \mathbf{\theta}_{i})^{2}\] \[\leq\left\|\mathbf{X}\right\|_{2}^{2}\sum_{i=1}^{k}\left\|\mathbf{x}_{i} \right\|_{2}^{2}\left\|\mathbf{\theta}_{i}\right\|_{2}^{2}\] (28) \[\leq C^{2}\left\|\mathbf{X}\right\|_{2}^{2}\sum_{i=1}^{k}\left\|\mathbf{x }_{i}\right\|_{2}^{2}\] \[=C^{2}\left\|\mathbf{X}\right\|_{2}^{2}\left\|\mathbf{X}\right\|_{F}^{2}\] \[\leq C^{2}d\left\|\mathbf{X}\right\|_{2}^{4}\] (29) \[=C^{2}d\left\|\mathbf{X}\mathbf{X}^{\top}\right\|_{2}^{2}\] (30) \[=C^{2}d\left\|\sum_{i=1}^{k}\mathbf{x}_{i}\mathbf{x}_{i}^{\top}\right\|_{ 2}^{2}, \tag{31}\]

where Eq. (27) follows by the matrix operator norm inequality, Eq. (28) follows by the Cauchy-Schwarz inequality, Eq. (29) follows by \(\left\|\mathbf{X}\right\|_{F}\leq\sqrt{d}\left\|\mathbf{X}\right\|_{2}\), Eq. (30) follows from \(\left\|\mathbf{X}\right\|_{2}^{2}=\left\|\mathbf{X}\mathbf{X}^{\top}\right\|_{2}\). 

The above result is tight. We can show that the lower bound of \(\left\|\sum_{i=1}^{k}\mathbf{x}_{i}\mathbf{x}_{i}^{\top}\mathbf{\theta}_{i}\right\|_{2}\) under the conditions in the lemma is exactly \(C\sqrt{d}\left\|\sum_{i=1}^{k}\mathbf{x}_{i}\mathbf{x}_{i}^{\top}\right\|_{2}\). Specifically, let \(k=2\), \(C=1\), \(d=2\), \(\mathbf{x}_{1}=[0,1]^{\top}\), \(\mathbf{x}_{2}=[1,0]^{\top}\), \(\mathbf{\theta}_{1}=[1,0]^{\top}\), \(\mathbf{\theta}_{2}=[0,1]^{\top}\), then we have \(\left\|\sum_{i=1}^{2}\mathbf{x}_{i}\mathbf{x}_{i}^{\top}\mathbf{\theta}_{i}\right\|_{2}= \left\|[1,1]^{\top}\right\|_{2}=\sqrt{2}\), and \(C\sqrt{d}\left\|\sum_{i=1}^{2}\mathbf{x}_{i}\mathbf{x}_{i}^{\top}\right\|_{2}=1\times \sqrt{2}\times\left\|\begin{bmatrix}1&0\\ 0&1\end{bmatrix}\right\|_{2}=\sqrt{2}\). Therefore, we have that the upper bound given in Lemma G.1 matches the lower bound.

We are now ready to prove the key Lemma 5.7 with the above Lemma G.1.

[MISSING_PAGE_EMPTY:21]

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_FAIL:23]

where \(\gamma_{1}\) is given in Definition 5.1.

Assume \(\lambda\leq 2\log(\frac{u}{\delta})+d\log(1+\frac{T_{i,t}}{\lambda d})\), which is typically held, then a sufficient condition for Eq. (50) is:

\[\frac{2\log(\frac{u}{\delta})+d\log(1+\frac{T_{i,t}}{\lambda d})}{2\tilde{ \lambda}_{x}T_{i,t}}<\frac{1}{4}(\frac{\gamma_{1}}{4}-\epsilon_{*}\sqrt{\frac{ 1}{2\tilde{\lambda}_{x}}})^{2}\,. \tag{51}\]

To satisfy the condition in Eq.(51), it is sufficient to show

\[\frac{2\log(\frac{u}{\delta})}{2\tilde{\lambda}_{x}T_{i,t}}<\frac{1}{8}(\frac{ \gamma_{1}}{4}-\epsilon_{*}\sqrt{\frac{1}{2\tilde{\lambda}_{x}}})^{2} \tag{52}\]

and

\[\frac{d\log(1+\frac{T_{i,t}}{\lambda d})}{2\tilde{\lambda}_{x}T_{i,t}}<\frac{1 }{8}(\frac{\gamma_{1}}{4}-\epsilon_{*}\sqrt{\frac{1}{2\tilde{\lambda}_{x}}})^{ 2}\,. \tag{53}\]

From Eq.(52), we can get:

\[T_{i,t}\geq\frac{8\log(\frac{u}{\delta})}{\tilde{\lambda}_{x}(\frac{\gamma_{1} }{4}-\epsilon_{*}\sqrt{\frac{1}{2\tilde{\lambda}_{x}}})^{2}}\,. \tag{54}\]

Following Lemma 9 in [25], we can get the following sufficient condition for Eq.(53):

\[T_{i,t}\geq\frac{8d\log(\frac{4}{\lambda\tilde{\lambda}_{x}(\frac{\gamma_{1}}{ 4}-\epsilon_{*}\sqrt{\frac{1}{2\tilde{\lambda}_{x}}})^{2}})}{\tilde{\lambda}_{ x}(\frac{\gamma_{1}}{4}-\epsilon_{*}\sqrt{\frac{1}{2\tilde{\lambda}_{x}}})^{2}}\,. \tag{55}\]

Assume \(\frac{u}{\delta}\geq\frac{4}{\lambda\tilde{\lambda}_{x}(\frac{\gamma_{1}}{4}- \epsilon_{*}\sqrt{\frac{1}{2\tilde{\lambda}_{x}}})^{2}}\), which is typically held, we can get that

\[T_{i,t}\geq\frac{8d}{\tilde{\lambda}_{x}(\frac{\gamma_{1}}{4}-\epsilon_{*} \sqrt{\frac{1}{2\tilde{\lambda}_{x}}})^{2}}\log(\frac{u}{\delta}) \tag{56}\]

is a sufficient condition for Eq.(49). Together with the condition that \(T_{i,t}\geq\frac{16}{\tilde{\lambda}_{x}^{2}}\log(\frac{8d}{\tilde{\lambda}_{x }^{2}\delta})\), we can get that if

\[T_{i,t}\geq\max\{\frac{8d}{\tilde{\lambda}_{x}(\frac{\gamma_{1}}{4}-\epsilon_ {*}\sqrt{\frac{1}{2\tilde{\lambda}_{x}}})^{2}}\log(\frac{u}{\delta}),\frac{16 }{\tilde{\lambda}_{x}^{2}}\log(\frac{8d}{\tilde{\lambda}_{x}^{2}\delta})\}, \forall i\in\mathcal{U}\,, \tag{57}\]

then with probability \(\geq 1-2\delta\):

\[\left\|\hat{\mathbf{\theta}}_{i,t}-\mathbf{\theta}^{j(i)}\right\|_{2}<\frac{\gamma_{1 }}{4}\,,\forall i\in\mathcal{U}\,.\]

By Lemma 8 in [25], and Assumption 3.2 of user arrival uniformness, we have that for all

\[t\geq T_{0}\triangleq 16u\log(\frac{u}{\delta})+4u\max\{\frac{8d}{\tilde{ \lambda}_{x}(\frac{\gamma_{1}}{4}-\epsilon_{*}\sqrt{\frac{1}{2\tilde{\lambda }_{x}}})^{2}}\log(\frac{u}{\delta}),\frac{16}{\tilde{\lambda}_{x}^{2}}\log( \frac{8d}{\tilde{\lambda}_{x}^{2}\delta})\}\,, \tag{58}\]

with probability at least \(1-\delta\), condition in Eq.(57) is satisfied.

Therefore we have that for all \(t\geq T_{0}\), with probability \(\geq 1-3\delta\):

\[\left\|\hat{\mathbf{\theta}}_{i,t}-\mathbf{\theta}^{j(i)}\right\|_{2}<\frac{\gamma_{1 }}{4}\,,\forall i\in\mathcal{U}\,. \tag{59}\]

Next, we show that with Eq.(59), we can get that the RCLUMB keeps a "good partition". First, if we delete the edge \((i,l)\), then user \(i\) and user \(j\) belong to different _ground-truth clusters_, i.e., \(\left\|\mathbf{\theta}_{i}-\mathbf{\theta}_{l}\right\|_{2}>0\). This is because by the deletion rule of the algorithm, the concentration bound, and triangle inequality,\(\left\|\mathbf{\theta}^{j(i)}-\mathbf{\theta}_{i,t}\right\|_{2}>0\). Second, we show that if \(\left\|\mathbf{\theta}_{i}-\mathbf{\theta}_{t}\right\|\geq\gamma_{1}>2\epsilon_{*}\sqrt{ \frac{2}{\lambda_{x}}}\), the RCLUMB algorithm will delete the edge \((i,l)\). This is because if \(\left\|\mathbf{\theta}_{i}-\mathbf{\theta}_{l}\right\|\geq\gamma_{1}\), then by the triangle inequality, and \(\left\|\hat{\mathbf{\theta}}_{i,t}-\mathbf{\theta}^{j(i)}\right\|_{2}<\frac{\gamma_{1} }{4}\), \(\left\|\hat{\mathbf{\theta}}_{l,t}-\mathbf{\theta}^{j(l)}\right\|_{2}<\frac{\gamma_{1} }{4}\), \(\mathbf{\theta}_{i}=\mathbf{\theta}^{j(i)}\), \(\mathbf{\theta}_{l}=\mathbf{\theta}^{j(l)}\), we have \(\left\|\hat{\mathbf{\theta}}_{i,t}-\hat{\mathbf{\theta}}_{l,t}\right\|_{2}\geq\left\| \mathbf{\theta}_{i}-\mathbf{\theta}_{l}\right\|-\left\|\hat{\mathbf{\theta}}_{i,t}-\mathbf{ \theta}^{j(i)}\right\|_{2}-\left\|\hat{\mathbf{\theta}}_{i,t}-\mathbf{\theta}^{j(i)} \right\|_{2}>\gamma_{1}-\frac{\gamma_{1}}{4}-\frac{\gamma_{1}}{4}=\frac{\gamma _{1}}{2}>\frac{\sqrt{\lambda}+\sqrt{2\lambda_{x}}+d\log(1+\frac{T_{1,t}}{3d})}{ \sqrt{\lambda+2\lambda_{x}}T_{i,t}}+\epsilon_{*}\sqrt{\frac{1}{2\lambda_{x}}}+ \frac{\sqrt{\lambda}+\sqrt{2\log(\frac{\gamma_{1}}{2})+d\log(1+\frac{T_{1,t}}{3 d})}}{\sqrt{\lambda+2\lambda_{x}}T_{1,t}}+\epsilon_{*}\sqrt{\frac{1}{2 \lambda_{x}}}\), which will trigger the deletion condition Line 10 in Algo.1.

From the above reasoning, we can get that at round \(t\), any user within \(\overline{V}_{t}\) is \(\zeta\)-close to \(i_{t}\), and all the users belonging to \(V_{j(i)}\) are contained in \(\overline{V}_{t}\), which means the algorithm has done a "good partition" at \(t\) by Definition 5.5. 

## Appendix I Proof of Lemma 5.6

We prove the result in two situations: when \(\overline{V}_{t}\in\mathcal{V}\) and when \(\overline{V}_{t}\notin\mathcal{V}\).

(1) Situation 1: for any \(t\geq T_{0}\) and \(\overline{V}_{t}\in\mathcal{V}\), which means that the current user \(i_{t}\) is clustered completely correctly, i.e., \(\overline{V}_{t}=V_{j(i_{t})}\), therefore \(\mathbf{\theta}_{l}=\mathbf{\theta}_{i_{t}},\forall l\in\overline{V}_{t}\), then we have:

\[\hat{\mathbf{\theta}}_{\overline{V}_{t},t-1}-\mathbf{\theta}_{i_{t}} =(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\;\mathbf{x}_{a_{s}^{\top}}^{ \top}+\lambda\mathbf{I})^{-1}(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}r_{s})-\mathbf{\theta}_{i_{t}}\] \[=(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\;\mathbf{x}_{a_{s}^{\top}}^{ \top}+\lambda\mathbf{I})^{-1}(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}(\mathbf{x}_{a_{s}^{\top}}^{ \top}\mathbf{\theta}_{i_{s}}+\mathbf{\epsilon}_{a_{s}^{i,s}}^{i_{s},s}+\eta_{s}))-\mathbf{ \theta}_{i_{t}}\] \[=(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\;\mathbf{x}_{a_{s}^{\top}}^{ \top}+\lambda\mathbf{I})^{-1}(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}(\mathbf{x}_{a_{s}^{\top}}^{ \top}\mathbf{\theta}_{i_{s}}+\mathbf{\epsilon}_{a_{s}^{i,s}}^{i_{s},s}+\eta_{s}))-\mathbf{ \theta}_{i_{t}}\] \[=(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\;\mathbf{x}_{a_{s}^{\top}}^{ \top}+\lambda\mathbf{I})^{-1}[(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}^{\top}}^{ \top}+\lambda\mathbf{I})\mathbf{\theta}_{i_{t}}-\lambda\mathbf{\theta}_{i_{t}}+\sum_{ \begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{\epsilon}_{a_{s}^{i,s}}^{i_{s},s}+\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{\eta}_{s}]-\mathbf{ \theta}_{i_{t}}\] \[=-\lambda\overline{M}\overline{V}_{t,t-1}^{-1}\mathbf{\theta}_{i_{t}} +\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\overline{M}\overline{V}_{t,t-1}^{-1} \mathbf{x}_{a_{s}}\mathbf{\epsilon}_{a_{s}^{i_{s},s}}^{i_{s},s}+\sum_{\begin{subarray} {c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\overline{M}\overline{V}_{t,t-1}^{1}\mathbf{x }_{a_{s}}\mathbf{\epsilon}_{a_{s}}^{i_{s},s}+\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{\overline{M}}_{\overline{V}_{t,t-1} ^{1}\mathbf{x}_{a_{s}}\mathbf{\eta}_{s}}^{1}\,.\]

Therefore we have

\[\left|\mathbf{x}_{a}^{\top}(\hat{\mathbf{\theta}}_{\overline{V}_{t},t-1}-\mathbf{\theta}_{i_ {t}})\right|\leq\lambda\left|\mathbf{x}_{a}^{\top}\overline{M}\overline{V}_{t,t-1}^ {-1}\mathbf{\theta}_{i_{t}}\right|+\left|\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a}^{\top}\overline{M}\overline{V}_{t,t-1}^{-1}\mathbf{x}_{a_{s}}\mathbf{\epsilon}_{a_{s}}^{i_{s},s}\right|+\left|\mathbf{x}_{a }^{\top}\overline{M}\overline{V}_{t,t-1}^{-1}\sum_{\begin{subarray}{c}s\in[t-1] \\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{\eta}_{s}\right|\,. \tag{60}\]

Next, we bound the three terms in Eq.(60). For the first term:

\[\lambda\left|\mathbf{x}_{a}^{\top}\overline{M}\overline{V}_{t,t-1}^{-1}\mathbf{\theta}_{i _{t}}\right|\leq\lambda\left\|\mathbf{x}_{a}\right\|_{\overline{M}\overline{V}_{t,t-1 }^{-1}}\sqrt{\lambda_{\text{max}}(\overline{M}\overline{V}_{t,t-1}^{-1})}\left\| \mathbf{\theta}_{i_{t}}\right\|_{2}\leq\sqrt{\lambda}\left\|\mathbf{x}_{a}\right\|_{ \overline{M}\overline{V}_{t,t-1}^{-1}}\,, \tag{61}\]

where we use the inequality of matrix norm, the Cauchy-Schwarz inequality, \(\left\|\mathbf{\theta}_{i_{t}}\right\|_{2}\leq 1\), and the fact that \(\lambda_{\text{max}}(\overline{M}\overline{V}_{t,t-1}^{-1})=\frac{1}{\lambda_{ \text{min}}(\overline{M}\overline{V}_{t,t-1})}\leq\frac{1}{\lambda}\).

For the second term in Eq.(60):

\[\left|\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_{ \overline{V}_{t},t-1}^{-1}\mathbf{x}_{a_{s}}\mathbf{\epsilon}_{a_{s}}^{i_{s},s}\right| \leq\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\left|\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_ {\overline{V}_{t},t-1}^{-1}\mathbf{x}_{a_{s}}\mathbf{\epsilon}_{a_{s}}^{i_{s},s}\right|\] \[\leq\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\left\|\mathbf{\epsilon}_{a_{s}}^{i_{s},s} \right\|_{\infty}\left|\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_{\overline{V}_{t},t- 1}^{-1}\mathbf{x}_{a_{s}}\right|\] \[\leq\epsilon_{s}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\left|\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_ {\overline{V}_{t},t-1}^{-1}\mathbf{x}_{a_{s}}\right|\,, \tag{62}\]

where in the second inequality we use the Holder's inequality.

For the last term, with probability at least \(1-\delta\):

\[\left|\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{ -1}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\eta_{s}\right| \leq\|\mathbf{x}_{a}\|_{\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1} }\left\|\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\eta_{s}\right\|_{\overline{ \mathbf{M}}_{\overline{V}_{t},t-1}^{-1}} \tag{63}\] \[\leq\|\mathbf{x}_{a}\|_{\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1} }\sqrt{2\log(\frac{1}{\delta})+\log(\frac{\text{det}(\overline{\mathbf{M}}_{ \overline{V}_{t},t-1})}{\text{det}(\lambda\mathbf{I})})}\] \[\leq\|\mathbf{x}_{a}\|_{\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1} }\sqrt{2\log(\frac{1}{\delta})+d\log(1+\frac{T}{\lambda d})}\,, \tag{64}\]

where the second inequality follows by Theorem 1 in [1], Eq.(64) is because \(\text{det}(\overline{\mathbf{M}}_{\overline{V}_{t},t-1})\leq\left(\frac{\text{ trace}(\lambda\mathbf{I}+\sum_{\begin{subarray}{c}s\in[t]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\overline{\mathbf{V}}_{a_{s}} ^{\top})}{d}\right)^{d}\leq\left(\frac{\lambda d+T_{\overline{V}_{t},t}}{d} \right)^{d}\leq\left(\frac{\lambda d+T}{d}\right)^{d}\), and \(\text{det}(\lambda\mathbf{I})=\lambda^{d}\).

Plugging Eq.(61), Eq.(62) and Eq.(64) into Eq.(60), we can prove Lemma 5.6 in situation 1, i.e., for any \(t\geq T_{0}\) and \(\overline{V}_{t}\in V\), with probability at least \(1-\delta\):

\[\left|\mathbf{x}_{a}^{\top}(\hat{\mathbf{\theta}}_{\overline{V}_{t},t-1}-\mathbf{\theta}_{i _{t}})\right|\leq\epsilon_{s}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\left|\mathbf{x}_{a}^{\top}\overline{\mathbf{M}} _{\overline{V}_{t},t-1}^{-1}\mathbf{x}_{a_{s}}\right|+\|\mathbf{x}_{a}\|_{\overline{\mathbf{ M}}_{\overline{V}_{t},t-1}^{-1}}\left(\sqrt{\lambda}+\sqrt{2\log(\frac{1}{\delta})+d \log(1+\frac{T}{\lambda d})}\right). \tag{65}\]

(2) Situation 2: for any \(t\geq T_{0}\) and \(\overline{V}_{t}\notin\mathcal{V}\), which means that the current user is _misclustered_ by the algorithm, i.e., \(\overline{V}_{t}\neq V_{j(i_{t})}\), but with Lemma H.1, with probability at least \(1-3\delta\), the current partition is a "good partition", i.e., \(\left\|\mathbf{\theta}_{l}-\mathbf{\theta}_{i_{t}}\right\|_{2}\leq 2\epsilon_{*}\sqrt{\frac{2}{ \lambda_{x}}},\forall l\in\overline{V}_{t}\), we have:

\[\hat{\mathbf{\theta}}_{\overline{V}_{t},t-1}-\mathbf{\theta}_{i_{t}} =(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\,\mathbf{x}_{a_{s}}^{\top}+ \lambda\mathbf{I})^{-1}(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}r_{s})-\mathbf{\theta}_{i_{t}}\] \[=(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\,\mathbf{x}_{a_{s}}^{\top}+ \lambda\mathbf{I})^{-1}\bigg{(}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}(\mathbf{x}_{a_{s}}^{\top}\mathbf{ \theta}_{i_{s}}+\mathbf{\epsilon}_{a_{s}}^{i_{s},s}+\eta_{s})\bigg{)}-\mathbf{\theta}_ {i_{t}}\] \[=\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray} {c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{\epsilon}_{a_{s}}^{i_{s},s}+\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s\in [t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\eta_{s}+\overline{\mathbf{M}}_{ \overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top}\mathbf{ \theta}_{i_{s}}-\mathbf{\theta}_{i_{t}}\] \[=\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray} {c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{\epsilon}_{a_{s}}^{i_{s},s}+\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s\in [t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\eta_{s}+\overline{\mathbf{M}}_{ \overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top}(\mathbf{ \theta}_{i_{s}}-\mathbf{\theta}_{i_{t}})\] \[\quad+\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}(\sum_{ \begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{\tau}_{a_{s}}^{\top}+ \lambda\mathbf{I})\mathbf{\theta}_{i_{t}}-\lambda\overline{\mathbf{M}}_{\overline{V}_{t},t -1}^{-1}\mathbf{\theta}_{i_{t}}-\mathbf{\theta}_{i_{t}}\] \[=\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray} {c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{\epsilon}_{a_{s}}^{i_{s},s}+\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s \in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\eta_{s}+\overline{\mathbf{M}}_{ \overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{\tau}_{a_{s}}^{\top}( \mathbf{\theta}_{i_{s}}-\mathbf{\theta}_{i_{t}})\] \[\quad-\lambda\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\mathbf{ \theta}_{i_{t}}\,.\]

Thus, with Lemma 5.7 and with the previous reasoning, with probability at least \(1-5\delta\), we have:

\[\left|\mathbf{x}_{a}^{\top}(\hat{\mathbf{\theta}}_{\overline{V}_{t},t-1}- \mathbf{\theta}_{i_{t}})\right| \leq\lambda\left|\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_{\overline{V}_ {t},t-1}^{-1}\mathbf{\theta}_{i_{t}}\right|+\left|\sum_{\begin{subarray}{c}s\in[t-1] \\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_{ \overline{V}_{t},t-1}^{-1}\mathbf{x}_{a_{s}}\mathbf{\epsilon}_{a_{s}}^{i_{s},s}\right|+ \left|\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{ \begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\eta_{s}\right|\] \[\quad+\left|\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top}(\mathbf{ \theta}_{i_{s}}-\mathbf{\theta}_{i_{t}})\right|\] \[\leq\epsilon_{*}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\left|\mathbf{x}_{a}^{\top}\overline{\mathbf{M}}_{ \overline{V}_{t},t-1}^{-1}\mathbf{x}_{a_{s}}\right|+\left|\mathbf{x}_{a}\right|_{ \overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}}\left(\sqrt{\lambda}+\sqrt{2\log( \frac{1}{\delta})+d\log(1+\frac{T}{\lambda d})}\right)\] \[\quad+\frac{\epsilon_{*}\sqrt{2d}}{\tilde{\lambda}_{x}^{\frac{3}{2 }}}\,.\]

Therefore, combining situation 1 and situation 2, the result of Lemma 5.6 then follows.

## Appendix J Technical Lemmas and Their Proofs

We first prove the following technical lemma which is used to prove Lemma H.1.

**Lemma J.1**.: _Under Assumption 3.3, at any time \(t\), for any fixed unit vector \(\mathbf{\theta}\in\mathbb{R}^{d}\)_

\[\mathbb{E}_{t}[(\mathbf{\theta}^{\top}\mathbf{x}_{a_{t}})^{2}|\,|\mathcal{A}_{t}|]\geq \tilde{\lambda}_{x}\triangleq\int_{0}^{\lambda_{x}}(1-e^{-\frac{(\lambda_{x}-\mathbf{x})^{2 }}{2\sigma^{2}}})^{C}dx\,. \tag{66}\]

Proof.: The proof of this lemma mainly follows the proof of Claim 1 in [12], but with more careful analysis, since their assumption is more stringent than ours.

Denote the feasible arms at round \(t\) by \(\mathcal{A}_{t}=\{\mathbf{x}_{t,1},\mathbf{x}_{t,2},\ldots,\mathbf{x}_{t,|\mathcal{A}_{t}|}\}\). Consider the corresponding i.i.d. random variables \(\theta_{i}=(\mathbf{\theta}^{\top}\mathbf{x}_{t,i})^{2}-\mathbb{E}_{t}[(\mathbf{\theta}^{ \top}\mathbf{x}_{t,i})^{2}|\,|\mathcal{A}_{t}|],i=1,2,\ldots,|\mathcal{A}_{t}|\). By Assumption 3.3, \(\theta_{i}\) s are sub-Gaussian random variables with variance bounded by \(\sigma^{2}\). Therefore, we have that for any \(\alpha>0\) and any \(i\in[\left|\mathcal{A}_{t}\right|]\):

\[\mathbb{P}_{t}(\theta_{i}<-\alpha|\left|\mathcal{A}_{t}\right|)\leq e^{-\frac{ \alpha^{2}}{2\sigma^{2}}}\,,\]

where \(\mathbb{P}_{t}(\cdot)\) is the shorthand for the conditional probability \(\mathbb{P}(\cdot|(i_{1},\mathcal{A}_{1},r_{1}),\ldots,(i_{t-1},\mathcal{A}_{t- 1},r_{t-1}),i_{t})\).

We also have that \(\mathbb{E}_{t}[(\boldsymbol{\theta}^{\top}\boldsymbol{x}_{t,i})^{2}|\left| \mathcal{A}_{t}\right|=\mathbb{E}_{t}[\boldsymbol{\theta}^{\top}\boldsymbol{x} _{t,i}\boldsymbol{x}_{t,i}^{\top}\boldsymbol{\theta}|\left|\mathcal{A}_{t} \right|]\geq\lambda_{\text{min}}(\mathbb{E}_{\boldsymbol{x}\sim\rho}[ \boldsymbol{x}\boldsymbol{x}^{\top}])\geq\lambda_{x}\) by Assumption 3.3. With the above inequalities, we can get

\[\mathbb{P}_{t}(\min_{i=1,\ldots,\left|\mathcal{A}_{t}\right|}(\boldsymbol{ \theta}^{\top}\boldsymbol{x}_{t,i})^{2}\geq\lambda_{x}-\alpha|\left|\mathcal{ A}_{t}\right|)\geq(1-e^{-\frac{\alpha^{2}}{2\sigma^{2}}})^{C}\,,\]

where \(C\) is the upper bound of \(\left|\mathcal{A}_{t}\right|\).

Therefore, we have

\[\mathbb{E}_{t}[(\boldsymbol{\theta}^{\top}\boldsymbol{x}_{a_{t}}) ^{2}|\left|\mathcal{A}_{t}\right|] \geq\mathbb{E}_{t}[\min_{i=1,\ldots,\left|\mathcal{A}_{t}\right| }(\boldsymbol{\theta}^{\top}\boldsymbol{x}_{t,i})^{2}|\left|\mathcal{A}_{t} \right|]\] \[\geq\int_{0}^{\infty}\mathbb{P}_{t}(\min_{i=1,\ldots,\left| \mathcal{A}_{t}\right|}(\boldsymbol{\theta}^{\top}\boldsymbol{x}_{t,i})^{2} \geq x|\left|\mathcal{A}_{t}\right|)dx\] \[\geq\int_{0}^{\lambda_{x}}(1-e^{-\frac{(\lambda_{x}-\mu)^{2}}{2 \sigma^{2}}})^{C}dx\triangleq\tilde{\lambda}_{x}\]

Finally, we prove the following lemma which is used in the proof of Theorem 5.3.

**Lemma J.2**.: \[\sum_{t=T_{0}+1}^{T}\min\{\mathbb{I}\{i_{t}\in V_{j}\}\left\| \boldsymbol{x}_{a_{t}}\right\|_{\overline{\boldsymbol{M}}_{V_{j},t-1}^{-1}}^{ 2},1\}\leq 2d\log(1+\frac{T}{\lambda d}),\forall j\in[m]\,.\] (67)

Proof.: \[\text{det}(\overline{\boldsymbol{M}}_{V_{j},T}) =\text{det}\bigg{(}\overline{\boldsymbol{M}}_{V_{j},T-1}+\mathbb{ I}\{i_{T}\in V_{j}\}\boldsymbol{x}_{ar}\boldsymbol{x}_{ar}^{\top}\bigg{)}\] \[=\text{det}(\overline{\boldsymbol{M}}_{V_{j},T-1})\text{det} \bigg{(}\boldsymbol{I}+\mathbb{I}\{i_{T}\in V_{j}\}\overline{\boldsymbol{M}} _{V_{j},T-1}^{-\frac{1}{2}}\boldsymbol{x}_{a_{T}}\boldsymbol{x}_{ar}^{\top} \overline{\boldsymbol{M}}_{V_{j},T-1}^{-\frac{1}{2}}\bigg{)}\] \[=\text{det}(\overline{\boldsymbol{M}}_{V_{j},T-1})\bigg{(}1+ \mathbb{I}\{i_{T}\in V_{j}\}\left\|\boldsymbol{x}_{a_{T}}\right\|_{\overline{ \boldsymbol{M}}_{V_{j},T-1}^{2}}^{2}\bigg{)}\] \[=\text{det}(\overline{\boldsymbol{M}}_{V_{j},T_{0}})\prod_{t=T_{ 0}+1}^{T}\bigg{(}1+\mathbb{I}\{i_{t}\in V_{j}\}\left\|\boldsymbol{x}_{a_{t}} \right\|_{\overline{\boldsymbol{M}}_{V_{j},t-1}^{2}}^{2}\bigg{)}\] \[\geq\text{det}(\lambda\boldsymbol{I})\prod_{t=T_{0}+1}^{T}\bigg{(} 1+\mathbb{I}\{i_{t}\in V_{j}\}\left\|\boldsymbol{x}_{a_{t}}\right\|_{\overline{ \boldsymbol{M}}_{V_{j},t-1}^{2}}^{2}\bigg{)}\,.\] (68)

\(\forall x\in[0,1]\), we have \(x\leq 2\log(1+x)\). Therefore

\[\sum_{t=T_{0}+1}^{T}\min\{\mathbb{I}\{i_{t}\in V_{j}\}\left\| \boldsymbol{x}_{a_{t}}\right\|_{\overline{\boldsymbol{M}}_{V_{j},t-1}^{2}}^{2},1\} \leq 2\sum_{t=T_{0}+1}^{T}\log\bigg{(}1+\mathbb{I}\{i_{t}\in V_{j} \}\left\|\boldsymbol{x}_{a_{t}}\right\|_{\overline{\boldsymbol{M}}_{V_{j},t-1} ^{2}}^{2}\bigg{)}\] \[=2\log\bigg{(}\prod_{t=T_{0}+1}^{T}\big{(}1+\mathbb{I}\{i_{t} \in V_{j}\}\left\|\boldsymbol{x}_{a_{t}}\right\|_{\overline{\boldsymbol{M}}_{V_{ j},t-1}^{2}}^{2}\big{)}\bigg{)}\] \[\leq 2[\log(\text{det}(\overline{\boldsymbol{M}}_{V_{j},T}))-\log (\text{det}(\lambda\boldsymbol{I}))]\] \[\leq 2\log\bigg{(}\frac{\text{trace}(\lambda\boldsymbol{I}+\sum_ {t=1}^{T}\mathbb{I}\{i_{t}\in V_{j}\}\boldsymbol{x}_{a_{t}}\boldsymbol{x}_{a_{t} }^{\top})}{\lambda d}\bigg{)}^{d}\] \[\leq 2d\log(1+\frac{T}{\lambda d})\,. \tag{69}\]Algorithms of RSCLUMB

This section introduces the Robust Set-based Clustering of Misspecified Bandits Algorithm (RSCLUMB). Unlike RCLUMB, which maintains a graph-based clustering structure, RSCLUMB maintains a set-based clustering structure. Besides, RCLUMB only splits clusters during the learning process, while RSCLUMB allows both split and merge operations. A brief illustration is that the agent will split a user out of its current set(cluster) if it finds an inconsistency between the user and its set, and if there are two clusters whose estimated preferences are close enough, the agent will merge them. A detailed discussion of the connection between the graph structure and the set structure can be found in [27].

Now we introduce the details of RSCLUMB. The algorithm first initializes a single set \(\mathcal{S}_{1}\) containing all users and updates it during the learning process. The whole learning process consists of phases (Algo. 2 Line 3), where the \(s-th\) phase contains \(2^{s}\) rounds. At the beginning of each phase, the agent marks all users as "unchecked", and if a user comes later, it will be marked as "checked". If all users in a cluster are checked, then this cluster will be marked as "checked" meaning it is an accurate cluster in the current phase. With this mechanism, every phase can maintain an accuracy level, and the agent can put the accurate clusters aside and focus on exploring the inaccurate ones. For each cluster \(V_{j}\), the algorithm maintains two estimated vectors \(\hat{\mathbf{\theta}}_{V_{j}}\) and \(\tilde{\mathbf{\theta}}_{V_{j}}\), where the \(\hat{\mathbf{\theta}}_{V_{j}}\) is similar to the \(\hat{\mathbf{\theta}}_{\overline{V}_{j}}\) in RCLUMB and is used for the recommendation, while the \(\tilde{\mathbf{\theta}}_{V_{j}}\) is the average of all the estimated user preference vectors in this cluster and is used for the split and merge operations.

At time \(t\) in phase \(s\), the user \(i_{\tau}\) comes with the item set \(\mathcal{D}_{\tau}\), where \(\tau\) represents the index of total time steps. Then the algorithm determines the cluster and makes a cluster-based recommendation. This process is similar to RCLUMB. After updating the information (Algo. 2 Line12), the agent checks if a split or a merge is possible (Algo. 2 Line13-17).

By our assumption, users in the same cluster have the same vectors. So a cluster can be regarded as a good cluster only when all the estimated user vectors are close to the estimated cluster vector. We call a user is consistent with the cluster if their estimated vectors are close enough. If a user is inconsistent with its current cluster, the agent will split it out. Two clusters are consistent when their estimated vectors are close, and the agent will merge them.

RSCLUMB maintains two sets of estimated cluster vectors: (i) cluster-level estimation with integrated user information, which is for recommendations (Line 12 and Line 10 in Algo.2); (ii) the average of estimated user vectors, which is used for robust clustering (Line 3 in Algo.3 and Line 2 in Algo.4). The previous set-based CB work [27] only uses (i) for both recommendations and clustering, which would lead to erroneous clustering under misspecifications, and cannot get any non-vacuous regret bound in CBMUM.

## Appendix L Main Theorem and Lemmas of RSCLUMB

**Theorem L.1** (main result on regret bound for RSCLUMB).: _With the same assumptions in Theorem 5.3, the expected regret of the RSCLUMB algorithm for T rounds satisfies:_

\[R(T) \leq O\bigg{(}u\left(\frac{d}{\tilde{\lambda}_{x}(\gamma_{1}- \zeta_{1})^{2}}+\frac{1}{\tilde{\lambda}_{x}^{2}}\right)\log T+\frac{\epsilon_ {*}\sqrt{d}T}{\tilde{\lambda}_{x}^{1.5}}+\epsilon_{*}T\sqrt{md\log T}+d\sqrt{ mT}\log T+\epsilon_{*}\sqrt{\frac{1}{\tilde{\lambda}_{x}}}T\bigg{)}\] \[\leq O(\epsilon_{*}T\sqrt{md\log T}+d\sqrt{mT}\log T) \tag{70}\]

**Lemma L.2**.: _For RSCLUMB, we use \(T_{1}\) to represent the corresponding \(T_{0}\) of RCLUMB. Then :_

\[T_{1} \triangleq 16u\log(\frac{u}{\delta})+4u\max\{\frac{16}{\tilde{ \lambda}_{x}^{2}}\log(\frac{8d}{\tilde{\lambda}_{x}^{2}\delta}),\frac{8d}{ \tilde{\lambda}_{x}(\frac{\gamma_{1}}{6}-\epsilon_{*}\sqrt{\frac{1}{2\tilde{ \lambda}_{x}}})^{2}}\log(\frac{u}{\delta})\}\] \[=O\bigg{(}u\left(\frac{d}{\tilde{\lambda}_{x}(\gamma_{1}-\zeta_{1 })^{2}}+\frac{1}{\tilde{\lambda}_{x}^{2}}\right)\log\frac{1}{\delta}\bigg{)}\]

[MISSING_PAGE_FAIL:30]

```
1:for any two checked clusters\(V_{j_{1}},V_{j_{2}}\) satisfying \[\left\|\hat{\mathbf{\theta}}_{j_{1}}-\hat{\mathbf{\theta}}_{j_{2}}\right\|<\frac{\alpha_{ 1}}{2}(F(T_{V_{j_{1}}})+F(T_{V_{j_{2}}}))+\frac{\alpha_{2}}{2}\epsilon_{*}\] do
2: Merge them: \[\mathbf{M}_{V_{j_{1}}}=\mathbf{M}_{j_{1}}+\mathbf{M}_{j_{2}},\mathbf{b}_{V_{j_{1}}} =\mathbf{b}_{V_{j_{1}}}+\mathbf{b}_{V_{j_{2}}},\] \[T_{V_{j_{1}}}=T_{V_{j_{1}}}+T_{V_{j_{2}}},C_{V_{j_{1}}}=C_{V_{j_ {1}}}\cup C_{V_{j_{2}}}\]
3: Set \(j(i)=j_{1},\forall i\in j_{2}\), delete \(V_{j_{2}}\)
```

**Algorithm 4** Merge

## Appendix M Proof of Lemma L.3

\[\begin{split}|\mathbf{x}_{a}^{\mathrm{T}}(\mathbf{\theta}_{i}-\hat{\mathbf{ \theta}}_{\overline{V}_{t},t})|&=|\mathbf{x}_{a}^{\mathrm{T}}(\mathbf{ \theta}_{i}-\mathbf{\theta}_{V_{t}})|+|\mathbf{x}_{a}^{\mathrm{T}}(\hat{\mathbf{\theta}}_{ \overline{V}_{t},t}-\mathbf{\theta}_{V_{t}})|\\ &\leq\left\|\mathbf{x}_{a}^{\mathrm{T}}\right\|\left\|\mathbf{\theta}_{i} -\mathbf{\theta}_{V_{t}}\right\|+|\mathbf{x}_{a}^{\mathrm{T}}(\hat{\mathbf{\theta}}_{ \overline{V}_{t},t}-\mathbf{\theta}_{V_{t}})|\\ &\leq 6\epsilon_{*}\sqrt{\frac{1}{2\tilde{\lambda}_{x}}}+|\mathbf{x}_{a }^{\mathrm{T}}(\hat{\mathbf{\theta}}_{\overline{V}_{t},t}-\mathbf{\theta}_{V_{t}})| \end{split} \tag{71}\]

where the last inequality holds due to the fact \(\|\mathbf{x}_{a}\|\leq 1\) and the condition of "split" and "merge". For \(|\mathbf{x}_{a}^{\mathrm{T}}(\hat{\mathbf{\theta}}_{\overline{V}_{t},t}-\mathbf{\theta}_{V _{t}})|\):

\[\begin{split}\hat{\mathbf{\theta}}_{\overline{V}_{t},t-1}-\mathbf{\theta} _{V_{t}}&=(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\,\mathbf{x}_{a_{s}}^{\top}+ \lambda\mathbf{I})^{-1}(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}r_{s})-\mathbf{\theta}_{V_{t}} \\ &=(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\,\mathbf{x}_{a_{s}}^{\top}+ \lambda\mathbf{I})^{-1}(\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}(\mathbf{x}_{a_{s}}^{\top} \mathbf{\theta}_{i_{s}}+\mathbf{\epsilon}_{a_{s}}^{i_{s},s}+\eta_{s}))-\mathbf{\theta}_{V_ {t}}\\ &=\overline{\mathbf{M}}_{\overline{V}_{t},t-1}\sum_{\begin{subarray}{ c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top}+ \overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s\in[t -1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\eta_{s}+\overline{\mathbf{M}}_{ \overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top} \mathbf{\theta}_{i_{s}}-\mathbf{\theta}_{V_{t}}\\ &=\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{ c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top}+ \overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s\in[t -1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{\eta}_{s}+\overline{ \mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top}( \mathbf{\theta}_{i_{s}}-\mathbf{\theta}_{V_{t}})\\ &\quad+\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}(\sum_{ \begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top}+ \lambda\mathbf{I})\mathbf{\theta}_{V_{t}}-\lambda\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\mathbf{\theta}_{V_{t}}-\mathbf{\theta}_{V_{t}}\\ &=\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s \in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{\epsilon}_{a_{s}}^{i_{s},s}+\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s \in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{\eta}_{s}+\overline{ \mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\sum_{\begin{subarray}{c}s\in[t-1]\\ i_{s}\in\overline{V}_{t}\end{subarray}}\mathbf{x}_{a_{s}}\mathbf{x}_{a_{s}}^{\top}( \mathbf{\theta}_{i_{s}}-\mathbf{\theta}_{V_{t}})\\ &\quad-\lambda\overline{\mathbf{M}}_{\overline{V}_{t},t-1}^{-1}\mathbf{\theta}_{V_{t}} \,.\end{split}\]

Thus, with the same method in Lemma 5.7 but replace \(\zeta=4\epsilon_{*}\sqrt{\frac{1}{2\lambda_{x}}}\) with \(\zeta_{1}=6\epsilon_{*}\sqrt{\frac{1}{2\lambda_{x}}}\), and with the previous reasoning, with probability at least \(1-5\delta\), we have:

\[|\mathbf{x}_{a}^{\mathrm{T}}(\hat{\mathbf{\theta}}_{\overline{V}_{t},t}-\mathbf{\theta}_{V _{t}})|\leq C_{a_{t}}+\frac{3\epsilon_{*}\sqrt{2d}}{2\tilde{\lambda}_{x}^{ \frac{3}{2}}} \tag{72}\]

The lemma can be concluded.

## Appendix N Proof of Lemma L.2

With the analysis in the proof of Lemma H.1, with probability at least \(1-\delta\):

\[\left\|\hat{\mathbf{\theta}}_{i,t}-\mathbf{\theta}^{j(i)}\right\|_{2}\leq\frac{\beta(T_{i,t},\frac{\delta}{u})+\epsilon_{*}\sqrt{T_{i,t}}}{\sqrt{\lambda+\lambda_{\min}( \mathbf{M}_{i,t})}},\forall i\in\mathcal{U}\,, \tag{73}\]

and the estimated error of the current cluster \(\left\|\tilde{\mathbf{\theta}}^{j(i)}-\mathbf{\theta}^{j(i)}\right\|\) also satisfies this inequality. For set-based clustering structure, to ensure for each user there is only one \(\zeta\)-close cluster, we let:

\[\frac{\beta(T_{i,t},\frac{\delta}{u})+\epsilon_{*}\sqrt{T_{i,t}}}{\sqrt{ \lambda+\lambda_{\min}(\mathbf{M}_{i,t})}}\leq\frac{\gamma_{1}}{6} \tag{74}\]

By assuming \(\lambda<2\log(\frac{u}{\delta})+d\log(1+\frac{T_{i,t}}{\lambda d})\), we can simplify it to

\[\frac{2\log(\frac{u}{\delta})+d\log(1+\frac{T_{i,t}}{\lambda d})}{2\tilde{ \lambda}_{x}T_{i,t}}<\frac{1}{4}(\frac{\gamma_{1}}{6}-\epsilon_{*}\sqrt{\frac {1}{2\tilde{\lambda}_{x}}})^{2} \tag{75}\]

which can be proved by \(\frac{2\log(\frac{u}{\delta})}{2\tilde{\lambda}_{x}T_{i,t}}\leq\frac{1}{8}( \frac{\gamma_{1}}{6}-\epsilon_{*}\sqrt{\frac{1}{2\tilde{\lambda}_{x}}})^{2}\) and \(\frac{d\log(1+\frac{T_{i,t}}{\lambda d})}{2\tilde{\lambda}_{x}T_{i,t}}\leq \frac{1}{8}(\frac{\gamma_{1}}{6}-\epsilon_{*}\sqrt{\frac{1}{2\tilde{\lambda}_{ x}}})^{2}\). It's obvious that the former one can be satisfied by \(T_{i,t}\geq\frac{8\log(u/\delta)}{\tilde{\lambda}_{x}(\frac{\gamma_{1}}{6}- \epsilon_{*}\sqrt{1/2\tilde{\lambda}_{x}})^{2}}\). As for the latter one, by [25] Lemma 9, we can get \(T_{i,t}\geq\frac{8d\log(\frac{16}{\tilde{\lambda}_{x}\lambda(\frac{1}{6}- \epsilon_{*}\sqrt{1/2\tilde{\lambda}_{x}})^{2}})}{4\tilde{\lambda}_{x}(\frac{ \gamma_{1}}{6}-\epsilon_{*}\sqrt{1/2\tilde{\lambda}_{x}})^{2}}\). By assuming \(\frac{u}{\delta}\geq\frac{16}{4\tilde{\lambda}_{x}\lambda(\frac{\gamma_{1}}{6} -\epsilon_{*}\sqrt{2/4\tilde{\lambda}_{x}})^{2}}\), the lemma is proved.

## Appendix O Proof of Theorem L.1

After \(2T_{1}\) rounds,in each phase, at most \(u\) times split operations will happen, we use \(u\log(T)\) to bound the regret generated in these rounds. Then in the remained rounds the cluster num will be no more than \(m\).

For the instantaneous regret \(R_{t}\) at round \(t\), with probability at least \(1-2\delta\) for some \(\delta\in(0,\frac{1}{2})\):

\[\begin{split} R_{t}&=(\mathbf{x}_{a_{t}}^{\top}\mathbf{ \theta}_{i_{t}}+\mathbf{\epsilon}_{a_{t}^{\top}}^{i_{t},t})-(\mathbf{x}_{a_{t}}^{\top} \mathbf{\theta}_{i_{t}}+\mathbf{\epsilon}_{a_{t},t}^{i_{t},t})\\ &=\mathbf{x}_{a_{t}^{\top}}^{\top}(\mathbf{\theta}_{i_{t}}-\hat{\mathbf{ \theta}}_{\overline{V}_{t,t-1}})+(\mathbf{x}_{a_{t}^{\top}}^{\top}\mathbf{\tilde{ \theta}}_{\overline{V}_{t,t-1}}+C_{a_{t}^{\top},t})-(\mathbf{x}_{a_{t}}^{\top}\mathbf{ \tilde{\theta}}_{\overline{V}_{t,t-1}}+C_{a_{t},t})\\ &\quad+\mathbf{x}_{a_{t}}^{\top}(\hat{\mathbf{\theta}}_{\overline{V}_{t,t -1}}-\mathbf{\theta}_{i_{t}})+C_{a_{t},t}-C_{a_{t}^{\top},t}+(\mathbf{\epsilon}_{a_{t }^{\top}}^{i_{t},t}-\mathbf{\epsilon}_{a_{t}^{\top}}^{i_{t},t})\\ &\leq 2C_{a_{t}}+2\epsilon_{*}+(12\epsilon_{*}\sqrt{\frac{1}{2 \tilde{\lambda}_{x}}}+\frac{3\epsilon_{*}\sqrt{2d}}{\tilde{\lambda}_{x}^{ \frac{3}{2}}})\mathbb{I}(\overline{V}_{t}\notin V)\end{split} \tag{76}\]

where the last inequality holds due to the UCB arm selection strategy, the concentration bound given in LemmaL.3 and the fact that \(\left\|\mathbf{\epsilon}^{i,t}\right\|_{\infty}\leq\epsilon_{*}\).

Define such events. Let:

\[\mathcal{E}_{2}=\{\text{All clusters }\overline{V}_{t}\text{ only contain users who satisfy }\left\|\tilde{\mathbf{\theta}}_{i}-\tilde{\mathbf{\theta}}_{\overline{V}_{t}}\right\|\leq\alpha_{1}(\sqrt{\frac{1+\log(1+T_{i,t})}{1+T_{i,t}}}+\sqrt{\frac{1+\log(1+T_{\overline{V}_{t,t}})}{1+T_{\overline{V}_{t,t}}}})+\alpha_{2}\epsilon_{*}\}\]

\[\mathcal{E}_{3}=\{r_{t}\leq 2C_{a_{t}}+2\epsilon_{*}+12\epsilon_{*}\sqrt{\frac{1}{2 \tilde{\lambda}_{x}}}+\frac{3\epsilon_{*}\sqrt{2d}}{\tilde{\lambda}_{x}^{ \frac{3}{2}}}\}\]

\[\mathcal{E}^{{}^{\prime}}=\mathcal{E}_{2}\cap\mathcal{E}_{3}\]

From previous analysis, we can know that \(\mathbb{P}(\mathcal{E}_{2})\geq 1-3\delta\) and \(\mathbb{P}(\mathcal{E}_{3})\geq 1-2\delta\), thus \(\mathbb{P}(\mathcal{E}^{{}^{\prime}}\geq 1-5\delta)\).

By taking \(\delta=\frac{1}{T}\), we can get:

\[\begin{split} E(R_{t})&=P(\mathcal{E})\mathbb{I}\{ \mathcal{E}\}R_{t}+P(\bar{\mathcal{E}})\mathbb{I}\{\bar{\mathcal{E}}\}R_{t}\\ &\leq\mathbb{I}\{\mathcal{E}\}R_{t}+5\\ &\leq 2T_{1}+2\epsilon_{*}T+(12\epsilon_{*}\sqrt{\frac{1}{2\tilde{ \lambda}_{x}}}+\frac{3\epsilon_{*}\sqrt{2d}}{\tilde{\lambda}_{x}^{\frac{3}{2}}})T +2\sum_{2T_{1}}^{T}C_{a_{t}}+5\end{split} \tag{77}\]

[MISSING_PAGE_EMPTY:33]

[MISSING_PAGE_FAIL:34]