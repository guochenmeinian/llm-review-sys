# Operator World Models for Reinforcement Learning

 Pietro Novelli

Istituto Italiano di Tecnologia

pietro.novelli@iit.it

&Marco Prattico

Istituto Italiano di Tecnologia

marco.prattico@iit.it

&Massimiliano Pontil

Istituto Italiano di Tecnologia

AI Centre, University College London

massimiliano.pontil@iit.it

&Carlo Ciliberto

AI Centre, University College London

c.ciliberto@ucl.ac.uk

###### Abstract

Policy Mirror Descent (PMD) is a powerful and theoretically sound methodology for sequential decision-making. However, it is not directly applicable to Reinforcement Learning (RL) due to the inaccessibility of explicit action-value functions. We address this challenge by introducing a novel approach based on learning a world model of the environment using conditional mean embeddings. Leveraging tools from operator theory we derive a closed-form expression of the action-value function in terms of the world model via simple matrix operations. Combining these estimators with PMD leads to POWR, a new RL algorithm for which we prove convergence rates to the global optimum. Preliminary experiments in finite and infinite state settings support the effectiveness of our method1.

Footnote 1: Code available at: github.com/CSML-IIT-UCL/powr

## 1 Introduction

In recent years, Reinforcement Learning (RL) [1] has seen significant progress, with methods capable of tackling challenging applications such as robotic manipulation [2], playing Go [3] or Atari games [4] and resource management [5] to name but a few. The central challenge in RL settings is to balance the trade-off between exploration and exploitation, namely to improve upon previous policies while gathering sufficient information about the environment dynamics. Several strategies have been proposed to tackle this issue, such as Q-learning-based methods [4], policy optimization [6, 7] or actor-critics [8] to name a few. In contrast, when full information about the environment is available, sequential decision-making methods need only to focus on exploitation. Here, strategies such as policy improvement or policy iteration [9] have been thoroughly studied from both the algorithmic and theoretical standpoints. Within this context, the understanding of Policy Mirror Descent (PMD) methods has recently enjoyed a significant step forward, with results guaranteeing convergence to a global optimum with associated rates [10, 11, 12].

In their original formulation, PMD methods require explicit knowledge of the action-value functions for all policies generated during the optimization process. This is clearly inaccessible in RL applications. Recently, [12] showed how PMD convergence rates can be extended to settings in which inexact estimators of the action-value function are used (see [13] for a similar result from a regret-based perspective). The resulting convergence rates, however, depend on uniform norm bounds on the approximation error, usually guaranteed only under unrealistic and inefficient assumptions such as the availability of a (perfect) simulator to be queried on arbitrary state-action pairs. Moreover, these strategies require repeating this sampling/learning process for any policy generated by the PMD algorithm, which is computationally expensive and demands numerous interactions with theenvironment. A natural question, therefore, is whether PMD approaches can be efficiently deployed in RL settings while enjoying the same strong theoretical guarantees.

In this work, we address these issues by proposing a novel approach to estimating the action-value function. Unlike previous methods that directly approximate the action-value function from samples, we first learn the transition operator and reward function associated with the Markov decision process (MDP). To model the transition operator, we adopt the Conditional Mean Embedding (CME) framework [14; 15]. We then leverage an operatorial characterization of the action-value function to express it in terms of these estimated quantities. This strategy draws a peculiar connection with world model methods and can be interpreted as world model learning via CMEs. The notion of world models for RL has been popularized by Ha and Schmidhuber in [16] distilling ideas from the early nineties [17; 18]. Traditional world model methods such as [16; 19] emphasize learning an implicit model of the environment in the form of a simulator. The simulator can be sampled directly in the latent representation space, which is usually of moderate dimension, resulting in a compressed and high-throughput model of the environment. This approach, however, requires extensive sampling for application to PMD and incurs into two sources of error in estimating the action-value function: model and sampling error. In contrast, CMEs can be used to estimate expectations without sampling and incur only in model error, for which learning bounds are available [20; 21]. One of our key results shows that by modeling the transition operator as a CME between suitable Sobolev spaces, we can compute estimates of the action-value function of any sufficiently smooth policy in closed form via efficient matrix operations.

Combining our estimates of the action-value function with the PMD framework we obtain a novel RL algorithm that we dub _Policy mirror descent with Operator World-models for Reinforcement learning (POWR)_. A byproduct of adopting CMEs to model the transition operator is that we can naturally extend PMD to infinite state space settings. We leverage recent advancements in characterizing the sample complexity of CME estimators to prove convergence rates for the proposed algorithm to the global maximum of the RL Problem. Our approach is similar in spirit to [22], which proposed a value iteration strategy based on CMEs. We extend these ideas to PMD strategies and refine previous results on convergence rates. Learning the transition operator with a least-squares based estimator was also recently considered in [23] and [24]. The latter proposed an optimistic strategy to prove near-optimal regret bounds in linear mixture MDP settings [25]. In contrast, in this work, we cast our problem within a linear MDP setting with possibly infinite latent dimension. We validate our approach on simple environments from the Gym library [26] both in finite and infinite state settings, reporting promising evidence in support of our theoretical analysis.

**Contributions**. The main contributions of this paper are: \(i)\) a CME-based world model framework, which enables us to generate estimators for the action-value function of a policy in closed form via matrix operations. \(ii)\) An (inexact) PMD algorithm combining the learned CMEs world models with mirror descent update steps to generate improved policies. \(iii)\) Showing that the algorithm is well-defined when learning the world model as an operator between a suitable family of Sobolev spaces. \(iv)\) Showing convergence rates of the proposed approach to the global maximum of the RL problem, under regularity assumptions on the MDP. \(v)\) Empirically testing the proposed approach in practice, comparing it with well-established baselines.

## 2 Problem Formulation and Policy Mirror Descent

We consider a Markov Decision Process (MDP) over a state space \(\mathcal{X}\) and action space \(\mathcal{A}\), with transition kernel \(\tau\). We assume \(\mathcal{X}\) and \(\mathcal{A}\) to be Polish, \(\tau:\Omega\rightarrow\mathcal{P}(\mathcal{X})\) to be a Borel measurable function from the joint space \(\Omega=\mathcal{X}\times\mathcal{A}\) to the space \(\mathcal{P}(\mathcal{X})\) of Borel probability measures on \(\mathcal{X}\). We define a policy to be a Borel measurable function \(\pi:\mathcal{X}\rightarrow\mathcal{P}(\mathcal{A})\). When \(\mathcal{A}\) (respectively \(\mathcal{X}\)) is a finite set, the space \(\mathcal{P}(\mathcal{A})=\Delta(\mathcal{A})\subseteq\mathbb{R}^{|A|}\) (respectively \(\mathcal{P}(\mathcal{X})=\Delta(\mathcal{X})\subseteq\mathbb{R}^{|X|}\)) corresponds to the probability simplex. Given a discount factor \(\gamma>0\), an initial state distribution \(\nu\in\mathcal{P}(\mathcal{X})\) and a Borel measurable bounded and non-negative _reward2_ function \(r:\Omega\rightarrow\mathbb{R}\) we denote by

Footnote 2: All the discussion in this work can be extended to the case where also the rewards are random and \(\tau:\Omega\rightarrow\mathcal{P}(\mathcal{X}\times\mathbb{R})\) takes values in the space of joint distributions over states and rewards \((X_{t+1},R_{t})\)

\[J(\pi)=\mathbb{E}_{\nu,\pi,\tau}\left[\sum_{t=0}^{\infty}\gamma^{t}r(X_{t},A_ {t})\right] \tag{1}\]the (discounted) expected return of the policy \(\pi\) applied to the MDP, yielding the Markov process \((X_{t},A_{t})_{t\in\mathbb{N}}\), where \(X_{0}\) is distributed according to \(\nu\) and for each \(t\in\mathbb{N}\) the action \(A_{t}\) is distributed according to \(\pi(\cdot|X_{t})\) and \(X_{t+1}\) according to \(\tau(\cdot|X_{t},A_{t})\).

In sequential decision settings, the goal is to find the optimal policy \(\pi_{*}\) maximizing (1) over the space of all measurable policies. In reinforcement learning, one typically assumes that knowledge of the transition \(\tau\), the reward \(r\), and (possibly) the starting distribution \(\nu\) is not available. It is only possible to gather information about these quantities by interacting with the MDP to sample state-action pairs \((x_{t},a_{t})\) and corresponding rewards \(r(x_{t},a_{t})\) and transitions \(x_{t+1}\).

**Policy Mirror Descent (PMD)**. In so-called tabular settings - in which both \(\mathcal{X}\) and \(\mathcal{A}\) are finite sets - the policy optimization problem amounts to maximizing (1) over the space \(\Pi=\Delta(\mathcal{A})\otimes\mathbb{R}^{|\mathcal{X}|}\) of column substochastic matrices, namely matrices \(M\in\mathbb{R}^{|\mathcal{A}|\times|\mathcal{X}|}\) with non-negative entries and whose columns sum up to one, namely \(M^{*}\mathbf{1}_{\mathcal{A}}=\mathbf{1}_{\mathcal{X}}\), with \(\mathbf{1}\) denoting the vector with all entries equal to one on the appropriate space. Borrowing from the convex optimization literature - where mirror descent algorithms offer a powerful approach to minimize a convex functional over a convex constraint set [27, 28] - recent work proposed to adopt mirror descent also for policy optimization, a strategy known as _policy mirror descent (PMD)_[10]. Even though the objective in (1) is not convex (or concave, since we are maximizing it), it turns out that mirror ascent can nevertheless enjoy global convergence to the maximum, with sublinear [11] or even linear rates [12], at the cost of dimension-dependent constants.

Starting from an initial policy \(\pi_{0}\), PMD generates a sequence \((\pi_{t})_{t\in\mathbb{N}}\) according to the update step

\[\pi_{t+1}(\cdot\,|\,x)=\operatorname*{argmin}_{p\in\Delta(\mathcal{A})}\quad- \eta\left\langle q_{\pi_{t}}(\cdot,x),p\right\rangle+D(p,\pi_{t}(\cdot|x)), \tag{2}\]

for any \(x\in\mathcal{X}\), with \(\eta>0\) a step size, \(D\) a suitable Bregman divergence [28] and \(q_{\pi}:\Omega\to\mathbb{R}\) the so-called _action-value_ function of a policy \(\pi\), see also [12, Sec. 4]. The action-value function

\[q_{\pi}(x,a)=\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}r(X_{t},A_{t}) \Bigg{|}X_{0}=x,A_{0}=a\right] \tag{3}\]

is the discounted return obtained by taking action \(a\in\mathcal{A}\) in state \(x\in\mathcal{X}\) and then following the policy \(\pi\). The solution to (2) crucially depends on the choice of \(D\). For example, in [10] the authors observed that if \(D\) is the Kullback-Leibler divergence, PMD corresponds to the Natural Policy Gradient originally proposed in [29] while [12] showed that if \(D\) is the squared euclidean distance, PMD recovers the Projected Policy Gradient method from [11].

**PMD in Reinforcement Learning**. A clear limitation to adopting PMD in RL settings is that (3) needs exact knowledge of the action-value functions \(q_{\pi_{t}}\) associated to each iterate \(\pi_{t}\) of the algorithm. This requires evaluating the expectation in (3), which is not possible in RL where we do not know the reward \(r\) and MDP transition distribution \(\tau\) in advance. While sampling strategies can be adopted to estimate \(q_{\pi_{t}}\), a key question is how the approximation error affects PMD.

The work in [12] provides an answer to this question, extending the analysis of PMD to the case where estimates \(\hat{q}_{\pi_{t}}\) are used in place of the true action-value function in (2). We recall here an informal version of the result for the case of sublinear convergence rates for PMD. We postpone a more rigorous statement of the theorem and its assumptions to Sec. 5, where we extend it to infinite state spaces \(\mathcal{X}\).

**Theorem 1** (Inexact PMD (Sec. 5 in [12]) - Informal).: _In the tabular setting, let \((\pi_{t})_{t\in\mathbb{N}}\) be a sequence of policies obtained by applying the PMD update in (2) with functions \(\hat{q}_{\pi_{t}}:\Omega\to\mathbb{R}\) in place of \(q_{\pi_{t}}\) and \(D\) a suitable Bregman divergence. For any \(T\in\mathbb{N}\) and \(\varepsilon>0\), if \(\left\|\hat{q}_{\pi_{t}}-q_{\pi_{t}}\right\|_{\infty}\leq\varepsilon\) for all \(t=1,\ldots,T\), then_

\[\max_{\pi}\;J(\pi)-J(\pi_{T})\leq O(\varepsilon+1/T). \tag{4}\]

Thm. 1 implies that inexact PMD retains the convergence rates of its exact counterpart, provided that the approximation error for each action-value function is of order \(1/T\) in uniform norm \(\left\|\cdot\right\|_{\infty}\). While this result supports estimating the action-value function in RL, implementing this strategy in practice poses two main challenges, even in tabular settings. First, approximating the expectation in (3) in \(\left\|\cdot\right\|_{\infty}\) norm via sampling requires "starting" the MDP from each state \(x\in\mathcal{X}\), multiple times. Thisis often not possible in RL, where we do not have control over the starting distribution \(\nu\). Second, repeating this sampling process to learn a \(\hat{q}_{\pi_{t}}\) for each policy \(\pi_{t}\) can become extremely expensive in terms of both the number of computations and interactions with the environment.

In this work, we propose a new strategy to tackle the problems above. Instead of re-sampling the MDP to estimate \(\hat{q}_{\pi_{t}}\) at each iteration \(t\), we learn estimators \(\hat{r}\) and \(\hat{\tau}\) for the reward and transition distribution, respectively. For any policy \(\pi\), we then leverage the relation between these quantities in (3) to generate an estimator \(\hat{q}\) for \(q_{\pi}\). This approach tackles the above challenges since 1) it enables us to control the approximation error on any action-value function in terms of the approximation error of \(\hat{r}\) and \(\hat{\tau}\); 2) it does not require sampling the MDP to learn a new \(\hat{q}_{\pi_{t}}\) for each \(\pi_{t}\) generated by PMD.

## 3 Operator World Models

In this section, we present an operator-based formulation of the problem introduced in Sec. 2 (see also [11]). This will be instrumental in extending the PMD theory to arbitrary state spaces \(\mathcal{X}\), to quantify the approximation error of the action-value function in terms of the approximation error of the reward and transition distribution, and to motivate conditional mean embeddings as the tool to learn these latter quantities.

**Conditional Expectation Operators**. We start by defining the _transfer operator_\(\mathsf{T}\) associated with the MDP transition distribution \(\tau\). Let \(B_{b}(\mathcal{X})\) denote the space of bounded Borel measurable functions on a space \(\mathcal{X}\). Formally, \(\mathsf{T}:B_{b}(\mathcal{X})\to B_{b}(\Omega)\) is the linear operator such that, for any \(f\in B_{b}(\mathcal{X})\)

\[(\mathsf{T}f)(x,a)=\int_{\mathcal{X}}f(x^{\prime})\;\tau(dx^{\prime}|x,a)= \mathbb{E}\left[f(X^{\prime})\mid x,a\right]\qquad\text{for all }(x,a)\in\Omega, \tag{5}\]

where \(X^{\prime}\) is sampled according to \(\tau(\cdot|x,a)\). Note that \(\mathsf{T}\) is the Markov operator [30, Ch. 19] encoding the dynamics of the MDP and its conjugate \(\mathsf{T}^{*}:\mathcal{M}(\Omega)\to\mathcal{M}(\mathcal{X})\) is the operator mapping signed Borel measures \(\mu\in\mathcal{M}(\Omega)\) to their transition via \(\tau\) as \((\mathsf{T}^{*}\mu)(\mathcal{B})=\int_{\mathcal{B}\times\Omega}\tau(dx^{\prime }|x,a)\mu(dx,da)\) for any measurable \(\mathcal{B}\subseteq\mathcal{X}\). For any policy \(\pi\) we define the operator \(\mathsf{P}_{\pi}:B_{b}(\Omega)\to B_{b}(\mathcal{X})\) such that for all \(g\in B_{b}(\Omega)\)

\[(\mathsf{P}_{\pi}g)(x)=\int_{\mathcal{A}}g(x,a)\;\pi(da|x)=\mathbb{E}\left[g( X,A)\mid X=x\right]\quad\text{for all }x\in\mathcal{X}, \tag{6}\]

where the expectation is taken over the action \(A\) sampled according to \(\pi(\cdot|x)\). Also \(\mathsf{P}_{\pi}\) is a Markov operator and its conjugate \(\mathsf{P}_{\pi}^{*}:\mathcal{M}(\mathcal{X})\to\mathcal{M}(\Omega)\) is the operator mapping any \(\nu\in\mathcal{M}(\mathcal{X})\) to its joint measure with \(\pi\), namely \((\mathsf{P}_{\pi}^{*}\nu)(\mathcal{C})=\int_{\mathcal{C}}\pi(da|x)\nu(dx)\) for any measurable \(\mathcal{C}\subseteq\Omega\).

**Operator Formulation of RL**. With these two operators in place, we can characterize the expected reward after a single interaction between a policy \(\pi\) and the MDP as \((\mathsf{TP}_{\pi}r)(x,a)=\mathbb{E}[r(X^{\prime},A^{\prime})|X_{0}=x,A_{0}=a]\). This observation can be applied recursively, yielding the operatorial characterization of the action-value function from (3)

\[q_{\pi}(x,a)=\sum_{t=0}^{\infty}\gamma^{t}\mathbb{E}[r(X_{t},A_{t})|X_{0}=x,A _{0}=a]=\sum_{t=0}^{\infty}(\gamma\mathsf{TP}_{\pi})^{t}r=(\mathsf{Id}-\gamma \mathsf{TP}_{\pi})^{-1}r, \tag{7}\]

where the last equality follows from \(\mathsf{T}\) and \(\mathsf{P}_{\pi}\) being Markov operators [30, Ch. 19] (\(\|\mathsf{T}\|=\|\mathsf{P}_{\pi}\|=1\)), making the Neumann series convergent. Analogously, we can reformulate the RL objective introduced in (1) as the pairing

\[J(\pi)=\left\langle\mathsf{P}_{\pi}(\mathsf{Id}-\gamma\mathsf{TP}_{\pi})^{-1} r,\nu\right\rangle=\left\langle\mathsf{P}_{\pi}q_{\pi},\nu\right\rangle, \tag{8}\]

for \(\nu\in\mathcal{P}(\mathcal{X})\) a starting distribution. In both (7) and (8) the operatorial formulation encodes the cumulative reward collected through the (possibly infinitely many) interactions of the policy with the MDP in closed form, as the inversion \((\mathsf{Id}-\gamma\mathsf{TP}_{\pi})^{-1}r\). This characterization motivates us to learn \(\mathsf{T}\) and \(r\) from data and then express any action-value function as the interaction of these two terms with the policy \(\pi\) as in (7), rather than learning each \(q_{\pi}\) independently for any \(\pi\).

**Learning the World Model via Conditional Mean Embeddings**. Conditional Mean Embeddings (CME) offer an effective tool to model and learn conditional expectation operators from data [15]. They cast the problem of learning \(\mathsf{T}\) by studying the restriction of its action on a suitable family of functions. Let \(\varphi:\mathcal{X}\to\mathcal{F}\) and \(\psi:\Omega\to\mathcal{G}\) two feature maps with values into the Hilbert spaces \(\mathcal{F}\) and \(\mathcal{G}\). With some abuse of notation (which is justified by them being Hilbert spaces), we interpret \(\mathcal{F}\) and \(\mathcal{G}\) as subspaces of functions in \(B_{b}(\mathcal{X})\) and \(B_{b}(\Omega)\) of the form \(f(x)=\langle f,\varphi(x)\rangle\) and \(g(x,a)=\langle g,\psi(x,a)\rangle\) for any \(f\in\mathcal{F}\) and \(g\in\mathcal{G}\) and any \((x,a)\in\Omega\). We say that the _linear MDP_ assumption holds with respect to \((\varphi,\psi)\) if

**Assumption 1** (Linear MDP - Well-specified CME).: _The restriction of \(\mathsf{T}\) to \(\mathcal{F}\) is a Hilbert-Schmidt operator \(\mathsf{T}|_{\mathcal{F}}\in\mathsf{HS}(\mathcal{F},\mathcal{G})\)._

In CME settings, the assumption above is known as requiring the CME of \(\tau\) to be _well-specified_. The following result, proved in Appendix A.2, clarifies this aspect and establishes the relation of Asm. 1 with the standard definition of linear MDP.

**Proposition 2** (Well-specified CME).: _Under Asm. 1, \((\mathsf{T}|_{\mathcal{F}})^{*}=(\mathsf{T}^{*})|_{\mathcal{G}}\) and, for any \((x,a)\in\Omega\)_

\[(\mathsf{T}|_{\mathcal{F}})^{*}\psi(x,a)=\int_{\mathcal{X}}\varphi(x^{\prime} )\;\tau(x^{\prime}|x,a)=\mathbb{E}[\varphi(X^{\prime})|X=x,A=a]. \tag{9}\]

Proposition 2 shows that (9) is equivalent to the standard linear MDP assumption [31, Ch. 8] when \(\mathcal{X}\) is a finite set (taking \(\varphi\) the one-hot encoding) while being weaker in infinite settings. From the CME perspective, the proposition characterizes the action of \((\mathsf{T}|_{\mathcal{F}})^{*}\) as sending evaluation vectors in \(\mathcal{G}\) to the conditional expectation of evaluation vectors in \(\mathcal{F}\) with respect to \(\tau\), the definition of conditional mean embedding of \(\tau\)[32, 15]. This characterization also suggests a learning strategy: (9) characterizes the action of \(\mathsf{T}\) as evaluating the conditional expectation of a vector \(\varphi(X^{\prime})\) given \((x,a)\). Given a set of points \((x_{i},a_{i})_{i=1}^{n}\) and corresponding \(x^{\prime}\) sampled from \(\tau(\cdot|x_{i},a_{i})\), this can be learned by minimizing the squared loss, yielding the estimator (see [15, Sec 4.2])

\[\mathsf{T}_{n}=\operatorname*{argmin}_{\mathsf{T}\in\mathsf{HS}(\mathcal{F}, \mathcal{G})}\;\frac{1}{n}\sum_{i=1}^{n}\left\lVert\varphi(x^{\prime}_{i})- \mathsf{T}^{*}\psi(x_{i},a_{i})\right\rVert_{\mathcal{F}}^{2}+\lambda\left\lVert \mathsf{T}\right\rVert_{\mathsf{HS}}^{2}=S_{n}^{*}K_{\lambda}^{-1}Z_{n}. \tag{10}\]

When \(\mathcal{F}\) and \(\mathcal{G}\) are finite dimensional, \(S_{n}\) and \(Z_{n}\) are matrices with \(n\) rows, each corresponding respectively to the vectors \(\psi(x_{i},a_{i})\) and \(\varphi(x^{\prime}_{i})\) for \(i=1,\ldots,n\). In the infinite setting, they generalize to operators \(S_{n}:\mathcal{G}\to\mathbb{R}^{n}\) and \(Z_{n}:\mathcal{F}\to\mathbb{R}^{n}\). The matrix \(K_{\lambda}=S_{n}S_{n}^{*}+n\lambda\mathsf{Id}_{n}\in\mathbb{R}^{n\times n}\) is the regularized Gram (or kernel) matrix with \((i,j)\)-th entry corresponding to

\[\left(K_{\lambda}\right)_{ij}=\langle\psi(x_{i},a_{i}),\psi(x_{j},a_{j}) \rangle+n\lambda\delta_{ij}. \tag{11}\]

We conclude our discussion on learning world models via CMEs by noting that in most RL settings, the reward function is unknown, too. Analogously to what we have described for \(\mathsf{T}_{n}\) and following the standard practice in supervised settings, we can learn an estimator for \(r\) solving a problem akin to (10). This yields a function of the form \(r_{n}=S_{n}^{*}b=\sum_{i=1}^{n}b_{i}\,\psi(x_{i},a_{i})\) as the linear combination of the embedded training points with the entries of the vector \(b=K_{\lambda}^{-1}y\) where \(y\in\mathbb{R}^{n}\) is the vector with entries \(y_{i}=r(x_{i},a_{i})\).

**Estimating the Action-value Function \(q_{\pi}\)**. We now propose our strategy to generate an estimator for the action-value function \(q_{\pi}\) of a given policy \(\pi\) in terms of an estimator for the reward \(r\) and a world model for \(\mathsf{T}\) learned in terms of the restriction to \(\mathcal{G}\) and \(\mathcal{F}\). To this end, we need to introduce the notion of compatibility between a policy \(\pi\) and the pair \((\mathcal{G},\mathcal{F})\).

**Definition 1** (\((\mathcal{G},\mathcal{F})\)-compatibility).: _A policy \(\pi:\mathcal{X}\to\mathcal{P}(\mathcal{A})\) is compatible with two subspaces \(\mathcal{F}\subseteq B_{b}(\mathcal{X})\) and \(\mathcal{G}\subseteq B_{b}(\Omega)\) if the restriction \(\mathsf{P}_{\pi}\) to \(\mathcal{G}\) is a bounded linear operator with range \(\subseteq\mathcal{F}\), that is \((\mathsf{P}_{\pi})|_{\mathcal{G}}:\mathcal{G}\to\mathcal{F}\)._

Definition 1 is analogous to the linear MDP Asm. 1 in that it requires the restriction of \(\mathsf{P}_{\pi}\) to \(\mathcal{G}\) to take values in the associated space \(\mathcal{F}\). However, it is slightly weaker since it requires this restriction to be bounded (and linear) rather than being an HS operator. We will discuss in Sec. 4 how this difference will allow us to show that a wide range of policies (in particular those generated by our POWR method) is \((\mathcal{G},\mathcal{F})\)-compatible for our choice of function spaces. Definition 1 is the key condition that enables us to generate an estimator for \(q_{\pi}\), as characterized by the following result.

**Proposition 3**.: _Let \(\mathsf{T}_{n}=S_{n}^{*}BZ_{n}\in\mathsf{HS}(\mathcal{F},\mathcal{G})\) and \(r_{n}=S_{n}^{*}b\in\mathcal{G}\) for respectively a \(B\in\mathbb{R}^{n\times n}\) and \(b\in\mathbb{R}^{n}\). Let \(\pi\) be \((\mathcal{G},\mathcal{F})\)-compatible. Then,_

\[\hat{q}_{\pi}=(\mathsf{Id}-\gamma\mathsf{T}_{n}\mathsf{P}_{\pi})^{-1}r_{n}=S_{ n}^{*}(\mathsf{Id}-\gamma BM_{\pi})^{-1}b \tag{12}\]

_where \(M_{\pi}=Z_{n}\mathsf{P}_{\pi}S_{n}^{*}\in\mathbb{R}^{n\times n}\) is the matrix with entries_

\[\left(M_{\pi}\right)_{ij}=\left\langle\varphi(x_{i}^{\prime}),\mathsf{P}_{\pi} \psi(x_{j},a_{j})\right\rangle=\int_{\mathcal{A}}\left\langle\psi(x_{i}^{ \prime},a),\psi(x_{j},a_{j})\right\rangle\;\pi(da|x_{i}^{\prime}). \tag{13}\]

Proposition 3 leverages a kernel trick argument to express the estimator for the action-value function of \(\pi\) as the linear combination \(\hat{q}_{\pi}=\sum_{i=1}^{n}c_{i}\,\psi(x_{i},a_{i})\) of the (embedded) training points \(\psi(x_{i},a_{i})\) and the entries \(c_{i}\) of the vector \(c=(\mathsf{Id}-\gamma BM_{\pi})^{-1}b\in\mathbb{R}^{n}\). We prove the result in Appendix A.4. We note that in (12) both \(B\) and \(M_{\pi}\) are \(n\times n\) matrices and therefore the characterization of \(\hat{q}_{\pi}\) amounts to solving a \(n\times n\) linear system. For settings where \(n\) is large, one can adopt random projection methods such as Nystrom approximation to learn \(\mathsf{T}_{n}\) and \(r_{n}\)[33]. These strategies have been recently shown to significantly reduce the computational load of learning while retaining the same empirical and theoretical performance as their non-approximated counterparts [34; 35].

We conclude this section noting how (13) implies that we only need to be able to evaluate \(\pi\), but we do not need explicit knowledge of \(\mathsf{P}_{\pi}\) as operator. As we shall see, this property will be instrumental to prove generalization bounds for our proposed PMD algorithm in Sec. 4.

## 4 Proposed Algorithm: POWR

We are ready to describe our algorithm for world model-based PMD. In the following, we restrict to the case where \(|\mathcal{A}|<\infty\) is a finite set. As introduced in Sec. 2, policy mirror descent methods are mainly characterized by the choice of Bregman divergence \(D\) used for the mirror descent update and, in the case of inexact methods, the estimator \(\hat{q}_{\pi_{t}}\) of the action-value function \(q_{\pi_{t}}\) for the intermediate policies generated by the algorithm.

In POWR, we combine the CME world model presented in Sec. 3 with mirror descent steps using the Kullback-Leibler divergence \(D_{\mathrm{KL}}(p;p^{\prime})=\sum_{a\in\mathcal{A}}p_{a}\log(p_{a}/p^{\prime }_{a})\) in the update of (2). It was shown in [10] that in this case PMD corresponds to Natural Policy Gradient [29]. As showed in [36; Example 9.10], the solution to (2) can be written in closed form for any \(x\in\mathcal{X}\) as

\[\pi_{t+1}(\cdot|x)=\frac{\pi_{t}(\cdot|x)e^{\eta\hat{q}_{\pi_{t}}(x,\cdot)}}{ \sum_{a\in\mathcal{A}}\pi_{t}(a|x)e^{\eta\hat{q}_{\pi_{t}}(x,a)}}, \tag{14}\]

where we used the estimated action-value function \(\hat{q}_{\pi}\) from Proposition 3. Additionally, the formula above can be applied recursively, expressing \(\pi_{t+1}\) as the softmax operator applied to the discountedsum of the action-value functions up to the current iteration

\[\pi_{t+1}(\cdot|x)=\textsc{softmax}\left(\log(\pi_{0}(\cdot|x))+\eta\sum_{s=0}^{t} \hat{q}_{\pi_{s}}(x,\cdot)\right). \tag{15}\]

**Choice of the Feature Maps**. A key question to address before adopting the action-value estimators introduced in Sec. 3 is choosing the two spaces \(\mathcal{F}\) and \(\mathcal{G}\) to perform world model learning. Specifically, to apply Proposition 3 and obtain proper estimators \(\hat{q}_{\pi_{t}}\), we need to guarantee that all policies generated by the PMD update (14) are \((\mathcal{G},\mathcal{F})\)-compatible (Definition 1). The following result describes a suitable family of such spaces.

**Proposition 4** (Separable Spaces).: _Let \(\phi:\mathcal{X}\rightarrow\mathcal{H}\) be a feature map into a Hilbert space \(\mathcal{H}\). Let \(\mathcal{F}=\mathcal{H}\otimes\mathcal{H}\) and \(\mathcal{G}=\mathbb{R}^{|\mathcal{A}|}\otimes\mathcal{H}\) with feature maps respectively \(\varphi(x)=\phi(x)\otimes\phi(x)\) and \(\psi(x,a)=\phi(x)\otimes e_{a}\), with \(e_{a}\in\mathbb{R}^{|\mathcal{A}|}\) the one-hot encoding of action \(a\in\mathcal{A}\). Let \(\pi:\mathcal{X}\rightarrow\Delta(\mathcal{A})\) be a policy such that \(\pi(a|\cdot)=\langle p_{a},\phi(\cdot)\rangle\) with \(p_{a}\in\mathcal{H}\) for any \(a\in\mathcal{A}\). Then, \(\pi\) is \((\mathcal{G},\mathcal{F})\)-compatible._

Proposition 4 (proof in Appendix A.5) states that for the specific choice of function spaces \(\mathcal{F}=\mathcal{H}\otimes\mathcal{H}\) and \(\mathcal{G}=\mathbb{R}^{|\mathcal{A}|}\otimes\mathcal{H}\), _we can guarantee \((\mathcal{G},\mathcal{F})\)-compatibility, provided that \(\mathcal{H}\) is rich enough to "contain" all \(\pi(a|\cdot)\) for \(a\in\mathcal{A}\)_. We postpone the discussion on identifying a suitable spaces \(\mathcal{H}\) for PMD to Sec. 5, since \((\mathcal{G},\mathcal{F})\)-compatibility is not needed to mechanically apply Proposition 3 and obtain an estimator \(\hat{q}_{\pi}\). This is because (12) exploits a kernel-trick to bypass the need to know \(\mathsf{P}_{\pi}\) explicitly and rather requires only to be able to evaluate \(\pi\) on the training data. The latter is possible for \(\pi_{t+1}\), thanks to the explicit form of the PMD update in (14). We can therefore present our algorithm.

**POWR**. Alg. 1 describes _Policy mirror descent with Operator World-models for Reinforcement learning (POWR)_. Following the intuition of Proposition 4, the algorithm assumes to work with separable spaces. During an initial phase, we learn the world model \(\mathsf{T}_{n}=S_{n}^{*}K_{\lambda}^{-1}Z_{n}\) and the reward \(r_{n}=S_{n}^{*}b\) fitting the conditional mean embedding described in (10) on a dataset \((x_{i},a_{i},x^{\prime}_{i},r_{i})_{i=1}^{n}\) (e.g. obtained via experience replay [37]). Once the world model has been learned, we optimize the policy and perform PMD iterations via (15). In this second phase, we first evaluate the past (cumulative) action-value estimators \(\sum_{a=0}^{t}\hat{q}_{\pi_{s}}\) to obtain the policy \(\pi_{t+1}(\cdot|x_{i})\) by (inexact) PMD via the softmax operator in (15). We use the newly obtained policy to compute the matrix \(M_{\pi_{t+1}}\) defined in (13), which is a key component to obtain the estimator \(\hat{q}_{\pi_{t+1}}\) for \(q_{\pi_{t+1}}\). We note that in the case of the separable spaces of Proposition 4, this matrix reduces to the \(n\times n\) matrix with entries\(k(x^{\prime}_{i},x_{j})\pi_{t+1}(a_{j}|x^{\prime}_{i})\), where \(k(x^{\prime}_{i},x_{j})=\langle\phi(x^{\prime}_{i}),\phi(x_{j})\rangle\) is the kernel matrix between initial and evolved states. Finally, we obtain \(c=(\mathsf{Id}-\gamma K_{\lambda}^{-1}M)^{-1}b\) and model \(\hat{q}_{\pi_{t+1}}=S_{n}^{*}c\) according to Proposition 3.

Clearly, the world model learning and PMD phases can be alternated in POWR, essentially finding a trade-off between exploration and exploitation. This could possibly lead to a refinement of the world model as more observations are integrated into the estimator. While in this work we do not investigate the effects of such alternating strategy, Thm. 7 offers relevant insights in this sense. The result characterizes the behavior of the PMD algorithm when combined with a varying (possibly increasing) accuracy in the estimation of the action-valued function (see Sec. 5 for more details).

## 5 Theoretical Analysis

We now show that POWR converges to the global maximizer of the RL problem in (1). To this end, we first identify a family of function spaces guaranteed to be compatible with the policies generated by Alg. 1. Then, we provide an extension of the result in [12] for inexact PMD to infinite state spaces \(\mathcal{X}\), showing the impact of the action-value approximation error on the convergence rates. For the estimator introduced in Proposition 3, we relate this error to the approximation errors of \(\mathsf{T}_{n}\) and \(r_{n}\) leveraging the Simulation Lemma A.6. Finally, we use recent advancements in the characterization of CMEs' fast learning rates to bound the sample complexity of these latter quantities, yielding error bounds for POWR.

**POWR is Well-defined**. To properly apply Proposition 3 to estimate the action-value function of any PMD iterate \(\pi_{t}\), we need to guarantee that every iterate belongs to the space \(\mathcal{H}\) according to Proposition 4. The following result provides such a family of spaces.

**Theorem 5**.: _Let \(\mathcal{X}\subset\mathbb{R}^{d}\) be a compact set and let \(\mathcal{H}=W^{2,s}(\mathcal{X})\) be the Sobolev space of smoothness \(s>0\) (see e.g. [38]). Let \(\pi_{t}(a|\cdot)\) and \(\hat{q}_{\pi_{t}}(\cdot,a)\) belong to \(\mathcal{H}\) for any \(a\in\mathcal{A}\) and \(\pi_{t}(a|x)>0\) for any \(x\in\mathcal{X}\). Then the policy \(\pi_{t+1}\) solution to the PMD update in (14) belongs to \(\mathcal{H}\)._

According to Thm. 5, Sobolev spaces offer a viable choice for compatibility with PMD-generated policies. This observation is further supported by the fact that Sobolev spaces of smoothness \(s>d/2\) are so-called _reproducing kernel Hilbert spaces (rkhs)_ (see e.g. [39, Ch. 10]). We recall that rkhs are always naturally equipped with a \(\phi:\mathcal{X}\to\mathcal{H}\) such that the inner product \(\langle\phi(x),\phi(x^{\prime})\rangle=k(x,x^{\prime})\) defines a so-called reproducing kernel, namely a positive definite function that is (usually) efficient to evaluate, even if \(\phi(x)\) is high or infinite dimensional. For example, \(\mathcal{H}=W^{2,s}(\mathcal{X})\) with \(s=\lceil\frac{d}{2}\rceil\) has associated kernel \(k(x,x^{\prime})=e^{-\|x-x^{\prime}\|/\sigma}\) with bandwidth \(\sigma>0\)[39]. By applying Thm. 5 to the iterates generated by Alg. 1 we have the following result.

**Corollary 6**.: _With the hypothesis of Proposition 4 let \(\mathcal{H}=W^{2,s}(\mathcal{X})\) with \(s>d/2\). Let \(\mathsf{T}_{n}\in\mathsf{HS}(\mathcal{F},\mathcal{G})\) and \(r_{n}\in\mathcal{G}\) characterized as in Proposition 3. Let \(\pi_{0}(a|\cdot)\propto e^{\eta_{0}(\cdot,a)}\) for \(q_{0}\) such that \(q_{0}(\cdot,a)\in\mathcal{H}\) any \(a\in\mathcal{A}\). Then, for any \(t\in\mathbb{N}\) the PMD iterates \(\pi_{t}\) generated by Alg. 1 are such that \(\pi_{t}(a|\cdot)\in\mathcal{H}\) and hence are \((\mathcal{G},\mathcal{F})\)-compatible._

The above corollary guarantees us that if we are able to learn our estimates for the action-value function in a suitably regular Sobolev space \(\mathcal{H}\), then POWR is well-defined. This is a necessary condition to then being able to study its theoretical behavior in our main result. We report the proofs of Thm. 5 and Cor. 6 in Appendix C.1.

**Inexact PMD Converges**. We now present a more rigorous version of the characterization of the convergence rates of the inexact PMD algorithm discussed informally in Thm. 1.

**Theorem 7** (Convergenge of Inexact PMD).: _Let \((\pi_{t})_{t\in\mathbb{N}}\) be a sequence of policies generated by \(Alg.\)\(1\) that are all \((\mathcal{G},\mathcal{F})\)-compatible. If the action-value functions \(\hat{q}_{\pi_{t}}\) are estimated with an error \(\left\|q_{\pi_{t}}-\hat{q}_{\pi_{t}}\right\|_{\infty}\leq\varepsilon_{t}\), the iterates of Alg. 1 converge to the optimal policy as_

\[J(\pi_{*})-J(\pi_{T})\leq\varepsilon_{T}+O\left(\frac{1}{T}+\frac{1}{T}\sum_{t =0}^{T-1}\varepsilon_{t}\right), \tag{16}\]

_where \(\pi_{*}:\mathcal{X}\to\Delta(\mathcal{A})\) is a measurable maximizer of (8)._

Thm. 7 shows that inexact PMD can behave comparably to its exact version provided that 1) the action value functions \(\hat{q}_{\pi_{t}}\) are estimated with increasing accuracy, and 2) that the sequence of policies is \((\mathcal{G},\mathcal{F})\)-compatible, for example in the Sobolev-based setting of Cor. 6. Specifically, if \(\left\|q_{\pi_{t}}-\hat{q}_{\pi_{t}}\right\|_{\infty}\leq O(1/t)\) for any \(t\in\mathbb{N}\), the convergence rate of inexact PMD is of order \(O(\log T/T)\), only a logarithmic factor slower than exact PMD. This means that we do not necessarily need a good approximation of the world model from the beginning but rather a strategy to improve upon such approximation as we perform more PMD iterations. This suggests adopting an alternating strategy between exploration (world model learning) and exploitation (PMD steps), as suggested in Sec. 4. We do not investigate this question in this work.

The demonstration technique used to prove Thm. 7 follows closely [12, Thm. 8 and 13]. We provide a proof in Appendix B.1 since the original result did not allow for a decreasing approximation error but rather assumed a constant one. Moreover, extending it to the case of infinite \(\mathcal{X}\) requires taking care of additional details related to potential measurability issues.

**Action-value approximation error in terms of World Model estimates**. Thm. 7 highlights the importance of studying the error of the estimator for the action-value functions produced by Alg. 1. These objects are obtained via the formula described in Proposition 3 in terms of \(\mathsf{T}_{n}\), \(r_{n}\) and \(\mathsf{P}_{\pi}\). The exact \(q_{\pi}\) has an analogous closed-form characterization in terms of \(\mathsf{T}\), \(r\) and \(\mathsf{P}_{\pi}\) as expressed in (7), and motivating our operator-based approach. The following result compares these quantities in terms of the approximation errors of the world model and the reward function.

**Lemma 8** (Implications of the Simulation Lemma).: _Let \(\mathsf{T}_{n}\) and \(r_{n}\) the empirical estimators of the transfer operator \(\mathsf{T}\) and reward function \(r\) as defined in Proposition 3, respectively. If \(\mathsf{T}\) satisfies Ass. 1, \(r\in\mathcal{G}\), and \(\gamma\big{\|}\mathsf{T}_{n}\big{\|}<\gamma^{\prime}<1\), then, for every \((\mathcal{G},\mathcal{F})\)-compatible policy \(\pi\)_

\[\big{\|}\hat{q}_{\pi}-q_{\pi}\big{\|}_{\infty}\leq\frac{1}{1-\gamma^{\prime}} \left[\text{const}_{\psi}\big{\|}r_{n}-r\big{\|}_{\mathcal{G}}+\frac{\gamma \big{\|}r\big{\|}_{\infty}}{1-\gamma}\big{\|}\mathsf{T}_{\|\mathcal{F}}- \mathsf{T}_{n}\big{\|}_{\mathsf{HS}}\right].\]In the result above, when applied to a function in \(\mathcal{G}\), such as \(r_{n}\), the uniform norm is to be interpreted as the uniform norm of the evaluation of such function, namely \(\left\|r_{n}\right\|_{\infty}=\sup_{(x,a)\in\Omega}\left|\,\left\langle r_{n}, \psi(x,a)\right\rangle\,\right|\), and analogously for \(\mathsf{T}_{n}\). The proof, reported in Appendix C.2, follows by first decomposing the difference \(q_{\pi}-\hat{q}_{\pi}\) with the simulation lemma [31, Lemma 2.2] and then applying the triangular inequality for the uniform norm.

**POWR converges**. We are ready to state the convergence result for Alg. 1. We consider the setting where the dataset used to learn \(\mathsf{T}_{n}\) (and \(r_{n}\)) is made of i.i.d. triplets \((x_{i},a_{i},x^{\prime}_{i})\) with \((x_{i},a_{i})\) sampled from a distribution \(\rho\in\mathcal{P}(\Omega)\) supported on all \(\Omega\) (such as the state occupancy measure (see e.g. [11] or Appendix A.3) of the uniform policy \(\pi(\cdot|x)=1/|\mathcal{A}|\)) and \(x^{\prime}_{i}\) sampled from \(\tau(\cdot|x_{i},a_{i})\). To guarantee bounds in uniform norm, the result makes a further regularity assumption, of the transfer operator (and the reward function)

**Assumption 2** (Strong Source Condition).: _Let \(\rho\in\mathcal{P}(\Omega)\) and \(C_{\rho}\) the covariance operator \(\sum_{a\in\mathcal{A}}\int_{\mathcal{X}}\rho(dx,a)\psi(x,a)\otimes\psi(x,a)\). The transition operator \(\mathsf{T}\) and the reward function \(r\) are such that \(\mathsf{T}|_{\mathcal{F}}\in\mathsf{HS}(\mathcal{F},\mathcal{G})\) and \(r\in\mathcal{G}\). Further, \(\left\|(\mathsf{T}|_{\mathcal{F}})^{*}C_{\rho}^{-\beta}\right\|_{\mathsf{HS}} <\infty\) and \(\left\|C_{\rho}^{-\beta}r\right\|_{\mathcal{G}}<\infty\) for some \(\beta>0\)._

Asm. 2 imposes a strong requirement to the so-called _source condition_, a quantity that describes how well the target objective of the learning process (here \(\mathsf{T}\) and \(r\)) "interact" with the sampling distribution. The assumption is always satisfied when the hypothesis space is finite dimensional (e.g. in the tabular RL setting) and imposes additional smoothness on \(\mathsf{T}\) and \(r\) when belonging to a Sobolev space. Equipped with this assumption, we can now state the convergence theorem for Alg. 1.

**Theorem 9**.: _Let \((\pi_{t})_{t\in\mathbb{N}}\) be a sequence of policies generated by Alg. 1 in the same setting of Cor. 6. If the action-value functions \(\hat{q}_{\pi_{t}}\) are estimated from a dataset \((x_{i},a_{i},x^{\prime}_{i})_{i=1}^{n}\) with \((x_{i},a_{i})\sim\rho\in\mathcal{P}(\Omega)\) such that Asm. 2 holds with parameter \(\beta\), the iterates of Alg. 1 converge to the optimal policy as_

\[J(\pi_{*})-J(\pi_{T})\leq O\left(\frac{1}{T}+\delta^{2}n^{-\alpha}\right)\]

_with probability not less than \(1-4e^{-\delta}\). Here, \(\alpha\in\left(\frac{\beta}{2+2\beta},\frac{\beta}{1+2\beta}\right)\) and \(\pi_{*}:\mathcal{X}\to\Delta(\mathcal{A})\) is a measurable maximizer of (8)._

The proof of Thm. 9 is reported in Appendix C and combines the results discussed in this section with fast convergence rates for the least-squares [40] and CME [21] estimators. In particular we first use Thm. 5 to guarantee that the policies produced by Alg. 1 are all \((\mathcal{G},\mathcal{F})\)-compatible and therefore that applying Proposition 3 to obtain an estimator for the action-value function is well-defined. Then, we use Lemma 8 to study the approximation error of these estimators in terms of our estimates for the world model and the reward function. Bounds on these quantities are then used in the result for inexact PMD convergence in Thm. 7. We note here that since the latter results require convergence in uniform norm, we cannot leverage standard results for least-squares and CME convergence, which characterize convergence in \(L_{2}(\Omega,\mu)\) and would only require Asm. 1 (Linear MDP) to hold. Rather, we need to impose Asm. 2 to guarantee faster rates in uniform norm.

## 6 Experimental results

We empirically evaluated POWR on classical _Gym_ environments [26], ranging from discrete (FrozenLake-v1,Taxi-v3) to continuous state spaces (MountainCar-v0). To ensure balancing between exploration and exploitation of our method, we alternated between running the environment with the current policy to collect samples for world model learning and running Alg. 1 for a number of steps to generate a new policy. Appendix D provides implementation details regarding this process as well as additional results.

Fig. 1 compares our approach with the performance of well-established baselines including A2C [41], DQN [4], TRPO [7], and PPO [6]. The figure reports the average cumulative reward obtained by the models on test environments with respect to the number of interactions with the MDP (_timesteps_ in log scale in the figure) across 7 different training runs. In all plots, the horizontal dashed line represents the "success" threshold for the corresponding environment, according to official guidelines. We observe that our method outperforms all competitors by a significant margin in terms of sample complexity, that is, the reward achieved after a given number of timesteps. In the case of the Taxi-v3environment, it avoids converging to a local optimum, in contrast every other method with the exception of DQN. On the downside, we note that our method exhibits less stability than other approaches, particularly during the initial stages of the training process. This is arguably due to a sub-optimal interplay between exploration and exploitation, which will be the subject of future work.

## 7 Conclusions and Future Work

Motivated by recent advancements in policy mirror descent (PMD), this work introduced a novel reinforcement learning (RL) algorithm leveraging these results. Our approach operates in two, possibly alternating, phases: learning a world model and planning via PMD. During exploration, we utilize conditional mean embeddings (CMEs) to learn a world model operator, showing that this procedure is well-posed when performed over suitable Sobolev spaces. The planning phase involves PMD steps for which we guarantee convergence to a global optimum at a polynomial rate under specific MDP regularities.

Our analysis opens avenues for further exploration. Firstly, extending PMD to infinite action spaces remains a challenge. While we introduced the operatorial perspective on RL for infinite state space settings, the PMD update with KL divergence requires approximation methods (e.g., Monte Carlo) whose impact on convergence requires investigation. Secondly, scalability to large environments requires adopting approximated yet efficient CME estimators like Nystrom [35] or reduced-rank regressors [42, 43]. Thirdly, a question we touched upon only empirically, is whether alternating world model learning with inexact PMD updates benefits the exploration-exploitation trade-off. Studying this strategy's impact on convergence is a promising future direction. Finally, a crucial question is generalizing our policy compatibility results beyond Sobolev spaces. Ideally, a representation learning process would identify suitable feature maps that guarantee compatibility with the PMD-generated policies while allowing for added flexibility in learning the world model.

## Acknowledgments and Disclosure of Funding

We acknowledge financial support from NextGenerationEU and MUR PNRR project PE0000013 CUP J53C22003010006 "Future Artificial Intelligence Research (FAIR)", EU grant ELISE (GA no 951847) and EU Project ELIAS (GA no 101120237).

## References

* [1] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* [2] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. _The International Journal of Robotics Research_, 39(1):3-20, 2020.

Figure 1: The plots show the average cumulative reward in different environments with respect to the timesteps (i.e. number of interactions with MDP). The dark lines represent the mean of the cumulative reward and the shaded area is the minimum and maximum values reached across \(7\) independent runs. The horizontal dashed lines represent the reward threshold proposed by the _Gym_ library [26].

* [3] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _Nature_, 529(7587):484-489, 2016.
* [4] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. _arXiv1312.5602_, 2013.
* [5] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. Resource management with deep reinforcement learning. In _Proceedings of the 15th ACM workshop on hot topics in networks_, pages 50-56, 2016.
* [6] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv1707.06347_, 2017.
* [7] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization, 2017.
* [8] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, 2018.
* [9] Dimitri Bertsekas. _Dynamic Programming and Optimal Control: Volume I_, volume 4. Athena scientific, 2012.
* [10] Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 5668-5675, 2020.
* [11] Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. _Journal of Machine Learning Research_, 22(98):1-76, 2021.
* [12] Lin Xiao. On the convergence rates of policy gradient methods. _Journal of Machine Learning Research_, 23(282):1-36, 2022.
* [13] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision processes. In _International Conference on Machine Learning_, pages 2160-2169. PMLR, 2019.
* [14] Kenji Fukumizu, Francis R. Bach, and Michael I. Jordan. Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces. _Journal of Machine Learning Research_, 5:73-99, 2004.
* [15] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Scholkopf, et al. Kernel mean embedding of distributions: A review and beyond. _Foundations and Trends(r) in Machine Learning_, 10(1-2):1-141, 2017.
* [16] David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. _Advances in neural information processing systems_, 31, 2018.
* [17] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. _ACM Sigart Bulletin_, 2(4):160-163, 1991.
* [18] Jurgen Schmidhuber. _Making the world differentiable: on using self supervised fully recurrent neural networks for dynamic reinforcement learning and planning in non-stationary environments_, volume 126. Inst. fur Informatik, 1990.
* [19] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.
* [20] Carlo Ciliberto, Lorenzo Rosasco, and Alessandro Rudi. A general framework for consistent structured prediction with implicit loss embeddings. _Journal of Machine Learning Research_, 21(98):1-67, 2020.

* Li et al. [2022] Zhu Li, Dimitri Meunier, Mattes Mollenhauer, and Arthur Gretton. Optimal rates for regularized conditional mean embedding learning. _Advances in Neural Information Processing Systems_, 35:4433-4445, 2022.
* Grunewalder et al. [2012] Steffen Grunewalder, Guy Lever, Luca Baldassarre, Massimiliano Pontil, and Arthur Gretton. Modelling transition dynamics in mdps with rkhs embeddings. In _Proceedings of the 29th International Conference on International Conference on Machine Learning_, pages 1603---1610, 2012.
* Rozwood et al. [2024] Preston Rozwood, Edward Mehrez, Ludger Paehler, Wen Sun, and Steven L Brunton. Koopman-assisted reinforcement learning. _arXiv preprint arXiv:2403.02290_, 2024.
* Moulin and Neu [2023] Antoine Moulin and Gergely Neu. Optimistic planning by regularized dynamic programming. In _International Conference on Machine Learning_, pages 25337-25357. PMLR, 2023.
* Ayoub et al. [2020] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_, pages 463-474. PMLR, 2020.
* Brockman et al. [2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arxiv. _arXiv preprint arXiv:1606.01540_, 10, 2016.
* Beck and Teboulle [2003] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. _Operations Research Letters_, 31(3):167-175, 2003.
* Bubeck [2015] Sebastien Bubeck. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* Kakade [2001] Sham M. Kakade. A natural policy gradient. _Advances in Neural Information Processing Systems_, 14, 2001.
* Aliprantis and Border [1999] C.D. Aliprantis and K.C. Border. _Infinite Dimensional Analysis: A Hitchhiker's Guide_. Studies in Economic Theory. Springer, 1999.
* Agarwal et al. [2021] Alekh Agarwal, Nan Jiang, Sham M. Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. 2021. _URL [https://lthteorybook.github.io_](https://lthteorybook.github.io_), 2022.
* Grunewalder et al. [2012] Steffen Grunewalder, Guy Lever, Luca Baldassarre, Sam Patterson, Arthur Gretton, and Massimilano Pontil. Conditional mean embeddings as regressors. In _Proceedings of the 29th International Conference on International Conference on Machine Learning_, pages 1803-1810, 2012.
* Williams and Seeger [2000] Christopher Williams and Matthias Seeger. Using the Nystrom method to speed up kernel machines. _Advances in Neural Information Processing Systems_, 13, 2000.
* Rudi et al. [2015] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: Nystrom computational regularization. _Advances in Neural Information Processing Systems_, 28, 2015.
* Meanti et al. [2023] Giacomo Meanti, Antoine Chatalic, Vladimir R. Kostic, Pietro Novelli, Massimiliano Pontil, and Lorenzo Rosasco. Estimating Koopman operators with sketching to provably learn large scale dynamical systems. _Advances in Neural Information Processing Systems_, 36, 2023.
* Beck [2017] Amir Beck. _First-Order Methods in Optimization_. Society for Industrial and Applied Mathematics, 2017.
* Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533, 2015.
* Adams and Fournier [2003] Robert A. Adams and John J. F. Fournier. _Sobolev Spaces_. Elsevier, 2003.
* Wendland [2004] Holger Wendland. _Scattered data approximation_, volume 17. Cambridge University Press, 2004.

* Fischer and Steinwart [2020] Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares algorithms. _Journal of Machine Learning Research_, 21(205):1-38, 2020.
* Mnih et al. [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. _arXiv1602.01783_, 2016.
* Kostic et al. [2022] Vladimir R. Kostic, Pietro Novelli, Andreas Maurer, Carlo Ciliberto, Lorenzo Rosasco, and Massimiliano Pontil. Learning dynamical systems via Koopman operator regression in reproducing kernel Hilbert spaces. _Advances in Neural Information Processing Systems_, 35:4017-4031, 2022.
* Turri et al. [2023] Giacomo Turri, Vladimir Kostic, Pietro Novelli, and Massimiliano Pontil. A randomized algorithm to solve reduced rank operator regression. _arXiv preprint arXiv:2312.17348_, 2023.
* Golub and Van Loan [2013] Gene H. Golub and Charles F. Van Loan. _Matrix Computations_. Johns Hopkins University Press, 2013.
* Kallenberg [2002] O. Kallenberg. _Foundations of Modern Probability_. Probability and Its Applications. Springer New York, 2002.
* Luise et al. [2019] Giulia Luise, Saverio Salzo, Massimiliano Pontil, and Carlo Ciliberto. Sinkhorn barycenters with free support via frank-wolfe algorithm. _Advances in Neural Information Processing Systems_, 32, 2019.
* Kostic et al. [2023] Vladimir R. Kostic, Karim Lounici, Pietro Novelli, and Massimiliano Pontil. Sharp spectral rates for koopman operator learning. _Advances in Neural Information Processing Systems_, 36, 2023.
* Raffin et al. [2021] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. _Journal of Machine Learning Research_, 22(268):1-8, 2021.

## Appendix

The appendices are organized as follows:

* Appendix A discuss the operatorial formulation of RL and show how to derive the operator-based results in this work.
* Appendix B focuses on policy mirror descent (PMD) and its convergence rate in the inexact setting.
* Appendix C proves the main result of this work, namely the theoretical analysis of POWR.
* Appendix D provide details on the experiments reported in this work.

## Appendix A Operatorial Results

### Auxiliary Lemma

We recall here a corollary of the Sherman-Woodbury identity [44].

**Lemma A.1**.: _Let \(A\) and \(B\) two conformable linear operators such that \((I+AB)^{-1}\) is invertible. Then \((I+AB)^{-1}A=A(I+BA)^{-1}\)_

Proof.: The result is obvious if \(A\) is invertible. More generally, we consider the following two applications of the Sherman-Woodbury [44] formula

\[(I+AB)^{-1}=I-A(I+BA)^{-1}B\] (A.1)

and

\[(I+BA)^{-1}=I-(I+BA)^{-1}BA.\] (A.2)

Multiplying the two equations by \(A\) respectively to the right and to the left, we obtain the desired result. 

### Markov operators and their properties

We recall here the notion of Markov operators, which is central for a number of results in the following. We refer to [30, Chapter 19] for more details on the topic.

**Definition A.1** (Markov operators).: _Let \(\mathcal{X}\) and \(\mathcal{Y}\) be Polish spaces. A bounded linear operator \(\mathcal{L}(B_{b}(\mathcal{X}),B_{b}(\mathcal{Y}))\) is a Markov operator if is positive and maps the unit function to itself, that is:_

\[\textbf{a. }f\geq 0\in B_{b}(\mathcal{X})\implies\mathsf{P}f\geq 0 \in B_{b}(\mathcal{Y}),\] \[\textbf{b. }\mathsf{P1}_{\mathcal{X}}=\mathbf{1}_{\mathcal{Y}},\]

_where \(\mathbf{1}_{\mathcal{X}}:\mathcal{X}\to\mathbb{R}\) (respectively \(\mathbf{1}_{\mathcal{Y}}\)) denotes the function taking constant value equal to \(1\) on \(\mathcal{X}\) (respectively \(\mathcal{Y}\))._

We recall that Markov operators are a convex subset of \(\mathcal{L}(B_{b}(\mathcal{X}),B_{b}(\mathcal{Y}))\). Here we denote this space as \(\mathcal{L}_{\mathrm{M}}(B_{b}(\mathcal{X}),B_{b}(\mathcal{Y}))\). Direct inspection of (5) and (6) shows that the transfer operator \(\mathsf{T}\) associated to an MDP and the policy operator \(\mathsf{P}_{\pi}\) associated to a policy \(\pi\) are both Markov operators.

**Markov Operators and Policy Operators**. In (6) we defined the policy operator \(\mathsf{P}_{\pi}\) associated to a policy \(\pi\). It turns out that the converse is also true, namely that any such Markov operator is a policy operator.

**Proposition A.2**.: _Let \(\mathsf{P}\in\mathcal{L}_{\mathrm{M}}(B_{b}(\mathcal{X}),B_{b}(\Omega))\) be a Markov operator. Then there exists \(\pi_{\mathsf{P}}\), such that the associated policy operator corresponds to \(\mathsf{P}\), namely \(\mathsf{P}_{\pi_{\mathsf{P}}}=\mathsf{P}\)._

Proof.: Define the map \(\pi_{\mathsf{P}}:\mathcal{X}\to\mathcal{M}(\mathcal{A})\) taking value in the space of bounded Borel measures over \(\mathcal{A}\) such that, for any \(x\in\mathcal{X}\) and any \(\mathcal{B}\subseteq\mathcal{A}\) Borel measurable subset

\[\pi_{\mathsf{P}}(\mathcal{B}|x)=(\mathsf{P1}_{\mathcal{X}\times \mathcal{B}})(x).\] (A.3)We need to guarantee that for every \(x\in\mathcal{X}\) the function \(\pi_{\mathsf{P}}(\cdot|x)\) is a signed measure. To show this, first note that the operation defined by \(\pi_{\mathsf{P}}\) is well-defined, since for any measurable set \(\mathcal{B}\) the function \(\mathbf{1}_{\mathcal{X}\times\mathcal{B}}\) is also measurable, making \(\pi_{\mathsf{P}}(\mathcal{B}|x)\) well defined as well. Moreover, since \(\mathbf{1}_{\emptyset}(a)=0\) for any \(a\in\mathcal{A}\), it implies that \(\mathbf{1}_{\mathcal{X}\times\emptyset}=0\) and therefore \(\pi_{\mathsf{P}}(\emptyset|x)=0\) for any \(x\in\mathcal{X}\). Finally, \(\sigma\)-additivity follows from the definition of indicator functions, namely \(\mathbf{1}_{\bigcup_{i=1}^{\infty}\mathcal{B}_{i}}=\sum_{i=1}^{\infty}\mathbf{ 1}_{\mathcal{B}_{i}}\) for any family of pair-wise disjoint sets \((\mathcal{B}_{i})_{i=1}^{n}\), which implies \(\pi_{\mathsf{P}}\left(\bigcup_{i=1}^{\infty}\mathcal{B}_{i}|x)=\sum_{i=1}^{ \infty}\pi_{\mathsf{P}}(\mathcal{B}_{i}|x)\text{ for any }x\in\mathcal{X}\).

We now apply the two properties of Markov operators to show that \(\pi_{\mathsf{P}}\) takes values in \(\mathcal{P}(\mathcal{A})\), namely it is a non-negative measure that sums to \(1\). Since Markov operators map non-negative functions in non-negative functions and since \(\mathbf{1}_{\mathcal{X}\times\mathcal{B}}\geq 0\) for any \(\mathcal{B}\subseteq\mathcal{X}\), we have \(\pi(\cdot|x)\geq 0\) as well for any \(x\in\mathcal{X}\). Moreover, since \(\Omega=\mathcal{X}\times\mathcal{A}\) and \(\mathsf{\bar{P}}\mathbf{1}_{\Omega}=\mathbf{1}_{\mathcal{X}}\), we have

\[\pi(\mathcal{A}|x)=(\mathsf{P}\ \mathbf{1}_{\Omega})(x)=\mathbf{1}_{\mathcal{X}} (x)=1,\] (A.4)

for any \(x\in\mathcal{X}\). Therefore \(\pi_{\mathsf{P}}(\cdot|x)\) is a probability measure for any \(x\in\mathcal{X}\). Direct application of (6) shows that the associated policy operator corresponds to \(\mathsf{P}\), namely \(\mathsf{P}_{\pi_{\mathsf{P}}}=\mathsf{P}\) as desired. 

Given the correspondence between policies and their Markov operator according to (6) and Proposition A.2, in the following we will denote the policy operator associated to a policy \(\pi\) only \(\mathsf{P}\) where clear from context.

With the definition of the Markov operator in place, we can now prove the following result introduced in the main paper.

**Proposition 2** (Well-specified CME).: _Under Assumption 1, \((\mathsf{T}|_{\mathcal{F}})^{*}=(\mathsf{T}^{*})|_{\mathcal{G}}\) and, for any \((x,a)\in\Omega\)_

\[(\mathsf{T}|_{\mathcal{F}})^{*}\psi(x,a)=\int_{\mathcal{X}}\varphi(x^{\prime}) \;\tau(x^{\prime}|x,a)=\mathbb{E}[\varphi(X^{\prime})|X=x,A=a]. \tag{9}\]

Proof.: Recall that since they are Hilbert spaces \(\mathcal{F}\cong\mathcal{F}^{*}\) and \(\mathcal{G}\cong\mathcal{G}^{*}\) are isometric to their dual and therefore we can interpret any \(f\in\mathcal{F}\) as the function \(f(\cdot)=\langle f,\varphi(\cdot)\rangle\) with some abuse of notation, where clear from context. By Assumption 1 we have that \(\mathsf{T}|_{\mathcal{F}}\) takes values in \(\mathcal{G}\). This means that \((\mathsf{T}|_{\mathcal{F}}f)\in\mathcal{G}\) or, in other words

\[\langle\mathsf{T}|_{\mathcal{F}}f,\psi(x,a)\rangle =(\mathsf{T}|_{\mathcal{F}}f)(x,a)\] \[=\int_{\mathcal{F}}f(x^{\prime})\;\tau(dx^{\prime}|x,a)\] \[=\int_{\mathcal{F}}\left\langle f,\varphi(x^{\prime})\right\rangle \;\tau(dx^{\prime}|x,a)\] \[=\left\langle f,\int\varphi(x^{\prime})\;\tau(dx^{\prime}|x,a) \right\rangle,\]

from which we obtain

\[\langle f,(\mathsf{T}|_{\mathcal{F}})^{*}\psi(x,a)\rangle=\left\langle f,\int \varphi(x^{\prime})\;\tau(dx^{\prime}|x,a)\right\rangle.\]

Since the above equality holds for any \(f\in\mathcal{F}\) (9) holds, as desired. 

We note that the result can be extended to the setting where \(\mathsf{T}|_{\mathcal{F}}(\mathcal{F})\subseteq\mathcal{G}\), namely the image of \(\mathsf{T}|_{\mathcal{F}}\) is contained in \(\mathcal{G}\), namely a sort of \((\mathcal{F},\mathcal{G})\)-compatibility for the transition operator (see Definition 1).

### The operatorial formulation of RL

According to the operatorial characterization in (7), the action value function of a policy \(\pi\) is directly related to the action of the associated policy operator \(\mathsf{P}\). To highlight this relation, we will adopt the following notation:

* **Action-value (or Q-)function.** \[q(\mathsf{P})=\left(\mathsf{Id}-\gamma\mathsf{TP}\right)^{-1}r.\] (A.5)* **Value function.** \[v(\mathsf{P})=\mathsf{P}q(\mathsf{P}).\] (A.6)
* **Cumulative reward.** The RL objective functional \[J(\mathsf{P})=\left\langle\mathsf{P}\left(\mathsf{Id}-\gamma\mathsf{ TP}\right)^{-1}r,\nu\right\rangle=\left\langle\mathsf{P}q(\mathsf{P}),\nu\right\rangle =\left\langle v(\mathsf{P}),\nu\right\rangle.\] (A.7)
* **State visitation (or State occupancy) measure.** By the characterization of the adjoints of \(\mathsf{P}\) and \(\mathsf{T}\) (see discussion in Section 3 we can represent the evolution of a state distribution \(\nu_{t}\) at time \(t\) to the next state distribution as \(\nu_{t+1}=\mathsf{T}^{*}\mathsf{P}^{*}\nu_{t}\). Applying this relation recursively, we recover the _state visitation probability_ associated to the starting state distribution \(\nu_{0}=\nu\in\mathcal{P}(\mathcal{X})\), the MDP with transition \(\mathsf{T}\) and the policy \(\mathsf{P}\) as \[d_{\nu}(\mathsf{P})=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}(\mathsf{T}^{*} \mathsf{P}^{*})^{t}\nu=(1-\gamma)\left(\mathsf{Id}-\gamma\mathsf{PT}\right)^{ -*}\nu,\] (A.8) where the \((1-\gamma)\gamma^{t}\) is a normalizing factor to guarantee that the series corresponds to a convex combination of the probability distributions \(\nu_{t}\), hence guaranteeing \(d_{\nu}(\mathsf{P})\) to be well-defined (namely it belongs to \(\mathcal{P}(\mathcal{X})\)).

**Previous well-known RL results in operator form**. Under the operatorial formulation of RL, we can recover several well-known results from the reinforcement literature with concise proofs. We recall here a few of these results that will be useful in the following.

**Remark A.1**.: _Algebraic manipulation of the cumulative expected reward \(J(\mathsf{P})\) implies_

\[J(\mathsf{P})=\left\langle\mathsf{P}\left(\mathsf{Id}-\gamma \mathsf{TP}\right)^{-1}r,\nu\right\rangle=\left\langle\mathsf{P}r,\left(\mathsf{ Id}-\gamma\mathsf{PT}\right)^{-*}\nu\right\rangle=\frac{1}{1-\gamma}\left\langle \mathsf{P}r,d_{\nu}(\mathsf{P})\right\rangle,\]

_where we used Lemma A.1 and \(d_{\nu}(\mathsf{P})\) is the state visitation distribution starting from \(\nu\) and following the policy \(\mathsf{P}\)._

The following result, known as _Performance Difference_ Lemma [see e.g. 11, Lemma 1.16], will be instrumental to prove the convergence rates for PMD in Theorem 7.

**Lemma A.3** (Performance difference).: _Let \(\mathsf{P}_{1}\), \(\mathsf{P}_{2}\) two policy operators. The following equality holds_

\[J(\mathsf{P}_{1})-J(\mathsf{P}_{2})=\frac{1}{1-\gamma}\left\langle(\mathsf{P }_{1}-\mathsf{P}_{2})q(\mathsf{P}_{2}),d_{\nu}(\mathsf{P}_{1})\right\rangle.\] (A.9)

Proof.: Using the definition of \(J(\mathsf{P}_{1})\) and Lemma A.1 one gets

\[J(\mathsf{P}_{1})-J(\mathsf{P}_{2})\] \[= \left\langle(\mathsf{Id}-\gamma\mathsf{P}_{1}\mathsf{T})^{-1} \mathsf{P}_{1}r,\nu\right\rangle-\left\langle\mathsf{P}_{2}\left(\mathsf{Id}- \gamma\mathsf{TP}_{2}\right)^{-1}r,\nu\right\rangle\] \[= \left\langle(\mathsf{Id}-\gamma\mathsf{P}_{1}\mathsf{T})^{-1} \mathsf{P}_{1}\left(\mathsf{Id}-\gamma\mathsf{TP}_{2}\right)\left(\mathsf{Id}- \gamma\mathsf{TP}_{2}\right)^{-1}r,\nu\right\rangle\] \[\quad-\left\langle(\mathsf{Id}-\gamma\mathsf{P}_{1}\mathsf{T})^{ -1}\left(\mathsf{Id}-\gamma\mathsf{P}_{1}\mathsf{T}\right)\mathsf{P}_{2}\left( \mathsf{Id}-\gamma\mathsf{TP}_{2}\right)^{-1}r,\nu\right\rangle\] \[= \left\langle(\mathsf{Id}-\gamma\mathsf{P}_{1}\mathsf{T})^{-1} \left[\mathsf{P}_{1}\left(\mathsf{Id}-\gamma\mathsf{TP}_{2}\right)-\left( \mathsf{Id}-\gamma\mathsf{P}_{1}\mathsf{T}\right)\mathsf{P}_{2}\right]\left( \mathsf{Id}-\gamma\mathsf{TP}_{2}\right)^{-1}r,\nu\right\rangle\] \[= \left\langle(\mathsf{Id}-\gamma\mathsf{P}_{1}\mathsf{T})^{-1} \left[\mathsf{P}_{1}-\mathsf{P}_{2}\right]\left(\mathsf{Id}-\gamma\mathsf{TP} _{2}\right)^{-1}r,\nu\right\rangle\] \[= \frac{1}{1-\gamma}\left\langle(\mathsf{P}_{1}-\mathsf{P}_{2})q( \mathsf{P}_{2}),d_{\nu}(\mathsf{P}_{1})\right\rangle.\]

A direct consequence of the operator formulation of the performance difference lemma is the following operator-based characterization of the differential behavior of the RL objective. The result can be found in [11] for the case of finite state and action spaces, however here the operatorial formulation allows for a much more concise proof.

**Corollary A.4** (Directional derivatives).: _For any two Markov \(\mathsf{P}_{1},\mathsf{P}_{2}:B_{b}(\mathcal{X})\to B_{b}(\Omega)\), we have that the directional derivative in \(\mathsf{P}_{1}\) towards \(\mathsf{P}_{2}\) is_

\[\lim_{h\to 0}\frac{J(\mathsf{P}_{1}+h(\mathsf{P}_{2}-\mathsf{P}_{1}))-J( \mathsf{P}_{1})}{h}=\frac{1}{1-\gamma}\left\langle(\mathsf{P}_{2}-\mathsf{P}_{ 1})q(\mathsf{P}_{1}),d_{\nu}(\mathsf{P}_{1})\right\rangle.\] (A.10)

Proof.: The result follows by recalling that the space of Markov operators is convex, namely for any \(h\in[0,1]\) the term \(\mathsf{P}_{h}=\mathsf{P}_{1}+h(\mathsf{P}_{2}-\mathsf{P}_{1})\) is still a Markov operator. Therefore, we can apply Lemma A.3 to obtain

\[J(\mathsf{P}_{h})-J(\mathsf{P}_{1}) =\left\langle(\mathsf{P}_{h}-\mathsf{P}_{1})q(\mathsf{P}_{1}),d_{ \nu}(\mathsf{P}_{h})\right\rangle\] (A.11) \[=h\left\langle(\mathsf{P}_{2}-\mathsf{P}_{1})q(\mathsf{P}_{1}),d _{\nu}(\mathsf{P}_{h})\right\rangle.\] (A.12)

We can therefore divide the above quantity by \(h\) and send \(h\to 0\). The result follows by observing that \(d_{\nu}(\mathsf{P}_{h})=(\mathsf{Id}-\gamma\mathsf{TP}_{h})^{-*}\nu\mapsto( \mathsf{Id}-\gamma\mathsf{TP}_{1})^{-*}\nu=d_{\nu}(\mathsf{P}_{1})\) for \(h\to 0\), since \(\left\|\mathsf{P}_{h}\right\|=1\) for any \(h\in[0,1]\) and the function \(\mathsf{M}\mapsto(I-\gamma\mathsf{TM})^{-1}\) is continuous on the open ball of radius \(1/\gamma>1\) in \(\mathcal{L}(B_{b}(\Omega),B_{b}(\mathcal{X}))\) with respect to the operator norm. 

**Properties of \((\mathsf{Id}-\gamma\mathsf{PT})^{-1}\)**. The quantity \((\mathsf{Id}-\gamma\mathsf{PT})^{-1}\) (note, not \((\mathsf{Id}-\gamma\mathsf{TP})^{-1}\)) plays a central role in the study of POWR. We prove here a few properties that will be useful in the following.

**Lemma A.5** (Properties of \(\mathsf{Id}-\gamma\mathsf{PT}\)).: _The following facts are true:_

1. _For any_ \(f\geq 0\in B_{b}(\mathcal{X})\) _it holds_ \((\mathsf{Id}-\gamma\mathsf{PT})^{-1}f\geq f\)_._
2. _The operator_ \((1-\gamma)(\mathsf{Id}-\gamma\mathsf{PT})^{-1}\) _is a Markov operator._
3. _For any positive measure_ \(\nu\in\mathcal{M}(B_{b}(\mathcal{X}))\) _it holds_ \((1-\gamma)\left\|(\mathsf{Id}-\gamma\mathsf{PT})^{-*}\nu\right\|_{\mathrm{TV}} =\left\|\nu\right\|_{\mathrm{TV}}\)_._
4. _For any positive measure_ \(\nu\in\mathcal{M}(B_{b}(\mathcal{X}))\) _it holds_ \(\left\|\mathsf{P}^{*}\nu\right\|_{\mathrm{TV}}=\left\|\nu\right\|_{\mathrm{TV}}\)_._
5. _For any bounded linear operator_ \(\mathsf{X}\)_, policy operator_ \(\mathsf{P}\) _and discount factor_ \(\gamma<\left\|\mathsf{X}\right\|\)_, it holds_ \(\left\|(\mathsf{Id}-\gamma\mathsf{XP})^{-1}\right\|_{\infty}\leq 1/(1-\gamma \left\|\mathsf{X}\right\|)\)_._

Proof.: Since both \(\mathsf{T}\) and \(\mathsf{P}\) are Markov operators by construction, it immediately follows that their composition is a Markov operator as well. Using the Neumann series representation of \((\mathsf{Id}-\gamma\mathsf{PT})^{-1}\) it follows that for all \(f\geq 0\in B_{b}(\mathcal{X})\)

\[(\mathsf{Id}-\gamma\mathsf{PT})^{-1}f=\sum_{t=0}^{\infty}\gamma^{t}(\mathsf{PT })^{t}f=f+\sum_{t=1}^{\infty}\gamma^{t}(\mathsf{PT})^{t}f\geq f\geq 0,\]

proving (1). Further,

\[(\mathsf{Id}-\gamma\mathsf{PT})^{-1}\mathbf{1}_{\mathcal{X}}=\sum_{t=0}^{ \infty}\gamma^{t}(\mathsf{PT})^{t}\mathbf{1}_{\mathcal{X}}=\sum_{t=0}^{\infty }\gamma^{t}\mathbf{1}_{\mathcal{X}}=\frac{\mathbf{1}_{\mathcal{X}}}{1-\gamma}\]

showing that \((1-\gamma)(\mathsf{Id}-\gamma\mathsf{PT})^{-1}\mathbf{1}_{\mathcal{X}}= \mathbf{1}_{\mathcal{X}}\) and proving (2). Finally, since \((1-\gamma)(\mathsf{Id}-\gamma\mathsf{PT})^{-1}\) and \(\mathsf{P}\) are Markov operators, (3) and (4) follow from the direct application of [30, Theorem 19.2]. For the last point (5), let \(f\in B_{b}(\Omega)\). As \(\mathsf{P}\) is a conditional expectation operator, it holds that

\[\left\|\mathsf{XP}\right\|_{\infty}\leq\left\|\mathsf{X}\right\|\mathsf{P}( \mathbf{1}_{\Omega}\left\|f\right\|_{\infty})=\left\|\mathsf{X}\right\|\left\|f \right\|_{\infty}\]

Where the inequality is just the conditional version of Jensen's inequality [45, Chapter 5] applied on the (convex) \(\left\|\cdot\right\|_{\infty}\) function, while the equality comes from the fact that \(\mathsf{TP}\) is a Markov operator. Then, we have

\[\sup_{\left\|f\right\|_{\infty}=1}\bigl{\|}(\mathsf{Id}-\gamma \mathsf{XP})^{-1}f\bigr{\|}_{\infty} =\sup_{\left\|f\right\|_{\infty}=1}\bigl{\|}\sum_{t=0}^{\infty}( \gamma\mathsf{XP})^{t}f\bigr{\|}_{\infty}\] \[\leq\sup_{\left\|f\right\|_{\infty}=1}\sum_{t=0}^{\infty}\gamma^{ t}\bigl{\|}(\mathsf{XP})^{t}f\bigr{\|}_{\infty}\] \[\leq\sup_{\left\|f\right\|_{\infty}=1}\sum_{t=0}^{\infty}\gamma^{ t}\bigl{\|}\mathsf{X}\bigr{\|}^{t}\bigl{\|}f\bigr{\|}_{\infty}\] \[(\left\|f\right\|_{\infty}=1) =\frac{1}{1-\gamma\bigl{\|}\mathsf{X}\bigr{\|}}.\]

**Simulation Lemma**. We report here the Simulation lemma, since it will be key to bridging the gap between Policy Mirror Descent and Conditional Mean Embeddings in Theorem 9 through Lemma 8.

**Lemma A.6** (Simulation Lemma [31]-Lemma 2.2).: _Let \(\gamma>0\) and let \(\mathsf{T}_{1}\), \(\mathsf{T}_{2}\) two linear operators with operator norm strictly less than \(\gamma\). Let \(\mathsf{P}\) be a policy operator. Denote by \(q(\mathsf{P},\mathsf{T})=(\mathsf{Id}-\gamma\mathsf{TP})^{-1}r\) the (generalized) action-value function associated to these terms and \(v(\mathsf{P},\mathsf{T})=\mathsf{P}q(\mathsf{P},\mathsf{T})\) the corresponding value function. Then the following equality holds_

\[q(\mathsf{P},\mathsf{T}_{1})-q(\mathsf{P},\mathsf{T}_{2})=\gamma\left(\mathsf{ Id}-\gamma\mathsf{T}_{1}\mathsf{P}\right)^{-1}\left(\mathsf{T}_{2}-\mathsf{T}_{1 }\right)v(\mathsf{P},\mathsf{T}_{2})\] (A.13)

Proof.: Using the same technique of the proof of Lemma A.3 one has

\[q(\mathsf{P},\mathsf{T}_{1})-q(\mathsf{P},\mathsf{T}_{2}) =\left(\mathsf{Id}-\gamma\mathsf{T}_{1}\mathsf{P}\right)^{-1}r- \left(\mathsf{Id}-\gamma\mathsf{T}_{2}\mathsf{P}\right)^{-1}r\] \[=\gamma\left(\mathsf{Id}-\gamma\mathsf{T}_{1}\mathsf{P}\right)^{- 1}\left(\mathsf{T}_{2}-\mathsf{T}_{1}\right)\mathsf{P}\left(\mathsf{Id}-\gamma \mathsf{T}_{2}A\right)^{-1}r\] \[=\gamma\left(\mathsf{Id}-\gamma\mathsf{T}_{1}\mathsf{P}\right)^{- 1}\left(\mathsf{T}_{2}-\mathsf{T}_{1}\right)v(\mathsf{P},\mathsf{T}_{2})\]

where we have used fact that for any two invertible operators \(\mathsf{M}\) and \(\mathsf{P}\) it holds \(\mathsf{M}^{-1}-\mathsf{P}^{-1}=\mathsf{M}^{-1}(\mathsf{P}-\mathsf{M})\mathsf{ P}^{-1}\) for the second equation and applied the operatorial characterization of the value function to conclude the proof. 

We then have the following result, which hinges on a generalization of the standard Simulation lemma in [31, Lemma 2.2] where we account also for the reward function to vary.

**Corollary A.7**.: _Let \(\gamma>0\) and let \(\mathsf{T}_{1}\), \(\mathsf{T}_{2}\) two linear operators with operator norm strictly less than \(\gamma\). Let \(r_{1}\) and \(r_{2}\) be two reward functions and \(\mathsf{P}\) a policy operator. Denote by \(q(\mathsf{P},\mathsf{T},r)=(\mathsf{Id}-\gamma\mathsf{TP})^{-1}r\) the (generalized) action-value function associated to these terms and \(v(\mathsf{P},\mathsf{T},r)=\mathsf{P}q(\mathsf{P},\mathsf{T},r)\) the corresponding value function. Then the following equality holds_

\[q(\mathsf{P},\mathsf{T}_{1},r_{1})-q(\mathsf{P},\mathsf{T}_{2},r_{2}) =(\mathsf{Id}-\gamma\mathsf{T}_{1}\mathsf{P})^{-1}(r_{1}-r_{2}) +\gamma(\mathsf{Id}-\gamma\mathsf{T}_{1}\mathsf{P})^{-1}(\mathsf{T}_{2}- \mathsf{T}_{1})v(\mathsf{P},\mathsf{T}_{2},r_{2}).\]

Proof.: The difference between action-value functions can be written as

\[q(\mathsf{P},\mathsf{T}_{1},r_{1})-q(\mathsf{P},\mathsf{T}_{2},r _{2}) =(\mathsf{Id}-\gamma\mathsf{T}_{1}\mathsf{P})^{-1}r_{1}-(\mathsf{ Id}-\gamma\mathsf{T}_{2}\mathsf{P})^{-1}r_{2}\] \[=(\mathsf{Id}-\gamma\mathsf{T}_{1}\mathsf{P})^{-1}(r_{1}-r_{2}) +\left[(\mathsf{Id}-\gamma\mathsf{T}_{1}\mathsf{P})^{-1}-(\mathsf{Id}-\gamma \mathsf{T}_{2}\mathsf{P})^{-1}\right]r_{2}\]

where we added and removed a term \((\mathsf{Id}-\gamma\mathsf{T}_{1}\mathsf{P})^{-1}r_{2}\). The result follows by plugging in the Simulation Lemma A.6 for the second term of the right hand side. 

The corollary above will be useful in Appendix C to control the approximation error of the estimates \(\hat{q}_{\pi_{\epsilon}}\) appearing in the convergence rates for inexact PMD in Theorem 7.

### Action-value Estimator for \((\mathcal{G},\mathcal{F})\)-compatible Policies

We can leverage the notation introduced in this section to prove the following form for the world model-based estimator of the action-value function.

**Proposition 3**.: _Let \(\mathsf{T}_{n}=S_{n}^{*}BZ_{n}\in\mathsf{HS}(\mathcal{F},\mathcal{G})\) and \(r_{n}=S_{n}^{*}b\in\mathcal{G}\) for respectively a \(B\in\mathbb{R}^{n\times n}\) and \(b\in\mathbb{R}^{n}\). Let \(\pi\) be \((\mathcal{G},\mathcal{F})\)-compatible. Then,_

\[\hat{q}_{\pi}=(\mathsf{Id}-\gamma\mathsf{T}_{n}\mathsf{P}_{\pi})^{-1}r_{n}=S_{ n}^{*}(\mathsf{Id}-\gamma BM_{\pi})^{-1}b \tag{12}\]

_where \(M_{\pi}=Z_{n}\mathsf{P}_{\pi}S_{n}^{*}\in\mathbb{R}^{n\times n}\) is the matrix with entries_

\[\left(M_{\pi}\right)_{ij}=\left\langle\varphi(x_{i}^{\prime}),\mathsf{P}_{\pi} \psi(x_{j},a_{j})\right\rangle=\int_{\mathcal{A}}\left\langle\psi(x_{i}^{ \prime},a),\psi(x_{j},a_{j})\right\rangle\;\pi(da|x_{i}^{\prime}). \tag{13}\]

Proof.: By hypothesis

\[\hat{q}_{\pi}=(\mathsf{Id}-\gamma\mathsf{T}_{n}\mathsf{P}_{\pi})^{-1}r_{n}=( \mathsf{Id}-\gamma S_{n}^{*}BZ_{n}\mathsf{P}_{\pi})^{-1}S_{n}^{*}b.\] (A.14)Eq. (12) follows by applying Lemma A.1. Eq. (13) can be verified by direct calculation. Denote by \((e_{i})_{i=1}^{m}\) the vectors of the canonical basis in \(\mathbb{R}^{n}\). Then, for any \(i,j=1,\ldots,n\)

\[(M_{\pi})_{ij}=\langle e_{i},M_{\pi}e_{j}\rangle=\langle e_{i},Z_{n}\mathsf{P}_{ \pi}S_{n}^{*}e_{j}\rangle=\langle Z_{n}^{*}e_{i},\mathsf{P}_{\pi}S_{n}^{*}e_{j} \rangle\,.\] (A.15)

Now, we recall that the two operators \(S_{n}:\mathcal{G}\rightarrow\mathbb{R}^{n}\) and \(Z_{n}:\mathcal{F}\rightarrow\mathbb{R}^{n}\) are the evaluation operators for respectively the points \((x_{i},a_{i})_{i=1}^{n}\) and \((x_{i}^{\prime})_{i=1}^{n}\). Namely, for any vector \(v\in\mathbb{R}^{n}\)

\[S_{n}^{*}v=\sum_{i=1}^{n}v_{i}\psi(x_{i},a_{i})\qquad\text{and}\qquad Z_{n}^{ *}v=\sum_{i=1}^{n}v_{i}\varphi(x_{i}^{\prime}).\] (A.16)

This implies that

\[(M_{\pi})_{ij}=\langle Z_{n}^{*}e_{i},\mathsf{P}_{\pi}S_{n}^{*}e_{j}\rangle= \langle\varphi(x_{i}^{\prime}),\mathsf{P}_{\pi}\psi(x_{j},a_{j})\rangle\] (A.17)

Since \(\mathsf{P}_{\pi}\) is \((\mathcal{G},\mathcal{F})\)-compatible by hypothesis, we can leverage the same reasoning used in Proposition 2 to show that

\[(\mathsf{P}_{\pi}|_{\mathcal{G}})^{*}\varphi(x^{\prime})=\int_{\mathcal{A}} \psi(x^{\prime},a)\,\pi(da|x^{\prime})\] (A.18)

for any \(x^{\prime}\in\mathcal{X}\). By plugging this equation in the previous characterization for \((M_{\pi})_{ij}\) we have

\[\langle\varphi(x_{i}^{\prime}),\mathsf{P}_{\pi}\psi(x_{j},a_{j})\rangle =\langle\mathsf{P}_{\pi}^{*}\varphi(x_{i}^{\prime}),\psi(x_{j},a_{ j})\rangle\] (A.19) \[=\left\langle\int_{\mathcal{A}}\psi(x_{i}^{\prime},a)\,\pi(da|x_{ i}^{\prime}),\psi(x_{j},a_{j})\right\rangle\] (A.20) \[=\int_{\mathcal{A}}\left\langle\psi(x_{i}^{\prime},a),\psi(x_{j}, a_{j})\right\rangle\,\pi(da|x_{i}^{\prime}),\] (A.21)

as required. 

### Separable Spaces

We show here the sufficient condition for \((\mathcal{G},\mathcal{F})\)-compatibility of a policy in the case of the separable spaces introduced in Section 4.

**Proposition 4** (Separable Spaces).: _Let \(\phi:\mathcal{X}\rightarrow\mathcal{H}\) be a feature map into a Hilbert space \(\mathcal{H}\). Let \(\mathcal{F}=\mathcal{H}\otimes\mathcal{H}\) and \(\mathcal{G}=\mathbb{R}^{|\mathcal{A}|}\otimes\mathcal{H}\) with feature maps respectively \(\varphi(x)=\phi(x)\otimes\phi(x)\) and \(\psi(x,a)=\phi(x)\otimes e_{a}\), with \(e_{a}\in\mathbb{R}^{|\mathcal{A}|}\) the one-hot encoding of action \(a\in\mathcal{A}\). Let \(\pi:\mathcal{X}\rightarrow\Delta(\mathcal{A})\) be a policy such that \(\pi(a|\cdot)=\langle p_{a},\phi(\cdot)\rangle\) with \(p_{a}\in\mathcal{H}\) for any \(a\in\mathcal{A}\). Then, \(\pi\) is \((\mathcal{G},\mathcal{F})\)-compatible._

Proof.: The proposition follows from observing that for any \(v\in\mathbb{R}^{|\mathcal{A}|}\) and \(h\in\mathcal{H}\), applying \(\mathsf{P}_{\pi}\) according to (6) to the function \(g(x,a)=\left\langle h,\phi(x)\right\rangle\left\langle v,De_{a}\right\rangle\) yields

\[(\mathsf{P}_{\pi}g)(x)=\sum_{a\in\mathcal{A}}g(x,a)\pi(a|x) =\left\langle h,\phi(x)\right\rangle\sum_{a\in\mathcal{A}}\left \langle v,De_{a}\right\rangle\pi(a|x)\] (A.22) \[=\left\langle h,\phi(x)\right\rangle\sum_{a\in\mathcal{A}}\left \langle v,De_{a}\right\rangle\left\langle p_{a},\phi(x)\right\rangle\] (A.23) \[=\left\langle h\otimes\sum_{a\in\mathcal{A}}\left\langle v,De_{ a}\right\rangle p_{a},\phi(x)\otimes\phi(x)\right\rangle\] (A.24)

Hence \((\mathsf{P}_{\pi}g)(x)=\left\langle f,\varphi(x)\right\rangle\) with \(f=h\otimes h^{\prime}\in\mathcal{H}\otimes\mathcal{H}=\mathcal{F}\) and \(h^{\prime}=\sum_{a\in\mathcal{A}}\left\langle v,De_{a}\right\rangle p_{a}\in \mathcal{H}\). Therefore, the restriction of \(\mathsf{P}_{\pi}\) to \(\mathcal{G}\) takes value in \(\mathcal{F}\) as desired. 

## Appendix B Policy Mirror Descent

In this section we briefly review the tools needed to formulate the PMD method and discuss the convergence rates for inexact PMD. Most of the discussion follows the presentation in [12] formulated within the notation used in this work.

[MISSING_PAGE_FAIL:20]

Where in the first line we used the definition of Bregman divergence [36, Definition 9.2]\(D(p;q):=\psi(p)-\psi(q)-\langle\nabla\psi(q),p-q\rangle\) for a suitable Legendre function \(\psi:\Delta(\mathcal{A})\to\mathbb{R}\), the first implication follows from the three-points property of Bregman divergences [36, Lemma 9.11], and the last implication from the positivity of \(D(\pi_{t+1}(x);\pi_{t}(x))\). 

**Corollary B.3** (MD iterations are monotonically increasing).: _This Corollary is essentially a restatement of [12, Lemma 7]. Let \((\mathsf{P}_{t})_{t\in\mathbb{N}}\) be the sequence of policy operators associated to the measurable minimizers of (B.1) for all \(t\in\mathbb{N}\). For all \(x\in\mathcal{X}\) it holds_

\[\left[(\mathsf{P}_{t+1}-\mathsf{P}_{t})q(\mathsf{P}_{t})\right](x)\geq 0\] (B.5)

_and_

\[J(\mathsf{P}_{t+1})-J(\mathsf{P}_{t})\geq 0\] (B.6)

_i.e. the objective function is always increased by a mirror descent iteration. Further, if \(\tilde{q}(\mathsf{P}_{t})\in B_{b}(\Omega)\) is such that \(\left\|q(\mathsf{P}_{t})-\tilde{q}(\mathsf{P}_{t})\right\|_{\infty}\leq \varepsilon_{t}\), then (B.5) holds inexactly on \(\tilde{q}(\mathsf{P}_{t})\) as_

\[\left[(\mathsf{P}_{t+1}-\mathsf{P}_{t})\tilde{q}(\mathsf{P}_{t})\right](x) \geq-2\varepsilon_{t}.\] (B.7)

Proof.: By setting \(\pi(x)=\pi_{t}(x)\) in (B.3), and recalling that \(D(p;q)\geq 0\) with equality if and only if \(p=q\), it follows that

\[\eta\left[(\mathsf{P}_{t+1}-\mathsf{P}_{t})q(\mathsf{P}_{t})\right](x)\geq D( \pi_{t}(x);\pi_{t+1}(x))\geq 0,\]

giving (B.5). Integrating (B.5) over \(\left(\mathsf{Id}-\gamma\mathsf{P}_{t+1}\mathsf{T}\right)^{-*}\nu\) and using the Performance Difference Lemma A.3 one gets (B.6). Finally, we get (B.7) from

\[\left[(\mathsf{P}_{t+1}-\mathsf{P}_{t})\tilde{q}(\mathsf{P}_{t}) \right](x) =\left[(\mathsf{P}_{t+1}-\mathsf{P}_{t})q(\mathsf{P}_{t})\right]( x)+\left[(\mathsf{P}_{t+1}-\mathsf{P}_{t})(\tilde{q}(\mathsf{P}_{t})-q(\mathsf{P}_{t})) \right](x)\] (B.8) \[\geq\left[(\mathsf{P}_{t+1}-\mathsf{P}_{t})(\tilde{q}(\mathsf{P} _{t})-q(\mathsf{P}_{t}))\right](x)\] \[\geq-\left\|(\mathsf{P}_{t+1}-\mathsf{P}_{t})(\tilde{q}(\mathsf{ P}_{t})-q(\mathsf{P}_{t}))\right\|_{\infty}\] \[\geq-2\varepsilon_{t}.\]

Where the first inequality follows from (B.5), and the latter from the fact that policy operators are Markov operators and have norm 1, and \(\left\|\mathsf{P}_{t+1}-\mathsf{P}_{t}\right\|\leq\left\|\mathsf{P}_{t+1} \right\|+\left\|\mathsf{P}_{t}\right\|=2\). 

### Convergence rates of PMD

We are finally ready to prove the convergence rates for the Policy Mirror Descent algorithm (B.1). The proof technique is loosely based on [12, Theorem 8, Lemma 12], and extends them to the case of general state spaces through the key Lemma B.1 and using a fully operatorial formalism.

**Theorem 7** (Convergence of Inexact PMD).: _Let \((\pi_{t})_{t\in\mathbb{N}}\) be a sequence of policies generated by \(Algorithm\)1 that are all \((\mathcal{G},\mathcal{F})\)-compatible. If the action-value functions \(\hat{q}_{\pi_{t}}\) are estimated with an error \(\left\|q_{\pi_{t}}-\hat{q}_{\pi_{t}}\right\|_{\infty}\leq\varepsilon_{t}\), the iterates of Algorithm 1 converge to the optimal policy as_

\[J(\pi_{*})-J(\pi_{T})\leq\varepsilon_{T}+O\left(\frac{1}{T}+\frac{1}{T}\sum_{t =0}^{T-1}\varepsilon_{t}\right), \tag{16}\]

_where \(\pi_{*}:\mathcal{X}\to\Delta(\mathcal{A})\) is a measurable maximizer of (8)._

Proof.: As usual, in this proof we denote the estimated and exact action-value functions as \(q_{n}(\mathsf{P}_{t}):=\hat{q}_{\pi_{t}}=(\mathsf{Id}-\gamma\mathsf{T}_{n} \mathsf{P}_{t})^{-1}r_{n}\) and \(q(\mathsf{P}_{t}):=q_{\pi_{t}}=(\mathsf{Id}-\gamma\mathsf{T}\mathsf{P}_{t})^ {-1}r\), respectively. From hypothesis, Algorithm 1 is well-defined since all policies it generates are \((\mathcal{G},\mathcal{F})\)-compatible. The resulting sequence of policies \((\pi_{t})_{t\in\mathbb{N}}\) are generated via the update rule (14) on the inexact action-value functions \(q_{n}(\mathsf{P}_{t})\), as defined in (12). As the update rule (14) is a (measurable) minimizer of (B.1) when \(D\) equals the Kullback-Leibler divergence, the three-points Lemma B.2 with \(\pi(x)=\pi_{*}(x)\) yields

\[\left[(\mathsf{P}_{*}-\mathsf{P}_{t+1})q_{n}(\mathsf{P}_{t})\right](x)\leq\frac {1}{\eta}D(\pi_{*}(x);\pi_{t}(x))-\frac{1}{\eta}D(\pi_{*}(x);\pi_{t+1}(x)).\]Adding and subtracting the term \(\left[(\mathsf{P}_{*}-\mathsf{P}_{t+1})q(\mathsf{P}_{t})\right](x)\), and bounding the remaining difference as \(\left[(\mathsf{P}_{*}-\mathsf{P}_{t+1})(q(\mathsf{P}_{t})-q_{n}(\mathsf{P}_{t}) )\right](x)\leq 2\varepsilon_{t}\) - see the derivation of (B.8) - one gets

\[\left[(\mathsf{P}_{*}-\mathsf{P}_{t+1})q(\mathsf{P}_{t})\right](x)\leq 2 \varepsilon_{t}+\frac{1}{\eta}D(\pi_{*}(x);\pi_{t}(x))-\frac{1}{\eta}D(\pi_{*}( x);\pi_{t+1}(x)).\]

Adding and subtracting \(\mathsf{P}_{t}q(\mathsf{P}_{t})\) on the left side gives

\[\left[(\mathsf{P}_{*}-\mathsf{P}_{t})q(\mathsf{P}_{t})\right](x)\leq\left[( \mathsf{P}_{t+1}-\mathsf{P}_{t})q(\mathsf{P}_{t})\right](x)+2\varepsilon_{t}+ \frac{1}{\eta}D(\pi_{*}(x);\pi_{t}(x))-\frac{1}{\eta}D(\pi_{*}(x);\pi_{t+1}(x)),\]

and integrating with respect to the positive measure \((\mathsf{Id}-\gamma\mathsf{P}_{*}\mathsf{T})^{-*}\nu\) and using the performance difference Lemma A.3 on the left hand side one has

\[J(\mathsf{P}_{*})-J(\mathsf{P}_{t})\leq \frac{1}{1-\gamma}\left\langle(\mathsf{P}_{t+1}-\mathsf{P}_{t})q (\mathsf{P}_{t})+2\varepsilon_{t},d_{\nu}(\mathsf{P}_{*})\right\rangle\] (B.9) \[\frac{1}{\eta(1-\gamma)}\left\langle D(\pi_{*};\pi_{t})-D(\pi_{*} ;\pi_{t+1}),d_{\nu}(\mathsf{P}_{*})\right\rangle,\]

where we used (A.8) on the right-hand-side terms. Since \((\mathsf{P}_{t+1}-\mathsf{P}_{t})q(\mathsf{P}_{t})+2\varepsilon_{t}\geq 0\) because of (B.7), we can use fact (1) from Lemma A.5 with \((\mathsf{Id}-\gamma\mathsf{P}_{t+1}\mathsf{T})^{-1}\) and the performance difference Lemma A.3 to get

\[\left\langle(\mathsf{P}_{t+1}-\mathsf{P}_{t})q(\mathsf{P}_{t})+2 \varepsilon_{t},d_{\nu}(\mathsf{P}_{*})\right\rangle \leq\left\langle(\mathsf{Id}-\gamma\mathsf{P}_{t+1}\mathsf{T})^{ -1}\left[(\mathsf{P}_{t+1}-\mathsf{P}_{t})q(\mathsf{P}_{t})+2\varepsilon_{t} \right],d_{\nu}(\mathsf{P}_{*})\right\rangle\] \[=\left\langle\mathsf{P}_{t+1}q(\mathsf{P}_{t+1}),d_{\nu}(\mathsf{ P}_{*})\right\rangle-\left\langle\mathsf{P}_{t}q(\mathsf{P}_{t}),d_{\nu}( \mathsf{P}_{*})\right\rangle+\frac{2\varepsilon_{t}}{1-\gamma}.\]

Substituting this bound in (B.9) and summing from \(t=0\ldots T-1\) one gets to

\[\sum_{t=0}^{T-1}J(\mathsf{P}_{*})-J(\mathsf{P}_{t})\leq \frac{1}{1-\gamma}\left((\mathsf{P}_{T}q(\mathsf{P}_{T}),d_{\nu} (\mathsf{P}_{*}))-\left\langle\mathsf{P}_{0}q(\mathsf{P}_{0}),d_{\nu}( \mathsf{P}_{*})\right\rangle\right)+\frac{2}{(1-\gamma)^{2}}\sum_{t=0}^{T-1} \varepsilon_{t}\] \[\frac{1}{\eta(1-\gamma)}\left\langle D(\pi_{*};\pi_{0})-D(\pi_{*} ;\pi_{T}),d_{\nu}(\mathsf{P}_{*})\right\rangle.\]

Using facts (3) and (4) from Lemma A.5 we have that the terms \(\left\langle\mathsf{P}q(\mathsf{P}),d_{\nu}(\mathsf{P}_{*})\right\rangle\) on the right hand side can be bounded as

\[\left\langle\mathsf{P}q(\mathsf{P}),d_{\nu}(\mathsf{P}_{*})\right\rangle =\left\langle\mathsf{P}(\mathsf{Id}-\gamma\mathsf{TP})^{-1}r,d_ {\nu}(\mathsf{P}_{*})\right\rangle\] \[=\left\langle(\mathsf{Id}-\gamma\mathsf{PT})^{-1}\mathsf{P}r,d_ {\nu}(\mathsf{P}_{*})\right\rangle\] \[=\left\langle r,\mathsf{P}^{*}(\mathsf{Id}-\gamma\mathsf{PT})^{- *}d_{\nu}(\mathsf{P}_{*})\right\rangle\] \[\left(\text{Duality}\right) \leq\left\|r\right\|_{\infty}\left\|\mathsf{P}^{*}(\mathsf{Id}- \gamma\mathsf{PT})^{-*}d_{\nu}(\mathsf{P}_{*})\right\|_{\mathrm{TV}}\] \[\left(\text{Lemma A.5 and }\left\|r\right\|_{\mathrm{TV}}=1\right) \leq\frac{\left\|r\right\|_{\infty}}{1-\gamma},\]

while \(-\left\langle D(\pi_{*};\pi_{T}),d_{\nu}(\mathsf{P}_{*})\right\rangle\) can be dropped due to the positivity of Bregman divergences yielding

\[\sum_{t=0}^{T-1}J(\mathsf{P}_{*})-J(\mathsf{P}_{t})\leq \frac{2}{(1-\gamma)^{2}}\left(\left\|r\right\|_{\infty}+\sum_{t=0 }^{T-1}\varepsilon_{t}\right)+\frac{1}{\eta(1-\gamma)}\left\langle D(\pi_{*} ;\pi_{0}),d_{\nu}(\mathsf{P}_{*})\right\rangle.\] (B.10)

Now notice that for all \(t<T\) it holds

\[J(\mathsf{P}_{t}) =\left\langle\mathsf{P}_{t}q(\mathsf{P}_{t}),\nu\right\rangle\] \[=\left\langle\mathsf{P}_{t}q_{n}(\mathsf{P}_{t}),\nu\right\rangle+ \left\langle\mathsf{P}_{t}(q(\mathsf{P}_{t})-q_{n}(\mathsf{P}_{t})),\nu\right\rangle\] \[\left(\text{Equation (\ref{eq:P_t})}\right) \leq\left\langle\mathsf{P}_{T}q_{n}(\mathsf{P}_{T}),\nu\right\rangle +\left\langle\mathsf{P}_{t}(q(\mathsf{P}_{t})-q_{n}(\mathsf{P}_{t})),\nu\right\rangle\] \[=\left\langle\mathsf{P}_{T}q_{n}(\mathsf{P}_{T}),\nu\right\rangle+ \left\langle\mathsf{P}_{t}(q(\mathsf{P}_{t})-q_{n}(\mathsf{P}_{t})),\nu\right\rangle +\left\langle\mathsf{P}_{T}(q_{n}(\mathsf{P}_{T})-q(\mathsf{P}_{T})),\nu\right\rangle\] \[\leq J(\mathsf{P}_{T})+\varepsilon_{t}+\varepsilon_{T},\]

so that

\[J(\mathsf{P}_{*})-J(\mathsf{P}_{T})\leq\varepsilon_{T}+\frac{1}{T}\sum_{t=0}^{T- 1}J(\mathsf{P}_{*})-J(\mathsf{P}_{t})+\varepsilon_{t}.\]

Combining this with (B.10) we obtain

\[J(\mathsf{P}_{*})-J(\mathsf{P}_{T})\leq\varepsilon_{T}+\frac{1}{T}\left[ \left(1+\frac{2}{(1-\gamma)^{2}}\right)\sum_{t=0}^{T-1}\varepsilon_{t}+\frac{2 \left\|r\right\|_{\infty}}{(1-\gamma)^{2}}+\frac{1}{\eta(1-\gamma)}\left\langle D (\pi_{*};\pi_{0}),d_{\nu}(\mathsf{P}_{*})\right\rangle\right],\]

leading to the desired bound.

POWR Convergence Rates

In this section, we prove the convergence of POWR. To do so, we need to first show that under the choice of spaces \(\mathcal{F}\) and \(\mathcal{G}\) proposed in this work, the resulting PMD iterations are well defined. Then, we need to bound the approximation error of the estimates for the action-value functions of the iterates produced by the inexact PDM algorithm, which appear in the rates of Theorem 7.

### POWR is Well-defined

In order to guarantee that the iterations of POWR generate policies \(\pi_{t}\) for which we can compute an estimator according to the formula in Proposition 3, we need to guarantee that all such policies are \((\mathcal{G},\mathcal{F})\)-compatible. In particular, we restrict to the case of the separable spaces introduced in Proposition 4, for which it turns out that it is sufficient to show that all policies belong to the space \(\mathcal{H}\) characterizing \(\mathcal{F}=\mathcal{H}\otimes\mathcal{H}\) and \(\mathcal{G}=\mathbb{R}^{|\mathcal{A}|}\otimes\mathcal{H}\). The following results provide a candidate for choosing such a space.

**Theorem 5**.: _Let \(\mathcal{X}\subset\mathbb{R}^{d}\) be a compact set and let \(\mathcal{H}=W^{2,s}(\mathcal{X})\) be the Sobolev space of smoothness \(s>0\) (see e.g. [38]). Let \(\pi_{t}(a|\cdot)\) and \(\hat{q}_{\pi_{t}}(\cdot,a)\) belong to \(\mathcal{H}\) for any \(a\in\mathcal{A}\) and \(\pi_{t}(a|x)>0\) for any \(x\in\mathcal{X}\). Then the policy \(\pi_{t+1}\) solution to the PMD update in (14) belongs to \(\mathcal{H}\)._

Proof.: We recall that Sobolev spaces [38] over a compact subset \(\mathcal{X}\) of \(\mathbb{R}^{D}\) are closed with respect to the operations of sum, multiplication, exponentiation or inversion (if the function is supported on the entire domain \(\mathcal{X}\)), namely for any two \(f,f^{\prime}\in\mathcal{H}\), \(f+f^{\prime},ff^{\prime},ef^{\prime}\in\mathcal{H}\) and, if \(f(x)>0\) for all \(x\in\mathcal{X}\), \(1/f\in\mathcal{H}\). This follows by applying the chain rule and the boundedness of derivatives over the compact \(\mathcal{X}\) (see for instance [46, Lemma E.2.2]). The proof follows by observing that the one-step update \(\pi_{t+1}\) in (14) is expressed precisely in terms of these operations and the hypothesis that \(\pi_{t}(a|\cdot)\) and \(\hat{q}_{\pi_{t}}(\cdot,a)\) belong to \(\mathcal{H}\) for any \(a\in\mathcal{A}\). 

Combining the choice of space \(\mathcal{H}\) according to the above result and combining with the PMD iterations of Algorithm 1 we have the following corollary.

**Corollary 6**.: _With the hypothesis of Proposition 4 let \(\mathcal{H}=W^{2,s}(\mathcal{X})\) with \(s>d/2\). Let \(\mathsf{T}_{n}\in\mathsf{HS}(\mathcal{F},\mathcal{G})\) and \(r_{n}\in\mathcal{G}\) characterized as in Proposition 3. Let \(\pi_{0}(a|\cdot)\propto e^{\eta_{0}(\cdot,a)}\) for \(q_{0}\) such that \(q_{0}(\cdot,a)\in\mathcal{H}\) any \(a\in\mathcal{A}\). Then, for any \(t\in\mathbb{N}\) the PMD iterates \(\pi_{t}\) generated by Algorithm 1 are such that \(\pi_{t}(a|\cdot)\in\mathcal{H}\) and hence are \((\mathcal{G},\mathcal{F})\)-compatible._

Proof.: We proceed by induction. Since \(\bar{q}(\cdot,a)\in\mathcal{H}\) we can apply the same reasoning in Theorem 5 to guarantee that \(\pi_{0}(a|\cdot)\in\mathcal{H}\) for any \(a\in\mathcal{A}\). Moreover, \(\pi_{0}(a|\cot)>0\) for any \(a\in\mathcal{A}\) since it is the (normalized) exponential of a function. Hence \(\pi_{0}\) is \((\mathcal{G},\mathcal{F})\)-compatible. Therefore, \(\hat{q}_{0}\) obtained according to Proposition 3 is well defined and belongs to \(\mathcal{G}\), implying \(\hat{q}_{0}(\cdot,a)\in\mathcal{H}\) for any \(a\in\mathcal{A}\). Now, assume by the inductive hypothesis that the policy \(\pi_{t}(a|\cdot)\) generated by POWR at time \(t\) and the corresponding estimator \(\hat{q}_{\pi_{t}}(\cdot,a)\) of the action value function belong to \(\mathcal{H}\) and that \(\pi_{t}(a|x)>0\) for any \((x,a)\in\Omega\). Then, by Theorem 5 we have that also \(\pi_{t+1}\) the solution to the PMD update in (14) belongs to \(\mathcal{H}\) (and is therefore \((\mathcal{G},\mathcal{F})\)-compatible). Additionally, since \(\pi_{t+1}\) can be expressed as the softmax of a (finite) sum of functions in \(\mathcal{H}\), we have also \(\pi_{t+1}(a|x)>0\) for al \((x,a)\in\Omega\), proving the inductive hypothesis and concluding the proof. 

The above corollary guarantees us that if we are able to learn our estimates for the action-value function in \(\mathcal{H}\) a suitably regular Sobolev space, then POWR is well-defined. This is a necessary condition to then being able to study its theoretical behavior in our main result.

### Controlling the Action-value Estimation Error

We now show how to control the estimation error for the action-value function. We start by considering the following application of the (generalized) Simulation lemma in Corollary A.7.

**Lemma 8** (Implications of the Simulation Lemma).: _Let \(\mathsf{T}_{n}\) and \(r_{n}\) the empirical estimators of the transfer operator \(\mathsf{T}\) and reward function \(r\) as defined in Proposition 3, respectively. Ifsatisfies Assumption 1, \(r\in\mathcal{G}\), and \(\gamma\big{\|}\mathsf{T}_{n}\big{\|}<\gamma^{\prime}<1\), then, for every \((\mathcal{G},\mathcal{F})\)-compatible policy \(\pi\)_

\[\big{\|}\hat{q}_{\pi}-q_{\pi}\big{\|}_{\infty}\leq\frac{1}{1-\gamma^{\prime}} \left[\text{const}_{\psi}\big{\|}r_{n}-r\big{\|}_{\mathcal{G}}+\frac{\gamma \big{\|}r\big{\|}_{\infty}}{1-\gamma}\big{\|}\mathsf{T}|_{\mathcal{F}}-\mathsf{ T}_{n}\big{\|}_{\mathsf{HS}}\right].\]

Proof.: Recall that in the notation of these appendices, the action value of a policy and its estimator via the world model CME framework are denoted \(q_{\pi}=q(\mathsf{P})\) and \(\hat{q}_{\pi}=q_{n}(\mathsf{P})\) respectively. We can apply Corollary A.7 to obtain

\[q_{n}(\mathsf{P})-q(\mathsf{P})=(\mathsf{Id}-\gamma\mathsf{T}_{n}\mathsf{P})^{ -1}(r_{n}-r)+\gamma(\mathsf{Id}-\gamma\mathsf{T}_{n}\mathsf{P})^{-1}(\mathsf{T }-\mathsf{T}_{n})v(\mathsf{P}).\]

Then, by Lemma A.5, point 5, we have

\[\big{\|}q_{n}(\mathsf{P})-q(\mathsf{P})\big{\|}_{\infty}\leq\frac{1}{1-\gamma^ {\prime}}\Big{[}\big{\|}r_{n}-r\big{\|}_{\infty}+\gamma\big{\|}(\mathsf{T}- \mathsf{T}_{n})v(\mathsf{P})\big{\|}_{\infty}\Big{]},\]

where \(v(\mathsf{P}):=\mathsf{P}(\mathsf{Id}-\gamma\mathsf{TP})^{-1}r\) is the value function of the MDP, and we used that \(\gamma\big{\|}\mathsf{T}_{n}\big{\|}<\gamma^{\prime}\). Because of Assumption 1, \(r\in\mathcal{G}\), and \(\mathsf{P}\) being \((\mathcal{G},\mathcal{F})\)-compatible, it holds that \(v(\mathsf{P})\in\mathcal{F}\), while Proposition 3 implies \(r_{n}\in\mathcal{G}\), and \((\mathsf{T}-\mathsf{T}_{n})v(\mathsf{P})\in\mathcal{G}\) as well. Therefore, using the reproducing property

\[\big{\|}r_{n}-r\big{\|}_{\infty}=\sup_{(x,a)\in\Omega}|\left\langle\psi(x,a),r _{n}-r\right\rangle_{\mathcal{G}}|\leq\|r_{n}-r\|_{\mathcal{G}}\sup_{(x,a)\in \Omega}\|\psi(x,a)\|_{\mathcal{G}}=C_{\psi}\left\|r_{n}-r\right\|_{\mathcal{G}}\]

where we assumed a bounded kernel \(\left\langle\psi(x,a),\psi(x,a)\right\rangle\leq C_{\psi}\) for all \((x,a)\in\Omega\). Similarly, for the term depending on \(\mathsf{T}_{n}-\mathsf{T}\) we have

\[\big{\|}(\mathsf{T}-\mathsf{T}_{n})v(\mathsf{P})\big{\|}_{\infty} =\sup_{(x,a)\in\Omega}|[(\mathsf{T}|_{\mathcal{F}}-\mathsf{T}_{n} )v(\mathsf{P})](x,a)|\] \[=\sup_{(x,a)\in\Omega}\big{|}\mathsf{T}r\left[(v(\mathsf{P}) \otimes\psi(x,a))(\mathsf{T}|_{\mathcal{F}}-\mathsf{T}_{n})\right]\Big{|}\] \[\leq\big{\|}\mathsf{T}|_{\mathcal{F}}-\mathsf{T}_{n}\big{\|}_{ \mathsf{HS}}\sup_{(x,a)\in\Omega}|v(\mathsf{P})(x,a)|\] \[\leq\frac{\|r\|_{\infty}}{1-\gamma}\big{\|}\mathsf{T}|_{\mathcal{ F}}-\mathsf{T}_{n}\big{\|}_{\mathsf{HS}}.\]

Combining the previous two bounds, we get to

\[\big{\|}q_{n}(\mathsf{P})-q(\mathsf{P})\big{\|}_{\infty}\leq\frac{1}{1-\gamma^ {\prime}}\left[C_{\psi}\big{\|}r_{n}-r\big{\|}_{\mathcal{G}}+\frac{\gamma \big{\|}r\big{\|}_{\infty}}{1-\gamma}\big{\|}\mathsf{T}|_{\mathcal{F}}- \mathsf{T}_{n}\big{\|}_{\mathsf{HS}}\right],\]

as desired. 

According to the result above, we can control the approximation error for the action value function in terms of the approximation errors \(\left\|r_{n}-r\right\|_{\mathcal{G}}\) and \(\big{\|}\mathsf{T}|_{\mathcal{F}}-\mathsf{T}_{n}\big{\|}_{\mathsf{HS}}\). This can be done by leveraging state-of-the-art statistical learning rates for the ridge regression and CME estimators from [40, 21, 47]. The following lemma connects Assumption 2 with the notation used in [40] which enables us to use the required result.

**Lemma C.1** (Relation between (A.8) and [40]'s definition).: _The following two facts are equivalent_

1. \(g\in\mathcal{G}\) _satisfies the strong source condition Assumption_ 2 _with parameter_ \(\beta\) _on the probability distribution_ \(\rho\)_._
2. \(g\in[\mathcal{G}]_{\rho}^{1+2\beta}\) _as in the notation of_ _[_40_]__._

Proof.: Using the same notations as in [40], we have

\[g\in[\mathcal{G}]_{\rho}^{1+2\beta}\iff g=\sum_{i\in\mathbb{N}}a_{i}\mu_{i}^{ \frac{1}{2}+\beta}e_{i}\,\text{ and }\big{\|}(a_{i})_{i\in\mathbb{N}}\big{\|}_{ \ell^{2}}<\infty.\]For \(1\implies 2\) we have that that for \(g\in[\mathcal{G}]_{\rho}^{1+2\beta}\)

\[C_{\rho}^{-\beta}g=\sum_{i\in\mathbb{N}}a_{i}\mu_{i}^{\frac{1}{4}}e_{i}\] (C.1)

whose \(\mathcal{G}\)-norm is \(\left\|C_{\rho}^{-\beta}g\right\|_{\mathcal{G}}=\left\|(a_{i})_{i\in\mathbb{N} }\right\|_{\ell^{2}}<\infty\).

For \(2\implies 1\), let \(g=\sum_{i\in\mathbb{N}}b_{i}\mu_{i}^{\frac{1}{4}}e_{i}\) with \(\left\|(b_{i})_{i\in\mathbb{N}}\right\|_{\ell^{2}}<\infty\) (since). Now, \(\left\|C_{\rho}^{-\beta}g\right\|_{\mathcal{G}}<\infty\) is equivalent to \(\left\|(b_{i}\mu_{i}^{-\beta})_{i\in\mathbb{N}}\right\|_{\ell^{2}}<\infty\). By letting \(b_{i}=\mu_{i}^{\beta}a_{i}\) we have that \(\left\|(a_{i})_{i\in\mathbb{N}}\right\|_{\ell^{2}}<\infty\) and that

\[g=\sum_{i\in\mathbb{N}}a_{i}\mu_{i}^{\frac{1}{4}+\beta}e_{i},\]

that is \(g\in[\mathcal{G}]_{\rho}^{1+2\beta}\). 

With the connection between [40] and Assumption 2 in place we can characterize the bound on the approximation error for the world model-based estimation of the action-value function.

**Proposition C.2**.: _Let \(\mathsf{T}_{n}\) and \(r_{n}\) the empirical estimators of the transfer operator \(\mathsf{T}\) and reward function \(r\) as defined in Proposition 3, respectively. When \(\mathsf{P}\) is a \((\mathcal{G},\mathcal{F})\)-compatible policy as in Definition 1 and the strong source condition Assumption 2 is attained with parameter \(\beta\), it holds_

\[\left\|q_{n}(\mathsf{P})-q(\mathsf{P})\right\|_{\infty}\leq O(\delta^{2}n^{- \alpha}),\] (C.2)

_with rates \(\alpha\in\left(\frac{\beta}{2+2\beta},\frac{\beta}{1+2\beta}\right)\) and probability not less than \(1-4e^{-\delta}\)._

Proof.: We use Lemma C.1 to apply Theorem 3.1 (ii) from [40] to show that under Assumption 2 with parameter \(\beta\) it holds, with probability not less than \(1-4e^{-\delta}\),

\[\left\|r_{n}-r\right\|_{\mathcal{G}}\leq\delta^{2}c_{r}n^{-\alpha_{r}}.\] (C.3)

The rate \(\alpha_{r}\in\left(\frac{\beta}{2+2\beta},\frac{\beta}{1+2\beta}\right)\) is determined by the properties of the inclusion \(\mathcal{G}\hookrightarrow B_{b}(\Omega)\), and the constant \(c_{r}>0\) is independent of \(n\) and \(\delta\). Similarly, point (2.) of [21, Theorem 2] shows that under Assumption 2

\[\left\|\mathsf{T}_{|\mathcal{F}}-\mathsf{T}_{n}\right\|_{\mathsf{HS}( \mathcal{F},\mathcal{G})}\leq\delta^{2}c_{\mathsf{T}}n^{-\alpha_{\mathsf{T}}}\] (C.4)

again with probability not less than \(1-4e^{-\delta}\), rates \(\alpha_{\mathsf{T}}\in\left(\frac{\beta}{2+2\beta},\frac{\beta}{1+2\beta}\right)\) and with \(c_{\mathsf{T}}>0\) independent of \(n\) and \(\delta\). Combining every bound and denoting \(\alpha:=\min(\alpha_{r},\alpha_{\mathsf{T}})\), we conclude

\[\left\|q_{n}(\mathsf{P})-q(\mathsf{P})\right\|_{\infty}\leq\frac{\delta^{2}}{ 1-\gamma^{\prime}}\left[C_{\psi}c_{r}+\frac{\gamma c_{\mathsf{T}}\left\|r \right\|_{\infty}}{1-\gamma}\right]n^{-\alpha}=O(\delta^{2}n^{-\alpha}).\] (C.5)

as required. 

### Convergence Rates for POWR

With a bound on the estimation error of the action-value function by Algorithm 1, we are finally ready to state the complexity bounds for POWR.

**Theorem 9**.: _Let \((\pi_{t})_{t\in\mathbb{N}}\) be a sequence of policies generated by Algorithm 1 in the same setting of Corollary 6. If the action-value functions \(\hat{q}_{\pi_{t}}\) are estimated from a dataset \((x_{i},a_{i};x_{i}^{\prime})_{i=1}^{n}\) with \((x_{i},a_{i})\sim\rho\in\mathcal{P}(\Omega)\) such that Assumption 2 holds with parameter \(\beta\), the iterates of Algorithm 1 converge to the optimal policy as_

\[J(\pi_{*})-J(\pi_{T})\leq O\left(\frac{1}{T}+\delta^{2}n^{-\alpha}\right)\]

_with probability not less than \(1-4e^{-\delta}\). Here, \(\alpha\in\left(\frac{\beta}{2+2\beta},\frac{\beta}{1+2\beta}\right)\) and \(\pi_{*}:\mathcal{X}\to\Delta(\mathcal{A})\) is a measurable maximizer of (8)._

Proof.: Since the setting of Corollary 6 implies that \(\mathsf{P}_{t}\) are \((\mathcal{G},\mathcal{F})\)-compatible for all \(t\), and Assumption 2 is holding, then \(q(\mathsf{P}_{t})\) and \(q_{n}(\mathsf{P}_{t})\) belong to \(\mathcal{G}\) for all \((\mathsf{P}_{t})_{t\in\mathbb{N}}\). This assures that we can use the statistical learning bounds Proposition C.2 into Theorem 7, yielding the final bound.

Experimental details

### Additional Results

In Fig. 2 we show the average timestep at which a reward threshold is met during the training phase. The testing environments are the same as introduced previously, with reward thresholds being the standard ones given in [26], except for the Taxi-v3 environment, where it is marginally lower. Interestingly, in this environment, only DQN and our algorithm are capable of achieving the original threshold within \(1.5\times 10^{6}\) timesteps during the training. On the other hand, the new lower threshold is also reached by the PPO algorithm.

Our approach attain the desired reward quicker than the competing algorithms. Furthermore, the timestep at which POWRreaches the threshold exhibits a lower variance compared to other techniques. This implies that our approach requires a stable amount of timesteps to learn how to solve a specific environment.

### Other methods

We compare the performance of our algorithm with several baselines. In particular, we considered A2C [41], DQN [4], TRPO [7] and PPO [6], which we implemented using the stable baselines library [48]. We used the standard hyperparameters in [48].

Figure 2: Mean timestep at which various algorithms attain a specified reward threshold during their training. The reward targets are set at \(0.8\) for FrozenLake-v1, 6 for Taxi-v3, and \(-110\) for MountainCar-v0. The absence of a box indicates that the corresponding algorithm was unable to meet the reward threshold within the training process.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: we provide theoretical and experimental proofs of our claims. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: see the conclusion section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We give all the proofs in the Appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In the paper, we provided the algorithm description and analysis, including the hyperparameters of the baselines and our proposed algorithm. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We provided the link to the code and all the hyperparameters we tested. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provided a description of the open-source environments, in which we tested our POWR and the baselines. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We showed seven independent runs for each experiment we reported to prove the effectiveness of our method. Moreover, in the plot, we can observe the bands based on the minimum and maximum values reached by the different runs. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: We provided the code. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We respected the ethics guidelines for the development of this work. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We believe that this work doesn't have any societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This work doesn't have such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We implemented our approach from scratch. We mentioned the baselines and the environments for testing involved in this work, that are open-source Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. ** If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provided the code of our algorithm. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]. Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.