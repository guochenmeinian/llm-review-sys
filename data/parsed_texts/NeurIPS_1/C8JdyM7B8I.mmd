# Towards Label-free Scene Understanding by Vision Foundation Models

Runnan Chen\({}^{1}\) Youquan Liu\({}^{2}\) Lingdong Kong\({}^{3}\) Nenglun Chen\({}^{1}\) Xinge Zhu\({}^{4}\)

Yuexin Ma\({}^{5}\) Tongliang Liu\({}^{6}\) Wenping Wang\({}^{7*}\)

\({}^{1}\)The University of Hong Kong \({}^{2}\)Hochschule Bremerhaven

\({}^{3}\)National University of Singapore \({}^{4}\)The Chinese University of Hong Kong

\({}^{5}\)ShanghaiTech University \({}^{6}\)The University of Sydney \({}^{7}\)Texas A&M University

Corresponding authors.

###### Abstract

Vision foundation models such as Contrastive Vision-Language Pre-training (CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot performance on image classification and segmentation tasks. However, the incorporation of CLIP and SAM for label-free scene understanding has yet to be explored. In this paper, we investigate the potential of vision foundation models in enabling networks to comprehend 2D and 3D worlds without labelled data. The primary challenge lies in effectively supervising networks under extremely noisy pseudo labels, which are generated by CLIP and further exacerbated during the propagation from the 2D to the 3D domain. To tackle these challenges, we propose a novel Cross-modality Noisy Supervision (CNS) method that leverages the strengths of CLIP and SAM to supervise 2D and 3D networks simultaneously. In particular, we introduce a prediction consistency regularization to co-train 2D and 3D networks, then further impose the networks' latent space consistency using the SAM's robust feature representation. Experiments conducted on diverse indoor and outdoor datasets demonstrate the superior performance of our method in understanding 2D and 3D open environments. Our 2D and 3D network achieves label-free semantic segmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%, respectively. For nuImages and nuScenes datasets, the performance is 22.1% and 26.8% with improvements of 3.5% and 6.0%, respectively. Code is available.2

Footnote 2: [https://github.com/runnanchen/Label-Free-Scene-Understanding](https://github.com/runnanchen/Label-Free-Scene-Understanding).

## 1 Introduction

Scene understanding aims to recognize the semantic information of objects within their contextual environment, which is a fundamental task for autonomous driving, robot navigation, digital city, etc. Existing methods have achieved remarkable advancements in 2D and 3D scene understanding [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. However, they heavily rely on extensive annotation efforts and often struggle to identify novel object categories that were not present in the training data. These limitations hinder their practical applicability in real-world scenarios where acquiring high-quality labelled data can be expensive and novel objects may appear [11, 12, 13, 14, 15, 16, 17]. Consequently, label-free scene understanding, which aims to perform semantic segmentation in real-world environments without requiring labelled data, emerges as a highly valuable yet relatively unexplored research topic.

Vision foundation models, _e.g._, Contrastive Vision-Language Pre-training (CLIP) [18] and Segment Anything (SAM) [19], have garnered significant attention due to their remarkable performance in addressing open-world vision tasks. CLIP, trained on a large-scale collection of freely available image-text pairs from websites, exhibits promising capabilities in open-vocabulary image classification. Onthe other hand, SAM learns from an extensive dataset comprising 1 billion masks across 11 million images, achieving impressive zero-shot image segmentation performance. However, initially designed for image classification, CLIP falls short in segmentation performance. Conversely, SAM excels in zero-shot image segmentation but lacks object semantics (Fig. 1). Additionally, both models are trained exclusively on 2D images without exposure to any 3D modal data. Given these considerations, a natural question arises: Can the combination of CLIP and SAM imbue both 2D and 3D networks with the ability to achieve label-free scene understanding in real-world open environments?

Despite recent efforts [20] leverage CLIP for image-based semantic segmentation, the pseudo labels generated by CLIP for individual pixels often exhibit significant noise, resulting in unsatisfactory performance. The up-to-date work [17] has extended this method to encompass 3D label-free scene understanding by transferring 2D knowledge to 3D space via projection. However, the pseudo labels assigned to 3D points are considerably noisier due to calibration errors, significantly limiting the accuracy of the networks. The primary challenge of utilizing vision foundation models for scene understanding is effectively supervising the networks using exceptionally noisy pseudo labels.

Inspired by SAM's impressive zero-shot segmentation capabilities, we propose a novel Cross-modality Noisy Supervision (CNS) framework incorporating CLIP and SAM to train 2D and 3D networks simultaneously. Specifically, we employ CLIP to densely pseudo-label 2D image pixels and transfer these labels to 3D points using the calibration matrix. Since pseudo-labels are extremely noisy, leading to unsatisfactory performance, we refine pseudo-labels using SAM's masks, producing more reliable pseudo-labels for supervision. To further mitigate error propagation and prevent the networks from overfitting the noisy labels, we consistently regularize the network predictions, _i.e._, co-training the 2D and 3D networks using the randomly switched pseudo labels, where the labels are from the prediction of 2D, 3D, and CLIP networks. Moreover, considering that individual objects are well distinguished in SAM feature space, we use the robust SAM feature to consistently regularize the latent space of 2D and 3D networks, which aids networks in producing semantic predictions with more precise boundaries and further reduces label noise. Note that the SAM feature space is frozen during training, thus severed as the anchor metric space for aligning the 2D and 3D features.

To verify the label-free scene understanding capability of our method in 2D and 3D real open worlds, we conduct experiments on indoor and outdoor datasets, _i.e._, ScanNet and nuScenes, where 2D and 3D data are simultaneously collected. Extensive results show that our method significantly outperforms state-of-the-art methods in understanding 2D and 3D scenes without training on any labelled data. Our 2D and 3D network achieves label-free semantic segmentation with 28.4% and 33.5% mIoU on ScanNet, improving 4.7% and 7.9%, respectively. For nuImages and nuScenes datasets, the performance is 22.1% and 26.8% with improvements of 3.5% and 6.0%, respectively. Quantitative and qualitative ablation studies also demonstrate the effectiveness of each module.

The key contributions of our work are summarized as follows.

* In this work, we study how vision foundation models enable networks to comprehend 2D and 3D environments without relying on labelled data.

Figure 1: We study how vision foundation models enable networks to comprehend 2D and 3D environments without relying on labelled data. To accomplish this, we introduce a novel framework called Cross-modality Noisy Supervision (CNS). By effectively harnessing the strengths of CLIP and SAM, our approach simultaneously trains 2D and 3D networks, yielding remarkable performance.

* We propose a novel Cross-modality Noisy Supervision framework to effectively and synchronously train 2D and 3D networks with severe label noise, including prediction consistency and latent space consistency regularization schemes.
* Experiments conducted on indoor and outdoor datasets show that our method significantly outperforms state-of-the-art methods on 2D and 3D semantic segmentation tasks.

## 2 Related Work

**Scene Understanding.** Scene understanding is a fundamental task for computer vision and plays a critical role for robotics, autonomous driving, smart city, etc. Significant advancements have been made by current supervised methods for 2D and 3D scene understanding [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34]. However, these methods heavily depend on extensive annotation efforts, which pose significant challenges when encountering novel object categories that were not included in the training data. In order to overcome these limitations, researchers have proposed self-supervised and semi-supervised methods [13; 35; 36; 37; 38; 39; 40; 41; 42; 43; 44; 45; 46; 47; 48; 49; 50; 51; 52; 53; 54] to train networks in a more data-efficient manner. Nevertheless, these approaches struggle to handle novel objects that are not present in training data and they often exhibit subpar performance when faced with significant domain gaps. Alternatively, some methods [11; 12; 55; 56; 57; 58; 59; 60; 61; 62; 63; 64] focus on open-world scene understanding that identifies novel object categories absents in the training data. However, these methods still require expensive annotation efforts on the close-set category data. Other methods [56; 65; 66; 67; 68; 67] try to alleviate 3D annotation efforts by distilling knowledge from 2D networks. However, they either need 2D labelled data to train networks or struggle to achieve satisfactory results in the context of label-free 3D scene understanding.

Recently, vision foundation models such as Contrastive Vision-Language Pre-training (CLIP) [18] and Segment Anything (SAM) [19] have garnered significant attention due to their remarkable performance in open-world vision tasks. MaskCLIP [20] extends the capabilities of CLIP to image semantic segmentation, achieving promising results in 2D scene understanding without labelled data. Furthermore, CLIP2Scene [17] expands upon MaskCLIP to enable 3D scene understanding through the use of a 2D-3D calibration matrix. In this study, our focus lies in label-free understanding using vision foundation models. Our ultimate goal is to perform semantic segmentation in real-world environments without relying on labelled data. We hope that our work will inspire and motivate further research in this area.

**Label-noise Representation Learning.** Label-Noise Representation Learning (LNRL) aims to train neural networks robustly in the presence of noisy labels. Numerous methods have been developed to tackle this challenge. Some methods [68; 69; 70; 71; 72; 73; 74] focus on designing accurate estimators of the noise transition matrix, which establish a connection between the clean class posterior and the noisy class posterior. Others [75; 76; 77; 78; 79; 80; 81] devise noise-tolerant objective functions through explicit or implicit regularization, enabling the learning of robust classifiers on noisy data. Furthermore, Some approaches [82; 83; 84; 85; 86] delve into the dynamic optimization process of LNRL based on the memorization effects, _i.e._, deep models tend to initially fit easy (clean) patterns and gradually overfit complex (noisy) patterns. However, the above methods are insufficient when clean labels are unavailable, which makes them inadequate for addressing our problem with entirely noisy labels. Furthermore, these methods are primarily designed for a single modality, and the exploration of multiple modality scenarios is still an unexplored topic. In this paper, we study the LNRL across different modalities without any clean labels.

## 3 Methodology

The vision foundation models have demonstrated remarkable performance in open-world scenarios. However, while CLIP excels in zero-shot image classification, it tends to produce noisy masks in segmentation tasks. Conversely, SAM generates impressive object masks in zero-shot segmentation but lacks object semantics. Therefore, our research aims to mitigate the impact of noisy labels by leveraging the strengths of various modalities, such as images, point clouds, CLIP, and SAM. In what follows, we revisit the vision foundation models applied in open-vocabulary classification and semantic segmentation, then present the Cross-modality Noisy Supervision (CNS) framework in detail.

### Revisiting Vision Foundation Models

Vision foundation model refers to powerful models pre-trained on large-scale multi-modal data and can be applied to a wide range of downstream tasks [87]. CLIP and SAM are well recognized due to their impressive open-world image classification and segmentation performance.

CLIP combines an image encoder (ResNet [88] or ViT [89]) and a text encoder (Transformer [90]), which independently map image and text representations to a shared embedding space. CLIP trains both encoders using a contrastive loss on a dataset of 400 million image-text pairs sourced from the Internet that contain diverse image classes and textual concepts. This training approach enables CLIP to achieve impressive performance in open-vocabulary recognition. CLIP utilizes a two-step process for zero-shot image classification. Initially, it obtains image embeddings and generates text embeddings by incorporating the class name into a predefined template. Then, it compares the similarity of image and text embeddings to determine the class. SAM serves as the fundamental model for image segmentation. It undergoes training using over 1 billion masks on a dataset of 11 million licensed images, enabling it to adapt seamlessly to new image distributions and tasks without prior exposure. Extensive evaluations have demonstrated that SAM's zero-shot performance is remarkable, frequently rivaling or surpassing the results of previous fully supervised approaches. Note that SAM works impressively on zero-shot image segmentation but lacks object semantics.

MaskCLIP [20] extends the CLIP's capabilities to image semantic segmentation. Specifically, MaskCLIP modifies the attention pooling layer of CLIP's image encoder to enable pixel-level mask prediction instead of global image-level prediction. CLIP2Scene [17] further expands MaskCLIP into 3D scene understanding by transferring the pixel-level pseudo-labels from the 2D image to the 3D point cloud using a 2D-3D calibration matrix. However, their performance is unsatisfactory due to the significant noise in the pseudo labels. To this end, we leverage the strengths of vision foundation models to supervise networks under extremely noisy pseudo labels.

### Cross-modality Noisy Supervision

As illustrated in Fig. 2, we introduce a novel Cross-modality Noisy Supervision (CNS) method that combines CLIP and SAM foundation models to concurrently train 2D and 3D networks for label-free scene understanding. Our approach involves pseudo-labelling the 2D images and 3D points using CLIP. To address the noise in pseudo-labelling, we employ SAM's masks to refine the pseudo-labels. To further mitigate error propagation and prevent overfitting to noisy labels, we

Figure 2: Illustration of the overall framework. Specifically, we employ CLIP to densely pseudo-label 2D image pixels and transfer these labels to 3D points using the calibration matrix. Acknowledging that network predictions and pseudo-labels are extremely noisy, we refine pseudo-labels using SAM’s masks for more reliable supervision. To mitigate error propagation and prevent the networks from overfitting the noisy labels, we randomly switch the different refined pseudo-labels to co-training the 2D and 3D networks. Besides, we consistently regularize the network’s latent space using SAM’s robust feature, which benefits the networks to produce clearer predictions.

propose a prediction consistency regularization mechanism. This mechanism randomly switches refined pseudo-labels from different modalities to co-train the 2D and 3D networks. Additionally, we consistently regularize the network's latent space using SAM's robust feature, which aids in generating more accurate predictions. In the following sections, we provide a comprehensive overview of our method, along with detailed explanations and insights.

**Pixel-point Projection.** Since vision foundation models are pre-trained on 2D images, we transfer knowledge from 2D images to 3D point clouds via pixel-point correspondences. Note that the dataset already provides the camera poses, so we obtain the pixel-point correspondence \(\{c_{i},s_{i},x_{i},p_{i}\}_{i=1}^{N}\) via simple projection, where \(c_{i}\), \(s_{i}\) and \(x_{i}\) indicates the \(i\)-th paired image pixel feature by CLIP, SAM, and 2D network, respectively. \(p_{i}\) is the \(i\)-th paired point feature by the 3D network \(\mathcal{F}_{3D}\) and \(N\) is the number of pairs.

**Pseudo-labeling by CLIP.** We follow the pioneer methods, MaskCLIP and CLIP2Scene, that adopt CLIP to dense pseudo-label the 2D image pixels and transfer these labels to 3D points. Specifically, given the calibrated pixel-point features \(\{c_{i},s_{i},x_{i},p_{i}\}_{i=1}^{N}\), we pseudo-label the pixel \(c_{i}\) as follows:

\[P_{c_{i}}=\operatorname*{arg\,max}_{l}\psi(c_{i}^{T},t_{l}), \tag{1}\]

where \(c_{i}\) is the \(i\)-th pixel feature by CLIP's image encoder. \(t_{l}\) is the CLIP's text embedding of the \(l\)-th class name into a pre-defined template. \(\psi\) is the dot product operation.

Since pixel-point features \(\{c_{i},s_{i},x_{i},p_{i}\}_{i=1}^{N}\) are spatial aligned, we can transfer the pseudo-label \(\{P_{c_{i}}\}_{i=1}^{N}\) to all image pixels and point coults, _i.e._,

\[P_{s_{i}}=P_{x_{i}}=P_{p_{i}}=P_{c_{i}}, \tag{2}\]

where \(\{P_{s_{i}}\}_{i=1}^{N}\), \(\{P_{x_{i}}\}_{i=1}^{N}\) and \(\{P_{p_{i}}\}_{i=1}^{N}\) are the transferred pseudo-labels from CLIP to SAM, 2D and 3D networks, respectively.

**Label Refinement by SAM.** Since CLIP is pre-trained on 2D images and text for zero-shot classification, it brings extreme noise to pixel-wise prediction. Nevertheless, another vision foundation model, Segment Anything (SAM), can provide impressive object masks in open-world environments, even rivaling or surpassing the previous fully supervised approaches. To this end, we consider refining the CLIP's pseudo-labels by SAM. Firstly, we follow the official implementation of SAM to generate all masks from the images. The mask ID of pixels are presented as \(\{M_{s_{i}}\}_{i=1}^{N}\), where \(M_{s_{i}}\in\{1,2,...,T\}\) and \(T\) indicates the number of masks. Once again, we could transfer the masks to all image pixels and point clouds based on the pixel-point correspondence \(\{c_{i},s_{i},x_{i},p_{i}\}_{i=1}^{N}\), _i.e._,

\[M_{c_{i}}=M_{x_{i}}=M_{p_{i}}=M_{s_{i}}, \tag{3}\]

where \(\{M_{c_{i}}\}_{i=1}^{N}\), \(\{M_{x_{i}}\}_{i=1}^{N}\) and \(\{M_{p_{i}}\}_{i=1}^{N}\) are the transferred masks from SAM to CLIP, 2D and 3D predictions, respectively.

We then adjust the pseudo-labels to be identical if they are within the same mask. For simplicity, we take the max voting strategy, _i.e._,

\[P_{c_{i}}^{*}=\operatorname*{arg\,max}_{l}\sum_{c_{j},M_{c_{i}}=M_{c_{j}}} \mathbbm{1}\{P_{c_{j}}=l\}, \tag{4}\]

where \(\mathbbm{1}\{\cdot\}\) is the indicator function. \(P_{c_{i}}^{*}\) is the SAM refined pseudo-label from \(P_{c_{i}}\). To this end, all pixels and point pseudo-labels within the same mask will be corrected by the semantic label most pixels belonging to, _i.e._, \(\{P_{c_{i}}^{*}\}_{i=1}^{N}\), \(\{P_{x_{i}}^{*}\}_{i=1}^{N}\) and \(\{P_{p_{i}}^{*}\}_{i=1}^{N}\).

**Prediction Consistency Regularization.** After obtaining the refined pseudo labels of all image pixels and points \(\{P_{c_{i}}^{*}\}_{i=1}^{N}\), \(\{P_{x_{i}}^{*}\}_{i=1}^{N}\) and \(\{P_{p_{i}}^{*}\}_{i=1}^{N}\), we consider how to train both 2D and 3D networks. The training process consists of two stages. In the first stage, we simultaneously supervise the 2D and 3D networks by \(\{P_{x_{i}}^{*}\}_{i=1}^{N}\) and \(\{P_{p_{i}}^{*}\}_{i=1}^{N}\) with Cross-entropy loss:

\[\mathcal{L}_{s1}=\sum_{i}CE(\theta_{s}(x_{i}),P_{x_{i}}^{*})+\sum_{i}CE(\varphi _{s}(p_{i}),P_{p_{i}}^{*}),CE(x,y)=-\log\frac{\exp(\psi(x,t_{y}))}{\sum_{l} \exp(\psi(x,t_{l}))}, \tag{5}\]

where \(\theta_{s}\) and \(\varphi_{s}\) are linear mapping layers of 2D \(\{x_{i}\}_{i=1}^{N}\) and 3D \(\{p_{i}\}_{i=1}^{N}\) features, respectively. \(t_{l}\) is the CLIP's text embedding of the \(l\)-th class name. Note that CLIP and 2D network inputs are the same images with different data augmentation. Thus we can utilize all CLIP's pseudo labels to train the 2D network.

In the second stage, we additionally include the self-training and cross-training process to reduce the error propagation flow of noisy prediction. Specifically, we first obtain the self-predicted pseudo-labels of 2D network \(\hat{P}^{*}_{x_{i}}\) and 3D network \(\hat{P}^{*}_{p_{i}}\), respectively. Note that \(\hat{P}_{x_{i}}=\operatorname*{arg\,max}_{l}\psi(\theta_{s}(x_{i}),t_{l})\), \(\hat{P}_{p_{i}}=\operatorname*{arg\,max}_{l}\psi(\varphi_{s}(p_{i}),t_{l})\) and then they are refined to be \(\hat{P}^{*}_{x_{i}}\) and \(\hat{P}^{*}_{p_{i}}\) by SAM (Formula 4). Next, we randomly feed the 2D, 3D, and refined CLIP pseudo-labels to train the 2D and 3D networks. The objective function is as follows:

\[\mathcal{L}_{s1}=\sum_{i}CE(\theta_{s}(x_{i}),t_{R^{*}_{i}})+\sum_{i}CE(\varphi _{s}(p_{i}),t_{R^{*}_{i}}), \tag{6}\]

where the pseudo-label \(R^{*}_{i}\in\{P^{*}_{x_{i}},P^{*}_{p_{i}},\hat{P}^{*}_{x_{i}},\hat{P}^{*}_{p_{ i}}\}\) is randomly assigned to 2D and 3D networks with equal possibility during training.

Note that two training stages are seamless, _i.e_., we train 2D and 3D networks in the first stage with a few epochs to warm up the network and then seamlessly switch to the second training stage. In this way, we reduce the error propagation flow of noisy prediction and prevent the networks from overfitting the noisy labels.

Latent Space Consistency RegularizationInspired by the remarkable performance of zero-shot segmentation achieved by SAM, where objects are usually distinguished with clear boundaries, we consider how SAM endows the 2D and 3D networks with the robust segment anything capability to further resist the prediction noise. The intuitive solution is to transfer the knowledge from the SAM feature space to the image and point cloud feature spaces learned by target 2D and 3D networks.

Previous methods [17; 14; 13] employ InfoNCE loss for cross-modal knowledge transfer. These methods initially establish positive pixel-point pairs and negative pairs. They subsequently employ the InfoNCE loss to encourage the positive pairs to be closer and push away the negative pairs in the embedding space. However, Because precise object semantics are missing, these methods face a common optimization-conflict challenge. For instance, if two pixels have identical semantics but are wrongly assigned to different clusters, the InfoNCE loss tends to separate them, which is unreasonable and adversely affects downstream task performance [17; 14]. To this end, we choose to directly pull in the point feature with its corresponding pixel in the SAM feature space. The objective function is:

\[\mathcal{L}_{f}=\sum_{i=1}^{N}(1-COS(\theta_{f}(x_{i}),\frac{\phi_{f}(s_{i})} {\|\phi_{f}(s_{i})\|_{2}}))+(1-COS(\varphi_{f}(p_{i}),\frac{\phi_{f}(s_{i})}{ \|\phi_{f}(s_{i})\|_{2}})), \tag{7}\]

where \(\theta_{f}(\cdot)\), \(\varphi_{f}(\cdot)\), and \(\phi_{f}(\cdot)\) indicate the linear mapping layers of 2D \(\{x_{i}\}_{i=1}^{N}\), 3D \(\{p_{i}\}_{i=1}^{N}\) and SAM feature \(\{s_{i}\}_{i=1}^{N}\), with the same output feature dimensions \(K_{f}\). \(COS(\cdot)\) is the cosine similarity. Note that the SAM feature is frozen during training.

Essentially, SAM has been trained on millions of images using large-scale masks, resulting in a highly robust feature representation contributing to its exceptional performance in zero-shot image segmentation. This robust feature representation can be used as a foundation for aligning the 2D and 3D features so that 2D and 3D paired features can be constrained into a single, unified SAM feature space to alleviate the problem of noisy predictions.

## 4 Experiments

To evaluate the superior performance and generalization capability of our method for scene understanding, we conduct experiments on both indoor and outdoor public datasets, namely ScanNet [91], nuScenes [92] and nuImages [93], and compare with current state-of-the-art approaches [20; 56; 17] on both 2D and 3D semantic segmentation tasks. Critical operations and losses of our method are also verified effectiveness by extensive ablation studies. In the following, we introduce datasets, implementation, comparison results, and ablation studies in detail.

Datasets.ScanNet [91] consists of 1,603 indoor scans, collected by RGB-D camera, with 20 classes, where 1,201 scans are allocated for training, 312 scans for validation, and 100 scans for testing. Additionally, we utilize 25,000 key frame images to train the 2D network. The nuScenes [92] dataset,collected in traffic scenarios by LiDAR and RGB camera, comprises 700 scenes for training, 150 for validation, and 150 for testing, focusing on LiDAR semantic segmentation with 16 classes. To be more specific, we leverage a total of 24,109 sweeps of LiDAR scans for training and 4,021 sweeps for validation. Each sweep is accompanied by six camera images, providing a comprehensive 360-degree view. Note that the dataset does not provide image labels. Thus the quantitative evaluation of the paired images is not presented. The nuImages [93] dataset provides 93,000 2D annotated images sourced from a significantly larger dataset. This includes 67,279 images for training, 16,445 for validation, and 9,752 for testing. The dataset consists of 11 shared classes with nuScenes.

Implementation Details.We utilize MinkowskiNet34 [95] as the 3D backbone and DeeplabV3 [96] as the 2D backbone in our approach, where DeeplabV3 model is pre-trained on the ImageNet dataset. Based on the MaskCLIP framework, we modify the attention pooling layer of CLIP's image encoder to extract dense pixel-text correspondences. Our framework is developed using PyTorch and trained on two NVIDIA Tesla A100 GPUs. During training, both CLIP and SAM are kept frozen. For prediction consistency regularization, we transition to stage two after ten epochs of stage one. To enhance the robustness of our model, we apply various data augmentations, such as random rotation along the z-axis and random flip for point clouds, as well as random horizontal flip, random crop, and random resize for images. For the ScanNet dataset, the training process takes approximately 10 hours for 30 epochs, with the image number set to 16. In the case of the nuScenes dataset, the training time is 40 hours for 20 epochs, with the image number set to 6.

\begin{table}
\begin{tabular}{c|c|c c|c c} \hline \hline \multirow{2}{*}{Methods} & \multirow{2}{*}{Publication} & \multicolumn{2}{c|}{ScanNet} & \multicolumn{2}{c}{nulImages} & nuScenes \\  & & \(2D\) & \(3D\) & \(2D\) & \(3D\) \\ \hline MaskCLIP [20] & ECCV 2022 & \(17.3\) & \(14.2\) & \(14.1\) & \(12.8\) \\ MaskCLIP+ [20] & ECCV 2022 & \(20.3\) & \(21.6\) & \(17.3\) & \(15.3\) \\ OpenScene [56] & CVPR 2023 & \(14.2\) & \(16.8\) & \(12.4\) & \(14.6\) \\ CLIP2Scene [17] & CVPR 2023 & \(23.7\) & \(25.6\) & \(18.6\) & \(20.8\) \\ \hline Ours & – & \(\mathbf{28.4}\) & \(\mathbf{33.5}\) & \(\mathbf{22.1}\) & \(\mathbf{26.8}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison (mIoU) with current state-of-the-art label-free methods for semantic segmentation tasks on the ScanNet [91], nuImages [93] and nuScenes [94] dataset.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Ablation Target & Setting & 2D mIoU (\%) & 3D mIoU (\%) \\ \hline \multirow{4}{*}{Cross-modality Noisy Supervision} & Baseline & \(17.3\) & \(14.2\) \\  & w/o CNS & \(24.5\)\({}_{(+7.20)}\) & \(19.4\)\({}_{(+5.20)}\) \\  & Base+CLIP & \(20.4\)\({}_{(+3.10)}\) & \(23.8\)\({}_{(+9.60)}\) \\  & Base+CLIP+LSC & \(22.5\)\({}_{(+5.20)}\) & \(28.7\)\({}_{(+14.5)}\) \\ \hline \multirow{2}{*}{Label Refinement} & w/o Refine & \(23.7\)\({}_{(+6.40)}\) & \(30.6\)\({}_{(+16.4)}\) \\  & Post-Refine & \(28.1\)\({}_{(+10.8)}\) & \(33.2\)\({}_{(+19.0)}\) \\ \hline \multirow{4}{*}{Prediction Consistency Regularization} & w/o CT & \(27.7\)\({}_{(+10.1)}\) & \(31.0\)\({}_{(+16.8)}\) \\  & w/o SCT & \(24.7\)\({}_{(+7.40)}\) & \(30.6\)\({}_{(+16.4)}\) \\ \cline{1-1}  & w/o CLIP & \(4.7\)\({}_{(-12.6)}\) & \(5.1\)\({}_{(-9.1)}\) \\ \cline{1-1}  & w 2D & \(6.9\)\({}_{(-10.4)}\) & \(8.8\)\({}_{(-5.4)}\) \\ \cline{1-1}  & w 3D & \(4.8\)\({}_{(-12.5)}\) & \(6.4\)\({}_{(-7.8)}\) \\ \hline \multirow{2}{*}{Latent Space Consistency Regularization} & CLIP & \(27.3\)\({}_{(+10.0)}\) & \(32.4\)\({}_{(+18.2)}\) \\  & SAM+CLIP & \(27.8\)\({}_{(+10.5)}\) & \(32.5\)\({}_{(+18.3)}\) \\ \cline{1-1}  & None & \(27.1\)\({}_{(+9.80)}\) & \(32.0\)\({}_{(+17.8)}\) \\ \hline \multirow{2}{*}{Image Numbers} & 8 & \(27.9\)\({}_{(+10.6)}\) & \(33.0\)\({}_{(+18.8)}\) \\  & 16 & \(28.4\)\({}_{(+11.1)}\) & \(33.5\)\({}_{(+19.3)}\) \\ \cline{1-1}  & 32 & \(28.3\)\({}_{(+11.0)}\) & \(33.3\)\({}_{(+19.1)}\) \\ \hline Full Configuration & Ours & \(\mathbf{28.4}\)\({}_{(+11.1)}\) & \(\mathbf{33.5}\)\({}_{(+19.3)}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ablation study on the ScanNet [94]_val_ set for label-free 2D and 3D semantic segmentation.

### Comparison Results

Our method was compared with several state-of-the-art methods, namely MaskCLIP, MaskCLIP+, OpenScene, and CLIP2Scene, on the ScanNet and nuScenes datasets. Note that their codes are publicly available. For a fair comparison, we share all experiment settings with the above methods, including data augmentations, training epochs, learning rate, and network backbones. The results, as depicted in Table 1, demonstrate that our method surpasses the performance of other approaches by a significant margin. In terms of label-free semantic segmentation, our 2D and 3D networks achieve remarkable results, achieving 28.4% and 33.5% mIoU on the ScanNet dataset, corresponding to improvements of 4.7% and 7.9%, respectively. Furthermore, on the nuScenes dataset, our method achieves a mIoU of 26.8% for 3D semantic segmentation, exhibiting a 6% improvement. Note that the nuScenes dataset does not provide image labels. Thus the quantitative evaluation of 2D semantic segmentation is not presented.

### Ablation Study

We conduct a series of ablation experiments to evaluate and demonstrate the effectiveness of different modules within our framework, as shown in Table. 2. In this section, we present the ablation configuration and provide an in-depth analysis of our experimental results.

Effect of Cross-modality Noisy Supervision.In 2D scene understanding, the **Baseline** is CLIP. For 3D scene understanding, we directly project the 2D prediction to the 3D point cloud. We also conducted **w/o CNS** that the Cross-modality Noisy Supervision was not applied, _i.e._, build upon Baseline, the 2D label is refined by SAM, and projected to 3D points. Besides, we impose the baseline method self-training with the CLIP pseudo-labels, termed **base+CLIP**. Building upon **base+CLIP**, we also impose Latent Space Consistency Regularization, termed **base+CLIP+ LSCR**. Experiments

Figure 3: Qualitative results of label-free semantic segmentation on the ScanNet 25k images dataset. From the left to the right column are the input, prediction by SAM, CLIP, ours w/o CNS, our full method, and ground truth, respectively.

show that SAM refinement promotes pseudo-label quality. Moreover, our Cross-modality Noisy Supervision can effectively supervise networks under extremely noisy pseudo labels. We also evaluate CNS qualitatively using the ScanNet dataset, as illustrated in Fig. 3 and 4. Our comprehensive approach demonstrates remarkable performance in label-free scene understanding, encompassing both 2D and 3D scenarios. We observed that our results are comparable to human annotations in numerous instances. Interestingly, the 2D and 3D networks exhibit the remarkable ability to "segment anything," which can be attributed to the knowledge transferred from SAM.

**Effect of Label Refinement.** To verify how the label refinement affects the performance, we do not use SAM to refine the pseudo-labels during training (**w/o Refine**); we perform prediction refinement in the final result (**Post-Refine**). Notably, we observed a significant drop in accuracy when SAM refinement was omitted during training. This suggests that SAM is crucial in enhancing the model's performance. However, we also noticed that post-refinement did not contribute to the performance. This can be attributed to the fact that the trained networks already produced semantic-consistent regions similar to those obtained with SAM refinement.

**Effect of Latent Space Consistency Regularization.** We utilize two main features to evaluate the latent space consistency regularization: the **CLIP** feature and the **CLIP+SAM** feature. In addition, we have experimented without any Latent Space Regularization (**None**). Results demonstrate that our full method incorporating the SAM feature performs best. On the other hand, we have observed that the CLIP feature alone is less effective in our task, primarily because the feature space it provides is not specifically designed for segmentation purposes.

**Effect of Prediction Consistency Regularization.** We conducted five configurations to examine the effects of prediction consistency regularization: 1) **w/o CT**: In this experiment, the 2D and 3D networks were not allowed to cross-train each other in the second training stage, _i.e_., the 2D network's labels were randomly switched between CLIP and 2D prediction. Meanwhile, the 3D network's labels were randomly switched between CLIP and 3D prediction; 2) **w/o SCT**: This experiment eliminates self- and cross-training in 2D and 3D networks, _i.e_., 2D and 3D network labels are derived solely from CLIP; 3) **w/o CLIP**: This experiment excludes the usage of CLIP's labels during the second training stage, _i.e_., both the 2D and 3D network's labels were randomly switched between 2D and 3D prediction; And 4) **w 2D**, 5) **w 3D**: we employ either 2D or 3D network prediction to

Figure 4: Qualitative results of label-free point cloud-based 3D semantic segmentation on the ScanNet dataset. From the left to the right are the input, prediction by CLIP, ours w/o CNS, our full method, and ground truth, respectively.

synchrony train 2D and 3D networks, respectively. Experiments show that SAM refinement promotes pseudo-label quality. Besides, The experimental results indicate that cross-training, self-training, and label-switching effectively improve performance. Importantly, we observed that the method fails when CLIP's supervision is absent during the second training stage. This failure can be attributed to the tendency of the 2D and 3D networks to "co-corrupt" each other with extremely noisy labels.

Effect of Images Numbers.We also show the performance when restricting image numbers to 8, 16, and 32, respectively. However, we observe that the performances are comparable. Therefore, for efficient training, we set the image number to 16.

## 5 Conclusion

In this work, we study how vision foundation models enable networks to comprehend 2D and 3D environments without relying on labelled data. To address noisy pseudo-labels, we introduce a cross-modality noisy supervision method to simultaneously train 2D and 3D networks, which involves the label refinement strategy, prediction consistency regularization, and latent space consistency regularization. Our method achieves state-of-the-art performance in understanding open worlds, as demonstrated by extensive experiments on indoor and outdoor datasets.

## 6 Acknowledgements

Yuexin Ma was partially supported by NSFC (No.62206173), MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (ShanghaiTech University), and Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShanghaiAI). Tongliang Liu was partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, and IC190100031.

## References

* [1]X. Zhu, H. Zhou, T. Wang, F. Hong, Y. Ma, W. Li, H. Li, and D. Lin (2020) Cylindrical and asymmetrical 3d convolution networks for lidar segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9939-9948. Cited by: SS1, SS2.
* [2]X. Wu, Y. Lao, L. Jiang, X. Liu, and H. Zhao (2022) Point transformer v2: grouped vector attention and partition-based pooling. In Advances in Neural Information Processing Systems, pp. 33330-33342. Cited by: SS1, SS2.
* [3]B. Cheng, M. D. Collins, Y. Zhu, T. Liu, T. S. Huang, H. Adam, and L. Chen (2020) Panoptic-deeplab: a simple, strong, and fast baseline for bottom-up panoptic segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12475-12485. Cited by: SS1, SS2.
* [4]C. R. Qi, H. Su, K. Mo, and L. J. Guibas (2017) PointNet: deep learning on point sets for 3d classification and segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 652-660. Cited by: SS1, SS2.
* [5]X. Zhu, H. Zhou, T. Wang, F. Hong, Y. Ma, W. Li, H. Li, and D. Lin (2021) Cylindrical and asymmetrical 3d convolution networks for lidar segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9939-9948. Cited by: SS1, SS2.
* [6]Y. Hou, X. Zhu, Y. Ma, C. Change Loy, and Y. Li (2022) Point-to-voxel knowledge distillation for lidar semantic segmentation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8479-8488. Cited by: SS1, SS2.
* [7]J. Xu, R. Zhang, J. Dou, Y. Zhu, J. Sun, and S. Pu (2021) RPVNet: a deep and efficient range-point-voxel fusion network for lidar point cloud segmentation. In IEEE/CVF International Conference on Computer Vision, pp. 16024-16033. Cited by: SS1, SS2.
* [8]R. Cheng, R. Razani, E. Taghavi, E. Li, and B. Liu (2021) (af)2-s3net: attentive feature fusion with adaptive feature selection for sparse semantic segmentation network. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 12547-12556. Cited by: SS1, SS2.
* [9]L. Kong, Y. Liu, R. Chen, Y. Ma, X. Zhu, Y. Li, Y. Hou, Y. Qiao, and Z. Liu (2023) Rethinking range view representation for lidar segmentation. In IEEE/CVF International Conference on Computer Vision, pp. 228-240. Cited by: SS1, SS2.

* [10] Fangzhou Hong, Lingdong Kong, Hui Zhou, Xinge Zhu, Hongsheng Li, and Ziwei Liu. Unified 3d and 4d panoptic segmentation via dynamic shifting network. _arXiv preprint arXiv:2203.07186_, 2022.
* [11] Runnan Chen, Xinge Zhu, Nenglun Chen, Wei Li, Yuexin Ma, Ruigang Yang, and Wenping Wang. Zero-shot point cloud segmentation by transferring geometric primitives. _arXiv preprint arXiv:2210.09923_, 2022.
* [12] Bjorn Michele, Alexandre Boulch, Gilles Puy, Maxime Bucher, and Renaud Marlet. Generative zero-shot learning for semantic segmentation of 3d point clouds. In _International Conference on 3D Vision_, pages 992-1002, 2021.
* [13] Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang, Hung-Ting Su, Zhe-Yu Liu, Chin-Tang Chen, Ching-Yu Tseng, and Winston H Hsu. Learning from 2d: Contrastive pixel-to-point knowledge transfer for 3d pretraining. _arXiv preprint arXiv:2104.04687_, 2021.
* [14] Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet. Image-to-lidar self-supervised distillation for autonomous driving data. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9891-9901, 2022.
* [15] Lingdong Kong, Jiawei Ren, Liang Pan, and Ziwei Liu. Lasermix for semi-supervised lidar semantic segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 21705-21715, 2023.
* [16] Lingdong Kong, Niamul Quader, and Venice Erin Liong. Conda: Unsupervised domain adaptation for lidar segmentation via regularized domain concatenation. In _IEEE International Conference on Robotics and Automation_, pages 9338-9345, 2023.
* [17] Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wenping Wang. Clip2scene: Towards label-efficient 3d scene understanding by clip. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7020-7030, 2023.
* [18] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* [19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In _IEEE/CVF International Conference on Computer Vision_, pages 4015-4026, 2023.
* [20] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In _European Conference on Computer Vision_, pages 696-712, 2022.
* [21] Yiteng Xu, Peishan Cong, Yichen Yao, Runnan Chen, Yuenan Hou, Xinge Zhu, Xuming He, Jingyi Yu, and Yuexin Ma. Human-centric scene understanding for 3d large-scale scenarios. In _IEEE/CVF International Conference on Computer Vision_, pages 20349-20359, 2023.
* [22] Youquan Liu, Runnan Chen, Xin Li, Lingdong Kong, Yuchen Yang, Zhaoyang Xia, Yeqi Bai, Xinge Zhu, Yuexin Ma, Yikang Li, et al. Uniseg: A unified multi-modal lidar segmentation network and the openpcseg codebase. In _IEEE/CVF International Conference on Computer Vision_, pages 21662-21673, 2023.
* [23] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. _Advances in Neural Information Processing Systems_, 34:17864-17875, 2021.
* [24] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In _IEEE/CVF International Conference on Computer Vision_, pages 7262-7272, 2021.
* [25] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1290-1299, 2022.
* [26] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In _IEEE/CVF International Conference on Computer Vision_, pages 12179-12188, 2021.
* [27] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Interimimage: Exploring large-scale vision foundation models with deformable convolutions. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14408-14419, 2022.

* [28] Andrew Tao, Karan Sapra, and Bryan Catanzaro. Hierarchical multi-scale attention for semantic segmentation. _arXiv preprint arXiv:2005.10821_, 2020.
* [29] Runnan Chen. Studies on attention modeling for visual understanding. _HKU Theses Online_, 2023.
* [30] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu. Cris: Clip-driven referring image segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11686-11695, 2022.
* [31] Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu. Robo3d: Towards robust and reliable 3d perception against corruptions. In _IEEE/CVF International Conference on Computer Vision_, pages 19994-2000, 2023.
* [32] Runnan Chen, Penghao Zhou, Wenzhe Wang, Nenglun Chen, Pai Peng, Xing Sun, and Wenping Wang. Pr-net: Preference reasoning for personalized video highlight detection. In _IEEE/CVF International Conference on Computer Vision_, pages 7980-7989, 2021.
* [33] MMDetection3D Contributors. Mmdetection3d: Openmmlab next-generation platform for general 3d object detection, 2020.
* [34] Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Liu. Benchmarking 3d perception robustness to common corruptions and sensor failure. In _International Conference on Learning Representations Workshops_, 2023.
* [35] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segment any point cloud sequences by distilling vision foundation models. In _Advances in Neural Information Processing Systems_, 2023.
* [36] Runnan Chen, Xinge Zhu, Nenglun Chen, Dawei Wang, Wei Li, Yuexin Ma, Ruigang Yang, Tongliang Liu, and Wenping Wang. Model2scene: Learning 3d scene representation via contrastive language-cad models pre-training. _arXiv preprint arXiv:2309.16956_, 2023.
* [37] Zhiwen Fan, Peihao Wang, Yifan Jiang, Xinyu Gong, Dejia Xu, and Zhangyang Wang. Nerf-sos: Any-view self-supervised object segmentation on complex scenes. In _International Conference on Learning Representations_, 2022.
* [38] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9729-9738, 2020.
* [39] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. In _Advances in Neural Information Processing Systems_, volume 33, pages 21271-21284, 2020.
* [40] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In _Advances in Neural Information Processing Systems_, volume 27, 2014.
* [41] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In _IEEE/CVF International Conference on Computer Vision_, pages 1422-1430, 2015.
* [42] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15750-15758, 2021.
* [43] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International Conference on Machine Learning_, pages 1597-1607, 2020.
* [44] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In _Advances in Neural Information Processing Systems_, volume 33, pages 9912-9924, 2020.
* [45] Runnan Chen, Xinge Zhu, Nenglun Chen, Dawei Wang, Wei Li, Yuexin Ma, Ruigang Yang, and Wenping Wang. Towards 3d scene understanding by referring synthetic models. _arXiv preprint arXiv:2203.10546_, 2022.
* [46] Nenglun Chen, Lingjie Liu, Zhiming Cui, Runnan Chen, Duygu Ceylan, Changhe Tu, and Wenping Wang. Unsupervised learning of intrinsic structural representation points. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9121-9130, 2020.

* [47] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang. Semi-supervised semantic segmentation with cross pseudo supervision. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2613-2622, 2021.
* [48] Yuchao Wang, Haochen Wang, Yujun Shen, Jingjing Fei, Wei Li, Guoqiang Jin, Liwei Wu, Rui Zhao, and Xinyi Le. Semi-supervised semantic segmentation using unreliable pseudo-labels. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4248-4257, 2022.
* [49] Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao. St++: Make self-training work better for semi-supervised semantic segmentation. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4268-4277, 2022.
* [50] Hanzhe Hu, Fangyun Wei, Han Hu, Qiwei Ye, Jinshi Cui, and Liwei Wang. Semi-supervised semantic segmentation via adaptive equalization learning. _Advances in Neural Information Processing Systems_, 34:22106-22118, 2021.
* [51] Xiangde Luo, Minhao Hu, Tao Song, Guotai Wang, and Shaoting Zhang. Semi-supervised medical image segmentation via cross teaching between cnn and transformer. In _International Conference on Medical Imaging with Deep Learning_, pages 820-833. PMLR, 2022.
* [52] Wei Shen, Zelin Peng, Xuehui Wang, Huayu Wang, Jiazhong Cen, Dongsheng Jiang, Lingxi Xie, Xiaokang Yang, and Q Tian. A survey on label-efficient deep image segmentation: Bridging the gap between weak supervision and dense prediction. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(8):9284-9305, 2023.
* [53] Yassine Ouali, Celine Hudelot, and Myriam Tami. Semi-supervised semantic segmentation with cross-consistency training. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12674-12684, 2020.
* [54] Runnan Chen, Yuexin Ma, Lingjie Liu, Nenglun Chen, Zhiming Cui, Guodong Wei, and Wenping Wang. Semi-supervised anatomical landmark detection via shape-regulated self-training. _Neurocomputing_, 471:335-345, 2022.
* [55] Yuhang Lu, Qi Jiang, Runnan Chen, Yuenan Hou, Xinge Zhu, and Yuexin Ma. See more and know more: Zero-shot point cloud segmentation via multi-modal visual data. In _IEEE/CVF International Conference on Computer Vision_, pages 21674-21684, 2023.
* [56] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. _arXiv preprint arXiv:2211.15654_, 2022.
* [57] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Language-driven open-vocabulary 3d scene understanding. _arXiv preprint arXiv:2211.16312_, 2022.
* [58] Luigi Riz, Cristiano Saltori, Elisa Ricci, and Fabio Poiesi. Novel class discovery for 3d point cloud semantic segmentation. _arXiv preprint arXiv:2303.11610_, 2023.
* [59] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, and Xiang Bai. A simple baseline for zero-shot semantic segmentation with pre-trained vision-language model. _arXiv preprint arXiv:2112.14757_, 2021.
* [60] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. In _International Conference on Learning Representations_, 2022.
* [61] Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick Perez. Zero-shot semantic segmentation. _Advances in Neural Information Processing Systems_, 32, 2019.
* [62] Peike Li, Yunchao Wei, and Yi Yang. Consistent structural relation learning for zero-shot segmentation. _Advances in Neural Information Processing Systems_, 33:10317-10327, 2020.
* [63] Ping Hu, Stan Sclaroff, and Kate Saenko. Uncertainty-aware learning for zero-shot semantic segmentation. _Advances in Neural Information Processing Systems_, 33:21713-21724, 2020.
* [64] Hui Zhang and Henghui Ding. Prototypical matching and open set rejection for zero-shot semantic segmentation. In _IEEE/CVF International Conference on Computer Vision_, pages 6974-6983, 2021.
* [65] Xu Yan, Jiantao Gao, Chaoda Zheng, Chaoda Zheng, Ruimao Zhang, Shenghui Cui, and Zhen Li. 2dpass: 2d priors assisted semantic segmentation on lidar point clouds. In _European Conference on Computer Vision_, pages 677-695, 2022.

* [66] Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet. Image-to-lidar self-supervised distillation for autonomous driving data. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9891-9901, 2022.
* [67] Xidong Peng, Runnan Chen, Feng Qiao, Lingdong Kong, Youquan Liu, Tai Wang, Xinge Zhu, and Yuexin Ma. Sam-guided unsupervised domain adaptation for 3d segmentation. _arXiv preprint arXiv:2310.08820_, 2023.
* [68] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1944-1952, 2017.
* [69] Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao. Learning with bounded instance and label-dependent label noise. In _International Conference on Machine Learning_, pages 1789-1799. PMLR, 2020.
* [70] Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 38(3):447-461, 2015.
* [71] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? _Advances in Neural Information Processing Systems_, 32, 2019.
* [72] Brendan Van Rooyen and Robert C Williamson. A theory of learning with corrupted labels. _J. Mach. Learn. Res._, 18(1):8501-8550, 2017.
* [73] Shikun Li, Xiaobo Xia, Hansong Zhang, Yibing Zhan, Shiming Ge, and Tongliang Liu. Estimating noise transition matrix with label correlations for noisy multi-label learning. In _Advances in Neural Information Processing Systems_, 2022.
* [74] Shuo Yang, Erkun Yang, Bo Han, Yang Liu, Min Xu, Gang Niu, and Tongliang Liu. Estimating instance-dependent bayes-label transition matrix using a deep neural network. In _International Conference on Machine Learning_, pages 25302-25312. PMLR, 2022.
* [75] Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy labels. _Advances in Neural Information Processing Systems_, 26, 2013.
* [76] Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Can gradient clipping mitigate label noise? In _International Conference on Learning Representations_, 2020.
* [77] Giorgio Patrini, Frank Nielsen, Richard Nock, and Marcello Carioni. Loss factorization, weakly supervised learning and label noise robustness. In _International Conference on Machine Learning_, pages 708-717. PMLR, 2016.
* [78] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of machine learning_. MIT press, 2018.
* [79] Brendan Van Rooyen, Aditya Menon, and Robert C Williamson. Learning with symmetric label noise: The importance of being unhinged. _Advances in Neural Information Processing Systems_, 28, 2015.
* [80] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. In _Conference On Learning Theory_, pages 297-299. PMLR, 2018.
* [81] Florian Jell. Theoretical foundations. In _Patent Filing Strategies and Patent Management: An Empirical Study_, pages 4-14. Springer, 2012.
* [82] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. _Advances in Neural Information Processing Systems_, 31, 2018.
* [83] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks. In _International conference on artificial intelligence and statistics_, pages 4313-4324. PMLR, 2020.
* [84] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* [85] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In _International Conference on Machine Learning_, pages 233-242. PMLR, 2017.

* [86] Yingbin Bai, Erkun Yang, Zhaoqing Wang, Yuxuan Du, Bo Han, Cheng Deng, Dadong Wang, and Tongliang Liu. Msr: Making self-supervised learning robust to aggressive augmentations. _arXiv preprint arXiv:2206.01999_, 2022.
* [87] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. _arXiv preprint arXiv:2108.07258_, 2021.
* [88] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 770-778, 2016.
* [89] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In _European Conference on Computer Vision_, pages 213-229, 2020.
* [90] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* [91] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5828-5839, 2017.
* [92] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11621-11631, 2020.
* [93] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11621-11631, 2020.
* [94] Whye Kit Fong, Rohit Mohan, Juana Valeria Hurtado, Lubing Zhou, Holger Caesar, Oscar Beijbom, and Abhinav Valada. Panoptic nuscenes: A large-scale benchmark for lidar panoptic segmentation and tracking. _IEEE Robotics and Automation Letters_, 7:3795-3802, 2022.
* [95] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3075-3084, 2019.
* [96] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. _arXiv preprint arXiv:1706.05587_, 2017.