# Accessing Higher Dimensions for Unsupervised Word Translation

Sida I. Wang

FAIR, Meta

###### Abstract

The striking ability of unsupervised word translation has been demonstrated recently with the help of low-dimensional word vectors / pretraining, which is used by all successful methods and assumed to be necessary. We test and challenge this assumption by developing a method that can also make use of high dimensional signal. Freed from the limits of low dimensions, we show that relying on low-dimensional vectors and their incidental properties miss out on better denoising methods and signals in high dimensions, thus stunting the potential of the data. Our results show that unsupervised translation can be achieved more easily and robustly than previously thought - less than 80MB and minutes of CPU time is required to achieve over 50% accuracy for English to Finnish, Hungarian, and Chinese translations when trained in the same domain; even under domain mismatch, the method still works fully unsupervised on English NewsCrawl to Chinese Wikipedia and English Europarl to Spanish Wikipedia, among others. These results challenge prevailing assumptions on the necessity and superiority of low-dimensional vectors and show that the higher dimension signal can be used rather than thrown away.

## 1 Introduction

The ability to translate words from one language to another without any parallel data nor supervision has been demonstrated in recent works (Lample et al., 2018; Artetxe et al., 2018,...), has long been attempted (Rapp, 1995; Ravi and Knight, 2011,...), and provides empirical answers to scientific questions about language grounding (Bender and Koller, 2020; Sogaard, 2023). However, this striking phenomenon has only been demonstrated with the help of pretrained word vectors or transformer models recently, lending further support to the necessity and superiority of low-dimensional representations. In this work, we develop and test _coocmap1_ for unsupervised word translation using only simple co-occurrence statistics easily computed from raw data. coocmap is dual method of vecmap (Artetxe et al., 2018), using co-occurrences statistics in place of low-dimensional vectors. The greedy and deterministic coocmap establishes the most direct route from raw data to the striking phenomenon of unsupervised word translation, and shows that pretrained representation is not the key.

Footnote 1: code released at [https://github.com/facebookresearch/coocmap](https://github.com/facebookresearch/coocmap)

More surprisingly, coocmap greatly improves the data efficiency and robustness over the baseline of vecmap-fastext, showing that relying on low-dimensional vectors is not only unnecessary but also inferior. With coocmap, 10-40MB of text data and a few minutes of CPU time is sufficient to achieve unsupervised word translation if the training corpora are in the same domain (e.g. both on Wikipedia, Figure 1). For context, this is less than the data used by Brown et al. (1993) to train IBM word alignment models. On cases that were reported not to work using unsupervised methods by Sogaard et al. (2018), we confirm their findings that using fasttext (Bojanowski et al., 2017) vectors indeed fails, while coocmap solved many of these cases with very little data as well. For our main results, we show that less similar language pairs such as English to Hungarian, Finnish and Chinese posed no difficulty and also start to work with 10MB of data for Hungarian, 30MB of data for Finnishand 20MB of data for Chinese. The hardest case of training with domain mismatch (e.g. Wikipedia vs. NewsCrawl or Europarl) is generally reported not to work (Marchisio et al., 2020) especially for dissimilar languages. After aggressively discarding some information using clip and drop, coocmap works on English to Hungarian and Chinese, significantly outperforming vectors, which do not work.

These results challenge the conventional wisdom ever since word vectors proved their worth both for cross-lingual applications (Ruder et al., 2019) and more generally where a popular textbook claims _dense vectors work better in every NLP task than sparse vectors_(Jurafsky and Martin, 2023, 6.8). We discuss reasons that disadvantage low-dimensional vectors unless compression is the goal, which is far more intuitive than the conventional wisdom that _dense vectors are both smaller and perform better_. For vectors to perform, they must have nice linear-algebraic properties (Mikolov et al., 2013, 2013), successfully denoise the data, while still retaining enough useful information. The linear algebraic and denoising properties are _incidental_, since they are not part of the training objective but are a consequence of being in low dimensions. These incidental properties come in conflict with the actual training objective to retain more information as dimension increases, leaving a small window of opportunity for success. The incidental denoising we get from having low dimensions, while interesting and sufficient in easy cases, is actually suboptimal and worse than the more intentional denoising in high dimensions used by coocmap. Furthermore, without the need to have low dimensions, coocmap can access useful information in higher dimensions. The higher dimensions contain knowledge such as _portland-oregon, cbs-60 minutes, molecular-weight, luisiana-purchase, tokyo-1964_ that tend to be lost in lower dimensions. We speculate that similarly processed co-occurrences would outperform low-dimensional vectors in other tasks too, especially if the natural but incidental robustness of vectors is not enough.

## 2 Problem formulation

This word translation task is also called lexicon or dictionary induction in the literature. The task is to translate words in one language to another (e.g. _hello_ to _bonjour_ in French) and is evaluated for accuracy (precision@1) on translations of specific words in a reference dictionary. We do this fully unsupervised, meaning we do not use any seed dictionary or character information. Let the datasets \(D_{1}\) and \(D_{2}\) be sequences of words from the vocabulary sets \(V_{1},V_{2}\). Since we do not use character information, we may refer to vocabulary by their integer indices \(V=[1,2,\ldots]\) for convenience. We would like to find mapping from \(V_{1}\) to \(V_{2}\) based on statistics of the datasets. In particular, we consider the window model where the sequential data is reduced to pairwise co-occurrences over context windows of a fixed size \(m\). The word-context co-occurrence matrix \(\mathrm{Co}\in\mathbb{R}^{|V|\times|V|}\) counts the number of times word \(w\) occurs in the context of \(c\), over a window of some size \(m\)

\[\mathrm{Co}(w,c)=\sum_{i=1}^{|D|}\sum_{-m\leq j\leq m,j\neq 0}\mathbb{I}[w_{i} =w,w_{i+j}=c] \tag{1}\]

where \(w_{i}\) is the \(i\)-th word of the dataset \(D\). The matrix \(\mathrm{Co}\) is the sufficient statistics of the popular word2vec (Mikolov et al., 2013, 2013) and fasttext (Bojanowski et al., 2017), which use the same

Figure 1: Our results focus on the data requirements as we varying the amount and domains of data. The unsupervised accuracy increases quickly with a sharp starting point point and this can happen surprisingly early (\(\sim\)10MB here), reaching >50% by 20MB. As data increases, supervised initialization with ground-truth dictionary (dict-init) does not make much difference as shown by the vanishing initialization gap. vecmap-fasttext needs more data and is less stable.

co-occurrence information, including additional objectives not explicit in the loss function:

\[\ell(\theta)=\sum_{i=1}^{|D|}\sum_{-m\leq j\leq m,j\neq 0}\log_{\theta}p(w_{i+j}|w _{i}). \tag{2}\]

For word translation, we obtain \(\mathrm{Co}_{1}\) and \(\mathrm{Co}_{2}\) from the two languages \(D_{1}\) and \(D_{2}\) separately.

Multilingual distributional hypothesis.If words are characterized by its co-occurrents (Harris, 1954), then translated words will keep their translated co-occurrents. In more precise notation, let \((s_{1},t_{1}),(s_{2},t_{2}),\ldots,(s_{n},t_{n})\) be \(n\) pairs of translated words, then for translation \((s,t)\)

\[X[s,s_{1}],X[s,s_{2}],\ldots,X[s,s_{n}]\sim Z[t,t_{1}],Z[t,t_{2}],\ldots,Z[t,t_ {n}],\]

for association matrices \(X=K(\mathrm{Co}_{1})\), \(Z=K(\mathrm{Co}_{2})\). While intuitive, this is not known to work unsupervised nor with minimal supervision. Rapp (1995) presents evidence and correctly speculates that there may be sufficient signal. Fung (1997); Rapp (1999) used a initial dictionary to successfully expand the vocabulary further using a mutual information based association matrix. Alvarez-Melis and Jaakkola (2018) operates on an association matrix generated from word vectors.

Isomorphism of word vectors.Despite the clear motivation in the association space, unsupervised translation was first shown to work in vector space, where a linear mapping between the vector spaces corresponds to word translation. This is called (approximate) _isomorphism_(Ruder et al., 2019). If \(s\) translate to \(t\), and \(X_{s},Z_{t}\) are their vectors, then \(X_{s}W\sim Z_{t}\) where \(W\) can be a rotation matrix, and the association metric can be cosine-distance, often after applying normalizations to raw word vectors. Successful methods solve this problem using adversarial learning (Lample et al., 2018) or heuristic initialization and iterative refinement (Artetxe et al., 2018; Hoshen and Wolf, 2018), among others (Zhang et al., 2017).

## 3 Method

We aim for simplicity in coocmap, and all steps consists of discrete optimization by arranging columns of \(X\) or measuring distances between rows of \(X\) where \(X\) is the association matrix based on co-occurrences (and \(Z\) for the other language). Almost all operations of coocmap has an analog in vecmap and we will describe both of them for clarity. There are two main steps in both methods, finding best matches and measuring \(\mathrm{cdist}\) pairwise distances. Each row of \(X\) corresponds to a word for both vecmap and coocmap and are inputs to \(\mathrm{cdist}\). vecmap finds a rotation \(W\) that best match given row vectors \(X[s,:]\) and \(Z[t,:]\) for \(s=[s_{1},s_{2},\ldots,s_{n}]\in V_{1}^{n}\) and \(t=[t_{1},t_{2},\ldots,t_{n}]\in V_{2}^{n}\). For coocmap, instead of solving for \(W\), we just re-arrange the columns of the association matrix with indices \(s,t\) directly to get \(X[:,s],Z[:,t]\) where the columns of \(X\) are words too.2

Footnote 2: We show a derivation in E on the equivalence of vecmap and coocmap.

```
\(X\in\mathbb{R}^{|V_{1}|\times|V_{1}|},Z\in\mathbb{R}^{|V_{2}|\times d}\); Input\(s,t\) Output\(s,t\) repeat \(W=\mathrm{solve}(X[s,:],Z[t,:])\) \(D=\mathrm{cdist}(XW,Z)\) \(s,t=\mathrm{match}(D)\) until no more improvement
```

**Algorithm 2** vecmap self-learning

\(\mathrm{cdist}(X,Z)_{ij}=\mathrm{dist}(X_{i},Z_{j})\) is a function that takes pairwise distances for each row of both inputs and then outputs the results in a matrix. The self-learning loop use the same improvement measurement as in vecmap \(\mathrm{mean}_{i}\max_{j}(\mathrm{cdist}(i,j))\), but without stochastic search. We explain how to generate \(X,Z\), \(s,t\), and \(\mathrm{match}\) next.

Measurement.It is important to normalize before taking measurements, and vecmap normalization consists of normalizing each row to have unit \(\ell_{2}\)-norm (\(\mathrm{unitr}\)), center so each column has 0 mean (\(\mathrm{centerc}\)), and normalizing rows (\(\mathrm{unitr}\)) again. For precision,

\[\mathrm{normalize}(Y):=\mathrm{unitr}(\mathrm{center}(\mathrm{unitr}(Y))), \tag{3}\]

where \(\mathrm{centerc}(Y):=Y-\mathrm{sumr}(Y)/r\), \(\mathrm{sumr}(Y):=\mathbf{1}^{T}Y\), and \(Y\in\mathbb{R}^{r\times c}\).

coocmap.The input matrix \(X=\operatorname{normalize}(\operatorname{Co}^{\circ\frac{1}{2}})\) is obtained from \(\operatorname{Co}\) by taking elementwise square-root then normalize. \(\operatorname{cdist}\) is the cosine-distance.

vecmap.From original word vectors \(X^{\prime}\), we use their normalized versions \(X=\operatorname{normalize}(X^{\prime})\). For \(\operatorname{solve}(X[s],Z[t])\), we use \(\operatorname{arg\,min}_{W\in\Omega}\left\|X[s,:]W-Z[t,:]\right\|_{F},\) for rotation matrices \(\Omega\). This is known as the Procrustes problem. \(\operatorname{cdist}\) is the cosine-distance.

Initialization.We need the initial input \(s,t\) for Algorithms 1 and 2. For the unsupervised initialization, we follow vecmap's method based on row-wise sorting. Let \(\operatorname{sort\_row}(X)\) make each row of \(X\) be sorted independently and \(\operatorname{normalize}(X)\) is defined in (3), then

\begin{tabular}{l l} \hline
**Input**\(X,Z\), & **Output**\(D\) \\ \(R(Y):=\operatorname{sort\_row}(K(Y))\) & \\ \(S(Y):=\operatorname{normalize}(R(Y))\) & \\ \(D=\operatorname{cdist}(S(X),S(Z))\) & \\ \hline \end{tabular}

The first step of vecmap \(X=(Y^{\prime}Y^{\prime}\tau)^{\frac{1}{2}}\) is actually converting from vector space to the association space. So it is natural to replace the first step by \(\operatorname{normalize}(\operatorname{Co}_{1}^{\circ 1/2})\) for coocmap.

Matching.The main problem once we have a distance matrix is to solve a matching problem

\[\min_{M}\sum_{(i,j)\in M}\operatorname{cdist}(i,j) \tag{4}\]

vecmap proposes a simple matching method, where we take \(j^{*}=\operatorname{arg\,min}_{j}\operatorname{cdist}(i,j)\) for each \(i\) in forward matching, and then take \(i^{*}=\operatorname{arg\,min}_{i}\operatorname{cdist}(i,j)\) for each \(j\) in backward matching. This always results in \(|V_{1}|+|V_{2}|\) matches where words on each side is guaranteed to appear at least once. For coocmap, there is complete freedom in forming matches \(i,j\) and often many words all match with a single word. As a result, hubness mitigation (Lazaridou et al., 2015) is even more essential compared to vecmap.

While there are many reasonable matching algorithms, we find that Cross-Domain Similarity Local Scaling (CSLS) (Lample et al., 2018) was enough to stabilize coocmap. Note that CSLS acts on the pairwise distances and therefore directly applies to \(\operatorname{cdist}\),

\[\operatorname{csls}(\operatorname{cdist}(i,j))=\operatorname{cdist}(i,j)- \frac{1}{2k}\left(\sum_{j^{\prime}\in N_{i}(k)}\operatorname{cdist}(i,j^{ \prime})+\sum_{i^{\prime}\in N_{j}(k)}\operatorname{cdist}(i^{\prime},j)\right)\]

where \(N_{i}(k)\) are the \(k\)-nearest neighbors to \(i\) according to \(\operatorname{cdist}\). We use \(\operatorname{csls}(\operatorname{cdist})\) as the input to (4) in place of \(\operatorname{cdist}\). Instead of always preferring the best absolute match, CSLS prefers matches that stand out relative to the \(k\) best alternatives.

### Clip and drop

This basic coocmap above already works well on the easier cases and these techniques provided small improvements. However, for difficult cases of training on different domains, clip is essential and drop gives a final boost that can be large. These operations are aggressive and throws away potentially useful information just like rank reduction, but we will show that they are far better especially under domain shifts.

clip.For clip, we threshold the top and bottom values by percentile. While there are just 2 numbers for the 2 thresholds, we use two percentiles operations to determine each of them so they are not dominated by any particular row. Let \(r_{i}=Q_{p}(X_{i})\) be the \(p\)th percentile value of the row \(i\) of \(X\). We threshold \(X\) at \(Q_{p}(r_{1},r_{2},\ldots,r_{|V|})\), where \(p=1\%\) for lowerbound and \(p=99\%\) for upperbound. This results in two thresholds for the entire matrix that seem to agree well across languages. For results on clip, we run coocmap with \(X=\operatorname{clip}(\operatorname{normalize}(\operatorname{Co}^{\circ 1/2}))\). Intuitively, if words are already very strongly associated, obsessing over exactly how strongly is not robust across languages and datasets but can incur large \(\ell_{2}\)s. For lowerbound, extremely negatively associated words is clearly not robust, since the smallest count is 0 and one can always use any two words together. Both bounds seem to improve over baseline, but the upperbound is the more important one.

drop (head).Drop the \(r=20\)_largest_ singular vectors, i.e. \(\mathrm{drop}_{r}(X)=X-X_{r}\in\mathbb{R}^{|V|\times|V|}\), where \(X_{r}\) is the rank-\(r\) approximation of \(X\) by SVD. In the experiments, we first get the solution of \(s,t\) from \(\mathrm{clip}(\mathrm{normalize}(\mathrm{Co}^{\circ 1/2}))\) then use coocmap on \(X=\mathrm{clip}(\mathrm{drop}_{r}(\mathrm{normalize}(\mathrm{Co}^{\circ 1/2})))\) with \(s,t\) as the initial input. Drop was good at getting a final boost from a good solution, but is usually worse than the basic coocmap in obtaining an initial solution. While the dropping top 1 singular vector is discussed by (Mu and Viswanath, 2018; Arora et al., 2017) and popular vectors implicitly drop already (see A), we see more benefits from dropping more and this seems to enable more benefits of the higher dimensions. We show examples in Appendix F in support of this viewpoint.

Truncate (tail).This is the usual rank reduction where \(\mathrm{trunc}_{r}(X)=X_{r}\) is the best rank-\(r\) approximation of \(X\) by SVD. We use this for analysis on the effect of rank.

## 4 Experiments

For the main results, we report accuracy as a function of data size and only show results in the fully unsupervised setting. The accuracy is measured by precision at 1 on the full MUSE dictionary for the given language pair on the 5000 most common words in each language (see Section 6 for limitations). Many results will be shown as scatter plots of accuracy vs. data size, each containing thousands of experiments and more informative than tables. They will capture stability and the qualitative behaviors of the transitional region as the amount of data varies. Each point in the scatter plots represents an experiment where a specific amount of data was taken from the head of the file for training co-occurrence matrices and fasttext vectors with default settings (skipgram, 300 dimension, more in B) for fasttext. cocmap use the same window size as fasttext (\(m=5\)), the same CSLS (\(k=10\)) and same optimization parameters as vecmap. coocmap does not require additional hyperparameters until we add clip and drop. The same amount of data is taken from both sides unless one side is exhausted. In our experiments, most cases either achieve > 50% accuracy (i.e. _works_) or near 0 accuracy and has a definite starting point (i.e. _starts to work_). Summary of these results are in Table 1 and we will show details on progressively harder tests.

Methods.we compare these methods for the main results.

* **dict-init**: initialize with the ground truth dictionary then apply coocmap.
* **coocmap**: fully unsupervised coocmap and improvements with **-clip**, **-drop** if needed.
* **vecmap-fasttext**: apply vecmap to 300 dimensional fasttext vectors trained with default settings.
* **vecmap-raw**: apply vecmap to a svd factorization of the co-occurence matrix. If \(\mathrm{Co}^{\circ 1/2}=USV^{\prime}\), then we use \(US_{r}\) as the word vectors where \(S_{r}\) only keeps top \(r\) singular values. \(r=300\).

For dict-init, we initialized using the ground truth dictionary (i.e. initial input \(s,t\) to Algorithm 1 are the true evaluation dictionary) and then test on the same dictionary after self-learning. This establishes an upper-bound for the amount of gains possible with a better initialization as long as we are using the basic coocmap measurements and self-learning. After coocmap works stably, their performance coincides, showing very few search failures and the limit of gains from initialization.

We use default parameters for fasttext in this section. This may be unfair to fasttext but better match other results reported in the literature where the default hyperparameters are used or pretrained vectors are downloaded. In Section B, we push on the potential of fasttext more and describe hyperparameters.

Data.We test on these languages paired with English (**en**): Spanish (**es**), French (**fr**), German (**de**), Hungarian (**hu**), Finnish (**fi**) and Chinese (**zh**).

For training data we use Wikipedia (**wiki**), Europarl (**parl**), and NewsCrawl (**news**), processed from the source and removing obvious markups so raw text remains. The data is processed using Huggingface WordLevel tokenizer with whitespace pre-tokenizer and lower-cased first.

**wiki** ([https://dumps.wikimedia.org/](https://dumps.wikimedia.org/)): Wikipedia downloaded directly from the official dumps (pages-meta-current), extract text using WikiExtractor (Attardi, 2015) and removed <doc id tags. We start from the first dump until is >1000MB of data for each language and shuffle the combined file. For zh, we also strip away all Latin characters [a-zA-Z] and segment using jieba: [https://github.com/fxsjy/jieba](https://github.com/fxsjy/jieba).

**parl** ([https://www.statmt.org/europarl/](https://www.statmt.org/europarl/)): Europarl (Koehn, 2005) was shuffled after downloading since this is parallel data.

news** ([https://data.statmt.org/news-crawl/](https://data.statmt.org/news-crawl/)): NewsCrawl 2019.es was downloaded and used as is. For 2018-2022.hu and 2018.en, we striped meta data by grep -v on http, trackingCode and { after which a small random sample does not obviously contain more meta data. This removed over 35% from hu news and 0.1% from en.

For evaluation data, we use the _full_ MUSE dictionaries for these language pairs. This allows for a more stable evaluation given our focus on qualitative behaviors, but at a cost of being less comparable to previous results.

### Same domain of data

Similar languages.To establish some baselines, we start with the easy settings where everything eventually works. Figure 2 shows that coocmap starts to work at around 10MB of data while vecmap trained on fasttext only starts to work once there are over 100MB of data. The transitional region is fairly sharp, and coocmap works reliably on each language pair with around 100MB of data, and vecmap with fastText also mostly works once there is around 100MB of data. vecmap-fasttext eventually outperforms coocmap after it was trained on >100MB of data.

Less similar languages.For hard cases from (Sogaard et al., 2018), all of which were reported not to work using MUSE (Lample et al., 2018) using fastText vectors. Here, we confirm that vecmap and fastText vectors also do not work on default settings. However, these are not even challenging to the basic coocmap where all cases started to work with less than 30MB of data.

\begin{table}
\begin{tabular}{l l|r r r}
**source** & **target** & **start** & **start’** & **works** \\ \hline enwiki & eswiki & 9 & 70 & 20 \\ enwiki & frwiki & 10 & 80 & 30 \\ enwiki & zhwiki & 14 & 700 & 50 \\ \hline enwiki & dewiki & 20 & 200 & 70 \\ enparl & huparl & 8 & – & 20 \\ enparl & fiparl & 30 & – & 80 \\ \hline \multicolumn{5}{c}{**Domain mismatch**} \\ \hline
**source** & **target** & **start** & **start’** & **works** \\ \hline enwiki & esnews & 30 & – & 70 \\ enwiki & hunews & 140 & – & 600 \\ enwiki & esparl & 500 & – & 500 \\ ennews & zhwiki & 800 & – & 800 \\ enwiki & huparl & – & – & – \\ enwiki & fiparl & – & – & – \\ \end{tabular}
\end{table}
Table 1: Summary of data requirements for coocmap in **MB** (1e6 bytes). **start**: when coocmap starts to work, **start’**: when vecmap-fasttext baseline starts to work (check Figure 2 to see that **start** is clear), – denotes failure for the whole range; **works**: when coocmap reaches 50% accuracy. Readings are rounded up to the next tick of the log plot, or 1.4 if less than the middle of 1 and 2. The same amount of data is used on both source and target sides, unless one side is used up (e.g. 100MB of huparl or 300MB of esparl). There are less than 0.2 **million tokens per MB** in all datasets, ranging from 0.13 in fiparl, 0.19 in zhwiki and 0.20 for ennews.

Figure 2: accuracy vs. datasize where the source data and target data are in the same domain, either both Wikipedia, or both Europarl (enparl-huparl, and enparl-fiparl). These cases are easy for coocmap but sometimes failed for vecmap-fasttext or required more data.

For the small but clean Europarl data, we tested on English to Finnish (fi) and Hungarian (hu). As shown in Figure 2, cocomap started to work at 9MB for hu and 30MB for fi. It finished the transition region when we used all 300MB of en-fi and 100MB of en-hu. vecmap-fasttext has yet to work, agreeing with the results of Sogaard et al. (2018). However, since vecmap-raw using simple SVD worked, it was surprising that fasttext did not. Indeed, decreasing the dimension to 100 from 300 enables vecmap-fasttext to start working at 50MB of data for enparl-huparl (vs 8MB for cocomap, Figure 6).

Next result is on English to Chinese (zh), where both are trained on Wikipedia. cocomap had a bit more instability vecmap-fasttext also sometimes works with around 1GB of data. The more robust version of cocomap-clip and drop is completely stable, and begin to work with less than 20MB of data.

### Under domain mismatch.

The most difficult case for unsupervised word translation is when data from different domains are used for each language (Sogaard et al., 2018; Marchisio et al., 2020). The original vecmap of Artetxe et al. (2018b) was tested on different types of crawls (WacKy, NewsCrawl, Common Crawl) which did not work in previous methods. Marchisio et al. (2022b) show that vecmap also failed for NewsCrawl to Common Crawl on harder languages. We test on Wikipedia to NewsCrawl and Europarl and see successes on enwiki-esnews, enwiki-hunews, and ennews-zhwiki, **enparl**-eswiki before finally failing on enwiki-fiparl and enwiki-huparl. See Figure 3.

On **enwiki-esnews**, where \(\sim\)100MB of data was required to reach 50%, though the basic coocmap becomes unstable and all vecmap or fasttext based methods failed to work at all. However, clip and drop not only stablizes this but enabled it start to work at 40MB of data.

On **enwiki-hunews**, cocomap mostly fails but get 5% accuracy with above 100MB of data. Impressively, clip and drop fully solves this problem as well, but even clip has a bit of instability, reaching 50% accuracy at 600MB of data.

On **ennews-zhwiki**, cocomap fails completely without clip and drop, and even then requires more data than before. Finally it works and reaching over 50% accuracy around 800MB. In C, we show the data still has higher potential, where truncating to 2000 dimensions enables ennews-zhwiki to work with 300MB or even 100MB of data, though still not reliably.

For more extreme domain mismatch, we test **enparl-eswiki**. In addition to being small, Europarl contains only parliamentary proceedings which has a distinct style and limited range of topics, to our surprise this also worked with 295MB of enparl, and 500MB from eswiki, reaching a final accuracy of 70% suddenly, although basic coocmap also showed no sign of working. All methods failed for enwiki-fiparl and enwiki-huparl in the range limited by Europarl (300MB for fiparl and 90MB for huparl) with up to 1GB of enwiki, reaching the end of our testing.

## 5 Analysis: why coocmap outperformed dense vectors

These main results show that high-dimensional coocmap is more data efficient and significantly more robust than the popular low-dimensional word vectors fasttext/word2vec, which contract prevailing assumptions that vectors are superior and necessary to enable unsupervised translation among other tasks. Ruder et al. (2019) states "word embeddings enables" various interesting cross-lingual phenomena. For unsupervised dictionary induction, landmark papers (Lample et al., 2018a; Artetxe et al., 2018b) needed vectors and even methods that _must_ use a \(|V|\times|V|\) input constructed these from low dimensional vectors anyway (Alvarez-Melis and Jaakkola, 2018; Marchisio et al., 2022a). More generally, the popular textbook Jurafsky and Martin (2023, 6.8) states "dense vectors work better in every NLP task than sparse vectors". Here, we provide natural reasons that disadvantage vectors if we do not mind having higher dimensions.

The conflicting dimensions of vectors.These properties must hold for vectors to work,

1. _Approximate isomorphism_: be able to translate by rotation/linear map
2. _Denoise_: reduce noise and information not robust for the task
3. _Retention_: keep enough information at a given dimension to achieve good performance

By testing the dimension of word vectors in Figure 4, we can see that isomorphism and denoising only holds in low-dimensions. To see this on enwiki-dewiki where fasttext works well, both vecmapfasttext and coocmap-fasttext are better than SVD truncation at low dimensions. Then accuracy goes to 0 as dimension increases. This is not because vectors fails to retain information, since coocmap-fasttext-clip works if we apply additional clipping. See B for more testing on the effect of dimension and other hyperparameters. Notably, lower dimensions have better isomorphism and denoising, and higher dimensions have better accuracy and retention. This trade off leaves a limited window for fasttext to work well. Still, selecting a good dimension is sufficient for fasttext vectors to match the accuracy and data efficiency of coocmap on the easiest cases (en-es, fr, de) and work well enough on dissimilar languages training on the same domain.

Sogaard et al. (2018) notes similar impact of dimensionality themselves, but without exploring enough range that would have solved their enparl-huparl and enparl-fiparl tests (with vecmap). Yin and Shen (2018) argues that optimal dimension may exist because of bias-variance tradeoff. Our experiments show that coocmap keeps improving with higher dimensions, and they may have been misled by relying on linear algebraic properties. The unreliability of isomorphism is also noted by Patra et al. (2019); Marchisio et al. (2022b).

Better denoising in high dimensions.Domain mismatch is where the vectors struggle the most regardless of dimension. In the easiest domain mismatch case of enwiki-esnews of Figure 4, vecmap-fasttext failed whereas coocmap-fasttext-clip worked (though not stably), showing that **clip** helps even when applied on top of the natural but incidental denoising of vectors.

The association matrix of coocmap \(\operatorname{normalize}(\operatorname{Co}^{\circ 1/2})\) is also better overall than other choices such as Mikolov et al. (2013); Levy and Goldberg (2014); Pennington et al. (2014); Rapp (1995). In A, we compared to other full-rank association matrices corresponding to popular vectors and show they also perform worse than coocmap. For instance, the positive pointwise mutual information matrix (PPMI) of Levy and Goldberg (2014) corresponds to word2vec/fasttext. While it can work with

Figure 3: Accuracy vs. data size with clip and drop. Except for enwiki-eswiki and enwiki-zhwiki (top left, top middle), the rest all have domain mismatch where vecmap-fasttext gets \(\approx\)0.

simple \(\ell_{2}\) cdist without normalize and can reach higher accuracy than the basic coocmap if tested on the same domain, it has lower data efficiency and completely fails on the easiest enwiki-esnews with domain mismatch, showing the same behaviors as fasttext. A more aggressive squishing such as \(\operatorname{normalize}(\log(1+\operatorname{Co}))\) works robustly too but is less accurate and less data efficient. Impressively, Rapp (1995) also works fully unsupervised with the prescribed \(\ell_{1}\).

Higher dimensions contain useful information.In all cases, coocmap-clip and coocmap-drop both become more accurate as dimension increases. Under domain mismatch, there is considerable gains above even 1000 dimensions, suggesting that we are losing useful information when going to lower dimensions. Analyzing the association matrices based on which values are clipped, we show in F that higher dimensions retain more world knowledge such as _portland-oregon_, _cbs-60 minutes, molecular-weight, basketball-jordan, luisiana-purchase, tokyo-1964_ and using low dimensional vectors misses out.

## 6 Discussions

Better vectors exist.coocmap shows that it is feasible to denoise in high dimensions better than with popular vectors. Thus, applying denoising suitable to task, the size/type of training data first before reducing dimension is a more principled ways to train lower dimensional vectors than the current practice of relying on poorly optimizing a supposed objective. In this view, training vectors can just focus on how to best retain information in low dimensions and vectors should just get monotonously better with increasing dimensions and stop at an acceptable loss of information for the task and data.

There is room for improvements for vectors of all dimensions. In Figure 4, fasttext got higher accuracy at impressively low dimensions in favorable conditions whereas SVD on coocmap-drop becomes more accurate with increasing dimension. Without actually constructing the better vectors, it is clear that meaningful improvements are possible: _for any given dimension d, there exists rank-d matrices that can achieve a higher accuracy than the max of all rank-d methods already tested in Figure 4._

But why vectors?Here is our opinion on why low-dimensional vectors appeared necessary and better beside social reasons. Co-occurrence counts from real data are noisy in very involved ways and violate standard statistical assumptions, however it is obligatory to apply principled statistics such as likelihood based on multinomial or \(\chi^{2}\), whereas vectors take away these bad options and allow less principled but better methods. Statistics says likelihood ratios is the most data efficient: \(D(s,t)=\sum_{i}p(s,i)\log\frac{p(s,i)}{p(t,i)}\), or perhaps the Hellinger distance \(H(s,t)=\sum_{i}||p(s,i)^{\frac{1}{2}}-p(t,i)^{\frac{1}{2}}||_{2}^{2}\) if we want more robustness. Using raw counts or a term like \(p\log p\) (super-linear in raw counts) always failed when measuring with the \(\ell_{2}\) norm. The likelihood-based method of Ravi and Knight (2011) likely would fail on real data for these reasons. Normalization is a common and crucial element of all working methods. The methods of Rapp (1995); Fung (1997); Rapp (1999) were based on statistical principles but with modifications to make normalized measurements. Levy and Goldberg (2014); Pennington et al. (2014) actually proposed better normalization methods too that would equally apply to co-occurrences (Appendix A).

Giving up on statistics and starting from vecmap lead to big improvements, ironic for our attempt to understand unsupervised translation and how exactly vectors enabled it. To prefer more empirical tests on a wider range of operations instead of believing that we can model natural language data with simple statistics might be another underrated lesson from deep learning. In this work, we give up on statistical modelling without giving up on higher dimensions, which makes available these key tools that were sufficient to outperform low-dimensional vectors: large squeezing operations like \(\operatorname{sqrt}\), normalizing against a baseline (subtracting means, divide by marginals), contrasting with others during matching (CSLS), clip and drop. With just normalize and CSLS, coocmap would have similar accuracy and higher data efficiency compared to fasttext. Clip made co-occurrences more robust than vectors while also increasing accuracy, showing the possibility of denoising in high-dimensions. Finally, drop enabled some access to the information in higher dimensions and lead to noticeably higher accuracy as dimension increases. This is actually a more intuitive situation than the prevailing understanding that going to very low dimensions is both more compact and more accurate.

IBM models.coocmap shows that unsupervised word translation would have been possible with the amount of compute and data used to train IBM word alignment models (Brown et al., 1993).

While we do not test on the exact en-fr Hansards (LDC), we know it has > 1.3 million sentence pairs from the Parliament of Canada. This can be compared to our Europarl en-hu experiments, which has 0.6 million sentence pairs for a more distant language pair, showing there may not be much additional information from the sentence alignments. cocoamp with drop got around 75% accuracy here. Though the compute requirement of cocoamp may have been difficult in '93 - by our estimate en-hu with \(|V|=5000\) probably requires \(|V|^{3}*100\approx 1\) Tera-FLOs, which should have been easy by the early 2000s on computers having Giga-FLOPS.

Limitations.The vocabulary size is a potentially important parameter unexplored in this paper where we used a low 5000 for cleaner/faster experiments. The most expensive step is factorization operations and measuring distances of high dimensional co-occurrences which is \(O(|V|^{3})\) as opposed to \(O(d|V|^{2})\) for vectors, though approximations are available with dimension reduction (see E). Having a low vocabulary size may explain why we see little benefit (except in our hardest cases such as C) from the potential denoising effects of truncation. Low-dimensional vectors is more easily scalable to larger vocabulary and can more easily used by neural models. However, BPE, indexing, truncating vocabulary, multiple window sizes and other alternatives are not explored to better use higher dimensions.

Speculations.As cross-domain is more challenging than just cross-lingual, it is especially likely that our findings may apply in other cross-domain tasks. So we speculate that if co-occurrences are similarly processed without going to low dimensions, they are likely to perform better on other tasks where more robustness is required than the natural robustness of low-dimensions. Similarly, neural models may also benefit from intentional denoising like clip and drop.

Beyond simple vectors, more recent language models may also suffer from low dimensions, which might be why they benefit from retrieval (Khandelwal et al., 2020), where low dimensional vectors may lose too much information for the task. Recently, contextual transformer representations superseded non-contextual vectors with more data and more dimensions pushing up the performance across tasks. Following Kaplan et al. (2020), there are important trade-off between dimensions, data and compute. If data is the limiting factor, it is plausible that better ways of using higher dimensions is helpful.

Related work.Rapp (1995) already hinted that there might be enough signal for fully unsupervised translation. Fung (1997); Rapp (1999); Haghighi et al. (2008) tested their high dimensional approaches with seed words, but without a full search procedure. Ravi and Knight (2011) succeeded in the case of time expressions and parallel data, but likely used insufficient normalization for harder cases. Mikolov et al. (2013) noticed the relation between low-dimensional rotations and translation. With good enough vectors, Zhang et al. (2017); Lample et al. (2018); Artetxe et al. (2018) achieved fully unsupervised word translation while showing that seed words do not matter too much as long as the unsupervised method works. There was much follow up on these successes, in particular, even methods that must use an association matrix (Alvarez-Melis and Jaakkola, 2018; Marchisio et al., 2022) constructed them from low-dimensional vectors.

### Broader Impact

For dictionary induction, this work shows it can be done with less data and is more robust to domain mismatch than previously thought. We have a working procedure that is more direct, using less compute, making it more accessible for educational purposes. Unsupervised translation means learning from the statistics of data alone and can make bland and silly mistakes where outputs should not be relied on unless they can be verified.

## Acknowledgments

Thanks to Mikel Artetxe, Kelly Marchisio, Luke Zettlemoyer, Scott Yih, Freda Shi, Hila Gonen and Tianyi Zhang for helpful discussions.

## References

* Alvarez-Melis and Jaakkola (2018) David Alvarez-Melis and Tommi Jaakkola. 2018. Gromov-wasserstein alignment of word embedding spaces. In _Proc. of EMNLP_, pages 1881-1890.
* Arora et al. (2017) Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simple but tough-to-beat baseline for sentence embeddings. In _ICLR_.
* Arora et al. (2017)Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018a. Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations. In _Proc. of AAAI_.
* Artetxe et al. (2018b) Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018b. A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In _Proc. of ACL_.
* Attardi (2015) Giusepppe Attardi. 2015. Wikiextractor. [https://github.com/attardi/wikiextractor](https://github.com/attardi/wikiextractor).
* Bender and Koller (2020) Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In _Proc of ACL_.
* Bojanowski et al. (2017) Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. _Transactions of the ACL_, 5:135-146.
* Brown et al. (1993) Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. _Computational Linguistics_.
* Church and Hanks (1990) Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. _Computational Linguistics_, 16(1):22-29.
* Dinu et al. (2015) Georgiana Dinu, Angeliki Lazaridou, and Marco Baroni. 2015. Improving zero-shot learning by mitigating the hubness problem. In _Proc. of ICLR_.
* Fung (1997) Pascale Fung. 1997. Finding terminology translations from non-parallel corpora. _Journal of Visual Languages and Computing_.
* Haghighi et al. (2008) Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In _Proc. of ACL_.
* Harris (1954) Zellig S. Harris. 1954. Distributional structure. In _Word_, volume 10, page 146-162.
* Hoshen and Wolf (2018) Yedid Hoshen and Lior Wolf. 2018. Non-adversarial unsupervised word translation. In _Proc. of EMNLP_.
* Jurafsky and Martin (2023) Dan Jurafsky and James H. Martin. 2023. _Speech and language processing: 3rd Edition_. Draft.
* Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. _CoRR_, abs/2001.08361.
* Khandelwal et al. (2020) Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In _ICLR_.
* Koehn (2005) Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In _Proceedings of Machine Translation Summit X: Papers_.
* Lample et al. (2018a) Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio Ranzato. 2018a. Unsupervised machine translation using monolingual corpora only. In _Proc. of ICLR_.
* Lample et al. (2018b) Guillaume Lample, Alexis Conneau, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. 2018b. Word translation without parallel data. In _Proc. of ICLR_.
* Lazaridou et al. (2015) Angeliki Lazaridou, Georgiana Dinu, and Marco Baroni. 2015. Hubness and pollution: Delving into cross-space mapping for zero-shot learning. In _Proc. of ACL_, pages 270-280.
* Levy and Goldberg (2014) Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In _NIPS_.
* Marchisio et al. (2020) Kelly Marchisio, Kevin Duh, and Philipp Koehn. 2020. When does unsupervised machine translation work? In _Proceedings of WMT_, pages 571-583.
* Marchisio et al. (2022a) Kelly Marchisio, Ali Saad-Eldin, Kevin Duh, Carey Priebe, and Philipp Koehn. 2022a. Bilingual lexicon induction for low-resource languages using graph matching via optimal transport. In _Proc of EMNLP_, pages 2545-2561.
* Marchisio et al. (2020)Kelly Marchisio, Neha Verma, Kevin Duh, and Philipp Koehn. 2022b. IsoVec: Controlling the relative isomorphism of word embedding spaces. In _Proc of EMNLP_, pages 6019-6033.
* Mikolov et al. (2013a) Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. _arXiv preprint arXiv:1301.3781_.
* Mikolov et al. (2013b) Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation.
* Mikolov et al. (2013c) Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013c. Distributed representations of words and phrases and their compositionality. In _Proc. of Neurips_.
* Mu and Viswanath (2018) Jiaqi Mu and Pramod Viswanath. 2018. All-but-the-top: Simple and effective postprocessing for word representations. In _ICLR_.
* Patra et al. (2019) Barun Patra, Joel Ruben Antony Moniz, Sarthak Garg, Matthew R. Gormley, and Graham Neubig. 2019. Bilingual lexicon induction with semi-supervision in non-isometric embedding spaces. In _Proc. of ACL_.
* Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe: Global vectors for word representation. In _Proc of EMNLP_.
* Rapp (1995) Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In _Proceedings of the Association for Computational Linguistics_.
* Rapp (1999) Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In _Proc. of ACL_.
* Ravi and Knight (2011) Sujith Ravi and Kevin Knight. 2011. Deciphering foreign language. In _Proc of ACL_.
* Ruder et al. (2019) Sebastian Ruder, Ivan Vulic, and Anders Sogaard. 2019. A survey of cross-lingual word embedding models. _Journal of Artificial Intelligence Research_, 65:569-631.
* Sogaard (2023) Anders Sogaard. 2023. Grounding the vector space of an octopus: Word meaning from raw text. _Minds & Machines_, 33:33-54.
* Sogaard et al. (2018) Anders Sogaard, Sebastian Ruder, and Ivan Vulic. 2018. On the limitations of unsupervised bilingual dictionary induction. In _Proc. of ACL_.
* Stratos et al. (2015) Karl Stratos, Michael Collins, and Daniel Hsu. 2015. Model-based word embeddings from decompositions of count matrices. In _Proc. of ACL_.
* Yin and Shen (2018) Zi Yin and Yuanyuan Shen. 2018. On the dimensionality of word embedding. In _Neurips_.
* Zhang et al. (2017a) Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. 2017a. Adversarial training for unsupervised bilingual lexicon induction. In _Proc. of ACL_.
* Zhang et al. (2017b) Meng Zhang, Yang Liu, Huanbo Luan, and Maosong Sun. 2017b. Earth mover's distance minimization for unsupervised bilingual lexicon induction. In _Proc. of EMNLP_.

Comparison of association matrices

In Figure 5, we compare to several choices of association method \(K(\mathrm{Co})\) and find all of them to be worse than \(\mathrm{Co}^{\circ 1/2}\) plus normalize overall, and the two popular methods from 2010s are better than the two methods from 1990s. These methods all perform normalization by factoring out the marginals one way or another.

Besides the association matrices themselves, we match them with their best normalization method from \(\mathrm{normalize},\ell_{1},\ell_{2}\). We report \(\ell_{2}\) rather than normalize for Levy and Goldberg (2014); Pennington et al. (2014) which proposed their own normalization. In all these cases, normalize gave almost identical results as \(\ell_{2}\), showing their own normalization was sufficient. normalize corresponds to \(\ell_{2}\) cdist since it did \(\ell_{2}\) normalization as the last step in (3). We use \(p(s,i)=\mathrm{Co}(s,i)/\sum_{s^{\prime},i^{\prime}}\mathrm{Co}(s^{\prime},i^{ \prime})\) when a probability was required.

On the top of Figure 5, we compare Rapp (1995); Fung (1997) from the 90s specifically designed for dictionary induction as well. Impressively, Rapp (1995) also performed well using \(\ell_{1}\) distance as prescribed, where it even started to work on enwiki-esnews. Fung (1997) did not work with the prescribed \(\ell_{2}\) criteria (not shown) but worked with \(\ell_{1}\) as well. We think Rapp (1999) is similar to our modification of Fung (1997) with \(\ell_{1}\), which contains its most important term. However, using \(\ell_{1}\) lead to lower data efficiency and lower accuracy in methods that also work with \(\ell_{2}\).

On bottom of Figure 5, we compare two popular association matrices from the 2010s. Notably, the positive PMI of (Levy and Goldberg, 2014) corresponds to word2vec/fasttext, and actually has similar performance characteristics as fasttext, where both reach higher accuracy than coocmap on easy cases but has lower data efficiency and fails on enwiki-esnews. Other methods that use normalize on \(\log\) also worked for enwiki-esnews. Of these, coocmap has the highest accuracy. Levy and Goldberg (2014) achieved higher accuracy while Pennington et al. (2014) worked on enwiki-esnews. The normalization proposed in both methods achieved almost identical results with \(\ell_{2}\) as with normalize. \(\ell_{1}\) also worked on them but clearly worse and has less data efficiency (not shown).

Figure 5: _top_: methods from the 90s, _bottom_: methods from 2010s word2vec and glove.

Pennington et al. (2014) did not specify what the biases \(b\) should be, instead just leaving them to gradient optimization, we picked them to subtract out the mean of logs whereas PMI would subtract the \(\log\) of means. \(\mathrm{Co}^{\circ 1/2}\) was also tested by Stratos et al. (2015) and found to be favourable. We also did not find the shifted PPMI to be better than the basic PPMI of Church and Hanks (1990).

## Appendix B Improving fasttext

In the main results, we used default parameters, where the important ones were skigram, lr: 0.05, dim: 300, epoch: 5. In analysis section, we improve fasttext further by tuning dimension, learning rate and epoch number, then by using coocmap, clip and drop. The effect of dimension has been specifically explored in Figure 4. Here we show how 100 dimension and 300 dimensions as we vary with the size of data - 100-dim tend to be more data efficient whereas 300-dim tend to reach higher performance. The learning rate was slowed as \(0.1(d/50)^{-1/2}\) to account for observed instability in higher dimensions. The epoch was increased to \(5\times(300/|D|)^{1/2}\) for data size \(D\) in MB to run more epoch on smaller data size. We did not observe too much difference between this and default parameters beyond stablizing higher dimensions.

vecmap-fasttext only fully works for the two cases with no domain mismatch - enwiki-dewiki and enparl-huparl. In contrast, coocmap-fasttext show that fasttext contains the information to tackle harder cases of enwiki-esnews, showing that while fasttext is retaining the necessary information, it does not have the necessary linear algebraic property for vecmap to work. On enparl-huparl, vecmap-fasttext did not work for 300 dimensions whereas coocmap-fasttext-clip did using the same vectors with clipping, achieving better accuracy than vecmap-fasttext 100 dim. On enwiki-esnews, only coocmap-fasttext-clip and coocmap-fasttext worked. Finally, all fasttext based methods failed on enwiki-hunews.

It is always possible that we are still using fasttext poorly, but we covered the main factors and clarified the role of dimension which seems sufficient to explain the observed behaviors.

Figure 6: Improving fasttext with hyperparameter and clipping. Drop was set to \(\min(20,20\frac{d}{400})\) resulting in a double curve since \(d=100,300\) were both used.

## Appendix C Clip, drop and truncate

In this section, we try to push our most difficult case of ennews-zhwiki further by varying hyperparameters as well as dimensions. In our main results in Figure 3, ennews-zhwiki barely had any transition before reaching high accuracy at 800MB, making us suspect there might be signal before that but the hyperparameter was not optimal. So here we try clip at 1.5% for **coopmap-clip-1.5, drop-1.5** instead of 1% as in every other experiment. We also adjust drop to \(\min(20,20\frac{d}{400})\) instead of a constant 20 so that less is dropped for lower dimensions \(d\).

In Figure 7, we get the typical behavior of increasing accuracy with increasing dimension for 1000MB of data all the way to the end. For 100MB and 300MB, this only worked when we also truncate some of the tail end of the SVD. Even here, the maximum accuracy was reached at around 2000 dimensions out of 5000, reaching > 50% accuracy at 300MB of data and 2000 dimensions. It is also remarkable that while clipping more resulted in more stability, the final accuracies are virtually identical when both clip and clip-1.5 works or when both drop and drop-1.5 works. This is a case where it is helpful to truncate some of the tail of the SVD as well, at least for 100MB and 300MB where all failed at full dimensions. However, the accuracy still increased up to 2000 dimensions, much higher than typical vectors for the vocabulary size according to conventional wisdom.

## Appendix D More details and ablations

### Effect of initialization

To rule out that fasttext is only limited by the unsupervised initialization step, Figure 8 shows when all methods use the unsupervised initialization of full rank coocmap (left), vs their own unsupervised initialization (right). Here vecmap-fasttext and coocmap-fasttext-sqrt both worked for a slightly wider range of dimensions but the differences are small. Using the clipped version of initialization also did not make much difference. Overall, varying initializations did not matter too much for unsupervised initializations. On the other hand, for supervised initialization, coocmap tend not be very sensitive once there is enough data. vecmap can be more sticky and is more affected by supervised initialization, possibly because \(W\) can reach a local minimum.

### Effect of matching method

The matching method was critical in getting coocmap to work, where simply applying the bidirectional matching does not work even on the simplest enwiki-eswiki. It starts to work in the first few iterations,

Figure 8: Accuracy vs. dimensions on enwiki-esnews. 1000MB of data. Left: use full coocmap initialization. Top red line is when coocmap-drop uses all 5000 dimensions. Right: use their own initialization.

Figure 7: Accuracy vs. **dimension** on ennews to zhwiki, on 100MB, 300MB and 1000MB of data.

but then half of the vocabulary gets matched to the one word (often but not always [UNK] ) and accuracy go to 0. Some hubness mitigation (Dinu et al., 2015) is a must for coocmap. This is not surprising since the indices \(s,t\) can make arbitrary matches, whereas the vector space cannot make arbitrary matches from a rigid rotation.

We tried a naive and greedy mutual matching method that also solved the problem, but CSLS is better, simpler and is more similar to prior works. A proper matching methods such as Hungarian algorithm or softer matching method already tested on bilingual dictionary induction (Marchisio et al., 2022a) should work as well.

Subwords.we used fasttext since it is more standard for this problem, and we also checked that the subwords information made no difference by turning them off in fasttext hyperparameters. This is reassuring for the findings to apply to word2vec as well.

## Appendix E From vectors to association matrices

We derive a more precise relationship between coocmap and vecmap using the notations of Section 3.

Let the \(d\)-dimensional vectors be \(X,Z\in\mathbb{R}^{|V|\times d}\) such that they are whitened \(X^{\intercal}X=Z^{\intercal}Z=I_{d}\). Then for coocmap, the association matrices is \(K(Y)=(YY^{\intercal})^{1/2}=YY^{\intercal}\). We can show that for dot product \(\operatorname{cdist}\), and permutations \(s,t\),

\[\operatorname{cdist}(K(X)[:,s],K(Z)[:,t])=\operatorname{cdist}(XW,Z) \tag{5}\]

where the LHS is the distance function in coocmap of Alg 1 and the RHS is the distance function in vecmap of Alg 2. To see this,

\[\operatorname{cdist}(K(X)[:,s],K(Z)[:,t])\] \[=K(X)[:,s]\cdot K(Z)[:,t]^{\intercal}\] \[=(XX[s])^{\intercal}\cdot(ZZ[t]^{\intercal})^{\intercal}\] \[=XX[s]^{\intercal}\cdot Z[t]Z^{\intercal}\] \[=X(X[s]^{\intercal}Z[t])Z^{\intercal}\] \[=XWZ^{\intercal}\] since

We can try relaxing the whitening assumption. w.l.o.g., let \(X=U_{1}\Sigma_{1},Z=U_{2}\Sigma_{2}\) where \(U_{i}^{\intercal}U_{i}=I_{d}\). This can be obtained via SVD on any \(X^{\prime}=U_{1}\Sigma_{1}V_{1}^{\intercal}\) so that the association matrix is not affected, i.e. \(K(X^{\prime})=(X^{\prime}X^{\prime}\tau)^{1/2}=(XX^{\intercal})^{1/2}=U_{1} \Sigma_{1}U_{1}^{\intercal}\). In this case, \(W=U_{1}[s]^{\intercal}U_{2}[t]\) is the mapping that would make the two sides equal.

However, if we try least square solve as before, we get \(W=\Sigma_{1}^{-1}U_{1}[s]^{\intercal}U_{2}[t]\Sigma_{2}\). The Procrustes solution for \(X[s]W=Z[t]\) (or \(U_{1}[s]\Sigma_{1}W=U_{2}[t]\Sigma_{2}\)) is \(W=\operatorname{svd-norm}(\Sigma_{1}^{\intercal}U_{1}[s]^{\intercal}U_{2}[t] \Sigma_{2}\)). \(\operatorname{svd-norm}\) takes the SVD but set all singular values to 1. Neither will make the two sides equal exactly but these have some similarities.

## Appendix F What is in higher dimensions?

Although the increasing accuracy, the absolute results and the consistency when we keep higher dimensions suggest that there are useful signals, it could always be that we are using lower dimensions poorly or benefting from other unknown effects. In this section, we compare the full-rank matrix and the low-rank matrix to see what exactly is the difference. If the information lost by low rank seems useful or robust, then we can be more confident in retaining this information. Indeed, the low rank version seem to lose or de-emphasize a lot of world knowledge, specific phrases and the like while making the association matrix more like a similarity matrix. In this section, we compare the full-rank matrix and the low-rank matrix to see what exactly is the difference. If the information lost by low rank looks useful or robust, then we can be more confident in retaining this information. Indeed, the low rank version seem to lose or de-emphasize a lot of world knowledge, specific phrases and the like while making the association matrix more like a similarity matrix.

We construct the full dimensional matrix, apply normalize and drop. Then we use the thresholds established by clip. If an entry needs to be clipped in the full-rank matrix, but somehow is already reduced and no longer need to be clipped in the lower-rank matrix, we record this as **full-rank+** in Table 9. Conversely, if low-rank instead increased an entry so that it should be clipped, we note it as **300-dim+**.

From these examples, the differences are clear. The full dimensional matrix contain more world knowledge and otherwise favor dissimilar words that are highly related: _richard-nixon (famous person), portland-oregon maine (geography), error-margin, molecular-weight (scientific term), koyo-1964 (olympic), basketball-jordan james (famous players), cbs-60 minutes (show on CBS), louisiana-parish purchase (famous event)_.

In contrast, the 300-dim+ favors identical or similar words like _state-state, age-age, who-whom, truth-true, koyo-pyeongchang, cbs-fox, family-households_, making the association matrix more like the similarity matrix. This is also intuitive from SVD as well: if the association matrix is \(USV^{\intercal}\), then the similarity matrix is \(US^{2}V^{\intercal}\) whereas low dimension is \(US_{d}V^{\intercal}\) with both \(S^{2}\) and \(S_{d}=\operatorname{trunc}_{d}(S)\) emphasizing higher singular values.

We also tested full-rank vs 1000-dim, where the effects are similar to the full-rank vs 300-dim shown, but more subtle and harder to present clearly. When we tested drop we found drop tend to reduce associations between pairs of numbers, words and punctuations and the like, whereas it increased associations between article and words. It is not clear if this is good or bad.

## Appendix G Example predictions

We include some predictions for extra information. The P@1 is quite generous to small vocabulary \(V\), since it does not evaluate on entries if the source is not in \(V_{1}\), or if no targets are in the \(V_{2}\), treating the evaluation as out-of-vocabulary instead. So we show the number of correct predictions and the source overlap with the reference dictionary, both out of 5000. Results are shown in Table 10.

Figure 9: Left: word pairs favored by the full rank matrix (full-rank+) on NewsCrawl or Wikipedia sorted by the amount of difference over all pairs of words. 300-dim+ not shown since 72/top 100 consists of identical words with the rest mostly synonyms. Right: same comparison but for selected words, also sorted by the difference.

Figure 10: 40 random samples from all predictions. Source indices are included to reflect frequency where smaller means more frequent in the source language (left columns).